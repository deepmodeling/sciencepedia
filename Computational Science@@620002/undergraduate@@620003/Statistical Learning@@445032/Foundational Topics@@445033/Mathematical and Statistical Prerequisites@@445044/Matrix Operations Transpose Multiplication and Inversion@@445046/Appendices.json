{"hands_on_practices": [{"introduction": "In virtually all practical linear regression models, an intercept term is included to account for the baseline value of the response when all features are zero. This practice explores the algebraic consequences of augmenting a feature matrix $X$ with a column of ones. By working through the block matrix structure of the resulting Gram matrix, you will gain a deeper understanding of how the intercept estimate relates to the slope coefficients and why data centering is such a powerful technique for simplifying computations and interpretations [@problem_id:3146970].", "problem": "In a linear model used in statistical learning, it is common to include an intercept term by augmenting the feature matrix with a column of ones. Consider a dataset with $n=4$ observations and $p=2$ features. The feature matrix is\n$$\nX=\\begin{pmatrix}\n0 & 1\\\\\n1 & 1\\\\\n2 & 3\\\\\n3 & 4\n\\end{pmatrix},\n$$\nand the intercept column is $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}$. Define the augmented design matrix $\\tilde{X}=\\begin{pmatrix}\\mathbf{1} & X\\end{pmatrix}$ and the Gram matrix $\\tilde{G}=\\tilde{X}^{\\top}\\tilde{X}$.\n\nUsing only the definitions of matrix transpose, multiplication, and inverse, do the following:\n- Express $\\tilde{G}$ in block form by partitioning conformably with the intercept and the features. Identify each block in terms of $n$, the column sums of $X$, and $X^{\\top}X$, starting from the definition of matrix multiplication.\n- Explain, based on your block expression, why centering the columns of $X$ (i.e., making each feature have sample mean zero) affects the off-diagonal blocks and can decouple the intercept from the slopes.\n- Derive, by solving a partitioned linear system constructed from $\\tilde{G}$ and without quoting any pre-memorized inversion formulas, a closed-form expression for the $(1,1)$ entry of $\\tilde{G}^{-1}$ in terms of $n$, $X^{\\top}X$, and the column sums of $X$.\n- For the specific $X$ given above, compute the exact value of the $(1,1)$ entry of $\\tilde{G}^{-1}$.\n\nProvide the final answer as a single exact number. No rounding is required, and no units are involved. The final answer must be only the value of the $(1,1)$ entry of $\\tilde{G}^{-1}$.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We can proceed with the derivation and calculation.\n\nThe augmented design matrix $\\tilde{X}$ is formed by prepending a column of ones, $\\mathbf{1}$, to the feature matrix $X$. Given that $\\mathbf{1}$ is an $n \\times 1$ matrix and $X$ is an $n \\times p$ matrix, we can write $\\tilde{X}$ in block form as $\\tilde{X} = \\begin{pmatrix} \\mathbf{1} & X \\end{pmatrix}$. For this problem, $n=4$ and $p=2$.\n\n**Part 1: Block form of the Gram matrix $\\tilde{G}$**\n\nThe Gram matrix $\\tilde{G}$ is defined as $\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X}$. To express this in block form, we first find the transpose of $\\tilde{X}$:\n$$\n\\tilde{X}^{\\top} = \\begin{pmatrix} \\mathbf{1} & X \\end{pmatrix}^{\\top} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix}\n$$\nHere, $\\mathbf{1}^{\\top}$ is a $1 \\times n$ row vector of ones, and $X^{\\top}$ is the $p \\times n$ transpose of the feature matrix.\n\nNow, we perform block matrix multiplication to find $\\tilde{G}$:\n$$\n\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix} \\begin{pmatrix} \\mathbf{1} & X \\end{pmatrix} = \\begin{pmatrix} \\mathbf{1}^{\\top}\\mathbf{1} & \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1} & X^{\\top}X \\end{pmatrix}\n$$\nWe identify each block:\n- The top-left block is $\\mathbf{1}^{\\top}\\mathbf{1} = \\sum_{i=1}^{n} 1 \\cdot 1 = n$. This is a $1 \\times 1$ block (a scalar).\n- The top-right block is $\\mathbf{1}^{\\top}X$. This is a $1 \\times p$ row vector where the $j$-th element is the dot product of $\\mathbf{1}$ with the $j$-th column of $X$, which is $\\sum_{i=1}^{n} x_{ij}$. Thus, $\\mathbf{1}^{\\top}X$ is the row vector of column sums of $X$.\n- The bottom-left block is $X^{\\top}\\mathbf{1}$. This is a $p \\times 1$ column vector. It is the transpose of $\\mathbf{1}^{\\top}X$: $( \\mathbf{1}^{\\top}X )^{\\top} = X^{\\top}(\\mathbf{1}^{\\top})^{\\top} = X^{\\top}\\mathbf{1}$. So, it is the column vector of column sums of $X$.\n- The bottom-right block is $X^{\\top}X$, which is the $p \\times p$ Gram matrix of the original features.\n\nTherefore, the block form of $\\tilde{G}$ is:\n$$\n\\tilde{G} = \\begin{pmatrix} n & \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1} & X^{\\top}X \\end{pmatrix}\n$$\n\n**Part 2: Effect of Centering Features**\n\nCentering the columns of $X$ means replacing each feature column $\\mathbf{x}_j$ with a new column $\\mathbf{x}'_j = \\mathbf{x}_j - \\bar{x}_j\\mathbf{1}$, where $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij}$ is the sample mean of the $j$-th feature. Let the centered matrix be denoted $X_c$.\n\nThe crucial effect of this transformation is on the sum of the elements in each new column. The sum of the elements in the $j$-th centered column is:\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = \\mathbf{1}^{\\top}(\\mathbf{x}_j - \\bar{x}_j\\mathbf{1}) = \\mathbf{1}^{\\top}\\mathbf{x}_j - \\bar{x}_j(\\mathbf{1}^{\\top}\\mathbf{1}) = \\left(\\sum_{i=1}^{n} x_{ij}\\right) - \\bar{x}_j \\cdot n\n$$\nBy definition of the mean $\\bar{x}_j$, we have $\\sum_{i=1}^{n} x_{ij} = n\\bar{x}_j$. Substituting this in, we get:\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = n\\bar{x}_j - n\\bar{x}_j = 0\n$$\nThis means that for the centered matrix $X_c$, the off-diagonal blocks of the corresponding Gram matrix $\\tilde{G}_c$ become zero matrices. The top-right block $\\mathbf{1}^{\\top}X_c$ is a $1 \\times p$ zero vector, and the bottom-left block $X_c^{\\top}\\mathbf{1}$ is a $p \\times 1$ zero vector.\n\nThe Gram matrix for the centered data thus becomes block-diagonal:\n$$\n\\tilde{G}_c = \\begin{pmatrix} n & \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1} & X_c^{\\top}X_c \\end{pmatrix}\n$$\nThe inverse of a block-diagonal matrix is the block-diagonal matrix of the inverses of the blocks:\n$$\n\\tilde{G}_c^{-1} = \\begin{pmatrix} n^{-1} & \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1} & (X_c^{\\top}X_c)^{-1} \\end{pmatrix}\n$$\nIn linear regression, the vector of estimated coefficients (intercept and slopes) is given by $\\hat{\\boldsymbol{\\beta}} = (\\tilde{X}^{\\top}\\tilde{X})^{-1}\\tilde{X}^{\\top}\\mathbf{y}$. The block-diagonal structure of $\\tilde{G}_c^{-1}$ implies that the estimation of the intercept term becomes decoupled from the estimation of the slope coefficients. The intercept estimate depends only on the top-left block, while the slope estimates depend only on the bottom-right block. This simplifies both the computation and the interpretation of the model coefficients.\n\n**Part 3: Derivation of the $(1,1)$ entry of $\\tilde{G}^{-1}$**\n\nWe wish to find the entry in the first row and first column of $\\tilde{G}^{-1}$. This entry is the first component of the first column of $\\tilde{G}^{-1}$. Let the first column of $\\tilde{G}^{-1}$ be denoted by the vector $\\mathbf{m}_1$. By definition of the matrix inverse, this column is the solution to the linear system $\\tilde{G}\\mathbf{m}_1 = \\mathbf{e}_1$, where $\\mathbf{e}_1$ is the first standard basis vector, i.e., $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$.\n\nWe partition the vector $\\mathbf{m}_1$ conformably with the block structure of $\\tilde{G}$. Since $\\tilde{G}$ is a $(p+1) \\times (p+1)$ matrix, $\\mathbf{m}_1$ is a $(p+1) \\times 1$ vector. We partition it as $\\mathbf{m}_1 = \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix}$, where $m_{11}$ is a scalar (the $(1,1)$ entry we are looking for) and $\\mathbf{g}$ is a $p \\times 1$ vector. The vector $\\mathbf{e}_1$ is partitioned as $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}$.\n\nThe linear system in block form is:\n$$\n\\begin{pmatrix} n & \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1} & X^{\\top}X \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}\n$$\nThis expands into two equations:\n1. $n \\cdot m_{11} + (\\mathbf{1}^{\\top}X)\\mathbf{g} = 1$\n2. $(X^{\\top}\\mathbf{1})m_{11} + (X^{\\top}X)\\mathbf{g} = \\mathbf{0}_{p \\times 1}$\n\nFrom the second equation, we solve for $\\mathbf{g}$. Assuming $X^{\\top}X$ is invertible (which is true if $X$ has full column rank):\n$$\n(X^{\\top}X)\\mathbf{g} = -(X^{\\top}\\mathbf{1})m_{11} \\implies \\mathbf{g} = -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11}\n$$\nNow, we substitute this expression for $\\mathbf{g}$ into the first equation:\n$$\nn \\cdot m_{11} + (\\mathbf{1}^{\\top}X) \\left( -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11} \\right) = 1\n$$\nSince $m_{11}$ is a scalar, we can factor it out:\n$$\nm_{11} \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right) = 1\n$$\nSolving for $m_{11}$, which is the $(1,1)$ entry of $\\tilde{G}^{-1}$, we obtain the closed-form expression:\n$$\n(\\tilde{G}^{-1})_{11} = m_{11} = \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right)^{-1}\n$$\n\n**Part 4: Computation for the specific matrix $X$**\n\nWe are given $n=4$ and $X=\\begin{pmatrix} 0 & 1\\\\ 1 & 1\\\\ 2 & 3\\\\ 3 & 4 \\end{pmatrix}$. We compute the necessary components for the formula derived above.\n\n- The vector of column sums of $X$ is:\n  $\\mathbf{1}^{\\top}X = \\begin{pmatrix} 0+1+2+3 & 1+1+3+4 \\end{pmatrix} = \\begin{pmatrix} 6 & 9 \\end{pmatrix}$.\n- The transpose is $X^{\\top}\\mathbf{1} = \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}$.\n\n- The matrix $X^{\\top}X$ is:\n  $X^{\\top}X = \\begin{pmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 1 & 3 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1\\\\ 1 & 1\\\\ 2 & 3\\\\ 3 & 4 \\end{pmatrix} = \\begin{pmatrix} 0+1+4+9 & 0+1+6+12 \\\\ 0+1+6+12 & 1+1+9+16 \\end{pmatrix} = \\begin{pmatrix} 14 & 19 \\\\ 19 & 27 \\end{pmatrix}$.\n\n- We need the inverse of $X^{\\top}X$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\n  The determinant of $X^{\\top}X$ is $\\det(X^{\\top}X) = (14)(27) - (19)(19) = 378 - 361 = 17$.\n  The inverse is $(X^{\\top}X)^{-1} = \\frac{1}{17}\\begin{pmatrix} 27 & -19 \\\\ -19 & 14 \\end{pmatrix}$.\n\n- Now we compute the quadratic form $(\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})$:\n  $$\n  (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) = \\begin{pmatrix} 6 & 9 \\end{pmatrix} \\left( \\frac{1}{17}\\begin{pmatrix} 27 & -19 \\\\ -19 & 14 \\end{pmatrix} \\right) \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}\n  $$\n  First, multiply the row vector by the matrix:\n  $$\n  \\frac{1}{17} \\begin{pmatrix} 6 & 9 \\end{pmatrix} \\begin{pmatrix} 27 & -19 \\\\ -19 & 14 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 6(27)+9(-19) & 6(-19)+9(14) \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 162-171 & -114+126 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} -9 & 12 \\end{pmatrix}\n  $$\n  Now, multiply by the column vector:\n  $$\n  \\frac{1}{17} \\begin{pmatrix} -9 & 12 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} = \\frac{1}{17}(-9(6) + 12(9)) = \\frac{1}{17}(-54 + 108) = \\frac{54}{17}\n  $$\n\n- Finally, we compute the $(1,1)$ entry of $\\tilde{G}^{-1}$:\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left( n - \\frac{54}{17} \\right)^{-1} = \\left( 4 - \\frac{54}{17} \\right)^{-1}\n  $$\n  $$\n  4 - \\frac{54}{17} = \\frac{4 \\times 17}{17} - \\frac{54}{17} = \\frac{68 - 54}{17} = \\frac{14}{17}\n  $$\n  Therefore,\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left(\\frac{14}{17}\\right)^{-1} = \\frac{17}{14}\n  $$", "answer": "$$\n\\boxed{\\frac{17}{14}}\n$$", "id": "3146970"}, {"introduction": "A well-posed linear regression requires that the features be linearly independent, but what happens when this assumption is violated? This problem addresses the critical issue of perfect multicollinearity, where one feature is an exact linear combination of others, rendering the Gram matrix $X^{\\top}X$ singular. You will investigate the connection between column redundancy, matrix rank, and eigenvalues, revealing why the standard ordinary least squares estimator is no longer uniquely defined and motivating the need for regularization methods [@problem_id:3146927].", "problem": "In linear regression within statistical learning, let $X \\in \\mathbb{R}^{n \\times p}$ denote a design matrix with columns $\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1},\\mathbf{x}_p$, where $\\mathbf{x}_p = \\mathbf{x}_1$ and $\\mathbf{x}_1 \\neq \\mathbf{0}$. Consider the Gram matrix $G = X^\\top X$, and recall that the ordinary least squares (OLS) estimator is defined by $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$ when the inverse exists, while the ridge estimator is defined by $\\hat{\\boldsymbol{\\beta}}_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top \\mathbf{y}$ for any $\\lambda > 0$. Using only core definitions of matrix transpose, multiplication, invertibility, and linear independence, determine which statements are true about the effect of duplicating a feature on $G$ and its invertibility.\n\nSelect all that apply:\n\nA. $G$ is singular, and at least one eigenvalue of $G$ equals $0$.\n\nB. Without regularization, the OLS solution is unique for all $\\mathbf{y} \\in \\mathbb{R}^n$.\n\nC. For any $\\lambda > 0$, the matrix $X^\\top X + \\lambda I$ is invertible even when $\\mathbf{x}_p = \\mathbf{x}_1$.\n\nD. Duplicating a column always increases every eigenvalue of $G$ by the same positive constant.\n\nE. If $\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1}\\}$ is linearly independent, then removing either $\\mathbf{x}_1$ or $\\mathbf{x}_p$ from $X$ yields a matrix with full column rank.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   $X \\in \\mathbb{R}^{n \\times p}$ is a design matrix.\n-   The columns of $X$ are denoted as $\\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_p$.\n-   A specific condition of linear dependence is given: $\\mathbf{x}_p = \\mathbf{x}_1$.\n-   A non-triviality condition is given: $\\mathbf{x}_1 \\neq \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^n$.\n-   The Gram matrix is defined as $G = X^\\top X$.\n-   The ordinary least squares (OLS) estimator is defined as $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$ for cases where the inverse exists.\n-   The ridge estimator is defined as $\\hat{\\boldsymbol{\\beta}}_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top \\mathbf{y}$ for any scalar $\\lambda > 0$.\n-   The task is to evaluate several statements about $G$ and its invertibility using only definitions of matrix operations and linear independence.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and mathematically grounded within the fields of statistical learning and linear algebra. The definitions provided for the design matrix, Gram matrix, OLS, and ridge regression are standard. The central condition, $\\mathbf{x}_p = \\mathbf{x}_1$, represents a case of perfect multicollinearity, which is a classic topic in regression analysis. The problem is objective, self-contained, and does not violate any scientific or mathematical principles. It is a valid and well-posed problem.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A detailed solution and evaluation of each option will now be performed.\n\n### Derivations from First Principles\nThe core of the problem lies in the condition $\\mathbf{x}_p = \\mathbf{x}_1$. This means the columns of the matrix $X$ are not linearly independent. A set of vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_k\\}$ is linearly dependent if there exist scalars $c_1, \\dots, c_k$, not all zero, such that $c_1\\mathbf{v}_1 + \\dots + c_k\\mathbf{v}_k = \\mathbf{0}$.\n\nFor the columns of $X = [\\mathbf{x}_1, \\dots, \\mathbf{x}_p]$, we have the linear combination:\n$$1 \\cdot \\mathbf{x}_1 + 0 \\cdot \\mathbf{x}_2 + \\dots + 0 \\cdot \\mathbf{x}_{p-1} + (-1) \\cdot \\mathbf{x}_p = \\mathbf{x}_1 - \\mathbf{x}_p = \\mathbf{0}$$\nThis can be expressed in matrix form as $X\\mathbf{c} = \\mathbf{0}$, where $\\mathbf{c}$ is a non-zero vector in $\\mathbb{R}^p$ given by $\\mathbf{c} = [1, 0, \\dots, 0, -1]^\\top$.\n\nThe existence of a non-zero vector $\\mathbf{c}$ such that $X\\mathbf{c} = \\mathbf{0}$ means that the null space of $X$ is non-trivial. This directly implies that the columns of $X$ are linearly dependent, and therefore $X$ does not have full column rank, i.e., $\\text{rank}(X) < p$.\n\nNow consider the Gram matrix $G = X^\\top X$. A fundamental property of the rank is that $\\text{rank}(X^\\top X) = \\text{rank}(X)$. Therefore, $\\text{rank}(G) < p$. Since $G$ is a $p \\times p$ matrix, having a rank less than $p$ means that $G$ is singular (i.e., not invertible).\n\nFurthermore, a square matrix is singular if and only if it has an eigenvalue equal to $0$. We can demonstrate this directly. Using the vector $\\mathbf{c}$ from above:\n$$G\\mathbf{c} = (X^\\top X)\\mathbf{c} = X^\\top (X\\mathbf{c}) = X^\\top \\mathbf{0} = \\mathbf{0}$$\nSince $\\mathbf{c} \\neq \\mathbf{0}$ and $G\\mathbf{c} = \\mathbf{0} = 0 \\cdot \\mathbf{c}$, by the definition of eigenvalues and eigenvectors, $\\mathbf{c}$ is an eigenvector of $G$ with a corresponding eigenvalue of $0$.\n\n### Option-by-Option Analysis\n\n**A. $G$ is singular, and at least one eigenvalue of $G$ equals $0$.**\nAs derived above, the linear dependence of the columns of $X$ implies that the Gram matrix $G = X^\\top X$ is singular. The singularity of a square matrix is equivalent to it having at least one eigenvalue equal to $0$. Our derivation explicitly showed that $G$ has an eigenvalue of $0$. Thus, this statement is a direct consequence of the problem setup.\n**Verdict: Correct.**\n\n**B. Without regularization, the OLS solution is unique for all $\\mathbf{y} \\in \\mathbb{R}^n$.**\nThe OLS solution is determined by the normal equations, $(X^\\top X)\\hat{\\boldsymbol{\\beta}} = X^\\top \\mathbf{y}$. A unique solution for $\\hat{\\boldsymbol{\\beta}}$ exists for any $X^\\top \\mathbf{y}$ if and only if the matrix $X^\\top X = G$ is invertible. As established in the analysis of option A, $G$ is singular. Therefore, its inverse, $(X^\\top X)^{-1}$, does not exist. The system of equations either has no solution or infinitely many solutions for $\\hat{\\boldsymbol{\\beta}}$, but never a unique one. The problem statement's formula for the OLS estimator, $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$, is explicitly conditioned on the existence of the inverse, which is not met here.\n**Verdict: Incorrect.**\n\n**C. For any $\\lambda > 0$, the matrix $X^\\top X + \\lambda I$ is invertible even when $\\mathbf{x}_p = \\mathbf{x}_1$.**\nLet $M = X^\\top X + \\lambda I$. The matrix $G = X^\\top X$ is a Gram matrix, which is always positive semi-definite. This means for any non-zero vector $\\mathbf{v} \\in \\mathbb{R}^p$, $\\mathbf{v}^\\top G \\mathbf{v} = \\mathbf{v}^\\top X^\\top X \\mathbf{v} = (X\\mathbf{v})^\\top (X\\mathbf{v}) = \\|X\\mathbf{v}\\|_2^2 \\ge 0$. This also implies that all eigenvalues of $G$ are non-negative. Let the eigenvalues of $G$ be $\\mu_1, \\dots, \\mu_p$ where $\\mu_i \\ge 0$ for all $i \\in \\{1, \\dots, p\\}$.\nThe eigenvalues of the matrix $M = G + \\lambda I$ are $\\mu_1+\\lambda, \\dots, \\mu_p+\\lambda$.\nSince we are given $\\lambda > 0$ and we know $\\mu_i \\ge 0$, every eigenvalue of $M$ is strictly positive: $\\mu_i + \\lambda > 0$.\nA matrix is invertible if and only if none of its eigenvalues are zero. Since all eigenvalues of $M$ are strictly positive, $M = X^\\top X + \\lambda I$ is always invertible for $\\lambda > 0$. This holds irrespective of the singularity of $X^\\top X$.\n**Verdict: Correct.**\n\n**D. Duplicating a column always increases every eigenvalue of $G$ by the same positive constant.**\nThis statement is imprecisely formulated as it compares eigenvalues of matrices of different dimensions. Let's interpret it as comparing the eigenvalues of the Gram matrix before and after duplication. Let $X_{p-1}$ be the matrix with columns $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}\\}$, and $G_{p-1} = X_{p-1}^\\top X_{p-1}$ be its $(p-1) \\times (p-1)$ Gram matrix. The matrix $X$ is formed by adding $\\mathbf{x}_1$ as the $p$-th column. The resulting Gram matrix $G$ is $p \\times p$.\nConsider a simple counterexample. Let $p=2$, $n=1$, and $\\mathbf{x}_1 = [2]$.\nThe matrix before duplication is $X_1 = [2]$. Its Gram matrix is $G_1 = X_1^\\top X_1 = [4]$. The only eigenvalue is $4$.\nAfter duplicating the column, the matrix is $X = [2, 2]$. The Gram matrix is $G_2 = X^\\top X = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} [2, 2] = \\begin{pmatrix} 4 & 4 \\\\ 4 & 4 \\end{pmatrix}$.\nThe eigenvalues of $G_2$ are the roots of the characteristic equation $\\det(G_2 - \\mu I) = 0$:\n$\\det\\begin{pmatrix} 4-\\mu & 4 \\\\ 4 & 4-\\mu \\end{pmatrix} = (4-\\mu)^2 - 16 = 0 \\implies (4-\\mu)^2 = 16 \\implies 4-\\mu = \\pm 4$.\nThe eigenvalues are $\\mu = 0$ and $\\mu = 8$.\nThe original eigenvalue set was $\\{4\\}$, and the new set is $\\{0, 8\\}$. One eigenvalue decreased from $4$ to $0$, and a new, larger eigenvalue appeared. The statement that \"every eigenvalue\" increases is false.\n**Verdict: Incorrect.**\n\n**E. If $\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1}\\}$ is linearly independent, then removing either $\\mathbf{x}_1$ or $\\mathbf{x}_p$ from $X$ yields a matrix with full column rank.**\nThe matrix $X$ has columns $[\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_p]$. We are given $\\mathbf{x}_p = \\mathbf{x}_1$.\nThe premise is that the set of vectors $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}\\}$ is linearly independent.\nCase 1: Remove column $\\mathbf{x}_p$. The resulting matrix is $X' = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}]$. According to the premise, the columns of $X'$ are linearly independent. A matrix whose columns are linearly independent has full column rank. So, $X'$ has full column rank.\nCase 2: Remove column $\\mathbf{x}_1$. The resulting matrix is $X'' = [\\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_p]$. Substituting $\\mathbf{x}_p = \\mathbf{x}_1$, we get $X'' = [\\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_1]$. The set of columns of $X''$ is $\\{\\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_1\\}$, which is just a permutation of the set of columns of $X'$, namely $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}\\}$. The linear independence of a set of vectors does not depend on their order. Since the columns of $X'$ are linearly independent (by the premise), the columns of $X''$ are also linearly independent. Therefore, $X''$ also has full column rank.\nThe statement holds for both cases.\n**Verdict: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "3146927"}, {"introduction": "Beyond simply fitting a model, a crucial aspect of statistical learning is diagnosing the model's behavior and identifying influential data points. This exercise introduces the hat matrix, $H = X(X^{\\top}X)^{-1}X^{\\top}$, a fundamental tool that reveals the underlying geometry of least squares projection. By deriving and computing a diagonal element of $H$, known as a leverage score, you will learn how matrix operations can quantify the influence of each observation on its own predicted value, providing essential insights for model assessment [@problem_id:3146914].", "problem": "Consider ordinary least squares (OLS) polynomial regression of degree $2$ in the statistical learning framework. You are given inputs $x_{1}, x_{2}, x_{3}, x_{4}$ and form the Vandermonde design matrix $V$ whose $i$-th row is $[1, x_{i}, x_{i}^{2}]$. The fitted values are linear in the observed responses, defining a linear operator known as the hat matrix, whose diagonal entries $h_{ii}$ quantify the leverage of observation $i$.\n\nStarting only from the normal equations $V^{\\top}V\\,\\hat{\\beta} = V^{\\top}y$ and the fact that OLS produces the orthogonal projection of $y$ onto the column space of $V$, derive an analytic expression for the diagonal hat value $h_{ii}$ in terms of the $i$-th row of $V$ and the matrix $(V^{\\top}V)^{-1}$. Then, for the specific input configuration $x_{1} = -1$, $x_{2} = 0$, $x_{3} = 1$, $x_{4} = 2$, compute the exact value of $h_{22}$ for the quadratic Vandermonde matrix described above. Express your final answer as an exact rational number.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of linear algebra and statistical regression, well-posed with sufficient information for a unique solution, and expressed in objective, formal language. There are no contradictions, ambiguities, or pseudo-scientific claims. We may proceed with the solution.\n\nThe problem consists of two parts: first, to derive a general expression for a diagonal element of the hat matrix, and second, to compute a specific value for a given configuration.\n\nPart 1: Derivation of the expression for $h_{ii}$.\n\nIn ordinary least squares (OLS) regression, the goal is to find the coefficient vector $\\hat{\\beta}$ that minimizes the sum of squared residuals. This leads to the normal equations:\n$$V^{\\top}V\\,\\hat{\\beta} = V^{\\top}y$$\nwhere $V$ is the design matrix and $y$ is the vector of observed responses.\n\nAssuming the columns of $V$ are linearly independent, the matrix $V^{\\top}V$ is invertible. We can solve for the estimated coefficient vector $\\hat{\\beta}$:\n$$\\hat{\\beta} = (V^{\\top}V)^{-1} V^{\\top}y$$\n\nThe fitted values of the model, denoted by the vector $\\hat{y}$, are given by:\n$$\\hat{y} = V\\hat{\\beta}$$\n\nSubstituting the expression for $\\hat{\\beta}$ into the equation for $\\hat{y}$, we obtain:\n$$\\hat{y} = V \\left( (V^{\\top}V)^{-1} V^{\\top}y \\right)$$\n\nBy the associativity of matrix multiplication, we can regroup the terms:\n$$\\hat{y} = \\left( V (V^{\\top}V)^{-1} V^{\\top} \\right) y$$\n\nThe matrix that transforms the observed responses $y$ into the fitted values $\\hat{y}$ is defined as the hat matrix, $H$. Therefore:\n$$H = V (V^{\\top}V)^{-1} V^{\\top}$$\nThis matrix $H$ represents the orthogonal projection onto the column space of $V$. The diagonal elements of this matrix, $h_{ii}$, are the leverage scores for each observation $i$.\n\nWe seek an expression for the element $h_{ii}$, which is the element in the $i$-th row and $i$-th column of $H$. Let $v_k^{\\top}$ denote the $k$-th row of the matrix $V$. Then $V$ can be expressed as a stack of its row vectors:\n$$V = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\\\ \\vdots \\\\ v_n^{\\top} \\end{pmatrix}$$\nThe transpose, $V^{\\top}$, is a concatenation of the column vectors $v_k$:\n$$V^{\\top} = \\begin{pmatrix} v_1 & v_2 & \\dots & v_n \\end{pmatrix}$$\nThe $(i,j)$-th element of a matrix product $A B C$ is given by $(\\text{row } i \\text{ of } A) \\times B \\times (\\text{col } j \\text{ of } C)$. To find the diagonal element $h_{ii}$ of $H = V (V^{\\top}V)^{-1} V^{\\top}$, we take the $i$-th row of $V$ and the $i$-th column of $V^{\\top}$.\n\nThe $i$-th row of $V$ is $v_i^{\\top}$.\nThe $i$-th column of $V^{\\top}$ is the transpose of the $i$-th row of $V$, which is $v_i$.\n\nTherefore, the expression for $h_{ii}$ is:\n$$h_{ii} = v_i^{\\top} (V^{\\top}V)^{-1} v_i$$\nThis is the required analytic expression for the diagonal hat value $h_{ii}$ in terms of the $i$-th row of $V$ and the matrix $(V^{\\top}V)^{-1}$.\n\nPart 2: Computation of $h_{22}$ for the specific input configuration.\n\nThe given inputs are $x_{1} = -1$, $x_{2} = 0$, $x_{3} = 1$, and $x_{4} = 2$. The regression is a polynomial of degree $2$. The rows of the Vandermonde design matrix $V$ are of the form $[1, x_{i}, x_{i}^{2}]$. The resulting $4 \\times 3$ matrix $V$ is:\n$$V = \\begin{pmatrix}\n1 & -1 & 1 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 1 \\\\\n1 & 2 & 4\n\\end{pmatrix}$$\nWe need to compute $h_{22}$. Using the formula derived above with $i=2$:\n$$h_{22} = v_2^{\\top} (V^{\\top}V)^{-1} v_2$$\nwhere $v_2^{\\top}$ is the second row of $V$, so $v_2^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$.\n\nFirst, we compute the matrix $V^{\\top}V$:\n$$V^{\\top}V = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & 0 & 1 & 2 \\\\ 1 & 0 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 1 \\\\ 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 2 & 4 \\end{pmatrix}$$\nThe entries of $V^{\\top}V$ are sums of powers of the $x_i$ values:\n$$V^{\\top}V = \\begin{pmatrix}\n\\sum_{i=1}^{4} 1 & \\sum_{i=1}^{4} x_i & \\sum_{i=1}^{4} x_i^2 \\\\\n\\sum_{i=1}^{4} x_i & \\sum_{i=1}^{4} x_i^2 & \\sum_{i=1}^{4} x_i^3 \\\\\n\\sum_{i=1}^{4} x_i^2 & \\sum_{i=1}^{4} x_i^3 & \\sum_{i=1}^{4} x_i^4\n\\end{pmatrix}$$\nLet's compute these sums:\n$\\sum x_i^0 = 1+1+1+1 = 4$\n$\\sum x_i^1 = -1+0+1+2 = 2$\n$\\sum x_i^2 = (-1)^2+0^2+1^2+2^2 = 1+0+1+4 = 6$\n$\\sum x_i^3 = (-1)^3+0^3+1^3+2^3 = -1+0+1+8 = 8$\n$\\sum x_i^4 = (-1)^4+0^4+1^4+2^4 = 1+0+1+16 = 18$\nSo, the matrix is:\n$$V^{\\top}V = \\begin{pmatrix} 4 & 2 & 6 \\\\ 2 & 6 & 8 \\\\ 6 & 8 & 18 \\end{pmatrix}$$\nNext, we must find the inverse, $(V^{\\top}V)^{-1}$. We first calculate the determinant:\n$$\\det(V^{\\top}V) = 4(6 \\cdot 18 - 8 \\cdot 8) - 2(2 \\cdot 18 - 6 \\cdot 8) + 6(2 \\cdot 8 - 6 \\cdot 6)$$\n$$\\det(V^{\\top}V) = 4(108 - 64) - 2(36 - 48) + 6(16 - 36)$$\n$$\\det(V^{\\top}V) = 4(44) - 2(-12) + 6(-20) = 176 + 24 - 120 = 80$$\nThe expression for $h_{22}$ is $v_2^{\\top} (V^{\\top}V)^{-1} v_2$. Since $v_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, the quadratic form $v_2^{\\top} M v_2$ simply extracts the top-left element, $M_{11}$, of any matrix $M$. Thus, $h_{22}$ is the $(1,1)$ element of $(V^{\\top}V)^{-1}$.\n\nThe $(1,1)$ element of the inverse matrix is given by $\\frac{C_{11}}{\\det(V^{\\top}V)}$, where $C_{11}$ is the $(1,1)$ cofactor of $V^{\\top}V$.\n$$C_{11} = (-1)^{1+1} \\det \\begin{pmatrix} 6 & 8 \\\\ 8 & 18 \\end{pmatrix} = 6 \\cdot 18 - 8 \\cdot 8 = 108 - 64 = 44$$\nTherefore, the $(1,1)$ element of $(V^{\\top}V)^{-1}$ is:\n$$((V^{\\top}V)^{-1})_{11} = \\frac{44}{80} = \\frac{11}{20}$$\nAnd so, we have:\n$$h_{22} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} (V^{\\top}V)^{-1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = ((V^{\\top}V)^{-1})_{11} = \\frac{11}{20}$$\n\nThe leverage of the second observation is $\\frac{11}{20}$.", "answer": "$$\\boxed{\\frac{11}{20}}$$", "id": "3146914"}]}