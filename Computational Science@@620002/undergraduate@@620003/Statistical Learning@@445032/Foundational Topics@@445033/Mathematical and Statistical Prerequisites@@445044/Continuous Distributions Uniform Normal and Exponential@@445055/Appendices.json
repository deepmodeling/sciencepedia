{"hands_on_practices": [{"introduction": "While the mean and variance summarize a distribution's center and spread, they do not describe its shape. This practice explores the fourth central moment—kurtosis—to quantify the \"tailedness\" of a distribution and demonstrates a crucial concept in feature engineering: standardizing a variable to have a mean of $0$ and variance of $1$ does not make it normally distributed. By comparing a standardized uniform variable to a standard normal one, you will gain a deeper appreciation for the inherent properties of different probability distributions [@problem_id:3110941].", "problem": "A data engineer prepares features for a statistical learning pipeline by standardizing (z-scoring) each feature to have zero mean and unit variance. Let $X$ be a continuous feature with a uniform distribution on the interval $[a,b]$ where $a<b$, and let $Y$ be a continuous feature with a normal distribution having mean $\\mu$ and variance $\\sigma^{2}$. Define the standardized variables $Z_{X} = (X - \\mathbb{E}[X]) / \\sqrt{\\operatorname{Var}(X)}$ and $Z_{Y} = (Y - \\mathbb{E}[Y]) / \\sqrt{\\operatorname{Var}(Y)}$. Use the fundamental definitions for continuous random variables: probability density function, expectation $\\mathbb{E}[\\cdot]$, variance $\\operatorname{Var}(\\cdot)$, and kurtosis $\\beta_{2}(W) = \\frac{\\mathbb{E}\\!\\left[(W - \\mathbb{E}[W])^{4}\\right]}{\\left(\\operatorname{Var}(W)\\right)^{2}}$ with excess kurtosis $\\gamma_{2}(W) = \\beta_{2}(W) - 3$. \n\nStarting only from these definitions, derive the population excess kurtosis $\\gamma_{2}(Z_{X})$ and $\\gamma_{2}(Z_{Y})$. Then, considering the common practice in Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) to assume approximately normal standardized features, interpret what the sign of the difference $\\Delta = \\gamma_{2}(Z_{X}) - \\gamma_{2}(Z_{Y})$ implies about tail heaviness after z-scoring. \n\nReport the single numerical value of $\\Delta$. No rounding is required.", "solution": "The problem is assessed as valid. It is scientifically grounded in standard probability theory and statistics, well-posed with all necessary definitions provided, and objective in its formulation. It requests a derivation and interpretation of standard statistical quantities.\n\nThe problem requires the derivation of the population excess kurtosis for a standardized uniform random variable, $\\gamma_{2}(Z_{X})$, and a standardized normal random variable, $\\gamma_{2}(Z_{Y})$. The final goal is to compute and interpret their difference, $\\Delta = \\gamma_{2}(Z_{X}) - \\gamma_{2}(Z_{Y})$.\n\nLet $W$ be a continuous random variable. The standardized variable $Z_W$ is defined as $Z_{W} = (W - \\mathbb{E}[W]) / \\sqrt{\\operatorname{Var}(W)}$. By construction, a standardized variable has a mean of $0$ and a variance of $1$.\n$\\mathbb{E}[Z_W] = \\mathbb{E}\\left[\\frac{W - \\mathbb{E}[W]}{\\sqrt{\\operatorname{Var}(W)}}\\right] = \\frac{1}{\\sqrt{\\operatorname{Var}(W)}} (\\mathbb{E}[W] - \\mathbb{E}[\\mathbb{E}[W]]) = \\frac{1}{\\sqrt{\\operatorname{Var}(W)}} (\\mathbb{E}[W] - \\mathbb{E}[W]) = 0$.\n$\\operatorname{Var}(Z_W) = \\operatorname{Var}\\left(\\frac{W - \\mathbb{E}[W]}{\\sqrt{\\operatorname{Var}(W)}}\\right) = \\frac{1}{\\operatorname{Var}(W)} \\operatorname{Var}(W - \\mathbb{E}[W]) = \\frac{1}{\\operatorname{Var}(W)} \\operatorname{Var}(W) = 1$.\n\nThe kurtosis, $\\beta_{2}(W)$, and excess kurtosis, $\\gamma_{2}(W)$, are defined as:\n$$ \\beta_{2}(W) = \\frac{\\mathbb{E}\\!\\left[(W - \\mathbb{E}[W])^{4}\\right]}{\\left(\\operatorname{Var}(W)\\right)^{2}} $$\n$$ \\gamma_{2}(W) = \\beta_{2}(W) - 3 $$\nFor a standardized variable $Z_W$, these formulas simplify. Since $\\mathbb{E}[Z_W]=0$ and $\\operatorname{Var}(Z_W)=1$, the kurtosis becomes the fourth moment of $Z_W$:\n$$ \\beta_{2}(Z_W) = \\frac{\\mathbb{E}\\!\\left[(Z_W - 0)^{4}\\right]}{1^{2}} = \\mathbb{E}[Z_W^4] $$\nThe excess kurtosis is therefore $\\gamma_{2}(Z_W) = \\mathbb{E}[Z_W^4] - 3$.\n\nFirst, we derive the excess kurtosis for the standardized normal variable, $\\gamma_{2}(Z_{Y})$.\nThe variable $Y$ follows a normal distribution, $Y \\sim N(\\mu, \\sigma^2)$. The standardized variable is $Z_{Y} = (Y - \\mathbb{E}[Y]) / \\sqrt{\\operatorname{Var}(Y)} = (Y - \\mu) / \\sigma$. By the properties of the normal distribution, $Z_Y$ follows a standard normal distribution, $Z_Y \\sim N(0, 1)$, with probability density function (PDF) $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\nWe need to compute $\\mathbb{E}[Z_Y^4]$:\n$$ \\mathbb{E}[Z_Y^4] = \\int_{-\\infty}^{\\infty} z^{4} \\phi(z) dz = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} z^{4} \\exp(-\\frac{z^{2}}{2}) dz $$\nWe solve this integral using integration by parts, $\\int u dv = uv - \\int v du$. Let $u = z^{3}$ and $dv = z \\exp(-z^{2}/2) dz$. Then $du = 3z^{2} dz$ and $v = -\\exp(-z^{2}/2)$.\n$$ \\int z^{4} \\exp(-\\frac{z^{2}}{2}) dz = -z^{3}\\exp(-\\frac{z^{2}}{2}) - \\int (-\\exp(-\\frac{z^{2}}{2}))(3z^{2}) dz = -z^{3}\\exp(-\\frac{z^{2}}{2}) + 3 \\int z^{2}\\exp(-\\frac{z^{2}}{2}) dz $$\nEvaluating the definite integral from $-\\infty$ to $\\infty$:\n$$ \\int_{-\\infty}^{\\infty} z^{4} \\exp(-\\frac{z^{2}}{2}) dz = \\left[-z^{3}\\exp(-\\frac{z^{2}}{2})\\right]_{-\\infty}^{\\infty} + 3 \\int_{-\\infty}^{\\infty} z^{2}\\exp(-\\frac{z^{2}}{2}) dz $$\nThe boundary term $\\left[-z^{3}\\exp(-z^{2}/2)\\right]_{-\\infty}^{\\infty}$ evaluates to $0$. The remaining integral is related to the variance of $Z_Y$:\n$$ \\operatorname{Var}(Z_Y) = \\mathbb{E}[Z_Y^2] - (\\mathbb{E}[Z_Y])^2 = \\int_{-\\infty}^{\\infty} z^{2} \\phi(z) dz - 0^2 = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} z^{2} \\exp(-\\frac{z^{2}}{2}) dz $$\nSince we know $\\operatorname{Var}(Z_Y)=1$, we have $\\int_{-\\infty}^{\\infty} z^{2} \\exp(-z^{2}/2) dz = \\sqrt{2\\pi}$.\nSubstituting this back:\n$$ \\mathbb{E}[Z_Y^4] = \\frac{1}{\\sqrt{2\\pi}} \\left( 3 \\int_{-\\infty}^{\\infty} z^{2}\\exp(-\\frac{z^{2}}{2}) dz \\right) = \\frac{3}{\\sqrt{2\\pi}} (\\sqrt{2\\pi}) = 3 $$\nSo, the kurtosis of a standard normal distribution is $\\beta_{2}(Z_{Y}) = 3$. The excess kurtosis is:\n$$ \\gamma_{2}(Z_{Y}) = \\beta_{2}(Z_{Y}) - 3 = 3 - 3 = 0 $$\n\nNext, we derive the excess kurtosis for the standardized uniform variable, $\\gamma_{2}(Z_{X})$.\nThe variable $X$ follows a uniform distribution, $X \\sim U(a,b)$. Its PDF is $f_X(x) = \\frac{1}{b-a}$ for $x \\in [a,b]$.\nThe mean is $\\mathbb{E}[X] = \\frac{a+b}{2}$ and the variance is $\\operatorname{Var}(X) = \\frac{(b-a)^2}{12}$.\nThe standardized variable is $Z_X = \\frac{X - \\mathbb{E}[X]}{\\sqrt{\\operatorname{Var}(X)}} = \\frac{X - (a+b)/2}{(b-a)/\\sqrt{12}}$.\nTo simplify the calculation of $\\mathbb{E}[Z_X^4]$, we can consider a simpler linear transformation of $X$ to a variable $U$ on the interval $[-1, 1]$. Let $U = \\frac{X - (a+b)/2}{(b-a)/2}$. This variable $U$ is uniformly distributed on $[-1, 1]$ with PDF $f_U(u) = 1/2$ for $u \\in [-1, 1]$.\nWe can express $Z_X$ in terms of $U$:\n$$ Z_X = \\frac{U \\cdot (b-a)/2}{(b-a)/\\sqrt{12}} = U \\frac{\\sqrt{12}}{2} = U \\frac{2\\sqrt{3}}{2} = U\\sqrt{3} $$\nNow we can compute the fourth moment of $Z_X$:\n$$ \\mathbb{E}[Z_X^4] = \\mathbb{E}[(U\\sqrt{3})^4] = \\mathbb{E}[9U^4] = 9\\mathbb{E}[U^4] $$\nWe compute $\\mathbb{E}[U^4]$ from its definition:\n$$ \\mathbb{E}[U^4] = \\int_{-1}^{1} u^{4} f_U(u) du = \\int_{-1}^{1} u^{4} \\left(\\frac{1}{2}\\right) du = \\frac{1}{2} \\left[ \\frac{u^5}{5} \\right]_{-1}^{1} = \\frac{1}{10} [1^5 - (-1)^5] = \\frac{1}{10} (1 - (-1)) = \\frac{2}{10} = \\frac{1}{5} $$\nThus, the fourth moment of $Z_X$ is $\\mathbb{E}[Z_X^4] = 9 \\times \\frac{1}{5} = \\frac{9}{5}$.\nThe kurtosis of the standardized uniform distribution is $\\beta_{2}(Z_X) = \\frac{9}{5}$. Its excess kurtosis is:\n$$ \\gamma_{2}(Z_{X}) = \\beta_{2}(Z_{X}) - 3 = \\frac{9}{5} - 3 = \\frac{9-15}{5} = -\\frac{6}{5} $$\n\nFinally, we compute the difference $\\Delta$:\n$$ \\Delta = \\gamma_{2}(Z_{X}) - \\gamma_{2}(Z_{Y}) = -\\frac{6}{5} - 0 = -\\frac{6}{5} $$\n\nInterpretation:\nThe excess kurtosis $\\gamma_2$ measures the \"tailedness\" of a distribution relative to the normal distribution, which serves as the benchmark with $\\gamma_2 = 0$.\nA positive excess kurtosis ($\\gamma_2 > 0$) indicates a leptokurtic distribution, which is more peaked and has heavier tails than a normal distribution.\nA negative excess kurtosis ($\\gamma_2 < 0$) indicates a platykurtic distribution, which is less peaked and has lighter tails.\nOur result $\\gamma_{2}(Z_{X}) = -6/5 = -1.2$ shows that the standardized uniform distribution is platykurtic. This is because the uniform distribution is bounded, so its probability density drops to zero outside a finite interval, meaning it has \"lighter\" tails (in fact, no tails) compared to the unbounded normal distribution.\nThe difference $\\Delta = -6/5$ is negative. This signifies that after z-scoring, the uniform feature $Z_X$ has substantially lighter tails than the normal feature $Z_Y$. Common methods like PCA and LDA can be sensitive to the distribution of features, and their theoretical properties are often derived under the assumption of normality. Using such methods on a uniformly distributed feature, under the implicit assumption that standardization makes it \"approximately normal,\" would be a poor approximation. Specifically, the model would incorrectly assume a non-zero probability for extreme values that can never occur for the bounded uniform feature. This highlights the fact that standardization centers and scales a distribution but does not change its fundamental shape, including its kurtosis.\n\nThe single numerical value of $\\Delta$ is $-\\frac{6}{5} = -1.2$.", "answer": "$$ \\boxed{-1.2} $$", "id": "3110941"}, {"introduction": "A fundamental task in statistics is to estimate the unknown parameters of a distribution from observed data. This exercise focuses on the exponential distribution, which is essential for modeling waiting times or lifetimes, and its rate parameter $\\lambda$. You will derive estimators for $\\lambda$ using two of the most important frameworks—the method of moments and maximum likelihood estimation—and then rigorously analyze their quality in terms of bias and mean squared error [@problem_id:3111040].", "problem": "A random sample $\\{X_{1},\\dots,X_{n}\\}$ is drawn independently from an exponential distribution with unknown rate parameter $\\lambda>0$, whose probability density function is $f(x\\mid \\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\ge 0$. Let $\\bar X=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ denote the sample mean.\n\nTwo classical point estimators for $\\lambda$ are considered:\n- The maximum likelihood estimator $\\hat{\\lambda}_{\\mathrm{MLE}}$, defined by maximizing the likelihood based on the joint density of $\\{X_{i}\\}_{i=1}^{n}$.\n- The method-of-moments estimator $\\tilde{\\lambda}$, defined by equating the population mean of $X$ to $\\bar X$.\n\nTasks:\n1) Starting from the definitions of maximum likelihood estimation and method of moments, derive $\\hat{\\lambda}_{\\mathrm{MLE}}$ and $\\tilde{\\lambda}$ in terms of $\\bar X$ and show whether they coincide.\n\n2) Let $S=\\sum_{i=1}^{n}X_{i}$. Using only the joint density of the sample and the well-tested fact that $S$ has a gamma distribution with shape parameter $n$ and rate parameter $\\lambda$, derive exact closed-form expressions for the finite-sample bias $\\operatorname{Bias}(\\tilde{\\lambda})=\\mathbb{E}[\\tilde{\\lambda}]-\\lambda$ and the variance $\\operatorname{Var}(\\tilde{\\lambda})$ for $n>2$, without relying on asymptotic approximations.\n\n3) Consider the scaled estimator $\\hat{\\lambda}_{U}=\\frac{c}{\\bar X}$ for some constant $c>0$. Determine the unique $c$ such that $\\hat{\\lambda}_{U}$ is unbiased for $\\lambda$ for $n>1$, and compute its mean squared error $\\operatorname{MSE}(\\hat{\\lambda}_{U})$ exactly for $n>2$.\n\n4) Define the mean squared error ratio $R(n)=\\frac{\\operatorname{MSE}(\\hat{\\lambda}_{U})}{\\operatorname{MSE}(\\tilde{\\lambda})}$. Provide $R(n)$ as a single simplified closed-form expression in terms of $n$ only. Your final reported answer must be this expression for $R(n)$ (no units). Do not round; report an exact expression.", "solution": "The problem statement has been validated and is deemed sound, well-posed, objective, and self-contained. It is a standard problem in theoretical statistics and is free of any scientific or logical flaws.\n\n1) Derivation of $\\hat{\\lambda}_{\\mathrm{MLE}}$ and $\\tilde{\\lambda}$\n\nThe maximum likelihood estimator (MLE) $\\hat{\\lambda}_{\\mathrm{MLE}}$ is found by maximizing the likelihood function. The likelihood function for a sample $\\{X_{1},\\dots,X_{n}\\}$ from an exponential distribution with rate $\\lambda$ is the joint probability density function:\n$$L(\\lambda \\mid x_1, \\dots, x_n) = \\prod_{i=1}^{n} f(x_i \\mid \\lambda) = \\prod_{i=1}^{n} \\lambda \\exp(-\\lambda x_i) = \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} x_i\\right)$$\nIt is more convenient to maximize the log-likelihood function, $\\ell(\\lambda) = \\ln L(\\lambda)$:\n$$\\ell(\\lambda) = n \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} x_i$$\nTo find the maximum, we differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero:\n$$\\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i = 0$$\nSolving for $\\lambda$ gives the MLE:\n$$\\hat{\\lambda}_{\\mathrm{MLE}} = \\frac{n}{\\sum_{i=1}^{n} X_i} = \\frac{1}{\\frac{1}{n} \\sum_{i=1}^{n} X_i} = \\frac{1}{\\bar X}$$\n\nThe method-of-moments (MOM) estimator $\\tilde{\\lambda}$ is found by equating the first population moment (the mean) to the first sample moment (the sample mean). The population mean of an exponential distribution with rate $\\lambda$ is:\n$$\\mathbb{E}[X] = \\int_{0}^{\\infty} x f(x \\mid \\lambda) dx = \\int_{0}^{\\infty} x \\lambda \\exp(-\\lambda x) dx = \\frac{1}{\\lambda}$$\nEquating this to the sample mean $\\bar X$:\n$$\\frac{1}{\\lambda} = \\bar X$$\nSolving for $\\lambda$ gives the MOM estimator:\n$$\\tilde{\\lambda} = \\frac{1}{\\bar X}$$\nThus, the maximum likelihood estimator and the method-of-moments estimator coincide: $\\hat{\\lambda}_{\\mathrm{MLE}} = \\tilde{\\lambda} = \\frac{1}{\\bar X}$.\n\n2) Bias and Variance of $\\tilde{\\lambda}$\n\nThe estimator is $\\tilde{\\lambda} = \\frac{1}{\\bar X} = \\frac{n}{\\sum_{i=1}^{n} X_i} = \\frac{n}{S}$. We are given that $S = \\sum_{i=1}^{n} X_i$ follows a gamma distribution with shape parameter $n$ and rate parameter $\\lambda$, denoted $S \\sim \\Gamma(n, \\lambda)$. The probability density function of $S$ is:\n$$f_S(s) = \\frac{\\lambda^n}{\\Gamma(n)} s^{n-1} \\exp(-\\lambda s) \\quad \\text{for } s \\ge 0$$\nTo find the bias and variance of $\\tilde{\\lambda}$, we need the moments of $\\frac{1}{S}$. For any $k>0$, the $k$-th inverse moment of $S$ is:\n$$\\mathbb{E}[S^{-k}] = \\int_{0}^{\\infty} s^{-k} f_S(s) ds = \\int_{0}^{\\infty} s^{-k} \\frac{\\lambda^n}{\\Gamma(n)} s^{n-1} \\exp(-\\lambda s) ds$$\n$$\\mathbb{E}[S^{-k}] = \\frac{\\lambda^n}{\\Gamma(n)} \\int_{0}^{\\infty} s^{n-k-1} \\exp(-\\lambda s) ds$$\nRecognizing the integral is related to the gamma function, we recall $\\int_{0}^{\\infty} t^{\\alpha-1} \\exp(-\\beta t) dt = \\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}$. Here, $\\alpha = n-k$ and $\\beta = \\lambda$. The integral is convergent for $n-k>0$.\n$$\\mathbb{E}[S^{-k}] = \\frac{\\lambda^n}{\\Gamma(n)} \\frac{\\Gamma(n-k)}{\\lambda^{n-k}} = \\frac{\\lambda^k \\Gamma(n-k)}{\\Gamma(n)}$$\nUsing the property $\\Gamma(z) = (z-1)\\Gamma(z-1)$, we have $\\Gamma(n) = (n-1)(n-2)\\dots(n-k)\\Gamma(n-k)$.\n$$\\mathbb{E}[S^{-k}] = \\frac{\\lambda^k}{(n-1)(n-2)\\dots(n-k)}$$\nThis holds for $n > k$.\n\nFor the bias, we need $\\mathbb{E}[\\tilde{\\lambda}]$. We use the formula for $\\mathbb{E}[S^{-k}]$ with $k=1$, which requires $n>1$. Since the problem specifies $n>2$, this condition is met.\n$$\\mathbb{E}[\\tilde{\\lambda}] = \\mathbb{E}\\left[\\frac{n}{S}\\right] = n \\mathbb{E}[S^{-1}] = n \\frac{\\lambda^1}{(n-1)} = \\frac{n\\lambda}{n-1}$$\nThe bias is then:\n$$\\operatorname{Bias}(\\tilde{\\lambda}) = \\mathbb{E}[\\tilde{\\lambda}] - \\lambda = \\frac{n\\lambda}{n-1} - \\lambda = \\lambda \\left(\\frac{n}{n-1} - 1\\right) = \\frac{\\lambda}{n-1}$$\nFor the variance, we need $\\mathbb{E}[\\tilde{\\lambda}^2]$.\n$$\\mathbb{E}[\\tilde{\\lambda}^2] = \\mathbb{E}\\left[\\left(\\frac{n}{S}\\right)^2\\right] = n^2 \\mathbb{E}[S^{-2}]$$\nUsing the formula for $\\mathbb{E}[S^{-k}]$ with $k=2$, which requires $n>2$:\n$$\\mathbb{E}[S^{-2}] = \\frac{\\lambda^2}{(n-1)(n-2)}$$\nSo, $\\mathbb{E}[\\tilde{\\lambda}^2] = \\frac{n^2 \\lambda^2}{(n-1)(n-2)}$.\nThe variance is:\n$$\\operatorname{Var}(\\tilde{\\lambda}) = \\mathbb{E}[\\tilde{\\lambda}^2] - (\\mathbb{E}[\\tilde{\\lambda}])^2 = \\frac{n^2 \\lambda^2}{(n-1)(n-2)} - \\left(\\frac{n\\lambda}{n-1}\\right)^2$$\n$$\\operatorname{Var}(\\tilde{\\lambda}) = \\frac{n^2 \\lambda^2}{(n-1)^2} \\left(\\frac{n-1}{n-2} - 1\\right) = \\frac{n^2 \\lambda^2}{(n-1)^2} \\left(\\frac{(n-1)-(n-2)}{n-2}\\right) = \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)}$$\n\n3) Unbiased Estimator $\\hat{\\lambda}_{U}$ and its MSE\n\nThe scaled estimator is $\\hat{\\lambda}_{U} = \\frac{c}{\\bar X} = \\frac{cn}{S}$. For this estimator to be unbiased, we must have $\\mathbb{E}[\\hat{\\lambda}_{U}] = \\lambda$.\n$$\\mathbb{E}[\\hat{\\lambda}_{U}] = \\mathbb{E}\\left[\\frac{cn}{S}\\right] = cn \\mathbb{E}[S^{-1}] = cn \\frac{\\lambda}{n-1}$$\nThis is valid for $n>1$. Setting this equal to $\\lambda$:\n$$cn \\frac{\\lambda}{n-1} = \\lambda \\implies c = \\frac{n-1}{n}$$\nThe unbiased estimator is $\\hat{\\lambda}_{U} = \\frac{(n-1)/n}{\\bar X} = \\frac{n-1}{n\\bar X} = \\frac{n-1}{S}$.\nThe mean squared error (MSE) of an estimator $\\hat{\\theta}$ is $\\operatorname{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + (\\operatorname{Bias}(\\hat{\\theta}))^2$. Since $\\hat{\\lambda}_{U}$ is unbiased, its bias is zero, and its MSE is equal to its variance.\n$$\\operatorname{MSE}(\\hat{\\lambda}_{U}) = \\operatorname{Var}(\\hat{\\lambda}_{U}) = \\operatorname{Var}\\left(\\frac{n-1}{S}\\right) = (n-1)^2 \\operatorname{Var}(S^{-1})$$\n$$\\operatorname{Var}(S^{-1}) = \\mathbb{E}[S^{-2}] - (\\mathbb{E}[S^{-1}])^2 = \\frac{\\lambda^2}{(n-1)(n-2)} - \\left(\\frac{\\lambda}{n-1}\\right)^2$$\nThis calculation requires $n>2$.\n$$\\operatorname{Var}(S^{-1}) = \\frac{\\lambda^2}{(n-1)^2} \\left(\\frac{n-1}{n-2} - 1 \\right) = \\frac{\\lambda^2}{(n-1)^2(n-2)}$$\nSubstituting this into the MSE expression:\n$$\\operatorname{MSE}(\\hat{\\lambda}_{U}) = (n-1)^2 \\left( \\frac{\\lambda^2}{(n-1)^2(n-2)} \\right) = \\frac{\\lambda^2}{n-2}$$\n\n4) Mean Squared Error Ratio $R(n)$\n\nWe first compute the MSE of $\\tilde{\\lambda}$ for $n>2$:\n$$\\operatorname{MSE}(\\tilde{\\lambda}) = \\operatorname{Var}(\\tilde{\\lambda}) + (\\operatorname{Bias}(\\tilde{\\lambda}))^2$$\nUsing the results from part 2):\n$$\\operatorname{MSE}(\\tilde{\\lambda}) = \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)} + \\left(\\frac{\\lambda}{n-1}\\right)^2 = \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)} + \\frac{\\lambda^2(n-2)}{(n-1)^2(n-2)}$$\n$$\\operatorname{MSE}(\\tilde{\\lambda}) = \\frac{\\lambda^2}{(n-1)^2(n-2)} (n^2 + n-2)$$\nThe quadratic term $n^2+n-2$ factors as $(n+2)(n-1)$.\n$$\\operatorname{MSE}(\\tilde{\\lambda}) = \\frac{\\lambda^2(n+2)(n-1)}{(n-1)^2(n-2)} = \\frac{\\lambda^2(n+2)}{(n-1)(n-2)}$$\nNow, we can compute the ratio $R(n) = \\frac{\\operatorname{MSE}(\\hat{\\lambda}_{U})}{\\operatorname{MSE}(\\tilde{\\lambda})}$.\n$$R(n) = \\frac{\\frac{\\lambda^2}{n-2}}{\\frac{\\lambda^2(n+2)}{(n-1)(n-2)}} = \\frac{\\lambda^2}{n-2} \\cdot \\frac{(n-1)(n-2)}{\\lambda^2(n+2)}$$\n$$R(n) = \\frac{n-1}{n+2}$$\nThis is the simplified closed-form expression for the ratio for $n>2$.", "answer": "$$\\boxed{\\frac{n-1}{n+2}}$$", "id": "3111040"}, {"introduction": "The probability integral transform (PIT) is a powerful theoretical tool that creates a profound link between all continuous distributions and the standard uniform distribution. This practice first asks you to formally prove this transformative result for the normal distribution, solidifying your theoretical understanding. You will then apply this principle in a practical coding exercise to build a calibration test, a critical diagnostic for evaluating the reliability of modern probabilistic models [@problem_id:3110957].", "problem": "You are given that a random variable $Z$ is standard normal, written $Z \\sim \\mathcal{N}(0,1)$, with cumulative distribution function $ \\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} \\, dt$. Consider the probability integral transform (PIT): define $U = \\Phi(Z)$. \n\nTask 1 (theoretical verification): Starting from the definition of a cumulative distribution function and the monotonicity of $ \\Phi(\\cdot)$, derive from first principles that $U$ has a standard uniform distribution, written $U \\sim \\mathrm{Unif}(0,1)$. Your derivation must not assume the result; it must use only core definitions such as the definition of a cumulative distribution function and properties of strictly increasing functions.\n\nTask 2 (calibration test design): In probabilistic prediction for a continuous target $Y$, a model outputs a predictive cumulative distribution function $F(y)$. The model is calibrated if, when $Y$ is drawn from the true data-generating process and $U = F(Y)$ is computed for each prediction–observation pair, the $U$ values are distributed as $ \\mathrm{Unif}(0,1)$. Design a practical, deterministic test of calibration based on PIT histograms as follows:\n- Use a histogram with $B$ equal-width bins over the interval $[0,1]$ to summarize the PIT values.\n- Use a chi-square goodness-of-fit test comparing the observed bin counts against the uniform expected counts. Clearly state the decision rule in terms of a significance level $\\alpha$.\n- To eliminate randomness and ensure reproducibility, generate a deterministic synthetic dataset representative of the ideal calibrated setting as follows. Set an integer $N \\ge 1$. Define grid points $u_i = \\frac{i - 1/2}{N}$ for $i = 1,2,\\dots,N$. Map these to “true” observations from a standard normal distribution by $y_i = \\Phi^{-1}(u_i)$, where $\\Phi^{-1}(\\cdot)$ is the quantile function (inverse cumulative distribution function) of $\\mathcal{N}(0,1)$. This construction produces a deterministic sample that is indistinguishable, in distribution, from draws $Y \\sim \\mathcal{N}(0,1)$ when used under the PIT framework.\n- Given a normal predictive family $F_{\\text{pred}}(y) = \\Phi\\!\\left(\\frac{y - \\mu_{\\text{pred}}}{\\sigma_{\\text{pred}}}\\right)$ with parameters $\\mu_{\\text{pred}} \\in \\mathbb{R}$ and $\\sigma_{\\text{pred}} > 0$, compute PIT values $u_i^{\\text{pred}} = F_{\\text{pred}}(y_i)$ and perform the histogram-based chi-square test against uniformity.\n\nYour program must implement this procedure and evaluate the following test suite of parameter settings. For each case, report a boolean indicating whether the null hypothesis of uniform PIT (calibration) is rejected at significance level $\\alpha$ using the chi-square goodness-of-fit test with $B$ bins. Use the deterministic construction described above; do not use any pseudorandom number generator.\n\nTest suite (each tuple is $(\\mu_{\\text{pred}}, \\sigma_{\\text{pred}}, N, B, \\alpha)$):\n- Case A (happy path, exact calibration): $(0, 1, 20000, 20, 0.01)$.\n- Case B (systematic mean error): $(0.75, 1, 20000, 20, 0.01)$.\n- Case C (underdispersion): $(0, 0.6, 20000, 20, 0.01)$.\n- Case D (overdispersion): $(0, 1.6, 20000, 20, 0.01)$.\n\nFinal output requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example, $[r_A,r_B,r_C,r_D]$ where each $r_{\\cdot}$ is a boolean. No additional text should be printed.", "solution": "### Task 1: Theoretical Verification of the Probability Integral Transform\n\nLet $Z$ be a random variable with a continuous and strictly increasing cumulative distribution function (CDF), $F_Z(z) = P(Z \\le z)$. In this specific problem, $Z \\sim \\mathcal{N}(0,1)$ and its CDF is denoted by $\\Phi(z)$. The function $\\Phi: \\mathbb{R} \\to (0,1)$ is continuous and strictly increasing.\n\nWe define a new random variable $U = \\Phi(Z)$. To determine the distribution of $U$, we derive its CDF, denoted $F_U(u) = P(U \\le u)$.\n\nThe range of the function $\\Phi(\\cdot)$ is the open interval $(0,1)$. Therefore, the random variable $U$ can only take values in $(0,1)$.\n- For any $u \\le 0$, it is impossible for $U \\le u$, so $P(U \\le u) = 0$.\n- For any $u \\ge 1$, it is certain that $U \\le u$, so $P(U \\le u) = 1$.\n\nNow, consider $u \\in (0,1)$. By definition,\n$$ F_U(u) = P(U \\le u) $$\nSubstituting the definition of $U$:\n$$ F_U(u) = P(\\Phi(Z) \\le u) $$\nSince $\\Phi(\\cdot)$ is a strictly increasing function, its inverse, the quantile function $\\Phi^{-1}(\\cdot)$, exists and is also strictly increasing. We can apply $\\Phi^{-1}$ to both sides of the inequality within the probability operator without changing the direction of the inequality:\n$$ F_U(u) = P(\\Phi^{-1}(\\Phi(Z)) \\le \\Phi^{-1}(u)) $$\nBy the property of inverse functions, $\\Phi^{-1}(\\Phi(Z)) = Z$. Thus, we have:\n$$ F_U(u) = P(Z \\le \\Phi^{-1}(u)) $$\nThe expression $P(Z \\le z_0)$ for some value $z_0$ is, by definition, the CDF of $Z$ evaluated at $z_0$. In our case, $z_0 = \\Phi^{-1}(u)$. Therefore,\n$$ F_U(u) = \\Phi(\\Phi^{-1}(u)) $$\nAgain applying the property of inverse functions, $\\Phi(\\Phi^{-1}(u)) = u$. So, for $u \\in (0,1)$,\n$$ F_U(u) = u $$\nCombining these results, the complete CDF of $U$ is:\n$$ F_U(u) = \\begin{cases} 0 & \\text{if } u \\le 0 \\\\ u & \\text{if } 0 < u < 1 \\\\ 1 & \\text{if } u \\ge 1 \\end{cases} $$\nThis is the CDF of the standard uniform distribution, $\\mathrm{Unif}(0,1)$. The derivation confirms from first principles that if $Z \\sim \\mathcal{N}(0,1)$, then $U = \\Phi(Z) \\sim \\mathrm{Unif}(0,1)$. This result holds more generally for any continuous random variable.\n\n### Task 2: Calibration Test Design and Implementation\nThe problem requires designing and implementing a deterministic test for the calibration of a probabilistic forecast. A forecast is calibrated if its PIT values are uniformly distributed on $[0,1]$.\n\n**Deterministic Dataset Construction**:\nTo create a synthetic dataset that is perfectly representative of a standard normal distribution, we avoid random sampling. Instead, we generate $N$ quantiles, $y_i$, corresponding to a uniform grid of probabilities $u_i$.\n1.  Define a uniform grid of $N$ probability values: $u_i = \\frac{i - 1/2}{N}$ for $i \\in \\{1, 2, \\dots, N\\}$.\n2.  Compute the corresponding quantiles from the standard normal distribution: $y_i = \\Phi^{-1}(u_i)$. This set $\\{y_i\\}$ serves as our ground-truth observations.\n\n**Calibration Test Procedure**:\nGiven a predictive model with CDF $F_{\\text{pred}}(y)$, we test its calibration against the observations $\\{y_i\\}$.\n1.  For each observation $y_i$, compute the PIT value using the model's predictive CDF: $u_i^{\\text{pred}} = F_{\\text{pred}}(y_i)$.\n2.  If the model represented by $F_{\\text{pred}}$ is perfectly calibrated (i.e., it matches the true data-generating process, $\\Phi$), then the set of values $\\{u_i^{\\text{pred}}\\}$ will be uniformly distributed. Specifically, if $F_{\\text{pred}}(y) = \\Phi(y)$, then $u_i^\\text{pred} = \\Phi(y_i) = \\Phi(\\Phi^{-1}(u_i)) = u_i$, producing a perfectly uniform grid. Any deviation of $F_{\\text{pred}}$ from $\\Phi$ will distort this uniformity.\n3.  To quantify this distortion, we use a chi-square goodness-of-fit test. The interval $[0,1]$ is divided into $B$ equal-width bins.\n4.  The observed count, $O_k$, is the number of $u_i^{\\text{pred}}$ values that fall into the $k$-th bin, where $k \\in \\{1, 2, \\dots, B\\}$.\n5.  Under the null hypothesis $H_0$ that the $u_i^{\\text{pred}}$ values are uniformly distributed, the expected count for each bin is $E_k = N/B$.\n6.  The chi-square test statistic is calculated as:\n    $$ \\chi^2 = \\sum_{k=1}^{B} \\frac{(O_k - E_k)^2}{E_k} $$\n7.  This statistic follows a chi-square distribution with $df = B-1$ degrees of freedom.\n8.  **Decision Rule**: We compute the p-value, which is the probability of observing a test statistic at least as extreme as the one calculated, assuming $H_0$ is true: $p = P(\\chi^2_{df} \\ge \\chi^2)$. The null hypothesis of calibration is rejected if the p-value is less than the specified significance level $\\alpha$. That is, reject $H_0$ if $p < \\alpha$.\n\n**Implementation Logic for Test Suite**:\nThe procedure is applied to each of the four cases.\n- **Case A (Exact Calibration)**: $\\mu_{\\text{pred}} = 0, \\sigma_{\\text{pred}} = 1$. The predictive CDF is $F_{\\text{pred}}(y) = \\Phi(\\frac{y-0}{1}) = \\Phi(y)$. The PIT values are $u_i^{\\text{pred}} = \\Phi(y_i) = \\Phi(\\Phi^{-1}(u_i)) = u_i$. The values $\\{u_i\\}$ form a perfect grid, so each bin will contain exactly $N/B$ points. Thus, $O_k = E_k$ for all $k$, leading to $\\chi^2 = 0$, $p=1$. Since $1 \\not< 0.01$, $H_0$ is not rejected.\n- **Case B (Systematic Mean Error)**: $\\mu_{\\text{pred}} = 0.75, \\sigma_{\\text{pred}} = 1$. The forecast is systematically biased. The PIT values $u_i^{\\text{pred}} = \\Phi(y_i - 0.75)$ will be shifted, concentrating at lower values and leading to a non-uniform histogram. The $\\chi^2$ statistic will be large, and the p-value is expected to be less than $\\alpha$, leading to rejection.\n- **Case C (Underdispersion)**: $\\mu_{\\text{pred}} = 0, \\sigma_{\\text{pred}} = 0.6$. The forecast is too confident (narrow). PIT values $u_i^{\\text{pred}} = \\Phi(y_i / 0.6)$ will be pushed toward the extremes of $0$ and $1$, creating a U-shaped distribution. The test will detect this non-uniformity, leading to rejection.\n- **Case D (Overdispersion)**: $\\mu_{\\text{pred}} = 0, \\sigma_{\\text{pred}} = 1.6$. The forecast is not confident enough (wide). PIT values $u_i^{\\text{pred}} = \\Phi(y_i / 1.6)$ will be concentrated around the median of $0.5$, creating a bell-shaped distribution. The test will detect this, leading to rejection.\nThe program below implements this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, chi2\n\ndef solve():\n    \"\"\"\n    Implements the deterministic calibration test for a series of parameter settings.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu_pred, sigma_pred, N, B, alpha)\n    test_cases = [\n        (0.0, 1.0, 20000, 20, 0.01),  # Case A: happy path, exact calibration\n        (0.75, 1.0, 20000, 20, 0.01), # Case B: systematic mean error\n        (0.0, 0.6, 20000, 20, 0.01),  # Case C: underdispersion\n        (0.0, 1.6, 20000, 20, 0.01),  # Case D: overdispersion\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_pred, sigma_pred, N, B, alpha = case\n\n        # Step 1: Generate the deterministic synthetic dataset.\n        # This creates a set of points that are perfectly representative of quantiles\n        # from a N(0,1) distribution, avoiding random sampling noise.\n        # The grid points u_i are from a Unif(0,1) distribution by construction.\n        u_i = (np.arange(1, N + 1) - 0.5) / N\n        \n        # The \"true\" observations y_i are the corresponding quantiles from N(0,1).\n        # norm.ppf is the inverse CDF (quantile function) of the normal distribution.\n        # With default loc=0 and scale=1, it is Phi^{-1}.\n        y_i = norm.ppf(u_i)\n\n        # Step 2: Compute PIT values from the predictive model.\n        # The predictive CDF is F_pred(y) = Phi((y - mu_pred) / sigma_pred).\n        # norm.cdf is the CDF of the normal distribution. With default loc=0 and\n        # scale=1, it is Phi.\n        u_i_pred = norm.cdf((y_i - mu_pred) / sigma_pred)\n\n        # Step 3: Perform the chi-square goodness-of-fit test.\n        # Create a histogram of the predictive PIT values u_i_pred.\n        # The range is [0, 1] and there are B equal-width bins.\n        observed_counts, _ = np.histogram(u_i_pred, bins=B, range=(0, 1))\n\n        # Under the null hypothesis of uniformity, the expected count in each bin is N/B.\n        expected_counts = N / B\n\n        # Calculate the chi-square statistic.\n        # Note: If expected_counts is 0, this would lead to division by zero.\n        # However, N and B are positive integers, so expected_counts > 0.\n        chi2_statistic = np.sum((observed_counts - expected_counts)**2 / expected_counts)\n\n        # The degrees of freedom for the chi-square test is B - 1.\n        df = B - 1\n\n        # Calculate the p-value.\n        # The survival function (sf) is 1 - cdf, which gives P(X > x).\n        # This is the probability of observing a chi2 statistic this large or larger.\n        p_value = chi2.sf(chi2_statistic, df)\n\n        # Step 4: Make a decision based on the significance level alpha.\n        # Reject the null hypothesis (that the PITs are uniform) if the p-value\n        # is less than alpha.\n        is_rejected = p_value < alpha\n        results.append(is_rejected)\n\n    # Final print statement in the exact required format.\n    # The boolean values are automatically converted to strings 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3110957"}]}