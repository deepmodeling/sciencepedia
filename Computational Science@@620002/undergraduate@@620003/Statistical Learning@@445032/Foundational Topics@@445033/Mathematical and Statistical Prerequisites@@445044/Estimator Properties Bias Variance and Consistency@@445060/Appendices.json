{"hands_on_practices": [{"introduction": "We begin our hands-on journey with a foundational question: when is the familiar Ordinary Least Squares (OLS) estimator truly the best choice? This practice [@problem_id:3118728] challenges the universal applicability of OLS by placing it in a heteroscedastic environment, where the variance of the errors is not constant. You will discover that while OLS remains unbiased, it loses its status as the most efficient estimator, paving the way for Weighted Least Squares (WLS) and motivating our broader exploration of optimal estimator design.", "problem": "Consider the heteroscedastic simple linear regression model without an intercept,\n$$\ny_{i} = \\beta x_{i} + \\epsilon_{i}, \\quad i=1,\\dots,n,\n$$\nwhere the regressor $x_{i}$ is independent of the error $\\epsilon_{i}$, the conditional mean satisfies $E[\\epsilon_{i}\\,|\\,x_{i}] = 0$, and the conditional variance is heteroscedastic with\n$$\n\\operatorname{Var}(\\epsilon_{i}\\,|\\,x_{i}) = \\sigma^{2} x_{i}^{2},\n$$\nfor a fixed constant $\\sigma^{2} > 0$. Assume $x_{i}$ are independent and identically distributed draws from a standard normal distribution $\\mathcal{N}(0,1)$ and are independent of $\\epsilon_{i}$.\n\nDefine three slope estimators:\n- Ordinary Least Squares (OLS): $\\hat{\\beta}_{\\text{OLS}}$ is the minimizer of $\\sum_{i=1}^{n} (y_{i} - \\beta x_{i})^{2}$, equivalently\n$$\n\\hat{\\beta}_{\\text{OLS}} = \\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{\\sum_{i=1}^{n} x_{i}^{2}}.\n$$\n- Weighted Least Squares (WLS) with correct weights: $\\hat{\\beta}_{\\star}$ is the minimizer of $\\sum_{i=1}^{n} w_{i}^{\\star} (y_{i} - \\beta x_{i})^{2}$ with $w_{i}^{\\star} = 1/(\\sigma^{2} x_{i}^{2})$.\n- WLS with incorrect weights: $\\tilde{\\beta}$ is the minimizer of $\\sum_{i=1}^{n} \\tilde{w}_{i} (y_{i} - \\beta x_{i})^{2}$ with $\\tilde{w}_{i} = 1/|x_{i}|$.\n\nStarting from the definitions of unbiasedness, variance, and consistency, and using only fundamental properties of conditional expectations, variances, and independent sums, do the following:\n1. Derive the exact conditional bias and variance of each estimator given the observed $x_{1},\\dots,x_{n}$.\n2. State whether each estimator is consistent for $\\beta$ as $n \\to \\infty$ under the stated data-generating process and weights.\n3. Finally, compute the large-sample limits as $n \\to \\infty$ of the ratios of their variances to the variance of the correctly weighted WLS estimator $\\hat{\\beta}_{\\star}$:\n$$\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}}\\,|\\,x_{1},\\dots,x_{n})}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}, \\quad\n\\frac{\\operatorname{Var}(\\tilde{\\beta}\\,|\\,x_{1},\\dots,x_{n})}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}, \\quad\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}.\n$$\n\nReport your answer as a single row vector containing the three limits in exact form, with no rounding. No physical units are involved.", "solution": "The problem asks for an analysis of three estimators for the slope parameter $\\beta$ in a heteroscedastic simple linear regression model without an intercept. The model is given by $y_{i} = \\beta x_{i} + \\epsilon_{i}$ for $i=1,\\dots,n$, with $E[\\epsilon_{i}\\,|\\,x_{i}] = 0$ and $\\operatorname{Var}(\\epsilon_{i}\\,|\\,x_{i}) = \\sigma^{2} x_{i}^{2}$. The regressors $x_i$ are i.i.d. draws from a standard normal distribution, $\\mathcal{N}(0,1)$.\n\nLet's first establish a general form for the estimators. The three estimators, Ordinary Least Squares (OLS) and the two variants of Weighted Least Squares (WLS), are all solutions to minimizing a weighted sum of squared residuals, $\\sum_{i=1}^{n} w_{i} (y_{i} - \\beta x_{i})^{2}$. The first-order condition with respect to $\\beta$ is:\n$$\n\\frac{\\partial}{\\partial \\beta} \\sum_{i=1}^{n} w_{i} (y_{i} - \\beta x_{i})^{2} = \\sum_{i=1}^{n} w_{i} (2) (y_{i} - \\beta x_{i}) (-x_{i}) = -2 \\sum_{i=1}^{n} w_{i} x_{i} (y_{i} - \\beta x_{i}) = 0\n$$\nSolving for $\\beta$, we obtain the general formula for a WLS estimator in this model:\n$$\n\\hat{\\beta} = \\frac{\\sum_{i=1}^{n} w_{i} x_{i} y_{i}}{\\sum_{i=1}^{n} w_{i} x_{i}^{2}}\n$$\nThis general form can represent all three estimators with the appropriate choice of weights $w_i$:\n1.  For $\\hat{\\beta}_{\\text{OLS}}$, the weights are uniform, $w_{i}=1$ for all $i$.\n2.  For $\\hat{\\beta}_{\\star}$, the weights are $w_{i}^{\\star} = 1/(\\sigma^{2} x_{i}^{2})$.\n3.  For $\\tilde{\\beta}$, the weights are $\\tilde{w}_{i} = 1/|x_{i}|$.\n\nLet $X$ denote the set of observed regressors $\\{x_1, \\dots, x_n\\}$. We analyze the properties of the estimators conditional on $X$.\n\n**1. Conditional Bias and Variance**\n\nTo find the conditional bias, we express $\\hat{\\beta}$ in terms of the true parameter $\\beta$ and the errors $\\epsilon_i$. Substituting $y_i = \\beta x_i + \\epsilon_i$ into the general formula for $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = \\frac{\\sum w_{i} x_{i} (\\beta x_{i} + \\epsilon_{i})}{\\sum w_{i} x_{i}^{2}} = \\frac{\\beta \\sum w_{i} x_{i}^{2} + \\sum w_{i} x_{i} \\epsilon_{i}}{\\sum w_{i} x_{i}^{2}} = \\beta + \\frac{\\sum_{i=1}^{n} w_{i} x_{i} \\epsilon_{i}}{\\sum_{i=1}^{n} w_{i} x_{i}^{2}}\n$$\nThe conditional expectation of $\\hat{\\beta}$ given $X$ is:\n$$\nE[\\hat{\\beta} \\,|\\, X] = E\\left[\\beta + \\frac{\\sum w_{i} x_{i} \\epsilon_{i}}{\\sum w_{i} x_{i}^{2}} \\bigg| X\\right] = \\beta + \\frac{\\sum w_{i} x_{i} E[\\epsilon_{i} \\,|\\, X]}{\\sum w_{i} x_{i}^{2}}\n$$\nFrom the problem statement, the errors $\\epsilon_i$ are independent of each other conditional on $X$, and $E[\\epsilon_{i}\\,|\\,x_{i}] = 0$. This implies $E[\\epsilon_{i}\\,|\\,X] = E[\\epsilon_{i}\\,|\\,x_i] = 0$. Therefore,\n$$\nE[\\hat{\\beta} \\,|\\, X] = \\beta\n$$\nThe conditional bias, $E[\\hat{\\beta} \\,|\\, X] - \\beta$, is $0$. This holds for any choice of weights $w_i$ that are functions of $x_i$. Thus, all three estimators $\\hat{\\beta}_{\\text{OLS}}$, $\\hat{\\beta}_{\\star}$, and $\\tilde{\\beta}$ are conditionally unbiased.\n\nTo find the conditional variance, we compute:\n$$\n\\operatorname{Var}(\\hat{\\beta} \\,|\\, X) = \\operatorname{Var}\\left(\\beta + \\frac{\\sum w_{i} x_{i} \\epsilon_{i}}{\\sum w_{i} x_{i}^{2}} \\bigg| X\\right) = \\frac{1}{\\left(\\sum w_{i} x_{i}^{2}\\right)^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} w_{i} x_{i} \\epsilon_{i} \\bigg| X\\right)\n$$\nGiven that the $\\epsilon_i$ are conditionally independent, the variance of the sum is the sum of variances:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} w_{i} x_{i} \\epsilon_{i} \\bigg| X\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(w_{i} x_{i} \\epsilon_{i} \\,|\\, X) = \\sum_{i=1}^{n} (w_{i} x_{i})^2 \\operatorname{Var}(\\epsilon_{i} \\,|\\, X)\n$$\nUsing the given heteroscedastic structure $\\operatorname{Var}(\\epsilon_{i}\\,|\\,x_{i}) = \\sigma^{2} x_{i}^{2}$, which is equivalent to $\\operatorname{Var}(\\epsilon_i \\,|\\, X) = \\sigma^2 x_i^2$:\n$$\n\\operatorname{Var}(\\hat{\\beta} \\,|\\, X) = \\frac{1}{\\left(\\sum w_{i} x_{i}^{2}\\right)^2} \\sum_{i=1}^{n} w_{i}^{2} x_{i}^{2} (\\sigma^{2} x_{i}^{2}) = \\sigma^{2} \\frac{\\sum_{i=1}^{n} w_{i}^{2} x_{i}^{4}}{\\left(\\sum_{i=1}^{n} w_{i} x_{i}^{2}\\right)^2}\n$$\nNow we apply this general formula to each estimator.\n\n- **OLS Estimator ($\\hat{\\beta}_{\\text{OLS}}$)**: $w_i = 1$.\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}} \\,|\\, X) = \\sigma^{2} \\frac{\\sum_{i=1}^{n} (1)^2 x_{i}^{4}}{\\left(\\sum_{i=1}^{n} (1) x_{i}^{2}\\right)^2} = \\sigma^{2} \\frac{\\sum_{i=1}^{n} x_{i}^{4}}{\\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^2}\n$$\n\n- **Correctly Weighted WLS Estimator ($\\hat{\\beta}_{\\star}$)**: $w_{i}^{\\star} = 1/(\\sigma^{2} x_{i}^{2})$.\nThe numerator term is $\\sum (w_{i}^{\\star})^2 x_{i}^{4} = \\sum \\left(\\frac{1}{\\sigma^2 x_i^2}\\right)^2 x_i^4 = \\sum \\frac{1}{\\sigma^4 x_i^4} x_i^4 = \\sum \\frac{1}{\\sigma^4} = \\frac{n}{\\sigma^4}$.\nThe denominator term is $\\sum w_{i}^{\\star} x_{i}^{2} = \\sum \\frac{1}{\\sigma^2 x_i^2} x_i^2 = \\sum \\frac{1}{\\sigma^2} = \\frac{n}{\\sigma^2}$.\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\star} \\,|\\, X) = \\sigma^{2} \\frac{n/\\sigma^4}{\\left(n/\\sigma^2\\right)^2} = \\sigma^{2} \\frac{n/\\sigma^4}{n^2/\\sigma^4} = \\frac{\\sigma^2}{n}\n$$\n\n- **Incorrectly Weighted WLS Estimator ($\\tilde{\\beta}$)**: $\\tilde{w}_i = 1/|x_i|$.\nThe numerator term is $\\sum (\\tilde{w}_{i})^2 x_{i}^{4} = \\sum \\left(\\frac{1}{|x_i|}\\right)^2 x_i^4 = \\sum \\frac{x_i^4}{x_i^2} = \\sum x_i^2$.\nThe denominator term is $\\sum \\tilde{w}_{i} x_{i}^{2} = \\sum \\frac{1}{|x_i|} x_i^2 = \\sum |x_i|$.\n$$\n\\operatorname{Var}(\\tilde{\\beta} \\,|\\, X) = \\sigma^{2} \\frac{\\sum_{i=1}^{n} x_{i}^{2}}{\\left(\\sum_{i=1}^{n} |x_{i}|\\right)^2}\n$$\n\n**2. Consistency**\n\nAn estimator $\\hat{\\theta}_n$ is consistent for $\\theta$ if it converges in probability to $\\theta$ as $n \\to \\infty$. A sufficient condition for a sequence of unbiased estimators to be consistent is that their variance converges to zero. Since all three estimators are unbiased, we check the limits of their variances. The variances are conditional on $X$ and are thus random variables. We check if they converge in probability to $0$. This involves applying the Law of Large Numbers (LLN) to the sample averages of functions of $x_i \\sim \\mathcal{N}(0,1)$. We need the following expectations:\n- $E[x_i^2] = 1$ (since $x_i$ has variance $1$ and mean $0$).\n- $E[x_i^4] = 3$ (the fourth moment of a standard normal distribution).\n- $E[|x_i|] = \\int_{-\\infty}^{\\infty} |z| \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz = 2 \\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz = \\sqrt{\\frac{2}{\\pi}}$.\n\nBy the LLN, as $n \\to \\infty$:\n$\\frac{1}{n} \\sum x_i^2 \\xrightarrow{p} 1$, $\\frac{1}{n} \\sum x_i^4 \\xrightarrow{p} 3$, and $\\frac{1}{n} \\sum |x_i| \\xrightarrow{p} \\sqrt{2/\\pi}$.\n\nFor $\\hat{\\beta}_{\\text{OLS}}$, $\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}} \\,|\\, X) = \\frac{\\sigma^2}{n} \\frac{\\frac{1}{n}\\sum x_i^4}{(\\frac{1}{n}\\sum x_i^2)^2} \\xrightarrow{p} \\frac{\\sigma^2}{n} \\frac{3}{1^2} = \\frac{3\\sigma^2}{n} \\to 0$.\nFor $\\hat{\\beta}_{\\star}$, $\\operatorname{Var}(\\hat{\\beta}_{\\star} \\,|\\, X) = \\frac{\\sigma^2}{n} \\to 0$.\nFor $\\tilde{\\beta}$, $\\operatorname{Var}(\\tilde{\\beta} \\,|\\, X) = \\frac{\\sigma^2}{n} \\frac{\\frac{1}{n}\\sum x_i^2}{(\\frac{1}{n}\\sum |x_i|)^2} \\xrightarrow{p} \\frac{\\sigma^2}{n} \\frac{1}{(\\sqrt{2/\\pi})^2} = \\frac{\\pi\\sigma^2}{2n} \\to 0$.\n\nSince the conditional variances of all three estimators converge to $0$ as $n \\to \\infty$, all three estimators ($\\hat{\\beta}_{\\text{OLS}}$, $\\hat{\\beta}_{\\star}$, $\\tilde{\\beta}$) are consistent for $\\beta$.\n\n**3. Large-Sample Variance Ratios**\n\nWe compute the limits of the ratios of the conditional variances as $n \\to \\infty$. These limits are interpreted as limits in probability.\n\n- **Ratio 1**:\n$$\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}}\\,|\\,X)}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)} = \\frac{\\sigma^{2} \\frac{\\sum x_{i}^{4}}{(\\sum x_{i}^{2})^2}}{\\sigma^2/n} = \\frac{n \\sum x_{i}^{4}}{(\\sum x_{i}^{2})^2} = \\frac{n(n \\cdot \\frac{1}{n}\\sum x_i^4)}{(n \\cdot \\frac{1}{n}\\sum x_i^2)^2} = \\frac{\\frac{1}{n}\\sum x_i^4}{(\\frac{1}{n}\\sum x_i^2)^2}\n$$\nTaking the limit as $n \\to \\infty$ and applying the LLN and the Continuous Mapping Theorem:\n$$\n\\lim_{n \\to \\infty} \\frac{\\frac{1}{n}\\sum x_i^4}{(\\frac{1}{n}\\sum x_i^2)^2} \\xrightarrow{p} \\frac{E[x_i^4]}{(E[x_i^2])^2} = \\frac{3}{1^2} = 3\n$$\n\n- **Ratio 2**:\n$$\n\\frac{\\operatorname{Var}(\\tilde{\\beta}\\,|\\,X)}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)} = \\frac{\\sigma^{2} \\frac{\\sum x_{i}^{2}}{(\\sum |x_{i}|)^2}}{\\sigma^2/n} = \\frac{n \\sum x_{i}^{2}}{(\\sum |x_{i}|)^2} = \\frac{n(n \\cdot \\frac{1}{n}\\sum x_i^2)}{(n \\cdot \\frac{1}{n}\\sum |x_i|)^2} = \\frac{\\frac{1}{n}\\sum x_i^2}{(\\frac{1}{n}\\sum |x_i|)^2}\n$$\nTaking the limit as $n \\to \\infty$:\n$$\n\\lim_{n \\to \\infty} \\frac{\\frac{1}{n}\\sum x_i^2}{(\\frac{1}{n}\\sum |x_i|)^2} \\xrightarrow{p} \\frac{E[x_i^2]}{(E[|x_i|])^2} = \\frac{1}{(\\sqrt{2/\\pi})^2} = \\frac{1}{2/\\pi} = \\frac{\\pi}{2}\n$$\n\n- **Ratio 3**:\n$$\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)} = 1\n$$\nThe limit of a constant is the constant itself, so the limit is $1$.\n\nThe three limits, in the order requested, are $3$, $\\pi/2$, and $1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 3 & \\frac{\\pi}{2} & 1 \\end{pmatrix}}\n$$", "id": "3118728"}, {"introduction": "Having seen that OLS can be inefficient, we now explore another of its vulnerabilities: high variance in the presence of collinear predictors. This exercise [@problem_id:3118692] introduces Ridge Regression, a classic technique that tackles this issue by deliberately introducing a small amount of bias to achieve a dramatic reduction in variance. By constructing a specific scenario and analyzing the estimator properties, you will gain a concrete understanding of the famous bias-variance tradeoff, a cornerstone of modern statistical learning and model regularization.", "problem": "Consider a fixed-design linear regression model with two predictors, where the response vector is generated according to $y = X \\beta + \\varepsilon$. Here $X \\in \\mathbb{R}^{n \\times 2}$ is a nonrandom design matrix, $\\beta \\in \\mathbb{R}^{2}$ is the true coefficient vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a noise vector with independent components satisfying $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Define the Ordinary Least Squares (OLS) estimator and the ridge estimator by\n$$\\hat{\\beta}_{\\mathrm{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y,$$\n$$\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda) = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} y,$$\nwhere $\\lambda \\ge 0$ is the ridge penalty parameter and $I_{2}$ is the $2 \\times 2$ identity matrix. Use the following fundamental definitions and facts:\n- The bias of an estimator $\\hat{\\theta}$ of a parameter $\\theta$ is $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$.\n- The variance of a linear estimator $A y$ under $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is $\\mathrm{Var}(A y) = \\sigma^{2} A A^{\\top}$.\n- For the OLS estimator, $\\mathbb{E}[\\hat{\\beta}_{\\mathrm{OLS}}] = \\beta$ and $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}) = \\sigma^{2} (X^{\\top} X)^{-1}$.\n- For the ridge estimator, $\\mathbb{E}[\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)] = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta$ and $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\sigma^{2} (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1}$.\n\nYour task is to construct a deterministic family of collinear designs $X$ that isolates the impact of collinearity on estimator variance, and then quantify bias and variance of OLS versus ridge. Specifically:\n\n1. Construct $X$ so that the Gram matrix $G = X^{\\top} X$ has eigen-decomposition $G = V \\mathrm{diag}(s_{1}, s_{2}) V^{\\top}$ with orthonormal eigenvectors $v_{1} = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$ and $v_{2} = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$, and eigenvalues $s_{1} = n(1+\\rho)$ and $s_{2} = n(1-\\rho)$ for a prescribed correlation level $\\rho \\in [0, 1)$. This ensures collinearity when $\\rho$ is close to $1$. Use $n$ even and construct $X$ via orthonormal columns in $\\mathbb{R}^{n}$ so that $X^{\\top} X$ matches the specified eigenstructure exactly. Take the true coefficient vector $\\beta = (1, 1)^{\\top}$ and noise variance $\\sigma^{2} = 1$.\n\n2. For a given $(n, \\rho, \\lambda)$ triple, compute:\n   - The squared Euclidean norm of the ridge bias vector, $\\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2}$, where\n     $$\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\left((X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2}\\right)\\beta.$$\n   - The ratio of the trace of the ridge variance to the trace of the OLS variance:\n     $$R_{\\mathrm{var}}(X, \\lambda) = \\frac{\\mathrm{tr}\\left(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\right)}{\\mathrm{tr}\\left(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}})\\right)} = \\frac{\\mathrm{tr}\\left(\\sigma^{2} (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1}\\right)}{\\mathrm{tr}\\left(\\sigma^{2} (X^{\\top} X)^{-1}\\right)}.$$\n\n3. Design and evaluate the following test suite of $(n, \\rho, \\lambda)$ to probe different regimes:\n   - Case A (high collinearity, variance reduction with near-zero bias): $n = 1000$, $\\rho = 0.999$, $\\lambda = 1$.\n   - Case B (boundary case, no regularization): $n = 1000$, $\\rho = 0.999$, $\\lambda = 0$.\n   - Case C (moderate collinearity): $n = 500$, $\\rho = 0.9$, $\\lambda = 1$.\n   - Case D (low collinearity): $n = 500$, $\\rho = 0.0$, $\\lambda = 1$.\n\n4. Your program must be a single, complete script that:\n   - Constructs $X$ exactly as specified in item 1 for each test case.\n   - Computes the quantities in item 2 for each test case.\n   - Produces a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\|\\mathrm{Bias}\\|^{2}_{\\mathrm{A}}, R_{\\mathrm{var},\\mathrm{A}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{B}}, R_{\\mathrm{var},\\mathrm{B}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{C}}, R_{\\mathrm{var},\\mathrm{C}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{D}}, R_{\\mathrm{var},\\mathrm{D}}]$.\n\nAll quantities are pure numbers with no physical units. Angles are not involved. Ensure all matrix computations are carried out using the specified formulas. The output must be reproducible and must not depend on any external randomness beyond the deterministic construction of $X$ described above.", "solution": "The problem is well-posed, scientifically grounded, and internally consistent. It provides a complete set of definitions, parameters, and constraints to derive a unique, deterministic solution. All quantities to be computed can be expressed as functions of the Gram matrix $G = X^{\\top} X$, for which a full eigendecomposition is provided. The instruction to \"construct $X$\" is a conceptual device to ensure such a matrix exists, which is guaranteed by the singular value decomposition theorem. However, since the final quantities depend only on $G=X^\\top X$, we can proceed by using its specified properties directly, bypassing the explicit construction of the $n \\times 2$ matrix $X$.\n\nOur procedure is as follows:\n1.  Derive an analytical formula for the squared Euclidean norm of the ridge bias vector, $\\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2}$, using the given eigendecomposition of $G$.\n2.  Derive an analytical formula for the variance ratio, $R_{\\mathrm{var}}(X, \\lambda)$, using the same eigendecomposition.\n3.  Apply these formulas to the four specified test cases.\n\nThe Gram matrix is defined as $G = X^{\\top} X = V \\Lambda V^{\\top}$, where $\\Lambda = \\mathrm{diag}(s_{1}, s_{2})$ is the diagonal matrix of eigenvalues and $V = [v_1, v_2]$ is the orthogonal matrix of corresponding eigenvectors.\nThe eigenvalues are given by $s_{1} = n(1+\\rho)$ and $s_{2} = n(1-\\rho)$.\nThe eigenvectors are $v_{1} = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$ and $v_{2} = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$, forming the matrix $V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$.\nThe true coefficient vector is $\\beta = (1, 1)^{\\top}$ and the noise variance is $\\sigma^2 = 1$.\n\n**1. Derivation of the Squared Bias Norm**\n\nThe bias of the ridge estimator is given by:\n$$ \\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\mathbb{E}[\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)] - \\beta = \\left( (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2} \\right) \\beta $$\nSubstituting $G = X^{\\top} X = V \\Lambda V^{\\top}$ and $I_{2} = V V^{\\top}$:\n$$ \\mathrm{Bias}(\\cdot) = \\left( (V \\Lambda V^{\\top} + \\lambda V V^{\\top})^{-1} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n$$ = \\left( (V (\\Lambda + \\lambda I_{2}) V^{\\top})^{-1} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n$$ = \\left( V (\\Lambda + \\lambda I_{2})^{-1} V^{\\top} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\nUsing $V^{\\top}V = I_{2}$:\n$$ = \\left( V (\\Lambda + \\lambda I_{2})^{-1} \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta = V \\left( (\\Lambda + \\lambda I_{2})^{-1} \\Lambda - I_{2} \\right) V^{\\top} \\beta $$\nThe central matrix term simplifies to:\n$$ (\\Lambda + \\lambda I_{2})^{-1} \\Lambda - I_{2} = \\begin{pmatrix} \\frac{s_1}{s_1+\\lambda} & 0 \\\\ 0 & \\frac{s_2}{s_2+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\lambda}{s_1+\\lambda} & 0 \\\\ 0 & -\\frac{\\lambda}{s_2+\\lambda} \\end{pmatrix} = -\\lambda(\\Lambda+\\lambda I_{2})^{-1} $$\nSo, the bias vector is $\\mathrm{Bias}(\\cdot) = -\\lambda V (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta$.\nTo find its squared norm, $\\|\\mathrm{Bias}(\\cdot)\\|_{2}^{2}$, we first transform $\\beta$ into the eigenvector basis:\n$$ V^{\\top}\\beta = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} $$\nThis shows that $\\beta$ is aligned with the first eigenvector $v_1$, as $\\beta = \\sqrt{2}v_1$.\nSince $V$ is an orthogonal matrix, it preserves the Euclidean norm. Thus, we have:\n$$ \\|\\mathrm{Bias}(\\cdot)\\|_{2}^{2} = \\|-\\lambda V (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta\\|_{2}^{2} = \\|-\\lambda (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta\\|_{2}^{2} $$\n$$ = \\left\\| -\\lambda \\begin{pmatrix} \\frac{1}{s_1+\\lambda} & 0 \\\\ 0 & \\frac{1}{s_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} -\\frac{\\lambda\\sqrt{2}}{s_1+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\left(-\\frac{\\lambda\\sqrt{2}}{s_1+\\lambda}\\right)^2 $$\nThis yields the final formula for the squared bias norm:\n$$ \\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2} = \\frac{2\\lambda^2}{(s_1+\\lambda)^2} $$\n\n**2. Derivation of the Variance Ratio**\n\nThe variance ratio is $R_{\\mathrm{var}} = \\frac{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}))}{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))}$. Given $\\sigma^2=1$, we analyze the numerator and denominator separately.\n\nDenominator, OLS variance trace:\n$$ \\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}})) = \\mathrm{tr}(\\sigma^2 (X^{\\top}X)^{-1}) = \\mathrm{tr}(G^{-1}) = \\mathrm{tr}((V\\Lambda V^\\top)^{-1}) = \\mathrm{tr}(V\\Lambda^{-1}V^\\top) $$\nUsing the cyclic property of the trace, $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$:\n$$ \\mathrm{tr}(V\\Lambda^{-1}V^\\top) = \\mathrm{tr}(V^\\top V\\Lambda^{-1}) = \\mathrm{tr}(\\Lambda^{-1}) = \\frac{1}{s_1} + \\frac{1}{s_2} $$\n\nNumerator, ridge variance trace:\n$$ \\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}})) = \\mathrm{tr}(\\sigma^2 (G+\\lambda I)^{-1} G (G+\\lambda I)^{-1}) $$\n$$ = \\mathrm{tr}( (V(\\Lambda+\\lambda I)V^\\top)^{-1} (V\\Lambda V^\\top) (V(\\Lambda+\\lambda I)V^\\top)^{-1} ) $$\n$$ = \\mathrm{tr}( V(\\Lambda+\\lambda I)^{-1} V^{\\top} V\\Lambda V^{\\top} V(\\Lambda+\\lambda I)^{-1} V^{\\top} ) $$\n$$ = \\mathrm{tr}( V (\\Lambda+\\lambda I)^{-1} \\Lambda (\\Lambda+\\lambda I)^{-1} V^{\\top} ) $$\nAgain, using the cyclic property of trace:\n$$ = \\mathrm{tr}( (\\Lambda+\\lambda I)^{-1} \\Lambda (\\Lambda+\\lambda I)^{-1} ) = \\mathrm{tr}\\left( \\begin{pmatrix} \\frac{s_1}{(s_1+\\lambda)^2} & 0 \\\\ 0 & \\frac{s_2}{(s_2+\\lambda)^2} \\end{pmatrix} \\right) = \\frac{s_1}{(s_1+\\lambda)^2} + \\frac{s_2}{(s_2+\\lambda)^2} $$\n\nCombining the numerator and denominator gives the expression for the variance ratio:\n$$ R_{\\mathrm{var}}(X, \\lambda) = \\frac{\\frac{s_1}{(s_1+\\lambda)^2} + \\frac{s_2}{(s_2+\\lambda)^2}}{\\frac{1}{s_1} + \\frac{1}{s_2}} $$\n\n**3. Computations for Test Cases**\n\nWe now apply these formulas to the given test cases, substituting $s_1 = n(1+\\rho)$ and $s_2 = n(1-\\rho)$.\n\n**Case A**: $(n, \\rho, \\lambda) = (1000, 0.999, 1)$.\n$s_1 = 1000(1.999) = 1999$, $s_2 = 1000(0.001) = 1$.\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(1999+1)^2} = \\frac{2}{2000^2} = 5 \\times 10^{-7}$.\n$R_{\\mathrm{var}} = \\frac{1999/(1999+1)^2 + 1/(1+1)^2}{1/1999 + 1/1} = \\frac{1999/4000000 + 1/4}{2000/1999} \\approx 0.25037$.\n\n**Case B**: $(n, \\rho, \\lambda) = (1000, 0.999, 0)$.\nThis corresponds to the OLS estimator.\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(0^2)}{(1999+0)^2} = 0$, as OLS is unbiased.\n$R_{\\mathrm{var}} = \\frac{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))}{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))} = 1$.\n\n**Case C**: $(n, \\rho, \\lambda) = (500, 0.9, 1)$.\n$s_1 = 500(1.9) = 950$, $s_2 = 500(0.1) = 50$.\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(950+1)^2} = \\frac{2}{951^2} \\approx 2.211 \\times 10^{-6}$.\n$R_{\\mathrm{var}} = \\frac{950/(950+1)^2 + 50/(50+1)^2}{1/950 + 1/50} \\approx 0.9630$.\n\n**Case D**: $(n, \\rho, \\lambda) = (500, 0.0, 1)$.\n$s_1 = 500(1) = 500$, $s_2 = 500(1) = 500$.\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(500+1)^2} = \\frac{2}{501^2} \\approx 7.968 \\times 10^{-6}$.\n$R_{\\mathrm{var}} = \\frac{500/(500+1)^2 + 500/(500+1)^2}{1/500 + 1/500} = \\frac{2 \\cdot 500/501^2}{2/500} = \\frac{500^2}{501^2} \\approx 0.9960$.\n\nThe implementation will encode these derived analytical formulas.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the squared bias norm and variance ratio for OLS vs. Ridge estimators\n    under different scenarios of collinearity.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, rho, lambda), description\n        (1000, 0.999, 1), # Case A: high collinearity, variance reduction\n        (1000, 0.999, 0), # Case B: boundary case, no regularization (OLS)\n        (500, 0.9, 1),    # Case C: moderate collinearity\n        (500, 0.0, 1),    # Case D: low collinearity (orthogonal design)\n    ]\n\n    results = []\n    for n, rho, lmbda in test_cases:\n        # The problem is structured such that all calculations depend only on the\n        # eigenvalues of the Gram matrix G = X'X, not on X itself.\n        # We calculate these eigenvalues directly.\n        s1 = float(n) * (1.0 + rho)\n        s2 = float(n) * (1.0 - rho)\n\n        # 1. Compute the squared Euclidean norm of the ridge bias vector.\n        # The analytical formula derived in the solution is:\n        # ||Bias||^2 = 2 * lambda^2 / (s1 + lambda)^2\n        # This simplification arises because the true beta vector is aligned with\n        # the first principal component of the design.\n        sq_bias_norm = 2.0 * lmbda**2 / (s1 + lmbda)**2\n\n        # 2. Compute the ratio of the trace of the ridge variance to the trace of OLS variance.\n        # The analytical formula derived in the solution is:\n        # R_var = (s1/(s1+l)^2 + s2/(s2+l)^2) / (1/s1 + 1/s2)\n        if lmbda == 0:\n            # For OLS (lambda=0), the ridge estimator is the OLS estimator,\n            # so the ratio of their variance traces is exactly 1.\n            r_var = 1.0\n        else:\n            # Denominator: Trace of OLS variance (scaled by sigma^2 = 1)\n            # The problem constraint rho in [0, 1) ensures s1 > 0 and s2 > 0.\n            # If rho is close to 1, s2 is close to 0, which is the source of high variance.\n            ols_var_trace = (1.0 / s1) + (1.0 / s2)\n\n            # Numerator: Trace of Ridge variance (scaled by sigma^2 = 1)\n            ridge_var_trace = s1 / (s1 + lmbda)**2 + s2 / (s2 + lmbda)**2\n\n            r_var = ridge_var_trace / ols_var_trace\n                \n        results.append(sq_bias_norm)\n        results.append(r_var)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3118692"}, {"introduction": "Our final practice extends the bias-variance tradeoff to the powerful framework of hierarchical models, a staple in fields from social sciences to bioinformatics. Consider the task of estimating means for multiple groups, where some groups may have sparse data; this exercise [@problem_id:3118672] contrasts a simple \"no-pooling\" approach with a more sophisticated \"partial-pooling\" or shrinkage estimator derived from Bayesian principles. You will see how borrowing strength across groups introduces a beneficial bias that shrinks estimates towards a common mean, reduces variance, and ultimately improves overall prediction accuracy as measured by Mean Squared Error.", "problem": "A researcher is modeling grouped data using a hierarchical normal-normal model. For group $g$, they observe $n_{g}$ samples $y_{g1}, y_{g2}, \\dots, y_{g n_{g}}$ that follow the data-generating process $y_{gi} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\sigma^{2})$ and the group-specific parameter follows the prior $\\theta_{g} \\sim \\mathcal{N}(\\mu, \\tau^{2})$, where $\\mu$, $\\sigma^{2}$, and $\\tau^{2}$ are known constants with $\\sigma^{2} > 0$ and $\\tau^{2} > 0$. Assume the samples are independent and identically distributed (i.i.d.) within each group conditional on $\\theta_{g}$.\n\nConsider two estimators for the group mean $\\theta_{g}$:\n- No pooling: $\\hat{\\theta}_{g}^{\\mathrm{NP}} = \\bar{y}_{g}$, where $\\bar{y}_{g} = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi}$.\n- Partial pooling: the estimator that minimizes posterior expected squared loss under the hierarchical prior and Gaussian likelihood.\n\nUsing the core definitions of bias, variance, and consistency of estimators, and starting from the Gaussian likelihood and prior, perform the following:\n- Derive the partial pooling estimator in closed form from Bayes’ rule and compute its conditional bias $E(\\hat{\\theta}_{g} \\mid \\theta_{g}) - \\theta_{g}$ and conditional variance $\\mathrm{Var}(\\hat{\\theta}_{g} \\mid \\theta_{g})$.\n- Compute the conditional bias and variance of the no-pooling estimator.\n- Explain, in terms of the estimator formulas you derive, how partial pooling reduces variance relative to no pooling, and analyze consistency (convergence in probability) of both estimators as $n_{g} \\to \\infty$.\n\nFinally, define the conditional mean squared error (MSE) for an estimator $\\hat{\\theta}_{g}$ as $\\mathrm{MSE}(\\hat{\\theta}_{g} \\mid \\theta_{g}) = E\\!\\left[(\\hat{\\theta}_{g} - \\theta_{g})^{2} \\mid \\theta_{g}\\right]$. Derive, simplify, and present as a single closed-form analytic expression the difference\n$$\\Delta = \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) - \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$$\nexpressed entirely in terms of $n_{g}$, $\\sigma^{2}$, $\\tau^{2}$, $\\mu$, and $\\theta_{g}$. Your final answer must be this exact symbolic expression for $\\Delta$.", "solution": "The problem statement is critically validated and found to be valid. It is a well-posed, scientifically grounded problem in Bayesian statistics, free of inconsistencies, ambiguities, or factual errors. All necessary information is provided to derive the requested quantities.\n\nWe begin by analyzing the two estimators for the group mean $\\theta_{g}$. All expectations $E[\\cdot]$ and variances $\\mathrm{Var}(\\cdot)$ are conditional on the true value of $\\theta_{g}$ unless stated otherwise.\n\n**No-Pooling Estimator: $\\hat{\\theta}_{g}^{\\mathrm{NP}}$**\n\nThe no-pooling estimator is defined as the sample mean for group $g$: $\\hat{\\theta}_{g}^{\\mathrm{NP}} = \\bar{y}_{g} = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi}$. The data $y_{gi}$ are drawn from $y_{gi} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\sigma^{2})$. Since the samples are independent and identically distributed (i.i.d.) conditional on $\\theta_{g}$, the sampling distribution of the sample mean $\\bar{y}_{g}$ is also normal.\n\nThe conditional expectation of $\\bar{y}_{g}$ is:\n$$E[\\bar{y}_{g} \\mid \\theta_{g}] = E\\left[\\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi} \\mid \\theta_{g}\\right] = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} E[y_{gi} \\mid \\theta_{g}] = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} \\theta_{g} = \\theta_{g}$$\nThe conditional variance of $\\bar{y}_{g}$ is:\n$$\\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = \\mathrm{Var}\\left(\\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi} \\mid \\theta_{g}\\right) = \\frac{1}{n_{g}^{2}} \\sum_{i=1}^{n_{g}} \\mathrm{Var}(y_{gi} \\mid \\theta_{g}) = \\frac{1}{n_{g}^{2}} (n_{g}\\sigma^{2}) = \\frac{\\sigma^{2}}{n_{g}}$$\nTherefore, the sampling distribution is $\\bar{y}_{g} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\frac{\\sigma^{2}}{n_{g}})$.\n\nThe conditional bias of $\\hat{\\theta}_{g}^{\\mathrm{NP}}$ is:\n$$E[\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}] - \\theta_{g} = E[\\bar{y}_{g} \\mid \\theta_{g}] - \\theta_{g} = \\theta_{g} - \\theta_{g} = 0$$\nThe no-pooling estimator is conditionally unbiased.\n\nThe conditional variance of $\\hat{\\theta}_{g}^{\\mathrm{NP}}$ is:\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = \\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{n_{g}}$$\n\n**Partial-Pooling Estimator: $\\hat{\\theta}_{g}^{\\mathrm{PP}}$**\n\nThe partial-pooling estimator, which minimizes posterior expected squared loss, is the posterior mean $E[\\theta_{g} \\mid y_{g1}, \\dots, y_{gn_{g}}]$. We find this by applying Bayes' rule: $p(\\theta_{g} \\mid \\mathbf{y}_{g}) \\propto p(\\mathbf{y}_{g} \\mid \\theta_{g}) p(\\theta_{g})$. The sample mean $\\bar{y}_{g}$ is a sufficient statistic for $\\theta_{g}$.\nThe likelihood for $\\bar{y}_{g}$ is $p(\\bar{y}_{g} \\mid \\theta_{g}) \\sim \\mathcal{N}(\\theta_{g}, \\frac{\\sigma^{2}}{n_{g}})$.\nThe prior for $\\theta_{g}$ is $p(\\theta_{g}) \\sim \\mathcal{N}(\\mu, \\tau^{2})$.\n\nThe posterior distribution is proportional to the product of these two Gaussian densities:\n$$p(\\theta_{g} \\mid \\bar{y}_{g}) \\propto \\exp\\left(-\\frac{(\\bar{y}_{g} - \\theta_{g})^{2}}{2\\sigma^{2}/n_{g}}\\right) \\exp\\left(-\\frac{(\\theta_{g} - \\mu)^{2}}{2\\tau^{2}}\\right)$$\nThis is a conjugate model, so the posterior for $\\theta_{g}$ is also a normal distribution, $p(\\theta_{g} \\mid \\bar{y}_{g}) \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$. The posterior precision is the sum of the prior and data precisions:\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\tau^{2}} + \\frac{1}{\\sigma^{2}/n_{g}} = \\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}$$\nThe posterior mean $\\mu_{\\text{post}}$ is a precision-weighted average of the prior mean and the data mean:\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left(\\frac{\\mu}{\\tau^{2}} + \\frac{\\bar{y}_{g}}{\\sigma^{2}/n_{g}}\\right) = \\left(\\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}\\right)^{-1} \\left(\\frac{\\mu}{\\tau^{2}} + \\frac{n_{g}\\bar{y}_{g}}{\\sigma^{2}}\\right)$$\nThe estimator is $\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\mu_{\\text{post}}$. Simplifying the expression:\n$$\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\frac{\\frac{\\mu}{\\tau^{2}} + \\frac{n_{g}\\bar{y}_{g}}{\\sigma^{2}}}{\\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}} = \\frac{\\sigma^{2}\\mu + n_{g}\\tau^{2}\\bar{y}_{g}}{\\sigma^{2} + n_{g}\\tau^{2}}$$\nThis can be expressed as a weighted average:\n$$\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\bar{y}_{g} + \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\mu = w\\bar{y}_{g} + (1-w)\\mu$$\nwhere the weight is $w = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$.\n\nThe conditional bias of $\\hat{\\theta}_{g}^{\\mathrm{PP}}$ is calculated by taking the expectation conditional on $\\theta_{g}$:\n$$E[\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}] = E[w\\bar{y}_{g} + (1-w)\\mu \\mid \\theta_{g}] = w E[\\bar{y}_{g} \\mid \\theta_{g}] + (1-w)\\mu = w\\theta_{g} + (1-w)\\mu$$\nBias $= E[\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}] - \\theta_{g} = w\\theta_{g} + (1-w)\\mu - \\theta_{g} = (w-1)\\theta_{g} + (1-w)\\mu = (1-w)(\\mu - \\theta_{g})$.\nSubstituting $1-w = \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$, the bias is:\n$$\\text{Bias}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g})$$\nThis estimator is biased towards the prior mean $\\mu$, an effect known as shrinkage.\n\nThe conditional variance of $\\hat{\\theta}_{g}^{\\mathrm{PP}}$ is:\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\mathrm{Var}(w\\bar{y}_{g} + (1-w)\\mu \\mid \\theta_{g}) = w^{2}\\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = w^{2} \\frac{\\sigma^{2}}{n_{g}}$$\nSubstituting the expression for $w$:\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n\n**Comparison and Consistency Analysis**\n\nVariance Reduction: The variance of the partial pooling estimator is $\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = w^{2} \\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$. Since $\\sigma^{2} > 0$ and $\\tau^{2} > 0$, the weight $w = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$ satisfies $0 < w < 1$. Thus, $w^{2} < 1$, which proves that the partial pooling estimator has a strictly smaller conditional variance than the no-pooling estimator. This reduction in variance is achieved by \"shrinking\" the estimate towards the prior mean $\\mu$, making it less susceptible to the sampling noise in the data $\\bar{y}_{g}$. This is an example of the bias-variance tradeoff; we introduce bias to reduce variance.\n\nConsistency: An estimator is consistent if it converges in probability to the true parameter as the sample size grows. A sufficient condition is that both its bias and variance converge to zero.\n\nFor $\\hat{\\theta}_{g}^{\\mathrm{NP}}$, as $n_{g} \\to \\infty$:\n- Bias is always $0$.\n- $\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{n_{g}} \\to 0$.\nThus, $\\hat{\\theta}_{g}^{\\mathrm{NP}}$ is a consistent estimator of $\\theta_{g}$.\n\nFor $\\hat{\\theta}_{g}^{\\mathrm{PP}}$, as $n_{g} \\to \\infty$:\n- Bias: $\\lim_{n_{g}\\to\\infty} \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g}) = 0$.\n- Variance: $\\lim_{n_{g}\\to\\infty} \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}} = \\lim_{n_{g}\\to\\infty} \\left(\\frac{\\tau^{2}}{\\sigma^{2}/n_{g} + \\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}} = \\left(\\frac{\\tau^{2}}{\\tau^{2}}\\right)^{2} \\cdot 0 = 0$.\nSince both bias and variance converge to $0$, $\\hat{\\theta}_{g}^{\\mathrm{PP}}$ is also a consistent estimator of $\\theta_{g}$. As $n_{g}$ increases, the weight $w$ approaches $1$, meaning $\\hat{\\theta}_{g}^{\\mathrm{PP}}$ converges to $\\hat{\\theta}_{g}^{\\mathrm{NP}}$. The data overwhelm the prior.\n\n**Mean Squared Error (MSE) Difference**\n\nThe conditional MSE is defined as $\\mathrm{MSE}(\\hat{\\theta}_{g} \\mid \\theta_{g}) = (E[\\hat{\\theta}_{g} \\mid \\theta_{g}] - \\theta_{g})^{2} + \\mathrm{Var}(\\hat{\\theta}_{g} \\mid \\theta_{g})$.\n\nFor the no-pooling estimator:\n$$\\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = (0)^{2} + \\frac{\\sigma^{2}}{n_{g}} = \\frac{\\sigma^{2}}{n_{g}}$$\n\nFor the partial-pooling estimator:\n$$\\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\left(\\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g})\\right)^{2} + \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n$$= \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}} + \\frac{n_{g}^{2}\\tau^{4}\\sigma^{2}}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} = \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}}$$\n\nWe now compute the difference $\\Delta = \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) - \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$:\n$$\\Delta = \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}} - \\frac{\\sigma^{2}}{n_{g}}$$\nUsing a common denominator of $n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}$:\n$$ \\Delta = \\frac{n_{g}(\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}) - \\sigma^{2}(n_{g}\\tau^{2} + \\sigma^{2})^{2}}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} $$\nExpanding the numerator:\n$$ \\text{Numerator} = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}^{2}\\tau^{4}\\sigma^{2} - \\sigma^{2}(n_{g}^{2}\\tau^{4} + 2n_{g}\\tau^{2}\\sigma^{2} + \\sigma^{4}) $$\n$$ = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}^{2}\\tau^{4}\\sigma^{2} - n_{g}^{2}\\tau^{4}\\sigma^{2} - 2n_{g}\\tau^{2}\\sigma^{4} - \\sigma^{6} $$\nThe $n_{g}^{2}\\tau^{4}\\sigma^{2}$ terms cancel:\n$$ \\text{Numerator} = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2}\\sigma^{4} - \\sigma^{6} $$\nFactoring out $\\sigma^{4}$:\n$$ \\text{Numerator} = \\sigma^{4}[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}] $$\nTherefore, the final expression for the difference in MSE is:\n$$ \\Delta = \\frac{\\sigma^{4}[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}]}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} $$", "answer": "$$\\boxed{\\frac{\\sigma^{4}\\left[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}\\right]}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}}}$$", "id": "3118672"}]}