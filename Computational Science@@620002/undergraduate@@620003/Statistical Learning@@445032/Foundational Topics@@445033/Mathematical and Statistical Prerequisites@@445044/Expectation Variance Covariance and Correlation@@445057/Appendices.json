{"hands_on_practices": [{"introduction": "Real-world data is rarely perfect, and measurement error is a common challenge in statistical analysis. This practice explores the mathematical consequences of having a noisy predictor variable in a simple linear regression context. By applying the fundamental properties of variance and covariance, you will derive the exact form of \"attenuation bias,\" demonstrating how random measurement error systematically weakens the observed relationship between variables and biases the correlation towards zero [@problem_id:3119226]. This is a critical exercise for learning to interpret regression results from imperfect data.", "problem": "Consider a real-valued statistical learning scenario with a single predictor subject to classical measurement error. Let the unobserved truth be $X^{\\text{true}}$ and the observed predictor be $X^{\\text{obs}} = X^{\\text{true}} + U$, where $U$ is the measurement error. The response is generated by a linear model $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$. Assume $E[U] = 0$, $E[\\varepsilon] = 0$, and that $U$ is independent of both $X^{\\text{true}}$ and $\\varepsilon$, with all variables having finite second moments. Define $\\sigma_{X}^{2} = \\operatorname{Var}(X^{\\text{true}})$, $\\sigma_{U}^{2} = \\operatorname{Var}(U)$, and $\\sigma_{\\varepsilon}^{2} = \\operatorname{Var}(\\varepsilon)$.\n\nSuppose a practitioner regresses $Y$ on $X^{\\text{obs}}$ using ordinary least squares (OLS), ignoring the presence of measurement error. Let $\\hat{\\beta}_{\\text{naive}}$ denote the population OLS slope obtained from regressing $Y$ on $X^{\\text{obs}}$. Using only the core definitions of expectation, variance, covariance, and correlation, and the stated independence assumptions, derive:\n\n1. The attenuation factor $\\lambda = \\hat{\\beta}_{\\text{naive}} / \\beta$ expressed in terms of $\\sigma_{X}^{2}$ and $\\sigma_{U}^{2}$.\n2. The multiplicative bias ratio in the correlation $R = \\operatorname{Corr}(X^{\\text{obs}}, Y) / \\operatorname{Corr}(X^{\\text{true}}, Y)$, simplified as far as possible in terms of $\\sigma_{X}^{2}$ and $\\sigma_{U}^{2}$.\n\nExpress your final answer as a single row matrix containing the two closed-form expressions $(\\lambda, R)$. No rounding is required.", "solution": "The problem statement is a well-posed and classic problem in statistical learning and econometrics, specifically concerning the consequences of measurement error in the predictor variable of a linear regression model. All necessary components and assumptions are provided, either explicitly or implicitly through standard model definitions. The model $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$ is a standard linear model, for which it is a fundamental assumption that the error term $\\varepsilon$ is uncorrelated with the predictor $X^{\\text{true}}$. Therefore, we will proceed under the standard assumption that $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$. All derivations will be based on the provided givens and fundamental definitions of expectation, variance, covariance, and correlation.\n\nLet the quantities be defined as in the problem statement:\n- True model: $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$\n- Measurement model: $X^{\\text{obs}} = X^{\\text{true}} + U$\n- Variances: $\\sigma_{X}^{2} = \\operatorname{Var}(X^{\\text{true}})$, $\\sigma_{U}^{2} = \\operatorname{Var}(U)$, $\\sigma_{\\varepsilon}^{2} = \\operatorname{Var}(\\varepsilon)$\n- Assumptions: $E[U] = 0$, $E[\\varepsilon] = 0$. $U$ is independent of $X^{\\text{true}}$ and $\\varepsilon$. By extension, we also assume $\\varepsilon$ is independent of $X^{\\text{true}}$. Independence implies zero covariance. Thus, we have $\\operatorname{Cov}(X^{\\text{true}}, U) = 0$, $\\operatorname{Cov}(U, \\varepsilon) = 0$, and $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$.\n\nFirst, we derive the attenuation factor $\\lambda = \\hat{\\beta}_{\\text{naive}} / \\beta$.\nThe population ordinary least squares (OLS) slope coefficient for the regression of a response variable $Y$ on a predictor $Z$ is given by the formula $\\operatorname{Cov}(Z, Y) / \\operatorname{Var}(Z)$. In this problem, the practitioner regresses $Y$ on the observed predictor $X^{\\text{obs}}$, so the naive OLS slope is:\n$$\n\\hat{\\beta}_{\\text{naive}} = \\frac{\\operatorname{Cov}(X^{\\text{obs}}, Y)}{\\operatorname{Var}(X^{\\text{obs}})}\n$$\nWe must compute the numerator and the denominator.\n\nThe denominator is the variance of the observed predictor, $\\operatorname{Var}(X^{\\text{obs}})$.\nUsing the definition $X^{\\text{obs}} = X^{\\text{true}} + U$:\n$$\n\\operatorname{Var}(X^{\\text{obs}}) = \\operatorname{Var}(X^{\\text{true}} + U)\n$$\nUsing the property $\\operatorname{Var}(A+B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) + 2\\operatorname{Cov}(A,B)$ and the assumption that $X^{\\text{true}}$ and $U$ are independent (so $\\operatorname{Cov}(X^{\\text{true}}, U) = 0$):\n$$\n\\operatorname{Var}(X^{\\text{obs}}) = \\operatorname{Var}(X^{\\text{true}}) + \\operatorname{Var}(U) = \\sigma_{X}^{2} + \\sigma_{U}^{2}\n$$\n\nThe numerator is the covariance between the observed predictor and the response, $\\operatorname{Cov}(X^{\\text{obs}}, Y)$.\nSubstituting the expressions for $X^{\\text{obs}}$ and $Y$:\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\operatorname{Cov}(X^{\\text{true}} + U, \\alpha + \\beta X^{\\text{true}} + \\varepsilon)\n$$\nUsing the bilinearity property of covariance and noting that covariance with a constant ($\\alpha$) is zero:\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\operatorname{Cov}(X^{\\text{true}} + U, \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) + \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) + \\operatorname{Cov}(U, \\beta X^{\\text{true}}) + \\operatorname{Cov}(U, \\varepsilon)\n$$\nWe evaluate each term:\n- $\\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) = \\beta \\operatorname{Cov}(X^{\\text{true}}, X^{\\text{true}}) = \\beta \\operatorname{Var}(X^{\\text{true}}) = \\beta \\sigma_{X}^{2}$\n- $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$ (standard model assumption)\n- $\\operatorname{Cov}(U, \\beta X^{\\text{true}}) = \\beta \\operatorname{Cov}(U, X^{\\text{true}}) = 0$ (since $U$ and $X^{\\text{true}}$ are independent)\n- $\\operatorname{Cov(U, \\varepsilon)} = 0$ (since $U$ and $\\varepsilon$ are independent)\n\nSumming these terms gives the numerator:\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\beta \\sigma_{X}^{2}\n$$\n\nNow, we can assemble the expression for $\\hat{\\beta}_{\\text{naive}}$:\n$$\n\\hat{\\beta}_{\\text{naive}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\n$$\nThe attenuation factor $\\lambda$ is the ratio of this naive slope to the true slope $\\beta$:\n$$\n\\lambda = \\frac{\\hat{\\beta}_{\\text{naive}}}{\\beta} = \\frac{\\frac{\\beta \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}{\\beta} = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\n$$\n\nSecond, we derive the multiplicative bias ratio in the correlation, $R = \\operatorname{Corr}(X^{\\text{obs}}, Y) / \\operatorname{Corr}(X^{\\text{true}}, Y)$.\nThe definition of the correlation coefficient is $\\operatorname{Corr}(A, B) = \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A)\\operatorname{Var}(B)}}$.\n\nWe first compute $\\operatorname{Corr}(X^{\\text{obs}}, Y)$. We have already found $\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\beta \\sigma_{X}^{2}$ and $\\operatorname{Var}(X^{\\text{obs}}) = \\sigma_{X}^{2} + \\sigma_{U}^{2}$. We need to find $\\operatorname{Var}(Y)$:\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\alpha + \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Var}(\\beta X^{\\text{true}} + \\varepsilon)\n$$\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\beta X^{\\text{true}}) + \\operatorname{Var}(\\varepsilon) + 2\\operatorname{Cov}(\\beta X^{\\text{true}}, \\varepsilon) = \\beta^2 \\operatorname{Var}(X^{\\text{true}}) + \\sigma_{\\varepsilon}^{2} + 2\\beta \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon)\n$$\nUsing the assumption $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$:\n$$\n\\operatorname{Var}(Y) = \\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}\n$$\nSo, the correlation between the observed predictor and the response is:\n$$\n\\operatorname{Corr}(X^{\\text{obs}}, Y) = \\frac{\\operatorname{Cov}(X^{\\text{obs}}, Y)}{\\sqrt{\\operatorname{Var}(X^{\\text{obs}})\\operatorname{Var}(Y)}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{(\\sigma_{X}^{2} + \\sigma_{U}^{2})(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}\n$$\n\nNext, we compute $\\operatorname{Corr}(X^{\\text{true}}, Y)$. We first need $\\operatorname{Cov}(X^{\\text{true}}, Y)$:\n$$\n\\operatorname{Cov}(X^{\\text{true}}, Y) = \\operatorname{Cov}(X^{\\text{true}}, \\alpha + \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) + \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = \\beta \\sigma_{X}^{2}\n$$\nThe variances are $\\operatorname{Var}(X^{\\text{true}}) = \\sigma_{X}^{2}$ and $\\operatorname{Var}(Y) = \\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}$.\nSo, the true correlation is:\n$$\n\\operatorname{Corr}(X^{\\text{true}}, Y) = \\frac{\\operatorname{Cov}(X^{\\text{true}}, Y)}{\\sqrt{\\operatorname{Var}(X^{\\text{true}})\\operatorname{Var}(Y)}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{\\sigma_{X}^{2}(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\operatorname{Corr}(X^{\\text{obs}}, Y)}{\\operatorname{Corr}(X^{\\text{true}}, Y)} = \\frac{\\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{(\\sigma_{X}^{2} + \\sigma_{U}^{2})(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}}{\\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{\\sigma_{X}^{2}(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}}\n$$\nThe terms $\\beta \\sigma_{X}^{2}$ and $\\sqrt{\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}}$ cancel from the numerator and denominator, leaving:\n$$\nR = \\frac{1/\\sqrt{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}{1/\\sqrt{\\sigma_{X}^{2}}} = \\frac{\\sqrt{\\sigma_{X}^{2}}}{\\sqrt{\\sigma_{X}^{2} + \\sigma_{U}^{2}}} = \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n$$\nThis is the simplified expression for the bias ratio in correlation. Note that $R = \\sqrt{\\lambda}$.\nThe two required expressions are $\\lambda = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}$ and $R = \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}} & \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n\\end{pmatrix}\n}\n$$", "id": "3119226"}, {"introduction": "Covariance and correlation measure association, but they can be misleading if a critical variable is overlooked. This hands-on coding exercise delves into Simpson's Paradox, a startling phenomenon where a trend that appears in different groups of data disappears or reverses when these groups are combined. You will generate synthetic data where a confounding group variable creates this paradox, and then use model selection to see how including the confounder resolves the issue, providing a powerful lesson on the dangers of omitted variable bias [@problem_id:3119190].", "problem": "You must write a complete program that constructs synthetic datasets exhibiting Simpson’s paradox in the context of subgroup-specific association between two real-valued random variables and then evaluates model selection when a confounding group indicator is omitted versus included. All computations must start from the foundational definitions of expectation, variance, covariance, and correlation, and ordinary least squares estimation as the minimizer of the sum of squared residuals.\n\nDefinition base:\n- For real-valued random variables $X$ and $Y$ with finite second moments, the expectation is $E[X]$, the variance is $\\operatorname{Var}(X) = E\\left[(X - E[X])^2\\right]$, the covariance is $\\operatorname{Cov}(X,Y) = E\\left[(X - E[X])(Y - E[Y])\\right]$, and the Pearson correlation is $\\operatorname{Corr}(X,Y) = \\dfrac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$.\n- For a dataset $\\{(x_i,y_i)\\}_{i=1}^n$, the sample analogs are obtained by replacing expectations with sample means and using the corresponding centered sums. Use the unbiased sample covariance and variance with divisor $n-1$; the correlation is the ratio of the sample covariance to the product of sample standard deviations.\n- Ordinary Least Squares (OLS) fits minimize $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ with respect to the regression parameters. The residual sum of squares is $\\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$. For model selection, use the Akaike Information Criterion (AIC), defined for Gaussian errors up to an additive constant by $\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$, where $k$ is the number of estimated parameters, and the model with the smaller $\\mathrm{AIC}^\\star$ is preferred.\n\nData-generating process:\n- There are $2$ subgroups indexed by $g \\in \\{0,1\\}$ with sample sizes $n_0$ and $n_1$. For each group $g$, generate $X \\mid G=g \\sim \\mathcal{N}(\\mu_{x,g}, \\sigma_x^2)$ and $Y \\mid X,G=g$ according to the linear model $Y = \\alpha_g + \\beta X + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$ independent of $X$.\n- This construction yields within-group linear association between $X$ and $Y$ with common slope $\\beta$ but potentially different intercepts $\\alpha_0$ and $\\alpha_1$, creating a confounding shift between groups in the $(X,Y)$ plane. By appropriate choice of $(\\mu_{x,0},\\mu_{x,1},\\alpha_0,\\alpha_1,\\beta,\\sigma_x,\\sigma_\\varepsilon)$, the marginal association between $X$ and $Y$ (after pooling the groups) can have the opposite sign from the within-group association, demonstrating Simpson’s paradox.\n\nComputations to perform for each dataset:\n1. Compute, for each group $g \\in \\{0,1\\}$, the unbiased sample covariance $\\widehat{\\operatorname{Cov}}_g(X,Y)$ and determine its sign using $\\operatorname{sign}(z)$ defined as $+1$ if $z > \\tau$, $-1$ if $z < -\\tau$, and $0$ otherwise, with tolerance $\\tau = 10^{-10}$.\n2. Compute, on the pooled data, the unbiased sample Pearson correlation $\\widehat{r} = \\widehat{\\operatorname{Corr}}(X,Y)$.\n3. Define a reversal indicator $R$ equal to $1$ if both subgroup covariances have the same nonzero sign and the sign of $\\widehat{r}$ is the opposite, and equal to $0$ otherwise.\n4. Fit two OLS models:\n   - Model $\\mathcal{M}_0$: $Y$ regressed on an intercept and $X$ only.\n   - Model $\\mathcal{M}_1$: $Y$ regressed on an intercept, $X$, and the binary group indicator $G$.\n   For each model, compute $\\mathrm{RSS}$ on the full sample, then compute $\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$, where $k=2$ for $\\mathcal{M}_0$ and $k=3$ for $\\mathcal{M}_1$, and $n = n_0 + n_1$. Select the model with the smaller $\\mathrm{AIC}^\\star$ and report its index as $0$ for $\\mathcal{M}_0$ or $1$ for $\\mathcal{M}_1$.\n5. Report for each dataset the triple $[\\widehat{r}_{\\text{rounded}}, R, M]$, where $\\widehat{r}_{\\text{rounded}}$ is $\\widehat{r}$ rounded to $4$ decimal places, $R \\in \\{0,1\\}$ is the reversal indicator, and $M \\in \\{0,1\\}$ is the selected model index.\n\nTest suite:\nUse the following $5$ parameter sets. Each case is a tuple $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$.\n\n- Case A (classic Simpson reversal with positive within-group association, negative marginal): $(12345, 200, 200, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6)$.\n- Case B (negative within-group association, positive marginal): $(54321, 200, 200, 4.0, 1.0, -1.0, 3.0, -3.0, 0.6, 0.6)$.\n- Case C (boundary case with near-zero marginal association by design, large $n$ for stability): $(102938, 4000, 4000, 1.5, -1.5, 1.0, -1.666666667, 1.666666667, 0.5, 0.5)$.\n- Case D (small-sample edge case, strong confounding, results may be noisy): $(777, 5, 5, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6)$.\n- Case E (very small within-group variance to emphasize between-group reversal): $(24680, 100, 100, 5.0, 1.0, 1.0, -4.0, 4.0, 0.05, 0.05)$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of the $5$ result triples, in order A through E, with no spaces, as a comma-separated list enclosed in square brackets. For example, the output should look like\n$[[r_A,R_A,M_A],[r_B,R_B,M_B],[r_C,R_C,M_C],[r_D,R_D,M_D],[r_E,R_E,M_E]]$\nwhere each $r_\\cdot$ is a floating-point number rounded to $4$ decimal places, and each $R_\\cdot$ and $M_\\cdot$ is an integer in $\\{0,1\\}$.", "solution": "The problem requires us to simulate datasets that exhibit Simpson's paradox, a statistical phenomenon where a trend observed within subgroups of data is reversed when the data are aggregated. We will then use a standard model selection criterion to determine if accounting for the subgroup structure yields a better statistical model.\n\n**1. Conceptual Framework: Simpson's Paradox and Confounding**\n\nSimpson's paradox arises due to the presence of a confounding variable. In this problem, the group indicator $G$ is the confounder. It is a variable that is correlated with both the predictor variable $X$ and the outcome variable $Y$. The data are generated such that:\n- The relationship between $X$ and $Y$ *within* each group $g$ is linear with a common slope $\\beta$: $Y = \\alpha_g + \\beta X + \\varepsilon$. The sign of $\\beta$ determines the direction of the true, underlying association.\n- The groups differ in their distribution of $X$ (via means $\\mu_{x,g}$) and their baseline level of $Y$ (via intercepts $\\alpha_g$).\nBy carefully choosing the parameters $(\\mu_{x,0}, \\mu_{x,1})$ and $(\\alpha_0, \\alpha_1)$, we can create a situation where the trend connecting the centers of the two group-level data clouds runs in the opposite direction to the trend within each cloud. When the data are pooled and the group structure is ignored, this spurious between-group trend can dominate the true within-group trend, leading to an aggregated (or marginal) correlation that has the opposite sign of $\\beta$.\n\n**2. Data Generation and Analysis Steps**\n\nFor each test case, we perform the following sequence of operations:\n\n**a. Data Simulation:**\nUsing the provided parameters $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$, we generate two datasets corresponding to groups $g=0$ and $g=1$.\n- For group $g=0$, we draw $n_0$ samples of $X_0 \\sim \\mathcal{N}(\\mu_{x,0}, \\sigma_x^2)$. Then, we compute the corresponding $Y_0$ values as $Y_0 = \\alpha_0 + \\beta X_0 + \\varepsilon_0$, where $\\varepsilon_0 \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$.\n- Similarly, for group $g=1$, we draw $n_1$ samples of $X_1 \\sim \\mathcal{N}(\\mu_{x,1}, \\sigma_x^2)$ and compute $Y_1 = \\alpha_1 + \\beta X_1 + \\varepsilon_1$, where $\\varepsilon_1 \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$.\n\n**b. Subgroup and Pooled Association Analysis:**\n- **Within-Group Covariance:** For each group $g \\in \\{0,1\\}$, we compute the unbiased sample covariance, which measures the direction and strength of the linear association within that group:\n  $$ \\widehat{\\operatorname{Cov}}_g(X,Y) = \\frac{1}{n_g-1} \\sum_{i=1}^{n_g} (x_{i,g} - \\bar{x}_g)(y_{i,g} - \\bar{y}_g) $$\n  We then determine its sign, $\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_g(X,Y))$, using the specified numerical tolerance $\\tau=10^{-10}$. In a successful simulation, both signs should match the sign of the true slope parameter $\\beta$.\n\n- **Pooled Correlation:** We combine the two datasets into a single pooled sample of size $n=n_0+n_1$. We then compute the Pearson sample correlation coefficient $\\widehat{r}$:\n  $$ \\widehat{r} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n  This measures the marginal linear association, ignoring the group structure.\n\n- **Reversal Indicator ($R$):** We formally check for the paradox. The indicator $R$ is set to $1$ if the paradox is observed, i.e., if the within-group associations are consistent and non-zero, and the pooled association is in the opposite direction. Formally: $R=1$ if $\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_0) = \\operatorname{sign}(\\widehat{\\operatorname{Cov}}_1) \\neq 0$ and $\\operatorname{sign}(\\widehat{r}) = -\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_0)$. Otherwise, $R=0$.\n\n**c. Model Comparison via OLS and AIC:**\nWe fit two linear models to the pooled data using Ordinary Least Squares (OLS). OLS finds the parameter estimates that minimize the Residual Sum of Squares ($\\mathrm{RSS}$).\n\n- **Model $\\mathcal{M}_0$ (Simple Model):** $Y_i = b_0 + b_1 X_i + e_i$. This model regresses $Y$ on $X$ and an intercept, ignoring the confounding group variable $G$. The number of estimated parameters is $k_0 = 2$.\n- **Model $\\mathcal{M}_1$ (Confounder Model):** $Y_i = b'_0 + b'_1 X_i + b'_2 G_i + e'_i$. This model includes the binary group indicator $G$ (e.g., $G_i=0$ for group $0$, $G_i=1$ for group $1$) as a predictor. This is equivalent to fitting a model with a common slope but different intercepts for each group, which matches our data-generating process. The number of parameters is $k_1 = 3$.\n\nFor each model $\\mathcal{M}_j$, we compute its $\\mathrm{RSS}_j$ and then its Akaike Information Criterion score:\n$$ \\mathrm{AIC}^\\star_j = n \\log(\\mathrm{RSS}_j/n) + 2k_j $$\nThe $\\mathrm{AIC}^\\star$ balances model fit (a lower $\\mathrm{RSS}$ is better) against model complexity (a lower $k$ is better). The model with the smaller $\\mathrm{AIC}^\\star$ value is preferred. We expect that when Simpson's paradox is present, $\\mathcal{M}_1$ will provide a substantially better fit (much lower $\\mathrm{RSS}_1$), outweighing its penalty for an extra parameter, and thus will be selected. We report the index $M$ of the selected model. For each test case, we assemble the required triple: $[\\widehat{r}_{\\text{rounded}}, R, M]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs synthetic datasets exhibiting Simpson’s paradox, evaluates model selection\n    with and without a confounding variable, and reports the results.\n    \"\"\"\n    test_cases = [\n        # (seed, n0, n1, mu_x0, mu_x1, beta, alpha0, alpha1, sigma_x, sigma_e)\n        (12345, 200, 200, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6),\n        (54321, 200, 200, 4.0, 1.0, -1.0, 3.0, -3.0, 0.6, 0.6),\n        (102938, 4000, 4000, 1.5, -1.5, 1.0, -1.666666667, 1.666666667, 0.5, 0.5),\n        (777, 5, 5, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6),\n        (24680, 100, 100, 5.0, 1.0, 1.0, -4.0, 4.0, 0.05, 0.05),\n    ]\n\n    results = []\n\n    def sign_func(z, tau=1e-10):\n        if z > tau:\n            return 1\n        elif z < -tau:\n            return -1\n        else:\n            return 0\n\n    for case in test_cases:\n        seed, n0, n1, mu_x0, mu_x1, beta, alpha0, alpha1, sigma_x, sigma_e = case\n\n        rng = np.random.default_rng(seed)\n\n        # Generate data for both groups\n        X0 = rng.normal(loc=mu_x0, scale=sigma_x, size=n0)\n        Y0 = alpha0 + beta * X0 + rng.normal(loc=0, scale=sigma_e, size=n0)\n\n        X1 = rng.normal(loc=mu_x1, scale=sigma_x, size=n1)\n        Y1 = alpha1 + beta * X1 + rng.normal(loc=0, scale=sigma_e, size=n1)\n\n        # 1. Compute subgroup unbiased sample covariance and its sign\n        cov0 = np.cov(X0, Y0, ddof=1)[0, 1]\n        cov1 = np.cov(X1, Y1, ddof=1)[0, 1]\n        sign_cov0 = sign_func(cov0)\n        sign_cov1 = sign_func(cov1)\n\n        # Pool the data\n        X_pooled = np.concatenate((X0, X1))\n        Y_pooled = np.concatenate((Y0, Y1))\n        n = n0 + n1\n\n        # 2. Compute pooled sample Pearson correlation\n        r_pooled = np.corrcoef(X_pooled, Y_pooled)[0, 1]\n        sign_r_pooled = sign_func(r_pooled)\n\n        # 3. Compute reversal indicator R\n        R = 0\n        if sign_cov0 != 0 and sign_cov0 == sign_cov1 and sign_r_pooled == -sign_cov0:\n            R = 1\n\n        # 4. Fit OLS models and compute AIC\n        # Model M0: Y ~ 1 + X\n        X_mat0 = np.vstack([np.ones(n), X_pooled]).T\n        # Solve (X'X)b = X'y for b, which is more stable than inv(X'X)\n        try:\n            b0 = np.linalg.solve(X_mat0.T @ X_mat0, X_mat0.T @ Y_pooled)\n            Y_hat0 = X_mat0 @ b0\n            RSS0 = np.sum((Y_pooled - Y_hat0)**2)\n            k0 = 2\n            AIC0 = n * np.log(RSS0 / n) + 2 * k0\n        except np.linalg.LinAlgError:\n            AIC0 = np.inf\n\n\n        # Model M1: Y ~ 1 + X + G\n        G_indicator = np.concatenate([np.zeros(n0), np.ones(n1)])\n        X_mat1 = np.vstack([np.ones(n), X_pooled, G_indicator]).T\n        try:\n            b1 = np.linalg.solve(X_mat1.T @ X_mat1, X_mat1.T @ Y_pooled)\n            Y_hat1 = X_mat1 @ b1\n            RSS1 = np.sum((Y_pooled - Y_hat1)**2)\n            k1 = 3\n            AIC1 = n * np.log(RSS1 / n) + 2 * k1\n        except np.linalg.LinAlgError:\n            AIC1 = np.inf\n        \n        # Select model with smaller AIC\n        M = 1 if AIC1 < AIC0 else 0\n\n        # 5. Formulate the result triple\n        r_rounded = round(r_pooled, 4)\n        results.append([r_rounded, R, M])\n\n    # Format the final output string as specified\n    output_str = '[' + ','.join([f'[{r},{res_R},{res_M}]' for r, res_R, res_M in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3119190"}, {"introduction": "Ensemble methods like Bootstrap Aggregating (bagging) are cornerstones of modern machine learning, valued for their ability to improve model stability by reducing variance. The effectiveness of bagging, however, is not unconditional; it hinges on the diversity of the models being averaged. This practice guides you through a theoretical analysis to quantify how the correlation between base estimators limits the overall variance reduction, showing that the final ensemble variance can never be lower than the average covariance between the models [@problem_id:3119186]. This reveals a core principle of ensemble design: the goal is not just to build accurate models, but to build accurate and uncorrelated ones.", "problem": "Consider a simplified analytical model for Bootstrap Aggregating (bagging) that isolates the role of dependence among base learners. Let there be $B$ base estimators whose predictions are averaged to form the bagged estimator. Each base estimator is constructed from $m$ noise contributions. Model the $b$-th base estimator as $X_b = \\theta + \\varepsilon_b$, where $\\theta$ is a fixed constant and\n$$\n\\varepsilon_b = \\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j},\n$$\nwith $\\{Z_{b,j}\\}$ being independent and identically distributed random variables satisfying $\\mathbb{E}[Z_{b,j}] = 0$ and $\\operatorname{Var}(Z_{b,j}) = \\sigma_Z^2$. To capture overlap among bootstrap samples, assume that for any pair of distinct base estimators $(b,c)$, a fraction $s$ of the $m$ noise terms are shared exactly (that is, $s m$ of the pairs $(Z_{b,j}, Z_{c,j})$ are identical random variables), while the remaining $(1-s)m$ terms are independent across $b$ and $c$. This models high overlap when $s$ is close to $1$, and low overlap when $s$ is close to $0$.\n\nYour task is to:\n- Use only the fundamental definitions of expectation, variance, covariance, and correlation to derive the variance of the bagged estimator\n$$\n\\overline{X} = \\frac{1}{B} \\sum_{b=1}^{B} X_b\n$$\nin terms of $B$, $m$, $\\sigma_Z^2$, and $s$.\n- From your derivation, express the single-estimator variance, the bagged-estimator variance under overlap $s$, and the variance ratio $\\operatorname{Var}(\\overline{X}) / \\operatorname{Var}(X_b)$, all as functions of $B$, $m$, $\\sigma_Z^2$, and $s$.\n- Additionally, compute the hypothetical bagged-estimator variance in the special case where the base estimators are independent (that is, as if $s=0$), to serve as a naive baseline for comparison.\n\nImplement a complete, runnable program that, for each test case below, outputs a list with three floating-point numbers in the order:\n$[$bagged\\_variance, independent\\_bagged\\_variance, variance\\_ratio$]$, where:\n- bagged\\_variance is $\\operatorname{Var}(\\overline{X})$ at the given $s$,\n- independent\\_bagged\\_variance is $\\operatorname{Var}(\\overline{X})$ for $s=0$,\n- variance\\_ratio is $\\operatorname{Var}(\\overline{X}) / \\operatorname{Var}(X_b)$ at the given $s$.\n\nUse the following test suite, where each tuple is $(B, m, \\sigma_Z^2, s)$:\n- Case $1$: $(B, m, \\sigma_Z^2, s) = (10, 100, 1.0, 0.8)$, a typical high-overlap scenario.\n- Case $2$: $(B, m, \\sigma_Z^2, s) = (10, 100, 1.0, 0.0)$, the independent baseline.\n- Case $3$: $(B, m, \\sigma_Z^2, s) = (100, 50, 2.0, 0.5)$, large ensemble with moderate overlap.\n- Case $4$: $(B, m, \\sigma_Z^2, s) = (1, 200, 1.0, 0.9)$, boundary with a single model.\n- Case $5$: $(B, m, \\sigma_Z^2, s) = (50, 100, 1.0, 0.95)$, extreme overlap.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a comma-separated list in square brackets. For example, the overall format must be\n$[ [\\text{case}_1], [\\text{case}_2], \\dots ]$\nwith no additional text. All outputs must be real numbers in decimal form. No physical units or angle units are involved in this problem.", "solution": "The objective is to derive the variance of the bagged estimator $\\overline{X} = \\frac{1}{B} \\sum_{b=1}^{B} X_b$ and related quantities. The derivation will proceed by first finding the variance of a single estimator, then the covariance between any two distinct estimators, and finally combining these to find the variance of their average.\n\nThe fundamental formula for the variance of a sum of random variables is:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} Y_i\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\operatorname{Cov}(Y_i, Y_j) = \\sum_{i=1}^{n} \\operatorname{Var}(Y_i) + \\sum_{i \\neq j} \\operatorname{Cov}(Y_i, Y_j)\n$$\nApplying this to the bagged estimator $\\overline{X}$:\n$$\n\\operatorname{Var}(\\overline{X}) = \\operatorname{Var}\\left(\\frac{1}{B} \\sum_{b=1}^{B} X_b\\right) = \\frac{1}{B^2} \\operatorname{Var}\\left(\\sum_{b=1}^{B} X_b\\right) = \\frac{1}{B^2} \\left( \\sum_{b=1}^{B} \\operatorname{Var}(X_b) + \\sum_{b \\neq c} \\operatorname{Cov}(X_b, X_c) \\right)\n$$\nTo evaluate this expression, we must compute $\\operatorname{Var}(X_b)$ and $\\operatorname{Cov}(X_b, X_c)$ for $b \\neq c$.\n\n**Step 1: Variance of a single base estimator, $\\operatorname{Var}(X_b)$**\n\nThe base estimator with index $b$, denoted $X_b$, is given by $X_b = \\theta + \\varepsilon_b$, where $\\theta$ is a constant. The variance is therefore:\n$$\n\\operatorname{Var}(X_b) = \\operatorname{Var}(\\theta + \\varepsilon_b) = \\operatorname{Var}(\\varepsilon_b)\n$$\nThe noise term $\\varepsilon_b$ is an average of $m$ i.i.d. random variables $Z_{b,j}$:\n$$\n\\varepsilon_b = \\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j}\n$$\nGiven that the variables $\\{Z_{b,j}\\}_{j=1}^m$ are independent for a fixed $b$, and $\\operatorname{Var}(Z_{b,j}) = \\sigma_Z^2$:\n$$\n\\operatorname{Var}(\\varepsilon_b) = \\operatorname{Var}\\left(\\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j}\\right) = \\frac{1}{m^2} \\operatorname{Var}\\left(\\sum_{j=1}^{m} Z_{b,j}\\right) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\operatorname{Var}(Z_{b,j})\n$$\n$$\n\\operatorname{Var}(\\varepsilon_b) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\sigma_Z^2 = \\frac{1}{m^2} (m \\sigma_Z^2) = \\frac{\\sigma_Z^2}{m}\n$$\nThus, the variance of a single base estimator is:\n$$\n\\operatorname{Var}(X_b) = \\frac{\\sigma_Z^2}{m}\n$$\n\n**Step 2: Covariance between two distinct base estimators, $\\operatorname{Cov}(X_b, X_c)$ for $b \\neq c$**\n\nFor two distinct estimators $X_b$ and $X_c$ (where $b \\neq c$), the covariance is:\n$$\n\\operatorname{Cov}(X_b, X_c) = \\operatorname{Cov}(\\theta + \\varepsilon_b, \\theta + \\varepsilon_c) = \\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c)\n$$\n$$\n\\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c) = \\operatorname{Cov}\\left(\\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j}, \\frac{1}{m} \\sum_{k=1}^{m} Z_{c,k}\\right) = \\frac{1}{m^2} \\operatorname{Cov}\\left(\\sum_{j=1}^{m} Z_{b,j}, \\sum_{k=1}^{m} Z_{c,k}\\right)\n$$\nBy the bilinearity of covariance:\n$$\n\\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\sum_{k=1}^{m} \\operatorname{Cov}(Z_{b,j}, Z_{c,k})\n$$\nThe problem states that a fraction $s$ of the noise terms are shared. Without loss of generality, we assume the first $sm$ terms are shared, meaning $Z_{b,j} = Z_{c,j}$ for $j \\in \\{1, \\dots, sm\\}$. The remaining $(1-s)m$ terms are independent. The covariance $\\operatorname{Cov}(Z_{b,j}, Z_{c,k})$ is non-zero only if $Z_{b,j}$ and $Z_{c,k}$ are the same random variable. This occurs when $j=k$ and $j \\in \\{1, \\dots, sm\\}$. In this case:\n$$\n\\operatorname{Cov}(Z_{b,j}, Z_{c,j}) = \\operatorname{Var}(Z_{b,j}) = \\sigma_Z^2\n$$\nFor all other pairs $(j,k)$, the variables are independent, so $\\operatorname{Cov}(Z_{b,j}, Z_{c,k}) = 0$. The double summation therefore reduces to a single sum over the shared indices:\n$$\n\\sum_{j=1}^{m} \\sum_{k=1}^{m} \\operatorname{Cov}(Z_{b,j}, Z_{c,k}) = \\sum_{j=1}^{sm} \\operatorname{Cov}(Z_{b,j}, Z_{c,j}) = \\sum_{j=1}^{sm} \\sigma_Z^2 = sm\\sigma_Z^2\n$$\nSubstituting this back into the expression for $\\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c)$:\n$$\n\\operatorname{Cov}(X_b, X_c) = \\frac{1}{m^2} (sm\\sigma_Z^2) = s \\frac{\\sigma_Z^2}{m}\n$$\nThis shows that the covariance between any two distinct estimators is the fraction of overlap $s$ times the variance of a single estimator.\n\n**Step 3: Variance of the bagged estimator, $\\operatorname{Var}(\\overline{X})$**\n\nNow we can evaluate the full expression for $\\operatorname{Var}(\\overline{X})$. The sum has $B$ variance terms (where $b=c$) and $B(B-1)$ covariance terms (where $b \\neq c$).\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{1}{B^2} \\left[ \\sum_{b=1}^{B} \\operatorname{Var}(X_b) + \\sum_{b \\neq c} \\operatorname{Cov}(X_b, X_c) \\right]\n$$\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{1}{B^2} \\left[ B \\cdot \\operatorname{Var}(X_b) + B(B-1) \\cdot \\operatorname{Cov}(X_b, X_c) \\right]\n$$\nSubstituting the expressions for variance and covariance:\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{1}{B^2} \\left[ B \\left(\\frac{\\sigma_Z^2}{m}\\right) + B(B-1) \\left(s \\frac{\\sigma_Z^2}{m}\\right) \\right]\n$$\nFactoring out common terms:\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{B}{B^2} \\frac{\\sigma_Z^2}{m} \\left[ 1 + (B-1)s \\right] = \\frac{1}{B} \\frac{\\sigma_Z^2}{m} \\left[ 1 + Bs - s \\right]\n$$\nRearranging the terms yields a more interpretable form:\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{\\sigma_Z^2}{m} \\left[ \\frac{1 - s}{B} + s \\right]\n$$\nThis is the final expression for the variance of the bagged estimator.\n\n**Step 4: Summary of requested quantities**\n\nBased on the derivation, we can now list the expressions for the required quantities.\n\n1.  **Single-estimator variance**: This was calculated in Step 1.\n    $$\n    \\operatorname{Var}(X_b) = \\frac{\\sigma_Z^2}{m}\n    $$\n\n2.  **Bagged-estimator variance (bagged\\_variance)**: This is the primary result from Step 3.\n    $$\n    \\operatorname{Var}(\\overline{X}) = \\frac{\\sigma_Z^2}{m} \\left( s + \\frac{1-s}{B} \\right)\n    $$\n\n3.  **Hypothetical independent bagged-estimator variance (independent\\_bagged\\_variance)**: This corresponds to setting $s=0$ in the formula for $\\operatorname{Var}(\\overline{X})$.\n    $$\n    \\operatorname{Var}(\\overline{X})_{s=0} = \\frac{\\sigma_Z^2}{m} \\left( 0 + \\frac{1-0}{B} \\right) = \\frac{\\sigma_Z^2}{mB}\n    $$\n    This is the classic result for the variance of the mean of $B$ independent random variables.\n\n4.  **Variance ratio (variance\\_ratio)**: This is the ratio of the bagged-estimator variance to the single-estimator variance.\n    $$\n    \\frac{\\operatorname{Var}(\\overline{X})}{\\operatorname{Var}(X_b)} = \\frac{\\frac{\\sigma_Z^2}{m} \\left(s + \\frac{1-s}{B}\\right)}{\\frac{\\sigma_Z^2}{m}} = s + \\frac{1-s}{B}\n    $$\nThis ratio quantifies the variance reduction achieved by bagging. As $B \\to \\infty$, the ratio approaches $s$. This shows that the potential for variance reduction is limited by the correlation $s$ between the base estimators. If $s > 0$, the variance cannot be reduced to zero, no matter how many estimators $B$ are used.\n\nThese expressions provide the basis for the numerical calculations in the program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# numpy and scipy are not required for this problem's calculations.\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (B, m, sigma_Z^2, s)\n    test_cases = [\n        (10, 100, 1.0, 0.8),    # Case 1\n        (10, 100, 1.0, 0.0),    # Case 2\n        (100, 50, 2.0, 0.5),    # Case 3\n        (1, 200, 1.0, 0.9),     # Case 4\n        (50, 100, 1.0, 0.95),   # Case 5\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case.\n        B, m, sigma_Z_sq, s = case\n\n        # Derived formulas from the analytical solution:\n        # bagged_variance = (sigma_Z^2 / m) * (s + (1-s)/B)\n        # independent_bagged_variance = sigma_Z^2 / (m * B)\n        # variance_ratio = s + (1-s)/B\n        \n        # 1. Calculate the bagged variance with the given overlap s.\n        bagged_variance = (sigma_Z_sq / m) * (s + (1.0 - s) / B)\n        \n        # 2. Calculate the hypothetical bagged variance for independent estimators (s=0).\n        independent_bagged_variance = sigma_Z_sq / (m * B)\n        \n        # 3. Calculate the variance reduction ratio.\n        # This is equivalent to bagged_variance / (sigma_Z_sq / m).\n        variance_ratio = s + (1.0 - s) / B\n        \n        case_results = [bagged_variance, independent_bagged_variance, variance_ratio]\n        all_results.append(case_results)\n\n    # Format the final output string according to the specified format.\n    # The required format is a list of lists, e.g., [[r1,r2,r3],[...],...].\n    result_strings = []\n    for res_list in all_results:\n        # Format each list of results as a string \"[n1,n2,n3]\"\n        # Using map(str, ...) ensures floating point numbers are converted correctly.\n        inner_string = f\"[{','.join(map(str, res_list))}]\"\n        result_strings.append(inner_string)\n    \n    # Join the individual case strings with commas and enclose in brackets.\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "3119186"}]}