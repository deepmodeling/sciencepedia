{"hands_on_practices": [{"introduction": "The squared error loss is a cornerstone of regression, but its quadratic nature makes it highly sensitive to outliers. A single aberrant data point can dramatically skew the entire model. This practice [@problem_id:3153932] introduces the Huber loss, a robust alternative that cleverly combines the desirable properties of squared error for small residuals with the robustness of absolute error for large ones. By implementing and comparing regression models under both loss functions, you will gain a practical understanding of the robustness trade-off and learn a standard algorithm for robust optimization, Iteratively Reweighted Least Squares (IRLS).", "problem": "Consider a univariate linear regression model with predictor $x \\in \\mathbb{R}$ and response $y \\in \\mathbb{R}$, where the hypothesis is $h_{\\boldsymbol{\\beta}}(x) = \\beta_0 + \\beta_1 x$ with parameters $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)$. The task is to perform Empirical Risk Minimization (ERM) by minimizing an average loss over a fixed dataset. Use only well-defined convex losses and ensure numerical stability of the optimization. The dataset $\\mathcal{D}$ is explicitly given as the following pairs $(x_i, y_i)$ for $i = 1, \\dots, n$ with $n = 9$:\n$$(0, 1.1),\\ (1, 2.9),\\ (2, 5.2),\\ (3, 7.0),\\ (4, 8.9),\\ (5, 11.2),\\ (2.5, 20.0),\\ (4.0, -5.0),\\ (10.0, 35.0).$$\nThe last three points are outliers by construction (two are extreme residuals and one has high leverage in $x$). Let the empirical risk be $R(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\ell(r_i)$ where $r_i = y_i - h_{\\boldsymbol{\\beta}}(x_i)$ is the residual and $\\ell$ is a convex loss.\n\nDefine the squared loss $\\ell_{\\mathrm{sq}}(r) = \\tfrac{1}{2} r^2$ and the Huber loss $L_\\delta(r)$ with threshold $\\delta > 0$ by\n$$\nL_\\delta(r) =\n\\begin{cases}\n\\tfrac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n\\delta \\left(|r| - \\tfrac{1}{2} \\delta\\right),  \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nYour program must:\n- Compute the minimizer $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$ of $R(\\boldsymbol{\\beta})$ under $\\ell_{\\mathrm{sq}}$ by solving the corresponding first-order optimality conditions using a numerically stable method.\n- For each specified value of $\\delta$, compute the minimizer $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{Huber}}(\\delta)$ of $R(\\boldsymbol{\\beta})$ under $L_\\delta$ using a principled convex optimization approach such as Iteratively Reweighted Least Squares (IRLS), ensuring convergence criteria are checked.\n\nExplain conceptually, in your solution, the robustness trade-off as $\\delta$ varies: how changing $\\delta$ alters the influence of outliers and the relationship to squared loss minimization.\n\nTest suite specification:\n- Use the dataset $\\mathcal{D}$ as given.\n- Evaluate Huber minimizers for the following thresholds: $\\delta \\in \\{0.5, 1.0, 3.0, 10^6\\}$.\n- The case $\\delta = 10^6$ is a boundary case approximating squared loss behavior.\n- The case $\\delta = 0.5$ is an edge case emphasizing robustness with near-linear behavior on most residuals.\n- The case $\\delta = 1.0$ is a moderate \"happy path\" case.\n- The case $\\delta = 3.0$ is a larger threshold reducing robustness but still capping extreme influences.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets with the following order:\n$$[\\beta_{0,\\mathrm{sq}}, \\beta_{1,\\mathrm{sq}}, \\beta_{0,\\mathrm{Huber}}(0.5), \\beta_{1,\\mathrm{Huber}}(0.5), \\beta_{0,\\mathrm{Huber}}(1.0), \\beta_{1,\\mathrm{Huber}}(1.0), \\beta_{0,\\mathrm{Huber}}(3.0), \\beta_{1,\\mathrm{Huber}}(3.0), \\beta_{0,\\mathrm{Huber}}(10^6), \\beta_{1,\\mathrm{Huber}}(10^6)].$$\nAll numeric entries must be real numbers. No physical units are involved.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Model**: Univariate linear regression hypothesis $h_{\\boldsymbol{\\beta}}(x) = \\beta_0 + \\beta_1 x$.\n- **Parameters**: $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)$.\n- **Task**: Perform Empirical Risk Minimization (ERM).\n- **Dataset** $\\mathcal{D}$: A set of $n=9$ pairs $(x_i, y_i)$:\n  $$(0, 1.1), (1, 2.9), (2, 5.2), (3, 7.0), (4, 8.9), (5, 11.2), (2.5, 20.0), (4.0, -5.0), (10.0, 35.0).$$\n- **Empirical Risk**: $R(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\ell(r_i)$, where the residual is $r_i = y_i - h_{\\boldsymbol{\\beta}}(x_i)$.\n- **Loss Functions**:\n    1.  Squared loss: $\\ell_{\\mathrm{sq}}(r) = \\tfrac{1}{2} r^2$.\n    2.  Huber loss with threshold $\\delta > 0$:\n        $$\n        L_\\delta(r) =\n        \\begin{cases}\n        \\tfrac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n        \\delta \\left(|r| - \\tfrac{1}{2} \\delta\\right),  \\text{if } |r| > \\delta.\n        \\end{cases}\n        $$\n- **Computational Requirements**:\n    1.  Compute the squared loss minimizer $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$ by solving first-order optimality conditions using a numerically stable method.\n    2.  Compute the Huber loss minimizer $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{Huber}}(\\delta)$ for $\\delta \\in \\{0.5, 1.0, 3.0, 10^6\\}$ using Iteratively Reweighted Least Squares (IRLS).\n- **Conceptual Requirement**: Explain the robustness trade-off as $\\delta$ varies.\n- **Output Format**: A single-line list of real numbers: $[\\beta_{0,\\mathrm{sq}}, \\beta_{1,\\mathrm{sq}}, \\beta_{0,\\mathrm{Huber}}(0.5), \\beta_{1,\\mathrm{Huber}}(0.5), \\dots, \\beta_{0,\\mathrm{Huber}}(10^6), \\beta_{1,\\mathrm{Huber}}(10^6)]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria:\n- **Scientifically Grounded**: The problem is well-founded in the theory of statistical learning, robust statistics, and convex optimization. Linear regression, squared loss, Huber loss, and Empirical Risk Minimization are all standard, well-established concepts.\n- **Well-Posed**: The objectives for both squared loss and Huber loss are convex functions of the parameters $\\boldsymbol{\\beta}$. As long as the design matrix is of full rank (which it is, as the $x_i$ values are not all identical), unique minimizers exist. The problem is self-contained and provides all necessary information.\n- **Objective**: The problem is stated using precise mathematical language and contains no subjective or ambiguous terminology.\n\nNo flaws are identified. The problem does not violate any scientific principles, is not incomplete or contradictory, is computationally feasible, and is well-posed. The inclusion of outliers and a range of $\\delta$ values constitutes a meaningful test of robust regression principles, not a trivial exercise.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A principled solution will be constructed.\n\n### Solution Derivation\n\nThe core of the problem is to find the parameter vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ that minimizes the average loss over the dataset $\\mathcal{D}$. This can be formulated in matrix notation. Let the response vector be $\\mathbf{y} \\in \\mathbb{R}^n$ and the design matrix be $\\mathbf{X} \\in \\mathbb{R}^{n \\times 2}$, where $n=9$.\n$$\n\\mathbf{y} = \\begin{pmatrix} 1.1 \\\\ 2.9 \\\\ 5.2 \\\\ 7.0 \\\\ 8.9 \\\\ 11.2 \\\\ 20.0 \\\\ -5.0 \\\\ 35.0 \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\\\ 1  3 \\\\ 1  4 \\\\ 1  5 \\\\ 1  2.5 \\\\ 1  4.0 \\\\ 1  10.0 \\end{pmatrix}\n$$\nThe vector of residuals is $\\mathbf{r} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$.\n\n**1. Minimization under Squared Loss (Ordinary Least Squares)**\n\nThe empirical risk for the squared loss is:\n$$R_{\\mathrm{sq}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} (y_i - (\\beta_0 + \\beta_1 x_i))^2 = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$$\nMinimizing $R_{\\mathrm{sq}}(\\boldsymbol{\\beta})$ is equivalent to minimizing the squared Euclidean norm of the residual vector, $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$. This is the classic Ordinary Least Squares (OLS) problem. The first-order optimality condition is found by setting the gradient with respect to $\\boldsymbol{\\beta}$ to zero:\n$$\\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{1}{2n} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right) = -\\frac{1}{n} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}$$\nThis simplifies to the well-known normal equations:\n$$\\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}$$\nThe solution for the optimal parameters $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$ is given by:\n$$\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\nFor numerical stability, direct computation of the inverse $(\\mathbf{X}^T \\mathbf{X})^{-1}$ is avoided. Instead, one can solve the linear system of the normal equations. A more robust approach, which is standard in numerical libraries, is to use a matrix decomposition of $\\mathbf{X}$, such as a QR or SVD decomposition, to solve the least-squares problem. For instance, if $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$, the problem reduces to solving the upper-triangular system $\\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{Q}^T\\mathbf{y}$ via back-substitution.\n\n**2. Minimization under Huber Loss (Robust Regression)**\n\nThe empirical risk for the Huber loss is:\n$$R_{\\mathrm{Huber}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n L_\\delta(y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})$$\nwhere $\\mathbf{x}_i^T$ is the $i$-th row of $\\mathbf{X}$. Since $L_\\delta$ is a convex function, $R_{\\mathrm{Huber}}$ is also convex, and a global minimum exists. The first-order optimality condition is $\\nabla_{\\boldsymbol{\\beta}} R_{\\mathrm{Huber}}(\\boldsymbol{\\beta}) = \\mathbf{0}$. The derivative of $L_\\delta(r)$ is the Huber influence function, $\\psi_\\delta(r) = L'_\\delta(r)$:\n$$\n\\psi_\\delta(r) = \\mathrm{clip}(r, -\\delta, \\delta) =\n\\begin{cases}\nr,  \\text{if } |r| \\le \\delta \\\\\n\\delta \\cdot \\mathrm{sgn}(r),  \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThe gradient of the risk is:\n$$\\nabla_{\\boldsymbol{\\beta}} R_{\\mathrm{Huber}} = \\frac{1}{n} \\sum_{i=1}^n - \\psi_\\delta(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta}) \\mathbf{x}_i = \\mathbf{0}$$\nThis gives the system of non-linear equations $\\sum_{i=1}^n \\psi_\\delta(r_i) \\mathbf{x}_i = \\mathbf{0}$. To solve this, we use the Iteratively Reweighted Least Squares (IRLS) algorithm. We define a weight function $w(r) = \\psi_\\delta(r)/r$:\n$$\nw(r) =\n\\begin{cases}\n1,  \\text{if } |r| \\le \\delta \\\\\n\\delta/|r|,  \\text{if } |r| > \\delta\n\\end{cases}\n$$\nUsing this weight, the condition becomes $\\sum_{i=1}^n w(r_i) r_i \\mathbf{x}_i = \\mathbf{0}$, which can be written as $\\sum_{i=1}^n w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta}) \\mathbf{x}_i = \\mathbf{0}$, where $w_i = w(r_i)$. This rearranges to a weighted least squares form:\n$$(\\mathbf{X}^T \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}) \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{y}$$\nHere, $\\mathbf{W}(\\boldsymbol{\\beta})$ is a diagonal matrix of weights $w_i$, which themselves depend on $\\boldsymbol{\\beta}$ through the residuals $r_i$. This dependency suggests an iterative solution:\n\n**IRLS Algorithm:**\n1.  **Initialize**: Choose an initial parameter estimate $\\boldsymbol{\\beta}^{(0)}$. The OLS solution $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$ is a suitable starting point.\n2.  **Iterate** for $k=0, 1, 2, \\dots$ until convergence:\n    a.  **Compute Residuals**: $r_i^{(k)} = y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta}^{(k)}$.\n    b.  **Compute Weights**: $w_i^{(k+1)} = w(r_i^{(k)})$. Form the diagonal weight matrix $\\mathbf{W}^{(k+1)}$.\n    c.  **Solve Weighted Least Squares**: Update the parameter estimate by solving the weighted least squares problem:\n        $$\\boldsymbol{\\beta}^{(k+1)} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n w_i^{(k+1)} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2$$\n        The solution is given by solving the linear system:\n        $$(\\mathbf{X}^T \\mathbf{W}^{(k+1)} \\mathbf{X}) \\boldsymbol{\\beta}^{(k+1)} = \\mathbf{X}^T \\mathbf{W}^{(k+1)} \\mathbf{y}$$\n3.  **Check Convergence**: The process is stopped when the change in the parameter vector, e.g., $\\|\\boldsymbol{\\beta}^{(k+1)} - \\boldsymbol{\\beta}^{(k)}\\|_2$, falls below a predefined tolerance.\n\n**3. Conceptual Explanation: Robustness and the Role of $\\delta$**\n\nThe key difference between squared loss and Huber loss lies in their treatment of large residuals (outliers).\n- **Squared Loss**: The contribution of a data point $i$ to the gradient of the loss is proportional to its residual, $r_i$. If an outlier causes a large residual, it exerts a correspondingly large \"pull\" on the fitted model. This makes OLS regression highly sensitive to outliers. In the IRLS framework, this is equivalent to setting all weights $w_i=1$, irrespective of the residual magnitude.\n\n- **Huber Loss**: The influence of a residual is bounded. For residuals $|r_i| > \\delta$, the influence is capped at a constant value, $\\pm\\delta$. In the IRLS algorithm, this is reflected by the weights. For an inlier with $|r_i| \\le \\delta$, the weight is $w_i=1$, as in OLS. For an outlier with $|r_i| > \\delta$, the weight becomes $w_i = \\delta/|r_i|  1$. This weight decreases as the residual magnitude increases, effectively down-weighting the influence of outliers on the parameter estimation.\n\nThe parameter $\\delta$ controls the trade-off between statistical efficiency and robustness:\n- **Large $\\delta$ (e.g., $\\delta=10^6$)**: The threshold $\\delta$ is so large that all residuals in the dataset will satisfy $|r_i| \\le \\delta$. In this case, the Huber loss $L_\\delta(r)$ becomes identical to the squared loss $\\frac{1}{2}r^2$ for all data points. Consequently, all weights $w_i$ in the IRLS algorithm will be $1$, and the Huber minimizer $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{Huber}}(\\delta)$ will converge to the OLS minimizer $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$.\n- **Small $\\delta$ (e.g., $\\delta=0.5$)**: The threshold is very low. Most residuals, except the smallest ones, will satisfy $|r_i| > \\delta$. For these points, the loss is effectively linear ($|r|$), not quadratic ($r^2$). The resulting fit is highly robust, as the influence of large outliers is severely curtailed. The behavior approaches that of Least Absolute Deviations (L1) regression.\n- **Intermediate $\\delta$**: A moderately chosen $\\delta$ provides a balance. It allows the fit to benefit from the high efficiency of squared loss for the bulk of the data (assumed to be \"inliers\") while simultaneously protecting the fit from being distorted by a few \"outliers\" with large residuals. The value of $\\delta$ essentially defines what one considers an outlier.\n\nIn summary, by varying $\\delta$, one can smoothly interpolate between the non-robust but efficient OLS regression and a highly robust regression that is less sensitive to extreme data points.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS and Huber regression parameters for a given dataset.\n    \"\"\"\n    # Define the dataset from the problem statement\n    data = np.array([\n        [0.0, 1.1],\n        [1.0, 2.9],\n        [2.0, 5.2],\n        [3.0, 7.0],\n        [4.0, 8.9],\n        [5.0, 11.2],\n        [2.5, 20.0],\n        [4.0, -5.0],\n        [10.0, 35.0]\n    ])\n\n    x_data = data[:, 0]\n    y_data = data[:, 1]\n    \n    # Construct the design matrix X with an intercept term\n    X = np.vstack([np.ones(len(x_data)), x_data]).T\n\n    # Test suite specification\n    delta_values = [0.5, 1.0, 3.0, 10**6]\n    \n    # List to store all computed beta values\n    all_beta_values = []\n\n    # --- Part 1: Squared Loss Minimization (OLS) ---\n    # Use numpy.linalg.lstsq for a numerically stable solution\n    beta_sq, _, _, _ = np.linalg.lstsq(X, y_data, rcond=None)\n    all_beta_values.extend(beta_sq)\n\n    # --- Part 2: Huber Loss Minimization (IRLS) ---\n    \n    def solve_huber_irls(X_mat, y_vec, delta, tol=1e-8, max_iter=100):\n        \"\"\"\n        Solves for Huber regression parameters using Iteratively Reweighted Least Squares (IRLS).\n        \n        Args:\n            X_mat (np.ndarray): Design matrix.\n            y_vec (np.ndarray): Response vector.\n            delta (float): Huber loss threshold.\n            tol (float): Convergence tolerance.\n            max_iter (int): Maximum number of iterations.\n            \n        Returns:\n            np.ndarray: The estimated parameter vector beta.\n        \"\"\"\n        # Initialize beta with the OLS solution\n        beta, _, _, _ = np.linalg.lstsq(X_mat, y_vec, rcond=None)\n        \n        for _ in range(max_iter):\n            beta_old = beta.copy()\n            \n            # 1. Compute residuals\n            residuals = y_vec - X_mat @ beta\n            abs_residuals = np.abs(residuals)\n            \n            # 2. Compute weights\n            # Start with weights of 1 for all points\n            weights = np.ones_like(y_vec)\n            # Identify outliers (where |r| > delta)\n            outlier_idx = abs_residuals > delta\n            # Update weights for outliers. This is safe from division by zero\n            # because abs_residuals[outlier_idx] > delta > 0.\n            weights[outlier_idx] = delta / abs_residuals[outlier_idx]\n\n            # 3. Solve weighted least squares problem\n            # This is done by transforming the system and using a standard lstsq solver\n            # sqrt_W = np.sqrt(np.diag(weights))\n            # X_w = sqrt_W @ X\n            # y_w = sqrt_W @ y\n            # A more efficient way without forming the diagonal matrix:\n            sqrt_w_vec = np.sqrt(weights)\n            X_w = X_mat * sqrt_w_vec[:, np.newaxis]\n            y_w = y_vec * sqrt_w_vec\n            \n            beta, _, _, _ = np.linalg.lstsq(X_w, y_w, rcond=None)\n            \n            # 4. Check for convergence\n            if np.linalg.norm(beta - beta_old)  tol:\n                break\n                \n        return beta\n\n    # Loop through each delta value and compute the Huber regression parameters\n    for delta in delta_values:\n        beta_huber = solve_huber_irls(X, y_data, delta)\n        all_beta_values.extend(beta_huber)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{val:.8f}' for val in all_beta_values)}]\")\n\nsolve()\n```", "id": "3153932"}, {"introduction": "In classification, is a model with 90% accuracy always better than one with 85%? Not necessarily. The quality of a probabilistic classifier depends not just on its accuracy (discrimination) but also on how well its predicted probabilities match the true frequencies (calibration). This exercise [@problem_id:3153985] presents a carefully constructed scenario to highlight this distinction. By comparing two classifiers with identical accuracy, you will see firsthand why a nuanced metric like cross-entropy is essential for evaluating model quality and why it prefers well-calibrated predictions over overconfident ones.", "problem": "Consider binary classification with labels $y \\in \\{0,1\\}$ and predicted class probabilities $p \\in [0,1]$. The $0$-$1$ error uses the decision rule $\\hat{y} = \\mathbf{1}\\{p \\ge 0.5\\}$ and is defined as the fraction of incorrect predictions. The cross-entropy (also called log-loss) for a dataset $\\{(y_i, p_i)\\}_{i=1}^n$ is defined as $\\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\ln p_i - (1-y_i)\\ln(1-p_i)\\right]$, where $\\ln$ denotes the natural logarithm.\n\nYou are given $n=10$ independent examples grouped as follows:\n- Group $\\mathcal{G}_1$: indices $\\{1,2,3,4,5\\}$ with true labels $(1,1,1,1,0)$.\n- Group $\\mathcal{G}_2$: indices $\\{6,7,8,9,10\\}$ with true labels $(0,0,0,0,1)$.\n\nTwo probabilistic classifiers output the following scores:\n- Classifier $\\mathcal{C}_1$: for every $i \\in \\mathcal{G}_1$, $p_i=0.8$; for every $i \\in \\mathcal{G}_2$, $p_i=0.2$.\n- Classifier $\\mathcal{C}_2$: for every $i \\in \\mathcal{G}_1$, $p_i=0.99$; for every $i \\in \\mathcal{G}_2$, $p_i=0.01$.\n\nUsing the $0.5$ threshold for $0$-$1$ decisions, compute the $0$-$1$ error of $\\mathcal{C}_1$ and $\\mathcal{C}_2$ and their cross-entropy values on this dataset. Then, contrast calibration and discrimination by deciding which statements below are correct and justify your reasoning from first principles of loss minimization.\n\nA. The two classifiers have the same $0$-$1$ error, but $\\mathcal{C}_1$ has strictly lower cross-entropy than $\\mathcal{C}_2$ on this dataset because its probability predictions are calibrated within the groups, which minimizes expected log-loss in each group.\n\nB. Both classifiers have the same cross-entropy on this dataset, since their hard decisions are identical.\n\nC. Making probabilities more extreme (closer to $0$ or $1$) always reduces cross-entropy when the $0$-$1$ error is held fixed.\n\nD. A classifier with lower cross-entropy must also have lower $0$-$1$ error on every dataset.\n\nE. On this dataset, $\\mathcal{C}_1$ is empirically perfectly calibrated at the two probability levels it uses: among examples where it predicts $0.8$, the positive rate equals $0.8$, and among examples where it predicts $0.2$, the positive rate equals $0.2$.\n\nSelect all that apply.", "solution": "The problem statement is scientifically sound, self-contained, and well-posed. All terms are defined according to standard conventions in statistical learning. I will proceed with the derivation and analysis.\n\nThe core of this problem requires computing two performance metrics—the $0$-$1$ error and the cross-entropy loss—for two classifiers, $\\mathcal{C}_1$ and $\\mathcal{C}_2$, on a given dataset, and then evaluating several statements about the results.\n\nThe dataset consists of $n=10$ examples, with true labels $y_i \\in \\{0, 1\\}$.\n- Group $\\mathcal{G}_1 = \\{1, ..., 5\\}$: True labels $(y_1, y_2, y_3, y_4, y_5) = (1, 1, 1, 1, 0)$.\n- Group $\\mathcal{G}_2 = \\{6, ..., 10\\}$: True labels $(y_6, y_7, y_8, y_9, y_{10}) = (0, 0, 0, 0, 1)$.\n\nThe decision rule for the $0$-$1$ error is $\\hat{y} = \\mathbf{1}\\{p \\ge 0.5\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The $0$-$1$ error is $\\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i \\neq y_i\\}$.\nThe cross-entropy loss is $L_{CE} = \\frac{1}{n} \\sum_{i=1}^n [-y_i \\ln p_i - (1-y_i) \\ln(1-p_i)]$.\n\n**Analysis of Classifier $\\mathcal{C}_1$**\n\n- For $i \\in \\mathcal{G}_1$, $\\mathcal{C}_1$ predicts $p_i = 0.8$. Since $0.8 \\ge 0.5$, the predicted label is $\\hat{y}_i=1$.\n- For $i \\in \\mathcal{G}_2$, $\\mathcal{C}_1$ predicts $p_i = 0.2$. Since $0.2  0.5$, the predicted label is $\\hat{y}_i=0$.\n\n**$0$-$1$ Error of $\\mathcal{C}_1$**:\n- For $\\mathcal{G}_1$: True labels are $(1,1,1,1,0)$, predicted labels are $(1,1,1,1,1)$. There is one error for sample $i=5$ ($y_5=0, \\hat{y}_5=1$).\n- For $\\mathcal{G}_2$: True labels are $(0,0,0,0,1)$, predicted labels are $(0,0,0,0,0)$. There is one error for sample $i=10$ ($y_{10}=1, \\hat{y}_{10}=0$).\n- Total errors = $1+1=2$. The $0$-$1$ error is $2/10 = 0.2$.\n\n**Cross-Entropy of $\\mathcal{C}_1$**:\nThe total loss is the sum of per-sample losses.\n- For $\\mathcal{G}_1$: We have $4$ samples with $(y=1, p=0.8)$ and $1$ sample with $(y=0, p=0.8)$. The sum of losses is $4(-\\ln(0.8)) + 1(-\\ln(1-0.8)) = -4\\ln(0.8) - \\ln(0.2)$.\n- For $\\mathcal{G}_2$: We have $1$ sample with $(y=1, p=0.2)$ and $4$ samples with $(y=0, p=0.2)$. The sum of losses is $1(-\\ln(0.2)) + 4(-\\ln(1-0.2)) = -\\ln(0.2) - 4\\ln(0.8)$.\nThe total cross-entropy is:\n$$L_{CE}(\\mathcal{C}_1) = \\frac{1}{10} [(-4\\ln(0.8) - \\ln(0.2)) + (-\\ln(0.2) - 4\\ln(0.8))] = \\frac{1}{10} [-8\\ln(0.8) - 2\\ln(0.2)]$$\nUsing $\\ln(0.8) \\approx -0.2231$ and $\\ln(0.2) \\approx -1.6094$:\n$$L_{CE}(\\mathcal{C}_1) \\approx \\frac{1}{10} [-8(-0.2231) - 2(-1.6094)] = \\frac{1}{10} [1.7848 + 3.2188] = 0.50036$$\n\n**Analysis of Classifier $\\mathcal{C}_2$**\n\n- For $i \\in \\mathcal{G}_1$, $\\mathcal{C}_2$ predicts $p_i = 0.99$. Since $0.99 \\ge 0.5$, $\\hat{y}_i=1$.\n- For $i \\in \\mathcal{G}_2$, $\\mathcal{C}_2$ predicts $p_i = 0.01$. Since $0.01  0.5$, $\\hat{y}_i=0$.\n\n**$0$-$1$ Error of $\\mathcal{C}_2$**:\nThe predicted labels $\\hat{y}_i$ for $\\mathcal{C}_2$ are identical to those of $\\mathcal{C}_1$. Therefore, the number of errors is also identical.\n- Total errors = $2$. The $0$-$1$ error is $2/10 = 0.2$.\n\n**Cross-Entropy of $\\mathcal{C}_2$**:\n- For $\\mathcal{G}_1$: We have $4$ samples with $(y=1, p=0.99)$ and $1$ sample with $(y=0, p=0.99)$. The sum of losses is $4(-\\ln(0.99)) + 1(-\\ln(1-0.99)) = -4\\ln(0.99) - \\ln(0.01)$.\n- For $\\mathcal{G}_2$: We have $1$ sample with $(y=1, p=0.01)$ and $4$ samples with $(y=0, p=0.01)$. The sum of losses is $1(-\\ln(0.01)) + 4(-\\ln(1-0.01)) = -\\ln(0.01) - 4\\ln(0.99)$.\nThe total cross-entropy is:\n$$L_{CE}(\\mathcal{C}_2) = \\frac{1}{10} [(-4\\ln(0.99) - \\ln(0.01)) + (-\\ln(0.01) - 4\\ln(0.99))] = \\frac{1}{10} [-8\\ln(0.99) - 2\\ln(0.01)]$$\nUsing $\\ln(0.99) \\approx -0.01005$ and $\\ln(0.01) \\approx -4.6052$:\n$$L_{CE}(\\mathcal{C}_2) \\approx \\frac{1}{10} [-8(-0.01005) - 2(-4.6052)] = \\frac{1}{10} [0.0804 + 9.2104] = 0.92908$$\n\n**Summary of Results**:\n- $0$-$1$ Error($\\mathcal{C}_1$) = $0.2$\n- $0$-$1$ Error($\\mathcal{C}_2$) = $0.2$\n- $L_{CE}(\\mathcal{C}_1) \\approx 0.50$\n- $L_{CE}(\\mathcal{C}_2) \\approx 0.93$\n\nBoth classifiers have the same discrimination performance (measured by $0$-$1$ error), but $\\mathcal{C}_1$ has a significantly lower cross-entropy loss. We now evaluate each statement.\n\n**Option-by-option analysis:**\n\n**A. The two classifiers have the same $0$-$1$ error, but $\\mathcal{C}_1$ has strictly lower cross-entropy than $\\mathcal{C}_2$ on this dataset because its probability predictions are calibrated within the groups, which minimizes expected log-loss in each group.**\n- **Part 1:** \"The two classifiers have the same $0$-$1$ error\". Our calculation shows this is true ($0.2$ for both).\n- **Part 2:** \"but $\\mathcal{C}_1$ has strictly lower cross-entropy than $\\mathcal{C}_2$\". Our calculation shows this is true ($L_{CE}(\\mathcal{C}_1) \\approx 0.50  L_{CE}(\\mathcal{C}_2) \\approx 0.93$).\n- **Part 3 (Reasoning):** \"because its probability predictions are calibrated within the groups, which minimizes expected log-loss in each group\". Cross-entropy is a proper scoring rule. For a set of events, the average cross-entropy is minimized when the predicted probability equals the empirical frequency of the positive class.\n  - In group $\\mathcal{G}_1$, the empirical frequency of the positive class ($y=1$) is $4/5 = 0.8$. $\\mathcal{C}_1$ predicts $p=0.8$ for this group.\n  - In group $\\mathcal{G}_2$, the empirical frequency of the positive class ($y=1$) is $1/5 = 0.2$. $\\mathcal{C}_1$ predicts $p=0.2$ for this group.\n  Since the predictions of $\\mathcal{C}_1$ match the empirical frequencies in each group, it is perfectly calibrated with respect to these groups. This calibration is precisely what minimizes the cross-entropy loss for any classifier that assigns a single probability value to each group. $\\mathcal{C}_2$ uses probabilities ($0.99$ and $0.01$) that do not match the empirical frequencies, resulting in a higher loss. The reasoning is sound.\n- **Verdict:** Correct.\n\n**B. Both classifiers have the same cross-entropy on this dataset, since their hard decisions are identical.**\n- The premise, \"Both classifiers have the same cross-entropy\", is factually incorrect, as demonstrated by our calculations ($0.50 \\neq 0.93$). The reasoning provided is also flawed. Cross-entropy is sensitive to the magnitude of the predicted probability $p$, not just whether $p \\ge 0.5$. It heavily penalizes overconfident incorrect predictions (e.g., predicting $p=0.01$ for a true label $y=1$), which is precisely why $L_{CE}(\\mathcal{C}_2) > L_{CE}(\\mathcal{C}_1)$.\n- **Verdict:** Incorrect.\n\n**C. Making probabilities more extreme (closer to $0$ or $1$) always reduces cross-entropy when the $0$-$1$ error is held fixed.**\n- This statement is a generalization that is falsified by the very example in the problem. $\\mathcal{C}_2$ uses more extreme probabilities ($0.99$ and $0.01$) than $\\mathcal{C}_1$ ($0.8$ and $0.2$). The $0$-$1$ error is held fixed at $0.2$. However, the cross-entropy of $\\mathcal{C}_2$ is higher, not lower, than that of $\\mathcal{C}_1$. While making predictions more extreme reduces the loss for correctly classified examples (e.g., $-\\ln(0.99)  -\\ln(0.8)$), it substantially increases the loss for incorrectly classified examples (e.g., $-\\ln(1-0.99) > -\\ln(1-0.8)$). The net effect depends on the balance, and it is not always a reduction.\n- **Verdict:** Incorrect.\n\n**D. A classifier with lower cross-entropy must also have lower $0$-$1$ error on every dataset.**\n- This statement posits a strict monotonic relationship between the two metrics that does not hold in general. Our analysis of $\\mathcal{C}_1$ and $\\mathcal{C}_2$ provides a direct counterexample. $\\mathcal{C}_1$ has a lower cross-entropy than $\\mathcal{C}_2$, but its $0$-$1$ error is equal to, not strictly lower than, that of $\\mathcal{C}_2$. Thus, optimizing for cross-entropy does not guarantee a strictly better $0$-$1$ error.\n- **Verdict:** Incorrect.\n\n**E. On this dataset, $\\mathcal{C}_1$ is empirically perfectly calibrated at the two probability levels it uses: among examples where it predicts $0.8$, the positive rate equals $0.8$, and among examples where it predicts $0.2$, the positive rate equals $0.2$.**\n- This is a factual statement about classifier $\\mathcal{C}_1$ that we can verify directly.\n  - The set of examples where $\\mathcal{C}_1$ predicts $p=0.8$ is group $\\mathcal{G}_1$. This group has $5$ examples with labels $(1,1,1,1,0)$. The proportion of positive labels (the positive rate) is $4/5 = 0.8$. This matches the prediction.\n  - The set of examples where $\\mathcal{C}_1$ predicts $p=0.2$ is group $\\mathcal{G}_2$. This group has $5$ examples with labels $(0,0,0,0,1)$. The proportion of positive labels is $1/5 = 0.2$. This also matches the prediction.\n- The statement is a precise and accurate description of $\\mathcal{C}_1$'s calibration on this dataset.\n- **Verdict:** Correct.\n\nBased on the analysis, statements A and E are correct.", "answer": "$$\\boxed{AE}$$", "id": "3153985"}, {"introduction": "While convex penalties like the LASSO are popular for their computational simplicity, they can introduce bias in coefficient estimates. Nonconvex penalties such as SCAD and MCP were designed to overcome this, offering improved statistical properties like sparsity and unbiasedness. The trade-off is that the optimization landscape becomes nonconvex, meaning algorithms can get stuck in local minima. This practice [@problem_id:3153982] challenges you to implement a coordinate descent algorithm to solve these problems and directly investigate the consequences of nonconvexity, including the crucial role of initialization in finding a good solution.", "problem": "Consider the penalized regression problem in statistical learning where the goal is to minimize an empirical risk composed of a mean squared error data fidelity term and a nonconvex regularizer. Let $X \\in \\mathbb{R}^{n \\times p}$ denote the design matrix, $y \\in \\mathbb{R}^n$ denote the response vector, and $b \\in \\mathbb{R}^p$ denote the coefficient vector. The empirical risk to be minimized is\n$$\n\\mathcal{L}(b) \\triangleq \\frac{1}{2n}\\lVert y - X b \\rVert_2^2 + \\sum_{j=1}^p \\mathcal{P}(b_j;\\lambda,\\theta),\n$$\nwhere the regularizer $\\mathcal{P}$ is either the Smoothly Clipped Absolute Deviation (SCAD) penalty with tuning parameter $\\lambda > 0$ and shape parameter $a > 2$, or the Minimax Concave Penalty (MCP) with tuning parameter $\\lambda > 0$ and shape parameter $\\gamma > 1$. Both SCAD and MCP are nonconvex penalties that encourage sparsity while reducing bias relative to convex penalties.\n\nStarting from fundamental definitions, implement a coordinate descent algorithm that cyclically minimizes $\\mathcal{L}(b)$ with respect to one coordinate $b_j$ at a time while keeping all other coordinates fixed. The algorithm must:\n- Standardize each feature column $X_{\\cdot j}$ to have zero mean and satisfy $\\lVert X_{\\cdot j} \\rVert_2^2 / n = 1$, and center $y$ to zero mean, so that single-coordinate updates are well-scaled.\n- Use two different initializations for $b$: the zero vector and the ordinary least squares solution obtained via the Moore-Penrose pseudoinverse.\n- For each coordinate update, derive and implement the exact closed-form coordinate-wise minimizer implied by the first-order optimality conditions of the one-dimensional subproblem under SCAD or MCP. Do not rely on generic black-box optimization routines; instead, use the explicit piecewise thresholding maps that arise from the stationarity equations and subgradient conditions.\n\nThe program must compute, for each test case specified below, the following quantities:\n1. The Euclidean norm difference between the coefficient vectors obtained from zero initialization and least squares initialization, namely $\\lVert b^{(0)} - b^{(\\mathrm{LS})} \\rVert_2$.\n2. The absolute difference between the minimized objective values under the two initializations, namely $\\left|\\mathcal{L}\\big(b^{(0)}\\big) - \\mathcal{L}\\big(b^{(\\mathrm{LS})}\\big)\\right|$.\n\nYour implementation must use deterministic data generation with fixed random seeds as indicated. The coordinate descent should terminate when the maximum absolute change in any coordinate is below a tolerance or when a maximum number of iterations is reached. Use the following test suite to explore different regimes, including the \"happy path,\" strong collinearity that can induce multiple local minima, and boundary conditions:\n\n- Test Case 1 (SCAD, orthogonal design, happy path):\n  - Set $n=50$, $p=3$. Generate $X$ by taking the first $p$ columns of a random orthonormal matrix in $\\mathbb{R}^{n \\times n}$ obtained via a $\\mathrm{QR}$ decomposition with random seed $0$, and then scale columns so that $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$. Let $b^\\star = [2.5, -1.0, 0.0]^\\top$. Generate $y = X b^\\star$ (no noise), then center $y$ and standardize $X$ as specified. Use SCAD with $\\lambda = 0.5$ and $a = 3.7$. Run coordinate descent with both initializations and compute the two requested quantities.\n- Test Case 2 (SCAD, perfect collinearity, local minima sensitivity):\n  - Set $n=60$, $p=2$. With random seed $1$, generate a vector $u \\in \\mathbb{R}^n$ with independent standard normal entries. Let $X = [u, u]$ (two identical columns), then center and scale to satisfy $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$. Generate $y = 3 u$, then center $y$. Use SCAD with $\\lambda = 1.5$ and $a = 3.7$. Run coordinate descent with both initializations and compute the two requested quantities.\n- Test Case 3 (MCP, boundary case $\\lambda = 0$):\n  - Set $n=80$, $p=4$. With random seed $2$, generate $X$ with independent standard normal entries, then center and scale columns to satisfy $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$. Let $b^\\star = [1.0, -2.0, 0.0, 0.5]^\\top$. Generate $y = X b^\\star$ (no noise), then center $y$. Use MCP with $\\lambda = 0$ and $\\gamma = 3.0$. Run coordinate descent with both initializations and compute the two requested quantities.\n- Test Case 4 (MCP, perfect collinearity, strong nonconvexity):\n  - Set $n=60$, $p=2$. With random seed $3$, generate $u \\in \\mathbb{R}^n$ with independent standard normal entries. Let $X = [u, u]$ (two identical columns), then center and scale to satisfy $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$. Generate $y = 2 u$, then center $y$. Use MCP with $\\lambda = 1.2$ and $\\gamma = 1.5$. Run coordinate descent with both initializations and compute the two requested quantities.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of two floating-point numbers $[\\lVert b^{(0)} - b^{(\\mathrm{LS})} \\rVert_2, \\left|\\mathcal{L}\\big(b^{(0)}\\big) - \\mathcal{L}\\big(b^{(\\mathrm{LS})}\\big)\\right|]$. For example, the output must look like:\n$$\n\\texttt{[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4]]}\n$$\nwith $x_i$ and $y_i$ expressed as decimal floating-point numbers.", "solution": "The problem requires the implementation of a coordinate descent algorithm to solve a penalized regression problem with a nonconvex regularizer, specifically the Smoothly Clipped Absolute Deviation (SCAD) or Minimax Concave Penalty (MCP). The solution must be derived from first principles, including the data standardization and the coordinate-wise update rules.\n\nThe objective function to minimize is the empirical risk $\\mathcal{L}(b)$:\n$$\n\\mathcal{L}(b) = \\frac{1}{2n}\\lVert y - X b \\rVert_2^2 + \\sum_{j=1}^p \\mathcal{P}(b_j;\\lambda,\\theta)\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $y \\in \\mathbb{R}^n$ is the response vector, $b \\in \\mathbb{R}^p$ is the coefficient vector, and $\\mathcal{P}$ is the penalty function with tuning parameters $\\lambda$ and a shape parameter $\\theta$ (which is $a$ for SCAD and $\\gamma$ for MCP).\n\nFirst, as per the problem statement, the data must be standardized. The response vector $y$ is centered to have a mean of zero. Each column $j$ of the design matrix, $X_{\\cdot j}$, is centered to have a mean of zero and then scaled such that its squared Euclidean norm divided by $n$ is equal to $1$, i.e., $\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2 = 1$. This standardization ensures that the penalty is applied fairly across all coefficients and simplifies the derivation of the coordinate update.\n\nThe core of the algorithm is coordinate descent, which iteratively minimizes the objective function with respect to a single coefficient $b_j$ while holding all other coefficients $b_k$ ($k \\neq j$) fixed. The one-dimensional subproblem for updating $b_j$ is:\n$$\n\\arg\\min_{b_j} \\mathcal{L}(b_1, \\dots, b_j, \\dots, b_p)\n$$\nLet's isolate the terms depending on $b_j$. The residual can be written as $y - Xb = (y - \\sum_{k\\neq j} X_{\\cdot k} b_k) - X_{\\cdot j} b_j$. Let $r_j = y - \\sum_{k\\neq j} X_{\\cdot k} b_k$ be the partial residual. The subproblem becomes:\n$$\n\\arg\\min_{b_j} \\left\\{ \\frac{1}{2n} \\lVert r_j - X_{\\cdot j} b_j \\rVert_2^2 + \\mathcal{P}(b_j) \\right\\}\n$$\nExpanding the squared norm and using the standardization $\\lVert X_{\\cdot j} \\rVert_2^2 = n$, we get:\n$$\n\\arg\\min_{b_j} \\left\\{ \\frac{1}{2n} (\\lVert r_j \\rVert_2^2 - 2b_j X_{\\cdot j}^\\top r_j + b_j^2 n) + \\mathcal{P}(b_j) \\right\\}\n$$\nIgnoring constant terms and simplifying, this is equivalent to:\n$$\n\\arg\\min_{b_j} \\left\\{ \\frac{1}{2} b_j^2 - \\frac{1}{n}(X_{\\cdot j}^\\top r_j)b_j + \\mathcal{P}(b_j) \\right\\}\n$$\nLet $z_j = \\frac{1}{n} X_{\\cdot j}^\\top r_j$. By completing the square, the subproblem is equivalent to finding the value of $b_j$ that minimizes $\\frac{1}{2}(b_j - z_j)^2 + \\mathcal{P}(b_j)$. This is a standard proximal operator problem. For efficient computation, $z_j$ is calculated as the current coefficient $b_j$ plus the scaled inner product of the $j$-th feature vector with the current full residual: $z_j = b_j + \\frac{1}{n} X_{\\cdot j}^\\top (y - X b)$.\n\nThe solution to this one-dimensional minimization is given by a thresholding function that depends on the penalty $\\mathcal{P}$. We derive the closed-form minimizers for SCAD and MCP.\n\n**SCAD Thresholding:**\nThe SCAD penalty is defined by its derivative. To find the minimizer, we solve the first-order condition $b_j - z_j + \\mathcal{P}'(b_j) = 0$. For the nonconvex SCAD penalty, this can have multiple solutions. The standard coordinate descent algorithm uses the following well-established thresholding function, which selects the appropriate minimizer:\n$$\nb_j^{\\text{new}} = S_{\\text{SCAD}}(z_j; \\lambda, a) = \\begin{cases}\n\\text{sgn}(z_j) \\max(0, |z_j|-\\lambda)  \\text{if } |z_j| \\le 2\\lambda \\\\\n\\frac{(a-1)z_j - \\text{sgn}(z_j)a\\lambda}{a-2}  \\text{if } 2\\lambda  |z_j| \\le a\\lambda \\\\\nz_j  \\text{if } |z_j| > a\\lambda\n\\end{cases}\n$$\nwhere $a > 2$ is the shape parameter.\n\n**MCP Thresholding:**\nSimilarly, for the MCP, the thresholding operator is derived from its first-order condition and is given by:\n$$\nb_j^{\\text{new}} = S_{\\text{MCP}}(z_j; \\lambda, \\gamma) = \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\text{sgn}(z_j) \\frac{\\gamma(|z_j| - \\lambda)}{\\gamma - 1}  \\text{if } \\lambda  |z_j| \\le \\gamma\\lambda \\\\\nz_j  \\text{if } |z_j| > \\gamma\\lambda\n\\end{cases}\n$$\nwhere $\\gamma > 1$ is the shape parameter.\n\nThe algorithm proceeds as follows:\n1.  Generate data $(X, y)$ for a given test case.\n2.  Standardize $X$ and center $y$.\n3.  Initialize two coefficient vectors: $b^{(0)}$ as the zero vector, and $b^{(\\mathrm{LS})}$ as the ordinary least squares solution, computed via the Moore-Penrose pseudoinverse of the standardized $X$.\n4.  For each initialization, run the coordinate descent algorithm:\n    a. Cycle through coordinates $j=1, \\dots, p$.\n    b. Compute $z_j$ using the current value of the coefficient vector $b$.\n    c. Update $b_j$ using the appropriate thresholding function ($S_{\\text{SCAD}}$ or $S_{\\text{MCP}}$).\n    d. Repeat until the maximum absolute change in any coordinate is below a tolerance (e.g., $10^{-8}$) or a maximum number of iterations is reached.\n5.  After convergence, compute the final objective function values $\\mathcal{L}(b^{(0)})$ and $\\mathcal{L}(b^{(\\mathrm{LS})})$, where $b^{(0)}$ and $b^{(\\mathrm{LS})}$ denote the converged vectors from the respective initializations.\n6.  Calculate the two required quantities: the Euclidean norm of the difference between the final coefficient vectors, $\\lVert b^{(0)} - b^{(\\mathrm{LS})} \\rVert_2$, and the absolute difference between their objective function values, $\\left|\\mathcal{L}(b^{(0)}) - \\mathcal{L}(b^{(\\mathrm{LS})})\\right|$.\n\nThis procedure is repeated for all four test cases, which are designed to probe the algorithm's behavior under different conditions, including an orthogonal design, perfect collinearity (which can expose multiple local minima), and a boundary case with zero penalty. The nonconvexity of SCAD and MCP means that the final solution can be sensitive to the starting point, a phenomenon explicitly tested by comparing the results from zero and OLS initializations.", "answer": "```python\nimport numpy as np\n\ndef scad_penalty(t, lambda_, a):\n    \"\"\"Computes the SCAD penalty value.\"\"\"\n    abs_t = np.abs(t)\n    val = 0.0\n    if abs_t = lambda_:\n        val = lambda_ * abs_t\n    elif abs_t = a * lambda_:\n        val = -(abs_t**2 - 2 * a * lambda_ * abs_t + lambda_**2) / (2 * (a - 1))\n    else:\n        val = (a + 1) * lambda_**2 / 2\n    return val\n\ndef mcp_penalty(t, lambda_, gamma):\n    \"\"\"Computes the MCP penalty value.\"\"\"\n    abs_t = np.abs(t)\n    val = 0.0\n    if abs_t = gamma * lambda_:\n        val = lambda_ * abs_t - abs_t**2 / (2 * gamma)\n    else:\n        val = 0.5 * gamma * lambda_**2\n    return val\n\ndef calculate_loss(X, y, b, penalty_type, lambda_, shape_param):\n    \"\"\"Calculates the objective function value.\"\"\"\n    n = X.shape[0]\n    mse = (0.5 / n) * np.sum((y - X @ b)**2)\n    \n    total_penalty = 0.0\n    if penalty_type == 'scad':\n        a = shape_param\n        if lambda_ > 0:\n            for val in b:\n                total_penalty += scad_penalty(val, lambda_, a)\n    elif penalty_type == 'mcp':\n        gamma = shape_param\n        if lambda_ > 0:\n            for val in b:\n                total_penalty += mcp_penalty(val, lambda_, gamma)\n\n    return mse + total_penalty\n\ndef scad_threshold(z, lambda_, a):\n    \"\"\"Computes the SCAD thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    if abs_z = 2 * lambda_:\n        return np.sign(z) * max(0, abs_z - lambda_)\n    elif abs_z = a * lambda_:\n        return ((a - 1) * z - np.sign(z) * a * lambda_) / (a - 2)\n    else:\n        return z\n\ndef mcp_threshold(z, lambda_, gamma):\n    \"\"\"Computes the MCP thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    if abs_z = lambda_:\n        return 0.0\n    elif abs_z = gamma * lambda_:\n        return np.sign(z) * gamma * (abs_z - lambda_) / (gamma - 1)\n    else:\n        return z\n\ndef coordinate_descent(X, y, b_init, penalty_type, lambda_, shape_param, tol=1e-8, max_iter=1000):\n    \"\"\"Performs coordinate descent for nonconvex penalized regression.\"\"\"\n    n, p = X.shape\n    b = b_init.copy()\n    \n    threshold_func = None\n    if penalty_type == 'scad':\n        threshold_func = lambda z: scad_threshold(z, lambda_, shape_param)\n    elif penalty_type == 'mcp':\n        threshold_func = lambda z: mcp_threshold(z, lambda_, shape_param)\n    else:\n        raise ValueError(\"Unknown penalty type\")\n\n    for _ in range(max_iter):\n        b_old = b.copy()\n        for j in range(p):\n            # Calculate z_j efficiently\n            # z_j = b_old[j] + (1/n) * X_j.T @ (y - X @ b_current)\n            # where b_current is the vector with updates for 0..j-1\n            residual = y - X @ b\n            z_j = b[j] + (X[:, j].T @ residual) / n\n            \n            b[j] = threshold_func(z_j)\n\n        if np.max(np.abs(b - b_old))  tol:\n            break\n            \n    return b\n\ndef run_case(n, p, seed, penalty_type, lambda_, shape_param, b_star, X_gen_type, y_gen_val):\n    \"\"\"Generates data, runs CD, and computes metrics for one test case.\"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate data\n    if X_gen_type == 'ortho':\n        Q, _ = np.linalg.qr(rng.standard_normal(size=(n, n)))\n        X = Q[:, :p]\n    elif X_gen_type == 'collinear':\n        u = rng.standard_normal(size=n)\n        X = np.tile(u, (p, 1)).T\n    else: # 'random'\n        X = rng.standard_normal(size=(n, p))\n    \n    # Generate y based on the original X before standardization\n    if y_gen_val == 'b_star':\n        y = X @ b_star \n    else:\n        u_orig = X[:, 0]\n        y = y_gen_val * u_orig\n    \n    # Standardize X and center y\n    X_mean = X.mean(axis=0)\n    X_centered = X - X_mean\n    X_scales = np.linalg.norm(X_centered, axis=0) / np.sqrt(n)\n    X_scales[X_scales == 0] = 1.0\n    X_std = X_centered / X_scales\n    \n    y_mean = y.mean()\n    y_centered = y - y_mean\n    \n    # Initializations\n    b_init_zero = np.zeros(p)\n    b_init_ls = np.linalg.pinv(X_std) @ y_centered\n\n    # Run coordinate descent from both initializations\n    b_final_zero = coordinate_descent(X_std, y_centered, b_init_zero, penalty_type, lambda_, shape_param)\n    b_final_ls = coordinate_descent(X_std, y_centered, b_init_ls, penalty_type, lambda_, shape_param)\n\n    # Calculate losses\n    loss_zero = calculate_loss(X_std, y_centered, b_final_zero, penalty_type, lambda_, shape_param)\n    loss_ls = calculate_loss(X_std, y_centered, b_final_ls, penalty_type, lambda_, shape_param)\n\n    # Compute final metrics\n    norm_diff = np.linalg.norm(b_final_zero - b_final_ls)\n    loss_diff = np.abs(loss_zero - loss_ls)\n    \n    return [norm_diff, loss_diff]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Test Case 1\n        {'n': 50, 'p': 3, 'seed': 0, 'penalty_type': 'scad', 'lambda_': 0.5, 'shape_param': 3.7,\n         'b_star': np.array([2.5, -1.0, 0.0]), 'X_gen_type': 'ortho', 'y_gen_val': 'b_star'},\n        # Test Case 2\n        {'n': 60, 'p': 2, 'seed': 1, 'penalty_type': 'scad', 'lambda_': 1.5, 'shape_param': 3.7,\n         'b_star': None, 'X_gen_type': 'collinear', 'y_gen_val': 3.0},\n        # Test Case 3\n        {'n': 80, 'p': 4, 'seed': 2, 'penalty_type': 'mcp', 'lambda_': 0.0, 'shape_param': 3.0,\n         'b_star': np.array([1.0, -2.0, 0.0, 0.5]), 'X_gen_type': 'random', 'y_gen_val': 'b_star'},\n        # Test Case 4\n        {'n': 60, 'p': 2, 'seed': 3, 'penalty_type': 'mcp', 'lambda_': 1.2, 'shape_param': 1.5,\n         'b_star': None, 'X_gen_type': 'collinear', 'y_gen_val': 2.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        norm_diff, loss_diff = run_case(**case)\n        results.append(f\"[{norm_diff:.10f},{loss_diff:.10f}]\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3153982"}]}