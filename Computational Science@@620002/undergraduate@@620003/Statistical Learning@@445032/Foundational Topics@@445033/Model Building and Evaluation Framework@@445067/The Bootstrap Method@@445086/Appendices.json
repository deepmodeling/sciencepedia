{"hands_on_practices": [{"introduction": "To truly understand the bootstrap, it is helpful to first perform the calculations by hand. This exercise [@problem_id:851901] strips away computational complexity, allowing you to focus on the fundamental mechanism of resampling. By manually calculating the standard error for a regression slope from a few given bootstrap samples, you will build a concrete intuition for how this powerful method estimates statistical uncertainty.", "problem": "In statistical analysis, the non-parametric pairs bootstrap is a powerful resampling technique used to estimate the uncertainty of a statistic when its analytical distribution is unknown or relies on strong assumptions. This problem explores the application of the bootstrap method to estimate the standard error of a regression coefficient.\n\nConsider a simple linear regression model, $Y = \\beta_0 + \\beta_1 X + \\epsilon$, where the slope parameter $\\beta_1$ is estimated from a set of $n$ data pairs $\\{(x_i, y_i)\\}_{i=1}^n$. The ordinary least squares (OLS) estimator for the slope is given by:\n$$\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means of $X$ and $Y$, respectively.\n\nThe bootstrap procedure to estimate the standard error of $\\hat{\\beta}_1$ is as follows:\n1.  Draw $B$ bootstrap samples, $\\mathcal{D}^*_1, \\mathcal{D}^*_2, ..., \\mathcal{D}^*_B$, each of size $n$, by sampling with replacement from the original data set $\\mathcal{D}$.\n2.  For each bootstrap sample $\\mathcal{D}^*_b$, calculate the slope estimate, denoted as $\\hat{\\beta}_{1,b}^*$.\n3.  The bootstrap estimate of the standard error of $\\hat{\\beta}_1$ is the sample standard deviation of the $B$ bootstrap slope estimates:\n    $$\n    \\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n    $$\n    where $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$ is the mean of the bootstrap estimates.\n\n**Problem:**\nSuppose you have an original dataset of size $n=3$:\n$$\n\\mathcal{D} = \\{ (0,0), (1, \\alpha), (2, \\beta) \\}\n$$\nwhere $\\alpha$ and $\\beta$ are real-valued parameters, and it is given that $\\beta \\neq 2\\alpha$, which ensures the three points are not collinear.\n\nYou perform a bootstrap analysis with a very small number of bootstrap samples, $B=2$. The two resulting bootstrap samples are:\n1.  $\\mathcal{D}^*_1 = \\{ (0,0), (0,0), (1, \\alpha) \\}$\n2.  $\\mathcal{D}^*_2 = \\{ (0,0), (1, \\alpha), (2, \\beta) \\}$ (which is identical to the original sample $\\mathcal{D}$)\n\nDerive a closed-form expression for the bootstrap estimate of the standard error of the slope, $\\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1)$, in terms of the parameters $\\alpha$ and $\\beta$.", "solution": "1. Relevant equations:\n   $\\displaystyle \\hat{\\beta}_1=\\frac{S_{xy}}{S_{xx}},\\quad S_{xy}=\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}),\\quad S_{xx}=\\sum_{i=1}^n (x_i-\\bar{x})^2.$  \n   For $B=2$, the bootstrap standard error is  \n   $$\n   \\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1)\n   =\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^2\\bigl(\\hat{\\beta}_{1,b}^*-\\bar{\\beta}_1^*\\bigr)^2}\n   =\\sqrt{\\sum_{b=1}^2\\bigl(\\hat{\\beta}_{1,b}^*-\\bar{\\beta}_1^*\\bigr)^2}\\,,\n   $$\n   since $B-1=1$.\n\n2. Compute $\\hat{\\beta}_{1,1}^*$ for $\\mathcal{D}_1^*=\\{(0,0),(0,0),(1,\\alpha)\\}$.\n   $$\n   \\bar{x}_1^*=\\frac{0+0+1}{3}=\\frac13,\\quad \\bar{y}_1^*=\\frac{0+0+\\alpha}{3}=\\frac\\alpha3;\n   $$\n   $$\n   S_{xy}^{(1)}=2\\bigl(-\\tfrac13\\bigr)\\bigl(-\\tfrac\\alpha3\\bigr)+\\bigl(\\tfrac23\\bigr)\\bigl(\\tfrac{2\\alpha}3\\bigr)\n   =\\frac{2\\alpha}{9}+\\frac{4\\alpha}{9}=\\frac{2\\alpha}{3},\\quad\n   S_{xx}^{(1)}=2\\!\\bigl(\\tfrac{1}{3}\\bigr)^2+\\bigl(\\tfrac{2}{3}\\bigr)^2=\\frac{2}{3},\n   $$\n   $$\n   \\hat{\\beta}_{1,1}^*=\\frac{S_{xy}^{(1)}}{S_{xx}^{(1)}}=\\frac{\\tfrac{2\\alpha}{3}}{\\tfrac{2}{3}}=\\alpha.\n   $$\n\n3. Compute $\\hat{\\beta}_{1,2}^*$ for the original sample $\\{(0,0),(1,\\alpha),(2,\\beta)\\}$.\n   $$\n   \\bar{x}=1,\\quad \\bar{y}=\\frac{\\alpha+\\beta}{3},\\quad\n   S_{xy}=\\bar{y}+(\\beta-\\bar{y})=\\beta,\\quad S_{xx}=1+0+1=2,\n   $$\n   $$\n   \\hat{\\beta}_{1,2}^*=\\frac{\\beta}{2}.\n   $$\n\n4. Mean of bootstrap slopes:\n   $$\n   \\bar{\\beta}_1^*=\\frac{\\alpha+\\tfrac{\\beta}{2}}{2}=\\frac{2\\alpha+\\beta}{4}.\n   $$\n   Deviations:\n   $$\n   \\hat{\\beta}_{1,1}^*-\\bar{\\beta}_1^*\n   =\\alpha-\\frac{2\\alpha+\\beta}{4}\n   =\\frac{2\\alpha-\\beta}{4},\\quad\n   \\hat{\\beta}_{1,2}^*-\\bar{\\beta}_1^*\n   =\\frac{\\beta}{2}-\\frac{2\\alpha+\\beta}{4}\n   =-\\frac{2\\alpha-\\beta}{4}.\n   $$\n   Sum of squared deviations:\n   $$\n   \\sum_{b=1}^2\\bigl(\\hat{\\beta}_{1,b}^*-\\bar{\\beta}_1^*\\bigr)^2\n   =2\\Bigl(\\frac{2\\alpha-\\beta}{4}\\Bigr)^2\n   =\\frac{(2\\alpha-\\beta)^2}{8}.\n   $$\n\n5. Bootstrap standard error:\n   $$\n   \\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1)\n   =\\sqrt{\\frac{(2\\alpha-\\beta)^2}{8}}\n   =\\frac{|2\\alpha-\\beta|}{2\\sqrt2}.\n   $$", "answer": "$$\\boxed{\\frac{\\lvert\\beta-2\\alpha\\rvert}{2\\sqrt2}}$$", "id": "851901"}, {"introduction": "Beyond estimating standard errors, resampling methods provide powerful tools for data diagnostics. This hands-on coding challenge [@problem_id:3180777] introduces the jackknife-after-bootstrap technique to quantify how much each individual data point influences the results of a bootstrap analysis. By implementing this procedure, you will not only sharpen your computational skills but also learn a sophisticated method for identifying influential observations in a regression context.", "problem": "You are given three independent test cases, each consisting of a fixed dataset of paired observations $\\{(x_i,y_i)\\}_{i=1}^n$. For each test case, you must use the bootstrap method combined with the jackknife-after-bootstrap procedure to quantify the influence of each observation on the bootstrap distribution of a chosen statistic and then identify high-influence points. The chosen statistic is the slope parameter of a simple linear regression fitted by Ordinary Least Squares (OLS), which minimizes the sum of squared residuals.\n\nFundamental base:\n- Ordinary Least Squares (OLS) estimates for simple linear regression minimize $S(b_0,b_1) = \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2$. The first-order optimality conditions imply the estimator $\\hat{b}_1$ satisfies $$ \\hat{b}_1 = \\dfrac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$ where $\\bar{x} = \\dfrac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y} = \\dfrac{1}{n}\\sum_{i=1}^n y_i$.\n- The bootstrap method approximates the sampling distribution of a statistic $\\theta$ by repeatedly drawing, with replacement, $n$ indices from $\\{1,\\dots,n\\}$ to form a bootstrap resample and computing the statistic on each resample. Let $B$ denote the number of bootstrap replicates. The empirical distribution of the $B$ bootstrap statistics approximates the true sampling distribution of $\\theta$.\n- The jackknife-after-bootstrap approach assesses the influence of each observation $i$ by recomputing the bootstrap distribution with that observation removed, yielding a new bootstrap mean for the statistic. Comparing this to the original bootstrap mean provides an influence measure.\n\nPrecise task:\n1. For each test case, compute the OLS slope statistic on each bootstrap resample of size $n$ using $B$ replicates to obtain a collection of slopes $\\{\\hat{b}_1^{*(b)}\\}_{b=1}^B$ from the full dataset. Let $\\bar{b}_{\\text{full}}$ be the mean of the valid bootstrap slopes and let $s_{\\text{full}}$ be their sample standard deviation (with degrees of freedom equal to $1$).\n2. For each observation index $i \\in \\{0,\\dots,n-1\\}$, remove that observation to form the reduced dataset of size $n-1$, generate $B$ bootstrap replicates on the reduced dataset (resampling $n-1$ points with replacement), compute the bootstrap slopes for this reduced dataset, and let $\\bar{b}_{(-i)}$ be the mean of the valid slopes from this reduced dataset.\n3. Define the influence measure for observation $i$ as\n$$\nI_i = \n\\begin{cases}\n\\dfrac{\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|}{s_{\\text{full}}}, & \\text{if } s_{\\text{full}} > 0 \\\\\n\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|, & \\text{if } s_{\\text{full}} = 0\n\\end{cases}\n$$\nand classify observation $i$ as high-influence if $I_i > \\tau$, where $\\tau$ is a given threshold.\n4. Implementation details you must respect:\n   - If a bootstrap resample yields zero variance in $x$ (that is, $\\sum_{j} (x_j - \\bar{x})^2 = 0$ for that resample), the OLS slope is undefined; you must discard that replicate and not include it in the computation of $\\bar{b}$ or $s$.\n   - If for a given dataset there are no valid bootstrap replicates at all (extremely unlikely for the provided test cases), define $I_i = 0$ for all $i$.\n   - Use $0$-based indexing for observations throughout.\n   - Do not use any physical units in your output; all results are unit-free real numbers or integers.\n\nYou must apply the above to the following test suite:\n\n- Test Case $1$:\n  - Data: $x = (0, 1, 2, 3, 10)$, $y = (0.5, 2.1, 3.9, 6.2, 50.0)$\n  - Replicates: $B = 1200$\n  - Threshold: $\\tau = 2.0$\n- Test Case $2$:\n  - Data: $x = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)$, $y = (0.1, 3.2, 6.1, 9.0, 12.0, 15.1, 18.2, 21.3, 24.1, 27.0)$\n  - Replicates: $B = 1200$\n  - Threshold: $\\tau = 2.0$\n- Test Case $3$:\n  - Data: $x = (0, 1, 1, 2, 2, 100)$, $y = (0.1, 0.7, 0.6, 1.0, 0.9, 55.0)$\n  - Replicates: $B = 1200$\n  - Threshold: $\\tau = 1.5$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must be a list of integers containing the $0$-based indices of the high-influence observations for that case, in ascending order. For example, the final output format must look like $[[i\\_1,i\\_2],[j\\_1,j\\_2,j\\_3],[]]$ where empty square brackets denote that no observation is high-influence for that case.", "solution": "The user's request is a well-defined computational statistics problem. The validation of the problem statement is as follows.\n\n**Problem Validation**\n\n1.  **Extract Givens**:\n    -   **Statistic of Interest**: The Ordinary Least Squares (OLS) slope parameter for simple linear regression, $\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$.\n    -   **Core Method**: The bootstrap method, which involves generating $B$ resamples of size $n$ by drawing with replacement from the original dataset.\n    -   **Influence Assessment Procedure (Jackknife-after-Bootstrap)**:\n        1.  Compute the mean $\\bar{b}_{\\text{full}}$ and sample standard deviation $s_{\\text{full}}$ (using $N-1$ degrees of freedom for variance) from $B$ bootstrap slope replicates on the full dataset.\n        2.  For each observation $i \\in \\{0, \\dots, n-1\\}$, remove the $i$-th point to create a reduced dataset. Compute the mean $\\bar{b}_{(-i)}$ from $B$ new bootstrap replicates on this reduced dataset.\n    -   **Influence Measure ($I_i$)**: $I_i = \\frac{|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}|}{s_{\\text{full}}}$ if $s_{\\text{full}} > 0$, and $I_i = |\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}|$ if $s_{\\text{full}} = 0$.\n    -   **High-Influence Criterion**: An observation $i$ is classified as high-influence if $I_i > \\tau$.\n    -   **Constraints**:\n        -   Bootstrap replicates with zero variance in the explanatory variable ($x$) are discarded.\n        -   If a dataset (full or reduced) yields no valid bootstrap replicates, the corresponding influence $I_i$ is treated as $0$.\n        -   $0$-based indexing is required.\n    -   **Data Suite**: Three test cases are provided, each with a dataset $(x, y)$, a number of replicates $B$, and a threshold $\\tau$.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientific Grounding**: The problem is firmly rooted in fundamental statistical theory. The OLS estimator is a cornerstone of regression analysis. The bootstrap is a canonical non-parametric technique for estimating sampling distributions. The described jackknife-after-bootstrap procedure is a valid, though computationally intensive, method for diagnosing influential data points. All definitions and formulas are correct. The problem is scientifically sound.\n    -   **Well-Posedness and Objectivity**: The problem is well-posed, providing a complete and unambiguous set of instructions, data, and edge-case handling rules. This ensures a unique, deterministic solution can be algorithmically derived. The use of a quantitative threshold $\\tau$ makes the classification objective.\n\n3.  **Verdict**: The problem is valid and can be solved as stated.\n\n**Principle-Based Solution Design**\n\nThe task requires implementing a multi-stage statistical analysis for each test case. The solution is designed by breaking down the procedure into modular, reusable functions, each grounded in a specific statistical principle.\n\n1.  **OLS Slope Calculation**:\n    The core statistic is the OLS slope parameter, $\\hat{b}_1$. For a given set of paired observations $\\{(x_j, y_j)\\}_{j=1}^m$, the estimator $\\hat{b}_1$ is calculated using the established formula:\n    $$\n    \\hat{b}_1 = \\frac{\\sum_{j=1}^m (x_j - \\bar{x})(y_j - \\bar{y})}{\\sum_{j=1}^m (x_j - \\bar{x})^2}\n    $$\n    where $\\bar{x} = \\frac{1}{m}\\sum_{j=1}^m x_j$ and $\\bar{y} = \\frac{1}{m}\\sum_{j=1}^m y_j$. A critical implementation detail is to handle cases where the denominator is zero, which occurs if all $x_j$ in a sample are identical. In such instances, the slope is undefined, and the replicate must be discarded as per the problem statement.\n\n2.  **Bootstrap Slope Distribution Generation**:\n    The bootstrap method provides a non-parametric estimate of a statistic's sampling distribution. This is achieved by simulating the sampling process by drawing data from the empirical distribution, which is the original sample itself. For a given dataset of size $m$, this involves:\n    -   Repeating $B$ times:\n        1.  Draw $m$ indices with replacement from $\\{0, 1, \\dots, m-1\\}$.\n        2.  Construct a bootstrap resample using the data at these indices.\n        3.  Compute the OLS slope $\\hat{b}_1^*$ on this resample.\n    -   Collect all valid slopes $\\{\\hat{b}_1^{*(b)}\\}_{b=1}^{B'}$ where $B' \\le B$.\n    This procedure will be applied to both the full dataset (size $n$) and each of the $n$ reduced datasets (size $n-1$).\n\n3.  **Main Procedure: Jackknife-after-Bootstrap Analysis**:\n    The main logic orchestrates the entire analysis for a single test case.\n    -   **Step A: Full Data Analysis**. First, we apply the bootstrap procedure to the complete dataset of size $n$ to generate a distribution of slopes. From this distribution, we calculate its mean, $\\bar{b}_{\\text{full}}$, and its sample standard deviation, $s_{\\text{full}}$. The standard deviation is computed with $N-1$ in the denominator (Bessel's correction), which corresponds to `ddof=1` in numerical libraries like NumPy. This establishes the baseline for our influence comparison.\n    -   **Step B: Leave-One-Out Analysis**. We then iterate through each observation $i$ from $0$ to $n-1$. In each iteration, we programmatically perform a jackknife deletion by removing observation $i$ from the dataset.\n    -   **Step C: Bootstrap on Reduced Data**. For each of these $n$ reduced datasets (of size $n-1$), we execute a full bootstrap procedure with $B$ replicates. We then calculate the mean of the resulting slope distribution, denoted $\\bar{b}_{(-i)}$. This value represents the typical slope estimate when observation $i$ is absent from the data generation process.\n    -   **Step D: Influence Calculation**. The influence of observation $i$, $I_i$, quantifies the change in the average bootstrap slope caused by its removal, standardized by the variability of the original bootstrap slopes. The formula is applied as specified:\n    $$\n    I_i = \n    \\begin{cases}\n    \\dfrac{\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|}{s_{\\text{full}}} & \\text{if } s_{\\text{full}} > 0 \\\\\n    \\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right| & \\text{if } s_{\\text{full}} = 0\n    \\end{cases}\n    $$\n    -   **Step E: Identification**. Finally, we compare each influence measure $I_i$ against the provided threshold $\\tau$. If $I_i > \\tau$, the $0$-based index $i$ is recorded as a high-influence point. The collected indices for the test case are then sorted in ascending order.\n\nThis structured approach ensures all specifications of the problem are met precisely while maintaining a clear connection between the algorithm and the underlying statistical principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It implements the jackknife-after-bootstrap procedure to identify high-influence points.\n    \"\"\"\n\n    def ols_slope(x, y):\n        \"\"\"\n        Calculates the OLS slope for a simple linear regression.\n\n        Args:\n            x (np.ndarray): The independent variable data.\n            y (np.ndarray): The dependent variable data.\n\n        Returns:\n            float: The OLS slope, or np.nan if the slope is undefined.\n        \"\"\"\n        n = len(x)\n        if n < 2:\n            return np.nan\n\n        x_mean = np.mean(x)\n        \n        # Denominator of the slope formula\n        ss_xx = np.sum((x - x_mean)**2)\n\n        # As per problem, if variance in x is zero, the slope is undefined.\n        if ss_xx == 0:\n            return np.nan\n\n        y_mean = np.mean(y)\n        # Numerator of the slope formula\n        ss_xy = np.sum((x - x_mean) * (y - y_mean))\n        \n        return ss_xy / ss_xx\n\n    def get_bootstrap_slopes(x, y, B):\n        \"\"\"\n        Generates a distribution of OLS slopes using the bootstrap method.\n\n        Args:\n            x (np.ndarray): The independent variable data.\n            y (np.ndarray): The dependent variable data.\n            B (int): The number of bootstrap replicates.\n\n        Returns:\n            np.ndarray: An array of valid bootstrap slopes.\n        \"\"\"\n        n = len(x)\n        if n == 0:\n            return np.array([])\n        \n        slopes = []\n        # Pre-generate all random indices for performance\n        # Each row is a set of indices for one bootstrap replicate\n        all_indices = np.random.choice(n, size=(B, n), replace=True)\n\n        for resample_indices in all_indices:\n            x_resample = x[resample_indices]\n            y_resample = y[resample_indices]\n            \n            slope = ols_slope(x_resample, y_resample)\n            \n            # Discard replicates where the slope is undefined\n            if not np.isnan(slope):\n                slopes.append(slope)\n                \n        return np.array(slopes)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (np.array([0, 1, 2, 3, 10]), np.array([0.5, 2.1, 3.9, 6.2, 50.0]), 1200, 2.0),\n        # Test Case 2\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), np.array([0.1, 3.2, 6.1, 9.0, 12.0, 15.1, 18.2, 21.3, 24.1, 27.0]), 1200, 2.0),\n        # Test Case 3\n        (np.array([0, 1, 1, 2, 2, 100]), np.array([0.1, 0.7, 0.6, 1.0, 0.9, 55.0]), 1200, 1.5)\n    ]\n\n    all_results = []\n    for x_full, y_full, B, tau in test_cases:\n        n = len(x_full)\n        \n        # 1. Compute bootstrap statistics for the full dataset.\n        full_slopes = get_bootstrap_slopes(x_full, y_full, B)\n        \n        # Per problem: if no valid replicates, I_i = 0 for all i.\n        # This means no high-influence points.\n        if len(full_slopes) == 0:\n            all_results.append([])\n            continue\n\n        b_full_mean = np.mean(full_slopes)\n        \n        # Per problem: sample standard deviation (ddof=1).\n        # If less than 2 valid slopes, standard deviation is 0.\n        b_full_std = 0.0\n        if len(full_slopes) >= 2:\n            b_full_std = np.std(full_slopes, ddof=1)\n            \n        high_influence_indices = []\n        # 2. Loop through each observation for jackknife-after-bootstrap.\n        for i in range(n):\n            # Create the reduced dataset by removing observation i.\n            x_reduced = np.delete(x_full, i)\n            y_reduced = np.delete(y_full, i)\n            \n            # Compute bootstrap mean for the reduced dataset.\n            reduced_slopes = get_bootstrap_slopes(x_reduced, y_reduced, B)\n            \n            # Per problem: if no valid replicates, I_i = 0.\n            if len(reduced_slopes) == 0:\n                influence_i = 0.0\n            else:\n                b_reduced_mean = np.mean(reduced_slopes)\n                \n                # 3. Define the influence measure for observation i.\n                diff = np.abs(b_reduced_mean - b_full_mean)\n                if b_full_std > 0:\n                    influence_i = diff / b_full_std\n                else:\n                    influence_i = diff\n            \n            # 4. Classify observation i as high-influence.\n            if influence_i > tau:\n                high_influence_indices.append(i)\n        \n        # Results must be in ascending order.\n        all_results.append(sorted(high_influence_indices))\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3180777"}, {"introduction": "The bootstrap is a remarkably effective tool, but it is essential to understand its theoretical properties and limitations. This problem [@problem_id:851841] challenges you to move beyond application and into analysis by calculating the exact theoretical coverage probability of a percentile bootstrap confidence interval. This exercise reveals that a method's nominal performance, such as a 95% confidence level, may not be achieved in practice, providing critical insight into the reliability of statistical procedures.", "problem": "The bootstrap is a powerful resampling method used in statistics to estimate the uncertainty of an estimator. This problem explores the theoretical properties of a non-parametric bootstrap confidence interval in a simplified setting.\n\nConsider a random sample of size $n=3$, denoted by $X_1, X_2, X_3$, drawn independently and identically from an exponential distribution with an unknown rate parameter $\\lambda$. The probability density function is given by $f(x; \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$. We are interested in estimating the median of this distribution, which we denote by $m$.\n\nA non-parametric percentile bootstrap confidence interval for the median is constructed as follows:\n1.  From the original observed sample $\\{x_1, x_2, x_3\\}$, a large number, $B$, of \"bootstrap samples\" are generated. Each bootstrap sample, $\\{x_1^*, x_2^*, x_3^*\\}$, is a new sample of size $n=3$ created by drawing with replacement from the original sample $\\{x_1, x_2, x_3\\}$.\n2.  For each of the $B$ bootstrap samples, the sample median, $\\hat{m}^*$, is calculated. The sample median of three numbers is the middle value when they are sorted.\n3.  The collection of these $B$ bootstrap medians, $\\{\\hat{m}^*_1, \\dots, \\hat{m}^*_B\\}$, forms an empirical distribution that approximates the sampling distribution of the sample median.\n4.  An approximate 95% confidence interval for the true median $m$ is given by the 2.5th and 97.5th percentiles of this empirical distribution.\n\nFor this problem, we consider the idealized case where the number of bootstrap resamples $B \\to \\infty$. In this limit, the percentiles are determined from the exact theoretical probability mass function of the bootstrap median $\\hat{m}^*$, conditional on the original sample. For a discrete distribution, the $p$-th percentile is defined as the minimum value $v$ such that the cumulative probability $P(\\hat{m}^* \\le v)$ is at least $p$.\n\nYour task is to derive the exact theoretical coverage probability of this 95% bootstrap confidence interval. The coverage probability is defined as the probability that the random interval, constructed from a sample $\\{X_1, X_2, X_3\\}$, successfully contains the true population median $m$.", "solution": "1. The bootstrap median $\\hat{m}^*$ from three draws with replacement from $\\{x_{(1)}, x_{(2)}, x_{(3)}\\}$ takes values $x_{(1)}, x_{(2)}, x_{(3)}$ with probabilities\n$$P(\\hat{m}^*=x_{(1)})=P(N_1 \\ge 2)=\\sum_{k=2}^3\\binom{3}{k}\\Bigl(\\tfrac13\\Bigr)^k\\Bigl(\\tfrac23\\Bigr)^{3-k}=\\frac7{27},$$\n$$P(\\hat{m}^*=x_{(3)})=P(N_3 \\ge 2)=\\frac7{27},\\quad\nP(\\hat{m}^*=x_{(2)})=1-\\frac{7+7}{27}=\\frac{13}{27}.$$\n2. The 2.5th percentile is the smallest $v$ with $P(\\hat{m}^* \\le v)\\ge0.025$. Since $P(\\hat{m}^* \\le x_{(1)})=7/27>0.025$, the lower bound is $x_{(1)}$.  The 97.5th percentile satisfies $P(\\hat{m}^* \\le x_{(2)})=20/27<0.975$ but $P(\\hat{m}^* \\le x_{(3)})=1\\ge0.975$, so the upper bound is $x_{(3)}$. Thus the interval is $[X_{(1)},X_{(3)}]$.\n3. The coverage is\n$$P\\bigl(X_{(1)}\\le m\\le X_{(3)}\\bigr)\n=1-P(X_{(3)} < m)-P(X_{(1)} > m).$$\nFor an exponential distribution with median $m$, $P(X<m)=\\tfrac12$ and $P(X>m)=\\tfrac12$. Hence\n$$P(X_{(3)} < m)=\\Bigl(\\tfrac12\\Bigr)^3,\\quad P(X_{(1)} > m)=\\Bigl(\\tfrac12\\Bigr)^3,$$\nso\n$$\\text{Coverage}=1-2\\Bigl(\\tfrac12\\Bigr)^3=1-\\tfrac14=\\tfrac34.$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "851841"}]}