{"hands_on_practices": [{"introduction": "To build a solid understanding of Leave-One-Out Cross-Validation (LOOCV), we begin with the simplest possible predictive model: one that always predicts the sample mean of the training data. This exercise [@problem_id:1912461] strips away the complexity of sophisticated models to focus on the core mechanics of the LOOCV procedure. By deriving a closed-form expression for the LOOCV error, you will discover a direct and elegant connection between the model's predictive performance and the fundamental statistical concept of sample variance.", "problem": "In the field of statistical learning, cross-validation is a fundamental technique for assessing how the results of a statistical analysis will generalize to an independent dataset. A common variant is Leave-One-Out Cross-Validation (LOOCV).\n\nConsider a dataset consisting of $n$ observations, $y_1, y_2, \\ldots, y_n$. We wish to evaluate a very simple predictive model: for any given training set, the model's prediction for a new data point is simply the arithmetic mean of the observations in that training set.\n\nThe LOOCV procedure for this dataset involves $n$ iterations. In the $i$-th iteration (for $i=1, \\ldots, n$), the $i$-th observation, $y_i$, is held out as the validation set, and the remaining $n-1$ observations are used as the training set. The model is trained on these $n-1$ observations, and a prediction is made for the held-out observation $y_i$. The squared error between the prediction and the actual value $y_i$ is then calculated.\n\nYour task is to derive a general, closed-form expression for the LOOCV Mean Squared Error (MSE), which is the average of these squared errors over all $n$ iterations. Express your final answer in terms of the number of observations, $n$, and the sample variance of the full dataset, $s^2$. The sample variance is defined as $s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$, where $\\bar{y}$ is the sample mean of all $n$ observations.", "solution": "We consider the predictive model that outputs the arithmetic mean of the training set. Let the full-sample mean be $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_{j}$. In the $i$-th LOOCV fold, the training set excludes $y_{i}$, so the leave-one-out mean is\n$$\n\\bar{y}_{-i} = \\frac{1}{n-1}\\sum_{\\substack{j=1 \\\\ j\\neq i}}^{n} y_{j} = \\frac{n\\bar{y} - y_{i}}{n-1}.\n$$\nThe prediction error for the held-out $y_{i}$ is\n$$\ny_{i} - \\bar{y}_{-i} = y_{i} - \\frac{n\\bar{y} - y_{i}}{n-1} = \\frac{n(y_{i} - \\bar{y})}{n-1}.\n$$\nThus, the squared error in the $i$-th fold is\n$$\n\\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{n^{2}}{(n-1)^{2}}(y_{i} - \\bar{y})^{2}.\n$$\nThe LOOCV Mean Squared Error (MSE) is the average of these squared errors over $i=1,\\ldots,n$:\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}.\n$$\nUsing the definition of the sample variance $s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}$, we substitute $\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} = (n-1)s^{2}$ to obtain\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\cdot (n-1) s^{2} = \\frac{n}{n-1} s^{2}.\n$$\nTherefore, the LOOCV MSE for the mean-as-predictor model is $\\frac{n}{n-1}s^{2}$.", "answer": "$$\\boxed{\\frac{n}{n-1}s^{2}}$$", "id": "1912461"}, {"introduction": "With the fundamentals in place, we now turn to a primary application of cross-validation: model selection. This practice [@problem_id:3139300] directly confronts a common pitfall in machine learningâ€”relying on training error, which can be misleadingly optimistic. By using a $k$-Nearest Neighbors (k-NN) classifier on a dataset with a carefully placed outlier, you will see how LOOCV provides a more honest assessment of generalization performance and guides the selection of a more robust model by favoring a larger value of $k$.", "problem": "You will implement and analyze Leave-One-Out Cross-Validation (LOOCV) for the $k$-Nearest Neighbors (k-NN) classifier on constructed datasets designed to reveal the effect of a single extreme outlier on model selection. The aim is to demonstrate from first principles why LOOCV can select a larger $k$ to counteract an outlier, while the training resubstitution error favors $k=1$. Begin from the following foundational definitions.\n\nLet there be a dataset of $N$ labeled points $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$. The $k$-Nearest Neighbors classifier predicts the label for a point by:\n- computing Euclidean distances to all candidate neighbors (excluding the query point under Leave-One-Out Cross-Validation),\n- selecting the $k$ points with smallest distances (if equal distances occur, break ties deterministically by the smaller original index),\n- returning the class with the majority of votes among these $k$ neighbors (for odd $k$, this avoids count ties; if a rare tie persists, break it by choosing the class whose neighbors have the smaller sum of distances, and if still tied, choose the smaller class label).\n\nDefine the empirical $0\\text{-}1$ loss for a predictor $\\hat{f}$ on a dataset as $$\\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}\\{\\hat{f}(x_i) \\neq y_i\\},$$ where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. Define the training resubstitution error as the empirical $0\\text{-}1$ loss measured by classifying each $x_i$ using the same dataset that contains $x_i$ and allowing $x_i$ to be selected among its own neighbors. Define Leave-One-Out Cross-Validation (LOOCV) error as the empirical $0\\text{-}1$ loss measured by classifying each $x_i$ using a model that excludes $x_i$ from the training set.\n\nYour program must:\n1. Implement $k$-Nearest Neighbors classification, Euclidean distance in $\\mathbb{R}^2$, training resubstitution error, and Leave-One-Out Cross-Validation error, strictly following the above rules.\n2. For each dataset in the test suite below and for candidate values $k \\in \\{1,3\\}$, compute:\n   - the $k$ that minimizes LOOCV error, using the tie-breaking rule \"choose the smallest $k$\" if multiple $k$ yield the same minimal error,\n   - the $k$ that minimizes training resubstitution error, using the same tie-breaking rule.\n3. Produce a single line of output containing the per-dataset results in order, formatted as a comma-separated list enclosed in square brackets, where each per-dataset result is itself a two-element list $[k_{\\text{LOOCV}}, k_{\\text{train}}]$. For example, the output should look like $$[[k_1^{\\text{LOOCV}},k_1^{\\text{train}}],[k_2^{\\text{LOOCV}},k_2^{\\text{train}}],[k_3^{\\text{LOOCV}},k_3^{\\text{train}}]].$$\n\nTest suite (all coordinates are in $\\mathbb{R}^2$ with no physical units):\n- Case $A$ (one extreme outlier centered in the cluster):\n  - Class $0$ points: $(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$.\n  - Class $1$ outlier: $(0,0)$.\n  - Candidate $k$: $\\{1,3\\}$.\n- Case $B$ (no outlier; well-separated clusters):\n  - Class $0$ points: $(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$.\n  - Class $1$ points: $(10,0)$, $(-10,0)$, $(0,10)$, $(0,-10)$.\n  - Candidate $k$: $\\{1,3\\}$.\n- Case $C$ (an extreme outlier far from all other points but sharing the label of one cluster):\n  - Class $0$ points: $(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$, $(100,100)$.\n  - Class $1$ points: $(3,0)$, $(3,1)$, $(4,0)$, $(4,1)$.\n  - Candidate $k$: $\\{1,3\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the exact format $$[[k_A^{\\text{LOOCV}},k_A^{\\text{train}}],[k_B^{\\text{LOOCV}},k_B^{\\text{train}}],[k_C^{\\text{LOOCV}},k_C^{\\text{train}}]].$$ The numbers in the output must be integers.", "solution": "The problem requires a comparative analysis of the training resubstitution error and the Leave-One-Out Cross-Validation (LOOCV) error for a $k$-Nearest Neighbors ($k$-NN) classifier. We will implement the classifier and error metrics according to the specified rules and apply them to three distinct datasets for candidate values of $k \\in \\{1, 3\\}$. The goal is to determine the optimal $k$ selected by each error metric, denoted $k_{\\text{train}}$ and $k_{\\text{LOOCV}}$ respectively.\n\nThe foundational definitions are as follows. The dataset is a collection of $N$ points $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$. A prediction $\\hat{f}(x)$ for a query point $x$ is made by majority vote of the $k$ nearest training points, where distance is Euclidean. Ties in distance are broken by the smaller original index of the points. The training resubstitution error, $E_{\\text{train}}$, is the $0\\text{-}1$ loss on the full training set, allowing a point to be its own a neighbor. The LOOCV error, $E_{\\text{LOOCV}}$, is the $0\\text{-}1$ loss computed by averaging the error of predicting each point $x_i$ using a model trained on the dataset excluding $(x_i, y_i)$. The optimal $k$ for each metric is the one that minimizes the corresponding error; ties are broken by selecting the smaller $k$.\n\nWe will analyze each case systematically.\n\n**Case A: One extreme outlier centered in the cluster.**\nThe dataset consists of $N=5$ points.\nClass $0$: $P_0=(1,0)$, $P_1=(-1,0)$, $P_2=(0,1)$, $P_3=(0,-1)$.\nClass $1$: $P_4=(0,0)$.\n\n**Training Resubstitution Error Analysis ($E_{\\text{train}}$):**\nFor $k=1$: When classifying a point $x_i$ from the training set, its nearest neighbor is always itself, with a distance of $0$. Therefore, the predicted label $\\hat{f}(x_i)$ is its own label $y_i$. This results in zero misclassifications.\n$E_{\\text{train}}(k=1) = \\frac{0}{5} = 0$.\nFor $k=3$: We find the $3$ nearest neighbors for each point, including the point itself.\n- For any Class $0$ point (e.g., $P_0=(1,0)$), its neighbors are itself ($P_0$, Class $0$, dist $0$), the outlier $P_4$ ($P_4$, Class $1$, dist $1$), and another Class $0$ point ($P_2$ or $P_3$, dist $\\sqrt{2}$). The labels are $\\{0, 1, 0\\}$, so the majority vote is Class $0$. The prediction is correct. This holds for $P_0, P_1, P_2, P_3$.\n- For the outlier $P_4=(0,0)$ (Class $1$), its neighbors are itself ($P_4$, Class $1$, dist $0$) and any two of the Class $0$ points, which are all equidistant at dist $1$. Using the index tie-breaking rule, we select $P_0$ and $P_1$. The neighbors' labels are $\\{1, 0, 0\\}$. The majority vote is Class $0$. The prediction is $0$, but the true label is $1$. This is one misclassification.\nTotal misclassifications for $k=3$ is $1$.\n$E_{\\text{train}}(k=3) = \\frac{1}{5} = 0.2$.\nComparing errors, $E_{\\text{train}}(k=1) = 0  E_{\\text{train}}(k=3) = 0.2$. Thus, $k_{\\text{train}} = 1$.\n\n**Leave-One-Out Cross-Validation Error Analysis ($E_{\\text{LOOCV}}$):**\nFor $k=1$: We classify each point $x_i$ using the other $N-1$ points.\n- For any Class $0$ point (e.g., $P_0=(1,0)$), its nearest neighbor in the remaining set $\\{P_1, P_2, P_3, P_4\\}$ is the outlier $P_4=(0,0)$ (Class $1$) at a distance of $1$. The prediction is Class $1$, while the true label is $0$. This is a misclassification. All $4$ Class $0$ points are misclassified this way.\n- For the outlier $P_4=(0,0)$ (Class $1$), its nearest neighbor in the set $\\{P_0, P_1, P_2, P_3\\}$ is any of them (all at distance $1$). By index tie-breaking, we select $P_0=(1,0)$ (Class $0$). The prediction is Class $0$, while the true label is $1$. This is a misclassification.\nTotal misclassifications for $k=1$ is $5$.\n$E_{\\text{LOOCV}}(k=1) = \\frac{5}{5} = 1.0$.\nFor $k=3$:\n- For any Class $0$ point (e.g., $P_0=(1,0)$), its $3$ nearest neighbors in the remaining set $\\{P_1, P_2, P_3, P_4\\}$ are $P_4$ (Class $1$, dist $1$), $P_2$ (Class $0$, dist $\\sqrt{2}$), and $P_3$ (Class $0$, dist $\\sqrt{2}$). The labels are $\\{1, 0, 0\\}$. The majority vote is Class $0$. The prediction is correct. This holds for all $4$ Class $0$ points.\n- For the outlier $P_4=(0,0)$ (Class $1$), its $3$ nearest neighbors in the set $\\{P_0, P_1, P_2, P_3\\}$ are any three of these points (all at distance $1$). By index tie-breaking, we select $P_0, P_1, P_2$, all of which are Class $0$. The labels are $\\{0, 0, 0\\}$. The prediction is Class $0$, while the true label is $1$. This is a misclassification.\nTotal misclassifications for $k=3$ is $1$.\n$E_{\\text{LOOCV}}(k=3) = \\frac{1}{5} = 0.2$.\nComparing errors, $E_{\\text{LOOCV}}(k=3) = 0.2  E_{\\text{LOOCV}}(k=1) = 1.0$. Thus, $k_{\\text{LOOCV}} = 3$.\nThe result for Case A is $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [3, 1]$.\n\n**Case B: No outlier; well-separated clusters.**\nThe dataset consists of $N=8$ points.\nClass $0$: $\\{(1,0), (-1,0), (0,1), (0,-1)\\}$.\nClass $1$: $\\{(10,0), (-10,0), (0,10), (0,-10)\\}$.\nFollowing the same methodology:\n$E_{\\text{train}}(k=1) = 0$. For $k=3$, each point in the outer cluster (Class $1$) is misclassified because its two nearest neighbors (besides itself) are from the inner cluster (Class $0$), resulting in $4$ errors. $E_{\\text{train}}(k=3) = \\frac{4}{8} = 0.5$. So $k_{\\text{train}} = 1$.\n$E_{\\text{LOOCV}}(k=1)$: Each Class $1$ point's nearest neighbor is a Class $0$ point, leading to $4$ errors. Each Class $0$ point's nearest neighbor is another Class $0$ point, leading to $0$ errors. $E_{\\text{LOOCV}}(k=1) = \\frac{4}{8} = 0.5$.\n$E_{\\text{LOOCV}}(k=3)$: Each Class $1$ point's three nearest neighbors are all Class $0$ points, leading to $4$ errors. Each Class $0$ point's three nearest neighbors are other Class $0$ points, leading to $0$ errors. $E_{\\text{LOOCV}}(k=3) = \\frac{4}{8} = 0.5$.\nErrors are tied: $E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = 0.5$. By the tie-breaking rule, we choose the smallest $k$. Thus, $k_{\\text{LOOCV}} = 1$.\nThe result for Case B is $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$.\n\n**Case C: An extreme outlier far from all other points.**\nThe dataset consists of $N=9$ points.\nClass $0$: $\\{(1,0), (-1,0), (0,1), (0,-1), (100,100)\\}$.\nClass $1$: $\\{(3,0), (3,1), (4,0), (4,1)\\}$.\nFollowing the same methodology:\n$E_{\\text{train}}(k=1) = 0$. For $k=3$, only the outlier point $(100,100)$ is misclassified, as its two nearest neighbors (besides itself) are from the Class 1 cluster. $E_{\\text{train}}(k=3) = \\frac{1}{9}$. So $k_{\\text{train}} = 1$.\n$E_{\\text{LOOCV}}(k=1)$: When the outlier $(100,100)$ is held out, its nearest neighbor is a Class $1$ point, resulting in one error. All other points are correctly classified. $E_{\\text{LOOCV}}(k=1) = \\frac{1}{9}$.\n$E_{\\text{LOOCV}}(k=3)$: When the outlier $(100,100)$ is held out, its three nearest neighbors are all Class $1$ points, resulting in one error. All other points are correctly classified. $E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$.\nErrors are tied: $E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$. By the tie-breaking rule, we choose the smallest $k$. Thus, $k_{\\text{LOOCV}} = 1$.\nThe result for Case C is $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$.\n\nThe final results are collected from each case.\n- Case A: $[3, 1]$\n- Case B: $[1, 1]$\n- Case C: $[1, 1]$\nThese results demonstrate the key insight: training resubstitution error will always trivially select $k=1$ (as its error is always $0$), which is a pathological aspect of this error metric. LOOCV provides a more honest estimate of generalization error. In Case A, it correctly identifies that a larger $k=3$ is needed to be robust against the central outlier. In Cases B and C, the geometry of the data is such that $k=1$ is preferred or tied with $k=3$, and the tie-breaking rule selects $k=1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes LOOCV vs. training error for k-NN on three datasets.\n    \"\"\"\n\n    test_cases = [\n        # Case A: one extreme outlier centered in the cluster\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[0, 0]]),\n        },\n        # Case B: no outlier; well-separated clusters\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[10, 0], [-10, 0], [0, 10], [0, -10]]),\n        },\n        # Case C: an extreme outlier far from all other points\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1], [100, 100]]),\n            \"class_1_points\": np.array([[3, 0], [3, 1], [4, 0], [4, 1]]),\n        }\n    ]\n    \n    candidate_k_values = [1, 3]\n    final_results = []\n\n    for case_data in test_cases:\n        # Prepare dataset with original indices for tie-breaking\n        points = []\n        labels = []\n        \n        index = 0\n        for p in case_data[\"class_0_points\"]:\n            points.append(p)\n            labels.append(0)\n            index += 1\n        for p in case_data[\"class_1_points\"]:\n            points.append(p)\n            labels.append(1)\n            index += 1\n\n        points = np.array(points)\n        labels = np.array(labels)\n        full_dataset = list(zip(range(len(points)), points, labels))\n\n        # --- k-NN Implementation ---\n        def euclidean_distance(p1, p2):\n            return np.sqrt(np.sum((p1 - p2)**2))\n\n        def k_nn_predict(query_point, training_data, k):\n            if not training_data:\n                # Undefined, but for this problem, training set is never empty\n                return 0 \n            \n            distances = []\n            for idx, train_point, label in training_data:\n                dist = euclidean_distance(query_point, train_point)\n                distances.append((dist, idx, label))\n            \n            # Sort by distance, then by original index for tie-breaking\n            distances.sort(key=lambda x: (x[0], x[1]))\n            \n            neighbors = distances[:k]\n            \n            neighbor_labels = [label for _, _, label in neighbors]\n            \n            # Majority vote\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n\n            if count_0  count_1:\n                return 0\n            elif count_1  count_0:\n                return 1\n            else: # Vote tie-break (unlikely for odd k here, but for completeness)\n                sum_dist_0 = sum(dist for dist, _, label in neighbors if label == 0)\n                sum_dist_1 = sum(dist for dist, _, label in neighbors if label == 1)\n                if sum_dist_0  sum_dist_1:\n                    return 0\n                elif sum_dist_1  sum_dist_0:\n                    return 1\n                else: # Tie in sum of distances, choose smaller class label\n                    return 0\n        \n        # --- Error Calculation ---\n        \n        # Training Resubstitution Error\n        train_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                # In training error, the point itself is part of the training set\n                # For k-NN, if k=1, the nearest neighbor to a point is itself.\n                # If k=1, error is always 0.\n                if k == 1:\n                    predicted_label = true_label\n                else:\n                    predicted_label = k_nn_predict(query_point, full_dataset, k)\n                    \n                if predicted_label != true_label:\n                    misclassifications += 1\n            train_errors[k] = misclassifications / len(points)\n        \n        # LOOCV Error\n        loocv_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                \n                # Create LOOCV training set by excluding point i\n                loocv_train_set = full_dataset[:i] + full_dataset[i+1:]\n                \n                predicted_label = k_nn_predict(query_point, loocv_train_set, k)\n                \n                if predicted_label != true_label:\n                    misclassifications += 1\n            loocv_errors[k] = misclassifications / len(points)\n\n        # --- Model Selection (Find best k) ---\n        \n        # Tie-breaking rule: choose smallest k\n        min_loocv_error = float('inf')\n        best_k_loocv = -1\n        for k in sorted(loocv_errors.keys()):\n            if loocv_errors[k]  min_loocv_error:\n                min_loocv_error = loocv_errors[k]\n                best_k_loocv = k\n        \n        min_train_error = float('inf')\n        best_k_train = -1\n        # For k=1 training error is always 0, so it will always be selected\n        # if 1 is a candidate k.\n        for k in sorted(train_errors.keys()):\n            if train_errors[k]  min_train_error:\n                min_train_error = train_errors[k]\n                best_k_train = k\n                \n        final_results.append([best_k_loocv, best_k_train])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{k_l},{k_t}]\" for k_l, k_t in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3139300"}, {"introduction": "A frequent criticism of LOOCV is its perceived computational burden, as it naively suggests fitting a model $n$ times. This advanced practice [@problem_id:3275464] demonstrates that this is often a misconception, especially for widely-used models like linear regression. You will implement a computationally efficient method to calculate LOOCV error using a single QR factorization, bypassing the need to refit the model $n$ times, and verify its mathematical equivalence to the brute-force approach.", "problem": "You are given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n \\ge p$ and full column rank, and a response vector $y \\in \\mathbb{R}^{n}$. Consider Ordinary Least Squares (OLS) and Leave-One-Out Cross-Validation (LOOCV). Define the orthogonal-triangular (QR) factorization as $X = Q R$ with $Q \\in \\mathbb{R}^{n \\times p}$ having orthonormal columns and $R \\in \\mathbb{R}^{p \\times p}$ upper triangular. The fitted values satisfy $\\hat{y} = H y$ where $H$ is the orthogonal projector onto $\\operatorname{col}(X)$, commonly known as the hat matrix. Implement LOOCV efficiently using QR without refitting the model from scratch for each left-out observation, and verify its equivalence to direct formulae involving the hat matrix. Also verify equivalence against a naive LOOCV implementation that refits after removing each observation.\n\nStarting only from the core definitions of OLS, orthogonal projection, and QR factorization, construct an algorithm that:\n- Computes LOOCV residuals for all $i \\in \\{1,\\dots,n\\}$ efficiently using a single reduced QR factorization of $X$.\n- Computes LOOCV residuals by direct use of the hat matrix $H = X (X^\\top X)^{-1} X^\\top$ for verification.\n- Computes LOOCV residuals naively by refitting $n$ times using a reduced QR factorization of the row-deleted matrices $X_{-i}$.\n\nYour program must produce boolean comparisons confirming the numerical equivalence (within specified tolerances) between:\n- The efficient QR-based LOOCV residuals and the naive LOOCV residuals.\n- The efficient QR-based LOOCV residuals and the hat-matrix-based LOOCV residuals.\n\nUse the following test suite with fixed random seeds to ensure reproducibility. In each test, include an intercept column as the first column of $X$ filled with ones.\n\n- Test case $1$ (general case, \"happy path\"): $n=12$, $p=3$. Construct $X$ with an intercept and two independent standard normal feature columns. Let $\\beta = (-1.5, 2.0, 0.5)$ and $y = X \\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, 0.1^2)$ and random seed $42$. Use tolerance $10^{-10}$ for the comparisons.\n- Test case $2$ (boundary with high leverage): $n=15$, $p=3$. Construct $X$ with an intercept and two independent standard normal features using seed $123$, then set the second and third feature of one specific row to large values to induce high leverage, for example, replace these two features in row $i=3$ (zero-based index $2$) by $50$ and $-50$. Let $\\beta = (0.0, 1.0, -0.5)$ and $y = X \\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, 0.01^2)$. Use tolerance $10^{-8}$ for the comparisons.\n- Test case $3$ (exact fit): $n=20$, $p=4$. Construct $X$ with an intercept and three independent standard normal features with random seed $2023$. Let $\\beta = (0.7, -1.2, 0.3, 2.0)$ and $y = X \\beta$ exactly (no noise). Use tolerance $10^{-10}$ for the comparisons.\n\nDefine the LOOCV residual for observation $i$ as $e_i^{(-i)} = y_i - \\hat{y}_i^{(-i)}$, where $\\hat{y}_i^{(-i)}$ is the prediction for $y_i$ from the model fitted on all observations except $i$. Your program must:\n- Implement an efficient QR-based computation of all $e_i^{(-i)}$ using a single reduced QR factorization of $X$ and orthogonal projector properties.\n- Implement a naive computation of all $e_i^{(-i)}$ by refitting $n$ times, each time using a reduced QR factorization of $X_{-i}$ to obtain $\\hat{\\beta}^{(-i)}$ and then $\\hat{y}_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)}$.\n- Implement a hat-matrix-based computation of all $e_i^{(-i)}$ using $H$, provided $X$ has full column rank.\n\nYour program must output a single line containing the results as a comma-separated list enclosed in square brackets, in the following order across the three tests:\n- For each test, first a boolean indicating whether the efficient QR-based LOOCV residuals match the naive refit LOOCV residuals within the specified tolerance, followed by a boolean indicating whether the efficient QR-based LOOCV residuals match the hat-matrix-based LOOCV residuals within the specified tolerance.\n\nThus the final output must be of the form $[\\text{b}_{11}, \\text{b}_{12}, \\text{b}_{21}, \\text{b}_{22}, \\text{b}_{31}, \\text{b}_{32}]$, where $\\text{b}_{jk}$ are booleans corresponding to test $j$ and comparison $k$.", "solution": "The solution involves implementing and verifying three different methods for calculating the Leave-One-Out Cross-Validation (LOOCV) residuals for an Ordinary Least Squares (OLS) model. The LOOCV residual for the $i$-th observation is $e_i^{(-i)} = y_i - \\hat{y}_i^{(-i)}$, where $\\hat{y}_i^{(-i)}$ is the prediction for $y_i$ from a model fitted on all data except observation $i$.\n\nA key theoretical result allows for the highly efficient computation of these residuals. The LOOCV residual can be expressed in terms of quantities derived from a single model fit on the *full* dataset:\n$$ e_i^{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - H_{ii}} = \\frac{e_i}{1 - H_{ii}} $$\nHere, $e_i$ is the ordinary residual for observation $i$, and $H_{ii}$ is the $i$-th diagonal element of the hat matrix $H = X(X^\\top X)^{-1}X^\\top$, known as the leverage. This formula forms the basis for our efficient methods. The provided code in the answer block implements the following three approaches for verification.\n\n**Method 1: Efficient LOOCV via a Single QR Factorization**\nThis is the most stable and efficient approach. It relies on the fact that the hat matrix can be expressed as $H = QQ^\\top$, where $Q$ comes from the reduced QR factorization of the design matrix, $X = QR$.\n1. Compute the reduced QR factorization of the full matrix $X$ once.\n2. Calculate the ordinary residuals: $e = y - \\hat{y} = y - QQ^\\top y$.\n3. Calculate all leverage values: The diagonal elements $H_{ii}$ are computed as the squared Euclidean norm of the $i$-th row of $Q$, i.e., $H_{ii} = \\|q_i^\\top\\|^2$.\n4. Compute all LOOCV residuals in a single vectorized operation using the formula $e_i / (1 - H_{ii})$.\n\n**Method 2: LOOCV via Explicit Hat Matrix**\nThis method directly uses the definition of the hat matrix to verify the formula. It is less numerically stable than the QR approach.\n1. Form the hat matrix explicitly: $H = X(X^\\top X)^{-1}X^\\top$.\n2. Calculate ordinary residuals $e = y - Hy$.\n3. Extract the diagonal elements $H_{ii}$ from $H$.\n4. Compute the LOOCV residuals using the formula $e_i / (1 - H_{ii})$.\n\n**Method 3: Naive LOOCV via Refitting**\nThis brute-force method serves as the ground truth. It follows the literal definition of LOOCV.\n1. Loop $n$ times, from $i=1$ to $n$.\n2. In each iteration, remove the $i$-th row from $X$ and $y$ to create $X_{-i}$ and $y_{-i}$.\n3. Fit a new OLS model on this reduced dataset to get coefficients $\\hat{\\beta}^{(-i)}$. This is done efficiently using a QR factorization of $X_{-i}$.\n4. Predict the held-out value: $\\hat{y}_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)}$.\n5. Calculate the LOOCV residual for observation $i$ as $y_i - \\hat{y}_i^{(-i)}$.\n\nThe program then compares the vector of residuals from Method 1 against those from Method 2 and Method 3, confirming their numerical equivalence within the specified tolerances for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test(n, p, beta_true, noise_std, seed, tolerance, high_leverage_config=None):\n    \"\"\"\n    Runs a single test case for comparing LOOCV computation methods.\n    \n    Args:\n        n (int): Number of observations.\n        p (int): Number of features (including intercept).\n        beta_true (np.ndarray): True coefficient vector.\n        noise_std (float): Standard deviation of the noise.\n        seed (int): Random seed for data generation.\n        tolerance (float): Absolute tolerance for numerical comparisons.\n        high_leverage_config (dict, optional): Configuration for high leverage point.\n    \n    Returns:\n        tuple[bool, bool]: A tuple of booleans indicating the success of the two comparisons.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate data\n    X = np.zeros((n, p))\n    X[:, 0] = 1  # Intercept column\n    if p  1:\n        X[:, 1:] = rng.standard_normal(size=(n, p - 1))\n\n    if high_leverage_config:\n        row_idx = high_leverage_config['row_index']\n        values = high_leverage_config['values']\n        # The first column is intercept, so features start at column index 1\n        X[row_idx, 1:] = values\n\n    epsilon = rng.normal(0, noise_std, n)\n    y = X @ beta_true + epsilon\n\n    # Method 1: Efficient LOOCV via a single QR factorization\n    Q, R = np.linalg.qr(X, 'reduced')\n    y_hat = Q @ (Q.T @ y)\n    residuals = y - y_hat\n    h_diag = np.sum(Q**2, axis=1)\n    loocv_residuals_eff = residuals / (1 - h_diag)\n\n    # Method 2: LOOCV via direct hat matrix calculation\n    # Note: This method is less numerically stable and is for verification only.\n    # It requires X to have full column rank, which is assumed by the problem.\n    try:\n        XTX_inv = np.linalg.inv(X.T @ X)\n        H = X @ XTX_inv @ X.T\n        y_hat_H = H @ y\n        residuals_H = y - y_hat_H\n        h_diag_H = np.diag(H)\n        loocv_residuals_hat = residuals_H / (1 - h_diag_H)\n        # Compare efficient QR vs hat-matrix method\n        comp2 = np.allclose(loocv_residuals_eff, loocv_residuals_hat, atol=tolerance, rtol=0)\n    except np.linalg.LinAlgError:\n        # If X is rank-deficient, inv(X.T @ X) will fail.\n        # This shouldn't happen with the test cases but is a good practice.\n        comp2 = False # Mark comparison as failed if matrix is singular\n\n    # Method 3: Naive LOOCV by refitting n times\n    loocv_residuals_naive = np.zeros(n)\n    for i in range(n):\n        X_minus_i = np.delete(X, i, axis=0)\n        y_minus_i = np.delete(y, i)\n        \n        # Solve OLS for the (i)-th fold using QR\n        Q_i, R_i = np.linalg.qr(X_minus_i, 'reduced')\n        beta_hat_i = np.linalg.solve(R_i, Q_i.T @ y_minus_i)\n        \n        # Predict the left-out observation\n        y_hat_i_pred = X[i, :] @ beta_hat_i\n        loocv_residuals_naive[i] = y[i] - y_hat_i_pred\n\n    # Compare efficient QR vs naive refitting method\n    comp1 = np.allclose(loocv_residuals_eff, loocv_residuals_naive, atol=tolerance, rtol=0)\n    \n    return comp1, comp2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 12, \"p\": 3, \"beta_true\": np.array([-1.5, 2.0, 0.5]),\n            \"noise_std\": 0.1, \"seed\": 42, \"tolerance\": 1e-10\n        },\n        {\n            \"n\": 15, \"p\": 3, \"beta_true\": np.array([0.0, 1.0, -0.5]),\n            \"noise_std\": 0.01, \"seed\": 123, \"tolerance\": 1e-8,\n            \"high_leverage_config\": {\"row_index\": 2, \"values\": [50.0, -50.0]}\n        },\n        {\n            \"n\": 20, \"p\": 4, \"beta_true\": np.array([0.7, -1.2, 0.3, 2.0]),\n            \"noise_std\": 0.0, \"seed\": 2023, \"tolerance\": 1e-10\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        comp1, comp2 = run_test(\n            n=case['n'],\n            p=case['p'],\n            beta_true=case['beta_true'],\n            noise_std=case['noise_std'],\n            seed=case['seed'],\n            tolerance=case['tolerance'],\n            high_leverage_config=case.get('high_leverage_config')\n        )\n        all_results.extend([comp1, comp2])\n    \n    # Format the final output as specified\n    results_str = [str(r).lower() for r in all_results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3275464"}]}