{"hands_on_practices": [{"introduction": "A key task in exploratory data analysis is to characterize the nature of the relationship between a predictor and a response variable. While the Pearson correlation coefficient measures the strength of a linear association, the Spearman rank correlation assesses the strength of a monotonic trend (one that consistently increases or decreases, but not necessarily in a straight line). This practice challenges you to interpret a common scenario where these two metrics diverge significantly, a critical skill for diagnosing non-linearity and selecting appropriate modeling strategies [@problem_id:3120045].", "problem": "A data analyst is performing Exploratory Data Analysis (EDA) for a regression task with a continuously measured predictor $X$ and response $Y$. The dataset contains $n=320$ observations $\\{(x_i,y_i)\\}_{i=1}^{n}$. The analyst notes that $X$ is right-skewed (sample skewness approximately $1.1$) and $Y$ is nonnegative. The sample Pearson correlation between $X$ and $Y$ is $0.43$, while the sample Spearman rank correlation is $0.87$. A boxplot-based Interquartile Range (IQR) check flags $5$ mild high-end $x_i$ outliers. After trimming the largest $1\\%$ of $x_i$ values, the correlations remain approximately the same: Pearson $0.42$ and Spearman $0.86$. No other data quality issues are detected.\n\nWhich of the following is the most plausible diagnostic conclusion about the relationship between $X$ and $Y$, and which modeling step is most appropriate as a next action for predicting $Y$ from $X$?\n\nA. The relationship is approximately linear with homoscedastic noise; the Pearson–Spearman discrepancy is due to sampling variability. Proceed with ordinary least squares (OLS) without transformation.\n\nB. The relationship is monotonic but nonlinear (for example, concave increasing). The high rank correlation with moderate Pearson suggests using a monotonic transformation of $X$ (such as $\\log$) or a monotonic model (for example, isotonic regression or a shape-constrained spline).\n\nC. $X$ and $Y$ are independent; the high Spearman rank correlation is an artifact of skewness. Avoid using $X$ in the model.\n\nD. The association is driven by a few extreme outliers; winsorize $X$ and fit a linear OLS model on the winsorized data.\n\nE. The relationship is non-monotonic (for example, U-shaped), so a high-degree polynomial (for example, degree $5$) is preferred; the Spearman value being high is consistent with such non-monotonicity under skewness.", "solution": "The user wants me to act as a meticulous STEM professor and solve a problem about exploratory data analysis.\n\n### Problem Validation\n\nFirst, I must validate the problem statement according to the specified protocol.\n\n**Step 1: Extract Givens**\n\n*   **Task Type:** Regression\n*   **Predictor Variable:** $X$, continuously measured.\n*   **Response Variable:** $Y$, continuously measured and nonnegative ($Y \\ge 0$).\n*   **Dataset:** $n=320$ observations, denoted as $\\{(x_i, y_i)\\}_{i=1}^{n}$.\n*   **Distribution of $X$:** Right-skewed, with sample skewness approximately $1.1$.\n*   **Outliers in $X$:** An Interquartile Range (IQR) check flags $5$ mild high-end $x_i$ outliers.\n*   **Sample Pearson Correlation:** $r_{XY} = 0.43$.\n*   **Sample Spearman Rank Correlation:** $\\rho_{XY} = 0.87$.\n*   **Effect of Trimming:** After trimming the largest $1\\%$ of $x_i$ values, the correlations are approximately unchanged: Pearson correlation becomes $0.42$ and Spearman correlation becomes $0.86$.\n*   **Other Issues:** No other data quality issues are detected.\n*   **Question:** What is the most plausible diagnostic conclusion about the relationship between $X$ and $Y$, and what is the most appropriate next modeling step for predicting $Y$ from $X$?\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding:** The problem is firmly grounded in the principles of statistics and data analysis. It uses standard, well-defined concepts such as Pearson and Spearman correlation, skewness, outliers, and regression modeling. All concepts are part of standard statistical learning curricula. The scenario is scientifically sound.\n2.  **Well-Posedness:** The problem provides a set of statistical measures and asks for the most plausible interpretation and subsequent action. The provided data is sufficient to draw a reasoned conclusion among the choices. A meaningful and likely unique diagnostic conclusion can be reached.\n3.  **Objectivity:** The problem statement is objective and quantitative. It provides numerical values for sample size, correlations, and skewness, and describes the outcome of a standard EDA procedure (trimming). It avoids subjective or ambiguous language.\n4.  **Consistency and Completeness:** The provided information is internally consistent.\n    *   The sample size is $n=320$. Trimming $1\\%$ means removing approximately $0.01 \\times 320 = 3.2$ data points, which we can interpret as removing the top $3$ or $4$ observations of $X$. This is consistent with the finding of $5$ mild high-end outliers.\n    *   The large discrepancy between a high Spearman correlation ($\\rho_{XY}=0.87$) and a moderate Pearson correlation ($r_{XY}=0.43$) is a classic statistical pattern that has well-known interpretations.\n    *   The fact that the correlations are stable after trimming the largest $x_i$ values is a crucial piece of information that is consistent with the other data and helps distinguish between potential causes. There are no contradictions. The problem is self-contained.\n5.  **Realism:** The scenario is highly realistic. In applied data analysis, it is common to encounter skewed predictors and non-linear relationships, leading to exactly this kind of discrepancy between Pearson and Spearman correlations. The values themselves are plausible.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and internally consistent. I will proceed with the solution.\n\n### Derivation and Option Analysis\n\nThe core of the problem lies in interpreting the collection of statistical evidence, primarily the stark difference between the Pearson correlation coefficient ($r_{XY}=0.43$) and the Spearman rank correlation coefficient ($\\rho_{XY}=0.87$).\n\n**Principle-Based Derivation**\n\n1.  **Interpreting Spearman Correlation:** The Spearman correlation coefficient, $\\rho$, measures the strength and direction of a monotonic relationship. It is calculated as the Pearson correlation coefficient of the ranked variables. A value of $\\rho_{XY} = 0.87$ is very close to $1$, indicating a strong positive monotonic relationship. This means that as $X$ increases, $Y$ consistently tends to increase (or at least not decrease).\n\n2.  **Interpreting Pearson Correlation:** The Pearson correlation coefficient, $r$, measures the strength and direction of a *linear* relationship. A value of $r_{XY} = 0.43$ indicates a moderate positive linear relationship.\n\n3.  **Reconciling the Discrepancy:** The significant discrepancy ($\\rho_{XY} \\gg r_{XY}$) is the most informative clue. It implies that the relationship between $X$ and $Y$ is strong and monotonic, but not linear. If the relationship were linear, the Pearson and Spearman correlations would be very close in value. A classic example of a relationship producing this signature is a curve, for instance, a logarithmic or power-law relationship, which is monotonic but not linear. A scatterplot of such data would show a clear trend following a curve, for which a straight line is a poor fit, resulting in a modest $r$ value. However, the ranks would still be highly correlated, resulting in a high $\\rho$ value.\n\n4.  **Analyzing the Role of Outliers:** Outliers can have a strong influence on the Pearson correlation while leaving the Spearman correlation relatively unaffected. This could be a potential explanation for the discrepancy. However, the problem provides a direct test of this hypothesis: trimming the largest $1\\%$ of $x_i$ values (which includes the most extreme values in the right-skewed distribution) has a negligible effect on the correlations ($r$ changes from $0.43$ to $0.42$, $\\rho$ from $0.87$ to $0.86$). This result robustly demonstrates that the low Pearson correlation is *not* an artifact of a few influential outliers. The cause must be the intrinsic non-linear structure of the relationship across the bulk of the data.\n\n5.  **Synthesizing a Diagnosis and Next Step:** The evidence overwhelmingly points to a strong, monotonic, non-linear increasing relationship between $X$ and $Y$. The right-skewness of $X$ and non-negativity of $Y$ are consistent with a concave relationship (e.g., $Y \\propto \\log(X)$) or a convex one (e.g., $Y \\propto \\exp(X)$). Given this diagnosis, a simple linear regression of $Y$ on $X$ is inappropriate as it would fail to capture the curvature, leading to biased predictions and incorrect inferences. The appropriate next step is to either:\n    a. Apply a monotonic transformation to one or both variables to linearize the relationship. Given that $X$ is right-skewed, a transformation such as $\\log(X)$ or $\\sqrt{X}$ is a standard and promising approach. One would then fit a linear model to the transformed data.\n    b. Use a model that can intrinsically capture a monotonic, non-linear relationship without transformation. Examples include isotonic regression or a Generalized Additive Model (GAM) with a shape-constrained spline (specifically, a monotonically increasing spline).\n\n**Option-by-Option Analysis**\n\n**A. The relationship is approximately linear with homoscedastic noise; the Pearson–Spearman discrepancy is due to sampling variability. Proceed with ordinary least squares (OLS) without transformation.**\n*   **Analysis:** This assertion is incorrect. The discrepancy between $r_{XY}=0.43$ and $\\rho_{XY}=0.87$ is far too large to be attributed to sampling variability, especially with a sample size of $n=320$. The high Spearman correlation directly contradicts the claim that the relationship is \"approximately linear.\" Proceeding with OLS would be a modeling error.\n*   **Verdict:** Incorrect.\n\n**B. The relationship is monotonic but nonlinear (for example, concave increasing). The high rank correlation with moderate Pearson suggests using a monotonic transformation of $X$ (such as $\\log$) or a monotonic model (for example, isotonic regression or a shape-constrained spline).**\n*   **Analysis:** This option aligns perfectly with our derivation. It correctly diagnoses the relationship as \"monotonic but nonlinear\" based on the correlation discrepancy. The example of a \"concave increasing\" relationship is a plausible scenario. Most importantly, it proposes the two most appropriate classes of modeling strategies: applying a linearizing monotonic transformation (like $\\log$) or using a flexible monotonic model (like isotonic regression or splines). This is the best course of action.\n*   **Verdict:** Correct.\n\n**C. $X$ and $Y$ are independent; the high Spearman rank correlation is an artifact of skewness. Avoid using $X$ in the model.**\n*   **Analysis:** This statement is fundamentally wrong. A Spearman correlation of $0.87$ provides extremely strong evidence *against* independence. Skewness itself does not create a strong monotonic relationship where none exists. Discarding $X$ as a predictor would mean ignoring a very strong, albeit non-linear, signal.\n*   **Verdict:** Incorrect.\n\n**D. The association is driven by a few extreme outliers; winsorize $X$ and fit a linear OLS model on the winsorized data.**\n*   **Analysis:** This hypothesis is directly refuted by the evidence in the problem statement. The experiment of trimming the top $1\\%$ of $x_i$ values showed that the correlations were stable. This proves the association is not driven by these few extreme values. Therefore, the premise for this action is false.\n*   **Verdict:** Incorrect.\n\n**E. The relationship is non-monotonic (for example, U-shaped), so a high-degree polynomial (for example, degree $5$) is preferred; the Spearman value being high is consistent with such non-monotonicity under skewness.**\n*   **Analysis:** This is incorrect. A high Spearman correlation is the primary indicator of a *monotonic* relationship. A non-monotonic relationship like a U-shape would produce a Spearman correlation close to $0$, because the ranks of $Y$ would first decrease and then increase as the ranks of $X$ increase, breaking any monotonic trend. The claim that high $\\rho$ is consistent with non-monotonicity is false. Consequently, modeling with a complex polynomial is not indicated by the data.\n*   **Verdict:** Incorrect.", "answer": "$$\\boxed{B}$$", "id": "3120045"}, {"introduction": "Real-world datasets often contain features measured on vastly different scales, from annual incomes in the tens of thousands to click-through rates as small proportions. This practice explores how such disparities can unintentionally bias sophisticated models like ridge and LASSO regression, which apply a penalty to coefficient magnitudes [@problem_id:3120036]. You will see how exploratory findings on feature distributions directly inform essential preprocessing steps like standardization, ensuring that the regularization penalty is applied fairly across all predictors.", "problem": "A company has collected a dataset with $n=500$ customers, a continuous response variable $Y$ (annual spending), and four predictors $X_1, X_2, X_3, X_4$. An exploratory data analysis (EDA) was performed by plotting histograms, kernel density estimates, and box plots for each $X_j$ and computing their empirical ranges and spread. The findings are summarized as follows.\n\n$X_1$ (annual income in dollars): approximately uniform on $[2 \\times 10^4, 2 \\times 10^5]$, sample mean $\\approx 8.0 \\times 10^4$, sample standard deviation $\\approx 3.0 \\times 10^4$.\n\n$X_2$ (click-through rate as a proportion): concentrated near $0$ with a long right tail on $[0,1]$, sample mean $\\approx 8 \\times 10^{-2}$, sample standard deviation $\\approx 5 \\times 10^{-2}$.\n\n$X_3$ (number of purchases per year): highly skewed right with several outliers near $200$, empirical range $[0, 200]$, sample mean $\\approx 12$, sample standard deviation $\\approx 20$.\n\n$X_4$ (survey satisfaction score on a $10$-point scale): approximately symmetric on $[0,10]$, sample mean $\\approx 6.5$, sample standard deviation $\\approx 1.5$.\n\nYou plan to fit linear models with regularization to predict $Y$ from $(X_1, X_2, X_3, X_4)$ using ridge regression and the least absolute shrinkage and selection operator (LASSO). Your colleague suggests “skip standardization because regularization will handle everything.” Based solely on the EDA, reason from the definitions of these methods and the observed scales and distributions to predict how the lack of standardization would differentially affect ridge versus LASSO, and propose a preprocessing plan that is justified by the EDA.\n\nWhich option most accurately states the expected impact of omitting standardization and gives an EDA-informed preprocessing plan?\n\nA. Without standardization, LASSO will tend to favor predictors measured on larger scales (for example $X_1$ and $X_3$) because their coefficients can be smaller in magnitude for the same predictive contribution, reducing the $\\ell_1$ penalty relative to small-scale features (for example $X_2$). Ridge will also be sensitive to scale but will shrink coefficients continuously rather than setting them exactly to zero, producing uneven shrinkage across features. A justified preprocessing plan is to center and scale all predictors to zero mean and unit variance, and additionally apply a robust transformation or scaling to handle skewness and outliers (for example, log or a median–interquartile-range scaling for $X_3$), while inspecting $X_2$ for bounded-proportion considerations.\n\nB. Ridge regression is invariant to feature scaling because it penalizes squared residuals, but LASSO is harmed only by multicollinearity. Therefore, do not standardize; instead, drop outliers from $X_3$ and leave the rest unchanged.\n\nC. Both ridge and LASSO are invariant to the measurement units of the predictors, so omitting standardization has no effect on either method. The best preprocessing is to convert $X_1$ from dollars to thousands of dollars and to leave $X_2, X_3, X_4$ as-is.\n\nD. Without standardization, LASSO will more often zero-out large-scale predictors (for example $X_1$ and $X_3$) because their coefficients are larger and incur more penalty, whereas small-scale predictors (for example $X_2$) are protected. Ridge is unaffected by scale. The best preprocessing is min–max normalization of only $X_2$ to $[0,1]$ since it is already bounded.", "solution": "### Problem Validation\nThe problem asks for an analysis of the effect of feature scaling on regularized linear models (ridge and LASSO) and to propose a preprocessing plan based on EDA findings. The concepts are standard in statistical learning, the data presented is realistic, and the question is well-posed and objective. The problem is valid.\n\n### Derivation and Option Analysis\n**1. Principles of Regularization and Scaling**\n\nBoth ridge regression and the least absolute shrinkage and selection operator (LASSO) augment the ordinary least squares (OLS) loss function with a penalty term on the coefficient magnitudes. We seek to find coefficients $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)$ that minimize an objective function.\n-   **LASSO Penalty:** $\\lambda \\sum_{j} |\\beta_j|$ ($\\ell_1$ norm)\n-   **Ridge Penalty:** $\\lambda \\sum_{j} \\beta_j^2$ ($\\ell_2$ norm)\n\nThe key insight is that the magnitude of a coefficient $\\beta_j$ is not an intrinsic measure of feature importance; it is contingent on the scale (units) of its corresponding predictor $X_j$. For a predictor measured on a large scale (e.g., $X_1$, annual income with standard deviation $\\approx 3 \\times 10^4$), a small-valued coefficient can produce a substantial change in the model's output. Conversely, for a predictor on a small scale (e.g., $X_2$, click-through rate with standard deviation $\\approx 5 \\times 10^{-2}$), a very large coefficient is required to produce a similar change.\n\nWithout standardization, the regularization penalty is applied inequitably. The model will unfairly penalize features that require large coefficients simply due to their small scale (like $X_2$), shrinking them more aggressively toward zero. Conversely, it will be lenient on large-scale features (like $X_1$) that have naturally small coefficients. This makes the model's feature selection (for LASSO) and coefficient shrinkage (for both) dependent on the arbitrary choice of units, not the true predictive power of the features. The colleague's suggestion is therefore fundamentally flawed; both methods are highly sensitive to feature scaling.\n\n**2. EDA-Informed Preprocessing Plan**\n\nThe EDA reveals vast differences in the scales of the predictors, as measured by their standard deviations:\n-   $\\sigma(X_1) \\approx 30,000$\n-   $\\sigma(X_2) \\approx 0.05$\n-   $\\sigma(X_3) \\approx 20$\n-   $\\sigma(X_4) \\approx 1.5$\n\nA sound preprocessing plan must address this.\n-   **Standardization:** The primary step is to standardize all predictors to put them on a common scale. The most common method is Z-score standardization (center to mean 0, scale to standard deviation 1). This ensures the regularization penalty is applied fairly.\n-   **Handling Skewness and Outliers:** The EDA notes that $X_3$ is highly right-skewed with outliers. Standard Z-scoring is sensitive to outliers (which inflate the mean and standard deviation). A more robust approach for $X_3$ is justified, such as applying a transformation to reduce skewness (e.g., $\\log(X_3 + c)$) before scaling, or using a robust scaler based on the median and interquartile range (IQR).\n-   **Bounded Variables:** For $X_2$, which is a proportion, its bounded nature and skewness might warrant special consideration, though standard scaling is often a sufficient first step.\n\n**3. Option-by-Option Analysis**\n\n*   **A:** This option accurately describes the effect of omitting standardization: large-scale predictors are favored (less penalized) because their coefficients are smaller. It correctly states that both Ridge and LASSO are sensitive to scale. The proposed preprocessing plan is excellent and directly addresses the issues identified in the EDA: standard scaling for all predictors as a baseline, with specific, appropriate remedies for the skewed/outlier-prone predictor $X_3$. This is the best choice.\n*   **B:** This statement contains multiple fundamental errors. Ridge regression is not invariant to feature scaling; its $\\ell_2$ penalty on coefficients makes it highly scale-dependent. LASSO's sensitivity is not limited to multicollinearity; it is highly scale-dependent. The conclusion to not standardize is incorrect.\n*   **C:** This statement is entirely false. As derived above, both methods are heavily dependent on the predictors' scales.\n*   **D:** This statement has the core logic reversed. Large-scale predictors have smaller coefficients for a given predictive effect and thus incur a smaller penalty; they are less likely to be zeroed-out. Small-scale predictors require larger coefficients and are penalized more heavily. The claim that Ridge is unaffected by scale is also false.", "answer": "$$\\boxed{A}$$", "id": "3120036"}, {"introduction": "Exploratory data analysis is a powerful creative process, but its very flexibility can be a double-edged sword, leading to what is known as the \"garden of forking paths\" where spurious discoveries are likely. This practice delves into the critical statistical issue of multiplicity in EDA, quantifying how looking at the data in many different ways inflates the rate of false discoveries [@problem_id:3120044]. By working through this exercise, you will understand the importance of disciplined protocols, such as pre-specifying analyses and using multiplicity corrections, to ensure your findings are robust and reproducible.", "problem": "A researcher performs exploratory data analysis (EDA) and then simple statistical learning on a dataset with response variable $Y$ and $m = 100$ standardized features $\\{X_1, \\dots, X_{100}\\}$. The researcher initially explores visual patterns by plotting $Y$ against each $X_j$ and, to enhance visual fit, tries $5$ distinct transformations per feature (for example, identity, logarithm, square root, polynomial of degree $2$, and winsorization), resulting in $5$ different scatterplots and corresponding simple linear regressions per feature. For each feature, the researcher selects the transformation producing the smallest $p$-value from the regression and flags the feature as “interesting” if that minimal $p$-value is below a nominal threshold $\\alpha = 0.05$. Assume the following base conditions for analysis:\n\n- Under the global null hypothesis in which all features are unrelated to $Y$, each regression $p$-value is independent and uniformly distributed on $[0,1]$.\n- A Type I error for a single test occurs with probability $\\alpha$.\n- Family-Wise Error Rate (FWER) is the probability of at least one Type I error across a family of tests.\n- False Discovery Rate (FDR) is the expected proportion of false discoveries among all discoveries.\n- Bonferroni correction aims to control FWER across a family of tests by tightening the per-test threshold.\n- The Benjamini–Hochberg (BH) procedure aims to control FDR at a target level $q$.\n\nConsider the multiplicity induced by inspecting many features and, for each feature, many transformations, and the researcher’s choice to report only the “best-looking” transformation per feature. Also consider how disciplined EDA protocols could mitigate false discoveries in the presence of such “garden of forking paths.”\n\nSelect all statements that are correct:\n\nA. Under the global null with $m = 100$ independent features and per-feature nominal threshold $\\alpha = 0.05$, the probability of observing at least one spurious significant result among the $100$ tests is approximately $0.994$.\n\nB. If, for each of the $100$ features, the researcher tries $5$ independent transformations and selects the smallest $p$-value per feature, then using the uncorrected threshold $\\alpha = 0.05$, the expected number of flagged features under the global null is approximately $5$.\n\nC. Applying a Bonferroni correction to control FWER at $\\alpha = 0.05$ across the $500$ transformation-specific tests yields a per-test significance threshold of $0.0001$.\n\nD. Under the Benjamini–Hochberg procedure targeting FDR $q = 0.10$ with $500$ independent tests where some features are truly associated with $Y$, the probability of at least one false discovery is guaranteed to be at most $0.10$.\n\nE. A disciplined EDA protocol that preregisters feature families and transformation sets, uses a holdout sample to confirm visually discovered patterns, and reports multiplicity-adjusted $p$-values can materially reduce false discoveries compared to ad hoc forking paths.", "solution": "The user has provided a problem concerning exploratory data analysis and its intersection with statistical inference, specifically focusing on the issues of multiple comparisons and false discoveries. The task is to validate the problem statement and then evaluate five assertions about the scenario described.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- Response variable: $Y$\n- Number of standardized features: $m = 100$, denoted $\\{X_1, \\dots, X_{100}\\}$\n- Analysis procedure per feature $X_j$: $5$ distinct transformations are applied. For each transformation, a simple linear regression of $Y$ on the transformed $X_j$ is performed.\n- Selection criterion: For each feature, the transformation yielding the smallest regression $p$-value is selected.\n- Definition of \"interesting\" feature: A feature is flagged if its minimal $p$-value (from the best transformation) is less than $\\alpha = 0.05$.\n- Base assumptions under the global null hypothesis (all features are unrelated to $Y$):\n    - Each regression $p$-value is independent.\n    - Each regression $p$-value is uniformly distributed on $[0,1]$.\n- Definitions:\n    - Type I error for a single test occurs with probability $\\alpha$.\n    - Family-Wise Error Rate (FWER): Probability of at least one Type I error.\n    - False Discovery Rate (FDR): Expected proportion of false discoveries among all discoveries.\n    - Bonferroni correction: Aims to control FWER.\n    - Benjamini–Hochberg (BH) procedure: Aims to control FDR at level $q$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is firmly based on established principles of statistics, including hypothesis testing, $p$-values, Type I errors, and multiple comparison corrections (FWER, FDR, Bonferroni, BH). The \"garden of forking paths\" is a well-documented issue in scientific research, making the scenario both relevant and scientifically sound. The assumptions (independence and uniform distribution of $p$-values under the null) are standard theoretical conditions for analyzing such problems.\n- **Well-Posed:** The problem is clearly stated and provides all necessary information to evaluate the correctness of the given statements. Each statement poses a question that can be answered through logical deduction and standard statistical calculations.\n- **Objective:** The problem is expressed in objective, technical language. The definitions and conditions are precise and unambiguous.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, with no internal contradictions or missing information that would preclude a rigorous analysis. I will now proceed with the solution.\n\n### Derivation and Option-by-Option Analysis\n\nThe core of the problem lies in understanding the consequences of multiple hypothesis testing, both explicit (testing $m=100$ features) and implicit (testing $5$ transformations per feature and selecting the best result).\n\n**A. Under the global null with $m = 100$ independent features and per-feature nominal threshold $\\alpha = 0.05$, the probability of observing at least one spurious significant result among the $100$ tests is approximately $0.994$.**\n\nThis statement simplifies the scenario to one test per feature, ignoring the multiple transformations for its self-contained analysis. Let's calculate the Family-Wise Error Rate (FWER) under these conditions.\n\nThe probability of a single test not being significant (not making a Type I error) under the null hypothesis is $1 - \\alpha$. Given $\\alpha = 0.05$, this probability is $1 - 0.05 = 0.95$.\n\nWe have $m = 100$ independent tests. The probability that none of these tests are significant is the product of the individual probabilities of non-significance:\n$$P(\\text{no significant results}) = (1 - \\alpha)^m = (0.95)^{100}$$\n\nThe FWER is the probability of at least one significant result, which is $1$ minus the probability of no significant results:\n$$FWER = 1 - P(\\text{no significant results}) = 1 - (0.95)^{100}$$\n\nUsing a calculator:\n$$(0.95)^{100} \\approx 0.0059205$$\n$$FWER = 1 - 0.0059205 \\approx 0.9940795$$\nThe calculated probability is approximately $0.994$, which matches the statement.\n\n**Verdict for A: Correct.**\n\n**B. If, for each of the $100$ features, the researcher tries $5$ independent transformations and selects the smallest $p$-value per feature, then using the uncorrected threshold $\\alpha = 0.05$, the expected number of flagged features under the global null is approximately $5$.**\n\nThis statement addresses the full scenario, including the multiple transformations. Let's first determine the true probability of flagging a single feature under the global null hypothesis. For a given feature, $5$ independent $p$-values $\\{p_1, p_2, p_3, p_4, p_5\\}$ are generated, each from a $U(0,1)$ distribution. The researcher selects $p_{\\text{min}} = \\min(p_1, \\dots, p_5)$. The feature is flagged if $p_{\\text{min}}  0.05$.\n\nThe probability of this event, let's call it $\\alpha'$, is:\n$$\\alpha' = P(p_{\\text{min}}  0.05)$$\nIt is easier to calculate the complement:\n$$1 - \\alpha' = P(p_{\\text{min}} \\ge 0.05)$$\nThis means all $5$ $p$-values must be greater than or equal to $0.05$:\n$$P(p_{\\text{min}} \\ge 0.05) = P(p_1 \\ge 0.05 \\text{ and } \\dots \\text{ and } p_5 \\ge 0.05)$$\nSince the $p$-values are independent and from $U(0,1)$, $P(p_i \\ge 0.05) = 1 - 0.05 = 0.95$.\n$$P(p_{\\text{min}} \\ge 0.05) = (0.95)^5 \\approx 0.77378$$\nTherefore, the actual Type I error rate for flagging a feature after this selection procedure is:\n$$\\alpha' = 1 - (0.95)^5 \\approx 1 - 0.77378 = 0.22622$$\nThe number of flagged features, let's call it $S$, follows a binomial distribution $S \\sim \\text{Bin}(m, \\alpha')$, with $m = 100$. The expected number of flagged features is:\n$$E[S] = m \\times \\alpha' = 100 \\times (1 - (0.95)^5) \\approx 100 \\times 0.22622 = 22.622$$\nThe statement claims the expected number is approximately $5$. This would only be true if the per-feature Type I error rate remained at $0.05$, i.e., $100 \\times 0.05 = 5$. This fails to account for the inflation of the error rate due to selecting the minimum of $5$ $p$-values. The actual expected number is much higher, at approximately $22.6$.\n\n**Verdict for B: Incorrect.**\n\n**C. Applying a Bonferroni correction to control FWER at $\\alpha = 0.05$ across the $500$ transformation-specific tests yields a per-test significance threshold of $0.0001$.**\n\nThe Bonferroni correction is a method to control the FWER. If a family of $K$ hypotheses is tested, the FWER is controlled at or below a level $\\alpha_{FWER}$ by testing each individual hypothesis at a significance level of $\\alpha_{\\text{ind}} = \\alpha_{FWER} / K$.\n\nIn this scenario, the researcher is implicitly conducting a family of tests. There are $m = 100$ features and $k = 5$ transformations per feature. The total number of regression tests performed is:\n$$K = m \\times k = 100 \\times 5 = 500$$\nTo control the FWER at $\\alpha_{FWER} = 0.05$ across these $500$ tests, the Bonferroni-corrected threshold for each individual test would be:\n$$\\alpha_{\\text{ind}} = \\frac{\\alpha_{FWER}}{K} = \\frac{0.05}{500} = \\frac{5 \\times 10^{-2}}{5 \\times 10^2} = 1 \\times 10^{-4} = 0.0001$$\nThe calculation is correct. This is the standard application of the Bonferroni correction.\n\n**Verdict for C: Correct.**\n\n**D. Under the Benjamini–Hochberg procedure targeting FDR $q = 0.10$ with $500$ independent tests where some features are truly associated with $Y$, the probability of at least one false discovery is guaranteed to be at most $0.10$.**\n\nThis statement confuses the False Discovery Rate (FDR) with the Family-Wise Error Rate (FWER).\n- **FWER** is the probability of making at least one false discovery (Type I error). We write this as $P(V \\ge 1)$, where $V$ is the number of false discoveries.\n- **FDR** is the expected proportion of false discoveries among all discoveries. We write this as $FDR = E[V/R]$, where $R$ is the total number of discoveries (rejections of the null). By convention, $V/R = 0$ if $R=0$.\n\nThe Benjamini–Hochberg (BH) procedure controls the FDR, guaranteeing that $FDR \\le q$ (under independence or positive regression dependency of the test statistics). It does *not* control the FWER. The quantity described in the statement, \"the probability of at least one false discovery\", is the definition of FWER.\n\nIn general, controlling FDR at level $q$ does not imply that FWER is controlled at level $q$. In fact, when there are true discoveries (as the statement assumes), it is typical for $FWER > FDR$. For instance, if $100$ discoveries are made, an FDR of $0.10$ means we expect $10$ of them to be false. In such a case, the probability of at least one false discovery, $P(V \\ge 1)$, would be extremely high, certainly not less than or equal to $0.10$. The BH procedure is less conservative than FWER-controlling procedures, allowing more discoveries at the cost of a higher chance of making at least one Type I error. The statement conflates these two distinct error metrics.\n\n**Verdict for D: Incorrect.**\n\n**E. A disciplined EDA protocol that preregisters feature families and transformation sets, uses a holdout sample to confirm visually discovered patterns, and reports multiplicity-adjusted $p$-values can materially reduce false discoveries compared to ad hoc forking paths.**\n\nThis statement describes a set of best practices in statistical research designed to enhance reproducibility and reduce spurious findings. Let's analyze each component:\n1.  **Preregistration of feature families and transformation sets:** This directly combats the \"garden of forking paths\" by requiring the researcher to specify the exact hypotheses to be tested *before* analyzing the data. This prevents data-driven selection of analyses, which, as shown in option B, inflates the Type I error rate.\n2.  **Use of a holdout sample:** This is the principle of train/test or exploration/confirmation split. A pattern discovered in an \"exploration\" set is a hypothesis. To be validated, this hypothesis must be tested on a separate \"confirmation\" (holdout) set of data that was not used in the initial discovery. This provides an unbiased assessment of the finding and is a powerful defense against capitalizing on chance.\n3.  **Reporting multiplicity-adjusted $p$-values:** This acknowledges that multiple tests were performed and correctly adjusts the statistical inference to account for this. Methods like Bonferroni or BH (options C and D) are designed for this purpose. It provides an honest measure of statistical significance.\n\nEach of these three practices—preregistration, holdout validation, and multiplicity adjustment—is a robust tool against false discoveries. A protocol combining them is the methodological standard for rigorous, hypothesis-generating research. It stands in stark contrast to an \"ad hoc\" approach where analysis choices are made post-data, which is known to dramatically increase the rate of false positives. Therefore, the statement is a correct and important assertion about sound scientific methodology.\n\n**Verdict for E: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "3120044"}]}