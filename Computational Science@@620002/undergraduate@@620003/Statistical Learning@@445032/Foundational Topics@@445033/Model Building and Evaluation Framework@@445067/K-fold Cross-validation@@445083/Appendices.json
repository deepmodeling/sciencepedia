{"hands_on_practices": [{"introduction": "Beyond selecting between different models, k-fold cross-validation is a powerful tool for tuning the hyperparameters of a single model pipeline. This first practice focuses on a common and important application: determining the optimal decision threshold for a binary classifier. By comparing a threshold derived from the full dataset to one aggregated from separate validation folds, you will gain direct experience in using CV for robust parameter estimation and see how it helps protect against overfitting to the specificities of your dataset [@problem_id:3139044].", "problem": "You are given binary classification outputs from a deep learning model in the form of predicted probabilities and true labels. Consider a decision rule parameterized by a threshold $t \\in [0,1]$ defined as $\\hat{y}_i(t) = 1$ if $p_i \\ge t$ and $\\hat{y}_i(t) = 0$ otherwise, where $\\mathbf{p} = (p_1,\\dots,p_N)$ are predicted probabilities and $\\mathbf{y} = (y_1,\\dots,y_N)$ are true labels with $y_i \\in \\{0,1\\}$. Define the confusion counts for threshold $t$ as $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, and $\\mathrm{FN}(t)$ for true positives, false positives, and false negatives, respectively.\n\nThe weighted $F$-score, $F_\\beta$, is defined for $\\beta > 0$ using the well-tested formulas for precision and recall. A count-based form that is well-defined under edge cases is\n$$\nF_\\beta(t) = \\frac{(1+\\beta^2)\\,\\mathrm{TP}(t)}{(1+\\beta^2)\\,\\mathrm{TP}(t) + \\beta^2\\,\\mathrm{FN}(t) + \\mathrm{FP}(t)}.\n$$\nYou will implement threshold selection for $F_\\beta$ using $k$-fold cross-validation (CV), compare an aggregated threshold against a global threshold, and examine effects under class imbalance. Use the following foundational base: the definitions of precision, recall, and $F_\\beta$, the binary decision rule $\\hat{y}_i(t)$, and the standard $k$-fold CV procedure that partitions indices into $k$ contiguous folds of as equal size as possible, differing by at most one.\n\nAlgorithmic requirements:\n- For any dataset $(\\mathbf{p},\\mathbf{y})$, define the global threshold $t^*$ as the value of $t$ that maximizes $F_\\beta(t)$ on the full dataset. Justify and implement a discrete search over the candidate set of thresholds consisting of all unique predicted probabilities in $\\mathbf{p}$, augmented with $0$ and $1+\\epsilon$ for $\\epsilon = 10^{-6}$. This is valid because the decision rule changes only when $t$ crosses a predicted probability, making $F_\\beta(t)$ piecewise constant between these points.\n- Resolve ties by choosing the smallest threshold among those that attain the maximum $F_\\beta$.\n- For $k$-fold cross-validation, partition the index set $\\{1,\\dots,N\\}$ into $k$ contiguous folds whose sizes differ by at most one (the first $r = N \\bmod k$ folds have size $\\lfloor N/k \\rfloor + 1$, and the remaining have size $\\lfloor N/k \\rfloor$). For each fold $j \\in \\{1,\\dots,k\\}$, compute the fold-optimal threshold $t_j$ that maximizes $F_\\beta(t)$ using only that fold's validation data. Define the aggregated threshold as $\\bar{t} = \\frac{1}{k}\\sum_{j=1}^k t_j$.\n- Compare the global threshold $t^*$ and aggregated threshold $\\bar{t}$ by evaluating $F_\\beta(t^*)$ and $F_\\beta(\\bar{t})$ on the full dataset.\n\nTest suite:\n- Case A (balanced, $k=5$, $\\beta=1$):\n  - $\\mathbf{y} = [$ $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$ $]$.\n  - $\\mathbf{p} = [$ $0.92$, $0.18$, $0.88$, $0.32$, $0.83$, $0.41$, $0.77$, $0.47$, $0.71$, $0.52$, $0.66$, $0.56$, $0.61$, $0.60$, $0.59$, $0.62$, $0.58$, $0.63$, $0.54$, $0.68$ $]$.\n- Case B (imbalanced, $k=3$, $\\beta=2$):\n  - $\\mathbf{y}$ has $N = 30$ with positives at indices $3$, $7$, $12$, $18$, $24$, $29$ (using $0$-based indexing for description): $\\mathbf{y} = [$ $0$, $0$, $0$, $1$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $1$ $]$.\n  - $\\mathbf{p} = [$ $0.05$, $0.15$, $0.20$, $0.90$, $0.25$, $0.30$, $0.35$, $0.87$, $0.40$, $0.45$, $0.50$, $0.55$, $0.89$, $0.58$, $0.62$, $0.66$, $0.70$, $0.74$, $0.86$, $0.78$, $0.60$, $0.52$, $0.48$, $0.44$, $0.92$, $0.42$, $0.38$, $0.34$, $0.80$, $0.88$ $]$.\n- Case C (extreme imbalance, $k=5$, $\\beta=0.5$):\n  - $\\mathbf{y}$ has $N = 15$ with a single positive at index $11$: $\\mathbf{y} = [$ $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$ $]$.\n  - $\\mathbf{p} = [$ $0.10$, $0.20$, $0.30$, $0.40$, $0.45$, $0.50$, $0.55$, $0.60$, $0.65$, $0.85$, $0.88$, $0.95$, $0.70$, $0.72$, $0.75$ $]$.\n\nYour program must:\n- Implement the threshold search and $k$-fold CV aggregation exactly as specified, including the tie-breaking rule and the candidate set augmentation with $\\epsilon = 10^{-6}$.\n- For each test case, compute the global threshold $t^*$, aggregated threshold $\\bar{t}$, and the corresponding scores $F_\\beta(t^*)$ and $F_\\beta(\\bar{t})$ evaluated on the full dataset.\n\nFinal output format:\n- Produce a single line of output containing the results for the three test cases as a flat comma-separated list enclosed in square brackets. The list must be\n  - [$t^*_A$, $\\bar{t}_A$, $F_\\beta(t^*_A)$, $F_\\beta(\\bar{t}_A)$, $t^*_B$, $\\bar{t}_B$, $F_\\beta(t^*_B)$, $F_\\beta(\\bar{t}_B)$, $t^*_C$, $\\bar{t}_C$, $F_\\beta(t^*_C)$, $F_\\beta(\\bar{t}_C)$],\n  with each numeric value formatted to six decimal places.", "solution": "The user has provided a well-defined computational problem in the domain of machine learning model evaluation. The problem is scientifically grounded, formalizable, and self-contained. All necessary data, definitions, and algorithmic procedures are specified without ambiguity. Therefore, the problem is valid and a solution will be provided.\n\nThe core of the problem is to compare two strategies for selecting a classification threshold, $t$, for a binary classifier. The performance of the classifier under a given threshold is measured by the weighted $F_\\beta$-score.\n\nFirst, let us formalize the components. We are given a set of $N$ true labels $\\mathbf{y} = (y_1, \\dots, y_N)$ where $y_i \\in \\{0,1\\}$, and a corresponding set of predicted probabilities $\\mathbf{p} = (p_1, \\dots, p_N)$ where $p_i \\in [0,1]$. A decision threshold $t$ is used to convert these probabilities into binary predictions $\\hat{\\mathbf{y}}(t) = (\\hat{y}_1(t), \\dots, \\hat{y}_N(t))$ according to the rule:\n$$\n\\hat{y}_i(t) = \\begin{cases} 1 & \\text{if } p_i \\ge t \\\\ 0 & \\text{if } p_i < t \\end{cases}\n$$\nBased on these predictions, we can compute the number of true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), and false negatives ($\\mathrm{FN}$) as functions of $t$:\n$$\n\\mathrm{TP}(t) = \\sum_{i=1}^N \\mathbf{1}(p_i \\ge t \\land y_i = 1)\n$$\n$$\n\\mathrm{FP}(t) = \\sum_{i=1}^N \\mathbf{1}(p_i \\ge t \\land y_i = 0)\n$$\n$$\n\\mathrm{FN}(t) = \\sum_{i=1}^N \\mathbf{1}(p_i < t \\land y_i = 1)\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. The problem provides a robust, count-based definition of the $F_\\beta$-score for a given $\\beta > 0$:\n$$\nF_\\beta(t) = \\frac{(1+\\beta^2)\\,\\mathrm{TP}(t)}{(1+\\beta^2)\\,\\mathrm{TP}(t) + \\beta^2\\,\\mathrm{FN}(t) + \\mathrm{FP}(t)}\n$$\nThis form is well-defined even when the total number of predicted positives or actual positives is zero, in which case the numerator becomes $0$. If the denominator is also $0$ (i.e., $\\mathrm{TP}=\\mathrm{FP}=\\mathrm{FN}=0$), the score is taken to be $0$.\n\nThe primary task is to implement an algorithm to find the optimal threshold $t$ that maximizes $F_\\beta(t)$. The problem mandates a specific search strategy. The search for an optimal $t$ does not require scanning the entire interval $[0,1]$. The values of $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, and $\\mathrm{FN}(t)$ only change when the threshold $t$ crosses one of the probability values $p_i$. Therefore, the function $F_\\beta(t)$ is piecewise constant. A complete search can be performed over a discrete set of candidate thresholds. The specified candidate set is composed of all unique probability scores in $\\mathbf{p}$, augmented with two special values: $0$ and $1+\\epsilon$ where $\\epsilon=10^{-6}$. The value $t=0$ accounts for the case where all samples are classified as positive, and $t=1+\\epsilon$ ensures all samples are classified as negative (as some $p_i$ could be exactly $1$). By searching over this sorted set of candidates, we can find the maximum $F_\\beta$ value. The problem specifies a tie-breaking rule: if multiple thresholds yield the same maximum $F_\\beta$ score, the smallest of these thresholds must be chosen.\n\nWith this optimization procedure defined, we can find two distinct types of thresholds:\n\n1.  **Global Threshold ($t^*$):** This threshold is found by applying the optimization procedure described above to the entire dataset $(\\mathbf{y}, \\mathbf{p})$. It represents the single best threshold for the observed data in its entirety.\n\n2.  **Aggregated Threshold ($\\bar{t}$):** This threshold is derived using $k$-fold cross-validation. The dataset's indices $\\{0, 1, \\dots, N-1\\}$ are partitioned into $k$ contiguous, non-overlapping folds. The partitioning is deterministic: the first $r = N \\bmod k$ folds contain $\\lfloor N/k \\rfloor + 1$ samples, and the remaining $k-r$ folds contain $\\lfloor N/k \\rfloor$ samples. For each fold $j \\in \\{1, \\dots, k\\}$, the data within that fold serves as a validation set. The optimal threshold for that fold, $t_j$, is found by maximizing $F_\\beta(t)$ using only the data points $(\\mathbf{y}_j, \\mathbf{p}_j)$ corresponding to fold $j$. The aggregated threshold is then computed as the arithmetic mean of these fold-specific thresholds:\n    $$\n    \\bar{t} = \\frac{1}{k} \\sum_{j=1}^k t_j\n    $$\nThis cross-validation approach simulates the process of finding a threshold on unseen data, and averaging the results provides a more robust estimate that is less prone to overfitting to a specific data split.\n\nFinally, the problem requires a comparison of the performance of these two thresholds, $t^*$ and $\\bar{t}$. The comparison is performed by evaluating the $F_\\beta$-score for each threshold on the **entire dataset**. This allows for a direct assessment of which threshold selection strategy yields a better result on the full distribution of observed data.\n\nThe implementation will consist of three main components: a function to calculate $F_\\beta(t)$, a function to find the optimal threshold for a given dataset, and a main routine to orchestrate the computation for each test case, including the k-fold splitting logic and final evaluation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's requirements for all test cases.\n    \"\"\"\n    \n    # Define test cases from the problem statement\n    case_a = {\n        \"y\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n        \"p\": [0.92, 0.18, 0.88, 0.32, 0.83, 0.41, 0.77, 0.47, 0.71, 0.52, 0.66, 0.56, 0.61, 0.60, 0.59, 0.62, 0.58, 0.63, 0.54, 0.68],\n        \"k\": 5,\n        \"beta\": 1.0,\n    }\n    \n    y_b = np.zeros(30, dtype=int)\n    pos_indices_b = [3, 7, 12, 18, 24, 29]\n    y_b[pos_indices_b] = 1\n    case_b = {\n        \"y\": y_b.tolist(),\n        \"p\": [0.05, 0.15, 0.20, 0.90, 0.25, 0.30, 0.35, 0.87, 0.40, 0.45, 0.50, 0.55, 0.89, 0.58, 0.62, 0.66, 0.70, 0.74, 0.86, 0.78, 0.60, 0.52, 0.48, 0.44, 0.92, 0.42, 0.38, 0.34, 0.80, 0.88],\n        \"k\": 3,\n        \"beta\": 2.0,\n    }\n    \n    y_c = np.zeros(15, dtype=int)\n    y_c[11] = 1\n    case_c = {\n        \"y\": y_c.tolist(),\n        \"p\": [0.10, 0.20, 0.30, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.85, 0.88, 0.95, 0.70, 0.72, 0.75],\n        \"k\": 5,\n        \"beta\": 0.5,\n    }\n\n    test_cases = [case_a, case_b, case_c]\n    epsilon = 1e-6\n    results = []\n\n    def calculate_f_beta(y, p, t, beta):\n        \"\"\"Calculates the F-beta score for a given threshold.\"\"\"\n        if y.size == 0:\n            return 0.0\n        \n        y_pred = (p >= t).astype(int)\n        \n        tp = np.sum((y_pred == 1) & (y == 1))\n        fp = np.sum((y_pred == 1) & (y == 0))\n        fn = np.sum((y_pred == 0) & (y == 1))\n        \n        beta_sq = beta**2\n        numerator = (1 + beta_sq) * tp\n        denominator = (1 + beta_sq) * tp + beta_sq * fn + fp\n        \n        if denominator == 0:\n            return 0.0\n        else:\n            return numerator / denominator\n\n    def find_optimal_threshold(y, p, beta, epsilon):\n        \"\"\"Finds the optimal threshold that maximizes F-beta score.\"\"\"\n        if y.size == 0:\n            return 0.0\n        \n        unique_p = np.unique(p)\n        candidate_thresholds = np.concatenate(([0.0], unique_p, [1.0 + epsilon]))\n        # Sorting is implicit from np.unique and concatenation order, but let's be explicit\n        candidate_thresholds.sort()\n        \n        best_t = candidate_thresholds[0]\n        max_f_beta = -1.0\n        \n        for t in candidate_thresholds:\n            f_beta_score = calculate_f_beta(y, p, t, beta)\n            if f_beta_score > max_f_beta:\n                max_f_beta = f_beta_score\n                best_t = t\n                \n        return best_t\n\n    def process_case(y, p, k, beta, epsilon):\n        \"\"\"Processes a single test case to find thresholds and scores.\"\"\"\n        y = np.array(y)\n        p = np.array(p)\n        N = len(y)\n        \n        # 1. Calculate global threshold t*\n        t_star = find_optimal_threshold(y, p, beta, epsilon)\n        \n        # 2. Calculate k-fold aggregated threshold t_bar\n        fold_thresholds = []\n        \n        n_per_fold = N // k\n        n_larger_folds = N % k\n        \n        current_idx = 0\n        for i in range(k):\n            fold_size = n_per_fold + 1 if i < n_larger_folds else n_per_fold\n            fold_indices = np.arange(current_idx, current_idx + fold_size)\n            \n            y_fold = y[fold_indices]\n            p_fold = p[fold_indices]\n            \n            t_j = find_optimal_threshold(y_fold, p_fold, beta, epsilon)\n            fold_thresholds.append(t_j)\n            \n            current_idx += fold_size\n            \n        t_bar = np.mean(fold_thresholds)\n        \n        # 3. Evaluate F-beta scores on the full dataset\n        f_beta_star = calculate_f_beta(y, p, t_star, beta)\n        f_beta_bar = calculate_f_beta(y, p, t_bar, beta)\n        \n        return t_star, t_bar, f_beta_star, f_beta_bar\n\n    for case in test_cases:\n        t_star, t_bar, f_beta_star, f_beta_bar = process_case(\n            case[\"y\"], case[\"p\"], case[\"k\"], case[\"beta\"], epsilon\n        )\n        results.extend([t_star, t_bar, f_beta_star, f_beta_bar])\n\n    # Format and print the final results\n    formatted_results = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3139044"}, {"introduction": "Data augmentation is an essential technique for improving model generalization, especially in fields like computer vision. However, its incorrect application within a cross-validation framework can lead to a critical error known as data leakage, which produces misleadingly optimistic performance estimates. This exercise tasks you with building a computational experiment to measure the difference in accuracy between a proper CV procedure and one where augmentation leaks across training and validation folds [@problem_id:3134696]. Implementing both scenarios will provide a concrete and memorable lesson on the importance of preserving the independence of the validation set.", "problem": "You are tasked with designing and implementing a computational experiment to analyze $k$-fold Cross-Validation (CV) in Statistical Learning under data augmentation with image rotations. The aim is to quantify how applying augmentation only to training folds compares to the scenario where augmentation leaks into validation folds. Your program must be a complete, runnable implementation that produces the requested outputs with no external input.\n\nFundamental base and definitions to be used:\n- Cross-Validation (CV): For a given dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ and a learning algorithm, the $k$-fold CV estimator of the generalization error is defined as the average validation loss over $k$ folds. Let the folds be index sets $V_1, \\dots, V_k$ that partition $\\{1,\\dots,n\\}$, and $T_j = \\{1,\\dots,n\\} \\setminus V_j$ denote the corresponding training indices. For a model $\\hat{f}_j$ trained on $T_j$, the CV estimator of accuracy is\n$$\n\\widehat{\\text{Acc}}_{\\text{CV}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V_j|} \\sum_{i \\in V_j} \\mathbf{1}\\{\\hat{f}_j(x_i) = y_i\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- Data augmentation: A label-preserving transformation $\\mathcal{A}_\\theta$ applied to an input $x$ parameterized by a rotation angle $\\theta$ measured in degrees. In this problem, $\\theta \\in \\{90, 180, 270\\}$, and $\\mathcal{A}_\\theta$ is image rotation by $\\theta$ degrees about the image center.\n- Independence principle: Proper CV requires that validation samples are independent of training samples conditional on the data-generating process. Leakage occurs when transformed versions of the same base sample appear in both training and validation folds, violating the independence assumption and biasing the estimator.\n\nDataset construction:\n- Consider $n$ base images $\\{x_i\\}_{i=1}^n$ of size $h \\times w$ with labels $y_i \\in \\{0,1\\}$, where class $0$ is a centered \"$+$\" pattern and class $1$ is a centered \"$\\times$\" pattern. Each base image is produced by perturbing the ideal template with small random offsets in line thickness and additive Gaussian noise with standard deviation $\\sigma$, then clipping pixel intensities to the interval $[0,1]$. Rotations by $\\theta \\in \\{90,180,270\\}$ preserve class labels.\n- The classifier is the nearest-centroid classifier under squared $\\ell_2$ distance. For a training set $(X_{\\text{train}}, Y_{\\text{train}})$, compute class centroids\n$$\n\\mu_c = \\frac{1}{|\\{i: Y_{\\text{train},i} = c\\}|} \\sum_{i:Y_{\\text{train},i}=c} X_{\\text{train},i}, \\quad c \\in \\{0,1\\},\n$$\nand predict $\\hat{y}(x) = \\arg\\min_{c \\in \\{0,1\\}} \\|x - \\mu_c\\|_2^2$.\n\nTwo CV estimators to compare:\n1. Proper augmentation CV: For each fold $j \\in \\{1,\\dots,k\\}$, augment only the training images by rotations in a given angle set $S$ (angles in degrees), i.e., use $\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}$ for $i \\in T_j$ to train $\\hat{f}_j$, and evaluate on the unaugmented validation images $\\{x_i: i \\in V_j\\}$.\n2. Leakage CV: First, globally augment the entire dataset by rotations in $S$ to form $D' = \\bigcup_{i=1}^n \\left(\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}\\right)$, then perform $k$-fold CV on $D'$, training and validating on these augmented samples. This procedure allows augmented variants of the same base image to appear across different folds, producing dependence between training and validation samples.\n\nYour program must:\n- Generate the synthetic dataset as specified.\n- Implement the nearest-centroid classifier.\n- Compute the average $k$-fold CV accuracy for both procedures and return the difference\n$$\n\\Delta = \\widehat{\\text{Acc}}_{\\text{CV}}^{\\text{leak}} - \\widehat{\\text{Acc}}_{\\text{CV}}^{\\text{proper}}.\n$$\n\nAngle units: All angles are measured in degrees.\n\nAnswer types: All outputs must be real numbers (floats).\n\nTest suite and coverage:\nImplement the following test cases, each defined as a tuple $(n, h, w, k, S, \\sigma, \\text{seed})$ and processed in order. For every test case, compute and output $\\Delta$.\n\n- Case $1$ (happy path): $(n, h, w, k, S, \\sigma, \\text{seed}) = (60, 16, 16, 5, \\{90, 180, 270\\}, 0.10, 42)$.\n- Case $2$ (boundary, no augmentation): $(60, 16, 16, 5, \\varnothing, 0.10, 43)$, where $\\varnothing$ denotes the empty set of angles.\n- Case $3$ (few folds): $(60, 16, 16, 2, \\{90, 180, 270\\}, 0.10, 44)$.\n- Case $4$ (high noise): $(60, 16, 16, 5, \\{90, 180, 270\\}, 0.50, 45)$.\n- Case $5$ (single-angle augmentation): $(60, 16, 16, 10, \\{90\\}, 0.10, 46)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases. For example, an output with three results should look like $[r_1,r_2,r_3]$, where each $r_i$ is a float. Angles are in degrees. No additional text should be printed.", "solution": "We start from the core definition of $k$-fold Cross-Validation (CV) and the independence requirement. The $k$-fold CV estimator averages validation performance across disjoint folds, and its reliability as an estimator of generalization performance depends on the conditional independence between training and validation sets given the data-generating process. When augmentation is applied only to training folds, transformations $\\mathcal{A}_\\theta$ enrich the training distribution while keeping validation samples independent draws from the original data distribution, preserving the CV estimator’s integrity. When augmentation leaks into validation folds—specifically by augmenting the entire dataset before constructing folds—augmented versions of the same base sample can be assigned both to training and validation sets, violating independence and inducing optimism bias.\n\nWe formalize the two estimators:\n- Proper augmentation CV estimator:\n$$\n\\widehat{\\text{Acc}}_{\\text{proper}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V_j|} \\sum_{i \\in V_j} \\mathbf{1}\\left\\{ \\arg\\min_{c \\in \\{0,1\\}} \\|x_i - \\mu_{c}^{(j)}\\|_2^2 = y_i \\right\\},\n$$\nwith\n$$\n\\mu_{c}^{(j)} = \\frac{1}{|\\{t \\in T_j: y_t = c\\}| + |\\{(t,\\theta): t \\in T_j, y_t = c, \\theta \\in S\\}|} \\left( \\sum_{t \\in T_j, y_t=c} x_t + \\sum_{t \\in T_j, y_t=c} \\sum_{\\theta \\in S} \\mathcal{A}_\\theta(x_t) \\right).\n$$\nValidation uses the unaugmented $x_i$ for $i \\in V_j$.\n\n- Leakage CV estimator:\nConstruct $D' = \\bigcup_{i=1}^n \\left(\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}\\right)$, then perform $k$-fold CV directly on $D'$. Let $V'_1, \\dots, V'_k$ be the folds and $T'_j$ be their complements. The leakage estimator is\n$$\n\\widehat{\\text{Acc}}_{\\text{leak}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V'_j|} \\sum_{(i,\\phi) \\in V'_j} \\mathbf{1}\\left\\{ \\arg\\min_{c \\in \\{0,1\\}} \\| \\mathcal{A}_\\phi (x_i) - \\tilde{\\mu}_{c}^{(j)} \\|_2^2 = y_i \\right\\},\n$$\nwith\n$$\n\\tilde{\\mu}_{c}^{(j)} = \\frac{1}{|\\{(t,\\psi) \\in T'_j: y_t = c\\}|} \\sum_{(t,\\psi) \\in T'_j, y_t=c} \\mathcal{A}_\\psi(x_t).\n$$\n\nBias mechanism:\nIn the proper procedure, $x_i$ in validation is not used (directly or via its transformed copies) in training. In the leakage procedure, for many $i$, some transformed $\\mathcal{A}_\\psi(x_i)$ can be present in training while $\\mathcal{A}_\\phi(x_i)$ is used for validation, creating high correlation between training and validation samples. For a nearest-centroid classifier, this correlation reduces $\\| \\mathcal{A}_\\phi (x_i) - \\tilde{\\mu}_{y_i}^{(j)} \\|_2^2$ and increases the probability of correct classification, hence $\\widehat{\\text{Acc}}_{\\text{leak}} \\ge \\widehat{\\text{Acc}}_{\\text{proper}}$ on average. The quantity of interest is the difference\n$$\n\\Delta = \\widehat{\\text{Acc}}_{\\text{leak}} - \\widehat{\\text{Acc}}_{\\text{proper}}.\n$$\n\nAlgorithmic design:\n1. Synthetic data generation:\n   - Create $n$ base images of size $h \\times w$. For class $0$, draw a centered \"$+$\" by setting pixels in a vertical and horizontal bar crossing the center, with random thickness (e.g., $1$ or $2$ pixels) and small random center offsets. For class $1$, draw a centered \"$\\times$\" by setting pixels along both diagonals with similar random thickness and offsets.\n   - Add independent Gaussian noise with standard deviation $\\sigma$ to each pixel and clip values to $[0,1]$.\n   - Assign labels such that $n/2$ images belong to each class (balanced), then shuffle.\n\n2. Augmentation operator:\n   - For angles $\\theta \\in S \\subseteq \\{90,180,270\\}$ measured in degrees, implement $\\mathcal{A}_\\theta$ via $90$-degree increments (i.e., $\\text{np.rot90}$ with $k = \\theta/90$), preserving labels.\n\n3. Classifier:\n   - Nearest-centroid classifier under squared $\\ell_2$ distance. Flatten images to vectors in $\\mathbb{R}^{h \\cdot w}$ for computations. Compute class centroids from the training set (including augmented samples as required) and predict the class of validation samples by nearest centroid.\n\n4. $k$-fold splitting:\n   - For proper augmentation CV, split the $n$ base samples into $k$ folds. For fold $j$, augment only the training set by angles in $S$, keep validation unaugmented, train centroids, and compute accuracy on the validation fold. Average across folds to obtain $\\widehat{\\text{Acc}}_{\\text{proper}}$.\n   - For leakage CV, first augment the entire dataset (include originals and rotations in $S$), then perform $k$-fold CV on this augmented dataset, training and validating on augmented samples to obtain $\\widehat{\\text{Acc}}_{\\text{leak}}$.\n\n5. Difference:\n   - Compute $\\Delta$ for each test case.\n\nTest suite coverage rationale:\n- Case $1$ uses moderate $k$, multiple angles, and small noise, representing a typical scenario.\n- Case $2$ uses $S = \\varnothing$ (no augmentation), where we expect $\\Delta \\approx 0$ because both procedures coincide.\n- Case $3$ uses small $k$, increasing the chance that augmented duplicates cross folds, potentially amplifying leakage effects.\n- Case $4$ increases noise $\\sigma$, examining robustness; leakage can artificially boost performance under noisy conditions.\n- Case $5$ uses $k = 10$ and a single angle, testing sensitivity to the augmentation set size.\n\nFinal output:\nA single line $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$ with floats corresponding to the cases above, in order.\n\nThis design strictly adheres to the independence principle underlying CV and exposes, through computation, how violating it via augmentation leakage produces optimistic bias in the estimated accuracy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_plus_image(h, w, thickness, off_y, off_x):\n    \"\"\"\n    Create a centered plus '+' pattern with given thickness and small offsets.\n    Intensities are 1.0 on the pattern, 0.0 elsewhere.\n    \"\"\"\n    img = np.zeros((h, w), dtype=np.float64)\n    cy = h // 2 + off_y\n    cx = w // 2 + off_x\n    # Horizontal bar\n    y_start = max(cy - thickness, 0)\n    y_end = min(cy + thickness + 1, h)\n    img[y_start:y_end, :] = 1.0\n    # Vertical bar\n    x_start = max(cx - thickness, 0)\n    x_end = min(cx + thickness + 1, w)\n    img[:, x_start:x_end] = 1.0\n    return img\n\ndef make_x_image(h, w, thickness, off_y, off_x):\n    \"\"\"\n    Create a centered 'x' pattern (two diagonals) with given thickness and small offsets.\n    Intensities are 1.0 on the pattern, 0.0 elsewhere.\n    \"\"\"\n    img = np.zeros((h, w), dtype=np.float64)\n    cy = h // 2 + off_y\n    cx = w // 2 + off_x\n    # Draw diagonals with thickness by marking pixels close to diagonal lines\n    yy, xx = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n    # Centered diagonals adjusted by offsets\n    # Line 1: slope +1 through (cy, cx)\n    # Distance from line y - cy = x - cx => y - x = cy - cx\n    d1 = np.abs((yy - xx) - (cy - cx))\n    # Line 2: slope -1 through (cy, cx)\n    # Distance from line y - cy = -(x - cx) => y + x = cy + cx\n    d2 = np.abs((yy + xx) - (cy + cx))\n    mask = (d1 <= thickness) | (d2 <= thickness)\n    img[mask] = 1.0\n    return img\n\ndef add_noise_and_clip(img, noise_std, rng):\n    noisy = img + rng.normal(loc=0.0, scale=noise_std, size=img.shape)\n    return np.clip(noisy, 0.0, 1.0)\n\ndef generate_dataset(n, h, w, noise_std, seed):\n    \"\"\"\n    Generate n base images: half '+' class (label 0), half 'x' class (label 1),\n    with random thickness and small offsets, plus Gaussian noise.\n    Returns images (n, h, w) and labels (n,).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    images = np.zeros((n, h, w), dtype=np.float64)\n    labels = np.zeros(n, dtype=np.int64)\n    # Balanced labels: 0 then 1 alternating\n    base_labels = np.array([0, 1] * (n // 2) + ([0] if n % 2 == 1 else []), dtype=np.int64)\n    # Shuffle labels for randomness\n    rng.shuffle(base_labels)\n    for i in range(n):\n        c = base_labels[i]\n        labels[i] = c\n        thickness = rng.integers(1, 3)  # 1 or 2\n        off_y = rng.integers(-1, 2)     # -1, 0, 1\n        off_x = rng.integers(-1, 2)     # -1, 0, 1\n        if c == 0:\n            img = make_plus_image(h, w, thickness, off_y, off_x)\n        else:\n            img = make_x_image(h, w, thickness, off_y, off_x)\n        img = add_noise_and_clip(img, noise_std, rng)\n        images[i] = img\n    return images, labels\n\ndef rotate_image(img, angle_deg):\n    \"\"\"\n    Rotate the image by angle degrees (must be in {90,180,270}) using np.rot90.\n    \"\"\"\n    if angle_deg % 90 != 0 or angle_deg == 0:\n        # Only support non-zero multiples of 90 as per augmentation set; skip 0 in augmentation\n        raise ValueError(\"Only 90, 180, 270 degrees supported for augmentation\")\n    k = (angle_deg // 90) % 4\n    return np.rot90(img, k)\n\ndef augment_images(images, labels, angles):\n    \"\"\"\n    For each image, return list of (original + rotated versions) according to angles.\n    angles: list of integers from {90, 180, 270}. Original is always included when leak=True;\n            for training-only augmentation, we'll explicitly include original.\n    \"\"\"\n    augmented_imgs = []\n    augmented_labels = []\n    for img, y in zip(images, labels):\n        # include original\n        augmented_imgs.append(img)\n        augmented_labels.append(y)\n        for ang in angles:\n            aug = rotate_image(img, ang)\n            augmented_imgs.append(aug)\n            augmented_labels.append(y)\n    return np.array(augmented_imgs), np.array(augmented_labels)\n\ndef kfold_indices(n, k, seed):\n    \"\"\"\n    Return list of k folds, each a numpy array of indices.\n    Shuffle indices using seed, then split into k contiguous folds as evenly as possible.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n, dtype=np.int64)\n    rng.shuffle(indices)\n    folds = []\n    fold_sizes = np.full(k, n // k, dtype=int)\n    fold_sizes[:(n % k)] += 1\n    start = 0\n    for fs in fold_sizes:\n        end = start + fs\n        folds.append(indices[start:end])\n        start = end\n    return folds\n\ndef compute_centroids(X_train, y_train):\n    \"\"\"\n    Compute class centroids for labels 0 and 1.\n    X_train: (m, d), y_train: (m,)\n    Returns mu0, mu1 as (d,) arrays.\n    \"\"\"\n    # In case a class is missing, handle by using zeros to avoid crash\n    c0 = (y_train == 0)\n    c1 = (y_train == 1)\n    if not np.any(c0) or not np.any(c1):\n        # If missing a class, return centroids computed from available class and zeros for missing\n        mu0 = X_train[c0].mean(axis=0) if np.any(c0) else np.zeros(X_train.shape[1], dtype=np.float64)\n        mu1 = X_train[c1].mean(axis=0) if np.any(c1) else np.zeros(X_train.shape[1], dtype=np.float64)\n        return mu0, mu1\n    mu0 = X_train[c0].mean(axis=0)\n    mu1 = X_train[c1].mean(axis=0)\n    return mu0, mu1\n\ndef predict_nearest_centroid(X_val, mu0, mu1):\n    \"\"\"\n    Predict labels for validation set X_val given centroids mu0 and mu1.\n    \"\"\"\n    # Compute squared distances to centroids\n    d0 = np.sum((X_val - mu0) ** 2, axis=1)\n    d1 = np.sum((X_val - mu1) ** 2, axis=1)\n    return (d1 < d0).astype(np.int64)  # predict 1 if distance to mu1 is smaller, else 0\n\ndef cross_val_accuracy_proper(images, labels, k, angles, seed):\n    \"\"\"\n    Proper augmentation: augment only training folds; validate on original base images.\n    \"\"\"\n    n = images.shape[0]\n    folds = kfold_indices(n, k, seed)\n    h, w = images.shape[1], images.shape[2]\n    accs = []\n    for fold in folds:\n        val_idx = fold\n        train_idx = np.setdiff1d(np.arange(n, dtype=np.int64), val_idx, assume_unique=True)\n        # Training data: original + augment rotations for training indices\n        X_train_list = []\n        y_train_list = []\n        for idx in train_idx:\n            img = images[idx]\n            y = labels[idx]\n            # include original\n            X_train_list.append(img.reshape(h * w))\n            y_train_list.append(y)\n            # include augmentations\n            for ang in angles:\n                aug = rotate_image(img, ang)\n                X_train_list.append(aug.reshape(h * w))\n                y_train_list.append(y)\n        X_train = np.stack(X_train_list, axis=0)\n        y_train = np.array(y_train_list, dtype=np.int64)\n        mu0, mu1 = compute_centroids(X_train, y_train)\n        # Validation data: original base images only\n        X_val = images[val_idx].reshape(len(val_idx), h * w)\n        y_val = labels[val_idx]\n        y_pred = predict_nearest_centroid(X_val, mu0, mu1)\n        acc = np.mean(y_pred == y_val)\n        accs.append(acc)\n    return float(np.mean(accs))\n\ndef cross_val_accuracy_leak(images, labels, k, angles, seed):\n    \"\"\"\n    Leakage augmentation: augment entire dataset first (include originals and rotations),\n    then perform k-fold CV on the augmented dataset.\n    \"\"\"\n    # Augment entire dataset: originals + rotations in angles\n    aug_imgs, aug_labels = augment_images(images, labels, angles)\n    n_aug = aug_imgs.shape[0]\n    folds = kfold_indices(n_aug, k, seed)\n    h, w = images.shape[1], images.shape[2]\n    accs = []\n    for fold in folds:\n        val_idx = fold\n        train_idx = np.setdiff1d(np.arange(n_aug, dtype=np.int64), val_idx, assume_unique=True)\n        X_train = aug_imgs[train_idx].reshape(len(train_idx), h * w)\n        y_train = aug_labels[train_idx]\n        mu0, mu1 = compute_centroids(X_train, y_train)\n        X_val = aug_imgs[val_idx].reshape(len(val_idx), h * w)\n        y_val = aug_labels[val_idx]\n        y_pred = predict_nearest_centroid(X_val, mu0, mu1)\n        acc = np.mean(y_pred == y_val)\n        accs.append(acc)\n    return float(np.mean(accs))\n\ndef compute_delta(n, h, w, k, angles, noise_std, seed):\n    images, labels = generate_dataset(n, h, w, noise_std, seed)\n    acc_proper = cross_val_accuracy_proper(images, labels, k, angles, seed)\n    acc_leak = cross_val_accuracy_leak(images, labels, k, angles, seed)\n    return acc_leak - acc_proper\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (n, h, w, k, angles_list, noise_std, seed)\n    test_cases = [\n        (60, 16, 16, 5, [90, 180, 270], 0.10, 42),  # Case 1: happy path\n        (60, 16, 16, 5, [],              0.10, 43),  # Case 2: boundary, no augmentation\n        (60, 16, 16, 2, [90, 180, 270],  0.10, 44),  # Case 3: few folds\n        (60, 16, 16, 5, [90, 180, 270],  0.50, 45),  # Case 4: high noise\n        (60, 16, 16, 10, [90],           0.10, 46),  # Case 5: single-angle augmentation\n    ]\n\n    results = []\n    for case in test_cases:\n        n, h, w, k, angles, noise_std, seed = case\n        delta = compute_delta(n, h, w, k, angles, noise_std, seed)\n        # Format with a reasonable precision\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3134696"}, {"introduction": "The theoretical guarantee of k-fold cross-validation rests on the assumption that the partitioned data folds are independent samples from the underlying data distribution. This final practice explores the consequences of violating this core assumption, which can occur when a dataset contains hidden duplicates or highly correlated samples. You will analyze a formal model of equicorrelated data to derive an exact mathematical expression for the optimistic bias of the CV risk estimator [@problem_id:3134661]. This exercise provides a deep, quantitative insight into a fundamental failure mode of CV, cementing your understanding of the principles that ensure its reliability.", "problem": "You are asked to formalize and analyze a specific failure mode of $k$-fold Cross-Validation (CV). The failure arises when a dataset contains hidden duplicates or near-identical samples whose residuals are positively correlated across folds, thereby violating the standard independence assumptions. Starting exclusively from core definitions of risk and covariance in Statistical Learning, you must derive the expected CV risk and the true generalization risk for a simple estimator under an explicit correlation model, and then quantify the bias in the CV estimate.\n\nConsider the following counterexample construction. There is a single latent group with $m$ near-identical observations. For each observation $j \\in \\{1,\\dots,m\\}$, the outcome is $y_j = \\mu + \\varepsilon_j$, where $\\mu$ is an unknown constant and $(\\varepsilon_1,\\dots,\\varepsilon_m)$ is a zero-mean equicorrelated noise vector with variance $\\sigma^2$ and pairwise correlation coefficient $\\rho \\in [0,1]$, meaning $\\operatorname{Var}(\\varepsilon_j) = \\sigma^2$ for all $j$, and $\\operatorname{Cov}(\\varepsilon_i,\\varepsilon_j) = \\rho \\sigma^2$ for all $i \\neq j$. The equicorrelation captures hidden duplication or near-identity among samples. Perform $k$-fold Cross-Validation (CV), with folds that evenly split the $m$ observations; assume $k$ divides $m$ exactly for simplicity. On each fold, train an estimator that predicts the validation outcomes by the training-set mean,\n$$\n\\widehat{\\mu}_{\\text{tr}} \\;=\\; \\mu \\;+\\; \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\text{train}} \\varepsilon_i,\n$$\nwhere $n_{\\text{tr}} = m \\cdot \\frac{k-1}{k}$ is the number of training observations in that fold. Evaluate squared error on the validation set. Define the true generalization risk as the expected squared error on a fresh, independent draw $y_{\\text{new}} = \\mu + \\varepsilon_{\\text{new}}$ with $\\varepsilon_{\\text{new}} \\sim \\mathcal{N}(0,\\sigma^2)$ independent of the training residuals.\n\nYour tasks:\n- Using only basic properties of expectation, variance, covariance, and the definition of risk as expected loss, derive a closed-form expression for the expected $k$-fold CV squared error under the given equicorrelation model. You must express the result in terms of $k$, $m$, $\\rho$, and $\\sigma^2$. Do not assume independence across folds; instead, use the stated correlation structure that applies to all pairs of duplicates, including those split across folds.\n- Derive a closed-form expression for the true generalization risk of the same estimator for a fresh, independent sample, again in terms of $k$, $m$, $\\rho$, and $\\sigma^2$.\n- Define the CV bias as the difference between the expected CV risk and the true risk. Prove that for any $\\rho \\in (0,1]$ the bias is strictly negative, thereby providing a mathematically explicit counterexample in which $k$-fold CV is optimistically biased due to hidden duplication-induced correlations.\n- Implement a complete, runnable program that, for a specified test suite, computes the bias for each test case as a single floating-point number equal to the difference between the expected CV risk and the true risk. Round each result to $6$ decimal places.\n\nTest suite:\n- Case $1$: $k=2$, $m=4$, $\\rho=0.3$, $\\sigma^2=1.0$.\n- Case $2$: $k=5$, $m=10$, $\\rho=0.9$, $\\sigma^2=1.0$.\n- Case $3$: $k=10$, $m=10$, $\\rho=0.0$, $\\sigma^2=2.0$.\n- Case $4$: $k=3$, $m=6$, $\\rho=0.5$, $\\sigma^2=0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, in the abstract form, produce [$b_1$,$b_2$,$b_3$,$b_4$], where each $b_i$ is the bias for Case $i$ rounded to $6$ decimal places.", "solution": "The problem requires the derivation and analysis of the bias in $k$-fold Cross-Validation (CV) under a specific data correlation model. We must derive the expected CV risk, the true generalization risk, and the resulting bias, starting from fundamental statistical definitions.\n\nLet the set of $m$ observations be indexed by $\\mathcal{D} = \\{1, 2, \\dots, m\\}$. The outcome for observation $j$ is $y_j = \\mu + \\varepsilon_j$, where $\\mu$ is an unknown constant. The noise vector $(\\varepsilon_1, \\dots, \\varepsilon_m)$ has $\\mathbb{E}[\\varepsilon_j] = 0$, $\\operatorname{Var}(\\varepsilon_j) = \\sigma^2$ for all $j$, and $\\operatorname{Cov}(\\varepsilon_i, \\varepsilon_j) = \\rho\\sigma^2$ for all $i \\neq j$.\n\nIn a $k$-fold CV scheme, for each fold $f \\in \\{1, \\dots, k\\}$, the data is partitioned into a training set $\\mathcal{D}_{\\text{tr}}^{(f)}$ of size $n_{\\text{tr}} = m \\frac{k-1}{k}$ and a validation set $\\mathcal{D}_{\\text{val}}^{(f)}$ of size $n_{\\text{val}} = \\frac{m}{k}$. The estimator for fold $f$ is the mean of the training data:\n$$\n\\widehat{\\mu}_{\\text{tr}}^{(f)} = \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} y_i = \\mu + \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\varepsilon_i\n$$\nLet $\\bar{\\varepsilon}_{\\text{tr}}^{(f)} = \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\varepsilon_i$. Then $\\widehat{\\mu}_{\\text{tr}}^{(f)} = \\mu + \\bar{\\varepsilon}_{\\text{tr}}^{(f)}$.\n\n### 1. Derivation of the Expected $k$-fold CV Risk\n\nThe CV risk is the average squared error on the validation sets. By the symmetry of the problem setup, the expected squared error is identical for any validation sample in any fold. We can therefore analyze a single, arbitrary validation point, say $j \\in \\mathcal{D}_{\\text{val}}^{(f)}$, without loss of generality. The CV risk is the expectation of the squared error for this point.\n\nThe prediction error for observation $j$ is:\n$$\ny_j - \\widehat{\\mu}_{\\text{tr}}^{(f)} = (\\mu + \\varepsilon_j) - (\\mu + \\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\varepsilon_j - \\bar{\\varepsilon}_{\\text{tr}}^{(f)}\n$$\nThe expected error is $\\mathbb{E}[\\varepsilon_j - \\bar{\\varepsilon}_{\\text{tr}}^{(f)}] = \\mathbb{E}[\\varepsilon_j] - \\mathbb{E}[\\bar{\\varepsilon}_{\\text{tr}}^{(f)}] = 0 - 0 = 0$.\nThus, the expected CV risk, $\\mathbb{E}[R_{CV}]$, is the variance of this error term:\n$$\n\\mathbb{E}[R_{CV}] = \\mathbb{E}\\left[ (y_j - \\widehat{\\mu}_{\\text{tr}}^{(f)})^2 \\right] = \\operatorname{Var}\\left(\\varepsilon_j - \\bar{\\varepsilon}_{\\text{tr}}^{(f)}\\right)\n$$\nUsing the property $\\operatorname{Var}(X-Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) - 2\\operatorname{Cov}(X,Y)$, we get:\n$$\n\\mathbb{E}[R_{CV}] = \\operatorname{Var}(\\varepsilon_j) + \\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)}) - 2\\operatorname{Cov}(\\varepsilon_j, \\bar{\\varepsilon}_{\\text{tr}}^{(f)})\n$$\nWe compute each term:\n1.  $\\operatorname{Var}(\\varepsilon_j) = \\sigma^2$ by definition.\n2.  $\\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)})$: The variance of the mean of $n_{\\text{tr}}$ correlated variables.\n    $$\n    \\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\operatorname{Var}\\left(\\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\varepsilon_i\\right) = \\frac{1}{n_{\\text{tr}}^2} \\left[ \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\operatorname{Var}(\\varepsilon_i) + \\sum_{i, l \\in \\mathcal{D}_{\\text{tr}}^{(f)}, i \\neq l} \\operatorname{Cov}(\\varepsilon_i, \\varepsilon_l) \\right]\n    $$\n    There are $n_{\\text{tr}}$ variance terms ($\\sigma^2$) and $n_{\\text{tr}}(n_{\\text{tr}}-1)$ covariance terms ($\\rho\\sigma^2$).\n    $$\n    \\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\frac{1}{n_{\\text{tr}}^2} \\left[ n_{\\text{tr}}\\sigma^2 + n_{\\text{tr}}(n_{\\text{tr}}-1)\\rho\\sigma^2 \\right] = \\frac{\\sigma^2}{n_{\\text{tr}}} [1 + (n_{\\text{tr}}-1)\\rho]\n    $$\n3.  $\\operatorname{Cov}(\\varepsilon_j, \\bar{\\varepsilon}_{\\text{tr}}^{(f)})$: The covariance between a validation point's noise and the mean of the training points' noise.\n    $$\n    \\operatorname{Cov}(\\varepsilon_j, \\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\operatorname{Cov}\\left(\\varepsilon_j, \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\varepsilon_i\\right) = \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\operatorname{Cov}(\\varepsilon_j, \\varepsilon_i)\n    $$\n    Since $j$ is in the validation set and $i$ is in the training set, $j \\neq i$. Thus, $\\operatorname{Cov}(\\varepsilon_j, \\varepsilon_i) = \\rho\\sigma^2$.\n    $$\n    \\operatorname{Cov}(\\varepsilon_j, \\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\frac{1}{n_{\\text{tr}}} \\sum_{i \\in \\mathcal{D}_{\\text{tr}}^{(f)}} \\rho\\sigma^2 = \\frac{n_{\\text{tr}}\\rho\\sigma^2}{n_{\\text{tr}}} = \\rho\\sigma^2\n    $$\nSubstituting these into the expression for $\\mathbb{E}[R_{CV}]$:\n$$\n\\mathbb{E}[R_{CV}] = \\sigma^2 + \\frac{\\sigma^2}{n_{\\text{tr}}}[1 + (n_{\\text{tr}}-1)\\rho] - 2\\rho\\sigma^2\n$$\n\n### 2. Derivation of the True Generalization Risk\n\nThe true generalization risk, $R_{\\text{true}}$, is the expected squared error for a new, independent observation $y_{\\text{new}} = \\mu + \\varepsilon_{\\text{new}}$. Here, $\\varepsilon_{\\text{new}}$ has mean $0$ and variance $\\sigma^2$, and is independent of all training samples $(\\varepsilon_1, \\dots, \\varepsilon_m)$. The risk is evaluated for the same type of estimator, trained on $n_{\\text{tr}}$ samples.\n$$\nR_{\\text{true}} = \\mathbb{E}\\left[ (y_{\\text{new}} - \\widehat{\\mu}_{\\text{tr}}^{(f)})^2 \\right]\n$$\nThe expectation is over both the training data and the new data. The error term is:\n$$\ny_{\\text{new}} - \\widehat{\\mu}_{\\text{tr}}^{(f)} = (\\mu + \\varepsilon_{\\text{new}}) - (\\mu + \\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\varepsilon_{\\text{new}} - \\bar{\\varepsilon}_{\\text{tr}}^{(f)}\n$$\nAs before, the expected error is $0$, so the risk is the variance:\n$$\nR_{\\text{true}} = \\operatorname{Var}(\\varepsilon_{\\text{new}} - \\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\operatorname{Var}(\\varepsilon_{\\text{new}}) + \\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)}) - 2\\operatorname{Cov}(\\varepsilon_{\\text{new}}, \\bar{\\varepsilon}_{\\text{tr}}^{(f)})\n$$\nWe compute each term:\n1.  $\\operatorname{Var}(\\varepsilon_{\\text{new}}) = \\sigma^2$ by definition.\n2.  $\\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)}) = \\frac{\\sigma^2}{n_{\\text{tr}}}[1 + (n_{\\text{tr}}-1)\\rho]$, as derived previously.\n3.  $\\operatorname{Cov}(\\varepsilon_{\\text{new}}, \\bar{\\varepsilon}_{\\text{tr}}^{(f)})$: Since $\\varepsilon_{\\text{new}}$ is independent of all $\\varepsilon_i$ in the training set, the covariance is $0$.\n\nSubstituting these in:\n$$\nR_{\\text{true}} = \\sigma^2 + \\frac{\\sigma^2}{n_{\\text{tr}}}[1 + (n_{\\text{tr}}-1)\\rho] - 0\n$$\n\n### 3. Derivation of the CV Bias and Proof of Negativity\n\nThe bias of the cross-validation estimate is the difference between the expected CV risk and the true generalization risk.\n$$\n\\text{Bias} = \\mathbb{E}[R_{CV}] - R_{\\text{true}}\n$$\nUsing the expressions derived above:\n$$\n\\text{Bias} = \\left( \\sigma^2 + \\frac{\\sigma^2}{n_{\\text{tr}}}[1 + (n_{\\text{tr}}-1)\\rho] - 2\\rho\\sigma^2 \\right) - \\left( \\sigma^2 + \\frac{\\sigma^2}{n_{\\text{tr}}}[1 + (n_{\\text{tr}}-1)\\rho] \\right)\n$$\nThe terms $\\sigma^2$ and the entire term involving $\\operatorname{Var}(\\bar{\\varepsilon}_{\\text{tr}}^{(f)})$ cancel out, leaving:\n$$\n\\text{Bias} = -2\\rho\\sigma^2\n$$\nThis remarkably simple result shows that the bias is independent of the number of folds $k$ and the sample size $m$. It depends only on the correlation $\\rho$ and the noise variance $\\sigma^2$. The bias arises entirely from the covariance between the validation data and the training data, a covariance which is absent when evaluating on a truly independent test set.\n\n**Proof of Negativity:**\nWe are asked to prove that the bias is strictly negative for $\\rho \\in (0, 1]$.\nGiven:\n- $\\rho \\in (0, 1]$, which implies $\\rho > 0$.\n- $\\sigma^2$ is a variance. For a non-trivial problem, we must have $\\sigma^2 > 0$. The test cases confirm this.\n\nSince $2 > 0$, $\\rho > 0$, and $\\sigma^2 > 0$, their product $2\\rho\\sigma^2$ is strictly positive.\nTherefore, the bias, which is equal to $-2\\rho\\sigma^2$, is strictly negative for any $\\rho \\in (0,1]$. This demonstrates that $k$-fold CV is optimistically biased (i.e., it underestimates the true error) when hidden correlations exist between samples, as modeled by $\\rho > 0$. If $\\rho=0$, the bias is $0$, recovering the standard result for independent data.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of k-fold Cross-Validation under an equicorrelation model.\n\n    The problem asks for the derivation of the CV bias, defined as the difference\n    between the expected CV risk and the true generalization risk. The derivation\n    in the solution text shows that:\n    \n    Expected CV Risk: E[R_CV] = sigma^2 + Var(mean(epsilon_tr)) - 2 * rho * sigma^2\n    True Risk: R_true = sigma^2 + Var(mean(epsilon_tr))\n    \n    Bias = E[R_CV] - R_true = -2 * rho * sigma^2\n\n    This simple formula is independent of k and m. This function implements this\n    result to calculate the bias for the specified test suite.\n    \"\"\"\n\n    # Test suite from the problem statement.\n    # Each tuple is (k, m, rho, sigma_sq).\n    test_cases = [\n        (2, 4, 0.3, 1.0),\n        (5, 10, 0.9, 1.0),\n        (10, 10, 0.0, 2.0),\n        (3, 6, 0.5, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters. k and m are not needed for the final bias calculation.\n        k, m, rho, sigma_sq = case\n        \n        # Calculate the bias using the derived formula.\n        bias = -2 * rho * sigma_sq\n        \n        # Format the result to 6 decimal places as a string.\n        results.append(f\"{bias:.6f}\")\n\n    # Print the final output in the required format: [b_1,b_2,b_3,b_4]\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver.\nsolve()\n```", "id": "3134661"}]}