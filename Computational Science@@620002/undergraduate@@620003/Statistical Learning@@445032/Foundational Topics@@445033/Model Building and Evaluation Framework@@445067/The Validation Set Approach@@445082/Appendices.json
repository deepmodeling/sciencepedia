{"hands_on_practices": [{"introduction": "This exercise delves into the fundamental trade-off inherent in the validation set approach: the split between training and validation data. By modeling the primary sources of error—one from fitting with finite data and another from evaluating with a noisy validation set—we can derive a theoretically optimal split ratio. This practice provides a principled, mathematical foundation for making this crucial decision, moving beyond simple rules of thumb. [@problem_id:3187610]", "problem": "A data set of total size $N$ consists of independent and identically distributed (i.i.d.) labeled observations drawn from an unknown distribution. You plan to evaluate $M \\geq 2$ fixed candidate models using the validation set approach: split the data into a training subset of size $n_{\\text{tr}}$ and a validation subset of size $n_{\\text{val}}$, with $n_{\\text{tr}} + n_{\\text{val}} = N$. Each candidate model is fit on the training subset and its predictive performance is evaluated on the validation subset by the empirical mean of the per-observation loss.\n\nAssume the following well-tested facts and parametric approximations hold:\n\n- The per-observation loss for a fixed, trained model has finite variance $\\sigma^{2}$, so the empirical mean validation loss has variance $\\sigma^{2}/n_{\\text{val}}$.\n\n- The expected excess risk of any fitted model due to finite training size scales like $c/n_{\\text{tr}}$ for some positive constant $c$ that depends on model complexity and the data distribution. For the purpose of optimization, aggregate this effect into a single positive scalar $\\alpha > 0$.\n\n- The expected regret of model selection based on noisy validation estimates increases with the variance of those estimates and with the number of competitors being compared. Under a local quadratic approximation to the selection surface, model the selection-induced component of expected regret as proportional to $(M-1)\\,\\sigma^{2}/n_{\\text{val}}$, with proportionality constant $\\beta > 0$.\n\nUnder these assumptions, the approximate expected total regret can be written as\n$$\n\\mathbb{E}[\\text{Regret}] \\approx \\frac{\\alpha}{n_{\\text{tr}}} \\;+\\; \\frac{\\beta\\,\\sigma^{2}(M-1)}{n_{\\text{val}}}.\n$$\n\nDerive the split ratio $n_{\\text{tr}}:n_{\\text{val}}$ that minimizes the approximate expected total regret subject to the constraint $n_{\\text{tr}} + n_{\\text{val}} = N$. Express your final answer as a single closed-form analytic expression for the optimal ratio $n_{\\text{tr}}/n_{\\text{val}}$. No rounding is required.", "solution": "The validation set approach estimates each candidate model’s predictive risk by the empirical mean of its losses on the validation subset. For a fixed trained model and i.i.d. losses with variance $\\sigma^{2}$, the variance of the empirical mean of $n_{\\text{val}}$ validation losses is $\\sigma^{2}/n_{\\text{val}}$, by the variance-of-the-mean identity and independence.\n\nWhen selecting among $M$ models by minimizing noisy empirical means, the expected regret from selection errors can be approximated locally as a constant multiple of the variance of those empirical means, aggregated over $M-1$ competitors. This yields the term $\\beta\\,\\sigma^{2}(M-1)/n_{\\text{val}}$ for some $\\beta>0$ that encodes how sensitive the selection rule is to noise and typical gaps between models. Separately, the expected excess risk from fitting with finite $n_{\\text{tr}}$ typically follows a learning-curve decay of order $1/n_{\\text{tr}}$; we aggregate this into $\\alpha/n_{\\text{tr}}$ for some $\\alpha>0$.\n\nCombining these contributions, the approximate expected total regret is\n$$\nR(n_{\\text{tr}}, n_{\\text{val}}) \\equiv \\frac{\\alpha}{n_{\\text{tr}}} + \\frac{\\beta\\,\\sigma^{2}(M-1)}{n_{\\text{val}}}\n\\quad \\text{subject to} \\quad n_{\\text{tr}} + n_{\\text{val}} = N.\n$$\nWe reduce the constrained problem to a single variable by writing $n_{\\text{val}} = N - n_{\\text{tr}}$:\n$$\n\\tilde{R}(n_{\\text{tr}}) = \\frac{\\alpha}{n_{\\text{tr}}} + \\frac{\\beta\\,\\sigma^{2}(M-1)}{N - n_{\\text{tr}}}, \\qquad 0 < n_{\\text{tr}} < N.\n$$\nTo find the minimizer, differentiate with respect to $n_{\\text{tr}}$:\n$$\n\\frac{d\\tilde{R}}{dn_{\\text{tr}}} = -\\frac{\\alpha}{n_{\\text{tr}}^{2}} + \\frac{\\beta\\,\\sigma^{2}(M-1)}{(N - n_{\\text{tr}})^{2}}.\n$$\nSet the derivative equal to zero for a critical point:\n$$\n-\\frac{\\alpha}{n_{\\text{tr}}^{2}} + \\frac{\\beta\\,\\sigma^{2}(M-1)}{(N - n_{\\text{tr}})^{2}} = 0\n\\quad \\Longleftrightarrow \\quad\n\\frac{\\alpha}{n_{\\text{tr}}^{2}} = \\frac{\\beta\\,\\sigma^{2}(M-1)}{(N - n_{\\text{tr}})^{2}}.\n$$\nTaking square roots on both sides yields\n$$\n\\frac{\\sqrt{\\alpha}}{n_{\\text{tr}}} = \\frac{\\sqrt{\\beta\\,\\sigma^{2}(M-1)}}{N - n_{\\text{tr}}}\n\\quad \\Longleftrightarrow \\quad\n\\frac{n_{\\text{tr}}}{N - n_{\\text{tr}}} = \\sqrt{\\frac{\\alpha}{\\beta\\,\\sigma^{2}(M-1)}}.\n$$\nRecognizing $N - n_{\\text{tr}} = n_{\\text{val}}$, we obtain the optimal split ratio\n$$\n\\frac{n_{\\text{tr}}}{n_{\\text{val}}} = \\sqrt{\\frac{\\alpha}{\\beta\\,\\sigma^{2}(M-1)}}.\n$$\nTo confirm this critical point is a minimum, note that $\\tilde{R}(n_{\\text{tr}})$ is the sum of two strictly convex functions on $(0, N)$: $n_{\\text{tr}} \\mapsto \\alpha/n_{\\text{tr}}$ and $n_{\\text{tr}} \\mapsto \\beta\\,\\sigma^{2}(M-1)/(N - n_{\\text{tr}})$, since their second derivatives are positive:\n$$\n\\frac{d^{2}}{dn_{\\text{tr}}^{2}}\\left(\\frac{\\alpha}{n_{\\text{tr}}}\\right) = \\frac{2\\alpha}{n_{\\text{tr}}^{3}} > 0, \n\\qquad\n\\frac{d^{2}}{dn_{\\text{tr}}^{2}}\\left(\\frac{\\beta\\,\\sigma^{2}(M-1)}{N - n_{\\text{tr}}}\\right) = \\frac{2\\beta\\,\\sigma^{2}(M-1)}{(N - n_{\\text{tr}})^{3}} > 0.\n$$\nTherefore, the unique critical point is the global minimizer. The optimal ratio is independent of $N$ and given by the closed-form expression above.\n\nIf one additionally wishes to express the optimal counts, they would be\n$$\nn_{\\text{tr}}^{\\star} = N \\cdot \\frac{\\sqrt{\\alpha}}{\\sqrt{\\alpha} + \\sqrt{\\beta\\,\\sigma^{2}(M-1)}},\n\\qquad\nn_{\\text{val}}^{\\star} = N \\cdot \\frac{\\sqrt{\\beta\\,\\sigma^{2}(M-1)}}{\\sqrt{\\alpha} + \\sqrt{\\beta\\,\\sigma^{2}(M-1)}},\n$$\nwhich are consistent with the ratio $\\frac{n_{\\text{tr}}^{\\star}}{n_{\\text{val}}^{\\star}} = \\sqrt{\\frac{\\alpha}{\\beta\\,\\sigma^{2}(M-1)}}$.", "answer": "$$\\boxed{\\sqrt{\\frac{\\alpha}{\\beta\\,\\sigma^{2}(M-1)}}}$$", "id": "3187610"}, {"introduction": "After splitting the data, how can we be confident that a performance difference observed on the validation set is real and not just statistical noise? This practice connects the validation set approach with the formal framework of statistical hypothesis testing. You will determine the minimum validation set size required to detect a meaningful performance gap between two models with a desired level of statistical confidence, a crucial skill for rigorous model comparison. [@problem_id:3187538]", "problem": "You are comparing two supervised learning models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, using the validation set approach. For each validation observation with feature–response pair $(\\boldsymbol{x}, y)$, define an instantaneous loss $\\ell(y, f(\\boldsymbol{x}))$. Assume the following:\n- For model $\\mathcal{M}_{j}$, the per-observation validation losses $\\{\\ell_{j,i}\\}_{i=1}^{n_{j}}$ are independent and identically distributed with mean $\\mu_{j}$ and variance $\\sigma^{2}$, for $j \\in \\{1,2\\}$.\n- The two models are evaluated on disjoint validation subsets to ensure independence across models. You allocate an equal number of validation observations to each model, so $n_{1} = n_{2} = n_{\\text{val}}/2$, where $n_{\\text{val}}$ is the total size of the validation set.\n- You will compare the average validation losses via a $2$-sample test for the difference in means, based on the Central Limit Theorem (CLT) and a standard normal approximation.\n\nYou will test the null hypothesis $H_{0}: \\mu_{1} - \\mu_{2} = 0$ against the $2$-sided alternative $H_{1}: \\mu_{1} - \\mu_{2} \\neq 0$ at significance level $\\alpha$. You want to design $n_{\\text{val}}$ large enough that, if the true difference in mean losses has magnitude $|\\mu_{1} - \\mu_{2}| = \\delta > 0$, the test has power $1 - \\beta$. Assume the common variance $\\sigma^{2}$ is known for design purposes. Let $z_{p}$ denote the $p$-th quantile of the standard normal distribution.\n\nStarting from first principles (the Central Limit Theorem and the definition of Type I/II error for a $2$-sample $z$-test), derive a closed-form analytic expression for the minimal total validation set size $n_{\\text{val}}$ in terms of $\\alpha$, $\\beta$, $\\sigma^{2}$, and $\\delta$ that achieves the desired power at level $\\alpha$. Express your final answer as a single analytic expression in terms of $\\alpha$, $\\beta$, $\\sigma^{2}$, $\\delta$, and $z_{p}$. No numerical approximation or rounding is required.", "solution": "Let $n = n_{1} = n_{2} = n_{\\text{val}}/2$ be the number of observations in each validation subset. Let $\\bar{L}_{1} = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{1,i}$ and $\\bar{L}_{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{2,i}$ be the sample mean losses for models $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, respectively.\n\nWe are interested in the difference of the mean losses, estimated by $\\bar{D} = \\bar{L}_{1} - \\bar{L}_{2}$.\nThe expected value of this estimator is $E[\\bar{D}] = E[\\bar{L}_{1}] - E[\\bar{L}_{2}] = \\mu_{1} - \\mu_{2}$.\nThe variance of the estimator, given that the two validation sets are independent, is $\\text{Var}(\\bar{D}) = \\text{Var}(\\bar{L}_{1}) + \\text{Var}(\\bar{L}_{2})$.\nSince the individual losses are i.i.d., the variance of each sample mean is $\\text{Var}(\\bar{L}_{j}) = \\frac{\\sigma^{2}}{n}$.\nTherefore, the variance of the difference is $\\text{Var}(\\bar{D}) = \\frac{\\sigma^{2}}{n} + \\frac{\\sigma^{2}}{n} = \\frac{2\\sigma^{2}}{n}$.\n\nBy the Central Limit Theorem, for a sufficiently large sample size $n$, the distribution of $\\bar{D}$ is approximately normal:\n$$ \\bar{D} \\approx \\mathcal{N}\\left(\\mu_{1} - \\mu_{2}, \\frac{2\\sigma^{2}}{n}\\right) $$\nUnder the null hypothesis $H_{0}: \\mu_{1} - \\mu_{2} = 0$, the test statistic $Z$ is constructed by standardizing $\\bar{D}$:\n$$ Z = \\frac{\\bar{D} - 0}{\\sqrt{\\text{Var}(\\bar{D})}} = \\frac{\\bar{L}_{1} - \\bar{L}_{2}}{\\sqrt{2\\sigma^{2}/n}} $$\nUnder $H_{0}$, $Z$ follows a standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$.\n\nFor a two-sided test at a significance level $\\alpha$, we reject $H_{0}$ if the observed value of $|Z|$ is greater than the critical value $z_{1-\\alpha/2}$. The rejection region is $|Z| > z_{1-\\alpha/2}$. This is equivalent to rejecting $H_{0}$ if $|\\bar{D}| > z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^{2}}{n}}$.\n\nThe power of the test is $1-\\beta$, which is the probability of correctly rejecting $H_{0}$ when the alternative hypothesis $H_{1}$ is true. We are given that under $H_{1}$, the true difference has magnitude $|\\mu_{1} - \\mu_{2}| = \\delta > 0$. We can analyze the case $\\mu_{1} - \\mu_{2} = \\delta$ without loss of generality; the case $\\mu_{1} - \\mu_{2} = -\\delta$ gives a symmetric result.\n\nThe power is the probability that $\\bar{D}$ falls into the rejection region, conditioned on $\\mu_{1} - \\mu_{2} = \\delta$:\n$$ 1 - \\beta = P\\left(|\\bar{D}| > z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^{2}}{n}} \\;\\Bigg|\\; \\mu_{1} - \\mu_{2} = \\delta\\right) $$\n$$ 1 - \\beta = P\\left(\\bar{D} > z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^{2}}{n}}\\right) + P\\left(\\bar{D} < -z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^{2}}{n}}\\right) $$\nwhere the probabilities are computed under the assumption that $\\bar{D} \\sim \\mathcal{N}(\\delta, \\frac{2\\sigma^{2}}{n})$.\nFor a well-designed test where power is reasonably high, the distribution of $\\bar{D}$ under $H_{1}$ is shifted far from $0$. The second term, $P(\\bar{D} < -z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^{2}}{n}})$, corresponds to rejecting in the \"wrong\" tail and is typically negligible. Thus, we approximate the power by considering only the primary term:\n$$ 1 - \\beta \\approx P\\left(\\bar{D} > z_{1-\\alpha/2} \\sqrt{\\frac{2\\sigma^{2}}{n}} \\;\\Bigg|\\; \\mu_{1} - \\mu_{2} = \\delta\\right) $$\nTo evaluate this probability, we standardize $\\bar{D}$ under the alternative hypothesis:\n$$ 1 - \\beta \\approx P\\left( \\frac{\\bar{D} - \\delta}{\\sqrt{2\\sigma^{2}/n}} > \\frac{z_{1-\\alpha/2}\\sqrt{2\\sigma^{2}/n} - \\delta}{\\sqrt{2\\sigma^{2}/n}} \\right) $$\nLet $Z' = \\frac{\\bar{D} - \\delta}{\\sqrt{2\\sigma^{2}/n}}$. Under $H_{1}$, $Z' \\sim \\mathcal{N}(0, 1)$. The equation becomes:\n$$ 1 - \\beta \\approx P\\left( Z' > z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^{2}/n}} \\right) $$\nIf $P(Z' > c) = 1-\\beta$, then $c$ must be the $\\beta$-quantile of the standard normal distribution, $c = z_{\\beta}$. By the symmetry of the normal distribution, $z_{\\beta} = -z_{1-\\beta}$. So we have:\n$$ -z_{1-\\beta} = z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^{2}/n}} $$\nNow, we solve for $n$:\n$$ \\frac{\\delta}{\\sqrt{2\\sigma^{2}/n}} = z_{1-\\alpha/2} + z_{1-\\beta} $$\n$$ \\frac{\\delta \\sqrt{n}}{\\sqrt{2\\sigma^{2}}} = z_{1-\\alpha/2} + z_{1-\\beta} $$\n$$ \\sqrt{n} = \\frac{\\sqrt{2\\sigma^{2}}}{\\delta} (z_{1-\\alpha/2} + z_{1-\\beta}) $$\nSquaring both sides gives the required sample size per group:\n$$ n = \\frac{2\\sigma^{2}(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nThe problem asks for the total validation set size, $n_{\\text{val}}$. Since $n = n_{\\text{val}}/2$, we have:\n$$ n_{\\text{val}} = 2n = \\frac{4\\sigma^{2}(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nThis expression gives the minimal total number of validation samples required to detect a true difference of magnitude $\\delta$ with power $1-\\beta$ at a significance level $\\alpha$.", "answer": "$$\\boxed{\\frac{4\\sigma^{2}(z_{1-\\alpha/2} + z_{1-\\beta})^{2}}{\\delta^{2}}}$$", "id": "3187538"}, {"introduction": "A critical, yet often overlooked, assumption is that our validation data perfectly reflects the environment where the model will be deployed. This exercise tackles a common violation of this assumption known as \"class prior shift,\" where the frequency of classes changes. You will use Bayes' rule to derive an adjustment for the classifier's decision threshold and quantify the resulting improvement, demonstrating how to adapt models to real-world distributional changes. [@problem_id:3187498]", "problem": "Consider binary classification with labels $Y \\in \\{0,1\\}$ and a feature vector $X \\in \\mathbb{R}^{d}$. A classifier produces a score $s(X)$ that is calibrated to the training distribution, meaning $s(X) = P_{\\text{train}}(Y=1 \\mid X)$, where the training class prior is $\\pi_{\\text{train}} = P_{\\text{train}}(Y=1)$. Assume a class prior shift between the training and validation distributions in the sense that the class-conditional feature densities satisfy $p_{\\text{train}}(X \\mid Y=y) = p_{\\text{val}}(X \\mid Y=y)$ for $y \\in \\{0,1\\}$, while the class prior changes to $\\pi_{\\text{val}} = P_{\\text{val}}(Y=1) \\neq \\pi_{\\text{train}}$. Predictions are made by thresholding the score: $\\hat{Y} = 1$ if $s(X) \\ge t$, and $\\hat{Y} = 0$ otherwise, under zero-one loss.\n\nStarting from Bayes' rule, derive an expression for the validation posterior $P_{\\text{val}}(Y=1 \\mid X)$ in terms of the calibrated score $s(X)$ and the priors $\\pi_{\\text{train}}$ and $\\pi_{\\text{val}}$, and show that the Bayes-optimal decision under the validation distribution can be implemented by thresholding $s(X)$ at a prior-adjusted threshold $t^{\\star}$ that depends on $\\pi_{\\text{train}}$ and $\\pi_{\\text{val}}$.\n\nTo quantify the effect of prior mismatch on validation error, suppose the score distribution conditional on the true label under the training distribution is\n$$\nf_{S \\mid Y=1}(s) = 2s, \\quad f_{S \\mid Y=0}(s) = 2(1-s), \\quad \\text{for } s \\in [0,1],\n$$\nwhich is consistent with calibration under $\\pi_{\\text{train}} = \\frac{1}{2}$ and a uniform marginal score density. Under the validation distribution, the expected zero-one error for a threshold $t$ is\n$$\n\\text{Err}_{\\text{val}}(t) = P_{\\text{val}}(\\hat{Y} \\neq Y) = (1-\\pi_{\\text{val}})\\int_{t}^{1} f_{S \\mid Y=0}(s) \\, ds + \\pi_{\\text{val}} \\int_{0}^{t} f_{S \\mid Y=1}(s) \\, ds.\n$$\n\nLet $\\pi_{\\text{train}} = \\frac{1}{2}$ and $\\pi_{\\text{val}} = \\frac{3}{10}$. Compute the absolute reduction in validation error $\\Delta$ obtained by using the prior-adjusted threshold $t^{\\star}$ instead of the naive threshold $t=\\frac{1}{2}$, that is,\n$$\n\\Delta = \\text{Err}_{\\text{val}}\\!\\left(\\frac{1}{2}\\right) - \\text{Err}_{\\text{val}}(t^{\\star}).\n$$\nRound your numerical answer to four significant figures. Express your final answer as a decimal without a percentage sign.", "solution": "The problem asks for three tasks: first, to derive an expression for the validation posterior probability $P_{\\text{val}}(Y=1 \\mid X)$ in terms of the calibrated training score $s(X) = P_{\\text{train}}(Y=1 \\mid X)$ and the training and validation priors, $\\pi_{\\text{train}}$ and $\\pi_{\\text{val}}$. Second, to show that the Bayes-optimal decision rule under the validation distribution corresponds to thresholding $s(X)$ at an adjusted threshold $t^{\\star}$. Third, to compute the reduction in validation error achieved by using this optimal threshold compared to a naive threshold.\n\nFirst, we derive the expression for the validation posterior $P_{\\text{val}}(Y=1 \\mid X)$. By Bayes' rule, the posterior probability on the validation distribution is:\n$$ P_{\\text{val}}(Y=1 \\mid X) = \\frac{p_{\\text{val}}(X \\mid Y=1) P_{\\text{val}}(Y=1)}{p_{\\text{val}}(X)} $$\nThe denominator, the marginal density $p_{\\text{val}}(X)$, can be expanded using the law of total probability:\n$$ p_{\\text{val}}(X) = p_{\\text{val}}(X \\mid Y=1) P_{\\text{val}}(Y=1) + p_{\\text{val}}(X \\mid Y=0) P_{\\text{val}}(Y=0) $$\nUsing the given priors $\\pi_{\\text{val}} = P_{\\text{val}}(Y=1)$ and $1-\\pi_{\\text{val}} = P_{\\text{val}}(Y=0)$, this becomes:\n$$ P_{\\text{val}}(Y=1 \\mid X) = \\frac{p_{\\text{val}}(X \\mid Y=1) \\pi_{\\text{val}}}{p_{\\text{val}}(X \\mid Y=1) \\pi_{\\text{val}} + p_{\\text{val}}(X \\mid Y=0) (1-\\pi_{\\text{val}})} $$\nThe problem states a class prior shift, where the class-conditional densities are unchanged: $p_{\\text{train}}(X \\mid Y=y) = p_{\\text{val}}(X \\mid Y=y)$ for $y \\in \\{0,1\\}$. Substituting these into the equation:\n$$ P_{\\text{val}}(Y=1 \\mid X) = \\frac{p_{\\text{train}}(X \\mid Y=1) \\pi_{\\text{val}}}{p_{\\text{train}}(X \\mid Y=1) \\pi_{\\text{val}} + p_{\\text{train}}(X \\mid Y=0) (1-\\pi_{\\text{val}})} $$\nNext, we relate the class-conditional densities $p_{\\text{train}}(X \\mid Y)$ to the calibrated score $s(X)$. From the definition $s(X) = P_{\\text{train}}(Y=1 \\mid X)$ and Bayes' rule on the training distribution:\n$$ s(X) = \\frac{p_{\\text{train}}(X \\mid Y=1) \\pi_{\\text{train}}}{p_{\\text{train}}(X)} \\implies p_{\\text{train}}(X \\mid Y=1) = \\frac{s(X) p_{\\text{train}}(X)}{\\pi_{\\text{train}}} $$\n$$ 1-s(X) = \\frac{p_{\\text{train}}(X \\mid Y=0) (1-\\pi_{\\text{train}})}{p_{\\text{train}}(X)} \\implies p_{\\text{train}}(X \\mid Y=0) = \\frac{(1-s(X)) p_{\\text{train}}(X)}{1-\\pi_{\\text{train}}} $$\nSubstituting these expressions for the class-conditional densities into the equation for $P_{\\text{val}}(Y=1 \\mid X)$:\n$$ P_{\\text{val}}(Y=1 \\mid X) = \\frac{\\frac{s(X) p_{\\text{train}}(X)}{\\pi_{\\text{train}}} \\pi_{\\text{val}}}{\\frac{s(X) p_{\\text{train}}(X)}{\\pi_{\\text{train}}} \\pi_{\\text{val}} + \\frac{(1-s(X)) p_{\\text{train}}(X)}{1-\\pi_{\\text{train}}} (1-\\pi_{\\text{val}})} $$\nThe term $p_{\\text{train}}(X)$ cancels from the numerator and denominator, yielding the desired expression:\n$$ P_{\\text{val}}(Y=1 \\mid X) = \\frac{\\frac{s(X) \\pi_{\\text{val}}}{\\pi_{\\text{train}}}}{\\frac{s(X) \\pi_{\\text{val}}}{\\pi_{\\text{train}}} + \\frac{(1-s(X))(1-\\pi_{\\text{val}})}{1-\\pi_{\\text{train}}}} $$\nThis can be simplified by multiplying the numerator and denominator by $\\pi_{\\text{train}}(1-\\pi_{\\text{train}})$:\n$$ P_{\\text{val}}(Y=1 \\mid X) = \\frac{s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}}}{s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}} + (1-s(X)) \\pi_{\\text{train}} (1-\\pi_{\\text{val}})} $$\nFor zero-one loss, the Bayes-optimal classifier predicts the class with the highest posterior probability. The decision rule under the validation distribution is to predict $\\hat{Y}=1$ if $P_{\\text{val}}(Y=1 \\mid X) \\ge \\frac{1}{2}$. Using the expression we derived, this is equivalent to:\n$$ \\frac{s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}}}{s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}} + (1-s(X)) \\pi_{\\text{train}} (1-\\pi_{\\text{val}})} \\ge \\frac{1}{2} $$\nAssuming $s(X) \\in (0,1)$, the denominator is positive, so we can cross-multiply:\n$$ 2 s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}} \\ge s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}} + (1-s(X)) \\pi_{\\text{train}} (1-\\pi_{\\text{val}}) $$\n$$ s(X) (1-\\pi_{\\text{train}}) \\pi_{\\text{val}} \\ge (1-s(X)) \\pi_{\\text{train}} (1-\\pi_{\\text{val}}) $$\n$$ s(X) [(1-\\pi_{\\text{train}}) \\pi_{\\text{val}} + \\pi_{\\text{train}} (1-\\pi_{\\text{val}})] \\ge \\pi_{\\text{train}} (1-\\pi_{\\text{val}}) $$\nThis shows that the optimal decision is to threshold the original score $s(X)$ at a new value $t^{\\star}$:\n$$ s(X) \\ge \\frac{\\pi_{\\text{train}} (1-\\pi_{\\text{val}})}{(1-\\pi_{\\text{train}}) \\pi_{\\text{val}} + \\pi_{\\text{train}} (1-\\pi_{\\text{val}})} = t^{\\star} $$\nNow we proceed to the numerical calculation. We are given $\\pi_{\\text{train}} = \\frac{1}{2}$ and $\\pi_{\\text{val}} = \\frac{3}{10}$.\nThe optimal threshold $t^{\\star}$ is:\n$$ t^{\\star} = \\frac{\\frac{1}{2} \\left(1-\\frac{3}{10}\\right)}{\\left(1-\\frac{1}{2}\\right) \\frac{3}{10} + \\frac{1}{2} \\left(1-\\frac{3}{10}\\right)} = \\frac{\\frac{1}{2} \\cdot \\frac{7}{10}}{\\frac{1}{2} \\cdot \\frac{3}{10} + \\frac{1}{2} \\cdot \\frac{7}{10}} = \\frac{\\frac{7}{20}}{\\frac{3}{20} + \\frac{7}{20}} = \\frac{\\frac{7}{20}}{\\frac{10}{20}} = \\frac{7}{10} = 0.7 $$\nThe validation error for a threshold $t$ is given by:\n$$ \\text{Err}_{\\text{val}}(t) = P_{\\text{val}}(\\hat{Y} \\neq Y) = (1-\\pi_{\\text{val}})\\int_{t}^{1} f_{S \\mid Y=0}(s) \\, ds + \\pi_{\\text{val}} \\int_{0}^{t} f_{S \\mid Y=1}(s) \\, ds $$\nWith $f_{S \\mid Y=1}(s) = 2s$ and $f_{S \\mid Y=0}(s) = 2(1-s)$, we evaluate the integrals:\n$$ \\int_{t}^{1} 2(1-s) \\, ds = \\left[ -(1-s)^2 \\right]_{t}^{1} = 0 - (-(1-t)^2) = (1-t)^2 $$\n$$ \\int_{0}^{t} 2s \\, ds = \\left[ s^2 \\right]_{0}^{t} = t^2 $$\nSubstituting these into the error expression with $\\pi_{\\text{val}} = \\frac{3}{10}$:\n$$ \\text{Err}_{\\text{val}}(t) = \\left(1-\\frac{3}{10}\\right) (1-t)^2 + \\frac{3}{10} t^2 = \\frac{7}{10} (1-t)^2 + \\frac{3}{10} t^2 $$\nWe need to calculate the error for the naive threshold $t=\\frac{1}{2}$ and the optimal threshold $t^{\\star}=\\frac{7}{10}$.\nFor $t=\\frac{1}{2}$:\n$$ \\text{Err}_{\\text{val}}\\!\\left(\\frac{1}{2}\\right) = \\frac{7}{10} \\left(1-\\frac{1}{2}\\right)^2 + \\frac{3}{10} \\left(\\frac{1}{2}\\right)^2 = \\frac{7}{10} \\left(\\frac{1}{4}\\right) + \\frac{3}{10} \\left(\\frac{1}{4}\\right) = \\left(\\frac{7}{10} + \\frac{3}{10}\\right) \\frac{1}{4} = 1 \\cdot \\frac{1}{4} = \\frac{1}{4} = 0.25 $$\nFor $t^{\\star}=\\frac{7}{10}$:\n$$ \\text{Err}_{\\text{val}}(t^{\\star}) = \\text{Err}_{\\text{val}}\\!\\left(\\frac{7}{10}\\right) = \\frac{7}{10} \\left(1-\\frac{7}{10}\\right)^2 + \\frac{3}{10} \\left(\\frac{7}{10}\\right)^2 = \\frac{7}{10} \\left(\\frac{3}{10}\\right)^2 + \\frac{3}{10} \\left(\\frac{7}{10}\\right)^2 $$\n$$ = \\frac{7}{10} \\cdot \\frac{9}{100} + \\frac{3}{10} \\cdot \\frac{49}{100} = \\frac{63}{1000} + \\frac{147}{1000} = \\frac{210}{1000} = 0.21 $$\nThe absolute reduction in validation error $\\Delta$ is the difference between these two error values:\n$$ \\Delta = \\text{Err}_{\\text{val}}\\!\\left(\\frac{1}{2}\\right) - \\text{Err}_{\\text{val}}(t^{\\star}) = 0.25 - 0.21 = 0.04 $$\nThe problem requires the answer to be rounded to four significant figures. Thus, $0.04$ is written as $0.04000$.", "answer": "$$\\boxed{0.04000}$$", "id": "3187498"}]}