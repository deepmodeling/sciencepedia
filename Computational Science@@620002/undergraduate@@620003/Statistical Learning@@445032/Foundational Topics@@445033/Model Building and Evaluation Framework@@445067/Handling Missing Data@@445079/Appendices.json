{"hands_on_practices": [{"introduction": "Our exploration into handling missing data begins with the most straightforward technique: mean imputation. This method replaces a missing value with the average of the observed values for that same feature. While simple to implement, its impact on the dataset's statistical properties requires careful consideration, as we will see. This first exercise [@problem_id:1437187] provides a concrete, hands-on calculation to ground your understanding of this fundamental imputation method.", "problem": "A research group is studying the transcriptional response of human cells to a novel therapeutic compound. They measured the expression levels of five key signaling genes (labeled G1, G2, G3, G4, G5) across four replicate cell cultures (labeled S1, S2, S3, S4). The data, represented as normalized expression units, are organized in a matrix where rows correspond to genes and columns correspond to replicates.\n\nDue to a smudge on the microarray slide, the expression value for gene G3 in replicate S3 could not be read. The recorded dataset is as follows, with 'X' denoting the missing value:\n\n- **G1:** [2.15, 2.30, 2.25, 2.18]\n- **G2:** [0.88, 0.95, 0.91, 1.02]\n- **G3:** [1.54, 1.71, X, 1.60]\n- **G4:** [3.40, 3.11, 3.25, 3.32]\n- **G5:** [0.45, 0.39, 0.41, 0.44]\n\nTo complete the dataset before further statistical analysis, the team decides to employ a basic data imputation technique. Your task is to calculate the value of 'X' using gene-wise mean imputation. This technique involves replacing the missing value for a specific gene with the arithmetic mean of the observed expression values for that same gene from all other replicates.\n\nCalculate the imputed expression value for gene G3 in replicate S3. Round your final answer to three significant figures.", "solution": "We use gene-wise mean imputation: replace the missing value by the arithmetic mean of the observed values for the same gene. For gene G3, the observed expression values are $1.54$, $1.71$, and $1.60$ across the available replicates.\n\nThe imputed value $X$ is computed as\n$$\nX=\\frac{1}{3}\\left(1.54+1.71+1.60\\right).\n$$\nFirst sum the observed values:\n$$\n1.54+1.71=3.25,\\quad 3.25+1.60=4.85.\n$$\nDivide by the number of observed values:\n$$\nX=\\frac{4.85}{3}=1.616\\overline{6}.\n$$\nRounding to three significant figures gives\n$$\nX\\approx 1.62.\n$$", "answer": "$$\\boxed{1.62}$$", "id": "1437187"}, {"introduction": "Having practiced the mechanics of mean imputation, we now critically examine its limitations. A good statistical method should preserve the underlying characteristics of the data, but simple imputation often fails this test. This next problem [@problem_id:1437194] uses a thought experiment to demonstrate how mean imputation can artificially reduce the variance and distort the shape of a dataset's distribution, especially in scenarios where the data is not concentrated around a single central point.", "problem": "In a systems biology experiment, the expression of a gene, `Gene-Z`, is analyzed across a large population of cells. The gene exhibits bistable behavior: a fraction $p$ of the cells are in a 'low-expression' state, and the remaining fraction $1-p$ are in a 'high-expression' state. We can model this idealized scenario by assuming that every cell in the low-expression state has an expression level of exactly $E_{\\text{low}}$, and every cell in the high-expression state has an expression level of exactly $E_{\\text{high}}$, where $E_{\\text{high}} > E_{\\text{low}}$.\n\nDuring data acquisition, a technical malfunction results in the loss of a fraction $f$ of the total data points. A follow-up analysis reveals that all of these lost measurements came exclusively from cells in the 'high-expression' state. Assume that $f  1-p$ to ensure some high-expression data remains observed.\n\nAn analyst, unaware of the bimodal nature of the data, decides to handle the missing values using simple mean imputation. They calculate the arithmetic mean of the remaining, observed data points and replace each missing value with this single calculated mean.\n\nThe goal is to quantify the distortion in the data's variability caused by this imputation method. Calculate the ratio of the population variance of the final, imputed dataset to the population variance of the original, complete dataset (before any data was lost). Express your answer as a symbolic expression in terms of $p$ and $f$.", "solution": "Let the two expression levels be $E_{\\text{low}}$ and $E_{\\text{high}}$, with $E_{\\text{high}}  E_{\\text{low}}$. Define $D = E_{\\text{high}} - E_{\\text{low}}$ and $a = E_{\\text{low}}$ so that $E_{\\text{high}} = a + D$. In the original complete population, the mean is\n$$\\mu_{\\text{orig}} = p\\,a + (1-p)(a + D) = a + (1-p)D,$$\nand the population variance of a two-point mixture is\n$$\\operatorname{Var}_{\\text{orig}} = p(1-p)D^{2}.$$\n\nA fraction $f$ of all data points is lost, all from the high state. The observed data therefore has weights $p$ at $a$ and $(1-p) - f$ at $a + D$, with total observed fraction $1 - f$. The observed mean used for imputation is\n$$m_{\\text{obs}} = \\frac{p\\,a + \\big((1-p)-f\\big)(a + D)}{1 - f} = a + \\frac{1 - p - f}{1 - f}\\,D.$$\nAfter mean imputation, the completed dataset has three point masses: $p$ at $a$, $(1-p)-f$ at $a + D$, and $f$ at $m_{\\text{obs}}$. The final mean is\n$$\\mu_{\\text{final}} = p\\,a + \\big((1-p)-f\\big)(a + D) + f\\,m_{\\text{obs}} = \\frac{p\\,a + \\big((1-p)-f\\big)(a + D)}{1 - f} = m_{\\text{obs}}.$$\nThus the imputed group at $m_{\\text{obs}}$ contributes zero to the variance. The final variance is\n$$\\operatorname{Var}_{\\text{final}} = p\\big(a - \\mu_{\\text{final}}\\big)^{2} + \\big(1-p - f\\big)\\big((a + D) - \\mu_{\\text{final}}\\big)^{2}.$$\nUsing $\\mu_{\\text{final}} = a + \\frac{1 - p - f}{1 - f}\\,D$, the deviations are\n$$a - \\mu_{\\text{final}} = -\\frac{1 - p - f}{1 - f}\\,D,\\qquad (a + D) - \\mu_{\\text{final}} = \\frac{p}{1 - f}\\,D.$$\nSubstituting,\n$$\\operatorname{Var}_{\\text{final}} = p\\left(\\frac{1 - p - f}{1 - f}\\right)^{2} D^{2} + \\big(1 - p - f\\big)\\left(\\frac{p}{1 - f}\\right)^{2} D^{2}.$$\nFactorizing and simplifying,\n$$\\operatorname{Var}_{\\text{final}} = \\frac{D^{2}}{(1 - f)^{2}}\\left[p(1 - p - f)^{2} + p^{2}(1 - p - f)\\right] = \\frac{D^{2}}{(1 - f)^{2}}\\,p(1 - p - f)\\big((1 - p - f) + p\\big) = \\frac{D^{2}}{(1 - f)^{2}}\\,p(1 - p - f)(1 - f) = \\frac{D^{2}p(1 - p - f)}{1 - f}.$$\nThe ratio of the final to the original population variance is therefore\n$$\\frac{\\operatorname{Var}_{\\text{final}}}{\\operatorname{Var}_{\\text{orig}}} = \\frac{\\frac{D^{2}p(1 - p - f)}{1 - f}}{p(1-p)D^{2}} = \\frac{1 - p - f}{(1 - f)(1 - p)}.$$\nThis depends only on $p$ and $f$, as required, and satisfies the checks that it equals $1$ when $f=0$ and decreases toward $0$ as $f \\to 1 - p$.", "answer": "$$\\boxed{\\frac{1-p-f}{(1-f)(1-p)}}$$", "id": "1437194"}, {"introduction": "The shortcomings of simple imputation motivate the need for more statistically rigorous approaches. In practice, analysts choose from several established methods, each with its own strengths, weaknesses, and theoretical guarantees. This advanced exercise [@problem_id:3127599] challenges you to move beyond calculation and engage with the core principles of modern missing data analysis, comparing the properties of complete-case analysis, inverse probability weighting (IPW), and multiple imputation (MI) under the key 'Missing At Random' (MAR) assumption.", "problem": "You observe $n$ independent and identically distributed units $(Y_i, X_i, R_i)$ for $i=1,\\dots,n$, where $Y_i \\in \\mathbb{R}$ is a response, $X_i \\in \\mathbb{R}^p$ is a covariate vector, and $R_i \\in \\{0,1\\}$ indicates whether $Y_i$ is observed ($R_i = 1$) or missing ($R_i = 0$). The data are generated from the linear model $Y_i = X_i^\\top \\beta + \\varepsilon_i$ with $E[\\varepsilon_i \\mid X_i] = 0$ and finite second moments, and the parameter of interest is $\\beta \\in \\mathbb{R}^p$. Assume that the probability of observation satisfies Missing At Random (MAR): $P(R_i = 1 \\mid X_i, Y_i) = P(R_i = 1 \\mid X_i) \\equiv \\pi(X_i)$, where $0  \\pi(X_i)  1$ for values of $X_i$ in the support.\n\nConsider three estimators of $\\beta$:\n\n- Complete-case Ordinary Least Squares (OLS), which fits the linear model using only the cases with $R_i = 1$.\n- Inverse Probability Weighting (IPW), which fits a weighted least squares regression among observed cases with weights $w_i = 1/\\pi(X_i)$, where $\\pi(X_i)$ is known or consistently estimated.\n- Multiple Imputation (MI), which replaces missing $Y_i$ values by independent draws from a correctly specified predictive distribution for $Y_i \\mid X_i$, fits OLS on each of $m$ completed datasets, and combines results using Rubinâ€™s rules for Multiple Imputation (MI).\n\nFrom the above fundamental definitions and the MAR property, select all statements that are correct about unbiasedness/consistency and relative efficiency, and derive the precise conditions under which complete-case OLS is unbiased for $\\beta$.\n\nA. Under MAR with $P(R=1 \\mid X,Y) = P(R=1 \\mid X)$, the linear model $E[Y \\mid X] = X^\\top \\beta$, $E[\\varepsilon \\mid X] = 0$, positivity $P(R=1 \\mid X)  0$, and full column rank of $E[X X^\\top \\mid R=1]$, complete-case OLS is unbiased for $\\beta$.\n\nB. IPW with weights $w(X) = 1/P(R=1 \\mid X)$ yields a consistent estimator of $\\beta$ under MAR even if the outcome model $E[Y \\mid X] = X^\\top \\beta$ is misspecified.\n\nC. MI yields a consistent estimator of $\\beta$ under MAR if the imputation model for $Y \\mid X$ is correctly specified and $m \\to \\infty$, but it can be biased if the missingness depends on $Y$ given $X$.\n\nD. Case-wise deletion is unbiased whenever missingness depends on $Y$ only (that is, $P(R=1 \\mid X, Y) = P(R=1 \\mid Y)$), because the relationship between $X$ and $Y$ among observed cases is preserved.\n\nE. When missingness depends on $X$ (and not on $Y$ given $X$), complete-case OLS is unbiased but typically less efficient than IPW or MI, because IPW and MI can exploit the information in the observation probabilities or the predictive distribution to reduce variance.\n\nChoose all that apply.", "solution": "The problem asks for an evaluation of several statements regarding three common methods for handling missing dataâ€”Complete-Case (CC) analysis, Inverse Probability Weighting (IPW), and Multiple Imputation (MI)â€”under a set of standard assumptions. The core assumptions are a linear model $Y_i = X_i^\\top \\beta + \\varepsilon_i$ with $E[\\varepsilon_i \\mid X_i] = 0$, and a Missing At Random (MAR) mechanism where $P(R_i = 1 \\mid X_i, Y_i) = P(R_i = 1 \\mid X_i) \\equiv \\pi(X_i)$. First, we derive the conditions under which the Complete-Case OLS estimator is unbiased for $\\beta$.\n\nThe CC OLS estimator, $\\hat{\\beta}_{CC}$, is obtained by applying Ordinary Least Squares to the subset of data where the response $Y_i$ is observed, i.e., where $R_i=1$. The estimator is given by:\n$$ \\hat{\\beta}_{CC} = \\left( \\sum_{i=1}^n R_i X_i X_i^\\top \\right)^{-1} \\left( \\sum_{i=1}^n R_i X_i Y_i \\right) $$\nFor this estimator to be unbiased for $\\beta$, the linear model relationship must hold for the selected sub-population. Specifically, the conditional expectation of the error term, given the covariates and the fact that the observation is selected, must be zero. That is, we require $E[\\varepsilon_i \\mid X_i, R_i=1] = 0$.\n\nLet's verify this condition using the problem's assumptions. The definition of conditional expectation states:\n$$ E[\\varepsilon_i \\mid X_i, R_i=1] = \\frac{E[\\varepsilon_i \\cdot \\mathbb{I}(R_i=1) \\mid X_i]}{P(R_i=1 \\mid X_i)} $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The positivity assumption $P(R_i=1 \\mid X_i)  0$ ensures the denominator is non-zero.\nWe analyze the numerator using the law of iterated expectations:\n$$ E[\\varepsilon_i \\cdot \\mathbb{I}(R_i=1) \\mid X_i] = E\\left[ E[\\varepsilon_i \\cdot \\mathbb{I}(R_i=1) \\mid X_i, \\varepsilon_i] \\mid X_i \\right] $$\n$$ = E\\left[ \\varepsilon_i \\cdot E[\\mathbb{I}(R_i=1) \\mid X_i, \\varepsilon_i] \\mid X_i \\right] $$\n$$ = E\\left[ \\varepsilon_i \\cdot P(R_i=1 \\mid X_i, \\varepsilon_i) \\mid X_i \\right] $$\nThe MAR assumption, $P(R_i=1 \\mid X_i, Y_i) = P(R_i=1 \\mid X_i)$, is equivalent to stating that missingness is independent of $Y_i$ after conditioning on $X_i$. Since $Y_i = X_i^\\top \\beta + \\varepsilon_i$, this is equivalent to $P(R_i=1 \\mid X_i, \\varepsilon_i) = P(R_i=1 \\mid X_i) = \\pi(X_i)$. Substituting this into the equation:\n$$ E[\\varepsilon_i \\cdot \\mathbb{I}(R_i=1) \\mid X_i] = E\\left[ \\varepsilon_i \\cdot \\pi(X_i) \\mid X_i \\right] $$\nSince $\\pi(X_i)$ is a function of $X_i$, it can be treated as a constant with respect to the inner expectation over $\\varepsilon_i$ conditional on $X_i$:\n$$ E[\\varepsilon_i \\cdot \\mathbb{I}(R_i=1) \\mid X_i] = \\pi(X_i) \\cdot E[\\varepsilon_i \\mid X_i] $$\nThe problem states that the data are generated from a model with $E[\\varepsilon_i \\mid X_i] = 0$. Therefore, the numerator is $0$:\n$$ E[\\varepsilon_i \\cdot \\mathbb{I}(R_i=1) \\mid X_i] = \\pi(X_i) \\cdot 0 = 0 $$\nThis implies $E[\\varepsilon_i \\mid X_i, R_i=1] = 0$. Consequently, the conditional expectation of $Y_i$ in the selected sample is:\n$$ E[Y_i \\mid X_i, R_i=1] = E[X_i^\\top \\beta + \\varepsilon_i \\mid X_i, R_i=1] = X_i^\\top \\beta + E[\\varepsilon_i \\mid X_i, R_i=1] = X_i^\\top \\beta $$\nThis shows that the linear relationship holds in the sub-population of complete cases. Thus, OLS applied to this sub-population will yield an unbiased and consistent estimator for $\\beta$, provided the standard OLS regularity conditions (e.g., non-collinearity of covariates in the sub-population) are met.\n\nNow we evaluate each statement.\n\n**A. Under MAR with $P(R=1 \\mid X,Y) = P(R=1 \\mid X)$, the linear model $E[Y \\mid X] = X^\\top \\beta$, $E[\\varepsilon \\mid X] = 0$, positivity $P(R=1 \\mid X)  0$, and full column rank of $E[X X^\\top \\mid R=1]$, complete-case OLS is unbiased for $\\beta$.**\nThis statement precisely summarizes the conditions derived above. The MAR assumption and the linear model with $E[\\varepsilon \\mid X] = 0$ are the crucial elements for the expectation of the error term to be zero in the selected sample. The positivity assumption is necessary for the conditional probabilities to be well-defined. The full column rank of $E[X X^\\top \\mid R=1]$ is the standard OLS condition ensuring that the moment matrix for the observed data is invertible, which guarantees a unique solution. Our derivation confirms that these conditions are sufficient for the unbiasedness of the complete-case estimator.\n**Verdict: Correct.**\n\n**B. IPW with weights $w(X) = 1/P(R=1 \\mid X)$ yields a consistent estimator of $\\beta$ under MAR even if the outcome model $E[Y \\mid X] = X^\\top \\beta$ is misspecified.**\nThe IPW estimator solves the estimating equation $\\sum_i w_i R_i X_i (Y_i - X_i^\\top \\beta) = 0$. For consistency, its population counterpart must be zero at the true $\\beta$: $E\\left[\\frac{R}{\\pi(X)} X (Y - X^\\top \\beta)\\right] = 0$. Under MAR, this simplifies to $E[X(Y - X^\\top \\beta)] = 0$. This population moment condition is identical to that of the OLS estimator on the full (unobserved) data. The consistency of the OLS estimator for $\\beta$ depends critically on the correct specification of the outcome model, i.e., $E[Y \\mid X] = X^\\top \\beta$. If this model is misspecified, OLS (and thus IPW) will consistently estimate the parameter of the best linear projection of $Y$ onto $X$, which is generally not the parameter $\\beta$ of the true data-generating process. IPW's role is to correct for selection bias caused by missing data, not to correct for a misspecified outcome model.\n**Verdict: Incorrect.**\n\n**C. MI yields a consistent estimator of $\\beta$ under MAR if the imputation model for $Y \\mid X$ is correctly specified and $m \\to \\infty$, but it can be biased if the missingness depends on $Y$ given $X$.**\nThis statement has two parts. First, under MAR, the distribution of $Y_i$ given $X_i$ is the same for both observed and missing units: $f(Y \\mid X, R=1) = f(Y \\mid X, R=0) = f(Y \\mid X)$. Therefore, an imputation model for $Y \\mid X$ based on the observed data is valid for imputing the missing data. If this model is correctly specified and the number of imputations $m$ is large, MI theory guarantees that the combined estimator is consistent for $\\beta$. Second, if the missingness depends on $Y$ given $X$, the mechanism is Missing Not At Random (MNAR). In this case, $f(Y \\mid X, R=1) \\neq f(Y \\mid X, R=0)$. An imputation model built on observed data will be incorrect for the missing data, leading to biased imputations and, consequently, a biased estimator for $\\beta$. Both parts of the statement are fundamental principles of multiple imputation.\n**Verdict: Correct.**\n\n**D. Case-wise deletion is unbiased whenever missingness depends on $Y$ only (that is, $P(R=1 \\mid X, Y) = P(R=1 \\mid Y)$), because the relationship between $X$ and $Y$ among observed cases is preserved.**\nCase-wise deletion is another name for complete-case analysis. This scenario, where missingness depends on the outcome $Y$, is a form of MNAR. As shown in the derivation for CC unbiasedness, the key condition is $E[\\varepsilon_i \\mid X_i, R_i=1] = 0$. If missingness depends on $Y_i = X_i^\\top \\beta + \\varepsilon_i$, then it also depends on $\\varepsilon_i$. This generally induces a correlation between $\\varepsilon_i$ and the selection indicator $R_i$, conditional on $X_i$. For example, if individuals with high values of $Y$ are less likely to be observed, then the observed sample will be depleted of individuals with large positive error terms $\\varepsilon_i$, leading to $E[\\varepsilon_i \\mid X_i, R_i=1]  0$ and a biased estimate of $\\beta$. The assertion that the relationship between $X$ and $Y$ is preserved is false; selecting on the outcome variable typically induces bias in the estimated coefficients of its predictors.\n**Verdict: Incorrect.**\n\n**E. When missingness depends on $X$ (and not on $Y$ given $X$), complete-case OLS is unbiased but typically less efficient than IPW or MI, because IPW and MI can exploit the information in the observation probabilities or the predictive distribution to reduce variance.**\nThe first part of the statement, that CC OLS is unbiased under this MAR mechanism, was proven in our initial derivation and in the analysis of option A. The second part compares the efficiency of CC to IPW and MI. CC analysis discards all information from units with missing $Y_i$, including their covariate values $X_i$. In contrast, both IPW and MI utilize the full set of covariates. IPW uses them to model or apply the observation probabilities $\\pi(X_i)$, and MI uses them to build a predictive model for $Y_i \\mid X_i$. By using more of the available information, these methods are, in principle, more statistically efficient. While specific implementations of IPW can have high variance (e.g., with extreme weights), the general principle is that methods that discard less information are \"typically\" more efficient. MI, in particular, is well-regarded for its efficiency gains over CC under MAR. The reasoning providedâ€”that these methods exploit information that CC ignoresâ€”is conceptually sound.\n**Verdict: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "3127599"}]}