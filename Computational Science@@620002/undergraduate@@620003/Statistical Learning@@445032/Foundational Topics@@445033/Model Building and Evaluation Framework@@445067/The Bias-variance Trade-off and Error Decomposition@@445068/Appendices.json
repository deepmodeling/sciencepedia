{"hands_on_practices": [{"introduction": "Multicollinearity, or high correlation between predictors, is a common issue in linear regression that can dramatically inflate the variance of estimators. This exercise explores how ridge regression directly confronts this problem by adding a penalty term, which stabilizes the model at the cost of introducing a small amount of bias. By deriving and comparing the mean-squared prediction error under both an orthogonal and a highly correlated design, you will gain a concrete, analytical understanding of how the interplay between data structure, represented by the matrix $X^{\\top} X$, and the regularization parameter $\\lambda$ governs the bias-variance trade-off [@problem_id:3180600].", "problem": "Consider the standard linear model $Y = X \\beta + \\varepsilon$ with $X \\in \\mathbb{R}^{n \\times p}$, true parameter vector $\\beta \\in \\mathbb{R}^{p}$, and noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. For a fixed regularization strength $\\lambda > 0$, the ridge estimator is $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y$. For a new input $x_{0} \\in \\mathbb{R}^{p}$, define the prediction $\\hat{f}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$. The mean-squared prediction error for the conditional mean at $x_{0}$ is $\\mathrm{MSE}(x_{0}) = \\mathbb{E}\\big[(\\hat{f}_{\\lambda}(x_{0}) - x_{0}^{\\top} \\beta)^{2}\\big]$, where the expectation is over the training noise $\\varepsilon$ only.\n\nYou will compare two designs with $p = 2$ and the same sample size $n$ and noise level $\\sigma^{2}$:\n- Design A (orthogonal): $X^{\\top} X = n I_{2}$.\n- Design B (highly correlated): $X^{\\top} X = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with correlation $\\rho \\in (0, 1)$.\n\nUsing only the model definition, the ridge estimator definition, and the definitions of expectation and variance, first derive closed-form expressions for the prediction bias $\\mathrm{Bias}(x_{0}) = \\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] - x_{0}^{\\top} \\beta$ and the prediction variance $\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0}))$ in each design. Then, for the specific values $n = 100$, $\\sigma^{2} = 1$, $\\lambda = 1$, $\\rho = 0.9$, $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, compute the ratio\n$$\nR \\;=\\; \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})},\n$$\nwhere $\\mathrm{MSE}_{A}(x_{0})$ and $\\mathrm{MSE}_{B}(x_{0})$ denote the mean-squared prediction errors under Design A and Design B, respectively. Here $\\mathrm{MSE}(x_{0})$ refers to the error for predicting the conditional mean $x_{0}^{\\top} \\beta$ and therefore equals the sum of squared bias and variance.\n\nProvide your final numerical value of $R$ rounded to four significant figures.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Model**: The standard linear model is $Y = X \\beta + \\varepsilon$.\n- **Dimensions and Distributions**: $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^{p}$, and the noise vector is $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$.\n- **Estimator**: The ridge regression estimator for the parameter vector $\\beta$ is $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y$, where $\\lambda > 0$ is the regularization parameter.\n- **Prediction**: The prediction for a new input $x_{0} \\in \\mathbb{R}^{p}$ is $\\hat{f}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$.\n- **Error Metric**: The mean-squared prediction error for the conditional mean $x_0^{\\top}\\beta$ is $\\mathrm{MSE}(x_{0}) = \\mathbb{E}\\big[(\\hat{f}_{\\lambda}(x_{0}) - x_{0}^{\\top} \\beta)^{2}\\big]$, where the expectation is over the training noise $\\varepsilon$. This can be decomposed as $\\mathrm{MSE}(x_{0}) = (\\mathrm{Bias}(x_{0}))^2 + \\mathrm{Var}(x_{0})$.\n- **Bias**: $\\mathrm{Bias}(x_{0}) = \\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] - x_{0}^{\\top} \\beta$.\n- **Variance**: $\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0}))$.\n- **Dimensions**: $p = 2$.\n- **Design A (Orthogonal)**: The design matrix satisfies $X^{\\top} X = n I_{2}$.\n- **Design B (Correlated)**: The design matrix satisfies $X^{\\top} X = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with correlation $\\rho \\in (0, 1)$.\n- **Numerical Values**: $n = 100$, $\\sigma^{2} = 1$, $\\lambda = 1$, $\\rho = 0.9$.\n- **Vectors**: $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n- **Objective**: Compute the ratio $R = \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in statistical learning theory, specifically the analysis of the ridge regression estimator.\n- **Scientifically Grounded**: The problem is built upon fundamental principles of linear models, statistical estimation, and the bias-variance trade-off. All concepts are standard in statistics.\n- **Well-Posed**: The problem provides all necessary definitions, data, and constraints to derive the required quantities and compute a unique numerical answer.\n- **Objective**: The problem is stated in precise mathematical language, free from any subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation of General Bias and Variance\nFirst, we derive general expressions for the bias and variance of the prediction $\\hat{f}_{\\lambda}(x_{0})$.\n\nThe expectation of the predictor is:\n$$\n\\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] = \\mathbb{E}[x_{0}^{\\top} \\hat{\\beta}_{\\lambda}] = x_{0}^{\\top} \\mathbb{E}[\\hat{\\beta}_{\\lambda}]\n$$\nSince $\\mathbb{E}[Y] = \\mathbb{E}[X \\beta + \\varepsilon] = X \\beta$, the expectation of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda}] = \\mathbb{E}[(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta)\n$$\nThe bias of the prediction is then:\n$$\n\\mathrm{Bias}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - x_{0}^{\\top} \\beta = x_{0}^{\\top} \\left[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_p \\right] \\beta\n$$\nUsing the identity $I_p = (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p)$, we simplify the term in the brackets:\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p) = -\\lambda (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\nSo, the bias is:\n$$\n\\mathrm{Bias}(x_{0}) = -\\lambda x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} \\beta\n$$\nThe variance of the prediction is:\n$$\n\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0})) = \\mathrm{Var}(x_{0}^{\\top} \\hat{\\beta}_{\\lambda}) = x_{0}^{\\top} \\mathrm{Var}(\\hat{\\beta}_{\\lambda}) x_{0}\n$$\nThe variance of the estimator is:\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\mathrm{Var}((X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y) = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\mathrm{Var}(Y) (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{p})^{-1})^{\\top}\n$$\nUsing $\\mathrm{Var}(Y) = \\sigma^2 I_n$ and the fact that $(X^{\\top} X + \\lambda I_{p})^{-1}$ is symmetric:\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\sigma^{2} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\nSo, the prediction variance is:\n$$\n\\mathrm{Var}(x_{0}) = \\sigma^{2} x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1} x_{0}\n$$\n\n### Analysis of Design A (Orthogonal)\nFor Design A, $X^{\\top} X = n I_{2}$.\nThe term $(X^{\\top} X + \\lambda I_{2})$ becomes $n I_{2} + \\lambda I_{2} = (n+\\lambda)I_2$. Its inverse is $\\frac{1}{n+\\lambda}I_2$.\n\n**Squared Bias for Design A**:\n$$\n(\\mathrm{Bias}_{A}(x_{0}))^2 = \\left( -\\lambda x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) \\beta \\right)^2 = \\frac{\\lambda^2}{(n+\\lambda)^2} (x_{0}^{\\top} \\beta)^2\n$$\nWith the given values, $x_{0}^{\\top} \\beta = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + (-1) \\cdot 1 = 0$.\nTherefore, $(\\mathrm{Bias}_{A}(x_{0}))^2 = 0$.\n\n**Variance for Design A**:\n$$\n\\mathrm{Var}_{A}(x_{0}) = \\sigma^{2} x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) (n I_2) \\left(\\frac{1}{n+\\lambda}I_2\\right) x_{0} = \\frac{n \\sigma^2}{(n+\\lambda)^2} x_{0}^{\\top} x_{0}\n$$\nWith the given values, $x_{0}^{\\top} x_{0} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1^2 + (-1)^2 = 2$.\nSo, $\\mathrm{Var}_{A}(x_{0}) = \\frac{100 \\cdot 1}{(100+1)^2} \\cdot 2 = \\frac{200}{101^2} = \\frac{200}{10201}$.\n\n**MSE for Design A**:\n$\\mathrm{MSE}_{A}(x_{0}) = (\\mathrm{Bias}_{A}(x_{0}))^2 + \\mathrm{Var}_{A}(x_{0}) = 0 + \\frac{200}{10201} = \\frac{200}{10201}$.\n\n### Analysis of Design B (Correlated)\nFor Design B, $X^{\\top} X = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$. Let's find the eigendecomposition of this matrix.\nThe eigenvalues of $\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ are $1+\\rho$ and $1-\\rho$.\nThe corresponding unnormalized eigenvectors are $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nSo, the eigenvalues of $X^{\\top} X$ are $\\lambda_1 = n(1+\\rho)$ and $\\lambda_2 = n(1-\\rho)$.\nThe normalized eigenvectors are $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe given vectors are $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\sqrt{2} v_1$ and $x_0 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\sqrt{2} v_2$. They are proportional to the eigenvectors.\n\nLet $U = \\begin{pmatrix} v_1 & v_2 \\end{pmatrix}$ be the matrix of eigenvectors. Then $X^{\\top}X = U D U^{\\top}$ where $D = \\mathrm{diag}(\\lambda_1, \\lambda_2)$.\nThen $X^{\\top}X + \\lambda I_2 = U D U^{\\top} + \\lambda U U^{\\top} = U(D+\\lambda I_2)U^{\\top}$.\nThe inverse is $(X^{\\top}X + \\lambda I_2)^{-1} = U(D+\\lambda I_2)^{-1}U^{\\top}$.\n\n**Squared Bias for Design B**:\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} \\beta\n$$\nWe have $x_0 = \\sqrt{2} v_2$ and $\\beta = \\sqrt{2} v_1$.\n$U^{\\top}x_0 = \\sqrt{2} U^{\\top}v_2 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $U^{\\top}\\beta = \\sqrt{2} U^{\\top}v_1 = \\sqrt{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda (\\sqrt{2}\\begin{pmatrix} 0 & 1 \\end{pmatrix}) (D+\\lambda I_2)^{-1} (\\sqrt{2}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}) = -2\\lambda \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\lambda_1+\\lambda} & 0 \\\\ 0 & \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -2\\lambda \\begin{pmatrix} 0 & \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0\n$$\nTherefore, $(\\mathrm{Bias}_{B}(x_{0}))^2 = 0$.\n\n**Variance for Design B**:\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} U D U^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} x_{0}\n$$\nThis simplifies to:\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U D(D+\\lambda I_2)^{-2} U^{\\top} x_{0}\n$$\nUsing $U^{\\top}x_0 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 (\\sqrt{2}\\begin{pmatrix} 0 & 1 \\end{pmatrix}) \\begin{pmatrix} \\frac{\\lambda_1}{(\\lambda_1+\\lambda)^2} & 0 \\\\ 0 & \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2} \\end{pmatrix} (\\sqrt{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}) = 2\\sigma^2 \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2}\n$$\nNow, substitute the numerical values:\n$\\lambda_2 = n(1-\\rho) = 100(1-0.9) = 100(0.1) = 10$.\n$\\sigma^2 = 1$, $\\lambda=1$.\n$$\n\\mathrm{Var}_{B}(x_{0}) = 2(1) \\frac{10}{(10+1)^2} = \\frac{20}{11^2} = \\frac{20}{121}\n$$\n\n**MSE for Design B**:\n$\\mathrm{MSE}_{B}(x_{0}) = (\\mathrm{Bias}_{B}(x_{0}))^2 + \\mathrm{Var}_{B}(x_{0}) = 0 + \\frac{20}{121} = \\frac{20}{121}$.\n\n### Calculation of the Ratio R\nThe ratio $R$ is the quotient of the two MSE values.\n$$\nR = \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})} = \\frac{20/121}{200/10201} = \\frac{20}{121} \\times \\frac{10201}{200} = \\frac{10201}{10 \\times 121} = \\frac{10201}{1210}\n$$\nFor the numerical value rounded to four significant figures:\n$R \\approx 8.4305785...$\nRounding to four significant figures gives $8.431$.", "answer": "$$\n\\boxed{8.431}\n$$", "id": "3180600"}, {"introduction": "In nonparametric settings, the bias-variance trade-off is often controlled by a smoothing parameter, such as the bandwidth $h$ in kernel regression. This hands-on simulation challenges you to model a function with varying smoothness, where a single global bandwidth is inherently a compromise. By implementing and comparing a global cross-validation approach with a spatially adaptive one, you will develop a practical intuition for how localizing the bias-variance trade-off can lead to significantly improved predictive performance [@problem_id:3180581].", "problem": "Consider the nonparametric regression setting with inputs $x \\in [0,1]$ drawn independently and identically distributed from a uniform distribution on $[0,1]$, and outputs $y$ given by $y = f(x) + \\varepsilon$, where $\\varepsilon$ is independent noise distributed as a normal distribution with mean $0$ and variance $\\sigma^2$. The target function $f(x)$ exhibits spatially varying smoothness and is defined for all $x \\in [0,1]$ by\n$$\nf(x) = \\sin(2\\pi x) + 0.5 \\exp\\!\\big(-200(x-0.75)^2\\big).\n$$\nThe goal is to design a simulation to investigate how spatially adaptive bandwidth selection in kernel regression can balance local bias and variance, compared to a single global bandwidth.\n\nStarting only from the definition of expected squared error and the model assumptions above, derive and implement an algorithm that, at fixed evaluation points $x_{\\text{smooth}} = 0.05$ and $x_{\\text{sharp}} = 0.75$, estimates the mean squared error (MSE), squared bias, and variance of two estimators of $f(x)$:\n- A global-bandwidth Nadaraya–Watson estimator using a Gaussian kernel $K(u) = \\exp\\!\\big(-u^2/2\\big)$ with a single bandwidth $h$ chosen by minimizing leave-one-out cross-validation (LOO-CV) error over a candidate set of bandwidths.\n- A spatially adaptive Nadaraya–Watson estimator that selects, at each evaluation point $x^\\star$, a local bandwidth $h(x^\\star)$ by minimizing a localized LOO-CV error around $x^\\star$. Use a Gaussian weighting window of pilot width $w_{\\text{pilot}}$ to localize the cross-validation objective.\n\nThe Nadaraya–Watson estimator with bandwidth $h$ at an evaluation point $x^\\star$ is defined by\n$$\n\\widehat{f}_h(x^\\star) = \\frac{\\sum_{i=1}^n K\\!\\left(\\frac{x^\\star - x_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\!\\left(\\frac{x^\\star - x_i}{h}\\right)},\n$$\nwhere $(x_i,y_i)$ are the training samples. For global bandwidth selection, the LOO-CV prediction at training input $x_j$ with bandwidth $h$ is\n$$\n\\widehat{y}_{-j,h}(x_j) = \\frac{\\sum_{i \\neq j} K\\!\\left(\\frac{x_j - x_i}{h}\\right) y_i}{\\sum_{i \\neq j} K\\!\\left(\\frac{x_j - x_i}{h}\\right)},\n$$\nand the global LOO-CV objective is the average of squared residuals across all $j$. For spatially adaptive selection at an evaluation point $x^\\star$, define localized weights\n$$\nw_j(x^\\star) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x_j - x^\\star}{w_{\\text{pilot}}}\\right)^2\\right),\n$$\nand minimize the weighted average of squared residuals near $x^\\star$,\n$$\nL(h; x^\\star) = \\frac{\\sum_{j=1}^n w_j(x^\\star)\\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2}{\\sum_{j=1}^n w_j(x^\\star)}.\n$$\n\nYou must estimate the MSE, squared bias, and variance at $x_{\\text{smooth}}$ and $x_{\\text{sharp}}$ via Monte Carlo simulation: for each parameter set, draw $n$ training samples, fit both estimators, and record $\\widehat{f}(x^\\star)$ across $R$ independent replicates. Use the definitions\n- squared bias at $x^\\star$: $\\left(\\mathbb{E}[\\widehat{f}(x^\\star)] - f(x^\\star)\\right)^2$,\n- variance at $x^\\star$: $\\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - \\mathbb{E}[\\widehat{f}(x^\\star)]\\right)^2\\right]$,\n- mean squared error at $x^\\star$: $\\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - f(x^\\star)\\right)^2\\right]$,\nwhere expectations are approximated by averages over the $R$ replicates.\n\nReport, for each parameter set, the improvement in local MSE at the two evaluation points defined as $I(x^\\star) = \\text{MSE}_{\\text{global}}(x^\\star) - \\text{MSE}_{\\text{adaptive}}(x^\\star)$, for $x^\\star \\in \\{x_{\\text{smooth}}, x_{\\text{sharp}}\\}$. Positive values indicate that the adaptive estimator reduces MSE relative to the global estimator.\n\nImplement the simulation for the following test suite of parameter values $(n, \\sigma, \\mathcal{H}, w_{\\text{pilot}}, R, \\text{seed})$, where $\\mathcal{H}$ is the candidate set of bandwidths:\n- Test case $1$: $(150, 0.1, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 12345)$.\n- Test case $2$: $(60, 0.1, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 23456)$.\n- Test case $3$: $(150, 0.4, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 34567)$.\n- Test case $4$: $(150, 0.1, \\{0.01, 0.02, 0.04\\}, 0.10, 120, 45678)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered by test case and, within each test case, by evaluation point $(x_{\\text{smooth}}, x_{\\text{sharp}})$. Each entry must be a float rounded to six decimal places. For example, the output must be of the form\n$$\n[\\Delta_1(x_{\\text{smooth}}), \\Delta_1(x_{\\text{sharp}}), \\Delta_2(x_{\\text{smooth}}), \\Delta_2(x_{\\text{sharp}}), \\Delta_3(x_{\\text{smooth}}), \\Delta_3(x_{\\text{sharp}}), \\Delta_4(x_{\\text{smooth}}), \\Delta_4(x_{\\text{sharp}})],\n$$\nwhere $\\Delta_k(x^\\star)$ denotes the MSE improvement $I(x^\\star)$ for test case $k$. There are no physical units or angles involved in this problem, and all outputs must be floats as specified. The program must be completely self-contained and must not require any user input or external files. Use the Gaussian kernel defined above and ensure all calculations are numerically stable (e.g., avoid division by zero in leave-one-out denominators).", "solution": "The user has requested a computational study comparing the performance of a global-bandwidth Nadaraya-Watson kernel regression estimator with a spatially adaptive counterpart. The problem is valid as it is scientifically grounded in statistical learning theory, well-posed, and provides a complete, consistent set of specifications for a reproducible simulation.\n\nThe core of the problem lies in the bias-variance trade-off, which is managed differently by the two estimators. The target function $f(x) = \\sin(2\\pi x) + 0.5 \\exp(-200(x-0.75)^2)$ is designed to have regions of differing smoothness. A global bandwidth $h$ must compromise: a large $h$ will reduce variance but create large bias in the sharp-peaked region around $x=0.75$ (oversmoothing), while a small $h$ will capture the peak better (lower bias) but suffer from high variance in the smooth sinusoidal region (undersmoothing). A spatially adaptive estimator aims to resolve this by selecting a smaller bandwidth near the sharp peak and a larger bandwidth in the smoother regions, thereby achieving a better local bias-variance balance.\n\nThe simulation will be structured as follows:\n1.  **Data Generation**: For each of the $R$ Monte Carlo replicates, a training set $\\{(x_i, y_i)\\}_{i=1}^n$ is generated. The inputs $x_i$ are drawn from a uniform distribution $U[0, 1]$. The outputs are generated according to the model $y_i = f(x_i) + \\varepsilon_i$, where $\\varepsilon_i$ is sampled from a normal distribution $N(0, \\sigma^2)$.\n\n2.  **Bandwidth Selection**: The critical step within each replicate is the selection of the bandwidth parameter $h$ from a candidate set $\\mathcal{H}$.\n    *   **Leave-One-Out Cross-Validation (LOO-CV) Pre-computation**: For efficiency, for each candidate bandwidth $h \\in \\mathcal{H}$, we first compute the LOO-CV predictions for all training points $x_j$, $j=1, \\dots, n$. The LOO-CV prediction at $x_j$ is given by\n    $$\n    \\widehat{y}_{-j,h}(x_j) = \\frac{\\sum_{i \\neq j} K\\left(\\frac{x_j - x_i}{h}\\right) y_i}{\\sum_{i \\neq j} K\\left(\\frac{x_j - x_i}{h}\\right)}\n    $$\n    where $K(u) = \\exp(-u^2/2)$ is the Gaussian kernel. A direct implementation is computationally expensive. We can reformulate this using the full sums. Let $S_y(x_j, h) = \\sum_{i=1}^n K\\left(\\frac{x_j - x_i}{h}\\right) y_i$ and $S_1(x_j, h) = \\sum_{i=1}^n K\\left(\\frac{x_j - x_i}{h}\\right)$. Since $K(0)=1$, the leave-one-out predictor can be calculated efficiently as:\n    $$\n    \\widehat{y}_{-j,h}(x_j) = \\frac{S_y(x_j, h) - y_j}{S_1(x_j, h) - 1}\n    $$\n    This calculation will be performed for all $j \\in \\{1,\\dots,n\\}$ and all $h \\in \\mathcal{H}$.\n\n    *   **Global Bandwidth Selection**: The global bandwidth, $h_{\\text{global}}$, is chosen to minimize the mean squared LOO-CV error over the entire training set:\n    $$\n    h_{\\text{global}} = \\arg\\min_{h \\in \\mathcal{H}} \\left\\{ \\frac{1}{n} \\sum_{j=1}^n \\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2 \\right\\}\n    $$\n\n    *   **Spatially Adaptive Bandwidth Selection**: For each evaluation point $x^\\star$, a local bandwidth, $h_{\\text{adaptive}}(x^\\star)$, is chosen by minimizing a weighted LOO-CV error. The weights emphasize training points near $x^\\star$. The objective function is:\n    $$\n    L(h; x^\\star) = \\frac{\\sum_{j=1}^n w_j(x^\\star)\\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2}{\\sum_{j=1}^n w_j(x^\\star)}, \\quad \\text{where} \\quad w_j(x^\\star) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{x_j - x^\\star}{w_{\\text{pilot}}}\\right)^2\\right)\n    $$\n    The local bandwidth is then $h_{\\text{adaptive}}(x^\\star) = \\arg\\min_{h \\in \\mathcal{H}} L(h; x^\\star)$. This procedure is carried out separately for $x^\\star = x_{\\text{smooth}} = 0.05$ and $x^\\star = x_{\\text{sharp}} = 0.75$.\n\n3.  **Prediction**: Once the bandwidths are selected for a given replicate, the Nadaraya-Watson estimator is used to compute the function estimates at the two evaluation points:\n    $$\n    \\widehat{f}_h(x^\\star) = \\frac{\\sum_{i=1}^n K\\left(\\frac{x^\\star - x_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\left(\\frac{x^\\star - x_i}{h}\\right)}\n    $$\n    For the global estimator, we use $h = h_{\\text{global}}$ for both $x_{\\text{smooth}}$ and $x_{\\text{sharp}}$. For the adaptive estimator, we use $h = h_{\\text{adaptive}}(x_{\\text{smooth}})$ when predicting at $x_{\\text{smooth}}$, and $h = h_{\\text{adaptive}}(x_{\\text{sharp}})$ when predicting at $x_{\\text{sharp}}$. For numerical stability, a small constant is added to the denominator to prevent division by zero.\n\n4.  **Error Estimation**: The process of data generation, bandwidth selection, and prediction is repeated $R$ times. This yields $R$ estimates for each estimator at each evaluation point. Let $\\{\\widehat{f}^{(r)}(x^\\star)\\}_{r=1}^R$ be the collection of estimates. The Mean Squared Error (MSE) is then approximated by the average over these replicates:\n    $$\n    \\text{MSE}(x^\\star) = \\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - f(x^\\star)\\right)^2\\right] \\approx \\frac{1}{R} \\sum_{r=1}^R \\left(\\widehat{f}^{(r)}(x^\\star) - f(x^\\star)\\right)^2\n    $$\n    This is calculated for both the global and adaptive estimators at both evaluation points.\n\n5.  **Final Output**: The final result for each test case is the improvement in MSE at each point, defined as $I(x^\\star) = \\text{MSE}_{\\text{global}}(x^\\star) - \\text{MSE}_{\\text{adaptive}}(x^\\star)$. A positive value signifies that the adaptive method provides a lower MSE.\n\nThe implementation will use `numpy` for efficient vectorized computations, which is crucial for the feasibility of the simulation. A seeded random number generator ensures the reproducibility of the results for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Monte Carlo simulation to compare global vs. spatially adaptive\n    bandwidth selection for Nadaraya-Watson kernel regression.\n    \"\"\"\n    test_cases = [\n        (150, 0.1, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 12345),\n        (60, 0.1, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 23456),\n        (150, 0.4, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 34567),\n        (150, 0.1, [0.01, 0.02, 0.04], 0.10, 120, 45678),\n    ]\n\n    all_results = []\n    \n    # Define the true function\n    def f_true(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.exp(-200 * (x - 0.75)**2)\n\n    # Define the Gaussian kernel\n    def gaussian_kernel(u):\n        return np.exp(-0.5 * u**2)\n\n    # Evaluation points\n    x_eval = np.array([0.05, 0.75]).reshape(-1, 1) # smooth, sharp\n    f_at_eval = f_true(x_eval)\n\n    # Numerical stability constant\n    STABILITY_EPS = 1e-12\n\n    for case in test_cases:\n        n, sigma, h_candidates, w_pilot, R, seed = case\n        h_candidates = np.array(h_candidates)\n        num_h = len(h_candidates)\n\n        rng = np.random.default_rng(seed)\n\n        preds_global = np.zeros((R, 2))\n        preds_adaptive = np.zeros((R, 2))\n\n        for r in range(R):\n            # 1. Generate data\n            x_train = rng.uniform(0, 1, size=(n, 1))\n            noise = rng.normal(0, sigma, size=(n, 1))\n            y_train = f_true(x_train) + noise\n\n            # 2. Pre-compute LOO predictions for all candidate bandwidths\n            # Pairwise differences for LOO\n            x_diff_loo = x_train - x_train.T\n            all_y_hat_loo = np.zeros((n, num_h))\n\n            for i, h in enumerate(h_candidates):\n                # Kernel matrix for LOO\n                K_mat_loo = gaussian_kernel(x_diff_loo / h)\n                \n                # Full sums S_y and S_1\n                S_y = K_mat_loo @ y_train\n                S_1 = K_mat_loo.sum(axis=1, keepdims=True)\n                \n                # Efficient LOO calculation\n                loo_denom = S_1 - 1.0\n                # Handle numerically unstable denominators\n                loo_denom[np.abs(loo_denom) < STABILITY_EPS] = STABILITY_EPS\n                \n                y_hat_loo_h = (S_y - y_train) / loo_denom\n                all_y_hat_loo[:, i] = y_hat_loo_h.flatten()\n\n            # 3. Global bandwidth selection\n            squared_errors_loo = (y_train - all_y_hat_loo)**2\n            global_cv_scores = squared_errors_loo.mean(axis=0)\n            best_h_idx_global = np.argmin(global_cv_scores)\n            h_global = h_candidates[best_h_idx_global]\n\n            # 4. Global prediction\n            diff_pred_global = x_eval - x_train.T\n            K_pred_global = gaussian_kernel(diff_pred_global / h_global)\n            pred_denom_g = K_pred_global.sum(axis=1, keepdims=True)\n            pred_denom_g[pred_denom_g < STABILITY_EPS] = STABILITY_EPS\n            f_hat_global = (K_pred_global @ y_train) / pred_denom_g\n            preds_global[r, :] = f_hat_global.flatten()\n\n            # 5. Spatially adaptive bandwidth selection and prediction\n            for j, x_star in enumerate(x_eval):\n                # Localization weights\n                weights_loc = np.exp(-0.5 * ((x_train - x_star) / w_pilot)**2)\n                weights_sum = weights_loc.sum()\n                \n                # Localized CV scores\n                weighted_sq_errors = weights_loc * squared_errors_loo\n                local_cv_scores = weighted_sq_errors.sum(axis=0) / weights_sum\n                \n                best_h_idx_adaptive = np.argmin(local_cv_scores)\n                h_adaptive = h_candidates[best_h_idx_adaptive]\n                \n                # Adaptive prediction\n                diff_pred_adaptive = x_star - x_train.T\n                K_pred_adaptive = gaussian_kernel(diff_pred_adaptive / h_adaptive)\n                pred_denom_a = K_pred_adaptive.sum()\n                if pred_denom_a < STABILITY_EPS: pred_denom_a = STABILITY_EPS\n                f_hat_adaptive = (K_pred_adaptive @ y_train) / pred_denom_a\n                preds_adaptive[r, j] = f_hat_adaptive.flatten()[0]\n\n        # 6. Calculate MSEs and improvement\n        mse_global = np.mean((preds_global - f_at_eval.T)**2, axis=0)\n        mse_adaptive = np.mean((preds_adaptive - f_at_eval.T)**2, axis=0)\n\n        improvement = mse_global - mse_adaptive\n        all_results.extend(improvement)\n\n    # Final print statement\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3180581"}, {"introduction": "The bias-variance trade-off extends beyond single-model regularization to fundamental architectural decisions, such as in multi-task learning. This exercise investigates the core dilemma: should we train a separate model for each task or a single, pooled model using all data? Through a combination of theoretical derivation and computation, you will quantify how pooling data reduces variance but introduces a bias term directly related to the divergence between tasks, providing a clear framework for when parameter sharing is most beneficial [@problem_id:3180551].", "problem": "You are given a multi-task linear learning setup with $T$ tasks, indexed by $t \\in \\{1,\\dots,T\\}$. For each task $t$, you observe input-output pairs $(x_{ti}, y_{ti})$ for $i \\in \\{1,\\dots,n_t\\}$, where $x_{ti} \\in \\mathbb{R}^d$ and $y_{ti} \\in \\mathbb{R}$. The data are generated according to the model\n$$\ny_{ti} = x_{ti}^\\top \\theta_t + \\varepsilon_{ti},\n$$\nwith $x_{ti} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, I_d)$ and $\\varepsilon_{ti} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$ independent of $x_{ti}$ across all tasks and samples. The vector $\\theta_t \\in \\mathbb{R}^d$ is the true task parameter for task $t$. Denote the total sample size by $N = \\sum_{t=1}^T n_t$.\n\nConsider a parameter-sharing estimator that enforces a single shared parameter $w \\in \\mathbb{R}^d$ across tasks, obtained by Ordinary Least Squares (OLS), that is, the minimizer of\n$$\n\\sum_{t=1}^T \\sum_{i=1}^{n_t} \\big(y_{ti} - x_{ti}^\\top w\\big)^2,\n$$\nand denote its estimator by $\\hat{w}$. Also consider separate per-task OLS estimators $\\hat{\\theta}_t$ fitted independently on each task $t$ by minimizing\n$$\n\\sum_{i=1}^{n_t} \\big(y_{ti} - x_{ti}^\\top \\theta\\big)^2.\n$$\n\nDefine the task-weighted average parameter\n$$\n\\theta_{\\text{avg}} = \\frac{1}{N} \\sum_{t=1}^T n_t \\theta_t,\n$$\nand the task divergence for task $t$ as\n$$\n\\Delta_t = \\big\\|\\theta_t - \\theta_{\\text{avg}}\\big\\|^2.\n$$\n\nYour tasks are:\n\n- Starting from the generative model and the definition of expected squared prediction error for a fresh test input $x \\sim \\mathcal{N}(0, I_d)$ from task $t$, decompose the expected error of the shared estimator’s prediction $x^\\top \\hat{w}$ into its squared bias, variance, and irreducible noise. Within this decomposition, express the squared bias term for task $t$ as a function of the divergence $\\Delta_t$ and the parameters of the setup. Do not use any shortcut formulas beyond first principles and well-tested facts about multivariate normal distributions.\n\n- Using well-tested facts about the Wishart distribution of $X^\\top X$ when $X$ has independent rows from a multivariate normal distribution, derive the expected variance term of the pooled shared estimator under the Gaussian design assumption, expressed in terms of $d$, $\\sigma^2$, and $N$, and similarly derive the expected variance term for the separate per-task estimators in terms of $d$, $\\sigma^2$, and $n_t$. Work in the regime where these expectations are well-defined, that is, assume $N > d + 1$ and $n_t > d + 1$ for all tasks.\n\n- Implement these derivations in a runnable program that, for a specified test suite, computes the squared bias for each task under the shared estimator and the expected variance terms for both the shared and the separate estimators.\n\nTest suite specification:\n\nLet $T = 2$ for all cases. For each test case, compute and output the following ordered list of five floats:\n$$\n\\big[\\text{bias}_1, \\text{bias}_2, \\text{var}_{\\text{pooled}}, \\text{var}_{\\text{sep},1}, \\text{var}_{\\text{sep},2}\\big],\n$$\nwhere $\\text{bias}_t$ is the squared bias term for task $t$ under the shared estimator, $\\text{var}_{\\text{pooled}}$ is the expected variance of the pooled shared estimator’s prediction for a fresh $x \\sim \\mathcal{N}(0, I_d)$, and $\\text{var}_{\\text{sep},t}$ is the expected variance for the separate per-task estimator’s prediction for task $t$.\n\nUse the following test cases:\n\n- Case $1$ (identical tasks, variance comparison):\n  - $d = 3$, $\\sigma^2 = 0.5$, $n_1 = 30$, $n_2 = 30$,\n  - $\\theta_1 = [1, 0, -1]$, $\\theta_2 = [1, 0, -1]$.\n\n- Case $2$ (unequal sample sizes, moderate divergence):\n  - $d = 3$, $\\sigma^2 = 0.5$, $n_1 = 20$, $n_2 = 40$,\n  - $\\theta_1 = [1, 0, 0]$, $\\theta_2 = [0, 1, 0]$.\n\n- Case $3$ (near-boundary sample sizes, stronger divergence):\n  - $d = 2$, $\\sigma^2 = 1.0$, $n_1 = 5$, $n_2 = 6$,\n  - $\\theta_1 = [2, -1]$, $\\theta_2 = [-2, 1]$.\n\n- Case $4$ (high divergence with larger samples):\n  - $d = 4$, $\\sigma^2 = 0.2$, $n_1 = 50$, $n_2 = 50$,\n  - $\\theta_1 = [10, 0, 0, 0]$, $\\theta_2 = [-10, 0, 0, 0]$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the concatenation of the results of all cases as a comma-separated list enclosed in square brackets. The list should contain $4 \\times 5 = 20$ floats in the order of the cases and within each case in the order specified above. Each float must be rounded to $6$ decimal places. For example, the output must look like\n$$\n[\\text{case1\\_bias1},\\text{case1\\_bias2},\\text{case1\\_var\\_pooled},\\text{case1\\_var\\_sep1},\\text{case1\\_var\\_sep2},\\dots,\\text{case4\\_var\\_sep2}]\n$$\nwith all numeric entries rounded to $6$ decimal places.", "solution": "The problem statement is analyzed and found to be valid. It is scientifically grounded in statistical learning theory, well-posed, objective, and internally consistent. We may proceed with the solution.\n\nThe task is to perform an error decomposition for two types of estimators in a multi-task linear regression setting. We will first derive the general expressions for the bias and variance terms and then apply them to the specific test cases.\n\n### Part 1: Error Decomposition and Bias of the Shared Estimator\n\nThe expected squared prediction error (ESPE) for a new data point $(x, y)$ from task $t$, where $y = x^\\top\\theta_t + \\varepsilon$ with $x \\sim \\mathcal{N}(0, I_d)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, using an estimator $\\hat{w}$, is given by:\n$$\n\\text{ESPE}_t = \\mathbb{E}_{\\mathcal{D}, x, \\varepsilon} \\left[ (y - x^\\top\\hat{w})^2 \\right]\n$$\nHere, the expectation is over the training data $\\mathcal{D}$ (which determines $\\hat{w}$), the new predictor $x$, and the new noise term $\\varepsilon$. Substituting the model for $y$:\n$$\n\\text{ESPE}_t = \\mathbb{E}_{\\mathcal{D}, x, \\varepsilon} \\left[ (x^\\top\\theta_t + \\varepsilon - x^\\top\\hat{w})^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x, \\varepsilon} \\left[ (x^\\top(\\theta_t - \\hat{w}) + \\varepsilon)^2 \\right]\n$$\nSince $\\varepsilon$ has mean zero and is independent of $\\mathcal{D}$ and $x$, the cross-term vanishes upon expectation:\n$$\n\\text{ESPE}_t = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (x^\\top(\\theta_t - \\hat{w}))^2 \\right] + \\mathbb{E}[\\varepsilon^2]\n$$\nThe term $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ is the irreducible error. The first term is the model error, which we decompose into bias and variance. Let $\\bar{w} = \\mathbb{E}_{\\mathcal{D}}[\\hat{w}]$.\n$$\n\\mathbb{E}_{\\mathcal{D}, x} \\left[ (x^\\top(\\theta_t - \\hat{w}))^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (x^\\top(\\theta_t - \\bar{w} + \\bar{w} - \\hat{w}))^2 \\right]\n$$\nExpanding the square and noting that the cross-term $\\mathbb{E}_{\\mathcal{D}}[(\\bar{w} - \\hat{w})]$ is zero, we get:\n$$\n\\mathbb{E}_{\\mathcal{D}, x} \\left[ (x^\\top(\\theta_t - \\bar{w}))^2 \\right] + \\mathbb{E}_{\\mathcal{D}, x} \\left[ (x^\\top(\\bar{w} - \\hat{w}))^2 \\right]\n$$\nThe first term is the squared bias of the prediction for task $t$, and the second is the variance of the prediction.\n\nTo calculate the squared bias term, we must first find $\\bar{w} = \\mathbb{E}[\\hat{w}]$. The shared estimator $\\hat{w}$ is the OLS solution across all $N = \\sum_{t=1}^T n_t$ data points:\n$$\n\\hat{w} = \\left(\\sum_{t=1}^T X_t^\\top X_t\\right)^{-1} \\left(\\sum_{t=1}^T X_t^\\top Y_t\\right)\n$$\nwhere $X_t$ is the $n_t \\times d$ design matrix and $Y_t$ is the $n_t \\times 1$ response vector for task $t$. Taking the expectation conditional on the design matrices $X_t$:\n$$\n\\mathbb{E}[ \\hat{w} | \\{X_t\\}_{t=1}^T ] = \\left(\\sum_t X_t^\\top X_t\\right)^{-1} \\left(\\sum_t X_t^\\top \\mathbb{E}[Y_t|X_t]\\right) = \\left(\\sum_t X_t^\\top X_t\\right)^{-1} \\left(\\sum_t (X_t^\\top X_t) \\theta_t\\right)\n$$\nTaking the full expectation requires evaluating $\\mathbb{E}_{\\{X_t\\}}[\\cdot]$ of this expression. Under the Gaussian i.i.d. design assumption, the expectation of the estimator simplifies significantly. Due to the rotational symmetry of the Gaussian distribution, a standard result in this setting (often used as a first-order approximation or derived via more advanced techniques) is that the expectation of the estimator is the weighted average of the true parameters:\n$$\n\\bar{w} = \\mathbb{E}[\\hat{w}] = \\frac{1}{N} \\sum_{t=1}^T n_t \\theta_t = \\theta_{\\text{avg}}\n$$\nThe squared bias of the prediction for task $t$ is therefore:\n$$\n(\\text{Bias}_t)^2 = \\mathbb{E}_{x} \\left[ (x^\\top(\\theta_t - \\bar{w}))^2 \\right] = \\mathbb{E}_{x} \\left[ (x^\\top(\\theta_t - \\theta_{\\text{avg}}))^2 \\right]\n$$\nSince $x \\sim \\mathcal{N}(0, I_d)$, we have $\\mathbb{E}[xx^\\top] = I_d$. The expression becomes:\n$$\n(\\text{Bias}_t)^2 = (\\theta_t - \\theta_{\\text{avg}})^\\top \\mathbb{E}[xx^\\top] (\\theta_t - \\theta_{\\text{avg}}) = (\\theta_t - \\theta_{\\text{avg}})^\\top I_d (\\theta_t - \\theta_{\\text{avg}}) = \\|\\theta_t - \\theta_{\\text{avg}}\\|^2\n$$\nThis is precisely the task divergence $\\Delta_t$ as defined in the problem. Thus, the squared bias term for task $t$ is $\\text{bias}_t = \\Delta_t$.\n\n### Part 2: Variance Derivation\n\nThe variance of the prediction is the second term from the decomposition, $\\mathbb{E}_{\\mathcal{D}, x} \\left[ (x^\\top(\\hat{w} - \\bar{w}))^2 \\right]$. As with the bias term, we can simplify the expectation over $x$:\n$$\n\\text{Var}_{\\text{pooled}} = \\mathbb{E}_{\\mathcal{D}} \\left[ (\\hat{w} - \\bar{w})^\\top \\mathbb{E}_x[xx^\\top] (\\hat{w} - \\bar{w}) \\right] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\|\\hat{w} - \\bar{w}\\|^2 \\right]\n$$\nThis is the trace of the covariance matrix of $\\hat{w}$: $\\text{tr}(\\text{Cov}(\\hat{w}))$.\nThe covariance of $\\hat{w}$ can be found using the law of total covariance: $\\text{Cov}(\\hat{w}) = \\mathbb{E}[\\text{Cov}(\\hat{w}|X)] + \\text{Cov}(\\mathbb{E}[\\hat{w}|X])$.\nThe first term, $\\mathbb{E}[\\text{Cov}(\\hat{w}|X)]$, arises from the noise in the responses $Y$.\n$$\n\\text{Cov}(\\hat{w}|X) = \\text{Cov}((X^\\top X)^{-1}X^\\top Y | X) = (X^\\top X)^{-1}X^\\top \\text{Cov}(Y) X(X^\\top X)^{-1}\n$$\nSince $\\text{Cov}(Y) = \\sigma^2 I_N$, this simplifies to:\n$$\n\\text{Cov}(\\hat{w}|X) = \\sigma^2 (X^\\top X)^{-1}X^\\top I_N X(X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1}\n$$\nThe overall design matrix $X$ is an $N \\times d$ matrix where each row is an independent draw from $\\mathcal{N}(0, I_d)$. Thus, the matrix $S = X^\\top X$ follows a Wishart distribution, $S \\sim W_d(N, I_d)$. The problem directs us to use properties of the Wishart distribution. The expectation of the inverse Wishart matrix $S^{-1}$ is known to be:\n$$\n\\mathbb{E}[S^{-1}] = \\mathbb{E}[(X^\\top X)^{-1}] = \\frac{I_d}{N-d-1}, \\quad \\text{for } N > d+1\n$$\nTaking the expectation of the conditional covariance over $X$, we get:\n$$\n\\mathbb{E}[\\text{Cov}(\\hat{w}|X)] = \\sigma^2 \\mathbb{E}[(X^\\top X)^{-1}] = \\frac{\\sigma^2}{N-d-1} I_d\n$$\nThe second term, $\\text{Cov}(\\mathbb{E}[\\hat{w}|X])$, represents variance due to the random design interacting with the task heterogeneity. In many standard analyses, this term is smaller or ignored for simplicity. Following this standard approach, we approximate the covariance of $\\hat{w}$ by $\\mathbb{E}[\\text{Cov}(\\hat{w}|X)]$.\nThus, the prediction variance of the shared estimator is:\n$$\n\\text{var}_{\\text{pooled}} = \\text{tr}(\\text{Cov}(\\hat{w})) \\approx \\text{tr}\\left(\\frac{\\sigma^2}{N-d-1} I_d\\right) = \\frac{\\sigma^2 d}{N-d-1}\n$$\nA similar derivation applies to the separate per-task estimators, $\\hat{\\theta}_t$. For each task $t$, the estimator is $\\hat{\\theta}_t = (X_t^\\top X_t)^{-1}X_t^\\top Y_t$. It is unbiased for $\\theta_t$, so its prediction bias on task $t$ is zero. The variance of the prediction is derived analogously. The design matrix $X_t$ has $n_t$ samples, so $X_t^\\top X_t \\sim W_d(n_t, I_d)$. The expected variance is:\n$$\n\\text{var}_{\\text{sep}, t} = \\text{tr}(\\text{Cov}(\\hat{\\theta}_t)) \\approx \\text{tr}\\left(\\sigma^2 \\mathbb{E}[(X_t^\\top X_t)^{-1}]\\right) = \\text{tr}\\left(\\frac{\\sigma^2}{n_t-d-1} I_d\\right) = \\frac{\\sigma^2 d}{n_t-d-1}\n$$\nThis formula is valid for $n_t > d+1$.\n\n### Summary of Formulas\n1.  Squared Bias for task $t$ (shared estimator): $\\text{bias}_t = \\left\\|\\theta_t - \\theta_{\\text{avg}}\\right\\|^2$, where $\\theta_{\\text{avg}} = \\frac{1}{N} \\sum_{k=1}^T n_k \\theta_k$.\n2.  Expected Prediction Variance (shared estimator): $\\text{var}_{\\text{pooled}} = \\frac{\\sigma^2 d}{N - d - 1}$.\n3.  Expected Prediction Variance (separate estimator for task $t$): $\\text{var}_{\\text{sep}, t} = \\frac{\\sigma^2 d}{n_t - d - 1}$.\n\nThese formulas are now implemented for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multi-task learning error decomposition problem for a a specified test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 3, \"sigma_sq\": 0.5, \"n\": [30, 30],\n            \"thetas\": [np.array([1, 0, -1]), np.array([1, 0, -1])]\n        },\n        {\n            \"d\": 3, \"sigma_sq\": 0.5, \"n\": [20, 40],\n            \"thetas\": [np.array([1, 0, 0]), np.array([0, 1, 0])]\n        },\n        {\n            \"d\": 2, \"sigma_sq\": 1.0, \"n\": [5, 6],\n            \"thetas\": [np.array([2, -1]), np.array([-2, 1])]\n        },\n        {\n            \"d\": 4, \"sigma_sq\": 0.2, \"n\": [50, 50],\n            \"thetas\": [np.array([10, 0, 0, 0]), np.array([-10, 0, 0, 0])]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        d = case[\"d\"]\n        sigma_sq = case[\"sigma_sq\"]\n        n_t = case[\"n\"]\n        thetas = case[\"thetas\"]\n        \n        T = len(n_t)\n        N = sum(n_t)\n        \n        # Calculate theta_avg\n        theta_avg = np.zeros(d)\n        for t in range(T):\n            theta_avg += n_t[t] * thetas[t]\n        theta_avg /= N\n        \n        # Calculate squared bias for each task\n        biases = []\n        for t in range(T):\n            bias_sq = np.linalg.norm(thetas[t] - theta_avg)**2\n            biases.append(bias_sq)\n            \n        # Calculate expected variance for pooled estimator\n        # Condition N > d + 1 is assumed to hold as per problem statement\n        var_pooled = (sigma_sq * d) / (N - d - 1)\n        \n        # Calculate expected variance for separate estimators\n        vars_sep = []\n        for t in range(T):\n            # Condition n_t > d + 1 is assumed to hold as per problem statement\n            var_sep_t = (sigma_sq * d) / (n_t[t] - d - 1)\n            vars_sep.append(var_sep_t)\n            \n        case_results = biases + [var_pooled] + vars_sep\n        all_results.extend(case_results)\n\n    # Format the final output string\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3180551"}]}