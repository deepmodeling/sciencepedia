{"hands_on_practices": [{"introduction": "A core task in building a useful classification model is selecting an appropriate decision threshold. This threshold, which converts a model's raw probability scores into final class predictions, is a hyperparameter that must be tuned on a validation set. This exercise provides hands-on practice in optimizing a threshold to maximize the $F_\\beta$ score and, more importantly, in quantifying how this optimization can \"overfit\" to the validation set by measuring the performance drop on a separate test set [@problem_id:3188635]. Understanding this \"overfitting gap\" is a critical skill for assessing the true generalizability of a model.", "problem": "Consider a binary classification model that outputs calibrated predicted probabilities on two disjoint datasets: a validation set $X_{\\text{val}}$ and a test set $X_{\\text{test}}$. Let the validation set consist of predicted probabilities $p^{(\\text{val})}_i \\in [0,1]$ with binary labels $y^{(\\text{val})}_i \\in \\{0,1\\}$ for $i = 1,\\dots,n_{\\text{val}}$, and similarly for the test set with $p^{(\\text{test})}_j \\in [0,1]$ and $y^{(\\text{test})}_j \\in \\{0,1\\}$ for $j = 1,\\dots,n_{\\text{test}}$. A threshold $t \\in [0,1]$ induces predicted labels $\\hat{y}_i(t) = 1$ if and only if $p_i \\ge t$ and $\\hat{y}_i(t) = 0$ otherwise.\n\nStarting from core definitions in statistical learning:\n- Precision $P$ is defined as $P = \\frac{TP}{TP + FP}$ with the convention $P = 0$ when $TP + FP = 0$, where $TP$ denotes True Positives, i.e., the count of instances with $(\\hat{y}=1, y=1)$, and $FP$ denotes False Positives, i.e., the count of instances with $(\\hat{y}=1, y=0)$.\n- Recall $R$ is defined as $R = \\frac{TP}{TP + FN}$ with the convention $R = 0$ when $TP + FN = 0$, where $FN$ denotes False Negatives, i.e., the count of instances with $(\\hat{y}=0, y=1)$.\n- The $F_\\beta$ score for $\\beta > 0$ is defined as the weighted harmonic mean of $P$ and $R$, namely $F_\\beta = \\frac{(1+\\beta^2) \\cdot P \\cdot R}{\\beta^2 \\cdot P + R}$, with the convention $F_\\beta = 0$ when the denominator is $0$.\n\nThe goal is to quantify threshold overfitting due to tuning on $X_{\\text{val}}$ under class distribution shift and varying $\\beta$. For a given $\\beta$, select the threshold $t^\\ast$ that maximizes $F_\\beta$ on $X_{\\text{val}}$ by exhaustive search over the finite candidate set $\\{0\\} \\cup S \\cup \\{1\\}$, where $S$ is the set of unique values of $p^{(\\text{val})}_i$. If multiple thresholds achieve the same maximum $F_\\beta$ on $X_{\\text{val}}$, break ties by selecting the largest threshold among the maximizers. Then, compute $F_\\beta$ on $X_{\\text{val}}$ and on $X_{\\text{test}}$ using $t^\\ast$, and report the overfitting gap defined as\n$$\n\\Delta = F_\\beta\\big(X_{\\text{val}}; t^\\ast\\big) - F_\\beta\\big(X_{\\text{test}}; t^\\ast\\big).\n$$\n\nYour program must implement the above procedure and produce the overfitting gaps for the following test suite of parameter values. All arrays are ordered lists; all numbers are real-valued scalars.\n\nTest case A (happy path, moderate $\\beta$, mild shift):\n- Validation predicted probabilities: $[0.92, 0.81, 0.76, 0.63, 0.58, 0.49, 0.45, 0.41, 0.35, 0.22, 0.17, 0.08]$.\n- Validation labels: $[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]$.\n- Test predicted probabilities: $[0.90, 0.84, 0.79, 0.70, 0.34, 0.30, 0.27, 0.20, 0.15, 0.12]$.\n- Test labels: $[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]$.\n- $\\beta = 1$.\n\nTest case B (recall-emphasis, stronger shift):\n- Validation predicted probabilities: $[0.88, 0.83, 0.77, 0.74, 0.62, 0.52, 0.47, 0.40, 0.33, 0.25]$.\n- Validation labels: $[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]$.\n- Test predicted probabilities: $[0.86, 0.80, 0.55, 0.51, 0.49, 0.45, 0.38, 0.31, 0.28, 0.18]$.\n- Test labels: $[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]$.\n- $\\beta = 2$.\n\nTest case C (flat scores, tie handling):\n- Validation predicted probabilities: $[0.50, 0.50, 0.50, 0.50]$.\n- Validation labels: $[1, 0, 0, 1]$.\n- Test predicted probabilities: $[0.50, 0.50, 0.50]$.\n- Test labels: $[0, 0, 1]$.\n- $\\beta = 1$.\n\nTest case D (precision-emphasis, extreme shift causing no positives predicted on $X_{\\text{test}}$ at $t^\\ast$):\n- Validation predicted probabilities: $[0.95, 0.90, 0.85, 0.40, 0.35, 0.30]$.\n- Validation labels: $[1, 1, 1, 0, 0, 0]$.\n- Test predicted probabilities: $[0.60, 0.55, 0.50, 0.45, 0.20, 0.10]$.\n- Test labels: $[1, 0, 0, 0, 0, 0]$.\n- $\\beta = 0.5$.\n\nImplementation requirements:\n- Use the exhaustive search over the specified candidate thresholds on $X_{\\text{val}}$ to select $t^\\ast$.\n- When computing $F_\\beta$, use the equivalent count-based expression\n$$\nF_\\beta = \\frac{(1+\\beta^2)\\,TP}{(1+\\beta^2)\\,TP + \\beta^2\\,FN + FP},\n$$\nwith the convention that $F_\\beta = 0$ if the denominator equals $0$.\n- For each test case, compute $\\Delta$ as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $\\Delta$ expressed as a decimal rounded to $6$ places, in the order A, B, C, D. For example, the format must be $\\texttt{[0.123456,0.234567,0.345678,0.456789]}$.", "solution": "The problem requires us to quantify threshold overfitting for a binary classification model. This is achieved by calculating the performance gap, $\\Delta$, between a validation set $X_{\\text{val}}$ and a test set $X_{\\text{test}}$. The performance metric is the $F_\\beta$ score. The core of the problem lies in first selecting an optimal classification threshold $t^\\ast$ on the validation set and then measuring how well the performance at this threshold generalizes to the unseen test set. A large positive gap $\\Delta = F_\\beta(X_{\\text{val}}; t^\\ast) - F_\\beta(X_{\\text{test}}; t^\\ast)$ indicates that the threshold $t^\\ast$ is overfitted to the specific characteristics of the validation data.\n\nThe solution is implemented by following a deterministic, step-by-step procedure for each test case provided.\n\n**Step 1: Identification of Candidate Thresholds**\n\nThe classification outcome for a sample with predicted probability $p$ changes only when the threshold $t$ crosses the value of $p$. Therefore, to find the threshold that maximizes the $F_\\beta$ score, it is sufficient to evaluate a finite set of candidate thresholds. The problem specifies this set to be $\\{0\\} \\cup S \\cup \\{1\\}$, where $S$ is the set of unique predicted probabilities in the validation set, $p^{(\\text{val})}_i$. We construct this set of candidates, denoted $T_{\\text{candidates}}$, and sort it in ascending order. Sorting is crucial for correctly implementing the specified tie-breaking rule.\n\n**Step 2: Optimal Threshold Selection on the Validation Set**\n\nWe perform an exhaustive search for the optimal threshold $t^\\ast$ over the sorted candidate set $T_{\\text{candidates}}$. For each candidate threshold $t \\in T_{\\text{candidates}}$, we compute the $F_\\beta$ score on the validation data $X_{\\text{val}}$.\n\nThe predicted label for an instance $i$ is given by $\\hat{y}_i(t) = 1$ if its probability score $p_i \\ge t$, and $\\hat{y}_i(t) = 0$ otherwise. Based on these predictions and the true labels $y_i$, we count the number of True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$).\n\nThe $F_\\beta$ score is then calculated using the provided count-based formula, which is numerically stable and avoids division by zero in intermediate precision and recall calculations:\n$$\nF_\\beta = \\frac{(1+\\beta^2) \\cdot TP}{(1+\\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}\n$$\nIf the denominator is zero (which occurs only if $TP=FP=FN=0$), the score is defined to be $F_\\beta = 0$.\n\nWe iterate through the sorted candidates $t \\in T_{\\text{candidates}}$ and track the maximum $F_\\beta$ score found so far, let's call it $F_{\\beta, \\text{max}}$, and the threshold that achieved it, $t^\\ast$. The update rule is as follows: if the current threshold $t$ yields an $F_\\beta$ score greater than or equal to $F_{\\beta, \\text{max}}$, we update $F_{\\beta, \\text{max}}$ to this new score and set $t^\\ast = t$. Because we are iterating through thresholds in increasing order, this rule ensures that if multiple thresholds yield the same maximum score, the largest of these thresholds is chosen as $t^\\ast$, satisfying the problem's tie-breaking requirement.\n\n**Step 3: Calculation of the Overfitting Gap**\n\nOnce the optimal threshold $t^\\ast$ is determined from the validation set, we can calculate the overfitting gap $\\Delta$.\n\nFirst, the $F_\\beta$ score on the validation set is simply the maximum score found in the previous step: $F_\\beta(X_{\\text{val}}; t^\\ast) = F_{\\beta, \\text{max}}$.\nSecond, we calculate the $F_\\beta$ score on the test set, $F_\\beta(X_{\\text{test}}; t^\\ast)$, by applying the same threshold $t^\\ast$ to the test probabilities $p^{(\\text{test})}_j$ to generate predictions and then using the same $F_\\beta$ formula with the test set's $TP$, $FP$, and $FN$ counts.\n\nThe overfitting gap is the difference between these two scores:\n$$\n\\Delta = F_\\beta(X_{\\text{val}}; t^\\ast) - F_\\beta(X_{\\text{test}}; t^\\ast)\n$$\n\nThis procedure is encapsulated in a program that processes each of the four specified test cases, calculates the corresponding $\\Delta$, and formats the results as a single comma-separated list. The implementation uses the `numpy` library for efficient vectorized computation of counts ($TP, FP, FN$) and array manipulations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_overfitting_gap(p_val, y_val, p_test, y_test, beta):\n    \"\"\"\n    Computes the F-beta overfitting gap for a given set of parameters.\n\n    1. Finds the optimal threshold t* on the validation set.\n    2. Calculates F_beta(X_val; t*) and F_beta(X_test; t*).\n    3. Returns the difference Delta.\n    \"\"\"\n    # Ensure inputs are numpy arrays for vectorized operations\n    p_val, y_val = np.array(p_val), np.array(y_val)\n    p_test, y_test = np.array(p_test), np.array(y_test)\n\n    # Step 1: Identify candidate thresholds\n    # The set is {0} U S U {1}, where S is the set of unique validation probabilities.\n    candidate_thresholds = sorted(list(set(p_val) | {0., 1.}))\n\n    # Step 2: Find the optimal threshold t* on the validation set\n    max_f_beta = -1.0\n    t_star = -1.0\n\n    beta_sq = beta**2\n    one_plus_beta_sq = 1 + beta_sq\n    \n    total_pos_val = np.sum(y_val)\n\n    for t in candidate_thresholds:\n        y_hat_val = (p_val >= t).astype(int)\n\n        tp = np.sum((y_hat_val == 1)  (y_val == 1))\n        fp = np.sum((y_hat_val == 1)  (y_val == 0))\n        # fn = np.sum((y_hat_val == 0)  (y_val == 1))\n        fn = total_pos_val - tp\n\n        numerator = one_plus_beta_sq * tp\n        denominator = (one_plus_beta_sq * tp) + (beta_sq * fn) + fp\n\n        f_beta = numerator / denominator if denominator > 0 else 0.0\n\n        # Tie-breaking rule: select the largest threshold.\n        # Since we iterate through sorted thresholds, this condition correctly captures it.\n        if f_beta >= max_f_beta:\n            max_f_beta = f_beta\n            t_star = t\n            \n    f_beta_val = max_f_beta\n\n    # Step 3: Calculate F_beta on the test set using t* and find the gap\n    total_pos_test = np.sum(y_test)\n    y_hat_test = (p_test >= t_star).astype(int)\n\n    tp_test = np.sum((y_hat_test == 1)  (y_test == 1))\n    fp_test = np.sum((y_hat_test == 1)  (y_test == 0))\n    # fn_test = np.sum((y_hat_test == 0)  (y_test == 1))\n    fn_test = total_pos_test - tp_test\n\n    numerator_test = one_plus_beta_sq * tp_test\n    denominator_test = (one_plus_beta_sq * tp_test) + (beta_sq * fn_test) + fp_test\n\n    f_beta_test = numerator_test / denominator_test if denominator_test > 0 else 0.0\n\n    gap = f_beta_val - f_beta_test\n    return gap\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {\n            \"p_val\": [0.92, 0.81, 0.76, 0.63, 0.58, 0.49, 0.45, 0.41, 0.35, 0.22, 0.17, 0.08],\n            \"y_val\": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n            \"p_test\": [0.90, 0.84, 0.79, 0.70, 0.34, 0.30, 0.27, 0.20, 0.15, 0.12],\n            \"y_test\": [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n            \"beta\": 1.0\n        },\n        # Test case B\n        {\n            \"p_val\": [0.88, 0.83, 0.77, 0.74, 0.62, 0.52, 0.47, 0.40, 0.33, 0.25],\n            \"y_val\": [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n            \"p_test\": [0.86, 0.80, 0.55, 0.51, 0.49, 0.45, 0.38, 0.31, 0.28, 0.18],\n            \"y_test\": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n            \"beta\": 2.0\n        },\n        # Test case C\n        {\n            \"p_val\": [0.50, 0.50, 0.50, 0.50],\n            \"y_val\": [1, 0, 0, 1],\n            \"p_test\": [0.50, 0.50, 0.50],\n            \"y_test\": [0, 0, 1],\n            \"beta\": 1.0\n        },\n        # Test case D\n        {\n            \"p_val\": [0.95, 0.90, 0.85, 0.40, 0.35, 0.30],\n            \"y_val\": [1, 1, 1, 0, 0, 0],\n            \"p_test\": [0.60, 0.55, 0.50, 0.45, 0.20, 0.10],\n            \"y_test\": [1, 0, 0, 0, 0, 0],\n            \"beta\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        gap = compute_overfitting_gap(\n            case[\"p_val\"], case[\"y_val\"],\n            case[\"p_test\"], case[\"y_test\"],\n            case[\"beta\"]\n        )\n        results.append(gap)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3188635"}, {"introduction": "The reliability of model selection hinges on the quality of the validation set. In the real world, data is rarely perfect, and systematic errors like mislabeled samples can mislead the validation process. This practice explores this critical issue by simulating structured label corruption within a validation set and examining its impact on hyperparameter selection for a $k$-Nearest Neighbors classifier [@problem_id:3188658]. You will measure how a model chosen using this flawed data performs on a clean test set, providing a clear illustration of why robustness checks and clean test data are essential.", "problem": "Consider binary classification in statistical learning under the Empirical Risk Minimization (ERM) principle. Let $D_{\\text{train}}$, $D_{\\text{val}}$, and $D_{\\text{test}}$ be independent and identically distributed samples drawn from the same data-generating process. For a hypothesis $h$ and a dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$, define the empirical misclassification risk as\n$$\nR(h; D) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{h(x_i) \\neq y_i\\}.\n$$\nHyperparameter selection follows ERM by choosing the hyperparameter that minimizes $R(h; D_{\\text{val}})$, where $D_{\\text{val}}$ is used solely for model selection. After selection, performance is evaluated on $D_{\\text{test}}$.\n\nYou will implement the $k$-Nearest Neighbors classifier with odd $k$ values to avoid tie ambiguity. For a query point $x$, $h_k(x)$ predicts the majority label among its $k$ nearest neighbors in $D_{\\text{train}}$ under squared Euclidean distance. Ties when $k$ is odd cannot occur; if an even $k$ were used, a deterministic tie-break rule (predict $0$) would be required, but this problem restricts to odd $k$.\n\nData generation is as follows. Features are in $\\mathbb{R}^2$. Draw $x \\in \\mathbb{R}^2$ from a standard normal distribution with identity covariance. Let the latent score be $s(x) = w^\\top x + b$, where $w \\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$. Generate labels by a noisy linear separator: draw $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ independently for each sample and set $y = \\mathbf{1}\\{s(x) + \\epsilon > 0\\}$. Use $w = [1.0, -0.8]$, $b = 0.1$, and $\\sigma = 0.3$. Use $n_{\\text{train}} = 200$, $n_{\\text{val}} = 150$, $n_{\\text{test}} = 400$, and $d = 2$.\n\nStructured corruption is applied to $D_{\\text{val}}$ only. Define a corruption region by parameters $(\\tau, \\omega)$ and flip labels deterministically for validation samples whose features satisfy $(x_1 > \\tau)$ and $(|x_2| \\le \\omega)$. That is, for such samples, replace $y$ by $1 - y$. All other labels remain unchanged. This corruption is structured, not random. No corruption is applied to $D_{\\text{train}}$ or $D_{\\text{test}}$.\n\nHyperparameter selection: given a grid $\\mathcal{K} = \\{1, 3, 5, 11, 21, 41\\}$, compute $R(h_k; D_{\\text{val}}^{\\text{corrupt}})$ for each $k \\in \\mathcal{K}$, where $D_{\\text{val}}^{\\text{corrupt}}$ denotes the validation set with structured corruption. Select\n$$\nk^\\star = \\arg\\min_{k \\in \\mathcal{K}} R(h_k; D_{\\text{val}}^{\\text{corrupt}}),\n$$\nbreaking ties by choosing the smallest $k$. Also compute the oracle selection\n$$\nk^\\dagger = \\arg\\min_{k \\in \\mathcal{K}} R(h_k; D_{\\text{val}}^{\\text{clean}}),\n$$\nwhere $D_{\\text{val}}^{\\text{clean}}$ has uncorrupted labels. Evaluate both selected models on $D_{\\text{test}}$ to obtain their test accuracies. Define a detection threshold $\\delta = 0.02$. Declare that $D_{\\text{test}}$ reveals misselection if either $k^\\star \\neq k^\\dagger$ or the absolute difference in test accuracies between the model selected with corrupted validation labels and the oracle-selected model exceeds $\\delta$.\n\nThere are no physical units in this problem. All rates and thresholds must be expressed as decimals or fractions. Angles, if any, are not used. You must implement the complete procedure described above and produce results for the following test suite of structured corruptions, each specified by $(\\tau, \\omega)$:\n- Test case $1$: $(\\tau, \\omega) = (100.0, 1.0)$,\n- Test case $2$: $(\\tau, \\omega) = (0.2, 0.5)$,\n- Test case $3$: $(\\tau, \\omega) = (-0.5, 10.0)$,\n- Test case $4$: $(\\tau, \\omega) = (0.0, 0.15)$.\n\nFor each test case, your program must output a list containing five items in the following order:\n[$k^\\star$, $k^\\dagger$, test_acc($k^\\star$), test_acc($k^\\dagger$), misselection_revealed],\nwhere $k^\\star$ and $k^\\dagger$ are integers, the accuracies are floats, and the misselection indicator is a boolean.\n\nFinal output format: Your program should produce a single line of output containing the results across all test cases as a comma-separated list enclosed in square brackets, where each element is the per-test-case list defined above. For example, the output should look like $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$ with no additional text.", "solution": "The design begins from the Empirical Risk Minimization (ERM) principle. For binary classification, the empirical misclassification risk on a dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ is\n$$\nR(h; D) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{h(x_i) \\neq y_i\\}.\n$$\nHyperparameter selection seeks a hypothesis $h_{\\lambda}$, parameterized by a hyperparameter $\\lambda$, that minimizes $R(h_{\\lambda}; D_{\\text{val}})$. This relies on the foundational assumption that $D_{\\text{val}}$ is a representative sample from the data-generating distribution. When the labels in $D_{\\text{val}}$ are corrupted, $R(h_{\\lambda}; D_{\\text{val}})$ can be biased and the selected $\\lambda$ may not minimize the true generalization risk. The test set $D_{\\text{test}}$ is held out and used to evaluate performance; its clean labels allow detection of misselection when performance deviates notably from the oracle selection based on clean validation labels.\n\nWe instantiate the classifier as $k$-Nearest Neighbors (KNN). For each query point $x$, KNN uses the set of $k$ nearest training examples under squared Euclidean distance. The prediction $h_k(x)$ is the majority label among those neighbors. With odd $k$, ties cannot occur because the majority count is strictly greater than $k/2$. This method is nonparametric and relies directly on $D_{\\text{train}}$, which remains uncorrupted.\n\nData generation uses a well-tested linear-separator model with additive Gaussian noise. We draw features $x \\in \\mathbb{R}^2$ independently from a standard normal distribution. The latent score is $s(x) = w^\\top x + b$ with fixed $w = [1.0, -0.8]$ and $b = 0.1$. Labels follow $y = \\mathbf{1}\\{s(x) + \\epsilon > 0\\}$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma = 0.3$. This yields class overlap, ensuring that hyperparameter choice affects accuracy meaningfully. We use sizes $n_{\\text{train}} = 200$, $n_{\\text{val}} = 150$, and $n_{\\text{test}} = 400$.\n\nStructured validation corruption flips labels deterministically in a geometric region: for $x = (x_1, x_2)$, if $(x_1 > \\tau)$ and $(|x_2| \\le \\omega)$, then the label is replaced by $1 - y$. This creates systematic bias in $D_{\\text{val}}$ that depends on location in feature space. The corruption parameters $(\\tau, \\omega)$ define the structure and severity of corruption.\n\nAlgorithmic steps:\n1. Generate $D_{\\text{train}}$, $D_{\\text{val}}$, and $D_{\\text{test}}$ according to the specified process, with fixed random seed to ensure determinism.\n2. For each test case $(\\tau, \\omega)$, produce $D_{\\text{val}}^{\\text{corrupt}}$ by flipping labels in the corruption region, leaving $D_{\\text{train}}$ and $D_{\\text{test}}$ unchanged.\n3. For each $k$ in the grid $\\mathcal{K} = \\{1, 3, 5, 11, 21, 41\\}$:\n   - Compute predictions on $D_{\\text{val}}$ using $h_k$ trained on $D_{\\text{train}}$ and compute $R(h_k; D_{\\text{val}}^{\\text{corrupt}})$ to perform selection with corrupted labels.\n   - Also compute $R(h_k; D_{\\text{val}}^{\\text{clean}})$ for oracle selection using clean validation labels.\n4. Select $k^\\star$ and $k^\\dagger$ by minimizing the corresponding empirical risks, breaking ties toward smaller $k$.\n5. Evaluate $h_{k^\\star}$ and $h_{k^\\dagger}$ on $D_{\\text{test}}$ to obtain test accuracies $\\text{test\\_acc}(k^\\star)$ and $\\text{test\\_acc}(k^\\dagger)$, respectively.\n6. Declare that $D_{\\text{test}}$ reveals misselection if either $k^\\star \\neq k^\\dagger$ or $|\\text{test\\_acc}(k^\\star) - \\text{test\\_acc}(k^\\dagger)| > \\delta$ with $\\delta = 0.02$.\n\nPrinciple-based reasoning: ERM prescribes minimizing empirical risk on a representative validation set to approximate minimizing expected risk. Structured corruption violates representativeness by systematically flipping labels in a region, which can favor hypotheses that agree with the corrupted labels rather than the underlying data-generating process. For KNN, larger $k$ values smooth decisions by averaging more neighbors; if a region’s labels are flipped, certain $k$ may align better with the corrupted labels, changing selection. The test set, being clean and independently drawn, provides an unbiased estimate of generalization performance. A discrepancy between the selected model under corruption and the oracle-selected model, measured on $D_{\\text{test}}$, indicates misselection. The threshold $\\delta$ sets a criterion for practical significance beyond random variation.\n\nTest suite coverage:\n- Case with $(\\tau, \\omega) = (100.0, 1.0)$ effectively causes no corruption because $(x_1 > 100.0)$ is overwhelmingly unlikely under a standard normal; this is a boundary condition expecting $k^\\star = k^\\dagger$ and small accuracy differences.\n- Case with $(\\tau, \\omega) = (0.2, 0.5)$ introduces moderate structured corruption in a region near the decision boundary, potentially shifting selection.\n- Case with $(\\tau, \\omega) = (-0.5, 10.0)$ flips labels in a large half-space slab, a strong corruption expected to cause misselection.\n- Case with $(\\tau, \\omega) = (0.0, 0.15)$ concentrates corruption near the origin along the $x_2$ axis, probing sensitivity to narrow localized corruption.\n\nThe program implements these steps and outputs, for each test case, the tuple $[k^\\star, k^\\dagger, \\text{test\\_acc}(k^\\star), \\text{test\\_acc}(k^\\dagger), \\text{misselection\\_revealed}]$, aggregated into a single bracketed list printed on one line, as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(n_train=200, n_val=150, n_test=400, d=2, w=np.array([1.0, -0.8]), b=0.1, noise_std=0.3, seed=42):\n    rng = np.random.default_rng(seed)\n    def make_split(n):\n        X = rng.normal(loc=0.0, scale=1.0, size=(n, d))\n        eps = rng.normal(loc=0.0, scale=noise_std, size=n)\n        s = X @ w + b + eps\n        y = (s > 0).astype(int)\n        return X, y\n    X_train, y_train = make_split(n_train)\n    X_val, y_val = make_split(n_val)\n    X_test, y_test = make_split(n_test)\n    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n\ndef corrupt_validation_labels(X_val, y_val, tau, omega):\n    # Structured corruption: flip labels when (x1 > tau) and (|x2| = omega)\n    mask = (X_val[:, 0] > tau)  (np.abs(X_val[:, 1]) = omega)\n    y_corrupt = y_val.copy()\n    y_corrupt[mask] = 1 - y_corrupt[mask]\n    return y_corrupt\n\ndef pairwise_sq_dists(X_query, X_train):\n    # Efficient squared Euclidean distances using expansion\n    # ||q - t||^2 = ||q||^2 + ||t||^2 - 2 q dot t\n    q_norm2 = np.sum(X_query**2, axis=1, keepdims=True)  # shape (n_query, 1)\n    t_norm2 = np.sum(X_train**2, axis=1)[None, :]        # shape (1, n_train)\n    dists = q_norm2 + t_norm2 - 2.0 * (X_query @ X_train.T)\n    return dists\n\ndef knn_predictions_sorted(train_X, train_y, query_X, sorted_idx, k):\n    # sorted_idx: indices of training points sorted by distance for each query, shape (n_query, n_train)\n    neighbors = sorted_idx[:, :k]  # shape (n_query, k)\n    neighbor_labels = train_y[neighbors]  # shape (n_query, k)\n    votes = np.sum(neighbor_labels, axis=1)  # number of 1's\n    # With odd k, majority is votes > k/2\n    preds = (votes > (k / 2.0)).astype(int)\n    return preds\n\ndef accuracy(preds, y_true):\n    return float(np.mean(preds == y_true))\n\ndef select_k(train_X, train_y, val_X, val_y_obs, sorted_idx_val, k_grid):\n    # Returns best k by minimizing misclassification risk (= 1 - accuracy), tie-breaker smallest k.\n    best_k = None\n    best_acc = -np.inf  # maximize accuracy\n    for k in k_grid:\n        preds = knn_predictions_sorted(train_X, train_y, val_X, sorted_idx_val, k)\n        acc = accuracy(preds, val_y_obs)\n        if acc > best_acc or (acc == best_acc and (best_k is None or k  best_k)):\n            best_acc = acc\n            best_k = k\n    return best_k, best_acc\n\ndef solve():\n    # Define the test cases from the problem statement: (tau, omega)\n    test_cases = [\n        (100.0, 1.0),   # effectively no corruption\n        (0.2, 0.5),     # moderate localized corruption\n        (-0.5, 10.0),   # severe widespread corruption\n        (0.0, 0.15),    # narrow band corruption\n    ]\n    # Generate data once (same underlying splits for all test cases)\n    (X_train, y_train), (X_val, y_val_clean), (X_test, y_test) = generate_data()\n    # Precompute sorted neighbor indices for val and test to reuse across k\n    dists_val = pairwise_sq_dists(X_val, X_train)\n    sorted_idx_val = np.argsort(dists_val, axis=1)\n    dists_test = pairwise_sq_dists(X_test, X_train)\n    sorted_idx_test = np.argsort(dists_test, axis=1)\n\n    k_grid = [1, 3, 5, 11, 21, 41]\n    delta = 0.02  # detection threshold for test accuracy difference\n\n    results = []\n    for tau, omega in test_cases:\n        # Apply structured corruption to validation labels\n        y_val_corrupt = corrupt_validation_labels(X_val, y_val_clean, tau, omega)\n\n        # Select k using corrupted validation labels\n        k_star, acc_val_star = select_k(X_train, y_train, X_val, y_val_corrupt, sorted_idx_val, k_grid)\n\n        # Oracle selection using clean validation labels\n        k_dagger, acc_val_dagger = select_k(X_train, y_train, X_val, y_val_clean, sorted_idx_val, k_grid)\n\n        # Evaluate both selections on the clean test set\n        preds_test_star = knn_predictions_sorted(X_train, y_train, X_test, sorted_idx_test, k_star)\n        test_acc_star = accuracy(preds_test_star, y_test)\n\n        preds_test_dagger = knn_predictions_sorted(X_train, y_train, X_test, sorted_idx_test, k_dagger)\n        test_acc_dagger = accuracy(preds_test_dagger, y_test)\n\n        # Determine if D_test reveals misselection\n        misselection_revealed = (k_star != k_dagger) or (abs(test_acc_star - test_acc_dagger) > delta)\n\n        results.append([k_star, k_dagger, round(test_acc_star, 6), round(test_acc_dagger, 6), misselection_revealed])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3188658"}, {"introduction": "When dealing with time-series data, the assumption that data points are independent and identically distributed (IID) no longer holds, making standard random splitting strategies inappropriate. The method used to partition data into training, validation, and test sets becomes a critical choice that directly impacts a model's perceived performance and its ability to generalize to the future. This hands-on practice challenges you to implement and evaluate two distinct time-aware splitting strategies—contiguous blocks and interleaved sampling—in a scenario with changing seasonal patterns [@problem_id:3188604]. By doing so, you will gain crucial insights into how to select robust feature extraction methods and validate models in a temporal context.", "problem": "You are given a synthetic, fully specified statistical learning task designed to evaluate how training, validation, and test splitting strategies interact with temporal covariates and static features when feature extraction uses sliding windows. Your goal is to write a program that implements a complete training-validation-test pipeline, selects a feature extraction window by validation, and reports the test-set mean squared error (expressed as a decimal), while respecting time-aware constraints. The program must be self-contained and must not read input.\n\nThe data generating process is defined as follows. There is a single static feature and a univariate temporal covariate. Let $t \\in \\{0,1,\\dots,T-1\\}$ index time. Define a temporal covariate sequence $c_t$ by\n$$\nc_t = A \\cdot \\sin\\left(\\frac{2\\pi}{P} \\, t + \\phi_t\\right),\n$$\nwhere the phase $\\phi_t$ may change once at a designated change-point $T_{\\text{shift}}$. Specifically,\n$$\n\\phi_t = \n\\begin{cases}\n0  \\text{if } t  T_{\\text{shift}},\\\\\n\\phi_{\\text{shift}}  \\text{if } t \\ge T_{\\text{shift}}.\n\\end{cases}\n$$\nAll trigonometric arguments are in radians. The static feature is a constant scalar $s$ for all times. The response $y_t$ is generated using a rolling mean of the temporal covariate with a true window size that may change at $T_{\\text{shift}}$:\n$$\nw^*(t) = \n\\begin{cases}\nw_1  \\text{if } t  T_{\\text{shift}},\\\\\nw_2  \\text{if } t \\ge T_{\\text{shift}},\n\\end{cases}\n$$\nand\n$$\ny_t = \\beta_0 + \\beta_1 \\cdot \\frac{1}{w^*(t)} \\sum_{j=1}^{w^*(t)} c_{t-j} + \\beta_2 \\cdot s,\n$$\nwhich is well-defined only when $t \\ge \\max\\{w_1,w_2\\}$. There is no stochastic noise.\n\nYou must build a linear model to predict $y_t$ from features constructed with a candidate rolling window $w$ applied to the observed covariate $c_t$:\n- For any candidate window $w$, the feature vector at time $t$ is\n$$\nx^{(w)}_t = \\left[\\,1,\\;\\frac{1}{w}\\sum_{j=1}^{w} c_{t-j},\\; s\\,\\right],\n$$\nvalid for $t \\ge w$. You must consider multiple candidate windows from a provided finite set $\\mathcal{W}$ and choose the best window by validation.\n\nTraining-validation-test process and constraints:\n- Let the set of “eligible” time indices be $\\mathcal{I} = \\{t_{\\min}, t_{\\min}+1, \\dots, T-1\\}$ where $t_{\\min} = \\max\\left(\\{w_1,w_2\\} \\cup \\mathcal{W}\\right)$ to ensure all rolling means are well-defined.\n- You must support two deterministic splitting strategies:\n\n  1. Contiguous time blocks with gaps: Given proportions $p_{\\text{train}} = 0.6$, $p_{\\text{val}} = 0.2$, and an integer gap $g \\ge 0$, split $\\mathcal{I}$ into disjoint contiguous segments for training, validation, and test, inserting a gap of size $g$ between train and validation, and another gap of size $g$ between validation and test. Let $n = |\\mathcal{I}|$. Define $n_{\\text{train}} = \\lfloor 0.6n \\rfloor$, $n_{\\text{val}} = \\lfloor 0.2n \\rfloor$, and let the remaining indices after accounting for gaps be assigned to the test set (this remaining count is $n - n_{\\text{train}} - n_{\\text{val}} - 2g$ and is assumed nonnegative in the provided test suite). The order of time must be preserved within each segment.\n  \n  2. Interleaved periodic split: Fix a period $m = 5$. Assign indices in $\\mathcal{I}$ to sets by their residue modulo $m$: training if $t \\bmod m \\in \\{0,1,2\\}$, validation if $t \\bmod m \\in \\{3\\}$, test if $t \\bmod m \\in \\{4\\}$.\n\n- For each candidate window $w \\in \\mathcal{W}$, fit an ordinary least squares linear regression on the training set to minimize empirical squared loss for predicting $y_t$ from $x^{(w)}_t$. Denote the training design matrix by $X_{\\text{train}}^{(w)}$ and the training response vector by $y_{\\text{train}}$. The least squares estimate $\\hat{\\theta}^{(w)}$ satisfies the normal equations $X_{\\text{train}}^{(w)\\top} X_{\\text{train}}^{(w)} \\hat{\\theta}^{(w)} = X_{\\text{train}}^{(w)\\top} y_{\\text{train}}$. Use any numerically stable method to obtain one solution (for instance, the Moore-Penrose pseudoinverse).\n- Evaluate each $w$ on the validation set by its mean squared error\n$$\n\\text{MSE}_{\\text{val}}(w) = \\frac{1}{|\\mathcal{V}|} \\sum_{t \\in \\mathcal{V}} \\left(y_t - x^{(w)}_t \\cdot \\hat{\\theta}^{(w)}\\right)^2,\n$$\nand select the window $\\hat{w}$ that minimizes $\\text{MSE}_{\\text{val}}(w)$. In case of ties in $\\text{MSE}_{\\text{val}}$, choose the smallest $w$ among the minimizers.\n- After selecting $\\hat{w}$, refit the model using the union of training and validation sets with the same feature definition to obtain $\\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}$.\n- Report the test-set mean squared error\n$$\n\\text{MSE}_{\\text{test}} = \\frac{1}{|\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left(y_t - x^{(\\hat{w})}_t \\cdot \\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}\\right)^2.\n$$\n\nAngle unit requirement: all angles are in radians.\n\nYour program must implement the above procedure for the following test suite of parameter settings. Each test case specifies $(T, P, A, \\beta_0, \\beta_1, \\beta_2, s, w_1, w_2, T_{\\text{shift}}, \\phi_{\\text{shift}}, \\text{split}, \\text{extra}, \\mathcal{W})$:\n\n- Test case $1$ (happy path, no seasonal phase or window change):\n  - $T=120$, $P=24$, $A=2$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=8$, $w_2=8$, $T_{\\text{shift}}=120$, $\\phi_{\\text{shift}}=0$.\n  - Split: contiguous with gap $g=3$; use $p_{\\text{train}}=0.6$, $p_{\\text{val}}=0.2$ as defined above.\n  - Candidate windows $\\mathcal{W}=\\{4,8,12\\}$.\n\n- Test case $2$ (seasonal phase and window change only at test time):\n  - $T=120$, $P=24$, $A=2$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=4$, $w_2=12$, $T_{\\text{shift}}=80$, $\\phi_{\\text{shift}}=\\frac{\\pi}{2}$.\n  - Split: contiguous with gap $g=3$; use $p_{\\text{train}}=0.6$, $p_{\\text{val}}=0.2$ as defined above.\n  - Candidate windows $\\mathcal{W}=\\{4,8,12\\}$.\n\n- Test case $3$ (interleaved split under seasonal phase and window change):\n  - $T=120$, $P=24$, $A=2$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=6$, $w_2=10$, $T_{\\text{shift}}=60$, $\\phi_{\\text{shift}}=\\frac{\\pi}{3}$.\n  - Split: interleaved with period $m=5$, assigning training to residues $\\{0,1,2\\}$, validation to $\\{3\\}$, test to $\\{4\\}$.\n  - Candidate windows $\\mathcal{W}=\\{4,6,10,12\\}$.\n\n- Test case $4$ (boundary on window size and no change):\n  - $T=40$, $P=10$, $A=1.5$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=1$, $w_2=1$, $T_{\\text{shift}}=40$, $\\phi_{\\text{shift}}=0$.\n  - Split: contiguous with gap $g=0$; use $p_{\\text{train}}=0.6$, $p_{\\text{val}}=0.2$ as defined above.\n  - Candidate windows $\\mathcal{W}=\\{1,2\\}$.\n\nImplementation requirements:\n- For each test case, build $c_t$ and $y_t$ exactly as described.\n- Compute features using only past covariate values $c_{t-j}$; do not use future values relative to $t$.\n- Apply the specified split strategy to the eligible index set $\\mathcal{I} = \\{t_{\\min},\\dots,T-1\\}$ with $t_{\\min} = \\max(\\{w_1,w_2\\}\\cup \\mathcal{W})$.\n- Train per-candidate models on training data, select $\\hat{w}$ by minimizing validation mean squared error with tie-breaker favoring smaller $w$, refit on train plus validation for $\\hat{w}$, and compute the test mean squared error.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, with each mean squared error rounded to exactly $6$ decimal places. For example, an output might look like $[0.000123,0.045678,0.010000,0.000000]$.", "solution": "The problem presents a synthetic, deterministic statistical learning task designed to evaluate model selection strategies in a time-series context with potential structural breaks. The procedure involves data generation, feature engineering using sliding windows, time-aware data splitting, model training, hyperparameter (window size) selection via validation, and final performance evaluation on a test set. The solution adheres strictly to the specified process.\n\n**1. Data Generation**\n\nFirst, for each test case, we generate the temporal covariate sequence $c_t$ and the response sequence $y_t$ over the time horizon $t \\in \\{0, 1, \\dots, T-1\\}$.\n\nThe temporal covariate $c_t$ is defined as a sinusoidal function:\n$$\nc_t = A \\cdot \\sin\\left(\\frac{2\\pi}{P} \\, t + \\phi_t\\right)\n$$\nwhere the phase $\\phi_t$ is piecewise constant, changing from $0$ to a specified value $\\phi_{\\text{shift}}$ at a given time $t=T_{\\text{shift}}$:\n$$\n\\phi_t = \n\\begin{cases}\n0  \\text{if } t  T_{\\text{shift}},\\\\\n\\phi_{\\text{shift}}  \\text{if } t \\ge T_{\\text{shift}}.\n\\end{cases}\n$$\n\nThe response variable $y_t$ is a linear function of a constant static feature $s$ and a rolling mean of the past values of $c_t$. The window size $w^*(t)$ used for this rolling mean is also piecewise constant, changing from $w_1$ to $w_2$ at $T_{\\text{shift}}$:\n$$\nw^*(t) = \n\\begin{cases}\nw_1  \\text{if } t  T_{\\text{shift}},\\\\\nw_2  \\text{if } t \\ge T_{\\text{shift}}.\n\\end{cases}\n$$\nThe response $y_t$ is then given by:\n$$\ny_t = \\beta_0 + \\beta_1 \\cdot \\left(\\frac{1}{w^*(t)} \\sum_{j=1}^{w^*(t)} c_{t-j}\\right) + \\beta_2 \\cdot s\n$$\nThis value is defined only for time points $t$ where the lookback window does not extend before $t=0$. To ensure all candidate models and the true data generating process are well-defined, we operate on a set of \"eligible\" time indices $\\mathcal{I} = \\{t_{\\min}, t_{\\min}+1, \\dots, T-1\\}$, where $t_{\\min}$ is the maximum of all window sizes involved (both true and candidate): $t_{\\min} = \\max\\left(\\{w_1, w_2\\} \\cup \\mathcal{W}\\right)$.\n\n**2. Data Splitting**\n\nThe set of eligible indices $\\mathcal{I}$ is partitioned into training, validation, and test sets according to one of two specified strategies:\n\n- **Contiguous Time Blocks:** The ordered indices of $\\mathcal{I}$ are split into three contiguous blocks for training, validation, and test. The training set takes the first $\\lfloor 0.6n \\rfloor$ indices and the validation set takes the next $\\lfloor 0.2n \\rfloor$ indices, where $n = |\\mathcal{I}|$. A temporal gap of $g$ indices is enforced between the training and validation sets, and again between the validation and test sets to simulate a delay in receiving data. The test set comprises all remaining indices.\n- **Interleaved Periodic Split:** Indices from $\\mathcal{I}$ are assigned to sets based on their value modulo a period $m=5$. Indices with $t \\bmod 5 \\in \\{0, 1, 2\\}$ form the training set, those with $t \\bmod 5 = 3$ form the validation set, and those with $t \\bmod 5 = 4$ form the test set. This strategy ensures that all three sets are drawn from across the entire time period, which can be advantageous if the data properties change over time.\n\n**3. Feature Engineering and Model Selection**\n\nThe predictive model is a linear regression model. For each candidate window size $w$ from the set $\\mathcal{W}$, a feature vector $x^{(w)}_t$ is constructed at each time $t$:\n$$\nx^{(w)}_t = \\left[\\,1,\\;\\frac{1}{w}\\sum_{j=1}^{w} c_{t-j},\\; s\\,\\right]\n$$\nThe three components of this vector correspond to the intercept, a feature derived from the temporal covariate, and the static feature.\n\nThe core of the task is to select the optimal window size $\\hat{w}$. This is achieved through the following steps:\n1.  For each candidate window $w \\in \\mathcal{W}$, an ordinary least squares (OLS) linear model is fitted using the training data. The model coefficients $\\hat{\\theta}^{(w)}$ are found by solving the normal equations, for which we use the Moore-Penrose pseudoinverse to ensure numerical stability: $\\hat{\\theta}^{(w)} = (X_{\\text{train}}^{(w)\\top} X_{\\text{train}}^{(w)})^{-1} X_{\\text{train}}^{(w)\\top} y_{\\text{train}}$. In implementation, this is calculated as $\\hat{\\theta}^{(w)} = \\text{pinv}(X_{\\text{train}}^{(w)}) \\cdot y_{\\text{train}}$.\n2.  The performance of each trained model $(\\hat{\\theta}^{(w)}, w)$ is evaluated on the validation set by computing the mean squared error (MSE):\n    $$\n    \\text{MSE}_{\\text{val}}(w) = \\frac{1}{|\\mathcal{V}|} \\sum_{t \\in \\mathcal{V}} \\left(y_t - x^{(w)}_t \\cdot \\hat{\\theta}^{(w)}\\right)^2\n    $$\n3.  The window size $\\hat{w}$ that yields the minimum validation MSE is selected as the best hyperparameter. A tie-breaking rule specifies that if multiple windows result in the same minimal MSE, the smallest window size among them is chosen.\n\n**4. Final Evaluation**\n\nAfter identifying the optimal window size $\\hat{w}$, the model is retrained. This time, the training data is augmented with the validation data, forming a combined training-validation set. This step leverages more data to get a potentially more robust estimate of the model parameters. The new coefficients $\\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}$ are obtained by fitting the model with feature set $x^{(\\hat{w})}_t$ on this combined dataset.\n\nFinally, the performance of this refitted model is assessed on the hitherto unused test set. The test MSE is calculated, representing the model's generalization error on unseen data:\n$$\n\\text{MSE}_{\\text{test}} = \\frac{1}{|\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left(y_t - x^{(\\hat{w})}_t \\cdot \\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}\\right)^2\n$$\nThis final value is the reported result for each test case. The entire procedure is deterministic, yielding a unique result for each set of input parameters.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (T, P, A, beta0, beta1, beta2, s, w1, w2, T_shift, phi_shift, split_type, extra, W_set)\n        (120, 24, 2, 0.5, 1.2, -0.7, 1.3, 8, 8, 120, 0.0, 'contiguous', 3, {4, 8, 12}),\n        (120, 24, 2, 0.5, 1.2, -0.7, 1.3, 4, 12, 80, np.pi/2, 'contiguous', 3, {4, 8, 12}),\n        (120, 24, 2, 0.5, 1.2, -0.7, 1.3, 6, 10, 60, np.pi/3, 'interleaved', 5, {4, 6, 10, 12}),\n        (40, 10, 1.5, 0.5, 1.2, -0.7, 1.3, 1, 1, 40, 0.0, 'contiguous', 0, {1, 2}),\n    ]\n\n    results = []\n    for params in test_cases:\n        mse = solve_case(*params)\n        results.append(f\"{mse:.6f}\")\n    \n    print(f\"[{','.join(results)}]\")\n\ndef solve_case(T, P, A, beta0, beta1, beta2, s, w1, w2, T_shift, phi_shift, split_type, extra, W_set):\n    \"\"\"\n    Solves a single instance of the statistical learning problem.\n    \"\"\"\n    # 1. Determine eligible time indices\n    t_min = max(list(W_set) + [w1, w2])\n\n    # 2. Generate data: c_t and y_t\n    t_range = np.arange(T)\n    phi_t = np.where(t_range  T_shift, 0, phi_shift)\n    c_t = A * np.sin(2 * np.pi / P * t_range + phi_t)\n\n    w_star_t = np.where(t_range  T_shift, w1, w2)\n    y_full = np.full(T, np.nan)\n    for t in range(t_min, T):\n        w_star = w_star_t[t]\n        # Rolling mean for the true response. Slicing c_t[t-w_star:t] corresponds to c_{t-j} for j=1..w_star\n        mean_c = np.mean(c_t[t - w_star : t])\n        y_full[t] = beta0 + beta1 * mean_c + beta2 * s\n    \n    # 3. Create data splits\n    eligible_indices = np.arange(t_min, T)\n    if split_type == 'contiguous':\n        n = len(eligible_indices)\n        g = extra\n        n_train = int(0.6 * n)\n        n_val = int(0.2 * n)\n        \n        train_indices = eligible_indices[:n_train]\n        val_indices = eligible_indices[n_train + g : n_train + g + n_val]\n        test_indices = eligible_indices[n_train + g + n_val + g :]\n    elif split_type == 'interleaved':\n        m = extra\n        train_indices = eligible_indices[np.isin(eligible_indices % m, [0, 1, 2])]\n        val_indices = eligible_indices[eligible_indices % m == 3]\n        test_indices = eligible_indices[eligible_indices % m == 4]\n    else:\n        raise ValueError(\"Unknown split type\")\n\n    # 4. Pre-compute rolling mean features for all candidate windows\n    rolling_means = {}\n    for w in W_set:\n        means_w = np.full(T, np.nan)\n        for t in range(w, T):\n            means_w[t] = np.mean(c_t[t-w:t])\n        rolling_means[w] = means_w\n\n    # 5. Model selection loop (hyperparameter tuning)\n    val_mses = {}\n    for w in W_set:\n        # Construct training data matrix and response vector\n        X_train = np.c_[np.ones(len(train_indices)), rolling_means[w][train_indices], np.full(len(train_indices), s)]\n        y_train = y_full[train_indices]\n        \n        # Fit model using pseudoinverse for numerical stability\n        theta_hat = np.linalg.pinv(X_train) @ y_train\n        \n        # Evaluate on validation set\n        if len(val_indices) > 0:\n            X_val = np.c_[np.ones(len(val_indices)), rolling_means[w][val_indices], np.full(len(val_indices), s)]\n            y_val = y_full[val_indices]\n            \n            y_pred_val = X_val @ theta_hat\n            val_mses[w] = np.mean((y_val - y_pred_val)**2)\n        else: # Handle cases with empty validation set\n            val_mses[w] = np.inf\n\n    # Select best window w_hat based on validation MSE, with tie-breaking\n    w_hat = min(val_mses.keys(), key=lambda w: (val_mses[w], w))\n\n    # 6. Refit on train+validation and evaluate on test set\n    refit_indices = np.sort(np.concatenate((train_indices, val_indices)))\n    \n    X_refit = np.c_[np.ones(len(refit_indices)), rolling_means[w_hat][refit_indices], np.full(len(refit_indices), s)]\n    y_refit = y_full[refit_indices]\n    \n    theta_refit = np.linalg.pinv(X_refit) @ y_refit\n    \n    if len(test_indices) > 0:\n        X_test = np.c_[np.ones(len(test_indices)), rolling_means[w_hat][test_indices], np.full(len(test_indices), s)]\n        y_test = y_full[test_indices]\n        \n        y_pred_test = X_test @ theta_refit\n        test_mse = np.mean((y_test - y_pred_test)**2)\n    else: # Handle cases with empty test set\n        test_mse = 0.0\n    \n    return test_mse\n\nsolve()\n```", "id": "3188604"}]}