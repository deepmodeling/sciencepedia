{"hands_on_practices": [{"introduction": "While standard gradient descent is a reliable workhorse for optimization, its convergence can be slow. Nesterov's accelerated gradient method famously improves the convergence rate from $O(1/t)$ to the optimal $O(1/t^2)$ for smooth convex functions by using a clever momentum term. In this practice [@problem_id:3126019], you will implement this powerful algorithm to empirically verify its accelerated rate and, just as importantly, diagnose its failure when the crucial assumption of a Lipschitz-continuous gradient is violated, providing deep insight into the interplay between theory and practice.", "problem": "You are tasked with implementing and empirically assessing an accelerated first-order optimization method for convex functions from the standpoint of gradients and convexity in statistical learning. The goal is to verify the theoretically expected convergence rate under smoothness assumptions and to diagnose the breakdown when the key smoothness assumption is violated. Your program must implement a single procedure to run the same accelerated method across all test cases and return a boolean decision per case.\n\nFundamental base and assumptions:\n- A function $f:\\mathbb{R}^d\\to\\mathbb{R}$ is convex if for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$ and $\\theta\\in[0,1]$, $f(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})\\le \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y})$.\n- A differentiable function has an $L$-Lipschitz gradient if there exists $L\\ge 0$ such that $\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|_2\\le L\\|\\mathbf{x}-\\mathbf{y}\\|_2$ for all $\\mathbf{x},\\mathbf{y}$. This implies the descent lemma: $f(\\mathbf{y})\\le f(\\mathbf{x})+\\nabla f(\\mathbf{x})^\\top(\\mathbf{y}-\\mathbf{x})+\\frac{L}{2}\\|\\mathbf{y}-\\mathbf{x}\\|_2^2$.\n- Nesterov accelerated gradient (NAG) is an accelerated first-order method that, under the $L$-Lipschitz gradient assumption, attains the optimal worst-case rate $O(1/t^2)$ in terms of objective suboptimality $f(\\mathbf{x}_t)-f^\\star$, where $t$ denotes the iteration index and $f^\\star$ is the minimum value.\n\nYour tasks:\n- Implement a single-version Nesterov accelerated gradient method that uses step size $1/L$ where $L$ is provided to your program on a per-test-case basis. Use a standard constant step size $1/L$ and the classical Nesterov extrapolation schedule that maintains an auxiliary sequence and a momentum parameter; apply the same implementation to all test cases. Your implementation must operate purely by evaluating the gradient and must not use any line search, backtracking, or oracle beyond the provided $L$.\n- For each test case, run the accelerated method for a prescribed number of iterations $T$, starting from the provided initial point $\\mathbf{x}_0$. Record the sequence of objective values $f(\\mathbf{x}_t)$.\n- To empirically assess whether the observed rate is consistent with $O(1/t^2)$, do the following:\n  1. Let $e_t = f(\\mathbf{x}_t) - f^\\star$, where $f^\\star$ is known for each test case and provided below. Form the sequence $s_t = t^2 e_t$ for $t=1,2,\\dots,T$.\n  2. Consider the last window of iterations $t\\in\\{\\lfloor 0.6T\\rfloor,\\dots,T\\}$. Compute the least-squares slope $\\hat{\\alpha}$ of the regression of $\\log e_t$ versus $\\log t$ over this window. Also compute the boundedness ratio $R = \\max_{t \\text{ in window}} s_t \\,/\\, \\min_{t \\text{ in window}} s_t$.\n  3. Return a boolean decision that is true if and only if both conditions hold simultaneously: $\\hat{\\alpha}\\le -1.8$ and $R \\le 10$. If the sequence ever produces a non-finite objective value, return false.\n- Your program must aggregate these decisions for all test cases and print them in a single line as a comma-separated Python list, for example $[\\text{True},\\text{False},\\text{True}]$.\n\nImplement the method and run it on the following test suite:\n\n- Test case $\\mathbf{A}$ (one-dimensional smooth quadratic, happy path):\n  - Dimension $d=1$.\n  - Function $f(x) = \\tfrac{1}{2}\\, a x^2$ with $a=10$, so the gradient is $\\nabla f(x) = a x$. This is convex with $L=a=10$.\n  - Minimizer $x^\\star=0$ and $f^\\star=0$.\n  - Initial point $x_0=5$.\n  - Use $L=10$ and $T=300$ iterations.\n\n- Test case $\\mathbf{B}$ (higher-dimensional smooth quadratic with varying curvature):\n  - Dimension $d=5$.\n  - Function $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\mathbf{x}$ with $A=\\mathrm{diag}(1,3,5,10,20)$ so $\\nabla f(\\mathbf{x})=A\\mathbf{x}$. This is convex with $L=\\lambda_{\\max}(A)=20$.\n  - Minimizer $\\mathbf{x}^\\star=\\mathbf{0}$ and $f^\\star=0$.\n  - Initial point $\\mathbf{x}_0=[5,-4,3,-2,1]^\\top$.\n  - Use $L=20$ and $T=400$ iterations.\n\n- Test case $\\mathbf{C}$ (convex but gradient not globally Lipschitz; diagnosis of failure):\n  - Dimension $d=5$.\n  - Function $f(\\mathbf{x})=\\sum_{i=1}^d |x_i|^{3/2}$. This function is convex and continuously differentiable with gradient components $\\nabla_i f(\\mathbf{x})=\\tfrac{3}{2}\\,\\mathrm{sign}(x_i)\\,|x_i|^{1/2}$, but its gradient is not Lipschitz on $\\mathbb{R}^d$ because its Jacobian norm is unbounded near $\\mathbf{x}=\\mathbf{0}$.\n  - Minimizer $\\mathbf{x}^\\star=\\mathbf{0}$ and $f^\\star=0$.\n  - Initial point $\\mathbf{x}_0=[5,-4,3,-2,1]^\\top$.\n  - Force the accelerated method to use the same constant $L=20$ and run $T=400$ iterations.\n  - The expected outcome is that the observed behavior will not match $O(1/t^2)$ according to the decision rule above.\n\nNumerical requirements:\n- Angles are not involved.\n- No physical units are involved.\n- The final output of your program must be a single line containing the three boolean decisions for test cases $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$, respectively, as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{True},\\text{False}]$.\n\nYour program must be fully self-contained and must not read any input.", "solution": "The user-provided problem has been rigorously validated and found to be well-posed, scientifically grounded, and internally consistent. All necessary parameters and definitions are provided for a complete and unique solution. The problem requires the implementation and empirical analysis of Nesterov's Accelerated Gradient (NAG) method on three distinct test cases, designed to highlight the importance of the gradient's Lipschitz continuity for achieving accelerated convergence rates.\n\n### Principle and Algorithm Design\n\nNesterov's Accelerated Gradient is a first-order optimization algorithm that improves upon the convergence rate of standard gradient descent for a class of convex functions. For a convex function $f$ with an $L$-Lipschitz continuous gradient (i.e., a function that is $L$-smooth), standard gradient descent has a convergence rate for the objective suboptimality, $f(\\mathbf{x}_t) - f^\\star$, of $O(1/t)$. NAG introduces a \"momentum\" term that incorporates information from previous steps to accelerate convergence, achieving the optimal rate of $O(1/t^2)$ for this class of functions.\n\nThe core idea is to evaluate the gradient not at the current iterate $\\mathbf{x}_t$, but at an extrapolated point $\\mathbf{y}_t$ that represents an \"aggressive\" step in the direction of recent progress. The update rule combines a standard gradient step with a momentum step. A common and effective implementation of NAG, which we will use, maintains two sequences of iterates, $\\mathbf{x}_t$ and $\\mathbf{y}_t$, and proceeds as follows:\n\nLet the initial point be $\\mathbf{x}_0$. Set the auxiliary sequence initial point $\\mathbf{y}_0 = \\mathbf{x}_0$. For iterations $t=0, 1, 2, \\dots, T-1$:\n\n1.  **Gradient Step:** The primary sequence is updated by taking a gradient step from the auxiliary point $\\mathbf{y}_t$ with a step size $\\eta = 1/L$.\n    $$\n    \\mathbf{x}_{t+1} = \\mathbf{y}_t - \\frac{1}{L} \\nabla f(\\mathbf{y}_t)\n    $$\n\n2.  **Momentum Step:** The auxiliary sequence is updated by projecting forward from the new point $\\mathbf{x}_{t+1}$ along the direction of the most recent step $(\\mathbf{x}_{t+1} - \\mathbf{x}_t)$.\n    $$\n    \\mathbf{y}_{t+1} = \\mathbf{x}_{t+1} + \\beta_t (\\mathbf{x}_{t+1} - \\mathbf{x}_t)\n    $$\n    The momentum parameter $\\beta_t$ must be chosen carefully. A standard schedule that guarantees the $O(1/t^2)$ rate is $\\beta_t = \\frac{t}{t+3}$ for our $0$-indexed iteration counter $t$.\n\nThe sequence of iterates whose convergence we analyze is $\\{\\mathbf{x}_t\\}_{t=1}^T$.\n\n### Empirical Rate Verification\n\nThe theoretical convergence rate $f(\\mathbf{x}_t) - f^\\star = O(1/t^2)$ implies that the error $e_t = f(\\mathbf{x}_t) - f^\\star$ behaves like $C/t^2$ for some constant $C$ and large $t$. We verify this empirically using two conditions as specified:\n\n1.  **Power-Law Decay Rate:** Taking the logarithm, we get $\\log e_t \\approx \\log C - 2 \\log t$. This shows a linear relationship between $\\log e_t$ and $\\log t$ with a slope of $-2$. We compute the least-squares slope $\\hat{\\alpha}$ of this relationship over the latter part of the iterations ($t \\in \\{\\lfloor 0.6T\\rfloor, \\dots, T\\}$). The condition $\\hat{\\alpha} \\le -1.8$ checks if the observed rate is consistent with the theoretical prediction, allowing for minor finite-iteration deviations.\n\n2.  **Asymptotic Boundedness:** The expression $s_t = t^2 e_t$ should approach the constant $C$ as $t \\to \\infty$. We check if this sequence is \"asymptotically bounded\" by computing the ratio $R = \\max(s_t) / \\min(s_t)$ over the same analysis window. A small ratio indicates that $s_t$ is settling to a nearly constant value. The condition $R \\le 10$ provides a generous tolerance for this behavior.\n\nA test case is considered to demonstrate the accelerated rate if and only if both conditions are met.\n\n### Analysis of Test Cases\n\n-   **Test Cases A and B:** These cases involve quadratic functions, $f(x) = \\frac{1}{2}ax^2$ and $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top A \\mathbf{x}$. The gradients, $\\nabla f(x) = ax$ and $\\nabla f(\\mathbf{x}) = A\\mathbf{x}$, are linear and therefore globally Lipschitz continuous. The provided Lipschitz constants ($L=10$ for A, $L=20$ for B) are correct. As these functions perfectly satisfy the assumptions for NAG, we expect the method to achieve its theoretical $O(1/t^2)$ rate. The empirical analysis should confirm this, leading to a `True` decision for both cases.\n\n-   **Test Case C:** The function is $f(\\mathbf{x}) = \\sum_i |x_i|^{3/2}$. This function is convex, but its gradient, with components $\\nabla_i f(\\mathbf{x}) = \\frac{3}{2}\\mathrm{sign}(x_i)|x_i|^{1/2}$, is not globally Lipschitz continuous. The Hessian's diagonal elements, $\\frac{\\partial^2 f}{\\partial x_i^2} = \\frac{3}{4}|x_i|^{-1/2}$, are unbounded as $x_i \\to 0$. This means the local smoothness of the function worsens dramatically near the origin. The guarantee for the $O(1/t^2)$ rate of NAG does not hold. Using a fixed step size based on a constant $L=20$ is inappropriate; as the iterates approach the origin, the local Lipschitz constant can become much larger than $20$, making the step size $1/20$ too large and leading to slow convergence or instability. Consequently, the observed convergence rate is expected to be substantially slower than $O(1/t^2)$. The empirical test is designed to detect this failure, and we anticipate a `False` decision.\n\nThe implementation will follow this logic precisely, executing the same NAG algorithm for all cases and applying the specified decision rule.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problem across all test cases and print the results.\n    \"\"\"\n\n    def run_nesterov_accelerated_gradient(func_f, grad_f, x0, L, T):\n        \"\"\"\n        Implements Nesterov's Accelerated Gradient (NAG) method.\n\n        Args:\n            func_f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The initial point.\n            L: The Lipschitz constant of the gradient.\n            T: The number of iterations.\n\n        Returns:\n            A list of objective values for iterations 1 to T, or None if a non-finite value is encountered.\n        \"\"\"\n        x_k = np.array(x0, dtype=np.float64)\n        y_k = np.array(x0, dtype=np.float64)\n        x_prev_k = np.array(x0, dtype=np.float64)\n        \n        objectives = []\n\n        for k in range(T): # Corresponds to iterations t=1, ..., T\n            x_prev_k = x_k\n            \n            # Gradient evaluation and update step for x\n            grad_val = grad_f(y_k)\n            if not np.all(np.isfinite(grad_val)):\n                return None\n            \n            x_k = y_k - (1.0 / L) * grad_val\n            \n            # Store objective value\n            obj_val = func_f(x_k)\n            if not np.isfinite(obj_val):\n                return None\n            objectives.append(obj_val)\n            \n            # Momentum update step for y\n            momentum_coeff = k / (k + 3.0)\n            y_k = x_k + momentum_coeff * (x_k - x_prev_k)\n            \n        return objectives\n\n    def analyze_convergence(objectives, f_star, T):\n        \"\"\"\n        Analyzes the convergence rate based on the sequence of objective values.\n\n        Args:\n            objectives: A list of T objective values.\n            f_star: The known minimum value of the function.\n            T: The total number of iterations.\n\n        Returns:\n            A boolean decision based on the specified criteria.\n        \"\"\"\n        if objectives is None:\n            return False\n\n        # Calculate errors e_t = f(x_t) - f_star for t=1,...,T\n        e_t = np.array([obj - f_star for obj in objectives])\n\n        # Define analysis window: iterations floor(0.6*T) to T\n        start_t = int(np.floor(0.6 * T))\n        if start_t > T:\n            return False \n        \n        window_indices = np.arange(start_t - 1, T)\n        if len(window_indices) < 2:\n            return False\n\n        t_vals = window_indices + 1\n        e_vals = e_t[window_indices]\n        \n        # Filter out non-positive errors to avoid issues with log\n        positive_mask = e_vals > 0\n        if np.sum(positive_mask) < 2: # Need at least two points for regression\n            return False\n\n        t_log = np.log(t_vals[positive_mask])\n        e_log = np.log(e_vals[positive_mask])\n\n        # Condition 1: Check the slope of log-log plot\n        try:\n            slope, _ = np.polyfit(t_log, e_log, 1)\n        except np.linalg.LinAlgError:\n            return False # Should not happen with valid data\n        \n        cond1 = slope <= -1.8\n\n        # Condition 2: Check the boundedness ratio of s_t = t^2 * e_t\n        s_t = (t_vals**2) * e_vals\n        s_t_valid = s_t[positive_mask]\n        \n        min_s_t = np.min(s_t_valid)\n        if min_s_t <= 0:\n            return False\n            \n        max_s_t = np.max(s_t_valid)\n        R = max_s_t / min_s_t\n        cond2 = R <= 10.0\n\n        return cond1 and cond2\n\n    # --- Test Cases Definition ---\n    # Case A\n    a_A = 10.0\n    func_A = lambda x: 0.5 * a_A * x**2\n    grad_A = lambda x: a_A * x\n    \n    # Case B\n    A_B = np.diag([1.0, 3.0, 5.0, 10.0, 20.0])\n    func_B = lambda x: 0.5 * x.T @ A_B @ x\n    grad_B = lambda x: A_B @ x\n\n    # Case C\n    # Note: np.sign(0) is 0, which gives the correct gradient at the origin.\n    func_C = lambda x: np.sum(np.power(np.abs(x), 1.5))\n    grad_C = lambda x: 1.5 * np.sign(x) * np.power(np.abs(x), 0.5)\n\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"func\": func_A, \"grad\": grad_A,\n            \"x0\": np.array([5.0]), \"L\": 10.0, \"T\": 300, \"f_star\": 0.0\n        },\n        {\n            \"id\": \"B\",\n            \"func\": func_B, \"grad\": grad_B,\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0, 1.0]), \"L\": 20.0, \"T\": 400, \"f_star\": 0.0\n        },\n        {\n            \"id\": \"C\",\n            \"func\": func_C, \"grad\": grad_C,\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0, 1.0]), \"L\": 20.0, \"T\": 400, \"f_star\": 0.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        objectives = run_nesterov_accelerated_gradient(\n            case[\"func\"], case[\"grad\"], case[\"x0\"], case[\"L\"], case[\"T\"]\n        )\n        decision = analyze_convergence(objectives, case[\"f_star\"], case[\"T\"])\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3126019"}, {"introduction": "Many fundamental problems in statistical learning, such as the LASSO for sparse regression, involve objective functions that are a composite of a smooth loss and a non-smooth regularizer. Standard gradient methods cannot be directly applied to such functions. This is where the proximal gradient method comes in, elegantly handling the objective by splitting it into a gradient step on the smooth part and a proximal step on the non-smooth part. Through this exercise [@problem_id:3126043], you will implement this widely used algorithm and empirically analyze its convergence, observing the transition from a fast linear rate under strong convexity to a slower sublinear rate without it.", "problem": "Consider the composite optimization problem in statistical learning where the objective is the sum of a smooth convex term and a convex non-smooth regularizer. Let $f(w) = g(w) + h(w)$ for $w \\in \\mathbb{R}^n$, where $g$ is differentiable and convex with a Lipschitz-continuous gradient, and $h$ is convex and possibly non-smooth but proximable. Specifically, you will study the case\n$$\ng(w) = \\tfrac{1}{2}\\lVert A w - b \\rVert_2^2,\\qquad h(w) = \\lambda \\lVert w \\rVert_1,\n$$\nwith the following fundamental definitions and facts as the starting point:\n- The gradient of the smooth term is $\\nabla g(w) = A^\\top (A w - b)$.\n- The gradient of $g$ is $L$-Lipschitz if there exists $L \\ge 0$ such that $\\lVert \\nabla g(u) - \\nabla g(v) \\rVert_2 \\le L \\lVert u - v \\rVert_2$ for all $u,v$. For the quadratic $g$ above, the smallest valid $L$ equals the spectral norm of $A^\\top A$, that is $L = \\lambda_{\\max}(A^\\top A)$.\n- The function $g$ is $\\mu$-strongly convex if $g(v) \\ge g(u) + \\nabla g(u)^\\top (v-u) + \\tfrac{\\mu}{2} \\lVert v-u \\rVert_2^2$ for all $u,v$. For the quadratic $g$ above, the largest valid $\\mu$ equals the smallest eigenvalue of $A^\\top A$, that is $\\mu = \\lambda_{\\min}(A^\\top A)$, which is strictly positive if and only if $A$ has full column rank.\n- The proximal map of $h$ with stepsize $\\alpha > 0$ is $\\mathrm{prox}_{\\alpha h}(v) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ h(u) + \\tfrac{1}{2\\alpha} \\lVert u - v \\rVert_2^2 \\right\\}$.\n\nYou will implement the proximal gradient method with a fixed stepsize $\\alpha = 1/L$, where each iteration is\n$$\nw^{k+1} = \\mathrm{prox}_{\\alpha h}\\big(w^k - \\alpha \\nabla g(w^k)\\big).\n$$\nFor $h(w) = \\lambda \\lVert w \\rVert_1$, the proximal map is the coordinate-wise soft-thresholding, which is a closed-form operation.\n\nYour tasks:\n1. For each test case, compute $L = \\lambda_{\\max}(A^\\top A)$ and $\\mu = \\lambda_{\\min}(A^\\top A)$.\n2. Implement proximal gradient with stepsize $\\alpha = 1/L$ from the given initial point $w^0$.\n3. Obtain a high-accuracy reference solution $w^\\star$ by running the algorithm for a large number of iterations to near-stationarity, and let $f^\\star = f(w^\\star)$.\n4. Analyze convergence as follows:\n   - If $\\mu > 0$, estimate an empirical linear factor by computing, over a tail segment of the iterates, the median of the ratios\n     $$\n     r_k = \\frac{\\lVert w^{k+1} - w^\\star \\rVert_2}{\\lVert w^{k} - w^\\star \\rVert_2},\n     $$\n     skipping indices with zero denominators. Denote this median by $q_{\\text{emp}}$. Also compute the theoretically predicted worst-case factor $q_{\\text{th}}$ that follows from the standard analysis of proximal gradient under $L$-Lipschitz continuity and $\\mu$-strong convexity of $g$ with stepsize $\\alpha = 1/L$. Your program should then report whether the empirical factor does not exceed the theoretical prediction up to a small tolerance, encoded as a boolean.\n   - If $\\mu = 0$, set $q_{\\text{th}} = -1.0$ and $q_{\\text{emp}} = -1.0$ as sentinels and verify the sublinear behavior by forming the tail sequence $c_k = k \\cdot (f(w^k) - f^\\star)$ and checking whether it is essentially nonincreasing within a small numerical tolerance on most steps; report this as a boolean.\n\nImplementation details to enforce scientific realism and testability:\n- Use the soft-thresholding formula for the proximal map of the $\\ell_1$-norm.\n- Use the Euclidean norm $\\lVert \\cdot \\rVert_2$ throughout.\n- To approximate $w^\\star$, iterate the method for $N_\\star = 20000$ steps starting from the given $w^0$.\n- For convergence diagnostics, run $K = 800$ iterations from the same initial point $w^0$ and use only the last $T = 200$ steps to compute empirical factors or sublinear checks.\n- For the strongly convex check, accept $q_{\\text{emp}} \\le q_{\\text{th}} + \\varepsilon_{\\text{lin}}$ with $\\varepsilon_{\\text{lin}} = 10^{-2}$ as success.\n- For the sublinear check when $\\mu = 0$, treat the sequence $\\{c_k\\}$ as essentially nonincreasing if at least a fraction $p = 0.95$ of the consecutive tail differences satisfy $c_{k+1} \\le c_k + \\varepsilon_{\\text{sub}}$ with $\\varepsilon_{\\text{sub}} = 10^{-9}$.\n- If $\\mu$ computed from eigenvalues is smaller than a numerical floor $\\delta = 10^{-12}$, treat it as zero for the purpose of deciding the branch $\\mu = 0$ versus $\\mu > 0$.\n\nTest suite:\nProvide results for the following three cases, each specified by $(A,b,\\lambda,w^0)$ with explicit numeric entries. All numbers are dimensionless.\n- Case $1$ (well-conditioned and strongly convex):\n  $$\n  A = \\begin{bmatrix}\n  3 & 0\\\\\n  0 & 2\\\\\n  0 & 0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  1\\\\\n  -2\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.2,\\quad\n  w^0 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}.\n  $$\n- Case $2$ (rank-deficient, not strongly convex):\n  $$\n  A = \\begin{bmatrix}\n  1 & 0\\\\\n  1 & 0\\\\\n  0 & 0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  1\\\\\n  0\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.1,\\quad\n  w^0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}.\n  $$\n- Case $3$ (ill-conditioned and strongly convex):\n  $$\n  A = \\begin{bmatrix}\n  10 & 0\\\\\n  0 & 1\\\\\n  0 & 0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  3\\\\\n  -1\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.05,\\quad\n  w^0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n  $$\n\nRequired outputs and format:\n- For each case in the order above, compute and output the sequence of five values\n  $$\n  \\big[L,\\ \\mu,\\ q_{\\text{th}},\\ q_{\\text{emp}},\\ \\text{check}\\big],\n  $$\n  where $L$ and $\\mu$ are as defined, $q_{\\text{th}}$ is the theoretically predicted worst-case linear factor when $\\mu > 0$ (and $-1.0$ when $\\mu = 0$), $q_{\\text{emp}}$ is the empirical factor as defined above (and $-1.0$ when $\\mu = 0$), and $\\text{check}$ is a boolean as defined above.\n- Your program should produce a single line of output containing all results concatenated across the three cases into one comma-separated list enclosed in square brackets, in the order\n  $$\n  \\big[L_1,\\mu_1,q_{\\text{th},1},q_{\\text{emp},1},\\text{check}_1,\\ L_2,\\mu_2,q_{\\text{th},2},q_{\\text{emp},2},\\text{check}_2,\\ L_3,\\mu_3,q_{\\text{th},3},q_{\\text{emp},3},\\text{check}_3\\big].\n  $$", "solution": "The user wants to analyze the convergence of the proximal gradient method for a LASSO-type problem. The problem is well-defined, scientifically sound, and all necessary parameters and procedures are explicitly stated. I will proceed with a full solution.\n\nThe objective function to minimize is $f(w) = g(w) + h(w)$, where $w \\in \\mathbb{R}^n$. The components are:\n- A smooth, convex loss term: $g(w) = \\frac{1}{2}\\lVert A w - b \\rVert_2^2$\n- A convex, non-smooth regularization term: $h(w) = \\lambda \\lVert w \\rVert_1$\n\nThe proximal gradient method updates the weight vector $w$ iteratively. The $k$-th iteration is given by:\n$$\nw^{k+1} = \\mathrm{prox}_{\\alpha h}\\left(w^k - \\alpha \\nabla g(w^k)\\right)\n$$\nwhere $\\alpha > 0$ is the stepsize.\n\nFirst, we need the components for this iteration:\n1.  **The gradient of $g(w)$**: The gradient of the quadratic loss term $g(w)$ is:\n    $$\n    \\nabla g(w) = A^\\top(A w - b)\n    $$\n2.  **The proximal map of $h(w)$**: For $h(w) = \\lambda \\lVert w \\rVert_1$, the proximal operator is the soft-thresholding function, which applies element-wise to a vector $v$:\n    $$\n    [\\mathrm{prox}_{\\alpha h}(v)]_i = \\mathrm{sign}(v_i) \\max(|v_i| - \\alpha \\lambda, 0)\n    $$\n    This operation effectively shrinks the components of $v$ towards zero, setting small components to exactly zero.\n\nThe convergence rate of the proximal gradient method depends on the properties of $g(w)$, specifically its gradient's Lipschitz constant $L$ and its strong convexity constant $\\mu$.\n-   The Lipschitz constant $L$ for $\\nabla g(w)$ is the largest eigenvalue (spectral norm) of the matrix $A^\\top A$. We denote this as $L = \\lambda_{\\max}(A^\\top A)$.\n-   The strong convexity constant $\\mu$ for $g(w)$ is the smallest eigenvalue of $A^\\top A$. We denote this as $\\mu = \\lambda_{\\min}(A^\\top A)$. $g(w)$ is strongly convex if and only if $\\mu > 0$, which occurs when $A$ has full column rank.\n\nThe problem specifies a fixed stepsize $\\alpha = 1/L$.\n\nThe solution process for each test case involves the following steps:\n1.  **Compute Constants**: Given the matrix $A$, we form $A^\\top A$ and compute its eigenvalues to find $L = \\lambda_{\\max}(A^\\top A)$ and $\\mu = \\lambda_{\\min}(A^\\top A)$. We apply a numerical floor $\\delta = 10^{-12}$: if the computed $\\mu$ is less than $\\delta$, we treat it as $0$.\n\n2.  **Find Reference Solution**: We run the proximal gradient algorithm for a large number of iterations, $N_\\star = 20000$, starting from the given initial point $w^0$. The final iterate is taken as a high-accuracy approximation of the true solution, $w^\\star$. The optimal function value is then $f^\\star = f(w^\\star)$.\n\n3.  **Generate Iterates for Analysis**: We run the algorithm again from $w^0$ for $K = 800$ iterations, storing the entire sequence of iterates $\\{w^0, w^1, \\dots, w^K\\}$.\n\n4.  **Analyze Convergence**: The analysis depends on whether $\\mu > 0$.\n\n    **Case 1: Strongly Convex ($\\mu > 0$)**\n    - The theory of proximal gradient descent with stepsize $\\alpha=1/L$ predicts linear convergence of the iterates to the solution $w^\\star$. The distance to the optimum is expected to decrease by a constant factor at each iteration.\n    - The theoretical worst-case convergence factor $q_{\\text{th}}$ is given by:\n      $$\n      q_{\\text{th}} = \\max\\left(\\left|1 - \\frac{\\mu}{L}\\right|, \\left|1 - \\frac{L}{L}\\right|\\right) = 1 - \\frac{\\mu}{L}\n      $$\n      since $\\mu \\le L$.\n    - We estimate the empirical convergence factor $q_{\\text{emp}}$ from the tail of the generated iterates (the last $T=200$ steps). For each step $k$ in this tail, we compute the ratio:\n      $$\n      r_k = \\frac{\\lVert w^{k+1} - w^\\star \\rVert_2}{\\lVert w^{k} - w^\\star \\rVert_2}\n      $$\n    - The empirical factor $q_{\\text{emp}}$ is the median of these ratios $\\{r_k\\}$.\n    - The final check verifies if the empirical rate is consistent with the theory, i.e., $q_{\\text{emp}} \\le q_{\\text{th}} + \\varepsilon_{\\text{lin}}$ for a tolerance $\\varepsilon_{\\text{lin}} = 10^{-2}$.\n\n    **Case 2: Convex, not Strongly Convex ($\\mu = 0$)**\n    - When $g(w)$ is not strongly convex, the convergence rate is sublinear. Theory predicts $f(w^k) - f^\\star = \\mathcal{O}(1/k)$.\n    - This implies that the sequence $c_k = k \\cdot (f(w^k) - f^\\star)$ should be bounded. We check for a more specific property: whether this sequence is essentially non-increasing in the tail.\n    - We set sentinel values $q_{\\text{th}} = -1.0$ and $q_{\\text{emp}} = -1.0$.\n    - For the last $T=200$ steps of the algorithm (from $k=K-T$ to $k=K-1$), we check if the condition $c_{k+1} \\le c_k + \\varepsilon_{\\text{sub}}$ holds, where $\\varepsilon_{\\text{sub}} = 10^{-9}$ is a small tolerance.\n    - The check is successful (evaluates to `True`) if this condition holds for at least a fraction $p=0.95$ of the steps in the tail.\n\nThis procedure is applied to each of the three test cases provided. The final output is a concatenated list of the five computed values ($L, \\mu, q_{\\text{th}}, q_{\\text{emp}}, \\text{check}$) for each case.", "answer": "```python\nimport numpy as np\n\n# Global constants from the problem statement\nN_STAR = 20000\nK = 800\nT = 200\nDELTA = 1e-12\nEPSILON_LIN = 1e-2\nEPSILON_SUB = 1e-9\nP = 0.95\n\ndef g(w, A, b):\n    \"\"\"Computes the value of the smooth term g(w).\"\"\"\n    return 0.5 * np.linalg.norm(A @ w - b)**2\n\ndef h(w, lambda_reg):\n    \"\"\"Computes the value of the non-smooth term h(w).\"\"\"\n    return lambda_reg * np.linalg.norm(w, 1)\n\ndef f_obj(w, A, b, lambda_reg):\n    \"\"\"Computes the value of the objective function f(w).\"\"\"\n    return g(w, A, b) + h(w, lambda_reg)\n\ndef grad_g(w, A, b):\n    \"\"\"Computes the gradient of the smooth term g(w).\"\"\"\n    return A.T @ (A @ w - b)\n\ndef prox_l1(v, step_size, lambda_reg):\n    \"\"\"Computes the proximal operator for the L1 norm (soft-thresholding).\"\"\"\n    return np.sign(v) * np.maximum(0, np.abs(v) - step_size * lambda_reg)\n\ndef run_proximal_gradient(w0, A, b, lambda_reg, alpha, n_iters):\n    \"\"\"Runs the proximal gradient algorithm for a given number of iterations.\"\"\"\n    w = np.copy(w0)\n    w_history = [np.copy(w)]\n    for _ in range(n_iters):\n        gradient = grad_g(w, A, b)\n        w_update = w - alpha * gradient\n        w = prox_l1(w_update, alpha, lambda_reg)\n        w_history.append(np.copy(w))\n    return w_history\n\ndef analyze_convergence(iterates, w_star, L, mu, A, b, lambda_reg):\n    \"\"\"Analyzes the convergence of the iterates based on mu.\"\"\"\n    if mu > 0:\n        # Strongly convex case\n        q_th = 1.0 - mu / L\n        \n        # Tail iterates from w^{K-T} to w^K (T+1 iterates)\n        tail_iterates = iterates[K - T : K + 1]\n        ratios = []\n        for i in range(T):\n            w_k = tail_iterates[i]\n            w_k_plus_1 = tail_iterates[i+1]\n            \n            denom = np.linalg.norm(w_k - w_star)\n            if denom > 1e-15:  # Avoid division by zero\n                num = np.linalg.norm(w_k_plus_1 - w_star)\n                ratios.append(num / denom)\n        \n        q_emp = np.median(ratios) if ratios else 0.0\n        check = q_emp <= q_th + EPSILON_LIN\n        \n    else:\n        # Convex, but not strongly convex case\n        q_th = -1.0\n        q_emp = -1.0\n        \n        f_star = f_obj(w_star, A, b, lambda_reg)\n        \n        success_count = 0\n        num_comparisons = T\n        \n        # Check c_{k+1} <= c_k + eps for k from K-T to K-1\n        for k in range(K - T, K):\n            f_k = f_obj(iterates[k], A, b, lambda_reg)\n            f_k_plus_1 = f_obj(iterates[k+1], A, b, lambda_reg)\n\n            c_k = k * (f_k - f_star)\n            c_k_plus_1 = (k + 1) * (f_k_plus_1 - f_star)\n            \n            if c_k_plus_1 <= c_k + EPSILON_SUB:\n                success_count += 1\n                \n        check = (success_count / num_comparisons) >= P\n\n    return q_th, q_emp, check\n\ndef solve_case(A_list, b_list, lambda_reg, w0_list):\n    \"\"\"Solves a single test case from the problem statement.\"\"\"\n    A = np.array(A_list, dtype=float)\n    b = np.array(b_list, dtype=float)\n    w0 = np.array(w0_list, dtype=float)\n\n    # 1. Compute L and mu\n    AtA = A.T @ A\n    eigenvalues = np.linalg.eigvalsh(AtA)\n    L = np.max(eigenvalues) if eigenvalues.size > 0 else 0.0\n    mu_raw = np.min(eigenvalues) if eigenvalues.size > 0 else 0.0\n    mu = mu_raw if mu_raw >= DELTA else 0.0\n    \n    alpha = 1.0 / L if L > 0 else 1.0\n\n    # 2. Compute reference solution w_star\n    w_star_history = run_proximal_gradient(w0, A, b, lambda_reg, alpha, N_STAR)\n    w_star = w_star_history[-1]\n\n    # 3. Generate iterates for analysis\n    iterates = run_proximal_gradient(w0, A, b, lambda_reg, alpha, K)\n    \n    # 4. Analyze convergence\n    q_th, q_emp, check = analyze_convergence(iterates, w_star, L, mu, A, b, lambda_reg)\n    \n    return [L, mu, q_th, q_emp, str(check).lower()]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (well-conditioned and strongly convex)\n        (\n            [[3, 0], [0, 2], [0, 0]], \n            [1, -2, 0], \n            0.2, \n            [2, -1]\n        ),\n        # Case 2 (rank-deficient, not strongly convex)\n        (\n            [[1, 0], [1, 0], [0, 0]], \n            [1, 0, 0], \n            0.1, \n            [0.5, 0.5]\n        ),\n        # Case 3 (ill-conditioned and strongly convex)\n        (\n            [[10, 0], [0, 1], [0, 0]], \n            [3, -1, 0], \n            0.05, \n            [2, 1]\n        ),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A, b, lambda_reg, w0 = case\n        result = solve_case(A, b, lambda_reg, w0)\n        all_results.extend(result)\n        \n    # Format the final output string\n    formatted_results = []\n    for item in all_results:\n        if isinstance(item, float):\n            formatted_results.append(f\"{item:.10f}\".rstrip('0').rstrip('.'))\n        else:\n            formatted_results.append(str(item))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3126043"}, {"introduction": "Classical gradient descent operates based on Euclidean geometry, which is not always suitable for problems with non-Euclidean constraints, like optimizing over the probability simplex $\\Delta_d$. Mirror descent provides a powerful generalization by using a \"mirror map\" to perform updates in a dual space that respects the problem's intrinsic geometry. In this advanced practice [@problem_id:3126055], you will explore this concept by implementing mirror descent with the entropy mirror map, leading to the elegant exponentiated gradient algorithm, and gain hands-on experience with the associated Bregman divergence.", "problem": "You are given the probability simplex in dimension $d$, defined as $\\Delta_d = \\{ p \\in \\mathbb{R}^d : \\sum_{j=1}^d p_j = 1,\\ p_j \\ge 0\\ \\forall j \\}$, and the entropy mirror map $\\psi : \\mathrm{int}(\\Delta_d) \\to \\mathbb{R}$ defined by $\\psi(p) = \\sum_{j=1}^d p_j \\log p_j$, where $\\log$ denotes the natural logarithm. Consider the convex log-loss $f(p) = - \\sum_{j=1}^d r_j \\log p_j$, where $r \\in \\Delta_d$ is a fixed label-distribution vector.\n\nTasks:\n1) Starting from core definitions only (gradient, convexity, and the definition of Bregman divergence), derive the gradient $\\nabla f(p)$ and the Bregman divergence $D_\\psi(p,q)$ associated with $\\psi$. Your derivation must begin from $f(p) = - \\sum_{j=1}^d r_j \\log p_j$ and the definition $D_\\psi(p,q) = \\psi(p) - \\psi(q) - \\langle \\nabla \\psi(q), p - q \\rangle$, and must not assume any specialized results about entropy without proof.\n2) Using first-order optimality conditions under the simplex constraint, determine the unique minimizer $p^\\star$ of $f(p)$ over $\\Delta_d$ and justify why it is unique.\n3) Derive mirror descent on $\\Delta_d$ with mirror map $\\psi$ for a general differentiable convex objective $f$, defined by the implicit update\n$$\n\\nabla \\psi(p_{t+1}) \\,=\\, \\nabla \\psi(p_t) \\,-\\, \\eta \\,\\nabla f(p_t),\n$$\nfollowed by enforcing $p_{t+1} \\in \\Delta_d$. Then specialize your update to the log-loss above to obtain an explicit coordinate-wise update for $p_{t+1}$ in terms of $p_t$, $r$, and the step size $\\eta > 0$. Your derivation must start from the implicit update and the definition of $\\nabla \\psi$ and should not assume any pre-known multiplicative form.\n4) Implement the specialized mirror descent in a program that:\n   - Initializes at a strictly positive $p_0 \\in \\mathrm{int}(\\Delta_d)$,\n   - Repeats the update for $T$ iterations with a constant step size $\\eta$,\n   - Returns the final iterate $p_T$ together with the final log-loss $f(p_T)$ and the Bregman divergence $D_\\psi(p_T, q)$ for a given $q \\in \\mathrm{int}(\\Delta_d)$.\n   - Uses the natural logarithm.\n   - Verifies numerically (within a tolerance) both that $p_T$ lies in the simplex and that $D_\\psi(p_T,q) \\ge 0$.\n   - Reports, in addition, the index of the maximum coordinate of $p_T$ using zero-based indexing.\n\nImplementation requirements:\n- Use only real arithmetic; no external data.\n- Ensure strict positivity during iteration; you may use a small numerical floor $\\varepsilon$ if needed for stability, provided you preserve feasibility by renormalization onto $\\Delta_d$.\n- Use the natural logarithm for all computations.\n- For booleans, use a tolerance of $\\tau = 10^{-10}$ to check the simplex sum constraint and nonnegativity of the divergence.\n\nTest suite:\nYour program must run the mirror descent procedure on the following three test cases:\n\n- Test case $1$ (happy path):\n  - $d = 3$,\n  - $p_0 = [\\,0.4,\\, 0.3,\\, 0.3\\,]$,\n  - $r = [\\,0,\\, 1,\\, 0\\,]$,\n  - $\\eta = 0.7$,\n  - $T = 5$,\n  - $q = [\\,0.2,\\, 0.5,\\, 0.3\\,]$.\n\n- Test case $2$ (soft labels and convergence behavior):\n  - $d = 3$,\n  - $p_0 = [\\,0.34,\\, 0.33,\\, 0.33\\,]$,\n  - $r = [\\,0.7,\\, 0.2,\\, 0.1\\,]$,\n  - $\\eta = 0.3$,\n  - $T = 20$,\n  - $q = [\\,\\frac{1}{3},\\, \\frac{1}{3},\\, \\frac{1}{3}\\,]$.\n\n- Test case $3$ (edge case with a very small initial coordinate):\n  - $d = 3$,\n  - $p_0 = [\\,0.499,\\, 0.499,\\, 0.002\\,]$,\n  - $r = [\\,0,\\, 0,\\, 1\\,]$,\n  - $\\eta = 0.01$,\n  - $T = 50$,\n  - $q = [\\,0.4,\\, 0.4,\\, 0.2\\,]$.\n\nRequired outputs per test case:\n- The final log-loss $f(p_T)$ as a float.\n- The Bregman divergence $D_\\psi(p_T, q)$ as a float.\n- The zero-based index of the largest coordinate of $p_T$ as an integer.\n- A boolean indicating whether $p_T$ lies in the simplex within tolerance $\\tau$, that is, whether $\\sum_j p_{T,j}$ differs from $1$ by at most $\\tau$ and all coordinates are at least $0$ within numerical tolerance.\n- A boolean indicating whether $D_\\psi(p_T,q) \\ge -\\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list with one entry per test case.\n- Each entry is itself a list in the order: $[\\,f(p_T),\\ D_\\psi(p_T,q),\\ \\mathrm{argmax\\_index},\\ \\mathrm{simplex\\_ok},\\ \\mathrm{div\\_ok}\\,]$.\n- The two floats must be rounded to $6$ decimal places.\n- Example of the required overall structure (illustrative only): $[[\\,0.123456,\\, 0.000000,\\, 2,\\, True,\\, True\\,],[\\,\\dots\\,],[\\,\\dots\\,]]$.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of convex optimization and statistical learning, well-posed, and all definitions and parameters are provided completely and consistently.\n\n### Task 1: Gradient Derivations\n\nThe first task is to derive the gradient of the log-loss function $f(p)$ and the Bregman divergence $D_\\psi(p,q)$ associated with the entropy mirror map $\\psi(p)$.\n\n**Gradient of the log-loss $f(p)$**\n\nThe log-loss function is given by $f(p) = - \\sum_{j=1}^d r_j \\log p_j$, where $r \\in \\Delta_d$ is a fixed vector and $p \\in \\mathrm{int}(\\Delta_d)$. The gradient, $\\nabla f(p)$, is a vector whose $k$-th component is the partial derivative of $f$ with respect to $p_k$.\n\n$$\n(\\nabla f(p))_k = \\frac{\\partial}{\\partial p_k} \\left( - \\sum_{j=1}^d r_j \\log p_j \\right)\n$$\n\nDue to the linearity of the derivative operator, we can move it inside the summation. The term $r_j \\log p_j$ only depends on $p_k$ when $j=k$. Therefore, the derivative is non-zero only for the $k$-th term of the sum.\n\n$$\n\\frac{\\partial}{\\partial p_k} (-r_j \\log p_j) = \\begin{cases} -r_k \\frac{1}{p_k} & \\text{if } j=k \\\\ 0 & \\text{if } j \\neq k \\end{cases}\n$$\n\nThus, the $k$-th component of the gradient is:\n\n$$\n(\\nabla f(p))_k = -\\frac{r_k}{p_k}\n$$\n\nThe full gradient vector is:\n\n$$\n\\nabla f(p) = \\begin{bmatrix} -r_1/p_1 \\\\ -r_2/p_2 \\\\ \\vdots \\\\ -r_d/p_d \\end{bmatrix}\n$$\n\n**Gradient of the Mirror Map $\\psi(p)$**\n\nThe entropy mirror map is $\\psi(p) = \\sum_{j=1}^d p_j \\log p_j$. To find its gradient, $\\nabla \\psi(p)$, we compute the partial derivative with respect to each component $p_k$.\n\n$$\n(\\nabla \\psi(p))_k = \\frac{\\partial}{\\partial p_k} \\left( \\sum_{j=1}^d p_j \\log p_j \\right)\n$$\n\nAgain, only the term where $j=k$ contributes to the derivative. We use the product rule for $\\frac{\\partial}{\\partial p_k}(p_k \\log p_k)$.\n\n$$\n\\frac{\\partial}{\\partial p_k}(p_k \\log p_k) = \\left(\\frac{\\partial p_k}{\\partial p_k}\\right) \\log p_k + p_k \\left(\\frac{\\partial (\\log p_k)}{\\partial p_k}\\right) = 1 \\cdot \\log p_k + p_k \\cdot \\frac{1}{p_k} = \\log p_k + 1\n$$\n\nSo, the $k$-th component of the gradient of $\\psi$ is $(\\nabla \\psi(p))_k = \\log p_k + 1$. The full gradient vector is:\n\n$$\n\\nabla \\psi(p) = \\begin{bmatrix} \\log p_1 + 1 \\\\ \\log p_2 + 1 \\\\ \\vdots \\\\ \\log p_d + 1 \\end{bmatrix}\n$$\n\n**Bregman Divergence $D_\\psi(p,q)$**\n\nThe Bregman divergence is defined as $D_\\psi(p,q) = \\psi(p) - \\psi(q) - \\langle \\nabla \\psi(q), p - q \\rangle$. We substitute the expressions for $\\psi$ and $\\nabla \\psi$ derived above.\n\n$$\nD_\\psi(p,q) = \\left(\\sum_{j=1}^d p_j \\log p_j\\right) - \\left(\\sum_{j=1}^d q_j \\log q_j\\right) - \\sum_{j=1}^d (\\log q_j + 1)(p_j - q_j)\n$$\n\nLet's expand the inner product term:\n\n$$\n\\sum_{j=1}^d (\\log q_j + 1)(p_j - q_j) = \\sum_{j=1}^d (p_j \\log q_j - q_j \\log q_j + p_j - q_j)\n$$\n\nSince both $p, q \\in \\Delta_d$, their components sum to $1$: $\\sum_{j=1}^d p_j = 1$ and $\\sum_{j=1}^d q_j = 1$. This implies $\\sum_{j=1}^d (p_j - q_j) = \\sum p_j - \\sum q_j = 1 - 1 = 0$. So the last part of the expanded sum vanishes.\n\n$$\n\\sum_{j=1}^d (\\log q_j + 1)(p_j - q_j) = \\sum_{j=1}^d (p_j \\log q_j - q_j \\log q_j)\n$$\n\nSubstituting this back into the definition of $D_\\psi(p,q)$:\n\n$$\nD_\\psi(p,q) = \\sum_{j=1}^d p_j \\log p_j - \\sum_{j=1}^d q_j \\log q_j - \\left( \\sum_{j=1}^d p_j \\log q_j - \\sum_{j=1}^d q_j \\log q_j \\right)\n$$\n\nThe terms involving $q_j \\log q_j$ cancel out.\n\n$$\nD_\\psi(p,q) = \\sum_{j=1}^d p_j \\log p_j - \\sum_{j=1}^d p_j \\log q_j = \\sum_{j=1}^d p_j (\\log p_j - \\log q_j)\n$$\n\nUsing the property of logarithms, $\\log a - \\log b = \\log(a/b)$, we arrive at the final form:\n\n$$\nD_\\psi(p,q) = \\sum_{j=1}^d p_j \\log \\frac{p_j}{q_j}\n$$\n\nThis is the well-known Kullback-Leibler (KL) divergence, $D_{KL}(p \\| q)$.\n\n### Task 2: Unique Minimizer of $f(p)$ over $\\Delta_d$\n\nWe seek to find $p^\\star = \\arg\\min_{p \\in \\Delta_d} f(p)$. The objective is $f(p) = - \\sum_{j=1}^d r_j \\log p_j$. This problem is equivalent to minimizing the cross-entropy between the distributions $r$ and $p$. We can relate this to the KL-divergence:\n\n$$\nD_{KL}(r \\| p) = \\sum_{j=1}^d r_j \\log \\frac{r_j}{p_j} = \\sum_{j=1}^d r_j \\log r_j - \\sum_{j=1}^d r_j \\log p_j = -H(r) + f(p)\n$$\n\nHere, $H(r) = -\\sum_j r_j \\log r_j$ is the entropy of the fixed distribution $r$ and is a constant. Therefore, minimizing $f(p)$ is equivalent to minimizing $D_{KL}(r \\| p)$.\n\nBy Gibbs' inequality, a fundamental property of KL-divergence, $D_{KL}(r \\| p) \\ge 0$ for any two probability distributions $r$ and $p$. Equality holds if and only if $p_j = r_j$ for all $j=1, \\dots, d$.\n\nSince we are minimizing $f(p)$ over the set $\\Delta_d$, which contains $r$, the minimum value is achieved when $D_{KL}(r \\| p) = 0$. This occurs precisely at $p=r$. Thus, the unique minimizer is $p^\\star = r$.\n\nThis argument also establishes uniqueness. Since $f(p)$ is strictly convex on the support of $r$ and the feasible set $\\Delta_d$ is convex, there can be only one global minimizer.\n\n### Task 3: Mirror Descent Update Rule\n\nThe mirror descent algorithm is defined by the implicit update $\\nabla \\psi(p_{t+1}) = \\nabla \\psi(p_t) - \\eta \\nabla f(p_t)$, followed by ensuring $p_{t+1} \\in \\Delta_d$.\n\nLet's write this update out for each coordinate $j=1, \\dots, d$:\n\n$$\n(\\nabla \\psi(p_{t+1}))_j = (\\nabla \\psi(p_t))_j - \\eta (\\nabla f(p_t))_j\n$$\n\nUsing our derived gradient for $\\psi$, $(\\nabla \\psi(p))_j = \\log p_j + 1$:\n\n$$\n\\log p_{t+1, j} + 1 = (\\log p_{t, j} + 1) - \\eta (\\nabla f(p_t))_j\n$$\n\nThe constant $1$ cancels from both sides:\n\n$$\n\\log p_{t+1, j} = \\log p_{t, j} - \\eta (\\nabla f(p_t))_j\n$$\n\nExponentiating both sides gives an explicit update for a temporary vector $\\tilde{p}_{t+1}$ whose components are not yet guaranteed to sum to $1$:\n\n$$\n\\tilde{p}_{t+1, j} = \\exp(\\log p_{t, j} - \\eta (\\nabla f(p_t))_j) = p_{t, j} \\exp(-\\eta (\\nabla f(p_t))_j)\n$$\n\nNow, we specialize this general mirror descent update for our specific log-loss objective $f(p)$. We substitute our result $(\\nabla f(p_t))_j = -r_j / p_{t, j}$:\n\n$$\n\\tilde{p}_{t+1, j} = p_{t, j} \\exp\\left(-\\eta \\left(-\\frac{r_j}{p_{t, j}}\\right)\\right) = p_{t, j} \\exp\\left(\\frac{\\eta r_j}{p_{t, j}}\\right)\n$$\n\nThis update is performed for each coordinate $j$. The resulting vector $\\tilde{p}_{t+1}$ will have all positive components (since $p_{t,j}>0$ and the exponential is always positive), but will not in general sum to $1$. To enforce the constraint $p_{t+1} \\in \\Delta_d$, we normalize the vector by its sum. Let $Z_t = \\sum_{k=1}^d \\tilde{p}_{t+1, k}$. The final, explicit, coordinate-wise update is:\n\n$$\np_{t+1, j} = \\frac{\\tilde{p}_{t+1, j}}{Z_t} = \\frac{p_{t, j} \\exp\\left(\\frac{\\eta r_j}{p_{t, j}}\\right)}{\\sum_{k=1}^d p_{t, k} \\exp\\left(\\frac{\\eta r_k}{p_{t, k}}\\right)}\n$$\n\nThis update rule, known as the exponentiated gradient update, ensures that if $p_t \\in \\mathrm{int}(\\Delta_d)$, then $p_{t+1}$ will also be in $\\mathrm{int}(\\Delta_d)$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes the mirror descent algorithm for a set of test cases\n    and formats the output as per the problem specification.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"d\": 3,\n            \"p0\": np.array([0.4, 0.3, 0.3]),\n            \"r\": np.array([0.0, 1.0, 0.0]),\n            \"eta\": 0.7,\n            \"T\": 5,\n            \"q\": np.array([0.2, 0.5, 0.3]),\n        },\n        # Test case 2 (soft labels and convergence behavior)\n        {\n            \"d\": 3,\n            \"p0\": np.array([0.34, 0.33, 0.33]),\n            \"r\": np.array([0.7, 0.2, 0.1]),\n            \"eta\": 0.3,\n            \"T\": 20,\n            \"q\": np.array([1/3, 1/3, 1/3]),\n        },\n        # Test case 3 (edge case with a very small initial coordinate)\n        {\n            \"d\": 3,\n            \"p0\": np.array([0.499, 0.499, 0.002]),\n            \"r\": np.array([0.0, 0.0, 1.0]),\n            \"eta\": 0.01,\n            \"T\": 50,\n            \"q\": np.array([0.4, 0.4, 0.2]),\n        },\n    ]\n\n    tau = 1e-10\n    results = []\n\n    for case in test_cases:\n        p = case[\"p0\"].copy()\n        r = case[\"r\"]\n        eta = case[\"eta\"]\n        T = case[\"T\"]\n        q = case[\"q\"]\n\n        for _ in range(T):\n            # To prevent division by zero or large exponent in cases where p_j is extremely small,\n            # we add a small epsilon. While the problem setup avoids this, it's robust practice.\n            # The derived update is p_j * exp(eta * r_j / p_j).\n            # We use a safe denominator.\n            denominators = p + 1e-30 # Add a tiny constant for numerical stability\n            \n            # The exponent in the update rule can become very large if p_j is small.\n            # It's better to compute log of unnormalized p, then exponentiate sum.\n            # log_p_unnorm_j = log(p_j) + eta * r_j / p_j\n            log_p_unnorm = np.log(p) + eta * r / denominators\n\n            # Renormalize in log-space to prevent overflow with large exponents before exponentiating\n            # log(Z) = log(sum(exp(log_p_unnorm_j)))\n            # A numerically stable way to compute log(sum(exp(x))) is logsumexp\n            max_log = np.max(log_p_unnorm)\n            log_Z = max_log + np.log(np.sum(np.exp(log_p_unnorm - max_log)))\n            \n            # p_{t+1, j} = exp(log_p_unnorm_j - log(Z))\n            log_p = log_p_unnorm - log_Z\n            p = np.exp(log_p)\n\n        p_T = p\n\n        # 1. Calculate final log-loss f(p_T)\n        # The sum is only over j where r_j > 0 to handle 0 * log(x) terms correctly.\n        active_indices = r > 0\n        if np.any(active_indices):\n            f_pT = -np.sum(r[active_indices] * np.log(p_T[active_indices]))\n        else:\n            f_pT = 0.0\n\n        # 2. Calculate Bregman divergence D_psi(p_T, q), which is KL(p_T || q)\n        # D = sum(p_T_j * log(p_T_j / q_j))\n        # Ensure positivity of arguments to log, although handled by problem setup.\n        p_T_safe = np.maximum(p_T, 1e-30)\n        q_safe = np.maximum(q, 1e-30)\n        D_psi_pT_q = np.sum(p_T * (np.log(p_T_safe) - np.log(q_safe)))\n\n        # 3. Find index of the largest coordinate\n        argmax_index = np.argmax(p_T)\n\n        # 4. Check if p_T is in the simplex\n        is_in_simplex = (abs(np.sum(p_T) - 1.0) <= tau) and np.all(p_T >= 0.0)\n        \n        # 5. Check non-negativity of divergence\n        is_div_non_negative = D_psi_pT_q >= -tau\n\n        results.append([\n            round(f_pT, 6),\n            round(D_psi_pT_q, 6),\n            int(argmax_index),\n            bool(is_in_simplex),\n            bool(is_div_non_negative)\n        ])\n\n    # Format the final output string exactly as required\n    formatted_results = []\n    for res in results:\n        # Convert each item in the inner list to its string representation\n        # Floats are formatted, others are converted directly.\n        f_val_str = f\"{res[0]:.6f}\"\n        d_val_str = f\"{res[1]:.6f}\"\n        idx_str = str(res[2])\n        simp_ok_str = str(res[3])\n        div_ok_str = str(res[4])\n        formatted_results.append(f\"[{f_val_str},{d_val_str},{idx_str},{simp_ok_str},{div_ok_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3126055"}]}