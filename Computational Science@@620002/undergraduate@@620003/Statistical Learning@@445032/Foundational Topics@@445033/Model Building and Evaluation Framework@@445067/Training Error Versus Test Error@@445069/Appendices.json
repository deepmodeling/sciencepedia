{"hands_on_practices": [{"introduction": "The K-Nearest Neighbors ($k$-NN) algorithm is one of the most intuitive classifiers, making predictions based on the 'votes' of the closest training samples. This simplicity, however, provides a powerful lesson in the dangers of using training error as a performance metric. In this exercise [@problem_id:3135589], you will see firsthand how a $1$-NN classifier perfectly memorizes the training data, achieving zero training error, and then discover through leave-one-out cross-validation why this apparent perfection is misleading and often indicates poor generalization.", "problem": "You are given a classification setting with a finite training dataset, where each observation is a pair $(\\mathbf{x}_i, y_i)$ with feature vector $\\mathbf{x}_i \\in \\mathbb{R}^d$ and class label $y_i \\in \\{0,1\\}$, for $i = 1, \\dots, n$. The fundamental base consists of: (i) the definition of the Euclidean distance $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$, (ii) the $k$-nearest neighbors rule that selects the $k$ training observations with the smallest distances to the query point, (iii) the empirical risk (resubstitution error) defined under $0$-$1$ loss as the average number of misclassifications made when each training point is predicted using a specified classifier and the full training set, and (iv) leave-one-out cross-validation that evaluates the classifier by, for each training point, predicting it using the training set with that point removed, and averaging the resulting $0$-$1$ losses. In all computations, ties in distances must be broken deterministically: when ordering neighbors by distance, sort by ascending distance, then prioritize the point itself if present, and finally break any remaining ties by ascending index. For majority voting in classification when $k \\ge 2$, if there is a tie in class counts among the $k$ neighbors, select the smallest class label.\n\nTask A: Starting from the definitions above, confirm that the empirical risk (resubstitution error) for the $k=1$ nearest neighbor classifier is $0$ when the neighbor set for each training point includes the point itself. Then, compute the leave-one-out error for $k=1$ and compare it to the resubstitution error for $k=2$ on the datasets specified below. The comparison must be based on numerical values of the error rates.\n\nTask B: Based on the observed numerical comparison, analyze the implications for model selection between $k=1$ and $k=2$ using resubstitution error versus leave-one-out error, grounding your discussion in the definitions provided (your analysis is to be presented in your solution, not printed by the code).\n\nUse the following test suite of datasets, each given by a matrix of features and a vector of labels. All features are two-dimensional, $d = 2$, and all class labels are in $\\{0,1\\}$. Distances are Euclidean. There are no physical units involved.\n\n- Dataset $1$ (well-separated classes):\n  - Features $X_1$: $\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$\n  - Labels $y_1$: $[0, 0, 1, 1]$\n- Dataset $2$ (overlap near the boundary):\n  - Features $X_2$: $\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$\n  - Labels $y_2$: $[0, 0, 1]$\n- Dataset $3$ (mixture with a noisy point):\n  - Features $X_3$: $\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$\n  - Labels $y_3$: $[0, 0, 1, 1, 0]$\n\nFor each dataset $j \\in \\{1,2,3\\}$, compute:\n- $E_{j}^{\\text{train}, k=1}$: the empirical risk (resubstitution error) for $k=1$ including the point itself among neighbors,\n- $E_{j}^{\\text{LOO}, k=1}$: the leave-one-out error for $k=1$ excluding the point itself,\n- $E_{j}^{\\text{train}, k=2}$: the empirical risk (resubstitution error) for $k=2$ including the point itself among neighbors with majority vote and the specified tie-breaking.\n\nYour program should produce a single line of output containing a list of three lists, one per dataset, where each inner list is $[E_{j}^{\\text{train}, k=1}, E_{j}^{\\text{LOO}, k=1}, E_{j}^{\\text{train}, k=2}]$. The format must be a comma-separated list enclosed in square brackets, for example: \"[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]\". All values must be floating-point numbers in $[0,1]$ representing average $0$-$1$ loss over the training points for the specified evaluation setting.", "solution": "The problem statement is evaluated for validity.\n\n### Step 1: Extract Givens\n- **Dataset**: A finite training dataset of $n$ observations, each being a pair $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ is a feature vector and $y_i \\in \\{0,1\\}$ is a class label, for $i = 1, \\dots, n$.\n- **Distance Metric**: Euclidean distance, $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$.\n- **Classifier Rule**: The $k$-nearest neighbors ($k$-NN) rule selects the $k$ training observations with the smallest distances to a query point.\n- **Error Metrics**:\n    - **Empirical risk (resubstitution error)** under $0$-$1$ loss is the average number of misclassifications when each training point is predicted using the classifier trained on the full training set.\n    - **Leave-one-out cross-validation (LOOCV) error** is the average $0$-$1$ loss where each training point is predicted using the classifier trained on the set of all other points.\n- **Tie-Breaking Rules**:\n    - **Distances**: Order neighbors by ascending distance. If distances are tied, the point itself is prioritized if present. Any remaining ties are broken by ascending index $i$.\n    - **Majority Vote**: For classification with $k \\ge 2$, if there is a tie in class counts among the $k$ neighbors, the smallest class label (i.e., class $0$) is selected.\n- **Tasks**:\n    - **Task A**: Confirm that the empirical risk for $k=1$ is $0$. Compute the LOOCV error for $k=1$ and the empirical risk for $k=2$ for three given datasets.\n    - **Task B**: Analyze the numerical results with respect to model selection.\n- **Datasets**: Three datasets are provided, each with $d=2$ and $y_i \\in \\{0,1\\}$.\n    - Dataset $1$: $X_1 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$.\n    - Dataset $2$: $X_2 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$.\n    - Dataset $3$: $X_3 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$, $y_3 = [0, 0, 1, 1, 0]$.\n- **Computations Required**: For each dataset $j \\in \\{1,2,3\\}$, compute $E_{j}^{\\text{train}, k=1}$, $E_{j}^{\\text{LOO}, k=1}$, and $E_{j}^{\\text{train}, k=2}$.\n- **Output Format**: A list of three lists: `[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on fundamental and standard concepts in statistical learning and pattern recognition, namely the $k$-NN algorithm, Euclidean distance, and standard methods for error estimation (resubstitution and cross-validation). All definitions are correct and standard.\n- **Well-Posed**: The problem is well-posed. It provides all necessary data, explicit algorithms, and deterministic tie-breaking rules, ensuring that a unique solution exists and is computable. The tasks are specific and quantifiable.\n- **Objective**: The problem is stated in precise, objective, and formal mathematical language, free from subjectivity or ambiguity.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a well-defined computational and analytical exercise in the field of statistical learning.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\n---\n\n### Task A: Confirmation and Computation\n\n**Part 1: Confirmation of Zero Empirical Risk for $k=1$**\n\nThe empirical risk, or resubstitution error, for a classifier is calculated by applying the classifier to each point $\\mathbf{x}_i$ in the training set and measuring the prediction error against the known label $y_i$. For the $k=1$ nearest neighbor classifier, we must find the single closest point to $\\mathbf{x}_i$ within the full training set $\\{(\\mathbf{x}_j, y_j)\\}_{j=1}^n$.\n\nBy definition, the Euclidean distance of a point to itself is $d(\\mathbf{x}_i, \\mathbf{x}_i) = 0$. For any other distinct point $\\mathbf{x}_j$ (where $j \\ne i$), the distance $d(\\mathbf{x}_i, \\mathbf{x}_j)$ must be greater than $0$. Consequently, the nearest neighbor to any training point $\\mathbf{x}_i$ is always the point $\\mathbf{x}_i$ itself. The problem's tie-breaking rules explicitly state to prioritize the point itself if present, reinforcing this conclusion.\n\nThe $1$-NN prediction for the label of $\\mathbf{x}_i$, denoted $\\hat{y}_i$, is thus the label of its nearest neighbor, which is $y_i$. The $0$-$1$ loss for this prediction is $L_{0-1}(\\hat{y}_i, y_i) = L_{0-1}(y_i, y_i) = 0$. Since the prediction is correct for every point in the training set, the total number of errors is $0$. The empirical risk, being the average loss, is therefore $E^{\\text{train}, k=1} = \\frac{1}{n} \\sum_{i=1}^n 0 = 0$. This holds universally for any dataset where all training points are unique. Therefore, for all datasets $j \\in \\{1,2,3\\}$, $E_{j}^{\\text{train}, k=1} = 0$.\n\n**Part 2: Numerical Computations**\n\nWe now compute $E_{j}^{\\text{LOO}, k=1}$ and $E_{j}^{\\text{train}, k=2}$ for each dataset.\n\n**Dataset 1**: $X_1 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$, $n=4$.\n- **$E_{1}^{\\text{LOO}, k=1}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Nearest point in $\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$ is $\\mathbf{x}_2$ (distance $1.0$). Prediction $\\hat{y}_1=y_2=0$. Correct.\n    - For $\\mathbf{x}_2=(0.0, 1.0)$, $y_2=0$: Nearest point in $\\{\\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4\\}$ is $\\mathbf{x}_1$ (distance $1.0$). Prediction $\\hat{y}_2=y_1=0$. Correct.\n    - For $\\mathbf{x}_3=(5.0, 5.0)$, $y_3=1$: Nearest point in $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\}$ is $\\mathbf{x}_4$ (distance $1.0$). Prediction $\\haty}_3=y_4=1$. Correct.\n    - For $\\mathbf{x}_4=(5.0, 6.0)$, $y_4=1$: Nearest point in $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\}$ is $\\mathbf{x}_3$ (distance $1.0$). Prediction $\\hat{y}_4=y_3=1$. Correct.\n    Total errors: $0$. $E_{1}^{\\text{LOO}, k=1} = 0/4 = 0.0$.\n- **$E_{1}^{\\text{train}, k=2}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Neighbors are itself and $\\mathbf{x}_2$. Labels are $\\{0, 0\\}$. Majority vote is $0$. Correct.\n    - For $\\mathbf{x}_2=(0.0, 1.0)$, $y_2=0$: Neighbors are itself and $\\mathbf{x}_1$. Labels are $\\{0, 0\\}$. Majority vote is $0$. Correct.\n    - For $\\mathbf{x}_3=(5.0, 5.0)$, $y_3=1$: Neighbors are itself and $\\mathbf{x}_4$. Labels are $\\{1, 1\\}$. Majority vote is $1$. Correct.\n    - For $\\mathbf{x}_4=(5.0, 6.0)$, $y_4=1$: Neighbors are itself and $\\mathbf{x}_3$. Labels are $\\{1, 1\\}$. Majority vote is $1$. Correct.\n    Total errors: $0$. $E_{1}^{\\text{train}, k=2} = 0/4 = 0.0$.\n\nResult for Dataset 1: $[0.0, 0.0, 0.0]$.\n\n**Dataset 2**: $X_2 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$, $n=3$.\n- **$E_{2}^{\\text{LOO}, k=1}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Distances to $\\mathbf{x}_2, \\mathbf{x}_3$ are $0.1, 0.05$. Nearest is $\\mathbf{x}_3$. Prediction $\\hat{y}_1=y_3=1$. Error.\n    - For $\\mathbf{x}_2=(0.0, 0.1)$, $y_2=0$: Distances to $\\mathbf{x}_1, \\mathbf{x}_3$ are $0.1, \\sqrt{0.05^2+0.1^2} \\approx 0.112$. Nearest is $\\mathbf{x}_1$. Prediction $\\hat{y}_2=y_1=0$. Correct.\n    - For $\\mathbf{x}_3=(0.05, 0.0)$, $y_3=1$: Distances to $\\mathbf{x}_1, \\mathbf{x}_2$ are $0.05, \\sqrt{0.05^2+0.1^2} \\approx 0.112$. Nearest is $\\mathbf{x}_1$. Prediction $\\hat{y}_3=y_1=0$. Error.\n    Total errors: $2$. $E_{2}^{\\text{LOO}, k=1} = 2/3$.\n- **$E_{2}^{\\text{train}, k=2}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_3$ (label $1$). Tie. Prediction is $0$. Correct.\n    - For $\\mathbf{x}_2=(0.0, 0.1)$, $y_2=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_1$ (label $0$). Majority vote is $0$. Correct.\n    - For $\\mathbf{x}_3=(0.05, 0.0)$, $y_3=1$: Neighbors are itself (label $1$) and $\\mathbf{x}_1$ (label $0$). Tie. Prediction is $0$. Error.\n    Total errors: $1$. $E_{2}^{\\text{train}, k=2} = 1/3$.\n\nResult for Dataset 2: $[0.0, 2/3, 1/3]$.\n\n**Dataset 3**: $X_3 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$, $y_3=[0, 0, 1, 1, 0]$, $n=5$.\n- **$E_{3}^{\\text{LOO}, k=1}$**:\n    - For $\\mathbf{x}_1=(0,0), y_1=0$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{1.1^2+1^2} \\approx 1.487$). Pred $\\hat{y}_1=y_5=0$. Correct.\n    - For $\\mathbf{x}_2=(0,2), y_2=0$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{1.1^2+(-1)^2} \\approx 1.487$). Pred $\\hat{y}_2=y_5=0$. Correct.\n    - For $\\mathbf{x}_3=(2,0), y_3=1$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{(2-1.1)^2+(0-1)^2} \\approx 1.345$). Pred $\\hat{y}_3=y_5=0$. Error.\n    - For $\\mathbf{x}_4=(2,2), y_4=1$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{(2-1.1)^2+(2-1)^2} \\approx 1.345$). Pred $\\hat{y}_4=y_5=0$. Error.\n    - For $\\mathbf{x}_5=(1.1,1), y_5=0$: Nearest is $\\mathbf{x}_3$ (dist $\\approx 1.345$). Pred $\\hat{y}_5=y_3=1$. Error.\n    Total errors: $3$. $E_{3}^{\\text{LOO}, k=1} = 3/5 = 0.6$.\n- **$E_{3}^{\\text{train}, k=2}$**:\n    - For $\\mathbf{x}_1=(0,0), y_1=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_5$ (label $0$). Majority vote $0$. Correct.\n    - For $\\mathbf{x}_2=(0,2), y_2=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_5$ (label $0$). Majority vote $0$. Correct.\n    - For $\\mathbf{x}_3=(2,0), y_3=1$: Neighbors are itself (label $1$) and $\\mathbf{x}_5$ (label $0$). Tie. Pred $0$. Error.\n    - For $\\mathbf{x}_4=(2,2), y_4=1$: Neighbors are itself (label $1$) and $\\mathbf{x}_5$ (label $0$). Tie. Pred $0$. Error.\n    - For $\\mathbf{x}_5=(1.1,1), y_5=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_3$ (label $1$). Tie. Pred $0$. Correct.\n    Total errors: $2$. $E_{3}^{\\text{train}, k=2} = 2/5 = 0.4$.\n\nResult for Dataset 3: $[0.0, 0.6, 0.4]$.\n\n### Task B: Analysis of Implications for Model Selection\n\nThe computed error rates are:\n- Dataset $1$: $[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.0, 0.0]$\n- Dataset $2$: $[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.667, 0.333]$\n- Dataset $3$: $[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.6, 0.4]$\n\nThese results reveal critical properties of the error estimation methods and their implications for model selection, specifically for choosing the hyperparameter $k$.\n\n1.  **Resubstitution Error is Overly Optimistic and a Poor Metric**: As confirmed analytically and shown numerically, the resubstitution error for $k=1$, $E^{\\text{train}, k=1}$, is always $0$. If a practitioner were to select $k$ by minimizing the resubstitution error, they would invariably choose $k=1$, as it achieves a perfect score. This choice corresponds to a model that has zero training error because it has memorized the training data. This phenomenon is a classic example of overfitting. The model has high variance and is sensitive to every single data point, including noise, but its training error gives a misleadingly optimistic assessment of its performance on new, unseen data.\n\n2.  **Leave-One-Out Cross-Validation Provides a More Realistic Assessment**: The leave-one-out error, $E^{\\text{LOO}, k=1}$, gives a more honest (less biased) estimate of the generalization error. For Dataset $1$, where classes are perfectly separated, LOOCV correctly identifies that a $1$-NN classifier performs flawlessly ($E_{1}^{\\text{LOO}, k=1}=0.0$). However, for Datasets $2$ and $3$, which feature class overlap and a noisy point respectively, the LOOCV error for $k=1$ is substantial ($0.667$ and $0.6$). This high error correctly signals that the $k=1$ model is not robust and overfits the training data. In noisy regions, a point's single nearest neighbor is likely to be from the wrong class, causing LOOCV to report high error.\n\n3.  **Comparing Model Complexities**: Choosing $k$ involves a bias-variance trade-off. A small $k$ (like $k=1$) results in a a low-bias, high-variance model with a very complex decision boundary. A larger $k$ increases bias but decreases variance, resulting in a smoother, less complex decision boundary. The severe performance degradation of $k=1$ under LOOCV in Datasets $2$ and $3$ indicates that its high variance leads to poor generalization.\n\n4.  **Interpretation of $E^{\\text{train}, k=2}$**: The resubstitution error for $k=2$ is non-zero for the noisy datasets. This is because the prediction is an average over two points. For instance, in Dataset $3$, when classifying point $\\mathbf{x}_3$ (label $1$), its neighbors are itself (label $1$) and the noisy point $\\mathbf{x}_5$ (label $0$). The resulting tie is broken by choosing label $0$, causing a misclassification even on the training data. While $E^{\\text{train}, k=2}$ is still an optimistically biased estimate, it is more informative than the vacuous $E^{\\text{train}, k=1}=0$. The comparison required by the problem, $E^{\\text{LOO}, k=1}$ versus $E^{\\text{train}, k=2}$, is not a methodologically sound way to choose between $k=1$ and $k=2$, as it compares a nearly unbiased error estimate for one model with a biased estimate for another. A proper comparison would be between $E^{\\text{LOO}, k=1}$ and $E^{\\text{LOO}, k=2}$. Nonetheless, the results ($0.667$ vs $0.333$ and $0.6$ vs $0.4$) illustrate a crucial point: an unstable model ($k=1$) may have a very high generalization error, potentially even higher than the (optimistic) training error of a more stable model ($k=2$).\n\nIn conclusion, for model selection in $k$-NN, resubstitution error is a flawed metric, particularly for small $k$. It systematically favors models that overfit. Cross-validation methods like LOOCV provide far more reliable estimates of a model's ability to generalize, and are essential for tuning hyperparameters like $k$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes k-NN error rates for three datasets as specified in the problem.\n    \"\"\"\n    datasets = [\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [5.0, 5.0], [5.0, 6.0]]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.1], [0.05, 0.0]]),\n            np.array([0, 0, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 2.0], [2.0, 0.0], [2.0, 2.0], [1.1, 1.0]]),\n            np.array([0, 0, 1, 1, 0])\n        )\n    ]\n\n    all_results = []\n\n    for X, y in datasets:\n        n, d = X.shape\n        \n        # Task 1: Empirical risk for k=1 (E_train, k=1)\n        # This is analytically guaranteed to be 0, as each point is its own nearest neighbor.\n        e_train_k1 = 0.0\n        \n        # Task 2: Leave-one-out error for k=1 (E_LOO, k=1)\n        loo_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            min_dist = np.inf\n            best_idx = -1\n            \n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                \n                if dist < min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j < best_idx:\n                        best_idx = j\n            \n            y_pred = y[best_idx]\n            if y_pred != y_true:\n                loo_errors += 1\n        \n        e_loo_k1 = loo_errors / n\n\n        # Task 3: Empirical risk for k=2 (E_train, k=2)\n        train_k2_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            # The first neighbor is the point itself.\n            neighbor1_label = y_true\n            \n            # Find the second neighbor (closest among other points).\n            min_dist = np.inf\n            best_idx = -1\n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                if dist < min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j < best_idx:\n                        best_idx = j\n            \n            neighbor2_label = y[best_idx]\n            \n            # Majority vote with tie-breaking\n            if neighbor1_label == neighbor2_label:\n                y_pred = neighbor1_label\n            else:  # Tie in votes (one for each class)\n                y_pred = 0  # Select smallest class label\n                \n            if y_pred != y_true:\n                train_k2_errors += 1\n        \n        e_train_k2 = train_k2_errors / n\n        \n        all_results.append([e_train_k1, e_loo_k1, e_train_k2])\n\n    # Format the final output string as specified.\n    outer_list_str = []\n    for res in all_results:\n        inner_list_str = f\"[{','.join(map(str, res))}]\"\n        outer_list_str.append(inner_list_str)\n    \n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```", "id": "3135589"}, {"introduction": "After seeing how easily a model can overfit, we now turn to a powerful technique for preventing it: regularization. Ridge regression combats the high variance of standard linear models by adding a penalty term that discourages overly large coefficients. This practice [@problem_id:3188165] invites you to trace the \"regularization path,\" observing how the training and test errors evolve as we increase the regularization strength, $\\lambda$. You will computationally verify the core principle of the bias-variance trade-off: a small increase in training error can lead to a significant improvement in test performance.", "problem": "You are given a well-defined comparison task of training error versus test error along a regularization path for ridge regression, formulated entirely in terms of empirical risk minimization with squared loss. For each test case, you will construct deterministic design matrices and responses using trigonometric functions with angles measured in radians, compute the ridge regression estimator along a prescribed grid of regularization strengths, and report the regularization value that minimizes the test error along with a check that the corresponding training error is not the minimum training error on the grid.\n\nBase definitions to be used:\n- Let $X \\in \\mathbb{R}^{n \\times d}$ be a design matrix and $y \\in \\mathbb{R}^{n}$ be a response vector. Ridge regression with squared loss is defined as the solution $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{d}$ of the optimization problem\n$$\n\\hat{\\beta}_{\\lambda} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\\right\\}.\n$$\n- For $\\lambda = 0$, this reduces to ordinary least squares empirical risk minimization. If the normal equations admit multiple solutions (for example, when $n < d$), use the Moore–Penrose pseudoinverse to obtain the minimum Euclidean norm solution that minimizes the empirical risk.\n- For any estimator $\\hat{\\beta}$, the training mean squared error (Mean Squared Error (MSE)) is\n$$\nR_{\\text{train}}(\\hat{\\beta}) = \\frac{1}{n}\\|y - X\\hat{\\beta}\\|_{2}^{2},\n$$\nand for a test set $(X_{\\text{test}}, y_{\\text{test}})$ with $m$ samples, the test mean squared error is\n$$\nR_{\\text{test}}(\\hat{\\beta}) = \\frac{1}{m}\\|y_{\\text{test}} - X_{\\text{test}}\\hat{\\beta}\\|_{2}^{2}.\n$$\n\nTask:\n- For each provided test case, compute $\\hat{\\beta}_{\\lambda}$ for every $\\lambda$ in the regularization grid\n$$\n\\Lambda = [\\,0,\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,1,\\,10,\\,100\\,].\n$$\n- For each $\\lambda \\in \\Lambda$, compute $R_{\\text{train}}(\\hat{\\beta}_{\\lambda})$ and $R_{\\text{test}}(\\hat{\\beta}_{\\lambda})$.\n- Identify the index $k^{\\star}$ of $\\lambda^{\\star} \\in \\Lambda$ that minimizes $R_{\\text{test}}(\\hat{\\beta}_{\\lambda})$. If there are ties, select the smallest index (that is, the first occurrence).\n- Determine whether the training error at $\\lambda^{\\star}$ is strictly larger than the minimum training error on the grid, that is, whether\n$$\nR_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}}) > \\min_{\\lambda \\in \\Lambda} R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) + \\varepsilon,\n$$\nwith a numerical tolerance $\\varepsilon = 10^{-12}$.\n\nAngle unit: All trigonometric functions use inputs in radians.\n\nFinal output formatting:\n- For each test case output a list $[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$, where:\n  - $k^{\\star}$ is an integer index into $\\Lambda$,\n  - $\\lambda^{\\star}$ is a float rounded to $6$ decimal places,\n  - $R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}})$ is a float rounded to $6$ decimal places,\n  - $\\text{train\\_nonmin}$ is a boolean indicating whether $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})$ is strictly greater than the minimum training MSE on the grid by at least $\\varepsilon$.\n- Aggregate the results for all test cases into a single line printed as a comma-separated list enclosed in square brackets (for example, $[\\text{result\\_1},\\text{result\\_2},\\text{result\\_3}]$), where each $\\text{result\\_i}$ is itself a list as described above.\n\nTest suite:\nConstruct three deterministic test cases by the following rules. All angles are in radians.\n\n- Test case $1$ (underdetermined, $n < d$, noisier):\n  - Training set sizes: $n = 8$, $d = 12$. Test set size: $m = 200$.\n  - For $i \\in \\{0,1,\\dots,n-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    X_{ij} = \\sin\\big((i+1)(j+1)\\big) + 0.1 \\cos(i - 2j).\n    $$\n  - For $i \\in \\{0,1,\\dots,m-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    (X_{\\text{test}})_{ij} = \\cos\\left(\\frac{(i+1)(j+1)}{2}\\right) + 0.1 \\sin(i + j).\n    $$\n  - True parameter for this case: for $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    (\\beta^{\\star})_{j} = (-1)^{j} \\cdot \\frac{j+1}{d}.\n    $$\n  - Noise:\n    $$\n    \\varepsilon_{i} = 0.3 \\cdot \\sin(3i + 1) \\quad \\text{for } i \\in \\{0,\\dots,n-1\\}, \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.3 \\cdot \\cos(5i + 2) \\quad \\text{for } i \\in \\{0,\\dots,m-1\\}.\n    $$\n  - Responses: $y = X \\beta^{\\star} + \\varepsilon$ and $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$.\n\n- Test case $2$ (overdetermined with strong collinearity, moderate noise):\n  - Training set sizes: $n = 40$, $d = 10$. Test set size: $m = 200$.\n  - For $i \\in \\{0,1,\\dots,n-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    X_{ij} = \\sin\\big(0.2(i+1)\\big) + 0.01 \\cos\\big(0.3(i+1) + 0.1(j+1)\\big).\n    $$\n  - For $i \\in \\{0,1,\\dots,m-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    (X_{\\text{test}})_{ij} = \\sin\\big(0.21(i+1)\\big) + 0.01 \\cos\\big(0.31(i+1) + 0.11(j+1)\\big).\n    $$\n  - True parameter for this case: for $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    (\\beta^{\\star})_{j} = \\frac{1}{j+1}.\n    $$\n  - Noise:\n    $$\n    \\varepsilon_{i} = 0.2 \\cdot \\cos\\big(0.5(i+1)\\big), \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.2 \\cdot \\sin\\big(0.4(i+1)\\big).\n    $$\n  - Responses: $y = X \\beta^{\\star} + \\varepsilon$ and $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$.\n\n- Test case $3$ (well-determined, low noise):\n  - Training set sizes: $n = 200$, $d = 5$. Test set size: $m = 200$.\n  - For $i \\in \\{0,1,\\dots,n-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    X_{ij} = \\sin\\left(\\frac{(i+1)(j+2)}{7}\\right) + 0.2 \\cos\\big((i+1) + 0.3(j+1)\\big).\n    $$\n  - For $i \\in \\{0,1,\\dots,m-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    (X_{\\text{test}})_{ij} = \\sin\\left(\\frac{(i+1)(j+2)}{7.3}\\right) + 0.2 \\cos\\big((i+1) + 0.33(j+1)\\big).\n    $$\n  - True parameter for this case: for $j \\in \\{0,1,\\dots,d-1\\}$,\n    $$\n    (\\beta^{\\star})_{j} = (-0.7)^{j}.\n    $$\n  - Noise:\n    $$\n    \\varepsilon_{i} = 0.02 \\cdot \\sin\\big(0.1(i+1) + 0.3\\big), \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.02 \\cdot \\cos\\big(0.12(i+1) - 0.1\\big).\n    $$\n  - Responses: $y = X \\beta^{\\star} + \\varepsilon$ and $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$.\n\nImplementation requirements:\n- For $\\lambda > 0$, obtain $\\hat{\\beta}_{\\lambda}$ by solving the first-order optimality condition for the ridge objective.\n- For $\\lambda = 0$, obtain a minimum Euclidean norm solution that minimizes empirical risk using the Moore–Penrose pseudoinverse.\n- Use the numerical tolerance $\\varepsilon = 10^{-12}$ when comparing real numbers for strict inequality in the training-error-not-minimum check.\n- Round all floats in the output to $6$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$ for one test case in the order of cases $1, 2, 3$.", "solution": "The problem requires a comparative analysis of training and test errors for ridge regression across a specified grid of regularization parameters, $\\Lambda$. This is a classic exercise in statistical learning that demonstrates the bias-variance trade-off. A deterministic procedure is provided to construct the training and test data for three distinct scenarios.\n\nThe core of the problem lies in solving the ridge regression optimization problem for an estimator $\\hat{\\beta}_{\\lambda}$:\n$$\n\\hat{\\beta}_{\\lambda} = \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\\right\\}\n$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix, $y \\in \\mathbb{R}^{n}$ is the response vector, and $\\lambda \\ge 0$ is the regularization parameter.\n\nTo find the solution $\\hat{\\beta}_{\\lambda}$, we derive the first-order optimality condition by taking the gradient of the objective function with respect to $\\beta$ and setting it to zero:\n$$\n\\nabla_{\\beta} \\left( \\frac{1}{n}(y - X\\beta)^T(y - X\\beta) + \\lambda \\beta^T\\beta \\right) = 0\n$$\n$$\n\\frac{1}{n} \\nabla_{\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) + \\lambda (2\\beta) = 0\n$$\n$$\n\\frac{1}{n} (-2X^T y + 2X^T X \\beta) + 2\\lambda\\beta = 0\n$$\nMultiplying by $n/2$, we get:\n$$\n-X^T y + X^T X \\beta + n\\lambda\\beta = 0\n$$\n$$\n(X^T X + n\\lambda I) \\beta = X^T y\n$$\nFor $\\lambda > 0$, the matrix $(X^T X + n\\lambda I)$ is guaranteed to be invertible because $X^T X$ is positive semi-definite, and adding a positive multiple of the identity matrix $I$ renders it positive definite. Thus, the unique solution is:\n$$\n\\hat{\\beta}_{\\lambda} = (X^T X + n\\lambda I)^{-1} X^T y\n$$\nFor the special case $\\lambda = 0$, the problem reduces to Ordinary Least Squares (OLS). If the matrix $X^T X$ is singular (e.g., in the underdetermined case where $n < d$), there are infinitely many solutions that minimize the training error. The problem specifies using the minimum Euclidean norm solution, which is uniquely given by the Moore-Penrose pseudoinverse $X^{+}$:\n$$\n\\hat{\\beta}_{0} = X^{+} y\n$$\n\nThe computational procedure is as follows:\n1.  For each test case, generate the training data $(X, y)$ and test data $(X_{\\text{test}}, y_{\\text{test}})$ according to the specified deterministic formulas. The parameters $n, d, m$, and the functions for generating the matrices, true parameter vector $\\beta^{\\star}$, and noise terms are provided. The data generation is implemented using vectorized `numpy` operations for efficiency.\n2.  Iterate through each regularization strength $\\lambda$ in the provided grid $\\Lambda = [\\,0,\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,1,\\,10,\\,100\\,]$.\n3.  In each iteration, compute the coefficient vector $\\hat{\\beta}_{\\lambda}$ using the appropriate formula: `np.linalg.pinv(X) @ y` for $\\lambda=0$ and `np.linalg.inv(XTX + n*lambda*I) @ XTy` for $\\lambda>0$.\n4.  With the computed $\\hat{\\beta}_{\\lambda}$, calculate the training and test Mean Squared Errors (MSE):\n    $$\n    R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) = \\frac{1}{n}\\|y - X\\hat{\\beta}_{\\lambda}\\|_{2}^{2}\n    $$\n    $$\n    R_{\\text{test}}(\\hat{\\beta}_{\\lambda}) = \\frac{1}{m}\\|y_{\\text{test}} - X_{\\text{test}}\\hat{\\beta}_{\\lambda}\\|_{2}^{2}\n    $$\n5.  After computing the errors for all $\\lambda \\in \\Lambda$, identify the index $k^{\\star}$ corresponding to the $\\lambda^{\\star}$ that minimizes the test error $R_{\\text{test}}$. In case of a tie, the smallest index is chosen.\n6.  Finally, perform the check to see if the training error at this optimal $\\lambda^{\\star}$, $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})$, is strictly greater than the minimum training error observed across the grid. The training error for ridge regression is a monotonically non-decreasing function of $\\lambda$. Therefore, the minimum training error occurs at $\\lambda=0$. The check, $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}}) > \\min_{\\lambda \\in \\Lambda} R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) + \\varepsilon$ with $\\varepsilon=10^{-12}$, verifies if moving away from the OLS solution (which has the best training performance) to a regularized solution yields an improvement in test performance, a hallmark of effective regularization.\n7.  The results for each test case—$[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$—are collected, and the final output is formatted as a string representation of a list of these results.", "answer": "```python\nimport numpy as np\n\ndef _solve_one_case(n, d, m, X_gen_rule, X_test_gen_rule, beta_star_gen_rule, eps_gen_rule, eps_test_gen_rule):\n    \"\"\"\n    Solves a single test case for the ridge regression problem.\n    \"\"\"\n    LAMBDA_GRID = [0.0, 1e-8, 1e-6, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n    EPSILON = 1e-12\n\n    # --- Data Generation ---\n    # Use 0-based indices for generation, consistent with problem statement's math notation.\n    \n    # Training set\n    i_train_idx = np.arange(n)[:, np.newaxis]\n    j_dim_idx = np.arange(d)[np.newaxis, :]\n    X = X_gen_rule(i_train_idx, j_dim_idx)\n\n    beta_star = beta_star_gen_rule(np.arange(d))\n    \n    eps_train = eps_gen_rule(np.arange(n))\n    \n    y = X @ beta_star + eps_train\n\n    # Test set\n    i_test_idx = np.arange(m)[:, np.newaxis]\n    X_test = X_test_gen_rule(i_test_idx, j_dim_idx)\n    \n    eps_test = eps_test_gen_rule(np.arange(m))\n    \n    y_test = X_test @ beta_star + eps_test\n    \n    # --- Ridge Path Calculation ---\n    train_errors = []\n    test_errors = []\n    \n    XTX = X.T @ X\n    XTy = X.T @ y\n    I_d = np.identity(d)\n    \n    for lambda_val in LAMBDA_GRID:\n        if lambda_val == 0.0:\n            # For lambda=0, use Moore-Penrose pseudoinverse for min-norm OLS solution\n            beta_hat = np.linalg.pinv(X) @ y\n        else:\n            # For lambda > 0, solve (X'X + n*lambda*I)beta = X'y\n            A = XTX + n * lambda_val * I_d\n            beta_hat = np.linalg.inv(A) @ XTy\n\n        # --- Error Calculation ---\n        # Training MSE: R_train = (1/n) * ||y - X*beta||^2\n        train_mse = np.mean(np.square(y - X @ beta_hat))\n        train_errors.append(train_mse)\n        \n        # Test MSE: R_test = (1/m) * ||y_test - X_test*beta||^2\n        test_mse = np.mean(np.square(y_test - X_test @ beta_hat))\n        test_errors.append(test_mse)\n        \n    # --- Analysis ---\n    # Find the index of lambda that minimizes test error\n    k_star = int(np.argmin(test_errors))\n    lambda_star = LAMBDA_GRID[k_star]\n    min_test_error = test_errors[k_star]\n\n    # Check if the training error at lambda_star is strictly greater than the minimum\n    min_train_error = np.min(train_errors)\n    train_error_at_best_lambda = train_errors[k_star]\n    train_nonmin = bool(train_error_at_best_lambda > min_train_error + EPSILON)\n\n    # --- Formatting ---\n    return [\n        k_star, \n        round(lambda_star, 6), \n        round(min_test_error, 6), \n        train_nonmin\n    ]\n\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (underdetermined, n < d, noisier)\n        {\n            \"n\": 8, \"d\": 12, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin((i+1)*(j+1)) + 0.1 * np.cos(i - 2*j),\n            \"X_test_gen_rule\": lambda i, j: np.cos((i+1)*(j+1)/2.0) + 0.1 * np.sin(i + j),\n            \"beta_star_gen_rule\": lambda j: ((-1)**j) * (j+1)/12.0,\n            \"eps_gen_rule\": lambda i: 0.3 * np.sin(3*i + 1),\n            \"eps_test_gen_rule\": lambda i: 0.3 * np.cos(5*i + 2)\n        },\n        # Test case 2 (overdetermined with strong collinearity, moderate noise)\n        {\n            \"n\": 40, \"d\": 10, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin(0.2*(i+1)) + 0.01 * np.cos(0.3*(i+1) + 0.1*(j+1)),\n            \"X_test_gen_rule\": lambda i, j: np.sin(0.21*(i+1)) + 0.01 * np.cos(0.31*(i+1) + 0.11*(j+1)),\n            \"beta_star_gen_rule\": lambda j: 1.0/(j+1),\n            \"eps_gen_rule\": lambda i: 0.2 * np.cos(0.5*(i+1)),\n            \"eps_test_gen_rule\": lambda i: 0.2 * np.sin(0.4*(i+1))\n        },\n        # Test case 3 (well-determined, low noise)\n        {\n            \"n\": 200, \"d\": 5, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin((i+1)*(j+2)/7.0) + 0.2 * np.cos((i+1) + 0.3*(j+1)),\n            \"X_test_gen_rule\": lambda i, j: np.sin((i+1)*(j+2)/7.3) + 0.2 * np.cos((i+1) + 0.33*(j+1)),\n            \"beta_star_gen_rule\": lambda j: (-0.7)**j,\n            \"eps_gen_rule\": lambda i: 0.02 * np.sin(0.1*(i+1) + 0.3),\n            \"eps_test_gen_rule\": lambda i: 0.02 * np.cos(0.12*(i+1) - 0.1)\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = _solve_one_case(**case_params)\n        results.append(result)\n\n    # The final output must be a string representation of a list of lists, with no spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3188165"}, {"introduction": "The principle of controlling complexity extends beyond linear models. Decision trees, with their ability to create highly intricate decision boundaries, are particularly prone to overfitting if allowed to grow unchecked until every leaf is pure. This exercise [@problem_id:3188147] challenges you to implement cost-complexity pruning, a method analogous to regularization for trees. You will start with a perfectly overfit tree and systematically simplify it, demonstrating how a smaller, less complex tree can often achieve better generalization by sacrificing its perfect score on the training data.", "problem": "You must write a complete and runnable program that constructs a binary classification decision tree grown to purity on a synthetic dataset, and then applies cost-complexity pruning to demonstrate the contrast between empirical training error and out-of-sample test error. Your code must implement all logic without relying on external machine learning libraries; only basic numerical libraries are allowed. All symbols, variables, and numbers in this problem are written in LaTeX.\n\nThe fundamental base you must use consists of the following definitions and principles.\n\n- Empirical risk with zero-one loss: given data $\\{(x_i,y_i)\\}_{i=1}^{n}$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{0,1\\}$, the empirical misclassification count of a classifier $h$ is $R(h) = \\sum_{i=1}^{n} \\mathbf{1}\\{h(x_i) \\neq y_i\\}$. The empirical misclassification rate is $\\hat r(h) = R(h)/n$.\n- Out-of-sample test misclassification rate is defined analogously on an independent test set drawn from the same data-generating mechanism as the training set; denote it by $r_{\\text{test}}(h)$.\n- A decision tree classifier recursively partitions $\\mathbb{R}^d$ by axis-aligned threshold splits and assigns a class label to each terminal region (leaf). A leaf’s prediction is the majority class among training samples that fall into that leaf.\n- Full growth to purity: continue splitting until every leaf contains samples from a single class or until every leaf contains at most one training sample. Under distinct feature vectors, this yields $\\hat r_{\\text{train}} = 0$.\n- Cost-complexity pruning: given a tree $T$, define the penalized empirical objective\n$$\nR_{\\alpha}(T) = R(T) + \\alpha \\cdot |T|,\n$$\nwhere $R(T)$ is the empirical misclassification count induced by $T$ on the training set and $|T|$ is the number of leaves in $T$. For any $\\alpha \\ge 0$, the pruned subtree $T_{\\alpha}$ minimizes $R_{\\alpha}(T)$ among all subtrees of the fully grown tree.\n\nYour program must implement:\n\n1. Training a binary decision tree on the training set using axis-aligned splits. Use the Gini impurity to choose splits whenever it reduces impurity. If no split reduces impurity but a node contains at least two samples with differing labels, you must still split so that full growth to purity is achieved (for example, by separating adjacent samples along some feature). Stop only when every leaf is pure or contains exactly one sample.\n2. Computing the empirical training misclassification rate $\\hat r_{\\text{train}}$ and the test misclassification rate $r_{\\text{test}}$ for a given tree.\n3. Computing the standard weakest-link cost-complexity pruning path for the fully grown tree. At each internal node $t$, let $R(t)$ be the empirical misclassification count if $t$ were replaced by a leaf predicting the node’s majority class, and let $R(T_t)$ be the current subtree misclassification count under $t$. Let $L_t$ be the number of leaves under node $t$. The node-specific pruning value is\n$$\ng(t) = \\frac{R(t) - R(T_t)}{L_t - 1}.\n$$\nPrune simultaneously all internal nodes achieving the smallest $g(t)$ to obtain the next subtree on the path, and repeat until a single-leaf tree remains. This yields a nondecreasing sequence $\\alpha_0 \\le \\alpha_1 \\le \\dots$ where each $\\alpha_k$ is the minimum $g(t)$ at step $k$.\n4. Selecting the subtree for a given $\\alpha \\ge 0$ as follows: choose the largest subtree on the pruning path whose associated threshold $\\alpha_k \\le \\alpha$. When $\\alpha = 0$, this must select the fully grown tree.\n\nData generation must be deterministic and scientifically realistic:\n\n- Input space dimension is $d=2$. For a given sample size $n$, generate features $X \\in \\mathbb{R}^{n \\times 2}$ with independent coordinates uniformly distributed on $[0,1]$. Use a deterministic pseudo-random number generator seeded by a specified integer to ensure reproducibility.\n- Define the underlying noise-free label by\n$$\ny^{\\star} = \\mathbf{1}\\{x_1 > \\tau(x_2)\\}, \\quad \\text{where } \\tau(u) = \\tfrac{1}{2} + \\tfrac{1}{4}\\sin(4\\pi u).\n$$\n- Inject label noise independently: with probability $\\eta \\in [0,1)$, flip $y^{\\star}$ to $1 - y^{\\star}$. Use an independent pseudo-random generator stream, also deterministically seeded, for the flips.\n- The test set is generated independently from the same mechanism with its own specified seed. Report all misclassification rates as real numbers in $[0,1]$.\n\nTest suite and required outputs:\n\nImplement the following three test cases. For each case, you must construct the fully grown tree $T_{\\text{full}}$, compute its training misclassification rate $\\hat r_{\\text{train}}(T_{\\text{full}})$ and test misclassification rate $r_{\\text{test}}(T_{\\text{full}})$, then produce the pruned tree $T_{\\alpha}$ for the specified $\\alpha$ and compute the corresponding rates. Your program must return a single list of three booleans, one per test case, aggregated as a single line string in the specified format.\n\n- Test case $1$ (happy path: demonstrate overfitting and improvement under pruning):\n  - Parameters: $n_{\\text{train}} = 120$, $n_{\\text{test}} = 8000$, $\\eta = 0.25$, train seed $= 1337$, test seed $= 2027$.\n  - Choice of $\\alpha$: scan the pruning path and choose the subtree $T_{\\alpha}$ that minimizes the test misclassification rate $r_{\\text{test}}(T_{\\alpha})$. Break ties by choosing the subtree with fewer leaves, and if still tied, choose the one appearing later on the pruning path (corresponding to the larger $\\alpha$). The boolean you must output for this case is\n    $$\n    b_1 = \\big(\\hat r_{\\text{train}}(T_{\\text{full}}) = 0\\big) \\wedge \\big(r_{\\text{test}}(T_{\\alpha}) < r_{\\text{test}}(T_{\\text{full}})\\big) \\wedge \\big(\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})\\big).\n    $$\n- Test case $2$ (boundary: $\\alpha = 0$ recovers no pruning):\n  - Parameters: $n_{\\text{train}} = 120$, $n_{\\text{test}} = 8000$, $\\eta = 0.25$, train seed $= 1337$, test seed $= 2027$, and $\\alpha = 0$.\n  - The boolean you must output for this case is\n    $$\n    b_2 = \\big(\\hat r_{\\text{train}}(T_{\\alpha}) = \\hat r_{\\text{train}}(T_{\\text{full}})\\big) \\wedge \\big(r_{\\text{test}}(T_{\\alpha}) = r_{\\text{test}}(T_{\\text{full}})\\big).\n    $$\n- Test case $3$ (edge: very large $\\alpha$ collapses to a stump):\n  - Parameters: $n_{\\text{train}} = 120$, $n_{\\text{test}} = 8000$, $\\eta = 0.25$, train seed $= 1337$, test seed $= 2027$, and $\\alpha = 10.0$.\n  - The boolean you must output for this case is\n    $$\n    b_3 = \\big(|T_{\\alpha}| = 1\\big) \\wedge \\big(\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})\\big).\n    $$\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[b_1,b_2,b_3]$. Each $b_i$ must be a boolean literal. For example, a valid output line is of the form `[True,False,True]`.\n\nNo physical units or angle units are required in this task. All probabilities and rates must be returned as real numbers in $[0,1]$, but only the booleans $b_1$, $b_2$, and $b_3$ must be printed as the final output.", "solution": "The user's request is to develop a Python program that implements a binary classification decision tree from first principles, including the cost-complexity pruning algorithm. The program must then be used to analyze the trade-off between training error and test error on a specified synthetic dataset. The problem is well-posed, scientifically grounded in statistical learning theory, and provides all necessary parameters and definitions for a deterministic and verifiable solution. Therefore, the problem is valid.\n\nThe solution is architected as a set of interacting components designed to be a faithful implementation of the specified algorithms.\n\n**1. Data Generation**\n\nA function generates the training and test datasets according to the problem's data-generating process. This process is deterministic, relying on pseudo-random number generators seeded with specified integers to ensure reproducibility.\n- The feature space is $\\mathbb{R}^2$, with coordinates $x_1$ and $x_2$ drawn independently from a uniform distribution on $[0, 1]$.\n- A true, noise-free class label $y^{\\star}$ is assigned based on a non-linear decision boundary:\n$$\ny^{\\star} = \\mathbf{1}\\{x_1 > \\tau(x_2)\\}, \\quad \\text{where } \\tau(u) = \\frac{1}{2} + \\frac{1}{4}\\sin(4\\pi u)\n$$\nThis boundary is a sine wave, creating a complex separation task.\n- Label noise is introduced by flipping the true label $y^{\\star}$ with a given probability $\\eta$. An independent seeded random number stream is used for this process to ensure that the noise is independent of the feature generation.\n\n**2. Decision Tree Growth**\n\nThe core of the classifier is a decision tree grown recursively. A `Node` class is used to represent the tree structure. Each node stores information about the data partitioned to it, the split criterion (if it's an internal node), or the prediction value (if it's a leaf).\n\nThe growth algorithm proceeds as follows:\n- **Recursive Partitioning**: Starting with the entire training set at the root node, the algorithm seeks the best axis-aligned split to partition the data.\n- **Split Criterion**: The Gini impurity is used to evaluate the quality of a split. For a set of samples $S$, the Gini impurity is $G(S) = 1 - \\sum_{c \\in \\{0,1\\}} p_c^2$, where $p_c$ is the proportion of samples of class $c$. The algorithm searches over all features and all possible split thresholds to find the split that results in the lowest weighted average Gini impurity of the child nodes. Possible split thresholds are taken as midpoints between consecutive unique feature values.\n- **Stopping and Purity Rule**: The recursion for a branch terminates if a node becomes \"pure\" (contains samples of only one class) or contains only a single sample. Critically, to achieve a training error of zero as specified, if a node is impure but no split reduces the Gini impurity, a split is still forced. This is accomplished by finding the first available feature and separating the first data point from the rest, guaranteeing that the tree will grow until every leaf is pure, effectively memorizing the training data, including the noise. This yields a fully grown tree $T_{\\text{full}}$ with an empirical training misclassification rate $\\hat r_{\\text{train}}(T_{\\text{full}}) = 0$.\n\n**3. Cost-Complexity Pruning**\n\nThe fully grown tree $T_{\\text{full}}$ is expected to be overfit to the training data. Cost-complexity pruning is implemented to find a sequence of smaller, potentially better-generalizing subtrees.\n- **Weakest-Link Pruning**: The algorithm generates a sequence of subtrees by iteratively pruning branches. At each step, it identifies the \"weakest link\"—the internal node whose removal leads to the smallest per-leaf increase in misclassification cost.\n- **Pruning Parameter $g(t)$**: For each internal node $t$ in the current tree, a parameter $g(t)$ is calculated:\n$$\ng(t) = \\frac{R(t) - R(T_t)}{L_t - 1}\n$$\nHere, $R(t)$ is the misclassification count on the training data if node $t$ were converted to a leaf, $R(T_t)$ is the total misclassification count of the entire subtree currently rooted at $t$, and $L_t$ is the number of leaves in that subtree.\n- **Path Generation**: The algorithm starts with $T_0 = T_{\\text{full}}$. In step $k$, it computes $g(t)$ for all internal nodes in the current tree $T_{k-1}$. The minimum value found, $\\alpha_k = \\min_t g(t)$, becomes the next threshold in the pruning sequence. A new tree $T_k$ is formed by taking $T_{k-1}$ and converting all nodes $t$ where $g(t) = \\alpha_k$ into leaves. This process is repeated until only the root node remains, yielding a sequence of trees $[T_0, T_1, \\dots, T_m]$ and corresponding a non-decreasing sequence of complexity parameters $[\\alpha_0, \\alpha_1, \\dots, \\alpha_m]$, where $\\alpha_0=0$. Deep copies of tree structures are used to maintain the distinct trees in the sequence.\n\n**4. Evaluation and Test Cases**\n\nThe program then uses the generated data, the full tree, and the pruning path to evaluate the three specified test cases.\n- For a given value of the complexity parameter $\\alpha$, the optimal subtree $T_\\alpha$ is selected from the generated path. The selection rule is to choose the tree $T_k$ from the path such that its corresponding parameter $\\alpha_k$ is the largest value less than or equal to the given $\\alpha$.\n- Misclassification rates for both training and test sets are computed for the relevant trees ($T_{\\text{full}}$ and $T_\\alpha$).\n- **Test Case 1**: This case requires finding the subtree on the pruning path that minimizes the test error. The search iterates through all trees in the path, calculates their test error, and selects the best one according to the specified tie-breaking rules (preferring smaller trees, then larger $\\alpha$). The boolean condition $b_1$ verifies that the full tree overfits ($\\hat r_{\\text{train}}=0$), the pruned tree generalizes better ($r_{\\text{test}}(T_{\\alpha}) < r_{\\text{test}}(T_{\\text{full}})$), and that this improvement comes at the cost of training performance ($\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})$).\n- **Test Case 2**: With $\\alpha=0$, the selection rule must choose the first tree in the path, which is $T_{\\text{full}}$ itself. The boolean condition $b_2$ verifies that the errors of the selected tree are identical to those of the full tree.\n- **Test Case 3**: With a very large $\\alpha=10.0$, the selection rule is expected to choose the most heavily pruned tree, which is the single-leaf root stump. The boolean condition $b_3$ verifies that the resulting tree has exactly one leaf ($|T_{\\alpha}| = 1$) and has a higher training error than the full tree.\n\nThe final output aggregates the boolean results of these three tests into a single formatted string.", "answer": "```python\nimport numpy as np\nimport copy\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main solver function to implement and test the decision tree and pruning algorithm.\n    \"\"\"\n\n    class Node:\n        \"\"\"\n        Represents a node in the decision tree.\n        \"\"\"\n        _id_counter = 0\n\n        def __init__(self, indices):\n            self.id = Node._id_counter\n            Node._id_counter += 1\n            \n            self.indices = np.array(indices, dtype=int)\n            \n            self.is_leaf = False\n            self.feature_idx = None\n            self.threshold = None\n            self.left = None\n            self.right = None\n            \n            self.value = None\n\n            self.n_samples = len(self.indices)\n            self.n0 = 0\n            self.n1 = 0\n            self.misclass_as_leaf = 0.0 # R(t)\n            self.gini = 0.0\n\n    class DecisionTree:\n        \"\"\"\n        Encapsulates the decision tree logic: growing, predicting, and helper methods.\n        \"\"\"\n        def __init__(self):\n            self.root = None\n\n        def _calculate_gini(self, y_node):\n            if len(y_node) == 0:\n                return 0.0\n            p1 = np.sum(y_node == 1) / len(y_node)\n            p0 = 1.0 - p1\n            return 1.0 - (p0**2 + p1**2)\n\n        def _find_split(self, X, y, indices):\n            if len(indices) <= 1:\n                return None\n\n            parent_gini = self._calculate_gini(y[indices])\n            best_gini = parent_gini\n            best_split = None\n            n_features = X.shape[1]\n\n            for feature_idx in range(n_features):\n                unique_vals = np.unique(X[indices, feature_idx])\n                if len(unique_vals) <= 1:\n                    continue\n\n                for i in range(len(unique_vals) - 1):\n                    threshold = (unique_vals[i] + unique_vals[i+1]) / 2.0\n                    \n                    left_indices = indices[X[indices, feature_idx] <= threshold]\n                    right_indices = indices[X[indices, feature_idx] > threshold]\n                    \n                    if len(left_indices) == 0 or len(right_indices) == 0:\n                        continue\n\n                    gini_left = self._calculate_gini(y[left_indices])\n                    gini_right = self._calculate_gini(y[right_indices])\n                    \n                    w_left = len(left_indices) / len(indices)\n                    w_right = len(right_indices) / len(indices)\n                    \n                    weighted_gini = w_left * gini_left + w_right * gini_right\n                    \n                    if weighted_gini < best_gini:\n                        best_gini = weighted_gini\n                        best_split = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'left_indices': left_indices,\n                            'right_indices': right_indices\n                        }\n            \n            # Force-split rule for purity if no Gini improvement\n            if best_split is None and parent_gini > 0:\n                for feature_idx in range(n_features):\n                    sorted_indices = indices[np.argsort(X[indices, feature_idx])]\n                    if X[sorted_indices[0], feature_idx] < X[sorted_indices[-1], feature_idx]:\n                        threshold = (X[sorted_indices[0], feature_idx] + X[sorted_indices[1], feature_idx]) / 2.0\n                        left_indices = sorted_indices[X[sorted_indices, feature_idx] <= threshold]\n                        right_indices = sorted_indices[X[sorted_indices, feature_idx] > threshold]\n                        best_split = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'left_indices': left_indices,\n                            'right_indices': right_indices\n                        }\n                        break\n            \n            return best_split\n\n        def _grow(self, X, y, indices):\n            node = Node(indices)\n            y_node = y[node.indices]\n            \n            node.n0 = np.sum(y_node == 0)\n            node.n1 = np.sum(y_node == 1)\n            node.value = 1 if node.n1 > node.n0 else 0\n            node.misclass_as_leaf = min(node.n0, node.n1)\n            node.gini = self._calculate_gini(y_node)\n\n            if node.gini == 0.0 or node.n_samples <= 1:\n                node.is_leaf = True\n                return node\n            \n            split = self._find_split(X, y, node.indices)\n\n            if split is None:\n                node.is_leaf = True\n                return node\n\n            node.feature_idx = split['feature_idx']\n            node.threshold = split['threshold']\n            node.left = self._grow(X, y, split['left_indices'])\n            node.right = self._grow(X, y, split['right_indices'])\n            \n            return node\n\n        def fit(self, X, y):\n            Node._id_counter = 0\n            self.root = self._grow(X, y, np.arange(len(y)))\n\n        def _predict_single(self, x, node):\n            if node.is_leaf:\n                return node.value\n            if x[node.feature_idx] <= node.threshold:\n                return self._predict_single(x, node.left)\n            return self._predict_single(x, node.right)\n\n        def predict(self, X):\n            return np.array([self._predict_single(x, self.root) for x in X])\n\n    def generate_data(n, feature_seed, noise_seed, eta):\n        rng_features = np.random.default_rng(feature_seed)\n        X = rng_features.uniform(0, 1, size=(n, 2))\n        \n        tau = 0.5 + 0.25 * np.sin(4 * np.pi * X[:, 1])\n        y_star = (X[:, 0] > tau).astype(int)\n        \n        rng_noise = np.random.default_rng(noise_seed)\n        flips = rng_noise.random(n) < eta\n        y = y_star.copy()\n        y[flips] = 1 - y[flips]\n        \n        return X, y\n\n    def get_misclassification_rate(y_true, y_pred):\n        return np.mean(y_true != y_pred)\n\n    def get_subtree_info(node):\n        if node.is_leaf:\n            return 1, node.misclass_as_leaf\n        \n        left_leaves, left_misclass = get_subtree_info(node.left)\n        right_leaves, right_misclass = get_subtree_info(node.right)\n        \n        return left_leaves + right_leaves, left_misclass + right_misclass\n\n    def get_leaf_count(node):\n        if node is None:\n            return 0\n        if node.is_leaf:\n            return 1\n        return get_leaf_count(node.left) + get_leaf_count(node.right)\n\n    def get_pruning_path(tree, X_train, y_train):\n        path = [(0.0, copy.deepcopy(tree))]\n        current_tree = tree\n\n        while True:\n            internal_nodes = []\n            q = deque([current_tree.root])\n            node_map = {current_tree.root.id: current_tree.root}\n            while q:\n                node = q.popleft()\n                if not node.is_leaf:\n                    internal_nodes.append(node)\n                    if node.left: q.append(node.left); node_map[node.left.id] = node.left\n                    if node.right: q.append(node.right); node_map[node.right.id] = node.right\n            \n            if not internal_nodes:\n                break\n            \n            g_values = []\n            for node in internal_nodes:\n                L_t, R_Tt = get_subtree_info(node)\n                R_t = node.misclass_as_leaf\n                if L_t > 1:\n                    g = (R_t - R_Tt) / (L_t - 1)\n                    g_values.append((g, node.id))\n            \n            if not g_values:\n                break\n            \n            min_g = min(g for g, _ in g_values)\n            \n            next_tree_obj = copy.deepcopy(current_tree)\n            \n            nodes_to_prune_ids = {nid for g, nid in g_values if np.isclose(g, min_g)}\n            \n            q = deque([next_tree_obj.root])\n            next_node_map = {next_tree_obj.root.id: next_tree_obj.root}\n            while q:\n                node = q.popleft()\n                if not node.is_leaf:\n                    if node.left: q.append(node.left); next_node_map[node.left.id] = node.left\n                    if node.right: q.append(node.right); next_node_map[node.right.id] = node.right\n\n            for node_id in nodes_to_prune_ids:\n                node_to_prune = next_node_map[node_id]\n                node_to_prune.is_leaf = True\n                node_to_prune.left = None\n                node_to_prune.right = None\n\n            current_tree = next_tree_obj\n            path.append((min_g, current_tree))\n        \n        return path\n\n    def select_tree_for_alpha(path, alpha):\n        best_tree = None\n        best_alpha_k = -1.0\n        for alpha_k, tree in path:\n            if alpha_k <= alpha and alpha_k >= best_alpha_k:\n                best_alpha_k = alpha_k\n                best_tree = tree\n        return best_tree\n\n    # --- Main Execution Logic ---\n    \n    # Common parameters for all test cases\n    n_train = 120\n    n_test = 8000\n    eta = 0.25\n    train_seed = 1337\n    test_seed = 2027\n\n    X_train, y_train = generate_data(n_train, train_seed, train_seed + 1, eta)\n    X_test, y_test = generate_data(n_test, test_seed, test_seed + 1, eta)\n\n    full_tree = DecisionTree()\n    full_tree.fit(X_train, y_train)\n\n    y_pred_train_full = full_tree.predict(X_train)\n    r_train_full = get_misclassification_rate(y_train, y_pred_train_full)\n\n    y_pred_test_full = full_tree.predict(X_test)\n    r_test_full = get_misclassification_rate(y_test, y_pred_test_full)\n\n    pruning_path = get_pruning_path(full_tree, X_train, y_train)\n    \n    results = []\n\n    # Test Case 1: Optimal alpha by test error\n    best_test_err = float('inf')\n    best_tree_alpha1 = None\n    best_leaf_count = float('inf')\n    best_alpha_val = -1.0\n\n    for alpha_k, tree_k in pruning_path:\n        y_pred_test_k = tree_k.predict(X_test)\n        test_err_k = get_misclassification_rate(y_test, y_pred_test_k)\n        leaf_count_k = get_leaf_count(tree_k.root)\n\n        if test_err_k < best_test_err:\n            best_test_err = test_err_k\n            best_tree_alpha1 = tree_k\n            best_leaf_count = leaf_count_k\n            best_alpha_val = alpha_k\n        elif np.isclose(test_err_k, best_test_err):\n            if leaf_count_k < best_leaf_count:\n                best_tree_alpha1 = tree_k\n                best_leaf_count = leaf_count_k\n                best_alpha_val = alpha_k\n            elif leaf_count_k == best_leaf_count and alpha_k > best_alpha_val:\n                best_tree_alpha1 = tree_k\n                best_alpha_val = alpha_k\n    \n    T_alpha1 = best_tree_alpha1\n    r_test_alpha1 = best_test_err\n    r_train_alpha1 = get_misclassification_rate(y_train, T_alpha1.predict(X_train))\n    \n    b1 = (np.isclose(r_train_full, 0.0)) and (r_test_alpha1 < r_test_full) and (r_train_alpha1 > r_train_full)\n    results.append(b1)\n    \n    # Test Case 2: alpha = 0\n    alpha2 = 0.0\n    T_alpha2 = select_tree_for_alpha(pruning_path, alpha2)\n    r_train_alpha2 = get_misclassification_rate(y_train, T_alpha2.predict(X_train))\n    r_test_alpha2 = get_misclassification_rate(y_test, T_alpha2.predict(X_test))\n    \n    b2 = np.isclose(r_train_alpha2, r_train_full) and np.isclose(r_test_alpha2, r_test_full)\n    results.append(b2)\n\n    # Test Case 3: alpha = 10.0\n    alpha3 = 10.0\n    T_alpha3 = select_tree_for_alpha(pruning_path, alpha3)\n    r_train_alpha3 = get_misclassification_rate(y_train, T_alpha3.predict(X_train))\n    num_leaves_alpha3 = get_leaf_count(T_alpha3.root)\n    \n    b3 = (num_leaves_alpha3 == 1) and (r_train_alpha3 > r_train_full)\n    results.append(b3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3188147"}]}