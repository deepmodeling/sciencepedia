{"hands_on_practices": [{"introduction": "This first exercise provides a hands-on look at the practical consequences of choosing between Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) representations. By analyzing how these different weighting schemes affect the decision boundary of a simple classifier, you will develop a geometric intuition for TF-IDF's core function: amplifying the signal from rare and informative words. This practice grounds abstract definitions in a concrete classification scenario. [@problem_id:3179893]", "problem": "Consider binary text classification with $k$-Nearest Neighbors (k-NN), using cosine similarity on vector representations of documents. Two standard representations are Bag-of-Words (BoW) and Term Frequency–Inverse Document Frequency (TF-IDF). In BoW, each document $d$ is represented by raw term counts $tf_{d,j} \\in \\mathbb{N}$ over a fixed vocabulary. In TF-IDF, each term $j$ is weighted by $tf_{d,j} \\cdot \\mathrm{idf}_{j}$, where the inverse document frequency (IDF) is defined as $\\mathrm{idf}_{j} = \\ln\\!\\left(\\frac{N}{df_{j}}\\right)$, with $N$ the corpus size and $df_{j}$ the document frequency of term $j$. Cosine similarity between two vectors is the dot product of their $\\ell^{2}$-normalized representations. All vectors in this problem are $\\ell^{2}$-normalized after weighting. We use $k=1$ (one-nearest neighbor) for classification.\n\nA synthetic corpus has size $N = 1000$ and vocabulary terms $\\{t_{1}, t_{2}, t_{3}\\}$. Two labeled training documents are given by their BoW counts:\n- Class $\\mathcal{A}$: $D_{\\mathcal{A}}$ has counts $(tf_{D_{\\mathcal{A}},1}, tf_{D_{\\mathcal{A}},2}, tf_{D_{\\mathcal{A}},3}) = (10, 0, 0)$.\n- Class $\\mathcal{B}$: $D_{\\mathcal{B}}$ has counts $(tf_{D_{\\mathcal{B}},1}, tf_{D_{\\mathcal{B}},2}, tf_{D_{\\mathcal{B}},3}) = (0, 2, 3)$.\n\nA query document $q$ has counts $(tf_{q,1}, tf_{q,2}, tf_{q,3}) = (3, 0, 1)$. The document frequencies are $df_{1} = 900$, $df_{2} = 50$, and $df_{3}$ is unknown. Let $w_{j} = \\mathrm{idf}_{j} = \\ln\\!\\left(\\frac{N}{df_{j}}\\right)$ denote the IDF for term $t_{j}$.\n\nStarting from the core definitions above (BoW counts, TF-IDF weighting, cosine similarity on $\\ell^{2}$-normalized vectors, and $1$-NN decision by largest cosine similarity), do the following:\n\n1. Define the decision boundary between classes $\\mathcal{A}$ and $\\mathcal{B}$ as the locus of queries whose cosine similarities to $D_{\\mathcal{A}}$ and $D_{\\mathcal{B}}$ are equal. Using the given synthetic data, derive an analytic condition in terms of $w_{1}$, $w_{2}$, and $w_{3}$ under which the BoW-based $1$-NN and TF-IDF-based $1$-NN produce different class decisions for the fixed query $q$. Your derivation must begin from the base definitions provided and proceed by first principles.\n\n2. Compute the unique critical document frequency $df_{3}^{\\star}$ at which the TF-IDF decision boundary passes through the query $q$ (that is, the TF-IDF cosine similarities of $q$ to $D_{\\mathcal{A}}$ and $D_{\\mathcal{B}}$ are equal), while BoW $1$-NN assigns $q$ to class $\\mathcal{A}$. Use the given $N$, $df_{1}$, and $df_{2}$ to determine $w_{1}$ and $w_{2}$, and solve for $df_{3}^{\\star}$. Round your final numeric answer to four significant figures. Express your answer as a real-valued document count. For conceptual visualization, interpret how the TF-IDF weighting changes the decision boundary relative to BoW in the $(t_{1}, t_{3})$ subspace, but report only the requested numeric $df_{3}^{\\star}$.", "solution": "The problem requires us to analyze the classification of a query document $q$ using a $1$-Nearest Neighbor ($1$-NN) classifier with cosine similarity, under two different text representation schemes: Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). We will first derive a general condition under which the two schemes produce different results for the given query, and then solve for a specific parameter value where the TF-IDF decision boundary crosses the query.\n\nFirst, let us define the unnormalized vectors for the training documents $D_{\\mathcal{A}}$ and $D_{\\mathcal{B}}$, and the query document $q$, based on their given term frequency counts $(tf_1, tf_2, tf_3)$.\n\nFor the BoW representation, the vectors are the raw term counts:\n$v_{\\mathcal{A}} = (10, 0, 0)$\n$v_{\\mathcal{B}} = (0, 2, 3)$\n$v_{q} = (3, 0, 1)$\n\nFor the TF-IDF representation, the components are weighted by $w_j = \\mathrm{idf}_j = \\ln(N/df_j)$. The resulting vectors are:\n$v'_{\\mathcal{A}} = (10 \\cdot w_1, 0 \\cdot w_2, 0 \\cdot w_3) = (10w_1, 0, 0)$\n$v'_{\\mathcal{B}} = (0 \\cdot w_1, 2 \\cdot w_2, 3 \\cdot w_3) = (0, 2w_2, 3w_3)$\n$v'_{q} = (3 \\cdot w_1, 0 \\cdot w_2, 1 \\cdot w_3) = (3w_1, 0, w_3)$\n\nCosine similarity between two vectors $u$ and $v$ is given by $S(u, v) = \\frac{u \\cdot v}{\\|u\\|_2 \\|v\\|_2}$. The $1$-NN decision rule assigns the query to the class of the training document with the highest cosine similarity.\n\nLet's first analyze the BoW case. We compute the $\\ell^2$-norms of the BoW vectors:\n$\\|v_{\\mathcal{A}}\\|_2 = \\sqrt{10^2 + 0^2 + 0^2} = 10$\n$\\|v_{\\mathcal{B}}\\|_2 = \\sqrt{0^2 + 2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$\n$\\|v_{q}\\|_2 = \\sqrt{3^2 + 0^2 + 1^2} = \\sqrt{9+1} = \\sqrt{10}$\n\nThe cosine similarities for the BoW case are:\n$S_{\\text{BoW}}(q, D_{\\mathcal{A}}) = \\frac{v_q \\cdot v_{\\mathcal{A}}}{\\|v_q\\|_2 \\|v_{\\mathcal{A}}\\|_2} = \\frac{(3)(10) + (0)(0) + (1)(0)}{\\sqrt{10} \\cdot 10} = \\frac{30}{10\\sqrt{10}} = \\frac{3}{\\sqrt{10}}$\n$S_{\\text{BoW}}(q, D_{\\mathcal{B}}) = \\frac{v_q \\cdot v_{\\mathcal{B}}}{\\|v_q\\|_2 \\|v_{\\mathcal{B}}\\|_2} = \\frac{(3)(0) + (0)(2) + (1)(3)}{\\sqrt{10} \\sqrt{13}} = \\frac{3}{\\sqrt{130}}$\n\nTo compare these values, we note that $10 < 130$, so $\\sqrt{10} < \\sqrt{130}$, which implies $\\frac{1}{\\sqrt{10}} > \\frac{1}{\\sqrt{130}}$. Therefore, $S_{\\text{BoW}}(q, D_{\\mathcal{A}}) > S_{\\text{BoW}}(q, D_{\\mathcal{B}})$. The BoW-based $1$-NN classifier assigns query $q$ to Class $\\mathcal{A}$.\n\nNow, let's analyze the TF-IDF case. The norms of the TF-IDF vectors are:\n$\\|v'_{\\mathcal{A}}\\|_2 = \\sqrt{(10w_1)^2} = 10w_1$ (since $w_1 = \\ln(1000/900)>0$)\n$\\|v'_{\\mathcal{B}}\\|_2 = \\sqrt{(2w_2)^2 + (3w_3)^2} = \\sqrt{4w_2^2 + 9w_3^2}$\n$\\|v'_{q}\\|_2 = \\sqrt{(3w_1)^2 + (w_3)^2} = \\sqrt{9w_1^2 + w_3^2}$\n\nThe cosine similarities for the TF-IDF case are:\n$S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}}) = \\frac{v'_q \\cdot v'_{\\mathcal{A}}}{\\|v'_q\\|_2 \\|v'_{\\mathcal{A}}\\|_2} = \\frac{(3w_1)(10w_1)}{\\sqrt{9w_1^2 + w_3^2} \\cdot 10w_1} = \\frac{3w_1}{\\sqrt{9w_1^2 + w_3^2}}$\n$S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}}) = \\frac{v'_q \\cdot v'_{\\mathcal{B}}}{\\|v'_q\\|_2 \\|v'_{\\mathcal{B}}\\|_2} = \\frac{(3w_1)(0) + (0)(2w_2) + (w_3)(3w_3)}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}} = \\frac{3w_3^2}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}}$\n\n**Part 1: Condition for Different Decisions**\nThe BoW and TF-IDF based classifiers produce different decisions if the TF-IDF classifier assigns $q$ to Class $\\mathcal{B}$, which happens when $S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}}) > S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}})$.\n$$ \\frac{3w_3^2}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}} > \\frac{3w_1}{\\sqrt{9w_1^2 + w_3^2}} $$\nSince the term $\\sqrt{9w_1^2 + w_3^2}$ is positive, we can multiply both sides by it and also divide by $3$:\n$$ \\frac{w_3^2}{\\sqrt{4w_2^2 + 9w_3^2}} > w_1 $$\nAssuming $w_1, w_2, w_3$ are positive (which they are for $1 \\le df_j < N$), both sides are positive, so we can square them without changing the inequality's direction:\n$$ \\frac{w_3^4}{4w_2^2 + 9w_3^2} > w_1^2 $$\n$$ w_3^4 > w_1^2 (4w_2^2 + 9w_3^2) $$\n$$ w_3^4 > 4w_1^2 w_2^2 + 9w_1^2 w_3^2 $$\nRearranging the terms gives the final analytic condition:\n$$ w_3^4 - 9w_1^2 w_3^2 - 4w_1^2 w_2^2 > 0 $$\n\n**Part 2: Calculation of Critical Document Frequency $df_3^{\\star}$**\nThe TF-IDF decision boundary passes through the query $q$ when the similarities are equal: $S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}}) = S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}})$. This corresponds to the equality case of the condition derived above:\n$$ w_3^4 - 9w_1^2 w_3^2 - 4w_1^2 w_2^2 = 0 $$\nThis is a quadratic equation for the variable $w_3^2$. Let $x = w_3^2$. The equation is $x^2 - (9w_1^2)x - (4w_1^2 w_2^2) = 0$.\n\nWe first compute the values of $w_1$ and $w_2$ using the provided data: $N=1000$, $df_1=900$, $df_2=50$.\n$w_1 = \\ln\\left(\\frac{1000}{900}\\right) = \\ln\\left(\\frac{10}{9}\\right)$\n$w_2 = \\ln\\left(\\frac{1000}{50}\\right) = \\ln(20)$\n\nWe solve for $x=w_3^2$ using the quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ with $a=1$, $b=-9w_1^2$, and $c=-4w_1^2 w_2^2$.\n$$ x = w_3^2 = \\frac{9w_1^2 \\pm \\sqrt{(-9w_1^2)^2 - 4(1)(-4w_1^2 w_2^2)}}{2} $$\n$$ w_3^2 = \\frac{9w_1^2 \\pm \\sqrt{81w_1^4 + 16w_1^2 w_2^2}}{2} $$\nSince $w_3^2$ must be non-negative, we must take the positive root. The term under the square root is $\\sqrt{81w_1^4 + 16w_1^2 w_2^2} > \\sqrt{81w_1^4} = 9w_1^2$, so the numerator is negative if we choose the minus sign. Thus, we select the plus sign:\n$$ w_3^2 = \\frac{9w_1^2 + \\sqrt{81w_1^4 + 16w_1^2 w_2^2}}{2} $$\nNow we substitute the numerical values for the weights:\n$w_1 = \\ln(10/9) \\approx 0.1053605$\n$w_2 = \\ln(20) \\approx 2.9957323$\n$w_1^2 \\approx 0.0111008$\n$w_2^2 \\approx 8.974415$\n\nThe quadratic equation for $x=w_3^2$ is approximately:\n$x^2 - 9(0.0111008)x - 4(0.0111008)(8.974415) = 0$\n$x^2 - 0.0999072x - 0.398556 = 0$\n\nSolving for $x$:\n$$ x = \\frac{0.0999072 + \\sqrt{(-0.0999072)^2 - 4(1)(-0.398556)}}{2} $$\n$$ x = \\frac{0.0999072 + \\sqrt{0.0099814 + 1.594224}}{2} $$\n$$ x = \\frac{0.0999072 + \\sqrt{1.6042054}}{2} $$\n$$ x = \\frac{0.0999072 + 1.2665723}{2} = \\frac{1.3664795}{2} = 0.68323975 $$\nSo, $w_3^2 \\approx 0.68323975$. Taking the square root to find $w_3$:\n$w_3 = \\sqrt{0.68323975} \\approx 0.8265831$\n\nThe critical document frequency $df_3^{\\star}$ is found by inverting the IDF formula, $w_3 = \\ln(N/df_3^{\\star})$:\n$$ df_3^{\\star} = \\frac{N}{\\exp(w_3)} = \\frac{1000}{\\exp(0.8265831)} $$\n$$ df_3^{\\star} = \\frac{1000}{2.285460} \\approx 437.5492 $$\nRounding to four significant figures, we get $df_3^{\\star} = 437.5$. As established at the beginning, the BoW classifier assigns $q$ to class $\\mathcal{A}$, so this result satisfies all conditions of the problem.", "answer": "$$ \\boxed{437.5} $$", "id": "3179893"}, {"introduction": "Moving beyond heuristic weighting schemes, this practice delves into the world of probabilistic generative models for text. You will derive the Expectation-Maximization (EM) algorithm for the Mixture of Unigrams model, a foundational topic model, entirely from first principles. This exercise builds crucial skills in applying statistical inference techniques and provides a conceptual stepping stone to understanding the more powerful Latent Dirichlet Allocation (LDA) model. [@problem_id:3179899]", "problem": "Consider the bag-of-words representation of a corpus with $D$ documents, vocabulary size $V$, and $K$ latent topics. Each document $d \\in \\{1,\\dots,D\\}$ is represented by a count vector $\\mathbf{n}_{d} = (n_{d,1},\\dots,n_{d,V})$ with total length $N_{d} = \\sum_{v=1}^{V} n_{d,v}$. In the mixture of unigrams model (a mixture of multinomials), the generative process for each document $d$ is: draw a single latent topic $z_{d} \\in \\{1,\\dots,K\\}$ according to mixing proportions $\\boldsymbol{\\pi} = (\\pi_{1},\\dots,\\pi_{K})$ on the probability simplex ($\\sum_{k=1}^{K} \\pi_{k} = 1$, $\\pi_{k} \\geq 0$), and then draw each of the $N_{d}$ word tokens independently from a topic-specific multinomial distribution $\\boldsymbol{\\phi}_{k} = (\\phi_{k,1},\\dots,\\phi_{k,V})$ on the vocabulary simplex ($\\sum_{v=1}^{V} \\phi_{k,v} = 1$, $\\phi_{k,v} \\geq 0$). The observed-data likelihood for a document $d$ under topic $k$ is $p(\\mathbf{n}_{d} \\mid z_{d}=k) = \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!} \\prod_{v=1}^{V} \\phi_{k,v}^{n_{d,v}}$.\n\nExpectation Maximization (EM) seeks to maximize the observed log-likelihood $\\ln p(\\{\\mathbf{n}_{d}\\}_{d=1}^{D} \\mid \\Theta)$, where $\\Theta = \\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\phi}_{k}\\}_{k=1}^{K}\\}$, by iterating an expectation step and a maximization step. The expectation step computes the posterior distribution of latent variables given current parameters, and the maximization step maximizes the expected complete-data log-likelihood subject to the probability simplex constraints. Begin from these core definitions and the general EM framework; do not invoke any pre-derived update formulas.\n\nTasks:\n- Derive, from first principles, the EM updates for the mixture of unigrams model: the expression for the responsibilities $\\gamma_{d,k} = p(z_{d}=k \\mid \\mathbf{n}_{d}, \\Theta)$ in the expectation step, and the parameter updates for $\\pi_{k}$ and $\\phi_{k,v}$ in the maximization step using Lagrange multipliers to enforce the simplex constraints.\n- Compare conceptual and modeling limitations of the mixture of unigrams to Latent Dirichlet Allocation (LDA), which assigns topics at the token level and places Dirichlet priors over document-topic proportions and topic-word distributions. Explain why a single-topic-per-document assumption restricts expressivity, and connect this to the bag-of-words setup. Discuss how Term Frequency–Inverse Document Frequency (TF-IDF) weighting differs from probabilistic generative modeling and why naively substituting TF-IDF weights for counts can break the multinomial likelihood interpretation.\n- Analyze convergence: argue why EM for this model guarantees a non-decreasing observed log-likelihood across iterations, and explain the possibility of convergence to local optima due to non-convexity. Provide a concise, principled explanation based on the EM lower bound and Jensen’s inequality.\n- Finally, consider the following concrete corpus and initialization. Let $V = 3$, $K = 2$, and $D = 2$. The documents are:\n  - Document $d=1$: $\\mathbf{n}_{1} = (n_{1,1}, n_{1,2}, n_{1,3}) = (2, 1, 0)$ with $N_{1} = 3$,\n  - Document $d=2$: $\\mathbf{n}_{2} = (n_{2,1}, n_{2,2}, n_{2,3}) = (0, 1, 2)$ with $N_{2} = 3$.\n  Initialize the parameters as $\\boldsymbol{\\pi} = (\\pi_{1}, \\pi_{2}) = (0.5, 0.5)$, $\\boldsymbol{\\phi}_{1} = (\\phi_{1,1}, \\phi_{1,2}, \\phi_{1,3}) = (0.6, 0.3, 0.1)$, and $\\boldsymbol{\\phi}_{2} = (\\phi_{2,1}, \\phi_{2,2}, \\phi_{2,3}) = (0.1, 0.3, 0.6)$. Perform one full EM iteration (compute the responsibilities and then update the parameters), and then compute the updated value of $\\phi_{1,1}$. Round your final numeric answer to four significant figures.", "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, well-posed with all necessary data and definitions, and objective in its formulation. It requests derivations, conceptual analysis, and a numerical calculation based on standard models and algorithms.\n\n### Part 1: Derivation of Expectation-Maximization (EM) Updates\n\nThe mixture of unigrams model assumes that each document $d$ is generated from a single latent topic $z_d$, drawn from a distribution with parameters $\\boldsymbol{\\pi}$, and its words are then drawn from the corresponding topic-word distribution $\\boldsymbol{\\phi}_{z_d}$. We seek to find the parameters $\\Theta = \\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\phi}_{k}\\}_{k=1}^{K}\\}$ that maximize the observed-data log-likelihood for a corpus of $D$ documents, $\\mathcal{L}(\\Theta) = \\ln p(\\{\\mathbf{n}_{d}\\}_{d=1}^{D} \\mid \\Theta) = \\sum_{d=1}^{D} \\ln p(\\mathbf{n}_{d} \\mid \\Theta)$. The Expectation-Maximization (EM) algorithm is an iterative method for this purpose.\n\nThe marginal likelihood for a single document $\\mathbf{n}_d$ is given by summing over the latent topic assignments:\n$$p(\\mathbf{n}_{d} \\mid \\Theta) = \\sum_{k=1}^{K} p(\\mathbf{n}_{d}, z_{d}=k \\mid \\Theta) = \\sum_{k=1}^{K} p(\\mathbf{n}_{d} \\mid z_d=k, \\Theta) p(z_d=k \\mid \\Theta)$$\n$$p(\\mathbf{n}_{d} \\mid \\Theta) = \\sum_{k=1}^{K} \\pi_k \\left( \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!} \\prod_{v=1}^{V} \\phi_{k,v}^{n_{d,v}} \\right)$$\n\n**E-Step: Compute Responsibilities**\n\nThe expectation step computes the posterior probability of the latent topic assignment for each document, given the observed word counts $\\mathbf{n}_d$ and the current parameter estimates $\\Theta^{(t)}$. These posterior probabilities are called responsibilities, denoted by $\\gamma_{d,k}$.\n$$\\gamma_{d,k} = p(z_{d}=k \\mid \\mathbf{n}_{d}, \\Theta^{(t)})$$\nUsing Bayes' theorem:\n$$\\gamma_{d,k} = \\frac{p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) p(z_{d}=k \\mid \\Theta^{(t)})}{p(\\mathbf{n}_{d} \\mid \\Theta^{(t)})} = \\frac{p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) p(z_{d}=k \\mid \\Theta^{(t)})}{\\sum_{j=1}^{K} p(\\mathbf{n}_{d} \\mid z_{d}=j, \\Theta^{(t)}) p(z_{d}=j \\mid \\Theta^{(t)})}$$\nSubstituting the model definitions, $p(z_{d}=k \\mid \\Theta^{(t)}) = \\pi_k^{(t)}$ and $p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) = C_d \\prod_{v=1}^{V} (\\phi_{k,v}^{(t)})^{n_{d,v}}$, where $C_d = \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!}$ is the multinomial coefficient. Since $C_d$ does not depend on the topic index $k$ or the parameters, it cancels out from the numerator and denominator.\n$$\\gamma_{d,k} = \\frac{\\pi_k^{(t)} \\prod_{v=1}^{V} (\\phi_{k,v}^{(t)})^{n_{d,v}}}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\prod_{v=1}^{V} (\\phi_{j,v}^{(t)})^{n_{d,v}}}$$\nThis is the update for the responsibilities in the E-step.\n\n**M-Step: Maximize Expected Complete-Data Log-Likelihood**\n\nThe maximization step finds the new parameter estimates $\\Theta^{(t+1)}$ that maximize the expected complete-data log-likelihood, $Q(\\Theta \\mid \\Theta^{(t)})$. The complete-data log-likelihood is:\n$$\\ln p(\\{\\mathbf{n}_d, z_d\\}_{d=1}^D \\mid \\Theta) = \\sum_{d=1}^{D} \\ln p(\\mathbf{n}_d, z_d \\mid \\Theta) = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\mathbb{I}(z_d=k) \\ln \\left( \\pi_k p(\\mathbf{n}_d \\mid z_d=k, \\Theta) \\right)$$\n$$= \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\mathbb{I}(z_d=k) \\left( \\ln \\pi_k + \\ln C_d + \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} \\right)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The expectation $Q(\\Theta \\mid \\Theta^{(t)})$ is taken with respect to the posterior $p(\\{z_d\\} \\mid \\{\\mathbf{n}_d\\}, \\Theta^{(t)})$. The expectation of the indicator function is the responsibility: $E[\\mathbb{I}(z_d=k)] = p(z_d=k \\mid \\mathbf{n}_d, \\Theta^{(t)}) = \\gamma_{d,k}$.\n$$Q(\\Theta \\mid \\Theta^{(t)}) = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\gamma_{d,k} \\left( \\ln \\pi_k + \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} \\right) + \\text{const.}$$\nThe term $\\sum_{d=1}^D \\sum_{k=1}^K \\gamma_{d,k} \\ln C_d$ is constant with respect to $\\Theta$ and can be ignored during maximization. We must maximize $Q$ subject to the constraints $\\sum_{k=1}^{K} \\pi_k = 1$ and $\\sum_{v=1}^{V} \\phi_{k,v} = 1$ for all $k \\in \\{1,\\dots,K\\}$.\n\nUpdate for $\\pi_k$: We form the Lagrangian for the terms involving $\\boldsymbol{\\pi}$:\n$$\\mathcal{L}_{\\pi} = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\gamma_{d,k} \\ln \\pi_k + \\lambda_{\\pi} \\left( \\sum_{k=1}^{K} \\pi_k - 1 \\right)$$\nTaking the derivative with respect to $\\pi_k$ and setting it to zero:\n$$\\frac{\\partial \\mathcal{L}_{\\pi}}{\\partial \\pi_k} = \\frac{1}{\\pi_k} \\sum_{d=1}^{D} \\gamma_{d,k} + \\lambda_{\\pi} = 0 \\implies \\pi_k = -\\frac{\\sum_{d=1}^{D} \\gamma_{d,k}}{\\lambda_{\\pi}}$$\nSumming over $k$: $\\sum_{k=1}^{K} \\pi_k = 1 = -\\frac{1}{\\lambda_{\\pi}} \\sum_{k=1}^{K} \\sum_{d=1}^{D} \\gamma_{d,k}$. Since $\\sum_{k=1}^{K} \\gamma_{d,k} = 1$, the double summation equals $D$. Thus, $1 = -D/\\lambda_{\\pi}$, so $\\lambda_{\\pi} = -D$. Substituting back gives the update rule:\n$$\\pi_k^{(t+1)} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k}}{D}$$\n\nUpdate for $\\phi_{k,v}$: The terms involving $\\boldsymbol{\\phi}_k$ are independent for each topic $k$. For a fixed $k$, we form the Lagrangian:\n$$\\mathcal{L}_{\\phi_k} = \\sum_{d=1}^{D} \\gamma_{d,k} \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} + \\lambda_{\\phi_k} \\left( \\sum_{v=1}^{V} \\phi_{k,v} - 1 \\right)$$\nTaking the derivative with respect to $\\phi_{k,v}$ and setting it to zero:\n$$\\frac{\\partial \\mathcal{L}_{\\phi_k}}{\\partial \\phi_{k,v}} = \\frac{1}{\\phi_{k,v}} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v} + \\lambda_{\\phi_k} = 0 \\implies \\phi_{k,v} = -\\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\lambda_{\\phi_k}}$$\nSumming over $v$: $\\sum_{v=1}^{V} \\phi_{k,v} = 1 = -\\frac{1}{\\lambda_{\\phi_k}} \\sum_{v=1}^{V} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}$. Let the double summation be $S_k = \\sum_{d=1}^{D} \\gamma_{d,k} \\sum_{v=1}^{V} n_{d,v} = \\sum_{d=1}^{D} \\gamma_{d,k} N_d$. This is the effective total number of words assigned to topic $k$. Then $1 = -S_k/\\lambda_{\\phi_k}$, so $\\lambda_{\\phi_k} = -S_k$. Substituting back gives the update rule:\n$$\\phi_{k,v}^{(t+1)} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\sum_{d=1}^{D} \\gamma_{d,k} N_d} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\sum_{j=1}^{V} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,j}}$$\n\n### Part 2: Conceptual and Modeling Limitations\n\nThe mixture of unigrams (MoU) model, while a foundational topic model, has significant limitations compared to more advanced models like Latent Dirichlet Allocation (LDA), and its probabilistic nature distinguishes it from heuristic methods like Term Frequency–Inverse Document Frequency (TF-IDF).\n\n- **Mixture of Unigrams vs. Latent Dirichlet Allocation (LDA)**: The primary distinction lies in the modeling of document-level topic composition. MoU assumes that each document is generated from exactly one topic. This \"single-topic-per-document\" assumption severely restricts its expressivity. A document (e.g., a scientific paper, a news article) often discusses multiple themes. LDA addresses this by assuming each document is a mixture of topics. For each word token within a document, a topic is chosen from that document's specific topic mixture, and then the word is drawn from the selected topic's word distribution. This allows a document to be represented by a proportion vector over topics (e.g., $40\\%$ physics, $30\\%$ computer science, $30\\%$ mathematics), which is far more realistic for most real-world corpora. This token-level topic assignment is a more fine-grained and powerful representation than the document-level assignment in MoU. The bag-of-words setup, which discards word order, is common to both models, but LDA leverages it more effectively by modeling topical mixtures.\n\n- **TF-IDF vs. Probabilistic Models**: TF-IDF is a numerical weighting scheme, not a probabilistic generative model. Its value for a term $t$ in a document $d$, $\\text{TF-IDF}(t,d)$, is a heuristic score designed to reflect the term's importance to that document within a corpus. In contrast, probabilistic models like MoU and LDA provide a generative story for how the document was created. The parameters of these models (e.g., $\\boldsymbol{\\phi}_k$) are probabilities with a clear interpretation. Naively substituting TF-IDF weights, which are non-integer real values, for the integer word counts $n_{d,v}$ in the multinomial likelihood function $p(\\mathbf{n}_{d} \\mid ...)=C_d\\prod_{v} \\phi_{k,v}^{n_{d,v}}$ fundamentally breaks the model. The multinomial distribution is defined for discrete counts. Using real-valued exponents $\\phi_{k,v}^{\\text{TF-IDF}_{d,v}}$ would change the function, but it would no longer represent the likelihood of observed counts under a multinomial model. The statistical foundation, including the generative process and the principled EM-based parameter estimation, would be invalidated.\n\n### Part 3: Convergence Analysis\n\nThe EM algorithm is guaranteed to produce a sequence of parameter estimates $\\Theta^{(t)}$ for which the observed-data log-likelihood $\\mathcal{L}(\\Theta^{(t)}) = \\ln p(X \\mid \\Theta^{(t)})$ is non-decreasing. This can be proven by introducing the evidence lower bound (ELBO) on the log-likelihood. For any auxiliary distribution $q(Z)$ over the latent variables $Z$, Jensen's inequality for the concave logarithm function allows us to write:\n$$\\mathcal{L}(\\Theta) = \\ln \\sum_Z p(X, Z \\mid \\Theta) = \\ln \\sum_Z q(Z)\\frac{p(X, Z \\mid \\Theta)}{q(Z)} \\geq \\sum_Z q(Z) \\ln \\frac{p(X, Z \\mid \\Theta)}{q(Z)} = \\mathcal{F}(q, \\Theta)$$\nThe EM algorithm can be viewed as a coordinate ascent algorithm on this lower bound $\\mathcal{F}(q, \\Theta)$.\n1.  **E-Step**: With parameters $\\Theta^{(t)}$ fixed, we maximize $\\mathcal{F}(q, \\Theta^{(t)})$ with respect to $q(Z)$. This maximum is achieved when $q(Z)$ is set to the posterior distribution of the latent variables, $p(Z \\mid X, \\Theta^{(t)})$. At this point, the lower bound is tight, i.e., $\\mathcal{L}(\\Theta^{(t)}) = \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t)})$.\n2.  **M-Step**: With $q(Z) = p(Z \\mid X, \\Theta^{(t)})$ fixed, we find new parameters $\\Theta^{(t+1)}$ by maximizing $\\mathcal{F}(q, \\Theta)$ with respect to $\\Theta$. By definition of maximization, $\\mathcal{F}(q, \\Theta^{(t+1)}) \\geq \\mathcal{F}(q, \\Theta^{(t)})$.\n\nCombining these steps, we have the sequence:\n$$\\mathcal{L}(\\Theta^{(t+1)}) \\geq \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t+1)}) \\geq \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t)}) = \\mathcal{L}(\\Theta^{(t)})$$\nThus, $\\mathcal{L}(\\Theta^{(t+1)}) \\geq \\mathcal{L}(\\Theta^{(t)})$.\n\nHowever, the log-likelihood surface for mixture models is generally non-convex. EM is a deterministic hill-climbing procedure on this surface. While it is guaranteed to find a stationary point (a local maximum or saddle point), it is not guaranteed to find the global maximum. The final parameters it converges to are sensitive to the initial parameter settings $\\Theta^{(0)}$.\n\n### Part 4: Numerical Calculation\n\nGiven: $V=3$, $K=2$, $D=2$.\n- Document $d=1$: $\\mathbf{n}_{1} = (2, 1, 0)$, $N_1 = 3$.\n- Document $d=2$: $\\mathbf{n}_{2} = (0, 1, 2)$, $N_2 = 3$.\nInitial parameters $\\Theta^{(0)}$:\n- $\\boldsymbol{\\pi}^{(0)} = (0.5, 0.5)$.\n- $\\boldsymbol{\\phi}_{1}^{(0)} = (0.6, 0.3, 0.1)$.\n- $\\boldsymbol{\\phi}_{2}^{(0)} = (0.1, 0.3, 0.6)$.\n\n**E-Step: Compute responsibilities $\\gamma_{d,k}$**\n\nFirst, we compute the unnormalized probabilities $\\alpha_{d,k} = \\pi_k^{(0)} \\prod_{v=1}^{V} (\\phi_{k,v}^{(0)})^{n_{d,v}}$.\nFor document $d=1$:\n- $\\alpha_{1,1} = 0.5 \\times (0.6^2 \\times 0.3^1 \\times 0.1^0) = 0.5 \\times (0.36 \\times 0.3) = 0.5 \\times 0.108 = 0.054$.\n- $\\alpha_{1,2} = 0.5 \\times (0.1^2 \\times 0.3^1 \\times 0.6^0) = 0.5 \\times (0.01 \\times 0.3) = 0.5 \\times 0.003 = 0.0015$.\nThe normalization constant is $\\sum_j \\alpha_{1,j} = 0.054 + 0.0015 = 0.0555$.\n- $\\gamma_{1,1} = \\frac{0.054}{0.0555} = \\frac{540}{555} = \\frac{36}{37}$.\n- $\\gamma_{1,2} = \\frac{0.0015}{0.0555} = \\frac{15}{555} = \\frac{1}{37}$.\n\nFor document $d=2$:\n- $\\alpha_{2,1} = 0.5 \\times (0.6^0 \\times 0.3^1 \\times 0.1^2) = 0.5 \\times (0.3 \\times 0.01) = 0.5 \\times 0.003 = 0.0015$.\n- $\\alpha_{2,2} = 0.5 \\times (0.1^0 \\times 0.3^1 \\times 0.6^2) = 0.5 \\times (0.3 \\times 0.36) = 0.5 \\times 0.108 = 0.054$.\nThe normalization constant is $\\sum_j \\alpha_{2,j} = 0.0015 + 0.054 = 0.0555$.\n- $\\gamma_{2,1} = \\frac{0.0015}{0.0555} = \\frac{15}{555} = \\frac{1}{37}$.\n- $\\gamma_{2,2} = \\frac{0.054}{0.0555} = \\frac{540}{555} = \\frac{36}{37}$.\n\n**M-Step: Update parameters**\n\nWe need to compute the updated value of $\\phi_{1,1}$, denoted $\\phi_{1,1}^{(1)}$.\nUsing the derived formula:\n$$\\phi_{1,1}^{(1)} = \\frac{\\sum_{d=1}^{2} \\gamma_{d,1} n_{d,1}}{\\sum_{d=1}^{2} \\gamma_{d,1} N_d}$$\nNumerator calculation:\n$$\\sum_{d=1}^{2} \\gamma_{d,1} n_{d,1} = \\gamma_{1,1} n_{1,1} + \\gamma_{2,1} n_{2,1} = \\left(\\frac{36}{37}\\right) \\times 2 + \\left(\\frac{1}{37}\\right) \\times 0 = \\frac{72}{37}$$\nDenominator calculation:\n$$\\sum_{d=1}^{2} \\gamma_{d,1} N_d = \\gamma_{1,1} N_1 + \\gamma_{2,1} N_2 = \\left(\\frac{36}{37}\\right) \\times 3 + \\left(\\frac{1}{37}\\right) \\times 3 = \\frac{108}{37} + \\frac{3}{37} = \\frac{111}{37} = 3$$\nThus, the updated parameter is:\n$$\\phi_{1,1}^{(1)} = \\frac{72/37}{3} = \\frac{72}{37 \\times 3} = \\frac{24}{37}$$\nAs a decimal rounded to four significant figures:\n$$\\phi_{1,1}^{(1)} \\approx 0.6486486... \\approx 0.6486$$", "answer": "$$\n\\boxed{0.6486}\n$$", "id": "3179899"}, {"introduction": "This final practice challenges you to synthesize your understanding of both heuristic and probabilistic methods to solve a practical problem: detecting anomalous documents. You will design a system that flags documents containing words from unseen topics by combining signals from an IDF-based heuristic and a probabilistic model's posterior confidence. This exercise illustrates how different text analysis tools can be used in a complementary fashion to build robust, real-world applications. [@problem_id:3179937]", "problem": "Design and implement a program that detects rare topic leakage in documents using two complementary signals derived from first principles of probabilistic text models and the definition of inverse document frequency. Rare topic leakage means that a document draws a few words from an unseen topic not present in training, which should cause anomalies in the posterior distribution over known topics and an over-representation of high inverse document frequency words.\n\nStart from the following bases:\n- The bag-of-words representation treats each document as counts over a fixed vocabulary, and the document likelihood under a single latent topic is given by a multinomial model with topic-specific categorical probabilities. The topic prior is categorical. From these definitions, derive the posterior over the topic given the bag-of-words counts and define an anomaly score as one minus the maximum posterior mass, which indicates how little confidence the model has in any single known topic.\n- Inverse document frequency (IDF) is a well-tested statistic that up-weights rare words across a training set. Use the smoothed inverse document frequency definition $idf(w) = \\ln\\!\\left(\\dfrac{1+N}{1+df(w)}\\right) + 1$, where $N$ is the number of training documents and $df(w)$ is the number of training documents containing word $w$ at least once.\n\nYour task:\n- Compute, for each test document, two boolean flags:\n  1. A posterior anomaly flag indicating whether the posterior distribution over known topics is anomalously diffuse: compute the posterior $p(z \\mid d)$ from first principles under the single-topic bag-of-words model with a categorical prior and a multinomial likelihood; define the anomaly score $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$ and flag if $A_{\\text{post}}(d) \\ge \\gamma$.\n  2. An inverse document frequency flag indicating whether a document contains a large fraction of high-$idf$ terms: compute the fraction $f_{\\text{high}}(d)$ of tokens in the document whose $idf$ value is at least a fixed threshold $t_{\\text{high}}$; flag if $f_{\\text{high}}(d) \\ge \\tau$.\n\nUse the following fully specified setting.\n\nVocabulary, topic-word distributions, and topic prior:\n- Vocabulary of size $V = 12$ with indices and labels: $0\\to$ \"common0\", $1\\to$ \"common1\", $2\\to$ \"t0a\", $3\\to$ \"t0b\", $4\\to$ \"t0c\", $5\\to$ \"t0d\", $6\\to$ \"t1a\", $7\\to$ \"t1b\", $8\\to$ \"t1c\", $9\\to$ \"t1d\", $10\\to$ \"rareA\", $11\\to$ \"rareB\".\n- Number of known topics $K = 2$. Topic prior (categorical) over known topics is uniform: $\\pi = [0.5, 0.5]$.\n- Topic-word probabilities for known topics (each row sums to $1$):\n  - Topic $0$: $\\phi_{0} = [0.1, 0.1, 0.185, 0.185, 0.185, 0.185, 0.014, 0.014, 0.014, 0.014, 0.002, 0.002]$.\n  - Topic $1$: $\\phi_{1} = [0.1, 0.1, 0.014, 0.014, 0.014, 0.014, 0.185, 0.185, 0.185, 0.185, 0.002, 0.002]$.\n- Posterior anomaly threshold: $\\gamma = 0.3$.\n\nTraining corpus for inverse document frequency computation:\n- Number of training documents $N = 6$.\n- Training documents are given by lists of vocabulary indices:\n  - $d_{\\text{train},0} = [2, 3, 4, 5, 2, 3, 0, 1]$,\n  - $d_{\\text{train},1} = [3, 5, 2, 4, 0]$,\n  - $d_{\\text{train},2} = [6, 7, 8, 9, 6, 7, 0, 1]$,\n  - $d_{\\text{train},3} = [6, 8, 9, 7, 1]$,\n  - $d_{\\text{train},4} = [0, 1, 0, 1]$,\n  - $d_{\\text{train},5} = [2, 6, 3, 7, 0, 1]$.\n- Use the smoothed inverse document frequency definition $idf(w) = \\ln\\!\\left(\\dfrac{1+N}{1+df(w)}\\right) + 1$ with natural logarithm.\n- High-$idf$ threshold and fraction threshold: $t_{\\text{high}} = 2.5$, $\\tau = 0.25$.\n\nTest suite (five documents), each given as a list of vocabulary indices:\n- Test $1$ (happy path, mostly topic $0$ words): $[2, 2, 3, 4, 5, 0, 1, 2]$.\n- Test $2$ (mild leakage: few unseen words embedded in topic $0$ content): $[2, 3, 10, 0, 2, 1, 11, 10, 2]$.\n- Test $3$ (strong leakage: many unseen words plus a common word): $[10, 10, 11, 11, 0]$.\n- Test $4$ (boundary: only unseen words): $[10, 11, 10, 11, 10]$.\n- Test $5$ (ambiguous: only common words): $[0, 1]$.\n\nImplementation requirements:\n- Represent each document using bag-of-words counts over the fixed vocabulary.\n- For the posterior anomaly flag, start strictly from the categorical prior over topics and the multinomial document likelihood under a single latent topic assumption to derive $p(z \\mid d)$ and then compute $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$. Flag if $A_{\\text{post}}(d) \\ge \\gamma$.\n- For the inverse document frequency flag, compute $idf(w)$ from the training corpus as defined, compute the fraction $f_{\\text{high}}(d)$ of tokens in the test document whose $idf(w) \\ge t_{\\text{high}}$, and flag if $f_{\\text{high}}(d) \\ge \\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a $2$-element list of booleans in the order $[posterior\\_anomaly\\_flag, idf\\_flag]$. For example, the overall output should look like $[[b_{1}, b_{2}], [b_{3}, b_{4}], \\ldots]$ with $b_{i} \\in \\{\\text{True}, \\text{False}\\}$.\n- No external input is provided at runtime; all data must be embedded in the program.", "solution": "We begin by formalizing the two detection signals from first principles and core definitions.\n\nPosterior anomaly from a single-topic bag-of-words model:\n- Start with a categorical prior over topics $p(z) = \\pi_{z}$ for $z \\in \\{0, 1, \\ldots, K-1\\}$ and a multinomial document likelihood under a single latent topic $z$, where the word probabilities are given by the topic-word distribution $\\phi_{z} \\in \\Delta^{V-1}$. Given a document $d$ represented as bag-of-words counts $\\{c_{w}\\}_{w=0}^{V-1}$ with total token count $n = \\sum_{w=0}^{V-1} c_{w}$, the multinomial likelihood is proportional to $\\prod_{w=0}^{V-1} \\phi_{z}(w)^{c_{w}}$, up to the multinomial coefficient which cancels across topics when computing the posterior over $z$.\n- By Bayes’ rule using these definitions, the unnormalized posterior over topics is $p(z \\mid d) \\propto p(z)\\,p(d \\mid z) \\propto \\pi_{z}\\,\\prod_{w=0}^{V-1} \\phi_{z}(w)^{c_{w}}$. To avoid numerical underflow in practice, it is standard to compute in the log domain: $\\log \\tilde{p}(z \\mid d) = \\log \\pi_{z} + \\sum_{w=0}^{V-1} c_{w} \\log \\phi_{z}(w)$, then subtract the maximum over $z$ and exponentiate to normalize by a softmax, yielding $p(z \\mid d)$.\n- Define the anomaly score $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$. This score is small when one topic clearly explains the document and large when the posterior is diffuse (for example, uniform at $1/K$). Given a threshold $\\gamma$, we flag a posterior anomaly if $A_{\\text{post}}(d) \\ge \\gamma$.\n\nInverse document frequency flag:\n- For the training corpus of $N$ documents, compute document frequency $df(w)$, the number of training documents that contain word $w$ at least once.\n- Use the smoothed inverse document frequency definition $idf(w) = \\ln\\!\\left(\\dfrac{1+N}{1+df(w)}\\right) + 1$ with the natural logarithm, which is a core, widely used definition. This smoothing avoids division by zero when $df(w) = 0$.\n- For a test document $d$ with token sequence, compute the fraction $f_{\\text{high}}(d)$ of tokens whose $idf$ meets or exceeds a fixed high threshold $t_{\\text{high}}$. If $f_{\\text{high}}(d) \\ge \\tau$ for a chosen $\\tau$, we flag an inverse document frequency anomaly. This operationalizes the intuition that unseen-topic leakage injects rare terms absent from training, which are precisely those with large $idf$.\n\nConcrete instantiation:\n- We use $K = 2$ topics, uniform prior $\\pi = [0.5, 0.5]$, and vocabulary size $V = 12$. The topic-word distributions are specified numerically and place most mass on disjoint topic-specific word subsets, assign moderate mass to two common words (indices $0$ and $1$), and tiny mass to two rare words (indices $10$ and $11$) to model unseen-topic-like tokens. This ensures that words from the unseen topic do not favor either known topic, thus spreading the posterior and enlarging $A_{\\text{post}}(d)$ when such words are prevalent.\n- The posterior is computed in the log domain as $\\log \\tilde{p}(z \\mid d) = \\log \\pi_{z} + \\sum_{w} c_{w} \\log \\phi_{z}(w)$, then normalized via a softmax across $z$. The anomaly score $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$ is compared against $\\gamma = 0.3$.\n- For $idf$, we compute $df(w)$ over $N = 6$ training documents, then $idf(w) = \\ln\\!\\left(\\dfrac{1+6}{1+df(w)}\\right) + 1$. Words absent from training (indices $10$ and $11$) will have the largest $idf$ values (approximately $2.9459$), while common words (indices $0$ and $1$) will have $idf$ equal to $1$. We set $t_{\\text{high}} = 2.5$ to capture only the truly unseen words and $\\tau = 0.25$ so that a document is flagged when at least one quarter of its tokens are such high-$idf$ words.\n\nTest suite coverage and expected qualitative behavior:\n- Test $1$ (mostly topic $0$ words) should yield a concentrated posterior (small $A_{\\text{post}}$) and few or no high-$idf$ tokens, so both flags are expected to be false.\n- Test $2$ (mild leakage) contains a few unseen words; the posterior remains concentrated on topic $0$ (posterior flag false), but the high-$idf$ fraction exceeds $\\tau$ (inverse document frequency flag true).\n- Test $3$ and Test $4$ (strong leakage and all unseen) induce diffuse posteriors near uniform over the known topics (posterior flag true) and have large high-$idf$ fractions (inverse document frequency flag true).\n- Test $5$ (only common words) yields a near-uniform posterior because such words do not discriminate between topics (posterior flag true), but contains no high-$idf$ tokens (inverse document frequency flag false).\n\nAlgorithmic steps implemented by the program:\n- Hard-code $\\phi$, $\\pi$, the training documents, thresholds $\\gamma$, $t_{\\text{high}}$, $\\tau$, and the five test documents.\n- Compute $df(w)$ by counting in how many training documents each word appears at least once, then compute $idf(w)$ via the provided definition.\n- For each test document, compute bag-of-words counts, evaluate $\\log \\tilde{p}(z \\mid d)$ for each $z \\in \\{0,1\\}$, normalize to obtain $p(z \\mid d)$, compute $A_{\\text{post}}(d)$, and flag if $A_{\\text{post}}(d) \\ge \\gamma$. Also compute $f_{\\text{high}}(d)$ as the fraction of tokens with $idf(w) \\ge t_{\\text{high}}$, and flag if $f_{\\text{high}}(d) \\ge \\tau$.\n- Aggregate per-document results as $[posterior\\_flag, idf\\_flag]$ booleans in the specified order and print a single-line list of these pairs.\n\nThis procedure is directly rooted in the categorical prior and multinomial likelihood definitions for bag-of-words documents and the inverse document frequency definition, with careful numerical computation in the log domain to ensure stability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_idf(training_docs, V):\n    \"\"\"\n    Compute smoothed inverse document frequency:\n    idf(w) = ln((1 + N) / (1 + df(w))) + 1\n    where df(w) is the number of training documents containing w.\n    \"\"\"\n    N = len(training_docs)\n    df = np.zeros(V, dtype=int)\n    for doc in training_docs:\n        if len(doc) == 0:\n            continue\n        unique_words = set(doc)\n        for w in unique_words:\n            df[w] += 1\n    idf = np.log((1 + N) / (1 + df.astype(float))) + 1.0\n    return idf\n\ndef posterior_anomaly_flag(doc, phi, pi, gamma, V):\n    \"\"\"\n    Compute posterior over topics under single-topic multinomial model:\n    p(z|d) ∝ pi[z] * product_w phi[z,w]^{c_w}\n    Implemented in log domain and normalized by softmax.\n    Anomaly score A_post = 1 - max_z p(z|d). Flag if >= gamma.\n    \"\"\"\n    K = phi.shape[0]\n    # Bag-of-words counts\n    counts = np.zeros(V, dtype=float)\n    for w in doc:\n        counts[w] += 1.0\n    # Compute log unnormalized posterior for each topic\n    # log p ~ log pi + sum_w c_w * log phi[z,w]\n    # Ensure no log(0): phi entries are strictly positive by construction\n    log_phi = np.log(phi)\n    log_pi = np.log(pi)\n    log_scores = np.zeros(K, dtype=float)\n    for z in range(K):\n        log_scores[z] = log_pi[z] + float(np.dot(counts, log_phi[z]))\n    # Softmax normalization\n    m = np.max(log_scores)\n    probs = np.exp(log_scores - m)\n    probs /= probs.sum()\n    p_max = float(np.max(probs))\n    anomaly_score = 1.0 - p_max\n    return anomaly_score >= gamma\n\ndef idf_flag(doc, idf, t_high, tau):\n    \"\"\"\n    Compute fraction of tokens with idf >= t_high and flag if fraction >= tau.\n    \"\"\"\n    if len(doc) == 0:\n        return False\n    high = 0\n    for w in doc:\n        if idf[w] >= t_high:\n            high += 1\n    frac = high / float(len(doc))\n    return frac >= tau\n\ndef solve():\n    # Vocabulary size\n    V = 12\n\n    # Topic-word distributions (K=2), rows sum to 1\n    phi = np.array([\n        [0.1,   0.1,   0.185, 0.185, 0.185, 0.185, 0.014, 0.014, 0.014, 0.014, 0.002, 0.002],  # topic 0\n        [0.1,   0.1,   0.014, 0.014, 0.014, 0.014, 0.185, 0.185, 0.185, 0.185, 0.002, 0.002],  # topic 1\n    ], dtype=float)\n\n    # Topic prior (uniform)\n    pi = np.array([0.5, 0.5], dtype=float)\n\n    # Posterior anomaly threshold\n    gamma = 0.3\n\n    # Training documents for IDF (N=6)\n    training_docs = [\n        [2, 3, 4, 5, 2, 3, 0, 1],\n        [3, 5, 2, 4, 0],\n        [6, 7, 8, 9, 6, 7, 0, 1],\n        [6, 8, 9, 7, 1],\n        [0, 1, 0, 1],\n        [2, 6, 3, 7, 0, 1],\n    ]\n\n    # Compute IDF\n    idf = compute_idf(training_docs, V)\n\n    # High-idf threshold and fraction threshold\n    t_high = 2.5\n    tau = 0.25\n\n    # Test cases: five documents\n    test_cases = [\n        [2, 2, 3, 4, 5, 0, 1, 2],             # Test 1\n        [2, 3, 10, 0, 2, 1, 11, 10, 2],       # Test 2\n        [10, 10, 11, 11, 0],                  # Test 3\n        [10, 11, 10, 11, 10],                 # Test 4\n        [0, 1],                               # Test 5\n    ]\n\n    results = []\n    for doc in test_cases:\n        post_flag = posterior_anomaly_flag(doc, phi, pi, gamma, V)\n        idf_flag_bool = idf_flag(doc, idf, t_high, tau)\n        results.append([post_flag, idf_flag_bool])\n\n    # Final print statement in the exact required format.\n    # This prints a single line: list of [posterior_flag, idf_flag] per test case.\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3179937"}]}