{"hands_on_practices": [{"introduction": "The Kaplan-Meier estimator is a cornerstone of survival analysis, providing a non-parametric estimate of the survival function. However, the reliability of this estimate, quantified by its variance, is not uniform and depends heavily on the structure of the data. This exercise [@problem_id:3179136] challenges you to think critically about how censoring—the loss of subjects from a study for reasons other than the event of interest—impacts the precision of your survival estimates. By comparing two scenarios with different levels of censoring, you will develop a deep intuition for Greenwood's formula and understand why smaller risk sets lead to greater uncertainty.", "problem": "A cohort study follows $n=100$ independent subjects from baseline. Consider estimating the survival function at $t=9$ months using the nonparametric Kaplan–Meier (KM) estimator. In both study designs below, there are exactly three distinct event times prior to $t=9$ months at $t_1=3$, $t_2=6$, and $t_3=9$ months with event counts $d_1=5$, $d_2=4$, and $d_3=3$, respectively. No ties between events and censorings occur exactly at the same recorded time. The designs differ only in how many subjects are censored between event times, as follows:\n\n- Design L (lower censoring): between $0$ and $3$ months, $c_{(0,3)}=0$ are censored; between $3$ and $6$ months, $c_{(3,6)}=2$ are censored; between $6$ and $9$ months, $c_{(6,9)}=3$ are censored. There are no other losses before $t=9$ months.\n- Design H (higher censoring): between $0$ and $3$ months, $c_{(0,3)}=10$ are censored; between $3$ and $6$ months, $c_{(3,6)}=10$ are censored; between $6$ and $9$ months, $c_{(6,9)}=10$ are censored. There are no other losses before $t=9$ months.\n\nUsing only fundamental definitions of the KM estimator, risk sets, and well-tested large-sample variance behavior for the KM estimator at a fixed time, decide which statement best characterizes how the variance at $t=9$ months compares between Design H and Design L. Choose the single best answer.\n\nA. The variance at $t=9$ months is smaller in Design H because fewer individuals contribute information, leading to reduced variability.\n\nB. The variance at $t=9$ months is larger in Design H because increased censoring before $t=9$ months reduces the risk set sizes at the event times, inflating the variability of the product-limit estimator.\n\nC. The variance at $t=9$ months is the same in both designs because the KM estimator and its variability depend only on the event counts and event times, not on censoring.\n\nD. The variance at $t=9$ months is undefined in Design H because heavy censoring invalidates nonparametric survival estimation before $t=9$ months.", "solution": "The variance of the Kaplan-Meier (KM) estimator is key to understanding its precision. The most common method for estimating this variance is Greenwood's formula:\n$$ \\widehat{\\text{Var}}(\\hat{S}(t)) \\approx [\\hat{S}(t)]^2 \\sum_{i: t_i \\le t} \\frac{d_i}{n_i(n_i - d_i)} $$\nwhere $d_i$ is the number of events at event time $t_i$, and $n_i$ is the number of subjects at risk just before $t_i$.\n\nTo compare the variance at $t=9$ months for Design L and Design H, we first need to calculate the risk set size $n_i$ at each event time ($t=3, 6, 9$) for both designs. The initial number of subjects is $n=100$.\n\n**For Design L (lower censoring):**\n- At $t_1=3$ months: $n_{1,L} = 100$ (no censoring before $t_1$). 5 events occur.\n- At $t_2=6$ months: $n_{2,L} = 100 - 5 (\\text{events}) - 2 (\\text{censored}) = 93$. 4 events occur.\n- At $t_3=9$ months: $n_{3,L} = 93 - 4 (\\text{events}) - 3 (\\text{censored}) = 86$.\n\n**For Design H (higher censoring):**\n- At $t_1=3$ months: $n_{1,H} = 100 - 10 (\\text{censored}) = 90$. 5 events occur.\n- At $t_2=6$ months: $n_{2,H} = 90 - 5 (\\text{events}) - 10 (\\text{censored}) = 75$. 4 events occur.\n- At $t_3=9$ months: $n_{3,H} = 75 - 4 (\\text{events}) - 10 (\\text{censored}) = 61$.\n\nComparing the risk sets, we see that $n_{i,H}  n_{i,L}$ for all three event times.\n\nLet's examine the terms in Greenwood's formula. Each term in the summation, $\\frac{d_i}{n_i(n_i-d_i)}$, increases as the risk set size $n_i$ decreases (since $d_i$ is the same for both designs). Because the risk sets are smaller in Design H, each term in the sum is larger for Design H than for Design L. Consequently, the entire summation term is larger for Design H.\n\nHeavier censoring reduces the number of subjects at risk, which means there is less information available to estimate the survival probabilities at later time points. This loss of information leads to greater uncertainty, which is reflected as a larger variance. Therefore, the variance of the KM estimator at $t=9$ months is larger in Design H. This reasoning supports option B. Increased censoring reduces the risk set sizes, which inflates the variance of the estimator.", "answer": "$$\\boxed{B}$$", "id": "3179136"}, {"introduction": "While visual comparison of survival curves is informative, a quantitative summary is often required to compare treatments or groups. The Restricted Mean Survival Time (RMST) offers a powerful and clinically interpretable metric, representing the average event-free time up to a specified horizon $\\tau$. This hands-on problem [@problem_id:3179108] guides you through the process of computing the RMST, which is simply the area under the survival curve. By translating this geometric concept into a computational algorithm, you will gain a practical tool for comparing survival distributions, particularly in situations where the proportional hazards assumption is violated and survival curves may cross.", "problem": "You are given two treatment arms, A and B, each with an estimated survival curve represented as a right-continuous, nonincreasing, piecewise-constant step function on a finite grid of times. The aim is to compute the restricted mean survival time (RMST) up to a finite time horizon and then compare the treatment arms in a way that remains valid under non-proportional hazards (NPH). The comparison should be made via the difference in restricted mean survival time at the horizon, which summarizes the overall average survival up to that time without relying on the proportional hazards assumption.\n\nFundamental base:\n- The survival function is defined as $S(t)=\\mathbb{P}(Tt)$ for a nonnegative time-to-event random variable $T$.\n- The restricted mean survival time up to a finite horizon $\\,\\tau\\,$ is defined as the area under the survival curve from $\\,t=0\\,$ to $\\,t=\\tau\\,$.\n- For a right-continuous step function $S(t)$ with a finite grid $0=g_0g_1\\dotsg_m$, the function is constant on each interval $[g_k,g_{k+1})$ and takes the value given at $g_k$. If $\\tau$ lies strictly inside an interval, the last interval is truncated at $\\tau$.\n\nProgram input specification:\n- The program should internally store a list of test cases. Each test case consists of:\n  - A strictly increasing time grid $[g_0,g_1,\\dots,g_m]$ with $g_0=0$ and $g_m\\ge \\tau$.\n  - Two lists of survival values, one for arm A and one for arm B, each of length $m+1$. The value at index $k$ is the right-continuous survival level on $[g_k,g_{k+1})$.\n  - A finite horizon $\\tau$ with $0\\le \\tau\\le g_m$.\n- All survival values are in $[0,1]$ and are nonincreasing in the grid index.\n\nTask:\n- For each test case, compute the restricted mean survival time for arm A and for arm B by integrating the provided step functions from $t=0$ up to $t=\\tau$ using the piecewise-constant interpretation described above.\n- Compute the difference $\\Delta=\\mathrm{RMST}_B-\\mathrm{RMST}_A$.\n- Report a preference indicator $I$ defined as:\n  - $I=1$ if $\\Delta0$,\n  - $I=-1$ if $\\Delta0$,\n  - $I=0$ if $\\Delta=0$,\n  using a numerical tolerance of $10^{-12}$ to decide equality.\n\nRequired numerical output format:\n- For each test case, output a list of four values: $[\\mathrm{RMST}_A,\\mathrm{RMST}_B,\\Delta,I]$.\n- The first three values must be floating-point numbers rounded to six decimal places. The last value must be an integer.\n- The program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the bracketed four-value list for a test case. For example: $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots]$.\n\nTest suite to implement inside the program:\n- Case 1 (non-proportional hazards with crossing survival and interior truncation):\n  - Grid: $[0.0,2.0,5.0,8.0,12.0]$\n  - Arm A: $[1.0,0.92,0.80,0.74,0.70]$\n  - Arm B: $[1.0,0.96,0.78,0.77,0.72]$\n  - Horizon: $\\tau=10.0$\n- Case 2 (horizon equals last grid point):\n  - Grid: $[0.0,3.0,6.0,9.0,12.0]$\n  - Arm A: $[1.0,0.90,0.82,0.70,0.60]$\n  - Arm B: $[1.0,0.95,0.75,0.72,0.65]$\n  - Horizon: $\\tau=12.0$\n- Case 3 (small horizon inside the first interval, tie expected):\n  - Grid: $[0.0,4.0,9.0]$\n  - Arm A: $[1.0,0.85,0.70]$\n  - Arm B: $[1.0,0.88,0.68]$\n  - Horizon: $\\tau=2.0$\n- Case 4 (multiple crossings and horizon on a grid point):\n  - Grid: $[0.0,1.0,2.5,4.0,7.5,10.0]$\n  - Arm A: $[1.0,0.97,0.90,0.86,0.80,0.78]$\n  - Arm B: $[1.0,0.99,0.91,0.85,0.79,0.76]$\n  - Horizon: $\\tau=7.5$\n\nYour program should compute the four requested values for each case and print them in the exact required format in a single line as described above. No user input is involved; all data are embedded in the program.", "solution": "The primary objective is to compute the Restricted Mean Survival Time (RMST) for two treatment arms, A and B, up to a specified time horizon $\\tau$. The RMST is a robust measure for comparing survival distributions, particularly when the proportional hazards assumption is not met. The survival function for each arm, $S(t)$, is provided as a right-continuous, piecewise-constant step function.\n\nThe RMST up to a time $\\tau > 0$ is defined as the total area under the survival curve from time $t=0$ to $t=\\tau$:\n$$\n\\mathrm{RMST}(\\tau) = \\int_{0}^{\\tau} S(t) dt\n$$\nThe survival functions, $S_A(t)$ and $S_B(t)$, are defined over a finite, strictly increasing time grid $G = \\{g_0, g_1, \\dots, g_m\\}$, where $g_0=0$. For each arm, a corresponding list of survival probabilities $s = \\{s_0, s_1, \\dots, s_m\\}$ is given. The problem specifies that the survival function $S(t)$ is constant on each interval $[g_k, g_{k+1})$, taking the value $s_k$.\n\nThe integration to compute $\\mathrm{RMST}(\\tau)$ is performed by summing the areas of the rectangles formed by the step function over the interval $[0, \\tau]$. The algorithm proceeds by initializing RMST to 0 and iterating through the intervals $[g_k, g_{k+1})$ defined by the grid points:\n1. Define the interval start $t_{\\text{start}} = g_k$ and end $t_{\\text{end}} = g_{k+1}$.\n2. If the horizon $\\tau$ is less than or equal to $t_{\\text{start}}$, the integration is complete, and the loop can terminate.\n3. Determine the effective end of the integration for the current interval, which is the minimum of the interval end $t_{\\text{end}}$ and the horizon $\\tau$. Let this be $t_{\\text{effective\\_end}} = \\min(\\tau, t_{\\text{end}})$.\n4. The width of the rectangle for this segment is $w_k = t_{\\text{effective\\_end}} - t_{\\text{start}}$.\n5. The survival probability (height of the rectangle) for this segment is $s_k$.\n6. The area to add is $A_k = s_k \\times w_k$.\n7. Add this area to the total: $\\mathrm{RMST} = \\mathrm{RMST} + A_k$.\n\nAfter computing $\\mathrm{RMST}_A$ and $\\mathrm{RMST}_B$ using this method, their difference is calculated:\n$$\n\\Delta = \\mathrm{RMST}_B - \\mathrm{RMST}_A\n$$\nA positive $\\Delta$ favors arm B, while a negative $\\Delta$ favors arm A. Finally, a preference indicator $I$ is defined based on $\\Delta$, using a numerical tolerance of $\\epsilon=10^{-12}$ to handle potential floating-point inaccuracies:\n$$\nI = \\begin{cases}\n1  \\text{if } \\Delta > \\epsilon \\\\\n-1  \\text{if } \\Delta  -\\epsilon \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nThe final output for each test case is a list containing the computed values: $[\\mathrm{RMST}_A, \\mathrm{RMST}_B, \\Delta, I]$, with the first three values rounded to six decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Restricted Mean Survival Time (RMST) problem for a set of predefined test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: non-proportional hazards with crossing survival and interior truncation\n        {\n            \"grid\": np.array([0.0, 2.0, 5.0, 8.0, 12.0]),\n            \"surv_a\": np.array([1.0, 0.92, 0.80, 0.74, 0.70]),\n            \"surv_b\": np.array([1.0, 0.96, 0.78, 0.77, 0.72]),\n            \"tau\": 10.0,\n        },\n        # Case 2: horizon equals last grid point\n        {\n            \"grid\": np.array([0.0, 3.0, 6.0, 9.0, 12.0]),\n            \"surv_a\": np.array([1.0, 0.90, 0.82, 0.70, 0.60]),\n            \"surv_b\": np.array([1.0, 0.95, 0.75, 0.72, 0.65]),\n            \"tau\": 12.0,\n        },\n        # Case 3: small horizon inside the first interval, tie expected\n        {\n            \"grid\": np.array([0.0, 4.0, 9.0]),\n            \"surv_a\": np.array([1.0, 0.85, 0.70]),\n            \"surv_b\": np.array([1.0, 0.88, 0.68]),\n            \"tau\": 2.0,\n        },\n        # Case 4: multiple crossings and horizon on a grid point\n        {\n            \"grid\": np.array([0.0, 1.0, 2.5, 4.0, 7.5, 10.0]),\n            \"surv_a\": np.array([1.0, 0.97, 0.90, 0.86, 0.80, 0.78]),\n            \"surv_b\": np.array([1.0, 0.99, 0.91, 0.85, 0.79, 0.76]),\n            \"tau\": 7.5,\n        },\n    ]\n\n    def calculate_rmst(grid, survival_values, tau):\n        \"\"\"\n        Calculates the RMST for a right-continuous piecewise-constant survival function.\n        \n        Args:\n            grid (np.ndarray): The time grid points.\n            survival_values (np.ndarray): The survival probabilities on each interval.\n            tau (float): The time horizon for restriction.\n            \n        Returns:\n            float: The calculated RMST.\n        \"\"\"\n        rmst = 0.0\n        # Iterate through the intervals defined by the grid\n        for i in range(len(grid) - 1):\n            t_start = grid[i]\n            t_end = grid[i+1]\n            \n            # If the integration horizon is before or at the start of this interval, we are done.\n            if tau = t_start:\n                break\n            \n            # The value of the survival function in this interval [t_start, t_end)\n            s_val = survival_values[i]\n            \n            # Calculate the width of the integration for this interval.\n            # It's the distance from t_start to either tau or t_end, whichever is smaller.\n            integration_end = min(tau, t_end)\n            width = integration_end - t_start\n            \n            # Add the area of this rectangular segment to the total RMST.\n            rmst += s_val * width\n            \n        return rmst\n\n    formatted_results = []\n    tolerance = 1e-12\n\n    for case in test_cases:\n        grid, surv_a, surv_b, tau = case[\"grid\"], case[\"surv_a\"], case[\"surv_b\"], case[\"tau\"]\n\n        rmst_a = calculate_rmst(grid, surv_a, tau)\n        rmst_b = calculate_rmst(grid, surv_b, tau)\n        \n        delta = rmst_b - rmst_a\n        \n        if delta  tolerance:\n            indicator = 1\n        elif delta  -tolerance:\n            indicator = -1\n        else:\n            indicator = 0\n            \n        # Format the result list for this case as a string\n        case_result_str = (\n            f\"[{rmst_a:.6f},{rmst_b:.6f},{delta:.6f},{indicator}]\"\n        )\n        formatted_results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3179108"}, {"introduction": "Survival analysis is not a one-size-fits-all discipline; different models, such as the Cox Proportional Hazards (PH) and Accelerated Failure Time (AFT) models, operate on fundamentally different assumptions. This capstone practice [@problem_id:3179084] immerses you in a realistic simulation study to explore the critical importance of model selection. By generating data where the proportional hazards assumption is explicitly violated, you will fit both a Cox and an AFT model and use the Restricted Mean Survival Time (RMST) to assess which provides more accurate predictions. This exercise powerfully demonstrates the consequences of model misspecification and hones your skills in validating and comparing competing statistical models.", "problem": "Consider a binary covariate $x \\in \\{0,1\\}$ affecting a time-to-event random variable $T$. Let the survival function be defined by $S(t \\mid x) = \\mathbb{P}(T  t \\mid x)$ for $t \\ge 0$, the probability density function by $f(t \\mid x)$, the hazard function by $h(t \\mid x) = \\frac{f(t \\mid x)}{S(t \\mid x)}$, and the cumulative hazard by $H(t \\mid x) = \\int_0^t h(u \\mid x) \\, du$. The Restricted Mean Survival Time (RMST) up to a finite horizon $\\tau  0$ is defined by\n$$\n\\mathrm{RMST}_x(\\tau) = \\int_0^\\tau S(t \\mid x)\\, dt.\n$$\n\nTwo widely used model classes are:\n- The Cox Proportional Hazards (PH) model, which posits $h(t \\mid x) = h_0(t)\\,\\exp(\\beta_{\\mathrm{Cox}} x)$ for an unspecified baseline hazard $h_0(t)$ and a regression coefficient $\\beta_{\\mathrm{Cox}}$.\n- The Accelerated Failure Time (AFT) model, which posits $\\log T = \\mu + \\beta_{\\mathrm{AFT}} x + \\sigma Z$, where $Z \\sim \\mathcal{N}(0,1)$, $\\mu \\in \\mathbb{R}$, $\\beta_{\\mathrm{AFT}} \\in \\mathbb{R}$, and $\\sigma  0$; this implies a log-normal survival distribution and typically violates the proportional hazards assumption when $\\beta_{\\mathrm{AFT}} \\ne 0$.\n\nStarting from the core definitions above, you must:\n1. Simulate right-censored survival data under a log-normal AFT data-generating process with parameters $(\\mu, \\beta, \\sigma)$ and independent exponential censoring with rate $\\lambda_c$. For each individual $i$, draw $x_i \\in \\{0,1\\}$, $Z_i \\sim \\mathcal{N}(0,1)$, set $T_i = \\exp(\\mu + \\beta x_i + \\sigma Z_i)$, draw $C_i \\sim \\mathrm{Exp}(\\lambda_c)$, and observe $Y_i = \\min(T_i, C_i)$ with event indicator $\\delta_i = \\mathbb{I}\\{T_i \\le C_i\\}$.\n2. Fit the Cox PH model by maximizing the partial likelihood for a single binary covariate $x$, and estimate the baseline cumulative hazard using the Breslow estimator. Use these to construct predicted survival functions $\\widehat{S}_{\\mathrm{Cox}}(t \\mid x)$.\n3. Fit the log-normal AFT model by maximum likelihood under right-censoring to estimate $(\\widehat{\\mu}, \\widehat{\\beta}, \\widehat{\\sigma})$, and construct predicted survival functions $\\widehat{S}_{\\mathrm{AFT}}(t \\mid x)$ using the fitted parameters.\n4. Compute the true survival function $S_{\\mathrm{true}}(t \\mid x)$ under the log-normal data-generating process and compute the corresponding true restricted mean survival times $\\mathrm{RMST}^{\\mathrm{true}}_x(\\tau) = \\int_0^\\tau S_{\\mathrm{true}}(t \\mid x) \\, dt$. Numerically approximate all integrals using a sufficiently fine grid on $[0,\\tau]$.\n5. For each model $M \\in \\{\\mathrm{Cox}, \\mathrm{AFT}\\}$ and group $x \\in \\{0,1\\}$, compute the predicted restricted mean survival time $\\mathrm{RMST}^{M}_x(\\tau) = \\int_0^\\tau \\widehat{S}_M(t \\mid x)\\, dt$, and its absolute calibration error with respect to truth, $E^{M}_x(\\tau) = \\left|\\mathrm{RMST}^{M}_x(\\tau) - \\mathrm{RMST}^{\\mathrm{true}}_x(\\tau)\\right|$. Aggregate calibration across the two groups by the mean absolute error,\n$$\n\\overline{E}^{M}(\\tau) = \\frac{E^{M}_0(\\tau) + E^{M}_1(\\tau)}{2}.\n$$\n6. Decide which model yields better RMST$(\\tau)$ calibration by comparing $\\overline{E}^{\\mathrm{AFT}}(\\tau)$ to $\\overline{E}^{\\mathrm{Cox}}(\\tau)$: the model with the smaller value is deemed better calibrated.\n\nYour program must implement the full pipeline described above and evaluate a test suite comprising the following parameter sets. Each test case is a tuple $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed})$, where $n$ is the sample size, $p$ is the probability of $x=1$, $\\mu$ is the log-scale intercept, $\\beta$ is the AFT effect, $\\sigma$ is the log-scale standard deviation, $\\lambda_c$ is the exponential censoring rate, $\\tau$ is the RMST horizon, and $\\text{seed}$ is the random seed:\n- Case $1$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (1000, 0.5, 2.0, 0.7, 0.6, 0.05, 10.0, 1)$.\n- Case $2$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (1000, 0.5, 2.0, 1.0, 0.9, 0.2, 8.0, 2)$.\n- Case $3$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (500, 0.2, 1.5, 0.8, 0.5, 0.1, 6.0, 3)$.\n- Case $4$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (400, 0.5, 1.0, 1.2, 1.0, 0.3, 5.0, 4)$.\n\nNumerical units: treat time in arbitrary units; no physical unit conversion is required. Angles are not involved. Percentages must be expressed as decimals.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case (in the order above), the element must be a boolean value indicating whether the Accelerated Failure Time model yields better RMST$(\\tau)$ calibration than the Cox Proportional Hazards model, i.e., whether $\\overline{E}^{\\mathrm{AFT}}(\\tau)  \\overline{E}^{\\mathrm{Cox}}(\\tau)$. The final output must look like\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4],\n$$\nwhere each $\\text{result}_k \\in \\{\\text{True},\\text{False}\\}$.", "solution": "This solution implements the six-step simulation and modeling pipeline described in the problem statement. Since the data is generated from an Accelerated Failure Time (AFT) model, the Cox Proportional Hazards (PH) model is misspecified, and we expect the AFT model to provide a more accurate fit.\n\n1.  **Data Generation**: For each test case, right-censored survival data is simulated from a log-normal AFT model. This involves generating a binary covariate, true event times based on the AFT model structure, and censoring times from an exponential distribution. The observed data consists of the minimum of the event and censoring times, along with an event indicator.\n\n2.  **Cox Model Fitting**: The Cox PH model coefficient is estimated by maximizing the partial log-likelihood. The baseline cumulative hazard is then estimated using the Breslow estimator. These components are combined to construct the predicted survival functions for the Cox model. The optimization is performed efficiently by sorting the data and using vectorized operations to calculate the sums over risk sets.\n\n3.  **AFT Model Fitting**: The parameters of the log-normal AFT model are estimated by maximizing the full log-likelihood for right-censored data. This involves defining the log-likelihood function based on the log-normal probability density and survival functions and using a numerical optimizer to find the parameters that best fit the data. The resulting parameters are used to construct the AFT model's predicted survival functions.\n\n4.  **True Quantities**: The true survival function is the log-normal survival function defined by the known data-generating parameters. The true Restricted Mean Survival Time (RMST) is calculated by numerically integrating this true survival function up to the horizon $\\tau$.\n\n5.  **RMST Calculation and Error Assessment**: The RMST is calculated for both the Cox-predicted and AFT-predicted survival functions using numerical integration (trapezoidal rule) over a fine time grid. The Mean Absolute Error (MAE) for each model is then computed by comparing its predicted RMSTs (for covariate groups $x=0$ and $x=1$) to the true RMSTs.\n\n6.  **Model Comparison**: The AFT model is determined to be better calibrated if its MAE is less than that of the Cox model. This comparison yields the final boolean result for each test case. As expected, because the AFT model is correctly specified for the simulated data, it consistently demonstrates lower error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation study and print the results.\n    \"\"\"\n\n    def _fit_cox_model(Y, delta, x):\n        \"\"\"\n        Fits a Cox PH model for a single binary covariate using partial likelihood.\n        Implements Breslow's method for handling event times.\n        Returns the coefficient and a function for the Breslow baseline cumulative hazard.\n        \"\"\"\n        sort_idx = np.argsort(Y)\n        Y_sorted = Y[sort_idx]\n        delta_sorted = delta[sort_idx]\n        x_sorted = x[sort_idx]\n\n        def neg_log_partial_likelihood(beta, Y_s, delta_s, x_s):\n            beta = beta[0]\n            risk_scores = np.exp(beta * x_s)\n            risk_set_sums = np.cumsum(risk_scores[::-1])[::-1]\n            \n            # Use a small epsilon for numerical stability\n            log_risk_set_sums = np.log(risk_set_sums + 1e-30)\n            \n            event_indices = (delta_s == 1)\n            linear_term_at_events = beta * x_s[event_indices]\n            log_risk_set_sums_at_events = log_risk_set_sums[event_indices]\n            \n            log_likelihood = np.sum(linear_term_at_events - log_risk_set_sums_at_events)\n            return -log_likelihood\n\n        res = minimize(\n            neg_log_partial_likelihood,\n            x0=np.array([0.0]),\n            args=(Y_sorted, delta_sorted, x_sorted),\n            method='BFGS'\n        )\n        beta_cox = res.x[0]\n\n        risk_scores_est = np.exp(beta_cox * x_sorted)\n        risk_set_sums_est = np.cumsum(risk_scores_est[::-1])[::-1]\n        \n        event_mask = (delta_sorted == 1)\n        unique_event_times, event_time_indices = np.unique(Y_sorted[event_mask], return_inverse=True)\n        num_events_at_time = np.bincount(event_time_indices)\n        \n        y_event_indices = np.searchsorted(Y_sorted, unique_event_times)\n        risk_set_sum_at_event_time = risk_set_sums_est[y_event_indices]\n        \n        hazard_jumps = num_events_at_time / (risk_set_sum_at_event_time + 1e-30)\n        cum_hazard_jumps = np.cumsum(hazard_jumps)\n\n        def breslow_estimator(t_grid):\n            H0_t = np.zeros_like(t_grid, dtype=float)\n            indices = np.searchsorted(unique_event_times, t_grid, side='right')\n            valid_indices = (indices  0)\n            H0_t[valid_indices] = cum_hazard_jumps[indices[valid_indices] - 1]\n            return H0_t\n            \n        return beta_cox, breslow_estimator\n\n    def _fit_aft_model(Y, delta, x):\n        \"\"\"\n        Fits a log-normal AFT model using maximum likelihood for right-censored data.\n        \"\"\"\n        log_Y = np.log(Y)\n        initial_params = np.array([1.0, 0.0, 0.0])\n        is_event = (delta == 1)\n        if np.sum(is_event)  2:\n            x_event = x[is_event]\n            if np.any(x_event != x_event[0]):\n                log_Y_event = log_Y[is_event]\n                X = np.vstack([np.ones(len(x_event)), x_event]).T\n                try:\n                    params, _, _, _ = np.linalg.lstsq(X, log_Y_event, rcond=None)\n                    mu_init, beta_init = params\n                    residuals = log_Y_event - X @ params\n                    sigma_init = np.std(residuals)\n                    if sigma_init > 1e-4:\n                        initial_params = np.array([mu_init, beta_init, np.log(sigma_init)])\n                except np.linalg.LinAlgError:\n                    pass\n\n        def neg_log_likelihood(params, log_Y_l, delta_l, x_l):\n            mu, beta, log_sigma = params\n            sigma = np.exp(log_sigma)\n            if sigma  1e-6: return np.inf\n\n            z = (log_Y_l - mu - beta * x_l) / sigma\n            \n            events_mask = (delta_l == 1)\n            loglik = np.sum(norm.logpdf(z[events_mask]) - log_sigma - log_Y_l[events_mask])\n            loglik += np.sum(norm.logsf(z[~events_mask]))\n            \n            return -loglik if np.isfinite(loglik) else np.inf\n\n        res = minimize(\n            neg_log_likelihood,\n            initial_params,\n            args=(log_Y, delta, x),\n            method='BFGS'\n        )\n        mu_aft, beta_aft, log_sigma_aft = res.x\n        sigma_aft = np.exp(log_sigma_aft)\n        return mu_aft, beta_aft, sigma_aft\n\n    def _run_simulation_case(n, p, mu, beta, sigma, lambda_c, tau, seed):\n        \"\"\"\n        Runs the full pipeline for one test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        x = rng.binomial(1, p, size=n)\n        Z = rng.standard_normal(size=n)\n        T = np.exp(mu + beta * x + sigma * Z)\n        if lambda_c > 0:\n            C = rng.exponential(scale=1 / lambda_c, size=n)\n        else:\n            C = np.full(n, np.inf)\n        Y = np.minimum(T, C)\n        delta = (T = C).astype(int)\n\n        t_grid = np.linspace(0, tau, 2001)\n\n        beta_cox, H0_func = _fit_cox_model(Y, delta, x)\n        H0_on_grid = H0_func(t_grid)\n        S_cox_0 = np.exp(-H0_on_grid)\n        S_cox_1 = np.exp(-H0_on_grid * np.exp(beta_cox))\n        rmst_cox_0 = np.trapz(S_cox_0, t_grid)\n        rmst_cox_1 = np.trapz(S_cox_1, t_grid)\n\n        mu_aft, beta_aft, sigma_aft = _fit_aft_model(Y, delta, x)\n        with np.errstate(divide='ignore'):\n            log_t_grid = np.log(t_grid)\n        \n        S_aft_0 = norm.sf((log_t_grid - mu_aft) / sigma_aft)\n        S_aft_1 = norm.sf((log_t_grid - (mu_aft + beta_aft)) / sigma_aft)\n        S_aft_0[0] = 1.0\n        S_aft_1[0] = 1.0\n        rmst_aft_0 = np.trapz(S_aft_0, t_grid)\n        rmst_aft_1 = np.trapz(S_aft_1, t_grid)\n\n        S_true_0 = norm.sf((log_t_grid - mu) / sigma)\n        S_true_1 = norm.sf((log_t_grid - (mu + beta)) / sigma)\n        S_true_0[0] = 1.0\n        S_true_1[0] = 1.0\n        rmst_true_0 = np.trapz(S_true_0, t_grid)\n        rmst_true_1 = np.trapz(S_true_1, t_grid)\n\n        E_cox_0 = np.abs(rmst_cox_0 - rmst_true_0)\n        E_cox_1 = np.abs(rmst_cox_1 - rmst_true_1)\n        E_bar_cox = (E_cox_0 + E_cox_1) / 2.0\n\n        E_aft_0 = np.abs(rmst_aft_0 - rmst_true_0)\n        E_aft_1 = np.abs(rmst_aft_1 - rmst_true_1)\n        E_bar_aft = (E_aft_0 + E_aft_1) / 2.0\n\n        return E_bar_aft  E_bar_cox\n\n    test_cases = [\n        (1000, 0.5, 2.0, 0.7, 0.6, 0.05, 10.0, 1),\n        (1000, 0.5, 2.0, 1.0, 0.9, 0.2, 8.0, 2),\n        (500, 0.2, 1.5, 0.8, 0.5, 0.1, 6.0, 3),\n        (400, 0.5, 1.0, 1.2, 1.0, 0.3, 5.0, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _run_simulation_case(*case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3179084"}]}