{"hands_on_practices": [{"introduction": "Calculating a treatment effect is only the first step; assessing its statistical significance is equally crucial. This practice moves beyond the point estimate to the heart of inference: the variance of the Difference-in-Differences (DiD) estimator. You will derive the variance from first principles, exploring how correlations within data—specifically, clustering by location and time—impact the precision of your estimate [@problem_id:3115377]. This exercise is fundamental for understanding why specialized standard errors are essential for valid hypothesis testing in panel data.", "problem": "Consider a balanced panel with locations indexed by $i \\in \\{1,\\dots,N\\}$ and times indexed by $t \\in \\{1,\\dots,T\\}$. A policy is introduced at time $t^{\\ast}$ and applies to a subset of locations, called the treated group $\\mathcal{G}$ with cardinality $N_{G}$, while the remaining locations form the control group $\\mathcal{C}$ with cardinality $N_{C}$, so that $N_{G} + N_{C} = N$. Define the pre-policy period $\\mathcal{P} = \\{t : t < t^{\\ast}\\}$ of size $T_{0}$ and the post-policy period $\\mathcal{S} = \\{t : t \\ge t^{\\ast}\\}$ of size $T_{1}$, with $T_{0} + T_{1} = T$. The outcome $y_{it}$ follows the additive model\n$$\ny_{it} = \\alpha_{i} + \\gamma_{t} + \\tau D_{it} + u_{it},\n$$\nwhere $D_{it} = 1$ if $i \\in \\mathcal{G}$ and $t \\in \\mathcal{S}$, and $D_{it} = 0$ otherwise. The estimand $\\tau$ is the average treatment effect. Define the Difference-in-Differences (DiD) estimator\n$$\n\\hat{\\tau} = \\big(\\bar{y}_{\\mathcal{G},\\mathcal{S}} - \\bar{y}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{y}_{\\mathcal{C},\\mathcal{S}} - \\bar{y}_{\\mathcal{C},\\mathcal{P}}\\big),\n$$\nwhere $\\bar{y}_{\\mathcal{G},\\mathcal{S}}$ denotes the average of $y_{it}$ over $i \\in \\mathcal{G}$ and $t \\in \\mathcal{S}$, and similarly for $\\bar{y}_{\\mathcal{G},\\mathcal{P}}$, $\\bar{y}_{\\mathcal{C},\\mathcal{S}}$, and $\\bar{y}_{\\mathcal{C},\\mathcal{P}}$.\n\nAssume $u_{it}$ has mean zero and the following covariance structure with constants $\\sigma^{2} > 0$, $\\rho_{L} \\in [-1,1]$, and $\\rho_{T} \\in [-1,1]$:\n- $\\mathrm{Var}(u_{it}) = \\sigma^{2}$ for all $i,t$,\n- $\\mathrm{Cov}(u_{it}, u_{is}) = \\rho_{L}\\sigma^{2}$ for all $i$ and all $t \\neq s$ (location-level correlation across time),\n- $\\mathrm{Cov}(u_{it}, u_{jt}) = \\rho_{T}\\sigma^{2}$ for all $t$ and all $i \\neq j$ (time-level correlation across locations),\n- $\\mathrm{Cov}(u_{it}, u_{js}) = 0$ when $i \\neq j$ and $t \\neq s$.\n\nStarting from first principles involving linear combinations of random variables and the definitions above (do not invoke any pre-packaged cluster-robust formulas), derive closed-form expressions for $\\mathrm{Var}(\\hat{\\tau})$ under:\n1. clustering by location only (set $\\rho_{T} = 0$),\n2. clustering by time only (set $\\rho_{L} = 0$),\n3. two-way clustering by both location and time (allow both $\\rho_{L}$ and $\\rho_{T}$).\n\nExpress your final results in terms of $N_{G}$, $N_{C}$, $T_{0}$, $T_{1}$, $\\sigma^{2}$, $\\rho_{L}$, and $\\rho_{T}$. Provide the three variances as a single row matrix in the order (location-only, time-only, two-way). No rounding is required.", "solution": "The problem asks for the variance of the Difference-in-Differences (DiD) estimator $\\hat{\\tau}$ under a specified covariance structure for the error term $u_{it}$. The problem is internally consistent, scientifically grounded in statistical theory, and well-posed. We may proceed with the solution.\n\nFirst, we express the DiD estimator $\\hat{\\tau}$ in terms of the model's components. The estimator is defined as:\n$$\n\\hat{\\tau} = \\big(\\bar{y}_{\\mathcal{G},\\mathcal{S}} - \\bar{y}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{y}_{\\mathcal{C},\\mathcal{S}} - \\bar{y}_{\\mathcal{C},\\mathcal{P}}\\big)\n$$\nSubstituting the model $y_{it} = \\alpha_{i} + \\gamma_{t} + \\tau D_{it} + u_{it}$ into the definitions of the four averages $\\bar{y}_{\\cdot,\\cdot}$, we observe that the fixed effects $\\alpha_i$ and $\\gamma_t$ are differenced out, as is the true treatment effect parameter $\\tau$. Let $\\bar{u}_{\\mathcal{G},\\mathcal{S}}$ be the average of $u_{it}$ over $i \\in \\mathcal{G}$ and $t \\in \\mathcal{S}$, and similarly for the other terms. The estimator can be written as:\n$$\n\\hat{\\tau} = \\tau + \\big(\\bar{u}_{\\mathcal{G},\\mathcal{S}} - \\bar{u}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{u}_{\\mathcal{C},\\mathcal{S}} - \\bar{u}_{\\mathcal{C},\\mathcal{P}}\\big)\n$$\nSince $\\mathrm{E}[u_{it}]=0$ for all $i,t$, it follows that $\\mathrm{E}[\\hat{\\tau}] = \\tau$. The variance of the estimator is determined by the variance of the combination of error term averages:\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\mathrm{Var}\\left( \\big(\\bar{u}_{\\mathcal{G},\\mathcal{S}} - \\bar{u}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{u}_{\\mathcal{C},\\mathcal{S}} - \\bar{u}_{\\mathcal{C},\\mathcal{P}}\\big) \\right)\n$$\nWe can express this as the variance of a single linear combination of the error terms $u_{it}$. Let $W = (\\bar{u}_{\\mathcal{G},\\mathcal{S}} - \\bar{u}_{\\mathcal{G},\\mathcal{P}}) - (\\bar{u}_{\\mathcal{C},\\mathcal{S}} - \\bar{u}_{\\mathcal{C},\\mathcal{P}})$. We can write $W = \\sum_{i=1}^{N} \\sum_{t=1}^{T} w_{it} u_{it}$, where the weights $w_{it}$ are defined as:\n\\begin{itemize}\n    \\item For $i \\in \\mathcal{G}$ and $t \\in \\mathcal{S}$ (treated group, post-period): $w_{it} = \\frac{1}{N_G T_1}$\n    \\item For $i \\in \\mathcal{G}$ and $t \\in \\mathcal{P}$ (treated group, pre-period): $w_{it} = -\\frac{1}{N_G T_0}$\n    \\item For $i \\in \\mathcal{C}$ and $t \\in \\mathcal{S}$ (control group, post-period): $w_{it} = -\\frac{1}{N_C T_1}$\n    \\item For $i \\in \\mathcal{C}$ and $t \\in \\mathcal{P}$ (control group, pre-period): $w_{it} = \\frac{1}{N_C T_0}$\n\\end{itemize}\nThe variance of this linear combination is given by:\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\mathrm{Var}(W) = \\mathrm{Var}\\left(\\sum_{i,t} w_{it} u_{it}\\right) = \\sum_{i,t} \\sum_{j,s} w_{it} w_{js} \\mathrm{Cov}(u_{it}, u_{js})\n$$\nWe use the covariance structure provided in the problem:\n\\begin{itemize}\n    \\item $\\mathrm{Cov}(u_{it}, u_{js}) = \\sigma^{2}$ if $i=j$ and $t=s$.\n    \\item $\\mathrm{Cov}(u_{it}, u_{js}) = \\rho_{L}\\sigma^{2}$ if $i=j$ and $t \\neq s$.\n    \\item $\\mathrm{Cov}(u_{it}, u_{js}) = \\rho_{T}\\sigma^{2}$ if $i \\neq j$ and $t=s$.\n    \\item $\\mathrm{Cov}(u_{it}, u_{js}) = 0$ if $i \\neq j$ and $t \\neq s$.\n\\end{itemize}\nWe can split the double summation into four parts based on these cases:\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\sum_{i,t} w_{it}^2 \\mathrm{Var}(u_{it}) + \\sum_{i} \\sum_{t \\neq s} w_{it} w_{is} \\mathrm{Cov}(u_{it}, u_{is}) + \\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt} \\mathrm{Cov}(u_{it}, u_{jt}) + \\sum_{i \\neq j, t \\neq s} w_{it} w_{js} \\cdot 0\n$$\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2 \\sum_{i,t} w_{it}^2 + \\rho_L \\sigma^2 \\sum_{i} \\sum_{t \\neq s} w_{it} w_{is} + \\rho_T \\sigma^2 \\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt}\n$$\nLet's evaluate each term.\n\nTerm 1: Sum of squared weights.\n$$\n\\sum_{i,t} w_{it}^2 = \\sum_{i \\in \\mathcal{G}, t \\in \\mathcal{S}} \\left(\\frac{1}{N_G T_1}\\right)^2 + \\sum_{i \\in \\mathcal{G}, t \\in \\mathcal{P}} \\left(\\frac{-1}{N_G T_0}\\right)^2 + \\sum_{i \\in \\mathcal{C}, t \\in \\mathcal{S}} \\left(\\frac{-1}{N_C T_1}\\right)^2 + \\sum_{i \\in \\mathcal{C}, t \\in \\mathcal{P}} \\left(\\frac{1}{N_C T_0}\\right)^2\n$$\nThe number of terms in these sums are $N_G T_1$, $N_G T_0$, $N_C T_1$, and $N_C T_0$, respectively.\n$$\n\\sum_{i,t} w_{it}^2 = \\frac{N_G T_1}{N_G^2 T_1^2} + \\frac{N_G T_0}{N_G^2 T_0^2} + \\frac{N_C T_1}{N_C^2 T_1^2} + \\frac{N_C T_0}{N_C^2 T_0^2} = \\frac{1}{N_G T_1} + \\frac{1}{N_G T_0} + \\frac{1}{N_C T_1} + \\frac{1}{N_C T_0}\n$$\nThis can be factored:\n$$\n\\sum_{i,t} w_{it}^2 = \\frac{1}{N_G}\\left(\\frac{1}{T_1} + \\frac{1}{T_0}\\right) + \\frac{1}{N_C}\\left(\\frac{1}{T_1} + \\frac{1}{T_0}\\right) = \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\nThe first term of the variance is $\\sigma^2 \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)$.\n\nTerm 2: Location-level correlation. The sum is $\\sum_{i} \\sum_{t \\neq s} w_{it} w_{is}$. We use the identity $\\sum_{t \\neq s} a_t a_s = (\\sum_t a_t)^2 - \\sum_t a_t^2$.\nFor any location $i \\in \\mathcal{G}$:\n$$ \\sum_{t} w_{it} = \\sum_{t \\in \\mathcal{S}} \\frac{1}{N_G T_1} + \\sum_{t \\in \\mathcal{P}} \\frac{-1}{N_G T_0} = T_1 \\cdot \\frac{1}{N_G T_1} - T_0 \\cdot \\frac{1}{N_G T_0} = \\frac{1}{N_G} - \\frac{1}{N_G} = 0 $$\nSimilarly, for any location $i \\in \\mathcal{C}$:\n$$ \\sum_{t} w_{it} = \\sum_{t \\in \\mathcal{S}} \\frac{-1}{N_C T_1} + \\sum_{t \\in \\mathcal{P}} \\frac{1}{N_C T_0} = -T_1 \\cdot \\frac{1}{N_C T_1} + T_0 \\cdot \\frac{1}{N_C T_0} = -\\frac{1}{N_C} + \\frac{1}{N_C} = 0 $$\nThus, $\\sum_t w_{it} = 0$ for all $i$. So, $\\sum_{t \\neq s} w_{it} w_{is} = 0 - \\sum_t w_{it}^2$.\n$$\n\\sum_{i} \\sum_{t \\neq s} w_{it} w_{is} = \\sum_{i} \\left( -\\sum_{t} w_{it}^2 \\right) = - \\sum_{i,t} w_{it}^2 = -\\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\nThe second term of the variance is $-\\rho_L \\sigma^2 \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)$.\n\nTerm 3: Time-level correlation. The sum is $\\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt}$. We use the identity $\\sum_{i \\neq j} a_i a_j = (\\sum_i a_i)^2 - \\sum_i a_i^2$.\nFor any time $t \\in \\mathcal{S}$:\n$$ \\sum_{i} w_{it} = \\sum_{i \\in \\mathcal{G}} \\frac{1}{N_G T_1} + \\sum_{i \\in \\mathcal{C}} \\frac{-1}{N_C T_1} = N_G \\cdot \\frac{1}{N_G T_1} - N_C \\cdot \\frac{1}{N_C T_1} = \\frac{1}{T_1} - \\frac{1}{T_1} = 0 $$\nSimilarly, for any time $t \\in \\mathcal{P}$:\n$$ \\sum_{i} w_{it} = \\sum_{i \\in \\mathcal{G}} \\frac{-1}{N_G T_0} + \\sum_{i \\in \\mathcal{C}} \\frac{1}{N_C T_0} = -N_G \\cdot \\frac{1}{N_G T_0} + N_C \\cdot \\frac{1}{N_C T_0} = -\\frac{1}{T_0} + \\frac{1}{T_0} = 0 $$\nThus, $\\sum_i w_{it} = 0$ for all $t$. So, $\\sum_{i \\neq j} w_{it} w_{jt} = 0 - \\sum_i w_{it}^2$.\n$$\n\\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt} = \\sum_{t} \\left( -\\sum_{i} w_{it}^2 \\right) = - \\sum_{i,t} w_{it}^2 = -\\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\nThe third term of the variance is $-\\rho_T \\sigma^2 \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)$.\n\nCombining all three terms, we get the general expression for the variance of $\\hat{\\tau}$:\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2 \\left(\\frac{1}{N_G}+\\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0}+\\frac{1}{T_1}\\right) - \\rho_L \\sigma^2 \\left(\\frac{1}{N_G}+\\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0}+\\frac{1}{T_1}\\right) - \\rho_T \\sigma^2 \\left(\\frac{1}{N_G}+\\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0}+\\frac{1}{T_1}\\right)\n$$\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2(1 - \\rho_L - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\nThis is the general expression for two-way clustering.\n\nWe can now find the solutions for the three specific cases required.\n\n1.  **Clustering by location only**: We set $\\rho_T = 0$.\n    $$\n    \\mathrm{Var}(\\hat{\\tau})_{\\text{loc}} = \\sigma^2(1 - \\rho_L) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n    $$\n\n2.  **Clustering by time only**: We set $\\rho_L = 0$.\n    $$\n    \\mathrm{Var}(\\hat{\\tau})_{\\text{time}} = \\sigma^2(1 - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n    $$\n\n3.  **Two-way clustering**: This is the general formula derived above.\n    $$\n    \\mathrm{Var}(\\hat{\\tau})_{\\text{two-way}} = \\sigma^2(1 - \\rho_L - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n    $$\n\nWe present these three results in a single row matrix as requested.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sigma^2(1 - \\rho_L) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right) & \\sigma^2(1 - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right) & \\sigma^2(1 - \\rho_L - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right) \\end{pmatrix}}\n$$", "id": "3115377"}, {"introduction": "The validity of a DiD estimate hinges on the parallel trends assumption, but this can be violated in subtle ways, such as by time-varying measurement error. This hands-on exercise simulates a modern scenario where an evolving language model introduces bias, and tasks you with implementing a powerful solution: the negative control method [@problem_id:3115392]. By working through this problem, you will learn to detect and correct for confounding drift, thereby strengthening the causal claims of your analysis.", "problem": "You are tasked with implementing a program that computes both an unadjusted and a negative-control-adjusted Difference-in-Differences (DiD) estimate of a policy effect on a measured share outcome, and to test for measurement drift using a negative control. The setting is a two-group, two-period design, with a treatment applied to one group at the second period. The educational goal is to derive, implement, and test the Difference-in-Differences estimator for a latent policy effect on an outcome that is measured with time-varying drift, using a negative control outcome to detect and adjust for the drift. Your derivation must start from the potential outcomes framework and the parallel trends assumption, and you must not assume any pre-given estimator formula.\n\nContext and definitions:\n- Consider two groups, indexed by $g \\in \\{\\mathrm{T}, \\mathrm{C}\\}$, where $\\mathrm{T}$ denotes the treated group and $\\mathrm{C}$ denotes the control group, and two time periods, indexed by $t \\in \\{0,1\\}$ where $t=0$ is the pre-period and $t=1$ is the post-period.\n- Let the latent outcome (free of measurement drift) be $Y^{\\ast}_{g t}$, interpreted as the true share of hate-speech in country-group $g$ at time $t$. Let the observed outcome be $Y_{g t}$, which is subject to time-varying measurement drift from a language model.\n- Let the negative control outcome be $W_{g t}$, which is unaffected by treatment but shares the same drift factor as the observed outcome.\n- Let the treatment effect on the latent outcome for the treated group in the post-period be $\\tau$. Let the potential outcomes framework and the parallel trends assumption apply to $Y^{\\ast}_{g t}$: in the absence of treatment, the treated and control groups would have evolved in parallel in expectation between $t=0$ and $t=1$.\n\nData-generating structure to be used for this problem:\n- The latent outcome is additively separable into group and time components, plus the treatment effect when it applies:\n$$\nY^{\\ast}_{g t} = \\mu_{g} + \\psi_{t} + \\tau \\cdot \\mathbf{1}\\{g = \\mathrm{T}\\} \\cdot \\mathbf{1}\\{t = 1\\}.\n$$\n- The observed outcome $Y_{g t}$ contains drift from a single common time-varying factor $L_{t}$ with group-specific loadings $b_{g}$:\n$$\nY_{g t} = Y^{\\ast}_{g t} + b_{g} \\cdot L_{t}.\n$$\n- The negative control $W_{g t}$ is unaffected by treatment and shares the same factor $L_{t}$ with group-specific loadings $d_{g}$:\n$$\nW_{g t} = d_{g} \\cdot L_{t}.\n$$\n- There is no stochastic noise in this problem; all quantities are deterministic means. All shares must be treated as decimals in $[0,1]$.\n\nYour tasks:\n1) Using the definitions of potential outcomes and the parallel trends assumption, derive a contrast of the four observed means $\\{Y_{\\mathrm{T}0}, Y_{\\mathrm{T}1}, Y_{\\mathrm{C}0}, Y_{\\mathrm{C}1}\\}$ that eliminates group-specific baselines and common time effects in $Y^{\\ast}_{g t}$ to identify the average treatment effect on the treated under ideal measurement. Implement this contrast to obtain an unadjusted estimator $\\widehat{\\tau}_{\\mathrm{DiD}}$ from the observed data.\n2) Using the negative control $W_{g t}$, derive an adjustment that removes the contribution of drift induced by $L_{t}$ from $\\widehat{\\tau}_{\\mathrm{DiD}}$. Assume the loadings $b_{g}$ and $d_{g}$ are known constants in this synthetic exercise. Under the single-factor drift structure, show how to form an adjusted estimator $\\widehat{\\tau}_{\\mathrm{adj}}$ by subtracting the component of the DiD contrast in $Y_{g t}$ that is attributable to drift, using an appropriate scalar that links the cross-group sensitivity of $Y_{g t}$ to that of $W_{g t}$. Implement this adjustment. In the degenerate case where the negative-control contrast is exactly zero, take $\\widehat{\\tau}_{\\mathrm{adj}} = \\widehat{\\tau}_{\\mathrm{DiD}}$.\n3) Provide a drift detection indicator based on the negative control: compute the DiD contrast on $W_{g t}$ and set a boolean $\\mathrm{drift\\_detected}$ equal to $\\mathrm{True}$ if its absolute value exceeds a numerical tolerance $\\varepsilon = 10^{-12}$, and $\\mathrm{False}$ otherwise.\n\nTest suite:\nFor each test case, you are given numerical parameters $(\\mu_{\\mathrm{T}}, \\mu_{\\mathrm{C}}, \\psi_{0}, \\psi_{1}, \\tau, b_{\\mathrm{T}}, b_{\\mathrm{C}}, d_{\\mathrm{T}}, d_{\\mathrm{C}}, L_{0}, L_{1})$. Use them to construct the observed cell means $Y_{g t}$ and $W_{g t}$ deterministically, then compute the required outputs.\n\n- Test case $1$ (happy path, no drift difference):\n  - $(\\mu_{\\mathrm{T}}, \\mu_{\\mathrm{C}}, \\psi_{0}, \\psi_{1}, \\tau) = (0.10, 0.12, 0.00, 0.01, 0.05)$\n  - $(b_{\\mathrm{T}}, b_{\\mathrm{C}}) = (0.02, 0.02)$\n  - $(d_{\\mathrm{T}}, d_{\\mathrm{C}}) = (1.0, 1.0)$\n  - $(L_{0}, L_{1}) = (1.0, 1.5)$\n\n- Test case $2$ (drift confounding without true effect):\n  - $(\\mu_{\\mathrm{T}}, \\mu_{\\mathrm{C}}, \\psi_{0}, \\psi_{1}, \\tau) = (0.20, 0.20, 0.00, 0.00, 0.00)$\n  - $(b_{\\mathrm{T}}, b_{\\mathrm{C}}) = (0.06, 0.02)$\n  - $(d_{\\mathrm{T}}, d_{\\mathrm{C}}) = (3.0, 1.0)$\n  - $(L_{0}, L_{1}) = (1.0, 2.0)$\n\n- Test case $3$ (true effect plus drift confounding):\n  - $(\\mu_{\\mathrm{T}}, \\mu_{\\mathrm{C}}, \\psi_{0}, \\psi_{1}, \\tau) = (0.15, 0.14, 0.00, 0.02, 0.04)$\n  - $(b_{\\mathrm{T}}, b_{\\mathrm{C}}) = (0.05, 0.02)$\n  - $(d_{\\mathrm{T}}, d_{\\mathrm{C}}) = (2.5, 1.0)$\n  - $(L_{0}, L_{1}) = (1.0, 1.8)$\n\n- Test case $4$ (boundary case, no drift change):\n  - $(\\mu_{\\mathrm{T}}, \\mu_{\\mathrm{C}}, \\psi_{0}, \\psi_{1}, \\tau) = (0.08, 0.10, 0.01, 0.02, 0.03)$\n  - $(b_{\\mathrm{T}}, b_{\\mathrm{C}}) = (0.04, 0.01)$\n  - $(d_{\\mathrm{T}}, d_{\\mathrm{C}}) = (2.0, 1.0)$\n  - $(L_{0}, L_{1}) = (2.0, 2.0)$\n\nProgram output requirements:\n- For each test case, output a list of three elements in the order $[\\widehat{\\tau}_{\\mathrm{DiD}}, \\widehat{\\tau}_{\\mathrm{adj}}, \\mathrm{drift\\_detected}]$.\n- Aggregate the results for all test cases into a single list, and print a single line containing this list of lists in the exact format: one Python-style list literal with commas, no spaces required, for example $[[0.123456,0.123000,True],[\\dots],\\dots]$.\n- Round all floating-point values to $6$ decimal places. Booleans must appear as $\\mathrm{True}$ or $\\mathrm{False}$.\n- All shares must be expressed as decimals, not percentages.", "solution": "## Derivation and Solution\n\nThe problem requires the derivation and implementation of Difference-in-Differences (DiD) estimators. All quantities are deterministic cell means, so we can omit expectation operators.\n\n### 1. Derivation of the Unadjusted DiD Estimator ($\\widehat{\\tau}_{\\mathrm{DiD}}$)\n\nThe objective is to estimate the Average Treatment Effect on the Treated (ATT), which is defined as the effect of the treatment on the treated group in the post-treatment period. Within the potential outcomes framework, we denote the potential outcomes for the latent variable $Y^{\\ast}$ as $Y^{\\ast}_{gt}(1)$ (with treatment) and $Y^{\\ast}_{gt}(0)$ (without treatment). The treatment is applied only to group $\\mathrm{T}$ at time $t=1$. The ATT is $\\tau = Y^{\\ast}_{\\mathrm{T}1}(1) - Y^{\\ast}_{\\mathrm{T}1}(0)$.\n\nWe observe $Y^{\\ast}_{\\mathrm{T}1} = Y^{\\ast}_{\\mathrm{T}1}(1)$. The term $Y^{\\ast}_{\\mathrm{T}1}(0)$ is the counterfactual: what would have happened to the treated group at $t=1$ had they not been treated. We must estimate this from the available data.\n\nThe **parallel trends assumption** is key. It posits that, in the absence of treatment, the change in the outcome for the treated group would have been the same as the change for the control group. Formally:\n$$\nY^{\\ast}_{\\mathrm{T}1}(0) - Y^{\\ast}_{\\mathrm{T}0}(0) = Y^{\\ast}_{\\mathrm{C}1}(0) - Y^{\\ast}_{\\mathrm{C}0}(0)\n$$\nSince the treated group is untreated at $t=0$ ($Y^{\\ast}_{\\mathrm{T}0}=Y^{\\ast}_{\\mathrm{T}0}(0)$) and the control group is never treated ($Y^{\\ast}_{\\mathrm{C}t}=Y^{\\ast}_{\\mathrm{C}t}(0)$), we can write:\n$$\nY^{\\ast}_{\\mathrm{T}1}(0) - Y^{\\ast}_{\\mathrm{T}0} = Y^{\\ast}_{\\mathrm{C}1} - Y^{\\ast}_{\\mathrm{C}0}\n$$\nRearranging to solve for the counterfactual $Y^{\\ast}_{\\mathrm{T}1}(0)$:\n$$\nY^{\\ast}_{\\mathrm{T}1}(0) = Y^{\\ast}_{\\mathrm{T}0} + (Y^{\\ast}_{\\mathrm{C}1} - Y^{\\ast}_{\\mathrm{C}0})\n$$\nSubstituting this into the definition of $\\tau$:\n$$\n\\tau = Y^{\\ast}_{\\mathrm{T}1} - Y^{\\ast}_{\\mathrm{T}1}(0) = Y^{\\ast}_{\\mathrm{T}1} - [Y^{\\ast}_{\\mathrm{T}0} + (Y^{\\ast}_{\\mathrm{C}1} - Y^{\\ast}_{\\mathrm{C}0})]\n$$\nRearranging gives the classic DiD expression for the latent variable:\n$$\n\\tau = (Y^{\\ast}_{\\mathrm{T}1} - Y^{\\ast}_{\\mathrm{T}0}) - (Y^{\\ast}_{\\mathrm{C}1} - Y^{\\ast}_{\\mathrm{C}0})\n$$\nThe unadjusted estimator, $\\widehat{\\tau}_{\\mathrm{DiD}}$, naively applies this formula to the *observed* outcomes $Y_{gt}$:\n$$\n\\widehat{\\tau}_{\\mathrm{DiD}} = (Y_{\\mathrm{T}1} - Y_{\\mathrm{T}0}) - (Y_{\\mathrm{C}1} - Y_{\\mathrm{C}0})\n$$\n\n### 2. Derivation of the Drift-Adjusted Estimator ($\\widehat{\\tau}_{\\mathrm{adj}}$)\n\nThe unadjusted estimator $\\widehat{\\tau}_{\\mathrm{DiD}}$ is biased if the measurement drift does not follow\na parallel trend. Let's quantify this bias. Substitute $Y_{g t} = Y^{\\ast}_{g t} + b_{g} L_{t}$ into the $\\widehat{\\tau}_{\\mathrm{DiD}}$ formula:\n$$\n\\widehat{\\tau}_{\\mathrm{DiD}} = \\left( (Y^{\\ast}_{\\mathrm{T}1} + b_{\\mathrm{T}}L_1) - (Y^{\\ast}_{\\mathrm{T}0} + b_{\\mathrm{T}}L_0) \\right) - \\left( (Y^{\\ast}_{\\mathrm{C}1} + b_{\\mathrm{C}}L_1) - (Y^{\\ast}_{\\mathrm{C}0} + b_{\\mathrm{C}}L_0) \\right)\n$$\nSeparating the terms related to $Y^{\\ast}$ and the drift terms $(b_g, L_t)$:\n$$\n\\widehat{\\tau}_{\\mathrm{DiD}} = \\left[ (Y^{\\ast}_{\\mathrm{T}1} - Y^{\\ast}_{\\mathrm{T}0}) - (Y^{\\ast}_{\\mathrm{C}1} - Y^{\\ast}_{\\mathrm{C}0}) \\right] + \\left[ (b_{\\mathrm{T}}L_1 - b_{\\mathrm{T}}L_0) - (b_{\\mathrm{C}}L_1 - b_{\\mathrm{C}}L_0) \\right]\n$$\nThe first bracket is the true treatment effect, $\\tau$. The second bracket is the bias term:\n$$\n\\text{Bias} = (b_{\\mathrm{T}} - b_{\\mathrm{C}}) (L_1 - L_0)\n$$\nThus, $\\widehat{\\tau}_{\\mathrm{DiD}} = \\tau + \\text{Bias}$. The drift only biases the estimate if the factor loadings differ between groups ($b_{\\mathrm{T}} \\neq b_{\\mathrm{C}}$) AND the drift factor changes over time ($L_1 \\neq L_0$).\n\nTo correct for this bias, we use the negative control outcome $W_{gt}$. We compute a DiD contrast on $W_{gt}$, let's call it $\\Delta_W$:\n$$\n\\Delta_W = (W_{\\mathrm{T}1} - W_{\\mathrm{T}0}) - (W_{\\mathrm{C}1} - W_{\\mathrm{C}0})\n$$\nSubstituting the definition $W_{g t} = d_{g} L_{t}$:\n$$\n\\Delta_W = (d_{\\mathrm{T}}L_1 - d_{\\mathrm{T}}L_0) - (d_{\\mathrm{C}}L_1 - d_{\\mathrm{C}}L_0) = (d_{\\mathrm{T}} - d_{\\mathrm{C}})(L_1 - L_0)\n$$\nWe have two equations:\n$1. \\text{Bias} = (b_{\\mathrm{T}} - b_{\\mathrm{C}})(L_1 - L_0)$\n$2. \\Delta_W = (d_{\\mathrm{T}} - d_{\\mathrm{C}})(L_1 - L_0)$\n\nAssuming $d_{\\mathrm{T}} - d_{\\mathrm{C}} \\neq 0$, we can express $(L_1 - L_0)$ from the second equation as $\\frac{\\Delta_W}{d_{\\mathrm{T}} - d_{\\mathrm{C}}}$. Substituting this into the first equation:\n$$\n\\text{Bias} = (b_{\\mathrm{T}} - b_{\\mathrm{C}}) \\frac{\\Delta_W}{d_{\\mathrm{T}} - d_{\\mathrm{C}}} = \\left(\\frac{b_{\\mathrm{T}} - b_{\\mathrm{C}}}{d_{\\mathrm{T}} - d_{\\mathrm{C}}}\\right) \\Delta_W\n$$\nThe scalar $\\gamma = \\frac{b_{\\mathrm{T}} - b_{\\mathrm{C}}}{d_{\\mathrm{T}} - d_{\\mathrm{C}}}$ links the drift effect in $Y_{gt}$ to the drift effect in $W_{gt}$. Since $b_g$ and $d_g$ are known, we can compute this scalar and estimate the bias.\n\nThe adjusted estimator $\\widehat{\\tau}_{\\mathrm{adj}}$ is obtained by subtracting the estimated bias from $\\widehat{\\tau}_{\\mathrm{DiD}}$:\n$$\n\\widehat{\\tau}_{\\mathrm{adj}} = \\widehat{\\tau}_{\\mathrm{DiD}} - \\left(\\frac{b_{\\mathrm{T}} - b_{\\mathrm{C}}}{d_{\\mathrm{T}} - d_{\\mathrm{C}}}\\right) \\Delta_W\n$$\nThis corrects the estimate to recover the true $\\tau$. The problem specifies that if the negative-control contrast $\\Delta_W$ is zero (within tolerance), no adjustment is made, so $\\widehat{\\tau}_{\\mathrm{adj}} = \\widehat{\\tau}_{\\mathrm{DiD}}$. This rule correctly handles cases where $L_1=L_0$ (no change in drift) or $d_{\\mathrm{T}}=d_{\\mathrm{C}}$ (parallel drift in the negative control), as in both scenarios $\\Delta_W = 0$.\n\n### 3. Drift Detection Indicator\nThe drift detection indicator is based directly on $\\Delta_W$. If this value is non-zero (beyond a small numerical tolerance), it implies that the drift is not parallel across groups ($d_T \\neq d_C$) and is not constant over time ($L_1 \\neq L_0$). This non-parallel drift in the negative control serves as a warning that the parallel trends assumption on the observed outcome $Y_{gt}$ is likely violated due to measurement drift, thus confounding $\\widehat{\\tau}_{\\mathrm{DiD}}$.\nThe indicator is defined as:\n$$\n\\mathrm{drift\\_detected} = \\begin{cases} \\mathrm{True} & \\text{if } |\\Delta_W| > \\varepsilon \\\\ \\mathrm{False} & \\text{if } |\\Delta_W| \\leq \\varepsilon \\end{cases}\n$$\nwith $\\varepsilon = 10^{-12}$.\n\n### Implementation Strategy\n\nFor each test case, the program will:\n1.  Receive the parameters $(\\mu_{\\mathrm{T}}, \\mu_{\\mathrm{C}}, \\psi_{0}, \\psi_{1}, \\tau, b_{\\mathrm{T}}, b_{\\mathrm{C}}, d_{\\mathrm{T}}, d_{\\mathrm{C}}, L_{0}, L_{1})$.\n2.  Compute the four observed means for $Y_{gt}$ and $W_{gt}$ using their respective data-generating process equations.\n3.  Calculate $\\widehat{\\tau}_{\\mathrm{DiD}} = (Y_{\\mathrm{T}1} - Y_{\\mathrm{T}0}) - (Y_{\\mathrm{C}1} - Y_{\\mathrm{C}0})$.\n4.  Calculate $\\Delta_W = (W_{\\mathrm{T}1} - W_{\\mathrm{T}0}) - (W_{\\mathrm{C}1} - W_{\\mathrm{C}0})$.\n5.  Determine $\\mathrm{drift\\_detected} = |\\Delta_W| > 10^{-12}$.\n6.  Calculate the adjustment term. If $|\\Delta_W| \\leq 10^{-12}$, the adjustment is $0$. Otherwise, the adjustment is $\\frac{b_{\\mathrm{T}} - b_{\\mathrm{C}}}{d_{\\mathrm{T}} - d_{\\mathrm{C}}} \\Delta_W$.\n7.  Calculate $\\widehat{\\tau}_{\\mathrm{adj}} = \\widehat{\\tau}_{\\mathrm{DiD}} - \\text{adjustment}$.\n8.  Round the estimators to $6$ decimal places and format the output as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes unadjusted and negative-control-adjusted DiD estimates.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu_T, mu_C, psi_0, psi_1, tau, b_T, b_C, d_T, d_C, L_0, L_1)\n        (0.10, 0.12, 0.00, 0.01, 0.05, 0.02, 0.02, 1.0, 1.0, 1.0, 1.5), # Case 1\n        (0.20, 0.20, 0.00, 0.00, 0.00, 0.06, 0.02, 3.0, 1.0, 1.0, 2.0), # Case 2\n        (0.15, 0.14, 0.00, 0.02, 0.04, 0.05, 0.02, 2.5, 1.0, 1.0, 1.8), # Case 3\n        (0.08, 0.10, 0.01, 0.02, 0.03, 0.04, 0.01, 2.0, 1.0, 2.0, 2.0)  # Case 4\n    ]\n\n    results = []\n    TOLERANCE = 1e-12\n\n    for case in test_cases:\n        mu_T, mu_C, psi_0, psi_1, tau, b_T, b_C, d_T, d_C, L_0, L_1 = case\n\n        # --- Data Generation ---\n        # Observed outcome Y_gt = Y*_gt + b_g * L_t\n        # where Y*_gt = mu_g + psi_t + tau * 1{g=T} * 1{t=1}\n        Y_T0 = mu_T + psi_0 + b_T * L_0\n        Y_T1 = mu_T + psi_1 + tau + b_T * L_1\n        Y_C0 = mu_C + psi_0 + b_C * L_0\n        Y_C1 = mu_C + psi_1 + b_C * L_1\n\n        # Negative control outcome W_gt = d_g * L_t\n        W_T0 = d_T * L_0\n        W_T1 = d_T * L_1\n        W_C0 = d_C * L_0\n        W_C1 = d_C * L_1\n\n        # --- Task 1: Unadjusted DiD Estimator ---\n        tau_did = (Y_T1 - Y_T0) - (Y_C1 - Y_C0)\n\n        # --- Task 2 & 3: Drift Detection and Adjusted Estimator ---\n        # DiD on the negative control\n        delta_W = (W_T1 - W_T0) - (W_C1 - W_C0)\n        \n        # Drift detection\n        drift_detected = abs(delta_W) > TOLERANCE\n\n        # Adjustment calculation\n        adjustment = 0.0\n        if drift_detected:\n            # The derivation shows Bias = ((b_T - b_C) / (d_T - d_C)) * delta_W\n            # And tau_adj = tau_did - Bias\n            delta_b = b_T - b_C\n            delta_d = d_T - d_C\n            \n            # The condition `drift_detected` ensures abs(delta_W) > 0.\n            # If delta_W is non-zero, it must be that (d_T - d_C) != 0 and (L_1 - L_0) != 0.\n            # So, delta_d cannot be zero if drift is detected.\n            # Thus, division by zero is avoided.\n            if abs(delta_d) > TOLERANCE:\n                gamma = delta_b / delta_d\n                adjustment = gamma * delta_W\n        \n        tau_adj = tau_did - adjustment\n\n        # Formatting results as per requirements\n        # Round floating point values to 6 decimal places.\n        # Booleans remain as True/False.\n        results.append([\n            round(tau_did, 6),\n            round(tau_adj, 6),\n            drift_detected\n        ])\n\n    # Convert the list of lists to the required string format\n    # [[0.123456,0.123000,True],...]\n    final_output_str = \"[\" + \",\".join([\n        f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]) + \"]\"\n    \n    print(final_output_str.replace(\"True\", \"True\").replace(\"False\", \"False\").replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "3115392"}, {"introduction": "To achieve more reliable causal estimates, modern methods combine DiD with machine learning techniques to adjust for covariates and protect against model misspecification. This advanced practice guides you through the implementation of a Targeted Maximum Likelihood Estimator (TMLE) for the DiD parameter [@problem_id:3115416]. By completing this exercise, you will gain hands-on experience with the powerful concept of double robustness, which yields a consistent estimate if either your outcome model or your treatment assignment model is correct.", "problem": "You are given a two-period panel setting for difference-in-differences with a binary treatment. For each unit indexed by $i \\in \\{1,\\dots,N\\}$, there is a covariate $X_i \\in \\mathbb{R}$, a treatment indicator $A_i \\in \\{0,1\\}$, and outcomes $Y_{i0}$ (pre-period, time $t=0$) and $Y_{i1}$ (post-period, time $t=1$). The true data-generating process is:\n- Covariate: $X_i \\sim \\mathcal{N}(0,1)$.\n- Treatment assignment (propensity score): $e_0(x) = \\mathrm{logistic}(\\alpha_0 + \\alpha_1 x)$ with $\\mathrm{logistic}(z) = 1/(1+e^{-z})$, and $A_i \\sim \\mathrm{Bernoulli}(e_0(X_i))$ independently across $i$.\n- Untreated outcome regression at time $t$: $\\mu_0(x,t) = \\mathbb{E}[Y_{it} \\mid A_i=0, X_i=x]$, specified below by linear functions in $x$ with an additive time trend.\n- Realized outcomes:\n  - $Y_{i0} = \\theta_0 + \\theta_1 X_i + U_{i0}$,\n  - $Y_{i1} = \\theta_0 + \\theta_1 X_i + \\delta + \\gamma X_i + \\tau A_i + U_{i1}$,\n  where $U_{i0}, U_{i1} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$ and are independent of $(X_i,A_i)$, and $\\tau$ is a constant treatment effect in the post period for treated units. Thus, $\\mu_0(x,0) = \\theta_0 + \\theta_1 x$ and $\\mu_0(x,1) = \\theta_0 + \\theta_1 x + \\delta + \\gamma x$.\n\nThe target parameter is the difference-in-differences effect among the treated under conditional parallel trends, defined as\n$$\n\\tau^\\ast = \\mathbb{E}\\big[(Y_{i1} - Y_{i0}) \\mid A_i=1\\big] - \\mathbb{E}\\big[\\mu_0(X_i,1)-\\mu_0(X_i,0) \\mid A_i=1\\big].\n$$\n\nYour task is to adapt Targeted Maximum Likelihood Estimation (TMLE; Targeted Maximum Likelihood Estimation (TMLE)) to estimate $\\tau^\\ast$ using a pseudo-outcome and one-step targeting, and to evaluate double robustness empirically. Proceed as follows.\n\nFundamental base and definitions to use:\n- Conditional parallel trends: For all $x$, $\\mathbb{E}[Y_{i1}(0)-Y_{i0}(0)\\mid X_i=x, A_i=1] = \\mathbb{E}[Y_{i1}(0)-Y_{i0}(0)\\mid X_i=x, A_i=0]$, where $Y_{it}(0)$ denotes the potential outcome without treatment at time $t$.\n- Untreated outcome regression $\\mu_0(x,t) = \\mathbb{E}[Y_{it}\\mid A_i=0, X_i=x, t]$.\n- Propensity score $e(x) = \\mathbb{P}(A_i=1\\mid X_i=x)$.\n- Law of iterated expectations and properties of least squares and logistic regression likelihood.\n\nEstimator construction to implement:\n1. Estimate the untreated outcome regressions separately at $t=0$ and $t=1$ using only the control group ($A_i=0$). Denote these initial estimates by $\\widehat{\\mu}_0(x,0)$ and $\\widehat{\\mu}_0(x,1)$. You must implement two modeling options:\n   - Correctly specified outcome model: linear regression with features $[1, x]$.\n   - Misspecified outcome model: intercept-only regression with feature $[1]$ (omitting $x$).\n2. Form the pseudo-outcome differences for each unit:\n   $$\n   D_i = \\Big(Y_{i1} - \\widehat{\\mu}_0(X_i,1)\\Big) - \\Big(Y_{i0} - \\widehat{\\mu}_0(X_i,0)\\Big).\n   $$\n3. Estimate the propensity score $\\widehat{e}(x)$ by logistic regression using the full sample with two modeling options:\n   - Correctly specified propensity model: logistic regression with features $[1, x]$.\n   - Misspecified propensity model: logistic regression with features $[1, x^2]$ (omitting the linear term in $x$).\n4. Construct a TMLE-style targeting step on the pseudo-outcome by updating the post-period untreated regression by a scalar fluctuation $\\epsilon \\in \\mathbb{R}$:\n   $$\n   \\widehat{\\mu}_0^{\\star}(x,1) = \\widehat{\\mu}_0(x,1) + \\epsilon,\\quad \\widehat{\\mu}_0^{\\star}(x,0) = \\widehat{\\mu}_0(x,0).\n   $$\n   Choose $\\epsilon$ to solve the weighted control moment equation\n   $$\n   \\sum_{i:A_i=0} \\omega_i \\Big[\\big(Y_{i1} - \\widehat{\\mu}_0^{\\star}(X_i,1)\\big) - \\big(Y_{i0} - \\widehat{\\mu}_0^{\\star}(X_i,0)\\big)\\Big] = 0,\n   $$\n   where $\\omega_i \\propto \\frac{\\widehat{e}(X_i)}{1-\\widehat{e}(X_i)}$ are normalized to sum to $1$ over controls. Implement this by computing the normalized weights\n   $$\n   \\omega_i = \\frac{\\frac{\\widehat{e}(X_i)}{1-\\widehat{e}(X_i)}}{\\sum_{j:A_j=0} \\frac{\\widehat{e}(X_j)}{1-\\widehat{e}(X_j)}}, \\quad \\text{for } A_i=0.\n   $$\n   Show that the solution is the scalar\n   $$\n   \\widehat{\\epsilon} = \\sum_{i:A_i=0} \\omega_i D_i,\n   $$\n   and define the targeted pseudo-outcome difference $D_i^{\\star} = D_i - \\widehat{\\epsilon}$ for all $i$.\n5. Compute the targeted TMLE difference-in-differences estimator as the treated average of $D_i^{\\star}$:\n   $$\n   \\widehat{\\tau}_{\\text{TMLE-DiD}} = \\frac{1}{\\sum_{i=1}^{N} A_i}\\sum_{i=1}^{N} A_i D_i^{\\star}.\n   $$\n\nEmpirical double robustness evaluation:\n- Implement a simulation with parameters $N=5000$, $\\alpha_0 = -0.2$, $\\alpha_1 = 0.8$, $\\theta_0 = 1.0$, $\\theta_1 = 1.0$, $\\delta = 0.5$, $\\gamma = 0.5$, $\\tau = 2.0$, $\\sigma = 1.0$. Use independent Gaussian noises $U_{i0}, U_{i1} \\sim \\mathcal{N}(0,\\sigma^2)$.\n- For numerical stability in propensity estimation, clip $\\widehat{e}(x)$ to the interval $[10^{-6}, 1-10^{-6}]$.\n- Fit linear regressions by ordinary least squares and logistic regression by iteratively reweighted least squares with a tiny $\\ell_2$ ridge penalty $\\lambda = 10^{-8}$ to ensure invertibility.\n\nTest suite:\n- Run four scenarios with the following configuration tuples $(\\text{correct outcome}, \\text{correct propensity}, \\text{random seed})$:\n  - Case $1$: $(\\text{True}, \\text{True}, 2025)$.\n  - Case $2$: $(\\text{True}, \\text{False}, 2026)$.\n  - Case $3$: $(\\text{False}, \\text{True}, 2027)$.\n  - Case $4$: $(\\text{False}, \\text{False}, 2028)$.\n- For each case, simulate once using the specified random seed, compute $\\widehat{\\tau}_{\\text{TMLE-DiD}}$, and report the estimate as a float rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above, for example, $[\\widehat{\\tau}_1,\\widehat{\\tau}_2,\\widehat{\\tau}_3,\\widehat{\\tau}_4]$ with each entry rounded to $6$ decimal places and no additional text.", "solution": "The problem requires the implementation of a Targeted Maximum Likelihood Estimator (TMLE) for the difference-in-differences (DiD) parameter under a conditional parallel trends assumption. The goal is to estimate the parameter and empirically verify the estimator's double robustness property through a simulation study.\n\nFirst, we analyze the target parameter, the difference-in-differences effect among the treated, defined as:\n$$\n\\tau^\\ast = \\mathbb{E}\\big[(Y_{i1} - Y_{i0}) \\mid A_i=1\\big] - \\mathbb{E}\\big[\\mu_0(X_i,1)-\\mu_0(X_i,0) \\mid A_i=1\\big].\n$$\nThe first term is the observed average change in outcome for the treated group. The second term represents the counterfactual change in outcome for the treated group had they not been treated, which is identified under the conditional parallel trends assumption by the observed change in the control group, conditional on covariates $X_i$.\n\nLet's derive the true value of $\\tau^\\ast$ under the specified data-generating process (DGP). The change in outcome for a treated unit ($A_i=1$) conditional on their covariate $X_i=x$ is:\n$$\n\\mathbb{E}[Y_{i1} - Y_{i0} \\mid A_i=1, X_i=x] = \\mathbb{E}[(\\theta_0 + \\theta_1 x + \\delta + \\gamma x + \\tau A_i + U_{i1}) - (\\theta_0 + \\theta_1 x + U_{i0}) \\mid A_i=1, X_i=x].\n$$\nSince $A_i=1$ and the errors $U_{it}$ have mean zero and are independent of $X_i$ and $A_i$, this simplifies to:\n$$\n\\mathbb{E}[Y_{i1} - Y_{i0} \\mid A_i=1, X_i=x] = \\delta + \\gamma x + \\tau.\n$$\nTaking the expectation over the distribution of $X_i$ for the treated population ($A_i=1$) gives:\n$$\n\\mathbb{E}[Y_{i1} - Y_{i0} \\mid A_i=1] = \\delta + \\gamma \\mathbb{E}[X_i \\mid A_i=1] + \\tau.\n$$\nFor the second term in $\\tau^\\ast$, we use the definition of the untreated outcome regression $\\mu_0(x,t)$. The change in the untreated potential outcome is:\n$$\n\\mu_0(x,1) - \\mu_0(x,0) = (\\theta_0 + \\theta_1 x + \\delta + \\gamma x) - (\\theta_0 + \\theta_1 x) = \\delta + \\gamma x.\n$$\nTaking its expectation over the covariate distribution for the treated gives:\n$$\n\\mathbb{E}[\\mu_0(X_i,1)-\\mu_0(X_i,0) \\mid A_i=1] = \\mathbb{E}[\\delta + \\gamma X_i \\mid A_i=1] = \\delta + \\gamma \\mathbb{E}[X_i \\mid A_i=1].\n$$\nSubstituting these two results back into the definition of $\\tau^\\ast$:\n$$\n\\tau^\\ast = (\\delta + \\gamma \\mathbb{E}[X_i \\mid A_i=1] + \\tau) - (\\delta + \\gamma \\mathbb{E}[X_i \\mid A_i=1]) = \\tau.\n$$\nThus, the target parameter $\\tau^\\ast$ is precisely the true constant treatment effect $\\tau$, which is given as $2.0$ in the simulation. This provides a ground truth for our evaluation.\n\nThe principle of double robustness states that the TMLE estimator for $\\tau^\\ast$ will be consistent if at least one of the two \"nuisance\" models—the outcome regression model for $\\mu_0(x,t)$ or the propensity score model for $e(x)$—is correctly specified. We will test this by running simulations where each model is either correctly or incorrectly specified.\n\nThe estimation procedure follows five steps:\n\n1.  **Estimate Untreated Outcome Regressions**: We first estimate the untreated outcome functions $\\mu_0(x,0)$ and $\\mu_0(x,1)$ using only data from the control group ($A_i=0$). We fit two separate linear regressions via Ordinary Least Squares (OLS) for outcomes $Y_{i0}$ and $Y_{i1}$. We implement two model specifications: a correctly specified model using features $[1, X_i]$ and a misspecified model using only an intercept $[1]$. These initial estimates are denoted $\\widehat{\\mu}_0(x,0)$ and $\\widehat{\\mu}_0(x,1)$.\n\n2.  **Form Pseudo-Outcome Differences**: For each unit $i$, we compute a pseudo-outcome $D_i$ which represents the residualized difference-in-differences:\n    $$\n    D_i = \\Big(Y_{i1} - \\widehat{\\mu}_0(X_i,1)\\Big) - \\Big(Y_{i0} - \\widehat{\\mu}_0(X_i,0)\\Big).\n    $$\n    This $D_i$ adjusts the individual-level DiD, $Y_{i1} - Y_{i0}$, by subtracting the estimated change based on covariates from the initial outcome model.\n\n3.  **Estimate Propensity Score**: We estimate the propensity score $e(x) = \\mathbb{P}(A_i=1 \\mid X_i=x)$ using logistic regression on the full sample. This is implemented using Iteratively Reweighted Least Squares (IRLS). Again, we use two specifications: a correctly specified model with features $[1, X_i]$ and a misspecified one with features $[1, X_i^2]$. The resulting estimate is $\\widehat{e}(x)$.\n\n4.  **Targeting Step**: This is the key TMLE step. We update, or \"target,\" our initial estimate of the outcome model to satisfy a crucial property. We introduce a single fluctuation parameter $\\epsilon \\in \\mathbb{R}$ to update the post-period outcome model: $\\widehat{\\mu}_0^{\\star}(x,1) = \\widehat{\\mu}_0(x,1) + \\epsilon$. The pre-period model is unchanged. We solve for $\\epsilon$ using the weighted control moment equation:\n    $$\n    \\sum_{i:A_i=0} \\omega_i \\Big[\\big(Y_{i1} - \\widehat{\\mu}_0^{\\star}(X_i,1)\\big) - \\big(Y_{i0} - \\widehat{\\mu}_0^{\\star}(X_i,0)\\big)\\Big] = 0,\n    $$\n    where weights $\\omega_i$ for control units are proportional to their odds of being treated, $\\omega_i \\propto \\frac{\\widehat{e}(X_i)}{1-\\widehat{e}(X_i)}$, and normalized to sum to $1$. Substituting the fluctuated model and the definition of $D_i$:\n    $$\n    \\sum_{i:A_i=0} \\omega_i \\Big[\\big(Y_{i1} - \\widehat{\\mu}_0(X_i,1) - \\epsilon\\big) - \\big(Y_{i0} - \\widehat{\\mu}_0(X_i,0)\\big)\\Big] = 0\n    $$\n    $$\n    \\sum_{i:A_i=0} \\omega_i (D_i - \\epsilon) = 0 \\implies \\sum_{i:A_i=0} \\omega_i D_i = \\epsilon \\sum_{i:A_i=0} \\omega_i.\n    $$\n    Since $\\sum_{i:A_i=0} \\omega_i = 1$, the solution is $\\widehat{\\epsilon} = \\sum_{i:A_i=0} \\omega_i D_i$. This $\\widehat{\\epsilon}$ is the weighted average of the pseudo-outcomes in the control group, where weights are chosen to make the control group's covariate distribution resemble that of the treated group. This step corrects for residual confounding that may be left over from an imperfect initial outcome model. We then define the targeted pseudo-outcome for all units as $D_i^{\\star} = D_i - \\widehat{\\epsilon}$.\n\n5.  **Compute Final Estimator**: The final TMLE-DiD estimator is a substitution estimator using the targeted model. This amounts to averaging the targeted pseudo-outcome $D_i^{\\star}$ over the treated group:\n    $$\n    \\widehat{\\tau}_{\\text{TMLE-DiD}} = \\frac{1}{\\sum_{i=1}^{N} A_i}\\sum_{i=1}^{N} A_i D_i^{\\star} = \\mathbb{E}_N[D_i^{\\star} \\mid A_i=1].\n    $$\n\nThis procedure is implemented according to the specified simulation design. The four cases test all combinations of model correctness. We expect Cases 1 (both correct), 2 (outcome correct), and 3 (propensity correct) to yield estimates close to $\\tau=2.0$, demonstrating double robustness. Case 4 (both incorrect) is expected to be biased.", "answer": "```python\nimport numpy as np\n\ndef fit_linear_regression(X_features, y, l2_penalty):\n    \"\"\"\n    Fits a linear regression model with L2 regularization (ridge).\n    The penalty is not applied to the intercept term.\n    beta = (X'X + lambda*I_tilde)^-1 * X'y\n    \"\"\"\n    n_features = X_features.shape[1]\n    lambda_mat = l2_penalty * np.eye(n_features)\n    if n_features > 0:\n        lambda_mat[0, 0] = 0.0 # No penalty on intercept\n    \n    A = X_features.T @ X_features + lambda_mat\n    b = X_features.T @ y\n    try:\n        beta = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        beta = np.linalg.pinv(A) @ b\n    return beta\n\ndef fit_logistic_regression_irls(X_features, y, l2_penalty, max_iter=100, tol=1e-7):\n    \"\"\"\n    Fits logistic regression using Iteratively Reweighted Least Squares (IRLS).\n    The L2 penalty is not applied to the intercept term.\n    \"\"\"\n    n_samples, n_features = X_features.shape\n    beta = np.zeros(n_features)\n    \n    # Penalty matrix I_tilde where I_tilde[0,0] = 0\n    lambda_mat = l2_penalty * np.eye(n_features)\n    lambda_mat[0, 0] = 0.0\n\n    for _ in range(max_iter):\n        eta = X_features @ beta\n        eta = np.clip(eta, -30, 30) # Avoid overflow\n        p = 1 / (1 + np.exp(-eta))\n        \n        # Weights for IRLS\n        w = p * (1 - p)\n        w = np.maximum(w, 1e-10) # Avoid zero weights\n        \n        # Hessian of negative log-likelihood (regularized)\n        H = (X_features.T * w) @ X_features + lambda_mat\n        \n        # Gradient of negative log-likelihood (regularized)\n        grad = X_features.T @ (p - y) + lambda_mat @ beta\n        \n        try:\n            delta = np.linalg.solve(H, grad)\n        except np.linalg.LinAlgError:\n            delta = np.linalg.pinv(H) @ grad\n            \n        beta -= delta\n        \n        if np.linalg.norm(delta) < tol:\n            break\n            \n    return beta\n\ndef predict_proba(X_features, beta):\n    \"\"\"Calculates probabilities from a logistic model.\"\"\"\n    eta = X_features @ beta\n    eta = np.clip(eta, -30, 30)\n    return 1 / (1 + np.exp(-eta))\n    \ndef generate_data(N, params, rng):\n    \"\"\"Generates data according to the specified Data-Generating Process.\"\"\"\n    X = rng.normal(0, 1, size=N)\n    \n    eta_A = params['alpha0'] + params['alpha1'] * X\n    prob_A = 1 / (1 + np.exp(-eta_A))\n    A = rng.binomial(1, prob_A, size=N)\n    \n    U0 = rng.normal(0, params['sigma'], size=N)\n    U1 = rng.normal(0, params['sigma'], size=N)\n    \n    Y0 = params['theta0'] + params['theta1'] * X + U0\n    Y1 = (params['theta0'] + params['theta1'] * X + \n          params['delta'] + params['gamma'] * X + \n          params['tau'] * A + U1)\n          \n    return X, A, Y0, Y1\n\ndef tmle_did_estimator(X, A, Y0, Y1, correct_outcome_model, correct_propensity_model):\n    \"\"\"Computes the Targeted Maximum Likelihood Estimator for DiD.\"\"\"\n    N = len(X)\n    l2_penalty = 1e-8\n    clip_bounds = (1e-6, 1 - 1e-6)\n\n    controls_mask = (A == 0)\n    treated_mask = (A == 1)\n\n    # 1. Estimate outcome regressions on controls\n    if correct_outcome_model:\n        X_feat_mu = np.c_[np.ones(N), X]\n    else:\n        X_feat_mu = np.c_[np.ones(N)]\n    \n    X_feat_mu_controls = X_feat_mu[controls_mask]\n    Y0_controls, Y1_controls = Y0[controls_mask], Y1[controls_mask]\n    \n    beta_mu0_0 = fit_linear_regression(X_feat_mu_controls, Y0_controls, l2_penalty)\n    beta_mu0_1 = fit_linear_regression(X_feat_mu_controls, Y1_controls, l2_penalty)\n    \n    mu0_hat_0 = X_feat_mu @ beta_mu0_0\n    mu0_hat_1 = X_feat_mu @ beta_mu0_1\n    \n    # 2. Form pseudo-outcome differences\n    D = (Y1 - mu0_hat_1) - (Y0 - mu0_hat_0)\n    \n    # 3. Estimate propensity score on full sample\n    if correct_propensity_model:\n        X_feat_e = np.c_[np.ones(N), X]\n    else:\n        X_feat_e = np.c_[np.ones(N), X**2]\n\n    beta_e = fit_logistic_regression_irls(X_feat_e, A, l2_penalty)\n    e_hat = predict_proba(X_feat_e, beta_e)\n    e_hat = np.clip(e_hat, clip_bounds[0], clip_bounds[1])\n    \n    # 4. Targeting step\n    D_controls = D[controls_mask]\n    e_hat_controls = e_hat[controls_mask]\n    \n    w_unscaled = e_hat_controls / (1 - e_hat_controls)\n    sum_w = np.sum(w_unscaled)\n    \n    epsilon_hat = 0.0\n    if sum_w > 1e-12: # Check for non-zero weights\n        w_norm = w_unscaled / sum_w\n        epsilon_hat = np.sum(w_norm * D_controls)\n        \n    D_star = D - epsilon_hat\n\n    # 5. Compute final estimator\n    D_star_treated = D_star[treated_mask]\n    if len(D_star_treated) == 0:\n        return np.nan\n    \n    tau_hat = np.mean(D_star_treated)\n    return tau_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation across four scenarios and print results.\n    \"\"\"\n    sim_params = {\n        'N': 5000,\n        'alpha0': -0.2, 'alpha1': 0.8,\n        'theta0': 1.0, 'theta1': 1.0,\n        'delta': 0.5, 'gamma': 0.5,\n        'tau': 2.0, 'sigma': 1.0\n    }\n    \n    test_cases = [\n        # (correct outcome, correct propensity, random seed)\n        (True, True, 2025),\n        (True, False, 2026),\n        (False, True, 2027),\n        (False, False, 2028),\n    ]\n    \n    results = []\n    \n    for correct_outcome, correct_propensity, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        X, A, Y0, Y1 = generate_data(sim_params['N'], sim_params, rng)\n        \n        tau_hat = tmle_did_estimator(X, A, Y0, Y1, correct_outcome, correct_propensity)\n        \n        results.append(round(tau_hat, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3115416"}]}