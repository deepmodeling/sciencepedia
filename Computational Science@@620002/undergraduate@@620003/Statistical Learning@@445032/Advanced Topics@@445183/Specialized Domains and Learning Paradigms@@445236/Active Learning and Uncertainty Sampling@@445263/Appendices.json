{"hands_on_practices": [{"introduction": "Active learning with uncertainty sampling relies on heuristics to estimate where a model is most confused. This first exercise challenges you to build a deep intuition for two of the most common methods: least-confidence and margin sampling. By deriving their relationship from first principles in a binary classification setting and then analyzing a multi-class scenario where they diverge, you will gain a clear understanding of the subtle but important differences in how they define and react to uncertainty [@problem_id:3095044]. This foundational analysis is key to selecting appropriate strategies in practice.", "problem": "A probabilistic multi-class classifier produces, for each input $x$, class-posterior probabilities $p(y \\mid x)$ over a finite label set $\\mathcal{Y}$, satisfying $\\sum_{y \\in \\mathcal{Y}} p(y \\mid x) = 1$ and $p(y \\mid x) \\in [0,1]$. In Active Learning (AL), uncertainty sampling selects $x$ according to an uncertainty score. Two common uncertainty scores are the least-confidence score and the margin score, defined using the order statistics of the posterior. Let $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p(y \\mid x)$ denote the largest posterior and $p_{(2)}(x)$ denote the second-largest posterior. Least-confidence (LC) sampling chooses $x$ that maximizes $u_{\\mathrm{LC}}(x) = 1 - p_{(1)}(x)$, and margin sampling (MS) chooses $x$ that minimizes $u_{\\mathrm{margin}}(x) = p_{(1)}(x) - p_{(2)}(x)$.\n\nStarting only from these definitions and the facts that the posteriors are nonnegative and sum to $1$, reason about when the two selection rules yield identical choices. In particular, determine whether they are equivalent in binary classification ($K=2$ classes) and whether they can diverge in multi-class settings ($K>2$). Then, for a $K=3$ setting with $\\mathcal{Y} = \\{1,2,3\\}$, consider two unlabeled points with posterior vectors\n- $x_A$: $(p(1 \\mid x_A), p(2 \\mid x_A), p(3 \\mid x_A)) = (\\,0.60,\\,0.39,\\,0.01\\,)$,\n- $x_B$: $(p(1 \\mid x_B), p(2 \\mid x_B), p(3 \\mid x_B)) = (\\,0.51,\\,0.26,\\,0.23\\,)$.\n\nUse the uncertainty definitions to compute $u_{\\mathrm{LC}}(x)$ and $u_{\\mathrm{margin}}(x)$ for $x_A$ and $x_B$, and determine which point each strategy would select.\n\nWhich option is correct?\n\nA. In binary classification ($K=2$), least-confidence and margin sampling always select the same point; in the $K=3$ example, least-confidence selects $x_B$ and margin sampling selects $x_A$.\n\nB. In binary classification ($K=2$), least-confidence and margin sampling can disagree; in the $K=3$ example, both strategies select $x_B$.\n\nC. In binary classification ($K=2$), the two strategies agree only when $p_{(1)}(x) = \\tfrac{1}{2}$; in the $K=3$ example, least-confidence selects $x_A$ and margin sampling selects $x_B$.\n\nD. For any number of classes ($K \\ge 2$), least-confidence and margin sampling always agree; in the $K=3$ example, both strategies select $x_A$.", "solution": "The problem statement is first validated for soundness and completeness.\n\n### Step 1: Extract Givens\n- A probabilistic multi-class classifier produces class-posteriors $p(y \\mid x)$ for an input $x$ over a label set $\\mathcal{Y}$ of size $K = |\\mathcal{Y}|$.\n- The posteriors satisfy $\\sum_{y \\in \\mathcal{Y}} p(y \\mid x) = 1$ and $p(y \\mid x) \\in [0,1]$.\n- Order statistics of the posterior are defined: $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p(y \\mid x)$ (the largest) and $p_{(2)}(x)$ (the second-largest).\n- Least-confidence (LC) sampling selects an unlabeled point $x$ that maximizes the score $u_{\\mathrm{LC}}(x) = 1 - p_{(1)}(x)$.\n- Margin sampling (MS) selects an unlabeled point $x$ that minimizes the score $u_{\\mathrm{margin}}(x) = p_{(1)}(x) - p_{(2)}(x)$.\n- The problem requires analysis of the equivalence of these two rules for binary ($K=2$) and multi-class ($K>2$) classification.\n- For a $K=3$ scenario with $\\mathcal{Y} = \\{1,2,3\\}$, two specific unlabeled points are given:\n  - $x_A$ with posterior vector $(p(1 \\mid x_A), p(2 \\mid x_A), p(3 \\mid x_A)) = (0.60, 0.39, 0.01)$.\n  - $x_B$ with posterior vector $(p(1 \\mid x_B), p(2 \\mid x_B), p(3 \\mid x_B)) = (0.51, 0.26, 0.23)$.\n- The task is to determine which point each strategy would select from the pair $\\{x_A, x_B\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is based on standard, well-established concepts in machine learning, specifically in the subfield of active learning. The definitions for least-confidence and margin sampling are canonical.\n- **Well-Posed:** The problem is clearly defined. The objectives are to compare two selection rules based on their formal definitions and to apply these rules to a concrete numerical example. The existence of a unique and meaningful solution is guaranteed.\n- **Objective:** The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n- **Completeness and Consistency:** All necessary definitions and data are provided. The posterior probabilities for the given examples correctly sum to $1$. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically sound query within the field of statistical learning. The solution will proceed.\n\n### Solution Derivation\n\nThe solution is derived in three parts as requested: analysis for binary classification, analysis for the general multi-class case, and application to the specific $K=3$ example.\n\n**Part 1: Analysis for Binary Classification ($K=2$)**\n\nLet the set of classes be $\\mathcal{Y} = \\{y_1, y_2\\}$. For any input $x$, the posterior probabilities are $p(y_1 \\mid x)$ and $p(y_2 \\mid x)$.\nFrom the sum-to-one property, we have $p(y_1 \\mid x) + p(y_2 \\mid x) = 1$.\nWithout loss of generality, let's assume $p(y_1 \\mid x) \\ge p(y_2 \\mid x)$. This implies $p(y_1 \\mid x) \\ge 0.5$.\nBy definition, the first- and second-largest posteriors are:\n$p_{(1)}(x) = p(y_1 \\mid x)$\n$p_{(2)}(x) = p(y_2 \\mid x) = 1 - p(y_1 \\mid x)$\n\nNow, we analyze the selection rules.\n1.  **Least-Confidence (LC) Sampling:** LC selects the point $x$ that maximizes $u_{\\mathrm{LC}}(x) = 1 - p_{(1)}(x)$. Maximizing $1 - p_{(1)}(x)$ is equivalent to minimizing $p_{(1)}(x)$. So, LC selects $\\arg\\min_x p_{(1)}(x)$. This strategy seeks the point where the classifier is least certain about its most-probable prediction.\n\n2.  **Margin Sampling (MS):** MS selects the point $x$ that minimizes $u_{\\mathrm{margin}}(x) = p_{(1)}(x) - p_{(2)}(x)$. Substituting the expression for $p_{(2)}(x)$ in the binary case:\n    $$u_{\\mathrm{margin}}(x) = p_{(1)}(x) - (1 - p_{(1)}(x)) = 2p_{(1)}(x) - 1$$\n    So, MS selects $\\arg\\min_x (2p_{(1)}(x) - 1)$.\n\nTo compare the strategies, we compare the objective functions they optimize. LC minimizes $p_{(1)}(x)$, and MS minimizes $2p_{(1)}(x) - 1$.\nThe function $f(z) = 2z - 1$ is a strictly increasing linear transformation of $z$. Therefore, for any set of points, the point that minimizes $p_{(1)}(x)$ is precisely the same point that minimizes $2p_{(1)}(x) - 1$.\nFor instance, if we are choosing between $x_i$ and $x_j$, and $p_{(1)}(x_i) < p_{(1)}(x_j)$, then it is also true that $2p_{(1)}(x_i) - 1 < 2p_{(1)}(x_j) - 1$. Both strategies will prefer $x_i$.\nThus, the ranking of all candidate points is identical under both criteria, and they will always select the same point. The first part of the problem statement is established: in binary classification, LC and MS are equivalent selection strategies.\n\n**Part 2: Divergence in Multi-class Settings ($K>2$)**\n\nFor $K > 2$, $p_{(2)}(x)$ is no longer a simple function of $p_{(1)}(x)$. The sum-to-one constraint is $\\sum_{i=1}^K p_{(i)}(x) = 1$, where $p_{(i)}(x)$ are the ordered posteriors. This means $p_{(2)}(x)$ can vary independently of $p_{(1)}(x)$ (within certain constraints).\n- LC sampling still selects $\\arg\\min_x p_{(1)}(x)$. This strategy is myopic, as it only considers the confidence in the top choice, ignoring the distribution of probability among the other classes.\n- MS sampling selects $\\arg\\min_x (p_{(1)}(x) - p_{(2)}(x))$. This strategy explicitly considers the ambiguity between the two most likely classes.\n\nA disagreement can occur if one point has a lower $p_{(1)}$ (favored by LC) while another has a smaller margin $p_{(1)}-p_{(2)}$ (favored by MS). The provided $K=3$ example will serve to demonstrate this divergence.\n\n**Part 3: Analysis of the $K=3$ Example**\n\nWe compute the uncertainty scores for the two given points, $x_A$ and $x_B$.\n\nFor point $x_A$:\n- Posteriors: $(0.60, 0.39, 0.01)$.\n- Ordered posteriors: $p_{(1)}(x_A) = 0.60$, $p_{(2)}(x_A) = 0.39$, $p_{(3)}(x_A) = 0.01$.\n- LC score: $u_{\\mathrm{LC}}(x_A) = 1 - p_{(1)}(x_A) = 1 - 0.60 = 0.40$.\n- Margin score: $u_{\\mathrm{margin}}(x_A) = p_{(1)}(x_A) - p_{(2)}(x_A) = 0.60 - 0.39 = 0.21$.\n\nFor point $x_B$:\n- Posteriors: $(0.51, 0.26, 0.23)$.\n- Ordered posteriors: $p_{(1)}(x_B) = 0.51$, $p_{(2)}(x_B) = 0.26$, $p_{(3)}(x_B) = 0.23$.\n- LC score: $u_{\\mathrm{LC}}(x_B) = 1 - p_{(1)}(x_B) = 1 - 0.51 = 0.49$.\n- Margin score: $u_{\\mathrm{margin}}(x_B) = p_{(1)}(x_B) - p_{(2)}(x_B) = 0.51 - 0.26 = 0.25$.\n\nNow, we determine the selection for each strategy:\n- **LC Selection:** The goal is to **maximize** $u_{\\mathrm{LC}}(x)$. Comparing the scores, $u_{\\mathrm{LC}}(x_B) = 0.49 > u_{\\mathrm{LC}}(x_A) = 0.40$. Therefore, least-confidence sampling selects point **$x_B$**.\n- **MS Selection:** The goal is to **minimize** $u_{\\mathrm{margin}}(x)$. Comparing the scores, $u_{\\mathrm{margin}}(x_A) = 0.21 < u_{\\mathrm{margin}}(x_B) = 0.25$. Therefore, margin sampling selects point **$x_A$**.\n\nThis example confirms that for $K>2$, the two strategies can indeed select different points. LC prefers $x_B$ because its most likely class ($0.51$) is less certain than $x_A$'s most likely class ($0.60$). MS prefers $x_A$ because the two most likely classes ($0.60$ and $0.39$) are closer together than for $x_B$ ($0.51$ and $0.26$), indicating greater ambiguity between the top two contenders.\n\n### Option-by-Option Analysis\n\nBased on the above derivation, each option is evaluated.\n\n**A. In binary classification ($K=2$), least-confidence and margin sampling always select the same point; in the $K=3$ example, least-confidence selects $x_B$ and margin sampling selects $x_A$.**\n- The first part, regarding equivalence in binary classification, was proven to be correct.\n- The second part, regarding the selection for the $K=3$ example, matches our calculations exactly.\n- **Verdict: Correct.**\n\n**B. In binary classification ($K=2$), least-confidence and margin sampling can disagree; in the $K=3$ example, both strategies select $x_B$.**\n- The first part is incorrect. Our proof shows they are equivalent for $K=2$.\n- The second part is incorrect. Our calculation shows that margin sampling selects $x_A$, not $x_B$.\n- **Verdict: Incorrect.**\n\n**C. In binary classification ($K=2$), the two strategies agree only when $p_{(1)}(x) = \\tfrac{1}{2}$; in the $K=3$ example, least-confidence selects $x_A$ and margin sampling selects $x_B$.**\n- The first part is incorrect. Our proof shows they always agree for $K=2$, not just in the special case of maximum uncertainty where $p_{(1)}(x) = 1/2$.\n- The second part is incorrect. The selections are reversed from our findings.\n- **Verdict: Incorrect.**\n\n**D. For any number of classes ($K \\ge 2$), least-confidence and margin sampling always agree; in the $K=3$ example, both strategies select $x_A$.**\n- The first part is incorrect. The $K=3$ example explicitly demonstrates that they can disagree.\n- The second part is incorrect. Our calculation shows that least-confidence sampling selects $x_B$, not $x_A$.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3095044"}, {"introduction": "Moving from theory to practice is a critical step in mastering any machine learning technique. This exercise guides you through the implementation of three canonical uncertainty sampling strategies: least-confidence, margin sampling, and entropy sampling. By writing code to calculate these uncertainty scores for a pool of data points, you will translate abstract mathematical formulas into tangible computational logic [@problem_id:3095122]. This practice solidifies your understanding and provides a practical toolkit for applying active learning in your own projects.", "problem": "You are given a pool-based active learning scenario for multiclass classification, where a probabilistic classifier with fixed parameters $\\hat{\\theta}$ provides, for each unlabeled input $x$, a categorical predictive distribution $p_{\\hat{\\theta}}(y \\mid x)$ over a finite label set $\\mathcal{Y}$. Your task is to implement three uncertainty sampling strategies—least-confidence, margin, and entropy sampling—and to ensure that they produce distinct selections on an appropriately constructed pool. The solution must be a complete, runnable program that evaluates a specified test suite and prints the requested outputs in the exact format.\n\nFundamental base and definitions:\n- For each unlabeled input $x$, the classifier outputs a vector of class probabilities $p_{\\hat{\\theta}}(y \\mid x)$ for $y \\in \\mathcal{Y}$ that satisfies $p_{\\hat{\\theta}}(y \\mid x) \\ge 0$ for all $y$ and $\\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) = 1$.\n- Let $p_{(1)}(x) \\ge p_{(2)}(x) \\ge \\cdots$ denote the sorted probabilities of $p_{\\hat{\\theta}}(y \\mid x)$ in descending order.\n- Least-confidence uncertainty for $x$ is measured by $\\max_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) = p_{(1)}(x)$. Least-confidence sampling selects the $x$ with the smallest $p_{(1)}(x)$ over the pool.\n- Margin uncertainty for $x$ is measured by the difference between the top two probabilities, $p_{(1)}(x) - p_{(2)}(x)$. Margin sampling selects the $x$ with the smallest $p_{(1)}(x) - p_{(2)}(x)$ over the pool.\n- Entropy uncertainty for $x$ is given by the Shannon entropy $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right] = - \\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) \\log p_{\\hat{\\theta}}(y \\mid x)$, where $\\log$ denotes the natural logarithm. Entropy sampling selects the $x$ with the largest $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right]$ over the pool.\n- Ties must be broken deterministically by selecting the smallest index under $0$-based indexing.\n\nProgramming requirements:\n- For each test case, you will be given a finite pool $\\mathcal{U} = \\{x_i\\}_{i=0}^{n-1}$, represented by the matrix of predicted class-probability vectors $\\left( p_{\\hat{\\theta}}(y \\mid x_i) \\right)$. Each row corresponds to one $x_i$, and each row sums to $1$ up to floating-point precision.\n- For each test case, compute:\n  $1.$ The index (under $0$-based indexing) selected by least-confidence sampling and the corresponding value $p_{(1)}(x)$ at that index.\n  $2.$ The index selected by margin sampling and the corresponding value $\\left(p_{(1)}(x) - p_{(2)}(x)\\right)$ at that index.\n  $3.$ The index selected by entropy sampling and the corresponding value $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right]$ at that index.\n  $4.$ The number of distinct indices among the three selections (that is, the cardinality of the set containing the three selected indices).\n- Use the convention $0 \\cdot \\log 0 = 0$ when computing entropy. Use the natural logarithm for $\\log$.\n- All floating-point outputs must be rounded to exactly $6$ decimal places.\n- Ties for any selection criterion must be resolved by choosing the smallest index under $0$-based indexing.\n\nTest suite:\n- Test case $\\mathbf{1}$ (happy path: three criteria select different inputs):\n  - Pool with four unlabeled inputs (rows) and three classes (columns):\n    - $x_0$: $\\left[0.39,\\; 0.38,\\; 0.23\\right]$\n    - $x_1$: $\\left[0.41,\\; 0.295,\\; 0.295\\right]$\n    - $x_2$: $\\left[0.500,\\; 0.495,\\; 0.005\\right]$\n    - $x_3$: $\\left[0.70,\\; 0.20,\\; 0.10\\right]$\n- Test case $\\mathbf{2}$ (boundary: exact tie across all criteria within the pool; tie-breaking by smallest index):\n  - Pool with three unlabeled inputs and three classes:\n    - $x_0$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n    - $x_1$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n    - $x_2$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n- Test case $\\mathbf{3}$ (edge case: zero probabilities present; near-deterministic predictions):\n  - Pool with four unlabeled inputs and three classes:\n    - $x_0$: $\\left[0.99,\\; 0.01,\\; 0.00\\right]$\n    - $x_1$: $\\left[0.60,\\; 0.40,\\; 0.00\\right]$\n    - $x_2$: $\\left[0.49,\\; 0.49,\\; 0.02\\right]$\n    - $x_3$: $\\left[0.55,\\; 0.45,\\; 0.00\\right]$\n\nFinal output specification:\n- For each test case, produce the following seven values in order as a flat sequence:\n  - $\\text{LC\\_idx}$, $p_{(1)}$ at $\\text{LC\\_idx}$,\n  - $\\text{M\\_idx}$, $\\left(p_{(1)} - p_{(2)}\\right)$ at $\\text{M\\_idx}$,\n  - $\\text{ENT\\_idx}$, $H$ at $\\text{ENT\\_idx}$,\n  - $\\text{distinct\\_count}$.\n- Aggregate the results for all test cases into a single list in the order of the test cases. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with all floating-point numbers rounded to exactly $6$ decimal places. For example, the output must have the form $\\left[\\text{v}_0,\\text{v}_1,\\ldots,\\text{v}_{m-1}\\right]$ where each $\\text{v}_j$ is an integer or a floating-point number with exactly $6$ decimal places.", "solution": "The problem statement has been critically examined and is determined to be **valid**. It is scientifically grounded in the principles of statistical learning, specifically active learning. The definitions for least-confidence, margin, and entropy sampling are standard and correct. The problem is well-posed, providing all necessary data, constraints, and deterministic tie-breaking rules, which ensures a unique and computable solution exists for each test case. The inputs are self-consistent, and the objectives are formalizable and objective.\n\nThe task is to implement and compare three distinct uncertainty sampling strategies for a multiclass classifier in a pool-based active learning setting. Given a pool of unlabeled data points $\\mathcal{U}$, each represented by a vector of predictive probabilities $p_{\\hat{\\theta}}(y \\mid x)$ over the set of classes $\\mathcal{Y}$, we must select the single most informative point to query for its true label. The \"informativeness\" or \"uncertainty\" is quantified by three different metrics.\n\nLet $P$ be a matrix of dimensions $n \\times k$, where $n$ is the number of data points in the pool and $k = |\\mathcal{Y}|$ is the number of classes. Each row $P_i$ corresponds to the probability vector $p_{\\hat{\\theta}}(y \\mid x_i)$. The objective is to find the index $i^*$ that corresponds to the most uncertain data point according to each of the following criteria.\n\n**1. Least-Confidence Sampling**\n\nThis strategy is based on the classifier's confidence in its most likely prediction. Uncertainty is considered maximal when the probability of the most probable class is minimal. The most confident prediction for an input $x$ is given by $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x)$. The least-confidence sampling strategy selects the data point $x^*$ from the pool $\\mathcal{U}$ that minimizes this value.\n\nThe selection index $i^*_{LC}$ is therefore:\n$$\ni^*_{LC} = \\arg\\min_{i \\in \\{0, \\dots, n-1\\}} p_{(1)}(x_i)\n$$\nIn case of a tie, the smallest index $i$ is chosen. The associated uncertainty value is $p_{(1)}(x_{i^*_{LC}})$.\n\n**2. Margin Sampling**\n\nThis strategy refines least-confidence sampling by considering the ambiguity between the two most likely classes. A small margin between the first and second most probable classes, $p_{(1)}(x)$ and $p_{(2)}(x)$, indicates that the classifier is struggling to discriminate between them, thus signaling high uncertainty. The margin is defined as $p_{(1)}(x) - p_{(2)}(x)$. Margin sampling selects the data point $x^*$ that has the smallest margin.\n\nThe selection index $i^*_{M}$ is:\n$$\ni^*_{M} = \\arg\\min_{i \\in \\{0, \\dots, n-1\\}} \\left( p_{(1)}(x_i) - p_{(2)}(x_i) \\right)\n$$\nAgain, ties are broken by selecting the smallest index. The associated uncertainty value is the margin itself, $p_{(1)}(x_{i^*_{M}}) - p_{(2)}(x_{i^*_{M}})$.\n\n**3. Entropy Sampling**\n\nThis is the most comprehensive of the three metrics as it takes into account the entire probability distribution. It uses Shannon entropy as a measure of uncertainty. A probability distribution that is sharply peaked (one high probability, others low) has low entropy, indicating low uncertainty. Conversely, a distribution that is close to uniform has high entropy, indicating high uncertainty. The entropy for a given input $x$ is:\n$$\nH[Y \\mid x, \\hat{\\theta}] = - \\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) \\log p_{\\hat{\\theta}}(y \\mid x)\n$$\nwhere $\\log$ is the natural logarithm, and the convention $0 \\cdot \\log 0 = 0$ is used. Entropy sampling selects the data point $x^*$ with the maximum entropy.\n\nThe selection index $i^*_{ENT}$ is:\n$$\ni^*_{ENT} = \\arg\\max_{i \\in \\{0, \\dots, n-1\\}} H[Y \\mid x_i, \\hat{\\theta}]\n$$\nTies are broken by selecting the smallest index. The associated uncertainty value is the entropy $H[Y \\mid x_{i^*_{ENT}}, \\hat{\\theta}]$.\n\n**Computational Procedure**\n\nFor each test case represented by an $n \\times k$ probability matrix $P$:\n1.  **Least-Confidence:** For each row, find the maximum probability. Then, find the index of the row with the minimum of these maximums.\n2.  **Margin:** For each row, sort the probabilities in descending order to find $p_{(1)}$ and $p_{(2)}$. Compute their difference. Then, find the index of the row with the minimum difference.\n3.  **Entropy:** For each row, compute the Shannon entropy, handling the $p=0$ case correctly by ensuring terms of the form $0 \\cdot \\log 0$ contribute $0$ to the sum. Then, find the index of the row with the maximum entropy.\n4.  **Distinct Indices:** Collect the three resulting indices ($i^*_{LC}$, $i^*_{M}$, $i^*_{ENT}$) and count the number of unique values in this collection.\n5.  **Formatting:** All floating-point results are rounded to $6$ decimal places and collated with the integer indices and counts into a single flat list as per the problem specification.\n\nThe implementation will utilize the `numpy` library, whose `argmin` and `argmax` functions inherently satisfy the specified tie-breaking rule by returning the first encountered index of the extremal value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements least-confidence, margin, and entropy uncertainty sampling\n    for a series of test cases and formats the output as specified.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        [\n            [0.39, 0.38, 0.23],\n            [0.41, 0.295, 0.295],\n            [0.500, 0.495, 0.005],\n            [0.70, 0.20, 0.10],\n        ],\n        # Test case 2\n        [\n            [1/3, 1/3, 1/3],\n            [1/3, 1/3, 1/3],\n            [1/3, 1/3, 1/3],\n        ],\n        # Test case 3\n        [\n            [0.99, 0.01, 0.00],\n            [0.60, 0.40, 0.00],\n            [0.49, 0.49, 0.02],\n            [0.55, 0.45, 0.00],\n        ]\n    ]\n\n    results = []\n    for case in test_cases:\n        p_matrix = np.array(case, dtype=np.float64)\n\n        # 1. Least-Confidence Sampling\n        # The goal is to find the point with the smallest most-confident prediction.\n        p1_values = np.max(p_matrix, axis=1)\n        lc_idx = np.argmin(p1_values)\n        lc_val = p1_values[lc_idx]\n\n        # 2. Margin Sampling\n        # The goal is to find the point with the smallest difference between the\n        # top two predictions.\n        # Sort probabilities in descending order for each row.\n        sorted_p = -np.sort(-p_matrix, axis=1)\n        p1 = sorted_p[:, 0]\n        p2 = sorted_p[:, 1]\n        margin_values = p1 - p2\n        m_idx = np.argmin(margin_values)\n        m_val = margin_values[m_idx]\n\n        # 3. Entropy Sampling\n        # The goal is to find the point with the highest Shannon entropy.\n        # We must handle the 0 * log(0) = 0 case.\n        # Create a temporary array where p=0 is replaced by p=1.\n        # This makes log(p)=0, so p*log(p) becomes 0, satisfying the convention.\n        p_for_log = p_matrix.copy()\n        p_for_log[p_for_log == 0] = 1.0\n        entropy_values = -np.sum(p_matrix * np.log(p_for_log), axis=1)\n        ent_idx = np.argmax(entropy_values)\n        ent_val = entropy_values[ent_idx]\n\n        # 4. Count of distinct indices\n        distinct_count = len({lc_idx, m_idx, ent_idx})\n\n        # Append results for the current test case.\n        results.extend([\n            lc_idx, lc_val,\n            m_idx, m_val,\n            ent_idx, ent_val,\n            distinct_count\n        ])\n\n    def format_val(v):\n        \"\"\"Formats an integer or float according to problem specs.\"\"\"\n        if isinstance(v, (int, np.integer)):\n            return str(v)\n        else:  # float or np.floating\n            return f\"{v:.6f}\"\n\n    # Format the final list of results for printing.\n    formatted_results = [format_val(r) for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3095122"}, {"introduction": "The ultimate goal of active learning is to achieve higher model performance with fewer labeled examples. This final practice provides a framework for quantifying this efficiency gain. You will fit an empirical power-law model, $R(n) = \\alpha n^{-\\beta} + \\gamma$, to learning curve data generated under both active and passive sampling scenarios [@problem_id:3095092]. By using this model to extrapolate performance, you can estimate the label savings offered by active learning to reach a specific performance target, connecting the mechanics of the sampling strategy to its overarching economic value.", "problem": "A learning curve for expected generalization risk in supervised statistical learning is often well-approximated by an empirical power-law-plus-plateau model of the form $R(n)=\\alpha n^{-\\beta}+\\gamma$, where $R(n)$ denotes the expected risk at $n$ labeled examples, $\\alpha \\ge 0$ is a scale parameter, $\\beta > 0$ is the decay rate, and $\\gamma \\ge 0$ is the irreducible-plateau term. Consider comparing passive sampling (independent and identically distributed labels drawn from the data distribution) to active learning with uncertainty sampling (a strategy that preferentially labels inputs on which the current model is most uncertain). We assume that both strategies produce noisy measurements of the risk for various label counts, and we fit the parameters of the learning curve model by nonlinear least squares.\n\nStarting from the fundamental base that expected risk $R(n)$ is nonincreasing in $n$ for well-posed learning problems and that reductions in $R(n)$ with increasing $n$ are commonly captured by the empirical model $R(n)=\\alpha n^{-\\beta}+\\gamma$, derive an expression for the smallest label count $n_{\\text{req}}$ such that $R(n)\\le \\epsilon$ for a given target error threshold $\\epsilon$. Use this expression to estimate the integer label savings between passive sampling and active sampling needed to reach the same target $\\epsilon$.\n\nYour task is to write a complete, runnable program that:\n- Fits the model parameters $(\\alpha,\\beta,\\gamma)$ to each strategy’s measured $(n,R)$ pairs via bounded nonlinear least squares.\n- Computes the minimal integer label count $n_{\\text{req}}$ for each strategy that satisfies $R(n)\\le \\epsilon$.\n- Computes the label savings $S = n_{\\text{req}}^{\\text{passive}} - n_{\\text{req}}^{\\text{active}}$ using the following conventions:\n    - If $\\epsilon \\le \\gamma$ for a strategy, then that strategy cannot reach the target, and $n_{\\text{req}}=+\\infty$.\n    - If both strategies yield $n_{\\text{req}}=+\\infty$, return $\\mathrm{NaN}$ for $S$.\n    - If exactly one strategy yields $n_{\\text{req}}=+\\infty$, return $+\\infty$ if passive is the one that cannot reach the target and $-\\infty$ if active is the one that cannot reach the target.\n    - Otherwise, return $S$ as a finite integer difference.\n- Uses the ceiling function to ensure $n_{\\text{req}}$ is an integer label count when finite.\n\nUse the following test suite of measured $(n,R)$ pairs and target $\\epsilon$ values. Each pair is given as $(n,R)$ with $n$ in labels, $R$ dimensionless, and $\\epsilon$ dimensionless. For each case, fit separate models to the active and passive measurements and then compute the label savings as specified.\n\nTest Case $1$ (happy path, active is more sample-efficient):\n- Active measurements: $(20,0.154)$, $(50,0.119)$, $(100,0.104)$, $(200,0.095)$, $(500,0.088)$, $(1000,0.085)$.\n- Passive measurements: $(20,0.322)$, $(50,0.242)$, $(100,0.199)$, $(200,0.171)$, $(500,0.145)$, $(1000,0.132)$.\n- Target: $\\epsilon=0.12$.\n\nTest Case $2$ (boundary case, $\\epsilon$ is close to the active plateau but below the passive plateau):\n- Active measurements: $(50,0.117)$, $(100,0.094)$, $(200,0.0795)$, $(500,0.067)$, $(1000,0.061)$, $(2000,0.057)$.\n- Passive measurements: $(50,0.236)$, $(100,0.190)$, $(200,0.158)$, $(500,0.129)$, $(1000,0.115)$, $(2000,0.105)$.\n- Target: $\\epsilon=0.055$.\n\nTest Case $3$ (edge case, target below both plateaus):\n- Active measurements: $(50,0.151)$, $(100,0.129)$, $(200,0.115)$, $(500,0.102)$, $(1000,0.096)$.\n- Passive measurements: $(50,0.190)$, $(100,0.160)$, $(200,0.139)$, $(500,0.121)$, $(1000,0.112)$.\n- Target: $\\epsilon=0.07$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be the label savings $S$ for the corresponding test case, expressed as an integer if finite, or as $inf$, $-inf$, or $nan$ as specified above. No additional text should be printed.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It is free of contradictions, ambiguities, or unsound premises. The task is a standard application of nonlinear regression and model-based extrapolation within the field of statistical learning.\n\nA learning curve for expected generalization risk, $R(n)$, as a function of the number of labeled examples, $n$, is given by the empirical model:\n$$R(n) = \\alpha n^{-\\beta} + \\gamma$$\nThe parameters are constrained as $\\alpha \\ge 0$, $\\beta > 0$, and $\\gamma \\ge 0$. The parameter $\\alpha$ is a scale factor, $\\beta$ is the learning rate, and $\\gamma$ is the irreducible error, which is the asymptotic risk as $n \\to \\infty$.\n\nThe primary goal is to derive an expression for the minimum integer number of samples, $n_{\\text{req}}$, needed to achieve a target risk level $\\epsilon$. This requires finding the smallest integer $n$ such that $R(n) \\le \\epsilon$.\n\nSince $R(n)$ is a non-increasing function of $n$ (for $\\alpha, \\beta > 0$), we can find the boundary case by setting $R(n) = \\epsilon$:\n$$\\alpha n^{-\\beta} + \\gamma = \\epsilon$$\nWe proceed to solve this equation for $n$:\n$$\\alpha n^{-\\beta} = \\epsilon - \\gamma$$\nFor a solution to exist, the sign of both sides must be consistent. The left side, $\\alpha n^{-\\beta}$, is non-negative because $\\alpha \\ge 0$ and $n > 0$. Thus, we must have $\\epsilon - \\gamma \\ge 0$, which implies $\\epsilon \\ge \\gamma$.\n\nCase 1: The target error is unreachable.\nIf $\\epsilon \\le \\gamma$, the target error $\\epsilon$ is less than or equal to the irreducible error $\\gamma$. For any non-trivial learning process ($\\alpha>0$), the risk $R(n) = \\alpha n^{-\\beta} + \\gamma$ will always be strictly greater than $\\gamma$, and therefore greater than $\\epsilon$. In this scenario, the target error is unattainable. As per the problem's specification, the required number of samples is infinite:\n$$n_{\\text{req}} = +\\infty \\quad \\text{if} \\quad \\epsilon \\le \\gamma$$\n\nCase 2: The target error is reachable.\nIf $\\epsilon > \\gamma$, a finite sample size can achieve the target. We can continue solving for $n$. Assuming $\\alpha > 0$:\n$$n^{-\\beta} = \\frac{\\epsilon - \\gamma}{\\alpha}$$\nTaking the reciprocal of both sides:\n$$n^{\\beta} = \\frac{\\alpha}{\\epsilon - \\gamma}$$\nRaising both sides to the power of $1/\\beta$:\n$$n = \\left( \\frac{\\alpha}{\\epsilon - \\gamma} \\right)^{1/\\beta}$$\nThis value of $n$ is the precise real number of samples required to achieve $R(n) = \\epsilon$. Since $R(n)$ is non-increasing, any sample size greater than or equal to this value will also meet the criterion $R(n) \\le \\epsilon$. The problem requests the *smallest integer* label count, which is found by applying the ceiling function. To ensure the number of samples is at least $1$, we take the maximum of the result and $1$.\n$$n_{\\text{req}} = \\max\\left(1, \\left\\lceil \\left( \\frac{\\alpha}{\\epsilon - \\gamma} \\right)^{1/\\beta} \\right\\rceil\\right) \\quad \\text{if} \\quad \\epsilon > \\gamma$$\n\nThe computational procedure to solve the problem is as follows:\n1.  For each sampling strategy (active and passive) in each test case, the model parameters $(\\alpha, \\beta, \\gamma)$ are determined by fitting the function $R(n)$ to the provided $(n, R)$ data points. This is done using bounded nonlinear least squares, specifically with the `scipy.optimize.curve_fit` function. The bounds for the parameters are set to respect the constraints $\\alpha \\ge 0$, $\\beta > 0$, and $\\gamma \\ge 0$. A small positive lower bound (e.g., $10^{-9}$) is used for $\\beta$ to ensure numerical stability and adhere to the strict inequality.\n2.  Once the optimal parameters $(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\gamma})$ are found for a strategy, the required sample size, $n_{\\text{req}}$, is calculated for the given target risk $\\epsilon$ using the derived expression.\n3.  The label savings, $S = n_{\\text{req}}^{\\text{passive}} - n_{\\text{req}}^{\\text{active}}$, is then computed based on the specific rules provided for handling finite, infinite ($+\\infty, -\\infty$), and undefined ($\\mathrm{NaN}$) outcomes.\n\nThis complete, principled approach is implemented for each of the provided test cases.\n\nThe data provided are:\nTest Case $1$:\n- Active measurements: $(20, 0.154)$, $(50, 0.119)$, $(100, 0.104)$, $(200, 0.095)$, $(500, 0.088)$, $(1000, 0.085)$.\n- Passive measurements: $(20, 0.322)$, $(50, 0.242)$, $(100, 0.199)$, $(200, 0.171)$, $(500, 0.145)$, $(1000, 0.132)$.\n- Target: $\\epsilon=0.12$.\n\nTest Case $2$:\n- Active measurements: $(50, 0.117)$, $(100, 0.094)$, $(200, 0.0795)$, $(500, 0.067)$, $(1000, 0.061)$, $(2000, 0.057)$.\n- Passive measurements: $(50, 0.236)$, $(100, 0.190)$, $(200, 0.158)$, $(500, 0.129)$, $(1000, 0.115)$, $(2000, 0.105)$.\n- Target: $\\epsilon=0.055$.\n\nTest Case $3$:\n- Active measurements: $(50, 0.151)$, $(100, 0.129)$, $(200, 0.115)$, $(500, 0.102)$, $(1000, 0.096)$.\n- Passive measurements: $(50, 0.190)$, $(100, 0.160)$, $(200, 0.139)$, $(500, 0.121)$, $(1000, 0.112)$.\n- Target: $\\epsilon=0.07$.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Solves the learning curve analysis problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"active\": {\n                \"n\": np.array([20, 50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.154, 0.119, 0.104, 0.095, 0.088, 0.085]),\n            },\n            \"passive\": {\n                \"n\": np.array([20, 50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.322, 0.242, 0.199, 0.171, 0.145, 0.132]),\n            },\n            \"epsilon\": 0.12,\n        },\n        {\n            \"active\": {\n                \"n\": np.array([50, 100, 200, 500, 1000, 2000]),\n                \"R\": np.array([0.117, 0.094, 0.0795, 0.067, 0.061, 0.057]),\n            },\n            \"passive\": {\n                \"n\": np.array([50, 100, 200, 500, 1000, 2000]),\n                \"R\": np.array([0.236, 0.190, 0.158, 0.129, 0.115, 0.105]),\n            },\n            \"epsilon\": 0.055,\n        },\n        {\n            \"active\": {\n                \"n\": np.array([50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.151, 0.129, 0.115, 0.102, 0.096]),\n            },\n            \"passive\": {\n                \"n\": np.array([50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.190, 0.160, 0.139, 0.121, 0.112]),\n            },\n            \"epsilon\": 0.07,\n        },\n    ]\n\n    def learning_curve_model(n, alpha, beta, gamma):\n        \"\"\"Power-law-plus-plateau model for learning curves.\"\"\"\n        return alpha * n**(-beta) + gamma\n\n    def get_n_req(n_data, r_data, epsilon):\n        \"\"\"\n        Fits the learning curve model and calculates the required number of samples.\n        \"\"\"\n        # Heuristic initial guesses for parameters\n        gamma_guess = r_data[-1]\n        beta_guess = 0.5\n        alpha_guess = (r_data[0] - gamma_guess) * (n_data[0]**beta_guess)\n        p0 = [alpha_guess, beta_guess, gamma_guess]\n\n        # Parameter bounds: alpha >= 0, beta > 0, gamma >= 0\n        bounds = ([0, 1e-9, 0], [np.inf, np.inf, np.inf])\n\n        try:\n            params, _ = curve_fit(\n                learning_curve_model,\n                n_data,\n                r_data,\n                p0=p0,\n                bounds=bounds,\n                maxfev=5000\n            )\n        except RuntimeError:\n            # If fit fails, assume it's impossible to determine n_req\n            return np.inf\n\n        alpha, beta, gamma = params\n\n        # If target epsilon is below or at the irreducible error plateau, it's unreachable.\n        if epsilon <= gamma:\n            return np.inf\n\n        # Handle the case where the numerator or the entire base might be problematic\n        base = alpha / (epsilon - gamma)\n        if base < 0:\n             # This can happen due to numerical precision issues if alpha is near zero\n             # or epsilon is extremely close to gamma. Treat as unreachable.\n            return np.inf\n\n        # Calculate required sample size n, ensuring it's at least 1.\n        n_calc = base**(1 / beta)\n        n_required = np.ceil(n_calc)\n        \n        return max(1, n_required)\n\n    results = []\n    for case in test_cases:\n        n_req_active = get_n_req(case[\"active\"][\"n\"], case[\"active\"][\"R\"], case[\"epsilon\"])\n        n_req_passive = get_n_req(case[\"passive\"][\"n\"], case[\"passive\"][\"R\"], case[\"epsilon\"])\n\n        is_active_inf = np.isinf(n_req_active)\n        is_passive_inf = np.isinf(n_req_passive)\n\n        if is_active_inf and is_passive_inf:\n            savings = np.nan\n        elif is_passive_inf:\n            savings = np.inf\n        elif is_active_inf:\n            savings = -np.inf\n        else:\n            savings = int(n_req_passive - n_req_active)\n        \n        results.append(savings)\n\n    def format_result(val):\n        if np.isnan(val):\n            return \"nan\"\n        elif np.isposinf(val):\n            return \"inf\"\n        elif np.isneginf(val):\n            return \"-inf\"\n        else:\n            return str(int(val))\n\n    formatted_results = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3095092"}]}