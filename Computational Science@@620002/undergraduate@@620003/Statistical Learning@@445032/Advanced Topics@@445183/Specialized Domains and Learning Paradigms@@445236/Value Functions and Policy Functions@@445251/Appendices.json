{"hands_on_practices": [{"introduction": "To master value and policy functions, we first return to the fundamentals of optimal control. This exercise challenges you to act as a planner with perfect knowledge of the environment's dynamics. By applying the principle of dynamic programming, you will work backward from the final time step to compute the optimal value function and policy, revealing the non-stationary nature of optimal decision-making in finite-horizon problems [@problem_id:3190850].", "problem": "Consider the following finite-horizon Markov Decision Process (MDP) with horizon $H = 3$. The state space is $\\mathcal{S} = \\{s, j, z\\}$. The action space depends on the state: at state $s$ the agent can choose between actions $a$ and $b$; at states $j$ and $z$ there is a single available action. The one-step reward function $r(\\cdot,\\cdot)$ and transition dynamics are given as follows:\n- If the current state is $s$ and action $a$ is chosen, then the immediate reward is $r(s,a) = 2$, and the next state is $z$ with probability $1$.\n- If the current state is $s$ and action $b$ is chosen, then the immediate reward is $r(s,b) = 0$, and the next state is $j$ with probability $\\tfrac{1}{2}$ and $s$ with probability $\\tfrac{1}{2}$.\n- If the current state is $j$, the immediate reward is $r(j,\\cdot) = 5$, and the next state is $z$ with probability $1$.\n- If the current state is $z$, the immediate reward is $r(z,\\cdot) = 0$, and the next state is $z$ with probability $1$.\n\nAssume episodes have exactly $H = 3$ decision stages, and rewards are accumulated at each stage $t \\in \\{1,2,3\\}$ from the current state-action pair at that stage. Let a time-dependent policy be denoted by $\\pi = \\{\\pi_{1}, \\pi_{2}, \\pi_{3}\\}$, where $\\pi_{t}$ maps the current state to an action at time $t$. Let the finite-horizon value function of a policy be defined by\n$$\nV_{t}^{\\pi}(x) \\equiv \\mathbb{E}^{\\pi}\\!\\left[\\sum_{k=t}^{H} r\\!\\left(S_{k}, A_{k}\\right)\\,\\middle|\\, S_{t} = x \\right],\n$$\nfor any state $x \\in \\mathcal{S}$ and time $t \\in \\{1,2,3\\}$. Here $\\mathbb{E}^{\\pi}[\\cdot]$ denotes the expectation under the trajectory distribution induced by policy $\\pi$, the Markov property, and the given transition probabilities.\n\nUsing only these definitions, the Markov property, and the law of total expectation, do the following:\n1) Determine, for each time $t \\in \\{1,2,3\\}$, which action maximizes the expected return at state $s$ under an optimal policy, and thereby justify whether the optimal policy is stationary or non-stationary over the horizon.\n2) Compute the optimal finite-horizon values $V_{t}^{\\ast}(s)$ for $t \\in \\{1,2,3\\}$ and compare them across $t$, explaining why they differ.\n\nProvide the exact value of $V_{1}^{\\ast}(s)$ as a reduced fraction as your final answer. Do not round; provide an exact value.", "solution": "The problem provides a complete and consistent description of a finite-horizon Markov Decision Process (MDP). It is scientifically grounded in the theory of stochastic optimal control, well-posed, and objective. All necessary parameters—state space $\\mathcal{S}$, action space $\\mathcal{A}$, reward function $r$, transition probabilities $P$, and horizon $H$—are clearly defined. The problem is valid and can be solved using the standard method of dynamic programming (backward induction).\n\nThe value function for a policy $\\pi = \\{\\pi_1, \\pi_2, \\dots, \\pi_H\\}$ starting from state $x$ at time $t$ is defined as:\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[\\sum_{k=t}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\nWe can derive the recursive relationship for this value function. By splitting the sum and applying the law of total expectation:\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[r(S_t, A_t) + \\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_t, A_t) | S_t = x] + \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\nGiven the policy $A_t = \\pi_t(S_t)$, the first term is $r(x, \\pi_t(x))$. For the second term, we condition on the next state $S_{t+1}$:\n$$\\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right] = \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) \\, \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_{t+1} = x' \\right]$$\nThe inner expectation is, by definition, $V_{t+1}^{\\pi}(x')$. Thus, we obtain the Bellman equation for a given policy $\\pi$:\n$$V_{t}^{\\pi}(x) = r(x, \\pi_t(x)) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) V_{t+1}^{\\pi}(x')$$\nThe optimal value function, $V_t^*(x) = \\max_{\\pi} V_t^\\pi(x)$, is found by choosing the action that maximizes the right-hand side at each step. This gives the Bellman optimality equation for a finite horizon:\n$$V_t^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_{t+1}^*(x') \\right\\}$$\nThe process starts at the final time step $H$ and works backward. For $t=H=3$, the sum in the value function definition has only one term:\n$$V_3^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_3, A_3) | S_3=x] = r(x, \\pi_3(x))$$\nTherefore, the optimal value function at time $t=3$ is simply the maximum immediate reward:\n$$V_3^*(x) = \\max_{u \\in \\mathcal{A}(x)} r(x, u)$$\nThis can be seen as applying the Bellman equation with a terminal condition $V_{H+1}^*(x) = V_4^*(x) = 0$ for all states $x \\in \\mathcal{S}$.\n\nWe now solve the problem by backward induction. The horizon is $H=3$.\n\n**Time Step $t=3$:**\nWe compute $V_3^*(x)$ for each state $x \\in \\{s, j, z\\}$.\n-   For state $s$: $V_3^*(s) = \\max_{u \\in \\{a,b\\}} r(s,u) = \\max\\{r(s,a), r(s,b)\\} = \\max\\{2, 0\\} = 2$. The optimal action is $\\pi_3^*(s) = a$.\n-   For state $j$: There is a single action. $V_3^*(j) = r(j, \\cdot) = 5$.\n-   For state $z$: There is a single action. $V_3^*(z) = r(z, \\cdot) = 0$.\n\n**Time Step $t=2$:**\nWe compute $V_2^*(x)$ using the values of $V_3^*(x')$.\n$$V_2^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_3^*(x') \\right\\}$$\n-   For state $s$: We compare the value of taking action $a$ versus action $b$.\n    -   Action $a$: The expected return is $r(s, a) + P(z|s,a)V_3^*(z) = 2 + 1 \\cdot 0 = 2$.\n    -   Action $b$: The expected return is $r(s, b) + P(j|s,b)V_3^*(j) + P(s|s,b)V_3^*(s) = 0 + \\frac{1}{2} \\cdot V_3^*(j) + \\frac{1}{2} \\cdot V_3^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(2) = \\frac{5}{2} + 1 = \\frac{7}{2}$.\n    -   $V_2^*(s) = \\max\\{2, \\frac{7}{2}\\} = \\frac{7}{2}$. The optimal action is $\\pi_2^*(s) = b$.\n-   For state $j$: $V_2^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_3^*(z) = 5 + 1 \\cdot 0 = 5$.\n-   For state $z$: $V_2^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_3^*(z) = 0 + 1 \\cdot 0 = 0$.\n\n**Time Step $t=1$:**\nWe compute $V_1^*(x)$ using the values of $V_2^*(x')$.\n$$V_1^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_2^*(x') \\right\\}$$\n-   For state $s$: We compare the value of taking action $a$ versus action $b$.\n    -   Action $a$: The expected return is $r(s, a) + P(z|s,a)V_2^*(z) = 2 + 1 \\cdot 0 = 2$.\n    -   Action $b$: The expected return is $r(s, b) + P(j|s,b)V_2^*(j) + P(s|s,b)V_2^*(s) = 0 + \\frac{1}{2} \\cdot V_2^*(j) + \\frac{1}{2} \\cdot V_2^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(\\frac{7}{2}) = \\frac{5}{2} + \\frac{7}{4} = \\frac{10}{4} + \\frac{7}{4} = \\frac{17}{4}$.\n    -   $V_1^*(s) = \\max\\{2, \\frac{17}{4}\\} = \\frac{17}{4}$. The optimal action is $\\pi_1^*(s) = b$.\n-   For state $j$: $V_1^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_2^*(z) = 5 + 1 \\cdot 0 = 5$.\n-   For state $z$: $V_1^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_2^*(z) = 0 + 1 \\cdot 0 = 0$.\n\nWe can now address the specific questions.\n\n1) For each time $t \\in \\{1,2,3\\}$, the optimal action at state $s$ is:\n-   At $t=3$, $\\pi_3^*(s) = a$, because $r(s,a)=2 > r(s,b)=0$.\n-   At $t=2$, $\\pi_2^*(s) = b$, because the expected future return from action $b$ (which is $\\frac{7}{2}$) is greater than the return from action $a$ (which is $2$).\n-   At $t=1$, $\\pi_1^*(s) = b$, because the expected future return from action $b$ (which is $\\frac{17}{4}$) is greater than the return from action $a$ (which is $2$).\nA policy is stationary if the optimal action for any given state does not change with time. Here, the optimal action for state $s$ is not constant: $\\pi_1^*(s) = b$, $\\pi_2^*(s) = b$, but $\\pi_3^*(s) = a$. Therefore, the optimal policy $\\pi^*$ is **non-stationary**. This is a characteristic feature of finite-horizon MDPs; the choice of action depends on the time remaining. At $t=3$, with no future, the agent greedily takes the immediate reward. At $t=1$ and $t=2$, the agent forgoes an immediate reward of $2$ by choosing action $b$ in favor of a chance to transition to states with higher future value ($j$ and $s$ itself), leading to a greater overall expected return.\n\n2) The optimal finite-horizon values $V_t^*(s)$ for $t \\in \\{1,2,3\\}$ are:\n-   $V_1^*(s) = \\frac{17}{4} = 4.25$\n-   $V_2^*(s) = \\frac{7}{2} = 3.5$\n-   $V_3^*(s) = 2$\nComparing them, we see that $V_1^*(s) > V_2^*(s) > V_3^*(s)$.\nThe value $V_t^*(s)$ represents the maximum possible expected total reward accumulated from time $t$ to the end of the horizon at $t=3$.\n$V_3^*(s)$ is the expected reward over one stage ($k=3$).\n$V_2^*(s)$ is the expected reward over two stages ($k=2,3$).\n$V_1^*(s)$ is the expected reward over three stages ($k=1,2,3$).\nSince all one-step rewards in this MDP are non-negative, the cumulative reward is non-decreasing as the number of stages increases. At each step before the last, there is an opportunity to accumulate more reward. For example, $V_2^*(s)$ is greater than $V_3^*(s)$ because starting at $t=2$ gives an additional time step to gather rewards compared to starting at $t=3$. The strict inequality holds because at $t=2$, the optimal action at $s$ leads to states where positive rewards can be obtained at $t=3$. Similarly, $V_1^*(s)$ is greater than $V_2^*(s)$ because there is one more stage ($t=1$) to collect rewards.\n\nThe final answer required is the exact value of $V_1^*(s)$.\n$$V_1^*(s) = \\frac{17}{4}$$", "answer": "$$\\boxed{\\frac{17}{4}}$$", "id": "3190850"}, {"introduction": "Moving from planning with a known model to learning from interaction introduces the classic exploration-exploitation dilemma. This practice explores how Q-learning addresses this challenge through the clever heuristic of \"optimism in the face of uncertainty.\" You will first conceptually prove how initializing action-values optimistically encourages a greedy agent to systematically explore its environment, and then calculate the tangible effect of this strategy on convergence time in a simple, deterministic world [@problem_id:3190816].", "problem": "Consider a deterministic Markov Decision Process (MDP) with finite state set and bounded rewards. Let the agent use tabular action-value learning with the one-step update\n$$\nQ_{t+1}(s_{t},a_{t}) \\leftarrow (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right),\n$$\nwhere $Q_{0}(s,a)=Q_{0}>0$ for all state-action pairs, the learning rate is $\\alpha \\in (0,1]$, and the discount factor is $\\gamma \\in (0,1)$. The agent selects actions greedily with respect to $Q_{t}$ at each decision time $t$, and ties are broken by a fixed, state-dependent but arbitrary ordering. Rewards are bounded by $0 \\le r_{t} \\le R_{\\max}$.\n\nPart A (conceptual). Using only the definitions above, and without assuming any particular MDP structure beyond determinism and bounded rewards, justify why an optimistic initialization $Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ encourages exploration under a purely greedy policy. In particular, argue that in any state, once an action has been tried at least once, its $Q$-value drops below the $Q$-values of any as-yet-untried actions in that state, ensuring the agent will eventually try all actions available in that state.\n\nPart B (calculation). Now specialize to a deterministic gridworld with a single nonterminal start cell $s$ and a terminal goal cell $g$. There are exactly two actions available in $s$:\n- Action $a_{R}$ deterministically moves to $g$ and yields reward $r=1$, after which the episode ends and the agent is reset to $s$ with no further reward.\n- Action $a_{L}$ deterministically leaves the agent in $s$ and yields reward $r=0$.\n\nLet the agent use the same one-step update with learning rate $\\alpha=1$, discount factor $\\gamma \\in (0,1)$, greedy action selection with ties broken in favor of $a_{L}$, and initialization $Q_{0}(s,a_{L})=Q_{0}(s,a_{R})=Q_{0}>0$, while $Q_{0}(g,\\cdot)=0$. Define the convergence time $T$ to be the smallest integer such that, after exactly $T$ action selections and their associated updates, the greedy action at the next visit to $s$ is $a_{R}$ and remains $a_{R}$ for all subsequent visits to $s$.\n\nDerive a closed-form expression for $T$ as a function of $Q_{0}$ and $\\gamma$. Your final answer must be a single analytic expression, not an inequality or equation. No numerical rounding is required.", "solution": "The user has provided a two-part problem concerning Q-learning in a deterministic Markov Decision Process. The problem is well-posed, scientifically grounded in the principles of reinforcement learning, and contains all necessary information for a complete solution.\n\n### Part A: Justification of Exploration via Optimistic Initialization\n\nThe goal is to justify why an optimistic initialization $Q_{0}  \\frac{R_{\\max}}{1-\\gamma}$ encourages exploration under a purely greedy policy. Exploration, in this context, means that the agent will eventually try all available actions in any given state.\n\nFirst, we must understand the significance of the quantity $\\frac{R_{\\max}}{1-\\gamma}$. For any state $s$ and any policy $\\pi$, the state-value function $V^{\\pi}(s)$ is the expected discounted sum of future rewards starting from state $s$ and following policy $\\pi$. Since rewards are bounded such that $r_t \\le R_{\\max}$, the maximum possible value of any state is bounded by the sum of discounted maximum rewards:\n$$V^*(s) = \\max_{\\pi} V^{\\pi}(s) \\le \\sum_{t=0}^{\\infty} \\gamma^t R_{\\max} = \\frac{R_{\\max}}{1-\\gamma}$$\nThe optimal action-value function $Q^*(s,a)$ is similarly bounded. Therefore, the condition $Q_{0}(s,a) = Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ for all $(s,a)$ pairs means that all initial Q-values are strictly greater than any possible optimal Q-value. This is the definition of \"optimistic initialization\".\n\nNow, we argue that any action, once taken, will have its Q-value updated to a value strictly less than $Q_0$. This will make any untried actions in that state appear more appealing to a greedy agent, thereby encouraging the agent to select them.\n\nLet's prove by induction that for all $t \\ge 1$, any Q-value $Q_t(s,a)$ that has been updated at least once is strictly less than $Q_0$, and any Q-value that has not been updated remains at $Q_0$.\nLet $(s_t, a_t)$ be the state-action pair selected at decision time $t$. The update rule is:\n$$Q_{t+1}(s_{t},a_{t}) = (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right)$$\nAll other Q-values remain unchanged: $Q_{t+1}(s,a) = Q_t(s,a)$ for $(s,a) \\neq (s_t,a_t)$.\n\n**Base Case:** At $t=0$, no values have been updated. All values are $Q_0$. Suppose at $t=0$, the agent takes action $a_0$ in state $s_0$. The Q-value $Q_0(s_0, a_0) = Q_0$.\nThe update rule gives:\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha\\left(r_{0} + \\gamma \\max_{a'} Q_{0}(s_{1},a')\\right)$$\nSince $Q_0(s_1, a') = Q_0$ for all $a'$, the expression simplifies to:\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha(r_{0} + \\gamma Q_{0})$$\nTo show that $Q_1(s_0, a_0)  Q_0$, we need to show that the target value, $r_0 + \\gamma Q_0$, is less than $Q_0$. From the given condition $Q_0 > \\frac{R_{\\max}}{1-\\gamma}$, it follows that $Q_0(1-\\gamma) > R_{\\max}$. Since $r_0 \\le R_{\\max}$, we have $r_0  Q_0(1-\\gamma)$, which rearranges to $r_0 + \\gamma Q_0  Q_0$.\nThe new value $Q_1(s_0, a_0)$ is a convex combination of $Q_0$ and a value strictly less than $Q_0$. Since $\\alpha \\in (0,1]$, the new value must be strictly less than $Q_0$. All other Q-values remain at $Q_0$.\n\n**Inductive Step:** Assume that at time $t$, all Q-values satisfy $Q_t(s,a) \\le Q_0$. Let's compute the update for $(s_t, a_t)$. The target value is $T_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$.\nBy the inductive hypothesis, $\\max_{a'} Q_t(s_{t+1}, a') \\le Q_0$. Thus, the target is bounded:\n$$T_t \\le r_t + \\gamma Q_0$$\nAs established before, $r_t + \\gamma Q_0  Q_0$. So, $T_t  Q_0$.\nThe new value is $Q_{t+1}(s_t, a_t) = (1-\\alpha)Q_t(s_t, a_t) + \\alpha T_t$. This is a convex combination of $Q_t(s_t, a_t) \\le Q_0$ and $T_t  Q_0$. Therefore, $Q_{t+1}(s_t, a_t)  Q_0$.\nFor any other pair $(s,a) \\neq (s_t, a_t)$, $Q_{t+1}(s,a) = Q_t(s,a) \\le Q_0$.\nThus, by induction, any Q-value, once updated, becomes strictly less than $Q_0$.\n\n**Conclusion:** Consider any state $s$. Let $A_{tried}$ be the set of actions that have been tried in state $s$, and $A_{untried}$ be the set of actions that have not. For any $a_{tried} \\in A_{tried}$, its value $Q_t(s, a_{tried})$ has been updated at least once, so $Q_t(s, a_{tried})  Q_0$. For any $a_{untried} \\in A_{untried}$, its value has never been updated, so $Q_t(s, a_{untried}) = Q_0$.\nA greedy agent at state $s$ selects the action $a = \\arg\\max_{a'} Q_t(s, a')$. The maximum value is $Q_0$, which corresponds to any of the actions in $A_{untried}$. Therefore, the agent is guaranteed to select an action from $A_{untried}$ (with the specific choice determined by the tie-breaking rule). This process continues until $A_{untried}$ is empty, ensuring that every action in state $s$ is eventually tried. This demonstrates that optimistic initialization with a greedy policy encourages systematic exploration.\n\n### Part B: Calculation of Convergence Time\n\nThe problem specifies a deterministic MDP with two states, $s$ and $g$, and two actions from $s$, $a_L$ and $a_R$. We are given $\\alpha=1$. The one-step update simplifies to:\n$$Q_{t+1}(s_t, a_t) \\leftarrow r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$$\n\nLet's trace the evolution of the Q-values, $Q(s, a_L)$ and $Q(s, a_R)$.\nInitial values: $Q_0(s, a_L) = Q_0$ and $Q_0(s, a_R) = Q_0$. Terminal state values are $Q_t(g, \\cdot) = 0$ for all $t$.\n\n**Step 0 (t=0):**\n- State: $s_0=s$. Q-values are tied: $Q_0(s, a_L) = Q_0(s, a_R)$.\n- Action: Tie-breaking favors $a_L$, so $a_0 = a_L$.\n- Outcome: Reward $r_0=0$, next state $s_1=s$.\n- Update: $Q_1(s, a_L) \\leftarrow r_0 + \\gamma \\max_{a'} Q_0(s, a') = 0 + \\gamma \\max(Q_0, Q_0) = \\gamma Q_0$.\n- Other Q-value is unchanged: $Q_1(s, a_R) = Q_0(s, a_R) = Q_0$.\n- After 1 action, the values are $Q_1(s, a_L) = \\gamma Q_0$ and $Q_1(s, a_R) = Q_0$.\n\n**Step 1 (t=1):**\n- State: $s_1=s$. Since $\\gamma \\in (0,1)$ and $Q_0>0$, we have $Q_1(s, a_R) > Q_1(s, a_L)$.\n- Action: The greedy choice is $a_1 = a_R$.\n- Outcome: Reward $r_1=1$, next state $s_2=g$ (terminal). Episode ends.\n- Update: $Q_2(s, a_R) \\leftarrow r_1 + \\gamma \\max_{a'} Q_1(g, a') = 1 + \\gamma \\cdot 0 = 1$.\n- Other Q-value is unchanged: $Q_2(s, a_L) = Q_1(s, a_L) = \\gamma Q_0$.\n- After 2 actions, the agent is reset to state $s$, and the values are $Q_2(s, a_L) = \\gamma Q_0$ and $Q_2(s, a_R) = 1$.\n\n**Subsequent Steps (t $\\ge$ 2):**\nThe agent is now at state $s$ and compares $Q(s, a_L) = \\gamma Q_0$ with $Q(s, a_R) = 1$. The greedy choice depends on which is larger.\nLet $m$ be the number of additional times $a_L$ is chosen consecutively. This happens as long as $Q(s,a_L) \\ge Q(s,a_R)$.\n\n- If $\\gamma Q_0  1$, at $t=2$ the agent chooses $a_R$. The update is $Q_3(s, a_R) \\leftarrow 1$, so the value does not change. The choice of $a_R$ becomes stable. Convergence is reached after $T=2$ steps. In this case, $m=0$.\n\n- If $\\gamma Q_0 \\ge 1$, at $t=2$ the agent chooses $a_L$.\n  - Update: $Q_3(s, a_L) \\leftarrow 0 + \\gamma \\max\\{Q_2(s, a_L), Q_2(s, a_R)\\} = \\gamma \\max\\{\\gamma Q_0, 1\\} = \\gamma (\\gamma Q_0) = \\gamma^2 Q_0$.\n  - $Q_3(s, a_R)=1$.\n- At $t=3$, the agent compares $\\gamma^2 Q_0$ and $1$. It will choose $a_L$ again if $\\gamma^2 Q_0 \\ge 1$.\n\nThis pattern continues. Let $m$ be the number of times $a_L$ is chosen after the initial $(a_L, a_R)$ sequence. $a_L$ is chosen for the $(k+1)$-th time (where $k=0, 1, ..., m-1$) if the condition $Q(s,a_L) \\ge 1$ holds. After the initial episode (2 steps) and $k$ additional $a_L$ choices, the value is $Q(s, a_L) = \\gamma^{k+1} Q_0$. The condition to choose $a_L$ again is therefore $\\gamma^{k+1} Q_0 \\ge 1$.\n\n$m$ is the count of non-negative integers $k$ for which $\\gamma^{k+1} Q_0 \\ge 1$.\n$$\\gamma^{k+1} \\ge \\frac{1}{Q_0}$$\nTaking logarithms with base $1/\\gamma > 1$:\n$$k+1 \\le \\log_{1/\\gamma}\\left(\\frac{1}{Q_0^{-1}}\\right) = \\log_{1/\\gamma}(Q_0)$$\n$$k \\le \\log_{1/\\gamma}(Q_0) - 1$$\nWe need to count the number of non-negative integers $k$ satisfying this.\nIf $\\log_{1/\\gamma}(Q_0)  1$ (i.e., $Q_0  1/\\gamma$ or $\\gamma Q_0  1$), then $\\log_{1/\\gamma}(Q_0) - 1  0$, so there are no non-negative integers $k$ satisfying the inequality. In this case, $m=0$.\nIf $\\log_{1/\\gamma}(Q_0) \\ge 1$, the non-negative integers are $k=0, 1, \\dots, \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor$. The count is $m = \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor + 1 = \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor$.\nCombining these cases, $m = \\max(0, \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor)$.\n\nThe total number of actions $T$ until convergence is the sum of actions taken.\n- $1$ action for the initial $a_L$.\n- $1$ action for the first $a_R$.\n- $m$ actions for the subsequent $a_L$ choices.\nTotal actions: $T = 1 + 1 + m = 2 + m$.\nAfter these $T$ actions, at decision time $t=T$, the value $Q_T(s, a_L)$ has become $\\gamma^{m+1} Q_0$. By the definition of $m$, we have $\\gamma^{m+1}Q_0  1$. So, at $t=T$, the agent compares $Q_T(s, a_L)  1$ and $Q_T(s, a_R) = 1$ and chooses $a_R$. This choice remains stable for all future visits. This matches the problem's definition of $T$.\n\nThe final expression for $T$ is:\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\log_{1/\\gamma}(Q_0) \\right\\rfloor\\right)$$\nThis can also be written using the natural logarithm:\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)$$", "answer": "$$ \\boxed{2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)} $$", "id": "3190816"}, {"introduction": "Effective policy improvement relies on accurate estimates of future returns, a task that sits at the heart of the statistical bias-variance trade-off. This exercise delves into Generalized Advantage Estimation (GAE), a powerful technique used in modern reinforcement learning algorithms that explicitly manages this trade-off via a parameter $\\lambda$. You will derive the GAE formulation and then implement a procedure to empirically select the optimal $\\lambda$, balancing the high bias of single-step estimates with the high variance of full Monte Carlo returns [@problem_id:3190870].", "problem": "Consider a fixed-policy Markov Decision Process (MDP) with discounted return. Let the discount factor be $\\gamma \\in (0,1)$, and let the value function under a fixed policy be $V^\\pi(s) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\mid s_t = s \\right]$. Define the temporal-difference residual at time $t$ as $\\delta_t = r_t + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)$. The Generalized Advantage Estimation (GAE) is a method that constructs an advantage estimator by exponentially weighting temporal-difference residuals with a parameter $\\lambda \\in [0,1]$. From foundational definitions, derive the relation between the exponentially accumulated residuals and a $\\lambda$-return estimator $G_t^\\lambda$ that uses $V^\\pi$ for bootstrapping. In particular:\n- Starting from the Bellman consistency and the definition of $\\delta_t$, derive the telescoping structure that justifies a $\\lambda$-dependent accumulation of errors.\n- Establish the limiting cases $\\lambda = 0$ and $\\lambda = 1$, carefully explaining what estimators they produce and why these cases correspond to higher bias and higher variance, respectively, in a fixed finite trajectory context.\n- Explain why tuning $\\lambda$ trades bias for variance and how minimizing an estimated Mean Squared Error (MSE) on held-out trajectories operationalizes this trade-off.\n\nThen, implement a program that fits $\\lambda$ by minimizing an estimate of the MSE of return prediction on held-out trajectories using the following self-contained setup. You must not use any external input. All arrays and parameters are provided here.\n\nYou will construct three independent cases. In each case, training trajectories are used to fit a scalar linear predictor $\\hat{G}(x) = w x$ by least squares to targets defined by $\\lambda$-returns $G_t^\\lambda$ computed from training data, and evaluation is performed on held-out trajectories using empirical Monte Carlo (MC) returns $R_t = \\sum_{k=0}^{K} \\gamma^k r_{t+k}$ as ground truth. For each case, select $\\lambda$ from the grid $\\{0.0, 0.25, 0.5, 0.75, 1.0\\}$ that minimizes the held-out MSE. In case of ties, select the smallest $\\lambda$.\n\nShared dynamics and notation across cases:\n- States are scalars $x_t \\in \\mathbb{R}$ evolving deterministically as $x_{t+1} = \\alpha x_t$ with $\\alpha = 0.8$.\n- Rewards are $r_t = x_t + \\varepsilon_t$, where $\\varepsilon_t$ is a specified noise term. In noise-free segments, $\\varepsilon_t = 0$.\n- A baseline value estimator is $V(x_t) = b \\, x_t$, with case-specific $b$.\n- All trajectories are finite and terminate at the last listed time index, after which $V(x_{T}) = 0$ for bootstrapping.\n- Use $\\gamma = 0.9$ throughout.\n- For each case, aggregate all training time steps over all training trajectories to fit $w$ via least squares (no intercept term), and aggregate all held-out time steps over all held-out trajectories to compute the MSE of $\\hat{G}(x_t) = w x_t$ relative to $R_t$.\n\nCase specifications:\n\nCase $1$ (happy path: moderately biased baseline and noisy training labels):\n- Training initial states: $x_0 \\in \\{1.0, 0.5, 1.5\\}$, each generating a trajectory of length $4$ via $x_{t+1} = 0.8 x_t$.\n- Training reward noises $\\varepsilon_t$ per trajectory (length $4$ each):\n  - For $x_0 = 1.0$: $\\varepsilon = [0.1, -0.05, 0.08, -0.02]$.\n  - For $x_0 = 0.5$: $\\varepsilon = [-0.02, 0.05, -0.01, 0.0]$.\n  - For $x_0 = 1.5$: $\\varepsilon = [0.0, 0.12, -0.06, 0.03]$.\n- Baseline coefficient: $b = 3.2$ so that $V(x_t) = 3.2 \\, x_t$.\n- Held-out initial states: $x_0 \\in \\{0.7, 1.1\\}$, each generating a trajectory of length $4$ via $x_{t+1} = 0.8 x_t$, and noise-free rewards ($\\varepsilon_t = 0$).\n\nCase $2$ (boundary case: perfect baseline, noise-free):\n- Training initial state: $x_0 = 1.2$, generating a trajectory of length $5$ with $x_{t+1} = 0.8 x_t$, and noise-free rewards ($\\varepsilon_t = 0$).\n- Baseline coefficient: $b = \\frac{1}{1 - \\gamma \\alpha} = \\frac{1}{1 - 0.9 \\cdot 0.8} = \\frac{1}{0.28}$, so $V(x_t) = \\frac{x_t}{0.28}$.\n- Held-out initial states: $x_0 \\in \\{0.9, 1.3\\}$, each generating a trajectory of length $4$ with $x_{t+1} = 0.8 x_t$, and noise-free rewards ($\\varepsilon_t = 0$).\n\nCase $3$ (edge case: poor baseline, noise-free):\n- Training initial states: $x_0 \\in \\{0.6, 1.4\\}$, each generating a trajectory of length $4$ with $x_{t+1} = 0.8 x_t$, and noise-free rewards ($\\varepsilon_t = 0$).\n- Baseline coefficient: $b = 1.0$, so $V(x_t) = x_t$.\n- Held-out initial state: $x_0 = 1.0$, generating a trajectory of length $5$ with $x_{t+1} = 0.8 x_t$, and noise-free rewards ($\\varepsilon_t = 0$).\n\nAlgorithmic requirements for your program:\n- For each candidate $\\lambda \\in \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$, compute training $\\lambda$-returns $G_t^\\lambda$ using the exponentially weighted accumulation of temporal-difference residuals and the given baseline $V(x_t)$, then fit the scalar $w$ via least squares to minimize $\\sum (w x_t - G_t^\\lambda)^2$ over all training time steps.\n- On the corresponding held-out trajectories, compute empirical MC returns $R_t = \\sum_{k=0}^{K} \\gamma^k r_{t+k}$, predict $\\hat{G}(x_t) = w x_t$, and compute the MSE as the mean of $(\\hat{G}(x_t) - R_t)^2$ over all held-out time steps.\n- Select the $\\lambda$ minimizing the held-out MSE; in case of ties, select the smallest $\\lambda$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result is the selected $\\lambda$ for Cases $1$, $2$, and $3$, respectively, reported as a floating-point number in the given grid.\n\nTest suite coverage rationale:\n- Case $1$ tests the variance-sensitive regime where noisy training labels and a moderately biased baseline induce a nontrivial bias-variance trade-off.\n- Case $2$ tests the boundary condition where a perfect baseline and noise-free data make all $\\lambda$ equivalent, requiring the specified tie-breaking rule.\n- Case $3$ tests the edge case where a poor baseline encourages higher $\\lambda$ to reduce bias via reduced bootstrapping, tending toward the empirical MC target.\n\nNo physical units or angle units are involved. Percentages do not appear; all quantities are pure scalars. The only outputs are the selected $\\lambda$ values as floats.", "solution": "We begin from the Bellman consistency for a fixed policy, $V^\\pi(s_t) = \\mathbb{E}[r_t + \\gamma V^\\pi(s_{t+1})]$. The temporal-difference residual is defined by $\\delta_t = r_t + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)$. On a finite trajectory terminating at time $T-1$, we may define an exponentially weighted accumulation of residuals controlled by $\\lambda \\in [0,1]$ over the remainder of the trajectory. Let $K_t$ be the remaining number of steps after time $t$, so the accumulation stops at index $t + K_t = T - 1$. The accumulated residuals form\n$$\nA_t^\\lambda = \\sum_{l=0}^{K_t} (\\gamma \\lambda)^l \\, \\delta_{t+l},\n$$\nwhich we interpret as a $\\lambda$-dependent advantage estimator. Using the accumulated residuals, the $\\lambda$-return is defined by\n$$\nG_t^\\lambda = V^\\pi(s_t) + A_t^\\lambda,\n$$\nwhich mixes bootstrapping via $V^\\pi$ and multi-step information via $\\delta_{t+l}$.\n\nTo understand the structure and limiting cases, note the telescoping that arises when $\\lambda = 1$. Expanding the sum for $\\lambda = 1$ gives\n$$\nG_t^1 = V^\\pi(s_t) + \\sum_{l=0}^{K_t} \\gamma^l \\left(r_{t+l} + \\gamma V^\\pi(s_{t+l+1}) - V^\\pi(s_{t+l})\\right).\n$$\nRegrouping the terms leads to\n$$\nG_t^1 = \\sum_{l=0}^{K_t} \\gamma^l r_{t+l} + \\left[\\gamma^{K_t+1} V^\\pi(s_{t+K_t+1})\\right],\n$$\nwhere the bracketed term vanishes if the trajectory terminates and bootstrapping uses $V^\\pi(s_{T}) = 0$. Hence, when $\\lambda = 1$ we recover the empirical Monte Carlo return\n$$\nR_t = \\sum_{l=0}^{K_t} \\gamma^l r_{t+l}.\n$$\nThis case minimizes bias because it relies purely on sampled rewards, but it can have high variance due to the stochasticity of returns.\n\nAt the other extreme, when $\\lambda = 0$, the accumulation reduces to\n$$\nA_t^0 = \\delta_t = r_t + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t),\n$$\nso\n$$\nG_t^0 = V^\\pi(s_t) + \\delta_t = r_t + \\gamma V^\\pi(s_{t+1}),\n$$\nwhich is the $1$-step bootstrapped target. This case tends to have higher bias because it relies more on the baseline $V^\\pi$ but lower variance because it uses fewer sampled rewards beyond the immediate step.\n\nTherefore, varying $\\lambda$ trades bias for variance: larger $\\lambda$ reduces bias by relying more on empirical returns and less on bootstrapping, whereas smaller $\\lambda$ reduces variance by relying more on the baseline and less on stochastic multi-step outcomes. Minimizing an estimate of Mean Squared Error (MSE) on held-out trajectories operationalizes this trade-off by selecting the $\\lambda$ that best balances squared bias and variance for prediction on unseen data.\n\nWe now detail the algorithmic design for fitting $\\lambda$ via held-out MSE of return prediction:\n\n1. For each candidate $\\lambda$ in the grid $\\{0.0, 0.25, 0.5, 0.75, 1.0\\}$, compute training $\\lambda$-returns $G_t^\\lambda$ as follows for each training trajectory:\n   - Compute baseline values $V(x_t) = b \\, x_t$.\n   - Compute temporal-difference residuals $\\delta_t = r_t + \\gamma V(x_{t+1}) - V(x_t)$, using $V(x_T) = 0$ at terminal.\n   - Compute advantages backward by $A_{T-1}^\\lambda = \\delta_{T-1}$ and $A_t^\\lambda = \\delta_t + \\gamma \\lambda A_{t+1}^\\lambda$, yielding $G_t^\\lambda = V(x_t) + A_t^\\lambda$ for all $t$.\n   Aggregating across all training trajectories yields training pairs $(x_t, G_t^\\lambda)$.\n\n2. Fit a scalar linear predictor $\\hat{G}(x_t) = w x_t$ by least squares over the aggregated training pairs. Since there is no intercept term, the solution is\n   $$\n   w = \\frac{\\sum_t x_t G_t^\\lambda}{\\sum_t x_t^2}.\n   $$\n\n3. On the held-out trajectories, compute empirical Monte Carlo returns $R_t = \\sum_{k=0}^{K_t} \\gamma^k r_{t+k}$, aggregate all held-out pairs $(x_t, R_t)$, and evaluate predictions $\\hat{G}(x_t) = w x_t$. Compute the Mean Squared Error\n   $$\n   \\text{MSE}(\\lambda) = \\frac{1}{N} \\sum_{t=1}^{N} \\left(\\hat{G}(x_t) - R_t\\right)^2,\n   $$\n   where $N$ is the total number of held-out time steps across all held-out trajectories.\n\n4. Select the $\\lambda$ that minimizes $\\text{MSE}(\\lambda)$; in case of ties, select the smallest $\\lambda$.\n\nCase-specific expectations in the test suite:\n- In Case $1$, training rewards include noise, and the baseline is moderately biased. Larger $\\lambda$ reduces bias but can overfit noisy labels; smaller $\\lambda$ reduces label variance via bootstrapping but increases bias. The selected $\\lambda$ reflects this balance.\n- In Case $2$, the baseline equals the exact value function with noise-free data. The $\\lambda$-return equals the baseline regardless of $\\lambda$ on deterministic trajectories that satisfy the Bellman equality exactly, producing identical MSE; hence the smallest $\\lambda$ is selected by tie-breaking.\n- In Case $3$, the baseline is poor and noise-free rewards are used. Higher $\\lambda$ reduces the influence of the poor baseline and aligns predictions closer to empirical returns, favoring larger $\\lambda$.\n\nThe program strictly implements these steps with the provided numeric parameters, computes the selected $\\lambda$ for each case, and prints them in the specified single-line format.", "answer": "```python\n# Python 3.12 code using only numpy and scipy (scipy unused here).\nimport numpy as np\n\ndef generate_trajectory(initial_x, alpha, length, noise=None):\n    \"\"\"\n    Generate a deterministic trajectory of states x_t with x_{t+1} = alpha * x_t.\n    Rewards r_t = x_t + epsilon_t, where epsilon_t is provided noise (or zero).\n    Returns arrays x, r for t=0..length-1.\n    \"\"\"\n    x = np.empty(length, dtype=float)\n    r = np.empty(length, dtype=float)\n    x[0] = initial_x\n    for t in range(1, length):\n        x[t] = alpha * x[t-1]\n    if noise is None:\n        noise = np.zeros(length, dtype=float)\n    r[:] = x + noise\n    return x, r\n\ndef compute_baseline_values(x, b):\n    \"\"\"\n    Baseline value function V(x_t) = b * x_t for each t.\n    \"\"\"\n    return b * x\n\ndef compute_lambda_returns(rewards, values, gamma, lam):\n    \"\"\"\n    Compute lambda-returns G_t^lambda for a single trajectory.\n    Using TD residuals delta_t = r_t + gamma * V_{t+1} - V_t,\n    and backward recursion for advantages A_t^lambda.\n\n    Terminal bootstrapping: V at the step after the last index is treated as 0.\n    \"\"\"\n    T = len(rewards)\n    # Prepare V_{t+1}; for t = T-1, V_{t+1} = 0\n    V_next = np.empty(T, dtype=float)\n    V_next[:-1] = values[1:]\n    V_next[-1] = 0.0\n    deltas = rewards + gamma * V_next - values\n    advantages = np.empty(T, dtype=float)\n    advantages[-1] = deltas[-1]\n    for t in range(T - 2, -1, -1):\n        advantages[t] = deltas[t] + gamma * lam * advantages[t + 1]\n    G_lambda = values + advantages\n    return G_lambda\n\ndef compute_mc_returns(rewards, gamma):\n    \"\"\"\n    Compute empirical Monte Carlo returns R_t = sum_{k=0}^K gamma^k r_{t+k}\n    via backward recursion on a single trajectory.\n    \"\"\"\n    T = len(rewards)\n    R = np.empty(T, dtype=float)\n    R[-1] = rewards[-1]\n    for t in range(T - 2, -1, -1):\n        R[t] = rewards[t] + gamma * R[t + 1]\n    return R\n\ndef fit_scalar_least_squares(xs, ys):\n    \"\"\"\n    Fit w in y ≈ w * x via least squares without intercept.\n    \"\"\"\n    xs = np.asarray(xs, dtype=float)\n    ys = np.asarray(ys, dtype=float)\n    denom = np.dot(xs, xs)\n    if denom == 0.0:\n        return 0.0\n    return float(np.dot(xs, ys) / denom)\n\ndef case_1_result(gamma, alpha, lambdas):\n    # Training trajectories\n    train_inits = [1.0, 0.5, 1.5]\n    train_length = 4\n    noises = [\n        np.array([0.1, -0.05, 0.08, -0.02], dtype=float),\n        np.array([-0.02, 0.05, -0.01, 0.0], dtype=float),\n        np.array([0.0, 0.12, -0.06, 0.03], dtype=float),\n    ]\n    b = 3.2\n    X_train_all = []\n    Y_train_by_lambda = {lam: [] for lam in lambdas}\n\n    for init, noise in zip(train_inits, noises):\n        x, r = generate_trajectory(init, alpha, train_length, noise=noise)\n        V = compute_baseline_values(x, b)\n        for lam in lambdas:\n            G_lam = compute_lambda_returns(r, V, gamma, lam)\n            Y_train_by_lambda[lam].extend(G_lam.tolist())\n        X_train_all.extend(x.tolist())\n\n    # Held-out trajectories (noise-free)\n    hold_inits = [0.7, 1.1]\n    hold_length = 4\n    X_hold_all = []\n    R_hold_all = []\n    for init in hold_inits:\n        xh, rh = generate_trajectory(init, alpha, hold_length, noise=None)\n        Rh = compute_mc_returns(rh, gamma)\n        X_hold_all.extend(xh.tolist())\n        R_hold_all.extend(Rh.tolist())\n\n    # Evaluate MSE for each lambda\n    best_lambda = None\n    best_mse = None\n    for lam in lambdas:\n        w = fit_scalar_least_squares(X_train_all, Y_train_by_lambda[lam])\n        preds = w * np.array(X_hold_all, dtype=float)\n        errs = preds - np.array(R_hold_all, dtype=float)\n        mse = float(np.mean(errs ** 2))\n        if (best_mse is None) or (mse  best_mse - 1e-12) or (abs(mse - best_mse) = 1e-12 and (best_lambda is None or lam  best_lambda)):\n            best_mse = mse\n            best_lambda = lam\n    return best_lambda\n\ndef case_2_result(gamma, alpha, lambdas):\n    # Perfect baseline, noise-free\n    train_init = 1.2\n    train_length = 5\n    b_true = 1.0 / (1.0 - gamma * alpha)\n\n    x, r = generate_trajectory(train_init, alpha, train_length, noise=None)\n    V = compute_baseline_values(x, b_true)\n\n    X_train_all = x.tolist()\n    # For perfect baseline under deterministic dynamics and noise-free rewards,\n    # lambda-returns equal the baseline targets for all lambda.\n    Y_train_by_lambda = {}\n    for lam in lambdas:\n        G_lam = compute_lambda_returns(r, V, gamma, lam)\n        Y_train_by_lambda[lam] = G_lam.tolist()\n\n    # Held-out trajectories (noise-free)\n    hold_inits = [0.9, 1.3]\n    hold_length = 4\n    X_hold_all = []\n    R_hold_all = []\n    for init in hold_inits:\n        xh, rh = generate_trajectory(init, alpha, hold_length, noise=None)\n        Rh = compute_mc_returns(rh, gamma)\n        X_hold_all.extend(xh.tolist())\n        R_hold_all.extend(Rh.tolist())\n\n    best_lambda = None\n    best_mse = None\n    for lam in lambdas:\n        w = fit_scalar_least_squares(X_train_all, Y_train_by_lambda[lam])\n        preds = w * np.array(X_hold_all, dtype=float)\n        errs = preds - np.array(R_hold_all, dtype=float)\n        mse = float(np.mean(errs ** 2))\n        if (best_mse is None) or (mse  best_mse - 1e-12) or (abs(mse - best_mse) = 1e-12 and (best_lambda is None or lam  best_lambda)):\n            best_mse = mse\n            best_lambda = lam\n    return best_lambda\n\ndef case_3_result(gamma, alpha, lambdas):\n    # Poor baseline, noise-free training and held-out\n    train_inits = [0.6, 1.4]\n    train_length = 4\n    b_poor = 1.0\n\n    X_train_all = []\n    Y_train_by_lambda = {lam: [] for lam in lambdas}\n    for init in train_inits:\n        x, r = generate_trajectory(init, alpha, train_length, noise=None)\n        V = compute_baseline_values(x, b_poor)\n        for lam in lambdas:\n            G_lam = compute_lambda_returns(r, V, gamma, lam)\n            Y_train_by_lambda[lam].extend(G_lam.tolist())\n        X_train_all.extend(x.tolist())\n\n    # Held-out (noise-free)\n    hold_init = 1.0\n    hold_length = 5\n    xh, rh = generate_trajectory(hold_init, alpha, hold_length, noise=None)\n    Rh = compute_mc_returns(rh, gamma)\n    X_hold_all = xh.tolist()\n    R_hold_all = Rh.tolist()\n\n    best_lambda = None\n    best_mse = None\n    for lam in lambdas:\n        w = fit_scalar_least_squares(X_train_all, Y_train_by_lambda[lam])\n        preds = w * np.array(X_hold_all, dtype=float)\n        errs = preds - np.array(R_hold_all, dtype=float)\n        mse = float(np.mean(errs ** 2))\n        if (best_mse is None) or (mse  best_mse - 1e-12) or (abs(mse - best_mse) = 1e-12 and (best_lambda is None or lam  best_lambda)):\n            best_mse = mse\n            best_lambda = lam\n    return best_lambda\n\ndef solve():\n    gamma = 0.9\n    alpha = 0.8\n    lambdas = [0.0, 0.25, 0.5, 0.75, 1.0]\n\n    results = []\n    # Case 1\n    results.append(case_1_result(gamma, alpha, lambdas))\n    # Case 2\n    results.append(case_2_result(gamma, alpha, lambdas))\n    # Case 3\n    results.append(case_3_result(gamma, alpha, lambdas))\n\n    # Print in required single-line format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3190870"}]}