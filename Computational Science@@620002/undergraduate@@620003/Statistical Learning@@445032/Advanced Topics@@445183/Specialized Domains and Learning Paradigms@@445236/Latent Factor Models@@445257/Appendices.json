{"hands_on_practices": [{"introduction": "A fundamental property of many latent factor models, such as those where a data matrix $X$ is approximated by $X \\approx Z W^\\top$, is their inherent non-uniqueness. This exercise delves into a key aspect of this issue: permutation ambiguity. Understanding that the latent factors can be arbitrarily reordered without changing the final approximation is the first step toward developing robust methods for comparing and interpreting factor models [@problem_id:3137743].", "problem": "Consider a latent factor model in statistical learning where a data matrix $X \\in \\mathbb{R}^{n \\times m}$ is approximated by a rank-$k$ factorization $X \\approx Z W^\\top$ with score matrix $Z \\in \\mathbb{R}^{n \\times k}$ and loading matrix $W \\in \\mathbb{R}^{m \\times k}$. The latent dimension is $k$, and columns of $Z$ and $W$ represent $k$ latent factors. A permutation matrix $P \\in \\mathbb{R}^{k \\times k}$ is a square binary matrix with exactly one entry equal to $1$ in each row and each column and zeros elsewhere. The Frobenius norm $\\lVert A \\rVert_F$ of a matrix $A$ is defined as the square root of the sum of squares of its entries, and the standard inner product between vectors $a, b \\in \\mathbb{R}^d$ is $\\langle a, b \\rangle = a^\\top b$.\n\nRepeated runs of a factorization algorithm may produce factor pairs $(Z_1, W_1)$ and $(Z_2, W_2)$ that both fit $X$ well but differ by a permutation of their columns (and other small numerical differences). To reason from first principles about identifiability under permutation ambiguity and about aligning factors across runs, assess the following statements:\n\nA. For any permutation matrix $P \\in \\mathbb{R}^{k \\times k}$, $(Z P)(W P)^\\top = Z W^\\top$.\n\nB. One principled way to match the $k$ factors across two runs is to choose $P \\in \\mathbb{R}^{k \\times k}$ that minimizes $\\lVert Z_1 - Z_2 P \\rVert_F^2$, which is equivalent to solving a linear assignment problem on a $k \\times k$ cost matrix whose $(i, j)$-entry measures the discrepancy between the $i$-th column of $Z_1$ and the $j$-th column of $Z_2$.\n\nC. Sorting the columns of $Z_1$ and $Z_2$ by their $\\ell_2$ norms and matching by this sorted order will always recover the correct permutation provided the columns are distinct.\n\nD. If each column of $Z_1$ and $Z_2$ has unit $\\ell_2$ norm, then maximizing $\\sum_{i=1}^k \\langle z^{(1)}_i, z^{(2)}_{\\pi(i)} \\rangle$ over all permutations $\\pi$ of $\\{1, \\dots, k\\}$ is equivalent to minimizing $\\lVert Z_1 - Z_2 P \\rVert_F^2$ over permutation matrices $P$, where $z^{(r)}_i$ denotes the $i$-th column of $Z_r$.\n\nE. Permuting the columns of $Z$ while permuting the rows of $W$ leaves $Z W^\\top$ unchanged.\n\nWhich statements are correct?", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard scenario in statistical learning concerning the identifiability of latent factor models. We proceed to analyze each statement.\n\nLet the data matrix be $X \\in \\mathbb{R}^{n \\times m}$, approximated by $X \\approx Z W^\\top$, with score matrix $Z \\in \\mathbb{R}^{n \\times k}$ and loading matrix $W \\in \\mathbb{R}^{m \\times k}$. The columns of $Z$ are denoted $\\{z_i\\}_{i=1}^k$ where $z_i \\in \\mathbb{R}^n$, and the columns of $W$ are $\\{w_i\\}_{i=1}^k$ where $w_i \\in \\mathbb{R}^m$. The factorization can be written as a sum of outer products: $Z W^\\top = \\sum_{i=1}^k z_i w_i^\\top$. A permutation matrix $P \\in \\mathbb{R}^{k \\times k}$ is an orthogonal matrix, meaning its columns (and rows) form an orthonormal basis. Consequently, $P^\\top P = P P^\\top = I_k$, where $I_k$ is the $k \\times k$ identity matrix.\n\n**Analysis of Statement A**\n\nThe statement asserts that for any permutation matrix $P \\in \\mathbb{R}^{k \\times k}$, $(Z P)(W P)^\\top = Z W^\\top$.\n\nWe evaluate the left-hand side of the equation. Let $Z' = Z P$ and $W' = W P$. We are interested in the product $Z' (W')^\\top$.\nUsing the property of the transpose of a matrix product, $(AB)^\\top = B^\\top A^\\top$, we have:\n$$ (W P)^\\top = P^\\top W^\\top $$\nSubstituting this into the expression gives:\n$$ (Z P)(W P)^\\top = (Z P)(P^\\top W^\\top) = Z (P P^\\top) W^\\top $$\nAs established, a permutation matrix $P$ is an orthogonal matrix, so $P P^\\top = I_k$. Therefore:\n$$ Z (P P^\\top) W^\\top = Z I_k W^\\top = Z W^\\top $$\nThe left-hand side is identical to the right-hand side. This identity demonstrates the permutation ambiguity of the factors: if $(Z, W)$ is a valid factorization, then $(ZP, WP)$ is an equivalent factorization yielding the same approximation, where the latent factors have been permuted.\n\nVerdict for A: **Correct**.\n\n**Analysis of Statement B**\n\nThe statement proposes a method for aligning factors from two different runs, $(Z_1, W_1)$ and $(Z_2, W_2)$. The method involves finding a permutation matrix $P$ that minimizes the Frobenius norm of the difference, $\\lVert Z_1 - Z_2 P \\rVert_F^2$. The statement claims this is equivalent to a linear assignment problem.\n\nLet $z_i^{(1)}$ be the $i$-th column of $Z_1$ and $z_j^{(2)}$ be the $j$-th column of $Z_2$. Right-multiplication of $Z_2$ by a permutation matrix $P$ permutes the columns of $Z_2$. Let the permutation on the indices $\\{1, \\dots, k\\}$ be $\\pi$. The permuted matrix $Z_2 P$ has its $i$-th column as $z_{\\pi(i)}^{(2)}$ for a suitable permutation matrix $P$ corresponding to $\\pi$. The objective is to find the permutation $\\pi$ that aligns the columns of $Z_1$ with those of $Z_2$.\n\nThe squared Frobenius norm is the sum of the squared $\\ell_2$ norms of the columns of the matrix. We can write the objective function as:\n$$ \\lVert Z_1 - Z_2 P \\rVert_F^2 = \\sum_{i=1}^k \\left\\lVert z_i^{(1)} - (Z_2 P)_i \\right\\rVert_2^2 = \\sum_{i=1}^k \\left\\lVert z_i^{(1)} - z_{\\pi(i)}^{(2)} \\right\\rVert_2^2 $$\nWe seek to minimize this sum over all possible permutations $\\pi$. Let us define a cost matrix $C \\in \\mathbb{R}^{k \\times k}$ where the entry $C_{ij}$ is the cost of matching the $i$-th column of $Z_1$ with the $j$-th column of $Z_2$:\n$$ C_{ij} = \\left\\lVert z_i^{(1)} - z_j^{(2)} \\right\\rVert_2^2 $$\nThe total cost for a given permutation $\\pi$ is $\\sum_{i=1}^k C_{i, \\pi(i)}$. Minimizing this total cost over all permutations $\\pi$ is the formal definition of the linear assignment problem (or minimum weight perfect matching in a bipartite graph).\n\nThus, minimizing $\\lVert Z_1 - Z_2 P \\rVert_F^2$ is indeed equivalent to solving a linear assignment problem with the cost matrix $C$ as defined, where each entry measures the discrepancy (squared Euclidean distance) between a column of $Z_1$ and a column of $Z_2$.\n\nVerdict for B: **Correct**.\n\n**Analysis of Statement C**\n\nThis statement suggests a heuristic for solving the alignment problem: sort the columns of $Z_1$ and $Z_2$ based on their $\\ell_2$ norms and match them according to this sorted order. The statement claims this will \"always\" recover the correct permutation. The \"correct\" permutation is the one that solves the assignment problem described in B. We can test this claim by constructing a counterexample.\n\nLet $k=2$ and the dimension $n$ be at least $2$. Consider the following factor matrices:\n$$ Z_1 = \\begin{bmatrix} z_1^{(1)} & z_2^{(1)} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\\\ \\vdots & \\vdots \\end{bmatrix} $$\n$$ Z_2 = \\begin{bmatrix} z_1^{(2)} & z_2^{(2)} \\end{bmatrix} = \\begin{bmatrix} 1.5 & 0 \\\\ 0 & 1.4 \\\\ \\vdots & \\vdots \\end{bmatrix} $$\n(Assume remaining entries are zero). The columns are distinct.\nThe $\\ell_2$ norms are:\n$$ \\lVert z_1^{(1)} \\rVert_2 = 1, \\quad \\lVert z_2^{(1)} \\rVert_2 = 2 $$\n$$ \\lVert z_1^{(2)} \\rVert_2 = 1.5, \\quad \\lVert z_2^{(2)} \\rVert_2 = 1.4 $$\nThe sorting heuristic proceeds as follows:\n- The columns of $Z_1$ sorted by norm are $(z_1^{(1)}, z_2^{(1)})$.\n- The columns of $Z_2$ sorted by norm are $(z_2^{(2)}, z_1^{(2)})$.\nThe heuristic matches $z_1^{(1)}$ with $z_2^{(2)}$ and $z_2^{(1)}$ with $z_1^{(2)}$. This corresponds to the permutation $\\pi(1)=2, \\pi(2)=1$. The cost is:\n$$ \\text{Cost}_{\\text{heuristic}} = \\lVert z_1^{(1)} - z_2^{(2)} \\rVert_2^2 + \\lVert z_2^{(1)} - z_1^{(2)} \\rVert_2^2 = \\left\\lVert \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.4 \\end{pmatrix} \\right\\rVert_2^2 + \\left\\lVert \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 $$\n$$ = (1^2 + (-1.4)^2) + ((-1.5)^2 + 2^2) = (1 + 1.96) + (2.25 + 4) = 2.96 + 6.25 = 9.21 $$\nNow, let's calculate the cost for the identity permutation, $\\pi(1)=1, \\pi(2)=2$:\n$$ \\text{Cost}_{\\text{identity}} = \\lVert z_1^{(1)} - z_1^{(2)} \\rVert_2^2 + \\lVert z_2^{(1)} - z_2^{(2)} \\rVert_2^2 = \\left\\lVert \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 + \\left\\lVert \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.4 \\end{pmatrix} \\right\\rVert_2^2 $$\n$$ = (-0.5)^2 + (0.6)^2 = 0.25 + 0.36 = 0.61 $$\nSince $\\text{Cost}_{\\text{identity}} < \\text{Cost}_{\\text{heuristic}}$, the optimal (correct) permutation is the identity permutation. The sorting heuristic failed to find it. The word \"always\" renders the statement false.\n\nVerdict for C: **Incorrect**.\n\n**Analysis of Statement D**\n\nThis statement claims an equivalence between two optimization problems under the condition that all factor columns in $Z_1$ and $Z_2$ have unit $\\ell_2$ norm.\nThe first problem is minimizing $\\lVert Z_1 - Z_2 P \\rVert_F^2$ over permutation matrices $P$. As shown in the analysis of B, this is equivalent to minimizing $\\sum_{i=1}^k \\lVert z_i^{(1)} - z_{\\pi(i)}^{(2)} \\rVert_2^2$ over permutations $\\pi$.\nLet's expand the squared norm term:\n$$ \\lVert z_i^{(1)} - z_{\\pi(i)}^{(2)} \\rVert_2^2 = \\langle z_i^{(1)} - z_{\\pi(i)}^{(2)}, z_i^{(1)} - z_{\\pi(i)}^{(2)} \\rangle $$\n$$ = \\langle z_i^{(1)}, z_i^{(1)} \\rangle - 2 \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle + \\langle z_{\\pi(i)}^{(2)}, z_{\\pi(i)}^{(2)} \\rangle $$\n$$ = \\lVert z_i^{(1)} \\rVert_2^2 + \\lVert z_{\\pi(i)}^{(2)} \\rVert_2^2 - 2 \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle $$\nThe total sum we wish to minimize is:\n$$ \\sum_{i=1}^k \\left( \\lVert z_i^{(1)} \\rVert_2^2 + \\lVert z_{\\pi(i)}^{(2)} \\rVert_2^2 - 2 \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle \\right) $$\nGiven the condition that $\\lVert z_i^{(1)} \\rVert_2 = 1$ and $\\lVert z_j^{(2)} \\rVert_2 = 1$ for all $i, j$:\n$$ \\sum_{i=1}^k \\left( 1^2 + 1^2 - 2 \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle \\right) = \\sum_{i=1}^k \\left( 2 - 2 \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle \\right) $$\n$$ = 2k - 2 \\sum_{i=1}^k \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle $$\nMinimizing this expression over $\\pi$ is equivalent to maximizing the term that depends on $\\pi$, which is $\\sum_{i=1}^k \\langle z_i^{(1)}, z_{\\pi(i)}^{(2)} \\rangle$.\nThis matches the second problem described in the statement. The equivalence holds.\n\nVerdict for D: **Correct**.\n\n**Analysis of Statement E**\n\nThis statement claims that \"Permuting the columns of $Z$ while permuting the rows of $W$ leaves $Z W^\\top$ unchanged.\"\n\nLet's formalize these operations.\n\"Permuting the columns of $Z$\": This operation corresponds to right-multiplication by a permutation matrix $P \\in \\mathbb{R}^{k \\times k}$. The new matrix is $Z' = Z P$.\n\"Permuting the rows of $W$\": This operation corresponds to left-multiplication by a permutation matrix $Q \\in \\mathbb{R}^{k \\times k}$ (since $W \\in \\mathbb{R}^{m \\times k}$, its rows are indexed $1, \\dots, m$, so this must mean permuting the factor indices, which corresponds to permuting rows of $W^\\top$, i.e., columns of $W$. Let's assume the intent was to permute the rows of $W$ as a matrix in $\\mathbb{R}^{m \\times k}$. This would be multiplication by a permutation matrix from $\\mathbb{R}^{m \\times m}$. This does not align with the factor structure. A more charitable interpretation is that \"permuting the rows of W\" is a confusing way of saying permuting the order of the factors, which means permuting the columns of W. But statement A already covers this (the *same* permutation must be used). Let's take the statement literally: \"permuting the rows of W\". This means multiplying by an $m \\times m$ permutation matrix $Q$. The new matrix is $W' = QW$. The new product is $Z(QW)^\\top = ZW^\\top Q^\\top$. This is clearly not equal to $ZW^\\top$ in general.\nLet's consider the most likely intended meaning, which is still incorrect: permuting the columns of $Z$ by $P_1$ and columns of $W$ by $P_2$. The new product is $(ZP_1)(WP_2)^\\top = ZP_1 P_2^\\top W^\\top$. This is only equal to $ZW^\\top$ if $P_1 = P_2$. The statement is thus fundamentally incorrect as written. The invariance is $(ZP)(WP)^\\top = ZW^\\top$.\n\nVerdict for E: **Incorrect**.\n\n**Summary**\n- Statement A is correct.\n- Statement B is correct.\n- Statement C is incorrect.\n- Statement D is correct.\n- Statement E is incorrect.\n\nThe correct statements are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "3137743"}, {"introduction": "Building on the concept of factor ambiguity, this practice addresses two critical questions when fitting a factor model: how many factors should we use, and how should we orient them for interpretability? This exercise guides you through implementing a sophisticated and widely-used solution based on bootstrap stability analysis. By repeatedly resampling your data and assessing how consistently the latent factors are recovered, you will learn a principled method for selecting the number of factors $k$ and an appropriate rotation, such as varimax, to achieve a robust and meaningful result [@problem_id:3137686].", "problem": "Consider a rectangular data matrix $X \\in \\mathbb{R}^{n \\times p}$ modeled by a latent factor model with additive noise. The latent factor model assumes that there exist a score matrix $Z \\in \\mathbb{R}^{n \\times k}$ and a loading matrix $W \\in \\mathbb{R}^{p \\times k}$ such that $X \\approx Z W^{\\top} + E$, where $E$ is a noise matrix. The factor dimension $k$ is unknown, and the factor solution is not identifiable without constraints due to rotational ambiguity: for any orthogonal matrix $R \\in \\mathbb{R}^{k \\times k}$ with $R^{\\top} R = I_k$, the pair $(Z R, W R)$ yields the same approximation $Z W^{\\top}$.\n\nThe goal is to examine the stability of factors under bootstrap resampling to select both the latent dimension $k$ and a rotation type. Two rotation types are considered: no rotation and varimax rotation. For each candidate $k$, you will perform bootstrap resampling of both rows and columns of $X$, refit factors on the resampled data, align the resampled loadings with a reference solution computed on the full data, and compute a stability score. The stability across bootstrap samples is then used to select $k$ using a principled rule, and to select the rotation type.\n\nYour program must implement the following components grounded in fundamental definitions:\n\n- Data standardization: Given $X$, construct $\\tilde{X}$ by centering each column to zero mean and scaling to unit variance, i.e., for each feature index $j \\in \\{1,\\dots,p\\}$, set $\\tilde{X}_{:,j} = (X_{:,j} - \\mu_j)/\\sigma_j$, where $\\mu_j$ is the mean and $\\sigma_j$ is the standard deviation of column $j$. If a column has zero variance, leave it unchanged to avoid division by zero.\n\n- Factor estimation via truncated singular value decomposition (SVD): For a given $k$, compute the rank-$k$ approximation of $\\tilde{X}$ via SVD, $\\tilde{X} \\approx U_k \\Sigma_k V_k^{\\top}$, where $U_k \\in \\mathbb{R}^{n \\times k}$, $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ is diagonal with nonnegative singular values, and $V_k \\in \\mathbb{R}^{p \\times k}$. Define the loading matrix for $k$ factors as $L_k = V_k \\Sigma_k$. This choice is consistent with Principal Component Analysis (PCA) loadings for standardized data. Apply rotation to $L_k$ according to the specified rotation type: either no rotation (identity) or an orthogonal varimax rotation, which seeks to make columns of $L_k$ more interpretable by maximizing the variance of squared loadings per factor subject to orthogonality.\n\n- Bootstrap resampling: For a fixed $k$ and a chosen rotation type, perform $B$ bootstrap replicates. Each replicate $b \\in \\{1, \\dots, B\\}$ resamples $n$ rows and $p$ columns of $\\tilde{X}$ independently with replacement to form $\\tilde{X}^{(b)} \\in \\mathbb{R}^{n \\times p}$. Fit the rank-$k$ factor loadings $L_k^{(b)}$ on $\\tilde{X}^{(b)}$ with the specified rotation.\n\n- Stability scoring via aligned absolute correlations: For each bootstrap replicate $b$, align $L_k^{(b)}$ to the full-data reference loadings $L_k^{\\text{ref}}$ (computed on $\\tilde{X}$ with the same $k$ and rotation type) by solving a matching problem that pairs the $k$ columns of $L_k^{(b)}$ with the $k$ columns of $L_k^{\\text{ref}}$. Specifically, for each pair of factor indices $(j, \\ell)$, compute the absolute Pearson correlation between the $j$-th bootstrap loading vector in $L_k^{(b)}$ and the $\\ell$-th reference loading vector in $L_k^{\\text{ref}}$, restricted to the resampled column indices of replicate $b$, including duplicates if columns were resampled multiple times. Denote this as $c_{j\\ell}^{(b)} \\in [0,1]$. Construct the $k \\times k$ matrix $C^{(b)}$ with entries $c_{j\\ell}^{(b)}$. Find the permutation $\\pi^{(b)}$ of $\\{1,\\dots,k\\}$ that maximizes $\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$ (equivalently, solve a linear assignment problem with cost $-C^{(b)}$). Define the stability score for replicate $b$ as the average of the matched absolute correlations, $s^{(b)} = \\frac{1}{k}\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$.\n\n- Aggregation and selection: For each candidate $k$ and rotation type, compute the mean stability $\\bar{s}_{k,r} = \\frac{1}{B}\\sum_{b=1}^{B} s^{(b)}$ and the standard error of the mean $\\text{se}_{k,r} = \\sqrt{\\frac{1}{B(B-1)}\\sum_{b=1}^{B} (s^{(b)} - \\bar{s}_{k,r})^2}$. For each $k$, select the rotation type $r_k$ that maximizes $\\bar{s}_{k,r}$. Then select the latent dimension $k^{\\star}$ using the one-standard-error rule: let $\\bar{s}_{\\max} = \\max_k \\bar{s}_{k, r_k}$ and let $k_{\\max}$ be the $k$ achieving this maximum with corresponding standard error $\\text{se}_{k_{\\max}, r_{k_{\\max}}}$. Choose $k^{\\star}$ to be the smallest $k$ such that $\\bar{s}_{k, r_k} \\ge \\bar{s}_{\\max} - \\text{se}_{k_{\\max}, r_{k_{\\max}}}$. Finally, choose the rotation type $r^{\\star} = r_{k^{\\star}}$.\n\nImplementation requirements:\n\n- Use only Singular Value Decomposition and orthogonal varimax rotation for factor estimation as described. Use absolute Pearson correlations for alignment. Use optimal assignment via the Hungarian method.\n\n- Use $B$ bootstrap replicates per $(k, \\text{rotation})$ setting.\n\n- Candidate latent dimensions are the integers in the set $\\{1,2,3,4,5\\}$.\n\n- Rotation types are \"none\" and \"varimax\". In the output, encode the rotation type as an integer: $0$ for \"none\" and $1$ for \"varimax\".\n\nTest suite specification:\n\nConstruct three synthetic test cases with fixed random seeds for reproducibility. For each case, generate $X$ according to $X = Z W^{\\top} + \\sigma \\cdot N$, where $Z$ and $W$ are specified below, and $N$ has independent and identically distributed standard normal entries. After generation, standardize columns to obtain $\\tilde{X}$ before fitting factors.\n\n- Test case $1$: Let $n = 120$, $p = 20$, $k_{\\text{true}} = 3$, $\\sigma = 0.5$. Construct $W \\in \\mathbb{R}^{20 \\times 3}$ with a block structure: for feature indices $1$ through $7$, set a strong loading on factor $1$; for indices $8$ through $14$, set a strong loading on factor $2$; for indices $15$ through $20$, set a strong loading on factor $3$. Add small Gaussian noise to $W$. Draw $Z \\in \\mathbb{R}^{120 \\times 3}$ with independent and identically distributed standard normal entries.\n\n- Test case $2$: Let $n = 100$, $p = 15$, $k_{\\text{true}} = 1$, $\\sigma = 0.8$. Construct $W \\in \\mathbb{R}^{15 \\times 1}$ with random entries amplified on a subset to favor one dominant factor. Draw $Z \\in \\mathbb{R}^{100 \\times 1}$ with independent and identically distributed standard normal entries.\n\n- Test case $3$: Let $n = 100$, $p = 18$, and pure noise with $\\sigma = 1.0$ by setting $Z W^{\\top}$ to the zero matrix. That is, $X = \\sigma \\cdot N$ where $N$ has independent and identically distributed standard normal entries.\n\nFor each test case, set a fixed random seed to ensure reproducibility.\n\nYour program must:\n\n- Implement the above procedure to compute $\\bar{s}_{k,r}$ and $\\text{se}_{k,r}$ for all $k \\in \\{1,2,3,4,5\\}$ and both rotation types.\n\n- Select $k^{\\star}$ and $r^{\\star}$ according to the one-standard-error rule and rotation choice described.\n\n- For each test case, return the pair $[k^{\\star}, \\text{rotation\\_code}]$ where $\\text{rotation\\_code} \\in \\{0,1\\}$ encodes the rotation type.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each element is the list $[k^{\\star}, \\text{rotation\\_code}]$ for the corresponding test case, in the order of test cases $1$, $2$, and $3$. For example, the format must be exactly like $[[k_1,r_1],[k_2,r_2],[k_3,r_3]]$.\n\nSet the number of bootstrap replicates to $B = 40$ and use only integer outputs as specified. No physical units or angles are involved in this problem. All numeric outputs must be integers.", "solution": "The problem requires the implementation of a comprehensive statistical procedure to select the number of latent factors, $k$, and the appropriate rotation type for a factor model. This is a common and important task in fields like psychometrics, finance, and bioinformatics, where underlying unobserved structures are inferred from high-dimensional data. The selection is based on the principle of model stability under bootstrap resampling. A model that consistently reveals the same factor structure across different perturbed versions of the data is considered more reliable.\n\nThe overall process can be broken down into the following logical steps, which form the basis of the computational solution:\n\n1.  **Model Specification and Data Preparation**:\n    The data is given by a matrix $X \\in \\mathbb{R}^{n \\times p}$, with $n$ observations and $p$ features. The latent factor model posits that $X \\approx ZW^{\\top} + E$, where $Z \\in \\mathbb{R}^{n \\times k}$ are the factor scores and $W \\in \\mathbb{R}^{p \\times k}$ are the factor loadings for an unknown number of factors $k$. Prior to analysis, each column of the data matrix $X$ is standardized to have a mean of $0$ and a standard deviation of $1$, resulting in $\\tilde{X}$. This ensures that all features contribute equally to the variance being explained. Columns with zero variance are left unchanged. For a standardized data matrix, Principal Component Analysis (PCA) and factor analysis are closely related, and we can use the Singular Value Decomposition (SVD) for factor estimation.\n\n2.  **Factor Estimation via SVD**:\n    For a given number of factors $k$, we compute the rank-$k$ approximation of the standardized data matrix $\\tilde{X}$ using truncated SVD: $\\tilde{X} \\approx U_k \\Sigma_k V_k^{\\top}$. Here, $U_k \\in \\mathbb{R}^{n \\times k}$ and $V_k \\in \\mathbb{R}^{p \\times k}$ have orthonormal columns, and $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ is a diagonal matrix of the top $k$ singular values. Following a common convention in PCA, the factor loadings matrix, $L_k \\in \\mathbb{R}^{p \\times k}$, is defined as $L_k = V_k \\Sigma_k$. Each column of $L_k$ represents a factor, and its elements represent the weights of the original $p$ features on that factor.\n\n3.  **Factor Rotation**:\n    The solution $Z W^{\\top}$ is invariant to orthogonal rotations, meaning $(ZR)(WR)^{\\top} = ZW^{\\top}$ for any orthogonal matrix $R$. This rotational ambiguity means the factors estimated by SVD are not unique and may not be easily interpretable. To address this, factor rotation techniques are employed. The problem considers two options:\n    *   **No Rotation**: The SVD-derived loadings $L_k$ are used as is. These are the principal components, which are orthogonal and ordered by the amount of variance they explain.\n    *   **Varimax Rotation**: This is an orthogonal rotation method that aims to simplify the factor structure. It seeks a rotation matrix $T$ such that the rotated loadings $L_k^{\\text{rot}} = L_k T$ have maximal \"simple structure\". This is achieved by maximizing the varimax criterion, which is the sum of the variances of the squared loadings for each factor. A high value indicates that each original feature tends to have a high loading on a small number of factors and near-zero loadings on the others, enhancing interpretability.\n\n4.  **Bootstrap Stability Assessment**:\n    The core of the selection procedure is to evaluate how stable the estimated factor structure is when the data is slightly perturbed. This is done via bootstrap resampling. For each candidate $k$ and each rotation type, the following procedure is repeated $B$ times:\n    *   **Resampling**: A bootstrap dataset $\\tilde{X}^{(b)}$ is created by resampling both the rows (observations) and columns (features) of the standardized full dataset $\\tilde{X}$ with replacement.\n    *   **Re-estimation**: A factor model with $k$ factors is fitted to the bootstrap dataset $\\tilde{X}^{(b)}$, yielding a new set of loadings $L_k^{(b)}$ (with the same rotation type as the reference). Note that since the columns of $\\tilde{X}^{(b)}$ are resampled from an already standardized matrix, their mean and variance will not generally be $0$ and $1$, so standardization is reapplied to $\\tilde{X}^{(b)}$.\n    *   **Alignment and Scoring**: The factors in the bootstrap solution $L_k^{(b)}$ must be aligned with the factors in the reference solution $L_k^{\\text{ref}}$ (computed on the full dataset $\\tilde{X}$). The SVD algorithm orders factors by singular value, an ordering that can change in a bootstrap sample. To resolve this, a matching problem is solved. We construct a $k \\times k$ matrix $C^{(b)}$ where the entry $c_{j\\ell}^{(b)}$ is the absolute Pearson correlation between the $j$-th factor of $L_k^{(b)}$ and the $\\ell$-th factor of $L_k^{\\text{ref}}$. The correlation is computed between the bootstrap loading vector and the reference loading vector restricted to the resampled feature indices. The optimal matching is found by identifying the permutation $\\pi^{(b)}$ of factors that maximizes the sum of correlations of matched pairs, $\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$. This is a linear assignment problem, solvable efficiently by the Hungarian method. The stability score for the replicate, $s^{(b)}$, is the average of these maximum matched correlations: $s^{(b)} = \\frac{1}{k}\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$.\n\n5.  **Aggregation and Model Selection**:\n    After $B$ bootstrap replicates, we obtain $B$ stability scores for each pair of $(k, \\text{rotation type})$.\n    *   **Aggregation**: For each $(k, r)$, we compute the mean stability score $\\bar{s}_{k,r}$ and its standard error $\\text{se}_{k,r}$.\n    *   **Selection Rule**: The final model is chosen using a two-step process. First, for each candidate dimension $k$, we select the rotation type $r_k$ that yields the highest mean stability $\\bar{s}_{k,r}$. This gives a sequence of best-stability scores, one for each $k$. Second, we apply the \"one-standard-error rule\" to this sequence to select the final dimension $k^{\\star}$. This rule balances model performance (stability) with parsimony. It selects the simplest model (smallest $k$) whose stability is not substantially worse than the most stable model. Specifically, if $k_{\\max}$ is the dimension with the absolute highest stability $\\bar{s}_{\\max}$, we choose $k^{\\star}$ as the smallest $k$ whose stability $\\bar{s}_{k, r_k}$ is within one standard error of the maximum: $\\bar{s}_{k, r_k} \\ge \\bar{s}_{\\max} - \\text{se}_{k_{\\max}, r_{k_{\\max}}}$. The final rotation type $r^{\\star}$ is then the one chosen for $k^{\\star}$ in the first step, $r^{\\star} = r_{k^{\\star}}$.\n\nThis systematic, data-driven methodology provides a robust framework for choosing the crucial hyperparameters of a latent factor model, avoiding arbitrary decisions and grounding the choice in the empirical stability of the inferred structure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\nfrom scipy.optimize import linear_sum_assignment\n\ndef generate_test_data(case_id, seed):\n    \"\"\"Generates synthetic data for the three test cases.\"\"\"\n    rng = np.random.default_rng(seed)\n    if case_id == 1:\n        n, p, k_true, sigma = 120, 20, 3, 0.5\n        W = np.zeros((p, k_true))\n        W[0:7, 0] = 1.0\n        W[7:14, 1] = 1.0\n        W[14:20, 2] = 1.0\n        W += rng.normal(0, 0.1, size=W.shape)\n        Z = rng.normal(size=(n, k_true))\n        X = Z @ W.T + sigma * rng.normal(size=(n, p))\n    elif case_id == 2:\n        n, p, k_true, sigma = 100, 15, 1, 0.8\n        W = rng.normal(size=(p, k_true))\n        amplified_indices = rng.choice(p, size=int(p / 2), replace=False)\n        W[amplified_indices, :] *= 3.0\n        Z = rng.normal(size=(n, k_true))\n        X = Z @ W.T + sigma * rng.normal(size=(n, p))\n    elif case_id == 3:\n        n, p, sigma = 100, 18, 1.0\n        X = sigma * rng.normal(size=(n, p))\n    else:\n        raise ValueError(\"Invalid case_id\")\n    return X\n\ndef standardize_data(X):\n    \"\"\"Standardizes columns of X to mean 0 and variance 1.\"\"\"\n    X_std = X.copy()\n    mu = np.mean(X_std, axis=0)\n    sigma = np.std(X_std, axis=0)\n    non_zero_std_mask = sigma > 1e-10\n    X_std[:, non_zero_std_mask] = (X_std[:, non_zero_std_mask] - mu[non_zero_std_mask]) / sigma[non_zero_std_mask]\n    return X_std\n\ndef varimax(L, tol=1e-8, max_iter=100):\n    \"\"\"Orthogonal varimax rotation using SVD.\"\"\"\n    p, k = L.shape\n    if k  2:\n        return L\n    \n    h = np.sqrt(np.sum(L**2, axis=1, keepdims=True))\n    h[h == 0] = 1.0\n    L_norm = L / h\n    \n    T = np.eye(k)\n    \n    for _ in range(max_iter):\n        L_rot = L_norm @ T\n        g = np.sum(L_rot**2, axis=0) / p\n        B = (1/p) * L_rot.T @ (L_rot**3 - L_rot @ np.diag(g))\n        \n        try:\n            U, _, Vt = svd(B)\n            R = U @ Vt\n        except np.linalg.LinAlgError:\n            break\n        \n        if np.sum((R - np.eye(k))**2)  tol:\n            break\n            \n        T = T @ R\n    \n    L_rotated_normalized = L_norm @ T\n    L_final = L_rotated_normalized * h\n    return L_final\n\ndef pearson_correlation(x, y):\n    \"\"\"Computes the Pearson correlation coefficient, handling zero variance.\"\"\"\n    mean_x, mean_y = np.mean(x), np.mean(y)\n    std_x, std_y = np.std(x), np.std(y)\n    \n    if std_x == 0 or std_y == 0:\n        return 0.0\n    \n    cov = np.mean((x - mean_x) * (y - mean_y))\n    return cov / (std_x * std_y)\n\ndef get_stability_scores(X, k, rotation_type, B, rng):\n    \"\"\"Computes stability scores for a given k and rotation type.\"\"\"\n    X_std = standardize_data(X)\n    n, p = X_std.shape\n    \n    try:\n        U, s, Vt = svd(X_std, full_matrices=False)\n        L_ref_unrotated = Vt[:k, :].T @ np.diag(s[:k])\n    except np.linalg.LinAlgError:\n        return np.array([])\n    \n    if rotation_type == 'varimax':\n        L_ref = varimax(L_ref_unrotated)\n    else:\n        L_ref = L_ref_unrotated\n        \n    scores = []\n    for _ in range(B):\n        row_indices = rng.choice(n, n, replace=True)\n        col_indices = rng.choice(p, p, replace=True)\n        \n        X_boot_raw = X_std[row_indices, :][:, col_indices]\n        X_boot_std = standardize_data(X_boot_raw)\n        \n        try:\n            U_b, s_b, Vt_b = svd(X_boot_std, full_matrices=False)\n            \n            num_sv = len(s_b)\n            if num_sv  k:\n                s_b_padded = np.zeros(k)\n                s_b_padded[:num_sv] = s_b\n                s_b = s_b_padded\n                Vt_b_padded = np.zeros((k, p))\n                Vt_b_padded[:Vt_b.shape[0], :] = Vt_b[:num_sv, :]\n                Vt_b = Vt_b_padded\n\n            L_boot_unrotated = Vt_b[:k, :].T @ np.diag(s_b[:k])\n            \n            if rotation_type == 'varimax':\n                L_boot = varimax(L_boot_unrotated)\n            else:\n                L_boot = L_boot_unrotated\n        except np.linalg.LinAlgError:\n            continue\n            \n        cost_matrix = np.zeros((k, k))\n        for j in range(k):\n            for l in range(k):\n                vec_boot = L_boot[:, j]\n                vec_ref_resampled = L_ref[col_indices, l]\n                corr = pearson_correlation(vec_boot, vec_ref_resampled)\n                cost_matrix[j, l] = np.abs(corr)\n\n        row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n        \n        max_corrs = cost_matrix[row_ind, col_ind]\n        score = np.mean(max_corrs) if k > 0 else 1.0\n        scores.append(score)\n        \n    return np.array(scores)\n\ndef solve():\n    test_specs = [\n        {'case_id': 1, 'seed': 1},\n        {'case_id': 2, 'seed': 2},\n        {'case_id': 3, 'seed': 3}\n    ]\n    \n    B = 40\n    candidate_ks = [1, 2, 3, 4, 5]\n    rotation_types = ['none', 'varimax']\n    final_results = []\n\n    for spec in test_specs:\n        X = generate_test_data(spec['case_id'], spec['seed'])\n        rng = np.random.default_rng(spec['seed'])\n        \n        stability_means = np.zeros((len(candidate_ks), len(rotation_types)))\n        stability_ses = np.zeros((len(candidate_ks), len(rotation_types)))\n\n        for i, k in enumerate(candidate_ks):\n            for j, r_type in enumerate(rotation_types):\n                scores = get_stability_scores(X, k, r_type, B, rng)\n                if len(scores) > 1:\n                    stability_means[i, j] = np.mean(scores)\n                    stability_ses[i, j] = np.std(scores, ddof=1) / np.sqrt(len(scores))\n                else: \n                    stability_means[i, j] = 0\n                    stability_ses[i, j] = 0\n\n        best_rotation_indices = np.argmax(stability_means, axis=1)\n        best_stabilities = np.array([stability_means[i, best_rotation_indices[i]] for i in range(len(candidate_ks))])\n        best_ses = np.array([stability_ses[i, best_rotation_indices[i]] for i in range(len(candidate_ks))])\n        \n        if np.all(best_stabilities == 0):\n            k_star = 1\n            r_star_code = 0\n        else:\n            k_max_idx = np.argmax(best_stabilities)\n            s_max = best_stabilities[k_max_idx]\n            se_at_max = best_ses[k_max_idx]\n            \n            threshold = s_max - se_at_max\n            \n            eligible_k_indices = np.where(best_stabilities >= threshold)[0]\n            k_star_idx = eligible_k_indices[0] if len(eligible_k_indices) > 0 else 0\n            \n            k_star = candidate_ks[k_star_idx]\n            r_star_code = best_rotation_indices[k_star_idx]\n\n        final_results.append([k_star, int(r_star_code)])\n        \n    print(f\"[{','.join(map(str, final_results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3137686"}, {"introduction": "While standard factor models are powerful tools for unsupervised dimensionality reduction, their discovered factors may not be relevant for a specific predictive task. This practice introduces you to the world of supervised factor models, which bridge this gap by incorporating a response variable $y$ directly into the factorization process. You will derive and implement an alternating minimization algorithm to solve for the latent factors, learning how this supervision helps identify a structure that is not only good for reconstructing the data $X$, but also maximally predictive of the outcome [@problem_id:3137661].", "problem": "Consider a supervised latent factor model that jointly explains a data matrix and a response. Let $X \\in \\mathbb{R}^{n \\times d}$ be a data matrix, $y \\in \\mathbb{R}^{n}$ be a response vector, $Z \\in \\mathbb{R}^{n \\times k}$ be latent scores, $W \\in \\mathbb{R}^{d \\times k}$ be loadings, and $\\beta \\in \\mathbb{R}^{k}$ be a regression vector linking latent scores to the response. The goal is to study identifiability of latent factors in the presence of supervised coupling by analyzing how the coupling strength affects the alignment of the estimated latent predictors with the true response-associated latent direction.\n\nStarting from fundamental definitions, consider minimizing the regularized joint least squares objective\n$$\n\\mathcal{L}(Z,W,\\beta) \\;=\\; \\lVert X - Z W^\\top \\rVert_F^2 \\;+\\; \\lambda \\lVert y - Z \\beta \\rVert_2^2 \\;+\\; \\alpha \\lVert W \\rVert_F^2 \\;+\\; \\alpha \\lVert \\beta \\rVert_2^2 \\;+\\; \\gamma \\lVert Z \\rVert_F^2,\n$$\nwhere $\\lambda \\ge 0$ controls the strength of the supervised coupling, $\\alpha  0$ and $\\gamma  0$ are small ridge penalties to ensure numerical stability, $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm, and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. Assume a data-generating process of the form\n$$\nX \\;=\\; Z_{\\text{true}} W_{\\text{true}}^\\top \\;+\\; \\varepsilon_X, \\quad y \\;=\\; Z_{\\text{true}} \\beta_{\\text{true}} \\;+\\; \\varepsilon_y,\n$$\nwith independent Gaussian noise $\\varepsilon_X$ and $\\varepsilon_y$, and where the first column of $Z_{\\text{true}}$ is the direction that drives the response via $\\beta_{\\text{true}}$.\n\nYour tasks are:\n- Derive, from first principles by setting gradients to zero, closed-form normal equations for each block of variables $W$, $\\beta$, and $Z$ that minimize $\\mathcal{L}(Z,W,\\beta)$ when the other blocks are held fixed.\n- Design an alternating minimization algorithm based on these normal equations that updates $W$, $\\beta$, and $Z$ in turn until convergence or for a fixed number of iterations. Use a single fixed random initialization of $Z$ per test case to isolate the effect of the coupling strength $\\lambda$.\n- Define the alignment metric as follows. Let $s \\in \\mathbb{R}^{n}$ be the true response-associated latent score vector equal to the first column of $Z_{\\text{true}}$. Let the estimated latent subspace be the column space of the final $Z$. The alignment is defined as\n$$\n\\text{align}(Z, s) \\;=\\; \\frac{\\lVert P_Z s \\rVert_2}{\\lVert s \\rVert_2},\n$$\nwhere $P_Z$ is the orthogonal projector onto the column space of $Z$. This takes values in $[0,1]$, with $1$ indicating perfect recovery of the true response-associated direction.\n- For each test case, search over a discrete grid of coupling strengths\n$$\n\\Lambda \\;=\\; [\\,0.0,\\, 0.05,\\, 0.1,\\, 0.5,\\, 1.0,\\, 2.0,\\, 5.0,\\, 10.0\\,]\n$$\nand return the minimal $\\lambda \\in \\Lambda$ for which $\\text{align}(Z, s) \\ge \\tau$, with threshold $\\tau = 0.9$. If no such $\\lambda$ achieves the threshold, return $-1.0$.\n\nData generation details per test case:\n- Draw $Z_{\\text{true}} \\in \\mathbb{R}^{n \\times k}$ with independent standard normal entries.\n- Draw $W_{\\text{true}} \\in \\mathbb{R}^{d \\times k}$ with independent normal entries of variance scaled to maintain numerical stability.\n- Draw $\\varepsilon_X$ with independent normal entries of variance $\\sigma_X^2$ and $\\varepsilon_y$ with independent normal entries of variance $\\sigma_y^2$.\n- Set $\\beta_{\\text{true}}$ as specified below; its first coordinate determines the strength of supervision via the first column of $Z_{\\text{true}}$.\n- Use a fixed pseudorandom seed per test case as given.\n\nTest suite:\n- Test case $1$: seed $= 42$, $n = 200$, $d = 60$, $k = 3$, $\\sigma_X = 0.25$, $\\sigma_y = 0.25$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (2.0, 0.0, 0.0)$.\n- Test case $2$: seed $= 7$, $n = 200$, $d = 60$, $k = 3$, $\\sigma_X = 0.5$, $\\sigma_y = 0.5$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (0.2, 0.0, 0.0)$.\n- Test case $3$: seed $= 123$, $n = 200$, $d = 60$, $k = 3$, $\\sigma_X = 0.25$, $\\sigma_y = 1.0$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (0.0, 0.0, 0.0)$.\n- Test case $4$: seed $= 99$, $n = 200$, $d = 60$, $k = 1$, $\\sigma_X = 0.25$, $\\sigma_y = 0.25$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (2.0)$.\n\nYour program must implement the alternating minimization algorithm, compute the alignment for each $\\lambda \\in \\Lambda$, select the minimal $\\lambda$ that achieves $\\text{align}(Z, s) \\ge \\tau$, and output the results for the four test cases in a single line as a comma-separated list enclosed in square brackets (for example, $[0.5,-1.0,2.0,0.0]$). No physical units or angle units are involved. The output for each test case must be a single float, with $-1.0$ used to indicate that no $\\lambda \\in \\Lambda$ achieved the threshold.", "solution": "The user has provided a well-posed problem in statistical learning concerning a supervised latent factor model. The task is to derive the update rules for an alternating minimization algorithm, implement it, and use it to find the minimum supervised coupling strength $\\lambda$ that ensures sufficient alignment of the estimated latent space with a true, known latent direction.\n\n### 1. Model and Objective Function\n\nThe problem revolves around minimizing the regularized joint least squares objective function $\\mathcal{L}(Z,W,\\beta)$ for a data matrix $X \\in \\mathbb{R}^{n \\times d}$, a response vector $y \\in \\mathbb{R}^{n}$, latent scores $Z \\in \\mathbb{R}^{n \\times k}$, loadings $W \\in \\mathbb{R}^{d \\times k}$, and a regression vector $\\beta \\in \\mathbb{R}^{k}$. The objective is:\n$$\n\\mathcal{L}(Z,W,\\beta) \\;=\\; \\lVert X - Z W^\\top \\rVert_F^2 \\;+\\; \\lambda \\lVert y - Z \\beta \\rVert_2^2 \\;+\\; \\alpha \\lVert W \\rVert_F^2 \\;+\\; \\alpha \\lVert \\beta \\rVert_2^2 \\;+\\; \\gamma \\lVert Z \\rVert_F^2\n$$\nHere, $\\lambda \\ge 0$ is the supervision strength, while $\\alpha  0$ and $\\gamma  0$ are ridge regularization parameters. The norms are Frobenius $\\lVert \\cdot \\rVert_F$ and Euclidean $\\lVert \\cdot \\rVert_2$.\n\n### 2. Derivation of Normal Equations for Alternating Minimization\n\nThe optimization is performed by alternatingly minimizing $\\mathcal{L}$ with respect to one block of variables ($W$, $\\beta$, or $Z$) while holding the others fixed. This requires deriving the closed-form solution for each subproblem by setting the respective gradient to zero.\n\n#### 2.1. Update for Loadings $W$\nFixing $Z$ and $\\beta$, we minimize the terms in $\\mathcal{L}$ that depend on $W$:\n$$\n\\mathcal{L}_W(W) = \\lVert X - Z W^\\top \\rVert_F^2 + \\alpha \\lVert W \\rVert_F^2\n$$\nTo find the minimum, we compute the gradient with respect to $W$ and set it to zero. Using the matrix derivative identities $\\nabla_A \\lVert M-BA^\\top\\rVert_F^2 = -2(M-BA^\\top)^\\top B$ and $\\nabla_A \\lVert A \\rVert_F^2 = 2A$, we have:\n$$\n\\nabla_W \\mathcal{L}_W = -2(X - ZW^\\top)^\\top Z + 2\\alpha W = -2(X^\\top Z - W Z^\\top Z) + 2\\alpha W = 0\n$$\nRearranging the terms to solve for $W$:\n$$\n-X^\\top Z + W(Z^\\top Z) + \\alpha W = 0 \\implies W(Z^\\top Z + \\alpha I_k) = X^\\top Z\n$$\nwhere $I_k$ is the $k \\times k$ identity matrix. This gives the normal equation for $W$:\n$$\nW = (X^\\top Z) (Z^\\top Z + \\alpha I_k)^{-1}\n$$\n\n#### 2.2. Update for Regression Vector $\\beta$\nFixing $Z$ and $W$, we minimize the terms in $\\mathcal{L}$ that depend on $\\beta$:\n$$\n\\mathcal{L}_\\beta(\\beta) = \\lambda \\lVert y - Z \\beta \\rVert_2^2 + \\alpha \\lVert \\beta \\rVert_2^2\n$$\nThis is a standard ridge regression problem. The gradient is:\n$$\n\\nabla_\\beta \\mathcal{L}_\\beta = \\lambda \\nabla_\\beta (y - Z\\beta)^\\top(y - Z\\beta) + \\alpha \\nabla_\\beta \\beta^\\top\\beta = \\lambda (-2Z^\\top (y - Z\\beta)) + 2\\alpha\\beta\n$$\nSetting the gradient to zero:\n$$\n-\\lambda Z^\\top y + \\lambda (Z^\\top Z)\\beta + \\alpha\\beta = 0 \\implies (\\lambda Z^\\top Z + \\alpha I_k) \\beta = \\lambda Z^\\top y\n$$\nThe resulting normal equation for $\\beta$ is:\n$$\n\\beta = (\\lambda Z^\\top Z + \\alpha I_k)^{-1} (\\lambda Z^\\top y)\n$$\n\n#### 2.3. Update for Latent Scores $Z$\nFixing $W$ and $\\beta$, we minimize the terms in $\\mathcal{L}$ that depend on $Z$:\n$$\n\\mathcal{L}_Z(Z) = \\lVert X - Z W^\\top \\rVert_F^2 + \\lambda \\lVert y - Z \\beta \\rVert_2^2 + \\gamma \\lVert Z \\rVert_F^2\n$$\nUsing the identities $\\nabla_A \\lVert M-AW \\rVert_F^2 = -2(M-AW)W^\\top$, $\\nabla_A \\lVert y-Ab\\rVert_2^2 = -2(y-Ab)b^\\top$, and $\\nabla_A \\lVert A \\rVert_F^2=2A$:\n$$\n\\nabla_Z \\mathcal{L}_Z = -2(X - ZW^\\top)W + \\lambda (-2(y - Z\\beta)\\beta^\\top) + 2\\gamma Z = 0\n$$\nRearranging the terms to solve for $Z$:\n$$\n-XW + Z(W^\\top W) - \\lambda y \\beta^\\top + \\lambda Z(\\beta\\beta^\\top) + \\gamma Z = 0\n$$\n$$\nZ(W^\\top W + \\lambda \\beta\\beta^\\top + \\gamma I_k) = XW + \\lambda y\\beta^\\top\n$$\nThe normal equation for $Z$ is:\n$$\nZ = (XW + \\lambda y \\beta^\\top) (W^\\top W + \\lambda \\beta\\beta^\\top + \\gamma I_k)^{-1}\n$$\n\n### 3. Algorithm and Evaluation\n\nThe overall algorithm proceeds as follows for each test case:\n1.  **Data Generation**: Generate $Z_{\\text{true}}$, $W_{\\text{true}}$, $\\varepsilon_X$, and $\\varepsilon_y$ using the specified parameters and random seed. Construct the observation matrix $X = Z_{\\text{true}} W_{\\text{true}}^\\top + \\varepsilon_X$ and response vector $y = Z_{\\text{true}} \\beta_{\\text{true}} + \\varepsilon_y$. The true response-associated latent score vector is $s = Z_{\\text{true}}[:, 0]$.\n2.  **Initialization**: For each test case, generate a single fixed random latent score matrix $Z_{init}$ using the specified seed.\n3.  **Parameter Search**: Iterate through each coupling strength $\\lambda$ in the grid $\\Lambda = [\\,0.0,\\, 0.05,\\, 0.1,\\, 0.5,\\, 1.0,\\, 2.0,\\, 5.0,\\, 10.0\\,]$.\n4.  **Alternating Minimization**: For each $\\lambda$, initialize $Z = Z_{init}$ and perform a fixed number of iterations (e.g., $100$) of the following updates:\n    a. Update $W$ using the derived equation.\n    b. Update $\\beta$ using the derived equation.\n    c. Update $Z$ using the derived equation.\n5.  **Alignment Calculation**: After the iterations, calculate the alignment of the final estimated latent space, represented by the columns of $Z$, with the true score vector $s$. This is done by computing $\\text{align}(Z, s) = \\lVert P_Z s \\rVert_2 / \\lVert s \\rVert_2$, where $P_Z = Q_Z Q_Z^\\top$ is the orthogonal projector onto the column space of $Z$, and $Q_Z$ is an orthonormal basis for that space (obtained via QR decomposition).\n6.  **Minimal $\\lambda$ Selection**: If the alignment meets or exceeds the threshold $\\tau=0.9$, the current $\\lambda$ is recorded as the result for the test case, and the search for this case terminates. If the loop over $\\Lambda$ completes without meeting the threshold, the result is $-1.0$.\n\nThe final output is a list of the minimal $\\lambda$ values for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, n, d, k, sigma_X, sigma_y, alpha, gamma, beta_true)\n        (42, 200, 60, 3, 0.25, 0.25, 1e-3, 1e-3, (2.0, 0.0, 0.0)),\n        (7, 200, 60, 3, 0.5, 0.5, 1e-3, 1e-3, (0.2, 0.0, 0.0)),\n        (123, 200, 60, 3, 0.25, 1.0, 1e-3, 1e-3, (0.0, 0.0, 0.0)),\n        (99, 200, 60, 1, 0.25, 0.25, 1e-3, 1e-3, (2.0,)),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef solve_test_case(params):\n    \"\"\"\n    Solves for a single test case.\n    \"\"\"\n    seed, n, d, k, sigma_X, sigma_y, alpha, gamma, beta_true_tuple = params\n    beta_true = np.array(beta_true_tuple, dtype=float).reshape(-1, 1)\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n    Z_true = rng.standard_normal(size=(n, k))\n    W_true = rng.normal(scale=1.0/np.sqrt(d), size=(d, k))\n    eps_X = rng.normal(scale=sigma_X, size=(n, d))\n    eps_y = rng.normal(scale=sigma_y, size=(n, 1))\n\n    X = Z_true @ W_true.T + eps_X\n    y = Z_true @ beta_true + eps_y\n\n    s = Z_true[:, 0]\n    \n    # --- Algorithm Parameters ---\n    LAMBDA_GRID = [0.0, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n    ALIGNMENT_THRESHOLD = 0.9\n    NUM_ITER = 100\n\n    # --- Single fixed random initialization for Z ---\n    Z_init = rng.standard_normal(size=(n, k))\n\n    # --- Search over lambda grid ---\n    for lam in LAMBDA_GRID:\n        Z = Z_init.copy()\n        W = np.zeros((d, k))\n        beta = np.zeros((k, 1))\n\n        # --- Alternating Minimization ---\n        for _ in range(NUM_ITER):\n            # Update W\n            # W = (X^T Z) (Z^T Z + alpha I_k)^-1\n            # Solve (Z^T Z + alpha I) W^T = Z^T X for W^T\n            A_W = Z.T @ Z + alpha * np.identity(k)\n            b_W = Z.T @ X\n            W_T = np.linalg.solve(A_W, b_W)\n            W = W_T.T\n\n            # Update beta\n            # beta = (lambda Z^T Z + alpha I_k)^-1 (lambda Z^T y)\n            # Solve (lambda Z^T Z + alpha I) beta = lambda Z^T y\n            A_beta = lam * (Z.T @ Z) + alpha * np.identity(k)\n            b_beta = lam * (Z.T @ y)\n            beta = np.linalg.solve(A_beta, b_beta)\n\n            # Update Z\n            # Z = (XW + lambda y beta^T) (W^T W + lambda beta beta^T + gamma I_k)^-1\n            # Solve (W^T W + ...)^T Z = (XW + ...)^T for Z\n            # Equivalently, solve B Z^T = A^T for Z^T where Z = A B^-1\n            A_Z_term1 = W.T @ W\n            A_Z_term2 = lam * (beta @ beta.T)\n            A_Z_term3 = gamma * np.identity(k)\n            A_Z = A_Z_term1 + A_Z_term2 + A_Z_term3\n\n            b_Z_term1 = X @ W\n            b_Z_term2 = lam * (y @ beta.T)\n            b_Z = b_Z_term1 + b_Z_term2\n            \n            # Solve A_Z^T Z^T = B^Z^T which is A_Z Z^T = B_Z^T\n            Z_T = np.linalg.solve(A_Z, b_Z.T)\n            Z = Z_T.T\n\n        # --- Alignment Calculation ---\n        # Compute orthonormal basis for the column space of Z\n        Q, _ = np.linalg.qr(Z, mode='reduced')\n        \n        # Project s onto the subspace spanned by Q and compute its norm\n        projected_s_norm = np.linalg.norm(Q.T @ s)\n        s_norm = np.linalg.norm(s)\n\n        alignment = 0.0 if s_norm == 0 else projected_s_norm / s_norm\n        \n        # --- Check Threshold ---\n        if alignment >= ALIGNMENT_THRESHOLD:\n            return lam\n\n    return -1.0\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3137661"}]}