{"hands_on_practices": [{"introduction": "The foundation of solving Markov Decision Processes (MDPs) lies in dynamic programming. This first exercise guides you through implementing the two most fundamental algorithms: value iteration for finding the optimal value function $v^{\\star}$, and iterative policy evaluation for finding the value of a fixed policy $v_{\\pi}$. You will explore how different update schemes—synchronous (Jacobi-style) versus in-place (Gauss-Seidel-style)—affect convergence speed, providing a practical link between reinforcement learning and classical numerical methods [@problem_id:3245192].", "problem": "You are given a discounted, finite-state, finite-action Markov Decision Process (MDP) in which the goal is to compute the unique fixed point of the Bellman optimality equations through iterative methods. Let the set of states be $\\mathcal{S} = \\{0, 1, \\dots, n-1\\}$ and the set of actions be $\\mathcal{A} = \\{0, 1, \\dots, m-1\\}$. For each state-action pair $(s,a)$, there is an expected immediate reward $r(s,a)$ and a transition probability kernel $P(s' \\mid s,a)$ over next states. The discount factor is $\\gamma \\in [0,1)$. Denote by $v \\in \\mathbb{R}^n$ a value vector over states.\n\nFundamental base:\n- The Bellman optimality operator $\\mathcal{T}$ is defined for $v \\in \\mathbb{R}^n$ componentwise by\n$$\n(\\mathcal{T} v)(s) \\triangleq \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v(s') \\right).\n$$\n- The optimal value $v^\\star$ is the unique fixed point of $\\mathcal{T}$ satisfying $v^\\star = \\mathcal{T} v^\\star$.\n- For any fixed policy $\\pi : \\mathcal{S} \\to \\mathcal{A}$, the policy evaluation equation is linear:\n$$\nv_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi, \\quad \\text{equivalently} \\quad (I - \\gamma P_\\pi) v_\\pi = r_\\pi,\n$$\nwhere $r_\\pi(s) \\triangleq r(s,\\pi(s))$ and $(P_\\pi)_{s,s'} \\triangleq P(s' \\mid s,\\pi(s))$.\n\nTasks:\n- Implement two iterative schemes for the optimality equations:\n  1. Synchronous value iteration (Jacobi-style), which updates all components of $v$ simultaneously using the previous iterate,\n  2. Gauss-Seidel-style value iteration, which updates components in-place in a fixed order within each sweep, reusing newly updated components immediately.\n- Implement two iterative schemes for policy evaluation of a given fixed policy $\\pi$:\n  1. Synchronous policy evaluation (Jacobi method) for the linear system $(I - \\gamma P_\\pi) v = r_\\pi$,\n  2. Gauss-Seidel policy evaluation for the same linear system.\n\nStopping rule:\n- For each method, start from $v^{(0)} = 0$ (the zero vector). After each full iteration (a full synchronous update or a full in-place sweep over all states), compute the update magnitude\n$$\n\\Delta^{(k)} \\triangleq \\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty,\n$$\nand stop at the first $k$ such that $\\Delta^{(k)} \\le \\varepsilon$, where $\\varepsilon$ is a given tolerance. Use the same $\\varepsilon$ for all methods and tests. Count and report the number of iterations taken for convergence.\n\nRelation to iterative methods for linear systems:\n- For the fixed policy case with $A \\triangleq I - \\gamma P_\\pi$ and $b \\triangleq r_\\pi$, the synchronous policy evaluation coincides with the Jacobi method applied to $A v = b$, and the in-place policy evaluation coincides with the Gauss-Seidel method applied to $A v = b$.\n\nTest suite:\nImplement your program to run the following three tests. Each test specifies $(P, R, \\gamma, \\pi)$, where $P$ is a tensor with components $P[s,a,s']$, $R$ is a matrix with components $R[s,a] = r(s,a)$, $\\gamma$ is the discount factor, and $\\pi$ is a fixed policy for the policy evaluation subtests.\n\n- Test $1$ (happy path, small $\\gamma$):\n  - States: $\\{0,1,2\\}$ with $n = 3$ and actions $\\{0,1\\}$ with $m = 2$.\n  - Transitions $P$:\n    - Action $0$: $P(0 \\mid 0,0) = 0.5$, $P(1 \\mid 0,0) = 0.5$; $P(1 \\mid 1,0) = 0.5$, $P(2 \\mid 1,0) = 0.5$; $P(2 \\mid 2,0) = 1.0$.\n    - Action $1$: $P(0 \\mid 0,1) = 0.7$, $P(1 \\mid 0,1) = 0.3$; $P(0 \\mid 1,1) = 0.4$, $P(2 \\mid 1,1) = 0.6$; $P(1 \\mid 2,1) = 1.0$.\n    - All unspecified $P(s' \\mid s,a)$ are $0$.\n  - Rewards $R$: $R(0,0) = 5$, $R(1,0) = 0$, $R(2,0) = 0$; $R(0,1) = 4$, $R(1,1) = 1$, $R(2,1) = 2$.\n  - Discount: $\\gamma = 0.9$.\n  - Fixed policy for evaluation: $\\pi(0) = 0$, $\\pi(1) = 1$, $\\pi(2) = 0$.\n- Test $2$ (boundary, $\\gamma$ near $1$):\n  - Same $P$ and $R$ as in Test $1$.\n  - Discount: $\\gamma = 0.99$.\n  - Fixed policy for evaluation: same $\\pi$ as in Test $1$.\n- Test $3$ (edge, absorbing structure):\n  - States: $\\{0,1\\}$ with $n = 2$ and actions $\\{0,1\\}$ with $m = 2$.\n  - Transitions $P$:\n    - Action $0$: $P(1 \\mid 0,0) = 1.0$; $P(1 \\mid 1,0) = 1.0$.\n    - Action $1$: $P(0 \\mid 0,1) = 1.0$; $P(1 \\mid 1,1) = 1.0$.\n    - All unspecified $P(s' \\mid s,a)$ are $0$.\n  - Rewards $R$: $R(0,0) = 1$, $R(1,0) = 0$; $R(0,1) = 0$, $R(1,1) = 0$.\n  - Discount: $\\gamma = 0.95$.\n  - Fixed policy for evaluation: $\\pi(0) = 1$, $\\pi(1) = 0$.\n  \nTolerance:\n- Use $\\varepsilon = 10^{-8}$ for all methods and all tests.\n\nRequired outputs:\n- For each test, compute and return the following four integers:\n  1. $N_{\\text{opt, sync}}$: iterations for synchronous value iteration to converge,\n  2. $N_{\\text{opt, GS}}$: iterations for Gauss-Seidel-style value iteration to converge,\n  3. $N_{\\text{eval, Jacobi}}$: iterations for synchronous policy evaluation (Jacobi) to converge,\n  4. $N_{\\text{eval, GS}}$: iterations for Gauss-Seidel policy evaluation to converge.\n- Aggregate the results of the three tests into a single flat list in the order\n$$\n[\\;N_{\\text{opt, sync}}^{(1)},\\; N_{\\text{opt, GS}}^{(1)},\\; N_{\\text{eval, Jacobi}}^{(1)},\\; N_{\\text{eval, GS}}^{(1)},\\; N_{\\text{opt, sync}}^{(2)},\\; N_{\\text{opt, GS}}^{(2)},\\; N_{\\text{eval, Jacobi}}^{(2)},\\; N_{\\text{eval, GS}}^{(2)},\\; N_{\\text{opt, sync}}^{(3)},\\; N_{\\text{opt, GS}}^{(3)},\\; N_{\\text{eval, Jacobi}}^{(3)},\\; N_{\\text{eval, GS}}^{(3)}\\;].\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For example, if there were two tests with two integers each, the format would be \"[1,2,3,4]\". In this problem, the line must contain $12$ integers in the order specified above.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n\n- **Sets**: State space $\\mathcal{S} = \\{0, 1, \\dots, n-1\\}$, action space $\\mathcal{A} = \\{0, 1, \\dots, m-1\\}$.\n- **MDP Components**:\n  - Expected immediate reward for state-action pair $(s,a)$: $r(s,a)$.\n  - Transition probability kernel: $P(s' \\mid s,a)$.\n  - Discount factor: $\\gamma \\in [0,1)$.\n- **Value Vector**: $v \\in \\mathbb{R}^n$.\n- **Bellman Optimality Operator**: $(\\mathcal{T} v)(s) \\triangleq \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v(s') \\right)$.\n- **Optimal Value Function**: $v^\\star$ is the unique fixed point satisfying $v^\\star = \\mathcal{T} v^\\star$.\n- **Fixed Policy Evaluation**: For a policy $\\pi : \\mathcal{S} \\to \\mathcal{A}$, the value function $v_\\pi$ satisfies the linear system $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$, where $r_\\pi(s) \\triangleq r(s,\\pi(s))$ and $(P_\\pi)_{s,s'} \\triangleq P(s' \\mid s,\\pi(s))$.\n- **Iterative Schemes to Implement**:\n  1. Synchronous value iteration (Jacobi-style).\n  2. Gauss-Seidel-style value iteration.\n  3. Synchronous policy evaluation (Jacobi method for the linear system).\n  4. Gauss-Seidel policy evaluation for the linear system.\n- **Stopping Rule**:\n  - Initial condition: $v^{(0)} = 0$.\n  - Iteration update: $\\Delta^{(k)} \\triangleq \\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty$.\n  - Termination: Stop at the first iteration $k$ where $\\Delta^{(k)} \\le \\varepsilon$.\n  - Tolerance: $\\varepsilon = 10^{-8}$.\n- **Test Suite**: Three specific test cases are provided, each defining the transition tensor $P$, reward matrix $R$, discount factor $\\gamma$, and a fixed policy $\\pi$.\n  - Test $1$: $n=3, m=2, \\gamma=0.9$.\n  - Test $2$: $n=3, m=2, \\gamma=0.99$.\n  - Test $3$: $n=2, m=2, \\gamma=0.95$.\n- **Required Outputs**: The number of iterations ($N_{\\text{opt, sync}}, N_{\\text{opt, GS}}, N_{\\text{eval, Jacobi}}, N_{\\text{eval, GS}}$) for each of the three tests, aggregated into a single list of $12$ integers.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is based on the standard theory of Markov Decision Processes and dynamic programming. The Bellman optimality equation, policy evaluation equations, and the iterative methods (Value Iteration, Jacobi, Gauss-Seidel) are fundamental concepts in this field and in numerical linear algebra. The condition $\\gamma \\in [0,1)$ is critical as it ensures the Bellman operator $\\mathcal{T}$ is a contraction mapping with respect to the infinity norm, which guarantees the existence of a unique fixed point $v^\\star$ and the convergence of value iteration from any starting point. This is a cornerstone result of dynamic programming. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. For each algorithm and test case, the task is to find the number of iterations to reach a specified precision. Since $\\gamma  1$, all iterative methods described are guaranteed to converge to a unique solution. The initial condition $v^{(0)}=0$ is provided, and the stopping criterion $\\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty \\le \\varepsilon$ is unambiguous. Therefore, a unique, stable, and meaningful integer solution (the number of iterations) exists for each required computation.\n- **Objective**: The problem is stated using precise mathematical language and definitions. All data and requirements are specified objectively, with no room for subjective interpretation.\n- **Complete and Consistent**: The problem statement is self-contained. It provides all necessary data for each test case ($P$, $R$, $\\gamma$, $\\pi$), the required algorithms, initial conditions, and a precise stopping rule. There are no contradictions in the provided information.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-structured, trivial, or unverifiable. The test cases explore different aspects of the algorithms, such as sensitivity to $\\gamma$ and behavior with absorbing states.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A reasoned solution will be provided.\n\nThe objective is to compute the optimal value function $v^\\star$ of a given Markov Decision Process (MDP) and to evaluate a given policy $\\pi$. We will implement four classical iterative algorithms to achieve this, using a specified convergence criterion. For each algorithm, we start with an initial value vector $v^{(0)}$ where all entries are $0$.\n\nThe first two methods address the non-linear Bellman optimality equation $v^\\star = \\mathcal{T}v^\\star$.\n\nThe first algorithm is **synchronous value iteration**. This method is analogous to the Jacobi method for linear systems. At each iteration $k$, the value of every state $s$ is updated simultaneously using the values from the previous iteration, $v^{(k-1)}$. The update rule for the entire vector $v$ is given by applying the Bellman operator: $v^{(k)} = \\mathcal{T}v^{(k-1)}$. Component-wise, this is:\n$$\nv^{(k)}(s) = \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v^{(k-1)}(s') \\right) \\quad \\text{for all } s \\in \\mathcal{S}.\n$$\nA full copy of $v^{(k-1)}$ is required to compute all components of $v^{(k)}$.\n\nThe second algorithm is **Gauss-Seidel-style value iteration**. This method performs in-place updates, analogous to the Gauss-Seidel method. The states are iterated through in a fixed order, for instance $s = 0, 1, \\dots, n-1$. When computing the new value for state $s$, the algorithm uses the most recently computed values for all other states. Specifically, for states $s'  s$, the new values from the current iteration $k$ are used, while for states $s' \\ge s$, the old values from iteration $k-1$ are used. The update rule within the sweep for iteration $k$ is:\n$$\nv(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s'=0}^{s-1} P(s' \\mid s,a) v^{(k)}(s') + \\gamma \\sum_{s'=s}^{n-1} P(s' \\mid s,a) v^{(k-1)}(s') \\right).\n$$\nThis is naturally implemented by updating the value vector in-place. Gauss-Seidel-style updates often converge faster than their synchronous counterparts.\n\nThe next two methods solve the linear system of equations for policy evaluation, $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$. This can be rewritten as the fixed-point equation $v_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi$.\n\nThe third algorithm is **synchronous policy evaluation**, which is precisely the **Jacobi method** applied to this system. Starting with $v^{(0)}=0$, the iterative scheme is:\n$$\nv^{(k)} = r_\\pi + \\gamma P_\\pi v^{(k-1)}.\n$$\nComponent-wise, this is:\n$$\nv^{(k)}(s) = r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) v^{(k-1)}(s').\n$$\nAs with synchronous value iteration, a full copy of the previous iterate $v^{(k-1)}$ is maintained.\n\nThe fourth algorithm is **Gauss-Seidel policy evaluation**. This applies the **Gauss-Seidel method** to the policy evaluation system. The updates are performed in-place. For each state $s$ in the fixed order $s=0, 1, \\dots, n-1$, the update is:\n$$\nv(s) \\leftarrow r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) v(s').\n$$\nThe sum on the right-hand side implicitly uses the new values for states already updated in the current sweep and old values for states yet to be updated.\n\nFor all four methods, iterations continue until the change in the value vector is sufficiently small, as measured by the infinity norm: $\\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty \\le \\varepsilon$, for the given tolerance $\\varepsilon = 10^{-8}$. The number of iterations $k$ required to satisfy this condition is recorded for each method and test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef synchronous_value_iteration(P, R, gamma, tol):\n    \"\"\"\n    Computes the optimal value function using synchronous value iteration (Jacobi-style).\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states, _ = R.shape\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        # Vectorized computation of Q-values for all state-action pairs\n        q_values = R + gamma * (P @ v_prev)  # Shape (n_states, n_actions)\n        v = np.max(q_values, axis=1)        # Shape (n_states,)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef gauss_seidel_value_iteration(P, R, gamma, tol):\n    \"\"\"\n    Computes the optimal value function using Gauss-Seidel-style value iteration.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states, _ = R.shape\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        for s in range(n_states):\n            # The mat-vec product uses the current state of v, which includes\n            # in-place updates from the current sweep for s'  s.\n            q_values_s = R[s, :] + gamma * (P[s, :, :] @ v)\n            v[s] = np.max(q_values_s)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef synchronous_policy_evaluation(P, R, pi, gamma, tol):\n    \"\"\"\n    Evaluates a fixed policy using the synchronous Jacobi method.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        pi (np.ndarray): Fixed policy vector.\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states = len(pi)\n    \n    # Construct r_pi and P_pi from the policy\n    s_indices = np.arange(n_states)\n    r_pi = R[s_indices, pi]\n    P_pi = P[s_indices, pi, :]\n    \n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        v = r_pi + gamma * (P_pi @ v_prev)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef gauss_seidel_policy_evaluation(P, R, pi, gamma, tol):\n    \"\"\"\n    Evaluates a fixed policy using the Gauss-Seidel method.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        pi (np.ndarray): Fixed policy vector.\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states = len(pi)\n    \n    # Construct r_pi and P_pi from the policy\n    s_indices = np.arange(n_states)\n    r_pi = R[s_indices, pi]\n    P_pi = P[s_indices, pi, :]\n\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        for s in range(n_states):\n            # In-place update using the current state of v\n            v[s] = r_pi[s] + gamma * (P_pi[s, :] @ v)\n\n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases, then prints the results.\n    \"\"\"\n    tol = 1e-8\n\n    # --- Test Case 1 ---\n    # n=3, m=2, gamma=0.9\n    P1 = np.zeros((3, 2, 3))\n    P1[0, 0, 0] = 0.5; P1[0, 0, 1] = 0.5\n    P1[1, 0, 1] = 0.5; P1[1, 0, 2] = 0.5\n    P1[2, 0, 2] = 1.0\n    P1[0, 1, 0] = 0.7; P1[0, 1, 1] = 0.3\n    P1[1, 1, 0] = 0.4; P1[1, 1, 2] = 0.6\n    P1[2, 1, 1] = 1.0\n    R1 = np.array([[5., 4.], [0., 1.], [0., 2.]])\n    gamma1 = 0.9\n    pi1 = np.array([0, 1, 0])\n\n    # --- Test Case 2 ---\n    # n=3, m=2, gamma=0.99 (same P and R as Test 1)\n    P2 = P1\n    R2 = R1\n    gamma2 = 0.99\n    pi2 = pi1\n    \n    # --- Test Case 3 ---\n    # n=2, m=2, gamma=0.95\n    P3 = np.zeros((2, 2, 2))\n    P3[0, 0, 1] = 1.0\n    P3[1, 0, 1] = 1.0\n    P3[0, 1, 0] = 1.0\n    P3[1, 1, 1] = 1.0\n    R3 = np.array([[1., 0.], [0., 0.]])\n    gamma3 = 0.95\n    pi3 = np.array([1, 0])\n\n    test_cases = [\n        (P1, R1, gamma1, pi1),\n        (P2, R2, gamma2, pi2),\n        (P3, R3, gamma3, pi3),\n    ]\n\n    results = []\n    for P, R, gamma, pi in test_cases:\n        N_opt_sync = synchronous_value_iteration(P, R, gamma, tol)\n        N_opt_GS = gauss_seidel_value_iteration(P, R, gamma, tol)\n        N_eval_Jacobi = synchronous_policy_evaluation(P, R, pi, gamma, tol)\n        N_eval_GS = gauss_seidel_policy_evaluation(P, R, pi, gamma, tol)\n        results.extend([N_opt_sync, N_opt_GS, N_eval_Jacobi, N_eval_GS])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3245192"}, {"introduction": "In many real-world applications, we do not have access to the true model of the environment and must instead learn from data collected by a different, potentially suboptimal, policy. This exercise tackles the crucial problem of Off-Policy Evaluation (OPE), where you will implement and compare three key estimators: a model-based approach, a per-decision Importance Sampling (IS) estimator, and a Doubly Robust (DR) estimator. This practice provides invaluable experience with the bias-variance trade-off and demonstrates how the DR estimator cleverly combines a learned model with IS corrections to achieve strong performance [@problem_id:3145244].", "problem": "You are given three off-policy evaluation tasks in the setting of a Markov Decision Process (MDP). The aim is to implement a doubly robust estimator that combines a model-based value estimate with an importance sampling correction. The work must be derived from first principles of probability and the Markov property. Your program must compute, for each test case, three quantities: the per-decision importance sampling estimate, the model-based value estimate under the target policy, and the doubly robust estimate. Your program must output a single line containing a list of lists, each inner list being the three numerical floats for a test case, in the specified order.\n\nFundamental base to use:\n- A Markov Decision Process (MDP) consists of a finite state space $S$, a finite action space $A$, a transition kernel $P(s' \\mid s,a)$, an immediate reward function $r(s,a)$, a discount factor $\\gamma \\in (0,1]$, a finite horizon $H \\in \\mathbb{N}$, and a policy $\\pi(a \\mid s)$ describing the decision rule.\n- The Markov property states that $P(s_{t+1} \\mid s_{0:t}, a_{0:t}) = P(s_{t+1} \\mid s_t, a_t)$, and $r_t = r(s_t,a_t)$, for each time step $t$.\n- The expected discounted return under a policy $\\pi$ from an initial state distribution $d_0(s)$ is $V^\\pi = \\mathbb{E}_{s_0 \\sim d_0}\\left[\\sum_{t=0}^{H-1} \\gamma^t r(s_t,a_t)\\right]$, where $(s_t,a_t)$ follow the MDP dynamics controlled by $\\pi$.\n- Importance sampling (IS) is based on the identity $\\mathbb{E}_p[f(X)] = \\mathbb{E}_q\\left[f(X)\\frac{p(X)}{q(X)}\\right]$ when $p$ is absolutely continuous with respect to $q$, applied step-wise to trajectories to correct for the mismatch between the behavior policy and the target policy.\n- Maximum likelihood estimation (MLE) over observed transitions can be used to form a model-based estimate $\\hat{P}(s' \\mid s,a)$ and $\\hat{r}(s,a)$, which induce dynamic programming recursions for model-based value functions.\n\nYour implementation requirements:\n- For each test case, estimate the model $\\hat{P}(s' \\mid s,a)$ from the provided behavior trajectories using maximum likelihood with add-one Laplace smoothing over next-state counts for each state-action pair to ensure well-defined probabilities. For the immediate reward estimator $\\hat{r}(s,a)$, use the empirical average of the observed rewards for each pair $(s,a)$; if a pair $(s,a)$ is never observed, set $\\hat{r}(s,a) = 0$.\n- Compute the model-based value under the target policy $\\pi$, denoted $\\hat{V}^\\pi$, by dynamic programming over the finite horizon $H$. Use the empirical initial state distribution $d_0$ induced by the dataset of initial states for each test case to evaluate the expected return: $\\hat{V}^\\pi = \\sum_{s \\in S} d_0(s)\\hat{V}_0(s)$, where $\\hat{V}_t(s)$ is computed by the recursion $\\hat{Q}_t(s,a) = \\hat{r}(s,a) + \\gamma \\sum_{s' \\in S} \\hat{P}(s' \\mid s,a)\\hat{V}_{t+1}(s')$ and $\\hat{V}_t(s) = \\sum_{a \\in A} \\pi(a \\mid s)\\hat{Q}_t(s,a)$, with terminal boundary condition $\\hat{V}_H(s)=0$ for all $s \\in S$.\n- Compute the per-decision importance sampling (IS) estimator using behavior trajectories and step-wise likelihood ratios $\\rho_{0:t} = \\prod_{k=0}^{t} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}$, where $b(a \\mid s)$ is the behavior policy for the given test case. The per-decision IS estimate is the average over episodes of the discounted sum of step-wise corrected rewards.\n- Implement a doubly robust (DR) estimator that uses the model-based $\\hat{Q}_t$ and $\\hat{V}_t$ computed above as a control variate and combines them with the per-decision importance weights. The construction must follow the principle that the estimator is unbiased if either the importance weights are correct or the model is correct. Use the standard step-wise combination based on the same $\\rho_{0:t}$ weights and the dynamic programming $\\hat{Q}_t$ and $\\hat{V}_t$ defined above; do not introduce clipping.\n- All calculations must be based only on the provided trajectories and policies; no external data or user inputs are allowed.\n\nTest suite:\nFor each test case, the state space is $S = \\{0,1,2\\}$ and the action space is $A = \\{0,1\\}$. Episodes are fixed-length and given as sequences of tuples $(s_t,a_t,r_t,s_{t+1})$ for $t=0,\\dots,H-1$. The policies $b(a \\mid s)$ and $\\pi(a \\mid s)$ are specified per state as probability distributions over actions.\n\n- Test Case 1 (happy path):\n    - Discount factor: $\\gamma = 0.95$.\n    - Horizon: $H=3$.\n    - Behavior policy $b$:\n        - $b(0 \\mid 0) = 0.7$, $b(1 \\mid 0) = 0.3$.\n        - $b(0 \\mid 1) = 0.4$, $b(1 \\mid 1) = 0.6$.\n        - $b(0 \\mid 2) = 0.5$, $b(1 \\mid 2) = 0.5$.\n    - Target policy $\\pi$:\n        - $\\pi(0 \\mid 0) = 0.2$, $\\pi(1 \\mid 0) = 0.8$.\n        - $\\pi(0 \\mid 1) = 0.6$, $\\pi(1 \\mid 1) = 0.4$.\n        - $\\pi(0 \\mid 2) = 0.3$, $\\pi(1 \\mid 2) = 0.7$.\n    - Episodes (each of length $H=3$):\n        1. $(0,0,1.0,1),(1,1,0.5,2),(2,1,1.2,2)$\n        2. $(0,1,1.5,2),(2,1,1.0,2),(2,0,0.7,1)$\n        3. $(1,0,0.8,0),(0,1,1.1,2),(2,1,1.0,2)$\n        4. $(2,0,0.9,1),(1,0,0.6,0),(0,1,1.4,2)$\n        5. $(1,1,0.4,2),(2,1,1.3,2),(2,0,0.5,1)$\n        6. $(0,0,1.1,1),(1,1,0.6,2),(2,1,1.0,2)$\n- Test Case 2 (boundary condition: behavior equals target policy):\n    - Discount factor: $\\gamma = 0.95$.\n    - Horizon: $H=3$.\n    - Behavior policy $b$ and target policy $\\pi$ are identical:\n        - $(0 \\mid 0) = 0.5$, $(1 \\mid 0) = 0.5$.\n        - $(0 \\mid 1) = 0.2$, $(1 \\mid 1) = 0.8$.\n        - $(0 \\mid 2) = 0.6$, $(1 \\mid 2) = 0.4$.\n    - Episodes (each of length $H=3$):\n        1. $(0,0,1.0,1),(1,1,1.2,2),(2,0,0.4,1)$\n        2. $(1,1,0.7,2),(2,0,0.6,0),(0,1,1.1,2)$\n        3. $(2,0,0.9,1),(1,1,0.8,2),(2,1,1.0,2)$\n        4. $(0,1,1.4,2),(2,0,0.5,0),(0,1,1.2,2)$\n        5. $(2,0,0.6,0),(0,1,1.0,2),(2,0,0.7,1)$\n- Test Case 3 (edge case: large importance weights without clipping):\n    - Discount factor: $\\gamma = 0.95$.\n    - Horizon: $H=4$.\n    - Behavior policy $b$:\n        - $b(0 \\mid 0) = 0.8$, $b(1 \\mid 0) = 0.2$.\n        - $b(0 \\mid 1) = 0.3$, $b(1 \\mid 1) = 0.7$.\n        - $b(0 \\mid 2) = 0.7$, $b(1 \\mid 2) = 0.3$.\n    - Target policy $\\pi$:\n        - $\\pi(0 \\mid 0) = 0.1$, $\\pi(1 \\mid 0) = 0.9$.\n        - $\\pi(0 \\mid 1) = 0.85$, $\\pi(1 \\mid 1) = 0.15$.\n        - $\\pi(0 \\mid 2) = 0.1$, $\\pi(1 \\mid 2) = 0.9$.\n    - Episodes (each of length $H=4$):\n        1. $(0,0,0.9,1),(1,1,0.3,2),(2,0,0.5,0),(0,1,1.0,2)$\n        2. $(1,1,0.6,2),(2,1,0.9,2),(2,0,0.4,1),(1,0,0.7,0)$\n        3. $(2,0,0.8,0),(0,0,0.9,1),(1,1,0.6,2),(2,1,1.2,2)$\n        4. $(0,1,1.3,2),(2,0,0.5,1),(1,0,0.6,0),(0,1,0.9,2)$\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes an inner list of three floats ordered as $[\\text{IS}, \\hat{V}^\\pi, \\text{DR}]$. The overall output must therefore look like $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}]]$, where each $x_{ij}$ is a float.\n\nAdditional constraints:\n- Off-Policy Evaluation (OPE), Importance Sampling (IS), Doubly Robust (DR), and Maximum Likelihood Estimation (MLE) acronyms must be explicitly defined on first use in your solution.\n- Angle units or physical units are not relevant here; do not include them.\n- No user input is allowed. All data is provided above and must be embedded in the solution.", "solution": "The problem requires the implementation and comparison of three estimators for Off-Policy Evaluation (OPE) in a finite-horizon Markov Decision Process (MDP). OPE is the task of estimating the expected return of a new policy, the target policy $\\pi$, using data collected under a different policy, the behavior policy $b$. We are asked to compute the value of $\\pi$ using:\n1.  A per-decision Importance Sampling (IS) estimator.\n2.  A model-based estimator using a model learned via Maximum Likelihood Estimation (MLE).\n3.  A Doubly Robust (DR) estimator, which combines the first two approaches.\n\nWe will first formally define the components and the derivation of each estimator based on the provided specifications. The state space is $S = \\{0, 1, 2\\}$, the action space is $A = \\{0, 1\\}$, and the horizon $H$ and discount factor $\\gamma$ vary per test case. Data is provided as a set of $N$ trajectories, where each trajectory $i$ is a sequence of transitions $\\tau^{(i)} = \\{(s_t^{(i)}, a_t^{(i)}, r_t^{(i)}, s_{t+1}^{(i)})\\}_{t=0}^{H-1}$.\n\n**1. Model-Based Estimation**\n\nThe model-based approach involves two stages: first, learning a model of the MDP from data, and second, using this model to compute the value of the target policy $\\pi$.\n\n**1.1. Model Learning**\nWe estimate the transition probabilities $\\hat{P}(s' \\mid s, a)$ and reward function $\\hat{r}(s, a)$ from the aggregated dataset of all trajectories.\n-   **Transition Model $\\hat{P}$**: We use Maximum Likelihood Estimation (MLE) with add-one (Laplace) smoothing. Let $N(s,a,s')$ be the number of times the transition from state $s$ to $s'$ was observed after taking action $a$. Let $N(s,a) = \\sum_{s' \\in S} N(s,a,s')$ be the total count for the state-action pair $(s,a)$. The smoothed probability is:\n    $$\n    \\hat{P}(s' \\mid s, a) = \\frac{N(s, a, s') + 1}{\\sum_{s'' \\in S} (N(s, a, s'') + 1)} = \\frac{N(s, a, s') + 1}{N(s, a) + |S|}\n    $$\n    This smoothing ensures that no transition is assigned a zero probability, which is crucial for the dynamic programming step.\n-   **Reward Model $\\hat{r}$**: The reward for a state-action pair $(s,a)$ is estimated as the empirical average of all rewards observed for that pair. Let $R(s,a)$ be the sum of rewards observed after taking action $a$ in state $s$.\n    $$\n    \\hat{r}(s, a) = \\begin{cases} \\frac{R(s, a)}{N(s, a)}  \\text{if } N(s, a)  0 \\\\ 0  \\text{if } N(s, a) = 0 \\end{cases}\n    $$\n\n**1.2. Value Computation via Dynamic Programming**\nWith the estimated model $(\\hat{P}, \\hat{r})$, we can compute the state-value function $\\hat{V}_t(s)$ and state-action value function $\\hat{Q}_t(s,a)$ for the target policy $\\pi$ using dynamic programming. The recursion proceeds backwards in time from the horizon $H$:\n-   **Boundary Condition**: At the terminal time step $t=H$, the value is zero: $\\hat{V}_H(s) = 0$ for all $s \\in S$.\n-   **Backward Recursion (for $t = H-1, \\dots, 0$)**:\n    1.  Compute the state-action value function $\\hat{Q}_t(s,a)$:\n        $$\n        \\hat{Q}_t(s, a) = \\hat{r}(s, a) + \\gamma \\sum_{s' \\in S} \\hat{P}(s' \\mid s, a) \\hat{V}_{t+1}(s')\n        $$\n    2.  Compute the state-value function $\\hat{V}_t(s)$ under the target policy $\\pi$:\n        $$\n        \\hat{V}_t(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\hat{Q}_t(s, a)\n        $$\n-   **Final Model-Based Estimate**: The overall value of the policy $\\pi$ is the expected value at time $t=0$ over the initial state distribution $d_0$. We use the empirical distribution of initial states $s_0^{(i)}$ from the dataset:\n    $$\n    d_0(s) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(s_0^{(i)} = s)\n    $$\n    where $\\mathbb{I}(\\cdot)$ is the indicator function. The model-based estimate is then:\n    $$\n    \\hat{V}^\\pi_{\\text{Model}} = \\sum_{s \\in S} d_0(s) \\hat{V}_0(s) = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}_0(s_0^{(i)})\n    $$\n\n**2. Per-Decision Importance Sampling (IS) Estimator**\n\nImportance Sampling (IS) is a technique for estimating properties of a distribution while having samples generated from a different distribution. In OPE, it corrects for the mismatch between the behavior policy $b$ and the target policy $\\pi$. The per-decision variant applies this correction at each step of a trajectory.\n\nThe importance weight for a single time step $t$ is the ratio $\\rho_t = \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}$. The cumulative importance weight for a trajectory up to time $t$ is the product of step-wise weights:\n$$\n\\rho_{0:t} = \\prod_{k=0}^{t} \\rho_k = \\prod_{k=0}^{t} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}\n$$\nThe per-decision IS estimator, $\\hat{V}^\\pi_{\\text{PDIS}}$, is the average over all trajectories of the sum of discounted rewards, where each term is re-weighted by the corresponding cumulative importance weight:\n$$\n\\hat{V}^\\pi_{\\text{PDIS}} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{H-1} \\gamma^t \\rho_{0:t}^{(i)} r_t^{(i)}\n$$\nThis estimator is unbiased but can suffer from high variance, especially when the policies $\\pi$ and $b$ are dissimilar, leading to large importance weights.\n\n**3. Doubly Robust (DR) Estimator**\n\nThe Doubly Robust (DR) estimator combines the model-based and IS estimators to leverage the strengths of both. It is \"doubly robust\" in the sense that it provides an unbiased estimate of the true value $V^\\pi$ if *either* the learned model $(\\hat{P}, \\hat{r})$ is correct *or* the importance weights are correct (i.e., the behavior policy $b$ is known).\n\nThe DR estimator uses the model-based estimates as a control variate to reduce the variance of the IS estimator. The step-wise DR estimator for a single trajectory $i$ is constructed as:\n$$\n\\hat{V}_{\\text{DR}}^{(i)} = \\hat{V}_0(s_0^{(i)}) + \\sum_{t=0}^{H-1} \\gamma^t \\rho_{0:t}^{(i)} \\delta_t^{(i)}\n$$\nwhere $\\hat{V}_0(s_0^{(i)})$ is the model-based value prediction for the initial state, and $\\delta_t^{(i)}$ is the model's one-step temporal-difference error on the observed transition, defined as:\n$$\n\\delta_t^{(i)} = r_t^{(i)} + \\gamma \\hat{V}_{t+1}(s_{t+1}^{(i)}) - \\hat{Q}_t(s_t^{(i)}, a_t^{(i)})\n$$\nHere, $r_t^{(i)}$ and $s_{t+1}^{(i)}$ are the actual observed reward and next state from the trajectory, while $\\hat{V}_{t+1}$ and $\\hat{Q}_t$ are the value functions computed via dynamic programming on the learned model. If the model is perfect, $\\mathbb{E}[\\delta_t^{(i)}] = 0$, and the estimator reduces to the model-based one. If the model is wrong, the second term corrects for the model's errors, weighted by the importance sampling ratios.\n\nThe final DR estimate is the average over all trajectories:\n$$\n\\hat{V}^\\pi_{\\text{DR}} = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}_{\\text{DR}}^{(i)}\n$$\nThis estimator typically has much lower variance than the IS estimator while retaining the property of being unbiased under weaker conditions than the model-based estimator.\n\n**Algorithmic Procedure Summary**\nFor each test case, the computation proceeds as follows:\n1.  Initialize data structures for policies, trajectories, and MDP parameters ($|S|$, $|A|$, $\\gamma$, $H$).\n2.  **Model Estimation**:\n    -   Parse the trajectory data to compute counts $N(s,a,s')$, $N(s,a)$ and reward sums $R(s,a)$.\n    -   Compute the transition model $\\hat{P}$ using Laplace smoothing and the reward model $\\hat{r}$ using empirical averages.\n3.  **Model-Based Evaluation**:\n    -   Initialize $\\hat{V}_H(s) = 0$.\n    -   Iterate $t$ from $H-1$ down to $0$ to compute all $\\hat{Q}_t$ and $\\hat{V}_t$ arrays.\n    -   Calculate $\\hat{V}^\\pi_{\\text{Model}}$ by averaging $\\hat{V}_0(s_0)$ over the initial states in the dataset.\n4.  **IS and DR Estimation**:\n    -   Initialize total sums for IS and DR estimates to zero.\n    -   For each trajectory $i$ in the dataset:\n        -   Initialize cumulative importance weight $\\rho_{prod} = 1.0$, and per-trajectory sums for IS and DR.\n        -   The DR sum for trajectory $i$ starts with the model-based value $\\hat{V}_0(s_0^{(i)})$.\n        -   Iterate $t$ from $0$ to $H-1$:\n            -   Update $\\rho_{prod} \\leftarrow \\rho_{prod} \\times \\frac{\\pi(a_t^{(i)} \\mid s_t^{(i)})}{b(a_t^{(i)} \\mid s_t^{(i)})}$.\n            -   Add $\\gamma^t \\rho_{prod} r_t^{(i)}$ to the IS sum for trajectory $i$.\n            -   Compute $\\delta_t^{(i)} = r_t^{(i)} + \\gamma \\hat{V}_{t+1}(s_{t+1}^{(i)}) - \\hat{Q}_t(s_t^{(i)}, a_t^{(i)})$.\n            -   Add $\\gamma^t \\rho_{prod} \\delta_t^{(i)}$ to the DR sum for trajectory $i$.\n        -   Add the completed per-trajectory sums to the total IS and DR sums.\n5.  **Final Estimates**:\n    -   Divide the total sums by the number of trajectories $N$ to get the final $\\hat{V}^\\pi_{\\text{PDIS}}$ and $\\hat{V}^\\pi_{\\text{DR}}$ estimates.\n6.  Collect the three estimates $[\\hat{V}^\\pi_{\\text{PDIS}}, \\hat{V}^\\pi_{\\text{Model}}, \\hat{V}^\\pi_{\\text{DR}}]$ for the current test case.\n7.  Repeat for all test cases and format the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the off-policy evaluation problem for the three given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"gamma\": 0.95,\n            \"H\": 3,\n            \"b\": np.array([[0.7, 0.3], [0.4, 0.6], [0.5, 0.5]]),\n            \"pi\": np.array([[0.2, 0.8], [0.6, 0.4], [0.3, 0.7]]),\n            \"episodes\": [\n                [(0, 0, 1.0, 1), (1, 1, 0.5, 2), (2, 1, 1.2, 2)],\n                [(0, 1, 1.5, 2), (2, 1, 1.0, 2), (2, 0, 0.7, 1)],\n                [(1, 0, 0.8, 0), (0, 1, 1.1, 2), (2, 1, 1.0, 2)],\n                [(2, 0, 0.9, 1), (1, 0, 0.6, 0), (0, 1, 1.4, 2)],\n                [(1, 1, 0.4, 2), (2, 1, 1.3, 2), (2, 0, 0.5, 1)],\n                [(0, 0, 1.1, 1), (1, 1, 0.6, 2), (2, 1, 1.0, 2)],\n            ],\n        },\n        # Test Case 2\n        {\n            \"gamma\": 0.95,\n            \"H\": 3,\n            \"b\": np.array([[0.5, 0.5], [0.2, 0.8], [0.6, 0.4]]),\n            \"pi\": np.array([[0.5, 0.5], [0.2, 0.8], [0.6, 0.4]]),\n            \"episodes\": [\n                [(0, 0, 1.0, 1), (1, 1, 1.2, 2), (2, 0, 0.4, 1)],\n                [(1, 1, 0.7, 2), (2, 0, 0.6, 0), (0, 1, 1.1, 2)],\n                [(2, 0, 0.9, 1), (1, 1, 0.8, 2), (2, 1, 1.0, 2)],\n                [(0, 1, 1.4, 2), (2, 0, 0.5, 0), (0, 1, 1.2, 2)],\n                [(2, 0, 0.6, 0), (0, 1, 1.0, 2), (2, 0, 0.7, 1)],\n            ],\n        },\n        # Test Case 3\n        {\n            \"gamma\": 0.95,\n            \"H\": 4,\n            \"b\": np.array([[0.8, 0.2], [0.3, 0.7], [0.7, 0.3]]),\n            \"pi\": np.array([[0.1, 0.9], [0.85, 0.15], [0.1, 0.9]]),\n            \"episodes\": [\n                [(0, 0, 0.9, 1), (1, 1, 0.3, 2), (2, 0, 0.5, 0), (0, 1, 1.0, 2)],\n                [(1, 1, 0.6, 2), (2, 1, 0.9, 2), (2, 0, 0.4, 1), (1, 0, 0.7, 0)],\n                [(2, 0, 0.8, 0), (0, 0, 0.9, 1), (1, 1, 0.6, 2), (2, 1, 1.2, 2)],\n                [(0, 1, 1.3, 2), (2, 0, 0.5, 1), (1, 0, 0.6, 0), (0, 1, 0.9, 2)],\n            ]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        gamma = case[\"gamma\"]\n        H = case[\"H\"]\n        b = case[\"b\"]\n        pi = case[\"pi\"]\n        episodes = case[\"episodes\"]\n        num_episodes = len(episodes)\n        \n        S_size = b.shape[0]\n        A_size = b.shape[1]\n\n        # 1. Model Estimation (MLE with Laplace smoothing)\n        transition_counts = np.zeros((S_size, A_size, S_size))\n        reward_sums = np.zeros((S_size, A_size))\n        sa_counts = np.zeros((S_size, A_size))\n\n        for ep in episodes:\n            for t, (s, a, r, s_next) in enumerate(ep):\n                transition_counts[s, a, s_next] += 1\n                reward_sums[s, a] += r\n                sa_counts[s, a] += 1\n        \n        P_hat = np.zeros((S_size, A_size, S_size))\n        r_hat = np.zeros((S_size, A_size))\n\n        for s in range(S_size):\n            for a in range(A_size):\n                if sa_counts[s, a] > 0:\n                    r_hat[s, a] = reward_sums[s, a] / sa_counts[s, a]\n                    # Laplace smoothing for P_hat\n                    P_hat[s, a, :] = (transition_counts[s, a, :] + 1) / (sa_counts[s, a] + S_size)\n                else:\n                    # If (s,a) is not observed, r_hat is 0 and P_hat is uniform (from smoothing)\n                    r_hat[s, a] = 0.0\n                    P_hat[s, a, :] = 1.0 / S_size\n\n        # 2. Model-based value estimation (Dynamic Programming)\n        V_hat = np.zeros((H + 1, S_size))\n        Q_hat = np.zeros((H, S_size, A_size))\n\n        for t in range(H - 1, -1, -1):\n            V_next = V_hat[t + 1, :]\n            expected_V_next = np.sum(P_hat * V_next, axis=2) # Shape: (S_size, A_size)\n            Q_hat[t, :, :] = r_hat + gamma * expected_V_next\n            V_hat[t, :] = np.sum(pi * Q_hat[t, :, :], axis=1)\n\n        initial_states = [ep[0][0] for ep in episodes]\n        V_model_based = np.mean([V_hat[0, s0] for s0 in initial_states])\n        \n        # 3. IS and DR estimation\n        total_is_return = 0.0\n        total_dr_return = 0.0\n        \n        for i in range(num_episodes):\n            ep = episodes[i]\n            s0 = ep[0][0]\n            \n            # Per-decision IS\n            episode_is_return = 0.0\n            rho_product = 1.0\n            for t in range(H):\n                s, a, r, s_next = ep[t]\n                rho_t = pi[s, a] / b[s, a]\n                rho_product *= rho_t\n                episode_is_return += (gamma**t) * rho_product * r\n            total_is_return += episode_is_return\n\n            # Doubly Robust\n            episode_dr_return = V_hat[0, s0]\n            rho_product = 1.0\n            for t in range(H):\n                s, a, r, s_next = ep[t]\n                rho_t = pi[s, a] / b[s, a]\n                rho_product *= rho_t\n                \n                delta_t = r + gamma * V_hat[t + 1, s_next] - Q_hat[t, s, a]\n                episode_dr_return += (gamma**t) * rho_product * delta_t\n            total_dr_return += episode_dr_return\n            \n        V_is = total_is_return / num_episodes\n        V_dr = total_dr_return / num_episodes\n        \n        all_results.append([V_is, V_model_based, V_dr])\n\n    # Format output as a list of lists of floats\n    print(f\"{all_results}\")\n\nsolve()\n```", "id": "3145244"}, {"introduction": "Many reinforcement learning algorithms, including the celebrated Q-learning, can suffer from a subtle but significant issue known as maximization bias, which leads to a systematic overestimation of action values. This theoretical practice guides you through a first-principles derivation to quantify this bias in a simplified setting. By analytically comparing the bias of standard Q-learning with that of Double Q-learning, you will gain a deep and rigorous understanding of how decoupling action selection from value estimation provides a powerful solution to this problem [@problem_id:3145285].", "problem": "Consider a Markov Decision Process (MDP) with a single nonterminal state $s$ and two available actions $a_{1}$ and $a_{2}$. The true action values are $q^{\\ast}(s,a_{1})=\\mu_{1}$ and $q^{\\ast}(s,a_{2})=\\mu_{2}$, with $\\mu_{1}\\geq \\mu_{2}$. Assume a tabular agent performing a one-step temporal difference bootstrap in which the only source of error in the target arises from the use of a greedy operator on noisy, unbiased value estimates. Specifically, the agent’s current estimates are $\\hat{q}(s,a_{i})=\\mu_{i}+\\varepsilon_{i}$ where $\\varepsilon_{1}$ and $\\varepsilon_{2}$ are independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$. There are no additional rewards or discounts involved in the target for this step; the focus is solely on the bias induced by the greedy maximum over noisy estimates.\n\n1. Using the definition of the Q-learning target $T_{Q}(s)=\\max\\{\\hat{q}(s,a_{1}),\\hat{q}(s,a_{2})\\}$, derive from first principles an analytic expression for the expected bias $b_{Q}=\\mathbb{E}[T_{Q}(s)]-\\mu_{1}$ in terms of $\\Delta=\\mu_{1}-\\mu_{2}$ and $\\sigma$.\n\n2. Now consider Double Q-learning (DQL), which maintains two independent unbiased estimators $Q^{A}$ and $Q^{B}$ with the same noise model: $\\hat{q}^{A}(s,a_{i})=\\mu_{i}+\\varepsilon^{A}_{i}$ and $\\hat{q}^{B}(s,a_{i})=\\mu_{i}+\\varepsilon^{B}_{i}$, where all noises are independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$. The Double Q-learning target is $T_{DQ}(s)=\\hat{q}^{B}(s,\\arg\\max_{a}\\hat{q}^{A}(s,a))$. Derive an analytic expression for the expected bias $b_{DQ}=\\mathbb{E}[T_{DQ}(s)]-\\mu_{1}$ in terms of $\\Delta$ and $\\sigma$.\n\n3. Quantify the bias reduction due to Double Q-learning by computing $\\Delta b=b_{Q}-b_{DQ}$, and present it as a single closed-form analytic expression in terms of $\\Delta$, $\\sigma$, and the standard normal cumulative distribution function $\\Phi$. Your final answer must be this single expression. Define $\\Phi(x)$ as the cumulative distribution function of the standard normal distribution. Do not provide a numerical approximation.", "solution": "The problem is well-posed and scientifically grounded, representing a standard theoretical analysis of maximization bias in reinforcement learning. We will proceed with the derivation in three parts as requested.\n\nThe setup involves a single state $s$ and two actions $a_1, a_2$ with true values $q^{\\ast}(s,a_{1})=\\mu_{1}$ and $q^{\\ast}(s,a_{2})=\\mu_{2}$, where $\\mu_{1}\\geq \\mu_{2}$. The agent's value estimates are noisy and unbiased. For a generic action $a_i$, the estimate is $\\hat{q}(s,a_{i})=\\mu_{i}+\\varepsilon_{i}$, where $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent and identically distributed (i.i.d.) random variables. We define $\\Delta = \\mu_{1}-\\mu_{2} \\geq 0$. We denote the probability density function (PDF) and cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$ by $\\phi(x)$ and $\\Phi(x)$ respectively.\n\nPart 1: Expected Bias of Q-learning, $b_{Q}$\n\nThe Q-learning target is given by $T_{Q}(s)=\\max\\{\\hat{q}(s,a_{1}),\\hat{q}(s,a_{2})\\}$.\nThe expected bias is defined as $b_{Q}=\\mathbb{E}[T_{Q}(s)]-\\mu_{1}$, where $\\mu_1$ is the value of the true optimal action.\n$b_{Q} = \\mathbb{E}[\\max\\{\\mu_{1}+\\varepsilon_{1}, \\mu_{2}+\\varepsilon_{2}\\}] - \\mu_{1}$.\nWe use the identity $\\max(x,y) = x + \\max(0, y-x)$.\n$\\mathbb{E}[\\max\\{\\mu_{1}+\\varepsilon_{1}, \\mu_{2}+\\varepsilon_{2}\\}] = \\mathbb{E}[\\mu_{1}+\\varepsilon_{1} + \\max\\{0, (\\mu_{2}+\\varepsilon_{2}) - (\\mu_{1}+\\varepsilon_{1})\\}]$.\nBy linearity of expectation, and since $\\mathbb{E}[\\varepsilon_{1}]=0$:\n$\\mathbb{E}[T_{Q}(s)] = \\mu_{1} + \\mathbb{E}[\\max\\{0, (\\mu_{2}-\\mu_{1}) + (\\varepsilon_{2}-\\varepsilon_{1})\\}]$.\nThe bias is therefore:\n$b_{Q} = \\mathbb{E}[\\max\\{0, -\\Delta + (\\varepsilon_{2}-\\varepsilon_{1})\\}]$.\nLet a new random variable be $W = \\varepsilon_{2}-\\varepsilon_{1}$. Since $\\varepsilon_1$ and $\\varepsilon_2$ are i.i.d. $\\mathcal{N}(0, \\sigma^2)$, $W$ is also normally distributed.\n$\\mathbb{E}[W] = \\mathbb{E}[\\varepsilon_{2}] - \\mathbb{E}[\\varepsilon_{1}] = 0 - 0 = 0$.\n$\\text{Var}(W) = \\text{Var}(\\varepsilon_{2}) + \\text{Var}(\\varepsilon_{1}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2$, due to independence.\nSo, $W \\sim \\mathcal{N}(0, 2\\sigma^2)$.\nLet $Z = W - \\Delta$. Then $Z \\sim \\mathcal{N}(-\\Delta, 2\\sigma^2)$. The bias is $b_Q = \\mathbb{E}[\\max\\{0, Z\\}]$.\nThis is the expectation of a right-censored normal variable. For a random variable $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$, the formula is $\\mathbb{E}[\\max\\{0,Y\\}] = \\mu_Y \\Phi(\\frac{\\mu_Y}{\\sigma_Y}) + \\sigma_Y \\phi(\\frac{\\mu_Y}{\\sigma_Y})$.\nIn our case, $\\mu_Y = -\\Delta$ and $\\sigma_Y = \\sqrt{2\\sigma^2} = \\sigma\\sqrt{2}$.\nSubstituting these into the formula:\n$b_{Q} = (-\\Delta) \\Phi\\left(\\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right) + \\sigma\\sqrt{2} \\phi\\left(\\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right)$.\nUsing the identities $\\Phi(-x) = 1-\\Phi(x)$ and $\\phi(-x) = \\phi(x)$:\n$b_{Q} = -\\Delta(1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})) + \\sigma\\sqrt{2}\\phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$.\n\nPart 2: Expected Bias of Double Q-learning, $b_{DQ}$\n\nDouble Q-learning uses two independent estimators, $Q^A$ and $Q^B$, with estimates $\\hat{q}^{A}(s,a_{i})=\\mu_{i}+\\varepsilon^{A}_{i}$ and $\\hat{q}^{B}(s,a_{i})=\\mu_{i}+\\varepsilon^{B}_{i}$, where all noise terms $\\varepsilon^{A}_i, \\varepsilon^{B}_i$ are i.i.d. $\\mathcal{N}(0,\\sigma^{2})$.\nThe target is $T_{DQ}(s)=\\hat{q}^{B}(s,a^{\\ast})$, where $a^{\\ast} = \\arg\\max_{a}\\hat{q}^{A}(s,a)$.\nThe expected bias is $b_{DQ} = \\mathbb{E}[T_{DQ}(s)] - \\mu_1$.\nWe compute the expectation by conditioning on the action choice, which depends only on the $A$ estimates.\n$\\mathbb{E}[T_{DQ}(s)] = \\mathbb{E}_{A,B}[\\hat{q}^{B}(s, a^{\\ast})] = \\mathbb{E}_{A}[\\mathbb{E}_{B}[\\hat{q}^{B}(s, a^{\\ast}) | a^{\\ast}]]$.\nSince $a^{\\ast}$ is determined by $\\varepsilon^A$ noises, it is independent of the $\\varepsilon^B$ noises. The inner expectation over $B$ yields the true mean of the selected action's $B$ estimate:\n$\\mathbb{E}_{B}[\\hat{q}^{B}(s, a^{\\ast}) | a^{\\ast}] = \\mu_{a^{\\ast}}$.\nSo, $\\mathbb{E}[T_{DQ}(s)] = \\mathbb{E}_{A}[\\mu_{a^{\\ast}}]$.\nThe expectation is over the choice of $a^{\\ast}$:\n$\\mathbb{E}_{A}[\\mu_{a^{\\ast}}] = P(a^{\\ast}=a_1)\\mu_1 + P(a^{\\ast}=a_2)\\mu_2$.\nLet's find the probability $P(a^{\\ast}=a_1)$.\n$P(a^{\\ast}=a_1) = P(\\hat{q}^{A}(s,a_1) \\geq \\hat{q}^{A}(s,a_2)) = P(\\mu_1+\\varepsilon^{A}_1 \\geq \\mu_2+\\varepsilon^{A}_2)$.\n$P(a^{\\ast}=a_1) = P(\\varepsilon^{A}_1 - \\varepsilon^{A}_2 \\geq \\mu_2 - \\mu_1) = P(\\varepsilon^{A}_1 - \\varepsilon^{A}_2 \\geq -\\Delta)$.\nThe random variable $W' = \\varepsilon^{A}_1 - \\varepsilon^{A}_2$ is distributed as $\\mathcal{N}(0, 2\\sigma^2)$.\n$P(W' \\geq -\\Delta) = P\\left(\\frac{W'}{\\sigma\\sqrt{2}} \\geq \\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right)$. Let $U = W'/(\\sigma\\sqrt{2}) \\sim \\mathcal{N}(0,1)$.\n$P(a^{\\ast}=a_1) = P(U \\geq \\frac{-\\Delta}{\\sigma\\sqrt{2}}) = 1 - \\Phi(\\frac{-\\Delta}{\\sigma\\sqrt{2}}) = \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$.\nAnd $P(a^{\\ast}=a_2) = 1 - P(a^{\\ast}=a_1) = 1 - \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$.\nNow, substitute these probabilities back into the expression for $\\mathbb{E}[T_{DQ}(s)]$:\n$\\mathbb{E}[T_{DQ}(s)] = \\mu_1 \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}) + \\mu_2 (1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}))$.\nThe bias is:\n$b_{DQ} = \\mathbb{E}[T_{DQ}(s)] - \\mu_1 = \\mu_1 \\Phi(\\dots) + \\mu_2 (1-\\Phi(\\dots)) - \\mu_1$.\n$b_{DQ} = (\\Phi(\\dots) - 1)\\mu_1 + (1-\\Phi(\\dots))\\mu_2 = (1-\\Phi(\\dots))(\\mu_2-\\mu_1) = -(1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}))\\Delta$.\n\nPart 3: Bias Reduction, $\\Delta b$\n\nThe bias reduction is the difference $\\Delta b = b_{Q} - b_{DQ}$.\n$b_{Q} = -\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right) + \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)$.\n$b_{DQ} = -\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right)$.\nSubtracting $b_{DQ}$ from $b_{Q}$:\n$\\Delta b = \\left[-\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right) + \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right] - \\left[-\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right)\\right]$.\nThe terms involving $\\Delta$ and $\\Phi$ cancel out perfectly.\n$\\Delta b = \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)$.\nThe problem requests the final answer in terms of $\\Delta$, $\\sigma$, and $\\Phi$. However, the rigorous derivation shows that the CDF terms $\\Phi$ cancel, leaving an expression dependent on the PDF $\\phi$. To present a self-contained closed-form expression, we substitute the definition of the standard normal PDF, $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$.\n$\\Delta b = \\sigma\\sqrt{2} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)^2\\right)$.\n$\\Delta b = \\frac{\\sigma\\sqrt{2}}{\\sqrt{2}\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{2 \\cdot 2\\sigma^2}\\right)$.\n$\\Delta b = \\frac{\\sigma}{\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{4\\sigma^2}\\right)$.\nThis is the final analytical expression for the bias reduction. It is a function of $\\Delta$ and $\\sigma$, but not $\\Phi$, due to the exact cancellation. This is the only correct final expression.", "answer": "$$\\boxed{\\frac{\\sigma}{\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{4\\sigma^2}\\right)}$$", "id": "3145285"}]}