{"hands_on_practices": [{"introduction": "Before we can effectively learn a value function, we must understand its fundamental properties. This exercise explores the crucial relationship between rewards and optimal policies, a concept central to the practice of reward shaping. By analyzing how different transformations of the reward function $r$ affect the final outcome, you will discover which changes preserve the optimal policy and, through a concrete example, see how seemingly innocent modifications can lead to entirely different learned behaviors [@problem_id:3163629].", "problem": "Consider an infinite-horizon discounted Markov Decision Process (MDP) with state set $\\{s_0,s_1,s_2\\}$ and discount factor $\\gamma = 0.9$. State $s_0$ has two available actions, $a_L$ and $a_R$, with deterministic transitions and immediate rewards given by:\n- Taking $a_L$ in $s_0$ yields immediate reward $r(s_0,a_L) = 0$ and transitions to $s_1$.\n- Taking $a_R$ in $s_0$ yields immediate reward $r(s_0,a_R) = 1$ and transitions to $s_2$.\n\nStates $s_1$ and $s_2$ are terminal in the sense that each has a single dummy action that leads to an absorbing terminal condition thereafter:\n- In $s_1$, the only action yields immediate reward $r(s_1,\\text{dummy}) = 5$ and then transitions to a terminal absorbing condition with no further rewards.\n- In $s_2$, the only action yields immediate reward $r(s_2,\\text{dummy}) = 0$ and then transitions to a terminal absorbing condition with no further rewards.\n\nDefine the optimal action-value function $Q^\\star(s,a)$ in the usual sense as the supremum of the expected discounted return starting from state-action pair $(s,a)$ when following an optimal policy thereafter. Now define a per-state reward normalization that maps the original reward function $r$ to a new function $r_{\\text{norm}}$ via\n$$\nr_{\\text{norm}}(s,a) \\;=\\; r(s,a) \\;-\\; \\mu(s), \\quad \\text{where } \\mu(s) \\equiv \\frac{1}{|\\mathcal{A}(s)|} \\sum_{a' \\in \\mathcal{A}(s)} r(s,a')\n$$\nand $\\mathcal{A}(s)$ is the available action set in state $s$. Let $Q^\\star_{\\text{norm}}$ be the optimal action-value function under $r_{\\text{norm}}$ with the same discount factor $\\gamma$.\n\nUsing only the definitions of return, optimality, and the Bellman optimality principle, reason about the effect of this per-state normalization on the action ranking at $s_0$. Then, consider the tabular Q-learning update\n$$\nQ_{t+1}(s_t,a_t) \\;=\\; Q_t(s_t,a_t) \\;+\\; \\alpha_t \\Big( r_t \\;+\\; \\gamma \\max_{a'} Q_t(s_{t+1},a') \\;-\\; Q_t(s_t,a_t) \\Big),\n$$\nwhere $\\alpha_t \\in (0,1]$ is a learning rate, and discuss invariances of this update to affine transformations of the reward $r$.\n\nSelect all statements that are correct:\n\nA. In the MDP described above, replacing $r$ by the per-state centered reward $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$ reverses the ranking of $Q^\\star(s_0,a_L)$ and $Q^\\star(s_0,a_R)$ compared to the original reward.\n\nB. For any MDP and any $\\gamma \\in (0,1)$, replacing $r$ by $r'(s,a,s') = r(s,a,s') + c$ with a constant $c \\in \\mathbb{R}$ leaves the set of optimal greedy actions at every state unchanged.\n\nC. For any MDP and any $\\gamma \\in (0,1)$, replacing $r$ by $r'(s,a,s') = a\\, r(s,a,s')$ with a scalar $a  0$ preserves the set of optimal greedy actions at every state.\n\nD. For any MDP and any $\\gamma \\in (0,1)$, replacing $r$ by $r'(s,a,s') = r(s,a,s') + b(s)$ with an arbitrary function $b(s)$ depending only on the current state $s$ preserves the set of optimal greedy actions at every state.\n\nE. In tabular Q-learning with arbitrary initialization $Q_0$, replacing rewards by $r'(s,a,s') = r(s,a,s') + c$ with constant $c$ makes the entire trajectory of greedy policies over time $t$ strictly identical to that under $r$ (same action choices at every time for every visited state), regardless of $Q_0$.\n\nF. Let $r'(s,a,s') = a\\, r(s,a,s') + b$ with $a  0$ and $b \\in \\mathbb{R}$. If one initializes $Q'_0(s,a) = a\\, Q_0(s,a) + \\tfrac{b}{1-\\gamma}$ and runs tabular Q-learning on $r'$ while simultaneously running tabular Q-learning on $r$ from $Q_0$, then for every time $t$ and every state-action pair $(s,a)$, one has $Q'_t(s,a) = a\\, Q_t(s,a) + \\tfrac{b}{1-\\gamma}$, hence the greedy policies along the two learning trajectories coincide at every time.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n- An infinite-horizon discounted Markov Decision Process (MDP) is considered.\n- State set: $\\{s_0, s_1, s_2\\}$.\n- Discount factor: $\\gamma = 0.9$.\n- Actions and deterministic transitions/rewards from state $s_0$:\n  - Action $a_L$: transitions to $s_1$, immediate reward $r(s_0, a_L) = 0$.\n  - Action $a_R$: transitions to $s_2$, immediate reward $r(s_0, a_R) = 1$.\n- State $s_1$: a single dummy action yields immediate reward $r(s_1, \\text{dummy}) = 5$ and transitions to a terminal absorbing state.\n- State $s_2$: a single dummy action yields immediate reward $r(s_2, \\text{dummy}) = 0$ and transitions to a terminal absorbing state.\n- Optimal action-value function: $Q^\\star(s,a)$ is defined as the supremum of the expected discounted return.\n- Per-state reward normalization: $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$, where $\\mu(s) = \\frac{1}{|\\mathcal{A}(s)|} \\sum_{a' \\in \\mathcal{A}(s)} r(s,a')$.\n- Tabular Q-learning update rule: $Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \\alpha_t ( r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a') - Q_t(s_t,a_t) )$ with learning rate $\\alpha_t \\in (0,1]$.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientifically Grounded:** The problem is formulated within the standard mathematical framework of MDPs and reinforcement learning. All terms and concepts are standard and well-defined.\n- **Well-Posed:** The MDP is fully specified with deterministic transitions and rewards. The questions posed are well-defined mathematical inquiries about the properties of value functions and learning algorithms. A unique solution exists for the quantities to be computed.\n- **Objective:** The problem is stated in precise, formal language, free of ambiguity or subjective content.\n- **Consistency and Completeness:** The problem provides all necessary information and contains no internal contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution proceeds with the analysis of each option.\n\n### Solution and Option Analysis\n\nThe optimal action-value function $Q^\\star(s, a)$ satisfies the Bellman optimality equation. For a deterministic transition $s \\xrightarrow{a} s'$, this is:\n$$Q^\\star(s,a) = r(s,a) + \\gamma \\max_{a'} Q^\\star(s', a') = r(s,a) + \\gamma V^\\star(s')$$\nwhere $V^\\star(s) = \\max_a Q^\\star(s,a)$ is the optimal state-value function.\n\n**A. Analysis of the specific MDP with reward normalization**\n\nFirst, we compute the optimal Q-values for the original reward function $r$.\nStates $s_1$ and $s_2$ are effectively terminal. An action there gives a final reward and the process ends (transitions to an absorbing state with $V=0$).\nFor state $s_1$:\n$$Q^\\star(s_1, \\text{dummy}) = r(s_1, \\text{dummy}) + \\gamma \\cdot 0 = 5$$\n$$V^\\star(s_1) = 5$$\nFor state $s_2$:\n$$Q^\\star(s_2, \\text{dummy}) = r(s_2, \\text{dummy}) + \\gamma \\cdot 0 = 0$$\n$$V^\\star(s_2) = 0$$\nNow, for state $s_0$:\n$$Q^\\star(s_0, a_L) = r(s_0, a_L) + \\gamma V^\\star(s_1) = 0 + 0.9 \\cdot 5 = 4.5$$\n$$Q^\\star(s_0, a_R) = r(s_0, a_R) + \\gamma V^\\star(s_2) = 1 + 0.9 \\cdot 0 = 1$$\nThe ranking is $Q^\\star(s_0, a_L)  Q^\\star(s_0, a_R)$, so the optimal action is $a_L$.\n\nNext, we compute the normalized rewards $r_{\\text{norm}}$.\nFor $s_0$, $\\mathcal{A}(s_0)=\\{a_L, a_R\\}$, so $|\\mathcal{A}(s_0)|=2$.\n$$\\mu(s_0) = \\frac{1}{2}(r(s_0, a_L) + r(s_0, a_R)) = \\frac{1}{2}(0+1) = 0.5$$\n$$r_{\\text{norm}}(s_0, a_L) = r(s_0, a_L) - \\mu(s_0) = 0 - 0.5 = -0.5$$\n$$r_{\\text{norm}}(s_0, a_R) = r(s_0, a_R) - \\mu(s_0) = 1 - 0.5 = 0.5$$\nFor $s_1$, $\\mathcal{A}(s_1)=\\{\\text{dummy}\\}$, so $|\\mathcal{A}(s_1)|=1$.\n$$\\mu(s_1) = r(s_1, \\text{dummy}) = 5$$\n$$r_{\\text{norm}}(s_1, \\text{dummy}) = 5 - 5 = 0$$\nFor $s_2$, $\\mathcal{A}(s_2)=\\{\\text{dummy}\\}$, so $|\\mathcal{A}(s_2)|=1$.\n$$\\mu(s_2) = r(s_2, \\text{dummy}) = 0$$\n$$r_{\\text{norm}}(s_2, \\text{dummy}) = 0 - 0 = 0$$\n\nNow we compute the optimal Q-values, $Q^\\star_{\\text{norm}}$, for $r_{\\text{norm}}$.\nFor $s_1$: $Q^\\star_{\\text{norm}}(s_1, \\text{dummy}) = r_{\\text{norm}}(s_1, \\text{dummy}) = 0$, so $V^\\star_{\\text{norm}}(s_1) = 0$.\nFor $s_2$: $Q^\\star_{\\text{norm}}(s_2, \\text{dummy}) = r_{\\text{norm}}(s_2, \\text{dummy}) = 0$, so $V^\\star_{\\text{norm}}(s_2) = 0$.\nFor $s_0$:\n$$Q^\\star_{\\text{norm}}(s_0, a_L) = r_{\\text{norm}}(s_0, a_L) + \\gamma V^\\star_{\\text{norm}}(s_1) = -0.5 + 0.9 \\cdot 0 = -0.5$$\n$$Q^\\star_{\\text{norm}}(s_0, a_R) = r_{\\text{norm}}(s_0, a_R) + \\gamma V^\\star_{\\text{norm}}(s_2) = 0.5 + 0.9 \\cdot 0 = 0.5$$\nThe new ranking is $Q^\\star_{\\text{norm}}(s_0, a_R)  Q^\\star_{\\text{norm}}(s_0, a_L)$, so the optimal action is $a_R$.\nThe ranking of actions at $s_0$ has been reversed.\n\nVerdict for A: **Correct**.\n\n**B. Invariance to a constant reward shift**\n\nLet $r'(s,a,s') = r(s,a,s') + c$. Let $Q^\\star$ and $Q'^\\star$ be the respective optimal action-value functions.\nWe can show by induction on the steps of value iteration or by direct verification that $Q'^\\star(s,a) = Q^\\star(s,a) + \\frac{c}{1-\\gamma}$.\nAssume this relation holds. The Bellman equation for $Q'^\\star$ is:\n$$Q'^\\star(s,a) = \\mathbb{E}_{s'}[r(s,a,s') + c + \\gamma \\max_{a'} Q'^\\star(s', a')]$$\nSubstitute our hypothesized relation:\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = \\mathbb{E}_{s'}[r(s,a,s') + c + \\gamma \\max_{a'} (Q^\\star(s', a') + \\frac{c}{1-\\gamma})]$$\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = \\mathbb{E}_{s'}[r(s,a,s')] + c + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^\\star(s', a')] + \\gamma \\frac{c}{1-\\gamma}$$\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = (\\mathbb{E}_{s'}[r(s,a,s')] + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^\\star(s', a')]) + (c + \\frac{\\gamma c}{1-\\gamma})$$\nThe first parenthesis is $Q^\\star(s,a)$. The second is $\\frac{c(1-\\gamma)+\\gamma c}{1-\\gamma} = \\frac{c}{1-\\gamma}$. The equation holds.\nThe optimal policy in state $s$ is given by $\\arg\\max_a Q^\\star(s,a)$. For the new rewards, it's $\\arg\\max_a Q'^\\star(s,a)$.\n$$\\arg\\max_a Q'^\\star(s,a) = \\arg\\max_a \\left( Q^\\star(s,a) + \\frac{c}{1-\\gamma} \\right) = \\arg\\max_a Q^\\star(s,a)$$\nSince $\\frac{c}{1-\\gamma}$ is a constant with respect to action $a$, it does not affect the argmax. The set of optimal actions is unchanged.\n\nVerdict for B: **Correct**.\n\n**C. Invariance to a positive scaling of rewards**\n\nLet $r'(s,a,s') = a\\,r(s,a,s')$ with $a  0$. Let $Q^\\star$ and $Q'^\\star$ be the respective optimal action-value functions.\nWe claim $Q'^\\star(s,a) = a\\,Q^\\star(s,a)$. We verify this using the Bellman equation.\n$$Q'^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma \\max_{a'} Q'^\\star(s', a')]$$\n$$a\\,Q^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma \\max_{a'} (a\\,Q^\\star(s', a'))]$$\nBecause $a0$, $\\max_{a'} (a\\,X(a')) = a\\,\\max_{a'} X(a')$.\n$$a\\,Q^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma a \\max_{a'} Q^\\star(s', a')]$$\n$$a\\,Q^\\star(s,a) = a \\left( \\mathbb{E}_{s'}[r(s,a,s') + \\gamma \\max_{a'} Q^\\star(s', a')] \\right)$$\nDividing by $a$ recovers the Bellman equation for $Q^\\star$. The relation holds.\nThe new policy is determined by:\n$$\\arg\\max_a Q'^\\star(s,a) = \\arg\\max_a (a\\,Q^\\star(s,a))$$\nSince $a  0$, maximizing $a\\,Q^\\star(s,a)$ is equivalent to maximizing $Q^\\star(s,a)$. The set of optimal actions is unchanged.\n\nVerdict for C: **Correct**.\n\n**D. Invariance to a state-dependent reward shift**\n\nLet $r'(s,a,s') = r(s,a,s') + b(s)$. This is a general form of reward shaping. For the optimal policy to be invariant, the shaping must be of the \"potential-based\" form $F(s,s') = \\gamma\\Phi(s') - \\Phi(s)$, where $\\Phi$ is some real-valued function on states. A shaping term $b(s)$ that depends only on the source state does not, in general, have this form.\nWe can use the result from option A as a direct counter-example. In that case, the reward transformation was $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$. This is an instance of the transformation in this option, with $b(s) = -\\mu(s)$. As we calculated, the optimal policy at state $s_0$ changed from preferring $a_L$ to preferring $a_R$. Therefore, a state-dependent reward shift does not generally preserve the set of optimal greedy actions.\n\nVerdict for D: **Incorrect**.\n\n**E. Invariance of Q-learning trajectory under a constant reward shift**\n\nThe statement claims that with $r' = r+c$ and an arbitrary initialization $Q_0$ (implying $Q'_0 = Q_0$), the sequence of greedy policies is identical.\nLet's trace the first update. Assume at $t=0$, we are in state $s_0$ and the greedy action is $a_0 = \\arg\\max_a Q_0(s_0,a)$. The same action is chosen for both systems. After observing transition to $s_1$ and reward $r_0$ (or $r'_0 = r_0+c$), the updates are:\n$$Q_1(s_0, a_0) = (1-\\alpha_0)Q_0(s_0,a_0) + \\alpha_0(r_0 + \\gamma \\max_{a'} Q_0(s_1,a'))$$\n$$Q'_1(s_0, a_0) = (1-\\alpha_0)Q'_0(s_0,a_0) + \\alpha_0(r'_0 + \\gamma \\max_{a'} Q'_0(s_1,a'))$$\nWith $Q'_0 = Q_0$ and $r'_0=r_0+c$:\n$$Q'_1(s_0, a_0) = (1-\\alpha_0)Q_0(s_0,a_0) + \\alpha_0(r_0+c + \\gamma \\max_{a'} Q_0(s_1,a'))$$\n$$Q'_1(s_0, a_0) = Q_1(s_0, a_0) + \\alpha_0 c$$\nFor any other state-action pair $(s,a) \\ne (s_0, a_0)$, no update occurs, so $Q_1(s,a) = Q_0(s,a)$ and $Q'_1(s,a) = Q_0(s,a)$.\nNow consider the greedy policy at $s_0$ for time $t=1$. It is $\\arg\\max_a Q_1(s_0,a)$ versus $\\arg\\max_a Q'_1(s_0,a)$.\nThe Q-values for actions other than $a_0$ are unchanged from $Q_0$, whereas $Q(s_0,a_0)$ has been updated. The update for $Q'$ includes an extra term $\\alpha_0 c$.\nThis can change the action ranking. Let $Q_0(s_0, a_0) = 10, Q_0(s_0, a_1)=9.9$. Action $a_0$ is chosen. Let the TD target be low, e.g., $r_0 + \\gamma \\max Q_0(s_1, \\cdot) = 5$, and $\\alpha_0=0.1$.\n$Q_1(s_0, a_0) = (1-0.1) \\cdot 10 + 0.1 \\cdot 5 = 9.5$. The new policy at $s_0$ prefers $a_1$, since $Q_1(s_0, a_1)=9.9  9.5$.\nFor $Q'$, let $c=20$. The TD target is $5+20=25$.\n$Q'_1(s_0, a_0) = (1-0.1) \\cdot 10 + 0.1 \\cdot 25 = 9 + 2.5 = 11.5$. The new policy at $s_0$ still prefers $a_0$, since $Q'_1(s_0,a_0)=11.5  Q'_1(s_0,a_1)=9.9$.\nThe greedy policies diverge.\n\nVerdict for E: **Incorrect**.\n\n**F. Invariance of Q-learning trajectory under affine transform with special initialization**\n\nThe reward transformation is $r' = a\\,r + b$ ($a0$) and the initialization is $Q'_0(s,a) = a\\,Q_0(s,a) + \\frac{b}{1-\\gamma}$. The claim is that $Q'_t(s,a) = a\\,Q_t(s,a) + \\frac{b}{1-\\gamma}$ for all $t$, and policies coincide.\nIf this Q-value relation holds for all $t$, then the greedy policies are identical:\n$\\arg\\max_{a'} Q'_t(s, a') = \\arg\\max_{a'} (a\\,Q_t(s,a') + \\frac{b}{1-\\gamma}) = \\arg\\max_{a'} Q_t(s,a')$.\nThis means both processes follow the same $(s_t, a_t)$ trajectory.\nWe prove the Q-value relation by induction.\nBase Case ($t=0$): The relation holds by definition of the initialization.\nInductive Step: Assume $Q'_t(s,a) = a\\,Q_t(s,a) + \\frac{b}{1-\\gamma}$ for all $(s,a)$. Consider the update at step $t$ for $(s_t, a_t)$.\n$$Q'_{t+1}(s_t,a_t) = (1-\\alpha_t)Q'_t(s_t,a_t) + \\alpha_t \\left( r'_t + \\gamma \\max_{a'} Q'_t(s_{t+1},a') \\right)$$\nSubstitute the inductive hypothesis and $r'_t=ar_t+b$:\n$$Q'_{t+1}(s_t,a_t) = (1-\\alpha_t)\\left(a\\,Q_t(s_t,a_t) + \\frac{b}{1-\\gamma}\\right) + \\alpha_t \\left( ar_t+b + \\gamma \\max_{a'} \\left(a\\,Q_t(s_{t+1},a') + \\frac{b}{1-\\gamma}\\right) \\right)$$\nSince $a0$, the $\\max$ term simplifies:\n$$ = (1-\\alpha_t)\\left(a\\,Q_t + \\frac{b}{1-\\gamma}\\right) + \\alpha_t \\left( ar_t+b + \\gamma \\left(a\\max_{a'}Q_t + \\frac{b}{1-\\gamma}\\right) \\right)$$\nGroup terms multiplied by $a$:\n$$a \\left[ (1-\\alpha_t)Q_t(s_t,a_t) + \\alpha_t \\left( r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a') \\right) \\right] = a\\,Q_{t+1}(s_t,a_t)$$\nGroup terms involving $b$:\n$$ (1-\\alpha_t)\\frac{b}{1-\\gamma} + \\alpha_t b + \\alpha_t \\gamma \\frac{b}{1-\\gamma} = b \\left[ \\frac{1-\\alpha_t}{1-\\gamma} + \\alpha_t + \\frac{\\alpha_t \\gamma}{1-\\gamma} \\right]$$\n$$ = b \\left[ \\frac{1-\\alpha_t + \\alpha_t(1-\\gamma) + \\alpha_t\\gamma}{1-\\gamma} \\right] = b \\left[ \\frac{1-\\alpha_t + \\alpha_t - \\alpha_t\\gamma + \\alpha_t\\gamma}{1-\\gamma} \\right] = b \\frac{1}{1-\\gamma}$$\nCombining these, we get $Q'_{t+1}(s_t,a_t) = a\\,Q_{t+1}(s_t,a_t) + \\frac{b}{1-\\gamma}$.\nFor any $(s,a) \\neq (s_t, a_t)$, the values are unchanged, so the relation also holds. The induction is complete.\n\nVerdict for F: **Correct**.", "answer": "$$\\boxed{ABCF}$$", "id": "3163629"}, {"introduction": "Theoretical models often assume clean, deterministic rewards, but practical applications of Q-learning must contend with noise. This stochasticity can cause an agent's value estimates $Q(s,a)$ to fluctuate, leading to unstable \"policy oscillations\" where the greedy action flips back and forth. In this hands-on coding exercise, you will simulate this phenomenon and test a powerful stabilization technique: Polyak-Ruppert averaging, which smooths the value function to yield a more stable policy [@problem_id:3163593].", "problem": "Consider a finite Markov Decision Process (MDP) with a single state $s$ and two actions $a \\in \\{0,1\\}$. The dynamics are deterministic: taking any action leaves the system in state $s$ with transition probability $1$. The immediate reward $R_t$ at time $t$ is stochastic and depends on the chosen action: conditioned on action $a$, the reward $R_t$ is an independent draw from a Normal distribution with mean $\\mu_a$ and standard deviation $\\sigma$. Let the discount factor be $\\gamma \\in (0,1)$.\n\nDefine the action-value function $Q_t(s,a)$ at time $t$, initialized at $Q_0(s,a)=0$ for both actions. The learning follows a stochastic approximation to the Bellman optimality recursion in the form of a one-step temporal-difference update with learning rate $\\alpha \\in (0,1]$:\n$$\nQ_{t+1}(s,a_t) = (1-\\alpha) Q_t(s,a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right),\n$$\nand $Q_{t+1}(s,a) = Q_t(s,a)$ for $a \\neq a_t$. Action selection at time $t$ is greedy with respect to the current estimate, meaning $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$, with ties broken deterministically by choosing the smaller action index.\n\nDefine a \"policy flip\" at time $t \\ge 2$ to occur when the greedy action selected at time $t$ differs from the greedy action selected at time $t-1$.\n\nNow introduce a smoothed action-value estimator $\\bar{Q}_t(s,a)$, initialized at $\\bar{Q}_0(s,a)=0$, defined by a Polyak-style exponential moving average of the raw $Q_t$:\n$$\n\\bar{Q}_t(s,a) = \\rho \\,\\bar{Q}_{t-1}(s,a) + (1-\\rho) \\, Q_t(s,a),\n$$\nfor a smoothing coefficient $\\rho \\in [0,1]$. In the smoothed variant, action selection is greedy with respect to $\\bar{Q}_t$, that is $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$ with the same tie-breaking rule. The underlying $Q_t$ is updated using the same temporal-difference rule as above; the smoothing update is applied after the $Q_t$ update at each time step.\n\nYour task is to implement two simulations over a horizon of $T$ steps for each test case:\n- A raw greedy simulation: greedy action selection with respect to $Q_t$.\n- A smoothed greedy simulation: greedy action selection with respect to $\\bar{Q}_t$.\n\nFor each simulation, count the total number of policy flips over the $T$ steps. For reproducibility, use the specified random seed in each test case to initialize a pseudo-random number generator and draw rewards from the correct Normal distributions.\n\nReport, for each test case, a boolean value that is true if and only if the number of flips under the smoothed greedy selection is strictly less than the number of flips under the raw greedy selection, and false otherwise.\n\nUse the following test suite, where each case is specified as $(\\mu_0,\\mu_1,\\sigma,\\alpha,\\gamma,T,\\rho,\\text{seed})$:\n- Case $1$: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 123)$.\n- Case $2$: $(0.05, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 456)$.\n- Case $3$: $(0.0, 0.0, 0.0, 0.9, 0.9, 200, 0.95, 789)$.\n- Case $4$: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 1.0, 321)$.\n\nAll quantities are unitless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the boolean result for the corresponding test case, in order, for example $[{\\text{True}},{\\text{False}},{\\text{True}},{\\text{True}}]$.", "solution": "The user-provided problem statement has been analyzed and validated.\n\n### Step 1: Extract Givens\n\n-   **MDP**: A single state $s$ and two actions $a \\in \\{0,1\\}$.\n-   **Transitions**: Deterministic, $P(s' = s | s, a) = 1$.\n-   **Rewards**: Stochastic, $R_t \\sim \\mathcal{N}(\\mu_a, \\sigma)$ for a chosen action $a_t$.\n-   **Discount Factor**: $\\gamma \\in (0,1)$.\n-   **Action-Value Function**: $Q_t(s,a)$ initialized as $Q_0(s,a)=0$ for $a \\in \\{0,1\\}$.\n-   **Learning Rate**: $\\alpha \\in (0,1]$.\n-   **TD Update Rule**: $Q_{t+1}(s,a_t) = (1-\\alpha) Q_t(s,a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right)$, and $Q_{t+1}(s,a) = Q_t(s,a)$ for $a \\neq a_t$.\n-   **Raw Action Selection**: Greedy, $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$. Ties are broken by choosing the smaller action index (i.e., action $0$).\n-   **Smoothed Action-Value Function**: $\\bar{Q}_t(s,a) = \\rho \\,\\bar{Q}_{t-1}(s,a) + (1-\\rho) \\, Q_t(s,a)$, initialized as $\\bar{Q}_0(s,a)=0$.\n-   **Smoothing Coefficient**: $\\rho \\in [0,1]$.\n-   **Smoothed Action Selection**: Greedy, $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$, with the same tie-breaking rule. The smoothing update is applied after the $Q_t$ update at each time step.\n-   **Policy Flip**: Occurs at time $t \\ge 2$ when the greedy action $a_t$ differs from the greedy action $a_{t-1}$.\n-   **Task**: For each test case, simulate two scenarios (raw and smoothed greedy selection) over a horizon of $T$ steps. Count the total number of policy flips in each scenario.\n-   **Output**: For each test case, a boolean value: true if the smoothed simulation has strictly fewer flips than the raw simulation, false otherwise.\n-   **Test Suite**: $(\\mu_0, \\mu_1, \\sigma, \\alpha, \\gamma, T, \\rho, \\text{seed})$\n    -   Case 1: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 123)$\n    -   Case 2: $(0.05, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 456)$\n    -   Case 3: $(0.0, 0.0, 0.0, 0.9, 0.9, 200, 0.95, 789)$\n    -   Case 4: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 1.0, 321)$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded in the established theory of reinforcement learning, specifically Q-learning. It describes a well-posed computational task. All parameters, initial conditions, update rules, and action selection mechanisms are explicitly defined. The use of a random seed for each test case ensures that the stochastic simulation is reproducible and yields a unique solution. The definition of a \"policy flip\" at time $t \\ge 2$ is slightly unconventional (as it ignores the first potential flip between $a_0$ and $a_1$), but it is unambiguous and can be implemented as stated. The problem is objective, formalizable, and presents a non-trivial comparison between standard Q-learning and a variant with value-function smoothing.\n\n### Step 3: Verdict and Action\n\nThe problem statement is deemed **valid**. A solution will be provided.\n\n### Principle-Based Algorithmic Design\n\nThe problem requires implementing and comparing two variants of a Q-learning agent in a simple, single-state Markov Decision Process (MDP). The core of the solution is to construct a simulation that iteratively updates action-value estimates and records the agent's behavior.\n\nSince the MDP has only one state $s$, the action-value function $Q_t(s,a)$ can be simplified to a two-element vector, where the elements correspond to the values of actions $a=0$ and $a=1$. Let's denote this vector as $\\mathbf{Q}_t = [Q_t(s,0), Q_t(s,1)]$. Similarly, the smoothed action-value function is represented by a vector $\\bar{\\mathbf{Q}}_t$.\n\nFor each test case, two separate simulations are run over a time horizon of $T$ steps, from $t=0$ to $t=T-1$. Each simulation must re-initialize the random number generator with the specified seed to ensure the comparability of the underlying stochastic reward process.\n\n**Simulation Loop ($t = 0, \\dots, T-1$)**\n\nThe simulation proceeds in discrete time steps. At the beginning of each step $t$, the agent possesses action-value estimates $\\mathbf{Q}_t$ and, in the smoothed case, $\\bar{\\mathbf{Q}}_t$, carried over from the previous step.\n\n1.  **Action Selection**: An action $a_t$ is chosen based on the current value estimates.\n    -   In the \"raw\" simulation, the agent is greedy with respect to $\\mathbf{Q}_t$: $a_t = \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$.\n    -   In the \"smoothed\" simulation, the agent is greedy with respect to $\\bar{\\mathbf{Q}}_t$: $a_t = \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$.\n    -   The tie-breaking rule dictates that if $Q_t(s,0) = Q_t(s,1)$ (or $\\bar{Q}_t(s,0) = \\bar{Q}_t(s,1)$), action $a=0$ is selected.\n\n2.  **Reward Observation**: A reward $R_t$ is drawn from a Normal distribution $\\mathcal{N}(\\mu_{a_t}, \\sigma)$.\n\n3.  **Action-Value Update ($Q$-update)**: The core learning step uses the one-step temporal-difference (TD) update. The value of the chosen action $a_t$ is updated, while the other remains unchanged. The new value vector $\\mathbf{Q}_{t+1}$ is computed as:\n    $$\n    Q_{t+1}(s, a_t) = (1-\\alpha) Q_t(s, a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right)\n    $$\n    $$\n    Q_{t+1}(s, a) = Q_t(s, a) \\quad \\text{for } a \\neq a_t\n    $$\n    Note that the TD target, $R_t + \\gamma \\max_{b} Q_t(s,b)$, is computed using the value estimates $\\mathbf{Q}_t$ *before* the current update.\n\n4.  **Smoothed Action-Value Update ($\\bar{Q}$-update)**: This step is performed only in the smoothed simulation. As per the problem description, this update occurs *after* the $Q$-update. The new smoothed value vector $\\bar{\\mathbf{Q}}_{t+1}$ is computed as an exponential moving average (EMA) of its previous value $\\bar{\\mathbf{Q}}_t$ and the newly computed raw value $\\mathbf{Q}_{t+1}$:\n    $$\n    \\bar{\\mathbf{Q}}_{t+1} = \\rho \\bar{\\mathbf{Q}}_t + (1-\\rho) \\mathbf{Q}_{t+1}\n    $$\n    This smoothing is intended to dampen the fluctuations in the raw Q-values caused by the stochasticity of rewards, potentially leading to a more stable policy.\n\n**Policy Flip Calculation**\n\nAfter completing the $T$ steps of a simulation, a history of taken actions, $(a_0, a_1, \\dots, a_{T-1})$, is available. A policy flip is counted whenever the action at time $t$ differs from the action at time $t-1$. According to the problem's literal definition, this count is performed for $t \\ge 2$. Thus, the total number of flips is the number of times $a_t \\ne a_{t-1}$ for $t \\in \\{2, 3, \\dots, T-1\\}$.\n\n**Final Comparison**\n\nFor each test case, the number of flips from the smoothed simulation (`smoothed_flips`) is compared to the number from the raw simulation (`raw_flips`). The final output for the case is the boolean result of the strict inequality `smoothed_flips  raw_flips`.", "answer": "```python\nimport numpy as np\n\ndef run_simulation(params, use_smoothing):\n    \"\"\"\n    Runs a single Q-learning simulation for a given set of parameters.\n\n    Args:\n        params (tuple): A tuple containing the simulation parameters:\n                        (mu0, mu1, sigma, alpha, gamma, T, rho, seed).\n        use_smoothing (bool): If True, uses smoothed Q-values for action selection.\n\n    Returns:\n        int: The total number of policy flips.\n    \"\"\"\n    mu0, mu1, sigma, alpha, gamma, T, rho, seed = params\n    mu = np.array([mu0, mu1])\n\n    rng = np.random.default_rng(seed)\n\n    Q = np.zeros(2)\n    Q_bar = np.zeros(2) if use_smoothing else None\n    \n    actions_history = []\n\n    for t in range(T):\n        # 1. Select action\n        if use_smoothing:\n            # Greedy action selection with respect to smoothed Q-values\n            # Tie-breaking: choose smaller index (action 0)\n            action = 0 if Q_bar[0] = Q_bar[1] else 1\n        else:\n            # Greedy action selection with respect to raw Q-values\n            action = 0 if Q[0] = Q[1] else 1\n        actions_history.append(action)\n\n        # 2. Get reward\n        reward = rng.normal(loc=mu[action], scale=sigma)\n\n        # 3. Update raw Q-value\n        # The TD target uses the Q-values from the beginning of the step\n        td_target = reward + gamma * np.max(Q)\n        Q[action] = (1 - alpha) * Q[action] + alpha * td_target\n        \n        # 4. Update smoothed Q-value (if applicable)\n        # This update uses the newly computed Q-value, as instructed\n        if use_smoothing:\n            Q_bar = rho * Q_bar + (1 - rho) * Q\n            \n    # Count flips for t = 2\n    flips = 0\n    if T = 3:\n        for t in range(2, T):\n            if actions_history[t] != actions_history[t-1]:\n                flips += 1\n            \n    return flips\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for each test case and comparing\n    the number of policy flips between raw and smoothed Q-learning.\n    \"\"\"\n    test_cases = [\n        # (mu0,   mu1, sigma, alpha, gamma,   T,  rho, seed)\n        (0.0,   0.0,   1.0,   0.9,   0.9, 400, 0.95,  123),\n        (0.05,  0.0,   1.0,   0.9,   0.9, 400, 0.95,  456),\n        (0.0,   0.0,   0.0,   0.9,   0.9, 200, 0.95,  789),\n        (0.0,   0.0,   1.0,   0.9,   0.9, 400,  1.0,  321),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run raw Q-learning simulation\n        raw_flips = run_simulation(case, use_smoothing=False)\n        \n        # Run smoothed Q-learning simulation\n        smoothed_flips = run_simulation(case, use_smoothing=True)\n        \n        # Compare flip counts and append boolean result\n        results.append(smoothed_flips  raw_flips)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3163593"}, {"introduction": "Standard experience replay samples uniformly, but what if an agent could learn more from \"surprising\" transitions? This is the idea behind Prioritized Experience Replay, a key technique in modern reinforcement learning. However, sampling non-uniformly introduces statistical bias, which must be corrected using importance sampling weights $w(i)$. This practice provides a clear, analytical setting to quantify the crucial bias-variance trade-off, allowing you to calculate how the correction exponent $\\beta$ navigates between a high-bias, low-variance estimator and an unbiased, potentially high-variance one [@problem_id:3163626].", "problem": "You are given a small, finite Markov Decision Process (MDP) and asked to analyze the effect of prioritized experience replay on the estimation of a one-step temporal-difference statistic in Q-learning. The foundation is the following set of definitions.\n\nStart from the standard definitions of a Markov Decision Process with a finite state space and action space, the Q-learning update rule, and temporal-difference error. In Q-learning, for a sampled transition $(s,a,r,s')$, the temporal-difference error is defined as $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$, where $\\gamma \\in [0,1)$ and $Q$ denotes the current action-value function. Prioritized replay samples transitions from a replay buffer $\\mathcal{D}$ according to a non-uniform probability that depends on a priority assigned to each transition. The priority is commonly a function of the absolute temporal-difference error. A common scheme sets a sampling probability for transition $i$ as $p(i) = \\frac{(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}}{\\sum_{j \\in \\mathcal{D}} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha}}$, where $\\epsilon  0$ ensures non-zero probability for all transitions and $\\alpha \\ge 0$ governs the degree of prioritization. Importance sampling weights are used to correct the bias induced by non-uniform sampling when estimating expectations over the uniform distribution over the buffer.\n\nIn this problem, you will work in a simplified but scientifically meaningful setting to quantify bias and variance of estimators under prioritized sampling, and to correct the bias using importance sampling. Consider a replay buffer $\\mathcal{D}$ of size $N$ containing transitions drawn from a fixed data-collection policy in an MDP with two qualitatively distinct types of transitions: “rare” transitions with reward $R  0$ and “common” transitions with reward $0$. Assume the Q-values are fixed at $Q(s,a)=0$ for all $(s,a)$ and that the discount factor $\\gamma$ is any value in $[0,1)$; with $Q(s,a)=0$, the temporal-difference error for each transition equals the immediate reward, i.e., $\\delta_i = r_i$. The buffer contains $N_{\\text{rare}}$ rare transitions and $N_{\\text{common}}=N-N_{\\text{rare}}$ common transitions. For prioritized replay with parameters $\\alpha \\ge 0$ and $\\epsilon  0$, each transition $i$ is sampled with probability \n$$\np(i) = \\frac{(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}}{\\sum_{j=1}^{N} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha}}.\n$$\n\nYou are to estimate the uniform-buffer mean of temporal-difference errors,\n$$\n\\mu \\equiv \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i,\n$$\nusing three estimators based on $m$ independent samples with replacement from the prioritized distribution $p(i)$:\n- The unweighted prioritized-sampling estimator (no correction), defined as the sample average of the drawn $\\delta$ values (equivalently, importance exponent $\\beta = 0$).\n- A partially corrected estimator using importance sampling weights with exponent $\\beta \\in (0,1)$, defined as the sample average of $w(i)^{\\beta} \\delta_i$, where $w(i) = \\frac{1/N}{p(i)}$.\n- The fully corrected unbiased estimator using $\\beta = 1$, defined as the sample average of $w(i) \\delta_i$.\n\nFor each of these three estimators, compute analytically (no Monte Carlo simulation) the bias and variance of the sample mean estimator over $m$ i.i.d. draws from $p(i)$. You may use the following fundamental facts:\n- The expected value of a sample mean of $m$ i.i.d. random variables equals the population expectation.\n- The variance of a sample mean of $m$ i.i.d. random variables equals the population variance divided by $m$.\n- For any discrete distribution over finitely many values, the expectation and variance are computed by finite sums over support points.\n\nYou must implement a complete, runnable program that, for each parameter set in the test suite below, computes:\n- The analytic bias and variance of the sample mean estimator under prioritized sampling without correction (equivalently $\\beta=0$).\n- The analytic bias and variance with a specified partial correction exponent $\\beta \\in (0,1)$.\n- The analytic bias and variance with full correction $\\beta=1$.\n\nUse the following test suite of parameter sets. Each set is a tuple $(N, N_{\\text{rare}}, R, \\alpha, \\epsilon, m, \\beta_{\\text{half}})$ with all quantities in standard mathematical units:\n- Case A (happy path): $(N=\\;1000, N_{\\text{rare}}=\\;10, R=\\;1.0, \\alpha=\\;0.6, \\epsilon=\\;10^{-3}, m=\\;256, \\beta_{\\text{half}}=\\;0.5)$.\n- Case B (strong prioritization and extreme rarity): $(N=\\;1000, N_{\\text{rare}}=\\;1, R=\\;2.0, \\alpha=\\;1.0, \\epsilon=\\;10^{-6}, m=\\;64, \\beta_{\\text{half}}=\\;0.5)$.\n- Case C (no prioritization baseline): $(N=\\;1000, N_{\\text{rare}}=\\;10, R=\\;1.0, \\alpha=\\;0.0, \\epsilon=\\;10^{-3}, m=\\;256, \\beta_{\\text{half}}=\\;0.5)$.\n- Case D (no rare events boundary): $(N=\\;100, N_{\\text{rare}}=\\;0, R=\\;1.0, \\alpha=\\;0.7, \\epsilon=\\;10^{-4}, m=\\;32, \\beta_{\\text{half}}=\\;0.5)$.\n- Case E (zero reward boundary): $(N=\\;200, N_{\\text{rare}}=\\;20, R=\\;0.0, \\alpha=\\;0.9, \\epsilon=\\;10^{-3}, m=\\;128, \\beta_{\\text{half}}=\\;0.5)$.\n\nYour program should, for each case, output a list of six floating-point numbers:\n$[\\text{bias}_{\\beta=0}, \\text{var}_{\\beta=0}, \\text{bias}_{\\beta=\\beta_{\\text{half}}}, \\text{var}_{\\beta=\\beta_{\\text{half}}}, \\text{bias}_{\\beta=1}, \\text{var}_{\\beta=1}]$,\nwhere $\\text{var}$ denotes the variance of the sample mean estimator and $\\text{bias}$ denotes the difference between the estimator’s expected value and $\\mu$. All results must be rounded to six decimal places.\n\nFinal output format requirement: Your program should produce a single line of output containing a comma-separated list of per-case results, each per-case result being the list described above, all enclosed in a single pair of square brackets, for example:\n\"[[x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}],[y_{1},y_{2},y_{3},y_{4},y_{5},y_{6}],...]\".", "solution": "The objective of this problem is to analytically compute the bias and variance of three different temporal-difference (TD) error estimators under prioritized experience replay. The analysis is conducted in a simplified, yet representative, Markov Decision Process (MDP) setting. The three estimators correspond to using no importance sampling (IS) correction ($\\beta=0$), partial correction ($\\beta \\in (0,1)$), and full correction ($\\beta=1$).\n\nThe analysis proceeds in the following steps:\n1.  Define the components of the simplified MDP and replay buffer.\n2.  Specify the true mean of the TD error, which is the target quantity to be estimated.\n3.  Formulate the sampling probabilities under the prioritized replay scheme.\n4.  Define a random variable for a single corrected sample drawn from the replay buffer.\n5.  Derive the general expressions for the expectation and variance of this random variable for an arbitrary correction exponent $\\beta$.\n6.  From these expressions, compute the bias and variance of the sample mean estimator over $m$ draws for the three specified values of $\\beta$.\n\n**1. Model and Replay Buffer Composition**\nThe replay buffer $\\mathcal{D}$ contains $N$ transitions. These are categorized into two types:\n- $N_{\\text{rare}}$ \"rare\" transitions, each with a reward $R$ and thus a TD error $\\delta_i = R$.\n- $N_{\\text{common}} = N - N_{\\text{rare}}$ \"common\" transitions, each with reward $0$ and TD error $\\delta_j = 0$.\n\nThis simplification follows from the problem statement where the action-value function is fixed at $Q(s,a)=0$ for all state-action pairs $(s,a)$, making the TD error $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ equal to the immediate reward $r$.\n\n**2. Target Quantity: True Mean**\nThe quantity we wish to estimate is the mean of the TD errors assuming a uniform distribution over the buffer $\\mathcal{D}$. This true mean, denoted by $\\mu$, is:\n$$ \\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i = \\frac{N_{\\text{rare}} \\cdot R + N_{\\text{common}} \\cdot 0}{N} = \\frac{N_{\\text{rare}} R}{N} $$\n\n**3. Prioritized Sampling Distribution**\nThe probability $p(i)$ of sampling transition $i$ is proportional to $(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}$.\nFor a rare transition, $\\lvert \\delta_i \\rvert = R$ (assuming $R \\ge 0$), so the priority term is $(R + \\epsilon)^{\\alpha}$.\nFor a common transition, $\\lvert \\delta_j \\rvert = 0$, so the priority term is $(\\epsilon)^{\\alpha}$.\n\nThe normalization constant $Z$ is the sum of these priority terms over all transitions in the buffer:\n$$ Z = \\sum_{j=1}^{N} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha} = N_{\\text{rare}}(R + \\epsilon)^{\\alpha} + N_{\\text{common}}(\\epsilon)^{\\alpha} $$\nThe probability of sampling a single specific rare transition is:\n$$ p_{\\text{rare}} = \\frac{(R + \\epsilon)^{\\alpha}}{Z} $$\nThe probability of sampling a single specific common transition is:\n$$ p_{\\text{common}} = \\frac{(\\epsilon)^{\\alpha}}{Z} $$\n\n**4. The Corrected Sample Estimator**\nAn estimator for $\\mu$ is formed by taking the sample mean of $m$ independent and identically distributed (i.i.d.) draws from the prioritized distribution $p(i)$. To correct for the sampling bias, each sampled TD error $\\delta_i$ is multiplied by an importance sampling weight $w(i) = \\frac{1/N}{p(i)}$, raised to a power $\\beta \\in [0,1]$.\n\nLet $X_{\\beta}$ be the random variable representing a single corrected sample, $X_{\\beta} = w(i)^{\\beta} \\delta_i$. The estimator is $\\hat{\\mu}_{\\beta} = \\frac{1}{m} \\sum_{k=1}^{m} X_{\\beta}^{(k)}$, where each $X_{\\beta}^{(k)}$ is an independent draw.\n\nThe value of $X_{\\beta}$ depends on whether a rare or common transition is drawn:\n- If a rare transition is drawn, $\\delta_i=R$, and the value is $x_{\\text{rare}} = \\left(\\frac{1/N}{p_{\\text{rare}}}\\right)^{\\beta} R$. This occurs with total probability $P(\\text{draw rare}) = N_{\\text{rare}} p_{\\text{rare}}$.\n- If a common transition is drawn, $\\delta_j=0$, and the value is $x_{\\text{common}} = 0$. This occurs with total probability $P(\\text{draw common}) = N_{\\text{common}} p_{\\text{common}}$.\n\n**5. General Derivation of Bias and Variance**\nThe bias and variance of the estimator $\\hat{\\mu}_{\\beta}$ can be found from the expectation $E[X_\\beta]$ and variance $V[X_\\beta]$ of a single draw.\n- Bias: $B[\\hat{\\mu}_{\\beta}] = E[\\hat{\\mu}_{\\beta}] - \\mu = E[X_{\\beta}] - \\mu$.\n- Estimator Variance: $V[\\hat{\\mu}_{\\beta}] = \\frac{V[X_{\\beta}]}{m} = \\frac{E[X_{\\beta}^2] - (E[X_{\\beta}])^2}{m}$.\n\nThe expectation of $X_{\\beta}$ is:\n$$ E[X_{\\beta}] = x_{\\text{rare}} \\cdot P(\\text{draw rare}) + x_{\\text{common}} \\cdot P(\\text{draw common}) $$\n$$ E[X_{\\beta}] = \\left(\\frac{1}{N p_{\\text{rare}}}\\right)^{\\beta} R \\cdot (N_{\\text{rare}} p_{\\text{rare}}) + 0 = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-\\beta} N^{-\\beta} $$\nThe expectation of $X_{\\beta}^2$ is:\n$$ E[X_{\\beta}^2] = (x_{\\text{rare}})^2 \\cdot P(\\text{draw rare}) + (x_{\\text{common}})^2 \\cdot P(\\text{draw common}) $$\n$$ E[X_{\\beta}^2] = \\left(\\left(\\frac{1}{N p_{\\text{rare}}}\\right)^{\\beta} R\\right)^2 \\cdot (N_{\\text{rare}} p_{\\text{rare}}) + 0 = R^2 \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-2\\beta} N^{-2\\beta} $$\n\n**6. Analysis for Specific Values of $\\beta$**\nThese general formulas can now be applied to the three cases of interest.\n\n**Case 1: No Correction ($\\beta=0$)**\nThe estimator is the simple sample average of uncorrected TD errors.\n- Expectation: $E[X_0] = R \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}}$.\n- Bias: $B[\\hat{\\mu}_0] = R \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}} - \\mu$.\n- Second Moment: $E[X_0^2] = R^2 \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}}$.\n- Estimator Variance: $V[\\hat{\\mu}_0] = \\frac{1}{m}(R^2 N_{\\text{rare}} p_{\\text{rare}} - (R N_{\\text{rare}} p_{\\text{rare}})^2) = \\frac{1}{m} R^2 N_{\\text{rare}} p_{\\text{rare}}(1 - N_{\\text{rare}} p_{\\text{rare}})$.\nThis estimator is generally biased, with lower variance compared to the uniform-sampling baseline, as prioritization focuses on specific transition types.\n\n**Case 2: Full Correction ($\\beta=1$)**\nThe estimator uses the standard importance sampling correction.\n- Expectation: $E[X_1] = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-1} N^{-1} = \\frac{R N_{\\text{rare}}}{N} = \\mu$.\n- Bias: $B[\\hat{\\mu}_1] = \\mu - \\mu = 0$. The estimator is unbiased, as expected from IS theory.\n- Second Moment: $E[X_1^2] = R^2 \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-2} N^{-2} = \\frac{R^2 N_{\\text{rare}}}{N^2 p_{\\text{rare}}}$.\n- Estimator Variance: $V[\\hat{\\mu}_1] = \\frac{1}{m}(E[X_1^2] - \\mu^2) = \\frac{1}{m}\\left(\\frac{R^2 N_{\\text{rare}}}{N^2 p_{\\text{rare}}} - \\left(\\frac{R N_{\\text{rare}}}{N}\\right)^2\\right) = \\frac{R^2}{m N^2}\\left(\\frac{N_{\\text{rare}}}{p_{\\text{rare}}} - N_{\\text{rare}}^2\\right)$.\nThe variance of this estimator can be higher or lower than that of uniform sampling, depending on the mismatch between the uniform and prioritized distributions.\n\n**Case 3: Partial Correction ($\\beta = \\beta_{\\text{half}} \\in (0,1)$)**\nThis represents a trade-off, aiming to reduce the bias of the uncorrected estimator while controlling the potentially high variance of the fully corrected one. The bias and variance are calculated directly from the general formulas derived in section 5 by substituting $\\beta = \\beta_{\\text{half}}$.\n- Bias: $B[\\hat{\\mu}_{\\beta_{\\text{half}}}] = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-\\beta_{\\text{half}}} N^{-\\beta_{\\text{half}}} - \\mu$.\n- Estimator Variance: $V[\\hat{\\mu}_{\\beta_{\\text{half}}}] = \\frac{1}{m}\\left( R^2 N_{\\text{rare}} (p_{\\text{rare}})^{1-2\\beta_{\\text{half}}} N^{-2\\beta_{\\text{half}}} - (E[X_{\\beta_{\\text{half}}}])^2 \\right)$.\n\nThese derived expressions allow for the direct analytical computation of the required quantities for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the analytic bias and variance of TD error estimators\n    under prioritized experience replay for a suite of test cases.\n    \"\"\"\n    # Parameter sets as (N, N_rare, R, alpha, epsilon, m, beta_half)\n    test_cases = [\n        (1000, 10, 1.0, 0.6, 1e-3, 256, 0.5), # Case A\n        (1000, 1, 2.0, 1.0, 1e-6, 64, 0.5),  # Case B\n        (1000, 10, 1.0, 0.0, 1e-3, 256, 0.5), # Case C\n        (100, 0, 1.0, 0.7, 1e-4, 32, 0.5),   # Case D\n        (200, 20, 0.0, 0.9, 1e-3, 128, 0.5), # Case E\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, N_rare, R, alpha, epsilon, m, beta_half = case\n        \n        # Use floating point numbers for calculations to ensure precision and\n        # compatibility with functions like np.power.\n        N, N_rare, R, m = float(N), float(N_rare), float(R), float(m)\n\n        case_results = []\n\n        # The true mean of the TD errors over the uniform buffer distribution.\n        # This is the target value for our estimators.\n        mu = (N_rare * R) / N if N > 0 else 0.0\n        \n        # The derived analytical formulas work for boundary cases (N_rare=0 or R=0)\n        # where mu becomes 0. In these cases, all TD errors are 0, so any sample is 0,\n        # leading to an estimator that is always 0, with zero bias and variance.\n        # The formulas correctly reflect this.\n        \n        if N_rare == 0 or R == 0.0:\n            p_rare = 1.0/N if N > 0 else 1.0 # Uniform sampling, not used in calculations but for completeness\n        else:\n            N_common = N - N_rare\n            # Calculate priorities for rare and common transitions.\n            # priority is proportional to (|delta| + epsilon)^alpha\n            priority_term_rare = np.power(R + epsilon, alpha)\n            priority_term_common = np.power(epsilon, alpha)\n            \n            # Normalization constant Z is the sum of all priorities.\n            Z = N_rare * priority_term_rare + N_common * priority_term_common\n            \n            # Probability of sampling a single specific rare transition.\n            # Z cannot be zero since N > 0 and epsilon > 0.\n            p_rare = priority_term_rare / Z\n        \n        betas_to_test = [0.0, beta_half, 1.0]\n        \n        for beta in betas_to_test:\n            # Let X_beta be the random variable for a single corrected sample: w(i)^beta * delta_i\n            # We calculate its population expectation E[X_beta] and variance Var(X_beta).\n            \n            # E[X_beta] = R * N_rare * (p_rare)^(1-beta) * N^(-beta)\n            E_X_beta = R * N_rare * np.power(p_rare, 1 - beta) * np.power(N, -beta)\n            \n            # Bias of the estimator is E[estimator] - true_mean\n            bias = E_X_beta - mu\n            \n            # E[X_beta^2] = R^2 * N_rare * (p_rare)^(1-2*beta) * N^(-2*beta)\n            E_X_beta_sq = np.power(R, 2) * N_rare * np.power(p_rare, 1 - 2 * beta) * np.power(N, -2 * beta)\n            \n            # Population variance Var(X_beta) = E[X_beta^2] - (E[X_beta])^2\n            V_X_beta = E_X_beta_sq - np.power(E_X_beta, 2)\n            \n            # The estimator's variance is Var(X_beta) / m for a sample size of m.\n            var_estimator = V_X_beta / m\n            \n            # Due to floating point inaccuracies, a variance that should be zero\n            # might calculate as a tiny negative number. Clamp at 0.\n            var_estimator = max(0.0, var_estimator)\n            \n            case_results.extend([bias, var_estimator])\n            \n        all_results.append([round(x, 6) for x in case_results])\n\n    # Format the final output string as a list of lists, per requirements.\n    # e.g., \"[[r1_1, r1_2, ...], [r2_1, r2_2, ...]]\"\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3163626"}]}