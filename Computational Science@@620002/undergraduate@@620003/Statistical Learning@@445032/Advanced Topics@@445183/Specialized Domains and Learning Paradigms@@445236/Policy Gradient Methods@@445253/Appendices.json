{"hands_on_practices": [{"introduction": "The foundation of any policy gradient method is the ability to compute the gradient of the expected return with respect to the policy parameters. This first exercise guides you through the fundamental derivation for a softmax policy, which is one of the most common choices for discrete action spaces. By working through this problem [@problem_id:3157980], you will derive the elegant and insightful structure of the policy gradient, revealing how the probability of an action and its corresponding reward contribute to the direction of steepest ascent for improving the policy.", "problem": "A finite-action stochastic decision problem has an action set of size $K$, labeled $\\{1,2,\\dots,K\\}$. A stationary stochastic policy selects action $i$ with probability $\\pi_{\\theta}(i)$, where the parameter $\\theta \\in \\mathbb{R}^{K}$ is unconstrained and mapped to the probability simplex via the softmax transformation\n$$\n\\pi_{\\theta}(i) \\;=\\; \\frac{\\exp(\\theta_{i})}{\\sum_{j=1}^{K} \\exp(\\theta_{j})} \\quad \\text{for } i \\in \\{1,2,\\dots,K\\}.\n$$\nFor each action $i$, the reward $R(i)$ is a random variable with finite mean $\\mu_{i} = \\mathbb{E}[R(i)]$. Consider the expected return\n$$\nJ(\\theta) \\;=\\; \\mathbb{E}_{a \\sim \\pi_{\\theta}}[R(a)] \\;=\\; \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\,\\mu_{i}.\n$$\nAssume the interchange of gradient and expectation is justified by standard regularity conditions, and that all objects are well-defined.\n\nTasks:\n1. Starting only from the definitions above, derive an explicit closed-form expression for the gradient $\\nabla_{\\theta} J(\\theta)$ in terms of $\\{\\pi_{\\theta}(i)\\}_{i=1}^{K}$ and $\\{\\mu_{i}\\}_{i=1}^{K}$, without introducing any additional parameterizations or constraints.\n2. Using your expression, analyze what happens to $\\nabla_{\\theta} J(\\theta)$ as one component $\\theta_{k} \\to +\\infty$ while all other components of $\\theta$ are held fixed. State whether the gradient vanishes in this limit, and justify your conclusion from first principles.\n3. Specialize to $K=3$ with reward means $\\mu = (\\mu_{1},\\mu_{2},\\mu_{3}) = (1,0,-1)$ and parameter $\\theta = (0,\\ln 2,-\\ln 2)$. Compute the gradient vector $\\nabla_{\\theta} J(\\theta)$ exactly.\n\nAnswer format requirement: Provide the final gradient vector for the specialized case in a single row using the LaTeX pmatrix environment. Do not include any units or text in the final boxed answer. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded in statistical learning theory, and contains all necessary information for a unique solution. We proceed with the derivation and analysis as requested.\n\n### Part 1: Derivation of the Gradient $\\nabla_{\\theta} J(\\theta)$\n\nThe objective function to be maximized is the expected return, given by:\n$$\nJ(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[R(a)] = \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\,\\mu_{i}\n$$\nwhere $\\mu_i = \\mathbb{E}[R(i)]$ is the mean reward for action $i$, and $\\pi_{\\theta}(i)$ is the softmax policy:\n$$\n\\pi_{\\theta}(i) = \\frac{\\exp(\\theta_{i})}{\\sum_{j=1}^{K} \\exp(\\theta_{j})}\n$$\nWe want to compute the gradient of $J(\\theta)$ with respect to the parameter vector $\\theta$, denoted $\\nabla_{\\theta} J(\\theta)$. The $k$-th component of this gradient is the partial derivative $\\frac{\\partial J(\\theta)}{\\partial \\theta_k}$.\n\nStarting with the definition of $J(\\theta)$, we differentiate with respect to $\\theta_k$:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\,\\mu_{i} = \\sum_{i=1}^{K} \\mu_{i} \\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k}\n$$\nThe interchange of differentiation and summation is justified as the sum is finite.\n\nTo compute the derivative of the softmax function $\\pi_{\\theta}(i)$, we can use the \"log-derivative trick,\" which states that $\\nabla \\pi = \\pi \\nabla \\ln \\pi$. Let's compute the derivative of $\\ln \\pi_{\\theta}(i)$:\n$$\n\\ln \\pi_{\\theta}(i) = \\ln\\left(\\frac{\\exp(\\theta_i)}{\\sum_{j=1}^{K} \\exp(\\theta_j)}\\right) = \\theta_i - \\ln\\left(\\sum_{j=1}^{K} \\exp(\\theta_j)\\right)\n$$\nNow, we differentiate this expression with respect to $\\theta_k$:\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\ln \\pi_{\\theta}(i) = \\frac{\\partial \\theta_i}{\\partial \\theta_k} - \\frac{\\frac{\\partial}{\\partial \\theta_k} \\sum_{j=1}^{K} \\exp(\\theta_j)}{\\sum_{j=1}^{K} \\exp(\\theta_j)}\n$$\nThe derivative of $\\theta_i$ with respect to $\\theta_k$ is the Kronecker delta, $\\delta_{ik}$. The derivative of the sum is:\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\sum_{j=1}^{K} \\exp(\\theta_j) = \\exp(\\theta_k)\n$$\nSubstituting this back, we get:\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\ln \\pi_{\\theta}(i) = \\delta_{ik} - \\frac{\\exp(\\theta_k)}{\\sum_{j=1}^{K} \\exp(\\theta_j)} = \\delta_{ik} - \\pi_{\\theta}(k)\n$$\nUsing the log-derivative trick, $\\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k} = \\pi_{\\theta}(i) \\frac{\\partial \\ln \\pi_{\\theta}(i)}{\\partial \\theta_k}$, we find:\n$$\n\\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k} = \\pi_{\\theta}(i) (\\delta_{ik} - \\pi_{\\theta}(k))\n$$\nThis gives two cases:\n1. If $i=k$: $\\frac{\\partial \\pi_{\\theta}(k)}{\\partial \\theta_k} = \\pi_{\\theta}(k) (1 - \\pi_{\\theta}(k))$\n2. If $i \\neq k$: $\\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k} = \\pi_{\\theta}(i) (0 - \\pi_{\\theta}(k)) = -\\pi_{\\theta}(i) \\pi_{\\theta}(k)$\n\nNow we substitute this result back into the expression for the gradient of $J(\\theta)$:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^{K} \\mu_{i} \\left[ \\pi_{\\theta}(i) (\\delta_{ik} - \\pi_{\\theta}(k)) \\right]\n$$\nWe can expand the sum:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\mu_k \\pi_{\\theta}(k) (1 - \\pi_{\\theta}(k)) + \\sum_{i \\neq k} \\mu_i (-\\pi_{\\theta}(i) \\pi_{\\theta}(k))\n$$\nFactoring out $\\pi_{\\theta}(k)$:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) \\left[ \\mu_k (1 - \\pi_{\\theta}(k)) - \\sum_{i \\neq k} \\mu_i \\pi_{\\theta}(i) \\right]\n$$\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) \\left[ \\mu_k - \\mu_k \\pi_{\\theta}(k) - \\sum_{i \\neq k} \\mu_i \\pi_{\\theta}(i) \\right]\n$$\nRecognizing that the terms inside the parentheses almost form the sum for $J(\\theta)$:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) \\left[ \\mu_k - \\left( \\mu_k \\pi_{\\theta}(k) + \\sum_{i \\neq k} \\mu_i \\pi_{\\theta}(i) \\right) \\right]\n$$\nThe term in the parenthesis is precisely $\\sum_{i=1}^{K} \\mu_i \\pi_{\\theta}(i) = J(\\theta)$. Therefore, we arrive at the elegant closed-form expression:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) (\\mu_k - J(\\theta))\n$$\nThe full gradient vector $\\nabla_{\\theta} J(\\theta)$ is a vector whose $k$-th component is given by this expression.\n\n### Part 2: Limit Analysis as $\\theta_k \\to +\\infty$\n\nWe are asked to analyze the behavior of the gradient $\\nabla_{\\theta} J(\\theta)$ as one component, say $\\theta_k$, approaches $+\\infty$ while all other components $\\theta_j$ ($j \\neq k$) are held fixed.\n\nFirst, let's analyze the limits of the policy probabilities $\\pi_{\\theta}(i)$ and the expected return $J(\\theta)$.\nThe probability of selecting action $k$ is:\n$$\n\\pi_{\\theta}(k) = \\frac{\\exp(\\theta_k)}{\\sum_{j=1}^{K} \\exp(\\theta_j)} = \\frac{\\exp(\\theta_k)}{\\exp(\\theta_k) + \\sum_{j \\neq k} \\exp(\\theta_j)}\n$$\nDividing the numerator and denominator by $\\exp(\\theta_k)$:\n$$\n\\pi_{\\theta}(k) = \\frac{1}{1 + \\sum_{j \\neq k} \\exp(\\theta_j - \\theta_k)}\n$$\nAs $\\theta_k \\to +\\infty$, the term $\\theta_j - \\theta_k \\to -\\infty$ for any fixed $\\theta_j$ ($j \\neq k$). Consequently, $\\exp(\\theta_j - \\theta_k) \\to 0$.\nThus, the limit of $\\pi_{\\theta}(k)$ is:\n$$\n\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k) = \\frac{1}{1 + 0} = 1\n$$\nFor any other action $i \\neq k$, the probability is:\n$$\n\\pi_{\\theta}(i) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^{K} \\exp(\\theta_j)} = \\frac{\\exp(\\theta_i - \\theta_k)}{1 + \\sum_{j \\neq k} \\exp(\\theta_j - \\theta_k)}\n$$\nAs $\\theta_k \\to +\\infty$, the numerator $\\exp(\\theta_i - \\theta_k) \\to 0$ and the denominator approaches $1$. Therefore:\n$$\n\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i) = 0 \\quad \\text{for } i \\neq k\n$$\nThis shows that the policy becomes deterministic, selecting action $k$ with probability $1$.\n\nNext, we find the limit of the expected return $J(\\theta)$:\n$$\n\\lim_{\\theta_k \\to +\\infty} J(\\theta) = \\lim_{\\theta_k \\to +\\infty} \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\mu_i = \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k)\\right)\\mu_k + \\sum_{i \\neq k} \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i)\\right)\\mu_i\n$$\n$$\n\\lim_{\\theta_k \\to +\\infty} J(\\theta) = (1)\\mu_k + \\sum_{i \\neq k} (0)\\mu_i = \\mu_k\n$$\nThe expected return converges to the mean reward of the deterministically chosen action $k$.\n\nNow we can analyze the limit of each component of the gradient vector $\\nabla_{\\theta} J(\\theta)$. Let $g_i = (\\nabla_{\\theta} J(\\theta))_i = \\pi_{\\theta}(i)(\\mu_i - J(\\theta))$.\n\nFor the $k$-th component of the gradient:\n$$\n\\lim_{\\theta_k \\to +\\infty} g_k = \\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k)(\\mu_k - J(\\theta)) = \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k)\\right) \\left(\\mu_k - \\lim_{\\theta_k \\to +\\infty} J(\\theta)\\right)\n$$\n$$\n\\lim_{\\theta_k \\to +\\infty} g_k = (1) (\\mu_k - \\mu_k) = 0\n$$\nFor any other component $i \\neq k$:\n$$\n\\lim_{\\theta_k \\to +\\infty} g_i = \\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i)(\\mu_i - J(\\theta)) = \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i)\\right) \\left(\\mu_i - \\lim_{\\theta_k \\to +\\infty} J(\\theta)\\right)\n$$\n$$\n\\lim_{\\theta_k \\to +\\infty} g_i = (0) (\\mu_i - \\mu_k) = 0\n$$\nSince all components of the gradient vector approach $0$, the gradient $\\nabla_{\\theta} J(\\theta)$ vanishes in this limit. This occurs because the softmax function saturates, making the policy probabilities insensitive to further increases in the logits, thus \"flattening\" the objective landscape.\n\n### Part 3: Specialized Case Computation\n\nWe are given $K=3$, with reward means $\\mu = (1, 0, -1)$ and parameters $\\theta = (0, \\ln 2, -\\ln 2)$.\n\nFirst, we compute the logits' exponentials:\n- $\\exp(\\theta_1) = \\exp(0) = 1$\n- $\\exp(\\theta_2) = \\exp(\\ln 2) = 2$\n- $\\exp(\\theta_3) = \\exp(-\\ln 2) = \\exp(\\ln(2^{-1})) = \\frac{1}{2}$\n\nThe sum of these exponentials is:\n$$\nS = \\sum_{j=1}^{3} \\exp(\\theta_j) = 1 + 2 + \\frac{1}{2} = 3.5 = \\frac{7}{2}\n$$\nNext, we compute the action probabilities $\\pi_{\\theta}(i) = \\frac{\\exp(\\theta_i)}{S}$:\n- $\\pi_{\\theta}(1) = \\frac{1}{7/2} = \\frac{2}{7}$\n- $\\pi_{\\theta}(2) = \\frac{2}{7/2} = \\frac{4}{7}$\n- $\\pi_{\\theta}(3) = \\frac{1/2}{7/2} = \\frac{1}{7}$\nThe sum of probabilities is $\\frac{2}{7}+\\frac{4}{7}+\\frac{1}{7} = \\frac{7}{7} = 1$, as expected.\n\nNow, we compute the expected return $J(\\theta)$:\n$$\nJ(\\theta) = \\sum_{i=1}^{3} \\pi_{\\theta}(i)\\mu_i = \\left(\\frac{2}{7}\\right)(1) + \\left(\\frac{4}{7}\\right)(0) + \\left(\\frac{1}{7}\\right)(-1) = \\frac{2}{7} - \\frac{1}{7} = \\frac{1}{7}\n$$\nFinally, we compute each component of the gradient vector using the formula $(\\nabla_{\\theta} J(\\theta))_k = \\pi_{\\theta}(k)(\\mu_k - J(\\theta))$:\n\n- Component 1 ($k=1$):\n$$\n(\\nabla_{\\theta} J(\\theta))_1 = \\pi_{\\theta}(1)(\\mu_1 - J(\\theta)) = \\frac{2}{7}\\left(1 - \\frac{1}{7}\\right) = \\frac{2}{7}\\left(\\frac{6}{7}\\right) = \\frac{12}{49}\n$$\n- Component 2 ($k=2$):\n$$\n(\\nabla_{\\theta} J(\\theta))_2 = \\pi_{\\theta}(2)(\\mu_2 - J(\\theta)) = \\frac{4}{7}\\left(0 - \\frac{1}{7}\\right) = \\frac{4}{7}\\left(-\\frac{1}{7}\\right) = -\\frac{4}{49}\n$$\n- Component 3 ($k=3$):\n$$\n(\\nabla_{\\theta} J(\\theta))_3 = \\pi_{\\theta}(3)(\\mu_3 - J(\\theta)) = \\frac{1}{7}\\left(-1 - \\frac{1}{7}\\right) = \\frac{1}{7}\\left(-\\frac{8}{7}\\right) = -\\frac{8}{49}\n$$\nThe gradient vector is $\\nabla_{\\theta} J(\\theta) = \\left(\\frac{12}{49}, -\\frac{4}{49}, -\\frac{8}{49}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{12}{49}  -\\frac{4}{49}  -\\frac{8}{49} \\end{pmatrix}}$$", "id": "3157980"}, {"introduction": "While the basic policy gradient estimator is unbiased, it often suffers from high variance, which can make learning slow and unstable. A cornerstone technique for mitigating this issue is to introduce a state-dependent baseline. This practice [@problem_id:3163430] provides a hands-on approach to understanding and implementing variance reduction by deriving the optimal scaling factor for a value-function-based baseline, a critical step in making policy gradient algorithms practical and efficient.", "problem": "Consider an episodic Reinforcement Learning (RL) setting with a stochastic policy parameterized by $\\theta$, denoted $\\pi_{\\theta}(a \\mid s)$. Let $G_t$ be the return observed at time $t$ for state $s_t$ and action $a_t$, and let $z_t = \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t)$ be the score function (also called the log-derivative of the policy). Define a baseline function $b(s)$ that depends only on the state $s$. A common variance reduction technique is to use a control variate baseline of the form $b(s) = \\alpha \\hat{V}(s)$, where $\\hat{V}(s)$ is an estimate of the state value function and $\\alpha \\in \\mathbb{R}$ is a scalar to be determined. The gradient estimator at time $t$ then becomes $g_t(\\alpha) = \\left(G_t - \\alpha \\hat{V}(s_t)\\right) z_t$.\n\nStarting from core definitions and well-tested facts, including the log-derivative trick for policy gradients and the control variates principle in variance reduction, derive a procedure to choose the optimal scalar $\\alpha$ that minimizes the empirical variance of the gradient estimator constructed from a finite set of samples $\\{(G_i, \\hat{V}(s_i), z_i)\\}_{i=1}^{n}$. You must use a variance criterion that is a mathematically well-defined scalar measure for a vector-valued random variable. For this problem, use the trace of the empirical covariance as the variance measure, namely compute the sample variance of the gradient estimator as $\\frac{1}{n-1} \\sum_{i=1}^{n} \\left\\| g_i(\\alpha) - \\bar{g}(\\alpha) \\right\\|_2^2$, where $\\bar{g}(\\alpha) = \\frac{1}{n} \\sum_{i=1}^{n} g_i(\\alpha)$ is the sample mean of the estimator and $\\|\\cdot\\|_2$ is the Euclidean norm. Your derivation must not assume any special distributional forms beyond the definitions provided, and must proceed from fundamental properties of control variates and the score function.\n\nImplement a complete program that:\n- Computes the empirically optimal scalar $\\alpha$ from the provided samples by minimizing the empirical second moment $\\sum_{i=1}^{n} \\left\\| g_i(\\alpha) \\right\\|_2^2$, with the understanding that minimizing this second moment yields the same minimizer as minimizing the variance measure specified above because the population mean of the gradient estimator does not depend on the baseline when the baseline is independent of the action.\n- Computes the sample variance (trace of empirical covariance) of the gradient estimator without a baseline (i.e., with $\\alpha = 0$) and with the optimal baseline (i.e., with the empirically optimal $\\alpha$).\n- Handles edge cases where the empirical minimization is ill-posed, such as when the denominator in the optimality condition becomes zero; in such cases, set $\\alpha = 0$.\n\nYour program must be deterministic, require no user input, and use the following test suite with the specified arrays. Each test case provides $(G, \\hat{V}, Z)$:\n1. Happy path with a mix of nonzero and zero score function values:\n   - $G = [1.5, 0.5, 2.0, 1.0, 0.0]$\n   - $\\hat{V} = [1.0, 0.2, 1.8, 0.9, 0.1]$\n   - $Z = \\left[\\,[0.8, -0.3],\\ [0.1, 0.2],\\ [1.2, 0.5],\\ [0.7, 0.1],\\ [0.0, 0.0]\\,\\right]$\n2. Boundary case where the baseline estimate is identically zero:\n   - $G = [2.0, 1.0, -0.5, 0.3]$\n   - $\\hat{V} = [0.0, 0.0, 0.0, 0.0]$\n   - $Z = \\left[\\,[0.5, -0.1],\\ [0.4, 0.3],\\ [0.2, 0.2],\\ [1.0, -0.5]\\,\\right]$\n3. Case with negative correlation between $G$ and $\\hat{V}$, and a three-dimensional score function:\n   - $G = [1.0, 2.0, -1.0, 0.0, 0.5, -0.2]$\n   - $\\hat{V} = [-1.5, -1.0, 1.0, 0.2, -0.1, 0.3]$\n   - $Z = \\left[\\,[0.3, -0.7, 0.1],\\ [0.5, 0.2, -0.4],\\ [0.8, 0.1, 0.3],\\ [0.0, 0.6, 0.6],\\ [1.0, -0.2, 0.0],\\ [0.2, 0.2, -0.1]\\,\\right]$\n\nYour program must compute, for each test case, the triple $[\\alpha, \\mathrm{var}_0, \\mathrm{var}_{\\alpha}]$ where $\\alpha$ is the empirically optimal scalar, $\\mathrm{var}_0$ is the sample variance of the gradient estimator with $\\alpha = 0$, and $\\mathrm{var}_{\\alpha}$ is the sample variance with the empirically optimal $\\alpha$, all as real numbers. The final output must be a single line containing the results as a comma-separated list enclosed in square brackets, with each test case represented as a three-element list, for example, $\\left[\\left[\\alpha_1,\\mathrm{var}_{0,1},\\mathrm{var}_{\\alpha,1}\\right],\\left[\\alpha_2,\\mathrm{var}_{0,2},\\mathrm{var}_{\\alpha,2}\\right],\\left[\\alpha_3,\\mathrm{var}_{0,3},\\mathrm{var}_{\\alpha,3}\\right]\\right]$. No physical units are involved, and all quantities are dimensionless real numbers.", "solution": "The problem requires the derivation of an optimal scalar coefficient for a baseline in a policy gradient estimator, with the goal of minimizing empirical variance. We are given a set of $n$ samples, $\\{(G_i, \\hat{V}_i, z_i)\\}_{i=1}^{n}$, where $G_i$ is the return, $\\hat{V}_i = \\hat{V}(s_i)$ is a state-value estimate, and $z_i = \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i \\mid s_i)$ is the score function vector. The gradient estimator is $g_i(\\alpha) = (G_i - \\alpha \\hat{V}_i) z_i$.\n\n### Principle of Variance Reduction with Baselines\n\nThe core idea of using a baseline $b(s)$ in the policy gradient estimator $g_t = (G_t - b(s_t)) z_t$ is to reduce the variance of the gradient estimate without introducing bias. The policy gradient theorem is $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\sum_t G_t z_t]$. To ensure the estimator remains unbiased, the expectation of the baseline term must be zero. For a baseline $b(s_t)$ that depends only on the state $s_t$ and not the action $a_t$, its contribution to the expected gradient is:\n$$\n\\mathbb{E}[b(s_t) z_t] = \\mathbb{E}_{s_t} \\left[ b(s_t) \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(\\cdot|s_t)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\right]\n$$\nThe inner expectation is the expectation of the score function, which is always zero:\n$$\n\\mathbb{E}_{a_t \\sim \\pi_{\\theta}(\\cdot|s_t)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] = \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} = \\nabla_{\\theta} \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) = \\nabla_{\\theta}(1) = \\mathbf{0}\n$$\nThus, $\\mathbb{E}[b(s_t) z_t] = \\mathbf{0}$, and the population mean of the gradient estimator $\\mathbb{E}[g_t(\\alpha)]$ is independent of $\\alpha$. This independence implies that minimizing the population variance of the estimator is equivalent to minimizing its population second moment.\n\n### Objective Function Formulation and Derivation\n\nThe problem asks to find the optimal $\\alpha$ that minimizes the empirical variance, defined as the trace of the empirical covariance matrix:\n$$\n\\mathrm{Var}_{\\mathrm{emp}}(\\alpha) = \\frac{1}{n-1} \\sum_{i=1}^{n} \\| g_i(\\alpha) - \\bar{g}(\\alpha) \\|_2^2\n$$\nwhere $\\bar{g}(\\alpha) = \\frac{1}{n}\\sum_{i=1}^{n} g_i(\\alpha)$ is the sample mean.\n\nThe problem statement directs us to simplify this by minimizing the empirical second moment, $\\sum_{i=1}^{n} \\| g_i(\\alpha) \\|_2^2$. This is a common and practical simplification, as the minimizer of this simpler objective is often a good approximation to the minimizer of the true empirical variance, particularly for a large number of samples $n$. We will proceed with this simplified objective, as instructed.\n\nLet the objective function to be minimized be $J(\\alpha)$:\n$$\nJ(\\alpha) = \\sum_{i=1}^{n} \\| g_i(\\alpha) \\|_2^2 = \\sum_{i=1}^{n} \\| (G_i - \\alpha \\hat{V}_i) z_i \\|_2^2\n$$\nSince $G_i$, $\\hat{V}_i$, and $\\alpha$ are scalars, we can factor them out of the norm:\n$$\nJ(\\alpha) = \\sum_{i=1}^{n} (G_i - \\alpha \\hat{V}_i)^2 \\| z_i \\|_2^2\n$$\nExpanding the square:\n$$\nJ(\\alpha) = \\sum_{i=1}^{n} \\left( G_i^2 - 2\\alpha G_i \\hat{V}_i + \\alpha^2 \\hat{V}_i^2 \\right) \\| z_i \\|_2^2\n$$\nThis is a quadratic function of $\\alpha$. To find the minimum, we compute the derivative with respect to $\\alpha$ and set it to zero:\n$$\n\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\sum_{i=1}^{n} \\left( G_i^2 \\| z_i \\|_2^2 - 2\\alpha G_i \\hat{V}_i \\| z_i \\|_2^2 + \\alpha^2 \\hat{V}_i^2 \\| z_i \\|_2^2 \\right) = 0\n$$\n$$\n\\sum_{i=1}^{n} \\left( -2 G_i \\hat{V}_i \\| z_i \\|_2^2 + 2\\alpha \\hat{V}_i^2 \\| z_i \\|_2^2 \\right) = 0\n$$\nGrouping terms with and without $\\alpha$:\n$$\n2\\alpha \\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2 = 2 \\sum_{i=1}^{n} G_i \\hat{V}_i \\| z_i \\|_2^2\n$$\nSolving for the optimal $\\alpha$, denoted $\\alpha^*$:\n$$\n\\alpha^* = \\frac{\\sum_{i=1}^{n} G_i \\hat{V}_i \\| z_i \\|_2^2}{\\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2}\n$$\nThe second derivative, $\\frac{d^2J}{d\\alpha^2} = 2 \\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2$, is non-negative, confirming that this value of $\\alpha^*$ corresponds to a minimum, provided the denominator is non-zero.\n\n### Edge Case Handling\n\nThe expression for $\\alpha^*$ is well-defined only if the denominator is non-zero. The denominator $\\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2$ is a sum of non-negative terms. It is zero if and only if for every sample $i$, either $\\hat{V}_i = 0$ or $z_i = \\mathbf{0}$. In this case, the baseline term $\\alpha \\hat{V}_i z_i$ is zero for all $i$, meaning the gradient estimator $g_i(\\alpha)$ is independent of $\\alpha$. The objective function $J(\\alpha)$ becomes constant, and any $\\alpha$ is a minimizer. The problem specifies that in this ill-posed scenario, we should set $\\alpha = 0$, which is a sensible choice as it corresponds to using no baseline.\n\n### Computational Procedure\n\nFor each test case, the implementation will follow these steps:\n$1$. Given the data arrays for $G$, $\\hat{V}$, and $Z$, calculate the squared Euclidean norm $\\|z_i\\|_2^2$ for each score vector $z_i$.\n$2$. Compute the numerator for $\\alpha^*$ as $\\sum_{i=1}^{n} G_i \\hat{V}_i \\| z_i \\|_2^2$.\n$3$. Compute the denominator for $\\alpha^*$ as $\\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2$.\n$4$. If the denominator is zero (or numerically close to it), set $\\alpha^* = 0$. Otherwise, compute $\\alpha^*$ using the derived formula.\n$5$. Define a function to compute the sample variance $\\mathrm{var}(\\alpha) = \\frac{1}{n-1} \\sum_{i=1}^{n} \\| g_i(\\alpha) - \\bar{g}(\\alpha) \\|_2^2$. This involves:\n    a. Calculating the gradient samples $g_i(\\alpha) = (G_i - \\alpha \\hat{V}_i) z_i$ for all $i=1, \\dots, n$.\n    b. Computing the sample mean vector $\\bar{g}(\\alpha) = \\frac{1}{n}\\sum_i g_i(\\alpha)$.\n    c. Summing the squared Euclidean norms of the deviations $\\| g_i(\\alpha) - \\bar{g}(\\alpha) \\|_2^2$ and dividing by $n-1$.\n$6$. Compute the variance for the estimator with no baseline, $\\mathrm{var}_0 = \\mathrm{var}(\\alpha=0)$.\n$7$. Compute the variance for the estimator with the optimal baseline, $\\mathrm{var}_{\\alpha} = \\mathrm{var}(\\alpha=\\alpha^*)$.\n$8$. Collect the triplet $[\\alpha^*, \\mathrm{var}_0, \\mathrm{var}_{\\alpha}]$ for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the policy gradient baseline problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            # 1. Happy path\n            [1.5, 0.5, 2.0, 1.0, 0.0],\n            [1.0, 0.2, 1.8, 0.9, 0.1],\n            [[0.8, -0.3], [0.1, 0.2], [1.2, 0.5], [0.7, 0.1], [0.0, 0.0]],\n        ),\n        (\n            # 2. Boundary case where V_hat is zero\n            [2.0, 1.0, -0.5, 0.3],\n            [0.0, 0.0, 0.0, 0.0],\n            [[0.5, -0.1], [0.4, 0.3], [0.2, 0.2], [1.0, -0.5]],\n        ),\n        (\n            # 3. Case with negative correlation and 3D score function\n            [1.0, 2.0, -1.0, 0.0, 0.5, -0.2],\n            [-1.5, -1.0, 1.0, 0.2, -0.1, 0.3],\n            [[0.3, -0.7, 0.1], [0.5, 0.2, -0.4], [0.8, 0.1, 0.3],\n             [0.0, 0.6, 0.6], [1.0, -0.2, 0.0], [0.2, 0.2, -0.1]],\n        ),\n    ]\n\n    def compute_optimal_baseline(G_list, V_hat_list, Z_list):\n        \"\"\"\n        Computes the optimal alpha and variances for a single test case.\n        \"\"\"\n        G = np.array(G_list, dtype=float)\n        V_hat = np.array(V_hat_list, dtype=float)\n        Z = np.array(Z_list, dtype=float)\n        n = G.shape[0]\n\n        if n = 1:\n            # Variance is not well-defined for n = 1 with ddof=1\n            return [0.0, 0.0, 0.0]\n\n        # Calculate squared L2 norm of each score vector z_i\n        z_norm_sq = np.sum(Z**2, axis=1)\n\n        # Calculate numerator and denominator for alpha_star\n        alpha_num = np.sum(G * V_hat * z_norm_sq)\n        alpha_den = np.sum(V_hat**2 * z_norm_sq)\n\n        # Handle edge case where the denominator is zero\n        if np.isclose(alpha_den, 0):\n            alpha_opt = 0.0\n        else:\n            alpha_opt = alpha_num / alpha_den\n\n        def calculate_variance(alpha):\n            \"\"\"Helper function to compute the sample variance for a given alpha.\"\"\"\n            # Calculate gradient estimator samples g_i(alpha) = (G_i - alpha*V_hat_i) * z_i\n            coeffs = G - alpha * V_hat\n            # Use broadcasting to multiply each scalar coefficient with its corresponding z vector\n            g_samples = coeffs[:, np.newaxis] * Z\n\n            # Compute sample mean of the gradient estimators\n            g_mean = np.mean(g_samples, axis=0)\n\n            # Compute deviations from the mean\n            deviations = g_samples - g_mean\n            \n            # Sum of squared L2 norms of deviations\n            sum_sq_norms = np.sum(deviations**2)\n            \n            # The sample variance is the trace of the empirical covariance matrix\n            # with denominator n-1 (ddof=1)\n            variance = sum_sq_norms / (n - 1)\n            return variance\n\n        # Calculate variance with no baseline (alpha = 0)\n        var_0 = calculate_variance(0.0)\n\n        # Calculate variance with optimal baseline\n        var_alpha = calculate_variance(alpha_opt)\n\n        return [alpha_opt, var_0, var_alpha]\n\n    results = []\n    for case in test_cases:\n        result = compute_optimal_baseline(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists is almost correct.\n    # We just need to remove spaces to match the specified format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3163430"}, {"introduction": "Many real-world scenarios involve dynamic action spaces where certain actions become illegal depending on the current state. A naive policy might waste learning capacity on these invalid actions, leading to inefficient or incorrect behavior. This exercise [@problem_id:3158020] addresses this practical challenge by demonstrating how to correctly compute the policy gradient for a policy that uses action masking, ensuring that probabilities are normalized only over the set of legal actions and that the learning updates are focused where they matter.", "problem": "Consider a discrete-action policy in Reinforcement Learning (RL) that uses action masking to forbid illegal actions in a given state. Let the action set be $\\{1,2,3,4\\}$ and let the mask be $m \\in \\{0,1\\}^{4}$, where $m_{i} = 1$ indicates that action $i$ is legal and $m_{i} = 0$ indicates that action $i$ is illegal. The policy is parameterized by a vector of logits $\\theta \\in \\mathbb{R}^{4}$, and the probability assigned to action $a$ in state $s$ under mask $m$ is defined by\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} ,\n$$\nwhich enforces zero probability for illegal actions and normalizes only over legal actions. Assume the chosen action $a$ is legal, so that $\\pi(a \\mid s, m, \\theta)  0$.\n\nStarting from the definition of the score function and basic rules of differentiation, derive the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$ for a legal chosen action $a$ under arbitrary mask $m$, taking care to justify why any components corresponding to illegal actions vanish. Then, evaluate this gradient at\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3 .\n$$\nYour final answer must be the resulting gradient vector expressed exactly in analytic form as a single row matrix. Do not approximate; no rounding is required. No physical units are involved.", "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Action set: $\\{1, 2, 3, 4\\}$\n- Mask: $m \\in \\{0, 1\\}^4$, where $m_i = 1$ for a legal action and $m_i=0$ for an illegal action.\n- Policy parameters (logits): $\\theta \\in \\mathbb{R}^4$.\n- Policy definition: $\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}$.\n- Condition: The chosen action $a$ is legal, which implies $m_a = 1$.\n- Task 1: Derive the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$.\n- Task 2: Evaluate the gradient for the specific case:\n  - $\\theta = \\begin{pmatrix} 0  1  2  -1 \\end{pmatrix}^T$\n  - $m = \\begin{pmatrix} 1  0  1  1 \\end{pmatrix}^T$\n  - $a = 3$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the theory of Reinforcement Learning, specifically concerning policy gradient methods. The formulation of a masked softmax policy and the calculation of its score function ($\\nabla_{\\theta} \\ln \\pi$) are standard and correct techniques in the field.\n- **Well-Posed:** The problem is clearly defined. It provides a specific functional form for the policy, all necessary parameters for evaluation, and a clear objective. The condition that the chosen action $a$ is legal ensures that the policy probability $\\pi(a | \\dots)$ is greater than $0$, and thus its logarithm is well-defined. A unique, stable, and meaningful solution exists.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe primary objective is to derive the gradient of the log-policy, also known as the score function. The policy is given by:\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nTo find the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$, we first compute the logarithm of the policy probability.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln \\left( \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\right)\n$$\nUsing the property of logarithms $\\ln(x/y) = \\ln(x) - \\ln(y)$, we get:\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln(\\exp(\\theta_{a}) \\, m_{a}) - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe problem states that the chosen action $a$ is legal, which means $m_a = 1$. This simplifies the first term: $\\ln(\\exp(\\theta_{a}) \\cdot 1) = \\theta_a$.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nNow, we compute the gradient of this expression with respect to the parameter vector $\\theta$. The gradient is a vector whose $j$-th component is the partial derivative with respect to $\\theta_j$.\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\frac{\\partial}{\\partial \\theta_j} \\left( \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) \\right)\n$$\nWe differentiate term by term:\n$$\n\\frac{\\partial}{\\partial \\theta_j} (\\theta_a) = \\delta_{aj}\n$$\nwhere $\\delta_{aj}$ is the Kronecker delta, which is $1$ if $j=a$ and $0$ otherwise.\n\nFor the second term, we use the chain rule for differentiation ($\\frac{d}{dx}\\ln(f(x)) = \\frac{f'(x)}{f(x)}$):\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\frac{1}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\cdot \\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe derivative of the sum with respect to $\\theta_j$ is:\n$$\n\\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\exp(\\theta_j) \\, m_j\n$$\nCombining these, the derivative of the second term is:\n$$\n\\frac{\\exp(\\theta_j) \\, m_j}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nThis expression is exactly the definition of the policy probability for action $j$, $\\pi(j \\mid s, m, \\theta)$.\n\nSo, the $j$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\delta_{aj} - \\pi(j \\mid s, m, \\theta)\n$$\nThis is the general form of the score function for a masked softmax policy.\n\nThe problem asks to justify why components corresponding to illegal actions vanish. Let $k$ be an index for an illegal action, meaning $m_k=0$. The $k$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = \\delta_{ak} - \\pi(k \\mid s, m, \\theta)\n$$\nSince $a$ is a legal action ($m_a=1$) and $k$ is an illegal action ($m_k=0$), we must have $a \\neq k$. Therefore, $\\delta_{ak} = 0$.\nThe probability of the illegal action $k$ is:\n$$\n\\pi(k \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{k}) \\, m_{k}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = \\frac{\\exp(\\theta_{k}) \\cdot 0}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = 0\n$$\nThus, the $k$-th component of the gradient becomes:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = 0 - 0 = 0\n$$\nThis confirms that the gradient components for all illegal actions are zero.\n\n### Gradient Evaluation\n\nWe are given the following values:\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3\n$$\nFirst, let's compute the normalization term, $Z = \\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}$:\n$$\nZ = \\exp(\\theta_1)m_1 + \\exp(\\theta_2)m_2 + \\exp(\\theta_3)m_3 + \\exp(\\theta_4)m_4\n$$\n$$\nZ = \\exp(0) \\cdot 1 + \\exp(1) \\cdot 0 + \\exp(2) \\cdot 1 + \\exp(-1) \\cdot 1\n$$\n$$\nZ = 1 \\cdot 1 + 0 + e^2 \\cdot 1 + e^{-1} \\cdot 1 = 1 + e^2 + e^{-1}\n$$\nNext, we compute the probability vector $\\vec{\\pi}$ whose components are $\\pi(j \\mid s, m, \\theta)$:\n$$\n\\pi(1) = \\frac{\\exp(0) \\cdot 1}{Z} = \\frac{1}{Z}\n$$\n$$\n\\pi(2) = \\frac{\\exp(1) \\cdot 0}{Z} = 0\n$$\n$$\n\\pi(3) = \\frac{\\exp(2) \\cdot 1}{Z} = \\frac{e^2}{Z}\n$$\n$$\n\\pi(4) = \\frac{\\exp(-1) \\cdot 1}{Z} = \\frac{e^{-1}}{Z}\n$$\nThe gradient vector is given by $\\nabla_{\\theta} \\ln \\pi(a=3 \\mid \\dots) = \\mathbf{e}_3 - \\vec{\\pi}$, where $\\mathbf{e}_3$ is the one-hot vector $(0, 0, 1, 0)^T$.\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\pi(1) \\\\ \\pi(2) \\\\ \\pi(3) \\\\ \\pi(4) \\end{pmatrix} = \\begin{pmatrix} -\\pi(1) \\\\ -\\pi(2) \\\\ 1 - \\pi(3) \\\\ -\\pi(4) \\end{pmatrix}\n$$\nSubstituting the probabilities:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ 1 - e^2/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (Z - e^2)/Z \\\\ -e^{-1}/Z \\end{pmatrix}\n$$\nSubstitute $Z = 1 + e^2 + e^{-1}$:\n$$\nZ - e^2 = (1 + e^2 + e^{-1}) - e^2 = 1 + e^{-1}\n$$\nSo the gradient vector is:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (1+e^{-1})/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\frac{1}{Z} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix}\n$$\nTo express this in a more elegant form, we can substitute $Z = 1 + e^2 + e^{-1}$ and clear the negative exponents by multiplying the numerator and denominator by $e$.\n$$\n\\frac{1}{Z} = \\frac{1}{1 + e^2 + e^{-1}} = \\frac{e}{e(1 + e^2 + e^{-1})} = \\frac{e}{e + e^3 + 1}\n$$\nNow multiply this scalar into the vector:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{e}{1+e+e^3} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e(1+e^{-1}) \\\\ e(-e^{-1}) \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e+1 \\\\ -1 \\end{pmatrix}\n$$\nThe components of the gradient vector are:\n\\begin{itemize}\n    \\item Component $1$: $-\\frac{e}{1+e+e^3}$\n    \\item Component $2$: $0$\n    \\item Component $3$: $\\frac{e+1}{1+e+e^3}$\n    \\item Component $4$: $-\\frac{1}{1+e+e^3}$\n\\end{itemize}\nThe final answer is to be provided as a row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{e}{e^3+e+1}  0  \\frac{e+1}{e^3+e+1}  -\\frac{1}{e^3+e+1} \\end{pmatrix}}\n$$", "id": "3158020"}]}