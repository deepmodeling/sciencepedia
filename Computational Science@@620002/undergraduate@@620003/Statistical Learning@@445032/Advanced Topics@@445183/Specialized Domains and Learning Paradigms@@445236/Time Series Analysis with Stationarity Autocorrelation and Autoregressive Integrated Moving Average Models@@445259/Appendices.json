{"hands_on_practices": [{"introduction": "A critical step in building an Autoregressive Integrated Moving Average (ARIMA) model is selecting the correct order of differencing, denoted by $d$. While differencing is essential for making a non-stationary series stationary, applying it unnecessarily—a mistake known as overdifferencing—can harm your model. This practice will show you that overdifferencing an integrated process introduces a specific, artificial structure that is easily recognizable in its autocorrelation plots [@problem_id:2372420]. Mastering the ability to spot this signature is a key diagnostic skill that helps ensure your model is appropriately specified.", "problem": "Consider a financial time series of log prices $\\{X_t\\}$ that follows a random walk: $X_t = X_{t-1} + u_t$, where $\\{u_t\\}$ is independent and identically distributed with mean $0$ and variance $\\sigma^2$. An analyst mistakenly applies second differencing to $\\{X_t\\}$ and studies the transformed series $Y_t = \\Delta^2 X_t = X_t - 2X_{t-1} + X_{t-2}$. Which of the following best describes the theoretical behavior of the autocorrelation function (ACF) and the partial autocorrelation function (PACF) of $\\{Y_t\\}$ implied by this mis-differencing?\n\nA. The ACF shows a single pronounced negative spike at lag $1$ of about $-0.5$ and is essentially $0$ for lags greater than $1$; the PACF decays gradually toward $0$, potentially with alternating signs.\n\nB. The ACF decays very slowly and positively from near $1$, characteristic of a near-unit-root autoregression; the PACF has a single large positive spike at lag $1$ and is essentially $0$ afterward.\n\nC. The ACF shows a single pronounced positive spike at lag $1$ near $+0.5$ and is essentially $0$ for lags greater than $1$; the PACF decays gradually toward $0$.\n\nD. Both the ACF and the PACF cut off after lag $2$, indicating a short-memory process with only two nonzero lags of dependence.\n\nE. The ACF alternates in sign with a slowly decaying oscillation over many lags; the PACF has a single large negative spike at lag $1$ and is essentially $0$ afterward.", "solution": "The process $\\{X_t\\}$ is a random walk:\n$$X_t = X_{t-1} + u_t$$\nThis means that the first difference of $\\{X_t\\}$ is the white noise process $\\{u_t\\}$:\n$$\\Delta X_t = X_t - X_{t-1} = u_t$$\nThe process $\\{u_t\\}$ is stationary as it is i.i.d. with constant mean $E[u_t]=0$ and constant variance $Var(u_t)=\\sigma^2$.\n\nThe analyst mistakenly applies a second difference to $\\{X_t\\}$, yielding the process $\\{Y_t\\}$:\n$$Y_t = \\Delta^2 X_t = \\Delta(\\Delta X_t) = \\Delta(u_t) = u_t - u_{t-1}$$\nThis shows that $\\{Y_t\\}$ is a Moving Average process of order $1$, denoted as MA($1$). The general form of an MA($1$) process is $Y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1}$. In our case, the noise term is $\\epsilon_t = u_t$ and the MA parameter is $\\theta_1 = -1$.\n\nTo determine the ACF and PACF of $\\{Y_t\\}$, we first compute its autocovariances.\nThe mean of $\\{Y_t\\}$ is:\n$$E[Y_t] = E[u_t - u_{t-1}] = E[u_t] - E[u_{t-1}] = 0 - 0 = 0$$\nThe variance, or autocovariance at lag $0$, is $\\gamma_0$:\n$$\\gamma_0 = Var(Y_t) = E[Y_t^2] = E[(u_t - u_{t-1})^2] = E[u_t^2 - 2u_t u_{t-1} + u_{t-1}^2]$$\nSince $\\{u_t\\}$ is i.i.d. with mean $0$, $E[u_t u_{t-1}] = E[u_t]E[u_{t-1}] = 0$. Therefore:\n$$\\gamma_0 = E[u_t^2] + E[u_{t-1}^2] = Var(u_t) + Var(u_{t-1}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2$$\nThe autocovariance at lag $1$, $\\gamma_1$, is:\n$$\\gamma_1 = Cov(Y_t, Y_{t-1}) = E[Y_t Y_{t-1}] = E[(u_t - u_{t-1})(u_{t-1} - u_{t-2})]$$\n$$= E[u_t u_{t-1} - u_t u_{t-2} - u_{t-1}^2 + u_{t-1} u_{t-2}]$$\nDue to the i.i.d. property of $\\{u_t\\}$, all cross-product terms with different time indices have an expectation of zero.\n$$\\gamma_1 = 0 - 0 - E[u_{t-1}^2] + 0 = -Var(u_{t-1}) = -\\sigma^2$$\nThe autocovariance at lag $k \\ge 2$, $\\gamma_k$, is:\n$$\\gamma_k = E[Y_t Y_{t-k}] = E[(u_t - u_{t-1})(u_{t-k} - u_{t-k-1})]$$\nFor $k \\ge 2$, all time indices in the expansion ($t, t-1, t-k, t-k-1$) are distinct. Thus, the expectation of every term is zero.\n$$\\gamma_k = 0 \\quad \\text{for } k \\ge 2$$\n\nNow we compute the autocorrelation function (ACF), $\\rho_k = \\gamma_k / \\gamma_0$.\nFor lag $1$:\n$$\\rho_1 = \\frac{\\gamma_1}{\\gamma_0} = \\frac{-\\sigma^2}{2\\sigma^2} = -0.5$$\nFor lags $k \\ge 2$:\n$$\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\frac{0}{2\\sigma^2} = 0$$\nThe theoretical ACF for $\\{Y_t\\}$ has a single non-zero value at lag $1$ of exactly $-0.5$ and cuts off to zero for all subsequent lags. This is the defining characteristic of an MA($1$) process.\n\nNext, we address the partial autocorrelation function (PACF). For a general MA($q$) process, the PACF tails off, meaning it decays towards zero rather than cutting off abruptly. The process $Y_t = u_t - u_{t-1}$ is a special case of an MA($1$) process with parameter $\\theta_1 = -1$. This is a non-invertible MA process, as invertibility requires $|\\theta_1| < 1$. The PACF, $\\phi_{kk}$, can be shown to follow the pattern:\n$$\\phi_{kk} = -\\frac{1}{k+1}$$\nFor example:\n- $\\phi_{11} = \\rho_1 = -0.5 = -1/2$\n- $\\phi_{22} = -1/3 \\approx -0.333$\n- $\\phi_{33} = -1/4 = -0.25$\nThis demonstrates a gradual, monotonic decay towards $0$ as the lag $k$ increases. The PACF does not alternate in sign.\n\nSummary of theoretical behavior:\n- **ACF**: Cuts off after lag $1$, with $\\rho_1 = -0.5$.\n- **PACF**: Decays gradually towards $0$ (specifically, $\\phi_{kk} = -1/(k+1)$).\n\nOption-by-Option Analysis\n\nA. The ACF shows a single pronounced negative spike at lag $1$ of about $-0.5$ and is essentially $0$ for lags greater than $1$; the PACF decays gradually toward $0$, potentially with alternating signs.\n- ACF description: \"single pronounced negative spike at lag $1$ of about $-0.5$\" and \"essentially $0$ for lags greater than $1$\". This is a precise description of our derived ACF.\n- PACF description: \"decays gradually toward $0$\". This is a correct description of the PACF of an MA process. The phrase \"potentially with alternating signs\" is a general statement about decaying PACFs; in this specific case, the decay is monotonic and negative, which is a form of gradual decay. This option aligns perfectly with our derivation.\n- Verdict: **Correct**.\n\nB. The ACF decays very slowly and positively from near $1$, characteristic of a near-unit-root autoregression; the PACF has a single large positive spike at lag $1$ and is essentially $0$ afterward.\n- ACF description: Incorrect. The ACF cuts off after lag $1$ and is negative. A slowly decaying positive ACF is characteristic of a process with a unit root or near-unit root in its autoregressive part, such as an AR($1$) with $\\phi \\approx 1$.\n- PACF description: Incorrect. The PACF decays gradually. A single spike in the PACF is characteristic of an AR process.\n- Verdict: **Incorrect**.\n\nC. The ACF shows a single pronounced positive spike at lag $1$ near $+0.5$ and is essentially $0$ for lags greater than $1$; the PACF decays gradually toward $0$.\n- ACF description: Incorrect. The spike at lag $1$ is negative ($-0.5$), not positive. A positive spike would imply a process like $Y_t = u_t + u_{t-1}$.\n- Verdict: **Incorrect**.\n\nD. Both the ACF and the PACF cut off after lag $2$, indicating a short-memory process with only two nonzero lags of dependence.\n- ACF description: Incorrect. The ACF cuts off after lag $1$, not $2$.\n- PACF description: Incorrect. The PACF decays gradually; it does not cut off.\n- Verdict: **Incorrect**.\n\nE. The ACF alternates in sign with a slowly decaying oscillation over many lags; the PACF has a single large negative spike at lag $1$ and is essentially $0$ afterward.\n- ACF description: Incorrect. The ACF cuts off. A decaying, oscillating ACF is characteristic of an AR($2$) process with complex roots or an AR process with negative coefficients.\n- PACF description: Incorrect. The PACF decays gradually. A single spike in the PACF is characteristic of an AR($1$) process.\n- Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2372420"}, {"introduction": "Once a model is identified, a primary application is forecasting future values. However, a point forecast alone is incomplete; understanding its uncertainty is crucial for decision-making. This computational exercise focuses on quantifying the reliability of forecasts from an autoregressive model [@problem_id:3187694]. By deriving and implementing the formula for forecast error variance, you will gain a concrete understanding of how forecast uncertainty accumulates over longer horizons, particularly for highly persistent processes where the autoregressive parameter $\\phi$ is close to $1$.", "problem": "Consider a first-order autoregressive process (AR) as a special case of the Autoregressive Integrated Moving Average (ARIMA) family, defined by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$, where $\\{\\varepsilon_t\\}$ is independent and identically distributed (i.i.d.) zero-mean noise with variance $\\sigma_\\varepsilon^2$, and $|\\phi| < 1$ ensures weak stationarity. In near-unit-root settings, such as $\\phi$ very close to $1$, the accumulation of shocks over time can produce substantial forecast uncertainty at long horizons even though the process is technically stationary.\n\nYour task is to implement a program that, for a set of test cases, quantifies long-horizon forecast uncertainty for the near-unit-root case and related comparators. For each test case, you are given numerical values of the autoregressive parameter $\\phi$, the noise variance $\\sigma_\\varepsilon^2$, the current observed value $x_t$, and a forecast horizon $h \\in \\mathbb{N}$. Starting only from the model definition and the properties of i.i.d. noise, compute the following for each test case:\n\n1. The $h$-step-ahead forecast error standard deviation $\\sqrt{\\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t])}$.\n2. The dimensionless ratio $r$ defined as the $0.95$-level forecast half-width divided by the magnitude of the forecast mean, that is $r = z_{0.975} \\cdot \\sqrt{\\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t])} \\big/ \\left|\\mathbb{E}[X_{t+h} \\mid X_t = x_t]\\right|$, where $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution. Express $r$ as a decimal number without a percentage sign.\n\nUse only the foundational definitions above; do not use any pre-specified forecasting formulas beyond the model and i.i.d. properties. Assume $\\varepsilon_t$ is Gaussian to justify the use of $z_{0.975}$.\n\nTest suite:\n- Case A (near-unit-root, moderate horizon): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 10$.\n- Case B (near-unit-root, long horizon): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 100$.\n- Case C (closer to unit root, long horizon): $\\phi = 0.999$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 100$.\n- Case D (near-unit-root, reduced noise, long horizon): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 0.1$, $x_t = 100.0$, $h = 100$.\n- Case E (more strongly stationary comparator): $\\phi = 0.9$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 100$.\n- Case F (near-unit-root, small current level): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 1.0$, $h = 100$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element comma-separated list of floats $[\\text{sd}, r]$ printed with six decimal places, for example: $[[\\text{sd}_A,\\ r_A],[\\text{sd}_B,\\ r_B],\\dots]$. No additional text should be printed. There are no physical units involved, and all angles are irrelevant to this problem.", "solution": "The model is a first-order autoregressive process, AR($1$), given by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$, where $\\{\\varepsilon_t\\}$ is a sequence of independent and identically distributed (i.i.d.) random variables with mean $\\mathbb{E}[\\varepsilon_t] = 0$ and variance $\\operatorname{Var}(\\varepsilon_t) = \\sigma_\\varepsilon^2$. We are given the condition $|\\phi| < 1$, which ensures weak stationarity. Our objective is to compute the $h$-step-ahead forecast mean and the variance of the forecast error, conditional on the observation $X_t = x_t$.\n\nFirst, we derive the $h$-step-ahead forecast mean, which is the conditional expectation $\\mathbb{E}[X_{t+h} \\mid X_t = x_t]$. We apply the law of total expectation iteratively.\nFor a forecast horizon of $h=1$:\n$$ \\mathbb{E}[X_{t+1} \\mid X_t = x_t] = \\mathbb{E}[\\phi X_t + \\varepsilon_{t+1} \\mid X_t = x_t] $$\nBy the linearity of expectation and noting that $X_t$ is the known value $x_t$ in the conditional information set:\n$$ \\mathbb{E}[X_{t+1} \\mid X_t = x_t] = \\phi x_t + \\mathbb{E}[\\varepsilon_{t+1} \\mid X_t = x_t] $$\nSince the noise term $\\varepsilon_{t+1}$ is independent of $X_t$, its conditional expectation equals its unconditional expectation, which is $0$.\n$$ \\mathbb{E}[X_{t+1} \\mid X_t = x_t] = \\phi x_t $$\nFor a forecast horizon of $h=2$:\n$$ \\mathbb{E}[X_{t+2} \\mid X_t = x_t] = \\mathbb{E}[\\phi X_{t+1} + \\varepsilon_{t+2} \\mid X_t = x_t] = \\phi \\mathbb{E}[X_{t+1} \\mid X_t = x_t] + \\mathbb{E}[\\varepsilon_{t+2} \\mid X_t = x_t] $$\nSubstituting the result for $h=1$ and using the independence of $\\varepsilon_{t+2}$ yields:\n$$ \\mathbb{E}[X_{t+2} \\mid X_t = x_t] = \\phi(\\phi x_t) + 0 = \\phi^2 x_t $$\nBy induction, the general formula for the $h$-step-ahead forecast mean is:\n$$ \\mathbb{E}[X_{t+h} \\mid X_t = x_t] = \\phi^h x_t $$\n\nNext, we derive the variance of the $h$-step-ahead forecast error, defined as $\\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t])$. This quantity is equivalent to the conditional variance $\\operatorname{Var}(X_{t+h} \\mid X_t = x_t)$. To find this, we first express $X_{t+h}$ in terms of $X_t$ and the subsequent noise terms by repeated substitution:\n$$ X_{t+1} = \\phi X_t + \\varepsilon_{t+1} $$\n$$ X_{t+2} = \\phi X_{t+1} + \\varepsilon_{t+2} = \\phi(\\phi X_t + \\varepsilon_{t+1}) + \\varepsilon_{t+2} = \\phi^2 X_t + \\phi\\varepsilon_{t+1} + \\varepsilon_{t+2} $$\nIn general, this recursive substitution expands to:\n$$ X_{t+h} = \\phi^h X_t + \\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k} $$\nThe forecast error, which we denote as $e_{t,h}$, is the difference between the actual future value $X_{t+h}$ and its forecast conditional on information at time $t$:\n$$ e_{t,h} = X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t] = \\left(\\phi^h x_t + \\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k}\\right) - \\phi^h x_t = \\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k} $$\nThe variance of this error is calculated using the properties of the i.i.d. noise terms. Since the terms $\\varepsilon_{t+1}, \\dots, \\varepsilon_{t+h}$ are mutually independent:\n$$ \\operatorname{Var}(e_{t,h}) = \\operatorname{Var}\\left(\\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k}\\right) = \\sum_{k=0}^{h-1} \\operatorname{Var}(\\phi^k \\varepsilon_{t+h-k}) $$\nUsing the variance property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ and the given fact $\\operatorname{Var}(\\varepsilon_j) = \\sigma_\\varepsilon^2$:\n$$ \\operatorname{Var}(e_{t,h}) = \\sum_{k=0}^{h-1} (\\phi^k)^2 \\sigma_\\varepsilon^2 = \\sigma_\\varepsilon^2 \\sum_{k=0}^{h-1} (\\phi^2)^k $$\nThe summation is a finite geometric series with $h$ terms, a first term of $1$, and a common ratio of $\\phi^2$. The sum is given by the formula $\\frac{1 - (\\text{ratio})^{\\text{num_terms}}}{1 - \\text{ratio}}$, which in our case is $\\frac{1 - (\\phi^2)^h}{1 - \\phi^2}$.\nTherefore, the $h$-step-ahead forecast error variance, which we denote as $V_h$, is:\n$$ V_h = \\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t]) = \\sigma_\\varepsilon^2 \\left(\\frac{1 - \\phi^{2h}}{1-\\phi^2}\\right) $$\nWith these derivations, the two quantities to be computed for each test case are:\n1. The forecast error standard deviation, $\\text{sd} = \\sqrt{V_h}$.\n2. The dimensionless ratio $r = z_{0.975} \\cdot \\text{sd} / \\left|\\mathbb{E}[X_{t+h} \\mid X_t = x_t]\\right|$, where $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution, whose use is justified by the problem's assumption that $\\varepsilon_t$ is Gaussian.\n\nThese derived formulas are implemented in the following program to compute the results for each specified test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes long-horizon forecast uncertainty metrics for an AR(1) process\n    for a given suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (phi, sigma_eps_sq, x_t, h)\n    test_cases = [\n        (0.99, 1.0, 100.0, 10),   # Case A\n        (0.99, 1.0, 100.0, 100),  # Case B\n        (0.999, 1.0, 100.0, 100), # Case C\n        (0.99, 0.1, 100.0, 100),  # Case D\n        (0.9, 1.0, 100.0, 100),   # Case E\n        (0.99, 1.0, 1.0, 100),    # Case F\n    ]\n\n    # The 0.975 quantile of the standard normal distribution, for a 95% interval.\n    z_0975 = norm.ppf(0.975)\n\n    results = []\n    for phi, sigma_eps_sq, x_t, h in test_cases:\n        # 1. Calculate the h-step-ahead forecast mean.\n        # E[X_{t+h} | X_t = x_t] = phi^h * x_t\n        forecast_mean = (phi**h) * x_t\n\n        # 2. Calculate the h-step-ahead forecast error variance.\n        # Var(error) = sigma_eps^2 * (1 - phi^(2h)) / (1 - phi^2)\n        # The condition |phi| < 1 ensures the denominator is non-zero.\n        forecast_error_variance = (sigma_eps_sq * (1.0 - phi**(2 * h))) / (1.0 - phi**2)\n\n        # 3. Calculate the forecast error standard deviation (sd).\n        forecast_error_std_dev = np.sqrt(forecast_error_variance)\n\n        # 4. Calculate the dimensionless ratio r.\n        # r = z * sd / |mean|\n        # Use np.abs to handle the magnitude of the mean.\n        # The mean will not be exactly zero for the given test cases,\n        # so division by zero is not a concern.\n        if np.abs(forecast_mean) < 1e-12: # Handle computationally near-zero means\n             # In this theoretical case, the ratio would be effectively infinite.\n             # We can represent it with a very large number or nan for robustness,\n             # although not needed for the given test suite.\n             r_ratio = float('inf')\n        else:\n             r_ratio = z_0975 * forecast_error_std_dev / np.abs(forecast_mean)\n\n        results.append([forecast_error_std_dev, r_ratio])\n\n    # Format the results into the exact required output string.\n    # e.g., [[sd_A, r_A],[sd_B, r_B],...] with 6 decimal places.\n    output_str = '[' + ','.join([f\"[{sd:.6f},{r:.6f}]\" for sd, r in results]) + ']'\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "3187694"}, {"introduction": "This final practice moves from individual techniques to a complete, end-to-end modeling workflow designed to solve a practical problem: detecting unusual spikes in a time series of counts. You will simulate a real-world scenario, such as monitoring social media hashtag usage for unexpected events [@problem_id:3187631]. This capstone exercise integrates variance stabilization, differencing, ARMA model fitting, and residual analysis, demonstrating how these components work in concert to transform raw data into actionable insights.", "problem": "Consider a discrete-time count process $x_t$ representing hashtag usage frequency per unit time. In many practical cases, count data can be reasonably approximated as following a Poisson-like mechanism with $\\operatorname{Var}(x_t)$ growing with $\\operatorname{E}(x_t)$, which motivates variance stabilization before modeling. Let $z_t$ denote a variance-stabilized transformation of $x_t$, and let $w_t$ denote the differenced series of order $d$, used to induce stationarity for integrated models.\n\nBase definitions and facts:\n- A time series $y_t$ is wide-sense stationary if $\\operatorname{E}(y_t) = \\mu$ for all $t$ and $\\operatorname{Cov}(y_t, y_{t+h})$ depends only on $h$, not on $t$.\n- The autocorrelation function (ACF) at lag $h$ is $\\rho(h) = \\frac{\\operatorname{Cov}(y_t,y_{t+h})}{\\operatorname{Var}(y_t)}$.\n- An Autoregressive Integrated Moving Average (ARIMA) model of order $(p,d,q)$ for $x_t$ means that the $d$-times differenced, possibly transformed series satisfies an Autoregressive Moving Average (ARMA)$(p,q)$ dynamics: $w_t = \\mu + \\sum_{i=1}^{p}\\phi_i w_{t-i} + \\epsilon_t + \\sum_{j=1}^{q}\\theta_j \\epsilon_{t-j}$, where $\\epsilon_t$ is white noise with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- For Poisson-like counts, the square-root transform $z_t = \\sqrt{x_t + c}$ with a small constant $c$ yields approximately constant variance; with the Delta method, for large $\\lambda$, $\\operatorname{Var}(\\sqrt{X+c}) \\approx \\frac{1}{4}$ when $X \\sim \\operatorname{Poisson}(\\lambda)$.\n- Differencing of order $d=1$ is defined by $w_t = z_t - z_{t-1}$ for $t \\ge 1$ (and produces a series indexed by $t=1,\\dots,T-1$). A shock in the original series at index $t^\\star$ manifests as a large $|w_{t^\\star}|$ or $|w_{t^\\star+1}|$, depending on the direction and persistence of the change.\n\nTask:\nImplement a program that, for each provided test case, treats hashtag frequency as a count time series, stabilizes variance, fits an ARIMA$(1,1,1)$ model via conditional Gaussian estimation on the differenced series (equivalently, fit an ARMA$(1,1)$ to $w_t$), and assesses shocks as large standardized residuals. Your implementation must adhere to the following modeling steps derived from first principles:\n\n1. Variance stabilization: Given counts $x_t$, produce $z_t$ using either the square-root transform $z_t = \\sqrt{x_t + 0.5}$ or the logarithmic transform $z_t = \\log(x_t + 1)$, as specified per test case.\n2. Differencing: For differencing order $d=1$, compute $w_t = z_t - z_{t-1}$ for $t=1,\\dots,T-1$.\n3. Centering: Compute $y_t = w_t - \\bar{w}$, where $\\bar{w}$ is the sample mean of $w_t$, to align with the ARMA$(1,1)$ form around zero mean.\n4. ARMA$(1,1)$ fitting by conditional sum-of-squares under Gaussian noise: Minimize the average squared innovations\n   $$J(\\phi,\\theta) = \\frac{1}{N}\\sum_{t=0}^{N-1}\\epsilon_t^2,$$\n   with the recursion\n   $$\\epsilon_t = y_t - \\phi y_{t-1} - \\theta \\epsilon_{t-1},$$\n   using initial conditions $y_{-1} = 0$ and $\\epsilon_{-1} = 0$. Constrain $\\phi,\\theta \\in (-1,1)$ to respect stationarity and invertibility in the conditional sense.\n5. Residual standardization: Estimate $\\sigma^2$ by $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{t=0}^{N-1}\\epsilon_t^2$ and compute standardized residuals $r_t = \\epsilon_t / \\hat{\\sigma}$.\n6. Shock identification: Using a specified threshold $T$ (dimensionless), declare a shock at original index $t = \\tau + d$ whenever $|r_\\tau| \\ge T$ for residual index $\\tau$ in the differenced series. Use $0$-based indexing for reported shock positions.\n\nTest suite:\nFor each case, generate the count time series $\\{x_t\\}$ exactly as specified, using $0$-based indexing and integer counts via flooring. Let $\\lfloor\\cdot\\rfloor$ denote the floor function.\n\n- Case A (happy path, moderate counts with two shocks):\n  - Length $T = 120$.\n  - Baseline: $b_t = 20 + 5\\sin\\left(\\frac{2\\pi t}{20}\\right)$ for $t=0,\\dots,119$.\n  - Shocks: $s_t = 40$ if $t \\in \\{30,75\\}$ and $s_t = 0$ otherwise.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor + s_t$.\n  - Variance stabilization: square-root $z_t = \\sqrt{x_t + 0.5}$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\n- Case B (edge case, no shocks):\n  - Length $T = 120$.\n  - Baseline: $b_t = 25 + 3\\sin\\left(\\frac{2\\pi t}{30}\\right)$ for $t=0,\\dots,119$.\n  - Shocks: $s_t = 0$ for all $t$.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor$.\n  - Variance stabilization: square-root $z_t = \\sqrt{x_t + 0.5}$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\n- Case C (boundary condition, short series with one shock):\n  - Length $T = 20$.\n  - Baseline: $b_t = 15 + 2\\sin\\left(\\frac{2\\pi t}{15}\\right)$ for $t=0,\\dots,19$.\n  - Shock: $s_t = 30$ if $t=10$ and $s_t = 0$ otherwise.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor + s_t$.\n  - Variance stabilization: square-root $z_t = \\sqrt{x_t + 0.5}$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\n- Case D (high counts, logarithmic stabilization, one shock):\n  - Length $T = 100$.\n  - Baseline: $b_t = 200 + 10\\sin\\left(\\frac{2\\pi t}{25}\\right)$ for $t=0,\\dots,99$.\n  - Shock: $s_t = 300$ if $t=60$ and $s_t = 0$ otherwise.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor + s_t$.\n  - Variance stabilization: logarithmic $z_t = \\log(x_t + 1)$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\nRequired output:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the list of detected shock indices (using $0$-based indexing). For example, the final output must be of the form\n$[\\,[\\text{indices for Case A}],\\,[\\text{indices for Case B}],\\,[\\text{indices for Case C}],\\,[\\text{indices for Case D}]\\,]$,\nwith each inner list consisting solely of integers in ascending order.\n\nNo physical units are involved; the threshold is dimensionless. Angles in trigonometric functions are in radians by construction via the $\\sin$ function’s argument. All reported quantities must be integers or lists of integers. The computation must be deterministic and require no random number generation.", "solution": "### 1. Time Series Generation and Transformation\n\nFor each test case, a discrete-time count process $x_t$ of length $T$ is generated. The process is defined by a baseline component $b_t$ and a shock component $s_t$, such that the final count at time $t$ is $x_t = \\lfloor b_t \\rfloor + s_t$.\n\nCount data often exhibit heteroscedasticity, where the variance is a function of the mean. For a Poisson process, $\\operatorname{Var}(x_t) = \\operatorname{E}(x_t) = \\lambda_t$. To satisfy the constant variance assumption of many time series models, a variance-stabilizing transformation is applied. The problem specifies two such transformations:\n1.  **Square-root transform:** $z_t = \\sqrt{x_t + c}$, with $c=0.5$. This transform is motivated by the Delta method, which shows that for a Poisson-distributed random variable $X$ with mean $\\lambda$, the variance of $\\sqrt{X}$ is approximately constant ($\\approx 1/4$) for large $\\lambda$.\n2.  **Logarithmic transform:** $z_t = \\log(x_t + 1)$. This transform is effective when the standard deviation of the data is proportional to the mean.\n\nThe resulting series is $z_t$.\n\n### 2. Stationarity Induction and Centering\n\nThe \"I\" in ARIMA stands for \"Integrated\" and implies that the raw time series is non-stationary but its differences are stationary. Stationarity is induced by differencing the transformed series $z_t$. For a differencing order of $d=1$, the new series $w_t$ is defined as:\n$$w_t = z_t - z_{t-1}, \\quad \\text{for } t = 1, \\dots, T-1$$\nThis operation produces a series of length $N = T-1$. A shock at an original index $t^\\star$ introduces large-magnitude values in $w_{t^\\star}$ and $w_{t^\\star+1}$.\n\nThe ARMA model is to be fitted to a zero-mean series. Therefore, the series $w_t$ is centered by subtracting its sample mean, $\\bar{w}$:\n$$y_\\tau = w_{\\tau+1} - \\bar{w}, \\quad \\text{for } \\tau = 0, \\dots, N-1$$\nwhere $\\bar{w} = \\frac{1}{N}\\sum_{t=1}^{T-1} w_t$. The new series $y_\\tau$ has a sample mean of zero. Note the re-indexing from $t$ to $\\tau$ for implementation convenience, where $\\tau = t-1$.\n\n### 3. ARMA(1,1) Parameter Estimation\n\nAn Autoregressive Moving Average ARMA(1,1) model is specified for the stationary, centered series $y_\\tau$. The model is given by:\n$$y_\\tau = \\phi y_{\\tau-1} + \\epsilon_\\tau + \\theta \\epsilon_{\\tau-1}$$\nwhere $\\epsilon_\\tau$ is a white noise process with $\\epsilon_\\tau \\sim \\mathcal{N}(0, \\sigma^2)$, and $\\phi$ and $\\theta$ are the autoregressive and moving average parameters, respectively.\n\nTo estimate $(\\phi, \\theta)$, we use the method of conditional sum of squares (CSS). This involves minimizing an objective function $J(\\phi, \\theta)$ representing the mean squared innovations (residuals). The innovations are defined by rearranging the model equation:\n$$\\epsilon_\\tau = y_\\tau - \\phi y_{\\tau-1} - \\theta \\epsilon_{\\tau-1}$$\nThe objective function to minimize is:\n$$J(\\phi, \\theta) = \\frac{1}{N}\\sum_{\\tau=0}^{N-1}\\epsilon_\\tau^2$$\nThe recursion requires initial conditions. As specified, we use $y_{-1} = 0$ and $\\epsilon_{-1} = 0$. The first innovation is then $\\epsilon_0 = y_0$.\n\nThis is a numerical optimization problem. We seek $(\\hat{\\phi}, \\hat{\\theta})$ such that:\n$$(\\hat{\\phi}, \\hat{\\theta}) = \\underset{\\phi, \\theta}{\\operatorname{argmin}} \\, J(\\phi, \\theta)$$\nThe optimization is performed subject to the constraints $\\phi \\in (-1, 1)$ and $\\theta \\in (-1, 1)$, which are necessary conditions for the stationarity and invertibility of the ARMA process. A numerical solver, such as a quasi-Newton method with box constraints (e.g., L-BFGS-B), is suitable for this task.\n\n### 4. Shock Detection via Standardized Residuals\n\nAfter obtaining the optimal parameters $(\\hat{\\phi}, \\hat{\\theta})$, the final residual series $\\hat{\\epsilon}_\\tau$ is calculated using the recursive formula with these estimated parameters. These residuals represent the portion of the data not explained by the fitted ARMA model. Shocks or anomalies are expected to manifest as large-magnitude residuals.\n\nThe variance of the white noise process, $\\sigma^2$, is estimated using the mean of the squared residuals:\n$$\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{\\tau=0}^{N-1}\\hat{\\epsilon}_\\tau^2$$\nNote that $\\hat{\\sigma}^2$ is simply the minimized value of the objective function $J(\\hat{\\phi}, \\hat{\\theta})$. The estimated standard deviation is $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$.\n\nThe residuals are then standardized to have unit variance:\n$$r_\\tau = \\frac{\\hat{\\epsilon}_\\tau}{\\hat{\\sigma}}$$\nStandardized residuals with large absolute values indicate points that are outliers with respect to the model.\n\nThe final step is to identify shock locations. A shock is declared whenever the absolute value of a standardized residual exceeds a specified dimensionless threshold, $T_{\\text{thresh}}$. That is, a shock is detected for each residual index $\\tau$ satisfying:\n$$|r_\\tau| \\ge T_{\\text{thresh}}$$\nThe problem specifies a rule to map the residual index $\\tau$ back to the original time series index $t$. For each $\\tau$ that meets the shock condition, the corresponding original index is reported as $t = \\tau + d$. With the differencing order $d=1$, this becomes:\n$$t = \\tau + 1$$\nThe final output for each test case is a sorted list of these detected shock indices $t$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef _process_case(case_params):\n    \"\"\"\n    Processes a single test case for shock detection in a time series.\n    \"\"\"\n    # Unpack parameters for the case\n    T_len, b_func, s_dict, transform_type, d_order, T_thresh = case_params\n\n    # 1. Generate the count time series {x_t}\n    t_series = np.arange(T_len)\n    baseline = b_func(t_series)\n    shocks = np.zeros(T_len)\n    for t_idx, val in s_dict.items():\n        shocks[t_idx] = val\n    x = np.floor(baseline) + shocks\n\n    # 2. Variance stabilization to get {z_t}\n    if transform_type == 'sqrt':\n        z = np.sqrt(x + 0.5)\n    elif transform_type == 'log':\n        z = np.log(x + 1)\n    else:\n        raise ValueError(\"Invalid transform type\")\n\n    # 3. Differencing to get {w_t}\n    if d_order != 1:\n        raise NotImplementedError(\"Only d=1 is implemented.\")\n    w = z[1:] - z[:-1]\n\n    # 4. Centering to get {y_t}\n    y = w - np.mean(w)\n    N = len(y)\n\n    # 5. ARMA(1,1) fitting by conditional sum-of-squares\n    def cost_function(params, y_data):\n        phi, theta = params\n        n_obs = len(y_data)\n        epsilons = np.zeros(n_obs)\n        \n        # Calculate residuals recursively\n        # Initial conditions y_{-1}=0, epsilon_{-1}=0 are implicit\n        \n        # for tau = 0\n        epsilons[0] = y_data[0] # from eps_0 = y_0 - phi*y_{-1} - theta*eps_{-1}\n        \n        # for tau = 1 to N-1\n        for tau in range(1, n_obs):\n            epsilons[tau] = y_data[tau] - phi * y_data[tau-1] - theta * epsilons[tau-1]\n            \n        return np.mean(epsilons**2)\n\n    # Use L-BFGS-B for optimization with bounds\n    initial_guess = [0.0, 0.0]\n    # Use bounds slightly inside (-1, 1) for stability\n    bnds = ((-0.999999, 0.999999), (-0.999999, 0.999999))\n    opt_result = minimize(cost_function, initial_guess, args=(y,), method='L-BFGS-B', bounds=bnds)\n    phi_hat, theta_hat = opt_result.x\n\n    # 6. Residual standardization\n    # Recalculate final residuals with optimal parameters\n    eps_hat = np.zeros(N)\n    eps_hat[0] = y[0]\n    for tau in range(1, N):\n        eps_hat[tau] = y[tau] - phi_hat * y[tau-1] - theta_hat * eps_hat[tau-1]\n\n    # Estimate sigma\n    sigma_hat_sq = np.mean(eps_hat**2)\n    sigma_hat = np.sqrt(sigma_hat_sq)\n\n    # Compute standardized residuals {r_t}\n    # Avoid division by zero if sigma_hat is very small\n    if sigma_hat < 1e-9:\n        r = np.zeros(N)\n    else:\n        r = eps_hat / sigma_hat\n\n    # 7. Shock identification\n    # Find residual indices tau where |r_tau| >= T_thresh\n    shock_residual_indices = np.where(np.abs(r) >= T_thresh)[0]\n\n    # Convert residual indices back to original time series indices t = tau + d\n    shock_original_indices = [int(tau + d_order) for tau in shock_residual_indices]\n    \n    return sorted(shock_original_indices)\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        (120, \n         lambda t: 20 + 5 * np.sin(2 * np.pi * t / 20), \n         {30: 40, 75: 40}, \n         'sqrt', 1, 3),\n        # Case B\n        (120, \n         lambda t: 25 + 3 * np.sin(2 * np.pi * t / 30), \n         {}, \n         'sqrt', 1, 3),\n        # Case C\n        (20, \n         lambda t: 15 + 2 * np.sin(2 * np.pi * t / 15), \n         {10: 30}, \n         'sqrt', 1, 3),\n        # Case D\n        (100, \n         lambda t: 200 + 10 * np.sin(2 * np.pi * t / 25), \n         {60: 300}, \n         'log', 1, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        shock_indices = _process_case(case)\n        results.append(shock_indices)\n\n    # Final print statement in the exact required format.\n    # The str() of a list produces the required '[...]' format for inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187631"}]}