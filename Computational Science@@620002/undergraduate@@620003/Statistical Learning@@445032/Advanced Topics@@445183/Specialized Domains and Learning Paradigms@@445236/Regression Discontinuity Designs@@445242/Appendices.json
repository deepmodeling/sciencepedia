{"hands_on_practices": [{"introduction": "The core idea of Regression Discontinuity is to estimate the jump in an outcome function at a specific cutoff. A tempting but flawed approach is to use a standard kernel smoother that pools data across the cutoff. This exercise provides a foundational, first-principles derivation to show precisely why this intuitive method fails and leads to significant bias, demonstrating mathematically why separate one-sided estimations are a cornerstone of valid RDD analysis [@problem_id:3168444].", "problem": "Consider a sharp regression discontinuity design at cutoff $c = 0$ with a real-valued running variable $X$ and outcome regression function $f(x) = \\mathbb{E}[Y \\mid X = x]$. Suppose the right and left limits of the regression function admit separate quadratic Taylor representations near $x = 0$ given by $f_{+}(x) = a_{+} + b_{+} x + c_{+} x^{2}$ for $x \\ge 0$ and $f_{-}(x) = a_{-} + b_{-} x + c_{-} x^{2}$ for $x < 0$, where all coefficients are real constants. Assume the running variable has a design density $g(x)$ that is uniform on $[-h,h]$ for some bandwidth $h > 0$, that is $g(x) = \\frac{1}{2h}$ for $x \\in [-h,h]$ and $g(x)=0$ otherwise. Let the kernel be the triangular kernel localized to a neighborhood of $0$, that is $K(u) = \\max\\{1 - |u|, 0\\}$, and define the weight function $w(x;h) = K(|x|/h) = \\max\\{1 - |x|/h, 0\\}$.\n\nYou will compare two local constant (Nadarayaâ€“Watson) kernel estimators at $x=0$:\n\n- One-sided estimators, which estimate the right-hand limit $f_{+}(0)$ using only $x \\in [0,h]$ and the left-hand limit $f_{-}(0)$ using only $x \\in [-h,0]$.\n- A two-sided estimator that incorrectly uses both sides $x \\in [-h,h]$ with symmetric weights to estimate a single value at $x=0$.\n\nThroughout, assume there is no sampling noise so that all expectations are taken with respect to the design density $g(x)$. Use the following fundamental definitions as the starting point:\n\n- The conditional mean function at a point is $f(x) = \\mathbb{E}[Y \\mid X = x]$.\n- A local constant kernel estimator at a point is given in expectation by the ratio of weighted integrals,\n$$\n\\mathbb{E}[\\hat f(0)] \\;=\\; \\frac{\\int w(x;h) f(x) g(x) \\, dx}{\\int w(x;h) g(x) \\, dx},\n$$\nwhere the integral is taken over the support on which the estimator uses data.\n\nDefine the one-sided expected estimators of the limits as\n$$\n\\mathbb{E}[\\hat f^{\\text{one}}_{+}(0)] \\;=\\; \\frac{\\int_{0}^{h} w(x;h) f_{+}(x) g(x) \\, dx}{\\int_{0}^{h} w(x;h) g(x) \\, dx}, \n\\quad\n\\mathbb{E}[\\hat f^{\\text{one}}_{-}(0)] \\;=\\; \\frac{\\int_{-h}^{0} w(x;h) f_{-}(x) g(x) \\, dx}{\\int_{-h}^{0} w(x;h) g(x) \\, dx}.\n$$\n\nDefine the two-sided expected estimator (incorrectly pooling both sides) as\n$$\n\\mathbb{E}[\\hat f^{\\text{two}}(0)] \\;=\\; \\frac{\\int_{-h}^{h} w(x;h) f(x) g(x) \\, dx}{\\int_{-h}^{h} w(x;h) g(x) \\, dx},\n$$\nwith $f(x) = f_{-}(x)$ for $x<0$ and $f(x) = f_{+}(x)$ for $x \\ge 0$.\n\nYour tasks:\n\n- Derive, from the above definitions and assumptions, closed-form expressions for the biases\n$$\n\\text{Bias}^{\\text{one}}_{+} \\;=\\; \\mathbb{E}[\\hat f^{\\text{one}}_{+}(0)] - f_{+}(0), \n\\quad\n\\text{Bias}^{\\text{one}}_{-} \\;=\\; \\mathbb{E}[\\hat f^{\\text{one}}_{-}(0)] - f_{-}(0),\n$$\nand\n$$\n\\text{Bias}^{\\text{two}}_{+} \\;=\\; \\mathbb{E}[\\hat f^{\\text{two}}(0)] - f_{+}(0),\n\\quad\n\\text{Bias}^{\\text{two}}_{-} \\;=\\; \\mathbb{E}[\\hat f^{\\text{two}}(0)] - f_{-}(0),\n$$\nas functions of $a_{-}, b_{-}, c_{-}, a_{+}, b_{+}, c_{+}$ and $h$.\n- Implement a program that evaluates these four biases for the following test suite of parameter sets $(a_{-}, b_{-}, c_{-}, a_{+}, b_{+}, c_{+}, h)$:\n    - Test case $1$: $(0, 0, 0, 1, 0, 0, 1)$.\n    - Test case $2$: $(0, -1, 0, 0, 2, 0, 0.6)$.\n    - Test case $3$: $(2, 1, 1, 2, 1, 2, 0.5)$.\n    - Edge case $4$: $(0.5, 0.2, 0, 1.0, -0.1, 0, 0.01)$.\n\nYour program must output a single line containing a comma-separated list enclosed in square brackets. For each test case in the order listed, append the four numbers in the order \n$[\\text{Bias}^{\\text{one}}_{+}, \\text{Bias}^{\\text{one}}_{-}, \\text{Bias}^{\\text{two}}_{+}, \\text{Bias}^{\\text{two}}_{-}]$.\nAll numbers must be decimal representations rounded to $6$ decimal places. For example, a valid output format is like $[x_{1},x_{2},\\ldots,x_{16}]$ with each $x_{i}$ a decimal rounded to $6$ places.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of non-parametric statistics and regression discontinuity analysis. It is well-posed, with all necessary functions, parameters, and definitions provided to derive a unique and meaningful solution. The language is objective and unambiguous. The problem is self-contained and free of contradictions.\n\nThe objective is to derive the analytical expressions for the bias of one-sided and two-sided local constant kernel estimators at a regression discontinuity point and then to evaluate these biases for a given set of parameters.\n\nThe true values of the regression function at the cutoff $x=0$ are $f_{+}(0) = a_{+}$ and $f_{-}(0) = a_{-}$. The biases are defined relative to these true values. The derivation proceeds by first calculating the denominators of the kernel estimator expressions, followed by the numerators, which will then yield the expected values of the estimators and finally their biases.\n\nThe given functions are:\n- Regression functions: $f_{+}(x) = a_{+} + b_{+} x + c_{+} x^{2}$ for $x \\ge 0$ and $f_{-}(x) = a_{-} + b_{-} x + c_{-} x^{2}$ for $x < 0$.\n- Design density: $g(x) = \\frac{1}{2h}$ for $x \\in [-h,h]$.\n- Weight function: $w(x;h) = \\max\\{1 - |x|/h, 0\\}$. For $x \\in [-h, h]$, this simplifies to $w(x;h) = 1 - |x|/h$.\n\nFirst, we compute the denominators for the estimators.\nThe denominator for the one-sided right estimator $\\mathbb{E}[\\hat f^{\\text{one}}_{+}(0)]$ is:\n$$ D_{+} = \\int_{0}^{h} w(x;h) g(x) \\, dx = \\int_{0}^{h} \\left(1 - \\frac{x}{h}\\right) \\frac{1}{2h} \\, dx $$\n$$ D_{+} = \\frac{1}{2h} \\left[ x - \\frac{x^2}{2h} \\right]_{0}^{h} = \\frac{1}{2h} \\left( h - \\frac{h^2}{2h} \\right) = \\frac{1}{2h} \\left( \\frac{h}{2} \\right) = \\frac{1}{4} $$\nBy symmetry, the denominator for the one-sided left estimator $\\mathbb{E}[\\hat f^{\\text{one}}_{-}(0)]$ is identical:\n$$ D_{-} = \\int_{-h}^{0} w(x;h) g(x) \\, dx = \\int_{-h}^{0} \\left(1 + \\frac{x}{h}\\right) \\frac{1}{2h} \\, dx $$\n$$ D_{-} = \\frac{1}{2h} \\left[ x + \\frac{x^2}{2h} \\right]_{-h}^{0} = \\frac{1}{2h} \\left( 0 - \\left(-h + \\frac{h^2}{2h}\\right) \\right) = \\frac{1}{2h} \\left( h - \\frac{h}{2} \\right) = \\frac{1}{4} $$\nThe denominator for the two-sided estimator $\\mathbb{E}[\\hat f^{\\text{two}}(0)]$ is the sum of the one-sided denominators:\n$$ D_{\\text{two}} = \\int_{-h}^{h} w(x;h) g(x) \\, dx = D_{+} + D_{-} = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n\nNext, we compute the numerators.\nThe numerator for the one-sided right estimator is:\n$$ N_{+} = \\int_{0}^{h} w(x;h) f_{+}(x) g(x) \\, dx = \\int_{0}^{h} \\left(1 - \\frac{x}{h}\\right) (a_{+} + b_{+} x + c_{+} x^2) \\frac{1}{2h} \\, dx $$\n$$ N_{+} = \\frac{1}{2h} \\int_{0}^{h} \\left( a_{+} + (b_{+} - \\frac{a_{+}}{h})x + (c_{+} - \\frac{b_{+}}{h})x^2 - \\frac{c_{+}}{h}x^3 \\right) dx $$\n$$ N_{+} = \\frac{1}{2h} \\left[ a_{+}x + (b_{+} - \\frac{a_{+}}{h})\\frac{x^2}{2} + (c_{+} - \\frac{b_{+}}{h})\\frac{x^3}{3} - \\frac{c_{+}}{h}\\frac{x^4}{4} \\right]_{0}^{h} $$\n$$ N_{+} = \\frac{1}{2h} \\left( a_{+}h + b_{+}\\frac{h^2}{2} - a_{+}\\frac{h}{2} + c_{+}\\frac{h^3}{3} - b_{+}\\frac{h^2}{3} - c_{+}\\frac{h^3}{4} \\right) $$\n$$ N_{+} = \\frac{1}{2h} \\left( \\frac{a_{+}h}{2} + \\frac{b_{+}h^2}{6} + \\frac{c_{+}h^3}{12} \\right) = \\frac{a_{+}}{4} + \\frac{b_{+}h}{12} + \\frac{c_{+}h^2}{24} $$\nThe expected value of the estimator is $\\mathbb{E}[\\hat f^{\\text{one}}_{+}(0)] = N_{+} / D_{+} = (\\frac{a_{+}}{4} + \\frac{b_{+}h}{12} + \\frac{c_{+}h^2}{24}) / (\\frac{1}{4}) = a_{+} + \\frac{b_{+}h}{3} + \\frac{c_{+}h^2}{6}$.\nThe bias is then:\n$$ \\text{Bias}^{\\text{one}}_{+} = \\mathbb{E}[\\hat f^{\\text{one}}_{+}(0)] - f_{+}(0) = \\left( a_{+} + \\frac{b_{+}h}{3} + \\frac{c_{+}h^2}{6} \\right) - a_{+} = \\frac{b_{+}h}{3} + \\frac{c_{+}h^2}{6} $$\n\nThe numerator for the one-sided left estimator is:\n$$ N_{-} = \\int_{-h}^{0} w(x;h) f_{-}(x) g(x) \\, dx = \\int_{-h}^{0} \\left(1 + \\frac{x}{h}\\right) (a_{-} + b_{-} x + c_{-} x^2) \\frac{1}{2h} \\, dx $$\nUsing the substitution $u = -x$, $du = -dx$:\n$$ N_{-} = \\int_{h}^{0} \\left(1 - \\frac{u}{h}\\right) (a_{-} - b_{-} u + c_{-} u^2) \\frac{1}{2h} (-du) = \\frac{1}{2h} \\int_{0}^{h} \\left(1 - \\frac{u}{h}\\right) (a_{-} - b_{-} u + c_{-} u^2) du $$\nThis integral has the same form as $N_{+}$, with $a_{+}$ replaced by $a_{-}$, $b_{+}$ by $-b_{-}$, and $c_{+}$ by $c_{-}$. Thus, we can write the result directly:\n$$ N_{-} = \\frac{a_{-}}{4} - \\frac{b_{-}h}{12} + \\frac{c_{-}h^2}{24} $$\nThe expected value of the estimator is $\\mathbb{E}[\\hat f^{\\text{one}}_{-}(0)] = N_{-} / D_{-} = (\\frac{a_{-}}{4} - \\frac{b_{-}h}{12} + \\frac{c_{-}h^2}{24}) / (\\frac{1}{4}) = a_{-} - \\frac{b_{-}h}{3} + \\frac{c_{-}h^2}{6}$.\nThe bias is then:\n$$ \\text{Bias}^{\\text{one}}_{-} = \\mathbb{E}[\\hat f^{\\text{one}}_{-}(0)] - f_{-}(0) = \\left( a_{-} - \\frac{b_{-}h}{3} + \\frac{c_{-}h^2}{6} \\right) - a_{-} = -\\frac{b_{-}h}{3} + \\frac{c_{-}h^2}{6} $$\n\nFinally, for the two-sided estimator, the numerator is $N_{\\text{two}} = N_{+} + N_{-}$:\n$$ N_{\\text{two}} = \\left( \\frac{a_{+}}{4} + \\frac{b_{+}h}{12} + \\frac{c_{+}h^2}{24} \\right) + \\left( \\frac{a_{-}}{4} - \\frac{b_{-}h}{12} + \\frac{c_{-}h^2}{24} \\right) = \\frac{a_{+} + a_{-}}{4} + \\frac{(b_{+} - b_{-})h}{12} + \\frac{(c_{+} + c_{-})h^2}{24} $$\nThe expected value of the two-sided estimator is $\\mathbb{E}[\\hat f^{\\text{two}}(0)] = N_{\\text{two}} / D_{\\text{two}}$:\n$$ \\mathbb{E}[\\hat f^{\\text{two}}(0)] = \\frac{\\frac{a_{+} + a_{-}}{4} + \\frac{(b_{+} - b_{-})h}{12} + \\frac{(c_{+} + c_{-})h^2}{24}}{\\frac{1}{2}} = \\frac{a_{+} + a_{-}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12} $$\nThe biases relative to the right and left limits are:\n$$ \\text{Bias}^{\\text{two}}_{+} = \\mathbb{E}[\\hat f^{\\text{two}}(0)] - f_{+}(0) = \\left( \\frac{a_{+} + a_{-}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12} \\right) - a_{+} $$\n$$ \\text{Bias}^{\\text{two}}_{+} = \\frac{a_{-} - a_{+}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12} $$\n$$ \\text{Bias}^{\\text{two}}_{-} = \\mathbb{E}[\\hat f^{\\text{two}}(0)] - f_{-}(0) = \\left( \\frac{a_{+} + a_{-}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12} \\right) - a_{-} $$\n$$ \\text{Bias}^{\\text{two}}_{-} = \\frac{a_{+} - a_{-}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12} $$\n\nSummary of Bias Formulas:\n1. $\\text{Bias}^{\\text{one}}_{+} = \\frac{b_{+}h}{3} + \\frac{c_{+}h^2}{6}$\n2. $\\text{Bias}^{\\text{one}}_{-} = -\\frac{b_{-}h}{3} + \\frac{c_{-}h^2}{6}$\n3. $\\text{Bias}^{\\text{two}}_{+} = \\frac{a_{-} - a_{+}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12}$\n4. $\\text{Bias}^{\\text{two}}_{-} = \\frac{a_{+} - a_{-}}{2} + \\frac{(b_{+} - b_{-})h}{6} + \\frac{(c_{+} + c_{-})h^2}{12}$\n\nThese formulas are implemented in the following program to compute the biases for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the biases of one-sided and two-sided kernel estimators in a\n    regression discontinuity design for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a_minus, b_minus, c_minus, a_plus, b_plus, c_plus, h)\n        (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0),\n        (0.0, -1.0, 0.0, 0.0, 2.0, 0.0, 0.6),\n        (2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.5),\n        (0.5, 0.2, 0.0, 1.0, -0.1, 0.0, 0.01),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a_minus, b_minus, c_minus, a_plus, b_plus, c_plus, h = case\n        \n        # Calculate Bias_one_plus\n        # Formula: b_plus * h / 3 + c_plus * h^2 / 6\n        bias_one_plus = (b_plus * h / 3) + (c_plus * h**2 / 6)\n        \n        # Calculate Bias_one_minus\n        # Formula: -b_minus * h / 3 + c_minus * h^2 / 6\n        bias_one_minus = (-b_minus * h / 3) + (c_minus * h**2 / 6)\n        \n        # Calculate expected value of the two-sided estimator first\n        # E[f_hat_two(0)] = (a_plus + a_minus)/2 + (b_plus - b_minus)*h/6 + (c_plus + c_minus)*h^2/12\n        exp_f_hat_two = ((a_plus + a_minus) / 2) + \\\n                        ((b_plus - b_minus) * h / 6) + \\\n                        ((c_plus + c_minus) * h**2 / 12)\n        \n        # Calculate Bias_two_plus\n        # Formula: E[f_hat_two(0)] - a_plus\n        bias_two_plus = exp_f_hat_two - a_plus\n        \n        # Calculate Bias_two_minus\n        # Formula: E[f_hat_two(0)] - a_minus\n        bias_two_minus = exp_f_hat_two - a_minus\n        \n        # Append the four calculated biases to the results list\n        results.extend([bias_one_plus, bias_one_minus, bias_two_plus, bias_two_minus])\n\n    # Format the results for the final output string\n    # Each number is formatted to 6 decimal places.\n    formatted_results = [f\"{num:.6f}\" for num in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3168444"}, {"introduction": "A successful Regression Discontinuity Design relies on the crucial assumption that the analyst's chosen cutoff precisely matches the rule that assigns the treatment. This hands-on problem explores what happens when this assumption is violated, a common scenario in the real world where policy implementation may not perfectly align with its official rule. By deriving the bias that arises from a misaligned cutoff, you will gain a deeper understanding of how violations of identification assumptions can undermine causal claims [@problem_id:3168533].", "problem": "Consider a Regression Discontinuity Design (RDD), defined as the difference in the right-hand and left-hand limits of the conditional expectation of the observed outcome at a cutoff. Let the running variable be $X \\in \\mathbb{R}$ with a continuously differentiable density near a cutoff $c \\in \\mathbb{R}$. Assume the observed outcome $Y$ is generated by\n$$\nY = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon,\n$$\nwhere $f:\\mathbb{R}\\to\\mathbb{R}$ is continuous at $c$, $\\tau \\in \\mathbb{R}$ is a constant treatment effect, $\\Delta > 0$ is a nonzero shift between the policy cutoff $c$ and the actual treatment start $c+\\Delta$, $\\varepsilon$ is a mean-zero disturbance satisfying $\\mathbb{E}[\\varepsilon \\mid X] = 0$, and $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. There is no manipulation at the cutoff $c$ (the density of $X$ is continuous at $c$).\n\nYour tasks are:\n\n1) Using the definition of the RDD estimand as\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x],\n$$\nderive $\\theta_{\\text{RDD}}(c)$ under the given data-generating process, and express its bias relative to the treatment effect parameter $\\tau$.\n\n2) Consider a local-constant RDD estimator with a uniform kernel and symmetric bandwidth $h>0$. That is, the estimator computes the difference between the sample average of $Y$ in the right-hand window $[c, c+h]$ and the sample average of $Y$ in the left-hand window $[c-h, c)$. Under large samples, a continuous density of $X$ at $c$, and the assumption that $f(x)$ is approximately constant in the neighborhood $[c-h, c+h]$ (i.e., $f(x) \\approx f(c)$ for $x \\in [c-h, c+h]$), derive the probability limit of this estimator and its bias relative to $\\tau$ as a function of $\\tau$, $\\Delta$, and $h$.\n\n3) Implement a program that computes the bias function derived in task 2 for the following test suite of parameter values. For each case, take $c=0$ (this choice does not affect the bias expression you derive) and return the bias as a real number (float):\n- Case A (happy path): $(\\tau, \\Delta, h) = (2.0, 0.3, 1.0)$.\n- Case B (boundary): $(\\tau, \\Delta, h) = (1.0, 1.0, 1.0)$.\n- Case C (edge with no treated in right window): $(\\tau, \\Delta, h) = (1.5, 1.2, 1.0)$.\n- Case D (near alignment): $(\\tau, \\Delta, h) = (0.75, 10^{-6}, 0.5)$.\n- Case E (negative treatment effect): $(\\tau, \\Delta, h) = (-1.5, 0.25, 0.5)$.\n\nThere are no physical units in this problem. All outputs must be expressed as real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where $r_i$ is the computed bias for the $i$-th case in the above order.", "solution": "The problem requires the analysis of a Regression Discontinuity Design (RDD) where the treatment assignment is shifted from the policy cutoff. We must first derive the theoretical RDD estimand and its bias, then derive the probability limit and bias of a practical local-constant estimator, and finally implement a program to compute this estimator's bias for specific parameter values.\n\n**Task 1: Derivation of the RDD Estimand and its Bias**\n\nThe RDD estimand is defined as the jump in the conditional expectation of the outcome $Y$ at the cutoff $c$:\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x]\n$$\nThe data-generating process is given by $Y = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon$. We first find the conditional expectation of $Y$ given $X=x$. Using the linearity of expectation and the assumption that $\\mathbb{E}[\\varepsilon \\mid X] = 0$, we have:\n$$\n\\mathbb{E}[Y \\mid X = x] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\} + \\mathbb{E}[\\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}\n$$\nNow, we evaluate the right-hand and left-hand limits at $x=c$.\n\nFor the right-hand limit, $x$ approaches $c$ from above, so $x > c$. Since the problem states $\\Delta > 0$, for values of $x$ sufficiently close to $c$ (specifically, in the interval $(c, c+\\Delta)$), the condition $x \\ge c+\\Delta$ is false. Thus, the indicator function $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ is $0$. Given that $f(x)$ is continuous at $c$, the limit is:\n$$\n\\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\downarrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\nFor the left-hand limit, $x$ approaches $c$ from below, so $x < c$. Since $\\Delta > 0$, it is always true that $x < c < c+\\Delta$. Therefore, the condition $x \\ge c+\\Delta$ is false, and the indicator function $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ is $0$. The limit is:\n$$\n\\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\uparrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\nSubstituting these limits into the definition of the RDD estimand gives:\n$$\n\\theta_{\\text{RDD}}(c) = f(c) - f(c) = 0\n$$\nThe bias of this estimand relative to the true treatment effect $\\tau$ is defined as $\\text{Bias} = \\theta_{\\text{RDD}}(c) - \\tau$. Therefore, the bias is:\n$$\n\\text{Bias} = 0 - \\tau = -\\tau\n$$\nThis result is intuitive: the RDD estimand looks for a discontinuity at $c$, but the actual discontinuity in the conditional mean function occurs at $c+\\Delta$. Due to the continuity of $f(x)$ at $c$, no jump is detected, leading to an estimate of $0$ and a bias of $-\\tau$.\n\n**Task 2: Derivation of the Estimator's Probability Limit and Bias**\n\nWe consider a local-constant RDD estimator, which is the difference in sample averages of $Y$ in the windows $[c, c+h]$ and $[c-h, c)$. In large samples (as $n \\to \\infty$), the probability limit (plim) of this estimator, denoted $\\hat{\\theta}_h$, is the difference in the true conditional expectations over these windows:\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h = \\mathbb{E}[Y \\mid c \\le X < c+h] - \\mathbb{E}[Y \\mid c-h \\le X < c]\n$$\nLet's analyze each term separately. For the left window, $[c-h, c)$, any value of $X$ satisfies $X < c < c+\\Delta$ (since $\\Delta > 0$). Thus, $\\mathbb{1}\\{X \\ge c + \\Delta\\} = 0$ for all $X$ in this window.\n$$\n\\mathbb{E}[Y \\mid c-h \\le X < c] = \\mathbb{E}[f(X) + \\tau \\cdot 0 + \\varepsilon \\mid c-h \\le X < c] = \\mathbb{E}[f(X) \\mid c-h \\le X < c]\n$$\nUsing the assumption that $f(x) \\approx f(c)$ for $x \\in [c-h, c+h]$, this simplifies to:\n$$\n\\mathbb{E}[Y \\mid c-h \\le X < c] \\approx f(c)\n$$\nFor the right window, $[c, c+h]$, the indicator function is not always zero.\n$$\n\\mathbb{E}[Y \\mid c \\le X < c+h] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid c \\le X < c+h]\n$$\nUsing the approximation $f(x) \\approx f(c)$ and the law of iterated expectations, this becomes:\n$$\n\\mathbb{E}[Y \\mid c \\le X < c+h] \\approx f(c) + \\tau \\cdot \\mathbb{E}[\\mathbb{1}\\{X \\ge c + \\Delta\\} \\mid c \\le X < c+h]\n$$\nThe expectation of the indicator function is the conditional probability $P(X \\ge c+\\Delta \\mid c \\le X < c+h)$. This can be written as:\n$$\nP(X \\ge c+\\Delta \\mid c \\le X < c+h) = \\frac{P((X \\ge c+\\Delta) \\cap (c \\le X < c+h))}{P(c \\le X < c+h)}\n$$\nThe numerator is the probability of the intersection of two intervals, which is $P(\\max(c, c+\\Delta) \\le X < c+h) = P(c+\\Delta \\le X < c+h)$. This interval is non-empty only if $c+\\Delta < c+h$, which means $\\Delta < h$. If $\\Delta \\ge h$, the probability is $0$.\nLet $g(x)$ be the probability density function (PDF) of $X$. Since $g(x)$ is continuous at $c$, for a small bandwidth $h$, we can approximate the density as constant, $g(x) \\approx g(c)$, over the interval $[c, c+h]$.\nThe probability in the numerator is $\\int_{c+\\Delta}^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-(c+\\Delta)) = g(c) \\cdot (h-\\Delta)$, for $\\Delta < h$. If $\\Delta \\ge h$, it is $0$. This can be written as $g(c) \\cdot \\max(0, h-\\Delta)$.\nThe probability in the denominator is $\\int_c^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-c) = g(c) \\cdot h$.\nSo, the conditional probability is:\n$$\nP(X \\ge c+\\Delta \\mid c \\le X < c+h) \\approx \\frac{g(c) \\cdot \\max(0, h-\\Delta)}{g(c) \\cdot h} = \\frac{\\max(0, h-\\Delta)}{h}\n$$\nThe probability limit of the estimator is then:\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h \\approx \\left( f(c) + \\tau \\frac{\\max(0, h-\\Delta)}{h} \\right) - f(c) = \\tau \\frac{\\max(0, h-\\Delta)}{h}\n$$\nThe asymptotic bias is the difference between this probability limit and the true parameter $\\tau$:\n$$\n\\text{Bias} = \\text{plim}_{n \\to \\infty} \\hat{\\theta}_h - \\tau \\approx \\tau \\frac{\\max(0, h-\\Delta)}{h} - \\tau = \\tau \\left( \\frac{\\max(0, h-\\Delta)}{h} - 1 \\right)\n$$\nWe can simplify this expression. Note that $\\max(0, h-\\Delta) = h - \\min(h, \\Delta)$.\n$$\n\\text{Bias} \\approx \\tau \\left( \\frac{h-\\min(h, \\Delta)}{h} - 1 \\right) = \\tau \\left( 1 - \\frac{\\min(h, \\Delta)}{h} - 1 \\right) = -\\tau \\frac{\\min(h, \\Delta)}{h}\n$$\nThis is the final expression for the bias. It depends on the true effect $\\tau$, the policy misalignment $\\Delta$, and the estimator's bandwidth $h$. This formula is valid for both cases:\n1. If $\\Delta < h$, $\\min(h, \\Delta) = \\Delta$, so Bias $\\approx -\\tau \\frac{\\Delta}{h}$.\n2. If $\\Delta \\ge h$, $\\min(h, \\Delta) = h$, so Bias $\\approx -\\tau \\frac{h}{h} = -\\tau$.\n\n**Task 3: Numerical Computation of the Bias**\n\nWe will now implement a program to calculate the bias using the derived formula $\\text{Bias} = -\\tau \\frac{\\min(h, \\Delta)}{h}$ for the provided test cases, with $c=0$. The value of $c$ does not affect the bias formula.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local-constant RDD estimator with a misaligned cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (tau, Delta, h)\n    test_cases = [\n        # Case A (happy path)\n        (2.0, 0.3, 1.0),\n        # Case B (boundary)\n        (1.0, 1.0, 1.0),\n        # Case C (edge with no treated in right window)\n        (1.5, 1.2, 1.0),\n        # Case D (near alignment)\n        (0.75, 1e-6, 0.5),\n        # Case E (negative treatment effect)\n        (-1.5, 0.25, 0.5),\n    ]\n\n    def calculate_bias(tau: float, delta: float, h: float) -> float:\n        \"\"\"\n        Calculates the asymptotic bias of the local-constant RDD estimator.\n\n        The formula for the bias is derived as:\n        Bias = -tau * min(h, delta) / h\n\n        Args:\n            tau: The true treatment effect.\n            delta: The shift between the policy cutoff and the treatment start.\n            h: The symmetric bandwidth of the estimator.\n\n        Returns:\n            The calculated bias as a float.\n        \"\"\"\n        # The derivation shows that the bandwidth h must be positive.\n        # The problem statement specifies h > 0, so no division by zero error.\n        bias = -tau * min(h, delta) / h\n        return bias\n\n    results = []\n    for case in test_cases:\n        tau_val, delta_val, h_val = case\n        result = calculate_bias(tau_val, delta_val, h_val)\n        results.append(result)\n\n    # Format the final output string as a comma-separated list in brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "3168533"}, {"introduction": "Even with a correctly specified one-sided estimator at the right cutoff, practical data limitations can introduce subtle biases. This exercise simulates a common scenario where the running variable has a bounded support, such as a test score ranging from 0 to 100. By implementing a local linear estimator, you will algorithmically demonstrate how the proximity of the RDD cutoff to one of these external boundaries can distort the estimate, providing insight into the practical challenges of non-parametric analysis [@problem_id:3168448].", "problem": "You are given a task to algorithmically evaluate how truncation of the running variable near a boundary affects the bias of a local linear Regression Discontinuity (RD) estimator. Start from the foundational definition that, under continuity of the conditional outcome functions, the RD estimand at a cutoff is the jump in the limiting conditional mean of the observed outcome at the cutoff. Then design and implement an algorithm that constructs the estimator from first principles using only one-sided local linear regressions.\n\nData-generating setup:\n- The running variable is supported on a compact interval, specifically $X \\in [0,100]$. You must treat the support as truncated at the bounds, meaning no observations exist for $x \\lt 0$ or $x \\gt 100$.\n- Generate a deterministic, dense grid of running variable values: $M$ equally spaced points on $[0,100]$, with $M = 10001$, so that the grid spacing is $0.01$ and the grid is $\\{0,0.01,0.02,\\dots,100\\}$.\n- Define potential outcome mean functions $m_0(x)$ and $m_1(x)$ by\n  $$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2,\\quad m_1(x) = m_0(x) + \\tau,$$\n  where the constant treatment effect is $\\tau = 3$.\n- For a given cutoff $c$, define the observed outcome deterministically as\n  $$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\},$$\n  with no stochastic noise added.\n\nEstimator to implement:\n- For a specified bandwidth $h \\gt 0$, construct a one-sided local linear estimator on each side of the cutoff $c$, using only observations within the window $|X - c| \\le h$ and on the respective side of $c$.\n- Use the triangular kernel\n  $$K(u) = \\max\\{0, 1 - |u|\\},\\quad u = \\frac{X - c}{h}.$$\n- For the left side, use all points with $X \\lt c$ and $|X - c| \\le h$; for the right side, use all points with $X \\ge c$ and $|X - c| \\le h$.\n- On each side, fit a weighted least squares linear regression of the form\n  $$Y = \\alpha + \\beta\\,(X - c),$$\n  where the weights are $K\\big((X - c)/h\\big)$. Extract the fitted intercept $\\widehat{\\alpha}_-$ from the left-side fit and $\\widehat{\\alpha}_+$ from the right-side fit.\n- Define the RD estimate as\n  $$\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-.$$\n- Define the estimator bias for given $(c,h)$ as\n  $$\\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau.$$\n\nScientific goal:\n- Show algorithmically how proximity of the cutoff $c$ to a boundary (e.g., near $100$) increases the absolute bias due to asymmetric support on one side and curvature in $m_0(x)$ and $m_1(x)$.\n- Because there is no noise and the grid is dense and deterministic, the resulting biases are deterministic functions of $(c,h)$ and the data-generating process.\n\nTest suite:\n- Use the following five test cases for $(c,h)$:\n  1. $(c,h) = (50,10)$, a within-support interior case intended as a baseline.\n  2. $(c,h) = (95,10)$, near the upper boundary to illustrate increased bias.\n  3. $(c,h) = (98,10)$, even closer to the upper boundary to stress boundary effects.\n  4. $(c,h) = (95,5)$, same cutoff as case $2$ but a smaller bandwidth to assess bias reduction from reduced curvature exposure.\n  5. $(c,h) = (5,10)$, near the lower boundary to illustrate boundary effects on the opposite side.\n\nAnswer specification:\n- For each test case, compute the bias $\\mathrm{bias}(c,h)$ as a real number.\n- Your program must produce a single line of output containing the five results as a comma-separated list enclosed in square brackets, e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, with each number printed as a decimal floating-point value. No units are involved. Angles are not used. If you choose to round, round to a fixed number of decimal places consistently across all outputs.\n\nYour program must be a complete, runnable implementation that performs the above computations exactly as specified and outputs only the single required line. No user input is permitted, and no external files may be used. The implementation language is specified separately and must be followed exactly.", "solution": "The problem requires the design and implementation of an algorithm to compute the bias of a local linear Regression Discontinuity (RD) estimator. The analysis focuses on how the bias is affected when the cutoff point is near the boundary of the data's support. The entire process is deterministic, using a dense grid of points and noise-free outcome functions.\n\nFirst, we formalize the problem setup based on the provided specifications. The running variable, denoted by $X$, has a compact support on the interval $[0, 100]$. We operate on a deterministic grid of $M = 10001$ equally spaced points within this interval, starting at $X=0$ and ending at $X=100$. The potential outcome mean functions are given by\n$$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2$$\n$$m_1(x) = m_0(x) + \\tau$$\nwhere the true treatment effect $\\tau$ is a constant equal to $3$. The observed outcome $Y$ for a given cutoff $c$ is deterministically defined as:\n$$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\}$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The RD estimand is the jump in the conditional expectation function at the cutoff, which in this noise-free setting is exactly $\\tau = m_1(c) - m_0(c) = 3$.\n\nThe task is to implement a one-sided local linear RD estimator. This involves performing a separate weighted least squares (WLS) regression on each side of the cutoff $c$. For a given bandwidth $h > 0$, the data used for the estimation are restricted to the window $|X - c| \\le h$. The regression model to be fitted is:\n$$Y = \\alpha + \\beta\\,(X - c)$$\nThe weights are provided by the triangular kernel, $K(u) = \\max\\{0, 1 - |u|\\}$, where $u = (X - c)/h$.\n\nFor the right side of the cutoff, we minimize the weighted sum of squared residuals for all data points $X_i$ such that $c \\le X_i \\le \\min(c+h, 100)$:\n$$ \\min_{\\alpha_+, \\beta_+} \\sum_{i: c \\le X_i \\le c+h} K\\left(\\frac{X_i - c}{h}\\right) \\left[ Y_i - \\left(\\alpha_+ + \\beta_+ (X_i - c)\\right) \\right]^2 $$\nThe solution to this WLS problem yields the estimated intercept $\\widehat{\\alpha}_+$. Similarly, for the left side, we use data points $X_i$ such that $\\max(c-h, 0) \\le X_i < c$ to find the intercept $\\widehat{\\alpha}_-$. Note the strict inequality $X_i < c$ for the left side and the interaction with the support boundaries at $0$ and $100$.\n\nThe WLS estimator for the coefficient vector $\\mathbf{b} = [\\alpha, \\beta]^T$ can be expressed in matrix form as:\n$$ \\widehat{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} (\\mathbf{X}^T \\mathbf{W} \\mathbf{y}) $$\nHere, $\\mathbf{y}$ is the vector of outcome values in the local window, $\\mathbf{X}$ is the design matrix with the first column being ones and the second column being the recentered variable $(X_i - c)$, and $\\mathbf{W}$ is a diagonal matrix containing the kernel weights $K((X_i-c)/h)$. The estimated intercept, $\\widehat{\\alpha}$, is the first element of the vector $\\widehat{\\mathbf{b}}$.\n\nOnce we have computed the intercepts from the left and right regressions, $\\widehat{\\alpha}_-$ and $\\widehat{\\alpha}_+$, the RD estimate of the treatment effect is:\n$$ \\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_- $$\nThe bias of this estimator for a given pair of $(c,h)$ is then calculated as:\n$$ \\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau $$\n\nThe scientific goal is to demonstrate how this bias changes as $c$ approaches a boundary of the support. The bias of a local linear estimator is principally due to the mismatch between the linear model and the true underlying function, which in this case has non-zero curvature ($m_0''(x) = 0.004$). For an interior point $c$ with a symmetric window $[c-h, c+h]$, the biases in $\\widehat{\\alpha}_+$ and $\\widehat{\\alpha}_-$ tend to have a similar structure and partially cancel when their difference is taken. However, when $c$ is near a boundary (e.g., $c=95$ with $h=10$), one of the estimation windows is truncated (e.g., the right-side window becomes $[95, 100]$ instead of $[95, 105]$). This truncation creates an asymmetric effective kernel, which breaks the symmetry of the bias terms. The cancellation is no longer effective, resulting in a larger overall bias in $\\widehat{\\tau}_{\\mathrm{RD}}$. Reducing the bandwidth $h$ shortens the estimation window, making the linear approximation more accurate over the smaller range and thus reducing the magnitude of the curvature-induced bias, even in boundary cases.\n\nThe algorithm proceeds by iterating through each test case $(c,h)$:\n1.  Generate the full set of outcome values $Y_i$ across the grid of $X_i \\in [0, 100]$.\n2.  For the given $(c,h)$, perform the WLS procedure for the left side, using data from $[\\max(0, c-h), c)$, to obtain $\\widehat{\\alpha}_-$.\n3.  Perform the WLS procedure for the right side, using data from $[c, \\min(100, c+h)]$, to obtain $\\widehat{\\alpha}_+$.\n4.  Calculate $\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-$.\n5.  Calculate and store the bias $\\widehat{\\tau}_{\\mathrm{RD}} - 3$.\nThis procedure is repeated for all five test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local linear Regression Discontinuity (RD) estimator\n    for several test cases, demonstrating boundary effects.\n    \"\"\"\n\n    # Define constants from the problem statement\n    M = 10001\n    TAU = 3.0\n\n    # Generate the deterministic grid for the running variable X\n    X_grid = np.linspace(0, 100, M)\n\n    def m0(x):\n        \"\"\"\n        Defines the potential outcome mean function for the control group.\n        \"\"\"\n        return 2.0 + 0.04 * x + 0.002 * x**2\n\n    def triangular_kernel(u):\n        \"\"\"\n        Defines the triangular kernel function.\n        \"\"\"\n        return np.maximum(0, 1 - np.abs(u))\n\n    def local_linear_fit(X_grid, Y_values, c, h, side):\n        \"\"\"\n        Performs a one-sided local linear regression and returns the\n        estimated intercept.\n\n        Args:\n            X_grid (np.ndarray): The grid of running variable values.\n            Y_values (np.ndarray): The corresponding observed outcome values.\n            c (float): The cutoff point.\n            h (float): The bandwidth.\n            side (str): 'left' or 'right' of the cutoff.\n\n        Returns:\n            float: The estimated intercept (alpha_hat).\n        \"\"\"\n        # Select data points based on side, bandwidth, and support [0, 100]\n        if side == 'right':\n            mask = (X_grid >= c) & (X_grid <= c + h)\n        elif side == 'left':\n            mask = (X_grid < c) & (X_grid >= c - h)\n        else:\n            raise ValueError(\"Side must be 'left' or 'right'\")\n        \n        X_local = X_grid[mask]\n        Y_local = Y_values[mask]\n\n        if len(X_local) < 2:\n            # Not enough points for a linear fit. This case is not expected\n            # with the problem's dense grid.\n            return np.nan\n\n        # Recenter the running variable\n        X_centered = X_local - c\n        \n        # Calculate kernel weights based on u = (X - c) / h\n        u = X_centered / h\n        weights = triangular_kernel(u)\n        \n        # Set up the weighted least squares problem: Y = alpha + beta * (X - c)\n        # Construct the design matrix.\n        design_matrix = np.vstack([np.ones(len(X_centered)), X_centered]).T\n\n        # Solve the normal equations: (X'WX)b = X'Wy\n        # This is more efficient than creating a diagonal weight matrix.\n        XT_W_X = design_matrix.T @ (weights[:, np.newaxis] * design_matrix)\n        XT_W_y = design_matrix.T @ (weights * Y_local)\n\n        try:\n            # Solve for the coefficient vector [alpha, beta]\n            coeffs = np.linalg.solve(XT_W_X, XT_W_y)\n            alpha_hat = coeffs[0]\n        except np.linalg.LinAlgError:\n            # This would occur if the matrix is singular.\n            alpha_hat = np.nan\n            \n        return alpha_hat\n\n    def compute_bias(c, h, X_grid, m0_func, tau_val):\n        \"\"\"\n        Computes the RD estimator bias for a given (c, h) pair.\n        \"\"\"\n        # Generate the observed outcome Y based on the cutoff c\n        Y_values = m0_func(X_grid) + tau_val * (X_grid >= c)\n        \n        # Get intercept estimates from the left and right sides\n        alpha_hat_plus = local_linear_fit(X_grid, Y_values, c, h, side='right')\n        alpha_hat_minus = local_linear_fit(X_grid, Y_values, c, h, side='left')\n        \n        # Compute the RD estimate of the treatment effect\n        tau_hat_rd = alpha_hat_plus - alpha_hat_minus\n        \n        # Compute the final bias\n        bias = tau_hat_rd - tau_val\n        return bias\n\n    # Define the test suite of (c, h) pairs\n    test_cases = [\n        (50, 10),\n        (95, 10),\n        (98, 10),\n        (95, 5),\n        (5, 10),\n    ]\n\n    results = []\n    for c, h in test_cases:\n        bias_result = compute_bias(c, h, X_grid, m0, TAU)\n        results.append(bias_result)\n\n    # Format the output as a comma-separated list in brackets,\n    # with consistent decimal formatting.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3168448"}]}