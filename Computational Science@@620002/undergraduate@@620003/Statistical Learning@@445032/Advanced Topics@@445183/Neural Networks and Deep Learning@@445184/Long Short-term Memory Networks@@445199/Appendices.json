{"hands_on_practices": [{"introduction": "To understand the power of Long Short-Term Memory networks (LSTMs), we must first appreciate the problem they were designed to solve: capturing long-term dependencies. This practice uses a simplified analytical model of the \"adding problem\" to demonstrate how the gradient signal, essential for learning, decays over time in different recurrent architectures [@problem_id:3191191]. By comparing a simple Recurrent Neural Network (RNN) with its gated counterparts, the Gated Recurrent Unit (GRU) and LSTM, you will quantitatively see why the additive update mechanism of LSTMs is crucial for preserving information over long sequences.", "problem": "You are asked to formalize and analyze the well-known \"adding problem\" in sequence learning, focusing on the issue of long-term dependencies in deep learning. The adding problem is defined as follows: given a sequence of real numbers in the interval $[0,1]$ of length $L$, exactly two positions are marked, and the target is the sum of the values at the marked positions. A model processes the sequence step by step and outputs a single scalar at the final time step. To isolate the effect of long-time dependencies, consider the worst case in which the first marked position is at $t=1$ and the final output is produced at $t=L$, so the learning signal for the earliest relevant time step must traverse $L-1$ recurrent transitions.\n\nWork with three architectures:\n- Recurrent Neural Network (RNN) with recurrent update using a smooth nonlinearity,\n- Gated Recurrent Unit (GRU),\n- Long Short-Term Memory (LSTM).\n\nAssume the following scientifically standard and widely accepted foundations:\n- The chain rule of differentiation for compositions of functions applies to backpropagation through time, so that gradients at earlier time steps are products of Jacobians across time steps.\n- For stability, the magnitude of backpropagated gradients is governed by the operator norm of the recurrent Jacobian factors along the path that carries memory across time steps.\n- The recurrent Jacobian for a Recurrent Neural Network (RNN) near the origin is dominated by the recurrent weight matrix, and its long-time behavior is controlled by its spectral radius.\n- The gating mechanisms in Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) architectures multiply the previous state by gate values, thereby directly scaling the gradient that flows through the main memory path.\n\nTo make the comparison explicit and computationally tractable, adopt the following consistent simplifications for the analysis:\n- Treat the recurrent Jacobian of the Recurrent Neural Network (RNN) as approximately time-invariant with spectral radius $\\rho$.\n- Treat the forget gate of the Long Short-Term Memory (LSTM) and the update gate of the Gated Recurrent Unit (GRU) as constant scalars across time: the LSTM forget gate is $f$, and the GRU update gate is $z$.\n- Use a normalized output and loss so that constant multiplicative factors outside the recurrent pathway can be absorbed into a single positive scalar $\\alpha$.\n\nParameter values to use in all computations:\n- $\\rho = 0.90$ for the Recurrent Neural Network (RNN),\n- $f = 0.99$ for the Long Short-Term Memory (LSTM),\n- $z = 0.05$ for the Gated Recurrent Unit (GRU),\n- $\\alpha = 1.0$.\n\nDefine the \"training-signal magnitude\" for the earliest marked input as the magnitude of the gradient of the loss with respect to the earliest marked input, propagated along the principal memory path from time $t=L$ back to $t=1$. Using the foundations stated above (chain rule and product of Jacobians), derive expressions for the training-signal magnitude for each architecture as functions of $L$ and the given parameters. Then implement these expressions in a program.\n\nTest suite:\n- Sequence lengths $L \\in \\{50, 75, 100, 500, 1000\\}$.\n- For each $L$, compute the training-signal magnitude for each architecture in the order: Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where the list is flattened across the test suite in ascending order of $L$. That is, the output must be:\n$[$RNN$(50)$,GRU$(50)$,LSTM$(50)$,RNN$(75)$,GRU$(75)$,LSTM$(75)$,RNN$(100)$,GRU$(100)$,LSTM$(100)$,RNN$(500)$,GRU$(500)$,LSTM$(500)$,RNN$(1000)$,GRU$(1000)$,LSTM$(1000)]$,\nwhere each entry is a float representing the training-signal magnitude (unitless). The final outputs are floats; no physical units or angles are involved, and no percentages must be printed.", "solution": "The user requests a formal analysis of the \"adding problem\" to compare the ability of three recurrent architectures—Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM)—to handle long-term dependencies. The analysis focuses on the magnitude of the gradient signal propagated from the final time step back to the first relevant input.\n\nThe problem is validated as follows:\n- **Step 1: Extract Givens**\n  - Problem: The \"adding problem\" for a sequence of length $L$.\n  - Goal: Compute the \"training-signal magnitude\" for an input at time $t=1$ when the output is at $t=L$.\n  - Architectures: RNN, GRU, LSTM.\n  - Foundations:\n    - Backpropagation through time is based on the chain rule.\n    - Gradient magnitude is governed by the operator norm of recurrent Jacobians.\n    - RNN Jacobian behavior is controlled by the spectral radius of the recurrent weight matrix.\n    - Gating mechanisms in GRU and LSTM scale the gradient flow.\n  - Simplifications for analysis:\n    - RNN recurrent Jacobian is treated as time-invariant with spectral radius $\\rho$.\n    - LSTM forget gate is a constant scalar $f$.\n    - GRU update gate is a constant scalar $z$.\n    - A single positive scalar $\\alpha$ absorbs all constant multiplicative factors outside the recurrent pathway.\n  - Parameter values:\n    - $\\rho = 0.90$\n    - $f = 0.99$\n    - $z = 0.05$\n    - $\\alpha = 1.0$\n  - Test Suite:\n    - Sequence lengths $L \\in \\{50, 75, 100, 500, 1000\\}$.\n    - Computation order: RNN, GRU, LSTM for each $L$.\n\n- **Step 2: Validate Using Extracted Givens**\n  - **Scientifically Grounded:** The problem is firmly rooted in the established principles of deep learning, specifically the analysis of gradient flow in recurrent networks (the vanishing/exploding gradient problem). The simplifications are standard for creating a tractable analytical model of a complex system.\n  - **Well-Posed:** The problem is clearly defined with all necessary parameters and assumptions, leading to a unique, computable solution for each case.\n  - **Objective:** The language is precise and technical, free of subjectivity.\n\n- **Step 3: Verdict and Action**\n  - The problem is deemed **valid**. The simplifications are explicitly stated and serve to isolate the core mechanism of gradient propagation, which is a standard and informative analytical technique. Proceeding with the solution.\n\nThe core principle for analyzing the training-signal magnitude is backpropagation through time (BPTT). The gradient of the loss function $\\mathcal{L}$ with respect to the hidden state $h_t$ at some time step $t$ is computed via the chain rule by propagating the gradient from a later time step $t+1$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\nTo find the gradient with respect to the state at time $t=1$ from the loss computed at time $t=L$, we must apply this rule recursively for $L-1$ steps:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_L} \\left( \\prod_{t=2}^{L} \\frac{\\partial h_t}{\\partial h_{t-1}} \\right)\n$$\nThe term $\\frac{\\partial h_t}{\\partial h_{t-1}}$ is the recurrent Jacobian matrix at time $t$. The \"training-signal magnitude\" for the input at $t=1$ is dominated by the magnitude of this product of Jacobians, which determines how much the error signal from the output at $t=L$ is amplified or diminished as it travels back to $t=1$. The problem defines a constant $\\alpha$ to absorb all non-recurrent factors, such as $\\frac{\\partial \\mathcal{L}}{\\partial h_L}$ and the final step $\\frac{\\partial h_1}{\\partial x_1}$. Thus, the signal magnitude $S(L)$ is proportional to the norm of the product of Jacobians. Let's analyze this for each architecture.\n\n**Recurrent Neural Network (RNN)**\nThe RNN hidden state update is of the form $h_t = \\phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$, where $\\phi$ is a nonlinear activation function like $\\tanh$. The recurrent Jacobian is $\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(\\phi'(...))W_{hh}$. The long-term behavior of the product of these matrices is governed by the spectral radius $\\rho$ of the recurrent weight matrix $W_{hh}$. The problem simplifies the analysis by assuming the effective magnitude contribution of each Jacobian step is a constant factor $\\rho$. Propagating the signal across $L-1$ time steps results in this factor being multiplied $L-1$ times.\nThe training-signal magnitude $S_{RNN}$ for a sequence of length $L$ is therefore:\n$$\nS_{RNN}(L) = \\alpha \\rho^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $\\rho = 0.90$, we have:\n$$\nS_{RNN}(L) = (0.90)^{L-1}\n$$\n\n**Long Short-Term Memory (LSTM)**\nThe key to the LSTM's ability to handle long dependencies is its cell state, $c_t$, which is updated via a gating mechanism: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$. Here, $f_t$ is the forget gate and $\\odot$ denotes element-wise multiplication. The gradient pathway through the cell state from $c_t$ to $c_{t-1}$ is primarily scaled by the forget gate, i.e., $\\frac{\\partial c_t}{\\partial c_{t-1}} \\approx \\text{diag}(f_t)$. The problem simplifies this by assuming the forget gate is a constant scalar $f$ for all time steps. The gradient signal flowing back through the main memory path is scaled by $f$ at each of the $L-1$ steps.\nThe training-signal magnitude $S_{LSTM}$ is:\n$$\nS_{LSTM}(L) = \\alpha f^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $f = 0.99$, we have:\n$$\nS_{LSTM}(L) = (0.99)^{L-1}\n$$\n\n**Gated Recurrent Unit (GRU)**\nThe GRU state update is $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$, where $z_t$ is the update gate. The term $(1-z_t)$ acts as a dynamic forget gate, controlling how much of the previous state $h_{t-1}$ is passed to the current state $h_t$. The gradient of $h_t$ with respect to $h_{t-1}$ is therefore directly scaled by this factor. The problem simplifies the analysis by assuming a constant scalar update gate $z$. Consequently, the scaling factor at each of the $L-1$ backpropagation steps is $(1-z)$.\nThe training-signal magnitude $S_{GRU}$ is:\n$$\nS_{GRU}(L) = \\alpha (1-z)^{L-1}\n$$\nGiven $\\alpha = 1.0$ and $z = 0.05$, the scaling factor is $(1-0.05) = 0.95$. We have:\n$$\nS_{GRU}(L) = (0.95)^{L-1}\n$$\n\nThese derived expressions will now be implemented to compute the required values for the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training-signal magnitude for RNN, GRU, and LSTM architectures\n    for the \"adding problem\" over various sequence lengths.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    rho_rnn = 0.90  # Spectral radius for RNN\n    f_lstm = 0.99   # Forget gate value for LSTM\n    z_gru = 0.05    # Update gate value for GRU\n    alpha = 1.0     # Constant multiplicative factor (can be omitted as it's 1.0)\n    \n    # Define the test cases for sequence length L from the problem statement.\n    test_cases_L = [50, 75, 100, 500, 1000]\n\n    results = []\n    for L in test_cases_L:\n        # The number of recurrent transitions is L-1.\n        exponent = L - 1\n\n        # Calculate the training-signal magnitude for each architecture.\n        # S(L) = alpha * (base)^(L-1)\n        \n        # RNN\n        s_rnn = alpha * (rho_rnn ** exponent)\n        \n        # GRU\n        # The scaling factor is (1 - z)\n        s_gru = alpha * ((1 - z_gru) ** exponent)\n        \n        # LSTM\n        s_lstm = alpha * (f_lstm ** exponent)\n        \n        # Append results in the specified order: RNN, GRU, LSTM\n        results.append(s_rnn)\n        results.append(s_gru)\n        results.append(s_lstm)\n\n    # Format the final output as a comma-separated list of floats in brackets.\n    # e.g., [val1,val2,val3,...]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3191191"}, {"introduction": "After seeing *why* LSTMs are needed, we now dive into *how* they work by examining the core of their memory: the cell state $c_t$. The forget gate $f_t$ and input gate $i_t$ directly control the flow of information, allowing the cell state to act as a conveyor belt that can carry information unchanged across many time steps [@problem_id:3142761]. This exercise guides you from a formal proof of perfect memory conservation to the implementation of a practical diagnostic tool, solidifying your understanding of how gates enable LSTMs to selectively preserve or update their internal state.", "problem": "Consider a Long Short-Term Memory (LSTM) cell whose cell state update is defined by the fundamental, element-wise recurrence\n$$\n\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t,\n$$\nwhere $t\\in\\{1,2,\\ldots,T\\}$, $\\mathbf{c}_t\\in\\mathbb{R}^d$ is the cell state at time $t$, $\\mathbf{f}_t\\in[0,1]^d$ is the forget gate, $\\mathbf{i}_t\\in[0,1]^d$ is the input gate, $\\mathbf{g}_t\\in[-1,1]^d$ is the candidate cell input, and $\\odot$ denotes the Hadamard product. Assume an initial state $\\mathbf{c}_0\\in\\mathbb{R}^d$ is given. Work entirely from this definition and standard properties of vector norms and inequalities.\n\nTasks:\n1) Show that if $\\mathbf{f}_t=\\mathbf{1}$ and $\\mathbf{i}_t=\\mathbf{0}$ for all $t$, then $\\mathbf{c}_t=\\mathbf{c}_0$ for all $t$. Your reasoning should use only the cell update definition and a valid proof technique.\n2) Propose a principled diagnostic to detect near-conserved memory segments in a trained network when $\\mathbf{c}_t$ is observable. Define a scalar per-step measure $s_t$ that quantifies relative change using the Euclidean norm, explain why it is appropriate based on the defining recurrence and norm inequalities, and define a near-conserved segment as a contiguous run of time indices where $s_t$ stays below a given tolerance $\\varepsilon>0$.\n3) Implement the diagnostic as an algorithm that, given $\\{\\mathbf{c}_t\\}_{t=0}^T$, a tolerance $\\varepsilon>0$, and a minimum segment length $L\\in\\mathbb{N}$, returns:\n   - the integer length of the longest run of consecutive time steps $t\\in\\{1,\\ldots,T\\}$ such that $s_t\\le \\varepsilon$, and\n   - a boolean indicating whether there exists at least one run of length at least $L$.\nUse the Euclidean norm and define\n$$\ns_t=\\frac{\\lVert \\mathbf{c}_t-\\mathbf{c}_{t-1}\\rVert_2}{\\lVert \\mathbf{c}_{t-1}\\rVert_2+\\delta},\n$$\nwith a fixed numerical stabilizer $\\delta=10^{-12}$.\n4) For each test case below, first generate $\\{\\mathbf{c}_t\\}_{t=0}^T$ by simulating the LSTM recurrence with the specified $\\mathbf{f}_t$, $\\mathbf{i}_t$, $\\mathbf{g}_t$, and $\\mathbf{c}_0$. Then apply your diagnostic to produce the two required outputs.\n\nTest suite:\n- Case A (perfect conservation): $T=20$, $d=3$, $\\mathbf{c}_0=[2,-1,0.5]$, $\\mathbf{f}_t=\\mathbf{1}$, $\\mathbf{i}_t=\\mathbf{0}$, $\\mathbf{g}_t=[0.3,-0.4,0.5]$ for all $t$, $\\varepsilon=10^{-12}$, $L=10$.\n- Case B (slow exponential drift via forget gate): $T=30$, $d=1$, $\\mathbf{c}_0=[1.0]$, $\\mathbf{f}_t=[0.99]$, $\\mathbf{i}_t=[0.0]$, $\\mathbf{g}_t=[0.0]$ for all $t$, $\\varepsilon=0.005$, $L=5$.\n- Case C (small constant input injection): $T=25$, $d=2$, $\\mathbf{c}_0=[1.0,1.0]$, $\\mathbf{f}_t=[1.0,1.0]$, $\\mathbf{i}_t=[0.02,0.02]$, $\\mathbf{g}_t=[0.5,0.5]$ for all $t$, $\\varepsilon=0.02$, $L=10$.\n- Case D (zero state, exact conservation): $T=10$, $d=3$, $\\mathbf{c}_0=[0.0,0.0,0.0]$, $\\mathbf{f}_t=\\mathbf{1}$, $\\mathbf{i}_t=\\mathbf{0}$, $\\mathbf{g}_t=[0.7,-0.3,0.1]$ for all $t$, $\\varepsilon=10^{-12}$, $L=5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case, in the order A, B, C, D, and is itself a two-element list containing:\n- the integer length of the longest near-conserved run of steps, and\n- the boolean indicating whether there exists a run of at least length $L$.\nThe line must have no spaces. For example, a valid output with two cases would look like $[[3,\\mathrm{True}],[0,\\mathrm{False}]]$.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of recurrent neural networks, specifically the Long Short-Term Memory (LSTM) cell architecture. The tasks are well-posed, objective, and self-contained, providing all necessary definitions, equations, and data for a unique solution. The problem requires a combination of mathematical proof, principled a priori reasoning, and algorithmic implementation, which is a standard format for an advanced computational science problem.\n\n### Task 1: Proof of State Conservation\n\nWe are asked to show that if the forget gate $\\mathbf{f}_t = \\mathbf{1}$ (a vector of all ones) and the input gate $\\mathbf{i}_t = \\mathbf{0}$ (a vector of all zeros) for all time steps $t \\in \\{1, 2, \\ldots, T\\}$, then the cell state remains constant, i.e., $\\mathbf{c}_t = \\mathbf{c}_0$ for all $t$.\n\nWe will use the principle of mathematical induction. Let $P(t)$ be the proposition $\\mathbf{c}_t = \\mathbf{c}_0$.\n\n**Base Case:** We must show that $P(1)$ is true. The cell state update equation is given as:\n$$\n\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t\n$$\nFor $t=1$, this becomes:\n$$\n\\mathbf{c}_1=\\mathbf{f}_1\\odot \\mathbf{c}_0+\\mathbf{i}_1\\odot \\mathbf{g}_1\n$$\nSubstituting the given conditions $\\mathbf{f}_1 = \\mathbf{1}$ and $\\mathbf{i}_1 = \\mathbf{0}$:\n$$\n\\mathbf{c}_1 = \\mathbf{1} \\odot \\mathbf{c}_0 + \\mathbf{0} \\odot \\mathbf{g}_1\n$$\nThe Hadamard product with the vector $\\mathbf{1}$ is the identity operation ($\\mathbf{1} \\odot \\mathbf{v} = \\mathbf{v}$), and the product with the vector $\\mathbf{0}$ results in the zero vector ($\\mathbf{0} \\odot \\mathbf{v} = \\mathbf{0}$). Thus:\n$$\n\\mathbf{c}_1 = \\mathbf{c}_0 + \\mathbf{0} = \\mathbf{c}_0\n$$\nThe base case $P(1)$ holds.\n\n**Inductive Hypothesis:** Assume that for some integer $k \\ge 1$, the proposition $P(k)$ is true, meaning $\\mathbf{c}_k = \\mathbf{c}_0$.\n\n**Inductive Step:** We must show that $P(k+1)$ is true, i.e., $\\mathbf{c}_{k+1} = \\mathbf{c}_0$. The update rule for $t=k+1$ is:\n$$\n\\mathbf{c}_{k+1}=\\mathbf{f}_{k+1}\\odot \\mathbf{c}_k+\\mathbf{i}_{k+1}\\odot \\mathbf{g}_{k+1}\n$$\nSubstituting the given conditions $\\mathbf{f}_{k+1} = \\mathbf{1}$ and $\\mathbf{i}_{k+1} = \\mathbf{0}$:\n$$\n\\mathbf{c}_{k+1}=\\mathbf{1}\\odot \\mathbf{c}_k+\\mathbf{0}\\odot \\mathbf{g}_{k+1}\n$$\nNow, we apply the inductive hypothesis, $\\mathbf{c}_k = \\mathbf{c}_0$:\n$$\n\\mathbf{c}_{k+1} = \\mathbf{1} \\odot \\mathbf{c}_0 + \\mathbf{0}\n$$\n$$\n\\mathbf{c}_{k+1} = \\mathbf{c}_0\n$$\nThus, $P(k+1)$ is true.\n\nBy the principle of mathematical induction, $\\mathbf{c}_t = \\mathbf{c}_0$ for all integers $t \\ge 1$. This completes the proof.\n\n### Task 2: Principled Diagnostic for Near-Conserved Memory\n\nThe goal is to design a scalar measure $s_t$ that quantifies the change in the cell state $\\mathbf{c}_t$ from one time step to the next. A state is \"conserved\" if it does not change, i.e., $\\mathbf{c}_t \\approx \\mathbf{c}_{t-1}$.\n\nLet's analyze the change vector, $\\Delta\\mathbf{c}_t = \\mathbf{c}_t - \\mathbf{c}_{t-1}$. Using the recurrence relation:\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t) - \\mathbf{c}_{t-1}\n$$\nBy adding and subtracting $\\mathbf{1}\\odot\\mathbf{c}_{t-1} = \\mathbf{c}_{t-1}$, we can isolate the effects of the gates:\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t\\odot \\mathbf{c}_{t-1} - \\mathbf{c}_{t-1}) + (\\mathbf{i}_t\\odot \\mathbf{g}_t)\n$$\nFactoring out $\\mathbf{c}_{t-1}$ from the first term gives:\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t - \\mathbf{1})\\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t\\odot \\mathbf{g}_t\n$$\nThis equation reveals the two sources of change in the cell state:\n1.  **Forgetting:** The term $(\\mathbf{f}_t - \\mathbf{1})\\odot \\mathbf{c}_{t-1}$ represents the portion of the previous state $\\mathbf{c}_{t-1}$ that is \"forgotten.\" Conservation requires this term to be near zero, which occurs when $\\mathbf{f}_t \\approx \\mathbf{1}$.\n2.  **Input:** The term $\\mathbf{i}_t\\odot \\mathbf{g}_t$ represents the new information being written to the cell state. Conservation requires this term to be near zero, which occurs when $\\mathbf{i}_t \\approx \\mathbf{0}$ (since $\\mathbf{g}_t$ is bounded).\n\nTo quantify the magnitude of this change, we use the Euclidean norm, $\\lVert \\Delta\\mathbf{c}_t \\rVert_2 = \\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$. This gives an absolute measure of change. However, a change of a certain magnitude might be significant for a small state vector but negligible for a large one. Therefore, a more robust measure is the *relative* change, which normalizes the absolute change by the magnitude of the state vector itself.\n\nThe proposed diagnostic,\n$$\ns_t=\\frac{\\lVert \\mathbf{c}_t-\\mathbf{c}_{t-1}\\rVert_2}{\\lVert \\mathbf{c}_{t-1}\\rVert_2+\\delta},\n$$\nis precisely this relative change. It measures the magnitude of the change vector as a fraction of the magnitude of the previous state vector.\n\n*   By using the Euclidean norm, it provides a scalar summary of the total change across all dimensions $d$ of the state vector.\n*   By normalizing by $\\lVert \\mathbf{c}_{t-1} \\rVert_2$, it becomes a scale-invariant measure.\n*   The small constant $\\delta > 0$ is a standard numerical stabilizer to prevent division by zero in the case where $\\mathbf{c}_{t-1} = \\mathbf{0}$. In this special case, as shown in test case D, if $\\mathbf{f}_t=\\mathbf{1}$ and $\\mathbf{i}_t=\\mathbf{0}$, then $\\mathbf{c}_t = \\mathbf{c}_{t-1} = \\mathbf{0}$, the numerator is zero, and $s_t=0$, correctly indicating perfect conservation.\n\nA very small value of $s_t$ (i.e., $s_t \\le \\varepsilon$ for some small $\\varepsilon > 0$) indicates that $\\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$ is small compared to $\\lVert \\mathbf{c}_{t-1} \\rVert_2$. This condition is met when both sources of change, forgetting and input, are minimal—that is, when $\\mathbf{f}_t \\approx \\mathbf{1}$ and $\\mathbf{i}_t \\approx \\mathbf{0}$. Therefore, $s_t$ is a principled and appropriate diagnostic for detecting near-conserved memory segments.\n\n### Task 3: Algorithm for Diagnostic Implementation\n\nThe algorithm takes as input the sequence of cell states $\\{\\mathbf{c}_t\\}_{t=0}^T$, a tolerance $\\varepsilon$, and a minimum length $L$. It proceeds in two main stages.\n\n**Stage 1: Compute Per-Step Relative Change**\n1.  Initialize an empty list, `s_values`.\n2.  Iterate for $t$ from $1$ to $T$.\n    a. Compute the numerator: `norm_diff` = $\\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$.\n    b. Compute the denominator: `norm_prev` = $\\lVert \\mathbf{c}_{t-1} \\rVert_2 + \\delta$.\n    c. Calculate $s_t = \\frac{\\text{norm\\_diff}}{\\text{norm\\_prev}}$ and append it to `s_values`.\n\n**Stage 2: Find Longest Run of Near-Conservation**\n1.  Initialize `max_run_length = 0` and `current_run_length = 0`.\n2.  Iterate through `s_values` (corresponding to $t=1, \\ldots, T$).\n    a. If the current value $s_t \\le \\varepsilon$:\n       i. Increment `current_run_length` by $1$.\n    b. Else (the run is broken):\n       i. Update `max_run_length = \\max(\\text{max\\_run\\_length, current\\_run\\_length})`.\n       ii. Reset `current_run_length = 0`.\n3.  After the loop finishes, perform one final update to account for a run that extends to the end of the sequence: `max_run_length = \\max(\\text{max\\_run\\_length, current\\_run\\_length})`.\n4.  Determine the boolean output: `found_long_run = (\\text{max\\_run\\_length} \\ge L)`.\n5.  Return the tuple (`max_run_length`, `found_long_run`).\n\nThis algorithm correctly implements the required diagnostic by first calculating the per-step measure and then applying a standard linear scan to find the longest contiguous subsequence satisfying the given condition.\n\n### Task 4: Application to Test Cases\n\nThe algorithm described above, combined with a preliminary simulation of the LSTM recurrence for each test case, yields the results. The simulation step simply applies the equation $\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t$ iteratively for $t=1, \\ldots, T$. The implementation is provided in the final answer.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LSTM memory conservation problem by simulating the cell state\n    updates for given test cases and then applying a diagnostic to find\n    near-conserved segments.\n    \"\"\"\n    \n    delta = 1e-12\n\n    test_cases = [\n        {\n            # Case A: perfect conservation\n            \"T\": 20, \"d\": 3, \"c0\": np.array([2.0, -1.0, 0.5]),\n            \"ft_gen\": lambda t: np.ones(3),\n            \"it_gen\": lambda t: np.zeros(3),\n            \"gt_gen\": lambda t: np.array([0.3, -0.4, 0.5]),\n            \"epsilon\": 1e-12, \"L\": 10\n        },\n        {\n            # Case B: slow exponential drift via forget gate\n            \"T\": 30, \"d\": 1, \"c0\": np.array([1.0]),\n            \"ft_gen\": lambda t: np.array([0.99]),\n            \"it_gen\": lambda t: np.array([0.0]),\n            \"gt_gen\": lambda t: np.array([0.0]),\n            \"epsilon\": 0.005, \"L\": 5\n        },\n        {\n            # Case C: small constant input injection\n            \"T\": 25, \"d\": 2, \"c0\": np.array([1.0, 1.0]),\n            \"ft_gen\": lambda t: np.ones(2),\n            \"it_gen\": lambda t: np.array([0.02, 0.02]),\n            \"gt_gen\": lambda t: np.array([0.5, 0.5]),\n            \"epsilon\": 0.02, \"L\": 10\n        },\n        {\n            # Case D: zero state, exact conservation\n            \"T\": 10, \"d\": 3, \"c0\": np.array([0.0, 0.0, 0.0]),\n            \"ft_gen\": lambda t: np.ones(3),\n            \"it_gen\": lambda t: np.zeros(3),\n            \"gt_gen\": lambda t: np.array([0.7, -0.3, 0.1]),\n            \"epsilon\": 1e-12, \"L\": 5\n        }\n    ]\n\n    def simulate_lstm(T, c0, ft_gen, it_gen, gt_gen):\n        \"\"\"Generates the cell state sequence {c_t}.\"\"\"\n        c_series = [c0]\n        c_prev = c0\n        for t in range(1, T + 1):\n            ft = ft_gen(t)\n            it = it_gen(t)\n            gt = gt_gen(t)\n            # LSTM cell state update recurrence\n            c_t = ft * c_prev + it * gt\n            c_series.append(c_t)\n            c_prev = c_t\n        return c_series\n\n    def run_diagnostic(c_series, epsilon, L):\n        \"\"\"\n        Calculates the longest run of near-conservation and checks if it\n        meets the minimum length L.\n        \"\"\"\n        T = len(c_series) - 1\n        \n        # Stage 1: Compute per-step relative change s_t\n        s_values = []\n        for t in range(1, T + 1):\n            c_t = c_series[t]\n            c_prev = c_series[t-1]\n            \n            norm_diff = np.linalg.norm(c_t - c_prev, ord=2)\n            norm_prev = np.linalg.norm(c_prev, ord=2)\n            \n            s_t = norm_diff / (norm_prev + delta)\n            s_values.append(s_t)\n            \n        # Stage 2: Find the longest run where s_t = epsilon\n        max_run_length = 0\n        current_run_length = 0\n        for s_t in s_values:\n            if s_t = epsilon:\n                current_run_length += 1\n            else:\n                if current_run_length > max_run_length:\n                    max_run_length = current_run_length\n                current_run_length = 0\n        \n        # Final check for a run extending to the end\n        if current_run_length > max_run_length:\n            max_run_length = current_run_length\n            \n        found_long_run = max_run_length >= L\n        \n        return [max_run_length, found_long_run]\n\n    results = []\n    for case in test_cases:\n        # Generate the time series data for the cell states\n        c_series = simulate_lstm(\n            case[\"T\"], case[\"c0\"], case[\"ft_gen\"],\n            case[\"it_gen\"], case[\"gt_gen\"]\n        )\n        # Apply the diagnostic to the generated series\n        result = run_diagnostic(c_series, case[\"epsilon\"], case[\"L\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    case_strings = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output_string = f\"[{','.join(case_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3142761"}, {"introduction": "While the forget gate allows an LSTM to maintain long-term memory, the input gate enables it to selectively store new information. This exercise places the LSTM in a practical context of analyzing biological sequences to isolate the function of the input gate, $i_t$ [@problem_id:2425706]. By simulating an LSTM processing a DNA sequence and calculating a \"motif write magnitude,\" you will see how the network's ability to learn is nullified when the input gate is \"knocked out,\" providing a clear intuition for its role in updating the cell's memory based on specific input patterns.", "problem": "Consider a single-layer Long Short-Term Memory (LSTM) recurrent neural network defined for deoxyribonucleic acid (DNA) sequence analysis with one-hot nucleotide inputs. Let the input at time step $t$ be $x_t \\in \\mathbb{R}^{4}$, encoding the nucleotide as $x_t = [\\mathbb{1}\\{A\\}, \\mathbb{1}\\{C\\}, \\mathbb{1}\\{G\\}, \\mathbb{1}\\{T\\}]$. The hidden state is $h_t \\in \\mathbb{R}^{2}$ and the cell state is $c_t \\in \\mathbb{R}^{2}$. The LSTM dynamics for $t \\in \\{1,\\dots,T\\}$ are defined by the equations\n$$\ni_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad\nf_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad\no_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o),\n$$\n$$\ng_t = \\tanh(W_g x_t + U_g h_{t-1} + b_g),\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t, \\quad\nh_t = o_t \\odot \\tanh(c_t),\n$$\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$, and $\\odot$ denotes elementwise multiplication. The initial conditions are $h_0 = [0,0]^\\top$ and $c_0 = [0,0]^\\top$.\n\nAll weight matrices and bias vectors are fixed and given as follows. For the input gate:\n$$\nW_i = \\begin{bmatrix}\n1.0  -1.0  0.5  0.0 \\\\\n0.3  0.8  -0.5  0.2\n\\end{bmatrix}, \\quad\nU_i = \\begin{bmatrix}\n0.2  -0.1 \\\\\n0.1  0.2\n\\end{bmatrix}, \\quad\nb_i = \\begin{bmatrix}\n0.0 \\\\\n-0.2\n\\end{bmatrix}.\n$$\nFor the forget gate:\n$$\nW_f = \\begin{bmatrix}\n-0.5  0.6  0.1  0.3 \\\\\n0.2  -0.3  0.4  -0.2\n\\end{bmatrix}, \\quad\nU_f = \\begin{bmatrix}\n0.1  0.0 \\\\\n0.0  0.1\n\\end{bmatrix}, \\quad\nb_f = \\begin{bmatrix}\n0.5 \\\\\n0.5\n\\end{bmatrix}.\n$$\nFor the output gate:\n$$\nW_o = \\begin{bmatrix}\n0.7  -0.4  0.0  0.2 \\\\\n-0.6  0.1  0.3  0.5\n\\end{bmatrix}, \\quad\nU_o = \\begin{bmatrix}\n0.2  0.1 \\\\\n-0.1  0.2\n\\end{bmatrix}, \\quad\nb_o = \\begin{bmatrix}\n0.0 \\\\\n0.1\n\\end{bmatrix}.\n$$\nFor the candidate state:\n$$\nW_g = \\begin{bmatrix}\n0.3  0.3  -0.2  0.1 \\\\\n0.5  -0.7  0.2  -0.1\n\\end{bmatrix}, \\quad\nU_g = \\begin{bmatrix}\n0.1  -0.2 \\\\\n0.2  0.1\n\\end{bmatrix}, \\quad\nb_g = \\begin{bmatrix}\n0.0 \\\\\n0.0\n\\end{bmatrix}.\n$$\n\nDefine a motif as a fixed nucleotide string of length $m$, for example $m = 3$ for the motif $ACG$. For any DNA sequence of length $T$, define the motif write magnitude as\n$$\n\\mathcal{M} = \\sum_{t \\in \\mathcal{I}} \\left\\| i_t \\odot g_t \\right\\|_1,\n$$\nwhere $\\left\\| \\cdot \\right\\|_1$ denotes the sum of absolute values over the two coordinates, and $\\mathcal{I}$ is the set of time indices that belong to any occurrence of the motif (if the motif occurs at starting index $s$, then $\\{s, s+1, \\dots, s+m-1\\} \\subset \\mathcal{I}$).\n\nTo simulate an input gate knockout, set $i_t \\equiv [0, 0]^\\top$ for all $t$ and keep all other equations unchanged. To simulate a forget-bias boundary condition, add a constant offset $\\Delta \\in \\mathbb{R}$ to both entries of $b_f$, i.e., use $b_f' = b_f + [\\Delta, \\Delta]^\\top$.\n\nYour task is to compute the motif write magnitude $\\mathcal{M}$ for each of the following test cases. The one-hot encoding is $A \\mapsto [1,0,0,0]$, $C \\mapsto [0,1,0,0]$, $G \\mapsto [0,0,1,0]$, $T \\mapsto [0,0,0,1]$. The initial conditions $h_0$ and $c_0$ are as specified above for all cases.\n\nTest suite:\n- Case $1$ (general case): sequence $S_1$ of length $T = 63$ given by $T^{30}$ concatenated with $ACG$ concatenated with $T^{30}$; motif $ACG$; no knockout; forget-bias offset $\\Delta = 0$.\n- Case $2$ (input gate knockout): same as Case $1$ but with input gate knockout (i.e., $i_t \\equiv 0$ for all $t$).\n- Case $3$ (boundary condition with high forget gate): same as Case $1$ but with forget-bias offset $\\Delta = 5.0$ applied to both entries of $b_f$.\n- Case $4$ (no motif present): sequence $S_4$ of length $T = 63$ given by $T^{63}$; motif $ACG$; no knockout; forget-bias offset $\\Delta = 0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_k$ is the motif write magnitude $\\mathcal{M}$ for Case $k$, rounded to $6$ decimal places. No physical units are involved, and no angles or percentages are used. The output must be deterministic given the definitions above.", "solution": "The problem presented is a well-posed and scientifically grounded computational task. It requires the implementation of a standard Long Short-Term Memory (LSTM) network forward pass, based on a complete and consistent set of defining equations and parameters. The objective is to calculate a precisely defined metric, the motif write magnitude $\\mathcal{M}$, under four distinct scenarios. The problem is valid and admits a unique, deterministic solution for each case.\n\nThe core of the problem lies in the iterative application of the LSTM state update equations. For each time step $t$ from $1$ to $T$, the network's hidden state $h_t$ and cell state $c_t$ are computed based on the input $x_t$ and the previous states $h_{t-1}$ and $c_{t-1}$. The initial states are given as null vectors: $h_0 = [0,0]^\\top$ and $c_0 = [0,0]^\\top$.\n\nThe governing equations are:\n$$\ni_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n$$\n$$\nf_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n$$\n$$\no_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n$$\n$$\ng_t = \\tanh(W_g x_t + U_g h_{t-1} + b_g)\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n$$\n$$\nh_t = o_t \\odot \\tanh(c_t)\n$$\nThe activation functions are the sigmoid function $\\sigma(z) = (1 + e^{-z})^{-1}$ and the hyperbolic tangent $\\tanh(z)$. All weight matrices ($W$), recurrent matrices ($U$), and bias vectors ($b$) are provided with fixed numerical values. The input $x_t$ is a $4$-dimensional one-hot vector corresponding to one of the four nucleotides {A, C, G, T}.\n\nThe quantity to be computed is the motif write magnitude, defined as:\n$$\n\\mathcal{M} = \\sum_{t \\in \\mathcal{I}} \\left\\| i_t \\odot g_t \\right\\|_1\n$$\nHere, $\\mathcal{I}$ is the set of time indices $t$ corresponding to the positions of the specified motif within the input sequence. The term $i_t \\odot g_t$ represents the new information written to the cell state at time $t$. The L1-norm, $\\left\\| \\cdot \\right\\|_1$, sums the absolute values of the components of this vector, quantifying its magnitude.\n\nThe computational procedure is as follows:\n1.  Initialize $h_0$ and $c_0$ to zero vectors of dimension $2$.\n2.  For each time step $t = 1, \\dots, T$:\n    a. Determine the input vector $x_t$ from the nucleotide at the corresponding position in the input sequence.\n    b. Compute the gate activations $i_t, f_t, o_t$ and the candidate state $g_t$ using the given equations and parameters.\n    c. Calculate the term $i_t \\odot g_t$ and store it.\n    d. Update the cell state to $c_t$ and the hidden state to $h_t$. These become $c_{t-1}$ and $h_{t-1}$ for the next time step.\n3.  Identify the set of time indices $\\mathcal{I}$ where the motif occurs.\n4.  Compute $\\mathcal{M}$ by summing the L1-norms of the stored $i_t \\odot g_t$ vectors for all $t \\in \\mathcal{I}$.\n\nWe now analyze each of the four test cases.\n\nCase $1$: General case.\nThe sequence is $S_1 = T^{30}ACGT^{30}$ and the motif is $ACG$. The sequence length is $T = 63$. The motif has length $m=3$ and occurs once, starting at time step $t=31$. Therefore, the set of relevant time indices is $\\mathcal{I} = \\{31, 32, 33\\}$. The calculation proceeds by running the LSTM for all $63$ time steps, and then summing the L1-norms of the vectors $i_t \\odot g_t$ for $t=31, 32, 33$.\n\nCase $2$: Input gate knockout.\nThe simulation is identical to Case 1, but with the constraint $i_t \\equiv [0, 0]^\\top$ for all $t$. This directly implies that the term being summed, $i_t \\odot g_t$, is always the zero vector. Consequently, the sum $\\mathcal{M}$ must be exactly $0$. No elaborate computation is necessary.\n\nCase $3$: Boundary condition with high forget gate.\nThe simulation is identical to Case 1, but the forget gate bias is modified to $b_f' = b_f + [\\Delta, \\Delta]^\\top$ with $\\Delta = 5.0$. The argument to the sigmoid function for the forget gate $f_t$ will be significantly increased, causing $f_t$ to be very close to $[1, 1]^\\top$. This means the network will \"forget\" very little of its previous cell state. The calculation proceeds as in Case 1, but using $b_f'$ in place of $b_f$.\n\nCase $4$: No motif present.\nThe sequence is $S_4 = T^{63}$. The motif $ACG$ does not occur in this sequence. Therefore, the set of indices $\\mathcal{I}$ is the empty set. The sum over an empty set is, by definition, $0$. Thus, $\\mathcal{M} = 0$. No computation is necessary for this case either.\n\nThe implementation will follow this logic, performing a full forward pass for Cases 1 and 3, and providing the trivial result of $0$ for Cases 2 and 4. All numerical values must be handled with appropriate precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the motif write magnitude for an LSTM under various conditions.\n    \"\"\"\n\n    # Helper function for the sigmoid activation\n    def sigmoid(z):\n        # Clip to avoid overflow in exp, which can happen with large inputs like in Case 3\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Helper function for the tanh activation. numpy's is numerically stable.\n    def tanh(z):\n        return np.tanh(z)\n\n    # Define all fixed weight matrices and bias vectors as numpy arrays.\n    # Dimensions are (output_dim, input_dim).\n    params = {\n        'Wi': np.array([[1.0, -1.0, 0.5, 0.0], [0.3, 0.8, -0.5, 0.2]]),\n        'Ui': np.array([[0.2, -0.1], [0.1, 0.2]]),\n        'bi': np.array([[0.0], [-0.2]]),\n        'Wf': np.array([[-0.5, 0.6, 0.1, 0.3], [0.2, -0.3, 0.4, -0.2]]),\n        'Uf': np.array([[0.1, 0.0], [0.0, 0.1]]),\n        'bf': np.array([[0.5], [0.5]]),\n        'Wo': np.array([[0.7, -0.4, 0.0, 0.2], [-0.6, 0.1, 0.3, 0.5]]),\n        'Uo': np.array([[0.2, 0.1], [-0.1, 0.2]]),\n        'bo': np.array([[0.0], [0.1]]),\n        'Wg': np.array([[0.3, 0.3, -0.2, 0.1], [0.5, -0.7, 0.2, -0.1]]),\n        'Ug': np.array([[0.1, -0.2], [0.2, 0.1]]),\n        'bg': np.array([[0.0], [0.0]])\n    }\n\n    # One-hot encoding for DNA nucleotides. Shape is (4, 1).\n    one_hot_map = {\n        'A': np.array([[1], [0], [0], [0]]),\n        'C': np.array([[0], [1], [0], [0]]),\n        'G': np.array([[0], [0], [1], [0]]),\n        'T': np.array([[0], [0], [0], [1]])\n    }\n\n    def run_simulation(sequence, motif, knockout, delta_bf):\n        \"\"\"\n        Performs a full forward pass of the LSTM and calculates the motif write magnitude.\n        \"\"\"\n        # Trivial cases can be returned immediately.\n        if knockout:\n            return 0.0\n        if motif not in sequence:\n            return 0.0\n        \n        T = len(sequence)\n        h_dim = 2\n        \n        # Initial conditions for hidden and cell states\n        h_prev = np.zeros((h_dim, 1))\n        c_prev = np.zeros((h_dim, 1))\n\n        # History to store the term i_t * g_t for each time step\n        it_gt_history = []\n        \n        # Apply forget-bias offset if specified\n        bf_mod = params['bf'] + delta_bf * np.ones((h_dim, 1))\n\n        # Iterate through the sequence (time steps t=1 to T)\n        for t_idx in range(T):\n            xt = one_hot_map[sequence[t_idx]]\n            \n            # LSTM gate and state calculations\n            it = sigmoid(params['Wi'] @ xt + params['Ui'] @ h_prev + params['bi'])\n            ft = sigmoid(params['Wf'] @ xt + params['Uf'] @ h_prev + bf_mod)\n            ot = sigmoid(params['Wo'] @ xt + params['Uo'] @ h_prev + params['bo'])\n            gt = tanh(params['Wg'] @ xt + params['Ug'] @ h_prev + params['bg'])\n\n            # Store the i_t * g_t term (element-wise product)\n            it_gt_history.append(it * gt)\n\n            # Update cell state and hidden state\n            ct = ft * c_prev + it * gt\n            ht = ot * tanh(ct)\n            \n            # Update previous states for the next iteration\n            h_prev = ht\n            c_prev = ct\n\n        # Calculate the motif write magnitude M\n        motif_write_magnitude = 0.0\n        m = len(motif)\n        \n        # Find all occurrences of the motif to determine the set of indices I.\n        # This implementation correctly handles non-overlapping and overlapping motifs.\n        motif_time_indices = set()\n        for i in range(T - m + 1):\n            if sequence[i:i+m] == motif:\n                for j in range(m):\n                    # The problem uses 1-based indexing for time t, which maps to\n                    # 0-based array index i. The motif at sequence indices i to i+m-1\n                    # corresponds to time steps t=i+1 to t=i+m.\n                    motif_time_indices.add(i + j)\n        \n        # Sum the L1 norms over the identified time indices\n        for idx in motif_time_indices:\n            motif_write_magnitude += np.sum(np.abs(it_gt_history[idx]))\n            \n        return motif_write_magnitude\n\n    # Define the test cases from the problem statement.\n    seq1 = 'T' * 30 + 'ACG' + 'T' * 30\n    seq4 = 'T' * 63\n    motif_str = 'ACG'\n    \n    test_cases = [\n        # Case 1: General case\n        {'sequence': seq1, 'motif': motif_str, 'knockout': False, 'delta_bf': 0.0},\n        # Case 2: Input gate knockout\n        {'sequence': seq1, 'motif': motif_str, 'knockout': True,  'delta_bf': 0.0},\n        # Case 3: High forget gate bias\n        {'sequence': seq1, 'motif': motif_str, 'knockout': False, 'delta_bf': 5.0},\n        # Case 4: No motif present\n        {'sequence': seq4, 'motif': motif_str, 'knockout': False, 'delta_bf': 0.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(\n            case['sequence'],\n            case['motif'],\n            case['knockout'],\n            case['delta_bf']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2425706"}]}