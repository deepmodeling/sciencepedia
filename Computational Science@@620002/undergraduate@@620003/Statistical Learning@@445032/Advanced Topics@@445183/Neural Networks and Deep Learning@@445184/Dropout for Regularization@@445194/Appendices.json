{"hands_on_practices": [{"introduction": "Applying dropout is more complex than just randomly removing neurons, especially when nonlinear activation functions are involved. Simply averaging the network's behavior at test time can introduce a systematic error, or bias, because for a nonlinear function $\\phi$, the expectation of the function is not the same as the function of the expectation ($\\mathbb{E}[\\phi(z)] \\neq \\phi(\\mathbb{E}[z])$). This practice [@problem_id:3117336] provides a concrete counterexample with a simple quadratic neuron, allowing you to quantify this bias and understand a core theoretical challenge that dropout must overcome.", "problem": "Consider a scalar regression model with a single neuron that applies a nonlinear activation to a linearly transformed input. Let the input be $x \\in \\mathbb{R}$, the weight be $w \\in \\mathbb{R}$, and the activation be the quadratic map $\\phi(z) = z^{2}$. During training, the model uses dropout: the pre-activation is multiplied by a random keep variable $r \\in \\{0,1\\}$ where $r \\sim \\mathrm{Bernoulli}(q)$ independently for each example, with keep probability $q \\in (0,1)$. Thus, the training-time prediction is\n$$\n\\hat{y}_{\\text{train}}(r) = \\phi(w\\,x\\,r) .\n$$\nAt test time, the practitioner employs mean scaling: the random keep variable is replaced by its mean, yielding the deterministic prediction\n$$\n\\hat{y}_{\\text{ms}} = \\phi(q\\,w\\,x) .\n$$\nUsing only the definitions of expectation, the Bernoulli distribution, and the given model, derive and compute the bias induced by mean scaling,\n$$\n\\text{bias} \\;=\\; \\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] \\;-\\; \\hat{y}_{\\text{ms}} ,\n$$\nfor the specific values $x=-4$, $w=1.5$, and $q=0.7$. Provide the final answer as a single real number. No rounding is required.", "solution": "The problem requires the calculation of the bias induced by using mean scaling at test time for a simple neural network model with dropout. The bias is defined as the difference between the expected prediction at training time and the deterministic prediction at test time.\n$$\n\\text{bias} \\;=\\; \\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] \\;-\\; \\hat{y}_{\\text{ms}}\n$$\nFirst, we write out the explicit forms of the two terms on the right-hand side. The training-time prediction is given by $\\hat{y}_{\\text{train}}(r) = \\phi(w\\,x\\,r)$. With the specified quadratic activation function $\\phi(z) = z^2$, this becomes:\n$$\n\\hat{y}_{\\text{train}}(r) = (w\\,x\\,r)^2 = w^2 x^2 r^2\n$$\nThe test-time prediction using mean scaling, where the random variable $r$ is replaced by its mean $\\mathbb{E}[r]=q$, is given by $\\hat{y}_{\\text{ms}} = \\phi(q\\,w\\,x)$. This evaluates to:\n$$\n\\hat{y}_{\\text{ms}} = (q\\,w\\,x)^2 = q^2 w^2 x^2\n$$\nNext, we must compute the expectation of the training-time prediction, $\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right]$. The random variable $r$ follows a Bernoulli distribution, $r \\sim \\mathrm{Bernoulli}(q)$, which means $r$ takes the value $1$ with probability $q$ and the value $0$ with probability $1-q$.\nThe expectation is:\n$$\n\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] = \\mathbb{E}_{r}\\!\\left[w^2 x^2 r^2\\right]\n$$\nSince $w$ and $x$ are constants with respect to the expectation over $r$, they can be factored out:\n$$\n\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] = w^2 x^2 \\mathbb{E}_{r}\\!\\left[r^2\\right]\n$$\nTo find $\\mathbb{E}_{r}[r^2]$, we use the definition of expectation for a discrete random variable:\n$$\n\\mathbb{E}_{r}\\!\\left[r^2\\right] = \\sum_{k \\in \\{0,1\\}} k^2 P(r=k) = (0^2) \\cdot P(r=0) + (1^2) \\cdot P(r=1)\n$$\nSubstituting the probabilities from the Bernoulli distribution:\n$$\n\\mathbb{E}_{r}\\!\\left[r^2\\right] = (0) \\cdot (1-q) + (1) \\cdot q = q\n$$\nAn important property of a random variable $r$ following a Bernoulli distribution is that for any integer power $n \\ge 1$, we have $r^n=r$, because $0^n=0$ and $1^n=1$. Consequently, $\\mathbb{E}_{r}[r^2] = \\mathbb{E}_{r}[r] = q$.\n\nSubstituting this result, $q$, back into the expression for the expected training prediction:\n$$\n\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] = w^2 x^2 q\n$$\nWe can now assemble the complete expression for the bias by subtracting the mean-scaled test-time prediction from the expected training-time prediction:\n$$\n\\text{bias} = (w^2 x^2 q) - (q^2 w^2 x^2)\n$$\nFactoring out the common terms $w^2 x^2 q$ yields the general symbolic expression for the bias in this model:\n$$\n\\text{bias} = w^2 x^2 q(1-q)\n$$\nFinally, we substitute the specific numerical values provided in the problem statement: $x=-4$, $w=1.5$, and $q=0.7$.\n$$\n\\text{bias} = (1.5)^2 (-4)^2 (0.7)(1-0.7)\n$$\n$$\n\\text{bias} = (2.25) (16) (0.7)(0.3)\n$$\nWe compute the products separately. First, the product of the squared terms:\n$$\n(2.25) \\cdot (16) = \\left(\\frac{9}{4}\\right) \\cdot (16) = 9 \\cdot 4 = 36\n$$\nNext, the product involving the keep probability:\n$$\n(0.7) \\cdot (0.3) = 0.21\n$$\nMultiplying these two results gives the final value for the bias:\n$$\n\\text{bias} = 36 \\cdot 0.21 = 7.56\n$$", "answer": "$$\n\\boxed{7.56}\n$$", "id": "3117336"}, {"introduction": "The most common implementation of dropout, known as \"inverted dropout,\" cleverly preserves the expected output magnitude between training and testing by scaling activations *during training*. This avoids the need for any modifications at test time, which simplifies inference. This exercise [@problem_id:3118056] illuminates the importance of this scaling by analyzing a hypothetical but instructive scenario where it is forgotten, demonstrating how it leads to a predictable train-test performance mismatch.", "problem": "A single-hidden-unit linear network is trained on a noise-free regression task using dropout as regularization but with an incorrect implementation of inverted dropout (no train-time scaling). The input feature $x$ is a random variable with $\\mathbb{E}[x]=0$ and $\\operatorname{Var}(x)=\\sigma_x^2$, and the true target is $y=\\beta x$. The network output is $\\hat{y}=v\\,(m\\,u\\,x)$, where $u$ and $v$ are weights and $m\\sim\\text{Bernoulli}(p)$ is an independent dropout mask applied to the hidden activation, with retention probability $p$. Normally, inverted dropout scales activations at training to keep their expectation unchanged between training and testing, but here no scaling is used during training. Assume the optimizer has achieved $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$ for every fixed $x$ under this incorrect training procedure. At test time, dropout is removed and no scaling is applied (i.e., $m=1$ deterministically).\n\nGiven $p=0.5$, $\\beta=2$, and $\\sigma_x^2=4$, which option correctly gives the test-time Mean Squared Error (MSE) and explains the source of the observed train–test mismatch?\n\nA. Test-time MSE is $16$, caused by an upward bias: the test output is amplified by a factor $1/p$ relative to training-time expectations, so $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x > \\beta x$.\n\nB. Test-time MSE is $4$, caused purely by variance due to the mask; there is no bias because the expectation matches at test.\n\nC. Test-time MSE is $0$, because matching the training expectation to the target guarantees perfect predictions at test.\n\nD. Test-time MSE is $4$, caused by a downward bias: the test output is attenuated relative to the training-time expectation.", "solution": "To determine the test-time Mean Squared Error (MSE), we must first find the effective weight the network learns during training and then use it to compute the test-time predictions.\n\n**1. Analyze the Training Phase**\nThe training procedure sets the network's parameters such that the expected output, conditioned on the input $x$, matches the true target $y$. The training-time output is $\\hat{y}_{\\text{train}} = v(m u x)$, where $m \\sim \\text{Bernoulli}(p)$.\nThe expectation is taken over the random mask $m$:\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = \\mathbb{E}_m[v m u x] = v u x \\, \\mathbb{E}[m]\n$$\nSince $\\mathbb{E}[m] = p$, this becomes:\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = p(vu)x\n$$\nThe training condition requires this to be equal to the true target, $y = \\beta x$.\n$$\np(vu)x = \\beta x\n$$\nThis implies that the learned effective weight of the network, $W = vu$, must be $W = \\frac{\\beta}{p}$. The network's weights are scaled up to compensate for the fact that the unit is active only a fraction $p$ of the time.\n\n**2. Analyze the Test Phase**\nAt test time, dropout is disabled ($m=1$) and no scaling is applied. The test-time prediction is:\n$$\n\\hat{y}_{\\text{test}} = v(1 \\cdot u x) = (vu)x = Wx = \\left(\\frac{\\beta}{p}\\right) x\n$$\nCompared to the true target $y=\\beta x$, the test-time output is amplified by a factor of $1/p$. This is a systematic upward bias because the weights, which were inflated during training, are now being used on the full, un-dropped network.\n\n**3. Calculate Test-Time MSE**\nThe MSE is the expected squared difference between the test-time prediction and the true target, over the distribution of $x$:\n$$\n\\text{MSE} = \\mathbb{E}_x[(\\hat{y}_{\\text{test}} - y)^2] = \\mathbb{E}_x\\left[ \\left( \\frac{\\beta}{p}x - \\beta x \\right)^2 \\right]\n$$\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\beta \\left(\\frac{1}{p} - 1\\right) x \\right)^2 \\right] = \\beta^2 \\left(\\frac{1-p}{p}\\right)^2 \\mathbb{E}_x[x^2]\n$$\nWe are given $\\mathbb{E}[x]=0$ and $\\operatorname{Var}(x)=\\sigma_x^2$, so $\\mathbb{E}[x^2] = \\operatorname{Var}(x) + (\\mathbb{E}[x])^2 = \\sigma_x^2$.\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1-p}{p}\\right)^2 \\sigma_x^2\n$$\nSubstituting the given values $p=0.5$, $\\beta=2$, and $\\sigma_x^2=4$:\n$$\n\\text{MSE} = (2)^2 \\left(\\frac{1-0.5}{0.5}\\right)^2 (4) = 4 \\left(\\frac{0.5}{0.5}\\right)^2 (4) = 4 \\cdot (1)^2 \\cdot 4 = 16\n$$\nThe test-time MSE is 16. This error arises from the upward bias where the test output is amplified by a factor of $1/p = 1/0.5 = 2$ compared to the true target. This matches option A.", "answer": "$$\\boxed{A}$$", "id": "3118056"}, {"introduction": "In modern neural networks, layers are rarely used in isolation; a crucial skill is understanding how different components, like dropout and Batch Normalization (BN), interact. The relative ordering of these two operations is a critical design choice that can have significant consequences for model training and inference. This practice [@problem_id:3118023] explores this delicate interplay, revealing how their order can create or prevent unexpected statistical shifts between the training and inference phases, ultimately impacting model stability and performance.", "problem": "You are given two otherwise identical deep neural network blocks that differ only in the placement of a dropout layer relative to a Batch Normalization (BN) layer. Batch Normalization (BN) denotes the standard transformation that, during training, normalizes each channel using batch mean and batch variance and, during inference, normalizes using running (exponentially averaged) mean and variance accumulated during training, followed by a learned affine transformation. Dropout uses the common “inverted” implementation during training: each unit is independently kept with probability $p$ and scaled by $1/p$, and during inference dropout is disabled. Consider a generic pre-activation $x$ entering the BN layer in each block, and assume that the dropout mask is independent of $x$. You will measure, for each block and for each relevant tensor location, the empirical layerwise statistics $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ on held-out data under training mode and under inference mode, to diagnose distribution shift. The two blocks are:\n- Block A (“dropout before BN”): previous activations $\\to$ dropout with keep probability $p$ and inverted scaling $\\to$ BN $\\to$ remaining layers.\n- Block B (“dropout after BN”): previous activations $\\to$ BN $\\to$ dropout with keep probability $p$ and inverted scaling $\\to$ remaining layers.\nAssume the upstream inputs to these blocks have finite first and second moments and that training uses sufficiently large batches so that BN’s running statistics converge to the corresponding training-time batch statistics. Under these assumptions, which statement best predicts the pattern you will observe when comparing $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ measured at the BN input and BN output across training versus inference for Block A versus Block B?\n\nA. In Block A, the BN running variance is inflated by the masking noise seen during training, so at inference the BN input has a smaller variance than the running variance and the BN output variance contracts to approximately $p$ (less than $1$) while the mean remains approximately $0$. In Block B, BN statistics closely match between training and inference at both BN input and BN output, so their means and variances align; any train–inference variance discrepancy appears only after the dropout layer, where training variance is larger by roughly $1/p$.\n\nB. In Block A, BN eliminates any train–inference mismatch, so both the BN input and BN output have exactly matching means and variances between training and inference. In Block B, placing dropout after BN induces a mean shift at the BN output proportional to $(1-p)$ at inference.\n\nC. In Block A, masking before BN biases the BN running mean toward $0$, so at inference the BN output has a nonzero mean mismatch, while its variance matches. In Block B, masking after BN biases the BN running variance upward, so at inference the BN output variance inflates by a factor of $1/p$.\n\nD. Placement does not matter with inverted dropout: for any $p$, the expectation and variance at every layer match between training and inference, so all measured $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ coincide at BN inputs and outputs in both blocks.", "solution": "To predict the observed pattern, we must analyze the statistical properties (mean and variance) of the data at the input and output of the Batch Normalization (BN) layer during both training and inference for each block. Let the input to the block be $y$, with mean $\\mu_y$ and variance $\\sigma_y^2$. Assume the BN layer's learnable parameters are $\\gamma$ and $\\beta$.\n\n**Analysis of Block A (Dropout $\\to$ BN)**\n\n1.  **Training Mode:**\n    -   The input to the BN layer is $x_{\\text{train}} = y \\cdot (m/p)$, where $m \\sim \\text{Bernoulli}(p)$ is the dropout mask.\n    -   The mean of this input is $\\operatorname{E}[x_{\\text{train}}] = \\operatorname{E}[y] \\cdot \\operatorname{E}[m/p] = \\mu_y \\cdot (p/p) = \\mu_y$.\n    -   The variance of this input is $\\operatorname{Var}[x_{\\text{train}}] = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$. This is larger than $\\sigma_y^2$ because of the noise from the random mask $m$.\n    -   The BN layer computes and stores these statistics as its running mean $\\mu_{\\text{run}} = \\mu_y$ and running variance $\\sigma_{\\text{run}}^2 = \\operatorname{Var}[x_{\\text{train}}]$. The running variance is thus an *inflated* estimate of the true data variance $\\sigma_y^2$.\n    -   The output of the BN layer, $z_{\\text{train}}$, is normalized to have mean $\\approx \\beta$ and variance $\\approx \\gamma^2$.\n\n2.  **Inference Mode:**\n    -   Dropout is disabled. The input to the BN layer is simply $x_{\\text{inf}} = y$.\n    -   The BN layer normalizes this input using the stored *running* statistics: $z_{\\text{inf}} = \\gamma \\frac{x_{\\text{inf}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta$.\n    -   The mean of the output is $\\operatorname{E}[z_{\\text{inf}}] \\approx \\beta$, which matches the training-time mean.\n    -   The variance of the output is $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\gamma^2 \\frac{\\operatorname{Var}[y]}{\\sigma_{\\text{run}}^2} = \\gamma^2 \\frac{\\sigma_y^2}{\\operatorname{Var}[x_{\\text{train}}]}$.\n    -   Since $\\operatorname{Var}[x_{\\text{train}}] > \\sigma_y^2$ (it's inflated by dropout noise), the ratio $\\frac{\\sigma_y^2}{\\operatorname{Var}[x_{\\text{train}}]}$ is less than 1. Specifically, if $\\mu_y=0$, then $\\operatorname{Var}[x_{\\text{train}}] = \\sigma_y^2/p$, and the output variance becomes $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\gamma^2 p$.\n    -   **Conclusion for Block A:** There is a train-inference mismatch. The BN layer uses an inflated running variance, causing the output variance at inference to be *contracted* (reduced) by a factor of approximately $p$.\n\n**Analysis of Block B (BN $\\to$ Dropout)**\n\n1.  **Training Mode:**\n    -   The input to the BN layer is $y$. It stores $\\mu_{\\text{run}} = \\mu_y$ and $\\sigma_{\\text{run}}^2 = \\sigma_y^2$.\n    -   The output of the BN layer, $x_{\\text{train}}$, has mean $\\approx \\beta$ and variance $\\approx \\gamma^2$.\n    -   This output is then passed through dropout. The final output of the block has its variance inflated by dropout.\n\n2.  **Inference Mode:**\n    -   The input to the BN layer is still $y$. Since the running statistics ($\\mu_y, \\sigma_y^2$) correctly reflect the statistics of $y$, the BN layer's normalization is consistent.\n    -   The output of the BN layer, $x_{\\text{inf}}$, will have mean $\\approx \\beta$ and variance $\\approx \\gamma^2$.\n    -   Dropout is disabled (identity function). The final block output is just $x_{\\text{inf}}$.\n    -   **Conclusion for Block B:** The statistics at both the input and output of the BN layer are consistent between training and inference. The only train-inference discrepancy occurs *after* the BN and dropout layers, as the training output is noisy while the inference output is not.\n\nThis detailed analysis matches the description in option A.", "answer": "$$\\boxed{A}$$", "id": "3118023"}]}