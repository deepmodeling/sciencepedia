{"hands_on_practices": [{"introduction": "At its heart, backpropagation is a specific application of a more general algorithm known as reverse-mode Automatic Differentiation (AD). This first exercise demystifies the \"magic\" of backpropagation by guiding you to implement it from scratch for a simple expression language [@problem_id:3100018]. By creating a \"tape\" to record a sequence of elementary operations and then \"replaying\" it in reverse, you will see exactly how the chain rule is applied mechanically to efficiently compute the gradient of an arbitrarily complex function.", "problem": "You are tasked with implementing reverse-mode Automatic Differentiation (AD) for a small scalar expression language and demonstrating how to compute adjoint variables, denoted by $\\bar{x} = \\partial L / \\partial x$, through a reverse replay of a tape of operations. Reverse-mode AD is synonymous with the backpropagation algorithm in computational graphs. Your implementation must start from first principles, specifically the chain rule of differentiation for composed functions, and the definition of a computational graph as a directed acyclic graph of primitive operations with a scalar loss $L$ at the root. You must design a tape structure that records the forward execution of primitive operations and then replay this tape in reverse to accumulate adjoints using the chain rule.\n\nYour small expression language must support scalar variables and constants, and the following primitive operations: binary addition $+$, binary subtraction $-$, binary multiplication $\\cdot$, binary division $\\div$, unary sine $\\sin(\\cdot)$, unary exponential $\\exp(\\cdot)$, and unary natural logarithm $\\log(\\cdot)$. All angles in trigonometric functions must be in radians. The domain constraints must be respected, for example the input to $\\log(\\cdot)$ must be strictly positive. You must design the computational tape to record each non-leaf operation with its operands and forward value to ensure a correct reverse replay.\n\nYour program must:\n- Build an internal computational graph and tape when evaluating a scalar loss $L$.\n- Compute the adjoint for each input variable $x_i$, that is $\\partial L / \\partial x_i$, via a single reverse replay of the tape starting from $\\bar{L} = \\partial L / \\partial L = 1$.\n- Produce, for each test case, a list whose first element is the scalar loss value $L$ and whose subsequent elements are the adjoints for the variables in the order they were introduced.\n\nStart from fundamental principles only: the chain rule for composed functions, the definition of adjoints $\\bar{v} = \\partial L / \\partial v$ for intermediate values $v$, and the semantics of a computational graph. Do not rely on pre-packaged differentiation formulas that skip the derivation path; instead derive and implement the local partial derivatives needed for the reverse replay using basic calculus for each primitive operation.\n\nImplement and run the following test suite. In each case, define the variables in the specified order, construct the expression using the primitive operations, and compute the outputs. All angles are in radians, and there are no physical units involved in this problem.\n\n- Test case $1$ (general composition): variables $x, y$, loss $L = \\sin(x \\cdot y) + \\exp(y)$, with $x = 0.5$, $y = -1.0$. Output format for this case: $[L, \\partial L / \\partial x, \\partial L / \\partial y]$.\n- Test case $2$ (boundary with zero and constants): variable $x$, loss $L = x \\cdot 0 + \\sin(0) + \\log(1)$, with $x = 2.0$. Output format: $[L, \\partial L / \\partial x]$.\n- Test case $3$ (repeated variable usage): variable $x$, loss $L = (x \\cdot x) \\cdot x$, with $x = 2.0$. Output format: $[L, \\partial L / \\partial x]$.\n- Test case $4$ (division and logarithm): variables $x, y$, loss $L = x \\div y + \\log(y)$, with $x = 1.0$, $y = 1.5$. Output format: $[L, \\partial L / \\partial x, \\partial L / \\partial y]$.\n- Test case $5$ (nested unary composition): variable $x$, loss $L = \\exp(\\sin(x))$, with $x = 0.0$. Output format: $[L, \\partial L / \\partial x]$.\n\nYour program should produce a single line of output containing the results of all test cases as a comma-separated list enclosed in square brackets, with each test case result itself being a comma-separated list enclosed in square brackets. For example, an output for two test cases would look like $[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$. Your final output must follow this format exactly, using standard floating-point numbers.", "solution": "The problem requires the implementation of reverse-mode Automatic Differentiation (AD), colloquially known as backpropagation, from first principles. This method computes the gradient of a scalar loss function $L$ with respect to a set of input variables $x_i$ by first performing a forward evaluation of the expression for $L$ to compute intermediate values and record a computational graph, followed by a reverse traversal of this graph to propagate gradients based on the chain rule.\n\n**Fundamental Principles: The Chain Rule and Adjoints**\n\nThe foundation of reverse-mode AD is the chain rule of calculus. If a scalar loss $L$ is a function of an intermediate variable $v_j$, which itself is a function of other variables $v_i$, the gradient of $L$ with respect to $v_i$ is given by the sum of contributions through all paths from $v_i$ to $L$. For a single path $L \\to v_j \\to v_i$, the chain rule states:\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\nIn the language of AD, we define the \"adjoint\" of a variable $v$ as $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$. Using this notation, the chain rule becomes:\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\nThe reverse-mode AD algorithm leverages this relationship by first computing the value of $L$ and then propagating the adjoints backward from $L$ to the input variables. The process starts by seeding the adjoint of the loss function itself, $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$.\n\n**The Computational Graph and Tape**\n\nAny scalar expression can be decomposed into a sequence of primitive operations (e.g., addition, multiplication, sine). This decomposition naturally forms a Directed Acyclic Graph (DAG), where nodes represent numerical values (input variables, constants, and intermediate results) and edges represent the primitive operations.\n\nThe forward evaluation of the expression, from inputs to the final loss $L$, is used to construct this graph. For our implementation, we use a \"tape\" data structure, which is a linearized representation of the graph. The tape is an ordered list of operations recorded during the forward pass. Each entry on the tape stores the type of operation, references to its input nodes, and a reference to its output node. This recording ensures we have the complete structure and all necessary intermediate values for the reverse pass.\n\n**The Forward Pass: Evaluation and Tape Recording**\n\nThe forward pass proceeds as follows:\n$1$. Input variables and constants are initialized as the starting nodes in our graph.\n$2$. The expression is evaluated sequentially. Each time a primitive operation is applied, two things happen:\n    a. The numerical result of the operation is computed and stored as a new node in the graph.\n    b. An entry is added to the tape, recording the operation type, its input node(s), and the newly created output node.\n\nFor example, for the expression $z = x \\cdot y$, we would compute the value of $z$ using the current values of $x$ and $y$, create a new node for $z$, and record `('mul', [x_node, y_node], z_node)` on the tape.\n\n**The Reverse Pass: Adjoint Accumulation**\n\nOnce the forward pass is complete and the final loss value $L$ is computed, the reverse pass begins. It traverses the tape in the reverse order of its creation.\n$1$. An array of adjoints, corresponding to each node in the graph, is initialized to zero.\n$2$. The adjoint of the final loss node is set to $1$, i.e., $\\bar{L} = 1$.\n$3$. For each operation $z = f(x_1, \\dots, x_n)$ on the tape (processed in reverse order):\n    a. We retrieve the already computed adjoint of the output, $\\bar{z}$.\n    b. We use the chain rule to calculate the contribution of $\\bar{z}$ to the adjoints of the inputs. The adjoint of each input $x_i$ is updated by accumulating this contribution:\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    The use of accumulation ($\\mathrel{+}=$) is crucial because a single variable might be used in multiple operations (i.e., it can be a parent to multiple children in the graph). Its total adjoint is the sum of the gradient signals flowing back from all its children. Reversing the tape guarantees that a node's adjoint ($\\bar{z}$) is fully calculated before it is propagated to its own inputs ($x_i$).\n\n**Adjoint Update Rules for Primitive Operations**\n\nThe local partial derivatives $\\frac{\\partial z}{\\partial x_i}$ are known for each primitive operation. The values of the inputs required for these derivatives (e.g., for $z = x \\cdot y$, $\\frac{\\partial z}{\\partial x} = y$) are available from the forward pass.\n\n- **Addition:** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= \\bar{z}$.\n\n- **Subtraction:** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= -\\bar{z}$.\n\n- **Multiplication:** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$.\n\n- **Division:** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$.\n\n- **Sine:** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$.\n\n- **Exponential:** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$.\n\n- **Natural Logarithm:** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$.\n\n**Implementation Design**\n\nThe implementation uses two main classes: `Graph` and `Node`. The `Graph` class manages the state of the computation: it stores the `values` of all nodes, the operation `tape`, and the computed `adjoints`. The `Node` class acts as a wrapper around a node's ID, providing an intuitive interface by overloading Python's arithmetic operators (`+`, `*`, etc.). When an operation like `c = a + b` is performed on `Node` objects, it transparently calls a method on the associated `Graph` object, which performs the forward calculation, records the operation on the tape, and returns a new `Node` for the result `c`. This object-oriented design allows for the construction of expressions in a natural way while correctly building the computational graph in the background. After the final loss `Node` is computed, a call to `Graph.compute_gradients()` executes the reverse pass as described above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "With a grasp of the general mechanism, we now apply its core logic—the multivariable chain rule—to a cornerstone of modern classification models: the softmax activation function combined with the cross-entropy loss [@problem_id:3101047]. This essential derivation reveals a remarkable simplification where the final gradient, $\\nabla_{\\mathbf{z}} L$, is simply the difference between the predicted probability vector $\\mathbf{p}$ and the one-hot target vector $\\mathbf{y}$. Mastering this analysis is key to understanding the training dynamics of deep learning classifiers.", "problem": "A multiclass classifier produces a vector of logits $\\mathbf{z} \\in \\mathbb{R}^{K}$, where the predicted class probabilities are given by the softmax function $p_{k}(\\mathbf{z}) = \\frac{\\exp(z_{k})}{\\sum_{j=1}^{K} \\exp(z_{j})}$ for $k \\in \\{1,\\dots,K\\}$. For a target label vector $\\mathbf{y} \\in \\mathbb{R}^{K}$ that represents a valid class distribution (for example, a one-hot vector) with $\\sum_{k=1}^{K} y_{k} = 1$ and $y_{k} \\ge 0$, the cross-entropy loss is defined by $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}(\\mathbf{z})$. Using only these definitions and standard calculus (such as the chain rule), derive a simplified closed-form expression for the gradient $\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\mathbf{y})$ in terms of $\\mathbf{z}$ and $\\mathbf{y}$. Then, rewrite the loss using the identity $\\ln\\!\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$ in a numerically stable way based on the log-sum-exp (LSE) technique, and explain why this stabilized form does not alter the gradient with respect to $\\mathbf{z}$. Finally, evaluate the gradient for the specific case $K = 3$, logits $\\mathbf{z} = (2,-1,0.5)$, and a one-hot target $\\mathbf{y} = (1,0,0)$. Round your final gradient vector to $4$ significant figures.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   Logits vector: $\\mathbf{z} \\in \\mathbb{R}^{K}$\n-   Predicted class probabilities (softmax function): $p_{k}(\\mathbf{z}) = \\frac{\\exp(z_{k})}{\\sum_{j=1}^{K} \\exp(z_{j})}$ for $k \\in \\{1,\\dots,K\\}$\n-   Target label vector: $\\mathbf{y} \\in \\mathbb{R}^{K}$\n-   Properties of the target vector: $\\sum_{k=1}^{K} y_{k} = 1$ and $y_{k} \\ge 0$\n-   Cross-entropy loss function: $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}(\\mathbf{z})$\n-   Identity for numerical stability: $\\ln\\!\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$ and the log-sum-exp (LSE) technique.\n-   Specific case for evaluation: $K = 3$, $\\mathbf{z} = (2,-1,0.5)$, $\\mathbf{y} = (1,0,0)$.\n-   Rounding requirement for the final gradient vector: $4$ significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, using standard and correct definitions of the softmax function and cross-entropy loss from the field of statistical learning. It is well-posed, providing all necessary information to derive a unique analytical expression for the gradient and to compute a specific numerical instance. The language is objective and unambiguous. The problem is self-contained and internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n---\nThe solution proceeds in three parts as requested: derivation of the gradient, analysis of the numerically stable form of the loss, and evaluation for a specific case.\n\n**Part 1: Derivation of the Gradient of the Cross-Entropy Loss**\n\nThe cross-entropy loss function is given by:\n$$L(\\mathbf{z};\\mathbf{y}) = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}(\\mathbf{z})$$\nWe seek to compute the gradient $\\nabla_{\\mathbf{z}} L$, whose components are the partial derivatives $\\frac{\\partial L}{\\partial z_i}$ for $i \\in \\{1, \\dots, K\\}$.\n\nUsing the chain rule, the partial derivative of $L$ with respect to an arbitrary logit $z_i$ is:\n$$\\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} y_k \\frac{\\partial}{\\partial z_i} (\\ln p_k) = -\\sum_{k=1}^{K} \\frac{y_k}{p_k} \\frac{\\partial p_k}{\\partial z_i}$$\n\nNext, we must find the partial derivative of the softmax function $p_k$ with respect to $z_i$. The softmax function is $p_k(\\mathbf{z}) = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$. Let $N_k = \\exp(z_k)$ and $D = \\sum_{j=1}^{K} \\exp(z_j)$, so $p_k = N_k/D$.\n\nWe consider two cases for the derivative $\\frac{\\partial p_k}{\\partial z_i}$ using the quotient rule.\n\nCase 1: $i = k$.\n$$\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\frac{\\partial N_k}{\\partial z_k} D - N_k \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{\\exp(z_k) \\left(\\sum_j \\exp(z_j)\\right) - \\exp(z_k) \\exp(z_k)}{\\left(\\sum_j \\exp(z_j)\\right)^2}$$\n$$\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)} - \\left(\\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}\\right)^2 = p_k - p_k^2 = p_k(1-p_k)$$\n\nCase 2: $i \\neq k$.\n$$\\frac{\\partial p_k}{\\partial z_i} = \\frac{\\frac{\\partial N_k}{\\partial z_i} D - N_k \\frac{\\partial D}{\\partial z_i}}{D^2} = \\frac{0 \\cdot D - \\exp(z_k) \\exp(z_i)}{\\left(\\sum_j \\exp(z_j)\\right)^2}$$\n$$\\frac{\\partial p_k}{\\partial z_i} = -\\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)} \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} = -p_k p_i$$\n\nThese two cases can be compactly written using the Kronecker delta, $\\delta_{ik}$, which is $1$ if $i=k$ and $0$ otherwise:\n$$\\frac{\\partial p_k}{\\partial z_i} = p_k \\delta_{ik} - p_k p_i = p_k(\\delta_{ik} - p_i)$$\nThis expression represents the Jacobian matrix of the softmax function.\n\nSubstituting this back into the derivative of the loss function:\n$$\\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} \\frac{y_k}{p_k} \\left( p_k(\\delta_{ik} - p_i) \\right) = -\\sum_{k=1}^{K} y_k (\\delta_{ik} - p_i)$$\nDistributing the summation:\n$$\\frac{\\partial L}{\\partial z_i} = -\\left( \\sum_{k=1}^{K} y_k \\delta_{ik} - \\sum_{k=1}^{K} y_k p_i \\right)$$\nThe first term simplifies because $\\delta_{ik}$ is non-zero only when $k=i$: $\\sum_{k=1}^{K} y_k \\delta_{ik} = y_i$.\nFor the second term, $p_i$ is constant with respect to the summation index $k$, so it can be factored out: $\\sum_{k=1}^{K} y_k p_i = p_i \\sum_{k=1}^{K} y_k$.\nGiven the constraint $\\sum_{k=1}^{K} y_k = 1$, the second term becomes $p_i \\cdot 1 = p_i$.\n\nSubstituting these results back:\n$$\\frac{\\partial L}{\\partial z_i} = -(y_i - p_i) = p_i - y_i$$\nThis elegantly simple result holds for all $i \\in \\{1, \\dots, K\\}$. Therefore, the gradient vector is the difference between the predicted probability vector and the target vector:\n$$\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\mathbf{y}) = \\mathbf{p}(\\mathbf{z}) - \\mathbf{y}$$\n\n**Part 2: Numerically Stable Form of the Loss and its Gradient**\n\nThe loss function $L = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}$ can be rewritten by substituting the definition of $p_k$:\n$$L = -\\sum_{k=1}^{K} y_{k} \\ln \\left( \\frac{\\exp(z_{k})}{\\sum_{j=1}^{K} \\exp(z_{j})} \\right) = -\\sum_{k=1}^{K} y_{k} \\left( \\ln(\\exp(z_k)) - \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right) \\right)$$\n$$L = -\\sum_{k=1}^{K} y_{k} \\left( z_k - \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right) \\right)$$\n$$L = -\\sum_{k=1}^{K} y_{k} z_k + \\left(\\sum_{k=1}^{K} y_k\\right) \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$$\nUsing $\\sum y_k = 1$, we get:\n$$L = -\\sum_{k=1}^{K} y_{k} z_k + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$$\nThe term $\\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$ is known as the log-sum-exp (LSE) function, $\\text{LSE}(\\mathbf{z})$.\n\nFor large values of $z_j$, $\\exp(z_j)$ can overflow standard floating-point representations. The LSE technique provides a numerically stable method for its computation. Let $z_{\\max} = \\max_j z_j$. We can rewrite the LSE term as:\n$$\\text{LSE}(\\mathbf{z}) = \\ln\\left(\\sum_{j=1}^{K} \\exp(z_j)\\right) = \\ln\\left(\\exp(z_{\\max}) \\sum_{j=1}^{K} \\exp(z_j-z_{\\max})\\right)$$\n$$= \\ln(\\exp(z_{\\max})) + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_j-z_{\\max})\\right) = z_{\\max} + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_j-z_{\\max})\\right)$$\nThis transformation is an algebraic identity. The function expressed in the \"stabilized\" form is mathematically identical to the original function. Since the function itself is unchanged, its gradient with respect to $\\mathbf{z}$ is also unchanged. The LSE trick is purely a computational device to prevent numerical overflow and underflow; it does not alter the mathematical properties of the function.\n\nTo verify this, we can differentiate the LSE form of the loss function:\n$$\\frac{\\partial L}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left( -\\sum_{k=1}^{K} y_{k} z_k + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right) \\right)$$\n$$\\frac{\\partial L}{\\partial z_i} = -y_i + \\frac{1}{\\sum_{j=1}^{K} \\exp(z_{j})} \\cdot \\frac{\\partial}{\\partial z_i} \\left(\\sum_{j=1}^{K} \\exp(z_j)\\right)$$\n$$\\frac{\\partial L}{\\partial z_i} = -y_i + \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_{j})} = -y_i + p_i$$\nThis confirms that the gradient is indeed $p_i - y_i$.\n\n**Part 3: Evaluation for a Specific Case**\n\nWe are given $K=3$, logits $\\mathbf{z} = (2, -1, 0.5)$, and a one-hot target $\\mathbf{y} = (1, 0, 0)$.\nThe gradient is $\\nabla_{\\mathbf{z}}L = \\mathbf{p} - \\mathbf{y}$. First, we compute the probability vector $\\mathbf{p}$.\n\nThe denominator of the softmax function is:\n$$D = \\sum_{j=1}^{3} \\exp(z_j) = \\exp(2) + \\exp(-1) + \\exp(0.5)$$\n$$D \\approx 7.389056 + 0.367879 + 1.648721 \\approx 9.405656$$\n\nThe probabilities are:\n$$p_1 = \\frac{\\exp(2)}{D} \\approx \\frac{7.389056}{9.405656} \\approx 0.785601$$\n$$p_2 = \\frac{\\exp(-1)}{D} \\approx \\frac{0.367879}{9.405656} \\approx 0.039112$$\n$$p_3 = \\frac{\\exp(0.5)}{D} \\approx \\frac{1.648721}{9.405656} \\approx 0.175287$$\n\nNow, we compute the components of the gradient $\\mathbf{g} = \\mathbf{p} - \\mathbf{y}$:\n$$g_1 = p_1 - y_1 \\approx 0.785601 - 1 = -0.214399$$\n$$g_2 = p_2 - y_2 \\approx 0.039112 - 0 = 0.039112$$\n$$g_3 = p_3 - y_3 \\approx 0.175287 - 0 = 0.175287$$\n\nRounding these values to $4$ significant figures:\n$$g_1 \\approx -0.2144$$\n$$g_2 \\approx 0.03911$$\n$$g_3 \\approx 0.1753$$\n\nThe gradient vector is approximately $(-0.2144, 0.03911, 0.1753)$.", "answer": "$$\\boxed{\\begin{pmatrix} -0.2144  0.03911  0.1753 \\end{pmatrix}}$$", "id": "3101047"}, {"introduction": "Implementing backpropagation for even a small neural network can be complex and error-prone. How can we be sure our code is correct? This final practice introduces \"gradient checking,\" a powerful debugging technique that compares the analytical gradient from backpropagation to a numerical approximation using finite differences [@problem_id:3100954]. You will build a two-layer network, implement its backpropagation algorithm, and then write a gradient checker to validate your work—an indispensable skill for any deep learning practitioner.", "problem": "Construct a verification of the backpropagation algorithm by comparing an analytically derived gradient of a two-layer neural network to a finite-difference approximation. The objective is to numerically confirm that the discrepancy between the two gradients behaves as a first-order truncation error when using a forward finite difference, that is, the error is $\\mathcal{O}(\\epsilon)$.\n\nUse the following purely mathematical setup.\n\n- Network architecture and data:\n  - Inputs have dimension $d = 3$, the hidden layer has $h = 3$ units with hyperbolic tangent activation, and the output layer has dimension $o = 1$ with a linear output.\n  - Given a mini-batch of size $n = 4$, the input matrix $X \\in \\mathbb{R}^{4 \\times 3}$ and target vector $y \\in \\mathbb{R}^{4 \\times 1}$ are:\n    $$\n    X =\n    \\begin{bmatrix}\n    0.2  -0.1  0.4 \\\\\n    -0.5  0.3  0.1 \\\\\n    0.0  -0.2  0.2 \\\\\n    0.1  0.4  -0.3\n    \\end{bmatrix}, \\quad\n    y =\n    \\begin{bmatrix}\n    0.5 \\\\ -0.1 \\\\ 0.2 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - Parameters for Test Case A are fixed as:\n    $$\n    W_1 =\n    \\begin{bmatrix}\n    0.3  -0.1  0.2 \\\\\n    -0.4  0.5  0.1 \\\\\n    0.2  0.3  -0.2\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    0.05 \\\\ -0.02 \\\\ 0.01\n    \\end{bmatrix}, \\quad\n    W_2 =\n    \\begin{bmatrix}\n    0.6  -0.7  0.2\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0.03\n    \\end{bmatrix}.\n    $$\n    For Test Case B, use the same shapes but scale every entry of $W_1, b_1, W_2, b_2$ by a factor of $0.1$.\n\n- Forward model and loss:\n  - For each row $x_i^\\top$ of $X$, define the hidden pre-activation $z_{1,i}^\\top = x_i^\\top W_1^\\top + b_1^\\top$, hidden activation $h_i^\\top = \\tanh(z_{1,i}^\\top)$, output pre-activation $z_{2,i} = h_i^\\top W_2^\\top + b_2$, and prediction $\\hat{y}_i = z_{2,i}$.\n  - Define the mean squared error loss\n    $$\n    L(W_1,b_1,W_2,b_2) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2.\n    $$\n\n- Analytical gradient via backpropagation:\n  - Using multivariable calculus, the chain rule, and the derivative identity $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$, derive the gradient of $L$ with respect to all parameters and implement it.\n  - Flatten the parameter set into a single vector $\\theta \\in \\mathbb{R}^{p}$ with $p = 16$ using the following order and memory layout:\n    1. Flatten $W_1 \\in \\mathbb{R}^{3 \\times 3}$ in row-major order.\n    2. Append $b_1 \\in \\mathbb{R}^{3}$.\n    3. Flatten $W_2 \\in \\mathbb{R}^{1 \\times 3}$ in row-major order.\n    4. Append $b_2 \\in \\mathbb{R}^{1}$.\n\n- Finite-difference approximation:\n  - For a given $\\epsilon  0$ and the standard basis vector $e_k$ in $\\mathbb{R}^p$, approximate the $k$-th component of $\\nabla_{\\theta} L$ by the forward difference\n    $$\n    \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}.\n    $$\n  - Use the list of step sizes\n    $$\n    \\mathcal{E} = \\left[ 10^{-1}, \\; 3 \\cdot 10^{-2}, \\; 10^{-2}, \\; 3 \\cdot 10^{-3}, \\; 10^{-3}, \\; 3 \\cdot 10^{-4}, \\; 10^{-4} \\right].\n    $$\n\n- Error metric and order verification:\n  - For each $\\epsilon \\in \\mathcal{E}$, compute the Euclidean norm of the difference between the analytical gradient and the finite-difference gradient,\n    $$\n    \\mathrm{err}(\\epsilon) = \\left\\| \\nabla_{\\theta} L - g_{\\mathrm{FD}}(\\epsilon) \\right\\|_2.\n    $$\n  - For consecutive $\\epsilon_i  \\epsilon_{i+1}$, compute the empirical order\n    $$\n    s_i = \\frac{\\log\\left( \\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}) \\right)}{\\log\\left( \\epsilon_i / \\epsilon_{i+1} \\right)}.\n    $$\n  - Define two boolean checks per test case:\n    1. Let $s_{\\mathrm{med}}$ be the median of $\\{ s_i \\}$. Define $\\mathrm{pass\\_order}$ to be true if $0.8 \\le s_{\\mathrm{med}} \\le 1.2$.\n    2. Define $\\mathrm{pass\\_mono}$ to be true if $\\mathrm{err}(\\epsilon)$ is strictly decreasing over the first $5$ values of $\\mathcal{E}$, that is, for $\\epsilon \\in \\{ 10^{-1}, 3\\cdot 10^{-2}, 10^{-2}, 3\\cdot 10^{-3}, 10^{-3} \\}$.\n\n- Test suite:\n  - Two test cases are specified by the parameter sets:\n    - Test Case A: the parameters exactly as given above.\n    - Test Case B: the same parameter shapes with every entry scaled by $0.1$ relative to Test Case A.\n  - In both cases, use the same $X$, $y$, and the same $\\mathcal{E}$.\n\n- Required program behavior and final output format:\n  - Your program must implement the forward model, derive and compute the analytical gradient via backpropagation from first principles, compute the finite-difference gradients for each $\\epsilon \\in \\mathcal{E}$, and evaluate the error norms and empirical orders.\n  - For each test case, produce a list with four entries: \n    $[ \\mathrm{err}(\\min \\mathcal{E}), \\; s_{\\mathrm{med}}, \\; \\mathrm{pass\\_order}, \\; \\mathrm{pass\\_mono} ]$.\n  - The final program output must be a single line containing a list with the two per-case lists, formatted exactly as a comma-separated list enclosed in square brackets, for example:\n    $$\n    \\left[ [a_1, a_2, a_3, a_4], [b_1, b_2, b_3, b_4] \\right]\n    $$\n    where each $a_j$ and $b_j$ is a boolean or a floating-point number. No other text must be printed.\n  - There are no physical units involved in this problem.\n\nYour implementation must be self-contained and must not read input. It must use the specified numerical values exactly as provided above.", "solution": "The objective is to numerically verify the correctness of the backpropagation algorithm for a two-layer neural network. This is achieved by comparing the analytically computed gradient with a numerical approximation obtained via the finite-difference method. The primary verification criterion is to confirm that the error between the analytical and numerical gradients decreases linearly with the finite-difference step size $\\epsilon$, characteristic of a forward-difference scheme's first-order truncation error, $\\mathcal{O}(\\epsilon)$.\n\n### Mathematical Model and Loss Function\n\nThe neural network architecture consists of an input layer, one hidden layer, and an output layer.\n- Input $X \\in \\mathbb{R}^{n \\times d}$ ($n=4, d=3$)\n- Hidden layer with $h=3$ units and $\\tanh$ activation.\n- Output layer with $o=1$ unit and linear activation.\n- Parameters: $W_1 \\in \\mathbb{R}^{h \\times d}$, $b_1 \\in \\mathbb{R}^{h \\times 1}$, $W_2 \\in \\mathbb{R}^{o \\times h}$, $b_2 \\in \\mathbb{R}^{o \\times 1}$.\n\nThe forward propagation of a mini-batch $X$ is defined by the following matrix operations:\n1.  **Hidden Layer Pre-activation**: The linear transformation for the hidden layer is given by $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$ is a vector of ones and its product with $b_1^\\top$ is handled via broadcasting. The resulting matrix $Z_1 \\in \\mathbb{R}^{n \\times h}$.\n2.  **Hidden Layer Activation**: The hyperbolic tangent activation function is applied element-wise: $H = \\tanh(Z_1)$, where $H \\in \\mathbb{R}^{n \\times h}$.\n3.  **Output Layer Pre-activation**: A second linear transformation produces the output pre-activations: $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$. The resulting matrix $Z_2 \\in \\mathbb{R}^{n \\times o}$.\n4.  **Prediction**: The network output is linear, so the prediction $\\hat{Y}$ is equal to the pre-activation: $\\hat{Y} = Z_2 \\in \\mathbb{R}^{n \\times o}$.\n\nThe performance of the network is quantified by the mean squared error (MSE) loss function, averaged over the mini-batch:\n$$\nL = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\|\\hat{Y} - Y\\|_F^2\n$$\nwhere $Y \\in \\mathbb{R}^{n \\times o}$ is the matrix of true target values.\n\n### Analytical Gradient via Backpropagation\n\nThe core of this task is to derive the gradient of the loss $L$ with respect to each parameter ($W_1, b_1, W_2, b_2$) using the multivariable chain rule. This process is known as backpropagation. We denote the gradient of the loss with respect to a matrix $M$ as $\\delta_M = \\frac{\\partial L}{\\partial M}$.\n\n1.  **Gradient at the Output**: The gradient of the loss with respect to the network's predictions $\\hat{Y}$ is:\n    $$\n    \\delta_{\\hat{Y}} = \\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{1}{n} (\\hat{Y} - Y)\n    $$\n    Since $\\hat{Y} =Z_2$, we have $\\delta_{Z_2} = \\delta_{\\hat{Y}}$.\n\n2.  **Gradients for the Output Layer ($W_2, b_2$)**:\n    Using the chain rule on $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$:\n    $$\n    \\delta_{W_2} = \\frac{\\partial L}{\\partial W_2} = \\delta_{Z_2}^\\top H\n    $$\n    $$\n    \\delta_{b_2} = \\frac{\\partial L}{\\partial b_2} = (\\mathrm{sum}(\\delta_{Z_2}, \\text{axis}=0))^\\top = \\delta_{Z_2}^\\top \\mathbf{1}\n    $$\n\n3.  **Propagating the Gradient to the Hidden Layer**:\n    The gradient is propagated back to the hidden layer's activations $H$:\n    $$\n    \\delta_H = \\frac{\\partial L}{\\partial H} = \\delta_{Z_2} W_2\n    $$\n    Next, the gradient is propagated through the $\\tanh$ activation function, using $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$:\n    $$\n    \\delta_{Z_1} = \\frac{\\partial L}{\\partial Z_1} = \\delta_H \\odot (1 - H^2)\n    $$\n    where $\\odot$ denotes the element-wise (Hadamard) product.\n\n4.  **Gradients for the Hidden Layer ($W_1, b_1$)**:\n    Finally, using the chain rule on $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$:\n    $$\n    \\delta_{W_1} = \\frac{\\partial L}{\\partial W_1} = \\delta_{Z_1}^\\top X\n    $$\n    $$\n    \\delta_{b_1} = \\frac{\\partial L}{\\partial b_1} = (\\mathrm{sum}(\\delta_{Z_1}, \\text{axis}=0))^\\top = \\delta_{Z_1}^\\top \\mathbf{1}\n    $$\n\nThese matrix-form equations provide a complete algorithm for computing the analytical gradient.\n\n### Numerical Verification\n\nTo verify the analytical gradient, we compare it against a numerical approximation.\n\n- **Parameter Vectorization**: All network parameters ($W_1, b_1, W_2, b_2$) are flattened and concatenated into a single vector $\\theta \\in \\mathbb{R}^{p}$, with $p=16$. The specified order is $W_1$ (row-major), $b_1$, $W_2$ (row-major), and $b_2$.\n\n- **Finite-Difference Approximation**: The gradient is approximated using the first-order forward-difference formula. The $k$-th component of the gradient vector is estimated as:\n  $$\n  g_{\\mathrm{FD},k}(\\epsilon) = \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}\n  $$\n  where $e_k$ is the $k$-th standard basis vector and $\\epsilon$ is a small step size.\n\n- **Error Analysis and Order Verification**:\n  The discrepancy between the analytical gradient $\\nabla_\\theta L$ and the finite-difference approximation $g_{\\mathrm{FD}}(\\epsilon)$ is measured by the Euclidean norm of their difference:\n  $$\n  \\mathrm{err}(\\epsilon) = \\| \\nabla_\\theta L - g_{\\mathrm{FD}}(\\epsilon) \\|_2\n  $$\n  For a first-order method, this error is expected to be proportional to $\\epsilon$, i.e., $\\mathrm{err}(\\epsilon) \\approx C\\epsilon$. This implies that the ratio of errors for two step sizes $\\epsilon_i$ and $\\epsilon_{i+1}$ should be approximately equal to the ratio of the step sizes themselves. To quantify this relationship, we compute the empirical order of convergence $s_i$:\n  $$\n  s_i = \\frac{\\log(\\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}))}{\\log(\\epsilon_i / \\epsilon_{i+1})}\n  $$\n  A value of $s_i \\approx 1$ confirms the expected first-order convergence, thus validating the analytical gradient implementation. We use the median of the computed $s_i$ values for robustness. The verification is considered successful if this median order $s_{\\mathrm{med}}$ is within the range $[0.8, 1.2]$ and if the error is monotonically decreasing for the initial, larger values of $\\epsilon$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the backpropagation verification problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n    X = np.array([\n        [0.2, -0.1, 0.4],\n        [-0.5, 0.3, 0.1],\n        [0.0, -0.2, 0.2],\n        [0.1, 0.4, -0.3]\n    ])\n    y = np.array([\n        [0.5],\n        [-0.1],\n        [0.2],\n        [0.0]\n    ])\n    n = X.shape[0]  # Mini-batch size, n=4\n\n    # Parameters for Test Case A\n    W1_a = np.array([\n        [0.3, -0.1, 0.2],\n        [-0.4, 0.5, 0.1],\n        [0.2, 0.3, -0.2]\n    ])\n    b1_a = np.array([[0.05], [-0.02], [0.01]])\n    W2_a = np.array([[0.6, -0.7, 0.2]])\n    b2_a = np.array([[0.03]])\n    \n    test_cases_params = [\n        (W1_a, b1_a, W2_a, b2_a),\n        (W1_a * 0.1, b1_a * 0.1, W2_a * 0.1, b2_a * 0.1) # Test Case B\n    ]\n\n    epsilons = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n    \n    # --- Helper functions for parameter manipulation ---\n    def params_to_vec(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            b2.flatten()\n        ])\n\n    def vec_to_params(theta):\n        W1 = theta[0:9].reshape(3, 3)\n        b1 = theta[9:12].reshape(3, 1)\n        W2 = theta[12:15].reshape(1, 3)\n        b2 = theta[15:16].reshape(1, 1)\n        return W1, b1, W2, b2\n        \n    # --- Core functions for forward/backward pass ---\n    def forward_pass(W1, b1, W2, b2):\n        Z1 = X @ W1.T + b1.T\n        H = np.tanh(Z1)\n        Z2 = H @ W2.T + b2\n        Y_hat = Z2\n        loss = (1 / (2 * n)) * np.sum((Y_hat - y)**2)\n        return loss, Z1, H, Y_hat\n\n    def analytical_gradient(W1, b1, W2, b2):\n        _loss, _Z1, H, Y_hat = forward_pass(W1, b1, W2, b2)\n        \n        # Backward pass\n        dZ2 = (1 / n) * (Y_hat - y)\n        dW2 = dZ2.T @ H\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dH = dZ2 @ W2\n        dZ1 = dH * (1 - H**2)\n        dW1 = dZ1.T @ X\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n        \n        return params_to_vec(dW1, db1, dW2, db2)\n\n    def run_verification(params):\n        W1, b1, W2, b2 = params\n        \n        # 1. Compute analytical gradient\n        grad_analytic = analytical_gradient(W1, b1, W2, b2)\n        \n        # 2. Compute finite-difference gradient for each epsilon\n        theta_base = params_to_vec(W1, b1, W2, b2)\n        loss_base, _, _, _ = forward_pass(W1, b1, W2, b2)\n        \n        errors = []\n        for eps in epsilons:\n            grad_fd = np.zeros_like(theta_base)\n            for k in range(len(theta_base)):\n                theta_p = np.copy(theta_base)\n                theta_p[k] += eps\n                \n                W1_p, b1_p, W2_p, b2_p = vec_to_params(theta_p)\n                loss_p, _, _, _ = forward_pass(W1_p, b1_p, W2_p, b2_p)\n                \n                grad_fd[k] = (loss_p - loss_base) / eps\n            \n            error = np.linalg.norm(grad_analytic - grad_fd)\n            errors.append(error)\n\n        # 3. Compute empirical orders\n        orders = []\n        for i in range(len(epsilons) - 1):\n            s_i = np.log(errors[i] / errors[i+1]) / np.log(epsilons[i] / epsilons[i+1])\n            orders.append(s_i)\n        \n        s_med = np.median(orders)\n        \n        # 4. Perform boolean checks\n        pass_order = 0.8 = s_med = 1.2\n        pass_mono = all(errors[i] > errors[i+1] for i in range(4))\n\n        # 5. Collect results\n        err_min_eps = errors[-1]\n        \n        return [err_min_eps, s_med, bool(pass_order), bool(pass_mono)]\n\n    # --- Main execution loop ---\n    final_results = []\n    for case_params in test_cases_params:\n        result = run_verification(case_params)\n        final_results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3100954"}]}