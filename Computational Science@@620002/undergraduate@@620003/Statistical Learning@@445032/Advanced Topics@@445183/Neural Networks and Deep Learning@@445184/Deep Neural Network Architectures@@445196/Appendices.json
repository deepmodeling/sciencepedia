{"hands_on_practices": [{"introduction": "One of the most fundamental questions in deep learning is how to design a network's architecture. Given a fixed computational budget, should you build a network that is wide or one that is deep? This practice explores this classic trade-off by modeling the generalization error as a sum of approximation error (what the network can represent) and estimation error (what can be learned from finite data) [@problem_id:3113786]. By implementing a search for the optimal width and depth, you will gain a concrete understanding of how architectural choices influence model capacity and performance under practical constraints.", "problem": "Consider fully connected deep feedforward neural networks with rectified linear unit (ReLU) activations, input dimension $d$, one scalar output, hidden layer width $m$ (the same for all hidden layers), and depth $L$ (the number of hidden layers). Let the total parameter budget be $P$. In supervised learning with Empirical Risk Minimization (ERM), a standard decomposition of the expected risk into approximation error plus estimation error motivates searching for an architecture that balances expressive power and learnability under a fixed budget.\n\nUse the following context-appropriate fundamental base:\n- Bias-variance trade-off and approximation-estimation decomposition: the expected risk of a trained network can be decomposed into an approximation error term and an estimation error term.\n- Capacity measures and generalization: the estimation error scales with capacity measures such as the Rademacher complexity, which increases with the number of parameters and the depth.\n- Parameter counting for fully connected networks: the number of parameters is the sum of weights and biases across layers.\n\nDefine the total parameter count $P_{\\text{total}}(m,L;d)$ for a network with $L$ hidden layers of width $m$ and input dimension $d$ by counting weights and biases:\n- First layer weights: $d \\cdot m$; first layer biases: $m$.\n- Hidden layers $2$ through $L$: $(L-1)$ layers, each with $m \\cdot m$ weights and $m$ biases.\n- Output layer weights: $m$; output layer bias: $1$.\n\nTherefore,\n$$\nP_{\\text{total}}(m,L;d) \\;=\\; d\\,m \\;+\\; (L-1)\\,m^2 \\;+\\; (L+1)\\,m \\;+\\; 1.\n$$\n\nModel the generalization error bound for an architecture $(m,L)$ under dataset size $n$ as the sum of an approximation term and an estimation term:\n- Approximation error term:\n$$\n\\mathcal{A}(m,L) \\;=\\; \\frac{C_a}{\\,m^{\\gamma}\\,(1+L)^{\\lambda}\\,},\n$$\nwith constants $C_a > 0$, $\\gamma \\in (0,1]$, and $\\lambda \\in (0,1]$ encoding diminishing returns in width and depth.\n- Estimation error term:\n$$\n\\mathcal{E}(m,L) \\;=\\; C_e \\,\\sqrt{\\frac{P_{\\text{total}}(m,L;d)\\,\\log(1+m)\\,L}{n}},\n$$\nwith constant $C_e > 0$, reflecting the growth of capacity with both parameter count and depth.\n\nThe combined theoretical bound is\n$$\n\\mathcal{G}(m,L) \\;=\\; \\mathcal{A}(m,L) \\;+\\; \\mathcal{E}(m,L).\n$$\n\nUnder the constraint $P_{\\text{total}}(m,L;d) \\le P$ and $m,L \\in \\mathbb{N}$ with $m \\ge 1$ and $L \\ge 1$, identify the architecture $(m,L)$ that minimizes $\\mathcal{G}(m,L)$. If multiple architectures achieve the same minimum up to a tolerance of $10^{-9}$, break ties by preferring the smallest $P_{\\text{total}}(m,L;d)$, then the smallest $L$, then the smallest $m$.\n\nTo ensure scientific realism and computational tractability, search $L$ over $1 \\le L \\le L_{\\text{cap}}$, where\n$$\nL_{\\text{cap}} \\;=\\; \\min\\!\\Big(L_{\\max}, \\,\\big\\lfloor \\sqrt{P} \\big\\rfloor\\Big), \\quad L_{\\max} \\;=\\; \\left\\lfloor \\frac{P - d - 1}{2} \\right\\rfloor,\n$$\nwhich guarantees feasibility at $m=1$ and caps depth by a budget-driven heuristic. For each $L$ in the search range, choose $m$ from $1$ up to the largest integer $m_{\\max}(L)$ satisfying $P_{\\text{total}}(m,L;d) \\le P$. For $L=1$, this reduces to a linear bound $m \\le \\left\\lfloor \\dfrac{P - 1}{d + 2} \\right\\rfloor$. For $L>1$, solve the quadratic inequality $(L-1)m^2 + (d + L + 1)m + 1 - P \\le 0$ to obtain the upper root.\n\nYour program must implement this search and produce the optimal $(m,L)$ for each test case below.\n\nTest suite (each test case is a tuple $(d,n,P,C_a,C_e,\\gamma,\\lambda)$):\n- Case $1$: $(10, 5000, 5000, 1.0, 0.5, 0.5, 0.3)$.\n- Case $2$: $(20, 1500, 600, 1.0, 0.6, 0.5, 0.3)$.\n- Case $3$: $(5, 20000, 25000, 0.8, 0.4, 0.5, 0.3)$.\n- Case $4$: $(5, 800, 25000, 0.8, 0.4, 0.5, 0.3)$.\n- Case $5$: $(30, 5000, 1200, 1.2, 0.5, 0.5, 0.3)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is the chosen architecture represented as a two-element list $[m,L]$, for the cases in the given order. For example, the output should look like $[[m_1,L_1],[m_2,L_2],[m_3,L_3],[m_4,L_4],[m_5,L_5]]$ with no additional text.", "solution": "The problem presented is a valid and well-posed exercise in the theoretical analysis of deep learning architectures. It requires finding the optimal neural network architecture, defined by its width $m$ and depth $L$, that minimizes a theoretical generalization error bound under a constraint on the total number of parameters. The problem is scientifically grounded in the principles of statistical learning theory, specifically the trade-off between approximation and estimation error, and is mathematically formalized as a discrete optimization problem.\n\nThe objective is to find the architecture $(m, L)$ that minimizes the generalization error bound $\\mathcal{G}(m,L)$ for a fully connected feedforward neural network. The network has an input dimension $d$, $L$ hidden layers each of width $m$, and a single scalar output. The search is constrained by a total parameter budget $P$. The variables $m$ and $L$ are positive integers.\n\nThe total number of parameters, $P_{\\text{total}}(m,L;d)$, for such a network is given by the sum of weights and biases in each layer:\n$$\nP_{\\text{total}}(m,L;d) = (d m + m) + (L-1)(m^2 + m) + (m+1) = (L-1)m^2 + (d+L+1)m + 1\n$$\nThis formula correctly counts the parameters for the first layer (input-to-hidden), the $L-1$ inter-hidden layers, and the final output layer.\n\nThe generalization error bound $\\mathcal{G}(m,L)$ is modeled as the sum of two competing terms: the approximation error $\\mathcal{A}(m,L)$ and the estimation error $\\mathcal{E}(m,L)$.\n$$\n\\mathcal{G}(m,L) = \\mathcal{A}(m,L) + \\mathcal{E}(m,L)\n$$\n\nThe approximation error,\n$$\n\\mathcal{A}(m,L) = \\frac{C_a}{m^{\\gamma}(1+L)^{\\lambda}}\n$$\nrepresents the inherent limitation of the network's function class to approximate the true underlying function. This term decreases as width $m$ and depth $L$ increase, reflecting the enhanced expressive power of larger networks. The parameters $\\gamma \\in (0,1]$ and $\\lambda \\in (0,1]$ model diminishing returns from increasing width and depth, respectively.\n\nThe estimation error,\n$$\n\\mathcal{E}(m,L) = C_e \\sqrt{\\frac{P_{\\text{total}}(m,L;d)\\log(1+m)L}{n}}\n$$\nrepresents the error resulting from learning the network parameters from a finite dataset of size $n$. This term increases with model complexity, which is captured here by the parameter count $P_{\\text{total}}$, depth $L$, and width-dependent factor $\\log(1+m)$. This form is motivated by capacity measures like Rademacher complexity, which often scale with the number of parameters and other architectural properties.\n\nThe optimization problem is thus:\n$$\n\\min_{m,L \\in \\mathbb{N}, m \\ge 1, L \\ge 1} \\mathcal{G}(m,L) \\quad \\text{subject to} \\quad P_{\\text{total}}(m,L;d) \\le P\n$$\n\nTo solve this, we must perform a search over a discrete and finite space of valid architectures. The problem provides a structured method for defining this search space.\n\nFirst, the search range for the depth $L$ is constrained to $1 \\le L \\le L_{\\text{cap}}$. The upper bound $L_{\\text{cap}}$ is defined as:\n$$\nL_{\\text{cap}} = \\min\\left( L_{\\max}, \\lfloor \\sqrt{P} \\rfloor \\right), \\quad \\text{where} \\quad L_{\\max} = \\left\\lfloor \\frac{P - d - 1}{2} \\right\\rfloor\n$$\nThe term $L_{\\max}$ is derived from the parameter constraint for the simplest possible width, $m=1$. For $m=1$, $P_{\\text{total}}(1,L;d) = 2L+d+1$. Requiring $2L+d+1 \\le P$ yields $L \\le (P-d-1)/2$. This ensures that for any $L$ in the search range up to $L_{\\max}$, at least one valid architecture exists (i.e., with $m=1$). The additional cap of $\\lfloor \\sqrt{P} \\rfloor$ is a heuristic to keep the search computationally tractable.\n\nSecond, for each chosen depth $L$ in its valid range, the search range for the width $m$ is constrained by $1 \\le m \\le m_{\\max}(L)$. The value $m_{\\max}(L)$ is the largest integer $m$ satisfying the parameter budget inequality $P_{\\text{total}}(m,L;d) \\le P$.\n- For $L=1$, the inequality is linear in $m$: $(d+2)m+1 \\le P$, which gives $m_{\\max}(1) = \\lfloor (P-1)/(d+2) \\rfloor$.\n- For $L>1$, the inequality is quadratic in $m$: $(L-1)m^2 + (d+L+1)m + (1-P) \\le 0$. Since the coefficient of $m^2$ is positive, the valid range for $m$ lies between the two roots of the corresponding quadratic equation. As $m$ must be positive, we are interested in the positive root, which provides the upper bound. Let $a=L-1$, $b=d+L+1$, and $c=1-P$. The upper root is $\\frac{-b + \\sqrt{b^2-4ac}}{2a}$. Thus, $m_{\\max}(L) = \\left\\lfloor \\frac{-(d+L+1) + \\sqrt{(d+L+1)^2 - 4(L-1)(1-P)}}{2(L-1)} \\right\\rfloor$.\n\nThe solution algorithm is a systematic search over this defined space:\n1. Initialize a structure to store the best architecture found so far, $(m^*, L^*)$, along with its generalization error $\\mathcal{G}^*$, and parameter count $P_{\\text{total}}^*$. Initialize $\\mathcal{G}^*$ to infinity.\n2. Iterate through each depth $L$ from $1$ to $L_{\\text{cap}}$.\n3. For each $L$, calculate the maximum valid width $m_{\\max}(L)$. If $m_{\\max}(L)  1$, no valid architecture exists for this depth, so continue to the next $L$.\n4. Iterate through each width $m$ from $1$ to $m_{\\max}(L)$.\n5. For each pair $(m, L)$, calculate $P_{\\text{total}}(m,L;d)$ and the total error $\\mathcal{G}(m,L)$.\n6. Compare the calculated $\\mathcal{G}(m,L)$ with the best-so-far $\\mathcal{G}^*$ using the specified tie-breaking rules. An update to the best architecture occurs if the current architecture $(m,L)$ is superior according to a lexicographical comparison of the tuple $(\\mathcal{G}, P_{\\text{total}}, L, m)$. Specifically, a new best is found if:\n   - $\\mathcal{G}(m,L)  \\mathcal{G}^* - 10^{-9}$, or\n   - $|\\mathcal{G}(m,L) - \\mathcal{G}^*| \\le 10^{-9}$ and $P_{\\text{total}}(m,L;d)  P_{\\text{total}}^*$, or\n   - $|\\mathcal{G}(m,L) - \\mathcal{G}^*| \\le 10^{-9}$, $P_{\\text{total}}(m,L;d) = P_{\\text{total}}^*$, and $L  L^*$, or\n   - $|\\mathcal{G}(m,L) - \\mathcal{G}^*| \\le 10^{-9}$, $P_{\\text{total}}(m,L;d) = P_{\\text{total}}^*$, $L = L^*$, and $m  m^*$.\n\nThis exhaustive search guarantees finding the global minimum of $\\mathcal{G}(m,L)$ within the constrained search space, and the detailed tie-breaking procedure ensures a unique solution is identified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Finds the optimal neural network architecture (m, L) by minimizing a theoretical \n    generalization error bound under a parameter budget constraint.\n    \"\"\"\n    test_cases = [\n        # (d, n, P, C_a, C_e, gamma, lambda)\n        (10, 5000, 5000, 1.0, 0.5, 0.5, 0.3),\n        (20, 1500, 600, 1.0, 0.6, 0.5, 0.3),\n        (5, 20000, 25000, 0.8, 0.4, 0.5, 0.3),\n        (5, 800, 25000, 0.8, 0.4, 0.5, 0.3),\n        (30, 5000, 1200, 1.2, 0.5, 0.5, 0.3)\n    ]\n\n    all_results = []\n\n    for d, n, P, C_a, C_e, gamma, lmbda in test_cases:\n        \n        best_g = float('inf')\n        best_p_total = float('inf')\n        best_m = -1\n        best_l = -1\n\n        # Determine the search range for depth L\n        # L_max ensures that at least m=1 is a feasible architecture\n        if P - d - 1  0: # Ensure argument to floor is non-negative\n             l_max = 0\n        else:\n             l_max = int(np.floor((P - d - 1) / 2))\n        \n        l_cap = min(l_max, int(np.floor(np.sqrt(P))))\n\n        # Iterate through all valid depths L\n        for L in range(1, l_cap + 1):\n            m_max = 0\n            if L == 1:\n                # Linear inequality for m\n                if d + 2 > 0:\n                    m_max = int(np.floor((P - 1) / (d + 2)))\n            else: # L > 1\n                # Quadratic inequality for m: a*m^2 + b*m + c = 0\n                a = float(L - 1)\n                b = float(d + L + 1)\n                c = float(1 - P)\n                \n                discriminant = b**2 - 4*a*c\n                if discriminant >= 0:\n                    # Positive root gives the upper bound for m\n                    m_root = (-b + np.sqrt(discriminant)) / (2*a)\n                    if m_root >= 1:\n                        m_max = int(np.floor(m_root))\n\n            if m_max  1:\n                continue\n\n            # Iterate through all valid widths m for the given L\n            for m in range(1, m_max + 1):\n                # Calculate total parameters\n                p_total = (L - 1) * m**2 + (d + L + 1) * m + 1\n                \n                # The search for m is bounded by m_max, which is calculated to satisfy\n                # p_total = P. This explicit check is a safeguard.\n                if p_total > P:\n                    continue\n\n                # Calculate Approximation Error\n                approx_error = C_a / (np.power(m, gamma) * np.power(1 + L, lmbda))\n                \n                # Calculate Estimation Error\n                est_error_radical = (p_total * np.log(1 + m) * L) / n\n                est_error = C_e * np.sqrt(est_error_radical)\n                \n                # Total generalization error bound\n                g_current = approx_error + est_error\n\n                # Apply update logic with tie-breaking\n                # Tolerance for floating point comparison of G\n                g_diff = g_current - best_g\n                is_better = False\n                \n                if g_diff  -1e-9:\n                    is_better = True\n                elif abs(g_diff) = 1e-9:\n                    # Tie in G, check P_total\n                    if p_total  best_p_total:\n                        is_better = True\n                    elif p_total == best_p_total:\n                        # Tie in P_total, check L\n                        if L  best_l:\n                            is_better = True\n                        elif L == best_l:\n                            # Tie in L, check m\n                            if m  best_m:\n                                is_better = True\n                \n                if is_better:\n                    best_g = g_current\n                    best_p_total = p_total\n                    best_m = m\n                    best_l = L\n\n        all_results.append([best_m, best_l])\n    \n    # Format the final output string to match the required format \"[[m1,L1],[m2,L2],...]\"\n    # by removing spaces from the default string representation of a list of lists.\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3113786"}, {"introduction": "Modern neural networks are complex systems where different components interact in subtle ways. This exercise illuminates one such critical interaction: the one between weight decay ($\\ell_2$ regularization) and Batch Normalization (BN) [@problem_id:3113809]. By optimizing a simple model with a single learnable BN scale parameter, you will see how BN's scale invariance alters the effect of weight decay and develop a deeper intuition for how to properly regularize modern architectures.", "problem": "Consider a simplified deep neural network architecture composed of a single affine channel followed by Batch Normalization (BN) with a learnable scale parameter and a binary classifier. The dataset consists of scalar inputs and binary labels. Let the raw inputs be denoted by $x_i \\in \\mathbb{R}$ and labels by $y_i \\in \\{-1,+1\\}$ for $i \\in \\{1,\\dots,n\\}$. The Batch Normalization (BN) transformation computes the normalized features using the batch mean and variance and a numerical stabilizer $\\varepsilon > 0$ as\n$$\n\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}},\n$$\nwhere\n$$\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i,\\quad \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2.\n$$\nWe set the BN shift parameter to zero and retain only the BN scale parameter $\\gamma \\in \\mathbb{R}$. The classifier score for sample $i$ is\n$$\ns_i(\\gamma) = \\gamma \\hat{x}_i.\n$$\nWe adopt empirical risk minimization with the logistic loss and weight decay (also called $\\ell_2$ regularization) applied directly to the BN scale parameter. The training objective is\n$$\nL(\\gamma) = \\frac{1}{n}\\sum_{i=1}^n \\log\\!\\left(1 + \\exp\\!\\left(-y_i\\, s_i(\\gamma)\\right)\\right) + \\frac{\\lambda}{2}\\,\\gamma^2,\n$$\nwhere $\\lambda \\ge 0$ is the weight decay coefficient. We quantify classification confidence by the functional margin\n$$\nm(\\gamma) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, s_i(\\gamma) \\right) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, \\gamma\\, \\hat{x}_i \\right).\n$$\n\nFundamental base. Use empirical risk minimization with logistic loss, define Batch Normalization (BN) exactly as above, and apply weight decay to $\\gamma$ as stated. Do not introduce any alternative losses, architectures, or regularizers. The task is to derive from these bases the algorithm to compute the unique minimizer $\\gamma^\\star$ of $L(\\gamma)$ and then compute $m(\\gamma^\\star)$.\n\nYour program must:\n- Implement the BN transformation exactly as specified, with the given $\\varepsilon$ for each test case.\n- Optimize $L(\\gamma)$ over $\\gamma \\in \\mathbb{R}$ to obtain $\\gamma^\\star$ using a principled numerical method grounded in the convexity properties of the objective.\n- Compute the margin $m(\\gamma^\\star)$ and report it as a real number.\n- For numerical reporting, round each margin to $6$ decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\,0.123456,0.234567,0.345678,0.456789\\,]$.\n\nTest suite. Use the following four test cases to study the interaction between weight decay and the BN scale parameter and its impact on margin. Each case provides $(x,y,\\varepsilon,\\lambda)$:\n- Case $1$ (general separable-like configuration, moderate weight decay):\n  - $x = [\\, -2.0,\\,-0.5,\\,0.0,\\,0.5,\\,2.0 \\,]$\n  - $y = [\\, -1,\\,-1,\\,+1,\\,+1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-5}$\n  - $\\lambda = 0.2$\n- Case $2$ (non-separable configuration, boundary with no explicit weight decay on $\\gamma$):\n  - $x = [\\, -1.0,\\,-0.8,\\,-0.2,\\,0.1,\\,0.2,\\,0.9 \\,]$\n  - $y = [\\, -1,\\,+1,\\,-1,\\,+1,\\,-1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-5}$\n  - $\\lambda = 0.0$\n- Case $3$ (near-degenerate variance, tests BN stabilization via $\\varepsilon$):\n  - $x = [\\, 1.0,\\,1.01,\\,0.99,\\,1.02 \\,]$\n  - $y = [\\, -1,\\,+1,\\,-1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-2}$\n  - $\\lambda = 0.1$\n- Case $4$ (strong weight decay suppresses scale):\n  - $x = [\\, -3.0,\\,-1.0,\\,1.0,\\,3.0 \\,]$\n  - $y = [\\, -1,\\,-1,\\,+1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-5}$\n  - $\\lambda = 5.0$\n\nAnswer specification. Your program should produce a single line of output containing the margins for the four cases, ordered as listed, rounded to $6$ decimal places, and formatted as a comma-separated list enclosed in square brackets, for example $[\\,m_1,m_2,m_3,m_4\\,]$, with each $m_k$ a real number. No other output is permitted. Angles are not involved, and no physical units appear, so no unit conversions are required.", "solution": "The problem requires the determination of a functional margin for a simple classifier after optimizing its single learnable parameter, the Batch Normalization (BN) scale $\\gamma$. The solution is grounded in the principles of convex optimization.\n\nFirst, we formalize the problem. The input data consists of pairs $(x_i, y_i)$ for $i=1, \\dots, n$, where $x_i \\in \\mathbb{R}$ are scalar features and $y_i \\in \\{-1, +1\\}$ are binary labels. The features undergo Batch Normalization. The BN statistics, batch mean $\\mu$ and batch variance $\\sigma^2$, are calculated as:\n$$\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i\n$$\n$$\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2\n$$\nThe normalized features $\\hat{x}_i$ are then computed using a numerical stabilizer $\\varepsilon > 0$:\n$$\n\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n$$\nThe classifier score for a sample $i$ is $s_i(\\gamma) = \\gamma \\hat{x}_i$. The objective is to find the optimal scale parameter $\\gamma^\\star$ that minimizes the regularized logistic loss:\n$$\nL(\\gamma) = \\frac{1}{n}\\sum_{i=1}^n \\log\\!\\left(1 + \\exp\\!\\left(-y_i\\, \\gamma\\, \\hat{x}_i\\right)\\right) + \\frac{\\lambda}{2}\\,\\gamma^2\n$$\nwhere $\\lambda \\ge 0$ is the weight decay coefficient.\n\nThe core of the task is to solve the one-dimensional optimization problem $\\gamma^\\star = \\arg\\min_{\\gamma \\in \\mathbb{R}} L(\\gamma)$. To establish a principled solution method, we analyze the convexity of the objective function $L(\\gamma)$.\nThe function $f(u) = \\log(1+e^u)$ is convex. The argument of the exponential, $u_i(\\gamma) = -y_i \\gamma \\hat{x}_i$, is a linear function of $\\gamma$. Since the composition of a convex function with an affine mapping is convex, each term $\\log\\!\\left(1 + \\exp\\!\\left(-y_i\\, \\gamma\\, \\hat{x}_i\\right)\\right)$ is a convex function of $\\gamma$. A sum of convex functions is also convex, so the first term of $L(\\gamma)$, the average logistic loss, is convex.\nThe second term, $\\frac{\\lambda}{2}\\gamma^2$, is the $\\ell_2$ regularization term. For $\\lambda \\ge 0$, this is a convex function. For $\\lambda > 0$, it is strictly convex.\n\nThe sum of a convex function and a strictly convex function is strictly convex. Therefore, for any $\\lambda > 0$, $L(\\gamma)$ is strictly convex. A strictly convex function has a unique global minimizer. For the case where $\\lambda = 0$, $L(\\gamma)$ is convex. A unique minimizer still exists provided the data is not perfectly separable by a hyperplane through the origin in the feature space (i.e., not all $y_i \\hat{x}_i$ have the same sign), which holds for the given test cases.\n\nThe unique minimizer $\\gamma^\\star$ can be found by solving for the root of the first derivative of the objective function, $\\frac{dL}{d\\gamma} = 0$. The derivative is:\n$$\n\\frac{dL}{d\\gamma} = -\\frac{1}{n}\\sum_{i=1}^n y_i \\hat{x}_i S(-y_i \\gamma \\hat{x}_i) + \\lambda\\gamma\n$$\nwhere $S(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function. This is a non-linear equation in $\\gamma$, which necessitates a numerical root-finding or optimization algorithm. Since $L(\\gamma)$ is a one-dimensional convex function, standard numerical methods such as Newton's method or Brent's method are guaranteed to converge to the unique global minimum. We will employ a numerical optimization routine to find $\\gamma^\\star$.\n\nOnce $\\gamma^\\star$ is determined, we compute the functional margin, defined as the minimum confidence-scaled score across all samples:\n$$\nm(\\gamma^\\star) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, s_i(\\gamma^\\star) \\right) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, \\gamma^\\star \\hat{x}_i \\right)\n$$\nThis value quantifies the classifier's performance on its \"worst-case\" sample.\n\nThe overall algorithm for each test case is as follows:\n$1$. Given the input data $(x, y)$ and parameters $(\\varepsilon, \\lambda)$, compute the batch statistics $\\mu$ and $\\sigma^2$.\n$2$. Compute the normalized features $\\hat{x}$ for all samples.\n$3$. Define the objective function $L(\\gamma)$ as specified. For robust numerical computation of the logistic loss term $\\log(1+\\exp(z))$, the identity $\\log(1+\\exp(z)) = \\log(1+\\exp(-\\vert z\\vert)) + \\max(z,0)$ is used to prevent floating-point overflow.\n$4$. Use a numerical scalar optimization algorithm, such as Brent's method, to find the unique minimizer $\\gamma^\\star$ of $L(\\gamma)$.\n$5$. Calculate the functional margin $m(\\gamma^\\star) = \\min(y \\odot (\\gamma^\\star \\hat{x}))$, where $\\odot$ denotes the element-wise product.\n$6$. Report the resulting margin, rounded to $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the optimal BN scale parameter `gamma` and computes the\n    functional margin for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general separable-like configuration, moderate weight decay)\n        {\n            \"x\": np.array([-2.0, -0.5, 0.0, 0.5, 2.0]),\n            \"y\": np.array([-1, -1, 1, 1, 1]),\n            \"eps\": 1e-5,\n            \"lam\": 0.2\n        },\n        # Case 2 (non-separable configuration, no explicit weight decay)\n        {\n            \"x\": np.array([-1.0, -0.8, -0.2, 0.1, 0.2, 0.9]),\n            \"y\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"eps\": 1e-5,\n            \"lam\": 0.0\n        },\n        # Case 3 (near-degenerate variance, tests BN stabilization via eps)\n        {\n            \"x\": np.array([1.0, 1.01, 0.99, 1.02]),\n            \"y\": np.array([-1, 1, -1, 1]),\n            \"eps\": 1e-2,\n            \"lam\": 0.1\n        },\n        # Case 4 (strong weight decay suppresses scale)\n        {\n            \"x\": np.array([-3.0, -1.0, 1.0, 3.0]),\n            \"y\": np.array([-1, -1, 1, 1]),\n            \"eps\": 1e-5,\n            \"lam\": 5.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x, y, eps, lam = case[\"x\"], case[\"y\"], case[\"eps\"], case[\"lam\"]\n        n = float(len(x))\n\n        # Step 1  2: Batch Normalization\n        mu = np.mean(x)\n        # Use ddof=0 for population variance, consistent with the problem statement\n        sigma_sq = np.var(x, ddof=0)\n        x_hat = (x - mu) / np.sqrt(sigma_sq + eps)\n\n        # Step 3: Define objective function L(gamma)\n        def objective_function(gamma, x_hat, y, n, lam):\n            \"\"\"\n            Computes the regularized logistic loss.\n            Uses a numerically stable implementation for log(1 + exp(z)).\n            \"\"\"\n            # Argument of the exponential in the logistic loss\n            z = -y * gamma * x_hat\n            \n            # Numerically stable computation of log(1 + exp(z)) for each sample\n            log_loss_terms = np.log(1 + np.exp(-np.abs(z))) + np.maximum(z, 0)\n            \n            # Average logistic loss\n            avg_log_loss = np.sum(log_loss_terms) / n\n            \n            # L2 regularization term\n            regularization = (lam / 2.0) * (gamma**2)\n            \n            return avg_log_loss + regularization\n\n        # Step 4: Find the optimal gamma\n        # minimize_scalar finds the minimum of a single-variable function.\n        # For a convex function, this finds the unique global minimum.\n        opt_result = minimize_scalar(\n            objective_function,\n            args=(x_hat, y, n, lam)\n        )\n        gamma_star = opt_result.x\n\n        # Step 5: Compute the functional margin\n        margin = np.min(y * gamma_star * x_hat)\n        \n        results.append(margin)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113809"}, {"introduction": "A well-trained model should not only be accurate but also well-calibrated, meaning its confidence scores should reflect its true correctness probability. Label smoothing is a popular regularization technique that discourages overconfidence and improves calibration. This practice guides you to derive and compute the precise effects of label smoothing on both the model's internal logit margin and its external calibration error [@problem_id:3113811], connecting a simple training heuristic to measurable and desirable changes in model behavior.", "problem": "Consider a $K$-class classifier with a single logit vector $\\mathbf{z} \\in \\mathbb{R}^K$ and the Softmax function defined by $p_i(\\mathbf{z}) = \\exp(z_i) \\big/ \\sum_{j=1}^K \\exp(z_j)$ for each class index $i \\in \\{1,\\dots,K\\}$. The training objective is the cross-entropy loss $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i(\\mathbf{z})$, where $\\mathbf{y}$ is a target probability vector. Label smoothing with parameter $\\epsilon \\in [0,1)$ replaces the one-hot target for a true class index $t$ by $\\tilde{y}_t = 1-\\epsilon$ and $\\tilde{y}_j = \\epsilon/(K-1)$ for all $j \\neq t$, which produces a valid probability distribution that sums to $1$.\n\nFrom the foundational definitions of Softmax and cross-entropy, and using only generally accepted facts about these functions (including the identity that the cross-entropy decomposes as $H(\\mathbf{y}) + \\mathrm{KL}(\\mathbf{y}\\,\\|\\,\\mathbf{p}(\\mathbf{z}))$, where $H$ denotes entropy and $\\mathrm{KL}$ is the Kullback–Leibler divergence), derive the following:\n\n1. The minimizing Softmax output under label smoothing equals the smoothed target, that is, $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$. Use this to obtain a canonical representative of logits $\\mathbf{z}^\\star$ achieving the minimum by taking $z^\\star_i = \\log \\tilde{y}_i + c$ for some constant $c \\in \\mathbb{R}$, and then define the logit margin for the true class as $m^\\star(\\epsilon,K) = z^\\star_t - \\max_{j \\neq t} z^\\star_j$. Express $m^\\star(\\epsilon,K)$ as a closed-form function of $\\epsilon$ and $K$ using the natural logarithm $\\log(\\cdot)$, and determine its limiting value when $\\epsilon = 0$.\n\n2. To analyze calibration in a mathematically controlled setting, consider a noiseless evaluation scenario in which the model’s top-1 prediction is always the true class and the accuracy conditioned on any confidence score is $1$. Define the one-bin Expected Calibration Error (ECE) as the absolute difference between the average predicted confidence for the true class and the empirical accuracy. Under the same minimizing solution $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$, derive a closed-form expression for the ECE as a function of $\\epsilon$ and $K$, and simplify it as far as possible.\n\nYour task is to implement a complete program that computes, for each test case $(K,\\epsilon)$, the pair of values $[m^\\star(\\epsilon,K), \\mathrm{ECE}(\\epsilon,K)]$ based on your derivations. Use the natural logarithm. When $m^\\star(\\epsilon,K)$ diverges, represent it as $+\\infty$.\n\nTest suite of parameter values to cover typical, edge, and boundary cases:\n- Case $1$: $K=10$, $\\epsilon=0.1$.\n- Case $2$: $K=5$, $\\epsilon=0.2$.\n- Case $3$: $K=2$, $\\epsilon=0$.\n- Case $4$: $K=3$, $\\epsilon=0.001$.\n- Case $5$: $K=100$, $\\epsilon=0.9$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of pairs, one pair per test case in the same order as above. Each numeric value must be rounded to six decimal places. If a value is $+\\infty$, print it as the literal string inf without any additional characters. The exact required format is:\n  - $[[m_1,e_1],[m_2,e_2],[m_3,e_3],[m_4,e_4],[m_5,e_5]]$\n  where each $m_i$ and $e_i$ are the margin and calibration values for the $i$-th test case, respectively, for example $[4.394449,0.100000]$.", "solution": "The user-provided problem is evaluated as valid, as it is scientifically grounded in the principles of statistical learning, is well-posed, objective, and internally consistent. We proceed with the derivation and solution.\n\nThe problem asks for the derivation of two quantities related to a $K$-class classifier using label smoothing: the optimal logit margin $m^\\star(\\epsilon,K)$ and the Expected Calibration Error $\\mathrm{ECE}(\\epsilon,K)$.\n\nThe cross-entropy loss between a target probability vector $\\mathbf{y}$ and a predicted probability vector $\\mathbf{p}(\\mathbf{z})$ is given by $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i(\\mathbf{z})$. It is a standard result in information theory that this loss can be decomposed into the entropy of the target distribution and the Kullback-Leibler (KL) divergence between the target and predicted distributions:\n$$\nL(\\mathbf{z};\\mathbf{y}) = H(\\mathbf{y}) + \\mathrm{KL}(\\mathbf{y}\\,\\|\\,\\mathbf{p}(\\mathbf{z}))\n$$\nwhere $H(\\mathbf{y}) = -\\sum_i y_i \\log y_i$ and $\\mathrm{KL}(\\mathbf{y}\\,\\|\\,\\mathbf{p}(\\mathbf{z})) = \\sum_i y_i \\log(y_i/p_i(\\mathbf{z}))$.\n\nIn our case, the target vector is the label-smoothed vector $\\tilde{\\mathbf{y}}$, where for a true class index $t$, the components are:\n$$\n\\tilde{y}_t = 1-\\epsilon\n$$\n$$\n\\tilde{y}_j = \\frac{\\epsilon}{K-1} \\quad \\text{for } j \\neq t\n$$\nThe loss to be minimized is $L(\\mathbf{z};\\tilde{\\mathbf{y}})$. The entropy term $H(\\tilde{\\mathbf{y}})$ does not depend on the model's logits $\\mathbf{z}$, so minimizing the loss $L$ is equivalent to minimizing the KL divergence term $\\mathrm{KL}(\\tilde{\\mathbf{y}}\\,\\|\\,\\mathbf{p}(\\mathbf{z}))$. The KL divergence $\\mathrm{KL}(Q\\|P)$ is non-negative and is minimized to $0$ if and only if the distributions are identical, i.e., $P=Q$. Therefore, the loss is minimized when the model's Softmax output $\\mathbf{p}(\\mathbf{z}^\\star)$ equals the smoothed target distribution $\\tilde{\\mathbf{y}}$.\n$$\n\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}\n$$\n\n**1. Derivation of the Logit Margin $m^\\star(\\epsilon,K)$**\n\nWe start from the optimal condition $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$. The Softmax function is defined as $p_i(\\mathbf{z}) = \\exp(z_i) / \\sum_{j=1}^K \\exp(z_j)$. At the optimum, for each class $i$:\n$$\np_i(\\mathbf{z}^\\star) = \\frac{\\exp(z^\\star_i)}{\\sum_{j=1}^K \\exp(z^\\star_j)} = \\tilde{y}_i\n$$\nTaking the natural logarithm of both sides gives:\n$$\n\\log(p_i(\\mathbf{z}^\\star)) = z^\\star_i - \\log\\left(\\sum_{j=1}^K \\exp(z^\\star_j)\\right) = \\log(\\tilde{y}_i)\n$$\nThis implies that $z^\\star_i = \\log(\\tilde{y}_i) + c$, where $c = \\log(\\sum_{j=1}^K \\exp(z^\\star_j))$ is a constant independent of the class index $i$. This confirms the canonical representation of the optimal logits.\n\nThe logit margin for the true class $t$ is defined as $m^\\star(\\epsilon,K) = z^\\star_t - \\max_{j \\neq t} z^\\star_j$. Using the derived form of the optimal logits:\n$$\nz^\\star_t = \\log(\\tilde{y}_t) + c = \\log(1-\\epsilon) + c\n$$\nFor any incorrect class $j \\neq t$, the logits are all equal:\n$$\nz^\\star_j = \\log(\\tilde{y}_j) + c = \\log\\left(\\frac{\\epsilon}{K-1}\\right) + c\n$$\nThus, $\\max_{j \\neq t} z^\\star_j$ is simply $\\log\\left(\\frac{\\epsilon}{K-1}\\right) + c$.\n\nThe margin $m^\\star(\\epsilon,K)$ is the difference between these logits. The constant $c$ cancels out:\n$$\nm^\\star(\\epsilon,K) = \\left(\\log(1-\\epsilon) + c\\right) - \\left(\\log\\left(\\frac{\\epsilon}{K-1}\\right) + c\\right) = \\log(1-\\epsilon) - \\log\\left(\\frac{\\epsilon}{K-1}\\right)\n$$\nUsing the property of logarithms $\\log a - \\log b = \\log(a/b)$, we obtain the closed-form expression:\n$$\nm^\\star(\\epsilon,K) = \\log\\left(\\frac{1-\\epsilon}{\\epsilon/(K-1)}\\right) = \\log\\left(\\frac{(1-\\epsilon)(K-1)}{\\epsilon}\\right)\n$$\nFor the case where $\\epsilon = 0$, we consider the limit as $\\epsilon \\to 0^+$. Given $K \\ge 2$, the term $(1-\\epsilon)(K-1)$ approaches $K-1 > 0$. The denominator $\\epsilon$ approaches $0$ from the positive side. Therefore, the argument of the logarithm approaches $+\\infty$.\n$$\n\\lim_{\\epsilon \\to 0^+} m^\\star(\\epsilon,K) = \\lim_{\\epsilon \\to 0^+} \\log\\left(\\frac{(1-\\epsilon)(K-1)}{\\epsilon}\\right) = +\\infty\n$$\n\n**2. Derivation of the Expected Calibration Error $\\mathrm{ECE}(\\epsilon,K)$**\n\nThe one-bin Expected Calibration Error is defined as the absolute difference between the average predicted confidence for the true class and the empirical accuracy.\n$$\n\\mathrm{ECE} = \\left| \\text{avg\\_confidence} - \\text{accuracy} \\right|\n$$\nThe problem specifies a noiseless evaluation scenario where:\n- The empirical accuracy is $1$.\n- The model's predictions have converged to the optimal solution under label smoothing, so the output probabilities are given by $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$.\n- The model's top-$1$ prediction is always the true class.\n\nThe \"average predicted confidence for the true class\" requires us to find the probability assigned to the true class for each sample and average these values. In this controlled setting, for any sample whose true class is $t$, the model consistently outputs the probability vector $\\tilde{\\mathbf{y}}$. The probability assigned to the true class is therefore always $\\tilde{y}_t = 1-\\epsilon$. The average of this constant value is simply $1-\\epsilon$.\n$$\n\\text{avg\\_confidence} = 1-\\epsilon\n$$\nThe empirical accuracy is given as $1$.\n$$\n\\text{accuracy} = 1\n$$\nSubstituting these into the ECE formula:\n$$\n\\mathrm{ECE}(\\epsilon,K) = \\left| (1-\\epsilon) - 1 \\right| = \\left| -\\epsilon \\right|\n$$\nSince the label smoothing parameter $\\epsilon$ is in the range $[0,1)$, it is non-negative. Therefore, $|\\!-\\epsilon| = \\epsilon$.\n$$\n\\mathrm{ECE}(\\epsilon,K) = \\epsilon\n$$\nThis result is independent of the number of classes $K$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal logit margin and Expected Calibration Error (ECE)\n    for a K-class classifier with label smoothing parameter epsilon.\n    \"\"\"\n\n    # Test suite of parameter values (K, epsilon)\n    test_cases = [\n        (10, 0.1),\n        (5, 0.2),\n        (2, 0),\n        (3, 0.001),\n        (100, 0.9),\n    ]\n\n    results = []\n    for K, epsilon in test_cases:\n        # 1. Calculate the logit margin m_star\n        # m_star is given by log(((1 - epsilon) * (K - 1)) / epsilon)\n        if epsilon == 0:\n            # The limit as epsilon -> 0+ of the margin is +infinity.\n            m_star = float('inf')\n        else:\n            # This branch handles epsilon > 0. The problem states epsilon in [0, 1).\n            # The context implies K >= 2, so K-1 >= 1.\n            m_star = np.log((1 - epsilon) * (K - 1) / epsilon)\n\n        # 2. Calculate the Expected Calibration Error (ECE)\n        # ECE is given by epsilon in this idealized scenario.\n        ece = epsilon\n\n        # Format the results for the final output string.\n        # If m_star is infinity, print the literal string 'inf'.\n        # Otherwise, round the numerical value to 6 decimal places.\n        if m_star == float('inf'):\n            m_star_str = 'inf'\n        else:\n            m_star_str = f\"{m_star:.6f}\"\n        \n        ece_str = f\"{ece:.6f}\"\n        \n        results.append(f\"[{m_star_str},{ece_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3113811"}]}