{"hands_on_practices": [{"introduction": "To truly understand how weight decay works, we can begin by analyzing its behavior in a simplified, idealized setting. This first practice asks you to determine the \"fixed point\" of the weight vector, which is the value it converges to when the updates no longer cause any change. By deriving this equilibrium under the simplifying assumption of a constant loss gradient, you will uncover the fundamental pull that weight decay exerts on the parameters, providing a clear intuition for its role in the optimization process. [@problem_id:3169493]", "problem": "You are training a single layer of a neural network with parameter vector $w \\in \\mathbb{R}^{d}$ using gradient descent (GD) with weight decay on the penalized objective $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$, where $\\lambda > 0$ is the weight decay coefficient. Assume that across iterations the gradient of the unregularized loss is constant and equal to a fixed vector $g \\in \\mathbb{R}^{d}$, and that the learning rate is a constant $\\eta > 0$. Starting only from the definition of the gradient descent update applied to $J(w)$, and from the fact that the gradient of the squared Euclidean norm is the identity scaled by its coefficient, derive the fixed point $w^{\\star}$ (the value that the iterates converge to if they converge) under the stated assumptions. You may assume $\\eta$ and $\\lambda$ are such that the iteration converges for any initialization. Express your final answer as a closed-form analytic expression in terms of $g$ and $\\lambda$ only. Do not introduce any numerical values.", "solution": "The user has requested the derivation of the fixed point for a gradient descent update with weight decay under specific assumptions. The problem is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- The parameter vector is $w \\in \\mathbb{R}^{d}$.\n- The optimization algorithm is gradient descent (GD).\n- The penalized objective function is $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$.\n- The weight decay coefficient is $\\lambda > 0$.\n- The gradient of the unregularized loss is a constant vector: $\\nabla L(w) = g \\in \\mathbb{R}^{d}$.\n- The learning rate is a constant $\\eta > 0$.\n- The gradient of the regularization term is given by the statement \"the gradient of the squared Euclidean norm is the identity scaled by its coefficient\", which implies $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\lambda w$.\n- It is assumed that the GD iteration converges to a fixed point, $w^{\\star}$, for any initialization.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard theoretical analysis of L2 regularization (weight decay) in machine learning. The assumption that the gradient of the loss function, $\\nabla L(w)$, is a constant vector $g$ is a simplification used to make the analysis tractable. This is a common modeling choice in optimization theory, representing the behavior of the algorithm on a simple quadratic objective or in a local region where the gradient is approximately constant. All provided terms ($\\lambda$, $\\eta$) are well-defined. The objective is to find the fixed point of the iterative process, which is a mathematically well-posed problem. The problem is self-contained, scientifically sound within its theoretical context, and free of contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Fixed Point\nThe process of gradient descent updates the parameter vector $w$ at each iteration $t$ according to the rule:\n$$\nw^{(t+1)} = w^{(t)} - \\eta \\nabla J(w^{(t)})\n$$\nwhere $w^{(t)}$ is the parameter vector at iteration $t$, and $\\eta$ is the learning rate.\n\nThe objective function to be minimized is given as:\n$$\nJ(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}\n$$\nTo apply the update rule, we first need to compute the gradient of $J(w)$ with respect to $w$. Using the linearity of the gradient operator, we have:\n$$\n\\nabla J(w) = \\nabla L(w) + \\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right)\n$$\nThe problem statement provides two key pieces of information for this gradient calculation:\n1. The gradient of the unregularized loss is a constant vector $g$:\n$$\n\\nabla L(w) = g\n$$\n2. The gradient of the regularization term, $\\frac{\\lambda}{2}\\|w\\|^{2}$, is $\\lambda w$. This is consistent with standard vector calculus, as the gradient of the squared Euclidean norm $\\|w\\|^{2} = w^T w$ is $2w$, so $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\frac{\\lambda}{2}(2w) = \\lambda w$.\n\nSubstituting these into the expression for $\\nabla J(w)$, we get:\n$$\n\\nabla J(w) = g + \\lambda w\n$$\nNote that the gradient of the total objective function is not constant, as it depends on the current parameter vector $w$.\n\nNow, we can write the specific gradient descent update rule for this problem by substituting this gradient into the general update equation:\n$$\nw^{(t+1)} = w^{(t)} - \\eta (g + \\lambda w^{(t)})\n$$\nA fixed point of this iteration, denoted as $w^{\\star}$, is a value for which the iteration no longer changes. That is, if $w^{(t)} = w^{\\star}$, then $w^{(t+1)}$ will also be equal to $w^{\\star}$. We find this point by setting $w^{(t+1)} = w^{(t)} = w^{\\star}$ in the update equation:\n$$\nw^{\\star} = w^{\\star} - \\eta (g + \\lambda w^{\\star})\n$$\nWe can now solve this equation for $w^{\\star}$. Subtracting $w^{\\star}$ from both sides gives:\n$$\n0 = - \\eta (g + \\lambda w^{\\star})\n$$\nSince the learning rate $\\eta$ is given as a positive constant ($\\eta > 0$), we can divide both sides by $-\\eta$ without affecting the equality:\n$$\n0 = g + \\lambda w^{\\star}\n$$\nThis equation reveals that the fixed point is the point at which the gradient of the objective function is the zero vector, which is the definition of a critical point of $J(w)$.\n\nTo isolate $w^{\\star}$, we rearrange the equation:\n$$\n\\lambda w^{\\star} = -g\n$$\nFinally, since the weight decay coefficient $\\lambda$ is also given as a positive constant ($\\lambda > 0$), we can divide by $\\lambda$:\n$$\nw^{\\star} = -\\frac{1}{\\lambda} g\n$$\nThis is the closed-form expression for the fixed point of the gradient descent iteration under the stated assumptions. The result depends only on the constant gradient of the loss, $g$, and the weight decay coefficient, $\\lambda$, as required. The learning rate $\\eta$ affects the rate of convergence to this fixed point but not its value. The assumption that the iteration converges ensures that $w^{\\star}$ is the limit of the sequence $\\{w^{(t)}\\}$.", "answer": "$$\n\\boxed{-\\frac{g}{\\lambda}}\n$$", "id": "3169493"}, {"introduction": "Building on our foundational understanding, we now explore a crucial practical aspect of applying weight decay: its interaction with the scale of input data. This exercise demonstrates that the effect of the regularization parameter, $\\lambda$, is not independent of how your features are scaled, meaning the \"optimal\" $\\lambda$ for one scaling may be incorrect for another. By working through this analysis, you will discover why data preprocessing steps like normalization are not just recommended but are often essential for the predictable and effective application of weight decay. [@problem_id:3169486]", "problem": "Consider a single-layer linear neuron (a one-layer neural network) with parameter vector $w \\in \\mathbb{R}^{d}$ that maps inputs $x \\in \\mathbb{R}^{d}$ to predictions $f_{w}(x) = w^{\\top} x$. The network is trained on a dataset $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ by minimizing the empirical risk with Mean Squared Error (MSE) and $\\ell_{2}$ weight decay. Formally, the objective is\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix with rows $x_{i}^{\\top}$, $y \\in \\mathbb{R}^{n}$ is the target vector, $\\lambda > 0$ is the weight decay coefficient, and $\\|\\cdot\\|$ denotes the Euclidean norm. Suppose every input is rescaled by a positive scalar $\\alpha > 0$, so that $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$ for all $i$, and the rescaled design matrix becomes $X' = \\alpha X$. To maintain identical predictions for the original inputs $x$, we simultaneously reparameterize the weights as $w' = \\frac{1}{\\alpha} w$, so that $f_{w'}(x') = f_{w}(x)$ for all $x$.\n\nDefine the rescaled objective\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2},\n$$\nwhere $\\lambda' > 0$ is the new weight decay coefficient applied after the input rescaling. Starting from the fundamental definitions of empirical risk minimization with MSE and $\\ell_{2}$ regularization, determine the expression for $\\lambda'$ in terms of $\\alpha$ and $\\lambda$ that makes the total objective value invariant under the joint transformation $(x_{i}, w, \\lambda) \\mapsto (x_{i}', w', \\lambda')$, in the sense that\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)\n$$\nfor all $w$, $X$, and $y$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\alpha$ and $\\lambda$. No rounding is required.", "solution": "The problem will first be validated according to the established criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Model: A single-layer linear neuron with parameters $w \\in \\mathbb{R}^{d}$ and inputs $x \\in \\mathbb{R}^{d}$.\n-   Prediction function: $f_{w}(x) = w^{\\top} x$.\n-   Dataset: $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$, with design matrix $X \\in \\mathbb{R}^{n \\times d}$ and target vector $y \\in \\mathbb{R}^{n}$.\n-   Original objective function: $J(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}$, where $\\lambda > 0$ and $\\|\\cdot\\|$ is the Euclidean norm.\n-   Input rescaling: $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$ for a positive scalar $\\alpha > 0$. The rescaled design matrix is $X' = \\alpha X$.\n-   Weight reparameterization: $w \\mapsto w' = \\frac{1}{\\alpha} w$. The problem states this ensures $f_{w'}(x') = f_{w}(x)$.\n-   Rescaled objective function: $J(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2}$, where $\\lambda' > 0$.\n-   Invariance condition: Find the expression for $\\lambda'$ such that $J(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)$ holds for all $w$, $X$, and $y$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard exercise in statistical learning, specifically analyzing the properties of ridge regression (linear regression with $\\ell_{2}$ regularization). All concepts are fundamental to machine learning theory.\n-   **Well-Posed:** The problem provides a clear objective and a specific invariance condition. It requests a uniquely determined relationship between the parameters, which can be found through direct algebraic manipulation.\n-   **Objective:** The problem is stated using formal mathematical language and definitions, with no subjective or ambiguous terms.\n-   **Completeness and Consistency:** All necessary variables, functions, and transformations are explicitly defined. There are no contradictions in the setup. The condition $f_{w'}(x') = f_{w}(x)$ is consistent with the definitions of $x'$ and $w'$, as ${w'}^{\\top} x' = (\\frac{1}{\\alpha}w)^{\\top}(\\alpha x) = \\frac{1}{\\alpha} w^{\\top} \\alpha x = w^{\\top} x$.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and internally consistent. The solution process may proceed.\n\n### Solution\n\nThe goal is to determine the expression for $\\lambda'$ in terms of $\\alpha$ and $\\lambda$ that ensures the objective function's value remains invariant under the specified transformations. The invariance condition is:\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(w'; X', y, \\lambda'\\right)\n$$\nwhere $w' = \\frac{1}{\\alpha} w$ and $X'$ is the matrix whose rows are $x_{i}'^{\\top} = (\\alpha x_{i})^{\\top}$.\n\nLet's analyze the rescaled objective function, $J(w'; X', y, \\lambda')$, by substituting the expressions for $x_{i}'$ and $w'$.\nThe rescaled objective is composed of two parts: the Mean Squared Error (MSE) term and the regularization term.\n\nFirst, consider the MSE term:\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2}\n$$\nSubstitute $w' = \\frac{1}{\\alpha} w$ and $x_{i}' = \\alpha x_{i}$:\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w\\right)^{\\top} (\\alpha x_{i})\n$$\nUsing the property of the transpose $(c a)^{\\top} = c a^{\\top}$ for a scalar $c$ and a vector $a$:\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w^{\\top}\\right) (\\alpha x_{i})\n$$\nSince scalar multiplication is commutative and associative:\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} \\cdot \\alpha\\right) (w^{\\top} x_{i}) = 1 \\cdot (w^{\\top} x_{i}) = w^{\\top} x_{i}\n$$\nThis confirms that the prediction for a given data point remains unchanged after the transformation, as stated in the problem.\nTherefore, the MSE term in the rescaled objective is identical to the original MSE term:\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} = \\text{MSE}\n$$\n\nNext, consider the regularization term in the rescaled objective:\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\|w'\\|^{2}\n$$\nSubstitute $w' = \\frac{1}{\\alpha} w$:\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left\\|\\frac{1}{\\alpha} w\\right\\|^{2}\n$$\nUsing the property of the Euclidean norm $\\|c v\\| = |c| \\|v\\|$ for a scalar $c$ and vector $v$:\n$$\n\\left\\|\\frac{1}{\\alpha} w\\right\\|^{2} = \\left(\\left|\\frac{1}{\\alpha}\\right| \\|w\\|\\right)^{2} = \\left(\\frac{1}{\\alpha}\\right)^{2} \\|w\\|^{2} = \\frac{1}{\\alpha^2} \\|w\\|^{2}\n$$\nThe problem states $\\alpha > 0$, so $|\\frac{1}{\\alpha}| = \\frac{1}{\\alpha}$.\nSubstituting this back into the expression for $\\text{Reg}'$:\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left(\\frac{1}{\\alpha^2} \\|w\\|^{2}\\right) = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nNow, we can write the full rescaled objective function in terms of the original variables:\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nThe invariance condition requires this to be equal to the original objective function for all $w$, $X$, and $y$:\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}\n$$\nSetting the two expressions for the total objective equal:\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nThe MSE terms are identical and thus cancel out, leaving the equality for the regularization terms:\n$$\n\\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nThis equality must hold for any parameter vector $w \\in \\mathbb{R}^d$. For any non-trivial case where $w$ is not the zero vector (i.e., $\\|w\\|^2 \\neq 0$), we can divide both sides by $\\frac{1}{2}\\|w\\|^2$:\n$$\n\\lambda = \\frac{\\lambda'}{\\alpha^2}\n$$\nSolving for $\\lambda'$ yields the desired relationship:\n$$\n\\lambda' = \\alpha^2 \\lambda\n$$\nThis result shows that to maintain an equivalent objective function under an input feature scaling of $\\alpha$ and a corresponding weight rescaling of $1/\\alpha$, the weight decay coefficient $\\lambda$ must be scaled by $\\alpha^2$.", "answer": "$$\\boxed{\\alpha^{2} \\lambda}$$", "id": "3169486"}, {"introduction": "Theory truly comes to life when translated into code. In this final practice, you will move from pen-and-paper analysis to hands-on implementation, building a training pipeline from first principles to compare weight decay with another powerful technique: spectral norm regularization. This coding exercise will solidify your understanding of the underlying gradients and also provide an empirical perspective on how different regularization strategies can be used to achieve distinct goals, such as general overfitting control versus improving a model's stability against input perturbations. [@problem_id:3169459]", "problem": "You will implement and compare two regularization strategies in a linear neural network (a single linear layer) trained for multi-output regression: weight decay and spectral norm regularization. The comparison must be grounded in first principles of statistical learning. You must write a complete program that trains two models on several synthetic tasks and outputs which regularizer better controls a specified criterion in each task.\n\nDefinitions and fundamental base:\n- Consider a linear network mapping input vectors to output vectors via a weight matrix. Let the model be parameterized by a weight matrix $W \\in \\mathbb{R}^{k \\times d}$, an input vector $x \\in \\mathbb{R}^{d}$, and a target vector $y \\in \\mathbb{R}^{k}$. The model prediction is $f_{W}(x) = W x$.\n- The empirical mean squared error loss over $n$ samples $\\{(x_i, y_i)\\}_{i=1}^{n}$ is $L_{\\text{MSE}}(W) = \\frac{1}{n k} \\sum_{i=1}^{n} \\lVert W x_i - y_i \\rVert_2^2$.\n- Weight decay (also called $\\ell_2$ regularization) augments the loss with the squared Frobenius norm of $W$, $\\lVert W \\rVert_F^2 = \\sum_{i,j} W_{ij}^2$, scaled by a nonnegative coefficient.\n- Spectral norm regularization augments the loss with the square of the spectral norm of $W$, where the spectral norm is the largest singular value of $W$; denote it by $\\sigma_{\\max}(W)$. The spectral norm equals the operator norm induced by the Euclidean norm and is the Lipschitz constant of the linear mapping $x \\mapsto W x$.\n- Stability of the linear network to input perturbations is captured by the Lipschitz constant of $x \\mapsto W x$, which equals $\\sigma_{\\max}(W)$. Lower spectral norm indicates less amplification of input perturbations.\n- Overfitting control is operationalized by out-of-sample performance: a model that better controls overfitting achieves lower test mean squared error when trained on noisy, high-dimensional data.\n\nYou must implement batch gradient descent from first principles to (approximately) minimize a regularized objective of the form $L_{\\text{MSE}}(W)$ plus a nonnegative regularization term. For each task, you will train two separate models from the same initialization: one with weight decay only and one with spectral norm regularization only. Use a fixed learning rate and a fixed number of iterations for all tasks, and do not use any external libraries beyond those specified in the execution environment. Your program must be fully self-contained and must not read any input.\n\nData generation protocol (to be implemented):\n- Fix a random seed to ensure reproducibility.\n- For given integers $d$ and $k$, construct a ground-truth linear map $B_{\\text{true}} \\in \\mathbb{R}^{k \\times d}$ by sampling random orthonormal matrices and setting prescribed singular values so that $B_{\\text{true}}$ has a controlled singular value spectrum. For an input $x \\sim \\mathcal{N}(0, I_d)$, generate targets by $y = B_{\\text{true}} x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_k)$.\n- Split into $n_{\\text{train}}$ training samples and $n_{\\text{test}}$ test samples.\n\nModel training protocol (to be implemented):\n- Initialize $W \\in \\mathbb{R}^{k \\times d}$ using a small random initialization that is the same for both models in a given task.\n- Train the weight decay model by minimizing $L_{\\text{MSE}}(W) + \\lambda_{\\text{wd}} \\lVert W \\rVert_F^2$ via batch gradient descent.\n- Train the spectral model by minimizing $L_{\\text{MSE}}(W) + \\lambda_{\\text{sn}} \\sigma_{\\max}(W)^2$ via batch gradient descent.\n\nEvaluation metrics and decision rule:\n- For tasks marked as stability-focused, compute the stability metric as $\\sigma_{\\max}(W)$ for each trained model. The spectral norm regularization is considered better on such a task if its trained model achieves strictly smaller $\\sigma_{\\max}(W)$ than the weight decay model (within a small numerical tolerance).\n- For tasks marked as overfitting-focused, compute the test mean squared error for each trained model, defined as $L_{\\text{test}}(W) = \\frac{1}{n_{\\text{test}} k} \\sum_{i=1}^{n_{\\text{test}}} \\lVert W x^{\\text{test}}_i - y^{\\text{test}}_i \\rVert_2^2$. The weight decay regularization is considered better on such a task if its trained model achieves strictly smaller $L_{\\text{test}}(W)$ than the spectral model (within a small numerical tolerance).\n\nTest suite:\nImplement the following three tasks. In each tuple below, the elements are $(n_{\\text{train}}, n_{\\text{test}}, d, k, \\sigma, \\lambda_{\\text{wd}}, \\lambda_{\\text{sn}}, \\text{type}, \\text{singular\\_values})$. The type is either \"stability\" or \"overfit\". The list of singular values defines the leading $m = \\min\\{k, d\\}$ singular values of $B_{\\text{true}}$; if the list is shorter than $m$, pad the remainder with small values equal to the last entry; if it is longer, truncate to length $m$.\n1. $(300, 200, 50, 5, 0.05, 0.001, 0.1, \\text{\"stability\"}, [5.0, 0.2, 0.1, 0.1, 0.05])$.\n2. $(30, 400, 200, 5, 0.5, 0.1, 0.0, \\text{\"overfit\"}, [1.0, 0.8, 0.6, 0.4, 0.2])$.\n3. $(200, 200, 20, 5, 0.1, 0.0, 0.0, \\text{\"stability\"}, [1.0, 0.7, 0.5, 0.3, 0.1])$.\n\nRequired output:\n- For each task, output a single integer decision code:\n  - For a stability-focused task: output $1$ if spectral norm regularization yields strictly smaller $\\sigma_{\\max}(W)$, output $-1$ if weight decay yields strictly smaller $\\sigma_{\\max}(W)$, and output $0$ if they are within a small numerical tolerance.\n  - For an overfitting-focused task: output $1$ if weight decay yields strictly smaller test mean squared error $L_{\\text{test}}(W)$, output $-1$ if spectral norm regularization yields strictly smaller $L_{\\text{test}}(W)$, and output $0$ if they are within a small numerical tolerance.\n\nFinal output format:\nYour program should produce a single line of output containing the three integer decision codes for the tasks, as a comma-separated list enclosed in square brackets (e.g., \"[1,0,-1]\"). There must be no extra text before or after the list.\n\nAngle units are not applicable. There are no physical units. All numerical answers must be produced as dimensionless scalars.\n\nThe program must adhere strictly to the specified execution environment and library limitations.", "solution": "The problem requires the implementation and comparison of two regularization methods, weight decay and spectral norm regularization, for a linear regression model. The validation of the problem statement confirms its scientific and mathematical soundness, completeness, and clarity. The problem is well-posed and provides a rigorous framework for comparing the effects of these regularizers on model stability and overfitting. We proceed with a principled solution.\n\n### 1. Model and Objective Function\n\nThe model is a linear map $f_W(x) = Wx$, where $x \\in \\mathbb{R}^d$ is the input vector, $W \\in \\mathbb{R}^{k \\times d}$ is the weight matrix, and the output is a vector in $\\mathbb{R}^k$.\n\nGiven a training dataset of $n$ samples $\\{(x_i, y_i)\\}_{i=1}^n$, the empirical risk is measured by the Mean Squared Error (MSE) loss:\n$$\nL_{\\text{MSE}}(W) = \\frac{1}{nk} \\sum_{i=1}^{n} \\lVert W x_i - y_i \\rVert_2^2\n$$\nRegularization is incorporated by adding a penalty term $R(W)$ to the loss, scaled by a hyperparameter $\\lambda \\ge 0$. The general regularized objective function is:\n$$\nJ(W) = L_{\\text{MSE}}(W) + \\lambda R(W)\n$$\nTraining the model involves finding the weight matrix $W$ that minimizes this objective function, which is performed using batch gradient descent.\n\n### 2. Batch Gradient Descent\n\nBatch gradient descent is an iterative optimization algorithm that updates the model parameters in the opposite direction of the gradient of the objective function. The update rule for the weight matrix $W$ at iteration $t$ is:\n$$\nW_{t+1} = W_t - \\eta \\nabla_W J(W_t)\n$$\nwhere $\\eta > 0$ is the learning rate and $\\nabla_W J(W)$ is the gradient of the objective function with respect to $W$. The gradient is the sum of the gradient of the MSE loss and the gradient of the regularization term:\n$$\n\\nabla_W J(W) = \\nabla_W L_{\\text{MSE}}(W) + \\lambda \\nabla_W R(W)\n$$\n\n### 3. Gradient Derivations\n\nTo implement gradient descent, we must derive the gradients for the MSE loss and for each of the two regularization terms.\n\n**3.1. Gradient of the MSE Loss**\n\nLet the training data be represented by matrices $X \\in \\mathbb{R}^{d \\times n}$ (where columns are input vectors $x_i$) and $Y \\in \\mathbb{R}^{k \\times n}$ (where columns are target vectors $y_i$). The MSE loss can be written in matrix form as:\n$$\nL_{\\text{MSE}}(W) = \\frac{1}{nk} \\lVert WX - Y \\rVert_F^2 = \\frac{1}{nk} \\text{Tr}((WX-Y)(WX-Y)^T)\n$$\nUsing standard matrix calculus identities, the gradient of the MSE loss with respect to $W$ is:\n$$\n\\nabla_W L_{\\text{MSE}}(W) = \\frac{2}{nk} (WX - Y)X^T\n$$\nThe dimensions of this gradient are $(k \\times n) \\times (n \\times d) = k \\times d$, matching the dimensions of $W$.\n\n**3.2. Gradient of Weight Decay (L2 Regularization)**\n\nWeight decay uses the squared Frobenius norm of $W$ as the penalty term:\n$$\nR_{\\text{wd}}(W) = \\lVert W \\rVert_F^2 = \\sum_{i=1}^k \\sum_{j=1}^d W_{ij}^2\n$$\nThe gradient is straightforward to compute:\n$$\n\\nabla_W R_{\\text{wd}}(W) = 2W\n$$\nThe objective function for the weight decay model is $L_{\\text{wd}}(W) = L_{\\text{MSE}}(W) + \\lambda_{\\text{wd}} \\lVert W \\rVert_F^2$, and its gradient is:\n$$\n\\nabla_W L_{\\text{wd}}(W) = \\frac{2}{nk} (WX - Y)X^T + 2\\lambda_{\\text{wd}}W\n$$\n\n**3.3. Gradient of Spectral Norm Regularization**\n\nSpectral norm regularization penalizes the squared spectral norm of $W$, which is the square of its largest singular value, $\\sigma_{\\max}(W)$.\n$$\nR_{\\text{sn}}(W) = \\sigma_{\\max}(W)^2\n$$\nThe spectral norm $\\sigma_{\\max}(W)$ is the solution to the variational problem $\\sigma_{\\max}(W) = \\max_{\\lVert u \\rVert_2=1, \\lVert v \\rVert_2=1} u^T W v$. Let the Singular Value Decomposition (SVD) of $W$ be $W = U \\Sigma V^T$. Let $\\sigma_1 = \\sigma_{\\max}(W)$ be the largest singular value. If $\\sigma_1$ is unique, the gradient of the spectral norm is given by $\\nabla_W \\sigma_{\\max}(W) = u_1 v_1^T$, where $u_1$ and $v_1$ are the left and right singular vectors corresponding to $\\sigma_1$.\n\nUsing the chain rule, the gradient of the squared spectral norm is:\n$$\n\\nabla_W R_{\\text{sn}}(W) = \\nabla_W (\\sigma_{\\max}(W)^2) = 2\\sigma_{\\max}(W) \\nabla_W \\sigma_{\\max}(W) = 2\\sigma_{\\max}(W) u_1 v_1^T\n$$\nThe objective function for the spectral norm model is $L_{\\text{sn}}(W) = L_{\\text{MSE}}(W) + \\lambda_{\\text{sn}} \\sigma_{\\max}(W)^2$, and its gradient is:\n$$\n\\nabla_W L_{\\text{sn}}(W) = \\frac{2}{nk} (WX - Y)X^T + 2\\lambda_{\\text{sn}}\\sigma_{\\max}(W) u_1 v_1^T\n$$\nThis gradient exists wherever the largest singular value is unique. In practice, this covers almost all matrices, and this formula can be used as the gradient in our descent algorithm.\n\n### 4. Conceptual Comparison and Implementation Strategy\n\n- **Weight Decay** isotropically shrinks all elements of the weight matrix $W$. By penalizing the sum of squared weights, it encourages solutions where energy is distributed across many small weights rather than concentrated in a few large ones. This is a general-purpose regularizer effective at preventing overfitting by limiting model complexity.\n\n- **Spectral Norm Regularization** anisotropically targets the largest singular value of $W$. Since $\\sigma_{\\max}(W)$ is the Lipschitz constant of the linear map $x \\mapsto Wx$, this regularizer directly controls the model's sensitivity to perturbations in the input. A model with a smaller spectral norm is more stable. This is particularly useful when robustness to input noise is a primary concern.\n\nThe implementation will proceed as follows for each task:\n1.  **Data Generation**: Construct a ground-truth matrix $B_{\\text{true}}$ with prescribed singular values and generate noisy training and test data according to $y = B_{\\text{true}}x + \\varepsilon$.\n2.  **Initialization**: Initialize a weight matrix $W_0$ with small random values.\n3.  **Training**: Train two models, one for weight decay and one for spectral norm regularization, starting from the identical $W_0$ and using batch gradient descent for a fixed number of iterations with a fixed learning rate.\n4.  **Evaluation**: Based on the task type (\"stability\" or \"overfit\"), compute the specified metric ($\\sigma_{\\max}(W)$ or test MSE) for both trained models and apply the decision rule to determine which regularizer performed better.\n\nThis structured approach ensures a fair comparison under identical conditions, allowing for an empirical analysis of the distinct effects of each regularization strategy.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import ortho_group\n\n# Global parameters for reproducibility and training\nSEED = 42\nLEARNING_RATE = 0.01\nN_ITERATIONS = 1000\nTOLERANCE = 1e-9\n\ndef generate_data(rng, n_train, n_test, d, k, sigma, singular_values):\n    \"\"\"\n    Generates synthetic data for a linear regression task.\n    \"\"\"\n    m = min(k, d)\n    s_full = np.zeros(m)\n    len_sv = len(singular_values)\n    if len_sv > 0:\n        s_full[:min(len_sv, m)] = singular_values[:m]\n        if len_sv  m:\n            s_full[len_sv:] = singular_values[-1]\n    \n    Sigma_diag = np.zeros((k, d))\n    np.fill_diagonal(Sigma_diag, s_full)\n\n    U = ortho_group.rvs(dim=k, random_state=rng)\n    V = ortho_group.rvs(dim=d, random_state=rng)\n    \n    B_true = U @ Sigma_diag @ V.T\n\n    n_total = n_train + n_test\n    X = rng.standard_normal(size=(d, n_total))\n    epsilon = rng.standard_normal(size=(k, n_total)) * sigma\n    Y = B_true @ X + epsilon\n\n    X_train, X_test = X[:, :n_train], X[:, n_train:]\n    Y_train, Y_test = Y[:, :n_train], Y[:, n_train:]\n\n    return X_train, Y_train, X_test, Y_test\n\ndef train_model(X_train, Y_train, k, d, reg_type, lambda_reg, W_init):\n    \"\"\"\n    Trains a linear model with specified regularization using batch gradient descent.\n    \"\"\"\n    n = X_train.shape[1]\n    W = np.copy(W_init)\n\n    for _ in range(N_ITERATIONS):\n        # MSE gradient\n        Y_hat = W @ X_train\n        grad_mse = (2 / (n * k)) * (Y_hat - Y_train) @ X_train.T\n\n        # Regularization gradient\n        grad_reg = np.zeros_like(W)\n        if reg_type == 'wd' and lambda_reg > 0:\n            grad_reg = 2 * lambda_reg * W\n        elif reg_type == 'sn' and lambda_reg > 0:\n            # Need SVD for spectral norm gradient\n            try:\n                U, s, Vh = np.linalg.svd(W, full_matrices=False)\n                # Gradient of L_2 norm is u_1 v_1^T, where u_1, v_1 are top singular vectors\n                # Gradient of squared L_2 norm is 2 * sigma_max * u_1 * v_1^T\n                grad_reg = 2 * lambda_reg * s[0] * np.outer(U[:, 0], Vh[0, :])\n            except np.linalg.LinAlgError:\n                # Handle cases where SVD might fail, though unlikely for dense matrices\n                pass\n\n        # Gradient update\n        W -= LEARNING_RATE * (grad_mse + grad_reg)\n    \n    return W\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        (300, 200, 50, 5, 0.05, 0.001, 0.1, \"stability\", [5.0, 0.2, 0.1, 0.1, 0.05]),\n        (30, 400, 200, 5, 0.5, 0.1, 0.0, \"overfit\", [1.0, 0.8, 0.6, 0.4, 0.2]),\n        (200, 200, 20, 5, 0.1, 0.0, 0.0, \"stability\", [1.0, 0.7, 0.5, 0.3, 0.1]),\n    ]\n\n    results = []\n\n    for i, case in enumerate(test_cases):\n        n_train, n_test, d, k, sigma, lambda_wd, lambda_sn, task_type, sv_list = case\n        \n        # Use a new RNG for each task to ensure task independence but reproducibility\n        rng = np.random.default_rng(SEED + i)\n        \n        X_train, Y_train, X_test, Y_test = generate_data(rng, n_train, n_test, d, k, sigma, sv_list)\n        \n        # Same random initialization for both models in a task\n        init_rng = np.random.default_rng(SEED + i + 100) # Different seed for init\n        W_init = init_rng.standard_normal(size=(k, d)) * 0.01\n\n        # Train weight decay model\n        W_wd = train_model(X_train, Y_train, k, d, 'wd', lambda_wd, W_init)\n\n        # Train spectral norm model\n        W_sn = train_model(X_train, Y_train, k, d, 'sn', lambda_sn, W_init)\n\n        if task_type == \"stability\":\n            sigma_max_wd = np.linalg.svd(W_wd, compute_uv=False)[0]\n            sigma_max_sn = np.linalg.svd(W_sn, compute_uv=False)[0]\n            \n            if sigma_max_sn  sigma_max_wd - TOLERANCE:\n                results.append(1)\n            elif sigma_max_wd  sigma_max_sn - TOLERANCE:\n                results.append(-1)\n            else:\n                results.append(0)\n\n        elif task_type == \"overfit\":\n            mse_wd = (1 / (n_test * k)) * np.sum(np.square(W_wd @ X_test - Y_test))\n            mse_sn = (1 / (n_test * k)) * np.sum(np.square(W_sn @ X_test - Y_test))\n\n            if mse_wd  mse_sn - TOLERANCE:\n                results.append(1)\n            elif mse_sn  mse_wd - TOLERANCE:\n                results.append(-1)\n            else:\n                results.append(0)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3169459"}]}