{"hands_on_practices": [{"introduction": "The foundational principle that makes Convolutional Neural Networks so effective for spatial data like images is translation equivariance. This property ensures that if you shift an object in the input, the network's representation of that object shifts by the same amount in the output. This practice guides you through analytically and numerically verifying this core concept, demonstrating how composing operations like circular convolution and pointwise nonlinearities preserves equivariance, and critically, how it can be broken by common but non-ideal boundary conditions [@problem_id:3126241].", "problem": "You are tasked with designing and verifying translation-equivariant spatial prediction networks using fundamental properties of discrete convolution, translation operators on finite grids, and pointwise nonlinearities. Translation equivariance means that for a function $f$ acting on two-dimensional arrays and a translation operator $T_{\\delta}$, the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ holds for all valid inputs $x$ and translations $\\delta$. The grids are finite with periodic boundary conditions unless explicitly stated otherwise.\n\nFoundational base and definitions to be used:\n- A two-dimensional discrete signal is a function $x: \\{0,\\dots,H-1\\} \\times \\{0,\\dots,W-1\\} \\to \\mathbb{R}$, represented as an array of shape $H \\times W$.\n- The circular translation operator $T_{\\delta}$ with $\\delta = (\\delta_y,\\delta_x)$ acts as $(T_{\\delta} x)[i,j] = x[(i-\\delta_y) \\bmod H, (j-\\delta_x) \\bmod W]$.\n- A two-dimensional discrete cross-correlation with kernel $k$ of odd size $k_h \\times k_w$ centered at $(\\lfloor k_h/2 \\rfloor,\\lfloor k_w/2 \\rfloor)$ is given, under circular boundary conditions, by\n  $$ (x \\star_{\\mathrm{circ}} k)[i,j] = \\sum_{m=0}^{k_h-1}\\sum_{n=0}^{k_w-1} x\\big((i+m-c_h) \\bmod H, (j+n-c_w) \\bmod W\\big)\\, k[m,n], $$\n  where $c_h = \\lfloor k_h/2 \\rfloor$ and $c_w = \\lfloor k_w/2 \\rfloor$.\n- A pointwise nonlinearity $\\sigma$ acts independently per spatial site, for example $\\sigma(u) = \\max(u,0)$ (rectified linear unit).\n- A constant bias $b \\in \\mathbb{R}$ added uniformly over space yields $y[i,j] = u[i,j] + b$.\n\nTasks:\n- Design a small family of networks composed from circular cross-correlation layers, constant biases, and pointwise nonlinearities, each producing an $H \\times W$ spatial map. For one specified counterexample, use zero-padding (non-circular) cross-correlation to illustrate a failure case.\n- For each network, verify analytically (by reasoning from the definitions) for which boundary condition the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ holds for all $x$ and $\\delta$, and for which it fails.\n- Implement a program that numerically tests the equivariance relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ using the maximum absolute difference criterion with tolerance $\\epsilon$, and reports, for each test case, a boolean indicating whether the property holds within tolerance.\n\nNumerical details and constraints:\n- Use arrays of shape $H \\times W$ with $H = 16$ and $W = 16$.\n- All kernels must have odd spatial dimensions.\n- For circular layers, use circular cross-correlation as defined above. For the one non-circular case, use zero-padding cross-correlation that treats values outside the domain as zero and returns an array of the same shape.\n- Use the rectified linear unit $\\sigma(u) = \\max(u,0)$ wherever a nonlinearity is requested.\n- Use tolerance $\\epsilon = 10^{-9}$ to decide equality via the condition $\\max_{i,j} |A[i,j] - B[i,j]| \\le \\epsilon$.\n- Randomness must be reproducible. For each test case, initialize a pseudorandom number generator with the provided seed $s$ and draw all entries of inputs and kernels independently from a standard normal distribution.\n\nTest suite:\nEach test case specifies $(H,W,\\delta,\\text{architecture},\\text{kernel sizes},\\text{seed})$. Implement the following six cases and evaluate the property $f(T_{\\delta} x) = T_{\\delta} f(x)$.\n\n- Case A (happy path, single layer):\n  - $H = 16$, $W = 16$, $\\delta = (3,-5)$, architecture: one circular cross-correlation layer with kernel size $3 \\times 3$, add a constant bias, followed by a rectified linear unit, seed $s = 0$.\n- Case B (composition of layers):\n  - $H = 16$, $W = 16$, $\\delta = (7,2)$, architecture: two circular cross-correlation layers, both with kernel size $3 \\times 3$, each with its own constant bias, with a rectified linear unit applied after the first layer and after the second layer, seed $s = 1$.\n- Case C (failure under zero padding):\n  - $H = 16$, $W = 16$, $\\delta = (5,4)$, architecture: one zero-padding cross-correlation layer (non-circular) with kernel size $3 \\times 3$, add a constant bias, followed by a rectified linear unit, seed $s = 2$.\n- Case D (bias invariance, large translation):\n  - $H = 16$, $W = 16$, $\\delta = (15,-16)$, architecture: one circular cross-correlation layer with kernel size $3 \\times 3$, add a nonzero constant bias, followed by a rectified linear unit, seed $s = 3$.\n- Case E (pointwise linear head):\n  - $H = 16$, $W = 16$, $\\delta = (1,1)$, architecture: one circular cross-correlation layer with kernel size $3 \\times 3$, followed by a rectified linear unit, followed by a pointwise linear layer implemented as a $1 \\times 1$ circular cross-correlation with its own bias, seed $s = 4$.\n- Case F (dilated circular layer):\n  - $H = 16$, $W = 16$, $\\delta = (2,-3)$, architecture: one circular cross-correlation layer whose kernel is constructed by inserting zeros into a base $3 \\times 3$ kernel with dilation factor $d = 2$, yielding an effective kernel size $5 \\times 5$, add a constant bias, followed by a rectified linear unit, seed $s = 5$.\n\nProgram input and output:\n- There is no external input. Your program must internally construct the six test cases as specified, generate the random inputs and kernels using the provided seeds, evaluate the equivariance relation for each case with tolerance $\\epsilon = 10^{-9}$, and aggregate the six boolean results into a single line of output formatted as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{False},\\dots]$ with exactly six entries and no extra whitespace.", "solution": "The problem requires an analysis of translation equivariance for several small neural network architectures built from standard components. A function $f$ is translation-equivariant if, for any valid input signal $x$ and any translation vector $\\delta$, the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ holds. Here, $T_{\\delta}$ is the circular translation operator defined on a finite two-dimensional grid of size $H \\times W$. The grid is defined by indices $\\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\}$.\n\nThe translation operator $T_{\\delta}$ with $\\delta = (\\delta_y, \\delta_x)$ is defined as $(T_{\\delta} x)[i,j] = x[(i-\\delta_y) \\bmod H, (j-\\delta_x) \\bmod W]$. This operation shifts the content of the signal $x$ by the vector $\\delta$, with periodic boundary conditions.\n\nA network $f$ is constructed as a composition of primitive operations, $f = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1$. The property of translation equivariance is preserved under composition. If two functions $g$ and $h$ are individually translation-equivariant, their composition $g \\circ h$ is also translation-equivariant. This can be shown as follows:\n$$ (g \\circ h)(T_{\\delta} x) = g(h(T_{\\delta} x)) $$\nBy the equivariance of $h$, $h(T_{\\delta} x) = T_{\\delta} h(x)$. Substituting this gives:\n$$ g(T_{\\delta} h(x)) $$\nBy the equivariance of $g$, applied to the input $y = h(x)$:\n$$ g(T_{\\delta} y) = T_{\\delta} g(y) \\implies g(T_{\\delta} h(x)) = T_{\\delta} g(h(x)) = T_{\\delta} (g \\circ h)(x) $$\nThus, we have $(g \\circ h)(T_{\\delta} x) = T_{\\delta} (g \\circ h)(x)$. To determine if a network $f$ is translation-equivariant, we must analyze each of its constituent layers.\n\nThe building blocks are:\n1.  Circular cross-correlation\n2.  Addition of a constant bias\n3.  Pointwise nonlinearity\n4.  Zero-padding cross-correlation (for one counterexample)\n\nLet's analyze each component for translation equivariance.\n\n1.  **Circular Cross-Correlation ($C_k$)**:\nLet the operation be $y = x \\star_{\\mathrm{circ}} k$. The output is given by:\n$$ y[i,j] = (x \\star_{\\mathrm{circ}} k)[i,j] = \\sum_{m=0}^{k_h-1}\\sum_{n=0}^{k_w-1} x\\big((i+m-c_h) \\bmod H, (j+n-c_w) \\bmod W\\big)\\, k[m,n] $$\nwhere $c_h = \\lfloor k_h/2 \\rfloor$ and $c_w = \\lfloor k_w/2 \\rfloor$.\nTo test for equivariance, we compare $T_{\\delta}(x \\star_{\\mathrm{circ}} k)$ with $(T_{\\delta} x) \\star_{\\mathrm{circ}} k$.\n\nFirst, let's compute the translated output, $(T_{\\delta} y)[i,j]$:\n$$ (T_{\\delta} y)[i,j] = y[(i-\\delta_y) \\bmod H, (j-\\delta_x) \\bmod W] $$\nSubstituting the definition of $y$:\n$$ (T_{\\delta} y)[i,j] = \\sum_{m, n} x\\Big(\\big(((i-\\delta_y) \\bmod H) + m-c_h\\big) \\bmod H, \\big(((j-\\delta_x) \\bmod W) + n-c_w\\big) \\bmod W\\Big) \\, k[m,n] $$\nDue to the properties of modular arithmetic, this simplifies to:\n$$ (T_{\\delta} y)[i,j] = \\sum_{m, n} x\\big((i-\\delta_y+m-c_h) \\bmod H, (j-\\delta_x+n-c_w) \\bmod W\\big) \\, k[m,n] $$\n\nNext, let's compute the output for a translated input, $y' = (T_{\\delta} x) \\star_{\\mathrm{circ}} k$:\n$$ y'[i,j] = \\sum_{m,n} (T_{\\delta} x)\\big((i+m-c_h) \\bmod H, (j+n-c_w) \\bmod W\\big) \\, k[m,n] $$\nUsing the definition of $T_{\\delta}$, $(T_{\\delta} x)[a,b] = x[(a-\\delta_y)\\bmod H, (b-\\delta_x)\\bmod W]$, we get:\n$$ y'[i,j] = \\sum_{m,n} x\\Big(\\big((i+m-c_h)\\bmod H - \\delta_y\\big) \\bmod H, \\big((j+n-c_w)\\bmod W - \\delta_x\\big) \\bmod W\\Big) \\, k[m,n] $$\nThis again simplifies to:\n$$ y'[i,j] = \\sum_{m,n} x\\big((i+m-c_h-\\delta_y) \\bmod H, (j+n-c_w-\\delta_x) \\bmod W\\big) \\, k[m,n] $$\nThe expressions for $(T_{\\delta} y)[i,j]$ and $y'[i,j]$ are identical. Therefore, circular cross-correlation is a translation-equivariant operation. This holds regardless of the kernel values or size, including $1 \\times 1$ kernels and dilated kernels, as long as the boundary condition is circular.\n\n2.  **Constant Bias Addition ($B_b$)**:\nLet the operation be $y[i,j] = u[i,j] + b$.\nThe translated output is $(T_{\\delta} y)[i,j] = y[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] = u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] + b$.\nThe output for a translated input is $(T_{\\delta} u + b)[i,j] = (T_{\\delta} u)[i,j] + b = u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] + b$.\nThe two expressions are identical. Thus, adding a constant bias is a translation-equivariant operation.\n\n3.  **Pointwise Nonlinearity ($\\Sigma$)**:\nLet the operation be $y[i,j] = \\sigma(u[i,j])$, where $\\sigma$ is a function applied to each element independently.\nThe translated output is $(T_{\\delta} y)[i,j] = y[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] = \\sigma\\big(u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W]\\big)$.\nThe output for a translated input is $\\sigma(T_{\\delta} u)[i,j] = \\sigma\\big((T_{\\delta} u)[i,j]\\big) = \\sigma\\big(u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W]\\big)$.\nThe expressions are identical. Any pointwise operation, including the rectified linear unit (ReLU), is translation-equivariant.\n\n4.  **Zero-Padding Cross-Correlation ($C'_k$)**:\nThis operation is defined similarly to circular cross-correlation, but inputs from outside the grid $\\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\}$ are treated as zero. This breaks the periodic symmetry of the grid. Consider an input signal $x$ that is zero everywhere except for a single point near a boundary, for instance $x[0,0]=1$. The output $y = C'_k(x)$ will have a pattern determined by the kernel $k$. Now consider a translated input $x' = T_\\delta x$. If $\\delta$ is such that the non-zero element is moved away from the boundary into the interior of the grid, the cross-correlation will be computed using neighboring signal values (which are zero in this example), not padded zeros. The resulting output pattern $y' = C'_k(x')$ will be identical in shape to $y$. However, if we compute the translation of the original output, $T_\\delta y$, the result will be a simple shift of the original output pattern, including any artifacts caused by the boundary. These two results, $y'$ and $T_\\delta y$, will not match in general because the padding interacts differently with the signal depending on its position relative to the boundary. Thus, cross-correlation with zero-padding is not translation-equivariant.\n\n**Analysis of Test Cases:**\n\n-   **Case A**: `circ_corr` $\\to$ `bias` $\\to$ `relu`. This is a composition of three translation-equivariant operations under circular boundary conditions. The resulting network is therefore **equivariant**.\n-   **Case B**: `(circ_corr -> bias -> relu)` $\\to$ `(circ_corr -> bias -> relu)`. This is a composition of two blocks, each of which is equivariant as established in Case A. The composition of equivariant functions is equivariant. The network is **equivariant**.\n-   **Case C**: `zero_pad_corr` $\\to$ `bias` $\\to$ `relu`. This network starts with a zero-padding cross-correlation layer, which is not translation-equivariant. The presence of a single non-equivariant layer makes the entire network **not equivariant**.\n-   **Case D**: `circ_corr` $\\to$ `bias` $\\to$ `relu`. This is the same architecture as Case A. The specific values of the translation vector $\\delta = (15,-16) \\equiv (-1,0) \\pmod{16}$ and the non-zero bias do not alter the fundamental property of equivariance. The network is **equivariant**.\n-   **Case E**: `circ_corr(3x3)` $\\to$ `relu` $\\to$ `circ_corr(1x1)` $\\to$ `bias`. A $1 \\times 1$ cross-correlation is a pointwise scaling, which is a translation-equivariant operation. All other components are also equivariant. The network is a composition of equivariant blocks and is therefore **equivariant**.\n-   **Case F**: `dilated_circ_corr` $\\to$ `bias` $\\to$ `relu`. A dilated circular cross-correlation is a specific instance of circular cross-correlation where the kernel has a sparse structure of inserted zeros. The proof of equivariance for circular cross-correlation is independent of the kernel's specific values, only depending on the circular summation. Thus, this layer is also equivariant. The network is **equivariant**.\n\nThe expected boolean results for the test suite are: [True, True, False, True, True, True].", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by numerically testing the translation equivariance\n    of different network architectures specified in the test cases.\n    \"\"\"\n\n    # --- Helper Functions for Network Layers ---\n\n    def circular_translation(x, delta):\n        \"\"\"Applies circular translation T_delta to a 2D array x.\"\"\"\n        # delta = (delta_y, delta_x)\n        # np.roll shifts array elements. A shift of (dy, dx) moves the\n        # element at (r, c) to (r+dy, c+dx).\n        # We want (T_delta x)[i,j] = x[i-delta_y, j-delta_x].\n        # This corresponds to shifting the content by delta.\n        return np.roll(x, shift=delta, axis=(0, 1))\n\n    def circular_cross_correlation(x, k):\n        \"\"\"Performs circular cross-correlation.\"\"\"\n        return correlate2d(x, k, mode='same', boundary='wrap')\n\n    def zero_padding_cross_correlation(x, k):\n        \"\"\"Performs zero-padding cross-correlation.\"\"\"\n        return correlate2d(x, k, mode='same', boundary='fill', fillvalue=0)\n\n    def relu(x):\n        \"\"\"Applies the Rectified Linear Unit pointwise.\"\"\"\n        return np.maximum(x, 0)\n    \n    def create_dilated_kernel(base_kernel, dilation_factor):\n        \"\"\"Creates a dilated kernel from a base kernel.\"\"\"\n        kh_base, kw_base = base_kernel.shape\n        d = dilation_factor\n        \n        # Effective kernel size calculation\n        kh_dil = kh_base + (kh_base - 1) * (d - 1)\n        kw_dil = kw_base + (kw_base - 1) * (d - 1)\n        \n        dilated_kernel = np.zeros((kh_dil, kw_dil))\n        \n        for i in range(kh_base):\n            for j in range(kw_base):\n                dilated_kernel[i * d, j * d] = base_kernel[i, j]\n                \n        return dilated_kernel\n\n    # --- Test Case Definitions ---\n    \n    test_cases = [\n        # Case A: (H, W, delta, architecture, kernel_sizes, seed)\n        (16, 16, (3, -5), 'A', [(3, 3)], 0),\n        # Case B\n        (16, 16, (7, 2), 'B', [(3, 3), (3, 3)], 1),\n        # Case C\n        (16, 16, (5, 4), 'C', [(3, 3)], 2),\n        # Case D\n        (16, 16, (15, -16), 'D', [(3, 3)], 3),\n        # Case E\n        (16, 16, (1, 1), 'E', [(3, 3), (1, 1)], 4),\n        # Case F\n        (16, 16, (2, -3), 'F', [(3, 3)], 5),\n    ]\n\n    results = []\n    epsilon = 1e-9\n\n    for case in test_cases:\n        H, W, delta, arch, kernel_sizes, seed = case\n        \n        # Initialize RNG for reproducibility for each case\n        np.random.seed(seed)\n        \n        # Generate random input\n        x = np.random.standard_normal((H, W))\n\n        # Define the network function f(x) for the current architecture\n        if arch == 'A':\n            k = np.random.standard_normal(kernel_sizes[0])\n            b = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(z, k) + b)\n        \n        elif arch == 'B':\n            k1 = np.random.standard_normal(kernel_sizes[0])\n            b1 = np.random.standard_normal()\n            k2 = np.random.standard_normal(kernel_sizes[1])\n            b2 = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(relu(circular_cross_correlation(z, k1) + b1), k2) + b2)\n            \n        elif arch == 'C':\n            k = np.random.standard_normal(kernel_sizes[0])\n            b = np.random.standard_normal()\n            f = lambda z: relu(zero_padding_cross_correlation(z, k) + b)\n            \n        elif arch == 'D':\n            k = np.random.standard_normal(kernel_sizes[0])\n            b = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(z, k) + b)\n\n        elif arch == 'E':\n            k1 = np.random.standard_normal(kernel_sizes[0])\n            k2 = np.random.standard_normal(kernel_sizes[1])\n            b2 = np.random.standard_normal()\n            f = lambda z: circular_cross_correlation(relu(circular_cross_correlation(z, k1)), k2) + b2\n\n        elif arch == 'F':\n            k_base = np.random.standard_normal(kernel_sizes[0])\n            k_dilated = create_dilated_kernel(k_base, dilation_factor=2)\n            b = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(z, k_dilated) + b)\n            \n        # --- Equivariance Test ---\n        # LHS: f(T_delta(x))\n        x_translated = circular_translation(x, delta)\n        lhs = f(x_translated)\n\n        # RHS: T_delta(f(x))\n        fx = f(x)\n        rhs = circular_translation(fx, delta)\n        \n        # Compare LHS and RHS\n        max_abs_diff = np.max(np.abs(lhs - rhs))\n        is_equivariant = max_abs_diff = epsilon\n        results.append(is_equivariant)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3126241"}, {"introduction": "While ideal convolutions are perfectly equivariant, real-world applications operate on finite grids, forcing us to make choices about how to handle boundaries. This practice moves from the ideal theory to the messy reality, revealing that padding is not merely a mechanical detail but a critical choice that can impact model behavior and robustness. You will act as an adversary, constructing inputs where simply changing the padding scheme—from zero-padding to reflection-padding—is enough to flip the network's final classification, providing a stark illustration of how boundary effects can be exploited [@problem_id:3126196].", "problem": "You are given a directive to construct adversarial inputs that exploit boundary conditions in Convolutional Neural Networks (CNNs). Begin from the foundational definitions of two-dimensional discrete convolution, translation, and receptive fields, then reason about how boundary conditions affect translation equivariance and effective receptive fields near image edges.\n\nFundamental base:\n- Define the two-dimensional discrete convolution of a finite image signal $x \\in \\mathbb{R}^{H \\times W}$ with a finite kernel $k \\in \\mathbb{R}^{m \\times n}$ on an infinite lattice as\n$$\n(y \\ast k)[i,j] \\triangleq \\sum_{u=0}^{m-1}\\sum_{v=0}^{n-1} k[u,v]\\; y[i+u-\\lfloor m/2 \\rfloor, j+v-\\lfloor n/2 \\rfloor],\n$$\nwhere $y$ is an extension of $x$ by a boundary condition. The extension $y$ is defined by a padding scheme, for example, zero padding or reflection padding. The unpadded case computes only where indices are valid. The receptive field of an output position is the subset of the input that influences that output via the convolution sum. The translation operator $T_{\\Delta i, \\Delta j}$ acts on $x$ by $(T_{\\Delta i, \\Delta j}x)[i,j] = x[i-\\Delta i,j-\\Delta j]$. In the ideal infinite-lattice case without boundary truncation, convolution is translation equivariant, meaning $(T_{\\Delta i, \\Delta j}(x) \\ast k) = T_{\\Delta i, \\Delta j}(x \\ast k)$.\n\nTask:\n- Implement a single-layer Convolutional Neural Network (CNN) with one $3 \\times 3$ convolutional kernel $K$ whose entries are all $1$, identity nonlinearity, and global average pooling that produces a scalar score $s$. Explicitly, for an output feature map $z$, define the pooled score\n$$\ns \\triangleq \\frac{1}{|z|}\\sum_{i,j} z[i,j],\n$$\nwhere $|z|$ is the number of elements in the feature map. Use a binary classifier that outputs the label $1$ if $s \\ge \\tau$ and $0$ otherwise, with the threshold fixed to $\\tau = 1.13$.\n\n- Construct input images $x \\in \\mathbb{R}^{8 \\times 8}$ with background intensity $0$ and a single bright square feature of intensity $1$ and size $3 \\times 3$ placed at specified coordinates $(i_0,j_0)$, meaning\n$$\nx[i,j] = \\begin{cases}\n1  \\text{if } i_0 \\le i \\le i_0+2 \\text{ and } j_0 \\le j \\le j_0+2,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n\n- Evaluate the network under three boundary conditions:\n    1. Same convolution with zero padding (\"same\\_zero\"), where the image is padded by a $1$-pixel border of zeros before convolution and the output size is $8 \\times 8$.\n    2. Same convolution with reflection padding (\"same\\_reflect\"), where the image is padded by a $1$-pixel reflection before convolution and the output size is $8 \\times 8$.\n    3. Valid convolution (\"valid\"), where no padding is applied and only positions where the $3 \\times 3$ kernel fully fits inside the image are convolved, resulting in an output size of $6 \\times 6$.\n\n- For each test case, compute two scores $s_A$ and $s_B$ using two specified boundary conditions $A$ and $B$, classify with the threshold $\\tau$, and return:\n    - A boolean indicating whether the predicted class under $A$ differs from the predicted class under $B$.\n    - The margin difference $s_B - s_A$ as a float.\n\nTest suite:\n- Use the following parameter values, each test case encoded as $(H, k, i_0, j_0, A, B)$:\n    1. $(8, 3, 2, 2, \\text{same\\_zero}, \\text{same\\_reflect})$ — feature near the center (happy path).\n    2. $(8, 3, 2, 5, \\text{same\\_zero}, \\text{same\\_reflect})$ — feature touching the right edge.\n    3. $(8, 3, 0, 0, \\text{valid}, \\text{same\\_reflect})$ — feature at the top-left corner, comparing valid versus reflective padding.\n    4. $(8, 3, 5, 2, \\text{valid}, \\text{same\\_zero})$ — feature touching the bottom edge, comparing valid versus zero padding.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list in the form $[\\text{flip}, \\text{margin}]$. For example, the output must look like\n$$\n[[\\text{bool}_1,\\text{float}_1],[\\text{bool}_2,\\text{float}_2],[\\text{bool}_3,\\text{float}_3],[\\text{bool}_4,\\text{float}_4]].\n$$\nNo physical units or angles are involved. All numeric outputs must be plain booleans and floats. The program must be self-contained and require no input.", "solution": "The problem asks us to analyze and implement a simple one-layer CNN to demonstrate how different padding schemes affect the final classification output. The network consists of a $3 \\times 3$ convolution with a kernel of all ones, followed by global average pooling, and then classification based on a threshold $\\tau = 1.13$.\n\nWe will analyze each test case by computing the output score for the two specified padding schemes, $A$ and $B$.\n\n**Core Components:**\n- **Input Image `x`**: An $8 \\times 8$ matrix with a $3 \\times 3$ block of ones on a zero background. The sum of pixel values in `x` is 9.\n- **Kernel `K`**: A $3 \\times 3$ matrix of ones. The sum of kernel weights is 9.\n- **Convolution `z`**: The output feature map. Its values are the sum of the input in a $3 \\times 3$ window. For this problem, we use cross-correlation, which is equivalent to convolution since the kernel is symmetric.\n- **Score `s`**: The mean of the feature map `z`. $s = \\text{mean}(z) = \\text{sum}(z) / |z|$, where $|z|$ is the number of elements in `z`.\n- **Classification**: Output is 1 if $s \\ge 1.13$, and 0 otherwise.\n\n**Analysis of Test Cases:**\n\n-   **Case 1:** Feature at $(2, 2)$, center of the image. $A=\\text{same\\_zero}$, $B=\\text{same\\_reflect}$.\n    - Since the feature is far from the boundaries, both zero and reflection padding produce the same padded input in the region of the feature.\n    - The output feature map `z` is $8 \\times 8$ for both. The sum of `z` is the sum of the input `x` (9) times the sum of the kernel `K` (9), which is 81.\n    - $s_A = s_B = 81 / (8 \\times 8) = 81/64 = 1.265625$.\n    - Class A: $1.265625 \\ge 1.13 \\implies 1$. Class B: $1.265625 \\ge 1.13 \\implies 1$.\n    - **Result:** Flip: `False`, Margin: $s_B - s_A = 0.0$.\n\n-   **Case 2:** Feature at $(2, 5)$, touching the right edge. $A=\\text{same\\_zero}$, $B=\\text{same\\_reflect}$.\n    - **Path A (`same_zero`)**: Padding with zeros. The convolution only sees the 9 ones from the original feature. $\\text{sum}(z_A) = 81$. $s_A = 81/64 = 1.265625$. Class A = 1.\n    - **Path B (`same_reflect`)**: The right edge at column 7 is reflected. This adds a reflected column of 3 ones to the padded image. The convolution now operates over an effective input containing 12 ones. The total sum of the output map $z_B$ is 108.\n    - $s_B = 108 / 64 = 1.6875$. Class B = 1.\n    - **Result:** Flip: `False`, Margin: $1.6875 - 1.265625 = 0.421875$.\n\n-   **Case 3:** Feature at $(0, 0)$, at the top-left corner. $A=\\text{valid}$, $B=\\text{same\\_reflect}$.\n    - **Path A (`valid`)**: No padding. The output `z_A` is $6 \\times 6$. We can calculate the sum of `z_A` by noting how many times the kernel overlaps the feature. The sum is 36.\n    - $s_A = 36 / (6 \\times 6) = 36/36 = 1.0$.\n    - Class A: $1.0  1.13 \\implies 0$.\n    - **Path B (`same_reflect`)**: Reflection padding at the corner creates an effective $4 \\times 4$ block of 16 ones in the padded input. The sum of the resulting $8 \\times 8$ output map $z_B$ is 81.\n    - $s_B = 81 / 64 = 1.265625$.\n    - Class B: $1.265625 \\ge 1.13 \\implies 1$.\n    - **Result:** Flip: `True`, Margin: $1.265625 - 1.0 = 0.265625$.\n\n-   **Case 4:** Feature at $(5, 2)$, touching the bottom edge. $A=\\text{valid}$, $B=\\text{same\\_zero}$.\n    - **Path A (`valid`)**: No padding, output `z_A` is $6 \\times 6$. The feature is at `x[5:8, 2:5]`. Only the last row of kernel positions (`i=5`) will overlap with the feature. The sum of the output map $z_A$ is 27.\n    - $s_A = 27 / (6 \\times 6) = 27/36 = 0.75$.\n    - Class A: $0.75  1.13 \\implies 0$.\n    - **Path B (`same_zero`)**: Padding with zeros. The output `z_B` is $8 \\times 8$. The sum of the output map is 81.\n    - $s_B = 81 / 64 = 1.265625$.\n    - Class B: $1.265625 \\ge 1.13 \\implies 1$.\n    - **Result:** Flip: `True`, Margin: $1.265625 - 0.75 = 0.515625$.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef cnn_forward_pass(x, padding_mode, kernel):\n    \"\"\"\n    Performs a single forward pass of the simple CNN for a given padding mode.\n\n    Args:\n        x (np.ndarray): The input image.\n        padding_mode (str): The padding mode ('same_zero', 'same_reflect', 'valid').\n        kernel (np.ndarray): The convolutional kernel.\n\n    Returns:\n        float: The scalar score 's'.\n    \"\"\"\n    if padding_mode == 'same_zero':\n        # Pad with a 1-pixel border of zeros. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='constant', constant_values=0)\n        # Correlate to get 8x8 output.\n        # (10x10 input corr 3x3) -> (10-3+1)x(10-3+1) = 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'same_reflect':\n        # Pad with a 1-pixel reflection. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='reflect')\n        # Correlate to get 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'valid':\n        # No padding. Output size will be 6x6.\n        # (8x8 input corr 3x3) -> (8-3+1)x(8-3+1) = 6x6 output.\n        z = correlate2d(x, kernel, mode='valid')\n    else:\n        raise ValueError(f\"Unknown padding mode: {padding_mode}\")\n\n    # Global Average Pooling\n    score = np.mean(z)\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and generate the final output.\n    \"\"\"\n    # Problem constants\n    H, W = 8, 8\n    kernel_size = 3\n    feature_size = 3\n    tau = 1.13\n    kernel = np.ones((kernel_size, kernel_size))\n\n    # Test suite: (i0, j0, mode_A, mode_B)\n    test_cases = [\n        (2, 2, 'same_zero', 'same_reflect'),\n        (2, 5, 'same_zero', 'same_reflect'),\n        (0, 0, 'valid', 'same_reflect'),\n        (5, 2, 'valid', 'same_zero')\n    ]\n\n    results = []\n    for i0, j0, mode_A, mode_B in test_cases:\n        # 1. Construct input image x\n        x = np.zeros((H, W), dtype=np.float64)\n        x[i0 : i0 + feature_size, j0 : j0 + feature_size] = 1.0\n\n        # 2. Evaluate for condition A\n        s_A = cnn_forward_pass(x, mode_A, kernel)\n        class_A = 1 if s_A >= tau else 0\n\n        # 3. Evaluate for condition B\n        s_B = cnn_forward_pass(x, mode_B, kernel)\n        class_B = 1 if s_B >= tau else 0\n\n        # 4. Compare results\n        classification_flipped = (class_A != class_B)\n        margin_difference = s_B - s_A\n\n        results.append([classification_flipped, margin_difference])\n\n    # Format the final output string exactly as specified.\n    # [ [bool_1,float_1], [bool_2,float_2], ... ]\n    # We build the string manually to avoid spaces introduced by str(list).\n    inner_strings = []\n    for flip, margin in results:\n        # Python's str(bool) is 'True'/'False'. The problem states \"plain booleans\".\n        # This is the most direct interpretation.\n        inner_strings.append(f\"[{str(flip).lower()},{margin}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3126196"}, {"introduction": "After understanding the \"why\" and \"how\" of convolutional layers, a practical deep learning engineer must also consider the \"how fast\". This practice shifts focus to the computational engine of CNNs, exploring the algorithmic efficiency of the convolution operation itself. You will perform a head-to-head comparison, deriving the computational cost (in Multiply-Accumulate operations, or MACs) for both direct spatial convolution and a more complex but often faster method leveraging the Fast Fourier Transform (FFT), revealing the trade-offs that deep learning libraries navigate to achieve high performance [@problem_id:3126261].", "problem": "Consider a two-dimensional convolutional layer in a Convolutional Neural Network (CNN) that maps an input tensor of spatial size $H \\times W$ with $c_{\\text{in}}$ input channels to an output tensor of the same spatial size $H \\times W$ with $c_{\\text{out}}$ output channels, using stride $1$ and zero-padding so that the output has the same spatial dimensions as the input. Each output channel is formed by convolving all $c_{\\text{in}}$ input channels with its corresponding set of $c_{\\text{in}}$ kernels of spatial size $k \\times k$ and summing the results across channels.\n\nUsing the foundational definition of discrete convolution and the convolution theorem for the Discrete Fourier Transform (DFT), perform the following:\n\nFirst, derive from first principles the total number of real multiply-accumulate operations (MACs) required to compute the layer by direct spatial-domain convolution as a function of $H$, $W$, $c_{\\text{in}}$, $c_{\\text{out}}$, and $k$.\n\nSecond, derive the total number of real MACs required by an approach that uses the Fast Fourier Transform (FFT) to perform convolution in the frequency domain with the following scientifically grounded cost model based on the Cooley–Tukey algorithm and separable two-dimensional transforms:\n- A single two-dimensional complex FFT of size $H \\times W$ requires approximately $\\frac{5}{2} H W \\left(\\log_{2} H + \\log_{2} W\\right)$ real MACs.\n- A single two-dimensional complex inverse FFT (IFFT) of size $H \\times W$ requires the same number of real MACs, $\\frac{5}{2} H W \\left(\\log_{2} H + \\log_{2} W\\right)$.\n- A single pointwise complex multiplication costs $3$ real MACs (counting $4$ real multiplications and $2$ real additions as $6$ real floating-point operations, i.e., $3$ MACs).\n- A single complex addition costs $1$ real MAC (counting $2$ real additions as $2$ real floating-point operations, i.e., $1$ MAC).\n\nAssume the FFT-based method pads each $k \\times k$ kernel to $H \\times W$ before transforming and proceeds by computing $c_{\\text{in}}$ FFTs of the input channels, $c_{\\text{in}} c_{\\text{out}}$ FFTs of the padded kernels, performing pointwise complex multiply-accumulate across the $c_{\\text{in}}$ input channels for each of the $c_{\\text{out}}$ outputs at every frequency bin, and then computing $c_{\\text{out}}$ inverse FFTs to return to the spatial domain.\n\nFinally, for typical intermediate CNN feature map dimensions $H = 128$ and $W = 128$, and channel counts $c_{\\text{in}} = 64$ and $c_{\\text{out}} = 128$, solve for the smallest kernel size $k$ such that the FFT-based method has equal total real MACs to the direct spatial method. Express your final numerical answer for $k$ rounded to four significant figures. No physical units are required for $k$.", "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of digital signal processing and computational complexity analysis as applied to convolutional neural networks. The problem is well-posed, providing all necessary definitions, constants, and a clear objective. The language is precise and unambiguous.\n\nWe will now proceed with the derivation. The problem asks for two derivations of the total number of real multiply-accumulate (MAC) operations for a convolutional layer, followed by a calculation to find the break-even kernel size.\n\n**Part 1: MACs for Direct Spatial-Domain Convolution**\n\nThe direct convolution operation computes each element of the output tensor by performing a dot product between a kernel and the corresponding receptive field in the input tensor.\n\nLet the input tensor have dimensions $H \\times W \\times c_{\\text{in}}$ and the output tensor have dimensions $H \\times W \\times c_{\\text{out}}$. The kernel size is $k \\times k$.\n\nTo compute a single value at a specific spatial location $(i,j)$ in one output channel, we must convolve a $k \\times k$ region of each of the $c_{\\text{in}}$ input channels with a corresponding $k \\times k$ kernel. This operation is then summed across all $c_{\\text{in}}$ channels.\n\nFor one output value, the number of multiplications is the size of the kernel's spatial dimensions multiplied by the number of input channels, which is $k \\times k \\times c_{\\text{in}} = k^2 c_{\\text{in}}$. Since these products are summed (accumulated), this corresponds to $k^2 c_{\\text{in}}$ MAC operations.\n\nThis calculation must be repeated for every spatial location in the output feature map. Since the output spatial dimensions are $H \\times W$, the number of MACs to produce one output channel is $(H \\times W) \\times (k^2 c_{\\text{in}})$.\n\nFinally, there are $c_{\\text{out}}$ output channels, each computed independently with its own set of kernels. Therefore, the total number of real MACs for the direct spatial convolution, denoted $N_{\\text{direct}}$, is:\n$$N_{\\text{direct}} = c_{\\text{out}} \\times (H \\cdot W \\cdot k^2 \\cdot c_{\\text{in}})$$\n$$N_{\\text{direct}} = H W c_{\\text{in}} c_{\\text{out}} k^2$$\n\n**Part 2: MACs for FFT-Based Frequency-Domain Convolution**\n\nThe FFT-based approach leverages the convolution theorem, which states that convolution in the spatial domain is equivalent to element-wise multiplication in the frequency domain. We will calculate the total MACs by summing the costs of the steps defined in the problem.\n\nIt is noted that performing convolution via multiplication in the frequency domain of size $H \\times W$ corresponds to a circular convolution. The calculation of linear convolution with 'same' padding, as implied by the direct method, would necessitate padding both the input and filters to a larger size (e.g., $(H+k-1) \\times (W+k-1)$). However, the problem explicitly prescribes the use of $H \\times W$ transforms for this analysis. Therefore, the following derivation strictly adheres to the specified algorithm and its associated costs.\n\nLet the cost of a single $H \\times W$ 2D complex FFT or IFFT be $C_{\\text{FFT}} = \\frac{5}{2} H W (\\log_{2} H + \\log_{2} W)$.\n\n1.  **FFT of Input Channels**: There are $c_{\\text{in}}$ input channels, each of size $H \\times W$. We perform one 2D FFT per channel.\n    Cost: $C_1 = c_{\\text{in}} \\cdot C_{\\text{FFT}} = c_{\\text{in}} \\frac{5}{2} H W (\\log_{2} H + \\log_{2} W)$.\n\n2.  **FFT of Padded Kernels**: There are $c_{\\text{in}} \\times c_{\\text{out}}$ kernels in total, each of size $k \\times k$. Each is padded to $H \\times W$ and transformed.\n    Cost: $C_2 = c_{\\text{in}} c_{\\text{out}} \\cdot C_{\\text{FFT}} = c_{\\text{in}} c_{\\text{out}} \\frac{5}{2} H W (\\log_{2} H + \\log_{2} W)$.\n\n3.  **Pointwise Multiply-Accumulate**: For each of the $c_{\\text{out}}$ output channels, we must combine the $c_{\\text{in}}$ transformed input channels with the corresponding $c_{\\text{in}}$ transformed kernels. For each of the $H \\times W$ frequency bins, this operation is $\\hat{O}_m = \\sum_{l=1}^{c_{\\text{in}}} \\hat{I}_l \\odot \\hat{K}_{l,m}$, where $\\odot$ is element-wise multiplication.\n    For a single frequency bin and a single output channel, this requires $c_{\\text{in}}$ complex multiplications and $c_{\\text{in}}-1$ complex additions.\n    Using the specified cost model:\n    -   Cost of complex multiplications: $c_{\\text{in}} \\times (3 \\text{ real MACs}) = 3c_{\\text{in}}$ real MACs.\n    -   Cost of complex additions: $(c_{\\text{in}}-1) \\times (1 \\text{ real MAC}) = c_{\\text{in}}-1$ real MACs.\n    Total cost per frequency bin, per output channel: $3c_{\\text{in}} + (c_{\\text{in}}-1) = 4c_{\\text{in}}-1$ real MACs.\n    This must be done for all $H \\times W$ frequency bins and for all $c_{\\text{out}}$ output channels.\n    Cost: $C_3 = H \\cdot W \\cdot c_{\\text{out}} \\cdot (4c_{\\text{in}}-1)$.\n\n4.  **Inverse FFT of Outputs**: We have $c_{\\text{out}}$ frequency-domain output maps, which must be transformed back to the spatial domain.\n    Cost: $C_4 = c_{\\text{out}} \\cdot C_{\\text{FFT}} = c_{\\text{out}} \\frac{5}{2} H W (\\log_{2} H + \\log_{2} W)$.\n\nThe total number of real MACs for the FFT-based method, $N_{\\text{FFT}}$, is the sum of these costs:\n$N_{\\text{FFT}} = C_1 + C_2 + C_3 + C_4$.\nLet $L = \\log_2 H + \\log_2 W$.\n$$N_{\\text{FFT}} = c_{\\text{in}} \\frac{5}{2} H W L + c_{\\text{in}} c_{\\text{out}} \\frac{5}{2} H W L + H W c_{\\text{out}} (4c_{\\text{in}}-1) + c_{\\text{out}} \\frac{5}{2} H W L$$\nFactoring out $H W$:\n$$N_{\\text{FFT}} = H W \\left[ \\frac{5}{2} L (c_{\\text{in}} + c_{\\text{in}}c_{\\text{out}} + c_{\\text{out}}) + c_{\\text{out}}(4c_{\\text{in}}-1) \\right]$$\n\n**Part 3: Solving for the Break-Even Kernel Size $k$**\n\nWe are asked to find the kernel size $k$ for which the two methods require an equal number of MACs, i.e., $N_{\\text{direct}} = N_{\\text{FFT}}$.\n$$H W c_{\\text{in}} c_{\\text{out}} k^2 = H W \\left[ \\frac{5}{2} L (c_{\\text{in}} + c_{\\text{in}}c_{\\text{out}} + c_{\\text{out}}) + c_{\\text{out}}(4c_{\\text{in}}-1) \\right]$$\nWe can cancel the $H W$ term from both sides:\n$$c_{\\text{in}} c_{\\text{out}} k^2 = \\frac{5}{2} L (c_{\\text{in}} + c_{\\text{in}}c_{\\text{out}} + c_{\\text{out}}) + 4c_{\\text{in}}c_{\\text{out}} - c_{\\text{out}}$$\nSolving for $k^2$:\n$$k^2 = \\frac{1}{c_{\\text{in}} c_{\\text{out}}} \\left[ \\frac{5}{2} L (c_{\\textin}} + c_{\\text{in}}c_{\\text{out}} + c_{\\text{out}}) + 4c_{\\text{in}}c_{\\text{out}} - c_{\\text{out}} \\right]$$\n$$k^2 = \\frac{5L}{2} \\left( \\frac{c_{\\text{in}}}{c_{\\text{in}} c_{\\text{out}}} + \\frac{c_{\\text{in}}c_{\\text{out}}}{c_{\\text{in}} c_{\\text{out}}} + \\frac{c_{\\text{out}}}{c_{\\text{in}} c_{\\text{out}}} \\right) + \\frac{4c_{\\text{in}}c_{\\text{out}}}{c_{\\text{in}} c_{\\text{out}}} - \\frac{c_{\\text{out}}}{c_{\\text{in}} c_{\\text{out}}}$$\n$$k^2 = \\frac{5L}{2} \\left( \\frac{1}{c_{\\text{out}}} + 1 + \\frac{1}{c_{\\text{in}}} \\right) + 4 - \\frac{1}{c_{\\text{in}}}$$\n\nNow, we substitute the given numerical values: $H = 128$, $W = 128$, $c_{\\text{in}} = 64$, $c_{\\text{out}} = 128$.\nFirst, calculate $L$:\n$$L = \\log_2(128) + \\log_2(128) = 7 + 7 = 14$$\nSubstitute all values into the expression for $k^2$:\n$$k^2 = \\frac{5(14)}{2} \\left( 1 + \\frac{1}{64} + \\frac{1}{128} \\right) + 4 - \\frac{1}{64}$$\n$$k^2 = 35 \\left( \\frac{128}{128} + \\frac{2}{128} + \\frac{1}{128} \\right) + 4 - \\frac{2}{128}$$\n$$k^2 = 35 \\left( \\frac{131}{128} \\right) + 4 - \\frac{2}{128}$$\n$$k^2 = \\frac{4585}{128} + \\frac{512}{128} - \\frac{2}{128}$$\n$$k^2 = \\frac{4585 + 512 - 2}{128} = \\frac{5095}{128}$$\nNow, we calculate the numerical value of $k^2$:\n$$k^2 = 39.8046875$$\nFinally, we solve for $k$:\n$$k = \\sqrt{39.8046875} \\approx 6.3090956...$$\nRounding to four significant figures, we get $k \\approx 6.309$.", "answer": "$$\\boxed{6.309}$$", "id": "3126261"}]}