{"hands_on_practices": [{"introduction": "The choice of activation function is a fundamental design decision in any neural network. This exercise provides a concrete, visual understanding of how this choice dictates the very texture of the function an MLP can learn, contrasting the sharp, piecewise-linear world of the Rectified Linear Unit (ReLU) with the smooth landscapes created by modern activations like the Sigmoid Linear Unit (SiLU) and Gaussian Error Linear Unit (GELU). Through this practice [@problem_id:3151225], you will quantify these differences and build intuition for why one might be preferred over another in different modeling scenarios.", "problem": "You are given a family of two-layer feedforward multilayer perceptrons (MLPs) with identical weights and biases but differing activation functions. The network maps from a two-dimensional input to a scalar output. The purpose is to compare how the choice of smooth activation functions, specifically the Sigmoid Linear Unit (SiLU) and the Gaussian Error Linear Unit (GELU), versus the Rectified Linear Unit (ReLU), affects two measurable properties: an empirical count of linear regions along a one-dimensional slice, and a finite-difference smoothness measure of the decision function over a two-dimensional grid.\n\nFundamental base\nUse the standard feedforward composition for an MLP with one hidden layer of width $h$, input $x \\in \\mathbb{R}^{2}$, weight matrices $W_{1} \\in \\mathbb{R}^{h \\times 2}$ and $W_{2} \\in \\mathbb{R}^{1 \\times h}$, and biases $b_{1} \\in \\mathbb{R}^{h}$ and $b_{2} \\in \\mathbb{R}$. The scalar output is\n$$\nf(x) \\;=\\; W_{2}\\,\\phi(W_{1}x + b_{1}) \\;+\\; b_{2},\n$$\nwhere $\\phi$ is an element-wise activation function. Consider the following three activation functions:\n- Rectified Linear Unit (ReLU): $\\phi(z) = \\max\\{0,z\\}$,\n- Sigmoid Linear Unit (SiLU): $\\phi(z) = z \\cdot \\sigma(z)$ with logistic sigmoid $\\sigma(z) = \\frac{1}{1+e^{-z}}$,\n- Gaussian Error Linear Unit (GELU): use the standard smooth approximation\n$$\n\\phi(z) \\;=\\; \\tfrac{1}{2}\\,z\\left(1 + \\tanh\\!\\big(\\sqrt{\\tfrac{2}{\\pi}}\\,(z + 0.044715\\,z^{3})\\big)\\right).\n$$\n\nArchitecture and parameters\nFix the hidden width to $h=10$. Define the weights and biases deterministically using trigonometric functions in radians (angles in radians are required):\n- For indices $i \\in \\{0,1,\\dots,9\\}$ and $j \\in \\{0,1\\}$,\n$$\n(W_{1})_{i,j} \\;=\\; 0.7\\,\\sin(i + 2j) \\;+\\; 0.05\\,\\cos(3i - j),\n$$\n- For $i \\in \\{0,1,\\dots,9\\}$,\n$$\n(b_{1})_{i} \\;=\\; 0.1\\,\\sin(i),\n$$\n- For $k \\in \\{0,1,\\dots,9\\}$,\n$$\n(W_{2})_{0,k} \\;=\\; 0.6\\,\\cos(k) \\;+\\; 0.05\\,\\sin(2k),\n$$\n- The output bias is $b_{2}=0$.\n\nTwo empirical measures\n1) Empirical one-dimensional linear region count. Consider the diagonal line slice $t \\mapsto x(t) = (t,t)$ for $t \\in [-2,2]$. Sample $N=1025$ equally spaced points $t_{0},t_{1},\\dots,t_{N-1}$ on $[-2,2]$, and evaluate $y_{\\ell}=f(x(t_{\\ell}))$. Let first differences be $s_{\\ell} = y_{\\ell+1}-y_{\\ell}$ for $\\ell \\in \\{0,1,\\dots,N-2\\}$ and second differences be $d_{\\ell} = s_{\\ell+1}-s_{\\ell}$ for $\\ell \\in \\{0,1,\\dots,N-3\\}$. Define a kink threshold $\\tau = 10^{-2}$. The empirical linear region count is\n$$\nR \\;=\\; \\sum_{\\ell=0}^{N-3} \\mathbf{1}\\big(|d_{\\ell}| > \\tau\\big),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator that equals $1$ if the condition is true and $0$ otherwise.\n\n2) Empirical two-dimensional smoothness by finite-difference gradient variation. On the square domain $[-2,2]\\times[-2,2]$, construct a uniform grid with $G=81$ points per axis, spacing $\\Delta = \\frac{4}{G-1}$. Evaluate $f$ on all grid points to obtain a scalar field $F[u,v]$ for $u,v \\in \\{0,1,\\dots,G-1\\}$. Approximate the gradient at interior points via central differences:\n$$\n\\partial_{x}F[u,v] \\approx \\frac{F[u,v+1] - F[u,v-1]}{2\\Delta}, \\quad\n\\partial_{y}F[u,v] \\approx \\frac{F[u+1,v] - F[u-1,v]}{2\\Delta},\n$$\nfor $u,v \\in \\{1,2,\\dots,G-2\\}$. Let $g[u,v] \\in \\mathbb{R}^{2}$ denote this approximate gradient on the interior. Define the average neighbor gradient variation\n$$\nS \\;=\\; \\frac{1}{E}\\left(\\sum_{u=1}^{G-2}\\sum_{v=1}^{G-3} \\left\\| g[u,v+1]-g[u,v]\\right\\|_{2}\n\\;+\\; \\sum_{u=1}^{G-3}\\sum_{v=1}^{G-2} \\left\\| g[u+1,v]-g[u,v]\\right\\|_{2}\\right),\n$$\nwhere $E = 2\\,(G-2)\\,(G-3)$ is the total number of interior horizontal and vertical neighbor pairs and $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. The scalar $S$ serves as a discrete smoothness proxy: smaller values indicate smoother variation of the decision function's gradient across the plane.\n\nTask\nImplement a program that constructs the deterministic weights and biases as above, instantiates the MLP for each of the three activations $\\mathrm{ReLU}$, $\\mathrm{SiLU}$, and $\\mathrm{GELU}$, and computes the pair $(R,S)$ for each activation, where $R$ is the empirical one-dimensional linear region count and $S$ is the empirical smoothness measure defined above.\n\nTest suite\nUse the following activation set:\n- Case $1$: $\\mathrm{ReLU}$,\n- Case $2$: $\\mathrm{SiLU}$,\n- Case $3$: $\\mathrm{GELU}$.\n\nAnswer specification and output format\n- For each case, compute $R$ and $S$ as defined. Report $R$ as an integer and $S$ rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated flat list in the order $[R_{\\mathrm{ReLU}},S_{\\mathrm{ReLU}},R_{\\mathrm{SiLU}},S_{\\mathrm{SiLU}},R_{\\mathrm{GELU}},S_{\\mathrm{GELU}}]$, enclosed in square brackets. For example, a valid output format is like $[12,0.037591,0,0.004112,0,0.003987]$.\n- No external input is provided; all parameters are fixed as stated above.", "solution": "The user wants to analyze and compare the properties of a two-layer Multilayer Perceptron (MLP) when using three different activation functions: the Rectified Linear Unit (ReLU), the Sigmoid Linear Unit (SiLU), and the Gaussian Error Linear Unit (GELU). The comparison is based on two empirical metrics: one measuring the number of \"kinks\" (non-linearities) along a 1D slice of the function's output, and another measuring the smoothness of the function's gradient over a 2D domain.\n\n### **Problem Validation**\n\nFirst, a validation of the problem statement is performed.\n\n**Step 1: Extract Givens**\n\n- **MLP Model**: $f(x) \\;=\\; W_{2}\\,\\phi(W_{1}x + b_{1}) \\;+\\; b_{2}$ for $x \\in \\mathbb{R}^{2}$.\n- **Architecture**: Hidden layer width $h=10$.\n- **Activation Functions ($\\phi$)**: ReLU ($\\max\\{0,z\\}$), SiLU ($z \\cdot \\sigma(z)$), and an approximation of GELU ($\\tfrac{1}{2}\\,z\\left(1 + \\tanh(\\sqrt{\\tfrac{2}{\\pi}}\\,(z + 0.044715\\,z^{3}))\\right)$).\n- **Parameters**: $W_{1} \\in \\mathbb{R}^{10 \\times 2}$, $b_{1} \\in \\mathbb{R}^{10}$, $W_{2} \\in \\mathbb{R}^{1 \\times 10}$, and $b_{2}=0$. The values are deterministically defined by specific trigonometric formulas.\n- **Metric 1 (Linear Region Count, $R$)**:\n    - Function slice: $x(t) = (t,t)$ for $t \\in [-2,2]$.\n    - Sampling: $N=1025$ points.\n    - Calculation: $R = \\sum \\mathbf{1}(|d_{\\ell}| > \\tau)$ where $d_{\\ell}$ are the second differences of the function values on the slice and $\\tau = 10^{-2}$ is a threshold.\n- **Metric 2 (Smoothness, $S$)**:\n    - Domain: $[-2,2]\\times[-2,2]$.\n    - Grid: $G=81$ points per axis.\n    - Calculation: $S$ is the average norm of the difference between gradients at adjacent interior points on the grid, computed via central differences. The formula is explicitly provided.\n- **Task**: For each of the three activation functions, compute the pair $(R,S)$.\n- **Output**: A flat, comma-separated list of the computed values $[R_{\\mathrm{ReLU}},S_{\\mathrm{ReLU}},R_{\\mathrm{SiLU}},S_{\\mathrm{SiLU}},R_{\\mathrm{GELU}},S_{\\mathrm{GELU}}]$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded in the domain of neural networks and numerical analysis. All functions, parameters, and procedures are defined with mathematical precision. The deterministic nature of the problem ensures it is well-posed, leading to a single, verifiable solution. The terminology is objective and unambiguous. The problem does not violate any of the invalidity criteria. For instance, it is not scientifically unsound, incomplete, or ill-posed.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A solution will be provided.\n\n### **Principle-Based Solution Design**\n\nThe core of the problem is to implement the MLP function $f(x)$ and then use it to compute the two metrics, $R$ and $S$, for each specified activation function.\n\n**1. Model Implementation**\n\nThe MLP is defined by $f(x) = W_{2} \\phi(W_{1}x + b_{1}) + b_{2}$. This can be implemented as a sequence of matrix-vector operations. The weights and biases are generated first, following the given deterministic formulas involving sine and cosine functions. It is critical to ensure that the angles are treated as being in radians, which is the default for standard numerical libraries like NumPy.\n\nThe three activation functions, ReLU, SiLU, and GELU, are implemented according to their mathematical definitions. ReLU is a simple maximum operation. SiLU involves the logistic sigmoid function $\\sigma(z) = (1+e^{-z})^{-1}$. GELU is given as a specific smooth approximation involving the hyperbolic tangent function.\n\nA generic MLP model function is constructed. It takes a set of input vectors $x$ and an activation function $\\phi$ and computes the corresponding outputs. This is designed to handle batches of inputs efficiently, for instance, all points on the 1D slice or the 2D grid simultaneously. For an input matrix $X$ of shape $(\\text{num\\_points}, 2)$, the computation proceeds as:\n1.  Compute pre-activations: $Z = X W_{1}^T + b_{1}$. Broadcasting handles the addition of the bias vector $b_1$.\n2.  Apply activation: $A = \\phi(Z)$. This is an element-wise operation.\n3.  Compute final output: $Y = A W_{2}^T + b_{2}$.\n\n**2. Metric R: Empirical Linear Region Count**\nThe problem defines this metric on a 1D slice $x(t)=(t,t)$ of the input space.\n1.  A set of $N=1025$ equidistant points $t_{\\ell}$ is generated in the interval $[-2,2]$. These form the input vectors $x(t_{\\ell})$.\n2.  The MLP model $f$ is evaluated at these points to get the outputs $y_{\\ell}$.\n3.  The first differences $s_{\\ell} = y_{\\ell+1} - y_{\\ell}$ and second differences $d_{\\ell} = s_{\\ell+1} - s_{\\ell}$ are computed. The second difference is a discrete approximation of the second derivative. For a piecewise linear function like that produced by a ReLU network, the second derivative is zero everywhere except at the \"kinks,\" where it is undefined (a Dirac delta). The numerical second difference will exhibit a spike at these points.\n4.  The number of kinks $R$ is counted by summing up the instances where the absolute value of the second difference $|d_{\\ell}|$ exceeds a threshold $\\tau = 10^{-2}$. For the smooth functions (SiLU, GELU), the second derivative is well-defined and continuous, so the second differences are expected to be small and not cross the threshold, leading to $R=0$. For ReLU, a positive count is expected.\n\n**3. Metric S: Empirical Smoothness**\nThis metric quantifies the smoothness of the function over a 2D grid.\n1.  A uniform grid of $G \\times G$ points (with $G=81$) is created over the square domain $[-2,2] \\times [-2,2]$. The MLP $f$ is evaluated at every point on this grid, yielding a scalar field $F[u,v]$.\n2.  The gradient of $f$ is approximated at each interior grid point using the central difference formula. This provides a vector field $g[u,v] \\approx \\nabla f$.\n3.  The smoothness measure $S$ is calculated as the average variation of this gradient field between adjacent points. This involves computing the Euclidean norm of the difference between gradient vectors at neighboring grid points, both horizontally and vertically.\n4.  The sum of these norm differences is normalized by the total number of pairs considered, $E = 2(G-2)(G-3)$. A smaller value of $S$ indicates that the gradient changes more slowly, implying a smoother function. We expect ReLU, being only $C^0$ continuous, to have a larger $S$ than the infinitely differentiable SiLU and GELU functions.\nThe implementation will systematically perform these computations for each of the three activation functions and format the results as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes empirical linearity and smoothness metrics for an MLP \n    with different activation functions.\n    \"\"\"\n    # ---- 1. Define Constants and Parameters ----\n    h = 10  # Hidden layer width\n\n    # Parameters for measure R (Linear Region Count)\n    N = 1025\n    tau = 1e-2\n\n    # Parameters for measure S (Smoothness)\n    G = 81\n    delta = 4 / (G - 1)\n\n    # ---- 2. Define Activation Functions ----\n    def relu(z):\n        \"\"\"Rectified Linear Unit activation function.\"\"\"\n        return np.maximum(0, z)\n\n    def silu(z):\n        \"\"\"Sigmoid Linear Unit activation function.\"\"\"\n        # This implementation is numerically stable for large negative z.\n        return np.where(z >= 0, z / (1 + np.exp(-z)), z * np.exp(z) / (1 + np.exp(z)))\n\n    def gelu(z):\n        \"\"\"Gaussian Error Linear Unit activation function (tanh approximation).\"\"\"\n        c = np.sqrt(2 / np.pi)\n        return 0.5 * z * (1 + np.tanh(c * (z + 0.044715 * z**3)))\n\n    activation_functions = {\n        \"ReLU\": relu,\n        \"SiLU\": silu,\n        \"GELU\": gelu,\n    }\n\n    # ---- 3. Generate Deterministic Weights and Biases ----\n    # W1: shape (10, 2)\n    i_w1 = np.arange(h).reshape(h, 1)\n    j_w1 = np.arange(2).reshape(1, 2)\n    W1 = 0.7 * np.sin(i_w1 + 2 * j_w1) + 0.05 * np.cos(3 * i_w1 - j_w1)\n\n    # b1: shape (10,)\n    i_b1 = np.arange(h)\n    b1 = 0.1 * np.sin(i_b1)\n\n    # W2: shape (1, 10)\n    k_w2 = np.arange(h)\n    W2 = (0.6 * np.cos(k_w2) + 0.05 * np.sin(2 * k_w2)).reshape(1, h)\n\n    # b2: scalar\n    b2 = 0.0\n\n    # ---- 4. Define Helper Functions for MLP and Metrics ----\n    def create_mlp_model(activation_fn):\n        \"\"\"Factory function to create an MLP model with a specific activation.\"\"\"\n        def model(x):\n            # x has shape (num_points, 2)\n            pre_activation = x @ W1.T + b1\n            hidden_output = activation_fn(pre_activation)\n            output = hidden_output @ W2.T + b2\n            return output.flatten()\n        return model\n\n    def calculate_R(f, n_samples, threshold):\n        \"\"\"Calculates the empirical linear region count R.\"\"\"\n        t = np.linspace(-2, 2, n_samples)\n        x_slice = np.stack([t, t], axis=-1)\n        y = f(x_slice)\n        \n        s = np.diff(y)  # First differences\n        d = np.diff(s)  # Second differences\n        \n        R = np.sum(np.abs(d) > threshold)\n        return int(R)\n\n    def calculate_S(f, grid_size, grid_spacing):\n        \"\"\"Calculates the empirical smoothness measure S.\"\"\"\n        ax_pts = np.linspace(-2, 2, grid_size)\n        xx, yy = np.meshgrid(ax_pts, ax_pts, indexing='xy')\n        grid_points = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n\n        F = f(grid_points).reshape(grid_size, grid_size)\n        \n        # Approximate gradient via central differences. F[u,v] is f(x_v, y_u).\n        dF_dx = (F[1:-1, 2:] - F[1:-1, :-2]) / (2 * grid_spacing)\n        dF_dy = (F[2:, 1:-1] - F[:-2, 1:-1]) / (2 * grid_spacing)\n        \n        g = np.stack([dF_dx, dF_dy], axis=-1)\n        \n        # Horizontal gradient variation\n        g_diff_h = g[:, 1:, :] - g[:, :-1, :]\n        sum1 = np.sum(np.linalg.norm(g_diff_h, axis=-1))\n\n        # Vertical gradient variation\n        g_diff_v = g[1:, :, :] - g[:-1, :, :]\n        sum2 = np.sum(np.linalg.norm(g_diff_v, axis=-1))\n\n        E = 2.0 * (grid_size - 2) * (grid_size - 3)\n        S = (sum1 + sum2) / E\n        return S\n\n    # ---- 5. Main Execution Loop ----\n    test_cases = [\"ReLU\", \"SiLU\", \"GELU\"]\n    final_results = []\n    \n    for case_name in test_cases:\n        activation_fn = activation_functions[case_name]\n        mlp_model = create_mlp_model(activation_fn)\n        \n        R = calculate_R(mlp_model, N, tau)\n        S = calculate_S(mlp_model, G, delta)\n        \n        final_results.append(str(R))\n        final_results.append(f\"{S:.6f}\")\n\n    # ---- 6. Format and Print Final Output ----\n    print(f\"[{','.join(final_results)}]\")\n\n# Execute the main function\nsolve()\n```", "id": "3151225"}, {"introduction": "We often think of neural network weights as inscrutable parameters found only through iterative optimization. This exercise [@problem_id:3151121] demystifies the expressive power of ReLU networks by tasking you with constructing one \"by hand\" to perfectly fit a set of data points. This process reveals the direct link between a network's structure and the geometric properties of the function it represents, connecting its interpolating capability to generalization concepts like the function's slope (Lipschitz constant) and margin.", "problem": "You are to implement and analyze a minimal yet mathematically consistent classification setting using a two-layer feedforward network (Multi-Layer Perceptron (MLP)) with Rectified Linear Unit (ReLU) nonlinearity to explore zero training error in the overparameterized regime and relate the resulting margin to test performance. Treat inputs as one-dimensional real numbers, and consider binary labels in $\\{-1, +1\\}$. The overall goal is to construct, from first principles, an explicit overparameterized two-layer ReLU MLP that exactly interpolates the training data, compute a principled margin for this model, and evaluate its test accuracy against a specified ground-truth classifier.\n\nFundamental base and definitions:\n- A feedforward two-layer network implements functions that are finite sums of shifted and scaled Rectified Linear Unit (ReLU) activations plus an output bias; namely, an output of the form $f(x) = \\sum_{j=1}^{m} a_j \\max(0, w_j x + b_j) + c$, where $m$ is the width (number of hidden units).\n- The Rectified Linear Unit (ReLU) is defined by $\\mathrm{ReLU}(z) = \\max(0, z)$.\n- The classifier induced by $f$ predicts $\\widehat{y}(x) = \\mathrm{sign}(f(x)) \\in \\{-1,+1\\}$, where $\\mathrm{sign}(t) = -1$ if $t < 0$ and $+1$ otherwise.\n- The empirical $0$–$1$ classification loss on a dataset $\\{(x_i,y_i)\\}_{i=1}^{n}$ is the average of the indicator $\\mathbb{1}\\{\\widehat{y}(x_i) \\neq y_i\\}$ over the $n$ points, but in this task you must ensure $\\widehat{y}(x_i) = y_i$ for all $i$, i.e., zero training error.\n- For one-dimensional piecewise linear $f$, define the (global) Lipschitz constant on a closed interval $[x_{\\min}, x_{\\max}]$ by $L = \\sup_{x \\in [x_{\\min}, x_{\\max}]} |f'(x)|$, where $f'$ exists almost everywhere and is piecewise constant. This $L$ equals the maximum absolute slope of $f$ across the linear pieces induced by the hidden units.\n- Define the functional margin on the training set as $\\gamma = \\min_{i} \\frac{y_i f(x_i)}{L}$, with the convention that if $L = 0$ then $\\gamma = +\\infty$.\n\nYour construction task:\n- For each provided training set, construct an explicit two-layer ReLU MLP with width at least $n$ that interpolates the given training samples $(x_i, y_i)$ exactly, achieving zero training error. You must not use iterative training; instead, derive the weights directly from the requirement $f(x_i) = y_i$ and the piecewise linear structure of ReLU networks. Your construction should be principled, relying only on foundational properties of ReLU sums forming piecewise linear functions, and must be valid for strictly increasing inputs $x_1 < x_2 < \\cdots < x_n$.\n- Compute the Lipschitz constant $L$ as the maximum absolute slope across the linear segments between consecutive $x_i$.\n- Compute the margin $\\gamma$ using the formula above.\n\nGround-truth for test performance:\n- Use the fixed ground-truth classifier $h^\\star(x) = \\mathrm{sign}(x)$ to define the true label at any real $x$, with the convention $\\mathrm{sign}(0) = +1$.\n\nTest accuracy specification:\n- For each test case, evaluate the classifier induced by your constructed $f$ on a uniform grid of $M$ test points over the closed interval $[x_{\\min}, x_{\\max}]$ spanned by the training inputs of that case, where $x_{\\min} = \\min_i x_i$ and $x_{\\max} = \\max_i x_i$. Compute the test accuracy as the fraction (in decimal form) of test points where $\\mathrm{sign}(f(x))$ matches $h^\\star(x)$.\n\nTest suite:\n- There are four test cases. Each case consists of a triplet $(\\text{inputs}, \\text{labels}, M)$, where:\n    1. Case A (well-separated classes, happy path): inputs $[-2.0, -1.0, 1.0, 2.0]$, labels $[-1, -1, +1, +1]$, $M = 401$.\n    2. Case B (highly localized alternation, boundary stress): inputs $[-0.1, 0.0, 0.1]$, labels $[-1, +1, -1]$, $M = 401$.\n    3. Case C (alternating labels over moderate distances, oscillatory): inputs $[-1.0, -0.5, 0.0, 0.5, 1.0]$, labels $[-1, +1, -1, +1, -1]$, $M = 801$.\n    4. Case D (uniform negative labels, extreme smoothness edge case): inputs $[-2.0, -0.5, 0.5, 2.0]$, labels $[-1, -1, -1, -1]$, $M = 801$.\n\nRequirements and constraints:\n- The constructed networks must be overparameterized, meaning the width $m$ is at least equal to the number of training samples $n$ and strictly greater than the input dimension ($1$).\n- You must achieve zero training error in all cases by exact interpolation, not by optimization.\n- For each case, output a pair `[gamma, accuracy]` where $\\gamma$ is the margin as defined and `accuracy` is the test accuracy computed against $h^\\star(x)$ on the specified grid; both should be represented as floating-point numbers. If $L = 0$, return $\\gamma = +\\infty$.\n- Final output format: Your program should produce a single line containing the results for the four cases as a comma-separated list of lists, in the form $[[\\gamma_1, a_1], [\\gamma_2, a_2], [\\gamma_3, a_3], [\\gamma_4, a_4]]$.\n\nNo physical units or angle units are involved in this problem. All numerical answers must be plain decimals.", "solution": "The problem is valid as it is mathematically well-defined, self-contained, and grounded in the established principles of neural networks and statistical learning theory. The task requires the direct construction of a function and the computation of its properties, which is a feasible and objective procedure.\n\nThe solution proceeds in four main steps for each test case:\n1.  **Constructing the Interpolating MLP**: We construct a two-layer ReLU network that exactly interpolates the training data $\\{(x_i, y_i)\\}_{i=1}^n$.\n2.  **Computing the Lipschitz Constant ($L$)**: We determine the maximum absolute slope of the constructed function.\n3.  **Computing the Margin ($\\gamma$)**: We apply the provided formula for the functional margin.\n4.  **Evaluating Test Accuracy**: We compare the predictions of our classifier against the ground-truth classifier on a specified test grid.\n\n### 1. Construction of the Interpolating MLP\nThe problem asks for a two-layer feedforward network of the form $f(x) = \\sum_{j=1}^{m} a_j \\max(0, w_j x + b_j) + c$ that interpolates the training data, i.e., $f(x_i) = y_i$ for all $i=1, \\dots, n$. We are given that the inputs are sorted: $x_1 < x_2 < \\dots < x_n$.\n\nA function composed of a sum of ReLU units is continuous and piecewise linear. The \"kinks\" or non-differentiable points of such a function occur where the arguments of the ReLU functions are zero. We can construct a piecewise linear function that passes through all points $(x_i, y_i)$ by placing kinks at each $x_i$. This leads to a specific choice of network parameters.\n\nLet's define our network with width $m=n$, where $n$ is the number of data points. This satisfies the overparameterization constraint $m \\ge n$. For each hidden unit $j \\in \\{1, \\dots, n\\}$, we set its weights and bias to create a kink at $x_j$. A simple choice is $w_j = 1$ and $b_j = -x_j$, so the activation is $\\max(0, x - x_j)$.\n\nThe full function can be expressed as:\n$$f(x) = c + \\sum_{j=1}^{n} a_j \\max(0, x - x_j)$$\nThis function is a valid two-layer ReLU network. The first layer computes $z_j = x - x_j$, and the second layer combines the ReLU activations of these $z_j$.\n\nTo ensure $f(x_i)=y_i$, we must determine the coefficients $c$ and $a_j$. The function's derivative $f'(x)$ is a step function. The slope in any interval $(x_k, x_{k+1})$ is given by $\\sum_{j=1}^k a_j$. To make $f(x)$ the linear interpolant of the points, its slope in the interval $(x_k, x_{k+1})$ must be:\n$$s_k = \\frac{y_{k+1} - y_k}{x_{k+1} - x_k} \\quad \\text{for } k=1, \\dots, n-1$$\nWe also enforce that the function is flat outside the data range, setting the slope to $0$ for $x < x_1$ and $x > x_n$. Let $s_0 = 0$ and $s_n = 0$.\n\nThe coefficient $a_j$ represents the change in slope at $x_j$. Therefore:\n- At $x_1$, the slope changes from $s_0=0$ to $s_1$. So, $a_1 = s_1 - s_0 = s_1$.\n- At $x_k$ for $k=2, \\dots, n-1$, the slope changes from $s_{k-1}$ to $s_k$. So, $a_k = s_k - s_{k-1}$.\n- At $x_n$, the slope changes from $s_{n-1}$ to $s_n=0$. So, $a_n = s_n - s_{n-1} = -s_{n-1}$.\n\nFinally, the output bias $c$ is determined by the condition $f(x_1) = y_1$. At $x=x_1$, all $\\max(0, x_1-x_j)$ terms are zero for $j \\geq 1$. Thus, $f(x_1) = c$, which implies $c = y_1$.\n\nThe final constructed function is:\n$$f(x) = y_1 + \\sum_{j=1}^{n} a_j \\max(0, x - x_j)$$\nThis construction guarantees $f(x_i) = y_i$ for all $i$, achieving zero training error.\n\n### 2. Computing the Lipschitz Constant ($L$)\nThe Lipschitz constant $L$ on the interval $[x_{\\min}, x_{\\max}] = [x_1, x_n]$ is the maximum absolute value of the function's derivative, $L = \\sup_{x \\in [x_1, x_n]} |f'(x)|$.\nSince our function $f(x)$ is piecewise linear with slopes $s_k$ on the segments $(x_k, x_{k+1})$, the Lipschitz constant is simply the maximum absolute slope:\n$$L = \\max_{k=1, \\dots, n-1} |s_k| = \\max_{k=1, \\dots, n-1} \\left| \\frac{y_{k+1} - y_k}{x_{k+1} - x_k} \\right|$$\nIf $n \\le 1$ or all slopes are zero, then $L=0$.\n\n### 3. Computing the Margin ($\\gamma$)\nThe functional margin is defined as $\\gamma = \\min_{i} \\frac{y_i f(x_i)}{L}$. By our construction, $f(x_i) = y_i$. The labels are $y_i \\in \\{-1, +1\\}$. Therefore, for each training point $i$:\n$$y_i f(x_i) = y_i^2 = 1$$\nThe expression for the margin simplifies to:\n$$\\gamma = \\min_{i} \\frac{1}{L} = \\frac{1}{L}$$\nAs per the problem definition, if $L=0$, the margin is $\\gamma = +\\infty$.\n\n### 4. Evaluating Test Accuracy\nThe test accuracy is computed on a uniform grid of $M$ points spanning the training data interval $[x_{\\min}, x_{\\max}]$. For each test point $x_{test}$:\n- The ground-truth label is $y_{true} = h^\\star(x_{test}) = \\mathrm{sign}(x_{test})$, where $\\mathrm{sign}(0)=+1$.\n- The model's prediction is $\\hat{y}_{pred} = \\mathrm{sign}(f(x_{test}))$, where $\\mathrm{sign}(t)=+1$ for $t \\ge 0$ and $-1$ for $t<0$.\nThe accuracy is the fraction of test points where $y_{true} = \\hat{y}_{pred}$.\n\nThis procedure is implemented for each of the four test cases provided. For instance, in Case A, the slopes are $s_1=0$, $s_2=1$, and $s_3=0$, yielding $L=1$ and $\\gamma=1/1=1$. The constructed function $f(x) = -1 + \\max(0, x+1) - \\max(0, x-1)$ aligns perfectly with the ground truth $h^\\star(x)$ over its domain, resulting in a test accuracy of $1.0$. The other cases are solved analogously.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It defines the test cases, processes each one, and prints the formatted results.\n    \"\"\"\n    test_cases = [\n        # Case A (well-separated classes, happy path)\n        ({'inputs': [-2.0, -1.0, 1.0, 2.0], 'labels': [-1, -1, 1, 1], 'M': 401}),\n        # Case B (highly localized alternation, boundary stress)\n        ({'inputs': [-0.1, 0.0, 0.1], 'labels': [-1, 1, -1], 'M': 401}),\n        # Case C (alternating labels over moderate distances, oscillatory)\n        ({'inputs': [-1.0, -0.5, 0.0, 0.5, 1.0], 'labels': [-1, 1, -1, 1, -1], 'M': 801}),\n        # Case D (uniform negative labels, extreme smoothness edge case)\n        ({'inputs': [-2.0, -0.5, 0.5, 2.0], 'labels': [-1, -1, -1, -1], 'M': 801})\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case['inputs'], case['labels'], case['M'])\n        all_results.append(result)\n\n    # Format and print the final output as a comma-separated list of lists.\n    # e.g., [[gamma1,acc1],[gamma2,acc2],...]\n    print(f\"[{','.join(f'[{g},{a}]' for g, a in all_results)}]\")\n\ndef analyze_case(inputs, labels, M):\n    \"\"\"\n    Constructs an MLP, computes its margin and test accuracy for a single case.\n\n    Args:\n        inputs (list): Sorted list of 1D input features.\n        labels (list): List of binary labels {-1, 1}.\n        M (int): Number of points in the test grid.\n\n    Returns:\n        list: A pair [gamma, accuracy].\n    \"\"\"\n    x = np.array(inputs, dtype=np.float64)\n    y = np.array(labels, dtype=np.float64)\n    n = len(x)\n\n    # 1. Compute slopes of the piecewise linear interpolant.\n    # m_k is the slope in the interval [x_k, x_{k+1}].\n    if n > 1:\n        m = (y[1:] - y[:-1]) / (x[1:] - x[:-1])\n    else:\n        m = np.array([])\n\n    # 2. Compute the Lipschitz constant L.\n    # L is the maximum absolute slope over the domain [x_min, x_max].\n    L = np.max(np.abs(m)) if m.size > 0 else 0.0\n\n    # 3. Compute the functional margin gamma.\n    # Since f(x_i) = y_i and y_i^2 = 1, gamma simplifies to 1/L.\n    gamma = math.inf if L == 0.0 else 1.0 / L\n\n    # 4. Construct the interpolating function f(x).\n    # f(x) = y_1 + sum_{j=1..n} a_j * ReLU(x - x_j), where a_j are slope changes.\n    a = np.zeros(n, dtype=np.float64)\n    if n > 1:\n        # a_1 = m_1 (in 0-indexing: a[0] = m[0])\n        a[0] = m[0]\n        # a_k = m_k - m_{k-1} for k=2..n-1\n        for i in range(1, n - 1):\n            a[i] = m[i] - m[i - 1]\n        # a_n = -m_{n-1} (in 0-indexing: a[n-1] = -m[n-2])\n        a[n - 1] = -m[n - 2]\n    \n    def f(xt_val):\n        # The function value is computed from its piecewise linear definition.\n        val = y[0]\n        # This loop implements sum_{j=1..n} a_j * max(0, xt - x_j)\n        for j in range(n):\n            val += a[j] * max(0, xt_val - x[j])\n        return val\n\n    # 5. Compute test accuracy.\n    x_min, x_max = x[0], x[-1]\n    test_grid = np.linspace(x_min, x_max, M, dtype=np.float64)\n    \n    matches = 0\n    for xt in test_grid:\n        # Ground truth: h*(x) = sign(x), with sign(0)=+1.\n        y_true = 1 if xt >= 0 else -1\n        \n        # Model's prediction: sign(f(x)), with sign(t>=0)=+1.\n        fx_val = f(xt)\n        y_pred = 1 if fx_val >= 0 else -1\n        \n        if y_true == y_pred:\n            matches += 1\n            \n    accuracy = matches / M\n    \n    return [gamma, accuracy]\n\nsolve()\n```", "id": "3151121"}, {"introduction": "Classical statistical learning theory suggests that model performance on unseen data follows a U-shaped curve, worsening after a certain point of increasing complexity. This practice [@problem_id:3151120] challenges that notion by guiding you through an empirical demonstration of the \"double descent\" phenomenon, a hallmark of modern deep learning. By observing how test error behaves as a model crosses the interpolation threshold—the point where it can perfectly memorize the training data—you will gain firsthand insight into the surprising and beneficial generalization behavior of highly overparameterized models.", "problem": "You are tasked with empirically demonstrating the double descent phenomenon in a feedforward Multilayer Perceptron (MLP) by sweeping the parameter–sample ratio and linking the interpolation threshold to a spike in test error. The core setting is as follows.\n\nStart from the foundational base of empirical risk minimization under squared loss. Let the input space be $\\mathbb{R}^d$. Consider a two-layer feedforward Multilayer Perceptron (MLP) with a Rectified Linear Unit (ReLU) nonlinearity, where the hidden layer weights are fixed and only the output layer weights are trained. For an input vector $x \\in \\mathbb{R}^d$, the MLP computes\n$$\n\\phi(x) = \\big(\\sigma(w_1^\\top x + b_1), \\ldots, \\sigma(w_m^\\top x + b_m)\\big) \\in \\mathbb{R}^m,\n$$\nwhere $\\sigma(z) = \\max\\{0, z\\}$ is the ReLU activation, $w_j \\in \\mathbb{R}^d$ and $b_j \\in \\mathbb{R}$ are fixed hidden-layer parameters, and $m$ is the number of hidden units. The predicted output is\n$$\n\\hat{y}(x) = a^\\top \\phi(x),\n$$\nwhere $a \\in \\mathbb{R}^m$ are the trained output-layer weights. Given training data $\\{(x_i, y_i)\\}_{i=1}^n$, define the design matrix\n$$\n\\Phi \\in \\mathbb{R}^{n \\times m}, \\quad \\Phi_{ij} = \\sigma(w_j^\\top x_i + b_j).\n$$\nThe empirical risk minimization problem under squared loss seeks $a$ minimizing $\\sum_{i=1}^n (\\hat{y}(x_i) - y_i)^2$, which is a linear least squares problem. The minimum-norm solution is given by the Moore–Penrose pseudoinverse:\n$$\na^\\star = \\Phi^+ y,\n$$\nwhere $y = (y_1, \\ldots, y_n)^\\top$ and $\\Phi^+$ denotes the pseudoinverse of $\\Phi$. The interpolation threshold is reached when the training mean squared error becomes zero, which typically occurs once $\\Phi$ reaches full row rank as $m$ grows, so that the system $\\Phi a = y$ admits a solution. The parameter–sample ratio is defined as $p/n$, where $p$ is the number of trained parameters. In this setting, $p = m$.\n\nThe double descent phenomenon refers to the typical behavior where the test error as a function of model capacity initially decreases, then increases near the interpolation threshold (where training error hits zero), and then decreases again beyond that threshold as capacity further increases. Your program will construct synthetic data from a teacher network and measure test mean squared error across a sweep of $m$ values corresponding to different parameter–sample ratios.\n\nData generation and evaluation protocol:\n- Draw inputs $x \\in \\mathbb{R}^d$ independently from a standard normal distribution.\n- Generate labels using a fixed teacher network with a small number of ReLU units:\n$$\ny = \\sum_{k=1}^{m_{\\text{teacher}}} \\beta_k \\, \\sigma(u_k^\\top x + c_k) + \\varepsilon,\n$$\nwhere $u_k \\in \\mathbb{R}^d$, $c_k \\in \\mathbb{R}$, and $\\beta_k \\in \\mathbb{R}$ are fixed teacher parameters, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is independent Gaussian noise with standard deviation $\\sigma$. Use $m_{\\text{teacher}} = 5$.\n- Train the student MLP by computing $a^\\star = \\Phi^+ y$ for each chosen $m$.\n- Compute the training mean squared error\n$$\n\\mathrm{MSE}_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}(x_i) - y_i \\right)^2,\n$$\nand the test mean squared error on an independent test set of size $n_{\\text{test}}$,\n$$\n\\mathrm{MSE}_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{y}(x_i^{\\text{test}}) - y_i^{\\text{test}} \\right)^2.\n$$\nSet $n_{\\text{test}} = \\max\\{3n, 200\\}$.\n\nInterpolation threshold and spike detection:\n- Define the interpolation tolerance $\\epsilon = 10^{-10}$ and the spike margin $\\delta = 0.1$ (meaning $10\\%$).\n- For a given $n$ and a sweep of $m$ values, define the interpolation threshold $m_{\\text{interp}}$ as the smallest $m$ in the sweep such that $\\mathrm{MSE}_{\\text{train}}(m) \\le \\epsilon$.\n- Define a spike at the interpolation threshold if $\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}})$ exceeds the median of $\\mathrm{MSE}_{\\text{test}}(m)$ over all other $m$ values by at least a factor of $(1+\\delta)$. Formally, if\n$$\n\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}}) > (1 + \\delta) \\cdot \\operatorname{median}\\left(\\{\\mathrm{MSE}_{\\text{test}}(m) : m \\in \\mathcal{M}, m \\ne m_{\\text{interp}}\\}\\right),\n$$\nthen output the boolean value $\\mathrm{True}$; otherwise output $\\mathrm{False}$. If no $m$ in the sweep achieves interpolation, output $\\mathrm{False}$.\n\nSweep design:\n- For each test case, construct the list of hidden unit counts $\\mathcal{M}$ by scaling $n$ with ratios $\\{0.5, 0.8, 1.0, 1.2, 1.5\\}$ and rounding to the nearest integer, ensuring $m \\ge 1$. That is,\n$$\n\\mathcal{M} = \\left\\{ \\max\\left(1, \\left\\lfloor r \\cdot n \\right\\rceil \\right) : r \\in \\{0.5, 0.8, 1.0, 1.2, 1.5\\} \\right\\}.\n$$\nWithin a single test case, ensure all student hidden-layer parameters $\\{(w_j, b_j)\\}_{j=1}^{m_{\\max}}$ are fixed once at the largest $m_{\\max} = \\max \\mathcal{M}$, and the models for smaller $m$ reuse the first $m$ features to make the sweep comparable.\n\nTest suite:\nRun your program on the following four test cases. For each case, report a single boolean indicating whether a spike is detected at the interpolation threshold according to the rule above. Use independent random seeds per case to fix all randomness deterministically.\n\n- Case $1$: $n = 60$, $d = 20$, $\\sigma = 0.5$, $\\text{seed} = 0$.\n- Case $2$: $n = 60$, $d = 20$, $\\sigma = 0.0$, $\\text{seed} = 1$.\n- Case $3$: $n = 24$, $d = 8$, $\\sigma = 0.5$, $\\text{seed} = 2$.\n- Case $4$: $n = 80$, $d = 30$, $\\sigma = 0.8$, $\\text{seed} = 3$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4]$, where each $\\mathrm{result}_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$ corresponding to the spike detection for Case $i$.", "solution": "The problem statement is valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of instructions for an empirical study of the double descent phenomenon in a random feature model. There are no contradictions, ambiguities, or factual inaccuracies. We may therefore proceed with a solution.\n\nThe objective is to empirically investigate the double descent phenomenon in a simplified multilayer perceptron (MLP). This phenomenon describes a U-shaped then downward-sloping curve for test error as a function of model capacity. The initial decrease corresponds to the classical bias-variance trade-off in the underparameterized regime. The test error then peaks near the interpolation threshold, where the model has just enough capacity to fit the training data perfectly. Beyond this point, in the overparameterized regime, the test error surprisingly decreases again.\n\nThe methodology is structured as a controlled numerical experiment based on a teacher-student framework.\n\n**1. Model Specification and Training**\n\nThe model is a two-layer feedforward network with a Rectified Linear Unit (ReLU) activation function, $\\sigma(z) = \\max\\{0, z\\}$. For an input $x \\in \\mathbb{R}^d$, the output is $\\hat{y}(x) = a^\\top \\phi(x)$, where $\\phi(x) = (\\sigma(w_1^\\top x + b_1), \\ldots, \\sigma(w_m^\\top x + b_m))$ is a vector of feature activations. A crucial aspect of this problem is that the hidden layer parameters, $\\{w_j, b_j\\}_{j=1}^m$, are fixed after random initialization. Only the output layer weights, $a \\in \\mathbb{R}^m$, are trained. This transforms the nonlinear neural network problem into a linear regression problem in a high-dimensional feature space, known as a random feature model. The number of hidden units, $m$, directly corresponds to the number of trainable parameters and serves as our measure of model capacity.\n\nGiven $n$ training samples $\\{(x_i, y_i)\\}_{i=1}^n$, we form the design matrix $\\Phi \\in \\mathbb{R}^{n \\times m}$, where each element is $\\Phi_{ij} = \\sigma(w_j^\\top x_i + b_j)$. The goal is to find the weight vector $a$ that minimizes the squared loss, $\\mathcal{L}(a) = \\sum_{i=1}^n (y_i - a^\\top \\phi(x_i))^2 = \\|y - \\Phi a\\|_2^2$.\n\nThe solution to this linear least-squares problem that also has the minimum Euclidean norm $\\|a\\|_2$ is given by $a^\\star = \\Phi^+ y$. Here, $\\Phi^+$ is the Moore-Penrose pseudoinverse of the design matrix $\\Phi$, and $y = (y_1, \\ldots, y_n)^\\top$ is the vector of training labels. The pseudoinverse provides a unique, stable solution for any shape of $\\Phi$, correctly handling both the underparameterized ($m < n$, typically full column rank) and overparameterized ($m > n$, typically full row rank) regimes.\n\n**2. Data Generation and Evaluation**\n\nWe employ a teacher-student setup to create a synthetic dataset with a known ground truth.\n- A fixed \"teacher\" network of the same architecture, with $m_{\\text{teacher}} = 5$ hidden units, generates the labels: $y = \\sum_{k=1}^{m_{\\text{teacher}}} \\beta_k \\, \\sigma(u_k^\\top x + c_k) + \\varepsilon$. The input vectors $x \\in \\mathbb{R}^d$ are drawn from a standard normal distribution. Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is added to the output, modeling inherent measurement error or unmodeled effects.\n- A \"student\" model (our trainable model) is then trained on a set of $n$ such data points.\n- The performance of the trained student model is evaluated using the Mean Squared Error (MSE) on both the training set and an independent, larger test set of size $n_{\\text{test}} = \\max\\{3n, 200\\}$.\n  - $\\mathrm{MSE}_{\\text{train}}(m) = \\frac{1}{n} \\|\\Phi a^\\star - y\\|^2$\n  - $\\mathrm{MSE}_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\| \\Phi_{\\text{test}} a^\\star - y_{\\text{test}}\\|^2$\n\n**3. Experimental Procedure and Spike Detection**\n\nThe core of the experiment is to track $\\mathrm{MSE}_{\\text{test}}(m)$ as we sweep the model capacity $m$. The problem defines a specific sweep protocol: the set of capacities $\\mathcal{M}$ is generated by scaling the number of samples $n$ by ratios $\\{0.5, 0.8, 1.0, 1.2, 1.5\\}$. This set of ratios is designed to probe model behavior in the underparameterized regime ($m/n < 1$), at the interpolation threshold ($m/n \\approx 1$), and in the overparameterized regime ($m/n > 1$). To ensure results are comparable across the sweep, the random features for the largest model ($m_{\\max} = \\max \\mathcal{M}$) are generated once, and smaller models simply use a subset of these features.\n\nThe double descent hypothesis predicts a spike in $\\mathrm{MSE}_{\\text{test}}$ around the point where the model first perfectly fits the training data. This point is formalized as the interpolation threshold, $m_{\\text{interp}}$, defined as the smallest $m \\in \\mathcal{M}$ for which $\\mathrm{MSE}_{\\text{train}}(m) \\le \\epsilon$, with a small tolerance $\\epsilon = 10^{-10}$.\n\nA \"spike\" is said to be detected if the test error at this threshold is significantly higher than the typical test error at other capacities. The condition is given by the inequality:\n$$\n\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}}) > (1 + \\delta) \\cdot \\operatorname{median}\\left(\\{\\mathrm{MSE}_{\\text{test}}(m) : m \\in \\mathcal{M}, m \\ne m_{\\text{interp}}\\}\\right)\n$$\nwhere the spike margin is $\\delta = 0.1$. If this condition is met, the result for the test case is $\\mathrm{True}$; otherwise, it is $\\mathrm{False}$. If no $m$ in the sweep achieves interpolation, the result is also $\\mathrm{False}$.\n\nThe implementation will execute this entire procedure for each of the four specified test cases, using the given random seeds for reproducibility, and report the boolean outcome of the spike detection test.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_case(n, d, sigma, seed):\n    \"\"\"\n    Runs a single simulation case for the double descent experiment.\n    \n    Args:\n        n (int): Number of training samples.\n        d (int): Input dimension.\n        sigma (float): Standard deviation of label noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        bool: True if a spike is detected, False otherwise.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Generate Teacher Network\n    m_teacher = 5\n    u_teacher = np.random.randn(m_teacher, d)\n    c_teacher = np.random.randn(m_teacher)\n    beta_teacher = np.random.randn(m_teacher)\n    \n    def teacher_model(X):\n        activations = np.maximum(0, X @ u_teacher.T + c_teacher)\n        return activations @ beta_teacher\n\n    # 2. Generate Training and Test Data\n    X_train = np.random.randn(n, d)\n    noise_train = sigma * np.random.randn(n)\n    y_train = teacher_model(X_train) + noise_train\n\n    n_test = max(3 * n, 200)\n    X_test = np.random.randn(n_test, d)\n    noise_test = sigma * np.random.randn(n_test)\n    y_test = teacher_model(X_test) + noise_test\n\n    # 3. Define Model Sweep\n    ratios = [0.5, 0.8, 1.0, 1.2, 1.5]\n    m_values = sorted([max(1, int(np.round(r * n))) for r in ratios])\n    m_max = m_values[-1]\n\n    # 4. Generate Student Network's Fixed Features\n    W_student = np.random.randn(m_max, d)\n    b_student = np.random.randn(m_max)\n\n    train_mses = []\n    test_mses = []\n\n    # 5. Sweep through model capacities (m)\n    for m in m_values:\n        # Select the first m features\n        W_m = W_student[:m, :]\n        b_m = b_student[:m]\n        \n        # Construct design matrix for training\n        Phi_train = np.maximum(0, X_train @ W_m.T + b_m)\n        \n        # Train model using Moore-Penrose pseudoinverse\n        # a_star = pinv(Phi_train) @ y_train\n        a_star = np.linalg.pinv(Phi_train) @ y_train\n        \n        # Evaluate Training MSE\n        y_train_pred = Phi_train @ a_star\n        train_mse = np.mean((y_train_pred - y_train) ** 2)\n        train_mses.append(train_mse)\n        \n        # Evaluate Test MSE\n        Phi_test = np.maximum(0, X_test @ W_m.T + b_m)\n        y_test_pred = Phi_test @ a_star\n        test_mse = np.mean((y_test_pred - y_test) ** 2)\n        test_mses.append(test_mse)\n\n    # 6. Analyze Results for Spike Detection\n    epsilon = 1e-10\n    delta = 0.1\n    \n    np_train_mses = np.array(train_mses)\n    np_test_mses = np.array(test_mses)\n    \n    # Find interpolation threshold m_interp\n    interp_indices = np.where(np_train_mses = epsilon)[0]\n    \n    if len(interp_indices) == 0:\n        # No model achieved interpolation\n        return False\n        \n    idx_interp = interp_indices[0]\n    # m_interp = m_values[idx_interp] # not needed for calculation\n    mse_test_at_interp = np_test_mses[idx_interp]\n\n    # Get test MSEs for all other m values\n    other_indices = np.arange(len(m_values)) != idx_interp\n    other_test_mses = np_test_mses[other_indices]\n\n    if len(other_test_mses) == 0:\n        return False\n\n    median_other_mses = np.median(other_test_mses)\n    \n    # Check for spike condition\n    is_spike = mse_test_at_interp > (1 + delta) * median_other_mses\n    \n    return is_spike\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, d, sigma, seed)\n        (60, 20, 0.5, 0),\n        (60, 20, 0.0, 1),\n        (24, 8, 0.5, 2),\n        (80, 30, 0.8, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, d, sigma, seed = case\n        result = run_single_case(n, d, sigma, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3151120"}]}