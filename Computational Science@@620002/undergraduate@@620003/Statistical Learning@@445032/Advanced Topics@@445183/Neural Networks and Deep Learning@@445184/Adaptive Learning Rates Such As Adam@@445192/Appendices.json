{"hands_on_practices": [{"introduction": "To truly appreciate why adaptive methods like Adam are so prevalent, it's essential to see them in action where traditional methods struggle. This first practice demonstrates one of Adam's primary advantages: its relative insensitivity to the scale of input features. You will implement both Gradient Descent and Adam to observe firsthand how Adam's per-parameter normalization allows it to adapt quickly and efficiently, even when the optimization landscape is suddenly stretched or compressed.[@problem_id:3096105]", "problem": "You are to write a complete, runnable program that empirically evaluates the scale invariance and adaptation speed of Adaptive Moment Estimation (Adam) compared to vanilla Stochastic Gradient Descent (SGD) under a curriculum that gradually changes the scale of a single feature during training. The core of the experiment is pure linear regression with squared loss. The test requires that you implement both optimizers from first principles and measure how many gradient steps each needs to attain a target loss after each scale change.\n\nDataset and model specification:\n- Generate a synthetic dataset with $N$ independent samples and $d$ features as follows. Fix a random seed equal to $42$ to make the result deterministic.\n- Let $N = 512$ and $d = 2$.\n- Draw the feature matrix $X \\in \\mathbb{R}^{N \\times d}$ with entries sampled independently from a standard normal distribution.\n- Let the ground-truth parameter vector be $w_{\\text{true}} = [\\,2.0,\\,-3.0\\,]^{\\top}$.\n- Generate targets $y \\in \\mathbb{R}^{N}$ using the linear model with additive noise: $y = X w_{\\text{true}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I)$ with $\\sigma = 0.1$.\n- The prediction function is $\\hat{y}(w) = X_{\\text{scaled}} w$, where $X_{\\text{scaled}}$ is obtained by scaling only the second feature column of $X$ by a positive scalar $s$. That is, if $X = [\\,x^{(1)},\\,x^{(2)}\\,]$, then $X_{\\text{scaled}} = [\\,x^{(1)},\\, s \\cdot x^{(2)}\\,]$ for a chosen $s$.\n- The loss is the Mean Squared Error (MSE) $L(w; X_{\\text{scaled}}, y) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(\\hat{y}_{i}(w) - y_{i}\\right)^{2}$.\n\nCurriculum and evaluation protocol:\n- A curriculum is a finite sequence of strictly positive scale factors $\\{s_{0}, s_{1}, \\dots, s_{P-1}\\}$ applied to the second feature, visited in order during a single training run of a given optimizer. The optimizer continues from its current state across phases; that is, the parameter vector $w$ and any optimizer-internal state are not reset when the scale changes to the next $s_{p}$.\n- For each phase $p \\in \\{0,1,\\dots,P-1\\}$, set the current design matrix to $X_{\\text{scaled}}^{(p)}$ by scaling the second feature of $X$ by $s_{p}$, and then take up to $T$ gradient steps using that fixed $X_{\\text{scaled}}^{(p)}$.\n- Define the target MSE threshold as $\\tau = c \\cdot \\sigma^{2}$ with $c = 2.0$. Concretely, with $\\sigma = 0.1$, this gives $\\tau = 2.0 \\cdot (0.1)^{2}$.\n- The adaptation steps for phase $p$ are defined as the minimal number of gradient steps taken within that phase until the MSE first drops to at most $\\tau$ (counting zero if the MSE is already at most $\\tau$ at the start of the phase), or equal to $T$ if the MSE never falls to at most $\\tau$ within that phase.\n- For a curriculum, define the average adaptation steps as the arithmetic mean of the per-phase adaptation steps over all phases in that curriculum.\n\nAlgorithms to implement from first principles:\n- For vanilla Stochastic Gradient Descent (SGD), use a fixed learning rate $\\alpha_{\\text{sgd}}$ and the full-batch gradient of the MSE. The full-batch gradient of $L$ with respect to $w$ is\n$$\n\\nabla L(w) \\;=\\; \\frac{2}{N}\\,X_{\\text{scaled}}^{\\top}\\left(X_{\\text{scaled}}w - y\\right).\n$$\n- For Adaptive Moment Estimation (Adam), design the update rule using exponential moving averages of the first and second moments of the gradient together with bias correction so that each coordinate update magnitude becomes approximately invariant to feature rescaling. You must derive and implement the use of exponential moving averages for both the gradient and its elementwise square with decay parameters, and incorporate bias correction before forming the per-parameter normalized step. No internal state of Adam (such as moment estimates or time index) may be reset between curriculum phases.\n\nHyperparameters:\n- Use $\\alpha_{\\text{sgd}} = 0.001$ for SGD.\n- For Adam, use learning rate $\\alpha_{\\text{adam}} = 0.01$, first moment decay $\\beta_{1} = 0.9$, second moment decay $\\beta_{2} = 0.999$, and numerical stability constant $\\varepsilon_{\\text{adam}} = 10^{-8}$.\n- Use the full-batch gradient in all steps.\n- For each curriculum phase, allow at most $T$ steps, where $T$ is specified per test case.\n\nTest suite:\n- All test cases share the same dataset constructed from the same $X$ and $y$ described above.\n- Each test case specifies a curriculum and a per-phase step budget $T$.\n- The test cases are:\n    1. Case A (happy path): scales $\\left[\\,1.0,\\,10.0,\\,100.0\\,\\right]$ with $T = 400$.\n    2. Case B (scale down then up): scales $\\left[\\,1.0,\\,0.1,\\,10.0\\,\\right]$ with $T = 400$.\n    3. Case C (boundary: no scale change): scales $\\left[\\,1.0\\,\\right]$ with $T = 200$.\n\nRequired outputs:\n- For each test case, compute the ratio\n$$\nr \\;=\\; \\frac{\\text{average adaptation steps of SGD over the curriculum}}{\\text{average adaptation steps of Adam over the curriculum}}.\n$$\n- Aggregate the three ratios in order $\\left[\\text{Case A},\\text{Case B},\\text{Case C}\\right]$ as a single line.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{resultA},\\text{resultB},\\text{resultC}\\right]$). Each element must be a real number.\n\nScientific realism and derivation base:\n- Begin from the definition of the MSE for linear regression, the full-batch gradient expression, and the definition of exponential moving averages with bias correction. Derive how per-coordinate normalization by a root second-moment estimate yields approximate invariance of update magnitudes to constant rescaling of a corresponding feature, and implement this in Adam. Do not reset optimizer state between phases to properly measure adaptation speed after a sudden rescaling. Ensure that your implementation is numerically stable and that the experiment is deterministic under the fixed seed.", "solution": "The problem requires an empirical comparison of vanilla full-batch Gradient Descent (GD) and Adaptive Moment Estimation (Adam) on a linear regression task. The comparison focuses on adaptation speed, measured by the number of optimization steps required to reach a target loss after a sudden change in the scale of one feature. This is a well-posed numerical experiment designed to highlight the scale-invariance properties of Adam. The problem statement is scientifically grounded, internally consistent, and provides all necessary parameters for a deterministic and reproducible result.\n\nWe begin by formalizing the mathematical setup. The dataset consists of $N=512$ samples with $d=2$ features. The feature matrix $X \\in \\mathbb{R}^{N \\times d}$ is generated by drawing entries from a standard normal distribution $\\mathcal{N}(0, 1)$. The target variable $y \\in \\mathbb{R}^{N}$ is generated by a linear model with additive Gaussian noise:\n$$\ny = X w_{\\text{true}} + \\varepsilon\n$$\nwhere the true parameter vector is $w_{\\text{true}} = [2.0, -3.0]^{\\top}$ and the noise is $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ with $\\sigma = 0.1$. For reproducibility, a fixed random seed of $42$ is used for data generation.\n\nThe predictive model is a linear function $\\hat{y}(w) = X_{\\text{scaled}} w$, where $X_{\\text{scaled}}$ is derived from $X$ by scaling its second feature column by a factor $s > 0$. If $X = [x^{(1)}, x^{(2)}]$, then $X_{\\text{scaled}} = [x^{(1)}, s \\cdot x^{(2)}]$. The objective is to find the parameter vector $w \\in \\mathbb{R}^d$ that minimizes the Mean Squared Error (MSE) loss:\n$$\nL(w; X_{\\text{scaled}}, y) = \\frac{1}{N} \\| X_{\\text{scaled}} w - y \\|_2^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( (X_{\\text{scaled}} w)_i - y_i \\right)^2\n$$\nThe optimal parameters $w^*$ that minimize this loss depend on the scale $s$. They satisfy $X_{\\text{scaled}}^\\top (X_{\\text{scaled}} w^* - y) = 0$. For the noise-free case, this implies $X_{\\text{scaled}} w^* \\approx X w_{\\text{true}}$, which holds if $w_1^* = w_{\\text{true},1}$ and $s \\cdot w_2^* = w_{\\text{true},2}$. Thus, the target parameter vector becomes approximately $w^*(s) = [2.0, -3.0/s]^\\top$.\n\nTo minimize the loss, we use gradient-based optimization. The gradient of the MSE loss with respect to $w$ is given by:\n$$\n\\nabla_w L(w) = \\frac{2}{N} X_{\\text{scaled}}^{\\top} (X_{\\text{scaled}} w - y)\n$$\nThis gradient is used by both optimization algorithms, implemented from first principles.\n\nFirst, we consider vanilla Gradient Descent (GD), which the problem refers to as SGD but with full-batch gradients. The update rule is:\n$$\nw_{t+1} = w_t - \\alpha_{\\text{gd}} \\nabla_w L(w_t)\n$$\nwhere $w_t$ is the parameter vector at step $t$ and $\\alpha_{\\text{gd}} = 0.001$ is the fixed learning rate. The performance of GD is highly sensitive to the scaling of the features. The components of the gradient $\\nabla_w L(w)$ depend on the scale $s$. The second component, $\\frac{\\partial L}{\\partial w_2}$, is strongly affected by $s$. A large $s$ can make this gradient component very large, causing oscillations and instability unless $\\alpha_{\\text{gd}}$ is very small. Conversely, a small $s$ leads to a vanishing gradient component, slowing down learning for $w_2$.\n\nSecond, we implement the Adam optimizer. Adam maintains an exponentially decaying average of past gradients (first moment) and past squared gradients (second moment). Let $g_t = \\nabla_w L(w_{t-1})$ be the gradient at step $t$. The moment estimates are updated as:\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n$$\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n$$\nwhere $g_t^2$ denotes the element-wise square, $\\beta_1 = 0.9$ is the first moment decay rate, and $\\beta_2 = 0.999$ is the second moment decay rate. These are biased estimates, especially during the initial steps. Adam corrects this bias by computing:\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{and} \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\nThe final parameter update is:\n$$\nw_t = w_{t-1} - \\alpha_{\\text{adam}} \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon_{\\text{adam}}}\n$$\nwith learning rate $\\alpha_{\\text{adam}} = 0.01$ and a small stability constant $\\varepsilon_{\\text{adam}} = 10^{-8}$. The key to Adam's adaptivity lies in the term $\\sqrt{\\hat{v}_t}$, which normalizes the update for each parameter coordinate. If the gradient component for a parameter $w_j$ is consistently large (e.g., due to a large feature scale $s$), its corresponding second moment estimate $v_{t,j}$ will also be large. Division by $\\sqrt{\\hat{v}_{t,j}}$ effectively reduces the step size for that parameter. Conversely, if the gradient is small, the step size is effectively increased. This mechanism makes Adam's update steps approximately invariant to the scale of the features, allowing it to adapt quickly when the landscape of the loss function changes.\n\nThe evaluation protocol involves a curriculum of scales $\\{s_0, s_1, \\dots, s_{P-1}\\}$. For each scale $s_p$, the optimizer is run for a maximum of $T$ steps. The optimizer's state ($w$ for GD; $w, m, v, t$ for Adam) persists across phases. We measure the number of steps required within each phase to bring the MSE below a threshold $\\tau = c \\cdot \\sigma^2 = 2.0 \\cdot (0.1)^2 = 0.02$. If the loss is already below $\\tau$ at the start of a phase, the step count for that phase is $0$. If $\\tau$ is not reached within $T$ steps, the count is $T$. The performance metric is the average adaptation steps over all phases in a curriculum. Finally, we compute the ratio of average steps for GD to that of Adam.\n\nThe following code implements this entire procedure, including data generation, the two optimizers from first principles, and the specified curriculum-based evaluation for the three test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment, evaluate optimizers, and print the result.\n    \"\"\"\n    \n    # 1. Dataset and Model Specification\n    N, d = 512, 2\n    w_true = np.array([2.0, -3.0])\n    sigma = 0.1\n    random_seed = 42\n\n    def generate_data(N, d, w_true, sigma, seed):\n        \"\"\"Generates synthetic dataset based on problem specifications.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(N, d))\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y = X @ w_true + noise\n        return X, y\n\n    X, y = generate_data(N, d, w_true, sigma, random_seed)\n\n    # 2. Loss and Gradient Functions\n    def mse_loss(w, X_scaled, y):\n        \"\"\"Computes the Mean Squared Error.\"\"\"\n        error = X_scaled @ w - y\n        return np.mean(error**2)\n\n    def gradient(w, X_scaled, y):\n        \"\"\"Computes the full-batch gradient of the MSE loss.\"\"\"\n        N_samples = X_scaled.shape[0]\n        error = X_scaled @ w - y\n        return (2 / N_samples) * X_scaled.T @ error\n\n    # 3. Experiment Runner\n    def run_experiment(optimizer_type, X_base, y_base, curriculum, T, hyperparams):\n        \"\"\"\n        Runs an optimization experiment for a given optimizer and curriculum.\n        Returns the average adaptation steps.\n        \"\"\"\n        w = np.zeros(d)\n        tau = 2.0 * (sigma**2)\n        \n        # Optimizer state\n        if optimizer_type == 'adam':\n            alpha, beta1, beta2, epsilon = hyperparams\n            m = np.zeros(d)\n            v = np.zeros(d)\n            t_global = 0\n        else: # gd\n            alpha, = hyperparams\n\n        phase_adaptation_steps = []\n\n        for s in curriculum:\n            X_scaled = X_base.copy()\n            X_scaled[:, 1] *= s\n\n            # Check loss at the beginning of the phase\n            initial_loss = mse_loss(w, X_scaled, y_base)\n            if initial_loss <= tau:\n                phase_adaptation_steps.append(0)\n                continue\n\n            steps_taken_in_phase = T\n            for step in range(1, T + 1):\n                grad = gradient(w, X_scaled, y_base)\n\n                if optimizer_type == 'gd':\n                    w -= alpha * grad\n                else:  # adam\n                    t_global += 1\n                    m = beta1 * m + (1 - beta1) * grad\n                    v = beta2 * v + (1 - beta2) * (grad**2)\n                    \n                    # Bias correction\n                    m_hat = m / (1 - beta1**t_global)\n                    v_hat = v / (1 - beta2**t_global)\n                    \n                    w -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n                \n                current_loss = mse_loss(w, X_scaled, y_base)\n                if current_loss <= tau:\n                    steps_taken_in_phase = step\n                    break\n            \n            phase_adaptation_steps.append(steps_taken_in_phase)\n\n        return np.mean(phase_adaptation_steps)\n\n    # 4. Hyperparameters and Test Suite\n    hyperparams_gd = (0.001,)\n    hyperparams_adam = (0.01, 0.9, 0.999, 1e-8)\n\n    test_cases = [\n        {'name': 'Case A', 'scales': [1.0, 10.0, 100.0], 'T': 400},\n        {'name': 'Case B', 'scales': [1.0, 0.1, 10.0], 'T': 400},\n        {'name': 'Case C', 'scales': [1.0], 'T': 200},\n    ]\n\n    ratios = []\n    for case in test_cases:\n        curriculum = case['scales']\n        T = case['T']\n\n        avg_steps_gd = run_experiment('gd', X, y, curriculum, T, hyperparams_gd)\n        avg_steps_adam = run_experiment('adam', X, y, curriculum, T, hyperparams_adam)\n\n        if avg_steps_adam == 0.0:\n            # If Adam takes 0 steps, it's infinitely efficient. Ratio is 1 if GD also takes 0.\n            ratio = 1.0 if avg_steps_gd == 0.0 else np.inf\n        else:\n            ratio = avg_steps_gd / avg_steps_adam\n        \n        ratios.append(ratio)\n    \n    # 5. Final Output\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```", "id": "3096105"}, {"introduction": "While Adam is a powerful general-purpose optimizer, it is not without its pitfalls. This exercise presents a fascinating \"cautionary tale\" by constructing a specific non-convex problem where Adam's adaptive mechanism can lead it to a suboptimal solution, while simple Stochastic Gradient Descent (SGD) finds a better path. By analyzing this scenario, you will gain a deeper insight into how the second-moment estimate $v_t$ influences Adam's trajectory and develop a more nuanced understanding of its behavior.[@problem_id:3096082]", "problem": "You are asked to construct and analyze a nonconvex toy optimization problem that illustrates how Adaptive Moment Estimation (Adam) can converge to a suboptimal basin while Stochastic Gradient Descent (SGD) escapes, with the qualitative cause linked to direction changes induced by the second-moment accumulator $v_t$. Your work must be expressed as a complete, runnable program that implements both SGD and Adam on the same deterministic nonconvex objective and compares their behaviors on a small test suite.\n\nStart from the following fundamental base: minimizing a differentiable scalar objective $L$ over a one-dimensional parameter $x$ using iterative first-order updates derived from the negative gradient direction. Use standard definitions of exponential moving averages to implement Adam’s adaptive scaling without writing down its closed-form update formula here; your implementation should be consistent with widely used practice for Adam, including bias correction. For SGD, use a fixed learning rate.\n\nDefine the nonconvex objective as\n$$\nL(x) = (x^2 - 1)^2 - c\\,x,\n$$\nwith tilt parameter $c = 0.3$. The exact gradient is\n$$\n\\nabla L(x) = 4x(x^2 - 1) - c.\n$$\nTo model a deterministic mini-batch gradient estimator with structured variability that can produce direction changes, use a periodic perturbation sequence added to the exact gradient. For each iteration index $t \\in \\{0,1,2,\\dots\\}$ define a period $p = 10$ and a perturbation\n$$\n\\delta_t = \\begin{cases}\n-a, & \\text{if } t \\bmod p \\neq p-1,\\\\\n+b, & \\text{if } t \\bmod p = p-1,\n\\end{cases}\n$$\nso that the gradient estimator is $g_t(x) = \\nabla L(x) + \\delta_t$. Intuitively, many small negative nudges with occasional large positive spikes yield a positive average over a period if $b$ is sufficiently large compared to $a$, yet Adam’s $v_t$ can shrink the effect of spikes and alter the net direction.\n\nImplement both optimizers on this $g_t(x)$ with the following details:\n- Use $T$ iterations and initialize $x_0$ as specified per test case.\n- For SGD, use a constant step size $\\eta$.\n- For Adam, use step size $\\alpha$, first-moment parameter $\\beta_1$, second-moment parameter $\\beta_2$, and numerical stabilizer $\\varepsilon$. Include standard bias correction for both first and second moments.\n\nFor each run, classify the final iterate $x_T$ into a basin by its sign: left basin if $x_T < 0$ (suboptimal for the given $c$), right basin if $x_T > 0$ (better basin for the given $c$). For each test case, output a boolean indicating whether Adam ends in the left basin while SGD ends in the right basin.\n\nTest Suite:\nUse the following four parameter sets to test different facets of the behavior. For all cases, use $c = 0.3$, period $p = 10$, noise $a = 0.03$, spike $b = 4.0$, and perturbation as defined above.\n\n- Case $1$ (happy path): $x_0 = 0.1$, $T = 4000$, SGD: $\\eta = 0.01$, Adam: $\\alpha = 0.01$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon = 10^{-8}$.\n- Case $2$ (starting at a stationary-looking region): $x_0 = 0.0$, $T = 4000$, SGD: $\\eta = 0.01$, Adam: $\\alpha = 0.01$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon = 10^{-8}$.\n- Case $3$ (slightly negative start): $x_0 = -0.05$, $T = 4000$, SGD: $\\eta = 0.01$, Adam: $\\alpha = 0.01$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon = 10^{-8}$.\n- Case $4$ (boundary condition altering $v_t$ memory): $x_0 = 0.1$, $T = 4000$, SGD: $\\eta = 0.01$, Adam: $\\alpha = 0.01$, $\\beta_1 = 0.9$, $\\beta_2 = 0.9$, $\\varepsilon = 10^{-8}$.\n\nFinal Output Specification:\nYour program should produce a single line of output containing the boolean results for the four test cases in order, as a comma-separated list enclosed in square brackets (for example, $[{\\rm True},{\\rm False},{\\rm True},{\\rm True}]$). No additional text should be printed. All numbers are unitless in this problem. Angles are not used. Percentages are not used; any fractional quantities appear as decimals as specified above.", "solution": "The problem is valid. It presents a well-defined computational task in numerical optimization, grounded in established principles and algorithms. The objective is to construct and analyze a specific scenario where the Adam optimizer converges to a suboptimal solution while standard Stochastic Gradient Descent (SGD) finds a better one. This is a known phenomenon, and the problem provides a precise, deterministic framework to reproduce it. All parameters and conditions are explicitly stated, making the problem self-contained, unambiguous, and computationally verifiable.\n\nThe core of the problem lies in the interaction between a nonconvex objective function, $L(x)$, and a specially constructed gradient estimator, $g_t(x)$.\n\nThe objective function is a tilted double-well potential:\n$$\nL(x) = (x^2 - 1)^2 - c\\,x\n$$\nwith a tilt parameter $c = 0.3$. This function has two local minima. The term $-c\\,x$ with $c>0$ makes the minimum at positive $x$ the global minimum (the \"better\" basin) and the minimum at negative $x$ a suboptimal local minimum (the \"left\" basin). The task is to see if an optimizer, starting near $x=0$, can navigate to the better basin at $x>0$. The gradient of this function is:\n$$\n\\nabla L(x) = 4x(x^2 - 1) - c\n$$\n\nInstead of the true gradient, the optimizers use a perturbed gradient $g_t(x) = \\nabla L(x) + \\delta_t$. The perturbation $\\delta_t$ is deterministic and periodic with period $p=10$. For $9$ out of $10$ iterations, it provides a small negative nudge $\\delta_t = -a = -0.03$. In the $10^{th}$ iteration, it provides a large positive spike $\\delta_t = +b = 4.0$. The average perturbation over one period is $\\frac{9(-a) + b}{p} = \\frac{9(-0.03) + 4.0}{10} = 0.373$, which is positive.\n\nWe will implement and compare two first-order optimization algorithms:\n\n1.  **Stochastic Gradient Descent (SGD)**: The update rule is a simple step in the negative gradient direction with a fixed learning rate $\\eta$:\n    $$\n    x_{t+1} = x_t - \\eta g_t(x_t)\n    $$\n    For SGD, the update direction is solely determined by the sign of the current gradient estimate $g_t(x_t)$. While individual updates may point in different directions, the cumulative effect over many iterations determines the trajectory. Summing the displacements over one period (for small $x$), the net effect of the nine rightward pushes outweighs the single leftward push, directing SGD towards the right basin ($x>0$).\n\n2.  **Adaptive Moment Estimation (Adam)**: Adam adapts the learning rate for each parameter based on estimates of the first and second moments of the gradients. The update involves the following steps at iteration $t$:\n    -   Calculate the gradient estimate: $g_t = g_t(x_{t-1})$\n    -   Update the biased first moment (mean): $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n    -   Update the biased second moment (uncentered variance): $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n    -   Compute bias-corrected moments: $\\hat{m}_t = m_t / (1 - \\beta_1^t)$ and $\\hat{v}_t = v_t / (1 - \\beta_2^t)$\n    -   Update the parameter: $x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$\n\nThe key to Adam's behavior in this problem lies in the second moment accumulator, $v_t$. The large positive gradient spike ($g_t$ with $\\delta_t = +b$) creates a very large value for $g_t^2$. This inflates $v_t$. Due to the high value of $\\beta_2$ (e.g., $0.999$), this inflation persists for many subsequent iterations, causing the effective learning rate $\\alpha/(\\sqrt{\\hat{v}_t} + \\varepsilon)$ to become very small. The update corresponding to the spike itself pushes $x$ to the left. The subsequent nine updates, which would normally push $x$ to the right, are heavily suppressed in magnitude. Consequently, the single large leftward push dominates the nine tiny rightward pushes, causing Adam to converge to the suboptimal left basin ($x0$).\n\nCase $4$ tests this hypothesis by setting $\\beta_2=0.9$. With a shorter memory for the second moment, $v_t$ decays much faster after a spike. The learning rate suppression is short-lived, allowing the subsequent rightward pushes to have a greater effect. This should be sufficient for Adam to overcome the single leftward push and escape to the right basin, just like SGD.\n\nThe program will implement these two algorithms, run them for the four specified test cases, and determine for each case if the condition (Adam converges to the left basin and SGD converges to the right basin) is met. This requires simulating each optimizer for $T=4000$ steps and then classifying the sign of the final parameter value $x_T$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the optimization problem for the given test suite.\n    \"\"\"\n\n    # --- Problem Definitions ---\n\n    def grad_L(x, c):\n        \"\"\"Computes the exact gradient of the objective function L(x).\"\"\"\n        return 4.0 * x * (x**2 - 1.0) - c\n\n    def delta_t(t, p, a, b):\n        \"\"\"Computes the deterministic perturbation at iteration index t.\"\"\"\n        if (t % p) == (p - 1):\n            return b\n        else:\n            return -a\n\n    def run_sgd(x0, T, eta, c, p, a, b):\n        \"\"\"Runs the SGD optimization.\"\"\"\n        x = float(x0)\n        for t in range(T):\n            g_t = grad_L(x, c) + delta_t(t, p, a, b)\n            x -= eta * g_t\n        return x\n\n    def run_adam(x0, T, alpha, beta1, beta2, epsilon, c, p, a, b):\n        \"\"\"Runs the Adam optimization.\"\"\"\n        x = float(x0)\n        m = 0.0\n        v = 0.0\n        \n        # Use iterative updates for powers to maintain numerical stability for large T\n        beta1_power = 1.0\n        beta2_power = 1.0\n\n        for t in range(T):\n            g_t = grad_L(x, c) + delta_t(t, p, a, b)\n            \n            m = beta1 * m + (1.0 - beta1) * g_t\n            v = beta2 * v + (1.0 - beta2) * (g_t**2)\n            \n            # Iteration number is t+1 (1-indexed)\n            beta1_power *= beta1\n            beta2_power *= beta2\n\n            m_hat = m / (1.0 - beta1_power)\n            v_hat = v / (1.0 - beta2_power)\n            \n            update = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n            x -= update\n            \n        return x\n\n    # --- Test Execution ---\n    \n    # Common parameters from the problem statement\n    c_param = 0.3\n    p_param = 10\n    a_param = 0.03\n    b_param = 4.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (happy path)\n        {'x0': 0.1, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 2: (starting at a stationary-looking region)\n        {'x0': 0.0, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 3: (slightly negative start)\n        {'x0': -0.05, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 4: (boundary condition altering v_t memory)\n        {'x0': 0.1, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.9, 'adam_eps': 1e-8},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run SGD simulation\n        x_sgd = run_sgd(case['x0'], case['T'], case['sgd_eta'], c_param, p_param, a_param, b_param)\n        \n        # Run Adam simulation\n        x_adam = run_adam(case['x0'], case['T'], case['adam_alpha'], case['adam_beta1'], case['adam_beta2'], case['adam_eps'], c_param, p_param, a_param, b_param)\n        \n        # Classify basins based on the sign of the final iterate\n        adam_in_left_basin = x_adam  0\n        sgd_in_right_basin = x_sgd  0\n        \n        # Determine if the specified condition is met\n        condition_met = adam_in_left_basin and sgd_in_right_basin\n        results.append(condition_met)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3096082"}, {"introduction": "Having explored a clear strength and a specific weakness of Adam, we now turn to a more practical and complex comparison in the context of sparse recovery. This practice challenges you to set up a fair empirical experiment comparing Adam and SGD on a linear regression task with a smooth $\\ell_1$ penalty, a common approach for finding sparse solutions. Your goal is to determine which optimizer more effectively identifies the true underlying features within a fixed computational budget, honing your skills in implementing and evaluating optimization algorithms for realistic machine learning problems.[@problem_id:3096054]", "problem": "You are asked to empirically evaluate whether Adaptive Moment Estimation (Adam) hastens sparse recovery compared to Stochastic Gradient Descent (SGD) when both are given the same gradient computation budget. The comparison must be made on a synthetic linear regression task with an explicitly specified smooth approximation to the $\\ell_1$ regularizer.\n\nStart from the following foundational base:\n- Empirical risk minimization seeks to choose parameters $\\mathbf{w} \\in \\mathbb{R}^d$ to minimize a sample-average loss plus regularization.\n- The squared error loss for linear models is a well-tested objective: for data $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and responses $\\mathbf{y} \\in \\mathbb{R}^n$, the empirical mean squared error is $\\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2$.\n- Sparsity can be promoted via an $\\ell_1$-type penalty, but to enable gradient-based algorithms, you must use a smooth approximation to $\\ell_1$.\n\nYour program must implement the following experiment:\n1. Generate a synthetic dataset for each test case:\n   - Draw a design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with independent standard normal entries and then scale each column to unit $\\ell_2$ norm.\n   - Construct a $K$-sparse ground-truth vector $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ by choosing $K$ indices uniformly at random and assigning nonzero values drawn from a distribution with magnitudes on the order of $1$ and random signs. The remaining entries are zero.\n   - Generate responses $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$ where $\\boldsymbol{\\varepsilon}$ has independent Gaussian entries with mean $0$ and standard deviation $\\sigma$.\n2. Define the objective function\n   $$L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2},$$\n   where the penalty term is a smooth approximation to $\\|\\mathbf{w}\\|_1$ using the function $w \\mapsto \\sqrt{w^2 + \\beta^2}$ with smoothing parameter $\\beta > 0$.\n3. Use mini-batch stochastic gradients with batch size $b$ to update $\\mathbf{w}$ from the zero vector initialization:\n   - For SGD: at each iteration $t$, update $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\widehat{\\nabla} L(\\mathbf{w}_t)$ using the mini-batch gradient estimate.\n   - For Adam: at each iteration $t$, update using the standard Adam rule with step size $\\eta_{\\text{Adam}}$ and fixed hyperparameters. Do not alter the gradient budget relative to SGD; both methods must perform the same number of iterations $T$ with the same batch size $b$, using the same sequence of mini-batches for fairness.\n4. Define the estimated support at iteration $t$ as $S_t = \\{ i \\in \\{1,\\dots,d\\} : |w_{t,i}| \\ge \\theta \\}$ and the true support as $S^\\star = \\{ i : w^\\star_i \\ne 0 \\}$.\n5. Define the earliest recovery time $t_{\\text{SGD}}$ (respectively $t_{\\text{Adam}}$) as the smallest $t \\in \\{1,2,\\dots,T\\}$ such that $S_t = S^\\star$ for SGD (respectively Adam). If no such $t$ exists within the budget, set $t$ to $+\\infty$.\n6. Your program must decide if Adam hastens sparse recovery compared to SGD under the same budget, using this rule:\n   - If at least one method exactly recovers the support within the budget, declare Adam hastens recovery if $t_{\\text{Adam}} \\le t_{\\text{SGD}}$.\n   - If neither method exactly recovers within the budget, compute the final iteration $t=T$ supports and compare the F1-score for the support sets, where for sets $A$ and $B$, the F1-score is defined as\n     $$\\text{F1}(A,B) = \\frac{2|A \\cap B|}{2|A \\cap B| + |A \\setminus B| + |B \\setminus A|}.$$\n     In this case, declare Adam hastens recovery if $\\text{F1}(S_T^{\\text{Adam}}, S^\\star) > \\text{F1}(S_T^{\\text{SGD}}, S^\\star)$.\n7. The random number generation must be deterministic per test case. Use a fixed seed per case so results are reproducible. The mini-batch sequence must be identical for SGD and Adam.\n\nImplementations must adhere to the foundational definitions without relying on any ungrounded shortcuts. In particular, derive the gradient of the smooth $\\ell_1$ penalty from first principles.\n\nTest suite:\n- Case $1$: $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (50,200,5,0.05,0.05,10^{-3},20,0.05,0.01,400,0.05)$\n- Case $2$: $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (60,120,8,0.2,0.1,10^{-2},30,0.03,0.01,600,0.08)$\n- Case $3$: $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (40,100,4,0,0.02,10^{-3},25,0.04,0.01,300,0.04)$\n- Case $4$: $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (80,200,10,0.15,0.15,5\\times10^{-2},40,0.02,0.005,800,0.1)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\"), where each boolean corresponds to the decision for the respective test case in the order above.", "solution": "The user problem is an empirical study to compare the performance of the Adam optimizer against Stochastic Gradient Descent (SGD) on a sparse recovery task. The task is formulated as a linear regression problem with a smooth approximation of the $\\ell_1$ regularizer.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n\n- **Objective Function:** $L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2}$ for parameters $\\mathbf{w} \\in \\mathbb{R}^d$.\n- **Synthetic Data Generation:**\n    - Design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ from independent $\\mathcal{N}(0,1)$ entries, with columns scaled to unit $\\ell_2$ norm.\n    - Ground-truth vector $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ is $K$-sparse. The $K$ non-zero positions are chosen uniformly at random. Non-zero values have magnitudes of order $1$ and random signs.\n    - Response vector $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$, with noise $\\boldsymbol{\\varepsilon}$ having i.i.d. entries from $\\mathcal{N}(0, \\sigma^2)$.\n- **Optimization Algorithms:**\n    - Mini-batch stochastic gradients with batch size $b$ and total iterations $T$.\n    - Initialization: $\\mathbf{w}_0 = \\mathbf{0}$.\n    - SGD update: $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\widehat{\\nabla} L(\\mathbf{w}_t)$.\n    - Adam update: Standard Adam rule with step size $\\eta_{\\text{Adam}}$.\n    - Both methods use the identical sequence of mini-batches.\n- **Evaluation Criteria:**\n    - True support: $S^\\star = \\{ i : w^\\star_i \\ne 0 \\}$.\n    - Estimated support at iteration $t$: $S_t = \\{ i : |w_{t,i}| \\ge \\theta \\}$.\n    - Earliest recovery time $t_{\\text{opt}}$: Smallest $t \\in \\{1,\\dots,T\\}$ where $S_t = S^\\star$. If no such $t$ exists, $t_{\\text{opt}} = +\\infty$.\n- **Comparison Rule:**\n    - If at least one method recovers the support (i.e., $\\min(t_{\\text{SGD}}, t_{\\text{Adam}})  +\\infty$): Adam hastens recovery if $t_{\\text{Adam}} \\le t_{\\text{SGD}}$.\n    - If neither method recovers: Adam hastens recovery if $\\text{F1}(S_T^{\\text{Adam}}, S^\\star) > \\text{F1}(S_T^{\\text{SGD}}, S^\\star)$.\n    - F1-score: $\\text{F1}(A,B) = \\frac{2|A \\cap B|}{2|A \\cap B| + |A \\setminus B| + |B \\setminus A|}$.\n- **Reproducibility:** A fixed random seed must be used for each test case.\n- **Test Cases:** Four cases with specific parameter sets for $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta)$.\n\n**1.2. Validation and Verdict**\n\n- **Scientific Grounding:** The problem is firmly rooted in the established principles of statistical learning, optimization, and sparse methods. All components (linear regression, $\\ell_1$ regularization, SGD, Adam) are standard. The use of a smooth approximation for the non-differentiable $\\ell_1$ norm is a well-known technique.\n- **Well-Posedness:** The problem is well-posed. The objective is clearly defined, the experimental procedure is deterministic due to fixed seeds, and the comparison metric is unambiguous, covering all possible outcomes. A unique, meaningful solution exists for each test case.\n- **Objectivity:** The problem is stated in precise, quantitative terms, free from subjective or ambiguous language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or logical contradiction. The term \"magnitudes on the order of 1\" is a standard convention in synthetic data generation, interpretable as sampling from a distribution like a uniform distribution around $1$, and does not render the problem invalid.\n\n**Verdict: The problem is valid.**\n\n### Step 2: Solution Derivation and Algorithmic Design\n\nThe core of the problem is to minimize the objective function $L(\\mathbf{w})$ using two different gradient-based optimizers and compare their effectiveness in identifying the sparse support of the true parameter vector $\\mathbf{w}^\\star$.\n\n**2.1. Objective Function and Gradient**\n\nThe objective function is\n$$L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2}$$\nThis function is a sum of a data fidelity term, $L_{\\text{data}}(\\mathbf{w})$, and a regularization term, $L_{\\text{reg}}(\\mathbf{w})$. To apply gradient-based optimizers, we must compute the gradient $\\nabla L(\\mathbf{w}) = \\nabla L_{\\text{data}}(\\mathbf{w}) + \\nabla L_{\\text{reg}}(\\mathbf{w})$.\n\nThe gradient of the data fidelity term (mean squared error) is:\n$$ \\nabla L_{\\text{data}}(\\mathbf{w}) = \\frac{1}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w} - \\mathbf{y}) $$\nThe gradient of the regularization term is found by differentiating with respect to each component $w_j$ for $j \\in \\{1, \\dots, d\\}$:\n$$ \\frac{\\partial}{\\partial w_j} L_{\\text{reg}}(\\mathbf{w}) = \\lambda \\frac{\\partial}{\\partial w_j} \\sqrt{w_j^2 + \\beta^2} = \\lambda \\frac{1}{2\\sqrt{w_j^2 + \\beta^2}} (2w_j) = \\frac{\\lambda w_j}{\\sqrt{w_j^2 + \\beta^2}} $$\nThe full gradient of the regularization term is a vector where each component is given by the above expression. This can be written compactly using element-wise operations:\n$$ \\nabla L_{\\text{reg}}(\\mathbf{w}) = \\lambda \\frac{\\mathbf{w}}{\\sqrt{\\mathbf{w}^2 + \\beta^2}} $$\nwhere the square and square root are applied element-wise. Since $\\beta > 0$, the denominator is always strictly positive, ensuring the gradient is well-defined.\n\n**2.2. Stochastic Mini-batch Gradient**\n\nIn practice, for large $n$, we use a stochastic approximation of the gradient computed on a mini-batch of data. Let $I \\subset \\{1, \\dots, n\\}$ be a set of indices of size $b$, defining a mini-batch. The stochastic gradient $\\widehat{\\nabla} L(\\mathbf{w})$ is:\n$$ \\widehat{\\nabla} L(\\mathbf{w}) = \\frac{1}{b} \\mathbf{X}_I^T (\\mathbf{X}_I \\mathbf{w} - \\mathbf{y}_I) + \\lambda \\frac{\\mathbf{w}}{\\sqrt{\\mathbf{w}^2 + \\beta^2}} $$\nwhere $\\mathbf{X}_I$ and $\\mathbf{y}_I$ are the rows of $\\mathbf{X}$ and entries of $\\mathbf{y}$ corresponding to the indices in $I$. The regularization part of the gradient is computed using the full parameter vector $\\mathbf{w}$ and is not stochastic.\n\n**2.3. Optimization Algorithms**\n\nBoth algorithms start with $\\mathbf{w}_0 = \\mathbf{0}$ and are run for $T$ iterations. For each iteration $t=0, 1, \\dots, T-1$, they compute a stochastic gradient $\\mathbf{g}_t = \\widehat{\\nabla} L(\\mathbf{w}_t)$ using the same mini-batch.\n\n- **Stochastic Gradient Descent (SGD):** The update rule is a simple step in the negative gradient direction:\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\mathbf{g}_t $$\n\n- **Adaptive Moment Estimation (Adam):** Adam maintains moving averages of the gradient and its square. Let $\\beta_1=0.9$, $\\beta_2=0.999$, and $\\epsilon=10^{-8}$ be the standard hyperparameters.\n    1. Initialize first moment vector $\\mathbf{m}_0 = \\mathbf{0}$ and second moment vector $\\mathbf{v}_0 = \\mathbf{0}$.\n    2. Update biased moment estimates:\n    $$ \\mathbf{m}_{t+1} = \\beta_1 \\mathbf{m}_t + (1 - \\beta_1) \\mathbf{g}_t $$\n    $$ \\mathbf{v}_{t+1} = \\beta_2 \\mathbf{v}_t + (1 - \\beta_2) \\mathbf{g}_t^2 \\quad \\text{(element-wise square)} $$\n    3. Compute bias-corrected moment estimates:\n    $$ \\hat{\\mathbf{m}}_{t+1} = \\frac{\\mathbf{m}_{t+1}}{1 - \\beta_1^{t+1}} $$\n    $$ \\hat{\\mathbf{v}}_{t+1} = \\frac{\\mathbf{v}_{t+1}}{1 - \\beta_2^{t+1}} $$\n    4. Update the parameters:\n    $$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{Adam}} \\frac{\\hat{\\mathbf{m}}_{t+1}}{\\sqrt{\\hat{\\mathbf{v}}_{t+1}} + \\epsilon} $$\n\n**2.4. Experimental Procedure and Evaluation**\n\nFor each test case, the following procedure is executed:\n1.  **Setup:** A specific random seed is used to ensure reproducibility.\n2.  **Data Generation:** The synthetic dataset $(\\mathbf{X}, \\mathbf{y})$ and the ground-truth sparse vector $\\mathbf{w}^\\star$ are generated as specified. The true support $S^\\star$ is recorded.\n3.  **Iteration:** Both SGD and Adam are run in parallel for $T$ iterations. In each iteration $t$, the same mini-batch is used to compute the gradients for both updates.\n4.  **Support Recovery Tracking:** After each parameter update (producing $\\mathbf{w}_{t+1}$), the estimated support $S_{t+1}$ is computed by thresholding the absolute values of the weights with $\\theta$. This is compared to the true support $S^\\star$. If they match, the recovery time (iteration number $t+1$) is recorded. This is done for both optimizers. The initial recovery times are set to $+\\infty$.\n5.  **Comparison:** After $T$ iterations, the final decision is made based on the specified rule:\n    - If at least one optimizer found the support, the one with the smaller (or equal, for Adam) recovery time wins.\n    - If neither found the support, their performance is compared using the F1-score of their final estimated supports against the true support. The optimizer with the higher F1-score wins.\nThe F1-score for an estimated support set $S$ and true support $S^\\star$ is computed as:\n$$ \\text{F1}(S, S^\\star) = \\frac{2|S \\cap S^\\star|}{|S| + |S^\\star|} $$\nThis is equivalent to the formula given in the problem statement. A boolean value is returned indicating if Adam is considered to have hastened recovery relative to SGD.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (50, 200, 5, 0.05, 0.05, 1e-3, 20, 0.05, 0.01, 400, 0.05),\n        (60, 120, 8, 0.2, 0.1, 1e-2, 30, 0.03, 0.01, 600, 0.08),\n        (40, 100, 4, 0, 0.02, 1e-3, 25, 0.04, 0.01, 300, 0.04),\n        (80, 200, 10, 0.15, 0.15, 5e-2, 40, 0.02, 0.005, 800, 0.1),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        decision = solve_one_case(params, seed=i)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_one_case(params, seed):\n    \"\"\"\n    Executes the experiment for a single set of parameters.\n    \"\"\"\n    d, n, K, sigma, lamb, beta, b, eta_sgd, eta_adam, T, theta = params\n    \n    # Use the case index as a seed for reproducibility\n    np.random.seed(seed)\n\n    # 1. Generate synthetic dataset\n    X = np.random.randn(n, d)\n    X /= np.linalg.norm(X, axis=0, keepdims=True)\n\n    w_star = np.zeros(d)\n    support_indices = np.random.choice(d, K, replace=False)\n    signs = np.random.choice([-1, 1], K)\n    magnitudes = np.random.uniform(0.5, 1.5, K)\n    w_star[support_indices] = signs * magnitudes\n    \n    S_star_set = set(support_indices)\n\n    noise = np.random.randn(n) * sigma\n    y = X @ w_star + noise\n\n    # Pre-generate mini-batch indices for all T iterations to ensure fairness\n    mini_batch_indices = [np.random.choice(n, b, replace=False) for _ in range(T)]\n\n    # Initialize parameters for both optimizers\n    w_sgd = np.zeros(d)\n    w_adam = np.zeros(d)\n    \n    # Adam-specific parameters\n    m = np.zeros(d)\n    v = np.zeros(d)\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    t_sgd_recovery = np.inf\n    t_adam_recovery = np.inf\n\n    for t in range(T):\n        # Get the same mini-batch for both optimizers\n        batch_idx = mini_batch_indices[t]\n        X_b, y_b = X[batch_idx], y[batch_idx]\n\n        # --- SGD Step ---\n        grad_data_sgd = (1/b) * X_b.T @ (X_b @ w_sgd - y_b)\n        grad_reg_sgd = lamb * w_sgd / np.sqrt(w_sgd**2 + beta**2)\n        grad_sgd = grad_data_sgd + grad_reg_sgd\n        w_sgd -= eta_sgd * grad_sgd\n        \n        # --- Adam Step ---\n        grad_data_adam = (1/b) * X_b.T @ (X_b @ w_adam - y_b)\n        grad_reg_adam = lamb * w_adam / np.sqrt(w_adam**2 + beta**2)\n        grad_adam = grad_data_adam + grad_reg_adam\n\n        m = beta1 * m + (1 - beta1) * grad_adam\n        v = beta2 * v + (1 - beta2) * (grad_adam**2)\n        \n        # Bias correction\n        m_hat = m / (1 - beta1**(t + 1))\n        v_hat = v / (1 - beta2**(t + 1))\n        \n        w_adam -= eta_adam * m_hat / (np.sqrt(v_hat) + epsilon)\n\n        # --- Evaluation at iteration t+1 ---\n        # Check for SGD recovery\n        if np.isinf(t_sgd_recovery):\n            S_sgd_current = set(np.where(np.abs(w_sgd) = theta)[0])\n            if S_sgd_current == S_star_set:\n                t_sgd_recovery = t + 1\n        \n        # Check for Adam recovery\n        if np.isinf(t_adam_recovery):\n            S_adam_current = set(np.where(np.abs(w_adam) = theta)[0])\n            if S_adam_current == S_star_set:\n                t_adam_recovery = t + 1\n\n    # Final comparison logic\n    if min(t_sgd_recovery, t_adam_recovery)  np.inf:\n        return t_adam_recovery = t_sgd_recovery\n    else:\n        # Neither recovered, compare F1 scores at the final iteration\n        S_T_sgd = set(np.where(np.abs(w_sgd) = theta)[0])\n        S_T_adam = set(np.where(np.abs(w_adam) = theta)[0])\n        \n        def f1_score(A, B):\n            intersection_size = len(A.intersection(B))\n            if len(A) == 0 and len(B) == 0:\n                return 1.0\n            denominator = len(A) + len(B)\n            if denominator == 0:\n                return 0.0 # Should not happen if K  0 and one set is not empty\n            \n            return 2.0 * intersection_size / denominator\n\n        f1_sgd = f1_score(S_T_sgd, S_star_set)\n        f1_adam = f1_score(S_T_adam, S_star_set)\n        \n        return f1_adam  f1_sgd\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3096054"}]}