{"hands_on_practices": [{"introduction": "The power of the Transformer architecture stems from its global attention mechanism, which allows every token to interact with every other token in a sequence. However, this comes at a quadratic computational cost, making it challenging for very long sequences. This practice explores a common solution: local windowed attention. By implementing both global and local attention from first principles [@problem_id:3100324], you will quantitatively measure the approximation error, gaining a hands-on understanding of the fundamental trade-off between computational efficiency and model expressiveness.", "problem": "You will compare global attention and local windowed attention in a single-head attention setting by quantifying the approximation error introduced when restricting attention to a finite window. You will implement both mechanisms from first principles and compute an error metric as a function of the window half-width $w$.\n\nDefinitions and setup:\n- Let the sequence length be $n \\in \\mathbb{N}$. Positions are indexed by $i,j \\in \\{1,\\dots,n\\}$.\n- Define the attention logit function $s_{i,j}$ as\n$$\ns_{i,j} = -\\frac{(i-j)^2}{2\\sigma^2} + \\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\},\n$$\nwhere $\\sigma > 0$ controls locality, $\\alpha \\in \\mathbb{R}$ boosts a single target index $j = n-i+1$ to encode a long-range dependency, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- Given a temperature $\\tau > 0$, define the global attention weights\n$$\na_{i,j} = \\frac{\\exp\\!\\left(s_{i,j}/\\tau\\right)}{\\sum_{k=1}^{n} \\exp\\!\\left(s_{i,k}/\\tau\\right)}.\n$$\n- Define the local windowed attention weights with half-width $w \\in \\mathbb{N}\\cup\\{0\\}$ by normalizing only over indices in the window $\\{j:\\ |i-j|\\le w\\}$:\n$$\n\\tilde{a}_{i,j}^{(w)} =\n\\begin{cases}\n\\frac{\\exp\\!\\left(s_{i,j}/\\tau\\right)}{\\sum\\limits_{k:\\ |i-k|\\le w} \\exp\\!\\left(s_{i,k}/\\tau\\right)} & \\text{if } |i-j| \\le w,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\n- Let the values be the scalar signal\n$$\nv_j = \\sin\\!\\left(\\omega j\\right)\\quad\\text{with}\\quad \\omega = \\frac{2\\pi}{n},\n$$\nwith angles measured in radians.\n- The global attention output is\n$$\no_i = \\sum_{j=1}^{n} a_{i,j} \\, v_j,\n$$\nand the local windowed output is\n$$\n\\tilde{o}_i^{(w)} = \\sum_{j=1}^{n} \\tilde{a}_{i,j}^{(w)} \\, v_j.\n$$\n- Define the mean squared approximation error as a function of $w$:\n$$\nE(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(o_i - \\tilde{o}_i^{(w)}\\right)^2.\n$$\n\nTask:\n- From these definitions, implement a program that computes $E(w)$ for each test case provided below. No stochastic elements are allowed; all computations must be deterministic.\n- Angles for the sine function must be in radians.\n\nTest suite (each case is $(n,\\sigma,\\tau,\\alpha,w)$):\n- Case $1$: $(32, 4.0, 1.0, 2.0, 3)$.\n- Case $2$: $(32, 4.0, 1.0, 2.0, 0)$.\n- Case $3$: $(32, 4.0, 1.0, 2.0, 31)$.\n- Case $4$: $(32, 2.0, 1.0, 6.0, 3)$.\n- Case $5$: $(64, 8.0, 0.5, 3.0, 5)$.\n\nAnswer specification:\n- For each test case, output the single float $E(w)$ rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above (for example, $[x_1,x_2,x_3,x_4,x_5]$ where each $x_k$ is the rounded value of $E(w)$ for case $k$).", "solution": "The problem requires a quantitative comparison between global and local windowed attention mechanisms by calculating the mean squared approximation error, $E(w)$, for several parameter sets. The solution will be derived by first principles, following the mathematical definitions provided.\n\nAt the core, we must compute two quantities for each query position $i$: the global attention output $o_i$ and the local windowed attention output $\\tilde{o}_i^{(w)}$. The error $E(w)$ is then the mean squared difference between these two output sequences.\n\n**1. Preliminaries: Indices and Value Signal**\n\nThe problem uses $1$-based indexing for a sequence of length $n$, where positions are indexed by $i, j \\in \\{1, \\dots, n\\}$. The value signal, which is to be aggregated by the attention mechanism, is a sinusoidal function of position:\n$$\nv_j = \\sin(\\omega j) \\quad \\text{with} \\quad \\omega = \\frac{2\\pi}{n}\n$$\nWe can precompute the vector of values $v = (v_1, v_2, \\dots, v_n)$ for use in later steps.\n\n**2. Attention Logits**\n\nBoth attention mechanisms utilize the same attention logit function, $s_{i,j}$, which scores the relevance of key position $j$ with respect to query position $i$:\n$$\ns_{i,j} = -\\frac{(i-j)^2}{2\\sigma^2} + \\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\}\n$$\nThis function has two components:\n- A Gaussian-like term $-\\frac{(i-j)^2}{2\\sigma^2}$ that assigns higher scores to positions $j$ that are closer to $i$. The parameter $\\sigma$ controls the width of this locality bias.\n- A long-range term $\\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\}$ that adds a specific bonus $\\alpha$ if position $j$ is symmetric to position $i$ relative to the sequence ends. This explicitly models a specific non-local dependency.\n\nFor implementation, we can construct an $n \\times n$ matrix $S$ where $S_{i,j} = s_{i,j}$.\n\n**3. Global Attention**\n\nThe global attention weights, $a_{i,j}$, are computed by applying a softmax function over all possible key positions $j \\in \\{1, \\dots, n\\}$, scaled by a temperature parameter $\\tau > 0$:\n$$\na_{i,j} = \\frac{\\exp(s_{i,j}/\\tau)}{\\sum_{k=1}^{n} \\exp(s_{i,k}/\\tau)}\n$$\nFor numerical stability, especially when some logits $s_{i,j}/\\tau$ are large, we use the log-sum-exp trick. For each query $i$, let $m_i = \\max_{k} (s_{i,k}/\\tau)$. The formula can be rewritten as:\n$$\na_{i,j} = \\frac{\\exp(s_{i,j}/\\tau - m_i)}{\\sum_{k=1}^{n} \\exp(s_{i,k}/\\tau - m_i)}\n$$\nThis prevents floating-point overflow. After computing the $n \\times n$ attention weight matrix $A = [a_{i,j}]$, the global attention output vector $o = (o_1, \\dots, o_n)$ is calculated as the matrix-vector product of $A$ and the value vector $v$:\n$$\no_i = \\sum_{j=1}^{n} a_{i,j} v_j \\quad \\implies \\quad o = A v\n$$\n\n**4. Local Windowed Attention**\n\nThe local windowed attention mechanism restricts the softmax normalization to a window of half-width $w$ around the query position $i$. The set of keys considered for query $i$ is $W_i = \\{k \\in \\{1, \\dots, n\\} : |i-k| \\le w\\}$. The local attention weights $\\tilde{a}_{i,j}^{(w)}$ are defined as:\n$$\n\\tilde{a}_{i,j}^{(w)} =\n\\begin{cases}\n\\frac{\\exp(s_{i,j}/\\tau)}{\\sum_{k \\in W_i} \\exp(s_{i,k}/\\tau)} & \\text{if } j \\in W_i,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nComputationally, this can be implemented efficiently. For each query $i$, we first identify the indices $k \\in W_i$. We then apply the stable softmax function only to the logits $\\{s_{i,k}/\\tau : k \\in W_i\\}$. The resulting weights are placed into the corresponding positions of the $i$-th row of the local attention matrix $\\tilde{A}^{(w)}$, with all other entries in that row being $0$.\n\nAn equivalent and vectorizable approach involves creating an $n \\times n$ boolean mask that is `true` for pairs $(i, j)$ where $|i-j| \\le w$. We can then create a modified logit matrix where entries outside the window are set to $-\\infty$. Applying the standard stable softmax procedure to this masked matrix directly yields $\\tilde{A}^{(w)}$, since $\\exp(-\\infty)=0$.\n\nThe local windowed output vector $\\tilde{o}^{(w)} = (\\tilde{o}_1^{(w)}, \\dots, \\tilde{o}_n^{(w)})$ is then computed similarly to the global case:\n$$\n\\tilde{o}_i^{(w)} = \\sum_{j=1}^{n} \\tilde{a}_{i,j}^{(w)} v_j \\quad \\implies \\quad \\tilde{o}^{(w)} = \\tilde{A}^{(w)} v\n$$\n\n**5. Mean Squared Approximation Error**\n\nFinally, the approximation error $E(w)$ is the mean of the squared differences between the corresponding elements of the global and local output vectors:\n$$\nE(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(o_i - \\tilde{o}_i^{(w)}\\right)^2\n$$\nThis metric quantifies how well the local attention mechanism approximates the global one. If $w$ is large enough to cover the entire sequence (i.e., $w \\ge n-1$), the local attention becomes identical to global attention, and the error $E(w)$ will be $0$.\n\nThe implementation will follow these steps for each test case, using vectorized operations in `numpy` for efficiency.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean squared approximation error between global and local windowed\n    attention for a series of test cases.\n    \"\"\"\n\n    def compute_E(n: int, sigma: float, tau: float, alpha: float, w: int) -> float:\n        \"\"\"\n        Calculates the mean squared approximation error E(w) for a given set of parameters.\n\n        Args:\n            n: Sequence length.\n            sigma: Standard deviation for the locality-based logit term.\n            tau: Temperature for softmax.\n            alpha: Coefficient for the long-range dependency term.\n            w: Half-width of the local attention window.\n\n        Returns:\n            The computed error E(w) as a float.\n        \"\"\"\n        # Step 1: Define 1-based index arrays for vectorized computation.\n        # i_vals will be a column vector (n, 1) and j_vals a row vector (1, n).\n        i_vals = np.arange(1, n + 1, dtype=float).reshape(n, 1)\n        j_vals = np.arange(1, n + 1, dtype=float).reshape(1, n)\n\n        # Step 2: Calculate the n x n attention logit matrix S.\n        # s_{i,j} = -(i-j)^2 / (2*sigma^2) + alpha * 1{j = n-i+1}\n        indicator = (j_vals == (n - i_vals + 1)).astype(float)\n        S = -((i_vals - j_vals)**2) / (2 * sigma**2) + alpha * indicator\n\n        # Step 3: Calculate the value vector v.\n        # v_j = sin(omega * j) with omega = 2*pi/n\n        omega = 2 * np.pi / n\n        # Use np.arange(1, n+1) for 1-based j indices.\n        v = np.sin(omega * np.arange(1, n + 1, dtype=float))\n\n        # Shared step: Scale logits by temperature.\n        S_scaled = S / tau\n\n        # --- Part 1: Global Attention ---\n        \n        # Step 4: Compute global attention weights A using a numerically stable softmax.\n        max_logits_global = np.max(S_scaled, axis=1, keepdims=True)\n        exp_logits_global = np.exp(S_scaled - max_logits_global)\n        sum_exp_logits_global = np.sum(exp_logits_global, axis=1, keepdims=True)\n        A = exp_logits_global / sum_exp_logits_global\n        \n        # Step 5: Compute the global attention output vector o.\n        o = A @ v\n\n        # --- Part 2: Local Windowed Attention ---\n\n        # Step 6: Create a boolean mask for the local window where |i-j| <= w.\n        window_mask = np.abs(i_vals - j_vals) <= w\n        \n        # Step 7: Compute local attention weights Atilde. First, mask the scaled logits.\n        # Values outside the window become -inf, ensuring they are zero after softmax.\n        S_scaled_local = np.where(window_mask, S_scaled, -np.inf)\n        \n        # Apply stable softmax to the windowed logits.\n        max_logits_local = np.max(S_scaled_local, axis=1, keepdims=True)\n        exp_logits_local = np.exp(S_scaled_local - max_logits_local)\n        sum_exp_logits_local = np.sum(exp_logits_local, axis=1, keepdims=True)\n        \n        # The sum won't be zero because each window contains at least the diagonal element.\n        Atilde = exp_logits_local / sum_exp_logits_local\n        \n        # Step 8: Compute the local windowed attention output vector otilde.\n        otilde = Atilde @ v\n\n        # --- Part 3: Error Calculation ---\n\n        # Step 9: Calculate the mean squared approximation error E(w).\n        error = np.mean((o - otilde)**2)\n        \n        return error\n\n    # Test suite from the problem statement\n    test_cases = [\n        (32, 4.0, 1.0, 2.0, 3),   # Case 1\n        (32, 4.0, 1.0, 2.0, 0),   # Case 2\n        (32, 4.0, 1.0, 2.0, 31),  # Case 3\n        (32, 2.0, 1.0, 6.0, 3),   # Case 4\n        (64, 8.0, 0.5, 3.0, 5),   # Case 5\n    ]\n\n    results = []\n    for params in test_cases:\n        # Unpack parameters and compute the result for each case\n        result = compute_E(*params)\n        results.append(result)\n\n    # Format the final output string as per the specification\n    # e.g., [0.123456,0.789012,...]\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n```", "id": "3100324"}, {"introduction": "After a Transformer model is trained, how can we interpret what it has learned? The attention maps themselves offer a window into the model's internal workings. In this exercise [@problem_id:3100381], you will use Shannon entropy, a powerful concept from information theory, to quantify the \"focus\" of attention distributions. By analyzing how the average entropy $H_{\\ell}$ changes across different layers $\\ell$, you will investigate the hypothesis that attention becomes more specialized and less uniform in deeper layers of the network.", "problem": "Consider a simplified multi-head attention mechanism as used in Transformer models within statistical learning. For each layer index $\\ell \\in \\{1,\\dots,L\\}$ and head index $h \\in \\{1,\\dots,H\\}$, the attention is represented by a matrix $A^{(\\ell,h)} \\in \\mathbb{R}^{Q \\times K}$, where each row of $A^{(\\ell,h)}$ is a probability distribution over $K$ keys for one of the $Q$ queries, produced by the softmax transformation of compatibility scores. The softmax transforms a real-valued score vector $s \\in \\mathbb{R}^K$ into a probability vector $p \\in \\mathbb{R}^K$ with components $p_i = \\exp(s_i) \\big/ \\sum_{j=1}^K \\exp(s_j)$, ensuring non-negativity and that the components sum to $1$. For a probability vector $p = (p_1,\\dots,p_K)$, the Shannon entropy in natural units (nats) is $H(p) = -\\sum_{i=1}^K p_i \\log p_i$, with the convention that $0 \\log 0$ is taken as $0$ by continuity.\n\nDefine the per-layer average attention entropy $H_\\ell$ as the average of the row-wise entropies across all heads and queries:\n$$\nH_\\ell = \\frac{1}{H Q} \\sum_{h=1}^H \\sum_{q=1}^Q H\\Big(A^{(\\ell,h)}_{q,:}\\Big),\n$$\nwhere $A^{(\\ell,h)}_{q,:}$ denotes the $q$-th row of $A^{(\\ell,h)}$. We say that a model exhibits lower entropy in deeper layers if $H_1 > H_2 > \\dots > H_L$ holds with strict inequalities. To make the decision robust to floating-point rounding, use a tolerance $\\varepsilon = 10^{-9}$ and interpret $H_i > H_{i+1}$ as $H_i - H_{i+1} > \\varepsilon$.\n\nYour task is to write a complete, runnable program that computes $H_\\ell$ for each layer $\\ell$ for a set of test cases, decides whether the entropies strictly decrease with depth under the stated tolerance, and outputs a single line containing a comma-separated list of boolean values enclosed in square brackets. Each boolean corresponds to one test case and indicates whether deeper layers exhibit lower entropy on average.\n\nUse the following test suite. Each test case specifies the integers $L$, $H$, $Q$, $K$, and the attention matrices $A^{(\\ell,h)}$. All entropies must be computed in nats, and the output must be booleans only.\n\nTest Case $1$ (general case; expected to decrease):\n- $L = 3$, $H = 2$, $Q = 3$, $K = 4$.\n- Layer $\\ell = 1$, head $h = 1$:\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.40 & 0.30 & 0.20 & 0.10 \\\\\n0.30 & 0.30 & 0.20 & 0.20\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 1$, head $h = 2$:\n$$\nA^{(1,2)} =\n\\begin{bmatrix}\n0.35 & 0.25 & 0.20 & 0.20 \\\\\n0.30 & 0.30 & 0.20 & 0.20 \\\\\n0.25 & 0.25 & 0.25 & 0.25\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 2$, head $h = 1$:\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.60 & 0.20 & 0.10 & 0.10 \\\\\n0.70 & 0.10 & 0.10 & 0.10 \\\\\n0.50 & 0.30 & 0.10 & 0.10\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 2$, head $h = 2$:\n$$\nA^{(2,2)} =\n\\begin{bmatrix}\n0.55 & 0.25 & 0.10 & 0.10 \\\\\n0.60 & 0.20 & 0.10 & 0.10 \\\\\n0.50 & 0.30 & 0.10 & 0.10\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 3$, head $h = 1$:\n$$\nA^{(3,1)} =\n\\begin{bmatrix}\n0.90 & 0.10 & 0.00 & 0.00 \\\\\n0.85 & 0.05 & 0.05 & 0.05 \\\\\n0.95 & 0.05 & 0.00 & 0.00\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 3$, head $h = 2$:\n$$\nA^{(3,2)} =\n\\begin{bmatrix}\n0.88 & 0.12 & 0.00 & 0.00 \\\\\n0.92 & 0.08 & 0.00 & 0.00 \\\\\n0.80 & 0.15 & 0.05 & 0.00\n\\end{bmatrix}.\n$$\n\nTest Case $2$ (boundary case; equal entropies across layers):\n- $L = 2$, $H = 1$, $Q = 2$, $K = 3$.\n- Layer $\\ell = 1$, head $h = 1$:\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.50 & 0.25 & 0.25 \\\\\n0.20 & 0.40 & 0.40\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 2$, head $h = 1$ (identical to layer $\\ell = 1$):\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.50 & 0.25 & 0.25 \\\\\n0.20 & 0.40 & 0.40\n\\end{bmatrix}.\n$$\n\nTest Case $3$ (edge case; mixed pattern with zero probabilities and non-monotonic entropies):\n- $L = 3$, $H = 1$, $Q = 3$, $K = 5$.\n- Layer $\\ell = 1$, head $h = 1$:\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.30 & 0.25 & 0.20 & 0.15 & 0.10 \\\\\n0.25 & 0.25 & 0.25 & 0.15 & 0.10 \\\\\n0.40 & 0.20 & 0.20 & 0.10 & 0.10\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 2$, head $h = 1$:\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.70 & 0.30 & 0.00 & 0.00 & 0.00 \\\\\n0.60 & 0.20 & 0.10 & 0.10 & 0.00 \\\\\n0.80 & 0.10 & 0.05 & 0.05 & 0.00\n\\end{bmatrix}.\n$$\n- Layer $\\ell = 3$, head $h = 1$:\n$$\nA^{(3,1)} =\n\\begin{bmatrix}\n0.50 & 0.20 & 0.15 & 0.10 & 0.05 \\\\\n0.45 & 0.25 & 0.15 & 0.10 & 0.05 \\\\\n0.40 & 0.30 & 0.15 & 0.10 & 0.05\n\\end{bmatrix}.\n$$\n\nYour program must:\n- Compute $H_\\ell$ in nats for each $\\ell$ in each test case.\n- Determine for each test case whether $H_1 > H_2 > \\dots > H_L$ holds under tolerance $\\varepsilon = 10^{-9}$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[{\\tt True},{\\tt False},{\\tt True}]$).\n\nNo physical units are involved beyond nats for entropy; angles and percentages are not used. The output type for each test case is a boolean. The program must be self-contained and must not read input from files or the user.", "solution": "The problem requires an analysis of the per-layer average attention entropy in a simplified multi-head attention mechanism. We are tasked with determining whether this entropy metric exhibits a strictly decreasing trend across successive layers of a model for a given set of test cases.\n\nThe solution is developed through a sequence of principled steps, starting from the fundamental definition of Shannon entropy and building up to the specified decision criterion.\n\nFirst, we address the core concept of Shannon entropy. For a discrete probability distribution represented by a vector $p = (p_1, p_2, \\dots, p_K)$ where $\\sum_{i=1}^K p_i = 1$ and $p_i \\ge 0$, the Shannon entropy $H(p)$ is defined in natural units (nats) as:\n$$\nH(p) = -\\sum_{i=1}^K p_i \\log(p_i)\n$$\nHere, $\\log$ denotes the natural logarithm. Entropy quantifies the uncertainty or randomness of the distribution. A distribution concentrated on a single outcome (e.g., $p = (1, 0, \\dots, 0)$) has an entropy of $0$, representing perfect certainty. Conversely, a uniform distribution (e.g., $p_i = 1/K$ for all $i$) has the maximum possible entropy of $\\log(K)$, representing maximum uncertainty. The problem specifies the convention that $0 \\log 0$ is defined as $0$, which arises from the limit $\\lim_{x \\to 0^+} x \\log x = 0$. This convention is critical for handling sparse probability vectors where some components are zero.\n\nNext, we formalize the per-layer average attention entropy, $H_\\ell$. The input data consists of attention matrices $A^{(\\ell,h)} \\in \\mathbb{R}^{Q \\times K}$ for each layer $\\ell \\in \\{1, \\dots, L\\}$ and each head $h \\in \\{1, \\dots, H\\}$. Each of the $Q$ rows of a given attention matrix $A^{(\\ell,h)}$ is a probability distribution over $K$ keys. The quantity $H_\\ell$ is the average of the Shannon entropies of all these row-wise probability distributions within a single layer $\\ell$. Its definition is:\n$$\nH_\\ell = \\frac{1}{H Q} \\sum_{h=1}^H \\sum_{q=1}^Q H\\Big(A^{(\\ell,h)}_{q,:}\\Big)\n$$\nwhere $A^{(\\ell,h)}_{q,:}$ is the $q$-th row of the matrix $A^{(\\ell,h)}$. The denominator $H \\times Q$ represents the total number of distributions (one for each query-head pair) being averaged over in layer $\\ell$.\n\nThe primary task is to verify whether the model exhibits lower entropy in deeper layers. This is formalized as the condition of strictly decreasing per-layer average entropies:\n$$\nH_1 > H_2 > \\dots > H_L\n$$\nThis is a chain of inequalities. For the condition to be satisfied, every adjacent pair of layers $(\\ell, \\ell+1)$ must satisfy $H_\\ell > H_{\\ell+1}$ for all $\\ell \\in \\{1, \\dots, L-1\\}$. Due to the potential for floating-point inaccuracies in numerical computation, a simple `>` comparison is insufficient. The problem provides a robust criterion using a tolerance $\\varepsilon = 10^{-9}$. The strict inequality $H_\\ell > H_{\\ell+1}$ is to be interpreted as:\n$$\nH_\\ell - H_{\\ell+1} > \\varepsilon\n$$\nIf this condition fails for even one pair of adjacent layers, the entire test case is considered to not exhibit the property of strictly decreasing entropy.\n\nThe algorithm to solve the problem for each test case is as follows:\n1.  Initialize a list to store the computed average entropies for each layer, let's call it `layer_entropies`.\n2.  For each layer $\\ell$ from $1$ to $L$:\n    a. Initialize a variable `current_layer_total_entropy` to $0$.\n    b. For each head $h$ from $1$ to $H$:\n        i. Access the attention matrix $A^{(\\ell,h)}$.\n        ii. For each query row $q$ from $1$ to $Q$:\n            - Extract the row vector $p = A^{(\\ell,h)}_{q,:}$.\n            - Compute the Shannon entropy $H(p)$. To handle the $0 \\log 0 = 0$ case, we first filter the vector $p$ to include only its non-zero elements, $p_{nz}$, and then compute $-\\sum p_{nz,i} \\log(p_{nz,i})$.\n            - Add the computed entropy $H(p)$ to `current_layer_total_entropy`.\n    c. Calculate the average entropy for the layer, $H_\\ell = \\text{current\\_layer\\_total\\_entropy} / (H \\times Q)$.\n    d. Append $H_\\ell$ to the `layer_entropies` list.\n3.  After computing all $H_\\ell$ values, check for the strictly decreasing condition.\n    a. Assume the condition holds, e.g., by setting a boolean flag `is_strictly_decreasing` to `True`.\n    b. Iterate from $\\ell = 0$ to $L-2$ (using $0$-based indexing for the `layer_entropies` list).\n    c. For each $\\ell$, check if `layer_entropies`[$\\ell$] $-$ `layer_entropies`[$\\ell+1$] $> \\varepsilon$.\n    d. If the check fails for any $\\ell$, set `is_strictly_decreasing` to `False` and break the loop, as the overall condition is violated.\n4.  The final value of `is_strictly_decreasing` is the result for the test case. This process is repeated for all provided test cases.\n\nThis structured procedure ensures that all definitions and constraints from the problem statement are respected, leading to a correct and verifiable outcome.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining if average attention entropy strictly\n    decreases with layer depth for a set of test cases.\n    \"\"\"\n    \n    # Define the tolerance for strict inequality checks.\n    epsilon = 1e-9\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {'L': 3, 'H': 2, 'Q': 3, 'K': 4},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.25, 0.25, 0.25, 0.25],\n                        [0.40, 0.30, 0.20, 0.10],\n                        [0.30, 0.30, 0.20, 0.20]\n                    ]),\n                    np.array([\n                        [0.35, 0.25, 0.20, 0.20],\n                        [0.30, 0.30, 0.20, 0.20],\n                        [0.25, 0.25, 0.25, 0.25]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.60, 0.20, 0.10, 0.10],\n                        [0.70, 0.10, 0.10, 0.10],\n                        [0.50, 0.30, 0.10, 0.10]\n                    ]),\n                    np.array([\n                        [0.55, 0.25, 0.10, 0.10],\n                        [0.60, 0.20, 0.10, 0.10],\n                        [0.50, 0.30, 0.10, 0.10]\n                    ])\n                ],\n                # Layer 3\n                [\n                    np.array([\n                        [0.90, 0.10, 0.00, 0.00],\n                        [0.85, 0.05, 0.05, 0.05],\n                        [0.95, 0.05, 0.00, 0.00]\n                    ]),\n                    np.array([\n                        [0.88, 0.12, 0.00, 0.00],\n                        [0.92, 0.08, 0.00, 0.00],\n                        [0.80, 0.15, 0.05, 0.00]\n                    ])\n                ]\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {'L': 2, 'H': 1, 'Q': 2, 'K': 3},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.50, 0.25, 0.25],\n                        [0.20, 0.40, 0.40]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.50, 0.25, 0.25],\n                        [0.20, 0.40, 0.40]\n                    ])\n                ]\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {'L': 3, 'H': 1, 'Q': 3, 'K': 5},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.30, 0.25, 0.20, 0.15, 0.10],\n                        [0.25, 0.25, 0.25, 0.15, 0.10],\n                        [0.40, 0.20, 0.20, 0.10, 0.10]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.70, 0.30, 0.00, 0.00, 0.00],\n                        [0.60, 0.20, 0.10, 0.10, 0.00],\n                        [0.80, 0.10, 0.05, 0.05, 0.00]\n                    ])\n                ],\n                # Layer 3\n                [\n                    np.array([\n                        [0.50, 0.20, 0.15, 0.10, 0.05],\n                        [0.45, 0.25, 0.15, 0.10, 0.05],\n                        [0.40, 0.30, 0.15, 0.10, 0.05]\n                    ])\n                ]\n            ]\n        }\n    ]\n\n    def calculate_shannon_entropy(p: np.ndarray) -> float:\n        \"\"\"\n        Computes the Shannon entropy of a probability distribution vector.\n        Handles the 0*log(0) = 0 case by filtering out zero probabilities.\n        \"\"\"\n        p_nonzero = p[p > 0]\n        if p_nonzero.size == 0:\n            return 0.0\n        return -np.sum(p_nonzero * np.log(p_nonzero))\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        L, H, Q = params['L'], params['H'], params['Q']\n        layers_data = case[\"data\"]\n        \n        layer_entropies = []\n        for l in range(L):\n            layer_data = layers_data[l]\n            total_layer_entropy = 0.0\n            \n            for h in range(H):\n                attention_matrix = layer_data[h]\n                for q in range(Q):\n                    prob_vector = attention_matrix[q, :]\n                    total_layer_entropy += calculate_shannon_entropy(prob_vector)\n            \n            avg_layer_entropy = total_layer_entropy / (H * Q)\n            layer_entropies.append(avg_layer_entropy)\n            \n        is_strictly_decreasing = True\n        if L > 1:\n            for i in range(L - 1):\n                # Check H_i > H_{i+1} using the specified tolerance\n                if not (layer_entropies[i] - layer_entropies[i+1] > epsilon):\n                    is_strictly_decreasing = False\n                    break\n        else:\n            # A single layer trivially satisfies the condition, but \n            # problem implies L > 1. A strict interpretation might depend on\n            # how to handle a chain of zero inequalities. False is safer.\n            # However, all test cases have L >= 2.\n            pass\n\n        results.append(is_strictly_decreasing)\n\n    # Final print statement in the exact required format.\n    # Note: str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3100381"}, {"introduction": "The characteristics of an attention distribution, such as its smoothness, are not just outcomes of training but can also be actively controlled to improve model generalization. This exercise [@problem_id:3100379] delves into the relationship between L2 regularization (weight decay) and attention entropy. You will derive and then numerically verify how the regularization strength $\\lambda$ influences the magnitude of learned query vectors, which in turn governs the sharpness of the attention distribution, providing a direct link between a core machine learning technique and the control of overfitting in Transformers.", "problem": "Consider a single-head scaled dot-product attention mechanism as used in Transformer models, where a learned query vector interacts with fixed key vectors to produce an attention distribution. Let $d$ denote the feature dimension. The keys are given by the rows of a fixed matrix $V \\in \\mathbb{R}^{n \\times d}$, and the query is a learned parameter vector $w \\in \\mathbb{R}^{d}$. The attention logits are defined by $z_i = \\frac{w^\\top v_i}{\\sqrt{d}}$ for $i \\in \\{1,\\dots,n\\}$, where $v_i$ is the $i$-th row of $V$. The attention distribution $A \\in \\mathbb{R}^{n}$ is obtained by the Softmax function, defined for any $z \\in \\mathbb{R}^{n}$ by $A_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n}\\exp(z_j)}$. The smoothness of the attention is measured by the Shannon entropy $H(A) = -\\sum_{i=1}^{n} A_i \\log A_i$, where the logarithm is the natural logarithm (units are in nats).\n\nThe query vector $w$ is learned from a set of input-target pairs using a linear model with Euclidean two-norm (L2) weight decay, also known as ridge regularization. Specifically, given $m$ input vectors collected as rows of $X \\in \\mathbb{R}^{m \\times d}$ and a target vector $t \\in \\mathbb{R}^{m}$, the learned parameter with weight decay $\\lambda \\ge 0$ is the unique minimizer of the regularized least-squares problem\n$$\n\\min_{w \\in \\mathbb{R}^{d}} \\ \\|Xw - t\\|_2^2 + \\lambda \\|w\\|_2^2,\n$$\nwhich equals\n$$\nw_\\lambda = (X^\\top X + \\lambda I_d)^{-1} X^\\top t,\n$$\nwhere $I_d$ is the $d \\times d$ identity matrix. The case $\\lambda = 0$ recovers Ordinary Least Squares (OLS).\n\nYour tasks:\n\n1. Using only fundamental definitions and well-tested formulas as a base, derive from first principles why increasing the weight decay parameter $\\lambda$ tends to increase the attention entropy $H(A)$ (i.e., makes attention smoother), under the assumptions that $V$ has bounded row norms and $X^\\top t$ is fixed. Your derivation should start from the definitions of ridge regularization, Softmax, and Shannon entropy, and proceed logically to explain the relationship between increasing $\\lambda$, shrinking parameter magnitude, reduced attention logit scale, and increased entropy. Do not introduce any untested shortcuts.\n\n2. Implement a complete, runnable program that computes $H(A)$ for each specified $\\lambda$ in the test suite below, using the definitions above. The program must:\n   - Compute $w_\\lambda$ using the given inputs $X$, $t$, and the regularization parameter $\\lambda$.\n   - Compute logits $z$ via $z = \\frac{V w_\\lambda}{\\sqrt{d}}$.\n   - Compute the attention distribution $A$ via the Softmax of $z$.\n   - Compute $H(A) = -\\sum_{i=1}^{n} A_i \\log A_i$.\n\nUse the following fixed test suite parameters:\n- Dimension $d = 4$ (an integer).\n- Number of data points $m = 6$ (an integer).\n- Number of keys $n = 5$ (an integer).\n- Data matrix $X \\in \\mathbb{R}^{6 \\times 4}$:\n$$\nX = \\begin{bmatrix}\n1.0 & -0.5 & 0.3 & 0.0 \\\\\n0.9 & 0.2 & -0.1 & 0.5 \\\\\n0.3 & 1.2 & 0.8 & -0.3 \\\\\n-0.7 & 0.5 & -1.1 & 0.4 \\\\\n1.1 & -0.8 & 0.0 & 0.6 \\\\\n0.0 & 0.3 & 0.9 & -0.9\n\\end{bmatrix}\n$$\n- Target vector $t \\in \\mathbb{R}^{6}$:\n$$\nt = \\begin{bmatrix}\n1.2 \\\\\n0.8 \\\\\n0.5 \\\\\n-0.3 \\\\\n1.0 \\\\\n0.2\n\\end{bmatrix}\n$$\n- Key matrix $V \\in \\mathbb{R}^{5 \\times 4}$:\n$$\nV = \\begin{bmatrix}\n0.6 & -0.1 & 0.2 & 0.0 \\\\\n0.1 & 0.5 & -0.4 & 0.3 \\\\\n-0.3 & 0.7 & 0.8 & -0.5 \\\\\n1.0 & -0.9 & 0.2 & 0.1 \\\\\n0.0 & 0.0 & 0.5 & 0.5\n\\end{bmatrix}\n$$\n- Regularization parameters $\\lambda$ (a list of nonnegative real numbers):\n$$\n\\Lambda = [0.0, \\ 0.1, \\ 1.0, \\ 10.0, \\ 100.0]\n$$\n\nAnswer specification:\n- For each $\\lambda \\in \\Lambda$, your program must output the corresponding attention entropy $H(A)$ as a real number (a float).\n- Your program should produce a single line of output containing the entropies, in the same order as $\\Lambda$, formatted as a comma-separated list enclosed in square brackets. For example, the output must look like $[h_1,h_2,\\dots,h_k]$, where each $h_i$ is a float. Express values in nats; no physical units or angle units are involved.\n\nDesign for coverage:\n- The test suite includes a \"happy path\" case ($\\lambda = 0.1$, $\\lambda = 1.0$), the boundary case with no regularization ($\\lambda = 0.0$), and edge cases with strong regularization ($\\lambda = 10.0$, $\\lambda = 100.0$). This range should reveal how $H(A)$ changes with $\\lambda$ and illustrate the link to overfitting control via attention smoothness.", "solution": "We begin from foundational definitions and well-tested formulas. The parameter vector with Euclidean two-norm (L2) weight decay is defined by\n$$\nw_\\lambda = \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ \\|Xw - t\\|_2^2 + \\lambda \\|w\\|_2^2 \\right\\}\n= (X^\\top X + \\lambda I_d)^{-1} X^\\top t,\n$$\nwhere $X \\in \\mathbb{R}^{m \\times d}$ has rows that are the data points, $t \\in \\mathbb{R}^{m}$ is the target vector, and $\\lambda \\ge 0$ is the weight decay parameter.\n\nThe scaled dot-product attention logits for a query $w \\in \\mathbb{R}^{d}$ and keys $v_i \\in \\mathbb{R}^{d}$ are defined by\n$$\nz_i = \\frac{w^\\top v_i}{\\sqrt{d}}, \\quad i \\in \\{1,\\dots,n\\}.\n$$\nThe attention distribution is the Softmax of $z$, with entries\n$$\nA_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}.\n$$\nThe Shannon entropy of $A$ is\n$$\nH(A) = -\\sum_{i=1}^{n} A_i \\log A_i,\n$$\nusing the natural logarithm and measured in nats.\n\nPrinciple-based derivation of how $\\lambda$ affects $H(A)$:\n\n1. From the definition of $w_\\lambda$, consider the spectral decomposition of $X^\\top X$, which is symmetric and positive semidefinite. Let $X^\\top X = U \\operatorname{diag}(s_1,\\dots,s_d) U^\\top$ with $U$ orthogonal and $s_k \\ge 0$ being the eigenvalues. Then\n$$\nw_\\lambda = U \\operatorname{diag}\\left(\\frac{1}{s_1+\\lambda},\\dots,\\frac{1}{s_d+\\lambda}\\right) U^\\top X^\\top t.\n$$\nFor fixed $X^\\top t$, increasing $\\lambda$ strictly increases each denominator $s_k + \\lambda$, which in turn shrinks each spectral component of $w_\\lambda$.\n\n2. The Euclidean norm of $w_\\lambda$ is a monotonically decreasing function of $\\lambda$. Indeed,\n$$\n\\|w_\\lambda\\|_2^2 = \\sum_{k=1}^{d} \\left(\\frac{c_k}{s_k + \\lambda}\\right)^2,\n$$\nwhere $c_k$ are the coordinates of $U^\\top X^\\top t$ in the eigenbasis. As $\\lambda$ increases, each term decreases because the map $x \\mapsto \\left(\\frac{c}{x}\\right)^2$ is decreasing for $x > 0$.\n\n3. Each attention logit can be bounded in magnitude by the Cauchy–Schwarz inequality:\n$$\n|z_i| = \\left|\\frac{w_\\lambda^\\top v_i}{\\sqrt{d}}\\right| \\le \\frac{\\|w_\\lambda\\|_2 \\, \\|v_i\\|_2}{\\sqrt{d}}.\n$$\nIf the rows of $V$ have bounded norms, then as $\\|w_\\lambda\\|_2$ decreases with increasing $\\lambda$, the absolute scale of the logits $z_i$ decreases.\n\n4. The Softmax distribution’s entropy increases as the logits are scaled down uniformly. To see this, consider a fixed nonconstant vector $z \\in \\mathbb{R}^{n}$ and define $A^{(s)} = \\operatorname{Softmax}(s z)$ for $s \\in (0, \\infty)$. This is equivalent to applying a temperature $T = 1/s$. As $s$ decreases (i.e., $T$ increases), the distribution $A^{(s)}$ becomes more uniform. For a strictly nonuniform $z$, the function $s \\mapsto H(A^{(s)})$ is strictly decreasing in $s$, hence strictly increasing in $T$. Intuitively, shrinking logits reduces confidence differences among entries, pushing $A$ towards the uniform distribution, which maximizes entropy at $H_{\\max} = \\log n$.\n\n5. Combining points $1$–$4$, increasing $\\lambda$ shrinks $w_\\lambda$, which reduces the logit scale, and therefore increases the entropy $H(A)$, making attention smoother. This mechanism illustrates a link to overfitting control: smaller $\\lambda$ permits larger parameter magnitudes that can overfit idiosyncratic patterns, producing peaky, low-entropy attention; larger $\\lambda$ penalizes such magnitudes, encouraging smoother, higher-entropy attention that is less prone to overfitting.\n\nAlgorithmic design for the program:\n\n- Inputs are fixed matrices $X$, $V$, and vector $t$, along with the list $\\Lambda$ of regularization parameters.\n- For each $\\lambda \\in \\Lambda$, compute $w_\\lambda$ using the linear system $(X^\\top X + \\lambda I_d) w_\\lambda = X^\\top t$. To ensure numerical stability and avoid explicit matrix inversion, solve this system with a reliable linear solver.\n- Compute logits $z = \\frac{V w_\\lambda}{\\sqrt{d}}$.\n- Compute $A$ via a numerically stable Softmax: subtract $\\max(z)$ before exponentiation to prevent overflow, i.e., $A_i = \\frac{\\exp(z_i - \\max(z))}{\\sum_j \\exp(z_j - \\max(z))}$.\n- Compute entropy $H(A) = -\\sum_i A_i \\log A_i$ using the natural logarithm.\n- Collect the entropies for all $\\lambda \\in \\Lambda$ and print them as a single line, comma-separated, enclosed in square brackets.\n\nTest suite coverage and expected qualitative behavior:\n\n- For $\\lambda = 0.0$ (no regularization), $w_\\lambda$ typically has larger magnitude, yielding more peaked attention and lower entropy.\n- For $\\lambda = 0.1$ and $\\lambda = 1.0$ (moderate regularization), entropy should increase compared to $\\lambda = 0.0$.\n- For $\\lambda = 10.0$ and $\\lambda = 100.0$ (strong regularization), $w_\\lambda$ becomes very small, logits approach zero, attention approaches uniform over $n = 5$ entries, and $H(A)$ approaches $\\log 5$.\n\nThe program follows these steps and outputs the entropies in the specified single-line format $[h_1,h_2,\\dots,h_k]$, where each $h_i$ is a float in nats.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute numerically stable softmax.\"\"\"\n    z_max = np.max(z)\n    exp_z = np.exp(z - z_max)\n    return exp_z / np.sum(exp_z)\n\ndef ridge_solution(X: np.ndarray, t: np.ndarray, lam: float) -> np.ndarray:\n    \"\"\"\n    Compute w_lambda = (X^T X + lam I)^{-1} X^T t\n    using a linear solver for numerical stability.\n    \"\"\"\n    d = X.shape[1]\n    XT_X = X.T @ X\n    A = XT_X + lam * np.eye(d)\n    b = X.T @ t\n    # Use solve; in case of numerical issues, fall back to least-squares.\n    try:\n        w = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Fall back to pseudo-inverse based computation\n        w = np.linalg.pinv(A) @ b\n    return w\n\ndef attention_entropy(V: np.ndarray, w: np.ndarray, d: int) -> float:\n    \"\"\"Compute attention logits, softmax, and Shannon entropy in nats.\"\"\"\n    logits = (V @ w) / np.sqrt(d)\n    A = softmax(logits)\n    # Shannon entropy: -sum A_i log A_i (natural log -> nats)\n    # Avoid log(0) by ensuring A has no exact zeros (softmax ensures this).\n    H = -np.sum(A * np.log(A))\n    return float(H)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    d = 4  # feature dimension\n    m = 6  # number of data points\n    n = 5  # number of keys\n\n    X = np.array([\n        [ 1.0, -0.5,  0.3,  0.0],\n        [ 0.9,  0.2, -0.1,  0.5],\n        [ 0.3,  1.2,  0.8, -0.3],\n        [-0.7,  0.5, -1.1,  0.4],\n        [ 1.1, -0.8,  0.0,  0.6],\n        [ 0.0,  0.3,  0.9, -0.9]\n    ], dtype=float)\n\n    t = np.array([1.2, 0.8, 0.5, -0.3, 1.0, 0.2], dtype=float)\n\n    V = np.array([\n        [ 0.6, -0.1,  0.2,  0.0],\n        [ 0.1,  0.5, -0.4,  0.3],\n        [-0.3,  0.7,  0.8, -0.5],\n        [ 1.0, -0.9,  0.2,  0.1],\n        [ 0.0,  0.0,  0.5,  0.5]\n    ], dtype=float)\n\n    lambdas = [0.0, 0.1, 1.0, 10.0, 100.0]\n\n    results = []\n    for lam in lambdas:\n        w_lam = ridge_solution(X, t, lam)\n        H = attention_entropy(V, w_lam, d)\n        results.append(H)\n\n    # Format results with consistent precision\n    formatted = [f\"{x:.6f}\" for x in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```", "id": "3100379"}]}