{"hands_on_practices": [{"introduction": "To truly understand how momentum builds upon gradient descent, it's essential to work through the update steps manually. This first exercise [@problem_id:2187765] has you compute the first two iterations of the classical momentum method on a simple quadratic function. By tracking the velocity vector $v_t$ and the parameter vector $w_t$, you will gain a concrete, step-by-step feel for how momentum accumulates past gradients to accelerate progress toward the minimum.", "problem": "An optimization algorithm is used to minimize a cost function $f(x, y)$, which depends on two parameters $x$ and $y$. The cost function is given by:\n$$f(x, y) = x^2 + 2y^2$$\n\nThe algorithm of choice is the classical momentum method. The update rules for the parameter vector $w_t = (x_t, y_t)$ and the velocity vector $v_t$ at step $t$ are given by:\n1. $v_t = \\beta v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\nwhere $\\nabla f(w_{t-1})$ is the gradient of the cost function evaluated at the previous parameter vector $w_{t-1}$.\n\nThe optimization starts at the initial parameter vector $w_0 = (x_0, y_0) = (4, 2)$. The initial velocity vector is $v_0 = (0, 0)$. The algorithm's hyperparameters are set to a learning rate $\\eta = 0.1$ and a momentum parameter $\\beta = 0.9$.\n\nYour task is to compute the parameter vector $w_2 = (x_2, y_2)$ after two full update steps. Express your answer as a row vector with two components.", "solution": "The cost function is $f(x,y)=x^{2}+2y^{2}$, so its gradient is\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\[4pt] \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\[4pt] 4y \\end{pmatrix}.\n$$\nThe momentum updates are $v_{t}=\\beta v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ and $w_{t}=w_{t-1}-v_{t}$ with $w_{0}=\\begin{pmatrix}4\\\\[2pt]2\\end{pmatrix}$, $v_{0}=\\begin{pmatrix}0\\\\[2pt]0\\end{pmatrix}$, $\\eta=\\frac{1}{10}$, and $\\beta=\\frac{9}{10}$.\n\nFirst step $t=1$:\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\[2pt]4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\[2pt]8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\[2pt]0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\[2pt]8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\[2pt]2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}.\n$$\n\nSecond step $t=2$:\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\[2pt]4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\[2pt]\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\[2pt]\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\[2pt]\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\[2pt]\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\[2pt]\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\[2pt]\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\[2pt]0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\[2pt]0\\end{pmatrix}.\n$$\nThus, after two full updates, the parameter vector is the row vector $\\begin{pmatrix}\\frac{46}{25}  0\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25}  0 \\end{pmatrix}}$$", "id": "2187765"}, {"introduction": "The momentum family includes a crucial variant known as Nesterov Accelerated Gradient (NAG), which often provides superior performance in practice. The key difference lies in the subtle but powerful change of calculating the gradient after applying the momentum step, allowing for a more effective \"look-ahead\" correction. This practice [@problem_id:2187801] challenges you to analyze a piece of pseudocode and identify whether it implements classical momentum or NAG, sharpening your ability to recognize these foundational algorithms from their implementation details.", "problem": "In the field of machine learning, iterative optimization algorithms are used to minimize a loss function $L(w)$ by updating a parameter vector $w$. Two popular momentum-based methods are Classical Momentum and Nesterov Accelerated Gradient (NAG).\n\nThe core idea of momentum is to accelerate movement in persistent directions of descent and to dampen oscillations. This is achieved by adding a \"velocity\" term, $v$, to the update rule. The update depends on a learning rate, $\\eta$, and a momentum coefficient, $\\beta$.\n\nThe key difference between the two methods lies in *where* the gradient of the loss function is evaluated.\n- **Classical Momentum**: Computes the gradient at the current parameter position, $w_t$.\n- **Nesterov Accelerated Gradient (NAG)**: First, it makes a \"look-ahead\" step in the direction of the previous velocity. It then computes the gradient at this look-ahead position to make a more informed correction.\n\nConsider the following pseudocode for a single update step of an optimization algorithm. The function `compute_gradient_at(point)` calculates the gradient of the loss function $L$ at the given `point`.\n\n```\nfunction update_step(w, v, eta, beta):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // beta: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - beta * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = beta * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\nWhich optimization algorithm does this pseudocode implement?\n\nA. Simple Gradient Descent with Momentum (Classical Momentum)\nB. Nesterov Accelerated Gradient (NAG)\nC. Simple Gradient Descent\nD. Adagrad\nE. RMSprop", "solution": "We are given an update scheme with parameters $w$ and velocity $v$, learning rate $\\eta$, and momentum coefficient $\\beta$. The pseudocode performs the following steps:\n1) Projected position:\n$$w_{\\text{proj}}=w-\\beta v.$$\n2) Gradient at the projected position:\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\beta v).$$\n3) Velocity update:\n$$v_{\\text{new}}=\\beta v+\\eta g.$$\n4) Parameter update:\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\beta v+\\eta g).$$\n\nClassical Momentum uses\n$$v_{t+1}=\\beta v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\nthat is, the gradient is evaluated at the current point $w_{t}$.\n\nNesterov Accelerated Gradient first computes a look-ahead point\n$$\\tilde{w}_{t}=w_{t}-\\beta v_{t},$$\nthen evaluates the gradient there,\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\beta v_{t}),$$\nand updates\n$$v_{t+1}=\\beta v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\nComparing these formulas with the pseudocode, we see exact agreement with the Nesterov procedure: the gradient is computed at the look-ahead position $w-\\beta v$, then the standard momentum-style velocity and parameter updates are applied. Therefore, the algorithm implemented by the pseudocode is Nesterov Accelerated Gradient.\n\nIt is not Simple Gradient Descent because there is a velocity term; it is not Classical Momentum because the gradient is not evaluated at $w$; and it is neither Adagrad nor RMSprop because there is no per-parameter adaptive scaling using accumulated squared gradients.", "answer": "$$\\boxed{B}$$", "id": "2187801"}, {"introduction": "While momentum methods can dramatically speed up convergence, they also introduce new hyperparameters, such as the momentum coefficient $\\beta$ and learning rate $\\eta$, that must be tuned carefully. An improper choice can lead to oscillatory or even unstable behavior, causing the optimization to diverge where standard gradient descent might not. This final exercise [@problem_id:2187798] provides a striking, hands-on demonstration of this risk, illustrating how certain parameter settings can cause the momentum optimizer to \"overshoot\" and spiral out of control, even on a simple convex problem.", "problem": "In the field of numerical optimization, different algorithms are used to find the minimum of a function. Consider the simple one-dimensional convex objective function $f(x) = \\frac{1}{2} C x^2$, where $C=1.0$. An analyst is comparing the behavior of two iterative optimization algorithms, starting from the initial position $x_0 = 10.0$.\n\nAlgorithm A is the Standard Gradient Descent (GD) method. The position is updated at each step $k$ according to the rule:\n$$x_{k+1} = x_k - \\eta_{A} \\nabla f(x_k)$$\nThe analyst uses a learning rate of $\\eta_A = 1.5$.\n\nAlgorithm B is the Momentum method. This method introduces a \"velocity\" term $v$ that accumulates past gradients. Starting with an initial velocity $v_0 = 0$, the updates are given by:\n$$v_{k+1} = \\beta v_k + \\eta_{B} \\nabla f(x_k)$$\n$$x_{k+1} = x_k - v_{k+1}$$\nFor this algorithm, the analyst uses a learning rate of $\\eta_B = 0.8$ and a momentum parameter of $\\beta = 0.8$.\n\nLet $x_5^{(A)}$ be the position of the iterate after 5 steps using Algorithm A, and $x_5^{(B)}$ be the position after 5 steps using Algorithm B. Calculate the ratio $R = x_5^{(B)} / x_5^{(A)}$. Report your final answer for $R$ rounded to three significant figures.", "solution": "The problem asks for the ratio of the final positions, $R = x_5^{(B)} / x_5^{(A)}$, after 5 iterations of two different optimization algorithms on the function $f(x) = \\frac{1}{2} C x^2$ with $C=1.0$. The gradient of the function is $\\nabla f(x) = \\frac{d}{dx}f(x) = C x = x$. The initial position is $x_0 = 10.0$ for both algorithms.\n\n**Part 1: Calculation for Algorithm A (Standard Gradient Descent)**\n\nAlgorithm A follows the update rule $x_{k+1} = x_k - \\eta_{A} \\nabla f(x_k)$.\nWith $\\nabla f(x_k) = x_k$ and $\\eta_A = 1.5$, the rule becomes:\n$x_{k+1} = x_k - 1.5 x_k = (1 - 1.5) x_k = -0.5 x_k$.\n\nWe start with $x_0 = 10.0$ and iterate 5 times:\n- Step 1: $x_1 = -0.5 \\times x_0 = -0.5 \\times 10.0 = -5.0$.\n- Step 2: $x_2 = -0.5 \\times x_1 = -0.5 \\times (-5.0) = 2.5$.\n- Step 3: $x_3 = -0.5 \\times x_2 = -0.5 \\times 2.5 = -1.25$.\n- Step 4: $x_4 = -0.5 \\times x_3 = -0.5 \\times (-1.25) = 0.625$.\n- Step 5: $x_5^{(A)} = -0.5 \\times x_4 = -0.5 \\times 0.625 = -0.3125$.\n\nThis algorithm appears to be converging (oscillating, but the magnitude decreases).\n\n**Part 2: Calculation for Algorithm B (Momentum)**\n\nAlgorithm B follows the update rules:\n$v_{k+1} = \\beta v_k + \\eta_{B} \\nabla f(x_k)$\n$x_{k+1} = x_k - v_{k+1}$\nWith $\\nabla f(x_k) = x_k$, $\\eta_B = 0.8$, and $\\beta = 0.8$, the rules are:\n$v_{k+1} = 0.8 v_k + 0.8 x_k$\n$x_{k+1} = x_k - v_{k+1}$\n\nWe start with $x_0 = 10.0$ and $v_0 = 0$ and iterate 5 times:\n\n- **Step 1 (k=0 to k=1):**\n  $v_1 = 0.8 v_0 + 0.8 x_0 = 0.8(0) + 0.8(10.0) = 8.0$.\n  $x_1 = x_0 - v_1 = 10.0 - 8.0 = 2.0$.\n\n- **Step 2 (k=1 to k=2):**\n  $v_2 = 0.8 v_1 + 0.8 x_1 = 0.8(8.0) + 0.8(2.0) = 6.4 + 1.6 = 8.0$.\n  $x_2 = x_1 - v_2 = 2.0 - 8.0 = -6.0$.\n\n- **Step 3 (k=2 to k=3):**\n  $v_3 = 0.8 v_2 + 0.8 x_2 = 0.8(8.0) + 0.8(-6.0) = 6.4 - 4.8 = 1.6$.\n  $x_3 = x_2 - v_3 = -6.0 - 1.6 = -7.6$.\n\n- **Step 4 (k=3 to k=4):**\n  $v_4 = 0.8 v_3 + 0.8 x_3 = 0.8(1.6) + 0.8(-7.6) = 1.28 - 6.08 = -4.8$.\n  $x_4 = x_3 - v_4 = -7.6 - (-4.8) = -2.8$.\n\n- **Step 5 (k=4 to k=5):**\n  $v_5 = 0.8 v_4 + 0.8 x_4 = 0.8(-4.8) + 0.8(-2.8) = -3.84 - 2.24 = -6.08$.\n  $x_5^{(B)} = x_4 - v_5 = -2.8 - (-6.08) = 3.28$.\n\nDespite the chosen parameters, which might seem reasonable, this algorithm is exhibiting signs of divergence as the magnitude of the position begins to increase after an initial decrease ($|x_0|=10, |x_1|=2, |x_2|=6, |x_3|=7.6, |x_4|=2.8, |x_5|=3.28$).\n\n**Part 3: Final Calculation**\n\nWe need to compute the ratio $R = x_5^{(B)} / x_5^{(A)}$.\nUsing the calculated values:\n$R = \\frac{3.28}{-0.3125} = -10.496$.\n\nThe problem asks for the answer rounded to three significant figures.\n$R \\approx -10.5$.", "answer": "$$\\boxed{-10.5}$$", "id": "2187798"}]}