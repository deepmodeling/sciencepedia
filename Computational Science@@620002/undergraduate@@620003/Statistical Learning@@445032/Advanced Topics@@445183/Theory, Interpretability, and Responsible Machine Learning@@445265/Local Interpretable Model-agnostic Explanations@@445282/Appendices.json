{"hands_on_practices": [{"introduction": "The reliability of a LIME explanation hinges on the quality of its local surrogate model. The standard approach uses weighted least squares, which minimizes a sum of squared errors. However, this method can be highly sensitive to outliers—data points that lie far from the region of interest or have unusually large prediction errors. This exercise [@problem_id:3140869] gives you hands-on practice in building more robust explanations by comparing the standard surrogate with one based on the Huber loss, a function less influenced by large errors, thereby improving the fidelity of the explanation in the presence of noisy perturbations.", "problem": "You are to implement and compare two local linear surrogate explanations in the sense of Local Interpretable Model-Agnostic Explanations (LIME), one trained with the squared error loss (denoted $L_2$ loss) and the other trained with the Huber loss, in order to examine robustness to outlier perturbations sampled far from a target point $x_0$.\n\nThe core of the task is to fit a locally weighted linear surrogate model to approximate a smooth black-box regression function $f:\\mathbb{R}^p \\to \\mathbb{R}$ near a fixed point $x_0 \\in \\mathbb{R}^p$. The local surrogate uses the original features as interpretable features, and the locality is enforced by a kernel that downweights points as they move away from $x_0$. You must implement both the $L_2$-based surrogate (weighted least squares) and the Huber-loss-based surrogate (weighted robust regression). You will then quantitatively compare the estimated local coefficients against the true gradient of $f$ at $x_0$.\n\nFundamental base:\n- Local surrogates in Local Interpretable Model-Agnostic Explanations (LIME) are defined by minimizing a locality-weighted empirical risk over a linear model. For a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ with locality weights $\\{w_i\\}_{i=1}^n$, the $L_2$ surrogate solves\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\left(y_i - \\beta_0 - \\beta^\\top x_i\\right)^2,\n$$\nand the Huber-loss surrogate solves\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\,\\rho_\\delta\\!\\left(y_i - \\beta_0 - \\beta^\\top x_i\\right),\n$$\nwhere the Huber loss $\\rho_\\delta$ with threshold $\\delta>0$ is defined by\n$$\n\\rho_\\delta(r)=\\begin{cases}\n\\frac{1}{2} r^2, & \\text{if } |r|\\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r|>\\delta.\n\\end{cases}\n$$\n- A Gaussian kernel is a standard choice to encode locality. For bandwidth $\\sigma>0$, define\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right).\n$$\n\nYou will work with a fixed, smooth black-box model $f$ in dimension $p=5$, defined by\n$$\nf(x) \\;=\\; \\tanh\\!\\left(a^\\top x\\right) \\;+\\; \\frac{1}{2}\\left(b^\\top x\\right)^2 \\;+\\; c^\\top x \\;+\\; 0.2\\,\\sin\\!\\left(d^\\top x\\right),\n$$\nwhere $a,b,c,d \\in \\mathbb{R}^5$ are fixed vectors and $x\\in\\mathbb{R}^5$. The exact analytical gradient $\\nabla f(x)$ must be computed and used to evaluate the quality of the surrogate’s slope coefficients at $x_0$.\n\nNeighborhood data must be generated by perturbing $x_0$ as follows. Given a contamination fraction $\\gamma \\in [0,1]$, a far-outlier distance scale $R>0$, local noise standard deviation $s_{\\text{local}}>0$, and far-outlier noise standard deviation $s_{\\text{far}}>0$, construct $n$ perturbations with $n_{\\text{out}}=\\lfloor \\gamma n \\rfloor$ “far” points and $n_{\\text{in}}=n-n_{\\text{out}}$ “near” points:\n- Near points: $x_i = x_0 + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$, for $i=1,\\dots,n_{\\text{in}}$.\n- Far points: sample unit directions $u_j \\in \\mathbb{S}^{p-1}$ uniformly by normalizing standard Gaussian vectors, then set $x_j = x_0 + R\\,u_j + \\eta_j$ with $\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$, for $j=1,\\dots,n_{\\text{out}}$.\n\nFor each synthetic dataset, compute $y_i=f(x_i)$ and locality weights $w_i$ using the Gaussian kernel with bandwidth $\\sigma>0$. Fit two local surrogates at $x_0$:\n- $L_2$ surrogate by minimizing the weighted squared error.\n- Huber-loss surrogate by minimizing the weighted Huber objective with a fixed threshold $\\delta>0$.\n\nEvaluation: Let $\\widehat{\\beta}^{(2)}$ denote the slope vector from the $L_2$ surrogate and $\\widehat{\\beta}^{(H)}$ denote the slope vector from the Huber surrogate. Let $g_0=\\nabla f(x_0)$ be the true gradient at $x_0$. Compute the slope error for each surrogate as the Euclidean norm\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2.\n$$\nReport the improvement of the Huber surrogate over the $L_2$ surrogate as the single float\n$$\n\\Delta = E_2 - E_H.\n$$\nPositive $\\Delta$ indicates that the Huber surrogate is more accurate with respect to the true local slope at $x_0$.\n\nImplementation requirements:\n- Use $p=5$, $n=400$, $s_{\\text{local}}=0.4$, $s_{\\text{far}}=0.1$, $\\delta=1.0$. Use a fixed random seed so results are deterministic.\n- Draw $x_0$, $a$, $b$, $c$, and $d$ from independent standard normal distributions once and keep them fixed across all test cases.\n- Use the Gaussian kernel for weights with given $\\sigma$.\n- Solve the $L_2$ problem by weighted least squares. Solve the Huber problem by a principled method such as Iteratively Reweighted Least Squares (IRLS), correctly integrating the locality weights $\\{w_i\\}$ into the objective. Convergence tolerance and iteration cap can be chosen reasonably, but the solution must be numerically stable.\n\nTest suite:\nRun your program for the following five parameter sets $(\\gamma, R, \\sigma)$:\n1. $(0.0, 5.0, 1.0)$\n2. $(0.2, 5.0, 1.0)$\n3. $(0.6, 10.0, 1.0)$\n4. $(0.2, 5.0, 0.2)$\n5. $(0.2, 5.0, 5.0)$\n\nFor each case, construct the neighborhood dataset as specified, fit both surrogates, compute $E_2$, $E_H$, and return $\\Delta=E_2-E_H$ as a float. To ensure comparability, within a run the underlying randomness apart from the test case parameters must be fixed by the single global seed.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the five $\\Delta$ values, rounded to six decimal places, enclosed in square brackets, for example, \"[0.123456,0.000001,-0.010203,0.500000,0.250000]\".", "solution": "The objective is to implement and compare two local linear surrogate models used for interpretable machine learning explanations, specifically within the framework of Local Interpretable Model-Agnostic Explanations (LIME). One surrogate is based on the standard squared error ($L_2$) loss, while the other employs the robust Huber loss. The comparison focuses on the accuracy of the estimated local linear coefficients (slope) relative to the true gradient of a black-box function, particularly in the presence of outliers in the local neighborhood data.\n\n### 1. Problem Formulation\n\nWe are given a smooth, nonlinear black-box function $f:\\mathbb{R}^p \\to \\mathbb{R}$, where the dimension is fixed at $p=5$. The function is defined as:\n$$\nf(x) = \\tanh(a^\\top x) + \\frac{1}{2}\\left(b^\\top x\\right)^2 + c^\\top x + 0.2\\,\\sin(d^\\top x)\n$$\nwhere $a, b, c, d \\in \\mathbb{R}^p$ are fixed parameter vectors. The local behavior of this function at a point of interest $x_0 \\in \\mathbb{R}^p$ is to be approximated by a linear model. The ground truth for the local linear approximation's slope is the gradient of $f$ evaluated at $x_0$, which we denote as $g_0 = \\nabla f(x_0)$. Using the chain rule, a component-wise differentiation yields the analytical gradient:\n$$\n\\nabla f(x) = \\left(1 - \\tanh^2(a^\\top x)\\right)a + \\left(b^\\top x\\right)b + c + 0.2\\cos(d^\\top x)d\n$$\n\nA synthetic dataset $\\{(x_i, y_i)\\}_{i=1}^n$ is generated around $x_0$ to train the local surrogates, where $y_i=f(x_i)$. The dataset of size $n=400$ is contaminated with a fraction $\\gamma$ of outliers. There are $n_{\\text{in}} = n - \\lfloor \\gamma n \\rfloor$ \"near\" points and $n_{\\text{out}} = \\lfloor \\gamma n \\rfloor$ \"far\" points.\n- Near points (inliers): $x_i = x_0 + \\epsilon_i$, with noise $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$.\n- Far points (outliers): $x_j = x_0 + R\\,u_j + \\eta_j$, where $u_j$ are uniformly sampled from the unit sphere $\\mathbb{S}^{p-1}$, $R$ is a large distance scale, and $\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$ is a small perturbation.\n\nTo enforce locality, each data point $(x_i, y_i)$ is assigned a weight $w_i$ based on its proximity to $x_0$, determined by a Gaussian kernel with bandwidth $\\sigma$:\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right)\n$$\n\n### 2. Local Surrogate Models\n\nThe surrogate model is a linear function $g(x) = \\beta_0 + \\beta^\\top x$, where $\\beta_0 \\in \\mathbb{R}$ is the intercept and $\\beta \\in \\mathbb{R}^p$ is the slope vector. The parameters are found by minimizing a weighted loss function. Let $\\tilde{x}_i = [1, x_i^\\top]^\\top$ be the augmented feature vector and $\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$ be the full coefficient vector.\n\n#### 2.1. $L_2$ Loss Surrogate (Weighted Least Squares)\nThe standard approach minimizes the sum of weighted squared errors:\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)^2\n$$\nThis is a standard Weighted Least Squares (WLS) problem. In matrix form, let $\\tilde{X}$ be the $n \\times (p+1)$ design matrix whose rows are $\\tilde{x}_i^\\top$, $\\mathbf{y}$ be the vector of responses, and $W$ be the $n \\times n$ diagonal matrix of locality weights $w_i$. The objective is to minimize $(\\mathbf{y} - \\tilde{X}\\tilde{\\beta})^\\top W (\\mathbf{y} - \\tilde{X}\\tilde{\\beta})$. The closed-form solution for the estimated coefficients, denoted $\\widehat{\\tilde{\\beta}}^{(2)}$, is:\n$$\n\\widehat{\\tilde{\\beta}}^{(2)} = (\\tilde{X}^\\top W \\tilde{X})^{-1} \\tilde{X}^\\top W \\mathbf{y}\n$$\nThe slope vector for explanation is $\\widehat{\\beta}^{(2)}$, which consists of the last $p$ elements of $\\widehat{\\tilde{\\beta}}^{(2)}$.\n\n#### 2.2. Huber Loss Surrogate (Robust Regression)\nTo improve robustness against outliers, we replace the squared error with the Huber loss $\\rho_\\delta(\\cdot)$ with a threshold $\\delta > 0$:\n$$\n\\rho_\\delta(r) = \\begin{cases}\n\\frac{1}{2} r^2, & \\text{if } |r|\\le \\delta \\\\\n\\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThe Huber surrogate's coefficients, $\\widehat{\\tilde{\\beta}}^{(H)}$, are found by solving the following convex optimization problem:\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\rho_\\delta\\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)\n$$\nThis problem does not have a closed-form solution and is solved numerically using Iteratively Reweighted Least Squares (IRLS). The first-order optimality conditions (estimating equations) are $\\sum_{i=1}^n w_i \\psi_\\delta(r_i) \\tilde{x}_i = \\mathbf{0}$, where $r_i = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}$ and $\\psi_\\delta(r) = \\rho'_\\delta(r)$ is the derivative of the Huber loss. These equations can be rewritten as $\\sum_{i=1}^n w_i \\omega_i(r_i) r_i \\tilde{x}_i = \\mathbf{0}$, where $\\omega_i(r_i) = \\psi_\\delta(r_i)/r_i$ are residual-dependent weights.\nThe IRLS algorithm proceeds as follows:\n1.  Initialize coefficients $\\tilde{\\beta}^{(0)}$, for instance, using the $L_2$ solution $\\widehat{\\tilde{\\beta}}^{(2)}$.\n2.  For iteration $k=0, 1, 2, \\dots$ until convergence:\n    a.  Compute residuals: $r_i^{(k)} = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}^{(k)}$.\n    b.  Compute IRLS weights: $\\omega_i^{(k)} = 1$ if $|r_i^{(k)}| \\le \\delta$, and $\\omega_i^{(k)} = \\delta / |r_i^{(k)}|$ if $|r_i^{(k)}| > \\delta$. These weights down-weight points with large residuals.\n    c.  Form a diagonal matrix of total weights $W_{\\text{total}}^{(k)}$ with diagonal entries $w_i \\cdot \\omega_i^{(k)}$, combining locality and residual weights.\n    d.  Update the coefficients by solving the WLS problem with these total weights:\n        $$\n        \\tilde{\\beta}^{(k+1)} = (\\tilde{X}^\\top W_{\\text{total}}^{(k)} \\tilde{X})^{-1} \\tilde{X}^\\top W_{\\text{total}}^{(k)} \\mathbf{y}\n        $$\n3.  The algorithm terminates when the change in coefficients, $\\|\\tilde{\\beta}^{(k+1)} - \\tilde{\\beta}^{(k)}\\|_2$, falls below a small tolerance. The final slope vector is $\\widehat{\\beta}^{(H)}$, comprising the last $p$ elements of the converged $\\tilde{\\beta}$.\n\n### 3. Evaluation\nThe quality of each surrogate is measured by the Euclidean distance between its estimated slope vector and the true gradient $g_0 = \\nabla f(x_0)$:\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2\n$$\nThe final metric reported is the improvement of the Huber surrogate over the $L_2$ surrogate, defined as the difference in their errors:\n$$\n\\Delta = E_2 - E_H\n$$\nA positive value of $\\Delta$ indicates that the Huber loss surrogate provides a more accurate estimate of the local behavior of $f$ at $x_0$, demonstrating its robustness to the outlier contamination.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares L2-loss and Huber-loss local surrogate models.\n    \"\"\"\n    # --- Problem Constants and Fixed Parameters ---\n    P = 5\n    N = 400\n    S_LOCAL = 0.4\n    S_FAR = 0.1\n    DELTA = 1.0\n    RANDOM_SEED = 42\n\n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate and fix the model parameters and the point of interest\n    x0 = rng.standard_normal(size=P)\n    a = rng.standard_normal(size=P)\n    b = rng.standard_normal(size=P)\n    c = rng.standard_normal(size=P)\n    d = rng.standard_normal(size=P)\n\n    test_cases = [\n        (0.0, 5.0, 1.0),\n        (0.2, 5.0, 1.0),\n        (0.6, 10.0, 1.0),\n        (0.2, 5.0, 0.2),\n        (0.2, 5.0, 5.0),\n    ]\n\n    # --- Helper Functions ---\n\n    def f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The black-box function f(x).\"\"\"\n        return (\n            np.tanh(a_p @ x_vec)\n            + 0.5 * (b_p @ x_vec) ** 2\n            + c_p @ x_vec\n            + 0.2 * np.sin(d_p @ x_vec)\n        )\n\n    def grad_f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The analytical gradient of f(x).\"\"\"\n        grad = (\n            (1 - np.tanh(a_p @ x_vec) ** 2) * a_p\n            + (b_p @ x_vec) * b_p\n            + c_p\n            + 0.2 * np.cos(d_p @ x_vec) * d_p\n        )\n        return grad\n\n    def solve_wls(X_tilde, y, weights):\n        \"\"\"Solves a weighted least squares problem.\"\"\"\n        W = np.diag(weights)\n        # Using np.linalg.solve for stability: (X.T @ W @ X) @ beta = X.T @ W @ y\n        lhs = X_tilde.T @ W @ X_tilde\n        rhs = X_tilde.T @ W @ y\n        try:\n            beta_tilde = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if singular\n            beta_tilde = np.linalg.pinv(lhs) @ rhs\n        return beta_tilde\n\n    def solve_huber_irls(X_tilde, y, locality_weights, delta, tol=1e-7, max_iter=100):\n        \"\"\"Solves a Huber regression problem using IRLS.\"\"\"\n        # Initialize with the standard WLS solution\n        beta_tilde = solve_wls(X_tilde, y, locality_weights)\n\n        for _ in range(max_iter):\n            residuals = y - X_tilde @ beta_tilde\n            abs_residuals = np.abs(residuals)\n            \n            # IRLS weights: handles r=0 case correctly via np.where\n            irls_weights = np.where(abs_residuals = delta, 1.0, delta / abs_residuals)\n            \n            total_weights = locality_weights * irls_weights\n            \n            beta_tilde_new = solve_wls(X_tilde, y, total_weights)\n            \n            # Check for convergence\n            if np.linalg.norm(beta_tilde_new - beta_tilde)  tol:\n                beta_tilde = beta_tilde_new\n                break\n            \n            beta_tilde = beta_tilde_new\n            \n        return beta_tilde\n\n    results = []\n\n    # --- Main Loop over Test Cases ---\n    for gamma, R, sigma in test_cases:\n        # 1. Generate neighborhood data\n        n_out = int(np.floor(gamma * N))\n        n_in = N - n_out\n\n        # Near points\n        epsilons = rng.normal(scale=S_LOCAL, size=(n_in, P))\n        X_in = x0 + epsilons\n\n        # Far points\n        if n_out > 0:\n            Z = rng.normal(size=(n_out, P))\n            directions = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n            etas = rng.normal(scale=S_FAR, size=(n_out, P))\n            X_out = x0 + R * directions + etas\n            X = np.vstack((X_in, X_out))\n        else:\n            X = X_in\n\n        # Compute responses y = f(x)\n        y = np.array([f_model(x_i, a, b, c, d) for x_i in X])\n\n        # 2. Compute locality weights\n        distances_sq = np.sum((X - x0) ** 2, axis=1)\n        locality_weights = np.exp(-distances_sq / (2 * sigma**2))\n\n        # 3. Prepare matrices for regression\n        X_tilde = np.c_[np.ones(N), X]\n\n        # 4. Solve for L2 surrogate\n        beta_tilde_2 = solve_wls(X_tilde, y, locality_weights)\n        beta_hat_2 = beta_tilde_2[1:]\n\n        # 5. Solve for Huber surrogate\n        beta_tilde_H = solve_huber_irls(X_tilde, y, locality_weights, DELTA)\n        beta_hat_H = beta_tilde_H[1:]\n\n        # 6. Evaluate against the true gradient\n        g0 = grad_f_model(x0, a, b, c, d)\n        \n        e2 = np.linalg.norm(beta_hat_2 - g0)\n        eH = np.linalg.norm(beta_hat_H - g0)\n        \n        delta_error = e2 - eH\n        results.append(delta_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{v:.6f}' for v in results)}]\")\n\nsolve()\n```", "id": "3140869"}, {"introduction": "After building a foundational LIME model, it is crucial to understand its limitations. A core assumption of simple LIME surrogates is that the black-box function behaves linearly in the local neighborhood being explained. This practice [@problem_id:3140899] serves as a \"stress test\" by challenging you to explain a function with a sharp discontinuity, a clear violation of the local linearity assumption. By investigating how well a linear surrogate can capture this threshold effect, you will develop a deeper intuition for the critical interplay between LIME's kernel width, $\\sigma$, and the faithfulness of the resulting explanation.", "problem": "Consider a black-box real-valued function $f:\\mathbb{R}^d \\to \\mathbb{R}$ that exhibits a sharp threshold on the first coordinate. Define the function as\n$$\nf(\\mathbf{x}) \\;=\\; J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\},\n$$\nwhere $\\mathbf{x} = (x_1,\\dots,x_d)$, $J \\in \\mathbb{R}$ is a fixed jump magnitude, $\\tau \\in \\mathbb{R}$ is the threshold, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nA local surrogate is constructed using the principle of Local Interpretable Model-Agnostic Explanations (LIME). The surrogate is a locally weighted linear regression fitted at a target point $\\mathbf{x}_0 \\in \\mathbb{R}^d$. The weight of a sample $\\mathbf{x}$ is given by an exponential kernel in the Euclidean norm\n$$\nw(\\mathbf{x}) \\;=\\; \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right),\n$$\nwith kernel width $\\sigma > 0$. The local linear surrogate has the form\n$$\n\\hat{f}(\\mathbf{x}) \\;=\\; \\beta_0 + \\sum_{j=1}^d \\beta_j x_j,\n$$\nwhere the coefficients are obtained by minimizing the weighted least squares objective\n$$\n\\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2,\n$$\ngiven a set of $N$ perturbation samples $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$ drawn independently from a Gaussian centered at $\\mathbf{x}_0$ with isotropic standard deviation $s$ on each coordinate:\n$$\n\\mathbf{x}^{(i)} \\sim \\mathcal{N}\\!\\left(\\mathbf{x}_0, s^2 \\mathbf{I}_d\\right).\n$$\n\nDefine the following measurable notion of whether the local surrogate captures the threshold effect near $\\mathbf{x}_0$:\n- The true local contrast across the kernel scale on the first coordinate is\n$$\nC_{\\text{true}} \\;=\\; \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|,\n$$\nwhere $\\mathbf{e}_1$ is the first canonical basis vector in $\\mathbb{R}^d$. For the given $f$, this simplifies to $C_{\\text{true}} = J$ if $(x_{0,1} - \\sigma)  \\tau \\le (x_{0,1} + \\sigma)$, and $C_{\\text{true}} = 0$ otherwise.\n- The surrogate-predicted local contrast at the same scale is\n$$\nC_{\\text{pred}} \\;=\\; \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| \\;=\\; \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|.\n$$\n\nDeclare that the local surrogate captures the threshold if the absolute error satisfies\n$$\n\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\;\\le\\; \\varepsilon \\,\\max\\{1, |J|\\},\n$$\nfor a fixed tolerance $\\varepsilon > 0$.\n\nTask. Write a complete program that:\n1. Implements the function $f$ as specified.\n2. For each test case below, generates $N$ samples $\\mathbf{x}^{(i)}$ from $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ with a fixed pseudo-random seed equal to $12345$ for reproducibility.\n3. Computes weights $w(\\mathbf{x}^{(i)})$ using the kernel width $\\sigma$ of the test case.\n4. Fits the locally weighted linear regression to obtain $(\\beta_0,\\dots,\\beta_d)$ using weighted least squares with an intercept term. Any numerically stable method is acceptable, but it must implement the stated weighted objective exactly.\n5. Computes $C_{\\text{true}}$ and $C_{\\text{pred}}$ as defined, and returns a boolean indicating whether the capture criterion holds with $\\varepsilon = 0.35$.\n6. Aggregates the boolean results for all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact format: for example, \"[True,False,True]\".\n\nTest suite. Use the following four test cases, each defined by the tuple $(d,\\tau,J,\\mathbf{x}_0,s,\\sigma,N)$:\n- Case A (near-threshold, small kernel): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.02,\\;0.0),\\;s=\\;0.2,\\;\\sigma=\\;0.08,\\;N=\\;12000)$.\n- Case B (far from threshold): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.5,\\;0.0),\\;s=\\;0.2,\\;\\sigma=\\;0.08,\\;N=\\;8000)$.\n- Case C (exactly at threshold, very wide kernel): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.0,\\;0.0),\\;s=\\;0.05,\\;\\sigma=\\;0.5,\\;N=\\;12000)$.\n- Case D (exactly at threshold, very narrow kernel, higher dimension): $(d=\\;3,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.0,\\;0.1,\\;-0.1),\\;s=\\;0.2,\\;\\sigma=\\;0.02,\\;N=\\;20000)$.\n\nAngle units do not apply. No physical units appear in this problem. Your program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, in the exact order of the test suite cases above, for example: \"[True,False,True,True]\". No additional text should be printed.", "solution": "### Step 1: Extract Givens\n\nThe problem provides the following definitions and data:\n\n- **Black-box function**: $f:\\mathbb{R}^d \\to \\mathbb{R}$ defined as $f(\\mathbf{x}) = J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\}$, where $\\mathbf{x} = (x_1,\\dots,x_d)$, $J \\in \\mathbb{R}$ is a jump magnitude, $\\tau \\in \\mathbb{R}$ is a threshold, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- **LIME weighting kernel**: $w(\\mathbf{x}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$, for a target point $\\mathbf{x}_0$ and kernel width $\\sigma > 0$.\n- **Local surrogate model**: A linear model $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$.\n- **Objective function**: The coefficients $(\\beta_0, \\beta_1, \\dots, \\beta_d)$ are determined by minimizing the weighted least squares error:\n$$ \\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2 $$\n- **Perturbation samples**: A set of $N$ samples $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$ drawn from $\\mathbf{x}^{(i)} \\sim \\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$.\n- **True local contrast**: $C_{\\text{true}} = \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|$.\n- **Predicted local contrast**: $C_{\\text{pred}} = \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| = \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|$.\n- **Capture criterion**: $\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\le \\varepsilon \\,\\max\\{1, |J|\\}$, with a fixed tolerance $\\varepsilon = 0.35$.\n- **Reproducibility**: A fixed pseudo-random seed of $12345$ must be used for sample generation.\n- **Test Cases**:\n    - Case A: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.02, 0.0), s=0.2, \\sigma=0.08, N=12000)$\n    - Case B: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.5, 0.0), s=0.2, \\sigma=0.08, N=8000)$\n    - Case C: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.0), s=0.05, \\sigma=0.5, N=12000)$\n    - Case D: $(d=3, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.1, -0.1), s=0.2, \\sigma=0.02, N=20000)$\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in the field of statistical learning, specifically concerning local interpretable model-agnostic explanations (LIME). The methods described—locally weighted linear regression, Gaussian kernel weighting, and Gaussian perturbation—are standard techniques. The function $f(\\mathbf{x})$ is a simple, well-defined mathematical function (a step function).\n- **Well-Posed**: The problem is well-posed. The objective is to compute a boolean value based on a series of deterministic calculations, given a fixed set of parameters and a fixed random seed. The weighted least squares problem has a unique, stable solution under the specified conditions (samples drawn from a continuous distribution).\n- **Objective**: The problem is stated in precise, objective mathematical language. All definitions, parameters, and criteria are quantitative and unambiguous.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data and definitions for each test case are provided. There are no internal contradictions.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. It is a well-defined computational task based on established principles in machine learning. A complete solution will be provided.\n\n### Solution\n\nThe task is to determine, for several test cases, whether a LIME-style local linear surrogate can capture the sharp threshold behavior of a given function $f(\\mathbf{x})$. This is assessed by comparing the true change in $f(\\mathbf{x})$ across a local interval with the change predicted by the surrogate model. The following algorithm is implemented for each test case.\n\nFirst, for a given test case with parameters $(d, \\tau, J, \\mathbf{x}_0, s, \\sigma, N)$, we generate $N$ perturbation samples. For reproducibility, the pseudo-random number generator is seeded with the value $12345$. Each sample $\\mathbf{x}^{(i)}$ is drawn from the multivariate normal distribution $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$, where $\\mathbf{I}_d$ is the $d \\times d$ identity matrix.\n\nNext, we evaluate the black-box function $f(\\mathbf{x}^{(i)}) = J \\cdot \\mathbf{1}\\{x_1^{(i)} \\ge \\tau\\}$ for each sample $\\mathbf{x}^{(i)}$. This yields a vector of responses $\\mathbf{y} \\in \\mathbb{R}^N$, where each element $y_i = f(\\mathbf{x}^{(i)})$.\n\nThe core of the LIME method is to fit a local surrogate model by solving a weighted least squares problem. The weight for each sample $\\mathbf{x}^{(i)}$ is calculated using the exponential kernel $w(\\mathbf{x}^{(i)}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}^{(i)} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$. These weights, $w_i = w(\\mathbf{x}^{(i)})$, form the diagonal entries of a weight matrix $\\mathbf{W}$.\n\nThe linear surrogate model is $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$. To find the coefficient vector $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_d)^T$, we solve the normal equations for weighted least squares:\n$$ \\boldsymbol{\\beta} = (\\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{X}_{\\text{aug}})^{-1} \\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y} $$\nHere, $\\mathbf{y}$ is the $N \\times 1$ vector of function values $f(\\mathbf{x}^{(i)})$. $\\mathbf{X}_{\\text{aug}}$ is the $N \\times (d+1)$ augmented design matrix, constructed by prepending a column of ones to the $N \\times d$ matrix of samples: $\\mathbf{X}_{\\text{aug}} = [\\mathbf{1}_N, \\mathbf{X}]$. The equation is solved numerically for $\\boldsymbol{\\beta}$ using a stable linear algebra solver, which is more robust than computing the matrix inverse directly.\n\nWith the coefficient vector $\\boldsymbol{\\beta}$ determined, we can calculate the predicted local contrast. The coefficient of interest is $\\beta_1$, which corresponds to the feature $x_1$. The predicted contrast is given by $C_{\\text{pred}} = |2\\sigma \\beta_1|$.\n\nWe then compute the true local contrast, $C_{\\text{true}}$, which is defined as the actual change in the function $f(\\mathbf{x})$ across the interval $[x_{0,1} - \\sigma, x_{0,1} + \\sigma]$ on the first coordinate:\n$$ C_{\\text{true}} = \\left| f(\\mathbf{x}_0 + \\sigma\\mathbf{e}_1) - f(\\mathbf{x}_0 - \\sigma\\mathbf{e}_1) \\right| $$\nwhere $\\mathbf{e}_1$ is the first standard basis vector. This is calculated by evaluating $f$ at the two points $\\mathbf{x}_{\\text{plus}} = \\mathbf{x}_0 + \\sigma\\mathbf{e}_1$ and $\\mathbf{x}_{\\text{minus}} = \\mathbf{x}_0 - \\sigma\\mathbf{e}_1$.\n\nFinally, we apply the capture criterion. The local surrogate is deemed to have captured the threshold effect if the absolute error between the predicted and true contrasts is within a specified tolerance:\n$$ |C_{\\text{pred}} - C_{\\text{true}}| \\le \\varepsilon \\max\\{1, |J|\\} $$\nWith the given value $\\varepsilon = 0.35$, this inequality is evaluated. The resulting boolean value (True or False) is recorded for the test case. This entire process is repeated for all four test cases, and the sequence of boolean results is formatted into the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LIME surrogate model validation problem for a series of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, tau, J, x_0, s, sigma, N)\n    test_cases = [\n        # Case A: near-threshold, small kernel\n        (2, 0.0, 4.0, (0.02, 0.0), 0.2, 0.08, 12000),\n        # Case B: far from threshold\n        (2, 0.0, 4.0, (0.5, 0.0), 0.2, 0.08, 8000),\n        # Case C: exactly at threshold, very wide kernel\n        (2, 0.0, 4.0, (0.0, 0.0), 0.05, 0.5, 12000),\n        # Case D: exactly at threshold, very narrow kernel, higher dimension\n        (3, 0.0, 4.0, (0.0, 0.1, -0.1), 0.2, 0.02, 20000),\n    ]\n\n    # Fixed tolerance for the capture criterion\n    epsilon = 0.35\n    \n    # Store boolean results for each case\n    results = []\n\n    for case in test_cases:\n        d, tau, J, x_0_tuple, s, sigma, N = case\n        x_0 = np.array(x_0_tuple)\n\n        # Set the pseudo-random seed for reproducibility for each case\n        np.random.seed(12345)\n        \n        # 1. Generate N samples from N(x_0, s^2 * I_d)\n        samples = x_0 + s * np.random.standard_normal(size=(N, d))\n        \n        # 2. Evaluate the black-box function f(x) for all samples\n        # f(x) = J * 1{x_1 = tau}\n        f_values = J * (samples[:, 0] = tau).astype(float)\n        \n        # 3. Compute weights w(x) for all samples\n        # w(x) = exp(-||x - x_0||^2 / (2 * sigma^2))\n        sq_dists = np.sum((samples - x_0)**2, axis=1)\n        weights = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # 4. Fit the locally weighted linear regression\n        # Create the augmented design matrix X_aug with an intercept column\n        X_aug = np.hstack([np.ones((N, 1)), samples])\n        Y = f_values\n        \n        # Construct the matrices for the normal equation: (X^T W X) beta = X^T W Y\n        # To do this efficiently, we use broadcasting with the weights vector\n        # instead of creating a large diagonal matrix W.\n        # Let A = X^T W X and b = X^T W Y\n        \n        # A = X_aug.T @ (weights[:, np.newaxis] * X_aug)\n        A = X_aug.T @ (weights.reshape(-1, 1) * X_aug) \n        # b = X_aug.T @ (weights * Y)\n        b = X_aug.T @ (weights * Y)\n        \n        # Solve the linear system A * beta = b for beta\n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse as a fallback for singular or ill-conditioned matrices\n            beta = np.linalg.pinv(A) @ b\n        \n        # The coefficient for the first coordinate, x_1, is beta[1]\n        beta_1 = beta[1]\n        \n        # 5. Compute C_true and C_pred\n        \n        # C_true = | f(x_0 + sigma*e_1) - f(x_0 - sigma*e_1) |\n        f_plus = J if (x_0[0] + sigma) = tau else 0.0\n        f_minus = J if (x_0[0] - sigma) = tau else 0.0\n        C_true = np.abs(f_plus - f_minus)\n        \n        # C_pred = | 2 * sigma * beta_1 |\n        C_pred = np.abs(2 * sigma * beta_1)\n        \n        # 6. Apply the capture criterion\n        abs_error = np.abs(C_pred - C_true)\n        tolerance_threshold = epsilon * np.max([1.0, np.abs(J)])\n        \n        is_captured = abs_error = tolerance_threshold\n        results.append(is_captured)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3140899"}, {"introduction": "Real-world phenomena are often complex, with feature interactions where the effect of one variable depends on the value of another. A simple linear LIME model, being purely additive, cannot capture such dependencies. This final practice [@problem_id:3140863] equips you with a powerful statistical technique to detect this non-additive behavior by comparing the goodness-of-fit between a purely additive local surrogate and an augmented model that includes a pairwise interaction term. A significant improvement in fit provides strong, quantitative evidence that the underlying black-box model exhibits feature interactions in the local region.", "problem": "Consider a black-box prediction function $f:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ and a target point $x_0 \\in \\mathbb{R}^d$. Local Interpretable Model-Agnostic Explanations (LIME) approximates $f$ near $x_0$ by fitting a locally weighted surrogate model using samples around $x_0$. Assume the following foundational base: locally weighted least squares regression fits coefficients by minimizing a weighted sum of squared residuals, and an additive model is a surrogate of the form $g(x)=\\sum_{j=1}^d g_j(x_j)$, where each $g_j$ depends only on feature $j$. Your task is to design a program that tests whether LIME can detect non-additivity by fitting an additive surrogate and measuring residual structure indicative of interactions near $x_0$.\n\nDefinitions and requirements:\n- Use locally weighted least squares with Gaussian weights. For samples $\\{x_i\\}_{i=1}^n$ drawn around $x_0$, define weights $w_i=\\exp\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right)$, where $\\sigma0$ is a kernel width parameter.\n- Construct an additive surrogate $g_{\\text{add}}(x)$ as a local polynomial model centered at $x_0$ that includes only univariate terms up to degree $p$ for each feature, and no cross-feature interaction terms. Specifically, let $z_i=x_i-x_0$, and define the design matrix for additive fitting to include a bias term and monomials $z_{i,j}^k$ for each feature index $j \\in \\{1,\\dots,d\\}$ and each degree $k \\in \\{1,\\dots,p\\}$, without any products of different features.\n- Fit $g_{\\text{add}}(x)$ by minimizing $\\sum_{i=1}^n w_i\\left(f(x_i)-g_{\\text{add}}(x_i)\\right)^2$ via weighted least squares with a small ridge regularization to ensure numerical stability.\n- Compute the baseline locally weighted mean squared error (MSE) $E_{\\text{add}}=\\frac{\\sum_{i=1}^n w_i\\left(f(x_i)-g_{\\text{add}}(x_i)\\right)^2}{\\sum_{i=1}^n w_i}$.\n- Augment the additive design with first-order pairwise interaction terms $z_{i,j}z_{i,k}$ for all $jk$, refit to obtain $g_{\\text{int}}(x)$, and compute $E_{\\text{int}}$ analogously.\n- Define the interaction improvement ratio $R=\\frac{E_{\\text{add}}-E_{\\text{int}}}{E_{\\text{add}}}$. Declare that non-additivity is detected if $R \\ge \\tau$ for a given threshold $\\tau \\in (0,1)$.\n\nSampling protocol:\n- For each case, draw $n$ samples $x_i$ independently from a multivariate normal distribution $\\mathcal{N}(x_0, s^2 I_d)$, where $s0$ is a sampling scale and $I_d$ is the $d \\times d$ identity matrix.\n- Evaluate responses $y_i=f(x_i)+\\epsilon_i$, with $\\epsilon_i \\sim \\mathcal{N}(0,\\eta^2)$, where $\\eta \\ge 0$ is a noise standard deviation.\n\nProgram tasks:\n- Implement the above local fitting and detection procedure.\n- Use $d=2$, $n=1000$, ridge regularization $\\lambda=10^{-8}$, and polynomial degree $p$ provided per test case.\n- For each test case, produce a boolean indicating whether non-additivity is detected.\n\nTest suite:\nProvide results for the following $5$ cases, each specified by $(f, x_0, \\sigma, s, p, \\eta, \\tau)$:\n- Case $1$: $f(x)=\\sin(x_1)+\\cos(x_2)$, $x_0=(0.2,-0.1)$, $\\sigma=0.3$, $s=0.25$, $p=3$, $\\eta=0$, $\\tau=0.12$.\n- Case $2$: $f(x)=x_1 x_2$, $x_0=(0.1,-0.2)$, $\\sigma=0.3$, $s=0.25$, $p=2$, $\\eta=0$, $\\tau=0.12$.\n- Case $3$: $f(x)=\\sin(x_1)+0.3 x_2 + 0.05 x_1 x_2$, $x_0=(0,0)$, $\\sigma=0.3$, $s=0.3$, $p=3$, $\\eta=0$, $\\tau=0.12$.\n- Case $4$: $f(x)=\\sin(x_1)+x_2^2$, $x_0=(0.5,-0.5)$, $\\sigma=0.25$, $s=0.25$, $p=3$, $\\eta=0.1$, $\\tau=0.12$.\n- Case $5$: $f(x)=\\sin(3 x_1)\\cos(3 x_2)$, $x_0=(0.0,0.0)$, $\\sigma=0.25$, $s=0.2$, $p=3$, $\\eta=0$, $\\tau=0.12$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots,result_5]$), where each $result_i$ is a boolean corresponding to case $i$ in order.", "solution": "The problem is valid. It presents a well-defined task in computational statistics, specifically in the area of local model interpretability, which is grounded in established principles of statistical learning. All provided data, definitions, and constraints are complete, consistent, and scientifically sound.\n\nHerein, we provide a principled solution to the problem of detecting non-additivity in a black-box function $f$ in the vicinity of a point $x_0$. This procedure is inspired by the Local Interpretable Model-Agnostic Explanations (LIME) framework, which approximates a complex model locally with a simpler, interpretable one.\n\n### 1. Theoretical Framework\n\nThe core idea is to compare the goodness-of-fit of two local surrogate models: a purely additive model and a model that includes interaction terms. A significant improvement in fit when interaction terms are added is taken as evidence of non-additive behavior in the original function $f$ near $x_0$.\n\n#### 1.1. Local Sampling and Weighting\n\nTo analyze the local behavior of $f$ around $x_0 \\in \\mathbb{R}^d$, we generate a set of $n$ samples, $\\{x_i\\}_{i=1}^n$, from a multivariate normal distribution $\\mathcal{N}(x_0, s^2 I_d)$, where $s$ is a sampling scale parameter. The corresponding responses are computed as $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\eta^2)$ represents optional observation noise.\n\nThe \"local\" nature of the analysis is enforced by assigning a weight to each sample $x_i$ that decreases with its distance from $x_0$. We use a Gaussian kernel for this purpose:\n$$\nw_i = \\exp\\left(-\\frac{\\|x_i - x_0\\|_2^2}{2\\sigma^2}\\right)\n$$\nwhere $\\sigma  0$ is the kernel width that controls the size of the \"neighborhood\".\n\n#### 1.2. Surrogate Models\n\nWe define two polynomial surrogate models centered at $x_0$ by using the shifted coordinates $z = x - x_0$.\n\n**Additive Model ($g_{\\text{add}}$):** This model is constrained to be a sum of functions of individual features. It includes a bias term and univariate polynomial terms up to degree $p$ for each feature. Its general form is:\n$$\ng_{\\text{add}}(x; \\boldsymbol{\\beta}) = \\beta_0 + \\sum_{j=1}^{d} \\sum_{k=1}^{p} \\beta_{j,k} (x_j - x_{0,j})^k\n$$\nThis model can only capture main effects and is, by construction, additive.\n\n**Interaction Model ($g_{\\text{int}}$):** This model extends the additive model by including first-order pairwise interaction terms. This allows it to capture simple forms of non-additivity where the effect of one feature depends on the level of another. Its form is:\n$$\ng_{\\text{int}}(x; \\boldsymbol{\\gamma}) = g_{\\text{add}}(x; \\boldsymbol{\\beta}') + \\sum_{1 \\le j  k \\le d} \\gamma_{j,k} (x_j - x_{0,j})(x_k - x_{0,k})\n$$\nFor the specified problem with $d=2$, this simplifies to:\n$$\ng_{\\text{int}}(x; \\boldsymbol{\\gamma}) = g_{\\text{add}}(x; \\boldsymbol{\\beta}') + \\gamma_{1,2} (x_1 - x_{0,1})(x_2 - x_{0,2})\n$$\n\n#### 1.3. Model Fitting via Weighted Least Squares\n\nThe coefficients of both models ($\\boldsymbol{\\beta}$ and $\\boldsymbol{\\gamma}$) are determined by minimizing the weighted sum of squared residuals. To ensure numerical stability, a ridge regularization term with parameter $\\lambda$ is added. For a generic surrogate model $g(x_i)$ with design matrix $\\mathbf{X}$ and coefficient vector $\\boldsymbol{\\theta}$, we solve the following optimization problem:\n$$\n\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} w_i(y_i - g(x_i; \\boldsymbol{\\theta}))^2 + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2\n$$\nThis is a weighted ridge regression problem. The solution is found by solving the normal equations:\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\theta} = \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nwhere $\\mathbf{W}$ is a diagonal matrix with $W_{ii} = w_i$, $\\mathbf{y}$ is the vector of responses, and $\\mathbf{I}$ is the identity matrix.\n\n#### 1.4. Detection Criterion\n\nAfter fitting both models and obtaining their respective coefficient vectors $\\hat{\\boldsymbol{\\beta}}_{\\text{add}}$ and $\\hat{\\boldsymbol{\\gamma}}_{\\text{int}}$, we evaluate their performance using the weighted mean squared error (MSE). For the additive model, this is:\n$$\nE_{\\text{add}} = \\frac{\\sum_{i=1}^n w_i (y_i - g_{\\text{add}}(x_i; \\hat{\\boldsymbol{\\beta}}_{\\text{add}}))^2}{\\sum_{i=1}^n w_i}\n$$\nAn analogous expression gives $E_{\\text{int}}$. Since $g_{\\text{int}}$ is a super-model of $g_{\\text{add}}$ (it has all the same features plus more), it must be that $E_{\\text{int}} \\le E_{\\text{add}}$. We quantify the improvement by calculating the interaction improvement ratio, $R$:\n$$\nR = \\frac{E_{\\text{add}} - E_{\\text{int}}}{E_{\\text{add}}}\n$$\nThis ratio represents the fraction of the additive model's error that is explained by including interaction terms. We declare that non-additivity is detected if this improvement exceeds a predefined threshold $\\tau \\in (0,1)$:\n$$\n\\text{Detection} \\iff R \\ge \\tau\n$$\n\n### 2. Algorithmic Implementation\n\nThe implementation proceeds by translating the above steps into a computational procedure. For each test case:\n1.  **Set Random Seed**: A fixed random seed is used for the sampling process to ensure reproducibility.\n2.  **Data Generation**: $n=1000$ samples $x_i \\in \\mathbb{R}^2$ are drawn from $\\mathcal{N}(x_0, s^2 I_2)$. The responses $y_i=f(x_i) + \\epsilon_i$ are computed.\n3.  **Weight Calculation**: The Gaussian weights $w_i$ are computed based on the distance of each $x_i$ from $x_0$.\n4.  **Design Matrix Construction**:\n    -   The centered coordinates $z_i = x_i - x_0$ are computed.\n    -   The additive design matrix, $\\mathbf{X}_{\\text{add}}$, is constructed. For $d=2$, its columns correspond to a bias term ($1$), $z_{i,1}^k$ for $k \\in \\{1,..,p\\}$, and $z_{i,2}^k$ for $k \\in \\{1,..,p\\}$.\n    -   The interaction design matrix, $\\mathbf{X}_{\\text{int}}$, is formed by appending a column for the interaction term $z_{i,1}z_{i,2}$ to $\\mathbf{X}_{\\text{add}}$.\n5.  **Solve Linear Systems**: For each model, the system $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\theta} = \\mathbf{X}^T \\mathbf{W} \\mathbf{y}$ is solved for the coefficient vector $\\boldsymbol{\\theta}$ using `numpy.linalg.solve`.\n6.  **Error and Ratio Calculation**: The predictions $\\hat{y}_{\\text{add}}$ and $\\hat{y}_{\\text{int}}$ are calculated. The weighted MSEs, $E_{\\text{add}}$ and $E_{\\text{int}}$, are computed. Finally, the improvement ratio $R$ is calculated.\n7.  **Decision**: The result is determined by comparing $R$ with the given threshold $\\tau$. The entire procedure is encapsulated in a function and applied to each of the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef detect_non_additivity(f, x0, sigma, s, p, eta, tau, n, d, lambda_reg, rng):\n    \"\"\"\n    Detects non-additivity by fitting and comparing local additive and interaction models.\n\n    Args:\n        f (callable): The black-box function f(x), where x is an (n, d) array.\n        x0 (np.ndarray): The target point of shape (d,).\n        sigma (float): Kernel width for Gaussian weights.\n        s (float): Standard deviation for sampling around x0.\n        p (int): Maximum polynomial degree for additive features.\n        eta (float): Standard deviation of Gaussian noise on responses.\n        tau (float): Threshold for the interaction improvement ratio.\n        n (int): Number of samples to draw.\n        d (int): Dimension of the input space.\n        lambda_reg (float): Ridge regularization parameter.\n        rng (np.random.Generator): A NumPy random number generator instance.\n\n    Returns:\n        bool: True if non-additivity is detected, False otherwise.\n    \"\"\"\n    # 1. Sampling\n    x = rng.multivariate_normal(x0, s**2 * np.eye(d), size=n)\n    if eta > 0:\n        noise = rng.normal(0, eta, size=n)\n    else:\n        noise = 0.0\n    y = f(x) + noise\n\n    # 2. Centering and Weights\n    z = x - x0\n    sq_dists = np.sum(z**2, axis=1)\n    w = np.exp(-sq_dists / (2 * sigma**2))\n    sum_w = np.sum(w)\n    if sum_w  1e-12:  # Avoid division by zero, though highly unlikely\n        sum_w = 1.0\n\n    # 3. Additive Model\n    # 3.1. Construct the design matrix X_add\n    num_add_features = 1 + d * p\n    X_add = np.zeros((n, num_add_features))\n    X_add[:, 0] = 1.0  # Bias term\n    col_idx = 1\n    for j in range(d):\n        for k in range(1, p + 1):\n            X_add[:, col_idx] = z[:, j]**k\n            col_idx += 1\n            \n    # 3.2. Solve for beta_add using weighted least squares\n    A_add = X_add.T @ (w[:, np.newaxis] * X_add) + lambda_reg * np.eye(num_add_features)\n    b_add = X_add.T @ (w * y)\n    beta_add = np.linalg.solve(A_add, b_add)\n\n    # 3.3. Calculate E_add (weighted mean squared error)\n    y_pred_add = X_add @ beta_add\n    residuals_add = y - y_pred_add\n    E_add = np.sum(w * residuals_add**2) / sum_w\n\n    # 4. Interaction Model\n    # 4.1. Construct the design matrix X_int\n    interaction_term = z[:, 0] * z[:, 1]\n    X_int = np.c_[X_add, interaction_term]\n    num_int_features = X_int.shape[1]\n    \n    # 4.2. Solve for beta_int\n    A_int = X_int.T @ (w[:, np.newaxis] * X_int) + lambda_reg * np.eye(num_int_features)\n    b_int = X_int.T @ (w * y)\n    beta_int = np.linalg.solve(A_int, b_int)\n\n    # 4.3. Calculate E_int\n    y_pred_int = X_int @ beta_int\n    residuals_int = y - y_pred_int\n    E_int = np.sum(w * residuals_int**2) / sum_w\n\n    # 5. Detection\n    if E_add  1e-12:  # Additive model fits perfectly, no improvement possible\n        return False\n    \n    R = (E_add - E_int) / E_add\n    \n    return R >= tau\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define problem constants\n    N_SAMPLES = 1000\n    DIMENSION = 2\n    LAMBDA_REG = 1e-8\n    RNG = np.random.default_rng(seed=42) # For reproducibility\n\n    # Define test functions\n    f1 = lambda x: np.sin(x[:, 0]) + np.cos(x[:, 1])\n    f2 = lambda x: x[:, 0] * x[:, 1]\n    f3 = lambda x: np.sin(x[:, 0]) + 0.3 * x[:, 1] + 0.05 * x[:, 0] * x[:, 1]\n    f4 = lambda x: np.sin(x[:, 0]) + x[:, 1]**2\n    f5 = lambda x: np.sin(3 * x[:, 0]) * np.cos(3 * x[:, 1])\n\n    test_cases = [\n        # (f, x0, sigma, s, p, eta, tau)\n        (f1, np.array([0.2, -0.1]), 0.3, 0.25, 3, 0.0, 0.12),\n        (f2, np.array([0.1, -0.2]), 0.3, 0.25, 2, 0.0, 0.12),\n        (f3, np.array([0.0, 0.0]), 0.3, 0.3, 3, 0.0, 0.12),\n        (f4, np.array([0.5, -0.5]), 0.25, 0.25, 3, 0.1, 0.12),\n        (f5, np.array([0.0, 0.0]), 0.25, 0.2, 3, 0.0, 0.12),\n    ]\n\n    results = []\n    for f, x0, sigma, s, p, eta, tau in test_cases:\n        result = detect_non_additivity(\n            f, x0, sigma, s, p, eta, tau,\n            n=N_SAMPLES, d=DIMENSION, lambda_reg=LAMBDA_REG, rng=RNG\n        )\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3140863"}]}