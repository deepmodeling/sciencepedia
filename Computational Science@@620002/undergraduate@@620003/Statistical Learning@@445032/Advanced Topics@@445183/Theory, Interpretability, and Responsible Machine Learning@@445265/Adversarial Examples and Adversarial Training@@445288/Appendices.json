{"hands_on_practices": [{"introduction": "To build robust systems, we must first learn how to measure robustness precisely. This practice moves beyond qualitative statements and into quantitative certification. You will derive a closed-form expression for the minimum $\\ell_{\\infty}$ perturbation required to fool a linear classifier, providing a guaranteed \"robustness radius\" around a given data point [@problem_id:3097068].", "problem": "Consider a multiclass linear classifier with $K$ classes acting on inputs $x \\in \\mathbb{R}^d$. For each class $i \\in \\{1,\\dots,K\\}$ the score is $f_i(x) = w_i^\\top x + b_i$ with parameters $w_i \\in \\mathbb{R}^d$ and $b_i \\in \\mathbb{R}$. The predicted class is $y = \\arg\\max_{i} f_i(x)$. An adversarial perturbation under the $\\ell_{\\infty}$ norm is any $\\delta \\in \\mathbb{R}^d$ such that $\\|\\delta\\|_{\\infty} \\le \\varepsilon$ for some $\\varepsilon \\ge 0$. Define the smallest $\\ell_{\\infty}$-norm needed to change the predicted class away from $y$ as the minimal $\\varepsilon$ for which there exists $\\delta$ with $\\|\\delta\\|_{\\infty} \\le \\varepsilon$ and some $i \\neq y$ satisfying $f_i(x+\\delta) \\ge f_y(x+\\delta)$.\n\nTask A (Derivation): Starting only from the definitions above and standard inequalities from real analysis, derive a closed-form expression for this minimal $\\varepsilon$ in terms of the model parameters $\\{w_i,b_i\\}_{i=1}^K$ and the input $x$. Your derivation must be explicit about how the expression arises from norm properties and score differences between the true class $y$ and any alternative class $i \\neq y$.\n\nTask B (Numerical validation on a concrete model): Consider $d = 2$, $K = 3$, and\n- $w_1 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, $b_1 = 1$,\n- $w_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $b_2 = 0$,\n- $w_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$, $b_3 = 2$,\nwith input $x = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. \nPerform the following:\n- Identify the predicted class $y$ at $x$.\n- Using your expression from Task A, compute the certified $\\ell_{\\infty}$ radius $r$ at $x$, defined as the minimal $\\varepsilon$ needed to change the predicted class away from $y$ in the sense above.\n- Validate the tightness numerically by explicitly constructing one adversarial perturbation $\\delta^\\star$ with $\\|\\delta^\\star\\|_{\\infty} = r$ that makes some $i \\neq y$ satisfy $f_i(x+\\delta^\\star) \\ge f_y(x+\\delta^\\star)$.\n\nProvide as your final answer only the single number $r$ for the given numeric instance in Task B (no units and no additional text). Do not round; give the exact value.", "solution": "The problem is evaluated to be valid as it is scientifically grounded in the principles of statistical learning, well-posed, objective, and contains all necessary information for a unique solution.\n\n**Task A: Derivation of the minimal $\\ell_{\\infty}$ perturbation**\n\nLet the input be $x \\in \\mathbb{R}^d$ and the linear classifier be defined by the score functions $f_i(x) = w_i^\\top x + b_i$ for $i \\in \\{1, \\dots, K\\}$. The predicted class is $y = \\arg\\max_{i} f_i(x)$. We assume that for the given $x$, the maximum is unique, so $f_y(x) > f_j(x)$ for all $j \\neq y$.\n\nAn adversarial perturbation $\\delta \\in \\mathbb{R}^d$ is sought such that $\\|\\delta\\|_{\\infty} \\le \\varepsilon$ for some $\\varepsilon \\ge 0$, and the predicted class at the new point $x+\\delta$ is no longer $y$. The boundary condition for this change occurs when, for at least one class $i \\neq y$, its score becomes equal to or greater than the score of the original class $y$. That is, for some $i \\neq y$:\n$$f_i(x+\\delta) \\ge f_y(x+\\delta)$$\nSubstituting the definitions of the score functions, we get:\n$$w_i^\\top (x+\\delta) + b_i \\ge w_y^\\top (x+\\delta) + b_y$$\nExpanding the terms yields:\n$$w_i^\\top x + w_i^\\top \\delta + b_i \\ge w_y^\\top x + w_y^\\top \\delta + b_y$$\nWe can rearrange this inequality to isolate the term involving $\\delta$:\n$$w_i^\\top \\delta - w_y^\\top \\delta \\ge (w_y^\\top x + b_y) - (w_i^\\top x + b_i)$$\nThe right-hand side is the difference in the original scores, $f_y(x) - f_i(x)$. The left-hand side can be factored:\n$$(w_i - w_y)^\\top \\delta \\ge f_y(x) - f_i(x)$$\nFor a fixed target class $i \\neq y$, we want to find the minimal $\\varepsilon$ such that there exists a $\\delta$ with $\\|\\delta\\|_{\\infty} \\le \\varepsilon$ that satisfies this inequality. To find the minimal $\\varepsilon$, we should choose a perturbation $\\delta$ that maximizes the left-hand side, $(w_i - w_y)^\\top \\delta$, for a given norm bound $\\|\\delta\\|_{\\infty} = \\varepsilon$.\n\nThe relationship between the $\\ell_1$ and $\\ell_{\\infty}$ norms is defined by HÃ¶lder's inequality, which states that for any two vectors $u, v \\in \\mathbb{R}^d$, $|u^\\top v| \\le \\|u\\|_1 \\|v\\|_{\\infty}$. The equality is achieved when the elements of $v$ are chosen appropriately based on the signs of the elements of $u$. To maximize the dot product $(w_i - w_y)^\\top \\delta$, we should choose $\\delta$ such that each of its components $\\delta_j$ has the same sign as the corresponding component of the vector $(w_i - w_y)$, and its magnitude is maximized, i.e., $|\\delta_j| = \\varepsilon$.\nSpecifically, the optimal choice for $\\delta$ is $\\delta = \\varepsilon \\cdot \\text{sign}(w_i - w_y)$, where the sign function is applied element-wise. With this choice, the dot product becomes:\n$$(w_i - w_y)^\\top \\delta = (w_i - w_y)^\\top (\\varepsilon \\cdot \\text{sign}(w_i - w_y)) = \\varepsilon \\sum_{j=1}^d (w_i - w_y)_j \\cdot \\text{sign}((w_i - w_y)_j) = \\varepsilon \\sum_{j=1}^d |(w_i - w_y)_j| = \\varepsilon \\|w_i - w_y\\|_1$$\nThis represents the maximum possible value for the left-hand side of the inequality given the constraint $\\|\\delta\\|_{\\infty} \\le \\varepsilon$.\n\nSubstituting this maximum value back into the inequality, we find the condition on $\\varepsilon$ to successfully attack class $y$ in favor of class $i$:\n$$\\varepsilon \\|w_i - w_y\\|_1 \\ge f_y(x) - f_i(x)$$\nSince $f_y(x) > f_i(x)$ and $\\|w_i - w_y\\|_1 > 0$ (assuming $w_i \\neq w_y$), the minimal $\\varepsilon$ required to make class $i$ a potential adversary, denoted $\\varepsilon_i$, is:\n$$\\varepsilon_i = \\frac{f_y(x) - f_i(x)}{\\|w_i - w_y\\|_1}$$\nThe problem asks for the smallest $\\varepsilon$ needed to change the predicted class away from $y$ to *any* other class. This corresponds to finding the \"most vulnerable\" direction, which is the class $i \\neq y$ that requires the smallest perturbation $\\varepsilon_i$. Therefore, the minimal overall perturbation $\\varepsilon$ is the minimum of these values over all possible adversarial target classes $i \\neq y$:\n$$\\varepsilon = \\min_{i \\neq y} \\left\\{ \\varepsilon_i \\right\\} = \\min_{i \\neq y} \\frac{f_y(x) - f_i(x)}{\\|w_i - w_y\\|_1}$$\nThis is the closed-form expression for the minimal $\\ell_{\\infty}$-norm perturbation.\n\n**Task B: Numerical validation on a concrete model**\n\nThe given parameters are $d=2$, $K=3$, with:\n$w_1 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, b_1 = 1$,\n$w_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, b_2 = 0$,\n$w_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}, b_3 = 2$,\nand the input is $x = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nFirst, we identify the predicted class $y$ by computing the scores $f_i(x)$:\n$f_1(x) = w_1^\\top x + b_1 = (2)(1) + (-1)(2) + 1 = 2 - 2 + 1 = 1$.\n$f_2(x) = w_2^\\top x + b_2 = (0)(1) + (1)(2) + 0 = 0 + 2 + 0 = 2$.\n$f_3(x) = w_3^\\top x + b_3 = (-1)(1) + (0)(2) + 2 = -1 + 0 + 2 = 1$.\nThe scores are $\\{1, 2, 1\\}$. The maximum score is $f_2(x)=2$, so the predicted class is $y=2$.\n\nNext, we compute the certified $\\ell_{\\infty}$ radius $r$, which is the minimal $\\varepsilon$ from Task A. We must evaluate the required perturbation for each alternative class $i \\in \\{1, 3\\}$.\nFor $i=1$:\nThe score difference is $f_y(x) - f_1(x) = f_2(x) - f_1(x) = 2 - 1 = 1$.\nThe weight difference vector is $w_1 - w_y = w_1 - w_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$.\nThe $\\ell_1$-norm of this vector is $\\|w_1 - w_2\\|_1 = |2| + |-2| = 4$.\nThe minimal perturbation for this pair is $\\varepsilon_1 = \\frac{1}{4}$.\n\nFor $i=3$:\nThe score difference is $f_y(x) - f_3(x) = f_2(x) - f_3(x) = 2 - 1 = 1$.\nThe weight difference vector is $w_3 - w_y = w_3 - w_2 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\nThe $\\ell_1$-norm of this vector is $\\|w_3 - w_2\\|_1 = |-1| + |-1| = 2$.\nThe minimal perturbation for this pair is $\\varepsilon_3 = \\frac{1}{2}$.\n\nThe certified radius $r$ is the minimum of these values:\n$r = \\min\\{\\varepsilon_1, \\varepsilon_3\\} = \\min\\left\\{\\frac{1}{4}, \\frac{1}{2}\\right\\} = \\frac{1}{4}$.\n\nFinally, we validate this result by constructing an adversarial perturbation $\\delta^\\star$ with $\\|\\delta^\\star\\|_{\\infty} = r = \\frac{1}{4}$. The minimum was found for the target class $i=1$. The optimal perturbation is constructed as $\\delta^\\star = r \\cdot \\text{sign}(w_1 - w_2)$.\nUsing $w_1 - w_2 = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$, we have $\\text{sign}(w_1 - w_2) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThus, $\\delta^\\star = \\frac{1}{4} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ -1/4 \\end{pmatrix}$.\nThe $\\ell_{\\infty}$-norm is $\\|\\delta^\\star\\|_{\\infty} = \\max\\{|1/4|, |-1/4|\\} = 1/4 = r$, as required.\nThe perturbed input is $x' = x + \\delta^\\star = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 1/4 \\\\ -1/4 \\end{pmatrix} = \\begin{pmatrix} 5/4 \\\\ 7/4 \\end{pmatrix}$.\n\nLet's compute the scores at $x'$:\n$f_1(x') = w_1^\\top x' + b_1 = (2)(\\frac{5}{4}) + (-1)(\\frac{7}{4}) + 1 = \\frac{10}{4} - \\frac{7}{4} + \\frac{4}{4} = \\frac{7}{4}$.\n$f_2(x') = w_2^\\top x' + b_2 = (0)(\\frac{5}{4}) + (1)(\\frac{7}{4}) + 0 = \\frac{7}{4}$.\n$f_3(x') = w_3^\\top x' + b_3 = (-1)(\\frac{5}{4}) + (0)(\\frac{7}{4}) + 2 = -\\frac{5}{4} + \\frac{8}{4} = \\frac{3}{4}$.\n\nWith the perturbation $\\delta^\\star$, the scores for class $1$ and class $2$ are now equal: $f_1(x') = f_2(x') = 7/4$. This satisfies the boundary condition $f_1(x+\\delta^\\star) \\ge f_2(x+\\delta^\\star)$, confirming that a perturbation of this minimal size is sufficient to cause a tie, and any infinitesimally larger perturbation along this direction would change the prediction from class $2$ to class $1$. This validates the tightness of the derived radius.\nThe computed certified radius is $r = \\frac{1}{4}$.", "answer": "$$\n\\boxed{\\frac{1}{4}}\n$$", "id": "3097068"}, {"introduction": "While linear models offer a clean starting point, real-world systems often use non-linear neural networks. This exercise tackles the challenge of analyzing a simple ReLU network, which is fundamentally piecewise-linear. You will formulate a Linear Program (LP) to compute the exact adversarial robustness within a local region, contrasting this formal verification approach with the widely used Projected Gradient Descent (PGD) heuristic attack [@problem_id:3097097].", "problem": "You are given a fixed, fully specified, two-layer feedforward network with Rectified Linear Unit (ReLU) nonlinearities. The network maps an input vector $x \\in \\mathbb{R}^2$ to a scalar logit $f(x)$ and is defined by\n$$\nf(x) = v^{\\top} \\,\\sigma(W x + b) + c,\n$$\nwhere $\\sigma(\\cdot)$ is the elementwise Rectified Linear Unit (ReLU), $\\sigma(z) = \\max\\{0,z\\}$, $W \\in \\mathbb{R}^{2 \\times 2}$, $b \\in \\mathbb{R}^{2}$, $v \\in \\mathbb{R}^{2}$, and $c \\in \\mathbb{R}$. The parameters are fixed as\n$$\nW = \\begin{bmatrix} 1.0 & -1.0 \\\\ 0.5 & 1.0 \\end{bmatrix}, \\quad\nb = \\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}, \\quad\nv = \\begin{bmatrix} 1.2 \\\\ -0.7 \\end{bmatrix}, \\quad\nc = 0.5.\n$$\nConsider binary labels $y \\in \\{-1, +1\\}$ with the classification decision given by $\\mathrm{sign}(f(x))$. For a given input $x_0 \\in [0,1]^2$ and label $y$, define the margin $m(x_0) = y \\cdot f(x_0)$. An adversarial perturbation $\\delta \\in \\mathbb{R}^2$ flips the classification if $y \\cdot f(x_0 + \\delta) \\le 0$.\n\nWithin a fixed activation region determined at $x_0$ by the sign pattern of the preactivations $z(x) = W x + b$, the network is piecewise affine. Fix the activation pattern at $x_0$ by recording, for each hidden unit $j \\in \\{1,2\\}$, whether $z_j(x_0) > 0$ (active), $z_j(x_0) < 0$ (inactive), or $z_j(x_0) = 0$ (on the boundary). Inside that same activation region, $f(x)$ reduces to an affine function\n$$\nf(x) = a^{\\top} x + d,\n$$\nfor some effective coefficients $a \\in \\mathbb{R}^2$ and $d \\in \\mathbb{R}$ determined by $W$, $b$, $v$, $c$, and the fixed activation pattern at $x_0$.\n\nYour tasks are, for each test case $(x_0, y)$ listed below:\n- Compute the exact minimum $\\ell_{\\infty}$-norm adversarial perturbation that flips the classification while staying inside the same activation region as at $x_0$ and inside the box domain $[0,1]^2$. Formulate and solve the following Linear Program (LP):\n  - Decision variables: $\\delta \\in \\mathbb{R}^2$ and $t \\in \\mathbb{R}$.\n  - Objective: minimize $t$.\n  - Constraints:\n    1. $\\ell_{\\infty}$-norm epigraph: for each coordinate $i \\in \\{1,2\\}$,\n       $$\n       \\delta_i - t \\le 0, \\quad -\\delta_i - t \\le 0, \\quad t \\ge 0.\n       $$\n    2. Box domain constraints:\n       $$\n       x_{0,i} + \\delta_i \\le 1, \\quad x_{0,i} + \\delta_i \\ge 0, \\quad \\text{for } i \\in \\{1,2\\}.\n       $$\n    3. Misclassification (margin) constraint in the fixed activation region:\n       $$\n       y \\cdot \\left(a^{\\top}(x_0 + \\delta) + d\\right) \\le 0.\n       $$\n    4. Fixed activation region constraints, for each hidden unit $j \\in \\{1,2\\}$:\n       - If $z_j(x_0) > 0$, enforce $W_{j,:}(x_0 + \\delta) + b_j \\ge 0$.\n       - If $z_j(x_0) < 0$, enforce $W_{j,:}(x_0 + \\delta) + b_j \\le 0$.\n       - If $z_j(x_0) = 0$, enforce equality $W_{j,:}(x_0 + \\delta) + b_j = 0$.\n  The exact optimal adversarial budget within the activation region is $t^{\\star}$, the optimal LP objective value, and an optimal perturbation is $\\delta^{\\star}$.\n- Run Projected Gradient Descent (PGD) to attempt to find a misclassifying perturbation under the $\\ell_{\\infty}$ budget $t^{\\star}$, projecting onto the intersection of the $\\ell_{\\infty}$ ball centered at $x_0$ with radius $t^{\\star}$ and the box $[0,1]^2$. Use the true network (not the affine surrogate) to compute gradients of the margin $m(x) = y \\cdot f(x)$, with the ReLU subgradient taken as $0$ at exactly zero preactivations. Use the update\n  $$\n  x^{k+1} \\leftarrow \\Pi_{\\mathcal{B}_{\\infty}(x_0, t^{\\star}) \\cap [0,1]^2}\\left(x^k - \\alpha \\cdot \\mathrm{sign}\\left(\\nabla_x m(x^k)\\right)\\right),\n  $$\n  where $\\Pi$ denotes projection, $\\alpha$ is the step size, and start from $x^0 = x_0$. Use $\\alpha = t^{\\star}/10$ and exactly $40$ iterations. Record two booleans: whether PGD produced a misclassification at the final iterate, namely $y \\cdot f(x^{40}) \\le 0$, and whether all PGD iterates stayed in the same activation region as $x_0$ (treat a preactivation equal to $0$ as staying on the boundary if its absolute value is less than or equal to $10^{-6}$ at all iterates).\n- For each test case, return a list with three entries: the scalar $t^{\\star}$ (rounded to $6$ decimals), a boolean indicating whether PGD flipped the classification under budget $t^{\\star}$, and a boolean indicating whether all PGD iterates stayed in the same activation region as $x_0$.\n\nTest suite. Use the following four test cases, each with the input constrained to the box $[0,1]^2$:\n1. $x_0 = [0.6,\\, 0.3]$, $y = +1$.\n2. $x_0 = [0.2,\\, 0.4]$, $y = +1$.\n3. $x_0 = [0.7,\\, 0.5]$, $y = -1$.\n4. $x_0 = [0.9,\\, 0.1]$, $y = -1$.\n\nFinal output format. Your program should produce a single line of output containing a list of per-test-case results, where each result is a list of the form `[t_star, pgd_flipped, pgd_stayed]`. The entire output must be a single line, formatted as, for example, `[[0.123456,True,False],[\\dots],\\dots]`, with floating-point entries rounded to $6$ decimals and no spaces anywhere in the line.", "solution": "The user has provided a well-defined problem in the domain of adversarial examples in statistical learning. The task is to analyze a small, two-layer Rectified Linear Unit (ReLU) network. For several test cases, we must first compute the minimum $\\ell_{\\infty}$ perturbation required to cause a misclassification while staying within a fixed activation region and the input domain $[0,1]^2$. This is achieved by solving a Linear Program (LP). Second, we must use the computed minimum perturbation budget, $t^{\\star}$, to run a Projected Gradient Descent (PGD) attack and verify its outcome.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens:**\n- **Network Definition:** A two-layer feedforward network $f(x) = v^{\\top} \\sigma(W x + b) + c$, where $\\sigma(\\cdot)$ is the element-wise ReLU function.\n- **Parameters:**\n  - $W = \\begin{bmatrix} 1.0 & -1.0 \\\\ 0.5 & 1.0 \\end{bmatrix}$\n  - $b = \\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}$\n  - $v = \\begin{bmatrix} 1.2 \\\\ -0.7 \\end{bmatrix}$\n  - $c = 0.5$\n- **Input Domain:** $x \\in [0,1]^2$.\n- **Labels:** $y \\in \\{-1, +1\\}$.\n- **Classification Rule:** $\\mathrm{sign}(f(x))$.\n- **Affine Approximation:** Inside a fixed activation region determined by $x_0$, the network reduces to $f(x) = a^{\\top} x + d$. The activation pattern is determined by the signs of the preactivations $z(x_0) = Wx_0 + b$.\n- **Task 1 (LP):** Minimize $t$ subject to:\n  1. $\\ell_{\\infty}$-norm: $\\delta_i - t \\le 0, \\quad -\\delta_i - t \\le 0, \\quad t \\ge 0$ for $i \\in \\{1,2\\}$.\n  2. Box domain: $0 \\le x_{0,i} + \\delta_i \\le 1$ for $i \\in \\{1,2\\}$.\n  3. Misclassification: $y \\cdot (a^{\\top}(x_0 + \\delta) + d) \\le 0$.\n  4. Fixed activation region: For each hidden unit $j \\in \\{1,2\\}$, the sign of $W_{j,:}(x_0 + \\delta) + b_j$ must match the sign of $W_{j,:}x_0 + b_j$.\n- **Task 2 (PGD):**\n  - **Budget:** $\\epsilon = t^{\\star}$ (from LP).\n  - **Update Rule:** $x^{k+1} \\leftarrow \\Pi_{\\mathcal{B}_{\\infty}(x_0, t^{\\star}) \\cap [0,1]^2}\\left(x^k - \\alpha \\cdot \\mathrm{sign}\\left(\\nabla_x m(x^k)\\right)\\right)$.\n  - **Margin:** $m(x) = y \\cdot f(x)$.\n  - **Gradient:** Subgradient of ReLU at zero is $0$.\n  - **Parameters:** Start at $x^0=x_0$, step size $\\alpha = t^{\\star}/10$, iterations $= 40$.\n  - **Outputs:**\n    - `pgd_flipped`: boolean, $y \\cdot f(x^{40}) \\le 0$.\n    - `pgd_stayed`: boolean, all iterates $x^k$ remain in the activation region of $x_0$ (boundary check tolerance $10^{-6}$).\n- **Test Cases:**\n  1. $x_0 = [0.6, 0.3], y = +1$.\n  2. $x_0 = [0.2, 0.4], y = +1$.\n  3. $x_0 = [0.7, 0.5], y = -1$.\n  4. $x_0 = [0.9, 0.1], y = -1$.\n- **Output Format:** A list of lists `[[t_star, pgd_flipped, pgd_stayed], ...]`, with $t^{\\star}$ rounded to $6$ decimals.\n\n**1.2. Validate Using Extracted Givens:**\n- **Scientific Grounding:** The problem is firmly rooted in established concepts of adversarial machine learning. Analyzing ReLU networks via piecewise affine functions and using LPs for verification are standard techniques. PGD is the canonical method for generating adversarial examples. All aspects are scientifically sound.\n- **Well-Posedness:** The problem is self-contained and provides all necessary parameters and specifications. The LP is well-defined and, for these constraints, is expected to have a unique optimal solution. The PGD algorithm is deterministic.\n- **Objectivity:** The problem is stated in precise mathematical and algorithmic terms, free of any subjectivity or ambiguity.\n\n**1.3. Verdict and Action:**\nThe problem is **valid**. It is a standard, well-posed exercise in computational machine learning. I will proceed to construct the solution.\n\n### Step 2: Solution\n\nThe solution will be implemented by addressing the two main tasks for each test case.\n\n**Part A: Minimal Adversarial Perturbation via Linear Programming**\n\nFor a given input $x_0$, we first determine the network's local affine behavior. The pre-activation for the hidden layer is $z(x) = Wx + b$. The activation pattern is determined by the sign of each component of $z(x_0)$.\nLet $S$ be a diagonal matrix where $S_{jj} = 1$ if $z_j(x_0)>0$ and $S_{jj}=0$ otherwise. Inside this region, the network's output is:\n$f(x) = v^{\\top} (S (Wx+b)) + c = (v^{\\top} S W) x + (v^{\\top} S b + c)$\nThis gives the affine coefficients $a = W^{\\top}S v$ and $d = v^{\\top}S b + c$.\n\nWith $a$ and $d$ determined, we construct a linear program to find the minimal perturbation $\\delta$. The decision variables are $\\delta \\in \\mathbb{R}^2$ and $t \\in \\mathbb{R}$, which we can pack into a single vector for the solver. The objective is to minimize $t$. The constraints are formulated as linear inequalities:\n1.  **$\\ell_{\\infty}$-Norm:** The condition $\\|\\delta\\|_{\\infty} \\le t$ is encoded as four linear inequalities: $\\delta_1 \\le t$, $-\\delta_1 \\le t$, $\\delta_2 \\le t$, and $-\\delta_2 \\le t$.\n2.  **Box Constraints:** The perturbed input $x_0 + \\delta$ must be in $[0,1]^2$. This gives four inequalities: $x_{0,i} + \\delta_i \\le 1$ and $x_{0,i} + \\delta_i \\ge 0$ for $i \\in \\{1,2\\}$, which can be rearranged to be linear in $\\delta_i$.\n3.  **Misclassification:** $y \\cdot f(x_0+\\delta) \\le 0$ becomes $y(a^{\\top}(x_0+\\delta)+d) \\le 0$, which is a linear inequality in $\\delta$.\n4.  **Activation Region:** For each hidden neuron $j$, the sign of the new pre-activation $z_j(x_0+\\delta)$ must match the sign of $z_j(x_0)$.\n    - If $z_j(x_0)>0$, we need $W_{j,:}(x_0+\\delta)+b_j \\ge 0$.\n    - If $z_j(x_0)<0$, we need $W_{j,:}(x_0+\\delta)+b_j \\le 0$.\n    - If $z_j(x_0)=0$, we need $W_{j,:}(x_0+\\delta)+b_j = 0$.\nThese are all linear constraints on $\\delta$. We use `scipy.optimize.linprog` to solve this LP and obtain the optimal budget $t^{\\star}$.\n\n**Part B: PGD Attack and Verification**\n\nUsing the budget $t^{\\star}$ from the LP, we run a PGD attack against the original, non-linear network. PGD iteratively attempts to minimize the margin $m(x) = y \\cdot f(x)$ by taking steps in the direction of the sign of the margin's gradient.\nThe gradient of the margin is $\\nabla_x m(x) = y \\cdot \\nabla_x f(x)$. The gradient of the network output is $\\nabla_x f(x) = W^{\\top} \\mathrm{diag}(\\sigma'(Wx+b)) v$, where the subgradient of the ReLU $\\sigma'(z_j)$ is $1$ if $z_j>0$ and $0$ otherwise, as specified.\n\nThe PGD update for $40$ iterations is:\n1.  Compute gradient sign: $g = \\mathrm{sign}(\\nabla_x m(x^k))$.\n2.  Update step: $x' = x^k - \\alpha g$, with step size $\\alpha = t^{\\star}/10$.\n3.  Projection: $x^{k+1} = \\Pi_{\\mathcal{B}_{\\infty}(x_0, t^{\\star}) \\cap [0,1]^2}(x')$. This projection clips the coordinates of $x'$ to the range $[\\max(0, x_{0,i}-t^{\\star}), \\min(1, x_{0,i}+t^{\\star})]$.\n\nAfter $40$ iterations, we check two conditions:\n1.  **`pgd_flipped`**: Whether the final point $x^{40}$ is misclassified, i.e., $y \\cdot f(x^{40}) \\le 0$.\n2.  **`pgd_stayed`**: During the PGD run, we must track whether every iterate $x^k$ for $k \\in \\{1, ..., 40\\}$ remains in the same activation region as $x_0$. For each neuron $j$ and each iterate $x^k$, we check if the sign partition of $z_j(x_0)$ is preserved for $z_j(x^k)$, respecting the $10^{-6}$ tolerance for boundary conditions. If any iterate leaves the region, this flag is set to `False`.\n\nThe results from these two steps for each test case are collected and formatted into the required single-line output.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n# Define global network parameters as they are fixed constants for all computations.\nW_ = np.array([[1.0, -1.0], [0.5, 1.0]], dtype=np.float64)\nb_ = np.array([-0.2, 0.1], dtype=np.float64)\nv_ = np.array([1.2, -0.7], dtype=np.float64)\nc_ = 0.5\n\ndef network_f(x):\n    \"\"\"Computes the network output logit f(x) for a single input vector x.\"\"\"\n    z = W_ @ x + b_\n    h = np.maximum(0, z)\n    return h @ v_ + c_\n\ndef grad_margin(x, y_label):\n    \"\"\"Computes the gradient of the margin y*f(x) w.r.t x.\"\"\"\n    z = W_ @ x + b_\n    # The problem specifies using a subgradient of 0 for ReLU at pre-activation 0.\n    # This is equivalent to checking for z > 0.\n    S_grad = np.diag((z > 0).astype(float))\n    grad_f = W_.T @ S_grad @ v_\n    return y_label * grad_f\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        (np.array([0.6, 0.3], dtype=np.float64), 1),\n        (np.array([0.2, 0.4], dtype=np.float64), 1),\n        (np.array([0.7, 0.5], dtype=np.float64), -1),\n        (np.array([0.9, 0.1], dtype=np.float64), -1),\n    ]\n\n    results = []\n    # Tolerance for determining the activation pattern for the LP formulation.\n    TOL = 1e-9\n\n    for x0, y in test_cases:\n        # ---- Part 1: LP for exact adversarial perturbation ----\n        z0 = W_ @ x0 + b_\n        # Diagonal matrix S indicates which ReLU units are active.\n        S = np.diag((z0 > TOL).astype(float))\n        # Effective affine parameters a and d for the local region.\n        a = W_.T @ S @ v_\n        d = v_.T @ S @ b_ + c_\n\n        # Variables for linprog are [delta_1, delta_2, t].\n        c_lp = np.array([0, 0, 1])\n        \n        A_ub_list, b_ub_list = [], []\n        \n        # l_inf norm constraints: |delta_i| <= t\n        A_ub_list.extend([[1, 0, -1], [-1, 0, -1], [0, 1, -1], [0, -1, -1]])\n        b_ub_list.extend([0, 0, 0, 0])\n        \n        # Box constraints: 0 <= x0 + delta <= 1\n        A_ub_list.extend([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0]])\n        b_ub_list.extend([1 - x0[0], x0[0], 1 - x0[1], x0[1]])\n        \n        # Misclassification constraint: y * (a^T(x0 + delta) + d) <= 0\n        f_x0_affine = a @ x0 + d\n        A_ub_list.append([y * a[0], y * a[1], 0])\n        b_ub_list.append(-y * f_x0_affine)\n        \n        A_eq_list, b_eq_list = [], []\n        # Activation region constraints\n        for j in range(2):\n            w_j = W_[j, :]\n            if z0[j] > TOL:  # Must remain non-negative\n                A_ub_list.append([-w_j[0], -w_j[1], 0])\n                b_ub_list.append(z0[j])\n            elif z0[j] < -TOL:  # Must remain non-positive\n                A_ub_list.append([w_j[0], w_j[1], 0])\n                b_ub_list.append(-z0[j])\n            else:  # Must remain exactly zero\n                A_eq_list.append([w_j[0], w_j[1], 0])\n                b_eq_list.append(-z0[j])\n\n        A_ub = np.array(A_ub_list)\n        b_ub = np.array(b_ub_list)\n        A_eq = np.array(A_eq_list) if A_eq_list else None\n        b_eq = np.array(b_eq_list) if b_eq_list else None\n        \n        bounds = [(None, None), (None, None), (0, None)]\n        res = linprog(c=c_lp, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n        \n        t_star = float('inf')\n        pgd_flipped = False\n        pgd_stayed = True  # Default to True; if t_star=0, no steps taken, so it stays.\n        \n        if res.success:\n            t_star = res.fun\n            # ---- Part 2: PGD attack verification ----\n            epsilon = t_star\n            alpha = epsilon / 10.0\n            n_iters = 40\n            \n            x_k = np.copy(x0)\n            \n            # Pre-compute projection bounds for efficiency\n            proj_min = np.maximum(0.0, x0 - epsilon)\n            proj_max = np.minimum(1.0, x0 + epsilon)\n            \n            # For checking activation region\n            CHECK_TOL = 1e-6\n            z0_check = W_ @ x0 + b_\n            \n            if n_iters > 0 and alpha > 0:\n                for _ in range(n_iters):\n                    grad = grad_margin(x_k, y)\n                    x_k = x_k - alpha * np.sign(grad)\n                    x_k = np.clip(x_k, proj_min, proj_max)\n                    \n                    if pgd_stayed:\n                        zk_check = W_ @ x_k + b_\n                        for j in range(2):\n                            if (z0_check[j] > CHECK_TOL and zk_check[j] < -CHECK_TOL) or \\\n                               (z0_check[j] < -CHECK_TOL and zk_check[j] > CHECK_TOL) or \\\n                               (abs(z0_check[j]) <= CHECK_TOL and abs(zk_check[j]) > CHECK_TOL):\n                                pgd_stayed = False\n                                break\n            \n            final_logit = network_f(x_k)\n            pgd_flipped = (y * final_logit) <= 0\n        \n        results.append([t_star, pgd_flipped, pgd_stayed])\n        \n    # Format the final output string as per requirements.\n    result_strings = []\n    for t_val, flip_val, stay_val in results:\n        # The float format f'{val:.6f}' correctly rounds and formats.\n        t_str = f'{round(t_val, 6):.6f}'\n        result_strings.append(f\"[{t_str},{str(flip_val)},{str(stay_val)}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3097097"}, {"introduction": "A defense is only as good as the evaluation it withstands, and relying on a single attack method can be misleading. This practice reveals the critical phenomenon of \"obfuscated gradients,\" where a model appears robust simply because its loss landscape is difficult for gradient-based attacks to navigate. By comparing the failure of a gradient-based attack with the success of a simple query-based attack, you will learn to diagnose this false sense of security and appreciate the need for rigorous evaluation protocols [@problem_id:3097022].", "problem": "Consider binary classification in the setting of empirical risk minimization with a scalar score function $z(x) = s \\cdot (w^\\top x + b)$ and probability $p(x)$ for the positive class. Let the loss for a labeled example $(x,y)$ with $y \\in \\{0,1\\}$ be the Bernoulli cross-entropy defined by $L(x,y) = -\\left[y \\log p(x) + (1-y)\\log(1-p(x))\\right]$. A common defense in adversarial training is input gradient regularization, which penalizes the squared norm $\\|\\nabla_x p(x)\\|_2^2$ averaged over training examples. However, if the activation is saturated (for instance, piecewise flat outside a central linear region), then $\\nabla_x p(x)$ can be forced to zero without improving true robustness, potentially leading to obfuscated gradients.\n\nStarting from the following base:\n- Empirical risk minimization over parameters $w \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$, and a fixed scale $s \\in \\mathbb{R}$.\n- The Bernoulli cross-entropy $L(x,y)$ and its chain-rule relation to $\\nabla_x L(x,y)$ via $p(x)$.\n- The Fast Gradient Sign Method (FGSM), defined for an $\\ell_\\infty$ budget $\\epsilon > 0$ as $x_{\\text{adv}} = x + \\epsilon \\cdot \\operatorname{sign}(\\nabla_x L(x,y))$.\n- A simple black-box attack that searches a finite set of query directions $v$ with $\\|v\\|_\\infty = \\epsilon$ and declares success if $\\hat{y}(x+v) \\neq \\hat{y}(x)$, where $\\hat{y}(x) = \\mathbf{1}\\{p(x) \\ge 1/2\\}$.\n\nConstruct a toy counterexample consisting of two models:\n1. A smooth model with the logistic activation $p_{\\text{soft}}(x) = \\sigma\\!\\left(s_{\\text{soft}} \\cdot (w^\\top x + b)\\right)$, where $\\sigma(u) = 1/(1+e^{-u})$.\n2. A saturated model with a hard-saturating activation $p_{\\text{hard}}(x) = \\frac{1}{2}\\left(\\operatorname{clip}\\!\\left(s_{\\text{hard}}\\cdot(w^\\top x + b), -1, 1\\right) + 1\\right)$, where $\\operatorname{clip}(u,-1,1)$ is piecewise linear and flat outside $[-1,1]$.\n\nFor the smooth model, $\\nabla_x L(x,y)$ is nonzero near the decision boundary; FGSM should often succeed within a modest $\\ell_\\infty$ budget. For the saturated model, when $s_{\\text{hard}}\\cdot(w^\\top x + b)$ lies outside $[-1,1]$, the derivative of $p_{\\text{hard}}$ with respect to $x$ is identically zero, making $\\nabla_x L(x,y)$ equal to zero even though the decision boundary in input space may still be within reach of a black-box search. This mismatch exemplifies obfuscated gradients: gradient-based attacks fail, while query-based attacks succeed.\n\nYour task is to write a complete program that:\n- Implements both $p_{\\text{soft}}(x)$ and $p_{\\text{hard}}(x)$ with fixed parameters $w$, $b$, $s_{\\text{soft}}$, and $s_{\\text{hard}}$.\n- Computes $\\nabla_x L(x,y)$ for both models using the chain rule with respect to $p(x)$ and $z(x)$, taking care to handle the saturated derivative for $p_{\\text{hard}}(x)$ correctly.\n- Applies FGSM under an $\\ell_\\infty$ budget $\\epsilon$ and returns whether the label flips.\n- Applies a black-box query attack that evaluates a finite set of perturbations $v$ with $\\|v\\|_\\infty = \\epsilon$ (coordinate axes and diagonals) and returns whether any flips the label.\n- Diagnoses obfuscated gradients for a test case as a boolean value defined by $\\left[\\text{FGSM fails}\\right] \\wedge \\left[\\text{black-box succeeds}\\right]$.\n\nUse the following fixed parameters and test suite. All vectors are in $\\mathbb{R}^2$:\n- Model parameters: $w = [1, 1]$, $b = 0$, $s_{\\text{soft}} = 1$, $s_{\\text{hard}} = 50$.\n- Budget: $\\epsilon = 0.2$ (this is dimensionless; no physical units apply).\n- Test cases (each test case specifies the model, input $x$, and label $y$):\n    1. Smooth model, $x = [0.05, 0.05]$, $y = 1$.\n    2. Saturated model, $x = [0.05, 0.05]$, $y = 1$.\n    3. Saturated model, $x = [0.5, 0.5]$, $y = 1$.\n    4. Smooth model, $x = [-0.05, -0.05]$, $y = 0$.\n\nFor each test case, your program must output whether obfuscated gradients are detected, defined as a boolean computed by the rule above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, `[result_1,result_2,result_3,result_4]` where each `result_i` is either `True` or `False`.\n\nThe program must be self-contained and require no user input. It must rely only on the specified runtime environment and libraries.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded task within the domain of statistical learning and adversarial examples. It provides all necessary mathematical definitions, parameters, and test cases to construct a working counterexample demonstrating the phenomenon of obfuscated gradients.\n\nThe solution proceeds by first deriving the necessary mathematical expressions for the model gradients, then specifying the algorithms for the adversarial attacks, and finally applying this framework to the provided test cases.\n\nThe core of the problem lies in the computation of the gradient of the loss function, $\\nabla_x L(x,y)$, which is central to the Fast Gradient Sign Method (FGSM) attack. The loss is the Bernoulli cross-entropy:\n$$L(x,y) = -\\left[y \\log p(x) + (1-y)\\log(1-p(x))\\right]$$\nwhere $y \\in \\{0,1\\}$ is the true label and $p(x)$ is the model's predicted probability for the positive class ($y=1$). The gradient $\\nabla_x L(x,y)$ can be found using the chain rule. Let $u(x)$ be the pre-activation score, such that $p(x) = f(u(x))$ for some activation function $f$.\n$$\\nabla_x L(x,y) = \\frac{\\partial L}{\\partial p} \\frac{dp}{du} \\nabla_x u(x)$$\nA known simplification gives $\\frac{\\partial L}{\\partial p} = \\frac{p(x)-y}{p(x)(1-p(x))}$. The remaining terms, $\\frac{dp}{du}$ and $\\nabla_x u(x)$, depend on the specific model.\n\nIn this problem, the pre-activation score is $u(x) = s \\cdot (w^\\top x + b)$. Its gradient with respect to the input $x$ is:\n$$\\nabla_x u(x) = s \\cdot w$$\nNow we analyze the two models provided.\n\n1.  **Smooth Model (Logistic Activation)**\n    The probability is given by $p_{\\text{soft}}(x) = \\sigma(u(x))$, where $u(x) = s_{\\text{soft}} \\cdot (w^\\top x + b)$ and $\\sigma(u) = 1/(1+e^{-u})$ is the logistic sigmoid function. The derivative of the sigmoid is a key property: $\\frac{d\\sigma}{du} = \\sigma(u)(1-\\sigma(u))$.\n    Substituting this into the chain rule for the gradient of the loss:\n    $$\\frac{dp_{\\text{soft}}}{du} = \\frac{d\\sigma(u(x))}{du} = p_{\\text{soft}}(x)(1-p_{\\text{soft}}(x))$$\n    Assembling the full gradient:\n    $$\\nabla_x L(x,y) = \\frac{p_{\\text{soft}}(x)-y}{p_{\\text{soft}}(x)(1-p_{\\text{soft}}(x))} \\cdot \\left[p_{\\text{soft}}(x)(1-p_{\\text{soft}}(x))\\right] \\cdot (s_{\\text{soft}} \\cdot w)$$\n    This simplifies elegantly to:\n    $$\\nabla_x L(x,y) = (p_{\\text{soft}}(x) - y) \\cdot s_{\\text{soft}} \\cdot w$$\n    This gradient is generally non-zero, especially near the decision boundary where $p_{\\text{soft}}(x)$ is not close to $y \\in \\{0, 1\\}$.\n\n2.  **Saturated Model (Hard-Saturating Activation)**\n    The probability is $p_{\\text{hard}}(x) = \\frac{1}{2}\\left(\\operatorname{clip}(u(x), -1, 1) + 1\\right)$, with $u(x) = s_{\\text{hard}} \\cdot (w^\\top x + b)$. The clip function is defined as:\n    $$\\operatorname{clip}(u, -1, 1) = \\begin{cases} 1 & \\text{if } u > 1 \\\\ u & \\text{if } -1 \\le u \\le 1 \\\\ -1 & \\text{if } u < -1 \\end{cases}$$\n    Its derivative (where it exists) is a step function:\n    $$\\frac{d}{du}\\operatorname{clip}(u, -1, 1) = \\begin{cases} 0 & \\text{if } |u| > 1 \\\\ 1 & \\text{if } |u| < 1 \\end{cases}$$\n    The derivative of $p_{\\text{hard}}$ with respect to $u$ is therefore:\n    $$\\frac{dp_{\\text{hard}}}{du} = \\frac{1}{2} \\cdot \\frac{d}{du}\\operatorname{clip}(u, -1, 1) = \\begin{cases} 0 & \\text{if } |u(x)| > 1 \\\\ 1/2 & \\text{if } |u(x)| < 1 \\end{cases}$$\n    The full gradient $\\nabla_x L(x,y)$ is:\n    $$\\nabla_x L(x,y) = \\frac{p_{\\text{hard}}(x)-y}{p_{\\text{hard}}(x)(1-p_{\\text{hard}}(x))} \\cdot \\frac{dp_{\\text{hard}}}{du} \\cdot (s_{\\text{hard}} \\cdot w)$$\n    The crucial observation is in the saturated region, where $|u(x)| = |s_{\\text{hard}} \\cdot (w^\\top x + b)| > 1$. In this region, $\\frac{dp_{\\text{hard}}}{du} = 0$, which causes the entire gradient to become zero:\n    $$\\nabla_x L(x,y) = 0 \\quad \\text{if } |s_{\\text{hard}} \\cdot (w^\\top x + b)| > 1$$\n    This is the mathematical source of the obfuscated gradient. The loss gradient vanishes not because the model is robust, but due to the choice of a non-smooth, saturating activation function.\n\nThe adversarial attacks are defined as follows:\n- **FGSM Attack**: A white-box attack that perturbs the input $x$ in the direction that maximally increases the loss, subject to an $\\ell_\\infty$ constraint $\\epsilon$. The adversarial example is $x_{\\text{adv}} = x + \\epsilon \\cdot \\operatorname{sign}(\\nabla_x L(x,y))$. The attack succeeds if the predicted label flips, i.e., $\\hat{y}(x_{\\text{adv}}) \\neq \\hat{y}(x)$, where $\\hat{y}(x) = \\mathbf{1}\\{p(x) \\ge 1/2\\}$. If $\\nabla_x L(x,y)=0$, then $x_{\\text{adv}}=x$, and the attack fails.\n\n- **Black-Box Attack**: A simple query-based attack that does not use gradient information. It tests a finite set of perturbations $v$ where $\\|v\\|_\\infty = \\epsilon$. The problem specifies using directions along coordinate axes and diagonals. For $\\mathbb{R}^2$, this corresponds to the eight vectors $v \\in \\{ [c_1, c_2] \\mid c_1, c_2 \\in \\{-\\epsilon, 0, \\epsilon\\}, \\text{ not both zero} \\}$. The attack succeeds if $\\hat{y}(x+v) \\neq \\hat{y}(x)$ for any $v$ in the set.\n\n**Diagnosing Obfuscated Gradients**: Obfuscated gradients are detected when the gradient-based attack (FGSM) fails, but a simple gradient-free attack (black-box search) succeeds. The condition is formally:\n$$[\\text{FGSM fails}] \\land [\\text{black-box succeeds}]$$\n\nFor example, in Test Case 2 (Saturated model, $x = [0.05, 0.05]$), the pre-activation is $u(x) = s_{\\text{hard}}(w^\\top x+b) = 50(0.05+0.05) = 5$. Since $|u(x)|=5 > 1$, the model is in the saturated region and $\\nabla_x L(x,y) = 0$. FGSM thus fails. However, the decision boundary is $x_1+x_2 = 0$. A black-box perturbation $v = [-\\epsilon, 0] = [-0.2, 0]$ yields $x' = [0.05, 0.05] + [-0.2, 0] = [-0.15, 0.05]$. For this new point, $w^\\top x' + b = -0.15+0.05 = -0.1 < 0$, causing a label flip. Thus, the black-box attack succeeds, and the condition for obfuscated gradients is met. Conversely, for a point far from the boundary (Test Case 3), even the black-box attack fails, indicating true robustness for that perturbation budget.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates models for obfuscated gradients in adversarial ML.\n    \"\"\"\n    # Fixed parameters\n    W = np.array([1.0, 1.0])\n    B = 0.0\n    S_SOFT = 1.0\n    S_HARD = 50.0\n    EPSILON = 0.2\n\n    # --- Model and Prediction Functions ---\n\n    def p_soft(x, w, b, s_soft):\n        \"\"\"Computes probability for the smooth model.\"\"\"\n        u = s_soft * (w @ x + b)\n        return 1.0 / (1.0 + np.exp(-u))\n\n    def p_hard(x, w, b, s_hard):\n        \"\"\"Computes probability for the saturated model.\"\"\"\n        u = s_hard * (w @ x + b)\n        clipped_u = np.clip(u, -1.0, 1.0)\n        return 0.5 * (clipped_u + 1.0)\n\n    def predict(p_val):\n        \"\"\"Computes the binary prediction from a probability.\"\"\"\n        return 1 if p_val >= 0.5 else 0\n\n    # --- Gradient Calculation Functions ---\n\n    def nabla_L_soft(x, y, w, b, s_soft):\n        \"\"\"Computes the loss gradient for the smooth model.\"\"\"\n        p_val = p_soft(x, w, b, s_soft)\n        return (p_val - y) * s_soft * w\n\n    def nabla_L_hard(x, y, w, b, s_hard):\n        \"\"\"Computes the loss gradient for the saturated model.\"\"\"\n        u = s_hard * (w @ x + b)\n        if np.abs(u) >= 1.0:\n            return np.array([0.0, 0.0])\n        \n        # This part is only for the linear region, but we can compute it\n        # more robustly to avoid division by zero if p is 0 or 1.\n        # Since the gradient is non-zero only when -1 < u < 1,\n        # p is strictly between 0 and 1, so no division by zero occurs.\n        p_val = p_hard(x, w, b, s_hard)\n        # Factor dp/du = 1/2, Factor du/dz=s_hard, Factor p(1-p)\n        # Simplified chain rule for the linear region:\n        # dL/dp * dp/dz * dz/dx = ((p-y)/(p(1-p))) * (s_hard/2) * w\n        # This can be numerically unstable if p is near 0 or 1, but for this problem\n        # the check for the saturated region |u|>=1 handles it.\n        numerator = p_val - y\n        denominator = p_val * (1.0 - p_val)\n        grad = (numerator / denominator) * (s_hard / 2.0) * w\n        return grad\n\n    # --- Adversarial Attack Functions ---\n\n    def fgsm_attack_succeeds(x, y, model_type):\n        \"\"\"\n        Performs an FGSM attack and returns True if the label flips.\n        \"\"\"\n        x = np.array(x)\n        \n        if model_type == 'soft':\n            p_func = p_soft\n            nabla_func = nabla_L_soft\n            s = S_SOFT\n        else: # model_type == 'hard'\n            p_func = p_hard\n            nabla_func = nabla_L_hard\n            s = S_HARD\n\n        y_pred_orig = predict(p_func(x, W, B, s))\n        \n        grad = nabla_func(x, y, W, B, s)\n        \n        # FGSM perturbation\n        x_adv = x + EPSILON * np.sign(grad)\n        \n        y_pred_adv = predict(p_func(x_adv, W, B, s))\n        \n        return y_pred_adv != y_pred_orig\n\n    def black_box_attack_succeeds(x, model_type):\n        \"\"\"\n        Performs a black-box query attack and returns True if any query flips the label.\n        \"\"\"\n        x = np.array(x)\n        \n        if model_type == 'soft':\n            p_func = p_soft\n            s = S_SOFT\n        else: # model_type == 'hard'\n            p_func = p_hard\n            s = S_HARD\n            \n        y_pred_orig = predict(p_func(x, W, B, s))\n        \n        # 8 query directions for R^2 (axes and diagonals)\n        perturbations = [\n            np.array([EPSILON, 0]), np.array([-EPSILON, 0]),\n            np.array([0, EPSILON]), np.array([0, -EPSILON]),\n            np.array([EPSILON, EPSILON]), np.array([EPSILON, -EPSILON]),\n            np.array([-EPSILON, EPSILON]), np.array([-EPSILON, -EPSILON])\n        ]\n        \n        for v in perturbations:\n            x_query = x + v\n            y_pred_query = predict(p_func(x_query, W, B, s))\n            if y_pred_query != y_pred_orig:\n                return True\n                \n        return False\n\n    # --- Test Suite ---\n    test_cases = [\n        {'model': 'soft', 'x': [0.05, 0.05], 'y': 1},\n        {'model': 'hard', 'x': [0.05, 0.05], 'y': 1},\n        {'model': 'hard', 'x': [0.5, 0.5], 'y': 1},\n        {'model': 'soft', 'x': [-0.05, -0.05], 'y': 0},\n    ]\n\n    results = []\n    for case in test_cases:\n        fgsm_failed = not fgsm_attack_succeeds(case['x'], case['y'], case['model'])\n        black_box_succeeded = black_box_attack_succeeds(case['x'], case['model'])\n        \n        # Diagnose obfuscated gradients as per the problem definition\n        obfuscated_gradient_detected = fgsm_failed and black_box_succeeded\n        results.append(obfuscated_gradient_detected)\n        \n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097022"}]}