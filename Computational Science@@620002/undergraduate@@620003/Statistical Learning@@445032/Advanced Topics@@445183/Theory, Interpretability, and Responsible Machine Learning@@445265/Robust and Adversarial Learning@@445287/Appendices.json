{"hands_on_practices": [{"introduction": "The first step in building robust systems is to understand how they can fail. This practice explores the concept of the breakdown point, which defines the theoretical limit of an estimator's resilience to contaminated data. By calculating the breakdown point for the mean, trimmed mean, and median, you will gain a tangible understanding of why some statistical methods are inherently more robust than others, providing a crucial foundation for designing resilient learning algorithms [@problem_id:3171418].", "problem": "Consider the epsilon-contamination model for robust statistical learning in one dimension, where a training sample of size $n$ consists of $(1-\\epsilon)n$ clean points drawn from a base distribution and $\\epsilon n$ adversarial outliers. In this problem, treat the base distribution as a degenerate distribution at $0$ so that clean points are $0$, and let the adversary place all outliers at a large positive value $M$. This worst-case contamination makes the analysis purely combinatorial and avoids randomness.\n\nDefine three estimators of location:\n1. The sample mean $\\bar{x}$.\n2. The symmetric $\\tau$-trimmed mean for $0 \\le \\tau < \\frac{1}{2}$, which trims the $k = \\lfloor \\tau n \\rfloor$ smallest and $k$ largest values and averages the remaining $n - 2k$ observations. For a sorted sample $x_{(1)} \\le \\cdots \\le x_{(n)}$, the trimmed mean is\n$$\n\\bar{x}_{\\mathrm{trim}} = \\frac{1}{n - 2k} \\sum_{i=k+1}^{n-k} x_{(i)}.\n$$\n3. The Tukey median defined by Tukey depth (in one dimension this equals the sample median). For $n$ odd, this is $x_{((n+1)/2)}$; for $n$ even, this is $\\frac{1}{2}\\left(x_{(n/2)} + x_{(n/2+1)}\\right)$.\n\nUse the foundational definition of breakdown point in robust statistics: for an estimator viewed as a functional $T$, the breakdown point at a distribution $P$ is the smallest contamination level $\\epsilon$ at which the estimator can be driven without bound by adversarial contamination, namely\n$$\n\\varepsilon^\\star(T, P) = \\inf \\left\\{\\epsilon \\in [0,1]: \\sup_Q \\left\\| T\\left( (1-\\epsilon)P + \\epsilon Q \\right) \\right\\| = \\infty \\right\\},\n$$\nwhere $Q$ ranges over all possible contaminating distributions. In the finite-sample setting with the explicit construction above, this reduces to asking for the minimal fraction of outliers $\\epsilon$ (equivalently an integer count $m = \\epsilon n$) such that as $M \\to \\infty$, the estimator value grows without bound.\n\nYour tasks:\n- Starting from the above robust breakdown definition and the explicit contamination construction with clean points at $0$ and adversarial outliers at $M$, derive the minimal contamination fraction $\\epsilon^\\star$ required to make each estimator unbounded as $M \\to \\infty$ in the finite-sample setting. Express the answer as a function of $n$ and $\\tau$ where applicable, justifying all rounding needed due to integer counts of outliers.\n- Also determine the asymptotic breakdown points for the three estimators, interpreting the epsilon-contamination model in the limit of large samples, and provide the values that do not depend on $n$.\n- Implement a deterministic program that, for each test case $(n,\\tau)$, returns two triplets:\n  - The asymptotic breakdown points $[\\varepsilon^\\star(\\text{mean}), \\varepsilon^\\star(\\text{trimmed mean}), \\varepsilon^\\star(\\text{Tukey median})]$.\n  - The finite-sample contamination thresholds in the explicit construction $[\\epsilon^\\star_{\\text{mean}}(n), \\epsilon^\\star_{\\text{trim}}(n,\\tau), \\epsilon^\\star_{\\text{Tukey}}(n)]$.\n\nTest suite:\n- Use the following parameter sets $(n,\\tau)$:\n  1. $(100, 0.1)$ as a general case.\n  2. $(101, 0.1)$ to test odd $n$ effects on the median.\n  3. $(100, 0.0)$ to test the boundary where trimming vanishes.\n  4. $(20, 0.2)$ to test discrete rounding effects with moderate trimming.\n  5. $(2, 0.0)$ to test the smallest even sample size.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. Each test case contributes a pair of lists of three floats: the first list is the asymptotic breakdown points in the order described above, and the second list is the finite-sample thresholds in the same order. The overall output is a list of these pairs for all test cases. For example, the format should be like\n$$\n\\left[\\left[[a_1,a_2,a_3],[b_1,b_2,b_3]\\right],\\left[[\\cdots],[\\cdots]\\right],\\ldots\\right]\n$$\nwhere $a_i$ and $b_i$ are decimal numbers.", "solution": "The problem is valid as it is scientifically grounded in the principles of robust statistics, is well-posed with clear definitions, and is expressed in objective, formal language. It is a standard theoretical problem to analyze the breakdown point of location estimators. We will proceed to derive the required quantities.\n\nThe problem defines a specific contamination scenario: for a sample of size $n$, we have $n-m$ \"clean\" data points at value $0$ and $m$ \"adversarial\" outlier points at a large positive value $M$. The contamination fraction is $\\epsilon = m/n$. We seek the minimal integer number of outliers, $m$, that causes an estimator's value to become unbounded as $M \\to \\infty$. The finite-sample breakdown point is this minimal fraction, $\\epsilon^\\star = m/n$. The asymptotic breakdown point is the limit of this fraction as $n \\to \\infty$.\n\nThe sorted sample, denoted $x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(n)}$, will consist of $n-m$ zeros followed by $m$ values of $M$. Specifically, $x_{(i)} = 0$ for $i \\le n-m$, and $x_{(i)} = M$ for $i > n-m$.\n\n**1. Sample Mean ($\\bar{x}$)**\n\nThe sample mean is defined as $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$.\nFor the given contaminated sample, the sum consists of $n-m$ terms equal to $0$ and $m$ terms equal to $M$.\nThe value of the sample mean is:\n$$\n\\bar{x} = \\frac{1}{n} \\left( (n-m) \\cdot 0 + m \\cdot M \\right) = \\frac{m M}{n}\n$$\nFor the estimator to be driven to infinity as $M \\to \\infty$, the coefficient of $M$ must be non-zero.\n$$\n\\lim_{M \\to \\infty} \\frac{m M}{n} = \\infty \\quad \\iff \\quad \\frac{m}{n} > 0 \\quad \\iff \\quad m > 0\n$$\nThe smallest integer $m$ satisfying $m > 0$ is $m=1$.\n\n- **Finite-Sample Breakdown Point**: The minimal number of outliers is $m=1$. The corresponding fraction is:\n$$ \\epsilon^\\star_{\\text{mean}}(n) = \\frac{1}{n} $$\n- **Asymptotic Breakdown Point**: Taking the limit as the sample size $n$ goes to infinity:\n$$ \\varepsilon^\\star(\\text{mean}) = \\lim_{n \\to \\infty} \\frac{1}{n} = 0 $$\n\n**2. Symmetric $\\tau$-Trimmed Mean ($\\bar{x}_{\\mathrm{trim}}$)**\n\nThe $\\tau$-trimmed mean is calculated by removing the $k = \\lfloor \\tau n \\rfloor$ smallest and $k$ largest observations and averaging the remaining $n-2k$ points. Its formula is:\n$$\n\\bar{x}_{\\mathrm{trim}} = \\frac{1}{n - 2k} \\sum_{i=k+1}^{n-k} x_{(i)}\n$$\nThe estimator's value is determined by the points $x_{(k+1)}, \\dots, x_{(n-k)}$. In our setup, these points are either $0$ or $M$. The estimator will be bounded (equal to $0$) if all points in the sum are $0$. It will become unbounded if at least one point in the sum is $M$.\n\nThe outliers at $M$ occupy the largest $m$ positions in the sorted sample: $x_{(n-m+1)}, \\dots, x_{(n)}$.\nThe largest $k$ values are trimmed. These are $x_{(n-k+1)}, \\dots, x_{(n)}$.\nIf the number of outliers $m$ is less than or equal to the number of trimmed high-end points $k$ (i.e., $m \\le k$), then all outliers will be part of the trimmed set. The largest index of a remaining point is $n-k$. The smallest index of an outlier is $n-m+1$. If $n-m+1 > n-k$, or $k+1 > m$, all outliers are trimmed.\nBreakdown occurs when at least one outlier is *not* trimmed. This happens when the number of outliers $m$ is large enough to \"spill over\" into the part of the sample that is averaged. This requires the number of outliers to be greater than the number of points trimmed from the top, $k$.\nThe condition for breakdown is therefore $m > k$.\nThe minimal integer $m$ that satisfies this is $m = k+1$. Substituting $k = \\lfloor \\tau n \\rfloor$, the minimal number of outliers is $m = \\lfloor \\tau n \\rfloor + 1$.\n\n- **Finite-Sample Breakdown Point**: The minimal contamination fraction is:\n$$ \\epsilon^\\star_{\\text{trim}}(n, \\tau) = \\frac{\\lfloor \\tau n \\rfloor + 1}{n} $$\n- **Asymptotic Breakdown Point**: We take the limit as $n \\to \\infty$. Using the property that $\\lim_{n\\to\\infty} \\frac{\\lfloor \\tau n \\rfloor}{n} = \\tau$:\n$$ \\varepsilon^\\star(\\text{trimmed mean}) = \\lim_{n \\to \\infty} \\frac{\\lfloor \\tau n \\rfloor + 1}{n} = \\lim_{n \\to \\infty} \\left(\\frac{\\lfloor \\tau n \\rfloor}{n} + \\frac{1}{n}\\right) = \\tau + 0 = \\tau $$\n\n**3. Tukey Median (Sample Median)**\n\nThe sample median's definition depends on the parity of $n$.\nBreakdown occurs if the median value is $M$ or includes a term proportional to $M$, which will happen if one of the central data points is an outlier with value $M$.\n\n- **Case 1: $n$ is odd**. Let $n = 2j+1$. The median is the single central value $x_{((n+1)/2)} = x_{(j+1)}$. Breakdown occurs if this point is an outlier. The outliers are $x_{(n-m+1)}, \\dots, x_{(n)}$. So we need the index of the median to be in the outlier range:\n$$ \\frac{n+1}{2} > n-m \\implies m > n - \\frac{n+1}{2} = \\frac{n-1}{2} $$\nSince $n$ is odd, $(n-1)/2 = j$ is an integer. The smallest integer $m$ satisfying $m > j$ is $m = j+1 = (n+1)/2$.\n\n- **Case 2: $n$ is even**. Let $n = 2j$. The median is the average of the two central values, $\\frac{1}{2}(x_{(n/2)} + x_{(n/2+1)}) = \\frac{1}{2}(x_{(j)} + x_{(j+1)})$. The estimator becomes unbounded if at least one of these two points is $M$. This is guaranteed if the higher-indexed point, $x_{(j+1)}$, is an outlier. The condition is:\n$$ \\frac{n}{2}+1 > n-m \\implies m > n - \\left(\\frac{n}{2}+1\\right) = \\frac{n}{2}-1 $$\nSince $n$ is even, $n/2-1 = j-1$ is an integer. The smallest integer $m$ satisfying $m > j-1$ is $m = j = n/2$.\n\nWe can unify these two cases. The minimal number of outliers is $m=(n+1)/2$ for odd $n$ and $m=n/2$ for even $n$. This corresponds to $m = \\lceil n/2 \\rceil$.\n\n- **Finite-Sample Breakdown Point**: The minimal contamination fraction is:\n$$ \\epsilon^\\star_{\\text{Tukey}}(n) = \\frac{\\lceil n/2 \\rceil}{n} $$\n- **Asymptotic Breakdown Point**: In the limit of large $n$, $\\lceil n/2 \\rceil \\approx n/2$.\n$$ \\varepsilon^\\star(\\text{Tukey median}) = \\lim_{n \\to \\infty} \\frac{\\lceil n/2 \\rceil}{n} = \\frac{1}{2} = 0.5 $$\n\n**Summary of Results**\n\n| Estimator | Asymptotic Breakdown Point ($\\varepsilon^\\star$) | Finite-Sample Breakdown Point ($\\epsilon^\\star$) |\n| :--- | :---: | :---: |\n| Sample Mean | $0$ | $\\frac{1}{n}$ |\n| Trimmed Mean | $\\tau$ | $\\frac{\\lfloor \\tau n \\rfloor + 1}{n}$ |\n| Tukey Median | $0.5$ | $\\frac{\\lceil n/2 \\rceil}{n}$ |\n\nThese formulas are implemented to compute the results for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the asymptotic and finite-sample breakdown points for three estimators\n    under an epsilon-contamination model for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100, 0.1),  # General case\n        (101, 0.1),  # Odd n effects on the median\n        (100, 0.0),  # Trimming vanishes, should equal mean\n        (20, 0.2),   # Discrete rounding effects\n        (2, 0.0),    # Smallest even sample size\n    ]\n\n    all_results = []\n    for n, tau in test_cases:\n        # 1. Asymptotic Breakdown Points\n        # These are derived theoretical limits as n -> infinity.\n        # Mean: 0\n        # Trimmed Mean: tau\n        # Median: 0.5\n        asymptotic_bps = [0.0, tau, 0.5]\n\n        # 2. Finite-Sample Breakdown Points\n        # These depend on the sample size n and integer arithmetic.\n        \n        # For the sample mean, 1 outlier is sufficient to cause breakdown.\n        # Minimal fraction of outliers = 1/n.\n        finite_mean_bp = 1 / n\n\n        # For the trimmed mean, breakdown occurs if outliers exceed the trim count k.\n        # k = floor(tau * n). Minimal outliers m = k + 1.\n        # Minimal fraction = (k + 1) / n.\n        k = np.floor(tau * n)\n        finite_trimmed_mean_bp = (k + 1) / n\n\n        # For the median, breakdown occurs if more than half the points are outliers.\n        # Minimal outliers m = ceil(n / 2).\n        # Minimal fraction = ceil(n/2) / n.\n        finite_median_bp = np.ceil(n / 2) / n\n\n        finite_bps = [finite_mean_bp, finite_trimmed_mean_bp, finite_median_bp]\n        \n        all_results.append([asymptotic_bps, finite_bps])\n\n    # Format the final output string to match the problem specification\n    # precisely (list of lists, no spaces).\n    case_strings = []\n    for async_res, finite_res in all_results:\n        async_str = f\"[{','.join(map(str, async_res))}]\"\n        finite_str = f\"[{','.join(map(str, finite_res))}]\"\n        case_strings.append(f\"[{async_str},{finite_str}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3171418"}, {"introduction": "While the breakdown point tells us about catastrophic failure, the influence function provides a more fine-grained analysis of an estimator's sensitivity. This practice challenges you to compute and compare the influence functions for Ordinary Least Squares (OLS) and robust Huber regression in the presence of heavy-tailed noise. Through this hands-on comparison, you will see how robust methods systematically down-weight the impact of outliers, a key mechanism for achieving stability in real-world data [@problem_id:3171489].", "problem": "Consider a one-dimensional linear regression model with intercept defined by the data-generating process $y = \\beta_0 + \\beta_1 x + \\varepsilon$, where $x$ is a scalar predictor and $\\varepsilon$ is independent noise. Adopt Empirical Risk Minimization (ERM) as the fundamental base: parameters are estimated by minimizing the expected loss under the data distribution. For Ordinary Least Squares (OLS), the loss is the squared loss, and for Huber regression the loss is the Huber loss with threshold parameter. Robustness is studied via the Influence Function (IF), which measures the infinitesimal change in the parameter estimate when the data distribution is contaminated at a single point.\n\nYou are to construct and analyze a statistically sound scenario with heavy-tailed noise as follows:\n\n1. Generate a synthetic dataset of size $n = 5000$ using the model $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ with parameters $\\beta_0 = 1.0$ and $\\beta_1 = 1.5$. Draw predictors $x_i$ independently from a standard normal distribution with mean $0$ and variance $1$. Draw noise $\\varepsilon_i$ independently from a Student's $t$ distribution with degrees of freedom $\\nu = 3$ and scale $s = 1.0$. Use the fixed random seed $42$ for reproducibility.\n\n2. In ERM for linear regression with intercept and one predictor, write the design vector as $\\tilde{x} = \\begin{bmatrix}1 \\\\ x\\end{bmatrix}$. Let the loss be $\\rho(r)$ as a function of the residual $r = y - \\tilde{x}^\\top \\theta$, where $\\theta = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1\\end{bmatrix}$. Define the score function $\\psi(r) = \\frac{d}{dr}\\rho(r)$ and the estimating equation $\\mathbb{E}[\\psi(r)\\tilde{x}] = 0$. The Influence Function (IF) at a contamination point $z_0 = (x_0, y_0)$ is given by the Gateaux derivative of the parameter functional with respect to an $\\varepsilon$-contamination: consider the contaminated distribution $(1 - \\varepsilon)P + \\varepsilon \\Delta_{z_0}$ and differentiate the solution of the estimating equation at $\\varepsilon = 0$. Use the sensitivity matrix $A = \\mathbb{E}\\left[\\frac{\\partial}{\\partial \\theta}\\{\\psi(r)\\tilde{x}\\}\\right]$ to express the IF. Compute expectations by sample averages over the generated data.\n\n3. For OLS, use the squared loss with $\\rho_{\\text{OLS}}(r) = \\frac{1}{2} r^2$ and $\\psi_{\\text{OLS}}(r) = r$. For Huber regression, use the Huber loss with threshold parameter $\\delta > 0$, defined by $\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2}r^2 & \\text{if } |r| \\le \\delta \\\\ \\delta|r| - \\frac{1}{2}\\delta^2 & \\text{if } |r| > \\delta \\end{cases}$, so that $\\psi_{\\delta}(r) = \\begin{cases} r & \\text{if } |r| \\le \\delta \\\\ \\delta \\,\\mathrm{sign}(r) & \\text{if } |r| > \\delta \\end{cases}$. Set the threshold as $\\delta = k \\cdot \\hat{\\sigma}$, where $k = 1.345$ is a constant and $\\hat{\\sigma}$ is a robust scale estimate of $\\varepsilon$ computed as the median absolute deviation (MAD) multiplied by $1.4826$. Use this $\\delta$ in computing $\\psi_{\\delta}$ and the sensitivity matrix for Huber regression.\n\n4. Approximate the sensitivity matrices by sample averages. For OLS, use $A_{\\text{OLS}} \\approx -\\frac{1}{n}\\sum_{i=1}^{n} \\tilde{x}_i \\tilde{x}_i^\\top$ because $\\psi_{\\text{OLS}}'(r) = 1$. For Huber, use $A_{\\text{Huber}} \\approx -\\frac{1}{n}\\sum_{i=1}^{n} I(|\\varepsilon_i| \\le \\delta)\\, \\tilde{x}_i \\tilde{x}_i^\\top$, where $I(\\cdot)$ is the indicator function and $\\psi_{\\delta}'(r) = 1$ if $|r| \\le \\delta$ and $0$ otherwise. Use the true parameter $\\theta^\\star = \\begin{bmatrix}1.0 \\\\ 1.5\\end{bmatrix}$ when forming residuals and sensitivity matrices, so that $r_i = \\varepsilon_i$.\n\n5. For a contamination point $z_0 = (x_0, y_0)$ with residual $r_0 = y_0 - \\tilde{x}_0^\\top \\theta^\\star$, the IF vector is $\\mathsf{IF}(z_0) = -A^{-1}\\{\\psi(r_0)\\tilde{x}_0\\}$. Compute this for OLS and for Huber regression. To compare robustness, report for each test case the scalar ratio $R = \\|\\mathsf{IF}_{\\text{Huber}}(z_0)\\|_2 \\big/ \\|\\mathsf{IF}_{\\text{OLS}}(z_0)\\|_2$, where $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nUse the following test suite of contamination points, expressed in terms of $(x_0, r_0)$, and define $y_0 = \\beta_0 + \\beta_1 x_0 + r_0$ for each:\n- Case $1$: $(x_0, r_0) = (0.0, 0.5)$, so $y_0 = 1.0 + 1.5 \\cdot 0.0 + 0.5$.\n- Case $2$: $(x_0, r_0) = (0.0, 10.0)$, so $y_0 = 1.0 + 1.5 \\cdot 0.0 + 10.0$.\n- Case $3$: $(x_0, r_0) = (5.0, 0.5)$, so $y_0 = 1.0 + 1.5 \\cdot 5.0 + 0.5$.\n- Case $4$: $(x_0, r_0) = (5.0, 10.0)$, so $y_0 = 1.0 + 1.5 \\cdot 5.0 + 10.0$.\n\nYour program should produce a single line of output containing the four ratios $R$ for the cases above as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$). The outputs must be real numbers (floats). No angles or physical units are involved in this problem.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded exercise in robust statistics, a subfield of statistical learning. All provided data, definitions, and equations are internally consistent, standard in the field, and sufficient to derive a unique numerical solution. The problem asks for a quantitative comparison of the robustness of Ordinary Least Squares (OLS) and Huber regression using the Influence Function (IF), which is a standard tool for this purpose.\n\nThe objective is to compute the ratio of the Euclidean norms of the Influence Functions for Huber regression versus OLS regression, $R = \\|\\mathsf{IF}_{\\text{Huber}}(z_0)\\|_2 \\big/ \\|\\mathsf{IF}_{\\text{OLS}}(z_0)\\|_2$, for four different contamination points $z_0$. A smaller ratio indicates greater robustness against the specific contamination. The procedure is as follows:\n\nFirst, we generate a synthetic dataset according to the problem specification.\n- Sample size: $n = 5000$.\n- Predictors $x_i$ for $i=1, \\dots, n$ are drawn independently from a standard normal distribution, $x_i \\sim \\mathcal{N}(0, 1)$.\n- Noise terms $\\varepsilon_i$ are drawn independently from a Student's $t$ distribution with $\\nu = 3$ degrees of freedom and scale $s=1.0$.\n- The data-generating process is $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ with true parameters $\\beta_0 = 1.0$ and $\\beta_1 = 1.5$.\nA fixed random seed of $42$ is used for reproducibility. The design vector for each observation is $\\tilde{x}_i = [1, x_i]^\\top$.\n\nThe analysis uses the true parameter vector $\\theta^\\star = [\\beta_0, \\beta_1]^\\top = [1.0, 1.5]^\\top$ to compute residuals, which simplifies the residuals to be the noise terms themselves, $r_i = y_i - \\tilde{x}_i^\\top \\theta^\\star = \\varepsilon_i$.\n\nFor Huber regression, a threshold parameter $\\delta$ is required for the loss function $\\rho_{\\delta}(r)$. This is set to $\\delta = k \\cdot \\hat{\\sigma}$, where $k=1.345$ and $\\hat{\\sigma}$ is a robust estimate of the standard deviation of the noise. We compute $\\hat{\\sigma}$ as $1.4826 \\times \\text{MAD}(\\varepsilon)$, where $\\text{MAD}(\\varepsilon)$ is the Median Absolute Deviation of the noise terms, given by $\\text{median}_i(|\\varepsilon_i - \\text{median}_j(\\varepsilon_j)|)$.\n\nThe Influence Function for a contamination point $z_0 = (x_0, y_0)$ is given by $\\mathsf{IF}(z_0) = -A^{-1}\\{\\psi(r_0)\\tilde{x}_0\\}$, where $r_0 = y_0 - \\tilde{x}_0^\\top\\theta^\\star$ is the residual at the contamination point, $\\psi(\\cdot)$ is the score function (the derivative of the loss function $\\rho(\\cdot)$), and $A$ is the sensitivity matrix.\n\nThe sensitivity matrix $A$ is defined as $A = \\mathbb{E}\\left[\\frac{\\partial}{\\partial \\theta}\\{\\psi(r)\\tilde{x}\\}\\right] = \\mathbb{E}[-\\psi'(r)\\tilde{x}\\tilde{x}^\\top]$. We approximate this expectation using a sample average over the generated dataset.\n- For OLS, the loss is $\\rho_{\\text{OLS}}(r) = \\frac{1}{2}r^2$, so the score function is $\\psi_{\\text{OLS}}(r) = r$ and its derivative is $\\psi'_{\\text{OLS}}(r) = 1$. The sensitivity matrix is $A_{\\text{OLS}} \\approx -\\frac{1}{n} \\sum_{i=1}^n \\tilde{x}_i \\tilde{x}_i^\\top$.\n- For Huber regression, the score function is $\\psi_{\\delta}(r) = \\text{sign}(r) \\min(|r|, \\delta)$. Its derivative is $\\psi'_{\\delta}(r) = I(|r| \\le \\delta)$, where $I(\\cdot)$ is the indicator function. The sensitivity matrix is $A_{\\text{Huber}} \\approx -\\frac{1}{n} \\sum_{i=1}^n I(|\\varepsilon_i| \\le \\delta) \\tilde{x}_i \\tilde{x}_i^\\top$.\n\nFor each of the four test cases defined by a contamination point $(x_0, r_0)$:\n1.  The design vector is $\\tilde{x}_0 = [1, x_0]^\\top$.\n2.  The score function values $\\psi_{\\text{OLS}}(r_0) = r_0$ and $\\psi_{\\delta}(r_0) = \\text{sign}(r_0) \\min(|r_0|, \\delta)$ are computed.\n3.  The vectors $\\{\\psi_{\\text{OLS}}(r_0)\\tilde{x}_0\\}$ and $\\{\\psi_{\\delta}(r_0)\\tilde{x}_0\\}$ are formed.\n4.  The Influence Function vectors $\\mathsf{IF}_{\\text{OLS}}(z_0) = -A_{\\text{OLS}}^{-1}\\{\\psi_{\\text{OLS}}(r_0)\\tilde{x}_0\\}$ and $\\mathsf{IF}_{\\text{Huber}}(z_0) = -A_{\\text{Huber}}^{-1}\\{\\psi_{\\delta}(r_0)\\tilde{x}_0\\}$ are computed by solving the corresponding linear systems.\n5.  The Euclidean norms of these vectors, $\\|\\mathsf{IF}_{\\text{OLS}}(z_0)\\|_2$ and $\\|\\mathsf{IF}_{\\text{Huber}}(z_0)\\|_2$, are calculated.\n6.  The final ratio $R$ is computed and collected.\n\nThis process is repeated for all test cases, and the resulting ratios are formatted into the required output.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Computes the ratio of influence function norms for Huber vs. OLS regression.\n    \"\"\"\n    # 1. Define constants and set up the simulation environment\n    n = 5000\n    beta_0 = 1.0\n    beta_1 = 1.5\n    theta_star = np.array([beta_0, beta_1])\n    nu = 3.0\n    s_scale = 1.0\n    seed = 42\n    k = 1.345\n    mad_const = 1.4826\n\n    # 2. Generate synthetic data\n    rng = np.random.default_rng(seed)\n    x = rng.normal(loc=0.0, scale=1.0, size=n)\n    # Per problem statement, residuals for matrix calculation are the noise terms\n    eps = stats.t.rvs(df=nu, loc=0.0, scale=s_scale, size=n, random_state=rng)\n    \n    # Construct the design matrix with an intercept term\n    X_tilde = np.c_[np.ones(n), x]\n\n    # 3. Calculate the Huber threshold parameter delta\n    # Compute MAD of the noise terms\n    med_eps = np.median(eps)\n    mad = np.median(np.abs(eps - med_eps))\n    \n    # Compute the robust scale estimate sigma_hat\n    sigma_hat = mad_const * mad\n    \n    # Compute the Huber threshold delta\n    delta = k * sigma_hat\n\n    # 4. Compute the sensitivity matrices A_OLS and A_Huber\n    # For OLS\n    A_ols = (-1/n) * (X_tilde.T @ X_tilde)\n    \n    # For Huber\n    # Find indices where residuals are within the delta threshold\n    inlier_indices = np.abs(eps) <= delta\n    # Create the sub-matrix of inliers\n    X_tilde_inliers = X_tilde[inlier_indices]\n    A_huber = (-1/n) * (X_tilde_inliers.T @ X_tilde_inliers)\n\n    # 5. Define test cases and compute ratios\n    test_cases = [\n        (0.0, 0.5),   # Case 1: No leverage, small residual\n        (0.0, 10.0),  # Case 2: No leverage, large residual (vertical outlier)\n        (5.0, 0.5),   # Case 3: High leverage, small residual\n        (5.0, 10.0),  # Case 4: High leverage, large residual\n    ]\n    \n    results = []\n    \n    for x0, r0 in test_cases:\n        x_tilde_0 = np.array([1.0, x0])\n        \n        # --- OLS Influence Function ---\n        # Score function value for OLS\n        psi_ols_r0 = r0\n        # Vector for the IF formula\n        v_ols = psi_ols_r0 * x_tilde_0\n        # Compute IF by solving the linear system A * IF = -v\n        if_ols = np.linalg.solve(A_ols, -v_ols)\n        # Compute the L2 norm\n        norm_if_ols = np.linalg.norm(if_ols)\n        \n        # --- Huber Influence Function ---\n        # Score function value for Huber (clipped at delta)\n        psi_huber_r0 = np.sign(r0) * min(np.abs(r0), delta)\n        # Vector for the IF formula\n        v_huber = psi_huber_r0 * x_tilde_0\n        # Compute IF by solving the linear system\n        if_huber = np.linalg.solve(A_huber, -v_huber)\n        # Compute the L2 norm\n        norm_if_huber = np.linalg.norm(if_huber)\n        \n        # --- Ratio Calculation ---\n        # Handle the case where the denominator might be zero (unlikely here)\n        if norm_if_ols == 0:\n            ratio = np.inf if norm_if_huber > 0 else 0\n        else:\n            ratio = norm_if_huber / norm_if_ols\n            \n        results.append(ratio)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3171489"}, {"introduction": "Moving from analyzing robustness to guaranteeing it, this practice introduces the concept of certified robustness. Instead of just hoping a model is robust, certification provides a mathematical proof that no attack within a specified threat model can fool the classifier. You will implement a certification method for a linear model, using the relationship between dual norms to compute provable robustness radii against both $\\ell_1$ and $\\ell_2$ adversaries, highlighting how these guarantees depend on both the model's parameters and the geometry of the chosen threat model [@problem_id:3171496].", "problem": "You are given a binary classifier in two dimensions defined by the linear scoring function $f(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x} + b$ with fixed parameters $\\mathbf{w} \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}$. Classification is by the sign of $f(\\mathbf{x})$, with label $y \\in \\{-1, +1\\}$ and a signed margin $m(\\mathbf{x}, y) = y \\cdot f(\\mathbf{x})$. Consider adversarial perturbations $\\boldsymbol{\\delta} \\in \\mathbb{R}^{2}$ constrained in an $\\ell_{p}$-norm ball around $\\mathbf{x}$, where the adversary seeks to change the classification sign by choosing $\\boldsymbol{\\delta}$ to make $f(\\mathbf{x} + \\boldsymbol{\\delta}) \\leq 0$. The linkage between dual norms via Hölder’s inequality provides a way to certify robustness by lower bounding the required perturbation size. You must construct a simple two-dimensional dataset, apply the fixed linear classifier, and compare certified robustness under $\\ell_{1}$ and $\\ell_{2}$ perturbation models using the gradient $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ and the dual norms relationship.\n\nBase facts to use:\n- Hölder’s inequality and dual norms: for any $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{d}$, $|\\mathbf{a}^{\\top} \\mathbf{b}| \\leq \\|\\mathbf{a}\\|_{q} \\|\\mathbf{b}\\|_{p}$ with $1/p + 1/q = 1$, and the dual of $p=1$ is $q=\\infty$, while the dual of $p=2$ is $q=2$.\n- For the linear model $f(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x} + b$, the gradient with respect to the input is $\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\mathbf{w}$.\n\nConstruct the dataset:\n- Use the following fixed parameters for the classifier: $\\mathbf{w} = (0.8, -0.6)$ and $b = 0.1$.\n- Consider the following five labeled points in $\\mathbb{R}^{2}$:\n    1. $\\mathbf{x}_{1} = (2.0, -1.0)$ with $y_{1} = +1$.\n    2. $\\mathbf{x}_{2} = (0.5, 0.5)$ with $y_{2} = -1$.\n    3. $\\mathbf{x}_{3} = (-1.0, 2.0)$ with $y_{3} = -1$.\n    4. $\\mathbf{x}_{4} = (0.125, 0.0)$ with $y_{4} = +1$.\n    5. $\\mathbf{x}_{5} = (-0.125, 0.0)$ with $y_{5} = +1$.\n\nTasks:\n- For each test point $(\\mathbf{x}_{i}, y_{i})$, compute the signed margin $m_{i} = y_{i} \\cdot f(\\mathbf{x}_{i})$.\n- Using the gradient $\\mathbf{g} = \\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\mathbf{w}$ and the dual norm linkage provided by Hölder’s inequality, determine the certified robustness radii against $\\ell_{1}$ and $\\ell_{2}$ adversaries. A certified radius is the largest $\\epsilon$ such that every perturbation $\\boldsymbol{\\delta}$ with $\\|\\boldsymbol{\\delta}\\|_{p} \\leq \\epsilon$ cannot change the sign of $f(\\mathbf{x})$. If $m_{i} \\leq 0$, the certified radius is $0$ because the point is already misclassified or on the decision boundary. Express the $\\ell_{1}$ and $\\ell_{2}$ certified radii for each point as nonnegative real numbers.\n- Round each computed radius to six decimal places.\n\nTest suite:\n- The five points listed above cover a general case with large positive margin, a misclassified point (negative margin), another correctly classified point with moderate margin, a near-boundary case with small positive margin, and a boundary case with exactly zero margin. This provides coverage of typical and edge conditions.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[\\text{radius}_{\\ell_{1}}, \\text{radius}_{\\ell_{2}}]$ in the same order as the test cases above. For example, the output should look like:\n$[[r_{1,\\ell_{1}}, r_{1,\\ell_{2}}],[r_{2,\\ell_{1}}, r_{2,\\ell_{2}}],\\dots,[r_{5,\\ell_{1}}, r_{5,\\ell_{2}}]]$\nAll radii must be floats rounded to six decimal places. No other text should be printed.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions, parameters, and data to solve a standard problem in the adversarial robustness of linear models. The task is to compute certified robustness radii for a given linear classifier and a set of data points, which is a well-defined mathematical procedure. The problem is therefore deemed **valid**.\n\nThe solution proceeds by first deriving the general formula for the certified robustness radius of a linear classifier, and then applying this formula to the specific points provided in the dataset.\n\nA linear classifier is defined by a scoring function $f(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x} + b$, where $\\mathbf{x} \\in \\mathbb{R}^{d}$ is the input vector, $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector, and $b \\in \\mathbb{R}$ is the bias. The predicted label is $\\text{sign}(f(\\mathbf{x}))$. The true label is denoted by $y \\in \\{-1, +1\\}$. A point $(\\mathbf{x}, y)$ is correctly classified if $y \\cdot f(\\mathbf{x}) > 0$. The quantity $m(\\mathbf{x}, y) = y \\cdot f(\\mathbf{x})$ is the signed margin.\n\nAdversarial robustness concerns the stability of the classifier's prediction under small perturbations to the input. An adversary seeks to find a perturbation $\\boldsymbol{\\delta}$ with a small norm $\\|\\boldsymbol{\\delta}\\|_{p}$ that causes a misclassification. For a correctly classified point, this means finding a $\\boldsymbol{\\delta}$ such that the sign of the classifier's output flips, i.e., $y \\cdot f(\\mathbf{x} + \\boldsymbol{\\delta}) \\leq 0$.\n\nThe certified robustness radius, $\\epsilon_p$, is the largest value such that for all perturbations $\\boldsymbol{\\delta}$ with $\\|\\boldsymbol{\\delta}\\|_{p} \\leq \\epsilon_p$, the classification remains unchanged, i.e., $y \\cdot f(\\mathbf{x} + \\boldsymbol{\\delta}) > 0$.\n\nFor a linear model, the change in the scoring function under a perturbation $\\boldsymbol{\\delta}$ is given exactly by a first-order approximation:\n$$f(\\mathbf{x} + \\boldsymbol{\\delta}) = \\mathbf{w}^{\\top} (\\mathbf{x} + \\boldsymbol{\\delta}) + b = (\\mathbf{w}^{\\top}\\mathbf{x} + b) + \\mathbf{w}^{\\top}\\boldsymbol{\\delta} = f(\\mathbf{x}) + \\nabla_{\\mathbf{x}}f(\\mathbf{x})^{\\top} \\boldsymbol{\\delta}$$\nSince $\\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\mathbf{w}$, we have $f(\\mathbf{x} + \\boldsymbol{\\delta}) = f(\\mathbf{x}) + \\mathbf{w}^{\\top}\\boldsymbol{\\delta}$.\n\nTo determine the robustness, we must find the perturbation $\\boldsymbol{\\delta}$ with norm $\\|\\boldsymbol{\\delta}\\|_p \\leq \\epsilon$ that maximally changes $y \\cdot f(\\mathbf{x} + \\boldsymbol{\\delta})$ in the negative direction.\n$$y \\cdot f(\\mathbf{x} + \\boldsymbol{\\delta}) = y \\cdot (f(\\mathbf{x}) + \\mathbf{w}^{\\top}\\boldsymbol{\\delta}) = y \\cdot f(\\mathbf{x}) + y \\cdot (\\mathbf{w}^{\\top}\\boldsymbol{\\delta})$$\nThe term $y \\cdot f(\\mathbf{x})$ is the initial signed margin $m(\\mathbf{x}, y)$. The adversary seeks to make the term $y \\cdot (\\mathbf{w}^{\\top}\\boldsymbol{\\delta})$ as negative as possible.\nFrom Hölder's inequality, we have $|\\mathbf{a}^{\\top}\\mathbf{b}| \\leq \\|\\mathbf{a}\\|_{q}\\|\\mathbf{b}\\|_{p}$ where $1/p + 1/q = 1$. This implies that the minimum value of $\\mathbf{w}^{\\top}\\boldsymbol{\\delta}$ is $-\\|\\mathbf{w}\\|_{q}\\|\\boldsymbol{\\delta}\\|_{p}$.\nTherefore, the minimum value of $y \\cdot f(\\mathbf{x} + \\boldsymbol{\\delta})$ is:\n$$\\min_{\\|\\boldsymbol{\\delta}\\|_p \\leq \\epsilon} \\left( y \\cdot f(\\mathbf{x} + \\boldsymbol{\\delta}) \\right) = y \\cdot f(\\mathbf{x}) - |y| \\cdot \\|\\mathbf{w}\\|_q \\epsilon = m(\\mathbf{x}, y) - \\|\\mathbf{w}\\|_q \\epsilon$$\nThe classifier is guaranteed to be robust as long as this minimum value is greater than $0$:\n$$m(\\mathbf{x}, y) - \\|\\mathbf{w}\\|_q \\epsilon > 0 \\implies \\epsilon < \\frac{m(\\mathbf{x}, y)}{\\|\\mathbf{w}\\|_q}$$\nThe certified radius $\\epsilon_p$ is the threshold value $\\frac{m(\\mathbf{x}, y)}{\\|\\mathbf{w}\\|_q}$. This formula is valid only for correctly classified points where $m(\\mathbf{x}, y) > 0$. If a point is misclassified or on the boundary ($m(\\mathbf{x}, y) \\leq 0$), no perturbation is needed to satisfy the non-positive margin condition, so the certified radius is $0$.\nThe general formula for the certified radius is thus:\n$$\\epsilon_p(\\mathbf{x}, y) = \\max\\left(0, \\frac{y \\cdot f(\\mathbf{x})}{\\|\\mathbf{w}\\|_q}\\right)$$\nwhere $1/p + 1/q = 1$.\n\nThe problem provides the classifier parameters: $\\mathbf{w} = (0.8, -0.6)$ and $b = 0.1$.\nWe need to compute the certified radii for $\\ell_1$ and $\\ell_2$ perturbations.\nFor $\\ell_1$ perturbations ($p=1$), the dual norm is the $\\ell_\\infty$-norm ($q=\\infty$).\n$$\\|\\mathbf{w}\\|_{\\infty} = \\max(|0.8|, |-0.6|) = 0.8$$\nFor $\\ell_2$ perturbations ($p=2$), the dual norm is the $\\ell_2$-norm ($q=2$).\n$$\\|\\mathbf{w}\\|_2 = \\sqrt{0.8^2 + (-0.6)^2} = \\sqrt{0.64 + 0.36} = \\sqrt{1} = 1.0$$\nThe formulas for the radii are:\n$$r_{\\ell_1} = \\max\\left(0, \\frac{m(\\mathbf{x}, y)}{0.8}\\right)$$\n$$r_{\\ell_2} = \\max\\left(0, \\frac{m(\\mathbf{x}, y)}{1.0}\\right)$$\n\nWe now apply these formulas to the five given data points.\n\n1.  **Point 1**: $\\mathbf{x}_{1} = (2.0, -1.0)$, $y_{1} = +1$\n    $f(\\mathbf{x}_1) = (0.8)(2.0) + (-0.6)(-1.0) + 0.1 = 1.6 + 0.6 + 0.1 = 2.3$\n    $m_1 = y_1 \\cdot f(\\mathbf{x}_1) = (+1)(2.3) = 2.3$. Since $m_1 > 0$:\n    $r_{1, \\ell_1} = 2.3 / 0.8 = 2.875$\n    $r_{1, \\ell_2} = 2.3 / 1.0 = 2.3$\n\n2.  **Point 2**: $\\mathbf{x}_{2} = (0.5, 0.5)$, $y_{2} = -1$\n    $f(\\mathbf{x}_2) = (0.8)(0.5) + (-0.6)(0.5) + 0.1 = 0.4 - 0.3 + 0.1 = 0.2$\n    $m_2 = y_2 \\cdot f(\\mathbf{x}_2) = (-1)(0.2) = -0.2$. Since $m_2 \\leq 0$ (misclassified):\n    $r_{2, \\ell_1} = 0.0$\n    $r_{2, \\ell_2} = 0.0$\n\n3.  **Point 3**: $\\mathbf{x}_{3} = (-1.0, 2.0)$, $y_{3} = -1$\n    $f(\\mathbf{x}_3) = (0.8)(-1.0) + (-0.6)(2.0) + 0.1 = -0.8 - 1.2 + 0.1 = -1.9$\n    $m_3 = y_3 \\cdot f(\\mathbf{x}_3) = (-1)(-1.9) = 1.9$. Since $m_3 > 0$:\n    $r_{3, \\ell_1} = 1.9 / 0.8 = 2.375$\n    $r_{3, \\ell_2} = 1.9 / 1.0 = 1.9$\n\n4.  **Point 4**: $\\mathbf{x}_{4} = (0.125, 0.0)$, $y_{4} = +1$\n    $f(\\mathbf{x}_4) = (0.8)(0.125) + (-0.6)(0.0) + 0.1 = 0.1 + 0.0 + 0.1 = 0.2$\n    $m_4 = y_4 \\cdot f(\\mathbf{x}_4) = (+1)(0.2) = 0.2$. Since $m_4 > 0$:\n    $r_{4, \\ell_1} = 0.2 / 0.8 = 0.25$\n    $r_{4, \\ell_2} = 0.2 / 1.0 = 0.2$\n\n5.  **Point 5**: $\\mathbf{x}_{5} = (-0.125, 0.0)$, $y_{5} = +1$\n    $f(\\mathbf{x}_5) = (0.8)(-0.125) + (-0.6)(0.0) + 0.1 = -0.1 + 0.0 + 0.1 = 0.0$\n    $m_5 = y_5 \\cdot f(\\mathbf{x}_5) = (+1)(0.0) = 0.0$. Since $m_5 \\leq 0$ (on the decision boundary):\n    $r_{5, \\ell_1} = 0.0$\n    $r_{5, \\ell_2} = 0.0$\n\nThe final calculated radii, rounded to six decimal places, are:\n- Point 1: $[2.875000, 2.300000]$\n- Point 2: $[0.000000, 0.000000]$\n- Point 3: $[2.375000, 1.900000]$\n- Point 4: $[0.250000, 0.200000]$\n- Point 5: $[0.000000, 0.000000]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the certified robustness radii for a linear classifier\n    against l1 and l2 perturbations for a given set of data points.\n    \"\"\"\n    # Fixed parameters for the classifier\n    w = np.array([0.8, -0.6])\n    b = 0.1\n\n    # Dataset of labeled points\n    test_cases = [\n        (np.array([2.0, -1.0]), 1),\n        (np.array([0.5, 0.5]), -1),\n        (np.array([-1.0, 2.0]), -1),\n        (np.array([0.125, 0.0]), 1),\n        (np.array([-0.125, 0.0]), 1)\n    ]\n\n    # Calculate dual norms of the weight vector w\n    # For l1 radius (p=1), we need the dual norm q=inf\n    norm_w_inf = np.linalg.norm(w, ord=np.inf)\n    # For l2 radius (p=2), we need the dual norm q=2\n    norm_w_2 = np.linalg.norm(w, ord=2)\n\n    results = []\n    for x, y in test_cases:\n        # Compute the scoring function output\n        f_x = w.T @ x + b\n        \n        # Compute the signed margin\n        margin = y * f_x\n        \n        # If the point is misclassified or on the boundary, the radius is 0\n        if margin <= 0:\n            r_l1 = 0.0\n            r_l2 = 0.0\n        else:\n            # Compute certified radii using the formula: margin / ||w||_q\n            r_l1 = margin / norm_w_inf\n            r_l2 = margin / norm_w_2\n            \n        results.append((r_l1, r_l2))\n\n    # Format the results into the required string format with 6 decimal places.\n    # e.g., [[r1_l1,r1_l2],[r2_l1,r2_l2],...]\n    formatted_results = [f\"[{r[0]:.6f},{r[1]:.6f}]\" for r in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(output_string)\n\nsolve()\n```", "id": "3171496"}]}