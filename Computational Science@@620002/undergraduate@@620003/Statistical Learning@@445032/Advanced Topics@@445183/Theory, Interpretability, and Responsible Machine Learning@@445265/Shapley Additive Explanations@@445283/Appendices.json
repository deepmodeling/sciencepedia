{"hands_on_practices": [{"introduction": "The primary purpose of SHAP is to explain the output of a machine learning model, not to uncover the ground-truth mechanisms of the data-generating process itself. This hands-on practice provides a powerful demonstration of this crucial distinction by setting up a synthetic experiment where the true data-generating function is known. By comparing the \"ground-truth\" feature attributions from this true function to the SHAP values of a simple, misspecified linear model trained on the data, you will quantitatively measure how a model's limitations and biases are directly reflected in its explanations [@problem_id:3173341].", "problem": "You are given a synthetic setting to study Shapley Additive Explanations (SHAP) from first principles. A model produces a scalar prediction as a function of three independent input features. You must implement a complete, runnable program that (i) generates data from a known data-generating process, (ii) fits a misspecified linear model by ordinary least squares, (iii) computes exact ground-truth Shapley attributions for the true data-generating function using the interventional definition, (iv) computes exact Shapley attributions for the fitted linear model under the same interventional semantics, and (v) reports quantitative discrepancies between the two for a fixed test suite of inputs.\n\nFundamental base:\n- Cooperative game theory Shapley value for a function of features: For a model output function $f(\\mathbf{x})$ with feature index set $\\mathcal{M}=\\{1,\\dots,M\\}$ and an instance $\\mathbf{x}$, define the set function $v(S)=\\mathbb{E}[f(X)\\mid X_S=\\mathbf{x}_S]$, where the expectation is taken with respect to a specified background distribution and $S\\subseteq\\mathcal{M}$. The Shapley value for feature $i$ is\n$$\n\\phi_i(\\mathbf{x})=\\sum_{S\\subseteq \\mathcal{M}\\setminus\\{i\\}} \\frac{|S|!\\,(M-|S|-1)!}{M!}\\,\\Big(v(S\\cup\\{i\\})-v(S)\\Big).\n$$\nInterventional SHAP uses the product of marginal feature distributions as the background, so if features are independent in the data-generating process, the conditional expectations factorize accordingly.\n- Ordinary least squares (OLS) solves $\\min_{\\boldsymbol{\\beta}}\\sum_{n=1}^N \\big(y_n - \\beta_0 - \\sum_{j=1}^M \\beta_j x_{n,j}\\big)^2$, yielding the linear predictor $\\hat f(\\mathbf{x})=\\hat\\beta_0+\\sum_{j=1}^M \\hat\\beta_j x_j$.\n\nData-generating process:\n- Number of features $M=3$. All angles are in radians.\n- Feature distributions are mutually independent:\n  - $X_1\\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)$ with $\\mu_1=1.0$ and $\\sigma_1=2.0$.\n  - $X_2\\sim \\mathrm{Uniform}(a_2,b_2)$ with $a_2=-1.0$ and $b_2=2.0$.\n  - $X_3\\sim \\mathrm{Exponential}(\\lambda_3)$ with rate $\\lambda_3=1.5$ (so $\\mathbb{E}[X_3]=1/\\lambda_3$).\n- True regression function and noisy outcomes:\n  - Let constants be $c=0.7$, $w_1=3.0$, $w_2=2.5$, $w_3=1.2$, and $w_{12}=-1.7$.\n  - Define the true conditional mean function $f^\\star(\\mathbf{x}) = c + w_1 x_1 + w_2 x_2^2 + w_3 \\sin(x_3) + w_{12} x_1 x_2$.\n  - Observations are $Y = f^\\star(\\mathbf{X}) + \\varepsilon$ with $\\varepsilon\\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$, independent of $\\mathbf{X}$, and $\\sigma_\\varepsilon=0.3$.\n- Fit the misspecified linear model $\\hat f(\\mathbf{x})=\\hat\\beta_0+\\hat\\beta_1 x_1+\\hat\\beta_2 x_2+\\hat\\beta_3 x_3$ by OLS on $N$ independent training samples with $N=20000$.\n\nGround-truth SHAP under the interventional background:\n- Use the interventional definition $v(S)=\\mathbb{E}[f^\\star(X)\\mid X_S=\\mathbf{x}_S]$ with the true independent feature distribution. This induces a baseline $f^\\star_0=\\mathbb{E}[f^\\star(X)]$ and attributions $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x})$ that sum to $f^\\star(\\mathbf{x})-f^\\star_0$.\n- You must derive and use closed-form expressions for $\\mathbb{E}[X_2^2]$ and $\\mathbb{E}[\\sin(X_3)]$ under the specified distributions. All angles are in radians.\n\nModel-based SHAP for the fitted linear model:\n- Under the same interventional background (product of marginals) used above, compute the exact SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x})$ for the fitted linear predictor $\\hat f(\\mathbf{x})$.\n\nDiscrepancy metric:\n- For any instance $\\mathbf{x}$, define the total absolute attribution discrepancy as\n$$\nD(\\mathbf{x})=\\sum_{i=1}^3 \\left|\\phi^{\\mathrm{lin}}_i(\\mathbf{x}) - \\phi^{\\mathrm{true}}_i(\\mathbf{x})\\right|.\n$$\n\nTest suite:\n- Use the following four test inputs, with $x_3$ specified in radians:\n  1. $\\mathbf{x}^{(1)}=\\big(\\mu_1,\\ \\tfrac{a_2+b_2}{2},\\ \\tfrac{1}{\\lambda_3}\\big)$.\n  2. $\\mathbf{x}^{(2)}=\\big(\\mu_1+\\sigma_1,\\ b_2,\\ \\tfrac{1}{\\lambda_3}+1.0\\big)$.\n  3. $\\mathbf{x}^{(3)}=\\big(\\mu_1-2\\sigma_1,\\ a_2,\\ \\tfrac{1}{\\lambda_3}+3.0\\big)$.\n  4. $\\mathbf{x}^{(4)}=\\big(\\mu_1+0.5,\\ 0.0,\\ \\arcsin(\\mathbb{E}[\\sin(X_3)])\\big)$, where $\\arcsin$ denotes the principal value in $[0,\\tfrac{\\pi}{2}]$.\n\nRequired computations and outputs:\n- Generate the training data with a fixed random seed to ensure determinism.\n- Fit the linear model by OLS.\n- For each test input $\\mathbf{x}^{(k)}$, compute the ground-truth interventional SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x}^{(k)})$ and the linear-model SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x}^{(k)})$ under the same interventional background.\n- For each test input, compute $D(\\mathbf{x}^{(k)})$ as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the four discrepancy values as a comma-separated list enclosed in square brackets, in the order of the four test inputs, i.e., a string of the form $[d_1,d_2,d_3,d_4]$ where each $d_k$ is a floating-point number.", "solution": "The problem requires a quantitative comparison of Shapley Additive Explanations (SHAP) for a true, non-linear data-generating process and a misspecified linear model fitted to data from that process. The core of the task is to derive and implement the exact formulas for interventional SHAP values for both the true function and the linear approximation.\n\nFirst, we establish the theoretical groundwork by deriving the necessary expected values, which form the baseline for the SHAP calculations. Second, we derive the closed-form expressions for the ground-truth SHAP values, $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x})$, based on the true model function $f^\\star(\\mathbf{x})$. Third, we derive the corresponding SHAP values, $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x})$, for the fitted ordinary least squares (OLS) linear model, $\\hat{f}(\\mathbf{x})$. Finally, we outline the full computational procedure to obtain the specified discrepancy metric for the given test suite.\n\nThe interventional SHAP framework requires expectations over a background distribution, which is defined as the product of the independent marginal distributions of the features. The feature distributions are given as:\n- $X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ with $\\mu_1=1.0$ and $\\sigma_1=2.0$.\n- $X_2 \\sim \\mathrm{Uniform}(a_2, b_2)$ with $a_2=-1.0$ and $b_2=2.0$.\n- $X_3 \\sim \\mathrm{Exponential}(\\lambda_3)$ with $\\lambda_3=1.5$.\nThe true data-generating function is $f^\\star(\\mathbf{x}) = c + w_1 x_1 + w_2 x_2^2 + w_3 \\sin(x_3) + w_{12} x_1 x_2$. The SHAP calculations require the expectations of the terms appearing in this function.\n\nThe first-order moments are:\n- $\\mathbb{E}[X_1] = \\mu_1 = 1.0$.\n- $\\mathbb{E}[X_2] = \\frac{a_2+b_2}{2} = \\frac{-1.0+2.0}{2} = 0.5$.\n- $\\mathbb{E}[X_3] = \\frac{1}{\\lambda_3} = \\frac{1}{1.5} = \\frac{2}{3}$.\nWe also need expectations of the non-linear terms in $f^\\star(\\mathbf{x})$:\n- $\\mathbb{E}[X_2^2]$: For a uniform distribution $\\mathrm{U}(a, b)$, the second moment is $\\mathbb{E}[X^2] = \\frac{1}{b-a} \\int_a^b x^2 dx = \\frac{b^2+ab+a^2}{3}$.\n$$ \\mathbb{E}[X_2^2] = \\frac{(2.0)^2 + (2.0)(-1.0) + (-1.0)^2}{3} = \\frac{4.0 - 2.0 + 1.0}{3} = \\frac{3.0}{3.0} = 1.0. $$\n- $\\mathbb{E}[\\sin(X_3)]$: For an exponential distribution with rate $\\lambda_3$, the PDF is $p(x) = \\lambda_3 e^{-\\lambda_3 x}$ for $x \\ge 0$. The expectation is computed via the Laplace transform of $\\sin(x)$:\n$$ \\mathbb{E}[\\sin(X_3)] = \\int_0^\\infty \\sin(x) \\lambda_3 e^{-\\lambda_3 x} dx = \\lambda_3 \\int_0^\\infty \\sin(x) e^{-\\lambda_3 x} dx. $$\nThe integral is the Laplace transform of $\\sin(t)$ with $a=1$, evaluated at $s=\\lambda_3$, which is $\\frac{1}{s^2+1}$.\n$$ \\mathbb{E}[\\sin(X_3)] = \\lambda_3 \\left( \\frac{1}{\\lambda_3^2+1} \\right) = \\frac{1.5}{1.5^2+1} = \\frac{1.5}{3.25} = \\frac{6}{13}. $$\nThese values are fundamental for calculating the baseline (expected) predictions for both models.\n\nThe SHAP values are defined by $\\phi_i(\\mathbf{x}) = \\sum_{S\\subseteq \\mathcal{M}\\setminus\\{i\\}} \\frac{|S|!\\,(M-|S|-1)!}{M!}\\,\\big(v(S\\cup\\{i\\})-v(S)\\big)$, where $v(S)=\\mathbb{E}[f^\\star(X)\\mid X_S=\\mathbf{x}_S]$. Due to feature independence, this simplifies to taking expectations over the features not in $S$. A key property of SHAP is additivity: if $f = g_1 + g_2$, then $\\phi_i(f) = \\phi_i(g_1) + \\phi_i(g_2)$. We decompose $f^\\star(\\mathbf{x})$ as follows:\n$$ f^\\star(\\mathbf{x}) = c + \\underbrace{w_1 x_1}_{f_1(x_1)} + \\underbrace{w_2 x_2^2}_{f_2(x_2)} + \\underbrace{w_3 \\sin(x_3)}_{f_3(x_3)} + \\underbrace{w_{12} x_1 x_2}_{f_{12}(x_1, x_2)}. $$\nWe compute the SHAP values for each component. For a function of a single variable $f_i(x_i)$, the only non-zero SHAP value is for feature $i$: $\\phi_i(f_i) = f_i(x_i) - \\mathbb{E}[f_i(X_i)]$. For the interaction term $g(x_1, x_2) = x_1 x_2$, the attributions for features $1$ and $2$ (in a $3$-feature space), derived from the Shapley formula, are:\n$$ \\phi_1(g) = \\frac{1}{2}(x_1-\\mathbb{E}[X_1])(x_2+\\mathbb{E}[X_2]), \\quad \\phi_2(g) = \\frac{1}{2}(x_2-\\mathbb{E}[X_2])(x_1+\\mathbb{E}[X_1]). $$\nCombining these results using additivity, we get the exact SHAP values for $f^\\star$:\n- $\\phi_1^{\\mathrm{true}}(\\mathbf{x}) = \\phi_1(f_1) + \\phi_1(f_{12}) = w_1(x_1 - \\mathbb{E}[X_1]) + \\frac{w_{12}}{2}(x_1 - \\mathbb{E}[X_1])(x_2 + \\mathbb{E}[X_2])$.\n- $\\phi_2^{\\mathrm{true}}(\\mathbf{x}) = \\phi_2(f_2) + \\phi_2(f_{12}) = w_2(x_2^2 - \\mathbb{E}[X_2^2]) + \\frac{w_{12}}{2}(x_2 - \\mathbb{E}[X_2])(x_1 + \\mathbb{E}[X_1])$.\n- $\\phi_3^{\\mathrm{true}}(\\mathbf{x}) = \\phi_3(f_3) = w_3(\\sin(x_3) - \\mathbb{E}[\\sin(X_3)])$.\nThese equations provide the ground-truth attributions against which the linear model's attributions will be compared.\n\nThe misspecified model is a linear function fitted by OLS: $\\hat{f}(\\mathbf{x}) = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 + \\hat\\beta_3 x_3$. The coefficients $(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\hat\\beta_3)$ are estimated from $N=20000$ training samples. For a linear model with independent features, the interventional SHAP values have a simple, exact form. The total contribution $ \\hat{f}(\\mathbf{x}) - \\mathbb{E}[\\hat{f}(X)] $ is:\n$$ (\\hatbeta_0 + \\sum_{j=1}^3 \\hat\\beta_j x_j) - (\\hatbeta_0 + \\sum_{j=1}^3 \\hat\\beta_j \\mathbb{E}[X_j]) = \\sum_{j=1}^3 \\hatbeta_j(x_j - \\mathbb{E}[X_j]). $$\nThis expression is already a sum of terms, each depending on a single feature. This unique decomposition gives the SHAP values directly:\n$$ \\phi_i^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_i(x_i - \\mathbb{E}[X_i]). $$\nSpecifically for our $3$-feature model:\n- $\\phi_1^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_1(x_1 - \\mathbb{E}[X_1])$.\n- $\\phi_2^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_2(x_2 - \\mathbb{E}[X_2])$.\n- $\\phi_3^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_3(x_3 - \\mathbb{E}[X_3])$.\nThese formulas show that the linear model incorrectly assumes that the attribution for a feature is simply its centered value scaled by a single coefficient, ignoring any non-linearities or interactions present in the true function $f^\\star$.\n\nThe implementation proceeds as follows:\n$1$. **Data Generation**: Generate $N=20000$ samples from the specified independent distributions for $X_1$, $X_2$, and $X_3$.\n$2$. **True Outcomes**: Compute the true function values $f^\\star(\\mathbf{X})$ and add Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ with $\\sigma_\\varepsilon=0.3$ to get the observed outcomes $Y$.\n$3$. **OLS Fitting**: Form the design matrix $\\mathbf{X}_{\\mathrm{b}}$ with a column of ones for the intercept. Solve the normal equations $\\mathbf{X}_{\\mathrm{b}}^T \\mathbf{X}_{\\mathrm{b}} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}_{\\mathrm{b}}^T Y$ to find the OLS coefficient vector $\\hat{\\boldsymbol{\\beta}} = (\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\hat\\beta_3)^T$.\n$4$. **SHAP Calculation**: For each test input $\\mathbf{x}^{(k)}$ from the provided suite:\n    - Calculate the ground-truth SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x}^{(k)})$ using the derived formulas and known parameters ($w_i$, etc.).\n    - Calculate the linear model's SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x}^{(k)})$ using the fitted coefficients $\\hat\\beta_i$.\n$5$. **Discrepancy Metric**: Compute the total absolute attribution discrepancy for each test input:\n$$ D(\\mathbf{x}^{(k)}) = \\sum_{i=1}^3 \\left|\\phi^{\\mathrm{lin}}_i(\\mathbf{x}^{(k)}) - \\phi^{\\mathrm{true}}_i(\\mathbf{x}^{(k)})\\right|. $$\nThe program will execute these steps and report the computed values of $D(\\mathbf{x}^{(k)})$ for the four test cases. A fixed random seed ensures deterministic and reproducible results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full procedure to calculate SHAP value discrepancies.\n    \"\"\"\n    # Use a fixed random seed for reproducibility\n    SEED = 0\n    rng = np.random.default_rng(SEED)\n\n    # ----------------------------------------------------------------------\n    # 1. Define constants and parameters from the problem statement\n    # ----------------------------------------------------------------------\n    # Data-generating process parameters\n    N = 20000\n    mu1, sigma1 = 1.0, 2.0\n    a2, b2 = -1.0, 2.0\n    lambda3 = 1.5\n    \n    # True function parameters\n    c = 0.7\n    w1 = 3.0\n    w2 = 2.5\n    w3 = 1.2\n    w12 = -1.7\n    \n    # Noise parameter\n    sigma_eps = 0.3\n\n    # ----------------------------------------------------------------------\n    # 2. Generate training data\n    # ----------------------------------------------------------------------\n    X1_train = rng.normal(mu1, sigma1, N)\n    X2_train = rng.uniform(a2, b2, N)\n    X3_train = rng.exponential(1.0 / lambda3, N)\n    X_train = np.stack([X1_train, X2_train, X3_train], axis=1)\n\n    f_star = c + w1*X1_train + w2*X2_train**2 + w3*np.sin(X3_train) + w12*X1_train*X2_train\n    Y_train = f_star + rng.normal(0, sigma_eps, N)\n\n    # ----------------------------------------------------------------------\n    # 3. Fit the misspecified linear model by OLS\n    # ----------------------------------------------------------------------\n    X_b = np.c_[np.ones(N), X_train]\n    # Solve X_b.T @ X_b @ beta_hat = X_b.T @ Y_train\n    beta_hat = np.linalg.solve(X_b.T @ X_b, X_b.T @ Y_train)\n    beta0_hat, beta1_hat, beta2_hat, beta3_hat = beta_hat\n\n    # ----------------------------------------------------------------------\n    # 4. Define background expectations and test suite\n    # ----------------------------------------------------------------------\n    E_X1 = mu1\n    E_X2 = (a2 + b2) / 2.0\n    E_X3 = 1.0 / lambda3\n    E_X2_sq = 1.0  # Derived as (b2^3 - a2^3) / (3*(b2-a2)) = 1.0\n    E_sinX3 = lambda3 / (lambda3**2 + 1.0) # Derived from Laplace transform\n\n    test_cases = [\n        (mu1, (a2 + b2) / 2.0, 1.0 / lambda3),\n        (mu1 + sigma1, b2, 1.0 / lambda3 + 1.0),\n        (mu1 - 2 * sigma1, a2, 1.0 / lambda3 + 3.0),\n        (mu1 + 0.5, 0.0, np.arcsin(E_sinX3))\n    ]\n\n    discrepancies = []\n    \n    # ----------------------------------------------------------------------\n    # 5. Calculate SHAP values and discrepancies for each test case\n    # ----------------------------------------------------------------------\n    for x_test in test_cases:\n        x1, x2, x3 = x_test\n\n        # --- Ground-truth SHAP values ---\n        phi1_true = w1 * (x1 - E_X1) + (w12 / 2.0) * (x1 - E_X1) * (x2 + E_X2)\n        phi2_true = w2 * (x2**2 - E_X2_sq) + (w12 / 2.0) * (x2 - E_X2) * (x1 + E_X1)\n        phi3_true = w3 * (np.sin(x3) - E_sinX3)\n        \n        # --- Linear model SHAP values ---\n        phi1_lin = beta1_hat * (x1 - E_X1)\n        phi2_lin = beta2_hat * (x2 - E_X2)\n        phi3_lin = beta3_hat * (x3 - E_X3)\n        \n        # --- Total absolute attribution discrepancy ---\n        D_x = (\n            np.abs(phi1_lin - phi1_true) + \n            np.abs(phi2_lin - phi2_true) + \n            np.abs(phi3_lin - phi3_true)\n        )\n        discrepancies.append(D_x)\n\n    # ----------------------------------------------------------------------\n    # 6. Format and print the final output\n    # ----------------------------------------------------------------------\n    print(f\"[{','.join(f'{d:.7f}' for d in discrepancies)}]\")\n\nsolve()\n\n```", "id": "3173341"}, {"introduction": "Building on the principle that SHAP explains the model, it follows that our choices in feature engineering have a profound impact on the resulting explanations. This exercise explores a common pitfall: applying SHAP to models that use naive encodings for complex data types, such as cyclic features like months of the year. You will implement an experiment that first reveals how a simple ordinal encoding leads to nonsensical SHAP values at the \"wrap-around\" point (December to January) and then demonstrates how a proper sine-cosine cyclical encoding produces interpretable and balanced attributions that respect the feature's nature [@problem_id:3173394].", "problem": "You are asked to implement a reproducible experiment to demonstrate how Shapley Additive Explanations (SHAP) can mislead when used with an ordinal encoding of cyclic features, and how a cyclic sine-cosine encoding restores balanced attributions. The core object is a deterministic model that maps a calendar month to a scalar output. Use the following specifications.\n\n- Fundamental base: use the cooperative game theory definition of Shapley values and the conditional expectation semantics for missing features in Shapley Additive Explanations (SHAP). Specifically, the value function uses conditional expectations of the model output given a subset of observed features, and the Shapley value of a feature is the average marginal contribution across all possible subsets and permutations. Do not assume any ad hoc shortcut formulas; build the reasoning from the definitions and basic symmetries.\n\n- Dataset and target: construct the set of months $\\{1,2,\\dots,12\\}$. Associate each month $m$ with an angle $\\theta_m = 2\\pi m / 12$ measured in radians. Define a deterministic ground-truth model\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m),\n$$\nwith $A = 2$ and $B = 1$.\n\n- Two encodings and models:\n  1. Ordinal encoding: treat the month as a single integer feature $m$ and fit a least-squares linear model $g(m) = \\alpha m + \\beta$ to the ground-truth outputs $f(m)$ over $m \\in \\{1,\\dots,12\\}$. In the SHAP framework with a single feature, the Shapley value for an instance $m$ is the difference between the model output at $m$ and the baseline expectation under the empirical distribution of months, that is,\n  $$\n  \\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)],\n  $$\n  where $M$ is uniformly distributed over $\\{1,\\dots,12\\}$.\n  2. Cyclic sine-cosine encoding: represent each month $m$ by two features $s_m = \\sin(\\theta_m)$ and $c_m = \\cos(\\theta_m)$ and use the linear model $h(s,c) = A s + B c$. For SHAP, use the conditional expectation semantics with the empirical uniform distribution over months. The Shapley values $\\phi_s(m)$ and $\\phi_c(m)$ should be computed from first principles via conditional expectations and marginal contributions of the coalitions $\\{\\}$, $\\{s\\}$, $\\{c\\}$, and $\\{s,c\\}$, leveraging the symmetries of $\\sin$ and $\\cos$ over the $12$ equally spaced angles.\n\n- Angle unit: all angles must be in radians.\n\n- Test suite: compute the following quantities to evaluate misinterpretations and attribution balance. Use the uniform empirical month distribution on $\\{1,\\dots,12\\}$ for all expectations.\n  1. Adjacency misinterpretation check across the circular boundary: compute the absolute SHAP gap for adjacent months at the boundary under the ordinal model,\n  $$\n  \\Delta_{\\text{ord}} = \\left|\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)\\right|,\n  $$\n  and compare to the cyclic model’s SHAP-sum gap,\n  $$\n  \\Delta_{\\text{cyc}} = \\left|(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))\\right|.\n  $$\n  Output a boolean indicating whether $\\Delta_{\\text{ord}} > \\Delta_{\\text{cyc}}$.\n  2. Equal-sine attribution balance for months with the same sine but different cosine ($m=1$ and $m=5$): output two booleans,\n     - whether $\\phi_s(1)$ equals $\\phi_s(5)$ up to numerical tolerance $10^{-12}$, and\n     - whether $\\phi_c(1)$ and $\\phi_c(5)$ are equal in magnitude and opposite in sign up to numerical tolerance $10^{-12}$; that is, whether $\\phi_c(1)+\\phi_c(5)$ is numerically zero.\n  3. Explicit SHAP components at a sine-dominant month ($m=3$, angle $\\theta_3 = \\pi/2$): output $\\phi_s(3)$ and $\\phi_c(3)$ as floats.\n  4. Explicit SHAP components at a cosine-dominant month ($m=6$, angle $\\theta_6 = \\pi$): output $\\phi_s(6)$ and $\\phi_c(6)$ as floats.\n  5. SHAP-sum consistency: for months $m \\in \\{1,3,5,6,12\\}$, verify that $\\phi_s(m)+\\phi_c(m)$ equals $f(m)$ within tolerance $10^{-12}$ (recall the baseline expectation $\\mathbb{E}[f(M)]$ is zero by symmetry). Output a single boolean that is true if and only if this consistency holds for all five months.\n  6. Misinterpretation ratio for the adjacency boundary: output the float\n  $$\n  R = \\frac{\\Delta_{\\text{ord}}}{\\max(\\Delta_{\\text{cyc}}, 10^{-12})}.\n  $$\n\n- Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n  $$\n  [\\text{adjacency\\_boolean},\\ \\text{equal\\_sine\\_boolean},\\ \\text{cos\\_opposite\\_boolean},\\ \\phi_s(3),\\ \\phi_c(3),\\ \\phi_s(6),\\ \\phi_c(6),\\ \\text{sum\\_consistency\\_boolean},\\ R].\n  $$\nAll numeric outputs must be in standard floating-point format without units; all booleans must be standard logical values.", "solution": "The problem statement is validated as scientifically sound, well-posed, objective, and complete. It presents a rigorous exercise in applying the foundational principles of Shapley Additive Explanations (SHAP) to demonstrate a known issue with feature engineering for cyclic variables. We will proceed with a full solution.\n\nThe core of the problem is to compute and compare SHAP values for two different representations of a cyclic feature (a calendar month). The ground-truth model is a deterministic function of the month $m \\in \\{1, 2, \\dots, 12\\}$:\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m)\n$$\nwhere $A = 2$, $B = 1$, and $\\theta_m = 2\\pi m / 12$. The empirical distribution of months is uniform over $\\{1, \\dots, 12\\}$.\n\n### Ordinal Encoding Model Analysis\n\nFirst, we consider the ordinal encoding, where the month is treated as a single integer feature $m$. A simple linear model $g(m) = \\alpha m + \\beta$ is fitted to the ground-truth data $(m, f(m))$ for $m=1, \\dots, 12$ using ordinary least squares.\n\nThe coefficients $\\alpha$ and $\\beta$ are given by:\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - \\bar{m})(f(m) - \\bar{f})}{\\sum_{m=1}^{12} (m - \\bar{m})^2}, \\quad \\beta = \\bar{f} - \\alpha \\bar{m}\n$$\nHere, $\\bar{m} = \\mathbb{E}[M] = \\frac{1}{12}\\sum_{m=1}^{12} m = 6.5$. The average of the ground-truth function $\\bar{f} = \\mathbb{E}[f(M)]$ is:\n$$\n\\bar{f} = \\frac{1}{12}\\sum_{m=1}^{12} \\left(A \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\cos\\left(\\frac{2\\pi m}{12}\\right)\\right) = 0\n$$\nThis is because the sums of sine and cosine over a complete cycle of equally spaced points are zero.\nWith $\\bar{f}=0$, the coefficients simplify to:\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - 6.5)f(m)}{\\sum_{m=1}^{12} (m - 6.5)^2}, \\quad \\beta = -\\alpha \\bar{m} = -6.5\\alpha\n$$\nThe denominator is a standard sum, $\\sum_{m=1}^{12} (m - 6.5)^2 = 143$. The numerator can be computed symbolically:\n$$\n\\sum_{m=1}^{12} m f(m) = A \\sum_{m=1}^{12} m \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\sum_{m=1}^{12} m \\cos\\left(\\frac{2\\pi m}{12}\\right)\n$$\nUsing known finite trigonometric sum identities, $\\sum_{k=1}^{N} k \\sin(2\\pi k/N) = -N/2 \\cot(\\pi/N)$ and $\\sum_{k=1}^{N} k \\cos(2\\pi k/N) = -N/2$. For $N=12$, $A=2$, and $B=1$:\n$$\n\\sum_{m=1}^{12} m f(m) = 2 \\left(-6 \\cot\\left(\\frac{\\pi}{12}\\right)\\right) + 1 \\left(\\frac{12}{2}\\right) = -12(2+\\sqrt{3}) + 6 = -24 - 12\\sqrt{3} + 6 = -18 - 12\\sqrt{3}\n$$\nSince $\\sum f(m)=0$, the numerator is $\\sum (m-6.5)f(m) = \\sum m f(m) = -18 - 12\\sqrt{3}$.\n$$\n\\alpha = \\frac{-18 - 12\\sqrt{3}}{143} \\approx -0.27122\n$$\nThe SHAP value for the single-feature ordinal model is $\\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)]$.\n$$\n\\mathbb{E}[g(M)] = \\mathbb{E}[\\alpha M + \\beta] = \\alpha\\mathbb{E}[M] + \\beta = \\alpha\\bar{m} + \\beta = \\alpha\\bar{m} + (\\bar{f} - \\alpha\\bar{m}) = \\bar{f} = 0\n$$\nThus, $\\phi_{\\text{ord}}(m) = g(m) = \\alpha m + \\beta$. The adjacency gap at the circular boundary is:\n$$\n\\Delta_{\\text{ord}} = |\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)| = |(12\\alpha+\\beta) - (\\alpha+\\beta)| = |11\\alpha| = 11 \\frac{18 + 12\\sqrt{3}}{143} = \\frac{18 + 12\\sqrt{3}}{13} \\approx 2.9834\n$$\nThis large gap reflects the model's failure to understand that month $12$ is adjacent to month $1$.\n\n### Cyclic Sine-Cosine Encoding Model Analysis\n\nNext, we use a two-feature encoding: $s_m = \\sin(\\theta_m)$ and $c_m = \\cos(\\theta_m)$. The model is $h(s, c) = As + Bc$, which is the ground-truth model itself. We compute the SHAP values $\\phi_s(m)$ and $\\phi_c(m)$ for an instance $(s_m, c_m)$ from first principles.\n\nThe features (players) are $F=\\{s, c\\}$. The Shapley values are:\n$$\n\\phi_s(m) = \\frac{1}{2}\\left(v(\\{s\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{c\\})\\right)\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}\\left(v(\\{c\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{s\\})\\right)\n$$\nThe value function $v(S)$ is the conditional expectation of the model output, where features in coalition $S$ are fixed to the instance's values, and other features are averaged over their conditional distribution. The distribution is uniform over the $12$ months.\n\n1.  $v(\\emptyset) = \\mathbb{E}[h(S,C)] = \\mathbb{E}[f(M)] = 0$, as shown before.\n2.  $v(\\{s,c\\}) = \\mathbb{E}[h(S,C) | s=s_m, c=c_m] = h(s_m, c_m) = A s_m + B c_m = f(m)$.\n3.  $v(\\{s\\}) = \\mathbb{E}[h(S,C) | s=s_m] = A s_m + B \\mathbb{E}[C | s=s_m]$. Due to the symmetries of the cosine function for angles with the same sine (i.e., $\\theta$ and $\\pi-\\theta$), we have $\\mathbb{E}[C | s=s_m] = 0$ for all $m$. Thus, $v(\\{s\\}) = A s_m$.\n4.  $v(\\{c\\}) = \\mathbb{E}[h(S,C) | c=c_m] = A \\mathbb{E}[S | c=c_m] + B c_m$. Due to the symmetries of the sine function for angles with the same cosine (i.e., $\\theta$ and $2\\pi-\\theta$), we have $\\mathbbE}[S | c=c_m] = 0$ for all $m$. Thus, $v(\\{c\\}) = B c_m$.\n\nSubstituting these into the Shapley formulas:\n$$\n\\phi_s(m) = \\frac{1}{2}(A s_m - 0) + \\frac{1}{2}((A s_m + B c_m) - B c_m) = A s_m\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}(B c_m - 0) + \\frac{1}{2}((A s_m + B c_m) - A s_m) = B c_m\n$$\nThe SHAP values are simply the individual terms of the linear model: $\\phi_s(m) = 2 \\sin(\\theta_m)$ and $\\phi_c(m) = \\cos(\\theta_m)$.\nThe sum of SHAP attributions is $\\phi_s(m) + \\phi_c(m) = A s_m + B c_m = f(m)$. This matches the SHAP property $\\sum \\phi_i = f(x) - \\mathbb{E}[f]$ since $\\mathbb{E}[f]=0$.\n\nThe adjacency gap for the cyclic model is:\n$$\n\\Delta_{\\text{cyc}} = |(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))| = |f(12) - f(1)|\n$$\n$$\nf(12) = 2\\sin(2\\pi) + \\cos(2\\pi) = 1\n$$\n$$\nf(1) = 2\\sin(\\pi/6) + \\cos(\\pi/6) = 2(1/2) + \\sqrt{3}/2 = 1+\\sqrt{3}/2\n$$\n$$\n\\Delta_{\\text{cyc}} = |1 - (1+\\sqrt{3}/2)| = \\sqrt{3}/2 \\approx 0.8660\n$$\n\n### Test Suite Evaluation\n\nWe now compute the required quantities.\n\n1.  **Adjacency misinterpretation**: $\\Delta_{\\text{ord}} \\approx 2.9834$ and $\\Delta_{\\text{cyc}} \\approx 0.8660$. Since $2.9834 > 0.8660$, the output is `True`. The ordinal model creates a large, artificial discontinuity at the year boundary.\n2.  **Equal-sine attribution**: For $m=1$ ($\\theta=\\pi/6$) and $m=5$ ($\\theta=5\\pi/6$), $\\sin(\\theta_1)=\\sin(\\theta_5)=1/2$.\n    $\\phi_s(1) = A s_1 = 2(1/2) = 1$. $\\phi_s(5) = A s_5 = 2(1/2) = 1$. They are equal, so the output is `True`.\n3.  **Opposite-cosine attribution**: For $m=1, 5$, $\\cos(\\theta_1)=\\sqrt{3}/2$ and $\\cos(\\theta_5)=-\\sqrt{3}/2$.\n    $\\phi_c(1) = B c_1 = \\sqrt{3}/2$. $\\phi_c(5) = B c_5 = -\\sqrt{3}/2$. Their sum is $0$, so the output is `True`. The attributions correctly reflect the features' relationship.\n4.  **SHAP components at $m=3$**: $\\theta_3=\\pi/2$, so $s_3=1, c_3=0$.\n    $\\phi_s(3) = A s_3 = 2(1) = 2.0$.\n    $\\phi_c(3) = B c_3 = 1(0) = 0.0$.\n5.  **SHAP components at $m=6$**: $\\theta_6=\\pi$, so $s_6=0, c_6=-1$.\n    $\\phi_s(6) = A s_6 = 2(0) = 0.0$.\n    $\\phi_c(6) = B c_6 = 1(-1) = -1.0$.\n6.  **SHAP-sum consistency**: As shown, $\\phi_s(m) + \\phi_c(m) = f(m)$ for all $m$. This check will pass for all specified months. The output is `True`.\n7.  **Misinterpretation ratio**:\n    $$\n    R = \\frac{\\Delta_{\\text{ord}}}{\\Delta_{\\text{cyc}}} = \\frac{(18 + 12\\sqrt{3})/13}{\\sqrt{3}/2} = \\frac{2(18 + 12\\sqrt{3})}{13\\sqrt{3}} = \\frac{36\\sqrt{3} + 72}{39} = \\frac{12\\sqrt{3} + 24}{13} \\approx 3.4450\n    $$\n    The ordinal model's boundary gap is over $3.4$ times larger than the cyclic model's, quantifying the misinterpretation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a reproducible experiment on SHAP values for cyclic features.\n    \"\"\"\n    # Define constants from the problem statement\n    A = 2.0\n    B = 1.0\n    TOL = 1e-12\n\n    # Construct the base dataset\n    months = np.arange(1, 13)\n    thetas = 2 * np.pi * months / 12\n    f_vals = A * np.sin(thetas) + B * np.cos(thetas)\n\n    # --- Ordinal Model Analysis ---\n    # Fit a least-squares linear model g(m) = alpha*m + beta\n    alpha, beta = np.polyfit(months, f_vals, 1)\n\n    # The SHAP value is phi_ord(m) = g(m) - E[g(M)].\n    # E[g(M)] = E[alpha*M + beta] = alpha*E[M] + beta.\n    # Since beta = mean(f) - alpha*mean(m) and mean(f) is numerically ~0,\n    # E[g(M)] is also numerically ~0.\n    # Therefore, phi_ord(m) is approximately g(m).\n    phi_ord_12 = alpha * 12 + beta\n    phi_ord_1 = alpha * 1 + beta\n    delta_ord = np.abs(phi_ord_12 - phi_ord_1)\n\n    # --- Cyclic Sine-Cosine Model Analysis ---\n    # Features are s_m = sin(theta_m) and c_m = cos(theta_m)\n    # The model is h(s,c) = A*s + B*c.\n    # As derived in the solution, due to feature distribution symmetries,\n    # the SHAP values are phi_s(m) = A*s_m and phi_c(m) = B*c_m.\n    s_vals = np.sin(thetas)\n    c_vals = np.cos(thetas)\n\n    phi_s = A * s_vals\n    phi_c = B * c_vals\n    \n    # Month indices for numpy arrays (0-indexed)\n    m1_idx, m3_idx, m5_idx, m6_idx, m12_idx = 0, 2, 4, 5, 11\n\n    # SHAP sum for the cyclic model\n    phi_sum_cyclic_12 = phi_s[m12_idx] + phi_c[m12_idx]\n    phi_sum_cyclic_1 = phi_s[m1_idx] + phi_c[m1_idx]\n    delta_cyc = np.abs(phi_sum_cyclic_12 - phi_sum_cyclic_1)\n\n    # --- Test Suite Evaluation ---\n\n    # 1. Adjacency misinterpretation check\n    adjacency_boolean = delta_ord > delta_cyc\n\n    # 2. Equal-sine attribution balance for m=1 and m=5\n    equal_sine_boolean = np.isclose(phi_s[m1_idx], phi_s[m5_idx], atol=TOL)\n\n    # 3. Opposite-cosine attribution balance for m=1 and m=5\n    cos_opposite_boolean = np.isclose(phi_c[m1_idx] + phi_c[m5_idx], 0, atol=TOL)\n\n    # 4. Explicit SHAP components at m=3\n    phi_s_3 = phi_s[m3_idx]\n    phi_c_3 = phi_c[m3_idx]\n\n    # 5. Explicit SHAP components at m=6\n    phi_s_6 = phi_s[m6_idx]\n    phi_c_6 = phi_c[m6_idx]\n\n    # 6. SHAP-sum consistency\n    test_months_indices = [m1_idx, m3_idx, m5_idx, m6_idx, m12_idx]\n    shap_sums = phi_s[test_months_indices] + phi_c[test_months_indices]\n    f_vals_test = f_vals[test_months_indices]\n    sum_consistency_boolean = np.all(np.isclose(shap_sums, f_vals_test, atol=TOL))\n    \n    # 7. Misinterpretation ratio\n    R = delta_ord / max(delta_cyc, 1e-12)\n\n    # Assemble final results list\n    results = [\n        adjacency_boolean,\n        equal_sine_boolean,\n        cos_opposite_boolean,\n        phi_s_3,\n        phi_c_3,\n        phi_s_6,\n        phi_c_6,\n        sum_consistency_boolean,\n        R\n    ]\n\n    # Format and print the final output\n    # Booleans are lowercased for standard Python str() conversion\n    formatted_results = [str(r).lower() if isinstance(r, bool) else f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3173394"}, {"introduction": "Real-world datasets often contain highly correlated features, posing a significant challenge for interpretation: if two features carry similar information, how should a model's prediction credit be divided between them? This practice delves into this question by examining how TreeSHAP, a highly efficient algorithm for tree-based models, allocates attribution between two collinear features. By deriving the coalition values from the foundational principles of TreeSHAP's path-probability semantics, you will investigate how the degree of collinearity influences the final SHAP values and gain insight into the nuanced behavior of credit allocation in the presence of multicollinearity [@problem_id:3173371].", "problem": "You are given a two-feature decision tree model and a synthetic data-generating process exhibiting strong collinearity. Your task is to implement a program that computes Shapley Additive explanations (SHAP) for this tree, using the TreeSHAP (Tree Shapley Additive explanations) path-probability semantics, and to quantitatively examine whether the two correlated features receive equal credit. The program must produce a single line of output in the specified format.\n\nConsider the following setup.\n\n- Data-generating process:\n  - There are two features, $X_1$ and $X_2$. Generate data according to $X_1 \\sim \\mathcal{N}(0,1)$, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, and $X_2 = X_1 + \\epsilon$. Here $\\mathcal{N}$ denotes the normal distribution. Assume independence between $X_1$ and $\\epsilon$.\n- Model: A fixed depth-$2$ decision tree with the following structure:\n  - Root node splits on $X_1$ at threshold $0$. If $x_1 \\le 0$, go to the left child; otherwise, go to the right leaf.\n  - Left child splits on $X_2$ at threshold $0$. If $x_2 \\le 0$, go to a left leaf with value $v_{LL}$; otherwise, go to a right leaf with value $v_{LR}$.\n  - Right child of the root is a leaf with value $v_R$.\n  - Use $v_{LL}=-1.0$, $v_{LR}=+1.0$, and $v_R=+2.0$.\n- Instance to explain: Use the fixed point $x=(x_1,x_2)=(0.2,0.25)$.\n\nFundamental base and definitions to use:\n- Let the Shapley value from cooperative game theory define the per-feature attribution. For $d=2$ features and value function $v(S)$ for a coalition $S \\subseteq \\{1,2\\}$, the Shapley value for feature $i$ is the average marginal contribution over all permutations of features. No shortcut formulas are provided; use the fundamental definition.\n- Under TreeSHAP semantics, when a feature used in a split is missing from the coalition $S$, the model prediction is evaluated by following both child branches, weighted by the empirical or distributional probability (path probability) that a random training sample reaching that node would take that branch. For the given tree, this reduces to using the probability of going left or right at each split, conditional on having reached that split.\n\nMathematical facts to employ as foundational starting points:\n- For the bivariate normal distribution with zero means, unit variances, and correlation $\\rho$, the orthant probability is $P(X \\le 0, Y \\le 0) = \\tfrac{1}{4} + \\tfrac{1}{2\\pi}\\arcsin(\\rho)$. This is a well-tested formula.\n- For the given data-generating process, $(X_1,X_2)$ is jointly normal with $\\operatorname{Var}(X_1)=1$, $\\operatorname{Var}(X_2)=1+\\sigma^2$, and $\\operatorname{Cov}(X_1,X_2)=1$. Therefore, the correlation is $\\rho = \\dfrac{1}{\\sqrt{1+\\sigma^2}}$.\n\nYour tasks:\n1) Derive, from first principles and the above definitions, the coalition values $v(S)$ for $S\\in\\{\\emptyset,\\{1\\},\\{2\\},\\{1,2\\}\\}$ for the fixed point $x=(0.2,0.25)$ under TreeSHAP semantics. In particular, you must compute the path probabilities at the root split on $X_1$ and at the left-child split on $X_2$ when either $X_1$ or $X_2$ is missing. Use exact probabilities obtained from the data-generating distribution (not empirical sampling).\n2) Using the Shapley definition for $d=2$ features, compute the per-feature attributions $\\phi_1$ and $\\phi_2$ for the fixed point $x$.\n3) Investigate the effect of collinearity on TreeSHAP path probabilities by varying $\\sigma$, and determine whether SHAP assigns equal credit to $X_1$ and $X_2$. Quantify the difference $\\phi_1 - \\phi_2$ for each $\\sigma$.\n\nTest suite:\n- Use the following set of $\\sigma$ values: $\\{0.0,\\;0.1,\\;0.5,\\;1.0,\\;3.0\\}$.\n\nRequired output:\n- For each $\\sigma$ in the test suite, compute the Shapley values and return a list $[\\phi_1,\\phi_2,\\phi_1-\\phi_2]$, with each value rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list for one $\\sigma$, in the same order as given. For example, a valid output format is $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$ with numeric entries rounded to $6$ decimal places.\n\nNo physical units are involved. Angles, percentages, or other unit conventions are not applicable here. All numeric quantities must be pure real numbers. The final result must be entirely determined by the given definitions and the specified test suite. The solution must not use any external data or randomness and must not require user input.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Data-generating process**:\n    - Two features, $X_1$ and $X_2$.\n    - $X_1 \\sim \\mathcal{N}(0,1)$.\n    - $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n    - $X_2 = X_1 + \\epsilon$.\n    - $X_1$ and $\\epsilon$ are independent.\n- **Model**: A fixed depth-$2$ decision tree.\n    - Root: Split on $X_1$ at threshold $0$. Left if $x_1 \\le 0$, right if $x_1 > 0$.\n    - Root's right child: Leaf with value $v_R = +2.0$.\n    - Root's left child: Split on $X_2$ at threshold $0$. Left if $x_2 \\le 0$, right if $x_2 > 0$.\n    - Left child's left leaf: Value $v_{LL} = -1.0$.\n    - Left child's right leaf: Value $v_{LR} = +1.0$.\n- **Instance to explain**: $x=(x_1,x_2)=(0.2,0.25)$.\n- **Fundamental definitions**:\n    - Shapley value for $d=2$ features for feature $i$: defined by the average marginal contribution over all permutations.\n    - TreeSHAP semantics: When a feature in a split is missing from coalition $S$, follow both child branches, weighted by the path probability that a random sample reaching the node would take that branch. Use exact probabilities from the data-generating distribution.\n- **Mathematical facts**:\n    - For a bivariate normal distribution $(X,Y)$ with zero means, unit variances, and correlation $\\rho$, the orthant probability is $P(X \\le 0, Y \\le 0) = \\tfrac{1}{4} + \\tfrac{1}{2\\pi}\\arcsin(\\rho)$.\n    - For the given process, $(X_1, X_2)$ is jointly normal with $\\operatorname{Var}(X_1)=1$, $\\operatorname{Var}(X_2)=1+\\sigma^2$, $\\operatorname{Cov}(X_1,X_2)=1$, and correlation $\\rho = \\dfrac{1}{\\sqrt{1+\\sigma^2}}$.\n- **Test suite**: $\\sigma \\in \\{0.0, 0.1, 0.5, 1.0, 3.0\\}$.\n- **Required output**: For each $\\sigma$, a list $[\\phi_1, \\phi_2, \\phi_1-\\phi_2]$ rounded to $6$ decimal places. The final output is a list of these lists.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n- **Scientifically Grounded**: The problem is rooted in the established theory of cooperative game theory (Shapley values) and its application to machine learning interpretability (SHAP). The data-generating process and model are synthetic but mathematically well-defined. All components are consistent with standard statistical and probability theory.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information: a specific model, a specific data distribution, a specific instance to explain, and explicit definitions for calculating the desired quantities. The tasks lead to a unique, computable solution.\n- **Objective**: The problem is stated in precise, objective mathematical language. It is free from ambiguity, subjectivity, or opinion-based claims.\n\nThe problem does not exhibit any of the invalidity flaws. It is a rigorous, self-contained theoretical exercise within the specified topic of statistical learning.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n## Solution Derivation\n\nThe Shapley values for two features, $\\phi_1$ and $\\phi_2$, are defined as:\n$$ \\phi_1 = \\frac{1}{2}\\left[ (v(\\{1\\}) - v(\\emptyset)) + (v(\\{1,2\\}) - v(\\{2\\})) \\right] $$\n$$ \\phi_2 = \\frac{1}{2}\\left[ (v(\\{2\\}) - v(\\emptyset)) + (v(\\{1,2\\}) - v(\\{1\\})) \\right] $$\nHere, $v(S)$ is the value function for a coalition $S \\subseteq \\{1, 2\\}$, representing the expected model output given the feature values in $S$. We must compute $v(S)$ for all four coalitions: $\\emptyset$, $\\{1\\}$, $\\{2\\}$, and $\\{1,2\\}$. The instance to explain is $x = (x_1, x_2) = (0.2, 0.25)$. Let the model be denoted by $f(x_1, x_2)$.\n\n### 1. Calculation of Coalition Values $v(S)$\n\n- **$v(\\{1,2\\})$**: Both features are known. We pass the instance $x=(0.2, 0.25)$ through the tree.\n  - The root node splits on $X_1$. Since $x_1 = 0.2 > 0$, we proceed to the right child.\n  - The right child is a leaf node with value $v_R = 2.0$.\n  - Therefore, $v(\\{1,2\\}) = f(0.2, 0.25) = 2.0$.\n\n- **$v(\\{1\\})$**: Feature $X_1$ is known ($x_1=0.2$), and $X_2$ is unknown. Per the TreeSHAP semantics, we evaluate the model by averaging over the distribution of $X_2$.\n  - The root node splits on $X_1$. Since $x_1 = 0.2 > 0$, the path is fixed to the right child.\n  - The model output is $v_R = 2.0$, irrespective of the value of $X_2$. The split on $X_2$ is not on this path.\n  - Therefore, the expectation is trivial: $v(\\{1\\}) = E[f(0.2, X_2)] = 2.0$.\n\n- **$v(\\{2\\})$**: Feature $X_2$ is known ($x_2=0.25$), and $X_1$ is unknown. We average over the distribution of $X_1$.\n  - The root node splits on $X_1$, which is missing. According to the specified \"path-probability semantics\", we follow both branches, weighted by the marginal probabilities.\n  - Path 1 (left): With probability $P(X_1 \\le 0) = 0.5$, we go to the left subtree.\n  - Path 2 (right): With probability $P(X_1 > 0) = 0.5$, we go to the right leaf. The output is $v_R = 2.0$.\n  - In the left subtree, the model splits on $X_2$. We know $x_2=0.25$. Since $0.25 > 0$, we go to the right leaf of this subtree, which has value $v_{LR}=1.0$.\n  - Combining these paths, the expected output is:\n    $$ v(\\{2\\}) = P(X_1 \\le 0) \\cdot v_{LR} + P(X_1 > 0) \\cdot v_R = 0.5 \\cdot (1.0) + 0.5 \\cdot (2.0) = 0.5 + 1.0 = 1.5 $$\n  - Note that this value is independent of the correlation parameter $\\sigma$.\n\n- **$v(\\emptyset)$**: Both features are unknown. This is the baseline expected output of the model over the entire data distribution.\n  - $v(\\emptyset) = E[f(X_1, X_2)] = P(X_1 > 0) \\cdot v_R + P(X_1 \\le 0, X_2 \\le 0) \\cdot v_{LL} + P(X_1 \\le 0, X_2 > 0) \\cdot v_{LR}$.\n  - We have $P(X_1 > 0) = 0.5$. The leaf values are $v_R=2.0$, $v_{LL}=-1.0$, $v_{LR}=1.0$.\n  - We use the provided formula for the bivariate normal orthant probability: $P(X_1 \\le 0, X_2 \\le 0) = \\frac{1}{4} + \\frac{1}{2\\pi}\\arcsin(\\rho)$, where $\\rho = \\frac{1}{\\sqrt{1+\\sigma^2}}$.\n  - The probability for the other leaf is $P(X_1 \\le 0, X_2 > 0) = P(X_1 \\le 0) - P(X_1 \\le 0, X_2 \\le 0) = 0.5 - (\\frac{1}{4} + \\frac{\\arcsin(\\rho)}{2\\pi}) = \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi}$.\n  - Substituting these into the expectation formula:\n    $$ v(\\emptyset) = 0.5 \\cdot (2.0) + \\left(\\frac{1}{4} + \\frac{\\arcsin(\\rho)}{2\\pi}\\right)(-1.0) + \\left(\\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi}\\right)(1.0) $$\n    $$ v(\\emptyset) = 1.0 - \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi} + \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi} $$\n    $$ v(\\emptyset) = 1.0 - \\frac{2\\arcsin(\\rho)}{2\\pi} = 1.0 - \\frac{\\arcsin(\\rho)}{\\pi} $$\n\n### 2. Calculation of Shapley Values $\\phi_1$ and $\\phi_2$\n\nNow we substitute the coalition values into the Shapley formulas.\n- For $\\phi_1$:\n  - Marginal contribution of $\\{1\\}$ given $\\emptyset$: $v(\\{1\\}) - v(\\emptyset) = 2.0 - \\left(1.0 - \\frac{\\arcsin(\\rho)}{\\pi}\\right) = 1.0 + \\frac{\\arcsin(\\rho)}{\\pi}$.\n  - Marginal contribution of $\\{1\\}$ given $\\{2\\}$: $v(\\{1,2\\}) - v(\\{2\\}) = 2.0 - 1.5 = 0.5$.\n  - Average marginal contribution:\n    $$ \\phi_1 = \\frac{1}{2} \\left[ \\left(1.0 + \\frac{\\arcsin(\\rho)}{\\pi}\\right) + 0.5 \\right] = \\frac{1}{2} \\left[ 1.5 + \\frac{\\arcsin(\\rho)}{\\pi} \\right] = 0.75 + \\frac{\\arcsin(\\rho)}{2\\pi} $$\n\n- For $\\phi_2$:\n  - Marginal contribution of $\\{2\\}$ given $\\emptyset$: $v(\\{2\\}) - v(\\emptyset) = 1.5 - \\left(1.0 - \\frac{\\arcsin(\\rho)}{\\pi}\\right) = 0.5 + \\frac{\\arcsin(\\rho)}{\\pi}$.\n  - Marginal contribution of $\\{2\\}$ given $\\{1\\}$: $v(\\{1,2\\}) - v(\\{1\\}) = 2.0 - 2.0 = 0.0$.\n  - Average marginal contribution:\n    $$ \\phi_2 = \\frac{1}{2} \\left[ \\left(0.5 + \\frac{\\arcsin(\\rho)}{\\pi}\\right) + 0.0 \\right] = \\frac{1}{2} \\left[ 0.5 + \\frac{\\arcsin(\\rho)}{\\pi} \\right] = 0.25 + \\frac{\\arcsin(\\rho)}{2\\pi} $$\n\n### 3. Investigation of Collinearity\nThe problem asks to quantify the difference $\\phi_1 - \\phi_2$.\n$$ \\phi_1 - \\phi_2 = \\left(0.75 + \\frac{\\arcsin(\\rho)}{2\\pi}\\right) - \\left(0.25 + \\frac{\\arcsin(\\rho)}{2\\pi}\\right) = 0.5 $$\nThis result shows that for the given model structure and instance to be explained, the difference in feature attributions is constant and does not depend on the degree of collinearity $\\sigma$. Collinearity, represented by $\\rho=1/\\sqrt{1+\\sigma^2}$, does affect the baseline value $v(\\emptyset)$ and thus the individual SHAP values $\\phi_1$ and $\\phi_2$, but its effect is identical on both, leading to cancellation in their difference. The inequality of attributions ($\\phi_1 > \\phi_2$) stems from the tree structure and the specific feature values of the instance; for $x_1=0.2$, the model's output is decided by $X_1$ alone, making it more influential for this specific prediction.\n\nThe program will now be implemented to compute these values for the specified test suite of $\\sigma$ values.\n- For each $\\sigma$, we compute $\\rho = 1/\\sqrt{1+\\sigma^2}$. For $\\sigma=0$, $\\rho=1$.\n- We then compute $\\phi_1 = 0.75 + \\arcsin(\\rho)/(2\\pi)$ and $\\phi_2 = 0.25 + \\arcsin(\\rho)/(2\\pi)$.\n- The difference is fixed at $\\phi_1 - \\phi_2 = 0.5$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SHAP values for a two-feature decision tree model with collinear data.\n    \"\"\"\n\n    # Define the test cases for sigma from the problem statement.\n    test_cases_sigma = [0.0, 0.1, 0.5, 1.0, 3.0]\n\n    all_results = []\n    \n    for sigma in test_cases_sigma:\n        # Step 1: Calculate the correlation rho based on sigma.\n        # The relationship is rho = 1 / sqrt(1 + sigma^2).\n        if sigma == 0.0:\n            rho = 1.0\n        else:\n            rho = 1.0 / np.sqrt(1.0 + sigma**2)\n\n        # Step 2: Calculate arcsin(rho).\n        arcsin_rho = np.arcsin(rho)\n        \n        # Step 3: Compute phi_1 and phi_2 using the derived formulas.\n        # phi_1 = 0.75 + arcsin(rho) / (2 * pi)\n        # phi_2 = 0.25 + arcsin(rho) / (2 * pi)\n        phi_1 = 0.75 + arcsin_rho / (2.0 * np.pi)\n        phi_2 = 0.25 + arcsin_rho / (2.0 * np.pi)\n        \n        # Step 4: Compute the difference phi_1 - phi_2.\n        # As derived, this difference is a constant 0.5.\n        diff = phi_1 - phi_2\n        \n        # Store the results for this sigma value.\n        all_results.append([phi_1, phi_2, diff])\n\n    # Step 5: Format the output string as specified.\n    # The output should be a single line: [[a1,b1,c1],[a2,b2,c2],...]\n    # Each number must be rounded to 6 decimal places.\n    result_strings = []\n    for res_list in all_results:\n        # Format the numbers in the inner list as strings with 6 decimal places.\n        s_list = [f\"{val:.6f}\" for val in res_list]\n        # Join them into a string representation of a list: \"[v1,v2,v3]\"\n        result_strings.append(f\"[{','.join(s_list)}]\")\n    \n    # Join the string representations of inner lists with commas.\n    final_output_str = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3173371"}]}