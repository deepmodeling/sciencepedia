{"hands_on_practices": [{"introduction": "Preprocessing techniques like feature scaling are often seen as mere technical steps, but they are a powerful form of inductive bias. This exercise challenges you to explore this connection by analyzing how different scaling methods reshape the geometry of a linear model's weight space. By deriving the maximum achievable margin variance under different constraints, you will uncover how scaling implicitly directs the learning algorithm to favor certain solutions over others, providing a deep insight into the interplay between data transformation and model behavior. [@problem_id:3129964]", "problem": "Consider a statistical learning setup with a linear hypothesis space $\\mathcal{H} = \\{x \\mapsto w^{\\top} x : w \\in \\mathbb{R}^{d}\\}$ and an input random vector $x \\in \\mathbb{R}^{d}$ that is drawn from a zero-mean multivariate Gaussian distribution with covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. A feature scaling matrix $S \\in \\mathbb{R}^{d \\times d}$ is applied to inputs before learning, sending $x$ to $z = S x$, and the learning algorithm penalizes the squared Euclidean norm of the weight vector in the scaled space, so that feasible weights satisfy $\\|w_{z}\\|_{2} \\leq 1$ in the scaled coordinates. The corresponding weight in the original coordinates is $w = S^{\\top} w_{z}$.\n\nThe geometric margin on an input $x$ for a given $w$ is the scalar $m = w^{\\top} x$, and when $x$ is random with distribution $\\mathcal{N}(0, \\Sigma)$, the margin is a univariate Gaussian with variance $\\operatorname{Var}(m) = \\operatorname{Var}(w^{\\top} x)$. The inductive bias produced by feature scaling is reflected in which directions in $\\mathbb{R}^{d}$ are favored subject to the unit-norm constraint in the scaled space.\n\nYou will examine two concrete scalings in dimension $d = 2$:\n- Raw scaling $S_{\\mathrm{raw}} = I$.\n- Whitening scaling $S_{\\mathrm{white}} = \\Sigma^{-1/2}$.\n\nLet the covariance be\n$$\n\\Sigma = \\begin{pmatrix}\n4 & \\frac{6}{5} \\\\\n\\frac{6}{5} & 1\n\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the definitions above, express the feasible set for $w$ in original coordinates under a general scaling $S$ as a quadratic constraint of the form $w^{\\top} M w \\leq 1$, and identify $M$ in terms of $S$.\n2. Using only fundamental facts about variance of linear forms and constrained optimization, derive the expression for the maximum possible margin variance $\\max\\{\\operatorname{Var}(w^{\\top} x) : w^{\\top} M w = 1\\}$ in terms of $\\Sigma$ and $M$, and explain why this maximization characterizes how scaling implicitly weights directions.\n3. Specialize your result to $S_{\\mathrm{raw}}$ and $S_{\\mathrm{white}}$ for the given $\\Sigma$ and compute the ratio of the maximal achievable margin variances under raw scaling to whitening. Provide the final ratio as a single exact, closed-form analytic expression. Do not round and do not include any units in your final answer.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a standard problem in statistical learning theory requiring an application of linear algebra and constrained optimization. We proceed with the solution.\n\nThe problem asks for a three-part solution. We will address each part in sequence.\n\n### Part 1: Feasible Set for $w$ in Original Coordinates\n\nWe are given the constraint on the weight vector $w_z$ in the scaled space:\n$$\n\\|w_z\\|_2 \\leq 1\n$$\nThis is equivalent to the quadratic constraint:\n$$\nw_z^{\\top} w_z \\leq 1\n$$\nThe relationship between the weight vector $w$ in the original space and $w_z$ in the scaled space is given by:\n$$\nw = S^{\\top} w_z\n$$\nwhere $S$ is the feature scaling matrix. To express the constraint in terms of $w$, we must first express $w_z$ in terms of $w$. Assuming the scaling matrix $S$ is invertible (which is a necessary condition for a one-to-one change of coordinates), we can take the inverse of its transpose:\n$$\nw_z = (S^{\\top})^{-1} w = S^{-T} w\n$$\nSubstituting this expression for $w_z$ into the quadratic constraint yields:\n$$\n(S^{-T} w)^{\\top} (S^{-T} w) \\leq 1\n$$\nUsing the property of transposes $(AB)^{\\top} = B^{\\top}A^{\\top}$, we get:\n$$\nw^{\\top} (S^{-T})^{\\top} (S^{-T}) w \\leq 1\n$$\nSince the transpose of an inverse is the inverse of the transpose, $(A^{-T})^{\\top} = (A^{\\top})^{-T} = (A^{-1})^{T \\top} = A^{-1}$. Thus, $(S^{-T})^{\\top} = S^{-1}$. The inequality becomes:\n$$\nw^{\\top} S^{-1} S^{-T} w \\leq 1\n$$\nThis expression can be rewritten as:\n$$\nw^{\\top} (S^{\\top} S)^{-1} w \\leq 1\n$$\nThis is a quadratic constraint of the form $w^{\\top} M w \\leq 1$, where the matrix $M$ is defined as:\n$$\nM = (S^{\\top} S)^{-1}\n$$\nThe matrix $M$ is symmetric and positive definite, as it is the inverse of a Gram matrix $S^{\\top}S$ (for invertible $S$). The feasible set for $w$ is an ellipsoid defined by this inequality.\n\n### Part 2: Maximum Margin Variance\n\nThe margin is given by $m = w^{\\top}x$. The input vector $x$ is a random variable drawn from a zero-mean multivariate Gaussian distribution, $x \\sim \\mathcal{N}(0, \\Sigma)$. The variance of the margin $m$ is:\n$$\n\\operatorname{Var}(m) = \\operatorname{Var}(w^{\\top}x) = E[(w^{\\top}x - E[w^{\\top}x])^2]\n$$\nSince $E[x]=0$, we have $E[w^{\\top}x] = w^{\\top}E[x] = 0$. The variance simplifies to:\n$$\n\\operatorname{Var}(w^{\\top}x) = E[(w^{\\top}x)^2] = E[w^{\\top}x x^{\\top}w] = w^{\\top}E[x x^{\\top}]w\n$$\nBy the definition of the covariance matrix for a zero-mean random vector, $\\Sigma = E[xx^{\\top}]$. Therefore, the margin variance is:\n$$\n\\operatorname{Var}(w^{\\top}x) = w^{\\top}\\Sigma w\n$$\nWe want to find the maximum possible margin variance subject to the constraint on $w$. The maximum will occur on the boundary of the feasible set, i.e., where $w^{\\top} M w = 1$. The optimization problem is:\n$$\n\\max_{w \\in \\mathbb{R}^d} w^{\\top}\\Sigma w \\quad \\text{subject to} \\quad w^{\\top} M w = 1\n$$\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(w, \\lambda) = w^{\\top}\\Sigma w - \\lambda(w^{\\top} M w - 1)\n$$\nTo find the extrema, we set the gradient with respect to $w$ to zero. Noting that $\\Sigma$ and $M$ are symmetric matrices, we use the identity $\\nabla_v (v^{\\top} A v) = 2 A v$:\n$$\n\\nabla_w \\mathcal{L} = 2\\Sigma w - 2\\lambda M w = 0\n$$\n$$\n\\Sigma w = \\lambda M w\n$$\nThis is a generalized eigenvalue problem. To find the value of the objective function at an extremum, we left-multiply by $w^{\\top}$:\n$$\nw^{\\top}\\Sigma w = \\lambda w^{\\top} M w\n$$\nUsing the constraint $w^{\\top} M w = 1$, we find that the objective function value is equal to the generalized eigenvalue $\\lambda$:\n$$\nw^{\\top}\\Sigma w = \\lambda\n$$\nTo maximize the variance, we must choose the largest generalized eigenvalue, $\\lambda_{\\max}$, of the pair $(\\Sigma, M)$.\n$$\n\\max\\{\\operatorname{Var}(w^{\\top} x) : w^{\\top} M w = 1\\} = \\lambda_{\\max}(\\Sigma, M)\n$$\nThis maximization characterizes the inductive bias because the matrix $M$, determined by the scaling $S$, shapes the search space for the weight vector $w$. The algorithm implicitly favors directions $w$ (the generalized eigenvectors) where the data variance ($w^{\\top}\\Sigma w$) is large relative to the penalty ($w^{\\top} M w$). The maximum achievable variance corresponds to the direction most amplified by this trade-off.\n\n### Part 3: Ratio of Maximal Variances\n\nWe are given the covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix} 4 & \\frac{6}{5} \\\\ \\frac{6}{5} & 1 \\end{pmatrix}\n$$\n**Case 1: Raw scaling ($S_{\\mathrm{raw}} = I$)**\nFor $S = S_{\\mathrm{raw}} = I$, the identity matrix, the constraint matrix $M$ is:\n$$\nM_{\\mathrm{raw}} = (I^{\\top}I)^{-1} = I^{-1} = I\n$$\nThe generalized eigenvalue problem $\\Sigma w = \\lambda M_{\\mathrm{raw}} w$ becomes the standard eigenvalue problem $\\Sigma w = \\lambda w$. The maximum variance is the largest eigenvalue of $\\Sigma$, $\\lambda_{\\max}(\\Sigma)$. The eigenvalues are roots of the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} 4-\\lambda & \\frac{6}{5} \\\\ \\frac{6}{5} & 1-\\lambda \\end{pmatrix} = (4-\\lambda)(1-\\lambda) - \\left(\\frac{6}{5}\\right)^2 = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + 4 - \\frac{36}{25} = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + \\frac{100 - 36}{25} = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + \\frac{64}{25} = 0\n$$\nUsing the quadratic formula:\n$$\n\\lambda = \\frac{5 \\pm \\sqrt{(-5)^2 - 4(1)(\\frac{64}{25})}}{2} = \\frac{5 \\pm \\sqrt{25 - \\frac{256}{25}}}{2} = \\frac{5 \\pm \\sqrt{\\frac{625-256}{25}}}{2}\n$$\n$$\n\\lambda = \\frac{5 \\pm \\sqrt{\\frac{369}{25}}}{2} = \\frac{5 \\pm \\frac{\\sqrt{369}}{5}}{2}\n$$\nSince $369 = 9 \\times 41$, $\\sqrt{369} = 3\\sqrt{41}$.\n$$\n\\lambda = \\frac{5 \\pm \\frac{3\\sqrt{41}}{5}}{2} = \\frac{25 \\pm 3\\sqrt{41}}{10}\n$$\nThe maximum eigenvalue is the larger root:\n$$\n\\operatorname{Var}_{\\mathrm{raw}}^* = \\lambda_{\\max}(\\Sigma) = \\frac{25 + 3\\sqrt{41}}{10}\n$$\n\n**Case 2: Whitening scaling ($S_{\\mathrm{white}} = \\Sigma^{-1/2}$)**\nFor $S = S_{\\mathrm{white}} = \\Sigma^{-1/2}$, where $\\Sigma^{-1/2}$ is the symmetric positive definite square root of $\\Sigma^{-1}$, the constraint matrix $M$ is:\n$$\nM_{\\mathrm{white}} = (S_{\\mathrm{white}}^{\\top} S_{\\mathrm{white}})^{-1} = ((\\Sigma^{-1/2})^{\\top}\\Sigma^{-1/2})^{-1}\n$$\nSince $\\Sigma^{-1/2}$ is symmetric, $(\\Sigma^{-1/2})^{\\top} = \\Sigma^{-1/2}$.\n$$\nM_{\\mathrm{white}} = (\\Sigma^{-1/2}\\Sigma^{-1/2})^{-1} = (\\Sigma^{-1})^{-1} = \\Sigma\n$$\nThe generalized eigenvalue problem is $\\Sigma w = \\lambda M_{\\mathrm{white}} w$, which becomes:\n$$\n\\Sigma w = \\lambda \\Sigma w\n$$\nSince $\\Sigma$ is positive definite, it is invertible. We can left-multiply by $\\Sigma^{-1}$:\n$$\nw = \\lambda w\n$$\nThis must hold for a non-zero eigenvector $w$, which implies that $\\lambda = 1$. The only generalized eigenvalue is $1$. Therefore, the maximum variance is:\n$$\n\\operatorname{Var}_{\\mathrm{white}}^* = 1\n$$\nThis result is intuitive: the optimization problem is $\\max w^{\\top}\\Sigma w$ subject to $w^{\\top}\\Sigma w=1$, which trivially has a maximum value of $1$.\n\n**Ratio Calculation**\nThe ratio of the maximal achievable margin variances is:\n$$\n\\frac{\\operatorname{Var}_{\\mathrm{raw}}^*}{\\operatorname{Var}_{\\mathrm{white}}^*} = \\frac{\\frac{25 + 3\\sqrt{41}}{10}}{1} = \\frac{25 + 3\\sqrt{41}}{10}\n$$", "answer": "$$\\boxed{\\frac{25 + 3\\sqrt{41}}{10}}$$", "id": "3129964"}, {"introduction": "A model's inductive bias is not only about its complexity but also its fundamental structure. What happens when our chosen hypothesis space, such as the space of additive models, is structurally mismatched with the true underlying nature of the data? This practice problem guides you through a theoretical derivation to find the best possible additive approximation to a multiplicative reality, allowing you to precisely quantify the minimum error that this structural bias introduces. [@problem_id:3130015]", "problem": "Consider a supervised learning setting where inputs are two real-valued random variables $X$ and $Y$, and the target is a real-valued function $T(X,Y)$. The hypothesis space is restricted to separable additive models of the form $h(x,y)=f(x)+g(y)$ for measurable functions $f$ and $g$. This restriction constitutes an inductive bias towards additivity.\n\nAssume the following fundamental base:\n- The learner minimizes the expected squared loss $L(h)=\\mathbb{E}\\left[(T(X,Y)-h(X,Y))^{2}\\right]$ over the hypothesis space.\n- Conditional expectation $\\mathbb{E}[Z\\mid \\mathcal{G}]$ is the orthogonal projection of a square-integrable random variable $Z$ onto the subspace of $\\mathcal{G}$-measurable functions in the Hilbert space $L^{2}$ with inner product $\\langle U,V\\rangle=\\mathbb{E}[UV]$.\n- For independent random variables $X$ and $Y$, the expectation of a product of functions that depend on $X$ and $Y$ separately factorizes, that is, $\\mathbb{E}[\\phi(X)\\,\\psi(Y)]=\\mathbb{E}[\\phi(X)]\\,\\mathbb{E}[\\psi(Y)]$.\n\nIn this scenario, suppose the true data-generating mechanism is multiplicative, specifically $T(x,y)=\\theta\\,x\\,y$ where $\\theta\\in\\mathbb{R}$ is a fixed scalar. The joint distribution of $(X,Y)$ is such that $X$ and $Y$ are independent with known distributions. You must:\n1. Derive, from the provided base, the optimal additive predictor $h^{\\star}(x,y)$ that minimizes the expected squared loss $L(h)$ over all $h(x,y)=f(x)+g(y)$, and express the minimal achievable expected loss $L(h^{\\star})$ in terms of distributional characteristics of $X$ and $Y$ and the scalar $\\theta$.\n2. Implement a program that computes the minimal expected loss $L(h^{\\star})$ for each of the following independent test cases. In all cases, $X$ and $Y$ are independent. When a distribution is specified:\n   - For a Normal distribution, denoted $\\mathcal{N}(\\mu,\\sigma^{2})$, use the given mean and variance directly.\n   - For a Uniform distribution on $[a,b]$, the variance is $(b-a)^{2}/12$ and the mean is $(a+b)/2$.\n   - For a Constant distribution at value $c$, the variance is $0$ and the mean is $c$.\n   The test suite consists of five cases:\n   - Case $1$: $X\\sim\\mathcal{N}(\\mu_{x},\\sigma_{x}^{2})$ with $\\mu_{x}=1$ and $\\sigma_{x}^{2}=2$, $Y\\sim\\mathcal{N}(\\mu_{y},\\sigma_{y}^{2})$ with $\\mu_{y}=3$ and $\\sigma_{y}^{2}=4$, and $\\theta=2$.\n   - Case $2$: $X\\sim\\text{Uniform}[0,1]$, $Y\\sim\\text{Uniform}[0,1]$, and $\\theta=1$.\n   - Case $3$: $X$ is Constant at $5$, $Y\\sim\\mathcal{N}(\\mu_{y},\\sigma_{y}^{2})$ with $\\mu_{y}=0$ and $\\sigma_{y}^{2}=1$, and $\\theta=3$.\n   - Case $4$: $X\\sim\\mathcal{N}(\\mu_{x},\\sigma_{x}^{2})$ with $\\mu_{x}=-2$ and $\\sigma_{x}^{2}=10$, $Y\\sim\\mathcal{N}(\\mu_{y},\\sigma_{y}^{2})$ with $\\mu_{y}=4$ and $\\sigma_{y}^{2}=5$, and $\\theta=-1$.\n   - Case $5$: $X\\sim\\text{Uniform}[-1,1]$, $Y\\sim\\text{Uniform}[-10^{-6},10^{-6}]$, and $\\theta=10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry equal to the minimal expected squared loss $L(h^{\\star})$ for the corresponding case, in the order listed above. For example, your output must have the format $[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4},\\text{result}_{5}]$. No physical units are involved, and all numerical outputs must be real-valued floats.", "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, mathematically well-posed, objective, and self-contained. All necessary components, including the true data-generating process, hypothesis space, loss function, and distributional assumptions, are clearly defined.\n\nThe objective is to find the optimal separable additive model $h^{\\star}(x,y) = f(x) + g(y)$ that minimizes the expected squared loss $L(h) = \\mathbb{E}[(T(X,Y) - h(X,Y))^2]$, where the true model is $T(X,Y) = \\theta XY$ and the random variables $X$ and $Y$ are independent.\n\nThis minimization problem can be framed as finding the orthogonal projection of the random variable $T$ onto the subspace of additive functions in the Hilbert space $L^2$ of square-integrable random variables. The subspace of additive functions, $\\mathcal{H}_{add}$, consists of all functions of the form $h(X,Y) = f(X) + g(Y)$ for measurable functions $f$ and $g$ such that $\\mathbb{E}[f(X)^2] < \\infty$ and $\\mathbb{E}[g(Y)^2] < \\infty$.\n\nThe optimal predictor, $h^{\\star}$, is the unique element in $\\mathcal{H}_{add}$ such that the error vector, $T - h^{\\star}$, is orthogonal to the subspace $\\mathcal{H}_{add}$. This orthogonality condition is expressed as $\\mathbb{E}[(T - h^{\\star})h] = 0$ for all $h \\in \\mathcal{H}_{add}$. This condition must hold for any function $f_0(X) \\in \\mathcal{H}_{add}$ and any function $g_0(Y) \\in \\mathcal{H}_{add}$. This yields two necessary conditions:\n$1.$ $\\mathbb{E}[(T - f^{\\star}(X) - g^{\\star}(Y)) f_0(X)] = 0$ for all valid functions $f_0$.\n$2.$ $\\mathbb{E}[(T - f^{\\star}(X) - g^{\\star}(Y)) g_0(Y)] = 0$ for all valid functions $g_0$.\n\nUsing the law of total expectation on the first condition:\n$\\mathbb{E}[\\mathbb{E}[(T - f^{\\star}(X) - g^{\\star}(Y)) f_0(X) | X]] = 0$\nSince $f_0(X)$ and $f^{\\star}(X)$ are measurable with respect to the sigma-algebra generated by $X$, they can be treated as constants within the conditional expectation given $X$.\n$\\mathbb{E}[f_0(X) (\\mathbb{E}[T|X] - f^{\\star}(X) - \\mathbb{E}[g^{\\star}(Y)|X])] = 0$\nGiven that $X$ and $Y$ are independent, $\\mathbb{E}[g^{\\star}(Y)|X] = \\mathbb{E}[g^{\\star}(Y)]$. The equation becomes:\n$\\mathbb{E}[f_0(X) (\\mathbb{E}[T|X] - f^{\\star}(X) - \\mathbb{E}[g^{\\star}(Y)])] = 0$\nThis must hold for any choice of $f_0$, which implies that the term multiplying $f_0(X)$ must be zero almost surely. Thus, we obtain an expression for $f^{\\star}(X)$:\n$f^{\\star}(X) = \\mathbb{E}[T|X] - \\mathbb{E}[g^{\\star}(Y)]$\nBy a symmetric argument for the second condition, we find the expression for $g^{\\star}(Y)$:\n$g^{\\star}(Y) = \\mathbb{E}[T|Y] - \\mathbb{E}[f^{\\star}(X)]$\n\nWe now have a system of two equations. Let $c_f = \\mathbb{E}[f^{\\star}(X)]$ and $c_g = \\mathbb{E}[g^{\\star}(Y)]$. Taking the expectation of each equation, we get $c_f = \\mathbb{E}[T] - c_g$ and $c_g = \\mathbb{E}[T] - c_f$. Both equations are equivalent to $c_f + c_g = \\mathbb{E}[T]$. This indicates an identifiability issue for $f^{\\star}$ and $g^{\\star}$ individually (we can add a constant to one and subtract it from the other), but their sum, $h^{\\star}$, is unique. Substituting $c_g = \\mathbb{E}[T] - c_f$ into the equation for $f^{\\star}(X)$ is not helpful, but we can solve for the sum $h^{\\star}(X,Y) = f^{\\star}(X) + g^{\\star}(Y)$.\nSumming the two equations for $f^{\\star}(X)$ and $g^{\\star}(Y)$:\n$f^{\\star}(X) + g^{\\star}(Y) = \\mathbb{E}[T|X] + \\mathbb{E}[T|Y] - \\mathbb{E}[f^{\\star}(X)] - \\mathbb{E}[g^{\\star}(Y)]$\n$h^{\\star}(X,Y) = \\mathbb{E}[T|X] + \\mathbb{E}[T|Y] - (\\mathbb{E}[f^{\\star}(X)] + \\mathbb{E}[g^{\\star}(Y)])$\n$h^{\\star}(X,Y) = \\mathbb{E}[T|X] + \\mathbb{E}[T|Y] - \\mathbb{E}[T]$\nThis is the general form of the optimal additive predictor.\n\nNow, we apply this result to the specific true model $T(X,Y) = \\theta XY$. Let $\\mu_X = \\mathbb{E}[X]$ and $\\mu_Y = \\mathbb{E}[Y]$.\nThe required expectations are:\n$\\mathbb{E}[T] = \\mathbb{E}[\\theta XY] = \\theta \\mathbb{E}[X]\\mathbb{E}[Y] = \\theta\\mu_X\\mu_Y$ (due to independence of $X,Y$)\n$\\mathbb{E}[T|X] = \\mathbb{E}[\\theta XY | X] = \\theta X \\mathbb{E}[Y|X] = \\theta X \\mu_Y$ (due to independence)\n$\\mathbb{E}[T|Y] = \\mathbb{E}[\\theta XY | Y] = \\theta Y \\mathbb{E}[X|Y] = \\theta Y \\mu_X$ (due to independence)\n\nSubstituting these into the expression for $h^{\\star}$:\n$h^{\\star}(X,Y) = \\theta X\\mu_Y + \\theta Y\\mu_X - \\theta\\mu_X\\mu_Y$\n\nWith the optimal predictor $h^{\\star}$, we can find the minimal achievable expected loss, $L(h^{\\star})$.\n$L(h^{\\star}) = \\mathbb{E}[(T - h^{\\star})^2]$\nThe prediction error is:\n$T - h^{\\star} = \\theta XY - (\\theta X\\mu_Y + \\theta Y\\mu_X - \\theta\\mu_X\\mu_Y)$\n$T - h^{\\star} = \\theta (XY - X\\mu_Y - Y\\mu_X + \\mu_X\\mu_Y)$\nThis expression can be factored:\n$T - h^{\\star} = \\theta (X(Y - \\mu_Y) - \\mu_X(Y - \\mu_Y)) = \\theta (X - \\mu_X)(Y - \\mu_Y)$\n\nNow we compute the expected squared error:\n$L(h^{\\star}) = \\mathbb{E}[(\\theta(X - \\mu_X)(Y - \\mu_Y))^2] = \\theta^2 \\mathbb{E}[(X - \\mu_X)^2(Y - \\mu_Y)^2]$\nSince $X$ and $Y$ are independent, any functions of them, such as $(X-\\mu_X)^2$ and $(Y-\\mu_Y)^2$, are also independent. Therefore, the expectation of the product is the product of the expectations:\n$L(h^{\\star}) = \\theta^2 \\mathbb{E}[(X - \\mu_X)^2] \\mathbb{E}[(Y - \\mu_Y)^2]$\nBy definition, the variance of a random variable $Z$ is $\\text{Var}(Z) = \\sigma_Z^2 = \\mathbb{E}[(Z - \\mu_Z)^2]$.\nThus, the minimal expected loss is:\n$$L(h^{\\star}) = \\theta^2 \\sigma_X^2 \\sigma_Y^2$$\nThis final expression gives the minimal achievable expected loss in terms of the scalar $\\theta$ and the variances of $X$ and $Y$, which are key distributional characteristics. This formula will be used to compute the results for the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimal expected squared loss for an additive model\n    approximating a multiplicative true model.\n\n    The minimal loss L(h*) for a true model T(X,Y) = theta * X * Y and an\n    additive hypothesis h(x,y) = f(x) + g(y) is given by the formula:\n    L(h*) = theta^2 * Var(X) * Var(Y)\n    where X and Y are independent random variables.\n    \"\"\"\n\n    # Test cases defined as tuples of (theta, var_x, var_y)\n    test_cases = [\n        # Case 1: X~N(1, 2), Y~N(3, 4), theta=2\n        # var_x = 2, var_y = 4\n        (2.0, 2.0, 4.0),\n\n        # Case 2: X~Uniform[0,1], Y~Uniform[0,1], theta=1\n        # a=0, b=1. var = (b-a)^2 / 12 = 1/12\n        (1.0, (1.0 - 0.0)**2 / 12.0, (1.0 - 0.0)**2 / 12.0),\n\n        # Case 3: X is Constant at 5, Y~N(0, 1), theta=3\n        # var_x = 0, var_y = 1\n        (3.0, 0.0, 1.0),\n\n        # Case 4: X~N(-2, 10), Y~N(4, 5), theta=-1\n        # var_x = 10, var_y = 5\n        (-1.0, 10.0, 5.0),\n\n        # Case 5: X~Uniform[-1,1], Y~Uniform[-1e-6, 1e-6], theta=10\n        # For X: a=-1, b=1. var_x = (1 - (-1))^2 / 12 = 4/12 = 1/3\n        # For Y: a=-1e-6, b=1e-6. var_y = (1e-6 - (-1e-6))^2 / 12 = (2e-6)^2 / 12\n        (10.0, (1.0 - (-1.0))**2 / 12.0, (1e-6 - (-1e-6))**2 / 12.0),\n    ]\n\n    results = []\n    for theta, var_x, var_y in test_cases:\n        # Calculate the minimal expected loss using the derived formula\n        min_loss = theta**2 * var_x * var_y\n        results.append(min_loss)\n\n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3130015"}, {"introduction": "The choice of a hypothesis space is a delicate balancing act, governed by the bias-variance tradeoff. A simple model may have high bias but low variance, while a complex model has the opposite profile. This hands-on simulation allows you to explore this tradeoff empirically by generating learning curves for two nested models, one of which is correctly specified but more complex. By observing at which sample size the more complex model's performance overtakes the simpler one, you will gain a practical understanding of why an inductive bias towards simplicity is often advantageous, especially when data is scarce. [@problem_id:3129988]", "problem": "You are tasked with designing and implementing a principled simulation to compare two hypothesis spaces in a supervised learning setting under limited sample sizes, and to analyze the bias-variance tradeoff via learning curves. The setting is as follows. Let the input be $\\mathbf{x} = (x_1, x_2)$ with $x_1$ and $x_2$ drawn independently and identically distributed (i.i.d.) from the uniform distribution on $[-1,1]$. The ground-truth regression function is\n$$\nf^\\star(\\mathbf{x}) \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_1 \\;+\\; \\beta_2 x_2 \\;+\\; \\beta_{12} x_1 x_2,\n$$\nand the observed output is\n$$\ny \\;=\\; f^\\star(\\mathbf{x}) \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise with variance $\\sigma^2$. Consider the following two hypothesis spaces (sets of functions) for linear least squares:\n- The additive hypothesis space without interaction terms,\n$$\n\\mathcal{H}_{\\text{add}} \\;=\\; \\left\\{ f(\\mathbf{x}) \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_1 \\;+\\; \\theta_2 x_2 \\right\\},\n$$\n- The interaction hypothesis space with an interaction term,\n$$\n\\mathcal{H}_{\\text{int}} \\;=\\; \\left\\{ f(\\mathbf{x}) \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_1 \\;+\\; \\theta_2 x_2 \\;+\\; \\theta_{12} x_1 x_2 \\right\\}.\n$$\nYour program must simulate learning curves and decompose error into bias and variance as functions of the training sample size $n \\in \\{8,32,128\\}$. For each fixed $n$, repeat training across $R$ independent replicates with $R=200$, each replicate using $n$ i.i.d. samples of $\\mathbf{x}$ and $y$ from the above data-generating process. For evaluation, use a fixed test set of size $M=512$ consisting of i.i.d. $\\mathbf{x}$ from the same distribution, and evaluate expected generalization error via the bias-variance decomposition.\n\nStart from the core definitions: for a learned predictor $\\hat{f}$, the expected squared prediction error at a fixed $\\mathbf{x}$ under squared loss is\n$$\n\\mathbb{E}\\!\\left[(\\hat{f}(\\mathbf{x}) - y)^2 \\mid \\mathbf{x}\\right] \\;=\\; \\left(\\mathbb{E}[\\hat{f}(\\mathbf{x})] - f^\\star(\\mathbf{x})\\right)^2 \\;+\\; \\mathrm{Var}(\\hat{f}(\\mathbf{x})) \\;+\\; \\sigma^2,\n$$\nwhere the expectation and variance are over the random training sample (and any algorithmic randomness). The learning curve at sample size $n$ is the average of this quantity over the test distribution of $\\mathbf{x}$. In your simulation, approximate these expectations by Monte Carlo averages across the $R$ replicates and across the $M$ test inputs.\n\nFor each test case, compute the learning curves for both $\\mathcal{H}_{\\text{add}}$ and $\\mathcal{H}_{\\text{int}}$, and then report the smallest training size $n$ (from the given set) at which the interaction hypothesis space’s generalization error is less than or equal to the additive hypothesis space’s generalization error. If no such crossing occurs for the given $n$ values, report $0$ for that test case.\n\nUse the following test suite of parameter values, where all parameters not explicitly listed are fixed to $\\beta_0 = 0$, $\\beta_1 = 1$, and $\\beta_2 = 1$:\n- Test case $1$: $\\beta_{12} = 0.8$, $\\sigma^2 = 0.05$,\n- Test case $2$: $\\beta_{12} = 0.8$, $\\sigma^2 = 1.0$,\n- Test case $3$: $\\beta_{12} = 0.0$, $\\sigma^2 = 0.05$.\n\nYour program must:\n- For each test case, simulate and estimate the learning curves at $n \\in \\{8,32,128\\}$ for both hypothesis spaces,\n- Use $R=200$ training replicates and a fixed test set size $M=512$ for the Monte Carlo approximations,\n- For each test case, return a single integer: the smallest $n$ at which the interaction space’s estimated generalization error is less than or equal to the additive space’s estimated generalization error, or $0$ if no crossing occurs.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly one integer per test case, in the order listed above.", "solution": "The problem requires a numerical simulation to compare the generalization performance of two nested linear models: an additive model, $\\mathcal{H}_{\\text{add}}$, and a model with an interaction term, $\\mathcal{H}_{\\text{int}}$. The comparison is framed within the context of the bias-variance tradeoff, and performance is evaluated as a function of the training sample size, $n$. The goal is to determine, for different data-generating parameters, the minimum sample size at which the more complex model, $\\mathcal{H}_{\\text{int}}$, achieves a generalization error less than or equal to that of the simpler model, $\\mathcal{H}_{\\text{add}}$.\n\nThe core of the solution is a Monte Carlo simulation designed to estimate the expected generalization error. The expected squared prediction error for a learned model $\\hat{f}$ at a specific input $\\mathbf{x}$ is decomposed into three components:\n$$\n\\mathbb{E}_{\\mathcal{D}, \\varepsilon}\\left[(\\hat{f}(\\mathbf{x}) - y)^2 \\mid \\mathbf{x}\\right] \\;=\\; \\underbrace{\\left(\\mathbb{E}_{\\mathcal{D}}[\\hat{f}(\\mathbf{x})] - f^\\star(\\mathbf{x})\\right)^2}_{\\text{Squared Bias}} \\;+\\; \\underbrace{\\mathbb{E}_{\\mathcal{D}}\\left[\\left(\\hat{f}(\\mathbf{x}) - \\mathbb{E}_{\\mathcal{D}}[\\hat{f}(\\mathbf{x})]\\right)^2\\right]}_{\\text{Variance}} \\;+\\; \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n$$\nHere, the expectation $\\mathbb{E}_{\\mathcal{D}}$ is taken over the distribution of training datasets $\\mathcal{D}$ of a fixed size $n$. The total generalization error is the expectation of this quantity over the distribution of test inputs $\\mathbf{x}$. Our simulation approximates these expectations by averaging over a large number of draws.\n\nThe simulation procedure is as follows:\n\nFirst, we establish a fixed test set to ensure a consistent evaluation benchmark. A set of $M=512$ input vectors $\\{\\mathbf{x}^{(i)}\\}_{i=1}^M$ is generated, where each $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)})$ has components drawn independently from $\\mathcal{U}[-1, 1]$. For these test points, we compute the true, noise-free function values $f^\\star(\\mathbf{x}^{(i)})$ using the ground-truth function $f^\\star(\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2$ with the parameters specified for each test case.\n\nNext, for each training size $n \\in \\{8, 32, 128\\}$, we conduct $R=200$ independent training replicates. In each replicate $r \\in \\{1, \\dots, R\\}$:\n1.  A new training dataset $\\mathcal{D}_r$ of size $n$ is generated. The inputs $\\{\\mathbf{x}_j\\}_{j=1}^n$ are drawn i.i.d. from $\\mathcal{U}[-1, 1] \\times \\mathcal{U}[-1, 1]$. The corresponding outputs $\\{y_j\\}_{j=1}^n$ are computed as $y_j = f^\\star(\\mathbf{x}_j) + \\varepsilon_j$, where $\\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$.\n2.  Two models are fit to the training data $\\mathcal{D}_r$: one from $\\mathcal{H}_{\\text{add}}$ and one from $\\mathcal{H}_{\\text{int}}$. This is done using ordinary least squares, which finds the parameter vector $\\hat{\\theta}$ that minimizes the sum of squared residuals. For a model with design matrix $X$ and target vector $y$, the solution is $\\hat{\\theta} = (X^T X)^{-1} X^T y$. Numerically, this is best computed using a stable method such as Singular Value Decomposition, as implemented in `numpy.linalg.lstsq`.\n    -   For $\\mathcal{H}_{\\text{add}}$, the design matrix has columns corresponding to an intercept, $x_1$, and $x_2$.\n    -   For $\\mathcal{H}_{\\text{int}}$, the design matrix has an additional column for the interaction term $x_1 x_2$.\n3.  The two fitted models, let's call them $\\hat{f}_{\\text{add}, r}$ and $\\hat{f}_{\\text{int}, r}$, are then used to make predictions on the $M$ fixed test points. This yields two prediction vectors, $\\hat{y}_{\\text{add}, r}$ and $\\hat{y}_{\\text{int}, r}$, which are stored.\n\nAfter completing all $R=200$ replicates for a given $n$, we have $R$ predictions for each of the $M$ test points for both hypothesis spaces. We can now approximate the bias and variance terms. For each test point $\\mathbf{x}^{(i)}$ and for each model class (e.g., 'add'), we perform the following calculations:\n-   The average prediction across replicates, $\\bar{f}_{\\text{add}}(\\mathbf{x}^{(i)}) = \\frac{1}{R} \\sum_{r=1}^R \\hat{f}_{\\text{add}, r}(\\mathbf{x}^{(i)})$, approximates $\\mathbb{E}_{\\mathcal{D}}[\\hat{f}_{\\text{add}}(\\mathbf{x}^{(i)})]$.\n-   The squared bias at $\\mathbf{x}^{(i)}$ is estimated as $(\\bar{f}_{\\text{add}}(\\mathbf{x}^{(i)}) - f^\\star(\\mathbf{x}^{(i)}))^2$.\n-   The variance at $\\mathbf{x}^{(i)}$ is estimated as $\\frac{1}{R} \\sum_{r=1}^R (\\hat{f}_{\\text{add}, r}(\\mathbf{x}^{(i)}) - \\bar{f}_{\\text{add}}(\\mathbf{x}^{(i)}))^2$.\n\nThe total generalization error for the model is then the sum of three components: the average of the estimated squared biases over the $M$ test points, the average of the estimated variances over the $M$ test points, and the irreducible error $\\sigma^2$.\n\nThis entire process is repeated for each training size $n$. Finally, by comparing the computed generalization errors for $\\mathcal{H}_{\\text{add}}$ and $\\mathcal{H}_{\\text{int}}$ at each $n$, we identify the smallest $n$ for which $\\text{Error}(\\mathcal{H}_{\\text{int}}) \\le \\text{Error}(\\mathcal{H}_{\\text{add}})$. If this condition is not met for any of the specified values of $n$, the result for that test case is $0$. The procedure is applied to each of the three test cases, which vary the ground-truth parameter $\\beta_{12}$ and the noise variance $\\sigma^2$ to illustrate different scenarios of the bias-variance tradeoff.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (beta_12, sigma_sq)\n        (0.8, 0.05),\n        (0.8, 1.0),\n        (0.0, 0.05),\n    ]\n\n    # Fixed parameters from problem statement\n    base_params = {'beta0': 0.0, 'beta1': 1.0, 'beta2': 1.0}\n    n_values = [8, 32, 128]\n    R = 200\n    M = 512\n\n    results = []\n    for beta_12, sigma_sq in test_cases:\n        params = base_params.copy()\n        params['beta_12'] = beta_12\n        params['sigma_sq'] = sigma_sq\n        \n        crossover_n = simulate_learning_curves(params, n_values, R, M)\n        results.append(crossover_n)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_learning_curves(params, n_values, R, M):\n    \"\"\"\n    Simulates learning curves for one set of parameters.\n\n    Args:\n        params (dict): Dictionary of parameters (beta0, beta1, beta2, beta_12, sigma_sq).\n        n_values (list): List of training sample sizes to test.\n        R (int): Number of training replicates.\n        M (int): Size of the fixed test set.\n\n    Returns:\n        int: The smallest n at which the interaction model's error is less than\n             or equal to the additive model's error, or 0 if no crossover occurs.\n    \"\"\"\n    # Use a fixed seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    beta_star = np.array([params['beta0'], params['beta1'], params['beta2'], params['beta_12']])\n    sigma_sq = params['sigma_sq']\n    sigma = np.sqrt(sigma_sq)\n\n    # 1. Generate a fixed test set\n    x_test_raw = np.random.uniform(-1, 1, size=(M, 2))\n    x1_test, x2_test = x_test_raw[:, 0], x_test_raw[:, 1]\n    \n    # Design matrices for the test set\n    X_test_add = np.c_[np.ones(M), x1_test, x2_test]\n    X_test_int = np.c_[X_test_add, x1_test * x2_test]\n    \n    # True function values on the test set\n    f_star_test = X_test_int @ beta_star\n\n    errors = {n: {} for n in n_values}\n\n    # 2. Iterate through training sizes\n    for n in n_values:\n        # Storage for predictions from all R replicates for the current n\n        preds_add = np.zeros((M, R))\n        preds_int = np.zeros((M, R))\n\n        # 3. Loop over R replicates\n        for r in range(R):\n            # a. Generate training data\n            x_train_raw = np.random.uniform(-1, 1, size=(n, 2))\n            x1_train, x2_train = x_train_raw[:, 0], x_train_raw[:, 1]\n            \n            X_train_add = np.c_[np.ones(n), x1_train, x2_train]\n            X_train_int = np.c_[X_train_add, x1_train * x2_train]\n\n            f_star_train = X_train_int @ beta_star\n            noise = np.random.normal(0, sigma, size=n)\n            y_train = f_star_train + noise\n            \n            # b. Fit models using linear least squares\n            theta_add, _, _, _ = np.linalg.lstsq(X_train_add, y_train, rcond=None)\n            theta_int, _, _, _ = np.linalg.lstsq(X_train_int, y_train, rcond=None)\n\n            # c. Make and store predictions on the test set\n            preds_add[:, r] = X_test_add @ theta_add\n            preds_int[:, r] = X_test_int @ theta_int\n\n        # 4. Calculate bias-squared, variance, and total generalization error\n        # For the additive model\n        E_f_hat_add = np.mean(preds_add, axis=1)\n        bias_sq_add = (E_f_hat_add - f_star_test)**2\n        var_add = np.var(preds_add, axis=1) # ddof=0 is default, correct for population var over replicates\n        total_err_add = np.mean(bias_sq_add) + np.mean(var_add) + sigma_sq\n        errors[n]['add'] = total_err_add\n        \n        # For the interaction model\n        E_f_hat_int = np.mean(preds_int, axis=1)\n        bias_sq_int = (E_f_hat_int - f_star_test)**2\n        var_int = np.var(preds_int, axis=1)\n        total_err_int = np.mean(bias_sq_int) + np.mean(var_int) + sigma_sq\n        errors[n]['int'] = total_err_int\n\n    # 5. Find the crossover point\n    for n in sorted(n_values):\n        if errors[n]['int'] = errors[n]['add']:\n            return n\n    \n    return 0\n\nsolve()\n```", "id": "3129988"}]}