{"hands_on_practices": [{"introduction": "To begin our practical exploration of the No Free Lunch (NFL) theorems, we first establish a foundational theoretical result. This exercise challenges you to prove that for any fixed scoring function, its expected performance, as measured by the Area Under the ROC Curve (AUC), is no better than random guessing when the labels are assigned randomly. By deriving this result, you will gain a rigorous understanding of how the absence of learnable patterns neutralizes the advantage of any specific algorithm [@problem_id:3153400].", "problem": "Consider a finite dataset $\\mathcal{D} = \\{x_{1}, x_{2}, \\dots, x_{N}\\}$ and a fixed scoring function $s:\\mathcal{X}\\to\\mathbb{R}$. The score $s(x_{i})$ induces a total preorder on $\\mathcal{D}$, where ties are permitted. Define the binary label vector $\\mathbf{y}\\in\\{0,1\\}^{N}$, with $y_{i}=1$ for positives and $y_{i}=0$ for negatives. Let the number of positives be $k$ and assume $1 \\leq k \\leq N-1$ so that both classes are present.\n\nArea Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC, abbreviated AUC) for the scoring function $s$ evaluated on $\\mathcal{D}$ and labels $\\mathbf{y}$ is defined by the pairwise ranking formulation\n$$\n\\mathrm{AUC}(s,\\mathbf{y}) \\;=\\; \\frac{1}{k\\,(N-k)} \\sum_{i:\\,y_{i}=1}\\;\\sum_{j:\\,y_{j}=0}\\Big( \\mathbb{I}\\big(s(x_{i}) > s(x_{j})\\big) \\;+\\; \\frac{1}{2}\\,\\mathbb{I}\\big(s(x_{i}) = s(x_{j})\\big) \\Big),\n$$\nwhere $\\mathbb{I}(\\cdot)$ denotes the indicator function.\n\nUnder the No Free Lunch (NFL) assumption for statistical learning, consider that the labeling $\\mathbf{y}$ is drawn uniformly at random from all $\\binom{N}{k}$ binary labelings with exactly $k$ positives (equivalently, the positive set is a uniformly random $k$-subset of $\\{1,\\dots,N\\}$), independently of the scoring function $s$.\n\nStarting only from the above definitions and assumptions, derive the expected value of $\\mathrm{AUC}(s,\\mathbf{y})$ over the random labeling model. Express your final answer as a single real number. If you need to account for ties in the scores $s(x_{i})$, use the $1/2$ tie convention already specified. No rounding is necessary.", "solution": "The problem asks for the expected value of the Area Under the ROC Curve (AUC) for a fixed scoring function $s$ under the assumption that the class labels $\\mathbf{y}$ are assigned randomly. Specifically, the label vector $\\mathbf{y} \\in \\{0,1\\}^{N}$ is drawn uniformly from the set of all $\\binom{N}{k}$ possible vectors containing exactly $k$ ones, where $1 \\leq k \\leq N-1$.\n\nThe quantity to be computed is the expectation $\\mathbb{E}_{\\mathbf{y}}[\\mathrm{AUC}(s,\\mathbf{y})]$. We start with the given definition of AUC:\n$$\n\\mathrm{AUC}(s,\\mathbf{y}) = \\frac{1}{k(N-k)} \\sum_{i: y_i=1} \\sum_{j: y_j=0} \\left( \\mathbb{I}(s(x_i) > s(x_j)) + \\frac{1}{2} \\mathbb{I}(s(x_i) = s(x_j)) \\right)\n$$\nThe summation is over pairs of indices $(i,j)$ where item $i$ is positive ($y_i=1$) and item $j$ is negative ($y_j=0$). To facilitate taking the expectation, we can rewrite the sum over all possible pairs of indices $(i,j)$ from $\\{1, \\dots, N\\}$ by introducing indicator variables for the labels. The condition $y_i=1$ and $y_j=0$ can be written as $y_i(1-y_j)=1$.\n$$\n\\mathrm{AUC}(s,\\mathbf{y}) = \\frac{1}{k(N-k)} \\sum_{i=1}^{N} \\sum_{j=1}^{N} y_i(1-y_j) \\left( \\mathbb{I}(s(x_i) > s(x_j)) + \\frac{1}{2} \\mathbb{I}(s(x_i) = s(x_j)) \\right)\n$$\nNote that if $i=j$, the term $y_i(1-y_i)$ is always $0$, so these terms do not contribute to the sum, which is consistent with the original definition where a positive and a negative example must be distinct items.\n\nLet's define the constant term for a given pair $(i, j)$ as $C_{ij} = \\mathbb{I}(s(x_i) > s(x_j)) + \\frac{1}{2} \\mathbb{I}(s(x_i) = s(x_j))$. The scoring function $s$ and the dataset $\\mathcal{D}$ are fixed, so $C_{ij}$ are constants with respect to the random variable $\\mathbf{y}$.\nThe expression for AUC becomes:\n$$\n\\mathrm{AUC}(s,\\mathbf{y}) = \\frac{1}{k(N-k)} \\sum_{i=1}^{N} \\sum_{j=1}^{N} y_i(1-y_j) C_{ij}\n$$\nNow we take the expectation over the random variable $\\mathbf{y}$. By the linearity of expectation, we can move the expectation operator inside the summations:\n$$\n\\mathbb{E}_{\\mathbf{y}}[\\mathrm{AUC}(s,\\mathbf{y})] = \\frac{1}{k(N-k)} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathbb{E}_{\\mathbf{y}}[y_i(1-y_j)] C_{ij}\n$$\nThe next step is to compute the expectation $\\mathbb{E}_{\\mathbf{y}}[y_i(1-y_j)]$.\nFor $i=j$, we have $\\mathbb{E}_{\\mathbf{y}}[y_i(1-y_i)] = \\mathbb{E}_{\\mathbf{y}}[0] = 0$.\nFor $i \\neq j$, $y_i(1-y_j)$ is an indicator variable for the event that $y_i=1$ and $y_j=0$. Thus, its expectation is the probability of this event:\n$$\n\\mathbb{E}_{\\mathbf{y}}[y_i(1-y_j)] = P(y_i=1 \\text{ and } y_j=0)\n$$\nThe total number of ways to assign $k$ positive labels to $N$ items is $\\binom{N}{k}$. To count the number of assignments where $y_i=1$ and $y_j=0$, we fix the labels for items $i$ and $j$. Then we must choose the remaining $k-1$ positive labels from the other $N-2$ items. The number of ways to do this is $\\binom{N-2}{k-1}$.\nTherefore, for $i \\neq j$, the probability is:\n$$\nP(y_i=1, y_j=0) = \\frac{\\binom{N-2}{k-1}}{\\binom{N}{k}} = \\frac{\\frac{(N-2)!}{(k-1)!(N-2-(k-1))!}}{\\frac{N!}{k!(N-k)!}} = \\frac{(N-2)!}{(k-1)!(N-k-1)!} \\cdot \\frac{k!(N-k)!}{N!}\n$$\n$$\n= \\frac{(N-2)!}{(k-1)!(N-k-1)!} \\cdot \\frac{k(k-1)! (N-k)(N-k-1)!}{N(N-1)(N-2)!} = \\frac{k(N-k)}{N(N-1)}\n$$\nSo, for any pair of distinct indices $(i, j)$, we have $\\mathbb{E}_{\\mathbf{y}}[y_i(1-y_j)] = \\frac{k(N-k)}{N(N-1)}$. This value is constant for all pairs $(i,j)$ where $i \\neq j$.\n\nSubstituting this result back into the expression for the expected AUC:\n$$\n\\mathbb{E}_{\\mathbf{y}}[\\mathrm{AUC}(s,\\mathbf{y})] = \\frac{1}{k(N-k)} \\sum_{i \\neq j} C_{ij} \\left( \\frac{k(N-k)}{N(N-1)} \\right)\n$$\nThe factor $\\frac{k(N-k)}{k(N-k)}$ cancels out, simplifying the expression to:\n$$\n\\mathbb{E}_{\\mathbf{y}}[\\mathrm{AUC}(s,\\mathbf{y})] = \\frac{1}{N(N-1)} \\sum_{i \\neq j} C_{ij}\n$$\nThe remaining task is to evaluate the sum $\\sum_{i \\neq j} C_{ij}$. Let's consider the sum of terms for a pair of symmetric indices $(i,j)$ and $(j,i)$, where $i \\neq j$:\n$$\nC_{ij} + C_{ji} = \\left( \\mathbb{I}(s(x_i) > s(x_j)) + \\frac{1}{2} \\mathbb{I}(s(x_i) = s(x_j)) \\right) + \\left( \\mathbb{I}(s(x_j) > s(x_i)) + \\frac{1}{2} \\mathbb{I}(s(x_j) = s(x_i)) \\right)\n$$\nSince $\\mathbb{I}(s(x_i) = s(x_j)) = \\mathbb{I}(s(x_j) = s(x_i))$, we can combine terms:\n$$\nC_{ij} + C_{ji} = \\mathbb{I}(s(x_i) > s(x_j)) + \\mathbb{I}(s(x_j) > s(x_i)) + \\mathbb{I}(s(x_i) = s(x_j))\n$$\nFor any two real numbers, such as the scores $s(x_i)$ and $s(x_j)$, exactly one of three relations must hold: $s(x_i) > s(x_j)$, $s(x_j) > s(x_i)$, or $s(x_i) = s(x_j)$. Therefore, the sum of the three indicator functions is always equal to $1$.\n$$\nC_{ij} + C_{ji} = 1\n$$\nThe total sum $\\sum_{i \\neq j} C_{ij}$ contains $N(N-1)$ terms, corresponding to all ordered pairs of distinct indices. We can group these terms into $\\binom{N}{2} = \\frac{N(N-1)}{2}$ unordered pairs $\\{i,j\\}$.\n$$\n\\sum_{i \\neq j} C_{ij} = \\sum_{1 \\leq i < j \\leq N} (C_{ij} + C_{ji}) = \\sum_{1 \\leq i < j \\leq N} 1\n$$\nThe number of terms in this sum is the number of unordered pairs of distinct indices, which is $\\binom{N}{2}$.\n$$\n\\sum_{i \\neq j} C_{ij} = \\binom{N}{2} = \\frac{N(N-1)}{2}\n$$\nFinally, we substitute this result back into our expression for the expected AUC:\n$$\n\\mathbb{E}_{\\mathbf{y}}[\\mathrm{AUC}(s,\\mathbf{y})] = \\frac{1}{N(N-1)} \\left( \\frac{N(N-1)}{2} \\right) = \\frac{1}{2}\n$$\nThe expected value of the AUC is exactly $\\frac{1}{2}$, regardless of the dataset $\\mathcal{D}$, the scoring function $s$, the size of the dataset $N$, or the number of positive examples $k$. This result is a manifestation of the \"No Free Lunch\" theorem in a finite sample setting, indicating that a fixed scorer has no predictive power on average over a uniform distribution of labeling tasks.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3153400"}, {"introduction": "The NFL theorem's core claim is that, averaged over all possible learning tasks, no algorithm is universally superior. While a full theoretical proof is abstract, we can verify this principle empirically. In this practice, you will implement two very different learning algorithms and test them across a universe of randomly generated tasks, demonstrating through statistical analysis that their average performance is indistinguishable [@problem_id:3153410]. This computational experiment transforms the theorem from an abstract statement into a tangible, testable hypothesis.", "problem": "You are given two supervised learning algorithms to compare under the framework of No Free Lunch theorems in statistical learning. The input domain is the finite set of all binary vectors of fixed length. Your task is to design synthetic tasks by sampling target functions uniformly over all possible labelings and to implement a paired hypothesis test that verifies whether the mean performance difference between the two learners is not significantly different from zero.\n\nFundamental base to use:\n- No Free Lunch (NFL) principle for supervised learning on finite domains: under a uniform distribution over target functions on a finite domain and zero-one loss, the expected generalization performance on unseen points is equal across all learning algorithms.\n- Definitions of risk and accuracy: for a hypothesis $h$ and target function $f$ with zero-one loss on a finite set $U$, the accuracy is $\\frac{1}{|U|}\\sum_{x\\in U}\\mathbf{1}\\{h(x)=f(x)\\}$.\n- Classical hypothesis testing with the paired Student's $t$-test: given paired differences $\\{d_t\\}_{t=1}^k$ assumed independent and approximately normally distributed with mean $\\mu_d$ and finite variance, we test the null hypothesis $H_0:\\mu_d=0$ with the test statistic $T=\\bar{d}/(s_d/\\sqrt{k})$ and degrees of freedom $k-1$, where $\\bar{d}$ is the sample mean and $s_d$ is the sample standard deviation with Bessel's correction.\n\nSet-up and precise definitions for the experiment:\n- Input space: for a positive integer $m$, define $X=\\{0,1\\}^m$, represented in lexicographic order by the integers $\\{0,1,\\dots,2^m-1\\}$ using binary expansion.\n- Training set: fix a training size $n$ with $0\\le n<2^m$. Let the training index set be the first $n$ elements of $X$ in lexicographic order, denoted $S=\\{x_0,\\dots,x_{n-1}\\}$. The unseen set is $U=X\\setminus S$.\n- Target function sampling: a target function $f:X\\to\\{0,1\\}$ is represented by a binary vector $y\\in\\{0,1\\}^{|X|}$. To sample $f$ uniformly, draw each $y_i$ independently from the Bernoulli distribution with parameter $1/2$, that is, $y_i\\sim\\mathrm{Bernoulli}(1/2)$, independently for all $i\\in\\{0,\\dots,|X|-1\\}$. For the special case where $k=2^{|X|}$, define the $k$ tasks as the exhaustive enumeration of all possible binary labelings of $X$, ordered lexicographically by $y$.\n- Learner $A_1$ (one-nearest neighbor on $S$ with Hamming distance): given $S$ and labels $y|_S$, define $A_1$ to predict for any $x\\in X$ the label of the nearest neighbor in $S$ under Hamming distance on $\\{0,1\\}^m$. In case of ties, break ties by choosing the training point in $S$ with the smallest lexicographic index.\n- Learner $A_2$ (empirical majority constant): given $S$ and labels $y|_S$, define $A_2$ to predict the empirical majority label in $S$ for all $x\\in X$. In case of a tie in $S$, predict $0$.\n- Per-task performance: for each sampled task $f$, train $A_1$ and $A_2$ on $S$ using $y|_S$ and compute their accuracies on $U$. Let $a_1$ and $a_2$ be the resulting accuracies. Define the paired difference $d=a_1-a_2$.\n\nStatistical test to implement:\n- For a given integer $k\\ge 1$, generate $k$ independent tasks by sampling $f$ uniformly as above (or exhaustively enumerating all $2^{|X|}$ labelings when $k=2^{|X|}$). Compute the paired differences $\\{d_t\\}_{t=1}^k$.\n- Test the null hypothesis $H_0:\\mu_d=0$ versus the two-sided alternative $H_1:\\mu_d\\ne 0$ using the paired Student's $t$-test at significance level $\\alpha\\in(0,1)$.\n- If $k<2$ or the sample standard deviation $s_d$ is zero, define the $p$-value to be $1$ and do not reject $H_0$.\n- Otherwise, compute $T=\\bar{d}/(s_d/\\sqrt{k})$ with degrees of freedom $k-1$, and the two-sided $p$-value $p=2\\cdot\\mathrm{sf}(|T|)$ for the Student's $t$-distribution, where $\\mathrm{sf}$ is the survival function. Decide to not reject $H_0$ when $p>\\alpha$.\n\nYour program must implement the above pipeline and produce a decision for each test case.\n\nTest suite and required output:\n- Use the following four test cases, each specified as a tuple $(m,n,k,\\text{seed},\\alpha)$:\n  1. $(5,8,64,12345,0.05)$\n  2. $(5,8,1,7,0.05)$\n  3. $(3,2,256,0,0.05)$ with exhaustive enumeration since $k=2^{|X|}$, and the seed is ignored.\n  4. $(6,10,100,2024,0.01)$\n- For cases where $k\\ne 2^{|X|}$, use the given seed to initialize a pseudo-random number generator to sample the $k$ functions $f$ as independent and identically distributed uniform labelings as specified.\n- For each test case, the required answer is a boolean indicating whether the mean performance difference is not significantly different from $0$ at level $\\alpha$, that is, return $\\text{True}$ if you do not reject $H_0$ and $\\text{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the four boolean decisions for the test cases in the same order as above, for example, \"[$\\text{True}$,$\\text{False}$,$\\text{True}$,$\\text{True}$]\". No additional text is permitted in the output line.", "solution": "The problem requires an empirical validation of a core tenet of the No Free Lunch (NFL) theorem for supervised learning. This theorem states that, when averaged over all possible target functions on a finite domain, no learning algorithm is superior to any other. We will implement a statistical experiment to test this principle by comparing two specific learning algorithms, $A_1$ and $A_2$, and performing a hypothesis test on their performance difference.\n\nThe theoretical foundation is the NFL theorem for supervised learning on a finite input space $X$. If we consider a uniform probability distribution over the set of all possible target functions $f: X \\to \\{0,1\\}$, the expected generalization accuracy on an unseen set of points $U \\subset X$ is identical for any learning algorithm. Let $\\mathcal{A}$ be the space of all learning algorithms, and let $S$ be the training set. For any two algorithms $A_1, A_2 \\in \\mathcal{A}$, the theorem implies:\n$$\n\\mathbb{E}_{f \\sim \\text{Unif}}[\\text{Acc}_U(A_1(S, f|_S))] = \\mathbb{E}_{f \\sim \\text{Unif}}[\\text{Acc}_U(A_2(S, f|_S))]\n$$\nwhere $\\text{Acc}_U(h) = \\frac{1}{|U|} \\sum_{x \\in U} \\mathbf{1}\\{h(x) = f(x)\\}$ is the accuracy of a hypothesis $h$ on the unseen set $U$. This directly leads to the conclusion that the expected difference in their accuracies is zero:\n$$\n\\mu_d = \\mathbb{E}_{f \\sim \\text{Unif}}[\\text{Acc}_U(A_1) - \\text{Acc}_U(A_2)] = 0\n$$\n\nThe experimental design translates this theoretical result into a statistical null hypothesis, $H_0: \\mu_d = 0$, which we test against the alternative $H_1: \\mu_d \\neq 0$. The experiment is defined by the following components:\n1.  **Input Space**: The domain is the set of all binary vectors of a fixed length $m$, $X = \\{0,1\\}^m$. For computational purposes, we represent elements of $X$ by integers $i \\in \\{0, 1, \\dots, 2^m-1\\}$ in lexicographical order.\n2.  **Data Partition**: The space $X$ is partitioned into a fixed training set $S$ of size $n$, consisting of the first $n$ points in lexicographical order, and an unseen set $U = X \\setminus S$.\n3.  **Task Generation**: A task is defined by a target function $f: X \\to \\{0,1\\}$, represented by a binary vector $y \\in \\{0,1\\}^{|X|}$. For a given number of tasks $k$, we generate $k$ such functions. If $k$ is smaller than the total number of possible functions, $2^{|X|}$, we sample them independently and uniformly, which is equivalent to drawing each label $y_i$ from a $\\mathrm{Bernoulli}(1/2)$ distribution. If $k=2^{|X|}$, we perform an exhaustive enumeration of all possible functions.\n4.  **Learning Algorithms**:\n    -   **$A_1$ (1-Nearest Neighbor)**: For any point $x \\in X$, this algorithm finds the point $x_s \\in S$ that is closest to $x$ under the Hamming distance. The prediction is the label of that neighbor, $f(x_s)$. Ties are broken by selecting the neighbor with the smallest lexicographical index.\n    -   **$A_2$ (Empirical Majority)**: This algorithm determines the majority label (0 or 1) on the training set $S$. It then predicts this same label for all points in $X$. In case of a tie in the training labels, it defaults to predicting $0$.\n\nFor each of the $k$ generated tasks, we train both algorithms on the training data $(S, f|_S)$ and calculate their respective accuracies, $a_1$ and $a_2$, on the unseen data $U$. The paired difference for task $t$ is $d_t = a_{1,t} - a_{2,t}$.\n\nThe statistical analysis is conducted on the sample of $k$ differences, $\\{d_t\\}_{t=1}^k$. We use a paired Student's $t$-test. The test statistic is calculated as:\n$$\nT = \\frac{\\bar{d}}{s_d / \\sqrt{k}}\n$$\nwhere $\\bar{d}$ is the sample mean of the differences, and $s_d$ is the sample standard deviation with Bessel's correction (i.e., using a denominator of $k-1$). The statistic $T$ follows a Student's $t$-distribution with $k-1$ degrees of freedom under the null hypothesis. The two-sided $p$-value is computed as $p = 2 \\cdot \\mathrm{sf}(|T|)$, where $\\mathrm{sf}$ is the survival function of the $t$-distribution. Per the problem specification, if $k < 2$ or if $s_d=0$, the $p$-value is taken to be $1$. We do not reject the null hypothesis $H_0$ if the resulting $p$-value is greater than the specified significance level $\\alpha$.\n\nThe implementation proceeds by first initializing the parameters for each test case $(m, n, k, \\text{seed}, \\alpha)$. It then generates the binary representations of the input space $X$ and partitions it into $S$ and $U$. A loop iterates $k$ times, each time generating a target function, training both learners, evaluating their accuracies on $U$, and computing the difference. After collecting all $k$ differences, the $t$-test is performed to obtain a $p$-value, which is then compared against $\\alpha$ to decide whether to reject the null hypothesis. The final output is a boolean indicating this decision for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Implements the experimental pipeline to test the No Free Lunch theorem.\n    For each test case, it compares two learning algorithms by performing a\n    paired t-test on their performance difference over a set of synthetic tasks.\n    \"\"\"\n    test_cases = [\n        (5, 8, 64, 12345, 0.05),\n        (5, 8, 1, 7, 0.05),\n        (3, 2, 256, 0, 0.05),\n        (6, 10, 100, 2024, 0.01),\n    ]\n\n    results = []\n    for m, n, k, seed, alpha in test_cases:\n        num_X = 2**m\n        num_U = num_X - n\n\n        # Pre-compute binary representations for the input space X\n        X_bin = np.zeros((num_X, m), dtype=np.int8)\n        for i in range(num_X):\n            binary_repr = np.binary_repr(i, width=m)\n            X_bin[i] = np.array(list(binary_repr), dtype=np.int8)\n\n        S_bin = X_bin[:n]\n        U_bin = X_bin[n:]\n\n        differences = []\n\n        is_exhaustive = (k == 2**num_X)\n        if not is_exhaustive:\n            rng = np.random.default_rng(seed)\n\n        for task_idx in range(k):\n            # Step 1: Generate a target function y\n            if is_exhaustive:\n                # Exhaustive enumeration of all possible labelings\n                y_repr = np.binary_repr(task_idx, width=num_X)\n                y = np.array(list(y_repr), dtype=np.int8)\n            else:\n                # Random sampling of labelings\n                y = rng.integers(0, 2, size=num_X, dtype=np.int8)\n\n            y_train = y[:n]\n            y_unseen = y[n:]\n\n            # Step 2: Run Learner A1 (1-NN)\n            h1_unseen = np.zeros(num_U, dtype=np.int8)\n            for i in range(num_U):\n                # Calculate Hamming distances from the current unseen point to all training points\n                distances = np.sum(S_bin != U_bin[i], axis=1)\n                # Find the index of the nearest neighbor. np.argmin breaks ties by first occurrence.\n                best_s_idx = np.argmin(distances)\n                h1_unseen[i] = y_train[best_s_idx]\n\n            # Step 3: Run Learner A2 (Empirical Majority)\n            num_ones = np.sum(y_train)\n            if num_ones > n / 2:\n                h2_pred = 1\n            else:  # Handles both minority and tie cases, predicting 0\n                h2_pred = 0\n            h2_unseen = np.full(num_U, h2_pred, dtype=np.int8)\n\n            # Step 4: Compute accuracies and their difference\n            # check if num_U is zero to avoid division by zero\n            if num_U > 0:\n                acc1 = np.sum(h1_unseen == y_unseen) / num_U\n                acc2 = np.sum(h2_unseen == y_unseen) / num_U\n                diff = acc1 - acc2\n            else:\n                diff = 0.0\n\n            differences.append(diff)\n\n        # Step 5: Perform paired t-test\n        diff_arr = np.array(differences)\n        \n        # As per problem, if k < 2, or s_d is 0, p-value is 1.\n        # np.std with ddof=1 produces NaN for k=1, so check k < 2 first.\n        s_d = 0.0\n        if k >= 2:\n            s_d = np.std(diff_arr, ddof=1)\n\n        if k  2 or s_d == 0:\n            p_value = 1.0\n        else:\n            d_bar = np.mean(diff_arr)\n            t_stat = d_bar / (s_d / np.sqrt(k))\n            df = k - 1\n            # Two-sided p-value\n            p_value = 2 * t.sf(np.abs(t_stat), df)\n\n        # Step 6: Make decision\n        # Do not reject H0 if p > alpha\n        decision = p_value > alpha\n        results.append(decision)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3153410"}, {"introduction": "The NFL theorems powerfully illustrate the necessity of inductive bias in supervised learning, but what are their limits? This exercise explores the crucial distinction between supervised and unsupervised learning. You will construct a dataset with clear geometric structure and show that while a supervised learner fails completely when the labels are randomized, an unsupervised algorithm can still successfully recover the inherent clusters in the data [@problem_id:3199442]. This demonstrates that while there may be no free lunch in predicting arbitrary labels, discovering intrinsic structure is a different, and often achievable, goal.", "problem": "You are given the foundational definitions of supervised learning and unsupervised learning. In supervised learning, the task is to learn a mapping from inputs $X \\in \\mathbb{R}^d$ to labels $Y \\in \\{0,1,\\dots,C-1\\}$ using labeled pairs $(X,Y)$. In unsupervised learning, the task is to discover structure in the inputs $X$ without using $Y$. Assume the standard probabilistic formalism in which data are drawn independently and identically distributed from a joint distribution $p(X,Y)$ and that classification accuracy is measured as the expected indicator $\\mathbb{E}[\\mathbf{1}\\{\\hat{Y}(X)=Y\\}]$ over a held-out test set, while unsupervised structure recovery is measured by comparing discovered clusters to the latent classes using a permutation-invariant index.\n\nUsing only these core definitions and the principle that when $Y$ is independent of $X$ no classifier can systematically outperform random guessing without additional assumptions, design a program that empirically demonstrates how supervised and unsupervised procedures differ under adversarial relabeling, while both procedures behave normally when labels reflect the latent structure. Concretely, for each test case specified below, you must:\n\n1) Data generation in $\\mathbb{R}^2$ with balanced classes:\n- Fix integers $C \\ge 2$ (number of classes) and $n \\ge 2$ (number of samples per class), and real parameters $\\sigma  0$ (cluster spread) and $R  0$ (radius). Use angles in radians.\n- For each class $c \\in \\{0,1,\\dots,C-1\\}$, define a mean\n$$\n\\mu_c = R \\begin{bmatrix} \\cos\\left( \\tfrac{2\\pi c}{C} \\right) \\\\ \\sin\\left( \\tfrac{2\\pi c}{C} \\right) \\end{bmatrix},\n$$\nand generate $n$ samples independently from the Gaussian distribution\n$$\nX \\mid (Y=c) \\sim \\mathcal{N}\\!\\left(\\mu_c, \\ \\sigma^2 I_2\\right),\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix. The resulting dataset has total size $N = C \\cdot n$. Split the data per class into a training set of size $n_{\\text{train}} = n/2$ and a test set of size $n_{\\text{test}} = n/2$ (assume $n$ is even in all test cases).\n\n2) Unsupervised structure recovery:\n- Run $k$-means clustering with $k=C$ on all $N$ inputs $X$ (ignore labels $Y$). Use any well-defined initialization procedure and a fixed iteration limit to ensure termination.\n- Compute the Adjusted Rand Index (ARI) between the clustering assignments and the true labels $Y$. The ARI between two partitions $\\mathcal{U}$ and $\\mathcal{V}$ of $N$ items with contingency table entries $n_{ij}$, row sums $a_i$, and column sums $b_j$ is\n$$\n\\operatorname{ARI} = \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} - \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{N}{2}}}{\\tfrac{1}{2}\\left[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}\\right] - \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{N}{2}}}.\n$$\n\n3) Supervised baseline with true labels:\n- Train a nearest-centroid classifier on the training set using the true labels $Y_{\\text{train}}$. For each class $c$, compute the centroid\n$$\n\\hat{\\mu}_c = \\frac{1}{|\\{i : Y_i=c, \\ i \\in \\text{train}\\}|} \\sum_{i: Y_i=c, \\ i \\in \\text{train}} X_i,\n$$\nand classify a test point $x$ by $\\hat{y}(x) = \\arg\\min_{c \\in \\{0,\\dots,C-1\\}} \\|x - \\hat{\\mu}_c\\|_2^2$. Compute the test accuracy as the fraction of correctly classified test points.\n\n4) Supervised under adversarially permuted labels:\n- Construct adversarial labels on the training set by applying a uniformly random permutation $\\pi$ to the training labels to obtain $Y_{\\text{train}}^{\\pi}$, which preserves class counts but destroys alignment between $X$ and $Y$.\n- Train the same nearest-centroid classifier using $(X_{\\text{train}}, Y_{\\text{train}}^{\\pi})$. Evaluate its test accuracy against the true test labels $Y_{\\text{test}}$.\n\nYour program must execute the above steps for each of the following test cases (this is the test suite), using the provided seeds for all random choices to guarantee reproducibility:\n\n- Test case $1$: $(C, n, \\sigma, R, \\text{seed}) = (3, 150, 0.6, 5.0, 1234)$.\n- Test case $2$: $(C, n, \\sigma, R, \\text{seed}) = (2, 200, 0.6, 4.0, 2021)$.\n- Test case $3$: $(C, n, \\sigma, R, \\text{seed}) = (5, 250, 0.6, 7.0, 9876)$.\n\nFinal output format:\n- For each test case, compute the triple $[ \\text{ARI}, \\ \\text{Acc}_{\\text{true}}, \\ \\text{Acc}_{\\text{perm}} ]$, where $\\text{ARI}$ is the Adjusted Rand Index from step $2$, $\\text{Acc}_{\\text{true}}$ is the supervised test accuracy from step $3$, and $\\text{Acc}_{\\text{perm}}$ is the supervised test accuracy from step $4$. Express all three values as decimals rounded to exactly three digits after the decimal point.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of these triples, enclosed in square brackets, for example:\n\"[ [a1,b1,c1],[a2,b2,c2],[a3,b3,c3] ]\" with each $a$, $b$, and $c$ replaced by the computed values.", "solution": "The problem statement is valid. It presents a well-defined computational experiment grounded in the fundamental principles of statistical machine learning to empirically demonstrate the conceptual differences between supervised and unsupervised learning. All parameters, algorithms, and evaluation metrics are clearly specified, forming a self-contained and reproducible task.\n\nThe experiment aims to highlight a core distinction: supervised learning algorithms learn a predictive mapping from inputs $X$ to outputs $Y$ and are thus critically dependent on the correctness of the training labels, while unsupervised learning algorithms seek inherent structure within the inputs $X$ alone, independent of any provided labels. This solution will implement the specified procedures to demonstrate this principle.\n\n**1. Data Generation**\n\nThe foundation of this experiment is a synthetic dataset where the geometric structure of the data points $X \\in \\mathbb{R}^2$ is intrinsically linked to latent class labels $Y \\in \\{0, 1, \\dots, C-1\\}$. For a given number of classes $C$, samples per class $n$, cluster spread $\\sigma$, and radius $R$, we generate data from a Gaussian mixture model. The mean $\\mu_c$ for each class $c$ is placed on a circle of radius $R$, with a uniform angular separation:\n$$\n\\mu_c = R \\begin{bmatrix} \\cos\\left( \\tfrac{2\\pi c}{C} \\right) \\\\ \\sin\\left( \\tfrac{2\\pi c}{C} \\right) \\end{bmatrix}.\n$$\nThe data points for class $c$ are then sampled from an isotropic Gaussian distribution centered at this mean:\n$$\nX \\mid (Y=c) \\sim \\mathcal{N}\\!\\left(\\mu_c, \\ \\sigma^2 I_2\\right),\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix. This process creates $C$ distinct clusters in the feature space $\\mathbb{R}^2$. For the experiment, the total dataset of $N=C \\cdot n$ points is generated, and then for each class, the data is deterministically split into a training set of size $n_{\\text{train}} = n/2$ and a test set of size $n_{\\text{test}} = n/2$.\n\n**2. Unsupervised Learning: Structure Recovery with k-means**\n\nUnsupervised learning aims to find patterns in data without reference to labels. Here, we apply the $k$-means clustering algorithm with $k=C$ to the entire set of input features $X$. The algorithm partitions the $N$ data points into $C$ clusters by iteratively minimizing the within-cluster sum of squares. Crucially, $k$-means is completely unaware of the true labels $Y$.\n\nThe success of this structure recovery is quantified by the Adjusted Rand Index (ARI). The ARI measures the similarity between two data clusterings, in this case, the clusters found by $k$-means and the ground-truth partition defined by the true labels $Y$. The ARI is calculated from a contingency table of the two partitions, which tabulates the number of points $n_{ij}$ that are in class $i$ of the true partition and cluster $j$ of the predicted partition. The formula is:\n$$\n\\operatorname{ARI} = \\frac{\\text{Index} - \\text{Expected Index}}{\\text{Max Index} - \\text{Expected Index}} = \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} - \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{N}{2}}}{\\tfrac{1}{2}\\left[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}\\right] - \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{N}{2}}},\n$$\nwhere $a_i$ and $b_j$ are the row and column sums of the contingency table. An ARI of $1.0$ signifies perfect recovery of the latent structure, while an ARI around $0.0$ indicates that the clustering is no better than random. Since the data generation process creates geometrically distinct clusters corresponding to the true labels, and $k$-means operates on this geometry, we expect a high ARI. This outcome is completely unaffected by how the labels $Y$ are stored or manipulated, as they are not used by the algorithm.\n\n**3. Supervised Learning: Classification with Nearest Centroids**\n\nSupervised learning, in contrast, explicitly uses the labels. We employ a nearest-centroid classifier.\n\n**3.1. Training with True Labels**\nFirst, the classifier is trained on the legitimate training set $(X_{\\text{train}}, Y_{\\text{train}})$. For each class $c$, it computes an empirical centroid $\\hat{\\mu}_c$, which is the mean of all training feature vectors belonging to that class:\n$$\n\\hat{\\mu}_c = \\frac{1}{n_{\\text{train}}} \\sum_{i: Y_{\\text{train},i}=c} X_{\\text{train},i}.\n$$\nSince the training data is drawn from the same well-structured distribution, these empirical centroids $\\hat{\\mu}_c$ will be reliable estimates of the true means $\\mu_c$. To classify a new test point $x$, the classifier finds the centroid closest to it in Euclidean distance:\n$$\n\\hat{y}(x) = \\arg\\min_{c \\in \\{0,\\dots,C-1\\}} \\|x - \\hat{\\mu}_c\\|_2^2.\n$$\nThe performance is measured by the accuracy on the test set, $\\text{Acc}_{\\text{true}} = \\frac{1}{N_{\\text{test}}} \\sum_{i=1}^{N_{\\text{test}}} \\mathbf{1}\\{\\hat{y}(X_{\\text{test},i}) = Y_{\\text{test},i}\\}$. Given the well-separated clusters, we expect $\\text{Acc}_{\\text{true}}$ to be very high, close to $1.0$.\n\n**3.2. Training with Adversarially Permuted Labels**\nNext, we simulate an adversarial scenario where the training labels are corrupted. We create a new set of labels $Y_{\\text{train}}^{\\pi}$ by applying a uniformly random permutation $\\pi$ to the original training labels $Y_{\\text{train}}$. This operation preserves the number of samples in each class but completely decouples the labels from the underlying geometric structure of $X_{\\text{train}}$. The joint distribution of $(X, Y^{\\pi})$ now has $Y^{\\pi}$ independent of $X$.\n\nThe same nearest-centroid classifier is then trained on the corrupted dataset $(X_{\\text{train}}, Y_{\\text{train}}^{\\pi})$. The resulting centroids will be nonsensical; each \"centroid\" will be an average over points from different true clusters, and they will likely all be located near the global mean of the data, which is the origin $(0,0)$.\n\nWhen this mal-trained classifier is evaluated on the original test set $(X_{\\text{test}}, Y_{\\text{test}})$, its predictions will be arbitrary with respect to the true labels. Its accuracy, $\\text{Acc}_{\\text{perm}}$, is expected to plummet to the level of random guessing. For a balanced $C$-class problem, this baseline accuracy is $1/C$.\n\nThis pair of supervised experiments demonstrates the algorithm's total reliance on a meaningful correlation between features and labels. When this correlation is destroyed, the supervised model fails to learn, confirming the foundational principle. The combined results will show a high $\\text{ARI}$ and $\\text{Acc}_{\\text{true}}$, but a low $\\text{Acc}_{\\text{perm}}$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom scipy.cluster.vq import kmeans, vq\n\ndef generate_data(C, n, sigma, R, rng):\n    \"\"\"Generates data from C Gaussian clusters on a circle.\"\"\"\n    n_total = C * n\n    X = np.zeros((n_total, 2))\n    Y = np.zeros(n_total, dtype=int)\n    \n    cov = np.array([[sigma**2, 0], [0, sigma**2]])\n\n    for c in range(C):\n        mean = np.array([R * np.cos(2 * np.pi * c / C), R * np.sin(2 * np.pi * c / C)])\n        start_idx = c * n\n        end_idx = (c + 1) * n\n        \n        X[start_idx:end_idx, :] = rng.multivariate_normal(mean, cov, size=n)\n        Y[start_idx:end_idx] = c\n        \n    return X, Y\n\ndef split_data(X, Y, C, n):\n    \"\"\"Splits data into training and test sets, stratified by class.\"\"\"\n    n_train_per_class = n // 2\n    n_test_per_class = n // 2\n    \n    X_train, Y_train = [], []\n    X_test, Y_test = [], []\n    \n    for c in range(C):\n        class_indices = np.where(Y == c)[0]\n        \n        train_indices = class_indices[:n_train_per_class]\n        test_indices = class_indices[n_train_per_class:]\n        \n        X_train.append(X[train_indices])\n        Y_train.append(Y[train_indices])\n        X_test.append(X[test_indices])\n        Y_test.append(Y[test_indices])\n\n    return np.vstack(X_train), np.hstack(Y_train), np.vstack(X_test), np.hstack(Y_test)\n\ndef adjusted_rand_index(labels_true, labels_pred):\n    \"\"\"Computes the Adjusted Rand Index.\"\"\"\n    # Based on the formula from the problem statement.\n    n_samples = len(labels_true)\n    classes = np.unique(labels_true)\n    clusters = np.unique(labels_pred)\n    \n    # Contingency table\n    contingency = np.zeros((classes.size, clusters.size), dtype=int)\n    for i in range(classes.size):\n        for j in range(clusters.size):\n            contingency[i, j] = np.sum((labels_true == classes[i])  (labels_pred == clusters[j]))\n\n    # Sum of combinations for n_ij\n    sum_comb_nij = np.sum([comb(n, 2, exact=True) for n in contingency.flatten()])\n    \n    # Sum of combinations for row sums (a_i) and col sums (b_j)\n    sum_comb_ai = np.sum([comb(n, 2, exact=True) for n in np.sum(contingency, axis=1)])\n    sum_comb_bj = np.sum([comb(n, 2, exact=True) for n in np.sum(contingency, axis=0)])\n    \n    # Total combinations\n    comb_N2 = comb(n_samples, 2, exact=True)\n    \n    # Expected index\n    expected_index = (sum_comb_ai * sum_comb_bj) / comb_N2\n    \n    # Max index\n    max_index = (sum_comb_ai + sum_comb_bj) / 2\n    \n    numerator = sum_comb_nij - expected_index\n    denominator = max_index - expected_index\n    \n    if denominator == 0:\n        # This can happen if the clustering is trivial (e.g., all points in one cluster)\n        # or perfect. If numerator is also 0 (perfect clustering), ARI is 1.\n        return 1.0 if numerator == 0 else 0.0\n        \n    return numerator / denominator\n\nclass NearestCentroid:\n    \"\"\"Nearest Centroid Classifier.\"\"\"\n    def __init__(self):\n        self.centroids_ = None\n        self.classes_ = None\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        self.centroids_ = np.array([X[y == c].mean(axis=0) for c in self.classes_])\n        return self\n\n    def predict(self, X):\n        if self.centroids_ is None:\n            raise RuntimeError(\"Classifier has not been trained. Call fit() first.\")\n        # Compute squared Euclidean distances using broadcasting\n        # (a-b)^2 = a^2 - 2ab + b^2\n        X_norm_sq = np.sum(X**2, axis=1)[:, np.newaxis]\n        centroids_norm_sq = np.sum(self.centroids_**2, axis=1)[np.newaxis, :]\n        dot_product = X @ self.centroids_.T\n        \n        distances_sq = X_norm_sq - 2 * dot_product + centroids_norm_sq\n        \n        return self.classes_[np.argmin(distances_sq, axis=1)]\n\ndef solve():\n    test_cases = [\n        # (C, n, sigma, R, seed)\n        (3, 150, 0.6, 5.0, 1234),\n        (2, 200, 0.6, 4.0, 2021),\n        (5, 250, 0.6, 7.0, 9876),\n    ]\n\n    all_results = []\n\n    for C, n, sigma, R, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # 1. Data generation\n        X, Y = generate_data(C, n, sigma, R, rng)\n        X_train, Y_train, X_test, Y_test = split_data(X, Y, C, n)\n\n        # 2. Unsupervised structure recovery (k-means)\n        # Use seed for reproducibility in kmeans initialization\n        centroids_kmeans, _ = kmeans(X, C, iter=300, seed=seed)\n        cluster_assignments, _ = vq(X, centroids_kmeans)\n        ari = adjusted_rand_index(Y, cluster_assignments)\n\n        # 3. Supervised baseline with true labels\n        clf_true = NearestCentroid()\n        clf_true.fit(X_train, Y_train)\n        Y_pred_true = clf_true.predict(X_test)\n        acc_true = np.mean(Y_pred_true == Y_test)\n\n        # 4. Supervised under adversarially permuted labels\n        Y_train_perm = rng.permutation(Y_train)\n        clf_perm = NearestCentroid()\n        clf_perm.fit(X_train, Y_train_perm)\n        Y_pred_perm = clf_perm.predict(X_test)\n        acc_perm = np.mean(Y_pred_perm == Y_test)\n        \n        # Collect and format results for the current case\n        case_result = f\"[{ari:.3f},{acc_true:.3f},{acc_perm:.3f}]\"\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3199442"}]}