{"hands_on_practices": [{"introduction": "When features are highly correlated, attributing a model's prediction to individual features becomes a thorny issue. How should credit be divided between two features that carry redundant information? This practice delves into this fundamental challenge using SHAP values, comparing standard attributions with a \"grouped\" approach where collinear features are treated as a single entity. By working through the analytics of a linear model with a controlled correlation structure, you will gain a deeper understanding of the fairness and consistency of feature attributions under collinearity. [@problem_id:3132668]", "problem": "You are given a linear model, a Gaussian background distribution, and a specific instance to explain. Your task is to implement the feature attribution method based on SHapley Additive exPlanations (SHAP), compare standard feature-level attributions against a grouped attribution that treats a set of collinear features as a single player, and quantify fairness criteria under collinearity.\n\nConsider the following setup. Let the model be the linear function\n$$\nf(\\mathbf{x}) = b + \\mathbf{w}^\\top \\mathbf{x},\n$$\nwith parameter vector $$\\mathbf{w} = (1, 1, 1/2)$$ and bias $$b = 0$$. The instance to be explained is\n$$\n\\mathbf{x} = (1, 1, 2).\n$$\nThe background distribution for inputs $$\\mathbf{X} = (X_1, X_2, X_3)$$ is the multivariate normal distribution $$\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$ with\n$$\n\\boldsymbol{\\mu} = (0,0,0)\n$$\nand covariance matrix\n$$\n\\boldsymbol{\\Sigma}(\\rho) = \\begin{pmatrix}\n1 & \\rho & 0\\\\\n\\rho & 1 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix},\n$$\nfor a given correlation parameter $$\\rho \\in [-1,1]$$. The features $$X_1$$ and $$X_2$$ are correlated with correlation coefficient $$\\rho$$, and $$X_3$$ is independent of both.\n\nDefine the set of feature indices as $$N = \\{1,2,3\\}$$ and a value function for any subset $$S \\subseteq N$$ by\n$$\nv(S) = \\mathbb{E}\\big[f(\\mathbf{X}) \\,\\big|\\, \\mathbf{X}_S = \\mathbf{x}_S\\big],\n$$\nwhere $$\\mathbf{X}_S$$ and $$\\mathbf{x}_S$$ denote the subvectors of $$\\mathbf{X}$$ and $$\\mathbf{x}$$ indexed by $$S$$.\n\nStandard feature-level SHAP values $$\\phi_i$$ for $$i \\in N$$ are the Shapley values of the cooperative game $$v(S)$$ with players $$N$$. Grouped-SHAP for the coalition $$G_1 = \\{1,2\\}$$ treats the coalition as a single player and computes the Shapley value $$\\phi_{G_1}$$ in the quotient game with players $$\\{G_1, \\{3\\}\\}$$ and value function inherited from $$v(S)$$ by mapping each coalition of players to the union of their member feature indices.\n\nYour program must:\n- Implement the value function $$v(S)$$ exactly under the given Gaussian background using the conditional expectation of a multivariate normal distribution.\n- Compute the standard SHAP values $$\\phi_1, \\phi_2, \\phi_3$$ for the model and instance.\n- Compute the grouped Shapley value $$\\phi_{G_1}$$ for the coalition $$G_1 = \\{1,2\\}$$.\n- Quantify three diagnostics for each test case:\n  1. Additivity error: \n     $$\n     \\left|\\sum_{i \\in N} \\phi_i - \\big(f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})]\\big)\\right|.\n     $$\n  2. Group consistency error:\n     $$\n     \\left| \\phi_{G_1} - (\\phi_1 + \\phi_2) \\right|.\n     $$\n  3. Symmetry deviation inside the collinear group:\n     $$\n     \\left| \\phi_1 - \\phi_2 \\right|.\n     $$\n\nUse the following test suite of correlation values:\n- $$\\rho = 0.0$$ (independent features),\n- $$\\rho = 0.9$$ (high collinearity),\n- $$\\rho = 1.0$$ (perfect collinearity).\n\nFor all test cases, take $$\\mathbf{w} = (1,1,1/2)$$, $$b = 0$$, $$\\boldsymbol{\\mu} = (0,0,0)$$, and $$\\mathbf{x} = (1,1,2)$$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case (in the order of the test suite) and is itself a list of three floating-point numbers \n$$\n[\\text{additivity\\_error}, \\text{group\\_consistency\\_error}, \\text{symmetry\\_deviation}].\n$$\nFor example, the output format must be\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]],\n$$\nwith each $$a_k, b_k, c_k$$ being a floating-point number. No physical units or angles are involved in this problem, and no percentages should be used anywhere in the output.", "solution": "The user wants to solve a problem related to SHAP (SHapley Additive exPlanations) for a linear model with a Gaussian background distribution. The problem asks for the computation of standard and grouped SHAP values and an analysis of three diagnostic metrics under varying levels of collinearity.\n\n### **Problem Validation**\nThe problem statement is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of givens for a well-defined task in the field of machine learning interpretability. The underlying concepts—linear models, multivariate normal distributions, conditional expectation, and Shapley values—are standard in mathematics and statistics. The case of perfect correlation ($\\rho=1$) results in a singular covariance matrix, but the required conditional expectations remain well-defined, as the conditioning variables are consistent with the linear dependency. The problem is valid.\n\n### **Step-by-Step Solution**\n\nThe solution proceeds in four steps:\n1.  Derive the characteristic value function $v(S)$ for all feature coalitions $S$.\n2.  Compute the standard feature-level SHAP values $\\phi_1, \\phi_2, \\phi_3$.\n3.  Compute the grouped SHAP value $\\phi_{G_1}$ for the coalition $G_1 = \\{1,2\\}$.\n4.  Calculate the three specified diagnostic metrics for each value of $\\rho$.\n\n**1. Derivation of the Value Function $v(S)$**\n\nThe model is $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} = 1 \\cdot x_1 + 1 \\cdot x_2 + \\frac{1}{2} \\cdot x_3$, since $b=0$. The background distribution is $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}(\\rho))$. The value function is defined as the conditional expectation $v(S) = \\mathbb{E}[f(\\mathbf{X}) | \\mathbf{X}_S = \\mathbf{x}_S]$. By linearity of expectation:\n$$v(S) = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{X} | \\mathbf{X}_S = \\mathbf{x}_S]$$\nThe conditional expectation for a multivariate normal distribution with mean $\\boldsymbol{\\mu} = \\mathbf{0}$ is $\\mathbb{E}[\\mathbf{X}_{\\bar{S}} | \\mathbf{X}_S = \\mathbf{x}_S] = \\boldsymbol{\\Sigma}_{\\bar{S}S} \\boldsymbol{\\Sigma}_{SS}^{-1} \\mathbf{x}_S$, where $\\bar{S}=N\\setminus S$. The components of $\\mathbf{X}$ we condition on remain fixed at their values in $\\mathbf{x}=(1,1,2)$.\n\nWe compute $v(S)$ for all $S \\subseteq N = \\{1,2,3\\}$:\n-   $S = \\emptyset$: $v(\\emptyset) = \\mathbb{E}[f(\\mathbf{X})] = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{X}] = \\mathbf{w}^\\top \\boldsymbol{\\mu} = 0$.\n-   $S = \\{1\\}$: We need $\\mathbb{E}[X_2|X_1=x_1]$ and $\\mathbb{E}[X_3|X_1=x_1]$. Since $X_3$ is independent of $X_1$, $\\mathbb{E}[X_3|X_1=x_1]=\\mathbb{E}[X_3]=0$. The conditional mean of $X_2$ given $X_1=x_1$ is $\\mathbb{E}[X_2|X_1=x_1]=\\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(x_1-\\mu_1) = 0 + \\frac{\\rho}{1}(x_1-0) = \\rho x_1$.\n    $v(\\{1\\}) = w_1x_1 + w_2(\\rho x_1) + w_3(0) = 1(1) + 1(\\rho \\cdot 1) = 1+\\rho$.\n-   $S = \\{2\\}$: By symmetry with feature $1$, $v(\\{2\\}) = w_2x_2 + w_1(\\rho x_2) = 1(1) + 1(\\rho \\cdot 1) = 1+\\rho$.\n-   $S = \\{3\\}$: $X_1, X_2$ are independent of $X_3$, so $\\mathbb{E}[X_1|X_3=x_3]=0$ and $\\mathbb{E}[X_2|X_3=x_3]=0$.\n    $v(\\{3\\}) = w_3x_3 = \\frac{1}{2}(2) = 1$.\n-   $S = \\{1,2\\}$: We need $\\mathbb{E}[X_3|X_1=x_1, X_2=x_2]$. As $X_3$ is independent of $(X_1, X_2)$, this is $\\mathbb{E}[X_3]=0$.\n    $v(\\{1,2\\}) = w_1x_1 + w_2x_2 = 1(1) + 1(1) = 2$. This holds even for $\\rho=1$, as we do not need to invert $\\boldsymbol{\\Sigma}_{\\{1,2\\},\\{1,2\\}}$.\n-   $S = \\{1,3\\}$: We need $\\mathbb{E}[X_2|X_1=x_1, X_3=x_3]$. Independence of $X_3$ from $(X_1, X_2)$ implies this is just $\\mathbb{E}[X_2|X_1=x_1]=\\rho x_1$.\n    $v(\\{1,3\\}) = w_1x_1 + w_3x_3 + w_2(\\rho x_1) = 1(1) + \\frac{1}{2}(2) + 1(\\rho \\cdot 1) = 2+\\rho$.\n-   $S = \\{2,3\\}$: By symmetry with $\\{1,3\\}$, $v(\\{2,3\\}) = 2+\\rho$.\n-   $S = \\{1,2,3\\}$: $v(\\{1,2,3\\}) = f(\\mathbf{x}) = 1(1)+1(1)+\\frac{1}{2}(2) = 3$.\n\n**2. Computation of Standard SHAP Values**\n\nThe Shapley value for a player $i$ in a game with player set $N$ is:\n$$\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)]$$\nFor $|N|=3$, the combinatorial weights are $1/3$ for $|S|=0$, $1/6$ for $|S|=1$, and $1/3$ for $|S|=2$.\n\n-   **$\\phi_1$**:\n    $\\phi_1 = \\frac{1}{3}[v(\\{1\\}) - v(\\emptyset)] + \\frac{1}{6}[v(\\{1,2\\}) - v(\\{2\\})] + \\frac{1}{6}[v(\\{1,3\\}) - v(\\{3\\})] + \\frac{1}{3}[v(\\{1,2,3\\}) - v(\\{2,3\\})]$\n    $\\phi_1 = \\frac{1}{3}(1+\\rho) + \\frac{1}{6}(2 - (1+\\rho)) + \\frac{1}{6}((2+\\rho) - 1) + \\frac{1}{3}(3 - (2+\\rho))$\n    $\\phi_1 = \\frac{1+\\rho}{3} + \\frac{1-\\rho}{6} + \\frac{1+\\rho}{6} + \\frac{1-\\rho}{3} = \\frac{2(1+\\rho)+(1-\\rho)+(1+\\rho)+2(1-\\rho)}{6} = \\frac{6}{6} = 1$.\n\n-   **$\\phi_2$**: The problem setup is symmetric with respect to features $1$ and $2$ ($w_1=w_2$, $x_1=x_2$, and their roles in the covariance matrix are exchangeable). The resulting value function $v(S)$ is also symmetric. By the symmetry axiom of Shapley values, $\\phi_1 = \\phi_2$. Thus, $\\phi_2 = 1$.\n\n-   **$\\phi_3$**:\n    $\\phi_3 = \\frac{1}{3}[v(\\{3\\}) - v(\\emptyset)] + \\frac{1}{6}[v(\\{1,3\\}) - v(\\{1\\})] + \\frac{1}{6}[v(\\{2,3\\}) - v(\\{2\\})] + \\frac{1}{3}[v(\\{1,2,3\\}) - v(\\{1,2\\})]$\n    $\\phi_3 = \\frac{1}{3}(1-0) + \\frac{1}{6}((2+\\rho) - (1+\\rho)) + \\frac{1}{6}((2+\\rho) - (1+\\rho)) + \\frac{1}{3}(3-2)$\n    $\\phi_3 = \\frac{1}{3}(1) + \\frac{1}{6}(1) + \\frac{1}{6}(1) + \\frac{1}{3}(1) = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1$.\n\nThe standard SHAP values are $(\\phi_1, \\phi_2, \\phi_3) = (1, 1, 1)$, independent of $\\rho$.\n\n**3. Computation of Grouped SHAP Value $\\phi_{G_1}$**\n\nWe consider a new game with two players: $P_1 = G_1 = \\{1,2\\}$ and $P_2 = \\{3\\}$. The value function $v'$ for this game is inherited from $v$:\n- $v'(\\emptyset) = v(\\emptyset) = 0$\n- $v'(\\{P_1\\}) = v(\\{1,2\\}) = 2$\n- $v'(\\{P_2\\}) = v(\\{3\\}) = 1$\n- $v'(\\{P_1, P_2\\}) = v(\\{1,2,3\\}) = 3$\n\nThe Shapley value for player $P_1$ in this 2-player game is:\n$$\\phi_{G_1} = \\frac{1}{2}[v'(\\{P_1\\}) - v'(\\emptyset)] + \\frac{1}{2}[v'(\\{P_1,P_2\\}) - v'(\\{P_2\\})]$$\n$$\\phi_{G_1} = \\frac{1}{2}(2 - 0) + \\frac{1}{2}(3 - 1) = 1 + 1 = 2$$\nThe grouped Shapley value $\\phi_{G_1}$ is $2$, also independent of $\\rho$.\n\n**4. Computation of Diagnostics**\nFor each test case $\\rho \\in \\{0.0, 0.9, 1.0\\}$, we compute the three metrics.\n\n1.  **Additivity error**: $|\\sum_{i \\in N} \\phi_i - (f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})])|$.\n    The sum of SHAP values is $\\phi_1+\\phi_2+\\phi_3 = 1+1+1 = 3$. The total effect to explain is $f(\\mathbf{x}) - v(\\emptyset) = 3 - 0 = 3$. The efficiency axiom of Shapley values guarantees $\\sum \\phi_i = v(N) - v(\\emptyset)$.\n    The error is $|3 - 3| = 0$.\n\n2.  **Group consistency error**: $| \\phi_{G_1} - (\\phi_1 + \\phi_2) |$.\n    We have $\\phi_{G_1} = 2$ and $\\phi_1 + \\phi_2 = 1 + 1 = 2$.\n    The error is $|2 - 2| = 0$. This is zero because the game is additively separable. The feature set $\\{1,2\\}$ is statistically independent of $\\{3\\}$, and the linear model has no interaction terms between these sets. This implies $v(S \\cup T) = v(S) + v(T)$ for $S \\subseteq \\{1,2\\}$ and $T \\subseteq \\{3\\}$, which leads to this form of group consistency.\n\n3.  **Symmetry deviation**: $| \\phi_1 - \\phi_2 |$.\n    As noted, the problem is symmetric with respect to features $1$ and $2$. This forces their Shapley values to be equal.\n    The error is $|1 - 1| = 0$.\n\nAll three diagnostic metrics are $0$ for all values of $\\rho$. The specific level of collinearity between features $1$ and $2$ does not affect these particular diagnostics due to the problem's structural properties (independence from feature $3$ and symmetry between features $1$ and $2$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SHAP-based diagnostics for a linear model with collinear features.\n    \n    The problem setup allows for an exact analytical solution, which is implemented\n    here. The diagnostic values are found to be zero in all cases due to the\n    structural properties of the problem:\n    1. Additivity Error is zero by the efficiency property of Shapley values.\n    2. Symmetry Deviation is zero due to the symmetric treatment of features 1 and 2\n       in the model, instance, and data distribution.\n    3. Group Consistency Error is zero because the game is additively separable,\n       a consequence of the feature set {1,2} being statistically independent of {3}.\n    \"\"\"\n    w = np.array([1.0, 1.0, 0.5])\n    x_inst = np.array([1.0, 1.0, 2.0])\n    # Bias b = 0 and background mean mu = (0,0,0) are constants in the problem.\n\n    test_rhos = [0.0, 0.9, 1.0]\n    \n    all_results = []\n    \n    for rho in test_rhos:\n        # 1. Define the value function v(S) based on analytical derivations.\n        #    v(S) = E[f(X) | X_S = x_S]\n        #    The keys are tuples of feature indices (1-based for clarity).\n        v = {}\n        v[()] = 0.0\n        v[(1,)] = (w[0] + w[1]*rho) * x_inst[0]\n        v[(2,)] = (w[1] + w[0]*rho) * x_inst[1]\n        v[(3,)] = w[2] * x_inst[2]          \n        v[(1, 2)] = w[0] * x_inst[0] + w[1] * x_inst[1]\n        v[(1, 3)] = w[0] * x_inst[0] + w[2] * x_inst[2] + w[1] * rho * x_inst[0]\n        v[(2, 3)] = w[1] * x_inst[1] + w[2] * x_inst[2] + w[0] * rho * x_inst[1]\n        v[(1, 2, 3)] = np.dot(w, x_inst)\n\n        # 2. Compute standard SHAP values for N=3 players.\n        #    The combinatorial weights are 1/3 for |S|=0, 1/6 for |S|=1, 1/3 for |S|=2.\n        \n        # phi_1 for feature 1\n        mc1_s0 = v[(1,)] - v[()]\n        mc1_s2 = v[(1, 2)] - v[(2,)]\n        mc1_s3 = v[(1, 3)] - v[(3,)]\n        mc1_s23 = v[(1, 2, 3)] - v[(2, 3)]\n        phi_1 = (1.0/3.0)*mc1_s0 + (1.0/6.0)*mc1_s2 + (1.0/6.0)*mc1_s3 + (1.0/3.0)*mc1_s23\n\n        # phi_2 for feature 2\n        mc2_s0 = v[(2,)] - v[()]\n        mc2_s1 = v[(1, 2)] - v[(1,)]\n        mc2_s3 = v[(2, 3)] - v[(3,)]\n        mc2_s13 = v[(1, 2, 3)] - v[(1, 3)]\n        phi_2 = (1.0/3.0)*mc2_s0 + (1.0/6.0)*mc2_s1 + (1.0/6.0)*mc2_s3 + (1.0/3.0)*mc2_s13\n        \n        # phi_3 for feature 3\n        mc3_s0 = v[(3,)] - v[()]\n        mc3_s1 = v[(1, 3)] - v[(1,)]\n        mc3_s2 = v[(2, 3)] - v[(2,)]\n        mc3_s12 = v[(1, 2, 3)] - v[(1, 2)]\n        phi_3 = (1.0/3.0)*mc3_s0 + (1.0/6.0)*mc3_s1 + (1.0/6.0)*mc3_s2 + (1.0/3.0)*mc3_s12\n\n        # 3. Compute grouped Shapley value phi_G1 for G1={1,2}.\n        #    This is a 2-player game with players P1=G1 and P2={3}.\n        v_prime = {\n            'empty': v[()],\n            'P1': v[(1, 2)],\n            'P2': v[(3,)],\n            'P12': v[(1, 2, 3)]\n        }\n        \n        phi_G1 = 0.5 * (v_prime['P1'] - v_prime['empty']) + 0.5 * (v_prime['P12'] - v_prime['P2'])\n        \n        # 4. Quantify diagnostics.\n        fx_minus_Ex = v[(1, 2, 3)] - v[()]\n        \n        additivity_error = abs((phi_1 + phi_2 + phi_3) - fx_minus_Ex)\n        group_consistency_error = abs(phi_G1 - (phi_1 + phi_2))\n        symmetry_deviation = abs(phi_1 - phi_2)\n        \n        case_results = [additivity_error, group_consistency_error, symmetry_deviation]\n        all_results.append(case_results)\n\n    # Format the final output string to match the problem specification exactly.\n    # The format is [[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]] with no spaces.\n    results_str = str(all_results).replace(\" \", \"\")\n    print(results_str)\n\nsolve()\n```", "id": "3132668"}, {"introduction": "A model's predictive power often comes from capturing complex interactions between features, where the effect of one feature depends on the value of another. This exercise explores how different interpretability methods handle such non-additive effects using the canonical XOR problem, which represents a pure interaction. You will compare the global perspective of Partial Dependence Plots (PDP) with the local views of Individual Conditional Expectation (ICE) and Shapley interaction values, revealing how some methods can fail to detect interactions that are crucial to the model's logic. [@problem_id:3132648]", "problem": "You are given a binary interaction model defined on two features. Let $x = (x_{1}, x_{2})$ with each $x_{i} \\in \\{0,1\\}$. Define the target function as the exclusive-or interaction $f(x) = \\mathbf{1}[x_{1} \\oplus x_{2}]$, where $\\mathbf{1}[\\cdot]$ is the indicator function and $\\oplus$ denotes logical exclusive-or. Consider a background data-generating process where $X_{1} \\sim \\text{Bernoulli}(p_{1})$ and $X_{2} \\sim \\text{Bernoulli}(p_{2})$, independent of each other. Interpretability quantities must be computed with respect to this background, using the following fundamental definitions:\n- Partial Dependence Plot (PDP): For feature $i \\in \\{1,2\\}$, the partial dependence function is $PD_{i}(a) = \\mathbb{E}_{X_{-i}}[f(x) \\mid x_{i}=a]$, where the expectation is taken over the background distribution of $X_{-i}$, treating the other feature(s) as random.\n- Individual Conditional Expectation (ICE): For a fixed instance with $x_{-i}^{\\star}$, the individual conditional expectation curve for feature $i$ is $ICE_{i}(a \\mid x_{-i}^{\\star}) = f(x)$ with $x_{i}=a$ and $x_{-i}=x_{-i}^{\\star}$.\n- Shapley interaction index: For features $i$ and $j$ at point $x$, define the characteristic function of a coalition $S \\subseteq \\{1,2\\}$ as $v(S) = \\mathbb{E}[f(X) \\mid X_{S} = x_{S}]$ under an interventional background in which features outside $S$ are independently sampled from their marginals. The Shapley interaction value between features $1$ and $2$ at $x$ is the average second-order discrete difference\n$$\\phi_{1,2}^{\\mathrm{int}}(x) = \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{1,2\\}} \\frac{|S|!\\,(2 - |S| - 2)!}{2\\,(2-1)!} \\big(v(S \\cup \\{1,2\\}) - v(S \\cup \\{1\\}) - v(S \\cup \\{2\\}) + v(S)\\big),$$\nwhich for $2$ features reduces to\n$$\\phi_{1,2}^{\\mathrm{int}}(x) = \\frac{1}{2}\\big(v(\\{1,2\\}) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)\\big).$$\nAll expectations above are with respect to the specified independent Bernoulli background.\n\nYour task: Write a complete program that constructs $f(x)$ and, for each specified test case $(p_{1}, p_{2})$, computes the following four quantities that together compare how Partial Dependence Plot (PDP), Individual Conditional Expectation (ICE), and the Shapley interaction index capture the model’s pure interaction.\n\nFor each test case $(p_{1}, p_{2})$, compute:\n1) The PDP main-effect magnitude for feature $1$, defined as $M_{1} = \\left|PD_{1}(1) - PD_{1}(0)\\right|$.\n2) The PDP main-effect magnitude for feature $2$, defined as $M_{2} = \\left|PD_{2}(1) - PD_{2}(0)\\right|$.\n3) An ICE heterogeneity metric for feature $1$, defined using two reference instances with $x_{2}^{\\star} \\in \\{0,1\\}$ as the mean absolute pointwise difference between the two ICE curves across $x_{1} \\in \\{0,1\\}$:\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\sum_{a \\in \\{0,1\\}} \\left| ICE_{1}(a \\mid x_{2}^{\\star}=0) - ICE_{1}(a \\mid x_{2}^{\\star}=1) \\right|.$$\n4) The average absolute Shapley interaction magnitude across the $4$ input points $x \\in \\{0,1\\}^{2}$:\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\sum_{x \\in \\{0,1\\}^{2}} \\left|\\phi_{1,2}^{\\mathrm{int}}(x)\\right|,$$\nwhere each $\\phi_{1,2}^{\\mathrm{int}}(x)$ uses the interventional background as defined above.\n\nUse the following test suite of background parameter values:\n- Case $1$: $(p_{1}, p_{2}) = (0.5, 0.5)$.\n- Case $2$: $(p_{1}, p_{2}) = (0.5, 0.8)$.\n- Case $3$: $(p_{1}, p_{2}) = (0.5, 1.0)$.\n\nYour program should output one result per test case, where each result is the list $[M_{1}, M_{2}, H_{\\mathrm{ICE}}, A_{\\mathrm{SHAP}}]$ in that order. The final output must be a single line containing a list of these three lists, printed as a comma-separated list enclosed in square brackets, for example:\n\"[[m_{1,1},m_{1,2},m_{1,3},m_{1,4}],[m_{2,1},m_{2,2},m_{2,3},m_{2,4}],[m_{3,1},m_{3,2},m_{3,3},m_{3,4}]]\".\n\nDesign notes for coverage:\n- Case $1$ tests the symmetric background where $p_{1}=p_{2}$ and both equal $0.5$, where a pure interaction should have $M_{1} = 0$ and $M_{2} = 0$, but nonzero interaction detected by ICE and Shapley interaction.\n- Case $2$ introduces marginal imbalance in $X_{2}$, showing that $M_{1}$ can become nonzero even without any additive main effect in $f$.\n- Case $3$ is an extreme skew for $X_{2}$ with $p_{2} = 1.0$ to test boundary behavior of these measures under a degenerate background distribution while keeping interventional expectations well-defined.\n\nNo physical units are involved. All returned quantities must be real numbers. The output format must be exactly a single line as described above. Do not print any explanatory text, only the required final list.", "solution": "The problem statement is formally evaluated and found to be valid. It is self-contained, scientifically grounded in the theory of statistical learning and model interpretability, and well-posed. All definitions and variables are stated with sufficient precision to allow for a unique and meaningful solution. We may therefore proceed with a formal derivation of the required quantities.\n\nThe target function is the exclusive-or (XOR) function on two binary features, $x_1, x_2 \\in \\{0, 1\\}$. It is given by $f(x_1, x_2) = \\mathbf{1}[x_1 \\oplus x_2]$. The explicit values of the function are:\n$$f(0, 0) = 0$$\n$$f(0, 1) = 1$$\n$$f(1, 0) = 1$$\n$$f(1, 1) = 0$$\nThe background data is generated by two independent Bernoulli random variables, $X_1 \\sim \\text{Bernoulli}(p_1)$ and $X_2 \\sim \\text{Bernoulli}(p_2)$. This implies $P(X_1=1) = p_1$, $P(X_1=0) = 1-p_1$, and similarly for $X_2$. We will now derive the analytical expressions for the four requested quantities.\n\n**1. PDP Main-Effect Magnitude for Feature 1 ($M_1$)**\n\nThe Partial Dependence Plot (PDP) function for feature $1$ at value $a \\in \\{0, 1\\}$ is defined as $PD_1(a) = \\mathbb{E}_{X_2}[f(a, X_2)]$. The expectation is taken over the background distribution of $X_2$.\n\nFor $a=1$:\n$$PD_1(1) = \\mathbb{E}_{X_2}[f(1, X_2)] = f(1, 0) \\cdot P(X_2=0) + f(1, 1) \\cdot P(X_2=1)$$\n$$PD_1(1) = 1 \\cdot (1-p_2) + 0 \\cdot p_2 = 1-p_2$$\nFor $a=0$:\n$$PD_1(0) = \\mathbb{E}_{X_2}[f(0, X_2)] = f(0, 0) \\cdot P(X_2=0) + f(0, 1) \\cdot P(X_2=1)$$\n$$PD_1(0) = 0 \\cdot (1-p_2) + 1 \\cdot p_2 = p_2$$\nThe PDP main-effect magnitude for feature $1$ is $M_1 = |PD_1(1) - PD_1(0)|$.\n$$M_1 = |(1-p_2) - p_2| = |1 - 2p_2|$$\n\n**2. PDP Main-Effect Magnitude for Feature 2 ($M_2$)**\n\nSimilarly, the PDP function for feature $2$ at value $a \\in \\{0, 1\\}$ is $PD_2(a) = \\mathbb{E}_{X_1}[f(X_1, a)]$.\n\nFor $a=1$:\n$$PD_2(1) = \\mathbb{E}_{X_1}[f(X_1, 1)] = f(0, 1) \\cdot P(X_1=0) + f(1, 1) \\cdot P(X_1=1)$$\n$$PD_2(1) = 1 \\cdot (1-p_1) + 0 \\cdot p_1 = 1-p_1$$\nFor $a=0$:\n$$PD_2(0) = \\mathbb{E}_{X_1}[f(X_1, 0)] = f(0, 0) \\cdot P(X_1=0) + f(1, 0) \\cdot P(X_1=1)$$\n$$PD_2(0) = 0 \\cdot (1-p_1) + 1 \\cdot p_1 = p_1$$\nThe PDP main-effect magnitude for feature $2$ is $M_2 = |PD_2(1) - PD_2(0)|$.\n$$M_2 = |(1-p_1) - p_1| = |1 - 2p_1|$$\n\n**3. ICE Heterogeneity Metric ($H_{\\mathrm{ICE}}$)**\n\nThe Individual Conditional Expectation (ICE) curve for feature $1$ at a fixed value $x_2^\\star$ is $ICE_1(a | x_2^\\star) = f(a, x_2^\\star)$. The heterogeneity metric $H_{\\mathrm{ICE}}$ compares the two possible ICE curves generated by setting $x_2^\\star=0$ and $x_2^\\star=1$.\n\nFor $x_2^\\star=0$, the ICE curve is $ICE_1(a|0) = f(a, 0)$.\n$$ICE_1(0|0) = f(0, 0) = 0$$\n$$ICE_1(1|0) = f(1, 0) = 1$$\nFor $x_2^\\star=1$, the ICE curve is $ICE_1(a|1) = f(a, 1)$.\n$$ICE_1(0|1) = f(0, 1) = 1$$\n$$ICE_1(1|1) = f(1, 1) = 0$$\nThe heterogeneity metric is defined as $H_{\\mathrm{ICE}} = \\frac{1}{2} \\sum_{a \\in \\{0,1\\}} |ICE_1(a | x_2^\\star=0) - ICE_1(a | x_2^\\star=1)|$.\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\left[ |ICE_1(0|0) - ICE_1(0|1)| + |ICE_1(1|0) - ICE_1(1|1)| \\right]$$\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\left[ |0 - 1| + |1 - 0| \\right] = \\frac{1}{2} (1 + 1) = 1$$\nThis metric is a constant value of $1$, indicating that the effect of feature $1$ is always perfectly inverted depending on the value of feature $2$, which is the definition of the XOR interaction. This measure is independent of the background distribution probabilities $p_1$ and $p_2$.\n\n**4. Average Absolute Shapley Interaction Magnitude ($A_{\\mathrm{SHAP}}$)**\n\nThe Shapley interaction value is $\\phi_{1,2}^{\\mathrm{int}}(x) = \\frac{1}{2} \\left(v(\\{1,2\\}) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)\\right)$. We first compute the terms of the characteristic function $v(S) = \\mathbb{E}[f(X) | X_S = x_S]$.\n$v(\\{1,2\\}) = f(x_1, x_2)$ as both features are fixed.\n$v(\\{1\\}) = \\mathbb{E}_{X_2}[f(x_1, X_2)] = PD_1(x_1)$. This is $p_2$ for $x_1=0$ and $1-p_2$ for $x_1=1$.\n$v(\\{2\\}) = \\mathbb{E}_{X_1}[f(X_1, x_2)] = PD_2(x_2)$. This is $p_1$ for $x_2=0$ and $1-p_1$ for $x_2=1$.\n$v(\\varnothing) = \\mathbb{E}[f(X_1, X_2)] = P(X_1=0, X_2=1) + P(X_1=1, X_2=0) = (1-p_1)p_2 + p_1(1-p_2)$.\n\nWe now compute $\\phi_{1,2}^{\\mathrm{int}}(x)$ for each of the four points $x \\in \\{0,1\\}^2$.\n- For $x=(0,0)$:\n$\\phi_{1,2}^{\\mathrm{int}}(0,0) = \\frac{1}{2} [f(0,0) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [0 - p_2 - p_1 + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[ -p_1 - p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2 ] = \\frac{1}{2}[-2p_1 p_2] = -p_1 p_2$.\n- For $x=(0,1)$:\n$\\phi_{1,2}^{\\mathrm{int}}(0,1) = \\frac{1}{2} [f(0,1) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [1 - p_2 - (1-p_1) + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[p_1 - p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_1 - 2p_1 p_2] = p_1(1-p_2)$.\n- For $x=(1,0)$:\n$\\phi_{1,2}^{\\mathrm{int}}(1,0) = \\frac{1}{2} [f(1,0) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [1 - (1-p_2) - p_1 + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[p_2 - p_1 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_2 - 2p_1 p_2] = p_2(1-p_1)$.\n- For $x=(1,1)$:\n$\\phi_{1,2}^{\\mathrm{int}}(1,1) = \\frac{1}{2} [f(1,1) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [0 - (1-p_2) - (1-p_1) + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[-2+p_1+p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_1+2p_2-2 - 2p_1 p_2] = - (1 - p_1 - p_2 + p_1 p_2) = -(1-p_1)(1-p_2)$.\n\nThe average absolute Shapley interaction magnitude is $A_{\\mathrm{SHAP}} = \\frac{1}{4} \\sum_{x \\in \\{0,1\\}^2} |\\phi_{1,2}^{\\mathrm{int}}(x)|$. Since $p_1, p_2 \\in [0, 1]$, all terms are non-negative.\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\left[ |-p_1 p_2| + |p_1(1-p_2)| + |p_2(1-p_1)| + |-(1-p_1)(1-p_2)| \\right]$$\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\left[ p_1 p_2 + p_1(1-p_2) + p_2(1-p_1) + (1-p_1)(1-p_2) \\right]$$\nExpanding and simplifying the sum in the brackets:\n$$p_1 p_2 + p_1 - p_1 p_2 + p_2 - p_1 p_2 + 1 - p_1 - p_2 + p_1 p_2 = 1$$\nThus, the average absolute Shapley interaction magnitude is constant:\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4}$$\n\n**Summary of Formulas and Computations for Test Cases**\n\nThe derived expressions are:\n1. $M_1 = |1 - 2p_2|$\n2. $M_2 = |1 - 2p_1|$\n3. $H_{\\mathrm{ICE}} = 1$\n4. $A_{\\mathrm{SHAP}} = 0.25$\n\nWe apply these to the given test cases:\n- **Case 1**: $(p_1, p_2) = (0.5, 0.5)$\n  - $M_1 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - Result: $[0.0, 0.0, 1.0, 0.25]$\n\n- **Case 2**: $(p_1, p_2) = (0.5, 0.8)$\n  - $M_1 = |1 - 2(0.8)| = |1 - 1.6| = 0.6$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - Result: $[0.6, 0.0, 1.0, 0.25]$\n\n- **Case 3**: $(p_1, p_2) = (0.5, 1.0)$\n  - $M_1 = |1 - 2(1.0)| = |1 - 2| = 1.0$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - Result: $[1.0, 0.0, 1.0, 0.25]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes interpretability quantities for the XOR function f(x1, x2) = x1 ^ x2.\n    \n    The quantities are:\n    1. M1: PDP main-effect magnitude for feature 1.\n    2. M2: PDP main-effect magnitude for feature 2.\n    3. H_ICE: ICE heterogeneity metric for feature 1.\n    4. A_SHAP: Average absolute Shapley interaction magnitude.\n\n    These are computed for three test cases of background distribution parameters (p1, p2).\n    \"\"\"\n    \n    # Test cases for (p1, p2)\n    test_cases = [\n        (0.5, 0.5), # Case 1\n        (0.5, 0.8), # Case 2\n        (0.5, 1.0), # Case 3\n    ]\n\n    all_results = []\n    \n    for p1, p2 in test_cases:\n        # Based on the analytical derivations:\n        # M_1 = |1 - 2*p2|\n        # M_2 = |1 - 2*p1|\n        # H_ICE = 1\n        # A_SHAP = 0.25\n        \n        # 1. PDP main-effect magnitude for feature 1\n        m1 = np.abs(1 - 2 * p2)\n        \n        # 2. PDP main-effect magnitude for feature 2\n        m2 = np.abs(1 - 2 * p1)\n        \n        # 3. ICE heterogeneity metric\n        # The metric is constant for the XOR function.\n        # H_ICE = 0.5 * (|f(0,0)-f(0,1)| + |f(1,0)-f(1,1)|)\n        # H_ICE = 0.5 * (|0-1| + |1-0|) = 0.5 * (1 + 1) = 1.0\n        h_ice = 1.0\n        \n        # 4. Average absolute Shapley interaction magnitude\n        # This metric is also constant for the XOR function under the given definition.\n        # The sum of absolute interaction values is:\n        # |p1*p2| + |p1*(1-p2)| + |p2*(1-p1)| + |(1-p1)*(1-p2)| = 1\n        # A_SHAP = (1/4) * 1 = 0.25\n        a_shap = 0.25\n        \n        # Store the results as a list of floating-point numbers\n        result = [float(m1), float(m2), float(h_ice), float(a_shap)]\n        all_results.append(result)\n\n    # Format the output string as specified in the problem statement.\n    # e.g., \"[[0.0,0.0,1.0,0.25],[0.6,0.0,1.0,0.25],[1.0,0.0,1.0,0.25]]\"\n    string_formatted_results = []\n    for res_list in all_results:\n        # Convert each inner list to its string representation and remove spaces\n        list_str = str(res_list).replace(\" \", \"\")\n        string_formatted_results.append(list_str)\n    \n    final_output = f\"[{','.join(string_formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3132648"}, {"introduction": "Beyond explaining *why* a model made a certain prediction, a powerful form of interpretability is understanding *what* it would take to change the outcome. This practice introduces the concept of algorithmic recourse, which generates actionable counterfactual explanations. You will solve a constrained optimization problem to find the minimum-cost change to an input that flips the decision of a linear classifier, providing a concrete and practical pathway for an individual to achieve a more favorable result. [@problem_id:3132630]", "problem": "You are given a binary linear classifier from a Support Vector Machine (SVM) with decision function $f(x)=\\operatorname{sign}(w^{\\top}x+\\beta)$ acting on $x\\in\\mathbb{R}^{2}$. In algorithmic recourse, an individual may change features by an action vector $\\delta\\in\\mathbb{R}^{2}$ to obtain a new point $x+\\delta$. The cost of an action is modeled by the squared Euclidean norm $c(\\delta)=\\|\\delta\\|_{2}^{2}$. Feasible actions are restricted by linear constraints $A\\delta\\leq s$. Starting from the definitions of the classifier, the decision region, and the cost, derive from first principles the optimization that returns the minimum-cost recourse action that flips the decision to the positive class, and solve it using general optimality conditions.\n\nConcretely, let $w=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$, $\\beta=-1$, and the current point be $x=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, so that the current decision is negative. Let the feasible action set enforce componentwise nonnegativity, that is, $A=-I_{2}$ and $s=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ so that $A\\delta\\leq s$ is equivalent to $\\delta\\geq 0$ coordinatewise. Formulate the constrained optimization problem for the minimum-cost recourse that flips the decision to the positive class and, by deriving and solving the first-order optimality conditions from the base principles of convex optimization, compute the exact optimal $\\delta^{\\star}$ that achieves $w^{\\top}(x+\\delta^{\\star})+\\beta\\geq 0$ with minimum $c(\\delta^{\\star})$.\n\nExpress your final answer as a single row vector $(\\delta_{1}^{\\star},\\delta_{2}^{\\star})$ in exact form. No rounding is required.", "solution": "The task is to find the minimum-cost recourse action for a binary linear classifier. We begin by validating the problem statement.\n\n### Step 1: Extract Givens\n- Binary linear classifier decision function: $f(x)=\\operatorname{sign}(w^{\\top}x+\\beta)$\n- Input space: $x\\in\\mathbb{R}^{2}$\n- Action vector: $\\delta\\in\\mathbb{R}^{2}$\n- New data point: $x+\\delta$\n- Cost function for an action: $c(\\delta)=\\|\\delta\\|_{2}^{2}$\n- Feasible action constraints: $A\\delta\\leq s$\n- Specific classifier parameters: $w=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$, $\\beta=-1$\n- Current data point: $x=\\begin{pmatrix}0\\\\0\\end{pmatrix}$\n- Specific action constraints: $A=-I_{2}$, $s=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, which corresponds to $\\delta \\geq 0$ componentwise.\n- Goal: Find the minimum-cost recourse $\\delta^{\\star}$ that flips the decision from negative to positive, i.e., $w^{\\top}(x+\\delta^{\\star})+\\beta\\geq 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the field of statistical learning, specifically algorithmic recourse.\n- **Scientific Grounding**: The concepts of SVMs, decision boundaries, and constrained optimization are standard and mathematically sound.\n- **Well-Posedness**: The objective function $c(\\delta)=\\|\\delta\\|_{2}^{2}$ is strictly convex. The feasible region, defined by a set of linear inequalities, is convex. The minimization of a strictly convex function over a non-empty closed convex set has a unique solution. Let's verify the starting condition. For $x=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, the decision argument is $w^{\\top}x+\\beta = \\begin{pmatrix}2 & -1\\end{pmatrix}\\begin{pmatrix}0\\\\0\\end{pmatrix} - 1 = -1$. Since $-1 < 0$, the initial classification is negative, as stated in the problem. The problem is consistent.\n- **Objectivity**: The problem is stated in precise mathematical terms.\n\n### Step 3: Verdict and Action\nThe problem is valid. We can proceed to the solution.\n\n### Solution Derivation\nThe problem asks for the minimum-cost action $\\delta = \\begin{pmatrix}\\delta_1 \\\\ \\delta_2\\end{pmatrix}$ that changes the classifier's output for a point $x$ from negative to positive, subject to certain feasibility constraints.\n\nFirst, we formulate the optimization problem based on the provided definitions.\nThe objective is to minimize the cost, which is the squared Euclidean norm of the action vector $\\delta$:\n$$\n\\text{Minimize} \\quad c(\\delta) = \\|\\delta\\|_{2}^{2} = \\delta_1^2 + \\delta_2^2\n$$\n\nThe action must achieve recourse, meaning the new point $x+\\delta$ must be classified as positive. This requires the decision function's argument to be non-negative:\n$$\nw^{\\top}(x+\\delta)+\\beta \\geq 0\n$$\nSubstituting the given values $w=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$, $x=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, and $\\beta=-1$:\n$$\n\\begin{pmatrix}2 & -1\\end{pmatrix} \\left( \\begin{pmatrix}0\\\\0\\end{pmatrix} + \\begin{pmatrix}\\delta_1\\\\\\delta_2\\end{pmatrix} \\right) - 1 \\geq 0\n$$\n$$\n2\\delta_1 - \\delta_2 - 1 \\geq 0\n$$\n\nThe action must also be feasible. The constraints are given by $A\\delta \\leq s$, where $A=-I_2$ and $s=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. This translates to:\n$$\n-I_2 \\delta \\leq 0 \\implies -\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}\\delta_1 \\\\ \\delta_2\\end{pmatrix} \\leq \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\nThis yields the componentwise non-negativity constraints:\n$$\n-\\delta_1 \\leq 0 \\implies \\delta_1 \\geq 0\n$$\n$$\n-\\delta_2 \\leq 0 \\implies \\delta_2 \\geq 0\n$$\n\nCombining these, we have the complete constrained optimization problem:\n$$\n\\begin{aligned}\n\\text{minimize}_{\\delta \\in \\mathbb{R}^2} \\quad & f(\\delta_1, \\delta_2) = \\delta_1^2 + \\delta_2^2 \\\\\n\\text{subject to} \\quad & 2\\delta_1 - \\delta_2 - 1 \\geq 0 \\\\\n                     & \\delta_1 \\geq 0 \\\\\n                     & \\delta_2 \\geq 0\n\\end{aligned}\n$$\nTo apply standard convex optimization techniques, we rewrite the constraints in the form $g_i(\\delta) \\leq 0$:\n$$\ng_1(\\delta) = -(2\\delta_1 - \\delta_2 - 1) = 1 - 2\\delta_1 + \\delta_2 \\leq 0\n$$\n$$\ng_2(\\delta) = -\\delta_1 \\leq 0\n$$\n$$\ng_3(\\delta) = -\\delta_2 \\leq 0\n$$\nThis is a convex optimization problem, as we are minimizing a convex function over a convex feasible set. Therefore, the Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality.\n\nThe Lagrangian function $\\mathcal{L}(\\delta_1, \\delta_2, \\lambda_1, \\lambda_2, \\lambda_3)$ is:\n$$\n\\mathcal{L} = \\delta_1^2 + \\delta_2^2 + \\lambda_1(1 - 2\\delta_1 + \\delta_2) + \\lambda_2(-\\delta_1) + \\lambda_3(-\\delta_2)\n$$\nwhere $\\lambda_1, \\lambda_2, \\lambda_3$ are the Lagrange multipliers.\n\nThe KKT conditions are:\n1.  **Stationarity**: The gradient of the Lagrangian with respect to $\\delta$ must be zero.\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial \\delta_1} = 2\\delta_1 - 2\\lambda_1 - \\lambda_2 = 0 \\implies 2\\delta_1 = 2\\lambda_1 + \\lambda_2\n    $$\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial \\delta_2} = 2\\delta_2 + \\lambda_1 - \\lambda_3 = 0 \\implies 2\\delta_2 = \\lambda_3 - \\lambda_1\n    $$\n2.  **Primal Feasibility**: The constraints must be satisfied.\n    $$\n    2\\delta_1 - \\delta_2 - 1 \\geq 0\n    $$\n    $$\n    \\delta_1 \\geq 0\n    $$\n    $$\n    \\delta_2 \\geq 0\n    $$\n3.  **Dual Feasibility**: The Lagrange multipliers must be non-negative.\n    $$\n    \\lambda_1 \\geq 0, \\quad \\lambda_2 \\geq 0, \\quad \\lambda_3 \\geq 0\n    $$\n4.  **Complementary Slackness**: The product of each multiplier and its corresponding constraint must be zero.\n    $$\n    \\lambda_1(1 - 2\\delta_1 + \\delta_2) = 0\n    $$\n    $$\n    \\lambda_2(-\\delta_1) = 0 \\implies \\lambda_2\\delta_1=0\n    $$\n    $$\n    \\lambda_3(-\\delta_2) = 0 \\implies \\lambda_3\\delta_2=0\n    $$\nWe now solve this system. The objective function seeks to minimize the distance from the origin. The origin $\\delta=(0,0)$ is not in the feasible set since $2(0)-0-1 = -1 < 0$. Thus, the optimal solution must lie on the boundary of the feasible region. This means at least one constraint must be active. The recourse constraint $2\\delta_1 - \\delta_2 - 1 \\geq 0$ must be active at the optimum, because if it were inactive, we could move $\\delta$ closer to the origin, reducing the cost, a contradiction.\nSo, we have $2\\delta_1 - \\delta_2 - 1 = 0$, which implies $\\lambda_1 > 0$ (if $\\lambda_1=0$, the solution would be the origin, which is not feasible).\n\nWe analyze cases based on the other two constraints:\n- **Case 1: $\\delta_1 > 0$ and $\\delta_2 > 0$.**\n  From complementary slackness, $\\lambda_2 = 0$ and $\\lambda_3 = 0$.\n  The stationarity conditions become:\n  $2\\delta_1 = 2\\lambda_1 \\implies \\delta_1 = \\lambda_1$\n  $2\\delta_2 = -\\lambda_1 \\implies \\delta_2 = -\\lambda_1/2$\n  Since we assumed $\\lambda_1 > 0$, this implies $\\delta_2 < 0$. This contradicts the primal feasibility condition $\\delta_2 \\geq 0$. Thus, this case is impossible.\n\n- **Case 2: $\\delta_1 > 0$ and $\\delta_2 = 0$.**\n  From complementary slackness, $\\lambda_2 = 0$ and $\\lambda_3 \\geq 0$.\n  The active recourse constraint $2\\delta_1 - \\delta_2 - 1 = 0$ becomes $2\\delta_1 - 0 - 1 = 0$, which yields $\\delta_1 = 1/2$.\n  So, the candidate solution is $\\delta^{\\star} = \\begin{pmatrix}1/2 \\\\ 0\\end{pmatrix}$.\n  Let's verify if we can find valid multipliers.\n  Stationarity:\n  $2\\delta_1 = 2\\lambda_1 + \\lambda_2 \\implies 2(1/2) = 2\\lambda_1 + 0 \\implies 1 = 2\\lambda_1 \\implies \\lambda_1 = 1/2$.\n  $2\\delta_2 = \\lambda_3 - \\lambda_1 \\implies 2(0) = \\lambda_3 - 1/2 \\implies \\lambda_3 = 1/2$.\n  Check dual feasibility for the multipliers: $\\lambda_1=1/2 \\geq 0$, $\\lambda_2=0 \\geq 0$, $\\lambda_3=1/2 \\geq 0$.\n  All KKT conditions are satisfied. The point $\\delta^{\\star} = \\begin{pmatrix}1/2 \\\\ 0\\end{pmatrix}$ is a valid solution.\n\n- **Case 3: $\\delta_1 = 0$ and $\\delta_2 > 0$.**\n  From complementary slackness, $\\lambda_2 \\geq 0$ and $\\lambda_3 = 0$.\n  The active recourse constraint $2\\delta_1 - \\delta_2 - 1 = 0$ becomes $2(0) - \\delta_2 - 1 = 0$, which yields $\\delta_2 = -1$.\n  This contradicts the primal feasibility condition $\\delta_2 \\geq 0$. Thus, this case is impossible.\n\n- **Case 4: $\\delta_1 = 0$ and $\\delta_2 = 0$.**\n  This is the origin, which is not feasible as shown earlier.\n\nThe only case that satisfies all KKT conditions is Case 2. Since the problem is convex, the point found is the unique global minimum.\nThe optimal action vector is $\\delta^{\\star} = \\begin{pmatrix}1/2 \\\\ 0\\end{pmatrix}$. The minimum cost is $c(\\delta^\\star) = (1/2)^2 + 0^2 = 1/4$.\nThe question asks for the optimal vector $\\delta^\\star$.\nThe optimal components are $\\delta_1^{\\star} = 1/2$ and $\\delta_2^{\\star} = 0$.\nExpressing this as a row vector: $(\\delta_{1}^{\\star},\\delta_{2}^{\\star}) = (1/2, 0)$.\nFinal check: at $x' = x+\\delta^\\star = \\begin{pmatrix}1/2 \\\\ 0\\end{pmatrix}$, the decision argument is $w^\\top x' + \\beta = 2(1/2) - 1(0) - 1 = 1-0-1=0$. Since $0 \\ge 0$, the decision has flipped to positive as required. The cost is minimized.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & 0\n\\end{pmatrix}\n}\n$$", "id": "3132630"}]}