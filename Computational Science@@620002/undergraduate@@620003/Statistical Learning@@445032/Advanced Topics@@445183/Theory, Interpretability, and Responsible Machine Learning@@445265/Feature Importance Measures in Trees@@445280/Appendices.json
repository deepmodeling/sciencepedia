{"hands_on_practices": [{"introduction": "This first exercise takes us back to basics. Before relying on software packages, it is crucial to understand how feature importance values are actually derived. This problem guides you through a manual, first-principles calculation of both impurity-based and permutation importance on a small, hand-crafted dataset, allowing you to directly compare their mechanisms and results [@problem_id:3121084].", "problem": "Consider a binary classification task with features $X_1$, $X_2$ and label $Y \\in \\{0,1\\}$. A hand-crafted decision tree of depth $2$ is defined as follows: at the root, split on $X_1 \\leq 0$; the left child (where $X_1=0$) splits on $X_2 \\leq 0$; the right child (where $X_1=1$) is a leaf. Each leaf predicts by majority class among the training samples that reach it.\n\nYou are given a synthetic dataset of $n=12$ observations with the following counts:\n- Three observations with $(X_1,X_2,Y)=(0,0,1)$,\n- One observation with $(X_1,X_2,Y)=(0,1,0)$,\n- Three observations with $(X_1,X_2,Y)=(1,0,1)$,\n- Five observations with $(X_1,X_2,Y)=(1,1,1)$.\n\nTasks:\n1. From first principles, compute the Gini impurity reduction at the root split on $X_1$ and at the left-child split on $X_2$, and form the impurity-based feature importances by weighting each split’s impurity reduction by the proportion of samples that reach the node at which the split occurs. Repeat the same derivation using Shannon entropy (natural logarithm).\n2. Define permutation importance for a feature as the expected decrease in classification accuracy (on this same dataset) when that feature’s values are uniformly randomly permuted across the $n$ samples while keeping the trained tree fixed. Compute the permutation importance for $X_1$ and for $X_2$ exactly (do not simulate; derive the expectations analytically).\n3. Finally, compute the difference between the permutation importance of $X_1$ and that of $X_2$.\n\nExpress your final answer as a single simplified fraction. No rounding is required.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of statistical learning, is well-posed with a clear and complete set of data and definitions, and is expressed in objective, formal language. There are no contradictions, ambiguities, or unsound premises. We may proceed with the solution.\n\nThe decision tree structure and dataset are as follows:\n- The total number of samples is $n=12$.\n- Data counts:\n  - $3$ samples: $(X_1=0, X_2=0, Y=1)$\n  - $1$ sample: $(X_1=0, X_2=1, Y=0)$\n  - $3$ samples: $(X_1=1, X_2=0, Y=1)$\n  - $5$ samples: $(X_1=1, X_2=1, Y=1)$\n- Tree structure:\n  - Root node: Splits on $X_1 \\leq 0$. Since $X_1$ only takes values $0$ and $1$, this separates samples based on $X_1=0$ versus $X_1=1$.\n  - Left child (for $X_1=0$): Splits on $X_2 \\leq 0$, separating samples based on $X_2=0$ versus $X_2=1$.\n  - Right child (for $X_1=1$): This is a leaf node.\n- Leaf node predictions are by majority class of the samples in that leaf.\n\nLet's populate the tree with the training data to determine the leaf predictions.\n- **Root Node (Node 0):** Contains all $n=12$ samples.\n  - Class counts: $N_0 = 1$ (for $Y=0$), $N_1 = 11$ (for $Y=1$).\n- **Split at Root on $X_1$:**\n  - **Left Child (Node 1, $X_1=0$):** Contains $3+1=4$ samples.\n    - Class counts: $N_0 = 1$, $N_1 = 3$.\n  - **Right Child (Node 2, $X_1=1$):** Contains $3+5=8$ samples. This is a leaf.\n    - Class counts: $N_0 = 0$, $N_1 = 8$. The majority class is $1$. So, this leaf predicts $\\hat{Y}=1$.\n- **Split at Node 1 on $X_2$:**\n  - **Left-Left Child (Node 3, $X_1=0, X_2=0$):** Contains $3$ samples. This is a leaf.\n    - Class counts: $N_0 = 0$, $N_1 = 3$. The majority class is $1$. So, this leaf predicts $\\hat{Y}=1$.\n  - **Left-Right Child (Node 4, $X_1=0, X_2=1$):** Contains $1$ sample. This is a leaf.\n    - Class counts: $N_0 = 1$, $N_1 = 0$. The majority class is $0$. So, this leaf predicts $\\hat{Y}=0$.\n\nThe fixed tree's prediction rule is:\n- If $X_1=1$, predict $\\hat{Y}=1$.\n- If $X_1=0$ and $X_2=0$, predict $\\hat{Y}=1$.\n- If $X_1=0$ and $X_2=1$, predict $\\hat{Y}=0$.\n\n**1. Impurity-Based Feature Importance**\n\nThis part is computed as requested, but is not used for the final answer which concerns permutation importance.\n\n**Gini Impurity:** The Gini impurity of a node with class proportions $p_k$ is $I_G = 1 - \\sum_k p_k^2$. The impurity reduction is $\\Delta I_G = I_{G,parent} - \\sum_{c \\in children} \\frac{N_c}{N_{parent}} I_{G,c}$. The feature importance is the sum of weighted impurity reductions for that feature: $Imp(F) = \\sum_{s \\in \\text{splits on } F} \\frac{N_s}{N} \\Delta I_G(s)$.\n\n- **Root split on $X_1$:**\n  - $I_G(\\text{Root}): p_0=\\frac{1}{12}, p_1=\\frac{11}{12} \\implies I_G(\\text{Root}) = 1 - ((\\frac{1}{12})^2 + (\\frac{11}{12})^2) = 1 - \\frac{1+121}{144} = \\frac{22}{144} = \\frac{11}{72}$.\n  - $I_G(\\text{Node 1, } X_1=0): N=4, p_0=\\frac{1}{4}, p_1=\\frac{3}{4} \\implies I_G(\\text{Node 1}) = 1 - ((\\frac{1}{4})^2 + (\\frac{3}{4})^2) = 1 - \\frac{1+9}{16} = \\frac{6}{16} = \\frac{3}{8}$.\n  - $I_G(\\text{Node 2, } X_1=1): N=8, p_0=0, p_1=1 \\implies I_G(\\text{Node 2}) = 1 - (0^2+1^2) = 0$.\n  - Impurity reduction: $\\Delta I_G(\\text{root}) = \\frac{11}{72} - (\\frac{4}{12} \\cdot \\frac{3}{8} + \\frac{8}{12} \\cdot 0) = \\frac{11}{72} - \\frac{1}{8} = \\frac{11-9}{72} = \\frac{2}{72} = \\frac{1}{36}$.\n  - Importance of $X_1$: $Imp_G(X_1) = \\frac{12}{12} \\cdot \\Delta I_G(\\text{root}) = \\frac{1}{36}$.\n\n- **Node 1 split on $X_2$:**\n  - $I_G(\\text{Node 1}) = \\frac{3}{8}$.\n  - $I_G(\\text{Node 3, } X_1=0, X_2=0): N=3, p_1=1 \\implies I_G(\\text{Node 3}) = 0$.\n  - $I_G(\\text{Node 4, } X_1=0, X_2=1): N=1, p_0=1 \\implies I_G(\\text{Node 4}) = 0$.\n  - Impurity reduction: $\\Delta I_G(\\text{Node 1}) = \\frac{3}{8} - (\\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 0) = \\frac{3}{8}$.\n  - Importance of $X_2$: $Imp_G(X_2) = \\frac{4}{12} \\cdot \\Delta I_G(\\text{Node 1}) = \\frac{1}{3} \\cdot \\frac{3}{8} = \\frac{1}{8}$.\n\n**Shannon Entropy:** The entropy is $I_H = -\\sum_k p_k \\ln(p_k)$.\n- **Root split on $X_1$:**\n  - $I_H(\\text{Root}) = -(\\frac{1}{12}\\ln\\frac{1}{12} + \\frac{11}{12}\\ln\\frac{11}{12}) = \\frac{\\ln(12) + 11 \\ln(12/11)}{12}$.\n  - $I_H(\\text{Node 1}) = -(\\frac{1}{4}\\ln\\frac{1}{4} + \\frac{3}{4}\\ln\\frac{3}{4}) = \\frac{\\ln(4) + 3\\ln(4/3)}{4}$.\n  - $I_H(\\text{Node 2}) = 0$.\n  - Information gain: $\\Delta I_H(\\text{root}) = I_H(\\text{Root}) - \\frac{4}{12}I_H(\\text{Node 1}) = \\frac{16\\ln(2)+15\\ln(3) - 11\\ln(11)}{12}$.\n  - Importance of $X_1$: $Imp_H(X_1) = \\frac{12}{12} \\Delta I_H(\\text{root}) = \\frac{16\\ln(2)+15\\ln(3) - 11\\ln(11)}{12}$.\n\n- **Node 1 split on $X_2$:**\n  - $I_H(\\text{Node 1}) = \\frac{\\ln(4) + 3\\ln(4/3)}{4}$.\n  - $I_H(\\text{Node 3}) = 0$ and $I_H(\\text{Node 4}) = 0$.\n  - Information gain: $\\Delta I_H(\\text{Node 1}) = I_H(\\text{Node 1}) = \\frac{2\\ln(2) - (3/4)(\\ln(3)-2\\ln(2))}{1} = \\frac{8\\ln(2)-3\\ln(3)}{4}$.\n  - Importance of $X_2$: $Imp_H(X_2) = \\frac{4}{12} \\Delta I_H(\\text{Node 1}) = \\frac{1}{3} \\frac{8\\ln(2)-3\\ln(3)}{4} = \\frac{8\\ln(2)-3\\ln(3)}{12}$.\n\n**2. Permutation Importance**\n\nPermutation importance for a feature $X_j$ is defined as $PI(X_j) = \\text{accuracy}_{\\text{original}} - E[\\text{accuracy}_{\\text{permuted}}]$.\n\nFirst, we compute the baseline accuracy on the original dataset.\n- For $(X_1=0, X_2=0, Y=1)$ ($3$ samples): prediction is $\\hat{Y}=1$. Correct.\n- For $(X_1=0, X_2=1, Y=0)$ ($1$ sample): prediction is $\\hat{Y}=0$. Correct.\n- For $(X_1=1, X_2=0, Y=1)$ ($3$ samples): prediction is $\\hat{Y}=1$. Correct.\n- For $(X_1=1, X_2=1, Y=1)$ ($5$ samples): prediction is $\\hat{Y}=1$. Correct.\nAll $12$ samples are classified correctly. Thus, $\\text{accuracy}_{\\text{original}} = \\frac{12}{12} = 1$.\n\nThe expected accuracy under permutation is $E[\\text{accuracy}] = \\frac{1}{n} \\sum_{i=1}^{n} P(\\hat{y}_i = y_i)$.\n\n**Permutation Importance of $X_1$**\nThe vector of $X_1$ values has $4$ zeros and $8$ ones. When permuted, for any sample $i$, the new value $X'_{1,i}$ has a probability $P(X'_{1,i}=0) = \\frac{4}{12} = \\frac{1}{3}$ and $P(X'_{1,i}=1) = \\frac{8}{12} = \\frac{2}{3}$. The values of $X_2$ and $Y$ for each sample remain fixed.\nWe calculate the probability of a correct prediction for each sample.\n- **$6$ samples with $X_2=0, Y=1$** (from groups $(0,0,1)$ and $(1,0,1)$):\n  - The prediction depends on $X'_{1,i}$.\n  - If $X'_{1,i}=0$ (prob $\\frac{1}{3}$), predict $\\hat{Y}=1$. Correct.\n  - If $X'_{1,i}=1$ (prob $\\frac{2}{3}$), predict $\\hat{Y}=1$. Correct.\n  - The prediction is always correct. $P(\\text{correct}) = 1$.\n- **$1$ sample with $X_2=1, Y=0$**:\n  - If $X'_{1,i}=0$ (prob $\\frac{1}{3}$), predict $\\hat{Y}=0$. Correct.\n  - If $X'_{1,i}=1$ (prob $\\frac{2}{3}$), predict $\\hat{Y}=1$. Incorrect.\n  - $P(\\text{correct}) = \\frac{1}{3}$.\n- **$5$ samples with $X_2=1, Y=1$**:\n  - If $X'_{1,i}=0$ (prob $\\frac{1}{3}$), predict $\\hat{Y}=0$. Incorrect.\n  - If $X'_{1,i}=1$ (prob $\\frac{2}{3}$), predict $\\hat{Y}=1$. Correct.\n  - $P(\\text{correct}) = \\frac{2}{3}$.\n\nThe expected number of correct predictions is $6 \\cdot 1 + 1 \\cdot \\frac{1}{3} + 5 \\cdot \\frac{2}{3} = 6 + \\frac{1}{3} + \\frac{10}{3} = 6 + \\frac{11}{3} = \\frac{18+11}{3} = \\frac{29}{3}$.\nThe expected accuracy is $E[\\text{accuracy}_{\\text{permuted } X_1}] = \\frac{29/3}{12} = \\frac{29}{36}$.\nThe permutation importance of $X_1$ is $PI(X_1) = 1 - \\frac{29}{36} = \\frac{7}{36}$.\n\n**Permutation Importance of $X_2$**\nThe vector of $X_2$ values has $3+3=6$ zeros and $1+5=6$ ones. When permuted, for any sample $i$, the new value $X'_{2,i}$ has probability $P(X'_{2,i}=0) = \\frac{6}{12} = \\frac{1}{2}$ and $P(X'_{2,i}=1) = \\frac{6}{12} = \\frac{1}{2}$. The values of $X_1$ and $Y$ remain fixed.\n- **$4$ samples with $X_1=0$**:\n  - $3$ samples have $Y=1$. Prediction is $\\hat{Y}=1$ if $X'_{2,i}=0$ (prob $\\frac{1}{2}$), and $\\hat{Y}=0$ if $X'_{2,i}=1$ (prob $\\frac{1}{2}$). $P(\\text{correct}) = \\frac{1}{2}$.\n  - $1$ sample has $Y=0$. Prediction is $\\hat{Y}=1$ if $X'_{2,i}=0$ (prob $\\frac{1}{2}$), and $\\hat{Y}=0$ if $X'_{2,i}=1$ (prob $\\frac{1}{2}$). $P(\\text{correct}) = \\frac{1}{2}$.\n- **$8$ samples with $X_1=1$**:\n  - All $8$ samples have $Y=1$. The tree predicts $\\hat{Y}=1$ for any sample with $X_1=1$, regardless of $X_2$. The prediction is always correct. $P(\\text{correct}) = 1$.\n\nThe expected number of correct predictions is $3 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} + 8 \\cdot 1 = \\frac{4}{2} + 8 = 2+8=10$.\nThe expected accuracy is $E[\\text{accuracy}_{\\text{permuted } X_2}] = \\frac{10}{12} = \\frac{5}{6}$.\nThe permutation importance of $X_2$ is $PI(X_2) = 1 - \\frac{5}{6} = \\frac{1}{6}$.\n\n**3. Final Calculation**\n\nThe final task is to compute the difference between the permutation importance of $X_1$ and that of $X_2$.\n$PI(X_1) - PI(X_2) = \\frac{7}{36} - \\frac{1}{6} = \\frac{7}{36} - \\frac{6}{36} = \\frac{1}{36}$.", "answer": "$$\\boxed{\\frac{1}{36}}$$", "id": "3121084"}, {"introduction": "Moving from a single tree to a Random Forest introduces new dynamics, especially regarding how importance is aggregated. This coding practice challenges you to implement the widely-used mean decrease in impurity (MDI) importance and observe its behavior in tricky, yet common, scenarios involving non-informative constant features and perfectly correlated predictors [@problem_id:3166180]. The results highlight how MDI distributes importance, a key consideration for correct interpretation.", "problem": "You are asked to implement, from first principles, a binary-classification Random Forest based on Classification and Regression Trees (CART), using the Gini impurity as the split criterion, and to compute normalized feature importance scores defined as the mean decrease in impurity. Your implementation must handle edge cases where some features are constant (zero variance). You will then run the implementation on three specified test cases and report a compact numerical summary.\n\nDefinitions and requirements:\n- A decision tree is grown recursively. At each internal node with data indices $\\mathcal{I}$, you must:\n  1) Randomly select a subset of features of size $m$ (without replacement) from the $d$ available features, independently at each node.\n  2) For each selected feature $j$, consider all candidate thresholds formed as midpoints between sorted unique values of that feature among the samples in $\\mathcal{I}$. If a feature has only a single unique value, it cannot be split at this node.\n  3) For a candidate threshold $\\tau$, define the left child as the set of indices with $x_{ij} \\le \\tau$ and the right child as the set of indices with $x_{ij} > \\tau$. Compute the weighted Gini impurity of this split and choose the feature and threshold that maximizes the impurity decrease. If the best impurity decrease is non-positive, make the node a leaf.\n- For any node with index set $\\mathcal{I}$ and class labels $y_i \\in \\{0,1\\}$, the Gini impurity is $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$, where $p_k$ is the class proportion in $\\mathcal{I}$.\n- For a split of $\\mathcal{I}$ into left $\\mathcal{L}$ and right $\\mathcal{R}$ using feature $j$ and threshold $\\tau$, the impurity decrease contributed by that split is\n$$\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R}).$$\n- A Random Forest with $T$ trees is constructed by training each tree on a bootstrap sample of size $N$ (sampling with replacement) from the original training set, and applying the node-wise random feature subspace selection of size $m$.\n- The feature importance for feature $j$ over one tree is the sum, over all internal nodes where the best split uses feature $j$, of $\\Delta(\\mathcal{I}, j, \\tau)$ weighted by the number of samples $|\\mathcal{I}|$ at that node. Aggregate this sum over all trees. Let $S$ denote the total across all features of these aggregated, sample-weighted decreases. If $S > 0$, define the normalized importance of feature $j$ as $I_j = \\frac{\\text{aggregated decrease for } j}{S}$ so that $\\sum_{j=1}^{d} I_j = 1$. If $S = 0$, define $I_j = 0$ for all $j$.\n- There is no restriction on tree depth other than the implicit stopping condition when no valid split yields positive impurity decrease.\n\nImplementation constraints:\n- You must implement the above procedure exactly as stated, using pure numerical operations. No external machine learning libraries are allowed.\n- All randomness must be made reproducible using the specified seeds.\n\nDatasets and test suite:\nYou will run your implementation on three cases. In each case, construct the feature matrix $X \\in \\mathbb{R}^{N \\times d}$ and labels $y \\in \\{0,1\\}^N$ as specified, then train a Random Forest with the specified parameters, and compute the normalized feature importances $\\{I_j\\}_{j=1}^d$.\n\nCase $1$ (constant and noise features with a single informative feature):\n- Data generation seed: $42$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = 0$ for all $i$ (a constant feature).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently for all $i$.\n- Define labels $y_i = 1$ if $x_{i1} > 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 2$, forest randomness seed $2024$.\n- Compute normalized importances $(I_1, I_2, I_3)$.\n\nCase $2$ (perfectly duplicated informative feature and one noise feature):\n- Data generation seed: $123$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = x_{i1}$ for all $i$ (a perfect duplicate).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently.\n- Define labels $y_i = 1$ if $x_{i1} > 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 1$, forest randomness seed $2025$.\n- Compute normalized importances $(I_1, I_2, I_3)$, and also compute the duplicate-share statistic $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$.\n\nCase $3$ (all features constant):\n- Data generation seed: $7$.\n- $N = 100$, $d = 2$.\n- Set $x_{i1} = 0$ and $x_{i2} = 3$ for all $i$ (both features constant).\n- Generate labels $y_i$ as independent Bernoulli with $P(y_i = 1) = 0.5$ using the given seed.\n- Random Forest parameters: number of trees $T = 10$, node-wise feature subset size $m = 2$, forest randomness seed $99$.\n- Compute normalized importances $(I_1, I_2)$ and their sum $Q = I_1 + I_2$.\n\nNumerical reporting:\n- For Case $1$, report the three floats $I_1$, $I_2$, $I_3$.\n- For Case $2$, report the four floats $S_{\\text{dup}}$, $I_1$, $I_2$, $I_3$.\n- For Case $3$, report the three floats $Q$, $I_1$, $I_2$.\n- Round each reported float to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing all the reported results as a comma-separated list enclosed in square brackets, in the following order:\n$[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, S_{\\text{dup}}^{(2)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, Q^{(3)}, I_1^{(3)}, I_2^{(3)}]$,\nwhere the superscripts indicate the case index. There are no physical units. Angles are not used. All proportions must be reported as decimals, not as percentages.", "solution": "The user has specified the task of implementing a Random Forest classifier for binary classification from first principles. The implementation must adhere to a precise set of definitions for Classification and Regression Trees (CART), Gini impurity, node splitting, and a specific method for calculating feature importance based on the mean decrease in impurity. The implementation will be validated against three distinct test cases, and a set of numerical results must be reported in a specific format.\n\n### Problem Validation\n\nFirst, I will validate the problem statement against the required criteria.\n\n**Step 1: Extract Givens**\n\n- **Algorithm:** A Random Forest composed of `T` Classification and Regression Trees (CART) for binary classification (`y \\in \\{0,1\\}`).\n- **Tree Construction (Recursion):**\n    - At each node `\\mathcal{I}`, randomly select `m` features (from `d` total) without replacement.\n    - For each selected feature `j`, find the optimal split.\n    - **Thresholds (`\\tau`):** Midpoints between sorted unique values of feature `j` among samples in `\\mathcal{I}`.\n    - **Split Rule:** Left child `\\mathcal{L} = \\{i \\in \\mathcal{I} | x_{ij} \\le \\tau\\}`, Right child `\\mathcal{R} = \\{i \\in \\mathcal{I} | x_{ij} > \\tau\\}`.\n    - **Stopping Conditions:** A node becomes a leaf if the best impurity decrease is non-positive, or if no valid splits can be made (e.g., all features at the node are constant).\n- **Split Criterion (Gini Impurity):**\n    - **Impurity of a node `\\mathcal{I}`:** `G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2`, where `p_k` is the proportion of class `k`.\n    - **Impurity Decrease (Gini Gain):** `\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R})`. The split with maximum `\\Delta` is chosen.\n- **Ensemble Method (Random Forest):**\n    - Each of the `T` trees is trained on a bootstrap sample of size `N` (sampling with replacement) from the training data of size `N`.\n- **Feature Importance:**\n    - For a single tree and feature `j`, importance is the sum of `|\\mathcal{I}| \\cdot \\Delta(\\mathcal{I}, j, \\tau)` over all nodes where `j` was the best split feature.\n    - Total importance for feature `j` is the sum aggregated over all `T` trees.\n    - **Normalization:** Let `S` be the sum of aggregated importances over all features. Normalized importance `I_j` is `\\frac{\\text{aggregated decrease for } j}{S}` if `S > 0`, and `I_j = 0` if `S = 0`.\n- **Test Cases & Parameters:**\n    - **Case 1:** Data seed=`42`, `N=200`, `d=3`. `x_1 \\sim \\mathcal{N}(0,1)`, `x_2=0`, `x_3 \\sim \\mathcal{N}(0,1)`. `y=1` if `x_1 > 0`, else `0`. RF params: `T=50`, `m=2`, forest seed=`2024`. Report: `(I_1, I_2, I_3)`.\n    - **Case 2:** Data seed=`123`, `N=200`, `d=3`. `x_1 \\sim \\mathcal{N}(0,1)`, `x_2=x_1`, `x_3 \\sim \\mathcal{N}(0,1)`. `y=1` if `x_1 > 0`, else `0`. RF params: `T=50`, `m=1`, forest seed=`2025`. Report: `(S_{\\text{dup}}, I_1, I_2, I_3)` where `S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}`.\n    - **Case 3:** Data seed=`7`, `N=100`, `d=2`. `x_1=0`, `x_2=3`. `y \\sim \\text{Bernoulli}(0.5)`. RF params: `T=10`, `m=2`, forest seed=`99`. Report: `(Q, I_1, I_2)` where `Q = I_1 + I_2`.\n- **Output:** A single comma-separated list of `10` floating-point numbers rounded to six decimal places, enclosed in brackets.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem describes the standard Random Forest algorithm. The CART methodology, Gini impurity criterion, and mean decrease in impurity (MDI) for feature importance are all well-established concepts in statistical learning. The definitions provided are mathematically and algorithmically correct.\n- **Well-Posed:** The problem is meticulously specified. All parameters (`N`, `d`, `T`, `m`), data generation procedures, and random seeds are provided, which ensures that the result is unique, deterministic, and computable. The edge cases (constant features, no positive gain) are explicitly defined.\n- **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology.\n\nThe problem does not violate any of the invalidity criteria. It is a well-defined, scientifically sound computational task.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the implementation and solution.\n\n### Algorithmic Implementation\n\nThe solution will be structured as a set of functions that collectively implement the specified Random Forest algorithm.\n\n1.  **`gini_impurity(y)`**: A helper function to compute the Gini impurity for a given array of labels `y`.\n2.  **`find_best_split(X, y, idxs, feature_subset)`**: This function iterates through a given subset of features. For each feature, it evaluates all valid thresholds (midpoints of unique values) to find the split that maximizes the Gini gain. It returns a dictionary containing the details of the best split found (feature index, threshold, gain, and child indices) or `None` if no valid split is found.\n3.  **`grow_tree(X, y, idxs, m, d, rng, tree_importances)`**: A recursive function that builds a single decision tree. At each step (node), it checks for stopping conditions (e.g., pure node). If not a leaf, it randomly selects `m` features, calls `find_best_split` to determine the optimal split, records the contribution to feature importance (`|\\mathcal{I}| \\cdot \\Delta`), and then recursively calls itself for the left and right children.\n4.  **`run_rf_case(X, y, T, m, forest_seed)`**: This function orchestrates the training of the entire Random Forest for a single test case. It initializes a random number generator with the forest seed. It then iterates `T` times, each time creating a bootstrap sample of the data and growing a tree using `grow_tree`. It aggregates the feature importances from all trees and returns the final normalized importance scores.\n5.  **`solve()`**: The main function that prepares the data for each of the three test cases as specified, calls `run_rf_case` with the appropriate parameters, computes the required summary statistics (`S_{\\text{dup}}`, `Q`), and prints the final combined results in the specified format.\n\nRandomness is managed carefully using `numpy.random.default_rng` to ensure reproducibility for data generation and all stochastic aspects of the algorithm (bootstrapping and feature selection). Special attention is given to the edge cases, such as handling constant features (which cannot be split) and normalizing importances only if the total impurity decrease is positive.", "answer": "```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Computes the Gini impurity of a set of labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    proportions = counts / n_samples\n    return 1.0 - np.sum(proportions**2)\n\ndef find_best_split(X, y, idxs, feature_subset):\n    \"\"\"Finds the best split for a node by iterating through features and thresholds.\"\"\"\n    X_node, y_node = X[idxs], y[idxs]\n    n_node = len(y_node)\n    \n    if n_node <= 1:\n        return None\n\n    g_parent = gini_impurity(y_node)\n    best_gain = -1.0\n    best_split_info = None\n\n    for j in feature_subset:\n        feature_values = X_node[:, j]\n        unique_vals = np.unique(feature_values)\n\n        if len(unique_vals) <= 1:\n            continue\n\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n        for tau in thresholds:\n            left_mask = feature_values <= tau\n            right_mask = ~left_mask\n\n            y_left, y_right = y_node[left_mask], y_node[right_mask]\n\n            if len(y_left) == 0 or len(y_right) == 0:\n                continue\n\n            g_left = gini_impurity(y_left)\n            g_right = gini_impurity(y_right)\n            n_left, n_right = len(y_left), len(y_right)\n            \n            w_left = n_left / n_node\n            w_right = n_right / n_node\n            \n            gain = g_parent - (w_left * g_left + w_right * g_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = {\n                    'feature': j, \n                    'threshold': tau, \n                    'gain': gain,\n                    'left_idxs': idxs[left_mask],\n                    'right_idxs': idxs[right_mask]\n                }\n                \n    return best_split_info\n\ndef grow_tree(X, y, idxs, m, d, rng, tree_importances):\n    \"\"\"Recursively grows a single decision tree and accumulates feature importances.\"\"\"\n    if len(np.unique(y[idxs])) == 1:\n        return\n\n    m_node = min(m, d)\n    feature_subset = rng.choice(d, size=m_node, replace=False)\n    \n    best_split = find_best_split(X, y, idxs, feature_subset)\n    \n    if best_split is None or best_split['gain'] <= 0:\n        return\n\n    j = best_split['feature']\n    gain = best_split['gain']\n    n_samples = len(idxs)\n    tree_importances[j] += n_samples * gain\n    \n    left_idxs = best_split['left_idxs']\n    right_idxs = best_split['right_idxs']\n    \n    grow_tree(X, y, left_idxs, m, d, rng, tree_importances)\n    grow_tree(X, y, right_idxs, m, d, rng, tree_importances)\n\ndef run_rf_case(X, y, T, m, forest_seed):\n    \"\"\"Runs the Random Forest algorithm for a given case and computes feature importances.\"\"\"\n    N, d = X.shape\n    forest_rng = np.random.default_rng(forest_seed)\n    total_importances = np.zeros(d)\n\n    for _ in range(T):\n        bootstrap_idxs = forest_rng.choice(N, size=N, replace=True)\n        X_sample, y_sample = X[bootstrap_idxs], y[bootstrap_idxs]\n        \n        tree_importances = np.zeros(d)\n        \n        # Grow a tree on the bootstrap sample\n        initial_idxs = np.arange(len(y_sample))\n        grow_tree(X_sample, y_sample, initial_idxs, m, d, forest_rng, tree_importances)\n        \n        total_importances += tree_importances\n\n    S = np.sum(total_importances)\n    if S > 0:\n        normalized_importances = total_importances / S\n    else:\n        normalized_importances = np.zeros(d)\n        \n    return normalized_importances\n\ndef solve():\n    \"\"\"Generates data for three cases, runs the RF, and reports the results.\"\"\"\n    all_results = []\n\n    # Case 1\n    data_seed, N, d = 42, 200, 3\n    T, m, forest_seed = 50, 2, 2024\n    rng_case1 = np.random.default_rng(data_seed)\n    X1 = np.zeros((N, d))\n    X1[:, 0] = rng_case1.normal(size=N)\n    X1[:, 1] = 0.0\n    X1[:, 2] = rng_case1.normal(size=N)\n    y1 = (X1[:, 0] > 0).astype(int)\n    importances1 = run_rf_case(X1, y1, T, m, forest_seed)\n    all_results.extend(importances1)\n\n    # Case 2\n    data_seed, N, d = 123, 200, 3\n    T, m, forest_seed = 50, 1, 2025\n    rng_case2 = np.random.default_rng(data_seed)\n    X2 = np.zeros((N, d))\n    X2[:, 0] = rng_case2.normal(size=N)\n    X2[:, 1] = X2[:, 0]\n    X2[:, 2] = rng_case2.normal(size=N)\n    y2 = (X2[:, 0] > 0).astype(int)\n    importances2 = run_rf_case(X2, y2, T, m, forest_seed)\n    I1_2, I2_2, I3_2 = importances2\n    denom = I1_2 + I2_2 + I3_2\n    S_dup = (I1_2 + I2_2) / denom if denom > 0 else 0.0\n    all_results.extend([S_dup, I1_2, I2_2, I3_2])\n\n    # Case 3\n    data_seed, N, d = 7, 100, 2\n    T, m, forest_seed = 10, 2, 99\n    rng_case3 = np.random.default_rng(data_seed)\n    X3 = np.zeros((N, d))\n    X3[:, 0] = 0.0\n    X3[:, 1] = 3.0\n    y3 = rng_case3.integers(0, 2, size=N)\n    importances3 = run_rf_case(X3, y3, T, m, forest_seed)\n    I1_3, I2_3 = importances3\n    Q = I1_3 + I2_3\n    all_results.extend([Q, I1_3, I2_3])\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3166180"}, {"introduction": "Feature importance scores can sometimes give a misleading sense of precision, but how much do these values depend on the model's own configuration? This practice explores the stability of feature importance by systematically varying a tree's hyperparameters, such as its maximum depth $d_{\\max}$ and minimum leaf size $\\ell_{\\min}$ [@problem_id:3121059]. By quantifying the sensitivity of importance scores, you will develop a more critical perspective and appreciate the concept of creating more robust importance estimates.", "problem": "You are given the task of implementing a deterministic regression tree learner to study the sensitivity of feature importance estimates to tree hyperparameters and to construct a hyperparameter-robust importance estimate via averaging across configurations. All computations must be performed from first principles using empirical risk minimization with axis-aligned splits.\n\nDefinitions and setup:\n- Consider a dataset with $n$ samples, features $X \\in \\mathbb{R}^{n \\times p}$ and targets $y \\in \\mathbb{R}^{n}$. A regression tree recursively partitions the input space using axis-aligned thresholds. At any node containing a sample index set $S \\subseteq \\{1,\\dots,n\\}$, define the impurity as the sum of squared deviations\n$$\n\\mathcal{I}(S) \\triangleq \\sum_{i \\in S} \\left(y_i - \\bar{y}_S\\right)^2,\\quad \\bar{y}_S \\triangleq \\frac{1}{|S|}\\sum_{i \\in S} y_i.\n$$\n- A split on feature $j \\in \\{1,\\dots,p\\}$ at threshold $t \\in \\mathbb{R}$ creates two child sets\n$$\nS_L(j,t) \\triangleq \\{i \\in S: X_{ij} \\le t\\}, \\quad S_R(j,t) \\triangleq \\{i \\in S: X_{ij} > t\\}.\n$$\n- The impurity decrease achieved by such a split is\n$$\n\\Delta \\mathcal{I}(S;j,t) \\triangleq \\mathcal{I}(S) - \\left(\\mathcal{I}(S_L(j,t)) + \\mathcal{I}(S_R(j,t))\\right).\n$$\n\nHyperparameters to control the tree:\n- Maximum depth $d_{\\max} \\in \\mathbb{N}$, where the root has depth $0$ and a node at depth $d$ can only split if $d < d_{\\max}$.\n- Minimum samples per leaf $\\ell_{\\min} \\in \\mathbb{N}$, where a proposed split $(j,t)$ is admissible only if $|S_L(j,t)| \\ge \\ell_{\\min}$ and $|S_R(j,t)| \\ge \\ell_{\\min}$.\n- Maximum features $m \\in \\mathbb{N}$, which limits the set of features that can be considered at each split deterministically to the first $m$ features by ascending index, i.e., $\\{1,2,\\dots,\\min(m,p)\\}$. There is no randomness in feature selection.\n\nTree-building rule:\n- At each eligible node, among all admissible splits $(j,t)$ using features $j \\in \\{1,2,\\dots,\\min(m,p)\\}$ and thresholds $t$ chosen as midpoints between consecutive distinct feature values observed in $S$, choose the split that maximizes $\\Delta \\mathcal{I}(S;j,t)$. If there are ties in $\\Delta \\mathcal{I}$, break ties by selecting the smallest feature index $j$, and if still tied, the smallest threshold $t$. If no admissible split yields strictly positive impurity decrease, stop splitting and declare a leaf.\n\nFeature importance:\n- For a fixed hyperparameter configuration $\\theta = (d_{\\max}, \\ell_{\\min}, m)$, define the unnormalized feature importance for feature $j$ as\n$$\n\\widehat{I}_j(\\theta) \\triangleq \\sum_{\\text{splits on } j} \\Delta \\mathcal{I}(S;j,t),\n$$\nthe sum of impurity decreases over all internal nodes that split on feature $j$.\n- Normalize per-tree importances for comparability across hyperparameters:\n$$\n\\widetilde{I}_j(\\theta) \\triangleq \\begin{cases}\n\\frac{\\widehat{I}_j(\\theta)}{\\sum_{k=1}^p \\widehat{I}_k(\\theta)}, & \\text{if } \\sum_{k=1}^p \\widehat{I}_k(\\theta) > 0,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n- Given a finite set of hyperparameter configurations $\\Theta$, define the hyperparameter-robust importance by averaging across configurations,\n$$\n\\overline{I}_j \\triangleq \\frac{1}{|\\Theta|} \\sum_{\\theta \\in \\Theta} \\widetilde{I}_j(\\theta),\n$$\nand the sensitivity as the standard deviation across configurations,\n$$\n\\sigma_j \\triangleq \\sqrt{\\frac{1}{|\\Theta|} \\sum_{\\theta \\in \\Theta} \\left(\\widetilde{I}_j(\\theta) - \\overline{I}_j\\right)^2 }.\n$$\n\nDeterminism and numerical details:\n- There must be no randomness; the selection of features and thresholds is deterministic as specified.\n- Threshold candidates for feature $j$ at node $S$ are midpoints between sorted unique values of $\\{X_{ij}: i \\in S\\}$.\n- All improvements and sums are computed exactly using finite-precision arithmetic. Treat an impurity decrease as strictly positive if $\\Delta \\mathcal{I}(S;j,t) > 0$.\n- When reporting numbers, round to $6$ decimals.\n\nTest suite:\nImplement the above for the following three test cases. For each test case, compute the vector $\\overline{I} = (\\overline{I}_1, \\dots, \\overline{I}_p)$ and the vector $\\sigma = (\\sigma_1, \\dots, \\sigma_p)$.\n\n- Test case $1$:\n  - Data with $n = 12$ and $p = 3$:\n    - Feature matrix $X \\in \\mathbb{R}^{12 \\times 3}$ with rows $(x_{i1}, x_{i2}, x_{i3})$ given by\n      - For $i \\in \\{1,2,3,4\\}$: $x_{i1} = 0$, $x_{i2} \\in \\{0,1,2,3\\}$ in order, $x_{i3} \\in \\{1,0,1,0\\}$ in order.\n      - For $i \\in \\{5,6,7,8\\}$: $x_{i1} = 1$, $x_{i2} \\in \\{0,1,2,3\\}$ in order, $x_{i3} \\in \\{1,0,1,0\\}$ in order.\n      - For $i \\in \\{9,10,11,12\\}$: $x_{i1} = 2$, $x_{i2} \\in \\{0,1,2,3\\}$ in order, $x_{i3} \\in \\{1,0,1,0\\}$ in order.\n    - Target vector $y \\in \\mathbb{R}^{12}$ defined by $y_i = 3 \\cdot x_{i1} + 2 \\cdot \\mathbf{1}\\{x_{i2} \\ge 1.5\\} + 0.1 \\cdot x_{i3}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n  - Hyperparameter grid $\\Theta_1$:\n    - $d_{\\max} \\in \\{1,2,3\\}$,\n    - $\\ell_{\\min} \\in \\{1,2\\}$,\n    - $m \\in \\{1,2,3\\}$.\n\n- Test case $2$:\n  - Data with $n = 4$ and $p = 2$:\n    - $X$ rows are $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$.\n    - $y$ entries are $0$, $1$, $1$, $0$.\n  - Hyperparameter grid $\\Theta_2$:\n    - $d_{\\max} \\in \\{1\\}$,\n    - $\\ell_{\\min} \\in \\{1,2\\}$,\n    - $m \\in \\{1,2\\}$.\n\n- Test case $3$:\n  - Data with $n = 6$ and $p = 3$:\n    - $X$ rows are $(0,0,0)$, $(1,0,0)$, $(0,1,0)$, $(1,1,0)$, $(0,0,1)$, $(1,0,1)$.\n    - $y$ entries are $0$, $0$, $0$, $0$, $5$, $5$.\n  - Hyperparameter grid $\\Theta_3$:\n    - $d_{\\max} \\in \\{1,2\\}$,\n    - $\\ell_{\\min} \\in \\{1,2\\}$,\n    - $m \\in \\{1,2,3\\}$.\n\nProgram requirements:\n- Implement the deterministic regression tree learner with the splitting and tie-breaking rules above.\n- For each test case, compute $\\widetilde{I}_j(\\theta)$ for all $\\theta$ in the specified grid, then compute $\\overline{I}_j$ and $\\sigma_j$.\n- Round all outputs to $6$ decimals.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets of length $3$ (one entry per test case). Each entry must itself be a list of two lists: the first is $\\overline{I}$ and the second is $\\sigma$, both in feature index order. For example: \n\"[[[a11,a12,...],[s11,s12,...]],[[a21,a22,...],[s21,s22,...]],[[a31,a32,...],[s31,s32,...]]]\"\n- All numbers must be rounded to $6$ decimals. No other text should be printed.", "solution": "The user has provided a well-defined computational problem in the domain of statistical learning. The task is to implement a deterministic regression tree learner, use it to evaluate feature importance across a grid of hyperparameters, and then compute robust estimates of feature importance and its sensitivity.\n\n### **Problem Validation**\n\nThe problem statement has been rigorously validated and is determined to be **valid**.\n\n1.  **Scientific Grounding**: The problem is based on the standard CART (Classification and Regression Trees) algorithm. The concepts of impurity (sum of squared deviations), impurity decrease, recursive partitioning, and feature importance (MDI - Mean Decrease in Impurity) are all fundamental and well-established in the field of machine learning.\n2.  **Well-Posedness**: The problem is specified with a high degree of precision, ensuring a unique solution. Key details contributing to this are:\n    *   **Deterministic Feature Selection**: At each node, the set of features to consider for a split is deterministically fixed to the first `m` features.\n    *   **Deterministic Threshold Selection**: Threshold candidates are explicitly defined as midpoints between consecutive unique feature values.\n    *   **Unambiguous Splitting Criterion**: The choice of the best split is based on maximizing the impurity decrease.\n    *   **Strict Tie-Breaking Rule**: A clear hierarchy for breaking ties (first by smallest feature index `j`, then by smallest threshold `t`) is provided, which guarantees a single optimal split at any node.\n    *   **Explicit Stopping Criteria**: The conditions for a node to become a leaf (`d_{\\max}`, `\\ell_{\\min}`, no positive gain) are clearly stated.\n3.  **Completeness and Consistency**: All necessary data (`X`, `y`), definitions (impurity, importance), hyperparameters (`d_{\\max}`, `\\ell_{\\min}`, `m`), and evaluation metrics (`\\overline{I}_j`, `\\sigma_j`) are provided. The three test cases are fully specified. There are no contradictions.\n\nThe problem is a formal and solvable algorithmic task, free from any of the invalidating flaws outlined in the prompt.\n\n### **Methodology and Algorithm Design**\n\nThe core of the solution is the implementation of a `DeterministicRegressionTree` class that builds a tree according to the specified rules and calculates feature importances. The overall process for each test case is as follows:\n\n1.  **Iterate over Hyperparameter Grid**: For each test case, we iterate through every hyperparameter configuration `\\theta = (d_{\\max}, \\ell_{\\min}, m)` in the specified grid `\\Theta`.\n2.  **Build Tree and Compute Unnormalized Importances**: For each `\\theta`, a regression tree is built on the dataset `(X, y)`. The tree construction is iterative using a queue of nodes to process, starting with the root.\n    *   **Node Processing**: For each node in the queue, we check stopping criteria (`d \\ge d_{\\max}`, `|S| < 2\\ell_{\\min}`). If the node is eligible for a split, we search for the best split.\n    *   **Finding the Best Split**: This is the most critical step. To find the optimal split `(j^*, t^*)` for a node `S`, we iterate through the allowed features `j \\in \\{1, \\dots, \\min(m, p)\\}` and their corresponding threshold candidates. For each potential split, we calculate the impurity decrease `\\Delta\\mathcal{I}(S; j, t)`. The impurity is defined as `\\mathcal{I}(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2`. For numerical stability and efficiency, this is calculated as `\\mathcal{I}(S) = \\sum_{i \\in S} y_i^2 - (\\sum_{i \\in S} y_i)^2 / |S|`.\n    *   **Tie-Breaking**: The selection of the best split must strictly adhere to the tie-breaking rule. The goal is to find `argmax_{j,t} \\Delta\\mathcal{I}` with preference for smaller `j` and then smaller `t` in case of a tie. This is achieved by finding the split `(j, t)` that lexicographically maximizes the tuple `(\\Delta\\mathcal{I}(S; j, t), -j, -t)`.\n    *   **Updating Importances**: If a valid split with strictly positive gain is found, its impurity decrease `\\Delta\\mathcal{I}` is added to the total unnormalized importance `\\widehat{I}_j(\\theta)` for the feature `j` used in the split. The two resulting child nodes are then added to the processing queue.\n3.  **Normalize Importances**: After a tree is fully built for a configuration `\\theta`, the raw importances `\\widehat{I}_j(\\theta)` are normalized to sum to one, yielding `\\widetilde{I}_j(\\theta)`. If the sum of raw importances is zero (i.e., no splits were made), the normalized importances are all zero.\n4.  **Aggregate Results**: The normalized importances `\\{\\widetilde{I}(\\theta)\\}_{\\theta \\in \\Theta}` are collected for all configurations.\n5.  **Compute Final Metrics**: The hyperparameter-robust importance `\\overline{I}_j` (mean) and sensitivity `\\sigma_j` (standard deviation) are calculated for each feature `j` by averaging over all configurations in `\\Theta`.\n\nThis step-by-step, deterministic procedure is implemented in Python using the `numpy` library for numerical computations. All results are rounded to six decimal places for the final output as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fast_impurity(y: np.ndarray) -> float:\n    \"\"\"\n    Calculates impurity for a set of target values using the computationally\n    efficient formula: Var(y) * n = sum(y^2) - (sum(y))^2 / n.\n    \"\"\"\n    n = y.shape[0]\n    if n == 0:\n        return 0.0\n    # Use 64-bit floats for precision in sums\n    y_f64 = y.astype(np.float64)\n    sum_y = np.sum(y_f64)\n    sum_y_sq = np.sum(y_f64**2)\n    return sum_y_sq - (sum_y * sum_y) / n\n\nclass DeterministicRegressionTree:\n    \"\"\"\n    A deterministic regression tree learner built according to the problem's specifications.\n    It builds a tree and calculates feature importances based on impurity decrease.\n    \"\"\"\n\n    def __init__(self, d_max: int, l_min: int, m: int):\n        self.d_max = d_max\n        self.l_min = l_min\n        self.m = m\n        self.n = 0\n        self.p = 0\n        self.feature_importances_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Builds the tree and computes feature importances.\"\"\"\n        self.n, self.p = X.shape\n        self.feature_importances_ = np.zeros(self.p, dtype=np.float64)\n        root_indices = np.arange(self.n)\n        \n        # Use a list as a queue for breadth-first tree construction\n        q = [(root_indices, 0)]  # (indices, depth)\n        \n        while q:\n            indices, depth = q.pop(0)\n\n            # Stopping Condition 1: Maximum depth reached\n            if depth >= self.d_max:\n                continue\n            \n            # Stopping Condition 2: Not enough samples to create a valid split\n            if indices.shape[0] < 2 * self.l_min:\n                continue\n\n            # Stopping Condition 3: Node is pure (all targets are identical)\n            node_y = y[indices]\n            if np.all(node_y == node_y[0]):\n                continue\n            \n            # Find the best possible split for the current node\n            best_split = self._find_best_split(indices, X, y)\n            \n            if best_split is not None:\n                j, gain, left_indices, right_indices = best_split\n                \n                self.feature_importances_[j] += gain\n                \n                q.append((left_indices, depth + 1))\n                q.append((right_indices, depth + 1))\n\n    def _find_best_split(self, indices: np.ndarray, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Finds the best split for a node to maximize impurity decrease,\n        respecting the specified deterministic tie-breaking rules.\n        \"\"\"\n        # We maximize (gain, -j, -t) lexicographically.\n        best_objective = (-1.0, 0.0, 0.0)\n        best_split_info = None\n\n        node_y = y[indices]\n        impurity_node = fast_impurity(node_y)\n        \n        if impurity_node < 1e-12: # Effectively pure\n            return None\n\n        # Iterate through the first `m` features\n        for j in range(min(self.m, self.p)):\n            node_X_j = X[indices, j]\n            \n            unique_vals = np.unique(node_X_j)\n            if unique_vals.shape[0] < 2:\n                continue\n            \n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            \n            for t in thresholds:\n                left_mask = node_X_j <= t\n                \n                n_left = np.sum(left_mask)\n                n_right = indices.shape[0] - n_left\n\n                if n_left < self.l_min or n_right < self.l_min:\n                    continue\n                \n                y_left = node_y[left_mask]\n                y_right = node_y[~left_mask]\n                impurity_left = fast_impurity(y_left)\n                impurity_right = fast_impurity(y_right)\n\n                gain = impurity_node - (impurity_left + impurity_right)\n                \n                current_objective = (gain, -j, -t)\n                \n                if current_objective > best_objective:\n                    best_objective = current_objective\n                    left_indices = indices[left_mask]\n                    right_indices = indices[~left_mask]\n                    best_split_info = (j, gain, left_indices, right_indices)\n        \n        # A split is only valid if it yields a strictly positive gain.\n        if best_objective[0] > 1e-12:\n            return best_split_info\n        else:\n            return None\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    \n    # --- Test Case 1 Setup ---\n    X1 = np.zeros((12, 3))\n    y1 = np.zeros(12)\n    idx = 0\n    for x1_val in [0, 1, 2]:\n        for i, x2_val in enumerate([0, 1, 2, 3]):\n            x3_val = [1, 0, 1, 0][i]\n            X1[idx, :] = [x1_val, x2_val, x3_val]\n            y1[idx] = 3 * x1_val + 2 * (1 if x2_val >= 1.5 else 0) + 0.1 * x3_val\n            idx += 1\n    Theta1 = []\n    for d_max in [1, 2, 3]:\n        for l_min in [1, 2]:\n            for m in [1, 2, 3]:\n                Theta1.append((d_max, l_min, m))\n\n    # --- Test Case 2 Setup ---\n    X2 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float64)\n    y2 = np.array([0, 1, 1, 0], dtype=np.float64)\n    Theta2 = []\n    for d_max in [1]:\n        for l_min in [1, 2]:\n            for m in [1, 2]:\n                Theta2.append((d_max, l_min, m))\n\n    # --- Test Case 3 Setup ---\n    X3 = np.array([[0,0,0], [1,0,0], [0,1,0], [1,1,0], [0,0,1], [1,0,1]], dtype=np.float64)\n    y3 = np.array([0, 0, 0, 0, 5, 5], dtype=np.float64)\n    Theta3 = []\n    for d_max in [1, 2]:\n        for l_min in [1, 2]:\n            for m in [1, 2, 3]:\n                Theta3.append((d_max, l_min, m))\n\n    test_cases = [\n        (X1, y1, Theta1),\n        (X2, y2, Theta2),\n        (X3, y3, Theta3),\n    ]\n\n    all_results = []\n    for X, y, Theta in test_cases:\n        p = X.shape[1]\n        all_norm_importances = []\n\n        for d_max, l_min, m in Theta:\n            tree = DeterministicRegressionTree(d_max=d_max, l_min=l_min, m=m)\n            tree.fit(X, y)\n            unnorm_I = tree.feature_importances_\n            \n            total_I = np.sum(unnorm_I)\n            if total_I > 0:\n                norm_I = unnorm_I / total_I\n            else:\n                norm_I = np.zeros(p)\n            all_norm_importances.append(norm_I)\n\n        importances_array = np.array(all_norm_importances)\n        mean_I = np.mean(importances_array, axis=0)\n        std_I = np.std(importances_array, axis=0)\n        \n        all_results.append((mean_I, std_I))\n\n    # Format the final output string exactly as specified\n    case_strings = []\n    for mean_I, std_I in all_results:\n        mean_list_str = [f\"{x:.6f}\" for x in mean_I]\n        std_list_str = [f\"{x:.6f}\" for x in std_I]\n        case_str = f\"[[{','.join(mean_list_str)}],[{','.join(std_list_str)}]]\"\n        case_strings.append(case_str)\n        \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3121059"}]}