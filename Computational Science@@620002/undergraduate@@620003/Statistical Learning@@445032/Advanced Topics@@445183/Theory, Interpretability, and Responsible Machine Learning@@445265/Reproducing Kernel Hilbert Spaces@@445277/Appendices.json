{"hands_on_practices": [{"introduction": "The core of any Hilbert space, including a Reproducing Kernel Hilbert Space (RKHS), is its inner product, which defines its geometry and allows us to measure distances and norms. This exercise provides foundational practice in applying the \"reproducing property,\" the defining feature of an RKHS, to compute the norm of a function. By working through this calculation [@problem_id:1033834], you will solidify your understanding of how functions are represented and manipulated within this powerful framework.", "problem": "Let $\\mathcal{H}$ be a Reproducing Kernel Hilbert Space (RKHS) of real-valued functions on the domain $X = \\mathbb{R}$. The space $\\mathcal{H}$ is defined by a positive-definite kernel $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$. The inner product in $\\mathcal{H}$, denoted by $\\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}}$, is characterized by the reproducing property: for any function $f \\in \\mathcal{H}$ and any point $x \\in \\mathbb{R}$, we have $f(x) = \\langle f, K_x \\rangle_{\\mathcal{H}}$, where the function $K_x \\in \\mathcal{H}$ is defined by $K_x(y) = K(y, x)$. A direct consequence of this property is that for any two points $x, y \\in \\mathbb{R}$, the inner product of the corresponding kernel functions is given by $\\langle K_x, K_y \\rangle_{\\mathcal{H}} = K(x, y)$. The norm of a function $f \\in \\mathcal{H}$ is then given by $\\|f\\|_{\\mathcal{H}} = \\sqrt{\\langle f, f \\rangle_{\\mathcal{H}}}$.\n\nConsider the specific case where $\\mathcal{H}$ is endowed with a Gaussian kernel:\n$$\nK(x, y) = \\exp(-\\gamma(x-y)^2)\n$$\nwhere $\\gamma$ is a positive real parameter.\n\nLet's define a function $f \\in \\mathcal{H}$ as a linear combination of two kernel functions centered at points $a$ and $b$:\n$$\nf = c_a K_a - c_b K_b\n$$\nwhere $c_a, c_b$ are real constants and $a, b \\in \\mathbb{R}$.\n\nYour task is to compute the squared norm of this function, $\\|f\\|_{\\mathcal{H}}^2$, for the specific parameter values $\\gamma=1$, $c_a=3$, $c_b=1$, $a=0$, and $b=1$.", "solution": "1. In an RKHS, for $f=\\sum_i c_iK_{x_i}$ we have\n$$\n\\|f\\|_{\\mathcal H}^2\n=\\Big\\langle\\sum_i c_iK_{x_i},\\sum_j c_jK_{x_j}\\Big\\rangle\n=\\sum_{i,j}c_ic_j\\langle K_{x_i},K_{x_j}\\rangle\n=\\sum_{i,j}c_ic_jK(x_i,x_j).\n$$\n\n2. Here $f=3K_0-K_1$. Expanding the inner product gives:\n$$\n\\|f\\|_{\\mathcal H}^2 = 9K(0,0) + K(1,1) - 6K(0,1).\n$$\n\n3. With $K(x,y)=\\exp(-(x-y)^2)$, we have $K(0,0)=1$, $K(1,1)=1$, $K(0,1)=e^{-1}$. Thus\n$$\n\\|f\\|_{\\mathcal H}^2\n=9\\cdot1+1\\cdot1-6\\,e^{-1}\n=10-6e^{-1}.\n$$", "answer": "$$\\boxed{10-6e^{-1}}$$", "id": "1033834"}, {"introduction": "Many advanced machine learning algorithms, such as Kernel Principal Component Analysis, require data to be centered in the high-dimensional feature space. This exercise [@problem_id:3170311] demystifies this abstract concept by asking you to prove a fundamental connection: the algebraic operation of centering a Gram matrix is equivalent to the geometric operation of subtracting the mean of the feature vectors. This practice builds crucial intuition about how data transformations in input space affect the geometry of the data in the feature space.", "problem": "Consider a positive semidefinite kernel $k$ on an input set $\\mathcal{X}$ with associated feature map $\\phi:\\mathcal{X}\\to\\mathcal{H}$ into a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$, satisfying $k(x,x')=\\langle \\phi(x),\\phi(x')\\rangle_{\\mathcal{H}}$ for all $x,x'\\in\\mathcal{X}$. Let a dataset $\\{x_{i}\\}_{i=1}^{n}\\subset\\mathcal{X}$ be given, and denote its Gram matrix by $K\\in\\mathbb{R}^{n\\times n}$ with entries $K_{ij}=k(x_{i},x_{j})$. Define the sample mean embedding $\\mu:=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(x_{i})\\in\\mathcal{H}$ and the centered feature vectors $\\tilde{\\phi}(x_{i}):=\\phi(x_{i})-\\mu$. The centered Gram matrix $K^{\\mathrm{c}}\\in\\mathbb{R}^{n\\times n}$ is formed by inner products $K^{\\mathrm{c}}_{ij}:=\\langle \\tilde{\\phi}(x_{i}),\\tilde{\\phi}(x_{j})\\rangle_{\\mathcal{H}}$.\n\n1. Using only the definitions above and basic linear algebra, derive a closed-form matrix expression for $K^{\\mathrm{c}}$ in terms of $K$ and an $n\\times n$ matrix constructed from the identity matrix and the all-ones vector. Your expression must be fully simplified and depend only on $K$ and $n$.\n\n2. Now specialize to the linear kernel $k(u,v)=u^{\\top}v$ on $\\mathbb{R}^{2}$. Construct two datasets of three points each:\n   - Dataset $\\mathcal{A}$: $x_{1}=(0,0)$, $x_{2}=(1,0)$, $x_{3}=(0,1)$.\n   - Dataset $\\mathcal{B}$: $y_{i}=x_{i}+c$ for all $i$, where $c=(2,2)$.\n   Compute the uncentered Gram matrices $K^{\\mathcal{A}}$ and $K^{\\mathcal{B}}$, then compute their centered versions $K^{\\mathcal{A},\\mathrm{c}}$ and $K^{\\mathcal{B},\\mathrm{c}}$.\n\n3. Report the Frobenius norm of the difference between the centered Gram matrices, $\\|K^{\\mathcal{A},\\mathrm{c}}-K^{\\mathcal{B},\\mathrm{c}}\\|_{F}$, as a single real number. No rounding is required.\n\nFinally, interpret the result in the context of mean embeddings in feature space, explaining why these two datasets can have identical centered Gram matrices but different uncentered Gram matrices.", "solution": "**Part 1: Derivation of the Centered Gram Matrix Expression**\n\nWe are tasked with deriving a closed-form matrix expression for the centered Gram matrix $K^{\\mathrm{c}}$ in terms of the uncentered Gram matrix $K$ and the sample size $n$.\n\nThe entries of the centered Gram matrix $K^{\\mathrm{c}}$ are defined as $K^{\\mathrm{c}}_{ij} = \\langle \\tilde{\\phi}(x_{i}), \\tilde{\\phi}(x_{j}) \\rangle_{\\mathcal{H}}$, where $\\tilde{\\phi}(x_{i}) = \\phi(x_{i}) - \\mu$ is the centered feature vector and $\\mu = \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k})$ is the sample mean embedding.\n\nUsing the bilinearity of the inner product, we expand the expression for $K^{\\mathrm{c}}_{ij}$:\n$$\nK^{\\mathrm{c}}_{ij} = \\langle \\phi(x_{i}) - \\mu, \\phi(x_{j}) - \\mu \\rangle_{\\mathcal{H}} = \\langle \\phi(x_{i}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} - \\langle \\phi(x_{i}), \\mu \\rangle_{\\mathcal{H}} - \\langle \\mu, \\phi(x_{j}) \\rangle_{\\mathcal{H}} + \\langle \\mu, \\mu \\rangle_{\\mathcal{H}}\n$$\n\nWe now express each term using the kernel function $k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}}$ and the entries of the Gram matrix $K_{ij} = k(x_i, x_j)$.\n\nThe first term is simply the definition of the uncentered Gram matrix entry:\n$$\n\\langle \\phi(x_{i}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} = k(x_{i}, x_{j}) = K_{ij}\n$$\n\nFor the second term, we substitute the definition of $\\mu$:\n$$\n\\langle \\phi(x_{i}), \\mu \\rangle_{\\mathcal{H}} = \\left\\langle \\phi(x_{i}), \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}\\langle \\phi(x_{i}), \\phi(x_{k}) \\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}k(x_{i}, x_{k}) = \\frac{1}{n}\\sum_{k=1}^{n}K_{ik}\n$$\n\nThe third term is analogous due to the symmetry of the inner product:\n$$\n\\langle \\mu, \\phi(x_{j}) \\rangle_{\\mathcal{H}} = \\left\\langle \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}), \\phi(x_{j}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}\\langle \\phi(x_{k}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}k(x_{k}, x_{j}) = \\frac{1}{n}\\sum_{k=1}^{n}K_{kj}\n$$\n\nFor the fourth term, we have:\n$$\n\\langle \\mu, \\mu \\rangle_{\\mathcal{H}} = \\left\\langle \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}), \\frac{1}{n}\\sum_{l=1}^{n}\\phi(x_{l}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}\\langle \\phi(x_{k}), \\phi(x_{l}) \\rangle_{\\mathcal{H}} = \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}K_{kl}\n$$\n\nCombining these terms, the expression for a single entry $K^{\\mathrm{c}}_{ij}$ is:\n$$\nK^{\\mathrm{c}}_{ij} = K_{ij} - \\frac{1}{n}\\sum_{k=1}^{n}K_{ik} - \\frac{1}{n}\\sum_{k=1}^{n}K_{kj} + \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}K_{kl}\n$$\n\nTo express this in matrix form, let $I_n$ be the $n \\times n$ identity matrix and $J_n$ be the $n \\times n$ matrix of all ones. The expression above can be identified as the $(i,j)$-th entry of a matrix product.\nThe term $\\frac{1}{n}\\sum_{k=1}^{n}K_{ik}$ is the $(i,j)$-th entry of the matrix $\\frac{1}{n}KJ_n$.\nThe term $\\frac{1}{n}\\sum_{k=1}^{n}K_{kj}$ is the $(i,j)$-th entry of the matrix $\\frac{1}{n}J_nK$.\nThe term $\\frac{1}{n^2}\\sum_{k,l}K_{kl}$ is the $(i,j)$-th entry of the matrix $\\frac{1}{n^2}J_nKJ_n$.\n\nThus, we can write the matrix equation:\n$$\nK^{\\mathrm{c}} = K - \\frac{1}{n}KJ_n - \\frac{1}{n}J_nK + \\frac{1}{n^2}J_nKJ_n\n$$\n\nThis expression can be factorized. Let us define the centering matrix $H_n = I_n - \\frac{1}{n}J_n$. This matrix is constructed from the identity and the all-ones matrix as requested. We now compute the product $H_n K H_n$:\n$$\nH_n K H_n = \\left(I_n - \\frac{1}{n}J_n\\right) K \\left(I_n - \\frac{1}{n}J_n\\right) = \\left(K - \\frac{1}{n}J_nK\\right) \\left(I_n - \\frac{1}{n}J_n\\right)\n$$\n$$\n= K - \\frac{1}{n}KJ_n - \\frac{1}{n}J_nK + \\frac{1}{n^2}J_nKJ_n\n$$\nThis is precisely the expression we derived for $K^{\\mathrm{c}}$. Therefore, the closed-form expression is:\n$$\nK^{\\mathrm{c}} = H_n K H_n = \\left(I_n - \\frac{1}{n}J_n\\right) K \\left(I_n - \\frac{1}{n}J_n\\right)\n$$\n\n**Part 2: Computations for Specific Datasets**\n\nWe specialize to the linear kernel $k(u,v)=u^{\\top}v$ on $\\mathbb{R}^2$ and $n=3$.\nThe datasets are:\n- Dataset $\\mathcal{A}$: $x_{1}=(0,0)$, $x_{2}=(1,0)$, $x_{3}=(0,1)$.\n- Dataset $\\mathcal{B}$: $y_{i}=x_{i}+c$ with $c=(2,2)$, giving $y_{1}=(2,2)$, $y_{2}=(3,2)$, $y_{3}=(2,3)$.\n\nFirst, we compute the uncentered Gram matrix $K^{\\mathcal{A}}$ with entries $K^{\\mathcal{A}}_{ij} = x_i^\\top x_j$:\n$$\nK^{\\mathcal{A}} = \\begin{pmatrix} x_1^\\top x_1 & x_1^\\top x_2 & x_1^\\top x_3 \\\\ x_2^\\top x_1 & x_2^\\top x_2 & x_2^\\top x_3 \\\\ x_3^\\top x_1 & x_3^\\top x_2 & x_3^\\top x_3 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n\nNext, we compute the uncentered Gram matrix $K^{\\mathcal{B}}$ with entries $K^{\\mathcal{B}}_{ij} = y_i^\\top y_j$:\n$$\nK^{\\mathcal{B}} = \\begin{pmatrix} y_1^\\top y_1 & y_1^\\top y_2 & y_1^\\top y_3 \\\\ y_2^\\top y_1 & y_2^\\top y_2 & y_2^\\top y_3 \\\\ y_3^\\top y_1 & y_3^\\top y_2 & y_3^\\top y_3 \\end{pmatrix} = \\begin{pmatrix} 8 & 10 & 10 \\\\ 10 & 13 & 12 \\\\ 10 & 12 & 13 \\end{pmatrix}\n$$\n\nNow we compute the centered Gram matrices. For $n=3$, the centering matrix is:\n$$\nH_3 = I_3 - \\frac{1}{3}J_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix}\n$$\n\nWe compute $K^{\\mathcal{A},\\mathrm{c}} = H_3 K^{\\mathcal{A}} H_3$:\n$$\nK^{\\mathcal{A},\\mathrm{c}} = \\frac{1}{3}\\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{9}\\begin{pmatrix} 0 & -1 & -1 \\\\ 0 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 5 & -4 \\\\ -1 & -4 & 5 \\end{pmatrix}\n$$\n\nAlternatively, for the linear kernel, the feature map is $\\phi(x)=x$. The centered features are $\\tilde{x}_i = x_i - \\mu_x$.\n$\\mu_x = \\frac{1}{3}(x_1+x_2+x_3) = \\frac{1}{3}((0,0)+(1,0)+(0,1)) = (\\frac{1}{3},\\frac{1}{3})$.\nThe centered feature vectors for dataset $\\mathcal{B}$ are $\\tilde{y}_i = y_i - \\mu_y$. The mean is $\\mu_y = \\frac{1}{3}\\sum(x_i+c) = (\\frac{1}{3}\\sum x_i) + c = \\mu_x+c$.\nSo, $\\tilde{y}_i = (x_i+c) - (\\mu_x+c) = x_i - \\mu_x = \\tilde{x}_i$.\nSince the centered feature vectors are identical for both datasets ($\\tilde{y}_i = \\tilde{x}_i$), their pairwise inner products must also be identical.\n$$\nK^{\\mathcal{B},\\mathrm{c}}_{ij} = \\langle \\tilde{y}_i, \\tilde{y}_j \\rangle = \\tilde{y}_i^\\top \\tilde{y}_j = \\tilde{x}_i^\\top \\tilde{x}_j = K^{\\mathcal{A},\\mathrm{c}}_{ij}\n$$\nTherefore, $K^{\\mathcal{B},\\mathrm{c}} = K^{\\mathcal{A},\\mathrm{c}}$. We find:\n$$\nK^{\\mathcal{A},\\mathrm{c}} = K^{\\mathcal{B},\\mathrm{c}} = \\frac{1}{9}\\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 5 & -4 \\\\ -1 & -4 & 5 \\end{pmatrix}\n$$\n\n**Part 3: Frobenius Norm and Interpretation**\n\nWe are asked to report the Frobenius norm of the difference between the centered Gram matrices. Since $K^{\\mathcal{A},\\mathrm{c}} = K^{\\mathcal{B},\\mathrm{c}}$, their difference is the $3 \\times 3$ zero matrix $\\mathbf{0}_{3\\times 3}$. The Frobenius norm is defined as $\\|A\\|_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$.\nFor the zero matrix, this is:\n$$\n\\|K^{\\mathcal{A},\\mathrm{c}} - K^{\\mathcal{B},\\mathrm{c}}\\|_{F} = \\|\\mathbf{0}_{3\\times 3}\\|_{F} = \\sqrt{0} = 0\n$$\n\n**Interpretation:**\nThe result, $\\|K^{\\mathcal{A},\\mathrm{c}}-K^{\\mathcal{B},\\mathrm{c}}\\|_{F} = 0$, demonstrates that the centered Gram matrices of datasets $\\mathcal{A}$ and $\\mathcal{B}$ are identical, even though their uncentered Gram matrices, $K^{\\mathcal{A}}$ and $K^{\\mathcal{B}}$, are different. This invariance has a clear interpretation in the context of mean embeddings and feature maps.\n\nCentering in the feature space, via the operation $\\tilde{\\phi}(x) = \\phi(x) - \\mu$, is designed to remove the information about the `location` (the mean $\\mu$) of the data cloud in the RKHS $\\mathcal{H}$, preserving only the information about its `shape` (the geometry of the data points relative to their mean).\n\nFor the specific case of the linear kernel $k(u,v)=u^\\top v$, the feature map can be taken as the identity, $\\phi(x)=x$. Dataset $\\mathcal{B}$ is a rigid translation of dataset $\\mathcal{A}$ by a constant vector $c$, i.e., $y_i = x_i + c$. Because the feature map is linear, this translation in the input space corresponds to an identical translation in the feature space: $\\phi(y_i) = \\phi(x_i+c) = x_i+c = \\phi(x_i) + \\phi(c)$ (where we can see $c$ as $\\phi(c)$).\n\nConsequently, the mean embedding of dataset $\\mathcal{B}$ is also translated by $c$ relative to that of dataset $\\mathcal{A}$:\n$\\mu_{\\mathcal{B}} = \\frac{1}{n}\\sum_i \\phi(y_i) = \\frac{1}{n}\\sum_i (\\phi(x_i)+c) = (\\frac{1}{n}\\sum_i \\phi(x_i)) + c = \\mu_{\\mathcal{A}} + c$.\n\nWhen we compute the centered feature vectors for dataset $\\mathcal{B}$, the translation term $c$ cancels out perfectly:\n$\\tilde{\\phi}(y_i) = \\phi(y_i) - \\mu_{\\mathcal{B}} = (\\phi(x_i) + c) - (\\mu_{\\mathcal{A}} + c) = \\phi(x_i) - \\mu_{\\mathcal{A}} = \\tilde{\\phi}(x_i)$.\n\nThe centered feature vectors for both datasets turn out to be identical. Since the centered Gram matrix $K^\\mathrm{c}$ is composed of the inner products of these centered vectors, it must be the same for both datasets. This shows that for the linear kernel, the geometry of the data relative to its mean is invariant to global translations of the dataset in the input space. Algorithms that depend on $K^\\mathrm{c}$, such as Kernel Principal Component Analysis (Kernel PCA), would therefore extract the exact same variance structure from both datasets.", "answer": "$$\n\\boxed{0}\n$$", "id": "3170311"}, {"introduction": "Evaluating a model's performance is crucial, and cross-validation is the gold standard for doing so. However, leave-one-out cross-validation (LOOCV) can be computationally prohibitive, seemingly requiring a model to be retrained for each data point left out. This practice [@problem_id:3170276] guides you through the derivation of a remarkable and elegant shortcut for kernel ridge regression, which allows for the calculation of the LOOCV error without any retraining. This powerfully illustrates how the mathematical structure of kernel methods can lead to significant practical and computational advantages.", "problem": "Consider training data $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ with $x_{i}$ in an input space $\\mathcal{X}$ and $y_{i}$ in $\\mathbb{R}$. Let $\\mathcal{H}$ be a Reproducing Kernel Hilbert Space (RKHS) induced by a positive definite kernel $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ with Gram matrix $K\\in\\mathbb{R}^{n\\times n}$ defined by $K_{ij}=k(x_{i}, x_{j})$. Kernel ridge regression (KRR) fits a function $f$ in $\\mathcal{H}$ by minimizing the regularized empirical risk\n$$\n\\sum_{i=1}^{n}\\left(f(x_{i})-y_{i}\\right)^{2}+\\lambda\\|f\\|_{\\mathcal{H}}^{2},\n$$\nwhere $\\lambda>0$ is a regularization parameter. By the representer theorem, the optimizer has the form $f(\\cdot)=\\sum_{j=1}^{n}\\alpha_{j}k(x_{j},\\cdot)$, where the coefficient vector $\\alpha\\in\\mathbb{R}^{n}$ solves $(K+\\lambda I)\\alpha=y$, with $y\\in\\mathbb{R}^{n}$ the vector of responses. The vector of fitted values on the training inputs is $\\hat{f}=K\\alpha$. Define the “hat” or smoother matrix $H\\in\\mathbb{R}^{n\\times n}$ by $H=K(K+\\lambda I)^{-1}$ so that $\\hat{f}=Hy$. For leave-one-out cross-validation (LOOCV), the prediction at $x_{i}$ is obtained by re-fitting KRR on the dataset with the $i$-th observation removed; denote this scalar prediction by $\\hat{f}^{(-i)}_{i}$.\n\nStarting only from the above definitions and standard matrix identities, derive an explicit closed-form formula for the LOOCV mean squared error\n$$\nL_{\\mathrm{LOO}}(\\lambda)=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}^{(-i)}_{i}\\right)^{2}\n$$\nexpressed purely in terms of $H$, its diagonal entries $h_{ii}$, and $y$. Report your final formula as a single closed-form analytic expression. Then, interpret the stability of this formula as $\\lambda\\to 0$ for strictly positive definite $K$ by analyzing the asymptotic behavior of the numerator and denominator terms that appear in your expression. No numerical evaluation is required, and your final reported answer must be a single analytic expression without units.", "solution": "We begin from the standard kernel ridge regression (KRR) setup in a Reproducing Kernel Hilbert Space (RKHS). The fitted function $f$ has the finite expansion $f(\\cdot)=\\sum_{j=1}^{n}\\alpha_{j}k(x_{j},\\cdot)$, and the coefficient vector $\\alpha$ solves the linear system\n$$\n(K+\\lambda I)\\alpha=y,\n$$\nwhere $K\\in\\mathbb{R}^{n\\times n}$ is the Gram matrix, $I$ is the identity, and $y\\in\\mathbb{R}^{n}$ is the response vector. The vector of fitted values at training inputs is\n$$\n\\hat{f}=K\\alpha=K(K+\\lambda I)^{-1}y=Hy,\n$$\nwhere we have defined the hat matrix $H=K(K+\\lambda I)^{-1}$. This shows that KRR is a linear smoother: the fitted values depend linearly on $y$ via $H$.\n\nWe define the leave-one-out cross-validation (LOOCV) prediction $\\hat{f}^{(-i)}_{i}$ as the prediction at $x_{i}$ obtained by re-fitting KRR on the dataset with the $i$-th observation removed. The LOOCV mean squared error is\n$$\nL_{\\mathrm{LOO}}(\\lambda)=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}^{(-i)}_{i}\\right)^{2}.\n$$\nOur goal is to express $L_{\\mathrm{LOO}}(\\lambda)$ purely in terms of $H$, its diagonal entries $h_{ii}$, and $y$.\n\nWe use a standard derivation based on linear algebra and block matrix identities. Consider the linear system $A\\alpha=y$ with $A=K+\\lambda I$. Partition $A$ and the vectors conformably by separating the $i$-th row and column:\n$$\nA=\\begin{pmatrix}\na & c^{\\top}\\\\\nc & M\n\\end{pmatrix},\\qquad\n\\alpha=\\begin{pmatrix}\\alpha_{i}\\\\ \\alpha_{-i}\\end{pmatrix},\\qquad\ny=\\begin{pmatrix}y_{i}\\\\ y_{-i}\\end{pmatrix},\n$$\nwhere $a=K_{ii}+\\lambda$, $c=K_{-i,i}$, and $M=K_{-i,-i}+\\lambda I$. The full-data solution satisfies\n$$\n\\begin{pmatrix}\na & c^{\\top}\\\\\nc & M\n\\end{pmatrix}\n\\begin{pmatrix}\\alpha_{i}\\\\ \\alpha_{-i}\\end{pmatrix}\n=\n\\begin{pmatrix}y_{i}\\\\ y_{-i}\\end{pmatrix}.\n$$\nThe LOOCV fit solves the reduced system\n$$\nM\\,\\alpha^{(-i)}=y_{-i},\n$$\nand the leave-one-out prediction at $x_{i}$ is\n$$\n\\hat{f}^{(-i)}_{i}=K_{i,-i}^{\\top}\\alpha^{(-i)}=c^{\\top}\\alpha^{(-i)}.\n$$\nFrom the full-data normal equations, we have $a\\,\\alpha_{i}+c^{\\top}\\alpha_{-i}=y_{i}$ and $c\\,\\alpha_{i}+M\\,\\alpha_{-i}=y_{-i}$. Solving the second equation for $\\alpha_{-i}$ gives\n$$\n\\alpha_{-i}=M^{-1}\\left(y_{-i}-c\\,\\alpha_{i}\\right).\n$$\nSubstituting into the first equation yields\n$$\na\\,\\alpha_{i}+c^{\\top}M^{-1}\\left(y_{-i}-c\\,\\alpha_{i}\\right)=y_{i},\n$$\nwhich rearranges to\n$$\n\\left(a-c^{\\top}M^{-1}c\\right)\\alpha_{i}=y_{i}-c^{\\top}M^{-1}y_{-i}.\n$$\nHence\n$$\n\\alpha_{-i}=M^{-1}y_{-i}-M^{-1}c\\,\\alpha_{i}.\n$$\nNow compute the LOOCV prediction:\n$$\n\\hat{f}^{(-i)}_{i}=c^{\\top}\\alpha^{(-i)}=c^{\\top}M^{-1}y_{-i}.\n$$\nUsing the expression for $\\alpha_{i}$ above, we can write\n$$\nc^{\\top}M^{-1}y_{-i}=y_{i}-\\left(a-c^{\\top}M^{-1}c\\right)\\alpha_{i}.\n$$\nTherefore,\n$$\ny_{i}-\\hat{f}^{(-i)}_{i}=\\left(a-c^{\\top}M^{-1}c\\right)\\alpha_{i}.\n$$\nNext, recall that the fitted value at $x_{i}$ under the full fit is\n$$\n\\hat{f}_{i}=K_{i,*}^{\\top}\\alpha=K_{ii}\\alpha_{i}+K_{i,-i}^{\\top}\\alpha_{-i}=K_{ii}\\alpha_{i}+c^{\\top}\\left(M^{-1}y_{-i}-M^{-1}c\\,\\alpha_{i}\\right)=c^{\\top}M^{-1}y_{-i}+\\left(K_{ii}-c^{\\top}M^{-1}c\\right)\\alpha_{i}.\n$$\nSubtracting from $y_{i}$ and comparing with the previous expression, we find\n$$\ny_{i}-\\hat{f}_{i}=y_{i}-c^{\\top}M^{-1}y_{-i}-\\left(K_{ii}-c^{\\top}M^{-1}c\\right)\\alpha_{i}=\\left(a-c^{\\top}M^{-1}c\\right)\\alpha_{i}-\\left(K_{ii}-c^{\\top}M^{-1}c\\right)\\alpha_{i}=\\lambda\\,\\alpha_{i}.\n$$\nThus the full-data residual at $i$ is $r_{i}=y_{i}-\\hat{f}_{i}=\\lambda\\,\\alpha_{i}$. Combining with $y_{i}-\\hat{f}^{(-i)}_{i}=\\left(a-c^{\\top}M^{-1}c\\right)\\alpha_{i}$, we obtain\n$$\ny_{i}-\\hat{f}^{(-i)}_{i}=\\frac{a-c^{\\top}M^{-1}c}{\\lambda}\\,\\left(y_{i}-\\hat{f}_{i}\\right).\n$$\nIt remains to recognize $a-c^{\\top}M^{-1}c$ in terms of the hat matrix diagonal $h_{ii}$. The Schur complement identity for the inverse of $A$ implies\n$$\n\\left(A^{-1}\\right)_{ii}=\\frac{1}{a-c^{\\top}M^{-1}c}.\n$$\nSince $H=KA^{-1}$, we have\n$$\nh_{ii}=\\left(H\\right)_{ii}=K_{ii}\\left(A^{-1}\\right)_{ii}+c^{\\top}\\left(A^{-1}\\right)_{-i,i}=1-\\lambda\\left(A^{-1}\\right)_{ii}.\n$$\nEquivalently,\n$$\n1-h_{ii}=\\lambda\\left(A^{-1}\\right)_{ii}=\\frac{\\lambda}{a-c^{\\top}M^{-1}c}.\n$$\nTherefore\n$$\n\\frac{a-c^{\\top}M^{-1}c}{\\lambda}=\\frac{1}{1-h_{ii}},\n$$\nwhich yields the fundamental LOOCV residual identity for KRR:\n$$\ny_{i}-\\hat{f}^{(-i)}_{i}=\\frac{y_{i}-\\hat{f}_{i}}{1-h_{ii}}=\\frac{y_{i}-\\left(Hy\\right)_{i}}{1-h_{ii}}.\n$$\nConsequently, the LOOCV mean squared error simplifies to\n$$\nL_{\\mathrm{LOO}}(\\lambda)=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{y_{i}-\\left(Hy\\right)_{i}}{1-h_{ii}}\\right)^{2}.\n$$\nThis expression depends only on the smoother matrix $H$, its diagonal entries $h_{ii}$, and the response vector $y$, and does not require refitting $n$ separate models.\n\nWe now interpret stability as $\\lambda\\to 0$ when $K$ is strictly positive definite. In this case, $K$ is invertible, and we can use the resolvent expansion\n$$\n(K+\\lambda I)^{-1}=K^{-1}-\\lambda K^{-2}+O(\\lambda^{2}),\n$$\nwhich implies\n$$\nH=K(K+\\lambda I)^{-1}=I-\\lambda K^{-1}+O(\\lambda^{2}).\n$$\nTherefore,\n$$\n1-h_{ii}=\\lambda\\left(K^{-1}\\right)_{ii}+O(\\lambda^{2}),\\qquad y_{i}-\\left(Hy\\right)_{i}=\\lambda\\left(K^{-1}y\\right)_{i}+O(\\lambda^{2}).\n$$\nThe LOOCV residual ratio has a finite limit:\n$$\n\\frac{y_{i}-\\left(Hy\\right)_{i}}{1-h_{ii}}=\\frac{\\lambda\\left(K^{-1}y\\right)_{i}+O(\\lambda^{2})}{\\lambda\\left(K^{-1}\\right)_{ii}+O(\\lambda^{2})}\\to \\frac{\\left(K^{-1}y\\right)_{i}}{\\left(K^{-1}\\right)_{ii}}\\quad\\text{as }\\lambda\\to 0.\n$$\nThus, analytically, $L_{\\mathrm{LOO}}(\\lambda)$ converges to a finite value when $K$ is strictly positive definite. However, from a numerical stability standpoint, both the numerator $y_{i}-\\left(Hy\\right)_{i}$ and the denominator $1-h_{ii}$ vanish linearly in $\\lambda$, so the formula involves ratios of two small quantities, which can be susceptible to numerical round-off and conditioning issues for very small $\\lambda$. If $K$ is not strictly positive definite (only positive semidefinite), $H$ converges to the projector onto the column space of $K$, and the denominators $1-h_{ii}$ need not vanish, altering the asymptotic behavior, but the same derivation yields the closed-form expression for any $\\lambda>0$.", "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{y_{i}-\\left(Hy\\right)_{i}}{1-h_{ii}}\\right)^{2}}$$", "id": "3170276"}]}