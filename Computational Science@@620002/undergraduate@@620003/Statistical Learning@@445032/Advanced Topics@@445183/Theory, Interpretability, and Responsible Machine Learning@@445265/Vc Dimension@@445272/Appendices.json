{"hands_on_practices": [{"introduction": "The capacity of a model class is a central theme in statistical learning. For linear classifiers, a natural starting point is to understand how their flexibility relates to the dimensionality of the data space. This practice explores a fundamental case: the class of hyperplanes constrained to pass through a fixed point, effectively removing the bias term. By determining the VC dimension of these homogeneous linear separators, we establish a baseline for the expressive power of linear models and uncover a clean, direct relationship between model capacity and dimensionality [@problem_id:3192452].", "problem": "Consider the instance space $\\mathcal{X} = \\mathbb{R}^{d} \\setminus \\{p\\}$, where $d \\in \\mathbb{N}$ and $p \\in \\mathbb{R}^{d}$ is a fixed point. Define the hypothesis class $\\mathcal{H}_{p}$ consisting of all halfspaces whose boundary hyperplane passes through $p$ and that classify points by the sign of the linear functional relative to $p$:\n$$\n\\mathcal{H}_{p} = \\left\\{ h_{w} : \\mathcal{X} \\to \\{-1,+1\\} \\,\\middle|\\, h_{w}(x) = \\operatorname{sign}\\!\\big(w^{\\top}(x - p)\\big),\\, w \\in \\mathbb{R}^{d} \\right\\}.\n$$\nHere $\\operatorname{sign}(t)$ denotes $+1$ if $t \\ge 0$ and $-1$ otherwise. Using only the core definitions of shattering and Vapnik–Chervonenkis (VC) dimension, determine the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{p})$ as a function of $d$. Your answer must be a single closed-form analytic expression in $d$. No rounding is required.", "solution": "We begin from the fundamental definition of Vapnik–Chervonenkis (VC) dimension. For a hypothesis class $\\mathcal{H}$ on instance space $\\mathcal{X}$, a finite set $S \\subset \\mathcal{X}$ is said to be shattered by $\\mathcal{H}$ if, for every labeling $\\ell : S \\to \\{-1,+1\\}$, there exists $h \\in \\mathcal{H}$ such that $h(x) = \\ell(x)$ for all $x \\in S$. The VC dimension $d_{\\mathrm{VC}}(\\mathcal{H})$ is the maximum cardinality of a set $S$ shattered by $\\mathcal{H}$.\n\nWe must compute $d_{\\mathrm{VC}}(\\mathcal{H}_{p})$ for\n$$\n\\mathcal{H}_{p} = \\left\\{ h_{w}(x) = \\operatorname{sign}\\!\\big(w^{\\top}(x - p)\\big) : w \\in \\mathbb{R}^{d} \\right\\}.\n$$\nObserve that for each $w \\in \\mathbb{R}^{d}$, the decision boundary $\\{x \\in \\mathbb{R}^{d} : w^{\\top}(x - p) = 0\\}$ is a hyperplane passing through $p$. By the translation $z = x - p$, the class $\\mathcal{H}_{p}$ is equivalent to the class of homogeneous linear separators in $\\mathbb{R}^{d}$ acting on $z$ via $h_{w}(z) = \\operatorname{sign}(w^{\\top} z)$.\n\nWe show a matching lower and upper bound that establishes $d_{\\mathrm{VC}}(\\mathcal{H}_{p}) = d$.\n\nLower bound ($d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\ge d$). Choose the set of $d$ points\n$$\nS = \\{x_{1}, x_{2}, \\dots, x_{d}\\} \\subset \\mathcal{X}\n$$\nwith\n$$\nx_{i} = p + e_{i}, \\quad i = 1,2,\\dots,d,\n$$\nwhere $e_{i}$ are the standard basis vectors of $\\mathbb{R}^{d}$. Consider any labeling $\\ell : S \\to \\{-1,+1\\}$, and let $y \\in \\{-1,+1\\}^{d}$ be the vector with components $y_{i} = \\ell(x_{i})$. Define\n$$\nw = y \\in \\mathbb{R}^{d}.\n$$\nThen for each $i$,\n$$\nw^{\\top}(x_{i} - p) = y^{\\top} e_{i} = y_{i},\n$$\nso $h_{w}(x_{i}) = \\operatorname{sign}(y_{i}) = y_{i} = \\ell(x_{i})$. Because this holds for every labeling $\\ell$ of $S$, the set $S$ is shattered by $\\mathcal{H}_{p}$. Therefore, $d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\ge d$.\n\nUpper bound ($d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\le d$). Suppose, for contradiction, that there exists a set $T = \\{x_{1}, x_{2}, \\dots, x_{d+1}\\} \\subset \\mathcal{X}$ of $d+1$ points that is shattered by $\\mathcal{H}_{p}$. Consider the vectors $v_{i} = x_{i} - p \\in \\mathbb{R}^{d}$. Since we have $d+1$ vectors in a $d$-dimensional space, the set $\\{v_{1}, \\dots, v_{d+1}\\}$ is linearly dependent. Thus there exist coefficients $a_{1}, \\dots, a_{d+1}$, not all zero, such that\n$$\n\\sum_{i=1}^{d+1} a_{i} v_{i} = 0.\n$$\nBecause the coefficients are not all zero and the sum is the zero vector, there must be at least one index with $a_{i}  0$ and at least one index with $a_{j}  0$. Define a labeling $\\ell : T \\to \\{-1,+1\\}$ by\n$$\n\\ell(x_{i}) = \\operatorname{sign}(a_{i}).\n$$\nIf $T$ were shattered, there would exist $w \\in \\mathbb{R}^{d}$ such that\n$$\n\\operatorname{sign}\\!\\big(w^{\\top}(x_{i} - p)\\big) = \\operatorname{sign}(a_{i}) \\quad \\text{for all } i.\n$$\nUnder this labeling, for every $i$ with $a_{i} \\ne 0$ we have $a_{i} w^{\\top}(x_{i} - p) \\ge 0$, and because there are both positive and negative $a_{i}$, and none of the corresponding inner products are zero under the realized signs, it follows that\n$$\n\\sum_{i=1}^{d+1} a_{i} w^{\\top}(x_{i} - p)  0.\n$$\nHowever, by linearity,\n$$\n\\sum_{i=1}^{d+1} a_{i} w^{\\top}(x_{i} - p) = w^{\\top} \\sum_{i=1}^{d+1} a_{i} (x_{i} - p) = w^{\\top} \\cdot 0 = 0,\n$$\na contradiction. Therefore, no set of size $d+1$ can be shattered by $\\mathcal{H}_{p}$, proving $d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\le d$.\n\nCombining the lower and upper bounds yields\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{p}) = d.\n$$\nThis is a closed-form expression in $d$, as required.", "answer": "$$\\boxed{d}$$", "id": "3192452"}, {"introduction": "Real-world models often come with constraints inherited from the systems they aim to describe. Building on our understanding of linear classifiers, we now investigate how such constraints affect model capacity [@problem_id:3192435]. This problem introduces a \"biologically plausible\" perceptron model where synaptic weights can only be excitatory (non-negative), a common constraint in computational neuroscience. Your task is to compute the VC dimension for this restricted class and compare it to that of an unrestricted perceptron, providing a concrete example of how limiting a model's parameters reduces its complexity.", "problem": "Consider the hypothesis class of perceptrons on $\\mathbb{R}^{d}$ defined by linear threshold functions. A perceptron assigns a label according to $h_{\\mathbf{w},b}(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{x} + b)$, where $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector and $b \\in \\mathbb{R}$ is the bias. In biologically plausible models with only excitatory synapses, weights are constrained to be nonnegative so that $\\mathbf{w} \\in \\mathbb{R}_{+}^{d} := \\{\\mathbf{w} \\in \\mathbb{R}^{d} : w_{i} \\geq 0 \\text{ for all } i\\}$. Let $\\mathcal{H}_{+} := \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}_{+}^{d},\\ b \\in \\mathbb{R}\\}$ denote this excitatory-only class, and let $\\mathcal{H} := \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}^{d},\\ b \\in \\mathbb{R}\\}$ denote the unrestricted class of halfspaces.\n\nStarting from the core definitions of Vapnik–Chervonenkis (VC) dimension and linear separability, compute the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{+})$ of excitatory-only perceptrons on $\\mathbb{R}^{d}$ and compare it to the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H})$ of unrestricted halfspaces on $\\mathbb{R}^{d}$. Justify your answer from first principles, using only standard facts about affine independence and monotonicity induced by nonnegative weights. Express your final answer as a two-entry row vector giving $d_{\\mathrm{VC}}(\\mathcal{H}_{+})$ and $d_{\\mathrm{VC}}(\\mathcal{H})$ in terms of $d$. No rounding is required and no physical units are involved.", "solution": "The problem asks for the Vapnik-Chervonenkis (VC) dimension of two hypothesis classes of perceptrons on $\\mathbb{R}^d$. The first is the unrestricted class $\\mathcal{H}$, and the second is the excitatory-only class $\\mathcal{H}_+$, where the weight vector $\\mathbf{w}$ is constrained to have non-negative components.\n\n**Step 1: Preliminaries and Definition of VC Dimension**\n\nThe Vapnik-Chervonenkis (VC) dimension of a hypothesis class $\\mathcal{C}$ of binary classifiers is the size of the largest finite set of points that can be shattered by $\\mathcal{C}$. A set of points $S = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ is said to be shattered by $\\mathcal{C}$ if for every possible labeling of the points in $S$, represented by a vector $\\boldsymbol{\\ell} = (\\ell_1, \\dots, \\ell_N) \\in \\{-1, +1\\}^N$, there exists a classifier $h \\in \\mathcal{C}$ such that $h(\\mathbf{x}_i) = \\ell_i$ for all $i = 1, \\dots, N$. This means $\\mathcal{C}$ can realize all $2^N$ possible dichotomies of the set $S$.\n\nThe classifiers are of the form $h_{\\mathbf{w},b}(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{x} + b)$. A labeling $\\{\\ell_i\\}$ is realized if there exist parameters $(\\mathbf{w}, b)$ specified by the hypothesis class such that $\\ell_i (\\mathbf{w}^{\\top}\\mathbf{x}_i + b)  0$ for all $i$. For the sign function, we define $\\mathrm{sign}(z)=+1$ if $z \\geq 0$ and $\\mathrm{sign}(z)=-1$ if $z  0$. The strict inequality is used to avoid issues with points lying on the boundary.\n\n**Step 2: VC Dimension of Unrestricted Halfspaces, $d_{\\mathrm{VC}}(\\mathcal{H})$**\n\nThe hypothesis class $\\mathcal{H} = \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}^{d},\\ b \\in \\mathbb{R}\\}$ corresponds to the set of all separating hyperplanes in $\\mathbb{R}^d$. It is a standard result in statistical learning theory that the VC dimension of this class is $d+1$. We demonstrate this by proving a lower and an upper bound.\n\n*   **Lower Bound: $d_{\\mathrm{VC}}(\\mathcal{H}) \\geq d+1$**\n\n    To show that the VC dimension is at least $d+1$, we must find a set of $d+1$ points that can be shattered by $\\mathcal{H}$. Consider the set of $d+1$ points in $\\mathbb{R}^d$ consisting of the origin and the standard basis vectors: $S = \\{\\mathbf{0}, \\mathbf{e}_1, \\dots, \\mathbf{e}_d\\}$.\n\n    For any arbitrary labeling $(\\ell_0, \\ell_1, \\dots, \\ell_d) \\in \\{-1, +1\\}^{d+1}$, we need to find $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ such that:\n    1.  $\\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{0} + b) = \\mathrm{sign}(b) = \\ell_0$\n    2.  $\\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{e}_i + b) = \\mathrm{sign}(w_i + b) = \\ell_i$ for $i=1, \\dots, d$.\n\n    We can construct $(\\mathbf{w}, b)$ as follows. Set $b = \\ell_0$.\n    Then, for $i=1, \\dots, d$, we need $\\mathrm{sign}(w_i + \\ell_0) = \\ell_i$.\n    - If $\\ell_i = \\ell_0$, we need $w_i + \\ell_0$ to have the same sign as $\\ell_0$. We can choose $w_i = 0$. Then $w_i + \\ell_0 = \\ell_0$, which has sign $\\ell_0$.\n    - If $\\ell_i = -\\ell_0$, we need $w_i + \\ell_0$ to have the opposite sign of $\\ell_0$. We can choose $w_i = -2\\ell_0$. Then $w_i + \\ell_0 = -2\\ell_0 + \\ell_0 = -\\ell_0$, which has sign $-\\ell_0 = \\ell_i$.\n\n    Since we can find a $(\\mathbf{w}, b)$ for any of the $2^{d+1}$ possible labelings, the set $S$ is shattered. This proves that $d_{\\mathrm{VC}}(\\mathcal{H}) \\geq d+1$.\n\n*   **Upper Bound: $d_{\\mathrm{VC}}(\\mathcal{H}) \\leq d+1$**\n\n    To show that the VC dimension is at most $d+1$, we must show that no set of $d+2$ points can be shattered. This is a direct consequence of Radon's Theorem.\n    Let $S = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{d+2}\\}$ be any set of $d+2$ points in $\\mathbb{R}^d$. By Radon's Theorem, $S$ can be partitioned into two disjoint subsets, $S_1$ and $S_2$, such that their convex hulls intersect: $\\mathrm{conv}(S_1) \\cap \\mathrm{conv}(S_2) \\neq \\emptyset$.\n\n    Consider the labeling that assigns $+1$ to all points in $S_1$ and $-1$ to all points in $S_2$. For this labeling to be realized by a hyperplane $h_{\\mathbf{w},b}$, we would need $\\mathbf{w}^\\top\\mathbf{x} + b  0$ for all $\\mathbf{x} \\in S_1$ and $\\mathbf{w}^\\top\\mathbf{x} + b  0$ for all $\\mathbf{x} \\in S_2$. By linearity, this implies the hyperplane separates the convex hulls of $S_1$ and $S_2$. However, since their convex hulls intersect, they cannot be separated by a hyperplane. Therefore, this specific labeling cannot be realized.\n\n    Since for any set of $d+2$ points there exists at least one labeling that cannot be realized, no set of $d+2$ points can be shattered. This proves $d_{\\mathrm{VC}}(\\mathcal{H}) \\leq d+1$.\n\nCombining the bounds, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}) = d+1$.\n\n**Step 3: VC Dimension of Excitatory-Only Perceptrons, $d_{\\mathrm{VC}}(\\mathcal{H}_{+})$**\n\nThe hypothesis class $\\mathcal{H}_{+} = \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}_{+}^{d}, b \\in \\mathbb{R}\\}$ consists of hyperplanes whose normal vectors $\\mathbf{w}$ have non-negative components ($w_i \\geq 0$ for all $i$). This constraint introduces a monotonicity property. If $\\mathbf{x} \\succeq \\mathbf{y}$ (i.e., $x_i \\geq y_i$ for all $i$), then for any $\\mathbf{w} \\in \\mathbb{R}_+^d$, we have $\\mathbf{w}^\\top\\mathbf{x} \\geq \\mathbf{w}^\\top\\mathbf{y}$. This implies $\\mathbf{w}^\\top\\mathbf{x}+b \\geq \\mathbf{w}^\\top\\mathbf{y}+b$. Thus, if $h(\\mathbf{y})=+1$, it is impossible to have $h(\\mathbf{x})=-1$.\n\n*   **Lower Bound: $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\geq d$**\n\n    We must find a set of $d$ points that can be shattered by $\\mathcal{H}_{+}$. Consider the set of standard basis vectors $S = \\{\\mathbf{e}_1, \\dots, \\mathbf{e}_d\\}$. This set is an antichain, meaning no element dominates another, so the monotonicity property does not trivially prevent shattering.\n\n    For any arbitrary labeling $(\\ell_1, \\dots, \\ell_d) \\in \\{-1, +1\\}^d$, we need to find $\\mathbf{w} \\in \\mathbb{R}_+^d$ and $b \\in \\mathbb{R}$ such that $\\mathrm{sign}(w_i + b) = \\ell_i$ for all $i$.\n    - Set $b = -1/2$.\n    - For each $i \\in \\{1, \\dots, d\\}$:\n        - If $\\ell_i = +1$, we need $w_i - 1/2 \\geq 0$, so $w_i \\geq 1/2$. We can choose $w_i = 1$. This satisfies $w_i \\geq 0$.\n        - If $\\ell_i = -1$, we need $w_i - 1/2  0$, so $w_i  1/2$. We can choose $w_i = 0$. This satisfies $w_i \\geq 0$.\n    This construction provides a valid $(\\mathbf{w}, b)$ for any labeling. Thus, the set $S$ can be shattered, which proves $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\geq d$.\n\n*   **Upper Bound: $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\leq d$**\n\n    We show that no set of $d+1$ points can be shattered. Let $S = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{d+1}\\}$ be an arbitrary set of $d+1$ points in $\\mathbb{R}^d$.\n\n    First, we can translate the entire coordinate system without changing the shatterability of the set by $\\mathcal{H}_{+}$. Let $\\mathbf{p} \\in \\mathbb{R}^d$ be a vector where each component $p_j$ is the minimum of the $j$-th coordinates of all points in $S$: $p_j = \\min_{i=1, \\dots, d+1} x_{ij}$. Let's define a new set of points $\\mathbf{y}_i = \\mathbf{x}_i - \\mathbf{p}$. A classifier $h_{\\mathbf{w},b}(\\mathbf{x})$ on the original points is equivalent to a classifier $h_{\\mathbf{w},b'}(\\mathbf{y})$ on the new points, where $b' = b + \\mathbf{w}^\\top\\mathbf{p}$. Since $\\mathbf{w}$ and $\\mathbf{p}$ are fixed for a given classifier, $b'$ is just another bias term. The constraint $\\mathbf{w} \\in \\mathbb{R}_+^d$ is unaffected. The new points $\\{\\mathbf{y}_i\\}$ have the property that $y_{ij} \\geq 0$ for all $i,j$.\n\n    The set $\\{\\mathbf{y}_1, \\dots, \\mathbf{y}_{d+1}\\}$ consists of $d+1$ vectors in $\\mathbb{R}^d$. Therefore, they must be linearly dependent. There exist scalars $\\alpha_1, \\dots, \\alpha_{d+1}$, not all zero, such that:\n    $$ \\sum_{i=1}^{d+1} \\alpha_i \\mathbf{y}_i = \\mathbf{0} $$\n    Let $P = \\{i : \\alpha_i  0\\}$ and $N = \\{i : \\alpha_i  0\\}$. At least one of these sets is non-empty. If either is empty, the corresponding labeling (all $+1$ or all $-1$) is trivially realizable by setting $\\mathbf{w}=\\mathbf{0}$ and $b=1$ or $b=-1$. However, we can always find a linear dependence such that both $P$ and $N$ are non-empty, unless all points lie on a single ray from the origin, a degenerate case we can ignore.\n\n    Consider the labeling given by $\\ell_i = \\mathrm{sgn}(\\alpha_i)$ for $i \\in P \\cup N$. Suppose, for the sake of contradiction, that this labeling can be realized by some $h_{\\mathbf{w},b} \\in \\mathcal{H}_{+}$, with $\\mathbf{w} \\in \\mathbb{R}_+^d$. Then we have:\n    - $\\mathbf{w}^\\top\\mathbf{y}_i + b  0$ for $i \\in P$\n    - $\\mathbf{w}^\\top\\mathbf{y}_i + b  0$ for $i \\in N$\n\n    For any $i \\in N$, we have $\\mathbf{w}^\\top\\mathbf{y}_i + b  0$. Since $\\mathbf{w} \\in \\mathbb{R}_+^d$ and all components of $\\mathbf{y}_i$ are non-negative, $\\mathbf{w}^\\top\\mathbf{y}_i \\geq 0$. This forces the bias term to be negative: $b  0$.\n\n    Now, multiply each inequality by the corresponding $\\alpha_i$ and sum over all $i$:\n    $$ \\sum_{i=1}^{d+1} \\alpha_i (\\mathbf{w}^\\top\\mathbf{y}_i + b)  0 $$\n    The reason for the strict inequality is that for each $i \\in P \\cup N$, $\\alpha_i$ has the same sign as $(\\mathbf{w}^\\top\\mathbf{y}_i + b)$, so their product is positive. The sum of positive terms is positive. Distributing the sum:\n    $$ \\sum_{i=1}^{d+1} \\alpha_i (\\mathbf{w}^\\top\\mathbf{y}_i) + \\sum_{i=1}^{d+1} \\alpha_i b  0 $$\n    $$ \\mathbf{w}^\\top \\left(\\sum_{i=1}^{d+1} \\alpha_i \\mathbf{y}_i\\right) + b \\left(\\sum_{i=1}^{d+1} \\alpha_i\\right)  0 $$\n    By the linear dependence, the first term is $\\mathbf{w}^\\top\\mathbf{0} = 0$. We are left with:\n    $$ b \\left(\\sum_{i=1}^{d+1} \\alpha_i\\right)  0 $$\n    It is a standard result from convex geometry that for a set of points like $\\{\\mathbf{y}_i\\}$, constructed by this translation, a linear dependence can be chosen such that $\\sum \\alpha_i \\geq 0$. Assuming this, we have two conditions:\n    1.  $b  0$ (derived from considering points labeled $-1$)\n    2.  $b (\\sum \\alpha_i)  0$ where $\\sum \\alpha_i \\geq 0$.\n\n    If $\\sum \\alpha_i  0$, these two conditions imply $b0$ and $b0$, a contradiction. If $\\sum \\alpha_i = 0$, the second condition becomes $0  0$, also a contradiction.\n    Therefore, the labeling based on the signs of the $\\alpha_i$ coefficients cannot be realized. This shows that no set of $d+1$ points can be shattered by $\\mathcal{H}_+$. This proves $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\leq d$.\n\nCombining the bounds, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) = d$.\n\n**Step 4: Comparison and Final Answer**\n\nWe have found that the VC dimension of unrestricted halfspaces in $\\mathbb{R}^d$ is $d_{\\mathrm{VC}}(\\mathcal{H}) = d+1$. The VC dimension of excitatory-only perceptrons (non-negative weights) in $\\mathbb{R}^d$ is $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) = d$.\n\nThe constraint of non-negative weights reduces the complexity of the hypothesis class, decreasing its VC dimension by exactly 1.\n\nThe final answer is a two-entry row vector giving $[d_{\\mathrm{VC}}(\\mathcal{H}_{+}), d_{\\mathrm{VC}}(\\mathcal{H})]$.\nThis is $[d, d+1]$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nd  d+1\n\\end{pmatrix}\n}\n$$", "id": "3192435"}, {"introduction": "The concept of VC dimension extends far beyond the geometric intuition of hyperplanes in $\\mathbb{R}^d$. To illustrate its generality, this practice examines a different type of hypothesis class defined on the real number line [@problem_id:3192499]. Here, classifiers are formed by thresholding piecewise-constant functions, where the model's complexity is intuitively captured by the maximum number of \"jumps\" ($k$) allowed. By connecting this discrete complexity parameter $k$ to the VC dimension, you will gain a deeper appreciation for how this tool can quantify the capacity of functionally-defined, non-linear model families.", "problem": "Let $k \\in \\mathbb{N}$ be fixed. Consider the class $\\mathcal{F}_{k}$ of real-valued functions $f:\\mathbb{R}\\to\\mathbb{R}$ that are piecewise-constant with at most $k$ jumps, meaning there exist at most $k$ points in $\\mathbb{R}$ at which $f$ changes its value. From $\\mathcal{F}_{k}$ form the hypothesis class of threshold classifiers\n$$\n\\mathcal{H}_{k} \\;=\\; \\left\\{\\, h_{f,\\theta}(x) \\;=\\; \\mathbf{1}\\{ f(x) \\ge \\theta \\} \\;:\\; f \\in \\mathcal{F}_{k},\\ \\theta \\in \\mathbb{R} \\,\\right\\}.\n$$\nUsing only the foundational definitions of shattering and Vapnik–Chervonenkis (VC) dimension, determine the Vapnik–Chervonenkis dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{k})$ as a function of $k$, and construct an explicit set $S \\subset \\mathbb{R}$ of size $d_{\\mathrm{VC}}(\\mathcal{H}_{k})$ that is shattered by $\\mathcal{H}_{k}$. Your final answer must be the exact closed-form expression for $d_{\\mathrm{VC}}(\\mathcal{H}_{k})$. No numerical rounding is required.", "solution": "The Vapnik-Chervonenkis (VC) dimension of a hypothesis class $\\mathcal{H}$, denoted $d_{\\mathrm{VC}}(\\mathcal{H})$, is the size of the largest finite set of points $S$ that can be shattered by $\\mathcal{H}$. A set $S$ is shattered by $\\mathcal{H}$ if for every possible binary labeling of the points in $S$, there exists a hypothesis $h \\in \\mathcal{H}$ that realizes that labeling.\n\nThe problem asks for the VC dimension of the hypothesis class\n$$\n\\mathcal{H}_{k} \\;=\\; \\left\\{\\, h_{f,\\theta}(x) \\;=\\; \\mathbf{1}\\{ f(x) \\ge \\theta \\} \\;:\\; f \\in \\mathcal{F}_{k},\\ \\theta \\in \\mathbb{R} \\,\\right\\}\n$$\nwhere $\\mathcal{F}_k$ is the class of real-valued functions $f:\\mathbb{R}\\to\\mathbb{R}$ that are piecewise-constant with at most $k$ jumps, for a fixed $k \\in \\mathbb{N}$.\n\nTo determine $d_{\\mathrm{VC}}(\\mathcal{H}_{k})$, we will establish a lower bound and an upper bound on its value. We will show that $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\ge k+1$ and $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\le k+1$, which together imply $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) = k+1$.\n\n**Part 1: Lower Bound, $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\ge k+1$**\n\nTo establish a lower bound, we must find a set of points $S$ of size $k+1$ that is shattered by $\\mathcal{H}_{k}$.\nLet us choose the set $S = \\{1, 2, \\dots, k+1\\}$. The size of this set is $|S| = k+1$.\nTo show that $S$ is shattered, we must demonstrate that for any possible labeling $y = (y_1, y_2, \\dots, y_{k+1}) \\in \\{0, 1\\}^{k+1}$, there exists a hypothesis $h \\in \\mathcal{H}_k$ such that $h(i) = y_i$ for all $i \\in S$. A hypothesis $h$ is defined by a function $f \\in \\mathcal{F}_k$ and a threshold $\\theta \\in \\mathbb{R}$.\n\nFor an arbitrary labeling $y \\in \\{0, 1\\}^{k+1}$, we construct a specific function $f_y : \\mathbb{R} \\to \\mathbb{R}$. Let the potential jump points be $t_i = i + 0.5$ for $i=1, 2, \\dots, k$. Define $f_y$ as follows:\n$$\nf_y(x) = \\begin{cases}\n    y_1  \\text{if } x  t_1 \\\\\n    y_i  \\text{if } t_{i-1} \\le x  t_i \\quad \\text{for } i=2, \\dots, k \\\\\n    y_{k+1}  \\text{if } x \\ge t_k\n\\end{cases}\n$$\nThis function $f_y$ is piecewise-constant by construction. The value of $f_y(x)$ can change only at the points $t_1, t_2, \\dots, t_k$. A jump occurs at $t_i$ if and only if $y_i \\neq y_{i+1}$. The total number of jumps is $\\sum_{i=1}^{k} \\mathbf{1}\\{y_i \\neq y_{i+1}\\}$. For any binary sequence of length $k+1$, the maximum number of such alternations is $k$ (e.g., for the sequence $(0, 1, 0, 1, \\dots)$). Thus, the number of jumps of $f_y$ is at most $k$, which means $f_y \\in \\mathcal{F}_k$.\n\nBy this construction, for each point $i \\in S = \\{1, 2, \\dots, k+1\\}$, the value of the function is $f_y(i) = y_i$. For example, for $i=1$, $1  t_1 = 1.5$, so $f_y(1) = y_1$. For $i \\in \\{2, \\dots, k\\}$, we have $t_{i-1} = i-0.5 \\le i  i+0.5 = t_i$, so $f_y(i) = y_i$. For $i=k+1$, we have $k+1  t_k = k+0.5$, so $f_y(k+1) = y_{k+1}$.\n\nNow, we select a threshold $\\theta$. Let $\\theta = 0.5$. The corresponding hypothesis $h_{f_y, \\theta}$ is given by $h(x) = \\mathbf{1}\\{f_y(x) \\ge 0.5\\}$.\nFor any point $i \\in S$, the classification is:\n$$\nh(i) = \\mathbf{1}\\{f_y(i) \\ge 0.5\\} = \\mathbf{1}\\{y_i \\ge 0.5\\}\n$$\nSince $y_i \\in \\{0, 1\\}$, the condition $y_i \\ge 0.5$ is true if and only if $y_i = 1$. Therefore, $h(i) = y_i$.\nThis holds for all $i \\in S$. We have successfully constructed a hypothesis in $\\mathcal{H}_k$ that realizes the arbitrary labeling $y$. Since this is possible for all $2^{k+1}$ labelings, the set $S$ is shattered by $\\mathcal{H}_k$.\nAs we have found a shattered set of size $k+1$, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\ge k+1$.\n\n**Part 2: Upper Bound, $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\le k+1$**\n\nTo establish an upper bound, we must show that no set of size $k+2$ can be shattered by $\\mathcal{H}_k$.\nLet $h \\in \\mathcal{H}_k$ be an arbitrary hypothesis. By definition, $h(x) = \\mathbf{1}\\{f(x) \\ge \\theta\\}$ for some function $f \\in \\mathcal{F}_k$ and threshold $\\theta \\in \\mathbb{R}$. The function $f$ is piecewise-constant with at most $k$ jumps. Let the set of jump points of $f$ be $T_f = \\{p_1, \\dots, p_m\\}$ where $m \\le k$.\nThe function $h(x)$ is also piecewise-constant. Its value can change only at the points where $f(x)$ changes value. Thus, the jump points of $h$ must be a subset of the jump points of $f$. So, $h(x)$ has at most $m \\le k$ jumps.\n\nLet $S' = \\{x_1, x_2, \\dots, x_{k+2}\\}$ be any set of $k+2$ distinct points on the real line, ordered such that $x_1  x_2  \\dots  x_{k+2}$.\nConsider the labeling of $S'$ induced by $h$: $(h(x_1), h(x_2), \\dots, h(x_{k+2}))$.\nLet's count the number of alternations in this sequence of labels, which is the number of times the label changes between adjacent points: $A = \\sum_{i=1}^{k+1} \\mathbf{1}\\{h(x_i) \\neq h(x_{i+1})\\}$.\nIf $h(x_i) \\neq h(x_{i+1})$, the function $h$ must change its value somewhere in the open interval $(x_i, x_{i+1})$. Since $h$ is a piecewise-constant function, this implies there must be at least one jump point of $h$ in $(x_i, x_{i+1})$.\nThe intervals $(x_1, x_2), (x_2, x_3), \\dots, (x_{k+1}, x_{k+2})$ are all disjoint. Therefore, the number of alternations $A$ is at most the total number of jump points of $h$. As we established, the number of jumps of $h$ is at most $k$.\nSo, for any $h \\in \\mathcal{H}_k$, the labeling it induces on $S'$ can have at most $k$ alternations.\n\nNow, consider the specific labeling $y' = (1, 0, 1, 0, \\dots)$. The number of alternations in this sequence of length $k+2$ is $(k+2)-1 = k+1$.\nSince $k+1  k$, this specific labeling $y'$ has more alternations than any hypothesis in $\\mathcal{H}_k$ can generate on the ordered set $S'$. Therefore, this labeling cannot be realized by any $h \\in \\mathcal{H}_k$.\nThis means that the set $S'$ cannot be shattered by $\\mathcal{H}_k$. As this holds for any set of $k+2$ points, we conclude that no set of size $k+2$ can be shattered.\nBy the definition of VC dimension, this implies $d_{\\mathrm{VC}}(\\mathcal{H}_{k})  k+2$, or $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\le k+1$.\n\n**Conclusion**\n\nFrom Part 1, we have $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\ge k+1$.\nFrom Part 2, we have $d_{\\mathrm{VC}}(\\mathcal{H}_{k}) \\le k+1$.\nCombining these two inequalities, we uniquely determine the VC dimension:\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{k}) = k+1.\n$$\nThe explicit set of size $k+1$ that is shattered is $S = \\{1, 2, \\dots, k+1\\}$.", "answer": "$$\\boxed{k+1}$$", "id": "3192499"}]}