{"hands_on_practices": [{"introduction": "At the heart of XGBoost's efficiency is its use of second-order information to guide the learning process. Instead of just following the steepest descent (the gradient), it uses a more refined quadratic approximation of the loss function. This practice takes you into the engine room of the algorithm, asking you to derive the gradient ($g_{ik}$) and diagonal Hessian ($h_{ik}$) for the multi-class softmax objective from first principles. Mastering this derivation will clarify how XGBoost formulates the target for each new tree in a classification setting. [@problem_id:3120267]", "problem": "Consider eXtreme Gradient Boosting (XGBoost), which optimizes a regularized objective by fitting decision trees to second-order Taylor expansions of the loss. For multi-class classification with $K$ classes, XGBoost uses the softmax model with class scores $f_{ik}$ for instance $i$ and class $k$, and probabilities defined by the softmax transform $p_{ik} = \\frac{\\exp(f_{ik})}{\\sum_{j=1}^{K} \\exp(f_{ij})}$. The per-instance loss is the negative log-likelihood of the true class under the softmax probabilities. \n\nStarting only from these definitions, derive expressions for the per-instance gradient $g_{ik}$ and the per-instance diagonal second derivative (the diagonal of the Hessian) $h_{ik}$ of the loss with respect to the score $f_{ik}$.\n\nThen, test your derivation on the following $3$-class toy dataset. There are $N=3$ training instances in a single leaf, with class labels\n$$\ny_1 = 1,\\quad y_2 = 1,\\quad y_3 = 3.\n$$\nAssume the current boosting iteration uses zero initialization for all class scores, so $f_{ik} = 0$ for all $i$ and $k$. Let the $\\ell_2$ regularization parameter be $\\lambda = \\frac{1}{2}$.\n\nUsing the second-order leaf-weight solution employed by XGBoost for class $k=2$, compute the optimal leaf weight\n$$\nw^{\\ast} = -\\frac{\\sum_{i=1}^{N} g_{i2}}{\\sum_{i=1}^{N} h_{i2} + \\lambda}.\n$$\n\nReport the value of $w^{\\ast}$ as a single number. No rounding is required.", "solution": "We begin from the fundamental definitions for multi-class logistic (softmax) modeling. For an instance $i$ with scores $\\mathbf{f}_i = (f_{i1}, \\dots, f_{iK})$, the softmax probability for class $k$ is\n$$\np_{ik} = \\frac{\\exp(f_{ik})}{\\sum_{j=1}^{K} \\exp(f_{ij})}.\n$$\nThe negative log-likelihood (cross-entropy) loss for instance $i$ and true class $y_i$ is\n$$\nL_i(\\mathbf{f}_i) = -\\ln\\left(p_{i, y_i}\\right) = -\\sum_{k=1}^{K} \\mathbf{1}[y_i = k] \\ln(p_{ik}),\n$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function.\n\nWe derive the gradient $\\frac{\\partial L_i}{\\partial f_{ik}}$ using the chain rule. First note that $L_i = -\\sum_{k} y_{ik} \\ln(p_{ik})$ where $y_{ik} = \\mathbf{1}[y_i=k]$. The derivative of $\\ln(p_{ik})$ with respect to $f_{il}$ uses the softmax derivative. The softmax Jacobian is well known and can be derived as follows:\n$$\np_{ik} = \\frac{\\exp(f_{ik})}{Z_i},\\quad \\text{with}\\quad Z_i = \\sum_{j=1}^{K} \\exp(f_{ij}).\n$$\nThen\n$$\n\\frac{\\partial p_{ik}}{\\partial f_{il}} = \\frac{\\exp(f_{ik})}{Z_i}\\left(\\delta_{kl} - \\frac{\\exp(f_{il})}{Z_i}\\right) = p_{ik}(\\delta_{kl} - p_{il}),\n$$\nwhere $\\delta_{kl}$ is the Kronecker delta. Consequently,\n$$\n\\frac{\\partial \\ln(p_{ik})}{\\partial f_{il}} = \\frac{1}{p_{ik}}\\frac{\\partial p_{ik}}{\\partial f_{il}} = \\delta_{kl} - p_{il}.\n$$\nThus, the gradient of $L_i$ with respect to $f_{il}$ is\n$$\n\\frac{\\partial L_i}{\\partial f_{il}} = -\\sum_{k=1}^{K} y_{ik}\\left(\\delta_{kl} - p_{il}\\right) = -y_{il} + \\left(\\sum_{k=1}^{K} y_{ik}\\right)p_{il}.\n$$\nSince $\\sum_{k=1}^{K} y_{ik} = 1$, we have\n$$\n\\frac{\\partial L_i}{\\partial f_{il}} = p_{il} - y_{il} = p_{il} - \\mathbf{1}[y_i=l].\n$$\nRelabeling the index $l$ as $k$ for clarity, the per-instance gradient is\n$$\ng_{ik} = \\frac{\\partial L_i}{\\partial f_{ik}} = p_{ik} - \\mathbf{1}[y_i = k].\n$$\n\nNext, we derive the Hessian. The Hessian with respect to $\\mathbf{f}_i$ is the matrix with entries\n$$\n\\frac{\\partial^2 L_i}{\\partial f_{ik}\\,\\partial f_{il}} = \\frac{\\partial}{\\partial f_{il}}\\left(p_{ik} - y_{ik}\\right) = \\frac{\\partial p_{ik}}{\\partial f_{il}} = p_{ik}\\left(\\delta_{kl} - p_{il}\\right).\n$$\nTherefore, the full Hessian for instance $i$ is\n$$\nH_i = \\operatorname{diag}(\\mathbf{p}_i) - \\mathbf{p}_i \\mathbf{p}_i^{\\top},\n$$\nwhere $\\mathbf{p}_i = (p_{i1},\\dots,p_{iK})$. The diagonal entries are\n$$\n\\frac{\\partial^2 L_i}{\\partial f_{ik}^2} = p_{ik}(1 - p_{ik}).\n$$\nIn XGBoost’s per-class tree construction, the second-order term used for class $k$ is the diagonal element for that class, so the per-instance diagonal second derivative is\n$$\nh_{ik} = p_{ik}(1 - p_{ik}).\n$$\n\nWe now test on the specified $3$-class dataset with $N=3$ instances and labels $y_1=1$, $y_2=1$, $y_3=3$. The current scores are $f_{ik}=0$ for all $i$ and $k$. Under this initialization,\n$$\np_{ik} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)+\\exp(0)} = \\frac{1}{3}\\quad \\text{for all } i,k.\n$$\n\nCompute the per-instance gradient for class $k=2$:\n- For instance $i=1$, $y_1=1 \\neq 2$, so $\\mathbf{1}[y_1=2]=0$ and $g_{12} = \\frac{1}{3} - 0 = \\frac{1}{3}$.\n- For instance $i=2$, $y_2=1 \\neq 2$, so $g_{22} = \\frac{1}{3} - 0 = \\frac{1}{3}$.\n- For instance $i=3$, $y_3=3 \\neq 2$, so $g_{32} = \\frac{1}{3} - 0 = \\frac{1}{3}$.\n\nAggregate gradient for class $2$ over the leaf:\n$$\nG_2 = \\sum_{i=1}^{3} g_{i2} = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1.\n$$\n\nCompute the per-instance diagonal second derivative for class $k=2$:\n$$\nh_{i2} = p_{i2}(1 - p_{i2}) = \\frac{1}{3}\\left(1 - \\frac{1}{3}\\right) = \\frac{1}{3}\\cdot \\frac{2}{3} = \\frac{2}{9}.\n$$\nAggregate Hessian for class $2$ over the leaf:\n$$\nH_2 = \\sum_{i=1}^{3} h_{i2} = 3 \\cdot \\frac{2}{9} = \\frac{6}{9} = \\frac{2}{3}.\n$$\n\nWith $\\lambda = \\frac{1}{2}$, the second-order optimal leaf weight for class $2$ is\n$$\nw^{\\ast} = -\\frac{G_2}{H_2 + \\lambda} = -\\frac{1}{\\frac{2}{3} + \\frac{1}{2}} = -\\frac{1}{\\frac{4}{6} + \\frac{3}{6}} = -\\frac{1}{\\frac{7}{6}} = -\\frac{6}{7}.\n$$\n\nThis is a single exact rational number, so no rounding is required.", "answer": "$$\\boxed{-\\frac{6}{7}}$$", "id": "3120267"}, {"introduction": "Once the gradients and Hessians are computed, XGBoost must decide how to grow its trees. Every potential split is evaluated based on the 'gain' it provides—a measure of how much it reduces the objective function. This process is tempered by regularization, and the hyperparameter $\\gamma$ plays a crucial role as a complexity penalty. This exercise challenges you to derive the exact condition under which $\\gamma$ prevents a split, connecting the abstract concept of regularization to the concrete mechanics of tree pruning. [@problem_id:3120320]", "problem": "Consider Extreme Gradient Boosting (XGBoost), which fits decision trees by minimizing a regularized training objective using a second-order Taylor expansion around current predictions. For a single leaf with weight $w$, suppose the per-iteration approximate contribution to the objective is\n$$\\sum_{i \\in \\text{leaf}} \\big( g_i w + \\tfrac{1}{2} h_i w^2 \\big) + \\tfrac{\\lambda}{2} w^2 + \\gamma,$$\nwhere $g_i$ and $h_i$ denote, respectively, the first and second derivatives of the loss with respect to the prediction at data point $i$, $\\lambda$ is the coefficient of the $\\ell_2$ regularization on leaf weights, and $\\gamma$ is the complexity penalty per leaf. Let $G = \\sum_{i \\in \\text{leaf}} g_i$ and $H = \\sum_{i \\in \\text{leaf}} h_i$ denote the aggregate first and second derivatives over that leaf.\n\nSuppose a candidate split partitions the data at a node into a left child with aggregates $(G_L, H_L)$ and a right child with aggregates $(G_R, H_R)$, while the parent has aggregates $(G_P, H_P)$ with $G_P = G_L + G_R$ and $H_P = H_L + H_R$. Assume $H_L > 0$, $H_R > 0$, and $\\lambda > 0$.\n\nStarting from the regularized objective above and without invoking any pre-memorized formulas, derive an exact analytic expression for the minimum value of the leaf penalty $\\gamma$ such that the net objective reduction from performing the split is non-positive (i.e., the split does not occur). Then, evaluate this minimum $\\gamma$ for the specific values $G_L = 7$, $H_L = 14$, $G_R = -3$, $H_R = 6$, and $\\lambda = 1$. Round your final numerical value for the minimal $\\gamma$ to four significant figures. Your final answer must be a single real number.", "solution": "The problem asks for the minimum value of $\\gamma$ such that a split is not performed. This occurs when the total objective function after the split is greater than or equal to the objective function if no split were made. We must first find the minimized objective value for a given tree structure.\n\nLet $\\tilde{\\mathcal{L}}$ be the total objective function for the new tree being added. For a tree with $T$ leaves, this is the sum of the contributions from each leaf:\n$$ \\tilde{\\mathcal{L}} = \\sum_{j=1}^{T} \\left[ \\sum_{i \\in I_j} \\left( g_i w_j + \\frac{1}{2} h_i w_j^2 \\right) + \\frac{\\lambda}{2} w_j^2 + \\gamma \\right] $$\nwhere $I_j$ is the set of data points in leaf $j$ and $w_j$ is the weight of leaf $j$.\n\nUsing the given aggregate sums $G_j = \\sum_{i \\in I_j} g_i$ and $H_j = \\sum_{i \\in I_j} h_i$, we can rewrite the objective for a single leaf $j$ as:\n$$ \\tilde{\\mathcal{L}}_j = G_j w_j + \\frac{1}{2} H_j w_j^2 + \\frac{\\lambda}{2} w_j^2 + \\gamma = G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 + \\gamma $$\nThe total objective for the tree is:\n$$ \\tilde{\\mathcal{L}} = \\sum_{j=1}^{T} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + T\\gamma $$\nFor a fixed tree structure (i.e., fixed leaves), this objective is a sum of independent quadratic functions of the leaf weights $w_j$. We can find the optimal weight $w_j^*$ for each leaf by minimizing its contribution to the objective. We take the derivative with respect to $w_j$ and set it to $0$:\n$$ \\frac{\\partial \\tilde{\\mathcal{L}}_j}{\\partial w_j} = G_j + (H_j + \\lambda) w_j = 0 $$\nGiven that $H_L > 0$, $H_R > 0$, and $\\lambda > 0$, it follows that $H_j > 0$ for any leaf, and thus $H_j + \\lambda > 0$. Therefore, a unique minimum exists. The optimal weight $w_j^*$ is:\n$$ w_j^* = -\\frac{G_j}{H_j + \\lambda} $$\nSubstituting this optimal weight back into the objective function for leaf $j$ gives the minimum objective value for that leaf:\n$$ \\tilde{\\mathcal{L}}_j^* = G_j \\left( -\\frac{G_j}{H_j + \\lambda} \\right) + \\frac{1}{2} (H_j + \\lambda) \\left( -\\frac{G_j}{H_j + \\lambda} \\right)^2 + \\gamma $$\n$$ \\tilde{\\mathcal{L}}_j^* = -\\frac{G_j^2}{H_j + \\lambda} + \\frac{1}{2} \\frac{G_j^2}{H_j + \\lambda} + \\gamma = -\\frac{1}{2} \\frac{G_j^2}{H_j + \\lambda} + \\gamma $$\nThis value is often referred to as the \"score\" of the leaf.\n\nNow, we compare the total objective value for two scenarios:\n1.  **No Split:** The parent node is treated as a single leaf. Here, $T=1$. The objective is:\n    $$ \\tilde{\\mathcal{L}}_{\\text{no-split}} = -\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma $$\n2.  **Split:** The parent node is split into two leaves, Left ($L$) and Right ($R$). Here, $T=2$. The total objective is the sum of the scores of the two new leaves:\n    $$ \\tilde{\\mathcal{L}}_{\\text{split}} = \\left( -\\frac{1}{2} \\frac{G_L^2}{H_L + \\lambda} + \\gamma \\right) + \\left( -\\frac{1}{2} \\frac{G_R^2}{H_R + \\lambda} + \\gamma \\right) = -\\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} \\right) + 2\\gamma $$\nA split is beneficial if it reduces the overall objective function, i.e., if $\\tilde{\\mathcal{L}}_{\\text{split}} < \\tilde{\\mathcal{L}}_{\\text{no-split}}$. The \"net objective reduction\", often called gain, is $\\text{Gain} = \\tilde{\\mathcal{L}}_{\\text{no-split}} - \\tilde{\\mathcal{L}}_{\\text{split}}$.\n\nThe problem states that the split does not occur if this net objective reduction is non-positive. So, we set the condition $\\text{Gain} \\le 0$:\n$$ \\tilde{\\mathcal{L}}_{\\text{no-split}} - \\tilde{\\mathcal{L}}_{\\text{split}} \\le 0 $$\n$$ \\left( -\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma \\right) - \\left( -\\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} \\right) + 2\\gamma \\right) \\le 0 $$\n$$ \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right) - \\gamma \\le 0 $$\nRearranging to solve for $\\gamma$:\n$$ \\gamma \\ge \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right) $$\nThe minimum value of $\\gamma$ that ensures the split does not occur is the value that makes the inequality an equality. Using $G_P = G_L + G_R$ and $H_P = H_L + H_R$:\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right) $$\nThis is the required analytic expression.\n\nNow we evaluate this expression for the given values: $G_L = 7$, $H_L = 14$, $G_R = -3$, $H_R = 6$, and $\\lambda = 1$.\nFirst, calculate the parent aggregates:\n$$ G_P = G_L + G_R = 7 + (-3) = 4 $$\n$$ H_P = H_L + H_R = 14 + 6 = 20 $$\nSubstitute these values into the expression for $\\gamma_{\\text{min}}$:\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{7^2}{14 + 1} + \\frac{(-3)^2}{6 + 1} - \\frac{4^2}{20 + 1} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{49}{15} + \\frac{9}{7} - \\frac{16}{21} \\right) $$\nTo sum the fractions, we find a common denominator, which is $\\text{lcm}(15, 7, 21) = 105$.\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{49 \\times 7}{15 \\times 7} + \\frac{9 \\times 15}{7 \\times 15} - \\frac{16 \\times 5}{21 \\times 5} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{343}{105} + \\frac{135}{105} - \\frac{80}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{343 + 135 - 80}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{478 - 80}{105} \\right) = \\frac{1}{2} \\left( \\frac{398}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{199}{105} $$\nAs a decimal, this is approximately $1.895238...$. Rounding to four significant figures gives $1.895$.", "answer": "$$\\boxed{1.895}$$", "id": "3120320"}, {"introduction": "Tuning hyperparameters is both a science and an art. Setting regularization parameters like $\\lambda$ and $\\gamma$ too loosely can lead to overfitting, but setting them too strictly can cause underfitting, where the model fails to capture even the basic patterns in the data. This diagnostic exercise presents a scenario where overly strong regularization prevents the model from learning at all. By working through this problem, you will learn to identify the symptoms of regularization-induced underfitting and reason about effective strategies to remedy it, a critical skill for any machine learning practitioner. [@problem_id:3120315]", "problem": "You are training an eXtreme Gradient Boosting (XGBoost) classifier, a form of gradient boosting decision trees (GBDT), on a binary classification task with logistic loss. The training uses a second-order approximation of the objective and includes standard tree regularization: an $\\ell_2$ penalty on leaf weights controlled by parameter $\\lambda$ and a per-split minimum loss reduction threshold controlled by parameter $\\gamma$. Consider the first boosting round starting from a constant score $f(x) = 0$ (so the initial class probability is $p_i = 0.5$ for every training instance), uniform sample weights, and assume the maximum depth is $d_{\\max} = 1$ at this round.\n\nYou are given the dataset with features $x_1, x_2 \\in \\{0,1\\}$ and labels $y \\in \\{0,1\\}$:\n- $i = 1$: $(x_1, x_2, y) = (0, 0, 0)$\n- $i = 2$: $(x_1, x_2, y) = (0, 0, 0)$\n- $i = 3$: $(x_1, x_2, y) = (0, 1, 1)$\n- $i = 4$: $(x_1, x_2, y) = (0, 1, 0)$\n- $i = 5$: $(x_1, x_2, y) = (1, 0, 1)$\n- $i = 6$: $(x_1, x_2, y) = (1, 0, 0)$\n- $i = 7$: $(x_1, x_2, y) = (1, 1, 1)$\n- $i = 8$: $(x_1, x_2, y) = (1, 1, 1)$\n\nHyperparameters at this round are $\\lambda = 4$ and $\\gamma = 0.25$. Assume no other constraints are active (for example, the minimum child weight threshold is small enough that it does not block splits).\n\nUsing only the core definitions of gradient boosting with second-order approximation and tree regularization (do not assume any shortcut formulas), reason from first principles about the aggregated first and second derivatives on candidate nodes to decide whether any single split on $x_1$ or $x_2$ can pass the regularization threshold at initialization. Then, answer the following multiple-choice question about how to detect and remedy possible underfitting in this setting.\n\nWhich statements are true?\n\nA. Lowering $\\gamma$ below a threshold determined by the raw split improvement (for example, making $\\gamma < 0.2$ in this dataset) or decreasing $\\lambda$ will increase the net split gain and can enable splits; this directly addresses underfitting caused by overly strong regularization.\n\nB. Increasing the number of boosting rounds $T$ while keeping $\\gamma = 0.25$ will reliably overcome the underfitting, because the ensemble can accumulate improvements even if every individual tree has no splits.\n\nC. A practical way to detect the underfitting here is to monitor the training objective and validation metric: if both plateau near the baseline after several rounds and each tree has only a single leaf (no accepted splits), this indicates underfitting induced by regularization thresholds.\n\nD. Standardizing $x_1$ and $x_2$ to zero mean and unit variance will, by itself, increase the split gain under the second-order criterion and overcome the $\\gamma$ threshold without changing $\\lambda$ or $\\gamma$.\n\nE. Increasing the maximum depth $d_{\\max}$ from $1$ to $3$ while keeping $\\gamma = 0.25$ and $\\lambda = 4$ guarantees that the first split will be accepted, because deeper trees have more leaves and therefore offset the split penalty.", "solution": "The core task is to determine if any split can be made in the first tree. This requires calculating the split gain for all possible splits and comparing it to the threshold $\\gamma$.\n\n**1. First and Second Derivatives (Gradients and Hessians)**\nThe loss function is the logistic loss for binary classification: $L(y_i, \\hat{y}_i) = -[y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]$, where $p_i = \\sigma(\\hat{y}_i) = 1 / (1 + e^{-\\hat{y}_i})$ and $\\hat{y}_i$ is the raw log-odds score.\n\nThe first and second derivatives of the loss with respect to the raw score $\\hat{y}_i$ are:\n-   First derivative (gradient), $g_i = \\frac{\\partial L}{\\partial \\hat{y}_i} = p_i - y_i$.\n-   Second derivative (Hessian), $h_i = \\frac{\\partial^2 L}{\\partial \\hat{y}_i^2} = p_i (1-p_i)$.\n\nAt the start of the first round ($t=1$), the model prediction is $\\hat{y}_i^{(0)} = 0$ for all instances $i$.\nThus, the initial probability is $p_i = \\sigma(0) = 1 / (1+e^0) = 0.5$ for all $i$.\nThe gradients are $g_i = 0.5 - y_i$.\nThe Hessians are $h_i = 0.5 \\times (1 - 0.5) = 0.25$ for all $i$.\n\nLet's compute the gradient $g_i$ for each instance in the dataset:\n-   $i=1, y=0: g_1 = 0.5 - 0 = 0.5$\n-   $i=2, y=0: g_2 = 0.5 - 0 = 0.5$\n-   $i=3, y=1: g_3 = 0.5 - 1 = -0.5$\n-   $i=4, y=0: g_4 = 0.5 - 0 = 0.5$\n-   $i=5, y=1: g_5 = 0.5 - 1 = -0.5$\n-   $i=6, y=0: g_6 = 0.5 - 0 = 0.5$\n-   $i=7, y=1: g_7 = 0.5 - 1 = -0.5$\n-   $i=8, y=1: g_8 = 0.5 - 1 = -0.5$\n\n**2. Split Gain Calculation**\nThe gain of a split is defined as the reduction in the objective function. For a split of a parent node $I$ into a left child $I_L$ and a right child $I_R$, the gain is:\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_I^2}{H_I + \\lambda} \\right] $$\nwhere $G = \\sum_{i \\in \\text{Node}} g_i$ and $H = \\sum_{i \\in \\text{Node}} h_i$. A split is accepted if $\\text{Gain} > \\gamma$.\n\nFirst, we compute the sums for the root node, which contains all instances $I = \\{1, 2, ..., 8\\}$:\n-   $G_I = \\sum_{i=1}^8 g_i = (0.5+0.5-0.5+0.5-0.5+0.5-0.5-0.5) = 4 \\times 0.5 + 4 \\times (-0.5) = 2 - 2 = 0$.\n-   $H_I = \\sum_{i=1}^8 h_i = 8 \\times 0.25 = 2$.\n\nNow, we evaluate the two possible splits (on $x_1$ and $x_2$).\n\n**Split on $x_1$ (at threshold $0.5$):**\n-   Left child $I_L$ (instances with $x_1=0$): $\\{1, 2, 3, 4\\}$.\n    -   $G_L = g_1+g_2+g_3+g_4 = 0.5+0.5-0.5+0.5 = 1$.\n    -   $H_L = 4 \\times 0.25 = 1$.\n-   Right child $I_R$ (instances with $x_1=1$): $\\{5, 6, 7, 8\\}$.\n    -   $G_R = g_5+g_6+g_7+g_8 = -0.5+0.5-0.5-0.5 = -1$.\n    -   $H_R = 4 \\times 0.25 = 1$.\n\nThe gain for this split is:\n$$ \\text{Gain}(x_1) = \\frac{1}{2} \\left[ \\frac{1^2}{1 + 4} + \\frac{(-1)^2}{1 + 4} - \\frac{0^2}{2 + 4} \\right] = \\frac{1}{2} \\left[ \\frac{1}{5} + \\frac{1}{5} - 0 \\right] = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5} = 0.2 $$\nWe must check if $\\text{Gain}(x_1) > \\gamma$. Here, $0.2 > 0.25$ is **false**. This split is rejected.\n\n**Split on $x_2$ (at threshold $0.5$):**\n-   Left child $I_L$ (instances with $x_2=0$): $\\{1, 2, 5, 6\\}$.\n    -   $G_L = g_1+g_2+g_5+g_6 = 0.5+0.5-0.5+0.5 = 1$.\n    -   $H_L = 4 \\times 0.25 = 1$.\n-   Right child $I_R$ (instances with $x_2=1$): $\\{3, 4, 7, 8\\}$.\n    -   $G_R = g_3+g_4+g_7+g_8 = -0.5+0.5-0.5-0.5 = -1$.\n    -   $H_R = 4 \\times 0.25 = 1$.\n\nThe gain for this split is:\n$$ \\text{Gain}(x_2) = \\frac{1}{2} \\left[ \\frac{1^2}{1 + 4} + \\frac{(-1)^2}{1 + 4} - \\frac{0^2}{2 + 4} \\right] = \\frac{1}{2} \\left[ \\frac{1}{5} + \\frac{1}{5} - 0 \\right] = \\frac{1}{5} = 0.2 $$\nWe must check if $\\text{Gain}(x_2) > \\gamma$. Again, $0.2 > 0.25$ is **false**. This split is also rejected.\n\n**Conclusion:** At the first boosting round, no split is accepted because the gain from any possible split ($0.2$) does not exceed the regularization threshold $\\gamma=0.25$. The model is underfitting due to overly strong regularization. The first tree will be a stump with only a root node.\n\n### Option-by-Option Analysis\n\n**A. Lowering $\\gamma$ below a threshold determined by the raw split improvement (for example, making $\\gamma < 0.2$ in this dataset) or decreasing $\\lambda$ will increase the net split gain and can enable splits; this directly addresses underfitting caused by overly strong regularization.**\n-   The condition for accepting a split is $\\text{Gain} > \\gamma$. In our case, the best gain is $0.2$. If we set $\\gamma < 0.2$ (e.g., $\\gamma=0.15$), then the condition $0.2 > 0.15$ would be true, and the split would be accepted. This allows the model to learn.\n-   The gain formula, $\\text{Gain} = \\frac{1}{2} [ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_I^2}{H_I + \\lambda} ]$, shows that gain is a decreasing function of $\\lambda$ (since $\\lambda$ is in the denominator of positive terms). Decreasing $\\lambda$ will increase the gain. For instance, if we decrease $\\lambda$ to $1$, the gain would become $\\frac{1}{2} [\\frac{1^2}{1+1} + \\frac{(-1)^2}{1+1} - 0] = \\frac{1}{2} [ 0.5+0.5 ] = 0.5$. Since $0.5 > 0.25$, the split would be accepted.\n-   Both actions (lowering $\\gamma$ or decreasing $\\lambda$) can enable splits that were previously rejected, allowing the model to fit the data better and thus addressing underfitting caused by regularization. The statement is entirely consistent with the mechanics of XGBoost.\n-   Verdict: **Correct**.\n\n**B. Increasing the number of boosting rounds $T$ while keeping $\\gamma = 0.25$ will reliably overcome the underfitting, because the ensemble can accumulate improvements even if every individual tree has no splits.**\n-   If the first tree has no splits, it is just a root node. The optimal weight for this node is $w_1^* = -G_I / (H_I + \\lambda) = -0 / (2+4) = 0$. The first tree is $f_1(x)=0$.\n-   The model update is $\\hat{y}_i^{(1)} = \\hat{y}_i^{(0)} + \\eta f_1(x_i) = 0 + \\eta \\cdot 0 = 0$. The predictions do not change.\n-   For the second round ($t=2$), the gradients $g_i$ and Hessians $h_i$ are calculated based on $\\hat{y}_i^{(1)}=0$. These will be identical to the ones in the first round.\n-   Consequently, the gain calculation for the second tree will be identical, and again no splits will be made. The second tree will also be $f_2(x)=0$.\n-   This process repeats indefinitely. The model predictions never move from the initial state. The ensemble cannot accumulate improvements if each new tree adds zero contribution.\n-   Verdict: **Incorrect**.\n\n**C. A practical way to detect the underfitting here is to monitor the training objective and validation metric: if both plateau near the baseline after several rounds and each tree has only a single leaf (no accepted splits), this indicates underfitting induced by regularization thresholds.**\n-   As shown in the analysis for B, if no splits are made, the model never learns. The predictions remain at the baseline ($p_i=0.5$).\n-   This means the training loss will be constant at its initial value, e.g., $\\text{logloss} = -\\ln(0.5) \\approx 0.693$. Any validation metric (accuracy, AUC, etc.) will also be constant at its baseline value.\n-   Observing this plateau in performance metrics, combined with inspecting the built trees and finding they are all single-node \"stumps\", is the classic diagnostic signature for this exact failure mode: underfitting caused by regularization parameters ($\\lambda$, $\\gamma$, `min_child_weight`, etc.) being too restrictive.\n-   Verdict: **Correct**.\n\n**D. Standardizing $x_1$ and $x_2$ to zero mean and unit variance will, by itself, increase the split gain under the second-order criterion and overcome the $\\gamma$ threshold without changing $\\lambda$ or $\\gamma$.**\n-   Decision tree algorithms, including XGBoost, are invariant to monotonic transformations of individual features. Standardization is one such transformation.\n-   The features $x_1$ and $x_2$ are binary, taking values in $\\{0, 1\\}$. A split on such a feature, e.g., $x_1 < c$ (for any $0<c<1$), partitions the dataset into a group where $x_1=0$ and a group where $x_1=1$.\n-   Standardizing a binary $\\{0,1\\}$ feature with equal counts transforms it to a $\\{-1, 1\\}$ feature. A split on the standardized feature, e.g., $x'_1 < 0$, would partition the dataset into a group where $x'_1=-1$ and one where $x'_1=1$. This is the *exact same partition of instances* as before.\n-   The split gain calculation depends only on the set of instances in the left and right child nodes ($I_L$ and $I_R$), not on the feature values themselves. Since the partition of instances is identical, the sums of gradients and Hessians ($G_L, H_L, G_R, H_R$) are identical, and thus the calculated gain is identical.\n-   Standardization will not change the split gain.\n-   Verdict: **Incorrect**.\n\n**E. Increasing the maximum depth $d_{\\max}$ from $1$ to $3$ while keeping $\\gamma = 0.25$ and $\\lambda = 4$ guarantees that the first split will be accepted, because deeper trees have more leaves and therefore offset the split penalty.**\n-   The `max_depth` parameter is a stopping condition. The XGBoost algorithm first calculates the gain for all possible splits at a given node. It then checks if the best gain is greater than $\\gamma$. Only if this condition is met is the split performed. After the split is made, the algorithm checks if the new nodes have reached the max_depth.\n-   The decision to make the *first split* at the root node is based solely on the gain calculation for that split. It is entirely independent of the `max_depth` parameter.\n-   If the gain for the root split ($0.2$) is insufficient to overcome $\\gamma$ ($0.25$), no split will be made, and the tree construction stops. The value of `max_depth` is irrelevant because the first level of the tree could not even be built. The potential to grow deeper does not retroactively approve a split with insufficient gain.\n-   Verdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3120315"}]}