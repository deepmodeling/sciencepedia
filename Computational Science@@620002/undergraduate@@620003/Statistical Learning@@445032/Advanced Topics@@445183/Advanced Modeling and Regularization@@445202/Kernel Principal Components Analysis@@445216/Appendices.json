{"hands_on_practices": [{"introduction": "Mastering Kernel PCA begins with understanding how to project new data onto the principal components learned from a training set. This exercise provides a concrete, step-by-step calculation of this out-of-sample extension [@problem_id:3136620]. By working through a small example by hand, you will solidify your grasp of how the kernel trick is used to compute projections in the feature space while correctly accounting for data centering.", "problem": "Consider a training set of $n=3$ points with a symmetric positive semidefinite kernel $k(\\cdot,\\cdot)$. Let $K \\in \\mathbb{R}^{3 \\times 3}$ denote the uncentered Gram matrix with entries $K_{ij} = k(x_i, x_j)$, given by\n$$\nK \\;=\\; \\begin{pmatrix}\n2 & 1 & 1 \\\\\n1 & 2 & 1 \\\\\n1 & 1 & 2\n\\end{pmatrix}.\n$$\nLet $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ be the centering matrix, where $\\mathbf{1} \\in \\mathbb{R}^3$ is the vector of all ones. The centered Gram matrix is $K_c = H K H$. The nonzero eigenpairs of $K_c$ are known to be\n$$\n\\lambda_1 \\,=\\, 1,\\quad u_1 \\,=\\, \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix},\\qquad\n\\lambda_2 \\,=\\, 1,\\quad u_2 \\,=\\, \\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\1\\\\-2\\end{pmatrix},\n$$\nwith $u_1$ and $u_2$ orthonormal and the remaining eigenvalue equal to $0$ with eigenvector proportional to $\\mathbf{1}$.\n\nA new point $x_{\\star}$ is observed, and its kernel evaluations with the training points are provided as\n$$\nk_{\\star} \\;=\\; \\begin{pmatrix}\nk(x_1, x_{\\star}) \\\\ k(x_2, x_{\\star}) \\\\ k(x_3, x_{\\star})\n\\end{pmatrix} \\;=\\; \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n\nUsing Kernel Principal Components Analysis (KPCA), compute the first principal coordinate $z_1(x_{\\star})$ of $x_{\\star}$ under the standard out-of-sample extension that correctly accounts for feature-space centering performed using the training set. Express your final answer as a single exact value. No rounding is required and no units are involved.", "solution": "The task is to compute the first principal coordinate $z_1(x_{\\star})$ of a new data point $x_{\\star}$ using Kernel Principal Components Analysis (KPCA). This coordinate is the projection of the centered feature vector of $x_{\\star}$ onto the first principal component direction in the feature space.\n\nLet $\\Phi: \\mathcal{X} \\to \\mathcal{F}$ be the feature map associated with the kernel $k$, such that $k(x, y) = \\Phi(x)^{\\top}\\Phi(y)$. The training data points are $\\{x_1, x_2, x_3\\}$.\n\nThe mean of the training data in the feature space is given by:\n$$ \\bar{\\Phi} = \\frac{1}{n} \\sum_{i=1}^n \\Phi(x_i) $$\nFor our problem, $n=3$, so $\\bar{\\Phi} = \\frac{1}{3} \\sum_{i=1}^3 \\Phi(x_i)$.\n\nThe centered feature vector for the new point $x_{\\star}$ is $\\tilde{\\Phi}(x_{\\star}) = \\Phi(x_{\\star}) - \\bar{\\Phi}$.\n\nThe principal components in KPCA, denoted by $v_k \\in \\mathcal{F}$, are the eigenvectors of the feature space covariance matrix $C = \\frac{1}{n} \\sum_{i=1}^n (\\Phi(x_i) - \\bar{\\Phi})(\\Phi(x_i) - \\bar{\\Phi})^{\\top}$. The eigenvectors $v_k$ are normalized to have unit length, i.e., $\\|v_k\\|^2 = 1$. The $k$-th principal coordinate of $x_{\\star}$ is the projection of $\\tilde{\\Phi}(x_{\\star})$ onto $v_k$:\n$$ z_k(x_{\\star}) = v_k^{\\top} \\tilde{\\Phi}(x_{\\star}) = v_k^{\\top} (\\Phi(x_{\\star}) - \\bar{\\Phi}) $$\n\nThe principal component vectors $v_k$ can be expressed as a linear combination of the centered training points, but can also be written in terms of the original (uncentered) feature vectors $\\Phi(x_i)$. The expansion coefficients are the components of the eigenvectors of the centered Gram matrix $K_c$. Specifically, for the $k$-th principal component:\n$$ v_k = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n (\\alpha^{(k)})_i \\Phi(x_i) $$\nHere, $\\lambda_k$ is the $k$-th eigenvalue of the centered Gram matrix $K_c = H K H$, and $\\alpha^{(k)}$ is the corresponding eigenvector normalized to $\\|\\alpha^{(k)}\\|=1$. The factor $\\frac{1}{\\sqrt{\\lambda_k}}$ ensures that $\\|v_k\\|^2 = 1$. This form relies on the property that $\\sum_i (\\alpha^{(k)})_i = 0$ for eigenvectors corresponding to non-zero eigenvalues of $K_c$, which simplifies the expression for $v_k$.\n\nNow, we can write the expression for $z_1(x_{\\star})$:\n$$ z_1(x_{\\star}) = v_1^{\\top} (\\Phi(x_{\\star}) - \\bar{\\Phi}) = \\left(\\frac{1}{\\sqrt{\\lambda_1}} \\sum_{i=1}^n (\\alpha^{(1)})_i \\Phi(x_i)\\right)^{\\top} \\left(\\Phi(x_{\\star}) - \\frac{1}{n}\\sum_{j=1}^n \\Phi(x_j)\\right) $$\nExpanding the dot product:\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_1}} \\left( \\sum_{i=1}^n (\\alpha^{(1)})_i \\Phi(x_i)^{\\top}\\Phi(x_{\\star}) - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha^{(1)})_i \\Phi(x_i)^{\\top}\\Phi(x_j) \\right) $$\nUsing the kernel trick, $k(x, y) = \\Phi(x)^{\\top}\\Phi(y)$, and the definition of the Gram matrix, $K_{ij}=k(x_i,x_j)$, we get:\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_1}} \\left( \\sum_{i=1}^n (\\alpha^{(1)})_i k(x_i, x_{\\star}) - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha^{(1)})_i K_{ij} \\right) $$\nThis expression can be written compactly using matrix and vector notation. Let $k_{\\star}$ be the column vector with entries $(k_{\\star})_i = k(x_i, x_{\\star})$, and $\\mathbf{1}$ be the column vector of all ones.\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_1}} \\left( (\\alpha^{(1)})^{\\top} k_{\\star} - \\frac{1}{n} (\\alpha^{(1)})^{\\top} K \\mathbf{1} \\right) $$\nWe are given the following values:\n$n=3$.\n$\\lambda_1 = 1$.\nThe eigenvector of $K_c$ is $\\alpha^{(1)} = u_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$.\nThe kernel evaluations for the new point are $k_{\\star} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\nThe uncentered Gram matrix is $K = \\begin{pmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 1 \\\\ 1 & 1 & 2 \\end{pmatrix}$.\n\nWe proceed to calculate the two terms in the expression for $z_1(x_{\\star})$.\n\nFirst term: $(\\alpha^{(1)})^{\\top} k_{\\star}$\n$$ (\\alpha^{(1)})^{\\top} k_{\\star} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & -1 & 0\\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} (1 \\cdot 3 + (-1) \\cdot 1 + 0 \\cdot 1) = \\frac{1}{\\sqrt{2}}(3 - 1) = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\n\nSecond term: $\\frac{1}{n} (\\alpha^{(1)})^{\\top} K \\mathbf{1}$\nFirst, let's compute the product $K\\mathbf{1}$:\n$$ K\\mathbf{1} = \\begin{pmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 1 \\\\ 1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+1+1 \\\\ 1+2+1 \\\\ 1+1+2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix} = 4\\mathbf{1} $$\nNow, we compute $(\\alpha^{(1)})^{\\top} K \\mathbf{1}$:\n$$ (\\alpha^{(1)})^{\\top} (4\\mathbf{1}) = 4 (\\alpha^{(1)})^{\\top} \\mathbf{1} = 4 \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & -1 & 0\\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{4}{\\sqrt{2}}(1 \\cdot 1 - 1 \\cdot 1 + 0 \\cdot 1) = \\frac{4}{\\sqrt{2}}(0) = 0 $$\nThe fact that this term is zero is expected. The eigenvector $\\alpha^{(1)}$ corresponds to a non-zero eigenvalue $\\lambda_1=1$ of $K_c = HKH$. This implies that $\\alpha^{(1)}$ is in the column space of $H$, which is the space of vectors orthogonal to $\\mathbf{1}$. Since $K\\mathbf{1}$ is a multiple of $\\mathbf{1}$, the dot product $(\\alpha^{(1)})^{\\top} K\\mathbf{1}$ must be zero.\n\nSo, the second term in the expression for $z_1(x_{\\star})$ is $\\frac{1}{3} \\cdot 0 = 0$.\n\nFinally, we substitute these values back into the equation for $z_1(x_{\\star})$:\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{1}} (\\sqrt{2} - 0) = \\sqrt{2} $$\nThe first principal coordinate of $x_{\\star}$ is $\\sqrt{2}$.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3136620"}, {"introduction": "The performance of Kernel PCA is highly sensitive to the choice of the kernel and its parameters. This practice explores a fundamental aspect of this sensitivity: how uniformly scaling a kernel, $\\tilde{k} = a\\,k$, impacts the analysis [@problem_id:3136615]. By deriving the scaling relationship for eigenvalues and projections, you will gain crucial insight into why direct comparison of eigenvalue magnitudes across different kernels can be misleading and learn the principle behind normalizing kernels for a fair comparison.", "problem": "Let $\\{x_{i}\\}_{i=1}^{n}$ be data, mapped by a feature map $\\Phi$ associated with a positive definite kernel $k$. Denote by $K \\in \\mathbb{R}^{n \\times n}$ the Gram matrix with entries $K_{ij} = k(x_{i}, x_{j})$, and by $K_{c}$ the centered Gram matrix obtained by applying the linear centering operator in the sample space to $K$. Kernel Principal Components Analysis (kernel PCA) in feature space is defined by the covariance operator $C = \\frac{1}{n} \\sum_{i=1}^{n} \\Phi_{c}(x_{i}) \\otimes \\Phi_{c}(x_{i})$, where $\\Phi_{c}(x)$ denotes the feature vector centered in feature space. Let the eigenpairs of $K_{c}$ be $(\\mu_{i}, u_{i})$ with $u_{i}^{\\top} u_{i} = 1$, and define the corresponding feature-space eigenvalues by $\\lambda_{i} = \\mu_{i}/n$. Let the $i$-th principal axis be $v_{i} = \\sum_{j=1}^{n} \\alpha_{ij} \\Phi_{c}(x_{j})$, where $\\alpha_{i} = u_{i}/\\sqrt{\\mu_{i}}$ ensures $v_{i}^{\\top} v_{i} = 1$. For a new input $x$, define the centered kernel vector $k_{c}(x) \\in \\mathbb{R}^{n}$ by $[k_{c}(x)]_{j} = k_{c}(x_{j}, x)$, and the $i$-th kernel PCA coordinate $z_{i}(x) = \\alpha_{i}^{\\top} k_{c}(x)$.\n\nNow consider a uniform scaling of the kernel by a positive scalar $a$, i.e., define the scaled kernel $\\tilde{k} = a\\,k$. Let all objects defined above be recomputed from $\\tilde{k}$, yielding $\\tilde{K}_{c}$, $(\\tilde{\\mu}_{i}, \\tilde{u}_{i})$, $\\tilde{\\lambda}_{i}$, $\\tilde{\\alpha}_{i}$, and the scaled coordinate $\\tilde{z}_{i}(x)$. Starting only from the definitions of kernel PCA in feature space and the relation between $K_{c}$ and $C$, derive how the eigenvalues and projections change under the scaling. In particular, compute the factor $\\gamma(a)$, independent of $i$ and $x$, such that $\\tilde{z}_{i}(x) = \\gamma(a)\\, z_{i}(x)$. Briefly explain, based on this derivation, one principled scalar normalization of a kernel matrix that enables fair comparison of spectra across different kernels on the same dataset.\n\nReport your final answer as the single closed-form expression for $\\gamma(a)$.", "solution": "The objective is to determine how the kernel PCA coordinates, denoted by $z_{i}(x)$, transform under a uniform scaling of the kernel function $k$ by a positive scalar $a$. Let the scaled kernel be $\\tilde{k} = a\\,k$. We will trace the effect of this scaling through all intermediate quantities defined in the problem.\n\n1.  **Effect on the Gram Matrix ($K$) and Centered Gram Matrix ($K_c$)**:\n    The entries of the new Gram matrix, $\\tilde{K}$, are given by $\\tilde{K}_{ij} = \\tilde{k}(x_i, x_j) = a\\,k(x_i, x_j) = a\\,K_{ij}$. Therefore, the entire matrix scales by $a$: $\\tilde{K} = a\\,K$.\n    The centered Gram matrix $K_c$ is obtained by a linear centering operator. A linear operator $L$ satisfies $L(cM) = cL(M)$ for any scalar $c$ and matrix $M$. The standard centering transformation $K_c = (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) K (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top})$ is linear in $K$. Thus, the scaled centered Gram matrix $\\tilde{K}_c$ is related to $K_c$ by:\n    $$ \\tilde{K}_c = a\\,K_c $$\n\n2.  **Effect on the Eigenpairs of the Centered Gram Matrix ($(\\mu_i, u_i)$)**:\n    The eigenpairs $(\\mu_i, u_i)$ of $K_c$ are defined by the equation $K_c u_i = \\mu_i u_i$, with the eigenvectors normalized such that $u_i^{\\top} u_i = 1$. Let the new eigenpairs for $\\tilde{K}_c$ be $(\\tilde{\\mu}_i, \\tilde{u}_i)$. Using the result from the previous step:\n    $$ \\tilde{K}_c \\tilde{u}_i = \\tilde{\\mu}_i \\tilde{u}_i $$\n    Substituting $\\tilde{K}_c = a\\,K_c$, we examine the action of $\\tilde{K}_c$ on the original eigenvectors $u_i$:\n    $$ \\tilde{K}_c u_i = (a\\,K_c) u_i = a\\,(K_c u_i) = a\\,(\\mu_i u_i) = (a\\,\\mu_i) u_i $$\n    This equation is an eigenvalue equation for $\\tilde{K}_c$ with eigenvectors $u_i$ and eigenvalues $a\\,\\mu_i$. Since the eigenvectors $u_i$ are already normalized, we can identify $\\tilde{u}_i = u_i$ and the new eigenvalues as:\n    $$ \\tilde{\\mu}_i = a\\,\\mu_i $$\n\n3.  **Effect on the Feature-Space Eigenvalues ($\\lambda_i$)**:\n    The feature-space eigenvalue $\\lambda_i$ is defined as $\\lambda_i = \\mu_i / n$. The scaled counterpart $\\tilde{\\lambda}_i$ is therefore:\n    $$ \\tilde{\\lambda}_i = \\frac{\\tilde{\\mu}_i}{n} = \\frac{a\\,\\mu_i}{n} = a \\left(\\frac{\\mu_i}{n}\\right) = a\\,\\lambda_i $$\n    The eigenvalues of the feature-space covariance operator also scale linearly with $a$.\n\n4.  **Effect on the Principal Axis Coefficients ($\\alpha_i$)**:\n    The coefficient vector $\\alpha_i$ for constructing the principal axes is defined as $\\alpha_i = u_i / \\sqrt{\\mu_i}$. The scaled coefficient vector $\\tilde{\\alpha}_i$ is:\n    $$ \\tilde{\\alpha}_i = \\frac{\\tilde{u}_i}{\\sqrt{\\tilde{\\mu}_i}} = \\frac{u_i}{\\sqrt{a\\,\\mu_i}} = \\frac{1}{\\sqrt{a}} \\frac{u_i}{\\sqrt{\\mu_i}} = \\frac{1}{\\sqrt{a}} \\alpha_i $$\n\n5.  **Effect on the Centered Kernel Vector ($k_c(x)$)**:\n    The centered kernel vector $k_c(x)$ for a new input $x$ has components $[k_c(x)]_j = k_c(x_j, x)$. The centering operation on the kernel function is $k_c(y, z) = k(y, z) - \\frac{1}{n} \\sum_{i=1}^n k(x_i, z) - \\frac{1}{n} \\sum_{j=1}^n k(y, x_j) + \\frac{1}{n^2} \\sum_{i,j=1}^n k(x_i, x_j)$. Since this operation is linear in $k$, scaling $k$ by $a$ results in a scaled centered kernel:\n    $$ \\tilde{k}_c(y, z) = a\\,k_c(y, z) $$\n    Consequently, the scaled centered kernel vector $\\tilde{k}_c(x)$ and the original one are related by:\n    $$ \\tilde{k}_c(x) = a\\,k_c(x) $$\n\n6.  **Effect on the Kernel PCA Coordinates ($z_i(x)$)**:\n    The $i$-th kernel PCA coordinate, or projection, is defined as $z_i(x) = \\alpha_i^{\\top} k_c(x)$. The new coordinate $\\tilde{z}_i(x)$ is computed using the scaled quantities:\n    $$ \\tilde{z}_i(x) = \\tilde{\\alpha}_i^{\\top} \\tilde{k}_c(x) $$\n    Substituting the results from steps 4 and 5:\n    $$ \\tilde{z}_i(x) = \\left(\\frac{1}{\\sqrt{a}} \\alpha_i\\right)^{\\top} (a\\,k_c(x)) = \\frac{1}{\\sqrt{a}} \\alpha_i^{\\top} (a\\,k_c(x)) = \\frac{a}{\\sqrt{a}} (\\alpha_i^{\\top} k_c(x)) = \\sqrt{a}\\,z_i(x) $$\n    This derivation shows that the projection coordinates scale by a factor of $\\sqrt{a}$. The factor $\\gamma(a)$ is therefore $\\sqrt{a}$. This factor is independent of the component index $i$ and the input point $x$, as required.\n    $$ \\gamma(a) = \\sqrt{a} $$\n\n**Principled Kernel Normalization**:\nThe derivation reveals that the feature-space eigenvalues $\\lambda_i$ scale linearly with the kernel scaling factor $a$ (i.e., $\\tilde{\\lambda}_i = a\\,\\lambda_i$). The sum of these eigenvalues, $\\sum_i \\lambda_i = \\frac{1}{n} \\text{Tr}(K_c)$, represents the total variance of the data in the feature space. This total variance also scales linearly with $a$.\n\nWhen comparing the spectral properties (the decay of eigenvalues) of different kernels on the same dataset, this scaling dependency can be a confounding factor. For instance, a kernel with a larger overall scale will have universally larger eigenvalues, obscuring the comparison of the relative importance of the principal components.\n\nA principled scalar normalization is to enforce that the total variance in the feature space is a constant, typically $1$. This allows for a fair comparison of the shape of the eigenvalue spectra. To achieve this, given a kernel $k$ and its corresponding Gram matrix $K$, one computes the centered matrix $K_c$. The total variance is $V = \\frac{1}{n}\\text{Tr}(K_c)$. To normalize this variance to $1$, the kernel matrix $K$ must be scaled by the factor $1/V$. The normalized matrix $K_{\\text{norm}}$ is:\n$$ K_{\\text{norm}} = \\frac{1}{V} K = \\frac{n}{\\text{Tr}(K_c)} K $$\nApplying kernel PCA to $K_{\\text{norm}}$ ensures that the sum of the resulting feature-space eigenvalues is $1$, thereby providing a canonical scale for comparing spectra across different kernel functions.", "answer": "$$\\boxed{\\sqrt{a}}$$", "id": "3136615"}, {"introduction": "While Kernel PCA excels at identifying nonlinear structures in data, its components exist in an abstract feature space. To make these findings interpretable and actionable, we often need to solve the \"pre-image problem\"—mapping results back to the original data space [@problem_id:3136635]. This coding exercise guides you through a powerful application of this concept: using KPCA to learn the manifold of a noisy parabolic curve and then implementing a pre-image approximation to denoise the data, revealing the underlying clean signal.", "problem": "You must implement a complete program that constructs a toy dataset where the first component of Kernel Principal Components Analysis (KPCA) corresponds to a known nonlinear function, and then uses a pre-image approximation to recover that function. The known nonlinear function is the square function, defined by $f(x) = x^{2}$. The dataset consists of two-dimensional points $(x,y)$ with $y = f(x) + \\varepsilon$, where $\\varepsilon$ is additive Gaussian noise. You will use the Radial Basis Function (RBF) kernel and retain a single component in KPCA, which induces a one-dimensional nonlinear principal curve expected to follow the graph of $y \\approx f(x)$. You will then compute the pre-image of each training point after projection onto this first component to obtain a denoised estimate of $y \\approx f(x)$.\n\nFundamental starting points and definitions:\n- Kernel Principal Components Analysis (KPCA) is defined on a dataset $\\{x_{i}\\}_{i=1}^{n}$ through a positive semidefinite kernel $k(x,z) = \\langle \\phi(x), \\phi(z) \\rangle$ induced by a feature map $\\phi(\\cdot)$ and the centered kernel matrix $K_{c}$. The centered kernel is given by $K_{c} = H K H$, where $H = I_{n} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ and $K_{ij} = k(x_{i}, x_{j})$.\n- The Radial Basis Function (RBF) kernel is defined as $k(x,z) = \\exp\\!\\left(-\\frac{\\lVert x - z \\rVert^{2}}{2\\sigma^{2}}\\right)$, where $\\sigma > 0$ is the bandwidth parameter.\n- KPCA components are obtained by the eigenvalue problem $K_{c} v_{\\ell} = \\lambda_{\\ell} v_{\\ell}$, sorted such that $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{n} \\ge 0$, with $v_{\\ell}$ normalized to unit Euclidean norm. For a training point $x_{k}$, its coordinate on component $\\ell$ is $y_{\\ell}(x_{k}) = \\sqrt{\\lambda_{\\ell}}\\, v_{\\ell,k}$.\n- The rank-$m$ reconstruction in feature space of a training point $x_{k}$ can be written as $\\hat{\\Phi}(x_{k}) = \\sum_{i=1}^{n} a_{i}^{(k)} \\Phi_{c}(x_{i})$, where $a_{i}^{(k)} = \\sum_{\\ell=1}^{m} v_{\\ell,i} v_{\\ell,k}$. In this problem, you must use $m = 1$, hence $a_{i}^{(k)} = v_{1,i} v_{1,k}$.\n- The pre-image problem seeks $\\hat{x}$ such that $\\phi(\\hat{x})$ is as close as possible to $\\hat{\\Phi}(x_{k})$ in feature space. For the RBF kernel, a well-tested fixed-point approximation for the pre-image is\n$$\n\\hat{x}^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_{j}^{(t)} x_{j}}{\\sum_{j=1}^{n} \\gamma_{j}^{(t)}}, \n\\quad \\text{where} \\quad \n\\gamma_{j}^{(t)} = a_{j}^{(k)} \\exp\\!\\left(-\\frac{\\lVert \\hat{x}^{(t)} - x_{j} \\rVert^{2}}{2\\sigma^{2}}\\right).\n$$\nYou should initialize $\\hat{x}^{(0)} = x_{k}$ and iterate until convergence.\n\nProgram requirements:\n1. Data generation: For each test case, generate $n$ independent samples $x_{i} \\sim \\mathrm{Uniform}([-1,1])$, set $y_{i}^{\\mathrm{true}} = f(x_{i}) = x_{i}^{2}$, and observe $y_{i} = y_{i}^{\\mathrm{true}} + \\varepsilon_{i}$ with $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{noise}}^{2})$. Construct points $X_{i} = (x_{i}, y_{i}) \\in \\mathbb{R}^{2}$.\n2. Kernel computation: Use the RBF kernel $k(x,z) = \\exp\\!\\left(-\\frac{\\lVert x - z \\rVert^{2}}{2\\sigma^{2}}\\right)$ on the two-dimensional inputs $X_{i} \\in \\mathbb{R}^{2}$. Compute the $n \\times n$ kernel matrix $K$ and center it to obtain $K_{c}$.\n3. KPCA: Solve $K_{c} v_{\\ell} = \\lambda_{\\ell} v_{\\ell}$ and select the first component $(\\lambda_{1}, v_{1})$ with the largest eigenvalue $\\lambda_{1}$. Use $m=1$.\n4. Reconstruction coefficients: For each training index $k \\in \\{1,\\dots,n\\}$, form $a^{(k)} \\in \\mathbb{R}^{n}$ with entries $a_{i}^{(k)} = v_{1,i} v_{1,k}$.\n5. Pre-image approximation: For each $k$, compute $\\hat{x}_{k} \\in \\mathbb{R}^{2}$ by the fixed-point iteration\n$$\n\\hat{x}_{k}^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_{j}^{(t)} X_{j}}{\\sum_{j=1}^{n} \\gamma_{j}^{(t)}}, \n\\quad \\gamma_{j}^{(t)} = a_{j}^{(k)} \\exp\\!\\left(-\\frac{\\lVert \\hat{x}_{k}^{(t)} - X_{j} \\rVert^{2}}{2\\sigma^{2}}\\right),\n$$\ninitialized at $\\hat{x}_{k}^{(0)} = X_{k}$, iterating until $\\lVert \\hat{x}_{k}^{(t+1)} - \\hat{x}_{k}^{(t)} \\rVert_{2} < \\varepsilon$ or until a maximum of $T$ iterations is reached. Use tolerance $\\varepsilon = 10^{-8}$ and $T = 200$.\n6. Function recovery and evaluation: Use the second coordinate of $\\hat{x}_{k}$ as $\\hat{y}_{k}$, your estimate of $f(x_{k})$. Compute the root-mean-squared error (RMSE) before denoising and after denoising:\n$$\n\\mathrm{RMSE}_{\\mathrm{before}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} \\left(y_{k} - y_{k}^{\\mathrm{true}}\\right)^{2}}, \n\\quad \n\\mathrm{RMSE}_{\\mathrm{after}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} \\left(\\hat{y}_{k} - y_{k}^{\\mathrm{true}}\\right)^{2}}.\n$$\n\nTest suite:\nRun your program on the following three test cases, each specified by $(n, \\sigma, \\sigma_{\\mathrm{noise}}, s)$ where $s$ is the random seed used both for $x_{i}$ sampling and the Gaussian noise.\n- Case $1$: $(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (80, 0.15, 0.05, 0)$.\n- Case $2$ (boundary, no noise): $(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (60, 0.12, 0.0, 1)$.\n- Case $3$ (smaller bandwidth, higher noise): $(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (80, 0.05, 0.10, 2)$.\n\nFinal output format:\n- For each case, output the pair $[\\mathrm{RMSE}_{\\mathrm{before}}, \\mathrm{RMSE}_{\\mathrm{after}}]$ rounded to $6$ decimals.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[[a,b],[c,d],[e,f]]$, with no spaces.\n\nAngle units, physical units, and percentages are not applicable to this problem; all quantities are dimensionless real numbers. Ensure scientific realism by following the precise definitions above without ad hoc heuristics or shortcuts not derivable from the stated foundations.", "solution": "The core objective is to demonstrate that KPCA can identify a nonlinear data manifold—in this case, a parabola—and that by projecting the noisy data onto this learned manifold and then reconstructing the points in the original space (the pre-image problem), we can effectively reduce noise.\n\n### 1. Data Generation\n\nThe first step is to generate a synthetic dataset according to the problem specification. For each test case, we are given a number of data points $n$, a noise standard deviation $\\sigma_{\\mathrm{noise}}$, and a random seed $s$ for reproducibility. We generate $n$ independent samples $x_i$ from a uniform distribution over the interval $[-1, 1]$. The true underlying function is $f(x) = x^2$. The \"true\" data points are $(x_i, y_i^{\\mathrm{true}})$, where $y_i^{\\mathrm{true}} = x_i^2$. We then create an observed dataset by adding Gaussian noise: the observed points are $X_i = (x_i, y_i) \\in \\mathbb{R}^2$, where $y_i = y_i^{\\mathrm{true}} + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{noise}}^2)$. This dataset $\\{X_i\\}_{i=1}^n$ forms a noisy parabola.\n\n### 2. Kernel Matrix Construction and Centering\n\nKPCA operates in a high-dimensional feature space implicitly defined by a kernel function. The specified kernel is the Radial Basis Function (RBF) kernel:\n$$k(x, z) = \\exp\\left(-\\frac{\\lVert x - z \\rVert^2}{2\\sigma^2}\\right)$$\nwhere $\\sigma$ is the bandwidth parameter. We compute the $n \\times n$ Gram matrix $K$ where each element $K_{ij} = k(X_i, X_j)$ for our $n$ data points in $\\mathbb{R}^2$.\n\nPCA requires data to be centered around the origin. In the feature space, this is achieved by centering the kernel matrix. The centered kernel matrix, $K_c$, is given by:\n$$K_c = H K H$$\nwhere $H$ is the centering matrix $H = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$. Here, $I_n$ is the $n \\times n$ identity matrix and $\\mathbf{1}$ is a column vector of $n$ ones. This operation ensures that the data $\\phi(X_i)$ mapped into the feature space have zero mean.\n\n### 3. Kernel Principal Component Extraction\n\nThe principal components in the feature space are found by solving the eigenvalue problem for the centered kernel matrix:\n$$K_c v_\\ell = \\lambda_\\ell v_\\ell$$\nThe eigenvalues $\\lambda_\\ell$ are proportional to the variance captured by each principal component, and the eigenvectors $v_\\ell$ (normalized to $\\|\\cdot\\|_2=1$) contain the coefficients that define the principal axes in terms of the mapped data points. We sort the eigenvalues in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$. Since our data lies primarily along a one-dimensional curve (a parabola), we expect the first principal component, corresponding to the largest eigenvalue $\\lambda_1$, to capture this dominant structure. The problem thus directs us to use only the first component, which means we retain only the eigenvector $v_1$ and eigenvalue $\\lambda_1$.\n\n### 4. Pre-image Reconstruction via Fixed-Point Iteration\n\nAfter projecting the data onto the first principal component in the feature space, we aim to find the corresponding point in the original input space. This is the \"pre-image problem\". Given a projection in feature space, we seek a point $\\hat{x}$ in the input space such that $\\phi(\\hat{x})$ is close to this projection. The problem specifies a fixed-point iteration method to approximate this pre-image.\n\nThe projection of a point $\\phi_c(X_k)$ onto the first principal component is given by its reconstruction, which involves coefficients $a_i^{(k)}$. For a rank-$1$ reconstruction ($m=1$), these coefficients are:\n$$a_i^{(k)} = v_{1,i} v_{1,k}$$\nwhere $v_{1,i}$ is the $i$-th element of the first eigenvector $v_1$. These coefficients define how the centered feature vectors of all training points, $\\phi_c(X_i)$, are weighted to form the reconstruction of $\\phi_c(X_k)$.\n\nFor each training point $X_k$, we seek its pre-image $\\hat{x}_k$. We initialize the search with the original data point, $\\hat{x}_k^{(0)} = X_k$, and iterate using the following update rule for $t=0, 1, 2, \\dots$:\n$$\\hat{x}_k^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_j^{(t)} X_j}{\\sum_{j=1}^{n} \\gamma_j^{(t)}}$$\nThe weights $\\gamma_j^{(t)}$ are defined as:\n$$\\gamma_j^{(t)} = a_j^{(k)} \\exp\\left(-\\frac{\\lVert \\hat{x}_k^{(t)} - X_j \\rVert^2}{2\\sigma^2}\\right)$$\nThis iteration is a weighted-average update. The weights $\\gamma_j^{(t)}$ combine two effects: the structural information from the PCA projection (via $a_j^{(k)}$) and a locality-based weighting (via the RBF kernel term), which gives more influence to training points $X_j$ that are close to the current pre-image estimate $\\hat{x}_k^{(t)}$. The iteration continues until the change in the estimate is negligible, i.e., $\\lVert \\hat{x}_k^{(t+1)} - \\hat{x}_k^{(t)} \\rVert_2 < \\varepsilon = 10^{-8}$, or a maximum number of iterations $T=200$ is reached.\n\n### 5. Evaluation of Denoising Performance\n\nThe result of the pre-image calculation for each $X_k$ is a denoised point $\\hat{x}_k = (\\hat{x}_{k,1}, \\hat{x}_{k,2})$. The second coordinate, which we can call $\\hat{y}_k = \\hat{x}_{k,2}$, is our denoised estimate of the original noisy $y_k$.\n\nTo quantify the performance, we compute the Root-Mean-Squared Error (RMSE) of the $y$-coordinates before and after this denoising procedure. The \"before\" RMSE measures the error of the observed noisy data with respect to the true function:\n$$\\mathrm{RMSE}_{\\mathrm{before}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} (y_k - y_k^{\\mathrm{true}})^2}$$\nThe \"after\" RMSE measures the error of our reconstructed estimates:\n$$\\mathrm{RMSE}_{\\mathrm{after}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} (\\hat{y}_k - y_k^{\\mathrm{true}})^2}$$\nA successful denoising is indicated by $\\mathrm{RMSE}_{\\mathrm{after}} < \\mathrm{RMSE}_{\\mathrm{before}}$. We will execute this entire procedure for the three test cases specified.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import pdist, squareform\n\ndef run_kpca_denoising(n, sigma, sigma_noise, seed):\n    \"\"\"\n    Performs KPCA-based denoising for a single test case.\n\n    Args:\n        n (int): Number of data points.\n        sigma (float): RBF kernel bandwidth.\n        sigma_noise (float): Standard deviation of Gaussian noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing [RMSE_before, RMSE_after].\n    \"\"\"\n    # 1. Data generation\n    rng = np.random.default_rng(seed)\n    x_coords = rng.uniform(-1, 1, n)\n    y_true = x_coords**2\n    noise = rng.normal(0, sigma_noise, n)\n    y_coords = y_true + noise\n    X = np.vstack((x_coords, y_coords)).T  # n x 2 matrix\n\n    # 6. Evaluation (before)\n    # The noise added is (y_coords - y_true), so RMSE_before is just the RMS of the noise\n    rmse_before = np.sqrt(np.mean((y_coords - y_true)**2))\n\n    # 2. Kernel computation\n    sq_dists = squareform(pdist(X, 'sqeuclidean'))\n    K = np.exp(-sq_dists / (2 * sigma**2))\n\n    # Center the kernel matrix\n    H = np.eye(n) - np.ones((n, n)) / n\n    Kc = H @ K @ H\n\n    # 3. KPCA (Eigen-decomposition)\n    # eigh returns eigenvalues in ascending order\n    eigenvalues, eigenvectors = eigh(Kc)\n    \n    # Select the first principal component (corresponding to the largest eigenvalue)\n    # lambda_1 = eigenvalues[-1] # Not directly needed for pre-image\n    v1 = eigenvectors[:, -1]\n\n    # 5. Pre-image approximation\n    X_hat = np.zeros_like(X)\n    T_max = 200\n    epsilon = 1e-8\n    \n    for k in range(n):\n        # 4. Reconstruction coefficients for point k\n        a_k = v1 * v1[k]\n\n        # Initialize pre-image iteration\n        x_hat_t = X[k, :] \n        \n        for _ in range(T_max):\n            # Compute distances from current estimate to all training points\n            dists_sq_t = np.sum((x_hat_t - X)**2, axis=1)\n            \n            # Compute gamma weights\n            gamma_t = a_k * np.exp(-dists_sq_t / (2 * sigma**2))\n            \n            # Compute next estimate\n            numerator = np.sum(gamma_t[:, np.newaxis] * X, axis=0)\n            denominator = np.sum(gamma_t)\n            \n            # Add small epsilon for numerical stability\n            if np.abs(denominator) < 1e-12:\n                denominator += 1e-12\n                \n            x_hat_t_plus_1 = numerator / denominator\n            \n            # Check for convergence\n            if np.linalg.norm(x_hat_t_plus_1 - x_hat_t) < epsilon:\n                x_hat_t = x_hat_t_plus_1\n                break\n                \n            x_hat_t = x_hat_t_plus_1\n        \n        X_hat[k, :] = x_hat_t\n\n    # 6. Evaluation (after)\n    y_hat = X_hat[:, 1]\n    rmse_after = np.sqrt(np.mean((y_hat - y_true)**2))\n\n    return [round(rmse_before, 6), round(rmse_after, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, sigma_noise, seed)\n        (80, 0.15, 0.05, 0),\n        (60, 0.12, 0.0, 1),\n        (80, 0.05, 0.10, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, sigma_noise, seed = case\n        result = run_kpca_denoising(n, sigma, sigma_noise, seed)\n        results.append(result)\n\n    # Format output string manually to avoid spaces\n    # str(list) adds spaces, e.g., '[0.1, 0.2]'. We need '[0.1,0.2]'.\n    results_str = [str(r).replace(\" \", \"\") for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3136635"}]}