{"hands_on_practices": [{"introduction": "While simple polynomial bases like $\\{1, x, x^2, \\dots\\}$ are easy to write down, they can be numerically unstable in practice. A more robust approach often involves using a basis of orthogonal functions. This foundational exercise [@problem_id:2161554] walks you through the Gram-Schmidt process, the standard mathematical tool for constructing an orthogonal basis from a non-orthogonal one, giving you first-hand experience with a procedure central to approximation theory and numerical analysis.", "problem": "In numerical analysis and approximation theory, it is often useful to construct a set of orthogonal basis functions from a simpler, non-orthogonal set. Consider the space of real-valued functions that are continuous on the interval $[0, 1]$. The inner product of two functions $f(x)$ and $g(x)$ in this space is defined as:\n$$ \\langle f, g \\rangle = \\int_{0}^{1} f(x)g(x) \\, dx $$\nTwo functions are considered orthogonal if their inner product is zero.\n\nStarting with the initial, non-orthogonal basis set of functions $\\{v_1(x), v_2(x)\\}$, where $v_1(x) = 1$ and $v_2(x) = x$, construct a new set of orthogonal basis functions $\\{u_1(x), u_2(x)\\}$ by applying the following procedure:\n1.  Set the first orthogonal function to be the same as the first initial function: $u_1(x) = v_1(x)$.\n2.  Construct the second orthogonal function by subtracting the projection of $v_2(x)$ onto $u_1(x)$:\n    $$ u_2(x) = v_2(x) - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1(x) $$\n\nFind the resulting orthogonal functions $u_1(x)$ and $u_2(x)$. Present your answer as a pair of functions.", "solution": "We work in the space of continuous real-valued functions on $[0,1]$ with inner product $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)\\,dx$. The initial set is $v_{1}(x)=1$ and $v_{2}(x)=x$.\n\nStep 1: Set $u_{1}(x)=v_{1}(x)=1$ by the given procedure.\n\nStep 2: Compute the projection coefficient of $v_{2}$ onto $u_{1}$:\n$$\n\\langle v_{2},u_{1}\\rangle=\\int_{0}^{1}x\\cdot 1\\,dx=\\int_{0}^{1}x\\,dx=\\frac{1}{2},\n\\qquad\n\\langle u_{1},u_{1}\\rangle=\\int_{0}^{1}1\\cdot 1\\,dx=\\int_{0}^{1}1\\,dx=1.\n$$\nThus,\n$$\nu_{2}(x)=v_{2}(x)-\\frac{\\langle v_{2},u_{1}\\rangle}{\\langle u_{1},u_{1}\\rangle}u_{1}(x)\n=x-\\frac{\\frac{1}{2}}{1}\\cdot 1=x-\\frac{1}{2}.\n$$\n\nVerification of orthogonality:\n$$\n\\langle u_{1},u_{2}\\rangle=\\int_{0}^{1}1\\cdot\\left(x-\\frac{1}{2}\\right)\\,dx=\\int_{0}^{1}x\\,dx-\\frac{1}{2}\\int_{0}^{1}1\\,dx=\\frac{1}{2}-\\frac{1}{2}=0,\n$$\nso $u_{1}$ and $u_{2}$ are orthogonal. Therefore, the orthogonal pair is $u_{1}(x)=1$ and $u_{2}(x)=x-\\frac{1}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}1 & x-\\frac{1}{2}\\end{pmatrix}}$$", "id": "2161554"}, {"introduction": "Building on the concept of orthogonality, this practice explores the crucial question of *why* we prefer orthogonal bases in statistical modeling. Through a computational experiment [@problem_id:3102236], you will directly compare the numerical stability of a naive monomial basis with that of a standardized orthogonal polynomial basis. This exercise demonstrates how an informed choice of basis functions can dramatically improve the robustness and interpretability of a regression model when inputs are scaled or shifted.", "problem": "Consider supervised regression with linear basis expansions in the context of statistical learning. Let a dataset consist of inputs $x_i$ and targets $t_i$ for $i=1,\\dots,n$, and consider a linear model $m(x) = \\sum_{j=0}^{d} w_j \\,\\phi_j(x)$ where the basis functions $\\phi_j$ are fixed. The least-squares estimator minimizes the sum of squared residuals and can be computed from a design matrix $X$ with entries $X_{ij} = \\phi_j(x_i)$ using well-tested numerical linear algebra. The two-norm condition number of a matrix, denoted by $\\kappa_2(X)$, quantifies numerical stability of solving linear systems with $X$.\n\nThis problem investigates the effect of input rescaling under the affine transformation $x \\mapsto y = a x + b$ on two families of basis functions: the monomial (Vandermonde) basis and an orthogonal polynomial basis. The monomial basis is defined by $\\phi_j(x) = x^j$ for $j=0,\\dots,d$, and its design matrix $V$ is the Vandermonde matrix. The orthogonal polynomial basis considered here is the Legendre family $\\{P_j(z)\\}_{j=0}^{d}$ evaluated on standardized inputs $z$. Standardization is performed by mapping the observed input range to the interval $[-1,1]$ via\n$$\nz = \\frac{2(x - m_x)}{r_x}, \\quad m_x = \\frac{x_{\\min} + x_{\\max}}{2}, \\quad r_x = x_{\\max} - x_{\\min},\n$$\nand analogously for the rescaled inputs $y$,\n$$\nz' = \\frac{2(y - m_y)}{r_y}, \\quad m_y = \\frac{y_{\\min} + y_{\\max}}{2}, \\quad r_y = y_{\\max} - y_{\\min}.\n$$\nThe Legendre polynomials satisfy the parity property $P_j(-z) = (-1)^j P_j(z)$.\n\nStarting from the core definitions above, you will derive algorithmic tests of the following claims:\n- Invariance claim for the orthogonal polynomial basis: when the design matrix is built from standardized inputs, the coefficients computed by least squares are invariant under $x \\mapsto y = a x + b$ up to a predictable parity adjustment when $a < 0$.\n- Sensitivity claim for the Vandermonde basis: monomial-basis coefficients and the condition number of the design matrix are sensitive to the scaling factor $a$ and shift $b$, often resulting in large changes.\n\nYou must implement a program that, for each test case specified below, performs the following steps in pure mathematical terms:\n1. Generate evenly spaced inputs $x_i$ over a specified interval $[x_{\\min}, x_{\\max}]$.\n2. Compute targets $t_i = f(x_i)$ where $f(x) = \\sin(x)$, with the angle unit in radians.\n3. Fit a degree-$d$ model using the orthogonal polynomial basis $\\{P_j(z)\\}_{j=0}^{d}$ on standardized inputs $z$ built from $x$, obtaining coefficients $w^{\\mathrm{leg}}$. Then apply the rescaling $y_i = a x_i + b$, rebuild standardized inputs $z'$ from $y$, and refit to obtain coefficients $w'^{\\mathrm{leg}}$.\n4. Test the invariance claim by checking whether $w'^{\\mathrm{leg}}$ equals $w^{\\mathrm{leg}}$ up to the parity adjustment $(-\\operatorname{sign}(a))^j$ on the $j$-th coefficient, that is, whether $\\max_j \\left| w'^{\\mathrm{leg}}_j - \\left(\\operatorname{sign}(a)\\right)^j w^{\\mathrm{leg}}_j \\right|$ is below a tolerance $\\epsilon$.\n5. Fit a degree-$d$ model using the monomial basis on the original inputs $x$, obtaining coefficients $w^{\\mathrm{van}}$, and on the rescaled inputs $y$, obtaining $w'^{\\mathrm{van}}$. Compute the relative coefficient change\n$$\n\\Delta = \\frac{\\left\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\right\\|_2}{\\left\\| w^{\\mathrm{van}} \\right\\|_2 + 10^{-12}},\n$$\nand the condition number ratio\n$$\n\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)},\n$$\nwhere $V_x$ and $V_y$ are the Vandermonde design matrices on $x$ and $y$, respectively.\n6. Return for each test case a list $[B, \\Delta, \\rho]$ where $B$ is the boolean result of the invariance test in step $4$.\n\nUse least squares with the standard two-norm objective and compute condition numbers using the two-norm. Use the tolerance $\\epsilon = 10^{-9}$ for the invariance test. Angles for the sine function must be in radians. No physical units other than radians appear in this problem.\n\nTest suite:\n- Case $1$: $n = 50$, degree $d = 10$, $x_{\\min} = -3$, $x_{\\max} = 3$, $a = 2$, $b = 1$.\n- Case $2$: $n = 6$, degree $d = 5$, $x_{\\min} = -1$, $x_{\\max} = 1$, $a = -1$, $b = 0$.\n- Case $3$: $n = 50$, degree $d = 10$, $x_{\\min} = -1$, $x_{\\max} = 2$, $a = 100$, $b = -5$.\n- Case $4$: $n = 50$, degree $d = 10$, $x_{\\min} = -3$, $x_{\\max} = 3$, $a = 0.01$, $b = 100$.\n- Case $5$: $n = 50$, degree $d = 10$, $x_{\\min} = -2$, $x_{\\max} = 4$, $a = 1$, $b = 10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one element per test case. Each element must itself be a list of the form $[B,\\Delta,\\rho]$. For example, a valid output format is [[True, 0.1, 2.0], [False, 3.5, 100.0]].", "solution": "The problem is valid as it is scientifically grounded in the principles of numerical linear algebra and statistical regression, well-posed with all necessary information provided, and objective in its formulation. It presents a standard, verifiable comparison between two common choices of basis functions.\n\nThe core of the problem lies in analyzing the numerical stability and coefficient invariance of linear models under an affine transformation of the input variable, $x \\mapsto y = a x + b$. We investigate two types of basis expansions: one using orthogonal polynomials (Legendre polynomials) on a standardized domain, and another using a naive monomial basis.\n\n**1. Orthogonal Polynomial Basis: Legendre Polynomials**\n\nThe key to the stability of the Legendre polynomial basis is the standardization of the input variable. For any variable $v$ defined over an interval $[v_{\\min}, v_{\\max}]$, the standardization map is given by:\n$$\nz(v) = \\frac{2(v - m_v)}{r_v}, \\quad \\text{where } m_v = \\frac{v_{\\min} + v_{\\max}}{2} \\text{ and } r_v = v_{\\max} - v_{\\min}.\n$$\nThis transformation maps the interval $[v_{\\min}, v_{\\max}]$ to $[-1, 1]$, which is the canonical domain of orthogonality for Legendre polynomials. The model is then constructed using basis functions $\\phi_j(x) = P_j(z(x))$.\n\nLet's analyze the effect of the affine transformation $y = a x + b$ on the standardized variable. The original inputs $x_i$ lie in $[x_{\\min}, x_{\\max}]$. The rescaled inputs $y_i$ will lie in a new interval $[y_{\\min}, y_{\\max}]$.\n\nCase 1: $a > 0$. The transformation is order-preserving.\n$y_{\\min} = a x_{\\min} + b$ and $y_{\\max} = a x_{\\max} + b$.\nThe new midpoint is $m_y = \\frac{(a x_{\\min} + b) + (a x_{\\max} + b)}{2} = a \\frac{x_{\\min} + x_{\\max}}{2} + b = a m_x + b$.\nThe new range is $r_y = (a x_{\\max} + b) - (a x_{\\min} + b) = a (x_{\\max} - x_{\\min}) = a r_x$.\nThe new standardized variable $z'$ is:\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{a r_x} = \\frac{2a(x - m_x)}{a r_x} = z(x).\n$$\nSince $z'(y_i) = z(x_i)$, the basis functions evaluated at the new points are identical to the old ones: $\\phi_j(y_i) = P_j(z'(y_i)) = P_j(z(x_i)) = \\phi_j(x_i)$. The design matrices for both fits are therefore identical. Since the target vector $\\mathbf{t}$ remains unchanged, the least-squares solution for the coefficients must also be identical: $w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$.\n\nCase 2: $a < 0$. The transformation is order-reversing.\n$y_{\\min} = a x_{\\max} + b$ and $y_{\\max} = a x_{\\min} + b$.\nThe midpoint $m_y = a m_x + b$ is derived similarly.\nThe new range is $r_y = (a x_{\\min} + b) - (a x_{\\max} + b) = a (x_{\\min} - x_{\\max}) = -a (x_{\\max} - x_{\\min}) = |a| r_x$.\nThe new standardized variable $z'$ is:\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{|a| r_x} = \\frac{a}{|a|} \\frac{2(x - m_x)}{r_x} = -z(x), \\text{ since } a < 0 \\implies a/|a| = -1.\n$$\nThe basis functions for the new fit are $\\phi_j(y_i) = P_j(z'(y_i)) = P_j(-z(x_i))$. Using the parity property of Legendre polynomials, $P_j(-z) = (-1)^j P_j(z)$, we get $\\phi_j(y_i) = (-1)^j P_j(z(x_i))$.\nLet $X^{\\mathrm{leg}}$ be the design matrix for the fit on $x$, with entries $(X^{\\mathrm{leg}})_{ij} = P_j(z(x_i))$. Let $X'^{\\mathrm{leg}}$ be the design matrix for the fit on $y$, with entries $(X'^{\\mathrm{leg}})_{ij} = P_j(z'(y_i))$. The relationship is $(X'^{\\mathrm{leg}})_{ij} = (-1)^j (X^{\\mathrm{leg}})_{ij}$. This can be written in matrix form as $X'^{\\mathrm{leg}} = X^{\\mathrm{leg}} S$, where $S$ is a diagonal matrix with $S_{jj} = (-1)^j$.\n\nThe two least-squares problems are to find $w^{\\mathrm{leg}}$ and $w'^{\\mathrm{leg}}$ that minimize $\\| X^{\\mathrm{leg}} w^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$ and $\\| X'^{\\mathrm{leg}} w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$. Substituting the relation between the matrices into the second problem gives $\\| (X^{\\mathrm{leg}} S) w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$. Let $w^* = S w'^{\\mathrm{leg}}$. The problem becomes finding $w^*$ that minimizes $\\| X^{\\mathrm{leg}} w^* - \\mathbf{t} \\|_2^2$. By uniqueness of the least-squares solution, $w^* = w^{\\mathrm{leg}}$. Thus, $S w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$. Since $S$ is its own inverse ($S^2=I$), we can solve for $w'^{\\mathrm{leg}}$: $w'^{\\mathrm{leg}} = S^{-1} w^{\\mathrm{leg}} = S w^{\\mathrm{leg}}$, which means $w'^{\\mathrm{leg}}_j = (-1)^j w^{\\mathrm{leg}}_j$.\n\nCombining both cases using $\\operatorname{sign}(a)$, we find that $w'^{\\mathrm{leg}}_j = (\\operatorname{sign}(a))^j w^{\\mathrm{leg}}_j$. The invariance test checks this relationship to a numerical tolerance $\\epsilon=10^{-9}$. The boolean $B$ is the result of this test.\n\n**2. Monomial Basis: Vandermonde Matrix**\n\nThe monomial basis is defined by $\\phi_j(x) = x^j$. The design matrix $V_x$ with entries $(V_x)_{ij} = x_i^j$ is a Vandermonde matrix. Unlike the orthogonal basis, this basis has no self-correcting standardization mechanism.\n\nWhen the input is transformed to $y = ax+b$, the new basis functions are $\\phi_j(y) = (ax+b)^j$. Using the binomial expansion:\n$$\n\\phi_j(y) = (ax+b)^j = \\sum_{k=0}^{j} \\binom{j}{k} (ax)^k b^{j-k} = \\sum_{k=0}^{j} \\left[ \\binom{j}{k} a^k b^{j-k} \\right] x^k = \\sum_{k=0}^{j} C_{jk} \\phi_k(x)\n$$\nEach new basis function is a linear combination of the old basis functions up to the same degree. This creates a complicated relationship between the coefficient vectors $w^{\\mathrm{van}}$ and $w'^{\\mathrm{van}}$, precluding any simple invariance. The relative change $\\Delta = \\frac{\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\|_2}{\\| w^{\\mathrm{van}} \\|_2 + 10^{-12}}$ is expected to be large.\n\nVandermonde matrices are notoriously ill-conditioned, especially when the interval of points is far from the origin or has a very large/small scale. The columns of the matrix, being powers of $x_i$, can become nearly collinear. For example, if all $|x_i| \\gg 1$, the vectors $[x_i^d]$ and $[x_i^{d-1}]$ will point in very similar directions, leading to a high condition number $\\kappa_2(V_x)$. The transformation $y=ax+b$ can drastically alter the input domain's scale and location, often exacerbating this problem. The ratio of condition numbers, $\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)}$, quantifies this instability. Large values of $|a|$ or $|b|$ are expected to yield large values of $\\rho$.\n\n**3. Algorithmic Implementation**\n\nThe described analysis is implemented as follows:\n- For each test case, we generate $n$ inputs $x_i$ and targets $t_i = \\sin(x_i)$.\n- For the Legendre basis, we construct the design matrices $X^{\\mathrm{leg}}$ and $X'^{\\mathrm{leg}}$ by first standardizing the $x_i$ and rescaled $y_i$ inputs, respectively, and then evaluating the Legendre polynomials $P_j$ for $j=0, \\dots, d$. The coefficients $w^{\\mathrm{leg}}$ and $w'^{\\mathrm{leg}}$ are found using `numpy.linalg.lstsq`. The invariance claim is tested, yielding a boolean $B$.\n- For the monomial basis, we construct the Vandermonde matrices $V_x$ and $V_y$ using `numpy.vander`. Coefficients $w^{\\mathrm{van}}$ and $w'^{\\mathrm{van}}$ are again found via least squares. The relative coefficient change $\\Delta$ and the condition number ratio $\\rho$ (using `numpy.linalg.cond`) are computed.\n- The results $[B, \\Delta, \\rho]$ are collected for all test cases and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre\n\ndef run_case(n, d, x_min, x_max, a, b):\n    \"\"\"\n    Performs the calculations for a single test case.\n    \"\"\"\n    # Step 1: Generate evenly spaced inputs and compute targets\n    x = np.linspace(x_min, x_max, n, dtype=np.float64)\n    t = np.sin(x)\n\n    # --- Part 1: Orthogonal Polynomial Basis (Legendre) ---\n\n    # Step 3 (part 1): Fit the model using the original inputs x\n    m_x = (x_min + x_max) / 2.0\n    r_x = x_max - x_min\n    # Handle the case where the interval has zero width\n    z = 2.0 * (x - m_x) / r_x if r_x != 0 else np.zeros_like(x)\n    \n    X_leg_x = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_x[:, j] = p_j(z)\n        \n    w_leg, _, _, _ = np.linalg.lstsq(X_leg_x, t, rcond=None)\n\n    # Step 3 (part 2): Apply rescaling y = ax + b and refit the model\n    y = a * x + b\n    y_min, y_max = np.min(y), np.max(y)\n    \n    m_y = (y_min + y_max) / 2.0\n    r_y = y_max - y_min\n    z_prime = 2.0 * (y - m_y) / r_y if r_y != 0 else np.zeros_like(y)\n\n    X_leg_y = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_y[:, j] = p_j(z_prime)\n\n    w_prime_leg, _, _, _ = np.linalg.lstsq(X_leg_y, t, rcond=None)\n\n    # Step 4: Test the invariance claim for the Legendre basis coefficients\n    epsilon = 1e-9\n    s_a = np.sign(a)\n    # The parity factor is (sign(a))^j for the j-th coefficient\n    parity_factor = np.array([s_a**j for j in range(d + 1)])\n    \n    expected_w_prime_leg = parity_factor * w_leg\n    max_abs_diff = np.max(np.abs(w_prime_leg - expected_w_prime_leg))\n    B = bool(max_abs_diff < epsilon)\n\n    # --- Part 2: Monomial Basis (Vandermonde) ---\n\n    # Step 5 (part 1): Fit models using the monomial basis\n    V_x = np.vander(x, d + 1, increasing=True)\n    w_van, _, _, _ = np.linalg.lstsq(V_x, t, rcond=None)\n\n    V_y = np.vander(y, d + 1, increasing=True)\n    w_prime_van, _, _, _ = np.linalg.lstsq(V_y, t, rcond=None)\n    \n    # Step 5 (part 2): Compute the relative coefficient change Delta\n    delta_num = np.linalg.norm(w_prime_van - w_van)\n    delta_den = np.linalg.norm(w_van) + 1e-12\n    Delta = delta_num / delta_den\n\n    # Step 5 (part 3): Compute the condition number ratio rho\n    kappa_x = np.linalg.cond(V_x, 2)\n    kappa_y = np.linalg.cond(V_y, 2)\n    \n    rho = kappa_y / kappa_x if kappa_x != 0 else np.inf\n\n    # Step 6: Return the list of results for this case\n    return [B, Delta, rho]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # n, d, xmin, xmax, a, b\n        (50, 10, -3.0, 3.0, 2.0, 1.0),\n        (6, 5, -1.0, 1.0, -1.0, 0.0),\n        (50, 10, -1.0, 2.0, 100.0, -5.0),\n        (50, 10, -3.0, 3.0, 0.01, 100.0),\n        (50, 10, -2.0, 4.0, 1.0, 10.0),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [B, Delta, rho] is achieved by Python's default str(list).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102236"}, {"introduction": "In modern machine learning, we often use an 'overcomplete' dictionary of basis functions where the number of functions $m$ is much larger than the number of data points $n$. This advanced practice [@problem_id:3102258] explores how regularization methods like the Lasso can select a sparse, meaningful model in this high-dimensional setting ($m \\gg n$). You will investigate theoretical guarantees for this recovery, such as the mutual coherence and the irrepresentable condition, to understand the critical link between a basis's properties and the success of model selection.", "problem": "You are given a linear model with a dictionary of basis functions. Let $\\,\\Phi \\in \\mathbb{R}^{n \\times m}\\,$ with $\\,m \\gg n\\,$ and columns normalized to unit $\\ell_2$-norm. A sparse coefficient vector $\\,\\beta^\\star \\in \\mathbb{R}^m\\,$ with support $\\,S = \\mathrm{supp}(\\beta^\\star)\\,$ generates observations $\\,y = \\Phi \\beta^\\star\\,$. You will fit coefficients via the Least Absolute Shrinkage and Selection Operator (Lasso), defined by the convex optimization problem\n$$\n\\hat{\\beta} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^m} \\;\\frac{1}{2}\\,\\lVert y - \\Phi \\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta \\rVert_1,\n$$\nwhere $\\,\\lambda \\ge 0\\,$ is a regularization parameter.\n\nYour tasks are to build specific overcomplete dictionaries $\\,\\Phi\\,$, solve the Lasso for given $\\,\\lambda\\,$, and compute identifiability and uniqueness diagnostics using the mutual coherence and the irrepresentable condition. Use the following standard definitions.\n\n- Mutual coherence (maximum absolute column correlation):\n$$\n\\mu(\\Phi) \\;=\\; \\max_{i \\ne j}\\; \\big|\\phi_i^\\top \\phi_j\\big|,\n$$\nwhere $\\,\\phi_j\\,$ denotes the $\\,j$-th column of $\\,\\Phi\\,$ and each column is scaled to satisfy $\\,\\lVert \\phi_j \\rVert_2 = 1$.\n- Mutual coherence uniqueness bound for an $\\,s$-sparse vector: the sufficient identifiability condition\n$$\ns \\;<\\; \\tfrac{1}{2}\\Big(1 + \\tfrac{1}{\\mu(\\Phi)}\\Big).\n$$\nIf $\\,\\mu(\\Phi) = 0\\,$, interpret the right-hand side as $+\\infty$.\n- Irrepresentable condition (sufficient for Lasso sign recovery). Let $\\,S=\\mathrm{supp}(\\beta^\\star)\\,$, $\\,S^c\\,$ be its complement, $\\,\\Phi_S \\in \\mathbb{R}^{n \\times |S|}\\,$ the sub-dictionary on $\\,S\\,$, and $\\,\\mathrm{sign}(\\beta^\\star_S)\\,$ the sign vector on $\\,S\\,$. Define\n$$\n\\eta \\;=\\; \\big\\lVert \\Phi_{S^c}^\\top \\Phi_S \\big(\\Phi_S^\\top \\Phi_S\\big)^{-1} \\,\\mathrm{sign}(\\beta^\\star_S) \\big\\rVert_\\infty.\n$$\nThe irrepresentable condition holds if $\\,\\eta < 1\\,$.\n\nImplement a program that, for each test case below, does all of the following:\n1) Constructs the specified overcomplete dictionary $\\,\\Phi\\,$ with columns normalized to unit norm.\n2) Constructs a sparse ground-truth $\\,\\beta^\\star\\,$ with support $\\,S\\,$ and forms $\\,y = \\Phi \\beta^\\star\\,$.\n3) Solves the Lasso problem for the provided $\\,\\lambda\\,$ using a correct numerical method that converges to a minimizer (for example, a cyclic coordinate descent using soft-thresholding derived from convex optimality conditions).\n4) Computes:\n   - $\\,B_{\\mathrm{supp}}\\,$: a boolean indicating whether the recovered support $\\,\\mathrm{supp}(\\hat{\\beta})\\,$ exactly equals $\\,S\\,$, where indices with $\\,|\\hat{\\beta}_j| \\le 10^{-6}\\,$ are treated as zero.\n   - $\\,B_{\\mathrm{sign}}\\,$: a boolean indicating whether the sign pattern on $\\,S\\,$ matches $\\,\\mathrm{sign}(\\beta^\\star_S)\\,$ exactly and $\\,\\hat{\\beta}_{S^c} = 0\\,$ under the same $\\,10^{-6}\\,$ threshold.\n   - $\\,B_{\\mathrm{IR}}\\,$: a boolean indicating whether the irrepresentable condition holds, using strict inequality with a numerical tolerance of $\\,10^{-8}\\,$ (i.e., declare true if $\\,\\eta < 1 - 10^{-8}$).\n   - $\\,B_{\\mu}\\,$: a boolean indicating whether the mutual-coherence uniqueness bound is satisfied, interpreting the case $\\,\\mu(\\Phi)=0\\,$ as always true for any finite $\\,s\\,$.\n5) Produces the final output as a single line containing a list of results for all test cases, where each test case contributes a list $[B_{\\mathrm{supp}}, B_{\\mathrm{sign}}, B_{\\mathrm{IR}}, B_{\\mu}]$.\n\nTest Suite (all angles are dimensionless and there are no physical units involved):\n- Test case A (random, overcomplete, easy recovery):\n  - $\\,n = 20\\,$, $\\,m = 100\\,$, random seed $\\,123\\,$.\n  - Construct $\\,\\Phi\\,$ by drawing independent standard normal entries and normalizing each column to unit norm.\n  - Support $\\,S = \\{5\\}\\,$ and $\\,\\beta^\\star_5 = 1.2\\,$, with all other entries zero. Thus $\\,s = 1\\,$.\n  - Set $\\,\\lambda = 0.05\\,$.\n- Test case B (overcomplete with an exact duplicate column causing non-uniqueness for support recovery):\n  - $\\,n = 20\\,$, $\\,m = 100\\,$, random seed $\\,456\\,$.\n  - Draw a random vector $\\,v \\in \\mathbb{R}^{n}\\,$ with independent standard normal entries and normalize it to unit norm.\n  - Form $\\,\\Phi\\,$ as follows: set column $\\,2\\,$ equal to $\\,v\\,$, set column $\\,0\\,$ equal to $\\,v\\,$ (a duplicate), and fill all other columns with independent standard normal vectors normalized to unit norm. Ensure columns $\\,0\\,$ and $\\,2\\,$ remain exact duplicates.\n  - Support $\\,S = \\{2\\}\\,$ and $\\,\\beta^\\star_2 = 1.0\\,$, with all other entries zero. Thus $\\,s = 1\\,$.\n  - Set $\\,\\lambda = 0.05\\,$.\n- Test case C (overcomplete with $\\,S^c\\,$ orthogonal to $\\,S\\,$, strong irrepresentability):\n  - $\\,n = 30\\,$, $\\,m = 200\\,$, random seed $\\,789\\,$.\n  - Let $\\,S = \\{0,1,2,3,4\\}\\,$, so $\\,s = 5\\,$. Set $\\,\\Phi_S\\,$ to be the first $\\,5\\,$ standard basis vectors in $\\,\\mathbb{R}^{n}\\,$. For $\\,S^c\\,$, draw random vectors supported only on coordinates $\\,\\{5,6,\\dots,29\\}\\,$ with independent standard normal entries and normalize each to unit norm; in particular, set the first $\\,5\\,$ entries of every $\\,\\phi_j\\,$ for $\\,j \\in S^c\\,$ to zero. This ensures $\\,\\Phi_{S^c}^\\top \\Phi_S = 0\\,$ and hence the irrepresentable condition holds.\n  - Set $\\,\\beta^\\star_j = 1.0\\,$ for all $\\,j \\in S\\,$ and zero otherwise.\n  - Set $\\,\\lambda = 0.05\\,$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to one test case in the order A, B, C, i.e., a line of the form\n  \"[[B_A_supp,B_A_sign,B_A_IR,B_A_mu],[B_B_supp,B_B_sign,B_B_IR,B_B_mu],[B_C_supp,B_C_sign,B_C_IR,B_C_mu]]\"\nwhere each entry is a boolean literal. No additional output or whitespace is permitted.", "solution": "The problem has been validated and is determined to be a well-posed and scientifically sound exercise in the field of statistical learning. It requires the implementation of the Lasso regression algorithm and the computation of several standard diagnostic metrics related to model selection and identifiability. The problem statement provides all necessary definitions, parameters, and procedures for three distinct test cases.\n\nThe approach to solving this problem involves three main steps for each test case:\n1.  Construction of the dictionary $\\,\\Phi \\in \\mathbb{R}^{n \\times m}\\,$ and the ground-truth data $\\,y = \\Phi \\beta^\\star\\,$, following the specific instructions for each case.\n2.  Numerical solution of the Lasso optimization problem to find an estimated coefficient vector $\\,\\hat{\\beta}\\,$.\n3.  Computation of four diagnostic booleans: $\\,B_{\\mathrm{supp}}\\,$, $\\,B_{\\mathrm{sign}}\\,$, $\\,B_{\\mathrm{IR}}\\,$, and $\\,B_{\\mu}\\,$.\n\nA detailed breakdown of the methodology for each step is provided below.\n\n**1. Lasso Solver: Cyclic Coordinate Descent**\n\nThe Lasso optimization problem is given by:\n$$\n\\hat{\\beta} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^m} \\;\\frac{1}{2}\\,\\lVert y - \\Phi \\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta \\rVert_1\n$$\nThis is a convex optimization problem. A widely used and effective method for solving it is cyclic coordinate descent. The algorithm iteratively minimizes the objective function with respect to a single coefficient $\\,\\beta_k\\,$, holding all other coefficients fixed.\n\nThe objective function, as a function of $\\,\\beta_k\\,$ alone, is:\n$$\nL(\\beta_k) = \\frac{1}{2}\\left\\lVert y - \\sum_{j \\ne k} \\phi_j \\beta_j - \\phi_k \\beta_k \\right\\rVert_2^2 + \\lambda |\\beta_k| + \\lambda \\sum_{j \\ne k} |\\beta_j|\n$$\nLet $\\,r^{(k)} = y - \\sum_{j \\ne k} \\phi_j \\beta_j\\,$ be the partial residual. The terms not involving $\\,\\beta_k\\,$ can be treated as constants. We minimize:\n$$\n\\frac{1}{2}\\left\\lVert r^{(k)} - \\phi_k \\beta_k \\right\\rVert_2^2 + \\lambda |\\beta_k| = \\frac{1}{2}\\left( \\lVert r^{(k)} \\rVert_2^2 - 2\\beta_k (\\phi_k^\\top r^{(k)}) + \\beta_k^2 \\lVert \\phi_k \\rVert_2^2 \\right) + \\lambda |\\beta_k|\n$$\nSince each column $\\,\\phi_k\\,$ is normalized to $\\,\\lVert \\phi_k \\rVert_2 = 1\\,$, this simplifies to minimizing:\n$$\n\\frac{1}{2}(\\beta_k^2 - 2\\beta_k (\\phi_k^\\top r^{(k)})) + \\lambda |\\beta_k|\n$$\nCompleting the square, this is equivalent to minimizing $\\,\\frac{1}{2}(\\beta_k - z_k)^2 + \\lambda |\\beta_k|\\,$, where $\\,z_k = \\phi_k^\\top r^{(k)}\\,$. The solution to this subproblem is given by the soft-thresholding operator:\n$$\n\\hat{\\beta}_k = S_\\lambda(z_k) = \\mathrm{sign}(z_k) \\max(|z_k| - \\lambda, 0)\n$$\nThe cyclic coordinate descent algorithm proceeds as follows:\n1.  Initialize $\\,\\hat{\\beta} = 0\\,$.\n2.  Repeat until convergence:\n    a. For $\\,k = 0, 1, \\dots, m-1\\,$:\n        i.  Compute $\\,z_k = \\phi_k^\\top (y - \\sum_{j \\ne k} \\phi_j \\hat{\\beta}_j) = \\phi_k^\\top y - \\sum_{j \\ne k} (\\phi_k^\\top \\phi_j) \\hat{\\beta}_j\\,$. An efficient update is $\\,z_k = \\phi_k^\\top (y - \\Phi\\hat{\\beta} + \\phi_k \\hat{\\beta}_k) = \\phi_k^\\top r_{current} + \\hat{\\beta}_k\\,$, where $\\,r_{current}\\,$ is the full residual.\n        ii. Update $\\,\\hat{\\beta}_k \\leftarrow S_\\lambda(z_k)\\,$.\n\nThe process is repeated for a fixed number of iterations or until the change in $\\,\\hat{\\beta}\\,$ falls below a specified tolerance.\n\n**2. Computation of Diagnostic Metrics**\n\nFor each test case, after computing $\\,\\hat{\\beta}\\,$, the following four metrics are evaluated.\n\n-   **$\\,B_{\\mathrm{supp}}\\,$ (Support Recovery):** This boolean is true if the set of indices of non-zero coefficients in $\\,\\hat{\\beta}\\,$ exactly matches the true support $\\,S\\,$. A numerical threshold of $\\,10^{-6}\\,$ is used to identify non-zero coefficients: $\\,\\mathrm{supp}(\\hat{\\beta}) = \\{j : |\\hat{\\beta}_j| > 10^{-6}\\}\\,$.\n    $$\n    B_{\\mathrm{supp}} = (\\mathrm{supp}(\\hat{\\beta}) = S)\n    $$\n\n-   **$\\,B_{\\mathrm{sign}}\\,$ (Sign Pattern Recovery):** This boolean is true if two conditions are met: (1) the recovered model has no false positives, i.e., $\\,\\mathrm{supp}(\\hat{\\beta}) \\subseteq S\\,$, and (2) for every index $\\,j \\in S\\,$, the sign of the estimated coefficient matches the true one, i.e., $\\,\\mathrm{sign}(\\hat{\\beta}_j) = \\mathrm{sign}(\\beta^\\star_j)\\,$.\n    This is a stronger condition than support recovery. If $\\,B_{\\mathrm{sign}}\\,$ is true, it implies that $\\,\\mathrm{supp}(\\hat{\\beta}) = S\\,$ (since $\\,\\beta^\\star_j \\ne 0\\,$ for $\\,j \\in S\\,$ implies $\\,\\hat{\\beta}_j \\ne 0\\,$ for $\\,j \\in S\\,$), and therefore $\\,B_{\\mathrm{supp}}\\,$ is also true.\n\n-   **$\\,B_{\\mathrm{IR}}\\,$ (Irrepresentable Condition):** This boolean checks if $\\,\\eta < 1\\,$, a sufficient condition for the Lasso to correctly identify the signs of the true coefficients for a sufficiently small $\\,\\lambda\\,$. The value $\\,\\eta\\,$ is defined as:\n    $$\n    \\eta \\;=\\; \\big\\lVert \\Phi_{S^c}^\\top \\Phi_S \\big(\\Phi_S^\\top \\Phi_S\\big)^{-1} \\,\\mathrm{sign}(\\beta^\\star_S) \\big\\rVert_\\infty\n    $$\n    where `S^c` is the complement of the support set `S`. The computation involves forming the sub-matrices $\\,\\Phi_S\\,$ and $\\,\\Phi_{S^c}\\,$, computing the Gram matrix $\\,G_S = \\Phi_S^\\top \\Phi_S\\,$, solving a linear system to find $\\,x = G_S^{-1} \\mathrm{sign}(\\beta^\\star_S)\\,$, and then finding the maximum absolute value of the vector $\\,\\Phi_{S^c}^\\top \\Phi_S x\\,$. The condition is checked with a numerical tolerance: $\\,\\eta < 1 - 10^{-8}\\,$.\n\n-   **$\\,B_{\\mu}\\,$ (Mutual Coherence Bound):** This boolean checks a sufficient condition for unique sparse recovery based on the mutual coherence $\\,\\mu(\\Phi)\\,$. The condition is:\n    $$\n    s \\;<\\; \\tfrac{1}{2}\\Big(1 + \\tfrac{1}{\\mu(\\Phi)}\\Big)\n    $$\n    where $\\,s = |S|\\,$ is the sparsity level. The mutual coherence $\\,\\mu(\\Phi) = \\max_{i \\ne j} |\\phi_i^\\top \\phi_j|\\,$ is the largest absolute inner product between distinct normalized columns of $\\,\\Phi\\,$. If $\\,\\mu(\\Phi)=0\\,$, the condition is considered met.\n\nThe implementation will proceed by defining Python functions for each of these components, which will be called for each of the three test cases specified in the problem. The final results will be aggregated into the required output format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Lasso problem.\n    \"\"\"\n\n    def lasso_cd_solver(Phi, y, lam, max_iter=2000, tol=1e-8):\n        \"\"\"Solves the Lasso problem using cyclic coordinate descent.\"\"\"\n        n, m = Phi.shape\n        beta = np.zeros(m)\n        \n        for iteration in range(max_iter):\n            beta_old = beta.copy()\n            for j in range(m):\n                # Efficiently calculate z_j = phi_j.T @ r_j + beta_j, where r_j is residual without feature j\n                # This equals phi_j.T @ y - phi_j.T @ Phi_not_j @ beta_not_j\n                # Pre-computing Gram matrix can be faster, but direct computation is also fine\n                # for moderate m.\n                r = y - Phi @ beta\n                z_j = Phi[:, j].T @ r + beta[j]\n                \n                # Soft-thresholding\n                if z_j > lam:\n                    beta[j] = z_j - lam\n                elif z_j < -lam:\n                    beta[j] = z_j + lam\n                else:\n                    beta[j] = 0.0\n            \n            if np.linalg.norm(beta - beta_old) < tol:\n                break\n                \n        return beta\n\n    def compute_diagnostics(Phi, s, S_true, beta_star, beta_hat, supp_thresh=1e-6, ir_tol=1e-8):\n        \"\"\"Computes the four boolean diagnostic metrics.\"\"\"\n        n, m = Phi.shape\n        \n        # B_supp: Exact support recovery\n        S_hat = set(np.where(np.abs(beta_hat) > supp_thresh)[0])\n        B_supp = (S_hat == S_true)\n        \n        # B_sign: Sign pattern recovery\n        S_true_list = sorted(list(S_true))\n        Sc_true_list = sorted(list(set(range(m)) - S_true))\n        \n        no_false_positives = np.all(np.abs(beta_hat[Sc_true_list]) <= supp_thresh)\n        \n        # Taking sign of zero to be zero\n        sign_beta_hat_S = np.sign(beta_hat[S_true_list])\n        sign_beta_star_S = np.sign(beta_star[S_true_list])\n        signs_match_on_S = np.all(sign_beta_hat_S == sign_beta_star_S)\n        \n        B_sign = no_false_positives and signs_match_on_S\n\n        # B_IR: Irrepresentable condition\n        Phi_S = Phi[:, S_true_list]\n        Phi_Sc = Phi[:, Sc_true_list]\n        \n        G_S = Phi_S.T @ Phi_S\n        \n        # Check if G_S is invertible; it should be for the given test cases\n        if np.linalg.cond(G_S) > 1 / np.finfo(G_S.dtype).eps:\n             # In a real scenario, handle this singularity.\n             # For this problem, test cases are designed to avoid it.\n             # If singular, IR condition is ill-defined/fails.\n             eta = np.inf\n        else:\n            inv_G_S_sign = np.linalg.solve(G_S, sign_beta_star_S)\n            v = Phi_Sc.T @ Phi_S @ inv_G_S_sign\n            eta = np.linalg.norm(v, ord=np.inf)\n\n        B_IR = (eta < 1.0 - ir_tol)\n        \n        # B_mu: Mutual coherence bound\n        G = Phi.T @ Phi\n        np.fill_diagonal(G, 0)\n        mu = np.max(np.abs(G))\n        \n        if mu < 1e-9: # Effectively zero\n            B_mu = True\n        else:\n            # Bound: s < 0.5 * (1 + 1/mu)\n            B_mu = s < 0.5 * (1.0 + 1.0 / mu)\n            \n        return [B_supp, B_sign, B_IR, B_mu]\n\n    test_cases = [\n        # A: Random overcomplete, s=1\n        {'n': 20, 'm': 100, 'seed': 123, 'S': {5}, 'beta_vals': {5: 1.2}, 'lam': 0.05, 'id': 'A'},\n        # B: Duplicate columns\n        {'n': 20, 'm': 100, 'seed': 456, 'S': {2}, 'beta_vals': {2: 1.0}, 'lam': 0.05, 'id': 'B'},\n        # C: Orthogonal sub-dictionaries\n        {'n': 30, 'm': 200, 'seed': 789, 'S': {0, 1, 2, 3, 4}, 'beta_vals': {0:1.0, 1:1.0, 2:1.0, 3:1.0, 4:1.0}, 'lam': 0.05, 'id': 'C'}\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        n, m, seed, S, beta_vals, lam = case['n'], case['m'], case['seed'], case['S'], case['beta_vals'], case['lam']\n        s = len(S)\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct Phi\n        if case['id'] == 'A':\n            Phi = rng.standard_normal(size=(n, m))\n            Phi /= np.linalg.norm(Phi, axis=0)\n        elif case['id'] == 'B':\n            v = rng.standard_normal(size=n)\n            v /= np.linalg.norm(v)\n            Phi = rng.standard_normal(size=(n, m))\n            for j in range(m):\n                 if j not in [0, 2]:\n                    Phi[:, j] /= np.linalg.norm(Phi[:, j])\n            Phi[:, 2] = v\n            Phi[:, 0] = v # Exact duplicate\n        elif case['id'] == 'C':\n            Phi = np.zeros((n, m))\n            Phi[:, :s] = np.eye(n, s)\n            for j in range(s, m):\n                v_j = rng.standard_normal(size=n)\n                v_j[:s] = 0 # Orthogonality to Phi_S\n                v_j /= np.linalg.norm(v_j)\n                Phi[:, j] = v_j\n        \n        # 2. Construct beta_star and y\n        beta_star = np.zeros(m)\n        for idx, val in beta_vals.items():\n            beta_star[idx] = val\n        y = Phi @ beta_star\n        \n        # 3. Solve Lasso\n        beta_hat = lasso_cd_solver(Phi, y, lam)\n        \n        # 4. Compute diagnostics\n        case_results = compute_diagnostics(Phi, s, S, beta_star, beta_hat)\n        results.append(case_results)\n\n    # Convert boolean to lowercase string literal for printing\n    # Example: [[True, True, True, False]] -> '[[true,true,true,false]]'\n    def format_bool_list(lst):\n        return '[' + ','.join(map(str, lst)).lower() + ']'\n    \n    output_str = '[' + ','.join(map(format_bool_list, results)) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3102258"}]}