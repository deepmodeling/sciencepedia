{"hands_on_practices": [{"introduction": "Kernel Ridge Regression learns more than just a set of predictions; it defines a continuous and differentiable function over the entire input space. This first exercise grounds the abstract theory in a concrete calculation, walking you through the complete process of defining the KRR estimator for a simple dataset. By deriving the predictor's functional form and then computing its derivative, you will gain hands-on insight into the mechanics of KRR and appreciate how it can be used not just for prediction, but for tasks like sensitivity analysis or gradient-based optimization [@problem_id:3136849].", "problem": "Consider Kernel Ridge Regression on one-dimensional inputs with training data $\\{(x_1, y_1), (x_2, y_2)\\}$, where $x_1 = 0$, $x_2 = 1$, $y_1 = 1$, and $y_2 = -1$. Let the hypothesis space be a Reproducing Kernel Hilbert Space (RKHS) associated with the Gaussian kernel $k(x, x') = \\exp(-(x - x')^2)$ and regularization parameter $\\lambda = \\frac{1}{2}$. The Kernel Ridge Regression estimator is defined as the minimizer over functions $f$ in the RKHS of the objective\n$$\n\\sum_{i=1}^{2} \\big(y_i - f(x_i)\\big)^{2} + \\lambda \\|f\\|_{\\mathcal{H}}^{2}.\n$$\nUsing the foundational statements of the Representer Theorem for RKHS and first-order optimality conditions, derive the functional form of the estimator and the associated linear system for the coefficients in terms of the kernel matrix. Then, by applying standard calculus to the kernel function, derive the derivative $\\frac{d}{dx} f(x)$ of the Kernel Ridge Regression predictor with respect to the input $x$. Finally, evaluate this derivative at the point $x^{\\star} = \\frac{1}{2}$ for the given data and parameters. Express your final answer as a single exact closed-form analytic expression involving only rational numbers and exponentials. Do not numerically approximate.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Training data points: $(x_1, y_1) = (0, 1)$ and $(x_2, y_2) = (1, -1)$.\n- Input vectors: $\\mathbf{x} \\in \\mathbb{R}^1$.\n- Kernel function: $k(x, x') = \\exp(-(x - x')^2)$.\n- Regularization parameter: $\\lambda = \\frac{1}{2}$.\n- Objective function to minimize: $J(f) = \\sum_{i=1}^{2} (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}}^2$.\n- Task: Derive the functional form of the estimator $f(x)$, the linear system for coefficients, the derivative $\\frac{d}{dx}f(x)$, and evaluate the derivative at $x^{\\star} = \\frac{1}{2}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on Kernel Ridge Regression (KRR), a standard and fundamental technique in machine learning and statistics. The use of a Gaussian kernel and a Reproducing Kernel Hilbert Space (RKHS) framework is well-established.\n- **Well-Posed:** The objective function is the sum of a squared error term and a squared norm penalty. This function is strictly convex for $\\lambda > 0$, as the kernel matrix $\\mathbf{K}$ for distinct points is positive definite, making $\\mathbf{K} + \\lambda \\mathbf{I}$ also positive definite and thus invertible. This guarantees the existence of a unique, stable, and meaningful solution.\n- **Objective:** The problem is stated in precise mathematical language, free from any subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is self-contained, scientifically sound, and well-posed. A complete solution will be provided.\n\n### Solution Derivation\nThe Kernel Ridge Regression (KRR) estimator $f(x)$ is the function in the Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$ that minimizes the regularized least-squares objective function:\n$$\nJ(f) = \\sum_{i=1}^{2} (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}}^2\n$$\nAccording to the Representer Theorem, the minimizer $f$ can be expressed as a linear combination of the kernel functions centered at the training data points:\n$$\nf(x) = \\sum_{i=1}^{2} \\alpha_i k(x_i, x)\n$$\nwhere $\\alpha_i$ are coefficients to be determined.\n\nThe norm of $f$ in the RKHS is given by:\n$$\n\\|f\\|_{\\mathcal{H}}^2 = \\left\\langle \\sum_{i=1}^{2} \\alpha_i k(x_i, \\cdot), \\sum_{j=1}^{2} \\alpha_j k(x_j, \\cdot) \\right\\rangle_{\\mathcal{H}} = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\alpha_i \\alpha_j k(x_i, x_j) = \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}\n$$\nwhere $\\mathbf{K}$ is the $2 \\times 2$ Gram matrix (or kernel matrix) with entries $K_{ij} = k(x_i, x_j)$, and $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2)^T$.\n\nThe predictions at the training points are $f(x_i) = \\sum_{j=1}^{2} \\alpha_j k(x_j, x_i) = (\\mathbf{K}\\boldsymbol{\\alpha})_i$. Let $\\mathbf{y} = (y_1, y_2)^T$. The objective function can be rewritten in matrix form as:\n$$\nJ(\\boldsymbol{\\alpha}) = (\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha})^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}\n$$\nTo find the optimal coefficients $\\boldsymbol{\\alpha}$ that minimize $J(\\boldsymbol{\\alpha})$, we compute the gradient of $J$ with respect to $\\boldsymbol{\\alpha}$ and set it to zero.\n$$\n\\nabla_{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha}) = -2 \\mathbf{K}^T (\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + 2 \\lambda \\mathbf{K} \\boldsymbol{\\alpha}\n$$\nSince the kernel matrix $\\mathbf{K}$ is symmetric ($\\mathbf{K}^T=\\mathbf{K}$), this simplifies to:\n$$\n\\nabla_{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha}) = -2 \\mathbf{K} (\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + 2 \\lambda \\mathbf{K} \\boldsymbol{\\alpha} = -2 \\mathbf{K} \\mathbf{y} + 2 \\mathbf{K}^2 \\boldsymbol{\\alpha} + 2 \\lambda \\mathbf{K} \\boldsymbol{\\alpha}\n$$\nSetting the gradient to zero:\n$$\n-2 \\mathbf{K} \\mathbf{y} + 2 \\mathbf{K}^2 \\boldsymbol{\\alpha} + 2 \\lambda \\mathbf{K} \\boldsymbol{\\alpha} = \\mathbf{0} \\implies \\mathbf{K}( (\\mathbf{K} + \\lambda \\mathbf{I})\\boldsymbol{\\alpha} - \\mathbf{y} ) = \\mathbf{0}\n$$\nSince the Gaussian kernel generates a positive definite kernel matrix for distinct points, $\\mathbf{K}$ is invertible. Therefore, we can left-multiply by $\\mathbf{K}^{-1}$ to obtain the linear system for the coefficients:\n$$\n(\\mathbf{K} + \\lambda \\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{y}\n$$\nNow, we substitute the given data. The training points are $x_1 = 0$ and $x_2 = 1$. The kernel is $k(x, x') = \\exp(-(x-x')^2)$. The kernel matrix $\\mathbf{K}$ is:\n$$\n\\mathbf{K} = \\begin{pmatrix} k(x_1, x_1) & k(x_1, x_2) \\\\ k(x_2, x_1) & k(x_2, x_2) \\end{pmatrix} = \\begin{pmatrix} k(0, 0) & k(0, 1) \\\\ k(1, 0) & k(1, 1) \\end{pmatrix}\n$$\n$K_{11} = k(0, 0) = \\exp(-(0-0)^2) = \\exp(0) = 1$.\n$K_{12} = k(0, 1) = \\exp(-(0-1)^2) = \\exp(-1)$.\n$K_{21} = k(1, 0) = \\exp(-(1-0)^2) = \\exp(-1)$.\n$K_{22} = k(1, 1) = \\exp(-(1-1)^2) = \\exp(0) = 1$.\nSo, $\\mathbf{K} = \\begin{pmatrix} 1 & \\exp(-1) \\\\ \\exp(-1) & 1 \\end{pmatrix}$.\n\nThe response vector is $\\mathbf{y} = (1, -1)^T$ and the regularization parameter is $\\lambda = \\frac{1}{2}$. The linear system becomes:\n$$\n\\left( \\begin{pmatrix} 1 & \\exp(-1) \\\\ \\exp(-1) & 1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} \\frac{3}{2} & \\exp(-1) \\\\ \\exp(-1) & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nTo solve for $\\boldsymbol{\\alpha}$, we invert the matrix $A = \\mathbf{K} + \\lambda \\mathbf{I}$:\n$$\n\\det(A) = \\left(\\frac{3}{2}\\right)^2 - (\\exp(-1))^2 = \\frac{9}{4} - \\exp(-2)\n$$\n$$\nA^{-1} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} & -\\exp(-1) \\\\ -\\exp(-1) & \\frac{3}{2} \\end{pmatrix}\n$$\nNow, we find $\\boldsymbol{\\alpha} = A^{-1}\\mathbf{y}$:\n$$\n\\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} & -\\exp(-1) \\\\ -\\exp(-1) & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} + \\exp(-1) \\\\ -\\exp(-1) - \\frac{3}{2} \\end{pmatrix}\n$$\nWe can simplify $\\alpha_1$:\n$$\n\\alpha_1 = \\frac{\\frac{3}{2} + \\exp(-1)}{\\frac{9}{4} - \\exp(-2)} = \\frac{\\frac{3}{2} + \\exp(-1)}{(\\frac{3}{2} - \\exp(-1))(\\frac{3}{2} + \\exp(-1))} = \\frac{1}{\\frac{3}{2} - \\exp(-1)} = \\frac{2\\exp(1)}{3\\exp(1) - 2}\n$$\nAnd $\\alpha_2 = -\\alpha_1 = -\\frac{2\\exp(1)}{3\\exp(1) - 2}$.\n\nThe KRR predictor is $f(x) = \\alpha_1 k(0, x) + \\alpha_2 k(1, x) = \\alpha_1 \\exp(-x^2) + \\alpha_2 \\exp(-(1-x)^2)$.\nWe need to find its derivative with respect to $x$:\n$$\n\\frac{d}{dx}f(x) = \\alpha_1 \\frac{d}{dx}\\exp(-x^2) + \\alpha_2 \\frac{d}{dx}\\exp(-(1-x)^2)\n$$\nUsing the chain rule:\n$$\n\\frac{d}{dx}f(x) = \\alpha_1 (-2x \\exp(-x^2)) + \\alpha_2 (-2(1-x)(-1) \\exp(-(1-x)^2))\n$$\n$$\n\\frac{d}{dx}f(x) = -2x \\alpha_1 \\exp(-x^2) + 2(1-x) \\alpha_2 \\exp(-(1-x)^2)\n$$\nFinally, we evaluate this derivative at the point $x^{\\star} = \\frac{1}{2}$:\n$$\n\\frac{d}{dx}f(x)\\bigg|_{x=1/2} = -2\\left(\\frac{1}{2}\\right)\\alpha_1 \\exp\\left(-\\left(\\frac{1}{2}\\right)^2\\right) + 2\\left(1 - \\frac{1}{2}\\right)\\alpha_2 \\exp\\left(-\\left(1 - \\frac{1}{2}\\right)^2\\right)\n$$\n$$\n= -\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right) + 2\\left(\\frac{1}{2}\\right)\\alpha_2 \\exp\\left(-\\left(\\frac{1}{2}\\right)^2\\right)\n$$\n$$\n= -\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right) + \\alpha_2 \\exp\\left(-\\frac{1}{4}\\right) = (\\alpha_2 - \\alpha_1)\\exp\\left(-\\frac{1}{4}\\right)\n$$\nSubstituting $\\alpha_2 = -\\alpha_1$:\n$$\n(-\\alpha_1 - \\alpha_1)\\exp\\left(-\\frac{1}{4}\\right) = -2\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right)\n$$\nNow, substituting the value of $\\alpha_1$:\n$$\n-2 \\left( \\frac{2\\exp(1)}{3\\exp(1) - 2} \\right) \\exp\\left(-\\frac{1}{4}\\right) = -\\frac{4\\exp(1)\\exp(-1/4)}{3\\exp(1) - 2}\n$$\n$$\n= -\\frac{4\\exp(1 - 1/4)}{3\\exp(1) - 2} = -\\frac{4\\exp(3/4)}{3\\exp(1) - 2}\n$$\nThis is the final exact closed-form analytic expression.", "answer": "$$\\boxed{-\\frac{4\\exp(3/4)}{3\\exp(1) - 2}}$$", "id": "3136849"}, {"introduction": "A crucial step in any machine learning workflow is model selection, such as choosing the optimal regularization parameter $\\lambda$. While leave-one-out cross-validation (LOOCV) is a thorough method for this, its naive implementation—retraining the model $n$ times for a dataset of size $n$—is often computationally prohibitive. This practice problem guides you through the derivation of a remarkable and powerful formula that allows you to compute all LOOCV predictions from a single model fit, showcasing a deep and elegant property of KRR and other linear smoothers [@problem_id:3136836].", "problem": "Consider a training sample $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$, a positive semi-definite kernel function $k(\\cdot, \\cdot)$ with Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ defined by $K_{ij} = k(x_{i}, x_{j})$, and a regularization parameter $\\lambda > 0$. Kernel Ridge Regression (KRR) is defined as the minimizer of the penalized empirical risk\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - f(x_{i})\\big)^{2} + \\lambda \\,\\|f\\|_{\\mathcal{H}}^{2},\n$$\nwhere $\\mathcal{H}$ denotes the Reproducing Kernel Hilbert Space (RKHS) induced by $k(\\cdot, \\cdot)$. It is a well-known fact that the fitted values at the training inputs are linear in the response vector, so there exists a matrix $S_{\\lambda} \\in \\mathbb{R}^{n \\times n}$, called the hat matrix, such that the vector of fitted values is $\\hat{\\boldsymbol{y}} = S_{\\lambda} \\boldsymbol{y}$. Let $s_{ii}$ denote the $i$-th diagonal entry of $S_{\\lambda}$, and let $\\hat{y}_{i}$ denote the $i$-th entry of $\\hat{\\boldsymbol{y}}$.\n\nFor each index $i \\in \\{1, \\dots, n\\}$, define the leave-one-out fitted value $\\hat{y}_{-i}$ to be the prediction at $x_{i}$ obtained by training KRR on all data except the pair $(x_{i}, y_{i})$. Starting only from the base definitions above and the linearity of the estimator in $\\boldsymbol{y}$, derive a closed-form analytic expression for $\\hat{y}_{-i}$ in terms of $\\hat{y}_{i}$, $y_{i}$, and $s_{ii}$. Your final answer must be a single closed-form expression.", "solution": "We begin from the stated base: Kernel Ridge Regression (KRR) produces fitted values at the training inputs that are linear in the response vector $\\boldsymbol{y} \\in \\mathbb{R}^{n}$. Therefore, there exists a matrix $S_{\\lambda} \\in \\mathbb{R}^{n \\times n}$, depending only on the inputs $\\{x_{i}\\}_{i=1}^{n}$ and $\\lambda$, such that\n$$\n\\hat{\\boldsymbol{y}} = S_{\\lambda} \\boldsymbol{y}.\n$$\nLet $s_{ij}$ denote the $(i,j)$ entry of $S_{\\lambda}$ and $s_{ii}$ its diagonal entry at index $i$. By linearity, the $i$-th fitted value can be written as\n$$\n\\hat{y}_{i} = \\sum_{j=1}^{n} s_{ij} y_{j} = s_{ii} y_{i} + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n\nNow consider the leave-one-out setting, where the pair $(x_{i}, y_{i})$ is removed and KRR is trained on the remaining $n-1$ pairs. Denote by $\\hat{y}_{-i}$ the prediction at $x_{i}$ from this leave-one-out fit. Because the estimator is linear in $\\boldsymbol{y}$ and the objective is quadratic, adding an observation whose residual is exactly zero does not alter the minimizer: if an observation satisfies $y_{i} = f(x_{i})$ under a given fit $f$, then its contribution to the empirical loss and to the gradient vanishes for that fit, and hence including or excluding that observation leaves the minimizer unchanged.\n\nLeverage this by considering training on the full $n$ points but replacing the $i$-th response by a variable $t \\in \\mathbb{R}$. By linearity,\n$$\n\\hat{y}_{i}(t) = s_{ii} t + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\nChoose $t$ to equal the leave-one-out prediction $\\hat{y}_{-i}$. Under this choice, the observation $(x_{i}, t)$ would have zero residual at the fitted function produced by the leave-one-out fit, so including it does not change the minimizer. Consequently, the fitted value at $x_{i}$ under the full-data-with-$t$ training must equal $t$ itself:\n$$\nt = \\hat{y}_{i}(t) = s_{ii} t + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\nSolving this scalar linear equation for $t$ gives\n$$\nt = \\frac{\\sum_{j \\neq i} s_{ij} y_{j}}{1 - s_{ii}}.\n$$\nFinally, express the sum over $j \\neq i$ in terms of $\\hat{y}_{i}$ and $y_{i}$:\n$$\n\\sum_{j \\neq i} s_{ij} y_{j} = \\hat{y}_{i} - s_{ii} y_{i}.\n$$\nSubstituting this into the expression for $t$ yields the desired closed-form formula for the leave-one-out prediction:\n$$\n\\hat{y}_{-i} = \\frac{\\hat{y}_{i} - s_{ii} y_{i}}{1 - s_{ii}}.\n$$\nThis expression depends only on the full-data fitted value at index $i$, the observed response at index $i$, and the $i$-th diagonal element of the hat matrix $S_{\\lambda}$.", "answer": "$$\\boxed{\\frac{\\hat{y}_{i} - s_{ii} y_{i}}{1 - s_{ii}}}$$", "id": "3136836"}, {"introduction": "The journey from a mathematical equation to a working algorithm is filled with practical challenges, and numerical stability is paramount among them. The core of KRR involves solving the linear system $(K + \\lambda I)\\alpha = y$, but if the matrix $K + \\lambda I$ is ill-conditioned, standard solvers can produce highly inaccurate results. This final hands-on exercise transitions from pure theory to computational practice, asking you to investigate this very issue. By implementing a numerical experiment, you will learn to diagnose ill-conditioning, understand its consequences, and apply a common stabilization technique known as \"jitter,\" exploring the critical trade-off between numerical accuracy and solution bias [@problem_id:3136815].", "problem": "You are asked to write a complete, runnable program that studies the numerical conditioning and stability of solving the kernel ridge regression normal equations via Cholesky factorization, and to propose minimal diagonal jitter magnitudes that stabilize the solver without introducing excessive bias. The problem must be solved entirely in pure mathematics and numerical linear algebra terms, and the final output must be a single-line list of floating-point jitter values, one per test case.\n\nBase definitions and facts:\n- A symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) if and only if all its eigenvalues are strictly positive. For an SPD matrix $A$, the Cholesky factorization $A = L L^{\\top}$ exists and is unique with $L$ lower triangular and positive diagonal.\n- The $2$-norm condition number of an SPD matrix $A$ is $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $A$.\n- In kernel ridge regression, solving for the dual coefficients reduces to solving a linear system of the form $(\\mathbf{K} + \\lambda I)\\boldsymbol{\\alpha} = \\mathbf{y}$, where $\\mathbf{K}$ is the kernel Gram matrix, $\\lambda \\ge 0$ is a regularization parameter, $I$ is the identity matrix, and $\\mathbf{y}$ is the target vector.\n- Adding a small multiple of the identity matrix $j I$ (called jitter) with $j \\ge 0$ can improve numerical stability by increasing $\\lambda_{\\min}$, but too large a $j$ biases the solution because it changes the system being solved.\n\nYour tasks:\n- For each test case below, construct a kernel matrix $\\mathbf{K}$ using the Gaussian radial basis function kernel $k(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|_2^2}{2\\sigma^2}\\right)$ with the specified bandwidth $\\sigma > 0$.\n- For the given base regularization $\\lambda \\ge 0$, form $M(\\lambda) = \\mathbf{K} + \\lambda I$.\n- Generate a ground-truth solution vector $\\boldsymbol{\\alpha}_{\\mathrm{true}} \\in \\mathbb{R}^n$ from a reproducible normal distribution, compute $\\mathbf{y} = M(\\lambda)\\,\\boldsymbol{\\alpha}_{\\mathrm{true}}$, and then consider, for a grid of jitter values $j$, the perturbed systems $A(j) = M(\\lambda) + j I$.\n- For each $j$ in the grid, do all of the following:\n  1. If $A(j)$ is not SPD (that is, if its minimum eigenvalue is not strictly positive), skip this $j$.\n  2. Compute the eigenvalue-based condition number $\\kappa_2(A(j))$.\n  3. Compute the reference solution $\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j)$ to $A(j)\\,\\boldsymbol{\\alpha} = \\mathbf{y}$ using an eigen-decomposition based solver, which is well-conditioned for SPD.\n  4. Compute the Cholesky-based solution $\\boldsymbol{\\alpha}_{\\mathrm{chol}}(j)$ to $A(j)\\,\\boldsymbol{\\alpha} = \\mathbf{y}$ using Cholesky factorization and triangular solves.\n  5. Quantify the relative numerical error of the Cholesky solve as $e_{\\mathrm{num}}(j) = \\|\\boldsymbol{\\alpha}_{\\mathrm{chol}}(j) - \\boldsymbol{\\alpha}_{\\mathrm{eig}}(j)\\|_2 / \\|\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j)\\|_2$.\n  6. Quantify the relative bias induced by adding jitter as $e_{\\mathrm{bias}}(j) = \\|\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j) - \\boldsymbol{\\alpha}_{\\mathrm{true}}\\|_2 / \\|\\boldsymbol{\\alpha}_{\\mathrm{true}}\\|_2$.\n- Select the smallest jitter $j^\\star$ from the grid that simultaneously satisfies the numerical stability and bias criteria\n  $$e_{\\mathrm{num}}(j^\\star) \\le \\tau_{\\mathrm{num}} \\quad \\text{and} \\quad e_{\\mathrm{bias}}(j^\\star) \\le \\tau_{\\mathrm{bias}},$$\n  with tolerances $\\tau_{\\mathrm{num}} = 10^{-8}$ and $\\tau_{\\mathrm{bias}} = 10^{-3}$.\n- If no $j$ on the grid satisfies both criteria, select, among those with $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$, the one with the smallest $e_{\\mathrm{num}}(j)$; if none satisfy the bias criterion either, select the $j$ for which Cholesky succeeds and $A(j)$ is SPD that minimizes $e_{\\mathrm{bias}}(j)$.\n\nTest suite:\n- Common setup:\n  - Use the Gaussian kernel $k(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|_2^2}{2\\sigma^2}\\right)$.\n  - Use the Euclidean $2$-norm for all norms described.\n  - The jitter grid is $\\{0\\} \\cup \\{10^k : k \\in \\{-16,-15,\\dots,-2\\}\\}$.\n  - For eigenvalue checks, treat $A(j)$ as SPD only if its numerically computed minimum eigenvalue is strictly positive.\n- Case $1$ (ill-conditioned but SPD, aims to require nonzero jitter):\n  - Dimension $n = 25$ with inputs $\\mathbf{x}_i = i/(n-1)$ for $i \\in \\{0,1,\\dots,n-1\\}$, treated as one-dimensional column vectors.\n  - Bandwidth $\\sigma = 10.0$.\n  - Regularization $\\lambda = 0$.\n  - Ground truth: draw $\\boldsymbol{\\alpha}_{\\mathrm{true}} \\sim \\mathcal{N}(0,1)^n$ using a fixed seed $42$.\n- Case $2$ (positive semidefinite with duplicates, Cholesky fails at zero jitter; requires positive jitter):\n  - Start with $25$ inputs $\\mathbf{x}_i = i/24$ for $i \\in \\{0,1,\\dots,24\\}$, then append $5$ duplicates of the inputs with indices $10$ through $14$ to obtain $n = 30$ total points, all as one-dimensional column vectors.\n  - Bandwidth $\\sigma = 0.2$.\n  - Regularization $\\lambda = 0$.\n  - Ground truth: draw $\\boldsymbol{\\alpha}_{\\mathrm{true}} \\sim \\mathcal{N}(0,1)^n$ using a fixed seed $7$.\n- Case $3$ (well-conditioned, zero jitter should suffice):\n  - Dimension $n = 60$ with inputs drawn independently from $\\mathrm{Uniform}[0,1]$ using a fixed seed $123$, as one-dimensional column vectors.\n  - Bandwidth $\\sigma = 2.0$.\n  - Regularization $\\lambda = 10^{-12}$.\n  - Ground truth: draw $\\boldsymbol{\\alpha}_{\\mathrm{true}} \\sim \\mathcal{N}(0,1)^n$ using a fixed seed $99$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one floating-point number per test case corresponding to the selected jitter $j^\\star$ for that case, in the order of Cases $1$, $2$, $3$. For example, the output should look like $[j_1,j_2,j_3]$.\n\nThere are no physical units or angles involved. All answers are dimensionless real numbers. The output values must be deterministic given the specified seeds and inputs. Do not print any additional text.", "solution": "The user seeks to determine the optimal diagonal jitter $j^\\star$ to add to the kernel ridge regression matrix $M(\\lambda) = \\mathbf{K} + \\lambda I$ for three distinct test cases. The optimal jitter $j^\\star$ is the smallest value from a specified grid that ensures the numerical stability of a Cholesky-based linear solver while keeping the induced solution bias within a given tolerance. The problem entails a systematic numerical investigation guided by principles of linear algebra and numerical analysis.\n\nThe overall procedure for each test case is as follows:\n1.  **System Setup**: Construct the data points $\\mathbf{x}_i$, the Gaussian RBF kernel matrix $\\mathbf{K}$, the base regression matrix $M(\\lambda) = \\mathbf{K} + \\lambda I$, and the ground-truth solution vector $\\boldsymbol{\\alpha}_{\\mathrm{true}}$. The target vector is then synthesized as $\\mathbf{y} = M(\\lambda) \\boldsymbol{\\alpha}_{\\mathrm{true}}$.\n2.  **Iterative Analysis**: For each jitter value $j$ in the grid $\\{0\\} \\cup \\{10^k : k \\in \\{-16, \\dots, -2\\}\\}$, form the perturbed matrix $A(j) = M(\\lambda) + j I$.\n3.  **Validation and Solution**:\n    a. We first verify that $A(j)$ is symmetric positive definite (SPD) by computing its eigenvalues and ensuring the minimum eigenvalue $\\lambda_{\\min}(A(j)) > 0$. If $A(j)$ is not SPD, it is unsuitable for Cholesky factorization, and we discard this value of $j$.\n    b. For a valid SPD matrix $A(j)$, we compute two solutions to the system $A(j)\\boldsymbol{\\alpha} = \\mathbf{y}$:\n        i. **Reference Solution $\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j)$**: This is computed using eigen-decomposition. If $A(j) = VDV^\\top$, where $V$ is the orthogonal matrix of eigenvectors and $D$ is the diagonal matrix of eigenvalues, the solution is $\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j) = V D^{-1} V^\\top \\mathbf{y}$. This method is numerically stable for SPD matrices and serves as our benchmark for accuracy.\n        ii. **Cholesky Solution $\\boldsymbol{\\alpha}_{\\mathrm{chol}}(j)$**: This is computed by first finding the Cholesky factorization $A(j) = LL^\\top$, and then solving two triangular systems: $L\\mathbf{z} = \\mathbf{y}$ (forward substitution) followed by $L^\\top \\boldsymbol{\\alpha} = \\mathbf{z}$ (backward substitution). This method is computationally efficient but can be numerically unstable if $A(j)$ is ill-conditioned (i.e., has a large condition number $\\kappa_2(A) = \\lambda_{\\max}/\\lambda_{\\min}$). If the Cholesky factorization fails for a matrix that is theoretically SPD, this indicates severe ill-conditioning, and this value of $j$ is discarded.\n4.  **Error Quantification**: Two error metrics are calculated for each valid $j$:\n    a. **Numerical Error**: $e_{\\mathrm{num}}(j) = \\frac{\\|\\boldsymbol{\\alpha}_{\\mathrm{chol}}(j) - \\boldsymbol{\\alpha}_{\\mathrm{eig}}(j)\\|_2}{\\|\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j)\\|_2}$. This measures the inaccuracy of the Cholesky-based solver relative to the stable eigen-decomposition based solver. A high value suggests that ill-conditioning is degrading the Cholesky solution.\n    b. **Bias Error**: $e_{\\mathrm{bias}}(j) = \\frac{\\|\\boldsymbol{\\alpha}_{\\mathrm{eig}}(j) - \\boldsymbol{\\alpha}_{\\mathrm{true}}\\|_2}{\\|\\boldsymbol{\\alpha}_{\\mathrm{true}}\\|_2}$. This measures how much the solution to the jitter-perturbed system $A(j)\\boldsymbol{\\alpha} = \\mathbf{y}$ deviates from the true solution $\\boldsymbol{\\alpha}_{\\mathrm{true}}$ of the original, unperturbed problem. Adding jitter $j>0$ inherently changes the problem we are solving, introducing bias.\n5.  **Optimal Jitter Selection**: The optimal jitter $j^\\star$ is selected based on a hierarchical criterion designed to find the smallest possible perturbation that achieves numerical stability without introducing excessive bias.\n    a. **Primary Criterion**: Find the smallest $j$ that satisfies both $e_{\\mathrm{num}}(j) \\le \\tau_{\\mathrm{num}}$ and $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$, where the tolerances are given as $\\tau_{\\mathrm{num}} = 10^{-8}$ and $\\tau_{\\mathrm{bias}} = 10^{-3}$.\n    b. **Secondary Criterion (if primary fails)**: If no $j$ meets both conditions, we relax the numerical stability requirement. Among all $j$ that satisfy the bias constraint $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$, we select the one that minimizes the numerical error $e_{\\mathrm{num}}(j)$.\n    c. **Tertiary Criterion (if secondary also fails)**: If no $j$ even satisfies the bias constraint, we prioritize finding a solution, however biased. Among all $j$ for which the matrix $A(j)$ was SPD and the Cholesky solver succeeded, we select the one that minimizes the bias error $e_{\\mathrm{bias}}(j)$.\n\nThis process is applied to each of the three test cases, which are designed to probe different scenarios: an ill-conditioned but theoretically SPD matrix, a singular matrix arising from duplicate data, and a well-conditioned matrix.\n\n**Case 1**: $\\mathbf{K}$ is generated with a large bandwidth $\\sigma=10.0$ and $\\lambda=0$. This creates an ill-conditioned kernel matrix whose smallest eigenvalues are very close to zero, making $A(0)=\\mathbf{K}$ numerically challenging for Cholesky factorization. A small, non-zero jitter is expected to be necessary to \"lift\" these small eigenvalues and stabilize the computation.\n\n**Case 2**: The input data contains duplicate points, and $\\lambda=0$. This makes the kernel matrix $\\mathbf{K}$ mathematically singular (i.e., it has at least one eigenvalue that is exactly zero) and thus positive semidefinite, not positive definite. Cholesky factorization of $A(0)=\\mathbf{K}$ is guaranteed to fail. A positive jitter $j>0$ is strictly required to make the matrix $A(j) = \\mathbf{K} + jI$ positive definite.\n\n**Case 3**: The matrix is formed with a small non-zero regularization $\\lambda = 10^{-12}$ and unique data points. The resulting matrix $A(0) = \\mathbf{K} + \\lambda I$ is expected to be well-conditioned and strictly positive definite. Therefore, we anticipate that no additional jitter will be necessary, and $j^\\star=0$ will satisfy the primary criterion.\n\nThe implementation will use `numpy` for linear algebra operations and `scipy.linalg` for specialized, robust solvers like `cholesky` and `solve_triangular`. Reproducibility is ensured by using fixed seeds for all random number generation. The final output will be a list containing the determined $j^\\star$ for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, eigh\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            'case_id': 1,\n            'n': 25,\n            'data_gen': lambda n, rng: np.linspace(0, 1, n).reshape(-1, 1),\n            'sigma': 10.0,\n            'lambda': 0.0,\n            'seed_alpha': 42,\n            'seed_data': None,\n        },\n        {\n            'case_id': 2,\n            'n_base': 25,\n            'data_gen': lambda n_base, rng: np.concatenate([\n                np.linspace(0, 1, n_base).reshape(-1, 1),\n                np.linspace(0, 1, n_base).reshape(-1, 1)[10:15]\n            ]),\n            'sigma': 0.2,\n            'lambda': 0.0,\n            'seed_alpha': 7,\n            'seed_data': None,\n        },\n        {\n            'case_id': 3,\n            'n': 60,\n            'data_gen': lambda n, rng: rng.uniform(0, 1, n).reshape(-1, 1),\n            'sigma': 2.0,\n            'lambda': 1e-12,\n            'seed_alpha': 99,\n            'seed_data': 123,\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        j_star = process_case(case_params)\n        results.append(j_star)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef rbf_kernel(X, sigma):\n    \"\"\"\n    Computes the Gaussian RBF kernel matrix.\n    k(x, z) = exp(-||x-z||_2^2 / (2*sigma^2))\n    \n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n        sigma (float): Bandwidth of the RBF kernel.\n\n    Returns:\n        np.ndarray: The kernel matrix K of shape (n, n).\n    \"\"\"\n    sq_dists = cdist(X, X, 'sqeuclidean')\n    return np.exp(-sq_dists / (2 * sigma**2))\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case to find the optimal jitter.\n    \"\"\"\n    # Tolerances\n    tau_num = 1e-8\n    tau_bias = 1e-3\n\n    # Generate data\n    data_rng = np.random.default_rng(params['seed_data']) if params['seed_data'] is not None else None\n    if params['case_id'] == 2:\n        X = params['data_gen'](params['n_base'], data_rng)\n        n = X.shape[0]\n    else:\n        n = params['n']\n        X = params['data_gen'](n, data_rng)\n\n    # Generate ground truth alpha\n    alpha_rng = np.random.default_rng(params['seed_alpha'])\n    alpha_true = alpha_rng.normal(0, 1, n)\n\n    # System setup\n    K = rbf_kernel(X, params['sigma'])\n    M_lambda = K + params['lambda'] * np.identity(n)\n    y = M_lambda @ alpha_true\n\n    # Jitter analysis\n    jitter_grid = [0.0] + [10.0**k for k in range(-16, -1)]\n    j_results = []\n\n    for j in jitter_grid:\n        A_j = M_lambda + j * np.identity(n)\n\n        # 1. Check if SPD via eigenvalues\n        try:\n            eigenvalues, V = eigh(A_j)\n            if np.min(eigenvalues) = 1e-15: # Use a small tolerance for strict positivity\n                continue\n        except np.linalg.LinAlgError:\n            continue\n\n        # 3. Reference solution (eigen-decomposition)\n        D_inv = np.diag(1.0 / eigenvalues)\n        alpha_eig = V @ (D_inv @ (V.T @ y))\n        \n        # 4. Cholesky solution\n        try:\n            L = cholesky(A_j, lower=True)\n            z = solve_triangular(L, y, lower=True, check_finite=False)\n            alpha_chol = solve_triangular(L.T, z, lower=False, check_finite=False)\n        except np.linalg.LinAlgError:\n            continue\n\n        # 5. Numerical error\n        norm_alpha_eig = np.linalg.norm(alpha_eig)\n        e_num = np.linalg.norm(alpha_chol - alpha_eig) / norm_alpha_eig if norm_alpha_eig > 0 else 0.0\n\n        # 6. Bias error\n        norm_alpha_true = np.linalg.norm(alpha_true)\n        e_bias = np.linalg.norm(alpha_eig - alpha_true) / norm_alpha_true if norm_alpha_true > 0 else 0.0\n\n        j_results.append({'j': j, 'e_num': e_num, 'e_bias': e_bias})\n    \n    # Selection logic\n    # Rule 1: Smallest j satisfying both criteria\n    s1_candidates = [r for r in j_results if r['e_num'] = tau_num and r['e_bias'] = tau_bias]\n    if s1_candidates:\n        return min(r['j'] for r in s1_candidates)\n\n    # Rule 2: Among those satisfying bias, find j that minimizes num_error\n    s2_candidates = [r for r in j_results if r['e_bias'] = tau_bias]\n    if s2_candidates:\n        # min will pick the first one in case of a tie in e_num, which corresponds to smaller j\n        best_choice = min(s2_candidates, key=lambda r: r['e_num'])\n        return best_choice['j']\n\n    # Rule 3: Among all successful, find j that minimizes bias_error\n    if j_results:\n        # min will pick the first one in case of a tie in e_bias, which corresponds to smaller j\n        best_choice = min(j_results, key=lambda r: r['e_bias'])\n        return best_choice['j']\n        \n    # Should not be reached in this problem\n    return float('nan')\n\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3136815"}]}