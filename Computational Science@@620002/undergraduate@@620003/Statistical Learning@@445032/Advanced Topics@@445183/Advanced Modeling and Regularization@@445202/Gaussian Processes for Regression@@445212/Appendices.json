{"hands_on_practices": [{"introduction": "A core strength of Gaussian Processes is their principled handling of uncertainty. This practice explores a crucial aspect of this: how the model uses the observation noise variance, $\\sigma^2$, to reconcile conflicting data points. By analyzing a simple but insightful scenario with two different measurements at the same input location ([@problem_id:3122972]), we can gain a deep understanding of how the GP posterior balances the prior belief from the kernel with the likelihood of the observed data.", "problem": "Consider a one-dimensional Gaussian process (GP) regression problem with a zero-mean prior and a squared-exponential (also called radial basis function) covariance function. The prior is specified by the following ingredients:\n- A latent function $f$ modeled as a Gaussian process (GP): $f \\sim \\mathcal{GP}(0, k)$.\n- A covariance function $k(x, x') = \\sigma_f^2 \\exp\\!\\left(-\\tfrac{1}{2} \\left(\\tfrac{x - x'}{\\ell}\\right)^2\\right)$ with fixed hyperparameters $\\sigma_f^2$ and $\\ell$.\n- Noisy observations $y_i = f(x_i) + \\varepsilon_i$, where the noise variables are independent and identically distributed as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nYou are given a dataset with repeated inputs and conflicting outputs:\n- Inputs $X = [x_1, x_2, x_3]^\\top = [0.0, 0.0, 1.0]^\\top$.\n- Outputs $y = [y_1, y_2, y_3]^\\top = [1.0, -1.0, 0.0]^\\top$.\n\nUse the squared-exponential covariance function with fixed hyperparameters $\\sigma_f^2 = 1.0$ and $\\ell = 0.6$. Assume independent Gaussian observation noise with variance $\\sigma^2$. For a test input $x_\\star = 0.0$, your task is to:\n1. Start from the joint Gaussian distribution of the training outputs and the latent function value at $x_\\star$, using only core multivariate normal properties and the given model assumptions.\n2. Derive the posterior predictive distribution of the latent function value $f_\\star = f(x_\\star)$ conditional on the observed data, in terms of $X$, $y$, $x_\\star$, $\\sigma_f^2$, $\\ell$, and $\\sigma^2$.\n3. Implement a numerically stable computation for the posterior predictive mean $m_\\star$ and variance $v_\\star$ of $f_\\star$ by using Cholesky factorization of the relevant covariance matrix.\n\nFocus on how repeated inputs with conflicting observations are handled by the Gaussian process posterior, and how the observation noise variance $\\sigma^2$ resolves the tension between conflicting $y$ values.\n\nTest Suite:\n- Use the three noise variance values $\\sigma^2 \\in \\{10^{-6}, 0.1, 10.0\\}$.\n- For each $\\sigma^2$ in the set above, compute the posterior predictive mean $m_\\star$ and variance $v_\\star$ of the latent function at $x_\\star = 0.0$.\n\nYour program must output a single line containing a comma-separated flat list with the results in the following order:\n- $[m_\\star(\\sigma^2{=}10^{-6}), v_\\star(\\sigma^2{=}10^{-6}), m_\\star(\\sigma^2{=}0.1), v_\\star(\\sigma^2{=}0.1), m_\\star(\\sigma^2{=}10.0), v_\\star(\\sigma^2{=}10.0)]$.\n\nNotes:\n- All quantities are purely mathematical. No physical units are involved.\n- Angles are not involved, so no angle unit is required.\n- The outputs must be real numbers (floating-point values).\n- The implementation should be numerically stable and directly compute the required quantities using the model as stated, without relying on any external data or user input.", "solution": "The problem asks for the posterior predictive mean and variance of a latent function from a one-dimensional Gaussian process (GP) regression model, given a specific dataset that includes repeated inputs with conflicting outputs. The problem will be solved by first deriving the general form of the posterior predictive distribution and then implementing a numerically stable computation using Cholesky factorization for the given test cases.\n\n### Step 1: Model Specification and Joint Distribution\n\nThe model is defined by a zero-mean GP prior on a latent function $f(x)$ and a Gaussian likelihood for the observations.\nThe prior is $f \\sim \\mathcal{GP}(0, k(x, x'))$, where the covariance function is the squared-exponential (or RBF) kernel:\n$$k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - x'}{\\ell}\\right)^2\\right)$$\nThe observations $y_i$ are related to the latent function values $f(x_i)$ through an additive, independent, and identically distributed Gaussian noise model:\n$$y_i = f(x_i) + \\varepsilon_i, \\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n\nGiven a set of $N$ training inputs $X = [x_1, \\dots, x_N]^\\top$ and corresponding outputs $\\mathbf{y} = [y_1, \\dots, y_N]^\\top$, we are interested in predicting the latent function value $f_\\star = f(x_\\star)$ at a new test point $x_\\star$.\n\nAccording to the properties of Gaussian processes, the joint distribution of the training outputs $\\mathbf{y}$ and the test latent value $f_\\star$ is a multivariate Gaussian. Let $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^\\top$ be the vector of latent function values at the training inputs. The joint distribution of $\\mathbf{f}$ and $f_\\star$ under the prior is:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^\\top & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\nwhere:\n- $K = k(X, X)$ is the $N \\times N$ covariance matrix of the training inputs, with entries $K_{ij} = k(x_i, x_j)$.\n- $\\mathbf{k}_\\star = k(X, x_\\star)$ is the $N \\times 1$ vector of covariances between the training inputs and the test input, with entries $(\\mathbf{k}_\\star)_i = k(x_i, x_\\star)$.\n- $k_{\\star\\star} = k(x_\\star, x_\\star)$ is the prior variance at the test input.\n\nThe noisy observations are given by $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$. The covariance between the noisy observations $\\mathbf{y}$ and the latent value $f_\\star$ is $\\text{Cov}(\\mathbf{y}, f_\\star) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}, f_\\star) = \\text{Cov}(\\mathbf{f}, f_\\star) + \\text{Cov}(\\boldsymbol{\\varepsilon}, f_\\star) = \\mathbf{k}_\\star$, since the noise $\\boldsymbol{\\varepsilon}$ is independent of the process $f$. The variance of the observations is $\\text{Var}(\\mathbf{y}) = \\text{Var}(\\mathbf{f} + \\boldsymbol{\\varepsilon}) = \\text{Var}(\\mathbf{f}) + \\text{Var}(\\boldsymbol{\\varepsilon}) = K + \\sigma^2 I$.\n\nTherefore, the joint distribution of $\\mathbf{y}$ and $f_\\star$ is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma^2 I & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^\\top & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\n\n### Step 2: Posterior Predictive Distribution\n\nThe posterior predictive distribution of $f_\\star$ given the data $(X, \\mathbf{y})$ and the test point $x_\\star$ is the conditional distribution $p(f_\\star | \\mathbf{y}, X, x_\\star)$. Using the standard formulas for conditional distributions in a multivariate Gaussian, we find that this distribution is also Gaussian, $f_\\star | \\mathbf{y}, X, x_\\star \\sim \\mathcal{N}(m_\\star, v_\\star)$, with mean $m_\\star$ and variance $v_\\star$ given by:\n$$m_\\star = \\mathbf{k}_\\star^\\top (K + \\sigma^2 I)^{-1} \\mathbf{y}$$\n$$v_\\star = k_{\\star\\star} - \\mathbf{k}_\\star^\\top (K + \\sigma^2 I)^{-1} \\mathbf{k}_\\star$$\n\n### Step 3: Numerically Stable Computation\n\nDirectly computing the inverse $(K + \\sigma^2 I)^{-1}$ is computationally expensive (cubic in $N$) and can be numerically unstable, especially if the matrix $K_y = K + \\sigma^2 I$ is ill-conditioned. A more stable method is to use Cholesky factorization. Since $K_y$ is symmetric and positive-definite (for $\\sigma^2 > 0$), it can be uniquely decomposed as $K_y = L L^\\top$, where $L$ is a lower-triangular matrix.\n\nThe posterior mean $m_\\star$ can be computed by first solving the linear system $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ for the vector $\\boldsymbol{\\alpha} = (K + \\sigma^2 I)^{-1} \\mathbf{y}$, and then calculating $m_\\star = \\mathbf{k}_\\star^\\top \\boldsymbol{\\alpha}$. Using the Cholesky factor $L$, the system $L L^\\top \\boldsymbol{\\alpha} = \\mathbf{y}$ is solved in two steps:\n1.  Solve $L \\mathbf{z} = \\mathbf{y}$ for $\\mathbf{z}$ using forward substitution.\n2.  Solve $L^\\top \\boldsymbol{\\alpha} = \\mathbf{z}$ for $\\boldsymbol{\\alpha}$ using backward substitution.\n\nThe posterior variance $v_\\star$ can be computed similarly. We need to evaluate the quadratic form $\\mathbf{k}_\\star^\\top K_y^{-1} \\mathbf{k}_\\star$. Let $\\mathbf{v} = L^{-1} \\mathbf{k}_\\star$, which can be found by solving the triangular system $L \\mathbf{v} = \\mathbf{k}_\\star$ using forward substitution. Then, the quadratic form is simply $\\mathbf{v}^\\top \\mathbf{v}$:\n$$\\mathbf{k}_\\star^\\top K_y^{-1} \\mathbf{k}_\\star = \\mathbf{k}_\\star^\\top (L L^\\top)^{-1} \\mathbf{k}_\\star = \\mathbf{k}_\\star^\\top (L^\\top)^{-1} L^{-1} \\mathbf{k}_\\star = (L^{-1} \\mathbf{k}_\\star)^\\top (L^{-1} \\mathbf{k}_\\star) = \\mathbf{v}^\\top \\mathbf{v}$$\nThe posterior variance is then:\n$$v_\\star = k_{\\star\\star} - \\mathbf{v}^\\top \\mathbf{v}$$\n\n### Step 4: Application to the Given Problem\n\nThe specific parameters and data are:\n- Hyperparameters: $\\sigma_f^2 = 1.0$, $\\ell = 0.6$.\n- Training inputs: $X = [0.0, 0.0, 1.0]^\\top$.\n- Training outputs: $\\mathbf{y} = [1.0, -1.0, 0.0]^\\top$.\n- Test input: $x_\\star = 0.0$.\n- Noise variances: $\\sigma^2 \\in \\{10^{-6}, 0.1, 10.0\\}$.\n\nFirst, we compute the necessary covariance terms. Let $c = k(0.0, 1.0) = 1.0 \\cdot \\exp\\left(-\\frac{(0-1)^2}{2 \\cdot 0.6^2}\\right) = \\exp\\left(-\\frac{1}{0.72}\\right) \\approx 0.24935$.\n- The $3 \\times 3$ training covariance matrix $K$ is:\n  $$K = \\begin{pmatrix} k(0,0) & k(0,0) & k(0,1) \\\\ k(0,0) & k(0,0) & k(0,1) \\\\ k(1,0) & k(1,0) & k(1,1) \\end{pmatrix} = \\begin{pmatrix} 1.0 & 1.0 & c \\\\ 1.0 & 1.0 & c \\\\ c & c & 1.0 \\end{pmatrix}$$\n- The covariance vector between training and test inputs is:\n  $$\\mathbf{k}_\\star = \\begin{pmatrix} k(0,0) \\\\ k(0,0) \\\\ k(1,0) \\end{pmatrix} = \\begin{pmatrix} 1.0 \\\\ 1.0 \\\\ c \\end{pmatrix}$$\n- The prior variance at the test point is:\n  $$k_{\\star\\star} = k(0,0) = 1.0$$\n\nThe matrix $K_y = K + \\sigma^2 I_3$ is used for the computations. Due to the symmetry of the problem—the duplicated input $x=0.0$ with symmetric outputs $y \\in \\{1.0, -1.0\\}$, the test point also being $x_\\star=0.0$, and the zero-mean prior—the posterior predictive mean $m_\\star$ at $x_\\star=0.0$ is expected to be $0.0$ for all values of $\\sigma^2$.\n\nThe posterior predictive variance $v_\\star$ will depend on $\\sigma^2$:\n- For very small $\\sigma^2$ (e.g., $10^{-6}$), the model is forced to explain the conflicting data $y_1=1.0$ and $y_2=-1.0$ at $x=0.0$ with minimal noise. It concludes with high confidence that the latent function value $f(0.0)$ must be their mean, $0.0$. Thus, the posterior variance $v_\\star$ will be very small.\n- For very large $\\sigma^2$ (e.g., $10.0$), the observations are considered highly noisy and provide little information. The posterior distribution reverts to the prior. The posterior mean $m_\\star$ approaches the prior mean ($0$), and the posterior variance $v_\\star$ approaches the prior variance $k_{\\star\\star} = 1.0$.\n- For intermediate $\\sigma^2$ (e.g., $0.1$), the variance will lie between these two extremes.\n\nThe following Python implementation will compute the precise values for $m_\\star$ and $v_\\star$ for each given $\\sigma^2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Computes the posterior predictive mean and variance for a Gaussian Process\n    regression problem with repeated inputs and conflicting outputs.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [1e-6, 0.1, 10.0]\n\n    # --- Problem Definition ---\n    # GP hyperparameters\n    sigma_f_sq = 1.0\n    ell = 0.6\n\n    # Training data\n    X_train = np.array([[0.0], [0.0], [1.0]])\n    y_train = np.array([1.0, -1.0, 0.0])\n\n    # Test point\n    x_star = np.array([[0.0]])\n    \n    # --- Kernel Function ---\n    def squared_exponential_kernel(a, b, sigma_f_sq_val, ell_val):\n        \"\"\"\n        Computes the squared-exponential covariance matrix between two sets of points.\n        a: (N, D) array of N points\n        b: (M, D) array of M points\n        Returns: (N, M) covariance matrix\n        \"\"\"\n        # For 1D inputs shaped (N, 1) and (M, 1), broadcasting (a - b.T)\n        # creates an (N, M) matrix of differences.\n        sq_dist = (a - b.T)**2\n        return sigma_f_sq_val * np.exp(-0.5 * sq_dist / ell_val**2)\n\n    # --- Pre-compute Covariances ---\n    # Covariance matrix of training inputs\n    K = squared_exponential_kernel(X_train, X_train, sigma_f_sq, ell)\n    \n    # Covariance between training and test points\n    k_star = squared_exponential_kernel(X_train, x_star, sigma_f_sq, ell).flatten()\n    \n    # Covariance of the test point with itself (prior variance)\n    k_star_star = squared_exponential_kernel(x_star, x_star, sigma_f_sq, ell)[0, 0]\n\n    results = []\n    \n    for sigma_sq in test_cases:\n        N = X_train.shape[0]\n        \n        # Build the covariance matrix for noisy observations\n        Ky = K + sigma_sq * np.eye(N)\n        \n        # --- Numerically Stable Computation using Cholesky Factorization ---\n        \n        # 1. Perform Cholesky decomposition: Ky = L * L.T\n        try:\n            L = cholesky(Ky, lower=True)\n        except np.linalg.LinAlgError:\n            # This should not happen for sigma_sq > 0 as Ky will be positive definite.\n            # Handle as an error case if it occurs.\n            results.extend([np.nan, np.nan])\n            continue\n            \n        # --- Posterior Mean Calculation ---\n        # m_star = k_star.T @ inv(Ky) @ y_train\n        # Solve L @ z = y_train for z\n        z = solve_triangular(L, y_train, lower=True, check_finite=False)\n        # Solve L.T @ alpha = z for alpha\n        alpha = solve_triangular(L.T, z, lower=False, check_finite=False)\n        \n        m_star = k_star.T @ alpha\n        \n        # --- Posterior Variance Calculation ---\n        # v_star = k_star_star - k_star.T @ inv(Ky) @ k_star\n        # Solve L @ v = k_star for v\n        v = solve_triangular(L, k_star, lower=True, check_finite=False)\n        \n        v_star = k_star_star - v.T @ v\n        \n        results.append(m_star)\n        results.append(v_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3122972"}, {"introduction": "Building on our understanding of observation noise, we now investigate its global effect on model behavior. This practice provides a rigorous, analytical look at the classic bias-variance trade-off, a fundamental concept in all of statistical learning. You will implement an exercise ([@problem_id:3122966]) to see precisely how the noise parameter $\\sigma_n^2$ acts as a regularization knob, controlling the smoothness and fidelity of the final regression function.", "problem": "You are given a regression setting based on a Gaussian process (GP) prior over an unknown real-valued function. The objective is to isolate and quantify how the observation noise variance controls over-smoothing and drives a bias–variance trade-off in the GP posterior mean. You must implement a complete program that performs this analysis deterministically and produces results in the exact format specified.\n\nAssume a zero-mean Gaussian process (GP) prior on the function $f(\\cdot)$ with covariance function $k(\\cdot,\\cdot)$, and assume independent and identically distributed observation noise. The training observations follow the model $y_i = f(x_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ and the $y_i$ are observed at $n$ training inputs $x_i$. The program must not rely on any randomness in the analysis; instead, it must analytically compute the effect of observation noise variance on the posterior mean’s bias and variance across noise replicates, for a fixed deterministic underlying function $f(\\cdot)$.\n\nBase setup to use:\n- Prior: zero-mean Gaussian process (GP) with squared exponential kernel $k(x,x') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)$.\n- Kernel hyperparameters: amplitude $\\sigma_f^2 = 1$, lengthscale $\\ell = 0.2$.\n- Observation noise variance: $\\sigma_n^2$ (varies across test cases).\n- Jitter for numerical stability: add $\\delta = 10^{-10}$ to the diagonal of covariance matrices where appropriate.\n- True underlying signal: $f(x) = \\sin(2\\pi x)$ on the domain $[0,1]$.\n- Training inputs: $n = 30$ evenly spaced points $x_i$ on $[0,1]$.\n- Test inputs: $m = 200$ evenly spaced points on $[0,1]$.\n\nFundamental base to start from:\n- Definition of a Gaussian process prior with mean function and covariance function.\n- Definition of the observation model $y_i = f(x_i) + \\varepsilon_i$ with independent Gaussian noise.\n- Properties of multivariate normal distributions, linearity of expectation, and covariance transformations under linear maps.\n\nYour tasks:\n1. Starting from the above definitions and properties, derive the posterior mean at the test inputs as a linear function of the training observations, and derive its expectation and variance across noise replicates induced by $\\varepsilon_i$, conditioned on the deterministic true function $f(\\cdot)$. Do not use any Monte Carlo sampling; the program must compute these quantities analytically.\n2. For each test input, compute the squared bias of the posterior mean (the square of the difference between the expected posterior mean and the true value $f(x)$ at that input), and compute the variance of the posterior mean across noise replicates. Then aggregate by averaging across the $m$ test inputs to produce two scalars: the average squared bias and the average variance for the given $\\sigma_n^2$.\n3. Provide results for a test suite of observation noise variances designed to probe low-noise and high-noise regimes:\n   - Case $1$: $\\sigma_n^2 = 10^{-6}$ (near-noiseless boundary condition).\n   - Case $2$: $\\sigma_n^2 = 10^{-3}$ (low-noise regime).\n   - Case $3$: $\\sigma_n^2 = 10^{-1}$ (moderate-noise regime).\n   - Case $4$: $\\sigma_n^2 = 1$ (high-noise regime).\n4. For each case, compute and return the ordered pair $\\left[\\overline{\\text{bias}^2}, \\overline{\\text{var}}\\right]$, where $\\overline{\\text{bias}^2}$ is the average over the $m$ test inputs of the squared bias of the posterior mean, and $\\overline{\\text{var}}$ is the average over the $m$ test inputs of the variance of the posterior mean induced by the observation noise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all four cases as a comma-separated list enclosed in square brackets and with each result represented as a two-element list. For example, the output should look like:\n  - \"[[b1,v1],[b2,v2],[b3,v3],[b4,v4]]\"\n- Each floating-point number must be rounded to $6$ decimal places in the output string.\n\nNotes:\n- The program must be fully deterministic and must not use any random sampling to estimate bias or variance.\n- No physical units or angle units are involved.\n- The final outputs for the test suite must be computable as floats and assembled into a single list line as specified.", "solution": "The problem requires an analytical evaluation of the bias-variance tradeoff in Gaussian process (GP) regression, specifically focusing on how the observation noise variance, $\\sigma_n^2$, influences the posterior mean function. We will derive the necessary expressions from first principles and then outline the computational steps to obtain the required results.\n\nA Gaussian process defines a prior distribution over functions. We are given a zero-mean GP prior, $f(\\cdot) \\sim \\mathcal{GP}(0, k(x, x'))$, where $k(x, x')$ is the squared exponential covariance function:\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)$$\nwith hyperparameters $\\sigma_f^2 = 1$ and $\\ell = 0.2$.\n\nThe training data consists of $n=30$ pairs $(\\mathbf{x}, \\mathbf{y})$, where $\\mathbf{x} = \\{x_i\\}_{i=1}^n$ are the input locations and $\\mathbf{y} = \\{y_i\\}_{i=1}^n$ are the noisy observations. The observation model is:\n$$y_i = f(x_i) + \\varepsilon_i, \\quad \\text{where} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$$\nThe noise terms $\\varepsilon_i$ are independent and identically distributed. In vector form, this is $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{f}$ is the vector of function values at training inputs, $\\mathbf{f} = [f(x_1), \\dots, f(x_n)]^T$, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I_n)$, with $I_n$ being the $n \\times n$ identity matrix.\n\nThe core of GP regression is to compute the posterior distribution of the function values $\\mathbf{f}_*$ at a set of $m=200$ test inputs $\\mathbf{x}_* = \\{x_j^*\\}_{j=1}^m$. The joint distribution of the training observations $\\mathbf{y}$ and the test function values $\\mathbf{f}_*$ under the GP prior is a multivariate Gaussian:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK(\\mathbf{x}, \\mathbf{x}) + \\sigma_n^2 I_n & K(\\mathbf{x}, \\mathbf{x}_*) \\\\\nK(\\mathbf{x}_*, \\mathbf{x}) & K(\\mathbf{x}_*, \\mathbf{x}_*)\n\\end{pmatrix}\n\\right)\n$$\nwhere $K(\\mathbf{A}, \\mathbf{B})$ denotes the matrix of covariances with entries $k(a_i, b_j)$. Using the standard formulas for conditional Gaussian distributions, the posterior distribution $p(\\mathbf{f}_*|\\mathbf{y}, \\mathbf{x}, \\mathbf{x}_*)$ is also Gaussian, with a posterior mean $\\bar{\\mathbf{f}}_*$ given by:\n$$ \\bar{\\mathbf{f}}_* = \\mathbb{E}[\\mathbf{f}_* | \\mathbf{y}] = K(\\mathbf{x}_*, \\mathbf{x}) [K(\\mathbf{x}, \\mathbf{x}) + \\sigma_n^2 I_n]^{-1} \\mathbf{y} $$\nFor numerical stability, we add a small jitter term $\\delta = 10^{-10}$ to the diagonal of $K(\\mathbf{x}, \\mathbf{x})$, effectively using the matrix $[K(\\mathbf{x}, \\mathbf{x}) + (\\sigma_n^2 + \\delta)I_n]$ for inversion. Let us define the linear operator $L = K(\\mathbf{x}_*, \\mathbf{x}) [K(\\mathbf{x}, \\mathbf{x}) + (\\sigma_n^2 + \\delta)I_n]^{-1}$, which is an $m\\times n$ matrix. The posterior mean can then be written compactly as a linear transformation of the observations:\n$$ \\bar{\\mathbf{f}}_* = L \\mathbf{y} $$\n\nThe problem requires us to analyze the bias and variance of this posterior mean estimator, $\\bar{\\mathbf{f}}_*$, across different realizations of the observation noise $\\boldsymbol{\\varepsilon}$. Crucially, this analysis is conditioned on a fixed, deterministic true function $f(x) = \\sin(2\\pi x)$. Therefore, the vector of true function values at the training points, $\\mathbf{f}_{\\text{true}} = [\\sin(2\\pi x_1), \\dots, \\sin(2\\pi x_n)]^T$, is a constant vector, not a random variable. The only source of randomness is the noise $\\boldsymbol{\\varepsilon}$. The observations are $\\mathbf{y} = \\mathbf{f}_{\\text{true}} + \\boldsymbol{\\varepsilon}$.\n\nFirst, we derive the expectation of the posterior mean estimator with respect to the noise distribution:\n$$ \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] = \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[L \\mathbf{y}] = L \\, \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\mathbf{f}_{\\text{true}} + \\boldsymbol{\\varepsilon}] $$\nBy linearity of expectation and since $\\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$:\n$$ \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] = L \\mathbf{f}_{\\text{true}} = K(\\mathbf{x}_*, \\mathbf{x}) [K(\\mathbf{x}, \\mathbf{x}) + (\\sigma_n^2 + \\delta)I_n]^{-1} \\mathbf{f}_{\\text{true}} $$\nThe bias of the estimator at the test points is the difference between this expected value and the true function values at the test points, $\\mathbf{f}_{*\\text{true}} = [\\sin(2\\pi x_1^*), \\dots, \\sin(2\\pi x_m^*)]^T$. The squared bias for each test point $j$ is $(\\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{f}_*(x_j^*)] - f(x_j^*))^2$. The average squared bias, $\\overline{\\text{bias}^2}$, is the mean of these values over all $m$ test points:\n$$ \\overline{\\text{bias}^2} = \\frac{1}{m} \\sum_{j=1}^{m} \\left( (\\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*])_j - (\\mathbf{f}_{*\\text{true}})_j \\right)^2 = \\frac{1}{m} \\| \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] - \\mathbf{f}_{*\\text{true}} \\|_2^2 $$\n\nSecond, we derive the variance of the posterior mean estimator. The covariance matrix of $\\bar{\\mathbf{f}}_*$ with respect to the noise is:\n$$ \\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] = \\text{Cov}_{\\boldsymbol{\\varepsilon}}[L \\mathbf{y}] = L \\, \\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\mathbf{y}] \\, L^T $$\nSince $\\mathbf{f}_{\\text{true}}$ is constant, $\\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\mathbf{y}] = \\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\mathbf{f}_{\\text{true}} + \\boldsymbol{\\varepsilon}] = \\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\boldsymbol{\\varepsilon}] = \\sigma_n^2 I_n$. Substituting this, we get:\n$$ \\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] = L (\\sigma_n^2 I_n) L^T = \\sigma_n^2 L L^T $$\nThe variance of the estimator at each test point $j$, $\\text{Var}_{\\boldsymbol{\\varepsilon}}[\\bar{f}_*(x_j^*)]$, is the $j$-th diagonal element of this covariance matrix. The average variance, $\\overline{\\text{var}}$, is the mean of these diagonal elements:\n$$ \\overline{\\text{var}} = \\frac{1}{m} \\sum_{j=1}^{m} (\\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*])_{jj} = \\frac{1}{m} \\text{Tr}(\\text{Cov}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*]) $$\n\nAs $\\sigma_n^2$ increases, the term $[K(\\mathbf{x}, \\mathbf{x}) + (\\sigma_n^2 + \\delta)I_n]^{-1}$ approaches zero. This causes $\\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*]$ to shrink towards the prior mean of zero, increasing the bias (underfitting or over-smoothing). Simultaneously, the variance $\\overline{\\text{var}}$ also tends to zero, as the estimator becomes insensitive to the noise realization. Conversely, for very small $\\sigma_n^2$, the model attempts to fit the training data very closely, leading to a smaller bias (limited by the model's expressive power) and a small variance (as the magnitude of noise fluctuations is small). The trade-off is most apparent for intermediate noise levels.\n\nThe implementation will proceed by:\n1. Defining the domain, the true function $f(x)$, and the kernel function $k(x, x')$.\n2. Generating the $n=30$ training inputs $\\mathbf{x}$ and $m=200$ test inputs $\\mathbf{x}_*$.\n3. Calculating the true function values $\\mathbf{f}_{\\text{true}}$ and $\\mathbf{f}_{*\\text{true}}$.\n4. For each specified value of $\\sigma_n^2$:\n    a. Construct the matrices $K(\\mathbf{x}, \\mathbf{x})$ and $K(\\mathbf{x}_*, \\mathbf{x})$.\n    b. Compute the matrix $L$ using a stable linear solve instead of explicit matrix inversion.\n    c. Calculate $\\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] = L \\mathbf{f}_{\\text{true}}$.\n    d. Compute $\\overline{\\text{bias}^2} = \\frac{1}{m} \\|\\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\bar{\\mathbf{f}}_*] - \\mathbf{f}_{*\\text{true}}\\|_2^2$.\n    e. Calculate the covariance matrix $\\sigma_n^2 L L^T$.\n    f. Compute $\\overline{\\text{var}}$ by averaging the diagonal elements of the covariance matrix.\n5. Store the pair $[\\overline{\\text{bias}^2}, \\overline{\\text{var}}]$ for each case and format the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the bias-variance tradeoff in Gaussian Process regression\n    by analytically computing the average squared bias and average variance\n    of the posterior mean estimator across a range of observation noise levels.\n    \"\"\"\n\n    # Base setup parameters\n    sigma_f2 = 1.0\n    length_scale = 0.2\n    jitter = 1e-10\n    n_train = 30\n    n_test = 200\n\n    # Define the squared exponential kernel\n    def squared_exp_kernel(x1, x2, sf2, l):\n        \"\"\"\n        Computes the squared exponential kernel matrix between two sets of points.\n        x1: (N, D) array\n        x2: (M, D) array\n        \"\"\"\n        # Pairwise squared Euclidean distances\n        sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n        return sf2 * np.exp(-0.5 / l**2 * sqdist)\n\n    # Define the true underlying signal\n    def true_func(x):\n        return np.sin(2 * np.pi * x)\n\n    # Generate training and test inputs\n    x_train = np.linspace(0, 1, n_train).reshape(-1, 1)\n    x_test = np.linspace(0, 1, n_test).reshape(-1, 1)\n\n    # Compute true function values at inputs\n    f_train_true = true_func(x_train).flatten()\n    f_test_true = true_func(x_test).flatten()\n\n    # Test cases for observation noise variance\n    test_cases_sigma_n2 = [1e-6, 1e-3, 1e-1, 1.0]\n\n    # Pre-compute covariance matrices that do not depend on observation noise\n    K_train_train = squared_exp_kernel(x_train, x_train, sigma_f2, length_scale)\n    K_test_train = squared_exp_kernel(x_test, x_train, sigma_f2, length_scale)\n\n    results = []\n    for sigma_n2 in test_cases_sigma_n2:\n        # 1. Compute the linear operator L = K(x*,x) * [K(x,x) + sigma_n^2*I]^-1\n        # Invert K_y = K(x,x) + (sigma_n^2 + jitter)*I\n        K_y = K_train_train + (sigma_n2 + jitter) * np.eye(n_train)\n        \n        # Solve L * K_y = K_test_train for L using a numerically stable linear solve.\n        # This is equivalent to solving K_y.T * L.T = K_test_train.T\n        # Since K_y is symmetric, we solve K_y * L.T = K_test_train.T\n        L_T = np.linalg.solve(K_y, K_test_train.T)\n        L = L_T.T\n\n        # 2. Compute the expectation of the posterior mean across noise replicates\n        expected_posterior_mean = L @ f_train_true\n\n        # 3. Compute the average squared bias\n        # Bias is the difference between the expected posterior mean and the true function\n        squared_bias = (expected_posterior_mean - f_test_true)**2\n        avg_squared_bias = np.mean(squared_bias)\n\n        # 4. Compute the variance of the posterior mean across noise replicates\n        # Covariance matrix of posterior mean is sigma_n^2 * L * L^T\n        posterior_mean_cov = sigma_n2 * (L @ L.T)\n        \n        # The variance at each test point is the diagonal of the covariance matrix\n        posterior_mean_variances = np.diag(posterior_mean_cov)\n        avg_variance = np.mean(posterior_mean_variances)\n\n        results.append([avg_squared_bias, avg_variance])\n\n    # Format the final output string as specified\n    output_parts = [f\"[{b:.6f},{v:.6f}]\" for b, v in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3122966"}, {"introduction": "Gaussian Processes truly shine when we incorporate domain-specific knowledge directly into the model via the covariance kernel. This exercise demonstrates this power by tackling a common challenge: predicting a periodic signal across large gaps where no data is available. By implementing a model with a periodic kernel ([@problem_id:3122875]), you will see how the GP can make sensible, long-range interpolations that respect the underlying structure of the data, a feat that is difficult for many other regression models.", "problem": "You are to implement one-dimensional Gaussian process regression to study interpolation across missing segments using a periodic covariance. Begin from the following principles and definitions only, and do not assume any closed-form expressions for posterior quantities until you derive them from these bases:\n- A Gaussian process (GP) is a collection of random variables such that any finite subcollection is jointly Gaussian. A zero-mean GP prior over an unknown function $f$ is written $f \\sim \\mathcal{GP}(0, k)$, where $k$ is a positive semidefinite covariance function.\n- Observations are noisy: given inputs $x_1,\\dots,x_n$, the observation model is $y_i = f(x_i) + \\epsilon_i$ with independent noise $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$.\n- The conditioning rule for a joint multivariate normal distribution gives the mean and covariance of any subset conditioned on another subset.\n\nYou will use a periodic covariance to encode prior knowledge that $f$ repeats every unit interval. Use the standard periodic covariance (also called the exponential sine-squared kernel):\n$$\nk(x,x') \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\frac{2\\,\\sin^2\\!\\big(\\pi\\,|x-x'|/p\\big)}{\\ell^2}\\right),\n$$\nwith amplitude $\\sigma_f$, length-scale $\\ell$, and period $p$.\n\nData-generating process:\n- True function: $f(x) = \\sin(2\\pi x) + 0.5\\sin(4\\pi x)$, where the sine function input is in radians.\n- Domain: $x \\in [0,1]$, matching one full period $p = 1$.\n- Training inputs: start from a uniform grid of $N_{\\text{train,all}} = 200$ points on $[0,1]$ inclusive. Remove any points that fall inside specified missing segments (gaps), yielding the final training set.\n- Observations: $y_i = f(x_i) + \\epsilon_i$, with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$, generated using a pseudorandom number generator seeded with $0$.\n- Test inputs: a uniform grid of $N_{\\text{test}} = 600$ points on $[0,1]$ inclusive.\n\nGaussian process prior and likelihood:\n- Use zero mean, the periodic kernel above, and additive independent Gaussian noise with variance $\\sigma_n^2$.\n\nTasks:\n1. From the definitions above, derive and implement the Gaussian process posterior predictive mean and standard deviation at the test inputs by conditioning a joint Gaussian prior over the training and test function values under the observation model with noise variance $\\sigma_n^2$.\n2. For each test input $x_\\ast$, compute the posterior predictive mean $m_\\ast$ and posterior predictive standard deviation $s_\\ast$.\n3. Define the union of gaps $\\mathcal{G} \\subset [0,1]$ from the provided intervals. Let $\\mathcal{G}^c$ denote its complement in $[0,1]$. On the test grid:\n   - Compute the mean absolute error inside the gaps,\n     $$\n     \\mathrm{MAE}_{\\text{gap}} = \\frac{1}{|\\{x_\\ast \\in \\mathcal{G}\\}|}\\sum_{x_\\ast \\in \\mathcal{G}} \\left| m_\\ast - f(x_\\ast) \\right|.\n     $$\n   - Compute the uncertainty inflation factor,\n     $$\n     R = \\frac{\\text{average of } s_\\ast \\text{ over } x_\\ast \\in \\mathcal{G}}{\\text{average of } s_\\ast \\text{ over } x_\\ast \\in \\mathcal{G}^c}.\n     $$\n\nKernel hyperparameters:\n- Use $\\sigma_f = 1.0$, $\\ell = 0.2$, and $p=1.0$.\n\nNumerical and implementation details:\n- Use a small positive diagonal jitter (e.g., $\\epsilon_{\\text{jitter}} = 10^{-10}$) added to the training covariance matrix for numerical stability.\n- Use the exact training noise variance $\\sigma_n^2$ both to corrupt the training outputs and in the regression model.\n- All computations must be deterministic by seeding the pseudorandom number generator with seed $0$.\n\nTest suite:\nCompute $\\mathrm{MAE}_{\\text{gap}}$ and $R$ for each of the following parameter sets, where each case specifies the gap set $\\mathcal{G}$ as a union of closed intervals and the noise standard deviation $\\sigma_n$:\n- Case $1$ (happy path): gaps $\\mathcal{G} = [0.35, 0.55]$, noise $\\sigma_n = 0.05$.\n- Case $2$ (narrow gap): gaps $\\mathcal{G} = [0.49, 0.51]$, noise $\\sigma_n = 0.05$.\n- Case $3$ (large gap, higher noise): gaps $\\mathcal{G} = [0.25, 0.70]$, noise $\\sigma_n = 0.10$.\n- Case $4$ (boundary gaps): gaps $\\mathcal{G} = [0.00, 0.10] \\cup [0.90, 1.00]$, noise $\\sigma_n = 0.05$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case in the order above, output two floating-point numbers rounded to $6$ decimal places: first $R$ and then $\\mathrm{MAE}_{\\text{gap}}$.\n- Therefore, the final single line must contain $8$ numbers in this order:\n$$\n[R_1, \\mathrm{MAE}_{\\text{gap},1}, R_2, \\mathrm{MAE}_{\\text{gap},2}, R_3, \\mathrm{MAE}_{\\text{gap},3}, R_4, \\mathrm{MAE}_{\\text{gap},4}].\n$$", "solution": "The problem requires the implementation of one-dimensional Gaussian process (GP) regression with a periodic kernel to predict a function's values across specified data gaps. The solution involves two main stages: first, the derivation of the posterior predictive equations from fundamental principles, and second, the implementation of these equations to compute the required metrics for a given set of test cases.\n\n### Derivation of the GP Posterior Predictive Distribution\n\nLet the set of $n$ training inputs be denoted by the matrix $X$ and the corresponding noisy observations by the vector $\\mathbf{y}$. Let the set of $m$ test inputs be denoted by $X_\\ast$. We seek the posterior predictive distribution $p(\\mathbf{f}_\\ast | X, \\mathbf{y}, X_\\ast)$, where $\\mathbf{f}_\\ast$ is the vector of latent function values at the test inputs $X_\\ast$.\n\n1.  **Prior Distribution**: A zero-mean Gaussian process prior is placed on the function $f$, written as $f \\sim \\mathcal{GP}(0, k)$. This implies that the latent function values at any collection of points follow a multivariate Gaussian distribution. For the combined set of training and test points, the joint prior over their function values, $\\mathbf{f}$ (at $X$) and $\\mathbf{f}_\\ast$ (at $X_\\ast$), is:\n    $$\n    \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{f}_\\ast \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} K(X, X) & K(X, X_\\ast) \\\\ K(X_\\ast, X) & K(X_\\ast, X_\\ast) \\end{pmatrix} \\right)\n    $$\n    Here, $K(A, B)$ denotes the covariance matrix computed by applying the kernel function $k(x, x')$ to each pair of points from sets $A$ and $B$. Specifically, $K(X, X) \\in \\mathbb{R}^{n \\times n}$, $K(X, X_\\ast) \\in \\mathbb{R}^{n \\times m}$, $K(X_\\ast, X) \\in \\mathbb{R}^{m \\times n}$, and $K(X_\\ast, X_\\ast) \\in \\mathbb{R}^{m \\times m}$.\n\n2.  **Likelihood**: The observation model is $y_i = f(x_i) + \\epsilon_i$, where the noise terms are independent and identically distributed as $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. In vector form, $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, with $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$, where $I$ is the $n \\times n$ identity matrix. The likelihood is $p(\\mathbf{y} | \\mathbf{f}) = \\mathcal{N}(\\mathbf{y} | \\mathbf{f}, \\sigma_n^2 I)$.\n\n3.  **Joint Distribution of Observables and Test Values**: To find the posterior predictive distribution, we first need the joint distribution of the observed data $\\mathbf{y}$ and the test function values $\\mathbf{f}_\\ast$. Since $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$ are independent zero-mean Gaussian random vectors, their sum $\\mathbf{y}$ is also a zero-mean Gaussian vector. The full joint distribution of $(\\mathbf{y}, \\mathbf{f}_\\ast)$ is therefore a multivariate Gaussian whose parameters are determined as follows:\n    -   **Mean**: $E[\\mathbf{y}] = E[\\mathbf{f} + \\boldsymbol{\\epsilon}] = E[\\mathbf{f}] + E[\\boldsymbol{\\epsilon}] = \\mathbf{0}$. The mean of $\\mathbf{f}_\\ast$ is also $\\mathbf{0}$.\n    -   **Covariance**: The block-covariance matrix is computed as:\n        -   $\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}) = \\text{Cov}(\\mathbf{f}) + \\text{Cov}(\\boldsymbol{\\epsilon}) = K(X, X) + \\sigma_n^2 I$. The cross-covariance terms are zero due to the independence of $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$.\n        -   $\\text{Cov}(\\mathbf{f}_\\ast, \\mathbf{f}_\\ast) = K(X_\\ast, X_\\ast)$.\n        -   $\\text{Cov}(\\mathbf{y}, \\mathbf{f}_\\ast) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}, \\mathbf{f}_\\ast) = \\text{Cov}(\\mathbf{f}, \\mathbf{f}_\\ast) = K(X, X_\\ast)$.\n    Thus, the joint distribution is:\n    $$\n    \\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_\\ast \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, X_\\ast) \\\\ K(X_\\ast, X) & K(X_\\ast, X_\\ast) \\end{pmatrix} \\right)\n    $$\n\n4.  **Conditioning to Find the Posterior**: We use the standard formulas for conditional Gaussian distributions. If $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$, then $p(\\mathbf{b}|\\mathbf{a}) = \\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$ with:\n    $$\n    \\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a) \\quad \\text{and} \\quad \\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}\n    $$\n    By making the substitutions $\\mathbf{a} \\leftrightarrow \\mathbf{y}$ and $\\mathbf{b} \\leftrightarrow \\mathbf{f}_\\ast$, we obtain the posterior predictive distribution $p(\\mathbf{f}_\\ast | X, \\mathbf{y}, X_\\ast) = \\mathcal{N}(\\mathbf{m}_\\ast, \\Sigma_\\ast)$, where:\n    -   **Posterior Predictive Mean** $\\mathbf{m}_\\ast$:\n        $$\n        \\mathbf{m}_\\ast = K(X_\\ast, X) (K(X, X) + \\sigma_n^2 I)^{-1} \\mathbf{y}\n        $$\n    -   **Posterior Predictive Covariance** $\\Sigma_\\ast$:\n        $$\n        \\Sigma_\\ast = K(X_\\ast, X_\\ast) - K(X_\\ast, X) (K(X, X) + \\sigma_n^2 I)^{-1} K(X, X_\\ast)\n        $$\n\n5.  **Implementation Quantities**: The posterior predictive mean for each test point $x_{\\ast, i}$ is the $i$-th element of $\\mathbf{m}_\\ast$. The posterior predictive standard deviation for each test point, $s_{\\ast,i}$, is the square root of the $i$-th diagonal element of $\\Sigma_\\ast$. For numerical stability, the matrix inversion is avoided. Let $K_y = K(X, X) + \\sigma_n^2 I$. We solve the linear system $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ for $\\boldsymbol{\\alpha}$, yielding $\\mathbf{m}_\\ast = K(X_\\ast, X) \\boldsymbol{\\alpha}$. Similarly, to compute the posterior variances, we solve $K_y V = K(X, X_\\ast)$ for the matrix $V$. The posterior covariance becomes $\\Sigma_\\ast = K(X_\\ast, X_\\ast) - K(X_\\ast, X) V$.\n\n### Computational Procedure\n\nFor each test case:\n1.  **Data Preparation**: The initial set of $N_{\\text{train,all}} = 200$ training inputs on $[0,1]$ is filtered to exclude points within the specified gaps $\\mathcal{G}$. The observed outputs $\\mathbf{y}$ are generated by evaluating $f(x) = \\sin(2\\pi x) + 0.5\\sin(4\\pi x)$ and adding Gaussian noise with standard deviation $\\sigma_n$. The noise generation is made deterministic by seeding the pseudorandom number generator with 0 for each test case. The test inputs are a fixed grid of $N_{\\text{test}} = 600$ points on $[0,1]$.\n2.  **Covariance Matrix Construction**: The periodic kernel $k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{2\\,\\sin^2(\\pi\\,|x-x'|/p)}{\\ell^2}\\right)$ with $\\sigma_f=1.0$, $\\ell=0.2$, and $p=1.0$ is used to build the covariance matrices.\n3.  **Posterior Calculation**: The noisy training covariance matrix $K_y = K(X, X) + \\sigma_n^2 I$ has a jitter term $\\epsilon_{\\text{jitter}} = 10^{-10}$ added to its diagonal for numerical stability. A Cholesky decomposition of $K_y$ provides an efficient and stable method to solve the linear systems for the posterior mean $\\mathbf{m}_\\ast$ and the diagonal of the posterior covariance $\\Sigma_\\ast$.\n4.  **Metric Evaluation**: The test points $X_\\ast$ are partitioned based on whether they fall inside the gaps ($\\mathcal{G}$) or outside ($\\mathcal{G}^c$). The metrics $\\mathrm{MAE}_{\\text{gap}}$ and $R$ are then computed according to their definitions.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef solve():\n    \"\"\"\n    Implements one-dimensional Gaussian process regression and evaluates its performance\n    on interpolating data gaps for several test cases.\n    \"\"\"\n    # Define problem constants\n    N_TRAIN_ALL = 200\n    N_TEST = 600\n    SIGMA_F = 1.0\n    L_SCALE = 0.2\n    PERIOD = 1.0\n    JITTER = 1e-10\n\n    # Test suite parameters\n    test_cases = [\n        {\"gaps\": [(0.35, 0.55)], \"sigma_n\": 0.05},\n        {\"gaps\": [(0.49, 0.51)], \"sigma_n\": 0.05},\n        {\"gaps\": [(0.25, 0.70)], \"sigma_n\": 0.10},\n        {\"gaps\": [(0.00, 0.10), (0.90, 1.00)], \"sigma_n\": 0.05},\n    ]\n\n    # Global data grids\n    x_train_all = np.linspace(0.0, 1.0, N_TRAIN_ALL)\n    x_test = np.linspace(0.0, 1.0, N_TEST)\n\n    # True function\n    def true_function(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.sin(4 * np.pi * x)\n\n    # Periodic kernel function\n    def periodic_kernel(a, b, sigma_f, l_scale, p):\n        dist = np.abs(a[:, None] - b[None, :])\n        sin_sq_term = np.sin(np.pi * dist / p)**2\n        return sigma_f**2 * np.exp(-2 * sin_sq_term / l_scale**2)\n\n    results = []\n    \n    for i, case in enumerate(test_cases):\n        sigma_n = case[\"sigma_n\"]\n        gaps = case[\"gaps\"]\n        \n        # Using a fixed seed of 0 for each case for deterministic results as requested\n        # by \"generated using a pseudorandom number generator seeded with 0\".\n        # This is interpreted as resetting the generator to the same state for each case's noise generation.\n        rng = np.random.default_rng(0) \n\n        # 1. Data Preparation\n        # Filter training points to exclude those in gaps\n        train_mask = np.ones_like(x_train_all, dtype=bool)\n        for g_min, g_max in gaps:\n            train_mask = (x_train_all  g_min) | (x_train_all > g_max)\n        x_train = x_train_all[train_mask]\n        \n        # Generate noisy observations\n        y_true_train = true_function(x_train)\n        noise = rng.normal(0, sigma_n, size=x_train.shape)\n        y_train = y_true_train + noise\n\n        # 2. Covariance Matrix Construction\n        K_train_train = periodic_kernel(x_train, x_train, SIGMA_F, L_SCALE, PERIOD)\n        K_test_train = periodic_kernel(x_test, x_train, SIGMA_F, L_SCALE, PERIOD)\n        K_test_test = periodic_kernel(x_test, x_test, SIGMA_F, L_SCALE, PERIOD)\n        \n        # 3. Posterior Calculation\n        K_y = K_train_train + (sigma_n**2) * np.eye(len(x_train))\n        K_y[np.diag_indices_from(K_y)] += JITTER\n\n        try:\n            # Use Cholesky decomposition for stable and efficient solution\n            # Note: scipy.linalg.cho_factor returns (U, False) for upper triangular factor\n            # so L is actually U here. The variable name is kept for consistency with theory texts.\n            L, lower = cho_factor(K_y, lower=False)\n            \n            # Solve for alpha = K_y^-1 * y_train\n            alpha = cho_solve((L, lower), y_train)\n            \n            # Calculate posterior mean\n            y_pred_mean = K_test_train @ alpha\n            \n            # Calculate posterior variance\n            # v = K_y^-1 * K_train_test\n            v = cho_solve((L, lower), K_test_train.T)\n            post_var = np.diag(K_test_test) - np.sum(K_test_train * v.T, axis=1)\n            # Ensure variance is non-negative due to potential numerical errors\n            post_var = np.maximum(post_var, 0)\n            y_pred_std = np.sqrt(post_var)\n\n        except np.linalg.LinAlgError:\n            print(f\"Cholesky decomposition failed for case {i+1}. Matrix may not be positive definite.\")\n            results.extend([float('nan'), float('nan')])\n            continue\n\n        # 4. Metric Evaluation\n        test_gap_mask = np.zeros_like(x_test, dtype=bool)\n        for g_min, g_max in gaps:\n            test_gap_mask |= (x_test >= g_min)  (x_test = g_max)\n            \n        y_true_test = true_function(x_test)\n\n        # MAE in gap\n        if np.any(test_gap_mask):\n            mae_gap = np.mean(np.abs(y_pred_mean[test_gap_mask] - y_true_test[test_gap_mask]))\n        else:\n            mae_gap = 0.0\n\n\n        # Uncertainty inflation factor R\n        s_star_gap = y_pred_std[test_gap_mask]\n        s_star_complement = y_pred_std[~test_gap_mask]\n\n        if len(s_star_gap) == 0 or len(s_star_complement) == 0:\n            R = 1.0 if len(s_star_gap) == 0 else float('inf')\n        else:\n            avg_s_gap = np.mean(s_star_gap)\n            avg_s_complement = np.mean(s_star_complement)\n            R = avg_s_gap / avg_s_complement if avg_s_complement > 0 else float('inf')\n        \n        results.extend([round(R, 6), round(mae_gap, 6)])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3122875"}]}