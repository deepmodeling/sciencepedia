{"hands_on_practices": [{"introduction": "Before building a full sampler, it is crucial to master the mechanics of a single Gibbs iteration. This exercise focuses on the core operation of the algorithm: drawing a new value for each variable from its distribution conditioned on the current state of the others. By manually performing one update step using the inverse transform sampling method [@problem_id:1920320], you will gain a concrete understanding of how the sampler moves from one state to the next, cementing the fundamental principles of conditional sampling.", "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.", "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.7368<0.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$", "id": "1920320"}, {"introduction": "A functioning Gibbs sampler must be able to explore the entire parameter space of the target distribution, a property known as ergodicity. This practice demonstrates a classic failure mode where a standard Gibbs sampler becomes trapped in a single mode of a multimodal distribution due to the geometry of the parameter space. You will implement both a failing sampler and a successful one that uses reparameterization—a clever change of variables—to resolve the issue, learning a vital lesson in MCMC diagnostics and problem-solving [@problem_id:3125089].", "problem": "Consider the following target distribution over a two-dimensional vector $X = (X_1, X_2) \\in \\mathbb{R}^2$. Let the support be the union of two disjoint axis-aligned rectangles with equal weight. Define\n$$\n\\mathcal{A} = \\{(x_1, x_2): x_1 \\in [-3,-1],\\ x_2 \\in [-3,-1]\\},\n\\quad\n\\mathcal{B} = \\{(x_1, x_2): x_1 \\in [1,3],\\ x_2 \\in [1,3]\\},\n$$\nand the target density\n$$\n\\pi(x_1,x_2) \\propto \\mathbf{1}_{\\mathcal{A}}(x_1,x_2) + \\mathbf{1}_{\\mathcal{B}}(x_1,x_2),\n$$\nwhere $\\mathbf{1}_{S}$ is the indicator function of the set $S$. The two components $\\mathcal{A}$ and $\\mathcal{B}$ are non-overlapping and separated by a zero-probability region.\n\nStart from the core definitions: A Gibbs sampler is a Markov chain that iteratively updates coordinates by sampling from full conditional distributions. A Markov chain is irreducible and ergodic if it can visit any state in the support with positive probability and possesses a unique stationary distribution which it converges to from any starting point. For this target, the full conditionals in the original $(x_1, x_2)$ coordinates are:\n- If $x_2 \\in [-3,-1]$, then $X_1 \\mid X_2 = x_2 \\sim \\text{Uniform}([-3,-1])$.\n- If $x_2 \\in [1,3]$, then $X_1 \\mid X_2 = x_2 \\sim \\text{Uniform}([1,3])$.\n- Analogously for $X_2 \\mid X_1$.\n\nBy inspection, if the chain starts in $\\mathcal{A}$, the conditionals enforce that it remains in $\\mathcal{A}$ forever; likewise for $\\mathcal{B}$. This violates irreducibility and demonstrates a failure mode of the Gibbs sampler in a non-ergodic, disconnected-support setup.\n\nA remedy is to reparameterize the state using a rotation that aligns the disconnection with a single coordinate. Define a bijective linear map to new coordinates $(U,V)$ by\n$$\nU = X_1 - X_2,\\quad V = X_1 + X_2,\n$$\nwith inverse\n$$\nX_1 = \\frac{U+V}{2},\\quad X_2 = \\frac{V-U}{2}.\n$$\nUnder this map, the support becomes two disjoint horizontal bands in $(u,v)$-space:\n$$\n\\mathcal{A}_{uv} = \\{(u,v): u \\in [-2,2],\\ v \\in [-6,-2]\\},\\quad\n\\mathcal{B}_{uv} = \\{(u,v): u \\in [-2,2],\\ v \\in [2,6]\\}.\n$$\nThe full conditionals in $(u,v)$ have the following forms:\n- For any $v \\in [-6,-2] \\cup [2,6]$, we have $U \\mid V=v \\sim \\text{Uniform}([-2,2])$.\n- For any $u \\in [-2,2]$, we have $V \\mid U=u$ supported on the union $[-6,-2] \\cup [2,6]$, with equal weights on the two intervals (because the target assigns equal mass to bands of equal length). Sampling $V$ alternates between these two disjoint intervals, enabling movement across the disconnected components in the original space after mapping back.\n\nYour task is to implement and compare two Gibbs samplers:\n\n1. Original-coordinate Gibbs sampler in $(x_1,x_2)$.\n   - At each iteration, update $X_1$ from its full conditional given the current $X_2$.\n   - Then update $X_2$ from its full conditional given the updated $X_1$.\n   - Because of the disconnected support, this sampler is non-ergodic and will remain stuck in whichever component the initialization is in.\n\n2. Reparameterized Gibbs sampler in $(u,v)$ using the rotation above.\n   - At each iteration, update $U \\mid V$ by sampling uniformly on $[-2,2]$.\n   - Then update $V \\mid U$ by first choosing one of the two intervals $[-6,-2]$ or $[2,6]$ with equal probability and then sampling uniformly within the chosen interval.\n   - Map samples back to $(x_1,x_2)$ via $x_1 = (u+v)/2$, $x_2 = (v-u)/2$ only for evaluation.\n\nDiagnostic and measurable quantity. For a given run, compute the empirical fraction\n$$\n\\widehat{p}_+ = \\frac{1}{T - B}\\sum_{t=B+1}^{T} \\mathbf{1}\\{X_1^{(t)} > 0\\},\n$$\nwhere $T$ is the total number of iterations, $B$ is the burn-in, and $X_1^{(t)}$ is the first coordinate at iteration $t$. In the stuck original-coordinate Gibbs, $\\widehat{p}_+$ is exactly $0$ if initialized in $\\mathcal{A}$ and exactly $1$ if initialized in $\\mathcal{B}$. In the reparameterized sampler, the chain traverses both components and $\\widehat{p}_+$ should be close to $0.5$.\n\nProgram requirements:\n- Implement both samplers exactly as specified.\n- Use a fixed random number generator seed per test case for reproducibility.\n- For each test case, run the sampler for $T$ iterations with burn-in $B$ and report $\\widehat{p}_+$ rounded to three decimal places.\n\nTest suite:\n- Case $1$ (original $(x_1,x_2)$, stuck in $\\mathcal{A}$): $T=10000$, $B=1000$, seed $= 123$, initialize at $(x_1,x_2)=(-2.5,-2.0)$.\n- Case $2$ (original $(x_1,x_2)$, stuck in $\\mathcal{B}$): $T=10000$, $B=1000$, seed $= 456$, initialize at $(x_1,x_2)=(2.5,2.1)$.\n- Case $3$ (reparameterized $(u,v)$, ergodic across components): $T=10000$, $B=1000$, seed $= 789$, initialize in $\\mathcal{A}$ at $(x_1,x_2)=(-2.0,-2.0)$, then map to $(u,v)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases and rounded to three decimals, for example, \"[0.000,1.000,0.503]\".", "solution": "The problem requires the implementation and comparison of two Gibbs sampling strategies for a bimodal target distribution with disconnected support. The objective is to demonstrate a common failure mode of the Gibbs sampler and its resolution through reparameterization.\n\nThe target probability density, $\\pi(x_1, x_2)$, is defined over $\\mathbb{R}^2$ and is proportional to the sum of indicator functions on two disjoint sets. The support is the union of two axis-aligned squares, $\\mathcal{A}$ and $\\mathcal{B}$, defined as:\n$$\n\\mathcal{A} = \\{(x_1, x_2): x_1 \\in [-3,-1],\\ x_2 \\in [-3,-1]\\}\n$$\n$$\n\\mathcal{B} = \\{(x_1, x_2): x_1 \\in [1,3],\\ x_2 \\in [1,3]\\}\n$$\nThe density is given by:\n$$\n\\pi(x_1,x_2) \\propto \\mathbf{1}_{\\mathcal{A}}(x_1,x_2) + \\mathbf{1}_{\\mathcal{B}}(x_1,x_2)\n$$\nwhere $\\mathbf{1}_{S}$ is the indicator function for a set $S$. Since the areas of $\\mathcal{A}$ and $\\mathcal{B}$ are equal (both are $4$), the distribution assigns equal probability mass of $0.5$ to each component.\n\n**Sampler 1: Original-Coordinate Gibbs Sampler**\nA Gibbs sampler generates a sequence of samples from a multivariate distribution by iteratively sampling each variable from its full conditional distribution, given the current values of all other variables. For the target density $\\pi(x_1, x_2)$, the full conditional distributions are:\n\n1.  For $X_1$ given $X_2=x_2$:\n    - If $x_2 \\in [-3,-1]$, the state $(x_1, x_2)$ must be in $\\mathcal{A}$. Thus, the conditional distribution for $X_1$ is uniform on the corresponding interval: $X_1 \\mid X_2=x_2 \\sim \\text{Uniform}([-3,-1])$.\n    - If $x_2 \\in [1,3]$, the state must be in $\\mathcal{B}$, so $X_1 \\mid X_2=x_2 \\sim \\text{Uniform}([1,3])$.\n    - For any other value of $x_2$, the conditional density is undefined as the probability is zero.\n\n2.  For $X_2$ given $X_1=x_1$:\n    - By symmetry, if $x_1 \\in [-3,-1]$, then $X_2 \\mid X_1=x_1 \\sim \\text{Uniform}([-3,-1])$.\n    - If $x_1 \\in [1,3]$, then $X_2 \\mid X_1=x_1 \\sim \\text{Uniform}([1,3])$.\n\nThe algorithm for one iteration $(t) \\to (t+1)$ is:\n1.  Sample $x_1^{(t+1)} \\sim p(x_1 \\mid x_2^{(t)})$.\n2.  Sample $x_2^{(t+1)} \\sim p(x_2 \\mid x_1^{(t+1)})$.\n\nThis sampler is non-ergodic. An ergodic Markov chain must be irreducible, meaning it can reach any part of its state space from any other part. Here, if the chain is initialized with a state $(x_1^{(0)}, x_2^{(0)}) \\in \\mathcal{A}$, the conditional for $x_1$ will only have support on $[-3,-1]$, and the subsequent conditional for $x_2$ will also have support on $[-3,-1]$. The chain will be trapped within $\\mathcal{A}$ indefinitely. Similarly, a chain initialized in $\\mathcal{B}$ will never transition to $\\mathcal{A}$. This violates irreducibility and the sampler fails to converge to the true target distribution $\\pi$.\n\n**Sampler 2: Reparameterized Gibbs Sampler**\nTo overcome this issue, we reparameterize the state space using a bijective linear map (a $45$-degree rotation and scaling):\n$$\nU = X_1 - X_2, \\quad V = X_1 + X_2\n$$\nThe inverse transformation is:\n$$\nX_1 = \\frac{U+V}{2}, \\quad X_2 = \\frac{V-U}{2}\n$$\nUnder this map, the support regions transform into disjoint horizontal bands in the $(u,v)$-plane:\n- For $(x_1,x_2) \\in \\mathcal{A}$: $u=x_1-x_2 \\in [-2,2]$ and $v=x_1+x_2 \\in [-6,-2]$. So, $\\mathcal{A}_{uv} = \\{(u,v): u \\in [-2,2], v \\in [-6,-2]\\}$.\n- For $(x_1,x_2) \\in \\mathcal{B}$: $u=x_1-x_2 \\in [-2,2]$ and $v=x_1+x_2 \\in [2,6]$. So, $\\mathcal{B}_{uv} = \\{(u,v): u \\in [-2,2], v \\in [2,6]\\}$.\n\nThe Jacobian of this transformation is a constant, $|J|=2$, so the transformed density $\\pi_{uv}(u,v)$ remains uniform over the new support $\\mathcal{A}_{uv} \\cup \\mathcal{B}_{uv}$. The full conditional distributions in the $(u,v)$ coordinates are:\n\n1.  For $U$ given $V=v$:\n    - For any $v \\in [-6,-2] \\cup [2,6]$, the support for $U$ is the interval $[-2,2]$. Thus, the conditional is $U \\mid V=v \\sim \\text{Uniform}([-2,2])$.\n\n2.  For $V$ given $U=u$:\n    - For any $u \\in [-2,2]$, the support for $V$ is the union of two disjoint intervals: $[-6,-2] \\cup [2,6]$. The original distribution assigned equal mass to $\\mathcal{A}$ and $\\mathcal{B}$. The linear transformation preserves the uniformity and relative mass. The two intervals for $V$ have equal length ($4$). Therefore, the conditional distribution for $V$ involves selecting one of the two intervals with equal probability ($0.5$) and then sampling uniformly within it.\n\nThe algorithm for one iteration $(t) \\to (t+1)$ is:\n1.  Sample $u^{(t+1)} \\sim p(u \\mid v^{(t)}) = \\text{Uniform}([-2,2])$.\n2.  Sample $v^{(t+1)} \\sim p(v \\mid u^{(t+1)})$, which is done by choosing an interval from $\\{[-6,-2], [2,6]\\}$ with probability $0.5$ each, then sampling uniformly from the chosen interval.\n\nThis reparameterized sampler is ergodic. The conditional distribution for $V$ explicitly allows the chain to jump between the two regions corresponding to the original modes $\\mathcal{A}$ and $\\mathcal{B}$. The chain can therefore explore the entire support of the distribution.\n\n**Diagnostic Measure**\nThe performance of the samplers is assessed using the empirical fraction of samples for which $X_1 > 0$:\n$$\n\\widehat{p}_+ = \\frac{1}{T - B}\\sum_{t=B+1}^{T} \\mathbf{1}\\{X_1^{(t)} > 0\\}\n$$\nwhere $T$ is the total number of iterations and $B$ is the burn-in period.\n- For the original sampler initialized in $\\mathcal{A}$, where $x_1 < 0$ always, we expect $\\widehat{p}_+ = 0$.\n- For the original sampler initialized in $\\mathcal{B}$, where $x_1 > 0$ always, we expect $\\widehat{p}_+ = 1$.\n- For the reparameterized sampler, which explores both modes, we expect it to spend approximately half its time in the region corresponding to $\\mathcal{B}$ (where $x_1>0$) and half in the region for $\\mathcal{A}$ (where $x_1<0$). Therefore, we expect $\\widehat{p}_+ \\approx 0.5$.\n\nThe test cases will validate these theoretical expectations.\n- Case 1: Original sampler, $T=10000$, $B=1000$, seed $= 123$, initial point $(-2.5,-2.0) \\in \\mathcal{A}$.\n- Case 2: Original sampler, $T=10000$, $B=1000$, seed $= 456$, initial point $(2.5,2.1) \\in \\mathcal{B}$.\n- Case 3: Reparameterized sampler, $T=10000$, $B=1000$, seed $= 789$, initial point $(-2.0,-2.0)$ (which is in $\\mathcal{A}$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gibbs_original(T, B, seed, initial_x):\n    \"\"\"\n    Implements the Gibbs sampler in the original (x1, x2) coordinates.\n    This sampler is expected to get stuck in one of the two modes.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    x1, x2 = initial_x\n    \n    samples_x1 = []\n    \n    for t in range(T):\n        # Update x1 based on the current x2\n        if -3.0 <= x2 <= -1.0:\n            x1 = rng.uniform(-3.0, -1.0)\n        elif 1.0 <= x2 <= 3.0:\n            x1 = rng.uniform(1.0, 3.0)\n        else:\n            # This case should not be reached with valid initial points\n            raise ValueError(\"x2 is outside the support.\")\n            \n        # Update x2 based on the new x1\n        if -3.0 <= x1 <= -1.0:\n            x2 = rng.uniform(-3.0, -1.0)\n        elif 1.0 <= x1 <= 3.0:\n            x2 = rng.uniform(1.0, 3.0)\n        else:\n            raise ValueError(\"x1 is outside the support.\")\n\n        if t >= B:\n            samples_x1.append(x1)\n            \n    count_positive = sum(1 for s in samples_x1 if s > 0)\n    p_hat_plus = count_positive / (T - B)\n    return p_hat_plus\n\ndef gibbs_reparameterized(T, B, seed, initial_x):\n    \"\"\"\n    Implements the Gibbs sampler in the reparameterized (u, v) coordinates.\n    This sampler should be ergodic and explore both modes.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Map initial point from (x1, x2) to (u, v)\n    x1_init, x2_init = initial_x\n    u = x1_init - x2_init\n    v = x1_init + x2_init\n    \n    samples_x1 = []\n    \n    for t in range(T):\n        # Update u by sampling from its full conditional U | V=v\n        u = rng.uniform(-2.0, 2.0)\n        \n        # Update v by sampling from its full conditional V | U=u\n        # This involves choosing one of two intervals with equal probability\n        if rng.random() < 0.5:\n            # Sample from the interval [-6, -2]\n            v = rng.uniform(-6.0, -2.0)\n        else:\n            # Sample from the interval [2, 6]\n            v = rng.uniform(2.0, 6.0)\n\n        if t >= B:\n            # Map back to (x1, x2) for evaluation and store x1\n            x1 = (u + v) / 2.0\n            samples_x1.append(x1)\n            \n    count_positive = sum(1 for s in samples_x1 if s > 0)\n    p_hat_plus = count_positive / (T - B)\n    return p_hat_plus\n    \ndef solve():\n    \"\"\"\n    Defines and runs the test cases specified in the problem statement.\n    \"\"\"\n    test_cases = [\n        {'sampler': 'original', 'T': 10000, 'B': 1000, 'seed': 123, 'initial_point': (-2.5, -2.0)},\n        {'sampler': 'original', 'T': 10000, 'B': 1000, 'seed': 456, 'initial_point': (2.5, 2.1)},\n        {'sampler': 'reparameterized', 'T': 10000, 'B': 1000, 'seed': 789, 'initial_point': (-2.0, -2.0)},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['sampler'] == 'original':\n            result = gibbs_original(case['T'], case['B'], case['seed'], case['initial_point'])\n        elif case['sampler'] == 'reparameterized':\n            result = gibbs_reparameterized(case['T'], case['B'], case['seed'], case['initial_point'])\n        \n        results.append(f\"{result:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3125089"}, {"introduction": "Gibbs sampling is not just a theoretical tool; it is a powerful engine for fitting complex Bayesian models in machine learning. This hands-on practice guides you through implementing a Gibbs sampler for a Bayesian Gaussian Mixture Model (GMM), a popular method for unsupervised clustering. By leveraging the elegance of conjugate priors, you will see how to derive simple, closed-form updates for cluster assignments, weights, and means, and ultimately build a complete program that can discover hidden structure in data [@problem_id:3235855].", "problem": "Implement a complete Gibbs sampler for a Bayesian Gaussian mixture model to perform clustering on one-dimensional data or two-dimensional data. The implementation must be a single, self-contained program that uses only the Numerical Python (NumPy) library and the Python standard library. The sampler must use a conjugate Bayesian model with known spherical covariance and must update component assignments, component means, and mixture weights via their full conditional distributions derived from Bayes’ theorem. The final output must be quantitative and testable against a provided test suite.\n\nThe fundamental base for the derivation must start from the definition of a mixture model, Bayes’ theorem and conditional independence, and well-tested facts about conjugate priors in the Gaussian family. Specifically, the model is a finite mixture model with a symmetric Dirichlet prior on mixture weights and a normal prior on component means, with the likelihood given by a Gaussian distribution with known spherical covariance. The sampler must be constructed by deriving the full conditionals for the latent component indicators, the mixture weights, and the component means.\n\nModeling assumptions:\n- Let the dataset consist of points $\\{x_i\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^d$, where $d \\in \\{1,2\\}$.\n- There are $K$ mixture components. Each data point $x_i$ has a latent component label $z_i \\in \\{0,1,\\dots,K-1\\}$.\n- The likelihood is $x_i \\mid z_i=k, \\mu_k \\sim \\mathcal{N}(\\mu_k, \\sigma^2 I_d)$ for all components $k$, where $I_d$ is the $d \\times d$ identity matrix and $\\sigma^2$ is known.\n- The prior for each component mean is $\\mu_k \\sim \\mathcal{N}(\\mu_0, \\tau^2 I_d)$ with known $\\mu_0$ and $\\tau^2$.\n- The mixture weights are $\\pi = (\\pi_0,\\dots,\\pi_{K-1}) \\sim \\operatorname{Dirichlet}(\\alpha,\\dots,\\alpha)$ with symmetric concentration parameter $\\alpha$.\n\nYour program must:\n- Derive and implement the full conditional distributions for $z_i$, $\\pi$, and $\\mu_k$ using Bayes’ theorem and the conjugacy of the normal likelihood with the normal prior and of the categorical likelihood with the Dirichlet prior.\n- Support both $d=1$ and $d=2$ by treating the known covariance as $\\sigma^2 I_d$.\n- Use a fixed random seed per test case to ensure reproducibility.\n- Run the Gibbs sampler for a specified number of iterations and produce final predicted cluster labels by taking, for each data point, the most frequent label over the last $L$ iterations of the Markov chain.\n- Compute the clustering quality using the Adjusted Rand Index (ARI), expressed as a decimal in the closed interval $[0,1]$.\n\nThe Adjusted Rand Index (ARI) must be computed exactly from the contingency of the true labels and the predicted labels by pair counting, and must be reported as a decimal number with four digits after the decimal point. No physical units are involved. Angles are not involved. Percentages must not be used; the ARI must be expressed as a decimal.\n\nTest suite:\nProvide three test cases that exercise different facets of the sampler. In all cases, data are synthetically generated from known Gaussian mixtures and then clustered by the implemented Gibbs sampler. The program must embed and run the following test cases without requiring any user input:\n\n- Case $1$ (happy path, one-dimensional, two clearly separated clusters):\n    - Dimension: $d=1$.\n    - True mixture means: $\\{-3.0, 3.0\\}$.\n    - Known standard deviation: $\\sigma=0.6$ (so variance $\\sigma^2=0.36$).\n    - Number of points per component: $\\{30, 30\\}$, so $N=60$.\n    - Number of components in the model: $K=2$.\n    - Prior mean: $\\mu_0=0.0$.\n    - Prior variance scaling: $\\tau^2=4.0$.\n    - Dirichlet concentration: $\\alpha=1.0$ (symmetric).\n    - Gibbs iterations: $T=1000$.\n    - Consensus window length: $L=200$ (use the last $L$ iterations to form consensus assignments per point).\n    - Random seed: $123$.\n    - Required output for this case: the ARI as a decimal rounded to four digits after the decimal point.\n\n- Case $2$ (two-dimensional, three moderately separated clusters):\n    - Dimension: $d=2$.\n    - True mixture means: $\\{(-3.0,-3.0), (0.0,4.0), (4.0,-1.0)\\}$.\n    - Known standard deviation: $\\sigma=0.7$ (so variance $\\sigma^2=0.49$).\n    - Number of points per component: $\\{40, 40, 40\\}$, so $N=120$.\n    - Number of components in the model: $K=3$.\n    - Prior mean: $\\mu_0=(0.0, 0.0)$.\n    - Prior variance scaling: $\\tau^2=4.0$.\n    - Dirichlet concentration: $\\alpha=1.0$ (symmetric).\n    - Gibbs iterations: $T=1200$.\n    - Consensus window length: $L=200$.\n    - Random seed: $321$.\n    - Required output for this case: the ARI as a decimal rounded to four digits after the decimal point.\n\n- Case $3$ (edge case, one-dimensional data from two clusters but the model specifies three components, testing empty or redundant components handling):\n    - Dimension: $d=1$.\n    - True mixture means: $\\{-2.0, 2.0\\}$.\n    - Known standard deviation: $\\sigma=0.8$ (so variance $\\sigma^2=0.64$).\n    - Number of points per component: $\\{50, 50\\}$, so $N=100$.\n    - Number of components in the model: $K=3$.\n    - Prior mean: $\\mu_0=0.0$.\n    - Prior variance scaling: $\\tau^2=4.0$.\n    - Dirichlet concentration: $\\alpha=1.0$ (symmetric).\n    - Gibbs iterations: $T=1200$.\n    - Consensus window length: $L=200$.\n    - Random seed: $555$.\n    - Required output for this case: the ARI as a decimal rounded to four digits after the decimal point.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly the ARI for Case $1$, then Case $2$, then Case $3$, each rounded to four digits after the decimal point. For example, the output must look like $[0.9876,0.9543,0.8732]$ (this is only an example of formatting, not the expected numerical values).", "solution": "The problem statement outlines a complete and well-defined task in computational statistics: to implement a Gibbs sampler for a Bayesian Gaussian mixture model (GMM) and evaluate its performance on synthetic datasets using the Adjusted Rand Index (ARI).\n\n### Step 1: Extract Givens\n\n-   **Model**: Bayesian Gaussian Mixture Model for clustering.\n-   **Data**: $N$ points $\\{x_i\\}_{i=1}^N$, where $x_i \\in \\mathbb{R}^d$ and the dimension $d$ is either $1$ or $2$.\n-   **Latent Variables**: A component label $z_i \\in \\{0, 1, \\dots, K-1\\}$ for each data point $x_i$.\n-   **Likelihood**: The distribution of a data point $x_i$ given its component assignment $z_i=k$ and component mean $\\mu_k$ is a multivariate normal distribution, $p(x_i | z_i=k, \\mu_k) = \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d)$. The covariance is spherical with a known variance $\\sigma^2$. $I_d$ is the $d \\times d$ identity matrix.\n-   **Priors**:\n    -   Component means: $\\mu_k \\sim \\mathcal{N}(\\mu_0, \\tau^2 I_d)$ for $k \\in \\{0, \\dots, K-1\\}$. The hyperparameters $\\mu_0$ and $\\tau^2$ are known.\n    -   Mixture weights: $\\pi = (\\pi_0, \\dots, \\pi_{K-1}) \\sim \\operatorname{Dirichlet}(\\alpha, \\dots, \\alpha)$. The symmetric concentration hyperparameter $\\alpha$ is known.\n-   **Task**: Implement a Gibbs sampler by deriving and using the full conditional distributions for $z_i$, $\\pi$, and $\\mu_k$.\n-   **Execution**: The sampler runs for $T$ iterations. The final cluster assignments are determined by finding the most frequent (modal) assignment for each point over the last $L$ iterations.\n-   **Evaluation**: The quality of the clustering is measured by the Adjusted Rand Index (ARI) against the true labels of the synthetically generated data.\n-   **Test Suite**: Three specific test cases are provided with all necessary parameters:\n    -   Case 1: $d=1$, $K=2$, clearly separated.\n    -   Case 2: $d=2$, $K=3$, moderately separated.\n    -   Case 3: $d=1$, data from a 2-component mixture, but model assumes $K=3$.\n-   **Output**: A list of ARI values for the three test cases, formatted as `[ari1,ari2,ari3]`, with each ARI rounded to four decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is a standard application of Bayesian inference and Markov chain Monte Carlo (MCMC) methods, specifically Gibbs sampling. The model is a classic GMM with conjugate priors, a cornerstone of unsupervised machine learning. All principles are derived from probability theory and statistics. The problem is valid.\n-   **Well-Posed**: The use of conjugate priors ensures that the full conditional distributions are standard, well-defined distributions from which it is straightforward to sample. The problem is specified with enough detail (hyperparameters, number of iterations, etc.) to ensure a unique procedure can be implemented. The fixed random seeds ensure the result is reproducible. The problem is valid.\n-   **Objective**: The problem is stated in precise mathematical language. The evaluation metric (ARI) is a standard, objective measure of clustering similarity. The problem is free of subjective claims. The problem is valid.\n-   **Self-Contained and Consistent**: The problem provides all necessary model definitions, hyperparameters, and test case parameters. There are no internal contradictions. The problem is valid.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. I will proceed with the solution by first deriving the full conditional distributions required for the Gibbs sampler and then providing the implementation.\n\n### Derivation of Full Conditional Distributions\n\nThe Gibbs sampling algorithm iteratively samples each variable (or block of variables) from its distribution conditioned on the current values of all other variables and the data. These are the full conditional distributions.\n\n**1. Full Conditional for Component Assignments, $z_i$**\n\nWe need to find $p(z_i | \\mathbf{x}, \\mathbf{z}_{-i}, \\mathbf{\\mu}, \\mathbf{\\pi})$, where $\\mathbf{z}_{-i}$ denotes all component assignments except for $z_i$. Due to conditional independence assumptions in the model, this simplifies to $p(z_i=k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi})$.\n\nUsing Bayes' theorem:\n$$p(z_i = k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi}) \\propto p(x_i | z_i = k, \\mathbf{\\mu}, \\mathbf{\\pi}) \\cdot p(z_i = k | \\mathbf{\\mu}, \\mathbf{\\pi})$$\nThe conditional distribution of $x_i$ depends only on its own component's mean $\\mu_k$, and the prior for $z_i$ depends only on the mixture weights $\\pi$.\n$$p(z_i = k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi}) \\propto p(x_i | z_i = k, \\mu_k) \\cdot p(z_i = k | \\mathbf{\\pi})$$\nSubstituting the model definitions:\n$$p(z_i = k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi}) \\propto \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d) \\cdot \\pi_k$$\nThis means that for each data point $x_i$, its new assignment $z_i$ is sampled from a Categorical distribution over $\\{0, \\dots, K-1\\}$ with probabilities proportional to $\\pi_k \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d)$, which we can write as:\n$$p(z_i=k|\\dots) \\propto \\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2\\right)$$\nThese values are computed for each $k \\in \\{0, \\dots, K-1\\}$ and then normalized to form the probability vector for the Categorical distribution.\n\n**2. Full Conditional for Mixture Weights, $\\pi$**\n\nWe seek the distribution $p(\\pi | \\mathbf{x}, \\mathbf{z}, \\mathbf{\\mu})$. By conditional independence, this is $p(\\pi | \\mathbf{z})$.\n$$p(\\mathbf{\\pi} | \\mathbf{z}) \\propto p(\\mathbf{z} | \\mathbf{\\pi}) \\cdot p(\\mathbf{\\pi})$$\nThe likelihood term $p(\\mathbf{z} | \\mathbf{\\pi})$ is for a set of i.i.d. Categorical variables, and $p(\\mathbf{\\pi})$ is the Dirichlet prior.\n$$p(\\mathbf{z} | \\mathbf{\\pi}) = \\prod_{i=1}^N p(z_i | \\mathbf{\\pi}) = \\prod_{i=1}^N \\pi_{z_i} = \\prod_{k=0}^{K-1} \\pi_k^{N_k}$$\nwhere $N_k = \\sum_{i=1}^N \\mathbb{I}(z_i = k)$ is the number of data points assigned to component $k$. The Dirichlet prior is:\n$$p(\\mathbf{\\pi}) \\propto \\prod_{k=0}^{K-1} \\pi_k^{\\alpha - 1}$$\nCombining these gives the posterior:\n$$p(\\mathbf{\\pi} | \\mathbf{z}) \\propto \\left( \\prod_{k=0}^{K-1} \\pi_k^{N_k} \\right) \\left( \\prod_{k=0}^{K-1} \\pi_k^{\\alpha - 1} \\right) = \\prod_{k=0}^{K-1} \\pi_k^{N_k + \\alpha - 1}$$\nThis is the kernel of a Dirichlet distribution. Thus, the full conditional for $\\pi$ is:\n$$\\mathbf{\\pi} | \\mathbf{z} \\sim \\operatorname{Dirichlet}(\\alpha + N_0, \\alpha + N_1, \\dots, \\alpha + N_{K-1})$$\nThis demonstrates the conjugacy of the Dirichlet prior with the Categorical/Multinomial likelihood.\n\n**3. Full Conditional for Component Means, $\\mu_k$**\n\nWe need $p(\\mu_k | \\mathbf{x}, \\mathbf{z}, \\mathbf{\\pi}, \\mathbf{\\mu}_{-k})$. This simplifies to $p(\\mu_k | \\{x_i: z_i=k\\})$.\n$$p(\\mu_k | \\mathbf{x}, \\mathbf{z}) \\propto \\left( \\prod_{i: z_i=k} p(x_i | \\mu_k) \\right) p(\\mu_k)$$\nThe likelihood part involves data points assigned to component $k$, and the prior is the normal distribution for $\\mu_k$.\n$$p(\\mu_k | \\mathbfx, \\mathbf{z}) \\propto \\left( \\prod_{i: z_i=k} \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d) \\right) \\mathcal{N}(\\mu_k | \\mu_0, \\tau^2 I_d)$$\nThe posterior for a Gaussian mean with a Gaussian prior is also Gaussian. The posterior precision is the sum of the prior precision and the likelihood precision. The precision of the prior is $(1/\\tau^2)I_d$. The precision from one data point is $(1/\\sigma^2)I_d$. For $N_k$ data points, the likelihood precision is $(N_k/\\sigma^2)I_d$.\nThe posterior precision matrix is $\\mathbf{P}_{\\text{post}} = \\left(\\frac{N_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)I_d$. The posterior covariance is $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{P}_{\\text{post}}^{-1} = \\hat{\\tau}_k^2 I_d$, where:\n$$\\hat{\\tau}_k^2 = \\left(\\frac{N_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$$\nThe posterior mean is a precision-weighted average of the prior mean and the data mean. Let $\\bar{x}_k = \\frac{1}{N_k}\\sum_{i:z_i=k} x_i$.\n$$\\hat{\\mu}_k = \\mathbf{\\Sigma}_{\\text{post}} \\left(\\frac{1}{\\sigma^2} \\sum_{i:z_i=k} x_i + \\frac{1}{\\tau^2} \\mu_0 \\right) = \\hat{\\tau}_k^2 \\left( \\frac{N_k}{\\sigma^2} \\bar{x}_k + \\frac{1}{\\tau^2} \\mu_0 \\right)$$\nIf $N_k=0$, the posterior for $\\mu_k$ reverts to its prior, $\\mathcal{N}(\\mu_0, \\tau^2 I_d)$. Thus, the full conditional is:\n$$\\mu_k | \\mathbf{x}, \\mathbf{z} \\sim \\mathcal{N}(\\hat{\\mu}_k, \\hat{\\tau}_k^2 I_d)$$\nThis illustrates the conjugacy of the Normal prior with the Normal likelihood (for the mean parameter).\n\n### The Gibbs Sampling Algorithm\n\n1.  **Initialization**: Initialize $\\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\pi}$ to random or heuristic values.\n2.  **Iteration**: For $t = 1, \\dots, T$:\n    a. Sample $\\mathbf{\\pi}^{(t+1)} \\sim p(\\mathbf{\\pi} | \\mathbf{z}^{(t)})$.\n    b. For each component $k=0, \\dots, K-1$, sample $\\mu_k^{(t+1)} \\sim p(\\mu_k | \\mathbf{x}, \\mathbf{z}^{(t)})$.\n    c. For each data point $i=1, \\dots, N$, sample $z_i^{(t+1)} \\sim p(z_i | x_i, \\mathbf{\\mu}^{(t+1)}, \\mathbf{\\pi}^{(t+1)})$.\n3.  **Consensus**: Collect the samples $\\{\\mathbf{z}^{(t)}\\}_{t=T-L+1}^T$. For each point $i$, the final assignment $\\hat{z}_i$ is the mode of $\\{z_i^{(t)}\\}_{t=T-L+1}^T$.\n\nThis procedure generates samples from the joint posterior distribution $p(\\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\pi} | \\mathbf{x})$. The implementation will follow this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport collections\n\ndef calculate_ari(labels_true, labels_pred):\n    \"\"\"\n    Computes the Adjusted Rand Index (ARI) from a contingency table.\n    This implementation is based on the pair-counting formula and does not use\n    any external libraries beyond NumPy.\n    \"\"\"\n    # Create the contingency table\n    n_true_labels = np.max(labels_true) + 1 if labels_true.size > 0 else 0\n    n_pred_labels = np.max(labels_pred) + 1 if labels_pred.size > 0 else 0\n    contingency_table = np.zeros((n_true_labels, n_pred_labels), dtype=int)\n    for i in range(len(labels_true)):\n        contingency_table[labels_true[i], labels_pred[i]] += 1\n    \n    def n_choose_2(n):\n        # Using np.asarray to handle both scalars and arrays\n        n_arr = np.asarray(n)\n        return n_arr * (n_arr-1) / 2.0\n\n    sum_comb_nij = np.sum(n_choose_2(contingency_table))\n    sum_comb_a = np.sum(n_choose_2(np.sum(contingency_table, axis=1)))\n    sum_comb_b = np.sum(n_choose_2(np.sum(contingency_table, axis=0)))\n\n    N = len(labels_true)\n    if N < 2:\n        return 1.0 if n_true_labels == n_pred_labels else 0.0\n\n    comb_N = n_choose_2(N)\n    \n    expected_index = sum_comb_a * sum_comb_b / comb_N\n    max_index = (sum_comb_a + sum_comb_b) / 2.0\n    \n    denominator = max_index - expected_index\n    if denominator == 0:\n        return 0.0 # Or 1.0 if index also equals max_index. Standard def is 0.\n        \n    ari = (sum_comb_nij - expected_index) / denominator\n    return ari\n\ndef run_gmm_gibbs_sampler(case):\n    \"\"\"\n    Runs the full Gibbs sampler for one test case.\n    \"\"\"\n    d = case['d']\n    true_means = case['true_means']\n    sigma = case['sigma']\n    counts_per_component = case['counts_per_component']\n    N = sum(counts_per_component)\n    K = case['K']\n    mu_0_raw = case['mu_0']\n    tau_sq = case['tau_sq']\n    alpha = case['alpha']\n    T = case['T']\n    L = case['L']\n    seed = case['seed']\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic data\n    X = []\n    true_labels = []\n    mu_0 = np.array(mu_0_raw) if d > 1 else np.array([mu_0_raw])\n    sigma_sq = sigma**2\n\n    for i, mean in enumerate(true_means):\n        n_points = counts_per_component[i]\n        cov = sigma_sq * np.eye(d)\n        points = rng.multivariate_normal(np.array(mean), cov, size=n_points)\n        X.append(points)\n        true_labels.append(np.full(n_points, i, dtype=int))\n    \n    X = np.vstack(X)\n    true_labels = np.concatenate(true_labels)\n    if d == 1:\n        X = X.reshape(-1, 1)\n\n    # 2. Gibbs Sampler\n    # Initialization\n    z = rng.integers(0, K, size=N)\n    pi = rng.dirichlet(np.full(K, alpha))\n    mu = rng.multivariate_normal(mu_0, tau_sq * np.eye(d), size=K)\n    \n    z_history = np.zeros((L, N), dtype=int)\n\n    # Main loop\n    for t in range(T):\n        # a. Update pi\n        counts = np.bincount(z, minlength=K)\n        pi = rng.dirichlet(alpha + counts)\n        \n        # b. Update mu_k for each component k\n        for k in range(K):\n            X_k = X[z == k]\n            N_k = X_k.shape[0]\n            \n            if N_k == 0:\n                # If cluster is empty, draw from prior\n                mu_hat_k = mu_0\n                tau_hat_k_sq = tau_sq\n            else:\n                prec_post = (N_k / sigma_sq) + (1 / tau_sq)\n                tau_hat_k_sq = 1.0 / prec_post\n                \n                sum_x_k = np.sum(X_k, axis=0)\n                mu_hat_k = tau_hat_k_sq * (sum_x_k / sigma_sq + mu_0 / tau_sq)\n\n            mu[k] = rng.multivariate_normal(mu_hat_k, tau_hat_k_sq * np.eye(d))\n        \n        # c. Update z_i for each data point i\n        # Vectorized calculation of log probabilities\n        dist_sq = np.sum((X[:, np.newaxis, :] - mu[np.newaxis, :, :])**2, axis=2) # Shape (N, K)\n        log_likelihoods = -0.5 * dist_sq / sigma_sq\n        log_probs_unnorm = np.log(pi) + log_likelihoods\n\n        # Normalize probabilities using log-sum-exp trick and sample\n        for i in range(N):\n            log_p_i = log_probs_unnorm[i, :]\n            log_p_i_stable = log_p_i - np.max(log_p_i)\n            p_i = np.exp(log_p_i_stable)\n            p_i /= np.sum(p_i)\n            z[i] = rng.choice(K, p=p_i)\n            \n        # Store z if in consensus window\n        if t >= T - L:\n            z_history[t - (T - L)] = z\n\n    # 3. Form consensus assignments\n    pred_labels = np.zeros(N, dtype=int)\n    for i in range(N):\n        # Find the most frequent label (mode)\n        point_history = z_history[:, i]\n        counts = np.bincount(point_history, minlength=K)\n        pred_labels[i] = np.argmax(counts)\n        \n    # 4. Compute ARI\n    ari = calculate_ari(true_labels, pred_labels)\n    \n    return ari\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and report results for all test cases.\n    \"\"\"\n    test_cases = [\n        {   # Case 1\n            \"d\": 1,\n            \"true_means\": [[-3.0], [3.0]],\n            \"sigma\": 0.6,\n            \"counts_per_component\": [30, 30],\n            \"K\": 2,\n            \"mu_0\": 0.0,\n            \"tau_sq\": 4.0,\n            \"alpha\": 1.0,\n            \"T\": 1000,\n            \"L\": 200,\n            \"seed\": 123\n        },\n        {   # Case 2\n            \"d\": 2,\n            \"true_means\": [[-3.0, -3.0], [0.0, 4.0], [4.0, -1.0]],\n            \"sigma\": 0.7,\n            \"counts_per_component\": [40, 40, 40],\n            \"K\": 3,\n            \"mu_0\": [0.0, 0.0],\n            \"tau_sq\": 4.0,\n            \"alpha\": 1.0,\n            \"T\": 1200,\n            \"L\": 200,\n            \"seed\": 321\n        },\n        {   # Case 3\n            \"d\": 1,\n            \"true_means\": [[-2.0], [2.0]],\n            \"sigma\": 0.8,\n            \"counts_per_component\": [50, 50],\n            \"K\": 3,\n            \"mu_0\": 0.0,\n            \"tau_sq\": 4.0,\n            \"alpha\": 1.0,\n            \"T\": 1200,\n            \"L\": 200,\n            \"seed\": 555\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        ari = run_gmm_gibbs_sampler(case)\n        results.append(f\"{ari:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3235855"}]}