{"hands_on_practices": [{"introduction": "Bayesian inference provides a principled framework for updating our beliefs about parameters as we collect data. This first practice focuses on the core mechanics of this process using a model for count data, which is ubiquitous in fields from reliability engineering to biology. By working with the Poisson-Gamma conjugate pair [@problem_id:3104618], you will derive the posterior and posterior predictive distributions from first principles, building a solid foundation for more complex Bayesian models.", "problem": "A reliability study records counts of component failures over varying lengths of monitored time. For observation $i \\in \\{1,\\dots,n\\}$, let $y_{i}$ be the observed count during exposure $E_{i}$ (in time units). Assume a Poisson regression with an exposure offset: conditional on an unknown baseline failure rate $\\theta$ (per unit exposure), the counts satisfy $y_{i} \\mid \\theta \\sim \\text{Poisson}(E_{i}\\,\\theta)$ independently. The prior on $\\theta$ is $\\text{Gamma}(a,b)$ in the shapeâ€“rate parameterization, with density $p(\\theta) = \\dfrac{b^{a}}{\\Gamma(a)}\\,\\theta^{a-1}\\exp(-b\\,\\theta)$ for $\\theta0$. Starting from the definition of the Poisson likelihood and the Gamma prior, derive the posterior predictive distribution for a future count $y_{\\text{new}}$ observed under exposure $E_{\\text{new}}$, expressing the predictive probability mass function $p(y_{\\text{new}}=k \\mid \\text{data})$ in closed form in terms of $a$, $b$, $\\sum_{i=1}^{n} y_{i}$, $\\sum_{i=1}^{n} E_{i}$, and $E_{\\text{new}}$, for any nonnegative integer $k$.\n\nNow consider the dataset with $n=5$ observations, exposures $(E_{1},E_{2},E_{3},E_{4},E_{5}) = (0.5,\\,1.0,\\,0.75,\\,0.25,\\,1.5)$ and counts $(y_{1},y_{2},y_{3},y_{4},y_{5}) = (1,\\,2,\\,2,\\,0,\\,1)$. Take prior hyperparameters $a=2$ and $b=2$. Using your derived posterior predictive distribution, compute the posterior predictive probability that the next observed count equals $4$ when the future exposure is $E_{\\text{new}}=2$. Express your final answer as a decimal and round to four significant figures.", "solution": "The problem requires the derivation of the posterior predictive distribution for a Poisson-Gamma conjugate model and a subsequent numerical calculation. The process involves three main steps: first, determining the posterior distribution of the rate parameter $\\theta$; second, using this posterior to derive the posterior predictive distribution for a new observation; and third, applying this result to the specific data provided.\n\nLet the data be denoted by $\\mathcal{D} = \\{(y_i, E_i)\\}_{i=1}^n$. The model specifies that the counts $y_i$ are conditionally independent given the rate parameter $\\theta$, following a Poisson distribution with mean $E_i\\theta$. The likelihood of the data is the product of the individual Poisson probability mass functions:\n$$p(\\mathcal{D} \\mid \\theta) = \\prod_{i=1}^{n} p(y_i \\mid \\theta) = \\prod_{i=1}^{n} \\frac{(E_i \\theta)^{y_i} \\exp(-E_i \\theta)}{y_i!}$$\nThis expression can be rewritten by separating terms that depend on $\\theta$:\n$$p(\\mathcal{D} \\mid \\theta) = \\left( \\prod_{i=1}^{n} \\frac{E_i^{y_i}}{y_i!} \\right) \\theta^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\theta \\sum_{i=1}^{n} E_i\\right)$$\nAs a function of $\\theta$, the likelihood is proportional to $\\theta^{\\sum y_i} \\exp(-\\theta \\sum E_i)$.\n\nThe prior distribution for $\\theta$ is a Gamma distribution, $\\theta \\sim \\text{Gamma}(a,b)$, with the probability density function:\n$$p(\\theta) = \\frac{b^a}{\\Gamma(a)} \\theta^{a-1} \\exp(-b\\theta)$$\nThe kernel of the prior is $p(\\theta) \\propto \\theta^{a-1} \\exp(-b\\theta)$.\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ given the data $\\mathcal{D}$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) p(\\theta)$$\n$$p(\\theta \\mid \\mathcal{D}) \\propto \\left( \\theta^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\theta \\sum_{i=1}^{n} E_i\\right) \\right) \\left( \\theta^{a-1} \\exp(-b\\theta) \\right)$$\nCombining the terms involving $\\theta$:\n$$p(\\theta \\mid \\mathcal{D}) \\propto \\theta^{a + \\sum_{i=1}^{n} y_i - 1} \\exp\\left(-\\left(b + \\sum_{i=1}^{n} E_i\\right)\\theta\\right)$$\nThis is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution due to conjugacy. Specifically, $\\theta \\mid \\mathcal{D} \\sim \\text{Gamma}(a', b')$, where the posterior hyperparameters are:\n$$a' = a + \\sum_{i=1}^{n} y_i$$\n$$b' = b + \\sum_{i=1}^{n} E_i$$\n\nNext, we derive the posterior predictive distribution for a new observation $y_{\\text{new}}$ with a given exposure $E_{\\text{new}}$. The likelihood for this new observation is $y_{\\text{new}} \\mid \\theta \\sim \\text{Poisson}(E_{\\text{new}}\\theta)$. The posterior predictive probability mass function (PMF) is obtained by marginalizing the product of the new observation's likelihood and the posterior distribution of $\\theta$ over all possible values of $\\theta$:\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\int_0^\\infty p(y_{\\text{new}}=k \\mid \\theta) \\, p(\\theta \\mid \\mathcal{D}) \\, d\\theta$$\nSubstituting the Poisson PMF for $y_{\\text{new}}$ and the Gamma posterior density for $\\theta$:\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\int_0^\\infty \\left( \\frac{(E_{\\text{new}}\\theta)^k \\exp(-E_{\\text{new}}\\theta)}{k!} \\right) \\left( \\frac{(b')^{a'}}{\\Gamma(a')} \\theta^{a'-1} \\exp(-b'\\theta) \\right) d\\theta$$\nWe can pull out the terms not dependent on $\\theta$ from the integral:\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{E_{\\text{new}}^k (b')^{a'}}{k! \\Gamma(a')} \\int_0^\\infty \\theta^{k+a'-1} \\exp(-(E_{\\text{new}}+b')\\theta) d\\theta$$\nThe integral is the kernel of a Gamma density, $\\text{Gamma}(k+a', E_{\\text{new}}+b')$, and its value is $\\frac{\\Gamma(k+a')}{(E_{\\text{new}}+b')^{k+a'}}$.\nSubstituting this result back into the expression:\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{E_{\\text{new}}^k (b')^{a'}}{k! \\Gamma(a')} \\frac{\\Gamma(k+a')}{(E_{\\text{new}}+b')^{k+a'}}$$\nThis can be rearranged into the form of a Negative Binomial distribution's PMF:\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{\\Gamma(k+a')}{k!\\Gamma(a')} \\frac{(b')^{a'} E_{\\text{new}}^k}{(E_{\\text{new}}+b')^{k+a'}} = \\binom{k+a'-1}{k} \\left(\\frac{b'}{E_{\\text{new}}+b'}\\right)^{a'} \\left(\\frac{E_{\\text{new}}}{E_{\\text{new}}+b'}\\right)^k$$\nSubstituting the expressions for $a'$ and $b'$ provides the final closed form:\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\binom{k + a + \\sum y_i - 1}{k} \\left(\\frac{b + \\sum E_i}{b + \\sum E_i + E_{\\text{new}}}\\right)^{a + \\sum y_i} \\left(\\frac{E_{\\text{new}}}{b + \\sum E_i + E_{\\text{new}}}\\right)^k$$\n\nFor the numerical part, we are given the data:\nPrior hyperparameters: $a=2$, $b=2$.\nExposures: $(E_1, \\dots, E_5) = (0.5, 1.0, 0.75, 0.25, 1.5)$.\nCounts: $(y_1, \\dots, y_5) = (1, 2, 2, 0, 1)$.\nFuture observation parameters: $k=4$, $E_{\\text{new}}=2$.\n\nFirst, we calculate the sums from the data:\n$$\\sum_{i=1}^5 y_i = 1+2+2+0+1 = 6$$\n$$\\sum_{i=1}^5 E_i = 0.5+1.0+0.75+0.25+1.5 = 4.0$$\nNext, we compute the posterior hyperparameters $a'$ and $b'$:\n$$a' = a + \\sum y_i = 2 + 6 = 8$$\n$$b' = b + \\sum E_i = 2 + 4.0 = 6.0$$\nNow we use the derived PMF to calculate $p(y_{\\text{new}}=4 \\mid \\mathcal{D})$:\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = \\binom{4+8-1}{4} \\left(\\frac{6}{2+6}\\right)^8 \\left(\\frac{2}{2+6}\\right)^4$$\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = \\binom{11}{4} \\left(\\frac{6}{8}\\right)^8 \\left(\\frac{2}{8}\\right)^4 = \\binom{11}{4} \\left(\\frac{3}{4}\\right)^8 \\left(\\frac{1}{4}\\right)^4$$\nWe calculate the binomial coefficient:\n$$\\binom{11}{4} = \\frac{11 \\times 10 \\times 9 \\times 8}{4 \\times 3 \\times 2 \\times 1} = 330$$\nThe probability is:\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = 330 \\times \\left(\\frac{3}{4}\\right)^8 \\left(\\frac{1}{4}\\right)^4 = 330 \\times \\frac{3^8}{4^8} \\times \\frac{1^4}{4^4} = 330 \\times \\frac{3^8}{4^{12}}$$\n$$3^8 = 6561$$\n$$4^{12} = 16777216$$\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = 330 \\times \\frac{6561}{16777216} = \\frac{2165130}{16777216} \\approx 0.1290518$$\nRounding to four significant figures, the result is $0.1291$.", "answer": "$$\\boxed{0.1291}$$", "id": "3104618"}, {"introduction": "Beyond updating beliefs, the choice of prior distribution is a powerful tool for embedding assumptions into a model and managing statistical challenges like collinearity. In this linear regression exercise, you will compare a standard Gaussian prior, which performs regularization, with a more sophisticated spike-and-slab prior designed for variable selection. This practice [@problem_id:3104643] will illustrate how different priors can resolve parameter non-identifiability and lead to different insights about which features are important.", "problem": "Consider the linear regression model in Bayesian statistical learning with a fixed design matrix and known noise variance. Let $X \\in \\mathbb{R}^{n \\times d}$ be the design matrix, $w \\in \\mathbb{R}^{d}$ be the parameter vector, and $y \\in \\mathbb{R}^{n}$ be the observed responses. The likelihood is $y \\mid w \\sim \\mathcal{N}\\!\\left(X w, \\sigma^2 I_n\\right)$, where $\\sigma^2  0$ is known. You will compare the posterior variance of $w$ under two priors and analyze identifiability in the presence of collinearity in $X$.\n\nThe two priors are:\n- A ridge-like Gaussian prior: $w \\sim \\mathcal{N}\\!\\left(0, \\tau^2 I_d\\right)$ with $\\tau^2  0$.\n- A spike-and-slab prior defined in a decorrelated basis obtained by the singular value decomposition. Let the singular value decomposition be $X = U S V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{d \\times d}$ are orthogonal matrices and $S \\in \\mathbb{R}^{n \\times d}$ has nonnegative singular values on its diagonal. Define $z = V^\\top w$. For each coordinate $z_i$ associated with a nonzero singular value $s_i$, use the prior $z_i \\sim \\pi \\,\\mathcal{N}\\!\\left(0, v_1\\right) + \\left(1 - \\pi\\right)\\,\\mathcal{N}\\!\\left(0, v_0\\right)$ with $0  \\pi  1$, $v_0 \\ll v_1$, and for coordinates orthogonal to the column space of $X$ (corresponding to zero singular values), use the same mixture prior. This mixture prior is a well-known formalization of a spike-and-slab prior in the transformed coefficient space.\n\nYour task is to compute, for each test case given below:\n1. The posterior covariance trace of $w$ under the ridge-like Gaussian prior.\n2. The posterior covariance trace of $w$ under the spike-and-slab prior using the singular value decomposition basis and exact one-dimensional conjugate updates within each singular component.\n3. The ratio of the spike-and-slab posterior covariance trace to the ridge posterior covariance trace.\n4. The matrix rank of $X$.\n\nYour program must implement the following principle-based computations:\n- Use Bayes' rule and linear Gaussian conjugacy to derive the ridge posterior and its covariance. The posterior covariance under the ridge prior is $(X^\\top X / \\sigma^2 + I_d / \\tau^2)^{-1}$, and the trace is the sum of its diagonal entries.\n- For the spike-and-slab prior, rotate into the singular value decomposition basis. Let $X = U S V^\\top$, let $r$ be the numerical rank of $X$ (the number of strictly positive singular values), let $U_r \\in \\mathbb{R}^{n \\times r}$ be the leading $r$ columns of $U$, and let $s_1, \\dots, s_r$ be the positive singular values. Define the sufficient statistics $y_{\\text{tilde}} = U_r^\\top y \\in \\mathbb{R}^r$. For each $i \\in \\{1, \\dots, r\\}$, the likelihood reduces to the one-dimensional observation $y_{\\text{tilde}, i} \\sim \\mathcal{N}\\!\\left(s_i z_i, \\sigma^2\\right)$. Under the mixture prior $z_i \\sim \\pi \\,\\mathcal{N}\\!\\left(0, v_1\\right) + \\left(1 - \\pi\\right)\\,\\mathcal{N}\\!\\left(0, v_0\\right)$, the posterior is a mixture of Gaussians with component-specific posterior variance $v_{i, j}^{\\text{post}} = \\left(s_i^2 / \\sigma^2 + 1 / v_j\\right)^{-1}$ and posterior mean $m_{i, j}^{\\text{post}} = v_{i, j}^{\\text{post}} \\,(s_i / \\sigma^2)\\, y_{\\text{tilde}, i}$ for $j \\in \\{0, 1\\}$ denoting the spike $(v_0)$ or slab $(v_1)$ component. The posterior component weights are\n$$\n\\gamma_{i, j} = \\frac{\\pi_j \\, \\mathcal{N}\\!\\left(y_{\\text{tilde}, i}; 0, \\sigma^2 + s_i^2 v_j\\right)}{\\sum_{k \\in \\{0, 1\\}} \\pi_k \\, \\mathcal{N}\\!\\left(y_{\\text{tilde}, i}; 0, \\sigma^2 + s_i^2 v_k\\right)},\n$$\nwhere $\\pi_1 = \\pi$, $\\pi_0 = 1 - \\pi$. The posterior variance of $z_i$ is\n$$\n\\operatorname{Var}(z_i \\mid y) = \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} \\, v_{i, j}^{\\text{post}} + \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j}\\, \\left(m_{i, j}^{\\text{post}}\\right)^2 - \\left(\\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j}\\, m_{i, j}^{\\text{post}}\\right)^2.\n$$\nFor the $d - r$ coordinates orthogonal to the column space (zero singular values), the posterior equals the prior, giving posterior variance $\\pi v_1 + (1 - \\pi) v_0$ per coordinate. The trace of the posterior covariance of $w$ equals the sum of the posterior variances across all coordinates in the singular value decomposition basis because trace is invariant under orthogonal change of basis.\n\nExplain the identifiability implications in your solution: under collinearity, there are directions in parameter space that the data cannot inform (null space directions), and each prior resolves non-identifiability differently.\n\nUse the following fixed test suite. Every number given below must be treated exactly as specified. The parameters $\\sigma^2$, $\\tau^2$, $\\pi$, $v_0$, and $v_1$ are scalars, and the program must not introduce any randomness.\n\nTest case $1$ (exact collinearity, moderate noise):\n$$\nX_1 =\n\\begin{bmatrix}\n1  1  0 \\\\\n2  2  1 \\\\\n3  3  1 \\\\\n4  4  2 \\\\\n5  5  3 \\\\\n6  6  3 \\\\\n7  7  4 \\\\\n8  8  4\n\\end{bmatrix},\\quad\ny_1 =\n\\begin{bmatrix}\n1.0 \\\\\n2.1 \\\\\n3.1 \\\\\n4.2 \\\\\n5.3 \\\\\n6.3 \\\\\n7.4 \\\\\n8.4\n\\end{bmatrix},\\quad\n\\sigma^2_1 = 0.25,\\quad \\tau^2_1 = 1.0,\\quad \\pi_1 = 0.5,\\quad v_{0,1} = 10^{-6},\\quad v_{1,1} = 1.0.\n$$\n\nTest case $2$ (near collinearity, moderate noise):\n$$\nX_2 =\n\\begin{bmatrix}\n1  0.999  0 \\\\\n2  2.001  1 \\\\\n3  2.998  1 \\\\\n4  4.002  2 \\\\\n5  4.997  3 \\\\\n6  6.003  3 \\\\\n7  6.996  4 \\\\\n8  8.004  4\n\\end{bmatrix},\\quad\ny_2 =\n\\begin{bmatrix}\n0.9995 \\\\\n2.1005 \\\\\n3.099 \\\\\n4.201 \\\\\n5.2985 \\\\\n6.3015 \\\\\n7.398 \\\\\n8.402\n\\end{bmatrix},\\quad\n\\sigma^2_2 = 0.25,\\quad \\tau^2_2 = 1.0,\\quad \\pi_2 = 0.5,\\quad v_{0,2} = 10^{-6},\\quad v_{1,2} = 1.0.\n$$\n\nTest case $3$ (full rank, low noise):\n$$\nX_3 =\n\\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1 \\\\\n2  0 \\\\\n0  2 \\\\\n2  2\n\\end{bmatrix},\\quad\ny_3 =\n\\begin{bmatrix}\n0.6 \\\\\n0.4 \\\\\n1.0 \\\\\n1.2 \\\\\n0.8 \\\\\n2.0\n\\end{bmatrix},\\quad\n\\sigma^2_3 = 0.01,\\quad \\tau^2_3 = 1.0,\\quad \\pi_3 = 0.5,\\quad v_{0,3} = 10^{-6},\\quad v_{1,3} = 1.0.\n$$\n\nTest case $4$ (exact collinearity, high noise, strong spike):\n$$\nX_4 =\n\\begin{bmatrix}\n1  1  0 \\\\\n2  2  1 \\\\\n3  3  1 \\\\\n4  4  2 \\\\\n5  5  3 \\\\\n6  6  3 \\\\\n7  7  4 \\\\\n8  8  4\n\\end{bmatrix},\\quad\ny_4 =\n\\begin{bmatrix}\n1.0 \\\\\n2.1 \\\\\n3.1 \\\\\n4.2 \\\\\n5.3 \\\\\n6.3 \\\\\n7.4 \\\\\n8.4\n\\end{bmatrix},\\quad\n\\sigma^2_4 = 5.0,\\quad \\tau^2_4 = 1.0,\\quad \\pi_4 = 0.1,\\quad v_{0,4} = 10^{-6},\\quad v_{1,4} = 1.0.\n$$\n\nYour program must compute, for each test case $t \\in \\{1, 2, 3, 4\\}$, a list $[\\operatorname{trace}_{\\text{ridge}, t}, \\operatorname{trace}_{\\text{spike\\text{-}slab}, t}, \\operatorname{ratio}_t, \\operatorname{rank}(X_t)]$, where $\\operatorname{ratio}_t = \\operatorname{trace}_{\\text{spike\\text{-}slab}, t} / \\operatorname{trace}_{\\text{ridge}, t}$, and then aggregate all four lists into a single line of output.\n\nFinal output format: your program should produce a single line containing a comma-separated list of the four per-test-case lists, enclosed in square brackets, for example $[\\,[\\cdot, \\cdot, \\cdot, \\cdot], [\\cdot, \\cdot, \\cdot, \\cdot], [\\cdot, \\cdot, \\cdot, \\cdot], [\\cdot, \\cdot, \\cdot, \\cdot]\\,]$. All numeric answers must be in plain floating-point representation or integers as appropriate. No physical units or angles are involved in this problem.", "solution": "The problem requires the analysis and comparison of posterior variances for a Bayesian linear regression model under two different prior distributions for the parameter vector $w$: a ridge-like Gaussian prior and a spike-and-slab mixture prior. The analysis is performed for several test cases, including scenarios with collinearity in the design matrix $X$.\n\n### Model Specification\nThe linear regression model is defined by the likelihood of the observed responses $y \\in \\mathbb{R}^n$ given the design matrix $X \\in \\mathbb{R}^{n \\times d}$ and the parameter vector $w \\in \\mathbb{R}^d$:\n$$\np(y \\mid X, w, \\sigma^2) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n)\n$$\nwhere $\\sigma^2  0$ is the known noise variance.\n\n### 1. Ridge-like Gaussian Prior\nThe first prior is a zero-mean isotropic Gaussian distribution on the parameters $w$:\n$$\np(w \\mid \\tau^2) = \\mathcal{N}(w \\mid 0, \\tau^2 I_d)\n$$\nwhere $\\tau^2  0$ is the prior variance. This prior corresponds to the regularizer used in Ridge regression.\n\nDue to the conjugacy of the Gaussian prior with the Gaussian likelihood, the posterior distribution $p(w \\mid y, X, \\sigma^2, \\tau^2)$ is also Gaussian. The posterior covariance matrix is given by the formula:\n$$\n\\Sigma_{\\text{ridge}} = \\left( \\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d \\right)^{-1}\n$$\nThe quantity to be computed is the trace of this posterior covariance matrix, $\\operatorname{trace}(\\Sigma_{\\text{ridge}})$, which represents the sum of the posterior variances of the individual parameter components $w_i$.\n\n### 2. Spike-and-Slab Prior\nThe second prior is a spike-and-slab prior, which is a mixture of two Gaussian distributions: one with a very small variance (the \"spike\") and one with a larger variance (the \"slab\"). This prior is defined in a decorrelated basis obtained from the Singular Value Decomposition (SVD) of the design matrix $X = U S V^\\top$.\n\nThe parameter vector $w$ is transformed into a new basis $z = V^\\top w$. Since $V$ is an orthogonal matrix, this is a rotation. The prior is specified on the components of $z$:\n$$\np(z_i) = \\pi \\mathcal{N}(z_i \\mid 0, v_1) + (1-\\pi) \\mathcal{N}(z_i \\mid 0, v_0)\n$$\nfor each $i \\in \\{1, \\dots, d\\}$, where $v_0$ is the small spike variance, $v_1$ is the larger slab variance, and $\\pi$ is the mixture probability of belonging to the slab component.\n\nIn the transformed basis, the likelihood model decouples. Let $\\tilde{y} = U^\\top y$. The likelihood for each component $\\tilde{y}_i$ becomes:\n$$\np(\\tilde{y}_i \\mid z_i, s_i, \\sigma^2) = \\mathcal{N}(\\tilde{y}_i \\mid s_i z_i, \\sigma^2)\n$$\nwhere $s_i$ is the $i$-th singular value of $X$.\n\nFor each $z_i$, we have a one-dimensional Bayesian inference problem. The posterior distribution of $z_i$ given $\\tilde{y}_i$ is a mixture of two Gaussians.\nThe posterior probability (responsibility) that $z_i$ came from component $j \\in \\{0, 1\\}$ (where $j=0$ denotes the spike and $j=1$ the slab) is given by Bayes' rule:\n$$\n\\gamma_{i, j} = \\frac{\\pi_j p(\\tilde{y}_i \\mid \\text{component } j)}{\\sum_{k \\in \\{0, 1\\}} \\pi_k p(\\tilde{y}_i \\mid \\text{component } k)}\n$$\nwhere $\\pi_1 = \\pi$, $\\pi_0 = 1 - \\pi$, and $p(\\tilde{y}_i \\mid \\text{component } j)$ is the marginal likelihood $\\int p(\\tilde{y}_i \\mid z_i) p(z_i \\mid \\text{component } j) dz_i = \\mathcal{N}(\\tilde{y}_i \\mid 0, \\sigma^2 + s_i^2 v_j)$.\n\nThe posterior for $z_i$ is $p(z_i | y) = \\sum_{j \\in \\{0, 1\\}} \\gamma_{i,j} \\mathcal{N}(z_i | m_{i,j}^{\\text{post}}, v_{i,j}^{\\text{post}})$, where the component posterior means and variances are:\n$$\nv_{i, j}^{\\text{post}} = \\left(\\frac{s_i^2}{\\sigma^2} + \\frac{1}{v_j}\\right)^{-1}\n$$\n$$\nm_{i, j}^{\\text{post}} = v_{i, j}^{\\text{post}} \\left(\\frac{s_i}{\\sigma^2}\\right) \\tilde{y}_i\n$$\nThe total posterior variance of $z_i$ is found using the law of total variance:\n$$\n\\operatorname{Var}(z_i \\mid y) = \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} v_{i, j}^{\\text{post}} + \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} \\left(m_{i, j}^{\\text{post}}\\right)^2 - \\left(\\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} m_{i, j}^{\\text{post}}\\right)^2\n$$\nThe total trace of the posterior covariance of $w$ is the sum of the posterior variances of the $z_i$ components due to the trace's invariance under orthogonal transformation ($w=Vz$):\n$$\n\\operatorname{trace}(\\operatorname{Cov}(w \\mid y)) = \\operatorname{trace}(\\operatorname{Cov}(Vz \\mid y)) = \\operatorname{trace}(V \\operatorname{Cov}(z \\mid y) V^\\top) = \\operatorname{trace}(\\operatorname{Cov}(z \\mid y)) = \\sum_{i=1}^d \\operatorname{Var}(z_i \\mid y)\n$$\nsince the posteriors for $z_i$ are independent.\n\n### 3. Identifiability and Collinearity\nCollinearity in $X$ means that some columns are linearly dependent, so $X$ is not full rank. This implies that $X^\\top X$ is singular and has a non-trivial null space. For any vector $w_0$ in the null space of $X$, $Xw = X(w+w_0)$, meaning the likelihood is flat along the directions defined by the null space. The parameters are non-identifiable from the data alone.\n\n- **Ridge Prior**: This prior resolves non-identifiability by adding a positive definite matrix $I_d/\\tau^2$ to the semi-definite precision matrix $X^\\top X/\\sigma^2$. The resulting posterior precision is always invertible. This is equivalent to adding a penalty term in frequentist statistics, which shrinks all parameters towards the prior mean (zero). For directions in the null space of $X$, the posterior for $w$ is simply the prior, with variance $\\tau^2$.\n\n- **Spike-and-Slab Prior**: This prior handles non-identifiability in the SVD basis. If a singular value $s_i$ is zero, the corresponding direction in parameter space is not informed by the data (the likelihood for $\\tilde{y}_i$ does not depend on $z_i$). Consequently, the posterior for $z_i$ is identical to its prior. The posterior variance for this component is the prior variance of the mixture, $\\operatorname{Var}(z_i) = \\pi v_1 + (1-\\pi)v_0$. The model thus isolates the non-identifiable components and relies on the prior for them, while using the data to update the identifiable components. For nearly collinear cases (very small but non-zero $s_i$), the data provides weak information. The spike-and-slab prior can adapt by assigning a high posterior probability to the \"spike\" component, effectively shrinking the parameter $z_i$ towards zero and acknowledging that it is not well-determined by the data. This provides a more nuanced shrinkage than the uniform shrinkage of the ridge prior.\n\nThe following Python code implements these calculations for the provided test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian linear regression problem for four test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        (\n            np.array([\n                [1, 1, 0], [2, 2, 1], [3, 3, 1], [4, 4, 2],\n                [5, 5, 3], [6, 6, 3], [7, 7, 4], [8, 8, 4]\n            ]),\n            np.array([1.0, 2.1, 3.1, 4.2, 5.3, 6.3, 7.4, 8.4]),\n            0.25, 1.0, 0.5, 1e-6, 1.0\n        ),\n        # Test case 2\n        (\n            np.array([\n                [1, 0.999, 0], [2, 2.001, 1], [3, 2.998, 1], [4, 4.002, 2],\n                [5, 4.997, 3], [6, 6.003, 3], [7, 6.996, 4], [8, 8.004, 4]\n            ]),\n            np.array([0.9995, 2.1005, 3.099, 4.201, 5.2985, 6.3015, 7.398, 8.402]),\n            0.25, 1.0, 0.5, 1e-6, 1.0\n        ),\n        # Test case 3\n        (\n            np.array([\n                [1, 0], [0, 1], [1, 1], [2, 0], [0, 2], [2, 2]\n            ]),\n            np.array([0.6, 0.4, 1.0, 1.2, 0.8, 2.0]),\n            0.01, 1.0, 0.5, 1e-6, 1.0\n        ),\n        # Test case 4\n        (\n            np.array([\n                [1, 1, 0], [2, 2, 1], [3, 3, 1], [4, 4, 2],\n                [5, 5, 3], [6, 6, 3], [7, 7, 4], [8, 8, 4]\n            ]),\n            np.array([1.0, 2.1, 3.1, 4.2, 5.3, 6.3, 7.4, 8.4]),\n            5.0, 1.0, 0.1, 1e-6, 1.0\n        )\n    ]\n\n    all_results = []\n    for case in test_cases:\n        X, y, sigma2, tau2, pi, v0, v1 = case\n        n, d = X.shape\n\n        # 1. Ridge posterior covariance trace\n        Id = np.identity(d)\n        precision_ridge = (X.T @ X) / sigma2 + Id / tau2\n        cov_ridge = np.linalg.inv(precision_ridge)\n        trace_ridge = np.trace(cov_ridge)\n\n        # 4. Matrix rank\n        rank_X = np.linalg.matrix_rank(X)\n\n        # 2. Spike-and-slab posterior covariance trace\n        _U, s_vals_full, Vt = np.linalg.svd(X, full_matrices=True)\n        s_vals = np.zeros(d)\n        s_vals[:len(s_vals_full)] = s_vals_full\n        U = _U[:, :d] # Ensure U is n x d if n > d\n        \n        y_tilde = U.T @ y\n\n        trace_ss = 0.0\n\n        for i in range(d):\n            s_i = s_vals[i]\n            y_tilde_i = y_tilde[i]\n\n            # If singular value is effectively zero, the posterior is the prior\n            if s_i  1e-9: # Numerical tolerance for zero singular value\n                trace_ss += pi * v1 + (1 - pi) * v0\n                continue\n\n            # Use log-likelihoods for numerical stability\n            log_marginal_lik_0 = np.log(1 - pi) + norm.logpdf(y_tilde_i, loc=0, scale=np.sqrt(sigma2 + s_i**2 * v0))\n            log_marginal_lik_1 = np.log(pi) + norm.logpdf(y_tilde_i, loc=0, scale=np.sqrt(sigma2 + s_i**2 * v1))\n\n            # Log-sum-exp trick to get posterior weights (gammas)\n            if log_marginal_lik_0 > log_marginal_lik_1:\n                gamma0 = 1.0 / (1.0 + np.exp(log_marginal_lik_1 - log_marginal_lik_0))\n                gamma1 = 1.0 - gamma0\n            else:\n                gamma1 = 1.0 / (1.0 + np.exp(log_marginal_lik_0 - log_marginal_lik_1))\n                gamma0 = 1.0 - gamma1\n\n            # Component-specific posterior parameters\n            v_post0 = 1.0 / (s_i**2 / sigma2 + 1.0 / v0)\n            v_post1 = 1.0 / (s_i**2 / sigma2 + 1.0 / v1)\n\n            m_post0 = v_post0 * (s_i / sigma2) * y_tilde_i\n            m_post1 = v_post1 * (s_i / sigma2) * y_tilde_i\n\n            # Posterior variance for z_i using law of total variance\n            var_z_i = (gamma0 * v_post0 + gamma1 * v_post1) + \\\n                      (gamma0 * m_post0**2 + gamma1 * m_post1**2) - \\\n                      (gamma0 * m_post0 + gamma1 * m_post1)**2\n            \n            trace_ss += var_z_i\n        \n        # 3. Ratio\n        ratio = trace_ss / trace_ridge\n\n        all_results.append([trace_ridge, trace_ss, ratio, float(rank_X)])\n\n    # Format output as specified\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3104643"}, {"introduction": "Real-world datasets often contain outliers or high-leverage points that can unduly influence statistical models. This final practice explores how Bayesian methods can build robust models by carefully selecting prior distributions for the parameters of a logistic regression classifier. By contrasting a standard Gaussian prior with a heavy-tailed Cauchy prior [@problem_id:3104628], you will gain hands-on experience in how to make your models less sensitive to extreme observations, a crucial skill for practical machine learning.", "problem": "You are asked to implement and analyze Bayesian logistic regression with a heavy-tailed prior on coefficients to assess posterior behavior in the presence of high leverage points. The goal is to derive the necessary expressions from first principles and then compute the Maximum A Posteriori (MAP) estimate and a Laplace approximation to the posterior. You will construct an algorithm to handle both a standard Gaussian prior and a robust Cauchy prior on the slope coefficient.\n\nStart from the following fundamental base:\n- Bayesâ€™ theorem: given data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, parameters $\\theta$, prior $p(\\theta)$, and likelihood $p(\\mathcal{D} \\mid \\theta)$, the posterior is $p(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) p(\\theta)$.\n- Logistic regression model with Bernoulli outcomes: $y_i \\in \\{0,1\\}$, with $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$ and $\\pi_i = \\sigma(\\eta_i)$ where $\\eta_i = \\beta_0 + \\beta_1 x_i$ and $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n- Log-likelihood additivity for independent observations: $\\log p(\\mathcal{D} \\mid \\theta) = \\sum_{i=1}^n \\log p(y_i \\mid \\theta)$.\n\nYou must:\n1) Derive the log-posterior for $(\\beta_0,\\beta_1)$ under two priors:\n- Intercept prior for all cases: $\\beta_0 \\sim \\mathcal{N}(0,\\sigma_0^2)$ with $\\sigma_0 = 10$.\n- Slope prior case A (Gaussian): $\\beta_1 \\sim \\mathcal{N}(0, s^2)$ with $s = 2.5$.\n- Slope prior case B (Cauchy): $\\beta_1 \\sim \\mathrm{Cauchy}(0, s)$ with $s = 2.5$.\n2) From the definitions, derive the gradient and Hessian of the log-posterior with respect to $(\\beta_0,\\beta_1)$.\n3) Implement a numerical optimizer to find the Maximum A Posteriori (MAP) estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)$ by minimizing the negative log-posterior. Then compute a Laplace approximation by inverting the negative Hessian of the log-posterior at the MAP to obtain an approximate posterior covariance matrix.\n4) Evaluate the posterior behavior with and without high leverage points by computing a posterior-predictive point estimate via the MAP at a target covariate value $x^{\\star} = 3$. Specifically, compute $\\hat{p}(y=1 \\mid x^{\\star}) = \\sigma(\\hat{\\beta}_0 + \\hat{\\beta}_1 x^{\\star})$.\n\nTest suite:\nUse the following deterministic datasets to exercise the algorithm. Each dataset is a list of pairs $(x_i,y_i)$:\n\n- Dataset $\\mathcal{D}_0$ (baseline): $[(-2,0),(-1,0),(0,0),(1,1),(2,1)]$.\n- Dataset $\\mathcal{D}_+$ (add a consistent high leverage point): $\\mathcal{D}_0 \\cup \\{(50,1)\\}$.\n- Dataset $\\mathcal{D}_-$ (add a contradictory high leverage point): $\\mathcal{D}_0 \\cup \\{(50,0)\\}$.\n- Dataset $\\mathcal{D}_{\\mathrm{sep}}$ (perfect separation boundary case): $[(-2,0),(-1,0),(1,1),(2,1)]$.\n\nFor each dataset, run the model twice, once with the Gaussian slope prior and once with the Cauchy slope prior, keeping the intercept prior fixed as specified. For each run, compute the single scalar $\\hat{p}(y=1 \\mid x^{\\star}=3)$ using the MAP as described above.\n\nRequired final output format:\n- Your program must produce a single line containing a comma-separated list of $8$ floating-point numbers enclosed in square brackets, ordered as:\n$[\\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_0), \\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_+), \\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_-), \\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_{\\mathrm{sep}}), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_0), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_+), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_-), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_{\\mathrm{sep}})]$.\n- Each number should be rounded to exactly $6$ decimal places.\n- There are no physical units involved.\n- Angles are not involved.\n- Percentages must not be used; probabilities must be reported as decimals in $[0,1]$.\n\nScientific realism and constraints:\n- Use only the definitions and basic calculus to derive the necessary expressions.\n- Use numerically stable computations for the logistic function to avoid overflow for large $\\lvert x \\rvert$, especially for $x = 50$.\n- The output must be reproducible and must not require any randomness.\n\nYour final program must be self-contained, require no input, and print exactly the one-line output in the specified format.", "solution": "The problem requires the implementation and analysis of Bayesian logistic regression under two different prior distributions for the slope coefficient, a Gaussian (Normal) prior and a heavy-tailed Cauchy prior. The goal is to find the Maximum A Posteriori (MAP) estimate of the model parameters and use it to make a prediction, assessing the influence of high-leverage data points. The derivation will proceed from first principles as requested.\n\nLet the model parameters be $\\beta = (\\beta_0, \\beta_1)^T$. The data are $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, where $y_i \\in \\{0, 1\\}$.\n\n**1. Model Specification**\n\nThe logistic regression model assumes that the outcomes $y_i$ are drawn from a Bernoulli distribution with a probability $\\pi_i$ that depends on the covariate $x_i$:\n$$y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$$\nThe probability $\\pi_i$ is related to a linear predictor $\\eta_i = \\beta_0 + \\beta_1 x_i$ via the logistic (sigmoid) function $\\sigma(z) = \\frac{1}{1+e^{-z}}$:\n$$\\pi_i = \\sigma(\\eta_i) = \\sigma(\\beta_0 + \\beta_1 x_i)$$\n\nThe likelihood of a single observation $(x_i, y_i)$ given the parameters $\\beta$ is $p(y_i \\mid \\beta) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}$. For an independent and identically distributed dataset, the total log-likelihood is the sum of individual log-likelihoods:\n$$\\mathcal{LL}(\\beta) = \\log p(\\mathcal{D} \\mid \\beta) = \\sum_{i=1}^n \\left[ y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i) \\right]$$\nA more convenient and numerically stable form is derived by substituting $\\pi_i = \\sigma(\\eta_i)$:\n$$\\mathcal{LL}(\\beta) = \\sum_{i=1}^n \\left[ y_i \\eta_i - \\log(1+e^{\\eta_i}) \\right]$$\nwhere $\\log(1+e^{\\eta_i})$ is the softplus function.\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior: $p(\\beta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\beta)p(\\beta)$. We consider two cases for the prior on the slope coefficient $\\beta_1$, while the prior on the intercept $\\beta_0$ is fixed.\n- **Intercept Prior:** $\\beta_0 \\sim \\mathcal{N}(0, \\sigma_0^2)$ with $\\sigma_0 = 10$.\n  The log-prior is $\\log p(\\beta_0) = -\\frac{\\beta_0^2}{2\\sigma_0^2} + C_0$.\n- **Slope Prior Case A (Gaussian):** $\\beta_1 \\sim \\mathcal{N}(0, s^2)$ with $s = 2.5$.\n  The log-prior is $\\log p(\\beta_1) = -\\frac{\\beta_1^2}{2s^2} + C_{1,G}$.\n- **Slope Prior Case B (Cauchy):** $\\beta_1 \\sim \\mathrm{Cauchy}(0, s)$ with $s = 2.5$.\n  The probability density function is $p(\\beta_1) = \\frac{1}{\\pi s (1+(\\beta_1/s)^2)} \\propto \\frac{1}{s^2+\\beta_1^2}$.\n  The log-prior is $\\log p(\\beta_1) = -\\log(s^2+\\beta_1^2) + C_{1,C}$.\n\n**2. Objective Function for MAP Estimation**\n\nThe MAP estimate $\\hat{\\beta}$ is the mode of the posterior distribution. It can be found by maximizing the log-posterior, which is equivalent to minimizing the negative log-posterior. Let $f(\\beta)$ be this objective function, ignoring constants.\n\n**Case A (Gaussian slope prior):** The total negative log-posterior is the sum of the negative log-likelihood and the negative log-priors (which correspond to regularization terms):\n$$f_G(\\beta) = \\underbrace{\\sum_{i=1}^n \\left[ \\log(1+e^{\\eta_i}) - y_i \\eta_i \\right]}_{\\text{Negative Log-Likelihood}} + \\underbrace{\\frac{\\beta_0^2}{2\\sigma_0^2}}_{\\text{Prior on } \\beta_0} + \\underbrace{\\frac{\\beta_1^2}{2s^2}}_{\\text{Prior on } \\beta_1}$$\n\n**Case B (Cauchy slope prior):**\n$$f_C(\\beta) = \\sum_{i=1}^n \\left[ \\log(1+e^{\\eta_i}) - y_i \\eta_i \\right] + \\frac{\\beta_0^2}{2\\sigma_0^2} + \\log(s^2 + \\beta_1^2)$$\nThe Gaussian prior applies a quadratic penalty to $\\beta_1$, which is sensitive to outliers. The Cauchy prior's penalty, $\\log(s^2+\\beta_1^2)$, grows much slower, making it more robust.\n\n**3. Gradient and Hessian for Optimization**\n\nTo find the minimum of $f(\\beta)$, we employ Newton's method, which requires the gradient $\\nabla f(\\beta)$ and Hessian $\\nabla^2 f(\\beta)$. We first compute these for the negative log-likelihood part, $f_{LL}(\\beta)$.\nUsing the property $\\frac{d}{dz}\\log(1+e^z) = \\sigma(z)$, the gradient is:\n$$\\nabla f_{LL}(\\beta) = \\sum_{i=1}^n \\begin{pmatrix} \\frac{\\partial \\eta_i}{\\partial \\beta_0}(\\sigma(\\eta_i) - y_i) \\\\ \\frac{\\partial \\eta_i}{\\partial \\beta_1}(\\sigma(\\eta_i) - y_i) \\end{pmatrix} = \\sum_{i=1}^n \\begin{pmatrix} \\pi_i - y_i \\\\ (\\pi_i-y_i)x_i \\end{pmatrix}$$\nUsing the property $\\frac{d}{dz}\\sigma(z) = \\sigma(z)(1-\\sigma(z))$, the Hessian is:\n$$\\nabla^2 f_{LL}(\\beta) = \\sum_{i=1}^n \\begin{pmatrix} \\frac{\\partial^2 f_{LL}}{\\partial \\beta_0^2}  \\frac{\\partial^2 f_{LL}}{\\partial \\beta_0 \\partial \\beta_1} \\\\ \\frac{\\partial^2 f_{LL}}{\\partial \\beta_1 \\partial \\beta_0}  \\frac{\\partial^2 f_{LL}}{\\partial \\beta_1^2} \\end{pmatrix} = \\sum_{i=1}^n \\pi_i(1-\\pi_i) \\begin{pmatrix} 1  x_i \\\\ x_i  x_i^2 \\end{pmatrix}$$\n\nThe total gradient and Hessian are the sum of the likelihood parts and the prior parts.\n**Case A (Gaussian):**\n$$\\nabla f_G(\\beta) = \\nabla f_{LL}(\\beta) + \\begin{pmatrix} \\beta_0/\\sigma_0^2 \\\\ \\beta_1/s^2 \\end{pmatrix}$$\n$$\\nabla^2 f_G(\\beta) = \\nabla^2 f_{LL}(\\beta) + \\begin{pmatrix} 1/\\sigma_0^2  0 \\\\ 0  1/s^2 \\end{pmatrix}$$\nThe objective function $f_G(\\beta)$ is convex, guaranteeing a unique minimum.\n\n**Case B (Cauchy):**\n$$\\nabla f_C(\\beta) = \\nabla f_{LL}(\\beta) + \\begin{pmatrix} \\beta_0/\\sigma_0^2 \\\\ 2\\beta_1/(s^2+\\beta_1^2) \\end{pmatrix}$$\n$$\\nabla^2 f_C(\\beta) = \\nabla^2 f_{LL}(\\beta) + \\begin{pmatrix} 1/\\sigma_0^2  0 \\\\ 0  (2s^2 - 2\\beta_1^2)/(s^2+\\beta_1^2)^2 \\end{pmatrix}$$\nThe objective function $f_C(\\beta)$ is not guaranteed to be convex because the Hessian term for the Cauchy prior can be negative if $|\\beta_1|  s$. However, for typical datasets, a unique MAP estimate is still found.\n\n**4. Numerical Solution**\n\nThe MAP estimate $\\hat{\\beta}$ is found by initializing $\\beta^{(0)} = (0,0)^T$ and applying the Newton-Raphson update rule iteratively until convergence:\n$$\\beta^{(k+1)} = \\beta^{(k)} - [\\nabla^2 f(\\beta^{(k)})]^{-1} \\nabla f(\\beta^{(k)})$$\nThe Laplace approximation to the posterior $p(\\beta \\mid \\mathcal{D})$ is a Gaussian distribution $\\mathcal{N}(\\hat{\\beta}, \\Sigma)$ where the covariance is the inverse of the negative log-posterior's Hessian at the MAP, $\\Sigma = [\\nabla^2 f(\\hat{\\beta})]^{-1}$. While the Hessian matrix is computed for the optimization, its inverse is not needed for the final output.\n\nOnce $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)$ is found, the posterior predictive probability for a new data point $x^{\\star}$ is computed using the MAP estimate:\n$$\\hat{p}(y=1 \\mid x^{\\star}, \\mathcal{D}) \\approx \\sigma(\\hat{\\beta}_0 + \\hat{\\beta}_1 x^{\\star})$$\nThis procedure will be applied to each dataset under both the Gaussian and Cauchy prior specifications.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Bayesian logistic regression with Gaussian and Cauchy priors to find\n    MAP estimates and compute posterior predictive probabilities.\n    \"\"\"\n    # Define problem parameters from the statement.\n    sigma0 = 10.0\n    s = 2.5\n    x_star = 3.0\n\n    # Define the test cases from the problem statement.\n    d0_data = np.array([[-2.0, 0.0], [-1.0, 0.0], [0.0, 0.0], [1.0, 1.0], [2.0, 1.0]])\n    datasets = {\n        'D0': d0_data,\n        'D+': np.vstack([d0_data, [50.0, 1.0]]),\n        'D-': np.vstack([d0_data, [50.0, 0.0]]),\n        'D_sep': np.array([[-2.0, 0.0], [-1.0, 0.0], [1.0, 1.0], [2.0, 1.0]])\n    }\n    dataset_order = ['D0', 'D+', 'D-', 'D_sep']\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        z = np.asarray(z)\n        # Use np.exp's behavior with vectors/scalars to handle both cases efficiently.\n        return np.where(z >= 0, \n                        1 / (1 + np.exp(-z)), \n                        np.exp(z) / (1 + np.exp(z)))\n\n    def find_map_beta(X, y, prior_type):\n        \"\"\"\n        Finds the Maximum A Posteriori (MAP) estimate for beta using Newton's method.\n        The parameters sigma0 and s are taken from the outer scope.\n        \"\"\"\n        beta = np.zeros(2, dtype=np.float64)\n        num_iterations = 30  # Ample for convergence in these cases\n        tolerance = 1e-9\n\n        for _ in range(num_iterations):\n            b0, b1 = beta[0], beta[1]\n            eta = X @ beta\n            pi = stable_sigmoid(eta)\n\n            # Gradient and Hessian of the negative log-likelihood part.\n            grad_ll = X.T @ (pi - y)\n            weights = pi * (1 - pi)\n            hess_ll = (X.T * weights) @ X\n\n            # Add contributions from the negative log-prior.\n            if prior_type == 'cauchy':\n                # Gaussian prior for beta0\n                grad_b0_prior = b0 / sigma0**2\n                hess_b0_prior = 1 / sigma0**2\n                # Cauchy prior for beta1\n                denom_b1 = (s**2 + b1**2)\n                grad_b1_prior = (2 * b1) / denom_b1\n                hess_b1_prior = (2 * s**2 - 2 * b1**2) / (denom_b1**2)\n            elif prior_type == 'normal':\n                # Gaussian prior for beta0\n                grad_b0_prior = b0 / sigma0**2\n                hess_b0_prior = 1 / sigma0**2\n                # Gaussian prior for beta1\n                grad_b1_prior = b1 / s**2\n                hess_b1_prior = 1 / s**2\n            \n            # Combine likelihood and prior parts to get posterior derivatives.\n            gradient = grad_ll + np.array([grad_b0_prior, grad_b1_prior])\n            hessian = hess_ll + np.diag([hess_b0_prior, hess_b1_prior])\n            \n            # Newton-Raphson update step. Use solve for numerical stability.\n            try:\n                step = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular (unlikely with priors).\n                step = np.linalg.pinv(hessian) @ -gradient\n\n            beta += step\n            \n            if np.linalg.norm(step)  tolerance:\n                break\n        \n        return beta\n\n    cauchy_results = []\n    normal_results = []\n\n    for ds_name in dataset_order:\n        data = datasets[ds_name]\n        y = data[:, 1]\n        X = np.c_[np.ones(len(y)), data[:, 0]]\n        \n        # --- Case B: Cauchy slope prior ---\n        beta_map_cauchy = find_map_beta(X, y, 'cauchy')\n        pred_prob_cauchy = stable_sigmoid(beta_map_cauchy[0] + beta_map_cauchy[1] * x_star)\n        cauchy_results.append(pred_prob_cauchy)\n        \n        # --- Case A: Normal slope prior ---\n        beta_map_normal = find_map_beta(X, y, 'normal')\n        pred_prob_normal = stable_sigmoid(beta_map_normal[0] + beta_map_normal[1] * x_star)\n        normal_results.append(pred_prob_normal)\n\n    # Combine results in the specified order for the final output.\n    final_results = cauchy_results + normal_results\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in final_results)}]\")\n\nsolve()\n```", "id": "3104628"}]}