{"hands_on_practices": [{"introduction": "Anomaly detection algorithms like One-Class SVM and Isolation Forest typically produce a continuous score rather than a binary normal/anomaly label. To convert these scores into actionable decisions, one must establish a threshold. This exercise explores a practical and common approach known as prevalence-based thresholding, where you define the number of anomalies to flag based on an assumed anomaly prevalence in your data. By implementing this method and calculating the resulting precision, you will gain hands-on experience in the crucial step of operationalizing and evaluating the output of anomaly detection models [@problem_id:3099063].", "problem": "You are given anomaly scores from two unsupervised anomaly detection methods for multiple test sets: One-Class Support Vector Machine (OCSVM) and Isolation Forest (IF). For each test set, you must set a prevalence-based threshold by selecting the top $k$ most anomalous points, where $k = \\lfloor \\pi n \\rfloor$, with $\\pi \\in [0,1]$ the target anomaly prevalence and $n$ the number of samples in the test set. Larger scores indicate a higher degree of anomaly. Break any ties at the threshold deterministically by preferring the smaller sample index. Compute the precision for each method under this thresholding rule, where precision is defined as $TP/(TP+FP)$ if $TP+FP > 0$, and defined as $0$ when $TP+FP = 0$.\n\nFundamental definitions and rules:\n- Given a vector of scores $\\mathbf{s} \\in \\mathbb{R}^n$, sort indices by descending score using a lexicographic rule that first orders by $-s_i$ and breaks ties by ascending index $i$. Select the first $k = \\lfloor \\pi n \\rfloor$ indices as predicted anomalies. This implements a prevalence-based threshold via order statistics.\n- True positives are $TP = \\sum_{i=1}^n \\mathbb{I}[\\hat{y}_i = 1 \\wedge y_i = 1]$, false positives are $FP = \\sum_{i=1}^n \\mathbb{I}[\\hat{y}_i = 1 \\wedge y_i = 0]$, and precision is $TP/(TP+FP)$ if $TP+FP > 0$, and $0$ otherwise.\n- When $\\pi = 0$, $k = 0$ and precision is defined to be $0$. When $\\pi = 1$, $k = n$ and precision equals the fraction of anomalies in the dataset.\n\nYour program must compute the precision for both methods on each of the following test cases and output the results in a single line as a comma-separated list enclosed in square brackets in the order specified below. All reported values must be floats rounded to four decimal places.\n\nTest suite:\n1. Test case $1$ (happy path, no ties):\n   - $n = 12$\n   - OCSVM scores $\\mathbf{s}^{(1)}_{\\mathrm{OCSVM}} = [0.10, 0.95, 0.40, 0.30, 0.60, 0.90, 0.20, 0.85, 0.50, 0.55, 0.15, 0.45]$\n   - IF scores $\\mathbf{s}^{(1)}_{\\mathrm{IF}} = [0.12, 0.88, 0.80, 0.65, 0.50, 0.90, 0.85, 0.40, 0.30, 0.45, 0.20, 0.60]$\n   - Ground-truth labels $\\mathbf{y}^{(1)} = [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]$\n   - Target prevalence $\\pi^{(1)} = 0.25$\n2. Test case $2$ (tie at cutoff, tie-break by index):\n   - $n = 10$\n   - OCSVM scores $\\mathbf{s}^{(2)}_{\\mathrm{OCSVM}} = [0.10, 0.50, 0.95, 0.70, 0.40, 0.85, 0.20, 0.60, 0.70, 0.30]$\n   - IF scores $\\mathbf{s}^{(2)}_{\\mathrm{IF}} = [0.30, 0.60, 0.92, 0.70, 0.45, 0.80, 0.20, 0.58, 0.89, 0.40]$\n   - Ground-truth labels $\\mathbf{y}^{(2)} = [0, 0, 1, 0, 0, 0, 0, 0, 1, 0]$\n   - Target prevalence $\\pi^{(2)} = 0.30$\n3. Test case $3$ (boundary case $\\pi = 0$):\n   - Reuse $\\mathbf{s}^{(1)}_{\\mathrm{OCSVM}}$, $\\mathbf{s}^{(1)}_{\\mathrm{IF}}$, and $\\mathbf{y}^{(1)}$ from test case $1$\n   - Target prevalence $\\pi^{(3)} = 0.00$\n4. Test case $4$ (boundary case $\\pi = 1$):\n   - Reuse $\\mathbf{s}^{(1)}_{\\mathrm{OCSVM}}$, $\\mathbf{s}^{(1)}_{\\mathrm{IF}}$, and $\\mathbf{y}^{(1)}$ from test case $1$\n   - Target prevalence $\\pi^{(4)} = 1.00$\n\nFinal output specification:\n- For each test case $j \\in \\{1,2,3,4\\}$, compute precision for OCSVM, denoted $P^{(j)}_{\\mathrm{OCSVM}}$, and for IF, denoted $P^{(j)}_{\\mathrm{IF}}$.\n- Your program should produce a single line of output containing a single list of $8$ floats, rounded to four decimal places, in the following order:\n  $[\\,P^{(1)}_{\\mathrm{OCSVM}}, P^{(1)}_{\\mathrm{IF}}, P^{(2)}_{\\mathrm{OCSVM}}, P^{(2)}_{\\mathrm{IF}}, P^{(3)}_{\\mathrm{OCSVM}}, P^{(3)}_{\\mathrm{IF}}, P^{(4)}_{\\mathrm{OCSVM}}, P^{(4)}_{\\mathrm{IF}}\\,]$.", "solution": "The problem is valid. It presents a well-defined computational task based on standard principles of performance evaluation for machine learning models. The problem statement is self-contained, with all necessary data, definitions, and environmental constraints provided. It is scientifically grounded in the field of statistical learning, logically consistent, and free of ambiguity or factual errors.\n\nThe task is to compute the precision of two anomaly detection models, One-Class Support Vector Machine (OCSVM) and Isolation Forest (IF), across four test scenarios. The evaluation is based on a prevalence-based thresholding strategy.\n\nThe core of the problem lies in correctly implementing the specified thresholding rule and the precision metric, including all edge cases. The procedure can be broken down into the following steps:\n\n**1. Determining the Number of Anomalies ($k$)**\nFor a given test set with $n$ samples and a target anomaly prevalence $\\pi \\in [0, 1]$, the number of data points to be flagged as anomalies, denoted by $k$, is determined by the floor function:\n$$k = \\lfloor \\pi n \\rfloor$$\nThis value $k$ represents the size of the set of predicted anomalies.\n\n**2. Identifying Anomalies via Score-Based Ranking**\nThe prediction of which samples are anomalous is based on their scores, $\\mathbf{s} \\in \\mathbb{R}^n$. A higher score indicates a greater likelihood of being an anomaly. To select the top $k$ anomalies, we must establish a deterministic ranking. The problem specifies a lexicographical sorting rule:\n- The primary sorting key is the anomaly score $s_i$, in descending order (i.e., we sort by -$s_i$ in ascending order).\n- The secondary sorting key is the sample index $i$, in ascending order. This rule is used to break any ties in the scores.\n\nTherefore, for two samples with indices $i$ and $j$, sample $i$ is considered more anomalous than sample $j$ if ($s_i > s_j$) or ($s_i = s_j$ and $i  j$). The set of predicted anomalies, $\\hat{Y}_1$, consists of the $k$ samples with the highest rank according to this rule.\n\n**3. Computing Precision**\nPrecision is a metric that measures the fraction of correctly identified anomalies among all samples predicted as anomalies. It is defined in terms of true positives ($TP$) and false positives ($FP$).\n- A True Positive ($TP$) is a sample that is correctly identified as an anomaly (predicted label $\\hat{y}_i = 1$ and true label $y_i = 1$).\n- A False Positive ($FP$) is a sample that is incorrectly identified as an anomaly (predicted label $\\hat{y}_i = 1$ and true label $y_i = 0$).\n\nThe total number of predicted anomalies is $TP + FP$, which is equal to $k$. The precision $P$ is then calculated as:\n$$P = \\begin{cases} \\frac{TP}{TP + FP}  \\text{if } TP + FP > 0 \\\\ 0  \\text{if } TP + FP = 0 \\end{cases}$$\nSince $TP+FP = k$, this simplifies to $P = TP / k$ for $k > 0$.\n\n**4. Handling Boundary Cases for Prevalence ($\\pi$)**\nThe problem defines specific behavior for the boundary values of $\\pi$:\n- If $\\pi = 0$, then $k = \\lfloor 0 \\cdot n \\rfloor = 0$. No samples are predicted as anomalies. In this case, $TP=0$ and $FP=0$, so $TP+FP=0$. The precision is explicitly defined to be $0$.\n- If $\\pi = 1$, then $k = \\lfloor 1 \\cdot n \\rfloor = n$. All samples are predicted as anomalies. The precision is defined as the overall fraction of true anomalies in the dataset. This is equivalent to $P = (\\sum_{i=1}^n y_i) / n$. This is consistent with the general formula, as $k=n$ and $TP$ becomes the total count of true anomalies.\n\n**Step-by-Step Calculation for a Single Test Case:**\n\nGiven a vector of scores $\\mathbf{s}$, a vector of true labels $\\mathbf{y}$, and a prevalence $\\pi$:\n1.  Calculate $n = \\text{length}(\\mathbf{s})$.\n2.  Calculate $k = \\lfloor \\pi n \\rfloor$.\n3.  If $k=0$, the precision is $0$.\n4.  If $k>0$, create a list of indices $I = [0, 1, \\dots, n-1]$.\n5.  Sort the indices $I$ based on the key $(-s_i, i)$ to get the sorted list $I'$.\n6.  Select the top $k$ indices from $I'$ to form the set of predicted anomaly indices, $I'_{\\text{anom}} = \\{I'_1, I'_2, \\dots, I'_k\\}$.\n7.  Calculate the number of true positives: $TP = \\sum_{i \\in I'_{\\text{anom}}} y_i$.\n8.  Calculate precision: $P = TP / k$.\n9.  This procedure is applied to the scores from both OCSVM and IF for each of the four test cases. The resulting eight precision values are collected, rounded, and formatted.\n\n**Execution on Test Cases:**\n\n- **Test Case 1**: $n = 12$, $\\pi = 0.25$, $k = \\lfloor 0.25 \\times 12 \\rfloor = 3$.\n  - OCSVM: The highest scores are at indices $1$ ($0.95$), $5$ ($0.90$), and $7$ ($0.85$). Predicted anomalies are at indices $\\{1, 5, 7\\}$. True labels are $y_1=1$, $y_5=0$, $y_7=1$. So, $TP=2$, $FP=1$. Precision $P = 2/3 \\approx 0.6667$.\n  - IF: The highest scores are at indices $5$ ($0.90$), $1$ ($0.88$), and $6$ ($0.85$). Predicted anoms: $\\{5, 1, 6\\}$. True labels are $y_5=0$, $y_1=1$, $y_6=0$. So, $TP=1$, $FP=2$. Precision $P = 1/3 \\approx 0.3333$.\n\n- **Test Case 2**: $n = 10$, $\\pi = 0.30$, $k = \\lfloor 0.30 \\times 10 \\rfloor = 3$.\n  - OCSVM: Highest scores are at index $2$ ($0.95$), $5$ ($0.85$). There is a tie for the third spot between index $3$ ($0.70$) and index $8$ ($0.70$). The tie-breaking rule (smaller index wins) selects index $3$. Predicted anoms: $\\{2, 5, 3\\}$. True labels are $y_2=1$, $y_5=0$, $y_3=0$. So, $TP=1$, $FP=2$. Precision $P = 1/3 \\approx 0.3333$.\n  - IF: The highest scores are at indices $2$ ($0.92$), $8$ ($0.89$), and $5$ ($0.80$). Predicted anoms: $\\{2, 8, 5\\}$. True labels are $y_2=1$, $y_8=1$, $y_5=0$. So, $TP=2$, $FP=1$. Precision $P = 2/3 \\approx 0.6667$.\n\n- **Test Case 3**: $\\pi = 0.0$.\n  - Per the problem definition, precision for both OCSVM and IF is $0.0000$.\n\n- **Test Case 4**: $\\pi = 1.0$.\n  - Per the problem definition, precision is the fraction of true anomalies in the dataset. The data is from test case $1$, where there are $4$ anomalies out of $12$ samples.\n  - For both OCSVM and IF, precision is $4/12 = 1/3 \\approx 0.3333$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes precision for OCSVM and IF methods on four test cases\n    and prints the results in the specified format.\n    \"\"\"\n\n    def compute_precision(scores, labels, pi):\n        \"\"\"\n        Calculates precision based on a prevalence-based threshold.\n\n        Args:\n            scores (np.ndarray): Anomaly scores for each sample.\n            labels (np.ndarray): Ground-truth labels (1 for anomaly, 0 for normal).\n            pi (float): Target anomaly prevalence.\n\n        Returns:\n            float: The calculated precision.\n        \"\"\"\n        n = len(scores)\n\n        # Handle the boundary case pi=0 as per problem definition.\n        if pi == 0.0:\n            return 0.0\n\n        num_anomalies = int(np.floor(pi * n))\n\n        # If k=0, no anomalies are predicted, so TP=0, FP=0, and precision is 0.\n        if num_anomalies == 0:\n            return 0.0\n        \n        # Handle the boundary case pi=1 as per problem definition.\n        if pi == 1.0:\n            return np.sum(labels) / n\n\n        # For 0  pi  1:\n        # Create a list of indices from 0 to n-1.\n        indices = np.arange(n)\n\n        # Sort indices based on the specified lexicographical rule:\n        # Primary key: score (descending).\n        # Secondary key: index (ascending).\n        sorted_indices = sorted(indices, key=lambda i: (-scores[i], i))\n\n        # Select the top k indices as predicted anomalies.\n        top_k_indices = sorted_indices[:num_anomalies]\n\n        # Calculate True Positives (TP).\n        tp = 0\n        for i in top_k_indices:\n            if labels[i] == 1:\n                tp += 1\n        \n        # The number of predicted positives is TP + FP = num_anomalies.\n        # Precision is TP / (TP + FP).\n        precision = tp / num_anomalies\n        \n        return precision\n\n    # Test suite data\n    # Test case 1 data\n    s_ocsvm1 = np.array([0.10, 0.95, 0.40, 0.30, 0.60, 0.90, 0.20, 0.85, 0.50, 0.55, 0.15, 0.45])\n    s_if1 = np.array([0.12, 0.88, 0.80, 0.65, 0.50, 0.90, 0.85, 0.40, 0.30, 0.45, 0.20, 0.60])\n    y1 = np.array([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0])\n    pi1 = 0.25\n\n    # Test case 2 data\n    s_ocsvm2 = np.array([0.10, 0.50, 0.95, 0.70, 0.40, 0.85, 0.20, 0.60, 0.70, 0.30])\n    s_if2 = np.array([0.30, 0.60, 0.92, 0.70, 0.45, 0.80, 0.20, 0.58, 0.89, 0.40])\n    y2 = np.array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n    pi2 = 0.30\n\n    # Test cases parameters\n    test_cases = [\n        # (s_ocsvm, s_if, y, pi)\n        (s_ocsvm1, s_if1, y1, pi1),  # Test case 1\n        (s_ocsvm2, s_if2, y2, pi2),  # Test case 2\n        (s_ocsvm1, s_if1, y1, 0.0),  # Test case 3\n        (s_ocsvm1, s_if1, y1, 1.0),  # Test case 4\n    ]\n\n    results = []\n    for s_ocsvm, s_if, y, pi in test_cases:\n        p_ocsvm = compute_precision(s_ocsvm, y, pi)\n        p_if = compute_precision(s_if, y, pi)\n        results.extend([p_ocsvm, p_if])\n\n    # Format results to four decimal places and print in the required format.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3099063"}, {"introduction": "After a model flags a data point as an anomaly, understanding *why* it was flagged is a critical step toward model interpretability. For a kernel-based model like a One-Class SVM, the gradient of the decision function with respect to the input, $\\nabla_{x} f(x)$, provides a powerful insight into the local geometry of the decision boundary. This vector points in the direction that makes a point \"more anomalous\". This hands-on calculation challenges you to compute this gradient and use it to find the smallest change required to reclassify an anomalous point as normal, providing a concrete, geometric understanding of the model's behavior [@problem_id:3099119].", "problem": "A one-class Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel is trained for anomaly detection in two dimensions. The decision function is defined by the kernel representation\n$$\nf(x) = \\sum_{i=1}^{n} \\alpha_{i} \\, k(x, x_{i}) - \\rho,\n$$\nwhere the Radial Basis Function (RBF) kernel is\n$$\nk(x, z) = \\exp(-\\gamma \\, \\|x - z\\|^{2}),\n$$\nand candidate points are flagged as anomalies when $f(x)  0$. Consider a model with $n=3$ support vectors and weights given by\n$$\n\\alpha_{1} = 0.5, \\quad \\alpha_{2} = 0.3, \\quad \\alpha_{3} = 0.2, \\quad \\gamma = 1.2, \\quad \\rho = 0.25,\n$$\nlocated at\n$$\nx_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad x_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad x_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nA test point\n$$\nx = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nis flagged as an anomaly. Starting from first principles for kernel methods and multivariate calculus, do the following:\n- Compute the gradient $\\nabla_{x} f(x)$ of the decision function at the point $x$.\n- Using a first-order approximation of $f$ near $x$, derive and compute the minimal Euclidean-norm perturbation vector $\\delta^{\\star}$ that moves $x$ onto the local decision boundary $f(x + \\delta) = 0$.\n\nExpress your final answer as the perturbation vector $\\delta^{\\star}$ in the form of a row matrix. Round each component of the vector to four significant figures. No physical units are involved.", "solution": "The problem is valid and requires the application of multivariate calculus to a kernel-based machine learning model. The objective is to find the minimal Euclidean-norm perturbation, $\\delta^{\\star}$, that moves a given point $x$ onto the decision boundary $f(x + \\delta^{\\star}) = 0$. This is achieved in two main steps: first, computing the gradient of the decision function, $\\nabla_x f(x)$, and second, using a first-order Taylor approximation to find the required perturbation.\n\n**1. Compute the Gradient $\\nabla_x f(x)$**\n\nThe decision function is $f(x) = \\sum_{i=1}^{n} \\alpha_{i} k(x, x_{i}) - \\rho$, with the RBF kernel $k(x, z) = \\exp(-\\gamma \\|x - z\\|^{2})$. The gradient of the kernel with respect to $x$ is:\n$$ \\nabla_x k(x, x_i) = \\nabla_x \\exp(-\\gamma \\|x - x_i\\|^2) = \\exp(-\\gamma \\|x - x_i\\|^2) \\cdot (-2\\gamma (x - x_i)) = -2\\gamma (x - x_i) k(x, x_i) $$\nThe gradient of the decision function is the sum of the gradients of its kernel terms (as $\\rho$ is a constant):\n$$ \\nabla_x f(x) = \\sum_{i=1}^{n} \\alpha_i \\nabla_x k(x, x_i) = -2\\gamma \\sum_{i=1}^{n} \\alpha_i k(x, x_i) (x - x_i) $$\nGiven parameters are $\\gamma=1.2$, $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and support vectors $x_1=\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, x_2=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, x_3=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ with weights $\\alpha=[0.5, 0.3, 0.2]$.\nFirst, we calculate the squared norms and kernel values:\n-   $\\|x-x_1\\|^2 = \\|(1,1)\\|^2 = 2 \\implies k(x,x_1) = e^{-1.2 \\cdot 2} = e^{-2.4} \\approx 0.090718$\n-   $\\|x-x_2\\|^2 = \\|(0,1)\\|^2 = 1 \\implies k(x,x_2) = e^{-1.2 \\cdot 1} = e^{-1.2} \\approx 0.301194$\n-   $\\|x-x_3\\|^2 = \\|(1,0)\\|^2 = 1 \\implies k(x,x_3) = e^{-1.2 \\cdot 1} = e^{-1.2} \\approx 0.301194$\nNow, substitute these into the gradient formula:\n$$ \\nabla_x f(x) = -2(1.2) \\left[ 0.5(0.090718)\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + 0.3(0.301194)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + 0.2(0.301194)\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right] $$\n$$ \\nabla_x f(x) = -2.4 \\left[ \\begin{pmatrix} 0.045359 \\\\ 0.045359 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0.090358 \\end{pmatrix} + \\begin{pmatrix} 0.060239 \\\\ 0 \\end{pmatrix} \\right] = -2.4 \\begin{pmatrix} 0.105598 \\\\ 0.135717 \\end{pmatrix} \\approx \\begin{pmatrix} -0.25344 \\\\ -0.32572 \\end{pmatrix} $$\n\n**2. Compute the Minimal Perturbation $\\delta^\\star$**\n\nWe use a first-order Taylor expansion of $f$ around $x$:\n$$ f(x + \\delta) \\approx f(x) + (\\nabla_x f(x))^T \\delta $$\nWe want to find $\\delta$ such that $f(x + \\delta) = 0$, so we set the approximation to zero:\n$$ (\\nabla_x f(x))^T \\delta \\approx -f(x) $$\nWe seek the minimal-norm solution for $\\delta$. The solution to minimizing $\\|\\delta\\|^2$ subject to a linear constraint $v^T\\delta=c$ is a vector parallel to $v$. Here, $v = \\nabla_x f(x)$. So, $\\delta^\\star$ must be proportional to the gradient:\n$$ \\delta^\\star = \\lambda \\nabla_x f(x) $$\nSubstituting this into the constraint gives:\n$$ (\\nabla_x f(x))^T (\\lambda \\nabla_x f(x)) = -f(x) \\implies \\lambda \\|\\nabla_x f(x)\\|^2 = -f(x) \\implies \\lambda = -\\frac{f(x)}{\\|\\nabla_x f(x)\\|^2} $$\nThus, the minimal perturbation is:\n$$ \\delta^\\star = -\\frac{f(x)}{\\|\\nabla_x f(x)\\|^2} \\nabla_x f(x) $$\nWe first need the value of $f(x)$:\n$$ f(x) = (0.5 \\cdot k(x,x_1) + 0.3 \\cdot k(x,x_2) + 0.2 \\cdot k(x,x_3)) - \\rho $$\n$$ f(x) = (0.5 \\cdot 0.090718 + 0.3 \\cdot 0.301194 + 0.2 \\cdot 0.301194) - 0.25 = 0.195956 - 0.25 = -0.054044 $$\nWe also need the squared norm of the gradient:\n$$ \\|\\nabla_x f(x)\\|^2 \\approx (-0.25344)^2 + (-0.32572)^2 \\approx 0.064232 + 0.106094 = 0.170326 $$\nNow we can find $\\delta^\\star$:\n$$ \\delta^\\star = -\\frac{-0.054044}{0.170326} \\begin{pmatrix} -0.25344 \\\\ -0.32572 \\end{pmatrix} \\approx 0.31730 \\begin{pmatrix} -0.25344 \\\\ -0.32572 \\end{pmatrix} \\approx \\begin{pmatrix} -0.080424 \\\\ -0.10338 \\end{pmatrix} $$\nRounding each component to four significant figures gives:\n$$ \\delta^\\star \\approx \\begin{pmatrix} -0.08042 \\\\ -0.1034 \\end{pmatrix} $$\nThe final answer is presented as a row matrix as requested.", "answer": "$$\\boxed{\\begin{pmatrix} -0.08042  -0.1034 \\end{pmatrix}}$$", "id": "3099119"}, {"introduction": "Setting a threshold by simply picking the top-scoring points is intuitive but not always optimal. A more powerful approach involves statistically modeling the score distributions for both normal and anomalous data. This exercise guides you through a semi-parametric calibration technique rooted in the Neyman-Pearson lemma, a cornerstone of statistical decision theory. You will fit Gaussian Mixture Models to score distributions and use the resulting likelihood ratio, $\\Lambda(s)=\\frac{p(s \\mid \\text{anomaly})}{p(s \\mid \\text{normal})}$, to create a decision rule that maximizes detection power for a fixed false positive rate. This practice demonstrates how to elevate a simple scoring mechanism into a statistically principled and more effective classification system [@problem_id:3099077].", "problem": "You will implement and evaluate a semi-parametric calibration method for anomaly detection decision scores, using Gaussian mixture models to estimate class-conditional score densities and likelihood-ratio thresholding under a fixed false positive rate. The context is anomaly detection with One-Class Support Vector Machine (OCSVM) and Isolation Forest (IF), both of which produce scalar decision scores per sample. The OCSVM decision function is typically denoted by $f(x)$; we adopt the anomaly score $s=-f(x)$ so that larger $s$ indicates higher anomaly likelihood. For IF, we treat its anomaly score $s$ as a scalar on the real line with larger values indicating higher anomaly likelihood. You are given synthetic, scientifically plausible score distributions representing typical outcomes from OCSVM and IF. Your program must estimate class-conditional densities for normal scores and injected anomaly scores using Gaussian mixtures and then apply the likelihood-ratio test principle to set thresholds. Finally, you will compare this calibrated decision rule to naive thresholding on raw scores at the same false positive rate.\n\nFoundational base to use:\n- Likelihood-ratio testing under the Neyman–Pearson paradigm: the most powerful test at a fixed false positive rate $\\alpha$ classifies a score $s$ as anomaly if the likelihood ratio $\\Lambda(s)=\\frac{p(s \\mid y=\\text{anomaly})}{p(s \\mid y=\\text{normal})}$ exceeds a threshold chosen to meet the specified false positive rate $\\alpha$, where $p(\\cdot \\mid y)$ denotes a class-conditional density.\n- Semi-parametric modeling with Gaussian mixtures: each class-conditional density is represented as a finite mixture of univariate Gaussians whose parameters are estimated from data via maximum likelihood.\n\nYour task:\n- For each test case, generate two sets of one-dimensional scores using a fixed random seed: one set from the normal score distribution and one set from the injected anomaly score distribution. Each distribution is defined as a Gaussian mixture with specified component weights, means, and standard deviations. Larger scores indicate greater anomaly likelihood.\n- Fit a Gaussian mixture to the normal scores and a (potentially different) Gaussian mixture to the anomaly scores by maximum likelihood using the Expectation–Maximization procedure. Each fit must be univariate and use the specified number of mixture components.\n- Compute the class-conditional density estimates $\\hat{p}(s \\mid y=\\text{normal})$ and $\\hat{p}(s \\mid y=\\text{anomaly})$. Construct the likelihood ratio $\\hat{\\Lambda}(s)=\\frac{\\hat{p}(s \\mid y=\\text{anomaly})}{\\hat{p}(s \\mid y=\\text{normal})}$.\n- Choose the calibrated threshold $\\tau$ to achieve false positive rate $\\alpha$ by setting $\\tau$ to the $1-\\alpha$ quantile of $\\hat{\\Lambda}(s)$ over the normal scores. Classify a score $s$ as anomaly if $\\hat{\\Lambda}(s)>\\tau$. Compute the resulting true positive rate (the fraction of anomaly scores classified as anomalies).\n- As a baseline, choose the raw score threshold $t$ to achieve the same false positive rate $\\alpha$ by setting $t$ to the $1-\\alpha$ quantile of the raw scores over the normal scores. Classify a score $s$ as anomaly if $s>t$. Compute the resulting true positive rate.\n- For each test case, output the improvement defined as the difference between the calibrated true positive rate and the raw-score true positive rate. The false positive rate $\\alpha$ values are specified as decimal fractions. No physical units are involved.\n\nTest suite and parameters:\n- Case $1$ (happy path, IF-like separation):\n  - Normal mixture: weights $[0.6,0.4]$, means $[0.22,0.38]$, standard deviations $[0.05,0.04]$, sample size $300$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.72]$, standard deviations $[0.06]$, sample size $30$.\n  - Fit with $K_{\\text{normal}}=2$ and $K_{\\text{anomaly}}=1$. False positive rate $\\alpha=0.1$. Seed $1$.\n- Case $2$ (bimodal normal, central anomalies, OCSVM-like score inversion accounted for by orientation):\n  - Normal mixture: weights $[0.5,0.5]$, means $[0.15,0.85]$, standard deviations $[0.04,0.06]$, sample size $300$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.50]$, standard deviations $[0.07]$, sample size $30$.\n  - Fit with $K_{\\text{normal}}=2$ and $K_{\\text{anomaly}}=1$. False positive rate $\\alpha=0.1$. Seed $2$.\n- Case $3$ (class imbalance and heteroscedastic anomalies):\n  - Normal mixture: weights $[1.0]$, means $[0.30]$, standard deviations $[0.05]$, sample size $1000$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.60]$, standard deviations $[0.20]$, sample size $10$.\n  - Fit with $K_{\\text{normal}}=1$ and $K_{\\text{anomaly}}=2$. False positive rate $\\alpha=0.05$. Seed $3$.\n- Case $4$ (near-identical distributions, boundary case):\n  - Normal mixture: weights $[1.0]$, means $[0.50]$, standard deviations $[0.10]$, sample size $500$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.55]$, standard deviations $[0.10]$, sample size $50$.\n  - Fit with $K_{\\text{normal}}=1$ and $K_{\\text{anomaly}}=1$. False positive rate $\\alpha=0.1$. Seed $4$.\n\nProgram requirements:\n- Implement univariate Gaussian mixture fitting via Expectation–Maximization from first principles, without external machine learning libraries.\n- Use reproducible sampling with the specified seeds.\n- For each case, compute the improvement as a float. Aggregate all improvements in order into a single output list.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the improvement for case $i$ expressed as a decimal number.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in statistical learning theory, well-posed with all necessary parameters provided, and objective in its formulation. The task is to implement and evaluate a semi-parametric calibration method for anomaly detection scores based on the Neyman-Pearson testing paradigm.\n\nThe core principle is to transform the raw anomaly score $s$ into a more discriminative space defined by the likelihood ratio $\\Lambda(s) = \\frac{p(s|y=\\text{anomaly})}{p(s|y=\\text{normal})}$, where $y$ denotes the class label. The Neyman-Pearson lemma states that a test which rejects the null hypothesis ($y=\\text{normal}$) when $\\Lambda(s)$ exceeds a certain threshold is the most powerful test for a given significance level (false positive rate).\n\nOur approach is semi-parametric because we do not assume a single, simple form for the class-conditional densities $p(s|y)$. Instead, we model each density as a univariate Gaussian Mixture Model (GMM), a flexible and powerful choice for representing complex, multimodal distributions. The density of a score $s$ under a GMM with $K$ components is given by:\n$$p(s) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(s | \\mu_k, \\sigma_k^2)$$\nwhere $\\pi_k$ are the mixture weights ($\\sum \\pi_k = 1$), and $\\mu_k$ and $\\sigma_k^2$ are the mean and variance of the $k$-th Gaussian component $\\mathcal{N}$.\n\nThe parameters of these GMMs ($\\pi_k, \\mu_k, \\sigma_k$) are not known a priori and must be estimated from the provided score data for both the normal and anomaly classes. We employ the Expectation-Maximization (EM) algorithm, an iterative procedure for finding maximum likelihood estimates of parameters in statistical models with latent variables. In the context of GMMs, the latent variables are the component identities for each data point.\n\nThe EM algorithm proceeds in two steps:\n1.  **Expectation (E-step)**: Given the current parameter estimates, we compute the posterior probability, or \"responsibility,\" that each data point $s_i$ belongs to each component $k$. This is given by Bayes' theorem:\n    $$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(s_i | \\mu_k, \\sigma_k^2)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(s_i | \\mu_j, \\sigma_j^2)}$$\n2.  **Maximization (M-step)**: We update the model parameters using the computed responsibilities to maximize the expected log-likelihood. The updates are:\n    -   Effective number of points in component $k$: $N_k = \\sum_{i=1}^{N} \\gamma_{ik}$\n    -   New weight: $\\pi_k^{\\text{new}} = \\frac{N_k}{N}$\n    -   New mean: $\\mu_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} s_i$\n    -   New variance: $(\\sigma_k^2)^{\\text{new}} = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} (s_i - \\mu_k^{\\text{new}})^2$\n\nThese steps are repeated until the log-likelihood of the data, $LL = \\sum_{i=1}^{N} \\log(\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(s_i | \\mu_k, \\sigma_k^2))$, converges. For the special case of $K=1$, the EM algorithm is unnecessary, as the maximum likelihood estimates are simply the sample mean and sample variance.\n\nThe full procedure for each test case is as follows:\n1.  **Data Generation**: Using the specified random seed, generate `normal_scores` and `anomaly_scores` from their respective GMM definitions.\n2.  **Density Estimation**: Fit a GMM to `normal_scores` with $K_{\\text{normal}}$ components and another GMM to `anomaly_scores` with $K_{\\text{anomaly}}$ components using the EM algorithm. This yields the estimated densities $\\hat{p}(s|y=\\text{normal})$ and $\\hat{p}(s|y=\\text{anomaly})$.\n3.  **Baseline Evaluation (Raw Scores)**:\n    -   Determine a threshold $t$ as the $(1-\\alpha)$ quantile of the `normal_scores`. This ensures that the false positive rate on the training normal data is $\\alpha$.\n    -   Calculate the true positive rate, $TPR_{raw}$, as the fraction of `anomaly_scores` that are greater than $t$.\n4.  **Calibrated Evaluation (Likelihood Ratio)**:\n    -   Compute the estimated likelihood ratio $\\hat{\\Lambda}(s) = \\frac{\\hat{p}(s|y=\\text{anomaly})}{\\hat{p}(s|y=\\text{normal})}$ for each score in the `normal_scores` set. A small epsilon is added to the denominator to prevent division by zero.\n    -   Determine a threshold $\\tau$ as the $(1-\\alpha)$ quantile of these computed likelihood ratio values.\n    -   Compute $\\hat{\\Lambda}(s)$ for each score in the `anomaly_scores` set.\n    -   Calculate the calibrated true positive rate, $TPR_{calibrated}$, as the fraction of these anomaly likelihood ratios that are greater than $\\tau$.\n5.  **Performance Comparison**: The final output for the case is the improvement, defined as the difference $TPR_{calibrated} - TPR_{raw}$. This quantifies the gain in detection power at a fixed false positive rate achieved by the calibration procedure.\n\nThis process is repeated for all specified test cases to evaluate the method's effectiveness under different score distribution scenarios.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef generate_gmm_samples(weights, means, stds, n_samples):\n    \"\"\"Generates samples from a Gaussian Mixture Model.\"\"\"\n    component_choices = np.random.choice(len(weights), size=n_samples, p=weights)\n    samples = np.random.normal(loc=np.array(means)[component_choices], scale=np.array(stds)[component_choices])\n    return samples\n\ndef gmm_pdf(x, weights, means, stds):\n    \"\"\"Calculates the PDF of a GMM at points x.\"\"\"\n    pdf_val = np.zeros_like(x, dtype=float)\n    for w, m, s in zip(weights, means, stds):\n        pdf_val += w * norm.pdf(x, loc=m, scale=s)\n    return pdf_val\n\ndef fit_gmm_em(data, K, n_iter=150, tol=1e-6):\n    \"\"\"Fits a univariate GMM using the Expectation-Maximization algorithm.\"\"\"\n    n_samples = len(data)\n    \n    if K == 1:\n        mean = np.mean(data)\n        std = np.std(data)\n        if std  1e-6: std = 1e-6 # handle cases with identical data points\n        return np.array([1.0]), np.array([mean]), np.array([std])\n\n    # Initialization\n    weights = np.ones(K) / K\n    # Deterministic initialization based on data range\n    means = np.linspace(np.min(data), np.max(data), K)\n    stds = np.full(K, np.std(data))\n    \n    prev_log_likelihood = -np.inf\n    \n    for _ in range(n_iter):\n        # E-step: Calculate responsibilities\n        weighted_pdfs = np.zeros((n_samples, K))\n        for k in range(K):\n            # Prevent std dev from becoming zero\n            s_k = stds[k] if stds[k] > 1e-6 else 1e-6\n            weighted_pdfs[:, k] = weights[k] * norm.pdf(data, means[k], s_k)\n        \n        total_likelihood_per_point = np.sum(weighted_pdfs, axis=1)\n        \n        # Add a small constant to prevent log(0) and division by zero\n        safe_total_likelihood = total_likelihood_per_point + 1e-9\n        responsibilities = weighted_pdfs / safe_total_likelihood[:, np.newaxis]\n        \n        # Check for convergence\n        log_likelihood = np.sum(np.log(safe_total_likelihood))\n        if abs(log_likelihood - prev_log_likelihood)  tol:\n            break\n        prev_log_likelihood = log_likelihood\n\n        # M-step: Update parameters\n        Nk = np.sum(responsibilities, axis=0)\n        safe_Nk = Nk + 1e-9 # Prevent division by zero\n        \n        weights = Nk / n_samples\n        means = np.sum(responsibilities * data[:, np.newaxis], axis=0) / safe_Nk\n        variances = np.sum(responsibilities * (data[:, np.newaxis] - means)**2, axis=0) / safe_Nk\n        stds = np.sqrt(variances)\n        \n        # Prevent component collapse\n        stds = np.maximum(stds, 1e-6)\n        \n    return weights, means, stds\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    test_cases = [\n        # Case 1 (happy path, IF-like separation)\n        {'normal_w': [0.6, 0.4], 'normal_m': [0.22, 0.38], 'normal_s': [0.05, 0.04], 'n_normal': 300,\n         'anomaly_w': [1.0], 'anomaly_m': [0.72], 'anomaly_s': [0.06], 'n_anomaly': 30,\n         'k_normal': 2, 'k_anomaly': 1, 'alpha': 0.1, 'seed': 1},\n        # Case 2 (bimodal normal, central anomalies)\n        {'normal_w': [0.5, 0.5], 'normal_m': [0.15, 0.85], 'normal_s': [0.04, 0.06], 'n_normal': 300,\n         'anomaly_w': [1.0], 'anomaly_m': [0.50], 'anomaly_s': [0.07], 'n_anomaly': 30,\n         'k_normal': 2, 'k_anomaly': 1, 'alpha': 0.1, 'seed': 2},\n        # Case 3 (class imbalance and heteroscedastic anomalies)\n        {'normal_w': [1.0], 'normal_m': [0.30], 'normal_s': [0.05], 'n_normal': 1000,\n         'anomaly_w': [1.0], 'anomaly_m': [0.60], 'anomaly_s': [0.20], 'n_anomaly': 10,\n         'k_normal': 1, 'k_anomaly': 2, 'alpha': 0.05, 'seed': 3},\n        # Case 4 (near-identical distributions)\n        {'normal_w': [1.0], 'normal_m': [0.50], 'normal_s': [0.10], 'n_normal': 500,\n         'anomaly_w': [1.0], 'anomaly_m': [0.55], 'anomaly_s': [0.10], 'n_anomaly': 50,\n         'k_normal': 1, 'k_anomaly': 1, 'alpha': 0.1, 'seed': 4},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        np.random.seed(case['seed'])\n        \n        # 1. Generate data\n        normal_scores = generate_gmm_samples(case['normal_w'], case['normal_m'], case['normal_s'], case['n_normal'])\n        anomaly_scores = generate_gmm_samples(case['anomaly_w'], case['anomaly_m'], case['anomaly_s'], case['n_anomaly'])\n        \n        # 2. Fit GMMs\n        w_n, m_n, s_n = fit_gmm_em(normal_scores, case['k_normal'])\n        w_a, m_a, s_a = fit_gmm_em(anomaly_scores, case['k_anomaly'])\n        \n        # 3. Baseline TPR (Raw Score Thresholding)\n        raw_score_threshold = np.quantile(normal_scores, 1 - case['alpha'])\n        raw_tpr = np.mean(anomaly_scores > raw_score_threshold)\n        \n        # 4. Calibrated TPR (Likelihood-Ratio Thresholding)\n        # Denominator for likelihood ratio, with a small epsilon for stability\n        epsilon = 1e-9\n        \n        # Calculate LR on normal scores to find the threshold\n        p_normal_on_normal = gmm_pdf(normal_scores, w_n, m_n, s_n)\n        p_anomaly_on_normal = gmm_pdf(normal_scores, w_a, m_a, s_a)\n        lr_on_normal = p_anomaly_on_normal / (p_normal_on_normal + epsilon)\n        calibrated_threshold = np.quantile(lr_on_normal, 1 - case['alpha'])\n        \n        # Calculate LR on anomaly scores to get TPR\n        p_normal_on_anomaly = gmm_pdf(anomaly_scores, w_n, m_n, s_n)\n        p_anomaly_on_anomaly = gmm_pdf(anomaly_scores, w_a, m_a, s_a)\n        lr_on_anomaly = p_anomaly_on_anomaly / (p_normal_on_anomaly + epsilon)\n        calibrated_tpr = np.mean(lr_on_anomaly > calibrated_threshold)\n        \n        # 5. Compute improvement and store result\n        improvement = calibrated_tpr - raw_tpr\n        results.append(improvement)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3099077"}]}