{"hands_on_practices": [{"introduction": "The core of Bayesian inference is updating beliefs as new evidence becomes available. A natural question arises: does the way we process this evidence matter? This exercise explores the fundamental consistency of Bayesian updating by comparing a \"batch\" update, where all data is considered at once, with a \"sequential\" update, where data is processed one observation at a time. Through this practice, you will discover a foundational property that makes Bayesian methods so robust and elegant. [@problem_id:1946578]", "problem": "A quality control engineer is assessing a new manufacturing process designed to produce highly reliable electronic components. The success rate of the process, defined as the probability $p$ that a component is non-defective, is unknown. The engineer's initial belief about this probability $p$ is modeled as a random variable following a Beta distribution with known parameters $\\alpha > 0$ and $\\beta > 0$. The probability density function for this belief is proportional to $p^{\\alpha-1}(1-p)^{\\beta-1}$.\n\nThe engineer observes a test batch of $n$ components. In this batch, a total of $k$ components are found to be non-defective (successes), and consequently, $n-k$ components are defective (failures). The engineer wishes to update their belief about $p$ based on these observations. Two different methods of updating are considered.\n\nMethod 1 (Batch Update): The engineer waits until all $n$ components have been tested. They then update their initial belief (the prior) using the total counts of $k$ successes and $n-k$ failures all at once. The updated belief (the posterior) is another Beta distribution with new parameters, which we will denote as $(\\alpha_{batch}, \\beta_{batch})$.\n\nMethod 2 (Sequential Update): The engineer updates their belief after each component is tested, one by one. Starting with the same initial belief $(\\alpha, \\beta)$, they update it after the first observation. The resulting posterior distribution then serves as the prior for the second observation. This procedure is repeated for all $n$ observations in the sequence. Let the parameters of the final Beta distribution after all $n$ updates be denoted by $(\\alpha_{seq}, \\beta_{seq})$.\n\nDetermine the expression for the parameter $\\alpha_{batch}$ resulting from the batch update and the expression for the parameter $\\beta_{seq}$ resulting from the sequential update. Present your answer as a pair of expressions $(\\alpha_{batch}, \\beta_{seq})$.", "solution": "We model the unknown success probability $p$ with a Beta prior having parameters $(\\alpha,\\beta)$, whose density is proportional to $p^{\\alpha-1}(1-p)^{\\beta-1}$. For $n$ independent Bernoulli trials with $k$ successes and $n-k$ failures, the likelihood is proportional to $p^{k}(1-p)^{n-k}$.\n\nBatch update: By Bayes’ rule and Beta-Binomial conjugacy, the posterior density is proportional to the product of prior and likelihood,\n$$\np^{\\alpha-1}(1-p)^{\\beta-1}\\cdot p^{k}(1-p)^{n-k}\n= p^{\\alpha-1+k}(1-p)^{\\beta-1+(n-k)}.\n$$\nThis is the kernel of a Beta distribution with parameters $(\\alpha+k,\\beta+n-k)$. Therefore, the batch-updated parameter for $p$ corresponding to the exponent of $p$ is\n$$\n\\alpha_{batch}=\\alpha+k.\n$$\n\nSequential update: Consider a single Bernoulli observation $x\\in\\{0,1\\}$ with prior $\\operatorname{Beta}(a,b)$. The likelihood is proportional to $p^{x}(1-p)^{1-x}$. The posterior is then proportional to\n$$\np^{a-1}(1-p)^{b-1}\\cdot p^{x}(1-p)^{1-x}\n= p^{(a-1)+x}(1-p)^{(b-1)+(1-x)},\n$$\nwhich is the kernel of $\\operatorname{Beta}(a+x,\\,b+1-x)$. Hence, each success ($x=1$) increments the first parameter by $1$, and each failure ($x=0$) increments the second parameter by $1$. After $k$ successes and $n-k$ failures, starting from $(\\alpha,\\beta)$, the final parameters are $(\\alpha+k,\\beta+n-k)$. Therefore, the sequentially updated second parameter is\n$$\n\\beta_{seq}=\\beta+(n-k).\n$$\n\nThus, the requested pair is $(\\alpha_{batch},\\beta_{seq})=(\\alpha+k,\\beta+n-k)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\alpha+k & \\beta+n-k\\end{pmatrix}}$$", "id": "1946578"}, {"introduction": "Many real-world phenomena, from counting defects in a product to modeling customer arrivals, are described by rates. This exercise demonstrates how to apply Bayesian principles to estimate an unknown rate parameter, $\\lambda$, of a Poisson process. By using a Gamma distribution as our prior, we can leverage the properties of conjugate pairs to make the update process mathematically tractable and intuitive. [@problem_id:1946607]", "problem": "An industrial process manufactures large sheets of a specialized polymer. A quality control specialist models the number of microscopic imperfections found in a randomly chosen 1 cm by 1 cm square sample of the polymer as a random variable following a Poisson distribution with an unknown mean rate $\\lambda$ imperfections per cm$^2$.\n\nBased on extensive experience with similar manufacturing processes, the specialist's prior belief about the parameter $\\lambda$ is modeled by a Gamma distribution with a shape parameter $\\alpha = 3$ and a rate parameter $\\beta = 2$. The probability density function of a Gamma($\\alpha, \\beta$) distribution is given by $f(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x)$ for $x > 0$.\n\nThe specialist then takes five independent 1 cm by 1 cm square samples and observes the following number of imperfections: 4, 2, 0, 3, 1.\n\nGiven this new data, what is the updated expected value (mean) of the rate parameter $\\lambda$? Express your answer as an exact fraction.", "solution": "Let the observed counts be denoted by $y_{1},\\dots,y_{n}$, where $n=5$ and $y_{1},\\dots,y_{5}\\in \\{4,2,0,3,1\\}$. The sampling model assumes $y_{i}\\mid \\lambda \\sim \\text{Poisson}(\\lambda)$ independently, so the likelihood is\n$$\nL(\\lambda; y_{1:n})=\\prod_{i=1}^{n}\\frac{\\exp(-\\lambda)\\lambda^{y_{i}}}{y_{i}!} \\propto \\lambda^{\\sum_{i=1}^{n} y_{i}} \\exp(-n\\lambda).\n$$\nThe prior is $\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)$ with density\n$$\np(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda), \\quad \\lambda>0.\n$$\nBy conjugacy, the posterior is proportional to the product of prior and likelihood:\n$$\np(\\lambda\\mid y_{1:n}) \\propto \\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)\\cdot \\lambda^{\\sum_{i=1}^{n} y_{i}}\\exp(-n\\lambda)\n= \\lambda^{\\alpha+\\sum_{i=1}^{n} y_{i}-1}\\exp\\!\\big(-( \\beta+n)\\lambda\\big),\n$$\nwhich is the kernel of a $\\text{Gamma}(\\alpha+\\sum_{i=1}^{n} y_{i},\\, \\beta+n)$ distribution (shape-rate parameterization).\n\nFor the given values, $\\alpha=3$, $\\beta=2$, $n=5$, and $\\sum_{i=1}^{5} y_{i}=4+2+0+3+1=10$, the posterior is\n$$\n\\lambda \\mid y_{1:5} \\sim \\text{Gamma}(3+10,\\, 2+5)=\\text{Gamma}(13,7).\n$$\nThe mean of a $\\text{Gamma}(\\alpha',\\beta')$ distribution (rate parameterization) is $\\alpha'/\\beta'$, hence the updated expected value is\n$$\n\\mathbb{E}[\\lambda \\mid y_{1:5}] = \\frac{13}{7}.\n$$", "answer": "$$\\boxed{\\frac{13}{7}}$$", "id": "1946607"}, {"introduction": "Our previous examples focused on binary outcomes or single counts, but many problems involve classifying data into one of several categories. This practice extends Bayesian inference to the multinomial setting, where we are interested in the probabilities of multiple, mutually exclusive outcomes. You will see how the Dirichlet distribution acts as a natural and convenient prior for these probabilities, generalizing the Beta distribution to more than two categories. [@problem_id:1946611]", "problem": "An urban planner is analyzing the behavior of a newly installed \"adaptive\" traffic light at a city intersection. The light cycles through three states: Red (R), Green (G), and Yellow (Y). Let $p_R$, $p_G$, and $p_Y$ represent the long-term proportions of time the light spends in the red, green, and yellow states, respectively. Note that $p_R + p_G + p_Y = 1$.\n\nBefore collecting any data, the planner assumes a state of complete ignorance about these proportions. This initial belief is modeled using a prior probability distribution over the possible values of $(p_R, p_G, p_Y)$. The chosen prior is a Dirichlet distribution with parameters $(\\alpha_R, \\alpha_G, \\alpha_Y) = (1, 1, 1)$.\n\nTo update this belief, the planner records the state of the traffic light at 120 randomly chosen moments throughout a day. The collected data consists of 58 observations of the Red state, 52 of the Green state, and 10 of the Yellow state.\n\nIt is a known property that if the prior distribution for the parameters of a multinomial process is a Dirichlet distribution, then the posterior distribution after observing data is also a Dirichlet distribution. Given the planner's prior belief and the collected data, determine the parameters $(\\alpha'_R, \\alpha'_G, \\alpha'_Y)$ of the resulting posterior Dirichlet distribution for $(p_R, p_G, p_Y)$.", "solution": "Let the prior for $(p_{R},p_{G},p_{Y})$ be $\\operatorname{Dirichlet}(\\alpha_{R},\\alpha_{G},\\alpha_{Y})$ with density proportional to $\\prod_{i \\in \\{R,G,Y\\}} p_{i}^{\\alpha_{i}-1}$. Given multinomial data with counts $(n_{R},n_{G},n_{Y})$, the likelihood is proportional to $\\prod_{i \\in \\{R,G,Y\\}} p_{i}^{n_{i}}$. By Bayes’ rule and the conjugacy of the Dirichlet prior to the multinomial likelihood, the posterior is proportional to\n$$\n\\prod_{i \\in \\{R,G,Y\\}} p_{i}^{\\alpha_{i}-1} \\times \\prod_{i \\in \\{R,G,Y\\}} p_{i}^{n_{i}} \\;=\\; \\prod_{i \\in \\{R,G,Y\\}} p_{i}^{(\\alpha_{i}+n_{i})-1},\n$$\nwhich is a $\\operatorname{Dirichlet}(\\alpha_{R}+n_{R},\\alpha_{G}+n_{G},\\alpha_{Y}+n_{Y})$ distribution.\n\nWith the given prior $(\\alpha_{R},\\alpha_{G},\\alpha_{Y})=(1,1,1)$ and observed counts $(n_{R},n_{G},n_{Y})=(58,52,10)$, the posterior parameters are\n$$\n(\\alpha'_{R},\\alpha'_{G},\\alpha'_{Y})=(1+58,\\,1+52,\\,1+10)=(59,53,11).\n$$", "answer": "$$\\boxed{\\begin{pmatrix}59 & 53 & 11\\end{pmatrix}}$$", "id": "1946611"}]}