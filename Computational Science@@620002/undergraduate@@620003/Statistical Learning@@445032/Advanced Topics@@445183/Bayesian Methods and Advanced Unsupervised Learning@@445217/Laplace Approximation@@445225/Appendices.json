{"hands_on_practices": [{"introduction": "The Laplace approximation is most transparent in the ideal case where the posterior distribution is already Gaussian, at which point the method becomes exact. This practice [@problem_id:3137211] provides a perfect baseline for understanding its mechanics by examining a multivariate Bayesian linear model. You will see firsthand its power to capture the full covariance structure of the posterior, a crucial feature that simpler methods like mean-field variational approximations fail to represent.", "problem": "You must write a complete, runnable program that constructs a two-parameter Bayesian posterior with strong correlation, computes the Laplace approximation’s covariance and correlation, and compares it to an independent Gaussian variational approximation by evaluating the Kullback–Leibler divergence (KL). Work entirely in a purely mathematical setting using the following generative model as the fundamental base.\n\nConsider a Bayesian linear model with two parameters. For given data matrix $X \\in \\mathbb{R}^{n \\times 2}$ and response vector $y \\in \\mathbb{R}^{n}$, assume the likelihood is Gaussian with known variance and the prior is isotropic Gaussian:\n- Likelihood: $y \\mid \\theta \\sim \\mathcal{N}(X \\theta, \\sigma^{2} I)$ where $\\theta \\in \\mathbb{R}^{2}$ and $I$ is the identity matrix.\n- Prior: $\\theta \\sim \\mathcal{N}(0, \\tau^{2} I)$.\n\nYou must derive, from Bayes’ rule and standard identities of the multivariate normal distribution, the form of the posterior $p(\\theta \\mid y)$, the form of the Laplace approximation about its mode, and the best independent Gaussian variational approximation $q(\\theta)$ that minimizes the Kullback–Leibler divergence (KL) from $q(\\theta)$ to the Laplace approximation. You must not assume any shortcut formulas beyond these foundational facts and identities.\n\nData generation for each test case should be performed as follows:\n- Draw $x_{1} \\in \\mathbb{R}^{n}$ with independent and identically distributed standard normal entries.\n- Draw $z \\in \\mathbb{R}^{n}$ with independent and identically distributed standard normal entries, independent of $x_{1}$.\n- Construct $x_{2} = \\rho x_{1} + \\sqrt{1 - \\rho^{2}} \\, z$ to control collinearity via $\\rho \\in (-1, 1)$.\n- Form $X = [x_{1}, x_{2}] \\in \\mathbb{R}^{n \\times 2}$.\n- Fix a true parameter vector $\\theta_{\\text{true}} \\in \\mathbb{R}^{2}$, generate noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$, and set $y = X \\theta_{\\text{true}} + \\varepsilon$.\n\nYour program must:\n- Compute the Laplace approximation at the maximum a posteriori (MAP) point. In this model the Laplace covariance at the mode equals the inverse of the posterior precision (negative Hessian) at the mode.\n- Compute the posterior correlation under the Laplace covariance, namely the correlation between the two coordinates of $\\theta$ implied by the Laplace covariance.\n- Compute the best independent Gaussian variational approximation $q(\\theta) = \\mathcal{N}(\\mu, \\operatorname{diag}(s^{2}))$ that minimizes the Kullback–Leibler divergence (KL) from $q(\\theta)$ to the Laplace approximation, and then evaluate this KL.\n- Report, for each test case, two floats: the Laplace posterior correlation and the KL value of the best independent Gaussian variational approximation relative to the Laplace approximation.\n\nAngles do not appear in this problem. No physical units are involved; report pure numbers. All final numerical outputs should be rounded to $6$ decimal places.\n\nTest Suite:\nUse the following test cases, where $n$ is the number of samples, $\\rho$ controls feature collinearity, $\\sigma$ is the noise standard deviation, $\\tau$ is the prior standard deviation, and the final element is the random seed to ensure reproducibility. Use the fixed true parameter $\\theta_{\\text{true}} = [1.5, -1.0]^{\\top}$ in all cases.\n\n- Case $1$: $(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.95, 0.5, 3.0, 0)$\n- Case $2$: $(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.50, 0.5, 3.0, 1)$\n- Case $3$: $(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.995, 0.5, 3.0, 2)$\n\nRequired final output format:\n- Your program should produce a single line of output containing a single list with $2 \\times 3 = 6$ floats: for each test case in the order given above, append the Laplace posterior correlation, then the KL value. The final output must be a single line in the exact format\n- Example shape (not actual values): $[c_{1},k_{1},c_{2},k_{2},c_{3},k_{3}]$\n- Each value must be rounded to $6$ decimal places.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. We will proceed with a full derivation and solution.\n\n### 1. Posterior Distribution Derivation\n\nThe analysis begins with Bayes' rule to find the posterior distribution of the parameters $\\theta$. The posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\theta | y, X) \\propto p(y | \\theta, X) p(\\theta)\n$$\nThe likelihood and prior are given as Gaussian distributions:\n-   Likelihood: $p(y | \\theta, X) = \\mathcal{N}(y | X\\theta, \\sigma^2 I) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta)\\right)$\n-   Prior: $p(\\theta) = \\mathcal{N}(\\theta | 0, \\tau^2 I) \\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\theta^T\\theta\\right)$\n\nThe logarithm of the posterior distribution is therefore:\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\nwhere $C$ is a normalization constant independent of $\\theta$. To identify the form of the posterior, we expand the terms and group by powers of $\\theta$:\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y^Ty - 2y^TX\\theta + \\theta^T X^T X \\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2}\\left( \\theta^T \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right) \\theta - 2\\theta^T \\left(\\frac{1}{\\sigma^2}X^Ty\\right) \\right) + C'\n$$\nThis expression is a quadratic form in $\\theta$, which indicates that the posterior distribution is also a multivariate Gaussian, $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Sigma_p)$. The log-density of a general multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ is $-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu) + \\text{const} = -\\frac{1}{2}(\\theta^T\\Sigma^{-1}\\theta - 2\\theta^T\\Sigma^{-1}\\mu + \\text{const})$.\n\nBy comparing the terms in the log-posterior with the general form, we can identify the posterior precision matrix $\\Lambda_p = \\Sigma_p^{-1}$ and the posterior mean $\\mu_p$:\n$$\n\\Lambda_p = \\Sigma_p^{-1} = \\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\n$$\n$$\n\\Lambda_p \\mu_p = \\frac{1}{\\sigma^2}X^Ty \\implies \\mu_p = \\Lambda_p^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right) = \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right)^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right)\n$$\nThus, the posterior is $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Lambda_p^{-1})$.\n\n### 2. Laplace Approximation and Posterior Correlation\n\nThe Laplace approximation provides a Gaussian approximation to a posterior distribution, centered at the mode of the posterior (the Maximum a Posteriori or MAP estimate, $\\theta_{\\text{MAP}}$). The covariance of this Gaussian is given by the negative inverse of the Hessian of the log-posterior evaluated at the mode.\n\nFirst, we find the mode by setting the gradient of the log-posterior with respect to $\\theta$ to zero:\n$$\n\\nabla_\\theta \\log p(\\theta | y, X) = \\frac{1}{\\sigma^2}(X^Ty - X^TX\\theta) - \\frac{1}{\\tau^2}\\theta = 0\n$$\nSolving for $\\theta$ gives $\\theta_{\\text{MAP}} = \\mu_p$. As the posterior is exactly Gaussian, its mode is equal to its mean.\n\nNext, we calculate the Hessian (the matrix of second derivatives):\n$$\n\\nabla_\\theta^2 \\log p(\\theta | y, X) = -\\frac{1}{\\sigma^2}X^TX - \\frac{1}{\\tau^2}I = -\\Lambda_p\n$$\nThe Hessian is constant and does not depend on $\\theta$. The covariance of the Laplace approximation, $\\Sigma_L$, is:\n$$\n\\Sigma_L = \\left(-\\nabla_\\theta^2 \\log p(\\theta | y, X)\\Big|_{\\theta_{\\text{MAP}}}\\right)^{-1} = (\\Lambda_p)^{-1} = \\Sigma_p\n$$\nFor this specific model, the Laplace approximation is not an approximation; it is the exact posterior distribution, $q_L(\\theta) = p(\\theta | y, X)$. The required posterior correlation is computed from this exact covariance matrix $\\Sigma_L = \\Sigma_p$. For a $2 \\times 2$ covariance matrix $\\Sigma_L = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the correlation is:\n$$\n\\rho_{12} = \\frac{\\Sigma_{12}}{\\sqrt{\\Sigma_{11}\\Sigma_{22}}}\n$$\n\n### 3. Variational Approximation and KL Divergence\n\nWe seek the best independent Gaussian variational approximation $q(\\theta) = q_1(\\theta_1)q_2(\\theta_2) = \\mathcal{N}(\\theta | \\mu_q, \\Sigma_q)$, where $\\Sigma_q$ is a diagonal matrix. The objective is to minimize the Kullback-Leibler (KL) divergence from $q(\\theta)$ to the true posterior $p(\\theta|y,X)$, which is equivalent to the Laplace approximation $q_L(\\theta)$ in this case. The objective is to minimize $\\text{KL}(q || q_L)$.\n\nThe general formula for the KL divergence between two multivariate Gaussian distributions $q=\\mathcal{N}(\\mu_q, \\Sigma_q)$ and $p=\\mathcal{N}(\\mu_p, \\Sigma_p)$ is:\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - d + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) + (\\mu_p - \\mu_q)^T\\Sigma_p^{-1}(\\mu_p - \\mu_q) \\right]\n$$\nwhere $d$ is the dimension ($d=2$ here). The term involving the means is minimized at zero when $\\mu_q = \\mu_p$.\n\nFor the covariance, the standard result from mean-field variational inference for a Gaussian target $p(\\theta) = \\mathcal{N}(\\mu_p, \\Sigma_p)$ is that the optimal factorized distribution $q(\\theta) = \\mathcal{N}(\\mu_q, \\Sigma_q)$ with $\\Sigma_q$ being diagonal has $\\mu_q = \\mu_p$ and a precision matrix $\\Lambda_q = \\Sigma_q^{-1}$ equal to the diagonal of the target's precision matrix, $\\Lambda_q = \\text{diag}(\\Lambda_p)$. That is, $\\Sigma_q = (\\text{diag}(\\Lambda_p))^{-1}$.\n\nWith $\\mu_q = \\mu_p$, the KL divergence simplifies to:\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - 2 + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) \\right]\n$$\nLet's evaluate the terms. Let $\\Lambda_p = \\Sigma_p^{-1}$. The variational precision is $\\Lambda_q = \\text{diag}(\\Lambda_{p,11}, \\Lambda_{p,22})$.\nThe trace term becomes:\n$$\n\\text{tr}(\\Sigma_p^{-1}\\Sigma_q) = \\text{tr}(\\Lambda_p \\Lambda_q^{-1}) = \\text{tr}\\left( \\begin{pmatrix} \\Lambda_{p,11} & \\Lambda_{p,12} \\\\ \\Lambda_{p,21} & \\Lambda_{p,22} \\end{pmatrix} \\begin{pmatrix} 1/\\Lambda_{p,11} & 0 \\\\ 0 & 1/\\Lambda_{p,22} \\end{pmatrix} \\right) = \\text{tr}\\begin{pmatrix} 1 & \\dots \\\\ \\dots & 1 \\end{pmatrix} = 2\n$$\nThe log-determinant ratio is:\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{|\\Lambda_q|}{|\\Lambda_p|} = \\frac{\\Lambda_{p,11}\\Lambda_{p,22}}{\\Lambda_{p,11}\\Lambda_{p,22} - \\Lambda_{p,12}^2} = \\frac{1}{1 - \\frac{\\Lambda_{p,12}^2}{\\Lambda_{p,11}\\Lambda_{p,22}}}\n$$\nThe correlation computed from the covariance matrix $\\Sigma_p$ is $\\rho_{12}$. The correlation computed from the precision matrix $\\Lambda_p$ is $\\rho_{\\Lambda,12} = \\frac{\\Lambda_{p,12}}{\\sqrt{\\Lambda_{p,11}\\Lambda_{p,22}}}$. For a $2 \\times 2$ matrix, it holds that $\\rho_{12} = -\\rho_{\\Lambda,12}$. Therefore, $\\rho_{12}^2 = \\rho_{\\Lambda,12}^2$.\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{1}{1 - \\rho_{12}^2}\n$$\nSubstituting these into the KL formula yields a remarkably simple result:\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\left(\\frac{1}{1 - \\rho_{12}^2}\\right) - 2 + 2 \\right] = -\\frac{1}{2}\\log(1 - \\rho_{12}^2)\n$$\nThe KL divergence, which measures the inadequacy of the factorized approximation, depends only on the squared posterior correlation of the parameters.\n\n### 4. Algorithm\n\nFor each test case:\n1.  Set the random seed. Generate the data matrix $X \\in \\mathbb{R}^{n \\times 2}$ with correlated columns according to the specified procedure with parameters $n$ and $\\rho$.\n2.  Compute the posterior precision matrix $\\Lambda_p = \\frac{1}{\\sigma^2}X^T X + \\frac{1}{\\tau^2}I$.\n3.  Invert $\\Lambda_p$ to find the posterior covariance matrix $\\Sigma_p = \\Sigma_L = \\Lambda_p^{-1}$.\n4.  Extract the elements $\\Sigma_{11}$, $\\Sigma_{22}$, and $\\Sigma_{12}$ from $\\Sigma_L$.\n5.  Calculate the posterior correlation $\\rho_{12} = \\Sigma_{12} / \\sqrt{\\Sigma_{11}\\Sigma_{22}}$.\n6.  Calculate the KL divergence of the best independent Gaussian variational approximation as $\\text{KL} = -0.5 \\log(1 - \\rho_{12}^2)$.\n7.  Round both results to $6$ decimal places and store them.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For a Bayesian linear model with Gaussian likelihood and prior, the posterior\n    is also Gaussian. This function computes its properties.\n    \"\"\"\n    \n    # Test cases: (n, rho, sigma, tau, seed)\n    test_cases = [\n        (200, 0.95, 0.5, 3.0, 0),\n        (200, 0.50, 0.5, 3.0, 1),\n        (200, 0.995, 0.5, 3.0, 2),\n    ]\n\n    # Fixed true parameter vector for data generation\n    theta_true = np.array([1.5, -1.0])\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, tau, seed = case\n\n        # 1. Data Generation\n        # Set a random number generator with a seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate x1 and z from standard normal distributions.\n        x1 = rng.standard_normal(n)\n        z = rng.standard_normal(n)\n        \n        # Construct x2 to have a specified correlation rho with x1.\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * z\n        \n        # Form the data matrix X.\n        X = np.stack([x1, x2], axis=1) # Shape: (n, 2)\n        \n        # Generate response vector y. Note: y is not needed for the posterior\n        # covariance, correlation, or the KL divergence, as these depend only\n        # on X, sigma, and tau in this model.\n        # eps = rng.normal(0, sigma, n)\n        # y = X @ theta_true + eps\n\n        # 2. Compute Posterior Precision and Covariance (Laplace Approximation)\n        # The posterior precision matrix is Lambda_p = (1/sigma^2) * X'X + (1/tau^2) * I\n        XtX = X.T @ X\n        lambda_p = (1 / sigma**2) * XtX + (1 / tau**2) * np.eye(2)\n        \n        # The posterior covariance matrix is the inverse of the precision matrix.\n        # This is also the covariance of the Laplace approximation.\n        sigma_l = np.linalg.inv(lambda_p)\n\n        # 3. Compute Posterior Correlation\n        # Extract elements of the covariance matrix.\n        sigma_11 = sigma_l[0, 0]\n        sigma_22 = sigma_l[1, 1]\n        sigma_12 = sigma_l[0, 1]\n        \n        # Calculate the correlation coefficient.\n        correlation = sigma_12 / np.sqrt(sigma_11 * sigma_22)\n\n        # 4. Compute KL Divergence\n        # The KL divergence for the best independent Gaussian variational approximation\n        # has a simple closed form related to the posterior correlation.\n        kl_divergence = -0.5 * np.log(1 - correlation**2)\n\n        # Append rounded results to the list.\n        results.append(round(correlation, 6))\n        results.append(round(kl_divergence, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3137211"}, {"introduction": "After seeing the ideal case, we now turn to a common scenario where the Laplace approximation's limitations become apparent. This exercise [@problem_id:3137250] explores a model with a skewed posterior, where the mode (which centers the Laplace approximation) differs from the mean. By investigating how this discrepancy affects the calculation of expected loss, you will uncover a key weakness of using a symmetric Gaussian to approximate an asymmetric distribution.", "problem": "Consider the following Bayesian one-parameter model, where independent observations are drawn from a Poisson distribution and the prior is Gamma. Let $\\{y_i\\}_{i=1}^n$ be independent draws from $\\text{Poisson}(\\lambda)$ with unknown rate $\\lambda > 0$, and use a Gamma prior $\\lambda \\sim \\text{Gamma}(a,b)$ with shape $a > 0$ and rate $b > 0$. Use Bayes' theorem together with the definitions of the Poisson likelihood and Gamma prior as the fundamental base. The Maximum A Posteriori (MAP) estimator is the posterior mode. The Laplace approximation approximates the posterior density near the MAP by a Gaussian distribution centered at the mode, with variance equal to the inverse of the negative second derivative (the negative Hessian for scalar) of the log-posterior at the mode.\n\nYour tasks:\n- Derive the posterior density up to a normalizing constant and express it in a recognized parametric form.\n- Derive the posterior mean and the posterior mode by differentiating the log-posterior.\n- Using the Laplace approximation centered at the posterior mode, derive the scalar Gaussian variance from the second derivative of the log-posterior at the mode.\n- For the convex risk given by the squared loss $L(\\lambda; t) = (\\lambda - t)^2$, compute the expected loss under the exact posterior and under the Laplace approximation. Use only the definitions of expectation for these distributions and quantities you derived.\n- Implement a program that, for each test case in the test suite below, computes the absolute difference between the exact expected loss and the Laplace-approximated expected loss.\n\nTest suite:\n- Case $1$: Prior $(a,b) = (2.5, 1.0)$, data $y = (0,1,2)$, loss target $t$ equal to the posterior mode of this case.\n- Case $2$: Prior $(a,b) = (0.3, 1.0)$, data $y = (1)$, loss target $t$ equal to the sample mean of the data for this case.\n- Case $3$: Prior $(a,b) = (2.0, 1.0)$, data $y = (10,12,9,11,8)$, loss target $t = 10.0$.\n- Case $4$: Prior $(a,b) = (3.0, 0.5)$, data $y = (0,0,0,1)$, loss target $t = 0.8$.\n\nProgram requirements:\n- Compute, for each case, the posterior shape $\\alpha$ and rate $\\beta$, the posterior mean and mode, the Laplace variance at the mode, and the expected squared loss under the exact posterior and under the Laplace approximation.\n- For each case, output the absolute difference between the exact expected loss and the Laplace-approximated expected loss as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number rounded to six decimal places (e.g., $[0.123456,0.000001,2.718282,1.000000]$).\n\nAll mathematical symbols, variables, functions, operators, and numbers in this problem statement are written using LaTeX. There are no physical units, angles, or percentages involved in this problem; all numerical outputs must be real numbers without units.", "solution": "The problem requires a thorough analysis of a Bayesian one-parameter model involving a Poisson likelihood and a Gamma prior. This framework is a classic example of conjugate families in Bayesian statistics. We will proceed by first validating the problem statement, which is found to be sound, and then systematically deriving the required quantities.\n\nThe model is specified as follows:\n- The data $\\{y_i\\}_{i=1}^n$ are independent and identically distributed draws from a Poisson distribution with parameter $\\lambda$: $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$.\n- The prior distribution for the unknown parameter $\\lambda$ is a Gamma distribution with shape parameter $a$ and rate parameter $b$: $\\lambda \\sim \\text{Gamma}(a, b)$.\n\nThe probability mass function (PMF) for a Poisson distribution is $P(y \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}$ for $y \\in \\{0, 1, 2, \\dots\\}$. The probability density function (PDF) for a Gamma distribution is $p(\\lambda \\mid a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}e^{-b\\lambda}$ for $\\lambda > 0$.\n\nFirst, we derive the posterior density of $\\lambda$. By Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior density:\n$$ p(\\lambda \\mid y_1, \\dots, y_n) \\propto P(y_1, \\dots, y_n \\mid \\lambda) \\, p(\\lambda) $$\nGiven that the observations are independent, the likelihood is the product of the individual PMFs:\n$$ P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\nAs a function of $\\lambda$, the likelihood is proportional to $e^{-n\\lambda} \\lambda^{\\sum y_i}$. The prior density is proportional to $\\lambda^{a-1}e^{-b\\lambda}$.\nCombining these, the posterior density is:\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\left(e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}\\right) \\left(\\lambda^{a-1}e^{-b\\lambda}\\right) $$\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{(\\sum y_i + a) - 1} e^{-(n+b)\\lambda} $$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution, which demonstrates the conjugacy of the Gamma prior for the Poisson likelihood. The posterior distribution is $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$, with updated parameters:\n- Posterior shape: $\\alpha = \\sum_{i=1}^n y_i + a$\n- Posterior rate: $\\beta = n + b$\n\nSecond, we derive the posterior mean and posterior mode. For a Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$, the mean and mode are well-known quantities.\n- The posterior mean is $E[\\lambda \\mid \\mathbf{y}] = \\frac{\\alpha}{\\beta} = \\frac{\\sum y_i + a}{n + b}$.\n- The posterior mode (Maximum A Posteriori estimate, $\\lambda_{\\text{MAP}}$) is found by maximizing the posterior density. For a Gamma distribution with shape $\\alpha > 1$, the mode is $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta} = \\frac{\\sum y_i + a - 1}{n + b}$. All test cases provided in the problem satisfy the condition $\\alpha > 1$.\n\nThird, we derive the variance for the Laplace approximation. The Laplace approximation approximates the posterior density with a Gaussian (Normal) distribution centered at the posterior mode, $\\lambda_{\\text{MAP}}$. The variance, which we denote $\\sigma_L^2$, is the inverse of the negative of the second derivative of the log-posterior density evaluated at the mode.\nThe log-posterior, up to an additive constant, is:\n$$ \\log p(\\lambda \\mid \\mathbf{y}) = (\\alpha-1)\\log\\lambda - \\beta\\lambda + C $$\nThe first derivative with respect to $\\lambda$ is:\n$$ \\frac{d}{d\\lambda} \\log p(\\lambda \\mid \\mathbf{y}) = \\frac{\\alpha-1}{\\lambda} - \\beta $$\nThe second derivative is:\n$$ \\frac{d^2}{d\\lambda^2} \\log p(\\lambda \\mid \\mathbf{y}) = -\\frac{\\alpha-1}{\\lambda^2} $$\nEvaluating the negative of the second derivative (the observed Fisher information) at the mode $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta}$:\n$$ J(\\lambda_{\\text{MAP}}) = -\\left(-\\frac{\\alpha-1}{(\\frac{\\alpha-1}{\\beta})^2}\\right) = \\frac{\\alpha-1}{(\\alpha-1)^2 / \\beta^2} = \\frac{\\beta^2}{\\alpha-1} $$\nThe variance of the Laplace approximation is the inverse of this quantity:\n$$ \\sigma_L^2 = [J(\\lambda_{\\text{MAP}})]^{-1} = \\frac{\\alpha-1}{\\beta^2} $$\nThus, the Laplace approximation to the posterior is $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$, where the mean is $\\mu_L = \\lambda_{\\text{MAP}} = \\frac{\\alpha-1}{\\beta}$ and the variance is $\\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$.\n\nFourth, we compute the expected squared loss $L(\\lambda; t) = (\\lambda - t)^2$. The expected loss under a probability distribution for $\\lambda$ is given by $E[(\\lambda-t)^2]$. This can be expanded using the definition of variance, $\\text{Var}(\\lambda) = E[\\lambda^2] - (E[\\lambda])^2$:\n$$ E[(\\lambda-t)^2] = E[\\lambda^2 - 2t\\lambda + t^2] = E[\\lambda^2] - 2tE[\\lambda] + t^2 $$\n$$ E[(\\lambda-t)^2] = (\\text{Var}(\\lambda) + (E[\\lambda])^2) - 2tE[\\lambda] + t^2 = \\text{Var}(\\lambda) + (E[\\lambda] - t)^2 $$\nThis formula relates the expected loss to the variance and the squared bias of the distribution's mean relative to the target $t$.\n\nWe apply this formula to both the exact posterior and the Laplace approximation.\n- For the exact posterior, $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$:\n  - Mean: $E_{\\text{post}}[\\lambda] = \\frac{\\alpha}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{post}}(\\lambda) = \\frac{\\alpha}{\\beta^2}$\n  - Expected loss: $E_{\\text{exact}} = \\text{Var}_{\\text{post}}(\\lambda) + (E_{\\text{post}}[\\lambda] - t)^2 = \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta} - t\\right)^2$.\n- For the Laplace approximation, $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$:\n  - Mean: $E_{\\text{Laplace}}[\\lambda] = \\mu_L = \\frac{\\alpha-1}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{Laplace}}(\\lambda) = \\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$\n  - Expected loss: $E_{\\text{Laplace}} = \\text{Var}_{\\text{Laplace}}(\\lambda) + (E_{\\text{Laplace}}[\\lambda] - t)^2 = \\frac{\\alpha-1}{\\beta^2} + \\left(\\frac{\\alpha-1}{\\beta} - t\\right)^2$.\n\nThe program will implement these final formulas to compute the absolute difference $|E_{\\text{exact}} - E_{\\text{Laplace}}|$ for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by calculating the absolute difference\n    between the expected squared loss under the exact posterior and under the Laplace approximation.\n    \"\"\"\n    \n    # Test suite definition: (prior_a, prior_b, data_y, t_config)\n    # t_config is a tuple (type, value) where type is 'mode', 'mean', or 'value'.\n    test_cases = [\n        (2.5, 1.0, [0, 1, 2], ('mode', None)),\n        (0.3, 1.0, [1], ('mean', None)),\n        (2.0, 1.0, [10, 12, 9, 11, 8], ('value', 10.0)),\n        (3.0, 0.5, [0, 0, 0, 1], ('value', 0.8)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a_prior, b_prior, y, t_config = case\n        \n        # Convert y to a numpy array for easier calculations\n        y = np.array(y)\n        \n        # Calculate sufficient statistics from data\n        n = len(y)\n        sum_y = np.sum(y)\n        \n        # Calculate posterior parameters\n        # Posterior is Gamma(alpha, beta)\n        alpha_post = sum_y + a_prior\n        beta_post = float(n + b_prior)\n\n        # Ensure posterior mode is well-defined (alpha > 1)\n        if alpha_post <= 1:\n            # This case is not expected based on problem validation\n            # but good practice to handle.\n            results.append(np.nan)\n            continue\n\n        # Determine the loss target t based on the configuration\n        t_type, t_val = t_config\n        t = 0.0\n        if t_type == 'value':\n            t = t_val\n        elif t_type == 'mode':\n            # Posterior mode (MAP)\n            t = (alpha_post - 1) / beta_post\n        elif t_type == 'mean':\n            # Sample mean of the data\n            t = np.mean(y)\n\n        # === Calculations for the exact posterior: Gamma(alpha_post, beta_post) ===\n        \n        # Mean of the exact posterior\n        mean_exact = alpha_post / beta_post\n        # Variance of the exact posterior\n        var_exact = alpha_post / (beta_post**2)\n        # Expected squared loss for the exact posterior\n        expected_loss_exact = var_exact + (mean_exact - t)**2\n\n        # === Calculations for the Laplace approximation: Normal(mu_L, sigma_L^2) ===\n        \n        # Mean of the Laplace approximation is the posterior mode\n        mean_laplace = (alpha_post - 1) / beta_post\n        # Variance of the Laplace approximation\n        var_laplace = (alpha_post - 1) / (beta_post**2)\n        # Expected squared loss for the Laplace approximation\n        expected_loss_laplace = var_laplace + (mean_laplace - t)**2\n\n        # Calculate the absolute difference between the two expected losses\n        abs_diff = abs(expected_loss_exact - expected_loss_laplace)\n        results.append(abs_diff)\n    \n    # Format the output as a comma-separated list of strings with 6 decimal places\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3137250"}, {"introduction": "Our final practice tackles a more advanced application: using the Laplace approximation to estimate the marginal likelihood, or model evidence, which is essential for Bayesian model comparison. You will work with a logistic regression model [@problem_id:3137141] where nearly collinear predictors create a challenging \"ridge\" in the likelihood surface. This exercise provides an opportunity to assess the approximation's accuracy for the log-evidence in a realistic, near-nonidentifiable setting.", "problem": "You are asked to study the behavior of the Laplace approximation for a two-parameter binary logistic regression model when the likelihood surface has ridges due to near-nonidentifiability. The model is defined as follows. For each observation index $i \\in \\{1,\\dots,n\\}$, there are covariates $x_i \\in \\mathbb{R}$ and $z_i \\in \\mathbb{R}$, and a binary response $y_i \\in \\{0,1\\}$. The conditional distribution is\n$$\ny_i \\mid \\boldsymbol{\\theta} \\sim \\text{Bernoulli}\\left(\\sigma(\\eta_i)\\right), \\quad \\eta_i = \\theta_1 x_i + \\theta_2 z_i, \\quad \\sigma(t) = \\frac{1}{1+\\exp(-t)},\n$$\nwith parameter vector $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)^\\top$. The prior on $\\boldsymbol{\\theta}$ is a centered isotropic Gaussian with variance $\\tau^2$,\n$$\n\\boldsymbol{\\theta} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\tau^2 \\mathbf{I}_2\\right).\n$$\nIn this setting, a ridge arises when the columns of the design matrix are nearly collinear so that the log-likelihood curvature has a tiny eigenvalue. You will construct such near-nonidentifiability by setting $z_i$ to be nearly proportional to $x_i$.\n\nFundamental base. Use only the following as the starting point:\n- The definition of the Bernoulli probability mass function and the logistic (sigmoid) link.\n- The rules for gradients and Hessians of compositions and sums.\n- The definition of the Gaussian probability density function (PDF).\n- The definition of the Laplace approximation as a second-order Taylor approximation of the log-integrand around its mode, together with Gaussian integral evaluation.\n\nData construction. Fix $n = 60$ and define, deterministically,\n- $x_i = \\frac{i - 0.5}{n}$ for $i \\in \\{1,\\dots,n\\}$,\n- $z_i = x_i + \\varepsilon \\cos\\left(\\frac{2\\pi i}{n}\\right)$ for a given $\\varepsilon \\ge 0$,\n- $y_i = \\mathbf{1}\\{x_i > 0.5\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nTasks to implement for each test case $(\\varepsilon,\\tau)$:\n- Construct the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times 2}$ with columns $\\mathbf{x}$ and $\\mathbf{z}$, where $\\mathbf{x} = (x_1,\\dots,x_n)^\\top$ and $\\mathbf{z} = (z_1,\\dots,z_n)^\\top$, and the response vector $\\mathbf{y} = (y_1,\\dots,y_n)^\\top$.\n- Define the log-likelihood function\n$$\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log\\!\\left(1 + \\exp(\\eta_i)\\right) \\right), \\quad \\eta_i = \\theta_1 x_i + \\theta_2 z_i,\n$$\nand the normalized log-prior\n$$\n\\log p(\\boldsymbol{\\theta}) = -\\frac{2}{2}\\log(2\\pi \\tau^2) - \\frac{1}{2\\tau^2}\\boldsymbol{\\theta}^\\top \\boldsymbol{\\theta},\n$$\nso that the log-integrand is $f(\\boldsymbol{\\theta}) = \\ell(\\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta})$.\n- Compute the posterior mode $\\hat{\\boldsymbol{\\theta}}$ by maximizing $f(\\boldsymbol{\\theta})$ using Newton's method with backtracking line search. Terminate when the Euclidean norm of the gradient is less than $10^{-8}$ or after $50$ iterations.\n- At $\\hat{\\boldsymbol{\\theta}}$, compute the smallest eigenvalue of the negative Hessian of the log-likelihood,\n$$\n\\lambda_{\\min} = \\lambda_{\\min}\\!\\left(-\\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})\\right).\n$$\nThis quantity diagnoses the ridge severity.\n- Compute the Laplace approximation to the log evidence (log marginal likelihood),\n$$\n\\log Z_{\\text{Lap}} = f(\\hat{\\boldsymbol{\\theta}}) + \\frac{2}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det\\!\\left(-\\nabla^2 f(\\hat{\\boldsymbol{\\theta}})\\right).\n$$\n- Compute a numerically stable approximation to the exact log evidence,\n$$\n\\log Z_{\\text{num}} \\approx \\log \\iint_{\\mathbb{R}^2} \\exp\\!\\left(\\ell(\\boldsymbol{\\theta})\\right) \\, p(\\boldsymbol{\\theta}) \\, d\\theta_1 \\, d\\theta_2,\n$$\nby a two-dimensional trapezoidal rule on a square grid $\\theta_j \\in [-L,L]$ with $L = 6\\tau$ and $401$ equally spaced points per axis, using a log-sum-exp accumulation that incorporates the trapezoid weights exactly. The Gaussian prior ensures integrability; the choice of $L$ captures practically all posterior mass for the given $\\tau$.\n- Report the absolute error in log evidence,\n$\nE = \\left|\\log Z_{\\text{Lap}} - \\log Z_{\\text{num}}\\right|.\n$\n\nTest suite. Run the above for the following parameter pairs $(\\varepsilon,\\tau)$:\n- $(0, 1)$,\n- $(10^{-3}, 1)$,\n- $(10^{-3}, 5)$,\n- $(10^{-1}, 1)$,\n- $(0, 5)$.\n\nOutput specification. Your program must:\n- Produce a single line containing a list of length $5$. Each element corresponds to one test case in the order listed above and must itself be a list of two floats $[\\lambda_{\\min}, E]$.\n- Each float must be rounded to exactly $6$ decimal places.\n- The final output must therefore look like a single-line string of the form \"[[a,b],[c,d],...]\" where each letter represents a float with $6$ digits after the decimal point and there are exactly $5$ inner lists.", "solution": "The problem requires an analysis of the Laplace approximation's accuracy for a two-parameter Bayesian logistic regression model, particularly under conditions of near-nonidentifiability. The solution involves deriving the necessary mathematical expressions for the posterior distribution, implementing a numerical optimization scheme to find the posterior mode, and then computing two different estimates for the marginal likelihood: one via the Laplace approximation and another via high-resolution numerical integration.\n\nFirst, we establish the mathematical foundation. The model for a single observation $i \\in \\{1, \\dots, n\\}$ is a Bernoulli trial, $y_i \\sim \\text{Bernoulli}(\\sigma(\\eta_i))$, where the linear predictor is $\\eta_i = \\theta_1 x_i + \\theta_2 z_i = \\mathbf{w}_i^\\top \\boldsymbol{\\theta}$ with $\\mathbf{w}_i = (x_i, z_i)^\\top$ and parameters $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)^\\top$. The function $\\sigma(t) = (1+\\exp(-t))^{-1}$ is the logistic sigmoid function.\n\nThe log-likelihood, $\\ell(\\boldsymbol{\\theta})$, for $n$ independent observations is the sum of the individual log-probability mass functions:\n$$\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log\\left(\\sigma(\\eta_i)^{y_i} (1-\\sigma(\\eta_i))^{1-y_i}\\right) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log(1 + \\exp(\\eta_i)) \\right).\n$$\nThe prior on the parameters $\\boldsymbol{\\theta}$ is a centered isotropic Gaussian distribution, $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^2 \\mathbf{I}_2)$, with a log-probability density function (PDF) given by:\n$$\n\\log p(\\boldsymbol{\\theta}) = \\log\\left(\\frac{1}{(2\\pi\\tau^2)^{2/2}} \\exp\\left(-\\frac{\\boldsymbol{\\theta}^\\top\\boldsymbol{\\theta}}{2\\tau^2}\\right)\\right) = -\\log(2\\pi\\tau^2) - \\frac{1}{2\\tau^2}\\boldsymbol{\\theta}^\\top\\boldsymbol{\\theta}.\n$$\nThe unnormalized log-posterior density, which we denote as $f(\\boldsymbol{\\theta})$, is the sum of the log-likelihood and the log-prior:\n$$\nf(\\boldsymbol{\\theta}) = \\ell(\\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta}).\n$$\nThe evidence, or marginal likelihood, is the integral of the unnormalized posterior density over the parameter space: $Z = \\int \\exp(f(\\boldsymbol{\\theta})) d\\boldsymbol{\\theta}$.\n\nTo find the posterior mode $\\hat{\\boldsymbol{\\theta}}$, we maximize $f(\\boldsymbol{\\theta})$ using Newton's method. This requires its gradient $\\nabla f(\\boldsymbol{\\theta})$ and Hessian $\\nabla^2 f(\\boldsymbol{\\theta})$. Using the chain rule and the identity $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$, we derive these quantities.\n\nThe gradient of the log-likelihood is:\n$$\n\\nabla \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left(y_i - \\sigma(\\eta_i)\\right) \\frac{\\partial\\eta_i}{\\partial\\boldsymbol{\\theta}} = \\sum_{i=1}^n \\left(y_i - \\sigma(\\eta_i)\\right) \\mathbf{w}_i = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\sigma}),\n$$\nwhere $\\mathbf{X}$ is the $n \\times 2$ design matrix, $\\mathbf{y}$ is the $n \\times 1$ response vector, and $\\boldsymbol{\\sigma}$ is the $n \\times 1$ vector of probabilities $\\sigma(\\eta_i)$.\n\nThe Hessian of the log-likelihood is:\n$$\n\\nabla^2 \\ell(\\boldsymbol{\\theta}) = -\\sum_{i=1}^n \\sigma(\\eta_i)(1-\\sigma(\\eta_i)) \\mathbf{w}_i \\mathbf{w}_i^\\top = -\\mathbf{X}^\\top \\mathbf{S} \\mathbf{X},\n$$\nwhere $\\mathbf{S}$ is an $n \\times n$ diagonal matrix with entries $S_{ii} = \\sigma(\\eta_i)(1-\\sigma(\\eta_i))$. This matrix is negative semi-definite, so $\\ell(\\boldsymbol{\\theta})$ is a concave function.\n\nThe derivatives of the log-prior are:\n$$\n\\nabla \\log p(\\boldsymbol{\\theta}) = -\\frac{1}{\\tau^2}\\boldsymbol{\\theta}, \\quad \\nabla^2 \\log p(\\boldsymbol{\\theta}) = -\\frac{1}{\\tau^2}\\mathbf{I}_2.\n$$\nCombining these, the gradient and Hessian of the log-posterior integrand $f(\\boldsymbol{\\theta})$ are:\n$$\n\\nabla f(\\boldsymbol{\\theta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\sigma}) - \\frac{1}{\\tau^2}\\boldsymbol{\\theta},\n$$\n$$\n\\nabla^2 f(\\boldsymbol{\\theta}) = -\\mathbf{X}^\\top \\mathbf{S} \\mathbf{X} - \\frac{1}{\\tau^2}\\mathbf{I}_2.\n$$\nSince $-\\mathbf{X}^\\top\\mathbf{S}\\mathbf{X}$ is negative semi-definite and $-\\frac{1}{\\tau^2}\\mathbf{I}_2$ is negative definite, their sum $\\nabla^2 f(\\boldsymbol{\\theta})$ is strictly negative definite. This ensures that $f(\\boldsymbol{\\theta})$ is a strictly concave function and has a unique maximum.\n\nWe implement Newton's method with backtracking line search to find the mode $\\hat{\\boldsymbol{\\theta}} = \\arg\\max_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta})$. The iterative update is $\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\alpha_t (\\nabla^2 f(\\boldsymbol{\\theta}^{(t)}))^{-1} \\nabla f(\\boldsymbol{\\theta}^{(t)})$, where $\\alpha_t \\in (0, 1]$ is the step size. The optimization terminates when $\\|\\nabla f(\\boldsymbol{\\theta})\\|_2 < 10^{-8}$ or after $50$ iterations.\n\nWith the mode $\\hat{\\boldsymbol{\\theta}}$ found, we compute the following quantities:\n1.  **Ridge Severity Diagnostic, $\\lambda_{\\min}$**: This is the smallest eigenvalue of the negative Hessian of the log-likelihood, evaluated at the mode: $\\lambda_{\\min} = \\lambda_{\\min}(-\\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})) = \\lambda_{\\min}(\\mathbf{X}^\\top \\mathbf{S}(\\hat{\\boldsymbol{\\theta}}) \\mathbf{X})$. A small value indicates a flat direction in the likelihood surface.\n\n2.  **Laplace Approximation, $\\log Z_{\\text{Lap}}$**: This method relies on a second-order Taylor expansion of $f(\\boldsymbol{\\theta})$ around $\\hat{\\boldsymbol{\\theta}}$, which approximates the posterior as a Gaussian. The resulting log-evidence is:\n    $$\n    \\log Z_{\\text{Lap}} = f(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(-\\nabla^2 f(\\hat{\\boldsymbol{\\theta}})),\n    $$\n    where the parameter dimension is $k=2$.\n\n3.  **Numerical Integration, $\\log Z_{\\text{num}}$**: A more accurate \"ground truth\" estimate is obtained by numerically integrating $Z = \\iint \\exp(f(\\boldsymbol{\\theta})) d\\theta_1 d\\theta_2$. We use a two-dimensional trapezoidal rule on a grid $\\theta_j \\in [-L, L], j \\in \\{1,2\\}$, with $L=6\\tau$ and $N_{pts}=401$ points per axis. The area element of a grid cell is $dA = (\\frac{2L}{N_{pts}-1})^2$. To ensure numerical stability, the sum is computed in the log domain using the log-sum-exp trick:\n    $$\n    \\log Z_{\\text{num}} = \\log\\left(\\sum_{j,k} w_{jk} \\exp(f(\\boldsymbol{\\theta}_{jk}))\\right) = \\text{logsumexp}_{j,k}\\left(f(\\boldsymbol{\\theta}_{jk}) + \\log w_{jk}\\right),\n    $$\n    where $w_{jk}$ are the trapezoidal weights ($dA$ for interior points, $dA/2$ for edge points not at corners, $dA/4$ for corner points).\n\n4.  **Absolute Error, $E$**: The final quantity of interest is the error of the approximation, $E = |\\log Z_{\\text{Lap}} - \\log Z_{\\text{num}}|$.\n\nThe implementation of these steps is performed in Python for each specified pair of $(\\varepsilon, \\tau)$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit, logsumexp\n\ndef solve():\n    \"\"\"\n    Computes Laplace approximation accuracy for a two-parameter Bayesian\n    logistic regression model under various conditions of near-nonidentifiability.\n    \"\"\"\n    test_cases = [\n        (0.0, 1.0),\n        (1e-3, 1.0),\n        (1e-3, 5.0),\n        (1e-1, 1.0),\n        (0.0, 5.0),\n    ]\n\n    results = []\n    for epsilon, tau in test_cases:\n        # Construct data and design matrix\n        n = 60\n        i_vals = np.arange(1, n + 1)\n        x = (i_vals - 0.5) / n\n        z = x + epsilon * np.cos(2 * np.pi * i_vals / n)\n        y = (x > 0.5).astype(float)\n        X = np.stack([x, z], axis=1)\n\n        # Define objective function and its derivatives\n        def f(theta, tau_val):\n            eta = X @ theta\n            log_lik = np.sum(y * eta - np.logaddexp(0, eta))\n            log_pri = -np.log(2 * np.pi * tau_val**2) - (theta @ theta) / (2 * tau_val**2)\n            return log_lik + log_pri\n\n        def grad_f(theta, tau_val):\n            eta = X @ theta\n            sigma = expit(eta)\n            grad_ll = X.T @ (y - sigma)\n            grad_lp = -theta / tau_val**2\n            return grad_ll + grad_lp\n\n        def hess_f(theta, tau_val):\n            eta = X @ theta\n            sigma = expit(eta)\n            weights = sigma * (1 - sigma)\n            hess_ll = -X.T @ (weights[:, np.newaxis] * X)\n            hess_lp = -np.eye(2) / tau_val**2\n            return hess_ll + hess_lp\n\n        # Find posterior mode using Newton's method\n        theta_hat = np.zeros(2)\n        max_iter = 50\n        grad_norm_tol = 1e-8\n        c1_bt = 0.25\n        rho_bt = 0.5\n\n        for _ in range(max_iter):\n            grad = grad_f(theta_hat, tau)\n            if np.linalg.norm(grad) < grad_norm_tol:\n                break\n            \n            hess = hess_f(theta_hat, tau)\n            delta = np.linalg.solve(hess, -grad)\n            \n            alpha = 1.0\n            f_current = f(theta_hat, tau)\n            grad_dot_delta = grad.T @ delta\n            while f(theta_hat + alpha * delta, tau) < f_current + c1_bt * alpha * grad_dot_delta:\n                alpha *= rho_bt\n                if alpha < 1e-9:\n                    break\n            \n            theta_hat = theta_hat + alpha * delta\n\n        # Compute smallest eigenvalue of negative log-likelihood Hessian\n        eta_hat = X @ theta_hat\n        sigma_hat = expit(eta_hat)\n        weights_hat = sigma_hat * (1 - sigma_hat)\n        neg_hess_ll = X.T @ (weights_hat[:, np.newaxis] * X)\n        lambda_min = np.linalg.eigvalsh(neg_hess_ll)[0]\n\n        # Compute Laplace approximation to log evidence\n        f_hat = f(theta_hat, tau)\n        neg_hess_total = -hess_f(theta_hat, tau)\n        _sign, logdet_neg_hess = np.linalg.slogdet(neg_hess_total)\n        log_Z_lap = f_hat + np.log(2 * np.pi) - 0.5 * logdet_neg_hess\n\n        # Compute numerical approximation to log evidence\n        L = 6 * tau\n        N_pts = 401\n        theta_axis = np.linspace(-L, L, N_pts)\n        d_theta = (2 * L) / (N_pts - 1)\n        \n        T1, T2 = np.meshgrid(theta_axis, theta_axis)\n        theta_grid = np.stack([T1.ravel(), T2.ravel()], axis=1)\n        \n        eta_grid = theta_grid @ X.T\n        log_lik_grid = np.sum(y * eta_grid - np.logaddexp(0, eta_grid), axis=1)\n        \n        theta_norm_sq_grid = np.sum(theta_grid**2, axis=1)\n        log_pri_grid = -np.log(2 * np.pi * tau**2) - theta_norm_sq_grid / (2 * tau**2)\n        f_grid = log_lik_grid + log_pri_grid\n        \n        log_w = np.full((N_pts, N_pts), np.log(d_theta**2))\n        log_w[0, :] += np.log(0.5)\n        log_w[-1, :] += np.log(0.5)\n        log_w[:, 0] += np.log(0.5)\n        log_w[:, -1] += np.log(0.5)\n        log_w_flat = log_w.ravel()\n        \n        log_Z_num = logsumexp(f_grid + log_w_flat)\n\n        # Compute absolute error\n        E = np.abs(log_Z_lap - log_Z_num)\n        \n        results.append([lambda_min, E])\n\n    # Format and print the final output\n    formatted_results = []\n    for res_pair in results:\n        formatted_pair = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        formatted_results.append(formatted_pair)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3137141"}]}