{"hands_on_practices": [{"introduction": "We begin by exploring the fundamental mechanics of the posterior distribution in Bayesian linear regression. This first exercise provides a simplified, yet powerful, scenario using a dataset with orthonormal features. By working through this special case [@problem_id:3103067], you will gain a clear, intuitive understanding of how the posterior mean and covariance are formed and see firsthand how feature design can lead to a \"decoupling\" of our beliefs about individual model parameters.", "problem": "Consider Bayesian linear regression (BLR), where a design matrix $X \\in \\mathbb{R}^{N \\times D}$ maps a weight vector $w \\in \\mathbb{R}^{D}$ to predicted targets via a linear model. The observed target vector is $t \\in \\mathbb{R}^{N}$. Assume a Gaussian likelihood with noise precision $\\beta > 0$ and an independent zero-mean Gaussian prior over coefficients with precision $\\alpha > 0$.\n\nConstruct a specific dataset with orthonormal columns as follows: let $N=3$ and $D=2$, and take\n$$\nX \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix},\n\\qquad\nt \\;=\\;\n\\begin{pmatrix}\n4 \\\\ -1 \\\\ 2\n\\end{pmatrix}.\n$$\nVerify that the columns of $X$ are orthonormal. Then, starting from the definitions of the Gaussian likelihood $p(t \\mid w, X, \\beta)$ and the Gaussian prior $p(w \\mid \\alpha)$, and applying Bayes' theorem $p(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta)\\,p(w \\mid \\alpha)$, derive the posterior distribution over $w$. In your derivation, complete the square to identify both the posterior mean and the posterior covariance matrix, and simplify the posterior covariance using the orthonormality of the columns of $X$.\n\nExplain, based on your derivation, how orthogonality of the columns of $X$ decouples the posterior distributions of the individual coefficients in $w$.\n\nFinally, with $\\alpha = 2$ and $\\beta = 3$, evaluate the exact posterior covariance matrix for this dataset. Report the posterior covariance matrix as your final answer. No rounding is required.", "solution": "The problem is valid as it is a standard exercise in Bayesian linear regression, is scientifically and mathematically sound, and provides all necessary information for a unique solution.\n\nFirst, we validate that the columns of the matrix $X$ are orthonormal. Let the columns be denoted by $x_1$ and $x_2$.\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\qquad x_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nOrthonormality requires that the inner product $x_i^T x_j = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. We check the inner products:\n$$\nx_1^T x_1 = (1)(1) + (0)(0) + (0)(0) = 1\n$$\n$$\nx_2^T x_2 = (0)(0) + (1)(1) + (0)(0) = 1\n$$\n$$\nx_1^T x_2 = (1)(0) + (0)(1) + (0)(0) = 0\n$$\nSince $x_1^T x_1 = 1$, $x_2^T x_2 = 1$, and $x_1^T x_2 = 0$, the columns are orthonormal. A more compact way to verify this is to compute $X^T X$:\n$$\nX^T X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2\n$$\nSince $X^T X = I_D$ where $D=2$ is the number of columns, the columns of $X$ are, by definition, orthonormal.\n\nNext, we derive the posterior distribution over the weights $w$. The model is specified by a Gaussian likelihood and a Gaussian prior.\nThe likelihood is $p(t \\mid w, X, \\beta) = \\mathcal{N}(t \\mid Xw, \\beta^{-1}I_N)$, where $I_N$ is the $N \\times N$ identity matrix. The probability density function is:\n$$\np(t \\mid w, X, \\beta) \\propto \\exp\\left( -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) \\right)\n$$\nThe prior on the weights is $p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, \\alpha^{-1}I_D)$, where $I_D$ is the $D \\times D$ identity matrix. Its probability density function is:\n$$\np(w \\mid \\alpha) \\propto \\exp\\left( -\\frac{\\alpha}{2} w^T w \\right)\n$$\nAccording to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta) \\, p(w \\mid \\alpha)\n$$\nWe work with the logarithm of the posterior, as this simplifies the product of exponentials into a sum of their arguments:\n$$\n\\ln p(w \\mid t, X, \\alpha, \\beta) = -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\nWe expand the quadratic term:\n$$\n(t - Xw)^T (t - Xw) = t^T t - t^T Xw - w^T X^T t + w^T X^T Xw = t^T t - 2w^T X^T t + w^T X^T Xw\n$$\nSubstituting this back into the log-posterior expression:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{\\beta}{2} (t^T t - 2w^T X^T t + w^T X^T Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\nWe group terms involving $w$:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t - \\frac{\\beta}{2} t^T t + \\text{const}\n$$\nDropping terms not dependent on $w$:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t + \\text{const'}\n$$\nThis is a quadratic form in $w$, which implies the posterior is a Gaussian distribution, $p(w \\mid t, \\dots) = \\mathcal{N}(w \\mid m_N, S_N)$. The log-density of a general multivariate Gaussian $\\mathcal{N}(w \\mid m_N, S_N)$ is:\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} (w - m_N)^T S_N^{-1} (w - m_N) + \\text{const} = -\\frac{1}{2} (w^T S_N^{-1} w - 2w^T S_N^{-1} m_N + m_N^T S_N^{-1} m_N) + \\text{const}\n$$\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} w^T S_N^{-1} w + w^T S_N^{-1} m_N + \\text{const''}\n$$\nBy comparing the coefficients of the quadratic and linear terms in $w$ between our expression for the log-posterior and the general log-Gaussian density, we can identify the posterior precision matrix (inverse covariance) $S_N^{-1}$ and the mean $m_N$.\nComparing the quadratic terms ($w^T(\\cdot)w$):\n$$\nS_N^{-1} = \\beta X^T X + \\alpha I_D\n$$\nThus, the posterior covariance matrix is:\n$$\nS_N = (\\beta X^T X + \\alpha I_D)^{-1}\n$$\nComparing the linear terms ($w^T(\\cdot)$):\n$$\nS_N^{-1} m_N = \\beta X^T t \\implies m_N = S_N (\\beta X^T t) = \\beta (\\beta X^T X + \\alpha I_D)^{-1} X^T t\n$$\nThese are the general expressions for the posterior mean and covariance in Bayesian linear regression.\n\nNow, we use the property that the columns of $X$ are orthonormal, which means $X^T X = I_D$. Substituting this into the expressions for $S_N$ and $m_N$:\nFor the posterior covariance $S_N$:\n$$\nS_N = (\\beta I_D + \\alpha I_D)^{-1} = ((\\beta + \\alpha) I_D)^{-1} = \\frac{1}{\\alpha + \\beta} I_D\n$$\nFor the posterior mean $m_N$:\n$$\nm_N = \\beta \\left(\\frac{1}{\\alpha + \\beta} I_D\\right) X^T t = \\frac{\\beta}{\\alpha + \\beta} X^T t\n$$\nThe orthonormality of the columns of $X$ significantly simplifies the posterior parameters. The posterior covariance matrix $S_N = \\frac{1}{\\alpha + \\beta} I_D$ is a diagonal matrix. In a multivariate Gaussian distribution, a diagonal covariance matrix signifies that the random variables are uncorrelated. For Gaussian variables, being uncorrelated is equivalent to being statistically independent.\nThe joint posterior density is:\n$$\np(w \\mid t, \\dots) \\propto \\exp\\left(-\\frac{1}{2} (w-m_N)^T S_N^{-1} (w-m_N)\\right) = \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w-m_N)^T (w-m_N)\\right)\n$$\n$$\n= \\exp\\left(-\\frac{\\alpha+\\beta}{2} \\sum_{j=1}^D (w_j - m_{N,j})^2\\right) = \\prod_{j=1}^D \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w_j - m_{N,j})^2\\right)\n$$\nThis shows that the joint posterior $p(w \\mid t, \\dots)$ factorizes into a product of individual posteriors for each coefficient $w_j$: $p(w_1, \\dots, w_D \\mid t, \\dots) = \\prod_{j=1}^D p(w_j \\mid t, \\dots)$. This factorization is the \"decoupling\" of the posterior distributions of the coefficients. Each $w_j$ has a univariate Gaussian posterior distribution with mean $m_{N,j}$ and variance $1/(\\alpha+\\beta)$. The orthonormality of the features ensures that learning about one weight $w_j$ provides no information about any other weight $w_k$ ($k \\neq j$) beyond what is already known from the prior.\n\nFinally, we evaluate the exact posterior covariance matrix for the given dataset with $\\alpha = 2$ and $\\beta = 3$. We use the simplified formula derived from the orthonormality of $X$'s columns. The dimensionality of the weight vector is $D=2$.\n$$\nS_N = \\frac{1}{\\alpha + \\beta} I_D = \\frac{1}{2 + 3} I_2 = \\frac{1}{5} I_2\n$$\nWriting this as a matrix:\n$$\nS_N = \\frac{1}{5} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix}}\n$$", "id": "3103067"}, {"introduction": "Having established the form of the posterior, we now turn to the practical challenge of computation, especially in high-dimensional settings where the number of features $p$ can be much larger than the number of data points $n$. This exercise [@problem_id:3103080] introduces the Woodbury matrix identity, a crucial linear algebra tool that allows us to transform a computationally prohibitive large matrix inversion into a much more manageable one. Mastering this technique is key to scaling Bayesian linear regression to modern, high-dimensional datasets.", "problem": "Consider Bayesian linear regression with a Gaussian likelihood and a zero-mean isotropic Gaussian prior. Specifically, let $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, and $w \\in \\mathbb{R}^{p}$, with\n- $y \\mid w \\sim \\mathcal{N}(Xw, \\sigma^{2} I_{n})$ for some noise variance $\\sigma^{2} \\gt 0$,\n- $w \\sim \\mathcal{N}(0, \\alpha^{-1} I_{p})$ for some precision $\\alpha \\gt 0$.\n\nBy Gaussian conjugacy, the posterior $p(w \\mid y)$ is Gaussian with a covariance matrix $S_{N} \\in \\mathbb{R}^{p \\times p}$. One standard form involves inverting a $p \\times p$ matrix. Instead of that direct form, identify an equivalent expression for $S_{N}$ that avoids directly inverting a $p \\times p$ matrix, and then interpret the computational implications in the regimes $p \\gg n$ and $n \\gg p$. Assume dense linear algebra with naive matrix inversion costs dominated by cubic time in the matrix dimension.\n\nWhich option is correct?\n\nA. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-1} X^{\\top}\\, \\bigl(\\sigma^{2} I_{n} + X \\alpha^{-1} X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$; this is advantageous when $p \\gg n$ because it replaces a $p \\times p$ inverse by an $n \\times n$ inverse (cost $\\mathcal{O}(n^{3})$ instead of $\\mathcal{O}(p^{3})$). When $n \\gg p$, the original $p \\times p$ form is preferable.\n\nB. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-1} X^{\\top}\\, \\bigl(\\sigma^{-2} I_{n} + X \\alpha^{-1} X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$; this is advantageous when $n \\gg p$ because it then replaces a $n \\times n$ inverse by a $p \\times p$ inverse, so the original form should be used when $p \\gg n$.\n\nC. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-2} X^{\\top}\\, \\bigl(I_{n} + (\\alpha \\sigma^{2})^{-1} X X^{\\top}\\bigr)^{-1}\\, X$; this eliminates matrix inversion altogether, so the time to form $S_{N}$ is $\\mathcal{O}(n p^{2})$ regardless of whether $p \\gg n$ or $n \\gg p$.\n\nD. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\sigma^{-2} X^{\\top}\\, \\bigl(I_{n} + (\\alpha \\sigma^{2})^{-1} X X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$; this expression is always numerically more stable than inverting a $p \\times p$ matrix and therefore should always be preferred regardless of $p$ and $n$.", "solution": "The problem statement is a standard formulation of Bayesian linear regression and is scientifically grounded, well-posed, and objective. It contains all necessary information for a unique solution.\n\nFirst, we will derive the standard expression for the posterior covariance matrix $S_N$. The posterior probability of the weights $w$ given the data $y$ is given by Bayes' theorem:\n$$p(w \\mid y) \\propto p(y \\mid w) p(w)$$\nThe likelihood $p(y \\mid w)$ is given as a Gaussian:\n$$p(y \\mid w) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw)\\right)$$\nThe prior $p(w)$ is also a Gaussian:\n$$p(w) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I_p) \\propto \\exp\\left(-\\frac{1}{2\\alpha^{-1}} w^\\top w\\right) = \\exp\\left(-\\frac{\\alpha}{2} w^\\top w\\right)$$\nThe posterior is therefore proportional to the product of these two exponential functions. The exponent of the posterior distribution, up to a constant, is:\n$$L(w) = -\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw) - \\frac{\\alpha}{2} w^\\top w$$\nTo find the form of the posterior, which is known to be Gaussian due to conjugacy, we analyze the terms in the exponent that are quadratic and linear in $w$:\n$$L(w) = -\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top Xw) - \\frac{\\alpha}{2} w^\\top w$$\n$$L(w) = \\frac{1}{\\sigma^2} y^\\top Xw - \\frac{1}{2} w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X\\right) w - \\frac{1}{2} w^\\top (\\alpha I_p) w + \\text{const.}$$\n$$L(w) = -\\frac{1}{2} w^\\top \\left(\\alpha I_p + \\frac{1}{\\sigma^2} X^\\top X\\right) w + \\left(\\frac{1}{\\sigma^2} X^\\top y\\right)^\\top w + \\text{const.}$$\nA general multivariate Gaussian distribution for $w$ has a log probability of the form $-\\frac{1}{2} (w-\\mu)^\\top \\Sigma^{-1} (w-\\mu) + \\text{const.}$, which expands to $-\\frac{1}{2} w^\\top \\Sigma^{-1} w + \\mu^\\top \\Sigma^{-1} w + \\text{const.}$.\nBy comparing the quadratic term, we can identify the inverse of the posterior covariance matrix $S_N$:\n$$S_N^{-1} = \\alpha I_p + \\frac{1}{\\sigma^2} X^\\top X$$\nSo, the posterior covariance is:\n$$S_N = \\left(\\alpha I_p + \\frac{1}{\\sigma^2} X^\\top X\\right)^{-1}$$\nThis is the standard form, which requires the inversion of a $p \\times p$ matrix, where $p$ is the number of features.\n\nTo find an alternative expression, we use the Woodbury matrix identity, which states:\n$$(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}$$\nWe can match the terms in our expression for $S_N^{-1}$ to the form $A+UCV$:\nLet $A = \\alpha I_p$, $U = X^\\top$, $C = \\frac{1}{\\sigma^2} I_n$, and $V = X$.\nThe corresponding inverse terms are:\n$A^{-1} = (\\alpha I_p)^{-1} = \\alpha^{-1} I_p$.\n$C^{-1} = (\\frac{1}{\\sigma^2} I_n)^{-1} = \\sigma^2 I_n$.\n\nSubstituting these into the Woodbury identity:\n$$S_N = (\\alpha I_p)^{-1} - (\\alpha I_p)^{-1} X^\\top \\left( (\\frac{1}{\\sigma^2} I_n)^{-1} + X (\\alpha I_p)^{-1} X^\\top \\right)^{-1} X (\\alpha I_p)^{-1}$$\n$$S_N = \\alpha^{-1} I_p - (\\alpha^{-1} I_p) X^\\top \\left( \\sigma^2 I_n + X (\\alpha^{-1} I_p) X^\\top \\right)^{-1} X (\\alpha^{-1} I_p)$$\n$$S_N = \\alpha^{-1} I_p - \\alpha^{-1} X^\\top \\left( \\sigma^2 I_n + \\alpha^{-1} X X^\\top \\right)^{-1} \\alpha^{-1} X$$\nSince $\\alpha^{-1}$ is a scalar, it can be moved freely. We can group the two $\\alpha^{-1}$ terms that multiply the central expression:\n$$S_N = \\alpha^{-1} I_p - \\alpha^{-2} X^\\top \\left( \\sigma^2 I_n + \\alpha^{-1} X X^\\top \\right)^{-1} X$$\nThis alternative form requires inverting an $n \\times n$ matrix, $(\\sigma^2 I_n + \\alpha^{-1} X X^\\top)$, where $n$ is the number of data points.\n\nNow we analyze the computational cost. Assume naive matrix multiplication of an $m \\times k$ and $k \\times l$ matrix is $\\mathcal{O}(mkl)$, and inversion of an $m \\times m$ matrix is $\\mathcal{O}(m^3)$.\n- The original form $S_N = (\\alpha I_p + \\sigma^{-2} X^\\top X)^{-1}$ requires computing $X^\\top X$ ($\\mathcal{O}(np^2)$) and inverting a $p \\times p$ matrix ($\\mathcal{O}(p^3)$). The cost is dominated by $\\mathcal{O}(np^2 + p^3)$.\n- The alternative form requires computing $XX^\\top$ ($\\mathcal{O}(pn^2)$) and inverting an $n \\times n$ matrix ($\\mathcal{O}(n^3)$). The cost is dominated by $\\mathcal{O}(pn^2 + n^3)$.\n\nLet's compare the costs in the two specified regimes:\n1.  **Regime $p \\gg n$ (many features, few samples):**\n    - Original cost is dominated by $\\mathcal{O}(p^3)$.\n    - Alternative cost is dominated by $\\mathcal{O}(n^3)$ (since $p > n$, $pn^2$ is smaller than $p^3$, and $n^3$ is much smaller than $p^3$).\n    - Since $p \\gg n$, we have $p^3 \\gg n^3$. Thus, the alternative form is computationally advantageous. It replaces a large $p \\times p$ inversion with a much smaller $n \\times n$ inversion.\n2.  **Regime $n \\gg p$ (few features, many samples):**\n    - Original cost is dominated by $\\mathcal{O}(np^2)$ to form $X^\\top X$ if $n$ is very large, but the inversion is $\\mathcal{O}(p^3)$.\n    - Alternative cost is dominated by $\\mathcal{O}(n^3)$.\n    - Since $n \\gg p$, we have $n^3 \\gg p^3$. In this case, the original form, which requires a $\\mathcal{O}(p^3)$ inversion, is far more efficient than the alternative form, which requires an $\\mathcal{O}(n^3)$ inversion.\n\nNow we evaluate each option.\n\nA. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-1} X^{\\top}\\, \\bigl(\\sigma^{2} I_{n} + X \\alpha^{-1} X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$; this is advantageous when $p \\gg n$ because it replaces a $p \\times p$ inverse by an $n \\times n$ inverse (cost $\\mathcal{O}(n^{3})$ instead of $\\mathcal{O}(p^{3})$). When $n \\gg p$, the original $p \\times p$ form is preferable.\n- The expression for $S_N$ is $S_N = \\alpha^{-1}I_p - \\alpha^{-1}X^\\top(\\sigma^2I_n + (\\alpha^{-1}X)X^\\top)^{-1}(\\alpha^{-1}X) = \\alpha^{-1}I_p - \\alpha^{-2}X^\\top(\\sigma^2I_n + \\alpha^{-1}XX^\\top)^{-1}X$. This matches our derived formula.\n- The computational analysis is also correct: the alternative form is better for $p \\gg n$ as it involves an $\\mathcal{O}(n^3)$ inversion instead of $\\mathcal{O}(p^3)$. For $n \\gg p$, the original form with its $\\mathcal{O}(p^3)$ inversion is preferable.\n- **Verdict:** Correct.\n\nB. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-1} X^{\\top}\\, \\bigl(\\sigma^{-2} I_{n} + X \\alpha^{-1} X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$; this is advantageous when $n \\gg p$ because it then replaces a $n \\times n$ inverse by a $p \\times p$ inverse, so the original form should be used when $p \\gg n$.\n- The formula is incorrect; it contains $\\sigma^{-2}I_n$ inside the inverse instead of the correct $\\sigma^2I_n$.\n- The computational reasoning is backwards. The alternative form is advantageous when $p \\gg n$, not $n \\gg p$. It replaces a $p \\times p$ inverse with an $n \\times n$ inverse, not the other way around.\n- **Verdict:** Incorrect.\n\nC. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-2} X^{\\top}\\, \\bigl(I_{n} + (\\alpha \\sigma^{2})^{-1} X X^{\\top}\\bigr)^{-1}\\, X$; this eliminates matrix inversion altogether, so the time to form $S_{N}$ is $\\mathcal{O}(n p^{2})$ regardless of whether $p \\gg n$ or $n \\gg p$.\n- The formula is incorrect. Factoring $\\sigma^2$ from our derived expression gives $S_N = \\alpha^{-1} I_p - \\alpha^{-2}\\sigma^{-2} X^\\top (I_n + (\\alpha \\sigma^2)^{-1} XX^\\top)^{-1} X$. This option is missing the $\\sigma^{-2}$ factor.\n- The claim that this \"eliminates matrix inversion altogether\" is false. The term $(\\dots)^{-1}$ clearly denotes an $n \\times n$ matrix inversion. The cost analysis is also incorrect.\n- **Verdict:** Incorrect.\n\nD. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\sigma^{-2} X^{\\top}\\, \\bigl(I_{n} + (\\alpha \\sigma^{2})^{-1} X X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$; this expression is always numerically more stable than inverting a $p \\times p$ matrix and therefore should always be preferred regardless of $p$ and $n$.\n- The formula, which can be written as $S_N = \\alpha^{-1} I_p - \\alpha^{-1}\\sigma^{-2} X^\\top (\\dots)^{-1} X$, is incorrect. As shown in the analysis for C, it should be $S_N = \\alpha^{-1} I_p - \\alpha^{-2}\\sigma^{-2} X^\\top (\\dots)^{-1} X$. It is missing a factor of $\\alpha^{-1}$.\n- The claim that it \"should always be preferred regardless of $p$ and $n$\" is false, as demonstrated by our computational cost analysis ($n \\gg p$ makes the original form better). The claim about numerical stability is unsubstantiated and not generally true.\n- **Verdict:** Incorrect.", "answer": "$$\\boxed{A}$$", "id": "3103080"}, {"introduction": "Our final practice moves beyond the assumption of a single, fixed model structure to address the more realistic scenario of model uncertainty. This comprehensive exercise [@problem_id:3103054] guides you through the implementation of Bayesian Model Averaging (BMA), a cornerstone of Bayesian methodology. You will learn to compute model evidence to score different feature subsets and then average their predictions, providing a more robust and honest quantification of predictive uncertainty than any single model could offer.", "problem": "You are asked to implement Bayesian Model Averaging (BMA) for Bayesian Linear Regression (BLR) across specified subsets of predictors and to compare the averaged predictions and uncertainties to those obtained from a single selected model. Work in a fully probabilistic framework where the regression weights have a zero-mean isotropic Gaussian prior and the observation noise variance is known and fixed. The derivation must begin from Bayes’ rule for both parameter and model spaces, and from properties of the multivariate Gaussian distribution. Do not assume any shortcut formulas; derive the closed-form marginal likelihood and predictive distribution starting from these base principles.\n\nAssume a standard BLR setup with design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$, a known noise variance $\\sigma^{2} \\in \\mathbb{R}_{+}$, and a prior over weights $\\mathbf{w} \\in \\mathbb{R}^{p}$ given by $\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\alpha^{-1} \\mathbf{I}_{p})$ with precision $\\alpha \\in \\mathbb{R}_{+}$. For any subset of predictors (a model) $\\mathcal{M} \\subseteq \\{1,\\dots,p\\}$, let $\\mathbf{X}_{\\mathcal{M}}$ denote the submatrix with the columns in $\\mathcal{M}$, and let $\\mathbf{x}_{\\star,\\mathcal{M}}$ denote the corresponding subvector of a query feature vector $\\mathbf{x}_{\\star} \\in \\mathbb{R}^{p}$. Assume a uniform prior over all listed models $\\mathcal{M}$ in the test suite.\n\nYour program must:\n- For each test case and for each listed model $\\mathcal{M}$, derive and evaluate the model evidence $p(\\mathbf{y} \\mid \\mathbf{X}_{\\mathcal{M}}, \\sigma^{2}, \\alpha)$ in closed form by integrating out $\\mathbf{w}$.\n- Use Bayes’ rule to compute the posterior model probabilities $p(\\mathcal{M} \\mid \\mathbf{y}, \\mathbf{X})$ from the evidences under a uniform prior over the listed models.\n- For each model, derive and evaluate in closed form the posterior predictive distribution at $\\mathbf{x}_{\\star,\\mathcal{M}}$, and extract its predictive mean and predictive variance.\n- Compute the BMA predictive mean and variance at $\\mathbf{x}_{\\star}$ using the law of total expectation and the law of total variance across the model posterior distribution.\n- Identify the single model with the largest evidence (maximum a posteriori under the uniform model prior) and report its predictive mean and variance at $\\mathbf{x}_{\\star}$.\n- Aggregate, for each test case, four quantities: the BMA predictive mean, the BMA predictive variance, the single best model’s predictive mean, and the single best model’s predictive variance.\n\nNumerical and implementation requirements:\n- All derivations must start from Bayes’ rule for parameters and models and from properties of multivariate Gaussian integrals. No heuristic approximations are allowed.\n- All matrix computations must be numerically stable. Use Cholesky factorizations for positive-definite matrices when evaluating determinants and quadratic forms.\n- There is no intercept unless explicitly included as a predictor; all priors are zero-mean.\n- Use the provided test suite exactly as specified.\n- There are no physical units, no angles, and no percentage outputs in this task.\n\nTest suite:\n- Test case $1$:\n  - $n = 6$, $p = 3$.\n  - $\\mathbf{X}^{(1)} = \\begin{bmatrix}\n  1.0 & 0.0 & 2.0\\\\\n  0.5 & -1.0 & 1.0\\\\\n  1.5 & 2.0 & -0.5\\\\\n  0.0 & 1.0 & 1.0\\\\\n  2.0 & -0.5 & 0.5\\\\\n  1.0 & 1.0 & 0.0\n  \\end{bmatrix}$,\n  $\\mathbf{y}^{(1)} = \\begin{bmatrix} 1.9 \\\\ 1.0 \\\\ 0.4 \\\\ 0.3 \\\\ 1.9 \\\\ 0.5 \\end{bmatrix}$,\n  $\\alpha^{(1)} = 2.0$,\n  $\\sigma^{2\\,(1)} = 0.25$,\n  $\\mathbf{x}^{(1)}_{\\star} = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n  model list $\\mathcal{S}^{(1)} = \\{\\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\}$.\n- Test case $2$:\n  - $n = 5$, $p = 2$.\n  - $\\mathbf{X}^{(2)} = \\begin{bmatrix}\n  1.0 & 0.9\\\\\n  0.8 & 0.7\\\\\n  1.2 & 1.1\\\\\n  0.0 & 0.1\\\\\n  0.5 & 0.4\n  \\end{bmatrix}$,\n  $\\mathbf{y}^{(2)} = \\begin{bmatrix} 0.41 \\\\ 0.35 \\\\ 0.46 \\\\ -0.01 \\\\ 0.22 \\end{bmatrix}$,\n  $\\alpha^{(2)} = 1000.0$,\n  $\\sigma^{2\\,(2)} = 0.5$,\n  $\\mathbf{x}^{(2)}_{\\star} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$,\n  model list $\\mathcal{S}^{(2)} = \\{\\{1\\}, \\{2\\}, \\{1,2\\}\\}$.\n- Test case $3$:\n  - $n = 8$, $p = 3$.\n  - $\\mathbf{X}^{(3)} = \\begin{bmatrix}\n  1.0 & 2.0 & -1.0\\\\\n  0.0 & -1.0 & 2.0\\\\\n  2.0 & 0.5 & 1.0\\\\\n  1.5 & -0.5 & 0.0\\\\\n  -1.0 & 1.0 & 1.0\\\\\n  0.5 & 0.5 & 0.5\\\\\n  1.0 & -1.0 & -1.0\\\\\n  2.0 & 2.0 & 2.0\n  \\end{bmatrix}$,\n  $\\mathbf{y}^{(3)} = \\begin{bmatrix} 0.6 \\\\ -0.3 \\\\ 0.4 \\\\ 0.4 \\\\ -0.4 \\\\ 0.05 \\\\ 0.3 \\\\ 0.4 \\end{bmatrix}$,\n  $\\alpha^{(3)} = 1.0$,\n  $\\sigma^{2\\,(3)} = 4.0$,\n  $\\mathbf{x}^{(3)}_{\\star} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$,\n  model list $\\mathcal{S}^{(3)} = \\{\\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\}$.\n\nFinal output format:\n- For each test case, return a list with four floating-point numbers: the BMA predictive mean at $\\mathbf{x}_{\\star}$, the BMA predictive variance at $\\mathbf{x}_{\\star}$, the single best model’s predictive mean at $\\mathbf{x}_{\\star}$, and the single best model’s predictive variance at $\\mathbf{x}_{\\star}$, in that order.\n- Each floating-point number must be rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of lists, for example, $[\\,[\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot]\\,]$ with no extra text.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and objective problem in statistical learning. It requires the derivation and implementation of Bayesian Model Averaging for Bayesian Linear Regression from first principles, which is a standard and verifiable procedure. All necessary data and parameters are provided.\n\n### Theoretical Derivation\n\nThe problem requires a full derivation of the quantities involved in Bayesian Model Averaging (BMA) for a linear regression model. We start from the fundamental principles of Bayesian inference.\n\nLet a model $\\mathcal{M}$ be defined by a specific subset of $d_{\\mathcal{M}}$ predictors from the total of $p$ available predictors. For this model, the design matrix is $\\mathbf{X}_{\\mathcal{M}} \\in \\mathbb{R}^{n \\times d_{\\mathcal{M}}}$, the weight vector is $\\mathbf{w}_{\\mathcal{M}} \\in \\mathbb{R}^{d_{\\mathcal{M}}}$, and the response vector is $\\mathbf{y} \\in \\mathbb{R}^{n}$.\n\nThe probabilistic model is defined by the likelihood and the prior:\n- **Likelihood**: The observations $\\mathbf{y}$ are assumed to be generated from a linear model with additive Gaussian noise. The noise variance $\\sigma^2$ is known. Let $\\beta = 1/\\sigma^2$ be the noise precision.\n$$p(\\mathbf{y} \\mid \\mathbf{X}_{\\mathcal{M}}, \\mathbf{w}_{\\mathcal{M}}, \\beta) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}}, \\beta^{-1}\\mathbf{I}_n)$$\n- **Prior**: The weights $\\mathbf{w}_{\\mathcal{M}}$ are given a zero-mean isotropic Gaussian prior with known precision $\\alpha$.\n$$p(\\mathbf{w}_{\\mathcal{M}} \\mid \\alpha) = \\mathcal{N}(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathbf{0}, \\alpha^{-1}\\mathbf{I}_{d_{\\mathcal{M}}})$$\n\nLet $\\mathcal{D} = \\{\\mathbf{X}, \\mathbf{y}\\}$ denote the data. For a specific model $\\mathcal{M}$, we use data $\\mathcal{D}_{\\mathcal{M}} = \\{\\mathbf{X}_{\\mathcal{M}}, \\mathbf{y}\\}$.\n\n**1. Posterior Distribution over Weights**\n\nThe posterior distribution of the weights $\\mathbf{w}_{\\mathcal{M}}$ for a given model $\\mathcal{M}$ is obtained via Bayes' rule:\n$$p(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathcal{D}_{\\mathcal{M}}, \\alpha, \\beta) \\propto p(\\mathbf{y} \\mid \\mathbf{X}_{\\mathcal{M}}, \\mathbf{w}_{\\mathcal{M}}, \\beta) p(\\mathbf{w}_{\\mathcal{M}} \\mid \\alpha)$$\nThe log of the posterior is proportional to the sum of the log-likelihood and log-prior:\n$$\\ln p(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathcal{D}_{\\mathcal{M}}) = -\\frac{\\beta}{2}(\\mathbf{y} - \\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}})^T(\\mathbf{y} - \\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}}) - \\frac{\\alpha}{2}\\mathbf{w}_{\\mathcal{M}}^T\\mathbf{w}_{\\mathcal{M}} + \\text{const.}$$\nExpanding the quadratic terms in $\\mathbf{w}_{\\mathcal{M}}$:\n$$\\ln p(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathcal{D}_{\\mathcal{M}}) = -\\frac{1}{2}\\left( \\beta\\mathbf{w}_{\\mathcal{M}}^T\\mathbf{X}_{\\mathcal{M}}^T\\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}} - 2\\beta\\mathbf{y}^T\\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}} + \\alpha\\mathbf{w}_{\\mathcal{M}}^T\\mathbf{w}_{\\mathcal{M}} \\right) + \\text{const.}$$\n$$= -\\frac{1}{2}\\left( \\mathbf{w}_{\\mathcal{M}}^T(\\beta\\mathbf{X}_{\\mathcal{M}}^T\\mathbf{X}_{\\mathcal{M}} + \\alpha\\mathbf{I}_{d_{\\mathcal{M}}})\\mathbf{w}_{\\mathcal{M}} - 2\\beta\\mathbf{y}^T\\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}} \\right) + \\text{const.}$$\nThis is a quadratic form in $\\mathbf{w}_{\\mathcal{M}}$, implying the posterior is Gaussian. By completing the square, we identify the posterior precision matrix $\\mathbf{A}_{\\mathcal{M}}$ and mean vector $\\mathbf{m}_{\\mathcal{M}}$. The exponent of a multivariate Gaussian $\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\mathbf{\\Sigma})$ is $-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) = -\\frac{1}{2}(\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}^T\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu})$.\nComparing terms, the posterior precision is:\n$$\\mathbf{A}_{\\mathcal{M}} = \\beta\\mathbf{X}_{\\mathcal{M}}^T\\mathbf{X}_{\\mathcal{M}} + \\alpha\\mathbf{I}_{d_{\\mathcal{M}}}$$\nAnd the posterior mean satisfies $\\mathbf{A}_{\\mathcal{M}}\\mathbf{m}_{\\mathcal{M}} = \\beta\\mathbf{X}_{\\mathcal{M}}^T\\mathbf{y}$, so:\n$$\\mathbf{m}_{\\mathcal{M}} = \\beta\\mathbf{A}_{\\mathcal{M}}^{-1}\\mathbf{X}_{\\mathcal{M}}^T\\mathbf{y}$$\nThe posterior covariance is $\\mathbf{S}_{\\mathcal{M}} = \\mathbf{A}_{\\mathcal{M}}^{-1}$. The posterior is $p(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathcal{D}_{\\mathcal{M}}) = \\mathcal{N}(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathbf{m}_{\\mathcal{M}}, \\mathbf{S}_{\\mathcal{M}})$.\n\n**2. Model Evidence (Marginal Likelihood)**\n\nThe model evidence for model $\\mathcal{M}$ is the probability of the data given the model, with the parameters integrated out:\n$$p(\\mathbf{y} \\mid \\mathcal{D}_{\\mathcal{M}}, \\alpha, \\beta) = \\int p(\\mathbf{y} \\mid \\mathbf{X}_{\\mathcal{M}}, \\mathbf{w}_{\\mathcal{M}}, \\beta) p(\\mathbf{w}_{\\mathcal{M}} \\mid \\alpha) d\\mathbf{w}_{\\mathcal{M}}$$\nThis integral is the normalization constant of the posterior. The log of the joint distribution $p(\\mathbf{y}, \\mathbf{w}_{\\mathcal{M}})$ is:\n$$\\ln p(\\mathbf{y}, \\mathbf{w}_{\\mathcal{M}}) = \\ln p(\\mathbf{y} \\mid \\mathbf{w}_{\\mathcal{M}}) + \\ln p(\\mathbf{w}_{\\mathcal{M}})$$\n$$= \\left( \\frac{n}{2}\\ln\\beta - \\frac{n}{2}\\ln(2\\pi) -\\frac{\\beta}{2}||\\mathbf{y}-\\mathbf{X}_{\\mathcal{M}}\\mathbf{w}_{\\mathcal{M}}||^2 \\right) + \\left( \\frac{d_{\\mathcal{M}}}{2}\\ln\\alpha - \\frac{d_{\\mathcal{M}}}{2}\\ln(2\\pi) - \\frac{\\alpha}{2}||\\mathbf{w}_{\\mathcal{M}}||^2 \\right)$$\nCompleting the square for $\\mathbf{w}_{\\mathcal{M}}$ as before yields a term $-\\frac{1}{2}(\\mathbf{w}_{\\mathcal{M}}-\\mathbf{m}_{\\mathcal{M}})^T\\mathbf{A}_{\\mathcal{M}}(\\mathbf{w}_{\\mathcal{M}}-\\mathbf{m}_{\\mathcal{M}})$ and other terms that do not depend on $\\mathbf{w}_{\\mathcal{M}}$. The integral over $\\mathbf{w}_{\\mathcal{M}}$ of the Gaussian part is $(2\\pi)^{d_{\\mathcal{M}}/2}|\\mathbf{A}_{\\mathcal{M}}|^{-1/2}$. The remaining terms constitute the log model evidence:\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) = \\frac{d_{\\mathcal{M}}}{2}\\ln\\alpha + \\frac{n}{2}\\ln\\beta - E(\\mathbf{m}_{\\mathcal{M}}) - \\frac{1}{2}\\ln|\\mathbf{A}_{\\mathcal{M}}| - \\frac{n}{2}\\ln(2\\pi)$$\nwhere $E(\\mathbf{m}_{\\mathcal{M}})$ is the negative log-joint evaluated at the posterior mean $\\mathbf{m}_{\\mathcal{M}}$, but with the terms in $\\mathbf{w}_{\\mathcal{M}}$ removed.\n$$E(\\mathbf{m}_{\\mathcal{M}}) = \\frac{\\beta}{2}||\\mathbf{y} - \\mathbf{X}_{\\mathcal{M}}\\mathbf{m}_{\\mathcal{M}}||^2 + \\frac{\\alpha}{2}\\mathbf{m}_{\\mathcal{M}}^T\\mathbf{m}_{\\mathcal{M}}$$\nThis closed-form expression for the log-evidence will be computed for each model.\n\n**3. Posterior Model Probabilities**\n\nGiven a set of candidate models $\\mathcal{S}$, and a uniform prior over these models, $p(\\mathcal{M}) = 1/|\\mathcal{S}|$, the posterior probability of a model $\\mathcal{M}$ is given by Bayes' rule for models:\n$$p(\\mathcal{M} \\mid \\mathcal{D}) = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}) p(\\mathcal{M})}{\\sum_{\\mathcal{M}' \\in \\mathcal{S}} p(\\mathbf{y} \\mid \\mathcal{M}') p(\\mathcal{M}')} = \\frac{p(\\mathbf{y} \\mid \\mathcal{M})}{\\sum_{\\mathcal{M}' \\in \\mathcal{S}} p(\\mathbf{y} \\mid \\mathcal{M}')}$$\nTo compute this stably, we use the log-evidences. Let $l_{\\mathcal{M}} = \\ln p(\\mathbf{y} \\mid \\mathcal{M})$ and $l_{\\text{max}} = \\max_{\\mathcal{M}' \\in \\mathcal{S}} l_{\\mathcal{M}'}$. Then:\n$$p(\\mathcal{M} \\mid \\mathcal{D}) = \\frac{\\exp(l_{\\mathcal{M}} - l_{\\text{max}})}{\\sum_{\\mathcal{M}' \\in \\mathcal{S}} \\exp(l_{\\mathcal{M}'} - l_{\\text{max}})}$$\n\n**4. Posterior Predictive Distribution**\n\nFor a new data point $\\mathbf{x}_{\\star}$, the predictive distribution of its output $y_{\\star}$ under a single model $\\mathcal{M}$ is:\n$$p(y_{\\star} \\mid \\mathbf{x}_{\\star,\\mathcal{M}}, \\mathcal{D}_{\\mathcal{M}}) = \\int p(y_{\\star} \\mid \\mathbf{x}_{\\star,\\mathcal{M}}, \\mathbf{w}_{\\mathcal{M}}, \\beta) p(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathcal{D}_{\\mathcal{M}}) d\\mathbf{w}_{\\mathcal{M}}$$\nHere, $p(y_{\\star} \\mid \\mathbf{x}_{\\star,\\mathcal{M}}, \\mathbf{w}_{\\mathcal{M}}, \\beta) = \\mathcal{N}(y_{\\star} \\mid \\mathbf{x}_{\\star,\\mathcal{M}}^T \\mathbf{w}_{\\mathcal{M}}, \\beta^{-1})$ and $p(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathcal{D}_{\\mathcal{M}}) = \\mathcal{N}(\\mathbf{w}_{\\mathcal{M}} \\mid \\mathbf{m}_{\\mathcal{M}}, \\mathbf{S}_{\\mathcal{M}})$. This is an integral over the product of two Gaussians (a convolution), which results in a Gaussian.\nThe mean of the predictive distribution is:\n$$\\mu_{\\star,\\mathcal{M}} = \\mathbb{E}[y_{\\star}] = \\mathbb{E}[\\mathbf{x}_{\\star,\\mathcal{M}}^T\\mathbf{w}_{\\mathcal{M}}] = \\mathbf{x}_{\\star,\\mathcal{M}}^T\\mathbb{E}[\\mathbf{w}_{\\mathcal{M}}] = \\mathbf{x}_{\\star,\\mathcal{M}}^T\\mathbf{m}_{\\mathcal{M}}$$\nThe variance of the predictive distribution is:\n$$\\sigma^2_{\\star,\\mathcal{M}} = \\text{Var}[y_{\\star}] = \\mathbb{E}[\\text{Var}[y_{\\star} \\mid \\mathbf{w}_{\\mathcal{M}}]] + \\text{Var}[\\mathbb{E}[y_{\\star} \\mid \\mathbf{w}_{\\mathcal{M}}]] = \\beta^{-1} + \\text{Var}[\\mathbf{x}_{\\star,\\mathcal{M}}^T\\mathbf{w}_{\\mathcal{M}}]$$\n$$\\sigma^2_{\\star,\\mathcal{M}} = \\sigma^2 + \\mathbf{x}_{\\star,\\mathcal{M}}^T \\text{Var}[\\mathbf{w}_{\\mathcal{M}}] \\mathbf{x}_{\\star,\\mathcal{M}} = \\sigma^2 + \\mathbf{x}_{\\star,\\mathcal{M}}^T \\mathbf{S}_{\\mathcal{M}} \\mathbf{x}_{\\star,\\mathcal{M}}$$\nSo, $p(y_{\\star} \\mid \\mathbf{x}_{\\star,\\mathcal{M}}, \\mathcal{D}_{\\mathcal{M}}) = \\mathcal{N}(y_{\\star} \\mid \\mu_{\\star,\\mathcal{M}}, \\sigma^2_{\\star,\\mathcal{M}})$.\n\n**5. Bayesian Model Averaging (BMA)**\n\nThe BMA predictive distribution is a mixture of the individual model predictive distributions, weighted by their posterior probabilities:\n$$p(y_{\\star} \\mid \\mathbf{x}_{\\star}, \\mathcal{D}) = \\sum_{\\mathcal{M} \\in \\mathcal{S}} p(y_{\\star} \\mid \\mathbf{x}_{\\star, \\mathcal{M}}, \\mathcal{D}_{\\mathcal{M}}) p(\\mathcal{M} \\mid \\mathcal{D})$$\nThe BMA predictive mean and variance are computed using the laws of total expectation and total variance.\n- **BMA Predictive Mean**:\n$$\\mu_{\\text{BMA}} = \\mathbb{E}[y_{\\star} \\mid \\mathcal{D}] = \\sum_{\\mathcal{M} \\in \\mathcal{S}} \\mathbb{E}[y_{\\star} \\mid \\mathcal{D}_{\\mathcal{M}}] p(\\mathcal{M} \\mid \\mathcal{D}) = \\sum_{\\mathcal{M} \\in \\mathcal{S}} \\mu_{\\star,\\mathcal{M}} p(\\mathcal{M} \\mid \\mathcal{D})$$\n- **BMA Predictive Variance**:\n$$\\sigma^2_{\\text{BMA}} = \\text{Var}[y_{\\star} \\mid \\mathcal{D}] = \\mathbb{E}_{\\mathcal{M}}[\\text{Var}[y_{\\star} \\mid \\mathcal{D}_{\\mathcal{M}}]] + \\text{Var}_{\\mathcal{M}}[\\mathbb{E}[y_{\\star} \\mid \\mathcal{D}_{\\mathcal{M}}]] $$\n$$= \\sum_{\\mathcal{M}\\in\\mathcal{S}} \\sigma^2_{\\star,\\mathcal{M}} p(\\mathcal{M}\\mid\\mathcal{D}) + \\left( \\sum_{\\mathcal{M}\\in\\mathcal{S}} \\mu_{\\star,\\mathcal{M}}^2 p(\\mathcal{M}\\mid\\mathcal{D}) - \\mu_{\\text{BMA}}^2 \\right)$$\n\n**6. Single Best Model**\n\nThe single best model, $\\mathcal{M}^*$, is the one that maximizes the posterior probability $p(\\mathcal{M} \\mid \\mathcal{D})$. Given the uniform prior over models, this is equivalent to maximizing the model evidence $p(\\mathbf{y} \\mid \\mathcal{M})$.\n$$\\mathcal{M}^* = \\arg\\max_{\\mathcal{M} \\in \\mathcal{S}} p(\\mathcal{M} \\mid \\mathcal{D}) = \\arg\\max_{\\mathcal{M} \\in \\mathcal{S}} \\ln p(\\mathbf{y} \\mid \\mathcal{M})$$\nThe predictive mean and variance for this model are $\\mu_{\\star,\\mathcal{M}^*}$ and $\\sigma^2_{\\star,\\mathcal{M}^*}$, respectively.\n\nFor numerical stability, matrix inverses and determinants are computed using Cholesky factorization. Specifically, for a positive definite matrix $\\mathbf{A}_{\\mathcal{M}} = \\mathbf{L}_{\\mathcal{M}}\\mathbf{L}_{\\mathcal{M}}^T$, $\\ln|\\mathbf{A}_{\\mathcal{M}}| = 2\\sum_i\\ln(\\text{diag}(\\mathbf{L}_{\\mathcal{M}})_i)$, and systems like $\\mathbf{A}_{\\mathcal{M}}\\mathbf{x}=\\mathbf{b}$ are solved via two triangular solves. The quadratic form $\\mathbf{v}^T\\mathbf{A}_{\\mathcal{M}}^{-1}\\mathbf{v}$ is calculated as $||\\mathbf{L}_{\\mathcal{M}}^{-1}\\mathbf{v}||^2$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run Bayesian Model Averaging on the test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 2.0], [0.5, -1.0, 1.0], [1.5, 2.0, -0.5],\n                [0.0, 1.0, 1.0], [2.0, -0.5, 0.5], [1.0, 1.0, 0.0]\n            ]),\n            \"y\": np.array([1.9, 1.0, 0.4, 0.3, 1.9, 0.5]),\n            \"alpha\": 2.0,\n            \"sigma2\": 0.25,\n            \"x_star\": np.array([1.0, 0.5, -0.5]),\n            \"models\": [{1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}]\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.9], [0.8, 0.7], [1.2, 1.1],\n                [0.0, 0.1], [0.5, 0.4]\n            ]),\n            \"y\": np.array([0.41, 0.35, 0.46, -0.01, 0.22]),\n            \"alpha\": 1000.0,\n            \"sigma2\": 0.5,\n            \"x_star\": np.array([1.0, 1.0]),\n            \"models\": [{1}, {2}, {1, 2}]\n        },\n        {\n            \"X\": np.array([\n                [1.0, 2.0, -1.0], [0.0, -1.0, 2.0], [2.0, 0.5, 1.0],\n                [1.5, -0.5, 0.0], [-1.0, 1.0, 1.0], [0.5, 0.5, 0.5],\n                [1.0, -1.0, -1.0], [2.0, 2.0, 2.0]\n            ]),\n            \"y\": np.array([0.6, -0.3, 0.4, 0.4, -0.4, 0.05, 0.3, 0.4]),\n            \"alpha\": 1.0,\n            \"sigma2\": 4.0,\n            \"x_star\": np.array([1.0, 1.0, 1.0]),\n            \"models\": [{1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['X'], case['y'], case['alpha'], case['sigma2'], case['x_star'], case['models'])\n        results.append(result)\n\n    final_results_str = []\n    for res_list in results:\n        formatted_list_str = '[' + ','.join(f'{x:.6f}' for x in res_list) + ']'\n        final_results_str.append(formatted_list_str)\n    \n    print(f\"[{','.join(final_results_str)}]\")\n\ndef process_case(X, y, alpha, sigma2, x_star, models):\n    \"\"\"\n    Processes a single test case for BMA.\n    \"\"\"\n    n, p = X.shape\n    beta = 1.0 / sigma2\n\n    model_log_evidences = []\n    model_pred_means = []\n    model_pred_variances = []\n\n    for model_indices_1based in models:\n        # Convert 1-based indices to 0-based\n        cols = sorted([i - 1 for i in model_indices_1based])\n        d_m = len(cols)\n        \n        if d_m == 0: continue\n\n        X_m = X[:, cols]\n        x_star_m = x_star[cols]\n\n        # Posterior calculation\n        A_m = beta * (X_m.T @ X_m) + alpha * np.identity(d_m)\n        \n        try:\n            L_m = np.linalg.cholesky(A_m)\n        except np.linalg.LinAlgError:\n            # Handle cases where A_m might not be perfectly positive definite due to precision\n            # A small regularization can help.\n            A_m += 1e-12 * np.identity(d_m)\n            L_m = np.linalg.cholesky(A_m)\n\n\n        # Solve m_m = beta * A_m^-1 @ X_m.T @ y using Cholesky\n        m_m_p1 = beta * (X_m.T @ y)\n        tmp = np.linalg.solve(L_m, m_m_p1)\n        m_m = np.linalg.solve(L_m.T, tmp)\n\n        # Log model evidence\n        log_det_A_m = 2 * np.sum(np.log(np.diag(L_m)))\n        \n        residual = y - X_m @ m_m\n        E_mN = (beta / 2.0) * (residual.T @ residual) + (alpha / 2.0) * (m_m.T @ m_m)\n\n        log_evidence = (d_m / 2.0) * np.log(alpha) + (n / 2.0) * np.log(beta) \\\n                       - E_mN - 0.5 * log_det_A_m - (n / 2.0) * np.log(2 * np.pi)\n        \n        model_log_evidences.append(log_evidence)\n\n        # Posterior predictive distribution\n        pred_mean_m = x_star_m.T @ m_m\n\n        # Calculate x_star.T @ S_m @ x_star using Cholesky\n        # v = L_m^-1 @ x_star_m\n        tmp_v = np.linalg.solve(L_m, x_star_m)\n        pred_var_m_weights_term = tmp_v.T @ tmp_v\n        pred_var_m = sigma2 + pred_var_m_weights_term\n\n        model_pred_means.append(pred_mean_m)\n        model_pred_variances.append(pred_var_m)\n\n    # Model posterior probabilities using log-sum-exp trick\n    log_evidences_arr = np.array(model_log_evidences)\n    max_log_evidence = np.max(log_evidences_arr)\n    unnormalized_probs = np.exp(log_evidences_arr - max_log_evidence)\n    post_probs = unnormalized_probs / np.sum(unnormalized_probs)\n\n    # BMA predictions\n    means_arr = np.array(model_pred_means)\n    variances_arr = np.array(model_pred_variances)\n\n    bma_mean = np.sum(means_arr * post_probs)\n    bma_var = np.sum(variances_arr * post_probs) + np.sum(means_arr**2 * post_probs) - bma_mean**2\n\n    # Single best model (Maximum A Posteriori model)\n    best_model_idx = np.argmax(log_evidences_arr)\n    best_model_mean = model_pred_means[best_model_idx]\n    best_model_var = model_pred_variances[best_model_idx]\n\n    return [bma_mean, bma_var, best_model_mean, best_model_var]\n\nsolve()\n```", "id": "3103054"}]}