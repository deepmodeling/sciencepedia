{"hands_on_practices": [{"introduction": "Isomap's power lies in its ability to approximate a manifold's intrinsic geodesic distances using a graph built on sampled data, but how accurate is this approximation? This practice challenges you to explore the fundamental relationship between a manifold's curvature, the sampling density, and the resulting approximation error [@problem_id:3133711]. By deriving and experimentally verifying the required sampling density from first principles, you will gain a quantitative understanding of how Isomap's performance is intrinsically linked to the geometry of the data.", "problem": "Consider a one-dimensional curve embedded in three-dimensional Euclidean space, denoted by a continuous, differentiable mapping $\\gamma: [a,b] \\to \\mathbb{R}^3$. The intrinsic metric on the curve is the geodesic distance equal to the arc length along $\\gamma$. Isometric Feature Mapping (Isomap) approximates geodesic distances by shortest path distances on a neighborhood graph constructed on sampled points of the manifold. For a one-dimensional curve with sufficiently fine sampling and a neighborhood that connects only consecutive samples along the curve, the Isomap path length between two sampled points is the sum of Euclidean chord lengths between consecutive samples along the curve segment, yielding a polygonal approximation to the true arc length.\n\nYour task is to design and test an experiment that varies curvature magnitude and quantifies how sampling density must scale to guarantee that Isomap recovers the intrinsic metric within a prescribed absolute distortion threshold $\\epsilon$. Begin from a fundamental geometric base and do not assume any target formula. Use the following definitions and well-tested facts as your starting point:\n\n- The curvature $\\kappa(s)$ of a plane or space curve parameterized by arc length $s$ measures how rapidly the curve deviates from a straight line locally.\n- The arc length between parameters $t_1$ and $t_2$ for a curve $\\gamma(t)$ with speed $v(t) = \\|\\gamma'(t)\\|$ is $L(t_1,t_2) = \\int_{t_1}^{t_2} v(t)\\, dt$.\n- For a circular arc of radius $R$ and arc length $s$, the straight chord length is $c = 2 R \\sin\\!\\left(\\frac{s}{2R}\\right)$, and the Maclaurin series for $\\sin(x)$ is $\\sin(x) = x - \\frac{x^3}{6} + \\cdots$.\n\nFrom this base, derive how the maximum curvature $\\kappa_{\\max}$ and the sampling interval measured in arc length $\\delta$ control the worst-case absolute distortion between the polygonal chain length and the true geodesic length along the curve. Use this to obtain a closed-form upper bound on the required maximum arc-length spacing $\\delta$ in terms of $\\epsilon$, $\\kappa_{\\max}$, and the total curve length $T$, and then derive the minimum integer number of samples $N$ needed so that the Isomap distances computed on the consecutive-neighbor chain have absolute distortion at most $\\epsilon$ for all pairs of sampled points.\n\nAfter completing the derivation, implement a program that:\n- Computes the maximum curvature $\\kappa_{\\max}$ and total length $T$ for each test curve.\n- Uses your derived bound to compute the minimal number of samples $N_{\\text{theory}}$ required so that the maximum absolute distortion between Isomap distances and true geodesic distances is at most $\\epsilon$.\n- Samples the curve at $N_{\\text{theory}}$ points, evenly spaced in arc length (not parameter space). Constructs the chain graph connecting only consecutive samples. Computes the Isomap distances as sums of Euclidean chord lengths along the chain, and the true geodesic distances as arc-length differences. Reports the observed maximum absolute distortion across all sampled point pairs.\n- Produces a single line of output containing the results for all test cases as a comma-separated list of two-element lists $[N_{\\text{theory}}, \\text{max\\_distortion}]$, enclosed in square brackets.\n\nAngle units for all trigonometric functions must be in radians.\n\nUse the following test suite that varies curvature and length to cover multiple cases:\n\n- Test $1$ (circular arc in a plane): radius $R = 1.5$, arc length $L = 3.0$, distortion threshold $\\epsilon = 0.005$. The curve is $\\gamma(s) = \\big(R \\cos(s/R), R \\sin(s/R), 0\\big)$ for $s \\in [0, L]$.\n- Test $2$ (planar sinusoid): amplitude $A = 0.5$, frequency $\\omega = 2.0$, parameter interval $t \\in [0, 2.0]$, distortion threshold $\\epsilon = 0.01$. The curve is $\\gamma(t) = \\big(t, A \\sin(\\omega t), 0\\big)$.\n- Test $3$ (helix): radius $a = 1.0$, pitch parameter $b = 0.2$, parameter interval $t \\in [0, 4.0]$, distortion threshold $\\epsilon = 0.005$. The curve is $\\gamma(t) = \\big(a \\cos t, a \\sin t, b t\\big)$.\n- Test $4$ (low-curvature circular arc): radius $R = 100.0$, arc length $L = 3.0$, distortion threshold $\\epsilon = 0.005$. The curve is $\\gamma(s) = \\big(R \\cos(s/R), R \\sin(s/R), 0\\big)$ for $s \\in [0, L]$.\n\nYour program should produce a single line of output containing the results for the four tests as a comma-separated list enclosed in square brackets, where each result is the two-element list $[N_{\\text{theory}}, \\text{max\\_distortion}]$. For example, the format must be exactly like $[[N_1,d_1],[N_2,d_2],[N_3,d_3],[N_4,d_4]]$ with no additional text.\n\nAll mathematical entities in this problem are written in LaTeX. Angles are in radians. No physical units are involved, so no unit conversions are required. Express numerical outputs as plain decimal numbers.", "solution": "The problem asks for a derivation of the minimum number of samples $N$ required to ensure that the Isomap distance on a 1D curve, approximated as a polygonal chain, remains within an absolute error $\\epsilon$ of the true geodesic distance. This derivation must be based on the curve's total length $T$ and its maximum curvature $\\kappa_{\\max}$.\n\nThe solution proceeds in four steps:\n1.  Analyze the local distortion on a single small segment of the curve.\n2.  Aggregate the local distortions to find an upper bound on the total distortion for any pair of points on the curve.\n3.  Use this upper bound to derive the required sampling density, expressed first as the maximum arc-length spacing $\\delta$ and then as the minimum number of samples $N$.\n4.  Apply this derived formula to the specified test cases and implement a numerical experiment to verify the theoretical bound.\n\n**1. Local Distortion Analysis**\n\nConsider a small segment of the curve between two consecutive sample points. The true geodesic distance is the arc length of this segment, denoted by $\\delta s$. The Isomap distance, in this problem's specific configuration, is the length of the straight-line chord connecting the points, denoted by $\\delta c$. The local absolute distortion is $\\delta s - \\delta c$.\n\nTo bound this error, we model the curve segment locally by its osculating circle. The radius of this circle is $R = 1/\\kappa$, where $\\kappa$ is the local curvature. For a circular arc of length $\\delta s$ and radius $R$, the corresponding chord length is given by the formula $\\delta c = 2R \\sin(\\frac{\\delta s}{2R})$.\n\nThe problem provides the Maclaurin series for $\\sin(x)$:\n$$ \\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\dots $$\nFor $x \\ge 0$, this is an alternating series whose terms decrease in magnitude. A known property of such series is that the sum is bounded by its partial sums. Specifically, for our purposes, $x - \\frac{x^3}{6} \\le \\sin(x) \\le x$.\n\nApplying the upper bound $\\sin(x) \\le x$ with $x = \\frac{\\delta s}{2R}$:\n$$ \\delta c = 2R \\sin\\left(\\frac{\\delta s}{2R}\\right) \\le 2R \\left(\\frac{\\delta s}{2R}\\right) = \\delta s $$\nThis confirms that the chord length is always less than or equal to the arc length, so the distortion $\\delta s - \\delta c$ is non-negative.\n\nTo find an upper bound on the distortion, we use the lower bound for $\\sin(x)$:\n$$ \\delta c = 2R \\sin\\left(\\frac{\\delta s}{2R}\\right) \\ge 2R \\left[ \\left(\\frac{\\delta s}{2R}\\right) - \\frac{1}{6}\\left(\\frac{\\delta s}{2R}\\right)^3 \\right] $$\n$$ \\delta c \\ge \\delta s - 2R \\frac{(\\delta s)^3}{48R^3} = \\delta s - \\frac{(\\delta s)^3}{24R^2} $$\nRearranging this inequality gives an upper bound on the local distortion:\n$$ \\delta s - \\delta c \\le \\frac{(\\delta s)^3}{24R^2} $$\nSubstituting the curvature $\\kappa = 1/R$, we obtain the local distortion bound in terms of curvature:\n$$ \\delta s - \\delta c \\le \\frac{\\kappa^2 (\\delta s)^3}{24} $$\n\n**2. Global Distortion Bound**\n\nThe problem specifies that the curve is sampled at $N$ points, evenly spaced in arc length. This creates $N-1$ segments, each of arc length $\\delta = T/(N-1)$, where $T$ is the total length of the curve.\n\nThe absolute distortion $D_{ij}$ between any two sample points $p_i$ and $p_j$ (with $j>i$) is the difference between the true geodesic distance $S_{ij}$ and the Isomap path length $C_{ij}$.\n$$ S_{ij} = \\sum_{k=i}^{j-1} \\delta s_k = (j-i)\\delta $$\n$$ C_{ij} = \\sum_{k=i}^{j-1} \\delta c_k $$\n$$ D_{ij} = S_{ij} - C_{ij} = \\sum_{k=i}^{j-1} (\\delta s_k - \\delta c_k) = \\sum_{k=i}^{j-1} (\\delta - \\delta c_k) $$\nTo find a global upper bound on the distortion for any pair $(i, j)$, we replace the local curvature $\\kappa_k$ in each segment with the maximum curvature over the entire curve, $\\kappa_{\\max}$.\n$$ D_{ij} = \\sum_{k=i}^{j-1} (\\delta - \\delta c_k) \\le \\sum_{k=i}^{j-1} \\frac{\\kappa_{\\max}^2 \\delta^3}{24} = (j-i) \\frac{\\kappa_{\\max}^2 \\delta^3}{24} $$\nThe maximum possible distortion $D_{\\max}$ will occur over the longest path, which connects the two endpoints of the curve (from $i=0$ to $j=N-1$). This path involves all $N-1$ segments.\n$$ D_{\\max} = D_{0,N-1} \\le (N-1) \\frac{\\kappa_{\\max}^2 \\delta^3}{24} $$\nSince the total length is $T = (N-1)\\delta$, we can rewrite this bound as:\n$$ D_{\\max} \\le T \\frac{\\kappa_{\\max}^2 \\delta^2}{24} $$\n\n**3. Derivation of Required Sampling Density**\n\nWe are given a maximum permissible absolute distortion $\\epsilon$. To guarantee that the Isomap approximation is sufficiently accurate, we require $D_{\\max} \\le \\epsilon$.\n$$ T \\frac{\\kappa_{\\max}^2 \\delta^2}{24} \\le \\epsilon $$\nSolving for the maximum allowable arc-length spacing $\\delta$:\n$$ \\delta^2 \\le \\frac{24\\epsilon}{T \\kappa_{\\max}^2} $$\n$$ \\delta \\le \\frac{1}{\\kappa_{\\max}} \\sqrt{\\frac{24\\epsilon}{T}} $$\nThis is the closed-form upper bound on the arc-length spacing.\n\nTo find the minimum number of samples $N$, we use the relation $N-1 = T/\\delta$. A smaller $\\delta$ corresponds to a larger number of samples. To satisfy the inequality for $\\delta$, we need:\n$$ N-1 \\ge \\frac{T}{\\frac{1}{\\kappa_{\\max}} \\sqrt{\\frac{24\\epsilon}{T}}} = T \\kappa_{\\max} \\sqrt{\\frac{T}{24\\epsilon}} = \\kappa_{\\max} \\sqrt{\\frac{T^3}{24\\epsilon}} $$\nSince $N$ must be an integer, the number of segments $N-1$ must be the smallest integer satisfying this condition. This is achieved by taking the ceiling of the right-hand side.\n$$ N-1 = \\left\\lceil \\kappa_{\\max} \\sqrt{\\frac{T^3}{24\\epsilon}} \\right\\rceil $$\nThe minimum number of samples $N_{\\text{theory}}$ is therefore:\n$$ N_{\\text{theory}} = \\left\\lceil \\kappa_{\\max} \\sqrt{\\frac{T^3}{24\\epsilon}} \\right\\rceil + 1 $$\nThis formula provides the theoretical minimum number of samples needed to guarantee the distortion is at most $\\epsilon$.\n\n**4. Application to Test Cases**\n\nThe derived formula and a numerical simulation are applied to each test case. For each curve, we must first compute its total arc length $T$ and maximum curvature $\\kappa_{\\max}$. The formula for curvature of a parametric curve $\\gamma(t)$ in $\\mathbb{R}^3$ is:\n$$ \\kappa(t) = \\frac{\\|\\gamma'(t) \\times \\gamma''(t)\\|}{\\|\\gamma'(t)\\|^3} $$\nThe total arc length is $T = \\int_{t_{\\min}}^{t_{\\max}} \\|\\gamma'(t)\\| dt$. For curves not parameterized by arc length, sampling evenly in arc length requires solving for the parameter $t$ that corresponds to a given arc length $s$, typically via numerical root-finding on the arc length function $s(t) = \\int_{t_{\\min}}^t \\|\\gamma'(\\tau)\\| d\\tau$.\n\nThe implementation will calculate $N_{\\text{theory}}$ for each curve, sample the curve at $N_{\\text{theory}}$ points evenly spaced in arc length, compute all pairwise geodesic and Isomap distances, and find the maximum observed absolute distortion.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the Isomap distortion problem for four test cases.\n    Derives and applies a formula for the minimum number of samples,\n    then runs a numerical experiment to verify the distortion bound.\n    \"\"\"\n\n    test_cases = [\n        {'id': 1, 'type': 'circle', 'R': 1.5, 'L': 3.0, 'epsilon': 0.005},\n        {'id': 2, 'type': 'sinusoid', 'A': 0.5, 'w': 2.0, 't_max': 2.0, 'epsilon': 0.01},\n        {'id': 3, 'type': 'helix', 'a': 1.0, 'b': 0.2, 't_max': 4.0, 'epsilon': 0.005},\n        {'id': 4, 'type': 'circle', 'R': 100.0, 'L': 3.0, 'epsilon': 0.005},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        if case['type'] == 'circle':\n            R = case['R']\n            T = case['L']\n            epsilon = case['epsilon']\n\n            kappa_max = 1.0 / R\n            \n            gamma = lambda s: np.array([R * np.cos(s / R), R * np.sin(s / R), 0.0])\n            s_range = [0, T]\n\n        elif case['type'] == 'sinusoid':\n            A = case['A']\n            w = case['w']\n            t_max = case['t_max']\n            epsilon = case['epsilon']\n\n            # Curvature calculation\n            # k(t) = |x'y'' - y'x''| / (x'^2 + y'^2)^(3/2)\n            # x(t)=t, y(t)=A*sin(w*t) => x'=1, x''=0, y'=A*w*cos(w*t), y''=-A*w^2*sin(w*t)\n            # k(t) = A*w^2*|sin(w*t)| / (1 + (A*w*cos(w*t))^2)^(3/2)\n            # Max curvature occurs when sin(wt)=1 and cos(wt)=0\n            kappa_max = A * w**2\n\n            gamma = lambda t: np.array([t, A * np.sin(w * t), 0.0])\n            speed = lambda t: np.sqrt(1 + (A * w * np.cos(w * t))**2)\n            \n            T = quad(speed, 0, t_max)[0]\n            s_range = [0, T]\n\n        elif case['type'] == 'helix':\n            a = case['a']\n            b = case['b']\n            t_max = case['t_max']\n            epsilon = case['epsilon']\n\n            # Curvature k = a / (a^2 + b^2)\n            kappa_max = a / (a**2 + b**2)\n            \n            # Speed ||gamma'(t)|| = sqrt(a^2 + b^2)\n            const_speed = np.sqrt(a**2 + b**2)\n            T = t_max * const_speed\n            \n            gamma = lambda t: np.array([a * np.cos(t), a * np.sin(t), b * t])\n            s_range = [0, T]\n\n        # Calculate theoretical number of samples N_theory\n        if T == 0 or kappa_max == 0:\n             # Handle trivial case, though not in test suite. Minimum 2 samples.\n             N_theory = 2\n        else:\n            # Formula: N = ceil(kappa_max * sqrt(T^3 / (24*epsilon))) + 1\n            val_inside_sqrt = (T**3) / (24 * epsilon)\n            num_segments = math.ceil(kappa_max * np.sqrt(val_inside_sqrt))\n            N_theory = int(num_segments) + 1\n        \n        # Ensure at least 2 samples for a segment\n        if N_theory  2:\n            N_theory = 2\n\n        # Sample the curve at N_theory points, evenly spaced in arc length\n        num_segments = N_theory - 1\n        delta_s = T / num_segments\n        target_arc_lengths = [i * delta_s for i in range(N_theory)]\n\n        sample_points = []\n        if case['type'] == 'circle':\n            # Parameterized by arc length s\n            s_samples = target_arc_lengths\n            for s in s_samples:\n                sample_points.append(gamma(s))\n        \n        elif case['type'] == 'helix':\n            # Constant speed, so arc length is proportional to parameter t\n            # s = t * const_speed => t = s / const_speed\n            const_speed = np.sqrt(case['a']**2 + case['b']**2)\n            t_samples = [s / const_speed for s in target_arc_lengths]\n            for t in t_samples:\n                sample_points.append(gamma(t))\n                \n        elif case['type'] == 'sinusoid':\n            # General case: need to solve for t for each s\n            arc_length_func = lambda t: quad(speed, 0, t)[0]\n            t_samples = [0.0]\n            \n            # Use Brent's method to find t_i for each target s_i\n            t_max_param = case['t_max']\n            for s_target in target_arc_lengths[1:]:\n                # Function to find root of: arc_length_func(t) - s_target = 0\n                f_to_solve = lambda t: arc_length_func(t) - s_target\n                # Search interval starts from the last found t\n                t_start_interval = t_samples[-1]\n                t_i = brentq(f_to_solve, t_start_interval, t_max_param)\n                t_samples.append(t_i)\n\n            for t in t_samples:\n                sample_points.append(gamma(t))\n\n        # Calculate max observed distortion\n        max_obs_distortion = 0.0\n        if N_theory > 1:\n            chords = [np.linalg.norm(sample_points[k+1] - sample_points[k]) for k in range(N_theory - 1)]\n            # Cumulative sums for efficient calculation of Isomap distances\n            isomap_prefix_sum = np.cumsum(np.concatenate(([0], chords)))\n            \n            for i in range(N_theory):\n                for j in range(i + 1, N_theory):\n                    geodesic_dist = (j - i) * delta_s\n                    # Isomap distance is the sum of chords between i and j\n                    isomap_dist = isomap_prefix_sum[j] - isomap_prefix_sum[i]\n                    distortion = abs(geodesic_dist - isomap_dist)\n                    if distortion > max_obs_distortion:\n                        max_obs_distortion = distortion\n        \n        results.append([N_theory, max_obs_distortion])\n\n    # Format the final output string\n    result_str = ','.join([f\"[{N},{d}]\" for N, d in results])\n    print(f\"[{result_str}]\")\n\n\nsolve()\n```", "id": "3133711"}, {"introduction": "While powerful, Isomap's standard implementation can fail when data is sampled non-uniformly, creating erroneous \"shortcut\" edges across low-density regions. This practice demonstrates this critical failure mode and guides you through implementing a solution: a density-aware graph pruning method [@problem_id:3133637]. By fixing a purpose-built \"broken\" dataset, you will learn how to make Isomap more robust for complex, real-world applications.", "problem": "You are asked to implement Isometric Mapping (Isomap) with a density-thresholded graph to remove edges that cross low-density valleys, and to test its behavior on a synthetic dataset that contains two parallel manifold paths connecting the same endpoints but sampled with different densities. The goal is to reason from first principles of graph-based geodesic approximation and classical Multidimensional Scaling (MDS), and to quantify the effect of density-thresholding on estimated geodesic distances.\n\nConstruct a deterministic dataset in two dimensions as follows. Let the ambient space be $\\mathbb{R}^2$. Consider the unit circle of radius $r = 1$ centered at the origin. Define two endpoints $A = (1, 0)$ and $B = (-1, 0)$. Generate two semicircular paths between $A$ and $B$:\n- A high-density upper semicircle: for $n_{\\text{high}}$ interior angles $\\theta_j$ equally spaced in $(0,\\pi)$ (in radians), include points $(\\cos \\theta_j, \\sin \\theta_j)$. Then include $A$ and $B$ as unique endpoints.\n- A low-density lower semicircle: for $n_{\\text{low}}$ interior angles $\\phi_\\ell$ equally spaced in $(0,\\pi)$ (in radians), include points $(\\cos \\phi_\\ell, -\\sin \\phi_\\ell)$. Do not include endpoints on this lower path so that both paths share the unique endpoints $A$ and $B$.\n\nAll angles must be in radians.\n\nUse the following fundamental base:\n- Graph geodesic approximation: Given data points $X = \\{x_i \\in \\mathbb{R}^D\\}_{i=1}^n$, construct a $k$-Nearest Neighbors (k-NN) graph with edge weights equal to Euclidean distances. The all-pairs shortest-path distances on this graph approximate manifold geodesic distances. This follows from the principle that, under sufficiently dense sampling and appropriately local connectivity, shortest paths on the graph converge to geodesics on the underlying manifold.\n- Classical Multidimensional Scaling (MDS): Given a symmetric distance matrix $\\Delta = [\\delta_{ij}]$, construct the centered Gram matrix\n$$\nB = -\\tfrac{1}{2} H \\Delta^{\\circ 2} H,\n$$\nwhere $H = I - \\tfrac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$, $I$ is the identity matrix, $\\mathbf{1}$ is the vector of ones, and $\\Delta^{\\circ 2}$ denotes elementwise squaring. If $B = Q \\Lambda Q^\\top$ is the eigendecomposition with eigenvalues in descending order, the embedding into $\\mathbb{R}^d$ is given by the rows of $Y = Q_d \\Lambda_d^{1/2}$, where $Q_d$ contains the top $d$ eigenvectors and $\\Lambda_d$ the corresponding eigenvalues. For this task, you must implement geodesic computation; the embedding step is not required for the test outputs.\n\nDefine a density-thresholded Isomap graph as follows. Let $D = [\\|x_i - x_j\\|_2]$ be the Euclidean distance matrix. Define a local density proxy at each point by\n$$\n\\rho_i = \\frac{1}{d_i^{(m)} + \\varepsilon},\n$$\nwhere $d_i^{(m)}$ is the distance from $x_i$ to its $m$-th nearest neighbor (excluding itself), and $\\varepsilon$ is a small positive constant to avoid division by zero. Given a density threshold $\\tau  0$, remove any existing edge $(i,j)$ in the $k$-NN graph if\n$$\n\\min(\\rho_i, \\rho_j)  \\tau.\n$$\nThis removes edges across low-density valleys.\n\nImplement the following procedures in a single program:\n1. Build the standard $k$-NN graph with symmetric edge weights equal to Euclidean distances.\n2. Compute the baseline geodesic distance $d_{\\text{base}}(A,B)$ between the endpoints using all-pairs shortest paths on this graph.\n3. Compute the density scores $\\rho_i$ using the $m$-th nearest neighbor rule above.\n4. For a given threshold $\\tau$, prune edges using the rule $\\min(\\rho_i, \\rho_j) \\ge \\tau$ and compute the density-thresholded geodesic distance $d_{\\text{den}}(A,B)$.\n5. If $A$ and $B$ are disconnected after pruning, define $d_{\\text{den}}(A,B) = +\\infty$.\n\nTest suite. Use the following fixed parameters and test cases to quantify the method’s behavior:\n- Dataset parameters: $r = 1$, $n_{\\text{high}} = 120$ interior points on the upper semicircle, $n_{\\text{low}} = 6$ interior points on the lower semicircle. Angles are equally spaced in $(0,\\pi)$ and expressed in radians. The dataset is the union of $\\{A\\}$, the upper interior points, $\\{B\\}$, and the lower interior points.\n- Graph parameters: $k = 6$ for $k$-NN, $m = 5$ for density estimation, and $\\varepsilon = 10^{-12}$.\n- Endpoints: $A$ is the first point, and $B$ is the $(1 + n_{\\text{high}})$-th point in the constructed array.\n\nDefine three test cases, each producing a boolean result:\n- Case $1$ (happy path): Use $\\tau = 1.5$. Output the boolean value of the predicate $d_{\\text{den}}(A,B)  d_{\\text{base}}(A,B) + 10^{-2}$.\n- Case $2$ (near-no pruning): Use $\\tau = 0.0$. Output the boolean value of the predicate $\\lvert d_{\\text{den}}(A,B) - d_{\\text{base}}(A,B) \\rvert  10^{-12}$.\n- Case $3$ (over-pruning): Use $\\tau = 10.0$. Output the boolean value of the predicate $d_{\\text{den}}(A,B) = +\\infty$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true_case1,true_case2,true_case3]\"), where each entry is the boolean result for the corresponding test case in the order above. The printed booleans must appear in Python’s default boolean formatting as \"True\" or \"False\", with no spaces inside the brackets.\n\nAll numerical values must be treated as dimensionless in this problem, and all angles are in radians. No user input is required or allowed; the program must be fully self-contained and deterministic.", "solution": "The problem asks for the implementation and analysis of a density-aware variant of the Isometric Mapping (Isomap) algorithm. The core task is to evaluate how pruning edges in a neighborhood graph based on local point density affects the estimation of geodesic distances on a synthetic dataset. The dataset is specifically designed with two paths of differing sampling densities to create a scenario where standard Isomap might produce an incorrect distance estimate.\n\nThe solution proceeds in four main stages:\n1.  Generation of a deterministic two-dimensional dataset.\n2.  Construction of a baseline neighborhood graph and computation of the shortest path distance between two specified endpoints, denoted $d_{\\text{base}}(A,B)$.\n3.  Implementation of a density-based edge pruning mechanism.\n4.  Computation of the shortest path distance on the pruned graph, $d_{\\text{den}}(A,B)$, and evaluation of its behavior for three distinct density thresholds.\n\n**1. Dataset Construction**\n\nThe dataset is constructed in the two-dimensional Euclidean space $\\mathbb{R}^2$. It comprises points lying on a unit circle with radius $r=1$ centered at the origin. Two semicircular paths connect the endpoints $A=(1, 0)$ and $B=(-1, 0)$.\n\n*   **High-Density Upper Semicircle:** This path consists of the endpoints $A$ and $B$, along with $n_{\\text{high}} = 120$ interior points. These points are generated as $(\\cos \\theta_j, \\sin \\theta_j)$, where the angles $\\theta_j$ for $j=1, \\dots, n_{\\text{high}}$ are equally spaced in the open interval $(0, \\pi)$. The total number of points on this path is $122$.\n*   **Low-Density Lower Semicircle:** This path consists of $n_{\\text{low}} = 6$ interior points, generated as $(\\cos \\phi_\\ell, -\\sin \\phi_\\ell)$, where the angles $\\phi_\\ell$ for $\\ell=1, \\dots, n_{\\text{low}}$ are also equally spaced in $(0, \\pi)$. This path shares the endpoints $A$ and $B$ from the upper path but does not include them in its own definition, ensuring a single instance of each endpoint.\n\nThe final dataset is a collection of $N = 1 + n_{\\text{high}} + 1 + n_{\\text{low}} = 1 + 120 + 1 + 6 = 128$ points. The points are ordered in an array such that point $A$ is at index $0$ and point $B$ is at index $1 + n_{\\text{high}} = 121$. This specific structure creates a manifold with non-uniform sampling density.\n\n**2. Baseline Geodesic Distance Calculation**\n\nThe Isomap algorithm approximates the intrinsic geodesic distance between points on a manifold by computing shortest paths on a neighborhood graph.\n\n*   **Step 1: Neighborhood Graph Construction.** We first construct a $k$-Nearest Neighbors ($k$-NN) graph, where $k=6$. The vertices of the graph correspond to the data points $\\{\\mathbf{x}_i\\}_{i=1}^N$. An edge is created between two points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ if one is among the $k$ nearest neighbors of the other. The graph is made symmetric: if $\\mathbf{x}_j$ is a neighbor of $\\mathbf{x}_i$, an edge $(i, j)$ is added, and if $\\mathbf{x}_i$ is a neighbor of $\\mathbf{x}_j$, an edge $(j, i)$ is also added. The weight of an edge $(i,j)$ is set to the Euclidean distance $\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$. Non-existent edges have an infinite weight. This produces a weighted adjacency matrix for the baseline graph.\n\n*   **Step 2: Shortest Path Computation.** The all-pairs shortest-path distances on this graph are computed using the Floyd-Warshall algorithm. The resulting distance matrix provides an approximation of the geodesic distances on the underlying manifold. The baseline geodesic distance between the endpoints, $d_{\\text{base}}(A,B)$, is the entry in this matrix corresponding to the path from point $A$ (index $0$) to point $B$ (index $121$). Due to the dataset's design, \"shortcut\" edges may form between the upper and lower paths, especially near the endpoints. This can cause $d_{\\text{base}}(A,B)$ to be an underestimate of the true manifold distance along the upper semicircle (which is $\\pi r = \\pi \\approx 3.14159$).\n\n**3. Density-Thresholded Graph Pruning**\n\nTo correct for such shortcuts, a density-based pruning strategy is introduced.\n\n*   **Step 1: Local Density Estimation.** A proxy for the local density at each point $\\mathbf{x}_i$ is defined as:\n    $$\n    \\rho_i = \\frac{1}{d_i^{(m)} + \\varepsilon}\n    $$\n    where $d_i^{(m)}$ is the Euclidean distance from $\\mathbf{x}_i$ to its $m$-th nearest neighbor (with $m=5$), and $\\varepsilon = 10^{-12}$ is a small constant to prevent division by zero. Points in densely sampled regions (like the upper semicircle) will have small $d_i^{(m)}$ values and thus high $\\rho_i$ scores. Conversely, points in sparsely sampled regions (like the lower semicircle) will have large $d_i^{(m)}$ and low $\\rho_i$.\n\n*   **Step 2: Edge Pruning.** Given a density threshold $\\tau  0$, an existing edge $(i,j)$ in the $k$-NN graph is removed if the density at either of its endpoints is below the threshold. The formal rule is: remove edge $(i,j)$ if\n    $$\n    \\min(\\rho_i, \\rho_j)  \\tau\n    $$\n    This rule effectively eliminates edges that connect a point in a high-density region to a point in a low-density region, which are often the problematic \"shortcut\" edges.\n\n**4. Evaluation of Test Cases**\n\nAfter pruning the graph for a given $\\tau$, the all-pairs shortest paths are re-computed using the Floyd-Warshall algorithm on the pruned graph. This yields the density-thresholded geodesic distance, $d_{\\text{den}}(A,B)$. If the pruning disconnects points $A$ and $B$, this distance is defined as $+\\infty$.\n\n*   **Case 1: $\\tau = 1.5$.** This \"happy path\" threshold is expected to be low enough to preserve edges within the high-density upper path but high enough to prune the shortcut edges connecting the upper and lower paths. This forces the shortest path to follow the upper semicircle more faithfully, resulting in a distance $d_{\\text{den}}(A,B)$ that is a better approximation of $\\pi$ and thus greater than the likely underestimated $d_{\\text{base}}(A,B)$. The predicate $d_{\\text{den}}(A,B)  d_{\\text{base}}(A,B) + 10^{-2}$ is expected to be true.\n\n*   **Case 2: $\\tau = 0.0$.** With a threshold of $0$, no edges are pruned because the density $\\rho_i$ is always positive. The pruned graph is identical to the base graph. Therefore, $d_{\\text{den}}(A,B)$ must equal $d_{\\text{base}}(A,B)$. The predicate $|\\,d_{\\text{den}}(A,B) - d_{\\text{base}}(A,B)\\,|  10^{-12}$ will be true.\n\n*   **Case 3: $\\tau = 10.0$.** This \"over-pruning\" threshold is set high enough to be greater than the density values of most, if not all, points in the dataset (even those on the high-density path). This will cause a massive removal of edges, very likely disconnecting the graph into isolated components or small fragments. As a result, no path will exist between $A$ and $B$, yielding $d_{\\text{den}}(A,B) = +\\infty$. The predicate $d_{\\text{den}}(A,B) = +\\infty$ is expected to be true.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.sparse.csgraph import floyd_warshall\n\ndef solve():\n    \"\"\"\n    Implements and tests a density-aware Isomap variant on a synthetic dataset.\n    \"\"\"\n    # Define parameters from the problem statement.\n    r, n_high, n_low = 1.0, 120, 6\n    k, m, epsilon = 6, 5, 1e-12\n    idx_A, idx_B = 0, 1 + n_high\n    test_taus = [1.5, 0.0, 10.0]\n\n    # --- 1. Dataset Generation ---\n    n_total = n_high + n_low + 2\n    points = np.zeros((n_total, 2))\n    \n    # Endpoint A\n    points[0] = [r, 0]\n    \n    # High-density upper semicircle (interior points)\n    theta = np.linspace(0, np.pi, n_high + 2)[1:-1]\n    points[1:1+n_high] = np.array([r * np.cos(theta), r * np.sin(theta)]).T\n    \n    # Endpoint B\n    points[1+n_high] = [-r, 0]\n    \n    # Low-density lower semicircle (interior points)\n    phi = np.linspace(0, np.pi, n_low + 2)[1:-1]\n    points[1+n_high+1:] = np.array([r * np.cos(phi), -r * np.sin(phi)]).T\n\n    # --- Euclidean Distance Matrix ---\n    D_euc = cdist(points, points)\n\n    # --- 2. Baseline Geodesic Distance Calculation ---\n    # Build k-NN graph\n    n_points = D_euc.shape[0]\n    G_base = np.full((n_points, n_points), np.inf)\n    np.fill_diagonal(G_base, 0)\n    \n    neighbor_indices = np.argsort(D_euc, axis=1)[:, 1:k+1]\n    for i in range(n_points):\n        for j in neighbor_indices[i]:\n            G_base[i, j] = D_euc[i, j]\n            G_base[j, i] = D_euc[j, i]  # Symmetrize graph\n\n    # Compute baseline geodesic distance\n    dist_matrix_base = floyd_warshall(csgraph=G_base, directed=False)\n    d_base = dist_matrix_base[idx_A, idx_B]\n\n    # --- 3. Density-Thresholded Graph Pruning ---\n    # Compute local densities\n    sorted_dists = np.sort(D_euc, axis=1)\n    d_m = sorted_dists[:, m]\n    rho = 1.0 / (d_m + epsilon)\n    \n    results = []\n    # --- 4. Evaluation of Test Cases ---\n    for tau in test_taus:\n        # Prune graph based on density\n        G_pruned = G_base.copy()\n        \n        # Vectorized pruning for efficiency\n        # Find all edges: where G_base is finite\n        # Create a matrix of min densities for each potential edge\n        rho_min_matrix = np.minimum.outer(rho, rho)\n        \n        # Define mask for edges to be pruned\n        pruning_mask = (G_base != np.inf)  (rho_min_matrix  tau)\n        G_pruned[pruning_mask] = np.inf\n\n        # Compute density-thresholded geodesic distance\n        dist_matrix_pruned = floyd_warshall(csgraph=G_pruned, directed=False)\n        d_den = dist_matrix_pruned[idx_A, idx_B]\n\n        # Evaluate the predicate for the current test case\n        if tau == 1.5:\n            result = d_den > d_base + 1e-2\n        elif tau == 0.0:\n            result = abs(d_den - d_base)  1e-12\n        elif tau == 10.0:\n            result = d_den == np.inf\n        else: # Should not be reached with the given test cases\n            result = False\n\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3133637"}, {"introduction": "Beyond geometric theory lies the practical challenge of scalability, as Isomap's all-pairs shortest path step is computationally intensive. The choice of algorithm—for instance, Floyd-Warshall versus running Dijkstra's algorithm from every node—has profound consequences for time and memory usage on large datasets. This practice challenges you to model these trade-offs and calculate the exact problem size $N$ where the optimal choice flips, providing a concrete lesson in the computational realities of machine learning [@problem_id:3133665].", "problem": "Consider Isometric Feature Mapping (Isomap), where the central computational step is to obtain all-pairs geodesic distances on a weighted, undirected $k$-nearest-neighbor graph of $N$ points with nonnegative edge weights. Two canonical strategies to obtain the geodesic distance matrix are: (a) the Floyd–Warshall algorithm, and (b) running Dijkstra’s algorithm from every node on the sparse $k$-nearest-neighbor graph. Assume the following foundational base: (i) the time complexity of Floyd–Warshall is $\\mathcal{O}(N^3)$ and it operates on a dense $N \\times N$ matrix; (ii) the single-source Dijkstra’s algorithm with a binary heap on a sparse graph with $V$ vertices and $E$ edges has time complexity $\\mathcal{O}((E+V)\\log V)$; (iii) a symmetric $k$-nearest-neighbor graph has average degree approximately $k$, so the number of undirected edges is $M \\approx \\frac{N k}{2}$, and the adjacency representation stores both directions (that is, approximately $2M$ directed arcs).\nYou will design a program that, for each test case, models feasibility on a standard laptop by estimating time and memory usage for each algorithm, then computes the smallest dataset size where the feasibility of the two algorithmic choices diverges.\nUse the following modeling assumptions for estimates:\n- Use double-precision floating point ($8$ bytes per entry) for all dense $N \\times N$ matrices.\n- Store each directed arc in the sparse adjacency as two fields: a $64$-bit integer neighbor index and a $64$-bit floating-point weight, for a total of $16$ bytes per directed arc.\n- For Floyd–Warshall, approximate time as $t_{\\mathrm{FW}}(N) = c_{\\mathrm{FW}} \\, N^3$ seconds, with $c_{\\mathrm{FW}} = 5 \\times 10^{-8}$.\n- For repeated Dijkstra, approximate time as $t_{\\mathrm{DK}}(N,k) = c_{\\mathrm{DK}} \\, \\left(\\frac{N^2 k}{2}\\right) \\log_2 N$ seconds, with $c_{\\mathrm{DK}} = 5 \\times 10^{-8}$.\n- For memory, approximate:\n  - Floyd–Warshall: $m_{\\mathrm{FW}}(N) = 8 N^2$ bytes (one dense distance/weight matrix).\n  - Repeated Dijkstra: $m_{\\mathrm{DK}}(N,k) = 8 N^2 + 16 \\times (2 \\times \\frac{N k}{2}) = 8 N^2 + 16 N k$ bytes (dense distance matrix plus sparse adjacency arcs).\n- Interpret $1$ gibibyte (GiB) as $2^{30}$ bytes.\nDefine feasibility for an algorithm at $(N,k)$ under a time budget $T$ seconds and a memory budget $R$ GiB as the conjunction of two inequalities: time estimate $\\le T$ and memory estimate $\\le R \\times 2^{30}$.\nFor each test case, define the flip size $N_{\\mathrm{flip}}$ as the smallest integer $N \\ge 2$ such that exactly one of the two algorithms is feasible under the given $k$, $T$, and $R$. If no such $N$ exists up to very large $N$ (assume the search space includes all $N$ for which the above models are meaningful), return $0$ for that test case.\nTest suite:\n- Case A: $(R, T, k) = (16 \\text{ GiB}, 1800 \\text{ s}, 12)$.\n- Case B: $(R, T, k) = (8 \\text{ GiB}, 600 \\text{ s}, 6)$.\n- Case C: $(R, T, k) = (32 \\text{ GiB}, 120 \\text{ s}, 50)$.\n- Case D: $(R, T, k) = (16 \\text{ GiB}, 900 \\text{ s}, 1200)$.\nYour program must:\n- Implement the feasibility checks exactly as defined above.\n- For each test case, compute and return $N_{\\mathrm{flip}}$.\n- Output a single line containing the four results as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{A},\\text{B},\\text{C},\\text{D}\\right]$ where each entry is an integer.\nAll times must be in seconds, and all memory quantities must be in bytes internally (with the conversion from GiB as specified). The final outputs must be integers. The program must not take any input and must run deterministically.", "solution": "The problem requires us to determine the smallest dataset size $N$, denoted $N_{\\mathrm{flip}}$, at which the feasibility of two algorithms for all-pairs shortest paths, Floyd-Warshall (FW) and repeated Dijkstra (DK), diverges, given specific computational budgets and performance models.\n\n### 1. Mathematical Modeling of Feasibility\n\nThe feasibility of each algorithm is determined by two constraints: a time budget $T$ and a memory budget $R$. The problem provides precise models for the time and memory costs as functions of the dataset size $N$ and the nearest-neighbor parameter $k$. All calculations will use bytes for memory and seconds for time, with $1$ GiB defined as $2^{30}$ bytes.\n\nLet $R_{bytes} = R \\times 2^{30}$.\nFor a given configuration $(R, T, k)$, an algorithm is feasible for a dataset of size $N$ if and only if both its estimated time and memory usage are within the specified budgets.\n\n**Floyd-Warshall (FW) Algorithm:**\nThe time and memory models are given as:\n- Time: $t_{\\mathrm{FW}}(N) = c_{\\mathrm{FW}} N^3$, with $c_{\\mathrm{FW}} = 5 \\times 10^{-8}$.\n- Memory: $m_{\\mathrm{FW}}(N) = 8 N^2$.\n\nThe feasibility conditions for FW are:\n$c_{\\mathrm{FW}} N^3 \\le T \\quad (1)$\n$8 N^2 \\le R_{bytes} \\quad (2)$\n\n**Repeated Dijkstra (DK) Algorithm:**\nThe time and memory models for a $k$-nearest-neighbor graph are:\n- Time: $t_{\\mathrm{DK}}(N,k) = c_{\\mathrm{DK}} \\left(\\frac{N^2 k}{2}\\right) \\log_2 N$, with $c_{\\mathrm{DK}} = 5 \\times 10^{-8}$.\n- Memory: $m_{\\mathrm{DK}}(N,k) = 8 N^2 + 16 N k$.\n\nThe feasibility conditions for DK are:\n$c_{\\mathrm{DK}} \\frac{N^2 k}{2} \\log_2 N \\le T \\quad (3)$\n$8 N^2 + 16 N k \\le R_{bytes} \\quad (4)$\n\n### 2. Derivation of Feasibility Thresholds\n\nFor each algorithm, the cost functions are monotonically increasing with $N$. This implies that for each algorithm, there exists a maximum integer dataset size, $N_{int}$, for which it is feasible. For any $N  N_{int}$, the algorithm is infeasible. This threshold is determined by the more restrictive of the time and memory constraints.\n\n**Floyd-Warshall Threshold ($N_{int, FW}$):**\nFrom inequality $(1)$, the time constraint imposes a limit on $N$:\n$N^3 \\le \\frac{T}{c_{\\mathrm{FW}}} \\implies N \\le \\left(\\frac{T}{c_{\\mathrm{FW}}}\\right)^{1/3}$.\nLet this be $N_{lim, FW, time}$.\n\nFrom inequality $(2)$, the memory constraint implies:\n$N^2 \\le \\frac{R_{bytes}}{8} \\implies N \\le \\sqrt{\\frac{R_{bytes}}{8}}$.\nLet this be $N_{lim, FW, mem}$.\n\nThe overall maximum size for FW is $N_{max, FW} = \\min(N_{lim, FW, time}, N_{lim, FW, mem})$. The largest integer size is therefore $N_{int, FW} = \\lfloor N_{max, FW} \\rfloor$.\n\n**Repeated Dijkstra Threshold ($N_{int, DK}$):**\nFrom inequality $(3)$, the time constraint gives a transcendental inequality:\n$N^2 \\log_2 N \\le \\frac{2 T}{c_{\\mathrm{DK}} k}$.\nSince the function $f(N) = N^2 \\log_2 N$ is monotonically increasing for $N \\ge 1$, we can find the maximum integer $N$ satisfying this condition using a numerical method, such as a binary search. Let this integer threshold be $N_{int, DK, time}$.\n\nFrom inequality $(4)$, the memory constraint is a quadratic inequality:\n$8 N^2 + 16 k N - R_{bytes} \\le 0$.\nThe positive root of the corresponding quadratic equation $8x^2 + 16kx - R_{bytes} = 0$ gives the upper bound on $N$:\n$N = \\frac{-16k + \\sqrt{(16k)^2 - 4(8)(-R_{bytes})}}{2(8)} = \\frac{-16k + \\sqrt{256k^2 + 32 R_{bytes}}}{16} = -k + \\sqrt{k^2 + \\frac{R_{bytes}}{8}}$.\nLet this be $N_{lim, DK, mem}$. The integer threshold is $\\lfloor N_{lim, DK, mem} \\rfloor$.\n\nThe overall integer threshold for DK is $N_{int, DK} = \\min(N_{int, DK, time}, \\lfloor N_{lim, DK, mem} \\rfloor)$.\n\n### 3. The Flip-Size ($N_{\\mathrm{flip}}$) Condition\n\nThe flip size $N_{\\mathrm{flip}}$ is defined as the smallest integer $N \\ge 2$ where the feasibility statuses of the two algorithms differ. Let $F_{FW}(N)$ and $F_{DK}(N,k)$ be the boolean feasibility functions. We seek the minimum $N \\ge 2$ such that $F_{FW}(N) \\neq F_{DK}(N,k)$.\n\nThe feasibility of algorithm `alg` for a given $N$ is simply determined by whether $N \\le N_{int, alg}$.\n- For any $N \\le \\min(N_{int, FW}, N_{int, DK})$, both algorithms are feasible.\n- For any $N  \\max(N_{int, FW}, N_{int, DK})$, both algorithms are infeasible.\n\nA divergence in feasibility can only occur in the range between these two states.\nLet's assume, without loss of generality, that $N_{int, FW}  N_{int, DK}$.\n- For any integer $N$ such that $N \\le N_{int, FW}$, both algorithms are feasible.\n- At $N = N_{int, FW} + 1$, FW becomes infeasible because $N  N_{int, FW}$. However, DK remains feasible because $N_{int, FW} + 1 \\le N_{int, DK}$.\nThis is the first value of $N$ where their feasibility differs. Thus, $N_{\\mathrm{flip}} = N_{int, FW} + 1 = \\min(N_{int, FW}, N_{int, DK}) + 1$.\n\nIf $N_{int, FW} = N_{int, DK}$, then for any $N \\le N_{int, FW}$, both are feasible. For any $N  N_{int, FW}$, both are infeasible. There is no $N$ for which exactly one algorithm is feasible. In this case, $N_{\\mathrm{flip}}=0$.\n\nThe general rule is:\n$N_{\\mathrm{flip}} = \\begin{cases} \\min(N_{int, FW}, N_{int, DK}) + 1  \\text{if } N_{int, FW} \\neq N_{int, DK} \\\\ 0  \\text{if } N_{int, FW} = N_{int, DK} \\end{cases}$\n\nThe implementation will calculate $N_{int, FW}$ and $N_{int, DK}$ for each test case and apply this rule to find $N_{\\mathrm{flip}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the flip size N_flip for each test case based on algorithmic\n    feasibility models.\n    \"\"\"\n\n    # Define the modeling constants.\n    C_FW = 5e-8\n    C_DK = 5e-8\n    GIB_TO_BYTES = 2**30\n\n    test_cases = [\n        # Case A: (R, T, k) = (16 GiB, 1800 s, 12)\n        (16, 1800, 12),\n        # Case B: (R, T, k) = (8 GiB, 600 s, 6)\n        (8, 600, 6),\n        # Case C: (R, T, k) = (32 GiB, 120 s, 50)\n        (32, 120, 50),\n        # Case D: (R, T, k) = (16 GiB, 900 s, 1200)\n        (16, 900, 1200),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        R, T, k = case\n        R_bytes = R * GIB_TO_BYTES\n\n        # --- Calculate N_int_fw ---\n        # Time limit for FW: c_fw * N^3 = T => N = (T / c_fw)^(1/3)\n        n_lim_fw_time = (T / C_FW)**(1/3)\n        \n        # Memory limit for FW: 8 * N^2 = R_bytes => N = sqrt(R_bytes / 8)\n        n_lim_fw_mem = math.sqrt(R_bytes / 8)\n        \n        n_int_fw = math.floor(min(n_lim_fw_time, n_lim_fw_mem))\n        \n        # --- Calculate N_int_dk ---\n        # Time limit for DK: c_dk * (N^2*k/2) * log2(N) = T\n        # => N^2 * log2(N) = 2 * T / (c_dk * k)\n        C_time_dk = (2 * T) / (C_DK * k)\n\n        # Binary search for the largest integer N satisfying the time constraint.\n        # The search range is from 2 up to a safe upper bound. The FW memory\n        # limit provides a generous and safe upper bound for N.\n        low = 2\n        high = math.ceil(n_lim_fw_mem) + 1  # Safe upper bound\n        n_int_dk_time = 1\n        \n        while low = high:\n            mid = (low + high) // 2\n            if mid  2: # Ensure mid is at least 2 for log2 to be non-negative.\n                low = mid + 1\n                continue\n            \n            # Use numpy's log2 for consistency with the problem statement\n            val = (mid**2) * np.log2(mid)\n            if val = C_time_dk:\n                n_int_dk_time = mid # This is a potential solution\n                low = mid + 1\n            else:\n                high = mid - 1\n        \n        # Memory limit for DK: 8N^2 + 16Nk - R_bytes = 0\n        # Positive root of 8x^2 + 16kx - C = 0\n        a, b, c = 8, 16 * k, -R_bytes\n        discriminant = b**2 - 4 * a * c\n        n_lim_dk_mem = (-b + math.sqrt(discriminant)) / (2 * a)\n\n        n_int_dk = math.floor(min(n_int_dk_time, n_lim_dk_mem))\n\n        # --- Determine N_flip ---\n        if n_int_fw == n_int_dk:\n            n_flip = 0\n        else:\n            # The flip occurs at the first N where one becomes infeasible.\n            # This is 1 + the minimum of the two feasibility thresholds.\n            n_flip = min(n_int_fw, n_int_dk) + 1\n        \n        results.append(int(n_flip))\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3133665"}]}