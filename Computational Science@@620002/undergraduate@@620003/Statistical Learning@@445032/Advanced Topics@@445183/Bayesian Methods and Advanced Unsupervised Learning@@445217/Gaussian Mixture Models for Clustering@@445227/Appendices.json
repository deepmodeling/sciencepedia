{"hands_on_practices": [{"introduction": "To truly understand Gaussian Mixture Models, we must first dissect how their parameters shape the clustering outcome. This first practice invites you to derive the decision boundary between two Gaussian components from first principles. By focusing on a simplified one-dimensional case [@problem_id:3122648], you will discover how the mixing weights, or prior probabilities ($\\pi_k$), shift the boundary, providing a core insight into the Bayesian nature of GMM classification.", "problem": "Consider a Gaussian mixture model (GMM) with $2$ components for clustering in one-dimensional (1-D) feature space. Let the model have component means $\\mu_1$ and $\\mu_2$, identical covariance (which in 1-D equals the variance) $\\sigma^2$ for both components, and mixing weights (priors) $\\pi_1$ and $\\pi_2$ satisfying $\\pi_1 + \\pi_2 = 1$ and $\\pi_k \\in (0,1)$ for each component index $k \\in \\{1,2\\}$. The data-generating density is $p(x) = \\pi_1 \\,\\mathcal{N}(x \\mid \\mu_1, \\sigma^2) + \\pi_2 \\,\\mathcal{N}(x \\mid \\mu_2, \\sigma^2)$, where $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ denotes the one-dimensional Gaussian density. The Bayes decision rule assigns a point $x$ to the component with larger posterior proportional to $\\pi_k \\,\\mathcal{N}(x \\mid \\mu_k, \\sigma^2)$. The decision boundary between the two components is the location $x^\\star$ where the posteriors are equal.\n\nYour tasks are:\n- Starting only from the definitions of a Gaussian density and the Bayes decision rule, derive from first principles the equation for the decision boundary $x^\\star$ defined by $\\pi_1 \\,\\mathcal{N}(x^\\star \\mid \\mu_1, \\sigma^2) = \\pi_2 \\,\\mathcal{N}(x^\\star \\mid \\mu_2, \\sigma^2)$, and solve this equation for $x^\\star$ in terms of $\\mu_1$, $\\mu_2$, $\\sigma^2$, $\\pi_1$, and $\\pi_2$.\n- Define the midpoint $m = (\\mu_1 + \\mu_2)/2$ and the displacement $\\Delta = x^\\star - m$. Analyze how the sign of $\\Delta$ depends on the mixing weights, and relate the sign of $\\Delta$ to whether the decision boundary shifts toward $\\mu_1$ or toward $\\mu_2$.\n\nImplement a program that, for each test case, computes the triple $[x^\\star, \\Delta, s]$ where $x^\\star$ is the decision boundary location, $\\Delta$ is the displacement from the midpoint, and $s$ is an integer sign indicator defined by\n- $s = 1$ if $\\Delta > 0$ (boundary shifts toward $\\mu_2$),\n- $s = -1$ if $\\Delta  0$ (boundary shifts toward $\\mu_1$),\n- $s = 0$ if $\\Delta = 0$ (no shift).\n\nUse the following test suite, which keeps covariances identical and varies mixing weights to probe the decision boundary shift:\n- Test case $1$: $\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.5$, $\\pi_2 = 0.5$.\n- Test case $2$: $\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.7$, $\\pi_2 = 0.3$.\n- Test case $3$: $\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.01$, $\\pi_2 = 0.99$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one triple per test case in order: $[[x^\\star_1, \\Delta_1, s_1], [x^\\star_2, \\Delta_2, s_2], [x^\\star_3, \\Delta_3, s_3]]$. No additional text should be printed.", "solution": "The user has provided a valid, well-posed problem grounded in statistical learning theory. The task is to derive the decision boundary for a two-component one-dimensional Gaussian Mixture Model (GMM) and analyze its properties.\n\n### Part 1: Derivation of the Decision Boundary $x^\\star$\n\nThe problem defines the decision boundary, $x^\\star$, as the point where the posterior probabilities of the two components are equal. In the context of a GMM, the posterior for component $k$ is proportional to the prior probability (mixing weight) $\\pi_k$ multiplied by the class-conditional density $\\mathcal{N}(x \\mid \\mu_k, \\sigma^2)$. Therefore, the decision boundary $x^\\star$ satisfies the equation:\n$$\n\\pi_1 \\,\\mathcal{N}(x^\\star \\mid \\mu_1, \\sigma^2) = \\pi_2 \\,\\mathcal{N}(x^\\star \\mid \\mu_2, \\sigma^2)\n$$\nThe one-dimensional Gaussian probability density function (PDF) is given by:\n$$\n\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\nSubstituting this definition into the boundary equation gives:\n$$\n\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) = \\pi_2 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right)\n$$\nThe normalization constant $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ is positive and common to both sides, so it can be cancelled:\n$$\n\\pi_1 \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) = \\pi_2 \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right)\n$$\nTo solve for $x^\\star$, we take the natural logarithm of both sides. This is a monotonic transformation that preserves the equality.\n$$\n\\ln\\left( \\pi_1 \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) \\right) = \\ln\\left( \\pi_2 \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right) \\right)\n$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^z) = z$, we get:\n$$\n\\ln(\\pi_1) - \\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2} = \\ln(\\pi_2) - \\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\n$$\nRearranging the terms to group the logarithmic parts and the quadratic parts:\n$$\n\\ln(\\pi_1) - \\ln(\\pi_2) = \\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2} - \\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\n$$\nUsing $\\ln(a) - \\ln(b) = \\ln(a/b)$ and multiplying both sides by $2\\sigma^2$:\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = (x^\\star-\\mu_1)^2 - (x^\\star-\\mu_2)^2\n$$\nWe expand the quadratic terms on the right-hand side:\n$$\n(x^\\star)^2 - 2x^\\star\\mu_1 + \\mu_1^2 - \\left((x^\\star)^2 - 2x^\\star\\mu_2 + \\mu_2^2\\right)\n$$\n$$\n= (x^\\star)^2 - 2x^\\star\\mu_1 + \\mu_1^2 - (x^\\star)^2 + 2x^\\star\\mu_2 - \\mu_2^2\n$$\nThe $(x^\\star)^2$ terms cancel out, leaving a linear equation in $x^\\star$:\n$$\n= 2x^\\star(\\mu_2 - \\mu_1) + (\\mu_1^2 - \\mu_2^2)\n$$\nUsing the difference of squares, $\\mu_1^2 - \\mu_2^2 = (\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2) = -(\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)$, this simplifies to:\n$$\n= 2x^\\star(\\mu_2 - \\mu_1) - (\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)\n$$\nSubstituting this back into our equation:\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = 2x^\\star(\\mu_2 - \\mu_1) - (\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)\n$$\nAssuming the means are distinct ($\\mu_1 \\neq \\mu_2$), we can divide by $2(\\mu_2 - \\mu_1)$:\n$$\n\\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = x^\\star - \\frac{\\mu_1 + \\mu_2}{2}\n$$\nSolving for $x^\\star$ yields the final expression for the decision boundary:\n$$\nx^\\star = \\frac{\\mu_1 + \\mu_2}{2} + \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right)\n$$\n\n### Part 2: Analysis of the Displacement $\\Delta$\n\nThe problem defines the midpoint $m = (\\mu_1 + \\mu_2)/2$ and the displacement $\\Delta = x^\\star - m$. Using our derived expression for $x^\\star$:\n$$\n\\Delta = \\left( \\frac{\\mu_1 + \\mu_2}{2} + \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) \\right) - \\frac{\\mu_1 + \\mu_2}{2}\n$$\n$$\n\\Delta = \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right)\n$$\nThe sign of $\\Delta$ determines the shift of the decision boundary relative to the midpoint $m$. Since variance $\\sigma^2$ is always positive, the sign of $\\Delta$ depends on the signs of $(\\mu_2 - \\mu_1)$ and $\\ln(\\pi_1/\\pi_2)$.\nFor the given test cases, $\\mu_1 = -1$ and $\\mu_2 = 1$, so $\\mu_2 - \\mu_1 = 2 > 0$. In this scenario, the sign of $\\Delta$ is exclusively determined by the sign of $\\ln(\\pi_1/\\pi_2)$.\n\n- If $\\pi_1 = \\pi_2$, then $\\pi_1/\\pi_2 = 1$ and $\\ln(1) = 0$. This gives $\\Delta = 0$. The decision boundary $x^\\star$ is exactly at the midpoint $m$. This corresponds to the sign indicator $s=0$.\n- If $\\pi_1 > \\pi_2$, then $\\pi_1/\\pi_2 > 1$ and $\\ln(\\pi_1/\\pi_2) > 0$. This gives $\\Delta > 0$. The boundary $x^\\star = m + \\Delta$ is shifted from the midpoint in the positive direction, which is toward $\\mu_2$. This corresponds to $s=1$. The higher prior of component $1$ pushes the boundary away from its mean $\\mu_1$.\n- If $\\pi_1  \\pi_2$, then $\\pi_1/\\pi_2  1$ and $\\ln(\\pi_1/\\pi_2)  0$. This gives $\\Delta  0$. The boundary $x^\\star = m + \\Delta$ is shifted from the midpoint in the negative direction, which is toward $\\mu_1$. This corresponds to $s=-1$. The higher prior of component $2$ pushes the boundary away from its mean $\\mu_2$.\n\nThese results align with the definition of the sign indicator $s$ provided in the problem statement. The derived formulas can now be implemented to solve the test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu1, mu2, sigma_sq, pi1, pi2)\n        (-1.0, 1.0, 0.25, 0.5, 0.5),\n        (-1.0, 1.0, 0.25, 0.7, 0.3),\n        (-1.0, 1.0, 0.25, 0.01, 0.99),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu1, mu2, sigma_sq, pi1, pi2 = case\n\n        # Calculate the midpoint m\n        m = (mu1 + mu2) / 2.0\n        \n        # The decision boundary x_star is derived from the condition:\n        # pi1 * N(x|mu1, sigma_sq) = pi2 * N(x|mu2, sigma_sq)\n        # This simplifies to the following equation, as derived in the solution:\n        # x_star = m + (sigma_sq / (mu2 - mu1)) * log(pi1 / pi2)\n        \n        # We can assume mu1 != mu2, as per the test cases.\n        # The priors pi_k are in (0,1), so division by pi2 is safe.\n        log_ratio = np.log(pi1 / pi2)\n        \n        x_star = m + (sigma_sq / (mu2 - mu1)) * log_ratio\n        \n        # The displacement delta is defined as x_star - m.\n        delta = x_star - m\n        \n        # The sign indicator s is the sign of delta.\n        # np.sign(x) returns 1.0 for x>0, -1.0 for x0, and 0.0 for x=0.\n        # We cast the result to an integer.\n        s = int(np.sign(delta))\n        \n        # Store the computed triple [x_star, delta, s].\n        results.append([x_star, delta, s])\n\n    # The required output format is a single line with no spaces:\n    # [[x_1,d_1,s_1],[x_2,d_2,s_2],[x_3,d_3,s_3]]\n    # Converting the list of lists to a string and removing spaces achieves this.\n    final_output = str(results).replace(\" \", \"\")\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3122648"}, {"introduction": "One of the key advantages of GMMs over methods like k-means is their ability to provide \"soft\" assignments, indicating the probability that a data point belongs to each cluster. This exercise [@problem_id:3122627] introduces a powerful tool from information theory, Shannon entropy, to quantify the uncertainty of these assignments. By calculating the entropy for each point, you will learn to identify ambiguous regions of cluster overlap, adding a layer of nuanced interpretation to your clustering results.", "problem": "You are tasked with implementing a program that quantifies assignment uncertainty in Gaussian mixture model clustering using the Shannon entropy of component responsibilities for each data point. Begin from fundamental principles: use Bayes' theorem to obtain posterior assignment probabilities (responsibilities) from a Gaussian mixture model, and use the definition of Shannon entropy for a discrete distribution. The implementation must be numerically stable and scientifically sound.\n\nAssume a Gaussian mixture with $K$ components in $d$ dimensions. Let the mixture have nonnegative mixing proportions $\\pi_k$ with $\\sum_{k=1}^K \\pi_k = 1$, component means $\\mu_k \\in \\mathbb{R}^d$, and positive-definite covariance matrices $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$ for $k \\in \\{1,\\dots,K\\}$. The multivariate normal probability density function (PDF) for component $k$ is\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)\\right).\n$$\nUsing Bayes' theorem, form the posterior assignment probabilities (responsibilities) $r_{ik}$ for each data point $x_i$ as the posterior probability of component $k$ given $x_i$, derived from the component priors $\\pi_k$ and the component likelihoods $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$. For each point $x_i$, compute the Shannon entropy of its assignment distribution over the $K$ components using the natural logarithm,\n$$\nH(x_i) = -\\sum_{k=1}^K r_{ik} \\log r_{ik}.\n$$\nInterpretation: large $H(x_i)$ occurs when multiple components have comparable posterior probability at $x_i$, indicating overlap; small $H(x_i)$ occurs when a single component dominates.\n\nFor each test case below, compute the fraction of points whose entropy $H(x_i)$ exceeds a given threshold $\\tau$. Express this fraction as a decimal (not a percentage). Your program must implement responsibilities via Bayes' theorem using the multivariate normal PDF as stated, compute entropies with the natural logarithm, and aggregate the final results across test cases.\n\nTest suite specification:\n- Case $1$ (one-dimensional, well-separated two components):\n  - $d = 1$, $K = 2$, $\\pi = [0.5, 0.5]$.\n  - $\\mu_1 = -3$, $\\mu_2 = 3$.\n  - $\\Sigma_1 = [0.5]$, $\\Sigma_2 = [0.5]$.\n  - $x_i \\in \\{-4, -3, -2, 2, 3, 4\\}$.\n  - Threshold $\\tau = 0.3$.\n- Case $2$ (one-dimensional, moderately overlapping two components):\n  - $d = 1$, $K = 2$, $\\pi = [0.5, 0.5]$.\n  - $\\mu_1 = 0$, $\\mu_2 = 1$.\n  - $\\Sigma_1 = [1]$, $\\Sigma_2 = [1]$.\n  - $x_i \\in \\{-0.2, 0.5, 0.8, 1.5\\}$.\n  - Threshold $\\tau = 0.5$.\n- Case $3$ (two-dimensional, overlap along one axis):\n  - $d = 2$, $K = 2$, $\\pi = [0.5, 0.5]$.\n  - $\\mu_1 = [0, 0]$, $\\mu_2 = [3, 0]$.\n  - $\\Sigma_1 = \\mathrm{diag}([1, 4])$, $\\Sigma_2 = \\mathrm{diag}([1, 4])$.\n  - $x_i \\in \\{[0, 0], [1.5, 0], [3, 0], [1.5, 2], [1.5, -2]\\}$.\n  - Threshold $\\tau = 0.4$.\n- Case $4$ (two-dimensional, uniform responsibilities as a boundary condition with identical components):\n  - $d = 2$, $K = 3$, $\\pi = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$.\n  - $\\mu_1 = [0, 0]$, $\\mu_2 = [0, 0]$, $\\mu_3 = [0, 0]$.\n  - $\\Sigma_1 = I$, $\\Sigma_2 = I$, $\\Sigma_3 = I$, where $I$ is the $2 \\times 2$ identity matrix.\n  - $x_i \\in \\{[2, 2], [0, 0], [-1, 1], [3, -3]\\}$.\n  - Threshold $\\tau = 1.0$.\n\nOutput requirement:\n- For each case, compute the fraction of points with $H(x_i) > \\tau$.\n- Your program should produce a single line of output containing these four fractions as a comma-separated list enclosed in square brackets (for example, $[r_1, r_2, r_3, r_4]$), where each $r_j$ is a decimal number.\n\nAssumptions and constraints:\n- Use the natural logarithm for all $\\log$ computations.\n- Ensure numerical stability by avoiding $\\log 0$ in entropy calculations; this may be handled by clipping responsibilities away from $0$ with a small positive constant.\n- No physical units are involved.", "solution": "The problem requires the implementation of a function to calculate the assignment uncertainty for data points in a Gaussian Mixture Model (GMM), quantified by the Shannon entropy of the component responsibilities. The solution must be derived from first principles, including Bayes' theorem and the definition of the multivariate normal distribution.\n\nFirst, we establish the theoretical foundation. A GMM is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. For a GMM with $K$ components in a $d$-dimensional space, the model is defined by the mixing proportions $\\pi_k$, the component means $\\mu_k \\in \\mathbb{R}^d$, and the component covariance matrices $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$ for $k \\in \\{1, \\dots, K\\}$. The mixing proportions are non-negative and sum to one: $\\sum_{k=1}^K \\pi_k = 1$. Each covariance matrix $\\Sigma_k$ is required to be positive-definite.\n\nThe probability density function (PDF) for a single multivariate normal component $k$ at a point $x \\in \\mathbb{R}^d$ is given by:\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)\\right)\n$$\nHere, $|\\Sigma_k|$ is the determinant of the covariance matrix $\\Sigma_k$, and the term $(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)$ is the squared Mahalanobis distance between the point $x$ and the mean $\\mu_k$.\n\nThe core of the problem is to determine, for each data point $x_i$, the posterior probability that it was generated by component $k$. This posterior probability is known as the responsibility, denoted $r_{ik}$. Using Bayes' theorem, the responsibility is calculated as:\n$$\nr_{ik} = P(k \\mid x_i) = \\frac{P(x_i \\mid k) P(k)}{P(x_i)} = \\frac{\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n$$\nIn this expression, $\\pi_k$ acts as the prior probability $P(k)$ of component $k$, and $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ is the likelihood $P(x_i \\mid k)$. The denominator, $\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)$, is the marginal probability of the data point $x_i$, often called the evidence, which acts as a normalization constant ensuring that $\\sum_{k=1}^K r_{ik} = 1$.\n\nDirect computation of the likelihoods and the evidence can be numerically unstable, especially in high dimensions or when the exponential term leads to underflow. A standard and robust technique is to work with log-probabilities. Let's define the log of the weighted likelihood for point $x_i$ and component $k$ as $L_{ik}$:\n$$\nL_{ik} = \\log\\left(\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right) = \\log(\\pi_k) + \\log\\left(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right)\n$$\nThe log-PDF is:\n$$\n\\log\\left(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma_k| - \\frac{1}{2}(x_i-\\mu_k)^\\top \\Sigma_k^{-1} (x_i-\\mu_k)\n$$\nThe responsibilities can be expressed in terms of $L_{ik}$ as $r_{ik} = \\frac{\\exp(L_{ik})}{\\sum_{j=1}^K \\exp(L_{ij})}$. To prevent numerical overflow/underflow in the exponential function, we use the log-sum-exp trick. Let $L_{\\text{max}, i} = \\max_{j} L_{ij}$. Then,\n$$\nr_{ik} = \\frac{\\exp(L_{ik} - L_{\\text{max}, i})}{\\sum_{j=1}^K \\exp(L_{ij} - L_{\\text{max}, i})}\n$$\nThis computation is numerically stable, as the largest exponent is now $0$.\n\nOnce the responsibilities $\\{r_{ik}\\}_{k=1}^K$ are computed for a data point $x_i$, they form a discrete probability distribution over the components. The uncertainty of this assignment is quantified by the Shannon entropy, using the natural logarithm:\n$$\nH(x_i) = -\\sum_{k=1}^K r_{ik} \\log r_{ik}\n$$\nThe entropy is maximized when the responsibilities are uniform (e.g., $r_{ik} = 1/K$ for all $k$), indicating high uncertainty. It is minimized (to $0$) when one component has a responsibility of $1$ and all others are $0$, indicating a certain assignment. In the computation, the term $r_{ik} \\log r_{ik}$ is taken to be $0$ if $r_{ik}=0$, corresponding to the limit $\\lim_{p \\to 0^+} p \\log p = 0$.\n\nThe final step is to process each of the given test cases. For each case, we iterate through all provided data points $\\{x_i\\}$. For each point, we compute its entropy $H(x_i)$. We then count how many points have an entropy greater than the specified threshold $\\tau$. The result for the case is the fraction of such points relative to the total number of points.\n\nThe implementation will follow this logic. A function will be created to compute the entropies for a set of points given the GMM parameters. This function will calculate the log-weighted-likelihoods for each point and component, apply the log-sum-exp trick to find the responsibilities, and then compute the Shannon entropy for each point's responsibility distribution. The main script will then use this function for each test case and calculate the required fraction.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes assignment uncertainty in GMM clustering using Shannon entropy.\n    \"\"\"\n\n    def compute_entropies(X, pis, mus, sigmas):\n        \"\"\"\n        Computes the Shannon entropy of responsibilities for each data point.\n\n        Args:\n            X (np.ndarray): Data points, shape (N, d).\n            pis (np.ndarray): Mixing proportions, shape (K,).\n            mus (np.ndarray): Component means, shape (K, d).\n            sigmas (np.ndarray): Component covariance matrices, shape (K, d, d).\n\n        Returns:\n            np.ndarray: Shannon entropy for each data point, shape (N,).\n        \"\"\"\n        N, d = X.shape\n        K = len(pis)\n\n        # log_weighted_likelihoods will store log(pi_k * N(x_i | mu_k, sigma_k))\n        log_weighted_likelihoods = np.zeros((N, K))\n\n        for k in range(K):\n            pi_k = pis[k]\n            mu_k = mus[k]\n            sigma_k = sigmas[k]\n            \n            # Pre-compute inverse and log-determinant of covariance matrix\n            inv_sigma_k = np.linalg.inv(sigma_k)\n            det_sigma_k = np.linalg.det(sigma_k)\n            log_det_sigma_k = np.log(det_sigma_k)\n\n            # Log of the normalization constant of the multivariate normal PDF\n            log_norm_const = -0.5 * (d * np.log(2. * np.pi) + log_det_sigma_k)\n\n            # Compute Mahalanobis distance for all points to the k-th center\n            X_minus_mu = X - mu_k\n            # (X_minus_mu @ inv_sigma_k) gives shape (N, d)\n            # The element-wise product followed by sum is equivalent to (x-mu)T.inv(S).(x-mu) for each row\n            mahalanobis_sq = np.sum((X_minus_mu @ inv_sigma_k) * X_minus_mu, axis=1)\n\n            # Log-likelihood for each point for component k\n            log_pdf = log_norm_const - 0.5 * mahalanobis_sq\n            \n            # Log of the joint probability P(x, k) = P(x|k)P(k)\n            log_weighted_likelihoods[:, k] = np.log(pi_k) + log_pdf\n\n        # Log-sum-exp trick for numerical stability to compute responsibilities\n        log_sum_exp_offset = np.max(log_weighted_likelihoods, axis=1, keepdims=True)\n        exp_terms = np.exp(log_weighted_likelihoods - log_sum_exp_offset)\n        sum_exp_terms = np.sum(exp_terms, axis=1, keepdims=True)\n        \n        responsibilities = exp_terms / sum_exp_terms\n        \n        # Compute Shannon entropy, handling the case where r_ik = 0.\n        # The term r*log(r) - 0 as r - 0.\n        # np.where is used to avoid log(0) which results in -inf, and 0*-inf = nan.\n        entropy_terms = np.where(responsibilities  0, responsibilities * np.log(responsibilities), 0)\n        entropies = -np.sum(entropy_terms, axis=1)\n        \n        return entropies\n\n    test_cases = [\n        # Case 1 (one-dimensional, well-separated)\n        {\n            \"points\": np.array([[-4.], [-3.], [-2.], [2.], [3.], [4.]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[-3.], [3.]]),\n            \"sigmas\": np.array([[[0.5]], [[0.5]]]),\n            \"tau\": 0.3\n        },\n        # Case 2 (one-dimensional, overlapping)\n        {\n            \"points\": np.array([[-0.2], [0.5], [0.8], [1.5]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[0.], [1.]]),\n            \"sigmas\": np.array([[[1.]], [[1.]]]),\n            \"tau\": 0.5\n        },\n        # Case 3 (two-dimensional, overlap along one axis)\n        {\n            \"points\": np.array([[0., 0.], [1.5, 0.], [3., 0.], [1.5, 2.], [1.5, -2.]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[0., 0.], [3., 0.]]),\n            \"sigmas\": np.array([[[1., 0.], [0., 4.]], [[1., 0.], [0., 4.]]]),\n            \"tau\": 0.4\n        },\n        # Case 4 (two-dimensional, identical components)\n        {\n            \"points\": np.array([[2., 2.], [0., 0.], [-1., 1.], [3., -3.]]),\n            \"pis\": np.array([1./3., 1./3., 1./3.]),\n            \"mus\": np.array([[0., 0.], [0., 0.], [0., 0.]]),\n            \"sigmas\": np.array([[[1., 0.], [0., 1.]], [[1., 0.], [0., 1.]], [[1., 0.], [0., 1.]]]),\n            \"tau\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"points\"]\n        pis = case[\"pis\"]\n        mus = case[\"mus\"]\n        sigmas = case[\"sigmas\"]\n        tau = case[\"tau\"]\n\n        entropies = compute_entropies(X, pis, mus, sigmas)\n        \n        num_points_above_threshold = np.sum(entropies  tau)\n        fraction = num_points_above_threshold / len(X)\n        results.append(fraction)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3122627"}, {"introduction": "Perhaps the most critical and challenging question in clustering is determining the \"correct\" number of clusters. This advanced practice [@problem_id:3122550] tackles this head-on by simulating a difficult scenario: distinguishing two very close clusters from a single one. You will use the Bayesian Information Criterion (BIC), a standard tool for model selection, to explore the crucial relationship between cluster separation, model complexity, and the amount of data needed for a reliable conclusion.", "problem": "You are asked to design and implement a complete, runnable program that empirically quantifies the sample size needed for reliable separation of two closely spaced components in a Gaussian mixture model using the Bayesian Information Criterion (BIC). The context is statistical learning, with a focus on Gaussian mixture models for clustering. The fundamental base to be used in your derivation and algorithmic design is maximum likelihood estimation for Gaussian models and the Bayesian Information Criterion as a large-sample approximation to the log marginal likelihood.\n\nConsider a two-component mixture of multivariate Gaussian distributions in dimension $d$, with parameters component means $\\mu_1,\\mu_2 \\in \\mathbb{R}^d$, component covariance matrices $\\Sigma_1,\\Sigma_2 \\in \\mathbb{R}^{d \\times d}$ that are positive definite, and mixing weights $\\pi_1,\\pi_2 \\in (0,1)$ with $\\pi_1 + \\pi_2 = 1$. The data consist of $n$ independent draws from this mixture. The components are nearly identical in the sense that the Euclidean distance $\\|\\mu_1 - \\mu_2\\|$ is much smaller than the dominant scale of the covariance, quantified by $\\sqrt{\\lambda_{\\max}(\\Sigma)}$, where $\\lambda_{\\max}(\\Sigma)$ denotes the largest eigenvalue of a covariance matrix.\n\nYour program must perform the following tasks for each test case:\n\n1. Simulate $n$ independent samples in $\\mathbb{R}^d$ from a two-component Gaussian mixture with equal mixing weights $\\pi_1 = \\pi_2 = \\frac{1}{2}$, identical covariance matrices $\\Sigma_1 = \\Sigma_2 = \\Sigma$, and means separated along the first coordinate axis as $\\mu_1 = \\left(-\\frac{\\delta}{2}, 0, \\dots, 0\\right)$ and $\\mu_2 = \\left(\\frac{\\delta}{2}, 0, \\dots, 0\\right)$, where $\\delta = \\|\\mu_1 - \\mu_2\\|$ is the separation magnitude.\n\n2. Fit two competing models to each simulated dataset by maximum likelihood:\n   - A single multivariate Gaussian model with mean $\\mu$ and covariance $\\Sigma$, estimated from the full dataset.\n   - A two-component Gaussian mixture model with parameters $(\\pi_1,\\pi_2,\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2)$ estimated using the Expectation-Maximization (EM) algorithm.\n\n3. Compute the Bayesian Information Criterion (BIC) for each fitted model. For a model with maximized log-likelihood $\\ell(\\hat{\\theta})$ and $p$ free parameters fitted to $n$ observations, the BIC is defined as $ \\mathrm{BIC} = -2 \\, \\ell(\\hat{\\theta}) + p \\, \\log(n) $. For the single Gaussian in dimension $d$, the parameter count is $p_{\\text{single}} = d + \\frac{d(d+1)}{2}$. For a two-component full-covariance Gaussian mixture in dimension $d$, the parameter count is $p_{\\text{mix}} = 2d + 2 \\cdot \\frac{d(d+1)}{2} + (2-1) = 2d + d(d+1) + 1$.\n\n4. Define a \"win\" for the two-component model if its BIC is strictly smaller than the single-Gaussian BIC, i.e., $\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$. For a given $n$, repeat the simulation and fitting $T$ times to estimate the fraction of wins. If the fraction of wins is at least the reliability threshold $\\tau$ (expressed as a decimal), declare that $n$ achieves reliable separation for the given test case.\n\n5. For each test case, search over a fixed candidate set of sample sizes to find the minimal $n$ that achieves reliable separation. If no candidate achieves reliable separation, return $-1$ for that test case.\n\nUse the following test suite to probe different degrees of component proximity and to cover a range from very hard to easier separation regimes. All quantities must be treated exactly as specified.\n\n- Common settings for all cases: dimension $d = 2$, covariance matrix $\\Sigma = s^2 I_2$ with $s = 1.0$ so that $\\sqrt{\\lambda_{\\max}(\\Sigma)} = s = 1.0$, equal mixing weights $\\pi_1 = \\pi_2 = \\frac{1}{2}$, number of trials $T = 12$, reliability threshold $\\tau = 0.75$, candidate sample sizes $n \\in \\{60, 120, 240, 360\\}$.\n- Case A (very small separation): $\\delta = 0.10$.\n- Case B (small separation): $\\delta = 0.20$.\n- Case C (moderate separation relative to $s$): $\\delta = 0.30$.\n\nYour program must implement a statistically sound Expectation-Maximization procedure for the two-component mixture, robustly compute log-likelihoods for both models, apply the parameter counts above to compute BIC, and perform the repeated-trial reliability assessment. Numerical stability measures such as small diagonal regularization added to covariance estimates are permitted. No external data or user input is allowed.\n\nFinal output format: Your program should produce a single line of output containing the minimal sample sizes for reliable separation for the three cases as a comma-separated list enclosed in square brackets, for example, $[n_A,n_B,n_C]$, where each $n_\\cdot$ is an integer from the candidate set or $-1$ if no candidate satisfies the reliability threshold.", "solution": "The problem requires an empirical investigation into the sample size $n$ needed to reliably distinguish a two-component Gaussian Mixture Model (GMM) from a simpler single-component Gaussian model using the Bayesian Information Criterion (BIC). The components of the mixture are specified to be closely spaced, making the model selection task non-trivial.\n\nThe methodology involves a series of computational experiments. For each test case, defined by a specific separation distance $\\delta$ between the component means, we search for the minimum sample size from a given set of candidates $\\{60, 120, 240, 360\\}$ that allows the more complex two-component model to be favored by the BIC with a reliability of at least $\\tau = 0.75$ over $T=12$ independent trials.\n\n**1. Data Simulation**\n\nFor each trial, a dataset of $n$ points is simulated in a $d=2$ dimensional space. The data are drawn from a two-component GMM with the following parameters:\n- Mixing weights: $\\pi_1 = \\pi_2 = 0.5$.\n- Component means: $\\mu_1 = \\left(-\\frac{\\delta}{2}, 0\\right)^T$ and $\\mu_2 = \\left(\\frac{\\delta}{2}, 0\\right)^T$. The parameter $\\delta$ controls the separation between the components.\n- Component covariances: $\\Sigma_1 = \\Sigma_2 = \\Sigma = s^2 I_2$, where $s=1.0$ and $I_2$ is the $2 \\times 2$ identity matrix. This implies isotropic and identical spherical components.\n\nTo generate a sample of size $n$, approximately $n/2$ points are drawn from the Gaussian distribution $\\mathcal{N}(\\mu_1, \\Sigma)$ and $n/2$ points are drawn from $\\mathcal{N}(\\mu_2, \\Sigma)$. The exact number of points from each component in any given trial is determined by $n$ independent Bernoulli draws with probability $0.5$.\n\n**2. Model Fitting and Evaluation**\n\nTwo models are fitted to each simulated dataset using Maximum Likelihood Estimation (MLE).\n\n**Model A: Single Multivariate Gaussian**\nA single Gaussian distribution, $\\mathcal{N}(\\mu, \\Sigma_{\\text{single}})$, is fitted to the entire dataset of $n$ points. The MLE parameters are the sample mean $\\hat{\\mu}$ and the sample covariance matrix $\\hat{\\Sigma}_{\\text{single}}$:\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n$$\n$$\n\\hat{\\Sigma}_{\\text{single}} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^T\n$$\nThe maximized log-likelihood, $\\ell_{\\text{single}}$, for this model can be calculated using the closed-form expression:\n$$\n\\ell_{\\text{single}} = -\\frac{n}{2} \\left( d \\log(2\\pi) + \\log|\\det(\\hat{\\Sigma}_{\\text{single}})| + d \\right)\n$$\nThe number of free parameters for this model in dimension $d=2$ is $p_{\\text{single}} = d + \\frac{d(d+1)}{2} = 2 + \\frac{2(3)}{2} = 5$.\n\n**Model B: Two-Component Gaussian Mixture Model**\nA two-component GMM is fitted to the data. The parameters $\\theta = \\{\\pi_1, \\pi_2, \\mu_1, \\mu_2, \\Sigma_1, \\Sigma_2\\}$ are estimated using the Expectation-Maximization (EM) algorithm, as there is no closed-form solution for the MLE.\n\n- **Initialization**: The EM algorithm's performance is sensitive to initial parameter choices. To obtain a robust initialization, a simple k-means clustering algorithm ($k=2$) is run for a few iterations. The resulting cluster means, proportions, and covariances are used as the starting point for the EM algorithm.\n\n- **Expectation-Step (E-Step)**: Given the current parameter estimates $\\theta^{(t)}$, the posterior probability (or responsibility) that data point $x_i$ was generated by component $k$ is calculated:\n$$\n\\gamma_{ik}^{(t+1)} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_i | \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(x_i | \\mu_j^{(t)}, \\Sigma_j^{(t)})}\n$$\n\n- **Maximization-Step (M-Step)**: The parameters are updated to maximize the expected log-likelihood, using the computed responsibilities:\n$$\nN_k^{(t+1)} = \\sum_{i=1}^n \\gamma_{ik}^{(t+1)}\n$$\n$$\n\\pi_k^{(t+1)} = \\frac{N_k^{(t+1)}}{n}\n$$\n$$\n\\mu_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^n \\gamma_{ik}^{(t+1)} x_i\n$$\n$$\n\\Sigma_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^n \\gamma_{ik}^{(t+1)} (x_i - \\mu_k^{(t+1)})(x_i - \\mu_k^{(t+1)})^T\n$$\nTo ensure numerical stability and prevent singularities, a small regularization term ($\\epsilon I_d$ with $\\epsilon = 10^{-6}$) is added to the covariance matrices after each update.\n\n- **Convergence**: The E-step and M-step are iterated until the change in the total log-likelihood, $\\ell_{\\text{mix}} = \\sum_{i=1}^n \\log\\left( \\sum_{k=1}^2 \\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\right)$, falls below a small tolerance.\n\nThe number of free parameters for this full-covariance GMM in dimension $d=2$ is $p_{\\text{mix}} = 2d + 2 \\cdot \\frac{d(d+1)}{2} + (2-1) = 4 + 6 + 1 = 11$.\n\n**3. Model Selection via BIC**\n\nThe Bayesian Information Criterion is used to compare the two fitted models. For a model with $p$ parameters and maximized log-likelihood $\\ell$, the BIC is:\n$$\n\\mathrm{BIC} = -2 \\ell + p \\log(n)\n$$\nThe model with the lower BIC is preferred. Thus, the two-component GMM is considered a \"win\" if $\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$. This condition can be expressed as a lower bound on the log-likelihood gain:\n$$\n2(\\ell_{\\text{mix}} - \\ell_{\\text{single}}) > (p_{\\text{mix}} - p_{\\text{single}})\\log(n)\n$$\nThe right-hand side represents the penalty for the added complexity of the GMM, which has $\\Delta p = p_{\\text{mix}} - p_{\\text{single}} = 6$ more parameters than the single Gaussian model.\n\n**4. Empirical Search for Minimal Sample Size**\n\nFor each test case (i.e., each value of $\\delta$), the program iterates through the candidate sample sizes $n \\in \\{60, 120, 240, 360\\}$ in ascending order. For each $n$, it performs $T=12$ simulation-and-fitting trials. It counts the number of times the GMM wins (i.e., $\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$). If the fraction of wins is at least the reliability threshold $\\tau=0.75$ (i.e., at least $9$ out of $12$ wins), that sample size $n$ is deemed sufficient for reliable separation. The program returns the first (and therefore minimal) $n$ that meets this criterion. If no candidate $n$ meets the threshold, it returns $-1$. This entire procedure is repeated for each of the three specified values of $\\delta$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import logsumexp\n\ndef fit_gmm_em(data, d, k, max_iter=100, tol=1e-4):\n    \"\"\"\n    Fits a k-component Gaussian Mixture Model using the EM algorithm.\n    \"\"\"\n    n, _ = data.shape\n    reg_cov = 1e-6\n\n    # Initialization using k-means\n    try:\n        # 1. Randomly initialize centers\n        centers = data[np.random.choice(n, k, replace=False)]\n        \n        # 2. A few k-means iterations\n        for _ in range(10):\n            distances = np.linalg.norm(data[:, np.newaxis, :] - centers[np.newaxis, :, :], axis=2)\n            labels = np.argmin(distances, axis=1)\n            \n            new_centers = np.array([data[labels == j].mean(axis=0) if np.sum(labels == j) > 0 else centers[j] for j in range(k)])\n\n            if np.allclose(centers, new_centers):\n                break\n            centers = new_centers\n\n        mus = centers\n        labels = np.argmin(np.linalg.norm(data[:, np.newaxis, :] - mus, axis=2), axis=1) # final assignment\n        \n        pis = np.array([np.mean(labels == j) for j in range(k)])\n        \n        sigmas = []\n        for j in range(k):\n            cluster_points = data[labels == j]\n            if len(cluster_points)  d:\n                # Not enough points, use global covariance\n                cov_j = np.cov(data, rowvar=False, bias=True)\n            else:\n                cov_j = np.cov(cluster_points, rowvar=False, bias=True)\n            sigmas.append(cov_j + reg_cov * np.identity(d))\n        sigmas = np.array(sigmas)\n\n    except (ValueError, np.linalg.LinAlgError):\n        return -np.inf\n\n    ll_old = -np.inf\n    for _ in range(max_iter):\n        # E-step\n        try:\n            log_probs = np.zeros((n, k))\n            for j in range(k):\n                log_probs[:, j] = np.log(pis[j]) + multivariate_normal.logpdf(data, mean=mus[j], cov=sigmas[j])\n        except (ValueError, np.linalg.LinAlgError):\n            return -np.inf\n\n        log_sum_probs = logsumexp(log_probs, axis=1)\n        log_responsibilities = log_probs - log_sum_probs[:, np.newaxis]\n        responsibilities = np.exp(log_responsibilities)\n\n        # M-step\n        nk = np.sum(responsibilities, axis=0)\n        \n        # Check for collapsed components\n        if np.any(nk  1e-9):\n            return -np.inf\n\n        pis = nk / n\n        mus = np.dot(responsibilities.T, data) / nk[:, np.newaxis]\n        \n        for j in range(k):\n            diff = data - mus[j]\n            weighted_diff = diff * np.sqrt(responsibilities[:, j])[:, np.newaxis]\n            sigmas[j] = (weighted_diff.T @ weighted_diff) / nk[j] + reg_cov * np.identity(d)\n        \n        # Convergence check\n        ll_new = np.sum(log_sum_probs)\n        if ll_new - ll_old  tol and ll_new > -np.inf:\n            break\n        ll_old = ll_new\n\n    if not np.isfinite(ll_old):\n        return -np.inf\n        \n    return ll_old\n\ndef log_likelihood_single_gaussian(data, d):\n    \"\"\"\n    Computes the maximized log-likelihood for a single Gaussian model.\n    \"\"\"\n    n, _ = data.shape\n    if n  2:\n        return -np.inf\n    \n    mean = np.mean(data, axis=0)\n    cov = np.cov(data, rowvar=False, bias=True)\n    cov += 1e-6 * np.identity(d) # Regularization\n\n    try:\n        sign, logdet = np.linalg.slogdet(cov)\n        if sign = 0:\n            return -np.inf\n    except np.linalg.LinAlgError:\n        return -np.inf\n\n    ll = -0.5 * n * (d * np.log(2 * np.pi) + logdet + d)\n    return ll\n\ndef find_minimal_n(delta, d, cov_matrix, T, tau, n_candidates):\n    \"\"\"\n    Searches for the minimal sample size n that achieves reliable separation.\n    \"\"\"\n    mu1 = np.zeros(d)\n    mu1[0] = -delta / 2\n    mu2 = np.zeros(d)\n    mu2[0] = delta / 2\n    \n    p_single = d + d * (d + 1) / 2\n    p_mix = 2 * d + d * (d + 1) + 1 # for k=2 components\n\n    for n in sorted(n_candidates):\n        win_count = 0\n        for _ in range(T):\n            # 1. Simulate data\n            n1 = np.random.binomial(n, 0.5)\n            n2 = n - n1\n            data1 = np.random.multivariate_normal(mu1, cov_matrix, size=n1)\n            data2 = np.random.multivariate_normal(mu2, cov_matrix, size=n2)\n            data = np.vstack((data1, data2)) if n1 > 0 and n2 > 0 else (data1 if n1 > 0 else data2)\n            np.random.shuffle(data)\n\n            # 2. Fit single Gaussian model\n            ll_single = log_likelihood_single_gaussian(data, d)\n            if not np.isfinite(ll_single):\n                continue\n            bic_single = -2 * ll_single + p_single * np.log(n)\n\n            # 3. Fit two-component GMM\n            ll_mix = fit_gmm_em(data, d, k=2)\n            if not np.isfinite(ll_mix):\n                continue\n            bic_mix = -2 * ll_mix + p_mix * np.log(n)\n\n            # 4. Compare BIC\n            if bic_mix  bic_single:\n                win_count += 1\n        \n        # 5. Check reliability\n        win_fraction = win_count / T\n        if win_fraction >= tau:\n            return n\n            \n    return -1\n\ndef solve():\n    # Set a seed for reproducibility.\n    np.random.seed(42)\n\n    # Common settings from problem statement\n    d = 2\n    s = 1.0\n    cov_matrix = np.identity(d) * (s**2)\n    T = 12\n    tau = 0.75\n    n_candidates = [60, 120, 240, 360]\n    \n    # Test cases\n    test_cases = [\n        {'name': 'Case A', 'delta': 0.10},\n        {'name': 'Case B', 'delta': 0.20},\n        {'name': 'Case C', 'delta': 0.30},\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = case['delta']\n        min_n = find_minimal_n(delta, d, cov_matrix, T, tau, n_candidates)\n        results.append(min_n)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3122550"}]}