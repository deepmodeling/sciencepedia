{"hands_on_practices": [{"introduction": "The F-test provides a formal way to determine if a group of predictors contributes significantly to a model. At its core, the F-statistic quantifies the trade-off between increased model complexity and improved fit. This exercise grounds your understanding in this fundamental principle by asking you to reconstruct the F-statistic from its building blocks: the residual sum of squares (RSS) from a full and a reduced model, demonstrating how the test measures the reduction in error achieved by adding new variables. [@problem_id:3130377]", "problem": "A data analyst fits two nested multiple linear regression models to the same data set with sample size $n=100$ for a response $Y$ and predictors $X_{1}, X_{2}, X_{3}, X_{4}, X_{5}$. The full model is $Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\beta_{3}X_{3}+\\beta_{4}X_{4}+\\beta_{5}X_{5}+\\varepsilon$. The reduced model imposes the joint null hypothesis $H_{0}:\\beta_{2}=\\beta_{3}=0$ and thus includes only $X_{1}, X_{4}, X_{5}$ along with the intercept.\n\nStatistical software reports the following for the two fits:\n- Full model residual degrees of freedom: $94$; residual sum of squares: $1800.0$.\n- Reduced model residual degrees of freedom: $96$; residual sum of squares: $1933.2$.\n- For the joint test that removes $X_{2}$ and $X_{3}$, the software displays a $p$-value of $3.46\\times 10^{-2}$.\n\nUsing only foundational definitions from linear regression and hypothesis testing, reconstruct the partial $F$ statistic for testing $H_{0}:\\beta_{2}=\\beta_{3}=0$ by combining the given sums of squares and degrees of freedom. Then, use the defining property of the $F$ distribution under $H_{0}$ to check whether your computed statistic is consistent with the reported $p$-value. Your final submitted answer must be the value of the $F$ statistic. Round your answer to four significant figures.", "solution": "The problem requires the calculation of a partial $F$ statistic for testing a joint null hypothesis on a subset of coefficients in a multiple linear regression model. The problem is first validated for correctness and consistency.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Sample size: $n=100$\n- Full model: $Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\beta_{3}X_{3}+\\beta_{4}X_{4}+\\beta_{5}X_{5}+\\varepsilon$\n- Reduced model corresponds to the null hypothesis $H_{0}:\\beta_{2}=\\beta_{3}=0$.\n- Full model residual sum of squares: $RSS_{F} = 1800.0$\n- Full model residual degrees of freedom: $df_{F} = 94$\n- Reduced model residual sum of squares: $RSS_{R} = 1933.2$\n- Reduced model residual degrees of freedom: $df_{R} = 96$\n- Reported $p$-value for the joint test: $3.46\\times 10^{-2}$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with standard hypothesis testing in linear regression. The provided data is checked for internal consistency.\nThe full model has $p=5$ predictors. The number of estimated parameters is $p+1=6$ (including the intercept $\\beta_0$). The residual degrees of freedom for the full model is correctly given as $df_{F} = n-(p+1) = 100 - (5+1) = 94$.\nThe reduced model excludes $X_2$ and $X_3$, leaving $3$ predictors. The number of estimated parameters is $3+1=4$. The residual degrees of freedom for the reduced model is correctly given as $df_{R} = n-(3+1) = 100 - 4 = 96$.\nThe null hypothesis $H_{0}:\\beta_{2}=\\beta_{3}=0$ involves restricting $q=2$ parameters. This is consistent with the difference in degrees of freedom: $q = df_{R} - df_{F} = 96 - 94 = 2$.\nThe residual sum of squares must be greater for the reduced (more constrained) model than for the full model. The given values $RSS_{R} = 1933.2 > RSS_{F} = 1800.0$ adhere to this principle.\nAll provided information is self-contained, consistent, and adheres to the principles of linear regression analysis. The problem is therefore valid.\n\n### Solution\n\nThe partial $F$ statistic is used to test the joint significance of a subset of predictors by comparing a full model to a reduced (nested) model. The null hypothesis, $H_{0}$, posits that the coefficients for a subset of predictors are all zero. The alternative hypothesis, $H_{A}$, is that at least one of these coefficients is non-zero.\n\nThe formula for the $F$ statistic is:\n$$ F = \\frac{(RSS_{R} - RSS_{F}) / q}{RSS_{F} / df_{F}} $$\nwhere:\n- $RSS_{R}$ is the residual sum of squares for the reduced model.\n- $RSS_{F}$ is the residual sum of squares for the full model.\n- $q$ is the number of coefficients being tested (i.e., the number of predictors excluded from the full model to create the reduced model).\n- $df_{F}$ is the residual degrees of freedom for the full model.\n\nFrom the problem statement, we have:\n- $RSS_{R} = 1933.2$\n- $RSS_{F} = 1800.0$\n- $df_{F} = 94$\n\nThe number of restrictions under the null hypothesis $H_{0}:\\beta_{2}=\\beta_{3}=0$ is $q=2$. This is also confirmed by the difference in the residual degrees of freedom between the two models: $q = df_{R} - df_{F} = 96 - 94 = 2$.\n\nSubstituting the given values into the formula for the $F$ statistic:\n$$ F = \\frac{(1933.2 - 1800.0) / 2}{1800.0 / 94} $$\nFirst, we calculate the numerator, which is the change in the residual sum of squares per restriction:\n$$ \\frac{RSS_{R} - RSS_{F}}{q} = \\frac{133.2}{2} = 66.6 $$\nNext, we calculate the denominator, which is the residual mean square (or unbiased variance estimate, $\\hat{\\sigma}^2$) from the full model:\n$$ \\frac{RSS_{F}}{df_{F}} = \\frac{1800.0}{94} \\approx 19.148936 $$\nNow, we compute the $F$ statistic:\n$$ F = \\frac{66.6}{1800.0 / 94} = \\frac{66.6 \\times 94}{1800.0} = \\frac{6260.4}{1800.0} = 3.478 $$\nRounding the result to four significant figures gives $3.478$.\n\nTo check for consistency, we consider the distribution of this statistic under the null hypothesis. The $F$ statistic follows an $F$ distribution with $q$ and $df_{F}$ degrees of freedom, which in this case is $F_{2, 94}$. The $p$-value is the probability of observing a value from this distribution that is greater than or equal to our calculated statistic: $p\\text{-value} = P(F_{2, 94} \\geq 3.478)$.\nUsing statistical software or a sufficiently detailed $F$-table, one can verify that the critical value for an $F_{2, 94}$ distribution at a significance level $\\alpha=0.05$ is approximately $3.10$. Since our calculated statistic $F = 3.478$ is greater than this critical value, the $p$-value must be less than $0.05$. The provided $p$-value of $3.46 \\times 10^{-2} = 0.0346$ is indeed less than $0.05$, confirming that our calculated $F$ statistic is consistent with the software output. A more precise calculation reveals that $P(F_{2, 94} \\geq 3.478) \\approx 0.0346$, which matches the given $p$-value. Thus, the calculation is correct.\n\nThe value of the $F$ statistic, rounded to four significant figures, is $3.478$.", "answer": "$$\\boxed{3.478}$$", "id": "3130377"}, {"introduction": "When we include a categorical variable with multiple levels in a regression, we represent it with a set of indicator variables. A common point of confusion is that the coefficients and p-values for these indicators change if we alter the reference category. This practice clarifies that while individual parameter estimates are relative to the chosen baseline, the F-test for the joint significance of all indicators is invariant, providing a stable and definitive assessment of the variable's overall predictive power. [@problem_id:3130441]", "problem": "A dataset with $n=60$ observations is analyzed using Ordinary Least Squares (OLS). The outcome $Y$ is modeled as a linear function of a centered continuous covariate $X$ (with an intercept) and a categorical factor $G$ with three levels $A$, $B$, and $C$. The factor $G$ is represented by two indicator variables in a full model that includes the intercept, $X$, and the indicators. A reduced model includes only the intercept and $X$.\n\nTwo equivalent encodings of $G$ are used:\n\n- Scheme S1 (reference $A$): indicators $I_{B}$ and $I_{C}$ for levels $B$ and $C$, respectively.\n- Scheme S2 (reference $C$): indicators $J_{A}$ and $J_{B}$ for levels $A$ and $B$, respectively.\n\nYou are given the following information from the model fits:\n\n- The reduced model (intercept and $X$ only) yields residual sum of squares $RSS_{R}=820$ and estimates $p_{R}=2$ parameters.\n- The full model (intercept, $X$, and two indicators for $G$) yields residual sum of squares $RSS_{F}=760$ and estimates $p_{F}=4$ parameters.\n- Under S1, the OLS estimates and their standard errors for the two group indicators are $\\hat{\\beta}_{B}=-3.0$ with standard error $1.8$ and $\\hat{\\beta}_{C}=-1.5$ with standard error $1.9$.\n- Under S2, the OLS estimates and their standard errors for the two group indicators are $\\hat{\\gamma}_{A}=+1.5$ with standard error $1.6$ and $\\hat{\\gamma}_{B}=-1.5$ with standard error $1.7$.\n\nTasks:\n\n1) Using only the definitions from the general linear model, determine whether changing the reference level for $G$ can alter the fitted values or the residual sum of squares of the full model, and explain what this implies for the joint test of $G$ as a block.\n\n2) Compute the individual $t$-statistics for the group indicators under S1 and S2 and state whether they changed across encodings.\n\n3) Compute the common value of the $F$-statistic for testing the null hypothesis that $G$ has no effect given the intercept and $X$ (that is, testing the two indicator coefficients jointly equal to zero). Round your final numeric answer to four significant figures.", "solution": "The problem is validated as self-contained, scientifically grounded in the theory of linear models, and well-posed. All provided data are consistent and sufficient to address the tasks.\n\n### Task 1: Invariance of Fitted Values, RSS, and Implication for the Joint Test\n\nThe general linear model is expressed in matrix form as $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{Y}$ is the response vector, $\\mathbf{X}$ is the design matrix, $\\boldsymbol{\\beta}$ is the vector of parameters, and $\\boldsymbol{\\epsilon}$ is the error vector. The Ordinary Least Squares (OLS) estimator for $\\boldsymbol{\\beta}$ is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$. The vector of fitted values is given by the orthogonal projection of $\\mathbf{Y}$ onto the column space of $\\mathbf{X}$, denoted $C(\\mathbf{X})$. This projection is $\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{H}\\mathbf{Y}$, where $\\mathbf{H}$ is the projection matrix, often called the \"hat matrix\".\n\nThe two encoding schemes, S1 and S2, for the categorical factor $G$ represent a re-parameterization of the full model. Let the design matrix for scheme S1 be $\\mathbf{X}_1$ and for scheme S2 be $\\mathbf{X}_2$.\nFor S1 (reference level A), the columns of the design matrix are $[\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{I}_B, \\mathbf{I}_C]$, where $\\mathbf{1}$ is a vector of ones for the intercept, $\\mathbf{X}_{\\text{cov}}$ is the vector for the continuous covariate $X$, and $\\mathbf{I}_B, \\mathbf{I}_C$ are the indicator vectors for levels $B$ and $C$.\nFor S2 (reference level C), the columns of the design matrix are $[\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{J}_A, \\mathbf{J}_B]$.\n\nThe indicator vectors are related by the identity $\\mathbf{I}_A + \\mathbf{I}_B + \\mathbf{I}_C = \\mathbf{1}$. The vectors used in the two schemes are also directly related: $\\mathbf{J}_A = \\mathbf{I}_A$ and $\\mathbf{J}_B = \\mathbf{I}_B$.\nThe basis vectors for the subspace associated with factor $G$ in S1 are $\\{\\mathbf{I}_B, \\mathbf{I}_C\\}$ (in the presence of an intercept), and in S2 they are $\\{\\mathbf{J}_A, \\mathbf{J}_B\\}$. Let's show that these sets of vectors, when combined with the intercept and covariate, span the same space.\nThe basis vectors for the column space $C(\\mathbf{X}_1)$ are $\\{\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{I}_B, \\mathbf{I}_C\\}$.\nThe basis vectors for the column space $C(\\mathbf{X}_2)$ are $\\{\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{J}_A, \\mathbf{J}_B\\}$.\nWe can express the S2 basis vectors in terms of the S1 basis vectors:\n$\\mathbf{J}_A = \\mathbf{I}_A = \\mathbf{1} - \\mathbf{I}_B - \\mathbf{I}_C$.\n$\\mathbf{J}_B = \\mathbf{I}_B$.\nSince each basis vector of $C(\\mathbf{X}_2)$ is a linear combination of the basis vectors of $C(\\mathbf{X}_1)$, it follows that $C(\\mathbf{X}_2) \\subseteq C(\\mathbf{X}_1)$. Similarly, we can express the S1 basis vectors in terms of the S2 basis:\n$\\mathbf{I}_C = \\mathbf{1} - \\mathbf{I}_A - \\mathbf{I}_B = \\mathbf{1} - \\mathbf{J}_A - \\mathbf{J}_B$.\n$\\mathbf{I}_B = \\mathbf{J}_B$.\nThis shows that $C(\\mathbf{X}_1) \\subseteq C(\\mathbf{X}_2)$.\nTherefore, the column spaces are identical: $C(\\mathbf{X}_1) = C(\\mathbf{X}_2)$.\n\nSince the column spaces are identical, the projection matrix $\\mathbf{H}$ is the same for both schemes. The fitted values, $\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}$, are therefore invariant to the choice of reference level. Consequently, the vector of residuals, $\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}}$, is also invariant. The residual sum of squares, $RSS_F = \\mathbf{e}^T\\mathbf{e}$, is therefore unchanged by the re-parameterization. The problem statement confirms this by providing a single value, $RSS_{F}=760$.\n\nThis invariance is crucial for the joint test of $G$. The $F$-test for the significance of the block of variables representing $G$ compares the full model (with $G$) to a reduced model (without $G$). The $F$-statistic is a function of $RSS_R$, $RSS_F$, $n$, $p_R$, and $p_F$. Since none of these quantities depend on the specific encoding scheme chosen for the full model, the resulting $F$-statistic for the joint null hypothesis $H_0: \\beta_B = \\beta_C = 0$ (under S1) or $H_0: \\gamma_A = \\gamma_B = 0$ (under S2) will be identical. The joint test provides an unambiguous assessment of the overall contribution of the factor $G$ to the model.\n\n### Task 2: Individual t-statistics\n\nThe $t$-statistic for a coefficient estimate $\\hat{\\beta}$ is calculated as:\n$$t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$$\n\nFor scheme S1 (reference level A):\nThe $t$-statistic for the indicator $I_B$ is:\n$$ t_{B, S1} = \\frac{\\hat{\\beta}_{B}}{SE(\\hat{\\beta}_{B})} = \\frac{-3.0}{1.8} = -1.666... $$\nThe $t$-statistic for the indicator $I_C$ is:\n$$ t_{C, S1} = \\frac{\\hat{\\beta}_{C}}{SE(\\hat{\\beta}_{C})} = \\frac{-1.5}{1.9} \\approx -0.78947... $$\n\nFor scheme S2 (reference level C):\nThe $t$-statistic for the indicator $J_A$ is:\n$$ t_{A, S2} = \\frac{\\hat{\\gamma}_{A}}{SE(\\hat{\\gamma}_{A})} = \\frac{+1.5}{1.6} = 0.9375 $$\nThe $t$-statistic for the indicator $J_B$ is:\n$$ t_{B, S2} = \\frac{\\hat{\\gamma}_{B}}{SE(\\hat{\\gamma}_{B})} = \\frac{-1.5}{1.7} \\approx -0.88235... $$\n\nThe individual $t$-statistics clearly change across encodings. This is because they test different hypotheses. For instance, $t_{B, S1}$ tests whether the mean response for group $B$ is different from that of the reference group $A$, while $t_{B, S2}$ tests whether group $B$ is different from the reference group $C$.\n\n### Task 3: The Common F-statistic\n\nThe $F$-statistic for testing the null hypothesis that a block of coefficients is zero is calculated by comparing the residual sum of squares of a full model ($RSS_F$) and a reduced model ($RSS_R$). The formula is:\n$$ F = \\frac{(RSS_R - RSS_F) / (p_F - p_R)}{RSS_F / (n - p_F)} $$\nwhere $p_F$ and $p_R$ are the number of parameters in the full and reduced models, respectively, and $n$ is the number of observations.\n\nThe givens are:\n- Reduced model RSS: $RSS_{R} = 820$\n- Full model RSS: $RSS_{F} = 760$\n- Number of parameters in reduced model: $p_{R} = 2$ (intercept and $X$)\n- Number of parameters in full model: $p_{F} = 4$ (intercept, $X$, and two indicators for $G$)\n- Number of observations: $n = 60$\n\nThe number of parameters being tested (the number of indicators for $G$) is $p_F - p_R = 4 - 2 = 2$.\nThe degrees of freedom for the numerator are $df_1 = p_F - p_R = 2$.\nThe degrees of freedom for the denominator are $df_2 = n - p_F = 60 - 4 = 56$.\n\nPlugging the values into the formula:\n$$ F = \\frac{(820 - 760) / (4 - 2)}{760 / (60 - 4)} $$\n$$ F = \\frac{60 / 2}{760 / 56} $$\n$$ F = \\frac{30}{760 / 56} $$\n$$ F = \\frac{30 \\times 56}{760} = \\frac{1680}{760} = \\frac{168}{76} = \\frac{42}{19} $$\n$$ F \\approx 2.2105263... $$\n\nRounding to four significant figures, the value is $2.211$.\nThis $F$-statistic tests the null hypothesis that factor $G$ has no effect, i.e., that the coefficients for both of its indicators are jointly zero. As explained in Task 1, this value is invariant to the choice of reference level.", "answer": "$$\\boxed{2.211}$$", "id": "3130441"}, {"introduction": "In modern data analysis, massive datasets can give statistical tests immense power to detect even minuscule effects, leading to results that are statistically significant but practically unimportant. This creates a critical distinction between a result being unlikely due to chance and it being large enough to matter in a real-world application. This problem challenges you to navigate this scenario, using the F-test results to distinguish between statistical significance and practical significance, a vital skill for any data scientist. [@problem_id:3130333]", "problem": "An online video platform uses linear regression with ordinary least squares (OLS) under the classical linear model assumptions that the errors $\\varepsilon_i$ are independent and identically distributed as normal with mean $0$ and variance $\\sigma^2$ to predict session watch time $y_i$ (in minutes) from a set of features. They consider two nested models on the same training data of size $n = 5000$:\n- A reduced model with $p_R = 6$ parameters (including the intercept).\n- A full model that adds $r = 3$ new predictors for a total of $p_F = 9$ parameters (including the intercept).\n\nThe three added predictors encode micro-interactions that require additional engineering and on-device logging, incurring non-trivial operational costs. The training sums of squares are:\n- Total sum of squares $TSS = 20000$,\n- Residual sum of squares for the reduced model $RSS_R = 10000$,\n- Residual sum of squares for the full model $RSS_F = 9950$.\n\nThe platform plans to use Fisher’s $F$-test at significance level $\\alpha = 0.05$ to test the joint null hypothesis that the $r = 3$ new coefficients are all zero. Based on these data and assumptions, select all statements that are most appropriate.\n\nA. The $F$-test for the null that the three added coefficients are all zero will likely be statistically significant at level $\\alpha = 0.05$ because $n$ is large, even though the improvement in fit is small.\n\nB. The effect size, as reflected by the change in $R^2$, indicates negligible practical significance; adding the new predictors should be justified only if they offer interpretability or clear business value relative to their cost.\n\nC. The $p$-value for the $F$-test is approximately equal to the change in $R^2$, so it will be around $0.0025$.\n\nD. Increasing $n$ always increases the change in $R^2$ when adding predictors, so practical significance grows with sample size.\n\nE. If the added predictors are costly to collect, a decision focused on practical significance may favor the reduced model even if the $F$-test rejects the null.", "solution": "The problem statement will first be validated for scientific soundness, consistency, and completeness.\n\n### Step 1: Extract Givens\n-   Model type: Linear regression with ordinary least squares (OLS).\n-   Assumptions: Classical Linear Model (CLM), specifically $\\varepsilon_i \\sim \\text{i.i.d. } N(0, \\sigma^2)$.\n-   Sample size: $n = 5000$.\n-   Reduced model parameters: $p_R = 6$.\n-   Full model parameters: $p_F = 9$.\n-   Number of new predictors tested: $r = 3$.\n-   Total Sum of Squares: $TSS = 20000$.\n-   Residual Sum of Squares for the reduced model: $RSS_R = 10000$.\n-   Residual Sum of Squares for the full model: $RSS_F = 9950$.\n-   Null Hypothesis ($H_0$): The coefficients of the $r=3$ new predictors are all zero.\n-   Significance Level: $\\alpha = 0.05$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard scenario in applied statistics: comparing nested linear models using an $F$-test.\n-   **Scientifically Grounded:** The setup is based on fundamental principles of statistical learning and econometrics, specifically hypothesis testing for multiple coefficients in a linear regression framework. The concepts ($RSS$, $TSS$, $F$-test) are standard.\n-   **Well-Posed:** The problem provides all necessary numerical values ($n$, $p_F$, $r$, $RSS_R$, $RSS_F$) to compute the $F$-statistic and make a decision. A unique solution can be derived.\n-   **Objective:** The problem is stated in precise, quantitative terms.\n-   **Consistency and Completeness:** The provided data are consistent. For nested models, it must be that $TSS \\ge RSS_R \\ge RSS_F$, which holds true: $20000 \\ge 10000 \\ge 9950$. Also, the number of parameters is consistent: $p_F = p_R + r$, which is $9 = 6 + 3$. The problem is self-contained.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. The solution will now be derived.\n\n### Derivation\nFirst, we compute the $F$-statistic for the joint null hypothesis that the $r=3$ additional coefficients are all zero. The formula for this test is:\n$$ F = \\frac{(RSS_R - RSS_F) / r}{RSS_F / (n - p_F)} $$\nSubstituting the given values:\n$$ F = \\frac{(10000 - 9950) / 3}{9950 / (5000 - 9)} = \\frac{50 / 3}{9950 / 4991} = \\frac{50}{3} \\cdot \\frac{4991}{9950} \\approx 16.667 \\cdot 0.5016 \\approx 8.3598 $$\nSo, the $F$-statistic is approximately $8.36$. This statistic follows an $F$-distribution with $r=3$ and $n - p_F = 5000 - 9 = 4991$ degrees of freedom, denoted $F_{3, 4991}$.\n\nTo make a decision, we compare this value to the critical value from the $F_{3, 4991}$ distribution at a significance level of $\\alpha = 0.05$. For large denominator degrees of freedom, this critical value is very close to that of an $F_{3, \\infty}$ distribution. The critical value $F_{crit}$ is approximately $2.607$.\nSince our calculated $F$-statistic $8.36 > 2.607$, we reject the null hypothesis $H_0$. The result is statistically significant.\n\nNext, we evaluate the practical significance by calculating the change in the coefficient of determination, $R^2$, where $R^2 = 1 - \\frac{RSS}{TSS}$.\nFor the reduced model:\n$$ R_R^2 = 1 - \\frac{RSS_R}{TSS} = 1 - \\frac{10000}{20000} = 1 - 0.5 = 0.5000 $$\nFor the full model:\n$$ R_F^2 = 1 - \\frac{RSS_F}{TSS} = 1 - \\frac{9950}{20000} = 1 - 0.4975 = 0.5025 $$\nThe improvement in $R^2$ is:\n$$ \\Delta R^2 = R_F^2 - R_R^2 = 0.5025 - 0.5000 = 0.0025 $$\nThe three additional predictors account for only an additional $0.25\\%$ of the variance in session watch time. This is a very small increase in explanatory power, suggesting a small effect size and limited practical significance.\n\n### Option-by-Option Analysis\n\n**A. The $F$-test for the null that the three added coefficients are all zero will likely be statistically significant at level $\\alpha = 0.05$ because $n$ is large, even though the improvement in fit is small.**\nOur calculations show the $F$-test is indeed statistically significant ($F \\approx 8.36 > F_{crit} \\approx 2.607$). The improvement in fit, as measured by the reduction in $RSS$ ($10000 - 9950 = 50$) or the increase in $R^2$ ($\\Delta R^2 = 0.0025$), is small relative to the total sums of squares. Statistical power, the ability to detect a non-zero effect, increases with sample size $n$. With a large sample size like $n = 5000$, even a small true effect is likely to be detected as statistically significant. The $F$-statistic formula can be written as $F = \\frac{\\Delta R^2 / r}{(1 - R_F^2) / (n - p_F)}$, which shows that for a fixed effect size ($\\Delta R^2$), the $F$-statistic grows approximately linearly with $n$. This statement correctly identifies the relationship between large sample size, small effect size, and statistical significance.\n**Verdict: Correct.**\n\n**B. The effect size, as reflected by the change in $R^2$, indicates negligible practical significance; adding the new predictors should be justified only if they offer interpretability or clear business value relative to their cost.**\nThe change in $R^2$ is $\\Delta R^2 = 0.0025$, an increase of only $0.25\\%$ in explained variance. In most applied contexts, this would be considered a negligible improvement in predictive power. The problem states the new predictors have \"non-trivial operational costs\". Therefore, it is a standard and correct principle of applied statistics that the decision to include these costly predictors should not be based on statistical significance alone. One must weigh the cost against the benefit, and in this case the benefit is minuscule. Justification would require other factors, such as the predictors providing crucial business insights or interpretations, which might outweigh their cost. The statement is a sound and appropriate conclusion based on the analysis.\n**Verdict: Correct.**\n\n**C. The $p$-value for the $F$-test is approximately equal to the change in $R^2$, so it will be around $0.0025$.**\nThe change in $R^2$ is $\\Delta R^2 = 0.0025$. The $p$-value is the probability of observing an $F$-statistic as extreme as or more extreme than $8.36$ under the null hypothesis, i.e., $P(F_{3, 4991} \\ge 8.36)$. The value of this probability is extremely small. Using statistical software, the $p$-value is approximately $1.9 \\times 10^{-5}$ or $0.000019$. This value is not close to $0.0025$. There is no general principle stating that the $p$-value of an $F$-test should approximate the change in $R^2$.\n**Verdict: Incorrect.**\n\n**D. Increasing $n$ always increases the change in $R^2$ when adding predictors, so practical significance grows with sample size.**\nThis statement is incorrect. The sample $R^2$ is an estimate of the population $R^2$. As the sample size $n$ increases, the sample $R^2$ and the sample $\\Delta R^2$ converge to their true, fixed population values. They do not systematically grow with $n$. It is the *statistical significance* (and the power of the test to detect the effect) that grows with $n$, not the *practical significance* or effect size (as measured by $\\Delta R^2$). This statement confuses these two distinct concepts.\n**Verdict: Incorrect.**\n\n**E. If the added predictors are costly to collect, a decision focused on practical significance may favor the reduced model even if the $F$-test rejects the null.**\nThis statement accurately describes the common dilemma between statistical and practical significance. Our analysis shows that the $F$-test rejects the null, indicating a statistically significant result. However, the practical significance (effect size) is very small ($\\Delta R^2 = 0.0025$). Given that the predictors are costly, a decision-maker focusing on the practical return on investment would likely conclude that the marginal improvement in model fit does not justify the cost. Therefore, favoring the simpler, less-costly reduced model is a rational and common outcome in such a scenario. The statement is a correct application of statistical reasoning in a practical setting.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABE}$$", "id": "3130333"}]}