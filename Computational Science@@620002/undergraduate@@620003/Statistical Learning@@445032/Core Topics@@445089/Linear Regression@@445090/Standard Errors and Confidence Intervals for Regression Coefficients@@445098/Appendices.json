{"hands_on_practices": [{"introduction": "The standard error of a regression coefficient quantifies its uncertainty, but this uncertainty can be influenced by how we specify our model and the properties of our data. This first practice explores the subtle but significant effect of centering a predictor variable, a common preprocessing step. By working through this example [@problem_id:3176657], you will discover how centering can dramatically stabilize the estimate of the intercept, $\\hat{\\beta}_0$, by reducing collinearity between it and the slope term, thereby providing a clearer interpretation of the model's baseline.", "problem": "A data analyst fits an Ordinary Least Squares (OLS) simple linear regression model to fixed-design data, assuming the classical linear model conditions: for $i=1,\\dots,n$, the responses satisfy $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ where the errors $\\varepsilon_i$ are independent and identically distributed with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ and the predictor values $x_i$ are nonrandom. The analyst reports the following summary for the original (uncentered) model:\n- Sample size $n = 100$.\n- Predictor mean $\\bar{x} = 75$.\n- Predictor sum of squares about the mean $S_{xx} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = 39600$.\n- Estimated slope $\\hat{\\beta}_1 = 0.45$.\n- Residual mean squared error $\\hat{\\sigma}^2 = 9$.\n\nThe analyst then centers the predictor by defining $x_i^{c} = x_i - \\bar{x}$ and refits the regression $y_i$ on an intercept and $x_i^{c}$ using OLS under the same assumptions.\n\nWhich statement best captures the quantitative and qualitative effects of centering on the estimated intercept and the slope $t$-statistic, and explains why?\n\nA. After centering, the intercept standard error is unchanged, but the slope estimate and its $t$-statistic change because centering modifies the spread of $x$.\n\nB. After centering, the intercept standard error drops from approximately $1.17$ to approximately $0.30$ because the predictor mean becomes zero, while the slope $t$-statistic is unchanged; centering does not alter $S_{xx}$ or the residuals relevant to the slope.\n\nC. After centering, both the intercept and slope standard errors are divided by $\\sqrt{n}$, and both $t$-statistics increase accordingly.\n\nD. After centering, the slope standard error increases because $S_{xx}$ decreases when $x$ is shifted, leading to a smaller $t$-statistic, while the intercept standard error remains the same.\n\nE. After centering, the slope confidence interval widens because the degrees of freedom change from $n-2$ to $n-1$, whereas the intercept confidence interval narrows slightly.", "solution": "The user wants me to analyze the effects of centering a predictor variable on the parameters and test statistics of a simple linear regression model.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Model (Original): $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, where $i=1,\\dots,n$.\n-   Assumptions: Errors $\\varepsilon_i$ are independent and identically distributed, with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Predictor values $x_i$ are nonrandom (fixed design).\n-   Sample size: $n = 100$.\n-   Predictor mean: $\\bar{x} = 75$.\n-   Predictor sum of squares about the mean: $S_{xx} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = 39600$.\n-   Estimated slope: $\\hat{\\beta}_1 = 0.45$.\n-   Residual mean squared error: $\\hat{\\sigma}^2 = 9$.\n-   Centered predictor: $x_i^{c} = x_i - \\bar{x}$.\n-   Model (Centered): $y_i = \\beta'_0 + \\beta'_1 x_i^c + \\varepsilon'_i$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is based on the theory of Ordinary Least Squares (OLS) regression, a cornerstone of classical statistics. The assumptions are standard for the classical linear model. The setup is scientifically and mathematically sound.\n-   **Well-Posed:** The problem provides all necessary numerical values and model definitions to compare the statistical properties of the original and centered regression models. A unique, meaningful solution can be derived.\n-   **Objective:** The problem is stated using precise, standard statistical terminology, leaving no room for subjective interpretation.\n\nNo flaws are identified. The problem is not unsound, incomplete, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the derivation and solution.\n\n### Derivation\n\nLet us first analyze the original (uncentered) model and then the centered model. We will denote quantities from the centered model with a prime (e.g., $\\hat{\\beta}'_0$).\n\n**1. Analysis of the Original (Uncentered) Model**\n\nThe OLS estimators for the parameters are given by:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nThe estimated variances of these estimators (under the given assumptions) are:\n$$ \\widehat{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}^2}{S_{xx}} $$\n$$ \\widehat{\\text{Var}}(\\hat{\\beta}_0) = \\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right) $$\nThe standard errors ($SE$) are the square roots of these estimated variances.\n\nUsing the provided data:\n-   $n = 100$\n-   $\\bar{x} = 75$\n-   $S_{xx} = 39600$\n-   $\\hat{\\beta}_1 = 0.45$\n-   $\\hat{\\sigma}^2 = 9$\n\nWe can calculate the standard error of the intercept, $SE(\\hat{\\beta}_0)$:\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_0)} = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} + \\frac{75^2}{39600} \\right)} $$\n$$ SE(\\hat{\\beta}_0) = \\sqrt{9 \\left( 0.01 + \\frac{5625}{39600} \\right)} \\approx \\sqrt{9 (0.01 + 0.142045)} = \\sqrt{9(0.152045)} = \\sqrt{1.368409} \\approx 1.16979 $$\nSo, the original intercept standard error is approximately $1.17$.\n\nThe standard error of the slope, $SE(\\hat{\\beta}_1)$, is:\n$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{9}{39600}} = \\sqrt{\\frac{1}{4400}} \\approx 0.015076 $$\nThe $t$-statistic for the slope, $t_{\\hat{\\beta}_1}$, is:\n$$ t_{\\hat{\\beta}_1} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = \\frac{0.45}{\\sqrt{9/39600}} = 0.45 \\times \\sqrt{4400} \\approx 29.85 $$\n\n**2. Analysis of the Centered Model**\n\nThe new predictor is $x_i^c = x_i - \\bar{x}$.\nThe mean of the centered predictor is $\\bar{x}^c = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x}) = \\bar{x} - \\bar{x} = 0$.\nThe sum of squares of the centered predictor about its mean is:\n$$ S_{x^c x^c} = \\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2 = \\sum_{i=1}^n (x_i - \\bar{x} - 0)^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 = S_{xx} = 39600 $$\nThus, centering does not change the sum of squares $S_{xx}$.\n\nNow, let's find the new OLS estimators, $\\hat{\\beta}'_0$ and $\\hat{\\beta}'_1$:\n$$ \\hat{\\beta}'_1 = \\frac{\\sum_{i=1}^n (x_i^c - \\bar{x}^c)(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{S_{xx}} = \\hat{\\beta}_1 $$\nThe slope estimate is invariant to centering the predictor. So, $\\hat{\\beta}'_1 = 0.45$.\n\n$$ \\hat{\\beta}'_0 = \\bar{y} - \\hat{\\beta}'_1 \\bar{x}^c = \\bar{y} - \\hat{\\beta}_1 (0) = \\bar{y} $$\nThe new intercept estimate is the sample mean of the response variable.\n\nThe fitted values for the centered model are $\\hat{y}'_i = \\hat{\\beta}'_0 + \\hat{\\beta}'_1 x_i^c = \\bar{y} + \\hat{\\beta}_1 (x_i - \\bar{x})$.\nThe fitted values for the original model are $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i = (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 x_i = \\bar{y} + \\hat{\\beta}_1 (x_i - \\bar{x})$.\nSince $\\hat{y}'_i = \\hat{y}_i$ for all $i$, the residuals $e'_i = y_i - \\hat{y}'_i$ and $e_i = y_i - \\hat{y}_i$ are identical.\nThis implies that the Residual Sum of Squares (RSS) is unchanged, and therefore the residual mean squared error $\\hat{\\sigma}'^2 = \\frac{\\text{RSS}}{n-2}$ is also unchanged. Thus, $\\hat{\\sigma}'^2 = \\hat{\\sigma}^2 = 9$.\n\nNow we compute the standard errors for the centered model:\nThe standard error of the new slope, $SE(\\hat{\\beta}'_1)$, is:\n$$ SE(\\hat{\\beta}'_1) = \\sqrt{\\frac{\\hat{\\sigma}'^2}{S_{x^c x^c}}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = SE(\\hat{\\beta}_1) $$\nThe slope standard error is unchanged.\n\nThe standard error of the new intercept, $SE(\\hat{\\beta}'_0)$, is:\n$$ SE(\\hat{\\beta}'_0) = \\sqrt{\\hat{\\sigma}'^2 \\left( \\frac{1}{n} + \\frac{(\\bar{x}^c)^2}{S_{x^c x^c}} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} + \\frac{0^2}{39600} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} \\right)} = \\sqrt{0.09} = 0.30 $$\nThe intercept standard error drops from approximately $1.17$ to exactly $0.30$. This reduction occurs because the term involving $\\bar{x}^2$ in the variance formula vanishes. Geometrically, the intercept in the centered model represents the predicted value at the mean of the predictor, where estimation is most precise. In the original model, it represents the predicted value at $x=0$, which is far from the data's center ($\\bar{x}=75$), requiring a long extrapolation.\n\nFinally, the $t$-statistic for the centered slope, $t_{\\hat{\\beta}'_1}$, is:\n$$ t_{\\hat{\\beta}'_1} = \\frac{\\hat{\\beta}'_1}{SE(\\hat{\\beta}'_1)} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = t_{\\hat{\\beta}_1} $$\nThe slope $t$-statistic is unchanged because neither the slope estimate nor its standard error changes.\n\n**Summary of Effects:**\n-   Intercept Estimate: Changes from $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$ to $\\hat{\\beta}'_0 = \\bar{y}$.\n-   Intercept Standard Error: Drops from $\\approx 1.17$ to $0.30$.\n-   Slope Estimate: Unchanged ($\\hat{\\beta}'_1 = \\hat{\\beta}_1$).\n-   Slope Standard Error: Unchanged.\n-   Slope $t$-statistic: Unchanged.\n-   Residuals and $\\hat{\\sigma}^2$: Unchanged.\n-   Degrees of freedom: Remain $n-2$ as two parameters are estimated in both models.\n\n### Option-by-Option Analysis\n\n**A. After centering, the intercept standard error is unchanged, but the slope estimate and its $t$-statistic change because centering modifies the spread of $x$.**\n-   The intercept standard error changes significantly (drops from $\\approx 1.17$ to $0.30$). This statement is incorrect.\n-   The slope estimate and its $t$-statistic are unchanged. This statement is incorrect.\n-   Centering is a location shift, which does not modify the spread ($S_{xx}$). This reasoning is incorrect.\nVerdict: **Incorrect**.\n\n**B. After centering, the intercept standard error drops from approximately $1.17$ to approximately $0.30$ because the predictor mean becomes zero, while the slope $t$-statistic is unchanged; centering does not alter $S_{xx}$ or the residuals relevant to the slope.**\n-   The statement that the intercept standard error drops from $\\approx 1.17$ to $\\approx 0.30$ is quantitatively correct, based on our calculations.\n-   The reason given, that the predictor mean becomes zero, is the correct explanation for the change in the intercept's variance formula.\n-   The statement that the slope $t$-statistic is unchanged is correct.\n-   The reasoning that centering does not alter $S_{xx}$ or the residuals is also correct and fundamental to why the slope statistics are invariant.\nVerdict: **Correct**.\n\n**C. After centering, both the intercept and slope standard errors are divided by $\\sqrt{n}$, and both $t$-statistics increase accordingly.**\n-   The slope standard error is unchanged, not divided by $\\sqrt{n}$.\n-   The intercept standard error changes from $\\sqrt{\\hat{\\sigma}^2 (1/n + \\bar{x}^2/S_{xx})}$ to $\\sqrt{\\hat{\\sigma}^2/n}$, which is not a simple division by $\\sqrt{n}$.\n-   The slope $t$-statistic is unchanged, not increased.\nVerdict: **Incorrect**.\n\n**D. After centering, the slope standard error increases because $S_{xx}$ decreases when $x$ is shifted, leading to a smaller $t$-statistic, while the intercept standard error remains the same.**\n-   The slope standard error is unchanged, not increased.\n-   $S_{xx}$ is unchanged, not decreased.\n-   The slope $t$-statistic is unchanged, not smaller.\n-   The intercept standard error decreases, not remains the same.\nEvery part of this statement is incorrect.\nVerdict: **Incorrect**.\n\n**E. After centering, the slope confidence interval widens because the degrees of freedom change from $n-2$ to $n-1$, whereas the intercept confidence interval narrows slightly.**\n-   The slope confidence interval, $\\hat{\\beta}_1 \\pm t_{n-2, \\alpha/2} SE(\\hat{\\beta}_1)$, is unchanged.\n-   The degrees of freedom remain $n-2$ because the model still has an intercept and a slope, for a total of two estimated parameters.\n-   The intercept confidence interval narrows substantially, not slightly. The standard error reduces by a factor of nearly $4$ ($1.17/0.3 \\approx 3.9$).\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "3176657"}, {"introduction": "Building on the idea of coefficient dependencies, we now examine a more extreme case: perfect multicollinearity. When predictors are perfectly linearly dependent, as occurs in the \"dummy variable trap,\" the model becomes unidentifiable, and standard errors for the redundant coefficients cannot be uniquely determined. This exercise [@problem_id:3176621] will guide you through diagnosing this issue and resolving it via reparameterization, illustrating the critical distinction between non-identifiable coefficients and estimable, interpretable contrasts like the difference between group means.", "problem": "A data analyst models hourly wages using a linear regression with an intercept and dummy variables for group membership. There are two groups, $A$ and $B$, and the analyst encodes membership with dummy variables $D_A$ and $D_B$, where $D_A=1$ for group $A$ and $0$ otherwise, and $D_B=1$ for group $B$ and $0$ otherwise. The observed data are:\n- Group $A$ wages: $y=(10,12,11)$,\n- Group $B$ wages: $y=(7,9,8)$,\nso the total sample size is $n=6$. The analyst first fits a model with an intercept and both dummies, and then considers reparameterizations to resolve any potential identifiability issues.\n\nSelect all statements that are correct about the existence of $(X^{\\top}X)^{-1}$, the identifiability of regression coefficients, and how reparameterization restores identifiable standard errors and confidence intervals for regression coefficients in this setting. You may assume a homoskedastic error model and use the usual $t$-based confidence intervals at the $95\\%$ level with residual degrees of freedom corresponding to the fitted model.\n\nA. With an intercept and both $D_A$ and $D_B$ included, the design matrix columns are perfectly collinear, so $X^{\\top}X$ is singular and $(X^{\\top}X)^{-1}$ does not exist.\n\nB. Reparameterizing to use an intercept and a single dummy (e.g., drop $D_B$ and keep $D_A$) yields a full column rank design, and the $95\\%$ two-sided confidence interval for the coefficient on $D_A$ (the mean difference $A$ minus $B$) is approximately $[0.73,\\,5.27]$.\n\nC. Alternatively, dropping the intercept and keeping both $D_A$ and $D_B$ also yields a full column rank design, and the $95\\%$ two-sided confidence interval for $\\beta_A-\\beta_B$ is identical to that in option B.\n\nD. Even without reparameterization, one can still obtain unique, software-independent standard errors and $95\\%$ confidence intervals for the individual coefficients on $D_A$ and $D_B$ in the overparameterized model with an intercept and both dummies, because ordinary least squares automatically ignores redundant columns.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It provides a standard, text-book scenario for demonstrating the concepts of multicollinearity, model parameterization, and estimability in linear regression. All data and conditions required for analysis are present and consistent. I will proceed with a full derivation and evaluation of each option.\n\nThe response vector $y$ combines the wages from both groups:\n$$y = (10, 12, 11, 7, 9, 8)^{\\top}$$\nThe dummy variables are defined as $D_A=1$ for group $A$ observations (the first three) and $D_A=0$ for group $B$ observations (the last three), and $D_B=1$ for group $B$ and $D_B=0$ for group $A$.\n\nA. With an intercept and both $D_A$ and $D_B$ included, the design matrix columns are perfectly collinear, so $X^{\\top}X$ is singular and $(X^{\\top}X)^{-1}$ does not exist.\n\nThe model is $y_i = \\beta_0 + \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon_i$. The design matrix $X$ has $n=6$ rows and $p=3$ columns (one for the intercept $\\beta_0$, one for $\\beta_A$, and one for $\\beta_B$).\nThe columns are an intercept vector $c_1$, a vector for $D_A$, $c_2$, and a vector for $D_B$, $c_3$:\n$$\nc_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nc_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe design matrix is $X = [c_1, c_2, c_3]$. A linear dependency exists among these columns because for every observation, an individual is in either group $A$ or group $B$, meaning $D_A + D_B = 1$. Thus, the sum of the dummy variable columns equals the intercept column:\n$$c_2 + c_3 = c_1 \\implies 1 \\cdot c_1 - 1 \\cdot c_2 - 1 \\cdot c_3 = \\mathbf{0}$$\nThis perfect collinearity means the columns of $X$ are linearly dependent, and the matrix $X$ is not of full column rank. The rank of $X$ is $2$, not $3$.\nConsequently, the matrix $X^{\\top}X$ is singular (not invertible). We can compute it to verify:\n$$\nX^{\\top}X = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 3 & 3 \\\\\n3 & 3 & 0 \\\\\n3 & 0 & 3\n\\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(X^{\\top}X) = 6(3 \\cdot 3 - 0) - 3(3 \\cdot 3 - 0) + 3(0 - 3 \\cdot 3) = 54 - 27 - 27 = 0$. Since the determinant is zero, $X^{\\top}X$ is singular, and its inverse $(X^{\\top}X)^{-1}$ does not exist. This situation is known as the dummy variable trap. The coefficients $\\beta_0, \\beta_A, \\beta_B$ are not uniquely identifiable.\n\nThe statement is **Correct**.\n\nB. Reparameterizing to use an intercept and a single dummy (e.g., drop $D_B$ and keep $D_A$) yields a full column rank design, and the $95\\%$ two-sided confidence interval for the coefficient on $D_A$ (the mean difference $A$ minus $B$) is approximately $[0.73,\\,5.27]$.\n\nThe reparameterized model is $y_i = \\beta'_0 + \\beta'_A D_{Ai} + \\epsilon'_i$. This is a standard approach to resolve the collinearity. The design matrix $X_1$ has two columns:\n$$\nX_1 = \\begin{pmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 0\n\\end{pmatrix}\n$$\nThese columns are linearly independent, so $X_1$ has full column rank (rank $2$), and $(X_1^{\\top}X_1)^{-1}$ exists.\nThe OLS estimator is $\\hat{\\beta}' = (X_1^{\\top}X_1)^{-1}X_1^{\\top}y$.\n$$\nX_1^{\\top}X_1 = \\begin{pmatrix} 6 & 3 \\\\ 3 & 3 \\end{pmatrix} \\quad \\implies \\quad (X_1^{\\top}X_1)^{-1} = \\frac{1}{18-9} \\begin{pmatrix} 3 & -3 \\\\ -3 & 6 \\end{pmatrix} = \\begin{pmatrix} 1/3 & -1/3 \\\\ -1/3 & 2/3 \\end{pmatrix}\n$$\n$$\nX_1^{\\top}y = \\begin{pmatrix} \\sum y_i \\\\ \\sum_{i \\in A} y_i \\end{pmatrix} = \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}' = \\begin{pmatrix} \\hat{\\beta}'_0 \\\\ \\hat{\\beta}'_A \\end{pmatrix} = \\begin{pmatrix} 1/3 & -1/3 \\\\ -1/3 & 2/3 \\end{pmatrix} \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix} = \\begin{pmatrix} (57-33)/3 \\\\ (-57+66)/3 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 3 \\end{pmatrix}\n$$\nIn this model, $\\beta'_0$ represents the mean wage for the reference group (group $B$, where $D_A=0$), and $\\beta'_A$ represents the difference in mean wages between group $A$ and group $B$. The sample mean for group $B$ is $(7+9+8)/3 = 8$, so $\\hat{\\beta}'_0=8$. The sample mean for group $A$ is $(10+12+11)/3 = 11$. The mean difference is $11-8=3$, so $\\hat{\\beta}'_A=3$. The coefficient interpretation in the statement is correct.\n\nTo find the confidence interval for $\\beta'_A$, we need its standard error. $SE(\\hat{\\beta}'_A) = \\sqrt{\\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22}}$.\nFirst, we find the residual sum of squares ($RSS$) to estimate the error variance $\\sigma^2$. The fitted values are $\\hat{y}_i = 8+3(1)=11$ for group $A$ and $\\hat{y}_i = 8+3(0)=8$ for group $B$.\n$$RSS = \\sum(y_i-\\hat{y}_i)^2 = (10-11)^2+(12-11)^2+(11-11)^2 + (7-8)^2+(9-8)^2+(8-8)^2 = 1+1+0+1+1+0 = 4$$\nThe number of parameters is $p=2$. The degrees of freedom for the residuals is $df=n-p=6-2=4$.\nThe unbiased estimate of the error variance is $\\hat{\\sigma}^2 = \\frac{RSS}{n-p} = \\frac{4}{4} = 1$.\nThe variance of $\\hat{\\beta}'_A$ is $\\text{Var}(\\hat{\\beta}'_A) = \\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22} = 1 \\cdot (2/3) = 2/3$.\nThe standard error is $SE(\\hat{\\beta}'_A) = \\sqrt{2/3}$.\nFor a $95\\%$ confidence interval with $df=4$, the critical t-value is $t_{0.025, 4} = 2.776$.\nThe confidence interval is $\\hat{\\beta}'_A \\pm t_{0.025, 4} \\cdot SE(\\hat{\\beta}'_A)$:\n$$3 \\pm 2.776 \\cdot \\sqrt{2/3} \\approx 3 \\pm 2.776 \\cdot 0.8165 \\approx 3 \\pm 2.266$$\nThis gives the interval $[0.734, 5.266]$, which rounds to $[0.73, 5.27]$.\n\nThe statement is **Correct**.\n\nC. Alternatively, dropping the intercept and keeping both $D_A$ and $D_B$ also yields a full column rank design, and the $95\\%$ two-sided confidence interval for $\\beta_A-\\beta_B$ is identical to that in option B.\n\nThe alternative model is $y_i = \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon''_{i}$. The design matrix $X_2$ is:\n$$\nX_2 = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n$$\nThe columns are orthogonal and thus linearly independent. $X_2$ has full column rank.\nThe OLS estimator is $\\hat{\\beta}'' = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$.\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 3 & 0 \\\\ 0 & 3 \\end{pmatrix} \\quad \\implies \\quad (X_2^{\\top}X_2)^{-1} = \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix}\n$$\n$$\nX_2^{\\top}y = \\begin{pmatrix} \\sum_{i \\in A} y_i \\\\ \\sum_{i \\in B} y_i \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}'' = \\begin{pmatrix} \\hat{\\beta}_A \\\\ \\hat{\\beta}_B \\end{pmatrix} = \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 8 \\end{pmatrix}\n$$\nIn this model, $\\beta_A$ is the mean wage for group $A$ and $\\beta_B$ is the mean wage for group $B$. Their estimates are the respective sample means.\nWe are interested in the confidence interval for the difference $\\beta_A - \\beta_B$. The point estimate is $\\hat{\\beta}_A - \\hat{\\beta}_B = 11 - 8 = 3$. This is identical to the point estimate $\\hat{\\beta}'_A$ from model B.\nThe variance of this difference is $\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\text{Var}(\\hat{\\beta}_A) + \\text{Var}(\\hat{\\beta}_B) - 2\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B)$.\nTo find the variance-covariance matrix of the coefficients, we first need $\\hat{\\sigma}^2$ for this model. The fitted values are $\\hat{y}_i = 11$ for group $A$ and $\\hat{y}_i = 8$ for group $B$. These are the same fitted values as in model B. Thus, the residuals and the $RSS$ are identical: $RSS=4$. The number of parameters is $p=2$, so $df=n-p=4$, which is also the same. Therefore, $\\hat{\\sigma}^2 = RSS/df = 4/4 = 1$.\nThe variance-covariance matrix is $\\text{Cov}(\\hat{\\beta}'') = \\hat{\\sigma}^2(X_2^{\\top}X_2)^{-1} = 1 \\cdot \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix}$.\nFrom this, $\\text{Var}(\\hat{\\beta}_A) = 1/3$, $\\text{Var}(\\hat{\\beta}_B) = 1/3$, and $\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B) = 0$.\nSo, $\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = 1/3 + 1/3 - 0 = 2/3$.\nThe standard error of the difference is $SE(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\sqrt{2/3}$.\nThe point estimate ($3$), standard error ($\\sqrt{2/3}$), and degrees of freedom ($4$) are all identical to those for $\\hat{\\beta}'_A$ in option B. Therefore, the $95\\%$ confidence interval for $\\beta_A-\\beta_B$ must be identical. Both models (in B and C) are valid reparameterizations spanning the same vector space of fitted values, and $\\beta_A-\\beta_B$ and $\\beta'_A$ are estimable functions representing the same physical quantity (mean difference), so their estimates and confidence intervals must be the same.\n\nThe statement is **Correct**.\n\nD. Even without reparameterization, one can still obtain unique, software-independent standard errors and $95\\%$ confidence intervals for the individual coefficients on $D_A$ and $D_B$ in the overparameterized model with an intercept and both dummies, because ordinary least squares automatically ignores redundant columns.\n\nAs established in A, the matrix $X^{\\top}X$ for the overparameterized model is singular. This means there is no unique solution for the coefficient vector $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_A, \\hat{\\beta}_B)^{\\top}$. The normal equations $X^{\\top}X\\hat{\\beta}=X^{\\top}y$ have infinitely many solutions.\nSpecifically, if $\\hat{\\beta}^*$ is one solution, then for any arbitrary scalar $c$, the vector $\\hat{\\beta}^* + c(1, -1, -1)^{\\top}$ is also a solution, since $(1, -1, -1)^{\\top}$ is in the null space of $X$.\nThe individual coefficients $\\beta_A$ and $\\beta_B$ are not \"estimable\" functions, meaning their estimates depend on an arbitrary constraint imposed to find a particular solution. Different statistical software packages impose different constraints (e.g., setting one coefficient to zero, using sum-to-zero contrasts), leading to different values for $\\hat{\\beta}_A$ and $\\hat{\\beta}_B$. For instance, setting $\\beta_B=0$ yields $\\hat{\\beta}_A=3$, while setting $\\beta_A=0$ yields $\\hat{\\beta}_B=-3$.\nSince the point estimates for $\\beta_A$ and $\\beta_B$ are not unique and are software-dependent, their standard errors and confidence intervals are also not uniquely defined or software-independent. The variance of $\\hat{\\beta}$ is formally given by $\\sigma^2(X^{\\top}X)^-$, where $(X^{\\top}X)^-$ is a generalized inverse of $X^{\\top}X$. The generalized inverse is not unique, and different choices lead to different (and meaningless) variances for non-estimable coefficients like $\\beta_A$ and $\\beta_B$.\nThe claim that OLS \"automatically ignores redundant columns\" is a crude description of what some software implementations do, but the choice of which column to ignore is arbitrary and affects the values of the individual coefficients. It is precisely because of this ambiguity that reparameterization is required to obtain meaningful, unique estimates and inference for interpretable parameters.\n\nThe statement is **Incorrect**.", "answer": "$$\\boxed{ABC}$$", "id": "3176621"}, {"introduction": "Often in regression analysis, we wish to compare the relative importance of predictors that are measured on different scales. Standardizing coefficients provides a way to make such comparisons by transforming them into unit-free quantities. This exercise [@problem_id:3176613] delves into the mechanics of this transformation, requiring you to derive the relationship between unstandardized coefficients ($\\hat{\\beta}_j$) and standardized coefficients ($\\hat{\\gamma}_j$) and, crucially, how their standard errors are related. This practice reinforces the fundamental properties of OLS estimators under linear rescaling of variables.", "problem": "A researcher fits a multiple linear regression using ordinary least squares (OLS) to predict a continuous outcome $Y$ from two predictors $X_{1}$ and $X_{2}$, with an intercept term. Assume the usual linear model conditions hold: $Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$, with $\\mathbb{E}[\\varepsilon \\mid X_{1}, X_{2}] = 0$ and $\\operatorname{Var}(\\varepsilon \\mid X_{1}, X_{2}) = \\sigma^{2}$, where $\\sigma^{2}$ is constant.\n\nFrom a sample of size $n$ (treat $n$ as fixed), the following empirical summaries are computed from the observed data:\n- Sample standard deviations: $s_{X_{1}} = 8$, $s_{X_{2}} = 0.5$, $s_{Y} = 15$.\n- Estimated unstandardized slopes and their reported standard errors: $\\hat{\\beta}_{1} = 2.50$ with $\\operatorname{SE}(\\hat{\\beta}_{1}) = 0.60$, and $\\hat{\\beta}_{2} = -5.40$ with $\\operatorname{SE}(\\hat{\\beta}_{2}) = 1.80$.\n\nDefine the standardized variables $Z_{Y} = (Y - \\bar{Y})/s_{Y}$, $Z_{1} = (X_{1} - \\bar{X}_{1})/s_{X_{1}}$, and $Z_{2} = (X_{2} - \\bar{X}_{2})/s_{X_{2}}$. Consider the regression of $Z_{Y}$ on $Z_{1}$ and $Z_{2}$ with intercept:\n$$\nZ_{Y} = \\gamma_{0} + \\gamma_{1} Z_{1} + \\gamma_{2} Z_{2} + \\text{error}.\n$$\n\nTasks:\n1. Starting from the OLS definition and the linearity of affine rescalings of regressors and response, derive an explicit analytic relationship between the vector of standardized slope coefficients $(\\gamma_{1}, \\gamma_{2})^{\\top}$ and the vector of unstandardized slope coefficients $(\\beta_{1}, \\beta_{2})^{\\top}$ in terms of $s_{X_{1}}, s_{X_{2}}, s_{Y}$. Your derivation should make clear why standardized slopes are unit-free.\n2. Using the sampling distribution of the OLS estimator under the stated assumptions, derive how the covariance matrix and, in particular, the standard errors of the standardized slopes $(\\hat{\\gamma}_{1}, \\hat{\\gamma}_{2})$ relate to those of $(\\hat{\\beta}_{1}, \\hat{\\beta}_{2})$, treating $s_{X_{1}}, s_{X_{2}}, s_{Y}$ as fixed scalars computed from the same sample.\n3. Using your result, compute the numerical value of the standard error of the standardized coefficient $\\hat{\\gamma}_{1}$ corresponding to $X_{1}$ from the summaries above.\n\nGive your final numerical answer for the standard error of $\\hat{\\gamma}_{1}$, rounded to four significant figures. Do not include any units in your final answer.", "solution": "The problem statement is evaluated for validity before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Model:** Multiple linear regression with intercept, $Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$.\n-   **Assumptions:** Ordinary Least Squares (OLS) conditions hold, including $\\mathbb{E}[\\varepsilon \\mid X_{1}, X_{2}] = 0$ and $\\operatorname{Var}(\\varepsilon \\mid X_{1}, X_{2}) = \\sigma^{2}$ (homoscedasticity).\n-   **Sample Size:** $n$.\n-   **Sample Standard Deviations:** $s_{X_{1}} = 8$, $s_{X_{2}} = 0.5$, $s_{Y} = 15$.\n-   **Estimated Unstandardized Coefficients:** $\\hat{\\beta}_{1} = 2.50$, $\\hat{\\beta}_{2} = -5.40$.\n-   **Standard Errors of Unstandardized Coefficients:** $\\operatorname{SE}(\\hat{\\beta}_{1}) = 0.60$, $\\operatorname{SE}(\\hat{\\beta}_{2}) = 1.80$.\n-   **Standardized Variables:** $Z_{Y} = (Y - \\bar{Y})/s_{Y}$, $Z_{1} = (X_{1} - \\bar{X}_{1})/s_{X_{1}}$, $Z_{2} = (X_{2} - \\bar{X}_{2})/s_{X_{2}}$.\n-   **Standardized Model:** $Z_{Y} = \\gamma_{0} + \\gamma_{1} Z_{1} + \\gamma_{2} Z_{2} + \\text{error}$.\n-   **Tasks:**\n    1.  Derive the relationship between standardized slopes $(\\gamma_{1}, \\gamma_{2})$ and unstandardized slopes $(\\beta_{1}, \\beta_{2})$.\n    2.  Derive the relationship between the standard errors of the standardized and unstandardized slope estimators.\n    3.  Compute the numerical value of $\\operatorname{SE}(\\hat{\\gamma}_{1})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n-   **Scientifically Grounded:** The problem is firmly rooted in the theory of linear regression, a core topic in statistics and statistical learning. All concepts, such as OLS, standardization, and standard errors, are standard and well-defined.\n-   **Well-Posed:** The problem provides all necessary data and relationships to derive the required expressions and compute the final value. The tasks are specified clearly, leading to a unique solution.\n-   **Objective:** The problem is stated in precise, mathematical language, free from any subjectivity or ambiguity.\n\nNo flaws are found. The problem is not scientifically unsound, non-formalizable, incomplete, unrealistic, or ill-posed. It is a standard, non-trivial exercise in understanding the properties of regression coefficients.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete solution will be provided.\n\nThe solution is presented in three parts, corresponding to the tasks in the problem statement.\n\n**1. Derivation of the relationship between standardized and unstandardized coefficients**\n\nWe begin with the unstandardized linear regression model for the population parameters:\n$$Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$$\nThe definitions of the standardized variables are $Z_{Y} = (Y - \\bar{Y})/s_{Y}$, $Z_{1} = (X_{1} - \\bar{X}_{1})/s_{X_{1}}$, and $Z_{2} = (X_{2} - \\bar{X}_{2})/s_{X_{2}}$. We can express the original variables in terms of the standardized variables and their sample means and standard deviations:\n$$Y = s_{Y} Z_{Y} + \\bar{Y}$$\n$$X_{1} = s_{X_{1}} Z_{1} + \\bar{X}_{1}$$\n$$X_{2} = s_{X_{2}} Z_{2} + \\bar{X}_{2}$$\nSubstituting these expressions into the unstandardized model equation yields:\n$$s_{Y} Z_{Y} + \\bar{Y} = \\beta_{0} + \\beta_{1} (s_{X_{1}} Z_{1} + \\bar{X}_{1}) + \\beta_{2} (s_{X_{2}} Z_{2} + \\bar{X}_{2}) + \\varepsilon$$\nTo derive the standardized model, we solve for $Z_{Y}$:\n$$s_{Y} Z_{Y} = (\\beta_{0} + \\beta_{1} \\bar{X}_{1} + \\beta_{2} \\bar{X}_{2} - \\bar{Y}) + (\\beta_{1} s_{X_{1}}) Z_{1} + (\\beta_{2} s_{X_{2}}) Z_{2} + \\varepsilon$$\n$$Z_{Y} = \\frac{\\beta_{0} + \\beta_{1} \\bar{X}_{1} + \\beta_{2} \\bar{X}_{2} - \\bar{Y}}{s_{Y}} + \\left(\\beta_{1} \\frac{s_{X_{1}}}{s_{Y}}\\right) Z_{1} + \\left(\\beta_{2} \\frac{s_{X_{2}}}{s_{Y}}\\right) Z_{2} + \\frac{\\varepsilon}{s_{Y}}$$\nThis equation has the form of the standardized regression model, $Z_{Y} = \\gamma_{0} + \\gamma_{1} Z_{1} + \\gamma_{2} Z_{2} + \\text{error'}$, where the new error term is $\\text{error'} = \\varepsilon/s_{Y}$. By comparing the coefficients, we establish the relationships:\n$$\\gamma_{1} = \\beta_{1} \\frac{s_{X_{1}}}{s_{Y}}$$\n$$\\gamma_{2} = \\beta_{2} \\frac{s_{X_{2}}}{s_{Y}}$$\nThe vector of standardized slope coefficients $(\\gamma_{1}, \\gamma_{2})^{\\top}$ relates to the unstandardized vector $(\\beta_{1}, \\beta_{2})^{\\top}$ via a diagonal scaling matrix composed of ratios of standard deviations.\n\nThese standardized coefficients, often called \"beta coefficients\" in some software packages, are unit-free. To see this, consider the units of the terms involved for $\\gamma_{1}$. Let $[Q]$ denote the units of a quantity $Q$. The units of the unstandardized slope $\\beta_1$ are $[Y]/[X_1]$. The units of the standard deviations are $[s_Y] = [Y]$ and $[s_{X_1}] = [X_1]$. Therefore, the units of $\\gamma_1$ are:\n$$[\\gamma_{1}] = [\\beta_{1}] \\frac{[s_{X_{1}}]}{[s_{Y}]} = \\frac{[Y]}{[X_{1}]} \\frac{[X_{1}]}{[Y]} = 1$$\nThe resulting coefficient is a dimensionless quantity. A similar argument applies to $\\gamma_{2}$.\n\n**2. Derivation of the relationship between standard errors**\n\nThe relationships derived above for the population parameters also hold for their OLS estimators due to the linearity of the estimation process. Thus, for the estimated coefficients:\n$$\\hat{\\gamma}_{1} = \\hat{\\beta}_{1} \\frac{s_{X_{1}}}{s_{Y}}$$\n$$\\hat{\\gamma}_{2} = \\hat{\\beta}_{2} \\frac{s_{X_{2}}}{s_{Y}}$$\nThe standard error of an estimator is the standard deviation of its sampling distribution, $\\operatorname{SE}(\\cdot) = \\sqrt{\\operatorname{Var}(\\cdot)}$. The problem specifies that the sample standard deviations $s_{X_{1}}, s_{X_{2}}, s_{Y}$ are to be treated as fixed, non-random scalars. This means we are finding the variance of the estimators conditional on these observed sample statistics.\n\nUsing the property of variance that for a random variable $X$ and a constant $c$, $\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X)$, we can find the variance of $\\hat{\\gamma}_{1}$:\n$$\\operatorname{Var}(\\hat{\\gamma}_{1}) = \\operatorname{Var}\\left(\\hat{\\beta}_{1} \\frac{s_{X_{1}}}{s_{Y}}\\right) = \\left(\\frac{s_{X_{1}}}{s_{Y}}\\right)^2 \\operatorname{Var}(\\hat{\\beta}_{1})$$\nTaking the square root of both sides gives the relationship between the standard errors. Since standard deviations are non-negative, the ratio $s_{X_1}/s_Y$ is also non-negative.\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\sqrt{\\left(\\frac{s_{X_{1}}}{s_{Y}}\\right)^2 \\operatorname{Var}(\\hat{\\beta}_{1})} = \\frac{s_{X_{1}}}{s_{Y}} \\sqrt{\\operatorname{Var}(\\hat{\\beta}_{1})} = \\frac{s_{X_{1}}}{s_{Y}} \\operatorname{SE}(\\hat{\\beta}_{1})$$\nAn analogous relationship holds for the second coefficient:\n$$\\operatorname{SE}(\\hat{\\gamma}_{2}) = \\frac{s_{X_{2}}}{s_{Y}} \\operatorname{SE}(\\hat{\\beta}_{2})$$\nMore generally, let $\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2})^{\\top}$ and $\\hat{\\boldsymbol{\\gamma}} = (\\hat{\\gamma}_{1}, \\hat{\\gamma}_{2})^{\\top}$. The relationship can be written in matrix form as $\\hat{\\boldsymbol{\\gamma}} = \\mathbf{S} \\hat{\\boldsymbol{\\beta}}$, where $\\mathbf{S}$ is the diagonal matrix:\n$$\\mathbf{S} = \\frac{1}{s_Y} \\begin{pmatrix} s_{X_1} & 0 \\\\ 0 & s_{X_2} \\end{pmatrix}$$\nThe covariance matrix of the standardized coefficients, $\\operatorname{Cov}(\\hat{\\boldsymbol{\\gamma}})$, is related to the covariance matrix of the unstandardized coefficients, $\\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}})$, by the rule $\\operatorname{Cov}(\\mathbf{A}\\mathbf{X}) = \\mathbf{A}\\operatorname{Cov}(\\mathbf{X})\\mathbf{A}^{\\top}$:\n$$\\operatorname{Cov}(\\hat{\\boldsymbol{\\gamma}}) = \\mathbf{S} \\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{S}^{\\top} = \\mathbf{S} \\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{S}$$\nThe diagonal elements of $\\operatorname{Cov}(\\hat{\\boldsymbol{\\gamma}})$ are the variances $\\operatorname{Var}(\\hat{\\gamma}_{1})$ and $\\operatorname{Var}(\\hat{\\gamma}_{2})$, which confirms the component-wise derivation above.\n\n**3. Numerical computation of $\\operatorname{SE}(\\hat{\\gamma}_{1})$**\n\nUsing the formula derived in Part 2 and the data provided in the problem statement:\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\frac{s_{X_{1}}}{s_{Y}} \\operatorname{SE}(\\hat{\\beta}_{1})$$\nThe given values are $s_{X_{1}} = 8$, $s_{Y} = 15$, and $\\operatorname{SE}(\\hat{\\beta}_{1}) = 0.60$. Substituting these values:\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\frac{8}{15} \\times 0.60$$\nThe calculation is as follows:\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\frac{8}{15} \\times \\frac{60}{100} = \\frac{8 \\times 4}{100} = \\frac{32}{100} = 0.32$$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $0.32$. To express this with four significant figures, we append two zeros.\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = 0.3200$$", "answer": "$$\\boxed{0.3200}$$", "id": "3176613"}]}