{"hands_on_practices": [{"introduction": "The magnitude of a regression coefficient is directly tied to the units of its predictor. This exercise demonstrates how to correctly adjust a coefficient when changing a predictor's scale—for instance, from centimeters to meters—and verifies the essential principle that the model's predictions for a physical quantity must remain invariant. This practice is crucial for interpreting and comparing the effects of variables measured on different scales [@problem_id:3132949].", "problem": "A researcher fits a multiple linear regression by ordinary least squares (OLS) to model systolic blood pressure as a function of height and age. The response is systolic blood pressure $y$ in $\\mathrm{mmHg}$, and the predictors are height $x_{1}$ in centimeters and age $x_{2}$ in years. The fitted model using centimeters is\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\,x_{1} \\;+\\; \\beta_{2}\\,x_{2} \\;+\\; \\varepsilon,\n$$\nwith estimated coefficients\n$$\n\\hat{\\beta}_{0} \\;=\\; 50,\\quad \\hat{\\beta}_{1} \\;=\\; 0.12,\\quad \\hat{\\beta}_{2} \\;=\\; 0.7.\n$$\nThe researcher then decides to report results using meters for height, defining $z_{1} \\equiv x_{1}/100$, and to write the model as\n$$\ny \\;=\\; \\gamma_{0} \\;+\\; \\gamma_{1}\\,z_{1} \\;+\\; \\gamma_{2}\\,x_{2} \\;+\\; \\varepsilon.\n$$\nTasks:\n- Starting from the definition of the multiple linear regression model and the principle that fitted predictions for the same physical individual must be invariant to a change of measurement units, derive how the coefficients transform when replacing $x_{1}$ by $z_{1} \\equiv x_{1}/100$. Use this to compute the numerical value of $\\hat{\\gamma}_{1}$.\n- For a person with height $x_{1} = 180$ centimeters (that is, $z_{1} = 1.80$ meters) and age $x_{2} = 50$ years, compute the fitted value $\\hat{y}$ using both the centimeter and meter representations, and verify equality.\n\nReport as your final answer only the value of the rescaled slope coefficient $\\hat{\\gamma}_{1}$, with no units. No rounding is required.", "solution": "### Solution Derivation\nLet the first fitted model, using height in centimeters (`x_1`), be denoted `M_1`, and the second fitted model, using height in meters (`z_1`), be denoted `M_2`.\n\nThe equation for the predicted systolic blood pressure, `\\hat{y}`, from model `M_1` is:\n$$\n\\hat{y}_{M_1} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2}\n$$\nThe equation for the predicted value from model `M_2` is:\n$$\n\\hat{y}_{M_2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\nThe core principle stated in the problem is that the predicted value for any given individual must be independent of the units used for the predictors. This is a principle of physical invariance. Therefore, for any individual with height `x_1` (in cm) or `z_1` (in m) and age `x_2`, the predicted values must be equal:\n$$\n\\hat{y}_{M_1} = \\hat{y}_{M_2}\n$$\nThis implies:\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\nWe are given the relationship between `x_1` and `z_1`: `z_{1} = x_{1}/100`, which is equivalent to `x_{1} = 100z_{1}`. To relate the coefficients, we substitute the definition of `z_1` into the right-hand side of the equality:\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}\\left(\\frac{x_{1}}{100}\\right) + \\hat{\\gamma}_{2}x_{2}\n$$\nLet us rearrange the right-hand side to group terms by the original predictors `x_1` and `x_2`:\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\left(\\frac{\\hat{\\gamma}_{1}}{100}\\right)x_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\nFor this identity to hold true for all possible values of the predictors `x_1` and `x_2`, the coefficients of the corresponding terms on both sides of the equation must be equal. This is a direct application of the principle of equating coefficients of polynomials.\n\nBy comparing the constant terms (intercepts), we find:\n$$\n\\hat{\\beta}_{0} = \\hat{\\gamma}_{0}\n$$\nBy comparing the coefficients of the `x_1` term, we find:\n$$\n\\hat{\\beta}_{1} = \\frac{\\hat{\\gamma}_{1}}{100}\n$$\nBy comparing the coefficients of the `x_2` term, we find:\n$$\n\\hat{\\beta}_{2} = \\hat{\\gamma}_{2}\n$$\nFrom the second equality, we can solve for `\\hat{\\gamma}_{1}`:\n$$\n\\hat{\\gamma}_{1} = 100 \\cdot \\hat{\\beta}_{1}\n$$\nUsing the given numerical value `\\hat{\\beta}_{1} = 0.12`, we compute `\\hat{\\gamma}_{1}`:\n$$\n\\hat{\\gamma}_{1} = 100 \\cdot 0.12 = 12\n$$\nThe other coefficients for the model in meters are `\\hat{\\gamma}_{0} = \\hat{\\beta}_{0} = 50` and `\\hat{\\gamma}_{2} = \\hat{\\beta}_{2} = 0.7`.\n\n### Verification\nWe now verify that the fitted value `\\hat{y}` is the same for an individual with height `x_1 = 180` cm and age `x_2 = 50` years. The corresponding height in meters is `z_1 = 180 / 100 = 1.80` m.\n\nUsing the first model (centimeters):\n$$\n\\hat{y}_{M_1} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = 50 + (0.12)(180) + (0.7)(50)\n$$\n$$\n\\hat{y}_{M_1} = 50 + 21.6 + 35 = 106.6\n$$\nUsing the second model (meters):\n$$\n\\hat{y}_{M_2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2} = 50 + (12)(1.80) + (0.7)(50)\n$$\n$$\n\\hat{y}_{M_2} = 50 + 21.6 + 35 = 106.6\n$$\nThe predicted values are identical, `\\hat{y}_{M_1} = \\hat{y}_{M_2} = 106.6`, which verifies the correctness of our derived coefficient transformation. The question asks for the numerical value of `\\hat{\\gamma}_{1}`.", "answer": "$$\n\\boxed{12}\n$$", "id": "3132949"}, {"introduction": "When working with categorical predictors, regression coefficients are always interpreted relative to a chosen baseline level. This exercise explores how changing this baseline alters individual coefficient values and even their signs, yet preserves the actual predicted outcomes and the essential contrasts between categories. Understanding this invariance is key to correctly interpreting models with categorical data and recognizing which aspects of the model are truly meaningful [@problem_id:3132938].", "problem": "A clinical dataset is modeled using multiple linear regression to study how systolic blood pressure, measured in millimeters of mercury (mmHg), depends on medication type and a continuous baseline risk score. Let the outcome be denoted by $y$ (mmHg). There are three mutually exclusive medication categories: $A$, $B$, and $C$. The baseline risk score is a continuous predictor denoted by $x$ (dimensionless). The model includes an intercept and no interaction terms. The categorical medication predictor is represented using standard indicator variables whose choice of baseline category is altered between two parameterizations of the same fitted model.\n\nParameterization $1$ uses medication $A$ as the baseline and employs indicators $I(B)$ and $I(C)$, defined by $I(B)=1$ if the patient received medication $B$ and $I(B)=0$ otherwise, and similarly for $I(C)$. The fitted model is\n$$\ny \\;=\\; b_{0} \\;+\\; b_{B}\\,I(B) \\;+\\; b_{C}\\,I(C) \\;+\\; \\gamma\\,x,\n$$\nwith estimated coefficients $b_{0}=130$, $b_{B}=-6$, $b_{C}=9$, and $\\gamma=1.8$.\n\nParameterization $2$ uses medication $B$ as the baseline and employs indicators $I(A)$ and $I(C)$, defined analogously. The fitted model is\n$$\ny \\;=\\; a_{0} \\;+\\; a_{A}\\,I(A) \\;+\\; a_{C}\\,I(C) \\;+\\; \\gamma\\,x,\n$$\nwith estimated coefficients $a_{0}=124$, $a_{A}=6$, $a_{C}=15$, and $\\gamma=1.8$.\n\nBoth parameterizations were obtained by refitting the same model to the same data using standard indicator coding with a single intercept and no interactions, so they produce identical fitted values and predictions for any $(\\text{medication}, x)$ combination.\n\nStarting from the definition of multiple linear regression and the construction of indicator variables for categorical predictors, derive the predicted category means under both parameterizations and formally justify which numerical quantity governs the contrast between two medication categories. Then, for the specific baseline risk score $x=20$, compute the predicted contrast in systolic blood pressure between medications $C$ and $A$, namely $y_{C}-y_{A}$, and report its numerical value. Express the final contrast in millimeters of mercury (mmHg). No rounding is required; provide the exact value. Finally, interpret why this computed contrast is invariant to the choice of baseline category even though the sign of some individual coefficients changes with the baseline choice.", "solution": "The analysis begins by formalizing the structure of the multiple linear regression model under the two specified parameterizations. In this model, the expected value of the outcome, systolic blood pressure ($y$), is a linear function of a continuous predictor, the baseline risk score ($x$), and a three-level categorical predictor, the medication type ($A$, $B$, or $C$). The model does not include interaction terms, which implies that the effect of the risk score $x$ on $y$ is assumed to be constant across all medication categories, and the difference in the effect of any two medications is constant across all values of $x$.\n\nFirst, we analyze Parameterization $1$, where medication $A$ serves as the baseline category. The model is given by:\n$$\n\\hat{y} = b_{0} + b_{B}I(B) + b_{C}I(C) + \\gamma x\n$$\nHere, $I(B)$ is an indicator variable that equals $1$ if the medication is $B$ and $0$ otherwise. Similarly, $I(C)$ is $1$ if the medication is $C$ and $0$ otherwise. For the baseline category $A$, both $I(B)$ and $I(C)$ are $0$.\n\nThe estimated coefficients are given as $b_{0}=130$, $b_{B}=-6$, $b_{C}=9$, and $\\gamma=1.8$.\n\nFrom this model, we can express the predicted mean systolic blood pressure for each medication category as a function of the risk score $x$:\n- For medication $A$ ($I(B)=0$, $I(C)=0$): $\\hat{y}_{A}(x) = b_{0} + \\gamma x = 130 + 1.8x$\n- For medication $B$ ($I(B)=1$, $I(C)=0$): $\\hat{y}_{B}(x) = b_{0} + b_{B} + \\gamma x = 130 - 6 + 1.8x = 124 + 1.8x$\n- For medication $C$ ($I(B)=0$, $I(C)=1$): $\\hat{y}_{C}(x) = b_{0} + b_{C} + \\gamma x = 130 + 9 + 1.8x = 139 + 1.8x$\n\nIn this parameterization, the intercept $b_{0}$ represents the predicted blood pressure for the baseline group (medication $A$) when the continuous predictor $x$ is zero. The coefficient $b_{B}$ represents the difference in predicted blood pressure between medication $B$ and medication $A$, i.e., $\\hat{y}_{B}(x) - \\hat{y}_{A}(x) = -6$ mmHg. Similarly, $b_{C}$ is the difference between medication $C$ and medication $A$, i.e., $\\hat{y}_{C}(x) - \\hat{y}_{A}(x) = 9$ mmHg.\n\nNext, we analyze Parameterization $2$, where medication $B$ serves as the baseline category. The model is given by:\n$$\n\\hat{y} = a_{0} + a_{A}I(A) + a_{C}I(C) + \\gamma x\n$$\nThe estimated coefficients are given as $a_{0}=124$, $a_{A}=6$, $a_{C}=15$, and $\\gamma=1.8$.\n\nThe predicted mean systolic blood pressure for each medication category is:\n- For medication $A$ ($I(A)=1$, $I(C)=0$): $\\hat{y}_{A}(x) = a_{0} + a_{A} + \\gamma x = 124 + 6 + 1.8x = 130 + 1.8x$\n- For medication $B$ ($I(A)=0$, $I(C)=0$): $\\hat{y}_{B}(x) = a_{0} + \\gamma x = 124 + 1.8x$\n- For medication $C$ ($I(A)=0$, $I(C)=1$): $\\hat{y}_{C}(x) = a_{0} + a_{C} + \\gamma x = 124 + 15 + 1.8x = 139 + 1.8x$\n\nAs expected, since both models represent the same fitted relationship to the same data, the predicted values ($\\hat{y}_{A}(x)$, $\\hat{y}_{B}(x)$, and $\\hat{y}_{C}(x)$) are identical under both parameterizations.\n\nThe quantity that governs the contrast between two medication categories is the difference in their predicted mean outcomes. For example, the contrast between medications $C$ and $A$ is defined as $\\hat{y}_{C}(x) - \\hat{y}_{A}(x)$. This difference is an estimable quantity, meaning its value is invariant to the specific choice of non-singular parameterization (i.e., the choice of baseline category).\n\nThe problem asks for the numerical value of the predicted contrast in systolic blood pressure between medications $C$ and $A$, denoted $y_{C}-y_{A}$, for a baseline risk score of $x=20$. We can compute this using either parameterization.\n\nUsing Parameterization $1$:\nThe contrast is directly given by the coefficient $b_{C}$, which represents the difference between category $C$ and the baseline category $A$.\n$$\n\\hat{y}_{C}(x) - \\hat{y}_{A}(x) = (b_{0} + b_{C} + \\gamma x) - (b_{0} + \\gamma x) = b_{C}\n$$\nThe value of this contrast is $b_C = 9$. Since there are no interaction terms with $x$, this contrast is constant for all values of $x$. Thus, for $x=20$, the contrast is $9$ mmHg.\n\nUsing Parameterization $2$:\nThe contrast must be calculated as the difference between the linear combinations of coefficients that define the means for categories $C$ and $A$.\n$$\n\\hat{y}_{C}(x) - \\hat{y}_{A}(x) = (a_{0} + a_{C} + \\gamma x) - (a_{0} + a_{A} + \\gamma x) = a_{C} - a_{A}\n$$\nSubstituting the given coefficient values:\n$$\na_{C} - a_{A} = 15 - 6 = 9\n$$\nThis confirms that the contrast is $9$ mmHg, regardless of the parameterization used. The value is also independent of the specific value of $x=20$.\n\nThe reason this contrast is invariant to the choice of baseline category is that the coefficients for the indicator variables always represent a comparison relative to the *current* baseline. Changing the baseline changes the reference point, and thus changes the coefficient values, but the underlying differences between any two categories remain constant.\n- With baseline $A$, $b_C$ represents the contrast $(C-A)$.\n- With baseline $B$, $a_A$ represents $(A-B)$ and $a_C$ represents $(C-B)$. The contrast $(C-A)$ can be recovered as $(C-B) - (A-B) = a_C - a_A$.\nThese relationships algebraically demonstrate the invariance. For instance, $a_A = \\hat{y}_A(x) - \\hat{y}_B(x) = (\\hat{y}_A(x) - \\hat{y}_A(x)) - (\\hat{y}_B(x) - \\hat{y}_A(x)) = 0 - b_B = -b_B$. Indeed, $6 = -(-6)$. Likewise, $a_C = \\hat{y}_C(x) - \\hat{y}_B(x) = (\\hat{y}_C(x) - \\hat{y}_A(x)) - (\\hat{y}_B(x) - \\hat{y}_A(x)) = b_C - b_B$. Indeed, $15 = 9 - (-6)$.\nThe contrast $(C-A)$ is an estimable function, and its estimate is unique. Individual coefficients like $b_C$ or $a_C$ are not, by themselves, estimable quantities without reference to a specific model parameterization; their values and interpretations depend on the chosen baseline. The sign change observed in $b_B = -6$ versus $a_A = 6$ is a direct consequence of reversing the direction of comparison: $b_B$ estimates $(B-A)$ while $a_A$ estimates $(A-B)$, so it is necessary that $a_A = -b_B$. The fundamental difference between the effects of medications $A$ and $B$ is preserved.\n\nThe final requested value is the predicted contrast in systolic blood pressure between medications $C$ and $A$, which is $9$ mmHg.", "answer": "$$\n\\boxed{9}\n$$", "id": "3132938"}, {"introduction": "One of the most powerful, and sometimes perplexing, aspects of multiple regression is how the relationship between a predictor and the outcome can change after accounting for other variables. This exercise reveals the phenomenon of suppression, where a variable's coefficient can flip its sign when a correlated predictor is included in the model. By deriving this effect from fundamental principles, you will gain a deeper understanding of the crucial distinction between marginal and partial associations [@problem_id:3132937].", "problem": "Consider a population in which the pair $\\{x_1, x_2\\}$ of predictors and the response $y$ are jointly distributed with finite second moments. The predictors are standardized so that $\\operatorname{E}[x_1] = \\operatorname{E}[x_2] = 0$ and $\\operatorname{Var}(x_1) = \\operatorname{Var}(x_2) = 1$. Suppose the predictors are positively associated, with $\\operatorname{Cov}(x_1, x_2) = \\rho$ for some $0 < \\rho < 1$, and the response has zero mean, $\\operatorname{E}[y] = 0$. Empirical investigation in this population yields the following covariances: $\\operatorname{Cov}(x_1, y) = 0.2$ and $\\operatorname{Cov}(x_2, y) = 0.8$. \n\nWe fit the multiple linear model $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$ by ordinary least squares (OLS), interpreted as the linear projection of $y$ onto the span of $\\{x_1, x_2\\}$ that minimizes the expected squared residual. Starting from the fundamental definitions of covariance, variance, and the least-squares optimality (normal equations) for linear projection, derive the coefficient $\\beta_1$ as a function of $\\rho$ and then evaluate it at $\\rho = 0.9$. In your derivation, make clear why the marginal association between $x_1$ and $y$ being positive does not preclude a negative partial coefficient $\\beta_1$ once $x_2$ is controlled (suppression), and explain this sign reversal in geometric terms using orthogonal projections.\n\nCompute the numerical value of $\\beta_1$ at $\\rho = 0.9$ and round your answer to four significant figures. No units are required for this coefficient.", "solution": "### Derivation of the Coefficient $\\beta_1$\nThe OLS procedure minimizes the expected squared residual, $S = \\operatorname{E}[\\varepsilon^2] = \\operatorname{E}[(y - \\beta_0 - \\beta_1 x_1 - \\beta_2 x_2)^2]$.\n\nFirst, we determine the intercept $\\beta_0$. Taking the expectation of the model equation:\n$$ \\operatorname{E}[y] = \\operatorname{E}[\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon] = \\beta_0 + \\beta_1 \\operatorname{E}[x_1] + \\beta_2 \\operatorname{E}[x_2] + \\operatorname{E}[\\varepsilon] $$\nBy construction, OLS ensures that the expected value of the residual is zero, $\\operatorname{E}[\\varepsilon] = 0$. Using the given zero-mean conditions, we have:\n$$ 0 = \\beta_0 + \\beta_1(0) + \\beta_2(0) + 0 $$\nThis implies $\\beta_0 = 0$. The model simplifies to $y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$.\n\nThe minimization of $S = \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)^2]$ leads to the normal equations, which state that the residual vector must be orthogonal to each predictor vector. In terms of expectation, this means $\\operatorname{E}[\\varepsilon x_1] = 0$ and $\\operatorname{E}[\\varepsilon x_2] = 0$.\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_1] = 0 \\implies \\operatorname{E}[yx_1] - \\beta_1 \\operatorname{E}[x_1^2] - \\beta_2 \\operatorname{E}[x_1x_2] = 0 $$\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_2] = 0 \\implies \\operatorname{E}[yx_2] - \\beta_1 \\operatorname{E}[x_1x_2] - \\beta_2 \\operatorname{E}[x_2^2] = 0 $$\nGiven that all variables have zero mean, the expectations of products are equal to covariances, and the expectation of squares are equal to variances:\n- $\\operatorname{E}[yx_1] = \\operatorname{Cov}(x_1, y) = 0.2$\n- $\\operatorname{E}[yx_2] = \\operatorname{Cov}(x_2, y) = 0.8$\n- $\\operatorname{E}[x_1^2] = \\operatorname{Var}(x_1) = 1$\n- $\\operatorname{E}[x_2^2] = \\operatorname{Var}(x_2) = 1$\n- $\\operatorname{E}[x_1x_2] = \\operatorname{Cov}(x_1, x_2) = \\rho$\n\nSubstituting these into the normal equations gives the following system of linear equations for $\\beta_1$ and $\\beta_2$:\n$$ \\begin{cases} (1) \\quad 1 \\cdot \\beta_1 + \\rho \\cdot \\beta_2 = 0.2 \\\\ (2) \\quad \\rho \\cdot \\beta_1 + 1 \\cdot \\beta_2 = 0.8 \\end{cases} $$\nWe can solve this system for $\\beta_1$. From equation (2), we express $\\beta_2$ in terms of $\\beta_1$:\n$$ \\beta_2 = 0.8 - \\rho \\beta_1 $$\nSubstituting this into equation (1):\n$$ \\beta_1 + \\rho(0.8 - \\rho \\beta_1) = 0.2 $$\n$$ \\beta_1 + 0.8\\rho - \\rho^2 \\beta_1 = 0.2 $$\n$$ \\beta_1(1 - \\rho^2) = 0.2 - 0.8\\rho $$\nThis yields the expression for $\\beta_1$ as a function of $\\rho$:\n$$ \\beta_1 = \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2} $$\n\n### Suppression Effect and Geometric Interpretation\nThe marginal association between $x_1$ and $y$ is given by $\\operatorname{Cov}(x_1, y) = 0.2$, which is positive. However, the partial coefficient $\\beta_1$ can be negative. This phenomenon is a form of suppression. The condition for $\\beta_1$ to be negative is:\n$$ \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2} < 0 $$\nSince $0 < \\rho < 1$, the denominator $1 - \\rho^2$ is positive. Thus, the sign of $\\beta_1$ is determined by the numerator:\n$$ 0.2 - 0.8 \\rho < 0 \\implies 0.2 < 0.8 \\rho \\implies \\rho > \\frac{0.2}{0.8} = 0.25 $$\nSo, if $\\rho > 0.25$, the partial effect of $x_1$ on $y$ is negative, despite their positive marginal covariance.\n\n**Conceptual Explanation:**\nThe coefficient $\\beta_1$ represents the expected change in $y$ for a one-unit change in $x_1$, while holding $x_2$ constant. The predictor $x_2$ has a strong positive association with $y$ ($\\operatorname{Cov}(x_2, y) = 0.8$), and $x_1$ is also positively correlated with $x_2$ ($\\rho > 0$). A portion of the observed positive association between $x_1$ and $y$ is mediated through $x_2$: an increase in $x_1$ is associated with an increase in $x_2$, which in turn is associated with a strong increase in $y$. Multiple regression \"partials out\" this indirect effect. If the indirect positive association via $x_2$ is sufficiently strong (i.e., $\\rho$ is large enough), it can mask an underlying negative direct association between $x_1$ and $y$.\n\n**Geometric Interpretation:**\nIn the vector space of zero-mean random variables, the OLS coefficient $\\beta_1$ can be understood through orthogonal projections, as specified by the Frisch-Waugh-Lovell theorem. The coefficient $\\beta_1$ is the regression coefficient of $y$ on the part of $x_1$ that is orthogonal to $x_2$. Let $x_{1|2}$ be the residual from regressing $x_1$ on $x_2$:\n$$ x_{1|2} = x_1 - \\frac{\\operatorname{Cov}(x_1, x_2)}{\\operatorname{Var}(x_2)} x_2 = x_1 - \\rho x_2 $$\nThen $\\beta_1$ is given by the simple regression of $y$ on $x_{1|2}$:\n$$ \\beta_1 = \\frac{\\operatorname{Cov}(y, x_{1|2})}{\\operatorname{Var}(x_{1|2})} $$\nThe numerator is:\n$$ \\operatorname{Cov}(y, x_1 - \\rho x_2) = \\operatorname{Cov}(y, x_1) - \\rho \\operatorname{Cov}(y, x_2) = 0.2 - \\rho(0.8) $$\nThe denominator is:\n$$ \\operatorname{Var}(x_1 - \\rho x_2) = \\operatorname{Var}(x_1) - 2\\rho \\operatorname{Cov}(x_1, x_2) + \\rho^2 \\operatorname{Var}(x_2) = 1 - 2\\rho(\\rho) + \\rho^2(1) = 1 - \\rho^2 $$\nThus, $\\beta_1 = \\frac{0.2 - 0.8\\rho}{1 - \\rho^2}$, confirming our derivation. Geometrically, $\\beta_1$ is not determined by the projection of the vector $y$ onto $x_1$ (which is positive), but by the projection of $y$ onto the vector $x_{1|2}$ (the part of $x_1$ orthogonal to $x_2$). If $\\rho$ is large and positive, the vectors $x_1$ and $x_2$ are close in direction. The vector $x_{1|2}$ represents the \"correction\" needed to go from $\\rho x_2$ to $x_1$. The projection of $y$ onto this \"correction\" vector can be negative even if the projection of $y$ onto $x_1$ is positive. This happens when the strong pull of $y$ towards $x_2$ (due to $\\operatorname{Cov}(y, x_2) = 0.8$) is so dominant that the vector component required for the residual part of $x_1$ becomes negative.\n\n### Numerical Calculation\nWe evaluate $\\beta_1$ at $\\rho = 0.9$:\n$$ \\beta_1 = \\frac{0.2 - 0.8(0.9)}{1 - (0.9)^2} = \\frac{0.2 - 0.72}{1 - 0.81} = \\frac{-0.52}{0.19} $$\n$$ \\beta_1 \\approx -2.736842... $$\nRounding to four significant figures, we get $\\beta_1 = -2.737$.\nThe result confirms the suppression effect: with a high correlation of $\\rho = 0.9$ (which is $> 0.25$), the partial coefficient $\\beta_1$ is indeed negative.", "answer": "$$\\boxed{-2.737}$$", "id": "3132937"}]}