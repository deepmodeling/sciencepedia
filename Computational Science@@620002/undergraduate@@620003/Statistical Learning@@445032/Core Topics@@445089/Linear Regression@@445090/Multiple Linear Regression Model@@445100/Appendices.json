{"hands_on_practices": [{"introduction": "At the heart of multiple linear regression lies the method of Ordinary Least Squares (OLS). The goal of OLS is to find the specific set of coefficients, denoted $\\hat{\\boldsymbol{\\beta}}$, that best fits the data by minimizing the sum of squared differences between observed and predicted outcomes. This exercise [@problem_id:1938980] provides fundamental practice in applying the core matrix formula of OLS, $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$, allowing you to see how model parameters are derived directly from the data.", "problem": "A food scientist is developing a new type of baked snack and wants to understand how baking conditions affect its crispiness. The crispiness is measured on a quantitative scale. The scientist conducts a small experiment with four batches, varying two factors: baking temperature and humidity. The factors are represented by coded variables, where $-1$ represents a low setting and $+1$ represents a high setting.\n\nThe proposed statistical model is a multiple linear regression model of the form:\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\nwhere:\n- $y$ is the crispiness score.\n- $x_1$ is the coded variable for baking temperature.\n- $x_2$ is the coded variable for humidity.\n- $\\beta_0$, $\\beta_1$, and $\\beta_2$ are the unknown model coefficients.\n- $\\epsilon$ is the random error term.\n\nThe results from the four experimental batches are as follows:\n- Batch 1: Temperature code ($x_1$) = -1, Humidity code ($x_2$) = -1, Crispiness ($y$) = 2.\n- Batch 2: Temperature code ($x_1$) = -1, Humidity code ($x_2$) = 1, Crispiness ($y$) = 4.\n- Batch 3: Temperature code ($x_1$) = 1, Humidity code ($x_2$) = -1, Crispiness ($y$) = 6.\n- Batch 4: Temperature code ($x_1$) = 1, Humidity code ($x_2$) = 1, Crispiness ($y$) = 8.\n\nUsing the method of least squares, determine the estimated values for the coefficients. Present your answer as a single row matrix containing the three estimated coefficients in the order $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$.", "solution": "The goal is to find the least squares estimates for the coefficients $\\beta_0$, $\\beta_1$, and $\\beta_2$ in the multiple linear regression model. The model can be written in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is the vector of responses, $\\mathbf{X}$ is the design matrix, $\\boldsymbol{\\beta}$ is the vector of coefficients, and $\\boldsymbol{\\epsilon}$ is the vector of errors.\n\nFirst, we construct the response vector $\\mathbf{y}$ and the design matrix $\\mathbf{X}$ from the given experimental data. The design matrix includes a column of ones for the intercept term $\\beta_0$.\n\nThe response vector $\\mathbf{y}$ contains the crispiness scores:\n$$\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix}\n$$\n\nThe design matrix $\\mathbf{X}$ is constructed with a leading column of ones for the intercept, followed by the columns for the coded variables $x_1$ and $x_2$:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  x_{11}  x_{21} \\\\ 1  x_{12}  x_{22} \\\\ 1  x_{13}  x_{23} \\\\ 1  x_{14}  x_{24} \\end{pmatrix} = \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix}\n$$\n\nThe vector of coefficients to be estimated is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$. The least squares estimate, denoted by $\\hat{\\boldsymbol{\\beta}}$, is given by the solution to the normal equations $\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$. Assuming $\\mathbf{X}^T\\mathbf{X}$ is invertible, the solution is:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\nWe will compute this in several steps. First, we find the transpose of $\\mathbf{X}$:\n$$\n\\mathbf{X}^T = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix}\n$$\n\nNext, we compute the product $\\mathbf{X}^T\\mathbf{X}$:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix}\n$$\nThe elements of the resulting matrix are:\n- $(1,1): 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(1,2): 1 \\cdot (-1) + 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(1,3): 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(2,1): (-1) \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(2,2): (-1) \\cdot (-1) + (-1) \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(2,3): (-1) \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(3,1): (-1) \\cdot 1 + 1 \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,2): (-1) \\cdot (-1) + 1 \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,3): (-1) \\cdot (-1) + 1 \\cdot 1 + (-1) \\cdot (-1) + 1 \\cdot 1 = 4$\n\nSo, the matrix is diagonal:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix} = 4\\mathbf{I}_3\n$$\nwhere $\\mathbf{I}_3$ is the $3 \\times 3$ identity matrix.\n\nThe inverse of this diagonal matrix is straightforward to compute:\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{4}\\mathbf{I}_3 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix}\n$$\n\nNow, we compute the product $\\mathbf{X}^T\\mathbf{y}$:\n$$\n\\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 1(2)+1(4)+1(6)+1(8) \\\\ -1(2)-1(4)+1(6)+1(8) \\\\ -1(2)+1(4)-1(6)+1(8) \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\n\nFinally, we multiply $(\\mathbf{X}^T\\mathbf{X})^{-1}$ by $\\mathbf{X}^T\\mathbf{y}$ to find $\\hat{\\boldsymbol{\\beta}}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}(20) \\\\ \\frac{1}{4}(8) \\\\ \\frac{1}{4}(4) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n\nThe least squares estimates are $\\hat{\\beta}_0 = 5$, $\\hat{\\beta}_1 = 2$, and $\\hat{\\beta}_2 = 1$. The problem asks for the answer as a row matrix $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 5  2  1 \\end{pmatrix}}\n$$", "id": "1938980"}, {"introduction": "A powerful model is not just about crunching numbers; it must be built on a solid theoretical foundation, a crucial aspect of which is 'identifiability.' This property ensures that a unique set of coefficients can be estimated from the data. This thought experiment [@problem_id:3152062] guides you through the 'dummy variable trap,' a classic example of perfect multicollinearity that violates identifiability, forcing you to think critically about how to correctly specify a model with categorical variables.", "problem": "Consider the multiple linear regression model $y = \\beta_{0} + \\beta_{1} x + \\gamma_{1} D_{1} + \\gamma_{2} D_{2} + \\gamma_{3} D_{3} + \\varepsilon$, where $y$ is the response, $x$ is a continuous predictor, $D_{j}$ are dummy variables for a categorical predictor with $3$ levels (each observation belongs to exactly one level so that for every row $D_{1} + D_{2} + D_{3} = 1$), and $\\varepsilon$ is an error term. The model includes an intercept ($\\beta_{0}$) and all $3$ dummy variables. Suppose we have $n = 6$ observations and each of the $3$ levels appears at least once (so that the dummy columns are nonzero).\n\nFrom first principles of multiple linear regression and Ordinary Least Squares (OLS), identifiability of the parameter vector requires the design matrix $X$ to have full column rank, which is equivalent to $X^{\\top} X$ being invertible. Based on this setup, select all statements that are correct.\n\nA. $X^{\\top} X$ is singular because the intercept column equals the sum of the dummy variable columns. Dropping one dummy variable (and keeping the intercept) makes $X^{\\top} X$ nonsingular provided each level appears at least once.\n\nB. Centering the continuous predictor $x$ at its sample mean eliminates the singularity of $X^{\\top} X$, so no dummy variable or intercept needs to be dropped.\n\nC. Removing the intercept while keeping all $3$ dummy variables yields a design matrix with full column rank, provided each level appears at least once.\n\nD. Keeping the intercept and all $3$ dummy variables but imposing the linear constraint $\\gamma_{1} + \\gamma_{2} + \\gamma_{3} = 0$ during estimation yields a unique solution without dropping any columns.\n\nE. Replacing $X^{\\top} X$ by $X^{\\top} X + \\lambda I$ with $\\lambda > 0$ is equivalent to making the model identifiable in the ordinary least squares sense; hence the unconstrained OLS estimates become unique and unchanged by the regularization.", "solution": "The problem statement describes a multiple linear regression model and asks for correct statements regarding the identifiability of its parameters under Ordinary Least Squares (OLS) estimation.\n\nThe model is given by:\n$$y = \\beta_{0} + \\beta_{1} x + \\gamma_{1} D_{1} + \\gamma_{2} D_{2} + \\gamma_{3} D_{3} + \\varepsilon$$\nwhere $y$ is the response, $x$ is a continuous predictor, and $D_1, D_2, D_3$ are dummy variables for a categorical predictor with $3$ levels. The problem specifies that the model includes an intercept term with coefficient $\\beta_0$.\n\nFor a set of $n=6$ observations, the model can be written in matrix form as $\\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $6 \\times 1$ vector of responses, $X$ is the $6 \\times 5$ design matrix, $\\boldsymbol{\\beta}$ is the $5 \\times 1$ vector of parameters, and $\\boldsymbol{\\varepsilon}$ is the $6 \\times 1$ vector of errors.\nThe parameter vector is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\gamma_1, \\gamma_2, \\gamma_3]^\\top$.\nThe design matrix $X$ consists of $5$ columns: one for the intercept, one for the continuous predictor $x$, and one for each of the $3$ dummy variables. Let these columns be denoted as $\\mathbf{1}$ (a vector of ones), $\\mathbf{x}$, $\\mathbf{d}_1$, $\\mathbf{d}_2$, and $\\mathbf{d}_3$.\n$$X = \\begin{bmatrix} \\mathbf{1}  \\mathbf{x}  \\mathbf{d}_1  \\mathbf{d}_2  \\mathbf{d}_3 \\end{bmatrix}$$\nThe problem states that for each observation, an individual belongs to exactly one of the $3$ levels of the categorical predictor. This implies that for each row $i$ of the data, $D_{i1} + D_{i2} + D_{i3} = 1$.\nThis has a critical consequence for the columns of the design matrix. If we sum the columns corresponding to the dummy variables, we get:\n$$\\mathbf{d}_1 + \\mathbf{d}_2 + \\mathbf{d}_3 = \\begin{pmatrix} D_{11} \\\\ D_{21} \\\\ \\vdots \\\\ D_{61} \\end{pmatrix} + \\begin{pmatrix} D_{12} \\\\ D_{22} \\\\ \\vdots \\\\ D_{62} \\end{pmatrix} + \\begin{pmatrix} D_{13} \\\\ D_{23} \\\\ \\vdots \\\\ D_{63} \\end{pmatrix} = \\begin{pmatrix} D_{11}+D_{12}+D_{13} \\\\ D_{21}+D_{22}+D_{23} \\\\ \\vdots \\\\ D_{61}+D_{62}+D_{63} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} = \\mathbf{1}$$\nThis shows that the intercept column $\\mathbf{1}$ is a perfect linear combination of the dummy variable columns $\\mathbf{d}_1, \\mathbf{d}_2, \\mathbf{d}_3$. We can express this linear dependency as:\n$$1 \\cdot \\mathbf{1} - 1 \\cdot \\mathbf{d}_1 - 1 \\cdot \\mathbf{d}_2 - 1 \\cdot \\mathbf{d}_3 = \\mathbf{0}$$\nBecause the columns of the design matrix $X$ are linearly dependent, $X$ does not have full column rank. The rank of $X$ is less than the number of its columns ($p=5$). Consequently, the matrix $X^\\top X$ is singular (not invertible). This means there is no unique solution for the parameter vector $\\boldsymbol{\\beta}$ using the OLS estimator $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$. This situation is known as perfect multicollinearity, or the \"dummy variable trap\".\n\nNow we evaluate each option based on this fundamental analysis.\n\nA. $X^{\\top} X$ is singular because the intercept column equals the sum of the dummy variable columns. Dropping one dummy variable (and keeping the intercept) makes $X^{\\top} X$ nonsingular provided each level appears at least once.\nThe first part of the statement is correct, as derived above: $\\mathbf{1} = \\mathbf{d}_1 + \\mathbf{d}_2 + \\mathbf{d}_3$. This linear dependency causes the singularity of $X^\\top X$.\nFor the second part, let's suppose we drop the column $\\mathbf{d}_3$. The new design matrix is $X_{new} = [\\mathbf{1} \\quad \\mathbf{x} \\quad \\mathbf{d}_1 \\quad \\mathbf{d}_2]$. The previous linear dependency is broken. A new linear dependency would require a linear combination of these four columns to be the zero vector. Assuming $x$ is a true continuous predictor, it is generally not a linear combination of the intercept and the remaining dummy variables. The columns $\\mathbf{1}, \\mathbf{d}_1, \\mathbf{d}_2$ are linearly independent because the existence of observations in each of the three levels guarantees that we cannot write one as a linear combination of the others. Therefore, the columns of $X_{new}$ are linearly independent, $X_{new}$ has full column rank, and $X_{new}^\\top X_{new}$ is nonsingular. This is the standard approach for resolving the dummy variable trap.\nThe statement is **Correct**.\n\nB. Centering the continuous predictor $x$ at its sample mean eliminates the singularity of $X^{\\top} X$, so no dummy variable or intercept needs to be dropped.\nCentering the predictor $x$ means replacing the column $\\mathbf{x}$ in the design matrix with a new column $\\mathbf{x}^* = \\mathbf{x} - \\bar{x}\\mathbf{1}$. The design matrix becomes $X_{new} = [\\mathbf{1} \\quad \\mathbf{x}^* \\quad \\mathbf{d}_1 \\quad \\mathbf{d}_2 \\quad \\mathbf{d}_3]$. The linear dependency $\\mathbf{1} = \\mathbf{d}_1 + \\mathbf{d}_2 + \\mathbf{d}_3$ involves only the intercept and dummy variable columns; it does not involve the column for the continuous predictor. Therefore, this transformation does not alter the fundamental linear dependency. The columns of $X_{new}$ remain linearly dependent: $1 \\cdot \\mathbf{1} + 0 \\cdot \\mathbf{x}^* - 1 \\cdot \\mathbf{d}_1 - 1 \\cdot \\mathbf{d}_2 - 1 \\cdot \\mathbf{d}_3 = \\mathbf{0}$. Thus, $X_{new}^\\top X_{new}$ is still singular. Centering can reduce non-essential multicollinearity (e.g., between a predictor and its polynomial terms), but it does not resolve the structural perfect multicollinearity of the dummy variable trap.\nThe statement is **Incorrect**.\n\nC. Removing the intercept while keeping all $3$ dummy variables yields a design matrix with full column rank, provided each level appears at least once.\nIf we remove the intercept, the design matrix becomes $X_{new} = [\\mathbf{x} \\quad \\mathbf{d}_1 \\quad \\mathbf{d}_2 \\quad \\mathbf{d}_3]$. The column $\\mathbf{1}$ is removed, so the linear dependency $\\mathbf{1} - \\mathbf{d}_1 - \\mathbf{d}_2 - \\mathbf{d}_3 = \\mathbf{0}$ is broken. We must check for any other possible linear dependencies among the new columns. First, consider the dummy columns $\\{\\mathbf{d}_1, \\mathbf{d}_2, \\mathbf{d}_3\\}$. A linear combination $c_1 \\mathbf{d}_1 + c_2 \\mathbf{d}_2 + c_3 \\mathbf{d}_3 = \\mathbf{0}$ implies that for an observation in level $1$, $c_1=0$; for an observation in level $2$, $c_2=0$; and for an observation in level $3$, $c_3=0$. Since each level appears at least once, this proves $c_1=c_2=c_3=0$, so $\\{\\mathbf{d}_1, \\mathbf{d}_2, \\mathbf{d}_3\\}$ is a linearly independent set. For the full set of columns to be linearly dependent, the continuous predictor column $\\mathbf{x}$ must be a linear combination of the dummy columns, i.e., $\\mathbf{x} = c_1 \\mathbf{d}_1 + c_2 \\mathbf{d}_2 + c_3 \\mathbf{d}_3$. This would mean $x$ takes only $3$ distinct values, one for each category level, which contradicts the nature of a general continuous predictor. Thus, assuming $x$ is not pathologically constructed, the columns of $X_{new}$ are linearly independent, and the matrix has full column rank. This is another standard method to resolve the dummy variable trap.\nThe statement is **Correct**.\n\nD. Keeping the intercept and all $3$ dummy variables but imposing the linear constraint $\\gamma_{1} + \\gamma_{2} + \\gamma_{3} = 0$ during estimation yields a unique solution without dropping any columns.\nThe non-identifiability of the original model means there is an infinite family of solutions. Any set of estimated coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\gamma}_1, \\hat{\\gamma}_2, \\hat{\\gamma}_3)$ that minimizes the sum of squared errors can be transformed into another valid solution $(\\hat{\\beta}_0 - c, \\hat{\\beta}_1, \\hat{\\gamma}_1 + c, \\hat{\\gamma}_2 + c, \\hat{\\gamma}_3 + c)$ for any constant $c$, without changing the model's predictions. This represents one degree of freedom in the parameter space. Imposing a single independent linear constraint on the parameters can resolve this ambiguity and enforce a unique solution. The constraint $\\gamma_{1} + \\gamma_{2} + \\gamma_{3} = 0$ is such a constraint. If two potential solutions are related by the constant $c$, we would have $\\sum \\hat{\\gamma}_j = 0$ and $\\sum (\\hat{\\gamma}_j + c) = 0$. The second equation simplifies to $(\\sum \\hat{\\gamma}_j) + 3c = 0$, which becomes $0 + 3c = 0$, forcing $c=0$. This implies the solution is unique under this constraint. This method is common in analysis of variance (ANOVA) and is an alternative to dropping a column from the design matrix.\nThe statement is **Correct**.\n\nE. Replacing $X^{\\top} X$ by $X^{\\top} X + \\lambda I$ with $\\lambda > 0$ is equivalent to making the model identifiable in the ordinary least squares sense; hence the unconstrained OLS estimates become unique and unchanged by the regularization.\nThe procedure described is Ridge Regression. The ridge estimator is $\\hat{\\boldsymbol{\\beta}}_{ridge} = (X^\\top X + \\lambda I)^{-1} X^\\top \\mathbf{y}$. The matrix $X^\\top X$ is positive semi-definite. Adding $\\lambda I$ for $\\lambda > 0$ ensures the resulting matrix $X^\\top X + \\lambda I$ is positive definite and thus invertible, providing a unique solution for $\\hat{\\boldsymbol{\\beta}}_{ridge}$. However, this procedure is fundamentally different from OLS. Ridge regression minimizes a penalized sum of squares, not the standard OLS criterion. The resulting estimate $\\hat{\\boldsymbol{\\beta}}_{ridge}$ is a biased estimate of $\\boldsymbol{\\beta}$. The statement claims this makes the \"unconstrained OLS estimates\" unique and unchanged. This is a contradiction. The unconstrained OLS estimates are not unique in the first place (there's a whole subspace of solutions), and the ridge estimate is a different, biased quantity. Therefore, ridge regression provides a unique regularized solution but does not make the OLS solution unique; it changes the problem being solved.\nThe statement is **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "3152062"}, {"introduction": "Beyond explaining relationships in existing data, the true power of a regression model often lies in its ability to predict future outcomes. However, a single-point forecast can be misleading without a sense of its reliability. This practice exercise [@problem_id:1938959] takes you to the final step of the modeling process: using a fitted regression equation to construct a prediction interval for a new observation, thereby quantifying the uncertainty associated with your forecast.", "problem": "A retail analytics firm is developing a model to predict the weekly sales of new pop-up stores. They have collected data from $n=30$ existing stores and fitted a multiple linear regression model of the form:\n$$\n\\text{Sales} = \\beta_0 + \\beta_1 \\cdot \\text{Size} + \\beta_2 \\cdot \\text{PopDensity} + \\epsilon\n$$\nwhere `Sales` is measured in thousands of dollars, `Size` is the floor area in square meters, and `PopDensity` is the population density in the surrounding 1-km radius, measured in thousands of people per square kilometer.\n\nThe team's analysis produced the following results based on the sample data:\n- Estimated regression equation: $\\hat{y} = 51.5 + 0.24 x_1 + 14.8 x_2$, where $x_1$ represents Size and $x_2$ represents PopDensity.\n- Residual standard error: $s_e = 12.5$ (in thousands of dollars).\n- The number of predictors in the model is $k=2$.\n\nThe company plans to open a new store with a `Size` of 520 square meters in an area with a `PopDensity` of 4.5 thousands of people per square kilometer. For this new observation, represented by the vector $x_0$, the statistical software reports the value of the leverage-related term as $x_0^T (X^T X)^{-1} x_0 = 0.082$. Here, $X$ is the design matrix for the original 30 observations.\n\nUsing the provided t-distribution critical value $t_{0.05, 27} = 1.703$, construct a 90% prediction interval for the weekly sales of this new store. Report the lower and upper bounds of the interval. Both values should be expressed in thousands of dollars and rounded to three significant figures.", "solution": "We are asked for a 90% prediction interval for a new observation in a multiple linear regression with $n=30$, $k=2$ predictors, residual standard error $s_e=12.5$, and leverage term $h_0=x_0^T(X^TX)^{-1}x_0=0.082$. The degrees of freedom are $n-k-1=27$, and the given critical value is $t_{0.05,27}=1.703$.\n\nThe point prediction at $x_1=520$, $x_2=4.5$ is obtained from the fitted equation $\\hat{y}=51.5+0.24x_1+14.8x_2$:\n$$\n\\hat{y}_0 = 51.5+0.24\\cdot 520+14.8\\cdot 4.5=51.5+124.8+66.6=242.9\n$$\n\nThe standard error for prediction is $s_e\\sqrt{1+h_0}=12.5\\sqrt{1+0.082}=12.5\\sqrt{1.082}$. The 90% prediction interval is\n$$\n\\hat{y}_0\\pm t_{0.05,27}\\,s_e\\sqrt{1+h_0}\n=242.9\\pm 1.703\\cdot 12.5\\cdot \\sqrt{1.082}\n$$\nCompute the margin:\n$$\n\\sqrt{1.082}\\approx 1.040192,\\quad 12.5\\cdot \\sqrt{1.082}\\approx 13.0024,\\quad\n1.703\\cdot 13.0024\\approx 22.143\n$$\nThus the interval bounds are\n$$\n\\text{Lower}=242.9-22.143\\approx 220.757,\\qquad\n\\text{Upper}=242.9+22.143\\approx 265.043\n$$\nRounding each bound to three significant figures (in thousands of dollars) gives $221$ and $265$.", "answer": "$$\\boxed{\\begin{pmatrix}221  265\\end{pmatrix}}$$", "id": "1938959"}]}