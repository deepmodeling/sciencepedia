{"hands_on_practices": [{"introduction": "The F-statistic is not just for testing the overall significance of a model against an empty one; it is a versatile tool for comparing any two nested linear models. This practice [@problem_id:3182401] will guide you through a common and important application: testing whether adding a set of interaction terms significantly improves a model that already contains the main effects. By calculating both the overall F-statistic and the partial F-statistic, you will gain a deeper understanding of how to partition variance to evaluate the contribution of specific groups of predictors.", "problem": "A manufacturing team models the output yield $y$ of $n=50$ batches as a linear function of three measured covariates: reactant concentration $x_{1}$, furnace temperature $x_{2}$, and catalyst indicator $x_{3}$ (coded $0$ or $1$). To capture synergistic effects, they include the product terms $x_{1}x_{2}$ and $x_{2}x_{3}$ in the model. Let the full model be\n$$\ny_{i} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1} x_{1i} \\;+\\; \\beta_{2} x_{2i} \\;+\\; \\beta_{3} x_{3i} \\;+\\; \\beta_{4} (x_{1i}x_{2i}) \\;+\\; \\beta_{5} (x_{2i}x_{3i}) \\;+\\; \\varepsilon_{i}, \\quad i=1,\\dots,50,\n$$\nwith $\\varepsilon_{i}$ assumed independent and identically distributed as mean-zero Gaussian with constant variance. The reduced model with only main effects is\n$$\ny_{i} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1} x_{1i} \\;+\\; \\beta_{2} x_{2i} \\;+\\; \\beta_{3} x_{3i} \\;+\\; \\varepsilon_{i}.\n$$\nFrom an Ordinary Least Squares (OLS) fit, the Analysis of Variance (ANOVA) quantities computed about the sample mean are:\n- total sum of squares $TSS = 1200$,\n- residual sum of squares for the full model $SSE_{\\text{full}} = 400$,\n- residual sum of squares for the reduced (main-effects-only) model $SSE_{\\text{main}} = 550$.\n\nStarting from the classical linear model assumptions and the core properties of quadratic forms under normal errors, derive the appropriate test statistics for:\n1. the overall regression significance of the full model (all five non-intercept coefficients jointly equal to zero), and\n2. the incremental gain from adding the two interaction terms $x_{1}x_{2}$ and $x_{2}x_{3}$ to the main-effects model, using the logic of nested models.\n\nCompute the numerical values of both test statistics. Express your final answer as a row matrix with the first entry equal to the overall significance statistic for the full model and the second entry equal to the partial statistic for the interactions. Round your two numbers to four significant figures. No units are required.", "solution": "The problem requires the derivation and computation of two distinct F-statistics related to a multiple linear regression analysis. The first tests the overall significance of the full model, while the second tests the significance of a subset of predictors (the interaction terms). The foundation for both tests is the comparison of nested linear models.\n\nLet a general linear model be specified in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is an $n \\times 1$ vector of observations, $\\mathbf{X}$ is an $n \\times p$ design matrix of rank $p$, $\\boldsymbol{\\beta}$ is a $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is an $n \\times 1$ vector of errors with $\\boldsymbol{\\varepsilon} \\sim N(0, \\sigma^2 \\mathbf{I})$. The residual sum of squares is given by $SSE = ||\\mathbf{y} - \\hat{\\mathbf{y}}||^2 = \\mathbf{y}^T(\\mathbf{I} - \\mathbf{H})\\mathbf{y}$, where $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ is the hat matrix. Under the normality assumption, $\\frac{SSE}{\\sigma^2} \\sim \\chi^2_{n-p}$.\n\nThe general F-test for a linear hypothesis $H_0: \\mathbf{L}\\boldsymbol{\\beta} = \\mathbf{0}$, where $\\mathbf{L}$ is a $q \\times p$ matrix of rank $q$, compares a full model (where the hypothesis is not imposed) with a reduced model (where the hypothesis is imposed). The test statistic is:\n$$\nF = \\frac{(SSE_R - SSE_F) / q}{SSE_F / (n-p_F)}\n$$\nwhere $SSE_R$ and $SSE_F$ are the residual sums of squares for the reduced and full models, respectively. The number of parameters in the full model is $p_F$, and $q$ is the number of linear constraints imposed by the null hypothesis, which also equals $p_F - p_R$, where $p_R$ is the number of parameters in the reduced model. Under $H_0$, this statistic follows an F-distribution with $q$ and $n-p_F$ degrees of freedom, denoted $F(q, n-p_F)$.\n\nThe provided data are:\n- Number of observations: $n=50$\n- Total Sum of Squares: $TSS = 1200$\n- Residual Sum of Squares (full model): $SSE_{\\text{full}} = 400$\n- Residual Sum of Squares (main-effects model): $SSE_{\\text{main}} = 550$\n\nThe full model has predictors $x_1$, $x_2$, $x_3$, $x_1x_2$, and $x_2x_3$. The number of non-intercept predictors is $k_{\\text{full}}=5$. The total number of parameters, including the intercept $\\beta_0$, is $p_{\\text{full}} = k_{\\text{full}} + 1 = 6$.\n\n### 1. Overall Regression Significance of the Full Model\n\nThis test assesses the null hypothesis that all non-intercept coefficients are zero: $H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0$.\n\n- **Full Model**: This is the model containing all $5$ predictors plus an intercept. The residual sum of squares is given as $SSE_F = SSE_{\\text{full}} = 400$. The number of parameters is $p_F = p_{\\text{full}} = 6$. The error degrees of freedom are $df_E = n - p_{\\text{full}} = 50 - 6 = 44$. The corresponding Mean Squared Error is $MSE_{\\text{full}} = \\frac{SSE_{\\text{full}}}{n - p_{\\text{full}}}$.\n\n- **Reduced Model**: Under $H_0$, the model becomes $y_i = \\beta_0 + \\varepsilon_i$. This is an intercept-only model. For an intercept-only model, the OLS estimate for $\\beta_0$ is the sample mean $\\bar{y}$, and the residual sum of squares is, by definition, the total sum of squares, $TSS$. Thus, $SSE_R = TSS = 1200$. The number of parameters is $p_R = 1$. The number of constraints is $q = p_{\\text{full}} - p_R = 6-1=5$, which corresponds to the $5$ coefficients set to zero.\n\nThe sum of squares explained by the regression is $SSR_{\\text{full}} = SSE_R - SSE_F = TSS - SSE_{\\text{full}}$. This quantity represents the reduction in error sum of squares achieved by using the full model over the intercept-only model.\n$SSR_{\\text{full}} = 1200 - 400 = 800$.\n\nThe F-statistic for overall significance is the ratio of the Mean Square for Regression ($MSR_{\\text{full}}$) to the Mean Square for Error ($MSE_{\\text{full}}$):\n$$\nF_{\\text{overall}} = \\frac{MSR_{\\text{full}}}{MSE_{\\text{full}}} = \\frac{SSR_{\\text{full}} / (p_{\\text{full}} - 1)}{SSE_{\\text{full}} / (n - p_{\\text{full}})}\n$$\nSubstituting the given values:\n$$\nF_{\\text{overall}} = \\frac{(1200 - 400) / 5}{400 / (50 - 6)} = \\frac{800 / 5}{400 / 44} = \\frac{160}{400/44} = 160 \\times \\frac{44}{400} = 0.4 \\times 44 = 17.6\n$$\nUnder $H_0$, this statistic follows an $F(5, 44)$ distribution.\n\n### 2. Incremental Gain from Interaction Terms\n\nThis test assesses the null hypothesis that the coefficients of the interaction terms are both zero: $H_0: \\beta_4 = \\beta_5 = 0$. This is a partial F-test comparing two nested models.\n\n- **Full Model**: This is again the model with all $5$ predictors: $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\beta_{3} x_{3i} + \\beta_{4} (x_{1i}x_{2i}) + \\beta_{5} (x_{2i}x_{3i}) + \\varepsilon_{i}$. Its SSE is $SSE_F = SSE_{\\text{full}} = 400$, and it has $p_F = p_{\\text{full}} = 6$ parameters. The denominator of the F-statistic is the MSE of this full model, $MSE_{\\text{full}} = \\frac{SSE_{\\text{full}}}{n - p_{\\text{full}}} = \\frac{400}{44}$.\n\n- **Reduced Model**: This is the model under $H_0$, which excludes the interaction terms. This is the main-effects model provided in the problem statement: $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\beta_{3} x_{3i} + \\varepsilon_{i}$. Its SSE is given as $SSE_R = SSE_{\\text{main}} = 550$. This model has $k_{\\text{main}} = 3$ predictors, so the number of parameters is $p_R = p_{\\text{main}} = 3+1 = 4$.\n\nThe number of constraints, $q$, is the number of additional parameters in the full model compared to the reduced model, which is $q = p_F - p_R = 6 - 4 = 2$. This matches the number of coefficients set to zero in the null hypothesis.\n\nThe F-statistic for this partial test is:\n$$\nF_{\\text{partial}} = \\frac{(SSE_{\\text{main}} - SSE_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{main}})}{SSE_{\\text{full}} / (n - p_{\\text{full}})}\n$$\nThe numerator represents the marginal reduction in SSE from adding the two interaction terms to the main-effects model, normalized by the number of added terms.\nSubstituting the given values:\n$$\nF_{\\text{partial}} = \\frac{(550 - 400) / 2}{400 / (50 - 6)} = \\frac{150 / 2}{400 / 44} = \\frac{75}{400 / 44} = 75 \\times \\frac{44}{400} = \\frac{3 \\times 25 \\times 44}{16 \\times 25} = \\frac{3 \\times 44}{16} = \\frac{3 \\times 11}{4} = \\frac{33}{4} = 8.25\n$$\nUnder $H_0$, this statistic follows an $F(2, 44)$ distribution.\n\nThe problem requires the two values rounded to four significant figures.\n$F_{\\text{overall}} = 17.6$ becomes $17.60$.\n$F_{\\text{partial}} = 8.25$ becomes $8.250$.\nThe final answer is presented as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n17.60  8.250\n\\end{pmatrix}\n}\n$$", "id": "3182401"}, {"introduction": "The standard formulas for regression analysis, including the F-test and the coefficient of determination $R^{2}$, are derived under the assumption that an intercept is included in the model. This exercise [@problem_id:3182409] explores the critical consequences of omitting the intercept, especially when the data's true relationship does not pass through the origin. You will discover how this seemingly minor model change alters the fundamental decomposition of sums of squares, potentially leading to inflated and misleading statistics.", "problem": "Consider the classical simple linear regression model under the assumptions of the Classical Linear Model (CLM): for observations indexed by $i = 1, \\dots, n$, the response is modeled as $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$, where the errors satisfy $\\varepsilon_{i} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^{2})$ and are independent of the predictor values $x_{i}$. The Ordinary Least Squares (OLS) estimator minimizes the residual sum of squares with respect to the model parameters that are included.\n\nYou will compare two specifications:\n- The model with intercept: $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$.\n- The model constrained through the origin (no intercept): $y_{i} = \\beta_{1} x_{i} + \\varepsilon_{i}$.\n\nSuppose you have $n = 10$ observations with the following non-centered summary statistics:\n- $\\sum_{i=1}^{10} x_{i} = 120$,\n- $\\sum_{i=1}^{10} y_{i} = 500$,\n- $\\sum_{i=1}^{10} x_{i}^{2} = 1600$,\n- $\\sum_{i=1}^{10} y_{i}^{2} = 27000$,\n- $\\sum_{i=1}^{10} x_{i} y_{i} = 6400$.\n\nUsing only the CLM assumptions and the geometric interpretation of least squares projections along with the appropriate sums-of-squares decompositions for each specification (centered when an intercept is included and non-centered when the intercept is excluded), do the following:\n1. Derive the test statistic for overall regression significance for each specification and identify the corresponding numerator and denominator degrees of freedom based on the number of parameters estimated.\n2. Compute both test statistics from the provided summary statistics.\n3. To quantify the effect of omitting the intercept in non-centered data, report the ratio of the no-intercept overall $F$ statistic to the with-intercept overall $F$ statistic.\n\nRound your final numerical answer to four significant figures. The final answer must be a single real number with no units.", "solution": "The F-statistic for overall regression significance tests the null hypothesis that all slope coefficients in the model are zero. It is defined as the ratio of the mean square due to regression (MSR) to the mean square error (MSE):\n$$F = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{SSR} / df_{\\text{reg}}}{\\text{SSE} / df_{\\text{res}}}$$\nwhere SSR is the sum of squares due to regression, SSE is the sum of squares of errors (residuals), and $df_{\\text{reg}}$ and $df_{\\text{res}}$ are the corresponding degrees of freedom. The decomposition of the total sum of squares depends on whether an intercept is included in the model.\n\n**1. Model with Intercept: $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$**\n\nFor a model with an intercept, the F-statistic tests $H_0: \\beta_1 = 0$. The sums of squares are *centered* around the sample mean $\\bar{y}$. The total sum of squares (SSTO) is partitioned as $\\text{SSTO} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\text{SSR} + \\text{SSE}$. The degrees of freedom are $df_{\\text{reg}} = 1$ (for $\\beta_1$) and $df_{\\text{res}} = n - 1 - 1 = 10 - 2 = 8$.\n\nFirst, we compute the sample means from the given sums and $n=10$:\n$$\\bar{x} = \\frac{\\sum x_i}{n} = \\frac{120}{10} = 12$$\n$$\\bar{y} = \\frac{\\sum y_i}{n} = \\frac{500}{10} = 50$$\n\nNext, we compute the centered sums of squares and cross-products:\n$$S_{xx} = \\sum (x_i - \\bar{x})^2 = \\sum x_i^2 - n\\bar{x}^2 = 1600 - 10(12^2) = 1600 - 1440 = 160$$\n$$S_{yy} = \\text{SSTO} = \\sum (y_i - \\bar{y})^2 = \\sum y_i^2 - n\\bar{y}^2 = 27000 - 10(50^2) = 27000 - 25000 = 2000$$\n$$S_{xy} = \\sum (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum x_i y_i - n\\bar{x}\\bar{y} = 6400 - 10(12)(50) = 6400 - 6000 = 400$$\n\nThe regression sum of squares ($\\text{SSR}_{\\text{int}}$) and error sum of squares ($\\text{SSE}_{\\text{int}}$) are:\n$$\\text{SSR}_{\\text{int}} = \\frac{S_{xy}^2}{S_{xx}} = \\frac{400^2}{160} = \\frac{160000}{160} = 1000$$\n$$\\text{SSE}_{\\text{int}} = \\text{SSTO} - \\text{SSR}_{\\text{int}} = 2000 - 1000 = 1000$$\n\nThe F-statistic for the model with an intercept, $F_{\\text{int}}$, is:\n$$F_{\\text{int}} = \\frac{\\text{SSR}_{\\text{int}} / 1}{\\text{SSE}_{\\text{int}} / (n-2)} = \\frac{1000 / 1}{1000 / 8} = 8$$\nThis F-statistic has $(1, 8)$ degrees of freedom.\n\n**2. Model without Intercept: $y_{i} = \\beta_{1} x_{i} + \\varepsilon_{i}$**\n\nFor a model without an intercept, the sums of squares are *not centered*. Variation is measured around the origin ($0$). The total uncentered sum of squares, $\\sum y_i^2$, is partitioned. The degrees of freedom are $df_{\\text{reg}} = 1$ (for $\\beta_1$) and $df_{\\text{res}} = n - 1 = 10 - 1 = 9$.\n\nThe regression sum of squares for the no-intercept model, $\\text{SSR}_{\\text{no-int}}$, is calculated from the raw (uncentered) sums of squares and cross-products:\n$$\\text{SSR}_{\\text{no-int}} = \\frac{(\\sum x_i y_i)^2}{\\sum x_i^2} = \\frac{6400^2}{1600} = \\frac{40960000}{1600} = 25600$$\n\nThe error sum of squares, $\\text{SSE}_{\\text{no-int}}$, is the remaining variation:\n$$\\text{SSE}_{\\text{no-int}} = \\sum y_i^2 - \\text{SSR}_{\\text{no-int}} = 27000 - 25600 = 1400$$\n\nThe F-statistic for the model without an intercept, $F_{\\text{no-int}}$, is:\n$$F_{\\text{no-int}} = \\frac{\\text{SSR}_{\\text{no-int}} / 1}{\\text{SSE}_{\\text{no-int}} / (n-1)} = \\frac{25600 / 1}{1400 / 9} = \\frac{25600 \\times 9}{1400} = \\frac{256 \\times 9}{14} = \\frac{1152}{7}$$\nThis F-statistic has $(1, 9)$ degrees of freedom.\n\n**3. Ratio of the F-statistics**\n\nThe final task is to compute the ratio of the no-intercept F-statistic to the with-intercept F-statistic.\n$$\\text{Ratio} = \\frac{F_{\\text{no-int}}}{F_{\\text{int}}} = \\frac{1152/7}{8} = \\frac{1152}{56} = \\frac{144}{7}$$\n\nConverting this fraction to a decimal and rounding to four significant figures:\n$$\\frac{144}{7} \\approx 20.571428...$$\nRounding to four significant figures gives $20.57$. This large ratio indicates that forcing the model through the origin, when the data's true intercept is likely far from zero, dramatically inflates the measure of explained variation relative to the total variation around the origin, thus producing a much larger (and often misleading) F-statistic.", "answer": "$$\\boxed{20.57}$$", "id": "3182409"}, {"introduction": "In data science, we often distinguish between a model's inferential validity and its predictive power. A significant F-statistic provides evidence that at least one predictor is related to the response, but what does a non-significant result imply? This thought-provoking exercise [@problem_id:3182416] presents a scenario where a model has strong predictive capabilities but fails to achieve statistical significance. This practice will challenge you to think about the role of statistical power and sample size, revealing the important tension that can exist between inference and prediction.", "problem": "An analyst fits a multiple linear regression with an intercept and $p$ predictors to a response $Y$ under the standard normal linear model $Y = X\\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$. The goal is to assess the overall regression significance of the $p$ predictors while also noting predictive performance on new data. Consider the following designed case intended to highlight a tension between inference and prediction when the sample size $n$ is small.\n\nYou are told that for a fitted model with $n = 9$ observations and $p = 3$ predictors (excluding the intercept), the total sum of squares about the mean is $\\operatorname{SST} = 88$, and the residual sum of squares for the fitted model is $\\operatorname{SSE} = 25$. Independently, the analyst also evaluates predictive performance using Leave-One-Out Cross-Validation (LOOCV) and finds the LOOCV mean squared error for the fitted model to be $5.6$, whereas for the intercept-only (null) model it is $12.3$.\n\nTasks:\n- Starting from the normal linear model assumptions and the definitions of the total sum of squares and the residual sum of squares, derive the overall regression test statistic used to assess $H_{0}: \\beta_{1} = \\cdots = \\beta_{p} = 0$ against the alternative that at least one slope is nonzero, explicitly identifying the relevant degrees of freedom.\n- Use the supplied summaries to compute the observed value of this test statistic.\n- Briefly explain, in one or two sentences, how it can be that the model exhibits superior predictive performance by LOOCV while the overall regression test may fail to reach conventional significance when $n$ is small.\n\nReport only the numerical value of the observed test statistic as your final answer. Round your result to three significant figures.", "solution": "The problem is assessed to be valid. It is self-contained, scientifically grounded in the principles of linear regression analysis, and well-posed. All necessary data for the required calculations are provided.\n\nThe primary task is to derive the F-statistic for the overall significance of a multiple linear regression model and then compute its value. The model is given by $Y = X\\beta + \\varepsilon$, where $Y$ is the $n \\times 1$ vector of observations, $X$ is the $n \\times (p+1)$ design matrix (including a column of ones for the intercept), $\\beta$ is the $(p+1) \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$.\n\nThe test for overall regression significance evaluates the null hypothesis $H_{0}: \\beta_{1} = \\beta_{2} = \\cdots = \\beta_{p} = 0$ against the alternative hypothesis $H_{A}$: at least one $\\beta_{j} \\neq 0$ for $j \\in \\{1, \\dots, p\\}$. This is a comparison between two nested models:\n1. The full model, which includes all $p$ predictors and an intercept.\n2. The reduced (or null) model, which includes only the intercept. This model is specified by the constraints of the null hypothesis.\n\nThe test statistic is constructed from the partitioning of the total sum of squares, $\\operatorname{SST}$. The total sum of squares measures the total variability in the response variable $Y$ around its sample mean $\\bar{y}$. It is defined as $\\operatorname{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$. The degrees of freedom associated with $\\operatorname{SST}$ are $df_{T} = n-1$.\n\nThe residual sum of squares, $\\operatorname{SSE}$, measures the variability that remains unexplained after fitting the regression model. It is defined as $\\operatorname{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, where $\\hat{y}_i$ are the fitted values from the full model. The degrees of freedom for error in the full model are $df_{E} = n - (\\text{number of parameters}) = n - (p+1)$.\n\nThe regression sum of squares, $\\operatorname{SSR}$, measures the variability in $Y$ that is explained by the regression model. It is the difference between the total variability and the unexplained variability: $\\operatorname{SSR} = \\operatorname{SST} - \\operatorname{SSE}$. The degrees of freedom for regression correspond to the number of predictor coefficients being tested, which is $p$. This can also be seen as the difference in the degrees of freedom for error between the reduced model ($n-1$) and the full model ($n-p-1$), so $df_{R} = (n-1) - (n-p-1) = p$.\n\nThe F-test statistic is the ratio of two mean squares. A mean square is a sum of squares divided by its degrees of freedom.\nThe Mean Square for Regression is $\\operatorname{MSR} = \\frac{\\operatorname{SSR}}{df_{R}} = \\frac{\\operatorname{SST} - \\operatorname{SSE}}{p}$.\nThe Mean Square for Error is $\\operatorname{MSE} = \\frac{\\operatorname{SSE}}{df_{E}} = \\frac{\\operatorname{SSE}}{n-p-1}$.\n\nThe F-statistic is defined as the ratio $F = \\frac{\\operatorname{MSR}}{\\operatorname{MSE}}$. Under the assumption that the null hypothesis $H_{0}$ is true, this statistic follows an F-distribution with $p$ and $n-p-1$ degrees of freedom.\nThe formula is:\n$$F = \\frac{(\\operatorname{SST} - \\operatorname{SSE}) / p}{\\operatorname{SSE} / (n-p-1)}$$\nThe relevant degrees of freedom are $p$ for the numerator and $n-p-1$ for the denominator.\n\nWe are given the following values:\n- Sample size: $n = 9$\n- Number of predictors: $p = 3$\n- Total sum of squares: $\\operatorname{SST} = 88$\n- Residual sum of squares: $\\operatorname{SSE} = 25$\n\nFirst, we calculate the degrees of freedom:\n- Numerator degrees of freedom: $df_{1} = p = 3$\n- Denominator degrees of freedom: $df_{2} = n - p - 1 = 9 - 3 - 1 = 5$\n\nNext, we calculate the mean squares:\n- $\\operatorname{SSR} = \\operatorname{SST} - \\operatorname{SSE} = 88 - 25 = 63$\n- $\\operatorname{MSR} = \\frac{\\operatorname{SSR}}{p} = \\frac{63}{3} = 21$\n- $\\operatorname{MSE} = \\frac{\\operatorname{SSE}}{n-p-1} = \\frac{25}{5} = 5$\n\nFinally, we compute the observed F-statistic:\n$$F_{\\text{obs}} = \\frac{\\operatorname{MSR}}{\\operatorname{MSE}} = \\frac{21}{5} = 4.2$$\nRounding to three significant figures, the value is $4.20$.\n\nThe final task is to explain the apparent contradiction between predictive performance and statistical significance. The LOOCV MSE for the fitted model ($5.6$) is less than half that of the null model ($12.3$), suggesting substantial predictive improvement. However, the F-statistic of $4.20$ with $(3, 5)$ degrees of freedom has a p-value of approximately $0.08$, which is not significant at a conventional $\\alpha = 0.05$ level (the critical value $F_{0.05, 3, 5}$ is $5.41$). This occurs because the F-test has low statistical power due to the very small sample size ($n=9$) and resulting small error degrees of freedom ($df_{E}=5$). Cross-validation assesses predictive utility, which can be evident even when the sample is too small for a formal hypothesis test to provide statistically significant evidence against the null.", "answer": "$$\\boxed{4.20}$$", "id": "3182416"}]}