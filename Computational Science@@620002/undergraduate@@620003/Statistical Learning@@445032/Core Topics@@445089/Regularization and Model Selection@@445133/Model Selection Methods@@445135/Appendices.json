{"hands_on_practices": [{"introduction": "Information criteria offer a powerful framework for model selection by penalizing model complexity to prevent overfitting. The Akaike Information Criterion ($AIC$) is a foundational method, but its derivation assumes a large sample size. This practice explores the Corrected Akaike Information Criterion ($AICc$), which adjusts the penalty for smaller samples, providing a more accurate estimate of prediction error when data is limited. By implementing both criteria, you will gain a hands-on understanding of when this correction is critical and how it can lead to the selection of a more parsimonious and generalizable model [@problem_id:3149493].", "problem": "Consider model selection between two nested parametric models in linear regression with Gaussian errors, grounded in Maximum Likelihood Estimation (MLE) and Kullback–Leibler divergence. The Akaike Information Criterion (AIC) and the Corrected Akaike Information Criterion (AICc) are both derived from MLE under the goal of minimizing expected information loss. The corrected version introduces an explicit small-sample adjustment that depends on the sample size and the model dimension. Your task is to build a complete program that constructs a scenario in which the corrected criterion outperforms the uncorrected one for small samples, and to quantify when the correction term materially changes the selected model.\n\nStarting from the fundamental base that the maximized log-likelihood for a Gaussian linear model with unknown variance depends on the residual sum of squares, assume the following setup for each test case:\n- Two candidate models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, with parameter counts $k_1$ and $k_2$ that include all estimated parameters (intercept, regression coefficients, and error variance), and a common sample size $n$.\n- The residual sum of squares for $\\mathcal{M}_1$ and $\\mathcal{M}_2$ are given as $RSS_1$ and $RSS_2$, respectively.\n- The smaller criterion value selects the model.\n\nDefine the \"material change\" as follows: the correction term materially changes the selected model if AIC would select $\\mathcal{M}_2$ while AICc would select $\\mathcal{M}_1$, or vice versa. Additionally, quantify the sample size threshold by computing the smallest integer $n^\\star$ that makes the two corrected criteria equal if the observed maximized log-likelihoods and parameter counts are held fixed while varying only $n$ in the correction term. If no finite solution exists, return a large sentinel integer.\n\nImplement a program that, for each test case, computes:\n- The selected model under AIC as an integer ($1$ for $\\mathcal{M}_1$, $2$ for $\\mathcal{M}_2$).\n- The selected model under AICc as an integer ($1$ for $\\mathcal{M}_1$, $2$ for $\\mathcal{M}_2$).\n- An integer flag indicating whether the correction term materially changes the selected model ($1$ if the selected models differ between AIC and AICc, $0$ otherwise).\n- The smallest integer $n^\\star$ such that, holding the fitted maximized log-likelihoods and parameter counts fixed, the corrected criteria are equal. If equality occurs at a non-integer $n$, take the smallest integer greater than that value. If no finite solution exists due to degeneracy, return a large sentinel integer (use $10^9$).\n\nUse the following test suite of parameter values, each specified as a tuple $(n, k_1, k_2, RSS_1, RSS_2)$:\n- Test $1$: $(12, 3, 4, 10.0, 8.0)$, a small-sample case with modest fit improvement for the more complex model.\n- Test $2$: $(100, 3, 4, 80.0, 78.0)$, a large-sample case where the correction term is small.\n- Test $3$: $(20, 3, 6, 10.0, 7.0)$, a small-to-moderate sample with a larger difference in model dimension.\n- Test $4$: $(200, 3, 6, 160.0, 152.0)$, a large-sample case with larger model dimension difference but minimal correction impact.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of four integers in the order $[aic\\_selected, aicc\\_selected, changed\\_flag, n^\\star]$. For example, the overall output format is $[[r_{11}, r_{12}, r_{13}, r_{14}], [r_{21}, r_{22}, r_{23}, r_{24}], [r_{31}, r_{32}, r_{33}, r_{34}], [r_{41}, r_{42}, r_{43}, r_{44}]]$.", "solution": "The problem requires an analysis of model selection between two linear regression models using the Akaike Information Criterion (AIC) and its small-sample correction, the Corrected Akaike Information Criterion (AICc). We must compute which model is selected by each criterion, determine if the selection differs, and calculate a critical sample size $n^\\star$ where the AICc values would be equal, holding other estimated quantities fixed.\n\nThe theoretical foundation for both AIC and AICc lies in information theory. They are designed to estimate the expected, relative Kullback-Leibler divergence between a fitted model and the unknown true data-generating process. A lower criterion value indicates a model that is expected to lose less information, providing a better balance between goodness-of-fit and model complexity.\n\nFor a parametric statistical model with $k$ estimated parameters, the general form of the AIC is given by:\n$$\nAIC = -2 \\hat{\\mathcal{L}} + 2k\n$$\nwhere $\\hat{\\mathcal{L}}$ is the maximized value of the log-likelihood function for the model.\n\nIn the specific context of a linear regression model with normally distributed errors, the parameters are the regression coefficients (including the intercept) and the error variance, $\\sigma^2$. If a model has $p$ predictor variables, there are $p+1$ regression coefficients to estimate. The variance $\\sigma^2$ is also estimated. Therefore, the total number of estimated parameters is $k = p+2$. The problem statement provides $k_1$ and $k_2$ as the total parameter counts for models $\\mathcal{M}_1$ and $\\mathcal{M}_2$, respectively.\n\nThe maximized log-likelihood for a Gaussian linear model, given a sample of size $n$ and a residual sum of squares ($RSS$), is:\n$$\n\\hat{\\mathcal{L}} = -\\frac{n}{2} \\left( \\log(2\\pi) + \\log\\left(\\frac{RSS}{n}\\right) + 1 \\right)\n$$\nSubstituting this into the AIC formula yields:\n$$\nAIC = n \\left( \\log(2\\pi) + \\log\\left(\\frac{RSS}{n}\\right) + 1 \\right) + 2k\n$$\nWhen comparing two models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, based on the same dataset of size $n$, the term $n(\\log(2\\pi) + 1)$ is a common additive constant and can be disregarded without affecting the selection. Furthermore, the term $-n\\log(n)$ is also common to both. Thus, for comparison purposes, a simplified an equivalent form can be used:\n$$\nAIC' = n\\log(RSS) + 2k\n$$\nWe will use this simplified form to determine the AIC-selected model. The model with the lower $AIC'$ value is preferred.\n\nThe Corrected Akaike Information Criterion, AICc, adjusts the penalty term to account for small sample sizes, where AIC tends to favor overly complex models. Its formula is:\n$$\nAICc = AIC + \\frac{2k(k+1)}{n - k - 1}\n$$\nThis correction term is significant when $n$ is small relative to $k$, but converges to $0$ as $n \\to \\infty$. The denominator requires that $n - k - 1 > 0$, or $n > k+1$, which is a necessary condition for the model to be identifiable and for AICc to be defined. For each test case, we compute $AICc_1$ and $AICc_2$ and select the model with the smaller value.\n\nThe analysis proceeds as follows for each test case $(n, k_1, k_2, RSS_1, RSS_2)$:\n\n$1$. **AIC-based Model Selection**: We compute $AIC'_1 = n\\log(RSS_1) + 2k_1$ and $AIC'_2 = n\\log(RSS_2) + 2k_2$. If $AIC'_1 < AIC'_2$, we select model $\\mathcal{M}_1$; otherwise, we select model $\\mathcal{M}_2$. This determines `aic_selected`.\n\n$2$. **AICc-based Model Selection**: We compute the full $AIC_1$ and $AIC_2$ values (or the simplified $AIC'_1$ and $AIC'_2$) and their respective correction terms.\n$$\nAICc_1 = AIC'_1 + \\frac{2k_1(k_1+1)}{n - k_1 - 1}\n$$\n$$\nAICc_2 = AIC'_2 + \\frac{2k_2(k_2+1)}{n - k_2 - 1}\n$$\nThe model with the lower $AICc$ value is selected. This determines `aicc_selected`.\n\n$3$. **Material Change Flag**: The `changed_flag` is set to $1$ if `aic_selected` $\\ne$ `aicc_selected`, and $0$ otherwise.\n\n$4$. **Critical Sample Size $n^\\star$**: We are asked to find the smallest integer $n^\\star$ that equates the two corrected criteria, assuming the maximized log-likelihoods (and thus the $AIC'$ values) are held fixed at their observed values, while allowing $n$ to vary only in the correction term. Let the observed values be $AIC'_{1,obs}$ and $AIC'_{2,obs}$. We must solve for $n$ in the equation:\n$$\nAIC'_{1,obs} + \\frac{2k_1(k_1+1)}{n - k_1 - 1} = AIC'_{2,obs} + \\frac{2k_2(k_2+1)}{n - k_2 - 1}\n$$\nLet $\\Delta AIC'_{obs} = AIC'_{1,obs} - AIC'_{2,obs}$, $P_1 = 2k_1(k_1+1)$, and $P_2 = 2k_2(k_2+1)$. The equation is:\n$$\n\\Delta AIC'_{obs} = \\frac{P_2}{n - k_2 - 1} - \\frac{P_1}{n - k_1 - 1}\n$$\nRearranging this leads to a quadratic equation in $n$ of the form $an^2 + bn + c = 0$, where:\n- $a = \\Delta AIC'_{obs}$\n- $b = -\\Delta AIC'_{obs}(k_1+k_2+2) - (P_2-P_1)$\n- $c = \\Delta AIC'_{obs}(k_1+1)(k_2+1) + P_2(k_1+1) - P_1(k_2+1)$\n\nThis equation can be solved for $n$ using the quadratic formula. We seek the smallest real root $n$ that satisfies the condition $n > \\max(k_1, k_2) + 1$. The final value $n^\\star$ is the smallest integer greater than or equal to this root (i.e., $\\lceil n \\rceil$). If no such finite, valid root exists (e.g., due to a non-positive discriminant, or roots that do not satisfy the condition, or a degenerate case where the criteria are equal for all $n$), a sentinel value of $10^9$ is returned. A degenerate case arises if $k_1=k_2$ and $RSS_1=RSS_2$, making $\\Delta AIC'_{obs}=0$ and $P_1=P_2$, which reduces the equation to $0=0$.\n\nThe implementation will now proceed by applying this four-step process to each of the provided test cases.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the test cases provided.\n    \"\"\"\n    \n    test_cases = [\n        # (n, k1, k2, RSS1, RSS2)\n        (12, 3, 4, 10.0, 8.0),\n        (100, 3, 4, 80.0, 78.0),\n        (20, 3, 6, 10.0, 7.0),\n        (200, 3, 6, 160.0, 152.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n, k1, k2, rss1, rss2 = case\n\n        # Step 1: Compute AIC and select model\n        # The comparative AIC can be simplified to n*log(RSS) + 2k, as other terms cancel out.\n        aic1_obs = n * np.log(rss1) + 2 * k1\n        aic2_obs = n * np.log(rss2) + 2 * k2\n        aic_selected = 1 if aic1_obs < aic2_obs else 2\n\n        # Step 2: Compute AICc and select model\n        # Check for valid denominators for AICc correction term\n        if n - k1 - 1 <= 0 or n - k2 - 1 <= 0:\n            # This case should not happen with the given test data, but is a necessary check.\n            # Set AICc to infinity to indicate it's not applicable.\n            aicc1 = float('inf')\n            aicc2 = float('inf')\n        else:\n            correction1 = (2 * k1 * (k1 + 1)) / (n - k1 - 1)\n            correction2 = (2 * k2 * (k2 + 1)) / (n - k2 - 1)\n            aicc1 = aic1_obs + correction1\n            aicc2 = aic2_obs + correction2\n\n        aicc_selected = 1 if aicc1 < aicc2 else 2\n\n        # Step 3: Determine if the selection materially changed\n        changed_flag = 1 if aic_selected != aicc_selected else 0\n\n        # Step 4: Compute n_star\n        n_star = calculate_n_star(aic1_obs - aic2_obs, k1, k2)\n        \n        results.append([aic_selected, aicc_selected, changed_flag, int(n_star)])\n\n    # Format the final output as a string representation of a list of lists.\n    # e.g., [[r11, r12, r13, r14], [r21, r22, r23, r24]]\n    # This avoids numpy formatting and creates the exact required output string.\n    output_str = \"[\" + \", \".join(map(str, results)) + \"]\"\n    output_str = output_str.replace(\" \", \"\") # Remove spaces for exact match\n    print(output_str)\n\ndef calculate_n_star(delta_aic, k1, k2):\n    \"\"\"\n    Calculates the smallest integer n* where AICc1(n) = AICc2(n),\n    holding observed log-likelihoods constant.\n    \"\"\"\n    sentinel_value = 10**9\n    \n    # Check for degeneracy: if models are identical in parameters and fit,\n    # their AIC and AICc values will always be identical.\n    if k1 == k2 and abs(delta_aic) < 1e-9:\n        return sentinel_value\n\n    p1 = 2 * k1 * (k1 + 1)\n    p2 = 2 * k2 * (k2 + 1)\n    \n    a = delta_aic\n    b = -delta_aic * (k1 + k2 + 2) - (p2 - p1)\n    c = delta_aic * (k1 + 1) * (k2 + 1) + p2 * (k1 + 1) - p1 * (k2 + 1)\n\n    min_n_valid = max(k1, k2) + 1\n\n    valid_roots = []\n\n    if abs(a) < 1e-9: # Linear equation bn + c = 0\n        if abs(b) > 1e-9:\n            n_sol = -c / b\n            if n_sol > min_n_valid:\n                valid_roots.append(n_sol)\n    else: # Quadratic equation\n        discriminant = b**2 - 4 * a * c\n        if discriminant >= 0:\n            sqrt_d = np.sqrt(discriminant)\n            n1 = (-b + sqrt_d) / (2 * a)\n            n2 = (-b - sqrt_d) / (2 * a)\n            \n            if n1 > min_n_valid:\n                valid_roots.append(n1)\n            if n2 > min_n_valid:\n                valid_roots.append(n2)\n\n    if not valid_roots:\n        return sentinel_value\n    else:\n        # Return the ceiling of the smallest valid root\n        return math.ceil(min(valid_roots))\n\nsolve()\n\n```", "id": "3149493"}, {"introduction": "Model selection involves a fundamental choice between different philosophical approaches. This exercise contrasts two of the most important paradigms: the information-theoretic approach, represented by the Bayesian Information Criterion ($BIC$), and the direct error estimation approach of $K$-fold Cross-Validation (CV). You will apply both methods to the classic problem of selecting the optimal degree for a polynomial regression model. This hands-on comparison will illuminate how the implicit complexity penalty of CV and the explicit, sample-size-dependent penalty of $BIC$ can lead to different conclusions, building your intuition for choosing the right tool for the job [@problem_id:3149417].", "problem": "You will study how feature engineering interacts with model selection by comparing polynomial feature degree selection via cross-validation to penalized likelihood selection using the Bayesian Information Criterion (BIC). Work entirely in a one-dimensional regression setting with polynomial feature engineering. Your task is to derive the necessary model selection criteria from first principles and to implement a program that applies both criteria on a fixed set of test instances, returning the selected polynomial degrees.\n\nBegin from the following foundational base:\n- Supervised learning with a regression target under the assumption of additive, independent, identically distributed Gaussian noise.\n- Ordinary least squares for linear regression as the maximum likelihood estimator under Gaussian noise for the conditional mean function.\n- The idea of risk estimation via data splitting and averaging and the notion of penalized likelihood for model selection.\n\nYou must not use any result that presupposes the target formulas. Instead, derive all formulae you need for the cross-validation risk estimate and the Bayesian Information Criterion from the above foundations.\n\nConsider the following data-generating process. For a given integer polynomial degree $d_{\\mathrm{true}} \\in \\{0,1,2,\\dots\\}$, define the true regression function\n$$\nf_{\\mathrm{true}}(x) \\;=\\; \\sum_{j=0}^{d_{\\mathrm{true}}} c_j\\, x^j,\n$$\nwith coefficients given deterministically by $c_0=1$ and $c_j = (-1)^j/(j+1)$ for all integers $j \\ge 1$. Data consist of $n$ independent draws $x_i \\sim \\mathrm{Uniform}(-1,1)$ and $y_i = f_{\\mathrm{true}}(x_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and independent of $x_i$. The regression model class for selection is the set of polynomial mean functions of degree $d \\in \\{0,1,\\dots,d_{\\max}\\}$, with a standard linear regression fit on the polynomial features $[1, x, x^2, \\dots, x^d]$.\n\nYour tasks:\n- Derive, from the Gaussian noise assumption and the maximum likelihood of the linear regression model, an explicit, implementable expression for the Bayesian Information Criterion for the model indexed by degree $d$. Clearly specify the parameter count used in the penalty. In the corner case that the residual sum of squares equals zero, state how to regularize it to avoid undefined logarithms by replacing it with a strictly positive constant $10^{-12}$ before applying the logarithm.\n- Derive a computable $K$-fold cross-validation estimate of the mean squared prediction error for each degree $d$, specifying precisely how folds are constructed and how the average is computed over folds, starting from the notion of expected prediction risk. Use a deterministic $K$-fold partition defined by assigning observation index $i \\in \\{0,1,\\dots,n-1\\}$ to fold $\\left(i \\bmod K\\right)$, without shuffling.\n- For both selection methods, specify a deterministic tie-breaking rule: if multiple degrees achieve the same minimal criterion value up to a numerical tolerance $\\tau$, choose the smallest degree among those. Use $\\tau=10^{-12}$.\n\nImplementation requirements:\n- For each candidate degree $d$, fit the linear model by ordinary least squares using the Moore–Penrose pseudoinverse to ensure numerical stability even when the design matrix is ill-conditioned. The design matrix for degree $d$ should include the intercept term as the column corresponding to $x^0$.\n- Use the natural logarithm in the Bayesian Information Criterion.\n- Mean squared error is defined as the average of squared residuals over the evaluation set.\n\nTest suite:\nEvaluate your implementation on the following five test cases, each specified by the tuple $(n, \\sigma, d_{\\mathrm{true}}, d_{\\max}, K, \\text{seed})$:\n- $T_1 = (100, 0.3, 3, 10, 5, 1234)$\n- $T_2 = (40, 0.1, 5, 8, 5, 202)$\n- $T_3 = (60, 1.0, 3, 10, 5, 999)$\n- $T_4 = (30, 0.3, 0, 6, 5, 77)$\n- $T_5 = (50, 0.5, 8, 8, 5, 555)$\n\nFor each test case:\n- Generate $n$ independent inputs $x_i$ from the uniform distribution on $[-1,1]$ using the given integer seed to initialize a pseudorandom number generator.\n- Generate $y_i$ according to the stated data-generating process with the given $\\sigma$ and $d_{\\mathrm{true}}$.\n- For $d \\in \\{0,1,\\dots,d_{\\max}\\}$, compute:\n  - The $K$-fold cross-validation mean squared error estimate and select the degree $d_{\\mathrm{CV}}$ minimizing it (with the specified tie-breaking).\n  - The Bayesian Information Criterion and select the degree $d_{\\mathrm{BIC}}$ minimizing it (with the specified tie-breaking).\n- Record for the test case the list $[d_{\\mathrm{CV}}, d_{\\mathrm{BIC}}, \\text{agree}]$, where $\\text{agree}$ is $1$ if $d_{\\mathrm{CV}}=d_{\\mathrm{BIC}}$ and $0$ otherwise.\n\nFinal output format:\nYour program should produce a single line of output that is a Python-style list literal of the five per-test-case lists, in the fixed order $[T_1, T_2, T_3, T_4, T_5]$. The output must be exactly one line and contain only this list literal. No physical units apply in this problem, and no angles or percentages are involved.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The definitions and constraints are clear, consistent, and formalizable into a standard statistical simulation study. The problem is therefore deemed **valid**.\n\nThe objective is to compare two model selection methods for polynomial regression: $K$-fold cross-validation (CV) and the Bayesian Information Criterion (BIC). We must derive the necessary formulas from foundational principles and implement them to select the optimal polynomial degree $d$ from a set of candidates $d \\in \\{0, 1, \\dots, d_{\\max}\\}$.\n\n**1. Model Formulation and Estimation**\n\nWe consider a one-dimensional regression problem with data $(\\mathbf{x}, \\mathbf{y})$, where $\\mathbf{x} = (x_1, \\dots, x_n)^T$ and $\\mathbf{y} = (y_1, \\dots, y_n)^T$. The model for a polynomial of degree $d$ posits that $y_i = f_d(x_i) + \\varepsilon_i$, where $f_d(x) = \\sum_{j=0}^d \\beta_j x^j$ and $\\varepsilon_i$ are independent and identically distributed (i.i.d.) Gaussian noise terms, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThis can be expressed in matrix form as $\\mathbf{y} = X_d \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where:\n-   $\\mathbf{y}$ is the $n \\times 1$ vector of observations.\n-   $X_d$ is the $n \\times (d+1)$ design matrix, with entries $(X_d)_{ij} = x_i^{j-1}$ for $i \\in \\{1, \\dots, n\\}$ and $j \\in \\{1, \\dots, d+1\\}$. The first column corresponds to the intercept term $x_i^0=1$.\n-   $\\boldsymbol{\\beta}$ is the $(d+1) \\times 1$ vector of coefficients $(\\beta_0, \\dots, \\beta_d)^T$.\n-   $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of noise terms.\n\nUnder the assumption of Gaussian noise, the maximum likelihood estimator (MLE) for $\\boldsymbol{\\beta}$ is the ordinary least squares (OLS) estimator. To ensure numerical stability, we compute this using the Moore-Penrose pseudoinverse, $X_d^+$, of the design matrix:\n$$\n\\hat{\\boldsymbol{\\beta}}_d = X_d^+ \\mathbf{y}\n$$\nThe predicted values are then $\\hat{\\mathbf{y}}_d = X_d \\hat{\\boldsymbol{\\beta}}_d$.\n\n**2. Derivation of the Bayesian Information Criterion (BIC)**\n\nThe BIC is a penalized likelihood criterion for model selection. We derive it from the likelihood function of the data under the specified model.\n\nThe probability density function for a single observation $y_i$ given $x_i$ and the model parameters $(\\boldsymbol{\\beta}, \\sigma^2)$ is:\n$$\np(y_i | x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_{i,d}^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\n$$\nwhere $\\mathbf{x}_{i,d}^T$ is the $i$-th row of the design matrix $X_d$.\n\nAssuming the observations are i.i.d., the likelihood of the entire dataset $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n$ is the product of individual probabilities:\n$$\nL(\\boldsymbol{\\beta}, \\sigma^2 | \\mathcal{D}) = \\prod_{i=1}^n p(y_i | x_i, \\boldsymbol{\\beta}, \\sigma^2)\n$$\nThe log-likelihood, $\\ell$, is:\n$$\n\\ell(\\boldsymbol{\\beta}, \\sigma^2) = \\log L = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i - \\mathbf{x}_{i,d}^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right)\n$$\n$$\n\\ell(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_{i,d}^T\\boldsymbol{\\beta})^2\n$$\nTo find the maximized log-likelihood, we substitute the MLEs for $\\boldsymbol{\\beta}$ and $\\sigma^2$. The MLE for $\\boldsymbol{\\beta}$ is the OLS estimate $\\hat{\\boldsymbol{\\beta}}_d$. The sum of squared errors for this fit is the Residual Sum of Squares, $\\mathrm{RSS}_d = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$. The MLE for the variance $\\sigma^2$ is found by differentiating $\\ell$ with respect to $\\sigma^2$ and setting the result to zero, which yields $\\hat{\\sigma}^2 = \\mathrm{RSS}_d / n$.\n\nSubstituting $\\hat{\\boldsymbol{\\beta}}_d$ and $\\hat{\\sigma}^2$ into the log-likelihood gives the maximized value, $\\ell_{\\max}$:\n$$\n\\ell_{\\max} = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}_d}{n}\\right) - \\frac{\\mathrm{RSS}_d}{2(\\mathrm{RSS}_d/n)} = -\\frac{n}{2}\\left(\\log(2\\pi) + \\log\\left(\\frac{\\mathrm{RSS}_d}{n}\\right) + 1\\right)\n$$\nThe BIC is defined as $\\mathrm{BIC} = k\\log n - 2\\ell_{\\max}$, where $k$ is the number of estimated parameters in the model. For a polynomial of degree $d$, we estimate $d+1$ coefficients ($\\beta_0, \\dots, \\beta_d$) and one variance parameter ($\\sigma^2$), so $k = (d+1) + 1 = d+2$.\n$$\n\\mathrm{BIC}(d) = (d+2)\\log n - 2\\left(-\\frac{n}{2}\\left(\\log(2\\pi) + \\log\\left(\\frac{\\mathrm{RSS}_d}{n}\\right) + 1\\right)\\right)\n$$\n$$\n\\mathrm{BIC}(d) = (d+2)\\log n + n\\log(2\\pi) + n + n\\log\\left(\\frac{\\mathrm{RSS}_d}{n}\\right)\n$$\nWhen comparing models for the same dataset, terms that do not depend on the model complexity $d$ (i.e., $n\\log(2\\pi)$ and $n$) can be dropped. This yields a proportional quantity that is equivalent for model selection:\n$$\n\\mathrm{BIC}(d) \\propto n\\log\\left(\\frac{\\mathrm{RSS}_d}{n}\\right) + (d+2)\\log n\n$$\nWe will use this form. Per the problem, the natural logarithm must be used. If $\\mathrm{RSS}_d = 0$, we substitute $\\mathrm{RSS}_d = 10^{-12}$ before applying the logarithm to prevent a mathematical error.\n\n**3. Derivation of the $K$-Fold Cross-Validation Estimate**\n\nCross-validation provides an estimate of the expected prediction error (risk), defined as $R(\\hat{f}) = \\mathbb{E}_{(x,y)}[(y - \\hat{f}(x))^2]$, where the expectation is over the true data-generating distribution.\n\n$K$-fold CV approximates this expectation by partitioning the dataset $\\mathcal{D}$ into $K$ roughly equal-sized, disjoint folds, $\\mathcal{D}_1, \\dots, \\mathcal{D}_K$. The problem specifies a deterministic partitioning rule: observation with index $i \\in \\{0, 1, \\dots, n-1\\}$ is assigned to fold $j = (i \\bmod K)$.\n\nFor each fold $j \\in \\{0, \\dots, K-1\\}$, we perform the following steps for a given model complexity (degree $d$):\n1.  Define the training set as $\\mathcal{T}_j = \\mathcal{D} \\setminus \\mathcal{D}_j$ and the validation set as $\\mathcal{V}_j = \\mathcal{D}_j$.\n2.  Fit the polynomial regression model of degree $d$ on the training data $\\mathcal{T}_j$ to obtain an estimator $\\hat{f}_d^{(-j)}$.\n3.  Compute the mean squared error (MSE) on the held-out validation data $\\mathcal{V}_j$:\n    $$\n    \\mathrm{MSE}_j(d) = \\frac{1}{|\\mathcal{V}_j|} \\sum_{(x_i, y_i) \\in \\mathcal{V}_j} (y_i - \\hat{f}_d^{(-j)}(x_i))^2\n    $$\n    where $|\\mathcal{V}_j|$ is the number of samples in fold $j$.\n\nThe $K$-fold CV estimate of the risk for degree $d$ is the average of these errors over all $K$ folds:\n$$\n\\mathrm{CV}_K(d) = \\frac{1}{K} \\sum_{j=0}^{K-1} \\mathrm{MSE}_j(d)\n$$\n\n**4. Model Selection and Tie-Breaking**\n\nFor both methods, the optimal degree is selected by minimizing the respective criterion over the set of candidate degrees $\\{0, 1, \\dots, d_{\\max}\\}$:\n$$\nd_{\\mathrm{BIC}} = \\arg\\min_{d} \\mathrm{BIC}(d) \\quad\\text{and}\\quad d_{\\mathrm{CV}} = \\arg\\min_{d} \\mathrm{CV}_K(d)\n$$\nThe problem specifies a deterministic tie-breaking rule: if multiple degrees yield a criterion value that is minimal up to a numerical tolerance $\\tau=10^{-12}$, the smallest degree among them is chosen. Formally, for a given score function $S(d)$, we find $S_{\\min} = \\min_d S(d)$. The selected degree is $d^* = \\min \\{d \\mid S(d) \\le S_{\\min} + \\tau \\}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares model selection for polynomial regression using\n    K-fold Cross-Validation and the Bayesian Information Criterion.\n    \"\"\"\n\n    # Helper function for selection with tie-breaking\n    def select_degree_with_tiebreak(scores, tau):\n        \"\"\"\n        Selects the best degree from a list of scores.\n        If multiple degrees are within tau of the minimum score,\n        the smallest degree is chosen.\n        \"\"\"\n        scores_arr = np.array(scores)\n        # Defensive check for cases where all models fail to fit.\n        if np.all(np.isinf(scores_arr)): \n            return 0\n        \n        min_score = np.nanmin(scores_arr)\n        # Find indices of scores close to the minimum. np.where returns sorted indices.\n        candidate_indices = np.where(scores_arr <= min_score + tau)[0]\n        # Return the smallest degree among candidates.\n        return candidate_indices[0]\n\n    # Test cases as specified in the problem statement\n    test_cases = [\n        (100, 0.3, 3, 10, 5, 1234),\n        (40, 0.1, 5, 8, 5, 202),\n        (60, 1.0, 3, 10, 5, 999),\n        (30, 0.3, 0, 6, 5, 77),\n        (50, 0.5, 8, 8, 5, 555),\n    ]\n\n    # Constants from the problem statement\n    TOLERANCE = 1e-12\n    RSS_REGULARIZATION = 1e-12\n\n    all_results = []\n\n    for n, sigma, d_true, d_max, K, seed in test_cases:\n        # 1. Generate Data\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(-1, 1, size=n)\n\n        # Generate true function values y_true = f_true(x)\n        coeffs_true = [1.0]\n        if d_true > 0:\n            coeffs_true.extend([(-1)**j / (j + 1) for j in range(1, d_true + 1)])\n        \n        X_true = np.vander(x, d_true + 1, increasing=True)\n        y_true = X_true @ np.array(coeffs_true)\n\n        # Add Gaussian noise\n        noise = rng.normal(0, sigma, size=n)\n        y = y_true + noise\n\n        bic_scores = []\n        cv_scores = []\n\n        # 2. Iterate through candidate polynomial degrees\n        for d in range(d_max + 1):\n            # Create the design matrix for degree d\n            X = np.vander(x, d + 1, increasing=True)\n            \n            # --- Bayesian Information Criterion (BIC) ---\n            beta_hat = np.linalg.pinv(X) @ y\n            y_hat = X @ beta_hat\n            \n            rss = np.sum((y - y_hat)**2)\n            \n            # Regularize if RSS is zero, as specified.\n            if rss == 0.0:\n                rss = RSS_REGULARIZATION\n\n            # Number of parameters k = (d+1) coefficients + 1 variance\n            k_params = d + 2\n            \n            # BIC formula: n*log(RSS/n) + k*log(n), using natural log\n            bic = n * np.log(rss / n) + k_params * np.log(n)\n            bic_scores.append(bic)\n\n            # --- K-Fold Cross-Validation ---\n            fold_errors = []\n            \n            # Deterministic fold assignment\n            fold_indices = np.arange(n) % K\n\n            for k_fold in range(K):\n                train_mask = (fold_indices != k_fold)\n                val_mask = (fold_indices == k_fold)\n\n                x_train, y_train = x[train_mask], y[train_mask]\n                x_val, y_val = x[val_mask], y[val_mask]\n\n                # If a training fold is too small to fit the model, its error is infinite.\n                if x_train.shape[0] < d + 1:\n                    fold_errors.append(np.inf)\n                    continue\n\n                X_train = np.vander(x_train, d + 1, increasing=True)\n                beta_hat_k = np.linalg.pinv(X_train) @ y_train\n                \n                X_val = np.vander(x_val, d + 1, increasing=True)\n                y_hat_val = X_val @ beta_hat_k\n                \n                # Mean Squared Error for the fold\n                mse_k = np.mean((y_val - y_hat_val)**2)\n                fold_errors.append(mse_k)\n            \n            cv_scores.append(np.mean(fold_errors))\n\n        # 3. Select best degree for each criterion using the tie-breaking rule\n        d_cv = select_degree_with_tiebreak(cv_scores, TOLERANCE)\n        d_bic = select_degree_with_tiebreak(bic_scores, TOLERANCE)\n\n        # 4. Record results for the test case\n        agree = 1 if d_cv == d_bic else 0\n        all_results.append([d_cv, d_bic, agree])\n\n    # 5. Print the final list of results in the required format\n    print(all_results)\n\n# Execute the main function\nsolve()\n```", "id": "3149417"}, {"introduction": "In Bayesian modeling, the choice of prior distributions is a critical part of model specification and can be viewed as a form of model selection. This advanced practice moves into the modern Bayesian workflow, comparing two powerful methods for estimating out-of-sample predictive accuracy: the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV). By implementing these computationally-driven techniques to select an optimal prior strength in a Bayesian linear regression model, you will learn how to assess and compare complex probabilistic models and understand the crucial interplay between priors, data, and predictive performance [@problem_id:3149441].", "problem": "You are tasked with implementing and analyzing Bayesian model selection sensitivity to the prior strength in a Gaussian linear regression, comparing the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV). The educational context is statistical learning at the intermediate undergraduate level, focusing on model selection methods. The analysis must be grounded in fundamental principles of Bayesian inference and predictive performance assessment.\n\nConsider the Gaussian linear regression model with known observation noise variance. Let there be $n$ observations with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^n$. The likelihood is defined by\n$$\ny \\mid X, \\beta \\sim \\mathcal{N}\\left(X \\beta,\\ \\sigma^2 I_n\\right),\n$$\nwhere $\\beta \\in \\mathbb{R}^p$ are the regression coefficients and $\\sigma^2 > 0$ is the known noise variance. Place a zero-mean isotropic Gaussian prior on $\\beta$:\n$$\n\\beta \\sim \\mathcal{N}\\left(0,\\ \\tau^2 I_p\\right),\n$$\nwhere $\\tau^2 > 0$ is the prior variance controlling shrinkage strength (smaller $\\tau^2$ indicates stronger prior regularization). You must study how varying $\\tau^2$ influences Bayesian model selection under two criteria: Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV), defined as follows.\n\n- Widely Applicable Information Criterion (WAIC): Evaluate out-of-sample predictive fit by aggregating the pointwise predictive densities across the posterior, correcting for effective model complexity. You must compute it by Monte Carlo integration over the posterior.\n- Leave-One-Out Cross-Validation (LOO-CV): Evaluate predictive performance by, for each observation $i$, holding it out, recomputing the posterior with the remaining $n-1$ observations, and then aggregating the log predictive density for the held-out observation across posterior draws.\n\nBase your derivations and implementation on the following foundational principles:\n- Bayesian updating under conjugate priors for Gaussian linear models.\n- The definition of expected log predictive density as the integral over the posterior.\n- The meaning of model complexity adjustment as variance of the log-likelihood under the posterior.\n\nData construction must be deterministic and purely mathematical. For a given $(n, p)$ with $p = 3$, construct $X$ and $y$ as follows. For $i = 1, 2, \\ldots, n$, define the $i$-th row of $X$ by\n$$\nx_{i1} = \\frac{i}{n}, \\quad x_{i2} = \\left(\\frac{i}{n}\\right)^2, \\quad x_{i3} = \\sin(i),\n$$\nwhere the argument of the sine function is in radians. Define $y$ by the linear relation\n$$\ny_i = 1.5\\,x_{i1} - 2.0\\,x_{i2} + 0.5\\,x_{i3}.\n$$\nAll trigonometric angles in this problem must be interpreted in radians.\n\nImplementation requirements:\n- Use Monte Carlo approximation with a fixed number of posterior draws $S$ to compute both WAIC and LOO-CV. Set $S = 3000$ posterior samples for all computations.\n- Use a single fixed random seed so that the results are exactly reproducible.\n- For WAIC, compute the pointwise log average of the likelihood over posterior draws and subtract the pointwise posterior variance of the log-likelihood. Sum these across observations to obtain a scalar score, and select the prior variance $\\tau^2$ that maximizes this score.\n- For LOO-CV, for each observation $i$, recompute the posterior leaving out $(x_i, y_i)$, compute the log average of the likelihood of $y_i$ under posterior draws from the leave-one-out posterior, sum these across observations to obtain a scalar score, and select the prior variance $\\tau^2$ that maximizes this score.\n- Return, for each test case, the indices of the optimal $\\tau^2$ under WAIC and under LOO-CV. Indices must be zero-based integers corresponding to positions in the provided grid for $\\tau^2$.\n\nTest suite:\nEvaluate the following four test cases. In all cases use $p = 3$ and the data construction specified above.\n1. Case A: $n = 20$, $\\sigma^2 = 1.0$, $\\tau^2$ grid $[0.1, 0.5, 1.0, 2.0, 10.0]$.\n2. Case B: $n = 20$, $\\sigma^2 = 0.25$, $\\tau^2$ grid $[0.01, 0.1, 0.5, 2.0, 100.0]$.\n3. Case C: $n = 20$, $\\sigma^2 = 4.0$, $\\tau^2$ grid $[0.01, 0.05, 0.1, 0.5, 2.0]$.\n4. Case D (boundary sample size): $n = 5$, $\\sigma^2 = 1.0$, $\\tau^2$ grid $[0.1, 1.0, 10.0]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of pair lists, enclosed in square brackets. Each pair is the zero-based indices of the optimal $\\tau^2$ under WAIC and LOO-CV, respectively, for one test case, in the order listed above. For example, an output with four cases would look like `[[3,3],[1,1],[2,2],[0,1]]`, where each inner pair contains the zero-based index of the optimal $\\tau^2$ under WAIC and LOO-CV, respectively. The final output must be printed as a single line exactly in this format, with no spaces.", "solution": "The problem requires an analysis of the sensitivity of Bayesian model selection criteria to prior strength in a Gaussian linear regression setting. We are asked to compare the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV) for selecting an optimal prior variance, $\\tau^2$, from a given grid of values.\n\nFirst, we establish the mathematical framework for the Bayesian analysis. The model is specified by:\n1.  A Gaussian likelihood for the response vector $y \\in \\mathbb{R}^n$ given the design matrix $X \\in \\mathbb{R}^{n \\times p}$ and regression coefficients $\\beta \\in \\mathbb{R}^p$:\n    $$ y \\mid X, \\beta \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n) $$\n    The noise variance $\\sigma^2$ is assumed to be known.\n2.  A zero-mean isotropic Gaussian prior on the regression coefficients $\\beta$:\n    $$ \\beta \\sim \\mathcal{N}(0, \\tau^2 I_p) $$\n    The hyperparameter $\\tau^2$ controls the strength of the prior regularization, with smaller values enforcing stronger shrinkage of the coefficients towards zero.\n\nDue to the conjugacy of the Gaussian prior with the Gaussian likelihood, the posterior distribution of $\\beta$ is also Gaussian, $p(\\beta \\mid y, X) \\sim \\mathcal{N}(\\mu_{post}, \\Sigma_{post})$. We derive the posterior parameters by combining the exponents of the likelihood and prior density functions. The posterior log-density is proportional to:\n$$ -\\frac{1}{2\\sigma^2} (y - X\\beta)^T(y - X\\beta) - \\frac{1}{2\\tau^2} \\beta^T\\beta $$\nCompleting the square for $\\beta$ reveals that the posterior precision matrix (inverse covariance) is $\\Sigma_{post}^{-1} = \\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\tau^2} I_p$, and the posterior mean is $\\mu_{post} = \\frac{1}{\\sigma^2} \\Sigma_{post} X^T y$. Thus, the posterior distribution is fully characterized by:\n$$ \\Sigma_{post} = \\left( \\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\tau^2} I_p \\right)^{-1} $$\n$$ \\mu_{post} = \\frac{1}{\\sigma^2} \\Sigma_{post} X^T y $$\n\nTo evaluate the model selection criteria, we employ Monte Carlo integration by drawing $S$ samples, $\\{\\beta^{(s)}\\}_{s=1}^S$, from the posterior distribution $\\mathcal{N}(\\mu_{post}, \\Sigma_{post})$.\n\n**Widely Applicable Information Criterion (WAIC)**\n\nWAIC provides an estimate of the out-of-sample predictive accuracy. The problem defines a score to be maximized, which is the sum of pointwise log predictive densities corrected by a penalty term for model complexity. For each data point $i \\in \\{1, \\dots, n\\}$, we compute:\n1.  The log pointwise predictive density, $\\text{lppd}_i$, approximated by the log of the average likelihood over the posterior samples:\n    $$ \\text{lppd}_i = \\log\\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i \\mid \\beta^{(s)})\\right) $$\n    Numerically, this is computed using the log-sum-exp trick to maintain stability: $\\text{lppd}_i = \\text{logsumexp}_{s}(\\log p(y_i \\mid \\beta^{(s)})) - \\log S$.\n2.  The complexity penalty, $p_{WAIC,i}$, approximated by the sample variance of the log-likelihood over the posterior samples:\n    $$ p_{WAIC,i} \\approx \\text{Var}_{s}(\\log p(y_i \\mid \\beta^{(s)})) $$\n\nThe total WAIC score to be maximized is $W = \\sum_{i=1}^n (\\text{lppd}_i - p_{WAIC,i})$. The log-likelihood for a single observation $y_i$ is $\\log p(y_i \\mid \\beta) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}$.\n\n**Leave-One-Out Cross-Validation (LOO-CV)**\n\nLOO-CV provides a more direct, albeit computationally intensive, estimate of out-of-sample predictive performance. For each observation $i=1, \\dots, n$, we perform the following steps:\n1.  Remove the $i$-th data point $(x_i, y_i)$ to form a reduced dataset $(X_{-i}, y_{-i})$.\n2.  Recompute the posterior distribution based on this reduced dataset. This yields a leave-one-out posterior $p(\\beta \\mid y_{-i}, X_{-i}) \\sim \\mathcal{N}(\\mu_{post,-i}, \\Sigma_{post,-i})$. The parameters are calculated as:\n    $$ \\Sigma_{post, -i} = \\left( \\frac{1}{\\sigma^2} X_{-i}^T X_{-i} + \\frac{1}{\\tau^2} I_p \\right)^{-1} $$\n    $$ \\mu_{post, -i} = \\frac{1}{\\sigma^2} \\Sigma_{post, -i} X_{-i}^T y_{-i} $$\n3.  Draw $S$ samples, $\\{\\beta^{(-i,s)}\\}_{s=1}^S$, from this leave-one-out posterior.\n4.  Compute the log predictive density for the held-out point $y_i$, averaged over these samples:\n    $$ \\text{lpd}_{-i} = \\log\\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i \\mid \\beta^{(-i,s)})\\right) $$\n\nThe total LOO-CV score to be maximized is the sum of these individual log predictive densities: $L = \\sum_{i=1}^n \\text{lpd}_{-i}$.\n\n**Computational Procedure**\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Initialize empty lists to store the WAIC and LOO-CV scores for each value of $\\tau^2$ in the provided grid.\n2.  For a given $n$, construct the data matrix $X$ and response vector $y$ as specified. Note that the data-generating process for $y$ is deterministic (noise-free), which is a common setup in simulation studies to provide a known ground truth.\n3.  Iterate through each $\\tau^2$ in the grid:\n    a. Calculate the WAIC score by first computing the full posterior, drawing $S=3000$ samples, and then aggregating the pointwise $\\text{lppd}_i$ and $p_{WAIC,i}$ values.\n    b. Calculate the LOO-CV score by iterating from $i=1$ to $n$, each time re-computing the posterior on the $n-1$ data points, drawing $S=3000$ new samples, and evaluating the log predictive density for the single held-out point. Sum these log densities.\n4.  After computing the scores for all $\\tau^2$ values, find the zero-based index of the maximum score for both WAIC and LOO-CV. These two indices form the result for the test case.\n5.  Aggregate the index pairs from all test cases into a final list and format for output. A fixed random seed ensures the reproducibility of the Monte Carlo sampling.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Bayesian model selection sensitivity to prior strength,\n    comparing WAIC and LOO-CV for a Gaussian linear regression model.\n    \"\"\"\n    S = 3000  # Number of Monte Carlo samples\n    P = 3     # Number of features\n    SEED = 42 # Fixed random seed for reproducibility\n\n    def compute_scores(n, p, sigma_sq, tau_sq, X, y, rng):\n        \"\"\"\n        Computes WAIC and LOO-CV scores for a given model configuration.\n        \"\"\"\n        # Common terms for log-likelihood calculations\n        log_lik_const = -0.5 * np.log(2 * np.pi * sigma_sq)\n        inv_2_sigma_sq = 1.0 / (2.0 * sigma_sq)\n\n        # ---------------- WAIC Calculation ----------------\n        \n        # Full posterior distribution\n        precision_post = (1.0 / sigma_sq) * (X.T @ X) + (1.0 / tau_sq) * np.identity(p)\n        cov_post = np.linalg.inv(precision_post)\n        mean_post = (1.0 / sigma_sq) * (cov_post @ X.T @ y)\n\n        # Draw samples from the full posterior\n        beta_samples = rng.multivariate_normal(mean_post, cov_post, size=S)\n\n        # Calculate pointwise log-likelihoods for all samples\n        # Shape: (n, S)\n        pred_means = X @ beta_samples.T\n        sq_errors = (y[:, np.newaxis] - pred_means)**2\n        log_liks = log_lik_const - sq_errors * inv_2_sigma_sq\n\n        # Compute lppd and p_waic for each data point\n        lppd = logsumexp(log_liks, axis=1) - np.log(S)\n        p_waic = np.var(log_liks, axis=1, ddof=1) # Using ddof=1 for sample variance estimate\n        \n        waic_score = np.sum(lppd - p_waic)\n\n        # ---------------- LOO-CV Calculation ----------------\n        \n        total_loo_score = 0.0\n        for i in range(n):\n            # Create leave-one-out dataset\n            X_loo = np.delete(X, i, axis=0)\n            y_loo = np.delete(y, i)\n            x_i, y_i = X[i], y[i]\n\n            # Recompute posterior for the LOO dataset\n            precision_post_loo = (1.0 / sigma_sq) * (X_loo.T @ X_loo) + (1.0 / tau_sq) * np.identity(p)\n            cov_post_loo = np.linalg.inv(precision_post_loo)\n            mean_post_loo = (1.0 / sigma_sq) * (cov_post_loo @ X_loo.T @ y_loo)\n\n            # Draw samples from the LOO posterior\n            beta_samples_loo = rng.multivariate_normal(mean_post_loo, cov_post_loo, size=S)\n            \n            # Compute predictive density for the held-out point\n            pred_means_i = x_i @ beta_samples_loo.T\n            sq_errors_i = (y_i - pred_means_i)**2\n            log_liks_i = log_lik_const - sq_errors_i * inv_2_sigma_sq\n            \n            lpd_i = logsumexp(log_liks_i) - np.log(S)\n            total_loo_score += lpd_i\n        \n        return waic_score, total_loo_score\n\n    test_cases = [\n        (20, 1.0, np.array([0.1, 0.5, 1.0, 2.0, 10.0])),\n        (20, 0.25, np.array([0.01, 0.1, 0.5, 2.0, 100.0])),\n        (20, 4.0, np.array([0.01, 0.05, 0.1, 0.5, 2.0])),\n        (5, 1.0, np.array([0.1, 1.0, 10.0]))\n    ]\n\n    all_results = []\n    rng = np.random.default_rng(seed=SEED)\n\n    for n, sigma_sq, tau_sq_grid in test_cases:\n        # Generate data deterministically based on n\n        X = np.zeros((n, P))\n        y = np.zeros(n)\n        for j in range(n):\n            i_math = j + 1.0\n            x_i1 = i_math / n\n            x_i2 = (i_math / n)**2\n            x_i3 = np.sin(i_math)\n            X[j, :] = [x_i1, x_i2, x_i3]\n            y[j] = 1.5 * x_i1 - 2.0 * x_i2 + 0.5 * x_i3\n\n        waic_scores = []\n        loo_scores = []\n        for tau_sq in tau_sq_grid:\n            waic, loo = compute_scores(n, P, sigma_sq, tau_sq, X, y, rng)\n            waic_scores.append(waic)\n            loo_scores.append(loo)\n        \n        best_waic_idx = np.argmax(waic_scores)\n        best_loo_idx = np.argmax(loo_scores)\n        \n        all_results.append([best_waic_idx, best_loo_idx])\n\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3149441"}]}