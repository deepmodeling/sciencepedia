{"hands_on_practices": [{"introduction": "To truly understand an algorithm, there is no substitute for building it from the ground up. This first practice challenges you to do just that for the LASSO problem, which seeks to minimize the objective function $\\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1$. By deriving the coordinate-wise update rule from first principles and implementing a complete solver, you will gain a deep, foundational understanding of how coordinate descent leverages simple, one-dimensional optimizations to solve a complex, high-dimensional problem [@problem_id:2861565].", "problem": "You are tasked with deriving and implementing a cyclic coordinate descent algorithm for the Least Absolute Shrinkage and Selection Operator (LASSO) problem using principles from convex optimization and signal processing.\n\nConsider the LASSO objective for a design matrix $A \\in \\mathbb{R}^{m \\times n}$ and observation vector $b \\in \\mathbb{R}^{m}$:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is a given regularization parameter and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm.\n\nYour tasks are:\n1. Derive, from first principles, the single-coordinate minimization update rule used in a cyclic coordinate descent method for $f(x)$. Start from the definition of $f(x)$ and the subgradient optimality condition for the $\\ell_1$ norm, and reason about the minimization of $f(x)$ with respect to a single coordinate $x_i$ while holding all other coordinates fixed. Use only foundational facts including properties of convex functions, subgradients of the absolute value, and basic linear algebra. Do not assume any specific closed-form update a priori.\n2. Show that the coordinate-wise minimizer is obtained by applying the soft-thresholding operator to an affine function of the current iterate and the residual. Clearly define all quantities introduced in your derivation.\n3. Implement a cyclic coordinate descent algorithm that uses the derived update. Your implementation must:\n   - Maintain the residual $r \\triangleq b - A x$ and update it incrementally after each coordinate update to achieve $\\mathcal{O}(m)$ cost per coordinate update.\n   - Use the soft-thresholding operator defined by $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$.\n   - Terminate when either the maximum absolute change in any coordinate during a full cycle is less than a tolerance $\\varepsilon$ or a maximum number of epochs is reached.\n   - Return the final iterate $x$ and, when requested, the sequence of objective values at the end of each epoch to assess monotonicity.\n\nFoundational base you may use:\n- Convexity of $\\|\\cdot\\|_2^2$ and $\\|\\cdot\\|_1$ and properties of their subgradients.\n- Subgradient optimality condition: $0 \\in \\partial f(x^\\star)$ at an optimum $x^\\star$ of a convex function $f$.\n- The subdifferential of the absolute value: for $t \\in \\mathbb{R}$, $\\partial |t| = \\{\\mathrm{sign}(t)\\}$ if $t \\ne 0$, and $\\partial |t| = [-1,1]$ if $t = 0$.\n- Linear algebra identities for residual updates.\n\nDefine the objective value as:\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\nTest suite:\nImplement your program to run the following five test cases and aggregate the results into a single output line.\n\n- Test 1 (orthonormal columns, analytical check): Set $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, $\\lambda = 0.7$. Run your coordinate descent to obtain $x_{\\mathrm{cd}}$. The known analytical solution for orthonormal columns is $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$. Output the scalar\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- Test 2 (general tall system, Karush–Kuhn–Tucker (KKT) check): Generate $A \\in \\mathbb{R}^{60 \\times 30}$ with independent standard normal entries and then normalize each column to have unit $\\ell_2$ norm. Use a fixed pseudorandom seed $0$ to make the instance deterministic. Define $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ with nonzero entries at indices $0,5,10,15,20$ with values $[2.5,-1.7,1.2,-0.9,1.8]$ respectively, and zeros elsewhere. Set $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta \\in \\mathbb{R}^{60}$ has independent normal entries with standard deviation $0.01$ generated with the same seed $0$. Let $\\lambda = 0.05$. Run coordinate descent to obtain $x_{\\mathrm{cd}}$. Verify the KKT conditions for the LASSO: letting $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$,\n  - If $x_{\\mathrm{cd},i} \\ne 0$, then $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$.\n  - If $x_{\\mathrm{cd},i} = 0$, then $|g_i| \\le \\lambda$.\n  Because of numerical error, implement a tolerance of $10^{-4}$ in these checks. Output the boolean $b_2$ indicating whether all coordinates satisfy the KKT conditions within tolerance.\n\n- Test 3 (large regularization drives solution to zero): Use $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, and $\\lambda = 10^6$. Output the boolean $b_3$ indicating whether the returned solution is the zero vector within an absolute tolerance of $10^{-12}$.\n\n- Test 4 (zero regularization reduces to least squares): Generate $A \\in \\mathbb{R}^{40 \\times 10}$ with independent standard normal entries using pseudorandom seed $1$. Generate $b \\in \\mathbb{R}^{40}$ with independent standard normal entries using seed $2$. Let $\\lambda = 0$. Let $x_{\\mathrm{ls}}$ denote the least-squares solution minimizing $\\frac{1}{2}\\|A x - b\\|_2^2$, computed by the standard linear least-squares method. Run coordinate descent to obtain $x_{\\mathrm{cd}}$. Output the scalar\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- Test 5 (monotone descent of objective across epochs): Generate $A \\in \\mathbb{R}^{30 \\times 15}$ and $b \\in \\mathbb{R}^{30}$ with independent standard normal entries using pseudorandom seed $3$. Let $\\lambda = 0.1$. Record the objective after each full pass over all coordinates and verify that the sequence is nonincreasing up to numerical tolerance $10^{-10}$. Output the boolean $b_5$ indicating whether monotonicity holds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[e_1, b_2, b_3, e_4, b_5]$. No physical units are involved in this problem, and no angle units are relevant. All numeric outputs should be real numbers or booleans as specified, with no percentage signs. Your implementation must be robust to the given instances and should not require any user input.", "solution": "We start from the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$. The function $f$ is convex because it is the sum of the convex function $\\frac{1}{2}\\|A x - b\\|_2^2$ and the convex function $\\lambda \\|x\\|_1$.\n\nCyclic coordinate descent minimizes $f$ with respect to one coordinate at a time while holding the others fixed. Fix an index $i \\in \\{1,\\dots,n\\}$ and write $a_i \\in \\mathbb{R}^m$ for the $i$-th column of $A$. Let $x \\in \\mathbb{R}^n$ be the current iterate, and define the residual\n$$\nr \\triangleq b - A x.\n$$\nBecause $A x = \\sum_{j=1}^n a_j x_j$, changing only $x_i$ to a new value $t \\in \\mathbb{R}$ leads to a new vector $x^{(i \\leftarrow t)}$ and residual\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\nThe objective as a function of $t$ (with other coordinates fixed) becomes\n\\begin{align*}\n\\phi_i(t) &\\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n&= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{constant independent of } t \\\\\n&= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{constant}.\n\\end{align*}\nExpanding the squared norm using $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2$, we get\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n&= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\nDiscarding the terms independent of $t$, the coordinate-wise objective reduces to the univariate convex function\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\nCompleting the square, define $d_i \\triangleq \\|a_i\\|_2^2$ and\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{when } d_i > 0.\n$$\nThen\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n&= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n&= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\nIgnoring the constant $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$, the minimization of $\\tilde{\\phi}_i(t)$ over $t$ is equivalent to minimizing\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\nThe subgradient optimality condition for this one-dimensional convex problem is\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\nwhere the subdifferential of the absolute value is $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$ if $t^\\star \\ne 0$, and $\\partial |t^\\star| = [-1, 1]$ if $t^\\star = 0$.\n\nConsider two cases.\n\nCase 1: $t^\\star \\ne 0$. Then the subgradient condition is\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\nThis implies $|c_i| > \\lambda/d_i$, and the solution is obtained by shrinking $c_i$ towards zero by $\\lambda/d_i$ while preserving sign:\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\nCase 2: $t^\\star = 0$. Then the subgradient condition becomes\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\nCombining both cases yields the soft-thresholding form\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\nEquivalently, using the residual definition $r = b - A x$, we have\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\nso the coordinate-wise minimizer is\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\nIf $d_i = \\|a_i\\|_2^2 = 0$ (a zero column), any change in $x_i$ does not affect the quadratic term; the minimizer of $\\lambda |t|$ is $t^\\star = 0$ for $\\lambda > 0$. In our implementation, we set $x_i \\leftarrow 0$ if $d_i = 0$ and $\\lambda > 0$; if $\\lambda = 0$ and $d_i = 0$, the coordinate is irrelevant and can be left unchanged.\n\nEfficient residual update: If $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$, then\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\nwhich costs $\\mathcal{O}(m)$ operations.\n\nConvergence and monotonicity: Each coordinate update exactly minimizes $f$ over that coordinate, so $f$ is nonincreasing after each coordinate update, and hence after each epoch (full pass over all coordinates). The algorithm terminates when the maximum absolute coordinate change in an epoch is below a tolerance or when a maximum number of epochs is reached.\n\nOptimality verification via Karush–Kuhn–Tucker (KKT) conditions: Let $g(x) \\triangleq A^\\top (A x - b)$ be the gradient of the smooth part. The KKT condition for optimality of $x^\\star$ in the LASSO is\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\nwhich is equivalent to the component-wise conditions\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0, & \\text{if } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda, & \\text{if } x_i^\\star = 0.\n\\end{cases}\n$$\nIn practice, we check these equalities and inequalities within a small numerical tolerance.\n\nTest cases and outputs: We implement the five test cases specified and compute\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ for orthonormal columns,\n- $b_2$ indicating KKT satisfaction within tolerance for the tall system,\n- $b_3$ indicating that the solution is zero for very large $\\lambda$,\n- $e_4$ the relative error to the least-squares solution when $\\lambda = 0$,\n- $b_5$ indicating monotone nonincreasing objective values across epochs.\n\nThe final program outputs the results as a single list $[e_1, b_2, b_3, e_4, b_5]$ on one line.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau <= 0:\n        return z\n    abs_z = abs(z)\n    if abs_z <= tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) -> float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = b.copy()  # r = b - A x, initially x=0\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                # If the column is zero, the quadratic does not depend on x_i.\n                # Minimizer of lambd * |t| is t=0 for lambd>0; do nothing if lambd==0.\n                xi_new = 0.0 if lambd > 0 else xi_old\n            else:\n                # c_i = x_i + (a_i^T r) / d_i = (a_i^T r + d_i x_i) / d_i\n                ci = xi_old + float(ai.T @ r) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update residual: r_new = r_old - a_i * delta\n                r -= ai * delta\n                x[i] = xi_new\n                max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta < tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) -> bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| <= lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi) > 1e-12:\n            if abs(gi + lambd * np.sign(xi)) > tol:\n                return False\n        else:\n            if abs(gi) - lambd > tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    # Analytical solution: S_lambda(b)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    # Normalize columns to unit norm\n    col_norms = np.linalg.norm(A2, axis=0)\n    # Avoid division by zero in extremely unlikely all-zero columns\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    for idx, val in zip(nz_idx, nz_vals):\n        x_true[idx] = val\n    noise = rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda -> zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(b3_ok)\n\n    # Test 4: Zero lambda -> least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    # Least squares solution via numpy\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    # Check nonincreasing sequence within small tolerance\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs <= 1e-10))\n    results.append(b5_ok)\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format booleans and floats in a single list\n    # Convert to string with Python default formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2861565"}, {"introduction": "Now that you have a working coordinate descent solver, we address a critical aspect of its practical application: feature standardization. The LASSO penalty, $\\lambda \\|\\beta\\|_1$, does not account for the scale of the features, meaning unstandardized predictors are penalized inequitably. This exercise uses a carefully designed toy dataset to demonstrate how differences in feature magnitudes can skew the path of coordinate descent and lead to incorrect model selection, highlighting why standardization is a crucial preprocessing step [@problem_id:3111928].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) regression problem, which seeks coefficients $\\beta \\in \\mathbb{R}^p$ minimizing the objective\n$$\n\\frac{1}{2}\\,\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta \\rVert_1,\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a design matrix with columns $X_{\\cdot j}$, $y \\in \\mathbb{R}^n$ is a response vector, and $\\lambda \\ge 0$ is a regularization parameter. In cyclic coordinate descent, one updates a single coefficient $\\beta_j$ at a time while keeping all other coordinates fixed, repeatedly cycling through all coordinates until convergence. In practice, the scale of the columns $X_{\\cdot j}$ substantially affects the coordinate-wise update, which can skew the effective shrinkage and bias feature selection when features are not standardized. For this task, feature standardization will mean scaling each column $X_{\\cdot j}$ so that its squared Euclidean norm equals the sample size $n$, i.e., $\\lVert X_{\\cdot j} \\rVert_2^2 = n$.\n\nYour task is to write a complete, runnable program that:\n- Implements cyclic coordinate descent to minimize the LASSO objective, starting from $\\beta = 0$, and iterating until the coefficients change by less than a fixed tolerance $\\tau$ in $\\ell_\\infty$ norm, or until a maximum number of iterations is reached.\n- Runs the solver in two modes for each test case: without feature standardization and with feature standardization (scaling each column to have squared Euclidean norm equal to $n$).\n- Uses the following deterministic toy datasets that are designed to reveal how lack of feature standardization skews coordinate updates and biases feature selection. In all cases, random draws are from a standard normal distribution, and the pseudorandom number generator must be seeded as specified in each case for reproducibility.\n\nTest suite definitions:\n- Case A (scale-skew and collinear features):\n  - Parameters: $n = 60$, $p = 2$, $c = 20$, $\\lambda = 200$, seed $= 13$.\n  - Data generation: draw $u \\in \\mathbb{R}^n$, $e_1 \\in \\mathbb{R}^n$, $e_2 \\in \\mathbb{R}^n$, and $e_y \\in \\mathbb{R}^n$, each with independent and identically distributed standard normal entries. Let $x_1 = u + 0.01\\, e_1$, $x_2 = c\\, u + 0.5\\, e_2$, and $y = u + 0.01\\, e_y$. Form $X = [x_1, x_2]$.\n- Case B (baseline, equal-scale features with one relevant and one irrelevant):\n  - Parameters: $n = 80$, $p = 2$, $\\lambda = 30$, seed $= 7$.\n  - Data generation: draw $u \\in \\mathbb{R}^n$, $v \\in \\mathbb{R}^n$, $e_1 \\in \\mathbb{R}^n$, $e_2 \\in \\mathbb{R}^n$, and $e_y \\in \\mathbb{R}^n$. Let $x_1 = u + 0.01\\, e_1$, $x_2 = v + 0.01\\, e_2$ with $u$ independent of $v$, and $y = u + 0.01\\, e_y$. Form $X = [x_1, x_2]$.\n- Case C (edge case with very strong regularization):\n  - Parameters: $n = 80$, $p = 2$, $\\lambda = 2000$, seed $= 11$.\n  - Data generation: same as Case B.\n\nAlgorithmic requirements:\n- Implement cyclic coordinate descent updates using the residual $r = y - X \\beta$ maintained across coordinates within each iteration, updating $r$ incrementally when a single $\\beta_j$ changes.\n- Use a convergence tolerance $\\tau = 10^{-8}$ and a maximum of $1000$ full cycles through all coordinates.\n- After convergence, identify selected features as those indices $j$ with $\\lvert \\beta_j \\rvert > 10^{-8}$.\n\nFinal output specification:\n- For each test case, produce a triple consisting of:\n  - The list of selected feature indices without standardization.\n  - The list of selected feature indices with standardization.\n  - A boolean indicating whether these two lists differ.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must be the triple described above, represented literally as a Python-style list. For example, the format should be like $[\\,[\\![\\dots]\\! ,\\ [\\![\\dots]\\!] ,\\ \\text{True}]\\, ,\\ \\dots]$ but printed as a standard Python list literal, e.g., $[[[1],[0],True],[[0],[0],False],[[\\,],[\\,],False]]$. No physical units, angle units, or percentages are involved; all outputs are purely mathematical lists and booleans.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem in the field of statistical learning and numerical optimization. All necessary parameters, data generation procedures, and algorithmic constraints are explicitly defined, allowing for a unique and verifiable solution.\n\nThe task is to implement a cyclic coordinate descent algorithm to solve the LASSO (Least Absolute Shrinkage and Selection Operator) regression problem. The objective function to minimize is:\n$$\nL(\\beta) = \\frac{1}{2}\\,\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta \\rVert_1\n$$\nwhere $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the coefficient vector, and $\\lambda \\ge 0$ is the regularization parameter. The $\\ell_1$-norm penalty $\\lVert \\beta \\rVert_1 = \\sum_{j=1}^p |\\beta_j|$ induces sparsity in the solution.\n\nCoordinate descent optimizes the objective function with respect to a single coefficient $\\beta_j$ at a time, holding all other coefficients $\\beta_k$ (for $k \\ne j$) fixed. This process is cycled through all coefficients until convergence.\n\nTo derive the update rule for a single coefficient $\\beta_j$, we consider the objective function as a function of only $\\beta_j$:\n$$\nL(\\beta_j) = \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - \\sum_{k \\ne j} X_{ik}\\beta_k - X_{ij}\\beta_j\\right)^2 + \\lambda \\sum_{k \\ne j} |\\beta_k| + \\lambda |\\beta_j|\n$$\nLet $r_{(-j)} = y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k$ be the partial residual vector, where $X_{\\cdot k}$ is the $k$-th column of $X$. The terms in $L(\\beta_j)$ that depend on $\\beta_j$ are:\n$$\nL_j(\\beta_j) = \\frac{1}{2} \\lVert r_{(-j)} - X_{\\cdot j}\\beta_j \\rVert_2^2 + \\lambda |\\beta_j|\n$$\nThis is a one-dimensional LASSO problem. To find the minimum, we take the subgradient with respect to $\\beta_j$ and set it to $0$. The subgradient $\\partial L_j(\\beta_j)$ is:\n$$\n\\partial L_j(\\beta_j) = -X_{\\cdot j}^T (r_{(-j)} - X_{\\cdot j}\\beta_j) + \\lambda \\partial |\\beta_j|\n$$\nwhere $\\partial |\\beta_j|$ is the subgradient of the absolute value function. Setting the subgradient to $0$ gives the optimality condition:\n$$\nX_{\\cdot j}^T X_{\\cdot j} \\beta_j - X_{\\cdot j}^T r_{(-j)} \\in \\lambda \\partial |\\beta_j|\n$$\nLet $\\rho_j = X_{\\cdot j}^T r_{(-j)}$ and $z_j = X_{\\cdot j}^T X_{\\cdot j} = \\lVert X_{\\cdot j} \\rVert_2^2$. The condition becomes $z_j \\beta_j - \\rho_j \\in \\lambda \\partial |\\beta_j|$. The solution to this is given by the soft-thresholding operator $S(\\cdot, \\cdot)$:\n$$\n\\hat{\\beta}_j = S\\left(\\frac{\\rho_j}{z_j}, \\frac{\\lambda}{z_j}\\right)\n$$\nwhere $S(a, \\nu) = \\text{sign}(a) \\max(|a| - \\nu, 0)$. This can be written piecewise as:\n$$\n\\hat{\\beta}_j =\n\\begin{cases}\n(\\rho_j + \\lambda) / z_j & \\text{if } \\rho_j < -\\lambda \\\\\n(\\rho_j - \\lambda) / z_j & \\text{if } \\rho_j > \\lambda \\\\\n0 & \\text{if } |\\rho_j| \\le \\lambda\n\\end{cases}\n$$\nFor an efficient implementation, we maintain the full residual $r = y - X\\beta$ and update it incrementally. The partial residual term $\\rho_j$ can be expressed using the full residual. Let $\\beta^{\\text{old}}$ be the coefficient vector before updating $\\beta_j$. The current full residual is $r = y - \\sum_k X_{\\cdot k}\\beta_k^{\\text{old}}$. The partial residual is $r_{(-j)} = y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k^{\\text{old}} = r + X_{\\cdot j}\\beta_j^{\\text{old}}$.\nTherefore, $\\rho_j = X_{\\cdot j}^T (r + X_{\\cdot j}\\beta_j^{\\text{old}}) = X_{\\cdot j}^T r + z_j \\beta_j^{\\text{old}}$.\n\nThe cyclic coordinate descent algorithm proceeds as follows:\n1. Initialize $\\beta = 0$, $r = y$. Precompute $z_j = \\lVert X_{\\cdot j} \\rVert_2^2$ for all $j=1, \\dots, p$.\n2. For each iteration (cycle):\n3.   For each coordinate $j=1, \\dots, p$:\n    a. Let $\\beta_j^{\\text{old}}$ be the current value of the $j$-th coefficient.\n    b. Update the residual to effectively compute the partial residual: $r \\leftarrow r + X_{\\cdot j}\\beta_j^{\\text{old}}$.\n    c. Compute $\\rho_j = X_{\\cdot j}^T r$.\n    d. Compute the new coefficient $\\beta_j^{\\text{new}}$ using the soft-thresholding rule with $\\rho_j$ and $z_j$.\n    e. Update the coefficient: $\\beta_j \\leftarrow \\beta_j^{\\text{new}}$.\n    f. Update the residual with the new coefficient: $r \\leftarrow r - X_{\\cdot j}\\beta_j^{\\text{new}}$.\n    g. Keep track of the maximum coefficient change, $\\max_j |\\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}|$, during the cycle.\n4.   After a full cycle, if the maximum coefficient change is less than a tolerance $\\tau=10^{-8}$, or a maximum number of iterations is reached, stop.\n\nThe problem requires running this algorithm in two modes: one with the original data $X$ and one with standardized features. Standardization ensures that the effective regularization is not skewed by the varying scales of the features. The specified standardization rule is to scale each column $X_{\\cdot j}$ to form a new column $X'_{\\cdot j}$ such that its squared Euclidean norm equals the sample size $n$, i.e., $\\lVert X'_{\\cdot j} \\rVert_2^2 = n$. The scaling factor for column $j$ is $s_j = \\sqrt{n / z_j}$. The standardized matrix is $X'_{\\cdot j} = s_j X_{\\cdot j}$. When using $X'$, all $z'_j$ in the update rule become equal to $n$, making the shrinkage term $\\lambda/n$ uniform across all coefficients.\n\nThe solution program implements this algorithm for the specified test cases, generates the data, runs the solver with and without standardization, and reports the indices of selected features (where $|\\beta_j| > 10^{-8}$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef coordinate_descent(X, y, lambda_val, tol=1e-8, max_iter=1000):\n    \"\"\"\n    Solves the LASSO problem using cyclic coordinate descent.\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n\n    # Precompute squared l2-norms of columns of X\n    z = np.sum(X**2, axis=0)\n\n    # Initialize residual. Since beta starts at 0, r = y - X @ 0 = y\n    r = y.copy()\n\n    for _ in range(max_iter):\n        max_change = 0.0\n\n        for j in range(p):\n            beta_old_j = beta[j]\n\n            # The term z[j] might be zero for a column of zeros.\n            # The data generation process makes this impossible, so we don't handle it.\n            if z[j] == 0:\n                continue\n            \n            # Simplified update using incremental residual updates.\n            # At the start of step j, r = y - sum_{k<j} X_k beta_k^new - sum_{k>=j} X_k beta_k^old.\n            # We need rho_j = X_j^T * (y - sum_{k!=j} X_k beta_k).\n            # This is equivalent to X_j^T * (r + X_j * beta_j^old).\n            rho_j = np.dot(X[:, j], r) + z[j] * beta_old_j\n\n            # Soft-thresholding\n            if rho_j < -lambda_val:\n                beta_new_j = (rho_j + lambda_val) / z[j]\n            elif rho_j > lambda_val:\n                beta_new_j = (rho_j - lambda_val) / z[j]\n            else:\n                beta_new_j = 0.0\n\n            delta_beta = beta_new_j - beta_old_j\n            \n            if delta_beta != 0.0:\n                # Update residual incrementally\n                r -= X[:, j] * delta_beta\n                beta[j] = beta_new_j\n\n            # Track maximum change for convergence\n            change = abs(delta_beta)\n            if change > max_change:\n                max_change = change\n        \n        if max_change < tol:\n            break\n            \n    return beta\n\ndef generate_data(case_params):\n    \"\"\"\n    Generates synthetic data based on test case parameters.\n    \"\"\"\n    n = case_params['n']\n    p = case_params['p']\n    seed = case_params['seed']\n    rng = np.random.default_rng(seed)\n\n    if case_params['id'] == 'A':\n        c = case_params['c']\n        u = rng.standard_normal(n)\n        e1 = rng.standard_normal(n)\n        e2 = rng.standard_normal(n)\n        ey = rng.standard_normal(n)\n        x1 = u + 0.01 * e1\n        x2 = c * u + 0.5 * e2\n        y = u + 0.01 * ey\n        X = np.stack([x1, x2], axis=1)\n    elif case_params['id'] in ['B', 'C']:\n        u = rng.standard_normal(n)\n        v = rng.standard_normal(n)\n        e1 = rng.standard_normal(n)\n        e2 = rng.standard_normal(n)\n        ey = rng.standard_normal(n)\n        x1 = u + 0.01 * e1\n        x2 = v + 0.01 * e2\n        y = u + 0.01 * ey\n        X = np.stack([x1, x2], axis=1)\n        \n    return X, y\n\ndef solve():\n    test_cases = [\n        {'id': 'A', 'n': 60, 'p': 2, 'c': 20, 'lambda': 200, 'seed': 13},\n        {'id': 'B', 'n': 80, 'p': 2, 'lambda': 30, 'seed': 7},\n        {'id': 'C', 'n': 80, 'p': 2, 'lambda': 2000, 'seed': 11},\n    ]\n\n    results = []\n    \n    TOL_BETA_SELECT = 1e-8\n    TOL_CONVERGENCE = 1e-8\n    MAX_ITER = 1000\n\n    for case in test_cases:\n        X, y = generate_data(case)\n        lambda_val = case['lambda']\n        n = case['n']\n\n        # Run without standardization\n        beta_no_std = coordinate_descent(X, y, lambda_val, TOL_CONVERGENCE, MAX_ITER)\n        indices_no_std = [j for j, b in enumerate(beta_no_std) if abs(b) > TOL_BETA_SELECT]\n\n        # Run with standardization\n        X_std = X.copy()\n        col_norms_sq = np.sum(X**2, axis=0)\n        # Avoid division by zero, although not expected for this problem's data\n        non_zero_mask = col_norms_sq > 0\n        scaling_factors = np.ones(X.shape[1])\n        scaling_factors[non_zero_mask] = np.sqrt(n / col_norms_sq[non_zero_mask])\n        X_std = X * scaling_factors\n\n        beta_std = coordinate_descent(X_std, y, lambda_val, TOL_CONVERGENCE, MAX_ITER)\n        indices_std = [j for j, b in enumerate(beta_std) if abs(b) > TOL_BETA_SELECT]\n\n        lists_differ = (indices_no_std != indices_std)\n        \n        results.append([indices_no_std, indices_std, lists_differ])\n\n    # Custom string formatting to match the required output literal\n    def format_list(lst):\n        return '[' + ','.join(map(str, lst)) + ']'\n        \n    def format_triple(triple):\n        return f\"[{format_list(triple[0])},{format_list(triple[1])},{str(triple[2])}]\"\n        \n    result_strings = [format_triple(res) for res in results]\n    final_output = '[' + ','.join(result_strings) + ']'\n    \n    print(final_output)\n\nsolve()\n```", "id": "3111928"}, {"introduction": "We conclude our hands-on tour by exploring a fascinating and counter-intuitive property of the LASSO: the non-uniqueness of solutions when features are perfectly correlated. In this exercise, you will create a simple dataset with a duplicated feature and observe how the cyclic coordinate descent algorithm's solution depends on the order of updates. This phenomenon, known as path dependence, provides a powerful insight into the geometry of the LASSO objective function and the behavior of the algorithms used to optimize it [@problem_id:3111866].", "problem": "You are tasked with implementing a cyclic coordinate descent algorithm for the Least Absolute Shrinkage and Selection Operator (LASSO), which minimizes the convex objective\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1,\n$$\nwhere $n$ is the number of samples, $p$ is the number of features, $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\boldsymbol{y} \\in \\mathbb{R}^{n}$ is the response vector, $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the coefficient vector, $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is the regularization parameter, $\\left\\| \\cdot \\right\\|_2$ denotes the Euclidean norm, and $\\left\\| \\cdot \\right\\|_1$ denotes the $L_1$ norm. Starting from fundamental convex optimization principles and subgradient optimality conditions, derive and implement a coordinate-wise update rule that minimizes the objective along one coordinate while holding the others fixed. Your implementation must:\n- Perform cyclic updates in a specified coordinate order over passes through the coordinates.\n- Maintain and update the residual $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$ efficiently after each coordinate update.\n- Terminate when the maximum absolute change in coordinates across one full pass is less than a tolerance (use $10^{-12}$) or when a maximum number of passes is reached (use $100$).\n\nThe central phenomenon to study is solution non-uniqueness when two columns of $\\boldsymbol{X}$ are exactly identical. In such cases, the objective depends only on the sum of the duplicated coefficients, and different allocations across the duplicate coordinates can achieve the same objective value. You must empirically demonstrate coordinate descent path dependence and symmetry-breaking: changing the order of coordinate updates can lead to different coefficient allocations across duplicate features despite identical predictions.\n\nImplement a program that runs the coordinate descent algorithm twice per test case with two different coordinate orders: first with order $[0,1]$ and then with order $[1,0]$. Compute and report quantitative metrics that capture differences between the two runs.\n\nUse the following fixed data for all test cases:\n- Let $n = 10$ and define the vector $\\boldsymbol{x} = [1,2,3,4,5,6,7,8,9,10]^\\top$.\n- Construct $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times 2}$ with two identical columns $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$.\n- Define three test cases differing by the response vector $\\boldsymbol{y}$ and the regularization parameter $\\lambda$:\n  1. Test Case A (happy path): $\\boldsymbol{y} = 3 \\boldsymbol{x}$ and $\\lambda = 5$.\n  2. Test Case B (sign edge case): $\\boldsymbol{y} = -3 \\boldsymbol{x}$ and $\\lambda = 5$.\n  3. Test Case C (boundary case): $\\boldsymbol{y} = 3 \\boldsymbol{x}$ and $\\lambda = 200$.\n\nInitialization and stopping:\n- Initialize $\\boldsymbol{\\beta}$ to the zero vector for all runs.\n- Use tolerance $10^{-12}$ and maximum passes $100$ as specified.\n\nFor each test case, run the algorithm twice (once with coordinate update order $[0,1]$ and once with $[1,0]$) and compute the following four metrics:\n1. The Euclidean norm of the difference between the two coefficient vectors, i.e., $ \\left\\| \\boldsymbol{\\beta}^{(01)} - \\boldsymbol{\\beta}^{(10)} \\right\\|_2 $, as a floating-point number.\n2. The absolute difference in the sum of the duplicate coefficients, i.e., $ \\left| \\left( \\beta^{(01)}_0 + \\beta^{(01)}_1 \\right) - \\left( \\beta^{(10)}_0 + \\beta^{(10)}_1 \\right) \\right| $, as a floating-point number.\n3. The Euclidean norm of the difference between the two prediction vectors, i.e., $ \\left\\| \\boldsymbol{X}\\boldsymbol{\\beta}^{(01)} - \\boldsymbol{X}\\boldsymbol{\\beta}^{(10)} \\right\\|_2 $, as a floating-point number.\n4. A boolean indicating symmetry-breaking, defined as whether the duplicate coefficients differ in at least one run, i.e., whether $ \\left| \\beta^{(01)}_0 - \\beta^{(01)}_1 \\right| > 10^{-9} $ or $ \\left| \\beta^{(10)}_0 - \\beta^{(10)}_1 \\right| > 10^{-9} $.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of the four metrics in the order specified above. For example, the output must have the form\n$$\n[\\,[m_{A,1},m_{A,2},m_{A,3},b_A],\\,[m_{B,1},m_{B,2},m_{B,3},b_B],\\,[m_{C,1},m_{C,2},m_{C,3},b_C]\\,],\n$$\nwhere $m_{\\cdot,\\cdot}$ are floating-point numbers and $b_{\\cdot}$ are booleans. No physical units or angles are involved; all quantities are dimensionless real numbers. Ensure scientific realism by adhering to the convexity of the objective and the definition of subgradient-based coordinate-wise minimization without relying on ad hoc heuristics.", "solution": "The user-provided problem is valid. It is a well-posed and scientifically grounded task in computational statistics, specifically focusing on the coordinate descent algorithm for LASSO regression. All necessary parameters and conditions are provided, and the objective is to explore the known phenomenon of path-dependent solutions in the presence of collinear features, which is a standard topic in optimization.\n\n### 1. The LASSO Objective Function\n\nThe problem asks for the minimization of the LASSO objective function, which is composed of a least-squares data fidelity term and an $L_1$-norm regularization term. The objective function $L(\\boldsymbol{\\beta})$ for a coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is given by:\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1\n$$\nHere, $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $n$ is the number of samples, $p$ is the number of features, and $\\lambda \\ge 0$ is the regularization parameter. The $L_1$ norm is defined as $\\left\\| \\boldsymbol{\\beta} \\right\\|_1 = \\sum_{j=1}^p |\\beta_j|$. This objective function is convex, which guarantees the existence of a global minimum.\n\n### 2. Derivation of the Coordinate-wise Update Rule\n\nCoordinate descent optimizes the objective function with respect to a single coordinate at a time, holding all other coordinates fixed. To derive the update for the $k$-th coefficient, $\\beta_k$, we consider all other coefficients $\\beta_j$ (for $j \\neq k$) to be constant.\n\nThe objective function can be written to isolate the terms involving $\\beta_k$:\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda \\sum_{j \\neq k} |\\beta_j| + \\lambda |\\beta_k|\n$$\nLet's define the partial residual $\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j$. These are the residuals if feature $k$ were removed from the model. The terms not involving $\\beta_k$ are constant with respect to the minimization over $\\beta_k$. The objective to minimize for $\\beta_k$ is thus:\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left\\| \\boldsymbol{r}_k - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda |\\beta_k|\n$$\nExpanding the quadratic term gives:\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left( \\boldsymbol{r}_k^\\top \\boldsymbol{r}_k - 2\\beta_k \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k^2 \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k} \\right) + \\lambda |\\beta_k|\n$$\nTo find the minimum of this convex function, we use the subgradient optimality condition, which states that $0$ must be in the subgradient of $L_k(\\beta_k)$ at the minimizer. The subgradient of the absolute value function $|\\cdot|$ is the sign function `sgn`, which is a set-valued function at $0$.\n$$\n\\frac{\\partial L_k(\\beta_k)}{\\partial \\beta_k} = \\frac{1}{n} \\left( -\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) \\right) + \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\nSetting the subgradient to contain $0$:\n$$\n0 \\in \\frac{1}{n} \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k - \\beta_k \\frac{1}{n} \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\nLet's define $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k$ and $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}$. The condition becomes:\n$$\n\\beta_k z_k \\in \\rho_k - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\nThis leads to the soft-thresholding function $S(a, \\delta) = \\text{sgn}(a)\\max(|a|-\\delta, 0)$. The solution for $\\beta_k$ is:\n$$\n\\beta_k = \\frac{S(\\rho_k, \\lambda)}{z_k}\n$$\nThis update rule minimizes the objective aong the $k$-th coordinate.\n\n### 3. The Coordinate Descent Algorithm and Efficient Updates\n\nThe cyclic coordinate descent algorithm iterates through all coordinates $k=0, 1, \\dots, p-1$ repeatedly until convergence. For computational efficiency, we avoid recomputing the partial residual $\\boldsymbol{r}_k$ at each step. Instead, we maintain the full residual $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$. The partial residual can be expressed in terms of the full residual and the current value of $\\beta_k$ before the update:\n$$\n\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j = \\left(\\boldsymbol{y} - \\sum_{j=1}^p \\boldsymbol{X}_{:,j}\\beta_j \\right) + \\boldsymbol{X}_{:,k}\\beta_k = \\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k\n$$\nSo, the term $\\rho_k$ can be calculated as $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k)$, where $\\boldsymbol{r}$ and $\\beta_k$ are the values before the update for coordinate $k$.\n\nAfter updating $\\beta_k$ from $\\beta_k^{\\text{old}}$ to $\\beta_k^{\\text{new}}$, the full residual $\\boldsymbol{r}$ must also be updated. The change in the prediction is $\\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$. The new residual is:\n$$\n\\boldsymbol{r}^{\\text{new}} = \\boldsymbol{r}^{\\text{old}} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})\n$$\nThis is an efficient $O(n)$ update. The algorithm is as follows:\n\n1.  Initialize $\\boldsymbol{\\beta} = \\boldsymbol{0}$ and $\\boldsymbol{r} = \\boldsymbol{y}$. Precompute $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top\\boldsymbol{X}_{:,k}$ for all $k$.\n2.  For each pass up to a maximum number of passes:\n    a. Initialize `max_abs_change` to $0$.\n    b. For each coordinate $k$ in a specified order:\n        i.   Store $\\beta_k^{\\text{old}} = \\beta_k$.\n        ii.  Calculate $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k^{\\text{old}})$.\n        iii. Calculate $\\beta_k^{\\text{new}} = S(\\rho_k, \\lambda) / z_k$.\n        iv.  Update the residual: $\\boldsymbol{r} \\leftarrow \\boldsymbol{r} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$.\n        v.   Update the coefficient: $\\beta_k \\leftarrow \\beta_k^{\\text{new}}$.\n        vi.  Update `max_abs_change` with $|\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}}|$.\n    c. If `max_abs_change` is less than a tolerance (e.g., $10^{-12}$), terminate.\n3.  Return the final coefficient vector $\\boldsymbol{\\beta}$.\n\n### 4. Non-uniqueness with Identical Columns\n\nThe problem sets up a scenario where two columns of $\\boldsymbol{X}$ are identical, i.e., $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$. In this case, the model prediction is:\n$$\n\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}_{:,0}\\beta_0 + \\boldsymbol{X}_{:,1}\\beta_1 = \\boldsymbol{x}\\beta_0 + \\boldsymbol{x}\\beta_1 = (\\beta_0 + \\beta_1)\\boldsymbol{x}\n$$\nThe objective function's data fidelity term depends only on the sum of the coefficients, $\\beta_{\\text{sum}} = \\beta_0 + \\beta_1$. The $L_1$ penalty term is $\\lambda(|\\beta_0| + |\\beta_1|)$. The optimization problem becomes:\n$$\n\\min_{\\beta_0, \\beta_1} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - (\\beta_0 + \\beta_1)\\boldsymbol{x} \\right\\|_2^2 + \\lambda(|\\beta_0| + |\\beta_1|)\n$$\nWhile the optimal value of the sum $\\beta_{\\text{sum}}$ is unique, there can be infinitely many pairs $(\\beta_0, \\beta_1)$ that produce this sum. For any given optimal $\\beta_{\\text{sum}}$, the $L_1$ penalty term $\\lambda(|\\beta_0| + |\\beta_1|)$ is minimized when the entire magnitude of $\\beta_{\\text{sum}}$ is allocated to a single coefficient (e.g., $\\beta_0 = \\beta_{\\text{sum}}, \\beta_1 = 0$ or vice-versa), assuming $\\beta_{\\text{sum}} \\neq 0$.\n\nThe cyclic coordinate descent algorithm breaks this symmetry based on the update order. When updating coordinate $0$ first, it will absorb as much of the signal as possible, potentially leaving a small residual for coordinate $1$. If the residual signal is below the threshold set by $\\lambda$, the update for $\\beta_1$ will be zero. Reversing the order to $[1, 0]$ will cause $\\beta_1$ to absorb the signal first, resulting in a different final coefficient vector $\\boldsymbol{\\beta}$ even though the sum $\\beta_0+\\beta_1$ and the predictions $\\boldsymbol{X}\\boldsymbol{\\beta}$ will be identical (up to numerical precision). This path dependence is a key characteristic of coordinate descent on non-strictly convex objectives.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the coordinate descent for LASSO to demonstrate path dependence.\n    \"\"\"\n\n    def coordinate_descent(X, y, lambda_val, order, tol, max_passes):\n        \"\"\"\n        Performs cyclic coordinate descent for LASSO.\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n        residual = y.copy()\n\n        # Precompute column normalization factors\n        z = np.array([(1/n) * X[:, k].T @ X[:, k] for k in range(p)])\n\n        def soft_threshold(a, delta):\n            return np.sign(a) * np.maximum(np.abs(a) - delta, 0)\n\n        for _ in range(max_passes):\n            max_abs_change = 0.0\n            \n            for k in order:\n                beta_old_k = beta[k]\n                \n                # Calculate rho_k using the efficient residual update formula\n                # rho_k = (1/n) * X_k^T * (y - sum_{j!=k} X_j * beta_j)\n                # which is equivalent to (1/n) * X_k^T * (residual + X_k * beta_old_k)\n                rho_k = (1/n) * X[:, k].T @ (residual + X[:, k] * beta_old_k)\n\n                # Update beta_k using soft-thresholding\n                # Added a check for z[k] being zero to avoid division by zero\n                if z[k] > 0:\n                    beta[k] = soft_threshold(rho_k, lambda_val) / z[k]\n                else:\n                    beta[k] = 0.0\n                \n                delta_beta_k = beta[k] - beta_old_k\n                \n                if delta_beta_k != 0:\n                    # Update residual efficiently\n                    residual -= X[:, k] * delta_beta_k\n\n                max_abs_change = max(max_abs_change, np.abs(delta_beta_k))\n\n            if max_abs_change < tol:\n                break\n                \n        return beta\n\n    # --- Fixed Data ---\n    n = 10\n    x_vec = np.arange(1, 11, dtype=float)\n    X = np.stack([x_vec, x_vec], axis=1)\n    \n    # --- Algorithm Parameters ---\n    tol = 1e-12\n    max_passes = 100\n    sym_tol = 1e-9\n\n    # --- Test Cases ---\n    test_cases_params = [\n        # Case A: happy path\n        {'y': 3 * x_vec, 'lambda': 5.0},\n        # Case B: sign edge case\n        {'y': -3 * x_vec, 'lambda': 5.0},\n        # Case C: boundary case (high regularization)\n        {'y': 3 * x_vec, 'lambda': 200.0},\n    ]\n\n    results = []\n    \n    for params in test_cases_params:\n        y = params['y']\n        lambda_val = params['lambda']\n\n        # Run with order [0, 1]\n        beta_01 = coordinate_descent(X, y, lambda_val, order=[0, 1], tol=tol, max_passes=max_passes)\n        \n        # Run with order [1, 0]\n        beta_10 = coordinate_descent(X, y, lambda_val, order=[1, 0], tol=tol, max_passes=max_passes)\n\n        # --- Compute Metrics ---\n        # 1. Euclidean norm of the difference between the two coefficient vectors\n        metric1 = np.linalg.norm(beta_01 - beta_10)\n\n        # 2. Absolute difference in the sum of the duplicate coefficients\n        metric2 = np.abs(np.sum(beta_01) - np.sum(beta_10))\n\n        # 3. Euclidean norm of the difference between the two prediction vectors\n        pred_01 = X @ beta_01\n        pred_10 = X @ beta_10\n        metric3 = np.linalg.norm(pred_01 - pred_10)\n\n        # 4. Boolean indicating symmetry-breaking\n        m4_cond1 = np.abs(beta_01[0] - beta_01[1]) > sym_tol\n        m4_cond2 = np.abs(beta_10[0] - beta_10[1]) > sym_tol\n        metric4 = m4_cond1 or m4_cond2\n\n        results.append([metric1, metric2, metric3, bool(metric4)])\n\n    # Format output as specified: [[m_A1,m_A2,m_A3,b_A],[m_B1,m_B2,m_B3,b_B],...]\n    inner_results_str = []\n    for metrics in results:\n        m1, m2, m3, b = metrics\n        # Use str(b) to get 'True' or 'False'\n        inner_results_str.append(f\"[{m1},{m2},{m3},{str(b)}]\")\n    \n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3111866"}]}