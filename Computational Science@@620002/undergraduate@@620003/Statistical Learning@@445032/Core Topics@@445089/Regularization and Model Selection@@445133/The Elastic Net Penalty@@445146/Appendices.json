{"hands_on_practices": [{"introduction": "To truly understand how elastic net models are fitted, we must look under the hood at the optimization algorithms that power them. This exercise guides you through the derivation of the proximal operator for the elastic net penalty [@problem_id:2164012]. This operator is a fundamental building block in modern, efficient solvers like proximal gradient descent, and deriving its closed-form expression reveals how the $L_1$ and $L_2$ penalties elegantly combine into a single, analytical update step.", "problem": "In many advanced signal processing applications, such as compressed sensing in Magnetic Resonance Imaging (MRI), a key task is to recover a signal $\\mathbf{x} \\in \\mathbb{R}^n$ from measurements. The elastic net regularization is a powerful technique for this, as it promotes solutions that are sparse (having many zero components) while also handling correlations between variables.\n\nProximal gradient methods are iterative algorithms well-suited for solving optimization problems involving such regularizers. A fundamental building block of these methods is the proximal operator. For a given function $f(\\mathbf{x})$, its proximal operator with a scalar parameter $\\gamma > 0$ is defined as:\n$$ \\text{prox}_{\\gamma f}(\\mathbf{z}) = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\left( f(\\mathbf{x}) + \\frac{1}{2\\gamma} \\|\\mathbf{x} - \\mathbf{z}\\|_2^2 \\right) $$\nA standard function used in expressing such operators is the element-wise soft-thresholding operator. For a vector $\\mathbf{a}$ and a threshold $\\kappa > 0$, it is defined as $(S_\\kappa(\\mathbf{a}))_i = \\text{sign}(a_i) \\max(|a_i| - \\kappa, 0)$ for each component $i$.\n\nConsider the elastic net penalty function $f(\\mathbf{x}) = \\lambda_1 \\|\\mathbf{x}\\|_1 + \\frac{\\lambda_2}{2} \\|\\mathbf{x}\\|_2^2$, where $\\lambda_1 > 0$ and $\\lambda_2 > 0$ are positive regularization parameters, $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n |x_i|$ is the $L_1$ norm, and $\\|\\mathbf{x}\\|_2^2 = \\sum_{i=1}^n x_i^2$ is the squared $L_2$ norm.\n\nYour task is to derive a closed-form analytical expression for the proximal operator $\\text{prox}_{\\gamma f}(\\mathbf{z})$. The final expression should be written in a compact vector form.", "solution": "We seek $\\operatorname{prox}_{\\gamma f}(\\mathbf{z})$ for $f(\\mathbf{x})=\\lambda_{1}\\|\\mathbf{x}\\|_{1}+\\frac{\\lambda_{2}}{2}\\|\\mathbf{x}\\|_{2}^{2}$ with $\\gamma>0$, $\\lambda_{1}>0$, and $\\lambda_{2}>0$. By definition,\n$$\n\\operatorname{prox}_{\\gamma f}(\\mathbf{z})=\\arg\\min_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\left(\\lambda_{1}\\|\\mathbf{x}\\|_{1}+\\frac{\\lambda_{2}}{2}\\|\\mathbf{x}\\|_{2}^{2}+\\frac{1}{2\\gamma}\\|\\mathbf{x}-\\mathbf{z}\\|_{2}^{2}\\right).\n$$\nThe objective is separable across coordinates because each term is a sum of functions applied component-wise. Hence, for each $i\\in\\{1,\\dots,n\\}$, we solve the scalar problem\n$$\n\\min_{x\\in\\mathbb{R}}\\left(\\lambda_{1}|x|+\\frac{\\lambda_{2}}{2}x^{2}+\\frac{1}{2\\gamma}(x-z)^{2}\\right),\n$$\nwhere $z$ denotes the $i$-th component $z_{i}$. Expand and group terms:\n$$\n\\lambda_{1}|x|+\\frac{1}{2}\\left(\\lambda_{2}+\\frac{1}{\\gamma}\\right)x^{2}-\\frac{z}{\\gamma}x+\\frac{z^{2}}{2\\gamma}.\n$$\nThe additive constant $\\frac{z^{2}}{2\\gamma}$ does not affect the minimizer. Let $a=\\lambda_{2}+\\frac{1}{\\gamma}$ and $b=\\frac{z}{\\gamma}$. Then equivalently we minimize\n$$\n\\frac{a}{2}x^{2}-bx+\\lambda_{1}|x|=\\frac{a}{2}\\left(x-\\frac{b}{a}\\right)^{2}+\\lambda_{1}|x|+\\text{constant}.\n$$\nSince $a>0$, dividing by the positive constant $a$ does not change the minimizer, so the problem is equivalent to\n$$\n\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}\\left(x-\\frac{b}{a}\\right)^{2}+\\frac{\\lambda_{1}}{a}|x|\\right).\n$$\nBy the known proximal identity $\\operatorname{prox}_{\\kappa|\\cdot|}(u)=S_{\\kappa}(u)$, the scalar minimizer is\n$$\nx^{\\star}=S_{\\lambda_{1}/a}\\!\\left(\\frac{b}{a}\\right).\n$$\nSubstituting $a=\\lambda_{2}+\\frac{1}{\\gamma}$ and $b=\\frac{z}{\\gamma}$ gives\n$$\nx^{\\star}=S_{\\frac{\\gamma\\lambda_{1}}{1+\\gamma\\lambda_{2}}}\\!\\left(\\frac{z}{1+\\gamma\\lambda_{2}}\\right).\n$$\nUsing the scaling property $S_{\\alpha\\tau}(\\alpha u)=\\alpha S_{\\tau}(u)$ for all $\\alpha>0$, this can be written as\n$$\nx^{\\star}=\\frac{1}{1+\\gamma\\lambda_{2}}\\,S_{\\gamma\\lambda_{1}}(z).\n$$\nApplying this element-wise over all components yields the compact vector form\n$$\n\\operatorname{prox}_{\\gamma f}(\\mathbf{z})=\\frac{1}{1+\\gamma\\lambda_{2}}\\,S_{\\gamma\\lambda_{1}}(\\mathbf{z}),\n$$\nwhere $S_{\\kappa}(\\cdot)$ is the element-wise soft-thresholding operator.", "answer": "$$\\boxed{\\frac{1}{1+\\gamma\\lambda_{2}}\\,S_{\\gamma\\lambda_{1}}(\\mathbf{z})}$$", "id": "2164012"}, {"introduction": "Moving from pure theory to practical application requires us to consider how our models interact with the characteristics of real-world data. This practice explores a critical preprocessing step: feature standardization [@problem_id:3182165]. Through a carefully designed thought experiment, you will discover that the penalty is not scale-invariant and see precisely why placing all predictors on a common scale is crucial for ensuring fair competition among variables during the selection process.", "problem": "You are fitting a linear model with an Elastic Net (EN) penalty to a response vector $y \\in \\mathbb{R}^{n}$ using a design matrix $X \\in \\mathbb{R}^{n \\times p}$. Consider the objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right),\n$$\nwith mixing parameter $\\alpha \\in (0,1]$ and tuning parameter $\\lambda > 0$. You will examine how feature standardization affects which variables enter the EN regularization path as $\\lambda$ decreases from $+\\infty$.\n\nAssume the following data conditions:\n- There are $p=2$ features with columns $x_{1},x_{2} \\in \\mathbb{R}^{n}$ that are centered, with sample inner product $(1/n)\\,x_{1}^{\\top}x_{2}=0$.\n- Their sample variances are $(1/n)\\,x_{1}^{\\top}x_{1}=s_{1}^{2}$ and $(1/n)\\,x_{2}^{\\top}x_{2}=s_{2}^{2}$, with $s_{1}=10$ and $s_{2}=1$.\n- Let $z_{j}=x_{j}/s_{j}$ denote the standardized columns, so $(1/n)\\,z_{j}^{\\top}z_{j}=1$ for $j\\in\\{1,2\\}$.\n- The response $y$ is centered and has equal standardized correlation with the two features: $(1/n)\\,z_{1}^{\\top}y=(1/n)\\,z_{2}^{\\top}y=1$.\n\nSuppose you fit the EN with $\\alpha=0.5$ and follow the solution path as $\\lambda$ decreases from a very large value to $0$. Consider two procedures:\n- Procedure U: Fit EN on the unstandardized columns $x_{1},x_{2}$.\n- Procedure S: First standardize the columns to $z_{1},z_{2}$ (unit variance), then fit EN.\n\nWhich statement best describes the effect of standardization on the order in which $x_{1}$ and $x_{2}$ enter the EN path and explains why failure to standardize can distort relative penalization and mislead selection?\n\nA. In Procedure U, $x_{1}$ becomes nonzero at a larger value of $\\lambda$ than $x_{2}$ and therefore enters first; in Procedure S, they enter simultaneously. Without standardization, the penalty acts on coefficients that scale inversely with feature variance, so the larger-variance feature $x_{1}$ is effectively penalized less for the same standardized signal, biasing selection toward $x_{1}$.\n\nB. In Procedure U, $x_{2}$ becomes nonzero first because its coefficient must be larger to fit the same signal and thus more easily overcomes the penalty; in Procedure S, $x_{1}$ still enters first. Without standardization, small-variance features are effectively penalized less.\n\nC. The ridge component $((1-\\alpha)\\lVert \\beta \\rVert_{2}^{2})$ eliminates all scale effects, so Procedures U and S produce the same entry order for any $\\alpha \\in (0,1)$; standardization is unnecessary for EN.\n\nD. Because $x_{1}$ and $x_{2}$ are orthogonal, both features always enter simultaneously for any $\\lambda$ in both Procedures U and S, provided their standardized correlations with $y$ are equal; orthogonality ensures scale invariance of the entry order.", "solution": "The user wants me to analyze the effect of feature standardization on the variable entry order in an Elastic Net regularization path.\n\n### Step 1: Extract Givens\n\n- **Objective Function:** $\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right)$\n- **Parameters:** $\\lambda > 0$, $\\alpha \\in (0,1]$. For this problem, specifically $\\alpha = 0.5$.\n- **Response:** $y \\in \\mathbb{R}^{n}$, centered.\n- **Features:** $p=2$ features, $x_{1}, x_{2} \\in \\mathbb{R}^{n}$.\n- **Data Conditions:**\n    - Features are centered.\n    - Sample orthogonality: $(1/n)\\,x_{1}^{\\top}x_{2}=0$.\n    - Sample variances: $(1/n)\\,x_{1}^{\\top}x_{1}=s_{1}^{2}$ and $(1/n)\\,x_{2}^{\\top}x_{2}=s_{2}^{2}$.\n    - Standard deviations: $s_{1}=10$ and $s_{2}=1$.\n    - Standardized features: $z_{j}=x_{j}/s_{j}$ for $j \\in \\{1,2\\}$, so $(1/n)\\,z_{j}^{\\top}z_{j}=1$.\n    - Standardized correlations with response: $(1/n)\\,z_{1}^{\\top}y = 1$ and $(1/n)\\,z_{2}^{\\top}y = 1$.\n- **Procedures:**\n    - **Procedure U:** Fit EN on unstandardized columns $x_{1}, x_{2}$.\n    - **Procedure S:** Fit EN on standardized columns $z_{1}, z_{2}$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is scientifically grounded, well-posed, and objective.\n1.  **Scientific/Factual Soundness**: The problem is formulated within the standard framework of regularized linear models (Elastic Net), a core topic in statistical learning and optimization. The objective function is the canonical form for the Elastic Net. The concepts of feature standardization, regularization paths, and variable entry order are well-defined and fundamental to the method.\n2.  **Well-Posed**: The problem provides a complete and consistent set of conditions. The number of features, their properties (orthogonality, variances), and their relationship to the response variable are all specified, allowing for a unique, determinable solution to the question of variable entry order.\n3.  **Objective**: The problem uses precise mathematical and statistical language, with no subjective or ambiguous terms.\n4.  **Completeness and Consistency**: The provided data ($s_1$, $s_2$, standardized correlations) are sufficient and non-contradictory.\n5.  **No other flaws detected**: The problem is not trivial, as it probes a crucial and often misunderstood aspect of penalized regression. It is formalizable and mathematically verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. Proceeding to the solution.\n\n### Derivation\n\nThe order in which variables enter the Elastic Net regularization path as the tuning parameter $\\lambda$ decreases from $+\\infty$ is determined by the Karush-Kuhn-Tucker (KKT) optimality conditions. A variable $j$ will enter the model (its coefficient $\\beta_j$ becomes non-zero) at the largest value of $\\lambda$ for which the KKT conditions can be satisfied with a non-zero $\\beta_j$.\n\nLet the objective function be $L(\\beta)$.\n$$\nL(\\beta) = \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\alpha \\sum_{j=1}^{p}|\\beta_j| + \\frac{1-\\alpha}{2}\\sum_{j=1}^{p}\\beta_j^2\\right)\n$$\nThe KKT conditions state that the subgradient of $L(\\beta)$ with respect to $\\beta_j$ at the optimal solution must contain $0$. The subgradient is:\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = -\\frac{1}{n}x_j^{\\top}(y - X\\beta) + \\lambda(\\alpha \\cdot \\text{sgn}(\\beta_j) + (1-\\alpha)\\beta_j)\n$$\nwhere $\\text{sgn}(\\beta_j)$ is the subgradient of the absolute value function, which is $\\{-1, 1\\}$ if $\\beta_j \\neq 0$ and $[-1, 1]$ if $\\beta_j = 0$.\n\nWe are interested in the threshold value of $\\lambda$ at which the first coefficient becomes non-zero. At this point, all other coefficients are still zero. We can thus set $\\beta=0$ in the KKT conditions. The condition for $\\beta_j$ to remain zero is that a value $s_j \\in [-1, 1]$ exists such that:\n$$\n-\\frac{1}{n}x_j^{\\top}(y - X\\cdot \\mathbf{0}) + \\lambda(\\alpha \\cdot s_j + (1-\\alpha)\\cdot 0) = 0\n$$\n$$\n-\\frac{1}{n}x_j^{\\top}y + \\lambda \\alpha s_j = 0 \\quad \\implies \\quad \\frac{1}{n}x_j^{\\top}y = \\lambda \\alpha s_j\n$$\nFor a solution with $\\beta_j=0$ to exist, we must have $|\\frac{1}{n}x_j^{\\top}y| \\le \\lambda \\alpha$, since $|s_j| \\le 1$.\nA coefficient $\\beta_j$ will become non-zero as soon as $\\lambda$ drops below the value where this condition can no longer be met. This threshold value, $\\lambda_{j,\\text{enter}}$, is:\n$$\n\\lambda_{j,\\text{enter}} = \\frac{|\\frac{1}{n}x_j^{\\top}y|}{\\alpha}\n$$\nThe first variable to enter the model is the one with the largest $\\lambda_{j,\\text{enter}}$, which corresponds to the feature $j$ that maximizes the absolute inner product $|\\frac{1}{n}x_j^{\\top}y|$. The ridge penalty term, proportional to $(1-\\alpha)$, has a gradient of $0$ at $\\beta=0$ and thus does not influence the entry order.\n\nNow, we apply this principle to the two procedures.\n\n**Procedure U: Fit on Unstandardized Data ($x_{1}, x_{2}$)**\nWe need to find the inner products $|\\frac{1}{n}x_1^{\\top}y|$ and $|\\frac{1}{n}x_2^{\\top}y|$.\nWe are given $(1/n)z_j^{\\top}y = 1$ and $z_j = x_j/s_j$.\nFor feature $1$:\n$$\n\\frac{1}{n}z_1^{\\top}y = \\frac{1}{n}\\left(\\frac{x_1}{s_1}\\right)^{\\top}y = \\frac{1}{s_1}\\left(\\frac{1}{n}x_1^{\\top}y\\right) = 1\n$$\n$$\n\\implies \\frac{1}{n}x_1^{\\top}y = s_1 = 10\n$$\nFor feature $2$:\n$$\n\\frac{1}{n}z_2^{\\top}y = \\frac{1}{n}\\left(\\frac{x_2}{s_2}\\right)^{\\top}y = \\frac{1}{s_2}\\left(\\frac{1}{n}x_2^{\\top}y\\right) = 1\n$$\n$$\n\\implies \\frac{1}{n}x_2^{\\top}y = s_2 = 1\n$$\nThe entry thresholds are:\n- For $x_1$: $\\lambda_{1,\\text{enter}} = \\frac{|10|}{\\alpha} = \\frac{10}{0.5} = 20$.\n- For $x_2$: $\\lambda_{2,\\text{enter}} = \\frac{|1|}{\\alpha} = \\frac{1}{0.5} = 2$.\n\nSince $\\lambda_{1,\\text{enter}} > \\lambda_{2,\\text{enter}}$, feature $x_1$ enters the model first (at a larger value of $\\lambda$).\n\n**Procedure S: Fit on Standardized Data ($z_{1}, z_{2}$)**\nThe features are now $z_1$ and $z_2$. We need to find the inner products $|\\frac{1}{n}z_1^{\\top}y|$ and $|\\frac{1}{n}z_2^{\\top}y|$. These are given directly in the problem statement.\n$$\n\\frac{1}{n}z_1^{\\top}y = 1\n$$\n$$\n\\frac{1}{n}z_2^{\\top}y = 1\n$$\nThe entry thresholds are:\n- For $z_1$: $\\lambda_{1,\\text{enter}} = \\frac{|1|}{\\alpha} = \\frac{1}{0.5} = 2$.\n- For $z_2$: $\\lambda_{2,\\text{enter}} = \\frac{|1|}{\\alpha} = \\frac{1}{0.5} = 2$.\n\nSince $\\lambda_{1,\\text{enter}} = \\lambda_{2,\\text{enter}}$, both features enter the model simultaneously.\n\n**Explanation of the Effect of Standardization**\nThe L1-penalty component of Elastic Net, $\\lambda \\alpha \\sum |\\beta_j|$, is applied directly to the coefficients $\\beta_j$. These coefficients are not scale-invariant. Consider the relationship between the coefficients of the unstandardized model ($\\beta$) and a hypothetical standardized model ($\\gamma$) where $y \\approx \\sum \\gamma_j z_j$. Substituting $z_j = x_j/s_j$, we get $y \\approx \\sum \\gamma_j (x_j/s_j) = \\sum (\\gamma_j/s_j)x_j$. Thus, $\\beta_j = \\gamma_j/s_j$.\n\nThe L1 penalty on the unstandardized coefficients is $\\lambda \\alpha \\sum|\\beta_j| = \\lambda \\alpha \\sum|\\gamma_j/s_j|$. The penalty on the underlying \"effect size\" $\\gamma_j$ is inversely proportional to the feature's standard deviation $s_j$.\nFor feature $x_1$, with $s_1=10$, the penalty on its effect is scaled by $1/10$.\nFor feature $x_2$, with $s_2=1$, the penalty on its effect is scaled by $1/1$.\nTherefore, the feature with the larger variance ($x_1$) is effectively penalized less. Its coefficient $\\beta_1$ can be smaller than $\\beta_2$ for the same predictive impact, allowing it to \"fly under the radar\" of the penalty more easily. This biases variable selection in favor of features with higher variance, which is what we observed in Procedure U. Standardization puts all features on an equal footing with respect to the penalty, which is why they enter simultaneously in Procedure S, reflecting their equal standardized correlation with the response.\n\n### Option-by-Option Analysis\n\n**A. In Procedure U, $x_{1}$ becomes nonzero at a larger value of $\\lambda$ than $x_{2}$ and therefore enters first; in Procedure S, they enter simultaneously. Without standardization, the penalty acts on coefficients that scale inversely with feature variance, so the larger-variance feature $x_{1}$ is effectively penalized less for the same standardized signal, biasing selection toward $x_{1}$.**\n- **Evaluation:** This statement aligns perfectly with the derivation.\n    - Procedure U: $x_1$ enters first at $\\lambda = 20$, while $x_2$ enters at $\\lambda = 2$. Correct.\n    - Procedure S: They enter simultaneously at $\\lambda=2$. Correct.\n    - Explanation: The reasoning that the penalty acts on coefficients that scale inversely with variance, leading to a smaller effective penalty on high-variance features, is precisely the correct mechanism.\n- **Verdict:** **Correct**.\n\n**B. In Procedure U, $x_{2}$ becomes nonzero first because its coefficient must be larger to fit the same signal and thus more easily overcomes the penalty; in Procedure S, $x_{1}$ still enters first. Without standardization, small-variance features are effectively penalized less.**\n- **Evaluation:** This contains multiple errors.\n    - Procedure U: It claims $x_2$ enters first, which is the opposite of the derived result.\n    - Procedure S: It claims $x_1$ enters first, which is false; they enter simultaneously.\n    - Explanation: It incorrectly claims small-variance features are penalized less. The opposite is true: a small-variance feature requires a larger coefficient, which is then more heavily penalized by the L1 norm.\n- **Verdict:** **Incorrect**.\n\n**C. The ridge component $((1-\\alpha)\\lVert \\beta \\rVert_{2}^{2})$ eliminates all scale effects, so Procedures U and S produce the same entry order for any $\\alpha \\in (0,1)$; standardization is unnecessary for EN.**\n- **Evaluation:** This is fundamentally incorrect. The derivation showed that the entry order is determined by the L1 penalty term and is independent of the ridge component (since its gradient is zero at $\\beta=0$). The ridge penalty is sensitive to scale, but it does not influence which variable becomes non-zero *first*. Furthermore, the derivation explicitly showed that Procedures U and S have different entry orders, disproving the main claim.\n- **Verdict:** **Incorrect**.\n\n**D. Because $x_{1}$ and $x_{2}$ are orthogonal, both features always enter simultaneously for any $\\lambda$ in both Procedures U and S, provided their standardized correlations with $y$ are equal; orthogonality ensures scale invariance of the entry order.**\n- **Evaluation:** While orthogonality simplifies the analysis, it does not guarantee scale invariance of the entry order. The entry order is determined by $|\\frac{1}{n}x_j^{\\top}y|$, which is scale-dependent. Our result for Procedure U, where $x_1$ enters before $x_2$ despite orthogonality, serves as a direct counterexample.\n- **Verdict:** **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3182165"}, {"introduction": "The true power of the elastic net penalty is best observed when it is put into action, especially in scenarios where its predecessor, the LASSO, falls short. This capstone practice challenges you to build a solver from scratch and witness the signature 'grouping effect' firsthand [@problem_id:3191319]. By constructing a dataset with correlated predictors, you will demonstrate how the elastic net can rescue a weak but important variable that the LASSO would otherwise discard, showcasing its robustness in high-dimensional settings.", "problem": "You are asked to write a complete, runnable program that constructs a synthetic regression setting in which the Least Absolute Shrinkage and Selection Operator (LASSO) fails to select a weak but necessary variable because it is highly correlated with a stronger variable, and then shows how the Elastic Net can recover the weak variable through its grouping effect. Your program must implement its own coordinate descent solvers for both penalties from first principles, and must not rely on any model-fitting functions from external libraries.\n\nBackground definitions to be used as the starting point:\n- Consider a linear model with response $y \\in \\mathbb{R}^{n}$ and predictors $X \\in \\mathbb{R}^{n \\times p}$; the Ordinary Least Squares (OLS) objective is the minimization of the mean squared error, i.e., minimizing $\\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2}$ over $\\beta \\in \\mathbb{R}^{p}$.\n- The LASSO adds an $\\ell_{1}$ penalty to encourage sparsity: minimize $\\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{1}$ over $\\beta \\in \\mathbb{R}^{p}$ for $\\lambda \\ge 0$.\n- The Elastic Net (EN) combines $\\ell_{1}$ and $\\ell_{2}$ penalties to encourage both sparsity and grouping: minimize\n$\\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right)$\nover $\\beta \\in \\mathbb{R}^{p}$ where $\\lambda \\ge 0$ and the mixing parameter $\\alpha \\in [0,1]$.\n- A standard and well-tested practice for such problems is to standardize the predictors (zero mean, unit variance) and center the response, so that a zero intercept is appropriate during fitting.\n\nYour task:\n- Generate two-predictor data $X = [X_{1}, X_{2}]$ with a specified correlation $\\rho$ between the columns. Use a linear signal $y = X\\beta + \\varepsilon$ with $\\beta = (\\beta_{1}, \\beta_{2})$ and Gaussian noise $\\varepsilon$ with mean $0$ and variance $\\sigma^{2}$.\n- Implement coordinate descent solvers for both the LASSO and the Elastic Net that minimize the objectives above, operating on standardized predictors and a centered response. Use a convergence tolerance and a fixed maximum number of iterations. Do not use closed-form \"black box\" solvers or training routines from external libraries.\n- For variable selection, treat a coefficient as \"selected\" if its absolute value exceeds a small threshold (you may use $10^{-6}$ for this purpose).\n\nTest suite:\nConstruct the following four cases to exercise and validate the selection behaviors. In each case, use the specified values and fixed seeds so that the results are reproducible. All numeric values below are required to be used exactly as given.\n\n- Case A (overshadowing under LASSO):\n  - Sample size $n = 400$.\n  - Correlation $\\rho = 0.98$ between $X_{1}$ and $X_{2}$.\n  - True coefficients $\\beta = (1.0, 0.15)$.\n  - Noise standard deviation $\\sigma = 0.05$.\n  - Penalty strength for LASSO $\\lambda = 0.12$.\n  - Random seed $202311$.\n  - Output the boolean indicating whether the LASSO selected the weak variable $X_{2}$.\n\n- Case B (same data, Elastic Net rescue):\n  - Use the exact same $X$ and $y$ as in Case A.\n  - Elastic Net with the same penalty strength $\\lambda = 0.12$ and mixing parameter $\\alpha = 0.2$.\n  - Output the boolean indicating whether the Elastic Net selected the weak variable $X_{2}$.\n\n- Case C (low correlation control under LASSO):\n  - Sample size $n = 400$.\n  - Correlation $\\rho = 0.10$ between $X_{1}$ and $X_{2}$.\n  - True coefficients $\\beta = (1.0, 0.15)$.\n  - Noise standard deviation $\\sigma = 0.05$.\n  - Penalty strength for LASSO $\\lambda = 0.12$.\n  - Random seed $202312$.\n  - Output the boolean indicating whether the LASSO selected the weak variable $X_{2}$.\n\n- Case D (perfect collinearity and grouping under Elastic Net):\n  - Sample size $n = 400$.\n  - Perfect collinearity $\\rho = 1.0$ so that $X_{2} = X_{1}$ deterministically.\n  - True coefficients $\\beta = (1.0, 0.15)$.\n  - Noise standard deviation $\\sigma = 0.01$.\n  - Elastic Net with penalty strength $\\lambda = 0.08$ and mixing parameter $\\alpha = 0.10$.\n  - Random seed $202313$.\n  - Output the boolean indicating whether the Elastic Net selected both $X_{1}$ and $X_{2}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the four boolean results, corresponding in order to Cases A, B, C, and D, as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,True]\".\n- There are no physical units, no angle units, and no percentages to report.\n\nScientific realism:\n- All data generation and parameters must be consistent with a Gaussian linear model and the definitions above.\n- The program must use the specified seeds and parameters exactly, with no user input and no external files.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded problem in computational statistics, with all necessary parameters and definitions provided for a unique and reproducible solution. It is free of contradictions, ambiguities, and factual errors.\n\nThe task is to demonstrate the variable selection properties of the Least Absolute Shrinkage and Selection Operator (LASSO) and the Elastic Net, particularly in a scenario involving highly correlated predictors. We will implement coordinate descent algorithms from first principles to solve the required optimization problems and apply them to four specific test cases.\n\nFirst, we establish the theoretical foundation for the solvers. The general problem is to find the coefficient vector $\\beta \\in \\mathbb{R}^p$ that minimizes a penalized least-squares objective function. The predictors $X \\in \\mathbb{R}^{n \\times p}$ are assumed to be standardized to have zero mean and unit standard deviation for each column, and the response $y \\in \\mathbb{R}^n$ is centered to have zero mean. Under this standardization, we have $\\frac{1}{n} \\sum_{i=1}^{n} x_{ij}^2 = 1$ for all columns $j \\in \\{1, \\dots, p\\}$.\n\nThe Elastic Net objective function is given by:\n$$\nL(\\beta) = \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right)\n$$\nwhere $\\lambda \\ge 0$ is the overall penalty strength and $\\alpha \\in [0,1]$ is the mixing parameter. The LASSO is a special case of the Elastic Net where $\\alpha=1$.\n\nWe use a coordinate descent algorithm. The algorithm iteratively optimizes the objective function with respect to each coefficient $\\beta_j$ individually, while holding all other coefficients $\\beta_k$ for $k \\neq j$ fixed. To derive the update rule for $\\beta_j$, we consider the terms in $L(\\beta)$ that depend on $\\beta_j$:\n$$\nL_j(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(y_i - \\sum_{k \\neq j} x_{ik}\\beta_k - x_{ij}\\beta_j\\right)^2 + \\lambda\\alpha|\\beta_j| + \\frac{\\lambda(1-\\alpha)}{2}\\beta_j^2 + C\n$$\nwhere $C$ contains terms not dependent on $\\beta_j$. Let $r^{(j)} = y - \\sum_{k \\neq j} X_k\\beta_k$ be the partial residual vector when the $j$-th predictor is excluded. The expression becomes:\n$$\nL_j(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n} (r_i^{(j)} - x_{ij}\\beta_j)^2 + \\lambda\\alpha|\\beta_j| + \\frac{\\lambda(1-\\alpha)}{2}\\beta_j^2 + C\n$$\nExpanding the squared term and using the fact that $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$, we get (up to constants):\n$$\nL_j(\\beta_j) \\propto \\frac{1}{2}\\beta_j^2 - \\beta_j\\left(\\frac{1}{n}\\sum_{i=1}^n x_{ij}r_i^{(j)}\\right) + \\lambda\\alpha|\\beta_j| + \\frac{\\lambda(1-\\alpha)}{2}\\beta_j^2\n$$\nLet $z_j = \\frac{1}{n}X_j^T r^{(j)} = \\frac{1}{n}X_j^T(y - \\sum_{k \\neq j} X_k\\beta_k)$. We need to minimize:\n$$\nf(\\beta_j) = \\frac{1}{2}(1 + \\lambda(1-\\alpha))\\beta_j^2 - z_j \\beta_j + \\lambda\\alpha|\\beta_j|\n$$\nThe subgradient of $f(\\beta_j)$ with respect to $\\beta_j$ is:\n$$\n\\partial f(\\beta_j) = (1 + \\lambda(1-\\alpha))\\beta_j - z_j + \\lambda\\alpha \\cdot \\partial|\\beta_j|\n$$\nwhere $\\partial|\\beta_j|$ is the subgradient of the absolute value function, which is $\\text{sgn}(\\beta_j)$ for $\\beta_j \\neq 0$ and the interval $[-1, 1]$ for $\\beta_j = 0$. Setting the subgradient to $0$ gives the stationarity condition:\n$$\nz_j = (1 + \\lambda(1-\\alpha))\\beta_j + \\lambda\\alpha \\cdot \\text{sgn}(\\beta_j) \\quad (\\text{if } \\beta_j \\neq 0)\n$$\nSolving for $\\beta_j$ gives:\n$$\n\\beta_j = \\frac{z_j - \\lambda\\alpha \\cdot \\text{sgn}(\\beta_j)}{1+\\lambda(1-\\alpha)}\n$$\nThis solution can be compactly written using the soft-thresholding operator, defined as $S(\\gamma, \\tau) = \\text{sgn}(\\gamma)\\max(|\\gamma|-\\tau, 0)$. The update for $\\beta_j$ is:\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S(z_j, \\lambda\\alpha)}{1+\\lambda(1-\\alpha)}\n$$\nThe quantity $z_j$ can be efficiently computed within the algorithm loop. Given the current estimate $\\beta$, we can write $z_j = \\frac{1}{n}X_j^T(y - X\\beta + X_j\\beta_j) = \\frac{1}{n}X_j^T(y-X\\beta) + \\beta_j$.\n\nFor the LASSO case, we set $\\alpha=1$. The update rule simplifies to:\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S(z_j, \\lambda)}{1+\\lambda(1-1)} = S(z_j, \\lambda)\n$$\nThis is the standard soft-thresholding update for LASSO. Therefore, a single solver for the Elastic Net can handle both problems.\n\nThe algorithm proceeds as follows:\n1. Initialize $\\beta$ as a zero vector.\n2. Iterate until convergence:\n   a. For each coefficient $j=1, \\dots, p$:\n      i. Calculate $z_j = \\frac{1}{n}X_j^T(y - X\\beta) + \\beta_j$.\n      ii. Update $\\beta_j$ using the Elastic Net update rule derived above.\n   b. Check if the change in the $\\beta$ vector is below a tolerance.\n\nWe now apply this framework to the four specified test cases.\n\n- **Case A (LASSO, high correlation)**: With two predictors $X_1$ and $X_2$ having a high positive correlation ($\\rho=0.98$), and a true model where the coefficient for $X_1$ is large ($\\beta_1=1.0$) and the coefficient for $X_2$ is small but non-zero ($\\beta_2=0.15$), the LASSO penalty is known to behave unstably. The term $\\frac{1}{n}X_1^T(y - X_2\\beta_2)$ will be large, pushing $\\beta_1$ into the model. Once $\\beta_1$ is included, it explains most of the variance shared by $X_1$ and $X_2$. The residual will have a very small correlation with $X_2$, causing its coefficient $\\beta_2$ to be shrunk to zero by the $\\ell_1$ penalty. We expect LASSO to fail to select the weak variable $X_2$.\n\n- **Case B (Elastic Net, high correlation)**: Using the same data as Case A, we apply the Elastic Net with $\\alpha=0.2$. The non-zero $\\ell_2$ penalty term ($\\frac{1-\\alpha}{2} \\lVert\\beta\\rVert_2^2$) encourages grouping. For highly correlated predictors, it penalizes solutions where one coefficient is large and the other is zero more than solutions where both coefficients are non-zero and share the \"load\". This grouping effect is expected to pull $\\beta_2$ into the model alongside $\\beta_1$, thus correctly identifying both variables.\n\n- **Case C (LASSO, low correlation)**: As a control, we reduce the correlation to $\\rho=0.10$. With low correlation, the predictors are nearly orthogonal. LASSO is well-behaved in this regime and is expected to correctly identify both the strong ($\\beta_1$) and weak ($\\beta_2$) signals, as they do not compete to explain the same variance in the response.\n\n- **Case D (Elastic Net, perfect collinearity)**: Here, $\\rho=1.0$, so $X_1 = X_2$. For LASSO, the solution to $\\beta_1+\\beta_2$ is unique, but the individual values are not; LASSO would arbitrarily set one coefficient to be non-zero and the other to zero. The Elastic Net's $\\ell_2$ penalty, however, is strictly convex and will enforce a unique solution. The term $\\beta_1^2 + \\beta_2^2$ is minimized for a fixed sum $\\beta_1+\\beta_2=c$ when $\\beta_1=\\beta_2=c/2$. Thus, we expect the Elastic Net to select both variables and assign them similar, non-zero coefficients.\n\nThe program below will generate the data and execute the solvers for each case, reporting the specified boolean outcomes based on whether a coefficient's absolute value exceeds the threshold of $10^{-6}$.", "answer": "```python\nimport numpy as np\n\ndef generate_data(n, beta_true, rho, sigma, seed):\n    \"\"\"\n    Generates synthetic data for a two-predictor linear model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate two independent standard normal vectors\n    Z1 = rng.standard_normal(n)\n    Z2 = rng.standard_normal(n)\n    \n    # Create correlated predictors X1 and X2\n    X1 = Z1\n    X2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n    \n    X = np.column_stack([X1, X2])\n    \n    # Generate response variable\n    noise = rng.normal(0, sigma, n)\n    y = X @ beta_true + noise\n    \n    # Standardize predictors and center response\n    X_mean = X.mean(axis=0)\n    # Using ddof=0 for population standard deviation as assumed in derivations\n    X_std = X.std(axis=0, ddof=0) \n    \n    # Handle the case of zero standard deviation (e.g., constant column)\n    # in the unlikely event rho=1 and numerical error makes std zero\n    X_std[X_std == 0] = 1.0  \n    \n    X_scaled = (X - X_mean) / X_std\n    y_centered = y - y.mean()\n    \n    return X_scaled, y_centered\n\ndef elastic_net_solver(X, y, lambda_, alpha, max_iter=1000, tol=1e-8):\n    \"\"\"\n    Coordinate descent solver for the Elastic Net problem.\n    LASSO is a special case with alpha=1.\n    \"\"\"\n    n_samples, n_features = X.shape\n    beta = np.zeros(n_features)\n    \n    # Since X is standardized, (1/n) * ||X_j||^2 = 1 for all j.\n    \n    for i in range(max_iter):\n        beta_old = beta.copy()\n        \n        for j in range(n_features):\n            # Calculate z_j = (1/n) * X_j^T * r^(j)\n            # Efficiently: z_j = (1/n) * X_j^T * (y - X*beta + X_j*beta_j)\n            # z_j = (1/n) * X_j^T * (y - X*beta) + beta_j\n            \n            # Note: X is standardized, so (1/n) * (X_j^T @ X_j) = 1\n            z_j = (X[:, j].T @ (y - X @ beta) / n_samples) + beta[j]\n\n            # Apply the soft-thresholding update rule for Elastic Net\n            l1_penalty = lambda_ * alpha\n            \n            if z_j > l1_penalty:\n                beta[j] = (z_j - l1_penalty) / (1 + lambda_ * (1 - alpha))\n            elif z_j  -l1_penalty:\n                beta[j] = (z_j + l1_penalty) / (1 + lambda_ * (1 - alpha))\n            else:\n                beta[j] = 0.0\n\n        # Check for convergence\n        if np.linalg.norm(beta - beta_old)  tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Sets up and solves the four test cases specified in the problem.\n    \"\"\"\n    selection_threshold = 1e-6\n    results = []\n\n    # --- Case A: LASSO, high correlation ---\n    n_A = 400\n    rho_A = 0.98\n    beta_true_A = np.array([1.0, 0.15])\n    sigma_A = 0.05\n    lambda_A = 0.12\n    alpha_A = 1.0  # LASSO\n    seed_A = 202311\n    \n    X_A, y_A = generate_data(n_A, beta_true_A, rho_A, sigma_A, seed_A)\n    beta_lasso_A = elastic_net_solver(X_A, y_A, lambda_A, alpha_A)\n    result_A = abs(beta_lasso_A[1]) > selection_threshold\n    results.append(result_A)\n\n    # --- Case B: Elastic Net, high correlation (same data as A) ---\n    lambda_B = 0.12\n    alpha_B = 0.2\n    \n    beta_en_B = elastic_net_solver(X_A, y_A, lambda_B, alpha_B)\n    result_B = abs(beta_en_B[1]) > selection_threshold\n    results.append(result_B)\n\n    # --- Case C: LASSO, low correlation ---\n    n_C = 400\n    rho_C = 0.10\n    beta_true_C = np.array([1.0, 0.15])\n    sigma_C = 0.05\n    lambda_C = 0.12\n    alpha_C = 1.0  # LASSO\n    seed_C = 202312\n    \n    X_C, y_C = generate_data(n_C, beta_true_C, rho_C, sigma_C, seed_C)\n    beta_lasso_C = elastic_net_solver(X_C, y_C, lambda_C, alpha_C)\n    result_C = abs(beta_lasso_C[1]) > selection_threshold\n    results.append(result_C)\n\n    # --- Case D: Elastic Net, perfect collinearity ---\n    n_D = 400\n    rho_D = 1.0\n    beta_true_D = np.array([1.0, 0.15])\n    sigma_D = 0.01\n    lambda_D = 0.08\n    alpha_D = 0.10\n    seed_D = 202313\n    \n    X_D, y_D = generate_data(n_D, beta_true_D, rho_D, sigma_D, seed_D)\n    beta_en_D = elastic_net_solver(X_D, y_D, lambda_D, alpha_D)\n    result_D = (abs(beta_en_D[0]) > selection_threshold) and (abs(beta_en_D[1]) > selection_threshold)\n    results.append(result_D)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191319"}]}