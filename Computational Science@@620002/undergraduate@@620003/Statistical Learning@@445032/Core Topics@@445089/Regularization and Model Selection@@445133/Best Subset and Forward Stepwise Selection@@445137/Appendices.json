{"hands_on_practices": [{"introduction": "Best subset selection promises to find the optimal model by exhaustively searching all possible combinations of predictors. However, its practical utility is limited by a daunting computational burden. This exercise provides a hands-on calculation to quantify this challenge, asking you to derive the number of models evaluated by both a brute-force approach and an idealized branch-and-bound algorithm [@problem_id:3105043]. By comparing these, you will gain a concrete appreciation for the scale of the problem and understand the motivation behind computationally cheaper heuristic methods.", "problem": "You are given a regression problem with $p$ candidate predictors and you wish to perform exact best subset selection restricted to model sizes at most $k$. Assume the criterion to be minimized is a monotone function of residual sum of squares so that comparing two models requires fitting each model separately and computing its training error. Your tasks are:\n\n1) Using only core combinatorial counting, derive an exact expression for the number of models evaluated by a brute-force enumerator that fits every subset of predictors of size at most $k$.\n\n2) Consider an exact best subset search implemented by branch-and-bound (B&B). Assume an optimistic but self-consistent pruning regime: the search tree is organized by recursively deciding to include or exclude a designated predictor (call it predictor $1$) first, and the bounding rule is perfect in the following sense: as soon as the path excludes predictor $1$, the entire remaining subtree is pruned without evaluating any complete model under that branch; while the branch that includes predictor $1$ is explored fully down to depth $k$ (evaluating all subsets of size at most $k$ that include predictor $1$). Under these assumptions, derive a closed-form expression for the total number of complete models that B&B evaluates as a function of $p$ and $k$.\n\n3) Using your expressions, compute the ratio of the brute-force model count to the optimistic B&B model count for $p=30$ and $k=10$. Round your final numerical ratio to four significant figures. Your final answer must be this single rounded number with no units.", "solution": "The problem asks for an analysis of the computational cost, measured by the number of models evaluated, for two different approaches to best subset selection: brute-force enumeration and an idealized branch-and-bound (B&B) algorithm. We will address each part of the problem sequentially.\n\nThe problem is valid as it is scientifically grounded in computational statistics, well-posed with clear objectives, and framed using objective, unambiguous language. The assumptions, while idealized for the B&B part, are an explicit and consistent part of the problem's theoretical setup.\n\n**Part 1: Brute-Force Enumerator Model Count**\n\nA brute-force approach to best subset selection, restricted to model sizes at most $k$, requires evaluating every possible subset of predictors whose size falls within the allowed range. The total number of predictors is given as $p$. The size of a model, which corresponds to the number of predictors in the chosen subset, can be any integer $j$ such that $0 \\le j \\le k$.\n\nFor any given size $j$, the number of distinct subsets of predictors that can be chosen from the full set of $p$ predictors is given by the binomial coefficient $\\binom{p}{j}$. This counts the number of ways to choose $j$ items from a set of $p$ without regard to order.\n\nThe total number of models evaluated by the brute-force enumerator, which we denote as $N_{BF}$, is the sum of the counts for each possible model size from $j=0$ (the null model, containing only an intercept) up to the maximum size $k$.\n\nTherefore, the exact expression for the number of models evaluated is:\n$$\nN_{BF} = \\sum_{j=0}^{k} \\binom{p}{j}\n$$\n\n**Part 2: Optimistic Branch-and-Bound Model Count**\n\nThe problem describes a specific, optimistic implementation of a branch-and-bound (B&B) search. The search tree is structured by making a recursive decision on a designated predictor, which we can label as predictor $1$. The two branches from the root node correspond to excluding predictor $1$ and including predictor $1$.\n\nThe problem states a \"perfect\" pruning rule with the following behavior:\n1.  If the path taken **excludes** predictor $1$, the entire corresponding subtree is pruned immediately, and no complete models under this branch are evaluated. This contributes $0$ to the total count of evaluated models.\n2.  If the path taken **includes** predictor $1$, the search is \"explored fully down to depth $k$\". This means the algorithm evaluates every subset of predictors that is of size at most $k$ and contains predictor $1$.\n\nOur task is to count the number of models evaluated under these specific conditions. Based on the rules, the total count is simply the number of models satisfying the condition in the second point. Let's denote this count as $N_{B\\}$.\n\nWe need to count the number of subsets of the $p$ predictors that simultaneously satisfy two conditions:\n- The size of the subset, $j$, is at most $k$, so $1 \\le j \\le k$. The size cannot be $0$ since predictor $1$ must be included.\n- The subset must contain predictor $1$.\n\nFor a model of a specific size $j$ (where $1 \\le j \\le k$), if predictor $1$ is required to be in the model, we only need to choose the remaining $j-1$ predictors. These must be chosen from the other available $p-1$ predictors. The number of ways to do this for a fixed size $j$ is $\\binom{p-1}{j-1}$.\n\nTo find the total number of models, $N_{B\\}$, we sum this quantity over all possible model sizes from $j=1$ to $j=k$:\n$$\nN_{B\\} = \\sum_{j=1}^{k} \\binom{p-1}{j-1}\n$$\nBy performing a change of index, letting $m = j-1$, the sum can be rewritten in a more conventional form. As $j$ goes from $1$ to $k$, $m$ goes from $0$ to $k-1$:\n$$\nN_{B\\} = \\sum_{m=0}^{k-1} \\binom{p-1}{m}\n$$\nThis is the closed-form expression for the total number of models evaluated by the specified BB algorithm.\n\n**Part 3: Ratio Calculation**\n\nWe are asked to compute the ratio $\\frac{N_{BF}}{N_{B\\}}$ for the specific values $p=30$ and $k=10$.\n\nUsing the derived expressions:\n$N_{BF} = \\sum_{j=0}^{10} \\binom{30}{j}$\n$N_{B\\} = \\sum_{m=0}^{9} \\binom{29}{m}$\n\nWe can compute these sums directly.\nFor $N_{B\\}$:\n$N_{B\\} = \\binom{29}{0} + \\binom{29}{1} + \\dots + \\binom{29}{9}$\n$N_{B\\} = 1 + 29 + 406 + 3654 + 23751 + 118755 + 475020 + 1560780 + 4292145 + 10015005 = 16489546$\n\nFor $N_{BF}$, we can use the combinatorial identity $\\binom{n}{r} = \\binom{n-1}{r} + \\binom{n-1}{r-1}$.\n$N_{BF} = \\sum_{j=0}^{k} \\binom{p}{j} = \\sum_{j=0}^{k} \\left( \\binom{p-1}{j} + \\binom{p-1}{j-1} \\right)$\n$N_{BF} = \\sum_{j=0}^{k} \\binom{p-1}{j} + \\sum_{j=0}^{k} \\binom{p-1}{j-1}$\nThe second sum is $\\sum_{m=-1}^{k-1} \\binom{p-1}{m}$, which equals $\\sum_{m=0}^{k-1} \\binom{p-1}{m}$ since $\\binom{p-1}{-1}=0$. This is exactly $N_{B\\}$.\nThe first sum is $\\sum_{j=0}^{k-1} \\binom{p-1}{j} + \\binom{p-1}{k} = N_{B\\} + \\binom{p-1}{k}$.\nThus, we have the relation: $N_{BF} = N_{B\\} + \\binom{p-1}{k} + N_{B\\} = 2 N_{B\\} + \\binom{p-1}{k}$.\n\nThe desired ratio $R$ is therefore:\n$$\nR = \\frac{N_{BF}}{N_{B\\}} = \\frac{2 N_{B\\} + \\binom{p-1}{k}}{N_{B\\}} = 2 + \\frac{\\binom{p-1}{k}}{N_{B\\}}\n$$\nSubstituting $p=30$ and $k=10$:\n$$\nR = 2 + \\frac{\\binom{29}{10}}{N_{B\\}} = 2 + \\frac{\\binom{29}{10}}{\\sum_{m=0}^{9} \\binom{29}{m}}\n$$\nWe have already calculated the denominator: $N_{B\\} = 16489546$.\nNow we calculate the numerator term:\n$\\binom{29}{10} = \\frac{29!}{10!19!} = 20030010$.\n\nNow, we compute the ratio:\n$R = 2 + \\frac{20030010}{16489546} \\approx 2 + 1.21470400... = 3.21470400...$\n\nThe problem requires rounding the final answer to four significant figures.\n$R \\approx 3.215$.", "answer": "$$\\boxed{3.215}$$", "id": "3105043"}, {"introduction": "As a computationally efficient alternative to best subset selection, forward stepwise selection builds a model greedily, adding one predictor at a time. While fast, we must ensure it doesn't overfit the training data. This coding practice will guide you through implementing the forward selection algorithm to directly observe the classic bias-variance trade-off [@problem_id:3104976]. You will see firsthand how training error steadily improves while validation error often follows a characteristic U-shaped curve, the minimum of which indicates the model with the best predictive power.", "problem": "Consider linear regression with forward stepwise selection. Let the training observations be $\\{(x_i, y_i)\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$. For any subset of predictors indexed by $S \\subseteq \\{1,\\dots,p\\}$, define the model with an intercept and predictors in $S$, and denote its training residual sum of squares (RSS) by\n$$\n\\text{RSS}(S) = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i(S)\\right)^2\n$$\nwhere $\\hat{y}_i(S)$ is the least squares fit using predictors in $S$. Forward stepwise selection constructs a nested sequence of selected sets $(S_k)_{k=0}^p$ such that $S_0 = \\varnothing$ and, for $k \\geq 0$, $S_{k+1} = S_k \\cup \\{j^*\\}$ where $j^*$ is chosen from $\\{1,\\dots,p\\} \\setminus S_k$ to minimize the training $\\text{RSS}$ at size $k+1$.\n\nStart from the following base concepts:\n- Linear least squares fits minimize the sum of squared errors over the span of the chosen predictors and an intercept.\n- If the set of regressors expands, the span of the design matrix columns (including the intercept) expands or stays the same, hence the minimized $\\text{RSS}$ cannot increase.\n\nYour tasks:\n1. Implement forward stepwise selection on a training set to produce models $S_k$ of sizes $k = 0,1,\\dots,p$. For each $k$, compute the training $\\text{RSS}$ and the validation mean squared error $\\text{MSE}_{\\text{val}}(k)$ on a separate validation set of size $m$, where $\\hat{y}^{\\text{val}}_i(S_k)$ uses the coefficients fitted on the training set:\n$$\n\\text{MSE}_{\\text{val}}(k) = \\frac{1}{m}\\sum_{i=1}^m \\left(y^{\\text{val}}_i - \\hat{y}^{\\text{val}}_i(S_k)\\right)^2\n$$\n2. Show via computation that the training $\\text{RSS}$ is strictly decreasing as $k$ increases in forward selection for generic data (use a small numerical tolerance to guard against floating point effects).\n3. Demonstrate that the validation error $\\text{MSE}_{\\text{val}}(k)$ can exhibit a U-shaped behavior as a function of $k$: initially decreasing due to reduced bias and then increasing due to variance inflation from overfitting. Design at least one dataset where the minimizing $k$ is strictly between $0$ and $p$.\n4. Use the following test suite of dataset generator parameters. Each dataset is generated by sampling predictors and responses as follows: draw $X_{\\text{train}} \\in \\mathbb{R}^{n \\times p}$ and $X_{\\text{val}} \\in \\mathbb{R}^{m \\times p}$ with independent standard normal entries; choose a coefficient vector $\\beta \\in \\mathbb{R}^p$; generate $y_{\\text{train}} = X_{\\text{train}} \\beta + \\varepsilon_{\\text{train}}$ and $y_{\\text{val}} = X_{\\text{val}} \\beta + \\varepsilon_{\\text{val}}$, where the noise terms have independent normal entries with mean $0$ and standard deviation $\\sigma$. Use an intercept in all fitted models. The parameters for the test suite are:\n   - Case A (intermediate optimum): $n = 50$, $m = 400$, $p = 20$, $\\beta = (3.0, -2.5, 1.5, 0, \\dots, 0)$, $\\sigma = 2.0$, seed $= 2024$.\n   - Case B (null model best): $n = 60$, $m = 400$, $p = 12$, $\\beta = (0, \\dots, 0)$, $\\sigma = 1.0$, seed $= 2025$.\n   - Case C (full model best): $n = 80$, $m = 400$, $p = 6$, $\\beta = (1.0, -1.5, 0.8, 2.0, -1.0, 0.5)$, $\\sigma = 0.2$, seed $= 7$.\n5. For each case, return three quantities:\n   - A boolean indicating whether the sequence $\\text{RSS}(S_k)$ is strictly decreasing in $k$ for $k = 0,1,\\dots,p$.\n   - The integer $k^\\star$ in $\\{0,1,\\dots,p\\}$ that minimizes $\\text{MSE}_{\\text{val}}(k)$.\n   - A boolean indicating whether the validation error exhibits a U-shaped profile in the sense that $0  k^\\star  p$ and both $\\max\\{\\text{MSE}_{\\text{val}}(0), \\dots, \\text{MSE}_{\\text{val}}(k^\\star - 1)\\}  \\text{MSE}_{\\text{val}}(k^\\star)$ and $\\max\\{\\text{MSE}_{\\text{val}}(k^\\star + 1), \\dots, \\text{MSE}_{\\text{val}}(p)\\}  \\text{MSE}_{\\text{val}}(k^\\star)$.\n6. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s result is a three-element list in the order described above. For example: \"[[True,3,True],[True,0,False],[True,6,False]]\". No physical units or angles are involved, and all percentages, if any, must be expressed as decimals. Ensure all mathematical computations adhere to standard linear algebra operations.\n\nThe program must implement the data generation and forward stepwise selection as specified, run the three cases, and produce exactly one line of output matching the format above.", "solution": "The user-provided problem is assessed as **valid**.\n\nThe problem is a well-defined computational exercise in statistical learning, focused on implementing and analyzing the behavior of forward stepwise selection for linear regression. It is scientifically grounded in established principles of regression, model selection, and the bias-variance trade-off. The problem statement is self-contained, providing all necessary parameters, data generation procedures, random seeds for reproducibility, and precise definitions for the required outputs. It is free of ambiguity, subjective claims, or factual errors. The tasks are computationally feasible and test fundamental concepts, such as the monotonic decrease of training error and the characteristic U-shaped curve of validation error, which are central to understanding model complexity and overfitting.\n\n### Principle-Based Design\n\nThe solution is architected around the core principles of linear regression and forward stepwise model selection.\n\n#### 1. Forward Stepwise Selection Algorithm\n\nForward stepwise selection is a greedy iterative algorithm for choosing a subset of predictors for a model. The process begins with a null model containing only an intercept and iteratively adds predictors one at a time.\n\n- **Initialization ($k=0$)**: The process starts with the simplest possible model, $S_0 = \\varnothing$, which contains no predictors, only an intercept. The prediction for every observation is simply the mean of the training response variable, $\\bar{y}_{\\text{train}}$. The initial training residual sum of squares, $\\text{RSS}(S_0)$, and validation mean squared error, $\\text{MSE}_{\\text{val}}(0)$, are computed for this base model.\n\n- **Iterative Step ($k \\to k+1$)**: For each step $k$ from $0$ to $p-1$, the algorithm considers adding one predictor from the set of predictors not yet in the model, $\\{1, \\dots, p\\} \\setminus S_k$. For each candidate predictor $j$, a temporary model with predictors $S_k \\cup \\{j\\}$ is fitted to the training data, and its training RSS is calculated. The algorithm greedily selects the predictor $j^*$ that results in the greatest reduction in RSS:\n$$\nj^* = \\arg\\min_{j \\in \\{1,\\dots,p\\} \\setminus S_k} \\text{RSS}(S_k \\cup \\{j\\})\n$$\nThe new model is then defined as $S_{k+1} = S_k \\cup \\{j^*\\}$. This procedure generates a sequence of nested models $S_0 \\subset S_1 \\subset \\dots \\subset S_p$ of increasing size from $0$ to $p$.\n\nThe fitting of each linear model is performed using the method of ordinary least squares (OLS). For a model with a set of predictors $S$, the design matrix $\\tilde{X}_S$ is constructed by augmenting the matrix of selected predictors $X_S$ with a leading column of ones for the intercept. The OLS coefficient vector $\\hat{\\beta}_S$ is found by solving the normal equations, which can be expressed as:\n$$\n\\hat{\\beta}_S = (\\tilde{X}_S^T \\tilde{X}_S)^{-1} \\tilde{X}_S^T y_{\\text{train}}\n$$\nA numerically stable approach like `numpy.linalg.lstsq` is used for this computation.\n\n#### 2. Monotonicity of Training RSS\n\nThe training RSS is a non-increasing function of the model size $k$. This is a fundamental property of least squares. When a predictor is added, the set of basis vectors for the linear model (the columns of the design matrix) is expanded. The OLS fit finds the projection of the response vector $y_{\\text{train}}$ onto the column space of the design matrix. Adding a vector to the basis can only expand this space or leave it unchanged. Therefore, the projection onto the new, larger space cannot be farther from $y_{\\text{train}}$ than the projection onto the original, smaller space. This means the length of the residual vector, and thus the RSS, cannot increase.\n\nFor \"generic\" data, such as that generated from continuous distributions (e.g., standard normal), the probability of a new predictor column being perfectly linearly dependent on the existing ones is zero. Thus, the column space strictly expands at each step, guaranteeing that the training RSS is **strictly decreasing**. We verify this computationally by checking if $\\text{RSS}(S_k) > \\text{RSS}(S_{k+1})$ for all $k$, using a small numerical tolerance to account for floating-point arithmetic.\n\n#### 3. Validation Error and the Bias-Variance Trade-off\n\nWhile training error monotonically decreases with model complexity, validation error exhibits a more complex behavior due to the bias-variance trade-off.\n\n- **Bias**: A simple model (small $k$) may fail to capture the true underlying relationship between the predictors and the response. This systematic error is known as bias. Initially, as we add relevant predictors, the model's flexibility increases, and it conforms better to the true signal, causing a sharp reduction in bias and, consequently, in validation error.\n\n- **Variance**: A complex model (large $k$) has high flexibility and can capture intricate patterns. However, it may start to fit the random noise in the training data, a phenomenon called overfitting. This leads to high variance, as the fitted model would change dramatically with a different training set. This overfitting degrades the model's ability to generalize to unseen data, causing the validation error to increase.\n\nThe interplay between bias and variance typically results in a U-shaped curve for the validation error as a function of model complexity $k$. The error decreases as bias is reduced and then increases as variance dominates. The optimal model corresponds to the value $k^\\star$ at the bottom of this \"U\", which balances bias and variance to achieve the best predictive performance on unseen data. The problem asks to identify this optimal $k^\\star$ and verify if the validation error profile is \"U-shaped\" in the sense that its minimum is strictly interior (i.e., $0  k^\\star  p$) and the error is higher at some point both before and after the minimum.\n\n#### 4. Implementation Strategy\n\nThe solution is implemented as a Python program adhering to the specified environment.\n\n1.  **Data Generation**: For each test case, a random number generator is seeded for reproducibility. Training and validation datasets ($X, y$) are sampled according to the specified dimensions ($n, m, p$), true coefficient vector ($\\beta$), and noise level ($\\sigma$).\n2.  **Forward Selection Loop**: The algorithm proceeds as described above. For each model size $k \\in \\{0, \\dots, p\\}$, the training `RSS` is stored. After identifying the best model $S_k$, its coefficients are finalized on the training data, and these coefficients are used to predict responses on the validation set to compute `MSE_val(k)`.\n3.  **Result Computation**: After the loop completes, the stored sequences `rss_k` and `mse_val_k` are analyzed:\n    - The strict decrease of `rss_k` is checked by computing the differences `rss_k[k] - rss_k[k+1]` and ensuring they are all positive (greater than a small tolerance).\n    - The optimal model size `k_star` is found using `numpy.argmin` on the `mse_val_k` array.\n    - The U-shape condition is evaluated by checking if `k_star` is between $0$ and $p$ and if there are validation error values strictly greater than the minimum on both sides of `k_star`.\n4.  **Output Formatting**: The results for each test case (a list of three values) are formatted into the precise string representation `[Boolean,Integer,Boolean]` and then aggregated into the final required output string `[[...],[...],[...]]`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(n, m, p, beta_coeffs, sigma, seed):\n    \"\"\"\n    Generates data, runs forward stepwise selection, and computes required metrics for one case.\n\n    Args:\n        n (int): Number of training observations.\n        m (int): Number of validation observations.\n        p (int): Number of predictors.\n        beta_coeffs (tuple): Non-zero coefficients for the true model.\n        sigma (float): Standard deviation of the noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing [is_rss_decreasing, k_star, is_u_shaped].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate data\n    X_train = rng.standard_normal(size=(n, p))\n    X_val = rng.standard_normal(size=(m, p))\n    \n    beta = np.zeros(p)\n    beta[:len(beta_coeffs)] = beta_coeffs\n    \n    eps_train = rng.normal(0, sigma, size=n)\n    eps_val = rng.normal(0, sigma, size=m)\n    \n    y_train = X_train @ beta + eps_train\n    y_val = X_val @ beta + eps_val\n\n    # Arrays to store results for k = 0, 1, ..., p\n    rss_k = np.zeros(p + 1)\n    mse_val_k = np.zeros(p + 1)\n    \n    selected_indices = []\n    available_indices = list(range(p))\n\n    # --- k = 0: Intercept-only model ---\n    y_train_mean = np.mean(y_train)\n    rss_k[0] = np.sum((y_train - y_train_mean)**2)\n    mse_val_k[0] = np.mean((y_val - y_train_mean)**2)\n\n    # --- k = 1 to p: Forward stepwise selection ---\n    for k in range(1, p + 1):\n        best_rss_at_step = np.inf\n        best_new_predictor = -1\n        \n        # Find the best predictor to add from the available set\n        for j in available_indices:\n            current_predictors = selected_indices + [j]\n            \n            # Create design matrix with intercept\n            X_train_subset = X_train[:, current_predictors]\n            design_matrix = np.c_[np.ones(n), X_train_subset]\n            \n            # Solve least squares and get RSS\n            _, residuals, _, _ = np.linalg.lstsq(design_matrix, y_train, rcond=None)\n            \n            rss_candidate = residuals[0] if residuals.size  0 else np.sum((y_train - design_matrix @ np.linalg.lstsq(design_matrix, y_train, rcond=None)[0])**2)\n\n            if rss_candidate  best_rss_at_step:\n                best_rss_at_step = rss_candidate\n                best_new_predictor = j\n        \n        # Add the best predictor to the model for this step\n        selected_indices.append(best_new_predictor)\n        available_indices.remove(best_new_predictor)\n        \n        # Store RSS for the model of size k\n        rss_k[k] = best_rss_at_step\n        \n        # --- Calculate validation MSE for the new model S_k ---\n        # Fit model on training data to get coefficients\n        X_train_final_subset = X_train[:, selected_indices]\n        design_matrix_train = np.c_[np.ones(n), X_train_final_subset]\n        coeffs, _, _, _ = np.linalg.lstsq(design_matrix_train, y_train, rcond=None)\n        \n        # Predict on validation data using the fitted coefficients\n        X_val_subset = X_val[:, selected_indices]\n        design_matrix_val = np.c_[np.ones(m), X_val_subset]\n        y_val_pred = design_matrix_val @ coeffs\n        \n        # Calculate validation MSE\n        mse_val_k[k] = np.mean((y_val - y_val_pred)**2)\n\n    # 2. Check if RSS sequence is strictly decreasing\n    is_decreasing = bool(np.all(np.diff(rss_k)  -1e-9))\n    \n    # 3. Find k that minimizes validation MSE\n    k_star = int(np.argmin(mse_val_k))\n    \n    # 4. Check for U-shaped profile as defined in the problem\n    is_u_shaped = False\n    if 0  k_star  p:\n        min_mse = mse_val_k[k_star]\n        # Check that there is at least one point  min_mse before and after k_star\n        has_larger_before = np.any(mse_val_k[:k_star]  min_mse)\n        has_larger_after = np.any(mse_val_k[k_star+1:]  min_mse)\n        if has_larger_before and has_larger_after:\n            is_u_shaped = True\n\n    return [is_decreasing, k_star, is_u_shaped]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Intermediate optimum\n        {'n': 50, 'm': 400, 'p': 20, 'beta_coeffs': (3.0, -2.5, 1.5), 'sigma': 2.0, 'seed': 2024},\n        # Case B: Null model best\n        {'n': 60, 'm': 400, 'p': 12, 'beta_coeffs': (), 'sigma': 1.0, 'seed': 2025},\n        # Case C: Full model best\n        {'n': 80, 'm': 400, 'p': 6, 'beta_coeffs': (1.0, -1.5, 0.8, 2.0, -1.0, 0.5), 'sigma': 0.2, 'seed': 7},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(**case)\n        results.append(result)\n\n    # Format the output string exactly as specified\n    formatted_results = []\n    for res in results:\n        # Manually format to get \"[True,3,True]\" without spaces\n        s = '[' + ','.join(map(str, res)).replace(\"True\", \"True\").replace(\"False\", \"False\") + ']'\n        formatted_results.append(s)\n    \n    final_output_string = '[' + ','.join(formatted_results) + ']'\n    \n    # Final print statement in the exact required format.\n    print(final_output_string.replace(' ',''))\n\nsolve()\n```", "id": "3104976"}, {"introduction": "The greedy nature of forward selection, while efficient, means it can get trapped in a local optimum and miss the globally best model, especially when predictors are synergistic. This exercise puts theory into practice by comparing the performance of pure forward selection, the optimal best subset method, and a hybrid approach that allows a single corrective 'swap' [@problem_id:3104971]. By working with carefully constructed datasets, you will explore scenarios where the greedy approach fails and see how even a small enhancement can sometimes, but not always, close the gap to the true best subset.", "problem": "Implement and compare three model selection procedures for linear regression with an intercept using ordinary least squares: pure forward stepwise selection, forward selection with a single allowed swap after reaching a fixed model size, and exhaustive best subset selection. Work from the core definitions of least squares regression and residual sum of squares, and use these principles to derive the algorithms.\n\nYou are given that, for a response vector $y \\in \\mathbb{R}^n$ and a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with columns $x_j$ corresponding to $p$ candidate predictors, an intercept is always included. For any index set $S \\subseteq \\{1,\\dots,p\\}$, the fitted values under ordinary least squares are obtained by minimizing the residual sum of squares\n$$\n\\mathrm{RSS}(S) \\;=\\; \\min_{\\beta_0,\\{\\beta_j\\}_{j \\in S}} \\; \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j \\in S} \\beta_j x_{ij}\\right)^2,\n$$\nwhere $\\beta_0 \\in \\mathbb{R}$ is the intercept and $\\beta_j \\in \\mathbb{R}$ are coefficients of the selected predictors. The minimal residual sum of squares arises from solving the least squares normal equations or using a pseudoinverse; you may assume the standard linear algebra solution is available.\n\nDefine the following procedures for a target model size $k$ with $1 \\le k \\le p$:\n\n- Pure forward stepwise selection: starting from the empty set $S=\\varnothing$, at each iteration add the single variable $j \\notin S$ that yields the largest reduction in $\\mathrm{RSS}(S \\cup \\{j\\})$. Repeat until $|S| = k$.\n\n- Forward selection with one swap (an occasional backward step): first run pure forward selection to size $k$ and obtain a set $S$. Then consider all single replacements $S' = (S \\setminus \\{u\\}) \\cup \\{v\\}$ with $u \\in S$ and $v \\notin S$, compute $\\mathrm{RSS}(S')$, and perform the single swap that yields the smallest $\\mathrm{RSS}$ if and only if it is strictly smaller than the current $\\mathrm{RSS}(S)$. Perform at most one such swap.\n\n- Best subset selection: evaluate all $\\binom{p}{k}$ subsets $S$ of size $k$ and pick the set minimizing $\\mathrm{RSS}(S)$.\n\nYour program must construct three synthetic test datasets designed to probe cases with synergistic predictor pairs and local minima:\n\n- Use the following base vectors for all test cases (these are length-$8$ vectors):\n  - $u = [\\,4,\\,-3,\\,5,\\,-4,\\,3,\\,-2,\\,1,\\,0\\,]$,\n  - $v = [\\,1,\\,2,\\,-1,\\,-2,\\,0,\\,1,\\,-1,\\,0\\,]$.\n  - For any auxiliary vector $a \\in \\mathbb{R}^8$ and a set of basis vectors $B = \\{b_1,\\dots,b_m\\}$ with $m \\in \\{0,1,2,\\dots\\}$, define the orthogonalized vector\n    $$\n    \\mathrm{orth}(a;B) \\;=\\; a \\;-\\; \\sum_{\\ell=1}^{m} \\hat{\\gamma}_{\\ell} \\, b_{\\ell},\n    $$\n    where the coefficients $\\hat{\\gamma}_{\\ell}$ solve the least squares problem\n$$\n\\min_{\\gamma \\in \\mathbb{R}^m} \\lVert a - \\sum_{\\ell=1}^m \\gamma_{\\ell} b_{\\ell} \\rVert_2^2.\n$$\n    This subtracts the projection of $a$ onto the span of $B$.\n\n- Test case $1$ (synergistic pair where a decoy can be corrected by a single swap):\n  - Let $n = 8$, $p = 4$, $k = 2$.\n  - Define $y = u + v$.\n  - Predictors:\n    - $x_1 = u$,\n    - $x_2 = v$,\n    - Let $s_0 = [\\,1,\\,-1,\\,2,\\,-2,\\,1,\\,-1,\\,2,\\,-2\\,]$ and $s = \\mathrm{orth}(s_0;\\{u,v\\})$,\n    - $x_3 = u + 0.5\\,y + 0.05\\,s$, which is a strong decoy correlated with $y$ and perturbed out of the span of $\\{u,v\\}$ to prevent exact representation with $x_2$,\n    - $x_4 = \\mathrm{orth}([\\,0.2,\\,-0.1,\\,0.05,\\,0.1,\\,-0.4,\\,0.3,\\,0.2,\\,-0.1\\,];\\{u,v,s\\})$, a nuisance noise direction.\n  - Intuition: the best size-$k$ subset is expected to be $\\{x_1,x_2\\}$, while pure forward may initially choose $x_3$ due to its inflated marginal alignment with $y$. A single swap should enable correcting the early mistake.\n\n- Test case $2$ (happy path where pure forward is already optimal):\n  - Let $n = 8$, $p = 3$, $k = 2$.\n  - Define $y = u + v$.\n  - Predictors:\n    - $x_1 = u$,\n    - $x_2 = v$,\n    - $x_3 = \\mathrm{orth}([\\,2,\\,-2,\\,1,\\,-1,\\,2,\\,-2,\\,1,\\,-1\\,];\\{u,v\\})$, a pure noise direction orthogonal to the signal span.\n  - Intuition: both forward and one-swap should match best subset since the signal is clean and the decoy is orthogonal.\n\n- Test case $3$ (local improvement by one swap but not globally optimal):\n  - Let $n = 8$, $p = 4$, $k = 2$.\n  - Define $y = u + v$.\n  - Predictors:\n    - $x_1 = u$,\n    - $x_2 = v$,\n    - Let $t_1 = \\mathrm{orth}([\\,1,\\,2,\\,3,\\,4,\\,5,\\,6,\\,7,\\,8\\,];\\{u,v\\})$,\n    - Let $t_2 = \\mathrm{orth}([\\,2,\\,-1,\\,0,\\,1,\\,-2,\\,3,\\,-3,\\,1\\,];\\{u,v,t_1\\})$,\n    - $x_3 = x_1 + 0.6\\,y + 0.2\\,t_1$,\n    - $x_4 = x_2 + 0.3\\,y + 0.4\\,t_2$.\n  - Intuition: forward may select $\\{x_3,x_4\\}$ due to high marginal alignment; a single swap to replace one decoy with its clean counterpart (e.g., $x_1$) can reduce residual sum of squares but may still fail to match the truly optimal pair $\\{x_1,x_2\\}$.\n\nFor each test case, compute:\n- The pure forward-selected set of size $k$ and its residual sum of squares.\n- The one-swap refined set and its residual sum of squares.\n- The best subset of size $k$ and its residual sum of squares.\n\nFor numerical comparison, use a tolerance $\\varepsilon = 10^{-8}$ when deciding equality of residual sums of squares.\n\nYour program must output, for each test case, a single integer code summarizing the outcome:\n- Output $0$ if the one-swap method does not change the forward-selected set and the forward-selected set matches the best subset within tolerance.\n- Output $1$ if the one-swap method does not change the forward-selected set and the forward-selected set is strictly suboptimal.\n- Output $2$ if the one-swap method changes the forward-selected set and strictly lowers the forward residual sum of squares but still does not match the best subset within tolerance.\n- Output $3$ if the one-swap method changes the forward-selected set and matches the best subset within tolerance.\n\nFinal output format: Your program should produce a single line containing a comma-separated list of the integer codes for test cases $1$, $2$, and $3$, enclosed in square brackets (e.g., \"[3,0,2]\"). No additional text should be printed. No physical units, angles, or percentages are involved in this problem; all outputs are unitless integers. The implementation must be fully deterministic and use only the specified environment.", "solution": "The user requires the implementation and comparison of three model selection algorithms for linear regression: pure forward stepwise selection, forward selection with a single swap, and best subset selection. The task involves creating synthetic datasets designed to expose the differing behaviors of these algorithms and classifying the outcomes according to a predefined scheme. The entire process must be grounded in the principles of ordinary least squares (OLS) and residual sum of squares (RSS).\n\n### 1. Fundamental Principle: Ordinary Least Squares and Residual Sum of Squares\n\nThe cornerstone of this problem is the ordinary least squares (OLS) linear regression model. Given a response vector $y \\in \\mathbb{R}^n$ and a set of $p$ candidate predictors represented by the columns of a matrix $X \\in \\mathbb{R}^{n \\times p}$, we seek to find a linear model that best explains $y$. For a chosen subset of predictors indexed by a set $S \\subseteq \\{1, \\dots, p\\}$, the model is:\n$$\ny_i = \\beta_0 + \\sum_{j \\in S} \\beta_j x_{ij} + \\epsilon_i\n$$\nwhere $\\beta_0$ is the intercept, $\\{\\beta_j\\}_{j \\in S}$ are the predictor coefficients, and $\\epsilon_i$ are the error terms. The OLS method finds the coefficients that minimize the Residual Sum of Squares (RSS), defined as the sum of the squared differences between the observed and predicted values of $y$:\n$$\n\\mathrm{RSS}(S) = \\min_{\\beta_0, \\{\\beta_j\\}_{j \\in S}} \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{j \\in S} \\beta_j x_{ij}\\right)\\right)^2\n$$\nThis is a standard matrix problem. Let $X_S$ be the $n \\times |S|$ matrix containing the columns of $X$ corresponding to indices in $S$. We form an augmented design matrix $A_S = [\\mathbf{1} | X_S]$, where $\\mathbf{1}$ is an $n \\times 1$ column of ones representing the intercept. The OLS problem is to find the coefficient vector $\\boldsymbol{\\beta}$ that minimizes $\\|\\boldsymbol{y} - A_S \\boldsymbol{\\beta}\\|_2^2$. The solution is found using the normal equations or, more robustly, via QR decomposition or Singular Value Decomposition (SVD). In our implementation, we use `numpy.linalg.lstsq`, which provides an efficient and numerically stable solution and directly returns the minimized RSS. A helper function, `calculate_rss`, is implemented to perform this core calculation for any given subset $S$.\n\nAn auxiliary calculation is the orthogonalization of a vector $a$ with respect to a set of basis vectors $B = \\{b_1, \\dots, b_m\\}$. This is defined as $a$ minus its projection onto the span of $B$:\n$$\n\\mathrm{orth}(a;B) = a - \\mathrm{proj}_{\\mathrm{span}(B)}(a)\n$$\nThe projection is found by solving the least squares problem $\\min_{\\gamma} \\|\\boldsymbol{a} - \\boldsymbol{B}\\gamma\\|_2^2$ for the coefficients $\\gamma$, where $B$ is the matrix with columns $\\{b_1, \\dots, b_m\\}$. The orthogonalized vector is then $a - B\\hat{\\gamma}$. This is also implemented using `numpy.linalg.lstsq`.\n\n### 2. Algorithmic Implementations\n\nBased on the core RSS calculation, we implement the three specified model selection algorithms for a fixed model size $k$.\n\n**Pure Forward Stepwise Selection:** This is a greedy, iterative algorithm.\n1.  Start with the empty model (null model), $S_0 = \\varnothing$.\n2.  For $i=1, \\dots, k$: find the single predictor $j \\notin S_{i-1}$ which, when added to the current model, results in the lowest RSS. That is, find \n$$\nj^* = \\arg\\min_{j \\notin S_{i-1}} \\mathrm{RSS}(S_{i-1} \\cup \\{j\\}).\n$$\n3.  Update the model: $S_i = S_{i-1} \\cup \\{j^*\\}$.\n4.  The final model is $S_k$.\nThis approach is computationally efficient but can fail to find the globally optimal model if predictors are synergistic, as an early greedy choice might be suboptimal in the long run.\n\n**Forward Selection with One Swap:** This procedure is a heuristic refinement of pure forward selection, designed to mitigate its shortsightedness.\n1.  First, perform pure forward selection to obtain a model $S_{fwd}$ of size $k$.\n2.  Next, explore the neighborhood of $S_{fwd}$ consisting of all models that can be reached by a single swap. A swap involves removing one predictor $u \\in S_{fwd}$ and adding one predictor $v \\notin S_{fwd}$.\n3.  Evaluate the RSS for every possible swapped model $S' = (S_{fwd} \\setminus \\{u\\}) \\cup \\{v\\}$.\n4.  Identify the single swap that results in the greatest reduction in RSS. If this best swap yields an RSS strictly lower than that of $S_{fwd}$, the model is updated. Otherwise, the model remains unchanged. At most one swap is performed.\n\n**Best Subset Selection:** This is the exhaustive, \"gold standard\" approach.\n1.  Examine every possible subset of predictors of size $k$. The number of such subsets is $\\binom{p}{k}$.\n2.  For each subset $S$, calculate $\\mathrm{RSS}(S)$.\n3.  The final model is the subset $S^*$ that has the minimum RSS among all candidates.\nWhile guaranteed to find the model with the lowest RSS for a given size $k$, this method is computationally infeasible for even moderate values of $p$. For the small values of $p$ in this problem ($p \\le 4$), it is perfectly feasible.\n\n### 3. Test Cases and Analysis\n\nThe problem specifies three test cases to probe the behavior of these algorithms. For all cases, $n=8$, $k=2$, and the true response is $y=u+v$ where $u,v$ are given base vectors. The best subset of size $k=2$ is always $\\{x_1, x_2\\}$ (where $x_1=u, x_2=v$), which perfectly explains $y$ and yields an RSS of (or numerically close to) $0$. The predictors are constructed to create specific challenges.\n\n-   **Case 1:** A \"decoy\" predictor $x_3$ is highly correlated with $y$, tempting forward selection to choose it first. This leads to a suboptimal model. However, the one-swap procedure is able to correct this mistake by swapping out the decoy for the correct predictor, reaching the global optimum. This is expected to result in code `3`.\n-   **Case 2:** The predictors are structured such that the greedy choices of forward selection align with the globally optimal path. A noise predictor is made orthogonal to the signal space, so it is never attractive to the algorithm. All three methods should agree on the optimal model. This is expected to result in code `0`.\n-   **Case 3:** Predictors $x_3$ and $x_4$ are constructed as \"confounded\" versions of the true predictors $x_1$ and $x_2$. Forward selection is expected to pick the pair $\\{x_3, x_4\\}$. A single swap can improve this model (e.g., swapping $x_3$ for $x_1$), but a second swap would be needed to reach the global optimum $\\{x_1, x_2\\}$, which is not allowed. The one-swap procedure thus finds a local minimum that is better than the forward selection result but still suboptimal. This is expected to result in code `2`.\n\nThe implementation consolidates these algorithms and the test case generation logic into a single program. The final output is an integer code for each test case, determined by comparing the resulting sets and their RSS values from the three algorithms against a numerical tolerance $\\varepsilon = 10^{-8}$.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Implements and compares pure forward selection, forward selection with one swap,\n    and best subset selection for linear regression.\n    \"\"\"\n\n    # Epsilon for floating point comparisons\n    EPS = 1e-8\n\n    def orth(a, B_vectors):\n        \"\"\"Orthogonalizes vector a with respect to the span of vectors in B.\"\"\"\n        if not B_vectors:\n            return a\n        B = np.stack(B_vectors, axis=1)\n        # Solve least squares for a = B*gamma\n        gamma_hat = np.linalg.lstsq(B, a, rcond=None)[0]\n        projection = B @ gamma_hat\n        return a - projection\n\n    def calculate_rss(X, y, S_indices):\n        \"\"\"Calculates Residual Sum of Squares for a given subset of predictors.\"\"\"\n        n = X.shape[0]\n        if not S_indices:\n            # Intercept-only model\n            A = np.ones((n, 1))\n        else:\n            S_list = sorted(list(S_indices))\n            X_S = X[:, S_list]\n            A = np.c_[np.ones(n), X_S]\n\n        res = np.linalg.lstsq(A, y, rcond=None)[1]\n        \n        # Fallback if lstsq doesn't return RSS (not expected for this problem's setup)\n        if res.size == 0:\n            coeffs = np.linalg.lstsq(A, y, rcond=None)[0]\n            y_hat = A @ coeffs\n            return np.sum((y - y_hat)**2)\n            \n        return res[0]\n\n    def forward_selection(X, y, k):\n        \"\"\"Performs pure forward stepwise selection.\"\"\"\n        p = X.shape[1]\n        current_S = set()\n        \n        for _ in range(k):\n            best_new_predictor = -1\n            min_rss = float('inf')\n            \n            available_predictors = set(range(p)) - current_S\n            for j in available_predictors:\n                candidate_S = current_S | {j}\n                rss = calculate_rss(X, y, candidate_S)\n                if rss  min_rss:\n                    min_rss = rss\n                    best_new_predictor = j\n            \n            if best_new_predictor != -1:\n                current_S.add(best_new_predictor)\n            \n        final_rss = calculate_rss(X, y, current_S)\n        return frozenset(current_S), final_rss\n\n    def forward_one_swap(X, y, S_fwd, rss_fwd):\n        \"\"\"Performs forward selection refinement with one swap.\"\"\"\n        p = X.shape[1]\n        all_predictors = set(range(p))\n        \n        best_swap_S = S_fwd\n        best_swap_rss = rss_fwd\n        \n        predictors_in = S_fwd\n        predictors_out = all_predictors - S_fwd\n        \n        for u in predictors_in: # predictor to remove\n            for v in predictors_out: # predictor to add\n                candidate_S = (S_fwd - {u}) | {v}\n                candidate_rss = calculate_rss(X, y, candidate_S)\n                if candidate_rss  best_swap_rss:\n                    best_swap_rss = candidate_rss\n                    best_swap_S = candidate_S\n                    \n        return frozenset(best_swap_S), best_swap_rss\n\n    def best_subset_selection(X, y, k):\n        \"\"\"Performs exhaustive best subset selection.\"\"\"\n        p = X.shape[1]\n        best_S = None\n        min_rss = float('inf')\n        \n        for S_tuple in combinations(range(p), k):\n            S = frozenset(S_tuple)\n            rss = calculate_rss(X, y, S)\n            if rss  min_rss:\n                min_rss = rss\n                best_S = S\n                \n        return best_S, min_rss\n\n    def generate_data(case_id):\n        \"\"\"Generates synthetic data for the specified test case.\"\"\"\n        u = np.array([4., -3., 5., -4., 3., -2., 1., 0.])\n        v = np.array([1., 2., -1., -2., 0., 1., -1., 0.])\n        y = u + v\n        n = 8\n        \n        if case_id == 1:\n            p, k = 4, 2\n            x1 = u\n            x2 = v\n            s0 = np.array([1., -1., 2., -2., 1., -1., 2., -2.])\n            s = orth(s0, [u, v])\n            x3 = u + 0.5 * y + 0.05 * s\n            x4_raw = np.array([0.2, -0.1, 0.05, 0.1, -0.4, 0.3, 0.2, -0.1])\n            x4 = orth(x4_raw, [u, v, s])\n            X = np.stack([x1, x2, x3, x4], axis=1)\n            return X, y, p, k\n        \n        elif case_id == 2:\n            p, k = 3, 2\n            x1 = u\n            x2 = v\n            x3_raw = np.array([2., -2., 1., -1., 2., -2., 1., -1.])\n            x3 = orth(x3_raw, [u, v])\n            X = np.stack([x1, x2, x3], axis=1)\n            return X, y, p, k\n            \n        elif case_id == 3:\n            p, k = 4, 2\n            x1 = u\n            x2 = v\n            t1_raw = np.array([1., 2., 3., 4., 5., 6., 7., 8.])\n            t1 = orth(t1_raw, [u, v])\n            t2_raw = np.array([2., -1., 0., 1., -2., 3., -3., 1.])\n            t2 = orth(t2_raw, [u, v, t1])\n            x3 = x1 + 0.6 * y + 0.2 * t1\n            x4 = x2 + 0.3 * y + 0.4 * t2\n            X = np.stack([x1, x2, x3, x4], axis=1)\n            return X, y, p, k\n        \n        else:\n            raise ValueError(\"Invalid case_id\")\n\n    results = []\n    for case_id in [1, 2, 3]:\n        X, y, p, k = generate_data(case_id)\n        \n        S_fwd, rss_fwd = forward_selection(X, y, k)\n        S_swap, rss_swap = forward_one_swap(X, y, S_fwd, rss_fwd)\n        S_best, rss_best = best_subset_selection(X, y, k)\n        \n        set_changed = (S_fwd != S_swap)\n        fwd_is_optimal = (abs(rss_fwd - rss_best) = EPS)\n        swap_is_optimal = (abs(rss_swap - rss_best) = EPS)\n\n        code = -1\n        if not set_changed:\n            if fwd_is_optimal:\n                code = 0\n            else:\n                code = 1\n        else: # set_changed is True\n            if swap_is_optimal:\n                code = 3\n            else:\n                code = 2\n        \n        results.append(code)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3104971"}]}