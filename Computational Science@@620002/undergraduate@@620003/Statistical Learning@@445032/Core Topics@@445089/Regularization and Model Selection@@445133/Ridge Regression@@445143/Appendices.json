{"hands_on_practices": [{"introduction": "Understanding a new statistical method often begins with seeing it in its simplest form. This first exercise strips ridge regression down to its core: fitting a single coefficient. By starting with the penalized sum of squares objective function and using basic calculus, you will derive the ridge estimator from first principles, building a solid foundation for tackling more complex, multi-variable problems. [@problem_id:1951876]", "problem": "In a machine learning context, we are tasked with fitting a simple linear model without an intercept, $y = \\beta x$, to a set of $n$ data points $(x_i, y_i)$. To prevent overfitting on a small dataset, we employ ridge regression. The ridge estimate for the coefficient $\\beta$ is the value that minimizes the penalized sum of squared errors, also known as the objective function $L(\\beta)$:\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\nwhere $\\lambda > 0$ is the regularization parameter that controls the amount of shrinkage.\n\nYour task is two-fold. First, by minimizing the objective function $L(\\beta)$, derive the general closed-form expression for the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ in terms of the data points $(x_i, y_i)$ and the parameter $\\lambda$.\n\nSecond, apply this derived expression to a specific dataset consisting of two points: $(x_1, y_1) = (1, 3)$ and $(x_2, y_2) = (2, 5)$. Calculate the numerical value of the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ using a regularization parameter of $\\lambda = 1$.\n\nProvide the final numerical value as an exact fraction.", "solution": "We minimize the penalized sum of squared errors for the no-intercept linear model $y=\\beta x$ with objective\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\nExpand the squared term and collect like powers of $\\beta$:\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\nDifferentiate with respect to $\\beta$ and set the derivative to zero (first-order optimality condition):\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\nSolve for $\\beta$:\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\nso the solution is the unique minimizer.\n\nApply this to $(x_{1},y_{1})=(1,3)$, $(x_{2},y_{2})=(2,5)$ with $\\lambda=1$:\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\nTherefore,\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "While deriving the estimator for a single parameter is insightful, real-world datasets almost always involve multiple predictors, which are best handled using linear algebra. This practice problem transitions you from summation notation to the standard matrix equation for ridge regression, a crucial step toward practical implementation. You will work with the pre-computed summary statistics that are often used as inputs for statistical software, focusing your attention on the core ridge calculation. [@problem_id:1951893]", "problem": "In the field of machine learning, ridge regression is a common technique used to regularize linear regression models. This is particularly useful for preventing overfitting and handling multicollinearity among predictor variables. The ridge regression estimator for the coefficient vector, $\\hat{\\beta}_{\\lambda}$, is given by the formula:\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\nHere, $X$ is the design matrix, $y$ is the vector of observed outcomes, $I$ is the identity matrix of appropriate dimensions, and $\\lambda$ is a non-negative regularization parameter.\n\nSuppose that for a particular dataset with two predictor variables, the following quantities have been pre-computed:\n$$ X^T X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\nUsing a regularization parameter of $\\lambda = 5$, determine the ridge regression coefficient vector $\\hat{\\beta}_5$.", "solution": "The ridge regression estimator is defined by\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\nWith the given data,\n$$\nX^{T}X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\nCompute the regularized matrix:\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} + 5 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 5 \\\\ 5 & 15 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is given by\n$$\n\\left(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\n$$\nApplying this,\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\nso\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix}.\n$$\nThen\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "The power of ridge regression lies in its tuning parameter, $\\lambda$, which controls the trade-off between bias and variance. But how do we choose the best value for $\\lambda$? This exercise walks you through the logical steps of $K$-fold cross-validation, a standard and robust method for hyperparameter tuning. Mastering this procedure is essential for applying ridge regression effectively to real data and building models that generalize well to new observations. [@problem_id:1951879]", "problem": "A data scientist is tasked with building a predictive model using ridge regression. Ridge regression is a type of linear regression that includes a penalty term to shrink the coefficient estimates, which is particularly useful for mitigating multicollinearity and preventing overfitting. The strength of this penalty is controlled by a non-negative tuning parameter, $\\lambda$. A crucial step in the modeling process is to select the optimal value of $\\lambda$ from a set of candidate values. A common method for this is K-fold cross-validation.\n\nThe data scientist has identified the following key actions to perform K-fold cross-validation to find the optimal $\\lambda$ and build the final model. The prediction error is measured using Mean Squared Error (MSE).\n\n(i) Choose the value of $\\lambda$ from the candidate set that yields the smallest average MSE across the folds.\n(ii) For each candidate value of $\\lambda$, calculate the average MSE by iterating through the K folds, each time training the model on K-1 folds and validating on the held-out fold.\n(iii) Randomly partition the entire dataset into K subsets, or \"folds,\" of approximately equal size.\n(iv) Train a final ridge regression model on the *entire* dataset using the optimal $\\lambda$ value selected in the previous step.\n\nWhat is the correct logical sequence of these actions?\n\nA. (iii) -> (i) -> (ii) -> (iv)\n\nB. (ii) -> (iii) -> (i) -> (iv)\n\nC. (iii) -> (ii) -> (i) -> (iv)\n\nD. (iii) -> (ii) -> (iv) -> (i)\n\nE. (ii) -> (i) -> (iv) -> (iii)", "solution": "Ridge regression fits coefficients $\\beta$ by minimizing the penalized least squares objective\n$$\n\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2},\n$$\nwhere $\\lambda \\geq 0$ is a tuning parameter chosen by cross-validation. To select $\\lambda$, define a candidate set $\\Lambda$ and perform K-fold cross-validation as follows.\n\nFirst, randomly partition the dataset indices $\\{1,\\dots,n\\}$ into $K$ disjoint folds $I_{1},\\dots,I_{K}$ of approximately equal size, which corresponds to action (iii). For each $\\lambda \\in \\Lambda$, iterate over folds $k=1,\\dots,K$: fit the ridge model using $\\lambda$ on the training set indexed by $I_{-k} = \\{1,\\dots,n\\} \\setminus I_{k}$ to obtain $\\hat{\\beta}^{(-k,\\lambda)}$, compute validation predictions $\\hat{y}_{i}^{(-k,\\lambda)}$ for $i \\in I_{k}$, and compute the fold MSE\n$$\n\\mathrm{MSE}_{k}(\\lambda) = \\frac{1}{|I_{k}|} \\sum_{i \\in I_{k}} \\left(y_{i} - \\hat{y}_{i}^{(-k,\\lambda)}\\right)^{2}.\n$$\nThen compute the average cross-validated error for $\\lambda$,\n$$\n\\overline{\\mathrm{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathrm{MSE}_{k}(\\lambda),\n$$\nwhich corresponds to action (ii). Select the optimal tuning parameter by\n$$\n\\lambda^{*} = \\arg\\min_{\\lambda \\in \\Lambda} \\overline{\\mathrm{MSE}}(\\lambda),\n$$\nwhich corresponds to action (i). Finally, refit the ridge regression model on the entire dataset using $\\lambda^{*}$, which corresponds to action (iv).\n\nThus, the correct logical sequence is (iii) → (ii) → (i) → (iv), i.e., option C.", "answer": "$$\\boxed{C}$$", "id": "1951879"}]}