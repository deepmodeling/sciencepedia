{"hands_on_practices": [{"introduction": "The defining feature of Lasso regression is its use of an $L_1$ penalty to regularize a model. This penalty is the sum of the absolute values of the coefficients, and it is this specific form that encourages sparse solutions where some coefficients are set exactly to zero. This first exercise provides a direct, hands-on calculation of the $L_1$ norm for a given set of coefficients, ensuring you grasp the fundamental component that drives Lasso's powerful feature selection capabilities. [@problem_id:1928640]", "problem": "In statistical learning, the Least Absolute Shrinkage and Selection Operator (LASSO) is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model. The penalty applied in LASSO is based on the $L_1$ norm of the vector of model coefficients.\n\nSuppose a data analyst has fit a LASSO regression model with four predictor variables. The final estimated coefficient vector for these predictors is given by $\\beta = (3, -1, 0, -4)^{\\top}$.\n\nCalculate the $L_1$ norm of this coefficient vector, denoted as $||\\beta||_1$.", "solution": "The $L_{1}$ norm of a vector is defined as the sum of the absolute values of its components. For $\\beta = (3, -1, 0, -4)^{\\top}$, this is\n$$\n\\|\\beta\\|_{1}=\\sum_{i=1}^{4}|\\beta_{i}|=|3|+|-1|+|0|+|-4|.\n$$\nEvaluating each absolute value,\n$$\n|3|=3,\\quad |-1|=1,\\quad |0|=0,\\quad |-4|=4,\n$$\nso\n$$\n\\|\\beta\\|_{1}=3+1+0+4=8.\n$$", "answer": "$$\\boxed{8}$$", "id": "1928640"}, {"introduction": "A crucial step in applying Lasso is choosing the right value for the tuning parameter, $\\lambda$, which controls the trade-off between model fit and sparsity. A value that is too high will over-penalize the coefficients, while a value that is too low will fail to regularize effectively. This problem simulates the standard and robust technique of k-fold cross-validation, allowing you to work through the process of calculating prediction error for a set of candidate $\\lambda$ values and selecting the one that yields the best predictive performance. [@problem_id:1928609]", "problem": "A materials scientist is developing a predictive model for the thermal conductivity, $y$, of a new alloy based on the concentrations of two key additives, $x_1$ and $x_2$. The proposed model is a linear relationship of the form $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$. To prevent overfitting and select a parsimonious model, the scientist decides to use the Least Absolute Shrinkage and Selection Operator (LASSO) regression technique.\n\nThe core of LASSO is the tuning parameter, $\\lambda$, which controls the amount of regularization. The optimal value of $\\lambda$ is to be chosen from the set $\\{0.1, 1.0, 10.0\\}$ using 3-fold cross-validation. The scientist has collected the following six data points:\n\n| Data Point | $x_1$ | $x_2$ | $y$ |\n| :--- | :--- | :--- | :--- |\n| 1 | 1.0 | 0.0 | 1.5 |\n| 2 | 0.0 | 1.0 | 2.5 |\n| 3 | 2.0 | 1.0 | 4.0 |\n| 4 | 1.0 | 2.0 | 5.0 |\n| 5 | 3.0 | 2.0 | 6.5 |\n| 6 | 2.0 | 3.0 | 7.5 |\n\nThe 3-fold cross-validation is set up as follows:\n- **Fold 1**: Data points 1, 2\n- **Fold 2**: Data points 3, 4\n- **Fold 3**: Data points 5, 6\n\nFor each of the three cross-validation iterations, a LASSO model was trained on two of the folds (the training set) and then used to predict on the remaining fold (the validation set). The resulting model coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ for each training run and each value of $\\lambda$ are provided below:\n\n**Coefficients from training on Folds 2 and 3 (for validating on Fold 1):**\n- For $\\lambda=0.1$: $\\hat{\\beta}_0=1.35, \\hat{\\beta}_1=0.84, \\hat{\\beta}_2=1.26$\n- For $\\lambda=1.0$: $\\hat{\\beta}_0=2.80, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.90$\n- For $\\lambda=10.0$: $\\hat{\\beta}_0=5.75, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n**Coefficients from training on Folds 1 and 3 (for validating on Fold 2):**\n- For $\\lambda=0.1$: $\\hat{\\beta}_0=0.40, \\hat{\\beta}_1=1.33, \\hat{\\beta}_2=1.49$\n- For $\\lambda=1.0$: $\\hat{\\beta}_0=1.13, \\hat{\\beta}_1=0.65, \\hat{\\beta}_2=1.55$\n- For $\\lambda=10.0$: $\\hat{\\beta}_0=4.50, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n**Coefficients from training on Folds 1 and 2 (for validating on Fold 3):**\n- For $\\lambda=0.1$: $\\hat{\\beta}_0=0.60, \\hat{\\beta}_1=1.14, \\hat{\\beta}_2=1.56$\n- For $\\lambda=1.0$: $\\hat{\\beta}_0=1.55, \\hat{\\beta}_1=0.40, \\hat{\\beta}_2=1.30$\n- For $\\lambda=10.0$: $\\hat{\\beta}_0=3.25, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\nYour task is to calculate the cross-validation error for each candidate value of $\\lambda$. The error for each fold should be measured using the Mean Squared Error (MSE). The overall cross-validation error for a given $\\lambda$ is the average of the MSEs from the three folds. Based on your calculations, which of the following is the optimal value for the tuning parameter $\\lambda$?\n\nA. $0.1$\n\nB. $1.0$\n\nC. $10.0$\n\nD. All values of $\\lambda$ result in the same cross-validation error.", "solution": "We model the response as $\\hat{y}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1}+\\hat{\\beta}_{2}x_{2}$. For each fold and $\\lambda$, we compute residuals $r_{i}=y_{i}-\\hat{y}_{i}$ on the validation points and the fold MSE as $\\text{MSE}=\\frac{1}{n}\\sum r_{i}^{2}$ with $n=2$. The cross-validation error for a given $\\lambda$ is the average of the three fold MSEs.\n\nFold 1 (validate on points 1 and 2; use coefficients from training on Folds 2 and 3):\n\nFor $\\lambda=0.1$: $(\\hat{\\beta}_{0},\\hat{\\beta}_{1},\\hat{\\beta}_{2})=(1.35,0.84,1.26)$.\n- Point 1: $(x_{1},x_{2},y)=(1,0,1.5)$, $\\hat{y}=1.35+0.84\\cdot 1+1.26\\cdot 0=2.19$, $r=1.5-2.19=-0.69$, $r^{2}=0.4761$.\n- Point 2: $(0,1,2.5)$, $\\hat{y}=1.35+0.84\\cdot 0+1.26\\cdot 1=2.61$, $r=2.5-2.61=-0.11$, $r^{2}=0.0121$.\n$$\\text{MSE}_{1}(0.1)=\\frac{0.4761+0.0121}{2}=0.2441.$$\n\nFor $\\lambda=1.0$: $(2.80,0.00,0.90)$.\n- Point 1: $\\hat{y}=2.80$, $r=-1.30$, $r^{2}=1.69$.\n- Point 2: $\\hat{y}=2.80+0.90=3.70$, $r=-1.20$, $r^{2}=1.44$.\n$$\\text{MSE}_{1}(1.0)=\\frac{1.69+1.44}{2}=1.565.$$\n\nFor $\\lambda=10.0$: $(5.75,0.00,0.00)$.\n- Point 1: $\\hat{y}=5.75$, $r=-4.25$, $r^{2}=18.0625$.\n- Point 2: $\\hat{y}=5.75$, $r=-3.25$, $r^{2}=10.5625$.\n$$\\text{MSE}_{1}(10.0)=\\frac{18.0625+10.5625}{2}=14.3125.$$\n\nFold 2 (validate on points 3 and 4; use coefficients from training on Folds 1 and 3):\n\nFor $\\lambda=0.1$: $(0.40,1.33,1.49)$.\n- Point 3: $(2,1,4.0)$, $\\hat{y}=0.40+1.33\\cdot 2+1.49\\cdot 1=4.55$, $r=-0.55$, $r^{2}=0.3025$.\n- Point 4: $(1,2,5.0)$, $\\hat{y}=0.40+1.33\\cdot 1+1.49\\cdot 2=4.71$, $r=0.29$, $r^{2}=0.0841$.\n$$\\text{MSE}_{2}(0.1)=\\frac{0.3025+0.0841}{2}=0.1933.$$\n\nFor $\\lambda=1.0$: $(1.13,0.65,1.55)$.\n- Point 3: $\\hat{y}=1.13+0.65\\cdot 2+1.55\\cdot 1=3.98$, $r=0.02$, $r^{2}=0.0004$.\n- Point 4: $\\hat{y}=1.13+0.65\\cdot 1+1.55\\cdot 2=4.88$, $r=0.12$, $r^{2}=0.0144$.\n$$\\text{MSE}_{2}(1.0)=\\frac{0.0004+0.0144}{2}=0.0074.$$\n\nFor $\\lambda=10.0$: $(4.50,0.00,0.00)$.\n- Point 3: $\\hat{y}=4.50$, $r=-0.50$, $r^{2}=0.25$.\n- Point 4: $\\hat{y}=4.50$, $r=0.50$, $r^{2}=0.25$.\n$$\\text{MSE}_{2}(10.0)=\\frac{0.25+0.25}{2}=0.25.$$\n\nFold 3 (validate on points 5 and 6; use coefficients from training on Folds 1 and 2):\n\nFor $\\lambda=0.1$: $(0.60,1.14,1.56)$.\n- Point 5: $(3,2,6.5)$, $\\hat{y}=0.60+1.14\\cdot 3+1.56\\cdot 2=7.14$, $r=-0.64$, $r^{2}=0.4096$.\n- Point 6: $(2,3,7.5)$, $\\hat{y}=0.60+1.14\\cdot 2+1.56\\cdot 3=7.56$, $r=-0.06$, $r^{2}=0.0036$.\n$$\\text{MSE}_{3}(0.1)=\\frac{0.4096+0.0036}{2}=0.2066.$$\n\nFor $\\lambda=1.0$: $(1.55,0.40,1.30)$.\n- Point 5: $\\hat{y}=1.55+0.40\\cdot 3+1.30\\cdot 2=5.35$, $r=1.15$, $r^{2}=1.3225$.\n- Point 6: $\\hat{y}=1.55+0.40\\cdot 2+1.30\\cdot 3=6.25$, $r=1.25$, $r^{2}=1.5625$.\n$$\\text{MSE}_{3}(1.0)=\\frac{1.3225+1.5625}{2}=1.4425.$$\n\nFor $\\lambda=10.0$: $(3.25,0.00,0.00)$.\n- Point 5: $\\hat{y}=3.25$, $r=3.25$, $r^{2}=10.5625$.\n- Point 6: $\\hat{y}=3.25$, $r=4.25$, $r^{2}=18.0625$.\n$$\\text{MSE}_{3}(10.0)=\\frac{10.5625+18.0625}{2}=14.3125.$$\n\nCompute average cross-validation error for each $\\lambda$:\n$$\\text{CV}(0.1)=\\frac{0.2441+0.1933+0.2066}{3}=0.214666\\ldots,$$\n$$\\text{CV}(1.0)=\\frac{1.565+0.0074+1.4425}{3}=1.004966\\ldots,$$\n$$\\text{CV}(10.0)=\\frac{14.3125+0.25+14.3125}{3}=9.625.$$\n\nThe smallest cross-validation error occurs at $\\lambda=0.1$, hence the optimal choice is option A.", "answer": "$$\\boxed{A}$$", "id": "1928609"}, {"introduction": "While we can define the Lasso model by its objective function, understanding how the optimal coefficients are actually determined provides deeper insight into its behavior. This exercise guides you through deriving the solution from first principles using the Karush-Kuhn-Tucker (KKT) optimality conditions, which are fundamental to constrained optimization. By applying these conditions in a simplified setting, you will discover the famous soft-thresholding operator and see precisely how the Lasso penalty shrinks some coefficients towards zero and sets others exactly to zero, effectively performing variable selection. [@problem_id:3184336]", "problem": "Consider the least absolute shrinkage and selection operator (LASSO) problem with no intercept: minimize the objective\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix, $y \\in \\mathbb{R}^{n}$ is a response vector, $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector, and $\\lambda \\ge 0$ is a tuning parameter. Assume that the columns of $X$ are standardized to be orthonormal, that is, $X^{\\top}X = I_{p}$, and that there is no intercept term in the model.\n\nTasks:\n1) Starting only from the convex optimality principle that a point $\\beta^{\\star}$ minimizes a convex function $f$ if and only if $\\mathbf{0} \\in \\partial f(\\beta^{\\star})$, and from the definition of the subgradient of the $\\ell_{1}$-norm, derive explicit Karush-Kuhn-Tucker (KKT) conditions (stationarity with subgradients) that characterize optimal solutions $\\beta^{\\star}$ to the LASSO problem in this setting. Your derivation must make clear how these conditions specialize for coordinates $j$ with $\\beta^{\\star}_{j} \\ne 0$ and for coordinates with $\\beta^{\\star}_{j} = 0$.\n\n2) Using your derived conditions, hand-solve the following three-feature instance with $p = 3$. Suppose $X^{\\top}X = I_{3}$ and\n$$\nX^{\\top}y = \\begin{pmatrix} 1.8 \\\\ -0.5 \\\\ 0.4 \\end{pmatrix}, \\qquad \\lambda = 0.6.\n$$\nCompute the optimal coefficient vector $\\beta^{\\star} \\in \\mathbb{R}^{3}$.\n\nProvide your final numerical answer for $\\beta^{\\star}$ as a single row vector.", "solution": "**Task 1: Derivation of the Karush-Kuhn-Tucker (KKT) conditions**\n\nThe LASSO objective function to be minimized is:\n$$\nf(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^{n}$, $\\beta \\in \\mathbb{R}^{p}$, and $\\lambda \\ge 0$. We are given that the design matrix $X$ has orthonormal columns, i.e., $X^{\\top}X = I_{p}$.\n\nThe objective function $f(\\beta)$ is a sum of two convex functions:\n$1$. $g(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$, which is differentiable and convex.\n$2$. $h(\\beta) = \\lambda \\|\\beta\\|_{1}$, which is convex but not differentiable at points where any component of $\\beta$ is zero.\n\nThe function $f(\\beta)$ is convex. According to the fundamental principle of convex optimization, a point $\\beta^{\\star}$ is a global minimizer of $f(\\beta)$ if and only if the zero vector is an element of the subdifferential of $f$ at $\\beta^{\\star}$:\n$$\n\\mathbf{0} \\in \\partial f(\\beta^{\\star})\n$$\nSince $g(\\beta)$ is differentiable, the subdifferential of the sum $f(\\beta) = g(\\beta) + h(\\beta)$ is the sum of the gradient of $g(\\beta)$ and the subdifferential of $h(\\beta)$:\n$$\n\\partial f(\\beta^{\\star}) = \\nabla g(\\beta^{\\star}) + \\partial h(\\beta^{\\star})\n$$\nFirst, we compute the gradient of $g(\\beta)$. We expand the squared norm:\n$$\ng(\\beta) = \\frac{1}{2}(y - X\\beta)^{\\top}(y - X\\beta) = \\frac{1}{2}(y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta)\n$$\nTaking the gradient with respect to $\\beta$:\n$$\n\\nabla g(\\beta) = \\frac{1}{2}(-2X^{\\top}y + 2X^{\\top}X\\beta) = X^{\\top}X\\beta - X^{\\top}y\n$$\nUsing the given condition that $X^{\\top}X = I_{p}$, the gradient simplifies to:\n$$\n\\nabla g(\\beta) = \\beta - X^{\\top}y\n$$\nAt the optimum $\\beta^{\\star}$, the gradient is $\\nabla g(\\beta^{\\star}) = \\beta^{\\star} - X^{\\top}y$.\n\nNext, we characterize the subdifferential of $h(\\beta) = \\lambda \\|\\beta\\|_{1} = \\lambda \\sum_{j=1}^{p} |\\beta_j|$. The subdifferential $\\partial h(\\beta)$ is the Cartesian product of the subdifferentials of its components $\\lambda |\\beta_j|$. For a single component $\\beta_j$, the subdifferential of $\\lambda|\\beta_j|$ is:\n$$\n\\partial (\\lambda |\\beta_j|) = \\begin{cases} \\{\\lambda \\cdot \\text{sgn}(\\beta_j)\\} & \\text{if } \\beta_j \\ne 0 \\\\ [-\\lambda, \\lambda] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\nwhere $\\text{sgn}(\\cdot)$ is the sign function. So, $\\partial h(\\beta)$ consists of all vectors $v \\in \\mathbb{R}^{p}$ with components $v_j \\in \\partial (\\lambda |\\beta_j|)$.\n\nThe optimality condition $\\mathbf{0} \\in \\nabla g(\\beta^{\\star}) + \\partial h(\\beta^{\\star})$ can now be written as:\n$$\n\\mathbf{0} \\in (\\beta^{\\star} - X^{\\top}y) + \\partial(\\lambda \\|\\beta^{\\star}\\|_{1})\n$$\nThis is equivalent to stating that there exists a subgradient vector $s \\in \\partial(\\|\\beta^{\\star}\\|_1)$ such that:\n$$\n\\mathbf{0} = (\\beta^{\\star} - X^{\\top}y) + \\lambda s\n$$\nRearranging gives the stationarity condition:\n$$\nX^{\\top}y - \\beta^{\\star} = \\lambda s\n$$\nWe now analyze this condition component-wise for $j \\in \\{1, 2, \\dots, p\\}$, letting $c_j = (X^{\\top}y)_j$:\n$$\nc_j - \\beta^{\\star}_j = \\lambda s_j, \\quad \\text{where } s_j \\in \\partial|\\beta^{\\star}_j|\n$$\n\nWe specialize this condition for the two cases as required.\n\n**Case 1: The optimal coefficient is non-zero, $\\beta^{\\star}_j \\ne 0$.**\nIn this case, the subgradient of $|\\beta^{\\star}_j|$ is single-valued: $s_j = \\text{sgn}(\\beta^{\\star}_j)$. The stationarity condition for this component becomes:\n$$\nc_j - \\beta^{\\star}_j = \\lambda \\cdot \\text{sgn}(\\beta^{\\star}_j)\n$$\nSolving for $\\beta^{\\star}_j$:\n$$\n\\beta^{\\star}_j = c_j - \\lambda \\cdot \\text{sgn}(\\beta^{\\star}_j)\n$$\nIf $\\beta^{\\star}_j > 0$, then $\\text{sgn}(\\beta^{\\star}_j) = 1$, which gives $\\beta^{\\star}_j = c_j - \\lambda$. Consistency requires $c_j - \\lambda > 0$, so $c_j > \\lambda$.\nIf $\\beta^{\\star}_j < 0$, then $\\text{sgn}(\\beta^{\\star}_j) = -1$, which gives $\\beta^{\\star}_j = c_j + \\lambda$. Consistency requires $c_j + \\lambda < 0$, so $c_j < -\\lambda$.\nIn both sub-cases, we have $|c_j| > \\lambda$, and the sign of $\\beta^{\\star}_j$ matches the sign of $c_j$. Thus, we can write $\\text{sgn}(\\beta^{\\star}_j) = \\text{sgn}(c_j)$. The explicit KKT condition for a non-zero coefficient is:\n$$\nc_j - \\beta^{\\star}_j = \\lambda \\cdot \\text{sgn}(c_j) \\quad \\text{or} \\quad \\beta^{\\star}_j = c_j - \\lambda \\cdot \\text{sgn}(c_j)\n$$\n\n**Case 2: The optimal coefficient is zero, $\\beta^{\\star}_j = 0$.**\nIn this case, the subgradient $s_j$ can be any value in the interval $[-1, 1]$. The stationarity condition for this component is:\n$$\nc_j - 0 = \\lambda s_j \\quad \\text{for some } s_j \\in [-1, 1]\n$$\nThis implies $c_j = \\lambda s_j$. Since $s_j$ can range over $[-1, 1]$, $c_j$ must lie in the range $[-\\lambda, \\lambda]$. The explicit KKT condition for a zero coefficient is:\n$$\n|c_j| \\le \\lambda\n$$\n\nIn summary, the KKT conditions derived from the subgradient optimality criterion are:\n- If $|(X^{\\top}y)_j| > \\lambda$, then $\\beta^{\\star}_j = (X^{\\top}y)_j - \\lambda \\cdot \\text{sgn}((X^{\\top}y)_j)$.\n- If $|(X^{\\top}y)_j| \\le \\lambda$, then $\\beta^{\\star}_j = 0$.\n\nThis solution is known as soft-thresholding and can be written compactly as $\\beta^{\\star}_j = S_{\\lambda}((X^{\\top}y)_j)$, where $S_{\\lambda}(z) = \\text{sgn}(z)\\max(0, |z|-\\lambda)$.\n\n**Task 2: Solving a specific instance**\n\nWe are given a three-feature instance ($p=3$) with $X^{\\top}X = I_3$ and:\n$$\nX^{\\top}y = \\begin{pmatrix} 1.8 \\\\ -0.5 \\\\ 0.4 \\end{pmatrix}, \\qquad \\lambda = 0.6\n$$\nWe apply the KKT conditions derived above to compute each component of the optimal coefficient vector $\\beta^{\\star}$. Let $c = X^{\\top}y$.\n\n**For component $j=1$:**\n$c_1 = 1.8$. We compare its absolute value to $\\lambda$:\n$|c_1| = 1.8$. Since $1.8 > 0.6$, we are in the case $|c_1| > \\lambda$.\nThe optimal coefficient is non-zero and is calculated as:\n$$\n\\beta^{\\star}_1 = c_1 - \\lambda \\cdot \\text{sgn}(c_1) = 1.8 - 0.6 \\cdot \\text{sgn}(1.8) = 1.8 - 0.6(1) = 1.2\n$$\n\n**For component $j=2$:**\n$c_2 = -0.5$. We compare its absolute value to $\\lambda$:\n$|c_2| = 0.5$. Since $0.5 \\le 0.6$, we are in the case $|c_2| \\le \\lambda$.\nThe optimal coefficient is zero:\n$$\n\\beta^{\\star}_2 = 0\n$$\n\n**For component $j=3$:**\n$c_3 = 0.4$. We compare its absolute value to $\\lambda$:\n$|c_3| = 0.4$. Since $0.4 \\le 0.6$, we are in the case $|c_3| \\le \\lambda$.\nThe optimal coefficient is zero:\n$$\n\\beta^{\\star}_3 = 0\n$$\n\nCombining these results, the optimal coefficient vector is:\n$$\n\\beta^{\\star} = \\begin{pmatrix} 1.2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nAs a row vector, this is $\\begin{pmatrix} 1.2 & 0 & 0 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.2 & 0 & 0 \\end{pmatrix}}\n$$", "id": "3184336"}]}