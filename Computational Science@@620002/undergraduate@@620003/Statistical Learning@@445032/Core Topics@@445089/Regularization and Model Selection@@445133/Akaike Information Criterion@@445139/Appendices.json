{"hands_on_practices": [{"introduction": "The first practice addresses a classic challenge in classification: choosing the right level of model flexibility. We compare two flavors of Gaussian Discriminant Analysis—a simpler model (LDA) that assumes all classes share a covariance matrix, and a more complex one (QDA) that allows each class to have its own. This exercise demonstrates how the Akaike Information Criterion ($AIC$) provides a principled way to balance the improved fit of the flexible model against the risk of overfitting due to its higher number of parameters [@problem_id:3097943].", "problem": "A two-class Gaussian Discriminant Analysis (GDA) model assumes that each labeled observation $(x_i, y_i)$ is generated by first drawing a class label $y_i \\in \\{0,1\\}$ from a categorical distribution with class probabilities $(\\pi_0, \\pi_1)$ and then drawing the feature vector $x_i \\in \\mathbb{R}^p$ from a multivariate normal distribution conditional on $y_i$. There are two common model families: a model with equal covariance across classes (often called Linear Discriminant Analysis (LDA)) and a model with class-specific covariances (often called Quadratic Discriminant Analysis (QDA)). Given labeled data and the two candidate model families, choose the one that minimizes the Akaike Information Criterion (AIC), where Akaike Information Criterion (AIC) is the classical information-theoretic model selection score built from the maximum of the likelihood and the number of free parameters.\n\nYour program must implement the following, using maximum likelihood estimation for all parameters in each candidate family:\n\n- For the equal-covariance family: estimate a single shared covariance matrix across classes along with the class-specific mean vectors and the class prior probabilities.\n- For the unequal-covariance family: estimate a separate covariance matrix for each class along with the class-specific mean vectors and the class prior probabilities.\n- For each family, compute the joint data log-likelihood of the labeled dataset under the estimated parameters, then compute the AIC for that family. Select the family with the smaller AIC.\n\nAll computations must be performed in pure mathematical terms according to the generative description above. The dataset is provided as synthetic Gaussian data generated with fixed seeds for reproducibility. For each test case, data are built by concatenating samples from two multivariate normal distributions with specified means and covariances and with known class labels. Covariance matrices are given via their Cholesky-like factors to guarantee positive definiteness: for each class $k \\in \\{0,1\\}$, the covariance is defined as $\\Sigma_k = L_k L_k^{\\top}$.\n\nYour program should produce a single line of output containing a list of booleans, one per test case, where each boolean is `True` if the unequal-covariance family is selected and `False` otherwise. The line must be formatted exactly as a comma-separated Python list, for example `[True, False, True]`.\n\nImplement and evaluate the following test suite. In all cases, use the specified random seed to generate the data, and generate exactly $n_0$ samples for class $0$ and $n_1$ samples for class $1$.\n\n- Test case $1$ (happy path, imbalanced classes, distinctly different covariances):\n  - Dimension $p = 2$.\n  - Sample sizes: $n_0 = 140$, $n_1 = 35$.\n  - Random seed: $202311$.\n  - Means: $\\mu_0 = [0, 0]$, $\\mu_1 = [2, -2]$.\n  - Covariance factors:\n    - $L_0 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.3 & 0.7 \\end{bmatrix}$, so $\\Sigma_0 = L_0 L_0^{\\top}$.\n    - $L_1 = \\begin{bmatrix} 1.6 & 0.0 \\\\ -0.4 & 1.0 \\end{bmatrix}$, so $\\Sigma_1 = L_1 L_1^{\\top}$.\n\n- Test case $2$ (boundary-leaning case with nearly equal covariances):\n  - Dimension $p = 3$.\n  - Sample sizes: $n_0 = 100$, $n_1 = 60$.\n  - Random seed: $202312$.\n  - Means: $\\mu_0 = [0, 0, 0]$, $\\mu_1 = [0.2, -0.1, 0.1]$.\n  - Covariance factors:\n    - $L_0 = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.05 & 1.0 & 0.0 \\\\ 0.02 & 0.03 & 1.0 \\end{bmatrix}$, so $\\Sigma_0 = L_0 L_0^{\\top}$.\n    - $L_1 = \\begin{bmatrix} 1.02 & 0.0 & 0.0 \\\\ 0.05 & 1.05 & 0.0 \\\\ 0.02 & 0.03 & 0.95 \\end{bmatrix}$, so $\\Sigma_1 = L_1 L_1^{\\top}$.\n\n- Test case $3$ (edge case in higher dimension, strong covariance differences, imbalanced):\n  - Dimension $p = 4$.\n  - Sample sizes: $n_0 = 120$, $n_1 = 30$.\n  - Random seed: $202313$.\n  - Means: $\\mu_0 = [0, 0, 0, 0]$, $\\mu_1 = [1.5, -1.5, 1.0, 0.5]$.\n  - Covariance factors:\n    - $L_0 = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.1 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.05 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 0.02 & 1.0 \\end{bmatrix}$, so $\\Sigma_0 = L_0 L_0^{\\top}$.\n    - $L_1 = \\begin{bmatrix} 1.7 & 0.0 & 0.0 & 0.0 \\\\ 0.5 & 1.3 & 0.0 & 0.0 \\\\ 0.1 & 0.4 & 1.1 & 0.0 \\\\ 0.0 & 0.1 & -0.2 & 1.0 \\end{bmatrix}$, so $\\Sigma_1 = L_1 L_1^{\\top}$.\n\nRequirements and clarifications:\n\n- Use the labeled data to estimate parameters via maximum likelihood under each family.\n- Treat the class prior probabilities $(\\pi_0,\\pi_1)$ as parameters to be estimated from the labels.\n- Count parameters appropriately when constructing the model selection score: include the class priors, class means, and covariance entries according to the family.\n- Angles are not involved; no physical units are involved.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, `[True, False, True]`), where each boolean indicates whether the unequal-covariance family is selected for the corresponding test case.", "solution": "The user wants to select between two Gaussian Discriminant Analysis (GDA) model families—one with a shared covariance matrix for all classes (akin to Linear Discriminant Analysis, LDA) and one with class-specific covariance matrices (akin to Quadratic Discriminant Analysis, QDA). The selection criterion is the Akaike Information Criterion (AIC), which must be minimized. The solution involves deriving and implementing the Maximum Likelihood Estimators (MLEs) for the parameters of each model family, calculating the maximized log-likelihood, and then computing the AIC for comparison.\n\nLet the dataset be $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i \\in \\mathbb{R}^p$ are feature vectors and $y_i \\in \\{0, 1\\}$ are class labels. Let $n_k$ be the number of samples in class $k$, so $N = n_0 + n_1$. The generative model assumes $y \\sim \\text{Categorical}(\\pi_0, \\pi_1)$ and $x|y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$.\n\nThe total log-likelihood of the data is given by:\n$$ \\ln \\mathcal{L}(\\theta | \\mathcal{D}) = \\sum_{i=1}^N \\ln p(x_i, y_i | \\theta) = \\sum_{i=1}^N \\left[ \\ln p(y_i | \\theta) + \\ln p(x_i | y_i, \\theta) \\right] $$\nThis expression separates into a term for the class labels and a term for the features.\n\n_1. Maximum Likelihood Estimation of Common Parameters_\nThe MLEs for the class prior probabilities $\\pi_k$ and the class means $\\mu_k$ are the same for both model families.\nThe log-likelihood for the labels is $\\sum_{i=1}^N \\ln p(y_i) = n_0 \\ln \\pi_0 + n_1 \\ln \\pi_1$. Maximizing this under the constraint $\\pi_0 + \\pi_1 = 1$ yields:\n$$ \\hat{\\pi}_k = \\frac{n_k}{N} $$\nThe log-likelihood for the features separates by class. For each class $k$, the MLE for the mean vector $\\mu_k$ is the sample mean of the data in that class:\n$$ \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i=k} x_i $$\n\n_2. Unequal-Covariance Model (QDA Family)_\nIn this model, each class has its own covariance matrix, $\\Sigma_0$ and $\\Sigma_1$.\n\n_Parameter Estimation_: The MLE for each covariance matrix $\\Sigma_k$ is the sample covariance matrix for the corresponding class's data:\n$$ \\hat{\\Sigma}_k = \\frac{1}{n_k} \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top $$\n\n_Number of Parameters ($k_{QDA}$)_:\n- Class priors: $1$ parameter (since $\\pi_0+\\pi_1=1$).\n- Class means: $2$ vectors of size $p$, so $2p$ parameters.\n- Covariance matrices: $2$ symmetric $p \\times p$ matrices. Each has $p(p+1)/2$ unique parameters. Total is $2 \\times \\frac{p(p+1)}{2} = p(p+1)$ parameters.\nTotal parameters: $k_{QDA} = 1 + 2p + p(p+1) = p^2 + 3p + 1$.\n\n_Maximized Log-Likelihood_: When the MLEs are substituted back into the log-likelihood function for a multivariate normal distribution, the expression for class $k$ simplifies to:\n$$ \\ln \\hat{\\mathcal{L}}_k = -\\frac{n_k}{2} \\left( p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_k| + p \\right) $$\nThe total maximized log-likelihood for the QDA model is the sum of the label log-likelihood and the feature log-likelihoods for each class:\n$$ \\ln \\hat{\\mathcal{L}}_{QDA} = \\left( \\sum_{k=0}^1 n_k \\ln \\hat{\\pi}_k \\right) + \\left( \\sum_{k=0}^1 -\\frac{n_k}{2} (p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_k| + p) \\right) $$\n\n_3. Equal-Covariance Model (LDA Family)_\nIn this model, all classes share a single covariance matrix, $\\Sigma$.\n\n_Parameter Estimation_: The MLE for the shared covariance matrix $\\Sigma$ is the pooled covariance matrix, weighted by class size:\n$$ \\hat{\\Sigma}_{pool} = \\frac{1}{N} \\sum_{k=0}^1 \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top = \\frac{n_0 \\hat{\\Sigma}_0 + n_1 \\hat{\\Sigma}_1}{N} $$\n\n_Number of Parameters ($k_{LDA}$)_:\n- Class priors: $1$ parameter.\n- Class means: $2p$ parameters.\n- Covariance matrix: $1$ symmetric $p \\times p$ matrix, with $p(p+1)/2$ unique parameters.\nTotal parameters: $k_{LDA} = 1 + 2p + \\frac{p(p+1)}{2}$.\n\n_Maximized Log-Likelihood_: The logic is analogous to the QDA case, but using the single pooled covariance matrix $\\hat{\\Sigma}_{pool}$ for all $N$ data points:\n$$ \\ln \\hat{\\mathcal{L}}_{LDA} = \\left( \\sum_{k=0}^1 n_k \\ln \\hat{\\pi}_k \\right) - \\frac{N}{2} \\left( p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_{pool}| + p \\right) $$\n\n_4. Model Selection using AIC_\nThe Akaike Information Criterion is defined as $AIC = 2k - 2 \\ln \\hat{\\mathcal{L}}$, where $k$ is the number of estimated parameters and $\\hat{\\mathcal{L}}$ is the maximized value of the likelihood function. We compute the AIC for each model:\n$$ AIC_{QDA} = 2k_{QDA} - 2 \\ln \\hat{\\mathcal{L}}_{QDA} $$\n$$ AIC_{LDA} = 2k_{LDA} - 2 \\ln \\hat{\\mathcal{L}}_{LDA} $$\nThe model with the lower AIC is preferred. The program will return `True` if $AIC_{QDA} < AIC_{LDA}$ and `False` otherwise. This procedure is applied to each test case to generate the final list of boolean results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_aic_selection(p, n0, n1, seed, mu0, mu1, L0, L1):\n    \"\"\"\n    Performs GDA model selection for a single test case.\n\n    Selects between an equal-covariance (LDA-like) and an unequal-covariance\n    (QDA-like) model using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    # 1. Generate synthetic data based on the problem specification\n    rng = np.random.default_rng(seed)\n    N = n0 + n1\n    \n    mu0_true = np.array(mu0)\n    mu1_true = np.array(mu1)\n    L0_true = np.array(L0)\n    L1_true = np.array(L1)\n    \n    Sigma0_true = L0_true @ L0_true.T\n    Sigma1_true = L1_true @ L1_true.T\n    \n    # Generate data for each class\n    X0 = rng.multivariate_normal(mu0_true, Sigma0_true, size=n0)\n    X1 = rng.multivariate_normal(mu1_true, Sigma1_true, size=n1)\n\n    # 2. Compute parameters and AIC for each model\n    # The term for the log-likelihood of labels is common to both models,\n    # so it's calculated once.\n    pi0_hat = n0 / N\n    pi1_hat = n1 / N\n    logL_labels = n0 * np.log(pi0_hat) + n1 * np.log(pi1_hat)\n\n    # --- Unequal-Covariance Model (QDA) ---\n    \n    # Number of parameters: 1 (priors) + 2*p (means) + p*(p+1) (two cov matrices)\n    k_qda = 1 + 2 * p + p * (p + 1)\n    \n    # MLE for class-specific covariance matrices\n    Sigma0_hat = np.cov(X0, rowvar=False, bias=True)\n    Sigma1_hat = np.cov(X1, rowvar=False, bias=True)\n    \n    # Log-determinants of the covariance matrices\n    _, log_det_S0 = np.linalg.slogdet(Sigma0_hat)\n    _, log_det_S1 = np.linalg.slogdet(Sigma1_hat)\n    \n    # Maximized log-likelihood for features under the QDA model\n    logL_feat_qda = -0.5 * n0 * (p * np.log(2 * np.pi) + log_det_S0 + p)\n    logL_feat_qda += -0.5 * n1 * (p * np.log(2 * np.pi) + log_det_S1 + p)\n    \n    # Total maximized log-likelihood and AIC for QDA\n    logL_qda = logL_labels + logL_feat_qda\n    aic_qda = 2 * k_qda - 2 * logL_qda\n\n    # --- Equal-Covariance Model (LDA) ---\n    \n    # Number of parameters: 1 (priors) + 2*p (means) + p*(p+1)/2 (one cov matrix)\n    k_lda = 1 + 2 * p + p * (p + 1) / 2\n    \n    # MLE for the pooled covariance matrix\n    Sigma_pool_hat = (n0 * Sigma0_hat + n1 * Sigma1_hat) / N\n    \n    # Log-determinant of the pooled covariance matrix\n    _, log_det_Spool = np.linalg.slogdet(Sigma_pool_hat)\n    \n    # Maximized log-likelihood for features under the LDA model\n    logL_feat_lda = -0.5 * N * (p * np.log(2 * np.pi) + log_det_Spool + p)\n\n    # Total maximized log-likelihood and AIC for LDA\n    logL_lda = logL_labels + logL_feat_lda\n    aic_lda = 2 * k_lda - 2 * logL_lda\n\n    # 3. Model Selection\n    # Return True if the unequal-covariance model (QDA) has a lower AIC\n    return aic_qda  aic_lda\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 2, \"n0\": 140, \"n1\": 35, \"seed\": 202311,\n            \"mu0\": [0, 0], \"mu1\": [2, -2],\n            \"L0\": [[1.0, 0.0], [0.3, 0.7]],\n            \"L1\": [[1.6, 0.0], [-0.4, 1.0]],\n        },\n        {\n            \"p\": 3, \"n0\": 100, \"n1\": 60, \"seed\": 202312,\n            \"mu0\": [0, 0, 0], \"mu1\": [0.2, -0.1, 0.1],\n            \"L0\": [[1.0, 0.0, 0.0], [0.05, 1.0, 0.0], [0.02, 0.03, 1.0]],\n            \"L1\": [[1.02, 0.0, 0.0], [0.05, 1.05, 0.0], [0.02, 0.03, 0.95]],\n        },\n        {\n            \"p\": 4, \"n0\": 120, \"n1\": 30, \"seed\": 202313,\n            \"mu0\": [0, 0, 0, 0], \"mu1\": [1.5, -1.5, 1.0, 0.5],\n            \"L0\": [[1.0, 0.0, 0.0, 0.0], [0.1, 1.0, 0.0, 0.0], [0.0, 0.05, 1.0, 0.0], [0.0, 0.0, 0.02, 1.0]],\n            \"L1\": [[1.7, 0.0, 0.0, 0.0], [0.5, 1.3, 0.0, 0.0], [0.1, 0.4, 1.1, 0.0], [0.0, 0.1, -0.2, 1.0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = _calculate_aic_selection(\n            p=case[\"p\"],\n            n0=case[\"n0\"],\n            n1=case[\"n1\"],\n            seed=case[\"seed\"],\n            mu0=case[\"mu0\"],\n            mu1=case[\"mu1\"],\n            L0=case[\"L0\"],\n            L1=case[\"L1\"]\n        )\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3097943"}, {"introduction": "This second practice showcases the versatility of $AIC$ by applying it to a problem from reliability engineering. We are tasked with modeling failure times and must choose between a simple exponential distribution, which assumes a constant failure rate, and a more flexible Weibull distribution, where the failure rate can vary. This problem is an excellent illustration of using $AIC$ to compare nested models, where the simpler model is a special case of the more complex one, to determine if the additional complexity is justified by the data [@problem_id:3097983].", "problem": "You are given independent failure-time observations in reliability engineering and asked to decide, for each dataset, whether introducing a shape parameter via the Weibull model improves explanatory power over the exponential model. Your decision must be based on a principled, derivation-based model selection criterion grounded in information-theoretic risk and maximum likelihood, rather than ad hoc goodness-of-fit measures.\n\nAssume the following parametric families for nonnegative failure times:\n\n- Exponential model with rate parameter $\\beta  0$, with probability density function $f_{\\text{exp}}(t \\mid \\beta)$ defined for $t  0$.\n- Weibull model with shape parameter $k  0$ and scale parameter $\\lambda  0$, with probability density function $f_{\\text{weib}}(t \\mid k,\\lambda)$ defined for $t  0$.\n\nAssume all observations within each dataset are independent and identically distributed conditional on the model parameters. Base your model selection on the continuous distributions and the framework of Maximum Likelihood Estimation (MLE) and Kullback–Leibler divergence, using a criterion that asymptotically favors the model minimizing expected Kullback–Leibler risk. This criterion must rely on the maximized log-likelihood and include a penalty proportional to the number of free parameters in the model. Do not use any heuristic or graphical methods.\n\nYour program must, for each dataset:\n- Estimate the parameters by maximizing the log-likelihood under the exponential and Weibull models.\n- Compute the information-theoretic model selection criterion derived from the asymptotic bias of the maximized log-likelihood, with a penalty term for the number of free parameters.\n- Decide that the Weibull shape parameter improves explanatory power if and only if the Weibull model’s criterion value is strictly smaller than the exponential model’s criterion value by at least a small numerical tolerance $\\epsilon$, where $\\epsilon = 10^{-8}$.\n\nAll failure times are in hours. No unit conversion is needed; simply treat the times as real-valued positive numbers. The final outputs are booleans; therefore, there is no requirement to report physical units in the output.\n\nTest Suite:\nUse the following five datasets of observed failure times (in hours):\n- Test Case $1$: $[0.7, 1.1, 1.6, 2.2, 3.0, 3.8, 4.7, 5.7, 6.8, 8.0, 9.3, 10.7, 12.2, 13.8, 15.5]$.\n- Test Case $2$: $[0.05, 0.16, 0.20, 0.35, 0.42, 0.59, 0.60, 0.83, 0.90, 1.31, 1.39, 1.60, 2.50]$.\n- Test Case $3$: $[0.2, 0.3, 0.5, 0.9, 1.5, 2.5, 4.0, 6.8, 11.0, 18.0]$.\n- Test Case $4$: $[1.5, 0.7, 2.1, 1.0, 0.4, 3.3, 2.8, 1.2, 0.9, 2.0, 1.8, 2.4, 0.6, 3.0]$.\n- Test Case $5$: $[1.0, 2.0, 3.0]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be a boolean indicating, for the corresponding test case, whether adding the Weibull shape parameter improves explanatory power under the criterion. For example, the output should look like $[b_1,b_2,b_3,b_4,b_5]$ where each $b_i$ is either `True` or `False`.", "solution": "The user has provided a well-posed problem in statistical model selection, grounded in the principles of reliability engineering and information theory. The task is to compare two nested parametric models for failure-time data—the exponential and the Weibull distributions—and to decide which model provides a better fit for several datasets. The decision criterion is specified to be based on maximizing the likelihood function, penalized by the number of model parameters, which is a direct description of the Akaike Information Criterion (AIC).\n\nThe validation process confirms that the problem is scientifically sound, well-posed, and objective. It contains all necessary information and is free from contradictions or ambiguity. I will now proceed with a formal derivation of the solution.\n\nThe core of the problem lies in the application of the Akaike Information Criterion ($AIC$). The $AIC$ for a given model is defined as:\n$$AIC = 2p - 2\\mathcal{L}_{\\text{max}}$$\nwhere $p$ is the number of estimated parameters in the model and $\\mathcal{L}_{\\text{max}}$ is the maximized value of the log-likelihood function for the model. The model with the lower $AIC$ value is considered to have a better trade-off between goodness of fit and model complexity.\n\nLet a dataset of failure times be denoted by $T = \\{t_1, t_2, \\ldots, t_n\\}$, where $n$ is the number of observations. We assume the observations are independent and identically distributed.\n\n**Model 1: The Exponential Model**\nThe exponential distribution has a single rate parameter $\\beta  0$. Its probability density function ($PDF$) is:\n$$f_{\\text{exp}}(t \\mid \\beta) = \\beta e^{-\\beta t} \\quad \\text{for } t  0$$\nThe number of parameters is $p_{\\text{exp}} = 1$. The likelihood function for the dataset $T$ is:\n$$L(\\beta \\mid T) = \\prod_{i=1}^{n} f_{\\text{exp}}(t_i \\mid \\beta) = \\prod_{i=1}^{n} \\beta e^{-\\beta t_i} = \\beta^n e^{-\\beta \\sum_{i=1}^{n} t_i}$$\nThe log-likelihood function is:\n$$\\ell_{\\text{exp}}(\\beta \\mid T) = \\ln(L(\\beta \\mid T)) = n \\ln(\\beta) - \\beta \\sum_{i=1}^{n} t_i$$\nTo find the Maximum Likelihood Estimate (MLE) for $\\beta$, we differentiate with respect to $\\beta$ and set the result to zero:\n$$\\frac{\\partial \\ell_{\\text{exp}}}{\\partial \\beta} = \\frac{n}{\\beta} - \\sum_{i=1}^{n} t_i = 0$$\nSolving for $\\beta$ yields the MLE, $\\hat{\\beta}$:\n$$\\hat{\\beta} = \\frac{n}{\\sum_{i=1}^{n} t_i} = \\frac{1}{\\bar{t}}$$\nwhere $\\bar{t}$ is the sample mean of the failure times.\nThe maximized log-likelihood, $\\mathcal{L}_{\\text{exp}}$, is obtained by substituting $\\hat{\\beta}$ back into the log-likelihood function:\n$$\\mathcal{L}_{\\text{exp}} = \\ell_{\\text{exp}}(\\hat{\\beta} \\mid T) = n \\ln(\\hat{\\beta}) - \\hat{\\beta} \\sum_{i=1}^{n} t_i = n \\ln\\left(\\frac{1}{\\bar{t}}\\right) - \\frac{1}{\\bar{t}}(n\\bar{t}) = -n \\ln(\\bar{t}) - n$$\nThe $AIC$ for the exponential model is therefore:\n$$AIC_{\\text{exp}} = 2p_{\\text{exp}} - 2\\mathcal{L}_{\\text{exp}} = 2(1) - 2(-n \\ln(\\bar{t}) - n) = 2 + 2n \\ln(\\bar{t}) + 2n$$\n\n**Model 2: The Weibull Model**\nThe Weibull distribution has a shape parameter $k  0$ and a scale parameter $\\lambda  0$. Its $PDF$ is:\n$$f_{\\text{weib}}(t \\mid k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{t}{\\lambda}\\right)^{k-1} e^{-(t/\\lambda)^k} \\quad \\text{for } t  0$$\nThe number of parameters is $p_{\\text{weib}} = 2$. The log-likelihood function for the dataset $T$ is:\n$$\\ell_{\\text{weib}}(k, \\lambda \\mid T) = \\sum_{i=1}^{n} \\ln\\left[\\frac{k}{\\lambda} \\left(\\frac{t_i}{\\lambda}\\right)^{k-1} e^{-(t_i/\\lambda)^k}\\right]$$\n$$ = \\sum_{i=1}^{n} \\left[ \\ln(k) - k\\ln(\\lambda) + (k-1)\\ln(t_i) - \\left(\\frac{t_i}{\\lambda}\\right)^k \\right]$$\n$$ = n\\ln(k) - nk\\ln(\\lambda) + (k-1)\\sum_{i=1}^{n}\\ln(t_i) - \\lambda^{-k}\\sum_{i=1}^{n}t_i^k$$\nTo find the MLEs $(\\hat{k}, \\hat{\\lambda})$, we take partial derivatives with respect to $k$ and $\\lambda$ and set them to zero. The derivative with respect to $\\lambda$ yields a convenient relationship:\n$$\\frac{\\partial \\ell_{\\text{weib}}}{\\partial \\lambda} = -\\frac{nk}{\\lambda} + \\frac{k}{\\lambda^{k+1}}\\sum_{i=1}^{n}t_i^k = 0 \\implies \\lambda^k = \\frac{1}{n}\\sum_{i=1}^{n}t_i^k$$\nThis equation provides the MLE for $\\lambda$ as a function of $k$: $\\hat{\\lambda}(k) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}t_i^k\\right)^{1/k}$.\nSubstituting this back into the log-likelihood function gives a profile log-likelihood that depends only on $k$. Taking the derivative of this profile log-likelihood with respect to $k$ and setting it to zero yields the following implicit equation for the MLE $\\hat{k}$:\n$$\\frac{1}{k} - \\frac{\\sum_{i=1}^{n} t_i^k \\ln(t_i)}{\\sum_{i=1}^{n} t_i^k} + \\frac{1}{n}\\sum_{i=1}^{n}\\ln(t_i) = 0$$\nThis equation cannot be solved for $k$ in closed form. Thus, $\\hat{k}$ must be found numerically using a root-finding algorithm. Once $\\hat{k}$ is determined, $\\hat{\\lambda}$ is calculated using the derived formula:\n$$\\hat{\\lambda} = \\left(\\frac{1}{n}\\sum_{i=1}^{n}t_i^{\\hat{k}}\\right)^{1/\\hat{k}}$$\nThe maximized log-likelihood, $\\mathcal{L}_{\\text{weib}}$, is then computed by substituting $(\\hat{k}, \\hat{\\lambda})$ into the log-likelihood function:\n$$\\mathcal{L}_{\\text{weib}} = n\\ln(\\hat{k}) - n\\hat{k}\\ln(\\hat{\\lambda}) + (\\hat{k}-1)\\sum_{i=1}^{n}\\ln(t_i) - \\hat{\\lambda}^{-\\hat{k}}\\sum_{i=1}^{n}t_i^{\\hat{k}}$$\nUsing the expression for $\\hat{\\lambda}$, the last term simplifies to $\\hat{\\lambda}^{-\\hat{k}}\\sum_{i=1}^{n}t_i^{\\hat{k}} = (\\frac{1}{n}\\sum t_i^{\\hat{k}})^{-1} \\sum t_i^{\\hat{k}} = n$. So,\n$$\\mathcal{L}_{\\text{weib}} = n\\ln(\\hat{k}) - n\\hat{k}\\ln(\\hat{\\lambda}) + (\\hat{k}-1)\\sum_{i=1}^{n}\\ln(t_i) - n$$\nThe $AIC$ for the Weibull model is:\n$$AIC_{\\text{weib}} = 2p_{\\text{weib}} - 2\\mathcal{L}_{\\text{weib}} = 2(2) - 2\\mathcal{L}_{\\text{weib}} = 4 - 2\\mathcal{L}_{\\text{weib}}$$\n\n**Decision Criterion**\nThe problem states that introducing the Weibull shape parameter improves explanatory power if and only if the Weibull model's criterion value is strictly smaller than the exponential model's by at least $\\epsilon = 10^{-8}$. This translates to the following condition:\n$$AIC_{\\text{weib}}  AIC_{\\text{exp}} - \\epsilon$$\nFor each test case, we will compute $AIC_{\\text{exp}}$ and $AIC_{\\text{weib}}$ and apply this rule to determine whether the more complex Weibull model is preferred.\n\n**Algorithm for Implementation**\nFor each dataset:\n1.  Convert the input list of failure times to a `numpy` array.\n2.  Calculate $AIC_{\\text{exp}}$ using the analytical formula.\n3.  Define the implicit function for the Weibull shape parameter $\\hat{k}$.\n4.  Use a numerical root-finder, such as `scipy.optimize.root_scalar`, to solve for $\\hat{k}$.\n5.  Calculate the corresponding scale parameter $\\hat{\\lambda}$.\n6.  Compute the maximized log-likelihood $\\mathcal{L}_{\\text{weib}}$.\n7.  Calculate $AIC_{\\text{weib}}$.\n8.  Compare $AIC_{\\text{weib}}$ and $AIC_{\\text{exp}}$ using the specified decision rule and append the resulting boolean value to the results list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the provided test cases.\n    It compares the an exponential model and a Weibull model\n    for failure time data using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        [0.7, 1.1, 1.6, 2.2, 3.0, 3.8, 4.7, 5.7, 6.8, 8.0, 9.3, 10.7, 12.2, 13.8, 15.5],\n        # Test Case 2\n        [0.05, 0.16, 0.20, 0.35, 0.42, 0.59, 0.60, 0.83, 0.90, 1.31, 1.39, 1.60, 2.50],\n        # Test Case 3\n        [0.2, 0.3, 0.5, 0.9, 1.5, 2.5, 4.0, 6.8, 11.0, 18.0],\n        # Test Case 4\n        [1.5, 0.7, 2.1, 1.0, 0.4, 3.3, 2.8, 1.2, 0.9, 2.0, 1.8, 2.4, 0.6, 3.0],\n        # Test Case 5\n        [1.0, 2.0, 3.0]\n    ]\n\n    epsilon = 1e-8\n    results = []\n\n    for data_list in test_cases:\n        data = np.array(data_list)\n        n = len(data)\n\n        # 1. Exponential Model AIC Calculation\n        t_bar = np.mean(data)\n        # Maximized log-likelihood for Exponential model\n        max_log_lik_exp = -n * np.log(t_bar) - n\n        # Number of parameters for Exponential model\n        p_exp = 1\n        aic_exp = 2 * p_exp - 2 * max_log_lik_exp\n\n        # 2. Weibull Model AIC Calculation\n        log_data = np.log(data)\n        sum_log_data = np.sum(log_data)\n\n        # We need to solve for k_hat from the implicit equation derived from MLE.\n        # This function represents the equation set to zero.\n        def k_equation(k, data, n, sum_log_data):\n            if k = 0:\n                return -np.inf  # Ensure k is positive\n            try:\n                t_k = np.power(data, k)\n                sum_t_k = np.sum(t_k)\n                sum_t_k_log_t = np.sum(t_k * log_data)\n                \n                # Check for numerical instability\n                if sum_t_k == 0 or not np.isfinite(sum_t_k):\n                    return np.nan\n\n                # The equation to be solved for k:\n                # 1/k - (sum(t_i^k * log(t_i)) / sum(t_i^k)) + (sum(log(t_i)) / n) = 0\n                return 1.0 / k - sum_t_k_log_t / sum_t_k + sum_log_data / n\n            except (OverflowError, ValueError):\n                return np.nan\n\n        try:\n            # Search for the root k_hat in a reasonable interval.\n            # brentq is a robust root-finding method for a bracketed root.\n            sol = root_scalar(k_equation, args=(data, n, sum_log_data),\n                              bracket=[0.1, 25.0], method='brentq')\n            k_hat = sol.root\n            \n            # MLE for lambda (scale parameter)\n            lambda_hat = (np.sum(np.power(data, k_hat)) / n)**(1.0 / k_hat)\n\n            # Maximized log-likelihood for Weibull model\n            # loglik = n*log(k) - n*k*log(lambda) + (k-1)*sum(log(t_i)) - n\n            max_log_lik_weib = (n * np.log(k_hat) - n * k_hat * np.log(lambda_hat) +\n                                (k_hat - 1) * sum_log_data - n)\n\n            # Number of parameters for Weibull model\n            p_weib = 2\n            aic_weib = 2 * p_weib - 2 * max_log_lik_weib\n\n        except ValueError:\n            # If root finding fails (e.g., no root in bracket, equation is ill-behaved),\n            # the model fit is problematic. We treat this as not an improvement.\n            # A huge AIC indicates a very poor fit.\n            aic_weib = np.inf\n        \n        # 3. Decision\n        # Weibull is preferred if its AIC is smaller by at least epsilon\n        is_weibull_better = aic_weib  aic_exp - epsilon\n        results.append(is_weibull_better)\n        \n    # Format and print the final output as a single list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097983"}, {"introduction": "Our final practice ventures into the realm of nonparametric regression, where model complexity is not just a simple count of parameters. We will use an $AIC$-based criterion to select the optimal bandwidth for a local polynomial smoother, a parameter that controls the trade-off between a smooth (high-bias) fit and a wiggly (high-variance) one. This advanced exercise requires you to generalize the concept of parameter count to *effective degrees of freedom* and demonstrates how the foundational principles of $AIC$ extend to more sophisticated machine learning models [@problem_id:3098018].", "problem": "Consider the task of bandwidth selection in local polynomial regression under the assumption of independent Gaussian noise. You are required to derive and implement a criterion based on the Akaike information criterion (AIC), starting from first principles, to choose an appropriate bandwidth. The final program must compute the AIC for several candidate bandwidths and select the bandwidth that minimizes this criterion, using the effective degrees of freedom defined via the trace of the smoother matrix.\n\nYou will work with local polynomial regression (Locally Estimated Scatterplot Smoothing (LOESS)) of degree $1$ on one-dimensional inputs. At each evaluation point $x_i$, fit a weighted least squares polynomial model using a Gaussian kernel weight $K(u) = \\exp\\!\\left(-\\frac{u^2}{2}\\right)$, and a bandwidth parameter $h  0$. The weights for data point $x_j$ when fitting at $x_i$ are $w_{ij} = \\exp\\!\\left(-\\frac{(x_j - x_i)^2}{2 h^2}\\right)$. The local design matrix at $x_i$ is $X_i \\in \\mathbb{R}^{n \\times 2}$ with columns $[1, (x_j - x_i)]$, and the diagonal weight matrix is $W_i \\in \\mathbb{R}^{n \\times n}$ with entries $w_{ij}$. The locally fitted value at $x_i$ is the intercept of the local weighted least squares solution. The resulting estimator across all $x_i$ can be written as a linear smoother $\\hat{\\mathbf{y}} = S(h)\\,\\mathbf{y}$, where $S(h) \\in \\mathbb{R}^{n \\times n}$ is the smoother matrix that depends on $h$.\n\nAssume the data are generated as $y_i = f(x_i) + \\varepsilon_i$ with independent $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, where $\\sigma^2  0$ is unknown. Let the residual sum of squares be $\\mathrm{RSS}(h) = \\sum_{i=1}^n (y_i - \\hat{y}_i(h))^2$. The effective degrees of freedom for the linear smoother is defined as $\\mathrm{df}(h) = \\mathrm{tr}\\!\\left(S(h)\\right)$.\n\nStarting from the Gaussian log-likelihood of the data under the model and using the maximum likelihood estimate for the noise variance, derive an expression for an AIC-based selection criterion of the form $AIC(h)$ that aggregates a goodness-of-fit term based on $\\mathrm{RSS}(h)$ and a complexity penalty based on $\\mathrm{df}(h)$. Treat the number of parameters in AIC as the effective degrees of freedom $\\mathrm{df}(h)$ for the smoother. Your derivation must begin from the Gaussian likelihood and avoid using pre-stated shortcut formulas for AIC.\n\nThen implement the resulting $AIC(h)$ and choose the bandwidth that minimizes it for each of the following test cases. In all cases, use local polynomial regression of degree $1$, the Gaussian kernel specified above, and the candidate bandwidth list $[0.03, 0.06, 0.10, 0.20, 0.35, 0.50]$.\n\nTest Suite:\n- Case $1$ (oscillatory signal): $n = 80$, seed $= 123$, input $x_i$ sampled independently from $\\mathrm{Uniform}(0,1)$ and then sorted increasingly. The mean function is $f(x) = \\sin(6\\pi x)$ and the noise standard deviation is $0.2$, so $y_i = f(x_i) + 0.2\\,\\eta_i$ where $\\eta_i \\sim \\mathcal{N}(0,1)$.\n- Case $2$ (approximately linear): $n = 70$, seed $= 456$, input $x_i$ sampled independently from $\\mathrm{Uniform}(0,1)$ and then sorted increasingly. The mean function is $f(x) = 0.5 + 2 x$ and the noise standard deviation is $0.15$, so $y_i = f(x_i) + 0.15\\,\\eta_i$ where $\\eta_i \\sim \\mathcal{N}(0,1)$.\n- Case $3$ (approximately constant): $n = 60$, seed $= 789$, input $x_i$ sampled independently from $\\mathrm{Uniform}(0,1)$ and then sorted increasingly. The mean function is $f(x) = 1.0$ and the noise standard deviation is $0.25$, so $y_i = f(x_i) + 0.25\\,\\eta_i$ where $\\eta_i \\sim \\mathcal{N}(0,1)$.\n\nRequirements:\n- Construct the smoother matrix $S(h)$ by explicitly forming, for each $x_i$, the local design matrix $X_i$ and weights $W_i$, and using weighted least squares to obtain the linear mapping from $\\mathbf{y}$ to $\\hat{y}_i$. Specifically, if $A_i = X_i^\\top W_i X_i$ and $B_i = X_i^\\top W_i$, then the $i$-th row of $S(h)$ must be given by the first row of $A_i^{-1} B_i$. Use a numerically stable pseudoinverse in computations to handle potential ill-conditioning near boundaries.\n- For each candidate bandwidth $h$, compute $\\hat{\\mathbf{y}}(h)$, $\\mathrm{RSS}(h)$, $\\mathrm{df}(h)$, and then the derived $AIC(h)$, and select the $h$ that minimizes $AIC(h)$.\n- Final Output Format: Your program should produce a single line of output containing the selected bandwidths for the three cases as a comma-separated list enclosed in square brackets. Each bandwidth must be printed to three decimal places (for example, $[0.200,0.100,0.500]$). No other text should be printed.\n\nAll answers must be unitless numbers. The final outputs are floats. Ensure your implementation and derivation are consistent and scientifically sound for the given assumptions.", "solution": "The problem is assessed as valid. It is scientifically grounded in statistical learning theory, well-posed with a clear objective and sufficient data, and free from any listed invalidity flaws.\n\n### Derivation of the AIC-based Criterion\n\nWe begin by establishing the statistical framework. The model for the data is given by $y_i = f(x_i) + \\varepsilon_i$ for $i=1, \\dots, n$, where the errors $\\varepsilon_i$ are independent and identically distributed according to a Gaussian distribution, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. In vector form, this is $\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\sigma^2 I_n)$, where $\\boldsymbol{\\mu}$ is the vector of true means $f(x_i)$.\n\nThe local polynomial regression produces a fitted vector $\\hat{\\mathbf{y}}(h) = S(h)\\mathbf{y}$, which serves as our estimate for the mean vector $\\boldsymbol{\\mu}$. The likelihood of observing the data $\\mathbf{y}$ given the fitted values $\\hat{\\mathbf{y}}(h)$ and the noise variance $\\sigma^2$ is the product of the individual Gaussian probability densities:\n$$ L(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\hat{y}_i(h))^2}{2\\sigma^2}\\right) $$\nThe log-likelihood function, denoted by $\\ell$, is therefore:\n$$ \\ell(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma^2) - \\frac{(y_i - \\hat{y}_i(h))^2}{2\\sigma^2} \\right) $$\nThis can be written more compactly using the residual sum of squares, $\\mathrm{RSS}(h) = \\sum_{i=1}^n (y_i - \\hat{y}_i(h))^2$:\n$$ \\ell(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{\\mathrm{RSS}(h)}{2\\sigma^2} $$\nTo find the maximized log-likelihood for a fixed bandwidth $h$, we first find the maximum likelihood estimate (MLE) of the unknown variance $\\sigma^2$. We differentiate $\\ell$ with respect to $\\sigma^2$ and set the result to zero:\n$$ \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\mathrm{RSS}(h)}{2(\\sigma^2)^2} = 0 $$\nSolving for $\\sigma^2$ gives the MLE:\n$$ \\hat{\\sigma}^2_{\\mathrm{ML}} = \\frac{\\mathrm{RSS}(h)}{n} $$\nSubstituting this estimate back into the log-likelihood function yields the maximized log-likelihood for a given $h$:\n$$ \\ell_{\\max}(h) = \\ell(\\hat{\\mathbf{y}}(h), \\hat{\\sigma}^2_{\\mathrm{ML}} | \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right) - \\frac{\\mathrm{RSS}(h)}{2\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right)} $$\n$$ \\ell_{\\max}(h) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right) - \\frac{n}{2} $$\n$$ \\ell_{\\max}(h) = -\\frac{n}{2}\\left(\\log(2\\pi) + 1\\right) - \\frac{n}{2}\\log(\\mathrm{RSS}(h)) + \\frac{n}{2}\\log(n) $$\n\nThe Akaike Information Criterion (AIC) is defined as:\n$$ \\mathrm{AIC} = -2 \\times (\\text{maximized log-likelihood}) + 2 \\times (\\text{number of parameters}) $$\nThe problem specifies that the effective number of parameters for the smoother is its effective degrees of freedom, $k = \\mathrm{df}(h) = \\mathrm{tr}(S(h))$. Applying the definition, we get:\n$$ \\mathrm{AIC}(h) = -2 \\left( -\\frac{n}{2}\\left(\\log(2\\pi) + 1\\right) - \\frac{n}{2}\\log(\\mathrm{RSS}(h)) + \\frac{n}{2}\\log(n) \\right) + 2 \\, \\mathrm{df}(h) $$\n$$ \\mathrm{AIC}(h) = n\\left(\\log(2\\pi) + 1\\right) + n\\log(\\mathrm{RSS}(h)) - n\\log(n) + 2 \\, \\mathrm{df}(h) $$\nFor the purpose of selecting the best bandwidth $h$ from a set of candidates, we need only consider the terms that depend on $h$. The terms $n(\\log(2\\pi) + 1)$ and $-n\\log(n)$ are constant across all candidate models. Dropping these constant terms, we obtain a criterion that is proportional to the full AIC and yields the same ranking of models:\n$$ \\mathrm{AIC_{sel}}(h) = n\\log(\\mathrm{RSS}(h)) + 2 \\, \\mathrm{df}(h) $$\nThis is the criterion we will implement to select the optimal bandwidth $h$. The bandwidth that minimizes this value will be chosen.\n\n### Implementation Strategy\n\nFor each test case and for each candidate bandwidth $h$, we will perform the following steps:\n\n1.  **Data Generation**: Generate the input vector $\\mathbf{x}$ and response vector $\\mathbf{y}$ of size $n$ according to the specified data generating process, including the random seed for reproducibility.\n\n2.  **Smoother Matrix Construction**: Construct the $n \\times n$ smoother matrix $S(h)$. This is done row by row. For each target point $x_i$ (where $i \\in \\{1, \\dots, n\\}$):\n    a.  Compute the weights $w_{ij} = \\exp\\left(-\\frac{(x_j - x_i)^2}{2 h^2}\\right)$ for all $j \\in \\{1, \\dots, n\\}$.\n    b.  Construct the local design matrix $X_i \\in \\mathbb{R}^{n \\times 2}$ whose rows are $[1, (x_j - x_i)]$.\n    c.  As per the problem, calculate $A_i = X_i^\\top W_i X_i$ and $B_i = X_i^\\top W_i$, where $W_i$ is the diagonal matrix of weights $w_{ij}$. Numerically, this is done efficiently by broadcasting the weight vector.\n    d.  The $i$-th row of $S(h)$, denoted $s_i^\\top$, is the first row of $A_i^{-1} B_i$. A pseudoinverse `np.linalg.pinv` is used for $A_i^{-1}$ to ensure numerical stability.\n\n3.  **Criterion Calculation**:\n    a.  Calculate the effective degrees of freedom, $\\mathrm{df}(h) = \\mathrm{tr}(S(h))$.\n    b.  Compute the fitted values $\\hat{\\mathbf{y}}(h) = S(h) \\mathbf{y}$.\n    c.  Compute the residual sum of squares, $\\mathrm{RSS}(h) = \\sum_{j=1}^n (y_j - \\hat{y}_j(h))^2$.\n    d.  Finally, compute the selection criterion value $\\mathrm{AIC_{sel}}(h) = n\\log(\\mathrm{RSS}(h)) + 2 \\, \\mathrm{df}(h)$.\n\n4.  **Bandwidth Selection**: After computing $\\mathrm{AIC_{sel}}(h)$ for all candidate bandwidths, the bandwidth $h$ that yields the minimum $\\mathrm{AIC_{sel}}$ value is selected as the optimal one for that test case.\n\nThis procedure is repeated for all three test cases, and the selected bandwidths are collected for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an AIC-based criterion for bandwidth selection\n    in local polynomial regression.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (oscillatory signal)\n        {\"n\": 80, \"seed\": 123, \"f\": lambda x: np.sin(6 * np.pi * x), \"sigma\": 0.2},\n        # Case 2 (approximately linear)\n        {\"n\": 70, \"seed\": 456, \"f\": lambda x: 0.5 + 2 * x, \"sigma\": 0.15},\n        # Case 3 (approximately constant)\n        {\"n\": 60, \"seed\": 789, \"f\": lambda x: np.ones_like(x), \"sigma\": 0.25},\n    ]\n\n    h_candidates = [0.03, 0.06, 0.10, 0.20, 0.35, 0.50]\n    \n    selected_bandwidths = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        seed = case[\"seed\"]\n        f = case[\"f\"]\n        sigma = case[\"sigma\"]\n\n        # Generate data\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(0, 1, n)\n        x = np.sort(x)\n        eta = rng.normal(0, 1, n)\n        y = f(x) + sigma * eta\n\n        aic_scores = []\n        \n        for h in h_candidates:\n            # Construct the smoother matrix S\n            S = np.zeros((n, n))\n            for i in range(n):\n                # Target point for the local fit\n                x_i = x[i]\n                \n                # Distances from the target point\n                d = x - x_i\n                \n                # Gaussian kernel weights\n                weights = np.exp(-d**2 / (2 * h**2))\n                \n                # Local design matrix X_i\n                X_i = np.vstack([np.ones(n), d]).T\n                \n                # Calculate A_i = X_i^T W_i X_i and B_i = X_i^T W_i efficiently\n                # (X_i.T * weights) performs X_i.T @ diag(weights)\n                A_i = (X_i.T * weights) @ X_i\n                B_i = X_i.T * weights\n                \n                # Use pseudoinverse for numerical stability\n                A_i_inv = np.linalg.pinv(A_i)\n                \n                # The i-th row of S is the first row of (A_i^-1 B_i)\n                s_i_row = (A_i_inv @ B_i)[0, :]\n                S[i, :] = s_i_row\n                \n            # Calculate effective degrees of freedom\n            df_h = np.trace(S)\n            \n            # Calculate fitted values and RSS\n            y_hat = S @ y\n            rss_h = np.sum((y - y_hat)**2)\n            \n            # Calculate AIC-based criterion\n            # AIC_sel(h) = n*log(RSS(h)) + 2*df(h)\n            if rss_h = 0: # Avoid log(0) or log(-)\n                aic_h = np.inf\n            else:\n                aic_h = n * np.log(rss_h) + 2 * df_h\n            \n            aic_scores.append(aic_h)\n            \n        # Select the bandwidth that minimizes AIC\n        best_h_index = np.argmin(aic_scores)\n        best_h = h_candidates[best_h_index]\n        selected_bandwidths.append(best_h)\n\n    # Format the final output\n    formatted_results = [f\"{bw:.3f}\" for bw in selected_bandwidths]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3098018"}]}