{"hands_on_practices": [{"introduction": "Before delving into complex scenarios, it is crucial to master the fundamental calculation of the adjusted coefficient of determination, or $\\bar{R}^2$. This first practice problem provides a direct application of the $\\bar{R}^2$ formula, ensuring you can correctly use the key inputs: the residual sum of squares ($\\mathrm{RSS}$), total sum of squares ($\\mathrm{TSS}$), sample size ($n$), and number of predictors ($p$). By working through this foundational exercise [@problem_id:1031765], you will build the computational skills necessary for the more conceptual applications that follow.", "problem": "In a multiple linear regression analysis with $n=25$ observations and $p=4$ predictor variables, the residual sum of squares (RSS) is given as 15.8 and the total sum of squares (TSS) is 120.5. Compute the adjusted coefficient of determination, denoted by $\\bar{R}^2$.  \n\nThe adjusted $R^2$ accounts for the number of predictors and is defined as:  \n$$  \n\\bar{R}^2 = 1 - \\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}  \n$$  \nProvide the exact value of $\\bar{R}^2$.", "solution": "1. Start with the definition:\n$$\n\\bar R^2 = 1 - \\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}\n$$\n2. Substitute $n=25$, $p=4$, $\\mathrm{RSS}=15.8$, $\\mathrm{TSS}=120.5$. Then the degrees of freedom are:\n$$\nn-p-1 = 25-4-1 = 20,\\qquad n-1 = 24.\n$$  \n3. Compute each mean square:  \n$$\n\\frac{\\mathrm{RSS}}{n-p-1}\n= \\frac{15.8}{20}\n= \\frac{79/5}{20}\n= \\frac{79}{100},\n\\quad\n\\frac{\\mathrm{TSS}}{n-1}\n= \\frac{120.5}{24}\n= \\frac{241/2}{24}\n= \\frac{241}{48}.\n$$  \n4. Form their ratio:  \n$$\n\\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}\n= \\frac{79/100}{241/48}\n= \\frac{79\\cdot48}{100\\cdot241}\n= \\frac{3792}{24100}\n= \\frac{948}{6025}.\n$$  \n5. Thus, the adjusted R-squared is:\n$$\n\\bar R^2\n=1 - \\frac{948}{6025}\n= \\frac{6025 - 948}{6025}\n= \\frac{5077}{6025}.\n$$", "answer": "$$\\boxed{5077/6025}$$", "id": "1031765"}, {"introduction": "The standard coefficient of determination, $R^2$, has a significant drawback: it never decreases when you add more predictors to a model, even if those predictors are pure noise. This practice [@problem_id:3152035] provides a hands-on simulation to demonstrate this phenomenon and highlight the superiority of adjusted $R^2$ for model comparison. By adding irrelevant \"noise\" variables to a regression, you will observe how $\\bar{R}^2$ penalizes this unnecessary complexity, providing a more honest assessment of model performance.", "problem": "Consider the multiple linear regression model with an intercept, where the response vector $y \\in \\mathbb{R}^n$ is generated by $y = X_{\\text{signal}}\\beta_{\\text{signal}} + \\varepsilon$, and the noise vector $\\varepsilon \\in \\mathbb{R}^n$ is independent and identically distributed Gaussian with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. The predictor matrix is decomposed as $X = [X_{\\text{signal}}, X_{\\text{noise}}]$, where $X_{\\text{signal}} \\in \\mathbb{R}^{n \\times p_{\\text{signal}}}$ contains the true signal predictors and $X_{\\text{noise}} \\in \\mathbb{R}^{n \\times q_{\\text{noise}}}$ contains additional noise predictors that are independent of both $X_{\\text{signal}}$ and $y$.\n\nStarting from the core definitions of the multiple linear regression model and Ordinary Least Squares (OLS), implement the following procedure to compare the coefficient of determination (R-squared) and adjusted R-squared under increasing number of predictors $p$, and validate the comparison via cross-validation error:\n\n- For each test case in the test suite below, generate data according to the specified parameters by drawing each column of $X_{\\text{signal}}$ and $X_{\\text{noise}}$ independently from a standard normal distribution, and drawing $\\varepsilon$ independently from a Gaussian distribution with variance $\\sigma^2$. Construct $y$ using the specified $\\beta_{\\text{signal}}$ and include an intercept term in all fitted models.\n- Fit two OLS models: a \"signal-only\" model using $X_{\\text{signal}}$ and an \"all-predictors\" model using both $X_{\\text{signal}}$ and $X_{\\text{noise}}$. Both models must include an intercept.\n- For each fitted model, compute the training coefficient of determination $R^2$ and the adjusted $R^2$ using their standard definitions arising from the decomposition of total variance and degrees-of-freedom adjustment for the intercept model.\n- Estimate the out-of-sample performance using $K$-fold Cross-Validation (CV), defined as partitioning the indices $\\{1,\\dots,n\\}$ into $K$ approximately equal folds, fitting on $K-1$ folds, predicting on the held-out fold, and averaging the mean squared error (MSE) across folds. Report the average CV MSE for each model.\n\nYour program must evaluate, for each test case, the following three boolean statements comparing the \"all-predictors\" model against the \"signal-only\" model:\n- $R^2$ of the all-predictors model is greater than or equal to the $R^2$ of the signal-only model.\n- Adjusted $R^2$ of the all-predictors model is strictly less than the adjusted $R^2$ of the signal-only model.\n- $K$-fold CV MSE of the all-predictors model is strictly greater than the $K$-fold CV MSE of the signal-only model.\n\nTest suite parameters (use the exact values and seeds provided):\n- Test case $1$: $n=400$, $p_{\\text{signal}}=5$, $q_{\\text{noise}}=40$, $\\beta_{\\text{signal}}=[2.0,-1.0,1.5,0.5,-0.8]$, $\\sigma=1.0$, $K=5$, random seed $42$.\n- Test case $2$: $n=120$, $p_{\\text{signal}}=3$, $q_{\\text{noise}}=80$, $\\beta_{\\text{signal}}=[1.8,-0.7,0.9]$, $\\sigma=1.2$, $K=5$, random seed $7$.\n- Test case $3$: $n=200$, $p_{\\text{signal}}=10$, $q_{\\text{noise}}=20$, $\\beta_{\\text{signal}}=[1.3,-0.6,0.7,0.5,-1.1,0.9,-0.4,0.3,-0.2,0.8]$, $\\sigma=2.0$, $K=5$, random seed $99$.\n\nAngle units and physical units are not applicable in this purely statistical problem. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of three booleans in the order described above. For example, the output format must be exactly of the form $[[b_{11},b_{12},b_{13}],[b_{21},b_{22},b_{23}],[b_{31},b_{32},b_{33}]]$ with $b_{ij} \\in \\{\\text{True},\\text{False}\\}$.", "solution": "The user wants to solve a statistical problem.\nHere's a plan:\n1.  **Validate the Problem Statement**: The problem is a standard statistical simulation study to demonstrate overfitting and the properties of different model selection metrics in multiple linear regression. It is scientifically sound, well-posed, objective, and complete. All parameters and procedures are clearly defined. The verdict is **valid**.\n2.  **Explain the Principles**: The solution will be structured around the core principles of Ordinary Least Squares (OLS), the coefficient of determination ($R^2$), the adjusted $R^2$, and K-fold cross-validation.\n3.  **Step-by-Step Logic**: Detail the algorithm for data generation, model fitting, metric calculation, and comparison as specified in the problem.\n4.  **Implement in Code**: A Python script using `numpy` will be developed to execute the procedure for the given test cases.\n\n### Principles and Methodology\n\nThis problem investigates the consequences of including irrelevant predictors in a multiple linear regression model. We compare a \"true\" model containing only signal predictors with an \"overfitted\" model that also includes noise predictors. The comparison is performed using three key metrics: the coefficient of determination ($R^2$), the adjusted $R^2$, and the cross-validated mean squared error (CV MSE).\n\n#### 1. The Ordinary Least Squares (OLS) Model\n\nThe multiple linear regression model aims to describe a response variable $y \\in \\mathbb{R}^n$ as a linear combination of predictor variables, contained in a design matrix $X \\in \\mathbb{R}^{n \\times p}$. For a model with an intercept, the equation is:\n$$ y = \\beta_0 + X\\beta + \\varepsilon $$\nwhere $\\beta_0$ is the intercept, $\\beta \\in \\mathbb{R}^p$ is the vector of predictor coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the error vector. This can be written more compactly by augmenting the design matrix with a column of ones, $X_{\\text{aug}} = [\\mathbf{1}_n, X]$, and the coefficient vector as $\\beta_{\\text{aug}} = [\\beta_0, \\beta^T]^T$. The model becomes $y = X_{\\text{aug}}\\beta_{\\text{aug}} + \\varepsilon$.\n\nThe OLS method finds the coefficient estimates $\\hat{\\beta}_{\\text{aug}}$ that minimize the Residual Sum of Squares (RSS), defined as:\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = (y - X_{\\text{aug}}\\hat{\\beta}_{\\text{aug}})^T (y - X_{\\text{aug}}\\hat{\\beta}_{\\text{aug}}) $$\nThe solution to this minimization problem is given by the normal equations:\n$$ \\hat{\\beta}_{\\text{aug}} = (X_{\\text{aug}}^T X_{\\text{aug}})^{-1} X_{\\text{aug}}^T y $$\nThis procedure will be applied to fit two models:\n1.  **Signal-only model**: Using the design matrix $X_{\\text{signal}} \\in \\mathbb{R}^{n \\times p_{\\text{signal}}}$.\n2.  **All-predictors model**: Using the design matrix $X = [X_{\\text{signal}}, X_{\\text{noise}}] \\in \\mathbb{R}^{n \\times (p_{\\text{signal}} + q_{\\text{noise}})}$.\n\n#### 2. Coefficient of Determination ($R^2$)\n\n$R^2$ measures the proportion of the variance in the response variable $y$ that is predictable from the predictor variables. It is defined based on the decomposition of the Total Sum of Squares (TSS):\n$$ \\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $$\nwhere $\\bar{y}$ is the mean of $y$. The decomposition is $\\text{TSS} = \\text{ESS} + \\text{RSS}$, where ESS is the Explained Sum of Squares. The $R^2$ is:\n$$ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\nA key property of $R^2$ is that it is non-decreasing when new predictors are added to the model. This is because adding a predictor can only decrease (or keep constant) the RSS, the minimized sum of squared errors. Therefore, we expect the $R^2$ of the \"all-predictors\" model to be greater than or equal to the $R^2$ of the \"signal-only\" model. This directly corresponds to the first boolean check: $R^2_{\\text{all}} \\ge R^2_{\\text{signal}}$.\n\n#### 3. Adjusted $R^2$\n\nThe non-decreasing nature of $R^2$ makes it a poor metric for comparing models with different numbers of predictors. A model can achieve a higher $R^2$ simply by including more variables, even if they are pure noise. The adjusted $R^2$ corrects for this by penalizing the score for each added predictor. It is defined as:\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{RSS} / (n - p - 1)}{\\text{TSS} / (n - 1)} $$\nHere, $p$ is the number of predictors (excluding the intercept). The term $\\text{RSS} / (n - p - 1)$ is an unbiased estimate of the error variance $\\sigma^2$. When a useless predictor is added, $p$ increases by $1$, which increases the penalty. If the reduction in RSS is not sufficient to outweigh this penalty, the adjusted $R^2$ will decrease. Since the \"all-predictors\" model adds $q_{\\text{noise}}$ purely random variables, their inclusion is not justified, and we expect the adjusted $R^2$ to be lower than that of the simpler \"signal-only\" model. This corresponds to the second boolean check: $R^2_{\\text{adj, all}} < R^2_{\\text{adj, signal}}$.\n\n#### 4. K-Fold Cross-Validation (CV)\n\nWhile adjusted $R^2$ provides a better in-sample comparison, a more robust method for evaluating a model's predictive power is to estimate its performance on unseen data. K-fold CV is a standard technique for this. The dataset is partitioned into $K$ equal-sized folds. For each fold $k \\in \\{1, \\dots, K\\}$, the model is trained on the other $K-1$ folds and then used to predict the responses for the held-out fold $k$. The Mean Squared Error (MSE) is calculated for each fold:\n$$ \\text{MSE}_k = \\frac{1}{n_k} \\sum_{i \\in \\text{fold } k} (y_i - \\hat{y}_i)^2 $$\nwhere $n_k$ is the number of observations in fold $k$. The overall CV score is the average of these MSEs:\n$$ \\text{MSE}_{\\text{CV}} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{MSE}_k $$\nBy including noise predictors, the \"all-predictors\" model tends to overfit the training data. It learns spurious correlations present in the specific training-set sample. When this overfitted model is applied to the unseen test fold, its performance is poor, resulting in a higher MSE. Therefore, we expect the CV MSE of the \"all-predictors\" model to be greater than that of the \"signal-only\" model, which corresponds to the third boolean check: $\\text{MSE}_{\\text{CV, all}} > \\text{MSE}_{\\text{CV, signal}}$.\n\n### Implementation Strategy\n\nThe following algorithm will be implemented for each test case:\n1.  **Initialization**: Set the random seed for reproducibility.\n2.  **Data Generation**: Generate $X_{\\text{signal}}$, $X_{\\text{noise}}$, and $\\varepsilon$ from their respective normal distributions using the specified parameters ($n$, $p_{\\text{signal}}$, $q_{\\text{noise}}$, $\\sigma$). Construct the response vector $y = X_{\\text{signal}}\\beta_{\\text{signal}} + \\varepsilon$.\n3.  **Fold Creation**: Partition the data indices $\\{0, \\dots, n-1\\}$ into $K$ folds. This partition will be used for both models to ensure a fair comparison.\n4.  **Model Analysis**:\n    *   For the **\"signal-only\" model** (using $X_1 = X_{\\text{signal}}$):\n        *   Fit the OLS model on the full dataset to get $\\hat{\\beta}_1$.\n        *   Calculate $R^2_1$ and $R^2_{\\text{adj}, 1}$.\n        *   Perform $K$-fold CV to compute $\\text{MSE}_{\\text{CV},1}$.\n    *   For the **\"all-predictors\" model** (using $X_2 = [X_{\\text{signal}}, X_{\\text{noise}}]$):\n        *   Fit the OLS model on the full dataset to get $\\hat{\\beta}_2$.\n        *   Calculate $R^2_2$ and $R^2_{\\text{adj}, 2}$.\n        *   Perform $K$-fold CV to compute $\\text{MSE}_{\\text{CV},2}$.\n5.  **Evaluate Boolean Conditions**:\n    *   Check if $R^2_2 \\ge R^2_1$.\n    *   Check if $R^2_{\\text{adj}, 2} < R^2_{\\text{adj}, 1}$.\n    *   Check if $\\text{MSE}_{\\text{CV}, 2} > \\text{MSE}_{\\text{CV}, 1}$.\n6.  **Store and Report**: Store the three boolean results and format them as required for the final output.", "answer": "```python\nimport numpy as np\n\ndef ols_fit(X, y):\n    \"\"\"\n    Fits an Ordinary Least Squares model with an intercept.\n    \n    Args:\n        X (np.ndarray): Predictor matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        \n    Returns:\n        tuple: (beta_hat, X_aug) where beta_hat are the fitted coefficients\n               (including intercept) and X_aug is the augmented design matrix.\n    \"\"\"\n    n_samples = X.shape[0]\n    X_aug = np.hstack([np.ones((n_samples, 1)), X])\n    \n    try:\n        # More stable than inv(X.T @ X) @ X.T @ y\n        beta_hat = np.linalg.solve(X_aug.T @ X_aug, X_aug.T @ y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular\n        beta_hat = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y\n        \n    return beta_hat, X_aug\n\ndef calculate_metrics(X_aug, y, beta_hat):\n    \"\"\"\n    Calculates R-squared and Adjusted R-squared.\n    \n    Args:\n        X_aug (np.ndarray): Augmented design matrix (with intercept).\n        y (np.ndarray): Response vector.\n        beta_hat (np.ndarray): Fitted coefficients.\n        \n    Returns:\n        tuple: (r_squared, adj_r_squared).\n    \"\"\"\n    n_samples, n_params = X_aug.shape\n    p_predictors = n_params - 1\n    \n    y_hat = X_aug @ beta_hat\n    rss = np.sum((y - y_hat)**2)\n    tss = np.sum((y - np.mean(y))**2)\n    \n    if tss == 0:\n        # Edge case: y is constant. R-squared is not well-defined.\n        # Fits will be perfect if model is just an intercept, otherwise RSS > 0.\n        return (1.0 if rss == 0 else 0.0), (1.0 if rss == 0 else 0.0)\n\n    r_squared = 1 - rss / tss\n    \n    # Degrees of freedom for error and total\n    df_err = n_samples - p_predictors - 1\n    df_tot = n_samples - 1\n    \n    if df_err = 0:\n        # If p >= n-1, adjusted R^2 can be negative or undefined.\n        # This problem's test cases avoid this.\n        adj_r_squared = r_squared\n    else:\n        adj_r_squared = 1 - (rss / df_err) / (tss / df_tot)\n        \n    return r_squared, adj_r_squared\n\ndef k_fold_cv_mse(X, y, K, folds):\n    \"\"\"\n    Calculates K-fold cross-validation Mean Squared Error.\n    \n    Args:\n        X (np.ndarray): Predictor matrix.\n        y (np.ndarray): Response vector.\n        K (int): Number of folds.\n        folds (list of np.ndarray): List of arrays, each containing indices for a fold.\n\n    Returns:\n        float: The average MSE across all K folds.\n    \"\"\"\n    mse_scores = []\n    n_samples = len(y)\n    \n    for k in range(K):\n        test_indices = folds[k]\n        all_indices = np.arange(n_samples)\n        train_indices = np.setdiff1d(all_indices, test_indices)\n        \n        X_train, y_train = X[train_indices], y[train_indices]\n        X_test, y_test = X[test_indices], y[test_indices]\n        \n        # Fit model on training data\n        beta_hat_train, _ = ols_fit(X_train, y_train)\n        \n        # Predict on test data\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        y_pred = X_test_aug @ beta_hat_train\n        \n        # Calculate MSE for the fold\n        mse = np.mean((y_test - y_pred)**2)\n        mse_scores.append(mse)\n        \n    return np.mean(mse_scores)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 400, 'p_signal': 5, 'q_noise': 40, 'beta_signal': [2.0, -1.0, 1.5, 0.5, -0.8], 'sigma': 1.0, 'K': 5, 'seed': 42},\n        {'n': 120, 'p_signal': 3, 'q_noise': 80, 'beta_signal': [1.8, -0.7, 0.9], 'sigma': 1.2, 'K': 5, 'seed': 7},\n        {'n': 200, 'p_signal': 10, 'q_noise': 20, 'beta_signal': [1.3, -0.6, 0.7, 0.5, -1.1, 0.9, -0.4, 0.3, -0.2, 0.8], 'sigma': 2.0, 'K': 5, 'seed': 99},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case['n']\n        p_signal = case['p_signal']\n        q_noise = case['q_noise']\n        beta_signal = np.array(case['beta_signal'])\n        sigma = case['sigma']\n        K = case['K']\n        seed = case['seed']\n        \n        np.random.seed(seed)\n        \n        # Generate data\n        X_signal = np.random.randn(n, p_signal)\n        X_noise = np.random.randn(n, q_noise)\n        epsilon = np.random.randn(n) * sigma\n        y = X_signal @ beta_signal + epsilon\n        \n        # Define predictor sets\n        X1 = X_signal  # Signal-only\n        X2 = np.hstack([X_signal, X_noise]) # All predictors\n        \n        # --- Model 1: Signal-only ---\n        beta_hat1, X1_aug = ols_fit(X1, y)\n        r2_1, adj_r2_1 = calculate_metrics(X1_aug, y, beta_hat1)\n\n        # --- Model 2: All predictors ---\n        beta_hat2, X2_aug = ols_fit(X2, y)\n        r2_2, adj_r2_2 = calculate_metrics(X2_aug, y, beta_hat2)\n        \n        # --- K-Fold CV ---\n        # Create folds once for fair comparison\n        indices = np.arange(n)\n        np.random.shuffle(indices)\n        folds = np.array_split(indices, K)\n        \n        cv_mse_1 = k_fold_cv_mse(X1, y, K, folds)\n        cv_mse_2 = k_fold_cv_mse(X2, y, K, folds)\n\n        # --- Evaluate boolean statements ---\n        bool1 = r2_2 >= r2_1\n        bool2 = adj_r2_2  adj_r2_1\n        bool3 = cv_mse_2 > cv_mse_1\n        \n        all_results.append([bool1, bool2, bool3])\n\n    # Format the final output string\n    inner_parts = [f\"[{','.join(map(lambda b: str(b), res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3152035"}, {"introduction": "One of the most powerful applications of adjusted $R^2$ is in model selection, particularly for choosing the right level of model complexity. This exercise [@problem_id:3096432] guides you through the process of fitting polynomial regression models of increasing degrees to a dataset. You will see firsthand how $\\bar{R}^2$ typically increases as the model captures more of the true signal, but then decreases as the model starts overfitting to noise, allowing you to identify the optimal polynomial degree.", "problem": "Consider a univariate polynomial regression model of degree $d$ fit by Ordinary Least Squares (OLS). Let the sample size be $n$. Such a model has $p=d$ predictor variables (corresponding to powers $x^1, \\dots, x^d$), and a total of $p+1$ parameters including the intercept. Let the response vector be $y \\in \\mathbb{R}^n$, and the fitted values be $\\hat{y} \\in \\mathbb{R}^n$. Define the residual sum of squares $\\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, the total sum of squares $\\mathrm{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2$, and the coefficient of determination (R-squared) $R^2 = 1 - \\mathrm{RSS}/\\mathrm{TSS}$. The unbiased estimator of the noise variance under the linear model with $p$ predictors and an intercept uses Degrees of Freedom (DoF) and is given by $\\hat{\\sigma}^2 = \\mathrm{RSS}/(n - p - 1)$ when $n - p - 1 > 0$. Angles used in any trigonometric functions (e.g., sine and cosine) are to be interpreted in radians.\n\nStarting only from these base definitions and facts, you must:\n\n- Derive a formula for the adjusted R-squared $\\bar{R}^2$ that properly accounts for the sample size $n$ and the number of predictors $p$, using the variance decomposition logic implicit in the above quantities.\n- Explain, in terms of $n$ and $p$, why $R^2$ is non-decreasing with increasing $d$ while $\\bar{R}^2$ may decrease if added regressors fail to explain signal beyond noise.\n- Implement a program that, for polynomial degrees $d \\in \\{0, 1, \\dots, d_{\\max}\\}$ with the constraint $d + 1  n$, computes $\\bar{R}^2$ for each $d$, identifies the degree $d^\\star$ that maximizes $\\bar{R}^2$, checks whether the sequence $\\{\\bar{R}^2(d)\\}$ is monotonically non-decreasing in $d$, and flags overfitting when there exists $d > d^\\star$ such that $\\bar{R}^2(d)  \\bar{R}^2(d^\\star)$.\n\nData generation and evaluation must be deterministic and scientifically realistic. For each test case, use equally spaced inputs $x_i$ on the interval $[-1, 1]$ defined by\n$$\nx_i = -1 + \\frac{2(i - 1)}{n - 1}, \\quad i = 1, 2, \\dots, n,\n$$\nand construct the response as a polynomial signal plus structured high-frequency noise:\n$$\ny_i = \\sum_{j=0}^{k} \\beta_j x_i^j \\;+\\; \\eta\\left(\\sin\\left(\\frac{2\\pi i}{n}\\right) + \\frac{1}{2}\\cos\\left(\\frac{6\\pi i}{n}\\right)\\right),\n$$\nwhere $k$ is the true polynomial degree, $\\beta_j$ are fixed coefficients, and $\\eta$ is a noise scale. All angles are in radians.\n\nYour program must:\n- For each test case, compute $\\bar{R}^2(d)$ for all valid degrees $d \\in \\{0, 1, \\dots, d_{\\max}\\}$ with $d + 1  n$.\n- Identify $d^\\star$ (the smallest degree attaining the maximum) and the corresponding $\\bar{R}^2(d^\\star)$.\n- Determine two boolean indicators: whether $\\{\\bar{R}^2(d)\\}$ is monotonically non-decreasing in $d$, and whether overfitting is detected as defined above.\n- Output, for each test case, a list containing $d^\\star$, the value $\\bar{R}^2(d^\\star)$ rounded to $6$ decimal places, the monotonicity boolean, the overfitting boolean, and the list of all $\\bar{R}^2(d)$ values (each rounded to $6$ decimal places) across the considered degrees in ascending order.\n\nTest Suite:\nUse the following parameter sets to exercise different regimes of $n$, $d_{\\max}$, signal complexity, and noise level.\n\n- Case A (happy path, moderate $n$, moderate noise):\n  - $n = 50$, $d_{\\max} = 14$, $k = 3$, $\\beta = [0.5, -1.0, 0.8, -0.3]$, $\\eta = 0.2$.\n- Case B (boundary-oriented, small $n$, high noise, degrees near the limit $d + 1  n$):\n  - $n = 12$, $d_{\\max} = 10$, $k = 2$, $\\beta = [1.0, -0.5, 0.25]$, $\\eta = 0.8$.\n- Case C (large $n$, richer true signal, low noise):\n  - $n = 200$, $d_{\\max} = 15$, $k = 5$, $\\beta = [0.2, 0.3, -0.6, 0.5, -0.4, 0.1]$, $\\eta = 0.1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each element is the list\n$$\n\\left[d^\\star, \\; \\text{rounded } \\bar{R}^2(d^\\star), \\; \\text{monotone flag}, \\; \\text{overfitting flag}, \\; \\text{list of rounded } \\bar{R}^2(d) \\text{ for all valid } d\\right],\n$$\nso the overall printed line must look like\n$$\n\\left[\\text{Case A result}, \\text{Case B result}, \\text{Case C result}\\right]\n$$\nfor the provided test suite. All floating-point values must be rounded to $6$ decimal places, and the booleans must be Python boolean literals. No percentage signs are permitted; express all quantities as decimals.", "solution": "The user's request consists of three parts: a derivation, a conceptual explanation, and a computational implementation. I will address these in order, adhering to the specified formatting rules.\n\n### Derivation of Adjusted R-squared ($\\bar{R}^2$)\n\nThe coefficient of determination, $R^2$, is defined as $R^2 = 1 - \\mathrm{RSS}/\\mathrm{TSS}$. This metric can be interpreted as the proportion of the total variance in the response variable $y$ that is explained by the model. However, the quantities $\\mathrm{RSS}$ and $\\mathrm{TSS}$ are not direct unbiased estimators of the underlying variances.\n\nThe adjusted R-squared, $\\bar{R}^2$, improves upon $R^2$ by using unbiased estimators for the variances. The logic stems from the decomposition of variance. The estimate of the error (residual) variance, $\\sigma^2$, is given by $\\hat{\\sigma}^2_{\\text{resid}} = \\mathrm{RSS}/(n-p-1)$, where $n$ is the sample size and $p$ is the number of predictors. The term $n-p-1$ represents the degrees of freedom for the residuals.\n\nSimilarly, an unbiased estimator for the total variance of the response variable $y$ is its sample variance, given by $\\hat{\\sigma}^2_{\\text{total}} = \\mathrm{TSS}/(n-1)$, where $n-1$ are the degrees of freedom. This corresponds to a model with only an intercept.\n\nThe adjusted R-squared is formulated by replacing the ratio of sums of squares in the $R^2$ formula with the ratio of these unbiased variance estimators:\n$$\n\\bar{R}^2 = 1 - \\frac{\\hat{\\sigma}^2_{\\text{resid}}}{\\hat{\\sigma}^2_{\\text{total}}}\n$$\nSubstituting the expressions for the estimators, we get:\n$$\n\\bar{R}^2 = 1 - \\frac{\\mathrm{RSS} / (n-p-1)}{\\mathrm{TSS} / (n-1)}\n$$\nWe can rearrange this formula to express $\\bar{R}^2$ in terms of the original $R^2$. From the definition of $R^2$, we have $\\mathrm{RSS}/\\mathrm{TSS} = 1 - R^2$. Substituting $\\mathrm{RSS} = (1 - R^2)\\mathrm{TSS}$ into the equation for $\\bar{R}^2$:\n$$\n\\bar{R}^2 = 1 - \\frac{(1 - R^2)\\mathrm{TSS} / (n-p-1)}{\\mathrm{TSS} / (n-1)}\n$$\nThe $\\mathrm{TSS}$ term cancels out:\n$$\n\\bar{R}^2 = 1 - \\frac{(1 - R^2) / (n-p-1)}{1 / (n-1)} = 1 - (1 - R^2) \\frac{n-1}{n-p-1}\n$$\nThis is the standard and final formula for the adjusted R-squared, consistent with the definition used throughout the article.\n\n### Explanation of $R^2$ vs. $\\bar{R}^2$ Behavior\n\nThe key difference between $R^2$ and $\\bar{R}^2$ lies in how they behave as the number of predictors, $p$, increases. In the context of polynomial regression, increasing the degree $d$ increases the number of predictors $p=d$.\n\nFor a sequence of nested models, such as polynomial regression where the degree $d$ is increased, the $R^2$ value is guaranteed to be non-decreasing. When a new predictor (e.g., $x^{d+1}$) is added to a model of degree $d$, the Ordinary Least Squares (OLS) fitting procedure minimizes the $\\mathrm{RSS}$. The set of possible functions for the degree $d+1$ model contains the best-fit polynomial from the degree $d$ model as a special case (by setting the coefficient of the highest power to $0$). Therefore, the minimized $\\mathrm{RSS}$ for the more complex model cannot be greater than that for the simpler model: $\\mathrm{RSS}_{d+1} \\le \\mathrm{RSS}_d$. Since $\\mathrm{TSS}$ is constant for a given dataset, and $R^2(d) = 1 - \\mathrm{RSS}_d / \\mathrm{TSS}$, it follows that $R^2(d+1) \\ge R^2(d)$. Thus, $R^2$ will always increase or stay the same as more predictors are added, even if they are irrelevant and only capture random noise.\n\nIn contrast, $\\bar{R}^2$ penalizes the addition of predictors that do not contribute meaningfully to explaining the variance. As shown in the derivation, $\\bar{R}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}$. When a predictor is added, $p$ increases by $1$. This causes the penalty factor, $\\frac{n-1}{n-p-1}$, to increase. For $\\bar{R}^2$ to increase, the decrease in $(1 - R^2)$ (which corresponds to an increase in $R^2$) must be large enough to overcome the increase in the penalty factor. If adding a new predictor results in only a marginal decrease in $\\mathrm{RSS}$ (as would happen if the predictor is merely fitting noise), the increase in the penalty term $\\frac{n-1}{n-p-1}$ will dominate, causing $\\bar{R}^2$ to decrease. This property makes $\\bar{R}^2$ a superior metric for comparing models with different numbers of predictors, as it helps guard against overfitting by indicating when the added complexity is not justified by a sufficient improvement in model fit. A decrease in $\\bar{R}^2$ is a classic sign of overfitting.\n\n### Computational Implementation\n\nThe following Python code implements the required analysis. For each test case, it generates the specified data, then fits polynomial models for degrees $d$ from $0$ up to $d_{\\max}$ (constrained by $d+1  n$). For each fit, it calculates the adjusted R-squared, $\\bar{R}^2$. Finally, it identifies the optimal degree $d^\\star$ that maximizes $\\bar{R}^2$ and determines the monotonicity and overfitting indicators based on the sequence of $\\bar{R}^2$ values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the polynomial regression problem, calculates adjusted R-squared for\n    different degrees, and identifies overfitting characteristics for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path, moderate n, moderate noise)\n        {'n': 50, 'd_max': 14, 'k': 3, 'beta': [0.5, -1.0, 0.8, -0.3], 'eta': 0.2},\n        # Case B (boundary-oriented, small n, high noise)\n        {'n': 12, 'd_max': 10, 'k': 2, 'beta': [1.0, -0.5, 0.25], 'eta': 0.8},\n        # Case C (large n, richer true signal, low noise)\n        {'n': 200, 'd_max': 15, 'k': 5, 'beta': [0.2, 0.3, -0.6, 0.5, -0.4, 0.1], 'eta': 0.1},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, d_max, k, beta, eta = case['n'], case['d_max'], case['k'], case['beta'], case['eta']\n        \n        # 1. Generate data\n        x = np.linspace(-1.0, 1.0, n)\n        \n        # Signal part of the response\n        beta_arr = np.array(beta)\n        true_powers = np.arange(k + 1)\n        # Using broadcasting to compute polynomial signal efficiently\n        x_signal_powers = x[:, np.newaxis] ** true_powers\n        signal = x_signal_powers @ beta_arr\n        \n        # Noise part of the response\n        i_vals = np.arange(1, n + 1)\n        noise = eta * (np.sin(2 * np.pi * i_vals / n) + 0.5 * np.cos(6 * np.pi * i_vals / n))\n        \n        y = signal + noise\n        \n        # 2. Calculate Total Sum of Squares (TSS)\n        y_bar = np.mean(y)\n        TSS = np.sum((y - y_bar)**2)\n        \n        r_bar_sq_list = []\n        valid_degrees = []\n        \n        # 3. Loop through polynomial degrees d\n        for d in range(d_max + 1):\n            p_predictors = d\n            num_params = p_predictors + 1\n            if num_params >= n:\n                break\n            \n            valid_degrees.append(d)\n            \n            # Construct the design matrix X for degree d\n            X = x[:, np.newaxis] ** np.arange(num_params)\n            \n            # Solve OLS using np.linalg.lstsq for numerical stability\n            _, rss_array, _, _ = np.linalg.lstsq(X, y, rcond=None)\n            \n            # The residual sum of squares RSS is the second return value\n            RSS = rss_array[0] if rss_array.size > 0 else 0.0\n\n            # Calculate adjusted R^2\n            if TSS == 0:\n                # This case is unlikely given the problem setup, but safe to handle.\n                # If TSS is 0, y is constant. An intercept-only model fits perfectly (RSS=0).\n                # The metric is ill-defined (0/0), conventionally set to 1 or 0.\n                adj_r2 = 1.0 if RSS == 0 else 0.0\n            else:\n                # Degrees of freedom for residuals and total\n                df_resid = n - num_params\n                df_total = n - 1\n                adj_r2 = 1 - (RSS / df_resid) / (TSS / df_total)\n            \n            r_bar_sq_list.append(adj_r2)\n\n        # 4. Post-processing to find d*, monotonicity, and overfitting flags\n        r_bar_sq_array = np.array(r_bar_sq_list)\n        \n        d_star = -1\n        max_r_bar_sq = -np.inf\n        d_star_idx = -1\n        \n        if r_bar_sq_array.size > 0:\n            d_star_idx = np.argmax(r_bar_sq_array)\n            d_star = valid_degrees[d_star_idx]\n            max_r_bar_sq = r_bar_sq_array[d_star_idx]\n\n        # Check for monotonicity: is the sequence of adj. R^2 values non-decreasing?\n        is_monotonic = all(r_bar_sq_array[i] = r_bar_sq_array[i+1] for i in range(len(r_bar_sq_array) - 1))\n        \n        # Check for overfitting: exists d > d* such that R_bar^2(d)  R_bar^2(d*)?\n        is_overfitting = False\n        if d_star_idx != -1:\n            for i in range(d_star_idx + 1, len(r_bar_sq_array)):\n                if r_bar_sq_array[i]  max_r_bar_sq:\n                    is_overfitting = True\n                    break\n        \n        # 5. Format results for the current test case\n        rounded_max_r_bar_sq = round(max_r_bar_sq, 6)\n        rounded_r_bar_sq_list = [round(val, 6) for val in r_bar_sq_list]\n        \n        case_result = [\n            d_star,\n            rounded_max_r_bar_sq,\n            is_monotonic,\n            is_overfitting,\n            rounded_r_bar_sq_list\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of lists and booleans matches the requirement.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3096432"}]}