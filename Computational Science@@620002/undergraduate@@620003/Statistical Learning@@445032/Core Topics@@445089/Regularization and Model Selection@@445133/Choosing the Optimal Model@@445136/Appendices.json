{"hands_on_practices": [{"introduction": "Choosing the \"best\" model is not an absolute judgment; it is fundamentally tied to the specific goals of your project. Different evaluation metrics prioritize different aspects of performance, and the right metric depends on the application's unique requirements. This exercise [@problem_id:3107729] demonstrates this critical principle by having you evaluate three models using distinct metrics—top-$k$ accuracy, budgeted F1-score, and Area Under the Receiver Operating Characteristic (AUROC)—revealing how the optimal choice of model changes with the evaluation criteria.", "problem": "Consider a multiclass classification setting with $3$ classes labeled as $0$, $1$, and $2$. There are $N=8$ samples with true class labels given by the vector $y=(0,1,2,1,0,2,1,0)$. We evaluate three candidate models $\\mathcal{M}_0$, $\\mathcal{M}_1$, and $\\mathcal{M}_2$ that output, for each sample, a vector of nonnegative scores across classes used for ranking. For each model $\\mathcal{M}_m$, let $S^{(m)} \\in \\mathbb{R}^{8 \\times 3}$ denote its score matrix, where $S^{(m)}_{i,c}$ is the score assigned by model $m$ to class $c \\in \\{0,1,2\\}$ for sample $i \\in \\{0,1,\\dots,7\\}$. The score matrices are:\n\n- For $\\mathcal{M}_0$: rows are $(0.45,0.40,0.15)$, $(0.10,0.80,0.10)$, $(0.30,0.20,0.50)$, $(0.25,0.60,0.15)$, $(0.30,0.65,0.05)$, $(0.25,0.25,0.50)$, $(0.20,0.75,0.05)$, $(0.55,0.25,0.20)$.\n- For $\\mathcal{M}_1$: rows are $(0.35,0.55,0.10)$, $(0.30,0.50,0.20)$, $(0.20,0.15,0.65)$, $(0.15,0.70,0.15)$, $(0.60,0.25,0.15)$, $(0.10,0.20,0.70)$, $(0.05,0.85,0.10)$, $(0.65,0.20,0.15)$.\n- For $\\mathcal{M}_2$: rows are $(0.30,0.35,0.35)$, $(0.05,0.92,0.03)$, $(0.25,0.40,0.35)$, $(0.10,0.85,0.05)$, $(0.20,0.55,0.25)$, $(0.15,0.35,0.50)$, $(0.02,0.95,0.03)$, $(0.40,0.45,0.15)$.\n\nWe define the following evaluation metrics, grounded in core definitions:\n\n1. Top-$k$ accuracy: For a given $k \\in \\{1,2,3\\}$, define the top-$k$ set for sample $i$ as the set of $k$ classes with highest scores according to a tie-breaking rule described below. The top-$k$ accuracy of a model is the proportion of samples for which the true class $y_i$ lies within this top-$k$ set. The tie-breaking rule for class ranking is: first sort classes by descending score, and for any ties in score, sort tied classes by ascending class index.\n\n2. F1-score (F1): For binary evaluation of class $1$ versus the rest, define the predicted positive set as those samples whose class-$1$ score is above a threshold. For any threshold, define True Positives ($\\mathrm{TP}$), False Positives ($\\mathrm{FP}$), and False Negatives ($\\mathrm{FN}$) in the one-versus-rest sense for class $1$. The precision is $\\mathrm{Precision} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FP})$, the recall is $\\mathrm{Recall} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$, and the F1-score is $F1 = 2 \\cdot \\mathrm{Precision} \\cdot \\mathrm{Recall} / (\\mathrm{Precision}+\\mathrm{Recall})$, equivalently $F1 = \\dfrac{2 \\mathrm{TP}}{2 \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}$. A use-case constraint imposes a resource budget on predicted positives: let $b \\in (0,1]$ be the allowed fraction of samples that may be flagged positive, so the predicted positives must be at most $\\lfloor b \\cdot N \\rfloor$. Under this constraint, the optimal threshold for maximizing $F1$ over all allowable thresholds will select the top $m$ samples by class-$1$ score for some $m \\in \\{0,1,\\dots,\\lfloor bN \\rfloor\\}$, subject to the same tie-breaking rule on indices when scores are equal.\n\n3. Area Under the Receiver Operating Characteristic (AUROC): For binary evaluation of class $1$ versus the rest, the Receiver Operating Characteristic (ROC) curve plots the True Positive Rate ($\\mathrm{TPR} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$) versus the False Positive Rate ($\\mathrm{FPR} = \\mathrm{FP}/(\\mathrm{FP}+\\mathrm{TN})$) across all thresholds on the class-$1$ scores. The Area Under the Receiver Operating Characteristic (AUROC) is the area under this curve, which can be computed equivalently via the Mann–Whitney statistic as the probability that a randomly chosen positive sample has a higher class-$1$ score than a randomly chosen negative sample, with ties counted as one-half.\n\nModel selection rule: For a given metric, the chosen model index is the smallest $m \\in \\{0,1,2\\}$ that achieves the maximal value of that metric among $\\{\\mathcal{M}_0,\\mathcal{M}_1,\\mathcal{M}_2\\}$, with ties broken by selecting the smallest index.\n\nYour program must compute, for each test case in the suite below:\n- The index of the model that maximizes top-$k$ accuracy.\n- The index of the model that maximizes $F1$ for class $1$ under the budget constraint $b$.\n- The index of the model that maximizes AUROC for class $1$.\n\nTest Suite:\n- Test case $1$: $k=1$, $b=0.5$.\n- Test case $2$: $k=2$, $b=0.25$.\n- Test case $3$: $k=2$, $b=0.125$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list of three integers in square brackets representing $[$top-$k$ model index, F1-under-$b$ model index, AUROC model index$]$. For example, the output should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$, with no spaces anywhere in the line.", "solution": "The problem is valid as it is scientifically grounded in the principles of statistical learning, well-posed with all necessary data and unambiguous definitions, and objective in its formulation. We will proceed with a full solution.\n\nThe problem requires us to evaluate three models, $\\mathcal{M}_0$, $\\mathcal{M}_1$, and $\\mathcal{M}_2$, on a multiclass classification task with $N=8$ samples and $3$ classes. The true labels are given by the vector $y=(0,1,2,1,0,2,1,0)$. For each model, we are provided an $8 \\times 3$ score matrix $S^{(m)}$. We will compute three metrics for each model: top-$k$ accuracy, a budgeted F1-score for class $1$, and the Area Under the Receiver Operating Characteristic (AUROC) for class $1$. For each metric, we select the model with the highest score, breaking ties by choosing the smallest model index $m \\in \\{0,1,2\\}$.\n\nLet's denote the set of models as $\\{\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2\\}$. The score matrices $S^{(0)}, S^{(1)}, S^{(2)}$ and the label vector $y$ are the primary inputs.\n\n**1. Top-$k$ Accuracy Calculation**\n\nThe top-$k$ accuracy is the fraction of samples for which the true class label $y_i$ is among the $k$ classes with the highest scores. The ranking of classes for a sample $i$ is determined by sorting scores in descending order, with ties broken by ascending class index.\n\nLet $S^{(m)}_{i, \\cdot}$ be the vector of scores for sample $i$ from model $\\mathcal{M}_m$. We form pairs $(S^{(m)}_{i,c}, c)$ for $c \\in \\{0,1,2\\}$. These pairs are sorted first by score (descending) and then by class index $c$ (ascending). The set of top-$k$ predicted classes, $\\hat{Y}_{i,k}^{(m)}$, is the set of the first $k$ class indices from this sorted list.\n\nThe top-$k$ accuracy for model $\\mathcal{M}_m$ is given by:\n$$ \\text{Accuracy}_k(\\mathcal{M}_m) = \\frac{1}{N} \\sum_{i=0}^{N-1} I(y_i \\in \\hat{Y}_{i,k}^{(m)}) $$\nwhere $I(\\cdot)$ is the indicator function.\n\n**2. Budgeted F1-Score for Class 1**\n\nThis metric evaluates the models' ability to identify samples of class $1$ (positives) versus classes $0$ and $2$ (negatives). The set of true positive samples is $P = \\{i | y_i = 1\\} = \\{1, 3, 6\\}$, so the total number of positives is $N_P = 3$. The set of true negative samples is $N_{set} = \\{i | y_i \\neq 1\\} = \\{0, 2, 4, 5, 7\\}$, with $N_N = 5$.\n\nA budget constraint limits the number of samples that can be predicted as positive to at most $m_{max} = \\lfloor b \\cdot N \\rfloor$. To maximize the F1-score, we must find the optimal number of predicted positives, $m^*$, where $0 \\le m^* \\le m_{max}$. For each possible number of predictions $m_{pred} \\in \\{0, 1, \\dots, m_{max}\\}$, we select the top $m_{pred}$ samples based on their class-$1$ scores, $S^{(m)}_{\\cdot, 1}$. The ranking of samples is determined by sorting their class-$1$ scores in descending order, with ties broken by ascending sample index $i$.\n\nFor each $m_{pred}$, we compute:\n- $\\mathrm{TP}(m_{pred})$: Number of true positives among the top $m_{pred}$ samples.\n- $\\mathrm{FP}(m_{pred})$: Number of true negatives among the top $m_{pred}$ samples.\n- $\\mathrm{FN}(m_{pred}) = N_P - \\mathrm{TP}(m_{pred})$.\n\nThe F1-score for a given $m_{pred}$ is:\n$$ F1(m_{pred}) = \\frac{2 \\cdot \\mathrm{TP}(m_{pred})}{2 \\cdot \\mathrm{TP}(m_{pred}) + \\mathrm{FP}(m_{pred}) + \\mathrm{FN}(m_{pred})} $$\nThe F1-score for model $\\mathcal{M}_m$ under budget $b$ is the maximum of these values:\n$$ F1_b(\\mathcal{M}_m) = \\max_{m_{pred}=0, \\dots, m_{max}} F1(m_{pred}) $$\n\n**3. AUROC for Class 1**\n\nThe AUROC for the binary task of class $1$ versus the rest can be computed using the Mann-Whitney U statistic. It measures the probability that a randomly chosen positive sample has a higher score for class 1 than a randomly chosen negative sample.\n\nLet $S_P^{(m)} = \\{S_{i,1}^{(m)} | i \\in P\\}$ be the set of class-$1$ scores for the positive samples and $S_N^{(m)} = \\{S_{j,1}^{(m)} | j \\in N_{set}\\}$ be the scores for the negative samples. The AUROC for model $\\mathcal{M}_m$ is:\n$$ \\text{AUROC}(\\mathcal{M}_m) = \\frac{1}{N_P \\cdot N_N} \\sum_{s_p \\in S_P^{(m)}} \\sum_{s_n \\in S_N^{(m)}} \\left( I(s_p > s_n) + 0.5 \\cdot I(s_p = s_n) \\right) $$\n\n**Model Selection**\nFor each test case and each metric, we calculate the metric's value for all three models, $\\{\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2\\}$. Let the computed values be $\\{v_0, v_1, v_2\\}$. The maximum value is $v_{max} = \\max(v_0, v_1, v_2)$. The chosen model index is $\\arg \\min \\{m | v_m = v_{max}\\}$.\n\n**Calculations for Test Cases**\n\nLet's compute the results for each test case.\n\n**Test Case 1: $k=1, b=0.5$**\n- $N=8$, so for F1-score, $m_{max} = \\lfloor 0.5 \\times 8 \\rfloor = 4$.\n- **Top-1 Accuracy:**\n  - $\\text{Acc}_1(\\mathcal{M}_0) = 7/8 = 0.875$\n  - $\\text{Acc}_1(\\mathcal{M}_1) = 7/8 = 0.875$\n  - $\\text{Acc}_1(\\mathcal{M}_2) = 4/8 = 0.5$\n  - Max is $0.875$, tied between $\\mathcal{M}_0$ and $\\mathcal{M}_1$. We select index $0$.\n- **F1-Score ($b=0.5$):**\n  - For $\\mathcal{M}_0$, max F1 is $6/7 \\approx 0.857$ (at $m_{pred}=4$).\n  - For $\\mathcal{M}_1$, max F1 is $6/7 \\approx 0.857$ (at $m_{pred}=4$).\n  - For $\\mathcal{M}_2$, max F1 is $1.0$ (at $m_{pred}=3$, where $\\mathrm{TP}=3, \\mathrm{FP}=0$).\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **AUROC:**\n  - $\\text{AUROC}(\\mathcal{M}_0) = 14/15 \\approx 0.9333$\n  - $\\text{AUROC}(\\mathcal{M}_1) = 14/15 \\approx 0.9333$\n  - $\\text{AUROC}(\\mathcal{M}_2) = 15/15 = 1.0$\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **Result for Test Case 1:** $[0, 2, 2]$\n\n**Test Case 2: $k=2, b=0.25$**\n- For F1-score, $m_{max} = \\lfloor 0.25 \\times 8 \\rfloor = 2$.\n- **Top-2 Accuracy:**\n  - $\\text{Acc}_2(\\mathcal{M}_0) = 8/8 = 1.0$\n  - $\\text{Acc}_2(\\mathcal{M}_1) = 8/8 = 1.0$\n  - $\\text{Acc}_2(\\mathcal{M}_2) = 6/8 = 0.75$\n  - Max is $1.0$, tied between $\\mathcal{M}_0$ and $\\mathcal{M}_1$. We select index $0$.\n- **F1-Score ($b=0.25$):**\n  - For $\\mathcal{M}_0$, max F1 is $0.8$ (at $m_{pred}=2$, $\\mathrm{TP}=2, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_1$, max F1 is $0.8$ (at $m_{pred}=2$, $\\mathrm{TP}=2, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_2$, max F1 is $0.8$ (at $m_{pred}=2$, $\\mathrm{TP}=2, \\mathrm{FP}=0$).\n  - All models score $0.8$. We select the smallest index, $0$.\n- **AUROC:** (This is independent of $k$ and $b$)\n  - Values are identical to Test Case 1.\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **Result for Test Case 2:** $[0, 0, 2]$\n\n**Test Case 3: $k=2, b=0.125$**\n- For F1-score, $m_{max} = \\lfloor 0.125 \\times 8 \\rfloor = 1$.\n- **Top-2 Accuracy:** (This is independent of $b$)\n  - Values are identical to Test Case 2.\n  - Max is $1.0$, tied between $\\mathcal{M}_0$ and $\\mathcal{M}_1$. We select index $0$.\n- **F1-Score ($b=0.125$):**\n  - For $\\mathcal{M}_0$, max F1 is $0.5$ (at $m_{pred}=1$, $\\mathrm{TP}=1, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_1$, max F1 is $0.5$ (at $m_{pred}=1$, $\\mathrm{TP}=1, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_2$, max F1 is $0.5$ (at $m_{pred}=1$, $\\mathrm{TP}=1, \\mathrm{FP}=0$).\n  - All models score $0.5$. We select the smallest index, $0$.\n- **AUROC:** (This is independent of $k$ and $b$)\n  - Values are identical to Test Case 1.\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **Result for Test Case 3:** $[0, 0, 2]$\n\nThe program will implement these computations systematically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model evaluation problem by calculating three metrics for three models\n    across three test cases and selecting the best model for each.\n    \"\"\"\n    # Define problem givens\n    y_true = np.array([0, 1, 2, 1, 0, 2, 1, 0])\n    num_samples = len(y_true)\n    num_classes = 3\n\n    scores = [\n        np.array([\n            [0.45, 0.40, 0.15], [0.10, 0.80, 0.10], [0.30, 0.20, 0.50],\n            [0.25, 0.60, 0.15], [0.30, 0.65, 0.05], [0.25, 0.25, 0.50],\n            [0.20, 0.75, 0.05], [0.55, 0.25, 0.20]\n        ]),\n        np.array([\n            [0.35, 0.55, 0.10], [0.30, 0.50, 0.20], [0.20, 0.15, 0.65],\n            [0.15, 0.70, 0.15], [0.60, 0.25, 0.15], [0.10, 0.20, 0.70],\n            [0.05, 0.85, 0.10], [0.65, 0.20, 0.15]\n        ]),\n        np.array([\n            [0.30, 0.35, 0.35], [0.05, 0.92, 0.03], [0.25, 0.40, 0.35],\n            [0.10, 0.85, 0.05], [0.20, 0.55, 0.25], [0.15, 0.35, 0.50],\n            [0.02, 0.95, 0.03], [0.40, 0.45, 0.15]\n        ])\n    ]\n\n    test_cases = [\n        {'k': 1, 'b': 0.5},\n        {'k': 2, 'b': 0.25},\n        {'k': 2, 'b': 0.125}\n    ]\n\n    def select_best_model(metric_scores):\n        \"\"\"Selects model index with max score, breaking ties by smallest index.\"\"\"\n        max_score = np.max(metric_scores)\n        best_model_idx = np.where(metric_scores == max_score)[0][0]\n        return best_model_idx\n\n    # --- Metric Calculation Functions ---\n\n    def calculate_top_k_accuracy(model_scores, y_true, k, num_classes):\n        correct_predictions = 0\n        class_indices = np.arange(num_classes)\n        for i in range(len(y_true)):\n            sample_scores = model_scores[i]\n            # Tie-breaking: sort by score desc, then class index asc\n            sorted_indices = np.lexsort((class_indices, -sample_scores))\n            top_k_classes = sorted_indices[:k]\n            if y_true[i] in top_k_classes:\n                correct_predictions += 1\n        return correct_predictions / len(y_true)\n\n    def calculate_budgeted_f1(model_scores, y_true, b, num_samples):\n        class1_scores = model_scores[:, 1]\n        is_positive = (y_true == 1)\n        \n        sample_indices = np.arange(num_samples)\n        # Tie-breaking: sort samples by class 1 score desc, then sample index asc\n        ranked_sample_indices = np.lexsort((sample_indices, -class1_scores))\n        \n        num_positives_total = np.sum(is_positive)\n        max_predictions = int(np.floor(b * num_samples))\n        \n        max_f1 = 0.0\n        \n        for m_pred in range(max_predictions + 1):\n            if m_pred == 0:\n                tp = 0\n                fp = 0\n            else:\n                predicted_pos_indices = ranked_sample_indices[:m_pred]\n                tp = np.sum(is_positive[predicted_pos_indices])\n                fp = m_pred - tp\n            \n            fn = num_positives_total - tp\n            \n            denominator = 2 * tp + fp + fn\n            if denominator > 0:\n                f1 = (2 * tp) / denominator\n                if f1 > max_f1:\n                    max_f1 = f1\n        return max_f1\n\n    def calculate_auroc(model_scores, y_true):\n        class1_scores = model_scores[:, 1]\n        is_positive = (y_true == 1)\n        \n        pos_scores = class1_scores[is_positive]\n        neg_scores = class1_scores[~is_positive]\n        \n        if len(pos_scores) == 0 or len(neg_scores) == 0:\n            return 0.5 \n\n        numerator = 0.0\n        for p_score in pos_scores:\n            for n_score in neg_scores:\n                if p_score > n_score:\n                    numerator += 1.0\n                elif p_score == n_score:\n                    numerator += 0.5\n        \n        denominator = len(pos_scores) * len(neg_scores)\n        return numerator / denominator\n\n    # --- Main Loop ---\n    \n    all_results = []\n    \n    # Pre-calculate AUROC as it's independent of test cases\n    auroc_scores = np.array([calculate_auroc(s, y_true) for s in scores])\n    best_auroc_model = select_best_model(auroc_scores)\n\n    for case in test_cases:\n        k = case['k']\n        b = case['b']\n        \n        # Top-k accuracy\n        top_k_scores = np.array([\n            calculate_top_k_accuracy(s, y_true, k, num_classes) for s in scores\n        ])\n        best_top_k_model = select_best_model(top_k_scores)\n        \n        # Budgeted F1\n        f1_scores = np.array([\n            calculate_budgeted_f1(s, y_true, b, num_samples) for s in scores\n        ])\n        best_f1_model = select_best_model(f1_scores)\n        \n        case_results = [best_top_k_model, best_f1_model, best_auroc_model]\n        all_results.append(case_results)\n\n    # Format and print the final output\n    output_str = f\"[{','.join([f'[{r[0]},{r[1]},{r[2]}]' for r in all_results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3107729"}, {"introduction": "A good probabilistic classifier must do more than just correctly rank positive and negative examples; its predicted probabilities should also be reliable. This practice [@problem_id:3107702] explores the crucial distinction between a model's ranking ability (discrimination), often measured by Area Under the Receiver Operating Characteristic Curve (AUC), and its probability calibration, measured by metrics like logarithmic loss. You will compare a high-AUC but miscalibrated model against a better-calibrated one to understand why accurate probabilities are essential for making optimal, cost-sensitive decisions.", "problem": "You are given two binary classifiers per test case, Model A and Model B. Each outputs a score for the positive class that is intended to represent an estimated probability. The classifiers differ in two ways: Model A tends to produce a high ranking quality as measured by Area Under the Receiver Operating Characteristic Curve (AUC), but may be miscalibrated; Model B produces lower ranking quality but is better calibrated. Your task is to select the optimal model under the logarithmic loss (also called negative log-likelihood or cross-entropy) and to examine the impact of decisions made by thresholding these scores under simple misclassification costs.\n\nFundamental base and definitions:\n- For binary labels $y \\in \\{0,1\\}$ and predicted probabilities $p \\in (0,1)$, the per-sample logarithmic loss is $-\\left(y \\log p + (1-y)\\log(1-p)\\right)$. The average log-loss over $n$ samples is the arithmetic mean of these terms.\n- To avoid undefined values from $\\log(0)$, one must use numerically stable probabilities $\\tilde{p} = \\min(\\max(p,\\epsilon),1-\\epsilon)$ for a small $\\epsilon > 0$, for example $\\epsilon = 10^{-15}$.\n- Suppose false positives incur cost $c_{\\text{fp}} > 0$ and false negatives incur cost $c_{\\text{fn}} > 0$. Under calibrated probabilities, the Bayes-optimal deterministic decision rule that minimizes expected misclassification cost is to predict positive if and only if $p \\ge \\tau$, where the threshold $\\tau$ satisfies $\\tau = \\dfrac{c_{\\text{fp}}}{c_{\\text{fp}} + c_{\\text{fn}}}$.\n\nProgram requirements:\n- For each test case, you are given arrays of true labels $y_i \\in \\{0,1\\}$ and two arrays of scores $p^{(A)}_i$ and $p^{(B)}_i$ corresponding to Model A and Model B, respectively. You are also given costs $c_{\\text{fp}}$ and $c_{\\text{fn}}$.\n- For each model in each test case, compute the average log-loss using numerically stabilized probabilities with $\\epsilon = 10^{-15}$.\n- Choose the optimal model under log-loss: output an integer $m$ equal to $0$ if Model A has strictly smaller average log-loss, $1$ if Model B has strictly smaller average log-loss. In case of a tie up to tolerance $\\delta = 10^{-12}$, break the tie by selecting the model with the smaller realized decision cost under the threshold $\\tau = \\dfrac{c_{\\text{fp}}}{c_{\\text{fp}} + c_{\\text{fn}}}$. If there is still a tie, choose Model B (i.e., output $1$).\n- For decision impact, compute the realized total misclassification cost for each model using threshold $\\tau$: predict $\\hat{y}=1$ if $p \\ge \\tau$ and $\\hat{y}=0$ otherwise. A prediction incurs cost $c_{\\text{fp}}$ if $\\hat{y}=1$ and $y=0$, cost $c_{\\text{fn}}$ if $\\hat{y}=0$ and $y=1$, and cost $0$ otherwise. Report the total cost (sum over all samples) for each model.\n\nTest suite:\nProvide the following four test cases. Each test case is a tuple $(y, p^{(A)}, p^{(B)}, c_{\\text{fp}}, c_{\\text{fn}})$.\n\n- Case 1 (general case where miscalibration can hurt log-loss despite strong ranking): \n  - $y = [1,0,1,0,1,0,0,1,0,1]$\n  - $p^{(A)} = [0.9,0.2,0.85,0.25,0.8,0.15,0.95,0.65,0.1,0.6]$\n  - $p^{(B)} = [0.7,0.35,0.65,0.4,0.6,0.3,0.45,0.55,0.35,0.6]$\n  - $c_{\\text{fp}} = 1.0$, $c_{\\text{fn}} = 1.0$\n- Case 2 (boundary with extreme scores requiring stabilization, asymmetric costs): \n  - $y = [1,0,1,0]$\n  - $p^{(A)} = [1.0,0.0,1.0,1.0]$\n  - $p^{(B)} = [0.8,0.2,0.8,0.2]$\n  - $c_{\\text{fp}} = 2.0$, $c_{\\text{fn}} = 1.0$\n- Case 3 (balanced costs, stronger separation vs calibration): \n  - $y = [1,0,0,1,0,1]$\n  - $p^{(A)} = [0.9,0.6,0.3,0.8,0.35,0.7]$\n  - $p^{(B)} = [0.7,0.5,0.45,0.65,0.4,0.55]$\n  - $c_{\\text{fp}} = 1.0$, $c_{\\text{fn}} = 1.0$\n- Case 4 (exact tie in scores to test deterministic tie-breaking): \n  - $y = [0,1,0,1,1]$\n  - $p^{(A)} = [0.4,0.6,0.5,0.55,0.65]$\n  - $p^{(B)} = [0.4,0.6,0.5,0.55,0.65]$\n  - $c_{\\text{fp}} = 3.0$, $c_{\\text{fn}} = 2.0$\n\nNumerical and output requirements:\n- Use $\\epsilon = 10^{-15}$ to stabilize probabilities for log-loss, and use $\\delta = 10^{-12}$ for tie-breaking on average log-loss comparisons.\n- For each test case, output a list $[m, L_A, L_B, C_A, C_B]$, where $m$ is an integer in $\\{0,1\\}$, $L_A$ and $L_B$ are the average log-loss values for Model A and Model B, respectively, and $C_A$ and $C_B$ are the realized total misclassification costs under threshold $\\tau$. Express $L_A$, $L_B$, $C_A$, and $C_B$ as decimal floats rounded to $6$ digits after the decimal point.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of the four per-case lists, enclosed in square brackets; for example, $[[m_1,L_{A,1},L_{B,1},C_{A,1},C_{B,1}],[m_2,L_{A,2},L_{B,2},C_{A,2},C_{B,2}],\\dots]$. There must be no extra text or spaces in the output line.", "solution": "The problem requires the selection of an optimal binary classifier from two candidates, Model A and Model B, based on a hierarchical criterion involving logarithmic loss and misclassification cost. This task touches upon fundamental concepts in statistical learning: model evaluation, calibration, and decision theory. The solution involves a precise, step-by-step implementation of the provided evaluation protocol.\n\nFirst, we address the primary evaluation metric, the average logarithmic loss, often referred to as cross-entropy loss. For a set of $n$ samples with true binary labels $y_i \\in \\{0, 1\\}$ and corresponding predicted probabilities $p_i \\in (0, 1)$, the average log-loss is defined as:\n$$\n\\bar{L}(y, p) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\nThis metric evaluates the quality of the predicted probabilities directly. A lower log-loss indicates a better-calibrated model, meaning its outputs are more accurate representations of the true likelihood of the positive class. To handle cases where a model predicts a probability of exactly $0$ or $1$, which would make the logarithm undefined, the probabilities $p_i$ are numerically stabilized. They are clipped to a small interval $[\\epsilon, 1-\\epsilon]$, where $\\epsilon$ is a small positive constant. The stabilized probability $\\tilde{p}_i$ is given by:\n$$\n\\tilde{p}_i = \\min(\\max(p_i, \\epsilon), 1-\\epsilon)\n$$\nIn this problem, $\\epsilon$ is specified as $10^{-15}$. For each model, Model A and Model B, we compute their respective average log-losses, denoted $L_A$ and $L_B$.\n\nSecond, we consider the impact of the models' predictions under a decision-theoretic framework. Given a cost $c_{\\text{fp}}$ for a false positive prediction and a cost $c_{\\text{fn}}$ for a false negative prediction, the decision rule that minimizes the expected misclassification cost (assuming the probabilities $p_i$ are perfectly calibrated) is to predict the positive class if and only if $p_i \\ge \\tau$, where the decision threshold $\\tau$ is:\n$$\n\\tau = \\frac{c_{\\text{fp}}}{c_{\\text{fp}} + c_{\\text{fn}}}\n$$\nFor each model, we apply this threshold to its score outputs $p_i$ to obtain a set of deterministic predictions $\\hat{y}_i$. The total realized misclassification cost is then calculated by summing the costs of all errors over the $n$ samples:\n$$\nC(y, \\hat{y}) = c_{\\text{fp}} \\sum_{i=1}^{n} \\mathbb{I}(\\hat{y}_i = 1 \\text{ and } y_i = 0) + c_{\\text{fn}} \\sum_{i=1}^{n} \\mathbb{I}(\\hat{y}_i = 0 \\text{ and } y_i = 1)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This provides the total costs $C_A$ and $C_B$ for Model A and Model B, respectively.\n\nFinally, the optimal model is selected using a defined hierarchical procedure. Let $m$ be an integer representing the chosen model ($0$ for A, $1$ for B).\n\n1.  The primary criterion is the average log-loss. The model with a strictly smaller average log-loss is chosen. To account for potential floating-point inaccuracies, a tie is declared if the absolute difference between the log-losses is within a tolerance $\\delta = 10^{-12}$.\n    $$\n    \\text{If } |L_A - L_B| > \\delta, \\text{ then }\n    m = \\begin{cases} 0  \\text{if } L_A  L_B \\\\ 1  \\text{if } L_B  L_A \\end{cases}\n    $$\n2.  If the log-losses are tied (i.e., $|L_A - L_B| \\le \\delta$), the tie is broken by the realized total misclassification cost. The model with the lower cost is selected.\n    $$\n    \\text{If } |L_A - L_B| \\le \\delta \\text{ and } C_A \\neq C_B, \\text{ then }\n    m = \\begin{cases} 0  \\text{if } C_A  C_B \\\\ 1  \\text{if } C_B  C_A \\end{cases}\n    $$\n3.  If both the log-losses and the costs are tied, a final deterministic rule is applied: Model B is chosen.\n    $$\n    \\text{If } |L_A - L_B| \\le \\delta \\text{ and } C_A = C_B, \\text{ then } m = 1\n    $$\n\nThis complete algorithm is applied to each test case. The results, consisting of the selected model index $m$ and the calculated values for $L_A, L_B, C_A, C_B$ (rounded to six decimal places), are aggregated into the final specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    # Define the constants from the problem statement.\n    EPSILON = 1e-15\n    DELTA = 1e-12\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (y, p_A, p_B, c_fp, c_fn)\n    test_cases = [\n        (\n            np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 1]),\n            np.array([0.9, 0.2, 0.85, 0.25, 0.8, 0.15, 0.95, 0.65, 0.1, 0.6]),\n            np.array([0.7, 0.35, 0.65, 0.4, 0.6, 0.3, 0.45, 0.55, 0.35, 0.6]),\n            1.0, 1.0\n        ),\n        (\n            np.array([1, 0, 1, 0]),\n            np.array([1.0, 0.0, 1.0, 1.0]),\n            np.array([0.8, 0.2, 0.8, 0.2]),\n            2.0, 1.0\n        ),\n        (\n            np.array([1, 0, 0, 1, 0, 1]),\n            np.array([0.9, 0.6, 0.3, 0.8, 0.35, 0.7]),\n            np.array([0.7, 0.5, 0.45, 0.65, 0.4, 0.55]),\n            1.0, 1.0\n        ),\n        (\n            np.array([0, 1, 0, 1, 1]),\n            np.array([0.4, 0.6, 0.5, 0.55, 0.65]),\n            np.array([0.4, 0.6, 0.5, 0.55, 0.65]),\n            3.0, 2.0\n        )\n    ]\n\n    results_str_list = []\n    \n    for y, p_a, p_b, c_fp, c_fn in test_cases:\n        # 1. Calculate average log-loss\n        def calculate_log_loss(y_true, p_pred):\n            p_clipped = np.clip(p_pred, EPSILON, 1 - EPSILON)\n            loss_per_sample = - (y_true * np.log(p_clipped) + (1 - y_true) * np.log(1 - p_clipped))\n            return np.mean(loss_per_sample)\n\n        l_a = calculate_log_loss(y, p_a)\n        l_b = calculate_log_loss(y, p_b)\n\n        # 2. Calculate realized total misclassification cost\n        tau = c_fp / (c_fp + c_fn)\n        \n        def calculate_cost(y_true, p_pred, threshold, cost_fp, cost_fn):\n            y_hat = (p_pred >= threshold).astype(int)\n            false_positives = np.sum((y_hat == 1)  (y_true == 0))\n            false_negatives = np.sum((y_hat == 0)  (y_true == 1))\n            total_cost = false_positives * cost_fp + false_negatives * cost_fn\n            return float(total_cost)\n\n        c_a = calculate_cost(y, p_a, tau, c_fp, c_fn)\n        c_b = calculate_cost(y, p_b, tau, c_fp, c_fn)\n\n        # 3. Select optimal model based on hierarchical rules\n        m = 0\n        if abs(l_a - l_b) = DELTA:\n            # Tie in log-loss, use cost to break tie\n            if c_a  c_b:\n                m = 0\n            elif c_b  c_a:\n                m = 1\n            else:\n                # Tie in cost, default to Model B\n                m = 1\n        elif l_a  l_b:\n            m = 0\n        else: # l_b  l_a\n            m = 1\n            \n        # 4. Format the output for the current case\n        # Round the float values to 6 decimal places before formatting\n        l_a_rounded = round(l_a, 6)\n        l_b_rounded = round(l_b, 6)\n        c_a_rounded = round(c_a, 6)\n        c_b_rounded = round(c_b, 6)\n        \n        case_result_str = (\n            f\"[{m},\"\n            f\"{l_a_rounded:.6f},\"\n            f\"{l_b_rounded:.6f},\"\n            f\"{c_a_rounded:.6f},\"\n            f\"{c_b_rounded:.6f}]\"\n        )\n        results_str_list.append(case_result_str)\n\n    # Final print statement in the exact required format without spaces.\n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3107702"}, {"introduction": "Ultimately, we build models to drive better decisions, and these decisions often have asymmetric costs and benefits. This practice [@problem_id:3107638] moves beyond standard proxy metrics to a full decision-theoretic framework, where the goal is to maximize a custom utility function derived from a real-world cost matrix. You will implement a cross-validation procedure to jointly select the best model hyperparameters and the optimal decision threshold, learning how to tailor your entire modeling pipeline to a specific business objective.", "problem": "You are given three independent binary classification tasks. For each task, your goal is to implement a principled procedure that jointly selects a model and a decision threshold by maximizing an estimate of expected utility induced by a specified cost matrix. The program you produce must be a complete, runnable program that carries out this procedure and outputs the final test-set utilities for all tasks in a single line.\n\nFundamental base. Use the following foundational definitions:\n- A binary classifier maps feature vectors $x \\in \\mathbb{R}^d$ to a real-valued score $s(x) \\in \\mathbb{R}$, which is then interpreted as a probability estimate $p(y=1 \\mid x) \\in [0,1]$ using the logistic function. Assume the probabilistic linear model known as logistic regression with $\\ell_2$ regularization is used to fit $p(y=1 \\mid x)$.\n- A decision rule with threshold $t \\in [0,1]$ predicts $\\hat{y}(x;t)=\\mathbf{1}\\{p(y=1 \\mid x) \\ge t\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- A cost matrix $C \\in \\mathbb{R}^{2 \\times 2}$ assigns a nonnegative cost $C_{ij}$ to predicting class $j \\in \\{0,1\\}$ when the true class is $i \\in \\{0,1\\}$. Define the utility matrix $U$ by $U_{ij}=-C_{ij}$. The expected utility of a classifier-threshold pair on a dataset $\\{(x_n,y_n)\\}_{n=1}^N$ is the empirical average $\\frac{1}{N}\\sum_{n=1}^N U_{y_n,\\hat{y}(x_n;t)}$.\n- $K$-fold Cross-Validation (Cross-Validation (CV)) partitions the training set into $K$ disjoint folds of approximately equal size, trains on $K-1$ folds, validates on the held-out fold, and averages the validation criterion across folds to estimate generalization performance.\n\nTask. For each case below:\n1. Use $K$-fold Cross-Validation with $K = 3$ to jointly select the regularization strength $\\lambda$ of logistic regression and the decision threshold $t$ by maximizing the average validation expected utility. The search sets are:\n   - Regularization candidates $\\Lambda=\\{\\lambda_1,\\lambda_2,\\lambda_3\\}=\\{\\,0.0,\\,0.1,\\,1.0\\,\\}$.\n   - Threshold grid $\\mathcal{T}=\\{\\,0.00,\\,0.05,\\,0.10,\\,\\dots,\\,0.95,\\,1.00\\,\\}$.\n   Use deterministic folds: assign example index $n$ (zero-based) to fold $n \\bmod K$.\n   Break ties by choosing the smallest $\\lambda$ among maximizers, and then the smallest $t$ among the remaining maximizers.\n2. Retrain logistic regression on the full training set using the selected $\\lambda$, then evaluate the expected utility on the provided test set using the selected threshold $t$.\n3. Report, for each case, the test-set expected utility as a float rounded to $6$ decimal places. Do not report intermediate values or parameters.\n\nDatasets and cost matrices.\n\nCase A:\n- Training features $X_{\\text{train}} \\in \\mathbb{R}^{6 \\times 2}$ and labels $y_{\\text{train}} \\in \\{0,1\\}^6$:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  0.0  0.0\\\\\n  0.2  0.1\\\\\n  0.4  0.2\\\\\n  0.6  0.8\\\\\n  0.8  0.7\\\\\n  1.0  0.9\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Test features $X_{\\text{test}} \\in \\mathbb{R}^{2 \\times 2}$ and labels $y_{\\text{test}} \\in \\{0,1\\}^2$:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  0.3  0.2\\\\\n  0.7  0.75\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  0\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Cost matrix $C=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$.\n\nCase B:\n- Training features and labels:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  -1.0  -1.0\\\\\n  -0.8  -0.6\\\\\n  -0.6  -0.8\\\\\n  0.5  0.4\\\\\n  0.6  0.3\\\\\n  0.2  0.1\\\\\n  -0.2  0.0\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Test features and labels:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  -0.7  -0.7\\\\\n  0.55  0.35\\\\\n  0.0  0.1\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  0\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Cost matrix $C=\\begin{bmatrix}0  1\\\\ 4  0\\end{bmatrix}$.\n\nCase C:\n- Training features and labels:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  0.0  1.0\\\\\n  0.1  0.9\\\\\n  0.9  0.1\\\\\n  1.0  0.0\\\\\n  0.45  0.55\\\\\n  0.55  0.45\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  0\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Test features and labels:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  0.52  0.48\\\\\n  0.48  0.52\\\\\n  0.6  0.4\\\\\n  0.4  0.6\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  1\\\\\n  0\\\\\n  1\\\\\n  0\n  \\end{bmatrix}.\n  $$\n- Cost matrix $C=\\begin{bmatrix}0  3\\\\ 1  0\\end{bmatrix}$.\n\nImplementation requirements.\n- Use logistic regression with $\\ell_2$ regularization and a bias term, trained by gradient-based optimization on the regularized negative log-likelihood. You may assume labels in $\\{0,1\\}$ and use the logistic link.\n- For each threshold $t \\in \\mathcal{T}$ and $\\lambda \\in \\Lambda$, estimate the mean validation expected utility across the $K$ folds, then select the pair $(\\lambda^\\star,t^\\star)$ that maximizes this quantity under the tie-breaking rule given above.\n- After selection, retrain on the full training set with $\\lambda^\\star$, then evaluate the mean test-set expected utility using $t^\\star$.\n- Angle units are not applicable. There are no physical units. All reported expected utilities must be decimals, rounded to $6$ digits after the decimal point.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, for example, $[u_A,u_B,u_C]$, where each $u_\\cdot$ is the rounded test-set expected utility as specified above.", "solution": "We must design a principled joint model-and-threshold selection scheme based on expected utility derived from a cost matrix. The fundamental base consists of the probabilistic interpretation of a binary classifier, the construction of a decision rule via thresholding, cost-to-utility conversion, and Cross-Validation (CV) as an estimator of out-of-sample performance.\n\nFirst, consider a binary classification setting with inputs $x \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$. A probabilistic linear classifier under the Bernoulli model uses the logistic link to produce $p_\\theta(y=1\\mid x)=\\sigma(w^\\top x + b)$, where $\\sigma(z)=\\frac{1}{1+e^{-z}}$, parameters $\\theta=(w,b)$ with $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, and regularization strength $\\lambda \\ge 0$ controls the $\\ell_2$ penalty on $w$. Training proceeds by minimizing the regularized negative log-likelihood over the training data $\\{(x_n,y_n)\\}_{n=1}^N$, which is a convex objective in $(w,b)$ and has a unique minimizer for fixed $\\lambda$.\n\nSecond, decision-making requires a threshold $t \\in [0,1]$ to convert probabilities into class predictions: $\\hat{y}(x;t)=\\mathbf{1}\\{\\sigma(w^\\top x+b)\\ge t\\}$. The consequences of decisions are captured by a cost matrix $C \\in \\mathbb{R}^{2\\times 2}$, where $C_{ij}$ is the cost incurred when the true class is $i$ and the prediction is $j$. We define the utility matrix $U$ by $U=-C$, so maximizing expected utility is equivalent to minimizing expected cost. Given a dataset, the empirical expected utility of $(\\theta,t)$ is\n$$\n\\bar{u}(\\theta,t) \\;=\\; \\frac{1}{N}\\sum_{n=1}^N U_{y_n,\\,\\hat{y}(x_n;t)}.\n$$\nThis quantities rewards correct decisions according to the utility matrix and penalizes mistakes according to the negative costs.\n\nThird, to choose the optimal model and threshold, we require an unbiased estimate of out-of-sample expected utility. $K$-fold Cross-Validation (CV) supplies this by partitioning the training set indices into $K$ folds $\\{\\mathcal{I}_k\\}_{k=0}^{K-1}$, training on $\\bigcup_{j\\ne k}\\mathcal{I}_j$ and validating on $\\mathcal{I}_k$. For any candidate $\\lambda$ and $t$, define\n$$\n\\widehat{u}_{\\text{CV}}(\\lambda,t) \\;=\\; \\frac{1}{K}\\sum_{k=0}^{K-1} \\frac{1}{|\\mathcal{I}_k|}\\sum_{n\\in \\mathcal{I}_k} U_{y_n,\\,\\hat{y}_{\\lambda,k}(x_n;t)},\n$$\nwhere $\\hat{y}_{\\lambda,k}$ is the decision rule obtained by training logistic regression with regularization $\\lambda$ on the training folds $\\bigcup_{j\\ne k}\\mathcal{I}_j$ and applying the threshold $t$ to its probabilistic outputs on the validation fold $\\mathcal{I}_k$. The pair $(\\lambda^\\star,t^\\star)$ is then chosen to maximize $\\widehat{u}_{\\text{CV}}(\\lambda,t)$ over the search grids. Ties are broken by selecting the smallest $\\lambda$ and, among those, the smallest $t$. Selecting $t$ within CV is essential because the validation probabilities depend on the trained model, and choosing $t$ on the same validation data as used to evaluate utility ensures that the threshold choice is tuned to generalize, reducing optimistic bias that would occur if $t$ were tuned on the final test set.\n\nAlgorithmic steps:\n1. For each case, build a deterministic $K$-fold partition by placing index $n$ into fold $n \\bmod K$, ensuring reproducibility and coverage of edge indices.\n2. For each $\\lambda \\in \\{0.0,0.1,1.0\\}$:\n   - For each fold $k \\in \\{0,1,2\\}$:\n     - Train logistic regression with $\\ell_2$ penalty $\\lambda$ on the union of the other folds, minimizing the regularized negative log-likelihood using gradient-based optimization. The gradient for $w$ combines the average residual term with the regularization; the gradient for $b$ is the average residual. Iterative updates $w \\leftarrow w - \\eta \\nabla_w$, $b \\leftarrow b - \\eta \\nabla_b$ reduce the convex objective, where $\\eta>0$ is a learning rate.\n     - Compute probabilistic predictions on the validation fold.\n   - For each threshold $t \\in \\{0.00,0.05,\\dots,1.00\\}$, convert the validation probabilities to predictions and compute the per-fold utilities, then average across folds to obtain $\\widehat{u}_{\\text{CV}}(\\lambda,t)$.\n3. Choose $(\\lambda^\\star,t^\\star)$ maximizing the Cross-Validation estimate, using the specified tie-breaking rule.\n4. Retrain logistic regression on the full training set using $\\lambda^\\star$, compute probabilistic predictions on the test set, apply $t^\\star$ to obtain decisions, and compute the mean test-set expected utility $\\bar{u}_{\\text{test}}$.\n5. Round each $\\bar{u}_{\\text{test}}$ to $6$ decimal places.\n\nDesign coverage rationale:\n- The threshold grid includes the boundary values $t=0$ and $t=1$, capturing edge cases where all examples are predicted as positive or negative, respectively, which is important under extreme cost asymmetries.\n- The regularization grid spans no regularization ($\\lambda=0.0$), mild ($\\lambda=0.1$), and strong ($\\lambda=1.0$) to test bias-variance trade-offs.\n- The datasets cover: balanced costs (Case A), asymmetric costs penalizing false negatives more (Case B), and asymmetric costs penalizing false positives more (Case C). This ensures that threshold shifts in opposite directions are exercised.\n\nThe program implements the above algorithm precisely, ensuring deterministic folds, joint selection of model complexity and threshold, retraining on the full training set after selection, and computing the final outputs $[u_A,u_B,u_C]$ in the required format. Each utility is a decimal (not a percentage), rounded to $6$ places, as specified.", "answer": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef train_logistic_regression(X, y, lambda_reg=0.0, lr=0.1, iters=3000):\n    \"\"\"\n    Train logistic regression with L2 regularization on weights (not bias).\n    X: (n_samples, n_features)\n    y: (n_samples,) in {0,1}\n    Returns weights w (n_features,) and bias b.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d, dtype=float)\n    b = 0.0\n    for _ in range(iters):\n        z = X @ w + b\n        p = sigmoid(z)\n        # gradient\n        residual = (p - y)  # shape (n,)\n        grad_w = (X.T @ residual) / n + lambda_reg * w\n        grad_b = np.sum(residual) / n\n        # update\n        w -= lr * grad_w\n        b -= lr * grad_b\n    return w, b\n\ndef predict_proba(X, w, b):\n    return sigmoid(X @ w + b)\n\ndef expected_utility_from_probs(y_true, y_prob, threshold, cost_matrix):\n    \"\"\"\n    y_true: (n,) in {0,1}\n    y_prob: (n,) in [0,1]\n    threshold: float in [0,1]\n    cost_matrix: 2x2 numpy array\n    Returns mean utility (negative cost).\n    \"\"\"\n    y_pred = (y_prob >= threshold).astype(int)\n    # Utility matrix is negative of cost matrix\n    U = -cost_matrix\n    # Map each pair (y_true[i], y_pred[i]) to utility\n    utilities = U[y_true, y_pred]\n    return float(np.mean(utilities))\n\ndef kfold_indices(n, K):\n    \"\"\"\n    Deterministic K-fold split by index modulo K.\n    Returns list of folds, each is a numpy array of indices.\n    \"\"\"\n    folds = [[] for _ in range(K)]\n    for idx in range(n):\n        folds[idx % K].append(idx)\n    return [np.array(f, dtype=int) for f in folds]\n\ndef joint_cv_select_threshold_and_lambda(X, y, cost_matrix, lambdas, thresholds, K=3, lr=0.1, iters=3000):\n    \"\"\"\n    Perform K-fold CV to jointly select lambda and threshold maximizing expected utility.\n    Returns selected (lambda_star, threshold_star).\n    Tie-breaking: smallest lambda, then smallest threshold.\n    \"\"\"\n    n = X.shape[0]\n    folds = kfold_indices(n, K)\n    # Precompute folds' train/val splits\n    fold_train_val = []\n    all_indices = np.arange(n, dtype=int)\n    for val_idx in folds:\n        train_idx = np.setdiff1d(all_indices, val_idx, assume_unique=True)\n        fold_train_val.append((train_idx, val_idx))\n\n    best_u = -np.inf\n    best_lambda = None\n    best_t = None\n\n    # Iterate lambdas in ascending order for tie-breaking\n    for lam in sorted(lambdas):\n        # For each fold, train model and store validation probabilities\n        val_probs_per_fold = []\n        y_val_per_fold = []\n        for (tr_idx, va_idx) in fold_train_val:\n            w, b = train_logistic_regression(X[tr_idx], y[tr_idx], lambda_reg=lam, lr=lr, iters=iters)\n            probs = predict_proba(X[va_idx], w, b)\n            val_probs_per_fold.append(probs)\n            y_val_per_fold.append(y[va_idx])\n        # Concatenate for averaging across folds per threshold\n        # However, to respect equal weighting per example, we compute fold-wise means and average,\n        # as specified in the problem statement.\n        # We'll compute per-fold utilities for each threshold and average across folds.\n        for t in sorted(thresholds):\n            u_folds = []\n            for probs, yv in zip(val_probs_per_fold, y_val_per_fold):\n                u = expected_utility_from_probs(yv, probs, t, cost_matrix)\n                u_folds.append(u)\n            u_cv = float(np.mean(u_folds))\n            if (u_cv > best_u) or (np.isclose(u_cv, best_u) and (best_lambda is not None) and (lam  best_lambda)) or (np.isclose(u_cv, best_u) and lam == best_lambda and (best_t is not None) and (t  best_t)):\n                best_u = u_cv\n                best_lambda = lam\n                best_t = t\n\n    return best_lambda, best_t\n\ndef solve():\n    # Define threshold grid and lambda grid\n    thresholds = np.round(np.linspace(0.0, 1.0, 21), 2)  # 0.00, 0.05, ..., 1.00\n    lambdas = [0.0, 0.1, 1.0]\n    K = 3\n\n    # Case A\n    X_train_A = np.array([\n        [0.0, 0.0],\n        [0.2, 0.1],\n        [0.4, 0.2],\n        [0.6, 0.8],\n        [0.8, 0.7],\n        [1.0, 0.9],\n    ], dtype=float)\n    y_train_A = np.array([0, 0, 0, 1, 1, 1], dtype=int)\n    X_test_A = np.array([\n        [0.3, 0.2],\n        [0.7, 0.75],\n    ], dtype=float)\n    y_test_A = np.array([0, 1], dtype=int)\n    C_A = np.array([[0.0, 1.0],\n                    [1.0, 0.0]], dtype=float)\n\n    # Case B\n    X_train_B = np.array([\n        [-1.0, -1.0],\n        [-0.8, -0.6],\n        [-0.6, -0.8],\n        [0.5, 0.4],\n        [0.6, 0.3],\n        [0.2, 0.1],\n        [-0.2, 0.0],\n    ], dtype=float)\n    y_train_B = np.array([0, 0, 0, 1, 1, 1, 1], dtype=int)\n    X_test_B = np.array([\n        [-0.7, -0.7],\n        [0.55, 0.35],\n        [0.0, 0.1],\n    ], dtype=float)\n    y_test_B = np.array([0, 1, 1], dtype=int)\n    C_B = np.array([[0.0, 1.0],\n                    [4.0, 0.0]], dtype=float)\n\n    # Case C\n    X_train_C = np.array([\n        [0.0, 1.0],\n        [0.1, 0.9],\n        [0.9, 0.1],\n        [1.0, 0.0],\n        [0.45, 0.55],\n        [0.55, 0.45],\n    ], dtype=float)\n    y_train_C = np.array([0, 0, 1, 1, 0, 1], dtype=int)\n    X_test_C = np.array([\n        [0.52, 0.48],\n        [0.48, 0.52],\n        [0.6, 0.4],\n        [0.4, 0.6],\n    ], dtype=float)\n    y_test_C = np.array([1, 0, 1, 0], dtype=int)\n    C_C = np.array([[0.0, 3.0],\n                    [1.0, 0.0]], dtype=float)\n\n    cases = [\n        (X_train_A, y_train_A, X_test_A, y_test_A, C_A),\n        (X_train_B, y_train_B, X_test_B, y_test_B, C_B),\n        (X_train_C, y_train_C, X_test_C, y_test_C, C_C),\n    ]\n\n    results = []\n    # Training parameters (chosen to ensure convergence on small datasets)\n    lr = 0.2\n    iters = 4000\n\n    for X_tr, y_tr, X_te, y_te, C in cases:\n        lam_star, t_star = joint_cv_select_threshold_and_lambda(\n            X_tr, y_tr, C, lambdas, thresholds, K=K, lr=lr, iters=iters\n        )\n        w, b = train_logistic_regression(X_tr, y_tr, lambda_reg=lam_star, lr=lr, iters=iters)\n        probs_test = predict_proba(X_te, w, b)\n        u_test = expected_utility_from_probs(y_te, probs_test, t_star, C)\n        # Round to 6 decimal places\n        results.append(f\"{u_test:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3107638"}]}