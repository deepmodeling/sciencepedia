{"hands_on_practices": [{"introduction": "This practice delves into the mathematical core of the LASSO's variable selection property. By assuming an orthonormal design matrix—a scenario where all predictors are uncorrelated and have equal variance—we can analytically derive the LASSO solution. This simplification strips away complexities to reveal a clean, intuitive result: the LASSO estimator is simply a \"soft-thresholded\" version of the standard least squares estimates, providing a crystal-clear understanding of how the $\\ell_1$ penalty induces sparsity [@problem_id:3191263].", "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$. Assume that the columns of $X$ are orthonormal, i.e., $X^\\top X = I_p$. The Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{p}$ is defined as the solution to the optimization problem\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\},\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\| \\cdot \\|_{1}$ and $\\| \\cdot \\|_{2}$ denote the $\\ell_{1}$-norm and $\\ell_{2}$-norm, respectively.\n\nStarting only from the above optimization definition, the orthonormality condition $X^\\top X = I_p$, and standard first-order optimality conditions (including subgradient calculus for the $\\ell_{1}$-norm), derive a coordinate-wise characterization of $\\hat{\\beta}(\\lambda)$ in terms of $z := X^\\top y$ and $\\lambda$. Use this characterization to state a necessary and sufficient condition for a coordinate $\\hat{\\beta}_{j}(\\lambda)$ to be nonzero, expressed directly in terms of $z_{j}$ and $\\lambda$.\n\nThen, consider a specific orthonormal design with $p = 6$, for which $z = X^\\top y$ is given by\n$$\nz = \\begin{pmatrix} 2.7 & -1.3 & 0.9 & 3.4 & -0.2 & -2.1 \\end{pmatrix}.\n$$\nFor the four regularization values $\\lambda \\in \\{0.5, 1.0, 2.0, 3.0\\}$, determine the corresponding selection sets $\\{ j : \\hat{\\beta}_{j}(\\lambda) \\neq 0 \\}$.\n\nFinally, as your reported calculation, provide the row vector whose entries are the cardinalities of these selection sets in the order $\\lambda = 0.5$, $\\lambda = 1.0$, $\\lambda = 2.0$, and $\\lambda = 3.0$. No rounding is needed.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard problem in statistical learning theory concerning the properties of the LASSO estimator under the simplifying assumption of an orthonormal design matrix. I will now proceed with the solution.\n\nThe LASSO estimator $\\hat{\\beta}(\\lambda)$ is the solution to the optimization problem:\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\}\n$$\nThe objective function, let's call it $L(\\beta)$, consists of a differentiable quadratic part and a non-differentiable but convex $\\ell_1$-norm penalty. We can simplify the quadratic part using the given orthonormality condition $X^\\top X = I_p$, where $I_p$ is the $p \\times p$ identity matrix.\nLet's expand the squared $\\ell_2$-norm:\n$$\n\\| y - X \\beta \\|_{2}^{2} = (y - X \\beta)^\\top (y - X \\beta) = y^\\top y - 2 y^\\top X \\beta + \\beta^\\top X^\\top X \\beta\n$$\nSubstituting $X^\\top X = I_p$, the expression becomes:\n$$\n\\| y - X \\beta \\|_{2}^{2} = y^\\top y - 2 (X^\\top y)^\\top \\beta + \\beta^\\top I_p \\beta = \\|y\\|_2^2 - 2 z^\\top \\beta + \\|\\beta\\|_2^2\n$$\nwhere we have used the definition $z := X^\\top y$.\n\nThe objective function $L(\\beta)$ can now be written as:\n$$\nL(\\beta) = \\frac{1}{2} \\left( \\|y\\|_2^2 - 2 z^\\top \\beta + \\|\\beta\\|_2^2 \\right) + \\lambda \\| \\beta \\|_{1}\n$$\nTo find the minimum, we can ignore the constant term $\\frac{1}{2}\\|y\\|_2^2$, making the optimization problem equivalent to minimizing:\n$$\n\\tilde{L}(\\beta) = \\frac{1}{2} \\|\\beta\\|_2^2 - z^\\top \\beta + \\lambda \\| \\beta \\|_{1}\n$$\nThis objective function is separable coordinate-wise. We can write it as a sum over the components $j=1, \\dots, p$:\n$$\n\\tilde{L}(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2} \\beta_j^2 - z_j \\beta_j + \\lambda |\\beta_j| \\right)\n$$\nTo minimize $\\tilde{L}(\\beta)$, we can minimize each term in the summation with respect to its corresponding $\\beta_j$ independently. Let $f_j(\\beta_j) = \\frac{1}{2} \\beta_j^2 - z_j \\beta_j + \\lambda |\\beta_j|$.\nSince $|\\beta_j|$ is not differentiable at $\\beta_j=0$, we use subgradient calculus. The first-order optimality condition states that $0$ must be in the subgradient of $f_j(\\beta_j)$ at the minimum $\\hat{\\beta}_j$.\nThe subgradient of $f_j(\\beta_j)$ is given by:\n$$\n\\partial f_j(\\beta_j) = \\beta_j - z_j + \\lambda \\cdot \\partial(|\\beta_j|)\n$$\nThe subgradient of the absolute value function $|\\beta_j|$ is:\n$$\n\\partial(|\\beta_j|) = \\begin{cases} \\{1\\} & \\text{if } \\beta_j > 0 \\\\ \\{-1\\} & \\text{if } \\beta_j < 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\nThe optimality condition $0 \\in \\partial f_j(\\hat{\\beta}_j)$ translates to $z_j - \\hat{\\beta}_j \\in \\lambda \\cdot \\partial(|\\hat{\\beta}_j|)$. We analyze this condition in three cases for the optimal value $\\hat{\\beta}_j$:\n\nCase 1: $\\hat{\\beta}_j > 0$.\nThe subgradient $\\partial(|\\hat{\\beta}_j|)$ is $\\{1\\}$. The condition becomes $z_j - \\hat{\\beta}_j = \\lambda$, which yields $\\hat{\\beta}_j = z_j - \\lambda$. For this solution to be consistent with the assumption $\\hat{\\beta}_j > 0$, we must have $z_j - \\lambda > 0$, or $z_j > \\lambda$.\n\nCase 2: $\\hat{\\beta}_j < 0$.\nThe subgradient $\\partial(|\\hat{\\beta}_j|)$ is $\\{-1\\}$. The condition becomes $z_j - \\hat{\\beta}_j = -\\lambda$, which yields $\\hat{\\beta}_j = z_j + \\lambda$. For this solution to be consistent with the assumption $\\hat{\\beta}_j < 0$, we must have $z_j + \\lambda < 0$, or $z_j < -\\lambda$.\n\nCase 3: $\\hat{\\beta}_j = 0$.\nThe subgradient $\\partial(|\\hat{\\beta}_j|)$ is the interval $[-1, 1]$. The condition becomes $z_j - 0 \\in \\lambda \\cdot [-1, 1]$, which is $z_j \\in [-\\lambda, \\lambda]$, or $|z_j| \\le \\lambda$.\n\nCombining these three cases gives the coordinate-wise characterization of the LASSO solution $\\hat{\\beta}(\\lambda)$:\n$$\n\\hat{\\beta}_j(\\lambda) = \\begin{cases} z_j - \\lambda & \\text{if } z_j > \\lambda \\\\ 0 & \\text{if } |z_j| \\le \\lambda \\\\ z_j + \\lambda & \\text{if } z_j < -\\lambda \\end{cases}\n$$\nThis is the soft-thresholding operator, which can be written compactly as $\\hat{\\beta}_j(\\lambda) = \\text{sign}(z_j) (|z_j| - \\lambda)_+$, where $(x)_+ = \\max(x, 0)$.\n\nFrom this characterization, we can state the necessary and sufficient condition for a coordinate $\\hat{\\beta}_j(\\lambda)$ to be nonzero. The coefficient $\\hat{\\beta}_j(\\lambda)$ is nonzero if and only if we are in Case 1 ($z_j > \\lambda$) or Case 2 ($z_j < -\\lambda$). These two conditions can be combined into a single condition:\n$$\n|z_j| > \\lambda\n$$\nThis is the necessary and sufficient condition for $\\hat{\\beta}_j(\\lambda) \\neq 0$.\n\nNow, we apply this to the specific problem. The vector $z = X^\\top y$ has components:\n$z_1 = 2.7$, $z_2 = -1.3$, $z_3 = 0.9$, $z_4 = 3.4$, $z_5 = -0.2$, $z_6 = -2.1$.\nLet's find the magnitudes of these components:\n$|z_1| = 2.7$, $|z_2| = 1.3$, $|z_3| = 0.9$, $|z_4| = 3.4$, $|z_5| = 0.2$, $|z_6| = 2.1$.\n\nWe need to determine the selection set $S_\\lambda = \\{ j : \\hat{\\beta}_{j}(\\lambda) \\neq 0 \\} = \\{ j : |z_j| > \\lambda \\}$ for $\\lambda \\in \\{0.5, 1.0, 2.0, 3.0\\}$.\n\nFor $\\lambda = 0.5$:\nWe check which $|z_j| > 0.5$.\n$|z_1| = 2.7 > 0.5$\n$|z_2| = 1.3 > 0.5$\n$|z_3| = 0.9 > 0.5$\n$|z_4| = 3.4 > 0.5$\n$|z_5| = 0.2 \\ngtr 0.5$\n$|z_6| = 2.1 > 0.5$\nThe selection set is $S_{0.5} = \\{1, 2, 3, 4, 6\\}$. The cardinality is $|S_{0.5}| = 5$.\n\nFor $\\lambda = 1.0$:\nWe check which $|z_j| > 1.0$.\n$|z_1| = 2.7 > 1.0$\n$|z_2| = 1.3 > 1.0$\n$|z_3| = 0.9 \\ngtr 1.0$\n$|z_4| = 3.4 > 1.0$\n$|z_5| = 0.2 \\ngtr 1.0$\n$|z_6| = 2.1 > 1.0$\nThe selection set is $S_{1.0} = \\{1, 2, 4, 6\\}$. The cardinality is $|S_{1.0}| = 4$.\n\nFor $\\lambda = 2.0$:\nWe check which $|z_j| > 2.0$.\n$|z_1| = 2.7 > 2.0$\n$|z_2| = 1.3 \\ngtr 2.0$\n$|z_3| = 0.9 \\ngtr 2.0$\n$|z_4| = 3.4 > 2.0$\n$|z_5| = 0.2 \\ngtr 2.0$\n$|z_6| = 2.1 > 2.0$\nThe selection set is $S_{2.0} = \\{1, 4, 6\\}$. The cardinality is $|S_{2.0}| = 3$.\n\nFor $\\lambda = 3.0$:\nWe check which $|z_j| > 3.0$.\n$|z_1| = 2.7 \\ngtr 3.0$\n$|z_2| = 1.3 \\ngtr 3.0$\n$|z_3| = 0.9 \\ngtr 3.0$\n$|z_4| = 3.4 > 3.0$\n$|z_5| = 0.2 \\ngtr 3.0$\n$|z_6| = 2.1 \\ngtr 3.0$\nThe selection set is $S_{3.0} = \\{4\\}$. The cardinality is $|S_{3.0}| = 1$.\n\nThe problem asks for the row vector of these cardinalities, in the order of the given $\\lambda$ values. The sequence of cardinalities is $5$, $4$, $3$, $1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 4 & 3 & 1 \\end{pmatrix}}\n$$", "id": "3191263"}, {"introduction": "Moving from theory to practice, we confront a crucial aspect of applying LASSO: its sensitivity to the scale of the predictors. Unlike methods like ordinary least squares, the LASSO penalty is not scale-invariant, meaning that predictors with larger variance can be unfairly penalized. This hands-on coding exercise demonstrates how variable selection can be skewed by unstandardized data and reinforces why standardizing predictors to have a common scale is an essential step in the data preprocessing pipeline [@problem_id:3191255].", "problem": "Consider the linear model $y = X \\beta + \\varepsilon$ with $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times p}$, and $\\beta \\in \\mathbb{R}^p$. The Least Absolute Shrinkage and Selection Operator (Lasso) estimator $\\hat{\\beta}$ is defined as the minimizer of the objective\n$$\n\\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\lVert \\cdot \\rVert_1$ denotes the sum of absolute values. The Lasso’s “variable selection” property refers to the phenomenon that the solution $\\hat{\\beta}$ can have entries that are exactly zero, thereby selecting a subset of predictors.\n\nTask: Investigate the effect of scaling of predictors on Lasso’s variable selection. Specifically, compare the variables selected by the Lasso when predictors are not standardized versus when predictors are standardized (centered to mean zero and scaled to unit empirical standard deviation).\n\nStarting from the above definition, implement a general algorithm to compute $\\hat{\\beta}$ by optimizing the Lasso objective for a given design matrix $X$, response vector $y$, and regularization parameter $\\lambda$. Use a coordinate-wise iterative approach that does not rely on any closed-form “shortcut” for the solution.\n\nFor all test cases below, use $p=3$ predictors and $n=90$ samples. Construct $X$ as follows: partition the $n$ samples into three disjoint blocks of equal size $30$. For predictor $j \\in \\{1,2,3\\}$, fill its values in block $j$ by drawing independently from a normal distribution with mean $0$ and standard deviation $s_j$, and set its values outside block $j$ to $0$. This produces three predictors that are mutually orthogonal and have different empirical variances determined by $\\{s_j\\}$. Use a pseudo-random number generator with a fixed seed of $0$ for reproducibility. For each test case, set $\\varepsilon = 0$ and $y = X \\beta$ exactly.\n\nFor each test case:\n- Compute $\\hat{\\beta}_{\\text{unstd}}$ using the original $X$ (unstandardized).\n- Compute $\\hat{\\beta}_{\\text{std}}$ using $X$ standardized column-wise to mean $0$ and unit empirical standard deviation.\n- Define the selected-variable set as the list of indices $j \\in \\{1,2,3\\}$ for which $|\\hat{\\beta}_j| > \\tau$ with tolerance $\\tau = 10^{-8}$.\n- Report both selected-variable sets using $1$-based indexing (i.e., predictors are labeled $1,2,3$).\n- Also report a boolean indicating whether the two sets differ.\n\nUse the following four test cases, each with $n=90$:\n1. Balanced variances (happy path): $s = [1.0, 1.0, 1.0]$, $\\beta = [0.4, 0.2, 0.06]$, $\\lambda = 0.05$.\n2. Large scale noise effect (unstandardized can select a small-coefficient large-variance predictor): $s = [0.5, 3.0, 1.0]$, $\\beta = [0.2, 0.05, 0.15]$, $\\lambda = 0.10$.\n3. Important small-scale predictor (unstandardized can drop an important predictor with small scale, standardized retains it): $s = [0.2, 1.0, 1.0]$, $\\beta = [0.6, 0.0, 0.0]$, $\\lambda = 0.05$.\n4. Boundary condition (very large regularization makes all coefficients zero): $s = [2.0, 0.5, 1.5]$, $\\beta = [1.0, 0.3, 0.7]$, $\\lambda = 10.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list consisting of: the selected-variable list for the unstandardized design, the selected-variable list for the standardized design, and the boolean indicating whether the sets differ. For example, a valid output format is\n$$\n\\text{[ [ [1,2], [1,2], False ], [ [2], [], True ], \\dots ]}\n$$\nwhere the inner lists contain integers and the boolean is either $\\text{True}$ or $\\text{False}$.", "solution": "We begin from the lasso objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\}.\n$$\nThis is a convex optimization problem, and one principled way to compute $\\hat{\\beta}$ is to use coordinate-wise optimization guided by subgradient calculus. The central observation is that the squared-error term is differentiable, whereas the absolute-value penalty is not differentiable at $0$ but has a well-defined subdifferential. The Karush–Kuhn–Tucker (KKT) conditions provide the necessary optimality criteria. For coordinate-wise updates, we consider the partial residual when updating coordinate $j$:\n$$\nr_{(j)} = y - \\sum_{k \\ne j} X_k \\beta_k,\n$$\nwhere $X_k$ is the $k$-th column of $X$. Optimizing the objective with respect to $\\beta_j$ while keeping other coordinates fixed reduces to minimizing a one-dimensional convex function in $\\beta_j$. The subgradient condition for the $j$-th coordinate is\n$$\n\\frac{1}{n} X_j^\\top \\left( y - \\sum_{k=1}^p X_k \\beta_k \\right) + \\lambda \\cdot g_j = 0,\n$$\nwhere $g_j \\in \\partial |\\beta_j|$ with $g_j = \\operatorname{sign}(\\beta_j)$ if $\\beta_j \\ne 0$ and $g_j \\in [-1,1]$ if $\\beta_j = 0$. Rearranging with the partial residual $r_{(j)}$ yields\n$$\n\\frac{1}{n} X_j^\\top r_{(j)} - \\frac{1}{n} X_j^\\top X_j \\beta_j + \\lambda \\cdot g_j = 0.\n$$\nDefine the scalars\n$$\np_j := \\frac{1}{n} X_j^\\top r_{(j)}, \\quad z_j := \\frac{1}{n} X_j^\\top X_j.\n$$\nThe coordinate-wise optimality simplifies to solving\n$$\n\\min_{\\beta_j \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} z_j \\beta_j^2 - p_j \\beta_j + \\lambda |\\beta_j| \\right\\}.\n$$\nBy completing the square and applying subgradient calculus, the solution is given by the soft-thresholding operator:\n$$\n\\beta_j \\leftarrow \\frac{1}{z_j} \\cdot \\operatorname{Soft}(p_j, \\lambda),\n$$\nwhere $\\operatorname{Soft}(a,t) = \\operatorname{sign}(a)\\cdot \\max\\{|a| - t, 0\\}$. This update is iterated across $j=1,\\dots,p$ repeatedly until convergence (for example, until the maximum coordinate change is below a small tolerance). The algorithm is a form of cyclic coordinate descent and is guaranteed to converge for this convex objective.\n\nWe now analyze the effect of scaling. Suppose we scale predictor $j$ by a factor $\\alpha_j > 0$, i.e., $\\tilde{X}_j = \\alpha_j X_j$. Then\n$$\n\\tilde{p}_j = \\frac{1}{n} \\tilde{X}_j^\\top r_{(j)} = \\alpha_j \\cdot \\frac{1}{n} X_j^\\top r_{(j)} = \\alpha_j p_j,\n$$\nand\n$$\n\\tilde{z}_j = \\frac{1}{n} \\tilde{X}_j^\\top \\tilde{X}_j = \\alpha_j^2 \\cdot \\frac{1}{n} X_j^\\top X_j = \\alpha_j^2 z_j.\n$$\nThe coordinate update becomes\n$$\n\\tilde{\\beta}_j \\leftarrow \\frac{1}{\\alpha_j^2 z_j} \\cdot \\operatorname{Soft}(\\alpha_j p_j, \\lambda).\n$$\nThe selection of variable $j$ (i.e., whether $\\tilde{\\beta}_j = 0$ or not) depends on whether $|\\alpha_j p_j|$ exceeds $\\lambda$; clearly, scaling changes the effective threshold crossing because $p_j$ scales linearly with $\\alpha_j$. When predictors are standardized to unit empirical standard deviation, the scaling factors are absorbed into the standardized design, making the across-variable comparisons of $p_j$ more balanced.\n\nIn our construction, predictors have disjoint support across three blocks, making them mutually orthogonal, i.e., $X_j^\\top X_k = 0$ for $j \\ne k$. Consider the noiseless case with $y = X \\beta$. Then\n$$\np_j = \\frac{1}{n} X_j^\\top r_{(j)} = \\frac{1}{n} X_j^\\top y = \\frac{1}{n} X_j^\\top X_j \\beta_j = z_j \\beta_j.\n$$\nHence the selection criterion $|\\operatorname{Soft}(p_j,\\lambda)| > 0$ reduces to $|p_j| > \\lambda$, i.e.,\n$$\n|z_j \\beta_j| > \\lambda.\n$$\nFor unstandardized predictors, $z_j$ reflects their empirical scale; those with larger $z_j$ are more likely to be selected at the same $\\lambda$, even if $|\\beta_j|$ is small. After standardization to unit empirical variance (so that $z_j \\approx 1$), the selection condition becomes approximately $|\\beta_j| > \\lambda$, equalizing the threshold across predictors and potentially changing the selected set.\n\nAlgorithm design:\n- Generate $X$ with the specified block-normal construction and fixed seed to ensure reproducibility.\n- Form $y = X \\beta$ with $\\varepsilon = 0$ to isolate the scaling effect.\n- Implement cyclic coordinate descent using the soft-threshold update derived from the subgradient/Karush–Kuhn–Tucker analysis.\n- Standardize $X$ by subtracting column means and dividing by column empirical standard deviations (with a safeguard for zero standard deviation).\n- Run the lasso solver on both $X$ and standardized $X$, then determine selected variables as those with $|\\hat{\\beta}_j| > \\tau$ for $\\tau = 10^{-8}$.\n- Report $1$-based indices for both selections and a boolean indicating whether the sets differ.\n\nCoverage in the test suite:\n- Balanced variances: both selections coincide because $z_j$ are equal across $j$.\n- Large scale noise effect: a small-coefficient predictor with large $z_j$ can be selected in the unstandardized case but not after standardization.\n- Important small-scale predictor: the unstandardized case can drop a truly important predictor when $z_j$ is small relative to $\\lambda$, whereas standardization retains it.\n- Boundary condition: a very large $\\lambda$ yields the empty model in both cases.\n\nThe final program follows these principles and prints a single line containing, for each test case, the unstandardized selected indices, the standardized selected indices, and the boolean difference.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(a, t):\n    if a > t:\n        return a - t\n    elif a < -t:\n        return a + t\n    else:\n        return 0.0\n\ndef lasso_coordinate_descent(X, y, lam, max_iter=10000, tol=1e-12):\n    \"\"\"\n    Solve the lasso problem:\n      (1/(2n)) ||y - X b||_2^2 + lam ||b||_1\n    using cyclic coordinate descent with soft-thresholding.\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p, dtype=float)\n    # Residual r = y - X b; with b=0 initially, r=y\n    r = y.copy()\n    for _ in range(max_iter):\n        b_old = b.copy()\n        for j in range(p):\n            xj = X[:, j]\n            # Add back the old contribution of coordinate j to residual\n            r += xj * b[j]\n            # Compute p_j and z_j\n            p_j = np.dot(xj, r) / n\n            z_j = np.dot(xj, xj) / n\n            # Update via soft-threshold\n            b[j] = soft_threshold(p_j, lam) / (z_j if z_j > 0 else 1.0)\n            # Subtract the new contribution\n            r -= xj * b[j]\n        # Convergence check\n        if np.max(np.abs(b - b_old)) < tol:\n            break\n    return b\n\ndef standardize_columns(X):\n    \"\"\"\n    Center columns to mean zero and scale to unit empirical std (ddof=0).\n    Returns standardized X, along with means and stds.\n    \"\"\"\n    means = X.mean(axis=0)\n    Xc = X - means\n    stds = Xc.std(axis=0, ddof=0)\n    stds_safe = np.where(stds == 0, 1.0, stds)\n    Xstd = Xc / stds_safe\n    return Xstd, means, stds_safe\n\ndef make_block_X(n, s):\n    \"\"\"\n    Construct X with 3 predictors, each active on a disjoint block of length n//3,\n    with values ~ N(0, s_j) in its block and zeros elsewhere.\n    \"\"\"\n    assert n % 3 == 0\n    m = n // 3\n    rng = np.random.default_rng(0)\n    X = np.zeros((n, 3), dtype=float)\n    for j in range(3):\n        start = j * m\n        end = (j + 1) * m\n        X[start:end, j] = rng.normal(0.0, s[j], size=m)\n    return X\n\ndef selected_indices(b, tol=1e-8):\n    # 1-based indexing for output\n    return [i + 1 for i, bi in enumerate(b) if abs(bi) > tol]\n\ndef run_case(n, s, beta_true, lam):\n    X = make_block_X(n, s)\n    y = X @ np.array(beta_true, dtype=float)\n    # Unstandardized lasso\n    b_unstd = lasso_coordinate_descent(X, y, lam)\n    sel_unstd = selected_indices(b_unstd)\n    # Standardized lasso\n    X_std, _, _ = standardize_columns(X)\n    b_std = lasso_coordinate_descent(X_std, y, lam)\n    sel_std = selected_indices(b_std)\n    differ = (sel_unstd != sel_std)\n    return [sel_unstd, sel_std, differ]\n\ndef solve():\n    test_cases = [\n        # 1. Balanced variances\n        {\"n\": 90, \"s\": [1.0, 1.0, 1.0], \"beta\": [0.4, 0.2, 0.06], \"lam\": 0.05},\n        # 2. Large scale noise effect: unstandardized selects a large-scale small-coefficient predictor\n        {\"n\": 90, \"s\": [0.5, 3.0, 1.0], \"beta\": [0.2, 0.05, 0.15], \"lam\": 0.10},\n        # 3. Important small-scale predictor: unstandardized drops it, standardized retains it\n        {\"n\": 90, \"s\": [0.2, 1.0, 1.0], \"beta\": [0.6, 0.0, 0.0], \"lam\": 0.05},\n        # 4. Boundary: very large lambda\n        {\"n\": 90, \"s\": [2.0, 0.5, 1.5], \"beta\": [1.0, 0.3, 0.7], \"lam\": 10.0},\n    ]\n    results = []\n    for case in test_cases:\n        res = run_case(case[\"n\"], case[\"s\"], case[\"beta\"], case[\"lam\"])\n        results.append(res)\n    print(f\"{results}\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3191255"}, {"introduction": "Real-world data often contains groups of highly correlated predictors, a situation known as multicollinearity, which poses a challenge for many statistical models. This practice explores how LASSO behaves in this scenario, often arbitrarily selecting one predictor from a correlated group while ignoring the others. You will then implement and contrast this with the Elastic Net, a popular extension that incorporates an $\\ell_2$ penalty to encourage a \"grouping effect,\" providing a more stable and often more interpretable model when predictors are not independent [@problem_id:3191214].", "problem": "Consider a linear regression model with grouped, highly correlated predictors. Let $n$ denote the number of observations and $p$ denote the number of predictors. Assume the data are generated from the model $y = X \\beta + \\varepsilon$ where $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^{p}$, and the noise $\\varepsilon \\in \\mathbb{R}^{n}$ is independent and identically distributed (IID) with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. The predictors are organized into two disjoint groups of equal size, each of size $3$, such that the predictors within each group have a common pairwise correlation parameter $\\rho \\in [0,1)$.\n\nYour task is to write a complete program that:\n- Implements least squares regression penalized by the $\\ell_1$ norm, known as the Least Absolute Shrinkage and Selection Operator (LASSO), by solving the optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\nand implements the Elastic Net (EN), which adds a quadratic penalty, by solving\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{\\lambda_2}{2} \\|\\beta\\|_2^2.\n$$\n- Uses a principled iterative algorithm that is guaranteed for convex objectives, such as coordinate descent with soft-thresholding, and works on standardized features. The algorithm should not rely on any external machine learning library.\n- Constructs synthetic predictor matrices $X$ with two groups of $3$ predictors each, using a scientifically sound correlated construction. Specifically, for each group, let $z \\in \\mathbb{R}^n$ be a latent factor with $z \\sim \\mathcal{N}(0, I_n)$ and for each predictor $j$ in the group, let $\\epsilon_j \\sim \\mathcal{N}(0, I_n)$ be independent of $z$. Define each predictor by\n$x_j = \\sqrt{\\rho} \\, z + \\sqrt{1-\\rho} \\, \\epsilon_j$,\nthen standardize each column by subtracting its sample mean and scaling so that $(1/n) \\sum_{i=1}^n x_{ij}^2 = 1$ holds exactly. Concatenate the two groups to form $X \\in \\mathbb{R}^{n \\times 6}$.\n- Generates responses by $y = X \\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n- For each fitted method, defines the selected variables as those indices $j$ with $|\\hat{\\beta}_j| > \\tau$, for a fixed threshold $\\tau = 10^{-3}$.\n- Reports, per group, the number of selected variables (the count of $j$ in the group where $|\\hat{\\beta}_j| > \\tau$), and uses these counts to evaluate properties:\n    1. Situations where LASSO picks one representative per highly correlated group.\n    2. Situations where LASSO misses an entire weak-signal group.\n    3. Contrast with Elastic Net, which tends to select multiple correlated predictors within a group due to the quadratic penalty.\n\nUse the following test suite with fixed seeds to ensure reproducibility. For each test case, construct data and fit both LASSO and Elastic Net using the specified parameters. In all cases, express $n$, $\\rho$, $\\sigma$, and penalty levels in their given numerical values.\n\nTest Suite:\n- Test Case $1$ (happy path: strong group and weak group, both highly correlated):\n    - $n = 800$\n    - Group $1$ correlation $\\rho_1 = 0.9$, Group $2$ correlation $\\rho_2 = 0.9$\n    - True coefficients: Group $1$ has $\\beta = (1, 1, 1)$, Group $2$ has $\\beta = (0.2, 0.2, 0.2)$\n    - Noise standard deviation $\\sigma = 0.1$\n    - LASSO penalty $\\lambda = 0.25$\n    - Elastic Net penalties $\\lambda_1 = 0.15$, $\\lambda_2 = 0.1$\n    - Random seed $123$\n    - Boolean evaluations to output for this case:\n        - $b_{1,1}$: LASSO selects exactly one variable in Group $1$.\n        - $b_{1,2}$: LASSO selects zero variables in Group $2$.\n        - $b_{1,3}$: Elastic Net selects at least two variables in Group $1$.\n        - $b_{1,4}$: Elastic Net selects at least one variable in Group $2$.\n- Test Case $2$ (edge case: weak-signal correlated group, and a null group):\n    - $n = 800$\n    - Group $1$ correlation $\\rho_1 = 0.95$, Group $2$ correlation $\\rho_2 = 0.95$\n    - True coefficients: Group $1$ has $\\beta = (0, 0, 0)$, Group $2$ has $\\beta = (0.3, 0.3, 0.3)$\n    - Noise standard deviation $\\sigma = 0.1$\n    - LASSO penalty $\\lambda = 0.30$\n    - Elastic Net penalties $\\lambda_1 = 0.18$, $\\lambda_2 = 0.12$\n    - Random seed $456$\n    - Boolean evaluations to output for this case:\n        - $b_{2,1}$: LASSO selects zero variables in Group $2$.\n        - $b_{2,2}$: Elastic Net selects at least one variable in Group $2$.\n- Test Case $3$ (boundary: uncorrelated predictors):\n    - $n = 800$\n    - Group $1$ correlation $\\rho_1 = 0.0$, Group $2$ correlation $\\rho_2 = 0.0$\n    - True coefficients: Group $1$ has $\\beta = (1, 1, 1)$, Group $2$ has $\\beta = (0.2, 0.2, 0.2)$\n    - Noise standard deviation $\\sigma = 0.1$\n    - LASSO penalty $\\lambda = 0.25$\n    - Elastic Net penalties $\\lambda_1 = 0.15$, $\\lambda_2 = 0.1$\n    - Random seed $789$\n    - Boolean evaluation to output for this case:\n        - $b_{3,1}$: LASSO and Elastic Net have identical selection counts for both groups (the counts per group are equal for the two methods).\n\nFinal Output Format:\n- Your program should produce a single line of output containing all boolean results in the order\n$$\n[b_{1,1}, b_{1,2}, b_{1,3}, b_{1,4}, b_{2,1}, b_{2,2}, b_{3,1}],\n$$\nenclosed in square brackets, with comma-separated values (e.g., \"[True,False,True,...]\").", "solution": "The problem requires the implementation and comparison of two regularized linear regression models, LASSO and Elastic Net, to demonstrate their variable selection properties in the presence of highly correlated predictors. The validation process confirms that the problem is scientifically sound, well-posed, and provides all necessary specifications for a reproducible numerical experiment.\n\n### 1. Problem Formulation and Data Generation\n\nWe consider a linear model $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the predictor matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is IID Gaussian noise with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. The number of predictors is $p=6$, organized into two groups of size $3$.\n\nThe predictor matrix $X$ is constructed to have a specific correlation structure within each group. For a group with a specified pairwise correlation $\\rho$, each predictor column $x_j$ is generated via a latent factor model:\n$$\nx_j = \\sqrt{\\rho} \\, z + \\sqrt{1-\\rho} \\, \\epsilon_j\n$$\nwhere $z \\in \\mathbb{R}^n$ is a shared latent factor with $z \\sim \\mathcal{N}(0, I_n)$, and $\\epsilon_j \\in \\mathbb{R}^n$ are independent idiosyncratic noise terms, also distributed as $\\mathcal{N}(0, I_n)$. This construction ensures that the theoretical correlation between any two predictors $x_j$ and $x_k$ within the same group is $\\text{Cov}(x_j, x_k) = \\rho$. The two groups are generated independently. After construction, the concatenated matrix $X \\in \\mathbb{R}^{n \\times 6}$ is standardized such that each column has a sample mean of $0$ and a sample second moment of $1$, i.e., $(1/n) \\sum_{i=1}^n x_{ij}^2 = 1$.\n\n### 2. Regularized Regression Models and Algorithm\n\nThe task is to implement two related models:\n\n**LASSO (Least Absolute Shrinkage and Selection Operator)** solves the optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\nThe $\\ell_1$-norm penalty, $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$, induces sparsity, forcing some coefficients to be exactly zero.\n\n**Elastic Net (EN)** adds an $\\ell_2$-norm penalty to the LASSO objective:\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{\\lambda_2}{2} \\|\\beta\\|_2^2\n$$\nThe quadratic penalty, $\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2$, encourages a \"grouping effect,\" where highly correlated predictors are selected or discarded together.\n\nWe solve these convex optimization problems using **coordinate descent**. The algorithm iteratively updates each coefficient $\\beta_j$ while holding all others fixed. For the Elastic Net objective, the update rule for the $j$-th coefficient is derived by minimizing the objective with respect to $\\beta_j$ alone. This yields a closed-form solution involving the soft-thresholding operator $S_\\alpha(z) = \\text{sgn}(z) \\max(|z|-\\alpha, 0)$. The update for $\\beta_j$ is:\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S_{\\lambda_1}(z_j)}{1 + \\lambda_2}\n$$\nwhere $z_j = \\frac{1}{n} X_j^T (y - \\sum_{k \\neq j} X_k \\beta_k)$ is the simple covariance between the $j$-th predictor and the partial residual. The LASSO update is a special case where $\\lambda_2=0$. The algorithm iterates through all coefficients until the change in the $\\beta$ vector becomes smaller than a specified tolerance. For this problem, we use standardized predictors and a centered response vector, which is standard practice for fitting intercept-less regularized models.\n\n### 3. Evaluation of Selection Properties\n\nThe implemented models are applied to three test cases with fixed parameters and random seeds to ensure reproducibility. For each model fit, a variable $j$ is considered \"selected\" if its estimated coefficient $\\hat{\\beta}_j$ has a magnitude greater than a small threshold, $|\\hat{\\beta}_j| > \\tau = 10^{-3}$. We then count the number of selected variables in each of the two predictor groups.\n\n- **Test Case 1** uses highly correlated predictors ($\\rho=0.9$) with one strong signal group ($\\beta_j=1$) and one weak signal group ($\\beta_j=0.2$). This setup is designed to show LASSO selecting a single representative from the strong group and ignoring the weak group entirely, while Elastic Net selects multiple predictors from the strong group and at least one from the weak group due to its grouping effect.\n- **Test Case 2** contrasts a null group ($\\beta_j=0$) with a very highly correlated ($\\rho=0.95$), weak signal group ($\\beta_j=0.3$). With a high penalty, LASSO is expected to miss the weak group, while the Elastic Net's grouping effect may help it identify the group's signal.\n- **Test Case 3** examines the case of uncorrelated predictors ($\\rho=0.0$). In this scenario, the grouping advantage of Elastic Net is nullified. The test evaluates whether the selection behavior of LASSO and Elastic Net becomes identical, contingent on the specific penalty parameters.\n\nThe program calculates the specified boolean evaluations for each case, which tests these expected behaviors and provides a quantitative result. The final output aggregates these boolean results into a single list.", "answer": "```python\nimport numpy as np\n\ndef generate_correlated_group(n, p_group, rho, rng):\n    \"\"\"Generates a group of correlated predictors using a given RNG.\"\"\"\n    # Latent common factor\n    z = rng.standard_normal(size=(n, 1))\n    # Independent idiosyncratic noise for each predictor\n    epsilon = rng.standard_normal(size=(n, p_group))\n    \n    # Construct predictors\n    X_group = np.sqrt(rho) * z + np.sqrt(1 - rho) * epsilon\n    return X_group\n\ndef generate_data(n, p_group, rho1, rho2, beta_true, sigma, seed):\n    \"\"\"Generates the full design matrix X and response y.\"\"\"\n    # Seed sequence for reproducible and independent random number generation\n    ss = np.random.SeedSequence(seed)\n    child_seeds = ss.spawn(3)\n    rng1, rng2, rng_noise = [np.random.default_rng(s) for s in child_seeds]\n\n    X1 = generate_correlated_group(n, p_group, rho1, rng1)\n    X2 = generate_correlated_group(n, p_group, rho2, rng2)\n    \n    # Concatenate to form the full design matrix\n    X = np.hstack((X1, X2))\n    \n    # Standardize X: mean 0 and sample variance 1.\n    # This satisfies (1/n) * sum(x_ij^2) = 1 because mean is 0.\n    X_mean = X.mean(axis=0)\n    X_std = X.std(axis=0, ddof=0)\n    # Avoid division by zero if a column is constant\n    X_std[X_std == 0] = 1.0\n    X_standardized = (X - X_mean) / X_std\n    \n    # Generate noise and response y\n    epsilon = rng_noise.normal(0, sigma, size=n)\n    y = X_standardized @ beta_true + epsilon\n    \n    return X_standardized, y\n\ndef fit_elastic_net(X, y, lambda1, lambda2, max_iter=2000, tol=1e-6):\n    \"\"\"\n    Fits an Elastic Net model using coordinate descent.\n    LASSO is a special case where lambda2 = 0.\n    \"\"\"\n    n, p = X.shape\n    \n    # Center y for intercept-less model\n    y_centered = y - y.mean()\n    \n    beta = np.zeros(p)\n    \n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        \n        for j in range(p):\n            # Calculate partial residual r_j = y - sum_{k!=j} X_k beta_k\n            r_j = y_centered - (X @ beta - X[:, j] * beta[j])\n            \n            # Argument to the soft-thresholding function\n            z_j = (X[:, j] @ r_j) / n\n            \n            # Apply the soft-thresholding update for Elastic Net\n            if z_j > lambda1:\n                beta[j] = (z_j - lambda1) / (1 + lambda2)\n            elif z_j < -lambda1:\n                beta[j] = (z_j + lambda1) / (1 + lambda2)\n            else:\n                beta[j] = 0.0\n        \n        # Check for convergence\n        if np.max(np.abs(beta - beta_old)) < tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.9, \"rho2\": 0.9,\n            \"beta_true\": np.array([1, 1, 1, 0.2, 0.2, 0.2]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.25,\n            \"en_lambda1\": 0.15, \"en_lambda2\": 0.1,\n            \"seed\": 123\n        },\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.95, \"rho2\": 0.95,\n            \"beta_true\": np.array([0, 0, 0, 0.3, 0.3, 0.3]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.30,\n            \"en_lambda1\": 0.18, \"en_lambda2\": 0.12,\n            \"seed\": 456\n        },\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.0, \"rho2\": 0.0,\n            \"beta_true\": np.array([1, 1, 1, 0.2, 0.2, 0.2]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.25,\n            \"en_lambda1\": 0.15, \"en_lambda2\": 0.1,\n            \"seed\": 789\n        }\n    ]\n\n    all_results = []\n    tau = 1e-3\n    p_group = 3\n\n    # Test Case 1\n    case = test_cases[0]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g1 = np.sum(selected_lasso[:p_group])\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g1 = np.sum(selected_en[:p_group])\n    count_en_g2 = np.sum(selected_en[p_group:])\n\n    b_1_1 = (count_lasso_g1 == 1)\n    b_1_2 = (count_lasso_g2 == 0)\n    b_1_3 = (count_en_g1 >= 2)\n    b_1_4 = (count_en_g2 >= 1)\n    all_results.extend([b_1_1, b_1_2, b_1_3, b_1_4])\n\n    # Test Case 2\n    case = test_cases[1]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g2 = np.sum(selected_en[p_group:])\n    \n    b_2_1 = (count_lasso_g2 == 0)\n    b_2_2 = (count_en_g2 >= 1)\n    all_results.extend([b_2_1, b_2_2])\n\n    # Test Case 3\n    case = test_cases[2]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g1 = np.sum(selected_lasso[:p_group])\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g1 = np.sum(selected_en[:p_group])\n    count_en_g2 = np.sum(selected_en[p_group:])\n\n    b_3_1 = (count_lasso_g1 == count_en_g1) and (count_lasso_g2 == count_en_g2)\n    all_results.append(b_3_1)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3191214"}]}