{"hands_on_practices": [{"introduction": "This first exercise provides a fundamental workout in the mechanics of logistic regression. Given a pre-fitted model, you will calculate the predicted probabilities of an event for several new data points and then compute the total deviance to assess how well the model's predictions match the observed outcomes. This practice is essential for building an intuition for the flow of information in a Generalized Linear Model (GLM)â€”from the linear predictor $\\eta$, through the logistic link function to generate a probability $p$, and finally to a key goodness-of-fit statistic [@problem_id:1930939].", "problem": "In a study on the effectiveness of a new pesticide, the outcome for an insect is binary: survival (denoted by $y=0$) or non-survival (denoted by $y=1$). The probability of non-survival, $p$, is modeled using a Generalized Linear Model (GLM) with a logistic link function, an approach commonly known as logistic regression. This model relates the probability $p$ to the concentration $x$ of the pesticide (in parts per million, ppm) via the equation for the log-odds:\n$$ \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x $$\nBased on extensive preliminary experiments, the fitted model parameters are determined to be $\\beta_0 = -1.50$ and $\\beta_1 = 0.80$.\n\nA new experiment is conducted on a small group of four insects, yielding the following data:\n- Insect 1: Concentration $x_1 = 1.0$ ppm, Outcome $y_1 = 0$ (survived)\n- Insect 2: Concentration $x_2 = 2.0$ ppm, Outcome $y_2 = 1$ (did not survive)\n- Insect 3: Concentration $x_3 = 3.0$ ppm, Outcome $y_3 = 1$ (did not survive)\n- Insect 4: Concentration $x_4 = 4.0$ ppm, Outcome $y_4 = 0$ (survived)\n\nThe total deviance of a model is a measure of how well the model fits the data. For a set of binary outcomes $y_i$ ($i=1, \\dots, n$) and their corresponding model-fitted probabilities $\\hat{p}_i$, the total deviance is given by the formula:\n$$ D = -2 \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right] $$\n\nCalculate the total deviance $D$ of the logistic regression model for this group of four insects. Round your final answer to four significant figures.", "solution": "The model uses the logistic link, so for each insect with concentration $x_{i}$, the linear predictor is $\\eta_{i}=\\beta_{0}+\\beta_{1}x_{i}$ and the fitted probability is $\\hat{p}_{i}=\\frac{1}{1+\\exp(-\\eta_{i})}$.\n\nGiven $\\beta_{0}=-1.50$ and $\\beta_{1}=0.80$, compute $\\eta_{i}$ and $\\hat{p}_{i}$:\n- Insect 1: $x_{1}=1.0$, $\\eta_{1}=-1.5+0.8(1)=-0.7$, so $\\hat{p}_{1}=\\frac{1}{1+\\exp(0.7)}\\approx 0.331812227$.\n- Insect 2: $x_{2}=2.0$, $\\eta_{2}=-1.5+0.8(2)=0.1$, so $\\hat{p}_{2}=\\frac{1}{1+\\exp(-0.1)}\\approx 0.524979188$.\n- Insect 3: $x_{3}=3.0$, $\\eta_{3}=-1.5+0.8(3)=0.9$, so $\\hat{p}_{3}=\\frac{1}{1+\\exp(-0.9)}\\approx 0.710949501$.\n- Insect 4: $x_{4}=4.0$, $\\eta_{4}=-1.5+0.8(4)=1.7$, so $\\hat{p}_{4}=\\frac{1}{1+\\exp(-1.7)}\\approx 0.845534734$.\n\nThe total deviance is\n$$\nD=-2\\sum_{i=1}^{4}\\left[y_{i}\\ln(\\hat{p}_{i})+(1-y_{i})\\ln(1-\\hat{p}_{i})\\right].\n$$\nCompute each term using the observed $y_{i}$:\n- Insect 1: $y_{1}=0$, contribution $-2\\ln(1-\\hat{p}_{1})=-2\\ln(0.668187773)\\approx 0.80637182$.\n- Insect 2: $y_{2}=1$, contribution $-2\\ln(\\hat{p}_{2})=-2\\ln(0.524979188)\\approx 1.28879255$.\n- Insect 3: $y_{3}=1$, contribution $-2\\ln(\\hat{p}_{3})=-2\\ln(0.710949501)\\approx 0.68230800$.\n- Insect 4: $y_{4}=0$, contribution $-2\\ln(1-\\hat{p}_{4})=-2\\ln(0.154465266)\\approx 3.73557206$.\n\nSum the contributions:\n$$\nD \\approx 0.80637182+1.28879255+0.68230800+3.73557206=6.51304443.\n$$\nRounded to four significant figures, the total deviance is $6.513$.", "answer": "$$\\boxed{6.513}$$", "id": "1930939"}, {"introduction": "A model's predictions should not depend on the units we use to measure our features, such as using meters instead of centimeters. This exercise explores the mathematical consequences of this principle by asking you to derive how logistic regression coefficients must transform under a linear rescaling of a predictor variable. By working through this problem, you will gain a deeper understanding of the interplay between the slope and intercept coefficients and see firsthand how their values are tied to the scale of your data [@problem_id:3185465].", "problem": "Consider a binary classification model using the logistic function with the logit link: for a feature vector $x = (x_{1}, \\dots, x_{d})$, the conditional probability of class $1$ is $p(x) = \\frac{1}{1 + \\exp(-\\eta(x))}$, where the linear predictor is $\\eta(x) = \\beta_{0} + \\sum_{k=1}^{d} \\beta_{k} x_{k}$. You decide to change the measurement units of a single feature $x_{j}$ by defining a rescaled feature $x_{j}^{\\text{new}} = \\frac{x_{j} - u}{v}$, where $u$ is a fixed real constant and $v$ is a fixed positive real constant. All other features $x_{k}$ for $k \\neq j$ are left unchanged.\n\nYou want the predicted probabilities $p(x)$ to remain exactly the same for every input after this rescaling, while expressing the model in terms of the transformed feature $x_{j}^{\\text{new}}$. That is, you seek new coefficients $\\tilde{\\beta}_{0}$, $\\tilde{\\beta}_{j}$, and $\\tilde{\\beta}_{k}$ for $k \\neq j$ such that the transformed linear predictor $\\tilde{\\eta}(x) = \\tilde{\\beta}_{0} + \\tilde{\\beta}_{j} x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\tilde{\\beta}_{k} x_{k}$ satisfies $p(x) = \\frac{1}{1 + \\exp(-\\tilde{\\eta}(x))}$ for all $x$. Express the pair $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$ in closed form in terms of $\\beta_{0}$, $\\beta_{j}$, $u$, and $v$. Provide your final answer as a row matrix using the LaTeX pmatrix environment with entries in the order $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$.", "solution": "The problem requires finding the new coefficients $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$ for a logistic regression model after a linear transformation of a single feature, $x_{j}$, such that the predicted probabilities remain unchanged for all input vectors $x$.\n\nThe conditional probability of class $1$ is given by the logistic function, $p(x) = \\sigma(\\eta(x))$, where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ and $\\eta(x)$ is the linear predictor. The logistic function $\\sigma(z)$ is a strictly monotonically increasing function of its argument $z$. Therefore, for the predicted probabilities to remain identical for any given input vector $x$ after transforming the feature $x_{j}$, the value of the linear predictor must also remain unchanged. This means the original linear predictor, $\\eta(x)$, must be equal to the new linear predictor, $\\tilde{\\eta}(x)$, for all $x$.\n\nThe original linear predictor is given by:\n$$ \\eta(x) = \\beta_{0} + \\sum_{k=1}^{d} \\beta_{k} x_{k} = \\beta_{0} + \\beta_{j} x_{j} + \\sum_{k \\neq j, k=1}^{d} \\beta_{k} x_{k} $$\n\nThe new feature $x_{j}^{\\text{new}}$ is defined by the transformation:\n$$ x_{j}^{\\text{new}} = \\frac{x_{j} - u}{v} $$\nwhere $u$ is a real constant and $v$ is a positive real constant. To express the original predictor in terms of the new feature, we must first express $x_j$ as a function of $x_{j}^{\\text{new}}$. Rearranging the transformation equation gives:\n$$ v x_{j}^{\\text{new}} = x_{j} - u $$\n$$ x_{j} = v x_{j}^{\\text{new}} + u $$\n\nNow, we substitute this expression for $x_{j}$ back into the equation for the original linear predictor $\\eta(x)$:\n$$ \\eta(x) = \\beta_{0} + \\beta_{j} (v x_{j}^{\\text{new}} + u) + \\sum_{k \\neq j} \\beta_{k} x_{k} $$\n\nDistributing the term $\\beta_{j}$ and regrouping the terms to match the structure of the new linear predictor gives:\n$$ \\eta(x) = \\beta_{0} + \\beta_{j} v x_{j}^{\\text{new}} + \\beta_{j} u + \\sum_{k \\neq j} \\beta_{k} x_{k} $$\n$$ \\eta(x) = (\\beta_{0} + \\beta_{j} u) + (\\beta_{j} v) x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\beta_{k} x_{k} $$\n\nThe new linear predictor is defined in terms of the new coefficients $\\tilde{\\beta}_{0}$, $\\tilde{\\beta}_{j}$, and $\\tilde{\\beta}_{k}$ for $k \\neq j$:\n$$ \\tilde{\\eta}(x) = \\tilde{\\beta}_{0} + \\tilde{\\beta}_{j} x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\tilde{\\beta}_{k} x_{k} $$\n\nFor the condition $\\eta(x) = \\tilde{\\eta}(x)$ to hold for all possible values of the feature vector $x$ (and thus for all values of $x_{j}^{\\text{new}}$ and $x_k$ for $k \\neq j$), the coefficients of the corresponding terms in the two polynomial expressions for the linear predictors must be equal. We can equate the coefficients term by term:\n\nComparing the expression for $\\eta(x)$ in terms of $x_{j}^{\\text{new}}$ with the expression for $\\tilde{\\eta}(x)$:\n$$ (\\beta_{0} + \\beta_{j} u) + (\\beta_{j} v) x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\beta_{k} x_{k} = \\tilde{\\beta}_{0} + \\tilde{\\beta}_{j} x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\tilde{\\beta}_{k} x_{k} $$\n\n1.  Equating the constant terms (the intercepts):\n    $$ \\tilde{\\beta}_{0} = \\beta_{0} + \\beta_{j} u $$\n\n2.  Equating the coefficients of the transformed feature $x_{j}^{\\text{new}}$:\n    $$ \\tilde{\\beta}_{j} = \\beta_{j} v $$\n\n3.  Equating the coefficients of the other features $x_{k}$ for $k \\neq j$:\n    $$ \\tilde{\\beta}_{k} = \\beta_{k} $$\n\nThe problem asks for the expressions for the pair $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$. Based on the derivation above, these are:\n$$ \\tilde{\\beta}_{0} = \\beta_{0} + \\beta_{j} u $$\n$$ \\tilde{\\beta}_{j} = \\beta_{j} v $$\n\nThese expressions give the new intercept and the new coefficient for the rescaled feature, ensuring that the model's predictions are invariant under the specified linear transformation of the feature $x_j$. The final answer will be presented as a row matrix as requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\beta_{0} + \\beta_{j} u & \\beta_{j} v\n\\end{pmatrix}\n}\n$$", "id": "3185465"}, {"introduction": "A classifier's job is rarely finished once it outputs a probability; the next step is to make a decision. This final practice bridges the gap between probabilistic modeling and decision theory, tackling the critical question of how to set an optimal classification threshold when the consequences of different errors are not equal. You will derive the ideal threshold for a logistic regression model by minimizing the expected cost, taking into account both the prevalence of the condition and the specific costs associated with false positives and false negatives [@problem_id:3185480].", "problem": "A binary classifier is trained to predict whether an individual has a condition ($Y=1$) or not ($Y=0$). The prevalence of the condition in the target population is $\\pi = \\mathbb{P}(Y=1)$ with $0 < \\pi < 1$. The classifier is a logistic regression model with linear predictor $\\eta(x)$ and output score $q(x)$ given by the logistic function $\\sigma$ and the logit link, namely $q(x) = \\sigma(\\eta(x))$ where $\\sigma(z) = \\left(1 + \\exp(-z)\\right)^{-1}$ and $\\operatorname{logit}(q) = \\ln\\!\\left(\\frac{q}{1-q}\\right) = \\eta(x)$. The model was trained on a balanced dataset so that its score $q(x)$ is calibrated to equal class priors, meaning $q(x) = \\frac{f_{1}(x)}{f_{0}(x) + f_{1}(x)}$, where $f_{y}(x)$ denotes the class-conditional density of features $x$ given $Y=y$.\n\nYou must choose a single probability threshold $t$ to apply to $q(x)$: predict $Y=1$ when $q(x) \\ge t$ and predict $Y=0$ otherwise. Misclassification incurs asymmetric costs: a False Negative (FN) incurs cost $C_{FN} > 0$, and a False Positive (FP) incurs cost $C_{FP} > 0$. Starting from core definitions of conditional risk minimization and Bayes decision theory, and treating $\\pi$, $C_{FN}$, and $C_{FP}$ as fixed and known, derive the closed-form expression for the cost-optimal threshold $t^{*}$ as a function of $\\pi$, $C_{FN}$, and $C_{FP}$ to be applied directly to $q(x)$.\n\nYour final answer must be a single analytical expression for $t^{*}$.", "solution": "The optimal decision threshold $t^*$ is found by minimizing the conditional risk. For a given observation with features $x$, we should predict $Y=1$ if the expected cost of predicting 1 is less than or equal to the expected cost of predicting 0. Let $p(x) = \\mathbb{P}(Y=1|x)$ be the true posterior probability for the target population.\n\nThe conditional risk (expected cost) for each action is:\n- Risk of predicting $Y=1$ (action $\\alpha_1$): $R(\\alpha_1|x) = C_{FP} \\cdot \\mathbb{P}(Y=0|x) = C_{FP}(1 - p(x))$. This is the cost of a False Positive multiplied by its probability.\n- Risk of predicting $Y=0$ (action $\\alpha_0$): $R(\\alpha_0|x) = C_{FN} \\cdot \\mathbb{P}(Y=1|x) = C_{FN} p(x)$. This is the cost of a False Negative multiplied by its probability.\n\nThe Bayes-optimal decision rule is to predict $Y=1$ if $R(\\alpha_1|x) \\le R(\\alpha_0|x)$:\n$$C_{FP}(1 - p(x)) \\le C_{FN} p(x)$$\nRearranging this inequality gives a condition on the true posterior odds:\n$$\\frac{p(x)}{1-p(x)} \\ge \\frac{C_{FP}}{C_{FN}}$$\n\nNext, we must relate the model's score $q(x)$ to the true posterior probability $p(x)$. The score $q(x)$ was calibrated on a balanced dataset (priors of 0.5), so it represents the ratio of class-conditional densities:\n$$q(x) = \\frac{f_1(x)}{f_0(x) + f_1(x)} \\implies \\frac{q(x)}{1-q(x)} = \\frac{f_1(x)}{f_0(x)}$$\nThis is the likelihood ratio.\n\nThe true posterior odds are related to the likelihood ratio and the true prevalence $\\pi$ by Bayes' theorem:\n$$\\frac{p(x)}{1-p(x)} = \\frac{f_1(x)\\pi}{f_0(x)(1-\\pi)} = \\frac{f_1(x)}{f_0(x)} \\cdot \\frac{\\pi}{1-\\pi}$$\nSubstituting the expression for the likelihood ratio gives the correction formula:\n$$\\frac{p(x)}{1-p(x)} = \\frac{q(x)}{1-q(x)} \\cdot \\frac{\\pi}{1-\\pi}$$\n\nNow, substitute this into the decision rule:\n$$\\frac{q(x)}{1-q(x)} \\cdot \\frac{\\pi}{1-\\pi} \\ge \\frac{C_{FP}}{C_{FN}}$$\nSolving for the score odds $\\frac{q(x)}{1-q(x)}$:\n$$\\frac{q(x)}{1-q(x)} \\ge \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}$$\nLet $K = \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}$. The decision rule is to predict $Y=1$ if $q(x) \\ge t^*$. We need to solve $q(x)/(1-q(x)) \\ge K$ for $q(x)$.\n$$q(x) \\ge K(1-q(x)) \\implies q(x) + Kq(x) \\ge K \\implies q(x)(1+K) \\ge K \\implies q(x) \\ge \\frac{K}{1+K}$$\nThe optimal threshold is $t^* = \\frac{K}{1+K}$. Substituting back the expression for $K$:\n$$t^* = \\frac{\\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}}{1 + \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}}$$\nMultiplying the numerator and denominator by $C_{FN}\\pi$ to simplify:\n$$t^* = \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi + C_{FP}(1-\\pi)}$$\nThis is the desired cost-optimal threshold to apply to the model's output score $q(x)$.", "answer": "$$\\boxed{\\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi + C_{FP}(1-\\pi)}}$$", "id": "3185480"}]}