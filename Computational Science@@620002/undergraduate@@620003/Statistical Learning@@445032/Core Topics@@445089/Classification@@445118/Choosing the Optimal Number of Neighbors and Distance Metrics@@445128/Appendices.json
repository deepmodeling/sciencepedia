{"hands_on_practices": [{"introduction": "To begin, we will build a $k$-NN classifier from the ground up, tackling the core challenge of hyperparameter tuning. This practice [@problem_id:3108117] walks you through implementing a cross-validation scheme to find the optimal number of neighbors, $k$. You will compare three common distance metrics—Manhattan ($L_1$), Euclidean ($L_2$), and Chebyshev ($L_{\\infty}$)—to see firsthand how the geometry of the metric influences the decision boundary and the best choice of $k$.", "problem": "You are to implement a complete, deterministic scheme to choose the number of neighbors $k$ in the $k$-Nearest Neighbors ($k$-NN) classifier by optimizing the F1 score rather than accuracy, and to analyze how the choice of distance metric shifts the $k$ that maximizes the F1 score. Work entirely from first principles and definitions. Begin from the following base in statistical learning: (i) the definition of the $k$-Nearest Neighbors classifier as a majority vote among the $k$ closest training points under a specified distance, (ii) the definition of $L_{p}$ norms as valid distances in $\\mathbb{R}^{d}$, and (iii) the definition of the F1 score for binary classification as the harmonic mean of precision and recall computed from the confusion matrix.\n\nYour program must:\n- Implement binary classification using the $k$-Nearest Neighbors rule with unweighted votes and the following deterministic tie-breaking rules: when sorting neighbors, break equal distances by ascending training index; when the vote count is tied, predict the smaller label, which is $0$.\n- Evaluate performance by the F1 score for the positive class label $1$ only. From a confusion matrix with counts $\\mathrm{TP}$, $\\mathrm{FP}$, and $\\mathrm{FN}$ for the positive class, define precision as $\\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FP})$ if $\\mathrm{TP}+\\mathrm{FP}>0$ and $0$ otherwise, recall as $\\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$ if $\\mathrm{TP}+\\mathrm{FN}>0$ and $0$ otherwise, and the F1 score as $2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall} / (\\mathrm{precision} + \\mathrm{recall})$ if $\\mathrm{precision} + \\mathrm{recall} > 0$ and $0$ otherwise.\n- Use three distance metrics on $\\mathbb{R}^{2}$: the Manhattan distance $L_{1}$, the Euclidean distance $L_{2}$, and the Chebyshev distance $L_{\\infty}$. For a difference vector $\\Delta x \\in \\mathbb{R}^{2}$, these are defined as $L_{1}(\\Delta x) = \\lvert \\Delta x_{1} \\rvert + \\lvert \\Delta x_{2} \\rvert$, $L_{2}(\\Delta x) = \\sqrt{(\\Delta x_{1})^{2} + (\\Delta x_{2})^{2}}$, and $L_{\\infty}(\\Delta x) = \\max\\{\\lvert \\Delta x_{1} \\rvert, \\lvert \\Delta x_{2} \\rvert\\}$.\n- Select $k$ by cross-validation using explicitly provided folds. For each candidate $k$ in the set $\\{1,3,5,7\\}$ and for each metric, pool predictions across all validation instances from all folds to form a single confusion matrix and compute a single F1 score (do not average per-fold F1 scores). Choose the $k$ that maximizes this pooled F1. In case of ties in F1 across $k$, choose the smallest $k$.\n- Report, for each test case, the $k$ selected for $L_{1}$, $L_{2}$, and $L_{\\infty}$, the corresponding F1 scores (rounded to three decimal places), and the integer shifts $\\Delta k_{2-1} = k^{\\star}_{L_{2}} - k^{\\star}_{L_{1}}$ and $\\Delta k_{\\infty-2} = k^{\\star}_{L_{\\infty}} - k^{\\star}_{L_{2}}$.\n\nFundamental definitions to use:\n- $k$-Nearest Neighbors classification: given a training set $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ with $x_{i} \\in \\mathbb{R}^{d}$ and $y_{i} \\in \\{0,1\\}$, for a query $x$ compute distances to all training points under a chosen metric, select the $k$ smallest distances (breaking ties by training index), and predict the label by unweighted majority vote among those $k$ labels (breaking ties in favor of label $0$).\n- Distance metrics: $L_{1}$, $L_{2}$, and $L_{\\infty}$ as above.\n- F1 score: defined from precision and recall for the positive class as above.\n\nTest suite:\nImplement your program to run on the following three datasets, each with prescribed $3$-fold partitions. Each dataset is two-dimensional, and labels are binary with $0$ as the negative class and $1$ as the positive class. Indices start at $0$. All coordinates are real numbers in $\\mathbb{R}$.\n\nTest case $1$ (balanced, well-separated clusters):\n- Points $x_{i} \\in \\mathbb{R}^{2}$ for $i \\in \\{0,1,\\dots,19\\}$:\n  - Class $0$ at indices $0$ to $9$: $(0.0,0.0)$, $(0.0,1.0)$, $(1.0,0.0)$, $(1.0,1.0)$, $(0.5,0.2)$, $(1.2,-0.1)$, $(0.3,1.4)$, $(1.5,0.7)$, $(-0.2,0.5)$, $(0.8,1.2)$.\n  - Class $1$ at indices $10$ to $19$: $(5.0,5.0)$, $(5.0,6.0)$, $(6.0,5.0)$, $(6.0,6.0)$, $(5.2,5.1)$, $(5.8,5.2)$, $(6.3,5.7)$, $(5.1,6.2)$, $(5.5,5.6)$, $(6.1,6.2)$.\n- Labels $y_{i}$: $y_{i} = 0$ for $i \\in \\{0,1,\\dots,9\\}$ and $y_{i} = 1$ for $i \\in \\{10,11,\\dots,19\\}$.\n- Folds:\n  - Fold $\\mathrm{A}$ (validation indices): $\\{0,1,2,10,11,12,13\\}$.\n  - Fold $\\mathrm{B}$ (validation indices): $\\{3,4,5,14,15,16\\}$.\n  - Fold $\\mathrm{C}$ (validation indices): $\\{6,7,8,9,17,18,19\\}$.\n\nTest case $2$ (imbalanced, overlapping):\n- Points $x_{i} \\in \\mathbb{R}^{2}$ for $i \\in \\{0,1,\\dots,19\\}$:\n  - Class $0$ at indices $0$ to $13$: $(0.0,0.0)$, $(0.2,-0.1)$, $(-0.1,0.3)$, $(0.4,0.1)$, $(0.6,0.2)$, $(0.7,0.4)$, $(0.8,0.2)$, $(1.0,0.0)$, $(1.1,0.3)$, $(1.2,0.1)$, $(1.3,0.2)$, $(0.9,0.6)$, $(0.5,0.8)$, $(0.2,0.9)$.\n  - Class $1$ at indices $14$ to $19$: $(1.1,1.1)$, $(1.2,1.0)$, $(1.3,1.1)$, $(1.4,1.2)$, $(1.5,1.3)$, $(1.6,1.2)$.\n- Labels $y_{i}$: $y_{i} = 0$ for $i \\in \\{0,1,\\dots,13\\}$ and $y_{i} = 1$ for $i \\in \\{14,15,\\dots,19\\}$.\n- Folds:\n  - Fold $\\mathrm{A}$ (validation indices): $\\{0,1,2,14,15,16,17\\}$.\n  - Fold $\\mathrm{B}$ (validation indices): $\\{3,4,5,6,18,19\\}$.\n  - Fold $\\mathrm{C}$ (validation indices): $\\{7,8,9,10,11,12,13\\}$.\n\nTest case $3$ (anisotropic with ties):\n- Points $x_{i} \\in \\mathbb{R}^{2}$ for $i \\in \\{0,1,\\dots,17\\}$:\n  - Class $0$ at indices $0$ to $9$: $(0.0,0.0)$, $(0.0,1.0)$, $(0.0,1.0)$, $(10.0,0.0)$, $(10.0,1.0)$, $(10.0,2.0)$, $(0.0,2.0)$, $(10.0,-1.0)$, $(0.0,-1.0)$, $(10.0,3.0)$.\n  - Class $1$ at indices $10$ to $17$: $(5.0,0.0)$, $(5.0,1.0)$, $(5.0,2.0)$, $(5.0,3.0)$, $(5.0,-1.0)$, $(5.0,1.0)$, $(5.0,2.0)$, $(5.0,0.0)$.\n- Labels $y_{i}$: $y_{i} = 0$ for $i \\in \\{0,1,\\dots,9\\}$ and $y_{i} = 1$ for $i \\in \\{10,11,\\dots,17\\}$.\n- Folds:\n  - Fold $\\mathrm{A}$ (validation indices): $\\{0,1,10,11,12,13\\}$.\n  - Fold $\\mathrm{B}$ (validation indices): $\\{2,3,4,14,15,16\\}$.\n  - Fold $\\mathrm{C}$ (validation indices): $\\{5,6,7,8,9,17\\}$.\n\nCandidate neighbor counts:\n- $k \\in \\{1,3,5,7\\}$.\n\nDistance metrics:\n- $L_{1}$, $L_{2}$, $L_{\\infty}$ as defined above.\n\nRequired final output format:\n- For each test case, output a list of $8$ values in the following order: $k^{\\star}_{L_{1}}$, $k^{\\star}_{L_{2}}$, $k^{\\star}_{L_{\\infty}}$, $\\mathrm{F1}_{L_{1}}$, $\\mathrm{F1}_{L_{2}}$, $\\mathrm{F1}_{L_{\\infty}}$, $\\Delta k_{2-1}$, $\\Delta k_{\\infty-2}$, where $\\mathrm{F1}$ values are rounded to three decimal places. Aggregate the three per-case lists into a single list.\n- Your program should produce a single line of output containing this aggregate as a comma-separated list with no spaces, for example: $[[1,3,5,0.750,0.800,0.820,2,2],[\\dots],[\\dots]]$.\n\nAngle units and physical units are not applicable.\n\nYour implementation must be self-contained and must not read input. The folds provided must be used as given, with training indices defined as the complement of validation indices for each fold within the dataset.", "solution": "The problem requires the implementation and analysis of a $k$-Nearest Neighbors (k-NN) classifier. The core tasks are to select the optimal number of neighbors $k$ from a candidate set $\\{1, 3, 5, 7\\}$ and to compare the results across three different distance metrics: Manhattan ($L_1$), Euclidean ($L_2$), and Chebyshev ($L_\\infty$). The optimization criterion is the F1 score for the positive class (label $1$), evaluated using a specified cross-validation procedure.\n\nThe solution is constructed from first principles, proceeding from the definitions of the components to the complete algorithm.\n\n### 1. Fundamental Principles\n\n#### 1.1. The $k$-Nearest Neighbors (k-NN) Classifier\nThe k-NN algorithm is a non-parametric method used for classification and regression. For a classification task, it operates as follows:\nGiven a training dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$, where $x_i \\in \\mathbb{R}^d$ are feature vectors and $y_i \\in \\{0, 1\\}$ are binary labels, the label for a new query point $x_q$ is predicted by the following steps:\n1.  **Distance Calculation**: Compute the distance from $x_q$ to every training point $x_i$ using a chosen distance metric.\n2.  **Neighbor Identification**: Identify the $k$ training points $(x_i, y_i)$ that are closest to $x_q$. This set of $k$ points constitutes the \"neighborhood\" of $x_q$.\n3.  **Majority Vote**: The predicted label $\\hat{y}_q$ for $x_q$ is the most frequent label among the $k$ neighbors.\n\nThe problem specifies deterministic tie-breaking rules, which are crucial for reproducibility:\n-   If multiple training points have the same distance to $x_q$, their relative order is determined by their original index in the training data, with smaller indices being considered \"closer\".\n-   If the majority vote results in a tie (e.g., for an even $k$, or with multiple classes), the prediction defaults to the smaller class label, which is $0$ in this binary case.\n\n#### 1.2. Distance Metrics in $\\mathbb{R}^2$\nThe choice of distance metric determines the geometry of the neighborhood and thus the shape of the decision boundary. The problem specifies three $L_p$ norms for a difference vector $\\Delta x = (\\Delta x_1, \\Delta x_2) \\in \\mathbb{R}^2$:\n-   **Manhattan Distance ($L_1$)**: $d_1(\\Delta x) = \\|\\Delta x\\|_1 = |\\Delta x_1| + |\\Delta x_2|$. This metric measures distance as if moving on a grid, and the \"circles\" of constant distance are diamonds.\n-   **Euclidean Distance ($L_2$)**: $d_2(\\Delta x) = \\|\\Delta x\\|_2 = \\sqrt{(\\Delta x_1)^2 + (\\Delta x_2)^2}$. This is the standard straight-line distance, and its circles are conventional circles.\n-   **Chebyshev Distance ($L_\\infty$)**: $d_\\infty(\\Delta x) = \\|\\Delta x\\|_\\infty = \\max(|\\Delta x_1|, |\\Delta x_2|)$. This measures the greatest distance along any single coordinate axis, and its circles are squares.\n\nThe differing geometries mean that for a given query point, the set of $k$ nearest neighbors can change depending on which metric is used. This in turn can lead to a different optimal value of $k$ for each metric.\n\n#### 1.3. Performance Evaluation: The F1 Score\nFor binary classification, a confusion matrix summarizes performance based on the counts of True Positives ($\\mathrm{TP}$), False Positives ($\\mathrm{FP}$), True Negatives ($\\mathrm{TN}$), and False Negatives ($\\mathrm{FN}$) for the positive class (label $1$).\n-   $\\mathrm{TP}$: Correctly predicted positive instances (True label $1$, Predicted label $1$).\n-   $\\mathrm{FP}$: Incorrectly predicted positive instances (True label $0$, Predicted label $1$).\n-   $\\mathrm{FN}$: Incorrectly predicted negative instances (True label $1$, Predicted label $0$).\n-   $\\mathrm{TN}$: Correctly predicted negative instances (True label $0$, Predicted label $0$).\n\nFrom these, we define two key metrics:\n-   **Precision**: The fraction of positive predictions that are correct. It measures the exactness of the classifier.\n    $$ \\text{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} $$\n-   **Recall (Sensitivity)**: The fraction of actual positive instances that are correctly identified. It measures the completeness of the classifier.\n    $$ \\text{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} $$\n\nThe F1 score is the harmonic mean of Precision and Recall, providing a single measure that balances both:\n$$ \\mathrm{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nThe problem correctly specifies handling cases where denominators are zero to avoid division by zero errors. The F1 score is particularly useful for imbalanced datasets where accuracy can be misleading.\n\n### 2. Algorithmic Procedure\n\nThe overall procedure involves a nested loop structure to perform model selection via cross-validation for each test case.\n\n**For each provided test case:**\n1.  Initialize a list to store the final results for this case.\n2.  **For each distance metric $d \\in \\{L_1, L_2, L_\\infty\\}$:**\n    a. Initialize a list to store the F1 scores for each candidate $k$.\n    b. **For each candidate neighbor count $k \\in \\{1, 3, 5, 7\\}$:**\n        i.   Initialize pooled confusion matrix counters: $\\mathrm{TP}_{\\text{pool}}=0$, $\\mathrm{FP}_{\\text{pool}}=0$, $\\mathrm{FN}_{\\text{pool}}=0$.\n        ii.  **For each cross-validation fold provided:**\n             A.  Identify the training set (all data points not in the current validation fold) and the validation set (points specified by the fold's indices).\n             B.  **For each validation point $(x_v, y_v)$:**\n                 1.  Calculate the distance from $x_v$ to every point $(x_t, y_t)$ in the training set using the current metric $d$. Store these as tuples of `(distance, training_index, training_label)`.\n                 2.  Sort these tuples first by distance (ascending) and then by training index (ascending) to resolve ties.\n                 3.  Select the top $k$ tuples from the sorted list. These are the $k$ nearest neighbors.\n                 4.  Count the votes from the labels of these $k$ neighbors. Let `count_1` be the number of neighbors with label $1$ and `count_0` be the number with label $0$.\n                 5.  Determine the predicted label $\\hat{y}_v$. If `count_1 > count_0`, $\\hat{y}_v = 1$. If `count_0 > count_1`, $\\hat{y}_v = 0$. If `count_0 == count_1` (a tie in votes), the prediction defaults to $0$.\n                 6.  Update the pooled confusion matrix counters based on the true label $y_v$ and predicted label $\\hat{y}_v$:\n                     - If $y_v=1$ and $\\hat{y}_v=1$, increment $\\mathrm{TP}_{\\text{pool}}$.\n                     - If $y_v=0$ and $\\hat{y}_v=1$, increment $\\mathrm{FP}_{\\text{pool}}$.\n                     - If $y_v=1$ and $\\hat{y}_v=0$, increment $\\mathrm{FN}_{\\text{pool}}$.\n        iii. After iterating through all folds, calculate the final F1 score for the current $k$ using the pooled counters $\\mathrm{TP}_{\\text{pool}}$, $\\mathrm{FP}_{\\text{pool}}$, and $\\mathrm{FN}_{\\text{pool}}$ and the specified formulas.\n        iv. Store the calculated F1 score for the current $k$.\n    c. After evaluating all candidate $k$ values, find the optimal $k^\\star$ for the current metric. This is the $k$ which yielded the maximum F1 score. If there's a tie in F1 scores, the smallest $k$ among the winners is chosen.\n    d. Store the optimal $k^\\star$ and its corresponding F1 score.\n\n3.  After iterating through all three metrics, the optimal values $(k^\\star_{L_1}, \\mathrm{F1}_{L_1})$, $(k^\\star_{L_2}, \\mathrm{F1}_{L_2})$, and $(k^\\star_{L_\\infty}, \\mathrm{F1}_{L_\\infty})$ are determined.\n4.  Calculate the required shifts: $\\Delta k_{2-1} = k^\\star_{L_2} - k^\\star_{L_1}$ and $\\Delta k_{\\infty-2} = k^\\star_{L_\\infty} - k^\\star_{L_2}$.\n5.  Assemble the final list for the test case: $[k^\\star_{L_1}, k^\\star_{L_2}, k^\\star_{L_\\infty}, \\mathrm{F1}_{L_1}, \\mathrm{F1}_{L_2}, \\mathrm{F1}_{L_\\infty}, \\Delta k_{2-1}, \\Delta k_{\\infty-2}]$, with F1 scores formatted to three decimal places.\n\nThis entire process is repeated for each of the three test cases provided. The final output is an aggregation of the results from all test cases into a single structured list. The implementation requires careful handling of data structures, loops, and the precise application of all specified rules.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the k-NN optimization problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0), (0.5, 0.2), (1.2, -0.1), (0.3, 1.4), (1.5, 0.7), (-0.2, 0.5), (0.8, 1.2),\n                (5.0, 5.0), (5.0, 6.0), (6.0, 5.0), (6.0, 6.0), (5.2, 5.1), (5.8, 5.2), (6.3, 5.7), (5.1, 6.2), (5.5, 5.6), (6.1, 6.2)\n            ]),\n            \"labels\": np.array([0]*10 + [1]*10),\n            \"folds\": [\n                {0, 1, 2, 10, 11, 12, 13},\n                {3, 4, 5, 14, 15, 16},\n                {6, 7, 8, 9, 17, 18, 19}\n            ]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, -0.1), (-0.1, 0.3), (0.4, 0.1), (0.6, 0.2), (0.7, 0.4), (0.8, 0.2), (1.0, 0.0), (1.1, 0.3), (1.2, 0.1), (1.3, 0.2), (0.9, 0.6), (0.5, 0.8), (0.2, 0.9),\n                (1.1, 1.1), (1.2, 1.0), (1.3, 1.1), (1.4, 1.2), (1.5, 1.3), (1.6, 1.2)\n            ]),\n            \"labels\": np.array([0]*14 + [1]*6),\n            \"folds\": [\n                {0, 1, 2, 14, 15, 16, 17},\n                {3, 4, 5, 6, 18, 19},\n                {7, 8, 9, 10, 11, 12, 13}\n            ]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.0, 1.0), (0.0, 1.0), (10.0, 0.0), (10.0, 1.0), (10.0, 2.0), (0.0, 2.0), (10.0, -1.0), (0.0, -1.0), (10.0, 3.0),\n                (5.0, 0.0), (5.0, 1.0), (5.0, 2.0), (5.0, 3.0), (5.0, -1.0), (5.0, 1.0), (5.0, 2.0), (5.0, 0.0)\n            ]),\n            \"labels\": np.array([0]*10 + [1]*8),\n            \"folds\": [\n                {0, 1, 10, 11, 12, 13},\n                {2, 3, 4, 14, 15, 16},\n                {5, 6, 7, 8, 9, 17}\n            ]\n        }\n    ]\n    \n    candidate_k = [1, 3, 5, 7]\n    metrics = {\n        'L1': lambda v: np.abs(v).sum(),\n        'L2': lambda v: np.sqrt(np.square(v).sum()),\n        'L_inf': lambda v: np.abs(v).max()\n    }\n\n    all_test_results = []\n\n    for case_data in test_cases:\n        points = case_data[\"points\"]\n        labels = case_data[\"labels\"]\n        folds = case_data[\"folds\"]\n        num_points = len(points)\n        all_indices = set(range(num_points))\n\n        case_results = {}\n\n        for metric_name, dist_func in metrics.items():\n            k_f1_scores = {}\n\n            for k in candidate_k:\n                tp, fp, fn = 0, 0, 0\n                \n                for val_indices in folds:\n                    train_indices = list(all_indices - val_indices)\n                    \n                    train_points = points[train_indices]\n                    train_labels = labels[train_indices]\n\n                    for val_idx in val_indices:\n                        query_point = points[val_idx]\n                        true_label = labels[val_idx]\n\n                        distances = []\n                        for i, train_idx in enumerate(train_indices):\n                            dist = dist_func(query_point - train_points[i])\n                            distances.append((dist, train_idx, train_labels[i]))\n                        \n                        distances.sort() # Sorts by first element (dist), then second (train_idx)\n\n                        neighbors_labels = [label for _, _, label in distances[:k]]\n                        \n                        count_1 = sum(1 for label in neighbors_labels if label == 1)\n                        count_0 = k - count_1\n\n                        if count_1 > count_0:\n                            pred_label = 1\n                        else: # includes count_0 > count_1 and count_0 == count_1\n                            pred_label = 0\n\n                        if pred_label == 1 and true_label == 1:\n                            tp += 1\n                        elif pred_label == 1 and true_label == 0:\n                            fp += 1\n                        elif pred_label == 0 and true_label == 1:\n                            fn += 1\n                \n                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n                f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n                \n                k_f1_scores[k] = f1_score\n\n            best_k = -1\n            max_f1 = -1.0\n            for k in sorted(k_f1_scores.keys()):\n                if k_f1_scores[k] > max_f1:\n                    max_f1 = k_f1_scores[k]\n                    best_k = k\n            \n            case_results[metric_name] = (best_k, max_f1)\n\n        k_l1, f1_l1 = case_results['L1']\n        k_l2, f1_l2 = case_results['L2']\n        k_linf, f1_linf = case_results['L_inf']\n        \n        delta_k_21 = k_l2 - k_l1\n        delta_k_inf2 = k_linf - k_l2\n\n        result_list = [\n            k_l1, k_l2, k_linf,\n            f\"{f1_l1:.3f}\", f\"{f1_l2:.3f}\", f\"{f1_linf:.3f}\",\n            delta_k_21, delta_k_inf2\n        ]\n        all_test_results.append(result_list)\n\n    # Construct the final output string manually to avoid spaces.\n    output_str = \"[\"\n    for i, res in enumerate(all_test_results):\n        output_str += f\"[{','.join(map(str, res))}]\"\n        if i  len(all_test_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3108117"}, {"introduction": "While standard distance metrics are a great starting point, they treat all features as equally important and independent, an assumption that rarely holds true. This practice [@problem_id:3108168] introduces a more sophisticated approach by using an adaptive metric derived from the data itself. You will implement a 'whitening' transformation, which constructs the Mahalanobis distance, to build a $k$-NN classifier that automatically accounts for your data's unique feature scaling and correlations.", "problem": "You are given the task of programmatically investigating how the choice of distance metric affects the optimal number of neighbors in the $k$-Nearest Neighbors (kNN) classifier. Your investigation must be grounded in the following core definitions and facts:\n\n- The $k$-Nearest Neighbors classifier predicts the class label for a query point by finding its $k$ closest training points under a specified distance and returning the majority class among those neighbors.\n- The Euclidean distance between vectors $x \\in \\mathbb{R}^d$ and $y \\in \\mathbb{R}^d$ is $d_{\\text{raw}}(x,y) = \\|x - y\\|_2$.\n- For data with sample covariance matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ estimated from training data, a whitening transform uses a matrix $W \\in \\mathbb{R}^{d \\times d}$ satisfying $W^\\top W \\approx \\hat{\\Sigma}^{-1}$ to rescale and decorrelate features. The associated distance is $d_W(x,y) = \\|W(x-y)\\|_2$, which equals the Mahalanobis distance induced by $\\hat{\\Sigma}^{-1}$ when $W$ is chosen as the symmetric matrix $W = U \\operatorname{diag}(\\lambda^{-1/2}) U^\\top$ where $\\hat{\\Sigma} = U \\operatorname{diag}(\\lambda) U^\\top$ is the spectral decomposition.\n- Leave-One-Out Cross-Validation (LOOCV) estimates classification error by, for each sample $i$, training on the remaining $n-1$ samples, predicting the label for sample $i$, and averaging the $0$-$1$ loss across all $n$ samples.\n\nYour program must:\n1. For each test case described below, generate a balanced binary classification dataset from two multivariate Gaussian classes with identical covariance but different means. Use two independent class means $m_0 \\in \\mathbb{R}^d$ and $m_1 \\in \\mathbb{R}^d$, a shared covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, and a total sample size $n$ with equal class sizes. All random draws must be reproducible via the given seed.\n2. For each candidate number of neighbors $k$ in the set of odd integers from $1$ up to $\\min(31, n-1)$ inclusive, perform LOOCV to estimate the misclassification rate for two metrics:\n   - Raw Euclidean metric $d_{\\text{raw}}(x,y) = \\|x-y\\|_2$.\n   - Whitened metric $d_W(x,y) = \\|W(x-y)\\|_2$, where $W$ is computed within each LOOCV fold from the training data only. Specifically, for training data $X_{\\text{train}} \\in \\mathbb{R}^{(n-1) \\times d}$, compute the sample covariance $\\hat{\\Sigma}$ and then construct $W$ via the symmetric spectral decomposition of $\\hat{\\Sigma}$ as $W = U \\operatorname{diag}(\\lambda^{-1/2}) U^\\top$. To ensure numerical stability, apply a ridge regularization $\\alpha I_d$ to $\\hat{\\Sigma}$ before inverting square roots, where $\\alpha$ is a small positive scalar based on the scale of $\\hat{\\Sigma}$. State and use a consistent rule to break ties in majority voting.\n3. For each metric, select the optimal $k$ as the smallest $k$ achieving the minimum LOOCV misclassification rate.\n\nYou must implement the above for the following test suite. In each case, use the specified parameters exactly. Symbols $n$, $d$, $m_0$, $m_1$, and $\\Sigma$ are as defined above.\n\n- Test Case $1$ (anisotropic scale): seed $= 0$, $n = 120$, $d = 2$, $m_0 = (-1, 0)$, $m_1 = (1, 0)$, and \n  $$\\Sigma = \\begin{bmatrix}1  0 \\\\ 0  9\\end{bmatrix}.$$\n- Test Case $2$ (isotropic): seed $= 1$, $n = 120$, $d = 2$, $m_0 = (0, 0)$, $m_1 = (1, 0)$, and \n  $$\\Sigma = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}.$$\n- Test Case $3$ (correlated features): seed $= 2$, $n = 120$, $d = 3$, $m_0 = (0, 0, 0)$, $m_1 = (1, 0, 0)$, and \n  $$\\Sigma = \\begin{bmatrix}1  0.8  0 \\\\ 0.8  1  0 \\\\ 0  0  0.2\\end{bmatrix}.$$\n- Test Case $4$ (small $n$, strong anisotropy): seed $= 3$, $n = 40$, $d = 2$, $m_0 = (0, 0)$, $m_1 = (0.8, 0)$, and \n  $$\\Sigma = \\begin{bmatrix}1  0 \\\\ 0  16\\end{bmatrix}.$$\n\nImplementation requirements and assumptions:\n- Use a deterministic pseudo-random number generator initialized with the given seed in each test case.\n- In LOOCV, for the whitened metric, the whitening matrix $W$ must be computed strictly from the training subset for each fold. Use a ridge regularization of the form $\\hat{\\Sigma}_{\\text{reg}} = \\hat{\\Sigma} + \\alpha I_d$ with $\\alpha = 10^{-6} \\cdot \\operatorname{trace}(\\hat{\\Sigma})/d$.\n- In majority voting for kNN, in case of a tie, predict class $0$.\n- The candidate set of $k$ values is all odd integers in $\\{1, 3, 5, \\dots, \\min(31, n-1)\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the list should itself be a two-element list $[k_{\\text{white}}, k_{\\text{raw}}]$, where $k_{\\text{white}}$ is the optimal $k$ under the whitened metric and $k_{\\text{raw}}$ is the optimal $k$ under the raw Euclidean metric, in the order of the test cases as listed above. For example, an output with four test cases should look like $[[k_{1,\\text{white}},k_{1,\\text{raw}}],[k_{2,\\text{white}},k_{2,\\text{raw}}],[k_{3,\\text{white}},k_{3,\\text{raw}}],[k_{4,\\text{white}},k_{4,\\text{raw}}]]$.", "solution": "The user has provided a problem that requires a programmatic investigation into the optimal choice of the number of neighbors, $k$, for the $k$-Nearest Neighbors (kNN) algorithm under two different distance metrics: the standard Euclidean distance and a whitened distance related to the Mahalanobis distance. The problem is well-defined, scientifically sound, and computationally feasible. It constitutes a valid exercise in statistical learning.\n\nThe core of the problem is to compare how the geometry of the distance metric interacts with the geometry of the data distribution. We will solve this by implementing the specified procedure: for each of four test cases, we will generate a synthetic dataset, and then use Leave-One-Out Cross-Validation (LOOCV) to find the optimal $k$ for each metric.\n\n### 1. Theoretical Framework\n\n**k-Nearest Neighbors (kNN):**\nThe kNN algorithm is a non-parametric classification method. To classify a query point $x_q$, it identifies the $k$ training samples that are closest to $x_q$ according to a chosen distance metric. The predicted class for $x_q$ is the majority class among these $k$ neighbors. In the case of a tie in the vote, the problem specifies a deterministic rule: predict class $0$. Since the candidate values for $k$ are all odd, and we have a binary classification problem, a tie is not possible. The rule is implemented as specified nonetheless.\n\n**Distance Metrics:**\n1.  **Raw Euclidean Distance:** For two vectors $x, y \\in \\mathbb{R}^d$, the Euclidean distance is given by:\n    $$\n    d_{\\text{raw}}(x,y) = \\|x - y\\|_2 = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}\n    $$\n    This metric is isotropic, meaning it treats all feature dimensions equally and does not account for scaling differences or correlations between them.\n\n2.  **Whitened Distance:** The data is transformed by a matrix $W$ before the Euclidean distance is computed. The distance is:\n    $$\n    d_W(x,y) = \\|W(x-y)\\|_2\n    $$\n    The problem specifies the construction of $W$ from the sample covariance matrix $\\hat{\\Sigma}$ of the training data. For a given training set, $\\hat{\\Sigma}$ is estimated. To handle potential singularity and improve numerical stability, a regularized covariance matrix $\\hat{\\Sigma}_{\\text{reg}}$ is formed:\n    $$\n    \\hat{\\Sigma}_{\\text{reg}} = \\hat{\\Sigma} + \\alpha I_d\n    $$\n    where $\\alpha = 10^{-6} \\cdot \\operatorname{trace}(\\hat{\\Sigma})/d$. Let the spectral decomposition of this symmetric, positive-definite matrix be $\\hat{\\Sigma}_{\\text{reg}} = U \\Lambda U^\\top$, where $U$ is the orthogonal matrix of eigenvectors and $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_d)$ is the diagonal matrix of positive eigenvalues. The whitening matrix $W$ is defined as the symmetric square root of the inverse of $\\hat{\\Sigma}_{\\text{reg}}$:\n    $$\n    W = (\\hat{\\Sigma}_{\\text{reg}})^{-1/2} = U \\Lambda^{-1/2} U^\\top\n    $$\n    where $\\Lambda^{-1/2} = \\operatorname{diag}(\\lambda_1^{-1/2}, \\dots, \\lambda_d^{-1/2})$. With this choice of $W$, the squared distance becomes:\n    $$\n    d_W(x,y)^2 = \\|W(x-y)\\|_2^2 = (W(x-y))^\\top (W(x-y)) = (x-y)^\\top W^\\top W (x-y)\n    $$\n    Since $W$ is symmetric ($W=W^\\top$) and $W^2 = \\hat{\\Sigma}_{\\text{reg}}^{-1}$, this simplifies to:\n    $$\n    d_W(x,y)^2 = (x-y)^\\top \\hat{\\Sigma}_{\\text{reg}}^{-1} (x-y)\n    $$\n    This is the Mahalanobis distance with respect to the regularized sample covariance. This metric is adaptive; it rescales features by their variance and accounts for their pairwise correlations, effectively transforming the data into a space where features are uncorrelated and have unit variance.\n\n**Leave-One-Out Cross-Validation (LOOCV):**\nLOOCV is an exhaustive method for estimating model performance. For a dataset of size $n$, it performs $n$ iterations. In each iteration $i$, the $i$-th data point $(x_i, y_i)$ is held out as the validation set, and the model is trained on the remaining $n-1$ data points. The trained model then predicts the label for $x_i\n$. The LOOCV error is the total number of incorrect predictions divided by $n$.\n\nA crucial detail is that for the whitened metric, the whitening matrix $W$ must be re-computed within each of the $n$ folds, using only the $n-1$ training samples available in that fold. This prevents data leakage from the validation point into the model's training process (specifically, into the construction of the distance metric itself).\n\n### 2. Algorithmic Procedure\n\nFor each of the four test cases:\n1.  **Data Generation:** Generate a dataset with $n$ samples and $d$ dimensions. Use the provided random `seed` to initialize a pseudorandom number generator. Draw $n/2$ samples from the multivariate normal distribution $\\mathcal{N}(m_0, \\Sigma)$ for class $0$, and $n/2$ samples from $\\mathcal{N}(m_1, \\Sigma)$ for class $1$.\n\n2.  **Cross-Validation Loop:** Iterate through each sample $i$ from $0$ to $n-1$.\n    a.  **Data Split:** Designate sample $i$ as the test point $(x_{\\text{test}}, y_{\\text{test}})$ and the remaining $n-1$ samples as the training set $(X_{\\text{train}}, y_{\\text{train}})$.\n    b.  **Metric-Specific Calculations:**\n        *   **Raw Metric:** Calculate the Euclidean distances from $x_{\\text{test}}$ to all points in $X_{\\text{train}}$. Sort the training labels $y_{\\text{train}}$ based on these distances to get an ordered list of neighbor labels.\n        *   **Whitened Metric:** First, compute the sample covariance matrix $\\hat{\\Sigma}$ from $X_{\\text{train}}$. Apply the specified regularization to get $\\hat{\\Sigma}_{\\text{reg}}$. Compute its spectral decomposition and construct the whitening matrix $W$. Then, calculate the whitened distances from $x_{\\text{test}}$ to all points in $X_{\\text{train}}$. Sort the training labels $y_{\\text{train}}$ based on these distances.\n    c.  **Error Accumulation:** For each candidate value of $k$ (odd integers from $1$ to $\\min(31, n-1)$), determine the predicted label by taking the majority class of the first $k$ neighbors from the sorted lists. If the prediction is incorrect, increment an error counter for that specific metric and $k$. An efficient way to do this for all $k$ simultaneously is to compute the cumulative sum of the sorted labels and check the majority condition at each required index.\n\n3.  **Optimal $k$ Selection:** After the LOOCV loop completes, we will have the total error count for each metric and each candidate $k$.\n    a.  For the raw metric, find the minimum error rate achieved across all candidate $k$'s. The optimal $k_{\\text{raw}}$ is the smallest $k$ that yields this minimum error.\n    b.  Similarly, find the optimal $k_{\\text{white}}$ for the whitened metric.\n\n4.  **Result Aggregation:** Store the pair $[k_{\\text{white}}, k_{\\text{raw}}]$ for the current test case and repeat for all cases. The final output is a list of these pairs.\n\nThis procedure rigorously compares the two metrics by finding the best possible performance for each (within the search space of $k$) on the given data, using a reliable error estimation technique. We expect the whitened metric to perform better (and potentially prefer simpler models, i.e., smaller $k$) on datasets where features are anisotropically scaled or correlated, as it corrects for these data structures. For isotropic data, its advantage should be minimal.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates how the choice of distance metric affects the optimal number\n    of neighbors in the k-Nearest Neighbors (kNN) classifier.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"seed\": 0, \"n\": 120, \"d\": 2, \"m0\": np.array([-1, 0]),\n            \"m1\": np.array([1, 0]), \"sigma\": np.array([[1, 0], [0, 9]])\n        },\n        {\n            \"seed\": 1, \"n\": 120, \"d\": 2, \"m0\": np.array([0, 0]),\n            \"m1\": np.array([1, 0]), \"sigma\": np.array([[1, 0], [0, 1]])\n        },\n        {\n            \"seed\": 2, \"n\": 120, \"d\": 3, \"m0\": np.array([0, 0, 0]),\n            \"m1\": np.array([1, 0, 0]),\n            \"sigma\": np.array([[1, 0.8, 0], [0.8, 1, 0], [0, 0, 0.2]])\n        },\n        {\n            \"seed\": 3, \"n\": 40, \"d\": 2, \"m0\": np.array([0, 0]),\n            \"m1\": np.array([0.8, 0]), \"sigma\": np.array([[1, 0], [0, 16]])\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        seed, n, d, m0, m1, sigma = case[\"seed\"], case[\"n\"], case[\"d\"], \\\n                                    case[\"m0\"], case[\"m1\"], case[\"sigma\"]\n\n        # 1. Generate dataset\n        rng = np.random.default_rng(seed)\n        n_per_class = n // 2\n        X0 = rng.multivariate_normal(m0, sigma, n_per_class)\n        X1 = rng.multivariate_normal(m1, sigma, n_per_class)\n        X = np.vstack((X0, X1))\n        y = np.array([0] * n_per_class + [1] * n_per_class)\n\n        # 2. Perform LOOCV to find optimal k for each metric\n        k_candidates = list(range(1, min(31, n - 1) + 2, 2))\n        errors_raw = {k: 0 for k in k_candidates}\n        errors_white = {k: 0 for k in k_candidates}\n\n        for i in range(n):\n            X_train = np.delete(X, i, axis=0)\n            y_train = np.delete(y, i)\n            x_test = X[i]\n            y_test = y[i]\n\n            # --- Raw Euclidean Metric ---\n            dists_raw = np.linalg.norm(X_train - x_test, axis=1)\n            sorted_neighbor_labels_raw = y_train[np.argsort(dists_raw)]\n\n            # --- Whitened Metric ---\n            # Compute whitening matrix W from this fold's training data\n            n_train, d_train = X_train.shape\n            cov = np.cov(X_train, rowvar=False, ddof=1)\n            \n            alpha = 1e-6 * np.trace(cov) / d_train\n            cov_reg = cov + alpha * np.identity(d_train)\n            \n            eigenvals, eigenvecs = np.linalg.eigh(cov_reg)\n            \n            # Clamp small/negative eigenvalues for stability\n            eigenvals[eigenvals = 0] = 1e-12\n\n            lambda_inv_sqrt = np.diag(1.0 / np.sqrt(eigenvals))\n            W = eigenvecs @ lambda_inv_sqrt @ eigenvecs.T\n            \n            diffs = X_train - x_test\n            # Since W is symmetric, W.T = W. dist = ||W * diff||\n            dists_white = np.linalg.norm(diffs @ W, axis=1)\n            sorted_neighbor_labels_white = y_train[np.argsort(dists_white)]\n\n            # --- Efficiently check all k values ---\n            label_sum_raw = np.cumsum(sorted_neighbor_labels_raw)\n            label_sum_white = np.cumsum(sorted_neighbor_labels_white)\n\n            for k in k_candidates:\n                # Prediction for raw metric\n                # Using 0-based indexing for cumsum, k neighbors are from index 0 to k-1\n                current_sum_raw = label_sum_raw[k - 1]\n                # Tie-breaking: predict class 0 if sum of labels is = k/2\n                pred_raw = 0 if current_sum_raw = k / 2 else 1\n                if pred_raw != y_test:\n                    errors_raw[k] += 1\n\n                # Prediction for whitened metric\n                current_sum_white = label_sum_white[k - 1]\n                pred_white = 0 if current_sum_white = k / 2 else 1\n                if pred_white != y_test:\n                    errors_white[k] += 1\n\n        # 3. Select optimal k for each metric\n        min_error_raw = min(errors_raw.values())\n        k_opt_raw = min(k for k, err in errors_raw.items() if err == min_error_raw)\n\n        min_error_white = min(errors_white.values())\n        k_opt_white = min(k for k, err in errors_white.items() if err == min_error_white)\n\n        all_results.append([k_opt_white, k_opt_raw])\n\n    # Format output as a string representation of a list of lists\n    result_str = \"[\" + \",\".join(f\"[{w},{r}]\" for w, r in all_results) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3108168"}, {"introduction": "The behavior of distance metrics can change dramatically in high-dimensional spaces, a challenge often called the 'curse of dimensionality.' This exercise [@problem_id:3108143] lets you explore this phenomenon by simulating data on a high-dimensional sphere, mimicking modern data types like text embeddings. By observing how the optimal number of neighbors $k$ changes with increasing dimension and using the cosine distance, you will gain critical intuition for applying $k$-NN effectively to complex datasets.", "problem": "You are given the task of empirically studying how the dimensionality $p$ of unit-normalized embedding vectors affects the choice of the optimal number of neighbors $k$ for $k$-nearest neighbors classification under cosine distance, in a controlled synthetic setting. Your program must simulate labeled data in several dimensions, evaluate validation accuracy for a fixed set of odd neighbor counts, and report the $k$ that maximizes validation accuracy for each dimension. The experiment should be repeatable and fully specified as follows.\n\nFundamental setting:\n- Cosine distance between two nonzero vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^p$ is defined by $d_{\\cos}(\\mathbf{x},\\mathbf{y}) = 1 - \\dfrac{\\mathbf{x}^\\top \\mathbf{y}}{\\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2}$. If all vectors are normalized to unit norm, this reduces to $d_{\\cos}(\\mathbf{x},\\mathbf{y}) = 1 - \\mathbf{x}^\\top \\mathbf{y}$.\n- The $k$-nearest neighbors classifier assigns a query to the majority label among its $k$ nearest training samples under the specified distance; use odd $k$ to avoid vote ties.\n\nData model:\n- There are exactly $2$ balanced classes with class prototypes $\\mathbf{u}_1, \\mathbf{u}_2 \\in \\mathbb{R}^p$ on the unit sphere with a prescribed inner product $\\mathbf{u}_1^\\top \\mathbf{u}_2 = c$, where $c \\in (-1,1)$ is a fixed scalar controlling class separation. Construct $\\mathbf{u}_1$ as the first basis vector, namely $\\mathbf{u}_1 = (1,0,\\dots,0)^\\top \\in \\mathbb{R}^p$, and construct $\\mathbf{u}_2$ as\n$$\n\\mathbf{u}_2 = c \\, \\mathbf{e}_1 + \\sqrt{1-c^2} \\, \\mathbf{e}_2,\n$$\nwhere $\\mathbf{e}_1, \\mathbf{e}_2$ are the first two canonical basis vectors in $\\mathbb{R}^p$.\n- For each class $y \\in \\{0,1\\}$ with prototype $\\mathbf{u}_{y+1}$, generate samples by\n$$\n\\tilde{\\mathbf{x}} = \\mathbf{u}_{y+1} + \\frac{\\sigma}{\\sqrt{p}} \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p), \\quad \\mathbf{x} = \\frac{\\tilde{\\mathbf{x}}}{\\lVert \\tilde{\\mathbf{x}} \\rVert_2}.\n$$\nThis yields unit-normalized embeddings with controlled perturbation magnitude governed by $\\sigma  0$.\n- Use exactly $2$ classes with equal sample counts per split. Use the same data generation parameters $\\sigma$ and $c$ across all test cases, but vary the dimension $p$.\n\nEvaluation protocol:\n- For each test case, create a training set of size $n_{\\text{train}}$ and a validation set of size $n_{\\text{val}}$, both class-balanced, using independent draws from the model above.\n- Consider the candidate neighbor counts $k \\in \\{1,3,5,7,9,11,13,15\\}$. For each $k$, build a $k$-nearest neighbors classifier under cosine distance using the training set and compute its validation accuracy, defined as the fraction of correctly classified validation samples expressed as a decimal in $[0,1]$.\n- Select the $k$ that attains the highest validation accuracy. If multiple $k$ values tie for the best validation accuracy, choose the smallest such $k$.\n\nDeterminism:\n- For each test case, use a fixed pseudorandom number generator seed equal to $s = 424242 + p$ to generate both the training and validation sets for that dimension $p$, so that results are reproducible. Do not use any other sources of randomness.\n\nTest suite:\n- Use the following three test cases, which vary only in the dimension $p$:\n  1. Case A: $p = 10$, $n_{\\text{train}} = 400$, $n_{\\text{val}} = 200$, $\\sigma = 0.7$, $c = 0.25$.\n  2. Case B: $p = 100$, $n_{\\text{train}} = 400$, $n_{\\text{val}} = 200$, $\\sigma = 0.7$, $c = 0.25$.\n  3. Case C: $p = 500$, $n_{\\text{train}} = 400$, $n_{\\text{val}} = 200$, $\\sigma = 0.7$, $c = 0.25$.\n\nOutput specification:\n- Your program must produce a single line of output containing the optimal neighbor counts, in the order of the test cases above, formatted as a comma-separated list enclosed in square brackets, for example $[1,3,5]$.\n- The outputs for each test case must be integers corresponding to the chosen $k$.\n\nConstraints:\n- Implement the full simulation, classification, and model selection procedure exactly as specified.\n- Ensure numerical stability by normalizing all generated vectors to unit norm before computing cosine similarities.\n- Use only the allowed libraries and adhere to the deterministic seeding rule.\n\nYour task: Write a complete, runnable program that generates the data, evaluates validation accuracy for each candidate $k$, selects the optimal $k$ per test case under the tie-breaking rule, and prints the three selected $k$ values in the required format on a single line.", "solution": "The user's problem is to conduct a numerical experiment to determine the optimal number of neighbors $k$ in a $k$-nearest neighbors (KNN) classifier. This investigation is performed under specific, controlled conditions: the data is synthetic, generated from a model of two classes on a unit hypersphere, and the classification is based on cosine distance. The experiment is to be repeated for three different data dimensionalities $p$ to observe how $p$ influences the choice of $k$.\n\nThe problem has been validated and is deemed valid. It is scientifically sound, well-posed, and all parameters and procedures are specified unambiguously, ensuring the experiment is reproducible. We will now proceed with a detailed solution.\n\n### 1. Principle of the Simulation\n\nThe core of the problem is to perform model selection for the hyperparameter $k$ of a KNN classifier. The general procedure for model selection involves:\n1.  Defining a set of candidate hyperparameter values.\n2.  Splitting the available data into a training set and a validation set.\n3.  For each candidate hyperparameter value, training a model on the training set.\n4.  Evaluating the performance of each trained model on the validation set using a chosen metric.\n5.  Selecting the hyperparameter value that yields the best performance on the validation set.\n\nThis problem specifies each of these components precisely.\n\n### 2. Data Generation Model\n\nThe data is generated in a way that creates two distinct, but potentially overlapping, classes of points on the surface of a $p$-dimensional unit hypersphere.\n\n**Class Prototypes**: The centers of the two classes are defined by two prototype vectors, $\\mathbf{u}_1$ and $\\mathbf{u}_2$, in $\\mathbb{R}^p$. These are constructed to be unit vectors with a specified cosine similarity (inner product), $\\mathbf{u}_1^\\top \\mathbf{u}_2 = c$.\n-   $\\mathbf{u}_1$ is set to the first canonical basis vector:\n    $$ \\mathbf{u}_1 = \\mathbf{e}_1 = (1, 0, \\dots, 0)^\\top $$\n-   $\\mathbf{u}_2$ is constructed in the plane spanned by $\\mathbf{e}_1$ and $\\mathbf{e}_2$:\n    $$ \\mathbf{u}_2 = c \\, \\mathbf{e}_1 + \\sqrt{1-c^2} \\, \\mathbf{e}_2 $$\n    This construction ensures $\\lVert \\mathbf{u}_1 \\rVert_2 = 1$, $\\lVert \\mathbf{u}_2 \\rVert_2 = 1$, and $\\mathbf{u}_1^\\top \\mathbf{u}_2 = c$. The parameter $c \\in (-1,1)$ controls the separation between the classes: $c \\to 1$ makes the classes very close, while $c \\to -1$ makes them antipodal.\n\n**Sample Generation**: For each class $y \\in \\{0, 1\\}$, data points are generated by taking the corresponding prototype $\\mathbf{u}_{y+1}$ and adding Gaussian noise.\n1.  A pre-normalized vector $\\tilde{\\mathbf{x}}$ is created:\n    $$ \\tilde{\\mathbf{x}} = \\mathbf{u}_{y+1} + \\frac{\\sigma}{\\sqrt{p}} \\mathbf{z}, \\quad \\text{where} \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p) $$\n    Here, $\\sigma  0$ controls the magnitude of the perturbation. The noise term $\\mathbf{z}$ is scaled by $\\frac{1}{\\sqrt{p}}$ to ensure that the expected squared norm of the noise, $\\mathbb{E}\\left[\\left\\lVert \\frac{\\sigma}{\\sqrt{p}} \\mathbf{z} \\right\\rVert_2^2\\right]$, is equal to $\\sigma^2$ and thus independent of the dimension $p$. This prevents the noise from overwhelming the signal (the prototype vector) as dimensionality increases.\n2.  The vector is then projected onto the unit hypersphere:\n    $$ \\mathbf{x} = \\frac{\\tilde{\\mathbf{x}}}{\\lVert \\tilde{\\mathbf{x}} \\rVert_2} $$\n    This step ensures all data points have a unit norm, which is a prerequisite for the simplified cosine distance calculation.\n\nFor each experimental case (defined by dimension $p$), we must generate a training set of size $n_{\\text{train}}$ and a validation set of size $n_{\\text{val}}$, both with an equal number of samples from each of the two classes.\n\n**Reproducibility**: To ensure deterministic results, a specific pseudorandom number generator seed, $s = 424242 + p$, is used for each dimension $p$.\n\n### 3. $k$-Nearest Neighbors Classification\n\nThe KNN classifier is a non-parametric algorithm. To classify a query point $\\mathbf{x}_q$, it performs the following steps:\n1.  Calculate the distance between $\\mathbf{x}_q$ and every point $\\mathbf{x}_i$ in the training set.\n2.  Identify the $k$ training points that are closest to $\\mathbf{x}_q$. These are its $k$-nearest neighbors.\n3.  Assign to $\\mathbf{x}_q$ the class label that is most frequent among its $k$-nearest neighbors (majority vote). The problem specifies using odd values of $k$ to prevent ties in the vote.\n\n**Distance Metric**: The problem mandates the use of cosine distance:\n$$ d_{\\cos}(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\frac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\lVert \\mathbf{x}_i \\rVert_2 \\lVert \\mathbf{x}_j \\rVert_2} $$\nSince all data points are normalized to have unit norm, i.e., $\\lVert \\mathbf{x} \\rVert_2 = 1$, this simplifies to:\n$$ d_{\\cos}(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\mathbf{x}_i^\\top \\mathbf{x}_j $$\nMinimizing this distance is equivalent to maximizing the cosine similarity, $\\text{sim}_{\\cos}(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$. Computationally, it is efficient to compute a matrix of cosine similarities between all validation points and all training points using a single matrix multiplication: $\\mathbf{S} = \\mathbf{X}_{\\text{val}} \\mathbf{X}_{\\text{train}}^\\top$. Each row of $\\mathbf{S}$ then contains the similarities of one validation point to all training points.\n\n### 4. Evaluation and Optimal $k$ Selection\n\nThe objective is to find the optimal $k$ from the candidate set $K = \\{1, 3, 5, 7, 9, 11, 13, 15\\}$. The procedure for a given test case is as follows:\n1.  Generate the training data $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$ and validation data $(\\mathbf{X}_{\\text{val}}, \\mathbf{y}_{\\text{val}})$.\n2.  For each validation point, find its nearest neighbors in the training set by sorting the cosine similarities.\n3.  For each candidate value $k \\in K$:\n    a. For each validation point, determine the predicted label by taking the majority vote among its $k$ nearest neighbors.\n    b. Calculate the validation accuracy:\n       $$ \\text{Accuracy}(k) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\mathbb{I}(\\hat{y}_i = y_i) $$\n       where $\\hat{y}_i$ is the predicted label and $y_i$ is the true label for the $i$-th validation sample, and $\\mathbb{I}(\\cdot)$ is the indicator function.\n4.  After computing the accuracy for all candidate $k$ values, select the optimal $k^*$. The selection rule is to choose the $k$ that maximizes the accuracy. If there is a tie, the smallest $k$ among those that achieved the maximum accuracy is chosen.\n\nThis entire process is repeated for each of the three test cases, varying the dimension $p$.\n\n### 5. Algorithmic Implementation\n\nThe implementation will follow these steps for each test case $(p, n_{\\text{train}}, n_{\\text{val}}, \\sigma, c)$:\n\n1.  **Initialize**: Set the seed for the random number generator to $s = 424242 + p$.\n2.  **Generate Data**:\n    -   Construct prototypes $\\mathbf{u}_1$ and $\\mathbf{u}_2$.\n    -   Generate $n_{\\text{train}}/2$ samples for class 0 and $n_{\\text{train}}/2$ for class 1 to form $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$. Normalize all vectors in $\\mathbf{X}_{\\text{train}}$.\n    -   Repeat the process for the validation set to obtain $(\\mathbf{X}_{\\text{val}}, \\mathbf{y}_{\\text{val}})$.\n3.  **Compute Similarities**: Calculate the similarity matrix $\\mathbf{S} = \\mathbf{X}_{\\text{val}} \\mathbf{X}_{\\text{train}}^\\top$. Its shape will be $(n_{\\text{val}}, n_{\\text{train}})$.\n4.  **Find Neighbor Indices**: For each validation sample (row of $\\mathbf{S}$), sort the training sample indices in descending order of similarity. This can be done efficiently using `numpy.argsort` on $-\\mathbf{S}$.\n5.  **Evaluate Each $k$**:\n    -   Iterate through the sorted candidate list $k \\in \\{1, 3, \\dots, 15\\}$.\n    -   For each $k$, select the top $k$ neighbor indices.\n    -   Use these indices to retrieve the labels of the neighbors from $\\mathbf{y}_{\\text{train}}$.\n    -   Use `scipy.stats.mode` to find the majority label for each validation sample.\n    -   Compute and store the accuracy.\n6.  **Select Optimal $k$**: Find the maximum accuracy achieved and the smallest $k$ that produced it.\n7.  **Store Result**: Append the optimal $k$ for the current test case to a results list.\n8.  **Output**: After processing all test cases, format and print the list of optimal $k$ values.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import mode\n\ndef solve():\n    \"\"\"\n    Main function to run the KNN simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (p, n_train, n_val, sigma, c)\n        (10, 400, 200, 0.7, 0.25),   # Case A\n        (100, 400, 200, 0.7, 0.25),  # Case B\n        (500, 400, 200, 0.7, 0.25),  # Case C\n    ]\n\n    candidate_ks = [1, 3, 5, 7, 9, 11, 13, 15]\n    \n    optimal_k_results = []\n\n    for p, n_train, n_val, sigma, c in test_cases:\n        seed = 424242 + p\n        optimal_k = find_optimal_k(p, n_train, n_val, sigma, c, seed, candidate_ks)\n        optimal_k_results.append(optimal_k)\n    \n    print(f\"[{','.join(map(str, optimal_k_results))}]\")\n\ndef generate_data(p, n_samples, sigma, c, u1, u2, rng):\n    \"\"\"\n    Generates balanced, unit-normalized data for two classes.\n    \"\"\"\n    n_per_class = n_samples // 2\n    \n    # Class 0 data generation\n    z0 = rng.normal(loc=0.0, scale=1.0, size=(n_per_class, p))\n    x_tilde0 = u1 + (sigma / np.sqrt(p)) * z0\n    \n    # Class 1 data generation\n    z1 = rng.normal(loc=0.0, scale=1.0, size=(n_per_class, p))\n    x_tilde1 = u2 + (sigma / np.sqrt(p)) * z1\n    \n    # Combine and normalize\n    X = np.vstack([x_tilde0, x_tilde1])\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    # Avoid division by zero, although very unlikely with this data model\n    norms[norms == 0] = 1.0\n    X_normalized = X / norms\n    \n    # Create labels\n    y = np.array([0] * n_per_class + [1] * n_per_class, dtype=int)\n    \n    return X_normalized, y\n\ndef find_optimal_k(p, n_train, n_val, sigma, c, seed, candidate_ks):\n    \"\"\"\n    Finds the optimal k for a given set of simulation parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct class prototypes\n    u1 = np.zeros(p)\n    u1[0] = 1.0\n    \n    u2 = np.zeros(p)\n    u2[0] = c\n    if p > 1:\n      u2[1] = np.sqrt(1 - c**2)\n\n    # 2. Generate training and validation sets\n    X_train, y_train = generate_data(p, n_train, sigma, c, u1, u2, rng)\n    X_val, y_val = generate_data(p, n_val, sigma, c, u1, u2, rng)\n    \n    # 3. Compute cosine similarities\n    # For unit vectors, cosine similarity is the dot product.\n    similarities = X_val @ X_train.T  # Shape: (n_val, n_train)\n    \n    # 4. Get indices of neighbors, sorted by similarity (descending)\n    # We sort the negative similarities to get descending order of similarity.\n    sorted_neighbor_indices = np.argsort(-similarities, axis=1)\n\n    accuracies = []\n    \n    # 5. Evaluate each k\n    for k in candidate_ks:\n        # Get labels of the k-nearest neighbors for each validation point\n        top_k_indices = sorted_neighbor_indices[:, :k]\n        top_k_labels = y_train[top_k_indices] # Shape: (n_val, k)\n        \n        # Predict labels by majority vote\n        # scipy.stats.mode is used for efficient majority voting along axis 1.\n        # For scipy>=1.9.0, it returns a ModeResult object. keepdims=False is default\n        # and returns a 1D array of modes if axis is specified.\n        predicted_labels = mode(top_k_labels, axis=1).mode.flatten()\n        \n        # Calculate accuracy\n        accuracy = np.mean(predicted_labels == y_val)\n        accuracies.append(accuracy)\n\n    # 6. Select optimal k based on the specified rule\n    accuracies = np.array(accuracies)\n    max_accuracy = np.max(accuracies)\n    \n    # Find all k values that achieve the max accuracy\n    best_k_indices = np.where(accuracies == max_accuracy)[0]\n    \n    # Tie-breaking: choose the smallest k\n    optimal_k_index = best_k_indices[0]\n    optimal_k = candidate_ks[optimal_k_index]\n    \n    return optimal_k\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3108143"}]}