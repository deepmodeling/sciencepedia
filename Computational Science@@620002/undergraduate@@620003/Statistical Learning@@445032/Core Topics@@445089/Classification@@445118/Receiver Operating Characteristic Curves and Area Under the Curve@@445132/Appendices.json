{"hands_on_practices": [{"introduction": "To truly master the Area Under the Curve (AUC), we begin with its most fundamental interpretation: the probability that a classifier ranks a randomly chosen positive instance higher than a randomly chosen negative one. This exercise provides a hands-on opportunity to calculate AUC from scratch on a small dataset, focusing on the critical and often overlooked detail of handling tied scores. By exploring a generalized definition of AUC [@problem_id:3167068], you will gain a deeper appreciation for its properties, such as its invariance to score transformations, and how different conventions for ties can affect the final value.", "problem": "Consider a binary classification setting where a scoring function assigns a real-valued score $S$ to each instance. The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) versus the False Positive Rate (FPR) as a decision threshold $\\tau$ sweeps from $+\\infty$ to $-\\infty$, where $\\text{TPR}(\\tau) = \\mathbb{P}(S \\ge \\tau \\mid Y=1)$ and $\\text{FPR}(\\tau) = \\mathbb{P}(S \\ge \\tau \\mid Y=0)$. The Area Under the Curve (AUC) is the area under this ROC curve.\n\nTo investigate the contribution of ties to the area under the curve, define for $\\lambda \\in [0,1]$ the generalized area under the curve\n$$\nAUC_\\lambda \\;=\\; \\mathbb{P}(S^+ > S^-) \\;+\\; \\lambda \\,\\mathbb{P}(S^+ = S^-),\n$$\nwhere $S^+$ is the score of a randomly selected positive instance and $S^-$ is the score of a randomly selected negative instance, with the randomness taken uniformly over the empirical positive and negative sets and independently between them.\n\nYou are given scores for $n_+ = 5$ positives and $n_- = 5$ negatives:\n- Positives: $S^+ \\in \\{0.2,\\,0.4,\\,0.6,\\,0.6,\\,0.8\\}$,\n- Negatives: $S^- \\in \\{0.1,\\,0.4,\\,0.4,\\,0.7,\\,0.9\\}$.\n\nFirst, compute $\\mathbb{P}(S^+ > S^-)$ and $\\mathbb{P}(S^+ = S^-)$ for the given lists by counting all $n_+ n_- = 25$ positiveâ€“negative pairs and converting counts to probabilities. Then, use these to evaluate $AUC_\\lambda$ for $\\lambda \\in \\{0,\\,1/2,\\,1\\}$.\n\nSelect all options that are correct.\n\nA. For the given scores, $AUC_{1/2} = 0.52$.\n\nB. For any strictly increasing function $g:\\mathbb{R}\\to\\mathbb{R}$, the value of $AUC_\\lambda$ computed from $g(S)$ is identical to that computed from $S$ for all $\\lambda \\in [0,1]$.\n\nC. If the score distributions are continuous so that $\\mathbb{P}(S^+ = S^-)=0$, then $AUC_\\lambda$ is independent of $\\lambda$ and equals $\\mathbb{P}(S^+ > S^-)$.\n\nD. The standard empirical area under the ROC curve obtained by sweeping thresholds and using the trapezoidal rule corresponds to $\\lambda = 1$.\n\nE. For fixed score distributions, $AUC_\\lambda$ is a linear function of $\\lambda$ with slope $\\mathbb{P}(S^+ = S^-)$.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides a clear definition for a generalized area under the curve, $AUC_\\lambda$, and gives a specific dataset to perform calculations and evaluate conceptual statements. There are no contradictions, ambiguities, or missing information.\n\nThe core of the problem requires calculating probabilities related to score comparisons and then analyzing the properties of the $AUC_\\lambda$ function.\n\nFirst, we compute the required probabilities from the given data.\nThe set of positive scores is $S^+ \\in \\{0.2, 0.4, 0.6, 0.6, 0.8\\}$, with $n_+ = 5$.\nThe set of negative scores is $S^- \\in \\{0.1, 0.4, 0.4, 0.7, 0.9\\}$, with $n_- = 5$.\nThe total number of pairs of one positive and one negative instance is $n_+ n_- = 5 \\times 5 = 25$.\n\nWe need to count the number of pairs $(S^+, S^-)$ that satisfy $S^+ > S^-$, $S^+ = S^-$, and $S^+ < S^-$. We can do this systematically for each positive score:\n- For $S^+ = 0.2$:\n  - $0.2 > 0.1$ ($1$ pair)\n  - $0.2 < 0.4$ ($2$ pairs)\n  - $0.2 < 0.7$ ($1$ pair)\n  - $0.2 < 0.9$ ($1$ pair)\n  - Total for $S^+=0.2$: $1$ pair with $S^+ > S^-$, $0$ pairs with $S^+ = S^-$.\n\n- For $S^+ = 0.4$:\n  - $0.4 > 0.1$ ($1$ pair)\n  - $0.4 = 0.4$ ($2$ pairs)\n  - $0.4 < 0.7$ ($1$ pair)\n  - $0.4 < 0.9$ ($1$ pair)\n  - Total for $S^+=0.4$: $1$ pair with $S^+ > S^-$, $2$ pairs with $S^+ = S^-$.\n\n- For each of the two scores $S^+ = 0.6$:\n  - $0.6 > 0.1$ ($1$ pair)\n  - $0.6 > 0.4$ ($2$ pairs)\n  - $0.6 < 0.7$ ($1$ pair)\n  - $0.6 < 0.9$ ($1$ pair)\n  - Total for each $S^+=0.6$: $3$ pairs with $S^+ > S^-$, $0$ pairs with $S^+ = S^-$.\n  - For both scores of $0.6$, this gives $2 \\times 3 = 6$ pairs with $S^+ > S^-$.\n\n- For $S^+ = 0.8$:\n  - $0.8 > 0.1$ ($1$ pair)\n  - $0.8 > 0.4$ ($2$ pairs)\n  - $0.8 > 0.7$ ($1$ pair)\n  - $0.8 < 0.9$ ($1$ pair)\n  - Total for $S^+=0.8$: $4$ pairs with $S^+ > S^-$, $0$ pairs with $S^+ = S^-$.\n\nSumming these counts:\n- Total number of pairs with $S^+ > S^-$: $1 + 1 + 6 + 4 = 12$.\n- Total number of pairs with $S^+ = S^-$: $0 + 2 + 0 + 0 = 2$.\n- The remaining pairs must have $S^+ < S^-$, with a count of $25 - 12 - 2 = 11$.\n\nNow, we convert these counts into probabilities by dividing by the total number of pairs, $25$:\n$$\n\\mathbb{P}(S^+ > S^-) = \\frac{12}{25} = 0.48\n$$\n$$\n\\mathbb{P}(S^+ = S^-) = \\frac{2}{25} = 0.08\n$$\n\nThe formula for the generalized area under the curve is:\n$$\nAUC_\\lambda = \\mathbb{P}(S^+ > S^-) + \\lambda \\mathbb{P}(S^+ = S^-)\n$$\nSubstituting the computed probabilities:\n$$\nAUC_\\lambda = 0.48 + \\lambda(0.08)\n$$\n\nNow we evaluate each option.\n\n**A. For the given scores, $AUC_{1/2} = 0.52$.**\nWe set $\\lambda = 1/2$ in our derived expression for $AUC_\\lambda$:\n$$\nAUC_{1/2} = 0.48 + \\frac{1}{2}(0.08) = 0.48 + 0.04 = 0.52\n$$\nThis statement matches our calculation.\nVerdict: **Correct**.\n\n**B. For any strictly increasing function $g:\\mathbb{R}\\to\\mathbb{R}$, the value of $AUC_\\lambda$ computed from $g(S)$ is identical to that computed from $S$ for all $\\lambda \\in [0,1]$.**\nThe definition of $AUC_\\lambda$ depends on the probabilities $\\mathbb{P}(S^+ > S^-)$ and $\\mathbb{P}(S^+ = S^-)$. These probabilities are determined by the ordering and equality of score pairs. A strictly increasing function $g$ preserves orderings, meaning that for any two numbers $a$ and $b$:\n- $a > b \\iff g(a) > g(b)$\n- $a = b \\iff g(a) = g(b)$\n- $a < b \\iff g(a) < g(b)$\nTherefore, the events $\\{S^+ > S^-\\}$ and $\\{g(S^+) > g(S^-)\\}$ are equivalent, and so are the events $\\{S^+ = S^-\\}$ and $\\{g(S^+) = g(S^-)\\}$. This means their probabilities are identical:\n$$\n\\mathbb{P}(g(S^+) > g(S^-)) = \\mathbb{P}(S^+ > S^-)\n$$\n$$\n\\mathbb{P}(g(S^+) = g(S^-)) = \\mathbb{P}(S^+ = S^-)\n$$\nSince the two component probabilities of $AUC_\\lambda$ are invariant under such a transformation $g$, the value of $AUC_\\lambda$ for any $\\lambda$ is also invariant. This is a fundamental property of AUC.\nVerdict: **Correct**.\n\n**C. If the score distributions are continuous so that $\\mathbb{P}(S^+ = S^-)=0$, then $AUC_\\lambda$ is independent of $\\lambda$ and equals $\\mathbb{P}(S^+ > S^-)$.**\nGiven the formula $AUC_\\lambda = \\mathbb{P}(S^+ > S^-) + \\lambda \\mathbb{P}(S^+ = S^-)$.\nIf we assume the distributions are continuous, the probability of a tie between two independent samples is zero, so $\\mathbb{P}(S^+ = S^-) = 0$.\nSubstituting this into the formula gives:\n$$\nAUC_\\lambda = \\mathbb{P}(S^+ > S^-) + \\lambda \\cdot 0 = \\mathbb{P}(S^+ > S^-)\n$$\nThe resulting expression, $\\mathbb{P}(S^+ > S^-)$, does not contain $\\lambda$. Therefore, $AUC_\\lambda$ is independent of $\\lambda$ under this condition. This is the standard definition of AUC for continuous scores.\nVerdict: **Correct**.\n\n**D. The standard empirical area under the ROC curve obtained by sweeping thresholds and using the trapezoidal rule corresponds to $\\lambda = 1$.**\nThe standard method for calculating the empirical AUC from a set of scores involves constructing the ROC curve and finding the area under it. The ROC curve is a set of points $(FPR, TPR)$. When ties are present between positive and negative instances at a certain score threshold, the curve moves diagonally. Using the trapezoidal rule to calculate the area under this piecewise linear curve is equivalent to linear interpolation between ROC points. This method is also equivalent to the Wilcoxon-Mann-Whitney U statistic, which, when normalized, calculates the AUC as:\n$$\nAUC = \\frac{1}{n_+ n_-} \\left( \\sum_{S^+ > S^-} 1 + \\frac{1}{2} \\sum_{S^+ = S^-} 1 \\right) = \\mathbb{P}(S^+ > S^-) + \\frac{1}{2}\\mathbb{P}(S^+ = S^-)\n$$\nThis corresponds to $AUC_{1/2}$, i.e., $\\lambda = 1/2$. This choice reflects the idea of randomly breaking ties. The case $\\lambda = 1$ corresponds to $AUC_1 = \\mathbb{P}(S^+ > S^-) + \\mathbb{P}(S^+ = S^-) = \\mathbb{P}(S^+ \\ge S^-)$, which represents an optimistic handling of ties where they are always counted as correct classifications. This is not the standard defined by the trapezoidal rule.\nVerdict: **Incorrect**.\n\n**E. For fixed score distributions, $AUC_\\lambda$ is a linear function of $\\lambda$ with slope $\\mathbb{P}(S^+ = S^-)$.**\nThe formula is $AUC_\\lambda = \\mathbb{P}(S^+ > S^-) + \\lambda \\mathbb{P}(S^+ = S^-)$.\nFor fixed score distributions, the quantities $\\mathbb{P}(S^+ > S^-)$ and $\\mathbb{P}(S^+ = S^-)$ are constants. Let's denote them by $C_1 = \\mathbb{P}(S^+ > S^-)$ and $C_2 = \\mathbb{P}(S^+ = S^-)$. The expression becomes:\n$$\nAUC_\\lambda = C_1 + C_2 \\lambda\n$$\nThis is an equation of the form $y = c + mx$, where $y = AUC_\\lambda$, $x = \\lambda$, the intercept is $c=C_1$, and the slope is $m=C_2$. Thus, $AUC_\\lambda$ is indeed a linear function of $\\lambda$, and its slope is the coefficient of $\\lambda$, which is $C_2 = \\mathbb{P}(S^+ = S^-)$.\nVerdict: **Correct**.\n\nFinal summary of verdicts:\n- A: Correct\n- B: Correct\n- C: Correct\n- D: Incorrect\n- E: Correct\n\nThe correct options are A, B, C, and E.", "answer": "$$\\boxed{ABCE}$$", "id": "3167068"}, {"introduction": "While AUC is a powerful evaluation metric, its true potential is unlocked when we use it as an objective for model training, a common practice in fields like learning-to-rank. This practice guides you through the essential steps of making AUC optimizable, starting with the derivation of its gradient, which reveals challenges due to its non-differentiable nature. You will then construct and differentiate a smooth surrogate for the AUC, providing a direct link between the theory of ROC analysis and the practical application of gradient-based optimization algorithms [@problem_id:3167109].", "problem": "Consider a binary classification and learning-to-rank scenario with $n$ instances indexed by $i \\in \\{1,\\dots,n\\}$, true labels $y_i \\in \\{0,1\\}$, and real-valued scores $s_i \\in \\mathbb{R}$. Let $\\mathcal{P} = \\{i: y_i = 1\\}$ denote the set of positives with $n_{+} = |\\mathcal{P}|$, and $\\mathcal{N} = \\{j: y_j = 0\\}$ denote the set of negatives with $n_{-} = |\\mathcal{N}|$. The empirical Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve is defined as the average over all positive-negative pairs of the indicator that the positive score exceeds the negative score.\n\nStarting from fundamental definitions of the Heaviside indicator and differentiability, do the following:\n\n1) Derive, from first principles, the gradient (or subgradient where appropriate) with respect to the score vector $s = (s_1,\\dots,s_n)$ of the empirical Area Under the Curve (AUC), defined by\n$$\n\\mathrm{AUC}(s) \\equiv \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\mathbf{1}\\{ s_i - s_j > 0 \\}.\n$$\nClearly state where the function is differentiable, and provide the resulting expression for $\\nabla_{s} \\mathrm{AUC}(s)$ or a subgradient when differentiability fails.\n\n2) To obtain a differentiable surrogate suitable for optimization, replace the indicator $\\mathbf{1}\\{x > 0\\}$ with the logistic sigmoid $\\sigma_{\\tau}(x) \\equiv \\frac{1}{1 + \\exp(-x/\\tau)}$ with temperature parameter $\\tau > 0$, and define the smooth surrogate\n$$\nA_{\\tau}(s) \\equiv \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\sigma_{\\tau}(s_i - s_j).\n$$\nDerive the exact analytic expression for the gradient $\\nabla_{s} A_{\\tau}(s)$.\n\n3) Consider the concrete case with $n = 4$, labels $y = (1, 0, 1, 0)$ so that $\\mathcal{P} = \\{1, 3\\}$ and $\\mathcal{N} = \\{2, 4\\}$, score vector $s = (1.2, 0.8, 0.5, 0.3)$, temperature $\\tau = 1$, and direction vector $v = (1, -2, 0.5, 3)$. Compute the directional derivative of the smooth surrogate at $s$ along $v$, that is,\n$$\nD A_{\\tau}(s)[v] \\equiv \\nabla_{s} A_{\\tau}(s)^{\\top} v,\n$$\nas a real number. Round your final numerical result to four significant figures. Express your answer as a unitless real number.", "solution": "The problem is evaluated to be scientifically grounded, well-posed, and contains all necessary information for a unique solution. The steps are logically consistent and pertain to standard concepts in statistical learning. Thus, the problem is valid.\n\nThe solution is presented in three parts, as requested.\n\n### Part 1: Gradient of the Empirical AUC\n\nThe empirical Area Under the Curve (AUC) is given by:\n$$\n\\mathrm{AUC}(s) = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\mathbf{1}\\{ s_i - s_j > 0 \\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, which can be defined using the Heaviside step function $H(x)$ as $\\mathbf{1}\\{x > 0\\} \\equiv H(x)$. The function $H(x)$ is $1$ for $x > 0$ and $0$ for $x \\leq 0$. The function $\\mathrm{AUC}(s)$ is a sum of these step functions and is thus piecewise constant.\n\nThe function is differentiable at any point $s$ for which $s_i \\neq s_j$ for all pairs $(i, j)$ with $i \\in \\mathcal{P}$ and $j \\in \\mathcal{N}$. At such points, a small perturbation in any score $s_k$ does not change the value of any indicator function, so the function is locally constant. The gradient is therefore the zero vector:\n$$\n\\nabla_{s} \\mathrm{AUC}(s) = \\mathbf{0} \\quad \\text{if } s_i \\neq s_j \\text{ for all } i \\in \\mathcal{P}, j \\in \\mathcal{N}.\n$$\nThe function is not differentiable at points where one or more scores of positive instances are tied with scores of negative instances, i.e., $s_i = s_j$ for some $i \\in \\mathcal{P}, j \\in \\mathcal{N}$. At these points, the concept of a subgradient or a generalized gradient is required.\n\nIn a distributional sense, the derivative of the Heaviside step function $H(x)$ is the Dirac delta function $\\delta(x)$. We use this to define a generalized gradient. The gradient of $\\mathrm{AUC}(s)$ with respect to the score vector $s$ is a vector where the $k$-th component is the partial derivative $\\frac{\\partial}{\\partial s_k} \\mathrm{AUC}(s)$.\n\nTo find the $k$-th component of the gradient, we differentiate the sum term by term:\n$$\n\\frac{\\partial}{\\partial s_k} \\mathrm{AUC}(s) = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\frac{\\partial}{\\partial s_k} H(s_i - s_j)\n$$\nUsing the chain rule, $\\frac{\\partial}{\\partial s_k} H(s_i - s_j) = \\delta(s_i - s_j) \\frac{\\partial}{\\partial s_k}(s_i - s_j)$. The term $\\frac{\\partial}{\\partial s_k}(s_i - s_j)$ evaluates to $1$ if $k=i$, $-1$ if $k=j$, and $0$ otherwise.\n\nWe consider two cases for the index $k$:\n\nCase 1: $k \\in \\mathcal{P}$ (the $k$-th instance is a positive).\nThe derivative $\\frac{\\partial}{\\partial s_k}(s_i - s_j)$ is non-zero only if $i=k$.\n$$\n\\frac{\\partial \\mathrm{AUC}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\delta(s_k - s_j) \\cdot (1) = \\frac{1}{n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\delta(s_k - s_j)\n$$\n\nCase 2: $k \\in \\mathcal{N}$ (the $k$-th instance is a negative).\nThe derivative $\\frac{\\partial}{\\partial s_k}(s_i - s_j)$ is non-zero only if $j=k$.\n$$\n\\frac{\\partial \\mathrm{AUC}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\delta(s_i - s_k) \\cdot (-1) = -\\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\delta(s_i - s_k)\n$$\nThese expressions define the components of the generalized gradient $\\nabla_{s} \\mathrm{AUC}(s)$. Where differentiable (i.e., no ties $s_i=s_j$), the arguments to the delta functions are all non-zero, making the gradient the zero vector, consistent with our earlier observation.\n\n### Part 2: Gradient of the Smooth Surrogate\n\nThe smooth surrogate for AUC is defined as:\n$$\nA_{\\tau}(s) = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\sigma_{\\tau}(s_i - s_j)\n$$\nwith the logistic sigmoid function $\\sigma_{\\tau}(x) = \\frac{1}{1 + \\exp(-x/\\tau)}$. Since $\\sigma_{\\tau}(x)$ is a smooth function for $\\tau > 0$, $A_{\\tau}(s)$ is also smooth and its gradient is well-defined everywhere.\n\nFirst, we derive the derivative of the sigmoid function with respect to its argument $x$:\n\\begin{align*}\n\\frac{d}{dx} \\sigma_{\\tau}(x) &= \\frac{d}{dx} \\left(1 + \\exp(-x/\\tau)\\right)^{-1} \\\\\n&= -1 \\cdot \\left(1 + \\exp(-x/\\tau)\\right)^{-2} \\cdot \\left(\\exp(-x/\\tau) \\cdot \\left(-\\frac{1}{\\tau}\\right)\\right) \\\\\n&= \\frac{1}{\\tau} \\frac{\\exp(-x/\\tau)}{\\left(1 + \\exp(-x/\\tau)\\right)^2} \\\\\n&= \\frac{1}{\\tau} \\left(\\frac{1}{1 + \\exp(-x/\\tau)}\\right) \\left(\\frac{\\exp(-x/\\tau)}{1 + \\exp(-x/\\tau)}\\right) \\\\\n&= \\frac{1}{\\tau} \\sigma_{\\tau}(x) \\left(\\frac{1 + \\exp(-x/\\tau) - 1}{1 + \\exp(-x/\\tau)}\\right) \\\\\n&= \\frac{1}{\\tau} \\sigma_{\\tau}(x) \\left(1 - \\frac{1}{1 + \\exp(-x/\\tau)}\\right) \\\\\n&= \\frac{1}{\\tau} \\sigma_{\\tau}(x) \\left(1 - \\sigma_{\\tau}(x)\\right)\n\\end{align*}\nLet's denote this derivative as $\\sigma'_{\\tau}(x)$. To find the $k$-th component of the gradient $\\nabla_{s} A_{\\tau}(s)$, we differentiate:\n$$\n\\frac{\\partial A_{\\tau}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} \\frac{\\partial}{\\partial s_k} \\sigma_{\\tau}(s_i - s_j)\n$$\nApplying the chain rule:\n$$\n\\frac{\\partial}{\\partial s_k} \\sigma_{\\tau}(s_i - s_j) = \\sigma'_{\\tau}(s_i - s_j) \\cdot \\frac{\\partial}{\\partial s_k}(s_i - s_j)\n$$\nAs before, we consider two cases for the index $k$:\n\nCase 1: $k \\in \\mathcal{P}$\n$$\n\\frac{\\partial A_{\\tau}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\sigma'_{\\tau}(s_k - s_j) = \\frac{1}{\\tau n_{+} n_{-}} \\sum_{j \\in \\mathcal{N}} \\sigma_{\\tau}(s_k - s_j) (1 - \\sigma_{\\tau}(s_k - s_j))\n$$\n\nCase 2: $k \\in \\mathcal{N}$\n$$\n\\frac{\\partial A_{\\tau}(s)}{\\partial s_k} = \\frac{1}{n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sigma'_{\\tau}(s_i - s_k) \\cdot (-1) = -\\frac{1}{\\tau n_{+} n_{-}} \\sum_{i \\in \\mathcal{P}} \\sigma_{\\tau}(s_i - s_k) (1 - \\sigma_{\\tau}(s_i - s_k))\n$$\nThese expressions give the exact analytic formula for each component of the gradient vector $\\nabla_{s} A_{\\tau}(s)$.\n\n### Part 3: Directional Derivative Calculation\n\nWe need to compute the directional derivative $D A_{\\tau}(s)[v] = \\nabla_{s} A_{\\tau}(s)^{\\top} v$.\nThe given parameters are:\n- Labels $y = (1, 0, 1, 0)$, so $\\mathcal{P} = \\{1, 3\\}$ and $\\mathcal{N} = \\{2, 4\\}$.\n- $n_{+} = 2$, $n_{-} = 2$.\n- Scores $s = (s_1, s_2, s_3, s_4) = (1.2, 0.8, 0.5, 0.3)$.\n- Temperature $\\tau = 1$. The sigmoid is $\\sigma_{1}(x) = (1 + \\exp(-x))^{-1}$.\n- Direction vector $v = (v_1, v_2, v_3, v_4) = (1, -2, 0.5, 3)$.\n\nThe gradient components are given by the formulas from Part 2, with the pre-factor being $\\frac{1}{\\tau n_{+} n_{-}} = \\frac{1}{1 \\cdot 2 \\cdot 2} = \\frac{1}{4}$. Let $\\sigma'(x) = \\sigma_1(x)(1-\\sigma_1(x))$.\n\nThe gradient components are:\n- For $k=1 \\in \\mathcal{P}$: $\\frac{\\partial A_1}{\\partial s_1} = \\frac{1}{4} \\left( \\sigma'(s_1-s_2) + \\sigma'(s_1-s_4) \\right)$\n- For $k=3 \\in \\mathcal{P}$: $\\frac{\\partial A_1}{\\partial s_3} = \\frac{1}{4} \\left( \\sigma'(s_3-s_2) + \\sigma'(s_3-s_4) \\right)$\n- For $k=2 \\in \\mathcal{N}$: $\\frac{\\partial A_1}{\\partial s_2} = -\\frac{1}{4} \\left( \\sigma'(s_1-s_2) + \\sigma'(s_3-s_2) \\right)$\n- For $k=4 \\in \\mathcal{N}$: $\\frac{\\partial A_1}{\\partial s_4} = -\\frac{1}{4} \\left( \\sigma'(s_1-s_4) + \\sigma'(s_3-s_4) \\right)$\n\nThe score differences are:\n- $s_1 - s_2 = 1.2 - 0.8 = 0.4$\n- $s_1 - s_4 = 1.2 - 0.3 = 0.9$\n- $s_3 - s_2 = 0.5 - 0.8 = -0.3$\n- $s_3 - s_4 = 0.5 - 0.3 = 0.2$\n\nThe values of $\\sigma'(x)$:\n- $\\sigma'(0.4) = \\sigma_1(0.4)(1-\\sigma_1(0.4)) \\approx 0.598688(1 - 0.598688) \\approx 0.240260$\n- $\\sigma'(0.9) = \\sigma_1(0.9)(1-\\sigma_1(0.9)) \\approx 0.710950(1 - 0.710950) \\approx 0.205504$\n- $\\sigma'(-0.3) = \\sigma_1(-0.3)(1-\\sigma_1(-0.3)) \\approx 0.425557(1 - 0.425557) \\approx 0.244460$\n- $\\sigma'(0.2) = \\sigma_1(0.2)(1-\\sigma_1(0.2)) \\approx 0.549834(1 - 0.549834) \\approx 0.247525$\n\nThe directional derivative is the dot product:\n$D A_1(s)[v] = \\sum_{k=1}^{4} \\frac{\\partial A_1}{\\partial s_k} v_k$.\n$$\nD A_1(s)[v] = \\frac{\\partial A_1}{\\partial s_1} v_1 + \\frac{\\partial A_1}{\\partial s_2} v_2 + \\frac{\\partial A_1}{\\partial s_3} v_3 + \\frac{\\partial A_1}{\\partial s_4} v_4\n$$\n$$\n= \\frac{1}{4} \\left( \\sigma'(0.4) + \\sigma'(0.9) \\right) \\cdot (1) \\\\\n- \\frac{1}{4} \\left( \\sigma'(0.4) + \\sigma'(-0.3) \\right) \\cdot (-2) \\\\\n+ \\frac{1}{4} \\left( \\sigma'(-0.3) + \\sigma'(0.2) \\right) \\cdot (0.5) \\\\\n- \\frac{1}{4} \\left( \\sigma'(0.9) + \\sigma'(0.2) \\right) \\cdot (3)\n$$\nWe can factor out $\\frac{1}{4}$ and group terms by $\\sigma'$ values:\n$$\n= \\frac{1}{4} \\left[ \\sigma'(0.4)(1+2) + \\sigma'(0.9)(1-3) + \\sigma'(-0.3)(2+0.5) + \\sigma'(0.2)(0.5-3) \\right]\n$$\n$$\n= \\frac{1}{4} \\left[ 3 \\sigma'(0.4) - 2 \\sigma'(0.9) + 2.5 \\sigma'(-0.3) - 2.5 \\sigma'(0.2) \\right]\n$$\nSubstituting the numerical values:\n$$\n\\approx \\frac{1}{4} \\left[ 3(0.240260) - 2(0.205504) + 2.5(0.244460) - 2.5(0.247525) \\right]\n$$\n$$\n= \\frac{1}{4} \\left[ 0.72078 - 0.411008 + 0.61115 - 0.6188125 \\right]\n$$\n$$\n= \\frac{1}{4} \\left[ 0.3021095 \\right] \\approx 0.075527375\n$$\nRounding to four significant figures, we get $0.07553$.", "answer": "$$\n\\boxed{0.07553}\n$$", "id": "3167109"}, {"introduction": "In the era of big data, calculating the exact AUC can be computationally prohibitive, as it naively requires comparing every positive-negative pair. This advanced exercise tackles this real-world challenge by guiding you through the implementation of a streaming AUC estimator using reservoir sampling, a clever technique for maintaining a representative sample from a massive data stream. By implementing this algorithm and deriving its probabilistic error bounds [@problem_id:3167103], you will bridge the gap between statistical theory and practical, large-scale machine learning engineering.", "problem": "You are given a streaming sequence of binary-labeled instances and a real-valued scoring function output for each instance. The goal is to estimate the Area Under the Receiver Operating Characteristic Curve (AUC) efficiently for massive datasets by processing the stream in fixed-size batches and using reservoir sampling to maintain a limited number of representative pairs. Receiver Operating Characteristic (ROC) is defined for a threshold $t$ on the score as the pair $(\\mathrm{False\\ Positive\\ Rate}, \\mathrm{True\\ Positive\\ Rate})$, where $\\mathrm{True\\ Positive\\ Rate}$ is the fraction of labeled positives with score at least $t$ and $\\mathrm{False\\ Positive\\ Rate}$ is the fraction of labeled negatives with score at least $t$. Area Under the Curve (AUC) is the area under the ROC curve as the threshold $t$ varies.\n\nImplement a batched estimator for AUC using the following constraints and requirements:\n\n- Streaming and batching: The dataset arrives as a stream of pairs $(s_i, y_i)$, where $s_i$ is a real-valued score and $y_i \\in \\{0,1\\}$ is the label. The stream should be processed in fixed-size batches of size $B$, updating a limited memory estimator as the stream progresses. No full dataset storage is allowed beyond what is needed to compute the exact empirical AUC for verification in this problem.\n- Reservoirs of positives and negatives: Maintain two reservoirs, one for positive-labeled scores and one for negative-labeled scores. Let the reservoir sizes be $m$ for positives and $n$ for negatives. Use reservoir sampling to ensure that, after processing the entire stream, each observed positive score has equal probability $m/N_+$ of inclusion in the positive reservoir (where $N_+$ is the total number of positives observed in the stream) and each observed negative score has equal probability $n/N_-$ of inclusion in the negative reservoir (where $N_-$ is the total number of negatives).\n- AUC estimation by pair comparisons: After the stream is fully processed, produce an AUC estimate by forming all possible pairs between the positive reservoir and the negative reservoir and averaging the comparison outcomes using the following tie-handling rule: for a pair $(s^+, s^-)$, contribute $1$ if $s^+ > s^-$, contribute $0$ if $s^+ < s^-$, and contribute $\\tfrac{1}{2}$ if $s^+ = s^-$. The estimator is the average of these contributions over all $m \\times n$ reservoir pairs.\n- Error bound as a function of sample size: Provide a theoretically justified bound $\\varepsilon(\\delta, m, n)$ on the absolute estimation error between the estimator and the empirical AUC computed from the full dataset, expressed as a function of the reservoir sizes $m$ and $n$ and a confidence parameter $\\delta \\in (0,1)$. The bound must be derived from a concentration inequality for functions with bounded differences and must be implemented to return a nonnegative real number. Your program must also verify, for each test case, whether the observed estimation error is within the bound, and return a boolean indicating the result.\n- Output specification: For each test case, output a list $[A_{\\mathrm{exact}}, A_{\\mathrm{est}}, \\varepsilon, \\mathrm{flag}]$, where $A_{\\mathrm{exact}}$ is the exact empirical AUC computed from all pairs in the dataset, $A_{\\mathrm{est}}$ is the reservoir-based estimate, $\\varepsilon$ is the computed bound for the given $\\delta$, and $\\mathrm{flag}$ is a boolean that is $\\mathrm{True}$ if $|A_{\\mathrm{est}} - A_{\\mathrm{exact}}| \\le \\varepsilon$, and $\\mathrm{False}$ otherwise. Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, e.g., $[[r_1],[r_2],[r_3]]$ where each $[r_k]$ follows the listed format.\n\nClarifications and required assumptions:\n- All comparisons and bounds must be expressed without physical units. No angle units are involved. Percentages must be expressed as decimals.\n- Acronyms must be fully defined upon first use: Receiver Operating Characteristic (ROC), Area Under the Curve (AUC).\n\nTest suite:\nImplement your program to generate and process the following five test cases deterministically, using the specified parameters and random seeds. In each case, create the stream by concatenating the positive and negative examples and shuffling with the given seed, then process with batch size $B$ while updating the reservoirs. For all cases, use the tie-handling rule specified above.\n\n1. Balanced, moderate separation:\n   - Positives: $N_+ = 2000$ scores sampled from a normal distribution with mean $1.0$ and standard deviation $1.0$.\n   - Negatives: $N_- = 2000$ scores sampled from a normal distribution with mean $0.0$ and standard deviation $1.0$.\n   - Reservoir sizes: $m = 200$, $n = 200$.\n   - Batch size: $B = 500$.\n   - Confidence: $\\delta = 0.05$.\n   - Shuffle seed: $42$.\n\n2. Small reservoirs:\n   - Positives: $N_+ = 1000$ from normal mean $0.5$, standard deviation $1.0$.\n   - Negatives: $N_- = 1000$ from normal mean $0.0$, standard deviation $1.0$.\n   - Reservoir sizes: $m = 1$, $n = 1$.\n   - Batch size: $B = 200$.\n   - Confidence: $\\delta = 0.10$.\n   - Shuffle seed: $123$.\n\n3. Imbalanced classes:\n   - Positives: $N_+ = 300$ from normal mean $0.8$, standard deviation $1.0$.\n   - Negatives: $N_- = 5000$ from normal mean $0.0$, standard deviation $1.0$.\n   - Reservoir sizes: $m = 100$, $n = 100$.\n   - Batch size: $B = 250$.\n   - Confidence: $\\delta = 0.05$.\n   - Shuffle seed: $17$.\n\n4. All scores equal:\n   - Positives: $N_+ = 1500$ all with score $0.0$.\n   - Negatives: $N_- = 1500$ all with score $0.0$.\n   - Reservoir sizes: $m = 50$, $n = 50$.\n   - Batch size: $B = 300$.\n   - Confidence: $\\delta = 0.05$.\n   - Shuffle seed: $777$.\n\n5. Near-perfect separation:\n   - Positives: $N_+ = 1000$ from normal mean $2.0$, standard deviation $0.1$.\n   - Negatives: $N_- = 1000$ from normal mean $0.0$, standard deviation $0.1$.\n   - Reservoir sizes: $m = 80$, $n = 120$.\n   - Batch size: $B = 250$.\n   - Confidence: $\\delta = 0.01$.\n   - Shuffle seed: $2025$.\n\nFinal output format:\nYour program must print exactly one line, a single Python-style list containing the five per-test-case result lists. For example:\n$[[A_{\\mathrm{exact}},A_{\\mathrm{est}},\\varepsilon,\\mathrm{flag}],\\ldots]$\nRound all floating-point outputs $A_{\\mathrm{exact}}$, $A_{\\mathrm{est}}$, and $\\varepsilon$ to $6$ decimal places in the final printed line.", "solution": "The problem requires the implementation of a streaming algorithm to estimate the Area Under the Receiver Operating Characteristic Curve (AUC), a common metric for evaluating binary classifiers. The estimation is performed on a limited memory budget by processing data in batches and using reservoir sampling. Additionally, a theoretical error bound on the estimate must be derived and verified.\n\n### 1. Theoretical Foundation of the Area Under the Curve (AUC)\n\nThe Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various discrimination thresholds. For a given dataset with positive instances having scores $\\{s^+_i\\}_{i=1}^{N_+}$ and negative instances with scores $\\{s^-_j\\}_{j=1}^{N_-}$, the empirical AUC is the area under this curve.\n\nA fundamental result in statistics establishes that this geometric area is equivalent to the probability that a randomly selected positive instance has a higher score than a randomly selected negative instance. This allows the AUC to be calculated using the Wilcoxon-Mann-Whitney U-statistic. Formally, the exact empirical AUC, $A_{\\mathrm{exact}}$, is given by:\n\n$$\nA_{\\mathrm{exact}} = \\frac{1}{N_+ N_-} \\sum_{i=1}^{N_+} \\sum_{j=1}^{N_-} \\psi(s^+_i, s^-_j)\n$$\n\nwhere the scoring function $\\psi$ handles comparisons and ties as specified:\n\n$$\n\\psi(a, b) = \\begin{cases} 1 & \\text{if } a > b \\\\ \\frac{1}{2} & \\text{if } a = b \\\\ 0 & \\text{if } a < b \\end{cases}\n$$\n\n### 2. Streaming Estimation with Reservoir Sampling\n\nFor massive datasets where storing all $N_+$ positive and $N_-$ negative scores is infeasible, we can estimate the AUC from a smaller, representative sample. The problem specifies a streaming context, where data arrives sequentially.\n\n**Reservoir Sampling (Algorithm R)** is an elegant and efficient method for drawing a simple random sample of a fixed size from a stream of unknown total length. We apply this technique independently to the substreams of positive and negative instances.\n\nThe algorithm proceeds as follows:\n- Two reservoirs, $R_{\\mathrm{pos}}$ of size $m$ and $R_{\\mathrm{neg}}$ of size $n$, are maintained for positive and negative scores, respectively.\n- We also track the total number of positive ($c_+$) and negative ($c_-$) instances seen so far.\n- When the $(c_+)$-th positive instance arrives:\n    - If the an instance is being added and the reservoir $R_{\\mathrm{pos}}$ is not yet full (i.e., $c_+ \\le m$), the new score is added to the reservoir.\n    - If the reservoir is full ($c_+ > m$), a random integer $j$ is generated uniformly from $[1, c_+]$. If $j \\le m$, the $j$-th element of the reservoir is replaced with the new score.\n- An identical process is applied to negative instances using $R_{\\mathrm{neg}}$, size $n$, and count $c_-$.\n\nThis procedure ensures that after the entire stream has been processed, each of the $N_+$ total positive scores has an equal probability $m/N_+$ of being in $R_{\\mathrm{pos}}$, and each of the $N_-$ total negative scores has an equal probability $n/N_-$ of being in $R_{\\mathrm{neg}}$. The batch size $B$ specified in the problem relates to the mechanics of data ingestion (e.g., for I/O efficiency) but does not alter the fundamental per-item logic of reservoir sampling.\n\nOnce the stream is exhausted, the reservoirs $R_{\\mathrm{pos}}$ and $R_{\\mathrm{neg}}$ contain uniform random samples of the full sets of positive and negative scores. The AUC estimate, $A_{\\mathrm{est}}$, is then computed as the empirical AUC on these smaller samples:\n\n$$\nA_{\\mathrm{est}} = \\frac{1}{mn} \\sum_{s^+ \\in R_{\\mathrm{pos}}} \\sum_{s^- \\in R_{\\mathrm{neg}}} \\psi(s^+, s^-)\n$$\n\nSince each score in the reservoirs is a uniform sample from its respective population, the expected value of this estimator, $E[A_{\\mathrm{est}}]$, is precisely the empirical AUC of the full dataset, $A_{\\mathrm{exact}}$. Thus, $A_{\\mathrm{est}}$ is an unbiased estimator of $A_{\\mathrm{exact}}$.\n\n### 3. Derivation of the Probabilistic Error Bound\n\nTo quantify the uncertainty of our estimate, we must derive a bound $\\varepsilon$ such that $|A_{\\mathrm{est}} - A_{\\mathrm{exact}}| \\le \\varepsilon$ with high probability, $1 - \\delta$. Since $E[A_{\\mathrm{est}}] = A_{\\mathrm{exact}}$, this is a problem of bounding the deviation of a function of random variables from its mean. We use McDiarmid's concentration inequality, which is suitable for functions with bounded differences.\n\nMcDiarmid's inequality states that if $f(X_1, \\dots, X_N)$ is a function of $N$ independent random variables, and replacing any single variable $X_k$ with an alternative value $X'_k$ changes the function's value by at most $d_k$, i.e.,\n$\n\\sup |f(X_1, \\dots, X_k, \\dots, X_N) - f(X_1, \\dots, X'_k, \\dots, X_N)| \\le d_k,\n$\nthen for any $\\varepsilon > 0$:\n$$\nP(|f(X_1, \\dots, X_N) - E[f]| \\ge \\varepsilon) \\le 2 \\exp\\left(-\\frac{2\\varepsilon^2}{\\sum_{k=1}^N d_k^2}\\right)\n$$\nIn our case, the random variables are the $m$ scores in $R_{\\mathrm{pos}}$ and the $n$ scores in $R_{\\mathrm{neg}}$. The function is $A_{\\mathrm{est}}(R_{\\mathrm{pos}}, R_{\\mathrm{neg}})$.\n\nLet's find the bounded differences, $d_k$:\n1.  **Changing one positive score:** If we replace one score $s^+_i \\in R_{\\mathrm{pos}}$ with a different score $s'^+_i$, the change in $A_{\\mathrm{est}}$ is confined to the terms involving $s^+_i$. The maximum possible change is:\n    $$\n    \\Delta_{s^+} = \\left| \\frac{1}{mn} \\sum_{j=1}^n \\psi(s^+_i, s^-_j) - \\frac{1}{mn} \\sum_{j=1}^n \\psi(s'^+_i, s^-_j) \\right| = \\frac{1}{mn} \\left| \\sum_{j=1}^n (\\psi(s^+_i, s^-_j) - \\psi(s'^+_i, s^-_j)) \\right|\n    $$\n    Since $\\psi \\in [0, 1]$, the difference $(\\psi - \\psi)$ is in $[-1, 1]$. The sum is bounded by $[-n, n]$, so the maximum absolute change in $A_{\\mathrm{est}}$ is $\\frac{n}{mn} = \\frac{1}{m}$. Thus, for each of the $m$ positive scores, $d_k = \\frac{1}{m}$.\n\n2.  **Changing one negative score:** By symmetric reasoning, replacing one score $s^-_j \\in R_{\\mathrm{neg}}$ changes $A_{\\mathrm{est}}$ by at most $\\frac{m}{mn} = \\frac{1}{n}$. Thus, for each of the $n$ negative scores, $d_k = \\frac{1}{n}$.\n\nThe sum of squared differences is $\\sum d_k^2 = \\sum_{i=1}^m \\left(\\frac{1}{m}\\right)^2 + \\sum_{j=1}^n \\left(\\frac{1}{n}\\right)^2 = m \\cdot \\frac{1}{m^2} + n \\cdot \\frac{1}{n^2} = \\frac{1}{m} + \\frac{1}{n}$.\n\nSubstituting this into McDiarmid's inequality:\n$$\nP(|A_{\\mathrm{est}} - A_{\\mathrm{exact}}| \\ge \\varepsilon) \\le 2 \\exp\\left(-\\frac{2\\varepsilon^2}{\\frac{1}{m} + \\frac{1}{n}}\\right)\n$$\nWe want this probability to be at most $\\delta$. Setting the right-hand side to $\\delta$ and solving for $\\varepsilon$:\n$$\n\\delta = 2 \\exp\\left(-\\frac{2\\varepsilon^2}{\\frac{1}{m} + \\frac{1}{n}}\\right) \\implies \\ln\\left(\\frac{\\delta}{2}\\right) = -\\frac{2\\varepsilon^2}{\\frac{1}{m} + \\frac{1}{n}} \\implies \\varepsilon^2 = \\frac{1}{2} \\left(\\frac{1}{m} + \\frac{1}{n}\\right) \\ln\\left(\\frac{2}{\\delta}\\right)\n$$\nThis gives the final error bound:\n$$\n\\varepsilon(\\delta, m, n) = \\sqrt{\\frac{1}{2} \\left(\\frac{1}{m} + \\frac{1}{n}\\right) \\ln\\left(\\frac{2}{\\delta}\\right)}\n$$\n\n### 4. Algorithmic Implementation\n\nThe implementation follows these steps for each test case:\n1.  **Data Generation:** Create arrays of positive and negative scores based on the specified distributions and parameters using a seeded random number generator for reproducibility.\n2.  **Exact AUC Calculation:** Compute $A_{\\mathrm{exact}}$ from the full score arrays. To do this efficiently without a naive $O(N_+ N_-)$ loop, one array (e.g., negative scores) is sorted. Then, for each score in the other array, a binary search (`numpy.searchsorted`) is used to quickly count the number of smaller and equal scores. This reduces the complexity to $O(N \\log N)$.\n3.  **Stream Simulation:** The positive and negative scores are combined and shuffled to simulate a random-order stream. The stream is then processed item by item, updating the two reservoirs according to Algorithm R.\n4.  **Estimated AUC Calculation:** After processing the entire stream, $A_{\\mathrm{est}}$ is calculated from the final reservoirs using the same efficient sort-and-search method.\n5.  **Error Bound Verification:** The theoretical bound $\\varepsilon$ is computed using the derived formula. The boolean flag is set to `True` if the observed absolute error $|A_{\\mathrm{est}} - A_{\\mathrm{exact}}|$ is within this bound, and `False` otherwise.\n6.  **Output Formatting:** The results for all test cases are collected and formatted into a single line of output as per the problem specification, with floating-point values rounded to six decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_auc(pos_scores, neg_scores):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC).\n\n    This function implements the Wilcoxon-Mann-Whitney U-statistic, which is\n    equivalent to the AUC. It uses an efficient O(N log N) algorithm based on\n    sorting and binary search.\n    \n    Args:\n        pos_scores (np.ndarray): An array of scores for positive instances.\n        neg_scores (np.ndarray): An array of scores for negative instances.\n\n    Returns:\n        float: The calculated AUC value.\n    \"\"\"\n    n_pos = len(pos_scores)\n    n_neg = len(neg_scores)\n\n    if n_pos == 0 or n_neg == 0:\n        return 0.5\n\n    # Sort negative scores for efficient counting\n    neg_scores = np.sort(neg_scores)\n    \n    numerator = 0.0\n    for s_p in pos_scores:\n        # Count of negative scores strictly less than s_p\n        num_less = np.searchsorted(neg_scores, s_p, side='left')\n        # Count of negative scores less than or equal to s_p\n        num_leq = np.searchsorted(neg_scores, s_p, side='right')\n        \n        num_equal = num_leq - num_less\n        \n        # Add contributions: 1 for each win, 0.5 for each tie\n        numerator += num_less + 0.5 * num_equal\n        \n    return numerator / (n_pos * n_neg)\n\ndef run_test_case(params):\n    \"\"\"\n    Runs a single test case for the streaming AUC estimation problem.\n    \n    Args:\n        params (tuple): A tuple containing all parameters for the test case.\n        \n    Returns:\n        list: A list containing [A_exact, A_est, epsilon, flag].\n    \"\"\"\n    N_plus, pos_mean, pos_std, N_minus, neg_mean, neg_std, m, n, B, delta, seed = params\n    \n    # Use a single RNG for all random operations for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate positive and negative score data\n    if pos_std is not None:\n        pos_scores = rng.normal(loc=pos_mean, scale=pos_std, size=N_plus)\n    else: # For the case with all scores equal\n        pos_scores = np.full(N_plus, pos_mean)\n        \n    if neg_std is not None:\n        neg_scores = rng.normal(loc=neg_mean, scale=neg_std, size=N_minus)\n    else:\n        neg_scores = np.full(N_minus, neg_mean)\n\n    # 2. Calculate the exact empirical AUC from the full dataset\n    A_exact = _calculate_auc(pos_scores, neg_scores)\n\n    # 3. Create a unified stream and shuffle it\n    pos_stream_items = [(score, 1) for score in pos_scores]\n    neg_stream_items = [(score, 0) for score in neg_scores]\n    stream = pos_stream_items + neg_stream_items\n    rng.shuffle(stream)\n\n    # 4. Process the stream and perform reservoir sampling\n    pos_reservoir = []\n    neg_reservoir = []\n    pos_count = 0\n    neg_count = 0\n\n    for score, label in stream:\n        if label == 1:\n            pos_count += 1\n            if len(pos_reservoir) < m:\n                pos_reservoir.append(score)\n            else:\n                j = rng.integers(0, pos_count) # Random index in [0, pos_count - 1]\n                if j < m:\n                    pos_reservoir[j] = score\n        else: # label == 0\n            neg_count += 1\n            if len(neg_reservoir) < n:\n                neg_reservoir.append(score)\n            else:\n                j = rng.integers(0, neg_count) # Random index in [0, neg_count - 1]\n                if j < n:\n                    neg_reservoir[j] = score\n\n    # 5. Calculate the estimated AUC from the reservoirs\n    A_est = _calculate_auc(np.array(pos_reservoir), np.array(neg_reservoir))\n    \n    # 6. Calculate the theoretical error bound epsilon using McDiarmid's inequality\n    if m > 0 and n > 0:\n        epsilon = np.sqrt(0.5 * (1/m + 1/n) * np.log(2/delta))\n    else:\n        epsilon = float('inf')\n\n    # 7. Verify if the observed error is within the theoretical bound\n    flag = np.abs(A_est - A_exact) <= epsilon\n    \n    return [A_exact, A_est, epsilon, flag]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Balanced, moderate separation\n        (2000, 1.0, 1.0, 2000, 0.0, 1.0, 200, 200, 500, 0.05, 42),\n        # 2. Small reservoirs\n        (1000, 0.5, 1.0, 1000, 0.0, 1.0, 1, 1, 200, 0.10, 123),\n        # 3. Imbalanced classes\n        (300, 0.8, 1.0, 5000, 0.0, 1.0, 100, 100, 250, 0.05, 17),\n        # 4. All scores equal\n        (1500, 0.0, None, 1500, 0.0, None, 50, 50, 300, 0.05, 777),\n        # 5. Near-perfect separation\n        (1000, 2.0, 0.1, 1000, 0.0, 0.1, 80, 120, 250, 0.01, 2025),\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        result = run_test_case(case_params)\n        all_results.append(result)\n\n    # Format the final output string as a Python-style list of lists.\n    formatted_results = []\n    for res in all_results:\n        # Unpack results and format floating point numbers to 6 decimal places.\n        a_exact, a_est, eps, flag = res\n        s = f\"[{a_exact:.6f},{a_est:.6f},{eps:.6f},{flag}]\"\n        formatted_results.append(s)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3167103"}]}