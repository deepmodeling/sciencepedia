{"hands_on_practices": [{"introduction": "To truly master classification metrics, it's essential to move beyond rote memorization of formulas and understand their deep-seated connections. This exercise challenges you to derive the $F_1$-score directly from a population's prevalence ($\\pi$), a classifier's recall ($R$), and its specificity ($S$). Completing this derivation provides a robust theoretical foundation, revealing how these fundamental properties interact to determine a model's balanced performance. [@problem_id:3094141]", "problem": "A binary classifier is evaluated on a dataset of $N$ independent samples drawn from a population in which the prevalence (fraction of truly positive instances) is $\\pi \\in [0,1]$. The classifier is tuned to achieve a target recall $R \\in [0,1]$ and a target specificity $S \\in [0,1]$, each interpreted exactly as their population counterparts on this dataset. Relying only on the fundamental definitions of prevalence, recall, specificity, precision, and the F₁-score, proceed as follows:\n\n- Using the definitions of recall and specificity together with prevalence, express the confusion matrix counts: true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$) in terms of $N$, $\\pi$, $R$, and $S$.\n\n- From these counts and the definition of precision, deduce the precision $P$.\n\n- Using the definition of the F₁-score as the harmonic mean of precision and recall, or equivalently in terms of the confusion matrix counts, derive a closed-form expression for the F₁-score purely as a function of $\\pi$, $R$, and $S$.\n\n- State the feasibility constraints on $N$, $\\pi$, $R$, and $S$ required for these quantities to be well-defined and for the counts to be nonnegative and interpretable as dataset counts.\n\nYour final answer must be the single, simplified analytic expression for the F₁-score as a function of $\\pi$, $R$, and $S$ only. No numerical evaluation is required, and no rounding is needed.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the fundamental principles of statistical learning and classification metrics. It is well-posed, objective, and contains sufficient information for a unique, analytical solution to be derived.\n\nThe derivation proceeds as follows.\n\nFirst, we define the total number of actual positive and actual negative instances in the dataset of size $N$ with a prevalence $\\pi$.\nThe number of actual positives, $P_{actual}$, is given by $P_{actual} = N\\pi$.\nThe number of actual negatives, $N_{actual}$, is given by $N_{actual} = N(1-\\pi)$.\n\nNext, we use the definitions of Recall ($R$) and Specificity ($S$) to express the four counts of the confusion matrix: true positives ($TP$), false negatives ($FN$), true negatives ($TN$), and false positives ($FP$).\n\nRecall, or the True Positive Rate, is the fraction of actual positives that are correctly identified. Its definition is:\n$$R = \\frac{TP}{P_{actual}} = \\frac{TP}{TP + FN}$$\nFrom this, we can express $TP$ in terms of $N$, $\\pi$, and $R$:\n$$TP = R \\cdot P_{actual} = R N\\pi$$\nThe number of false negatives are the actual positives that were not identified as positive:\n$$FN = P_{actual} - TP = N\\pi - R N\\pi = N\\pi(1-R)$$\n\nSpecificity, or the True Negative Rate, is the fraction of actual negatives that are correctly identified. Its definition is:\n$$S = \\frac{TN}{N_{actual}} = \\frac{TN}{TN + FP}$$\nFrom this, we can express $TN$ in terms of $N$, $\\pi$, and $S$:\n$$TN = S \\cdot N_{actual} = S N(1-\\pi)$$\nThe number of false positives are the actual negatives that were not identified as negative:\n$$FP = N_{actual} - TN = N(1-\\pi) - S N(1-\\pi) = N(1-\\pi)(1-S)$$\n\nSummarizing the confusion matrix counts in terms of the given parameters:\n- $TP = N\\pi R$\n- $FN = N\\pi(1-R)$\n- $TN = N(1-\\pi)S$\n- $FP = N(1-\\pi)(1-S)$\n\nNext, we deduce the expression for Precision ($P$). Precision, or the Positive Predictive Value, is the fraction of predicted positives that are actually positive. Its definition is:\n$$P = \\frac{TP}{TP + FP}$$\nSubstituting the expressions for $TP$ and $FP$:\n$$P = \\frac{N\\pi R}{N\\pi R + N(1-\\pi)(1-S)}$$\nAssuming $N \\neq 0$, we can cancel $N$ from the numerator and denominator:\n$$P = \\frac{\\pi R}{\\pi R + (1-\\pi)(1-S)}$$\n\nNow, we derive the closed-form expression for the $F_1$-score. The $F_1$-score is defined as the harmonic mean of Precision and Recall. A more direct definition in terms of the confusion matrix counts is:\n$$F_1 = \\frac{2TP}{2TP + FP + FN}$$\nThis form is computationally robust as it avoids potential division-by-zero issues if Precision is undefined (i.e., if $TP+FP=0$).\nSubstituting the expressions for $TP$, $FP$, and $FN$:\n$$F_1 = \\frac{2(N\\pi R)}{2(N\\pi R) + N(1-\\pi)(1-S) + N\\pi(1-R)}$$\nAgain, assuming $N \\neq 0$, we cancel $N$ from all terms:\n$$F_1 = \\frac{2\\pi R}{2\\pi R + (1-\\pi)(1-S) + \\pi(1-R)}$$\nTo simplify, we expand and collect terms in the denominator:\n$$ \\text{Denominator} = 2\\pi R + (1 - S - \\pi + \\pi S) + (\\pi - \\pi R) $$\n$$ \\text{Denominator} = (2\\pi R - \\pi R) + (-\\pi + \\pi) + 1 - S + \\pi S $$\n$$ \\text{Denominator} = \\pi R + 1 - S + \\pi S $$\nThis can be factored to group terms involving $\\pi$:\n$$ \\text{Denominator} = \\pi(R+S) + 1 - S $$\nThus, the final simplified expression for the $F_1$-score is:\n$$F_1 = \\frac{2\\pi R}{\\pi(R+S) + 1 - S}$$\n\nFinally, we state the feasibility constraints.\nThe given parameters $\\pi, R, S$ are defined in the interval $[0,1]$.\nThe number of samples, $N$, must be a non-negative integer, typically $N \\in \\mathbb{Z}^+$.\nFor the problem to be interpretable on a real dataset, the counts $TP$, $FP$, $TN$, and $FN$ must be non-negative integers.\n- Non-negativity is guaranteed since $N \\ge 0$ and $\\pi, R, S \\in [0,1]$.\n- For the counts to be integers, the following conditions must hold:\n  1. The number of actual positives, $N\\pi$, must be an integer.\n  2. The number of actual negatives, $N(1-\\pi)$, must be an integer. (This is implied by condition 1 if $N$ is an integer).\n  3. The number of true positives, $TP = (N\\pi)R$, must be an integer.\n  4. The number of true negatives, $TN = N(1-\\pi)S$, must be an integer.\nThe derived expression for $F_1$ is well-defined as long as its denominator is not zero. The denominator $\\pi(R+S) + 1 - S$ is zero if and only if $\\pi=0$ and $S=1$. This corresponds to a scenario with no positive instances and a classifier that makes no positive predictions ($FP=0$). In this specific case, $TP$, $FP$, and $FN$ are all $0$, leading to an $F_1$ score of $0$ by convention. Our formula yields the indeterminate form $\\frac{0}{0}$ in this case, but correctly evaluates to $0$ in the limit as $\\pi \\to 0$ for any $S<1$.", "answer": "$$\\boxed{\\frac{2\\pi R}{\\pi(R+S) + 1 - S}}$$", "id": "3094141"}, {"introduction": "Theoretical knowledge comes to life when applied to concrete scenarios. This practice places you in the role of an analyst evaluating two automated assistants in a realistic, high-stakes setting: officiating a professional sport where fouls are rare. By constructing confusion matrices for a high-recall and a high-precision system, you will gain practical experience with the precision-recall trade-off and see how the $F_1$-score serves as a decisive tool for selecting the most balanced model in an imbalanced data context. [@problem_id:3094207]", "problem": "A professional sports league is piloting two automated assistants to help referees detect a rare foul during games. The league has a labeled evaluation set of $N=10000$ plays, of which $F=100$ are actual fouls (ground truth), reflecting the rarity of fouls. For each automated assistant, a \"flag\" indicates the assistant predicts a foul on a play.\n\nAssistant $\\mathsf{A}$ (designed for high recall and low precision) flags $990$ plays; among these flagged plays, $90$ are actual fouls. Assistant $\\mathsf{B}$ (designed for high precision and low recall) flags $40$ plays; among these flagged plays, $38$ are actual fouls. Plays that are not flagged are predicted as \"no foul.\"\n\nConstruct, for each assistant, the $2\\times 2$ confusion matrix with counts of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). Then, using the standard definitions based on these counts, compute:\n- Accuracy,\n- Precision ($P$),\n- Recall ($R$),\n- Specificity,\n- The $F_1$-score.\n\nFinally, determine which assistant yields the larger $F_1$-score on this evaluation set.\n\nChoose one option:\n\nA. Assistant $\\mathsf{A}$ (high-$R$/low-$P$) has the larger $F_1$-score.\n\nB. Assistant $\\mathsf{B}$ (high-$P$/low-$R$) has the larger $F_1$-score.\n\nC. Both assistants have the same $F_1$-score.\n\nD. It cannot be determined without additional information about class prevalence beyond the given counts.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Total number of plays in the evaluation set: $N = 10000$.\n- Total number of actual fouls (positive class): $F = 100$.\n- The number of actual non-fouls (negative class) is $N - F = 10000 - 100 = 9900$.\n- For Assistant $\\mathsf{A}$:\n  - It flags (predicts as positive) $990$ plays.\n  - Among the flagged plays, $90$ are actual fouls.\n- For Assistant $\\mathsf{B}$:\n  - It flags (predicts as positive) $40$ plays.\n  - Among the flagged plays, $38$ are actual fouls.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on standard, well-defined metrics in statistical learning and machine learning (confusion matrix, accuracy, precision, recall, $F_1$-score). These concepts are mathematically rigorous and widely used for evaluating classification models.\n- **Well-Posedness**: The problem provides all necessary information to construct the confusion matrices for both assistants and subsequently calculate the required performance metrics. The final question is unambiguous.\n- **Objectivity**: The problem is stated in objective terms, using standard terminology from the field of statistics.\n- **Consistency and Completeness**: The provided numbers are internally consistent and sufficient. For each assistant, the number of true positives and the total number of predicted positives are given. Combined with the ground truth counts of actual positives and actual negatives for the entire dataset, all components of the confusion matrix can be uniquely determined. For instance, for Assistant $\\mathsf{A}$, we have $TP_A=90$ and $TP_A+FP_A=990$, which implies $FP_A=900$. Given that actual positives are $100$, $FN_A = 100 - TP_A = 10$. Given actual negatives are $9900$, $TN_A = 9900 - FP_A = 9000$. The sum $TP_A+FP_A+FN_A+TN_A=90+900+10+9000=10000=N$, confirming consistency. A similar check confirms consistency for Assistant $\\mathsf{B}$.\n\n**Step 3: Verdict and Action**\n- The problem is valid as it is scientifically grounded, well-posed, objective, complete, and consistent. The solution process may proceed.\n\n---\n\nThe task is to compare the $F_1$-score of two automated assistants, $\\mathsf{A}$ and $\\mathsf{B}$. The analysis requires constructing a $2 \\times 2$ confusion matrix for each assistant and calculating several performance metrics.\n\nThe confusion matrix for a binary classification task is structured as follows:\n- **True Positives ($TP$)**: Number of positive instances correctly classified as positive.\n- **False Positives ($FP$)**: Number of negative instances incorrectly classified as positive (Type I error).\n- **True Negatives ($TN$)**: Number of negative instances correctly classified as negative.\n- **False Negatives ($FN$)**: Number of positive instances incorrectly classified as negative (Type II error).\n\nFrom these counts, we define the following metrics:\n- **Precision ($P$)**: The proportion of predicted positive instances that are actually positive.\n  $$P = \\frac{TP}{TP + FP}$$\n- **Recall ($R$)**: The proportion of actual positive instances that are correctly identified. Also known as Sensitivity or True Positive Rate.\n  $$R = \\frac{TP}{TP + FN}$$\n- **$F_1$-score**: The harmonic mean of Precision and Recall.\n  $$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\n- **Accuracy**: The proportion of all instances that are correctly classified.\n  $$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n- **Specificity**: The proportion of actual negative instances that are correctly identified. Also known as True Negative Rate.\n  $$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n\nThe total number of actual positives (fouls) is $100$, and the total number of actual negatives (no fouls) is $10000 - 100 = 9900$.\n\n**Analysis of Assistant $\\mathsf{A}$**\n- Predicted positives (flagged plays) = $TP_A + FP_A = 990$.\n- True Positives ($TP_A$): Among the flagged plays, $90$ are actual fouls. So, $TP_A = 90$.\n- False Positives ($FP_A$): $FP_A = 990 - TP_A = 990 - 90 = 900$.\n- False Negatives ($FN_A$): The total number of actual fouls is $100$. $FN_A = (\\text{Actual Positives}) - TP_A = 100 - 90 = 10$.\n- True Negatives ($TN_A$): The total number of non-fouls is $9900$. $TN_A = (\\text{Actual Negatives}) - FP_A = 9900 - 900 = 9000$.\n\nThe confusion matrix for Assistant $\\mathsf{A}$ is:\n$$\n\\begin{array}{|c|cc|c|}\n\\hline\n & \\text{Predicted Foul} & \\text{Predicted No Foul} & \\text{Total} \\\\\n\\hline\n\\text{Actual Foul} & TP_A = 90 & FN_A = 10 & 100 \\\\\n\\text{Actual No Foul} & FP_A = 900 & TN_A = 9000 & 9900 \\\\\n\\hline\n\\text{Total} & 990 & 9010 & 10000 \\\\\n\\hline\n\\end{array}\n$$\nMetrics for Assistant $\\mathsf{A}$:\n- Accuracy$_A = \\frac{90 + 9000}{10000} = \\frac{9090}{10000} = 0.909$.\n- Specificity$_A = \\frac{9000}{9000 + 900} = \\frac{9000}{9900} = \\frac{10}{11} \\approx 0.9091$.\n- Precision $P_A = \\frac{TP_A}{TP_A + FP_A} = \\frac{90}{990} = \\frac{1}{11}$.\n- Recall $R_A = \\frac{TP_A}{TP_A + FN_A} = \\frac{90}{100} = 0.90 = \\frac{9}{10}$.\n- $F_{1,A}$-score $= 2 \\cdot \\frac{P_A \\cdot R_A}{P_A + R_A} = 2 \\cdot \\frac{(\\frac{1}{11}) \\cdot (\\frac{9}{10})}{(\\frac{1}{11}) + (\\frac{9}{10})} = 2 \\cdot \\frac{\\frac{9}{110}}{\\frac{10+99}{110}} = 2 \\cdot \\frac{9}{109} = \\frac{18}{109}$.\nNumerically, $F_{1,A} \\approx 0.1651$.\n\n**Analysis of Assistant $\\mathsf{B}$**\n- Predicted positives (flagged plays) = $TP_B + FP_B = 40$.\n- True Positives ($TP_B$): Among the flagged plays, $38$ are actual fouls. So, $TP_B = 38$.\n- False Positives ($FP_B$): $FP_B = 40 - TP_B = 40 - 38 = 2$.\n- False Negatives ($FN_B$): $FN_B = (\\text{Actual Positives}) - TP_B = 100 - 38 = 62$.\n- True Negatives ($TN_B$): $TN_B = (\\text{Actual Negatives}) - FP_B = 9900 - 2 = 9898$.\n\nThe confusion matrix for Assistant $\\mathsf{B}$ is:\n$$\n\\begin{array}{|c|cc|c|}\n\\hline\n & \\text{Predicted Foul} & \\text{Predicted No Foul} & \\text{Total} \\\\\n\\hline\n\\text{Actual Foul} & TP_B = 38 & FN_B = 62 & 100 \\\\\n\\text{Actual No Foul} & FP_B = 2 & TN_B = 9898 & 9900 \\\\\n\\hline\n\\text{Total} & 40 & 9960 & 10000 \\\\\n\\hline\n\\end{array}\n$$\nMetrics for Assistant $\\mathsf{B}$:\n- Accuracy$_B = \\frac{38 + 9898}{10000} = \\frac{9936}{10000} = 0.9936$.\n- Specificity$_B = \\frac{9898}{9898 + 2} = \\frac{9898}{9900} \\approx 0.9998$.\n- Precision $P_B = \\frac{TP_B}{TP_B + FP_B} = \\frac{38}{40} = 0.95 = \\frac{19}{20}$.\n- Recall $R_B = \\frac{TP_B}{TP_B + FN_B} = \\frac{38}{100} = 0.38 = \\frac{19}{50}$.\n- $F_{1,B}$-score $= 2 \\cdot \\frac{P_B \\cdot R_B}{P_B + R_B} = 2 \\cdot \\frac{(0.95) \\cdot (0.38)}{0.95 + 0.38} = 2 \\cdot \\frac{0.361}{1.33} = \\frac{0.722}{1.33} = \\frac{722}{1330} = \\frac{361}{665}$.\nNumerically, $F_{1,B} \\approx 0.5429$.\n\n**Comparison and Conclusion**\nWe must compare the $F_1$-scores:\n- $F_{1,A} = \\frac{18}{109} \\approx 0.1651$\n- $F_{1,B} = \\frac{361}{665} \\approx 0.5429$\n\nClearly, $F_{1,B} > F_{1,A}$. Assistant $\\mathsf{B}$ has the larger $F_1$-score. The problem notes that Assistant $\\mathsf{A}$ is high-recall/low-precision ($R_A=0.9$, $P_A\\approx 0.09$) and Assistant $\\mathsf{B}$ is high-precision/low-recall ($P_B=0.95$, $R_B=0.38$), which our calculations confirm. Our conclusion is that the high-precision/low-recall assistant has the higher $F_1$-score in this scenario.\n\n**Evaluation of Options**\n- **A. Assistant $\\mathsf{A}$ (high-$R$/low-$P$) has the larger $F_1$-score.** This is **Incorrect**. Our calculation shows $F_{1,A} < F_{1,B}$.\n- **B. Assistant $\\mathsf{B}$ (high-$P$/low-$R$) has the larger $F_1$-score.** This is **Correct**. Our calculation shows $F_{1,B} > F_{1,A}$.\n- **C. Both assistants have the same $F_1$-score.** This is **Incorrect**. $\\frac{18}{109} \\neq \\frac{361}{665}$.\n- **D. It cannot be determined without additional information about class prevalence beyond the given counts.** This is **Incorrect**. The class prevalence (number of actual fouls and non-fouls) for the evaluation set is given ($100$ fouls in $10000$ plays), and all necessary counts to compute the $F_1$-scores are provided or can be derived. The calculation is definitive.", "answer": "$$\\boxed{B}$$", "id": "3094207"}, {"introduction": "In modern machine learning, your choice of evaluation metric directly steers the model selection and tuning process. This advanced practice simulates a hyperparameter grid search, demonstrating a critical concept: the \"best\" model settings often change depending on whether you optimize for accuracy or for the $F_1$-score. By working through this simulated scenario, you will see why the $F_1$-score is indispensable for developing effective models on imbalanced datasets, where maximizing simple accuracy can be profoundly misleading. [@problem_id:3094204]", "problem": "You are given an abstracted binary classification scenario intended to separate model, hyperparameter, and metric effects in a scientifically realistic and mathematically reproducible way. The base objects are counts in the confusion matrix for a binary classifier: true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), true negatives ($\\mathrm{TN}$), and false negatives ($\\mathrm{FN}$). All metrics must be derived solely from these counts. Using these definitions, derive and implement expressions for accuracy, precision, recall, specificity, and the $F_1$-score, and then perform hyperparameter selection under two different objectives on the same imbalanced data.\n\nThe decision mechanism for both classifiers is a threshold on a one-dimensional decision score. Let the positive-class score be a Gaussian random variable $s_{+} \\sim \\mathcal{N}(\\mu_{+}, \\sigma^{2})$ and the negative-class score be $s_{-} \\sim \\mathcal{N}(\\mu_{-}, \\sigma^{2})$, with $\\mu_{+} > 0$ and $\\mu_{-}  0$. The total number of positive examples is $N_{+}$ and the total number of negative examples is $N_{-}$, which together control the imbalance. For a decision threshold $\\tau$ applied to a score $s$, predicted positives are those with $s \\ge \\tau$.\n\nTwo classifiers are considered:\n- Support Vector Machine (SVM): The raw decision score is thresholded directly, so the decision rule is $s \\ge \\tau$ with $s \\in \\mathbb{R}$ and $\\tau \\in \\mathbb{R}$.\n- Logistic Regression: The raw score $s$ is passed through the logistic sigmoid to produce a probability $p = \\frac{1}{1 + e^{-s}} \\in (0,1)$, and the decision rule is $p \\ge \\tau$ with $\\tau \\in (0,1)$. Equivalently, this is a threshold on $s$ at the logit level, $s \\ge \\operatorname{logit}(\\tau)$, where $\\operatorname{logit}(\\tau) = \\ln\\left(\\frac{\\tau}{1 - \\tau}\\right)$.\n\nA single scalar hyperparameter $\\theta > 0$ controls the separation of the two Gaussian score distributions by scaling their means. Specifically, for a given base separation $\\mu > 0$, use $\\mu_{+} = \\theta \\mu$ and $\\mu_{-} = -\\theta \\mu$. The noise level $\\sigma > 0$ is fixed per test case.\n\nFor any choice of $(\\theta, \\tau)$, the expected confusion matrix counts are given by:\n- $\\mathrm{TP} = N_{+} \\cdot \\mathbb{P}(s_{+} \\ge \\tau)$,\n- $\\mathrm{FP} = N_{-} \\cdot \\mathbb{P}(s_{-} \\ge \\tau)$,\n- $\\mathrm{TN} = N_{-} - \\mathrm{FP}$,\n- $\\mathrm{FN} = N_{+} - \\mathrm{TP}$,\nwhere $\\mathbb{P}(s \\ge \\tau)$ is computed from the normal distribution. For Logistic Regression, ensure the threshold is applied at the correct level by using $s \\ge \\operatorname{logit}(\\tau)$.\n\nStarting from the fundamental definitions of the confusion matrix entries, derive the formulas for accuracy, precision, recall, specificity, and $F_1$-score, and then implement them. For each test case below, perform a grid search over $\\theta$ and $\\tau$ for both SVM and Logistic Regression, and record:\n- the $(\\theta,\\tau)$ pair that maximizes $F_1$-score,\n- the $(\\theta,\\tau)$ pair that maximizes accuracy.\n\nYour objective is to show that the F1-optimal hyperparameters differ from the accuracy-optimal ones on imbalanced data and to illustrate any contrasts between Support Vector Machine (SVM) and Logistic Regression in this setting.\n\nUse the following fixed grids for the search:\n- $\\Theta = \\{\\,0.8,\\,1.0,\\,1.2\\,\\}$,\n- For Logistic Regression thresholds, $T_{\\mathrm{LR}} = \\{\\,0.2,\\,0.5,\\,0.8\\,\\}$,\n- For SVM thresholds, $T_{\\mathrm{SVM}} = \\{\\, -0.5,\\,0.0,\\,0.5 \\,\\}$.\n\nCompute and report the results for the following four test cases, each defined by $(N_{+}, N_{-}, \\mu, \\sigma)$:\n1. $N_{+} = 100$, $N_{-} = 900$, $\\mu = 1.5$, $\\sigma = 1.0$ (moderately imbalanced, reasonably separated).\n2. $N_{+} = 50$, $N_{-} = 4950$, $\\mu = 1.0$, $\\sigma = 1.0$ (severely imbalanced).\n3. $N_{+} = 500$, $N_{-} = 500$, $\\mu = 1.5$, $\\sigma = 1.0$ (balanced reference).\n4. $N_{+} = 100$, $N_{-} = 900$, $\\mu = 0.3$, $\\sigma = 2.5$ (moderately imbalanced, poorly separated).\n\nYour program must:\n- For each test case and each classifier, search the specified grids to find the $(\\theta,\\tau)$ that maximizes F1-score and the $(\\theta,\\tau)$ that maximizes accuracy. If there are ties, select the first encountered in the grid order.\n- Use expected counts computed from the normal distribution without sampling, ensuring determinism.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of eight floating-point numbers in the following order:\n$[\\,\\theta_{\\mathrm{F1,LR}},\\,\\tau_{\\mathrm{F1,LR}},\\,\\theta_{\\mathrm{Acc,LR}},\\,\\tau_{\\mathrm{Acc,LR}},\\,\\theta_{\\mathrm{F1,SVM}},\\,\\tau_{\\mathrm{F1,SVM}},\\,\\theta_{\\mathrm{Acc,SVM}},\\,\\tau_{\\mathrm{Acc,SVM}}\\,]$.\nNo percentages should be printed; all quantities must be plain decimal numbers. There are no physical units. Angle units are not involved.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Base Objects**: Confusion matrix counts: true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), true negatives ($\\mathrm{TN}$), false negatives ($\\mathrm{FN}$).\n- **Data Distribution**:\n    - Positive-class score: $s_{+} \\sim \\mathcal{N}(\\mu_{+}, \\sigma^{2})$.\n    - Negative-class score: $s_{-} \\sim \\mathcal{N}(\\mu_{-}, \\sigma^{2})$.\n- **Class Sizes**: Total positive examples $N_{+}$, total negative examples $N_{-}$.\n- **Hyperparameter $\\theta$**: $\\mu_{+} = \\theta \\mu$ and $\\mu_{-} = -\\theta \\mu$, where $\\mu > 0$ is a base separation and $\\theta > 0$.\n- **Decision Rule**: A score $s$ is classified as positive if $s \\ge \\tau'$, where $\\tau'$ is the effective threshold on the score.\n- **Classifier Models**:\n    - **Support Vector Machine (SVM)**: Raw score $s$ is thresholded at $\\tau \\in \\mathbb{R}$. The effective threshold is $\\tau' = \\tau$.\n    - **Logistic Regression (LR)**: Score $s$ yields probability $p = \\frac{1}{1 + e^{-s}}$. The decision rule is $p \\ge \\tau$, with $\\tau \\in (0,1)$. This is equivalent to an effective threshold on the score $s$ at $\\tau' = \\operatorname{logit}(\\tau) = \\ln\\!\\left(\\frac{\\tau}{1 - \\tau}\\right)$.\n- **Expected Counts Formulas**:\n    - $\\mathrm{TP} = N_{+} \\cdot \\mathbb{P}(s_{+} \\ge \\tau')$.\n    - $\\mathrm{FP} = N_{-} \\cdot \\mathbb{P}(s_{-} \\ge \\tau')$.\n    - $\\mathrm{TN} = N_{-} - \\mathrm{FP}$.\n    - $\\mathrm{FN} = N_{+} - \\mathrm{TP}$.\n- **Search Grids**:\n    - $\\Theta = \\{\\,0.8,\\,1.0,\\,1.2\\,\\}$.\n    - $T_{\\mathrm{LR}} = \\{\\,0.2,\\,0.5,\\,0.8\\,\\}$.\n    - $T_{\\mathrm{SVM}} = \\{\\, -0.5,\\,0.0,\\,0.5 \\,\\}$.\n- **Test Cases** $(N_{+}, N_{-}, \\mu, \\sigma)$:\n    1. $(100, 900, 1.5, 1.0)$\n    2. $(50, 4950, 1.0, 1.0)$\n    3. $(500, 500, 1.5, 1.0)$\n    4. $(100, 900, 0.3, 2.5)$\n- **Optimization Objectives**: For each classifier and test case, find the $(\\theta, \\tau)$ pair that maximizes the $F_1$-score and the pair that maximizes accuracy.\n- **Tie-Breaking Rule**: If scores are equal, select the first pair encountered in the specified grid order.\n- **Output Format**: A single line representing a list, where each element is a list of 8 floating-point numbers for one test case: $[\\,\\theta_{\\mathrm{F1,LR}},\\,\\tau_{\\mathrm{F1,LR}},\\,\\theta_{\\mathrm{Acc,LR}},\\,\\tau_{\\mathrm{Acc,LR}},\\,\\theta_{\\mathrm{F1,SVM}},\\,\\tau_{\\mathrm{F1,SVM}},\\,\\theta_{\\mathrm{Acc,SVM}},\\,\\tau_{\\mathrm{Acc,SVM}}\\,]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in statistical learning theory. It uses standard models (SVM, LR), metrics (accuracy, $F_1$-score), and modeling assumptions (Gaussian score distributions, a common abstraction in signal detection theory and classifier analysis). The mathematics are standard and correct.\n- **Well-Posed**: The problem is deterministic, as it uses expected counts derived from probability distributions rather than random sampling. The search space is a finite grid, and the optimization objectives are clearly defined with an explicit tie-breaking rule. This ensures a unique and computable solution exists.\n- **Objective**: The problem is stated in precise, formal mathematical language, free from subjectivity or ambiguity.\n- **Completeness**: All necessary data, including model parameters, search grids, and test cases, are provided. The problem is self-contained.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational exercise in statistical model evaluation and hyperparameter selection. A solution will be provided.\n\n***\n\nThe solution requires a systematic derivation of the performance metrics from fundamental principles, followed by the implementation of a deterministic grid search to identify optimal hyperparameters under different objectives.\n\n### 1. Derivation of Performance Metrics\n\nAll performance metrics are functions of the four fundamental quantities of the confusion matrix: $\\mathrm{TP}$ (True Positives), $\\mathrm{FP}$ (False Positives), $\\mathrm{TN}$ (True Negatives), and $\\mathrm{FN}$ (False Negatives). The total number of positive instances is $N_{+} = \\mathrm{TP} + \\mathrm{FN}$ and the total number of negative instances is $N_{-} = \\mathrm{TN} + \\mathrm{FP}$.\n\n- **Accuracy**: The fraction of all decisions that are correct.\n$$ \\text{Accuracy} = \\frac{\\mathrm{TP} + \\mathrm{TN}}{\\mathrm{TP} + \\mathrm{TN} + \\mathrm{FP} + \\mathrm{FN}} = \\frac{\\mathrm{TP} + \\mathrm{TN}}{N_{+} + N_{-}} $$\n- **Precision (Positive Predictive Value)**: The fraction of positive predictions that are correct.\n$$ \\text{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} $$\nThis metric is undefined if the denominator is zero. By convention, if $\\mathrm{TP} + \\mathrm{FP} = 0$, precision is taken as $0$.\n\n- **Recall (Sensitivity, True Positive Rate)**: The fraction of actual positive instances that are correctly identified.\n$$ \\text{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} = \\frac{\\mathrm{TP}}{N_{+}} $$\n- **Specificity (True Negative Rate)**: The fraction of actual negative instances that are correctly identified.\n$$ \\text{Specificity} = \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} = \\frac{\\mathrm{TN}}{N_{-}} $$\n- **$F_1$-Score**: The harmonic mean of Precision and Recall. It provides a single score that balances both metrics.\n$$ F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nSubstituting the definitions of Precision and Recall, and simplifying, we obtain a more numerically stable form that avoids potential division by zero if either precision or recall is zero:\n$$ F_1 = \\frac{2 \\mathrm{TP}}{2 \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}} $$\nIf $\\mathrm{TP} = 0$, then $F_1 = 0$. This form is robust provided $2\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}  0$, which holds as long as $N_{+}  0$ and $N_{-}  0$.\n\n### 2. Calculation of Expected Confusion Matrix Counts\n\nThe core of the problem is to compute the expected values for $\\mathrm{TP}$ and $\\mathrm{FP}$ based on the Gaussian score distributions. Let $s$ be a random variable representing the classifier's raw score, with $s \\sim \\mathcal{N}(\\mu_s, \\sigma^2)$. The probability of this score exceeding a threshold $\\tau'$ is given by the survival function (SF), or $1 - \\text{CDF}$, of the normal distribution.\n\nLet $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$, and let $\\Phi(z)$ be its CDF.\n$$ \\mathbb{P}(s \\ge \\tau') = \\mathbb{P}\\left(\\frac{s - \\mu_s}{\\sigma} \\ge \\frac{\\tau' - \\mu_s}{\\sigma}\\right) = \\mathbb{P}\\left(Z \\ge \\frac{\\tau' - \\mu_s}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau' - \\mu_s}{\\sigma}\\right) $$\nThis is the survival function of the standard normal distribution evaluated at $z = (\\tau' - \\mu_s) / \\sigma$.\n\nFor our specific problem, with a given hyperparameter $\\theta$:\n- The positive-class score distribution is $s_{+} \\sim \\mathcal{N}(\\mu_{+}, \\sigma^2)$ where $\\mu_{+} = \\theta\\mu$.\n- The negative-class score distribution is $s_{-} \\sim \\mathcal{N}(\\mu_{-}, \\sigma^2)$ where $\\mu_{-} = -\\theta\\mu$.\n\nThe true positive rate (TPR) and false positive rate (FPR) are the probabilities that a positive and negative instance, respectively, are classified as positive.\n$$ \\mathrm{TPR}(\\tau', \\theta) = \\mathbb{P}(s_{+} \\ge \\tau') = 1 - \\Phi\\left(\\frac{\\tau' - \\theta\\mu}{\\sigma}\\right) $$\n$$ \\mathrm{FPR}(\\tau', \\theta) = \\mathbb{P}(s_{-} \\ge \\tau') = 1 - \\Phi\\left(\\frac{\\tau' + \\theta\\mu}{\\sigma}\\right) $$\n\nThe effective threshold $\\tau'$ depends on the classifier model:\n- For the **SVM**, the threshold $\\tau$ is applied directly to the raw score $s$. Thus, $\\tau' = \\tau$.\n- For **Logistic Regression**, the threshold $\\tau \\in (0,1)$ is applied to the probability output $p = (1+e^{-s})^{-1}$. The condition $p \\ge \\tau$ is equivalent to $s \\ge \\ln(\\tau/(1-\\tau))$. Thus, the effective threshold is $\\tau' = \\operatorname{logit}(\\tau)$.\n\nWith these rates, the expected counts are:\n$$ \\mathrm{TP} = N_{+} \\cdot \\mathrm{TPR}(\\tau', \\theta) $$\n$$ \\mathrm{FP} = N_{-} \\cdot \\mathrm{FPR}(\\tau', \\theta) $$\n$$ \\mathrm{FN} = N_{+} \\cdot (1 - \\mathrm{TPR}(\\tau', \\theta)) = N_{+} - \\mathrm{TP} $$\n$$ \\mathrm{TN} = N_{-} \\cdot (1 - \\mathrm{FPR}(\\tau', \\theta)) = N_{-} - \\mathrm{FP} $$\n\n### 3. Hyperparameter Optimization Procedure\n\nFor each test case and classifier, a grid search is performed over the discrete sets of hyperparameters $\\theta \\in \\Theta$ and $\\tau \\in T$. The procedure is as follows:\n\n1.  Initialize four pairs of variables to store the optimal $(\\theta, \\tau)$ for each of the four objectives: $(\\theta_{\\mathrm{F1,LR}}, \\tau_{\\mathrm{F1,LR}})$, $(\\theta_{\\mathrm{Acc,LR}}, \\tau_{\\mathrm{Acc,LR}})$, $(\\theta_{\\mathrm{F1,SVM}}, \\tau_{\\mathrm{F1,SVM}})$, and $(\\theta_{\\mathrm{Acc,SVM}}, \\tau_{\\mathrm{Acc,SVM}})$. Also, initialize the corresponding maximum scores found so far to a value of $-1.0$.\n\n2.  Iterate through each classifier (LR, SVM).\n\n3.  For the selected classifier, iterate through each $\\theta$ in the ordered grid $\\Theta = [0.8, 1.0, 1.2]$.\n\n4.  For each $\\theta$, iterate through each $\\tau$ in the corresponding ordered threshold grid ($T_{\\mathrm{LR}} = [0.2, 0.5, 0.8]$ or $T_{\\mathrm{SVM}} = [-0.5, 0.0, 0.5]$).\n\n5.  For each $(\\theta, \\tau)$ pair:\n    a. Determine the effective score threshold $\\tau'$.\n    b. Calculate $\\mathrm{TPR}$ and $\\mathrm{FPR}$ using the formulas derived in Section 2.\n    c. Compute the expected counts $\\mathrm{TP}, \\mathrm{FP}, \\mathrm{TN}, \\mathrm{FN}$.\n    d. Calculate the Accuracy and F1-score using the formulas from Section 1.\n    e. Compare the calculated F1-score with the best F1-score found so far for the current classifier. If the new score is strictly greater, update the best score and store the current $(\\theta, \\tau)$ pair.\n    f. Perform a similar comparison and update for the Accuracy score. The strict inequality `` implements the tie-breaking rule of selecting the first pair encountered in the specified grid order.\n\n6.  After iterating through all grid points, the stored $(\\theta, \\tau)$ pairs represent the optimal hyperparameters for each objective.\n\n7.  Collect the eight resulting parameter values in the specified order for the current test case. This process is repeated for all four test cases. The results are then formatted into the required output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the hyperparameter optimization problem for binary classification.\n    \"\"\"\n    \n    # Define the parameter grids. These are treated as ordered lists\n    # to respect the tie-breaking rule.\n    THETA_GRID = [0.8, 1.0, 1.2]\n    T_LR_GRID = [0.2, 0.5, 0.8]   # For Logistic Regression\n    T_SVM_GRID = [-0.5, 0.0, 0.5] # For Support Vector Machine\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N_plus, N_minus, mu_base, sigma)\n        (100, 900, 1.5, 1.0),\n        (50, 4950, 1.0, 1.0),\n        (500, 500, 1.5, 1.0),\n        (100, 900, 0.3, 2.5),\n    ]\n\n    all_results = []\n\n    for n_plus, n_minus, mu_base, sigma in test_cases:\n        \n        # --- Logistic Regression Optimization ---\n        best_f1_lr = -1.0\n        params_f1_lr = (None, None)\n        best_acc_lr = -1.0\n        params_acc_lr = (None, None)\n\n        for theta in THETA_GRID:\n            for tau in T_LR_GRID:\n                # Calculate effective threshold for LR: logit(tau)\n                # Handle tau=0 or tau=1 for numerical stability, though not in grid.\n                if tau = 0 or tau >= 1: continue \n                tau_eff = np.log(tau / (1 - tau))\n\n                # Calculate means based on theta\n                mu_plus = theta * mu_base\n                mu_minus = -theta * mu_base\n\n                # Calculate True Positive Rate and False Positive Rate\n                # Use survival function (1 - CDF) for P(X >= x)\n                tpr = norm.sf((tau_eff - mu_plus) / sigma)\n                fpr = norm.sf((tau_eff - mu_minus) / sigma)\n\n                # Calculate expected confusion matrix counts\n                tp = n_plus * tpr\n                fp = n_minus * fpr\n                fn = n_plus * (1 - tpr)\n                tn = n_minus * (1 - fpr)\n\n                # Calculate metrics\n                # Accuracy\n                accuracy = (tp + tn) / (n_plus + n_minus)\n                \n                # F1-score using robust formula: 2*TP / (2*TP + FP + FN)\n                f1_denom = 2 * tp + fp + fn\n                f1_score = (2 * tp / f1_denom) if f1_denom > 0 else 0.0\n\n                # Update best parameters for LR\n                # Tie-breaking: strict '>' ensures first-in-grid wins\n                if f1_score > best_f1_lr:\n                    best_f1_lr = f1_score\n                    params_f1_lr = (theta, tau)\n                \n                if accuracy > best_acc_lr:\n                    best_acc_lr = accuracy\n                    params_acc_lr = (theta, tau)\n\n        # --- SVM Optimization ---\n        best_f1_svm = -1.0\n        params_f1_svm = (None, None)\n        best_acc_svm = -1.0\n        params_acc_svm = (None, None)\n\n        for theta in THETA_GRID:\n            for tau in T_SVM_GRID:\n                # Effective threshold for SVM is just tau\n                tau_eff = tau\n\n                # Calculate means based on theta\n                mu_plus = theta * mu_base\n                mu_minus = -theta * mu_base\n\n                # Calculate True Positive Rate and False Positive Rate\n                tpr = norm.sf((tau_eff - mu_plus) / sigma)\n                fpr = norm.sf((tau_eff - mu_minus) / sigma)\n\n                # Calculate expected confusion matrix counts\n                tp = n_plus * tpr\n                fp = n_minus * fpr\n                fn = n_plus * (1 - tpr)\n                tn = n_minus * (1 - fpr)\n                \n                # Calculate metrics\n                accuracy = (tp + tn) / (n_plus + n_minus)\n                f1_denom = 2 * tp + fp + fn\n                f1_score = (2 * tp / f1_denom) if f1_denom > 0 else 0.0\n\n                # Update best parameters for SVM\n                if f1_score > best_f1_svm:\n                    best_f1_svm = f1_score\n                    params_f1_svm = (theta, tau)\n                \n                if accuracy > best_acc_svm:\n                    best_acc_svm = accuracy\n                    params_acc_svm = (theta, tau)\n\n        # Collate results for the current test case\n        case_results = [\n            params_f1_lr[0], params_f1_lr[1],\n            params_acc_lr[0], params_acc_lr[1],\n            params_f1_svm[0], params_f1_svm[1],\n            params_acc_svm[0], params_acc_svm[1],\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists.\n    # The `str` of a list already provides the correct '[...]' format for each sublist.\n    # We join these string representations with commas.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3094204"}]}