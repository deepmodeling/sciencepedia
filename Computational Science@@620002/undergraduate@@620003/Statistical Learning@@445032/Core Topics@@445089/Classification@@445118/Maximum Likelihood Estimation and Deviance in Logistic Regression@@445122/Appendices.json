{"hands_on_practices": [{"introduction": "This practice takes us back to the fundamentals of deviance in logistic regression. By deriving the deviance as a sum of individual contributions from each observation, you will gain a granular understanding of how specific data points impact the overall model fit. This exercise sharpens your mathematical skills and introduces the concept of deviance residuals as a diagnostic tool for identifying poorly-fitted points. [@problem_id:3147524]", "problem": "Consider a binary-response logistic regression model fit by Maximum Likelihood Estimation (MLE). Let the observed responses be $\\{y_{i}\\}_{i=1}^{n}$ with $y_{i}\\in\\{0,1\\}$ and let the fitted probabilities be $\\{\\hat{p}_{i}\\}_{i=1}^{n}$, where each $y_{i}$ is modeled as conditionally independent $\\operatorname{Bernoulli}(\\hat{p}_{i})$. Begin from the definition of the Bernoulli log-likelihood and the definition of the deviance as twice the difference between the saturated-model log-likelihood and the fitted-model log-likelihood. Using only these definitions, derive an expression that shows the deviance is a sum of observation-specific terms. Then, using your derived expression, compute the total deviance for the following eight observations:\n$$\n\\begin{aligned}\n&y_{1}=1,\\ \\hat{p}_{1}=0.84;\\quad y_{2}=0,\\ \\hat{p}_{2}=0.15;\\quad y_{3}=1,\\ \\hat{p}_{3}=0.62;\\\\\n&y_{4}=0,\\ \\hat{p}_{4}=0.80;\\quad y_{5}=1,\\ \\hat{p}_{5}=0.30;\\quad y_{6}=0,\\ \\hat{p}_{6}=0.25;\\\\\n&y_{7}=1,\\ \\hat{p}_{7}=0.95;\\quad y_{8}=0,\\ \\hat{p}_{8}=0.55.\n\\end{aligned}\n$$\nFinally, design a diagnostic rule that flags observations with the largest pointwise deviance contributions, ensuring that at most $k=2$ observations are flagged, and apply it to the data above to identify which indices would be flagged. Round your final numerical value for the total deviance to four significant figures. No physical units are involved, and the final numerical answer must be a single real number.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical learning, specifically concerning logistic regression and goodness-of-fit measures. The problem is well-posed, with all necessary data and definitions provided to derive the required expressions, compute the numerical result, and apply the diagnostic rule. There are no contradictions, ambiguities, or factual inaccuracies.\n\nWe begin by deriving the general expression for the deviance in a binary logistic regression model. The problem states that the deviance $D$ is defined as twice the difference between the log-likelihood of the saturated model, $\\ell_{sat}$, and the log-likelihood of the fitted model, $\\ell_{fit}$:\n$$\nD = 2(\\ell_{sat} - \\ell_{fit})\n$$\nThe observations $\\{y_i\\}_{i=1}^n$ are modeled as independent Bernoulli trials, where $y_i \\in \\{0, 1\\}$. The probability mass function for a single observation $Y_i$ with parameter $p_i$ is $P(Y_i=y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$. The log-likelihood for a single observation is therefore $\\ln(P(Y_i=y_i)) = y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)$.\n\nFirst, we determine the log-likelihood of the fitted model, $\\ell_{fit}$. This is obtained by summing the individual log-likelihoods, using the fitted probabilities $\\{\\hat{p}_i\\}_{i=1}^n$:\n$$\n\\ell_{fit} = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]\n$$\nNext, we determine the log-likelihood of the saturated model, $\\ell_{sat}$. A saturated model is one that fits the data perfectly. For a Bernoulli observation $y_i$, a perfect fit is achieved if the model's probability equals the outcome. That is, the probability parameter for the saturated model, $p_i^{sat}$, is equal to $y_i$.\n$$\np_i^{sat} = y_i\n$$\nThe log-likelihood of the saturated model is thus:\n$$\n\\ell_{sat} = \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i^{sat}) + (1 - y_i) \\ln(1 - p_i^{sat}) \\right] = \\sum_{i=1}^{n} \\left[ y_i \\ln(y_i) + (1 - y_i) \\ln(1 - y_i) \\right]\n$$\nWe must evaluate the terms in this sum.\n- If $y_i = 1$, the term is $1 \\ln(1) + (1-1) \\ln(1-1) = \\ln(1) + 0 \\ln(0) = 0$.\n- If $y_i = 0$, the term is $0 \\ln(0) + (1-0) \\ln(1-0) = 0 \\ln(0) + \\ln(1) = 0$.\nIn both cases, we use the fact that $\\lim_{x\\to 0^+} x \\ln(x) = 0$. Thus, each term in the summation for $\\ell_{sat}$ is zero, which means the total log-likelihood for the saturated model is zero:\n$$\n\\ell_{sat} = 0\n$$\nSubstituting the expressions for $\\ell_{sat}$ and $\\ell_{fit}$ into the definition of deviance gives:\n$$\nD = 2 \\left( 0 - \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i) \\right] \\right)\n$$\n$$\nD = -2 \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]\n$$\nThis expression shows the deviance as a sum of observation-specific terms. The contribution of the $i$-th observation to the total deviance, which we can call the pointwise deviance $d_i$, is:\n$$\nd_i = -2 \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]\n$$\nThis expression simplifies based on the value of $y_i$:\n- If $y_i=1$, then $d_i = -2 \\ln(\\hat{p}_i)$.\n- If $y_i=0$, then $d_i = -2 \\ln(1-\\hat{p}_i)$.\n\nNow, we compute the total deviance for the given $n=8$ observations by calculating each $d_i$ and summing them.\n$d_1$: For $y_1=1, \\hat{p}_1=0.84$, $d_1 = -2 \\ln(0.84) \\approx 0.34870$\n$d_2$: For $y_2=0, \\hat{p}_2=0.15$, $d_2 = -2 \\ln(1-0.15) = -2 \\ln(0.85) \\approx 0.32504$\n$d_3$: For $y_3=1, \\hat{p}_3=0.62$, $d_3 = -2 \\ln(0.62) \\approx 0.95607$\n$d_4$: For $y_4=0, \\hat{p}_4=0.80$, $d_4 = -2 \\ln(1-0.80) = -2 \\ln(0.20) \\approx 3.21888$\n$d_5$: For $y_5=1, \\hat{p}_5=0.30$, $d_5 = -2 \\ln(0.30) \\approx 2.40795$\n$d_6$: For $y_6=0, \\hat{p}_6=0.25$, $d_6 = -2 \\ln(1-0.25) = -2 \\ln(0.75) \\approx 0.57536$\n$d_7$: For $y_7=1, \\hat{p}_7=0.95$, $d_7 = -2 \\ln(0.95) \\approx 0.10258$\n$d_8$: For $y_8=0, \\hat{p}_8=0.55$, $d_8 = -2 \\ln(1-0.55) = -2 \\ln(0.45) \\approx 1.59702$\n\nThe total deviance is the sum $D = \\sum_{i=1}^{8} d_i$:\n$$\nD \\approx 0.34870 + 0.32504 + 0.95607 + 3.21888 + 2.40795 + 0.57536 + 0.10258 + 1.59702\n$$\n$$\nD \\approx 9.53160\n$$\nRounding to four significant figures, the total deviance is $9.532$.\n\nFinally, we design and apply the diagnostic rule. The rule is to flag observations with the largest pointwise deviance contributions, with at most $k=2$ observations flagged. This requires ranking the individual deviance contributions, $d_i$, in descending order and selecting the top two.\nThe calculated pointwise deviances are:\n$d_4 \\approx 3.21888$\n$d_5 \\approx 2.40795$\n$d_8 \\approx 1.59702$\n$d_3 \\approx 0.95607$\n$d_6 \\approx 0.57536$\n$d_1 \\approx 0.34870$\n$d_2 \\approx 0.32504$\n$d_7 \\approx 0.10258$\n\nThe two largest values are $d_4$ and $d_5$. These correspond to observations with indices $4$ and $5$. Therefore, the diagnostic rule would flag observations $y_4$ and $y_5$. These large deviance contributions arise because the model assigned a high probability of success ($\\hat{p}_4=0.80$) to an observed failure ($y_4=0$), and a low probability of success ($\\hat{p}_5=0.30$) to an observed success ($y_5=1$).\n\nThe final numerical answer required is the total deviance rounded to four significant figures.", "answer": "$$\n\\boxed{9.532}\n$$", "id": "3147524"}, {"introduction": "A common point of confusion in classification tasks is the difference between a model's probabilistic accuracy and its performance on a classification task. This exercise is designed to clarify that distinction by showing that deviance, a measure of model fit, is independent of the decision threshold used for classification. Understanding this separation is crucial for both evaluating models properly and deploying them effectively to make decisions under varying costs. [@problem_id:3147489]", "problem": "Consider a binary outcome setting modeled by logistic regression, where for each observation $i \\in \\{1,2,\\dots,n\\}$ with features $\\mathbf{x}_i$, the model produces a fitted probability $p_i \\in (0,1)$ of $Y_i=1$. The Bernoulli likelihood $L(\\boldsymbol{\\beta})$ for parameters $\\boldsymbol{\\beta}$ is defined as the probability of the observed data under the model, and the log-likelihood is $\\log L(\\boldsymbol{\\beta})$. The residual deviance is defined as $-2$ times the logarithm of the likelihood ratio comparing the fitted model to the saturated model, and the saturated model is the model that fits the observed data perfectly. A decision rule maps each fitted probability $p_i$ to a predicted class $\\hat{y}_i(t) \\in \\{0,1\\}$ using a threshold $t \\in (0,1)$, where $\\hat{y}_i(t)=1$ if $p_i \\ge t$ and $\\hat{y}_i(t)=0$ otherwise.\n\nYou are given $n=6$ observations with observed outcomes and fitted probabilities from a single logistic regression fit:\n- $y = (y_1,y_2,y_3,y_4,y_5,y_6) = (1,1,0,1,0,0)$,\n- $p = (p_1,p_2,p_3,p_4,p_5,p_6) = (0.85,0.55,0.29,0.31,0.25,0.05)$.\n\nConsider two decision thresholds $t_1=0.50$ and $t_2=0.30$.\n\nWhich of the following statements are correct?\n\nA. Changing the decision threshold does not affect the residual deviance of this fixed logistic regression fit; for the data above, the residual deviance computed from the fitted probabilities is approximately $5.23$, and it would be the same regardless of using $t=0.50$ or $t=0.30$ to make class decisions.\n\nB. Under $t=0.50$ the misclassification count is $1$, whereas under $t=0.30$ the misclassification count is $0$.\n\nC. For any fixed threshold $t$, minimizing the misclassification count is equivalent to maximizing the Bernoulli likelihood (and hence minimizing deviance) for logistic regression.\n\nD. When misclassification costs are asymmetric, an optimal decision rule uses a threshold $t^{\\ast}$ that depends on the cost ratio, while the Maximum Likelihood Estimation (MLE) fit and its deviance remain threshold-independent; thus model evaluation by deviance and policy decision by thresholding are separable.\n\nE. Lowering the threshold necessarily decreases the deviance because it makes more positive predictions align with true positives.\n\nSelect all that apply.", "solution": "The user requires a critical validation of the problem statement, followed by a detailed solution deriving the correct answer and evaluating each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Model: Logistic regression for a binary outcome $Y_i \\in \\{0, 1\\}$.\n- For each observation $i \\in \\{1, 2, \\dots, n\\}$, the model produces a fitted probability $p_i = P(Y_i=1 | \\mathbf{x}_i)$.\n- The residual deviance is defined as $-2$ times the logarithm of the likelihood ratio comparing the fitted model to the saturated model. The saturated model fits the data perfectly.\n- Decision rule: For a threshold $t \\in (0,1)$, the predicted class is $\\hat{y}_i(t) = 1$ if $p_i \\ge t$ and $\\hat{y}_i(t) = 0$ if $p_i < t$.\n- Sample size: $n = 6$.\n- Observed outcomes: $y = (y_1, y_2, y_3, y_4, y_5, y_6) = (1, 1, 0, 1, 0, 0)$.\n- Fitted probabilities: $p = (p_1, p_2, p_3, p_4, p_5, p_6) = (0.85, 0.55, 0.29, 0.31, 0.25, 0.05)$.\n- Decision thresholds for consideration: $t_1 = 0.50$ and $t_2 = 0.30$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is grounded in the standard theory of statistical learning, specifically logistic regression, maximum likelihood estimation, deviance, and classification thresholds. All definitions provided are standard.\n- **Well-Posed:** The problem provides all necessary data ($y$, $p$, $t_1$, $t_2$) and clear definitions to evaluate the statements. A unique analysis is possible.\n- **Objective:** The language is formal and unambiguous. The problem concerns verifiable mathematical and statistical properties.\n- **Flaw Check:** The problem statement is free from the listed flaws.\n    - It does not violate any scientific principles. The data point $(y_4=1, p_4=0.31)$, where the model assigns low probability to an event that occurred, represents a model error and is a realistic feature of statistical modeling.\n    - It is a formal problem in its domain.\n    - The setup is complete and consistent.\n    - The data is plausible for a small-sample problem.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution and option-by-option analysis.\n\n---\n\n### Solution Derivation\n\nThe core of the problem lies in distinguishing between a model's goodness-of-fit, measured by likelihood and deviance, and its classification performance, measured by metrics like misclassification count which depend on a chosen decision threshold.\n\nThe log-likelihood of a logistic regression model for a set of $n$ independent observations is:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] $$\nwhere $p_i$ are the fitted probabilities, which are functions of the model parameters $\\boldsymbol{\\beta}$.\n\nThe saturated model is a hypothetical model that fits the data perfectly, meaning its log-likelihood is $0$. The residual deviance of the fitted model is defined as:\n$$ D = -2 \\times (\\ell(\\text{fitted model}) - \\ell(\\text{saturated model})) = -2 \\ell(\\text{fitted model}) $$\n$$ D = -2 \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] $$\nCrucially, the deviance $D$ depends only on the true outcomes $y_i$ and the fitted probabilities $p_i$. It does not depend on any decision threshold $t$.\n\n### Option-by-Option Analysis\n\n**A. Changing the decision threshold does not affect the residual deviance of this fixed logistic regression fit; for the data above, the residual deviance computed from the fitted probabilities is approximately $5.23$, and it would be the same regardless of using $t=0.50$ or $t=0.30$ to make class decisions.**\n\nThe first part of the statement, \"Changing the decision threshold does not affect the residual deviance,\" is correct by definition. The deviance is a measure of fit based on the probabilities $p_i$ produced by the Maximum Likelihood Estimation (MLE) procedure, which does not involve a threshold. The threshold is applied as a separate, subsequent step to make class predictions.\n\nLet's verify the calculation.\nGiven $y = (1, 1, 0, 1, 0, 0)$ and $p = (0.85, 0.55, 0.29, 0.31, 0.25, 0.05)$.\nThe log-likelihood terms are:\n$y_1=1: \\ln(0.85)$\n$y_2=1: \\ln(0.55)$\n$y_3=0: \\ln(1-0.29) = \\ln(0.71)$\n$y_4=1: \\ln(0.31)$\n$y_5=0: \\ln(1-0.25) = \\ln(0.75)$\n$y_6=0: \\ln(1-0.05) = \\ln(0.95)$\nThe total log-likelihood is:\n$$ \\ell = \\ln(0.85) + \\ln(0.55) + \\ln(0.71) + \\ln(0.31) + \\ln(0.75) + \\ln(0.95) $$\nUsing natural logarithms:\n$$ \\ell \\approx -0.162519 - 0.597837 - 0.342490 - 1.171183 - 0.287682 - 0.051293 \\approx -2.613004 $$\nThe deviance is $D = -2 \\times \\ell$:\n$$ D \\approx -2 \\times (-2.613004) \\approx 5.226008 $$\nThis value is approximately $5.23$. Both the conceptual claim and the calculation are correct.\n\nVerdict: **Correct**.\n\n**B. Under $t=0.50$ the misclassification count is $1$, whereas under $t=0.30$ the misclassification count is $0$.**\n\nLet's calculate the predicted classes $\\hat{y}$ for each threshold and compare them to the true outcomes $y = (1, 1, 0, 1, 0, 0)$. The fitted probabilities are $p = (0.85, 0.55, 0.29, 0.31, 0.25, 0.05)$.\n\n- For threshold $t_1 = 0.50$:\nThe rule is $\\hat{y}_i = 1$ if $p_i \\ge 0.50$.\n$p_1=0.85 \\ge 0.50 \\implies \\hat{y}_1=1$ (Correct, $y_1=1$)\n$p_2=0.55 \\ge 0.50 \\implies \\hat{y}_2=1$ (Correct, $y_2=1$)\n$p_3=0.29 < 0.50 \\implies \\hat{y}_3=0$ (Correct, $y_3=0$)\n$p_4=0.31 < 0.50 \\implies \\hat{y}_4=0$ (Incorrect, $y_4=1$)\n$p_5=0.25 < 0.50 \\implies \\hat{y}_5=0$ (Correct, $y_5=0$)\n$p_6=0.05 < 0.50 \\implies \\hat{y}_6=0$ (Correct, $y_6=0$)\nThe predicted vector is $\\hat{y}(0.50) = (1, 1, 0, 0, 0, 0)$. There is one mismatch at index $4$. The misclassification count is $1$.\n\n- For threshold $t_2 = 0.30$:\nThe rule is $\\hat{y}_i = 1$ if $p_i \\ge 0.30$.\n$p_1=0.85 \\ge 0.30 \\implies \\hat{y}_1=1$ (Correct, $y_1=1$)\n$p_2=0.55 \\ge 0.30 \\implies \\hat{y}_2=1$ (Correct, $y_2=1$)\n$p_3=0.29 < 0.30 \\implies \\hat{y}_3=0$ (Correct, $y_3=0$)\n$p_4=0.31 \\ge 0.30 \\implies \\hat{y}_4=1$ (Correct, $y_4=1$)\n$p_5=0.25 < 0.30 \\implies \\hat{y}_5=0$ (Correct, $y_5=0$)\n$p_6=0.05 < 0.30 \\implies \\hat{y}_6=0$ (Correct, $y_6=0$)\nThe predicted vector is $\\hat{y}(0.30) = (1, 1, 0, 1, 0, 0)$. This is identical to the true outcome vector $y$. The misclassification count is $0$.\n\nThe statement is a factual report of two calculations, both of which are correct.\n\nVerdict: **Correct**.\n\n**C. For any fixed threshold $t$, minimizing the misclassification count is equivalent to maximizing the Bernoulli likelihood (and hence minimizing deviance) for logistic regression.**\n\nThis statement is fundamentally incorrect. It conflates two different objective functions.\n1.  **Maximizing Likelihood (Minimizing Deviance):** The goal of MLE for logistic regression is to find parameters $\\boldsymbol{\\beta}$ that maximize the log-likelihood $\\sum [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$. This is a continuous and differentiable function of the probabilities $p_i$ (and thus $\\boldsymbol{\\beta}$). It provides a \"soft\" measure of fit, rewarding confident correct predictions more than marginal ones. This optimization procedure does not involve a threshold $t$.\n2.  **Minimizing Misclassification Count:** This objective function, $\\sum I(y_i \\neq \\hat{y}_i(t))$, is a discrete, non-differentiable \"hard\" count of errors that depends on a pre-specified threshold $t$.\nMaximizing likelihood does not, in general, lead to the same solution as minimizing the misclassification count for a given $t$. The very data in this problem demonstrates this: the MLE fit is fixed, but the misclassification count changes from $1$ to $0$ as the threshold changes.\n\nVerdict: **Incorrect**.\n\n**D. When misclassification costs are asymmetric, an optimal decision rule uses a threshold $t^{\\ast}$ that depends on the cost ratio, while the Maximum Likelihood Estimation (MLE) fit and its deviance remain threshold-independent; thus model evaluation by deviance and policy decision by thresholding are separable.**\n\nThis statement accurately describes a key principle of statistical decision theory.\n- **Optimal Threshold with Asymmetric Costs:** Let $C_{10}$ be the cost of a false positive (predicting $1$ when truth is $0$) and $C_{01}$ be the cost of a false negative (predicting $0$ when truth is $1$). To minimize expected cost for an observation with probability $p_i$, one should predict $1$ if the expected cost of predicting $1$ is less than the expected cost of predicting $0$: $(1-p_i)C_{10} < p_iC_{01}$. This inequality rearranges to $p_i > \\frac{C_{10}}{C_{10} + C_{01}}$. The optimal threshold is $t^{\\ast} = \\frac{C_{10}}{C_{10} + C_{01}}$, which clearly depends on the costs.\n- **Threshold-Independence of MLE/Deviance:** As established in the analysis of option A, the MLE procedure determines the probabilities $p_i$ by maximizing the likelihood function. This process is independent of any subsequent use of the probabilities, including classification with asymmetric costs. The deviance is simply a transformation of this maximized likelihood.\n- **Separability:** The conclusion correctly follows. The task of building a probabilistic model (MLE fit) is separate from the task of using that model to make decisions (policy decision by thresholding). The model provides the probabilities, and the decision rule applies costs and a threshold to those probabilities. This separation is fundamental.\n\nVerdict: **Correct**.\n\n**E. Lowering the threshold necessarily decreases the deviance because it makes more positive predictions align with true positives.**\n\nThis statement is incorrect on two levels.\n- The premise, \"Lowering the threshold necessarily decreases the deviance,\" is false. As established repeatedly, the deviance of a fitted model is a fixed quantity that does not depend on the decision threshold.\n- The justification, \"...because it makes more positive predictions align with true positives,\" describes a change in a classification metric (like the True Positive Rate or Sensitivity), not a change in the model's deviance. While lowering the threshold does increase the number of positive predictions, which can increase the number of true positives, it has no effect on the deviance. The statement confounds a classification performance outcome with a likelihood-based goodness-of-fit measure.\n\nVerdict: **Incorrect**.", "answer": "$$\n\\boxed{ABD}\n$$", "id": "3147489"}, {"introduction": "With a solid grasp of deviance, we can now use it as a powerful tool for model comparison. This practice guides you through a classic case of Simpson's paradox, where a trend appears in different groups of data but disappears or reverses when these groups are combined. You will use the deviance drop between a pooled model and subgroup-specific models to quantitatively demonstrate the presence of confounding, highlighting how deviance serves as a key metric in the likelihood-ratio test framework for selecting better-fitting models. [@problem_id:3147557]", "problem": "You are given a binary-outcome classification setting modeled by logistic regression. The outcome variable is $y \\in \\{0,1\\}$, the predictor is a binary treatment indicator $x \\in \\{0,1\\}$, and there is a binary subgroup indicator $g \\in \\{A,B\\}$. The statistical learning objective is to compare a pooled logistic regression model that ignores subgroup membership with a pair of subgroup-specific logistic regression models fit independently within each subgroup. The comparison is performed using the deviance under Maximum Likelihood Estimation (MLE), starting from the definition of the Bernoulli likelihood and the logistic link. Your task is to derive, implement, and compute the following, from first principles, without relying on pre-derived shortcut formulas:\n\n- Implement MLE for logistic regression by directly optimizing the Bernoulli log-likelihood under the logistic link, using a principled iterative method that follows from differentiating the log-likelihood with respect to the parameters.\n- Define and compute the deviance of a fitted logistic regression model from the foundational definition where the saturated model assigns $y_i$ exactly to each observation, and the fitted model uses the estimated probabilities from MLE.\n- For a pooled model, fit a single logistic regression to the entire dataset using only an intercept and the single predictor $x$ (i.e., subgroup $g$ is ignored).\n- For the subgroup-specific model, fit two separate logistic regressions, one within subgroup $A$ and one within subgroup $B$, each using an intercept and the single predictor $x$. The overall subgroup-specific log-likelihood is the sum of the two subgroup log-likelihoods, and the corresponding deviance is computed from this combined log-likelihood.\n- Compute the deviance drop, defined as the pooled-model deviance minus the subgroup-specific deviance. A positive deviance drop indicates that the subgroup-specific modeling yields a lower deviance (better fit) than the pooled modeling.\n\nUse the following test suite of datasets, expressed as counts to be expanded into individual observations. Each dataset specifies, for each subgroup $g \\in \\{A,B\\}$ and treatment level $x \\in \\{0,1\\}$, the total number of observations and the number of successes. You must construct the individual binary outcomes $y$ consistent with these counts, and the corresponding $x$ and $g$ labels. The datasets are designed to probe the presence or absence of Simpson’s paradox and to exercise both typical and edge-case behavior.\n\n- Test case $1$ (strong Simpson’s paradox: treatment helpful within subgroups but harmful when pooled due to confounding by subgroup):\n  - Subgroup $A$: \n    - $x=0$: total $80$, successes $56$.\n    - $x=1$: total $20$, successes $16$.\n  - Subgroup $B$:\n    - $x=0$: total $20$, successes $2$.\n    - $x=1$: total $80$, successes $16$.\n- Test case $2$ (balanced assignment, treatment helpful both within subgroups and in aggregate):\n  - Subgroup $A$:\n    - $x=0$: total $50$, successes $35$.\n    - $x=1$: total $50$, successes $40$.\n  - Subgroup $B$:\n    - $x=0$: total $50$, successes $5$.\n    - $x=1$: total $50$, successes $10$.\n- Test case $3$ (edge case: no treatment effect and no subgroup differences; pooled and subgroup-specific fits should be similar):\n  - Subgroup $A$:\n    - $x=0$: total $40$, successes $20$.\n    - $x=1$: total $40$, successes $20$.\n  - Subgroup $B$:\n    - $x=0$: total $40$, successes $20$.\n    - $x=1$: total $40$, successes $20$.\n\nYour program must:\n- For each test case, build the individual-level dataset consistent with the specified counts.\n- Fit the pooled logistic regression model with parameters $(\\beta_0, \\beta_1)$, where $\\beta_0$ is the intercept and $\\beta_1$ multiplies $x$.\n- Fit two subgroup-specific logistic regression models:\n  - For subgroup $A$, fit $(\\beta_{0A}, \\beta_{1A})$ using only the data where $g=A$.\n  - For subgroup $B$, fit $(\\beta_{0B}, \\beta_{1B})$ using only the data where $g=B$.\n- Compute the pooled deviance and the subgroup-specific deviance, each defined from the Bernoulli likelihood and the saturated model, and report the deviance drop (pooled deviance minus subgroup-specific deviance) for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the deviance drops for the three test cases as a comma-separated list enclosed in square brackets, for example `[d1,d2,d3]`, where each $d_i$ is a floating-point number rounded to $6$ decimal places. No other text should be printed.\n\nAll quantities in this problem are pure numbers without physical units, and no angles are involved. Express all numerical answers as decimals (not percentages). Ensure that your implementation is grounded in the foundational definitions of the Bernoulli likelihood and logistic link, and that the iterative optimization procedure is derived from these foundations rather than using pre-packaged shortcuts.", "solution": "The user wants to analyze binary outcome data using logistic regression, comparing a pooled model to a subgroup-specific model based on deviance. The entire process, from Maximum Likelihood Estimation (MLE) to deviance calculation, must be derived from first principles.\n\n### Step 1: Extract Givens\n- **Outcome Variable**: $y \\in \\{0, 1\\}$ (binary).\n- **Predictor Variables**: $x \\in \\{0, 1\\}$ (binary treatment indicator), $g \\in \\{A, B\\}$ (binary subgroup indicator).\n- **Statistical Model**: Logistic regression, which models the probability of success $p = P(y=1)$ using the logistic link function.\n- **Objective**: Compare a pooled model versus a subgroup-specific model using the deviance drop.\n- **Pooled Model**: A single logistic regression of $y$ on an intercept and $x$. The linear predictor is $\\eta = \\beta_0 + \\beta_1 x$.\n- **Subgroup-Specific Model**: Two separate logistic regressions: one for subgroup $A$ (data where $g=A$) with predictor $\\eta_A = \\beta_{0A} + \\beta_{1A} x$, and one for subgroup $B$ (data where $g=B$) with predictor $\\eta_B = \\beta_{0B} + \\beta_{1B} x$.\n- **Estimation Method**: Maximum Likelihood Estimation (MLE) by directly optimizing the Bernoulli log-likelihood using a principled iterative method derived from its derivatives.\n- **Comparison Metric**: Deviance, defined as $D = 2(\\ell_{sat} - \\ell_{fit})$, where $\\ell_{sat}$ is the log-likelihood of a saturated model that perfectly predicts each outcome $y_i$, and $\\ell_{fit}$ is the log-likelihood of the fitted model. The deviance drop is defined as $D_{pooled} - D_{subgroup}$.\n- **Datasets (Test Cases)**: Three datasets are provided as counts of successes and totals for each combination of subgroup $g$ and treatment $x$.\n    - **Test Case 1**: Subgroup A: ($x=0$: $56/80$ successes; $x=1$: $16/20$ successes). Subgroup B: ($x=0$: $2/20$ successes; $x=1$: $16/80$ successes).\n    - **Test Case 2**: Subgroup A: ($x=0$: $35/50$ successes; $x=1$: $40/50$ successes). Subgroup B: ($x=0$: $5/50$ successes; $x=1$: $10/50$ successes).\n    - **Test Case 3**: Subgroup A: ($x=0$: $20/40$ successes; $x=1$: $20/40$ successes). Subgroup B: ($x=0$: $20/40$ successes; $x=1$: $20/40$ successes).\n- **Required Output**: A single line containing a comma-separated list of the deviance drops for the three test cases, rounded to $6$ decimal places: $[d_1, d_2, d_3]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is rooted in fundamental statistical theory, specifically generalized linear models, logistic regression, maximum likelihood estimation, and model comparison via deviance. These are all standard, well-established concepts. The datasets provided, including one exhibiting Simpson's paradox, are classic examples used in teaching these concepts. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The objective is clear, the models are specified, the data is provided, and the required output format is unambiguous. The instruction to derive the methods from first principles provides a clear path to a unique and meaningful solution. The data does not exhibit perfect separation, ensuring that finite MLE estimates for the parameters exist.\n- **Objective**: The problem statement is objective, using precise statistical terminology and providing concrete numerical data. There are no subjective or opinion-based elements.\n\nThe problem does not exhibit any of the invalidity flaws. It is scientifically sound, formally specified, complete, and objective.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution requires implementing logistic regression from first principles. This involves maximizing the log-likelihood function to find the MLE of the model parameters and then using these parameters to compute the model deviance.\n\n**1. Logistic Regression Model and Likelihood**\n\nFor an individual observation $i$, with outcome $y_i \\in \\{0, 1\\}$ and a vector of predictors $\\mathbf{x}_i$ (which includes a $1$ for the intercept), the logistic regression model assumes the outcome follows a Bernoulli distribution, $y_i \\sim \\text{Bernoulli}(p_i)$. The probability of success $p_i$ is related to a linear combination of the predictors, $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, via the logit link function's inverse (the logistic or sigmoid function):\n$$ p_i = P(y_i=1 | \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{1}{1 + e^{-\\eta_i}} $$\nThe likelihood for a single observation is given by the Bernoulli probability mass function:\n$$ L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1 - p_i)^{1 - y_i} $$\nFor a dataset of $N$ independent observations, the total likelihood is the product of individual likelihoods, $L(\\boldsymbol{\\beta}) = \\prod_{i=1}^N L_i(\\boldsymbol{\\beta})$. It is computationally more convenient to work with the log-likelihood, $\\ell(\\boldsymbol{\\beta})$:\n$$ \\ell(\\boldsymbol{\\beta}) = \\log(L(\\boldsymbol{\\beta})) = \\sum_{i=1}^N \\log(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)] $$\nSubstituting the expression for $p_i$ in terms of $\\eta_i$:\n$$ \\log(p_i) = \\eta_i - \\log(1 + e^{\\eta_i}) $$\n$$ \\log(1-p_i) = -\\log(1 + e^{\\eta_i}) $$\nThe log-likelihood becomes:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i \\eta_i - \\log(1 + e^{\\eta_i})] = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) - \\log(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})] $$\n\n**2. Maximum Likelihood Estimation via Newton-Raphson**\n\nTo find the MLE parameters $\\hat{\\boldsymbol{\\beta}}$, we must maximize $\\ell(\\boldsymbol{\\beta})$. This is achieved by finding the roots of its gradient, $\\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{0}$. We use the Newton-Raphson method, an iterative algorithm that requires the gradient (score vector) and the Hessian matrix (matrix of second derivatives) of the log-likelihood.\n\nThe gradient with respect to a parameter $\\beta_j$ is:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^N \\left[ y_i x_{ij} - \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} x_{ij} \\right] = \\sum_{i=1}^N (y_i - p_i) x_{ij} $$\nIn matrix form, with $\\mathbf{X}$ as the design matrix, $\\mathbf{y}$ as the outcome vector, and $\\mathbf{p}$ as the vector of probabilities, the gradient is:\n$$ \\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p}) $$\nThe Hessian matrix $H$ has elements $H_{jk} = \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^N (y_i - p_i) x_{ij} = \\sum_{i=1}^N - \\frac{\\partial p_i}{\\partial \\beta_k} x_{ij} $$\nUsing the chain rule, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$. Since $\\frac{\\partial p_i}{\\partial \\eta_i} = p_i(1-p_i)$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_k} = x_{ik}$, we get:\n$$ H_{jk} = \\sum_{i=1}^N -p_i(1-p_i) x_{ij} x_{ik} $$\nIn matrix form, this is $H(\\boldsymbol{\\beta}) = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with diagonal elements $W_{ii} = p_i(1-p_i)$.\n\nThe Newton-Raphson update rule for iteration $t+1$ is:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [H(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla \\ell(\\boldsymbol{\\beta}^{(t)}) $$\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p}^{(t)}) $$\nThis algorithm is also known as Iteratively Reweighted Least Squares (IRLS). We initialize $\\boldsymbol{\\beta}$ (e.g., to zeros) and iterate until convergence.\n\n**3. Deviance Calculation**\n\nThe deviance of a model is $D = 2(\\ell_{sat} - \\ell_{fit})$.\n- $\\ell_{fit}$ is the log-likelihood of the fitted model, $\\ell(\\hat{\\boldsymbol{\\beta}})$, calculated using the MLE parameters.\n- $\\ell_{sat}$ is the log-likelihood of the saturated model. A saturated model has one parameter per observation, allowing it to fit the data perfectly. For a Bernoulli outcome $y_i$, the saturated model predicts probability $\\hat{p}_{i,sat} = y_i$. The log-likelihood for observation $i$ is $y_i \\log(\\hat{p}_{i,sat}) + (1-y_i) \\log(1 - \\hat{p}_{i,sat})$. If $y_i=1$, this term is $1 \\log(1) + 0 \\log(0) = 0$. If $y_i=0$, the term is $0 \\log(0) + 1 \\log(1) = 0$. (The convention $0 \\log(0) = 0$ is used). Thus, the log-likelihood contribution from each observation is $0$, and the total log-likelihood for the saturated model is $\\ell_{sat} = 0$.\n\nTherefore, the deviance simplifies to:\n$$ D = -2 \\ell_{fit} = -2 \\sum_{i=1}^N [y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i)] $$\nwhere $\\hat{p}_i$ are the probabilities predicted by the fitted model.\n\n**4. Model Comparison**\n\nWe fit the two specified models:\n- **Pooled Model**: A single logistic regression on the entire dataset. The design matrix $\\mathbf{X}$ has two columns: an intercept (all 1s) and the treatment indicator $x$. We compute $\\hat{\\boldsymbol{\\beta}}_{pooled}$ and the corresponding deviance, $D_{pooled}$.\n- **Subgroup-Specific Model**: We perform two independent fits.\n    1. For subgroup $A$, using only data where $g=A$, we fit a model to get $\\hat{\\boldsymbol{\\beta}}_A$ and its deviance $D_A$.\n    2. For subgroup $B$, using only data where $g=B$, we fit a model to get $\\hat{\\boldsymbol{\\beta}}_B$ and its deviance $D_B$.\nThe total log-likelihood for the subgroup-specific approach is $\\ell_{subgroup} = \\ell_A + \\ell_B$. The total deviance is $D_{subgroup} = D_A + D_B$.\n\nThe deviance drop is $D_{pooled} - D_{subgroup}$. This value is the likelihood-ratio test statistic for comparing the two nested models. A positive value indicates that the more complex subgroup-specific model provides a better fit to the data than the simpler pooled model.\n\nThis framework is implemented for each test case to compute the required deviance drops.\n```python\nimport numpy as np\n\n# A small epsilon for numerical stability in log calculations and matrix inversion\nEPSILON = 1e-15\n\ndef sigmoid(eta):\n    \"\"\"Computes the sigmoid function.\"\"\"\n    # Clip eta to avoid overflow in exp\n    eta = np.clip(eta, -500, 500)\n    return 1.0 / (1.0 + np.exp(-eta))\n\ndef construct_dataset(counts):\n    \"\"\"\n    Expands dataset from counts into individual observations.\n    \n    Args:\n        counts (list): A list of tuples, where each tuple is\n                       (subgroup_char, x_val, total, successes).\n\n    Returns:\n        tuple: A tuple of numpy arrays (y, x, g).\n    \"\"\"\n    y_list, x_list, g_list = [], [], []\n    g_map = {'A': 0, 'B': 1}\n    for g_char, x_val, total, successes in counts:\n        failures = total - successes\n        # Add successes\n        y_list.extend([1] * successes)\n        x_list.extend([x_val] * successes)\n        g_list.extend([g_map[g_char]] * successes)\n        # Add failures\n        y_list.extend([0] * failures)\n        x_list.extend([x_val] * failures)\n        g_list.extend([g_map[g_char]] * failures)\n    return np.array(y_list), np.array(x_list), np.array(g_list)\n\ndef fit_logistic_regression(X, y, tol=1e-8, max_iter=30):\n    \"\"\"\n    Fits a logistic regression model using Newton-Raphson (IRLS).\n\n    Args:\n        X (np.ndarray): Design matrix (N_samples, N_features), with intercept.\n        y (np.ndarray): Binary outcome vector (N_samples,).\n        tol (float): Convergence tolerance for the norm of the step vector.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The fitted coefficient vector beta.\n    \"\"\"\n    # Initialize beta coefficients to zero\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        eta = X @ beta\n        p = sigmoid(eta)\n        \n        # Diagonal of the weight matrix W\n        W_diag = p * (1 - p)\n        # Ensure weights are not exactly zero to avoid singular matrix\n        W_diag = np.maximum(W_diag, EPSILON)\n        \n        # Gradient of the log-likelihood\n        grad = X.T @ (y - p)\n        \n        # Hessian of the log-likelihood is -X.T @ W @ X\n        # We compute J = -H, the Fisher Information Matrix\n        # (W_diag[:, np.newaxis] * X) performs row-wise multiplication (efficiently)\n        J = X.T @ (W_diag[:, np.newaxis] * X)\n        \n        try:\n            # Solve J * step = grad for the step vector\n            # This is more numerically stable than computing the inverse of J\n            step = np.linalg.solve(J, grad)\n        except np.linalg.LinAlgError:\n            # This may happen with (quasi-)complete separation, but the data\n            # given in the problem does not have this issue.\n            break\n\n        beta = beta + step\n        \n        if np.linalg.norm(step)  tol:\n            break\n            \n    return beta\n\ndef calculate_deviance(y, X, beta):\n    \"\"\"\n    Calculates the deviance of a fitted logistic regression model.\n\n    Args:\n        y (np.ndarray): True outcome vector.\n        X (np.ndarray): Design matrix.\n        beta (np.ndarray): Fitted coefficient vector.\n\n    Returns:\n        float: The deviance of the model.\n    \"\"\"\n    if y.size == 0:\n        return 0.0\n\n    eta = X @ beta\n    p_hat = sigmoid(eta)\n    \n    # Clip probabilities to avoid log(0)\n    p_hat = np.clip(p_hat, EPSILON, 1 - EPSILON)\n    \n    # Log-likelihood of the fitted model\n    log_likelihood = np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))\n    \n    # Deviance = -2 * log_likelihood (since log_likelihood of saturated model is 0\n    # for individual Bernoulli trials)\n    return -2.0 * log_likelihood\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the result.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (strong Simpson's paradox)\n        [('A', 0, 80, 56), ('A', 1, 20, 16), ('B', 0, 20, 2), ('B', 1, 80, 16)],\n        # Test case 2 (balanced assignment, consistent effect)\n        [('A', 0, 50, 35), ('A', 1, 50, 40), ('B', 0, 50, 5), ('B', 1, 50, 10)],\n        # Test case 3 (edge case: no effect)\n        [('A', 0, 40, 20), ('A', 1, 40, 20), ('B', 0, 40, 20), ('B', 1, 40, 20)],\n    ]\n    \n    results = []\n    \n    for case_counts in test_cases:\n        y, x, g = construct_dataset(case_counts)\n        \n        # Design matrix with intercept\n        X = np.c_[np.ones(x.shape[0]), x]\n        \n        # 1. Pooled Model\n        beta_pooled = fit_logistic_regression(X, y)\n        deviance_pooled = calculate_deviance(y, X, beta_pooled)\n        \n        # 2. Subgroup-specific Model\n        \n        # Subgroup A\n        mask_A = (g == 0)\n        y_A, X_A = y[mask_A], X[mask_A]\n        beta_A = fit_logistic_regression(X_A, y_A)\n        deviance_A = calculate_deviance(y_A, X_A, beta_A)\n\n        # Subgroup B\n        mask_B = (g == 1)\n        y_B, X_B = y[mask_B], X[mask_B]\n        beta_B = fit_logistic_regression(X_B, y_B)\n        deviance_B = calculate_deviance(y_B, X_B, beta_B)\n\n        deviance_subgroup = deviance_A + deviance_B\n        \n        # 3. Deviance Drop\n        deviance_drop = deviance_pooled - deviance_subgroup\n        results.append(round(deviance_drop, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\n# To get the answer, one would run the following:\n# solve()\n```", "answer": "[33.72591,2.062064,0.0]", "id": "3147557"}]}