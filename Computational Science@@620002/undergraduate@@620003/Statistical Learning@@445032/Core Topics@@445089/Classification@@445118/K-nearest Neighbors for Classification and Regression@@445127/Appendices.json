{"hands_on_practices": [{"introduction": "A core challenge in machine learning is assessing a model's true performance on unseen data. Using the training data itself for evaluation can be misleadingly optimistic, a phenomenon starkly illustrated by the 1-Nearest Neighbor classifier. This exercise [@problem_id:3135589] provides a hands-on demonstration of this issue, contrasting the naive resubstitution error with the more robust leave-one-out cross-validation error to build intuition about overfitting and the necessity of proper model validation.", "problem": "You are given a classification setting with a finite training dataset, where each observation is a pair $(\\mathbf{x}_i, y_i)$ with feature vector $\\mathbf{x}_i \\in \\mathbb{R}^d$ and class label $y_i \\in \\{0,1\\}$, for $i = 1, \\dots, n$. The fundamental base consists of: (i) the definition of the Euclidean distance $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$, (ii) the $k$-nearest neighbors rule that selects the $k$ training observations with the smallest distances to the query point, (iii) the empirical risk (resubstitution error) defined under $0$-$1$ loss as the average number of misclassifications made when each training point is predicted using a specified classifier and the full training set, and (iv) leave-one-out cross-validation that evaluates the classifier by, for each training point, predicting it using the training set with that point removed, and averaging the resulting $0$-$1$ losses. In all computations, ties in distances must be broken deterministically: when ordering neighbors by distance, sort by ascending distance, then prioritize the point itself if present, and finally break any remaining ties by ascending index. For majority voting in classification when $k \\ge 2$, if there is a tie in class counts among the $k$ neighbors, select the smallest class label.\n\nTask A: Starting from the definitions above, confirm that the empirical risk (resubstitution error) for the $k=1$ nearest neighbor classifier is $0$ when the neighbor set for each training point includes the point itself. Then, compute the leave-one-out error for $k=1$ and compare it to the resubstitution error for $k=2$ on the datasets specified below. The comparison must be based on numerical values of the error rates.\n\nTask B: Based on the observed numerical comparison, analyze the implications for model selection between $k=1$ and $k=2$ using resubstitution error versus leave-one-out error, grounding your discussion in the definitions provided (your analysis is to be presented in your solution, not printed by the code).\n\nUse the following test suite of datasets, each given by a matrix of features and a vector of labels. All features are two-dimensional, $d = 2$, and all class labels are in $\\{0,1\\}$. Distances are Euclidean. There are no physical units involved.\n\n- Dataset $1$ (well-separated classes):\n  - Features $X_1$: $\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$\n  - Labels $y_1$: $[0, 0, 1, 1]$\n- Dataset $2$ (overlap near the boundary):\n  - Features $X_2$: $\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$\n  - Labels $y_2$: $[0, 0, 1]$\n- Dataset $3$ (mixture with a noisy point):\n  - Features $X_3$: $\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$\n  - Labels $y_3$: $[0, 0, 1, 1, 0]$\n\nFor each dataset $j \\in \\{1,2,3\\}$, compute:\n- $E_{j}^{\\text{train}, k=1}$: the empirical risk (resubstitution error) for $k=1$ including the point itself among neighbors,\n- $E_{j}^{\\text{LOO}, k=1}$: the leave-one-out error for $k=1$ excluding the point itself,\n- $E_{j}^{\\text{train}, k=2}$: the empirical risk (resubstitution error) for $k=2$ including the point itself among neighbors with majority vote and the specified tie-breaking.\n\nYour program should produce a single line of output containing a list of three lists, one per dataset, where each inner list is $[E_{j}^{\\text{train}, k=1}, E_{j}^{\\text{LOO}, k=1}, E_{j}^{\\text{train}, k=2}]$. The format must be a comma-separated list enclosed in square brackets, for example: \"[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]\". All values must be floating-point numbers in $[0,1]$ representing average $0$-$1$ loss over the training points for the specified evaluation setting.", "solution": "The problem statement is evaluated for validity.\n\n### Step 1: Extract Givens\n- **Dataset**: A finite training dataset of $n$ observations, each being a pair $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ is a feature vector and $y_i \\in \\{0,1\\}$ is a class label, for $i = 1, \\dots, n$.\n- **Distance Metric**: Euclidean distance, $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$.\n- **Classifier Rule**: The $k$-nearest neighbors ($k$-NN) rule selects the $k$ training observations with the smallest distances to a query point.\n- **Error Metrics**:\n    - **Empirical risk (resubstitution error)** under $0$-$1$ loss is the average number of misclassifications when each training point is predicted using the classifier trained on the full training set.\n    - **Leave-one-out cross-validation (LOOCV) error** is the average $0$-$1$ loss where each training point is predicted using the classifier trained on the set of all other points.\n- **Tie-Breaking Rules**:\n    - **Distances**: Order neighbors by ascending distance. If distances are tied, the point itself is prioritized if present. Any remaining ties are broken by ascending index $i$.\n    - **Majority Vote**: For classification with $k \\ge 2$, if there is a tie in class counts among the $k$ neighbors, the smallest class label (i.e., class $0$) is selected.\n- **Tasks**:\n    - **Task A**: Confirm that the empirical risk for $k=1$ is $0$. Compute the LOOCV error for $k=1$ and the empirical risk for $k=2$ for three given datasets.\n    - **Task B**: Analyze the numerical results with respect to model selection.\n- **Datasets**: Three datasets are provided, each with $d=2$ and $y_i \\in \\{0,1\\}$.\n    - Dataset $1$: $X_1 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$.\n    - Dataset $2$: $X_2 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$.\n    - Dataset $3$: $X_3 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$, $y_3 = [0, 0, 1, 1, 0]$.\n- **Computations Required**: For each dataset $j \\in \\{1,2,3\\}$, compute $E_{j}^{\\text{train}, k=1}$, $E_{j}^{\\text{LOO}, k=1}$, and $E_{j}^{\\text{train}, k=2}$.\n- **Output Format**: A list of three lists: `[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on fundamental and standard concepts in statistical learning and pattern recognition, namely the $k$-NN algorithm, Euclidean distance, and standard methods for error estimation (resubstitution and cross-validation). All definitions are correct and standard.\n- **Well-Posed**: The problem is well-posed. It provides all necessary data, explicit algorithms, and deterministic tie-breaking rules, ensuring that a unique solution exists and is computable. The tasks are specific and quantifiable.\n- **Objective**: The problem is stated in precise, objective, and formal mathematical language, free from subjectivity or ambiguity.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a well-defined computational and analytical exercise in the field of statistical learning.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\n---\n\n### Task A: Confirmation and Computation\n\n**Part 1: Confirmation of Zero Empirical Risk for $k=1$**\n\nThe empirical risk, or resubstitution error, for a classifier is calculated by applying the classifier to each point $\\mathbf{x}_i$ in the training set and measuring the prediction error against the known label $y_i$. For the $k=1$ nearest neighbor classifier, we must find the single closest point to $\\mathbf{x}_i$ within the full training set $\\{(\\mathbf{x}_j, y_j)\\}_{j=1}^n$.\n\nBy definition, the Euclidean distance of a point to itself is $d(\\mathbf{x}_i, \\mathbf{x}_i) = 0$. For any other distinct point $\\mathbf{x}_j$ (where $j \\ne i$), the distance $d(\\mathbf{x}_i, \\mathbf{x}_j)$ must be greater than $0$. Consequently, the nearest neighbor to any training point $\\mathbf{x}_i$ is always the point $\\mathbf{x}_i$ itself. The problem's tie-breaking rules explicitly state to prioritize the point itself if present, reinforcing this conclusion.\n\nThe $1$-NN prediction for the label of $\\mathbf{x}_i$, denoted $\\hat{y}_i$, is thus the label of its nearest neighbor, which is $y_i$. The $0$-$1$ loss for this prediction is $L_{0-1}(\\hat{y}_i, y_i) = L_{0-1}(y_i, y_i) = 0$. Since the prediction is correct for every point in the training set, the total number of errors is $0$. The empirical risk, being the average loss, is therefore $E^{\\text{train}, k=1} = \\frac{1}{n} \\sum_{i=1}^n 0 = 0$. This holds universally for any dataset where all training points are unique. Therefore, for all datasets $j \\in \\{1,2,3\\}$, $E_{j}^{\\text{train}, k=1} = 0$.\n\n**Part 2: Numerical Computations**\n\nWe now compute $E_{j}^{\\text{LOO}, k=1}$ and $E_{j}^{\\text{train}, k=2}$ for each dataset.\n\n**Dataset 1**: $X_1 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$, $n=4$.\n- **$E_{1}^{\\text{LOO}, k=1}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Nearest point in $\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$ is $\\mathbf{x}_2$ (distance $1.0$). Prediction $\\hat{y}_1=y_2=0$. Correct.\n    - For $\\mathbf{x}_2=(0.0, 1.0)$, $y_2=0$: Nearest point in $\\{\\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4\\}$ is $\\mathbf{x}_1$ (distance $1.0$). Prediction $\\hat{y}_2=y_1=0$. Correct.\n    - For $\\mathbf{x}_3=(5.0, 5.0)$, $y_3=1$: Nearest point in $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\}$ is $\\mathbf{x}_4$ (distance $1.0$). Prediction $\\hat{y}_3=y_4=1$. Correct.\n    - For $\\mathbf{x}_4=(5.0, 6.0)$, $y_4=1$: Nearest point in $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\}$ is $\\mathbf{x}_3$ (distance $1.0$). Prediction $\\hat{y}_4=y_3=1$. Correct.\n    Total errors: $0$. $E_{1}^{\\text{LOO}, k=1} = 0/4 = 0.0$.\n- **$E_{1}^{\\text{train}, k=2}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Neighbors are itself and $\\mathbf{x}_2$. Labels are $\\{0, 0\\}$. Majority vote is $0$. Correct.\n    - For $\\mathbf{x}_2=(0.0, 1.0)$, $y_2=0$: Neighbors are itself and $\\mathbf{x}_1$. Labels are $\\{0, 0\\}$. Majority vote is $0$. Correct.\n    - For $\\mathbf{x}_3=(5.0, 5.0)$, $y_3=1$: Neighbors are itself and $\\mathbf{x}_4$. Labels are $\\{1, 1\\}$. Majority vote is $1$. Correct.\n    - For $\\mathbf{x}_4=(5.0, 6.0)$, $y_4=1$: Neighbors are itself and $\\mathbf{x}_3$. Labels are $\\{1, 1\\}$. Majority vote is $1$. Correct.\n    Total errors: $0$. $E_{1}^{\\text{train}, k=2} = 0/4 = 0.0$.\n\nResult for Dataset 1: $[0.0, 0.0, 0.0]$.\n\n**Dataset 2**: $X_2 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$, $n=3$.\n- **$E_{2}^{\\text{LOO}, k=1}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Distances to $\\mathbf{x}_2, \\mathbf{x}_3$ are $0.1, 0.05$. Nearest is $\\mathbf{x}_3$. Prediction $\\hat{y}_1=y_3=1$. Error.\n    - For $\\mathbf{x}_2=(0.0, 0.1)$, $y_2=0$: Distances to $\\mathbf{x}_1, \\mathbf{x}_3$ are $0.1, \\sqrt{0.05^2+0.1^2} \\approx 0.112$. Nearest is $\\mathbf{x}_1$. Prediction $\\hat{y}_2=y_1=0$. Correct.\n    - For $\\mathbf{x}_3=(0.05, 0.0)$, $y_3=1$: Distances to $\\mathbf{x}_1, \\mathbf{x}_2$ are $0.05, \\sqrt{0.05^2+0.1^2} \\approx 0.112$. Nearest is $\\mathbf{x}_1$. Prediction $\\hat{y}_3=y_1=0$. Error.\n    Total errors: $2$. $E_{2}^{\\text{LOO}, k=1} = 2/3$.\n- **$E_{2}^{\\text{train}, k=2}$**:\n    - For $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_3$ (label $1$). Tie. Prediction is $0$. Correct.\n    - For $\\mathbf{x}_2=(0.0, 0.1)$, $y_2=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_1$ (label $0$). Majority vote is $0$. Correct.\n    - For $\\mathbf{x}_3=(0.05, 0.0)$, $y_3=1$: Neighbors are itself (label $1$) and $\\mathbf{x}_1$ (label $0$). Tie. Prediction is $0$. Error.\n    Total errors: $1$. $E_{2}^{\\text{train}, k=2} = 1/3$.\n\nResult for Dataset 2: $[0.0, 2/3, 1/3]$.\n\n**Dataset 3**: $X_3 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$, $y_3=[0, 0, 1, 1, 0]$, $n=5$.\n- **$E_{3}^{\\text{LOO}, k=1}$**:\n    - For $\\mathbf{x}_1=(0,0), y_1=0$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{1.1^2+1^2} \\approx 1.487$). Pred $\\hat{y}_1=y_5=0$. Correct.\n    - For $\\mathbf{x}_2=(0,2), y_2=0$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{1.1^2+(-1)^2} \\approx 1.487$). Pred $\\hat{y}_2=y_5=0$. Correct.\n    - For $\\mathbf{x}_3=(2,0), y_3=1$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{(-0.9)^2+1^2} \\approx 1.345$). Pred $\\hat{y}_3=y_5=0$. Error.\n    - For $\\mathbf{x}_4=(2,2), y_4=1$: Nearest is $\\mathbf{x}_5$ (dist $\\sqrt{(-0.9)^2+(-1.2)^2} = 1.5$). Pred $\\hat{y}_4=y_5=0$. Error.\n    - For $\\mathbf{x}_5=(1.1,1), y_5=0$: Nearest is $\\mathbf{x}_3$ (dist $\\approx 1.345$). Pred $\\hat{y}_5=y_3=1$. Error.\n    Total errors: $3$. $E_{3}^{\\text{LOO}, k=1} = 3/5 = 0.6$.\n- **$E_{3}^{\\text{train}, k=2}$**:\n    - For $\\mathbf{x}_1=(0,0), y_1=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_5$ (label $0$). Majority vote $0$. Correct.\n    - For $\\mathbf{x}_2=(0,2), y_2=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_5$ (label $0$). Majority vote $0$. Correct.\n    - For $\\mathbf{x}_3=(2,0), y_3=1$: Neighbors are itself (label $1$) and $\\mathbf{x}_5$ (label $0$). Tie. Pred $0$. Error.\n    - For $\\mathbf{x}_4=(2,2), y_4=1$: Neighbors are itself (label $1$) and $\\mathbf{x}_5$ (label $0$). Tie. Pred $0$. Error.\n    - For $\\mathbf{x}_5=(1.1,1), y_5=0$: Neighbors are itself (label $0$) and $\\mathbf{x}_3$ (label $1$). Tie. Pred $0$. Correct.\n    Total errors: $2$. $E_{3}^{\\text{train}, k=2} = 2/5 = 0.4$.\n\nResult for Dataset 3: $[0.0, 0.6, 0.4]$.\n\n### Task B: Analysis of Implications for Model Selection\n\nThe computed error rates are:\n- Dataset $1$: $[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.0, 0.0]$\n- Dataset $2$: $[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.667, 0.333]$\n- Dataset $3$: $[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.6, 0.4]$\n\nThese results reveal critical properties of the error estimation methods and their implications for model selection, specifically for choosing the hyperparameter $k$.\n\n1.  **Resubstitution Error is Overly Optimistic and a Poor Metric**: As confirmed analytically and shown numerically, the resubstitution error for $k=1$, $E^{\\text{train}, k=1}$, is always $0$. If a practitioner were to select $k$ by minimizing the resubstitution error, they would invariably choose $k=1$, as it achieves a perfect score. This choice corresponds to a model that has zero training error because it has memorized the training data. This phenomenon is a classic example of overfitting. The model has high variance and is sensitive to every single data point, including noise, but its training error gives a misleadingly optimistic assessment of its performance on new, unseen data.\n\n2.  **Leave-One-Out Cross-Validation Provides a More Realistic Assessment**: The leave-one-out error, $E^{\\text{LOO}, k=1}$, gives a more honest (less biased) estimate of the generalization error. For Dataset $1$, where classes are perfectly separated, LOOCV correctly identifies that a $1$-NN classifier performs flawlessly ($E_{1}^{\\text{LOO}, k=1}=0.0$). However, for Datasets $2$ and $3$, which feature class overlap and a noisy point respectively, the LOOCV error for $k=1$ is substantial ($0.667$ and $0.6$). This high error correctly signals that the $k=1$ model is not robust and overfits the training data. In noisy regions, a point's single nearest neighbor is likely to be from the wrong class, causing LOOCV to report high error.\n\n3.  **Comparing Model Complexities**: Choosing $k$ involves a bias-variance trade-off. A small $k$ (like $k=1$) results in a a low-bias, high-variance model with a very complex decision boundary. A larger $k$ increases bias but decreases variance, resulting in a smoother, less complex decision boundary. The severe performance degradation of $k=1$ under LOOCV in Datasets $2$ and $3$ indicates that its high variance leads to poor generalization.\n\n4.  **Interpretation of $E^{\\text{train}, k=2}$**: The resubstitution error for $k=2$ is non-zero for the noisy datasets. This is because the prediction is an average over two points. For instance, in Dataset $3$, when classifying point $\\mathbf{x}_3$ (label $1$), its neighbors are itself (label $1$) and the noisy point $\\mathbf{x}_5$ (label $0$). The resulting tie is broken by choosing label $0$, causing a misclassification even on the training data. While $E^{\\text{train}, k=2}$ is still an optimistically biased estimate, it is more informative than the vacuous $E^{\\text{train}, k=1}=0$. The comparison required by the problem, $E^{\\text{LOO}, k=1}$ versus $E^{\\text{train}, k=2}$, is not a methodologically sound way to choose between $k=1$ and $k=2$, as it compares a nearly unbiased error estimate for one model with a biased estimate for another. A proper comparison would be between $E^{\\text{LOO}, k=1}$ and $E^{\\text{LOO}, k=2}$. Nonetheless, the results ($0.667$ vs $0.333$ and $0.6$ vs $0.4$) illustrate a crucial point: an unstable model ($k=1$) may have a very high generalization error, potentially even higher than the (optimistic) training error of a more stable model ($k=2$).\n\nIn conclusion, for model selection in $k$-NN, resubstitution error is a flawed metric, particularly for small $k$. It systematically favors models that overfit. Cross-validation methods like LOOCV provide far more reliable estimates of a model's ability to generalize, and are essential for tuning hyperparameters like $k$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes k-NN error rates for three datasets as specified in the problem.\n    \"\"\"\n    datasets = [\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [5.0, 5.0], [5.0, 6.0]]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.1], [0.05, 0.0]]),\n            np.array([0, 0, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 2.0], [2.0, 0.0], [2.0, 2.0], [1.1, 1.0]]),\n            np.array([0, 0, 1, 1, 0])\n        )\n    ]\n\n    all_results = []\n\n    for X, y in datasets:\n        n, d = X.shape\n        \n        # Task 1: Empirical risk for k=1 (E_train, k=1)\n        # This is analytically guaranteed to be 0, as each point is its own nearest neighbor.\n        e_train_k1 = 0.0\n        \n        # Task 2: Leave-one-out error for k=1 (E_LOO, k=1)\n        loo_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            min_dist = np.inf\n            best_idx = -1\n            \n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                \n                if dist < min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j < best_idx:\n                        best_idx = j\n            \n            y_pred = y[best_idx]\n            if y_pred != y_true:\n                loo_errors += 1\n        \n        e_loo_k1 = loo_errors / n\n\n        # Task 3: Empirical risk for k=2 (E_train, k=2)\n        train_k2_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            # The first neighbor is the point itself.\n            neighbor1_label = y_true\n            \n            # Find the second neighbor (closest among other points).\n            min_dist = np.inf\n            best_idx = -1\n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                if dist < min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j < best_idx:\n                        best_idx = j\n            \n            neighbor2_label = y[best_idx]\n            \n            # Majority vote with tie-breaking\n            if neighbor1_label == neighbor2_label:\n                y_pred = neighbor1_label\n            else:  # Tie in votes (one for each class)\n                y_pred = 0  # Select smallest class label\n                \n            if y_pred != y_true:\n                train_k2_errors += 1\n        \n        e_train_k2 = train_k2_errors / n\n        \n        all_results.append([e_train_k1, e_loo_k1, e_train_k2])\n\n    # Format the final output string as specified.\n    outer_list_str = []\n    for res in all_results:\n        inner_list_str = f\"[{','.join(map(str, res))}]\"\n        outer_list_str.append(inner_list_str)\n    \n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```", "id": "3135589"}, {"introduction": "Standard classification algorithms often treat all errors as equal, but in many real-world applications, some mistakes are far more costly than others. This practice [@problem_id:3135604] challenges you to adapt the k-NN algorithm for such scenarios by deriving and implementing a cost-sensitive decision rule. By weighting neighbor votes according to class-specific misclassification costs, you will learn to align your model with the true objective of minimizing application-specific risk.", "problem": "You are given a binary and multi-class classification setting in which a cost is incurred for misclassifying an example. Consider a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ and a class label $y \\in \\mathcal{Y} = \\{0,1,\\dots, K-1\\}$. Let the misclassification loss be class-dependent: predicting $\\hat{y} \\neq y$ incurs a cost $C(y)$, while correct classification incurs zero cost. Assume $C(y) \\ge 0$ for all $y \\in \\mathcal{Y}$ and that $C(y)$ is known. The goal is to design a cost-sensitive $k$-Nearest Neighbors (k-NN) classifier that aligns with minimizing expected conditional risk and to test it on specified scenarios.\n\nTask:\n1) Starting from the fundamental definitions of classification risk and loss, derive the decision condition that minimizes the expected conditional risk at a point $\\mathbf{x}$ under the class-dependent misclassification loss described above. Your derivation should begin from the definition of expected conditional risk $R(\\hat{y} \\mid \\mathbf{x}) = \\mathbb{E}[L(\\hat{y}, Y) \\mid \\mathbf{x}]$ and the given loss structure, without using any pre-derived shortcut formulas.\n2) Implement a $k$-Nearest Neighbors classifier that is cost-sensitive in the sense that it is consistent with the decision condition you derived in part ($1$). Use Euclidean distance in $\\mathbb{R}^d$. Assume class labels are integers starting at $0$, and that the cost vector $C$ is provided as a real-valued array with $C(j)$ in position $j$. If multiple classes tie under the final decision score, break ties by choosing the smallest class label (i.e., the smallest integer).\n3) Apply your implementation to the following independent test cases. For each case, compute the predicted class for the single query point. All points are in $\\mathbb{R}^2$ (unitless), and costs are unitless:\n- Case $1$ (baseline, equal costs):\n  - Training inputs: (($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - Training labels: ($0$, $0$, $1$, $1$)\n  - Query: ($0.1$, $0.1$)\n  - $k = 3$\n  - Costs $C$: ($1.0$, $1.0$)\n- Case $2$ (skewed costs flip the decision):\n  - Training inputs: (($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - Training labels: ($0$, $0$, $1$, $1$)\n  - Query: ($0.1$, $0.1$)\n  - $k = 3$\n  - Costs $C$: ($1.0$, $4.0$)\n- Case $3$ (multi-class, three classes with distinct costs):\n  - Training inputs: (($0.0$, $0.0$), ($2.0$, $0.0$), ($0.0$, $2.0$))\n  - Training labels: ($0$, $1$, $2$)\n  - Query: ($0.9$, $0.9$)\n  - $k = 3$\n  - Costs $C$: ($1.0$, $2.0$, $3.0$)\n- Case $4$ (boundary case $k = 1$):\n  - Training inputs: (($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - Training labels: ($0$, $0$, $1$, $1$)\n  - Query: ($0.05$, $0.05$)\n  - $k = 1$\n  - Costs $C$: ($1.0$, $100.0$)\n- Case $5$ (even-$k$-like tie behavior reproduced with $k = 3$ via weighted tie):\n  - Training inputs: (($0.0$, $0.0$), ($1.0$, $0.0$), ($2.0$, $0.0$))\n  - Training labels: ($0$, $1$, $1$)\n  - Query: ($0.9$, $0.0$)\n  - $k = 3$\n  - Costs $C$: ($2.0$, $1.0$)\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, print the list of predicted classes for Cases $1$ through $5$ in order, e.g., \"[$a_1$, $a_2$, $a_3$, $a_4$, $a_5$]\" where each $a_i$ is an integer prediction for Case $i$.", "solution": "The problem is valid as it is scientifically grounded in statistical decision theory, well-posed with a complete and consistent setup, and computationally feasible. We will proceed with a full solution.\n\nThe solution is presented in two parts. First, we derive the optimal decision rule that minimizes the expected conditional risk under the specified class-dependent misclassification cost. Second, we formulate a k-Nearest Neighbors (k-NN) classifier that implements this decision rule and apply it to the given test cases.\n\n### Part 1: Derivation of the Optimal Decision Rule\n\nThe objective is to find a decision function $\\hat{y}(\\mathbf{x})$ that, for any given feature vector $\\mathbf{x}$, chooses a class label $\\hat{y}$ from the set $\\mathcal{Y} = \\{0, 1, \\dots, K-1\\}$ to minimize the expected conditional risk.\n\nThe expected conditional risk, $R(\\hat{y} \\mid \\mathbf{x})$, of predicting class $\\hat{y}$ when the true feature vector is $\\mathbf{x}$, is defined as the expectation of the loss function $L(\\hat{y}, Y)$ over the conditional probability distribution of the true class $Y$, given $\\mathbf{X} = \\mathbf{x}$.\nLet $p(j \\mid \\mathbf{x}) = P(Y=j \\mid \\mathbf{X}=\\mathbf{x})$ be the true conditional probability of class $j$. The risk is:\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = \\mathbb{E}[L(\\hat{y}, Y) \\mid \\mathbf{X}=\\mathbf{x}] = \\sum_{j=0}^{K-1} L(\\hat{y}, j) p(j \\mid \\mathbf{x})\n$$\nThe problem specifies a class-dependent misclassification loss:\n$$\nL(\\hat{y}, j) =\n\\begin{cases}\n0 & \\text{if } \\hat{y} = j \\\\\nC(j) & \\text{if } \\hat{y} \\neq j\n\\end{cases}\n$$\nwhere $C(j) \\ge 0$ is the cost of misclassifying an instance that truly belongs to class $j$.\n\nSubstituting the loss function into the risk equation, we can separate the term for the case where the prediction $\\hat{y}$ is correct ($j = \\hat{y}$) from the terms where it is incorrect ($j \\neq \\hat{y}$):\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = L(\\hat{y}, \\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} L(\\hat{y}, j) p(j \\mid \\mathbf{x})\n$$\nSince $L(\\hat{y}, \\hat{y}) = 0$ and $L(\\hat{y}, j) = C(j)$ for $j \\neq \\hat{y}$, this simplifies to:\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = 0 \\cdot p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x}) = \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\nThe Bayes-optimal classifier is the one that selects the class $\\hat{y}^*$ that minimizes this risk:\n$$\n\\hat{y}^* = \\arg\\min_{\\hat{y} \\in \\mathcal{Y}} R(\\hat{y} \\mid \\mathbf{x}) = \\arg\\min_{\\hat{y} \\in \\mathcal{Y}} \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\nTo make this minimization problem more tractable, we can rewrite the sum. The total expected cost of misclassification across all possible true classes is $\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x})$. We can express this sum as:\n$$\n\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x}) = C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\nRearranging to isolate the term we want to minimize:\n$$\n\\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x}) = \\left( \\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x}) \\right) - C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x})\n$$\nThe term in the parentheses, $\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x})$, is the expected total cost and does not depend on our choice of prediction $\\hat{y}$. Therefore, minimizing the left-hand side is equivalent to maximizing the term being subtracted, $C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x})$.\n\nThus, the optimal decision rule is to choose the class that maximizes the product of its misclassification cost and its conditional probability:\n$$\n\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) \\right\\}\n$$\nNote that if costs are uniform, i.e., $C(y) = c$ for all $y$ for some constant $c > 0$, this rule simplifies to $\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} p(\\hat{y} \\mid \\mathbf{x})$, which is the standard Bayes classifier rule that minimizes the probability of error (equivalent to $0$-$1$ loss).\n\n### Part 2: Cost-Sensitive k-Nearest Neighbors Classifier\n\nThe k-NN algorithm is a non-parametric method that estimates the conditional probabilities $p(j \\mid \\mathbf{x})$ from the training data. For a given query point $\\mathbf{x}$, we first identify its neighborhood, $N_k(\\mathbf{x})$, which consists of the $k$ training samples closest to $\\mathbf{x}$ according to some distance metric (here, Euclidean distance).\n\nThe k-NN estimate for the conditional probability $p(j \\mid \\mathbf{x})$ is the fraction of neighbors in $N_k(\\mathbf{x})$ that belong to class $j$. Let $n_j(\\mathbf{x})$ be the number of data points in $N_k(\\mathbf{x})$ with class label $j$. Then, the estimate $\\hat{p}(j \\mid \\mathbf{x})$ is:\n$$\n\\hat{p}(j \\mid \\mathbf{x}) = \\frac{n_j(\\mathbf{x})}{k}\n$$\nwhere $\\sum_{j=0}^{K-1} n_j(\\mathbf{x}) = k$.\n\nSubstituting this estimate into our derived optimal decision rule:\n$$\n\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) \\hat{p}(\\hat{y} \\mid \\mathbf{x}) \\right\\} = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) \\frac{n_{\\hat{y}}(\\mathbf{x})}{k} \\right\\}\n$$\nSince $k$ is a positive constant with respect to the maximization over $\\hat{y}$, we can remove it from the expression. The final decision rule for our cost-sensitive k-NN classifier is to compute a score $S(j)$ for each class $j$ and choose the class with the highest score:\n$$\n\\hat{y}^* = \\arg\\max_{j \\in \\mathcal{Y}} S(j) \\quad \\text{where} \\quad S(j) = C(j) n_j(\\mathbf{x})\n$$\nIf multiple classes have the same maximal score, the tie is broken by choosing the class with the smallest integer label.\n\nThe algorithm proceeds as follows:\n1. For a query point $\\mathbf{x}_{query}$, compute the Euclidean distance to all training points $\\mathbf{x}_i$.\n2. Identify the $k$ training points with the smallest distances. These are the nearest neighbors.\n3. For each class $j \\in \\{0, 1, \\dots, K-1\\}$, count the number of neighbors, $n_j$, belonging to that class.\n4. For each class $j$, calculate the score $S(j) = C(j) \\cdot n_j$.\n5. The predicted class is the one with the highest score. Ties are broken by selecting the smallest class index.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cost_sensitive_knn_predict(X_train, y_train, x_query, k, costs):\n    \"\"\"\n    Predicts the class for a single query point using a cost-sensitive k-NN classifier.\n\n    Args:\n        X_train (np.ndarray): Training data feature vectors, shape (n_samples, n_features).\n        y_train (np.ndarray): Training data labels, shape (n_samples,).\n        x_query (np.ndarray): The query point feature vector, shape (n_features,).\n        k (int): The number of neighbors to consider.\n        costs (np.ndarray): The cost vector C, where costs[j] is C(j).\n\n    Returns:\n        int: The predicted class label.\n    \"\"\"\n    # 1. Compute Euclidean distances from the query point to all training points.\n    distances = np.linalg.norm(X_train - x_query, axis=1)\n\n    # 2. Find the indices of the k-nearest neighbors.\n    # np.argsort provides a stable sort, which is sufficient for breaking\n    # distance ties implicitly.\n    neighbor_indices = np.argsort(distances)[:k]\n\n    # Get the labels of the k-nearest neighbors.\n    neighbor_labels = y_train[neighbor_indices]\n\n    # 3. For each class, count the number of neighbors belonging to it.\n    num_classes = len(costs)\n    # np.bincount is highly efficient for this counting task.\n    # It requires non-negative integer labels. We ensure there's a bin for every class\n    # even if a class is not present in the neighbors, by setting minlength.\n    n_j = np.bincount(neighbor_labels, minlength=num_classes)\n\n    # 4. Calculate the score S(j) = C(j) * n_j for each class.\n    scores = costs * n_j\n    \n    # 5. The prediction is the class with the maximum score.\n    # np.argmax breaks ties by returning the index of the first occurrence\n    # of the maximum value, which corresponds to the smallest class label\n    # as required by the tie-breaking rule.\n    prediction = np.argmax(scores)\n    \n    return int(prediction)\n\ndef solve():\n    \"\"\"\n    Sets up and solves the 5 test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline, equal costs)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.1, 0.1]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 1.0]),\n        },\n        # Case 2 (skewed costs flip the decision)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.1, 0.1]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 4.0]),\n        },\n        # Case 3 (multi-class, three classes with distinct costs)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [2.0, 0.0], [0.0, 2.0]]),\n            \"y_train\": np.array([0, 1, 2]),\n            \"query\": np.array([0.9, 0.9]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Case 4 (boundary case k = 1)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.05, 0.05]),\n            \"k\": 1,\n            \"costs\": np.array([1.0, 100.0]),\n        },\n        # Case 5 (even-k-like tie behavior reproduced with k = 3 via weighted tie)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [1.0, 0.0], [2.0, 0.0]]),\n            \"y_train\": np.array([0, 1, 1]),\n            \"query\": np.array([0.9, 0.0]),\n            \"k\": 3,\n            \"costs\": np.array([2.0, 1.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        prediction = cost_sensitive_knn_predict(\n            X_train=case[\"X_train\"],\n            y_train=case[\"y_train\"],\n            x_query=case[\"query\"],\n            k=case[\"k\"],\n            costs=case[\"costs\"],\n        )\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3135604"}, {"introduction": "Choosing the hyperparameter $k$ is critical to the performance of a k-NN classifier, and cross-validation is the standard empirical tool for this task. This advanced practice [@problem_id:3135635] introduces a powerful alternative rooted in statistical learning theory, where $k$ is selected by minimizing an upper bound on the classification error derived from concentration inequalities. You will implement a procedure based on the empirical Bernstein inequality to quantify and manage uncertainty in the k-NN probability estimate, providing a theoretically-grounded approach to model selection.", "problem": "You are given a binary classification task and asked to select the number of neighbors $k$ for the $k$-nearest neighbors classifier using a theoretically motivated procedure and to compare it to a cross-validation choice. The theoretical procedure must rely on an empirical variance estimate of neighbor labels and a concentration inequality that provides a high-probability bound on the error of the empirical mean. Assume the following standard setup.\n\nFundamental base:\n- Let $(X,Y)$ be a random pair with $X \\in \\mathbb{R}^d$ and $Y \\in \\{0,1\\}$.\n- For a fixed input $x$, the conditional class probability is $p(x) = \\mathbb{E}[Y \\mid X = x]$.\n- The $k$-nearest neighbors rule estimates $p(x)$ by the average of the labels of the $k$ nearest training points to $x$ and classifies by comparing this estimate to $0.5$.\n- Use the empirical Bernstein inequality for bounded random variables, which is a well-tested concentration bound. For independent samples $Z_1, \\dots, Z_k \\in [0,1]$ with empirical mean $\\hat{\\mu}$ and unbiased sample variance $s^2$, it states that with probability at least $1 - \\delta$, \n$$\n\\left|\\hat{\\mu} - \\mu\\right| \\leq \\sqrt{\\frac{2 s^2 \\ln\\left(\\frac{3}{\\delta}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta}\\right)}{k - 1},\n$$\nwhere $\\mu = \\mathbb{E}[Z_i]$. In our setting, for each training point $x_i$, let $Z_j$ be the neighbor labels and set $\\delta_i = \\delta/n$ to allow a union bound across all $n$ training points.\n\nYour program must implement the following precise procedure.\n\n1. Data generation:\n   - For each test case, generate $n$ training points in dimension $d$ as follows. Let $n_0 = \\lfloor n/2 \\rfloor$ and $n_1 = n - n_0$ be class sizes. Draw class $0$ points independently from a multivariate normal $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I}_d)$ and class $1$ points independently from $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I}_d)$. Assign labels $0$ and $1$ correspondingly. Use Euclidean distance for nearest neighbor computations. The random number generator must be initialized with the specified seed for each case, and then used to generate the training data deterministically.\n   - No separate test set is required; all evaluation is performed via leave-one-out on the training set.\n\n2. Candidate $k$ set:\n   - For each test case, evaluate all candidate $k$ values given in the case. For all logic below, when $k = 1$ treat the second term in the empirical Bernstein bound, $\\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}$, as $+\\infty$ so that the bound becomes vacuous for $k=1$.\n\n3. Empirical Bernstein bound-based selection:\n   - For each training point $x_i$, compute its $k$ nearest neighbors among the other training points (exclude $x_i$ itself), and let $\\hat{p}_i(k)$ be the empirical mean of the $k$ neighbor labels. Let $s_i^2(k)$ be the unbiased sample variance of those labels,\n     $$\n     s_i^2(k) = \\frac{1}{k - 1} \\sum_{j=1}^k \\left(y_{i,j} - \\hat{p}_i(k)\\right)^2,\n     $$\n     where $y_{i,j}$ are the neighbor labels. Define the per-point bound half-width\n     $$\n     B_i(k) = \\sqrt{\\frac{2 s_i^2(k) \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}, \\quad \\delta_i = \\frac{\\delta}{n}.\n     $$\n   - Define the ambiguity indicator\n     $$\n     A_i(k) = \\begin{cases}\n     1 & \\text{if } \\left|\\hat{p}_i(k) - \\frac{1}{2}\\right| \\leq B_i(k), \\\\\n     0 & \\text{otherwise}.\n     \\end{cases}\n     $$\n     Intuitively, $A_i(k) = 1$ signals that the empirical estimate $\\hat{p}_i(k)$ is not reliably on one side of $\\frac{1}{2}$ within the bound width. Define the bound-based upper bound on misclassification rate\n     $$\n     U(k) = \\frac{1}{n} \\sum_{i=1}^n A_i(k).\n     $$\n   - Select $k_{\\text{bound}}$ as the $k$ that minimizes $U(k)$. Break ties by choosing the smallest $k$.\n\n4. Cross-validation comparison:\n   - Compute the leave-one-out error\n     $$\n     E_{\\text{LOO}}(k) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i^{(-i)}(k) \\neq y_i \\right\\},\n     $$\n     where $\\hat{y}_i^{(-i)}(k)$ is the predicted label for $x_i$ from the $k$ nearest neighbors excluding $x_i$, with the decision rule\n     $$\n     \\hat{y}_i^{(-i)}(k) = \\begin{cases}\n     1 & \\text{if } \\hat{p}_i(k) \\geq \\frac{1}{2}, \\\\\n     0 & \\text{otherwise}.\n     \\end{cases}\n     $$\n     Select $k_{\\text{CV}}$ as the $k$ that minimizes $E_{\\text{LOO}}(k)$. Break ties by choosing the smallest $k$.\n\n5. Output:\n   - For each test case, output the quadruple $[k_{\\text{bound}}, U(k_{\\text{bound}}), k_{\\text{CV}}, E_{\\text{LOO}}(k_{\\text{CV}})]$.\n   - Express $U(\\cdot)$ and $E_{\\text{LOO}}(\\cdot)$ as decimals in $[0,1]$ rounded to four decimal places.\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case formatted exactly as a bracketed quadruple with no spaces. For example, an output for two cases must look like $[[1,0.0000,3,0.0500],[2,0.1000,2,0.1000]]$.\n\nTest suite:\n- Case $1$: $n = 60$, $d = 2$, $\\boldsymbol{\\mu}_0 = (-1,-1)$, $\\boldsymbol{\\mu}_1 = (1,1)$, $\\sigma = 0.3$, candidate $k$ values $[1,3,5,7,9,11]$, total confidence parameter $\\delta = 0.1$, random seed $0$.\n- Case $2$: $n = 100$, $d = 2$, $\\boldsymbol{\\mu}_0 = (-0.5,0)$, $\\boldsymbol{\\mu}_1 = (0.5,0)$, $\\sigma = 0.6$, candidate $k$ values $[2,4,6,8,10,12,14]$, $\\delta = 0.05$, random seed $1$.\n- Case $3$: $n = 80$, $d = 1$, $\\boldsymbol{\\mu}_0 = (-0.3)$, $\\boldsymbol{\\mu}_1 = (0.3)$, $\\sigma = 0.8$, candidate $k$ values $[1,5,15,25,35]$, $\\delta = 0.2$, random seed $2$.\n\nAll nearest neighbor computations must use Euclidean distance and exclude the point itself (leave-one-out within the training set). Ensure that all computations strictly adhere to the definitions above.", "solution": "The user-provided problem has been analyzed and validated. It is scientifically sound, well-posed, and objective. All necessary information is provided, and the procedural steps are clearly and unambiguously defined. The problem is a substantive task in computational statistics, requiring the implementation of a data generation process and two distinct methods for model selection in the context of the $k$-nearest neighbors algorithm. No flaws were detected.\n\nThe solution proceeds by first generating a synthetic dataset for a binary classification problem from a two-component Gaussian mixture model. Subsequently, two procedures are implemented to select the optimal number of neighbors, $k$, from a given set of candidates. The entire evaluation is performed using a leave-one-out methodology on the generated training data.\n\n### 1. Data Generation and Leave-One-Out Setup\n\nFor each test case, we generate a training set of $n$ points in a $d$-dimensional space, $\\mathbb{R}^d$. The data consists of two classes, labeled $0$ and $1$. The number of points in each class is $n_0 = \\lfloor n/2 \\rfloor$ and $n_1 = n - n_0$, respectively. Points for class $0$ are drawn from a multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I}_d)$, and points for class $1$ are from $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I}_d)$. The parameters $n, d, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\sigma$, and a random seed are specified for each case to ensure deterministic reproducibility.\n\nAll subsequent calculations are based on a leave-one-out (LOO) scheme. For each training point $x_i$ with label $y_i$, all computations are performed using its $k$ nearest neighbors from the set of other training points, $\\{x_j\\}_{j \\neq i}$. The distance metric used is Euclidean distance.\n\n### 2. Method 1: Bound-Based Selection for $k_{\\text{bound}}$\n\nThis method selects $k$ by minimizing an upper bound on the classification error rate, which is derived from a concentration inequality. The core idea is to identify and count \"ambiguous\" classifications. A classification for a point $x_i$ is considered ambiguous if its estimated conditional class probability, $\\hat{p}_i(k)$, is statistically indistinguishable from the decision boundary of $0.5$.\n\nLet $y_{i,1}, \\dots, y_{i,k}$ be the labels of the $k$ nearest neighbors of $x_i$. The local class probability is estimated by their empirical mean:\n$$\n\\hat{p}_i(k) = \\frac{1}{k} \\sum_{j=1}^k y_{i,j}\n$$\nTo quantify the statistical uncertainty in this estimate, we use the empirical Bernstein inequality. This provides a high-probability confidence interval for the true local probability $p(x_i) = \\mathbb{E}[Y \\mid X=x_i]$. With probability at least $1-\\delta_i$, the true mean $\\mu$ (here, $p(x_i)$) is bounded by:\n$$\n|\\hat{p}_i(k) - p(x_i)| \\leq B_i(k)\n$$\nwhere $B_i(k)$ is the bound half-width. To ensure this holds simultaneously for all $n$ training points with total confidence $1-\\delta$, we apply a union bound by setting $\\delta_i = \\delta/n$. The half-width $B_i(k)$ is defined as:\n$$\nB_i(k) = \\sqrt{\\frac{2 s_i^2(k) \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}\n$$\nHere, $s_i^2(k)$ is the unbiased sample variance of the neighbor labels:\n$$\ns_i^2(k) = \\frac{1}{k - 1} \\sum_{j=1}^k \\left(y_{i,j} - \\hat{p}_i(k)\\right)^2\n$$\nA point $x_i$ is declared ambiguous for a given $k$ if its confidence interval for $p(x_i)$ contains the decision boundary $0.5$. This is captured by the ambiguity indicator, $A_i(k)$:\n$$\nA_i(k) = \\begin{cases}\n1 & \\text{if } |\\hat{p}_i(k) - 0.5| \\leq B_i(k) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe value $k$ is chosen to minimize the total fraction of ambiguous points, $U(k)$:\n$$\nU(k) = \\frac{1}{n} \\sum_{i=1}^n A_i(k)\n$$\nThe optimal $k$ under this criterion is $k_{\\text{bound}} = \\arg\\min_k U(k)$, with ties broken by selecting the smallest $k$. For $k=1$, the variance $s_i^2(1)$ is undefined. The problem specifies that the bound is vacuous in this case, meaning $B_i(1) = \\infty$, which implies $A_i(1) = 1$ for all $i$, and thus $U(1)=1$.\n\n### 3. Method 2: Leave-One-Out Cross-Validation for $k_{\\text{CV}}$\n\nThis is a standard empirical method for model selection. It directly estimates the generalization error of the classifier for different values of $k$.\n\nFor each training point $x_i$, a prediction $\\hat{y}_i^{(-i)}(k)$ is made using its $k$ nearest neighbors from the rest of the data. The prediction is based on the majority vote among the neighbors, equivalent to thresholding the estimated probability $\\hat{p}_i(k)$:\n$$\n\\hat{y}_i^{(-i)}(k) = \\begin{cases}\n1 & \\text{if } \\hat{p}_i(k) \\geq 0.5 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe leave-one-out error for a given $k$ is the fraction of points that are misclassified:\n$$\nE_{\\text{LOO}}(k) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i^{(-i)}(k) \\neq y_i \\right\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The optimal $k$ under this criterion is $k_{\\text{CV}} = \\arg\\min_k E_{\\text{LOO}}(k)$, with ties again broken by choosing the smallest $k$. This method selects the $k$ that performs best on the training data itself when evaluated in a leave-one-out fashion.\n\n### 4. Algorithmic Procedure\n\nThe implementation follows these steps for each test case:\n1.  Initialize the pseudo-random number generator with the specified seed and generate the training data $(X, y)$.\n2.  For efficiency, pre-compute the $n \\times n$ matrix of pairwise Euclidean distances between all points in $X$.\n3.  For each candidate value of $k$:\n    a. Initialize accumulators for the total ambiguity score and total LOO errors.\n    b. For each point $x_i$, $i=1, \\dots, n$:\n        i. Identify the indices of its $k$ nearest neighbors using the pre-computed distance matrix, excluding $x_i$ itself.\n        ii. Retrieve the labels of these neighbors and compute $\\hat{p}_i(k)$ and $s_i^2(k)$.\n        iii. Calculate the LOO prediction $\\hat{y}_i^{(-i)}(k)$ and update the LOO error count if $\\hat{y}_i^{(-i)}(k) \\neq y_i$.\n        iv. Calculate the Bernstein bound half-width $B_i(k)$ (handling the $k=1$ case separately) and determine the ambiguity indicator $A_i(k)$, updating the ambiguity score.\n    c. Compute the final scores $U(k)$ and $E_{\\text{LOO}}(k)$ for the current $k$.\n4.  After evaluating all candidate $k$ values, determine $k_{\\text{bound}}$ by finding the $k$ that minimizes $U(k)$, and $k_{\\text{CV}}$ by finding the $k$ that minimizes $E_{\\text{LOO}}(k)$. Tie-breaking rules are applied as specified.\n5.  Format the final result as the quadruple $[k_{\\text{bound}}, U(k_{\\text{bound}}), k_{\\text{CV}}, E_{\\text{LOO}}(k_{\\text{CV}})]$, with the rate values rounded to four decimal places.", "answer": "```python\nimport numpy as np\n\ndef run_one_case(n, d, mu_0, mu_1, sigma, k_values, delta, seed):\n    \"\"\"\n    Executes a single test case according to the problem description.\n\n    Args:\n        n (int): Total number of training points.\n        d (int): Dimension of the feature space.\n        mu_0 (list or float): Mean vector for class 0.\n        mu_1 (list or float): Mean vector for class 1.\n        sigma (float): Standard deviation for the isotropic covariance.\n        k_values (list): List of candidate k values to evaluate.\n        delta (float): Total confidence parameter for Bernstein bound.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing (k_bound, U(k_bound), k_CV, E_LOO(k_CV)).\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    n_0 = n // 2\n    n_1 = n - n_0\n\n    if d == 1:\n        # np.multivariate_normal expects mean to be 1-D array-like\n        mu_0_arr = [mu_0]\n        mu_1_arr = [mu_1]\n    else:\n        mu_0_arr = mu_0\n        mu_1_arr = mu_1\n\n    cov = (sigma**2) * np.identity(d)\n    X_0 = rng.multivariate_normal(mu_0_arr, cov, size=n_0)\n    X_1 = rng.multivariate_normal(mu_1_arr, cov, size=n_1)\n\n    X = np.vstack((X_0, X_1))\n    if d == 1 and X.ndim > 1 and X.shape[1] != d:\n        X = X.reshape(n, d)\n    y = np.array([0] * n_0 + [1] * n_1)\n\n    # 2. Pre-compute pairwise Euclidean distances\n    sum_sq = np.sum(X**2, axis=1, keepdims=True)\n    dist_sq = sum_sq + sum_sq.T - 2 * np.dot(X, X.T)\n    dist_sq[dist_sq < 0] = 0  # Correct for numerical floating point inaccuracies\n    distances = np.sqrt(dist_sq)\n\n    results_per_k = []\n    delta_i = delta / n\n    log_term = np.log(3 / delta_i)\n\n    # 3. Iterate over candidate k values\n    for k in k_values:\n        total_A_i = 0\n        total_loo_errors = 0\n\n        # 4. Leave-one-out evaluation for each point\n        for i in range(n):\n            dists_i = distances[i, :]\n            sorted_indices = np.argsort(dists_i)\n            neighbor_indices = sorted_indices[1:k + 1]\n            neighbor_labels = y[neighbor_indices]\n\n            p_hat = np.mean(neighbor_labels)\n            \n            # --- Cross-Validation Error Calculation ---\n            y_hat = 1 if p_hat >= 0.5 else 0\n            if y_hat != y[i]:\n                total_loo_errors += 1\n\n            # --- Bernstein Bound-based Ambiguity Calculation ---\n            if k == 1:\n                A_i = 1 # Per problem spec, bound is vacuous, ambiguity is 1.\n            else:\n                s_squared = np.var(neighbor_labels, ddof=1)\n                \n                term1 = np.sqrt(2 * s_squared * log_term / k)\n                term2 = 3 * log_term / (k - 1)\n                B_i = term1 + term2\n                \n                if np.abs(p_hat - 0.5) <= B_i:\n                    A_i = 1\n                else:\n                    A_i = 0\n            \n            total_A_i += A_i\n\n        U_k = total_A_i / n\n        E_LOO_k = total_loo_errors / n\n        results_per_k.append({'k': k, 'U': U_k, 'E_LOO': E_LOO_k})\n\n    # 5. Select best k based on U(k) and E_LOO(k)\n    best_bound_res = sorted(results_per_k, key=lambda x: (x['U'], x['k']))[0]\n    k_bound = best_bound_res['k']\n    U_at_k_bound = best_bound_res['U']\n\n    best_cv_res = sorted(results_per_k, key=lambda x: (x['E_LOO'], x['k']))[0]\n    k_cv = best_cv_res['k']\n    E_LOO_at_k_cv = best_cv_res['E_LOO']\n\n    return k_bound, U_at_k_bound, k_cv, E_LOO_at_k_cv\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'n': 60, 'd': 2, 'mu_0': [-1,-1], 'mu_1': [1,1], 'sigma': 0.3,\n         'k_values': [1,3,5,7,9,11], 'delta': 0.1, 'seed': 0},\n        {'n': 100, 'd': 2, 'mu_0': [-0.5,0], 'mu_1': [0.5,0], 'sigma': 0.6,\n         'k_values': [2,4,6,8,10,12,14], 'delta': 0.05, 'seed': 1},\n        {'n': 80, 'd': 1, 'mu_0': -0.3, 'mu_1': 0.3, 'sigma': 0.8,\n         'k_values': [1,5,15,25,35], 'delta': 0.2, 'seed': 2},\n    ]\n\n    results_str = []\n    for case in test_cases:\n        k_bound, u_val, k_cv, e_val = run_one_case(\n            case['n'], case['d'], case['mu_0'], case['mu_1'], case['sigma'],\n            case['k_values'], case['delta'], case['seed']\n        )\n        formatted_result = f\"[{k_bound},{u_val:.4f},{k_cv},{e_val:.4f}]\"\n        results_str.append(formatted_result)\n\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3135635"}]}