{"hands_on_practices": [{"introduction": "This practice serves as the theoretical cornerstone for understanding Linear Discriminant Analysis. Before tackling complex, real-world scenarios, it is crucial to master the ideal case where the underlying assumptions of LDA hold perfectly. This exercise [@problem_id:3139739] will guide you through the derivation of the Bayes-optimal decision rule for Gaussian-distributed classes, revealing its direct connection to the LDA framework and allowing you to calculate the absolute lower bound on classification error, known as the Bayes error rate.", "problem": "Consider a binary classification problem with class label $Y \\in \\{0,1\\}$, equal class prior probabilities $\\pi_{0}=\\pi_{1}=\\tfrac{1}{2}$, and feature vector $X \\in \\mathbb{R}^{2}$. Assume the class-conditional distributions are multivariate Gaussian with a common covariance matrix,\n$$\nX \\mid Y=k \\sim \\mathcal{N}(\\mu_{k},\\Sigma), \\quad k \\in \\{0,1\\},\n$$\nwhere\n$$\n\\mu_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\mu_{1}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\Sigma=\\begin{pmatrix}1 & 0.5 \\\\ 0.5 & 2\\end{pmatrix}.\n$$\nStarting from the definition of the Bayes decision rule for equal misclassification costs, derive the Bayes classifier for this setting and the exact misclassification probability of the Bayes classifier. Express your final answer in exact closed form using the standard normal cumulative distribution function $\\Phi(\\cdot)$ and the given parameters. Your final answer must be a single closed-form analytic expression.\n\nAdditionally, suppose you form a plug-in linear discriminant analysis (LDA) classifier by estimating $\\mu_{0}$, $\\mu_{1}$, and $\\Sigma$ via maximum likelihood from $n$ labeled samples per class and then classifying with the resulting linear rule. In your solution, explain from first principles whether and to what value the plug-in classifier’s misclassification probability converges as $n \\to \\infty$, and why.\n\nNo numerical approximation or rounding is required; provide the exact expression for the Bayes misclassification probability.", "solution": "The problem asks for the derivation of the Bayes classifier and its misclassification probability for a binary classification problem with Gaussian class-conditional densities, and to analyze the asymptotic behavior of a plug-in Linear Discriminant Analysis (LDA) classifier.\n\nFirst, we derive the Bayes classifier. The Bayes decision rule seeks to minimize the probability of misclassification. For a binary classification problem with class labels $Y \\in \\{0,1\\}$ and equal misclassification costs, the rule assigns a feature vector $x$ to the class with the highest posterior probability. That is, we predict class $1$ if $P(Y=1|X=x) > P(Y=0|X=x)$, and class $0$ otherwise.\n\nUsing Bayes' theorem, the posterior probability is $P(Y=k|X=x) = \\frac{p(x|Y=k)\\pi_k}{p(x)}$, where $p(x|Y=k) = f_k(x)$ is the class-conditional density and $\\pi_k = P(Y=k)$ is the class prior probability. The decision rule becomes: predict class $1$ if $f_1(x)\\pi_1 > f_0(x)\\pi_0$.\n\nThe problem states that the priors are equal, $\\pi_0 = \\pi_1 = \\frac{1}{2}$. Thus, the rule simplifies to comparing the class-conditional densities: predict class $1$ if $f_1(x) > f_0(x)$. This is equivalent to comparing their logarithms, $\\ln f_1(x) > \\ln f_0(x)$, since the logarithm is a strictly increasing function.\n\nThe class-conditional densities are given as multivariate Gaussian: $X | Y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma)$. The log-density for class $k$ is:\n$$\n\\ln f_k(x) = -\\frac{p}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma| - \\frac{1}{2}(x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k)\n$$\nwhere $p=2$ is the dimension of $x$. The decision rule $\\ln f_1(x) > \\ln f_0(x)$ becomes:\n$$\n-\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1) > -\\frac{1}{2}(x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0)\n$$\nMultiplying by $-2$ reverses the inequality:\n$$\n(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1) < (x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0)\n$$\nExpanding the quadratic forms yields:\n$$\nx^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_1 + \\mu_1^T\\Sigma^{-1}\\mu_1 < x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_0 + \\mu_0^T\\Sigma^{-1}\\mu_0\n$$\nThe term $x^T\\Sigma^{-1}x$ cancels. Rearranging the remaining terms to isolate $x$ gives the linear decision boundary:\n$$\n2x^T\\Sigma^{-1}(\\mu_1 - \\mu_0) > \\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0\n$$\nThe Bayes classifier is defined by this rule. It predicts class $1$ if the inequality holds. This is the decision rule for LDA. The left-hand side is a linear function of $x$.\n\nNext, we derive the misclassification probability of this Bayes classifier. Let's define a linear score $S(x) = (\\mu_1-\\mu_0)^T \\Sigma^{-1} x$. The decision rule can be written as: predict class $1$ if $S(x) > \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0)$.\nThe random variable $S(X)$ is a linear projection of the Gaussian random vector $X$. Therefore, its distribution is also Gaussian. We find its conditional distribution for each class.\n\nIf $X \\sim \\mathcal{N}(\\mu_k, \\Sigma)$, then a linear transformation $A X$ has distribution $\\mathcal{N}(A\\mu_k, A\\Sigma A^T)$. Here, the transformation is $A = (\\mu_1-\\mu_0)^T \\Sigma^{-1}$.\nThe conditional mean of $S(X)$ given $Y=k$ is:\n$$\nm_k = E[S(X)|Y=k] = (\\mu_1-\\mu_0)^T \\Sigma^{-1} E[X|Y=k] = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_k\n$$\nThe conditional variance of $S(X)$ is common for both classes because $\\Sigma$ is common:\n$$\n\\sigma_S^2 = \\text{Var}(S(X)|Y=k) = ((\\mu_1-\\mu_0)^T \\Sigma^{-1}) \\Sigma ((\\mu_1-\\mu_0)^T \\Sigma^{-1})^T = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0)\n$$\nThis quantity is the squared Mahalanobis distance between the class means, denoted by $\\Delta^2$. So, $\\sigma_S^2 = \\Delta^2$.\nThe means of the scores are $m_0 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_0$ and $m_1 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_1$. Note that $m_1 - m_0 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0) = \\Delta^2$.\n\nThe decision threshold for $S(X)$ is $T = \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0) = \\frac{1}{2}(m_1+m_0)$. This threshold lies exactly midway between the two conditional means of the score.\n\nThe total misclassification probability, or Bayes error rate, is:\n$P(\\text{error}) = \\pi_0 P(\\text{predict } 1 | Y=0) + \\pi_1 P(\\text{predict } 0 | Y=1)$.\nWith $\\pi_0=\\pi_1=\\frac{1}{2}$:\n$P(\\text{error}) = \\frac{1}{2} P(S(X) > T | Y=0) + \\frac{1}{2} P(S(X) \\le T | Y=1)$.\nGiven $Y=0$, $S(X) \\sim \\mathcal{N}(m_0, \\Delta^2)$. $P(S(X) > T | Y=0) = P\\left(\\frac{S(X)-m_0}{\\Delta} > \\frac{T-m_0}{\\Delta}\\right) = P\\left(Z > \\frac{(m_0+m_1)/2 - m_0}{\\Delta}\\right) = P\\left(Z > \\frac{m_1-m_0}{2\\Delta}\\right) = P\\left(Z > \\frac{\\Delta^2}{2\\Delta}\\right) = P(Z > \\frac{\\Delta}{2}) = 1 - \\Phi(\\frac{\\Delta}{2}) = \\Phi(-\\frac{\\Delta}{2})$, where $Z \\sim \\mathcal{N}(0,1)$.\n\nGiven $Y=1$, $S(X) \\sim \\mathcal{N}(m_1, \\Delta^2)$. $P(S(X) \\le T | Y=1) = P\\left(\\frac{S(X)-m_1}{\\Delta} \\le \\frac{T-m_1}{\\Delta}\\right) = P\\left(Z \\le \\frac{(m_0+m_1)/2 - m_1}{\\Delta}\\right) = P\\left(Z \\le \\frac{m_0-m_1}{2\\Delta}\\right) = P\\left(Z \\le -\\frac{\\Delta^2}{2\\Delta}\\right) = \\Phi(-\\frac{\\Delta}{2})$.\n\nBoth conditional error probabilities are equal. The total misclassification probability is:\n$P(\\text{error}) = \\frac{1}{2}\\Phi(-\\frac{\\Delta}{2}) + \\frac{1}{2}\\Phi(-\\frac{\\Delta}{2}) = \\Phi(-\\frac{\\Delta}{2})$.\n\nNow, we calculate $\\Delta^2$ using the given parameters:\n$$\n\\mu_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\mu_{1}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\Sigma=\\begin{pmatrix}1 & 0.5 \\\\ 0.5 & 2\\end{pmatrix}.\n$$\nThe determinant of $\\Sigma$ is $\\det(\\Sigma) = (1)(2) - (0.5)(0.5) = 2 - 0.25 = 1.75 = \\frac{7}{4}$.\nThe inverse of $\\Sigma$ is:\n$$\n\\Sigma^{-1} = \\frac{1}{7/4} \\begin{pmatrix} 2 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 2 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix} = \\begin{pmatrix} 8/7 & -2/7 \\\\ -2/7 & 4/7 \\end{pmatrix}.\n$$\nThe difference in means is $\\mu_1 - \\mu_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe squared Mahalanobis distance is:\n$$\n\\Delta^2 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0) = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 8/7 & -2/7 \\\\ -2/7 & 4/7 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\n\\Delta^2 = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} (8/7)(1) + (-2/7)(2) \\\\ (-2/7)(1) + (4/7)(2) \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 4/7 \\\\ 6/7 \\end{pmatrix} = (1)(\\frac{4}{7}) + (2)(\\frac{6}{7}) = \\frac{4+12}{7} = \\frac{16}{7}.\n$$\nThe Mahalanobis distance is $\\Delta = \\sqrt{\\frac{16}{7}} = \\frac{4}{\\sqrt{7}}$.\nThe Bayes misclassification probability is therefore:\n$$\nP(\\text{error}) = \\Phi\\left(-\\frac{\\Delta}{2}\\right) = \\Phi\\left(-\\frac{1}{2} \\cdot \\frac{4}{\\sqrt{7}}\\right) = \\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right).\n$$\n\nFinally, regarding the plug-in LDA classifier: this classifier is constructed by first estimating the unknown parameters $\\mu_0$, $\\mu_1$, and $\\Sigma$ from a training set of $n$ samples per class, and then \"plugging\" these estimates into the LDA decision rule derived above. The standard estimators for these parameters are the sample means and the pooled sample covariance matrix.\n$$\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i: y_i=k} x_i, \\quad \\hat{\\Sigma} = \\frac{1}{2n-2} \\sum_{k=0}^{1} \\sum_{i: y_i=k} (x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T\n$$\nAccording to the Law of Large Numbers, these estimators are consistent. As the number of samples $n$ tends to infinity, the estimated parameters converge in probability to their true values:\n$$\n\\hat{\\mu}_k \\xrightarrow{p} \\mu_k \\quad \\text{and} \\quad \\hat{\\Sigma} \\xrightarrow{p} \\Sigma \\quad \\text{as } n \\to \\infty.\n$$\nThe LDA decision boundary is a continuous function of these parameters. By the Continuous Mapping Theorem, the decision function of the plug-in classifier, $\\hat{\\delta}(x)$, converges in probability to the Bayes optimal decision function, $\\delta(x)$.\n$$\n\\hat{\\delta}(x) = x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_1 - \\hat{\\mu}_0) - \\frac{1}{2}(\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 - \\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0) \\xrightarrow{p} \\delta(x) \\quad \\text{as } n \\to \\infty\n$$\nConsequently, the misclassification probability of the plug-in LDA classifier, which is an expectation over the true data distribution, converges to the misclassification probability of the optimal Bayes classifier. This minimal possible error rate is the Bayes error rate.\nTherefore, as $n \\to \\infty$, the misclassification probability of the plug-in LDA classifier converges to the exact Bayes misclassification probability we calculated, which is $\\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right)$.", "answer": "$$\\boxed{\\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right)}$$", "id": "3139739"}, {"introduction": "Having established the theoretical optimum, we now turn to a common practical challenge: data that isn't perfectly clean. The standard estimators for LDA are sensitive to outliers, which can severely distort the decision boundary and degrade performance. This hands-on coding exercise [@problem_id:3139742] provides a concrete demonstration of this vulnerability and empowers you to implement a robust version of LDA that can mitigate the influence of such contaminating points.", "problem": "Consider a binary classification problem in two dimensions with classes indexed by $0$ and $1$. The Bayes decision rule for zero-one loss selects the class label $y \\in \\{0,1\\}$ that maximizes the posterior probability $P(Y=y \\mid \\mathbf{x})$. Assume class-conditional densities are multivariate Gaussian with a common covariance matrix and equal class priors. From this base, derive the linear decision rule that compares two discriminant functions, and implement the corresponding Linear Discriminant Analysis (LDA) classifier for both a standard estimator and a robust estimator.\n\nYour program must do the following, using only the information provided here.\n\n1) Modeling assumptions and estimators:\n- Assume that for each class $k \\in \\{0,1\\}$, the training data $\\mathbf{X}_k \\in \\mathbb{R}^{n_k \\times 2}$ are independently drawn from a Gaussian distribution with class mean $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^2$ and a shared covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$, and that class priors are equal, $P(Y=0) = P(Y=1) = 1/2$.\n- The standard LDA estimator must use the sample mean $\\hat{\\boldsymbol{\\mu}}_k$ for each class $k$, and the pooled covariance estimator $\\hat{\\boldsymbol{\\Sigma}}$ obtained by averaging within-class unbiased sample covariances weighted by $(n_k - 1)$.\n- The robust LDA estimator must use an $\\alpha$-trimmed strategy for robustness: for each class $k$, compute the coordinate-wise median $\\tilde{\\boldsymbol{m}}_k$, compute Euclidean distances of each class sample to $\\tilde{\\boldsymbol{m}}_k$, drop the $\\lceil \\alpha n_k \\rceil$ farthest points, compute the trimmed class mean $\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_k$ and the pooled covariance $\\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})}$ from the trimmed sets. Use $\\alpha = 0.2$. If a covariance matrix is not invertible due to finite-sample effects, add a small positive ridge $\\lambda \\mathbf{I}$ with $\\lambda = 10^{-6}$.\n\n2) Training datasets:\n- Clean training sets (no outlier):\n  - Class $0$: $\\mathbf{X}_0^{(\\mathrm{clean})} = \\{(0,0), (0.2,0.1), (-0.1,-0.2), (0.1,0)\\}$, so $n_0 = 4$.\n  - Class $1$: $\\mathbf{X}_1^{(\\mathrm{clean})} = \\{(2,0), (2.1,0.2), (1.9,-0.1), (2.2,0.05)\\}$, so $n_1 = 4$.\n- Outlier-contaminated dataset for class $0$:\n  - Class $0$ with one outlier: $\\mathbf{X}_0^{(\\mathrm{out})} = \\mathbf{X}_0^{(\\mathrm{clean})} \\cup \\{(10,0)\\}$, so $n_0 = 5$.\n  - Class $1$ remains $\\mathbf{X}_1^{(\\mathrm{clean})}$.\n\n3) Classifiers and prediction rule:\n- Implement the standard LDA classifier using $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$ estimated from the specified training sets.\n- Implement the robust LDA classifier using $(\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_0, \\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_1, \\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})})$ estimated by $\\alpha$-trimming with $\\alpha = 0.2$ as specified.\n- Use the linear discriminant rule derived from the Gaussian shared-covariance model and equal priors to produce predicted labels in $\\{0,1\\}$ for any input $\\mathbf{x} \\in \\mathbb{R}^2$.\n\n4) Test suite and required outputs:\n- Define the query point $\\mathbf{q} = (1.1, 0.0)$.\n- Define the query set $\\mathcal{S} = \\{(0.96,0.0), (1.03,0.0), (1.10,0.0), (1.17,0.0), (1.24,0.0)\\}$.\n- Train three classifiers:\n  - Standard LDA on the clean data $(\\mathbf{X}_0^{(\\mathrm{clean})}, \\mathbf{X}_1^{(\\mathrm{clean})})$.\n  - Standard LDA on the outlier-contaminated data $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$.\n  - Robust LDA on the outlier-contaminated data with $\\alpha = 0.2$ trimming applied separately to each class before estimating means and the pooled covariance.\n- Compute the following six boolean outputs:\n  - $b_1$: the label predicted for $\\mathbf{q}$ by standard LDA trained on clean data equals $1$.\n  - $b_2$: the label predicted for $\\mathbf{q}$ by standard LDA trained on outlier-contaminated data equals $0$.\n  - $b_3$: the label predicted for $\\mathbf{q}$ by robust LDA trained on outlier-contaminated data equals the label predicted by standard LDA on clean data.\n  - $b_4$: the labels predicted by standard LDA and robust LDA on outlier-contaminated data for $\\mathbf{q}$ are different.\n  - $b_5$: there exists at least one point in $\\mathcal{S}$ whose predicted label under standard LDA trained on clean data differs from its predicted label under standard LDA trained on outlier-contaminated data.\n  - $b_6$: for every point in $\\mathcal{S}$, its predicted label under standard LDA trained on clean data equals its predicted label under robust LDA trained on outlier-contaminated data.\n\n5) Final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the six booleans enclosed in square brackets, in the order $[b_1, b_2, b_3, b_4, b_5, b_6]$. For example, a syntactically valid example would be $[True,False,True,True,False,True]$.\n\nNo physical units are involved. Angles are not used. All calculations are purely numerical in $\\mathbb{R}^2$ as specified. The program must be self-contained and must not read any input.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem in the domain of statistical learning, specifically focusing on Linear Discriminant Analysis (LDA) and the effect of outliers on parameter estimation. It requires the implementation of standard and robust estimators and their application to specified datasets.\n\n### Principles and Derivations\n\nThe core of this problem rests on Bayesian decision theory for classification. For a binary classification task with classes $k \\in \\{0, 1\\}$ and a zero-one loss function, the Bayes decision rule assigns a feature vector $\\mathbf{x}$ to the class with the highest posterior probability.\n$$ \\hat{y} = \\arg\\max_{k \\in \\{0,1\\}} P(Y=k \\mid \\mathbf{x}) $$\nUsing Bayes' theorem, $P(Y=k \\mid \\mathbf{x}) \\propto P(\\mathbf{x} \\mid Y=k) P(Y=k) = f_k(\\mathbf{x}) \\pi_k$, where $f_k(\\mathbf{x})$ is the class-conditional probability density and $\\pi_k$ is the class prior.\n\nThe problem states that the class-conditional densities are multivariate Gaussian distributions with class-specific means $\\boldsymbol{\\mu}_k$ and a common covariance matrix $\\boldsymbol{\\Sigma}$:\n$$ f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\\right) $$\nwhere $d=2$. The priors are given as equal, $\\pi_0 = \\pi_1 = 1/2$.\n\nThe decision rule is equivalent to maximizing the logarithm of the posterior, from which we derive the discriminant function $\\delta_k(\\mathbf{x})$:\n$$ \\delta_k(\\mathbf{x}) = \\ln(f_k(\\mathbf{x}) \\pi_k) = \\ln f_k(\\mathbf{x}) + \\ln \\pi_k $$\nSubstituting the Gaussian density and dropping terms that are constant across classes $k$ (namely $-\\frac{d}{2}\\ln(2\\pi)$, $-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|$, and $\\ln \\pi_k$ which is constant due to equal priors), the maximization problem simplifies to:\n$$ \\hat{y} = \\arg\\max_{k \\in \\{0,1\\}} \\left[ -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right] $$\nThis is equivalent to minimizing the squared Mahalanobis distance from $\\mathbf{x}$ to the class mean $\\boldsymbol{\\mu}_k$:\n$$ \\hat{y} = \\arg\\min_{k \\in \\{0,1\\}} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right] $$\nThe decision boundary is the set of points where the Mahalanobis distances to both class means are equal. An input $\\mathbf{x}$ is classified as class $1$ if its Mahalanobis distance to $\\boldsymbol{\\mu}_1$ is smaller than to $\\boldsymbol{\\mu}_0$, and class $0$ otherwise.\n\n### Parameter Estimation\n\nIn practice, the true parameters $\\boldsymbol{\\mu}_k$ and $\\boldsymbol{\\Sigma}$ are unknown and must be estimated from training data. Let the training data for class $k$ be $\\mathbf{X}_k \\in \\mathbb{R}^{n_k \\times d}$.\n\n1.  **Standard Estimator**:\n    -   The class mean $\\boldsymbol{\\mu}_k$ is estimated by the sample mean: $\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\mathbf{x}_{i}$.\n    -   The shared covariance matrix $\\boldsymbol{\\Sigma}$ is estimated by the pooled covariance matrix $\\hat{\\boldsymbol{\\Sigma}}$, which averages the within-class scatter matrices:\n        $$ \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n_0 + n_1 - 2} \\sum_{k=0}^{1} \\sum_{i=1}^{n_k} (\\mathbf{x}_{i,k} - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_{i,k} - \\hat{\\boldsymbol{\\mu}}_k)^T $$\n        This corresponds to the specified procedure of averaging the unbiased sample covariances weighted by $(n_k-1)$.\n\n2.  **Robust Estimator**:\n    To mitigate the influence of outliers, an $\\alpha$-trimmed estimation strategy with $\\alpha=0.2$ is used.\n    -   For each class $k$, the coordinate-wise median $\\tilde{\\boldsymbol{m}}_k$ is computed.\n    -   The Euclidean distance $||\\mathbf{x}_i - \\tilde{\\boldsymbol{m}}_k||_2$ is calculated for each sample $\\mathbf{x}_i$ in the class.\n    -   The $\\lceil \\alpha n_k \\rceil$ samples with the largest distances are identified as outliers and removed from the training set.\n    -   The standard estimators for the mean $\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_k$ and pooled covariance $\\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})}$ are then computed using these trimmed datasets.\n\n### Implementation and Logic\n\nThe solution involves training three distinct LDA classifiers as specified:\n1.  **Classifier 1 (C1)**: Standard LDA trained on the clean datasets $(\\mathbf{X}_0^{(\\mathrm{clean})}, \\mathbf{X}_1^{(\\mathrm{clean})})$.\n2.  **Classifier 2 (C2)**: Standard LDA trained on the outlier-contaminated datasets $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$.\n3.  **Classifier 3 (C3)**: Robust LDA (with $\\alpha=0.2$ trimming) trained on the outlier-contaminated datasets $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$.\n\nFor each classifier, the estimated parameters $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$ are computed. Before prediction, the invertibility of $\\hat{\\boldsymbol{\\Sigma}}$ is checked. If it is singular (determinant is close to zero), it is regularized by adding $\\lambda\\mathbf{I}$ with $\\lambda = 10^{-6}$.\n\nPredictions for the query point $\\mathbf{q}$ and the query set $\\mathcal{S}$ are made using the derived Mahalanobis distance rule for each of the three classifiers. Finally, the six boolean conditions ($b_1$ through $b_6$) are evaluated based on these predictions.\n-   $b_1$: `$C1(\\mathbf{q}) = 1$`\n-   $b_2$: `$C2(\\mathbf{q}) = 0$`\n-   $b_3$: `$C3(\\mathbf{q}) = C1(\\mathbf{q})$`\n-   $b_4$: `$C2(\\mathbf{q}) \\neq C3(\\mathbf{q})$`\n-   $b_5$: $\\exists \\mathbf{p} \\in \\mathcal{S} \\text{ s.t. } C1(\\mathbf{p}) \\neq C2(\\mathbf{p})$\n-   $b_6$: $\\forall \\mathbf{p} \\in \\mathcal{S}, C1(\\mathbf{p}) = C3(\\mathbf{p})$\n\nThe final output is a list of these boolean values.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates standard and robust LDA classifiers based on the problem specification.\n    \"\"\"\n\n    # 1. Data Definitions\n    X0_clean = np.array([[0.0, 0.0], [0.2, 0.1], [-0.1, -0.2], [0.1, 0.0]])\n    X1_clean = np.array([[2.0, 0.0], [2.1, 0.2], [1.9, -0.1], [2.2, 0.05]])\n    X0_out = np.vstack([X0_clean, [10.0, 0.0]])\n\n    q = np.array([1.1, 0.0])\n    S_set = np.array([\n        [0.96, 0.0], [1.03, 0.0], [1.10, 0.0], [1.17, 0.0], [1.24, 0.0]\n    ])\n    \n    alpha = 0.2\n    regularization_lambda = 1e-6\n\n    # 2. Estimator and Classifier Implementation\n    def train_standard_lda(X0, X1):\n        \"\"\"\n        Estimates means and pooled covariance for standard LDA.\n        \"\"\"\n        n0, d = X0.shape\n        n1, _ = X1.shape\n        \n        mu0 = np.mean(X0, axis=0)\n        mu1 = np.mean(X1, axis=0)\n\n        # Sum of squares (scatter matrices)\n        S0 = (X0 - mu0).T @ (X0 - mu0)\n        S1 = (X1 - mu1).T @ (X1 - mu1)\n        \n        # Pooled covariance matrix\n        Sigma_pooled = (S0 + S1) / (n0 + n1 - 2)\n        \n        return mu0, mu1, Sigma_pooled\n\n    def train_robust_lda(X0, X1, alpha_val):\n        \"\"\"\n        Implements the alpha-trimmed robust estimation procedure.\n        \"\"\"\n        trimmed_datasets = []\n        for X_class in [X0, X1]:\n            n_class, _ = X_class.shape\n            m_class = np.median(X_class, axis=0)\n            dists = np.linalg.norm(X_class - m_class, axis=1)\n            n_trim = math.ceil(alpha_val * n_class)\n            \n            if n_trim > 0 and n_trim  n_class:\n                trim_indices = np.argsort(dists)[-n_trim:]\n                X_trimmed = np.delete(X_class, trim_indices, axis=0)\n                trimmed_datasets.append(X_trimmed)\n            else:\n                trimmed_datasets.append(X_class)\n        \n        X0_trimmed, X1_trimmed = trimmed_datasets\n        \n        return train_standard_lda(X0_trimmed, X1_trimmed)\n\n    def classify(x, params):\n        \"\"\"\n        Predicts class label for a point x using estimated LDA parameters.\n        \"\"\"\n        mu0, mu1, Sigma = params\n        \n        # Regularize if singular\n        if np.isclose(np.linalg.det(Sigma), 0):\n            Sigma = Sigma + regularization_lambda * np.identity(Sigma.shape[0])\n            \n        Sigma_inv = np.linalg.inv(Sigma)\n        \n        # Compare Mahalanobis distances\n        dist0 = (x - mu0).T @ Sigma_inv @ (x - mu0)\n        dist1 = (x - mu1).T @ Sigma_inv @ (x - mu1)\n        \n        return 0 if dist0  dist1 else 1\n\n    # 3. Training the Three Classifiers\n    # C1: Standard LDA on clean data\n    params_c1 = train_standard_lda(X0_clean, X1_clean)\n    \n    # C2: Standard LDA on outlier-contaminated data\n    params_c2 = train_standard_lda(X0_out, X1_clean)\n    \n    # C3: Robust LDA on outlier-contaminated data\n    params_c3 = train_robust_lda(X0_out, X1_clean, alpha)\n\n    # 4. Evaluating the Six Boolean Conditions\n    # Predictions for the query point q\n    pred_q_c1 = classify(q, params_c1)\n    pred_q_c2 = classify(q, params_c2)\n    pred_q_c3 = classify(q, params_c3)\n\n    # b1: label(q) by C1 is 1\n    b1 = (pred_q_c1 == 1)\n    \n    # b2: label(q) by C2 is 0\n    b2 = (pred_q_c2 == 0)\n\n    # b3: label(q) by C3 equals label by C1\n    b3 = (pred_q_c3 == pred_q_c1)\n    \n    # b4: labels for q by C2 and C3 are different\n    b4 = (pred_q_c2 != pred_q_c3)\n\n    # Predictions for the query set S\n    preds_S_c1 = np.array([classify(p, params_c1) for p in S_set])\n    preds_S_c2 = np.array([classify(p, params_c2) for p in S_set])\n    preds_S_c3 = np.array([classify(p, params_c3) for p in S_set])\n\n    # b5: Exists p in S with different labels from C1 and C2\n    b5 = np.any(preds_S_c1 != preds_S_c2)\n    \n    # b6: For every p in S, label from C1 equals label from C3\n    b6 = np.all(preds_S_c1 == preds_S_c3)\n\n    # 5. Final Output\n    results = [b1, b2, b3, b4, b5, b6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139742"}, {"introduction": "Building a classifier is one thing, but how do we know if it's a good one? A model that performs perfectly on its training data may fail miserably on new, unseen data—a phenomenon known as overfitting. This final practice [@problem_id:3139695] introduces a principled method to diagnose overfitting by comparing the model's performance on training versus validation data, moving beyond simple error rates to a more nuanced analysis of the learned discriminant direction itself.", "problem": "You are given a two-class classification setting under the Gaussian with shared covariance assumption, the regime in which Linear Discriminant Analysis (LDA) arises as the Bayes decision rule. Let the class-conditional distributions be multivariate normal with a common covariance matrix. From first principles, the Bayes decision rule for this model is linear, and the Fisher criterion arises from projecting onto a discriminant direction that separates the classes by maximizing a ratio of between-class variance to within-class variance.\n\nYour task is to derive and implement a principled overfitting-detection test for the discriminant subspace learned on training data by comparing the Fisher ratio on training versus validation data, and to relate the observed discrepancy to the optimism of the resubstitution error. Work from the following bases only:\n- The definition of the Fisher criterion for two classes: along a direction $w$, the between-class scatter is $w^{\\top} S_{b} w$ and the within-class scatter is $w^{\\top} S_{w} w$, where $S_{b}$ and $S_{w}$ are, respectively, the between-class and within-class scatter matrices computed from sample moments.\n- Under the Gaussian with shared covariance model and equal class priors, the Bayes decision rule is linear and coincides with LDA, which uses the pooled within-class covariance and class means estimated from training data.\n\nYou must:\n- Use training data to estimate the class means $\\mu_{0}$ and $\\mu_{1}$ and the pooled within-class scatter matrix $S_{w}^{(\\mathrm{train})}$. Define the Fisher direction $w$ as the solution to the generalized eigenproblem implicit in the Fisher criterion, which in the two-class case is proportional to $(S_{w}^{(\\mathrm{train})})^{+}(\\mu_{1}-\\mu_{0})$, where $(\\cdot)^{+}$ denotes the Moore–Penrose pseudoinverse.\n- Compute the Fisher ratio on training data using the learned direction $w$ as\n$$\nJ_{\\mathrm{train}}(w) \\equiv \\frac{w^{\\top} S_{b}^{(\\mathrm{train})} w}{w^{\\top} S_{w}^{(\\mathrm{train})} w},\n$$\nand on validation data as\n$$\nJ_{\\mathrm{val}}(w) \\equiv \\frac{w^{\\top} S_{b}^{(\\mathrm{val})} w}{w^{\\top} S_{w}^{(\\mathrm{val})} w},\n$$\nwith $S_{b}^{(\\cdot)}$ defined by the outer product of the difference of the sample means on the corresponding split and $S_{w}^{(\\cdot)}$ the pooled within-class scatter on that split.\n- Define the relative Fisher drop statistic\n$$\ng \\equiv 1 - \\frac{J_{\\mathrm{val}}(w)}{J_{\\mathrm{train}}(w)},\n$$\nwith the convention that if $J_{\\mathrm{train}}(w) \\le 0$ then set $g \\equiv 1$.\n- Use the following dimension- and sample-size-aware tolerance to decide overfitting:\n$$\n\\tau(p,n) \\equiv \\min\\!\\left(\\frac{1}{2}, \\sqrt{\\frac{p}{\\max(1,\\,n - p)}}\\right),\n$$\nwhere $p$ is the feature dimension and $n$ is the total number of training samples (sum over both classes). Declare “overfitting detected” if $g  \\tau(p,n)$.\n- Train an LDA classifier on the training split by estimating the pooled covariance and class means, and evaluate the resubstitution error (training error) and the validation error. Report the optimism as the difference between the validation error and the resubstitution error, expressed as a decimal in $[0,1]$.\n\nData generation protocol for each test case:\n- Generate training and validation data from two $p$-dimensional Gaussian classes, class $0$ with mean $\\mu_{0}$ and class $1$ with mean $\\mu_{1}$, both with identity covariance matrix. Use the specified fixed random seed to ensure reproducibility. Draw $n_{\\mathrm{tr}}$ samples per class for training and $n_{\\mathrm{va}}$ samples per class for validation.\n\nTest suite:\n- Case A (well-separated, adequate sample size):\n  - Dimension $p = 2$\n  - Training per class $n_{\\mathrm{tr}} = 50$\n  - Validation per class $n_{\\mathrm{va}} = 400$\n  - Means $\\mu_{0} = (0, 0)$, $\\mu_{1} = (2.5, 0)$\n  - Covariance $I_{p}$\n  - Random seed $13$\n- Case B (no signal, high-dimensional, small sample; prone to overfitting in the discriminant direction):\n  - Dimension $p = 10$\n  - Training per class $n_{\\mathrm{tr}} = 6$\n  - Validation per class $n_{\\mathrm{va}} = 400$\n  - Means $\\mu_{0} = (0,\\ldots,0)$, $\\mu_{1} = (0,\\ldots,0)$\n  - Covariance $I_{p}$\n  - Random seed $7$\n- Case C (moderate separation, moderate sample size):\n  - Dimension $p = 5$\n  - Training per class $n_{\\mathrm{tr}} = 20$\n  - Validation per class $n_{\\mathrm{va}} = 400$\n  - Means $\\mu_{0} = (0,0,0,0,0)$, $\\mu_{1} = (1,0,0,0,0)$\n  - Covariance $I_{p}$\n  - Random seed $101$\n\nProgram requirements:\n- For each case, generate the data according to the parameters above, estimate the Fisher direction from training data, compute $J_{\\mathrm{train}}(w)$ and $J_{\\mathrm{val}}(w)$, the relative drop $g$, the threshold $\\tau(p,n)$ with $n = 2 n_{\\mathrm{tr}}$, decide the overfitting flag as a boolean, and compute the optimism defined as validation error minus resubstitution error under the LDA classifier trained on the training split.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list of three elements in the order: the boolean overfitting decision, the relative drop $g$ as a floating-point number, and the optimism as a floating-point number. The final output should therefore look like\n\"[ [flag_A,g_A,opt_A], [flag_B,g_B,opt_B], [flag_C,g_C,opt_C] ]\" with booleans and numbers in their usual machine-readable forms.", "solution": "The problem requires the development and implementation of a test for detecting overfitting in Linear Discriminant Analysis (LDA). This is accomplished by quantifying the degradation in performance of a learned discriminant direction when evaluated on a separate validation set, compared to its performance on the training set. The core metric for this comparison is the Fisher ratio, which measures class separability. A significant drop in this ratio from training to validation suggests that the discriminant direction was over-tuned to the specific noise and sampling artifacts of the training data. This statistical overfitting is then related to the optimism in the classification error rate, defined as the difference between the validation error and the resubstitution (training) error.\n\nThe theoretical foundation is LDA in a two-class setting. We assume the data for each class $k \\in \\{0, 1\\}$ is drawn from a $p$-dimensional Gaussian distribution, $\\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})$. A key assumption for LDA is that the covariance matrices are shared across classes, i.e., $\\boldsymbol{\\Sigma}_0 = \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}$. Under this model, and with equal class priors, the Bayes optimal decision boundary is linear.\n\nFisher's LDA seeks a direction vector $\\boldsymbol{w} \\in \\mathbb{R}^p$ that maximizes the ratio of the between-class scatter to the within-class scatter when the data is projected onto this direction. This ratio is the Fisher criterion, $J(\\boldsymbol{w})$. The projected between-class scatter is related to the squared distance between projected class means, $(\\boldsymbol{w}^\\top \\hat{\\boldsymbol{\\mu}}_1 - \\boldsymbol{w}^\\top \\hat{\\boldsymbol{\\mu}}_0)^2$, and the projected within-class scatter is the sum of variances within each projected class, $\\sum_{k=0}^{1} \\sum_{\\boldsymbol{x}_i \\in C_k} (\\boldsymbol{w}^\\top \\boldsymbol{x}_i - \\boldsymbol{w}^\\top \\hat{\\boldsymbol{\\mu}}_k)^2$, where $\\hat{\\boldsymbol{\\mu}}_k$ are the sample means. This can be expressed in matrix form as:\n$$\nJ(\\boldsymbol{w}) = \\frac{\\boldsymbol{w}^\\top \\boldsymbol{S}_b \\boldsymbol{w}}{\\boldsymbol{w}^\\top \\boldsymbol{S}_w \\boldsymbol{w}}\n$$\nHere, $\\boldsymbol{S}_b$ is the between-class scatter matrix and $\\boldsymbol{S}_w$ is the within-class scatter matrix. For a two-class problem, these are computed from the sample data as:\n$$\n\\boldsymbol{S}_b = (\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)^\\top\n$$\n$$\n\\boldsymbol{S}_w = \\sum_{k=0}^{1} \\boldsymbol{S}_k = \\sum_{k=0}^{1} \\sum_{\\boldsymbol{x}_i \\in C_k} (\\boldsymbol{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\boldsymbol{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^\\top\n$$\nwhere $C_k$ denotes the set of samples belonging to class $k$. The vector $\\boldsymbol{w}$ that maximizes $J(\\boldsymbol{w})$ is the solution to the generalized eigenvalue problem $\\boldsymbol{S}_b\\boldsymbol{w} = \\lambda \\boldsymbol{S}_w\\boldsymbol{w}$. For the rank-$1$ matrix $\\boldsymbol{S}_b$ in the two-class case, the solution simplifies to:\n$$\n\\boldsymbol{w} \\propto \\boldsymbol{S}_w^{-1} (\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)\n$$\nIn practice, especially in high-dimensional settings where the number of samples $n$ is not much larger than the dimension $p$, the matrix $\\boldsymbol{S}_w$ may be ill-conditioned or singular. To ensure a stable solution, we use the Moore-Penrose pseudoinverse, denoted by $(\\cdot)^{+}$. The discriminant direction $\\boldsymbol{w}$ is thus estimated from the training data as:\n$$\n\\boldsymbol{w} = (\\boldsymbol{S}_{w}^{(\\mathrm{train})})^{+} (\\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})} - \\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})})\n$$\nThe value of the Fisher criterion achieved on the training data, which the vector $\\boldsymbol{w}$ was optimized for, is:\n$$\nJ_{\\mathrm{train}}(\\boldsymbol{w}) = \\frac{\\boldsymbol{w}^\\top \\boldsymbol{S}_{b}^{(\\mathrm{train})} \\boldsymbol{w}}{\\boldsymbol{w}^\\top \\boldsymbol{S}_{w}^{(\\mathrm{train})} \\boldsymbol{w}}\n$$\nwhere $\\boldsymbol{S}_{b}^{(\\mathrm{train})}$ and $\\boldsymbol{S}_{w}^{(\\mathrm{train})}$ are computed from the training data.\n\nTo detect overfitting, we assess how well the direction $\\boldsymbol{w}$ generalizes to unseen validation data. We compute the Fisher ratio using the same $\\boldsymbol{w}$, but with scatter matrices derived from the validation set:\n$$\nJ_{\\mathrm{val}}(\\boldsymbol{w}) = \\frac{\\boldsymbol{w}^\\top \\boldsymbol{S}_{b}^{(\\mathrm{val})} \\boldsymbol{w}}{\\boldsymbol{w}^\\top \\boldsymbol{S}_{w}^{(\\mathrm{val})} \\boldsymbol{w}}\n$$\nAn overfit model has learned a direction $\\boldsymbol{w}$ that is highly specific to the training data, resulting in a high $J_{\\mathrm{train}}(\\boldsymbol{w})$. This direction is unlikely to be optimal for the validation set, leading to a significantly lower $J_{\\mathrm{val}}(\\boldsymbol{w})$. We quantify this degradation using the relative Fisher drop statistic, $g$:\n$$\ng \\equiv 1 - \\frac{J_{\\mathrm{val}}(\\boldsymbol{w})}{J_{\\mathrm{train}}(\\boldsymbol{w})}\n$$\nA value of $g$ close to $0$ implies similar performance on training and validation sets (good generalization), while a large positive value indicates a substantial performance drop (overfitting). Per the problem, if $J_{\\mathrm{train}}(\\boldsymbol{w}) \\le 0$ (a degenerate case), we set $g=1$.\n\nThe significance of the drop $g$ depends on the problem's statistical complexity, namely the feature dimension $p$ and the total training sample size $n$. Some performance drop is always expected due to finite sample effects. The overfitting decision is formalized by comparing $g$ to a dimension- and sample-size-aware tolerance $\\tau(p,n)$:\n$$\n\\tau(p,n) \\equiv \\min\\!\\left(\\frac{1}{2}, \\sqrt{\\frac{p}{\\max(1,\\,n - p)}}\\right)\n$$\nThis threshold increases with the ratio $p/n$, capturing the principle that overfitting is more likely in high-dimensional, small-sample regimes. We declare overfitting if $g  \\tau(p,n)$.\n\nFinally, we connect this test to classification performance. The LDA classifier is trained on the training data. Its decision rule, assuming equal class priors, classifies a new point $\\boldsymbol{x}$ based on the sign of $\\boldsymbol{w}^\\top \\boldsymbol{x} - c$, where the threshold $c$ is set midway between the projected class means: $c = \\frac{1}{2}\\boldsymbol{w}^\\top(\\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})} + \\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})})$. The resubstitution error is the error rate of this classifier on the training data. The validation error is its error rate on the validation data. The optimism is the difference:\n$$\n\\text{Optimism} = (\\text{Validation Error}) - (\\text{Resubstitution Error})\n$$\nA large, positive optimism is a direct symptom of overfitting. The proposed test based on $g$ is expected to flag cases with high optimism as being overfit.\n\nThe implementation will proceed by executing the following steps for each test case:\n$1$. Generate training and validation datasets according to the specified parameters ($p, n_{\\mathrm{tr}}, n_{\\mathrm{va}}, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma} = \\boldsymbol{I}_p$) and random seed.\n$2$. From the training data, compute the sample means $\\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})}, \\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})}$, and the scatter matrices $\\boldsymbol{S}_{b}^{(\\mathrm{train})}, \\boldsymbol{S}_{w}^{(\\mathrm{train})}$.\n$3$. Calculate the Fisher discriminant direction $\\boldsymbol{w} = (\\boldsymbol{S}_{w}^{(\\mathrm{train})})^{+} (\\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})} - \\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})})$.\n$4$. Compute scatter matrices $\\boldsymbol{S}_{b}^{(\\mathrm{val})}$ and $\\boldsymbol{S}_{w}^{(\\mathrm{val})}$ from the validation data.\n$5$. Calculate $J_{\\mathrm{train}}(\\boldsymbol{w})$ and $J_{\\mathrm{val}}(\\boldsymbol{w})$, and from them, the statistic $g$.\n$6$. Calculate the threshold $\\tau(p, n)$ where $n = 2 n_{\\mathrm{tr}}$, and determine the boolean overfitting flag.\n$7$. Construct the LDA classifier using $\\boldsymbol{w}$ and the threshold $c$ derived from training data.\n$8$. Evaluate the resubstitution and validation error rates to compute the optimism.\n$9$. Collate the results for all test cases into the required output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, performing LDA,\n    and calculating the overfitting statistics and error optimism.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: well-separated, adequate sample size\n        {\n            \"p\": 2, \"n_tr\": 50, \"n_va\": 400,\n            \"mu0\": np.array([0.0, 0.0]), \"mu1\": np.array([2.5, 0.0]),\n            \"seed\": 13\n        },\n        # Case B: no signal, high-dimensional, small sample\n        {\n            \"p\": 10, \"n_tr\": 6, \"n_va\": 400,\n            \"mu0\": np.zeros(10), \"mu1\": np.zeros(10),\n            \"seed\": 7\n        },\n        # Case C: moderate separation, moderate sample size\n        {\n            \"p\": 5, \"n_tr\": 20, \"n_va\": 400,\n            \"mu0\": np.zeros(5), \"mu1\": np.array([1.0, 0.0, 0.0, 0.0, 0.0]),\n            \"seed\": 101\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p, n_tr, n_va = case[\"p\"], case[\"n_tr\"], case[\"n_va\"]\n        mu0, mu1 = case[\"mu0\"], case[\"mu1\"]\n        seed = case[\"seed\"]\n\n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        cov_matrix = np.identity(p)\n        \n        X_tr_0 = rng.multivariate_normal(mu0, cov_matrix, size=n_tr)\n        X_tr_1 = rng.multivariate_normal(mu1, cov_matrix, size=n_tr)\n        X_va_0 = rng.multivariate_normal(mu0, cov_matrix, size=n_va)\n        X_va_1 = rng.multivariate_normal(mu1, cov_matrix, size=n_va)\n\n        X_train = np.vstack((X_tr_0, X_tr_1))\n        y_train = np.hstack((np.zeros(n_tr, dtype=int), np.ones(n_tr, dtype=int)))\n        X_val = np.vstack((X_va_0, X_va_1))\n        y_val = np.hstack((np.zeros(n_va, dtype=int), np.ones(n_va, dtype=int)))\n\n        # Helper function to compute sample means and scatter matrices\n        def get_scatter_matrices(X0, X1):\n            mu0_hat = np.mean(X0, axis=0)\n            mu1_hat = np.mean(X1, axis=0)\n            \n            # Within-class scatter\n            S0 = (X0 - mu0_hat).T @ (X0 - mu0_hat)\n            S1 = (X1 - mu1_hat).T @ (X1 - mu1_hat)\n            Sw = S0 + S1\n            \n            # Between-class scatter\n            mu_diff = mu1_hat - mu0_hat\n            Sb = np.outer(mu_diff, mu_diff)\n            \n            return mu0_hat, mu1_hat, Sw, Sb\n\n        # 2. Training: Estimate Fisher direction w\n        mu0_tr, mu1_tr, Sw_tr, Sb_tr = get_scatter_matrices(X_tr_0, X_tr_1)\n        w = np.linalg.pinv(Sw_tr) @ (mu1_tr - mu0_tr)\n        \n        # 3. Compute Fisher Ratios\n        J_train_num = w.T @ Sb_tr @ w\n        J_train_den = w.T @ Sw_tr @ w\n        J_train = J_train_num / J_train_den if J_train_den > 1e-15 else 0.0\n\n        _, _, Sw_val, Sb_val = get_scatter_matrices(X_va_0, X_va_1)\n        J_val_num = w.T @ Sb_val @ w\n        J_val_den = w.T @ Sw_val @ w\n        J_val = J_val_num / J_val_den if J_val_den > 1e-15 else 0.0\n            \n        # 4. Compute Relative Fisher Drop (g) and Overfitting Decision\n        g = 1.0 - (J_val / J_train) if J_train > 0.0 else 1.0\n        \n        n_total_train = 2 * n_tr\n        tau_denom = max(1.0, n_total_train - p)\n        tau = min(0.5, np.sqrt(p / tau_denom))\n        \n        is_overfitting = g > tau\n        \n        # 5. LDA Classifier Errors and Optimism\n        threshold = w.T @ (mu0_tr + mu1_tr) / 2.0\n        \n        # Resubstitution error\n        predictions_train = (X_train @ w > threshold).astype(int)\n        resub_error = np.mean(predictions_train != y_train)\n        \n        # Validation error\n        predictions_val = (X_val @ w > threshold).astype(int)\n        val_error = np.mean(predictions_val != y_val)\n        \n        optimism = val_error - resub_error\n        \n        all_results.append([is_overfitting, g, optimism])\n\n    # Final print statement in the exact required format.\n    formatted_cases = []\n    for r in all_results:\n        # The required format is a list of lists, comma-separated, with no spaces.\n        # str() is used for booleans and numbers for standard machine-readable form.\n        case_str = f\"[{str(r[0])},{str(r[1])},{str(r[2])}]\"\n        formatted_cases.append(case_str)\n        \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```", "id": "3139695"}]}