{"hands_on_practices": [{"introduction": "When moving from binary to multiclass classification, a tempting approach is to train independent binary classifiers in a one-vs-rest (OVR) scheme. This exercise demonstrates the potential pitfalls of this strategy, where predicted probabilities can become inconsistent and uncalibrated. By building a concrete scenario [@problem_id:3151582], you will see firsthand why the joint-normalized probabilities produced by the softmax function are essential for a coherent multiclass model, providing a clear and valid probability distribution across all categories.", "problem": "You are asked to construct and analyze a concrete multiclass classification scenario that highlights the difference between a one-vs-rest (OVR) scheme of multiple binary logistic regressions and a single multinomial logistic regression with the softmax link. Your task is to implement the probability mappings implied by the canonical inverse link functions of the Bernoulli and categorical distributions, apply them to specified linear models, and quantify how the OVR scheme can yield overlapping decision regions that are inconsistent with normalized probabilities, in contrast to the softmax model that enforces normalization.\n\nBase definitions to use:\n- For a Bernoulli distribution, use the canonical inverse link that maps a linear predictor to a valid probability in $\\left(0,1\\right)$.\n- For a categorical distribution with $K$ classes, use the canonical inverse link induced by the exponential family representation that yields a normalized probability vector $\\left(p_1,\\dots,p_K\\right)$ satisfying $\\sum_{k=1}^K p_k=1$.\n\nSetup:\n- Consider $K=3$ classes and feature vectors $x \\in \\mathbb{R}^2$ augmented with an intercept, i.e., use $\\bar{x} = \\left(1,x_1,x_2\\right)^\\top$.\n- Define three class-specific weight vectors for the one-vs-rest models (each a binary logistic regression against the rest) as rows of a matrix $W_{\\text{ovr}} \\in \\mathbb{R}^{3 \\times 3}$:\n  $$\n  W_{\\text{ovr}} =\n  \\begin{bmatrix}\n  1.0 & 1.5 & -0.1 \\\\\n  1.0 & -0.1 & 1.5 \\\\\n  0.6 & 0.7 & 0.7\n  \\end{bmatrix}.\n  $$\n  For class $k \\in \\{1,2,3\\}$, the OVR linear predictor is $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$, where $w_k^\\top$ is the $k$-th row of $W_{\\text{ovr}}$.\n- Define a multinomial (softmax) model with the same linear scores, using a weight matrix $W_{\\text{soft}} \\in \\mathbb{R}^{3 \\times 3}$ equal to $W_{\\text{ovr}}$:\n  $$\n  W_{\\text{soft}} = W_{\\text{ovr}}.\n  $$\n  For class $k \\in \\{1,2,3\\}$, the softmax linear score is $s_k = \\beta_k^\\top \\bar{x}$, where $\\beta_k^\\top$ is the $k$-th row of $W_{\\text{soft}}$.\n\nComputations to perform for each test input $\\bar{x}$:\n1. Compute the OVR class probabilities $\\left(p_1^{\\text{ovr}},p_2^{\\text{ovr}},p_3^{\\text{ovr}}\\right)$ using the Bernoulli canonical inverse link applied to $\\left(z_1^{\\text{ovr}},z_2^{\\text{ovr}},z_3^{\\text{ovr}}\\right)$. Let $S_{\\text{ovr}}=\\sum_{k=1}^3 p_k^{\\text{ovr}}$. Also compute $N_{0.5}$, the number of OVR classes with $p_k^{\\text{ovr}} \\ge 0.5$, and an overlap indicator $I_{\\text{ovr}}$ that is $1$ if at least two classes simultaneously satisfy $p_k^{\\text{ovr}} \\ge 0.5$ and $0$ otherwise.\n2. Compute the softmax probabilities $\\left(p_1^{\\text{soft}},p_2^{\\text{soft}},p_3^{\\text{soft}}\\right)$ using the categorical canonical inverse link applied to $\\left(s_1,s_2,s_3\\right)$, and let $S_{\\text{soft}}=\\sum_{k=1}^3 p_k^{\\text{soft}}$.\n\nTest suite:\n- Use the following five feature vectors (each $x$ given as $\\left[x_1,x_2\\right]$), which are chosen to test a general overlap case, moderate negative region, mixed-sign strong evidence, and two extreme edge cases:\n  - $x^{(1)} = \\left[0.4,\\,0.4\\right]$\n  - $x^{(2)} = \\left[-1.0,\\,-1.0\\right]$\n  - $x^{(3)} = \\left[3.0,\\,-3.0\\right]$\n  - $x^{(4)} = \\left[-5.0,\\,-5.0\\right]$\n  - $x^{(5)} = \\left[10.0,\\,10.0\\right]$\n- For each $x^{(i)}$, construct $\\bar{x}^{(i)} = \\left(1,x_1^{(i)},x_2^{(i)}\\right)^\\top$ and evaluate the quantities described above.\n\nNumerical stability:\n- Use numerically stable implementations of the Bernoulli and categorical canonical inverse links to handle large-magnitude linear predictors without overflow.\n\nRequired final output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $i$-th element is itself a list\n  $$\n  \\left[\\,\\text{round}(S_{\\text{ovr}},6),\\,\\text{round}(S_{\\text{soft}},6),\\,N_{0.5},\\,\\mathbf{1}\\{I_{\\text{ovr}}=1\\}\\,\\right],\n  $$\n  with $S_{\\text{ovr}}$ and $S_{\\text{soft}}$ rounded to $6$ decimal places, $N_{0.5}$ an integer, and $\\mathbf{1}\\{I_{\\text{ovr}}=1\\}$ a boolean indicator represented as either True or False. The five elements must be reported in the order of the test suite above. For example, the output format must look like:\n  $$\n  \\big[\\,[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]\\,\\big]\n  $$\n  with no spaces.", "solution": "The user's request is a valid computational problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Classes**: $K=3$ classes.\n-   **Feature Vectors**: Input vectors $x \\in \\mathbb{R}^2$ are augmented with an intercept term to form $\\bar{x} = (1, x_1, x_2)^\\top$.\n-   **One-vs-Rest (OVR) Model**:\n    -   Weight matrix:\n      $$\n      W_{\\text{ovr}} =\n      \\begin{bmatrix}\n      1.0 & 1.5 & -0.1 \\\\\n      1.0 & -0.1 & 1.5 \\\\\n      0.6 & 0.7 & 0.7\n      \\end{bmatrix}.\n      $$\n    -   Linear predictor for class $k \\in \\{1, 2, 3\\}$: $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$, where $w_k^\\top$ is the $k$-th row of $W_{\\text{ovr}}$.\n-   **Multinomial (Softmax) Model**:\n    -   Weight matrix: $W_{\\text{soft}} = W_{\\text{ovr}}$.\n    -   Linear score for class $k \\in \\{1, 2, 3\\}$: $s_k = \\beta_k^\\top \\bar{x}$, where $\\beta_k^\\top$ is the $k$-th row of $W_{\\text{soft}}$.\n-   **Inverse Link Functions**:\n    -   **Bernoulli**: The canonical inverse link mapping a linear predictor to a probability in $(0, 1)$.\n    -   **Categorical**: The canonical inverse link producing a normalized probability vector $(p_1, \\dots, p_K)$ such that $\\sum_{k=1}^K p_k = 1$.\n-   **Computations for each $\\bar{x}$**:\n    1.  **OVR**: Compute probabilities $(p_1^{\\text{ovr}}, p_2^{\\text{ovr}}, p_3^{\\text{ovr}})$, their sum $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$, the count $N_{0.5}$ of classes where $p_k^{\\text{ovr}} \\ge 0.5$, and an overlap indicator $I_{\\text{ovr}}$ (equal to $1$ if $N_{0.5} \\ge 2$, else $0$).\n    2.  **Softmax**: Compute probabilities $(p_1^{\\text{soft}}, p_2^{\\text{soft}}, p_3^{\\text{soft}})$ and their sum $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$.\n-   **Test Suite**: A set of five feature vectors $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$:\n    -   $x^{(1)} = [0.4, 0.4]$\n    -   $x^{(2)} = [-1.0, -1.0]$\n    -   $x^{(3)} = [3.0, -3.0]$\n    -   $x^{(4)} = [-5.0, -5.0]$\n    -   $x^{(5)} = [10.0, 10.0]$\n-   **Output Format**: For each test vector, produce a list $[\\text{round}(S_{\\text{ovr}}, 6), \\text{round}(S_{\\text{soft}}, 6), N_{0.5}, \\mathbf{1}\\{I_{\\text{ovr}}=1\\}]$, where the last element is a boolean (`True` or `False`).\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is well-grounded in the theory of generalized linear models (GLMs). The specified canonical inverse link for a Bernoulli distribution is the logistic (sigmoid) function, and for a categorical distribution, it is the softmax function. The comparison of one-vs-rest and multinomial logistic regression is a standard and important topic in statistical learning.\n-   **Well-Posed**: The problem is well-posed. All parameters, data, and required computations are explicitly defined, leading to a unique, stable, and meaningful numerical solution.\n-   **Objective**: The problem is stated objectively using precise mathematical notation and terminology, without any subjective or ambiguous language.\n-   **Conclusion**: The problem is free of scientific unsoundness, incompleteness, contradictions, and other flaws listed in the validation criteria.\n\n**Step 3: Verdict and Action**\n\n-   **Verdict**: The problem is **valid**.\n-   **Action**: A complete solution will be provided.\n\n### Solution Derivation\n\nThis problem requires a comparison of two common multiclass classification strategies: a one-vs-rest (OVR) scheme using multiple independent binary classifiers and a joint multinomial (softmax) regression model. The core of the task is to apply the correct probability mapping functions to the linear scores produced by the given weight matrices.\n\n**1. Canonical Inverse Link Functions**\n\n-   **Bernoulli/OVR**: The canonical inverse link for the Bernoulli distribution is the logistic function, often called the sigmoid function, $\\sigma(z)$. It maps a linear predictor $z \\in \\mathbb{R}$ to a probability in $(0, 1)$.\n    $$\n    p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n    $$\n    In the OVR scheme, this function is applied independently to each class-specific linear predictor $z_k^{\\text{ovr}}$.\n\n-   **Categorical/Softmax**: The canonical inverse link for the categorical distribution is the softmax function. It maps a vector of linear scores $s = (s_1, \\dots, s_K)^\\top$ to a normalized probability vector $p = (p_1, \\dots, p_K)^\\top$ where each $p_k \\in (0, 1)$ and $\\sum_{k=1}^K p_k = 1$.\n    $$\n    p_k = \\frac{e^{s_k}}{\\sum_{j=1}^K e^{s_j}}\n    $$\n    The softmax function ensures that the resulting probabilities form a valid distribution over the $K$ classes.\n\n**2. Model Computations**\n\nFor each input feature vector $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$, we first augment it to $\\bar{x}^{(i)} = [1, x_1^{(i)}, x_2^{(i)}]^\\top$. The linear scores for both models are computed using the same weight matrix $W = W_{\\text{ovr}} = W_{\\text{soft}}$. The vector of scores/predictors is given by $z^{(i)} = W\\bar{x}^{(i)}$.\n\nLet's denote the score vector elements as $z_1, z_2, z_3$.\n\n-   **OVR Probabilities**: The probabilities for the OVR model are calculated by applying the sigmoid function independently to each score:\n    $$\n    p_k^{\\text{ovr}} = \\sigma(z_k) = \\frac{1}{1 + e^{-z_k}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    The sum $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$ is generally not equal to $1$. We then count $N_{0.5} = \\sum_{k=1}^3 \\mathbf{1}\\{p_k^{\\text{ovr}} \\ge 0.5\\}$ and set the overlap indicator $I_{\\text{ovr}}$ to be true if $N_{0.5} \\ge 2$.\n\n-   **Softmax Probabilities**: The probabilities for the softmax model are calculated jointly:\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k}}{e^{z_1} + e^{z_2} + e^{z_3}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    By construction, the sum $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$ will always be equal to $1$.\n\n**3. Numerical Stability**\n\nTo prevent numerical overflow/underflow with large magnitude inputs to the exponential function, stable implementations are required.\n\n-   **Stable Sigmoid $\\sigma(z)$**:\n    -   If $z \\ge 0$, use $1 / (1 + e^{-z})$. This avoids $e$ raised to a large positive power.\n    -   If $z < 0$, use an algebraically equivalent form $e^z / (1 + e^z)$. This again avoids $e$ raised to a large positive power.\n\n-   **Stable Softmax**: The \"log-sum-exp\" trick is used. Let $c = \\max_k(z_k)$. The softmax probabilities can be rewritten as:\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k - c}}{\\sum_{j=1}^K e^{z_j - c}}\n    $$\n    This transformation centers the exponents around $0$, preventing overflow from large positive scores while maintaining numerical precision.\n\n**4. Walkthrough for Test Case $x^{(1)} = [0.4, 0.4]$**\n\n1.  **Augment vector**: $\\bar{x}^{(1)} = [1, 0.4, 0.4]^\\top$.\n2.  **Compute linear scores**:\n    $$\n    z = W \\bar{x}^{(1)} =\n    \\begin{bmatrix}\n    1.0 & 1.5 & -0.1 \\\\\n    1.0 & -0.1 & 1.5 \\\\\n    0.6 & 0.7 & 0.7\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    1.0 \\\\\n    0.4 \\\\\n    0.4\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.0 + 0.6 - 0.04 \\\\\n    1.0 - 0.04 + 0.6 \\\\\n    0.6 + 0.28 + 0.28\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.56 \\\\\n    1.56 \\\\\n    1.16\n    \\end{bmatrix}\n    $$\n3.  **OVR Calculation**:\n    -   $p_1^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_2^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_3^{\\text{ovr}} = \\sigma(1.16) \\approx 0.761352$\n    -   $S_{\\text{ovr}} = 0.826384 + 0.826384 + 0.761352 = 2.414120$\n    -   All three probabilities are $\\ge 0.5$, so $N_{0.5} = 3$.\n    -   Since $N_{0.5} \\ge 2$, the overlap indicator is `True`.\n\n4.  **Softmax Calculation**:\n    -   Using scores $z_1=1.56, z_2=1.56, z_3=1.16$.\n    -   $e^{1.56} \\approx 4.7588$, $e^{1.56} \\approx 4.7588$, $e^{1.16} \\approx 3.1899$.\n    -   Sum of exponentials $\\approx 12.7075$.\n    -   $p_1^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_2^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_3^{\\text{soft}} = 3.1899 / 12.7075 \\approx 0.251064$\n    -   $S_{\\text{soft}} = 0.374468 + 0.374468 + 0.251064 = 1.000000$.\n\n5.  **Result for $x^{(1)}$**:\n    $[\\text{round}(2.414120, 6), \\text{round}(1.0, 6), 3, \\text{True}] \\rightarrow [2.41412, 1.0, 3, \\text{True}]$.\n\nThis process is repeated for all five test vectors to generate the final output. The OVR scheme yields unnormalized probabilities that sum to $2.41$ and classifies the point into all three classes (since all $p_k>0.5$), highlighting its ambiguity. The softmax model provides a valid, normalized probability distribution, assigning the highest (but not overwhelming) probability to classes $1$ and $2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares probabilities from one-vs-rest (OVR) and softmax\n    models for a series of test cases.\n    \"\"\"\n\n    # Define the weight matrix for both OVR and Softmax models.\n    W = np.array([\n        [1.0, 1.5, -0.1],\n        [1.0, -0.1, 1.5],\n        [0.6, 0.7, 0.7]\n    ])\n\n    # Define the test suite of feature vectors.\n    test_cases_x = [\n        np.array([0.4, 0.4]),\n        np.array([-1.0, -1.0]),\n        np.array([3.0, -3.0]),\n        np.array([-5.0, -5.0]),\n        np.array([10.0, 10.0])\n    ]\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable implementation of the sigmoid function.\"\"\"\n        # Condition to avoid overflow in exp for negative z\n        # and to maintain precision for positive z.\n        # np.where(condition, x, y) is used for element-wise conditional evaluation.\n        return np.where(\n            z >= 0,\n            1.0 / (1.0 + np.exp(-z)),\n            np.exp(z) / (1.0 + np.exp(z))\n        )\n\n    def stable_softmax(s):\n        \"\"\"Numerically stable implementation of the softmax function.\"\"\"\n        # Subtract the max for numerical stability (log-sum-exp trick).\n        s_max = np.max(s)\n        s_shifted = s - s_max\n        exps = np.exp(s_shifted)\n        return exps / np.sum(exps)\n\n    results_for_print = []\n\n    for x in test_cases_x:\n        # Augment the feature vector with an intercept term (x_0 = 1).\n        x_aug = np.insert(x, 0, 1.0)\n\n        # Compute the linear scores/predictors z = W * x_aug.\n        # Since W is (3, 3) and x_aug is (3,), the result is a (3,) vector.\n        z = W @ x_aug\n\n        # 1. One-vs-Rest (OVR) computations\n        p_ovr = stable_sigmoid(z)\n        S_ovr = np.sum(p_ovr)\n        \n        # Count classes where probability is >= 0.5\n        N_05 = np.sum(p_ovr >= 0.5)\n        \n        # Overlap indicator: True if at least two classes meet the threshold\n        I_ovr = (N_05 >= 2)\n\n        # 2. Softmax computations\n        p_soft = stable_softmax(z)\n        S_soft = np.sum(p_soft) # This will be 1.0 by definition.\n\n        # Format the sub-result as a string to avoid spaces when printing\n        # the final list of lists.\n        # Rounding S_ovr and S_soft to 6 decimal places as required.\n        s_ovr_rounded = f\"{S_ovr:.6f}\"\n        s_soft_rounded = f\"{S_soft:.6f}\"\n        \n        sub_result_str = f\"[{s_ovr_rounded},{s_soft_rounded},{N_05},{I_ovr}]\"\n        results_for_print.append(sub_result_str)\n\n    # Final print statement in the exact required format.\n    # Joining the string-formatted sub-results with a comma.\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n\n```", "id": "3151582"}, {"introduction": "A correct mathematical formula does not always guarantee a robust numerical implementation, especially in machine learning. This practice [@problem_id:3151616] tackles the critical issue of numerical instability when implementing the softmax function, where direct computation of the exponential term $\\exp(\\eta)$ can lead to overflow or underflow errors. You will implement and test the \"log-sum-exp trick,\" an indispensable technique that ensures your model remains stable and accurate even when dealing with the large-magnitude linear scores common in practice.", "problem": "You are given a computational task arising from the normalization step of the multinomial logistic regression model. For a feature vector $x \\in \\mathbb{R}^p$ and $K$ class-specific parameter vectors $\\beta_j \\in \\mathbb{R}^p$, the linear predictors are $\\eta_j = x^\\top \\beta_j$ for $j \\in \\{1,\\dots,K\\}$. The normalization constant in the softmax function requires computing $\\log \\sum_{j=1}^K \\exp(\\eta_j)$, which is well defined in real arithmetic but can be numerically unstable in finite-precision floating-point arithmetic when $\\lVert \\eta \\rVert$ is large.\n\nStarting from the fundamental definitions of the multinomial logistic regression predictors and the logarithm and exponential functions, implement two computational methods for the quantity $\\log \\sum_{j=1}^K \\exp(\\eta_j)$:\n- A direct method that applies the exponential and summation followed by the logarithm without any special handling of numeric range.\n- A numerically stable method that computes the same value while avoiding overflow or underflow through a mathematically justified transformation that preserves equality in exact arithmetic.\n\nThen, for each test case provided below, do the following:\n1. Compute the vector $\\eta \\in \\mathbb{R}^K$ using $\\eta_j = x^\\top \\beta_j$.\n2. Compute the Euclidean norm $\\lVert \\eta \\rVert_2$.\n3. Compute the direct result and the numerically stable result for $\\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n4. Use a reliable reference implementation to approximate the mathematically correct value of $\\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n5. For each method, determine whether its result is a finite number, and compute the absolute error relative to the reference when the result is finite. If the result is not finite, report the error as $+\\infty$.\n\nYour program must use the following test suite. Each test defines $p$, $K$, a feature vector $x \\in \\mathbb{R}^p$, and a target vector $t \\in \\mathbb{R}^K$. Construct parameters $\\beta_j \\in \\mathbb{R}^p$ so that $x^\\top \\beta_j = t_j$ for each $j$ by setting\n$$\n\\beta_j \\;=\\; \\frac{t_j}{x^\\top x}\\, x \\quad \\text{for each } j \\in \\{1,\\dots,K\\}.\n$$\nThis construction guarantees that $x^\\top \\beta_j = t_j$ in exact arithmetic.\n\nTest suite:\n- Case A (moderate values, happy path):\n  - $p = 3$, $K = 3$\n  - $x = [0.5,\\,-1.0,\\,2.0]$\n  - $t = [-1.2,\\,0.3,\\,2.1]$\n- Case B (very large positive values, overflow-prone for the direct method):\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [1000.0,\\,1001.0,\\,999.0]$\n- Case C (very large negative values, underflow-prone for the direct method):\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,-1001.0,\\,-999.0]$\n- Case D (mixed extreme values, challenging dynamic range):\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,0.0,\\,1000.0]$\n\nFor each case, output a sequence of five values in the order:\n- $\\lVert \\eta \\rVert_2$ as a floating-point number,\n- a boolean indicating whether the direct result is finite,\n- a boolean indicating whether the stable result is finite,\n- the absolute error of the direct result relative to the reference as a floating-point number (use $+\\infty$ if not finite),\n- the absolute error of the stable result relative to the reference as a floating-point number (use $+\\infty$ if not finite).\n\nFinal output format:\nYour program should produce a single line of output containing the results of all cases concatenated in the order A, B, C, D, as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3,...,r20]\". All booleans must appear as unquoted True or False. No additional text or whitespace should be printed.", "solution": "The problem asks for the implementation and comparison of two methods for computing the log-sum-exp function, defined as $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$, where $\\eta \\in \\mathbb{R}^K$ is a vector of linear predictors from a multinomial logistic regression model. The motivation is to handle numerical instability that arises in floating-point arithmetic.\n\n### Step 1: Extract Givens\n- **Task**: Implement a direct method and a numerically stable method for computing $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n- **Linear Predictors**: $\\eta_j = x^\\top \\beta_j$ for $j \\in \\{1, \\dots, K\\}$, with $x \\in \\mathbb{R}^p$ and $\\beta_j \\in \\mathbb{R}^p$.\n- **Parameter Construction**: For given test vectors $x$ and $t$, the parameters $\\beta_j$ are constructed as $\\beta_j = \\frac{t_j}{x^\\top x} x$. This construction ensures $\\eta_j = t_j$ for each $j$, as shown by the substitution:\n$$ x^\\top \\beta_j = x^\\top \\left( \\frac{t_j}{x^\\top x} x \\right) = \\frac{t_j}{x^\\top x} (x^\\top x) = t_j $$\nThis identity holds provided $x^\\top x \\neq 0$, i.e., $x$ is not the zero vector. All provided test cases use non-zero $x$ vectors. Therefore, for each test case, the vector of linear predictors $\\eta$ is exactly the given target vector $t$.\n- **Test Cases**:\n    - **A**: $p=3, K=3, x=[0.5, -1.0, 2.0], t=[-1.2, 0.3, 2.1]$\n    - **B**: $p=3, K=3, x=[1.0, 1.0, 1.0], t=[1000.0, 1001.0, 999.0]$\n    - **C**: $p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, -1001.0, -999.0]$\n    - **D**: $p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, 0.0, 1000.0]$\n- **Required Outputs per Case**: A sequence of $5$ values:\n    1.  Euclidean norm $\\|\\eta\\|_2$.\n    2.  Boolean: Is the direct result finite?\n    3.  Boolean: Is the stable result finite?\n    4.  Absolute error of the direct result (or $+\\infty$).\n    5.  Absolute error of the stable result (or $+\\infty$).\n- **Reference Value**: Use a reliable reference implementation to approximate the correct value.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is critically evaluated:\n- **Scientifically Grounded**: The problem addresses the well-known numerical instability of the log-sum-exp calculation, which is a fundamental component of the softmax function used in multinomial logistic regression and other machine learning models. The mathematical transformation used to achieve stability is standard and correct. The problem is firmly rooted in numerical analysis and statistical learning theory.\n- **Well-Posed**: The problem is unambiguous. All inputs ($x, t, p, K$) are specified for each test case. The quantity to be computed is clearly defined. The construction of $\\beta_j$ leading to $\\eta = t$ is explicit. The evaluation metrics (norm, finiteness, absolute error) are precise. A unique solution exists for each computation.\n- **Objective**: The problem is stated in objective, mathematical language. It requires a computational solution based on numerical inputs and established algorithms, free of subjective interpretation.\n- **Completeness and Consistency**: The problem is self-contained. All necessary formulas and data are provided. There are no contradictions in the setup.\n- **No other flaws**: The problem is not trivial (it demonstrates a critical numerical issue), not metaphorical, and is directly relevant to the stated topic. It is verifiable through computation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n### Solution Derivation\n\nFor a given vector $\\eta = (\\eta_1, \\eta_2, \\dots, \\eta_K)$, we need to compute $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n\n**Method 1: Direct Computation**\nThis method implements the formula as written.\n1.  Compute $v_j = \\exp(\\eta_j)$ for each $j \\in \\{1, \\dots, K\\}$.\n2.  Compute the sum $S = \\sum_{j=1}^K v_j$.\n3.  Compute the final result $R_{\\text{direct}} = \\log(S)$.\n\nThis approach is numerically unstable. For standard double-precision floating-point numbers, $\\exp(z)$ overflows to $+\\infty$ for $z \\gtrsim 709.78$. If any $\\eta_j$ is large, the corresponding $\\exp(\\eta_j)$ becomes $+\\infty$, leading to an infinite sum and an infinite result. Conversely, $\\exp(z)$ underflows to $0$ for $z \\lesssim -745.13$. If all $\\eta_j$ values are large and negative, every term in the sum may underflow to $0.0$, making the sum $0.0$. The logarithm $\\log(0.0)$ is $-\\infty$.\n\n**Method 2: Numerically Stable Computation (Log-Sum-Exp Trick)**\nThis method is based on a mathematical identity that repositions the values to a numerically safe range. Let $m = \\max_{j} \\eta_j$. We can rewrite the expression by factoring out $\\exp(m)$:\n$$ L(\\eta) = \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j) \\right) $$\n$$ = \\log \\left( \\exp(m) \\sum_{j=1}^K \\frac{\\exp(\\eta_j)}{\\exp(m)} \\right) $$\nUsing the properties of logarithms and exponentials, $\\log(a \\cdot b) = \\log(a) + \\log(b)$ and $\\exp(a)/\\exp(b) = \\exp(a-b)$, we get:\n$$ = \\log(\\exp(m)) + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\n$$ = m + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\nThis gives the stable computation steps:\n1.  Find the maximum value in the vector: $m = \\max(\\eta_1, \\dots, \\eta_K)$.\n2.  Compute the shifted vector $\\eta_j' = \\eta_j - m$.\n3.  Compute $v_j' = \\exp(\\eta_j')$ for each $j$.\n4.  Compute the sum $S' = \\sum_{j=1}^K v_j'$.\n5.  Compute the final result $R_{\\text{stable}} = m + \\log(S')$.\n\nThis method is stable because the arguments to the exponential function, $\\eta_j' = \\eta_j - m$, are always less than or equal to $0$. The maximum argument is $0$, resulting in $\\exp(0)=1$. This prevents overflow. Furthermore, since at least one term in the sum $S'$ is exactly $1$, the sum is guaranteed to be at least $1$. This prevents the argument of the logarithm from being $0$ due to the underflow of all terms, thereby avoiding a result of $-\\infty$.\n\n**Computational Steps for Each Test Case**\nFor each case defined by $x$ and $t$:\n1.  Set $\\eta = t$.\n2.  Compute the Euclidean norm $\\|\\eta\\|_2 = \\sqrt{\\sum_{j=1}^K \\eta_j^2}$.\n3.  Compute $R_{\\text{direct}}$ using the direct method.\n4.  Compute $R_{\\text{stable}}$ using the stable method.\n5.  Obtain the reference value $R_{\\text{ref}}$ using `scipy.special.logsumexp`, which is a professionally implemented version of the stable method.\n6.  Determine if $R_{\\text{direct}}$ and $R_{\\text{stable}}$ are finite numbers.\n7.  Calculate the absolute errors $E_{\\text{direct}} = |R_{\\text{direct}} - R_{\\text{ref}}|$ and $E_{\\text{stable}} = |R_{\\text{stable}} - R_{\\text{ref}}|$. If a result is not finite, its corresponding error is reported as $+\\infty$.\n8.  Collect and format the five required values: $(\\|\\eta\\|_2, \\text{isfinite}(R_{\\text{direct}}), \\text{isfinite}(R_{\\text{stable}}), E_{\\text{direct}}, E_{\\text{stable}})$. The results from all test cases will be concatenated into a single list for the final output.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the numerical stability problem for the log-sum-exp function.\n    \"\"\"\n    \n    # Each test case is defined by p, K, x, and t.\n    # We can directly use t as eta, as per the problem's construction.\n    test_cases = {\n        'A': {\n            'p': 3, 'K': 3,\n            'x': np.array([0.5, -1.0, 2.0]),\n            't': np.array([-1.2, 0.3, 2.1]),\n        },\n        'B': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([1000.0, 1001.0, 999.0]),\n        },\n        'C': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, -1001.0, -999.0]),\n        },\n        'D': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, 0.0, 1000.0]),\n        }\n    }\n\n    all_results = []\n    \n    # The problem asks to process cases in order A, B, C, D.\n    case_order = ['A', 'B', 'C', 'D']\n    \n    for case_key in case_order:\n        case_data = test_cases[case_key]\n        t = case_data['t']\n        \n        # As per the problem description, eta_j = t_j.\n        eta = t.astype(np.float64) # Use float64 for precision.\n        \n        # 1. Compute the vector eta (already done) and its Euclidean norm.\n        norm_eta = np.linalg.norm(eta)\n        \n        # 2. Compute the direct result.\n        # This implementation follows the naive formula log(sum(exp(eta))).\n        # It is prone to overflow/underflow.\n        with np.errstate(over='ignore'): # Suppress overflow warnings for this calculation\n            exp_eta_direct = np.exp(eta)\n            sum_exp_direct = np.sum(exp_eta_direct)\n            res_direct = np.log(sum_exp_direct)\n            \n        # 3. Compute the numerically stable result.\n        # This uses the log-sum-exp trick: m + log(sum(exp(eta - m))).\n        m = np.max(eta)\n        res_stable = m + np.log(np.sum(np.exp(eta - m)))\n        \n        # 4. Use a reliable reference implementation.\n        # scipy.special.logsumexp provides a robust implementation.\n        ref_val = logsumexp(eta)\n        \n        # 5. Determine finiteness and compute absolute errors.\n        is_finite_direct = np.isfinite(res_direct)\n        is_finite_stable = np.isfinite(res_stable)\n        \n        err_direct = np.abs(res_direct - ref_val) if is_finite_direct else np.inf\n        err_stable = np.abs(res_stable - ref_val) if is_finite_stable else np.inf\n\n        # Append the five required values for the current case.\n        all_results.extend([\n            norm_eta,\n            is_finite_direct,\n            is_finite_stable,\n            err_direct,\n            err_stable\n        ])\n\n    # Final print statement in the exact required format.\n    # Convert bools to 'True'/'False' strings without quotes.\n    # Format floats to a consistent representation.\n    formatted_results = []\n    for r in all_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Using repr() to get a standard floating point representation\n            formatted_results.append(repr(r))\n            \n    # The requested format is a comma-separated list inside square brackets.\n    # map(str,...) is sufficient and clean.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3151616"}, {"introduction": "Preventing overfitting is central to building generalizable models, and regularization is the primary tool for this task. While explicit penalties like the $L_2$ norm are widely taught, this exercise [@problem_id:3151637] reveals a more subtle and powerful connection between the optimization algorithm and model complexity. By comparing the trajectory of gradient descent with early stopping to a fully converged, explicitly regularized model, you will discover that the number of training iterations itself acts as an implicit regularizer, offering a profound insight into the dynamics of modern model training.", "problem": "You will write a complete program to compare early stopping and explicit Euclidean norm squared ($L_2$) regularization when minimizing the binary logistic loss by Gradient Descent (GD). The goal is to show, by principled construction and computation, that early stopping acts as an implicit regularizer controlling the effective complexity of $x^\\top \\beta$ through the norm $\\lVert \\beta \\rVert_2$. Your program must implement the required optimization routines from first principles and produce results for a fixed test suite. The final output must be a single line string representing a Python list of boolean values.\n\nFundamental base to use. You must begin from the Bernoulli likelihood model with the logistic link for binary classification, where for an input vector $x \\in \\mathbb{R}^d$ and parameter $\\beta \\in \\mathbb{R}^d$, the conditional probability of label $y \\in \\{0,1\\}$ is $p_\\beta(y=1 \\mid x) = \\sigma(x^\\top \\beta)$ with the logistic function $\\sigma(t) = \\frac{1}{1 + e^{-t}}$. The empirical negative log-likelihood (average logistic loss) for a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is\n$$\n\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\log\\!\\left(1+e^{x_i^\\top \\beta}\\right) - y_i \\, x_i^\\top \\beta \\right].\n$$\nYou must derive and implement its gradient with respect to $\\beta$ and extend it to include explicit Euclidean norm squared ($L_2$) regularization with penalty parameter $\\lambda > 0$, that is, the penalized objective $\\ell_\\lambda(\\beta) = \\ell(\\beta) + \\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2$.\n\nOptimization design constraints. Implement batch Gradient Descent (GD) starting from $\\beta_0 = 0$, using a constant step size chosen from a provable global smoothness bound. Let $X \\in \\mathbb{R}^{n \\times d}$ be the data matrix whose rows are $x_i^\\top$. An upper bound on the Lipschitz constant of $\\nabla \\ell(\\beta)$ is\n$$\nL = \\frac{1}{4n}\\,\\lambda_{\\max}(X^\\top X),\n$$\nwhere $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue. For the penalized objective $\\ell_\\lambda(\\beta)$, an upper bound on the Lipschitz constant is $L + \\lambda$. In both cases, use the constant step size $\\eta = \\frac{1}{L + \\lambda}$, with $\\lambda = 0$ for the unpenalized early-stopping trajectory. For the penalized problem, iterate GD until convergence to a small tolerance, whereas for early stopping, halt GD after a fixed number of iterations $T$.\n\nEffective complexity and comparison protocol. Regard the Euclidean norm $\\lVert \\beta \\rVert_2$ and the induced predicted probabilities $\\hat{p}_i = \\sigma(x_i^\\top \\beta)$ as measurements of effective model complexity and fit, respectively. For a given early-stopped iterate $\\beta^{(T)}$, compare its predictions to those of the converged penalized solution $\\hat{\\beta}_\\lambda$ across a grid of $\\lambda$ values and select the $\\lambda$ minimizing the mean squared difference (average over the dataset) of predicted probabilities:\n$$\n\\text{MSE}(\\lambda;T) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\sigma(x_i^\\top \\beta^{(T)}) - \\sigma(x_i^\\top \\hat{\\beta}_\\lambda) \\right]^2.\n$$\nThis comparison operationalizes the claim that early stopping implicitly regularizes in a manner similar to explicit $L_2$ penalization.\n\nTest suite. Use the three fixed datasets below. In each case, include a column of ones as the first feature (intercept) and penalize all coordinates equally for simplicity.\n\n- Dataset A (well-conditioned, $n=5$, $d=2$):\n$$\nX_A = \\begin{bmatrix}\n1 & -2 \\\\\n1 & -1 \\\\\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2\n\\end{bmatrix},\\quad\ny_A = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1\n\\end{bmatrix}.\n$$\n\n- Dataset B (correlated features, $n=5$, $d=3$):\n$$\nX_B = \\begin{bmatrix}\n1 & 1.0 & 1.0 \\\\\n1 & 0.9 & 0.9 \\\\\n1 & 0.8 & 0.8 \\\\\n1 & -0.8 & -0.8 \\\\\n1 & -1.0 & -1.0\n\\end{bmatrix},\\quad\ny_B = \\begin{bmatrix}\n1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0\n\\end{bmatrix}.\n$$\n\n- Dataset C (linearly separable, edge case, $n=6$, $d=2$):\n$$\nX_C = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2 \\\\\n1 & -1 \\\\\n1 & -2 \\\\\n1 & -3\n\\end{bmatrix},\\quad\ny_C = \\begin{bmatrix}\n1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}.\n$$\n\nGrid of penalty values. Use the fixed grid\n$$\n\\Lambda = \\{2.0,\\, 1.0,\\, 0.5,\\, 0.2,\\, 0.1,\\, 0.05,\\, 0.02,\\, 0.01,\\, 0.005\\}.\n$$\n\nNumerical protocol to implement.\n\n- Implement functions to compute the average logistic loss $\\ell(\\beta)$ and its gradient, and the penalized objective $\\ell_\\lambda(\\beta)$ and its gradient.\n- Implement batch Gradient Descent (GD) with the step size $\\eta = \\frac{1}{L+\\lambda}$, where $L = \\frac{1}{4n}\\lambda_{\\max}(X^\\top X)$, starting at $\\beta_0 = 0$.\n- Early stopping: run exactly $T$ iterations with $\\lambda = 0$.\n- Explicit $L_2$ regularization: for each $\\lambda \\in \\Lambda$, run GD until convergence with tolerance $10^{-10}$ on the parameter update norm or a maximum of $10000$ iterations, and record the converged $\\hat{\\beta}_\\lambda$.\n- For a given dataset and $T$, compute $\\text{MSE}(\\lambda;T)$ over $\\lambda \\in \\Lambda$ and record the minimum MSE and the corresponding $\\lambda^\\star(T)$ index in the grid.\n\nRequired tests and outputs. Your program must compute the following five boolean results, in order, and print them as a single Python list on one line, with no extra text:\n\n- Test $1$ (happy path): On Dataset A with $T=40$, the minimum MSE between early-stopped and penalized predictions over $\\Lambda$ is at most $0.03$. Output a boolean indicating whether this holds.\n- Test $2$ (correlated predictors): On Dataset B with $T=20$, the minimum MSE between early-stopped and penalized predictions over $\\Lambda$ is at most $0.03$. Output a boolean indicating whether this holds.\n- Test $3$ (linearly separable edge case): On Dataset C with $T=40$, the minimum MSE between early-stopped and penalized predictions over $\\Lambda$ is at most $0.06$. Output a boolean indicating whether this holds.\n- Test $4$ (implicit complexity control via early stopping): On Dataset A with $T \\in \\{2,5,10,20,40\\}$, let $k(T)$ be the index in $\\Lambda$ (with $0$-based indexing in the given ordering) achieving the minimum $\\text{MSE}(\\lambda;T)$. Check whether the sequence $k(2),k(5),k(10),k(20),k(40)$ is monotonically nondecreasing, which corresponds to $\\lambda^\\star(T)$ being monotonically nonincreasing as $T$ grows. Output a boolean indicating whether this holds.\n- Test $5$ (norm growth with more training): On Dataset A, compare the Euclidean norms of early-stopped parameters at $T=5$ and $T=40$. Output a boolean indicating whether $\\lVert \\beta^{(40)} \\rVert_2 > \\lVert \\beta^{(5)} \\rVert_2$.\n\nAngle and physical units. There are no angles or physical quantities in this problem, so no units are required.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated Python list enclosed in square brackets, for example, \"[True,False,True,True,False]\".", "solution": "The problem requires a computational demonstration of the principle that early stopping in gradient-based optimization of a statistical model acts as an implicit form of regularization, akin to explicit $L_2$ penalization. We will formalize this for binary logistic regression, implement the necessary algorithms from first principles, and verify the relationship on a provided test suite.\n\n### 1. The Logistic Regression Model and Objective Function\n\nWe begin with the statistical model for binary classification. Given a feature vector $x \\in \\mathbb{R}^d$ and a parameter vector $\\beta \\in \\mathbb{R}^d$, the conditional probability of the label $y \\in \\{0,1\\}$ is modeled via the logistic function $\\sigma(t) = (1 + e^{-t})^{-1}$:\n$$\np_\\beta(y=1 \\mid x) = \\sigma(x^\\top \\beta)\n$$\nFor a dataset of $n$ independent observations $\\{(x_i, y_i)\\}_{i=1}^n$, the parameters $\\beta$ are typically estimated by minimizing the average negative log-likelihood, which is equivalent to the average binary cross-entropy or logistic loss:\n$$\n\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\log\\!\\left(1+e^{x_i^\\top \\beta}\\right) - y_i \\, x_i^\\top \\beta \\right]\n$$\nThis objective function is convex, ensuring that any local minimum is also a global minimum. However, for linearly separable data, the magnitude of the optimal $\\beta$ vector that perfectly separates the data is infinite, leading to overfitting and poor generalization. The predicted probabilities approach $0$ and $1$ exactly, indicating an overconfident model.\n\n### 2. Regularization Strategies\n\nTo prevent overfitting and control model complexity, we introduce regularization.\n\n**Explicit $L_2$ Regularization:** A penalty term proportional to the squared Euclidean norm of the parameters, $\\lVert \\beta \\rVert_2^2 = \\beta^\\top\\beta$, is added to the loss function. This yields the penalized objective:\n$$\n\\ell_\\lambda(\\beta) = \\ell(\\beta) + \\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2\n$$\nHere, $\\lambda > 0$ is the regularization parameter that controls the trade-off between fitting the data (minimizing $\\ell(\\beta)$) and keeping the parameter norms small. The addition of the strictly convex term $\\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2$ makes the objective $\\ell_\\lambda(\\beta)$ strictly convex, guaranteeing a unique finite minimizer, denoted $\\hat{\\beta}_\\lambda$.\n\n**Implicit Regularization via Early Stopping:** This technique involves minimizing the *unpenalized* loss $\\ell(\\beta)$ using an iterative optimization algorithm, but halting the process after a fixed number of iterations, $T$. Starting from an initial parameter value of $\\beta^{(0)} = 0$, the gradient descent algorithm generates a sequence of iterates $\\beta^{(1)}, \\beta^{(2)}, \\dots$. By stopping at iterate $\\beta^{(T)}$, the solution is prevented from growing to infinite norm, even if the data is separable. The number of iterations $T$ implicitly controls the complexity, as larger $T$ allows the parameters to move farther from the origin.\n\n### 3. Gradient-Based Optimization\n\nWe employ batch Gradient Descent (GD) to minimize the objective functions. The update rule for the parameters at step $k$ is:\n$$\n\\beta^{(k+1)} = \\beta^{(k)} - \\eta \\nabla_\\beta J(\\beta^{(k)})\n$$\nwhere $\\eta > 0$ is the step size (learning rate) and $J(\\beta)$ is the objective function.\n\n**Gradient Derivation:**\nThe gradient of the unpenalized loss $\\ell(\\beta)$ is:\n$$\n\\nabla_\\beta \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{e^{x_i^\\top \\beta}}{1+e^{x_i^\\top \\beta}} x_i - y_i x_i \\right) = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(x_i^\\top \\beta) - y_i) x_i\n$$\nIn matrix notation, where $X \\in \\mathbb{R}^{n \\times d}$ is the data matrix, $y \\in \\mathbb{R}^n$ is the label vector, and $p = \\sigma(X\\beta)$ is the vector of predicted probabilities, the gradient is:\n$$\n\\nabla_\\beta \\ell(\\beta) = \\frac{1}{n} X^\\top(p - y)\n$$\nFor the $L_2$-regularized objective $\\ell_\\lambda(\\beta)$, the gradient is:\n$$\n\\nabla_\\beta \\ell_\\lambda(\\beta) = \\nabla_\\beta \\ell(\\beta) + \\nabla_\\beta \\left(\\frac{\\lambda}{2}\\beta^\\top\\beta\\right) = \\frac{1}{n} X^\\top(p - y) + \\lambda \\beta\n$$\n\n**Step Size Selection:**\nTo guarantee convergence, the step size $\\eta$ must be chosen appropriately. For an objective function whose gradient is Lipschitz continuous with constant $L_{grad}$, GD is guaranteed to converge for any step size $\\eta \\in (0, 2/L_{grad})$. A common and safe choice is $\\eta = 1/L_{grad}$. The Hessian of $\\ell(\\beta)$ is $\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_i \\sigma(x_i^\\top\\beta)(1-\\sigma(x_i^\\top\\beta)) x_i x_i^\\top$. Since $\\sigma(t)(1-\\sigma(t)) \\le 1/4$ for all $t$, the Hessian is bounded as $\\nabla^2 \\ell(\\beta) \\preceq \\frac{1}{4n} X^\\top X$. Thus, an upper bound on the Lipschitz constant of $\\nabla \\ell(\\beta)$ is $L = \\frac{1}{4n}\\lambda_{\\max}(X^\\top X)$, where $\\lambda_{\\max}(\\cdot)$ is the maximum eigenvalue. For the penalized objective $\\ell_\\lambda(\\beta)$, the Hessian is $\\nabla^2 \\ell_\\lambda(\\beta) = \\nabla^2 \\ell(\\beta) + \\lambda I$, so its Lipschitz constant is bounded by $L + \\lambda$. We use the step size $\\eta = \\frac{1}{L+\\lambda}$ (with $\\lambda=0$ for the unpenalized case).\n\n### 4. Comparison Protocol\n\nThe core of the problem is to show that for an early-stopped solution $\\beta^{(T)}$, there exists a regularization parameter $\\lambda$ such that the fully converged regularized solution $\\hat{\\beta}_\\lambda$ is \"close\" to $\\beta^{(T)}$. We operationalize this by finding the $\\lambda$ from a predefined grid $\\Lambda$ that minimizes the mean squared error between the predicted probabilities of the two models:\n$$\n\\lambda^\\star(T) = \\underset{\\lambda \\in \\Lambda}{\\text{argmin}} \\left\\{ \\text{MSE}(\\lambda;T) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\sigma(x_i^\\top \\beta^{(T)}) - \\sigma(x_i^\\top \\hat{\\beta}_\\lambda) \\right]^2 \\right\\}\n$$\nThis protocol allows us to establish a direct correspondence between the number of training iterations $T$ and an equivalent regularization strength $\\lambda^\\star$. The provided tests will verify this relationship and other properties of the optimization trajectories.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the entire problem by implementing the logistic regression comparison\n    and running the specified test suite.\n    \"\"\"\n\n    # --- Fixed Problem Data ---\n    LAMBDA_GRID = np.array([2.0, 1.0, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005])\n\n    X_A = np.array([[1, -2], [1, -1], [1, 0], [1, 1], [1, 2]], dtype=float)\n    y_A = np.array([0, 0, 0, 1, 1], dtype=float)\n\n    X_B = np.array([[1, 1.0, 1.0], [1, 0.9, 0.9], [1, 0.8, 0.8], [1, -0.8, -0.8], [1, -1.0, -1.0]], dtype=float)\n    y_B = np.array([1, 1, 1, 0, 0], dtype=float)\n\n    X_C = np.array([[1, 0], [1, 1], [1, 2], [1, -1], [1, -2], [1, -3]], dtype=float)\n    y_C = np.array([1, 1, 1, 0, 0, 0], dtype=float)\n\n    # --- Core Implementation ---\n\n    def sigmoid(t):\n        \"\"\"Computes the logistic sigmoid function element-wise.\"\"\"\n        # Clip to avoid overflow/underflow in exp\n        t = np.clip(t, -500, 500)\n        return 1.0 / (1.0 + np.exp(-t))\n\n    def gradient_descent(X, y, lam=0.0, T=None, max_iter=10000, tol=1e-10):\n        \"\"\"\n        Performs batch gradient descent for logistic regression.\n        \n        Args:\n            X (np.ndarray): Data matrix (n, d).\n            y (np.ndarray): Label vector (n,).\n            lam (float): L2 regularization parameter.\n            T (int, optional): Number of iterations for early stopping. If None, runs to convergence.\n            max_iter (int): Maximum iterations for convergence run.\n            tol (float): Tolerance for convergence on parameter update norm.\n\n        Returns:\n            np.ndarray: The optimized parameter vector beta (d,).\n        \"\"\"\n        n, d = X.shape\n        beta = np.zeros(d)\n        \n        # Lipschitz constant of the unpenalized loss gradient\n        L = np.max(np.linalg.eigvalsh(X.T @ X)) / (4.0 * n)\n        \n        # Step size based on provable bound\n        eta = 1.0 / (L + lam)\n        \n        run_iterations = T if T is not None else max_iter\n\n        for _ in range(run_iterations):\n            beta_old = beta.copy()\n            \n            p = sigmoid(X @ beta)\n            gradient = (X.T @ (p - y)) / n + lam * beta\n            beta -= eta * gradient\n            \n            if T is None:  # Convergence check only if not early stopping\n                if np.linalg.norm(beta - beta_old) < tol:\n                    break\n        return beta\n\n    def perform_comparison(X, y, T, lambda_grid):\n        \"\"\"\n        Compares early stopping with explicit L2 regularization.\n\n        Args:\n            X (np.ndarray): Data matrix.\n            y (np.ndarray): Label vector.\n            T (int): Number of iterations for early stopping.\n            lambda_grid (np.ndarray): Grid of lambda values for L2 regularization.\n\n        Returns:\n            tuple: (minimum MSE, index of best lambda)\n        \"\"\"\n        # 1. Get early-stopped solution\n        beta_es = gradient_descent(X, y, lam=0.0, T=T)\n        p_es = sigmoid(X @ beta_es)\n\n        mses = []\n        # Pre-calculate all regularized solutions if needed elsewhere\n        # but for this function, we do it in one pass.\n        for lam in lambda_grid:\n            # 2. Get L2-regularized solution (run to convergence)\n            beta_reg = gradient_descent(X, y, lam=lam, T=None)\n            p_reg = sigmoid(X @ beta_reg)\n            \n            # 3. Compute MSE of predictions\n            mse = np.mean((p_es - p_reg)**2)\n            mses.append(mse)\n            \n        mses = np.array(mses)\n        min_mse = np.min(mses)\n        best_lambda_idx = np.argmin(mses)\n        \n        return min_mse, best_lambda_idx\n\n    results = []\n\n    # --- Test 1 ---\n    min_mse_A, _ = perform_comparison(X_A, y_A, T=40, lambda_grid=LAMBDA_GRID)\n    results.append(min_mse_A <= 0.03)\n\n    # --- Test 2 ---\n    min_mse_B, _ = perform_comparison(X_B, y_B, T=20, lambda_grid=LAMBDA_GRID)\n    results.append(min_mse_B <= 0.03)\n\n    # --- Test 3 ---\n    min_mse_C, _ = perform_comparison(X_C, y_C, T=40, lambda_grid=LAMBDA_GRID)\n    results.append(min_mse_C <= 0.06)\n\n    # --- Test 4 ---\n    T_values = [2, 5, 10, 20, 40]\n    k_indices = []\n    \n    # Pre-compute regularized solutions for Dataset A to speed up\n    betas_reg_A = [gradient_descent(X_A, y_A, lam=lam, T=None) for lam in LAMBDA_GRID]\n    ps_reg_A = [sigmoid(X_A @ beta_reg) for beta_reg in betas_reg_A]\n\n    for T_val in T_values:\n        beta_es = gradient_descent(X_A, y_A, lam=0.0, T=T_val)\n        p_es = sigmoid(X_A @ beta_es)\n        \n        mses = [np.mean((p_es - p_reg)**2) for p_reg in ps_reg_A]\n        k_indices.append(np.argmin(mses))\n        \n    is_monotonic = all(k_indices[i] <= k_indices[i+1] for i in range(len(k_indices) - 1))\n    results.append(is_monotonic)\n\n    # --- Test 5 ---\n    beta_5 = gradient_descent(X_A, y_A, lam=0.0, T=5)\n    beta_40 = gradient_descent(X_A, y_A, lam=0.0, T=40)\n    \n    norm_5 = np.linalg.norm(beta_5)\n    norm_40 = np.linalg.norm(beta_40)\n    \n    results.append(norm_40 > norm_5)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3151637"}]}