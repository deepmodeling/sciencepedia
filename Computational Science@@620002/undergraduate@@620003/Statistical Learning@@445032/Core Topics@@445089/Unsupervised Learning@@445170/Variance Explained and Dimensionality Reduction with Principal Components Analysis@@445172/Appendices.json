{"hands_on_practices": [{"introduction": "A fundamental question in PCA is deciding how many principal components are needed to represent the data without significant loss of information. This exercise provides a direct, analytical way to explore this trade-off by modeling eigenvalue spectra with a geometric decay. By solving for the number of components needed to capture a target percentage of variance [@problem_id:3191932], you will develop a concrete intuition for how the rate of eigenvalue decay—the \"eigen-gap\"—governs the effectiveness of dimensionality reduction.", "problem": "A dataset consisting of $p=50$ centered and standardized predictors is modeled using Principal Components Analysis (PCA). Let the sample covariance matrix be denoted by $\\Sigma \\in \\mathbb{R}^{p \\times p}$ with spectral decomposition $\\Sigma = U \\Lambda U^{\\top}$, where $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\lambda_{2},\\dots,\\lambda_{p})$ and $U$ contains the orthonormal eigenvectors. Assume the eigenvalues are ordered non-increasingly so that $\\lambda_{1} \\ge \\lambda_{2} \\ge \\dots \\ge \\lambda_{p} \\ge 0$. The Explained Variance Ratio (EVR) of the first $k$ principal components is defined as the fraction of total variance captured by the subspace spanned by the first $k$ eigenvectors.\n\nConsider two plausible eigen-spectra, each parameterized by a geometric decay ratio $r \\in (0,1)$ that controls the eigen-gap (i.e., $\\lambda_{i+1}/\\lambda_{i} = r$):\n- Spectrum $\\mathcal{A}$: $\\lambda_{i} = r_{\\mathcal{A}}^{\\,i-1}$ with $r_{\\mathcal{A}} = 0.80$ for $i=1,2,\\dots,50$.\n- Spectrum $\\mathcal{B}$: $\\lambda_{i} = r_{\\mathcal{B}}^{\\,i-1}$ with $r_{\\mathcal{B}} = 0.95$ for $i=1,2,\\dots,50$.\n\nStarting from the definitions of PCA and the EVR, derive the cumulative EVR as a function of $k$ for each spectrum. Then, for each spectrum, determine the minimal integer $k$ such that the cumulative EVR is at least the target level $\\tau \\in \\{0.90, 0.95, 0.99\\}$.\n\nReport the six minimal integers in the following order as your final answer: $\\big(k_{\\mathcal{A}}(0.90),\\,k_{\\mathcal{A}}(0.95),\\,k_{\\mathcal{A}}(0.99),\\,k_{\\mathcal{B}}(0.90),\\,k_{\\mathcal{B}}(0.95),\\,k_{\\mathcal{B}}(0.99)\\big)$. No rounding instruction is necessary because $k$ is an integer by definition. Express the targets as decimals (e.g., $0.90$ rather than a percentage).", "solution": "The user wants to find the minimum number of principal components, $k$, required to explain a certain proportion of total variance for two different eigenvalue spectra.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n*   Number of predictors: $p=50$.\n*   Data is centered and standardized.\n*   Sample covariance matrix: $\\Sigma \\in \\mathbb{R}^{p \\times p}$.\n*   Spectral decomposition: $\\Sigma = U \\Lambda U^{\\top}$, where $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\lambda_{2},\\dots,\\lambda_{p})$.\n*   Eigenvalue ordering: $\\lambda_{1} \\ge \\lambda_{2} \\ge \\dots \\ge \\lambda_{p} \\ge 0$.\n*   Explained Variance Ratio (EVR) definition: The fraction of total variance captured by the first $k$ principal components.\n*   Spectrum $\\mathcal{A}$: Eigenvalues are $\\lambda_{i} = r_{\\mathcal{A}}^{\\,i-1}$ with $r_{\\mathcal{A}} = 0.80$ for $i=1,2,\\dots,50$.\n*   Spectrum $\\mathcal{B}$: Eigenvalues are $\\lambda_{i} = r_{\\mathcal{B}}^{\\,i-1}$ with $r_{\\mathcal{B}} = 0.95$ for $i=1,2,\\dots,50$.\n*   Target EVR levels: $\\tau \\in \\{0.90, 0.95, 0.99\\}$.\n*   Objective: For each spectrum and each target level $\\tau$, find the minimal integer $k$ such that the cumulative EVR is at least $\\tau$.\n*   Reporting format: A vector of six integers $\\big(k_{\\mathcal{A}}(0.90),\\,k_{\\mathcal{A}}(0.95),\\,k_{\\mathcal{A}}(0.99),\\,k_{\\mathcal{B}}(0.90),\\,k_{\\mathcal{B}}(0.95),\\,k_{\\mathcal{B}}(0.99)\\big)$.\n\n**Validation Verdict:**\nThe problem is scientifically grounded in the principles of statistical learning, specifically Principal Components Analysis (PCA). It is mathematically well-posed, providing all necessary information ($p$, spectral models, target thresholds) to arrive at a unique, meaningful solution. The language is objective and precise. The problem does not violate any of the invalidity criteria. It is a standard, formalizable exercise in applying the definitions of PCA and variance explained.\n\n**Conclusion:** The problem is valid.\n\n### Step 2: Solution Derivation\n\nThe analysis begins from the fundamental definitions of PCA. The total variance in the dataset is given by the trace of the sample covariance matrix $\\Sigma$, which is equal to the sum of its eigenvalues.\n$$ V_{\\text{total}} = \\operatorname{tr}(\\Sigma) = \\sum_{i=1}^{p} \\lambda_i $$\nThe variance captured by the first $k$ principal components is the sum of the first $k$ largest eigenvalues.\n$$ V_k = \\sum_{i=1}^{k} \\lambda_i $$\nThe cumulative Explained Variance Ratio (EVR) for the first $k$ components, denoted $\\text{EVR}(k)$, is the ratio of the variance captured by the first $k$ components to the total variance.\n$$ \\text{EVR}(k) = \\frac{V_k}{V_{\\text{total}}} = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i} $$\nThe problem specifies an eigenvalue spectrum following a geometric progression: $\\lambda_i = r^{i-1}$ for $i=1, 2, \\dots, p$. The sums in the EVR expression are finite geometric series. The sum of the first $n$ terms of a geometric series with first term $a=1$ and common ratio $r$ is $\\sum_{j=0}^{n-1} ar^j = a\\frac{1-r^n}{1-r}$. In our case, $a=1$.\n\nThe numerator is the sum of the first $k$ eigenvalues:\n$$ \\sum_{i=1}^{k} \\lambda_i = \\sum_{j=0}^{k-1} r^j = \\frac{1 - r^k}{1 - r} $$\nThe denominator is the sum of all $p$ eigenvalues:\n$$ \\sum_{i=1}^{p} \\lambda_i = \\sum_{j=0}^{p-1} r^j = \\frac{1 - r^p}{1 - r} $$\nSubstituting these into the EVR formula, the $(1-r)$ terms cancel:\n$$ \\text{EVR}(k) = \\frac{\\frac{1 - r^k}{1 - r}}{\\frac{1 - r^p}{1 - r}} = \\frac{1 - r^k}{1 - r^p} $$\nThe problem requires finding the minimal integer $k$ such that $\\text{EVR}(k) \\ge \\tau$ for given values of $\\tau$. We must solve the inequality:\n$$ \\frac{1 - r^k}{1 - r^p} \\ge \\tau $$\nRearranging the terms to isolate $k$:\n$$ 1 - r^k \\ge \\tau (1 - r^p) $$\n$$ -r^k \\ge \\tau(1 - r^p) - 1 $$\n$$ r^k \\le 1 - \\tau(1 - r^p) $$\nTo solve for $k$, we take the natural logarithm of both sides. Since $r \\in (0,1)$, its logarithm $\\ln(r)$ is negative. Therefore, dividing by $\\ln(r)$ will reverse the inequality sign.\n$$ k \\ln(r) \\le \\ln\\left(1 - \\tau(1 - r^p)\\right) $$\n$$ k \\ge \\frac{\\ln\\left(1 - \\tau(1 - r^p)\\right)}{\\ln(r)} $$\nSince $k$ must be an integer, the minimal integer $k$ satisfying this condition is the smallest integer greater than or equal to the right-hand side, which is given by the ceiling function.\n$$ k = \\left\\lceil \\frac{\\ln\\left(1 - \\tau(1 - r^p)\\right)}{\\ln(r)} \\right\\rceil $$\nWe now apply this formula for each specified spectrum and target level $\\tau$, with $p=50$.\n\n**Calculations for Spectrum $\\mathcal{A}$ ($r_{\\mathcal{A}} = 0.80$):**\nThe constant term for this spectrum is $1 - r_{\\mathcal{A}}^p = 1 - (0.80)^{50}$.\n1.  For $\\tau=0.90$:\n    $$ k_{\\mathcal{A}}(0.90) = \\left\\lceil \\frac{\\ln\\left(1 - 0.90(1 - (0.80)^{50})\\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.1000128...)}{\\ln(0.80)} \\right\\rceil = \\lceil 10.318... \\rceil = 11 $$\n2.  For $\\tau=0.95$:\n    $$ k_{\\mathcal{A}}(0.95) = \\left\\lceil \\frac{\\ln\\left(1 - 0.95(1 - (0.80)^{50})\\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.0500135...)}{\\ln(0.80)} \\right\\rceil = \\lceil 13.423... \\rceil = 14 $$\n3.  For $\\tau=0.99$:\n    $$ k_{\\mathcal{A}}(0.99) = \\left\\lceil \\frac{\\ln\\left(1 - 0.99(1 - (0.80)^{50})\\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.0100141...)}{\\ln(0.80)} \\right\\rceil = \\lceil 20.630... \\rceil = 21 $$\n\n**Calculations for Spectrum $\\mathcal{B}$ ($r_{\\mathcal{B}} = 0.95$):**\nThe constant term for this spectrum is $1 - r_{\\mathcal{B}}^p = 1 - (0.95)^{50}$.\n1.  For $\\tau=0.90$:\n    $$ k_{\\mathcal{B}}(0.90) = \\left\\lceil \\frac{\\ln\\left(1 - 0.90(1 - (0.95)^{50})\\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.169250...)}{\\ln(0.95)} \\right\\rceil = \\lceil 34.628... \\rceil = 35 $$\n2.  For $\\tau=0.95$:\n    $$ k_{\\mathcal{B}}(0.95) = \\left\\lceil \\frac{\\ln\\left(1 - 0.95(1 - (0.95)^{50})\\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.123097...)}{\\ln(0.95)} \\right\\rceil = \\lceil 40.838... \\rceil = 41 $$\n3.  For $\\tau=0.99$:\n    $$ k_{\\mathcal{B}}(0.99) = \\left\\lceil \\frac{\\ln\\left(1 - 0.99(1 - (0.95)^{50})\\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.086175...)}{\\ln(0.95)} \\right\\rceil = \\lceil 47.788... \\rceil = 48 $$\n\nThe six minimal integers, in the requested order, are $(11, 14, 21, 35, 41, 48)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11 & 14 & 21 & 35 & 41 & 48\n\\end{pmatrix}\n}\n$$", "id": "3191932"}, {"introduction": "While powerful, PCA is not a black-box tool that can be applied to any dataset without thought. This practice problem [@problem_id:3191947] highlights a critical real-world consideration: the effect of categorical variables on a PCA model. By working through a carefully constructed hypothetical scenario with dummy-coded features, you will discover how these encodings can artificially inflate variance and dominate the principal components, potentially masking the true structure in your continuous data. This exercise will build your critical thinking skills regarding data preprocessing, an essential step for the valid application of PCA.", "problem": "A data analyst is studying dimensionality reduction with Principal Components Analysis (PCA) on a mixed feature set that includes a continuous feature and a categorical feature encoded as dummy variables. The data consist of $n = 6$ observations divided evenly among three groups $\\{A, B, C\\}$, encoded with two dummy variables $z_1 = \\mathbb{1}\\{G = B\\}$ and $z_2 = \\mathbb{1}\\{G = C\\}$, where $G$ denotes group membership. The continuous feature $x$ is independent of group membership and has small variability. The observations are as follows (each observation is a row vector $(x, z_1, z_2)$):\n- Group $A$: $(x, z_1, z_2) = (-0.1, 0, 0)$, $(0.1, 0, 0)$\n- Group $B$: $(x, z_1, z_2) = (-0.1, 1, 0)$, $(0.1, 1, 0)$\n- Group $C$: $(x, z_1, z_2) = (-0.1, 0, 1)$, $(0.1, 0, 1)$\n\nThe analyst performs PCA under the standard convention for PCA in statistical learning: columns are centered (subtracting empirical means), and principal components are obtained as the orthonormal directions that maximize projected variance, equivalently the eigenvectors of the covariance (second-moment) matrix of the centered data. Explained variance of a principal component is its associated eigenvalue, and the explained variance ratio is the eigenvalue divided by the sum of all eigenvalues.\n\nThe analyst then considers two scenarios:\n- Scenario $1$: PCA is applied to the centered matrix with columns $(x, z_1, z_2)$.\n- Scenario $2$: The continuous feature $x$ is demeaned within groups (so $x$ is replaced by $x - \\bar{x}_{G}$, where $\\bar{x}_{G}$ is the mean of $x$ within the group of the observation), and PCA is applied either to the residual $x$ alone (dropping the dummy columns) or to the residual $x$ together with the centered dummy columns.\n\nWhich of the following statements are correct?\n\nA. In Scenario $1$, the first principal component aligns with a contrast between groups $B$ and $C$ (approximately proportional to $z_1 - z_2$), and its explained variance ratio is greater than $0.7$.\n\nB. If the columns were not centered before PCA, the dummy variables would contribute no variance because they are binary, so they could not dominate any principal component.\n\nC. In Scenario $2$, if PCA is applied to the residual $x$ alone (dropping $z_1$ and $z_2$), the first principal component explains approximately $100\\%$ of the remaining variance.\n\nD. In Scenario $2$, if $x$ is demeaned within groups but the centered dummy columns are kept, the largest eigenvalue remains dominated by the dummy subspace (approximately $1/3$), so the first principal component is still aligned with the categorical variable.\n\nE. If, instead of two dummy variables, the categorical feature were encoded as a single contrast-coded column $w$ with values $+1$ for group $B$, $-1$ for group $C$, and $0$ for group $A$, then after centering the variance of $w$ would be $2/9$, so the largest eigenvalue would decrease relative to the two-dummy encoding.", "solution": "The user wants me to solve a statistical learning problem about Principal Components Analysis (PCA).\n\n### Step 1: Extract Givens\n-   Number of observations: $n = 6$.\n-   Number of groups: $3$ (labeled $\\{A, B, C\\}$), with $2$ observations per group.\n-   Features: one continuous feature $x$, and one categorical feature $G$ encoded by two dummy variables $z_1 = \\mathbb{1}\\{G = B\\}$ and $z_2 = \\mathbb{1}\\{G = C\\}$.\n-   Data matrix with rows $(x, z_1, z_2)$:\n    -   Group $A$ ($2$ obs): $(-0.1, 0, 0)$, $(0.1, 0, 0)$\n    -   Group $B$ ($2$ obs): $(-0.1, 1, 0)$, $(0.1, 1, 0)$\n    -   Group $C$ ($2$ obs): $(-0.1, 0, 1)$, $(0.1, 0, 1)$\n-   PCA procedure: columns are centered by subtracting their empirical means. PCs are eigenvectors of the covariance matrix of the centered data.\n-   Covariance matrix definition: \"second-moment matrix of the centered data,\" which implies a divisor of $n$.\n-   Scenario $1$: PCA on the centered matrix with columns $(x, z_1, z_2)$.\n-   Scenario $2$: The feature $x$ is replaced by its within-group demeaned version, $x_{resid} = x - \\bar{x}_{G}$. Then, PCA is applied to either (a) $x_{resid}$ alone, or (b) $x_{resid}$ with the centered dummy columns.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem uses PCA, a standard and well-established technique in statistics and machine learning. The context of analyzing mixed continuous and categorical data is a common and valid application area.\n-   **Well-Posed**: The data is explicitly provided, and the procedures (centering, PCA, demeaning) are defined. The problem is self-contained and provides enough information to calculate a unique solution.\n-   **Objective**: The problem is stated in precise, objective language. The data is numerical, and the tasks are computational and interpretive within a standard framework.\n-   **No Flaws Found**: The problem does not violate scientific principles, is not incomplete or contradictory, is not based on unrealistic premises, and is not ill-posed. The description of $x$ as having \"small variability\" and being \"independent of group membership\" is consistent with the provided data, where the distribution of $x$ is identical across groups and its variance is small relative to the variance contributed by the dummy variables.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed with the solution derivation.\n\n### Analysis for Scenario 1\nFirst, we construct the full data matrix $X$ with columns corresponding to $x$, $z_1$, and $z_2$.\n$$\nX = \\begin{pmatrix}\n-0.1 & 0 & 0 \\\\\n0.1 & 0 & 0 \\\\\n-0.1 & 1 & 0 \\\\\n0.1 & 1 & 0 \\\\\n-0.1 & 0 & 1 \\\\\n0.1 & 0 & 1\n\\end{pmatrix}\n$$\nThe PCA procedure begins with centering the columns. We calculate the mean of each column:\n-   $\\bar{x} = \\frac{1}{6}(-0.1+0.1-0.1+0.1-0.1+0.1) = 0$\n-   $\\bar{z}_1 = \\frac{1}{6}(0+0+1+1+0+0) = \\frac{2}{6} = \\frac{1}{3}$\n-   $\\bar{z}_2 = \\frac{1}{6}(0+0+0+0+1+1) = \\frac{2}{6} = \\frac{1}{3}$\n\nThe centered data matrix, $X_c$, has columns $x_c = x-\\bar{x}$, $z_{1c} = z_1-\\bar{z}_1$, and $z_{2c} = z_2-\\bar{z}_2$.\n$$\nX_c = \\begin{pmatrix}\n-0.1 & -1/3 & -1/3 \\\\\n0.1 & -1/3 & -1/3 \\\\\n-0.1 & 2/3 & -1/3 \\\\\n0.1 & 2/3 & -1/3 \\\\\n-0.1 & -1/3 & 2/3 \\\\\n0.1 & -1/3 & 2/3\n\\end{pmatrix}\n$$\nNext, we compute the covariance matrix $S = \\frac{1}{n} X_c^T X_c$. First, we find the dot products for $X_c^T X_c$:\n-   $\\sum x_{ci}^2 = 6 \\times (0.1)^2 = 0.06$\n-   $\\sum z_{1ci}^2 = 4 \\times (-1/3)^2 + 2 \\times (2/3)^2 = 4/9 + 8/9 = 12/9 = 4/3$\n-   $\\sum z_{2ci}^2 = 4 \\times (-1/3)^2 + 2 \\times (2/3)^2 = 12/9 = 4/3$\n-   $\\sum x_{ci} z_{1ci} = 0$ (due to symmetry of $x$ values within each group)\n-   $\\sum x_{ci} z_{2ci} = 0$ (for the same reason)\n-   $\\sum z_{1ci} z_{2ci} = 2 \\times (-1/3)(-1/3) + 2 \\times (2/3)(-1/3) + 2 \\times (-1/3)(2/3) = 2/9 - 4/9 - 4/9 = -6/9 = -2/3$\n\nSo, $X_c^T X_c = \\begin{pmatrix} 0.06 & 0 & 0 \\\\ 0 & 4/3 & -2/3 \\\\ 0 & -2/3 & 4/3 \\end{pmatrix}$.\nThe covariance matrix is $S = \\frac{1}{6} X_c^T X_c$:\n$$\nS = \\begin{pmatrix} 0.01 & 0 & 0 \\\\ 0 & \\frac{4}{18} & \\frac{-2}{18} \\\\ 0 & \\frac{-2}{18} & \\frac{4}{18} \\end{pmatrix} = \\begin{pmatrix} 0.01 & 0 & 0 \\\\ 0 & 2/9 & -1/9 \\\\ 0 & -1/9 & 2/9 \\end{pmatrix}\n$$\nThe eigenvalues and eigenvectors of $S$ determine the principal components. Since $S$ is block-diagonal, one eigenvector is $(1, 0, 0)^T$ with eigenvalue $\\lambda_x = 0.01$. The other two are determined by the submatrix for the dummy variables, $S_z = \\begin{pmatrix} 2/9 & -1/9 \\\\ -1/9 & 2/9 \\end{pmatrix}$.\nThe characteristic equation for $S_z$ is $(2/9 - \\lambda)^2 - (-1/9)^2 = 0$, which gives $2/9 - \\lambda = \\pm 1/9$. The eigenvalues are therefore $\\lambda = 2/9 \\mp 1/9$.\n-   Largest eigenvalue: $\\lambda_1 = 2/9 + 1/9 = 3/9 = 1/3$.\n-   Second largest eigenvalue: $\\lambda_2 = 2/9 - 1/9 = 1/9$.\nThe eigenvalues of $S$ in descending order are $\\lambda_1 = 1/3$, $\\lambda_2 = 1/9$, and $\\lambda_3 = 0.01$.\n\nThe eigenvector for $\\lambda_1 = 1/3$ is found from $(S_z - (1/3)I)v = 0$, which gives $\\begin{pmatrix} -1/9 & -1/9 \\\\ -1/9 & -1/9 \\end{pmatrix}v = 0$. This implies the eigenvector is proportional to $(1, -1)^T$. The full, normalized PC loading vector is $(0, 1/\\sqrt{2}, -1/\\sqrt{2})^T$.\n\nThe total variance is the trace of $S$, $\\operatorname{Tr}(S) = \\sum \\lambda_i = 1/3 + 1/9 + 0.01 = 4/9 + 1/100 = \\frac{400+9}{900} = \\frac{409}{900}$.\nThe explained variance ratio for the first PC is $\\frac{\\lambda_1}{\\operatorname{Tr}(S)} = \\frac{1/3}{409/900} = \\frac{1}{3} \\times \\frac{900}{409} = \\frac{300}{409} \\approx 0.7335$.\n\n### Analysis for Scenario 2\nIn this scenario, $x$ is replaced by $x_{resid} = x - \\bar{x}_{G}$. We must compute the within-group means for $x$:\n-   Group A: $\\bar{x}_A = \\frac{-0.1 + 0.1}{2} = 0$.\n-   Group B: $\\bar{x}_B = \\frac{-0.1 + 0.1}{2} = 0$.\n-   Group C: $\\bar{x}_C = \\frac{-0.1 + 0.1}{2} = 0$.\nSince all within-group means are $0$, the residual vector is identical to the original $x$ vector: $x_{resid} = x$.\n\n**Scenario 2a**: PCA is applied to $x_{resid}$ alone.\nThis means performing PCA on the single column vector $x = (-0.1, 0.1, -0.1, 0.1, -0.1, 0.1)^T$. This is a one-dimensional dataset. In one dimension, there is only one principal component, and it must, by definition, capture all ($100\\%$) of the variance of the data. The variance of this data is $\\lambda_3 = 0.01$, which is non-zero.\n\n**Scenario 2b**: PCA is applied to $x_{resid}$ together with the centered dummy columns.\nThe problem states that we apply PCA to the data formed by \"the residual $x$ together with the centered dummy columns\". Let's assume the standard PCA procedure is then applied to these specified columns. The columns are $(x_{resid}, z_1, z_2)$. Since $x_{resid} = x$, this set of columns is $(x, z_1, z_2)$. The PCA procedure requires centering these columns, which is exactly the procedure for Scenario $1$. Thus, the PCA results (covariance matrix, eigenvalues, eigenvectors) for Scenario $2$b are identical to those for Scenario $1$.\n\n### Option-by-Option Analysis\n\n**A. In Scenario $1$, the first principal component aligns with a contrast between groups $B$ and $C$ (approximately proportional to $z_1 - z_2$), and its explained variance ratio is greater than $0.7$.**\nThe first PC loading vector is proportional to $(0, 1, -1)^T$, which corresponds to the score $z_1 - z_2$ (when applied to the uncentered dummy variables for interpretation). This score is $1$ for group B, $-1$ for group C, and $0$ for group A, representing a contrast between B and C. The calculated explained variance ratio is $300/409 \\approx 0.7335$, which is greater than $0.7$.\n**Verdict: Correct.**\n\n**B. If the columns were not centered before PCA, the dummy variables would contribute no variance because they are binary, so they could not dominate any principal component.**\nThis statement contains multiple falsehoods. First, a binary variable that is not constant has non-zero variance. For instance, the empirical variance of $z_1$ is $\\bar{z}_1(1-\\bar{z}_1) = (1/3)(2/3) = 2/9 \\neq 0$. Second, PCA on uncentered data considers the second moment about the origin, not the variance (second central moment). The sum of squares for the $z_1$ column is $1^2+1^2=2$, and for $z_2$ is $1^2+1^2=2$. For the $x$ column, it's $6 \\times (0.1)^2 = 0.06$. The $z$ columns have much larger second moments than the $x$ column and would therefore dominate the first principal components.\n**Verdict: Incorrect.**\n\n**C. In Scenario $2$, if PCA is applied to the residual $x$ alone (dropping $z_1$ and $z_2$), the first principal component explains approximately $100\\%$ of the remaining variance.**\nAs determined in the analysis of Scenario 2a, PCA on a single column of data (a 1D dataset) results in a single principal component. This component explains exactly $100\\%$ of the variance in that data. The term \"remaining variance\" refers to the variance of the residual vector $x_{resid}$, which is what is being analyzed. Saying it explains \"approximately $100\\%$\" is a true statement.\n**Verdict: Correct.**\n\n**D. In Scenario $2$, if $x$ is demeaned within groups but the centered dummy columns are kept, the largest eigenvalue remains dominated by the dummy subspace (approximately $1/3$), so the first principal component is still aligned with the categorical variable.**\nAs shown in the analysis of Scenario 2b, the PCA procedure in this case is identical to that in Scenario $1$ because $x_{resid} = x$. Therefore, all results are the same. The largest eigenvalue is exactly $1/3$, which is \"approximately $1/3$\". Its corresponding eigenvector, $(0, 1/\\sqrt{2}, -1/\\sqrt{2})^T$, lies entirely in the subspace spanned by the centered dummy variables, so the statement \"dominated by the dummy subspace\" and \"aligned with the categorical variable\" is correct. The properties \"remain\" and are \"still\" the same as in Scenario 1.\n**Verdict: Correct.**\n\n**E. If, instead of two dummy variables, the categorical feature were encoded as a single contrast-coded column $w$ with values $+1$ for group $B$, $-1$ for group $C$, and $0$ for group $A$, then after centering the variance of $w$ would be $2/9$, so the largest eigenvalue would decrease relative to the two-dummy encoding.**\nThe new column $w$ is $(0, 0, 1, 1, -1, -1)^T$. Its mean is $\\bar{w}=0$, so it is already centered. The sum of squares is $\\sum w_i^2 = 0+0+1+1+1+1=4$. The variance (using the $1/n$ formula) is $\\text{Var}(w) = 4/6 = 2/3$. The statement claims the variance is $2/9$, which is false. For the second part, PCA would be on the columns $(x, w)$. Since $\\sum x_i w_i = 0$, the covariance matrix would be diagonal with entries $\\text{Var}(x)=0.01$ and $\\text{Var}(w)=2/3$. The new largest eigenvalue would be $2/3 \\approx 0.667$. The largest eigenvalue in the original two-dummy encoding was $1/3 \\approx 0.333$. Thus, the largest eigenvalue would *increase*, not decrease.\n**Verdict: Incorrect.**\n\nThe correct statements are A, C, and D.", "answer": "$$\\boxed{ACD}$$", "id": "3191947"}, {"introduction": "Moving from theoretical models to empirical data, how do we decide if an observed eigenvalue is \"large enough\" to represent a real signal rather than random noise? This hands-on coding challenge [@problem_id:3191949] introduces parallel analysis, a robust, simulation-based method to answer exactly that question. By implementing this procedure, you will create a statistical tool that generates a null distribution for eigenvalues from random data, allowing you to establish a principled threshold for retaining components. This exercise bridges the gap between PCA theory and its practical, data-driven application in hypothesis testing.", "problem": "You are to implement a complete, runnable program that performs a parallel analysis for Principal Components Analysis (PCA) and uses it to make component-wise hypothesis tests about the presence of low-dimensional structure beyond independent Gaussian noise. The derivation and the algorithm must be grounded in the following base definitions and facts: Principal Components Analysis (PCA) of a centered data matrix $X \\in \\mathbb{R}^{n \\times p}$ proceeds by forming the unbiased sample covariance $S = \\frac{1}{n-1} X^{\\top} X$ and then computing its eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$, where $\\mathrm{trace}(S) = \\sum_{i=1}^p \\lambda_i$ equals the total sample variance. Under a null model in which features are independent and identically distributed standard normal, i.e., each entry of $Z \\in \\mathbb{R}^{n \\times p}$ is drawn from $\\mathcal{N}(0,1)$ independently, the sample covariance eigenvalues are those of a Wishart matrix and fluctuate around $1$ due to sampling variability. Parallel analysis is a resampling-based procedure that estimates, for each component index $i$, a threshold $t_i$ as an empirical percentile of the null distribution of the $i$-th largest eigenvalue, and then compares the observed $\\lambda_i$ to $t_i$ to decide whether to retain component $i$. You must express any proportion or percentile as a decimal in $[0,1]$.\n\nImplement the following hypothesis testing scenario for each test case. For each $i \\in \\{1,\\dots,p\\}$, test the null hypothesis $H_{0,i}$ that there is no signal in component $i$ beyond independent standard normal noise, versus the alternative $H_{1,i}$ that there is signal. Use the following decision rule: compute $t_i$ as the empirical $q$-th quantile (with $q \\in (0,1)$) of the null distribution of the $i$-th largest eigenvalue, estimated by Monte Carlo with $m$ null simulations. Reject $H_{0,i}$ if and only if $\\lambda_i > t_i$. The number of retained components is therefore the count of indices $i$ for which this strict inequality holds.\n\nYour program must:\n- For the observed data: construct $X \\in \\mathbb{R}^{n \\times p}$ by first sampling a base matrix $W$ with entries independently drawn from $\\mathcal{N}(0,1)$ using a specified pseudorandom seed, and then applying a diagonal “spike” to the first $r$ features to encode a signal. That is, for a given nonnegative integer $r \\le p$ and a nonnegative real number $s$, multiply the first $r$ columns of $W$ by $\\sqrt{1+s}$ to obtain $X$. Center the columns of $X$ by subtracting the column means before computing $S = \\frac{1}{n-1} X^{\\top} X$ and its eigenvalues $\\lambda_1 \\ge \\cdots \\ge \\lambda_p$.\n- For the null simulations: for each of $m$ replicates, generate $Z \\in \\mathbb{R}^{n \\times p}$ with entries independently drawn from $\\mathcal{N}(0,1)$ using a specified pseudorandom seed, center the columns of $Z$ by subtracting the column means, form $S_0 = \\frac{1}{n-1} Z^{\\top} Z$, compute the eigenvalues of $S_0$, sort them in descending order, and record the resulting vector. After $m$ replicates, compute, for each component index $i$, the empirical $q$-th quantile across the $m$ recorded values at that index to obtain $t_i$.\n- Count the number of indices $i$ such that $\\lambda_i > t_i$ and output that count as an integer for the test case.\n\nDesign choices you must follow:\n- Use the same centering and covariance formula for observed and null data so that thresholds are calibrated to the observed estimator.\n- Use a strict inequality $\\lambda_i > t_i$ for rejection, so that $\\lambda_i = t_i$ does not reject.\n- Use the specified seeds for reproducibility and the NumPy pseudorandom number generator interface that allows explicit seeding.\n\nTest suite. Implement and run your program on the following three test cases. Each case specifies $(n,p,r,s,q,m,\\text{seed}_{\\text{obs}},\\text{seed}_{\\text{null}})$:\n- Case A (noise-only, high percentile): $(n=\\,$$300$$,\\,p=\\,$$10$$,\\,r=\\,$$0$$,\\,s=\\,$$0.0$$,\\,q=\\,$$0.99$$,\\,m=\\,$$2000$$,\\,\\text{seed}_{\\text{obs}}=\\,$$20240501$$,\\,\\text{seed}_{\\text{null}}=\\,$$424242$$)$.\n- Case B (clear low-rank signal): $(n=\\,$$300$$,\\,p=\\,$$10$$,\\,r=\\,$$3$$,\\,s=\\,$$4.0$$,\\,q=\\,$$0.95$$,\\,m=\\,$$1000$$,\\,\\text{seed}_{\\text{obs}}=\\,$$20240502$$,\\,\\text{seed}_{\\text{null}}=\\,$$424242$$)$.\n- Case C (boundary-scale spike): $(n=\\,$$80$$,\\,p=\\,$$20$$,\\,r=\\,$$1$$,\\,s=\\,$$1.0$$,\\,q=\\,$$0.95$$,\\,m=\\,$$2000$$,\\,\\text{seed}_{\\text{obs}}=\\,$$20240503$$,\\,\\text{seed}_{\\text{null}}=\\,$$777$$)$.\n\nOutput specification. Your program must produce exactly one line on standard output consisting of a left square bracket, the three integer results for the test cases in order A, B, C, separated by commas with no spaces, and then a right square bracket. For example, if the results for cases A, B, C were $k_A$, $k_B$, and $k_C$, then the single line must be the characters “[” followed by $k_A$, then “,”, then $k_B$, then “,”, then $k_C$, then “]”, with no additional characters, spaces, or lines.", "solution": "The problem requires the implementation of a statistical procedure known as parallel analysis to determine the number of significant principal components in a dataset. This procedure is framed as a component-wise hypothesis test. The solution involves generating a specific \"spiked\" dataset, calculating its principal component eigenvalues, and then comparing these eigenvalues to thresholds derived from a Monte Carlo simulation under a null model of pure noise.\n\n**1. Scientific Principles and Context**\n\nPrincipal Components Analysis (PCA) is a cornerstone of dimensionality reduction in statistics and machine learning. Given a data matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n$ observations and $p$ features, which is assumed to be centered (column means are zero), PCA seeks a lower-dimensional representation by finding a new set of orthogonal axes, called principal components. These components are ordered such that the first component captures the largest possible variance in the data, the second captures the maximum remaining variance, and so on.\n\nMathematically, the principal components are the eigenvectors of the sample covariance matrix, $S = \\frac{1}{n-1} X^{\\top} X$. The variance captured by each component is given by the corresponding eigenvalue. Let the eigenvalues of $S$ be sorted in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$. The total variance in the data is $\\mathrm{trace}(S) = \\sum_{i=1}^p \\lambda_i$.\n\nA critical question in applying PCA is how many components to retain. Retaining too few may discard important signal, while retaining too many may incorporate noise, defeating the purpose of dimensionality reduction. Parallel analysis addresses this by establishing a baseline for eigenvalue magnitudes that would be expected purely from random sampling variability.\n\nThe null hypothesis, $H_{0,i}$ for each component $i$, is that there is no structure or signal in the $i$-th component beyond what would be seen in a dataset composed of independent and identically distributed (IID) Gaussian noise. Specifically, the null model posits that the data matrix is $Z \\in \\mathbb{R}^{n \\times p}$, where each entry $Z_{jk} \\sim \\mathcal{N}(0,1)$. The eigenvalues of the covariance matrix of such a noise matrix follow a known theoretical distribution (the Marchenko-Pastur law in the asymptotic limit, and Wishart distribution for finite samples), but are typically simulated empirically for practical application.\n\nParallel analysis compares the observed eigenvalues $\\lambda_i$ from the actual data to the distribution of eigenvalues from the null model. A component is deemed significant if its eigenvalue is statistically larger than what's expected from noise. The problem formalizes this comparison as a hypothesis test: for each component $i$, we reject $H_{0,i}$ if $\\lambda_i > t_i$, where $t_i$ is a high percentile (e.g., the $q=0.95$ quantile) of the simulated null distribution for the $i$-th eigenvalue. The number of rejected hypotheses gives the number of components to retain.\n\n**2. Algorithmic Procedure**\n\nThe implementation follows a sequence of well-defined steps for each test case, which is specified by the parameters $(n, p, r, s, q, m, \\text{seed}_{\\text{obs}}, \\text{seed}_{\\text{null}})$.\n\n**Step 2.1: Analysis of Observed Data**\n\n1.  **Generate Data Matrix:** An initial matrix $W \\in \\mathbb{R}^{n \\times p}$ is generated, with each entry drawn independently from a standard normal distribution, $\\mathcal{N}(0,1)$. This process is seeded with $\\text{seed}_{\\text{obs}}$ for reproducibility.\n2.  **Introduce Signal (Spike Model):** To simulate a low-rank signal structure, the first $r$ columns of $W$ are scaled by a factor of $\\sqrt{1+s}$. This yields the observed data matrix $X$. If $r=0$ or $s=0$, this step introduces no signal, and $X=W$.\n3.  **Center Data:** The columns of $X$ are centered by subtracting their respective means. Let this centered matrix be denoted $X_c$. For any column $j$, $X_{c, \\cdot j} = X_{\\cdot j} - \\bar{X}_{\\cdot j}$, where $\\bar{X}_{\\cdot j} = \\frac{1}{n}\\sum_{i=1}^n X_{ij}$.\n4.  **Compute Eigenvalues:** The unbiased sample covariance matrix is computed as $S = \\frac{1}{n-1} X_c^{\\top} X_c$. The eigenvalues of $S$ are then calculated and sorted in descending order to produce the vector of observed eigenvalues, $(\\lambda_1, \\lambda_2, \\dots, \\lambda_p)$.\n\n**Step 2.2: Simulation of the Null Distribution**\n\n1.  **Initialize Monte Carlo Simulation:** An array is prepared to store the eigenvalues from $m$ simulations, resulting in a matrix of size $m \\times p$. A separate random number generator is initialized with $\\text{seed}_{\\text{null}}$.\n2.  **Replication Loop:** The following process is repeated $m$ times:\n    a. A null data matrix $Z \\in \\mathbb{R}^{n \\times p}$ is generated, with entries drawn IID from $\\mathcal{N}(0,1)$.\n    b. The matrix $Z$ is centered to produce $Z_c$.\n    c. The sample covariance matrix $S_0 = \\frac{1}{n-1} Z_c^{\\top} Z_c$ is formed.\n    d. The eigenvalues of $S_0$ are computed, sorted in descending order, and stored as one row in the results array.\n\n**Step 2.3: Hypothesis Testing and Final Count**\n\n1.  **Calculate Thresholds:** For each component $i \\in \\{1,\\dots,p\\}$, the $q$-th empirical quantile, $t_i$, is calculated from the set of $m$ simulated eigenvalues for that component. This yields a vector of thresholds $(t_1, t_2, \\dots, t_p)$.\n2.  **Compare and Count:** Each observed eigenvalue $\\lambda_i$ is compared to its corresponding threshold $t_i$. The number of retained components is the total count of indices $i$ for which the strict inequality $\\lambda_i > t_i$ holds. This integer count is the final result for the test case.\n\nThis procedure is applied to each of the three test cases provided.\n-   **Case A** ($n=300, p=10, r=0, s=0.0, q=0.99, m=2000, \\text{seed}_{\\text{obs}}=20240501, \\text{seed}_{\\text{null}}=424242$): A noise-only case. With $q=0.99$, we expect very few, if any, components to be retained by chance (approximately $1\\%$ of the $p=10$ components, so likely $0$ or $1$).\n-   **Case B** ($n=300, p=10, r=3, s=4.0, q=0.95, m=1000, \\text{seed}_{\\text{obs}}=20240502, \\text{seed}_{\\text{null}}=424242$): A strong signal case with $r=3$ spiked components. The spike strength $s=4.0$ is substantial, so we expect the analysis to correctly identify these $3$ components.\n-   **Case C** ($n=80, p=20, r=1, s=1.0, q=0.95, m=2000, \\text{seed}_{\\text{obs}}=20240503, \\text{seed}_{\\text{null}}=777$): A boundary-scale case with a single, weaker spike ($r=1, s=1.0$) and a smaller sample size $n=80$ relative to the number of features $p=20$. This tests the detection power of the method in a more challenging scenario.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_parallel_analysis(n, p, r, s, q, m, seed_obs, seed_null):\n    \"\"\"\n    Performs a parallel analysis for a single test case.\n\n    Args:\n        n (int): Number of observations.\n        p (int): Number of features.\n        r (int): Number of 'spiked' features with signal.\n        s (float): Signal strength parameter for the spike.\n        q (float): Quantile to use for the threshold.\n        m (int): Number of Monte Carlo simulations for the null distribution.\n        seed_obs (int): Pseudorandom seed for generating observed data.\n        seed_null (int): Pseudorandom seed for generating null data.\n\n    Returns:\n        int: The number of components retained based on the parallel analysis.\n    \"\"\"\n    # Initialize the random number generator for observed data\n    rng_obs = np.random.default_rng(seed_obs)\n\n    # Step 1: Analyze observed data\n    # Generate base matrix W from N(0,1)\n    W = rng_obs.standard_normal((n, p))\n    \n    # Create the observed data matrix X by spiking the first r columns\n    X = W.copy()\n    if r > 0:\n        signal_factor = np.sqrt(1 + s)\n        X[:, :r] *= signal_factor\n        \n    # Center the data\n    X_centered = X - X.mean(axis=0)\n    \n    # Compute the sample covariance matrix\n    # S = (X_centered.T @ X_centered) / (n - 1)\n    # Using np.cov is robust and standard. It expects variables in rows,\n    # so we transpose X_centered. `ddof=1` for unbiased estimator.\n    S = np.cov(X_centered, rowvar=False, ddof=1)\n    \n    # Compute eigenvalues of S. eigvalsh is for Hermitian (real-symmetric) matrices.\n    # It returns eigenvalues in ascending order, so we reverse them.\n    observed_eigenvalues = np.linalg.eigvalsh(S)[::-1]\n\n    # Step 2: Simulate the null distribution\n    # Initialize the random number generator for null simulations\n    rng_null = np.random.default_rng(seed_null)\n    \n    null_eigenvalues_matrix = np.zeros((m, p))\n    \n    for i in range(m):\n        # Generate null data matrix Z from N(0,1)\n        Z = rng_null.standard_normal((n, p))\n        \n        # Center the null data\n        Z_centered = Z - Z.mean(axis=0)\n        \n        # Compute the null sample covariance matrix\n        S_null = np.cov(Z_centered, rowvar=False, ddof=1)\n        \n        # Compute and store sorted eigenvalues\n        null_eigenvalues_matrix[i, :] = np.linalg.eigvalsh(S_null)[::-1]\n\n    # Step 3: Calculate thresholds and make decisions\n    # Compute the q-th quantile for each component across all simulations\n    thresholds = np.quantile(null_eigenvalues_matrix, q, axis=0)\n    \n    # Count the number of observed eigenvalues that exceed the threshold\n    retained_components_count = np.sum(observed_eigenvalues > thresholds)\n    \n    return retained_components_count\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite: (n, p, r, s, q, m, seed_obs, seed_null)\n    test_cases = [\n        # Case A: Noise-only, high percentile\n        (300, 10, 0, 0.0, 0.99, 2000, 20240501, 424242),\n        # Case B: Clear low-rank signal\n        (300, 10, 3, 4.0, 0.95, 1000, 20240502, 424242),\n        # Case C: Boundary-scale spike\n        (80, 20, 1, 1.0, 0.95, 2000, 20240503, 777),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_parallel_analysis(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format \"[k_A,k_B,k_C]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191949"}]}