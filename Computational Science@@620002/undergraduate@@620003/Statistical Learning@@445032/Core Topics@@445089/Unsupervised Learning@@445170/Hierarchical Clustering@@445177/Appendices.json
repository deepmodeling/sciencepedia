{"hands_on_practices": [{"introduction": "Before any clustering can begin, preparing your data is a critical first step. This practice explores how hierarchical clustering, particularly with variance-based linkage like Ward's method, is highly sensitive to the scale of your input features. By dissecting how features with large variances can dominate the clustering process, you will see how standardization neutralizes this effect, leading to potentially more meaningful clusters [@problem_id:3129004]. This exercise builds crucial intuition about the inner workings of Ward's method and the absolute necessity of thoughtful data preprocessing.", "problem": "You are given a collection of data matrices and asked to study the effect of feature scaling on agglomerative hierarchical clustering with Ward’s minimum variance criterion. The fundamental base for this study is the definition of within-cluster dispersion and the increase in within-cluster sum of squares induced by merging two clusters. Let $X \\in \\mathbb{R}^{n \\times d}$ denote a data matrix with $n$ samples and $d$ features. Define the mean of a cluster $C \\subset \\{1,\\dots,n\\}$ on feature $j$ as $\\mu_{C,j} = \\frac{1}{|C|}\\sum_{i \\in C} x_{ij}$. For two disjoint clusters $A$ and $B$, the Ward merge height (the increase in within-cluster sum of squares due to merging $A$ and $B$) is\n$$\n\\Delta(A,B;X) = \\frac{|A|\\,|B|}{|A|+|B|}\\sum_{j=1}^d \\left( \\mu_{A,j} - \\mu_{B,j} \\right)^2.\n$$\nFeature standardization is defined columnwise by subtracting the feature mean and dividing by the population standard deviation. For feature $j$, set $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}$ and $\\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_{ij}-\\bar{x}_j)^2}$. The standardized data matrix $Z \\in \\mathbb{R}^{n \\times d}$ is defined by $z_{ij} = \\frac{x_{ij}-\\bar{x}_j}{\\sigma_j}$ when $\\sigma_j > 0$, and $z_{ij} = 0$ for all $i$ when $\\sigma_j = 0$ (constant feature columns cannot be rescaled to unit variance and are set to zero after centering).\n\nTasks:\n- For each given test matrix $X$, perform agglomerative hierarchical clustering using Ward’s method on the raw matrix $X$, obtaining the sequence of merges. Use this sequence of merges to compute, for each merge step $t$, the raw merge height $\\Delta_t^{\\text{raw}} = \\Delta(A_t,B_t;X)$ and the standardized merge height computed on the same pair of clusters $\\Delta_t^{\\text{std}} = \\Delta(A_t,B_t;Z)$, where $Z$ is the standardized version of $X$ (using the rule above) and $(A_t,B_t)$ are the clusters merged at step $t$ according to the raw-data Ward procedure.\n- Quantify the change in merge heights due to standardization by computing the scalar\n$$\nS_{\\text{total}} = \\sum_{t=1}^{n-1} \\left| \\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}} \\right|.\n$$\n- Identify the variables driving these changes. For each merge step $t$, decompose the height difference into featurewise contributions:\n$$\nc_{j}(t) = \\frac{|A_t|\\,|B_t|}{|A_t|+|B_t|}\\left[\\left(\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}}\\right)^2 - \\left(\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}}\\right)^2\\right],\n$$\nwhere $\\mu_{C,j}^{\\text{raw}}$ and $\\mu_{C,j}^{\\text{std}}$ are cluster means computed on $X$ and $Z$, respectively. Aggregate absolute contributions across all merges for each feature $j$:\n$$\nS_j = \\sum_{t=1}^{n-1} \\left| c_j(t) \\right|.\n$$\nDefine the set of driving variables as the indices $j$ achieving the maximum $S_j$ (include all ties), reported as zero-based indices in ascending order.\n\nYour program must implement the above computation for the following test suite of data matrices. Each matrix is specified explicitly. Ensure numerical plausibility and internal consistency, and use the exact values below.\n\nTest suite:\n- Test $1$ ($n=6$, $d=3$):\n$$\nX^{(1)} = \\begin{bmatrix}\n100 & 0 & 0.10 \\\\\n102 & 0.50 & 0.20 \\\\\n98 & -0.20 & 0.00 \\\\\n0 & 5 & -0.10 \\\\\n3 & 4.50 & -0.30 \\\\\n-2 & 5.20 & 0.00\n\\end{bmatrix}.\n$$\n- Test $2$ ($n=5$, $d=3$), with a constant first column:\n$$\nX^{(2)} = \\begin{bmatrix}\n10 & -1 & 50 \\\\\n10 & 0 & 51 \\\\\n10 & 1 & 49 \\\\\n10 & 2 & 48 \\\\\n10 & -2 & 52\n\\end{bmatrix}.\n$$\n- Test $3$ ($n=8$, $d=4$), with disparate scales:\n$$\nX^{(3)} = \\begin{bmatrix}\n100.0 & 0.0 & 0.10 & 10000.0 \\\\\n101.5 & -0.5 & 0.05 & 10002.0 \\\\\n98.5 & 0.3 & 0.15 & 9998.0 \\\\\n100.8 & -0.2 & 0.12 & 10001.0 \\\\\n0.0 & 5.0 & -0.10 & 10050.0 \\\\\n-1.5 & 4.5 & -0.05 & 10052.0 \\\\\n2.0 & 5.2 & -0.12 & 10049.0 \\\\\n-0.8 & 4.8 & -0.08 & 10051.5\n\\end{bmatrix}.\n$$\n- Test $4$ ($n=3$, $d=2$), minimal case:\n$$\nX^{(4)} = \\begin{bmatrix}\n0 & 0 \\\\\n10 & 0 \\\\\n0 & 10\n\\end{bmatrix}.\n$$\n\nOutput specification:\n- For each test matrix $X^{(k)}$, compute $S_{\\text{total}}^{(k)}$ and the set of driving variable indices $J^{(k)} = \\{ j : S_j \\text{ is maximal} \\}$.\n- Round each $S_{\\text{total}}^{(k)}$ to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[S_{\\text{total}}^{(k)},[j_1,j_2,\\dots]]$ with zero-based feature indices in ascending order. For example: $[[s_1,[j_{1,1},j_{1,2}]], [s_2,[j_{2,1}]], [s_3,[j_{3,1},j_{3,2},j_{3,3}]], [s_4,[j_{4,1}]]]$. No spaces are permitted anywhere in the output string.\n\nConstraints and notes:\n- Use Euclidean geometry in $\\mathbb{R}^d$ and Ward’s minimum variance criterion based solely on within-cluster sum of squares. The merge sequence must be obtained by applying Ward’s method to the raw data matrix $X$, and the standardized heights must be evaluated on the same merge sequence.\n- All computations are purely numerical with no physical units involved. Angles do not apply.\n- Ensure robust handling of constant feature columns where $\\sigma_j = 0$; per the definition above, these columns become zeros after centering and do not contribute to standardized differences.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-defined and self-contained computational task in statistical learning, grounded in established principles of hierarchical clustering and data standardization. The problem statement is free from scientific unsoundness, ambiguity, or contradictions.\n\n### Principle-Based Solution Design\n\nThe problem requires an analysis of how feature standardization affects the merge heights in agglomerative hierarchical clustering using Ward's method. The solution is structured around the following core principles and computational steps.\n\n1.  **Feature Standardization**: The first step is to transform the raw data matrix $X \\in \\mathbb{R}^{n \\times d}$ into a standardized matrix $Z \\in \\mathbb{R}^{n \\times d}$. This is a common preprocessing step in many machine learning algorithms to ensure that features with larger scales do not dominate the analysis. The specified transformation for each feature $j$ is $z_{ij} = (x_{ij} - \\bar{x}_j) / \\sigma_j$, where $\\bar{x}_j$ is the feature mean and $\\sigma_j$ is the population standard deviation. This process centers each feature to have a mean of $0$ and scales it to have a standard deviation of $1$. A crucial detail is the handling of constant features where $\\sigma_j = 0$. For such features, the problem dictates that the standardized values $z_{ij}$ are set to $0$ for all samples $i$. This is logical, as a constant feature provides no information for distinguishing between samples, and after centering (subtracting its own mean), it becomes a column of zeros.\n\n2.  **Ward's Hierarchical Clustering**: The clustering process is performed on the **raw data matrix** $X$. Ward's minimum variance method is a form of agglomerative clustering. It starts with each data point as its own cluster and, at each step, merges the pair of clusters that results in the minimum increase in the total within-cluster sum of squares (ESS). The total ESS is the sum of squared distances between each point and the centroid of its assigned cluster. The increase in ESS upon merging two clusters $A$ and $B$ is given by the formula for the Ward merge height:\n    $$\n    \\Delta(A,B;X) = \\frac{|A|\\,|B|}{|A|+|B|}\\sum_{j=1}^d \\left( \\mu_{A,j} - \\mu_{B,j} \\right)^2\n    $$\n    where $|C|$ is the size of cluster $C$ and $\\mu_{C,j}$ is the mean of cluster $C$ for feature $j$. This formula represents the squared Euclidean distance between the centroids of the merging clusters, weighted by a factor that depends on their sizes. Applying this method yields a sequence of $n-1$ merges, $(A_t, B_t)_{t=1}^{n-1}$, which defines the hierarchy.\n\n3.  **Comparative Analysis of Merge Heights**: The core of the analysis is to compare the merge heights calculated on the raw data $X$ versus the standardized data $Z$. Crucially, the sequence of merges is determined once from the raw data $X$. For each merge step $t$ in this fixed sequence, two quantities are computed:\n    -   The raw merge height, $\\Delta_t^{\\text{raw}} = \\Delta(A_t, B_t; X)$.\n    -   The standardized merge height, $\\Delta_t^{\\text{std}} = \\Delta(A_t, B_t; Z)$. This is computed using the same formula but with cluster means derived from the standardized data $Z$.\n\n4.  **Quantification of Change**: The effect of standardization is quantified through two aggregate metrics:\n    -   $S_{\\text{total}} = \\sum_{t=1}^{n-1} | \\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}} |$: This scalar measures the total absolute change in merge heights across the entire clustering process.\n    -   $S_j = \\sum_{t=1}^{n-1} | c_j(t) |$: This metric decomposes the change by feature. The term $c_j(t)$ isolates the contribution of feature $j$ to the height difference at merge step $t$. Summing the absolute contributions over all merges gives a measure of the total impact of feature $j$ on the height discrepancies. A feature with a large $S_j$ is one whose scaling significantly alters the inter-cluster distances, thus being identified as a \"driving variable\" of the change. The set of driving variables is defined as the feature indices corresponding to the maximum value of $S_j$.\n\n### Algorithmic Implementation\n\nThe solution is implemented by following these steps for each provided test matrix:\n\n1.  Read the input matrix $X$ of size $n \\times d$.\n2.  Compute the standardized matrix $Z$. For each column $j$, calculate the mean $\\bar{x}_j$ and population standard deviation $\\sigma_j$. If $\\sigma_j > 0$, compute $z_{ij}$ as specified. If $\\sigma_j = 0$, set the $j$-th column of $Z$ to zeros.\n3.  Perform Ward's hierarchical clustering on $X$ using `scipy.cluster.hierarchy.linkage` with `method='ward'`. This returns an $(n-1) \\times 4$ linkage matrix $L$, which encodes the sequence of merges.\n4.  Initialize $S_{\\text{total}} = 0$ and a vector $S_j$ of zeros with length $d$.\n5.  Iterate through each merge $t=1, \\dots, n-1$ as described in the linkage matrix $L$:\n    a. Identify the two clusters, $A_t$ and $B_t$, being merged at this step. This is done by recursively tracing the cluster indices from the linkage matrix back to the original data points.\n    b. Compute the sizes $|A_t|$ and $|B_t|$ and the coefficient $w_t = \\frac{|A_t|\\,|B_t|}{|A_t|+|B_t|}$.\n    c. Calculate the centroid vectors for both clusters using the raw data ($\\mu_{A_t}^{\\text{raw}}, \\mu_{B_t}^{\\text{raw}}$) and the standardized data ($\\mu_{A_t}^{\\text{std}}, \\mu_{B_t}^{\\text{std}}$).\n    d. Compute the raw and standardized merge heights:\n       $\\Delta_t^{\\text{raw}} = w_t \\sum_{j=1}^d (\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}})^2$\n       $\\Delta_t^{\\text{std}} = w_t \\sum_{j=1}^d (\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}})^2$\n    e. Update the total change: $S_{\\text{total}} \\leftarrow S_{\\text{total}} + |\\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}}|$.\n    f. Compute the feature-wise contributions: $c_j(t) = w_t [(\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}})^2 - (\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}})^2]$.\n    g. Update the feature-wise aggregate changes: $S_j \\leftarrow S_j + |c_j(t)|$ for all $j=1, \\dots, d$.\n6.  After iterating through all merges, find the maximum value in the $S_j$ vector.\n7.  Identify all feature indices $j$ for which $S_j$ is equal to this maximum value (using a small tolerance for floating-point comparison). These are the driving variables.\n8.  Format the final result, consisting of the rounded $S_{\\text{total}}$ and the sorted list of driving variable indices, as specified.\n\nThis systematic procedure ensures all quantities are calculated according to their definitions, leading to the correct and verifiable final answer.", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [100, 0, 0.10],\n            [102, 0.50, 0.20],\n            [98, -0.20, 0.00],\n            [0, 5, -0.10],\n            [3, 4.50, -0.30],\n            [-2, 5.20, 0.00]\n        ], dtype=float),\n        np.array([\n            [10, -1, 50],\n            [10, 0, 51],\n            [10, 1, 49],\n            [10, 2, 48],\n            [10, -2, 52]\n        ], dtype=float),\n        np.array([\n            [100.0, 0.0, 0.10, 10000.0],\n            [101.5, -0.5, 0.05, 10002.0],\n            [98.5, 0.3, 0.15, 9998.0],\n            [100.8, -0.2, 0.12, 10001.0],\n            [0.0, 5.0, -0.10, 10050.0],\n            [-1.5, 4.5, -0.05, 10052.0],\n            [2.0, 5.2, -0.12, 10049.0],\n            [-0.8, 4.8, -0.08, 10051.5]\n        ], dtype=float),\n        np.array([\n            [0, 0],\n            [10, 0],\n            [0, 10]\n        ], dtype=float)\n    ]\n\n    results = [_solve_one_case(X) for X in test_cases]\n    \n    # Format the output string to remove spaces\n    print(str(results).replace(\" \", \"\"))\n\ndef _solve_one_case(X):\n    \"\"\"\n    Solves the problem for a single data matrix X.\n    \"\"\"\n    n, d = X.shape\n\n    # 1. Standardize data matrix X to get Z\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)  # Population std dev (ddof=0 is default)\n    \n    # Use np.divide for safe division, setting output to 0 where std is 0\n    Z = np.divide(X - means, stds, out=np.zeros_like(X, dtype=float), where=stds!=0)\n\n    # 2. Perform Ward's clustering on raw data X\n    L = linkage(X, method='ward', metric='euclidean')\n\n    # 3. Initialize collectors for S_total and S_j\n    S_total = 0.0\n    S_j = np.zeros(d, dtype=float)\n    \n    # Cache for cluster point indices\n    # clusters[i] stores the list of original point indices for cluster i\n    clusters = {i: [i] for i in range(n)}\n    \n    # 4. Iterate through the n-1 merges from the linkage matrix\n    for t in range(n - 1):\n        # Identify merged clusters by their indices\n        c1_idx, c2_idx = int(L[t, 0]), int(L[t, 1])\n        \n        # Retrieve the original data point indices for each cluster\n        A_points = clusters[c1_idx]\n        B_points = clusters[c2_idx]\n        \n        # Create the new merged cluster and add it to the cache\n        new_cluster_idx = n + t\n        clusters[new_cluster_idx] = A_points + B_points\n        \n        nA, nB = len(A_points), len(B_points)\n        coeff = (nA * nB) / (nA + nB)\n        \n        # Calculate cluster means (centroids) for raw and standardized data\n        mu_A_raw = np.mean(X[A_points, :], axis=0)\n        mu_B_raw = np.mean(X[B_points, :], axis=0)\n        \n        mu_A_std = np.mean(Z[A_points, :], axis=0)\n        mu_B_std = np.mean(Z[B_points, :], axis=0)\n        \n        # Calculate merge heights (increase in ESS)\n        delta_raw = coeff * np.sum((mu_A_raw - mu_B_raw)**2)\n        delta_std = coeff * np.sum((mu_A_std - mu_B_std)**2)\n        \n        # Accumulate total change\n        S_total += np.abs(delta_std - delta_raw)\n        \n        # Calculate feature-wise contributions to the height difference\n        diff_sq_std = (mu_A_std - mu_B_std)**2\n        diff_sq_raw = (mu_A_raw - mu_B_raw)**2\n        c_t = coeff * (diff_sq_std - diff_sq_raw)\n        \n        # Accumulate absolute feature-wise contributions\n        S_j += np.abs(c_t)\n        \n    # 5. Identify driving variables (indices of max S_j)\n    max_Sj = np.max(S_j)\n    # Use a tolerance for floating-point comparison\n    driving_vars = np.where(np.isclose(S_j, max_Sj))[0].tolist()\n    \n    return [round(S_total, 6), driving_vars]\n\nsolve()\n```", "id": "3129004"}, {"introduction": "The heart of any clustering algorithm is its definition of \"distance\" or \"dissimilarity,\" a choice that fundamentally shapes the results. This exercise directly contrasts two of the most common metrics: Euclidean distance, which is sensitive to the magnitude and absolute position of data points, and cosine distance, which focuses purely on the angle or direction between vectors. By quantitatively comparing the resulting dendrograms using standard evaluation techniques, you will learn to select a distance metric that aligns with the notion of similarity most relevant to your specific analytical goal [@problem_id:3129024].", "problem": "You are given two ways to define dissimilarity between observations represented as vectors in a real vector space: the Euclidean distance and the cosine distance. In hierarchical clustering, a dendrogram is a rooted tree obtained by successively merging clusters based on a linkage rule; in this problem, the linkage rule is average linkage. A dendrogram induces cophenetic distances between leaves, defined as the height at which two leaves first join in the tree. The goodness-of-fit of a dendrogram to the original dissimilarities is commonly evaluated by the Pearson product-moment correlation coefficient (PPMCC) between the original pairwise dissimilarities and the cophenetic distances. Additionally, given two dendrograms on the same leaf set, the Robinson–Foulds (RF) distance between the dendrograms is defined in terms of their cluster splits and quantifies their structural discrepancy.\n\nStarting from the following fundamental base:\n- A metric space is a pair $(\\mathcal{X}, d)$ where $d$ is a distance function satisfying non-negativity, identity of indiscernibles, symmetry, and the triangle inequality.\n- A hierarchical agglomerative clustering procedure constructs a rooted binary tree by iteratively merging the two clusters with the minimal dissimilarity according to a linkage rule, starting from singleton clusters. In average linkage, the dissimilarity between two clusters is the average of pairwise distances between elements of the clusters.\n- The cophenetic distance between two leaves is the height at which they are first joined in the dendrogram. The cophenetic correlation coefficient is the PPMCC between the vector of original pairwise distances and the vector of cophenetic distances induced by the dendrogram, both arranged in the same condensed order.\n- The Robinson–Foulds distance between two rooted trees on the same leaf set is the size of the symmetric difference between the sets of nontrivial cluster splits (all internal-node leaf sets excluding singletons and the full set).\n\nTask:\n1. For each test case described below, construct two dendrograms using hierarchical agglomerative clustering with average linkage: one from Euclidean distances and one from cosine distances.\n2. Compute the cophenetic correlation coefficient for each dendrogram with respect to its original dissimilarity vector (Euclidean for the Euclidean dendrogram, cosine for the cosine dendrogram).\n3. Compute the Robinson–Foulds distance between the Euclidean dendrogram and the cosine dendrogram built on the same set of leaves.\n4. Return, for each test case, a list containing three values: the Euclidean cophenetic correlation coefficient (as a float), the cosine cophenetic correlation coefficient (as a float), and the Robinson–Foulds distance (as an integer).\n\nAngle unit specification: whenever angles appear, they are specified in degrees. Your program must internally treat the given angles as degrees when generating vectors (if needed), though actual computations may convert to radians.\n\nTest suite (each test case is a set of points in $\\mathbb{R}^2$):\n- Test case A (directionally separated clusters):\n  - Points on ray $R_1$: $[1, 0]$, $[2, 0]$, $[3, 0]$.\n  - Points on ray $R_2$: $[\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2}]$, $[1, \\sqrt{3}]$, $[\\tfrac{3}{2}, \\tfrac{3\\sqrt{3}}{2}]$.\n- Test case B (feature scaling distortion):\n  - Take all points from Test case A and multiply the first coordinate of every point by $10$.\n- Test case C (nearly collinear rays with large magnitude differences):\n  - Let $\\theta_A = 44^\\circ$ and $\\theta_B = 46^\\circ$.\n  - Define unit direction vectors $u_A = [\\cos(\\theta_A), \\sin(\\theta_A)]$ and $u_B = [\\cos(\\theta_B), \\sin(\\theta_B)]$.\n  - Points: $1 \\cdot u_A$, $2 \\cdot u_A$, $4 \\cdot u_A$, $10 \\cdot u_B$, $11 \\cdot u_B$, $12 \\cdot u_B$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The element corresponding to each test case must itself be a list of the form $[c_{\\text{eucl}}, c_{\\text{cos}}, r]$, where $c_{\\text{eucl}}$ is the Euclidean cophenetic correlation coefficient as a float, $c_{\\text{cos}}$ is the cosine cophenetic correlation coefficient as a float, and $r$ is the Robinson–Foulds distance as an integer. For example, an output with three test cases should look like $[[c_1^{\\text{eucl}}, c_1^{\\text{cos}}, r_1],[c_2^{\\text{eucl}}, c_2^{\\text{cos}}, r_2],[c_3^{\\text{eucl}}, c_3^{\\text{cos}}, r_3]]$.", "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed scientific problem. The definitions, data, and tasks are clear, self-contained, and grounded in established principles of statistical learning and numerical computation. All necessary information is provided.\n\n### Principle-Based Solution\n\nThe objective is to compare the outcomes of hierarchical agglomerative clustering when applied with two different dissimilarity measures—Euclidean distance and cosine distance—on three distinct datasets. The comparison is performed using two quantitative metrics: the cophenetic correlation coefficient (CPCC), which measures the fidelity of a dendrogram to the original dissimilarities, and the Robinson-Foulds (RF) distance, which measures the topological difference between two dendrograms.\n\nThe solution proceeds systematically for each test case as follows:\n\n1.  **Computation of Dissimilarity Matrices**:\n    For each test case, we begin with a set of $N$ points in $\\mathbb{R}^2$, represented as an $N \\times 2$ matrix $X$. Two separate pairwise dissimilarity matrices are computed.\n    -   **Euclidean Distance**: This is the standard $L_2$ norm of the difference between two vectors. For points $p_i = (p_{i1}, p_{i2})$ and $p_j = (p_{j1}, p_{j2})$, the Euclidean distance is $d_E(p_i, p_j) = \\sqrt{(p_{i1} - p_{j1})^2 + (p_{i2} - p_{j2})^2}$. This metric is sensitive to both the magnitude and the absolute positions of the vectors in the space.\n    -   **Cosine Distance**: This metric measures the cosine of the angle between two vectors, making it invariant to their magnitudes (i.e., scaling the vectors by a positive constant does not change their cosine distance). It is defined as $d_C(p_i, p_j) = 1 - \\frac{p_i \\cdot p_j}{\\|p_i\\| \\|p_j\\|}$, where $\\|p\\|$ denotes the Euclidean norm of $p$. A cosine distance of $0$ implies the vectors point in the same direction, while a distance of $1$ implies they are orthogonal.\n\n    These dissimilarity matrices are computed in a condensed 1D array format, as required by scientific computing libraries like SciPy. This format stores the upper-triangular part of the symmetric dissimilarity matrix.\n\n2.  **Hierarchical Agglomerative Clustering**:\n    Using the computed dissimilarity matrices, we perform hierarchical clustering. The chosen algorithm is agglomerative with the 'average' linkage criterion. This procedure starts by treating each of the $N$ points as its own cluster. In each subsequent step, the two clusters with the minimum dissimilarity are merged. For average linkage, the dissimilarity between two clusters $C_a$ and $C_b$ is defined as the average of all pairwise dissimilarities between points in the two clusters:\n    $$d(C_a, C_b) = \\frac{1}{|C_a||C_b|} \\sum_{p_i \\in C_a, p_j \\in C_b} d(p_i, p_j)$$\n    This process is repeated $N-1$ times until all points are contained within a single cluster. The result is a dendrogram, a tree structure representing the nested merging of clusters. The output of the clustering algorithm is a linkage matrix, denoted as $Z$, which encodes the sequence of merges and the dissimilarity (height) at which each merge occurred. We generate two such matrices: $Z_{\\text{Eucl}}$ from the Euclidean distances and $Z_{\\text{cos}}$ from the cosine distances.\n\n3.  **Cophenetic Correlation Coefficient (CPCC) Calculation**:\n    The CPCC evaluates how well the dendrogram preserves the original pairwise dissimilarities of the data points. First, we compute the cophenetic distance matrix from the dendrogram. The cophenetic distance $d_{\\text{coph}}(p_i, p_j)$ between two points $p_i$ and $p_j$ is the height in the dendrogram at which they are first merged into the same cluster. This height corresponds to the linkage distance between the two sub-clusters that were merged.\n    The CPCC is then calculated as the Pearson product-moment correlation coefficient between the elements of the original dissimilarity matrix and the corresponding elements of the cophenetic distance matrix. Let $d$ be the vector of original dissimilarities and $d_{\\text{coph}}$ be the vector of cophenetic distances, both in condensed form. The CPCC, $c$, is:\n    $$c = \\frac{\\sum_{i<j} (d_{ij} - \\bar{d}) (d_{\\text{coph}, ij} - \\bar{d}_{\\text{coph}})}{\\sqrt{\\sum_{i<j} (d_{ij} - \\bar{d})^2 \\sum_{i<j} (d_{\\text{coph}, ij} - \\bar{d}_{\\text{coph}})^2}}$$\n    A CPCC value close to $1$ indicates a high-fidelity representation of the original dissimilarities by the dendrogram. We compute two coefficients: $c_{\\text{Eucl}}$ for the Euclidean-based dendrogram and $c_{\\text{cos}}$ for the cosine-based dendrogram.\n\n4.  **Robinson-Foulds (RF) Distance Calculation**:\n    The RF distance provides a measure of the topological dissimilarity between the two dendrograms, $T_{\\text{Eucl}}$ and $T_{\\text{cos}}$. It is defined based on the concept of \"cluster splits\". In a rooted tree with $N$ leaves, each internal node corresponds to a cluster containing a subset of the leaves. This subset of leaves is called a split. The problem specifies \"nontrivial splits,\" which are those containing more than one leaf and fewer than all $N$ leaves.\n    Let $S(T)$ be the set of all nontrivial splits for a tree $T$. The RF distance is the size of the symmetric difference between the split sets of the two trees:\n    $$d_{\\text{RF}}(T_{\\text{Eucl}}, T_{\\text{cos}}) = |S(T_{\\text{Eucl}}) \\Delta S(T_{\\text{cos}})| = |S(T_{\\text{Eucl}}) \\setminus S(T_{\\text{cos}})| + |S(T_{\\text{cos}}) \\setminus S(T_{\\text{Eucl}})|$$\n    To compute this, we first extract the set of all nontrivial splits from each linkage matrix ($Z_{\\text{Eucl}}$ and $Z_{\\text{cos}}$). A split is represented as a `frozenset` of leaf indices. An RF distance of $0$ indicates that the two dendrograms have identical branching structures.\n\nThe entire procedure is encapsulated into a function that takes a data matrix, performs all the above steps, and returns the three required values: $c_{\\text{Eucl}}$, $c_{\\text{cos}}$, and the integer RF distance $r$. This function is then applied to each of the three test cases specified in the problem.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, cophenet\n\ndef get_splits(Z: np.ndarray) -> set:\n    \"\"\"\n    Computes the set of nontrivial splits from a linkage matrix.\n\n    A split is the set of leaf indices in a cluster formed at an internal node.\n    Nontrivial splits exclude singletons and the full set of leaves.\n\n    Args:\n        Z: The linkage matrix from scipy.cluster.hierarchy.linkage.\n\n    Returns:\n        A set of frozensets, where each frozenset contains the integer indices\n        of the leaves in a nontrivial split.\n    \"\"\"\n    N = Z.shape[0] + 1\n    # The clusters dictionary maps a node index to the set of leaf indices under it.\n    # Leaf nodes (0 to N-1) are initialized with their own index.\n    # Internal nodes (N to 2N-2) are added during the iteration.\n    clusters = {i: frozenset({i}) for i in range(N)}\n    splits = set()\n    \n    # Each row in Z represents an internal node and a merge operation.\n    for i in range(N - 1):\n        # Indices of the two clusters being merged.\n        c1_idx = int(Z[i, 0])\n        c2_idx = int(Z[i, 1])\n        \n        # Retrieve the leaf sets for the clusters being merged.\n        leaves1 = clusters[c1_idx]\n        leaves2 = clusters[c2_idx]\n        \n        # Form the new cluster by taking the union of the leaf sets.\n        new_cluster_leaves = leaves1.union(leaves2)\n        \n        # The new cluster corresponds to a new internal node.\n        # Its index is N + i.\n        new_cluster_idx = N + i\n        clusters[new_cluster_idx] = new_cluster_leaves\n        \n        # A split is nontrivial if it is not a singleton (guaranteed by merge)\n        # and not the full set of all leaves.\n        if len(new_cluster_leaves) < N:\n            splits.add(new_cluster_leaves)\n            \n    return splits\n\ndef solve_case(points: np.ndarray) -> list:\n    \"\"\"\n    Performs clustering and evaluation for a single test case.\n\n    Args:\n        points: A NumPy array of shape (N, D) representing the data points.\n\n    Returns:\n        A list containing [c_eucl, c_cos, rf_dist].\n    \"\"\"\n    # Euclidean-based analysis\n    dist_eucl = pdist(points, 'euclidean')\n    Z_eucl = linkage(dist_eucl, method='average')\n    c_eucl, _ = cophenet(Z_eucl, dist_eucl)\n    splits_eucl = get_splits(Z_eucl)\n    \n    # Cosine-based analysis\n    dist_cos = pdist(points, 'cosine')\n    # Handle potential NaN from pdist if a zero-vector is present. This is not\n    # expected for the given test cases but is good practice.\n    if np.any(np.isnan(dist_cos)):\n        dist_cos = np.nan_to_num(dist_cos)\n    Z_cos = linkage(dist_cos, method='average')\n    c_cos, _ = cophenet(Z_cos, dist_cos)\n    splits_cos = get_splits(Z_cos)\n    \n    # Robinson-Foulds distance calculation\n    rf_dist = len(splits_eucl.symmetric_difference(splits_cos))\n    \n    return [c_eucl, c_cos, rf_dist]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print the results.\n    \"\"\"\n    # Test Case A: Directionally separated clusters\n    case_a = np.array([\n        [1.0, 0.0], [2.0, 0.0], [3.0, 0.0],\n        [0.5, np.sqrt(3)/2], [1.0, np.sqrt(3)], [1.5, 3*np.sqrt(3)/2]\n    ])\n\n    # Test Case B: Feature scaling distortion\n    case_b = case_a.copy()\n    case_b[:, 0] *= 10\n\n    # Test Case C: Nearly collinear rays with large magnitude differences\n    theta_A_deg = 44.0\n    theta_B_deg = 46.0\n    theta_A_rad = np.deg2rad(theta_A_deg)\n    theta_B_rad = np.deg2rad(theta_B_deg)\n    u_A = np.array([np.cos(theta_A_rad), np.sin(theta_A_rad)])\n    u_B = np.array([np.cos(theta_B_rad), np.sin(theta_B_rad)])\n    case_c = np.array([\n        1.0 * u_A, 2.0 * u_A, 4.0 * u_A,\n        10.0 * u_B, 11.0 * u_B, 12.0 * u_B\n    ])\n\n    test_cases = [case_a, case_b, case_c]\n    \n    results = []\n    for case in test_cases:\n        result = solve_case(case)\n        results.append(result)\n\n    # Format the output string to match the specified format without extra spaces.\n    case_strings = []\n    for res in results:\n        # Format each list [float, float, int] into a string \"[f,f,i]\"\n        case_strings.append(f\"[{res[0]:.10f},{res[1]:.10f},{res[2]}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3129024"}, {"introduction": "After building a dendrogram, two key questions immediately arise: \"How many clusters should I choose?\" and \"How good is this clustering?\" The silhouette score provides a robust and intuitive method to answer both. This practice involves cutting a dendrogram at various levels to produce different numbers of clusters, denoted by $k$, and then calculating the average silhouette score for each partition [@problem_id:3129027]. Mastering this technique will equip you with a standard, quantitative method for selecting an optimal number of clusters and assessing the overall quality and coherence of your final solution.", "problem": "You are given finite sets of points in the Euclidean plane and asked to perform agglomerative hierarchical clustering with average linkage, then evaluate cluster quality via the silhouette coefficient across multiple cut levels. Your task is to implement a program that, for each specified dataset and a given maximum number of clusters, computes the average silhouette over cut levels and selects the number of clusters that maximizes this average.\n\nFundamental base and definitions to be used:\n- Euclidean distance: For points $x_{i} \\in \\mathbb{R}^{2}$ and $x_{j} \\in \\mathbb{R}^{2}$, define the distance $d(i,j) = \\lVert x_{i} - x_{j} \\rVert_{2}$.\n- Agglomerative hierarchical clustering with average linkage: Starting from singleton clusters, repeatedly merge the pair of clusters with the smallest average pairwise distance between their members until a single cluster remains. This induces a binary dendrogram.\n- Cutting the dendrogram: For an integer $k \\geq 2$, a “cut” that yields exactly $k$ clusters is obtained by selecting a horizontal cut level in the dendrogram that produces $k$ connected components. Use the criterion that directly enforces exactly $k$ clusters.\n- Silhouette for point $i$: For a clustering into disjoint nonempty clusters $\\{C_{1},\\dots,C_{k}\\}$ on points $\\{x_{1},\\dots,x_{n}\\}$, let $C(i)$ denote the cluster containing $i$. Define\n  - Intra-cluster dissimilarity $a(i)$ as the average of $d(i,j)$ over all $j \\in C(i)$ with $j \\neq i$. If $\\lvert C(i) \\rvert = 1$, set $a(i) = 0$.\n  - For any other cluster $C' \\neq C(i)$, define the cross-cluster average dissimilarity from $i$ to $C'$ as the average of $d(i,j)$ over all $j \\in C'$. Let $b(i)$ be the minimum of these averages over all $C' \\neq C(i)$.\n  - The silhouette of $i$ is\n    $$ s(i) = \\begin{cases}\n    0, & \\text{if } \\lvert C(i) \\rvert = 1, \\\\\n    \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}, & \\text{otherwise, with the convention } s(i)=0 \\text{ if } a(i)=b(i)=0.\n    \\end{cases} $$\n- Average silhouette at cut level $k$: $\\bar{s}(k) = \\dfrac{1}{n} \\sum_{i=1}^{n} s(i)$, where $n$ is the number of points.\n- Selection rule: For each dataset, evaluate $\\bar{s}(k)$ for all integers $k$ in $\\{2,3,\\dots, \\min\\{k_{\\max}, n\\}\\}$. Let $k^{\\star}$ be any $k$ achieving the maximum average silhouette. In case of ties within absolute tolerance $\\varepsilon = 10^{-12}$, choose the smallest such $k$.\n\nInput to be embedded in your program (no external input is allowed):\n- You must use Euclidean distance and average linkage for hierarchical clustering and produce exactly $k$ clusters at each cut.\n- Test suite with three datasets, each specified by a list of planar coordinates and a parameter $k_{\\max}$:\n  - Dataset A (well-separated two-cluster structure), with $n = 12$ points and $k_{\\max} = 5$:\n    - Points:\n      $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$, $(0.5,0)$, $(0,0.5)$,\n      $(5,5)$, $(5,6)$, $(6,5)$, $(6,6)$, $(5.5,5)$, $(5,5.5)$.\n  - Dataset B (well-separated three-cluster structure), with $n = 15$ points and $k_{\\max} = 6$:\n    - Points:\n      $(-6,0)$, $(-6,0.2)$, $(-6,-0.2)$, $(-5.8,0)$, $(-6.2,0)$,\n      $(0,0)$, $(0,0.2)$, $(0,-0.2)$, $(0.2,0)$, $(-0.2,0)$,\n      $(6,0)$, $(6,0.2)$, $(6,-0.2)$, $(5.8,0)$, $(6.2,0)$.\n  - Dataset C (degenerate identical points), with $n = 5$ points and $k_{\\max} = 4$:\n    - Points:\n      $(10,-3)$, $(10,-3)$, $(10,-3)$, $(10,-3)$, $(10,-3)$.\n\nProgram requirements:\n- For each dataset, compute $\\bar{s}(k)$ for all integer $k$ in $\\{2,\\dots,\\min\\{k_{\\max}, n\\}\\}$ using the definitions above, then select $k^{\\star}$ according to the tie-breaking rule with tolerance $\\varepsilon = 10^{-12}$.\n- Your program should produce a single line of output containing the selected $k^{\\star}$ for each dataset, in order A, B, C, as a comma-separated list enclosed in square brackets. For example, a valid output format is like $[2,3,2]$.\n- The answer values are integers.\n\nNote: No physical units or angles are involved. All numerical comparisons for tie-breaking must use the absolute tolerance $\\varepsilon = 10^{-12}$ as specified. Ensure that your implementation is consistent with the definitions stated above.", "solution": "The user-provided problem is valid. It is scientifically grounded in the established principles of statistical learning, specifically agglomerative hierarchical clustering and cluster validation using the silhouette coefficient. All terms, such as Euclidean distance ($d(i,j)$), average linkage, and the silhouette score ($s(i)$), are mathematically well-defined. The problem is self-contained, providing all necessary data and constraints, including the datasets, the clustering parameters, the range of clusters to evaluate ($k$), and a precise tie-breaking rule. The computational task is feasible and leads to a unique, verifiable solution for each dataset.\n\nThe solution is implemented by following a structured, multi-step process for each dataset provided.\n\n**Step 1: Agglomerative Hierarchical Clustering**\n\nFor each dataset, we begin by computing the pairwise distances between all points. The problem specifies the Euclidean distance, $d(i,j) = \\lVert x_{i} - x_{j} \\rVert_{2}$. These distances are organized into a condensed distance matrix.\n\nNext, we perform agglomerative hierarchical clustering using the average linkage criterion. This method starts by treating each data point as a singleton cluster. It then iteratively merges the two clusters, $C_A$ and $C_B$, that have the smallest average pairwise distance, defined as:\n$$\nD(C_A, C_B) = \\frac{1}{|C_A| |C_B|} \\sum_{i \\in C_A} \\sum_{j \\in C_B} d(i,j)\n$$\nThis process continues until all points are contained within a single cluster, producing a binary tree structure known as a dendrogram. This entire clustering process is performed once per dataset. The `scipy.spatial.distance.pdist` function is used for distance computation and `scipy.cluster.hierarchy.linkage` with `method='average'` for creating the dendrogram.\n\n**Step 2: Dendrogram Cutting and Cluster Assignment**\n\nFor each dataset with $n$ points and a given maximum cluster count $k_{\\max}$, we evaluate cluster configurations for $k \\in \\{2, 3, \\dots, \\min\\{k_{\\max}, n\\}\\}$. For each integer $k$ in this range, we \"cut\" the dendrogram to partition the data into exactly $k$ clusters. This is achieved by identifying the top $n-k$ merges in the hierarchy and assigning points to the resulting $k$ branches. The function `scipy.cluster.hierarchy.fcluster` with the `criterion='maxclust'` option is used for this purpose. This yields a set of cluster labels for all $n$ points for each value of $k$.\n\n**Step 3: Silhouette Score Calculation**\n\nFor each partition into $k$ clusters, we compute the average silhouette score, $\\bar{s}(k)$, to assess its quality. The silhouette score for a single point $i$, denoted $s(i)$, measures how well it fits into its assigned cluster compared to neighboring clusters. It is calculated as follows:\n\n1.  **Intra-cluster Dissimilarity, $a(i)$**: This is the average distance from point $i$ to all other points $j$ within its own cluster, $C(i)$.\n    $$ a(i) = \\frac{1}{|C(i)| - 1} \\sum_{j \\in C(i), j \\neq i} d(i,j) $$\n    As per the problem definition, if point $i$ is in a singleton cluster ($|C(i)| = 1$), then $a(i) = 0$.\n\n2.  **Inter-cluster Dissimilarity, $b(i)$**: For each other cluster $C'$ ($C' \\neq C(i)$), we calculate the average distance from point $i$ to all points in $C'$. $b(i)$ is the minimum of these values over all other clusters.\n    $$ b(i) = \\min_{C' \\neq C(i)} \\left\\{ \\frac{1}{|C'|} \\sum_{j \\in C'} d(i,j) \\right\\} $$\n\n3.  **Silhouette $s(i)$**: The silhouette for point $i$ is given by the formula:\n    $$ s(i) = \\begin{cases} 0, & \\text{if } |C(i)| = 1 \\\\ \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}, & \\text{otherwise} \\end{cases} $$\n    A special convention is used if $a(i) = b(i) = 0$, where $s(i)$ is defined as $0$. This case is relevant for datasets containing identical points, such as Dataset C.\n\nThe average silhouette score for the $k$-cluster partition, $\\bar{s}(k)$, is the mean of $s(i)$ over all $n$ data points:\n$$ \\bar{s}(k) = \\frac{1}{n} \\sum_{i=1}^{n} s(i) $$\n\n**Step 4: Optimal Cluster Number Selection**\n\nAfter computing $\\bar{s}(k)$ for all valid $k$, we select the optimal number of clusters, $k^{\\star}$. The selection rule requires finding the $k$ that maximizes the average silhouette score. To handle potential ties, the problem specifies a two-step rule:\n1.  First, determine the maximum score achieved, $S_{\\max} = \\max_{k} \\{\\bar{s}(k)\\}$.\n2.  Then, identify the set of all $k$ values for which the score is tied with $S_{\\max}$ within an absolute tolerance of $\\varepsilon = 10^{-12}$. A score $\\bar{s}(k')$ is considered tied if $|\\bar{s}(k') - S_{\\max}| \\leq \\varepsilon$.\n3.  From this set of \"tied-for-maximum\" $k$ values, select the smallest one.\n\nThis procedure ensures a unique and deterministic $k^{\\star}$ for each dataset. This logic is implemented by first computing all scores, then finding the maximum, and finally iterating through the scores in increasing order of $k$ to find the first one that falls within the tolerance range of the maximum.\n\nFor Dataset A (two well-separated groups), the highest silhouette score is expected at $k=2$. For Dataset B (three well-separated groups), $k=3$ is expected to be optimal. For Dataset C (all points identical), all distances are $0$, leading to $a(i) = 0$ and $b(i) = 0$ for all points and all clusterings. Consequently, $s(i)=0$ and $\\bar{s}(k)=0$ for all $k$. The tie-breaking rule will then select the smallest tested $k$, which is $2$.", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_average_silhouette(dist_matrix, labels, n):\n    \"\"\"\n    Calculates the average silhouette score for a given clustering.\n    Follows the definitions specified in the problem statement.\n    \"\"\"\n    if n == 0:\n        return 0.0\n\n    unique_labels, label_indices, label_counts = np.unique(labels, return_inverse=True, return_counts=True)\n    num_clusters = len(unique_labels)\n\n    if num_clusters <= 1:\n        return 0.0\n\n    silhouettes = np.zeros(n)\n    \n    for i in range(n):\n        my_label_idx = label_indices[i]\n        my_label = unique_labels[my_label_idx]\n        my_cluster_size = label_counts[my_label_idx]\n\n        # Per problem definition, s(i) = 0 for singleton clusters.\n        if my_cluster_size == 1:\n            silhouettes[i] = 0.0\n            continue\n        \n        # Calculate a(i): mean distance to other points in the same cluster.\n        in_cluster_mask = (labels == my_label)\n        in_cluster_mask[i] = False\n        a_i = np.sum(dist_matrix[i, in_cluster_mask]) / (my_cluster_size - 1)\n\n        # Calculate b(i): min mean distance to points in any other cluster.\n        b_i = np.inf\n        for j, other_label in enumerate(unique_labels):\n            if j == my_label_idx:\n                continue\n            \n            other_cluster_mask = (labels == other_label)\n            mean_dist = np.mean(dist_matrix[i, other_cluster_mask])\n            b_i = min(b_i, mean_dist)\n\n        # Calculate s(i).\n        denominator = max(a_i, b_i)\n        if denominator == 0:\n            # Handles the convention where s(i)=0 if a(i)=b(i)=0.\n            silhouettes[i] = 0.0\n        else:\n            silhouettes[i] = (b_i - a_i) / denominator\n            \n    return np.mean(silhouettes)\n\ndef find_optimal_k(points, k_max):\n    \"\"\"\n    Performs hierarchical clustering and finds the optimal k based on silhouette score.\n    \"\"\"\n    n = len(points)\n    epsilon = 1e-12\n\n    if n <= 1:\n        # According to problem constraints (k>=2), n must be at least 2.\n        # This case should not be reached.\n        return None\n\n    # Step 1: Perform hierarchical clustering with average linkage.\n    condensed_dist_matrix = pdist(points, 'euclidean')\n    linkage_matrix = linkage(condensed_dist_matrix, method='average')\n    \n    # We need the full n x n distance matrix for silhouette calculations.\n    dist_matrix = squareform(condensed_dist_matrix)\n\n    # Step 2: Evaluate average silhouette for each k.\n    silhouette_scores = {}\n    k_range = range(2, min(k_max, n) + 1)\n    \n    if not k_range:\n        return None\n\n    for k in k_range:\n        labels = fcluster(linkage_matrix, t=k, criterion='maxclust')\n        avg_silhouette = calculate_average_silhouette(dist_matrix, labels, n)\n        silhouette_scores[k] = avg_silhouette\n\n    # Step 3: Select the optimal k using the specified tie-breaking rule.\n    if not silhouette_scores:\n        return None\n        \n    # Find the true maximum score\n    max_score = max(silhouette_scores.values())\n\n    # Find the smallest k that is tied for the maximum score\n    best_k = -1\n    for k in sorted(silhouette_scores.keys()):\n        score = silhouette_scores[k]\n        if abs(score - max_score) <= epsilon:\n            best_k = k\n            break # Found the smallest k in the tie-set, so we can stop.\n    \n    return best_k\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"points\": np.array([\n                (0,0), (0,1), (1,0), (1,1), (0.5,0), (0,0.5),\n                (5,5), (5,6), (6,5), (6,6), (5.5,5), (5,5.5)\n            ]),\n            \"k_max\": 5\n        },\n        # Dataset B\n        {\n            \"points\": np.array([\n                (-6,0), (-6,0.2), (-6,-0.2), (-5.8,0), (-6.2,0),\n                (0,0), (0,0.2), (0,-0.2), (0.2,0), (-0.2,0),\n                (6,0), (6,0.2), (6,-0.2), (5.8,0), (6.2,0)\n            ]),\n            \"k_max\": 6\n        },\n        # Dataset C\n        {\n            \"points\": np.array([\n                (10,-3), (10,-3), (10,-3), (10,-3), (10,-3)\n            ]),\n            \"k_max\": 4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_star = find_optimal_k(case[\"points\"], case[\"k_max\"])\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129027"}]}