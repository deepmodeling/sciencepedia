{"hands_on_practices": [{"introduction": "One of the celebrated strengths of K-medoids is its robustness against outliers, as it minimizes a sum of distances rather than a sum of squared distances. This practice provides a concrete, calculation-based exploration of this property [@problem_id:3135235]. By analyzing the total clustering cost, you will determine the exact point at which it becomes more 'economical' for the algorithm to isolate a distant data point into its own singleton cluster, rather than distorting an existing one.", "problem": "Consider the $k$-medoids clustering objective on the real line with Euclidean distance, where medoids are restricted to be chosen from the data points. Let the original dataset be the multiset $X = \\{-1, 0, 1, 9, 10, 11\\}$, and fix $k = 2$. A new point $z$ located at position $s \\in \\mathbb{R}$ is added to form $X' = X \\cup \\{z\\}$. The $k$-medoids objective seeks a subset $M \\subset X'$ of cardinality $k$ and an assignment of each point in $X'$ to the nearest medoid in $M$ that minimizes the total sum of distances. Starting from the foundational definition that the $k$-medoids objective equals the sum, over all points, of the distance to the nearest chosen medoid, determine the threshold distance $D^*$ such that if $r = \\min\\{|s|, |s - 10|\\} > D^*$, the globally optimal $k$-medoids solution uses $z$ as one of the medoids (so that $z$ forms a singleton cluster). Express your final answer as an exact number (no rounding needed).", "solution": "The user wants to find the threshold distance $D^*$ for a new point $z$ added to a dataset $X$, such that if the point's distance from the original clusters is greater than $D^*$, it becomes one of the two medoids in a $k=2$ clustering.\n\nFirst, let's establish the necessary definitions.\nThe original dataset is the multiset $X = \\{-1, 0, 1, 9, 10, 11\\}$.\nThe number of medoids is $k=2$.\nA new point $z$ at position $s \\in \\mathbb{R}$ is added, forming the new dataset $X' = X \\cup \\{z\\}$.\nThe medoids must be chosen from the points in $X'$.\nThe objective is to find a set of medoids $M \\subset X'$ with cardinality $|M|=k=2$ that minimizes the total sum of Euclidean distances of each point in $X'$ to its nearest medoid in $M$. The cost function is $C(M) = \\sum_{p \\in X'} \\min_{m \\in M} |p-m|$.\nThe variable $r$ is defined as $r = \\min\\{|s|, |s - 10|\\}$. We seek a threshold $D^*$ such that for any $s$ where $r > D^*$, the globally optimal set of medoids $M_{opt}$ must contain $z$.\n\nThis problem requires comparing the minimum cost achievable under two scenarios:\n1.  Case 1: The two medoids are chosen only from the original set $X$. Let this minimum cost be $C_{no-z}$.\n2.  Case 2: One medoid is the new point $z$, and the other is chosen from $X$. Let this minimum cost be $C_{with-z}$.\n\nThe condition for $z$ to be part of the optimal solution is $C_{with-z} < C_{no-z}$. We need to find the threshold $D^*$ on $r$ that guarantees this inequality.\n\nLet's analyze Case 1: $z$ is not a medoid.\nThe set of medoids is $M = \\{m_1, m_2\\}$, where $m_1, m_2 \\in X$. The cost is:\n$$C(M) = \\sum_{p \\in X'} \\min_{m \\in M} |p-m| = \\left( \\sum_{x \\in X} \\min(|x-m_1|, |x-m_2|) \\right) + \\min(|s-m_1|, |s-m_2|)$$\nThe dataset $X$ consists of two distinct clusters: $X_1 = \\{-1, 0, 1\\}$ and $X_2 = \\{9, 10, 11\\}$. Intuitively, to minimize the sum of distances for points in $X$, the two medoids should be chosen one from each cluster. The best single medoid for a set of points on a line is its median.\nFor $X_1$, the median is $0$. The sum of distances is $|-1-0| + |0-0| + |1-0| = 2$.\nFor $X_2$, the median is $10$. The sum of distances is $|9-10| + |10-10| + |11-10| = 2$.\nLet's choose the medoid set $M_A = \\{0, 10\\}$. The cost contribution from points in $X$ is:\n$$\\sum_{x \\in X} \\min(|x-0|, |x-10|) = \\sum_{x \\in X_1} |x-0| + \\sum_{x \\in X_2} |x-10| = 2 + 2 = 4$$\nAny other choice of two medoids from $X$ will result in a higher cost for the points in $X$. For example, for $M=\\{1,9\\}$, the cost on $X$ is $(\\sum_{x \\in X_1} |x-1|) + (\\sum_{x \\in X_2} |x-9|) = (2+1+0) + (0+1+2) = 6$.\nWhen the new point $z$ is far from $X$ (i.e., for large $r$), the term $\\sum_{x \\in X} \\min(|x-m_1|, |x-m_2|)$ dominates the choice of medoids for clustering X. The pair $\\{0,10\\}$ minimizes this term.\nThe cost component for the new point $z$ with medoids $\\{0, 10\\}$ is $\\min(|s-0|, |s-10|)$, which is exactly the definition of $r$.\nThus, the total cost for this choice of medoids is $C(M_A) = 4 + r$.\nFor large $r$, other choices of medoids from $X$ result in a higher cost. For example, considering $M_B = \\{0, 11\\}$, the cost on $X$ is $\\sum_{x \\in X}\\min(|x-0|,|x-11|) = (1+0+1) + (2+1+0) = 5$. For large positive $s$, the cost for $z$ would be $|s-11| = s-11$. The total cost is $5+s-11 = s-6$. For $M_A=\\{0,10\\}$, the cost is $4+|s-10|=4+s-10=s-6$. The costs are equal in this limit. A similar analysis for large negative $s$ shows that pairs like $\\{-1,10\\}$ also yield the same minimal cost $4+r$. Therefore, we can confidently state that for large $r$:\n$$C_{no-z} = 4 + r$$\n\nNow, let's analyze Case 2: $z$ is a medoid.\nThe set of medoids is $M = \\{m_1, z\\}$, where $m_1 \\in X$. The cost is:\n$$C(M) = \\sum_{p \\in X'} \\min(|p-m_1|, |p-s|)$$\nSince $z$ is a medoid, its distance to the nearest medoid is $0$. The cost simplifies to a sum over the points in $X$:\n$$C_{with-z} = \\min_{m_1 \\in X} \\left( \\sum_{x \\in X} \\min(|x-m_1|, |x-s|) \\right)$$\nThe problem asks for a threshold $D^*$ for $r$. We consider the situation where $r$ is large. A large $r$ implies that $s$ is far from the interval $[0, 10]$.\nIf $s$ is very far from $X$, then for any point $x \\in X$ and any potential medoid $m_1 \\in X$, the distance $|x-m_1|$ will be significantly smaller than $|x-s|$.\nFormally, for any $x, m_1 \\in X$, we have $|x-m_1| \\leq |11 - (-1)| = 12$. The distance $|x-s|$ will be larger than this for a sufficiently large $|s|$. For example, if $s > 11$, the point in $X$ closest to $s$ is $11$. The condition $|x-m_1|<|x-s|$ needs to hold for all $x, m_1 \\in X$. The worst case is for $x=11$ and $m_1=-1$: we need $|11-(-1)| < |11-s|$, so $12 < s-11$, which means $s > 23$. If $s < -1$, the worst case is $x=-1, m_1=11$: we need $|-1-11| < |-1-s|$, so $12 < |-1-s| = -s-1$, which means $s < -13$.\nIf $r > D^*$ for a sufficiently large $D^*$, the condition holds, and every point $x \\in X$ will be assigned to medoid $m_1$.\nThe cost expression then simplifies to finding the 1-medoid of the set $X$:\n$$C_{with-z} = \\min_{m_1 \\in X} \\sum_{x \\in X} |x-m_1|$$\nWe need to calculate this sum for each possible medoid $m_1 \\in X$. Let $S(m_1) = \\sum_{x \\in X} |x-m_1|$.\n- $S(-1) = (0) + (1) + (2) + (10) + (11) + (12) = 36$\n- $S(0) = (1) + (0) + (1) + (9) + (10) + (11) = 32$\n- $S(1) = (2) + (1) + (0) + (8) + (9) + (10) = 30$\n- $S(9) = (10) + (9) + (8) + (0) + (1) + (2) = 30$\n- $S(10) = (11) + (10) + (9) + (1) + (0) + (1) = 32$\n- $S(11) = (12) + (11) + (10) + (2) + (1) + (0) = 36$\nThe minimum value is $30$, which occurs when the medoid $m_1$ is chosen as either $1$ or $9$.\nSo, for sufficiently large $r$, the minimum cost when $z$ is a medoid is:\n$$C_{with-z} = 30$$\n\nFinally, we determine the threshold $D^*$.\nThe condition for the globally optimal solution to use $z$ as a medoid is $C_{with-z} < C_{no-z}$.\nSubstituting the derived costs:\n$$30 < 4 + r$$\n$$26 < r$$\nThis gives a threshold value of $D^*=26$. For $r>26$, the cost of choosing $z$ as a medoid ($30$) is strictly less than the cost of not choosing it ($4+r > 4+26 = 30$).\nThe logic used to simplify the cost functions is valid for $r>26$. For example, $r>26$ implies either $s>36$ or $s<-16$. Both of these satisfy the conditions ($s>23$ or $s<-13$) for all points in $X$ being closer to an internal medoid $m_1$ than to $z$.\n\nTherefore, the threshold distance is $D^*=26$.", "answer": "$$\n\\boxed{26}\n$$", "id": "3135235"}, {"introduction": "The K-medoids algorithm is fundamentally shaped by the chosen distance metric, as the notion of 'closeness' defines the clusters. This hands-on coding exercise [@problem_id:3135257] challenges you to find the optimal medoids for a dataset using three different metrics: Manhattan ($d_1$), Euclidean ($d_2$), and Cosine distance. By comparing the resulting medoid sets, you will gain a practical understanding of how the geometric or angular interpretation of dissimilarity can lead to distinct clustering outcomes.", "problem": "You are given the task of implementing and analyzing $k$-medoids clustering under three different distance metrics on the same datasets, and of quantifying the sensitivity of medoid locations to the choice of metric. The three metrics are the Manhattan distance $d_1$, the Euclidean distance $d_2$, and the cosine angular distance $d_{\\cos}$. The fundamental base for this task is the definition of a metric space, the definition of a clustering objective that minimizes the sum of distances from points to their assigned representative, and the definition of medoids as representatives constrained to be actual data points. Your program must use a deterministic selection rule for any ties in medoid choice: whenever a tie occurs, always choose the smallest index. Indices must be assigned to data points in their given order, starting from $0$.\n\nDefinitions to use:\n- The Manhattan distance $d_1$ between two vectors $x \\in \\mathbb{R}^p$ and $y \\in \\mathbb{R}^p$ is $d_1(x,y)=\\sum_{i=1}^p |x_i-y_i|$.\n- The Euclidean distance $d_2$ between two vectors $x \\in \\mathbb{R}^p$ and $y \\in \\mathbb{R}^p$ is $d_2(x,y)=\\sqrt{\\sum_{i=1}^p (x_i-y_i)^2}$.\n- The cosine angular distance $d_{\\cos}$ between two vectors $x \\in \\mathbb{R}^p$ and $y \\in \\mathbb{R}^p$ is $d_{\\cos}(x,y)=1-\\dfrac{x \\cdot y}{\\|x\\|\\|y\\|}$, where $x \\cdot y$ is the dot product and $\\|x\\|$ is the Euclidean norm. If either $\\|x\\|=0$ or $\\|y\\|=0$ and not both are zero, define $d_{\\cos}(x,y)=1$; if both norms are zero, define $d_{\\cos}(x,y)=0$.\n\nThe $k$-medoids objective is to choose a set of $k$ indices $M \\subset \\{0,1,\\dots,n-1\\}$ and assign each data point to the nearest chosen medoid under the selected metric, minimizing the total within-cluster distance. A medoid is one of the data points. You must implement a deterministic procedure that, starting from this objective and using only well-tested rules and definitions, finds a set of medoid indices for each metric and each dataset. Determinism must be guaranteed by always breaking ties in favor of the smallest index.\n\nAfter obtaining the medoid sets for the three metrics for each dataset, quantify the sensitivity of medoid locations to metric choice using the Jaccard set dissimilarity between medoid index sets. Given two sets $A$ and $B$, the Jaccard similarity is $J(A,B)=\\dfrac{|A \\cap B|}{|A \\cup B|}$, and the Jaccard dissimilarity is $1-J(A,B)$. For each dataset, compute the three pairwise dissimilarities for the metric pairs $(d_1,d_2)$, $(d_1,d_{\\cos})$, and $(d_2,d_{\\cos})$.\n\nTest suite:\n- Dataset $X_1 \\in \\mathbb{R}^{9 \\times 2}$ with $k_1=3$:\n$$\nX_1=\\begin{bmatrix}\n-0.5 & 0.2 \\\\\n0.3 & -0.2 \\\\\n0.1 & 0.3 \\\\\n5.1 & 5.2 \\\\\n4.9 & 5.05 \\\\\n5.3 & 4.9 \\\\\n-5.2 & 5.1 \\\\\n-4.8 & 4.9 \\\\\n-5.1 & 5.3\n\\end{bmatrix}, \\quad k_1=3.\n$$\n- Dataset $X_2 \\in \\mathbb{R}^{5 \\times 2}$ with $k_2=1$:\n$$\nX_2=\\begin{bmatrix}\n1 & 0 \\\\\n2 & 0 \\\\\n3 & 0 \\\\\n0 & 10 \\\\\n0 & 8\n\\end{bmatrix}, \\quad k_2=1.\n$$\n- Dataset $X_3 \\in \\mathbb{R}^{4 \\times 2}$ with $k_3=2$:\n$$\nX_3=\\begin{bmatrix}\n1 & 0 \\\\\n2 & 0 \\\\\n3 & 0 \\\\\n10 & 0\n\\end{bmatrix}, \\quad k_3=2.\n$$\n\nYour program must:\n- For each dataset $X_i$ and its $k_i$, compute the medoid index set under $d_1$, under $d_2$, and under $d_{\\cos}$ using a deterministic $k$-medoids procedure derived from the objective definition and consistent with the tie-breaking rule described above.\n- For each dataset, compute the three Jaccard dissimilarities between the medoid index sets for the metric pairs $(d_1,d_2)$, $(d_1,d_{\\cos})$, and $(d_2,d_{\\cos})$, in that order.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list of three floats corresponding to the three dissimilarities for the dataset in the specified order. For example, the outer list is $\\big[\\,[\\delta_{12}^{(1)},\\delta_{1c}^{(1)},\\delta_{2c}^{(1)}],\\,[\\delta_{12}^{(2)},\\delta_{1c}^{(2)},\\delta_{2c}^{(2)}],\\,[\\delta_{12}^{(3)},\\delta_{1c}^{(3)},\\delta_{2c}^{(3)}]\\,\\big]$, where $\\delta_{ab}^{(i)}$ is the Jaccard dissimilarity between the medoid index sets found using metrics $a$ and $b$ on dataset $i$. Express each float rounded to $3$ decimal places.", "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the domain of statistical learning, specifically k-medoids clustering. The problem is well-posed, objective, and self-contained, with all necessary data, definitions, and constraints provided for a unique, deterministic solution. An exhaustive search approach is computationally feasible for the given dataset sizes and guarantees finding the optimal solution as defined by the k-medoids objective function.\n\n### Principle-Based Design\n\nThe core of this problem is to find an optimal set of $k$ medoids for a given dataset of $n$ points, $X = \\{x_0, x_1, \\dots, x_{n-1}\\}$, which minimizes a total cost function. A medoid must be one of the data points from $X$. For a chosen set of $k$ medoid indices, $M \\subset \\{0, 1, \\dots, n-1\\}$ where $|M|=k$, the total within-cluster sum of distances is defined as:\n\n$$\nC(M) = \\sum_{i=0}^{n-1} \\min_{m \\in M} d(x_i, x_{m})\n$$\n\nHere, $d(x_i, x_m)$ is the distance between data point $x_i$ and the medoid data point $x_m$ under a specified metric. The task requires this optimization to be performed for three distinct distance metrics: Manhattan ($d_1$), Euclidean ($d_2$), and Cosine ($d_{\\cos}$).\n\nFinding the globally optimal set of medoids $M^*$ that minimizes $C(M)$ is an NP-hard problem. However, the datasets provided are small enough to permit an exhaustive search over all possible combinations of $k$ medoids. The number of such combinations is given by the binomial coefficient $\\binom{n}{k}$. For the given datasets:\n- $X_1$: $n=9, k=3 \\implies \\binom{9}{3} = 84$ combinations.\n- $X_2$: $n=5, k=1 \\implies \\binom{5}{1} = 5$ combinations.\n- $X_3$: $n=4, k=2 \\implies \\binom{4}{2} = 6$ combinations.\n\nSince these numbers are small, an exhaustive search is computationally tractable and guarantees finding the global minimum of the cost function $C(M)$, thus fulfilling the problem's requirement for a procedure derived directly from the objective definition.\n\nThe solution proceeds as follows:\n\n1.  **Distance Functions**: Implement the three required distance metrics according to their definitions:\n    -   Manhattan distance: $d_1(x,y)=\\sum_{i=1}^p |x_i-y_i|$\n    -   Euclidean distance: $d_2(x,y)=\\sqrt{\\sum_{i=1}^p (x_i-y_i)^2}$\n    -   Cosine angular distance: $d_{\\cos}(x,y)=1-\\frac{x \\cdot y}{\\|x\\|\\|y\\|}$, with special handling for zero vectors as specified.\n\n2.  **Optimal Medoid Search**: For each dataset and each metric:\n    a.  Pre-compute the $n \\times n$ distance matrix for the given metric to optimize subsequent cost calculations.\n    b.  Generate all possible combinations of $k$ indices from the set $\\{0, 1, \\dots, n-1\\}$.\n    c.  For each combination (a candidate medoid set $M$):\n        i.  Calculate the total cost $C(M)$ by summing the distances from each of the $n$ data points to its nearest medoid in $M$.\n    d.  Identify the medoid set $M^*$ that yields the minimum cost. To ensure determinism in case of ties (i.e., multiple sets yielding the same minimum cost), we rely on the lexicographical ordering of combinations produced by standard library functions (like Python's `itertools.combinations`). The first set encountered that achieves the minimum cost is selected. This is achieved by only updating the best-found set when a strictly smaller cost is found.\n\n3.  **Sensitivity Analysis**: After determining the optimal medoid index sets for each of the three metrics ($M_{d_1}$, $M_{d_2}$, $M_{d_{\\cos}}$) for a given dataset:\n    a.  Calculate the Jaccard dissimilarity for the three pairs of sets: $(M_{d_1}, M_{d_2})$, $(M_{d_1}, M_{d_{\\cos}})$, and $(M_{d_2}, M_{d_{\\cos}})$.\n    b.  The Jaccard dissimilarity between two sets $A$ and $B$ is given by $1 - J(A,B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$. This quantifies the dissimilarity in medoid locations resulting from the change in metric.\n\n4.  **Output Formatting**: The calculated dissimilarities for each dataset are collected and formatted as a list of lists of floating-point numbers, rounded to three decimal places, according to the specified output format.\n\nThis structured approach ensures correctness by directly optimizing the objective function, guarantees determinism through a well-defined tie-breaking procedure, and meets all requirements of the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to solve the k-medoids problem for the given test suite.\n    \"\"\"\n\n    def d_manhattan(p1, p2):\n        \"\"\"Computes Manhattan distance.\"\"\"\n        return np.sum(np.abs(p1 - p2))\n\n    def d_euclidean(p1, p2):\n        \"\"\"Computes Euclidean distance.\"\"\"\n        return np.linalg.norm(p1 - p2)\n\n    def d_cosine(p1, p2):\n        \"\"\"Computes Cosine angular distance with specified handling for zero vectors.\"\"\"\n        norm_p1 = np.linalg.norm(p1)\n        norm_p2 = np.linalg.norm(p2)\n\n        if norm_p1 == 0 and norm_p2 == 0:\n            return 0.0\n        if norm_p1 == 0 or norm_p2 == 0:\n            return 1.0\n        \n        dot_product = np.dot(p1, p2)\n        cos_sim = dot_product / (norm_p1 * norm_p2)\n        # Clamp to handle potential floating point inaccuracies\n        cos_sim = np.clip(cos_sim, -1.0, 1.0)\n        return 1.0 - cos_sim\n\n    def find_optimal_medoids(data, k, metric_func):\n        \"\"\"\n        Finds the optimal set of k medoids using an exhaustive search.\n        \"\"\"\n        n = data.shape[0]\n        indices = range(n)\n        \n        dist_matrix = np.zeros((n, n))\n        for i in range(n):\n            for j in range(i, n):\n                d = metric_func(data[i], data[j])\n                dist_matrix[i, j] = d\n                dist_matrix[j, i] = d\n\n        min_total_cost = float('inf')\n        best_medoids = None\n\n        for medoid_indices_tuple in combinations(indices, k):\n            current_total_cost = 0.0\n            for i in indices:\n                min_dist_to_medoid = min(dist_matrix[i, m] for m in medoid_indices_tuple)\n                current_total_cost += min_dist_to_medoid\n            \n            if current_total_cost < min_total_cost:\n                min_total_cost = current_total_cost\n                best_medoids = medoid_indices_tuple\n        \n        return set(best_medoids)\n\n    def jaccard_dissimilarity(set1, set2):\n        \"\"\"Computes the Jaccard dissimilarity between two sets.\"\"\"\n        intersection_size = len(set1.intersection(set2))\n        union_size = len(set1.union(set2))\n        if union_size == 0:\n            return 0.0\n        return 1.0 - (intersection_size / union_size)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [-0.5, 0.2], [0.3, -0.2], [0.1, 0.3],\n            [5.1, 5.2], [4.9, 5.05], [5.3, 4.9],\n            [-5.2, 5.1], [-4.8, 4.9], [-5.1, 5.3]\n        ]), 3),\n        (np.array([\n            [1, 0], [2, 0], [3, 0],\n            [0, 10], [0, 8]\n        ]), 1),\n        (np.array([\n            [1, 0], [2, 0], [3, 0], [10, 0]\n        ]), 2)\n    ]\n\n    metrics = {\n        'd1': d_manhattan,\n        'd2': d_euclidean,\n        'dcos': d_cosine\n    }\n    \n    all_results = []\n\n    for data, k in test_cases:\n        medoid_sets = {}\n        for name, func in metrics.items():\n            medoid_sets[name] = find_optimal_medoids(data, k, func)\n        \n        m1 = medoid_sets['d1']\n        m2 = medoid_sets['d2']\n        m_cos = medoid_sets['dcos']\n\n        dis_1_2 = jaccard_dissimilarity(m1, m2)\n        dis_1_cos = jaccard_dissimilarity(m1, m_cos)\n        dis_2_cos = jaccard_dissimilarity(m2, m_cos)\n        \n        dataset_results = [round(dis_1_2, 3), round(dis_1_cos, 3), round(dis_2_cos, 3)]\n        all_results.append(dataset_results)\n    \n    # Format the final output string precisely\n    output_parts = []\n    for res_list in all_results:\n        formatted_floats = [f\"{x:.3f}\" for x in res_list]\n        list_str = f\"[{','.join(formatted_floats)}]\"\n        output_parts.append(list_str)\n        \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3135257"}, {"introduction": "In real-world applications, ensuring an algorithm is deterministic and reproducible is critical, yet ambiguities can arise when data points are equidistant from multiple cluster centers. This practice [@problem_id:3135303] delves into a subtle but important aspect of implementing K-medoids: handling ties in the dissimilarity matrix. You will implement an exact solver under two different, explicit tie-breaking policies to observe how these rules can alter the final medoid selection and cluster assignments, reinforcing the importance of careful and unambiguous algorithm design.", "problem": "You are given a precomputed dissimilarity matrix $D \\in \\mathbb{R}^{n \\times n}$ with symmetric, non-negative entries and $D_{i,i} = 0$ for all $i \\in \\{0,\\dots,n-1\\}$. In $k$-medoids clustering, a set of medoids $M \\subseteq \\{0,\\dots,n-1\\}$ with $|M| = k$ is chosen to minimize the clustering objective\n$$\nJ(M) \\equiv \\sum_{i=0}^{n-1} \\min_{m \\in M} D_{i,m}.\n$$\nWhen $D$ contains ties, there are two distinct sources of ambiguity that must be resolved by explicit tie-breaking rules:\n- Assignment ties: For a given $M$, if for some index $i$ there exist multiple medoids $m \\in M$ that attain the same minimum value of $D_{i,m}$, then an assignment tie occurs. An assignment tie-breaking rule $T_a$ must specify which medoid receives $i$ whenever such a tie occurs.\n- Medoid-set ties: If there exist multiple medoid sets $M$ that minimize $J(M)$, then a medoid-set tie occurs. A medoid-set tie-breaking rule $T_m$ must specify which medoid set to return among the minimizers.\n\nDesign two deterministic tie-breaking policies that each resolve both types of ties:\n- Policy $\\mathcal{A}$: $T_m$ selects the lexicographically smallest medoid set (when medoid indices are written in ascending order), and $T_a$ assigns tied points to the smallest medoid index.\n- Policy $\\mathcal{B}$: $T_m$ selects the lexicographically largest medoid set (when medoid indices are written in ascending order), and $T_a$ assigns tied points to the largest medoid index.\n\nStarting from the core definition of the $k$-medoids objective $J(M)$ and set membership properties on $\\{0,\\dots,n-1\\}$, implement an exact solver that:\n- Enumerates all medoid sets $M$ with $|M| = k$ and computes $J(M)$ using the given $D$.\n- Applies $T_m$ from the specified policy to choose among medoid-set minimizers.\n- Produces the final assignment of each index $i$ to a medoid in $M$ using $T_a$ from the specified policy.\n\nDemonstrate how tie-breaking affects medoid identity and the stability of $J(M)$ using the following test suite. For each test case, use the provided $D$ and $k$, run the solver under both policies $\\mathcal{A}$ and $\\mathcal{B}$, and report the results.\n\nTest suite:\n- Case $1$ (happy path with multiple optimal medoid sets): Let $n = 6$, $k = 2$, and\n$$\nD_1 = \\begin{pmatrix}\n0 & 1 & 1 & 4 & 4 & 4 \\\\\n1 & 0 & 1 & 4 & 4 & 4 \\\\\n1 & 1 & 0 & 4 & 4 & 4 \\\\\n4 & 4 & 4 & 0 & 1 & 1 \\\\\n4 & 4 & 4 & 1 & 0 & 1 \\\\\n4 & 4 & 4 & 1 & 1 & 0 \\\\\n\\end{pmatrix}.\n$$\n- Case $2$ (boundary condition with $k = 1$ and a symmetric tie among all possible medoids): Let $n = 4$, $k = 1$, and\n$$\nD_2 = \\begin{pmatrix}\n0 & 1 & 2 & 1 \\\\\n1 & 0 & 1 & 2 \\\\\n2 & 1 & 0 & 1 \\\\\n1 & 2 & 1 & 0 \\\\\n\\end{pmatrix}.\n$$\n- Case $3$ (edge case with duplicated points and assignment ties): Let $n = 5$, $k = 2$, and\n$$\nD_3 = \\begin{pmatrix}\n0 & 0 & 2 & 2 & 3 \\\\\n0 & 0 & 2 & 2 & 3 \\\\\n2 & 2 & 0 & 0 & 3 \\\\\n2 & 2 & 0 & 0 & 3 \\\\\n3 & 3 & 3 & 3 & 0 \\\\\n\\end{pmatrix}.\n$$\n\nFor each case, your program must compute and return a single list containing:\n- The medoid indices under policy $\\mathcal{A}$ as a list in ascending order.\n- The assignment vector under policy $\\mathcal{A}$ as a list where the $i$-th entry is the medoid index assigned to $i$.\n- The objective value $J(M)$ under policy $\\mathcal{A}$ as an integer.\n- The medoid indices under policy $\\mathcal{B}$ as a list in ascending order.\n- The assignment vector under policy $\\mathcal{B}$ as a list where the $i$-th entry is the medoid index assigned to $i$.\n- The objective value $J(M)$ under policy $\\mathcal{B}$ as an integer.\n- A boolean indicating whether $J(M)$ is identical under both policies for this case.\n- A boolean indicating whether the medoid identities are identical under both policies for this case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following aggregated format across the three cases:\n$$\n[\\text{case1\\_result},\\text{case2\\_result},\\text{case3\\_result}],\n$$\nwhere each $\\text{caseX\\_result}$ is itself the list described above. No additional text should be printed.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n\n1.  **Objective Function**: The $k$-medoids clustering objective to be minimized is $J(M) \\equiv \\sum_{i=0}^{n-1} \\min_{m \\in M} D_{i,m}$, where $M$ is a set of $k$ medoid indices.\n2.  **Dissimilarity Matrix**: A matrix $D \\in \\mathbb{R}^{n \\times n}$ is provided, which is symmetric ($D_{i,j} = D_{j,i}$), has non-negative entries ($D_{i,j} \\ge 0$), and a zero diagonal ($D_{i,i} = 0$).\n3.  **Tie-Breaking Sources**:\n    -   **Assignment ties**: For a given medoid set $M$, a point $i$ has the same minimum dissimilarity to multiple medoids in $M$.\n    -   **Medoid-set ties**: Multiple distinct medoid sets $M$ yield the same minimum value of the objective function $J(M)$.\n4.  **Tie-Breaking Policies**:\n    -   **Policy $\\mathcal{A}$**:\n        -   $T_m$ (Medoid-set): Select the lexicographically smallest medoid set among the minimizers.\n        -   $T_a$ (Assignment): Assign a tied point to the medoid with the smallest index.\n    -   **Policy $\\mathcal{B}$**:\n        -   $T_m$ (Medoid-set): Select the lexicographically largest medoid set among the minimizers.\n        -   $T_a$ (Assignment): Assign a tied point to the medoid with the largest index.\n5.  **Task**: Implement an exact solver that enumerates all possible medoid sets, applies the specified tie-breaking policies, and determines the final medoid set and assignments.\n6.  **Test Cases**:\n    -   **Case 1**: $n = 6$, $k = 2$, $D_1 = \\begin{pmatrix} 0 & 1 & 1 & 4 & 4 & 4 \\\\ 1 & 0 & 1 & 4 & 4 & 4 \\\\ 1 & 1 & 0 & 4 & 4 & 4 \\\\ 4 & 4 & 4 & 0 & 1 & 1 \\\\ 4 & 4 & 4 & 1 & 0 & 1 \\\\ 4 & 4 & 4 & 1 & 1 & 0 \\end{pmatrix}$.\n    -   **Case 2**: $n = 4$, $k = 1$, $D_2 = \\begin{pmatrix} 0 & 1 & 2 & 1 \\\\ 1 & 0 & 1 & 2 \\\\ 2 & 1 & 0 & 1 \\\\ 1 & 2 & 1 & 0 \\end{pmatrix}$.\n    -   **Case 3**: $n = 5$, $k = 2$, $D_3 = \\begin{pmatrix} 0 & 0 & 2 & 2 & 3 \\\\ 0 & 0 & 2 & 2 & 3 \\\\ 2 & 2 & 0 & 0 & 3 \\\\ 2 & 2 & 0 & 0 & 3 \\\\ 3 & 3 & 3 & 3 & 0 \\end{pmatrix}$.\n7.  **Required Output**: For each case, a list containing eight elements: medoids under $\\mathcal{A}$, assignments under $\\mathcal{A}$, objective value under $\\mathcal{A}$, medoids under $\\mathcal{B}$, assignments under $\\mathcal{B}$, objective value under $\\mathcal{B}$, a boolean for equality of objective values, and a boolean for equality of medoid sets. The final output must be a single line containing a list of these result lists for the three cases.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is analyzed against the validation criteria.\n\n-   **Scientifically Grounded**: The problem is rooted in the well-established field of statistical learning, specifically unsupervised clustering. The $k$-medoids algorithm and its objective function $J(M)$ are standard concepts. The properties of the dissimilarity matrix $D$ are consistent with its mathematical definition. The problem is scientifically sound.\n-   **Well-Posed**: The task is to find an optimal medoid set by exhaustive enumeration. The search space, consisting of all $k$-element subsets of a set of size $n$, is finite. The objective function $J(M)$ is uniquely defined for any given medoid set. The provided tie-breaking rules, $T_m$ and $T_a$, are deterministic and unambiguous, ensuring that for each policy ($\\mathcal{A}$ and $\\mathcal{B}$), a single, unique solution exists. The problem is therefore well-posed.\n-   **Objective**: The problem is formulated using precise mathematical language and definitions. All terms are clearly defined, and the test cases are specified with concrete numerical data, leaving no room for subjective interpretation.\n-   **No Flaws Detected**: The problem does not violate any fundamental principles, is not metaphorical, contains all necessary information, is computationally feasible for the given constraints ($n$ is small), and is clearly structured.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be developed and implemented as requested.\n\n### Solution Derivation\n\nThe solution requires an exact algorithm that performs an exhaustive search over all possible medoid sets. The principles guiding the design are as follows.\n\n**1. Combinatorial Enumeration of Candidate Medoid Sets**\nThe set of all possible medoid sets is the collection of all $k$-element subsets of the set of indices $\\{0, 1, \\dots, n-1\\}$. This collection of candidate medoid sets, denoted $\\mathcal{C}_M$, can be systematically generated. The size of this search space is given by the binomial coefficient $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$. For the small values of $n$ and $k$ in the test suite, this enumeration is computationally tractable.\n\n**2. Calculation of the Objective Function**\nFor each candidate medoid set $M \\in \\mathcal{C}_M$, the objective function $J(M)$ must be computed. The definition is $J(M) = \\sum_{i=0}^{n-1} \\min_{m \\in M} D_{i,m}$. This calculation involves, for each data point $i$, finding its minimum dissimilarity to any of the chosen medoids in $M$, and then summing these minimum dissimilarities over all data points.\n\n**3. Minimization and Medoid-Set Tie-Breaking ($T_m$)**\nAfter computing $J(M)$ for all $M \\in \\mathcal{C}_M$, the minimum objective value, $J_{\\min} = \\min_{M \\in \\mathcal{C}_M} J(M)$, is identified. It is possible that multiple medoid sets achieve this minimum value. Let the set of optimal medoid sets be $\\mathcal{M}_{\\text{opt}} = \\{M \\in \\mathcal{C}_M \\mid J(M) = J_{\\min}\\}$. A medoid-set tie occurs if $|\\mathcal{M}_{\\text{opt}}| > 1$. The tie-breaking rule $T_m$ is applied to select a single medoid set from $\\mathcal{M}_{\\text{opt}}$:\n-   Under Policy $\\mathcal{A}$, the lexicographically smallest set is chosen. Assuming the indices within each set are sorted, this corresponds to finding $\\min(\\mathcal{M}_{\\text{opt}})$ using standard tuple comparison.\n-   Under Policy $\\mathcal{B}$, the lexicographically largest set is chosen, i.e., $\\max(\\mathcal{M}_{\\text{opt}})$.\n\nLet the uniquely chosen medoid set be $M_{\\text{final}}$.\n\n**4. Final Assignment and Assignment Tie-Breaking ($T_a$)**\nWith the final medoid set $M_{\\text{final}}$ determined, each data point $i \\in \\{0, \\dots, n-1\\}$ is assigned to a medoid in $M_{\\text{final}}$. The assignment for point $i$ is a medoid $m_i^* \\in M_{\\text{final}}$ such that $D_{i, m_i^*} = \\min_{m \\in M_{\\text{final}}} D_{i,m}$. An assignment tie occurs if the set of medoids satisfying this condition, $M_i^{\\text{tie}} = \\{m \\in M_{\\text{final}} \\mid D_{i,m} = \\min_{m' \\in M_{\\text{final}}} D_{i,m'}\\}$, contains more than one element. The assignment tie-breaking rule $T_a$ resolves this:\n-   Under Policy $\\mathcal{A}$, the point $i$ is assigned to the medoid with the smallest index in $M_i^{\\text{tie}}$, so $m_i^* = \\min(M_i^{\\text{tie}})$.\n-   Under Policy $\\mathcal{B}$, the point $i$ is assigned to the medoid with the largest index in $M_i^{\\text{tie}}$, so $m_i^* = \\max(M_i^{\\text{tie}})$.\n\nThis process yields a final assignment vector where the $i$-th element is the index of the medoid to which point $i$ is assigned.\n\nThe algorithm proceeds by encapsulating this logic into a function that can be executed for each policy and each test case. The results are then aggregated and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef solve_k_medoids_exact(D, k, policy):\n    \"\"\"\n    Implements an exact k-medoids solver with specified tie-breaking policies.\n\n    Args:\n        D (np.ndarray): A precomputed n x n dissimilarity matrix.\n        k (int): The number of medoids.\n        policy (str): The tie-breaking policy, either 'A' or 'B'.\n            - Policy 'A': Lexicographically smallest for medoid-set ties,\n                          smallest index for assignment ties.\n            - Policy 'B': Lexicographically largest for medoid-set ties,\n                          largest index for assignment ties.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The final medoid indices, sorted.\n            - list: The assignment vector for each point.\n            - int: The objective function value.\n    \"\"\"\n    n = D.shape[0]\n    indices = range(n)\n    \n    # Generate all candidate medoid sets\n    candidate_medoid_sets = list(combinations(indices, k))\n    \n    min_objective_val = float('inf')\n    optimal_medoid_sets = []\n    \n    # Step 1 & 2: Enumerate and calculate objective function J(M)\n    for medoids_tuple in candidate_medoid_sets:\n        medoids = list(medoids_tuple)\n        \n        # For each point, find the dissimilarity to the nearest medoid\n        dissimilarities = np.min(D[:, medoids], axis=1)\n        objective_val = np.sum(dissimilarities)\n        \n        # Check for a new minimum or a tie\n        if objective_val < min_objective_val:\n            min_objective_val = objective_val\n            optimal_medoid_sets = [medoids_tuple]\n        elif objective_val == min_objective_val:\n            optimal_medoid_sets.append(medoids_tuple)\n\n    # Step 3: Apply medoid-set tie-breaking (Tm)\n    if policy == 'A':\n        final_medoids_tuple = min(optimal_medoid_sets)\n    elif policy == 'B':\n        final_medoids_tuple = max(optimal_medoid_sets)\n    else:\n        raise ValueError(\"Policy must be 'A' or 'B'\")\n        \n    final_medoids = list(final_medoids_tuple)\n\n    # Step 4: Apply assignment tie-breaking (Ta)\n    assignments = np.zeros(n, dtype=int)\n    for i in range(n):\n        # Dissimilarities from point i to each chosen medoid\n        d_to_medoids = D[i, final_medoids]\n        min_dist = np.min(d_to_medoids)\n        \n        # Find all medoids that achieve this minimum distance\n        tying_medoid_indices_in_list = np.where(d_to_medoids == min_dist)[0]\n        tying_medoids = [final_medoids[j] for j in tying_medoid_indices_in_list]\n        \n        if policy == 'A':\n            assignments[i] = min(tying_medoids)\n        else: # Policy 'B'\n            assignments[i] = max(tying_medoids)\n            \n    # The objective value is an integer as per problem statement's Ds\n    return sorted(final_medoids), assignments.tolist(), int(round(min_objective_val))\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 6, \"k\": 2,\n            \"D\": np.array([\n                [0, 1, 1, 4, 4, 4],\n                [1, 0, 1, 4, 4, 4],\n                [1, 1, 0, 4, 4, 4],\n                [4, 4, 4, 0, 1, 1],\n                [4, 4, 4, 1, 0, 1],\n                [4, 4, 4, 1, 1, 0],\n            ])\n        },\n        {\n            \"n\": 4, \"k\": 1,\n            \"D\": np.array([\n                [0, 1, 2, 1],\n                [1, 0, 1, 2],\n                [2, 1, 0, 1],\n                [1, 2, 1, 0],\n            ])\n        },\n        {\n            \"n\": 5, \"k\": 2,\n            \"D\": np.array([\n                [0, 0, 2, 2, 3],\n                [0, 0, 2, 2, 3],\n                [2, 2, 0, 0, 3],\n                [2, 2, 0, 0, 3],\n                [3, 3, 3, 3, 0],\n            ])\n        }\n    ]\n\n    all_case_results = []\n    for case in test_cases:\n        D_matrix, k_val = case[\"D\"], case[\"k\"]\n        \n        medoids_A, assignments_A, J_A = solve_k_medoids_exact(D_matrix, k_val, 'A')\n        medoids_B, assignments_B, J_B = solve_k_medoids_exact(D_matrix, k_val, 'B')\n        \n        j_equal = (J_A == J_B)\n        medoids_equal = (medoids_A == medoids_B)\n        \n        case_result = [\n            medoids_A,\n            assignments_A,\n            J_A,\n            medoids_B,\n            assignments_B,\n            J_B,\n            j_equal,\n            medoids_equal\n        ]\n        all_case_results.append(case_result)\n\n    # Custom string formatting to match the required output (no spaces in lists)\n    def format_item(item):\n        if isinstance(item, list):\n            return f\"[{','.join(map(str, item))}]\"\n        return str(item).lower() if isinstance(item, bool) else str(item)\n\n    case_strings = []\n    for case_res in all_case_results:\n        item_strings = [format_item(item) for item in case_res]\n        case_strings.append(f\"[{','.join(item_strings)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3135303"}]}