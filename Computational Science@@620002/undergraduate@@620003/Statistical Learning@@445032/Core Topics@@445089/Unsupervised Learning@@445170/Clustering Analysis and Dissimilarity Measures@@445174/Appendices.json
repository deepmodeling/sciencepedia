{"hands_on_practices": [{"introduction": "The choice of a distance metric is a foundational decision in clustering analysis, but its effectiveness is deeply intertwined with data preprocessing. This exercise explores the fundamental differences between the Euclidean ($L_2$) and Manhattan ($L_1$) distances. Through a hands-on coding practice [@problem_id:3109629], you will quantify how feature scaling can dramatically alter clustering outcomes, revealing the particular sensitivity of the Manhattan distance to the relative scales of the feature axes.", "problem": "You are given three small datasets of points in a feature space and a fixed number of clusters for each dataset. Your task is to write a complete, runnable program that, for each dataset, analyzes how normalization to unit variance per feature affects clustering under the Manhattan distance, compares clustering outcomes to those from Euclidean distance, and quantifies axis-aligned sensitivity of the Manhattan distance before and after normalization.\n\nFoundational definitions to be used are as follows. Let there be $n$ points $\\{\\mathbf{x}_i\\}_{i=1}^n$ in $\\mathbb{R}^d$, where $\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{id})$. The Manhattan distance (also known as the $\\ell_1$ distance) between two points $\\mathbf{x}$ and $\\mathbf{y}$ is\n$$\nD_{\\text{Manhattan}}(\\mathbf{x}, \\mathbf{y}) = \\sum_{j=1}^d \\left| x_j - y_j \\right|.\n$$\nThe Euclidean distance (also known as the $\\ell_2$ distance) between two points $\\mathbf{x}$ and $\\mathbf{y}$ is\n$$\nD_{\\text{Euclidean}}(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{j=1}^d \\left( x_j - y_j \\right)^2 }.\n$$\nNormalization to unit variance per feature is achieved by scaling each feature $j$ by its standard deviation $\\sigma_j$, defined using the population variance\n$$\n\\sigma_j = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{ij} - \\bar{x}_j \\right)^2 }, \\quad \\text{where} \\quad \\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}.\n$$\nThe normalized data $\\tilde{\\mathbf{x}}_i$ is given by componentwise division without mean-centering:\n$$\n\\tilde{x}_{ij} = \\begin{cases}\n\\frac{x_{ij}}{\\sigma_j} & \\text{if } \\sigma_j > 0, \\\\\nx_{ij} & \\text{if } \\sigma_j = 0.\n\\end{cases}\n$$\nPerform clustering using Hierarchical Clustering (HC) with complete linkage, where the distance between two clusters is the maximum pairwise distance between their points, and cutting the dendrogram to produce exactly $k$ clusters via the maximum-clusters criterion.\n\nTo compare two clusterings, form the set of co-clustered unordered index pairs under each clustering. For a clustering represented by labels $\\{c_i\\}_{i=1}^n$, define the set\n$$\nS = \\{ (i, l) \\mid 1 \\le i < l \\le n, \\; c_i = c_l \\}.\n$$\nGiven two such sets $S_1$ and $S_2$, quantify their similarity using the Jaccard index\n$$\nJ(S_1, S_2) = \\frac{ \\left| S_1 \\cap S_2 \\right| }{ \\left| S_1 \\cup S_2 \\right| }.\n$$\nFinally, quantify the axis-aligned sensitivity of the Manhattan distance as follows. For the dataset $X \\in \\mathbb{R}^{n \\times d}$, define the mean absolute difference per feature\n$$\n\\mu_j = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i < l \\le n} \\left| x_{ij} - x_{lj} \\right|,\n$$\nand the mean Manhattan distance over all unordered pairs\n$$\nM = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i < l \\le n} \\sum_{j=1}^d \\left| x_{ij} - x_{lj} \\right|.\n$$\nDefine the axis contribution fractions $f_j = \\mu_j / M$ and the sensitivity index\n$$\nS_{\\text{axis}} = \\max_{1 \\le j \\le d} f_j.\n$$\nCompute $S_{\\text{axis}}$ before and after normalization and report the reduction $S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$.\n\nImplement the following for each dataset:\n- Compute hierarchical clustering with complete linkage and Manhattan distance on the raw data to obtain labels $\\{c_i^{\\text{L1,raw}}\\}$.\n- Normalize to unit variance per feature to obtain $\\tilde{X}$.\n- Compute hierarchical clustering with complete linkage and Manhattan distance on $\\tilde{X}$ to obtain labels $\\{c_i^{\\text{L1,norm}}\\}$.\n- Compute hierarchical clustering with complete linkage and Euclidean distance on $\\tilde{X}$ to obtain labels $\\{c_i^{\\text{L2,norm}}\\}$.\n- Compute the Jaccard index $J\\!\\left(S\\!\\left(c^{\\text{L1,raw}}\\right), S\\!\\left(c^{\\text{L1,norm}}\\right)\\right)$.\n- Compute the Jaccard index $J\\!\\left(S\\!\\left(c^{\\text{L1,norm}}\\right), S\\!\\left(c^{\\text{L2,norm}}\\right)\\right)$.\n- Compute the reduction in axis sensitivity $S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$.\n\nTest suite:\n- Test case $1$ ($k = 2$, $d = 2$): points\n  $\\left(-8, 0\\right)$, $\\left(-7, 1\\right)$, $\\left(-6, -1\\right)$, $\\left(-9, 0.5\\right)$, $\\left(-7, -0.5\\right)$, $\\left(3, 4\\right)$, $\\left(4, 4.5\\right)$, $\\left(2.5, 5\\right)$, $\\left(3.5, 3.5\\right)$, $\\left(4.5, 5.2\\right)$.\n- Test case $2$ ($k = 3$, $d = 3$): points\n  $\\left(0, 0, 100\\right)$, $\\left(1, 0, 100\\right)$, $\\left(0, 1, 100\\right)$, $\\left(5, 5, 100\\right)$, $\\left(6, 5, 100\\right)$, $\\left(5, 6, 100\\right)$, $\\left(-4, 5, 100\\right)$, $\\left(-5, 5, 100\\right)$, $\\left(-4, 6, 100\\right)$.\n- Test case $3$ ($k = 2$, $d = 2$): points\n  $\\left(0, 0\\right)$, $\\left(10, 0\\right)$, $\\left(0, 50\\right)$, $\\left(10, 50\\right)$, $\\left(100, 5\\right)$, $\\left(110, 5\\right)$.\n\nOutput specification:\n- For each test case, compute a triple of floats $\\left[J_1, J_2, R\\right]$ where $J_1 = J\\!\\left(S\\!\\left(c^{\\text{L1,raw}}\\right), S\\!\\left(c^{\\text{L1,norm}}\\right)\\right)$, $J_2 = J\\!\\left(S\\!\\left(c^{\\text{L1,norm}}\\right), S\\!\\left(c^{\\text{L2,norm}}\\right)\\right)$, and $R = S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$.\n- Express all floats as decimals rounded to three places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the triple for one test case. The required format is\n$$\n\\left[ [J_{1,1}, J_{2,1}, R_1], [J_{1,2}, J_{2,2}, R_2], [J_{1,3}, J_{2,3}, R_3] \\right].\n$$\nEnsure scientific realism by strictly adhering to the definitions above and to the specified test suite; do not introduce any external randomness or data.", "solution": "The problem requires an analysis of the effects of feature normalization on clustering outcomes using the Manhattan distance. This analysis involves comparing clusterings, quantifying the similarity between them, and measuring the change in axis-aligned sensitivity of the distance metric. The problem is well-posed, scientifically grounded in the principles of statistical learning, and provides all necessary definitions, data, and parameters for a deterministic solution.\n\nThe solution is implemented by following a structured, multi-step computational process for each provided dataset.\n\n**Step 1: Data Representation and Normalization**\n\nFor each test case, the input data points are first represented as an $n \\times d$ matrix $X$, where $n$ is the number of points and $d$ is the number of features (dimensions).\n\nNormalization to unit variance is performed on this matrix. For each feature (column) $j \\in \\{1, \\dots, d\\}$, the population standard deviation $\\sigma_j$ is calculated:\n$$\n\\sigma_j = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{ij} - \\bar{x}_j \\right)^2 }\n$$\nwhere $\\bar{x}_j$ is the mean of the $j$-th feature. The normalized data matrix $\\tilde{X}$ is then obtained by componentwise division. Each element $x_{ij}$ of the original matrix $X$ is transformed into $\\tilde{x}_{ij}$:\n$$\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sigma_j}\n$$\nA special case is handled where a feature has zero variance ($\\sigma_j = 0$), in which case the original values are retained ($\\tilde{x}_{ij} = x_{ij}$). This normalization process scales each feature so that its variance becomes $1$, without altering its mean from what is implied by the original data divided by $\\sigma_j$. This is distinct from standard scaling, as mean-centering is not performed.\n\n**Step 2: Hierarchical Clustering**\n\nHierarchical Clustering (HC) with complete linkage is applied. The algorithm proceeds as follows:\n1.  A pairwise distance matrix is computed for all points using a specified distance metric. The problem requires using both Manhattan ($D_{\\text{Manhattan}}$) and Euclidean ($D_{\\text{Euclidean}}$) distances.\n2.  The `complete` linkage criterion is used, where the distance between two clusters is defined as the maximum distance between any point in the first cluster and any point in the second cluster.\n3.  The clustering process generates a dendrogram, which is a tree representing the hierarchy of merges.\n4.  This dendrogram is cut to form exactly $k$ clusters, as specified for each test case. This is achieved by using the `maxclust` criterion.\n\nThis process is performed three times for each dataset to obtain three distinct clusterings:\n-   $c^{\\text{L1,raw}}$: Using Manhattan distance on the original, raw data $X$.\n-   $c^{\\text{L1,norm}}$: Using Manhattan distance on the normalized data $\\tilde{X}$.\n-   $c^{\\text{L2,norm}}$: Using Euclidean distance on the normalized data $\\tilde{X}$.\n\n**Step 3: Comparison of Clusterings via Jaccard Index**\n\nTo compare the outcomes of two different clusterings (e.g., before and after normalization), we first construct the set of all unordered pairs of data points that are placed in the same cluster. For a given clustering represented by an array of labels $\\{c_i\\}_{i=1}^n$, this set is:\n$$\nS = \\{ (i, l) \\mid 1 \\le i < l \\le n, \\; c_i = c_l \\}\n$$\nGiven two such sets, $S_1$ and $S_2$, from two different clusterings, their similarity is quantified by the Jaccard index:\n$$\nJ(S_1, S_2) = \\frac{ \\left| S_1 \\cap S_2 \\right| }{ \\left| S_1 \\cup S_2 \\right| }\n$$\nThis index measures the ratio of the number of co-clustered pairs common to both clusterings to the total number of unique co-clustered pairs across both. A value of $1$ indicates identical clusterings, while $0$ indicates no shared co-clustered pairs. We compute two such indices:\n-   $J_1 = J\\!\\left(S\\!\\left(c^{\\text{L1,raw}}\\right), S\\!\\left(c^{\\text{L1,norm}}\\right)\\right)$, comparing Manhattan clustering before and after normalization.\n-   $J_2 = J\\!\\left(S\\!\\left(c^{\\text{L1,norm}}\\right), S\\!\\left(c^{\\text{L2,norm}}\\right)\\right)$, comparing Manhattan and Euclidean clusterings on normalized data.\n\n**Step 4: Axis-Aligned Sensitivity Analysis**\n\nThe Manhattan distance is sensitive to the scale of the features, as it is a simple sum of absolute differences along each axis. To quantify this, we define an axis sensitivity index, $S_{\\text{axis}}$. This requires calculating the mean absolute difference for each feature, $\\mu_j$, and the total mean Manhattan distance, $M$, over all pairs of points:\n$$\n\\mu_j = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i < l \\le n} \\left| x_{ij} - x_{lj} \\right| \\quad \\text{and} \\quad M = \\sum_{j=1}^d \\mu_j\n$$\nThe contribution of each feature to the total mean Manhattan distance is given by the fraction $f_j = \\mu_j / M$. The sensitivity index is the maximum of these fractions, capturing the contribution of the most dominant feature:\n$$\nS_{\\text{axis}} = \\max_{1 \\le j \\le d} f_j\n$$\nBy normalizing the data to unit variance, the scales of the features are equalized, which is expected to reduce the dominance of any single feature. We compute $S_{\\text{axis}}$ for both the raw data ($S_{\\text{axis}}^{\\text{before}}$) and the normalized data ($S_{\\text{axis}}^{\\text{after}}$). The reduction in sensitivity is then $R = S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$.\n\n**Step 5: Final Computations and Output**\n\nThe procedure outlined in Steps 1-4 is executed for each of the three test cases provided. For each case, we compute the triplet of values $[J_1, J_2, R]$, where each float is rounded to three decimal places. The results for all test cases are then aggregated into a single list as specified by the output format.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef compute_sensitivity(X: np.ndarray) -> float:\n    \"\"\"Computes the axis-aligned sensitivity index for a dataset.\"\"\"\n    n, d = X.shape\n    if n < 2:\n        return 0.0\n\n    num_pairs = n * (n - 1) / 2\n    \n    # Calculate mu_j for each feature j\n    # mu_j is the mean absolute difference for feature j over all pairs of points\n    mu_j = np.zeros(d)\n    for j in range(d):\n        # pdist on a single column gives the condensed vector of absolute differences\n        # between all pairs, for which we take the sum.\n        col_distances = pdist(X[:, j:j+1], metric='cityblock')\n        mu_j[j] = np.sum(col_distances) / num_pairs\n\n    # M is the mean Manhattan distance over all pairs of points, which is sum of mu_j\n    M = np.sum(mu_j)\n    \n    # If all points are identical, M=0. f_j would be 0/0.\n    # In this scenario, no axis dominates, so sensitivity is arguably 0 or 1/d.\n    # We return 0 as no single axis contributes more than any other.\n    if M == 0:\n        return 0.0\n\n    # f_j are the axis contribution fractions\n    f_j = mu_j / M\n    \n    # S_axis is the maximum contribution fraction\n    S_axis = np.max(f_j)\n    \n    return S_axis\n\ndef get_co_clustered_pairs(labels: np.ndarray) -> set:\n    \"\"\"Generates the set of co-clustered unordered pairs.\"\"\"\n    n = len(labels)\n    pairs = set()\n    for i in range(n):\n        for l in range(i + 1, n):\n            if labels[i] == labels[l]:\n                pairs.add((i, l))\n    return pairs\n\ndef jaccard_index(set1: set, set2: set) -> float:\n    \"\"\"Computes the Jaccard index between two sets.\"\"\"\n    intersection_size = len(set1.intersection(set2))\n    union_size = len(set1.union(set2))\n    \n    if union_size == 0:\n        return 1.0  # By convention, Jaccard index of two empty sets is 1\n        \n    return intersection_size / union_size\n\ndef analyze_dataset(data: list, k: int) -> list:\n    \"\"\"\n    Performs the full analysis for a single dataset: clustering, comparison,\n    and sensitivity measurement.\n    \"\"\"\n    X_raw = np.array(data, dtype=float)\n    \n    # 1. Compute axis sensitivity on raw data\n    S_axis_before = compute_sensitivity(X_raw)\n\n    # 2. Perform hierarchical clustering on raw data with Manhattan distance\n    dist_l1_raw = pdist(X_raw, metric='cityblock')\n    Z_l1_raw = linkage(dist_l1_raw, method='complete')\n    c_l1_raw = fcluster(Z_l1_raw, t=k, criterion='maxclust')\n\n    # 3. Normalize data to unit variance (population std dev)\n    # The problem specifies using population std dev (ddof=0 in numpy.std)\n    sigmas = np.std(X_raw, axis=0, ddof=0)\n    # Handle columns with zero variance by not scaling them (division by 1)\n    non_zero_sigmas = np.where(sigmas > 0, sigmas, 1.0)\n    X_norm = X_raw / non_zero_sigmas\n\n    # 4. Compute axis sensitivity on normalized data\n    S_axis_after = compute_sensitivity(X_norm)\n\n    # 5. Perform clustering on normalized data\n    # Manhattan distance\n    dist_l1_norm = pdist(X_norm, metric='cityblock')\n    Z_l1_norm = linkage(dist_l1_norm, method='complete')\n    c_l1_norm = fcluster(Z_l1_norm, t=k, criterion='maxclust')\n\n    # Euclidean distance\n    dist_l2_norm = pdist(X_norm, metric='euclidean')\n    Z_l2_norm = linkage(dist_l2_norm, method='complete')\n    c_l2_norm = fcluster(Z_l2_norm, t=k, criterion='maxclust')\n\n    # 6. Compute Jaccard indices by comparing co-clustering sets\n    S_l1_raw = get_co_clustered_pairs(c_l1_raw)\n    S_l1_norm = get_co_clustered_pairs(c_l1_norm)\n    S_l2_norm = get_co_clustered_pairs(c_l2_norm)\n\n    J1 = jaccard_index(S_l1_raw, S_l1_norm)\n    J2 = jaccard_index(S_l1_norm, S_l2_norm)\n\n    # 7. Compute reduction in axis sensitivity\n    R = S_axis_before - S_axis_after\n\n    return [round(J1, 3), round(J2, 3), round(R, 3)]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run analysis, and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"k\": 2, \"d\": 2, \"points\": [\n                (-8, 0), (-7, 1), (-6, -1), (-9, 0.5), (-7, -0.5),\n                (3, 4), (4, 4.5), (2.5, 5), (3.5, 3.5), (4.5, 5.2)\n            ]\n        },\n        {\n            \"k\": 3, \"d\": 3, \"points\": [\n                (0, 0, 100), (1, 0, 100), (0, 1, 100),\n                (5, 5, 100), (6, 5, 100), (5, 6, 100),\n                (-4, 5, 100), (-5, 5, 100), (-4, 6, 100)\n            ]\n        },\n        {\n            \"k\": 2, \"d\": 2, \"points\": [\n                (0, 0), (10, 0), (0, 50), (10, 50),\n                (100, 5), (110, 5)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_triple = analyze_dataset(case[\"points\"], case[\"k\"])\n        results.append(result_triple)\n        \n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{res[0]:.3f},{res[1]:.3f},{res[2]:.3f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3109629"}, {"introduction": "Even with properly scaled features, standard distance metrics can fail when data clusters are not spherical. This practice demonstrates this common pitfall by presenting elongated clusters that fool algorithms like DBSCAN when used with the Euclidean distance. In this exercise [@problem_id:3109620], you will first witness this failure and then implement a solution using the powerful Mahalanobis distance, which adapts to the data's covariance structure to correctly identify the underlying groups.", "problem": "You are given the task to implement a complete program that constructs synthetic data comprising two elongated Gaussian clusters, applies clustering using Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and evaluates the effect of switching the dissimilarity measure from Euclidean distance to Mahalanobis distance adapted to the sample covariance. The program should be entirely self-contained and produce a single line of output aggregating results for a small test suite.\n\nThe fundamental base consists of the following well-tested definitions and facts:\n- A bivariate normal distribution with mean vector $\\mu \\in \\mathbb{R}^2$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ is specified by the probability density function $p(x) = \\frac{1}{2\\pi \\sqrt{\\det(\\Sigma)}} \\exp\\left( -\\frac{1}{2}(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)$ for $x \\in \\mathbb{R}^2$.\n- The Euclidean distance between two points $x, y \\in \\mathbb{R}^2$ is $d_E(x,y) = \\|x - y\\|_2 = \\sqrt{(x - y)^\\top (x - y)}$.\n- Given a symmetric positive-definite covariance matrix $\\Sigma$, the Mahalanobis distance between $x, y \\in \\mathbb{R}^2$ is $d_M(x,y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}$. If $\\Sigma = L L^\\top$ via the Cholesky factorization with $L$ lower triangular, then $d_M(x,y) = \\|L^{-1}(x - y)\\|_2$, which means Mahalanobis distance equals Euclidean distance after a linear whitening transform $z = L^{-1} x$.\n- DBSCAN is defined by parameters $\\varepsilon > 0$ (neighborhood radius) and $m \\in \\mathbb{N}$ (minimum number of points). For a dataset $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$ and a dissimilarity $d(\\cdot,\\cdot)$, define the $\\varepsilon$-neighborhood of $x_i$ as $N_\\varepsilon(x_i) = \\{x_j : d(x_i, x_j) \\le \\varepsilon\\}$. A point $x_i$ is a core point if $|N_\\varepsilon(x_i)| \\ge m$. Clusters are formed by iteratively expanding from core points using density-reachability, assigning both core points and border points (points within the neighborhood of some core point) to the same cluster, while points not assigned to any cluster are labeled as noise.\n\nProblem requirements:\n1. Construct two elongated Gaussian clusters in $\\mathbb{R}^2$ with a shared covariance matrix designed via eigen-decomposition. Use orthonormal eigenvectors $v_1 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$ and $v_2 = \\frac{1}{\\sqrt{2}}(-1,1)^\\top$, and eigenvalues $\\lambda_1$ (large, producing elongation along $v_1$) and $\\lambda_2$ (small, producing thin spread along $v_2$). Let $Q = [v_1 \\ v_2] \\in \\mathbb{R}^{2 \\times 2}$, and set $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\lambda_2) Q^\\top$. Place the two cluster means symmetrically along the low-variance direction $v_2$ at $\\mu_1 = -d \\, v_2$ and $\\mu_2 = +d \\, v_2$, where $d > 0$ governs the half-separation magnitude.\n2. Implement DBSCAN from first principles using the given definitions, without relying on external clustering libraries. Use both $d_E$ and $d_M$, where for the Mahalanobis case you must estimate the covariance from the entire dataset using the unbiased sample covariance estimator $$\\hat{\\Sigma} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^\\top,$$ where $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. Ensure numerical stability by adding a small ridge $\\lambda I$ with $\\lambda = 10^{-8}$ to $\\hat{\\Sigma}$ before Cholesky factorization.\n3. For each test case, run DBSCAN twice: once with Euclidean distance (baseline) and once with Mahalanobis distance adapted to $\\hat{\\Sigma}$ via the whitening transform. Use the same $\\varepsilon$ and $m$ across the two runs in each test case. For each run, report the integer number of discovered clusters (excluding noise labeled as $-1$). Assess separation improvement by a boolean indicating whether the Mahalanobis run finds strictly more clusters than the Euclidean run for that test case.\n\nTest suite:\n- Test case $1$ (happy path): $n_{\\text{per}} = 400$ points per cluster (total $n = 800$), $\\lambda_1 = 100$, $\\lambda_2 = 0.25$, $d = 3$, $\\varepsilon = 7.0$, $m = 10$, random seed $= 123$.\n- Test case $2$ (boundary condition): $n_{\\text{per}} = 400$, $\\lambda_1 = 100$, $\\lambda_2 = 0.25$, $d = 3$, $\\varepsilon = 5.5$, $m = 10$, random seed $= 321$.\n- Test case $3$ (edge case with smaller separation): $n_{\\text{per}} = 300$, $\\lambda_1 = 100$, $\\lambda_2 = 0.25$, $d = 2$, $\\varepsilon = 5.0$, $m = 10$, random seed $= 42$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of the form $[\\text{clusters\\_euclidean}, \\text{clusters\\_mahalanobis}, \\text{improved}]$. For example, the output format is $$[[c_{E,1}, c_{M,1}, b_1],[c_{E,2}, c_{M,2}, b_2],[c_{E,3}, c_{M,3}, b_3]],$$ with $c_{E,i}$ and $c_{M,i}$ integers and $b_i$ booleans. No physical units, angle units, or percentage representations are involved in this problem.", "solution": "The user-provided problem has been analyzed and is determined to be **valid**. The problem is scientifically sound, well-posed, objective, and contains all necessary information for a unique, verifiable solution. It is a standard exercise in statistical learning, demonstrating the utility of adaptive distance metrics like the Mahalanobis distance in clustering algorithms for non-spherical data distributions.\n\n### Principle-Based Solution Design\n\nThe solution proceeds in three main stages: synthetic data generation, implementation and application of the DBSCAN clustering algorithm with two different distance metrics, and evaluation of the results.\n\n#### 1. Synthetic Data Generation\n\nThe problem requires the creation of a dataset comprising two distinct, elongated clusters. This is achieved by sampling from two bivariate Gaussian distributions, $\\mathcal{N}(\\mu_1, \\Sigma)$ and $\\mathcal{N}(\\mu_2, \\Sigma)$.\n\n1.  **Covariance Matrix $\\Sigma$**: The clusters are designed to be elongated and rotated. This is defined through the eigen-decomposition of the shared covariance matrix, $\\Sigma = Q \\Lambda Q^\\top$.\n    -   The eigenvectors $v_1 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$ and $v_2 = \\frac{1}{\\sqrt{2}}(-1,1)^\\top$ form the columns of the orthogonal rotation matrix $Q = [v_1 \\ v_2]$. These vectors define the principal axes of the clusters.\n    -   The eigenvalues, $\\lambda_1$ (large) and $\\lambda_2$ (small), are the variances along these principal axes. The large value of $\\lambda_1$ causes elongation along $v_1$, while the small $\\lambda_2$ ensures the clusters are thin along $v_2$. The eigenvalue matrix is $\\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2)$.\n\n2.  **Cluster Means $\\mu_1, \\mu_2$**: The two clusters are separated along their narrow axis, $v_2$. The mean vectors are placed symmetrically about the origin at $\\mu_1 = -d \\cdot v_2$ and $\\mu_2 = d \\cdot v_2$, where $d$ is a given separation parameter. The total Euclidean distance between the cluster centers is $2d$.\n\nFor each test case, we generate $n_{\\text{per}}$ data points from each of the two distributions, resulting in a total dataset of $n = 2 n_{\\text{per}}$ points. A specified random seed ensures the reproducibility of the generated data.\n\n#### 2. DBSCAN Implementation and Application\n\nWe implement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm from first principles. DBSCAN identifies clusters based on the density of points.\n\n-   **Core Concepts**: The algorithm is governed by two parameters: the neighborhood radius $\\varepsilon$ and the minimum number of points $m$. A point is a **core point** if its $\\varepsilon$-neighborhood contains at least $m$ points (including itself). A **border point** is not a core point but lies in the neighborhood of a core point. A **noise point** is neither a core nor a border point. Clusters are formed by connecting core points that are in each other's neighborhoods, and all border points are assigned to the cluster of a nearby core point.\n\n-   **Implementation**: Our implementation iterates through all points. If an unvisited point is found to be a core point, a new cluster is initiated, and all density-reachable points are found via a queue-based expansion process. To improve efficiency, we pre-calculate the matrix of all pairwise distances between points before starting the main loop. Points not assigned to any cluster are labeled as noise ($-1$).\n\nWe apply this DBSCAN implementation in two scenarios for each test case:\n\n**a. Baseline: Euclidean Distance ($d_E$)**\nThe first run uses the standard Euclidean distance, $d_E(x,y) = \\|x-y\\|_2$. For elongated clusters, this metric can be misleading. The distance between two points at opposite ends of the same cluster can be larger than the distance between two points in different but adjacent clusters. This often causes DBSCAN to incorrectly merge the distinct elongated clusters, especially if $\\varepsilon$ is chosen to be comparable to or larger than the inter-cluster separation.\n\n**b. Adaptive: Mahalanobis Distance ($d_M$)**\nThe second run addresses the shortcomings of the Euclidean metric by using the Mahalanobis distance, which accounts for the covariance of the data. Given a covariance matrix $S$, the distance is $d_M(x,y) = \\sqrt{(x-y)^\\top S^{-1}(x-y)}$. This metric effectively measures distance in a \"whitened\" space where the data has an identity covariance matrix (i.e., is spherical).\n\nThe solution utilizes an efficient method to apply this metric, as suggested by the problem statement:\n1.  **Estimate Covariance**: The sample covariance matrix $\\hat{\\Sigma}$ is estimated from the entire dataset using the unbiased estimator: $\\hat{\\Sigma} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^\\top$.\n2.  **Regularization & Decomposition**: For numerical stability, a small ridge $\\lambda I$ (with $\\lambda=10^{-8}$) is added to $\\hat{\\Sigma}$. The resulting matrix is decomposed using Cholesky factorization: $\\hat{\\Sigma}_{\\text{reg}} = L L^\\top$, where $L$ is a lower-triangular matrix.\n3.  **Whitening Transform**: The entire dataset $X$ is transformed into a new dataset $Z$ where the Mahalanobis distance on $X$ corresponds to the Euclidean distance on $Z$. The transformation is $z_i^\\top = x_i^\\top (L^{-1})^\\top$.\n4.  **Clustering**: DBSCAN is then run on the whitened data $Z$ using the standard Euclidean distance. The distance between transformed points $z_i$ and $z_j$ is $\\|z_i - z_j\\|_2 = \\|L^{-1}(x_i - x_j)\\|_2$, which is precisely the Mahalanobis distance between the original points $x_i$ and $x_j$.\n\nAfter this transformation, the elongated clusters in the original space become spherical in the whitened space, making them easily separable by a density-based algorithm like DBSCAN, provided $\\varepsilon$ is chosen appropriately.\n\n#### 3. Evaluation\nFor each test case, the number of clusters found is determined by counting the number of unique positive cluster labels assigned by DBSCAN (noise, labeled $-1$, is excluded). The total outputs are:\n-   $c_E$: Number of clusters found using Euclidean distance.\n-   $c_M$: Number of clusters found using Mahalanobis distance.\n-   `improved`: A boolean flag, `True` if $c_M > c_E$, and `False` otherwise.\n\nThis evaluation directly quantifies the advantage of using a data-adaptive distance metric for the given clustering problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, inv\n\n# Global constants for DBSCAN labels\nUNVISITED = 0\nNOISE = -1\n\ndef _range_query(n_points, dist_matrix, p_idx, eps):\n    \"\"\"Finds all points within eps distance of point p_idx using a precomputed distance matrix.\"\"\"\n    return np.where(dist_matrix[p_idx] <= eps)[0]\n\ndef _expand_cluster(n_points, dist_matrix, labels, p_idx, neighbor_indices, cluster_id, eps, m):\n    \"\"\"Expands a cluster from a core point.\"\"\"\n    labels[p_idx] = cluster_id\n    \n    # Use a list as a queue, processing with an index to avoid pop(0)\n    queue = list(neighbor_indices)\n    head_idx = 0\n    while head_idx < len(queue):\n        current_p_idx = queue[head_idx]\n        head_idx += 1\n\n        if labels[current_p_idx] == NOISE:\n            labels[current_p_idx] = cluster_id  # Change noise to border point\n        \n        if labels[current_p_idx] == UNVISITED:\n            labels[current_p_idx] = cluster_id\n            \n            # Find neighbors of the current point\n            current_neighbor_indices = _range_query(n_points, dist_matrix, current_p_idx, eps)\n            \n            if len(current_neighbor_indices) >= m:\n                # This is a core point, add its neighbors to the queue\n                for neighbor_idx in current_neighbor_indices:\n                    # Add only points that are unvisited or noise\n                    if labels[neighbor_idx] in [UNVISITED, NOISE]:\n                        queue.append(neighbor_idx)\n\ndef dbscan(X, eps, m):\n    \"\"\"\n    Performs DBSCAN clustering from first principles.\n\n    Args:\n        X (np.ndarray): Data points, shape (n_samples, n_features).\n        eps (float): The radius of a neighborhood.\n        m (int): The minimum number of points required to form a dense region.\n\n    Returns:\n        np.ndarray: Array of cluster labels for each point. Noise is labeled -1.\n    \"\"\"\n    n_points = X.shape[0]\n    \n    # Pre-compute pairwise Euclidean distance matrix for efficiency\n    dist_matrix = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1))\n    \n    labels = np.full(n_points, UNVISITED, dtype=int)\n    cluster_id = 0\n    \n    for i in range(n_points):\n        if labels[i] != UNVISITED:\n            continue\n            \n        neighbor_indices = _range_query(n_points, dist_matrix, i, eps)\n        \n        if len(neighbor_indices) < m:\n            labels[i] = NOISE\n        else:\n            cluster_id += 1\n            _expand_cluster(n_points, dist_matrix, labels, i, neighbor_indices, cluster_id, eps, m)\n            \n    return labels\n\ndef generate_data(n_per, lambda1, lambda2, d, seed):\n    \"\"\"Generates two elongated Gaussian clusters.\"\"\"\n    np.random.seed(seed)\n    \n    v1 = np.array([1, 1]) / np.sqrt(2)\n    v2 = np.array([-1, 1]) / np.sqrt(2)\n    \n    Q = np.column_stack([v1, v2])\n    Lambda = np.diag([lambda1, lambda2])\n    Sigma = Q @ Lambda @ Q.T\n    \n    mu1 = -d * v2\n    mu2 = d * v2\n    \n    cluster1 = np.random.multivariate_normal(mu1, Sigma, n_per)\n    cluster2 = np.random.multivariate_normal(mu2, Sigma, n_per)\n    \n    return np.vstack([cluster1, cluster2])\n\ndef run_test_case(n_per, lambda1, lambda2, d, eps, m, seed):\n    \"\"\"Runs a single test case for the problem.\"\"\"\n    X = generate_data(n_per, lambda1, lambda2, d, seed)\n    n = X.shape[0]\n\n    # 1. Euclidean DBSCAN\n    labels_euclidean = dbscan(X, eps, m)\n    # Number of clusters is the max label, excluding noise (-1)\n    clusters_euclidean = len(np.unique(labels_euclidean[labels_euclidean > 0]))\n\n    # 2. Mahalanobis DBSCAN via whitening transform\n    # Estimate sample covariance\n    mu_hat = np.mean(X, axis=0)\n    Sigma_hat = np.cov(X, rowvar=False)\n    \n    # Regularize for numerical stability\n    ridge = 1e-8\n    Sigma_hat_reg = Sigma_hat + ridge * np.eye(X.shape[1])\n    \n    # Cholesky decomposition and whitening\n    L = cholesky(Sigma_hat_reg, lower=True)\n    L_inv = inv(L)\n    \n    # Transform data into whitened space\n    # z_i^T = (L^{-1} x_i^T)^T = x_i (L^{-1})^T\n    Z = X @ L_inv.T\n    \n    # Run DBSCAN on whitened data (equivalent to Mahalanobis on original data)\n    labels_mahalanobis = dbscan(Z, eps, m)\n    clusters_mahalanobis = len(np.unique(labels_mahalanobis[labels_mahalanobis > 0]))\n    \n    # 3. Evaluate improvement\n    improved = clusters_mahalanobis > clusters_euclidean\n    \n    return [clusters_euclidean, clusters_mahalanobis, improved]\n\ndef solve():\n    \"\"\"Main function to run test suite and print results.\"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_per, lambda1, lambda2, d, eps, m, seed)\n        (400, 100, 0.25, 3, 7.0, 10, 123),\n        (400, 100, 0.25, 3, 5.5, 10, 321),\n        (300, 100, 0.25, 2, 5.0, 10, 42),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_per, lambda1, lambda2, d, eps, m, seed = case\n        result = run_test_case(n_per, lambda1, lambda2, d, eps, m, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    case_strings = [f\"[{c_e},{c_m},{str(b).lower()}]\" for c_e, c_m, b in results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n\n```", "id": "3109620"}, {"introduction": "Real-world datasets are rarely perfect and often contain outliers that can distort clustering results. This final practice shifts focus from the distance metric itself to a critical choice within hierarchical clustering: the linkage criterion. By simulating datasets with outliers [@problem_id:3109639], you will quantitatively compare how single, complete, and average linkage methods behave, gaining practical insight into which criteria are more robust for isolating anomalous data points.", "problem": "You are to write a complete, runnable program that, for a given set of two-dimensional point clouds, computes hierarchical agglomerative clustering under the Euclidean norm (also called the $L_2$ norm) and quantitatively assesses the sensitivity of different linkage criteria to outliers. The program must use the following foundational definitions and facts as the base of reasoning:\n\n- The Euclidean norm $L_2$ on $\\mathbb{R}^2$ defines the dissimilarity between points $x = (x_1, x_2)$ and $y = (y_1, y_2)$ as $d(x,y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}$.\n- Hierarchical agglomerative clustering begins with $n$ singleton clusters and repeatedly merges the two clusters with the smallest inter-cluster dissimilarity according to a chosen linkage criterion until a single cluster remains.\n- The linkage criteria define the inter-cluster dissimilarity between nonempty sets $A$ and $B$ of points:\n    - Single linkage: $D_{\\text{single}}(A,B) = \\min_{x \\in A, y \\in B} d(x,y)$.\n    - Complete linkage: $D_{\\text{complete}}(A,B) = \\max_{x \\in A, y \\in B} d(x,y)$.\n    - Average linkage: $D_{\\text{average}}(A,B) = \\frac{1}{|A||B|} \\sum_{x \\in A} \\sum_{y \\in B} d(x,y)$.\n- The hierarchical clustering procedure produces a sequence of merges at heights $h_1 \\le h_2 \\le \\dots \\le h_{n-1}$, where each $h_k$ is the dissimilarity at which the $k$-th merge occurs; these heights define the dendrogram.\n\nFor each dataset, you will be given explicit indices labeling the outlier points. Define the set of outliers as $O \\subset \\{0,1,\\dots,n-1\\}$ and the set of inliers as $I = \\{0,1,\\dots,n-1\\} \\setminus O$. For a given linkage criterion, define the outlier isolation height $H$ to be the dendrogram height at the first merge where any cluster containing at least one outlier merges with a cluster containing only inliers. That is, if at some step two clusters $C_a$ and $C_b$ with merge height $h$ satisfy $\\left( C_a \\cap O \\ne \\emptyset \\right)$ and $\\left( C_b \\subseteq I \\right)$, or vice versa, and this is the earliest step where such a condition holds, then $H = h$.\n\nTo provide a scale for comparison, define the inlier compactness scale $S$ as the median over all pairwise Euclidean distances between distinct inliers, that is\n$$\nS = \\text{median}\\left( \\{ d(x_i, x_j) : i,j \\in I, i \\ne j \\} \\right).\n$$\nFor each linkage criterion, compute the isolation ratio $R = H / S$. This ratio quantifies how comparatively late the outliers are merged into the inlier mass relative to the internal inlier scale.\n\nYour task is to:\n- Implement hierarchical agglomerative clustering under $L_2$ for single, complete, and average linkage.\n- For each dataset in the test suite, compute the isolation height $H$ for each linkage, the scale $S$, and the isolation ratios $R$.\n- Demonstrate the claim that complete linkage isolates outliers under $L_2$ by producing, for each dataset, a boolean flag that is true if and only if the isolation ratio under complete linkage is strictly greater than the isolation ratios under both single and average linkage.\n\nTest suite:\nUse the following datasets. Each dataset is a list of points in $\\mathbb{R}^2$ given in order, and an explicit list of outlier indices $O$.\n\n- Test case 1 (happy path; one extreme outlier):\n    - Points: $(0.0,0.0)$, $(0.2,-0.1)$, $(-0.1,0.3)$, $(0.3,0.1)$, $(-0.2,-0.2)$, $(0.1,-0.3)$, $(20.0,0.0)$\n    - Outlier indices: $[6]$\n- Test case 2 (two compact inlier groups plus an extreme outlier):\n    - Points: $(0.0,0.0)$, $(0.1,0.0)$, $(0.0,0.1)$, $(-0.1,0.0)$, $(1.0,1.0)$, $(1.1,1.0)$, $(1.0,1.1)$, $(0.9,1.0)$, $(10.0,10.0)$\n    - Outlier indices: $[8]$\n- Test case 3 (moderately distant outlier):\n    - Points: $(0.0,0.0)$, $(0.2,0.0)$, $(-0.1,0.2)$, $(0.3,-0.1)$, $(-0.2,-0.1)$, $(0.1,-0.2)$, $(3.0,3.0)$\n    - Outlier indices: $[6]$\n- Test case 4 (two nearby outliers):\n    - Points: $(0.0,0.0)$, $(0.2,-0.1)$, $(-0.1,0.3)$, $(0.3,0.1)$, $(-0.2,-0.2)$, $(0.1,-0.3)$, $(20.0,0.0)$, $(20.0,0.5)$\n    - Outlier indices: $[6,7]$\n\nRequired final output format:\nYour program should produce a single line of output that aggregates the results of all test cases into one list. For each test case, output a sublist containing exactly four elements in the following order: the isolation height $H$ for single linkage, the isolation height $H$ for complete linkage, the isolation height $H$ for average linkage, and the boolean flag described above. The overall output must be a single line containing the top-level list of these per-test-case sublists, with elements comma-separated and enclosed in square brackets, for example\n$$\n[[H_{\\text{single}},H_{\\text{complete}},H_{\\text{average}},\\text{flag}],\\dots]\n$$\nAll numeric results must be real numbers with no unit, and the boolean must be either true or false. The exact printed representation should have no spaces.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of statistical learning, specifically cluster analysis. The definitions for hierarchical agglomerative clustering, Euclidean distance, and the linkage criteria (single, complete, average) are standard and mathematically precise. The objective—to compute the outlier isolation height for each linkage method and compare them—is well-posed, and all necessary data and conditions are provided.\n\nThe task is to implement hierarchical agglomerative clustering to analyze the sensitivity of different linkage criteria to outliers. For each provided dataset, consisting of a point cloud in $\\mathbb{R}^2$ and a designated set of outlier indices $O$, we must compute the outlier isolation height $H$ for single, complete, and average linkage. Finally, we determine a boolean flag indicating whether complete linkage isolates the outlier at a strictly greater merge height than the other two methods.\n\nThe core of the solution is a correct implementation of hierarchical agglomerative clustering and the subsequent analysis of the resulting dendrogram. The algorithm proceeds as follows for each test case:\n\n1.  **Data Representation**: The input points are represented as an $n \\times 2$ matrix, where $n$ is the number of points. The set of outlier indices is denoted by $O$, and the set of inlier indices is $I = \\{0, 1, \\dots, n-1\\} \\setminus O$.\n\n2.  **Distance Calculation**: The dissimilarity between any two points $x, y \\in \\mathbb{R}^2$ is their Euclidean ($L_2$) distance, $d(x,y)$. We pre-compute all pairwise distances between the $n$ points, yielding a condensed distance matrix. This is efficiently accomplished using `scipy.spatial.distance.pdist`.\n\n3.  **Hierarchical Clustering**: For each of the three linkage criteria—single, complete, and average—we perform hierarchical agglomerative clustering. The `scipy.cluster.hierarchy.linkage` function is ideal for this. It takes the condensed distance matrix and the linkage method as input and produces a linkage matrix $Z$. The matrix $Z$ is of size $(n-1) \\times 4$ and encodes the full hierarchy of merges. Each row $Z_k$ corresponds to the $k$-th merge (ordered by distance) and contains `[index_1, index_2, distance, new_cluster_size]`. The `distance` is the dissimilarity between the two merging clusters, which corresponds to the height of the merge in the dendrogram.\n\n4.  **Outlier Isolation Height ($H$) Determination**: The central task is to find the specific merge height $H$ defined as the first instance where a cluster containing at least one outlier point merges with a cluster containing only inlier points. To find $H$, we must inspect the sequence of merges provided by the linkage matrix $Z$.\n    - We initialize $n$ clusters, each containing a single point. The composition of each cluster (i.e., the set of original point indices it contains) must be tracked throughout the merging process. We can use a list of sets, say `cluster_contents`, for this purpose, where `cluster_contents[j]` stores the indices for cluster $j$. For $j < n$, `cluster_contents[j]` is simply $\\{j\\}$.\n    - We iterate through the rows of the linkage matrix $Z$, from $k=0$ to $n-2$. Each row $Z_k$ represents merging two clusters, identified by indices `idx1` and `idx2`, at a height $h_k$. The new cluster formed by this merge is assigned the index $n+k$.\n    - For each merge, we retrieve the contents of the merging clusters, $C_1 = \\text{cluster\\_contents}[\\text{idx1}]$ and $C_2 = \\text{cluster\\_contents}[\\text{idx2}]$.\n    - We then check their status relative to the outlier set $O$:\n        - A cluster $C$ is an \"outlier-containing cluster\" if $C \\cap O \\neq \\emptyset$.\n        - A cluster $C$ is an \"inlier-only cluster\" if $C \\cap O = \\emptyset$.\n    - The condition for an isolation merge is that one of the merging clusters is outlier-containing and the other is inlier-only. That is, $(C_1 \\cap O \\neq \\emptyset \\text{ and } C_2 \\cap O = \\emptyset)$ or $(C_1 \\cap O = \\emptyset \\text{ and } C_2 \\cap O \\neq \\emptyset)$.\n    - Since the rows of $Z$ are ordered by increasing merge height, the height $h_k$ of the first merge that satisfies this condition is the outlier isolation height $H$.\n    - After checking the condition, we update our tracking structure by creating the new merged cluster: $\\text{cluster\\_contents}[n+k] = C_1 \\cup C_2$.\n\n5.  **Comparative Analysis**: This process is repeated for each of the three linkage criteria, yielding $H_{\\text{single}}$, $H_{\\text{complete}}$, and $H_{\\text{average}}$. The problem requires producing a boolean flag that is true if and only if the isolation ratio $R = H/S$ for complete linkage is strictly greater than for the other two. As the inlier compactness scale $S$ is a positive constant for any given dataset, the comparison $R_{\\text{complete}} > R_{\\text{single}}$ is equivalent to $H_{\\text{complete}} > H_{\\text{single}}$. Therefore, the flag is computed as the result of the logical expression $(H_{\\text{complete}} > H_{\\text{single}}) \\land (H_{\\text{complete}} > H_{\\text{average}})$.\n\n6.  **Final Output**: The computed values $[H_{\\text{single}}, H_{\\text{complete}}, H_{\\text{average}}, \\text{flag}]$ for each test case are collected and formatted into a single string as specified by the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical clustering problem for a suite of test cases.\n    For each case, it computes outlier isolation heights for single, complete,\n    and average linkage, and determines a flag comparing them.\n    \"\"\"\n    test_cases = [\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, -0.1), (-0.1, 0.3), (0.3, 0.1), \n                (-0.2, -0.2), (0.1, -0.3), (20.0, 0.0)\n            ]),\n            \"outliers\": [6]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.1, 0.0), (0.0, 0.1), (-0.1, 0.0), \n                (1.0, 1.0), (1.1, 1.0), (1.0, 1.1), (0.9, 1.0), \n                (10.0, 10.0)\n            ]),\n            \"outliers\": [8]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, 0.0), (-0.1, 0.2), (0.3, -0.1), \n                (-0.2, -0.1), (0.1, -0.2), (3.0, 3.0)\n            ]),\n            \"outliers\": [6]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, -0.1), (-0.1, 0.3), (0.3, 0.1), \n                (-0.2, -0.2), (0.1, -0.3), (20.0, 0.0), (20.0, 0.5)\n            ]),\n            \"outliers\": [6, 7]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        points = case[\"points\"]\n        outlier_indices = set(case[\"outliers\"])\n        \n        case_results = []\n        for linkage_method in ['single', 'complete', 'average']:\n            n = points.shape[0]\n            \n            # Perform hierarchical clustering using the specified linkage method\n            # The metric 'euclidean' is implicitly used by pdist\n            dist_matrix = pdist(points, 'euclidean')\n            Z = linkage(dist_matrix, method=linkage_method)\n            \n            # Track the original point indices within each cluster\n            cluster_contents = [{i} for i in range(n)]\n            \n            isolation_height = -1.0\n            \n            # Iterate through the merges in the order they occur\n            for i in range(n - 1):\n                # Z[i, 0] and Z[i, 1] are the indices of the clusters being merged\n                # Z[i, 2] is the merge height (distance)\n                idx1, idx2, height, _ = Z[i]\n                idx1, idx2 = int(idx1), int(idx2)\n                \n                # Get the set of original points for each of the two merging clusters\n                clust1_content = cluster_contents[idx1]\n                clust2_content = cluster_contents[idx2]\n                \n                # Check if a cluster contains any outliers\n                is_outlier1 = bool(clust1_content.intersection(outlier_indices))\n                is_outlier2 = bool(clust2_content.intersection(outlier_indices))\n                \n                # The isolation merge is the first where one cluster has outliers\n                # and the other does not.\n                if is_outlier1 != is_outlier2:\n                    isolation_height = height\n                    break # Found the first such merge, so this is H\n                \n                # If not an isolation merge, create the new cluster for subsequent steps.\n                # The new cluster's index is n + i.\n                new_cluster_content = clust1_content.union(clust2_content)\n                cluster_contents.append(new_cluster_content)\n            \n            case_results.append(isolation_height)\n\n        H_single, H_complete, H_average = case_results\n        \n        # The flag is true iff H_complete is strictly greater than the other two.\n        flag = (H_complete > H_single) and (H_complete > H_average)\n        \n        all_results.append([H_single, H_complete, H_average, flag])\n\n    # Format the final output string as per requirements:\n    # no spaces, lowercase booleans, nested lists.\n    result_str_parts = []\n    for res in all_results:\n        flag_str = 'true' if res[3] else 'false'\n        part = f\"[{res[0]},{res[1]},{res[2]},{flag_str}]\"\n        result_str_parts.append(part)\n    \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3109639"}]}