{"hands_on_practices": [{"introduction": "To truly understand Principal Component Analysis, we must first dissect its fundamental components. This exercise strips away the complexity of real-world data to focus on the core mechanics of PCA through an analytical calculation. By working with a clean, hypothetical covariance matrix, you will derive the principal components (eigenvectors) and their associated variances (eigenvalues) from first principles, building a solid intuition for how loadings represent underlying patterns like 'averages' and 'contrasts' among variables. [@problem_id:3161250]", "problem": "You are given a centered three-variable dataset modeled as a zero-mean random vector $X = (X_{1}, X_{2}, X_{3})^{\\top}$ with a known population covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n1  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\\n\\tfrac{1}{2}  1  \\tfrac{1}{2} \\\\\n\\tfrac{1}{2}  \\tfrac{1}{2}  1\n\\end{pmatrix}.\n$$\nThis describes three standardized measurements that share a common positive association. Using only first principles from the definition of covariance, symmetric matrices, and the characterization of Principal Component Analysis (PCA) as the eigendecomposition of the covariance matrix, carry out the following:\n\n1. Compute the eigenvalues and a corresponding set of orthonormal eigenvectors of $\\Sigma$ analytically.\n2. Define the component loadings as the entries of the unit-norm eigenvectors of $\\Sigma$, and interpret each eigenvector in terms of “average” versus “contrast” across variables, explaining how the signs and relative magnitudes of the loadings encode these notions.\n3. Finally, compute the exact fraction of the total variance explained by the “average” component (the one that puts equal positive weight on all three variables). Express this final fraction in simplest exact form as a rational number. No rounding is required.\n\nState only this final fraction as your final answer. Do not include any units or additional text in your final answer.", "solution": "The problem asks for the fraction of total variance explained by the \"average\" component in a Principal Component Analysis (PCA) of a three-variable system. The system is described by a zero-mean random vector $X = (X_{1}, X_{2}, X_{3})^{\\top}$ with the given population covariance matrix $\\Sigma$.\n\nFirst, we validate the problem statement. The givens are the random vector $X$ and its covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n1  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\\n\\tfrac{1}{2}  1  \\tfrac{1}{2} \\\\\n\\tfrac{1}{2}  \\tfrac{1}{2}  1\n\\end{pmatrix}.\n$$\nThe problem specifies that PCA is to be understood as the eigendecomposition of this covariance matrix. The tasks are to find the eigenvalues and eigenvectors, interpret them, and compute a specific fraction of variance. The problem is scientifically grounded in standard multivariate statistics, well-posed with all necessary information provided, and stated objectively. The matrix $\\Sigma$ is symmetric and, as we will show, positive definite, making it a valid covariance matrix. The problem is therefore valid.\n\nWe proceed with the solution in three parts as requested.\n\nPart 1: Compute eigenvalues and orthonormal eigenvectors of $\\Sigma$.\nThe principal components are the eigenvectors of the covariance matrix $\\Sigma$, and their variances are the corresponding eigenvalues. We find the eigenvalues $\\lambda$ by solving the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$\n\\det \\begin{pmatrix}\n1-\\lambda  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\\n\\tfrac{1}{2}  1-\\lambda  \\tfrac{1}{2} \\\\\n\\tfrac{1}{2}  \\tfrac{1}{2}  1-\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant, we get:\n$$\n(1-\\lambda)\\left( (1-\\lambda)^2 - \\left(\\frac{1}{2}\\right)^2 \\right) - \\frac{1}{2}\\left( \\frac{1}{2}(1-\\lambda) - \\frac{1}{4} \\right) + \\frac{1}{2}\\left( \\frac{1}{4} - \\frac{1}{2}(1-\\lambda) \\right) = 0\n$$\n$$\n(1-\\lambda)\\left(\\lambda^2 - 2\\lambda + 1 - \\frac{1}{4}\\right) - \\frac{1}{2}\\left(\\frac{1}{4} - \\frac{\\lambda}{2}\\right) + \\frac{1}{2}\\left(-\\frac{1}{4} + \\frac{\\lambda}{2}\\right) = 0\n$$\n$$\n(1-\\lambda)(\\lambda^2 - 2\\lambda + \\frac{3}{4}) - \\frac{1}{8} + \\frac{\\lambda}{4} - \\frac{1}{8} + \\frac{\\lambda}{4} = 0\n$$\n$$\n\\lambda^2 - 2\\lambda + \\frac{3}{4} - \\lambda^3 + 2\\lambda^2 - \\frac{3}{4}\\lambda - \\frac{1}{4} + \\frac{\\lambda}{2} = 0\n$$\n$$\n-\\lambda^3 + 3\\lambda^2 - \\left(2 + \\frac{3}{4} - \\frac{1}{2}\\right)\\lambda + \\left(\\frac{3}{4} - \\frac{1}{4}\\right) = 0\n$$\n$$\n-\\lambda^3 + 3\\lambda^2 - \\frac{9}{4}\\lambda + \\frac{1}{2} = 0\n$$\nMultiplying by $-4$ gives a cleaner polynomial:\n$$\n4\\lambda^3 - 12\\lambda^2 + 9\\lambda - 2 = 0\n$$\nBy inspection or the rational root theorem, we can test potential roots. Let's test $\\lambda=2$: $4(2)^3 - 12(2)^2 + 9(2) - 2 = 32 - 48 + 18 - 2 = 0$. So, $\\lambda=2$ is a root.\nLet's test $\\lambda=1/2$: $4(\\frac{1}{2})^3 - 12(\\frac{1}{2})^2 + 9(\\frac{1}{2}) - 2 = 4(\\frac{1}{8}) - 12(\\frac{1}{4}) + \\frac{9}{2} - 2 = \\frac{1}{2} - 3 + \\frac{9}{2} - 2 = \\frac{10}{2} - 5 = 0$. So, $\\lambda=1/2$ is also a root.\nSince $(\\lambda-2)$ is a factor of the polynomial, we can perform polynomial division, which yields:\n$$\n(\\lambda-2)(4\\lambda^2 - 4\\lambda + 1) = 0\n$$\nThe quadratic factor is a perfect square: $4\\lambda^2 - 4\\lambda + 1 = (2\\lambda - 1)^2$.\nThus, the characteristic equation is $(\\lambda-2)(2\\lambda-1)^2 = 0$.\nThe eigenvalues are $\\lambda_1 = 2$ (with multiplicity $1$) and $\\lambda_2 = \\lambda_3 = \\frac{1}{2}$ (with multiplicity $2$).\n\nNow, we find a set of orthonormal eigenvectors.\nFor $\\lambda_1 = 2$: We solve $(\\Sigma - 2I)v = 0$.\n$$\n\\begin{pmatrix} -1  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  -1  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  \\tfrac{1}{2}  -1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis system of linear equations simplifies to $v_1 = v_2 = v_3$. An unnormalized eigenvector is $(1, 1, 1)^{\\top}$. To obtain a unit-norm eigenvector $u_1$, we divide by its norm $\\sqrt{1^2+1^2+1^2} = \\sqrt{3}$.\n$$ u_1 = \\begin{pmatrix} 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\end{pmatrix} $$\n\nFor $\\lambda_2 = \\lambda_3 = \\frac{1}{2}$: We solve $(\\Sigma - \\frac{1}{2}I)v = 0$.\n$$\n\\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  \\tfrac{1}{2}  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis reduces to a single equation $v_1 + v_2 + v_3 = 0$. This describes a two-dimensional eigenspace (a plane) orthogonal to the eigenvector $u_1$, as expected for a symmetric matrix. We need to find an orthonormal basis for this plane.\nLet's choose a simple vector satisfying the condition, for instance, $v' = (1, -1, 0)^{\\top}$. Normalizing it gives our second eigenvector $u_2$:\n$$ u_2 = \\frac{1}{\\sqrt{1^2+(-1)^2+0^2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} $$\nFor the third eigenvector $u_3$, it must be in the same plane ($u_{3,1}+u_{3,2}+u_{3,3}=0$) and also orthogonal to $u_2$ ($u_2 \\cdot u_3 = 0$). The orthogonality condition gives $\\frac{1}{\\sqrt{2}}u_{3,1} - \\frac{1}{\\sqrt{2}}u_{3,2} = 0$, which implies $u_{3,1} = u_{3,2}$. Substituting this into the plane equation gives $2u_{3,1} + u_{3,3} = 0$, or $u_{3,3} = -2u_{3,1}$. Thus, any such vector has the form $(c, c, -2c)^{\\top}$. We normalize it to find $u_3$:\n$$ u_3 = \\frac{1}{\\sqrt{c^2+c^2+(-2c)^2}} \\begin{pmatrix} c \\\\ c \\\\ -2c \\end{pmatrix} = \\frac{1}{\\sqrt{6c^2}} \\begin{pmatrix} c \\\\ c \\\\ -2c \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{6} \\\\ 1/\\sqrt{6} \\\\ -2/\\sqrt{6} \\end{pmatrix} $$\n\nPart 2: Interpret the eigenvectors.\nThe eigenvectors are the principal components, and their elements are the \"loadings\".\n- The first principal component, corresponding to $u_1=(1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})^{\\top}$, has equal positive loadings on all three variables. This component represents the weighted average of the variables, $Z_1 = \\frac{1}{\\sqrt{3}}(X_1+X_2+X_3)$, capturing their tendency to vary together. This is the \"average\" component referred to in the problem.\n- The second and third principal components, $u_2$ and $u_3$, have both positive and negative loadings. They represent \"contrasts\". For example, $u_2=(1/\\sqrt{2}, -1/\\sqrt{2}, 0)^{\\top}$ captures the difference between $X_1$ and $X_2$. $u_3=(1/\\sqrt{6}, 1/\\sqrt{6}, -2/\\sqrt{6})^{\\top}$ captures a contrast between the average of $X_1$ and $X_2$ versus $X_3$.\n\nPart 3: Compute the fraction of total variance explained.\nThe total variance in the dataset is the sum of the variances of the individual variables, which is the trace of the covariance matrix $\\Sigma$.\n$$ \\text{Total Variance} = \\text{tr}(\\Sigma) = 1 + 1 + 1 = 3 $$\nThis sum must also equal the sum of the eigenvalues: $\\sum_{i=1}^3 \\lambda_i = 2 + \\frac{1}{2} + \\frac{1}{2} = 3$, which is consistent.\nThe variance explained by each principal component is its corresponding eigenvalue. The \"average\" component is the first principal component, with variance $\\lambda_1 = 2$.\nThe fraction of the total variance explained by the \"average\" component is the ratio of its variance to the total variance:\n$$ \\text{Fraction of Variance} = \\frac{\\lambda_1}{\\text{tr}(\\Sigma)} = \\frac{2}{3} $$\nThis is an exact rational number in its simplest form.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3161250"}, {"introduction": "Principal Component Analysis is designed to capture the directions of maximum *variance* in a dataset, and variance is inherently a measure of spread around a central point. This makes data centering—the act of subtracting the mean from each variable—a crucial prerequisite for meaningful results. This practice explores the consequences of omitting this step, demonstrating how an uncentered analysis can mistakenly focus on the data's average location in space rather than the interesting patterns of variation around it. [@problem_id:3161268]", "problem": "Consider a data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose rows index $n$ observations and columns index $p$ variables. You augment $X$ with a constant column to obtain $X' \\in \\mathbb{R}^{n \\times (p+1)}$ defined by $X' = [X \\ \\ \\alpha \\mathbf{1}_n]$, where $\\alpha \\in \\mathbb{R}$ is a fixed constant and $\\mathbf{1}_n \\in \\mathbb{R}^n$ is the vector of ones. Define the column-centering operation as left-multiplication by the centering matrix $C \\in \\mathbb{R}^{n \\times n}$, where $C = I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top$, so that the centered matrix is $X_c' = C X'$. Principal Components Analysis (PCA) is performed by computing the eigenvectors (loadings) of the sample covariance matrix of the centered data.\n\nStarting from first principles and core definitions:\n- Column centering subtracts, from each column, its sample mean across rows.\n- The sample covariance matrix of the centered data is the symmetric matrix that captures pairwise centered second moments of the variables.\n- PCA loadings are the eigenvectors of the sample covariance matrix, and principal component scores are the projections of the centered data onto these loadings.\n\nUsing these bases, analyze the effect of the added constant column on $X'$ under column centering and discuss the consequences for PCA loadings when centering is omitted. Then select all statements that are true.\n\nA. After column centering via $C = I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top$, the added constant column becomes the zero vector and does not affect the sample covariance matrix or the principal component loadings.\n\nB. Even without centering, the constant column has zero variance and therefore cannot affect principal components computed from the second moment matrix $X'^\\top X'$.\n\nC. Centering is critical because principal component loadings are defined as eigenvectors of the sample covariance matrix of the centered data; without centering, loadings can be dominated by the mean offset, yielding components that explain location rather than variation.\n\nD. Scaling each column to unit variance (standardization) is sufficient to remove the influence of a constant column even if the data are not centered.\n\nE. Column centering is implemented by right-multiplying $X'$ by $I_{p+1} - \\frac{1}{p+1}\\mathbf{1}_{p+1}\\mathbf{1}_{p+1}^\\top$, so the constant column is preserved after centering because this operation acts on rows, not columns.\n\nSelect all correct options.", "solution": "The problem asks us to analyze the effect of adding a constant column to a data matrix $X$ when performing Principal Component Analysis (PCA), contrasting the outcomes when data is properly centered versus when it is not.\n\n**Case 1: With Column Centering (Standard PCA)**\n\nThe standard procedure for PCA begins with column centering. The augmented matrix is $X' = [X \\ \\ \\alpha \\mathbf{1}_n]$. The centering operation is applied by left-multiplying with the centering matrix $C = I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top$. Let's analyze the effect of $C$ on the added constant column, $\\mathbf{x}_{\\text{const}} = \\alpha \\mathbf{1}_n$.\n\n$$\nC \\mathbf{x}_{\\text{const}} = \\left(I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top\\right) (\\alpha \\mathbf{1}_n) = \\alpha \\mathbf{1}_n - \\frac{\\alpha}{n}\\mathbf{1}_n (\\mathbf{1}_n^\\top \\mathbf{1}_n)\n$$\n\nSince $\\mathbf{1}_n^\\top \\mathbf{1}_n = n$, the expression simplifies to:\n$$\n\\alpha \\mathbf{1}_n - \\frac{\\alpha}{n}\\mathbf{1}_n (n) = \\alpha \\mathbf{1}_n - \\alpha \\mathbf{1}_n = \\mathbf{0}_n\n$$\n\nThis shows that after centering, the added constant column becomes a column of zeros. The centered matrix is $X_c' = [CX \\ \\ \\mathbf{0}_n]$. The sample covariance matrix is proportional to $(X_c')^\\top X_c'$.\n$$\n(X_c')^\\top X_c' = \\begin{bmatrix} (CX)^\\top \\\\ \\mathbf{0}_n^\\top \\end{bmatrix} [CX \\ \\ \\mathbf{0}_n] = \\begin{bmatrix} (CX)^\\top CX  \\mathbf{0} \\\\ \\mathbf{0}^\\top  0 \\end{bmatrix}\n$$\n\nThe covariance matrix for the augmented data is block-diagonal. The principal components (eigenvectors) related to the original data $X$ are derived from the $(CX)^\\top CX$ block and are unaffected, simply being padded with a zero in the last position. The constant column introduces a new, trivial principal component with zero variance. This directly validates statement **A**.\n\n**Case 2: Without Column Centering (Uncentered PCA)**\n\nIf centering is omitted, the analysis is performed on the second moment matrix, $(X')^\\top X'$, not the covariance matrix.\n$$\n(X')^\\top X' = \\begin{bmatrix} X^\\top \\\\ \\alpha \\mathbf{1}_n^\\top \\end{bmatrix} [X \\ \\ \\alpha \\mathbf{1}_n] = \\begin{bmatrix} X^\\top X  \\alpha X^\\top \\mathbf{1}_n \\\\ \\alpha \\mathbf{1}_n^\\top X  \\alpha^2 \\mathbf{1}_n^\\top \\mathbf{1}_n \\end{bmatrix} = \\begin{bmatrix} X^\\top X  n\\alpha \\bar{\\mathbf{x}} \\\\ n\\alpha \\bar{\\mathbf{x}}^\\top  n\\alpha^2 \\end{bmatrix}\n$$\nwhere $\\bar{\\mathbf{x}} = \\frac{1}{n}X^\\top\\mathbf{1}_n$ is the vector of column means of the original matrix $X$.\n\nUnlike the centered case, this matrix is generally not block-diagonal. The off-diagonal block $n\\alpha \\bar{\\mathbf{x}}$ couples the original variables with the constant column. Although the constant column itself has zero *variance*, its non-zero *mean* ($\\alpha$) interacts with the means of the other columns. This fundamentally changes the eigenvectors. This invalidates statement **B**.\n\n**Evaluating the Options**\n\n*   **A. Correct.** As shown above, centering annihilates the constant column, preventing it from affecting the covariance structure and loadings of the original variables.\n*   **B. Incorrect.** Without centering, the analysis is based on the second moment matrix, where the constant column's mean value interacts with the means of other columns, thus affecting the principal components.\n*   **C. Correct.** This statement provides the fundamental rationale for centering. PCA's goal is to explain variance (spread around a mean). Without centering, the analysis is performed on raw second moments, which conflate variance with the mean (location). The resulting \"principal components\" may simply describe the data cloud's offset from the origin rather than its intrinsic structure of variation.\n*   **D. Incorrect.** Scaling a column to unit variance requires dividing by its standard deviation. A constant column has a standard deviation of zero, making this operation undefined (division by zero).\n*   **E. Incorrect.** The operation described, right-multiplication by $I_{p+1} - \\frac{1}{p+1}\\mathbf{1}_{p+1}\\mathbf{1}_{p+1}^\\top$, performs *row* centering, not the column centering required for PCA.\n\nTherefore, the only true statements are A and C.", "answer": "$$\\boxed{AC}$$", "id": "3161268"}, {"introduction": "When your variables are measured in different units or have wildly different scales, should the variable with the largest variance automatically dominate the analysis? This practical coding exercise delves into one of the most important decisions when applying PCA: whether to use the covariance matrix or the correlation matrix. By simulating data with varying levels of measurement error, you will see firsthand how covariance-based PCA can be sensitive to variable scaling and why correlation-based PCA (which implicitly standardizes the data) often yields more stable and interpretable components. [@problem_id:3161286]", "problem": "Consider two observed variables $y_1$ and $y_2$ generated from a common latent signal $z$ and independent measurement errors. Let $z$ be a zero-mean random variable with variance $\\operatorname{Var}(z)=1$, and define $y_1=z+\\epsilon_1$ and $y_2=z+\\epsilon_2$, where $\\epsilon_1$ and $\\epsilon_2$ are zero-mean, mutually independent, and independent of $z$, with variances $\\operatorname{Var}(\\epsilon_1)=\\tau_1^2$ and $\\operatorname{Var}(\\epsilon_2)=\\tau_2^2$. The goal is to analyze how increasing the measurement error variance $\\tau_1^2$ inflates the weight of variable $y_1$ in the first principal component loading under Principal Components Analysis (PCA) computed from the covariance matrix, and why PCA computed from the correlation matrix mitigates this inflation.\n\nUse the following foundational definitions:\n- The covariance matrix $S$ of $(y_1,y_2)$ is given by $S=\\begin{bmatrix}\\operatorname{Var}(y_1)\\operatorname{Cov}(y_1,y_2)\\\\ \\operatorname{Cov}(y_1,y_2)\\operatorname{Var}(y_2)\\end{bmatrix}$.\n- The correlation matrix $R$ of $(y_1,y_2)$ is given by $R=D^{-1/2}SD^{-1/2}$, where $D=\\operatorname{diag}(\\operatorname{Var}(y_1),\\operatorname{Var}(y_2))$.\n- Principal Components Analysis (PCA) computes principal directions as the eigenvectors of the chosen scatter matrix (either $S$ for covariance-PCA or $R$ for correlation-PCA). The first principal component loading vector is the unit-norm eigenvector associated with the largest eigenvalue.\n- For a loading vector $v$ corresponding to the first principal component, the loading of variable $y_j$ is the $j$-th entry of $v$. Because eigenvectors are defined up to sign, use the magnitude of the loading, i.e., $\\lvert v_j \\rvert$.\n\nLet $\\tau_2^2=0.5$ be fixed. For each test case, construct $S$ and $R$ using the model above and compute:\n1. The magnitude of the loading for variable $y_1$ in the first principal component under covariance-PCA, denoted $l_1^{\\text{cov}}$.\n2. The magnitude of the loading for variable $y_1$ in the first principal component under correlation-PCA, denoted $l_1^{\\text{cor}}$.\n3. The ratio $r=\\dfrac{l_1^{\\text{cov}}}{l_1^{\\text{cor}}}$.\n\nYour program must implement the calculations exactly in the infinite-sample limit implied by the model, i.e., compute $S$ and $R$ analytically from $(\\tau_1^2,\\tau_2^2)$ without any Monte Carlo sampling.\n\nTest suite:\n- Case A (baseline): $\\tau_1^2=0.0$.\n- Case B (moderate error): $\\tau_1^2=0.5$.\n- Case C (large error): $\\tau_1^2=2.0$.\n- Case D (very large error): $\\tau_1^2=20.0$.\n\nFor each case, output the ratio $r$ as a float. Your program should produce a single line of output containing the four ratios in order as a comma-separated list enclosed in square brackets (e.g., $[r_A,r_B,r_C,r_D]$). There are no physical units or angle units in this problem. The answers are floats and must be computed deterministically from the definitions above.", "solution": "We begin from the generative model definitions and the basic properties of covariance and correlation. The latent signal $z$ has $\\operatorname{Var}(z)=1$, the measurement errors are independent with $\\operatorname{Var}(\\epsilon_1)=\\tau_1^2$ and $\\operatorname{Var}(\\epsilon_2)=\\tau_2^2$, and $z,\\epsilon_1,\\epsilon_2$ are mutually independent. Therefore, by the bilinearity of covariance and independence, we have\n$$\n\\operatorname{Var}(y_1)=\\operatorname{Var}(z)+\\operatorname{Var}(\\epsilon_1)=1+\\tau_1^2,\n\\quad\n\\operatorname{Var}(y_2)=\\operatorname{Var}(z)+\\operatorname{Var}(\\epsilon_2)=1+\\tau_2^2,\n$$\nand\n$$\n\\operatorname{Cov}(y_1,y_2)=\\operatorname{Cov}(z+\\epsilon_1,z+\\epsilon_2)=\\operatorname{Cov}(z,z)+\\operatorname{Cov}(z,\\epsilon_2)+\\operatorname{Cov}(\\epsilon_1,z)+\\operatorname{Cov}(\\epsilon_1,\\epsilon_2)=1+0+0+0=1.\n$$\nThus the covariance matrix $S$ is\n$$\nS=\\begin{bmatrix}1+\\tau_1^2  1 \\\\ 1  1+\\tau_2^2\\end{bmatrix}.\n$$\nThe correlation matrix $R$ of $(y_1,y_2)$ rescales by the inverse standard deviations of $y_1$ and $y_2$. Let $D=\\operatorname{diag}(1+\\tau_1^2,\\,1+\\tau_2^2)$. Then\n$$\nR=D^{-1/2}SD^{-1/2}\n=\\begin{bmatrix}\n\\frac{1}{\\sqrt{1+\\tau_1^2}}  0 \\\\\n0  \\frac{1}{\\sqrt{1+\\tau_2^2}}\n\\end{bmatrix}\n\\begin{bmatrix}\n1+\\tau_1^2  1 \\\\\n1  1+\\tau_2^2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{1+\\tau_1^2}}  0 \\\\\n0  \\frac{1}{\\sqrt{1+\\tau_2^2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  \\rho \\\\\n\\rho  1\n\\end{bmatrix},\n$$\nwhere\n$$\n\\rho=\\frac{\\operatorname{Cov}(y_1,y_2)}{\\sqrt{\\operatorname{Var}(y_1)\\operatorname{Var}(y_2)}}\n=\\frac{1}{\\sqrt{(1+\\tau_1^2)(1+\\tau_2^2)}}.\n$$\nPrincipal Components Analysis (PCA) computes orthonormal eigenvectors of the scatter matrix. For covariance-PCA, we use $S$, and for correlation-PCA, we use $R$. The first principal component loading vector is the unit-norm eigenvector associated with the largest eigenvalue. For a loading vector $v$ corresponding to the largest eigenvalue of the chosen matrix, the loading magnitude for variable $y_1$ is $\\lvert v_1\\rvert$.\n\nWe now analyze how increasing $\\tau_1^2$ affects the first principal component under the two choices of scatter matrix.\n\nUnder covariance-PCA, the matrix $S$ has entries $S_{11}=1+\\tau_1^2$, $S_{22}=1+\\tau_2^2$, and $S_{12}=S_{21}=1$. As $\\tau_1^2$ increases while $\\tau_2^2$ remains fixed, $S_{11}$ grows, and $S$ becomes increasingly diagonally dominant in the $(1,1)$ entry relative to $(2,2)$, while the off-diagonal entry $S_{12}$ stays at $1$. Intuitively, the first principal component maximizes the Rayleigh quotient $v^\\top S v$ subject to $\\lVert v\\rVert_2=1$, and as $S_{11}$ grows, directions with larger weight on the first coordinate achieve larger variance. This tilts the first principal eigenvector toward the $y_1$ axis, increasing $\\lvert v_1\\rvert$.\n\nUnder correlation-PCA, the matrix $R$ has unit diagonal entries and off-diagonal entry $\\rho=\\frac{1}{\\sqrt{(1+\\tau_1^2)(1+\\tau_2^2)}}$. Increasing $\\tau_1^2$ does not change the diagonal entries of $R$ (they remain $1$), but decreases $\\rho$ because the numerator stays fixed at $1$ while the denominator increases. The eigenstructure of the symmetric matrix\n$$\nR=\\begin{bmatrix}1  \\rho \\\\ \\rho  1\\end{bmatrix}\n$$\nis simple: the eigenvectors are $u_+=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$ with eigenvalue $1+\\rho$, and $u_-=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$ with eigenvalue $1-\\rho$. For $\\rho\\ge 0$, the largest eigenvalue is $1+\\rho$, and its eigenvector $u_+$ has equal weights on $y_1$ and $y_2$, with loading magnitude $\\lvert (u_+)_1\\rvert=\\frac{1}{\\sqrt{2}}$. Critically, this eigenvector is independent of $\\rho$, so increasing $\\tau_1^2$ does not inflate the loading magnitude for $y_1$ under correlation-PCA; it stays at $\\frac{1}{\\sqrt{2}}$.\n\nAlgorithmically, for each test case:\n1. Set $\\tau_2^2=0.5$ and read the given $\\tau_1^2$.\n2. Construct $S=\\begin{bmatrix}1+\\tau_1^2  1 \\\\ 1  1+\\tau_2^2\\end{bmatrix}$.\n3. Construct $R=D^{-1/2}SD^{-1/2}$ with $D=\\operatorname{diag}(1+\\tau_1^2,\\,1+\\tau_2^2)$.\n4. Compute the eigenvalues and eigenvectors of $S$, select the eigenvector corresponding to the largest eigenvalue, normalize it to unit norm, and set $l_1^{\\text{cov}}$ to the absolute value of its first entry.\n5. Repeat step 4 for $R$ to obtain $l_1^{\\text{cor}}$.\n6. Compute the ratio $r=\\dfrac{l_1^{\\text{cov}}}{l_1^{\\text{cor}}}$.\n7. Aggregate the four ratios $[r_A,r_B,r_C,r_D]$ into a single printed line.\n\nThis procedure deterministically quantifies the inflation of the weight of variable $y_1$ in the first principal component under covariance-PCA caused by increased measurement error variance $\\tau_1^2$, and demonstrates that correlation-PCA mitigates this effect by standardizing variables, keeping the principal direction symmetric when only two variables with identical diagonal entries are considered. The test suite includes a baseline with no added error, moderate error, large error, and very large error, thereby covering typical behavior and asymptotic tilting under covariance-PCA while showing robustness of correlation-PCA loadings.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef first_pc_loading_var1(matrix: np.ndarray) - float:\n    \"\"\"\n    Compute the magnitude of the loading for variable 1 (index 0)\n    in the first principal component (largest eigenvalue) for a\n    symmetric 2x2 matrix.\n    \"\"\"\n    # Use eigh for symmetric matrices; returns eigenvalues in ascending order.\n    evals, evecs = np.linalg.eigh(matrix)\n    idx = int(np.argmax(evals))\n    v = evecs[:, idx]\n    # Ensure unit-norm eigenvector (eigh returns normalized eigenvectors).\n    # Take absolute value to remove sign ambiguity.\n    return float(abs(v[0]))\n\ndef build_covariance_matrix(tau1_sq: float, tau2_sq: float) - np.ndarray:\n    \"\"\"\n    Build the 2x2 covariance matrix S for y1 = z + e1, y2 = z + e2,\n    with Var(z)=1, Var(e1)=tau1_sq, Var(e2)=tau2_sq, and all independent.\n    \"\"\"\n    var1 = 1.0 + tau1_sq\n    var2 = 1.0 + tau2_sq\n    cov12 = 1.0\n    return np.array([[var1, cov12],\n                     [cov12, var2]], dtype=float)\n\ndef build_correlation_matrix_from_cov(S: np.ndarray) - np.ndarray:\n    \"\"\"\n    Build the correlation matrix R = D^{-1/2} S D^{-1/2},\n    where D = diag(S[0,0], S[1,1]).\n    \"\"\"\n    var1 = S[0, 0]\n    var2 = S[1, 1]\n    inv_sqrt_d = np.array([[1.0 / np.sqrt(var1), 0.0],\n                           [0.0, 1.0 / np.sqrt(var2)]], dtype=float)\n    R = inv_sqrt_d @ S @ inv_sqrt_d\n    return R\n\ndef solve():\n    # Fixed tau2^2 as specified in the problem.\n    tau2_sq = 0.5\n    # Define the test cases for tau1^2.\n    test_cases = [0.0, 0.5, 2.0, 20.0]\n\n    results = []\n    for tau1_sq in test_cases:\n        # Construct covariance matrix S.\n        S = build_covariance_matrix(tau1_sq, tau2_sq)\n        # Construct correlation matrix R.\n        R = build_correlation_matrix_from_cov(S)\n        # Compute loadings for variable 1 under covariance-PCA and correlation-PCA.\n        l1_cov = first_pc_loading_var1(S)\n        l1_cor = first_pc_loading_var1(R)\n        # Compute ratio r = l1_cov / l1_cor.\n        r = l1_cov / l1_cor\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161286"}]}