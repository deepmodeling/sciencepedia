{"hands_on_practices": [{"introduction": "The Gauss-Markov theorem identifies the \"Best Linear Unbiased Estimator\" (BLUE). Before we can appreciate what makes an estimator \"best,\" we must first be able to identify which estimators are \"linear\" and \"unbiased.\" This exercise will sharpen your understanding of these two foundational properties by asking you to sort through a variety of proposed estimators. By applying the definitions of linearity and unbiasedness, you will discover that multiple estimators can satisfy these criteria, which naturally leads to the question of how to choose among them. [@problem_id:1919590]", "problem": "In a scientific experiment, a fundamental constant $\\mu$ is measured three times, yielding observations $y_1, y_2,$ and $y_3$. The measurements are modeled by the simple linear regression equation $y_i = \\mu + \\epsilon_i$ for $i=1, 2, 3$. The terms $\\epsilon_i$ represent uncorrelated random measurement errors, each with an expected value of zero ($E[\\epsilon_i] = 0$) and a constant, unknown variance $\\sigma^2 > 0$.\n\nAn estimator $\\hat{\\mu}$ for the parameter $\\mu$ is a function of the observations.\n- An estimator is defined as **linear** if it can be expressed in the form $\\hat{\\mu} = c_1 y_1 + c_2 y_2 + c_3 y_3$ for some constants $c_1, c_2, c_3$.\n- An estimator is defined as **unbiased** if its expected value equals the true parameter value, i.e., $E[\\hat{\\mu}] = \\mu$.\n\nConsider the following six different estimators proposed for $\\mu$:\n\nA. $\\hat{\\mu}_A = \\frac{1}{6}(y_1 + 2y_2 + 3y_3)$\n\nB. $\\hat{\\mu}_B = \\frac{1}{4}y_1 + \\frac{1}{2}y_2$\n\nC. $\\hat{\\mu}_C = \\frac{1}{3}(y_1 + y_2 + y_3)$\n\nD. $\\hat{\\mu}_D = y_1 - y_2 + y_3$\n\nE. $\\hat{\\mu}_E = \\frac{1}{2}(y_1 + y_3)$\n\nF. $\\hat{\\mu}_F = \\frac{y_1 y_2}{y_3}$\n\nWhich of the following proposed estimators for $\\mu$ are both linear and unbiased? Select all that apply.", "solution": "We are given the model $y_{i}=\\mu+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$ and $\\operatorname{Var}(\\epsilon_{i})=\\sigma^{2}>0$, and the $\\epsilon_{i}$ are uncorrelated. Thus $E[y_{i}]=\\mu$ for each $i$.\n\nA linear estimator has the form $\\hat{\\mu}=c_{1}y_{1}+c_{2}y_{2}+c_{3}y_{3}$. Its expectation is\n$$\nE[\\hat{\\mu}]=c_{1}E[y_{1}]+c_{2}E[y_{2}]+c_{3}E[y_{3}]=(c_{1}+c_{2}+c_{3})\\mu.\n$$\nFor unbiasedness, we require $E[\\hat{\\mu}]=\\mu$ for all $\\mu$, which holds if and only if\n$$\nc_{1}+c_{2}+c_{3}=1.\n$$\n\nNow check each proposed estimator:\n- A: $\\hat{\\mu}_{A}=\\frac{1}{6}y_{1}+\\frac{2}{6}y_{2}+\\frac{3}{6}y_{3}$ is linear, and $\\frac{1}{6}+\\frac{2}{6}+\\frac{3}{6}=1$, so unbiased.\n- B: $\\hat{\\mu}_{B}=\\frac{1}{4}y_{1}+\\frac{1}{2}y_{2}+0\\cdot y_{3}$ is linear, but $\\frac{1}{4}+\\frac{1}{2}+0=\\frac{3}{4}\\neq 1$, so biased.\n- C: $\\hat{\\mu}_{C}=\\frac{1}{3}y_{1}+\\frac{1}{3}y_{2}+\\frac{1}{3}y_{3}$ is linear, and sums to $1$, so unbiased.\n- D: $\\hat{\\mu}_{D}=1\\cdot y_{1}+(-1)\\cdot y_{2}+1\\cdot y_{3}$ is linear, and $1-1+1=1$, so unbiased.\n- E: $\\hat{\\mu}_{E}=\\frac{1}{2}y_{1}+0\\cdot y_{2}+\\frac{1}{2}y_{3}$ is linear, and $\\frac{1}{2}+0+\\frac{1}{2}=1$, so unbiased.\n- F: $\\hat{\\mu}_{F}=\\frac{y_{1}y_{2}}{y_{3}}$ is not linear in $(y_{1},y_{2},y_{3})$, so it fails the linearity requirement (unbiasedness is not relevant once linearity fails).\n\nTherefore, the estimators that are both linear and unbiased are A, C, D, and E.", "answer": "$$\\boxed{ACDE}$$", "id": "1919590"}, {"introduction": "Now that we know multiple linear unbiased estimators can exist for the same parameter, we need a criterion to decide which one is \"best.\" In statistics, efficiency is key, and the most efficient estimator is the one with the minimum variance. A smaller variance implies that the estimates are more tightly clustered around the true parameter value, making the estimator more reliable. This practice guides you to compare the efficiency of two valid unbiased estimators by calculating their respective variances. This direct comparison will make the concept of \"best\" tangible and demonstrate why minimizing variance is a critical goal in estimation. [@problem_id:1919577]", "problem": "In an experiment to determine a physical constant $\\mu$, three independent measurements $y_1, y_2,$ and $y_3$ are taken. The measurements are modeled by the equation $y_i = \\mu + \\epsilon_i$ for $i \\in \\{1, 2, 3\\}$. The random errors $\\epsilon_i$ are uncorrelated, each with an expected value of zero and a constant, finite variance denoted by $\\sigma^2$.\n\nTwo different estimators for $\\mu$ are proposed:\n1.  The standard sample mean: $\\hat{\\mu}_1 = \\frac{y_1 + y_2 + y_3}{3}$\n2.  An alternative estimator that discards the second measurement: $\\hat{\\mu}_2 = \\frac{y_1 + y_3}{2}$\n\nBoth $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$ can be shown to be unbiased estimators of $\\mu$. Your task is to compare their efficiency. Specifically, calculate the ratio of the variance of the alternative estimator to the variance of the sample mean, $\\frac{\\text{Var}(\\hat{\\mu}_2)}{\\text{Var}(\\hat{\\mu}_1)}$.", "solution": "We model each measurement as $y_{i}=\\mu+\\epsilon_{i}$ with $\\mathbb{E}[\\epsilon_{i}]=0$, $\\operatorname{Var}(\\epsilon_{i})=\\sigma^{2}$, and $\\operatorname{Cov}(\\epsilon_{i},\\epsilon_{j})=0$ for $i\\neq j$. Hence $\\operatorname{Var}(y_{i})=\\sigma^{2}$ and $\\operatorname{Cov}(y_{i},y_{j})=0$ for $i\\neq j$.\n\nFor any linear combination of uncorrelated random variables, the variance is given by\n$$\n\\operatorname{Var}\\Big(\\sum_{i}a_{i}Y_{i}\\Big)=\\sum_{i}a_{i}^{2}\\operatorname{Var}(Y_{i})+2\\sum_{i<j}a_{i}a_{j}\\operatorname{Cov}(Y_{i},Y_{j}),\n$$\nwhich reduces to $\\sum_{i}a_{i}^{2}\\operatorname{Var}(Y_{i})$ when the $Y_{i}$ are uncorrelated.\n\nFor $\\hat{\\mu}_{1}=\\frac{y_{1}+y_{2}+y_{3}}{3}$, we have\n$$\n\\operatorname{Var}(\\hat{\\mu}_{1})=\\operatorname{Var}\\Big(\\tfrac{1}{3}y_{1}+\\tfrac{1}{3}y_{2}+\\tfrac{1}{3}y_{3}\\Big)=\\Big(\\tfrac{1}{3}\\Big)^{2}\\sigma^{2}+\\Big(\\tfrac{1}{3}\\Big)^{2}\\sigma^{2}+\\Big(\\tfrac{1}{3}\\Big)^{2}\\sigma^{2}=\\frac{\\sigma^{2}}{3}.\n$$\n\nFor $\\hat{\\mu}_{2}=\\frac{y_{1}+y_{3}}{2}$, we have\n$$\n\\operatorname{Var}(\\hat{\\mu}_{2})=\\operatorname{Var}\\Big(\\tfrac{1}{2}y_{1}+\\tfrac{1}{2}y_{3}\\Big)=\\Big(\\tfrac{1}{2}\\Big)^{2}\\sigma^{2}+\\Big(\\tfrac{1}{2}\\Big)^{2}\\sigma^{2}=\\frac{\\sigma^{2}}{2}.\n$$\n\nTherefore, the ratio of variances is\n$$\n\\frac{\\operatorname{Var}(\\hat{\\mu}_{2})}{\\operatorname{Var}(\\hat{\\mu}_{1})}=\\frac{\\sigma^{2}/2}{\\sigma^{2}/3}=\\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1919577"}, {"introduction": "We have seen that some linear unbiased estimators are more efficient than others. The final step is to move from simply comparing estimators to actively finding the optimal one. This exercise provides a constructive proof of the Gauss-Markov theorem in a simple but powerful case. You will derive the Best Linear Unbiased Estimator (BLUE) by setting up a general form for a linear unbiased estimator and then using calculus to find the weights that minimize its variance. This hands-on derivation reveals that the sample mean is not just an intuitive choice, but the mathematically optimal one under the given conditions. [@problem_id:1919555]", "problem": "An experimentalist makes two independent measurements, $y_1$ and $y_2$, of a physical quantity whose true, unknown value is $\\mu$. Each measurement is subject to a random error. The statistical model for these measurements is given by $y_i = \\mu + \\epsilon_i$ for $i=1, 2$.\n\nThe random errors $\\epsilon_1$ and $\\epsilon_2$ are assumed to satisfy the following standard properties:\n1.  The expected value of each error is zero, i.e., $E[\\epsilon_i] = 0$.\n2.  The errors have a common, finite variance, denoted by $\\sigma^2 > 0$.\n3.  The errors are uncorrelated, meaning $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$.\n\nTo combine the two measurements into a single, improved estimate for $\\mu$, a linear estimator of the form $\\tilde{\\mu} = w y_1 + (1-w) y_2$ is proposed, where $w$ is a real-valued weight. Your task is to find the specific numerical value of $w$ that minimizes the variance of this estimator, $\\text{Var}(\\tilde{\\mu})$.", "solution": "The goal is to find the value of the weight $w$ that minimizes the variance of the estimator $\\tilde{\\mu} = w y_1 + (1-w) y_2$. The variance of the estimator is denoted by $\\text{Var}(\\tilde{\\mu})$.\n\nFirst, we express the variance of $\\tilde{\\mu}$ using the properties of variance. For a linear combination of random variables, the variance is given by:\n$$\n\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\n$$\nIn our case, the estimator is $\\tilde{\\mu} = w y_1 + (1-w) y_2$. The variables are $y_1$ and $y_2$, with coefficients $a=w$ and $b=1-w$.\n$$\n\\text{Var}(\\tilde{\\mu}) = \\text{Var}(w y_1 + (1-w) y_2) = w^2 \\text{Var}(y_1) + (1-w)^2 \\text{Var}(y_2) + 2w(1-w) \\text{Cov}(y_1, y_2)\n$$\nNext, we need to find the variance of each measurement $y_i$ and the covariance between them.\nThe variance of $y_i$ is:\n$$\n\\text{Var}(y_i) = \\text{Var}(\\mu + \\epsilon_i)\n$$\nSince $\\mu$ is a constant, its variance is zero. Thus, the variance of $y_i$ is simply the variance of the error term $\\epsilon_i$.\n$$\n\\text{Var}(y_i) = \\text{Var}(\\epsilon_i) = \\sigma^2\n$$\nThis holds for both $i=1$ and $i=2$.\n\nThe covariance between $y_1$ and $y_2$ is:\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\mu + \\epsilon_1, \\mu + \\epsilon_2)\n$$\nSince $\\mu$ is a constant, it does not affect the covariance.\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\epsilon_1, \\epsilon_2)\n$$\nThe problem states that the errors are uncorrelated, which means $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$. Therefore, $\\text{Cov}(y_1, y_2) = 0$.\n\nNow, we substitute these back into the expression for $\\text{Var}(\\tilde{\\mu})$:\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 (\\sigma^2) + (1-w)^2 (\\sigma^2) + 2w(1-w)(0)\n$$\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 \\sigma^2 + (1-w)^2 \\sigma^2\n$$\nWe can factor out the constant variance $\\sigma^2$:\n$$\n\\text{Var}(\\tilde{\\mu}) = \\sigma^2 [w^2 + (1-w)^2]\n$$\nTo find the value of $w$ that minimizes this variance, we can define a function $f(w) = w^2 + (1-w)^2$ and find its minimum. Since $\\sigma^2 > 0$, minimizing $f(w)$ is equivalent to minimizing $\\text{Var}(\\tilde{\\mu})$.\n\nLet's expand the function $f(w)$:\n$$\nf(w) = w^2 + (1 - 2w + w^2) = 2w^2 - 2w + 1\n$$\nThis is a quadratic function of $w$, representing a parabola opening upwards. The minimum can be found using calculus by taking the first derivative with respect to $w$ and setting it to zero.\n$$\n\\frac{df}{dw} = \\frac{d}{dw}(2w^2 - 2w + 1) = 4w - 2\n$$\nSet the derivative to zero to find the critical point:\n$$\n4w - 2 = 0\n$$\n$$\n4w = 2\n$$\n$$\nw = \\frac{2}{4} = \\frac{1}{2}\n$$\nTo confirm this is a minimum, we can use the second derivative test. The second derivative is:\n$$\n\\frac{d^2f}{dw^2} = \\frac{d}{dw}(4w - 2) = 4\n$$\nSince the second derivative is positive ($4 > 0$), the critical point $w = 1/2$ corresponds to a local minimum. Because $f(w)$ is a parabola, this is a global minimum.\n\nThus, the variance of the estimator $\\tilde{\\mu}$ is minimized when $w = 1/2$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1919555"}]}