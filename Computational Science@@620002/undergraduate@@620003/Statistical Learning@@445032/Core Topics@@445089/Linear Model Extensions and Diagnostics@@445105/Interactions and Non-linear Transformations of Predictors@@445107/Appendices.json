{"hands_on_practices": [{"introduction": "When we include an interaction term like $x_1 x_2$ in a model, we often want to test if it is statistically significant. However, the precision of our estimate can be compromised by relationships between the predictors themselves. This practice explores the concept of multicollinearity, specifically how correlation between main effects ($x_1$, $x_2$) can inflate the variance of the estimated coefficient for their interaction term. By simulating this phenomenon [@problem_id:3132247], you will gain a concrete understanding of why high correlation between predictors can lead to wide confidence intervals and uncertain conclusions about interaction effects.", "problem": "You are asked to examine, via controlled simulation, how correlation between predictors inflates uncertainty in the estimated coefficient of an interaction term in a normal linear model. Build a program that, for a set of specified test cases, estimates the empirical coverage and the average length of a two-sided confidence interval for the interaction coefficient under varying predictor correlation. The problem is framed in purely mathematical terms and requires using the fundamental base of the normal linear model, Ordinary Least Squares (OLS), and small-sample inference with the Student t-distribution.\n\nConsider the normal linear model\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independently of the predictors. For each simulation replicate, generate the predictors as a correlated bivariate normal vector\n$$\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n\\sim \\mathcal{N}\\!\\left(\n\\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix},\n\\begin{pmatrix}\n1  \\rho \\\\\n\\rho  1\n\\end{pmatrix}\n\\right)\n$$\nwith fixed means $\\mu_1 = \\mu_2 = 1$ and correlation $\\rho \\in (-1,1)$. Generate the response using the model with fixed parameters $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$, and $\\sigma = 1$.\n\nYour program must, for each test case, perform Monte Carlo simulation with $R$ independent replicates. In each replicate: draw a sample of size $n$ from the specified distribution for $(x_1,x_2)$ and $\\varepsilon$, fit the OLS model including an intercept, main effects $x_1$, $x_2$, and the interaction $x_1 x_2$, and construct a two-sided confidence interval for $\\beta_{12}$ at nominal level $1-\\alpha = 0.95$ using the Student t-distribution with $n-p$ degrees of freedom, where $p=4$ is the number of regression coefficients including the intercept. Record whether the true $\\beta_{12}$ is contained in the interval (coverage indicator equal to $1$ if yes and $0$ otherwise) and the interval length (a nonnegative real number). After $R$ replicates, report for each test case:\n- the empirical coverage probability as a decimal (the average of the coverage indicators), and\n- the average interval length (the average of the replicate interval lengths).\n\nUse the following fixed values for all test cases:\n- $R = 500$,\n- $\\alpha = 0.05$,\n- $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$,\n- $\\sigma = 1$,\n- $\\mu_1 = 1$, $\\mu_2 = 1$,\n- random seed fixed to $123456$ for reproducibility.\n\nTest suite:\n1. $(n,\\rho) = (200, 0.0)$,\n2. $(n,\\rho) = (200, 0.9)$,\n3. $(n,\\rho) = (200, -0.9)$,\n4. $(n,\\rho) = (60, 0.99)$.\n\nOutput specification:\n- For each test case in the listed order, output a two-element list $[c,\\ell]$ where $c$ is the empirical coverage probability and $\\ell$ is the average confidence-interval length. Both $c$ and $\\ell$ must be rounded to $3$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list in the same format, for example:\n$[[c_1,\\ell_1],[c_2,\\ell_2],[c_3,\\ell_3],[c_4,\\ell_4]]$.\n- There are no physical units or angles in this problem. Coverage must be expressed as a decimal, not with a percent sign.\n\nScientific realism and derivation base:\n- Use only the standard properties of the normal linear model, the OLS estimator, and small-sample t-based confidence intervals as the fundamental base. Do not assume or use any shortcut formulas beyond these fundamentals.\n- Ensure all random number generation is controlled by the fixed seed so that outputs are reproducible.\n\nYour program must be a complete, runnable script that requires no user input and no external files, and must adhere to the specified output format exactly.", "solution": "The problem is valid. It is a well-posed and scientifically grounded exercise in computational statistics, investigating the effect of multicollinearity on inference for an interaction term within the standard normal linear model framework. All required parameters, constants, and procedures are clearly specified.\n\nThe objective is to conduct a Monte Carlo simulation to quantify how the correlation $\\rho$ between two predictors, $x_1$ and $x_2$, affects the empirical coverage and average length of the confidence interval for the interaction coefficient $\\beta_{12}$.\n\nThe statistical foundation is the normal linear model, which in matrix form is expressed as:\n$$\ny = X\\beta + \\varepsilon\n$$\nwhere $y$ is an $n \\times 1$ vector of observations, $X$ is the $n \\times p$ design matrix, $\\beta$ is the $p \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of unobserved errors. For this problem, the model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon$, so the number of parameters is $p=4$. The coefficient vector is $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\beta_{12}]^T$. The $i$-th row of the design matrix $X$ is $[1, x_{i1}, x_{i2}, x_{i1}x_{i2}]$. The errors $\\varepsilon_i$ are assumed to be independent and identically distributed as $\\mathcal{N}(0, \\sigma^2)$.\n\nThe Ordinary Least Squares (OLS) estimator for the coefficient vector $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\nUnder the model assumptions, the conditional distribution of the OLS estimator given the design matrix $X$ is normal, with mean $\\beta$ and variance-covariance matrix:\n$$\n\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X^T X)^{-1}\n$$\nIn a practical setting, the error variance $\\sigma^2$ is unknown and must be estimated from the data. The unbiased estimator for $\\sigma^2$ is:\n$$\ns^2 = \\frac{e^T e}{n-p} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}\n$$\nwhere $e = y - X\\hat{\\beta}$ is the vector of residuals and $n-p$ are the degrees of freedom. The estimated variance of the coefficient vector is then $\\hat{\\text{Var}}(\\hat{\\beta}) = s^2(X^T X)^{-1}$. The standard error of a single coefficient estimate $\\hat{\\beta}_j$ is the square root of the $j$-th diagonal element of this matrix:\n$$\nse(\\hat{\\beta}_j) = \\sqrt{s^2 \\left((X^T X)^{-1}\\right)_{jj}}\n$$\nwhere the indices $j$ range from $0$ to $p-1=3$. Our coefficient of interest, $\\beta_{12}$, corresponds to index $j=3$.\n\nFor small samples, inference is based on the Student's t-distribution. The pivotal quantity for the interaction coefficient $\\beta_{12}$ is:\n$$\n\\frac{\\hat{\\beta}_{12} - \\beta_{12}}{se(\\hat{\\beta}_{12})} \\sim t_{n-p}\n$$\nwhere $t_{n-p}$ is the Student's t-distribution with $n-p$ degrees of freedom. A two-sided confidence interval for $\\beta_{12}$ at a nominal level of $1-\\alpha$ is constructed as:\n$$\n\\hat{\\beta}_{12} \\pm t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})\n$$\nwhere $t_{1-\\alpha/2, n-p}$ is the upper $(1-\\alpha/2)$ critical value of the $t_{n-p}$ distribution. The length of this interval is $2 \\cdot t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})$. High correlation between predictors in the design matrix $X$ (multicollinearity) inflates the diagonal elements of $(X^T X)^{-1}$, leading to larger standard errors and, consequently, wider confidence intervals. This simulation is designed to demonstrate this effect quantitatively.\n\nThe simulation proceeds as follows for each test case $(n, \\rho)$:\n1.  Initialize a random number generator with the fixed seed $123456$.\n2.  Perform a loop for $R=500$ replicates. In each replicate:\n    a. Generate a sample of $n$ predictor pairs $(x_1, x_2)$ from a bivariate normal distribution with mean $\\mu = [1, 1]^T$ and covariance matrix $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$.\n    b. Construct the $n \\times 4$ design matrix $X$ with columns for the intercept, $x_1$, $x_2$, and the interaction term $x_1 x_2$.\n    c. Generate a sample of $n$ errors $\\varepsilon$ from $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=1$.\n    d. Generate the response vector $y$ using the true model $y = X\\beta_{true} + \\varepsilon$, where $\\beta_{true} = [0, 1, 1, 0.5]^T$.\n    e. Compute the OLS estimate $\\hat{\\beta} = (X^T X)^{-1} X^T y$.\n    f. Calculate the estimated error variance $s^2$.\n    g. Calculate the standard error of the interaction coefficient estimate, $se(\\hat{\\beta}_{12})$, using the formula above.\n    h. Determine the critical value $t_{crit} = t_{1-\\alpha/2, n-4}$ for $\\alpha=0.05$.\n    i. Construct the confidence interval for $\\beta_{12}$ and calculate its length.\n    j. Record a binary indicator ($1$ if the interval contains the true value $\\beta_{12}=0.5$, $0$ otherwise) and the interval length.\n3.  After all replicates, calculate the average of the coverage indicators to obtain the empirical coverage probability and the average of the interval lengths.\n4.  Store the final rounded results for the test case.\n\nThis procedure is repeated for all four test cases, and the results are compiled into the final specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Simulates the effect of predictor correlation on the confidence interval \n    of an interaction term in a linear model.\n    \"\"\"\n    # Define fixed parameters from the problem statement\n    R = 500\n    ALPHA = 0.05\n    BETA_TRUE = np.array([0.0, 1.0, 1.0, 0.5])  # [beta0, beta1, beta2, beta12]\n    SIGMA = 1.0\n    MU = np.array([1.0, 1.0])\n    RANDOM_SEED = 123456\n    \n    # Define test cases (n, rho)\n    test_cases = [\n        (200, 0.0),\n        (200, 0.9),\n        (200, -0.9),\n        (60, 0.99),\n    ]\n\n    all_results = []\n    \n    # Initialize a single random number generator for reproducibility across all cases\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    for n, rho in test_cases:\n        coverage_indicators = []\n        interval_lengths = []\n\n        # Number of model parameters (intercept, x1, x2, x1*x2)\n        p = 4\n        # Degrees of freedom for the t-distribution\n        df = n - p\n        # Critical t-value for a two-sided (1-alpha)% confidence interval\n        t_crit = t.ppf(1 - ALPHA / 2, df)\n\n        for _ in range(R):\n            # Step 1: Generate the data for one replicate\n            \n            # Define the covariance matrix for the predictors (x1, x2)\n            cov_matrix = np.array([[1.0, rho], [rho, 1.0]])\n            \n            # Generate n samples of [x1, x2]\n            predictors = rng.multivariate_normal(MU, cov_matrix, size=n)\n            x1 = predictors[:, 0]\n            x2 = predictors[:, 1]\n            \n            # Construct the design matrix X of size n x p\n            X = np.ones((n, p))\n            X[:, 1] = x1\n            X[:, 2] = x2\n            X[:, 3] = x1 * x2\n            \n            # Generate n error terms\n            epsilon = rng.normal(loc=0, scale=SIGMA, size=n)\n            \n            # Generate the response variable y\n            y = X @ BETA_TRUE + epsilon\n\n            # Step 2: Fit the OLS model and get coefficient estimates\n            \n            # Calculate (X'X)^-1 for standard errors\n            inv_xtx = np.linalg.inv(X.T @ X)\n            \n            # Calculate OLS coefficient estimates: beta_hat = (X'X)^-1 * X'y\n            beta_hat = inv_xtx @ X.T @ y\n            \n            # Step 3: Construct the confidence interval for the interaction term (beta_12)\n            \n            # Calculate residuals\n            residuals = y - X @ beta_hat\n            \n            # Calculate residual sum of squares (RSS)\n            rss = residuals.T @ residuals\n            \n            # Estimate the error variance (s^2)\n            s2 = rss / df\n            \n            # Calculate the standard error of the interaction coefficient estimate\n            # This is the 4th coefficient (index 3)\n            se_beta12 = np.sqrt(s2 * inv_xtx[3, 3])\n            \n            # Calculate the margin of error\n            margin_of_error = t_crit * se_beta12\n            \n            # Calculate the length of the confidence interval\n            length = 2 * margin_of_error\n            interval_lengths.append(length)\n            \n            # Get the point estimate for the interaction coefficient\n            beta12_hat = beta_hat[3]\n            \n            # Define the confidence interval bounds\n            lower_bound = beta12_hat - margin_of_error\n            upper_bound = beta12_hat + margin_of_error\n            \n            # Check if the true parameter value is covered by the interval\n            true_beta12 = BETA_TRUE[3]\n            covered = 1 if (lower_bound = true_beta12 = upper_bound) else 0\n            coverage_indicators.append(covered)\n\n        # Calculate empirical coverage and average length for the current test case\n        empirical_coverage = np.mean(coverage_indicators)\n        avg_length = np.mean(interval_lengths)\n        \n        # Format results as specified (rounded to 3 decimal places)\n        result = [round(empirical_coverage, 3), round(avg_length, 3)]\n        all_results.append(result)\n\n    # Final print statement in the exact required format: [[c1,l1],[c2,l2],...]\n    formatted_results = [f\"[{c},{l}]\" for c, l in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3132247"}, {"introduction": "A standard linear model assumes that an interaction effect, such as that captured by a $\\beta_{12} x_1 x_2$ term, is present and constant across the entire domain of the predictors. Real-world phenomena are often more complex, with interactions that may only exist in specific regions. This exercise [@problem_id:3132277] contrasts a global, parametric model (polynomial regression) with a flexible, local model (a regression tree) to see which better approximates a function with a localized interaction. This practice highlights the critical importance of model flexibility and demonstrates how non-parametric methods can discover structures that global models miss.", "problem": "You are asked to write a complete, runnable program that compares a global polynomial regression with interaction against a local piecewise-constant regression tree on data where the interaction between predictors exists only in a subset of the input space. Your implementation must follow definitions from empirical risk minimization using squared loss, ordinary least squares, and greedy sum-of-squares splitting.\n\nConsider predictors $x_1$ and $x_2$ sampled independently from the uniform distribution on $[0,1]$. The latent regression function is\n$$\nf(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1  \\tau_1, x_2  \\tau_2\\}\\, c\\, x_1 x_2,\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, $a_1 = 1.0$, $a_2 = -1.0$, $b_1 = 0.5$, $b_2 = -0.5$, and $(\\tau_1,\\tau_2)$ and $c$ are scenario-specific parameters. Observations are generated as\n$$\ny = f(x_1,x_2) + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise.\n\nYour program must implement and compare the following two estimators:\n\n- Global polynomial with interaction: Fit a linear model using ordinary least squares (OLS) on the feature vector\n$$\n\\phi(x_1,x_2) = \\left[1,\\, x_1,\\, x_2,\\, x_1^2,\\, x_2^2,\\, x_1 x_2\\right].\n$$\nThis model imposes a single global interaction term $x_1 x_2$ across the entire input space.\n\n- Local regression tree: Fit a binary, axis-aligned regression tree that predicts a constant in each leaf. Build the tree by greedily minimizing the empirical sum of squared errors at each split. At a node with sample responses $\\{y_i\\}_{i=1}^n$, the node impurity is\n$$\n\\mathrm{SSE} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2,\\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n$$\nA candidate split on feature $j \\in \\{1,2\\}$ at threshold $t$ partitions the node into left and right children; choose the split that minimizes the sum of child SSE subject to a minimum leaf size constraint. Stop splitting when the maximum depth is reached, no valid split satisfies the minimum leaf size, or no split yields a reduction in SSE. Each leaf predicts the sample mean of $y$ within that leaf.\n\nFor evaluation, for each scenario below, generate a training set of size $n_{\\text{train}}$ and an independent test set of size $n_{\\text{test}} = 20000$. Train both models on the training data. On the test set, compute the out-of-sample mean squared error (MSE) against the noise-free target $f(x_1,x_2)$:\n$$\n\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i})\\right)^2.\n$$\nThis MSE measures function approximation quality, not noise prediction.\n\nUse the following test suite of scenarios. For each scenario $k$, use the specified random seeds for training and test generation to ensure reproducibility. The tree uses the indicated maximum depth and minimum leaf size.\n\n- Scenario $1$: $n_{\\text{train}} = 400$, $c = 3.0$, $(\\tau_1,\\tau_2) = (0.5, 0.5)$, $\\sigma = 0.1$, tree maximum depth $= 2$, tree minimum leaf size $= 20$, training seed $= 7$, test seed $= 97$.\n- Scenario $2$: $n_{\\text{train}} = 400$, $c = 0.5$, $(\\tau_1,\\tau_2) = (0.6, 0.6)$, $\\sigma = 0.1$, tree maximum depth $= 2$, tree minimum leaf size $= 20$, training seed $= 8$, test seed $= 98$.\n- Scenario $3$: $n_{\\text{train}} = 400$, $c = 3.0$, $(\\tau_1,\\tau_2) = (0.5, 0.5)$, $\\sigma = 1.0$, tree maximum depth $= 2$, tree minimum leaf size $= 40$, training seed $= 9$, test seed $= 99$.\n- Scenario $4$: $n_{\\text{train}} = 800$, $c = 5.0$, $(\\tau_1,\\tau_2) = (0.85, 0.85)$, $\\sigma = 0.1$, tree maximum depth $= 3$, tree minimum leaf size $= 10$, training seed $= 10$, test seed $= 100$.\n\nYour program must, for each scenario in order, output two floating-point numbers: first the MSE of the global polynomial with interaction, then the MSE of the local regression tree. Concatenate all scenarios’ results into a single list.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers enclosed in square brackets, with each number printed to exactly $6$ decimal places, in the order\n$$\n[\\mathrm{MSE}^{\\text{global}}_1,\\mathrm{MSE}^{\\text{tree}}_1,\\mathrm{MSE}^{\\text{global}}_2,\\mathrm{MSE}^{\\text{tree}}_2,\\mathrm{MSE}^{\\text{global}}_3,\\mathrm{MSE}^{\\text{tree}}_3,\\mathrm{MSE}^{\\text{global}}_4,\\mathrm{MSE}^{\\text{tree}}_4].\n$$\nNo other text should be printed.", "solution": "The problem requires a comparison between two regression models—a global polynomial model and a local regression tree—on a simulated dataset where an interaction term is present only in a specific sub-region of the feature space. The comparison is based on the out-of-sample Mean Squared Error (MSE) against the true, noise-free data generating function.\n\n### Step 1: Problem Validation\n\nThe first step is to validate the problem statement.\n\n**Givens Extracted:**\n- **Predictors:** $x_1, x_2$ are sampled independently from a Uniform distribution on $[0,1]$.\n- **Latent Function:** $f(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1  \\tau_1, x_2  \\tau_2\\}\\, c\\, x_1 x_2$.\n- **Constants:** $a_1 = 1.0$, $a_2 = -1.0$, $b_1 = 0.5$, $b_2 = -0.5$.\n- **Noise Model:** $y = f(x_1,x_2) + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n- **Estimator 1 (Global Polynomial):** Ordinary Least Squares (OLS) fit to the feature vector $\\phi(x_1,x_2) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$.\n- **Estimator 2 (Local Regression Tree):** A binary, axis-aligned tree built by greedily minimizing the sum of squared errors (SSE) at each split, subject to a maximum depth and a minimum leaf size. Leaf nodes predict the sample mean of responses.\n- **Tree Impurity:** $\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\bar{y})^2$.\n- **Evaluation Metric:** Out-of-sample MSE on a test set of size $n_{\\text{test}} = 20000$, calculated as $\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i}))^2$.\n- **Scenarios:** Four scenarios are defined with specific parameters for $n_{\\text{train}}$, $c$, $(\\tau_1,\\tau_2)$, $\\sigma$, tree maximum depth, tree minimum leaf size, and random seeds for training and testing data generation.\n\n**Validation Against Criteria:**\n- **Scientific Grounding:** The problem is firmly rooted in standard statistical learning theory, employing well-established concepts like OLS, regression trees, empirical risk minimization with squared error loss, and simulation-based model comparison. All definitions are standard and mathematically sound.\n- **Well-Posedness:** The problem provides a complete set of instructions. The data generation process, model structures, fitting procedures, and evaluation metrics are all explicitly defined. The use of random seeds ensures that the numerical results are reproducible. A unique and meaningful solution exists.\n- **Objectivity:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n- **Completeness and Consistency:** All necessary parameters for each of the four scenarios are provided, and there are no internal contradictions.\n- **Relevance:** The problem directly addresses the topic of modeling interactions and non-linearities, a central theme in statistical learning. It poses a meaningful question about the trade-offs between a global model that may be misspecified and a local, more flexible model that might overfit or be better at capturing local phenomena.\n\n**Verdict:** The problem is valid, scientifically sound, well-posed, and complete. We may proceed with the solution.\n\n### Step 2: Solution Derivation and Algorithmic Design\n\nThe core of the task is to implement the data generation process, the two specified models, and the evaluation procedure for each given scenario.\n\n**Data Generation**\nFor each scenario, we generate a training set $(X_{\\text{train}}, y_{\\text{train}})$ of size $n_{\\text{train}}$ and a test set $(X_{\\text{test}}, y_{\\text{test}})$ of size $n_{\\text{test}} = 20000$. The predictors $x_1, x_2$ are drawn from $\\text{Uniform}(0,1)$. The true function values $f(x_1, x_2)$ are computed, and the observed responses $y$ are generated by adding Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$. The random number generator is seeded as specified for reproducibility.\n\n**Model 1: Global Polynomial with Interaction**\nThis model assumes the relationship between the predictors and the response can be approximated by a single polynomial function over the entire feature space:\n$$\n\\hat{f}(x_1, x_2) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_1^2 + \\hat{\\beta}_4 x_2^2 + \\hat{\\beta}_5 x_1 x_2\n$$\nThe coefficients $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\dots, \\hat{\\beta}_5]^T$ are estimated using OLS. Given the training data $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$, we form the design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  x_{1,1}  x_{2,1}  x_{1,1}^2  x_{2,1}^2  x_{1,1}x_{2,1} \\\\\n1  x_{1,2}  x_{2,2}  x_{1,2}^2  x_{2,2}^2  x_{1,2}x_{2,2} \\\\\n\\vdots  \\vdots  \\vdots  \\vdots  \\vdots  \\vdots \\\\\n1  x_{1,n_{\\text{train}}}  x_{2,n_{\\text{train}}}  x_{1,n_{\\text{train}}}^2  x_{2,n_{\\text{train}}}^2  x_{1,n_{\\text{train}}}x_{2,n_{\\text{train}}}\n\\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n_{\\text{train}}} \\end{pmatrix}\n$$\nThe OLS solution finds the $\\hat{\\boldsymbol{\\beta}}$ that minimizes the residual sum of squares $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$. This solution is given by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. For numerical stability, this linear system is preferably solved using methods like QR decomposition or SVD, which is handled by library functions like `scipy.linalg.lstsq`. Once $\\hat{\\boldsymbol{\\beta}}$ is found, predictions for new data points in the test set are made by $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$.\n\n**Model 2: Local Regression Tree**\nThis model partitions the feature space $[0,1] \\times [0,1]$ into disjoint rectangular regions and fits a simple constant model (the mean of the responses) in each region. The partitioning is done greedily and recursively.\n\n- **Node Representation:** A tree is composed of nodes. An internal node specifies a split (a feature index $j \\in \\{1,2\\}$ and a threshold $t$), and has two children (left and right). A leaf node has no children and stores a prediction value (the sample mean of $y$ for the training points that fall into its region).\n\n- **Splitting an Internal Node:** At a given node containing a subset of training data, we search for the best split. A split is defined by a feature $j$ and a value $t$. It partitions the data into a left set $\\{ \\mathbf{x}_i \\mid x_{i,j} \\le t \\}$ and a right set $\\{ \\mathbf{x}_i \\mid x_{i,j}  t \\}$. The quality of a split is measured by the reduction in the total sum of squared errors:\n$$\n\\text{Gain}(j, t) = \\text{SSE}_{\\text{parent}} - (\\text{SSE}_{\\text{left}} + \\text{SSE}_{\\text{right}})\n$$\nwhere $\\text{SSE}_{\\text{region}} = \\sum_{i \\in \\text{region}} (y_i - \\bar{y}_{\\text{region}})^2$. We search over all features $j \\in \\{1,2\\}$ and all valid split points $t$ to find the combination that maximizes this gain. A split is valid only if it respects the minimum leaf size constraint: both the left and right children must contain at least the minimum number of samples. The potential split points $t$ for a feature can be efficiently chosen as the midpoints between consecutive unique sorted values of that feature.\n\n- **Recursive Tree Construction:** The algorithm proceeds as follows:\n  1. Start with the root node containing all training data.\n  2. For a node, check stopping conditions:\n     a. The current depth equals the maximum allowed depth.\n     b. The number of samples in the node is less than twice the minimum leaf size (since any split would violate the constraint).\n     c. All response values $y_i$ in the node are identical (SSE is $0$, no further improvement is possible).\n  3. If a stopping condition is met, create a leaf node and store the mean of the responses as its prediction value.\n  4. Otherwise, find the best split $(j^*, t^*)$ by maximizing the SSE gain.\n  5. If no split provides a positive gain, or no valid split exists, create a leaf node.\n  6. Otherwise, create an internal node with the split $(j^*, t^*)$. Partition the data and recursively build the left and right subtrees.\n\n- **Prediction:** To predict for a new point $\\mathbf{x}_{\\text{new}}$, we traverse the tree from the root. At each internal node, we compare the relevant feature of $\\mathbf{x}_{\\text{new}}$ with the node's threshold to decide whether to go left or right, until we reach a leaf node. The prediction is the value stored in that leaf.\n\n**Evaluation**\nFor each scenario, after training both models on the training data, we generate predictions $\\hat{f}_{\\text{poly}}$ and $\\hat{f}_{\\text{tree}}$ for the features in the large test set. The performance is measured by the MSE against the true function values $f_{\\text{test}}$ (without noise):\n$$\n\\text{MSE}_{\\text{poly}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{poly}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\n$$\n\\text{MSE}_{\\text{tree}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{tree}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\nThese two values are computed for each of the four scenarios and reported.\n\nThis systematic approach ensures that all requirements of the problem are met, providing a rigorous comparison between a global parametric model and a local non-parametric model. The implementation will follow these principles precisely.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the data generation, model training,\n    evaluation, and printing of results for all specified scenarios.\n    \"\"\"\n\n    # --- Helper Classes and Functions ---\n\n    class Node:\n        \"\"\"Represents a node in the regression tree.\"\"\"\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n            self.feature_index = feature_index  # Feature to split on\n            self.threshold = threshold          # Threshold for the split\n            self.left = left                    # Left subtree (for values = threshold)\n            self.right = right                  # Right subtree (for values  threshold)\n            self.value = value                  # Prediction value if it's a leaf node\n\n    class DecisionTree:\n        \"\"\"\n        A regression tree that uses greedy sum-of-squares splitting.\n        \"\"\"\n        def __init__(self, max_depth=2, min_samples_leaf=1):\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.root = None\n\n        def fit(self, X, y):\n            \"\"\"Build the regression tree from training data.\"\"\"\n            self.root = self._grow_tree(X, y)\n\n        def _grow_tree(self, X, y, depth=0):\n            \"\"\"Recursively grow the tree.\"\"\"\n            n_samples, n_features = X.shape\n            \n            # Check stopping criteria\n            is_pure = len(np.unique(y)) == 1\n            if (depth = self.max_depth or\n                    n_samples  2 * self.min_samples_leaf or\n                    is_pure):\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            best_split = self._find_best_split(X, y, n_samples, n_features)\n\n            if best_split['gain'] = 0:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n            \n            left_indices = X[:, best_split['feature_index']] = best_split['threshold']\n            right_indices = ~left_indices\n            \n            left_subtree = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n            right_subtree = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n            \n            return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)\n\n        def _find_best_split(self, X, y, n_samples, n_features):\n            \"\"\"Find the best feature and threshold to split on.\"\"\"\n            best_split = {'gain': -1}\n            current_sse = self._calculate_sse(y)\n\n            for feat_idx in range(n_features):\n                thresholds = np.unique(X[:, feat_idx])\n                \n                # Use midpoints between unique sorted values as potential splits\n                if len(thresholds)  1:\n                    test_thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n                else: \n                    continue\n\n                for threshold in test_thresholds:\n                    left_indices = X[:, feat_idx] = threshold\n                    right_indices = ~left_indices\n                    \n                    if np.sum(left_indices)  self.min_samples_leaf or np.sum(right_indices)  self.min_samples_leaf:\n                        continue\n                        \n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    sse_left = self._calculate_sse(y_left)\n                    sse_right = self._calculate_sse(y_right)\n                    \n                    total_child_sse = sse_left + sse_right\n                    gain = current_sse - total_child_sse\n\n                    if gain  best_split['gain']:\n                        best_split = {\n                            'feature_index': feat_idx,\n                            'threshold': threshold,\n                            'gain': gain\n                        }\n            return best_split\n\n        @staticmethod\n        def _calculate_sse(y):\n            \"\"\"Calculate Sum of Squared Errors.\"\"\"\n            if len(y) == 0:\n                return 0\n            mean = np.mean(y)\n            return np.sum((y - mean) ** 2)\n\n        def predict(self, X):\n            \"\"\"Make predictions for a set of samples.\"\"\"\n            return np.array([self._predict_one(x, self.root) for x in X])\n\n        def _predict_one(self, x, node):\n            \"\"\"Traverse the tree to predict for a single sample.\"\"\"\n            if node.value is not None:\n                return node.value\n            if x[node.feature_index] = node.threshold:\n                return self._predict_one(x, node.left)\n            else:\n                return self._predict_one(x, node.right)\n\n    def true_f(x1, x2, c, tau1, tau2):\n        \"\"\"Computes the latent noise-free function value.\"\"\"\n        a1, a2, b1, b2 = 1.0, -1.0, 0.5, -0.5\n        interaction_term = c * x1 * x2 * ((x1  tau1)  (x2  tau2))\n        return a1 * x1 + a2 * x2 + b1 * x1**2 + b2 * x2**2 + interaction_term\n\n    def generate_data(n, seed, c, tau1, tau2, sigma):\n        \"\"\"Generates training or test data.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.uniform(0, 1, size=(n, 2))\n        x1, x2 = X[:, 0], X[:, 1]\n        \n        f_vals = true_f(x1, x2, c, tau1, tau2)\n        noise = rng.normal(0, sigma, size=n)\n        y = f_vals + noise\n        \n        return X, y, f_vals\n\n    def fit_predict_polynomial(X_train, y_train, X_test):\n        \"\"\"Fits OLS polynomial model and predicts on test data.\"\"\"\n        # Construct design matrix for training\n        X_design_train = np.c_[\n            np.ones(X_train.shape[0]),\n            X_train[:, 0],\n            X_train[:, 1],\n            X_train[:, 0]**2,\n            X_train[:, 1]**2,\n            X_train[:, 0] * X_train[:, 1]\n        ]\n        \n        # Solve for coefficients using least squares\n        beta, _, _, _ = linalg.lstsq(X_design_train, y_train)\n        \n        # Construct design matrix for testing\n        X_design_test = np.c_[\n            np.ones(X_test.shape[0]),\n            X_test[:, 0],\n            X_test[:, 1],\n            X_test[:, 0]**2,\n            X_test[:, 1]**2,\n            X_test[:, 0] * X_test[:, 1]\n        ]\n        \n        # Make predictions\n        y_pred = X_design_test @ beta\n        return y_pred\n    \n    # --- Scenarios and Main Execution Logic ---\n\n    test_cases = [\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 7, 'test_seed': 97},\n        {'n_train': 400, 'c': 0.5, 'tau1': 0.6, 'tau2': 0.6, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 8, 'test_seed': 98},\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 1.0, 'depth': 2, 'min_leaf': 40, 'train_seed': 9, 'test_seed': 99},\n        {'n_train': 800, 'c': 5.0, 'tau1': 0.85, 'tau2': 0.85, 'sigma': 0.1, 'depth': 3, 'min_leaf': 10, 'train_seed': 10, 'test_seed': 100},\n    ]\n\n    all_results = []\n    n_test = 20000\n\n    for case in test_cases:\n        # Generate data\n        X_train, y_train, _ = generate_data(case['n_train'], case['train_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n        X_test, _, f_test = generate_data(n_test, case['test_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n\n        # Model 1: Global Polynomial\n        poly_preds = fit_predict_polynomial(X_train, y_train, X_test)\n        mse_poly = np.mean((poly_preds - f_test)**2)\n        \n        # Model 2: Local Regression Tree\n        tree = DecisionTree(max_depth=case['depth'], min_samples_leaf=case['min_leaf'])\n        tree.fit(X_train, y_train)\n        tree_preds = tree.predict(X_test)\n        mse_tree = np.mean((tree_preds - f_test)**2)\n\n        all_results.extend([mse_poly, mse_tree])\n    \n    # Format and print final results\n    print(f\"[{','.join([f'{r:.6f}' for r in all_results])}]\")\n\nsolve()\n\n```", "id": "3132277"}, {"introduction": "After fitting a complex model, identifying which variables interact and by how much is crucial for interpretation. While coefficient magnitudes can be informative in simple linear models, we need more general tools for assessing interaction strength. This exercise introduces Friedman's $H$-statistic, a powerful and model-agnostic tool for quantifying interaction strength based on partial dependence functions. By implementing this statistic from first principles to measure the proportion of a function's variance explained by an interaction [@problem_id:3132262], you will develop a practical skill for moving beyond significance tests to gauge the real-world importance of interaction effects.", "problem": "You are tasked with implementing an estimator for pairwise interaction strength using Friedman’s interaction statistic in a controlled simulation. Work in the framework of statistical learning where interactions and non-linear transformations of predictors are central. Consider a function with three independent predictors and a single pairwise interaction between the first two predictors: for any integer sample size $n \\geq 2$, draw $n$ independent samples $x_{1}, x_{2}, x_{3} \\sim \\text{Uniform}(-1,1)$ and define the deterministic target function $$f(x_{1}, x_{2}, x_{3}) = a_{1} x_{1} + a_{2} x_{2} + a_{3} x_{3} + \\beta \\, x_{1} x_{2}.$$ Use the following core definitions as the starting point of your derivation and implementation: the concept of partial dependence and the inclusion-exclusion logic that isolates interaction components by integrating out nuisance variables, and the variance operator applied to functions of random variables. Specifically, the interaction component between $x_{1}$ and $x_{2}$ is defined in terms of expectations with respect to the joint distribution of the predictors, and the interaction strength $H_{12}$ should be derived as a normalized measure of variability due to the interaction component alone relative to the variability of the full function. This must be derived from first principles without employing any shortcut formulas not obtained directly from the provided definitions.\n\nYour program must:\n- Generate the samples $x_{1}, x_{2}, x_{3}$ for each test case using a pseudorandom number generator initialized with the provided integer seed, where each predictor is independently drawn from $\\text{Uniform}(-1,1)$.\n- Compute an empirical estimate of the pairwise interaction component for $(x_{1}, x_{2})$ using only expectations (approximated by sample averages) over the nuisance variable(s), in accordance with the inclusion-exclusion logic for isolating interactions from partial dependence.\n- Compute the empirical Friedman interaction statistic $H_{12}$ as the square root of the ratio between the empirical variance of the estimated interaction component and the empirical variance of the full function $f(x_{1}, x_{2}, x_{3})$. All expectations and variances must be computed with respect to the simulated sample for each test case. No external models or fitting procedures are permitted; the oracle function $f$ is known and must be used directly.\n- Produce one real-valued output per test case, equal to the estimated $H_{12}$, using deterministic seeds to ensure reproducibility. Angles are not involved, and no physical units apply. Express the final numeric output values as plain decimal floats.\n\nTest suite:\n- Case $1$: $n = 64$, $a_{1} = 1$, $a_{2} = 1$, $a_{3} = 1$, $\\beta = 1$, seed $= 7$.\n- Case $2$: $n = 256$, $a_{1} = 1$, $a_{2} = 1$, $a_{3} = 1$, $\\beta = 1$, seed $= 7$.\n- Case $3$: $n = 1024$, $a_{1} = 1$, $a_{2} = 1$, $a_{3} = 1$, $\\beta = 1$, seed $= 7$.\n- Case $4$: $n = 256$, $a_{1} = 1$, $a_{2} = 1$, $a_{3} = 1$, $\\beta = 0$, seed $= 11$.\n- Case $5$: $n = 256$, $a_{1} = 0.5$, $a_{2} = 0.5$, $a_{3} = 0.5$, $\\beta = 3$, seed $= 13$.\n\nDesign for coverage:\n- Cases $1$–$3$ hold the function fixed and vary $n$ to assess sample size effects on estimation.\n- Case $4$ is a boundary condition with no interaction ($\\beta = 0$) and should yield an estimate near $0$.\n- Case $5$ stresses the estimator with a strong interaction relative to the additive components.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[result1,result2,result3]}$). The list must contain exactly $5$ floats corresponding to the estimated $H_{12}$ values for the $5$ test cases, in the order they are specified above.", "solution": "The problem of estimating Friedman’s interaction statistic, $H_{jk}$, is a well-defined task in statistical learning. It requires a rigorous application of statistical principles, namely partial dependence and the decomposition of a function's variance. We shall derive the required empirical estimator from first principles.\n\n**Step 1: Foundational Principles**\n\nLet $f(\\mathbf{x}) = f(x_1, x_2, \\dots, x_p)$ be a function of $p$ predictor variables. The predictors are random variables with a joint distribution $p(\\mathbf{x})$.\n\nThe **Partial Dependence Function (PDP)** for a subset of variables $S \\subset \\{1, 2, \\dots, p\\}$ with values $\\mathbf{x}_S$ is defined as the expectation of $f(\\mathbf{x})$ over the marginal distribution of the complement variables $C = \\{1, 2, \\dots, p\\} \\setminus S$:\n$$f_S(\\mathbf{x}_S) = E_{X_C}[f(\\mathbf{x}_S, X_C)] = \\int f(\\mathbf{x}_S, \\mathbf{x}_C) \\, p(\\mathbf{x}_C) \\, d\\mathbf{x}_C$$\nwhere $p(\\mathbf{x}_C)$ is the marginal probability density of the variables in $C$.\n\nThe **Interaction Component** between two variables, say $x_j$ and $x_k$, is defined using an inclusion-exclusion logic. It isolates the part of the function that depends jointly on $x_j$ and $x_k$ after accounting for their individual main effects. The two-variable interaction function $f_{jk}(x_j, x_k)$ is:\n$$f_{jk}(x_j, x_k) = f_{\\{j,k\\}}(x_j, x_k) - f_{\\{j\\}}(x_j) - f_{\\{k\\}}(x_k) + E[f(X)]$$\nwhere $f_{\\{j,k\\}}$, $f_{\\{j\\}}$, and $f_{\\{k\\}}$ are the PDPs for the respective variable sets, and $E[f(X)]$ is the global mean of the function.\n\n**Friedman's H-statistic** for the interaction between $x_j$ and $x_k$, denoted $H_{jk}$, normalizes the variance of the interaction component by the variance of the total function $f$:\n$$H_{jk}^2 = \\frac{\\text{Var}[f_{jk}(X_j, X_k)]}{\\text{Var}[f(X)]}$$\nThe statistic $H_{jk}$ is the square root of this ratio, providing a measure of interaction strength on a scale from $0$ to $1$.\n\n**Step 2: Empirical Estimation**\n\nThe problem requires empirical estimation based on a provided sample of size $n$: $\\{\\mathbf{x}_i = (x_{1i}, x_{2i}, x_{3i})\\}_{i=1}^n$. All expectations are replaced by sample averages.\n\nGiven the function $f(x_1, x_2, x_3) = a_1 x_1 + a_2 x_2 + a_3 x_3 + \\beta x_1 x_2$. We need to compute the empirical estimate for $H_{12}$.\n\nFirst, we define the empirical estimators for each term in the interaction component formula for a specific observation $i$, which has predictor values $(x_{1i}, x_{2i}, x_{3i})$.\n\nThe **empirical PDP for $(x_1, x_2)$** evaluated at $(x_{1i}, x_{2i})$ is estimated by averaging over the sample values of the nuisance variable $x_3$:\n$$\\hat{f}_{\\{1,2\\}}(x_{1i}, x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n f(x_{1i}, x_{2i}, x_{3l})$$\n\nThe **empirical PDP for $x_1$** evaluated at $x_{1i}$ is estimated by averaging over the sample pairs $(x_{2l}, x_{3l})$:\n$$\\hat{f}_{\\{1\\}}(x_{1i}) = \\frac{1}{n} \\sum_{l=1}^n f(x_{1i}, x_{2l}, x_{3l})$$\n\nThe **empirical PDP for $x_2$** evaluated at $x_{2i}$ is estimated by averaging over the sample pairs $(x_{1l}, x_{3l})$:\n$$\\hat{f}_{\\{2\\}}(x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n f(x_{1l}, x_{2i}, x_{3l})$$\n\nThe **empirical global mean** is simply the average of the function over the entire sample:\n$$\\hat{E}[f] = \\bar{f} = \\frac{1}{n} \\sum_{l=1}^n f(x_{1l}, x_{2l}, x_{3l})$$\n\nCombining these, the estimated interaction component for the $i$-th observation is:\n$$\\hat{f}_{12,i} = \\hat{f}_{12}(x_{1i}, x_{2i}) = \\hat{f}_{\\{1,2\\}}(x_{1i}, x_{2i}) - \\hat{f}_{\\{1\\}}(x_{1i}) - \\hat{f}_{\\{2\\}}(x_{2i}) + \\bar{f}$$\n\n**Step 3: Derivation for the Specific Function**\n\nThis general formula is computationally intensive ($O(n^2)$ to compute each $\\hat{f}_{12,i}$). However, the problem provides a specific, simple functional form. We can substitute $f(x_1, x_2, x_3) = a_1 x_1 + a_2 x_2 + a_3 x_3 + \\beta x_1 x_2$ into the empirical PDP definitions to obtain a highly efficient computation.\n\nLet $\\bar{x}_k = \\frac{1}{n} \\sum_{l=1}^n x_{kl}$ be the sample mean of predictor $k$, and let $C_{12} = \\frac{1}{n} \\sum_{l=1}^n x_{1l}x_{2l}$ be the sample mean of the product $x_1 x_2$.\n\nSubstituting $f$ into the PDP estimators:\n$$ \\hat{f}_{\\{1,2\\}}(x_{1i}, x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1i} + a_2 x_{2i} + a_3 x_{3l} + \\beta x_{1i} x_{2i}) = a_1 x_{1i} + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{1i} x_{2i} $$\n$$ \\hat{f}_{\\{1\\}}(x_{1i}) = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1i} + a_2 x_{2l} + a_3 x_{3l} + \\beta x_{1i} x_{2l}) = a_1 x_{1i} + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta x_{1i} \\bar{x}_2 $$\n$$ \\hat{f}_{\\{2\\}}(x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1l} + a_2 x_{2i} + a_3 x_{3l} + \\beta x_{1l} x_{2i}) = a_1 \\bar{x}_1 + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{2i} \\bar{x}_1 $$\n$$ \\bar{f} = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1l} + a_2 x_{2l} + a_3 x_{3l} + \\beta x_{1l} x_{2l}) = a_1 \\bar{x}_1 + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta C_{12} $$\n\nNow, substituting these into the formula for $\\hat{f}_{12,i}$ and canceling terms:\n\\begin{align*}\n\\hat{f}_{12,i} = (a_1 x_{1i} + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{1i} x_{2i}) \\\\\n           \\quad - (a_1 x_{1i} + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta x_{1i} \\bar{x}_2) \\\\\n           \\quad - (a_1 \\bar{x}_1 + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{2i} \\bar{x}_1) \\\\\n           \\quad + (a_1 \\bar{x}_1 + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta C_{12})\n\\end{align*}\nThe additive terms involving $a_1$, $a_2$, and $a_3$ all cancel, leaving only the terms multiplied by $\\beta$:\n$$ \\hat{f}_{12,i} = \\beta x_{1i} x_{2i} - \\beta x_{1i} \\bar{x}_2 - \\beta x_{2i} \\bar{x}_1 + \\beta C_{12} $$\n$$ \\hat{f}_{12,i} = \\beta (x_{1i} x_{2i} - x_{1i} \\bar{x}_2 - x_{2i} \\bar{x}_1 + C_{12}) $$\nThis simplified formula allows for the calculation of all $n$ values of the interaction component in $O(n)$ time after computing the initial sample means.\n\n**Step 4: Final Algorithm for Empirical $H_{12}$**\n\nFor each test case with parameters $n$, $a_1$, $a_2$, $a_3$, $\\beta$, and a random seed:\n\n1.  Generate an $n \\times 3$ data matrix $X$, where each column is an independent draw of $n$ samples from a $\\text{Uniform}(-1, 1)$ distribution. Let the columns be $X_1$, $X_2$, $X_3$.\n2.  Compute the $n$ values of the full function $f_i = f(x_{1i}, x_{2i}, x_{3i})$ for $i=1, \\dots, n$.\n3.  Calculate the empirical variance of the full function's values (using $n$ as the denominator):\n    $$ \\widehat{\\text{Var}}[f] = \\frac{1}{n} \\sum_{i=1}^n (f_i - \\bar{f})^2 $$\n4.  Compute the sample means $\\bar{x}_1, \\bar{x}_2$ and the sample mean of the product $C_{12} = \\frac{1}{n}\\sum_i x_{1i}x_{2i}$.\n5.  Using the derived formula, compute the $n$ values of the estimated interaction component, $\\hat{f}_{12,i}$, for $i=1, \\dots, n$.\n6.  Calculate the empirical variance of these interaction component values:\n    $$ \\widehat{\\text{Var}}[\\hat{f}_{12}] = \\frac{1}{n} \\sum_{i=1}^n (\\hat{f}_{12,i} - \\overline{\\hat{f}_{12}})^2 $$\n    where $\\overline{\\hat{f}_{12}}$ is the mean of the $\\hat{f}_{12,i}$ values.\n7.  Compute the estimated H-statistic:\n    $$ \\hat{H}_{12} = \\sqrt{\\frac{\\widehat{\\text{Var}}[\\hat{f}_{12}]}{\\widehat{\\text{Var}}[f]}} $$\n    If $\\widehat{\\text{Var}}[f]$ is zero, the result is taken to be $0$, which occurs if $f$ is constant across the sample. This will happen for Case $4$ where $\\beta=0$, making $\\hat{f}_{12,i}$ identically zero and thus $\\widehat{\\text{Var}}[\\hat{f}_{12}]=0$.\n\nThis procedure is implemented to calculate the required values for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Friedman's H-statistic for pairwise interaction for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a1, a2, a3, beta, seed)\n        (64, 1, 1, 1, 1, 7),\n        (256, 1, 1, 1, 1, 7),\n        (1024, 1, 1, 1, 1, 7),\n        (256, 1, 1, 1, 0, 11),\n        (256, 0.5, 0.5, 0.5, 3, 13),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, a1, a2, a3, beta, seed = case\n        \n        # 1. Generate samples for predictors x1, x2, x3.\n        # Initialize the pseudorandom number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n        # Draw n samples for 3 predictors from Uniform(-1, 1).\n        X = rng.uniform(low=-1.0, high=1.0, size=(n, 3))\n        x1, x2, x3 = X[:, 0], X[:, 1], X[:, 2]\n\n        # 2. Compute the values of the full function f for each sample.\n        f_vals = a1 * x1 + a2 * x2 + a3 * x3 + beta * x1 * x2\n        \n        # 3. Calculate the empirical variance of the full function.\n        # np.var uses the population variance formula (denominator n), as required.\n        var_f = np.var(f_vals)\n\n        # Handle the edge case where the function variance is zero.\n        if var_f == 0.0:\n            # If the total variance is zero, the interaction strength must be zero.\n            results.append(0.0)\n            continue\n            \n        # 4. Compute necessary sample means for the interaction component formula.\n        x1_mean = np.mean(x1)\n        x2_mean = np.mean(x2)\n        # C12 is the mean of the product of x1 and x2.\n        c12 = np.mean(x1 * x2)\n\n        # 5. Compute the values of the estimated interaction component.\n        # This uses the efficient formula derived from first principles for the given f.\n        f_12_vals = beta * (x1 * x2 - x1 * x2_mean - x2 * x1_mean + c12)\n\n        # 6. Calculate the empirical variance of the interaction component.\n        var_int = np.var(f_12_vals)\n\n        # 7. Compute the estimated H-statistic.\n        h_stat = np.sqrt(var_int / var_f)\n        results.append(h_stat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.17f}'.rstrip('0').rstrip('.') for r in results)}]\")\n\nsolve()\n```", "id": "3132262"}]}