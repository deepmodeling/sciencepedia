{"hands_on_practices": [{"introduction": "We begin by exploring the fundamental relationship between observed data and model residuals. This first practice [@problem_id:3183487] guides you through calculating the hat matrix $H$ from first principles and using it to see how a change in a single observation $y_i$ propagates to the entire vector of residuals, $e = (I-H)y$. This exercise makes tangible the crucial but non-obvious fact that OLS residuals are correlated, with a structure determined entirely by the predictor variables.", "problem": "Consider a fixed-design linear regression with $n=5$ observations and $p=3$ predictors. Let the design matrix $X \\in \\mathbb{R}^{5 \\times 3}$ have columns $c_{0}, c_{1}, c_{2}$ defined by\n$$\nc_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_{1} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad\nc_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 0 \\end{pmatrix},\n$$\nso that\n$$\nX = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & -1 & -1 \\\\\n1 & 0 & 0\n\\end{pmatrix}.\n$$\nYou may assume the response vector $y \\in \\mathbb{R}^{5}$ is arbitrary, and the least-squares fitted vector is the orthogonal projection of $y$ onto the column space of $X$ under the standard Euclidean inner product. The residual vector is $e = y - \\hat{y}$.\n\nTasks:\n1. Construct explicitly the linear operator on $\\mathbb{R}^{5}$ that maps any $y$ to its least-squares fitted value $\\hat{y}$ in the column space of $X$. Compute this operator exactly for the given $X$.\n2. Using the operator from part 1, analyze perturbations to $y$ of the form $y = y_{0} + \\delta v$, where $y_{0}$ is fixed, $\\delta \\in \\mathbb{R}$ is a scalar, and $v$ is a standard basis vector $e_{k}$ in $\\mathbb{R}^{5}$ (i.e., the $k$-th coordinate direction). Determine how the residual vector $e = (I - H) y$ responds per coordinate direction.\n3. For the specific perturbation with $\\delta = 3$ and $v = e_{3}$, determine the resulting change in the residual at observation $i=5$.\n\nProvide your final answer as a single exact number. No rounding is required and there are no physical units in this problem.", "solution": "The problem is evaluated to be valid as it is scientifically grounded in linear regression theory, well-posed with a unique solution, and formally stated with no ambiguities or contradictions. We proceed with the solution.\n\nThe problem requires a three-part analysis of a linear regression model defined by the design matrix $X$. The three tasks are:\n1.  To construct the projection operator onto the column space of $X$, known as the hat matrix $H$.\n2.  To analyze the response of the residual vector to a specific type of perturbation in the response vector $y$.\n3.  To compute the change in a specific residual for a given perturbation.\n\nLet the linear model be $y = X\\beta + \\epsilon$, where $y \\in \\mathbb{R}^{5}$ is the response vector, $X \\in \\mathbb{R}^{5 \\times 3}$ is the design matrix, $\\beta \\in \\mathbb{R}^{3}$ is the vector of coefficients, and $\\epsilon \\in \\mathbb{R}^{5}$ is the error vector. The least-squares estimate of $\\beta$ is $\\hat{\\beta} = (X^T X)^{-1}X^T y$. The fitted values are given by $\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1}X^T y$.\n\n**Part 1: Construction of the Hat Matrix**\n\nThe linear operator that maps $y$ to $\\hat{y}$ is the hat matrix, $H$, defined as:\n$$ H = X(X^T X)^{-1}X^T $$\nWe are given the design matrix:\n$$ X = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & -1 & -1 \\\\\n1 & 0 & 0\n\\end{pmatrix} $$\nFirst, we compute the matrix $X^T X$:\n$$ X^T X = \\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 & 0 \\\\\n1 & 1 & -1 & -1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & -1 & -1 \\\\\n1 & 0 & 0\n\\end{pmatrix} $$\nThe columns of $X$, denoted $c_0, c_1, c_2$, are orthogonal. We can see this by computing the off-diagonal elements of $X^T X$:\n$c_0^T c_1 = 1-1+1-1+0 = 0$\n$c_0^T c_2 = 1+1-1-1+0 = 0$\n$c_1^T c_2 = 1-1-1+1+0 = 0$\nThe diagonal elements are the squared norms of the columns:\n$c_0^T c_0 = 1^2+1^2+1^2+1^2+1^2 = 5$\n$c_1^T c_1 = 1^2+(-1)^2+1^2+(-1)^2+0^2 = 4$\n$c_2^T c_2 = 1^2+1^2+(-1)^2+(-1)^2+0^2 = 4$\nThus, $X^T X$ is a diagonal matrix:\n$$ X^T X = \\begin{pmatrix}\n5 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} $$\nThe inverse, $(X^T X)^{-1}$, is also a diagonal matrix with the reciprocals of the diagonal entries:\n$$ (X^T X)^{-1} = \\begin{pmatrix}\n\\frac{1}{5} & 0 & 0 \\\\\n0 & \\frac{1}{4} & 0 \\\\\n0 & 0 & \\frac{1}{4}\n\\end{pmatrix} $$\nNow we can compute the hat matrix $H$. Since the columns of $X$ are orthogonal, $H$ can be written as the sum of projection matrices onto each column:\n$$ H = \\sum_{k=0}^{2} \\frac{c_k c_k^T}{c_k^T c_k} $$\nAlternatively, the element $H_{ij}$ of the hat matrix is given by $H_{ij} = X_i (X^T X)^{-1} X_j^T$, where $X_i$ is the $i$-th row of $X$. Using the derived $(X^TX)^{-1}$:\n$$ H_{ij} = \\frac{x_{i1} x_{j1}}{5} + \\frac{x_{i2} x_{j2}}{4} + \\frac{x_{i3} x_{j3}}{4} $$\nLet's compute the entries of $H$:\nThe diagonal elements (leverages) $h_{ii} = H_{ii}$:\n$h_{11} = h_{22} = h_{33} = h_{44} = \\frac{1^2}{5} + \\frac{(\\pm 1)^2}{4} + \\frac{(\\pm 1)^2}{4} = \\frac{1}{5} + \\frac{1}{4} + \\frac{1}{4} = \\frac{7}{10}$.\n$h_{55} = \\frac{1^2}{5} + \\frac{0^2}{4} + \\frac{0^2}{4} = \\frac{1}{5} = \\frac{2}{10}$.\nThe off-diagonal elements $h_{ij} = H_{ij}$ for $i \\ne j$:\n$h_{12} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot (-1)}{4} + \\frac{1 \\cdot 1}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{13} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot 1}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{14} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot (-1)}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} - \\frac{1}{2} = -\\frac{3}{10}$.\n$h_{15} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot 0}{4} + \\frac{1 \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{23} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot 1}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} - \\frac{1}{2} = -\\frac{3}{10}$.\n$h_{24} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot (-1)}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{25} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot 0}{4} + \\frac{1 \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{34} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot (-1)}{4} + \\frac{(-1) \\cdot (-1)}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{35} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot 0}{4} + \\frac{(-1) \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$.\n$h_{45} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot 0}{4} + \\frac{(-1) \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$.\nUsing symmetry ($h_{ij}=h_{ji}$), the complete hat matrix is:\n$$ H = \\frac{1}{10} \\begin{pmatrix}\n7 & 2 & 2 & -3 & 2 \\\\\n2 & 7 & -3 & 2 & 2 \\\\\n2 & -3 & 7 & 2 & 2 \\\\\n-3 & 2 & 2 & 7 & 2 \\\\\n2 & 2 & 2 & 2 & 2\n\\end{pmatrix} $$\n\n**Part 2: Analysis of Perturbations**\n\nThe residual vector is $e = y - \\hat{y}$. Since $\\hat{y} = Hy$, the residual vector is given by $e = y - Hy = (I - H)y$, where $I$ is the $5 \\times 5$ identity matrix.\nLet the original response vector be $y_0$ and its corresponding residual vector be $e_0 = (I-H)y_0$.\nThe response vector is perturbed to $y' = y_0 + \\delta v$, where $v$ is a standard basis vector $e_k$. The new residual vector $e'$ is:\n$$ e' = (I-H)y' = (I-H)(y_0 + \\delta e_k) $$\nBy the linearity of the operator $(I-H)$, we have:\n$$ e' = (I-H)y_0 + \\delta(I-H)e_k = e_0 + \\delta(I-H)e_k $$\nThe change in the residual vector is $\\Delta e = e' - e_0$:\n$$ \\Delta e = \\delta(I-H)e_k $$\nThe vector $(I-H)e_k$ is simply the $k$-th column of the matrix $(I-H)$. Therefore, a perturbation of size $\\delta$ to the $k$-th observation $y_k$ causes the residual vector to change by $\\delta$ times the $k$-th column of $(I-H)$. The change in the $i$-th residual, $\\Delta e_i$, is given by $\\Delta e_i = \\delta((I-H)e_k)_i = \\delta(I-H)_{ik}$.\n\nThe matrix $I-H$ is:\n$$ I-H = \\frac{1}{10} \\begin{pmatrix}\n10 & 0 & 0 & 0 & 0 \\\\\n0 & 10 & 0 & 0 & 0 \\\\\n0 & 0 & 10 & 0 & 0 \\\\\n0 & 0 & 0 & 10 & 0 \\\\\n0 & 0 & 0 & 0 & 10\n\\end{pmatrix} - \\frac{1}{10} \\begin{pmatrix}\n7 & 2 & 2 & -3 & 2 \\\\\n2 & 7 & -3 & 2 & 2 \\\\\n2 & -3 & 7 & 2 & 2 \\\\\n-3 & 2 & 2 & 7 & 2 \\\\\n2 & 2 & 2 & 2 & 2\n\\end{pmatrix} $$\n$$ I-H = \\frac{1}{10} \\begin{pmatrix}\n3 & -2 & -2 & 3 & -2 \\\\\n-2 & 3 & 3 & -2 & -2 \\\\\n-2 & 3 & 3 & -2 & -2 \\\\\n3 & -2 & -2 & 3 & -2 \\\\\n-2 & -2 & -2 & -2 & 8\n\\end{pmatrix} $$\n\n**Part 3: Specific Perturbation Calculation**\n\nWe are asked to find the change in the residual at observation $i=5$ for a specific perturbation.\nThe given perturbation has $\\delta = 3$ and $v = e_3$. This means the perturbation is applied to the $3$-rd observation, so $k=3$. We are interested in the change in the $5$-th residual, so $i=5$.\nUsing the formula derived in Part 2:\n$$ \\Delta e_i = \\delta(I-H)_{ik} $$\nSubstituting the given values:\n$$ \\Delta e_5 = 3 \\cdot (I-H)_{53} $$\nFrom the matrix $I-H$ calculated above, the element in the $5$-th row and $3$-rd column is $(I-H)_{53} = -\\frac{2}{10}$.\nTherefore, the change in the residual at observation $i=5$ is:\n$$ \\Delta e_5 = 3 \\cdot \\left(-\\frac{2}{10}\\right) = -\\frac{6}{10} = -\\frac{3}{5} $$", "answer": "$$\\boxed{-\\frac{3}{5}}$$", "id": "3183487"}, {"introduction": "Having seen how the hat matrix operates as a whole, we now focus on its diagonal elements—the leverages $h_{ii}$—which quantify each data point's potential for influence. This exercise [@problem_id:3183500] uses a hypothetical scenario with duplicated observations to build a concrete intuition for leverage. By analyzing this special case, you will discover how a point's position in the predictor space constrains the model's fit and how leverage reflects a point's remoteness from its peers.", "problem": "Consider a simple linear regression with intercept estimated by Ordinary Least Squares (OLS), where the design matrix $X$ has two columns: an intercept and a single covariate $x$. Start with the $n=3$ design\n$$\nX_{\\text{before}}=\\begin{pmatrix}\n1 & 0\\\\\n1 & 1\\\\\n1 & 2\n\\end{pmatrix},\n$$\nand response vector\n$$\ny=\\begin{pmatrix}\n1\\\\\n3\\\\\n2\n\\end{pmatrix}.\n$$\nNow construct a duplicated design by adding a fourth observation whose covariate row is identical to the second row of $X_{\\text{before}}$, yielding\n$$\nX_{\\text{after}}=\\begin{pmatrix}\n1 & 0\\\\\n1 & 1\\\\\n1 & 2\\\\\n1 & 1\n\\end{pmatrix},\n\\quad\ny_{\\text{after}}=\\begin{pmatrix}\n1\\\\\n3\\\\\n2\\\\\n5\n\\end{pmatrix},\n$$\nso that rows $2$ and $4$ of $X_{\\text{after}}$ are identical. Let $H=X(X^{\\top}X)^{-1}X^{\\top}$ be the hat matrix, $h_{ii}$ its diagonal elements (the leverages), and $e=(I-H)y$ the residual vector.\n\nUsing only the core definitions of OLS and the hat matrix, and without invoking any pre-packaged shortcut formulas, do the following for the duplicated design $X_{\\text{after}}$: derive the leverages for the duplicated rows and explain, via $e=(I-H)y$, how identical rows in $X$ constrain the fitted values and residuals for those indices. As a baseline for comparison, also compute the leverage $h_{22}$ for $X_{\\text{before}}$.\n\nYour final task is to report the exact change in leverage for the duplicated covariate pattern caused by duplication, defined as\n$$\n\\Delta h \\equiv h_{22}^{(\\text{after})}-h_{22}^{(\\text{before})}.\n$$\nExpress the final answer as an exact fraction. No rounding is required.", "solution": "The analysis proceeds in three parts as requested. First, we compute the leverage for the specified observation in the initial design. Second, we perform the same computation for the duplicated design and explain the resulting constraints on the model's fitted values and residuals. Finally, we compute the change in leverage.\n\nThe hat matrix is defined as $H=X(X^{\\top}X)^{-1}X^{\\top}$, where $X$ is the design matrix. The leverage of the $i$-th observation, $h_{ii}$, is the $i$-th diagonal element of $H$. It can be computed directly as $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$, where $x_i^{\\top}$ is the $i$-th row of $X$.\n\n**1. Leverage Calculation for the Initial Design ($X_{\\text{before}}$)**\n\nThe initial design matrix is given by:\n$$\nX_{\\text{before}} = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n$$\nFirst, we compute the matrix $X_{\\text{before}}^{\\top}X_{\\text{before}}$:\n$$\nX_{\\text{before}}^{\\top}X_{\\text{before}} = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 2 \\\\\n0 \\cdot 1 + 1 \\cdot 1 + 2 \\cdot 1 & 0 \\cdot 0 + 1 \\cdot 1 + 2 \\cdot 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 & 3 \\\\\n3 & 5\n\\end{pmatrix}\n$$\nNext, we find the inverse of this matrix. The determinant is $\\det(X_{\\text{before}}^{\\top}X_{\\text{before}}) = (3)(5) - (3)(3) = 15 - 9 = 6$.\nThe inverse is:\n$$\n(X_{\\text{before}}^{\\top}X_{\\text{before}})^{-1} = \\frac{1}{6}\n\\begin{pmatrix}\n5 & -3 \\\\\n-3 & 3\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{5}{6} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{1}{2}\n\\end{pmatrix}\n$$\nWe are asked to compute the leverage $h_{22}^{(\\text{before})}$. The second row of $X_{\\text{before}}$ is $x_2^{\\top} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$.\n$$\nh_{22}^{(\\text{before})} = x_2^{\\top}(X_{\\text{before}}^{\\top}X_{\\text{before}})^{-1}x_2 =\n\\begin{pmatrix}\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{5}{6} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n1 \\cdot \\frac{5}{6} + 1 \\cdot (-\\frac{1}{2}) & 1 \\cdot (-\\frac{1}{2}) + 1 \\cdot \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{5}{6} - \\frac{3}{6} & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{2}{6} & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\frac{1}{3}\n$$\nThus, the leverage for the second observation in the original design is $h_{22}^{(\\text{before})} = \\frac{1}{3}$.\n\n**2. Leverage and Constraints for the Duplicated Design ($X_{\\text{after}}$)**\n\nThe duplicated design matrix is:\n$$\nX_{\\text{after}} = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 1\n\\end{pmatrix}\n$$\nThe corresponding response vector is $y_{\\text{after}} = \\begin{pmatrix} 1 & 3 & 2 & 5 \\end{pmatrix}^{\\top}$. Rows $2$ and $4$ of $X_{\\text{after}}$ are identical.\nWe compute $X_{\\text{after}}^{\\top}X_{\\text{after}}$:\n$$\nX_{\\text{after}}^{\\top}X_{\\text{after}} = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n0 & 1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n4 & 4 \\\\\n4 & 6\n\\end{pmatrix}\n$$\nThe determinant is $\\det(X_{\\text{after}}^{\\top}X_{\\text{after}}) = (4)(6) - (4)(4) = 24 - 16 = 8$.\nThe inverse is:\n$$\n(X_{\\text{after}}^{\\top}X_{\\text{after}})^{-1} = \\frac{1}{8}\n\\begin{pmatrix}\n6 & -4 \\\\\n-4 & 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{3}{4} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{1}{2}\n\\end{pmatrix}\n$$\nThe duplicated rows are $x_2^{\\top} = x_4^{\\top} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$. Their leverages, $h_{22}^{(\\text{after})}$ and $h_{44}^{(\\text{after})}$, must be equal. We compute this value:\n$$\nh_{22}^{(\\text{after})} = h_{44}^{(\\text{after})} =\n\\begin{pmatrix}\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{3}{4} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n\\frac{3}{4} - \\frac{1}{2} & -\\frac{1}{2} + \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{4} & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\frac{1}{4}\n$$\nThe leverages for the duplicated rows are $h_{22}^{(\\text{after})} = h_{44}^{(\\text{after})} = \\frac{1}{4}$.\n\nNow we explain the constraints on fitted values and residuals. Let $x_i^{\\top}$ and $x_j^{\\top}$ be two identical rows of the design matrix $X$. The fitted value for the $k$-th observation is $\\hat{y}_k = x_k^{\\top}\\hat{\\beta}$, where $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$. Since $x_i^{\\top}=x_j^{\\top}$, it is immediately evident that their fitted values must be identical:\n$$\n\\hat{y}_i = x_i^{\\top}\\hat{\\beta} = x_j^{\\top}\\hat{\\beta} = \\hat{y}_j\n$$\nThis is the primary constraint imposed by duplicated rows in $X$: the model must produce the same prediction for each duplicated point.\n\nThis can also be explained via the hat matrix $H$. The $(k, l)$-th element of $H$ is $H_{kl} = x_k^{\\top}(X^{\\top}X)^{-1}x_l$. If $x_i^{\\top} = x_j^{\\top}$, then for any column index $l \\in \\{1, \\dots, n\\}$, we have $H_{il} = x_i^{\\top}(X^{\\top}X)^{-1}x_l = x_j^{\\top}(X^{\\top}X)^{-1}x_l = H_{jl}$. This means that the $i$-th and $j$-th rows of the hat matrix are identical.\nThe fitted values are given by the vector $\\hat{y} = Hy$. The $i$-th component is $\\hat{y}_i = \\sum_{l=1}^n H_{il}y_l$, and the $j$-th component is $\\hat{y}_j = \\sum_{l=1}^n H_{jl}y_l$. Since the rows $H_{i\\cdot}$ and $H_{j\\cdot}$ are identical, it follows that $\\hat{y}_i = \\hat{y}_j$. For our problem, this means $\\hat{y}_2 = \\hat{y}_4$.\n\nThe residuals are defined as $e = (I-H)y$, or element-wise, $e_k = y_k - \\hat{y}_k$. For the duplicated indices $i$ and $j$, we have:\n$$\ne_i = y_i - \\hat{y}_i\n$$\n$$\ne_j = y_j - \\hat{y}_j\n$$\nSince $\\hat{y}_i = \\hat{y}_j$, any difference in the observed responses $y_i$ and $y_j$ is transferred directly to the residuals:\n$$\ne_i - e_j = (y_i - \\hat{y}_i) - (y_j - \\hat{y}_j) = y_i - y_j\n$$\nIn the given problem, $y_2 = 3$ and $y_4 = 5$, and we found $\\hat{y}_2 = \\hat{y}_4$. We can compute the value $\\hat{y} = 11/4$. The residuals are $e_2 = 3 - \\frac{11}{4} = \\frac{1}{4}$ and $e_4 = 5 - \\frac{11}{4} = \\frac{9}{4}$. Their difference is $e_2 - e_4 = \\frac{1}{4} - \\frac{9}{4} = -\\frac{8}{4} = -2$, which is equal to $y_2 - y_4 = 3 - 5 = -2$.\n\n**3. Change in Leverage ($\\Delta h$)**\n\nThe final task is to compute the change in leverage for the duplicated covariate pattern.\n$$\n\\Delta h = h_{22}^{(\\text{after})} - h_{22}^{(\\text{before})}\n$$\nUsing the values derived above:\n$$\n\\Delta h = \\frac{1}{4} - \\frac{1}{3} = \\frac{3}{12} - \\frac{4}{12} = -\\frac{1}{12}\n$$\nThe duplication of the observation with covariate $x=1$ causes the leverage of each of the individual points at that covariate value to decrease.", "answer": "$$\n\\boxed{-\\frac{1}{12}}\n$$", "id": "3183500"}, {"introduction": "A point with high leverage has the *potential* to be influential, but does that guarantee it will drastically change our regression model? This final practice [@problem_id:3183398] tackles this critical question by introducing Cook's distance, a primary tool for diagnosing influence. By constructing and comparing two carefully designed datasets, you will verify that high influence arises from the interaction of high leverage and a large residual, resolving a common point of confusion in regression diagnostics.", "problem": "Consider an ordinary least squares linear regression model with intercept. Let the design matrix be denoted by $X \\in \\mathbb{R}^{n \\times k}$ with full column rank, the response vector be $y \\in \\mathbb{R}^{n}$, and the fitted coefficient vector be $\\hat{\\beta} \\in \\mathbb{R}^{k}$, where $\\hat{\\beta}$ minimizes the sum of squared residuals. Denote the fitted values by $\\hat{y} \\in \\mathbb{R}^{n}$, the residual vector by $e \\in \\mathbb{R}^{n}$, and the mean squared error by $\\widehat{\\sigma}^{2}$. The hat matrix is the linear operator $H \\in \\mathbb{R}^{n \\times n}$ that maps the observed responses $y$ to the fitted responses $\\hat{y}$, and its diagonal entries $h_{ii}$ are called leverages. Cook’s distance $D_{i}$ measures the influence of the $i$th observation on the fitted regression, defined as the scaled change in the fitted values or, equivalently, in the estimated coefficients when the $i$th observation is deleted. You will use these core definitions, together with the normal equations for least squares, to derive computable expressions and evaluate them on a small test suite.\n\nYour task is to:\n- Construct $X$, compute $\\hat{\\beta}$, $\\hat{y}$, $e$, $\\widehat{\\sigma}^{2}$, and the hat matrix $H$, then extract the leverage $h_{ii}$ for a specified index $i$.\n- Compute Cook’s distance $D_{i}$ for the same index $i$ using its definition as the scaled change in fit under deletion of the $i$th observation, expressed in terms of $e_{i}$, $h_{ii}$, $k$, and $\\widehat{\\sigma}^{2}$.\n- Show, through the test suite below, that a large leverage $h_{ii}$ does not, by itself, imply a large Cook’s distance $D_{i}$, because $D_{i}$ depends on both $h_{ii}$ and the corresponding residual $e_{i}$.\n\nAssumptions and base facts you may use:\n- The least squares estimator $\\hat{\\beta}$ satisfies the normal equations $X^{\\top} X \\hat{\\beta} = X^{\\top} y$.\n- The fitted values can be written as a linear map of $y$, that is, $\\hat{y} = H y$ for some matrix $H$ that depends only on $X$ and satisfies $H^{2} = H$ and $H^{\\top} = H$.\n- The residuals satisfy $e = y - \\hat{y}$, and the mean squared error is $\\widehat{\\sigma}^{2} = \\lVert e \\rVert_{2}^{2} / (n - k)$.\n- Leverage $h_{ii}$ is the $i$th diagonal element of $H$, and Cook’s distance $D_{i}$ is the standard influence measure defined via the scaled change in fit upon deletion of the $i$th observation.\n\nImplementation requirements:\n- Use zero-based indexing for the observation index $i$.\n- For each test case, compute the pair $(h_{ii}, D_{i})$ for the specified $i$ and report both as floating-point numbers rounded to exactly $6$ decimal places.\n\nTest suite specification:\n- Test case $\\mathrm{A}$ (high leverage with small residual):\n  - Dimensions: $n = 8$, $k = 3$ (intercept plus two predictors).\n  - Rows of $X$ are $[1, x_{1}, x_{2}]$ given by:\n    - Row $0$: $[1, 0, 0]$\n    - Row $1$: $[1, 1, 0]$\n    - Row $2$: $[1, 1, 1]$\n    - Row $3$: $[1, 2, 1]$\n    - Row $4$: $[1, 0, 2]$\n    - Row $5$: $[1, 2, 2]$\n    - Row $6$: $[1, 1, 1]$\n    - Row $7$: $[1, 10, 10]$\n  - True coefficient (used only to synthesize $y$): $\\beta_{\\mathrm{true}} = [1.0, 2.0, -1.0]$.\n  - Response $y$:\n    - $y_{0} = 1.0 + 0 \\cdot 2.0 + 0 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{1} = 1.0 + 1 \\cdot 2.0 + 0 \\cdot (-1.0) - 0.2 = 2.8$\n    - $y_{2} = 1.0 + 1 \\cdot 2.0 + 1 \\cdot (-1.0) + 0.05 = 2.05$\n    - $y_{3} = 1.0 + 2 \\cdot 2.0 + 1 \\cdot (-1.0) - 0.05 = 3.95$\n    - $y_{4} = 1.0 + 0 \\cdot 2.0 + 2 \\cdot (-1.0) + 0.0 = -1.0$\n    - $y_{5} = 1.0 + 2 \\cdot 2.0 + 2 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{6} = 1.0 + 1 \\cdot 2.0 + 1 \\cdot (-1.0) - 0.1 = 1.9$\n    - $y_{7} = 1.0 + 10 \\cdot 2.0 + 10 \\cdot (-1.0) + 0.0 = 11.0$\n  - Index: $i = 7$.\n\n- Test case $\\mathrm{B}$ (high leverage with large residual):\n  - Same $X$ as in test case $\\mathrm{A}$.\n  - Same $y$ as in test case $\\mathrm{A}$ except the last entry:\n    - $y_{7} = 21.0$.\n  - Index: $i = 7$.\n\n- Test case $\\mathrm{C}$ (no extreme leverage, moderate residual):\n  - Dimensions: $n = 8$, $k = 3$ (intercept plus two predictors).\n  - Rows of $X$ are $[1, x_{1}, x_{2}]$ given by:\n    - Row $0$: $[1, 0.0, 0.0]$\n    - Row $1$: $[1, 1.0, 0.0]$\n    - Row $2$: $[1, 1.0, 1.0]$\n    - Row $3$: $[1, 2.0, 1.0]$\n    - Row $4$: $[1, 0.0, 2.0]$\n    - Row $5$: $[1, 2.0, 2.0]$\n    - Row $6$: $[1, 1.0, 1.0]$\n    - Row $7$: $[1, 0.5, 1.5]$\n  - True coefficient (used only to synthesize $y$): $\\beta_{\\mathrm{true}} = [1.0, 2.0, -1.0]$.\n  - Response $y$:\n    - $y_{0} = 1.0 + 0.0 \\cdot 2.0 + 0.0 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{1} = 1.0 + 1.0 \\cdot 2.0 + 0.0 \\cdot (-1.0) - 0.2 = 2.8$\n    - $y_{2} = 1.0 + 1.0 \\cdot 2.0 + 1.0 \\cdot (-1.0) + 0.05 = 2.05$\n    - $y_{3} = 1.0 + 2.0 \\cdot 2.0 + 1.0 \\cdot (-1.0) - 0.05 = 3.95$\n    - $y_{4} = 1.0 + 0.0 \\cdot 2.0 + 2.0 \\cdot (-1.0) + 0.0 = -1.0$\n    - $y_{5} = 1.0 + 2.0 \\cdot 2.0 + 2.0 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{6} = 1.0 + 1.0 \\cdot 2.0 + 1.0 \\cdot (-1.0) - 0.1 = 1.9$\n    - $y_{7} = 1.0 + 0.5 \\cdot 2.0 + 1.5 \\cdot (-1.0) + 0.05 = 0.55$\n  - Index: $i = 2$.\n\nFinal output format:\n- Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as $[h_{\\mathrm{A}}, D_{\\mathrm{A}}, h_{\\mathrm{B}}, D_{\\mathrm{B}}, h_{\\mathrm{C}}, D_{\\mathrm{C}}]$, where each entry is a floating-point number rounded to exactly $6$ decimal places. For example, the printed string must look like $[\\dots]$ with commas separating the entries and no extra spaces.\n\nWhat you must explain in your code comments and verify numerically:\n- In test case $\\mathrm{A}$, the row with $[1, 10, 10]$ is far in predictor space, producing a large leverage $h_{77}$, but because the residual $e_{7}$ is small, the Cook’s distance $D_{7}$ is small.\n- In test case $\\mathrm{B}$, the same large leverage $h_{77}$ combined with a large residual $e_{7}$ yields a large Cook’s distance $D_{7}$.\n- In test case $\\mathrm{C}$, with no extreme predictor values, both the leverage $h_{22}$ and Cook’s distance $D_{2}$ are expected to be modest.", "solution": "The problem requires the calculation of leverage and Cook's distance for several ordinary least squares (OLS) linear regression scenarios to demonstrate their relationship. We begin by formalizing the required quantities.\n\nThe OLS model is given by $y = X\\beta + \\epsilon$, where $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times k}$ is the design matrix of full column rank, $\\beta \\in \\mathbb{R}^{k}$ is the coefficient vector, and $\\epsilon \\in \\mathbb{R}^{n}$ is the error vector. The OLS estimator $\\hat{\\beta}$ minimizes the sum of squared residuals, $\\mathrm{SSR} = \\lVert y - X\\beta \\rVert_{2}^{2}$.\n\nThe solution to this minimization problem is given by the normal equations:\n$$\nX^{\\top} X \\hat{\\beta} = X^{\\top} y\n$$\nSince $X$ has full column rank, the matrix $X^{\\top} X \\in \\mathbb{R}^{k \\times k}$ is invertible. Thus, the unique least squares estimator is:\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n\nThe vector of fitted values, $\\hat{y}$, is the projection of $y$ onto the column space of $X$:\n$$\n\\hat{y} = X \\hat{\\beta} = X (X^{\\top} X)^{-1} X^{\\top} y\n$$\nThis defines the hat matrix, $H \\in \\mathbb{R}^{n \\times n}$, as the linear operator that maps $y$ to $\\hat{y}$:\n$$\nH = X (X^{\\top} X)^{-1} X^{\\top}\n$$\nThe hat matrix is a projection matrix, meaning it is symmetric ($H^{\\top} = H$) and idempotent ($H^{2} = H$).\n\nThe leverage of the $i$-th observation, $h_{ii}$, is the $i$-th diagonal element of the hat matrix $H$. It is given by:\n$$\nh_{ii} = x_i^{\\top} (X^{\\top} X)^{-1} x_i\n$$\nwhere $x_i^{\\top}$ is the $i$-th row of $X$. Leverage $h_{ii}$ measures the influence of the observed response $y_i$ on its own fitted value $\\hat{y}_i$, since $\\hat{y}_i = \\sum_{j=1}^{n} H_{ij} y_j = h_{ii} y_i + \\sum_{j \\neq i} H_{ij} y_j$. The value of $h_{ii}$ lies in the range $[1/n, 1]$ and reflects how far the predictor vector $x_i$ is from the mean of all predictor vectors. Points with high leverage are potential outliers in the predictor space.\n\nThe residual vector is the difference between the observed and fitted values:\n$$\ne = y - \\hat{y} = (I - H)y\n$$\nwhere $I$ is the $n \\times n$ identity matrix. The sum of squared residuals is $\\mathrm{SSR} = \\lVert e \\rVert_{2}^{2} = e^{\\top}e$. An unbiased estimator for the variance of the errors, $\\sigma^2$, is the mean squared error (MSE):\n$$\n\\widehat{\\sigma}^{2} = \\frac{e^{\\top}e}{n-k}\n$$\n\nCook's distance, $D_i$, measures the effect of deleting the $i$-th observation on the estimated coefficients. It is defined as the scaled Euclidean distance between the vector of fitted values computed with all data and the vector of fitted values computed after deleting observation $i$. A convenient and widely used formula for Cook's distance expresses it in terms of the leverage $h_{ii}$, the corresponding residual $e_i$, the number of predictors $k$, and the MSE $\\widehat{\\sigma}^2$:\n$$\nD_i = \\frac{e_i^2 h_{ii}}{k \\widehat{\\sigma}^2 (1-h_{ii})^2}\n$$\nThis formula highlights that an observation's influence (as measured by $D_i$) is a function of both its leverage ($h_{ii}$) and its residual ($e_i$). A high leverage point ($h_{ii}$ close to $1$) does not necessarily have a large Cook's distance if its residual $e_i$ is small (i.e., the point lies close to the regression line fitted by the other points). Conversely, even a point with moderate leverage can be influential if its residual is very large.\n\nThe computational procedure for each test case is as follows:\n1.  Construct the design matrix $X$ and response vector $y$. Let $n$ and $k$ be the number of rows and columns of $X$, respectively.\n2.  Compute the matrix product $X^{\\top}X$ and its inverse $(X^{\\top}X)^{-1}$.\n3.  Compute the hat matrix $H = X(X^{\\top}X)^{-1}X^{\\top}$.\n4.  Extract the leverage $h_{ii}$ from the $i$-th diagonal element of $H$.\n5.  Compute the estimated coefficients $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$.\n6.  Compute the fitted values $\\hat{y} = X\\hat{\\beta}$ and the residuals $e = y - \\hat{y}$.\n7.  Extract the residual $e_i$ for the specified index $i$.\n8.  Compute the mean squared error $\\widehat{\\sigma}^2 = \\frac{e^{\\top}e}{n-k}$.\n9.  Compute Cook's distance $D_i$ using the formula $D_i = \\frac{e_i^2 h_{ii}}{k \\widehat{\\sigma}^2 (1-h_{ii})^2}$.\n\nThis procedure will be applied to the three test cases specified.\n-   **Test Case A**: The predictor vector for observation $i=7$, $x_7 = [1, 10, 10]$, is an outlier in the predictor space, which should result in a high leverage $h_{77}$. However, the corresponding response $y_7=11.0$ is constructed to be very close to the value predicted by the true model, suggesting a small residual $e_7$. Consequently, Cook's distance $D_7$ is expected to be small.\n-   **Test Case B**: The design matrix $X$ is identical to Case A, so the leverage $h_{77}$ is unchanged and remains high. The response $y_7$ is changed to $21.0$, which is far from the value predicted by the model. This will generate a large residual $e_7$. The combination of high leverage and a large residual is expected to produce a large Cook's distance $D_7$.\n-   **Test Case C**: The design matrix $X$ has no extreme predictor values. Therefore, the leverage for any observation, including $i=2$, is expected to be modest. The residual $e_2$ is also expected to be small. As a result, Cook's distance $D_2$ should be small.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes leverage and Cook's distance for three test cases to illustrate\n    their relationship.\n    \"\"\"\n\n    test_cases = [\n        # Test Case A: High leverage, small residual\n        {\n            \"id\": \"A\",\n            \"X\": np.array([\n                [1, 0, 0], [1, 1, 0], [1, 1, 1], [1, 2, 1],\n                [1, 0, 2], [1, 2, 2], [1, 1, 1], [1, 10, 10]\n            ]),\n            \"y\": np.array([\n                1.1, 2.8, 2.05, 3.95, -1.0, 1.1, 1.9, 11.0\n            ]),\n            \"i\": 7\n        },\n        # Test Case B: High leverage, large residual\n        {\n            \"id\": \"B\",\n            \"X\": np.array([\n                [1, 0, 0], [1, 1, 0], [1, 1, 1], [1, 2, 1],\n                [1, 0, 2], [1, 2, 2], [1, 1, 1], [1, 10, 10]\n            ]),\n            \"y\": np.array([\n                1.1, 2.8, 2.05, 3.95, -1.0, 1.1, 1.9, 21.0\n            ]),\n            \"i\": 7\n        },\n        # Test Case C: No extreme leverage, moderate residual\n        {\n            \"id\": \"C\",\n            \"X\": np.array([\n                [1, 0.0, 0.0], [1, 1.0, 0.0], [1, 1.0, 1.0], [1, 2.0, 1.0],\n                [1, 0.0, 2.0], [1, 2.0, 2.0], [1, 1.0, 1.0], [1, 0.5, 1.5]\n            ]),\n            \"y\": np.array([\n                1.1, 2.8, 2.05, 3.95, -1.0, 1.1, 1.9, 0.55\n            ]),\n            \"i\": 2\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        i = case[\"i\"]\n\n        n, k = X.shape\n\n        # Step 1: Compute (X^T X)^-1\n        try:\n            XtX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            print(f\"Error: X^T X is singular for case {case['id']}.\")\n            continue\n\n        # Step 2: Compute the Hat Matrix H and extract leverage h_ii\n        # H = X @ (X^T X)^-1 @ X^T\n        H = X @ XtX_inv @ X.T\n        h_ii = H[i, i]\n\n        # Step 3: Compute OLS coefficients, fitted values, and residuals\n        # beta_hat = (X^T X)^-1 @ X^T @ y\n        beta_hat = XtX_inv @ X.T @ y\n        y_hat = X @ beta_hat\n        e = y - y_hat\n        e_i = e[i]\n\n        # Step 4: Compute Mean Squared Error (MSE)\n        # sigma_hat^2 = e^T e / (n - k)\n        e_norm_sq = e.T @ e\n        sigma2_hat = e_norm_sq / (n - k)\n\n        # Step 5: Compute Cook's Distance D_i\n        # D_i = (e_i^2 * h_ii) / (k * sigma_hat^2 * (1 - h_ii)^2)\n        # Numerically stable calculation for D_i\n        # Small values of (1 - h_ii) can cause issues if h_ii is close to 1.\n        # However, for this problem, direct computation is sufficient.\n        denominator = k * sigma2_hat * (1 - h_ii)**2\n        if abs(denominator) < 1e-15:\n            # Handle potential division by zero, though unlikely here\n            D_i = float('inf')\n        else:\n            D_i = (e_i**2 * h_ii) / denominator\n\n        # In Case A, the predictor vector at index i=7, x_7 = [1, 10, 10], is far from the\n        # other points, leading to a high leverage value h_77. However, the response y_7 = 11.0\n        # is consistent with the model, resulting in a very small residual e_7. The small\n        # residual term e_7^2 dominates the large leverage in the Cook's distance formula,\n        # yielding a small D_7.\n        \n        # In Case B, the predictors are the same, so h_77 is the same high value.\n        # But y_7 is now 21.0, creating a large discrepancy from the fitted line.\n        # This large residual e_7, combined with the high leverage h_77,\n        # produces a very large Cook's distance D_7, indicating a highly influential point.\n\n        # In Case C, the predictor space is more balanced. There are no extreme points.\n        # As a result, the leverage h_22 is modest. The residual e_2 is also small.\n        # Consequently, Cook's distance D_2 is very small, indicating low influence.\n        \n        results.extend([h_ii, D_i])\n\n    print(f\"[{','.join([f'{x:.6f}' for x in results])}]\")\n\nsolve()\n```", "id": "3183398"}]}