{"hands_on_practices": [{"introduction": "How we represent categorical data in a regression model is a critical first step. This exercise tackles the \"dummy variable trap,\" a common pitfall that introduces perfect multicollinearity into the design matrix $X$. By computationally demonstrating why including an intercept and all $k$ dummy indicators for a $k$-level category makes the matrix $X^\\top X$ non-invertible, you will gain a practical understanding of how to correctly specify your model to ensure a unique solution exists. [@problem_id:3099895]", "problem": "Consider the linear regression model $y = X \\beta + \\varepsilon$ with design matrix $X \\in \\mathbb{R}^{n \\times p}$, coefficient vector $\\beta \\in \\mathbb{R}^{p}$, and noise $\\varepsilon \\in \\mathbb{R}^{n}$. A core assumption for Ordinary Least Squares (OLS) is that $X$ has full column rank, which is equivalent to $X^\\top X$ being invertible. In statistical learning with categorical predictors, one common construction uses an intercept column and dummy (one-hot) indicator columns for each of $k$ categories. The so-called \"dummy variable trap\" occurs when the intercept is included together with all $k$ dummy indicators, producing linear dependence among the columns of $X$ and thus a non-invertible $X^\\top X$.\n\nUsing only definitions and linear algebra, write a program that, for a given categorical predictor, constructs the design matrix $X$ in three ways and checks the invertibility of $X^\\top X$ in each case:\n- Initial construction: include an intercept column $1_n$ and all $k$ dummy indicator columns.\n- Remedy A: drop the intercept and include all $k$ dummy indicator columns.\n- Remedy B: keep the intercept and drop exactly one dummy indicator column, chosen as the least frequent category in the dataset (if there is a tie, drop the lowest-index category among those with minimum frequency).\n\nYour program must compute invertibility by determining whether the numerical rank of $X^\\top X$ equals $p$, where $p$ is the number of columns in the constructed $X$. Rank should be computed in a numerically stable manner using singular value decomposition and a standard tolerance.\n\nImplement the above for the following test suite of categorical assignments (each test case provides the category index for each row and the total number of categories $k$):\n- Test case $1$: $n = 8$, $k = 3$, category indices $[0,1,2,0,1,2,0,1]$.\n- Test case $2$: $n = 7$, $k = 3$, category indices $[0,1,0,1,0,1,1]$.\n- Test case $3$: $n = 6$, $k = 2$, category indices $[0,1,0,1,0,1]$.\n\nFor each test case, produce three boolean results in the fixed order:\n$[$initial invertibility, remedy A invertibility, remedy B invertibility$]$.\n\nYour program should produce a single line of output containing all results flattened into one list as a comma-separated sequence enclosed in square brackets, in the order of test cases $1$, $2$, $3$. Concretely, the output must be\n$[$initial$_1$, remedyA$_1$, remedyB$_1$, initial$_2$, remedyA$_2$, remedyB$_2$, initial$_3$, remedyA$_3$, remedyB$_3]$,\nwhere each entry is a boolean. No physical units or angle units are involved in this problem, and no percentages are required.", "solution": "The problem requires an analysis of the \"dummy variable trap\" in linear regression, a phenomenon of perfect multicollinearity that arises from a specific encoding of categorical predictors. We are tasked with constructing three different design matrices for a given categorical variable and determining the invertibility of the resulting $X^\\top X$ matrix for each case. Invertibility is a critical condition for the existence of a unique Ordinary Least Squares (OLS) estimator, $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$. A matrix $X^\\top X$ is invertible if and only if the design matrix $X$ has full column rank.\n\nThe core principle at play is linear independence. A set of vectors $\\{v_1, v_2, \\dots, v_p\\}$ is linearly independent if the only solution to the equation $c_1 v_1 + c_2 v_2 + \\dots + c_p v_p = 0$ is the trivial solution $c_1 = c_2 = \\dots = c_p = 0$. If a non-trivial solution exists, the vectors are linearly dependent. A matrix has full column rank if and only if its columns form a linearly independent set.\n\nWe will analyze three constructions for a categorical predictor with $k$ levels, observed over $n$ samples. The categorical data is provided as a vector of indices, where each index is in $\\{0, 1, \\dots, k-1\\}$.\n\n1.  **Initial Construction**: The design matrix $X_{initial}$ is formed by including an intercept column (a vector of ones, $1_n$) along with all $k$ dummy indicator columns, $d_0, d_1, \\dots, d_{k-1}$. Each column $d_j$ is a vector where the $i$-th element is $1$ if the $i$-th sample belongs to category $j$, and $0$ otherwise. This matrix has $p = k+1$ columns. A fundamental linear dependency exists because for any sample, it must belong to exactly one category. Therefore, the sum of the dummy indicator columns is always equal to the intercept column:\n    $$ \\sum_{j=0}^{k-1} d_j = 1_n $$\n    This can be rewritten as $1 \\cdot 1_n - 1 \\cdot d_0 - \\dots - 1 \\cdot d_{k-1} = 0$, which is a non-trivial linear combination of the columns of $X_{initial}$ that equals the zero vector. Thus, the columns are linearly dependent, $rank(X_{initial}) < p$, and $X_{initial}^\\top X_{initial}$ is never invertible.\n\n2.  **Remedy A (No Intercept)**: The design matrix $X_A$ is formed by including all $k$ dummy columns but omitting the intercept. The matrix is $X_A = [d_0 | d_1 | \\dots | d_{k-1}]$, with $p=k$ columns. To check for linear independence, we consider the equation $\\sum_{j=0}^{k-1} c_j d_j = 0$. For any sample $i$ belonging to category $j'$, the equation for that row simplifies to $c_{j'} \\cdot 1 = 0$, implying $c_{j'} = 0$. If every category from $0$ to $k-1$ is present in the data at least once, this logic forces all coefficients $c_0, \\dots, c_{k-1}$ to be zero. The columns are then linearly independent, and $X_A$ has full column rank. However, if any category $j'$ is not present in the dataset, its corresponding column $d_{j'}$ will be a zero vector. The zero vector is trivially linearly dependent with any set of other vectors (e.g., $1 \\cdot d_{j'} = 0$), so $X_A$ would not have full column rank.\n\n3.  **Remedy B (Drop One Dummy)**: The design matrix $X_B$ is formed by including the intercept and $k-1$ of the $k$ dummy columns. Let the dropped column be $d_{j^*}$. The matrix is $X_B = [1_n | d_0 | \\dots | d_{j^*-1} | d_{j^*+1} | \\dots | d_{k-1}]$, with $p=k$ columns. A linear dependency would exist if $1_n$ could be expressed as a linear combination of the remaining $k-1$ dummy columns. The sum of the remaining dummy columns is $\\sum_{j \\neq j^*} d_j$. Since $\\sum_{j=0}^{k-1} d_j = 1_n$, we have $\\sum_{j \\neq j^*} d_j = 1_n - d_{j^*}$. This sum equals $1_n$ if and only if $d_{j^*} = 0$. This occurs precisely when the dropped category $j^*$ has zero observations. In such a scenario, the linear dependency is not resolved. Otherwise, if the dropped category is present in the data, the remaining columns will be linearly independent, and $X_B$ will have full column rank. The problem specifies dropping the dummy for the least frequent category, which is a sound heuristic to avoid dropping a column corresponding to a large group but fails to resolve the dependency if the least frequent category has a frequency of zero.\n\nThe computational procedure is to implement these three constructions for each test case. For each constructed matrix $X$, its corresponding Gram matrix $M = X^\\top X$ is computed. The invertibility of $M$ is then determined by comparing its numerical rank with its dimension, $p$. The rank is computed robustly using Singular Value Decomposition (SVD), as requested. A boolean result is recorded for each of the three methods for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _check_invertibility(X: np.ndarray) -> bool:\n    \"\"\"\n    Checks if X.T @ X is invertible by determining if its rank equals its number of columns.\n    Rank is computed using SVD, which is numerically stable.\n    The problem asks to check the invertibility of X^T X by checking its rank.\n    \"\"\"\n    p = X.shape[1]\n    \n    # A matrix with 0 columns has a 0x0 Gram matrix.\n    # The 0x0 matrix is invertible (its determinant is 1).\n    # This case is not reached by the test suite but is handled for correctness.\n    if p == 0:\n        return True\n    \n    # Form the Gram matrix X-transpose-X\n    XTX = X.T @ X\n    \n    # Compute the rank of the Gram matrix.\n    # np.linalg.matrix_rank uses SVD and a tolerance based on machine precision.\n    rank_XTX = np.linalg.matrix_rank(XTX)\n    \n    # The matrix is invertible if and only if it has full rank.\n    return rank_XTX == p\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # (category_indices, k)\n        (np.array([0, 1, 2, 0, 1, 2, 0, 1]), 3),\n        (np.array([0, 1, 0, 1, 0, 1, 1]), 3),\n        (np.array([0, 1, 0, 1, 0, 1]), 2),\n    ]\n\n    all_results = []\n    \n    for category_indices, k in test_cases:\n        n = len(category_indices)\n        \n        # --- Common setup for each case ---\n        # Create dummy indicator columns for k categories.\n        # The columns of `dummies` correspond to categories 0, 1, ..., k-1.\n        dummies = np.zeros((n, k), dtype=int)\n        dummies[np.arange(n), category_indices] = 1\n        \n        # Create the intercept column (a vector of ones).\n        intercept = np.ones((n, 1), dtype=int)\n\n        # --- Method 1: Initial Construction (with dummy variable trap) ---\n        # Include intercept and all k dummy variables.\n        X_initial = np.hstack((intercept, dummies))\n        all_results.append(_check_invertibility(X_initial))\n\n        # --- Method 2: Remedy A (drop intercept) ---\n        # Include all k dummy variables, no intercept.\n        X_A = dummies\n        all_results.append(_check_invertibility(X_A))\n\n        # --- Method 3: Remedy B (drop one dummy variable) ---\n        # Keep intercept, drop one dummy variable.\n        # Find the least frequent category. np.bincount counts occurrences.\n        # minlength=k ensures all categories get a count, even if it is 0.\n        frequencies = np.bincount(category_indices, minlength=k)\n        \n        # np.argmin() finds the index of the first occurrence of the minimum value,\n        # which satisfies the tie-breaking rule (drop lowest-index category).\n        category_to_drop = np.argmin(frequencies)\n        \n        # In X_initial, column 0 is the intercept. Columns 1 to k are the dummies\n        # for categories 0 to k-1. So, the column for `category_to_drop`\n        # is at index `1 + category_to_drop`.\n        col_index_to_drop = 1 + category_to_drop\n        X_B = np.delete(X_initial, col_index_to_drop, axis=1)\n        all_results.append(_check_invertibility(X_B))\n\n    # Format the final output as a comma-separated string of booleans.\n    # The default string representation of a boolean in Python is \"True\" or \"False\".\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3099895"}, {"introduction": "Not all data points exert the same pull on a regression line; some have the potential to be highly influential. This practice introduces two fundamental diagnostic concepts: leverage ($h_{ii}$), which identifies observations with unusual predictor values, and Cook's distance ($D_i$), which measures the actual impact of removing an observation on the model's estimates. Through this exercise, you will learn to distinguish between potential and realized influence, a key skill for identifying problematic data points that may be distorting your findings. [@problem_id:3099870]", "problem": "You are working within the ordinary least squares framework of linear regression. The foundational base is the following: given a design matrix $X$ with full column rank and a response vector $y$, the ordinary least squares estimator minimizes the sum of squared residuals and its fitted values are the orthogonal projection of $y$ onto the column space of $X$. The projection operator’s diagonal elements quantify the leverage of each observation, and influence is assessed by how much the fitted model would change if a single observation were perturbed or removed. In this task, you will plant a single high-leverage point in $X$ for each test case, compute leverages $h_{ii}$ from the projection operator tied to $X$ and fitted residuals, and then evaluate influence using the Cook’s distance $D_i$ definition rooted in residual size, leverage, and model degrees of freedom.\n\nOperational requirements:\n- For each test case, augment $X$ by an intercept column so that the total number of parameters equals the number of columns of the augmented $X$.\n- Fit the ordinary least squares model using the augmented $X$ and compute residuals.\n- Compute the leverage values $h_{ii}$ as the diagonal entries of the projection operator associated with $X$.\n- Compute Cook’s distance $D_i$ using its definition in terms of residuals, leverage, number of parameters, and residual variance.\n- Use the following detection rules:\n  - High leverage rule-of-thumb: flag observation $i$ as high leverage if $h_{ii} > \\frac{2p}{n}$, where $p$ is the number of columns of the augmented $X$ and $n$ is the number of observations.\n  - Influential observation rule-of-thumb: flag observation $i$ as influential if $D_i > \\frac{4}{n}$.\n- For each test case, report the following six quantities in order:\n  $[$\n  the leverage $h_{i^\\star i^\\star}$ of the planted index $i^\\star$ rounded to six decimal places,\n  the $0$-based index of the observation with maximum leverage,\n  the Cook’s distance $D_{i^\\star}$ of the planted index $i^\\star$ rounded to six decimal places,\n  the $0$-based index of the observation with maximum Cook’s distance,\n  a boolean indicating whether the planted index is high leverage under the rule-of-thumb (use $True$ or $False$),\n  and a boolean indicating whether the planted index is influential under the rule-of-thumb (use $True$ or $False$)\n  $]$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case’s six outputs enclosed in its own square brackets; for example, $[ [\\dots], [\\dots], [\\dots] ]$, without any additional spaces or text.\n\nTest suite (each case plants one high-leverage row in $X$; indices are $0$-based, and all numbers are real-valued):\n- Case $1$ (single predictor, moderate residual at the planted point):\n  - $n = 10$, planted index $i^\\star = 9$.\n  - Predictor values for the single column of $X$ (before adding the intercept): $[-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]$.\n  - Response $y$: $[-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 21.2]$.\n- Case $2$ (single predictor, very large residual at the planted point):\n  - $n = 10$, planted index $i^\\star = 9$.\n  - Predictor values: $[-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]$.\n  - Response $y$: $[-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 36.0]$.\n- Case $3$ (two predictors, planted point extreme in both coordinates):\n  - $n = 12$, planted index $i^\\star = 11$.\n  - Predictor values for the two columns of $X$ (before adding the intercept):\n    - First predictor: $[-1.0, 0.2, -0.3, 0.5, -0.1, 0.4, -0.7, 0.0, 0.3, -0.2, 0.6, 8.0]$.\n    - Second predictor: $[0.5, -0.1, 0.2, -0.4, 0.3, -0.2, 0.1, -0.5, 0.4, -0.3, 0.0, -9.0]$.\n  - Response $y$: $[-1.5, 0.95, -0.17, 1.68, 0.04, 1.34, -0.64, 0.97, 0.57, 0.48, 1.4, 21.55]$.\n- Case $4$ (single predictor, planted point extremely far from the rest to push leverage near the upper boundary $1$):\n  - $n = 8$, planted index $i^\\star = 7$.\n  - Predictor values: $[-0.5, 0.2, -0.1, 0.3, -0.2, 0.1, 0.0, 50.0]$.\n  - Response $y$: $[1.51, 2.18, 1.93, 2.29, 1.8, 2.12, 1.99, 52.01]$.\n- Case $5$ (single predictor, moderate high leverage with varied noise):\n  - $n = 15$, planted index $i^\\star = 14$.\n  - Predictor values: $[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 0.1, -0.3, 0.5, 6.0]$.\n  - Response $y$: $[-0.3, -0.5, 0.0, -0.6, -0.1, 0.5, -0.2, 0.6, 0.1, 0.5, 0.0, 0.25, -0.45, 0.85, 2.0]$.\n\nImplementation notes:\n- In all cases, augment $X$ with an intercept column of ones.\n- Use $0$-based indexing for all indices.\n- Round the reported leverage and Cook’s distance for the planted index to six decimal places.\n- Use $True$ or $False$ for booleans.\n- The final output must be a single line in the exact format: a top-level bracketed list of five bracketed sublists, each sublist containing the six requested values for its test case, with commas and no spaces anywhere in the line.", "solution": "The user-provided problem has been rigorously validated and is determined to be a well-posed, scientifically grounded task within the domain of statistical regression diagnostics. The problem statement is complete, objective, and internally consistent, permitting a unique and meaningful solution.\n\nThe core of this problem is the computation and interpretation of two key regression diagnostics: leverage and Cook's distance. These metrics are used to identify observations that have an unusually large impact on the results of an ordinary least squares (OLS) regression analysis.\n\n### Theoretical Framework\n\nThe standard multiple linear regression model is given by:\n$$\ny = X\\beta + \\epsilon\n$$\nwhere $y$ is an $n \\times 1$ vector of observations, $X$ is an $n \\times p$ design matrix of full column rank (containing predictors and typically an intercept column), $\\beta$ is a $p \\times 1$ vector of unknown coefficients, and $\\epsilon$ is an $n \\times 1$ vector of uncorrelated error terms with mean $0$ and constant variance $\\sigma^2$.\n\nThe OLS estimator for $\\beta$ is the vector $\\hat{\\beta}$ that minimizes the sum of squared residuals, $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$. This estimator is given by the solution to the normal equations:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n\nThe fitted values are the orthogonal projection of $y$ onto the column space of $X$:\n$$\n\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y = H y\n$$\nThe matrix $H = X(X^T X)^{-1} X^T$ is known as the projection matrix or \"hat matrix\" because it maps $y$ to $\\hat{y}$.\n\n**Leverage**\nThe leverage of the $i$-th observation, denoted $h_{ii}$, is the $i$-th diagonal element of the hat matrix $H$. It quantifies the influence of the observed response $y_i$ on its own fitted value $\\hat{y}_i$, since $\\hat{y}_i = \\sum_{j=1}^n H_{ij} y_j = h_{ii}y_i + \\sum_{j \\neq i} H_{ij} y_j$. The leverage value $h_{ii}$ is determined solely by the design matrix $X$ and represents the \"potential\" for an observation to be influential due to its position in the predictor space. Points that are far from the center of the predictor data cloud have high leverage. Leverages are bounded by $0 \\le h_{ii} \\le 1$, and their sum is equal to the number of parameters, $\\sum_{i=1}^n h_{ii} = p$. A common rule-of-thumb flags an observation as a high-leverage point if its leverage value exceeds twice the average leverage, i.e., $h_{ii} > 2p/n$.\n\n**Cook's Distance**\nWhile leverage measures potential influence, Cook's distance, $D_i$, measures the actual overall change in the model's fitted values when the $i$-th observation is removed. It is a combined measure of the observation's leverage and its residual size. An observation with high leverage may or may not be influential; it becomes influential if its corresponding residual is also large. Cook's distance is defined as:\n$$\nD_i = \\frac{ \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 }{p \\cdot \\hat{\\sigma}^2}\n$$\nwhere $\\hat{y}_{j(i)}$ is the fitted value for observation $j$ from a model fit without observation $i$, and $\\hat{\\sigma}^2$ is the mean squared error from the model fit with all observations. A more convenient computational formula is:\n$$\nD_i = \\frac{e_i^2}{p \\cdot \\hat{\\sigma}^2} \\left[ \\frac{h_{ii}}{(1 - h_{ii})^2} \\right]\n$$\nwhere $e_i = y_i - \\hat{y}_i$ is the $i$-th residual and $\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_{j=1}^n e_j^2$ is the mean squared error (or residual variance). A common rule-of-thumb suggests that an observation is influential if $D_i > 4/n$.\n\n### Algorithmic Procedure\n\nFor each test case, the following procedure is executed to compute the required six quantities:\n1.  **Construct Design Matrix**: The provided predictor values are formed into a matrix, which is then augmented by prepending a column of ones to account for the model intercept. This results in the final $n \\times p$ design matrix $X$.\n2.  **Fit OLS Model**: The OLS coefficient estimates $\\hat{\\beta}$ are computed by solving the linear system $X^T X \\hat{\\beta} = X^T y$. This is numerically more stable than explicitly computing the inverse of $X^T X$.\n3.  **Compute Leverage**: The hat matrix $H$ is computed as $H = X (X^T X)^{-1} X^T$. The leverage values $h_{ii}$ are then extracted from the diagonal of $H$.\n4.  **Compute Residuals and MSE**: The fitted values $\\hat{y} = X\\hat{\\beta}$ are calculated, followed by the residuals $e = y - \\hat{y}$. The mean squared error is then computed as $\\hat{\\sigma}^2 = \\frac{e^T e}{n-p}$.\n5.  **Compute Cook's Distances**: Using the leverages $h_{ii}$, residuals $e_i$, number of parameters $p$, and MSE $\\hat{\\sigma}^2$, the Cook's distances $D_i$ for all observations are calculated using the formula above.\n6.  **Extract and Report Results**:\n    - The leverage $h_{i^\\star i^\\star}$ and Cook's distance $D_{i^\\star}$ for the specified planted point $i^\\star$ are retrieved.\n    - The indices of the observations with the maximum leverage and maximum Cook's distance are found.\n    - The planted point's leverage $h_{i^\\star i^\\star}$ is compared against the threshold $2p/n$ to determine if it is a high-leverage point.\n    - The planted point's Cook's distance $D_{i^\\star}$ is compared against the threshold $4/n$ to determine if it is an influential point.\n    - These six values are compiled into a list for the test case.\n\nThis procedure is repeated for all test cases, and the final results are aggregated into a single formatted string as specified.", "answer": "```python\nimport numpy as np\n\ndef analyze_case(X_raw: np.ndarray, y: np.ndarray, istar: int):\n    \"\"\"\n    Performs leverage and influence analysis for a single OLS regression case.\n\n    Args:\n        X_raw: The raw predictor matrix (without intercept).\n        y: The response vector.\n        istar: The 0-based index of the planted point.\n\n    Returns:\n        A list containing the six required diagnostic quantities.\n    \"\"\"\n    n = len(y)\n    \n    # Ensure X_raw is a 2D array\n    if X_raw.ndim == 1:\n        X_raw = X_raw.reshape(-1, 1)\n\n    # 1. Construct Design Matrix with an intercept\n    X = np.c_[np.ones(n), X_raw]\n    n, p = X.shape\n\n    # 2. Fit OLS Model\n    try:\n        XtX = X.T @ X\n        Xty = X.T @ y\n        beta_hat = np.linalg.solve(XtX, Xty)\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n         # Problem specification guarantees full column rank, so this is a fallback.\n        pinv_X = np.linalg.pinv(X)\n        beta_hat = pinv_X @ y\n        XtX_inv = np.linalg.pinv(XtX)\n\n    # 3. Compute Leverage\n    H = X @ XtX_inv @ X.T\n    h = np.diag(H)\n\n    # 4. Compute Residuals and MSE\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    rss = e.T @ e\n    mse = rss / (n - p)\n\n    # 5. Compute Cook's Distances\n    # Using np.errstate to handle potential division by zero if h_ii = 1.\n    # In such a case, the residual e_i must be 0, leading to a 0/0 form.\n    # The calculated value would be NaN, but this doesn't occur in the test data.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        D = (e**2 / (p * mse)) * (h / (1 - h)**2)\n    D = np.nan_to_num(D, nan=0.0) # Replace any NaN with 0 for argmax consistency\n\n    # 6. Extract and Report Results\n    h_istar = h[istar]\n    max_h_idx = int(np.argmax(h))\n    \n    D_istar = D[istar]\n    max_D_idx = int(np.argmax(D))\n    \n    is_high_leverage = h_istar > (2 * p / n)\n    is_influential = D_istar > (4 / n)\n    \n    return [\n        round(h_istar, 6),\n        max_h_idx,\n        round(D_istar, 6),\n        max_D_idx,\n        is_high_leverage,\n        is_influential,\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"X_raw\": np.array([-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]),\n            \"y\": np.array([-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 21.2]),\n            \"istar\": 9\n        },\n        # Case 2\n        {\n            \"X_raw\": np.array([-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]),\n            \"y\": np.array([-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 36.0]),\n            \"istar\": 9\n        },\n        # Case 3\n        {\n            \"X_raw\": np.c_[\n                [-1.0, 0.2, -0.3, 0.5, -0.1, 0.4, -0.7, 0.0, 0.3, -0.2, 0.6, 8.0],\n                [0.5, -0.1, 0.2, -0.4, 0.3, -0.2, 0.1, -0.5, 0.4, -0.3, 0.0, -9.0]\n            ],\n            \"y\": np.array([-1.5, 0.95, -0.17, 1.68, 0.04, 1.34, -0.64, 0.97, 0.57, 0.48, 1.4, 21.55]),\n            \"istar\": 11\n        },\n        # Case 4\n        {\n            \"X_raw\": np.array([-0.5, 0.2, -0.1, 0.3, -0.2, 0.1, 0.0, 50.0]),\n            \"y\": np.array([1.51, 2.18, 1.93, 2.29, 1.8, 2.12, 1.99, 52.01]),\n            \"istar\": 7\n        },\n        # Case 5\n        {\n            \"X_raw\": np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 0.1, -0.3, 0.5, 6.0]),\n            \"y\": np.array([-0.3, -0.5, 0.0, -0.6, -0.1, 0.5, -0.2, 0.6, 0.1, 0.5, 0.0, 0.25, -0.45, 0.85, 2.0]),\n            \"istar\": 14\n        }\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case[\"X_raw\"], case[\"y\"], case[\"istar\"])\n        all_results.append(result)\n\n    # Format the final output string exactly as required (no spaces)\n    result_strings = []\n    for res in all_results:\n        # Format: [leverage,max_lev_idx,cook_dist,max_cook_idx,is_high_lev,is_influential]\n        # Example: [0.97278,9,0.000955,2,True,False]\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{str(res[4])},{str(res[5])}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3099870"}, {"introduction": "The assumption of constant error variance, or homoscedasticity, is often violated in practice, which can invalidate the standard errors from Ordinary Least Squares (OLS). This exercise demonstrates how to address heteroskedasticity using Weighted Least Squares (WLS) and introduces the concept of heteroskedasticity-consistent \"sandwich\" estimators for standard errors. By comparing these methods, you will learn how to obtain efficient estimates and reliable inferences even when the classical assumptions do not hold. [@problem_id:3099884]", "problem": "Consider a linear regression model with sampling or precision weights, expressed as $Y = X\\beta + \\epsilon$ where $E[\\epsilon \\mid X] = 0$. The goal is to analyze how incorporating observation-specific nonnegative weights $w_i$ into estimation affects the assumptions of independence and homoscedasticity, and how to compute both model-based and robust variances of the estimator in a weighted context.\n\nStarting from the definition of least squares as minimizing a sum of squared errors, define Weighted Least Squares (WLS) as the minimization of the weighted sum of squared residuals. In addition, consider a heteroskedasticity-consistent covariance estimator (often called a \"sandwich\" estimator) that does not rely on a correct specification of the variance model. Your program must:\n- Derive the WLS estimator of $\\beta$ from first principles by minimizing the weighted sum of squared residuals and solving the resulting normal equations.\n- Compute the model-based covariance of the WLS estimator under the classical assumption that after weighting the errors are conditionally homoscedastic with variance proportional to $W^{-1}$, where $W$ is the diagonal matrix of weights.\n- Compute a \"sandwich\" covariance estimator for the WLS estimator that is consistent under general forms of heteroskedasticity of unknown form.\n- Quantify the extent to which the weighting renders residuals approximately homoscedastic by computing a dispersion metric of standardized residuals. Let $r_i = y_i - x_i^\\top \\hat{\\beta}$ denote residuals and $z_i = \\sqrt{w_i}\\, r_i$ denote standardized residuals. Define the dispersion metric as $D = \\mathrm{std}(|z|)/\\mathrm{mean}(|z|)$, where $\\mathrm{std}$ is the sample standard deviation and $\\mathrm{mean}$ is the sample mean. Smaller values of $D$ indicate closer adherence to homoscedasticity after weighting.\n\nUse the following test suite, each case providing $(X, y, w)$ with $n = 6$ observations and $p = 2$ predictors (intercept and slope). In all cases, the design matrix $X$ has rows $[1, x_i]$ with $x_i \\in \\{\\,0, 1, 2, 3, 4, 5\\,\\}$, that is $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, $x_3 = 3$, $x_4 = 4$, $x_5 = 5$.\n\nCase A (baseline, homoscedastic and independent errors, equal weights):\n- True coefficients: $\\beta = [\\,1,\\,2\\,]$.\n- Errors: $\\epsilon = [\\,0.2,\\,-0.1,\\,0.0,\\,0.1,\\,-0.2,\\,0.0\\,]$.\n- Response: $y_i = 1 + 2\\,x_i + \\epsilon_i$ for $i \\in \\{0,1,2,3,4,5\\}$.\n- Weights: $w = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$.\n\nCase B (heteroskedastic errors, weights equal inverse variances, independence holds):\n- True coefficients: $\\beta = [\\,1,\\,2\\,]$.\n- Standardized error pattern: $z = [\\,0.5,\\,-0.5,\\,0.5,\\,-0.5,\\,0.5,\\,-0.5\\,]$.\n- Weights: $w = [\\,4.0,\\,4.0,\\,1.0,\\,1.0,\\,0.25,\\,0.25\\,]$.\n- Errors: $\\epsilon_i = z_i / \\sqrt{w_i}$ for $i \\in \\{0,1,2,3,4,5\\}$.\n- Response: $y_i = 1 + 2\\,x_i + \\epsilon_i$.\n\nCase C (same data as Case B, mis-specified weights, independence holds):\n- Use the same $X$ and $y$ as in Case B.\n- Weights: $w = [\\,0.25,\\,0.25,\\,1.0,\\,1.0,\\,4.0,\\,4.0\\,]$.\n\nCase D (edge case with extreme weights and strong leverage):\n- True coefficients: $\\beta = [\\,0,\\,-1\\,]$.\n- Standardized error pattern: $z = [\\,1,\\,-1,\\,1,\\,-1,\\,1,\\,-1\\,]$.\n- Weights: $w = [\\,1000.0,\\,0.001,\\,10.0,\\,0.1,\\,50.0,\\,0.5\\,]$.\n- Errors: $\\epsilon_i = z_i / \\sqrt{w_i}$ for $i \\in \\{0,1,2,3,4,5\\}$.\n- Response: $y_i = 0 - 1\\,x_i + \\epsilon_i = -x_i + \\epsilon_i$.\n\nFor each case, compute:\n1. The WLS estimator $\\hat{\\beta}$.\n2. The model-based standard errors for $\\hat{\\beta}$ under the assumption $\\mathrm{Var}(\\epsilon \\mid X) = \\sigma^2 W^{-1}$ estimated by $\\hat{\\sigma}^2$ from the weighted residual sum of squares.\n3. The sandwich (heteroskedasticity-consistent) standard errors for $\\hat{\\beta}$.\n4. The dispersion metric $D$ of the standardized residuals.\n\nFinal output format:\n- Your program should produce a single line of output containing, for each case, a list of seven floats in the order $[\\,\\hat{\\beta}_0,\\,\\hat{\\beta}_1,\\,\\mathrm{se}_{\\mathrm{model},0},\\,\\mathrm{se}_{\\mathrm{model},1},\\,\\mathrm{se}_{\\mathrm{sand},0},\\,\\mathrm{se}_{\\mathrm{sand},1},\\,D\\,]$.\n- Aggregate the four case results into a single list, printed on one line as a comma-separated list enclosed in square brackets, for example $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$.\n\nYour program must be a complete, runnable program with no external input. All calculations must be deterministic and follow directly from the definitions and assumptions described above. Express all numerical results as floats in the final output list.", "solution": "The problem requires the derivation and application of the Weighted Least Squares (WLS) estimator, along with two types of covariance estimators for its coefficients: a model-based estimator and a heteroskedasticity-consistent (sandwich) estimator. We will first derive the necessary formulas from first principles and then outline the computational steps for the provided test cases.\n\nThe standard linear model is given by $Y = X\\beta + \\epsilon$, where $Y$ is an $n \\times 1$ vector of observations, $X$ is an $n \\times p$ design matrix of rank $p$, $\\beta$ is a $p \\times 1$ vector of unknown coefficients, and $\\epsilon$ is an $n \\times 1$ vector of unobserved errors with conditional mean $E[\\epsilon \\mid X] = 0$. In Ordinary Least Squares (OLS), we assume the errors are uncorrelated and have constant variance, i.e., $\\mathrm{Var}(\\epsilon \\mid X) = \\sigma^2 I_n$, where $I_n$ is the $n \\times n$ identity matrix.\n\nWLS addresses the case of heteroskedasticity, where the error variances are not equal: $\\mathrm{Var}(\\epsilon_i \\mid X) = \\sigma_i^2$. We introduce a set of known, positive weights $w_i$, typically chosen to be inversely proportional to the error variances, $w_i \\propto 1/\\sigma_i^2$.\n\nThe WLS estimator is found by minimizing the weighted sum of squared residuals (WRSS):\n$$\nS(\\beta) = \\sum_{i=1}^{n} w_i (y_i - x_i^\\top \\beta)^2\n$$\nwhere $y_i$ is the $i$-th observation, and $x_i^\\top$ is the $i$-th row of the matrix $X$. Let $W$ be the $n \\times n$ diagonal matrix with the weights $w_i$ on the diagonal. The objective function can be written in matrix form as:\n$$\nS(\\beta) = (Y - X\\beta)^\\top W (Y - X\\beta)\n$$\nTo find the estimator $\\hat{\\beta}$ that minimizes $S(\\beta)$, we differentiate $S(\\beta)$ with respect to $\\beta$ and set the result to zero.\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (Y^\\top W Y - 2\\beta^\\top X^\\top W Y + \\beta^\\top X^\\top W X \\beta) = -2X^\\top W Y + 2X^\\top W X \\beta\n$$\nSetting the derivative to zero yields the WLS normal equations:\n$$\nX^\\top W X \\hat{\\beta} = X^\\top W Y\n$$\nAssuming the matrix $X^\\top W X$ is invertible (which holds if $X$ has full column rank and all $w_i > 0$), we can solve for the WLS estimator $\\hat{\\beta}$:\n$$\n\\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W Y\n$$\nThis estimator is linear in $Y$ and, under the condition $E[\\epsilon \\mid X] = 0$, it is unbiased:\n$E[\\hat{\\beta} \\mid X] = E[(X^\\top W X)^{-1} X^\\top W (X\\beta + \\epsilon) \\mid X] = \\beta + (X^\\top W X)^{-1} X^\\top W E[\\epsilon \\mid X] = \\beta$.\n\nNext, we derive the covariance matrix of $\\hat{\\beta}$. The deviation of the estimator from the true value is:\n$$\n\\hat{\\beta} - \\beta = (X^\\top W X)^{-1} X^\\top W \\epsilon\n$$\nThe covariance matrix of $\\hat{\\beta}$ is then:\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = E[(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)^\\top \\mid X] = (X^\\top W X)^{-1} X^\\top W E[\\epsilon\\epsilon^\\top \\mid X] W X (X^\\top W X)^{-1}\n$$\nLet $\\Omega = E[\\epsilon\\epsilon^\\top \\mid X]$ be the true covariance matrix of the errors. The general form of the covariance matrix for $\\hat{\\beta}$ is:\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = (X^\\top W X)^{-1} (X^\\top W \\Omega W X) (X^\\top W X)^{-1}\n$$\n\nThe calculation of standard errors depends on the assumptions made about $\\Omega$.\n\n**1. Model-Based Covariance Estimator**\nThis approach assumes that the weights are correctly specified, meaning they are inversely proportional to the true error variances. This implies that $\\mathrm{Var}(\\epsilon_i \\mid X) = \\sigma^2/w_i$ for some unknown constant $\\sigma^2$. In matrix form, $\\Omega = \\sigma^2 W^{-1}$. Substituting this into the general covariance formula simplifies it significantly:\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = (X^\\top W X)^{-1} X^\\top W (\\sigma^2 W^{-1}) W X (X^\\top W X)^{-1} = \\sigma^2 (X^\\top W X)^{-1}\n$$\nTo make this operational, we need to estimate $\\sigma^2$. Under the assumption $\\Omega = \\sigma^2 W^{-1}$, the transformed errors $\\epsilon^* = W^{1/2}\\epsilon$ are homoscedastic with variance $\\sigma^2 I_n$. An unbiased estimator for $\\sigma^2$ is derived from the weighted residual sum of squares of the regression, divided by the degrees of freedom, $n-p$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} w_i(y_i - x_i^\\top \\hat{\\beta})^2 = \\frac{(Y-X\\hat{\\beta})^\\top W (Y-X\\hat{\\beta})}{n-p}\n$$\nThe estimated model-based covariance matrix is therefore:\n$$\n\\widehat{\\mathrm{Cov}}_{\\mathrm{model}}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X^\\top W X)^{-1}\n$$\nThe model-based standard errors for the coefficients $\\hat{\\beta}_j$ are the square roots of the diagonal elements of this matrix. This estimator is only valid if the assumption $\\Omega = \\sigma^2 W^{-1}$ holds.\n\n**2. Sandwich (Heteroskedasticity-Consistent) Covariance Estimator**\nThis estimator is robust to misspecification of the weight matrix $W$. It does not assume $\\Omega = \\sigma^2 W^{-1}$. Instead, it estimates the middle part (the \"meat\") of the general covariance formula directly. We typically assume the errors are uncorrelated but possibly heteroskedastic, so $\\Omega = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, ..., \\sigma_n^2)$. The \"meat\" term is $M = X^\\top W \\Omega W X = \\sum_{i=1}^n w_i^2 \\sigma_i^2 x_i x_i^\\top$.\nTo estimate $M$, we replace the unknown variances $\\sigma_i^2$ with their empirical estimates, the squared residuals $r_i^2 = (y_i - x_i^\\top \\hat{\\beta})^2$. The estimated meat matrix is:\n$$\n\\hat{M} = \\sum_{i=1}^{n} w_i^2 r_i^2 x_i x_i^\\top = X^\\top W \\hat{\\Omega} W X\n$$\nwhere $\\hat{\\Omega} = \\mathrm{diag}(r_1^2, r_2^2, ..., r_n^2)$. The sandwich covariance estimator is then:\n$$\n\\widehat{\\mathrm{Cov}}_{\\mathrm{sand}}(\\hat{\\beta}) = (X^\\top W X)^{-1} \\hat{M} (X^\\top W X)^{-1}\n$$\nThe sandwich standard errors are the square roots of the diagonal elements of this matrix. They are \"consistent\" in the sense that they provide a valid approximation of the true sampling variability of $\\hat{\\beta}$ even when the weights $w_i$ do not correctly model the heteroskedasticity. When the weights are correctly specified (as in Case B), the model-based and sandwich estimators should yield similar results. When they are misspecified (as in Case C), they are expected to diverge.\n\n**3. Dispersion Metric $D$**\nTo assess how well the weighting scheme achieves homoscedasticity, we examine the standardized residuals. The raw residuals are $r_i = y_i - x_i^\\top \\hat{\\beta}$. The standardized residuals, which correspond to the residuals of the transformed OLS problem, are $z_i = \\sqrt{w_i} r_i$. If the weighting is successful, the $z_i$ should have roughly constant variance. The problem defines a dispersion metric $D$ to quantify this:\n$$\nD = \\frac{\\mathrm{std}(|z|)}{\\mathrm{mean}(|z|)}\n$$\nwhere $|z|$ is the vector of absolute values of the standardized residuals, $\\mathrm{std}$ is the sample standard deviation (using a denominator of $n-1$), and $\\mathrm{mean}$ is the sample mean. A smaller value of $D$ indicates that the absolute standardized residuals have low variability relative to their average magnitude, suggesting a closer adherence to homoscedasticity after weighting. For example, if all $|z_i|$ were identical, $\\mathrm{std}(|z|)$ would be $0$, and thus $D$ would be $0$, indicating perfect homoscedasticity in magnitude.", "answer": "```python\nimport numpy as np\n\ndef solve_case(X, y, w):\n    \"\"\"\n    Performs WLS estimation and computes required statistics for a single case.\n    \n    Args:\n        X (np.ndarray): Design matrix (n x p).\n        y (np.ndarray): Response vector (n x 1).\n        w (np.ndarray): Weight vector (n x 1).\n\n    Returns:\n        list: A list of 7 floats containing beta_hat, model-based SEs, \n              sandwich SEs, and the dispersion metric D.\n    \"\"\"\n    n, p = X.shape\n    W = np.diag(w)\n\n    # 1. Compute WLS estimator beta_hat\n    # beta_hat = (X^T W X)^-1 X^T W y\n    XT_W = X.T @ W\n    XT_W_X = XT_W @ X\n    inv_XT_W_X = np.linalg.inv(XT_W_X)\n    XT_W_y = XT_W @ y\n    beta_hat = inv_XT_W_X @ XT_W_y\n\n    # Compute residuals\n    residuals = y - X @ beta_hat\n    \n    # 2. Compute model-based standard errors\n    # cov_model = sigma_hat^2 * (X^T W X)^-1\n    # sigma_hat^2 = sum(w_i * r_i^2) / (n - p)\n    wrss = np.sum(w * residuals**2)\n    sigma_sq_hat = wrss / (n - p)\n    cov_model = sigma_sq_hat * inv_XT_W_X\n    se_model = np.sqrt(np.diag(cov_model))\n    \n    # 3. Compute sandwich (heteroskedasticity-consistent) standard errors\n    # cov_sand = (X^T W X)^-1 * (X^T W Omega_hat W X) * (X^T W X)^-1\n    # where Omega_hat = diag(r_i^2)\n    # The \"meat\" is M_hat = sum(w_i^2 * r_i^2 * x_i * x_i^T)\n    meat = np.zeros((p, p))\n    for i in range(n):\n        xi = X[i, :].reshape(p, 1)\n        meat += (w[i]**2) * (residuals[i]**2) * (xi @ xi.T)\n    \n    cov_sand = inv_XT_W_X @ meat @ inv_XT_W_X\n    se_sand = np.sqrt(np.diag(cov_sand))\n    \n    # 4. Compute dispersion metric D\n    # z_i = sqrt(w_i) * r_i\n    # D = std(|z|) / mean(|z|)\n    z = np.sqrt(w) * residuals\n    abs_z = np.abs(z)\n    \n    # Check for mean(|z|) == 0 to avoid division by zero\n    mean_abs_z = np.mean(abs_z)\n    if mean_abs_z == 0:\n        D = 0.0 # If all residuals are zero, variance is zero, dispersion is zero.\n    else:\n        # Use ddof=1 for sample standard deviation\n        std_abs_z = np.std(abs_z, ddof=1)\n        D = std_abs_z / mean_abs_z\n\n    return [\n        beta_hat[0], beta_hat[1],\n        se_model[0], se_model[1],\n        se_sand[0], se_sand[1],\n        D\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run calculations, and print results.\n    \"\"\"\n    x_i = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    X = np.vstack([np.ones_like(x_i), x_i]).T\n\n    # Case A\n    beta_a = np.array([1.0, 2.0])\n    eps_a = np.array([0.2, -0.1, 0.0, 0.1, -0.2, 0.0])\n    y_a = X @ beta_a + eps_a\n    w_a = np.ones(6)\n\n    # Case B\n    beta_b = np.array([1.0, 2.0])\n    z_b = np.array([0.5, -0.5, 0.5, -0.5, 0.5, -0.5])\n    w_b = np.array([4.0, 4.0, 1.0, 1.0, 0.25, 0.25])\n    eps_b = z_b / np.sqrt(w_b)\n    y_b = X @ beta_b + eps_b\n\n    # Case C\n    y_c = y_b  # same data as Case B\n    w_c = np.array([0.25, 0.25, 1.0, 1.0, 4.0, 4.0])\n\n    # Case D\n    beta_d = np.array([0.0, -1.0])\n    z_d = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0])\n    w_d = np.array([1000.0, 0.001, 10.0, 0.1, 50.0, 0.5])\n    eps_d = z_d / np.sqrt(w_d)\n    y_d = X @ beta_d + eps_d\n\n    test_cases = [\n        (X, y_a, w_a),\n        (X, y_b, w_b),\n        (X, y_c, w_c),\n        (X, y_d, w_d),\n    ]\n\n    all_results = []\n    for X_case, y_case, w_case in test_cases:\n        result = solve_case(X_case, y_case, w_case)\n        all_results.append(result)\n\n    # Format output as a string representing a list of lists.\n    # e.g., \"[[r1, r2, ...], [r1, r2, ...]]\"\n    case_strings = []\n    for case_result in all_results:\n        case_strings.append(f\"[{','.join(f'{x:.7f}' for x in case_result)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3099884"}]}