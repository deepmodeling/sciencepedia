{"hands_on_practices": [{"introduction": "To build a strong intuition for standardized residuals, it's helpful to start with the simplest possible regression model: one with only an intercept. This exercise [@problem_id:3176927] bridges the gap between the familiar concept of a z-score and the new concept of a standardized residual. By working through this special case, you will see how the regression framework naturally generalizes the idea of standardizing a data point.", "problem": "Consider a univariate linear regression with an intercept only: for observations indexed by $i \\in \\{1,2,\\ldots,n\\}$, the model is $y_{i} = \\beta_{0} + \\varepsilon_{i}$ with $\\varepsilon_{i}$ independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$. Let the design matrix be $X = \\mathbf{1}$, the $n \\times 1$ vector of ones. Let $\\hat{y}_{i}$ denote the fitted values, $e_{i} = y_{i} - \\hat{y}_{i}$ the residuals, and $H$ the hat matrix with entries $h_{ij}$. Define the residual sum of squares $\\mathrm{RSS} = \\sum_{i=1}^{n} e_{i}^{2}$ and the residual standard error $s = \\sqrt{\\mathrm{RSS}/(n-p)}$ with $p=1$. The standardized residual is defined as $r_{i} = \\dfrac{e_{i}}{s \\sqrt{1 - h_{ii}}}$. Let $\\bar{y} = \\dfrac{1}{n}\\sum_{i=1}^{n} y_{i}$ and the sample standard deviation $s_{y} = \\sqrt{\\dfrac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}$. Define the $z$-score $z_{i} = \\dfrac{y_{i} - \\bar{y}}{s_{y}}$.\n\nStarting from the standard definitions of the hat matrix, residuals, and variance estimators in the linear model, and without assuming any result specific to the intercept-only case in advance, derive $h_{ii}$, show the relationship between $s$ and $s_{y}$, and obtain a closed-form expression for the standardized residual $r_{i}$ written purely in terms of $z_{i}$ and $n$. Provide your final answer as a single closed-form analytic expression for $r_{i}$ in terms of $z_{i}$ and $n$ only. No numerical rounding is required.", "solution": "The objective is to derive a closed-form expression for the standardized residual $r_{i}$ in terms of the $z$-score $z_{i}$ and the sample size $n$ for a univariate linear regression model with only an intercept. The derivation proceeds by first principles, as required.\n\nThe model is specified as $y_{i} = \\beta_{0} + \\varepsilon_{i}$ for $i \\in \\{1, 2, \\ldots, n\\}$, where the errors $\\varepsilon_{i}$ are independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$. The design matrix for this model is $X = \\mathbf{1}$, which is an $n \\times 1$ column vector of ones.\n\nFirst, we determine the ordinary least squares (OLS) estimate of the parameter $\\beta_{0}$, denoted as $\\hat{\\beta}_{0}$. The general formula for the OLS estimate vector is $\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y$. We compute the components for our specific model.\nThe term $X^{T}X$ is:\n$$X^{T}X = \\mathbf{1}^{T}\\mathbf{1} = \\sum_{i=1}^{n} 1^{2} = n$$\nThe inverse is $(X^{T}X)^{-1} = n^{-1} = \\frac{1}{n}$.\nThe term $X^{T}y$ is:\n$$X^{T}y = \\mathbf{1}^{T}y = \\sum_{i=1}^{n} y_{i}$$\nBy definition, the sample mean is $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, which implies $\\sum_{i=1}^{n} y_{i} = n\\bar{y}$.\nSubstituting these into the OLS formula yields the scalar estimate $\\hat{\\beta}_{0}$:\n$$\\hat{\\beta}_{0} = \\left(\\frac{1}{n}\\right)(n\\bar{y}) = \\bar{y}$$\n\nNext, we calculate the fitted values, $\\hat{y}_{i}$. The vector of fitted values is $\\hat{y} = X\\hat{\\beta}_{0}$.\n$$\\hat{y} = \\mathbf{1}\\bar{y}$$\nThis implies that each individual fitted value is $\\hat{y}_{i} = \\bar{y}$ for all $i \\in \\{1, \\ldots, n\\}$.\n\nThe hat matrix, $H$, is defined as $H = X(X^{T}X)^{-1}X^{T}$. Using our previously computed terms:\n$$H = \\mathbf{1}\\left(\\frac{1}{n}\\right)\\mathbf{1}^{T} = \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{T}$$\nThe outer product $\\mathbf{1}\\mathbf{1}^{T}$ results in an $n \\times n$ matrix where every entry is $1$. Therefore, the hat matrix $H$ is an $n \\times n$ matrix where every entry $h_{ij}$ is equal to $\\frac{1}{n}$. The diagonal elements of the hat matrix, which are required for the standardized residual calculation, are thus $h_{ii} = \\frac{1}{n}$ for all $i$.\n\nThe residuals are defined by $e_{i} = y_{i} - \\hat{y}_{i}$. Substituting the expression for the fitted values, we obtain:\n$$e_{i} = y_{i} - \\bar{y}$$\n\nThe Residual Sum of Squares (RSS) is the sum of the squared residuals:\n$$\\mathrm{RSS} = \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}$$\n\nThe residual standard error, $s$, is defined as $s = \\sqrt{\\mathrm{RSS}/(n-p)}$, where $p$ is the number of estimated parameters in the model. For the intercept-only model, we estimate one parameter, $\\beta_0$, so $p=1$.\n$$s = \\sqrt{\\frac{\\mathrm{RSS}}{n-1}} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}{n-1}}$$\n\nThe problem provides the definition of the sample standard deviation of $y$ as $s_{y} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}$. By direct comparison of the expressions for $s$ and $s_{y}$, we establish the crucial relationship $s = s_{y}$.\n\nWe are now prepared to derive the expression for the standardized residual, $r_{i}$. The definition is given as $r_{i} = \\dfrac{e_{i}}{s \\sqrt{1 - h_{ii}}}$.\nWe substitute the expressions derived above: $e_{i} = y_{i} - \\bar{y}$, $s = s_{y}$, and $h_{ii} = \\frac{1}{n}$.\n$$r_{i} = \\frac{y_{i} - \\bar{y}}{s_{y} \\sqrt{1 - \\frac{1}{n}}}$$\nSimplifying the term in the denominator:\n$$r_{i} = \\frac{y_{i} - \\bar{y}}{s_{y} \\sqrt{\\frac{n-1}{n}}}$$\n\nThe final step is to express this result in terms of the given $z$-score, which is defined as $z_{i} = \\dfrac{y_{i} - \\bar{y}}{s_{y}}$. We substitute $z_i$ into our expression for $r_i$:\n$$r_{i} = \\frac{z_{i}}{\\sqrt{\\frac{n-1}{n}}}$$\nRearranging this expression gives the final closed-form relationship between the standardized residual $r_i$ and the $z$-score $z_i$:\n$$r_{i} = z_{i} \\sqrt{\\frac{n}{n-1}}$$\nThis expression depends only on $z_{i}$ and $n$, as required.", "answer": "$$\\boxed{z_{i} \\sqrt{\\frac{n}{n-1}}}$$", "id": "3176927"}, {"introduction": "While standardized residuals account for the varying variance of raw residuals, they can be influenced by the very outliers they are meant to detect. This hands-on coding exercise [@problem_id:3176898] demonstrates this limitation by constructing a dataset with two distinct types of influential points. You will implement both standardized and studentized residuals from scratch to see firsthand how the latter provides a more robust tool for outlier detection.", "problem": "Consider the classical linear regression model $y = X\\beta + \\varepsilon$ where $X \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix (with a column of ones to model an intercept), $\\beta \\in \\mathbb{R}^p$ is the parameter vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a random error vector assumed to satisfy $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$. Use the Ordinary Least Squares (OLS) estimator $\\hat{\\beta}$ and the associated residual vector $e = y - X\\hat{\\beta}$, the hat matrix $H = X(X^\\top X)^{-1}X^\\top$ with diagonal entries $h_{ii}$, and degrees of freedom $n - p$ to construct and analyze standardized residuals and studentized residuals from first principles.\n\nYou must write a complete program that:\n- Constructs several datasets, each consisting of a base set of points lying close to a true line together with two special points:\n  1. One point with large leverage (large $h_{ii}$) but small residual magnitude $|e_i|$.\n  2. One point with small leverage (small $h_{ii}$) but large residual magnitude $|e_i|$.\n- Fits the OLS model with intercept and one predictor for each dataset, computes $e_i$ and $h_{ii}$ for all observations, and then computes the standardized residual $r_i$ and the studentized residual $t_i$ for each observation, starting from the model assumptions and core definitions in linear regression. Do not use any prepackaged regression function; derive and implement all needed quantities directly from the definitions of OLS, the hat matrix, and residual-based variance estimators.\n\nFor the base line, use the deterministic model $y = \\beta_0 + \\beta_1 x$ with $\\beta_0 = 2$ and $\\beta_1 = 1.5$. Build each dataset by:\n- Creating $n_{\\text{base}}$ equally spaced design points $x$ over the closed interval $[-x_{\\max}, x_{\\max}]$, symmetric about $0$, with zero noise.\n- Appending the high-leverage point at $x_{\\text{HL}}$ with response exactly on the true line plus a specified small offset $\\epsilon_{\\text{HL}}$.\n- Appending the low-leverage point at $x = 0$ but with response offset by a specified large amount $\\delta_{\\text{LL}}$ away from the true line.\n\nIndex observations starting at $0$ and append the two special points at the end, in the order: high-leverage point first, then low-leverage point. Thus, for each dataset, the index of the high-leverage point is $n_{\\text{base}}$ and the index of the low-leverage large-residual point is $n_{\\text{base}} + 1$.\n\nUsing only the model assumptions and definitions, compute:\n- The OLS estimate $\\hat{\\beta}$ and residuals $e_i$ for all $i$.\n- The leverage values $h_{ii}$ for all $i$ via the hat matrix $H$.\n- The standardized residuals $r_i$ and studentized residuals $t_i$ for all $i$.\n\nYour program must process the following test suite of parameter sets, each given as a tuple $(n_{\\text{base}}, x_{\\max}, x_{\\text{HL}}, \\delta_{\\text{LL}}, \\epsilon_{\\text{HL}})$:\n- Test case $1$: $(12, 3, 15, 15, 0)$.\n- Test case $2$: $(10, 2, 50, 10, 0)$.\n- Test case $3$: $(12, 3, 8, 6, 0.5)$.\n\nFor each dataset, produce four quantities:\n1. The index $i_r$ of the observation with the largest absolute standardized residual $|r_i|$.\n2. The index $i_t$ of the observation with the largest absolute studentized residual $|t_i|$.\n3. A boolean $b_1$ indicating whether, for the low-leverage large-residual point, the studentized residual magnitude exceeds the standardized residual magnitude, that is, whether $|t_i| > |r_i|$ at index $n_{\\text{base}}+1$.\n4. A boolean $b_2$ indicating whether, for the high-leverage small-residual point, the standardized residual magnitude is at least as large as the studentized residual magnitude, that is, whether $|r_i| \\ge |t_i|$ at index $n_{\\text{base}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list $[i_r, i_t, b_1, b_2]$ corresponding to one test case, in the same order as the test suite (for example, $[[i_{r,1}, i_{t,1}, b_{1,1}, b_{2,1}], [i_{r,2}, i_{t,2}, b_{1,2}, b_{2,2}], [i_{r,3}, i_{t,3}, b_{1,3}, b_{2,3}]]$). No physical units or angle units are involved in this problem.", "solution": "The user-provided problem is valid. It is a well-defined computational task in the field of statistical learning, grounded in the established principles of linear regression analysis. The problem is self-contained, scientifically sound, and all parameters and objectives are clearly specified. The solution proceeds by implementing the required calculations from first principles as requested.\n\n### Theoretical Foundation and Method\n\nThe problem requires an analysis of residuals in a simple linear regression model, $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. This can be expressed in matrix form as $y = X\\beta + \\varepsilon$, where $y$ is the $n \\times 1$ vector of responses, $X$ is the $n \\times p$ design matrix (with $p=2$ for simple linear regression), $\\beta$ is the $p \\times 1$ vector of parameters, and $\\varepsilon$ is the $n \\times 1$ vector of random errors. The errors are assumed to be independent and identically distributed with mean $0$ and variance $\\sigma^2$, i.e., $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$.\n\n**1. Ordinary Least Squares (OLS) Estimation**\nThe OLS estimator $\\hat{\\beta}$ for the parameter vector $\\beta$ is found by minimizing the sum of squared residuals, $S(\\beta) = (y - X\\beta)^\\top(y - X\\beta)$. This yields the normal equations, $(X^\\top X)\\hat{\\beta} = X^\\top y$. Assuming the matrix $X^\\top X$ is invertible (which holds true if $X$ has full column rank), the unique OLS estimator is:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\nThe vector of fitted values is $\\hat{y} = X\\hat{\\beta}$, and the vector of residuals is $e = y - \\hat{y}$.\n\n**2. The Hat Matrix and Leverage**\nThe fitted values can be expressed as a linear transformation of the observed values $y$:\n$$ \\hat{y} = X((X^\\top X)^{-1} X^\\top y) = H y $$\nThe matrix $H = X(X^\\top X)^{-1}X^\\top$ is known as the \"hat matrix\" because it transforms $y$ into $\\hat{y}$. It is an $n \\times n$ symmetric and idempotent ($H^2 = H$) projection matrix. The diagonal elements of the hat matrix, $h_{ii}$, are called the leverages. Each $h_{ii}$ measures how much the $i$-th response value $y_i$ influences its own fitted value $\\hat{y}_i$, since $\\hat{y}_i = \\sum_{j=1}^n H_{ij} y_j = h_{ii}y_i + \\sum_{j \\ne i} h_{ij} y_j$. Leverage values are bounded by $0 \\le h_{ii} \\le 1$. A high leverage value indicates that the $i$-th observation is an outlier in the space of the predictors (the $x$-values).\n\n**3. Residual Analysis**\nThe residuals are given by $e = y - \\hat{y} = y - Hy = (I - H)y$. The variance-covariance matrix of the residuals is:\n$$ \\mathrm{Var}(e) = \\mathrm{Var}((I-H)y) = (I-H)\\mathrm{Var}(y)(I-H)^\\top = (I-H)(\\sigma^2 I)(I-H) = \\sigma^2(I-H) $$\nThe variance of a single residual $e_i$ is therefore $\\mathrm{Var}(e_i) = \\sigma^2(1 - h_{ii})$. An unbiased estimator for the error variance $\\sigma^2$ is the Mean Squared Error (MSE):\n$$ \\hat{\\sigma}^2 = \\frac{e^\\top e}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p} $$\nwhere $n-p$ are the residual degrees of freedom.\n\n**4. Standardized Residuals**\nStandardized residuals account for the fact that raw residuals $e_i$ do not have the same variance. The standardized residual for observation $i$, denoted $r_i$, is the raw residual scaled by an estimate of its standard deviation:\n$$ r_i = \\frac{e_i}{\\widehat{\\mathrm{sd}}(e_i)} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}} $$\nThis metric adjusts each residual for its leverage. A high-leverage point (large $h_{ii}$) will have its residual magnified.\n\n**5. Studentized Residuals**\nA key issue with standardized residuals is that the observation $i$ being evaluated contributes to the estimate $\\hat{\\sigma}$. If observation $i$ is a significant outlier, it can inflate $\\hat{\\sigma}$, thereby \"masking\" its own outlier status by resulting in a smaller standardized residual. The studentized residual (or externally studentized residual), $t_i$, addresses this by using a variance estimate $\\hat{\\sigma}_{(i)}^2$ computed from a regression fit with the $i$-th observation removed:\n$$ t_i = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1 - h_{ii}}} $$\nA computationally efficient formula relates $t_i$ to the standardized residual $r_i$:\n$$ t_i = r_i \\sqrt{\\frac{n - p - 1}{n - p - r_i^2}} $$\nThis formula demonstrates that $|t_i| > |r_i|$ if and only if $r_i^2 > 1$, meaning the studentized residual further amplifies the magnitude of already large standardized residuals, making it a more sensitive diagnostic for detecting outliers.\n\n### Implementation Strategy\n\nThe program will process each test case as follows:\n1.  **Data Construction**: For each set of parameters $(n_{\\text{base}}, x_{\\max}, x_{\\text{HL}}, \\delta_{\\text{LL}}, \\epsilon_{\\text{HL}})$, generate the $x$ and $y$ vectors. This involves creating the base points on the line $y = 2 + 1.5x$, and appending the specified high-leverage and low-leverage/large-residual points.\n2.  **Model Fitting**: Construct the design matrix $X$ by prepending a column of ones to the $x$ vector. Then, compute the OLS estimate $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ using matrix operations.\n3.  **Residual and Leverage Calculation**: Compute the residuals $e = y - X\\hat{\\beta}$ and the leverages $h_{ii}$ as the diagonal of the hat matrix $H = X(X^\\top X)^{-1}X^\\top$.\n4.  **Diagnostic Computation**: Calculate the variance estimate $\\hat{\\sigma}^2$, and then compute the vectors of standardized residuals $r$ and studentized residuals $t$ using their respective formulas.\n5.  **Result Extraction**: From the computed vectors, identify the indices of the maximum absolute standardized and studentized residuals, and evaluate the two specified boolean conditions concerning the special points.\n6.  **Output Formatting**: Collate the results for all test cases into a list of lists and format it as a string for the final output.\n\nThe implementation relies on the `numpy` library for numerical linear algebra operations, adhering strictly to the formulas derived from first principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_residuals(n_base: int, x_max: float, x_HL: float, delta_LL: float, epsilon_HL: float) -> list:\n    \"\"\"\n    Constructs a dataset, fits an OLS model from first principles, and computes\n    standardized and studentized residuals to identify influential points.\n\n    Args:\n        n_base: Number of base points for the regression line.\n        x_max: The maximum absolute value for the range of base x-points.\n        x_HL: The x-coordinate of the high-leverage point.\n        delta_LL: The y-offset for the low-leverage, large-residual point.\n        epsilon_HL: The y-offset for the high-leverage point.\n\n    Returns:\n        A list containing [i_r, i_t, b1, b2]:\n        i_r: Index of the max absolute standardized residual.\n        i_t: Index of the max absolute studentized residual.\n        b1: Boolean, |t_i| > |r_i| for the low-leverage point.\n        b2: Boolean, |r_i| >= |t_i| for the high-leverage point.\n    \"\"\"\n    # True model parameters\n    beta0_true = 2.0\n    beta1_true = 1.5\n\n    # 1. Construct the dataset\n    # Base points on the true line\n    x_base = np.linspace(-x_max, x_max, n_base)\n    y_base = beta0_true + beta1_true * x_base\n    \n    # High-leverage point\n    x_hl = float(x_HL)\n    y_hl = (beta0_true + beta1_true * x_hl) + epsilon_HL\n    \n    # Low-leverage, large-residual point\n    x_ll = 0.0\n    y_ll = (beta0_true + beta1_true * x_ll) + delta_LL\n    \n    # Combine into the full dataset\n    x = np.concatenate((x_base, [x_hl, x_ll]))\n    y = np.concatenate((y_base, [y_hl, y_ll]))\n    \n    n = len(x)\n    p = 2  # Number of parameters (intercept beta0, slope beta1)\n    \n    # Indices of the special points\n    idx_hl = n_base\n    idx_ll = n_base + 1\n    \n    # 2. Fit OLS model from first principles\n    # Construct the design matrix X\n    X = np.c_[np.ones(n), x]\n    \n    # OLS estimator: beta_hat = (X'X)^-1 X'y\n    XTX_inv = np.linalg.inv(X.T @ X)\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Predicted values and residuals\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    \n    # 3. Compute leverage values (h_ii)\n    # Hat matrix: H = X(X'X)^-1 X'\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # 4. Compute standardized and studentized residuals\n    \n    # Unbiased estimator for error variance sigma^2\n    rss = e.T @ e\n    sigma2_hat = rss / (n - p)\n    sigma_hat = np.sqrt(sigma2_hat)\n    \n    # Standardized residuals: r_i = e_i / (sigma_hat * sqrt(1 - h_ii))\n    # Handle potential division by zero if h_ii is close to 1\n    sqrt_1_minus_h = np.sqrt(1 - h)\n    r = np.zeros_like(e)\n    valid_indices_r = (1 - h) > 1e-12\n    r[valid_indices_r] = e[valid_indices_r] / (sigma_hat * sqrt_1_minus_h[valid_indices_r])\n    \n    # Studentized residuals: t_i = r_i * sqrt((n - p - 1) / (n - p - r_i^2))\n    df_full = n - p\n    df_del = n - p - 1\n    \n    denom_t_sq = df_full - r**2\n    t = np.zeros_like(r)\n    \n    # Handle r_i^2 < n-p for valid square root\n    valid_indices_t = denom_t_sq > 1e-12\n    t[valid_indices_t] = r[valid_indices_t] * np.sqrt(df_del / denom_t_sq[valid_indices_t])\n    \n    # If r_i^2 -> n-p, t -> inf\n    infinite_indices = np.abs(denom_t_sq) <= 1e-12\n    t[infinite_indices] = np.inf * np.sign(r[infinite_indices])\n    \n    # 5. Determine the required outputs\n    i_r = int(np.argmax(np.abs(r)))\n    i_t = int(np.argmax(np.abs(t)))\n\n    b1 = bool(np.abs(t[idx_ll]) > np.abs(r[idx_ll]))\n    b2 = bool(np.abs(r[idx_hl]) >= np.abs(t[idx_hl]))\n    \n    return [i_r, i_t, b1, b2]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_base, x_max, x_HL, delta_LL, epsilon_HL)\n        (12, 3, 15, 15, 0),\n        (10, 2, 50, 10, 0),\n        (12, 3, 8, 6, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack tuple into named arguments for clarity\n        n_base, x_max, x_HL, delta_LL, epsilon_HL = case\n        result = calculate_residuals(n_base, x_max, x_HL, delta_LL, epsilon_HL)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() conversion of a list includes spaces, which is standard.\n    # The problem example is symbolic; this literal interpretation of the template is safest.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3176898"}, {"introduction": "Residual diagnostics are powerful, but they have limits, especially when a model is overfit to the training data. This thought experiment [@problem_id:3176882] explores the extreme scenario of a 'perfect fit,' where the number of predictors is so large that the model interpolates the data exactly. By analyzing this case, you will understand why diagnostics like standardized and studentized residuals collapse and how this collapse serves as a critical red flag for an over-parameterized model.", "problem": "Consider Ordinary Least Squares (OLS) linear regression with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^n$. The fitted values are the orthogonal projection of $y$ onto the column space of $X$, realized by the symmetric idempotent projector $H$, so that $\\hat{y} = H y$ and the residual vector is $e = y - \\hat{y} = (I_n - H) y$, where $I_n$ is the $n \\times n$ identity matrix. The leverage of observation $i$ is the $i$th diagonal element $h_{ii}$ of $H$. The standardized residual for observation $i$ is given by $r_i = e_i / \\big( s \\sqrt{1 - h_{ii}} \\big)$, where $s$ is the usual unbiased estimator of the noise scale based on residuals, and the externally studentized residual is given by $\\tilde{r}_i = e_i / \\big( s_{(i)} \\sqrt{1 - h_{ii}} \\big)$, where $s_{(i)}$ is the unbiased noise estimate computed from the model refit with observation $i$ removed.\n\nSuppose the model achieves a perfect fit to the training data in the sense that the column space of $X$ equals $\\mathbb{R}^n$. Starting from the properties of orthogonal projection operators and the definitions above, determine which of the following statements are true.\n\nA. Under perfect fit with $\\operatorname{col}(X) = \\mathbb{R}^n$, the hat matrix satisfies $H = I_n$ and the residual vector is $e = 0$.\n\nB. Under perfect fit, the leverage values satisfy $h_{ii} = 0$ for all $i$, which implies that standardized residuals are all $0$.\n\nC. Under perfect fit, standardized residuals $r_i$ and externally studentized residuals $\\tilde{r}_i$ are undefined, because $\\sqrt{1 - h_{ii}} = 0$ and the scale factors $s$ and $s_{(i)}$ are not well-defined in a saturated fit.\n\nD. Perfect fit implies zero training error but compromises outlier detection and generalization; residual-based diagnostics collapse, signaling overfitting.\n\nE. Even under perfect fit, externally studentized residuals $\\tilde{r}_i$ remain well-defined and follow a $t$ distribution with $n - p - 1$ degrees of freedom.", "solution": "The problem statement is subjected to validation before proceeding.\n\nThe core premise is that for an Ordinary Least Squares (OLS) model with design matrix $X \\in \\mathbb{R}^{n \\times p}$, the model achieves a \"perfect fit,\" which is rigorously defined as the column space of $X$ spanning the entire ambient space, i.e., $\\operatorname{col}(X) = \\mathbb{R}^n$. The problem uses standard definitions for the hat matrix $H$, residual vector $e$, leverage $h_{ii}$, standardized residuals $r_i$, and externally studentized residuals $\\tilde{r}_i$.\n\nThe condition $\\operatorname{col}(X) = \\mathbb{R}^n$ implies that the rank of the matrix $X$ is equal to the dimension of the space, which is $n$. So, $\\operatorname{rank}(X) = n$. Since the rank of a matrix cannot exceed the number of its rows or columns, we have $\\operatorname{rank}(X) \\le \\min(n, p)$. For $\\operatorname{rank}(X) = n$, it is necessary that $p \\ge n$. This scenario corresponds to a \"saturated\" model, where the number of predictors is at least as large as the number of observations. This is a valid, though extreme, theoretical case in regression analysis, often used to study the properties of overfitting. The problem is scientifically grounded, well-posed, and objective. Thus, the problem statement is valid.\n\nWe now proceed with the derivation based on the given premise.\n\nFirst, we analyze the properties of the hat matrix $H$. The hat matrix $H$ is defined as the orthogonal projection matrix onto the column space of $X$, $\\operatorname{col}(X)$. Given the condition $\\operatorname{col}(X) = \\mathbb{R}^n$, $H$ is the projection matrix onto $\\mathbb{R}^n$. The only projection matrix that maps every vector in $\\mathbb{R}^n$ onto the entirety of $\\mathbb{R}^n$ is the identity matrix, $I_n$.\n$$ H = I_n $$\n\nNext, we determine the fitted values $\\hat{y}$ and the residual vector $e$.\nThe fitted values are given by $\\hat{y} = H y$. Substituting $H = I_n$:\n$$ \\hat{y} = I_n y = y $$\nThis confirms the meaning of a \"perfect fit\": the model's predictions exactly match the observed data.\nThe residual vector is $e = y - \\hat{y}$. Substituting $\\hat{y} = y$:\n$$ e = y - y = 0 $$\nThe residual vector is the zero vector, meaning every individual residual $e_i = 0$ for $i = 1, \\dots, n$.\n\nNow, we evaluate the leverage values, $h_{ii}$. The leverage of observation $i$ is the $i$-th diagonal element of the hat matrix $H$. Since $H = I_n$, its diagonal elements are all $1$.\n$$ h_{ii} = 1 \\quad \\text{for all } i \\in \\{1, \\dots, n\\} $$\n\nWe can now analyze the standardized and studentized residuals.\nThe standardized residual for observation $i$ is defined as $r_i = e_i / \\big( s \\sqrt{1 - h_{ii}} \\big)$.\nWe found that the numerator is $e_i = 0$. For the denominator, we have $\\sqrt{1 - h_{ii}} = \\sqrt{1 - 1} = 0$. Thus, the expression for $r_i$ takes the indeterminate form $0/0$.\nFurthermore, let's examine the scale estimator $s$. The usual unbiased estimator of the error variance is $s^2 = \\text{RSS} / (n-p)$, where $\\text{RSS} = \\sum_{i=1}^n e_i^2$ is the residual sum of squares. Since $e=0$, we have $\\text{RSS} = 0$. The degrees of freedom for the error are $n-p$. As established, the condition $\\operatorname{col}(X) = \\mathbb{R}^n$ requires $p \\ge n$, which means the degrees of freedom $n-p \\le 0$. A valid variance estimate requires positive degrees of freedom. So, $s^2 = 0 / (\\text{non-positive value})$, which is undefined.\nTherefore, the standardized residuals $r_i$ are undefined for two reasons: the denominator contains a factor of zero ($\\sqrt{1-h_{ii}}$), and the scale estimate $s$ is itself not well-defined.\n\nThe externally studentized residual is $\\tilde{r}_i = e_i / \\big( s_{(i)} \\sqrt{1 - h_{ii}} \\big)$.\nThis expression also has a numerator $e_i = 0$ and a factor $\\sqrt{1-h_{ii}} = 0$ in the denominator, so it is also of the indeterminate form $0/0$. The leave-one-out scale estimate $s_{(i)}$ is computed from a model fit on $n-1$ observations with $p$ predictors. The degrees of freedom for this new model would be $(n-1) - p$. Since $p \\ge n$, these degrees of freedom are negative or zero. The model on the reduced dataset would also produce a perfect fit (assuming general position of the columns of $X$), resulting in zero RSS. Thus, $s_{(i)}^2$ would also be undefined.\n\nWith these results, we can evaluate each statement.\n\n**A. Under perfect fit with $\\operatorname{col}(X) = \\mathbb{R}^n$, the hat matrix satisfies $H = I_n$ and the residual vector is $e = 0$.**\nOur derivation shows that if $\\operatorname{col}(X)=\\mathbb{R}^n$, the projection matrix $H$ must be $I_n$. Consequently, the residual vector $e = (I_n-H)y = (I_n-I_n)y = 0$. This statement is a direct and correct mathematical consequence of the premise.\n**Verdict: Correct.**\n\n**B. Under perfect fit, the leverage values satisfy $h_{ii} = 0$ for all $i$, which implies that standardized residuals are all $0$.**\nOur derivation shows that under perfect fit, $H=I_n$, which means the leverage values are $h_{ii}=1$ for all $i$. The claim that $h_{ii}=0$ is false. Therefore, the statement is incorrect.\n**Verdict: Incorrect.**\n\n**C. Under perfect fit, standardized residuals $r_i$ and externally studentized residuals $\\tilde{r}_i$ are undefined, because $\\sqrt{1 - h_{ii}} = 0$ and the scale factors $s$ and $s_{(i)}$ are not well-defined in a saturated fit.**\nOur derivation shows that for all $i$, $h_{ii}=1$, which makes the term $\\sqrt{1-h_{ii}}$ equal to $0$. This alone makes the denominators of $r_i$ and $\\tilde{r}_i$ zero. Additionally, we showed that the degrees of freedom for error are non-positive ($n-p \\le 0$), making the scale estimators $s$ and $s_{(i)}$ undefined. The statement provides a complete and correct explanation for why these residual diagnostics are undefined in this scenario.\n**Verdict: Correct.**\n\n**D. Perfect fit implies zero training error but compromises outlier detection and generalization; residual-based diagnostics collapse, signaling overfitting.**\nThis statement provides a statistical interpretation of the mathematical results.\n- \"Perfect fit implies zero training error\": True, as we showed $e=0$.\n- \"compromises outlier detection\": True. Outlier detection methods based on residuals are completely ineffective since all residuals are exactly zero. All points also have maximum leverage $h_{ii}=1$, so leverage plots are also uninformative for distinguishing points.\n- \"compromises ... generalization\": True. A model with $p \\ge n$ that perfectly fits the training data is a canonical example of overfitting. Such a model captures the noise in the training data, and is expected to perform poorly on unseen data. This is a fundamental concept in statistical learning.\n- \"residual-based diagnostics collapse\": True. As shown in the analysis for option C, quantities like $r_i$ and $\\tilde{r}_i$ become undefined.\n- \"signaling overfitting\": True. This collapse of diagnostics is a key sign of a saturated, overfitted model.\nThe entire statement is a sound and correct description of the consequences of the given scenario.\n**Verdict: Correct.**\n\n**E. Even under perfect fit, externally studentized residuals $\\tilde{r}_i$ remain well-defined and follow a $t$ distribution with $n - p - 1$ degrees of freedom.**\nThis statement is false. As shown in the analysis for option C, the externally studentized residuals $\\tilde{r}_i$ are undefined. The t-distribution result requires standard regression assumptions, including $n > p$, which is violated here. Furthermore, the degrees of freedom parameter for the t-distribution must be positive, but here $n-p-1$ would be negative, which is invalid.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ACD}$$", "id": "3176882"}]}