{"hands_on_practices": [{"introduction": "One of the most intuitive ways to detect non-linearity is to see if a simple linear model fits the data well in some regions but poorly in others. This exercise [@problem_id:3114947] formalizes this idea by having you compute the coefficient of determination, $R^2$, within a moving window across your data. By examining the variability of these local $R^2$ values and using a bootstrap procedure to assess its statistical significance, you will develop a powerful non-parametric tool for diagnosing hidden curvature in the underlying regression function $f(x)$.", "problem": "You are given a univariate predictor-response dataset $\\{(x_i,y_i)\\}_{i=1}^n$ with an unknown regression function $f(x)$ under the additive noise model $y_i = f(x_i) + \\varepsilon_i$, where $\\varepsilon_i$ are independent and identically distributed with zero mean and finite variance. Your goal is to diagnose non-linearity in $f(x)$ by computing the local coefficient of determination within moving windows over $x$ and evaluating whether the variability of these local coefficients departs from what would be expected under a globally linear relationship.\n\nUsing only fundamental definitions from ordinary least squares and resampling logic, design a program that does the following for each test case:\n\n1) Sort the data by the predictor so that $x_1 \\le x_2 \\le \\cdots \\le x_n$.\n\n2) For a fixed window size $m$ with $3 \\le m < n$, define overlapping contiguous windows of indices $W_j = \\{j, j+1, \\dots, j + m - 1\\}$ for $j \\in \\{1, 2, \\dots, n - m + 1\\}$. For each window $W_j$, fit a local ordinary least squares line $y \\approx a_j + b_j x$ using only $\\{(x_i,y_i): i \\in W_j\\}$, and compute the local coefficient of determination\n$$\nR^2_j \\;=\\; 1 \\;-\\; \\frac{\\sum_{i \\in W_j} (y_i - \\widehat{y}_i)^2}{\\sum_{i \\in W_j} (y_i - \\overline{y}_{W_j})^2},\n$$\nwhere $\\widehat{y}_i = a_j + b_j x_i$ for $i \\in W_j$, and $\\overline{y}_{W_j}$ is the sample mean of $\\{y_i : i \\in W_j\\}$. In the degenerate case where $\\sum_{i \\in W_j} (y_i - \\overline{y}_{W_j})^2 = 0$, set $R^2_j = 1$ if $\\sum_{i \\in W_j} (y_i - \\widehat{y}_i)^2 = 0$ and $R^2_j = 0$ otherwise.\n\n3) Define the variability statistic\n$$\nS_{\\text{obs}} \\;=\\; \\text{sd}\\big( \\{R^2_j\\}_{j=1}^{n-m+1} \\big),\n$$\nthe sample standard deviation of the local coefficients of determination across windows.\n\n4) Construct a null reference for $S_{\\text{obs}}$ under the global linear model $y_i = \\alpha + \\beta x_i + \\varepsilon_i$ by:\n   - Fitting the global ordinary least squares line $y \\approx \\alpha + \\beta x$ to all $n$ data points, obtaining $\\widehat{y}_i$ and residuals $e_i = y_i - \\widehat{y}_i$.\n   - For a positive integer $B$, and for each $b \\in \\{1,2,\\dots,B\\}$, generate a bootstrap dataset $y_i^{(b)} = \\widehat{y}_i + e^{*(b)}_i$ where $\\{e^{*(b)}_i\\}_{i=1}^n$ are sampled with replacement from $\\{e_i\\}_{i=1}^n$. For each bootstrap dataset, compute the corresponding windowed local coefficients of determination and their sample standard deviation $S_b$ in the same way as $S_{\\text{obs}}$.\n   - Compute the one-sided p-value\n$$\np \\;=\\; \\frac{1 + \\sum_{b=1}^B \\mathbb{I}\\{ S_b \\ge S_{\\text{obs}} \\}}{B + 1}.\n$$\n\n5) Decide non-linearity via the rule: declare non-linearity detected if $p < \\alpha$, where $\\alpha$ is a given significance level.\n\nRequirements:\n- Use the ordinary least squares solution for line fitting within each window and globally, grounded in the definition that ordinary least squares minimizes $\\sum_i (y_i - a - b x_i)^2$ over $(a,b)$.\n- Use the definition of the coefficient of determination $R^2$ within each window as specified above.\n- Implement the bootstrap by residual resampling as described.\n\nAngle units are not applicable. There are no physical units. All proportions must be expressed as decimals.\n\nTest Suite:\nYour program must evaluate the following four test cases. For each, generate data internally using a pseudorandom number generator seeded as specified. For all cases use window size $m = 31$, number of bootstrap replicates $B = 199$, and significance level $\\alpha = 0.05$.\n\n- Case $1$ (globally linear with low noise; expect no non-linearity):\n  - Seed $= 12345$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently for $i = 1, \\dots, n$.\n  - Response generation: $y_i = -0.5 + 2.0\\,x_i + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, 0.2^2)$ independently.\n\n- Case $2$ (quadratic curvature; expect non-linearity):\n  - Seed $= 67890$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently.\n  - Response generation: $y_i = 1.0\\,x_i^2 + 0.0\\,x_i + 0.0 + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, 0.3^2)$ independently.\n\n- Case $3$ (piecewise linear with a kink; expect non-linearity):\n  - Seed $= 4242$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently.\n  - Response generation: \n$$y_i = \\begin{cases} -1.0\\,x_i + 0.0 + \\varepsilon_i, & x_i < 0, \\\\ 2.5\\,x_i + 0.0 + \\varepsilon_i, & x_i \\ge 0, \\end{cases}$$ with $\\varepsilon_i \\sim \\mathcal{N}(0, 0.25^2)$ independently.\n\n- Case $4$ (globally linear with high noise; expect no non-linearity):\n  - Seed $= 999$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently.\n  - Response generation: $y_i = 0.0 + 1.5\\,x_i + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, 2.0^2)$ independently.\n\nOutput specification:\n- For each case, output a boolean indicating whether non-linearity is detected by the above testing rule (true if $p < \\alpha$, false otherwise).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the cases $1$ through $4$ (for example, `[True,False,True,False]`).", "solution": "The problem asks for the design and implementation of a statistical procedure to detect non-linearity in a predictor-response relationship, $y_i = f(x_i) + \\varepsilon_i$. The method is based on analyzing the variability of the coefficient of determination, $R^2$, computed over moving windows of the data. The decision for or against non-linearity is made via a formal hypothesis test, with a null distribution generated using a bootstrap resampling technique.\n\nThe procedure is validated as scientifically sound, well-posed, and fully specified. All parameters, data generation models, and computational formulas are provided unambiguously.\n\n### Principle-Based Design\n\nThe fundamental principle of this test is that a globally linear function should appear locally linear everywhere. Conversely, a globally non-linear function will exhibit varying degrees of linearity across its domain.\n\n$1$. **The Test Statistic**: The core idea is to slide a window of fixed size $m$\nacross the data (which has been sorted by the predictor, $x$) and perform a simple linear regression within each window. For each window $W_j = \\{j, j+1, \\dots, j + m - 1\\}$, we compute the local coefficient of determination, $R^2_j$.\nIf the true function $f(x)$ is linear, $f(x) = \\alpha + \\beta x$, then the model in each window is also linear. The local $R^2_j$ values should be relatively high and stable, with fluctuations primarily due to the random noise $\\varepsilon_i$. However, if $f(x)$ is non-linear (e.g., quadratic), the quality of a linear fit will change depending on the window's position. In regions of high curvature, a straight line is a poor approximation, leading to a low $R^2_j$. In flatter regions, the fit will be better, yielding a higher $R^2_j$. This induces significant variability in the sequence of $\\{R^2_j\\}$ values.\nThe test statistic $S_{\\text{obs}} = \\text{sd}\\big( \\{R^2_j\\}_{j=1}^{n-m+1} \\big)$, the sample standard deviation of these local coefficients of determination, is designed to capture precisely this variability. A large value of $S_{\\text{obs}}$ suggests non-linearity.\n\n$2$. **Hypothesis Testing and the Bootstrap**: To objectively decide if $S_{\\text{obs}}$ is \"large enough\" to reject linearity, we frame the problem as a hypothesis test. The null hypothesis, $H_0$, is that the relationship is globally linear: $y_i = \\alpha + \\beta x_i + \\varepsilon_i$. We need to determine the distribution of the statistic $S$ under this null hypothesis.\nThe bootstrap provides a computational method for approximating this null distribution. The specified residual bootstrap procedure works as follows:\n- First, we fit the global linear model to the entire dataset $\\{(x_i,y_i)\\}_{i=1}^n$ using ordinary least squares (OLS), obtaining the estimated coefficients $\\widehat{\\alpha}$ and $\\widehat{\\beta}$, a set of fitted values $\\widehat{y}_i = \\widehat{\\alpha} + \\widehat{\\beta} x_i$, and the corresponding residuals $e_i = y_i - \\widehat{y}_i$. These residuals represent our best estimate of the underlying noise process.\n- We then generate $B$ bootstrap datasets. Each synthetic dataset $\\{(x_i, y_i^{(b)})\\}$ is created by holding the predictors $x_i$ and the null-model fit $\\widehat{y}_i$ fixed, and adding a new set of noise terms: $y_i^{(b)} = \\widehat{y}_i + e^{*(b)}_i$. The bootstrap residuals $\\{e^{*(b)}_i\\}_{i=1}^n$ are obtained by sampling with replacement from the original set of residuals $\\{e_i\\}_{i=1}^n$.\n- By construction, each bootstrap dataset conforms to the null hypothesis of linearity, while preserving the noise characteristics of the original data.\n- For each of the $B$ bootstrap datasets, we compute the statistic $S_b$ in the exact same manner as $S_{\\text{obs}}$. The collection $\\{S_b\\}_{b=1}^B$ forms an empirical reference distribution for our test statistic under $H_0$.\n\n$3$. **P-value and Decision**: The one-sided p-value is the probability of observing a test statistic as large or larger than $S_{\\text{obs}}$, assuming $H_0$ is true. This is estimated from the bootstrap samples using the formula:\n$$\np \\;=\\; \\frac{1 + \\sum_{b=1}^B \\mathbb{I}\\{ S_b \\ge S_{\\text{obs}} \\}}{B + 1}\n$$\nThe inclusion of $1$ in the numerator and denominator prevents a p-value of $0$ and provides a more stable estimate. If this p-value is smaller than a pre-defined significance level $\\alpha$ (e.g., $0.05$), we reject the null hypothesis and conclude that there is statistically significant evidence of non-linearity.\n\n### Algorithmic Implementation Steps\n\nThe implementation follows the described logic for each test case.\n\n$1$. **Data Generation and Sorting**: Data $\\{(x_i,y_i)\\}_{i=1}^n$ are generated according to the specified model and random seed. The data pairs are then sorted based on the predictor values $x_i$ to enable the use of contiguous moving windows.\n\n$2$. **Calculation of $S_{\\text{obs}}$**: A loop iterates from $j = 1$ to $n - m + 1$. In each iteration, the data for the window $W_j$ are extracted. An OLS line $y \\approx a_j + b_j x$ is fitted to this window's data. The OLS solution for the slope $b_j$ and intercept $a_j$ minimizes the sum of squared errors and is given by known formulas:\n$$\nb_j = \\frac{\\sum_{i \\in W_j} (x_i - \\overline{x}_{W_j})(y_i - \\overline{y}_{W_j})}{\\sum_{i \\in W_j} (x_i - \\overline{x}_{W_j})^2}, \\quad a_j = \\overline{y}_{W_j} - b_j \\overline{x}_{W_j}\n$$\nwhere $\\overline{x}_{W_j}$ and $\\overline{y}_{W_j}$ are the sample means in window $W_j$. From the fitted values $\\widehat{y}_i = a_j + b_j x_i$, the local coefficient of determination is calculated as:\n$$\nR^2_j \\;=\\; 1 \\;-\\; \\frac{\\text{SSE}_j}{\\text{SST}_j} \\;=\\; 1 \\;-\\; \\frac{\\sum_{i \\in W_j} (y_i - \\widehat{y}_i)^2}{\\sum_{i \\in W_j} (y_i - \\overline{y}_{W_j})^2}\n$$\nAfter collecting all $R^2_j$ values, their sample standard deviation is computed to yield $S_{\\text{obs}}$.\n\n$3$. **Bootstrap Simulation**: The global OLS fit is performed on the full sorted dataset to get residuals. Then, a loop runs $B$ times. In each iteration, a bootstrap dataset is created, the full windowing procedure is re-run on this new dataset to calculate a bootstrap statistic $S_b$.\n\n$4$. **Conclusion**: The computed p-value is compared against the significance level $\\alpha = 0.05$. The function returns `True` (non-linearity detected) if $p < \\alpha$, and `False` otherwise. This process is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_S(x_sorted, y, m):\n    \"\"\"\n    Computes the variability statistic S for a given dataset (x, y) and window size m.\n    Assumes x_sorted is already sorted, and y corresponds to x_sorted.\n    \n    Args:\n        x_sorted (np.ndarray): Sorted predictor values.\n        y (np.ndarray): Corresponding response values.\n        m (int): Window size.\n        \n    Returns:\n        float: The sample standard deviation of local R^2 values.\n    \"\"\"\n    n = len(x_sorted)\n    num_windows = n - m + 1\n    r2_values = np.zeros(num_windows)\n\n    for j in range(num_windows):\n        x_w = x_sorted[j:j+m]\n        y_w = y[j:j+m]\n        \n        y_mean_w = np.mean(y_w)\n        sst_w = np.sum((y_w - y_mean_w)**2)\n\n        # Handle the degenerate case where all y-values in the window are the same.\n        # Problem statement: \"In the degenerate case where sum(y_i - y_bar)^2 = 0, \n        # set R^2_j = 1 if sum(y_i - y_hat_i)^2 = 0 and R^2_j = 0 otherwise.\"\n        # For an OLS fit, if SST=0, then SSE must also be 0 because the best fit line\n        # is the horizontal line at the mean, which perfectly fits the constant data.\n        if sst_w == 0:\n            r2_values[j] = 1.0\n            continue # Move to the next window\n            \n        x_mean_w = np.mean(x_w)\n        ss_xx_w = np.sum((x_w - x_mean_w)**2)\n        \n        # Handle the degenerate case where all x-values in the window are the same.\n        if ss_xx_w == 0:\n            # The best linear fit is a horizontal line at the mean of y.\n            y_hat_w = np.full_like(y_w, y_mean_w)\n        else:\n            ss_xy_w = np.sum((x_w - x_mean_w) * (y_w - y_mean_w))\n            b_w = ss_xy_w / ss_xx_w\n            a_w = y_mean_w - b_w * x_mean_w\n            y_hat_w = a_w + b_w * x_w\n        \n        sse_w = np.sum((y_w - y_hat_w)**2)\n        \n        r2_values[j] = 1.0 - sse_w / sst_w\n            \n    # Per the problem, compute the sample standard deviation. This requires ddof=1.\n    return np.std(r2_values, ddof=1)\n\ndef _test_nonlinearity(x, y, m, B, alpha):\n    \"\"\"\n    Performs the full non-linearity test procedure as described in the problem.\n    \n    Args:\n        x (np.ndarray): Predictor values.\n        y (np.ndarray): Response values.\n        m (int): Window size.\n        B (int): Number of bootstrap replicates.\n        alpha (float): Significance level.\n        \n    Returns:\n        bool: True if non-linearity is detected, False otherwise.\n    \"\"\"\n    n = len(x)\n\n    # 1. Sort the data by the predictor\n    sort_indices = np.argsort(x)\n    x_sorted = x[sort_indices]\n    y_sorted = y[sort_indices]\n\n    # 2. & 3. Compute the observed variability statistic S_obs\n    S_obs = _calculate_S(x_sorted, y_sorted, m)\n\n    # 4. Construct a null reference distribution using a bootstrap procedure\n    # Fit the global ordinary least squares line\n    x_mean_global = np.mean(x_sorted)\n    y_mean_global = np.mean(y_sorted)\n    ss_xx_global = np.sum((x_sorted - x_mean_global)**2)\n    ss_xy_global = np.sum((x_sorted - x_mean_global) * (y_sorted - y_mean_global))\n    \n    if ss_xx_global == 0:\n        b_global = 0.0\n    else:\n        b_global = ss_xy_global / ss_xx_global\n    \n    a_global = y_mean_global - b_global * x_mean_global\n    \n    y_hat_global = a_global + b_global * x_sorted\n    residuals = y_sorted - y_hat_global\n\n    # Bootstrap loop\n    S_boot = np.zeros(B)\n    for b_idx in range(B):\n        # Generate a bootstrap dataset\n        boot_residuals = np.random.choice(residuals, size=n, replace=True)\n        y_boot = y_hat_global + boot_residuals\n        # Compute the statistic for the bootstrap sample\n        S_boot[b_idx] = _calculate_S(x_sorted, y_boot, m)\n\n    # Compute the one-sided p-value\n    numerator = 1.0 + np.sum(S_boot >= S_obs)\n    denominator = float(B + 1)\n    p_value = numerator / denominator\n\n    # 5. Decide non-linearity based on the significance level\n    return p_value < alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        {'seed': 12345, 'n': 220, 'case_type': 'linear_low_noise'},\n        {'seed': 67890, 'n': 220, 'case_type': 'quadratic'},\n        {'seed': 4242, 'n': 220, 'case_type': 'piecewise'},\n        {'seed': 999, 'n': 220, 'case_type': 'linear_high_noise'}\n    ]\n    \n    m = 31\n    B = 199\n    alpha = 0.05\n    \n    results = []\n\n    for case in test_cases:\n        np.random.seed(case['seed'])\n        n = case['n']\n        \n        # Generate predictor variable\n        x = np.random.uniform(-2.5, 2.5, n)\n        \n        # Generate response variable based on the case type\n        if case['case_type'] == 'linear_low_noise':\n            eps = np.random.normal(loc=0.0, scale=0.2, size=n)\n            y = -0.5 + 2.0 * x + eps\n        elif case['case_type'] == 'quadratic':\n            eps = np.random.normal(loc=0.0, scale=0.3, size=n)\n            y = 1.0 * x**2 + eps\n        elif case['case_type'] == 'piecewise':\n            eps = np.random.normal(loc=0.0, scale=0.25, size=n)\n            y = np.where(x < 0, -1.0 * x, 2.5 * x) + eps\n        elif case['case_type'] == 'linear_high_noise':\n            eps = np.random.normal(loc=0.0, scale=2.0, size=n)\n            y = 1.5 * x + eps\n\n        # Perform the test and store the boolean result\n        is_nonlinear = _test_nonlinearity(x, y, m, B, alpha)\n        results.append(is_nonlinear)\n\n    # Print final result in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3114947"}, {"introduction": "While visual and non-parametric checks are useful, classical statistics provides formal tests for model misspecification. The Ramsey RESET test [@problem_id:3115010] is a cornerstone diagnostic that cleverly checks for omitted non-linearities by augmenting a linear model with powers of its own fitted values, $\\widehat{y}^{\\,j}$. This practice will guide you through implementing the associated $F$-test and using Monte Carlo simulation to explore its statistical power, giving you a deep understanding of how to assess a test's performance under different scenarios.", "problem": "You are asked to implement and study the Ramsey Regression Specification Error Test (RESET), a classical diagnostic for detecting omitted nonlinear terms in a linear model, and to calibrate its power under quadratic alternatives via simulation. Your program must be a complete, runnable implementation that generates synthetic data, applies the test, and reports empirical rejection rates.\n\nConsider a univariate predictor setting with independent and identically distributed observations. For each simulation replication, generate data as follows. For a given sample size $n$, draw predictors $x_i \\sim \\mathcal{N}(0,1)$ independently for $i=1,\\dots,n$. Given parameters $\\beta_0$, $\\beta_1$, $\\gamma$, and noise scale $\\sigma>0$, generate responses according to the data generating process\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\gamma x_i^2 \\;+\\; \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\ \\text{independently}.\n$$\n\nDefine the restricted linear model that omits the nonlinear term as\n$$\ny_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; u_i,\n$$\nand estimate it by ordinary least squares (OLS). Let $\\widehat{y}_i$ denote the fitted values from this restricted model. The Ramsey Regression Specification Error Test (RESET) augments the restricted model with powers of the fitted values to form an unrestricted model,\n$$\ny_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; \\sum_{j=2}^{p} \\delta_j \\,\\widehat{y}_i^{\\,j} \\;+\\; v_i,\n$$\nwhere $p \\ge 2$ is the chosen RESET order. The null hypothesis asserts that all augmentation coefficients are zero, i.e., $H_0: \\delta_2=\\dots=\\delta_p=0$. You must implement the classical nested-model decision rule using the Fisher–Snedecor distribution: compute the ordinary least squares residual sum of squares for the restricted and unrestricted models, compare them by the appropriate ratio that yields an $F$ statistic with numerator degrees of freedom equal to the number of added regressors and denominator degrees of freedom equal to the unrestricted model’s residual degrees of freedom, and reject $H_0$ at significance level $\\alpha$ if the statistic falls in the rejection region determined by the upper $\\alpha$ quantile of the $F$ distribution. The significance level $\\alpha$ must be expressed as a decimal (e.g., $0.05$).\n\nYour program must perform Monte Carlo simulation to estimate the rejection probability (empirical power) under quadratic alternatives by repeating the following steps $R$ times for each parameter set: simulate a dataset under the specified parameters, run the Ramsey RESET at order $p$ and level $\\alpha$, and record whether $H_0$ is rejected. The final result for a parameter set is the average of these rejection indicators across $R$ replications. Under the special case $\\gamma=0$, this quantity estimates the empirical size.\n\nFoundational base you must rely on:\n- The ordinary least squares estimator minimizes the sum of squared residuals and solves the normal equations for linear regression.\n- For nested linear models with Gaussian errors, the scaled reduction in residual sum of squares has an $F$ distribution under the null hypothesis, with degrees of freedom given by the number of added parameters (numerator) and the unrestricted model’s residual degrees of freedom (denominator).\n- The Ramsey RESET augments the restricted model by powers of the restricted model’s fitted values to proxy omitted nonlinear functions of the regressors; if such nonlinearities are present, these augmentations improve fit, leading to rejection more often than the nominal level.\n\nImplement the simulation using the following test suite of parameter sets. Each test case is a tuple $(n,\\beta_0,\\beta_1,\\gamma,\\sigma,\\alpha,R,p,\\text{seed})$:\n- Case A (empirical size at the null): $(n,\\beta_0,\\beta_1,\\gamma,\\sigma,\\alpha,R,p,\\text{seed}) = (\\,200,\\,0,\\,1,\\,0,\\,1,\\,0.05,\\,500,\\,3,\\,12345\\,)$.\n- Case B (moderate nonlinearity, larger sample): $(\\,200,\\,0,\\,1,\\,0.2,\\,1,\\,0.05,\\,500,\\,3,\\,54321\\,)$.\n- Case C (strong nonlinearity, smaller sample, lower order): $(\\,50,\\,1,\\,1,\\,0.8,\\,1,\\,0.05,\\,500,\\,2,\\,2024\\,)$.\n- Case D (no linear effect, noisy, lenient level): $(\\,50,\\,0,\\,0,\\,0.5,\\,3,\\,0.1,\\,400,\\,3,\\,777\\,)$.\n\nFor each case, your program must output the estimated rejection probability as a float in decimal form. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[0.052,0.412,0.873,0.268]`). Round each reported rejection probability to three decimal places. No input reading is required; all parameters are embedded in the program. Use the specified random seeds to ensure reproducibility across runs.", "solution": "The problem requires the implementation of a Monte Carlo simulation to study the statistical power of the Ramsey Regression Specification Error Test (RESET). The simulation will estimate the test's rejection probability under various parameter configurations, including a null case to assess its empirical size and alternative cases with quadratic nonlinearity to assess its power.\n\n### Theoretical Framework\n\nThe analysis begins with a defined data generating process (DGP) where the true relationship between a response variable $y_i$ and a predictor $x_i$ includes a quadratic term. This specification is given by:\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\gamma x_i^2 \\;+\\; \\varepsilon_i\n$$\nwhere $x_i \\sim \\mathcal{N}(0,1)$ and the errors $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and identically distributed. The parameter $\\gamma$ controls the degree of nonlinearity. If $\\gamma=0$, the true model is linear. If $\\gamma \\neq 0$, a simple linear model is misspecified due to an omitted variable, $x_i^2$.\n\nThe Ramsey RESET is a general test for such misspecification. The procedure involves the following steps:\n\n1.  **Estimate a Restricted Model**: First, we postulate a simple linear model, which is potentially misspecified. This is the restricted model under the null hypothesis of correct specification:\n    $$\n    y_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; u_i\n    $$\n    This model is estimated using Ordinary Least Squares (OLS), yielding estimated coefficients $\\widehat{\\theta}_0$ and $\\widehat{\\theta}_1$, and the corresponding fitted values:\n    $$\n    \\widehat{y}_i \\;=\\; \\widehat{\\theta}_0 \\;+\\; \\widehat{\\theta}_1 x_i\n    $$\n\n2.  **Formulate an Unrestricted (Augmented) Model**: The core insight of the RESET is that if the true model is nonlinear, the fitted values $\\widehat{y}_i$ from the mis-specified linear model will themselves be a function of the predictors and thus may capture some of the omitted nonlinear effects. Powers of these fitted values, $\\widehat{y}_i^j$, can then serve as proxies for the unknown nonlinear terms. The unrestricted model is formed by augmenting the restricted model with these proxies:\n    $$\n    y_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; \\sum_{j=2}^{p} \\delta_j \\,\\widehat{y}_i^{\\,j} \\;+\\; v_i\n    $$\n    where $p \\ge 2$ is an integer defining the order of the test.\n\n### The F-Test for Nested Models\n\nThe hypothesis test for model specification is framed as a test on the coefficients of the added terms. The null hypothesis, $H_0$, states that the restricted model is correctly specified, which implies that the coefficients of the augmented terms are all zero:\n$$\nH_0: \\delta_2 = \\delta_3 = \\dots = \\delta_p = 0\n$$\nThe alternative hypothesis, $H_1$, is that at least one $\\delta_j \\neq 0$ for $j \\in \\{2, \\dots, p\\}$.\n\nSince the restricted model is nested within the unrestricted model (it is obtained by setting the $\\delta_j$ coefficients to zero), we can use an $F$-test to compare their goodness-of-fit. Let $\\text{RSS}_R$ be the Residual Sum of Squares from the OLS estimation of the restricted model, and let $\\text{RSS}_U$ be the RSS from the unrestricted model. The $F$-statistic is defined as:\n$$\nF = \\frac{(\\text{RSS}_R - \\text{RSS}_U) / q}{\\text{RSS}_U / (n - k_U)}\n$$\nThe parameters for this statistic are:\n-   $n$: The number of observations.\n-   $q$: The number of restrictions, which is the number of added regressors. Here, $q = p-1$.\n-   $k_U$: The total number of parameters in the unrestricted model. This model includes an intercept ($1$), the original predictor $x_i$ ($1$), and the powers of $\\widehat{y}_i$ from $2$ to $p$ ($p-1$ terms). Thus, $k_U = 1 + 1 + (p-1) = p+1$.\n\nThe degrees of freedom for the $F$-test are therefore $df_1 = q = p-1$ for the numerator and $df_2 = n - k_U = n - (p+1)$ for the denominator. The test statistic is:\n$$\nF = \\frac{(\\text{RSS}_R - \\text{RSS}_U) / (p-1)}{\\text{RSS}_U / (n - (p+1))}\n$$\nUnder the null hypothesis $H_0$, this statistic follows an $F$-distribution with $p-1$ and $n-(p+1)$ degrees of freedom, i.e., $F \\sim F_{p-1, n-(p+1)}$.\n\nThe decision rule for a given significance level $\\alpha$ is to reject $H_0$ if the calculated $F$-statistic exceeds the critical value $F^{(\\alpha)}_{p-1, n-(p+1)}$, which is the upper $\\alpha$-quantile of the corresponding $F$-distribution.\n\n### Monte Carlo Simulation Algorithm\n\nTo estimate the rejection probability for each parameter set $(n, \\beta_0, \\beta_1, \\gamma, \\sigma, \\alpha, R, p, \\text{seed})$, we perform the following simulation:\n\n1.  **Initialization**: For a given test case, set the random number generator seed for reproducibility. Initialize a rejection counter to zero.\n2.  **Replication Loop**: Repeat $R$ times:\n    a. **Data Generation**: Generate a dataset of size $n$. Draw $x_i \\sim \\mathcal{N}(0,1)$ and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Compute $y_i = \\beta_0 + \\beta_1 x_i + \\gamma x_i^2 + \\varepsilon_i$.\n    b. **Model Estimation**:\n       i.  Define the restricted design matrix $X_R$ of size $n \\times 2$ (a column of ones and the $x$ vector). Estimate the restricted model $y = X_R\\beta_R + u$ using OLS to obtain the fitted values $\\widehat{y} = X_R\\widehat{\\beta}_R$ and the residual sum of squares $\\text{RSS}_R$.\n       ii. Construct the powers $\\widehat{y}^2, \\dots, \\widehat{y}^p$. Form the unrestricted design matrix $X_U$ of size $n \\times (p+1)$ by augmenting $X_R$ with these new regressors. Estimate the unrestricted model $y = X_U\\beta_U + v$ using OLS to obtain $\\text{RSS}_U$.\n    c. **Hypothesis Test**:\n       i.  Calculate the degrees of freedom: $df_1 = p-1$ and $df_2 = n-(p+1)$.\n       ii. Compute the $F$-statistic. Due to finite-precision arithmetic, if $\\text{RSS}_R < \\text{RSS}_U$ or $\\text{RSS}_U$ is near zero, the statistic is ill-defined; in such cases, we do not reject $H_0$. Otherwise, compute $F = ((\\text{RSS}_R - \\text{RSS}_U)/df_1) / (\\text{RSS}_U/df_2)$.\n       iii. Find the critical value $F_{\\text{crit}} = F^{(\\alpha)}_{df_1, df_2}$ using the inverse cumulative distribution function (percent-point function) of the $F$-distribution.\n    d. **Decision**: If $F > F_{\\text{crit}}$, increment the rejection counter.\n3.  **Result Calculation**: After $R$ replications, the estimated rejection probability is the total number of rejections divided by $R$. This value is rounded to three decimal places.\n\nThis procedure is repeated for all specified test cases to evaluate the empirical size (when $\\gamma=0$) and power (when $\\gamma \\neq 0$) of the Ramsey RESET.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Implements and studies the Ramsey RESET diagnostic test via Monte Carlo simulation.\n\n    This function iterates through a suite of test cases, each defining a specific\n    data generating process and test parameters. For each case, it simulates data,\n    applies the Ramsey RESET, and calculates the empirical rejection rate over\n    many replications.\n    \"\"\"\n    test_cases = [\n        # (n, beta0, beta1, gamma, sigma, alpha, R, p, seed)\n        (200, 0, 1, 0, 1, 0.05, 500, 3, 12345),      # Case A: Empirical size\n        (200, 0, 1, 0.2, 1, 0.05, 500, 3, 54321),     # Case B: Moderate nonlinearity\n        (50, 1, 1, 0.8, 1, 0.05, 500, 2, 2024),       # Case C: Strong nonlinearity\n        (50, 0, 0, 0.5, 3, 0.1, 400, 3, 777)          # Case D: No linear effect, noisy\n    ]\n\n    results = []\n    for n, beta0, beta1, gamma, sigma, alpha, R, p, seed in test_cases:\n        # Set seed for reproducibility for each test case\n        rng = np.random.default_rng(seed)\n        rejection_count = 0\n\n        for _ in range(R):\n            # Step 1: Generate synthetic data according to the DGP\n            x = rng.normal(loc=0, scale=1, size=n)\n            epsilon = rng.normal(loc=0, scale=sigma, size=n)\n            y = beta0 + beta1 * x + gamma * x**2 + epsilon\n\n            # Step 2: Estimate the restricted linear model (y ~ 1 + x)\n            X_r = np.c_[np.ones(n), x]\n            \n            # Use np.linalg.lstsq to perform OLS regression\n            # It returns coefficients, sum of squared residuals, rank, singular values\n            beta_r_hat, residuals_r, _, _ = np.linalg.lstsq(X_r, y, rcond=None)\n            \n            # If the model is not full rank or sample size is too small, lstsq returns empty residuals\n            if residuals_r.size == 0:\n                continue # Skip this replication if OLS fails\n\n            rss_r = residuals_r[0]\n            y_hat = X_r @ beta_r_hat\n\n            # Step 3: Estimate the unrestricted model (y ~ 1 + x + y_hat^2 + ... + y_hat^p)\n            # Create the augmented regressors\n            y_hat_powers = np.array([y_hat**j for j in range(2, p + 1)]).T\n            X_u = np.c_[X_r, y_hat_powers]\n\n            beta_u_hat, residuals_u, _, _ = np.linalg.lstsq(X_u, y, rcond=None)\n\n            if residuals_u.size == 0:\n                continue # Skip if OLS fails\n\n            rss_u = residuals_u[0]\n\n            # Step 4: Compute the F-statistic\n            k_r = X_r.shape[1]  # Number of parameters in restricted model (2)\n            k_u = X_u.shape[1]  # Number of parameters in unrestricted model (p+1)\n            \n            df1 = k_u - k_r   # Numerator degrees of freedom (p-1)\n            df2 = n - k_u     # Denominator degrees of freedom (n-(p+1))\n            \n            f_statistic = 0.0\n            # Ensure valid degrees of freedom and that RSS_R > RSS_U\n            # RSS_R should be >= RSS_U. A small negative difference can occur due to floating point error.\n            if df2 > 0 and rss_u > 1e-9 and (rss_r - rss_u) > 1e-9:\n                f_statistic = ((rss_r - rss_u) / df1) / (rss_u / df2)\n\n            # Step 5: Perform the hypothesis test\n            # Get the critical value from the F-distribution\n            critical_value = f.ppf(1 - alpha, df1, df2) if df1 > 0 and df2 > 0 else np.inf\n\n            if f_statistic > critical_value:\n                rejection_count += 1\n        \n        # Calculate the empirical rejection probability (power or size)\n        rejection_prob = rejection_count / R\n        results.append(round(rejection_prob, 3))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3115010"}, {"introduction": "Beyond simply detecting non-linearity, a crucial skill is choosing an appropriate model to capture it. This advanced practice [@problem_id:3114979] sets up a comparative study between models of increasing flexibility: a simple polynomial, more adaptable splines, and a powerful kernel-based method. By using cross-validation to compare their predictive performance and determining the \"detection threshold\" for each, you will learn a systematic, modern approach to model selection and understand the trade-offs between different non-linear techniques.", "problem": "You are asked to implement a complete, runnable program that empirically diagnoses non-linearity in data by constructing a nested curriculum of diagnostics: start with a residual-based augmentation, then move to splines, then to kernel methods. Your program will compare the empirical detection thresholds of each diagnostic method under varying noise levels, using a controlled simulation design. All mathematical symbols, variables, functions, operators, and numbers must be expressed in LaTeX using inline math $...$ or display math $$...$$.\n\nThe foundational base for this problem comprises the following well-tested definitions and procedures:\n- Ordinary Least Squares (OLS): Given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, the OLS estimator $\\hat{\\beta}$ minimizes the squared error $\\|y - X \\beta\\|_2^2$.\n- Residuals: For fitted values $\\hat{y} = X \\hat{\\beta}$, residuals are $r = y - \\hat{y}$.\n- $K$-fold cross-validation: Split indices $\\{1,\\ldots,n\\}$ into $K$ disjoint folds of approximately equal size; for each fold $k$, fit a model on $\\{1,\\ldots,n\\} \\setminus \\text{fold}_k$ and evaluate the mean squared error (MSE) on $\\text{fold}_k$; aggregate across folds.\n- Student's $t$ distribution: For paired fold-wise error differences with $K$ folds, a $t$ statistic $t = \\bar{d}/(s_d/\\sqrt{K})$ has, under standard assumptions, approximately a Student's $t$ distribution with $K-1$ degrees of freedom, where $\\bar{d}$ is the sample mean and $s_d$ is the sample standard deviation of the differences.\n\nYour task is to implement the following simulation-based detection pipeline and report detection thresholds.\n\nData generating process:\n- Draw inputs $x_i \\sim \\text{Uniform}(0,1)$ independently for $i = 1,\\ldots,n$.\n- Define a linear baseline $y_i^{\\text{lin}} = \\beta_0 + \\beta_1 x_i$ and a non-linear component $g(x_i) = \\sin(2\\pi x_i)$.\n- For a given non-linearity amplitude $a \\ge 0$ and noise standard deviation $\\sigma > 0$, generate\n$$\ny_i = y_i^{\\text{lin}} + a \\, g(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2) \\text{ independently}.\n$$\n\nDiagnostics to compare (in increasing flexibility; each compared against the same linear baseline):\n1. Residual-based augmentation (polynomial add-on): augment the linear model with $x^2$ (interpretable as detecting curvature visible in a residual plot). Concretely, compare baseline $[1, x]$ versus augmented $[1, x, x^2]$.\n2. Splines: fit a cubic regression spline using a truncated power basis with internal knots at fixed locations; design matrix $[1, x, (x - t_1)_+^3, \\ldots, (x - t_{N_k})_+^3]$, where $(u)_+ = \\max(u,0)$ and $N_k$ is the number of knots.\n3. Kernel method via Random Fourier Features (RFF) approximation of the Gaussian kernel: map $x$ to a feature vector $\\phi(x) \\in \\mathbb{R}^{m}$ with entries $\\sqrt{2/m} \\cos(\\omega_j x + b_j)$ where $\\omega_j \\sim \\mathcal{N}(0, \\ell^{-2})$ and $b_j \\sim \\text{Uniform}(0,2\\pi)$, then fit linear ridge regression on the features (include an intercept).\n\nDetection logic (uniform across all diagnostics):\n- Use $K$-fold cross-validation to compute fold-wise mean squared errors for the baseline model, $\\{\\text{MSE}_{\\text{base},k}\\}_{k=1}^K$, and for a diagnostic model, $\\{\\text{MSE}_{\\text{diag},k}\\}_{k=1}^K$.\n- Compute paired differences $d_k = \\text{MSE}_{\\text{base},k} - \\text{MSE}_{\\text{diag},k}$ for $k=1,\\ldots,K$.\n- Compute the one-sided $t$ statistic\n$$\nt = \\frac{\\bar{d}}{s_d/\\sqrt{K}}, \\quad \\text{with} \\quad \\bar{d} = \\frac{1}{K}\\sum_{k=1}^K d_k,\\quad s_d^2 = \\frac{1}{K-1}\\sum_{k=1}^K (d_k - \\bar{d})^2.\n$$\n- Let $p$ be the one-sided $p$-value under a Student's $t$ distribution with $K-1$ degrees of freedom, for the alternative $\\bar{d} > 0$. Declare non-linearity detected if $\\bar{d} > 0$ and $p < \\alpha$.\n\nEmpirical detection probability and threshold:\n- For fixed $(a,\\sigma)$, repeat the data generation and detection decision independently $R$ times; estimate the detection probability as the fraction of replications that declared detection.\n- For a grid of amplitudes $\\mathcal{A} = \\{a_1 < a_2 < \\cdots < a_G\\}$, define the empirical detection threshold as the smallest $a_g \\in \\mathcal{A}$ with estimated detection probability at least a target power $\\pi^\\star$. If no $a_g$ reaches the target power, report a sentinel value.\n\nTest suite and fixed parameters:\n- Use $n = 120$, $\\beta_0 = 0$, $\\beta_1 = 1$, $K = 5$ folds, $\\alpha = 0.05$, $R = 40$, and target power $\\pi^\\star = 0.8$.\n- Use spline internal knots at $t_j \\in \\{0.2, 0.4, 0.6, 0.8\\}$.\n- For Random Fourier Features use feature dimension $m = 40$, Gaussian kernel length scale $\\ell = 0.2$, and ridge penalty $\\lambda = 10^{-3}$.\n- Amplitude grid $\\mathcal{A} = \\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$.\n- Noise levels to test: $\\sigma \\in \\{0.1, 0.5, 1.0\\}$.\n\nOutput specification:\n- For each noise level $\\sigma$ in the given order, compute the empirical detection threshold for the residual-based augmentation, the spline, and the kernel method, respectively. If the target power is not reached for any amplitude in $\\mathcal{A}$, output the sentinel value $-1.0$ for that diagnostic under that noise.\n- Your program should produce a single line of output containing the nine thresholds as a comma-separated list enclosed in square brackets, in the order\n[$\\theta_{\\text{poly}}(\\sigma=0.1)$, $\\theta_{\\text{spline}}(\\sigma=0.1)$, $\\theta_{\\text{kernel}}(\\sigma=0.1)$, $\\theta_{\\text{poly}}(\\sigma=0.5)$, $\\theta_{\\text{spline}}(\\sigma=0.5)$, $\\theta_{\\text{kernel}}(\\sigma=0.5)$, $\\theta_{\\text{poly}}(\\sigma=1.0)$, $\\theta_{\\text{spline}}(\\sigma=1.0)$, $\\theta_{\\text{kernel}}(\\sigma=1.0)].\n- Express each threshold as a decimal rounded to two digits after the decimal point.\n\nAngle units are not applicable. No physical units are involved. Percentages must not be used; all probabilities should be treated as real numbers in $[0,1]$.\n\nYour program must be self-contained, must not require input, and must follow these exact specifications. Use a fixed random seed internally to ensure reproducibility of results.", "solution": "The problem posed is to conduct a simulation-based study to empirically evaluate and compare the detection thresholds of three distinct statistical methods for diagnosing non-linearity in a regression context. The problem is well-defined, scientifically grounded in established statistical learning principles, and provides a complete set of parameters and procedures for a reproducible computational experiment. Therefore, the problem is deemed valid. We proceed with a detailed description of the solution methodology.\n\n**1. Data Generating Process (DGP)**\n\nThe simulation is predicated on a controlled data generating process. For each of the $R$ replications, we generate a dataset of size $n = 120$.\nFirst, the predictor variable $x_i$ is drawn independently from a uniform distribution:\n$$x_i \\sim \\text{Uniform}(0, 1) \\quad \\text{for } i = 1, \\dots, n$$\nThe response variable $y_i$ is constructed as a sum of a linear component, a non-linear component, and a random noise term. The linear baseline is given by $y_i^{\\text{lin}} = \\beta_0 + \\beta_1 x_i$, with specified coefficients $\\beta_0 = 0$ and $\\beta_1 = 1$. The non-linear signal is a sinusoidal function $g(x_i) = \\sin(2\\pi x_i)$. The overall model for the response is:\n$$y_i = (\\beta_0 + \\beta_1 x_i) + a \\cdot g(x_i) + \\varepsilon_i$$\nwhere $a \\ge 0$ is the amplitude of the non-linear component, and $\\varepsilon_i$ are independent and identically distributed noise terms drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. This framework allows us to systematically vary the non-linearity strength $a$ and the noise level $\\sigma$.\n\n**2. Diagnostic Methods and Model Comparison**\n\nWe compare three diagnostic models of increasing flexibility against a common baseline model. The baseline model is always the simple linear regression model, which attempts to fit the data using only an intercept and a linear term for $x$. Its design matrix is $X_{\\text{base}} = [1, x]$.\n\nThe three diagnostic methods are:\n1.  **Residual-based Augmentation (Polynomial)**: This method augments the linear model with a quadratic term, $x^2$. The design matrix for this augmented model is $X_{\\text{poly}} = [1, x, x^2]$. This model is effective at capturing simple, symmetric curvature that might be apparent in a plot of residuals from the linear model against the predictor $x$.\n\n2.  **Cubic Splines**: This method offers greater flexibility by using a piecewise polynomial function. We employ a cubic regression spline with a truncated power basis. The design matrix is constructed as:\n    $$X_{\\text{spline}} = [1, x, (x - t_1)_+^3, \\dots, (x - t_4)_+^3]$$\n    where $(u)_+ = \\max(u, 0)$ and the internal knots are fixed at $t_j \\in \\{0.2, 0.4, 0.6, 0.8\\}$. The spline model can adapt to more complex, localized non-linear patterns than a simple global polynomial.\n\n3.  **Kernel Method (via Random Fourier Features)**: This is the most flexible method, designed to approximate a model based on a Gaussian kernel $k(x, x') = \\exp(-\\|x-x'\\|^2 / (2\\ell^2))$. Direct kernel methods can be computationally intensive. Here, we use Random Fourier Features (RFF) as a computationally efficient approximation. The input $x$ is mapped into a higher-dimensional feature space of dimension $m=40$ using a feature map $\\phi(x) \\in \\mathbb{R}^{m}$. The $j$-th component of $\\phi(x)$ is:\n    $$\\phi_j(x) = \\sqrt{\\frac{2}{m}} \\cos(\\omega_j x + b_j)$$\n    where the frequencies $\\omega_j$ are drawn from $\\mathcal{N}(0, \\ell^{-2})$ and the phase shifts $b_j$ are drawn from $\\text{Uniform}(0, 2\\pi)$. The kernel length scale is fixed at $\\ell=0.2$. A linear ridge regression model is then fitted to these features, with an added intercept term. The ridge penalty $\\lambda = 10^{-3}$ is used to regularize the model and prevent overfitting in the high-dimensional feature space.\n\n**3. Statistical Detection Logic**\n\nFor each diagnostic method, its performance is compared to the baseline linear model using $K$-fold cross-validation, with $K = 5$. The dataset is split into $K$ disjoint folds. For each fold $k \\in \\{1, \\dots, K\\}$, the baseline model and the diagnostic model are trained on the other $K-1$ folds. Their respective Mean Squared Errors (MSE), denoted $\\text{MSE}_{\\text{base},k}$ and $\\text{MSE}_{\\text{diag},k}$, are then calculated on the held-out data of fold $k$.\n\nThis procedure yields $K$ paired error measurements. We compute the paired differences $d_k = \\text{MSE}_{\\text{base},k} - \\text{MSE}_{\\text{diag},k}$. A positive value for $d_k$ indicates that the diagnostic model had lower prediction error on fold $k$, suggesting it provided a better fit.\n\nTo statistically assess whether the diagnostic model is significantly better, we perform a one-sided paired $t$-test. The null hypothesis is that the diagnostic model is not better than the baseline ($\\mathbb{E}[d] \\le 0$), while the alternative hypothesis is that it is better ($\\mathbb{E}[d] > 0$). The $t$-statistic is calculated as:\n$$t = \\frac{\\bar{d}}{s_d / \\sqrt{K}}$$\nwhere $\\bar{d} = \\frac{1}{K}\\sum_{k=1}^K d_k$ is the sample mean of the differences and $s_d^2 = \\frac{1}{K-1}\\sum_{k=1}^K (d_k - \\bar{d})^2$ is the sample variance.\n\nUnder the null hypothesis, this statistic approximately follows a Student's $t$ distribution with $K-1=4$ degrees of freedom. We calculate the one-sided $p$-value. Non-linearity is declared \"detected\" if two conditions are met: the diagnostic model has a lower average cross-validation error ($\\bar{d} > 0$) and the result is statistically significant ($p < \\alpha$, with significance level $\\alpha = 0.05$).\n\n**4. Empirical Detection Threshold**\n\nThe overall simulation proceeds by testing a grid of non-linearity amplitudes $\\mathcal{A} = \\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$ for each specified noise level $\\sigma \\in \\{0.1, 0.5, 1.0\\}$.\n\nFor each pair $(a, \\sigma)$ and for each diagnostic method, we repeat the entire process of data generation and detection $R = 40$ times. The detection probability is estimated as the fraction of these $R$ replications in which non-linearity was detected.\n\nThe empirical detection threshold, $\\theta$, for a given diagnostic method and noise level, is defined as the smallest amplitude $a_g \\in \\mathcal{A}$ for which the estimated detection probability meets or exceeds a target power of $\\pi^\\star = 0.8$. If no amplitude in the grid achieves this target power, the threshold is reported as the sentinel value $-1.0$.\n\nThe final output consists of the nine computed thresholds, rounded to two decimal places, for each combination of diagnostic method and noise level, ordered as specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\ndef solve():\n    \"\"\"\n    Implements the full simulation pipeline to find detection thresholds for \n    non-linearity diagnostics.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    N_SAMPLES = 120\n    BETA_0 = 0.0\n    BETA_1 = 1.0\n    N_FOLDS = 5\n    ALPHA = 0.05\n    N_REPLICATIONS = 40\n    TARGET_POWER = 0.8\n    SPLINE_KNOTS = np.array([0.2, 0.4, 0.6, 0.8])\n    RFF_M = 40\n    RFF_L = 0.2\n    RIDGE_LAMBDA = 1e-3\n    AMPLITUDE_GRID = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n    SIGMA_LEVELS = [0.1, 0.5, 1.0]\n    SENTINEL_VALUE = -1.0\n    \n    # Set a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    def generate_data(a, sigma):\n        \"\"\"Generates data from the specified model.\"\"\"\n        x = np.random.uniform(0, 1, N_SAMPLES)\n        y_lin = BETA_0 + BETA_1 * x\n        g_x = np.sin(2 * np.pi * x)\n        epsilon = np.random.normal(0, sigma, N_SAMPLES)\n        y = y_lin + a * g_x + epsilon\n        return x, y\n\n    def solve_ols(X, y):\n        \"\"\"Solves Ordinary Least Squares.\"\"\"\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        return beta\n\n    def solve_ridge(X, y, lam):\n        \"\"\"Solves Ridge Regression with a penalty on all but the intercept.\"\"\"\n        p = X.shape[1]\n        penalty_matrix = lam * np.eye(p)\n        penalty_matrix[0, 0] = 0.0  # Do not penalize the intercept\n        A = X.T @ X + penalty_matrix\n        b = X.T @ y\n        beta = np.linalg.solve(A, b)\n        return beta\n\n    def make_design_matrix(x, method, knots=None, omega=None, b=None):\n        \"\"\"Creates the design matrix for a given method.\"\"\"\n        x_reshaped = x.reshape(-1, 1)\n        if method == 'base':\n            return np.c_[np.ones(len(x)), x]\n        elif method == 'poly':\n            return np.c_[np.ones(len(x)), x, x**2]\n        elif method == 'spline':\n            base_matrix = np.c_[np.ones(len(x)), x]\n            # Truncated power basis: (x - t)_+^3\n            spline_features = np.maximum(0, x_reshaped - knots)**3\n            return np.c_[base_matrix, spline_features]\n        elif method == 'kernel':\n            # RFF features: sqrt(2/m) * cos(omega*x + b)\n            rff_features = np.sqrt(2.0 / RFF_M) * np.cos(x_reshaped * omega + b)\n            return np.c_[np.ones(len(x)), rff_features]\n        else:\n            raise ValueError(f\"Unknown method '{method}'\")\n\n    def run_cv_comparison(x, y, diag_method_name, knots, rff_params):\n        \"\"\"\n        Performs K-fold CV and returns paired MSE differences.\n        \"\"\"\n        indices = np.arange(N_SAMPLES)\n        np.random.shuffle(indices)\n        fold_indices = np.array_split(indices, N_FOLDS)\n        \n        mse_diffs = []\n        for k in range(N_FOLDS):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[j] for j in range(N_FOLDS) if j != k])\n            \n            x_train, y_train = x[train_idx], y[train_idx]\n            x_val, y_val = x[val_idx], y[val_idx]\n            \n            # Base model\n            X_base_train = make_design_matrix(x_train, 'base')\n            beta_base = solve_ols(X_base_train, y_train)\n            X_base_val = make_design_matrix(x_val, 'base')\n            y_pred_base = X_base_val @ beta_base\n            mse_base = np.mean((y_val - y_pred_base)**2)\n            \n            # Diagnostic model\n            if diag_method_name == 'kernel':\n                omega, b = rff_params\n                X_diag_train = make_design_matrix(x_train, 'kernel', omega=omega, b=b)\n                beta_diag = solve_ridge(X_diag_train, y_train, RIDGE_LAMBDA)\n                X_diag_val = make_design_matrix(x_val, 'kernel', omega=omega, b=b)\n            else: # poly or spline\n                X_diag_train = make_design_matrix(x_train, diag_method_name, knots=knots)\n                beta_diag = solve_ols(X_diag_train, y_train)\n                X_diag_val = make_design_matrix(x_val, diag_method_name, knots=knots)\n            \n            y_pred_diag = X_diag_val @ beta_diag\n            mse_diag = np.mean((y_val - y_pred_diag)**2)\n            \n            mse_diffs.append(mse_base - mse_diag)\n            \n        return np.array(mse_diffs)\n\n    def test_detection(mse_diffs):\n        \"\"\"Performs a one-sided paired t-test.\"\"\"\n        d_bar = np.mean(mse_diffs)\n        if d_bar <= 0:\n            return False\n        \n        s_d = np.std(mse_diffs, ddof=1)\n        if s_d == 0:\n            # If all differences are identical and positive, it's a perfect improvement.\n            # This can happen in noise-free or low-noise scenarios. Treat as significant.\n            return True if d_bar > 0 else False\n        \n        t_stat = d_bar / (s_d / np.sqrt(N_FOLDS))\n        p_val = t_dist.sf(t_stat, df=N_FOLDS - 1)\n        \n        return p_val < ALPHA\n\n    all_thresholds = []\n    \n    diagnostic_methods = ['poly', 'spline', 'kernel']\n    \n    for sigma in SIGMA_LEVELS:\n        for method_name in diagnostic_methods:\n            detection_probs = {}\n            for a in AMPLITUDE_GRID:\n                detection_count = 0\n                for _ in range(N_REPLICATIONS):\n                    x_data, y_data = generate_data(a, sigma)\n                    \n                    # RFF parameters must be fixed for a given dataset (i.e., per replication)\n                    rff_p = None\n                    if method_name == 'kernel':\n                        omega = np.random.normal(0, 1.0 / RFF_L, size=RFF_M)\n                        b = np.random.uniform(0, 2 * np.pi, size=RFF_M)\n                        rff_p = (omega, b)\n\n                    mse_differences = run_cv_comparison(x_data, y_data, method_name, SPLINE_KNOTS, rff_p)\n                    \n                    if test_detection(mse_differences):\n                        detection_count += 1\n                \n                detection_probs[a] = detection_count / N_REPLICATIONS\n            \n            # Find the detection threshold\n            threshold = SENTINEL_VALUE\n            for a_val in AMPLITUDE_GRID:\n                if detection_probs[a_val] >= TARGET_POWER:\n                    threshold = a_val\n                    break\n            \n            all_thresholds.append(threshold)\n            \n    # Format the final output\n    formatted_results = [f\"{t:.2f}\" for t in all_thresholds]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3114979"}]}