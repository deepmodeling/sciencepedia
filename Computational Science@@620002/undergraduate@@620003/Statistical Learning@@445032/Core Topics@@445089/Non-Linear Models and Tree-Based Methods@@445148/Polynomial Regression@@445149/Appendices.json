{"hands_on_practices": [{"introduction": "While the monomial basis, consisting of terms like $1, x, x^2, \\dots$, is the most intuitive way to construct a polynomial, it can lead to severe numerical instability, especially for higher degrees. This hands-on practice tackles this foundational challenge by guiding you through the implementation of the Modified Gram-Schmidt process to create a stable, orthonormal polynomial basis. By comparing the results from this robust method against a direct fit, you will gain a crucial understanding of why numerical conditioning is not just a theoretical detail but a practical necessity for accurate modeling [@problem_id:3158794].", "problem": "You are given a collection of empirical sample points $\\{(x_i,y_i)\\}_{i=1}^n$ and an integer polynomial degree $d \\ge 0$. Consider the feature map defined by the monomial basis on the sample points, where the design matrix $\\Phi \\in \\mathbb{R}^{n \\times (d+1)}$ has entries $(\\Phi)_{ik} = x_i^k$ for $i \\in \\{1,\\dots,n\\}$ and $k \\in \\{0,\\dots,d\\}$. Define the empirical inner product on $\\mathbb{R}^n$ by $\\langle u, v \\rangle = \\frac{1}{n} \\sum_{i=1}^n u_i v_i$, and its induced norm $\\|u\\| = \\sqrt{\\langle u, u \\rangle}$. The task is to implement a Modified Gram–Schmidt process that orthonormalizes the columns of $\\Phi$ with respect to this empirical inner product. Using this orthonormalized basis, compute the least-squares fit to the response vector $y$ and compare the resulting polynomial coefficients to those obtained when fitting directly with the raw monomial basis.\n\nFundamental base:\n- The least-squares estimator is defined as the minimizer of the empirical risk function $R(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^d \\beta_k x_i^k \\right)^2$ over $\\beta \\in \\mathbb{R}^{d+1}$.\n- The Modified Gram–Schmidt algorithm constructs an orthonormal set of vectors from an initial set by iteratively subtracting projections and normalizing with respect to the chosen inner product.\n\nYour program must:\n1. Construct the monomial design matrix $\\Phi$ corresponding to the inputs $x_i$ and degree $d$.\n2. Implement Modified Gram–Schmidt with the empirical inner product $\\langle \\cdot, \\cdot \\rangle$ to obtain a matrix $Q \\in \\mathbb{R}^{n \\times (d+1)}$ whose columns are orthonormal and whose column span equals that of $\\Phi$.\n3. Compute the least-squares fit in the orthonormal basis and express the fitted model back in the raw monomial coefficient vector $\\beta \\in \\mathbb{R}^{d+1}$.\n4. Compute the least-squares fit directly in the raw monomial basis to obtain another coefficient vector in $\\mathbb{R}^{d+1}$.\n5. For each test case, calculate two quantities:\n   - The maximum absolute coefficient difference between the two fitted monomial coefficient vectors, given by $\\max_{0 \\le k \\le d} |\\beta^{(\\mathrm{mono})}_k - \\beta^{(\\mathrm{ortho})}_k|$, where $\\beta^{(\\mathrm{mono})}$ denotes the coefficients from fitting with $\\Phi$ and $\\beta^{(\\mathrm{ortho})}$ denotes the coefficients obtained by fitting in the orthonormal basis and mapping back to the monomial basis.\n   - The orthonormality deviation of $Q$ under the empirical inner product, measured by the Frobenius norm $\\left\\| \\frac{1}{n} Q^\\top Q - I_{d+1} \\right\\|_{\\mathrm{F}}$, where $I_{d+1}$ is the $(d+1) \\times (d+1)$ identity matrix and $\\|\\cdot\\|_{\\mathrm{F}}$ denotes the Frobenius norm.\n\nAngle unit specification:\n- Any occurrence of $\\sin(\\cdot)$ or $\\cos(\\cdot)$ in the test suite uses radians.\n\nTest suite:\nYour program must evaluate the following five test cases. Each case is specified by $(x, y, d)$, where $x \\in \\mathbb{R}^n$, $y \\in \\mathbb{R}^n$, and $d \\in \\mathbb{N}$.\n- Case A (general, low degree): $n=30$, $x_i$ equally spaced in $[-1,1]$, i.e., $x_i = -1 + \\frac{2(i-1)}{29}$ for $i=1,\\dots,30$. Define $y_i = 2 - 0.5 x_i + 1.2 x_i^2 + 0.05 \\sin(7(i-1))$, and degree $d=2$.\n- Case B (square-ish, cubic): $n=8$, $x = [0.0, 0.15, 0.27, 0.35, 0.5, 0.7, 0.85, 1.0]$. Define $y_i = -1 + 0.3 x_i + 0.7 x_i^2 - 0.2 x_i^3 + 0.02 \\cos(5(i-1))$, and degree $d=3$.\n- Case C (boundary, constant model): $n=10$, $x_i = \\frac{i-1}{9}$ for $i=1,\\dots,10$. Define $y_i = \\sin(i-1) + 0.5$, and degree $d=0$.\n- Case D (near ill-conditioned, higher degree): $n=50$, $x_i$ equally spaced in $[-1,1]$, i.e., $x_i = -1 + \\frac{2(i-1)}{49}$ for $i=1,\\dots,50$. Define $y_i = 1 + 0.001 x_i + 0.002 x_i^2 - 0.0015 x_i^3 + 0.0005 x_i^4 - 0.0003 x_i^5 + 0.0002 x_i^6 - 0.0001 x_i^7 + 0.00005 x_i^8 + 0.001 \\cos(13(i-1))$, and degree $d=8$.\n- Case E (square Vandermonde, quintic): $n=6$, $x = [-2.0, -1.0, -0.5, 0.2, 1.5, 3.0]$. Define $y_i = 0.4 - 1.1 x_i + 0.5 x_i^2 - 0.3 x_i^3 + 0.2 x_i^4 - 0.1 x_i^5 + 10^{-4} \\sin(i-1)$, and degree $d=5$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results for the five test cases, where each test case result is a list of two floats $[\\text{coeff\\_diff}, \\text{ortho\\_error}]$. The overall output must be a single list of these five lists, for example, $[[r_{A1}, r_{A2}],[r_{B1}, r_{B2}],[r_{C1}, r_{C2}],[r_{D1}, r_{D2}],[r_{E1}, r_{E2}]]$, with no additional text.", "solution": "The problem requires a comparison of two methods for solving a polynomial least-squares regression problem. The core task is to find the coefficient vector $\\beta \\in \\mathbb{R}^{d+1}$ that minimizes the empirical risk, defined as the mean squared error between the predicted values and the observed responses.\n\nGiven a set of $n$ data points $\\{(x_i, y_i)\\}_{i=1}^n$, we wish to fit a polynomial of degree $d$. The model is given by $f(x) = \\sum_{k=0}^d \\beta_k x^k$. The empirical risk function to be minimized is:\n$$ R(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2 = \\frac{1}{n} \\left\\| y - \\Phi\\beta \\right\\|_2^2 $$\nHere, $y \\in \\mathbb{R}^n$ is the vector of responses, $\\beta \\in \\mathbb{R}^{d+1}$ is the vector of coefficients to be determined, and $\\Phi \\in \\mathbb{R}^{n \\times (d+1)}$ is the design matrix. The columns of $\\Phi$ are formed by the monomial basis functions, such that $(\\Phi)_{ik} = x_i^k$ for $i \\in \\{1, \\dots, n\\}$ and $k \\in \\{0, \\dots, d\\}$.\n\nWe will explore two methods for finding the optimal coefficients $\\beta$.\n\nMethod 1: Direct Solution via Normal Equations\n\nThe standard method for solving this linear least-squares problem is to find the $\\beta$ that makes the gradient of the risk function, $\\nabla_\\beta R(\\beta)$, equal to zero.\n$$ \\nabla_\\beta R(\\beta) = \\frac{2}{n} \\Phi^\\top (\\Phi\\beta - y) = 0 $$\nThis simplifies to the well-known normal equations:\n$$ (\\Phi^\\top \\Phi) \\beta = \\Phi^\\top y $$\nThe solution, which we denote $\\beta^{(\\mathrm{mono})}$, can be found by solving this linear system. However, this approach can be numerically unstable. The condition number of the matrix $\\Phi^\\top \\Phi$ is the square of the condition number of $\\Phi$. For the monomial basis, the columns of $\\Phi$ (i.e., $x^0, x^1, \\dots, x^d$) can become nearly linearly dependent, especially for high degrees $d$ or when data points $x_i$ are clustered. This makes $\\Phi^\\top \\Phi$ ill-conditioned and the numerical solution $\\beta^{(\\mathrm{mono})}$ sensitive to small perturbations and floating-point errors.\n\nMethod 2: Orthonormal Basis Approach\n\nTo mitigate the numerical instability, we can first transform the monomial basis into an orthonormal basis. The problem specifies using the Modified Gram-Schmidt (MGS) algorithm to orthonormalize the columns of $\\Phi$, denoted $\\{\\phi_k\\}_{k=0}^d$. The orthonormality is defined with respect to the empirical inner product $\\langle u, v \\rangle = \\frac{1}{n} u^\\top v$ for $u, v \\in \\mathbb{R}^n$.\n\nThe MGS algorithm generates a new matrix $Q \\in \\mathbb{R}^{n \\times (d+1)}$ whose columns $\\{q_k\\}_{k=0}^d$ are orthonormal, i.e., $\\langle q_j, q_k \\rangle = \\delta_{jk}$, and an upper triangular matrix $R \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ such that $\\Phi = QR$. The columns of $Q$ span the same space as the columns of $\\Phi$. The MGS process proceeds as follows:\nLet $v_k$ be the working vectors, initialized as $v_k = \\phi_k$.\nFor $k = 0, \\dots, d$:\n1. Compute the norm of the current vector: $r_{kk} = \\|v_k\\| = \\sqrt{\\langle v_k, v_k \\rangle} = \\sqrt{\\frac{1}{n} v_k^\\top v_k}$.\n2. Normalize the vector to get the $k$-th orthonormal basis vector: $q_k = v_k / r_{kk}$.\n3. For all subsequent vectors $j = k+1, \\dots, d$:\n   a. Compute the projection coefficient: $r_{kj} = \\langle q_k, v_j \\rangle = \\frac{1}{n} q_k^\\top v_j$.\n   b. Subtract the projection of $v_j$ onto $q_k$: $v_j \\leftarrow v_j - r_{kj} q_k$. This step is what characterizes the \"modified\" algorithm, as it uses the updated vectors for subsequent projections, which enhances numerical stability compared to the classical version.\n\nWith the decomposition $\\Phi = QR$, the regression model can be rewritten as $y \\approx \\Phi\\beta = (QR)\\beta$. We can re-parameterize the model in terms of the orthonormal basis $Q$ with a new coefficient vector $\\alpha \\in \\mathbb{R}^{d+1}$: $y \\approx Q\\alpha$. The risk function becomes:\n$$ R(\\alpha) = \\frac{1}{n} \\|y - Q\\alpha\\|_2^2 $$\nThe corresponding normal equations are $(\\frac{1}{n} Q^\\top Q) \\alpha = \\frac{1}{n} Q^\\top y$. By construction, the columns of $Q$ are orthonormal under the specified inner product, so $\\frac{1}{n} Q^\\top Q = I_{d+1}$, the identity matrix. The system simplifies dramatically, yielding a direct solution for $\\alpha$:\n$$ \\alpha = \\frac{1}{n} Q^\\top y $$\nTo find the coefficients in the original monomial basis, we relate $\\alpha$ and $\\beta$. From $Q\\alpha \\approx \\Phi\\beta = QR\\beta$, and assuming $Q$ has full column rank, we can left-multiply by $\\frac{1}{n}Q^\\top$ to get $(\\frac{1}{n}Q^\\top Q)\\alpha \\approx (\\frac{1}{n}Q^\\top Q)R\\beta$, which simplifies to $\\alpha \\approx R\\beta$. Since $R$ is upper triangular, the system $R\\beta = \\alpha$ can be solved efficiently and stably for $\\beta$ using back substitution. We denote this solution $\\beta^{(\\mathrm{ortho})}$.\n\nThe comparison of $\\beta^{(\\mathrm{mono})}$ and $\\beta^{(\\mathrm{ortho})}$ serves as a practical demonstration of the numerical benefits of orthogonalization. Any significant difference, quantified by $\\max_{k} |\\beta^{(\\mathrm{mono})}_k - \\beta^{(\\mathrm{ortho})}_k|$, highlights the instability of the direct normal equations approach. The second metric, the orthonormality deviation $\\left\\| \\frac{1}{n} Q^\\top Q - I \\right\\|_{\\mathrm{F}}$, measures how well the MGS implementation succeeded in producing an orthonormal basis in finite-precision arithmetic.\n\nThe procedure for each test case is as follows:\n1. Construct the monomial design matrix $\\Phi$ for the given data $\\{x_i\\}$ and degree $d$.\n2. Solve $(\\Phi^\\top \\Phi) \\beta^{(\\mathrm{mono})} = \\Phi^\\top y$ for $\\beta^{(\\mathrm{mono})}$.\n3. Apply the Modified Gram-Schmidt algorithm with the empirical inner product to $\\Phi$ to obtain matrices $Q$ and $R$.\n4. Calculate $\\alpha = \\frac{1}{n} Q^\\top y$.\n5. Solve the upper triangular system $R \\beta^{(\\mathrm{ortho})} = \\alpha$ for $\\beta^{(\\mathrm{ortho})}$.\n6. Compute the maximum absolute difference between the two coefficient vectors.\n7. Compute the Frobenius norm of $\\left( \\frac{1}{n} Q^\\top Q - I \\right)$ to check the quality of the orthonormalization.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    def process_case(x: np.ndarray, y: np.ndarray, d: int):\n        \"\"\"\n        Processes a single test case for polynomial regression comparison.\n\n        Args:\n            x: Input feature vector.\n            y: Response vector.\n            d: Polynomial degree.\n\n        Returns:\n            A list containing two floats: [coefficient_difference, orthonormality_error].\n        \"\"\"\n        n = len(x)\n\n        # 1. Construct the monomial design matrix Phi\n        # Phi_{ik} = x_i^k\n        # np.vander with increasing=True produces columns x^0, x^1, ..., x^d\n        phi = np.vander(x, d + 1, increasing=True)\n\n        # 2. Compute coefficients beta_mono directly using normal equations\n        # (phi.T @ phi) @ beta = phi.T @ y\n        try:\n            lhs_mono = phi.T @ phi\n            rhs_mono = phi.T @ y\n            beta_mono = np.linalg.solve(lhs_mono, rhs_mono)\n        except np.linalg.LinAlgError:\n            # For ill-conditioned cases, use least squares solver as a fallback\n            beta_mono = np.linalg.lstsq(phi, y, rcond=None)[0]\n\n        # 3. Orthonormalize Phi using Modified Gram-Schmidt (MGS)\n        # to get Phi = QR, where Q's columns are orthonormal w.r.t.\n        # the empirical inner product <u,v> = (1/n) * u.T @ v.\n        q = np.zeros_like(phi, dtype=float)\n        r = np.zeros((d + 1, d + 1), dtype=float)\n        v = phi.copy()\n\n        for k in range(d + 1):\n            # Compute norm of v_k w.r.t. empirical inner product\n            # r_kk = ||v_k|| = sqrt(<v_k, v_k>)\n            norm_vk_sq = np.dot(v[:, k], v[:, k]) / n\n            r[k, k] = np.sqrt(norm_vk_sq)\n\n            # Normalize to get the k-th orthonormal vector q_k\n            if r[k, k] > 1e-15:\n                q[:, k] = v[:, k] / r[k, k]\n            else:\n                # Handle linear dependence, though not expected in test cases\n                q[:, k] = 0.0\n\n            # Update subsequent vectors v_j\n            for j in range(k + 1, d + 1):\n                # r_kj = <q_k, v_j>\n                r[k, j] = np.dot(q[:, k], v[:, j]) / n\n                v[:, j] = v[:, j] - r[k, j] * q[:, k]\n        \n        # 4. Compute coefficients beta_ortho using the orthonormal basis\n        # alpha = (1/n) * Q.T @ y\n        alpha = (q.T @ y) / n\n        \n        # Solve R @ beta_ortho = alpha\n        # This is an upper triangular system, solved by back substitution.\n        beta_ortho = solve_triangular(r, alpha, lower=False)\n        \n        # 5. Calculate the two required quantities\n        # Max absolute coefficient difference\n        coeff_diff = np.max(np.abs(beta_mono - beta_ortho))\n        \n        # Orthonormality deviation of Q\n        # || (1/n) * Q.T @ Q - I ||_F\n        gram_matrix = (q.T @ q) / n\n        identity = np.eye(d + 1)\n        ortho_error = np.linalg.norm(gram_matrix - identity, 'fro')\n\n        return [coeff_diff, ortho_error]\n\n    # Define test cases\n    test_cases = []\n\n    # Case A\n    n_a = 30\n    d_a = 2\n    x_a = np.linspace(-1, 1, n_a)\n    i_a = np.arange(n_a)\n    y_a = 2 - 0.5 * x_a + 1.2 * x_a**2 + 0.05 * np.sin(7 * i_a)\n    test_cases.append({'x': x_a, 'y': y_a, 'd': d_a})\n\n    # Case B\n    d_b = 3\n    x_b = np.array([0.0, 0.15, 0.27, 0.35, 0.5, 0.7, 0.85, 1.0])\n    n_b = len(x_b)\n    i_b = np.arange(n_b)\n    y_b = -1 + 0.3 * x_b + 0.7 * x_b**2 - 0.2 * x_b**3 + 0.02 * np.cos(5 * i_b)\n    test_cases.append({'x': x_b, 'y': y_b, 'd': d_b})\n    \n    # Case C\n    n_c = 10\n    d_c = 0\n    x_c = np.linspace(0, 1, n_c)\n    i_c = np.arange(n_c)\n    y_c = np.sin(i_c) + 0.5\n    test_cases.append({'x': x_c, 'y': y_c, 'd': d_c})\n    \n    # Case D\n    n_d = 50\n    d_d = 8\n    x_d = np.linspace(-1, 1, n_d)\n    i_d = np.arange(n_d)\n    y_d = (1 + 0.001 * x_d + 0.002 * x_d**2 - 0.0015 * x_d**3 +\n           0.0005 * x_d**4 - 0.0003 * x_d**5 + 0.0002 * x_d**6 -\n           0.0001 * x_d**7 + 0.00005 * x_d**8 + 0.001 * np.cos(13 * i_d))\n    test_cases.append({'x': x_d, 'y': y_d, 'd': d_d})\n    \n    # Case E\n    d_e = 5\n    x_e = np.array([-2.0, -1.0, -0.5, 0.2, 1.5, 3.0])\n    n_e = len(x_e)\n    i_e = np.arange(n_e)\n    y_e = (0.4 - 1.1 * x_e + 0.5 * x_e**2 - 0.3 * x_e**3 +\n           0.2 * x_e**4 - 0.1 * x_e**5 + 1e-4 * np.sin(i_e))\n    test_cases.append({'x': x_e, 'y': y_e, 'd': d_e})\n\n    # Process all cases and collect results\n    results = []\n    for case in test_cases:\n        result = process_case(case['x'], case['y'], case['d'])\n        results.append(result)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3158794"}, {"introduction": "A key vulnerability of standard polynomial regression, which relies on Ordinary Least Squares (OLS), is its extreme sensitivity to outliers. Because OLS minimizes the sum of squared errors, a few adversarial data points can drastically skew the entire model. This exercise provides a practical demonstration of this \"data poisoning\" and introduces a powerful solution: robust regression using the Huber loss. Through implementing an Iteratively Reweighted Least Squares (IRLS) algorithm, you will directly compare the fragility of OLS with the resilience of a robust approach, learning how to build models that are less susceptible to data contamination [@problem_id:3175135].", "problem": "You will investigate the sensitivity of polynomial regression to adversarial data poisoning by comparing Ordinary Least Squares (OLS) and a robust variant based on the Huber loss. Your task is to implement, from first principles, polynomial regression of degree $d$ with basis $\\{1, x, x^2, \\dots, x^d\\}$, and to quantify how a small set of adversarial points influences the fitted model. The evaluation will be conducted on several predefined test cases to cover a typical situation, boundary cases, and high-leverage scenarios. The final deliverable is a complete program that constructs the datasets, fits both models, computes specified sensitivity metrics, and prints the results in the exact format specified below.\n\nFundamental base to start from:\n- Definition of polynomial regression: given data $(x_i, y_i)$, fit a polynomial $p(x) = \\sum_{j=0}^{d} \\beta_j x^j$ by minimizing a data fidelity criterion.\n- For OLS, the data fidelity criterion is the sum of squared residuals $\\sum_{i=1}^{n} r_i^2$, where $r_i = y_i - p(x_i)$.\n- For the robust variant, use the Huber loss $\\rho_{\\delta}(r)$ defined as \n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^2, & \\text{if } |r| \\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nImplement the robust fit using Iteratively Reweighted Least Squares (IRLS), where at each iteration weights are derived from the Huber score function $\\psi_{\\delta}(r) = \\frac{d}{dr}\\rho_{\\delta}(r)$ and applied in a weighted least squares solve. You must estimate the scale of the residuals using the median absolute deviation (MAD) to set the Huber threshold.\n\nData generation:\n- True inlier model: a cubic polynomial $p_{\\text{true}}(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$ with coefficients $c_3 = $ $0.5$, $c_2 = $ $-0.2$, $c_1 = $ $-1.0$, $c_0 = $ $0.3$.\n- Degree for fitting: $d = $ $3$.\n- Inliers: generate $n_{\\text{in}} = $ $30$ input points $x$ uniformly on $[-1, 1]$. For each, set $y = p_{\\text{true}}(x) + \\epsilon$ with independent Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma = $ $0.05$. Use a fixed random seed $s = $ $20240217$ so that the inlier set is identical across test cases.\n- Adversarial outliers: for each test case, append a small set of specified $(x, y)$ pairs to the inliers as listed below.\n\nModel fitting requirements:\n- Ordinary Least Squares (OLS): minimize $\\sum_{i=1}^{n} r_i^2$ over $\\beta \\in \\mathbb{R}^{d+1}$.\n- Robust regression with Huber loss: minimize $\\sum_{i=1}^{n} \\rho_{\\delta}(r_i)$ with IRLS, where $r_i = y_i - \\sum_{j=0}^{d} \\beta_j x_i^j$. At each iteration, compute weights $w_i$ from\n$$\n\\psi_{\\delta}(r) = \n\\begin{cases}\nr, & \\text{if } |r| \\le \\delta,\\\\\n\\delta \\,\\mathrm{sign}(r), & \\text{if } |r| > \\delta,\n\\end{cases}\n\\quad\\text{and}\\quad\nw_i = \n\\begin{cases}\n1, & \\text{if } r_i = 0\\ \\text{or}\\ |r_i| \\le \\delta,\\\\\n\\frac{\\delta}{|r_i|}, & \\text{if } |r_i| > \\delta.\n\\end{cases}\n$$\nUse weighted least squares to update $\\beta$. Use the following specifications:\n- Scale estimate $\\hat{s}$ from residuals $r$ via median absolute deviation: $\\hat{s} = $ $1.4826$ $\\cdot \\mathrm{median}(|r - \\mathrm{median}(r)|)$. If $\\hat{s}$ is extremely small, fall back to $\\max(\\mathrm{std}(r), \\varepsilon)$.\n- Huber threshold $\\delta = \\kappa \\hat{s}$ with $\\kappa = $ $1.345$.\n- Convergence when the relative change in $\\beta$ is below $\\tau = $ $10^{-10}$ or reaching $M = $ $100$ iterations.\n- Numerical stability constant $\\varepsilon = $ $10^{-12}$ wherever needed to avoid division by zero in ratios.\n\nSensitivity metrics:\n- Let $\\beta^{\\text{clean}}_{\\text{OLS}}$ and $\\beta^{\\text{clean}}_{\\text{Huber}}$ be the coefficients fitted on inliers only, and $\\beta^{\\text{poison}}_{\\text{OLS}}$ and $\\beta^{\\text{poison}}_{\\text{Huber}}$ be the coefficients fitted on the combined inliers+outliers.\n- Coefficient drift for a method $m \\in \\{\\text{OLS}, \\text{Huber}\\}$ is $\\Delta_m = \\|\\beta^{\\text{poison}}_{m} - \\beta^{\\text{clean}}_{m}\\|_2$.\n- Drift ratio (how much more OLS drifts than robust): \n$$\nR_{\\text{drift}} = \\frac{\\Delta_{\\text{OLS}} + \\varepsilon}{\\Delta_{\\text{Huber}} + \\varepsilon}.\n$$\n- Root mean squared error (RMSE) on inliers using a coefficient vector $\\beta$ is \n$$\n\\mathrm{RMSE}_{\\text{in}}(\\beta) = \\sqrt{\\frac{1}{n_{\\text{in}}} \\sum_{i=1}^{n_{\\text{in}}} \\left(y_i^{\\text{in}} - \\sum_{j=0}^{d} \\beta_j (x_i^{\\text{in}})^j\\right)^2 }.\n$$\n- RMSE inflation for a method $m$ is \n$$\nI_m = \\frac{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{poison}}_{m})}{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{clean}}_{m})}.\n$$\n- RMSE inflation ratio:\n$$\nR_{\\text{RMSE}} = \\frac{I_{\\text{OLS}}}{I_{\\text{Huber}}}.\n$$\n\nTest suite (fixed and deterministic):\n- Global constants: $d = $ $3$, $c_3 = $ $0.5$, $c_2 = $ $-0.2$, $c_1 = $ $-1.0$, $c_0 = $ $0.3$, $n_{\\text{in}} = $ $30$, $\\sigma = $ $0.05$, $s = $ $20240217$, $\\kappa = $ $1.345$, $\\tau = $ $10^{-10}$, $M = $ $100$, $\\varepsilon = 10^{-12}$.\n- Case $1$ (no poisoning): $k = $ $0$ outliers.\n- Case $2$ (few high-leverage extremes): $k = $ $2$, outliers at $x = [-3.0, 3.0]$, $y = [60.0, -60.0]$.\n- Case $3$ (cluster of distant points): $k = $ $5$, outliers at $x = [4.0, 5.0, 6.0, 7.0, 8.0]$, $y = [200.0, 200.0, 200.0, 200.0, 200.0]$.\n- Case $4$ (vertical outliers within inlier domain): $k = $ $5$, outliers at $x = [-0.8, -0.4, 0.0, 0.4, 0.8]$, $y = [40.0, -40.0, 50.0, -50.0, 45.0]$.\n\nRequired final output format:\n- Your program must produce a single line of output containing a list of $8$ floating-point numbers: for each test case in the order $1,2,3,4$, print first $R_{\\text{drift}}$ and then $R_{\\text{RMSE}}$. Each value must be rounded to exactly $6$ decimal places. The output must be a single line containing a comma-separated list enclosed in square brackets, for example $[v_1,v_2,\\dots,v_8]$ with no additional whitespace or text.\n\nNotes:\n- There are no physical units in this problem.\n- Angles are not involved; no angle unit is required.\n- All computations must be deterministic given the specified seed.", "solution": "### Step 1: Extract Givens\n- **Polynomial Regression Model**: $p(x) = \\sum_{j=0}^{d} \\beta_j x^j$ with basis $\\{1, x, x^2, \\dots, x^d\\}$.\n- **Degree for Fitting**: $d = 3$.\n- **OLS Criterion**: Minimize $\\sum_{i=1}^{n} r_i^2$, where $r_i = y_i - p(x_i)$.\n- **Huber Loss Criterion**: Minimize $\\sum_{i=1}^{n} \\rho_{\\delta}(r_i)$, with $\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2} r^2, & \\text{if } |r| \\le \\delta \\\\ \\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r| > \\delta \\end{cases}$.\n- **IRLS for Huber Regression**: Use Iteratively Reweighted Least Squares.\n- **IRLS Weights**: $w_i = \\begin{cases} 1, & \\text{if } r_i = 0\\ \\text{or}\\ |r_i| \\le \\delta \\\\ \\frac{\\delta}{|r_i|}, & \\text{if } |r_i| > \\delta \\end{cases}$, derived from the score function $\\psi_{\\delta}(r) = \\frac{d}{dr}\\rho_{\\delta}(r)$.\n- **Scale Estimation**: $\\hat{s} = 1.4826 \\cdot \\mathrm{median}(|r - \\mathrm{median}(r)|)$, with a fallback to $\\max(\\mathrm{std}(r), \\varepsilon)$ if $\\hat{s}$ is extremely small.\n- **Huber Threshold**: $\\delta = \\kappa \\hat{s}$ with $\\kappa = 1.345$.\n- **IRLS Convergence**: Relative change in $\\beta$ below $\\tau = 10^{-10}$ or max iterations $M = 100$.\n- **Numerical Stability Constant**: $\\varepsilon = 10^{-12}$.\n- **True Inlier Model**: $p_{\\text{true}}(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$ with $c_3 = 0.5$, $c_2 = -0.2$, $c_1 = -1.0$, $c_0 = 0.3$.\n- **Inlier Data**: $n_{\\text{in}} = 30$ points with $x$ uniform on $[-1, 1]$ and $y = p_{\\text{true}}(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma = 0.05$.\n- **Random Seed**: $s = 20240217$.\n- **Adversarial Outliers**:\n    - Case 1: $k = 0$ outliers.\n    - Case 2: $k = 2$ outliers at $(x, y) = ([-3.0, 3.0], [60.0, -60.0])$.\n    - Case 3: $k = 5$ outliers at $(x, y) = ([4.0, 5.0, 6.0, 7.0, 8.0], [200.0, 200.0, 200.0, 200.0, 200.0])$.\n    - Case 4: $k = 5$ outliers at $(x, y) = ([-0.8, -0.4, 0.0, 0.4, 0.8], [40.0, -40.0, 50.0, -50.0, 45.0])$.\n- **Sensitivity Metrics**:\n    - Coefficient Drift: $\\Delta_m = \\|\\beta^{\\text{poison}}_{m} - \\beta^{\\text{clean}}_{m}\\|_2$ for $m \\in \\{\\text{OLS}, \\text{Huber}\\}$.\n    - Drift Ratio: $R_{\\text{drift}} = \\frac{\\Delta_{\\text{OLS}} + \\varepsilon}{\\Delta_{\\text{Huber}} + \\varepsilon}$.\n    - RMSE on Inliers: $\\mathrm{RMSE}_{\\text{in}}(\\beta) = \\sqrt{\\frac{1}{n_{\\text{in}}} \\sum_{i=1}^{n_{\\text{in}}} \\left(y_i^{\\text{in}} - \\sum_{j=0}^{d} \\beta_j (x_i^{\\text{in}})^j\\right)^2 }$.\n    - RMSE Inflation: $I_m = \\frac{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{poison}}_{m})}{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{clean}}_{m})}$.\n    - RMSE Inflation Ratio: $R_{\\text{RMSE}} = \\frac{I_{\\text{OLS}}}{I_{\\text{Huber}}}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on standard, fundamental concepts in numerical optimization and statistics, namely Ordinary Least Squares (OLS), robust regression, Huber loss, and the Iteratively Reweighted Least Squares (IRLS) algorithm. All definitions and procedures are standard in the field. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. OLS provides a unique solution given that the design matrix has full column rank, which is guaranteed for polynomial regression with $n_{\\text{in}}=30$ distinct $x$-values and degree $d=3$. The Huber loss function is convex, ensuring that the minimization problem has a unique solution, and the standard IRLS algorithm is a reliable method for finding it.\n- **Objective**: The problem is stated in precise, quantitative terms. All parameters, data generation procedures, algorithms, and evaluation metrics are defined formally and without ambiguity.\n- **Completeness and Consistency**: All necessary numerical values, constants, algorithms, and definitions are provided. The problem is self-contained and free of contradictions. The fixed random seed ensures reproducibility.\n- **Realism and Feasibility**: The problem is a computational simulation. The specified computations are well within the capabilities of standard numerical libraries and are computationally feasible. The values are abstract and do not represent physical quantities, thus avoiding physical implausibility.\n- **Structure and Triviality**: The problem is well-structured, guiding the user from data generation to model fitting and finally to sensitivity analysis. It is not trivial, as it requires correct implementation of a non-linear optimization algorithm (IRLS) and careful calculation of several specific metrics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Principle-Based Design\nThe objective is to compare the sensitivity of two regression methods, Ordinary Least Squares (OLS) and a robust method using Huber loss, to adversarial data poisoning. We will implement both methods to fit a degree-$d$ polynomial, $p(x) = \\sum_{j=0}^{d} \\beta_j x^j$, to datasets contaminated with outliers.\n\n**1. Linear Algebraic Formulation**\n\nBoth regression methods can be expressed within the framework of linear algebra. For a set of $n$ data points $(x_i, y_i)$, the polynomial model can be written as a system of linear equations $y \\approx X\\beta$, where $y = [y_1, \\dots, y_n]^T$ is the vector of observed values, $\\beta = [\\beta_0, \\dots, \\beta_d]^T$ is the vector of coefficients to be determined, and $X$ is the $n \\times (d+1)$ design matrix (a Vandermonde matrix):\n$$\nX = \\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{pmatrix}\n$$\nThe residual vector is defined as $r = y - X\\beta$.\n\n**2. Ordinary Least Squares (OLS) Regression**\n\nOLS fitting seeks the coefficient vector $\\beta_{\\text{OLS}}$ that minimizes the sum of squared residuals, also known as the squared L2-norm of the residual vector.\n$$\n\\text{minimize} \\quad L(\\beta) = \\sum_{i=1}^{n} r_i^2 = \\|y - X\\beta\\|_2^2\n$$\nThis is a quadratic function of $\\beta$, and its minimum can be found by setting the gradient to zero: $\\nabla_{\\beta} L(\\beta) = -2X^T(y - X\\beta) = 0$. This leads to the well-known normal equations:\n$$\n(X^T X)\\beta = X^T y\n$$\nAssuming $X^T X$ is invertible (which holds if $X$ has full column rank), the unique solution is $\\beta_{\\text{OLS}} = (X^T X)^{-1} X^T y$. Numerically, this system is best solved using methods like QR decomposition or SVD, as encapsulated in a standard linear least-squares solver.\n\n**3. Robust Regression with Huber Loss via IRLS**\n\nThe quadratic nature of the OLS loss function makes it highly sensitive to outliers, as large residuals contribute disproportionately to the total error. Robust regression aims to mitigate this by using a loss function that grows less rapidly for large residuals. The Huber loss function, $\\rho_{\\delta}(r)$, is a hybrid that behaves quadratically for small residuals but linearly for large ones:\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^2, & \\text{if } |r| \\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nThe parameter $\\delta$ is a threshold separating small from large residuals. The objective is to minimize $\\sum_{i=1}^{n} \\rho_{\\delta}(r_i)$. Since this objective is not a simple quadratic function, an iterative approach is required. Iteratively Reweighted Least Squares (IRLS) is a standard algorithm for this. It works by solving a sequence of weighted least squares problems that converge to the solution of the robust objective.\n\nThe first-order condition for minimizing the Huber objective is $\\sum_{i=1}^{n} \\psi_{\\delta}(r_i) x_{ij} = 0$ for each $j=0, \\dots, d$, where $\\psi_{\\delta}(r) = \\rho'_{\\delta}(r)$ is the score function. By defining a weight function $w(r) = \\psi_{\\delta}(r)/r$, this condition can be rewritten as $\\sum_{i=1}^{n} w(r_i) r_i x_{ij} = 0$, which are the normal equations for a weighted least squares problem with weights $w_i = w(r_i)$. The weights are given by:\n$$\nw_i = \\frac{\\psi_{\\delta}(r_i)}{r_i} = \n\\begin{cases}\n1, & \\text{if } |r_i| \\le \\delta, \\\\\n\\frac{\\delta}{|r_i|}, & \\text{if } |r_i| > \\delta.\n\\end{cases}\n$$\nThis formulation gives rise to the IRLS algorithm:\n\n1.  **Initialization**: Obtain an initial estimate for the coefficients, $\\beta^{(0)}$. The OLS solution is a suitable choice.\n2.  **Iteration**: For $k = 0, 1, 2, \\dots$ until convergence:\n    a.  **Calculate Residuals**: $r^{(k)} = y - X\\beta^{(k)}$.\n    b.  **Estimate Scale**: The threshold $\\delta$ depends on the scale of the residuals. A robust estimate of scale is the Median Absolute Deviation (MAD): $\\hat{s} = C \\cdot \\text{median}(|r^{(k)} - \\text{median}(r^{(k)})|)$, where $C = 1.4826$ makes it a consistent estimator for the standard deviation under normality. If $\\hat{s}$ is pathologically small (e.g., less than a small constant $\\varepsilon$), we fall back to a more stable estimate, $\\max(\\text{std}(r^{(k)}), \\varepsilon)$.\n    c.  **Set Threshold**: $\\delta = \\kappa \\hat{s}$, with $\\kappa = 1.345$ being a standard choice for high efficiency.\n    d.  **Compute Weights**: $W^{(k)}$ is a diagonal matrix with entries $w_i^{(k)} = \\min(1, \\delta / |r_i^{(k)}|)$. The case $r_i^{(k)} = 0$ is handled by setting $w_i^{(k)}=1$.\n    e.  **Solve Weighted Least Squares**: Update the coefficients by solving the WLS problem:\n        $$\n        \\beta^{(k+1)} = \\arg\\min_{\\beta} \\|(W^{(k)})^{1/2}(y - X\\beta)\\|_2^2\n        $$\n        This is equivalent to solving the weighted normal equations $(X^T W^{(k)} X)\\beta^{(k+1)} = X^T W^{(k)} y$.\n    f. **Check Convergence**: The process terminates if the relative change in the coefficient vector is below a tolerance $\\tau$, i.e., $\\frac{\\|\\beta^{(k+1)} - \\beta^{(k)}\\|_2}{\\|\\beta^{(k)}\\|_2 + \\varepsilon} < \\tau$, or if a maximum number of iterations $M$ is reached.\n\n**4. Data Generation and Analysis**\n\nFollowing the problem specification, a \"clean\" dataset of $n_{\\text{in}} = 30$ inliers is generated from a true cubic polynomial with added Gaussian noise. This dataset is fixed using a specific random seed. For each of the four test cases, a \"poisoned\" dataset is created by appending the specified outliers.\n\nWe first compute the \"clean\" coefficient vectors, $\\beta^{\\text{clean}}_{\\text{OLS}}$ and $\\beta^{\\text{clean}}_{\\text{Huber}}$, by fitting both models to the inlier-only dataset. Then, for each test case, we compute the \"poisoned\" vectors, $\\beta^{\\text{poison}}_{\\text{OLS}}$ and $\\beta^{\\text{poison}}_{\\text{Huber}}$, by fitting the models to the respective combined datasets.\n\nThe sensitivity of each method is quantified using two ratios:\n-   $R_{\\text{drift}}$: This measures how much more the OLS coefficients drift from their \"clean\" values compared to the Huber coefficients when outliers are introduced. A value much larger than $1$ indicates that OLS is significantly less stable.\n-   $R_{\\text{RMSE}}$: This measures how much more the OLS model's predictive accuracy on the original clean data degrades compared to the Huber model. A value much larger than $1$ shows that the OLS model is \"pulled\" further away from the true underlying trend, resulting in a poorer fit for the inliers.\n\nThe calculation proceeds by looping through the test cases, performing the required fits, and computing these metrics according to their precise mathematical definitions. For Case $1$ (no outliers), the \"poisoned\" set is identical to the \"clean\" set, leading to drift and inflation ratios of $1$ by definition.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform the polynomial regression sensitivity analysis.\n    \"\"\"\n    # ------------------ Global Constants and Test Suite Setup ------------------\n    d = 3\n    true_coeffs = np.array([0.3, -1.0, -0.2, 0.5])  # c0, c1, c2, c3\n    n_in = 30\n    sigma = 0.05\n    seed = 20240217\n    kappa = 1.345\n    tau = 1e-10\n    M = 100\n    eps = 1e-12\n\n    test_cases = [\n        # Case 1: no poisoning\n        {'name': 'Case 1', 'outliers': None},\n        # Case 2: few high-leverage extremes\n        {'name': 'Case 2', 'outliers': {'x': np.array([-3.0, 3.0]), 'y': np.array([60.0, -60.0])}},\n        # Case 3: cluster of distant points\n        {'name': 'Case 3', 'outliers': {'x': np.array([4.0, 5.0, 6.0, 7.0, 8.0]), 'y': np.array([200.0, 200.0, 200.0, 200.0, 200.0])}},\n        # Case 4: vertical outliers within inlier domain\n        {'name': 'Case 4', 'outliers': {'x': np.array([-0.8, -0.4, 0.0, 0.4, 0.8]), 'y': np.array([40.0, -40.0, 50.0, -50.0, 45.0])}}\n    ]\n\n    # ------------------ Helper Functions ------------------\n    def create_design_matrix(x, degree):\n        \"\"\"Creates the Vandermonde design matrix for polynomial regression.\"\"\"\n        return np.vander(x, degree + 1, increasing=True)\n\n    def fit_ols(X, y):\n        \"\"\"Fits OLS regression using numpy.linalg.lstsq.\"\"\"\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        return beta\n\n    def fit_huber(X, y):\n        \"\"\"Fits robust regression with Huber loss using IRLS.\"\"\"\n        # Initial guess with OLS\n        beta = fit_ols(X, y)\n        \n        for _ in range(M):\n            beta_old = beta\n            \n            # Calculate residuals\n            r = y - X @ beta\n            \n            # Estimate scale (MAD)\n            median_r = np.median(r)\n            mad = np.median(np.abs(r - median_r))\n            s = 1.4826 * mad\n            \n            # Fallback for scale estimate\n            if s < eps:\n                s = max(np.std(r), eps)\n            \n            # Set Huber threshold\n            delta = kappa * s\n            \n            # Calculate weights\n            abs_r = np.abs(r)\n            weights = np.ones_like(r)\n            # Find non-zero residuals to avoid division by zero\n            # Where abs_r > delta, weight is delta/abs_r.\n            # Where abs_r <= delta, weight is 1.\n            # Combine: weight = min(1, delta/abs_r)\n            # Use np.where to handle the abs_r == 0 case implicitly (weight remains 1)\n            weights = np.where(abs_r > eps, np.minimum(1.0, delta / abs_r), 1.0)\n            \n            # Solve weighted least squares\n            W_sqrt = np.sqrt(weights)\n            X_w = X * W_sqrt[:, np.newaxis]\n            y_w = y * W_sqrt\n            beta = fit_ols(X_w, y_w)\n            \n            # Check for convergence\n            diff = np.linalg.norm(beta - beta_old)\n            norm_beta = np.linalg.norm(beta_old)\n            if diff / (norm_beta + eps) < tau:\n                break\n        \n        return beta\n\n    def calculate_rmse(y_true, X_eval, beta):\n        \"\"\"Calculates RMSE on a given dataset.\"\"\"\n        y_pred = X_eval @ beta\n        return np.sqrt(np.mean((y_true - y_pred)**2))\n\n    # ------------------ Main Logic ------------------\n    # Generate inlier data (fixed for all test cases)\n    rng = np.random.default_rng(seed)\n    x_inlier = rng.uniform(-1, 1, n_in)\n    p_true_inlier = create_design_matrix(x_inlier, d) @ true_coeffs\n    y_inlier = p_true_inlier + rng.normal(0, sigma, n_in)\n    X_inlier = create_design_matrix(x_inlier, d)\n\n    # --- Clean Fits (done once) ---\n    beta_clean_ols = fit_ols(X_inlier, y_inlier)\n    beta_clean_huber = fit_huber(X_inlier, y_inlier)\n\n    rmse_in_clean_ols = calculate_rmse(y_inlier, X_inlier, beta_clean_ols)\n    rmse_in_clean_huber = calculate_rmse(y_inlier, X_inlier, beta_clean_huber)\n    \n    final_results = []\n\n    # --- Loop through test cases ---\n    for case in test_cases:\n        if case['outliers'] is not None:\n            x_outlier, y_outlier = case['outliers']['x'], case['outliers']['y']\n            x_poison = np.concatenate((x_inlier, x_outlier))\n            y_poison = np.concatenate((y_inlier, y_outlier))\n        else:\n            # Case 1: no poisoning, poisoned set is the clean set\n            x_poison, y_poison = x_inlier, y_inlier\n\n        X_poison = create_design_matrix(x_poison, d)\n        \n        # --- Poisoned Fits ---\n        beta_poison_ols = fit_ols(X_poison, y_poison)\n        beta_poison_huber = fit_huber(X_poison, y_poison)\n        \n        # --- Calculate Metrics ---\n        # Coefficient Drift\n        delta_ols = np.linalg.norm(beta_poison_ols - beta_clean_ols)\n        delta_huber = np.linalg.norm(beta_poison_huber - beta_clean_huber)\n        r_drift = (delta_ols + eps) / (delta_huber + eps)\n\n        # RMSE Inflation\n        rmse_in_poison_ols = calculate_rmse(y_inlier, X_inlier, beta_poison_ols)\n        rmse_in_poison_huber = calculate_rmse(y_inlier, X_inlier, beta_poison_huber)\n       \n        # Handle cases where baseline RMSE is near zero\n        inflation_ols = rmse_in_poison_ols / (rmse_in_clean_ols + eps)\n        inflation_huber = rmse_in_poison_huber / (rmse_in_clean_huber + eps)\n        \n        # In this problem, clean RMSEs are non-zero, but this is good practice\n        r_rmse = inflation_ols / (inflation_huber + eps)\n\n        final_results.append(r_drift)\n        final_results.append(r_rmse)\n\n    # Format and print the final output\n    output_str = f\"[{','.join(f'{v:.6f}' for v in final_results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3175135"}, {"introduction": "Polynomial regression fits a single, smooth function across the entire range of your data, making it a \"global\" method. This characteristic becomes a limitation when the underlying function exhibits sharp, local behaviors like jumps, corners, or narrow spikes. This coding exercise highlights this limitation by comparing the performance of a global polynomial against that of a locally adaptive cubic spline. By quantifying the local and global errors for both models on challenging datasets, you will develop a deeper intuition for the trade-offs between global and piecewise approaches and learn to identify scenarios where polynomial regression may not be the optimal tool [@problem_id:3175215].", "problem": "You are to implement and compare two regression models for one-dimensional data: a global polynomial regression and a cubic smoothing spline. Your goal is to quantify and contrast their local bias near sharp changes and their overall accuracy on a set of well-defined test cases. You must derive your approach from the foundational statement that a regression estimator is obtained by minimizing an empirical loss in the form of a sum of squared residuals, and that a cubic smoothing spline is a piecewise polynomial of degree $3$ that balances fidelity to the data against curvature.\n\nFundamental base to use:\n- Definition of least squares: Given data points $\\{(x_i,y_i)\\}_{i=1}^n$, an estimator $\\hat{f}$ is obtained by minimizing $\\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2$ over a function class.\n- Polynomial regression: The function class is $\\{ \\sum_{k=0}^p \\beta_k x^k \\}$ with degree $p$ fixed in advance.\n- Piecewise polynomials (cubic smoothing spline): The function class consists of functions that are polynomials of degree $3$ on subintervals with continuity up to the second derivative, determined by a smoothing parameter that trades off data fit and roughness.\n\nProgramming tasks and specifications:\n- For each test case below, generate a training set $\\{(x_i,y_i)\\}_{i=1}^n$ with $n=201$ points where $x_i$ are equally spaced on $[0,1]$. The true function value is $f(x_i)$, and observed values are $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma=0.01$. Use a single fixed random seed $12345$ for the entire program so that the noise is reproducible.\n- Fit a degree-$p$ polynomial regression with $p=9$ by minimizing the sum of squared residuals on the training set. Implement it by constructing the Vandermonde design matrix and solving the least squares problem.\n- Fit a cubic smoothing spline (piecewise cubic polynomial) to the same training set, with smoothing factor $s = n \\sigma^2$. This produces a piecewise polynomial of degree $3$ with continuous first and second derivatives that approximately balances the residual sum of squares against a curvature penalty. Use a standard cubic smoothing spline routine that adheres to these properties.\n- For evaluation, create a dense grid of $m=4001$ points on $[0,1]$ and compute the predicted values $\\hat{f}_{\\text{poly}}(x)$ and $\\hat{f}_{\\text{spline}}(x)$ on this grid along with the true values $f(x)$.\n- Define the local mean absolute bias in a window of half-width $h$ centered at $c$ as\n$$\n\\text{LMA} = \\frac{1}{|\\mathcal{I}|} \\sum_{x \\in \\mathcal{I}} \\left| \\hat{f}(x) - f(x) \\right| \\quad \\text{with} \\quad \\mathcal{I} = \\{ x \\in [0,1] : |x-c| \\le h \\},\n$$\ncomputed on the dense grid. Define the global mean absolute error as\n$$\n\\text{GMAE} = \\frac{1}{m} \\sum_{j=1}^{m} \\left| \\hat{f}(x_j) - f(x_j) \\right|.\n$$\n- For each test case, compute:\n    - The local mean absolute bias for the polynomial model and for the spline model.\n    - The global mean absolute error for the polynomial model and for the spline model.\n    - Two boolean indicators: whether the spline has strictly lower local mean absolute bias than the polynomial, and whether the spline has strictly lower global mean absolute error than the polynomial.\n\nTest suite:\n- Case $1$ (jump discontinuity):\n    - True function: $f_1(x) = 0.2 + 0.5 x + 0.7 H(x - 0.5)$, where $H$ is the Heaviside step function.\n    - Center $c=0.5$, half-width $h=0.05$.\n- Case $2$ (sharp corner):\n    - True function: $f_2(x) = x - x^2 + 0.5 \\, |x - 0.6|$.\n    - Center $c=0.6$, half-width $h=0.05$.\n- Case $3$ (narrow spike):\n    - True function: $f_3(x) = 0.1 \\sin(8 \\pi x) + \\exp\\!\\left( -\\frac{(x-0.7)^2}{2 \\cdot 0.02^2} \\right)$.\n    - Center $c=0.7$, half-width $h=0.03$.\n\nFinal output format:\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a sub-list containing six values in the following order:\n    $[$ local mean absolute bias of polynomial rounded to six decimal places, local mean absolute bias of spline rounded to six decimal places, a boolean indicating whether the spline’s local bias is strictly smaller, global mean absolute error of polynomial rounded to six decimal places, global mean absolute error of spline rounded to six decimal places, a boolean indicating whether the spline’s global error is strictly smaller $]$.\n- The final output must therefore be a single line of the form\n$[ [a_{1}, b_{1}, t_{1}, c_{1}, d_{1}, u_{1}], [a_{2}, b_{2}, t_{2}, c_{2}, d_{2}, u_{2}], [a_{3}, b_{3}, t_{3}, c_{3}, d_{3}, u_{3}] ]$\nwith no spaces, where $a_i,b_i,c_i,d_i$ are floats rounded to six decimals and $t_i,u_i$ are booleans. No additional text should be printed.", "solution": "The problem requires a comparative analysis of global polynomial regression and cubic smoothing splines. The analysis will be performed on three test cases featuring functions with non-smooth characteristics, such as a jump discontinuity, a sharp corner, and a narrow spike. The objective is to quantify the local bias and global accuracy of each method.\n\nThe analytical process begins from the foundational principle that a regression estimator $\\hat{f}$ for a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ is derived by minimizing the sum of squared residuals (SSR), $\\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2$, over a specified class of functions.\n\n**1. Model Formulation and Estimation Strategy**\n\n**a) Global Polynomial Regression**\nFor polynomial regression of degree $p$, the function class is the set of all polynomials of degree at most $p$. The estimator takes the form:\n$$\n\\hat{f}_{\\text{poly}}(x) = \\sum_{k=0}^{p} \\beta_k x^k = \\mathbf{x}^T \\boldsymbol{\\beta}\n$$\nwhere $\\mathbf{x}^T = [1, x, x^2, \\dots, x^p]$ and $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_p]^T$ is the vector of coefficients. The coefficients are determined by solving the linear least squares problem:\n$$\n\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n (y_i - \\hat{f}_{\\text{poly}}(x_i))^2 \\equiv \\min_{\\boldsymbol{\\beta}} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2\n$$\nHere, $\\mathbf{y} = [y_1, \\dots, y_n]^T$ is the vector of observed values, and $\\mathbf{X}$ is an $n \\times (p+1)$ Vandermonde matrix where the $(i, j)$-th entry is $X_{ij} = x_i^{j-1}$ for $j=1, \\dots, p+1$. A numerically stable method, such as one based on QR decomposition, is employed to solve this system for $\\boldsymbol{\\beta}$, rather than direct computation via the normal equations $(\\mathbf{X}^T \\mathbf{X}) \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}$.\n\n**b) Cubic Smoothing Spline**\nA cubic smoothing spline is a piecewise cubic polynomial. Unlike a global polynomial, its structure is locally adaptive. The estimator $\\hat{f}_{\\text{spline}}$ is found by solving a penalized least squares problem, which balances fidelity to the data with a penalty on the function's \"roughness.\" The optimization problem is:\n$$\n\\min_{f} \\left\\{ \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\int_{x_1}^{x_n} [f''(t)]^2 dt \\right\\}\n$$\nThe term $\\int [f''(t)]^2 dt$ is a measure of the total curvature of the function $f$. The smoothing parameter $\\lambda \\ge 0$ controls the trade-off: a larger $\\lambda$ results in a smoother function, while $\\lambda \\to 0$ leads to a function that interpolates the data. An equivalent formulation, used by many numerical libraries, is to find the function $f$ that minimizes the roughness penalty $\\int [f''(t)]^2 dt$ subject to the constraint that the SSR is below a certain threshold $s$:\n$$\n\\sum_{i=1}^n (y_i - f(x_i))^2 \\le s\n$$\nThe problem specifies using a smoothing factor $s = n\\sigma^2$, which is a standard choice. This choice implies that the total residual variance of the fit should be approximately equal to the total variance of the measurement noise, as expected for a good fit that does not overfit to the noise.\n\n**2. Algorithmic Implementation Steps**\n\nFor each specified test case, the following procedure is executed:\n\n1.  **Data Generation**: A training set of $n=201$ points $\\{(x_i, y_i)\\}_{i=1}^n$ is generated. The $x_i$ values are uniformly spaced in the interval $[0,1]$. The corresponding $y_i$ values are computed as $y_i = f(x_i) + \\epsilon_i$, where $f(x)$ is the true function for the test case and $\\epsilon_i$ are independent and identically distributed random variables drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.01$. A fixed random seed ($12345$) ensures reproducibility of the noise vector. A dense evaluation grid of $m=4001$ points is also created on $[0,1]$.\n\n2.  **Model Fitting**:\n    - For polynomial regression, a Vandermonde matrix $\\mathbf{X}$ of size $201 \\times 10$ is constructed for polynomial degree $p=9$. The linear least squares problem $\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{y}$ is solved for the coefficient vector $\\boldsymbol{\\beta}$ using `numpy.linalg.lstsq`.\n    - For the cubic smoothing spline, the `scipy.interpolate.UnivariateSpline` function is used. It is provided with the training data $(x_i, y_i)$ and the smoothing factor $s = n \\sigma^2 = 201 \\cdot (0.01)^2 = 0.0201$. The `k=3` (cubic) default is used.\n\n3.  **Prediction and Evaluation**:\n    - Both fitted models, $\\hat{f}_{\\text{poly}}$ and $\\hat{f}_{\\text{spline}}$, are used to compute predicted values on the dense evaluation grid of $m=4001$ points.\n    - Two metrics are computed to assess performance. The Global Mean Absolute Error (GMAE) is calculated over the entire evaluation grid:\n      $$\n      \\text{GMAE} = \\frac{1}{m} \\sum_{j=1}^{m} \\left| \\hat{f}(x_j) - f(x_j) \\right|\n      $$\n    - The Local Mean Absolute (LMA) bias is computed within a specific window $[c-h, c+h]$ defined for each test case:\n      $$\n      \\text{LMA} = \\frac{1}{|\\mathcal{I}|} \\sum_{x_j \\in \\mathcal{I}} \\left| \\hat{f}(x_j) - f(x_j) \\right|\n      $$\n      where $\\mathcal{I} = \\{x_j : |x_j - c| \\le h \\}$ is the set of evaluation points within the window.\n\n4.  **Comparison**: The LMA and GMAE values for the spline are compared to those for the polynomial to determine if the spline provides a strictly better fit, both locally and globally. These comparisons yield two boolean indicators per test case.\n\nThe final output compiles these six values (two LMA, two GMAE, two booleans) for each of the three test cases into a single structured list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import UnivariateSpline\n\ndef solve():\n    \"\"\"\n    Implements and compares polynomial regression and cubic smoothing splines\n    on three test cases with non-smooth features.\n    \"\"\"\n\n    # --- Problem Constants ---\n    n = 201  # Number of training points\n    m = 4001 # Number of evaluation points\n    sigma = 0.01  # Standard deviation of noise\n    p = 9 # Degree of the polynomial\n    seed = 12345 # Random seed for reproducibility\n    s = n * sigma**2 # Smoothing factor for the spline\n\n    # --- Setup Data Grids and Random Number Generator ---\n    rng = np.random.default_rng(seed)\n    x_train = np.linspace(0, 1, n)\n    x_eval = np.linspace(0, 1, m)\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            \"name\": \"Jump Discontinuity\",\n            \"f\": lambda x: 0.2 + 0.5 * x + 0.7 * np.heaviside(x - 0.5, 1.0),\n            \"c\": 0.5,\n            \"h\": 0.05\n        },\n        {\n            \"name\": \"Sharp Corner\",\n            \"f\": lambda x: x - x**2 + 0.5 * np.abs(x - 0.6),\n            \"c\": 0.6,\n            \"h\": 0.05\n        },\n        {\n            \"name\": \"Narrow Spike\",\n            \"f\": lambda x: 0.1 * np.sin(8 * np.pi * x) + np.exp(-(x - 0.7)**2 / (2 * 0.02**2)),\n            \"c\": 0.7,\n            \"h\": 0.03\n        }\n    ]\n\n    all_results_data = []\n\n    for case in test_cases:\n        f_true_func = case[\"f\"]\n        c, h = case[\"c\"], case[\"h\"]\n\n        # --- 1. Generate Data ---\n        y_true_train = f_true_func(x_train)\n        noise = rng.normal(0, sigma, n)\n        y_train = y_true_train + noise\n        y_true_eval = f_true_func(x_eval)\n\n        # --- 2. Fit Models ---\n\n        # a) Polynomial Regression\n        X_train = np.vander(x_train, p + 1, increasing=True)\n        poly_coeffs, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n        \n        X_eval = np.vander(x_eval, p + 1, increasing=True)\n        y_pred_poly = X_eval @ poly_coeffs\n\n        # b) Cubic Smoothing Spline\n        spline = UnivariateSpline(x_train, y_train, s=s, k=3)\n        y_pred_spline = spline(x_eval)\n\n        # --- 3. Calculate Metrics ---\n\n        # Global Mean Absolute Error (GMAE)\n        gmae_poly = np.mean(np.abs(y_pred_poly - y_true_eval))\n        gmae_spline = np.mean(np.abs(y_pred_spline - y_true_eval))\n        \n        # Local Mean Absolute (LMA) bias\n        local_mask = np.abs(x_eval - c) <= h\n        \n        local_y_true = y_true_eval[local_mask]\n        local_y_pred_poly = y_pred_poly[local_mask]\n        local_y_pred_spline = y_pred_spline[local_mask]\n        \n        lma_poly = np.mean(np.abs(local_y_pred_poly - local_y_true))\n        lma_spline = np.mean(np.abs(local_y_pred_spline - local_y_true))\n        \n        # --- 4. Boolean Comparisons ---\n        spline_better_local = lma_spline < lma_poly\n        spline_better_global = gmae_spline < gmae_poly\n\n        # --- 5. Store Results ---\n        case_results = [\n            lma_poly,\n            lma_spline,\n            spline_better_local,\n            gmae_poly,\n            gmae_spline,\n            spline_better_global\n        ]\n        all_results_data.append(case_results)\n\n    # --- 6. Format Final Output ---\n    # The output must be a single line string `[[...],[...],[...]]` with no spaces.\n    \n    formatted_case_strings = []\n    for res in all_results_data:\n        lma_p, lma_s, lma_b, gma_p, gma_s, gma_b = res\n        \n        # Format each part of the sub-list\n        # Floats are rounded to 6 decimal places.\n        # Booleans are converted to their string representation ('True' or 'False').\n        part_str = (f\"[{lma_p:.6f},\"\n                    f\"{lma_s:.6f},\"\n                    f\"{str(lma_b)},\"\n                    f\"{gma_p:.6f},\"\n                    f\"{gma_s:.6f},\"\n                    f\"{str(gma_b)}]\")\n        formatted_case_strings.append(part_str)\n    \n    final_output_string = f\"[{','.join(formatted_case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```", "id": "3175215"}]}