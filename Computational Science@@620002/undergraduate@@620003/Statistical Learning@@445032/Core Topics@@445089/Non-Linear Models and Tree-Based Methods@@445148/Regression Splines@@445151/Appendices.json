{"hands_on_practices": [{"introduction": "The power of regression splines comes from their flexibility, but this flexibility is controlled by the number and location of knots. This exercise provides direct, hands-on experience with the art of knot placement for unpenalized splines. By attempting to approximate a function with a very steep curve, you will compare a simple uniform knot strategy with one that intelligently concentrates knots in the more challenging region, building crucial intuition about how knot placement governs local model flexibility and approximation accuracy [@problem_id:3168964].", "problem": "You are to implement and evaluate cubic regression splines to approximate the inverse function under domain truncation away from the singularity at $x=0$. The target function is $f(x)=1/x$, but evaluation is restricted to the truncated domain $x\\in[\\tau,1]$ for a given truncation parameter $\\tau\\in(0,1)$. The approximation must use a cubic regression spline defined via the truncated power basis with a specified set of internal knots. The goals are to: construct the spline from first principles of least squares approximation, examine the influence of knot placement near $x=\\tau$ to handle the steep behavior of $f(x)$, and quantify the approximation error on a dense evaluation grid.\n\nFundamental base to be used:\n- Ordinary Least Squares (OLS): Given a set of basis functions $\\{\\phi_{j}(x)\\}_{j=1}^{p}$, data $\\{(x_{i},y_{i})\\}_{i=1}^{n}$, and a linear model $g(x)=\\sum_{j=1}^{p}\\beta_{j}\\phi_{j}(x)$, the OLS estimator minimizes $\\sum_{i=1}^{n}(y_{i}-g(x_{i}))^{2}$.\n- Regression spline basis: A cubic regression spline with internal knots $\\{t_{1},\\dots,t_{K}\\}$ can be represented using the truncated power basis $\\{1,x,x^{2},x^{3},(x-t_{1})_{+}^{3},\\dots,(x-t_{K})_{+}^{3}\\}$, where $(u)_{+}=\\max\\{u,0\\}$.\n\nImplementation requirements:\n- Use the truncated power basis functions $\\phi_{0}(x)=1$, $\\phi_{1}(x)=x$, $\\phi_{2}(x)=x^{2}$, $\\phi_{3}(x)=x^{3}$, and for each internal knot $t_{j}$, $\\phi_{3+j}(x)=(x-t_{j})_{+}^{3}$.\n- For a given $\\tau$ and a list of internal knots $\\{t_{j}\\}$ strictly inside $[\\tau,1]$, construct a design matrix $X\\in\\mathbb{R}^{n\\times p}$ at $n$ training points $\\{x_{i}\\}$ in $[\\tau,1]$ sampled uniformly. Let $y_{i}=f(x_{i})=1/x_{i}$. Compute the OLS solution $\\hat{\\beta}$ that minimizes $\\|X\\beta-y\\|_{2}^{2}$.\n- Use the fitted model $\\hat{g}(x)=\\sum_{j}\\hat{\\beta}_{j}\\phi_{j}(x)$ to compute errors on a dense evaluation grid $\\{x^{\\ast}_{\\ell}\\}$ in $[\\tau,1]$ sampled uniformly.\n- Report two error metrics on the evaluation grid: the root mean squared error $\\mathrm{RMSE}=\\sqrt{\\frac{1}{m}\\sum_{\\ell=1}^{m}\\big(\\hat{g}(x^{\\ast}_{\\ell})-f(x^{\\ast}_{\\ell})\\big)^{2}}$ and the maximum absolute error $\\mathrm{MAX}=\\max_{\\ell}|\\hat{g}(x^{\\ast}_{\\ell})-f(x^{\\ast}_{\\ell})|$.\n\nKnot placement schemes to compare:\n- Uniform internal knots: for $K$ knots, use $t_{j}=\\tau+(1-\\tau)\\cdot\\frac{j}{K+1}$ for $j=1,\\dots,K$.\n- Geometrically concentrated near $\\tau$: for $K$ knots and exponent $q1$, use $t_{j}=\\tau+(1-\\tau)\\cdot\\left(\\frac{j}{K+1}\\right)^{q}$ for $j=1,\\dots,K$, which increases density of knots near $x=\\tau$.\n\nNumerical details to enforce:\n- Use $n=N_{\\text{train}}=2000$ uniformly spaced training points on $[\\tau,1]$ for all cases.\n- Use $m=N_{\\text{test}}=10000$ uniformly spaced evaluation points on $[\\tau,1]$ for all cases.\n- All computations are unitless; no physical units are involved.\n- The final reported numbers should be rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path): $\\tau=0.1$, $K=6$, uniform knots.\n- Case $2$ (same $\\tau$ but concentrated knots): $\\tau=0.1$, $K=6$, geometric knots with $q=3$.\n- Case $3$ (stronger truncation stress): $\\tau=0.01$, $K=12$, uniform knots.\n- Case $4$ (stronger truncation with concentrated knots): $\\tau=0.01$, $K=12$, geometric knots with $q=3$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[\\mathrm{RMSE},\\mathrm{MAX}]$ rounded to $6$ decimals. For example, the output should look like $[[a_{1},b_{1}],[a_{2},b_{2}],[a_{3},b_{3}],[a_{4},b_{4}]]$ where each $a_{i}$ and $b_{i}$ are floats with $6$ decimal places corresponding to the $\\mathrm{RMSE}$ and $\\mathrm{MAX}$ for case $i$ in the order listed above.\n\nYour task is to write a complete, runnable program that implements this specification and prints the single-line output in the exact required format. No input reading is allowed; all parameters are hard-coded per the test suite above.", "solution": "The problem requires the implementation and evaluation of a cubic regression spline to approximate the function $f(x) = 1/x$ on a truncated domain $[\\tau, 1]$, where $\\tau \\in (0,1)$. The approximation is constructed using the method of Ordinary Least Squares (OLS) with a basis defined by truncated power functions.\n\nThe model for the cubic regression spline, denoted by $\\hat{g}(x)$, is a piecewise cubic polynomial that is continuous and has continuous first and second derivatives at a set of $K$ internal knots $\\{t_j\\}_{j=1}^K$. Such a function can be represented as a linear combination of basis functions. For this problem, we employ the truncated power basis. The model takes the form:\n$$\n\\hat{g}(x) = \\beta_0 \\phi_0(x) + \\beta_1 \\phi_1(x) + \\beta_2 \\phi_2(x) + \\beta_3 \\phi_3(x) + \\sum_{j=1}^{K} \\beta_{3+j} \\phi_{3+j}(x)\n$$\nwhere the basis functions are $\\phi_0(x) = 1$, $\\phi_1(x) = x$, $\\phi_2(x) = x^2$, $\\phi_3(x) = x^3$, and $\\phi_{3+j}(x) = (x-t_j)_{+}^{3}$ for each internal knot $t_j$. The function $(u)_{+} = \\max(u, 0)$ is the positive part function. This formulation defines a linear model with a total of $p = 4+K$ parameters, which are the coefficients $\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_{3+K})^T$.\n\nTo determine the coefficient vector $\\hat{\\beta}$, we utilize a set of $n=N_{\\text{train}}$ training points $\\{x_i\\}_{i=1}^n$ sampled uniformly from the domain $[\\tau, 1]$. The corresponding response values are taken from the target function, $y_i = f(x_i) = 1/x_i$. The OLS method finds the coefficient vector $\\hat{\\beta}$ that minimizes the sum of squared residuals (SSR):\n$$\n\\text{SSR}(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{g}(x_i))^2 = \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=0}^{p-1} \\beta_j \\phi_j(x_i)\\right)^2\n$$\nThis minimization problem can be expressed succinctly in matrix notation. Let $y \\in \\mathbb{R}^n$ be the vector of observed responses where $y_i = 1/x_i$. Let $X \\in \\mathbb{R}^{n \\times p}$ be the design matrix, with entries $X_{i,j} = \\phi_j(x_i)$. The OLS problem is to find the vector $\\beta$ that minimizes the squared Euclidean norm of the residual vector:\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\|y - X\\beta\\|_2^2\n$$\nThe solution to this standard linear least squares problem is given by the normal equations, $X^T X \\hat{\\beta} = X^T y$. Assuming that the matrix $X^T X$ is invertible (which holds if $n \\ge p$ and the basis functions are linearly independent over the set of training points), the unique solution is:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\nFor numerical stability, especially if $X$ is ill-conditioned, this solution is best computed using methods like QR decomposition or Singular Value Decomposition (SVD), as implemented in standard numerical libraries.\n\nOnce the optimal coefficient vector $\\hat{\\beta}$ is found, the fitted spline model $\\hat{g}(x) = \\sum_j \\hat{\\beta}_j \\phi_j(x)$ is fully determined. Its accuracy is evaluated on a separate, dense grid of $m=N_{\\text{test}}$ evaluation points $\\{x^*_\\ell\\}_{\\ell=1}^m$, also uniformly spaced on $[\\tau, 1]$. The model's predictions on this grid are $\\hat{y}^*_\\ell = \\hat{g}(x^*_\\ell)$. Performance is quantified using two error metrics:\n\n1.  Root Mean Squared Error (RMSE), which measures the average magnitude of the approximation error:\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{m} \\sum_{\\ell=1}^{m} (\\hat{y}^*_\\ell - f(x^*_\\ell))^2}\n    $$\n2.  Maximum Absolute Error (MAX), which measures the worst-case error over the evaluation grid:\n    $$\n    \\text{MAX} = \\max_{1 \\le \\ell \\le m} |\\hat{y}^*_\\ell - f(x^*_\\ell)|\n    $$\n\nThe placement of knots is critical for the spline's flexibility and approximation power. The problem specifies two strategies for placing $K$ internal knots within the interval $(\\tau, 1)$:\n\n-   **Uniform knots**: The knots are spaced evenly across the domain, providing uniform flexibility.\n    $$\n    t_j = \\tau + (1-\\tau) \\cdot \\frac{j}{K+1}, \\quad j=1, \\dots, K\n    $$\n-   **Geometrically concentrated knots**: The knots are concentrated towards the left boundary $x=\\tau$ by applying an exponent $q1$. This strategy allocates more model flexibility to the region where the function $f(x)=1/x$ is steepest and changes most rapidly.\n    $$\n    t_j = \\tau + (1-\\tau) \\cdot \\left(\\frac{j}{K+1}\\right)^q, \\quad j=1, \\dots, K\n    $$\n\nThe implementation will construct a function to generate the design matrix for any given set of points $x$ and knots $\\{t_j\\}$. This function will be used to create the training matrix $X_{\\text{train}}$ and evaluation matrix $X_{\\text{test}}$. The OLS solution $\\hat{\\beta}$ is computed using $X_{\\text{train}}$ and the corresponding $y_{\\text{train}}$. Subsequently, $\\hat{\\beta}$ is applied to $X_{\\text{test}}$ to generate predictions and compute the RMSE and MAX error metrics for each test case specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy  # Scipy is specified as an available library.\n\ndef solve():\n    \"\"\"\n    Implements and evaluates cubic regression splines for approximating f(x)=1/x\n    on a truncated domain, comparing different knot placement strategies.\n    \"\"\"\n    # Define fixed numerical details from the problem statement.\n    N_train = 2000\n    N_test = 10000\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'tau': 0.1, 'K': 6, 'knot_type': 'uniform', 'q': None},\n        {'tau': 0.1, 'K': 6, 'knot_type': 'geometric', 'q': 3},\n        {'tau': 0.01, 'K': 12, 'knot_type': 'uniform', 'q': None},\n        {'tau': 0.01, 'K': 12, 'knot_type': 'geometric', 'q': 3},\n    ]\n\n    results = []\n\n    def target_function(x):\n        \"\"\"The function to be approximated, f(x) = 1/x.\"\"\"\n        return 1.0 / x\n\n    def generate_knots(tau, K, knot_type, q):\n        \"\"\"Generates internal knots based on the specified scheme.\"\"\"\n        if K == 0:\n            return np.array([])\n        \n        # Proportions for knot placement in the (0, 1) interval\n        proportions = np.arange(1, K + 1) / (K + 1.0)\n        \n        if knot_type == 'geometric':\n            # Concentrate knots near the start of the interval\n            proportions = proportions**q\n            \n        # Scale and shift proportions to the [tau, 1] domain\n        return tau + (1.0 - tau) * proportions\n\n    def construct_design_matrix(x, knots):\n        \"\"\"Constructs the design matrix using the truncated power basis.\"\"\"\n        # Ensure x is a column vector for broadcasting purposes\n        x_col = x.reshape(-1, 1)\n        \n        # Polynomial basis functions: 1, x, x^2, x^3\n        num_poly_basis = 4\n        poly_basis = np.hstack([x_col**i for i in range(num_poly_basis)])\n        \n        if knots.size == 0:\n            return poly_basis\n            \n        # Truncated power basis functions: (x - t_j)_+^3\n        # Broadcasting x_col (n, 1) with knots (1, K) gives a (n, K) matrix\n        knots_row = knots.reshape(1, -1)\n        diff = x_col - knots_row\n        trunc_basis = np.maximum(0, diff)**3\n        \n        # Combine polynomial and truncated power basis functions\n        return np.hstack([poly_basis, trunc_basis])\n\n    for case in test_cases:\n        tau = case['tau']\n        K = case['K']\n        knot_type = case['knot_type']\n        q = case['q']\n\n        # 1. Generate training data points and response values\n        x_train = np.linspace(tau, 1, N_train)\n        y_train = target_function(x_train)\n        \n        # 2. Generate test grid and true function values\n        x_test = np.linspace(tau, 1, N_test)\n        y_test = target_function(x_test)\n\n        # 3. Generate knots for the current case\n        knots = generate_knots(tau, K, knot_type, q)\n\n        # 4. Construct the training design matrix and solve for coefficients\n        X_train = construct_design_matrix(x_train, knots)\n        # Use numpy's least squares solver for numerical stability\n        beta_hat, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n        # 5. Evaluate the fitted spline model on the test grid\n        X_test = construct_design_matrix(x_test, knots)\n        y_pred = X_test @ beta_hat\n        \n        # 6. Calculate error metrics\n        errors = y_pred - y_test\n        rmse = np.sqrt(np.mean(errors**2))\n        max_abs_error = np.max(np.abs(errors))\n        \n        results.append([rmse, max_abs_error])\n\n    # 7. Format and print the final output in the exact required format.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3168964"}, {"introduction": "Manually placing knots can be effective but laborious. An alternative approach is to use a generous number of knots and then control the model's flexibility using a penalty on its \"roughness.\" This practice delves into the theoretical heart of this method: the smoothing parameter $\\lambda$. Through a fundamental calculus derivation, you will discover how the scale of your predictor variable dramatically impacts the interpretation of the penalty term, a critical insight for robustly applying penalized splines to real-world datasets with arbitrary units [@problem_id:3168910].", "problem": "Consider a dataset $\\{(x_i,y_i)\\}_{i=1}^{n}$ with $x_i \\in [a,b]$ and a twice continuously differentiable function $f:[a,b]\\to \\mathbb{R}$. In penalized least squares smoothing with a cubic regression spline, one minimizes the criterion\n$$\nJ(f) \\;=\\; \\sum_{i=1}^{n} \\big(y_i - f(x_i)\\big)^2 \\;+\\; \\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx,\n$$\nwhere $\\lambda  0$ is a smoothing parameter controlling the trade-off between fidelity to the data and the roughness penalty $\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx$.\n\nDefine the rescaled variable $u \\in [0,1]$ by $u = (x-a)/(b-a)$ and the rescaled function $g:[0,1]\\to \\mathbb{R}$ by $g(u) = f\\big(a + (b-a)u\\big)$. Suppose one writes the corresponding rescaled penalized criterion in the form\n$$\n\\tilde{J}(g) \\;=\\; \\sum_{i=1}^{n} \\big(y_i - g(u_i)\\big)^2 \\;+\\; \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du,\n$$\nwith $u_i = (x_i-a)/(b-a)$ and some transformed smoothing parameter $\\tilde{\\lambda}  0$.\n\nUsing only fundamental facts from calculus, derive the exact transformation of the roughness penalty under this change of variables and determine the value of $\\tilde{\\lambda}$ in terms of $\\lambda$ and $b-a$. Your final answer must be a single closed-form expression for $\\tilde{\\lambda}$ in terms of $\\lambda$ and $b-a$.", "solution": "The problem is valid as it is scientifically grounded in the principles of calculus and statistical learning, is well-posed, objective, and contains a complete and consistent setup. We can proceed with the derivation.\n\nThe problem states two penalized least squares criteria, one in terms of a function $f(x)$ on the interval $[a,b]$ and another in terms of a rescaled function $g(u)$ on the interval $[0,1]$. For these two criteria to represent the same underlying optimization problem, they must be equal for any function $f$ and its corresponding rescaled version $g$. The original criterion is\n$$\nJ(f) = \\sum_{i=1}^{n} \\big(y_i - f(x_i)\\big)^2 + \\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx\n$$\nThe rescaled criterion is\n$$\n\\tilde{J}(g) = \\sum_{i=1}^{n} \\big(y_i - g(u_i)\\big)^2 + \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\nThe relationship between the functions and variables is given by $u = (x-a)/(b-a)$ and $g(u) = f(a + (b-a)u)$. The data points are related by $u_i = (x_i-a)/(b-a)$.\n\nFirst, let us examine the sum-of-squares terms. The definition of the rescaled function $g$ evaluated at the rescaled points $u_i$ is $g(u_i) = f(a + (b-a)u_i)$. By the definition of $u_i$, we have $x_i = a + (b-a)u_i$. Therefore, $g(u_i) = f(x_i)$. This implies that the sum-of-squares terms are identical:\n$$\n\\sum_{i=1}^{n} \\big(y_i - f(x_i)\\big)^2 = \\sum_{i=1}^{n} \\big(y_i - g(u_i)\\big)^2\n$$\nFor the two criteria $J(f)$ and $\\tilde{J}(g)$ to be equal, it must be true that the penalty terms are also equal:\n$$\n\\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\nOur goal is to find the relationship between $\\tilde{\\lambda}$ and $\\lambda$ by transforming the integral on the left-hand side from the variable $x$ to the variable $u$.\n\nThe transformation of variables is given by $x = a + (b-a)u$. First, we find the differential $dx$ in terms of $du$:\n$$\n\\frac{dx}{du} = b-a \\quad \\implies \\quad dx = (b-a) \\, du\n$$\nNext, we determine the limits of integration in the new variable $u$. When $x=a$, we have $u = (a-a)/(b-a) = 0$. When $x=b$, we have $u = (b-a)/(b-a) = 1$.\n\nNow, we must express the second derivative $f''(x)$ in terms of derivatives of $g$ with respect to $u$. The relationship is $g(u) = f(x(u))$, where $x(u) = a + (b-a)u$. We apply the chain rule for derivatives. The first derivative of $g(u)$ is:\n$$\ng'(u) = \\frac{dg}{du} = \\frac{df}{dx} \\frac{dx}{du} = f'(x) \\cdot (b-a)\n$$\nTo find the second derivative, we differentiate $g'(u)$ with respect to $u$, again using the chain rule:\n$$\ng''(u) = \\frac{d^2g}{du^2} = \\frac{d}{du} \\Big( f'(x(u)) \\cdot (b-a) \\Big) = (b-a) \\cdot \\frac{d}{du} \\big(f'(x(u))\\big)\n$$\n$$\ng''(u) = (b-a) \\cdot \\left( \\frac{d(f'(x))}{dx} \\frac{dx}{du} \\right) = (b-a) \\cdot \\Big( f''(x) \\cdot (b-a) \\Big) = (b-a)^2 f''(x)\n$$\nFrom this relationship, we can express $f''(x)$ in terms of $g''(u)$:\n$$\nf''(x) = \\frac{1}{(b-a)^2} g''(u)\n$$\nWe now have all the necessary components to transform the integral defining the roughness penalty. We substitute the expressions for $f''(x)$, $dx$, and the integration limits into the original integral:\n$$\n\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\int_{0}^{1} \\left( \\frac{1}{(b-a)^2} g''(u) \\right)^2 (b-a) \\, du\n$$\n$$\n\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\int_{0}^{1} \\frac{1}{(b-a)^4} \\big(g''(u)\\big)^2 (b-a) \\, du\n$$\nSimplifying the constant factor, we obtain:\n$$\n\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\frac{1}{(b-a)^3} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\nNow we return to the equality of the penalty terms:\n$$\n\\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\nSubstituting our transformed integral:\n$$\n\\lambda \\left( \\frac{1}{(b-a)^3} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du \\right) = \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\nAssuming a non-trivial smoothing problem where the roughness integral is non-zero (i.e., $g''$ is not identically zero), we can divide both sides by $\\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du$ to solve for $\\tilde{\\lambda}$:\n$$\n\\tilde{\\lambda} = \\frac{\\lambda}{(b-a)^3}\n$$\nThis is the desired relationship between the original smoothing parameter $\\lambda$ and the rescaled smoothing parameter $\\tilde{\\lambda}$.", "answer": "$$\n\\boxed{\\frac{\\lambda}{(b-a)^3}}\n$$", "id": "3168910"}, {"introduction": "This practice brings together the concepts of basis functions and penalization into a complete implementation of a penalized spline smoother. You will explore the relationship between the penalty parameter $\\lambda$ and the effective degrees of freedom, $\\mathrm{df}(\\lambda)$, which measures the model's flexibility. By implementing the smoother using both the truncated power basis and the numerically superior B-spline basis, you will numerically verify that concepts like $\\mathrm{df}(\\lambda)$ are basis-invariant while also gaining a practical appreciation for why the choice of basis is critical for stable and accurate computation [@problem_id:3168939].", "problem": "You are given a penalized least squares regression problem for cubic regression splines on a compact interval with fixed interior knots. The foundational starting point is the definition of a linear penalized least squares estimator: given a design matrix of basis functions $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ and a nonnegative penalty parameter $\\lambda \\in \\mathbb{R}_{\\ge 0}$, the estimator $\\hat{\\boldsymbol{\\beta}}$ minimizes the objective $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\,\\mathcal{J}(\\boldsymbol{\\beta})$, where $\\mathcal{J}(\\boldsymbol{\\beta})$ is a quadratic form that measures roughness of the function represented by $\\boldsymbol{\\beta}$ in the chosen spline basis. The corresponding fitted values are $\\hat{\\mathbf{y}} = \\mathbf{S}_\\lambda \\,\\mathbf{y}$, where $\\mathbf{S}_\\lambda$ is the linear smoother matrix associated with the penalized estimator. The effective degrees of freedom is defined as $\\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{S}_\\lambda)$, where $\\mathrm{tr}(\\cdot)$ denotes the matrix trace.\n\nYour task is to implement and compare two bases that span the same cubic regression spline space on $[0,1]$, using the same roughness penalty expressed as the integral of squared second derivatives, and to compute and compare the effective degrees of freedom across a test suite of penalty parameters:\n\n- Basis A: cubic B-spline basis of degree $3$ with open knot vector built from interior knots.\n- Basis B: truncated power basis for cubic regression splines, consisting of the columns $\\{1, x, x^2, x^3, (x-\\kappa_j)_+^3\\}_{j=1}^r$, where $(\\cdot)_+$ denotes the positive part and $\\{\\kappa_j\\}$ are the interior knots.\n\nUse the following specifications:\n\n- Use $n = 80$ equally spaced input locations $x_i = i/(n-1)$ for $i \\in \\{0,1,\\dots,n-1\\}$ on the interval $[0,1]$.\n- Use interior knots $\\{\\kappa_1,\\kappa_2,\\kappa_3\\} = \\{0.2, 0.5, 0.8\\}$.\n- Define the roughness penalty for a coefficient vector $\\boldsymbol{\\beta}$ representing a function $f(x)$ in a given basis as $\\mathcal{J}(\\boldsymbol{\\beta}) = \\int_0^1 \\left(f''(x)\\right)^2 \\, dx$. Implement this penalty in coefficient space via a symmetric positive semidefinite matrix $\\mathbf{\\Omega} \\in \\mathbb{R}^{p \\times p}$ constructed numerically by approximating the integral with the trapezoidal rule on a uniform grid of $G = 2001$ points on $[0,1]$. For each basis function $b_j(x)$, compute its second derivative $b_j''(x)$ on the grid and set $\\Omega_{ij} \\approx \\int_0^1 b_i''(x)\\,b_j''(x)\\,dx$ using the trapezoidal rule.\n- Construct the linear smoother matrix $\\mathbf{S}_\\lambda$ associated with the penalized least squares solution for each basis and each $\\lambda$. Compute $\\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{S}_\\lambda)$ for each basis.\n- Use the penalty parameters $\\lambda \\in \\{0, 0.1, 10, 10^6\\}$.\n\nYour program must:\n\n1. Build both design matrices $\\mathbf{X}_{\\text{B}}$ and $\\mathbf{X}_{\\text{TP}}$ for the two bases at the $n$ input locations.\n2. Build both penalty matrices $\\mathbf{\\Omega}_{\\text{B}}$ and $\\mathbf{\\Omega}_{\\text{TP}}$ via numerical integration on the grid that approximates $\\int_0^1 b_i''(x)\\,b_j''(x)\\,dx$.\n3. For each $\\lambda$ in the test suite, compute the corresponding smoother matrices $\\mathbf{S}_{\\lambda,\\text{B}}$ and $\\mathbf{S}_{\\lambda,\\text{TP}}$ and their traces $\\mathrm{df}_{\\text{B}}(\\lambda)$ and $\\mathrm{df}_{\\text{TP}}(\\lambda)$.\n4. Report, for the first three values of $\\lambda$ in the test suite $\\{0, 0.1, 10\\}$, the absolute differences $|\\mathrm{df}_{\\text{B}}(\\lambda) - \\mathrm{df}_{\\text{TP}}(\\lambda)|$, rounded to $6$ decimal places. Then, for the largest value $\\lambda = 10^6$, report $\\mathrm{df}_{\\text{B}}(10^6)$ rounded to $6$ decimal places. Use the Moore–Penrose pseudoinverse for any required matrix inverses to ensure numerical stability.\n\nAngle units do not apply. There are no physical units.\n\nTest suite and expected coverage:\n\n- $\\lambda = 0$: boundary case of no penalty; the two bases span the same space, so the corresponding ordinary least squares projection should be basis-invariant.\n- $\\lambda = 0.1$: moderate smoothing.\n- $\\lambda = 10$: strong smoothing.\n- $\\lambda = 10^6$: extremely strong smoothing; with the squared second-derivative penalty, the null space corresponds to linear functions, so the effective degrees of freedom should be close to $2$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets with $4$ floating-point numbers in the following order: the absolute degree-of-freedom differences for $\\lambda = 0$, $\\lambda = 0.1$, and $\\lambda = 10$ (in that order), followed by $\\mathrm{df}_{\\text{B}}(10^6)$, all rounded to $6$ decimal places. For example, an output line could look like $[0.000000,0.000001,0.000003,2.000000]$.", "solution": "The problem requires a comparison of two different bases for cubic regression splines—the B-spline basis and the truncated power basis—within a penalized least squares framework. The core of the task is to compute and compare the effective degrees of freedom for a spline smoother under a roughness penalty defined by the integrated squared second derivative of the fitted function.\n\nThe general form of the penalized least squares objective function is:\n$$ \\text{PLS}(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\boldsymbol{\\beta}^\\top \\mathbf{\\Omega} \\boldsymbol{\\beta} $$\nwhere $\\mathbf{X}$ is the $n \\times p$ design matrix of basis functions evaluated at the $n$ data points, $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of basis coefficients, $\\lambda \\ge 0$ is the smoothing parameter, and $\\mathbf{\\Omega}$ is a $p \\times p$ symmetric positive semidefinite matrix that defines the penalty in terms of the coefficients.\n\nThe solution $\\hat{\\boldsymbol{\\beta}}$ that minimizes this objective function is given by the normal equations:\n$$ (\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top\\mathbf{y} $$\nTo ensure numerical stability, particularly when $(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})$ may be singular or ill-conditioned, we use the Moore-Penrose pseudoinverse, denoted by $(\\cdot)^+$, yielding:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+} \\mathbf{X}^\\top \\mathbf{y} $$\nThe vector of fitted values $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ is a linear function of the observed values $\\mathbf{y}$:\n$$ \\hat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+} \\mathbf{X}^\\top \\mathbf{y} = \\mathbf{S}_\\lambda \\mathbf{y} $$\nThe matrix $\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+} \\mathbf{X}^\\top$ is known as the smoother matrix. The effective degrees of freedom of the penalized spline fit is defined as the trace of this matrix:\n$$ \\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{S}_\\lambda) $$\nUsing the cyclic property of the trace, $\\mathrm{tr}(AB) = \\mathrm{tr}(BA)$, this can be computed more efficiently as:\n$$ \\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{X}^\\top\\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+}) $$\nThis is computationally advantageous as it involves the trace of a $p \\times p$ matrix rather than an $n \\times n$ matrix, and typically $p \\ll n$.\n\nThe problem specifies the function space of cubic splines on $[0,1]$ with a set of $r=3$ interior knots $\\boldsymbol{\\kappa} = \\{0.2, 0.5, 0.8\\}$. The input data consists of $n=80$ equally spaced points $x_i = i/(n-1)$ for $i \\in \\{0, \\dots, n-1\\}$.\n\nThe roughness penalty is defined intrinsically on the function space as $\\mathcal{J}(f) = \\int_0^1 (f''(x))^2 dx$. For a function represented in a basis $f(x) = \\sum_{j=1}^p \\beta_j b_j(x)$, the penalty becomes a quadratic form $\\boldsymbol{\\beta}^\\top\\mathbf{\\Omega}\\boldsymbol{\\beta}$, where the entries of the penalty matrix are given by $\\Omega_{ij} = \\int_0^1 b_i''(x)b_j''(x)dx$. This integral is approximated numerically using the trapezoidal rule on a fine grid of $G=2001$ points on $[0,1]$.\n\nWe will now construct the design and penalty matrices for the two specified bases.\n\n**Basis A: Cubic B-spline Basis**\nFor a cubic spline ($k=3$) on $[0,1]$ with $r=3$ interior knots, we use an open knot vector to ensure the function is well-behaved at the boundaries. The knot vector $\\mathbf{t}_{\\text{B}}$ is constructed by augmenting the interior knots with $k+1 = 4$ knots at each boundary:\n$$ \\mathbf{t}_{\\text{B}} = \\{ \\underbrace{0, 0, 0, 0}_{4}, 0.2, 0.5, 0.8, \\underbrace{1, 1, 1, 1}_{4} \\} $$\nThe number of B-spline basis functions is $p_{\\text{B}} = (\\text{number of knots}) - k - 1 = 11 - 3 - 1 = 7$.\nThe design matrix $\\mathbf{X}_{\\text{B}}$ is an $n \\times p_{\\text{B}}$ matrix where $(\\mathbf{X}_{\\text{B}})_{ij} = B_{j,3}(x_i)$, with $B_{j,3}(x)$ being the $j$-th cubic B-spline basis function defined by the knot vector $\\mathbf{t}_{\\text{B}}$.\nThe penalty matrix $\\mathbf{\\Omega}_{\\text{B}}$ is a $p_{\\text{B}} \\times p_{\\text{B}}$ matrix where $(\\mathbf{\\Omega}_{\\text{B}})_{ij} \\approx \\int_0^1 B_{i,3}''(x) B_{j,3}''(x) dx$, computed numerically. The B-spline basis is known for its excellent numerical stability due to the local support of its basis functions.\n\n**Basis B: Truncated Power Basis**\nThis basis for the same spline space consists of a polynomial basis of degree $3$ plus one truncated power function for each interior knot:\n$$ \\{1, x, x^2, x^3, (x-0.2)_+^3, (x-0.5)_+^3, (x-0.8)_+^3 \\} $$\nwhere $(u)_+ = \\max(0, u)$. The number of basis functions is $p_{\\text{TP}} = 4 + r = 4 + 3 = 7$.\nThe design matrix $\\mathbf{X}_{\\text{TP}}$ is an $n \\times p_{\\text{TP}}$ matrix where the columns are these $7$ functions evaluated at the data points $x_i$.\nThe second derivatives of these basis functions are:\n$$ \\{0, 0, 2, 6x, 6(x-0.2)_+, 6(x-0.5)_+, 6(x-0.8)_+ \\} $$\nThe penalty matrix $\\mathbf{\\Omega}_{\\text{TP}}$ is a $p_{\\text{TP}} \\times p_{\\text{TP}}$ matrix where $(\\mathbf{\\Omega}_{\\text{TP}})_{ij} \\approx \\int_0^1 b_i''(x)b_j''(x)dx$. Note that the first two rows and columns of $\\mathbf{\\Omega}_{\\text{TP}}$ are zero, reflecting that the penalty does not apply to functions with zero second derivative (i.e., linear functions), which constitute the null space of the penalty. While conceptually simple, this basis is known to be numerically unstable because its basis functions are highly collinear.\n\n**Theoretical Equivalence and Numerical Implementation**\nSince both bases span the same function space and the penalty functional $\\mathcal{J}(f)$ is defined invariantly on that space, the smoother matrix $\\mathbf{S}_\\lambda$ and its trace, $\\mathrm{df}(\\lambda)$, must be theoretically identical regardless of the chosen basis. Any observed differences in the computed values of $\\mathrm{df}(\\lambda)$ arise from numerical approximation errors in the penalty matrix construction and, more significantly, from the differing numerical conditioning of the two bases. The problem's requirement to compare the effective degrees of freedom serves to numerically verify this theoretical invariance and highlight the practical consequences of basis choice.\n\nThe computational procedure is as follows:\n1.  For each basis (B-spline and Truncated Power), construct the design matrix ($\\mathbf{X}_{\\text{B}}$, $\\mathbf{X}_{\\text{TP}}$) and the penalty matrix ($\\mathbf{\\Omega}_{\\text{B}}$, $\\mathbf{\\Omega}_{\\text{TP}}$).\n2.  For each specified value of $\\lambda \\in \\{0, 0.1, 10, 10^6\\}$, compute the effective degrees of freedom $\\mathrm{df}_{\\text{B}}(\\lambda)$ and $\\mathrm{df}_{\\text{TP}}(\\lambda)$ using the formula $\\mathrm{tr}(\\mathbf{X}^\\top\\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+})$.\n3.  Calculate the absolute difference $|\\mathrm{df}_{\\text{B}}(\\lambda) - \\mathrm{df}_{\\text{TP}}(\\lambda)|$ for $\\lambda \\in \\{0, 0.1, 10\\}$.\n4.  Report the value of $\\mathrm{df}_{\\text{B}}(10^6)$ as a demonstration of the behavior under extreme penalization. As $\\lambda \\to \\infty$, the fit is forced into the null space of the penalty, which for the second-derivative penalty is the space of linear polynomials (dimension $2$). Thus, we expect $\\mathrm{df}(\\lambda) \\to 2$.\n\nThe implementation will utilize functions from `numpy` for linear algebra and `scipy.interpolate` for robust B-spline computations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import BSpline\nfrom scipy.linalg import pinv\n\ndef solve():\n    \"\"\"\n    Computes and compares effective degrees of freedom for cubic regression splines\n    using two different bases: B-splines and truncated power functions.\n    \"\"\"\n\n    # --- Problem Specifications ---\n    n = 80\n    interior_knots = np.array([0.2, 0.5, 0.8])\n    x_data = np.linspace(0, 1, n)\n    lambdas = [0, 0.1, 10, 10**6]\n\n    # Grid for numerical integration\n    G = 2001\n    z_grid = np.linspace(0, 1, G)\n    \n    # --- Helper Functions ---\n\n    def build_bspline_objects(knots, degree):\n        \"\"\"Builds B-spline basis functions and their second derivatives.\"\"\"\n        num_interior_knots = len(knots)\n        # Create open knot vector\n        knot_vector = np.concatenate(\n            ([0] * (degree + 1), knots, [1] * (degree + 1))\n        )\n        num_basis_funcs = len(knot_vector) - degree - 1\n        \n        basis_funcs = []\n        deriv2_funcs = []\n        for i in range(num_basis_funcs):\n            c = np.zeros(num_basis_funcs)\n            c[i] = 1.0\n            spl = BSpline(knot_vector, c, degree, extrapolate=False)\n            basis_funcs.append(spl)\n            deriv2_funcs.append(spl.derivative(nu=2))\n        return basis_funcs, deriv2_funcs\n\n    def build_truncated_power_basis_objects(knots, degree):\n        \"\"\"Creates callables for truncated power basis functions and their derivatives.\"\"\"\n        p = degree + 1\n        basis_funcs = []\n        deriv2_funcs = []\n\n        # Polynomial part\n        for d in range(p):\n            basis_funcs.append(lambda x, deg=d: x**deg)\n            if d  2:\n                deriv2_funcs.append(lambda x: np.zeros_like(x))\n            elif d == 2:\n                deriv2_funcs.append(lambda x: np.full_like(x, 2.0))\n            elif d == 3:\n                deriv2_funcs.append(lambda x: 6.0 * x)\n        \n        # Truncated power part\n        for k in knots:\n            basis_funcs.append(lambda x, knot=k, deg=degree: np.maximum(0, x - knot)**deg)\n            deriv2_funcs.append(lambda x, knot=k, deg=degree: deg * (deg - 1) * np.maximum(0, x - knot)**(deg-2))\n\n        return basis_funcs, deriv2_funcs\n\n    def build_design_matrix(basis_objects, x_points):\n        \"\"\"Builds the design matrix from basis functions.\"\"\"\n        p = len(basis_objects)\n        n_pts = len(x_points)\n        X = np.zeros((n_pts, p))\n        for j, func in enumerate(basis_objects):\n            X[:, j] = func(x_points)\n        return X\n\n    def build_penalty_matrix(deriv2_objects, grid):\n        \"\"\"Builds the penalty matrix Omega using numerical integration.\"\"\"\n        p = len(deriv2_objects)\n        Omega = np.zeros((p, p))\n        \n        # Pre-compute second derivatives on the grid\n        deriv2_vals = np.zeros((len(grid), p))\n        for j, func in enumerate(deriv2_objects):\n            deriv2_vals[:, j] = func(grid)\n\n        for i in range(p):\n            for j in range(i, p):\n                integrand = deriv2_vals[:, i] * deriv2_vals[:, j]\n                integral = np.trapz(integrand, grid)\n                Omega[i, j] = integral\n                if i != j:\n                    Omega[j, i] = integral\n        return Omega\n\n    def calculate_df(X, Omega, lam):\n        \"\"\"Calculates the effective degrees of freedom.\"\"\"\n        p = X.shape[1]\n        X_T_X = X.T @ X\n        M = X_T_X + lam * Omega\n        M_inv = pinv(M)\n        df_matrix = X_T_X @ M_inv\n        return np.trace(df_matrix)\n\n    # --- Main Computations ---\n    \n    # Basis A: B-splines\n    bspline_basis, bspline_deriv2 = build_bspline_objects(interior_knots, degree=3)\n    X_B = build_design_matrix(bspline_basis, x_data)\n    Omega_B = build_penalty_matrix(bspline_deriv2, z_grid)\n\n    # Basis B: Truncated Power Basis\n    tpb_basis, tpb_deriv2 = build_truncated_power_basis_objects(interior_knots, degree=3)\n    X_TP = build_design_matrix(tpb_basis, x_data)\n    Omega_TP = build_penalty_matrix(tpb_deriv2, z_grid)\n    \n    dfs_B = {lam: calculate_df(X_B, Omega_B, lam) for lam in lambdas}\n    dfs_TP = {lam: calculate_df(X_TP, Omega_TP, lam) for lam in lambdas}\n\n    # --- Format Results ---\n    \n    # Absolute differences for lambda = 0, 0.1, 10\n    diff_0 = abs(dfs_B[0] - dfs_TP[0])\n    diff_0_1 = abs(dfs_B[0.1] - dfs_TP[0.1])\n    diff_10 = abs(dfs_B[10] - dfs_TP[10])\n    \n    # df for B-spline at lambda = 10^6\n    df_B_1e6 = dfs_B[10**6]\n\n    results = [diff_0, diff_0_1, diff_10, df_B_1e6]\n    formatted_results = [f\"{v:.6f}\" for v in results]\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3168939"}]}