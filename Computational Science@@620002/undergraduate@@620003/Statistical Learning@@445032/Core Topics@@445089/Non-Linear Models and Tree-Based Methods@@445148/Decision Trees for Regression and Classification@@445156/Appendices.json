{"hands_on_practices": [{"introduction": "Decision trees excel at modeling non-linear relationships in data, a task where linear models often fall short. This hands-on practice challenges you to build and compare a decision tree and a linear classifier on a classic non-linearly separable problem, the exclusive-or (XOR) dataset [@problem_id:3113048]. By implementing both models from first principles, you will gain a concrete understanding of how a tree's recursive partitioning captures complex patterns that a single linear boundary cannot.", "problem": "Construct a program that simulates binary classification data with an exclusive-or (XOR) structure and compares the performance of a depth-$2$ decision tree classifier to a linear least-squares classifier. The data-generating process is defined as follows. Draw features $x_1$ and $x_2$ independently from the continuous uniform distribution on the unit interval, $x_1 \\sim \\text{Uniform}(0,1)$ and $x_2 \\sim \\text{Uniform}(0,1)$. For given thresholds $a \\in (0,1)$ and $b \\in (0,1)$, define the label by the XOR rule\n$$\nY = \\mathbf{1}\\big((x_1 > a) \\oplus (x_2 > b)\\big),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function and $\\oplus$ denotes logical exclusive-or. Independently corrupt labels with symmetric label noise at rate $\\eta \\in [0,1/2)$ by flipping $Y$ with probability $\\eta$.\n\nImplement two classifiers from first principles:\n\n- A linear least-squares classifier: choose coefficients $\\mathbf{w} \\in \\mathbb{R}^3$ to minimize the empirical mean squared error on the training set between the linear score $f(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2$ and the binary labels in $\\{0,1\\}$. Classify by $\\hat{Y} = \\mathbf{1}(f(\\mathbf{x}) \\ge 0.5)$. Report the misclassification rate on an independently drawn test set.\n\n- A decision tree classifier of maximum depth $2$ with axis-aligned splits, built greedily using Gini impurity. At any node containing a multiset of labels, define the Gini impurity as\n$$\nG = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2,\n$$\nwhere $p_k$ is the empirical proportion of class $k$ at that node. For a candidate split on feature $j \\in \\{1,2\\}$ at threshold $t$, partition the node into left child $\\{x_j \\le t\\}$ and right child $\\{x_j > t\\}$ and choose the split that minimizes the weighted average of child impurities. Restrict candidate thresholds to midpoints between consecutive sorted unique observed feature values at the node. Stop splitting when either the maximum depth $2$ is reached or the node is pure. Predict the majority class at leaves, breaking ties in favor of class $0$. If multiple splits yield the same impurity, break ties by choosing the smaller feature index, and if still tied, the smaller threshold.\n\nFor each test case below, you must:\n\n- Generate a training set of size $n_{\\text{train}}$ and an independent test set of size $n_{\\text{test}}$ using the specified parameters $(a,b,\\eta)$ and independent random seeds for the draws. Use the exact seeds provided to ensure reproducibility.\n\n- Train both models using only the training set.\n\n- Compute the test misclassification rate for each model as the empirical proportion of $\\hat{Y} \\ne Y$ on the test set.\n\n- Return, for each test case, the pair $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$ as real numbers.\n\nTest suite (each row denotes $(n_{\\text{train}}, n_{\\text{test}}, a, b, \\eta, \\text{seed}_{\\text{train}}, \\text{seed}_{\\text{test}})$):\n\n- Case 1: $(400, 5000, 0.5, 0.5, 0.0, 42, 4242)$.\n- Case 2: $(400, 5000, 0.2, 0.8, 0.0, 1, 11)$.\n- Case 3: $(2000, 5000, 0.5, 0.5, 0.1, 7, 77)$.\n- Case 4: $(30, 5000, 0.5, 0.5, 0.0, 2024, 2025)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself the two-element list $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$ corresponding to the cases in order. For example, the output should look like\n$$\n[[e_1^{\\text{lin}}, e_1^{\\text{tree}}],[e_2^{\\text{lin}}, e_2^{\\text{tree}}],[e_3^{\\text{lin}}, e_3^{\\text{tree}}],[e_4^{\\text{lin}}, e_4^{\\text{tree}}]]\n$$\nwith each $e_i^{\\cdot}$ a real number. No additional text should be printed.", "solution": "The problem requires the implementation and comparison of two distinct classification algorithms on a synthetic dataset exhibiting an exclusive-or (XOR) structure. The validity of the problem statement is confirmed, as it is scientifically grounded in statistical learning theory, well-posed with specific data generation protocols and deterministic algorithmic definitions, and objective in its formulation. We will proceed with a full solution.\n\nThe core of the problem lies in the data-generating process. Features $x_1$ and $x_2$ are drawn from independent uniform distributions on the interval $[0,1]$. The binary label $Y$ is defined by the XOR operation on whether these features exceed given thresholds $a$ and $b$, respectively: $Y = \\mathbf{1}\\big((x_1 > a) \\oplus (x_2 > b)\\big)$. This rule partitions the unit square feature space into four quadrants, with labels alternating between $0$ and $1$. Such a structure is not linearly separable, presenting a classic challenge for linear models. Label noise is introduced by flipping the true label $Y$ with a specified probability $\\eta$.\n\nWe will construct and evaluate two classifiers: a linear least-squares classifier and a depth-$2$ decision tree.\n\n**1. Linear Least-Squares Classifier**\n\nThis approach repurposes a linear regression model for a classification task. The model posits a linear relationship between the features and a score function, $f(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2$. For a training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$, where $\\mathbf{x}_i = (x_{i1}, x_{i2})$ and $y_i \\in \\{0, 1\\}$, the coefficients $\\mathbf{w} = (w_0, w_1, w_2)^T$ are chosen to minimize the empirical mean squared error (MSE):\n$$\n\\text{MSE}(\\mathbf{w}) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (y_i - f(\\mathbf{x}_i))^2\n$$\nThis is a standard ordinary least squares (OLS) problem. By defining a design matrix $\\mathbf{X}_b$ of size $n_{\\text{train}} \\times 3$, where the $i$-th row is $(1, x_{i1}, x_{i2})$, the objective can be written in vector form as minimizing $\\|\\mathbf{y} - \\mathbf{X}_b \\mathbf{w}\\|_2^2$. The optimal coefficient vector $\\hat{\\mathbf{w}}$ is found by solving the normal equations, yielding the closed-form solution:\n$$\n\\hat{\\mathbf{w}} = (\\mathbf{X}_b^T \\mathbf{X}_b)^{-1} \\mathbf{X}_b^T \\mathbf{y}\n$$\nwhere it is assumed that $\\mathbf{X}_b^T \\mathbf{X}_b$ is invertible. Once $\\hat{\\mathbf{w}}$ is determined from the training data, predictions on new data points are made by thresholding the linear score. The problem specifies a threshold of $0.5$:\n$$\n\\hat{Y} = \\mathbf{1}(f(\\mathbf{x}) \\ge 0.5) = \\mathbf{1}(\\hat{w}_0 + \\hat{w}_1 x_1 + \\hat{w}_2 x_2 \\ge 0.5)\n$$\n\n**2. Decision Tree Classifier**\n\nThe decision tree classifier is a non-linear model that partitions the feature space into hyper-rectangles and assigns a class label to each one. The tree is built greedily from the root downwards. At each node, the algorithm seeks the best axis-aligned split that separates the data into two child nodes.\n\nThe \"best\" split is defined as the one that maximally reduces impurity. The measure of impurity used here is the Gini impurity, defined for a node containing a multiset of labels with empirical class proportions $p_k$ for $k \\in \\{0,1\\}$ as:\n$$\nG = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2 = 1 - (p_0^2 + p_1^2)\n$$\nA Gini impurity of $0$ indicates a pure node (all samples belong to one class). For a candidate split on feature $j$ at threshold $t$, the data at a parent node $S$ is partitioned into a left child $S_L = \\{\\mathbf{x_i} \\in S \\mid x_{ij} \\le t\\}$ and a right child $S_R = \\{\\mathbf{x_i} \\in S \\mid x_{ij} > t\\}$. The quality of the split is measured by the weighted average of the Gini impurities of the children:\n$$\nI_{\\text{split}} = \\frac{|S_L|}{|S|} G(S_L) + \\frac{|S_R|}{|S|} G(S_R)\n$$\nThe algorithm exhaustively searches over all features $j \\in \\{1,2\\}$ and all valid candidate thresholds $t$ to find the split that minimizes $I_{\\text{split}}$. Candidate thresholds are restricted to the midpoints between consecutive unique sorted values of a feature observed at the current node.\n\nThe tree-building process is governed by specific rules:\n- **Maximum Depth**: Splitting stops once the tree reaches a depth of $2$. A tree of depth $0$ is a single root node; a tree of depth $2$ has a root, its children, and its grandchildren (which must be leaves).\n- **Purity**: Splitting at a node also stops if the node is pure.\n- **Prediction**: At leaf nodes, the predicted class is the majority class of the samples in that node. Ties are broken in favor of class $0$.\n- **Tie-Breaking for Splits**: If multiple splits yield the same minimal Gini impurity, the tie is broken by first choosing the one with the smaller feature index ($x_1$ before $x_2$), and if still tied, the one with the smaller threshold value.\n\n**Evaluation**\n\nFor each specified test case, we will generate a training set of size $n_{\\text{train}}$ and an independent test set of size $n_{\\text{test}}$ using the provided parameters $(a,b,\\eta)$ and random seeds. Both models will be trained on the training data. Their performance will be evaluated by computing the misclassification rate—the proportion of incorrect predictions—on the test set. The final output for each case will be the pair of misclassification rates $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$.", "answer": "```python\nimport numpy as np\n\ndef generate_data(n_samples, a, b, eta, seed):\n    \"\"\"\n    Generates synthetic XOR data with label noise.\n    \n    Args:\n        n_samples (int): Number of data points to generate.\n        a (float): Threshold for feature x1.\n        b (float): Threshold for feature x2.\n        eta (float): Symmetric label noise rate.\n        seed (int): Random seed for reproducibility.\n        \n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing features (X) and labels (Y).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.uniform(0, 1, size=(n_samples, 2))\n    x1, x2 = X[:, 0], X[:, 1]\n    \n    # True labels based on the XOR rule\n    y_true = np.logical_xor(x1 > a, x2 > b).astype(int)\n    \n    # Introduce symmetric label noise\n    noise_mask = rng.random(n_samples) < eta\n    y_noisy = y_true.copy()\n    y_noisy[noise_mask] = 1 - y_noisy[noise_mask]\n    \n    return X, y_noisy\n\nclass LinearLeastSquaresClassifier:\n    \"\"\"\n    A linear classifier trained by minimizing mean squared error.\n    \"\"\"\n    def __init__(self):\n        self.w = None\n\n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fits the linear model using the normal equations.\n        \n        Args:\n            X_train (np.ndarray): Training features.\n            y_train (np.ndarray): Training labels.\n        \"\"\"\n        X_b = np.c_[np.ones(X_train.shape[0]), X_train]\n        try:\n            # Solve (X_b^T X_b) w = X_b^T y\n            XtX = X_b.T @ X_b\n            XtX_inv = np.linalg.inv(XtX)\n            self.w = XtX_inv @ X_b.T @ y_train\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse if matrix is singular\n            self.w = np.linalg.pinv(X_b) @ y_train\n\n    def predict(self, X):\n        \"\"\"\n        Predicts labels for new data.\n        \n        Args:\n            X (np.ndarray): Features of data to predict.\n            \n        Returns:\n            np.ndarray: Predicted binary labels {0, 1}.\n        \"\"\"\n        if self.w is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n        X_b = np.c_[np.ones(X.shape[0]), X]\n        scores = X_b @ self.w\n        return (scores >= 0.5).astype(int)\n\nclass DecisionTreeClassifier:\n    \"\"\"\n    A decision tree classifier with Gini impurity and max depth.\n    \"\"\"\n    class Node:\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n            self.feature_index = feature_index\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n            self.value = value\n        \n        def is_leaf(self):\n            return self.value is not None\n\n    def __init__(self, max_depth=2):\n        self.max_depth = max_depth\n        self.root = None\n        self.n_classes_ = 2 # Fixed for this problem\n\n    def _gini(self, y):\n        \"\"\"Calculates Gini impurity.\"\"\"\n        if y.size == 0:\n            return 0.0\n        p = np.bincount(y, minlength=self.n_classes_) / y.size\n        return 1 - np.sum(p**2)\n\n    def _majority_vote(self, y):\n        \"\"\"Predicts class, breaking ties in favor of 0.\"\"\"\n        counts = np.bincount(y, minlength=self.n_classes_)\n        return 0 if counts[0] >= counts[1] else 1\n\n    def _find_best_split(self, X, y):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples, n_features = X.shape\n        if n_samples <= 1:\n            return None\n\n        parent_gini = self._gini(y)\n        best_gini = parent_gini\n        best_split = None\n\n        for feature_idx in range(n_features):\n            unique_vals = np.unique(X[:, feature_idx])\n            if unique_vals.size <= 1:\n                continue\n            \n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n            for t in thresholds:\n                left_indices = X[:, feature_idx] <= t\n                right_indices = ~left_indices\n                \n                y_left, y_right = y[left_indices], y[right_indices]\n                \n                if y_left.size == 0 or y_right.size == 0:\n                    continue\n\n                p_left = y_left.size / n_samples\n                p_right = y_right.size / n_samples\n                \n                weighted_gini = p_left * self._gini(y_left) + p_right * self._gini(y_right)\n\n                # Tie-breaking logic as per problem description\n                if weighted_gini < best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_idx, t)\n                elif weighted_gini == best_gini:\n                    if best_split is not None:\n                        if feature_idx < best_split[0]:\n                            best_split = (feature_idx, t)\n                        elif feature_idx == best_split[0] and t < best_split[1]:\n                            best_split = (feature_idx, t)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"Recursively builds the decision tree.\"\"\"\n        is_pure = len(np.unique(y)) == 1\n        is_max_depth = depth >= self.max_depth\n\n        if is_pure or is_max_depth:\n            leaf_value = self._majority_vote(y)\n            return self.Node(value=leaf_value)\n\n        best_split = self._find_best_split(X, y)\n        if best_split is None:\n            leaf_value = self._majority_vote(y)\n            return self.Node(value=leaf_value)\n            \n        feature_idx, threshold = best_split\n        left_mask = X[:, feature_idx] <= threshold\n        right_mask = ~left_mask\n        \n        left_child = self._build_tree(X[left_mask, :], y[left_mask], depth + 1)\n        right_child = self._build_tree(X[right_mask, :], y[right_mask], depth + 1)\n        \n        return self.Node(feature_idx, threshold, left_child, right_child)\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree from training data.\"\"\"\n        y_int = y.astype(int)\n        self.root = self._build_tree(X, y_int, 0)\n    \n    def _predict_single(self, x, node):\n        \"\"\"Traverses the tree for a single prediction.\"\"\"\n        if node.is_leaf():\n            return node.value\n        \n        if x[node.feature_index] <= node.threshold:\n            return self._predict_single(x, node.left)\n        else:\n            return self._predict_single(x, node.right)\n            \n    def predict(self, X):\n        \"\"\"Predicts labels for new data.\"\"\"\n        if self.root is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n        return np.array([self._predict_single(x, self.root) for x in X])\n\ndef solve():\n    test_cases = [\n        (400, 5000, 0.5, 0.5, 0.0, 42, 4242),\n        (400, 5000, 0.2, 0.8, 0.0, 1, 11),\n        (2000, 5000, 0.5, 0.5, 0.1, 7, 77),\n        (30, 5000, 0.5, 0.5, 0.0, 2024, 2025),\n    ]\n\n    results = []\n    for params in test_cases:\n        n_train, n_test, a, b, eta, seed_train, seed_test = params\n\n        # Generate data\n        X_train, y_train = generate_data(n_train, a, b, eta, seed_train)\n        X_test, y_test = generate_data(n_test, a, b, eta, seed_test)\n\n        # Linear Least-Squares Classifier\n        linear_model = LinearLeastSquaresClassifier()\n        linear_model.fit(X_train, y_train)\n        y_pred_linear = linear_model.predict(X_test)\n        error_linear = np.mean(y_pred_linear != y_test)\n        \n        # Decision Tree Classifier\n        tree_model = DecisionTreeClassifier(max_depth=2)\n        tree_model.fit(X_train, y_train)\n        y_pred_tree = tree_model.predict(X_test)\n        error_tree = np.mean(y_pred_tree != y_test)\n        \n        results.append([error_linear, error_tree])\n    \n    # Format the final output string exactly as requested\n    output_str = \"[\" + \",\".join([f\"[{e_lin},{e_tree}]\" for e_lin, e_tree in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3113048"}, {"introduction": "While powerful, the greedy algorithm used to build decision trees can easily overfit by exploiting spurious correlations, especially those present in small data subgroups. This exercise provides a practical demonstration of this risk and introduces a key regularization technique to mitigate it: the minimum child weight constraint [@problem_id:3112969]. You will construct a scenario where a greedy decision stump is tempted by a misleading feature and test how this simple constraint guides the model toward a more generalizable decision.", "problem": "Consider binary classification with decision trees using greedy splits driven by empirical risk minimization. The empirical risk for a leaf that predicts a single class is defined by the misclassification rate. A decision stump is a decision tree of depth $1$ that splits once on a single feature threshold. Let the training set be $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$, and let all training samples have unit weight. A split on feature $j \\in \\{0,1\\}$ at threshold $\\tau$ partitions the data into a left child $\\{i : x_{i,j} \\le \\tau\\}$ and a right child $\\{i : x_{i,j} > \\tau\\}$. The empirical risk of the stump is the weighted sum of child misclassification rates. A minimum child weight constraint requires that each child must contain at least $w_{\\min}$ total sample weight, which with unit weights is equivalent to at least $w_{\\min}$ samples per child. The greedy stump selects the feature and threshold that minimize empirical risk subject to the minimum child weight constraint. If no valid split reduces the parent empirical risk, the stump defaults to predicting the majority class without splitting.\n\nYou will construct a scientifically plausible scenario with spurious correlations arising in small subgroups that can mislead a greedy split. The data-generation process is as follows, with all random variables independent:\n\n- Training set size $N = 200$ and test set size $M = 5000$.\n- Feature $x_0$ is drawn as $x_{0} \\sim \\mathcal{N}(0,1)$ for each sample. The label is generated by $y = \\mathbb{1}\\{x_{0} + \\epsilon > 0\\}$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 3.0$; this yields a weakly informative feature $x_0$ due to high noise.\n- Feature $x_1$ for training is drawn as $x_{1} \\sim \\mathrm{Uniform}(-2,2)$ except for a small subgroup of size $s = 15$ among the positive-labeled training samples, for which $x_{1}$ is set to an extreme value $10$. This creates a spurious correlation in a small subgroup that a greedy algorithm can exploit by isolating this subgroup with a high threshold on $x_1$.\n- Feature $x_1$ for test is drawn as $x_{1} \\sim \\mathrm{Uniform}(-2,2)$ for all samples with no extremes, so the spurious pattern does not generalize.\n\nDefine the empirical risk of a leaf that predicts the majority class within that leaf as the fraction of samples in the leaf not belonging to the majority class. For a split, the empirical risk is the sum of misclassifications across children divided by $N$. The greedy stump examines thresholds at midpoints between consecutive sorted distinct feature values for each feature. It selects the split that yields the smallest empirical risk and satisfies the minimum child weight constraint $w_{\\min}$ for both children; ties are broken by choosing the smaller feature index and then the smaller threshold. If no valid split reduces the parent empirical risk, the stump performs no split and predicts the overall majority class.\n\nTask:\n- Implement the described data generation and decision stump learning with the minimum child weight constraint.\n- Train the stump on the training set and test whether the learned split is spurious by checking if the selected feature is $x_1$. If the stump does not split or selects $x_0$, treat this as having avoided the spurious split.\n\nTest suite:\n- Use the fixed training and test distributions above with $N = 200$, $M = 5000$, and $s = 15$.\n- Evaluate the following values of the minimum child weight $w_{\\min}$:\n  1. $w_{\\min} = 1$ (no effective constraint).\n  2. $w_{\\min} = 15$ (boundary equals the spurious subgroup size).\n  3. $w_{\\min} = 16$ (just above the spurious subgroup size).\n  4. $w_{\\min} = 500$ (so large that no split is possible).\n\nFor each case, your program must output a boolean indicating whether the learned stump avoids the spurious split, as defined above. The final output format must be a single line containing a list of the four booleans in the exact order of the test suite, printed as a comma-separated list enclosed in square brackets, for example, $[b_1,b_2,b_3,b_4]$, where each $b_i$ is either True or False.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Model**: Decision stump (depth-1 decision tree) for binary classification.\n- **Data**: Training set $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$. All sample weights are $1$.\n- **Objective**: Greedy split selection based on minimizing empirical risk.\n- **Split Rule**: A split on feature $j$ at threshold $\\tau$ creates a left child $\\{i : x_{i,j} \\le \\tau\\}$ and a right child $\\{i : x_{i,j} > \\tau\\}$.\n- **Leaf Risk**: The empirical risk of a leaf is its misclassification rate, i.e., the fraction of samples not belonging to the majority class in that leaf.\n- **Stump Risk**: The empirical risk of the stump is the sum of misclassifications across both children, divided by the total number of samples $N$.\n- **Constraint**: A minimum child weight constraint $w_{\\min}$ requires each child node to contain at least $w_{\\min}$ samples.\n- **Candidate Thresholds**: Midpoints between consecutive sorted distinct feature values.\n- **Tie-Breaking**: If multiple splits yield the same minimum risk, the one with the smaller feature index is chosen. If feature indices are also tied, the one with the smaller threshold is chosen.\n- **No-Split Condition**: If no valid split reduces the parent empirical risk, the stump does not split and predicts the overall majority class.\n- **Training Data Generation ($N=200$):**\n    - $x_0 \\sim \\mathcal{N}(0,1)$.\n    - $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma=3.0$.\n    - $y = \\mathbb{1}\\{x_0 + \\epsilon > 0\\}$, where $\\mathbb{1}$ is the indicator function.\n    - $x_1 \\sim \\mathrm{Uniform}(-2,2)$ for most samples.\n    - A subgroup of $s=15$ positive-labeled ($y=1$) training samples has $x_1$ set to $10$.\n- **Test Data Generation ($M=5000$):**\n    - $x_0 \\sim \\mathcal{N}(0,1)$.\n    - $\\epsilon \\sim \\mathcal{N}(0, 3.0^2)$.\n    - $y = \\mathbb{1}\\{x_0 + \\epsilon > 0\\}$.\n    - $x_1 \\sim \\mathrm{Uniform}(-2,2)$ for ALL samples (no spurious pattern).\n- **Task**: For a set of $w_{\\min}$ values, determine if the trained stump avoids the spurious split on feature $x_1$. Avoiding the spurious split is defined as selecting feature $x_0$ or making no split at all.\n- **Test Suite**: $w_{\\min} \\in \\{1, 15, 16, 500\\}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the principles of statistical learning. It describes a standard algorithm (decision stump with greedy training) and uses a synthetic data generation process to investigate a common pathology: overfitting to spurious correlations. The use of a minimum child size (`w_min`) is a standard regularization technique to prevent such overfitting.\n- **Well-Posed**: The problem is specified with high precision. All parameters ($N, M, s, \\sigma$), algorithmic procedures (greedy search, risk calculation, threshold selection), constraints ($w_{\\min}$), and tie-breaking rules are explicitly defined. This ensures that the procedure leads to a unique, deterministic outcome for a given random seed.\n- **Objective**: The problem is stated in objective, mathematical language. The task is to implement the specified algorithm and report its behavior, which is a purely computational exercise free of subjectivity.\n\nThe problem does not violate any of the invalidity criteria. It is a well-defined, scientifically sound problem in computational statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n## Solution\n\nThe problem requires us to implement a decision stump learning algorithm and test its behavior on a specially constructed dataset designed to have a spurious correlation. The goal is to observe how the `minimum child weight` constraint, $w_{\\min}$, affects the algorithm's susceptibility to this spurious pattern.\n\n### 1. Principle and Experimental Design\nThe core of the problem lies in the conflict between a genuinely (but weakly) informative feature, $x_0$, and a spuriously \"perfect\" feature, $x_1$.\n- **Feature $x_0$**: This feature is causally related to the label $y$ through the equation $y = \\mathbb{1}\\{x_0 + \\epsilon > 0\\}$. However, the high noise variance ($\\sigma^2=9.0$) makes this relationship weak, meaning a split on $x_0$ will likely provide only a modest reduction in empirical risk.\n- **Feature $x_1$**: This feature is generally uninformative. However, a small subgroup of $s=15$ training samples with $y=1$ are artificially assigned an extreme value $x_1=10$. A greedy algorithm can discover a split on $x_1$ (e.g., at a threshold $\\tau$ between $2$ and $10$) that perfectly isolates these $15$ samples into a \"pure\" child node with zero misclassifications. This can lead to a very large reduction in the overall empirical risk on the training set, making it a highly attractive, yet spurious, split.\n- **Regularization via $w_{\\min}$**: The minimum child weight constraint $w_{\\min}$ is a form of regularization. By requiring each child node to contain a minimum number of samples, we can forbid splits that isolate very small subgroups. If $w_{\\min}$ is set to be larger than the size of the spurious subgroup ($s=15$), the greedy algorithm will be prevented from making this locally optimal but globally poor choice, potentially forcing it to select the more robust feature $x_0$ or not split at all.\n\n### 2. Data Generation\nFirst, we implement the data generation process for the training set of size $N=200$. For reproducibility, a fixed random seed is used.\n1.  Generate feature $x_0$ from a standard normal distribution, $x_0 \\sim \\mathcal{N}(0,1)$.\n2.  Generate noise $\\epsilon$ from a normal distribution with mean $0$ and standard deviation $\\sigma=3.0$, $\\epsilon \\sim \\mathcal{N}(0, 3.0^2)$.\n3.  Compute the binary labels $y = \\mathbb{1}\\{x_0 + \\epsilon > 0\\}$.\n4.  Generate feature $x_1$ from a uniform distribution, $x_1 \\sim \\mathrm{Uniform}(-2,2)$.\n5.  Identify the indices of all samples where $y=1$. From this set, randomly select $s=15$ indices and set their corresponding $x_1$ values to $10.0$. This injects the spurious correlation.\n6.  Combine $x_0$ and $x_1$ into a feature matrix $X$.\n\n### 3. Decision Stump Algorithm\nThe stump is trained by finding the single split (a feature $j$ and threshold $\\tau$) that minimizes the total misclassification count, subject to the $w_{\\min}$ constraint.\n1.  **Calculate Parent Risk**: First, calculate the misclassification count of the root node (i.e., if no split is made). This is the count of the minority class in the entire dataset. This value serves as the initial `best_misclass` score to beat.\n2.  **Iterate Through Splits**:\n    - For each feature $j \\in \\{0, 1\\}$:\n        - Determine the set of candidate thresholds. These are the midpoints between consecutive unique values of the feature $x_j$.\n        - For each threshold $\\tau$ in increasing order:\n            a.  **Partition Data**: Split the samples into a left set ($x_j \\le \\tau$) and a right set ($x_j > \\tau$).\n            b.  **Check Constraint**: Count the number of samples in the left ($n_{left}$) and right ($n_{right}$) sets. If $n_{left} < w_{\\min}$ or $n_{right} < w_{\\min}$, this is an invalid split; continue to the next threshold.\n            c.  **Calculate Risk**: For the valid split, calculate the total misclassification count. This is the sum of misclassifications in the left child and the right child. The misclassification count for a child is the number of samples in the minority class within that child.\n            d.  **Update Best Split**: Compare the current split's misclassification count with the `best_misclass` found so far. If the current count is strictly smaller (`<`), update `best_misclass` with this new count and record the current feature $j$ and threshold $\\tau$ as the best split. The strict inequality and the loop order (feature $0$ then $1$; smaller thresholds first) correctly implement the specified tie-breaking rule.\n3.  **Return Result**: After checking all valid splits, the feature index of the best split is returned. If no split improved upon the parent risk, the initial feature index of $-1$ is returned, indicating no split was made.\n\n### 4. Analysis of Test Cases\nThe procedure is executed for each value of $w_{\\min}$ in the test suite $\\{1, 15, 16, 500\\}$.\n-   **Case 1: $w_{\\min} = 1$**: This constraint is trivial. The spurious split on $x_1$ creates a child of size $15$, which is $\\ge 1$. This split is highly attractive because it creates a pure node, leading to a significant risk reduction. The algorithm is expected to select feature $x_1$. The result should be `False` (spurious split not avoided).\n-   **Case 2: $w_{\\min} = 15$**: The spurious split creates a child of size exactly $15$. Since the constraint is $n_{child} \\ge w_{\\min}$, this split is still valid ($15 \\ge 15$). The algorithm is again expected to select feature $x_1$. The result should be `False`.\n-   **Case 3: $w_{\\min} = 16$**: The spurious split is now invalid because its child of size $15$ does not meet the minimum weight of $16$ ($15 < 16$). The algorithm is forced to ignore this \"trap\" and must find an alternative split. It will either select a split on the weakly informative feature $x_0$ (if one reduces risk) or make no split at all. In either scenario, feature $x_1$ is not selected. The result should be `True` (spurious split avoided).\n-   **Case 4: $w_{\\min} = 500$**: The total number of samples is $N=200$. It is impossible to create two children that both satisfy the constraint of having at least $500$ samples. Therefore, no valid splits exist. The algorithm will not split. As feature $x_1$ is not selected, the result is `True`.\n\nThe implementation will generate a boolean list corresponding to these four outcomes.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the data generation and decision stump learning to test the effect\n    of the minimum child weight constraint on avoiding spurious splits.\n    \"\"\"\n    # Define problem parameters\n    N = 200\n    s = 15\n    sigma = 3.0\n    w_min_cases = [1, 15, 16, 500]\n\n    # Set a fixed seed for reproducibility of the random dataset\n    np.random.seed(42)\n\n    # --- Data Generation --\n    # This block creates the single training set used for all test cases.\n    \n    # Feature x_0 is drawn from a standard normal distribution\n    x0 = np.random.randn(N)\n    # The label y is determined by x_0 plus high-variance noise\n    epsilon = np.random.normal(0, sigma, N)\n    y_train = (x0 + epsilon > 0).astype(int)\n\n    # Feature x_1 is mostly uniform noise\n    x1 = np.random.uniform(-2, 2, N)\n    \n    # Identify indices of positive-labeled samples to inject the spurious pattern\n    positive_indices = np.where(y_train == 1)[0]\n    \n    # Set x_1 to an extreme value for a small subgroup of 's' positive samples.\n    # This creates a spurious correlation that a greedy algorithm might exploit.\n    if len(positive_indices) >= s:\n        spurious_indices = np.random.choice(positive_indices, size=s, replace=False)\n        x1[spurious_indices] = 10.0\n    else:\n        # This case is unlikely with N=200 but makes the code more robust.\n        x1[positive_indices] = 10.0\n\n    X_train = np.column_stack((x0, x1))\n    \n    results = []\n    for w_min in w_min_cases:\n        # --- Decision Stump Training ---\n        n_samples = X_train.shape[0]\n\n        # Calculate the misclassification count of the parent node (no split scenario).\n        # This is the number of samples in the minority class.\n        n_pos_parent = np.sum(y_train)\n        parent_misclass = min(n_pos_parent, n_samples - n_pos_parent)\n\n        best_misclass = parent_misclass\n        best_split = {'feature': -1, 'threshold': np.inf}\n\n        # Iterate through features (j=0 for x_0, j=1 for x_1)\n        for j in range(X_train.shape[1]):\n            feature_values = X_train[:, j]\n            \n            # Candidate thresholds are midpoints of unique sorted feature values.\n            unique_vals = np.unique(feature_values)\n            if len(unique_vals) < 2:\n                continue\n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n            # Evaluate each potential split\n            for tau in thresholds:\n                # Partition data into left and right children\n                left_mask = feature_values <= tau\n                \n                n_left = np.sum(left_mask)\n                n_right = n_samples - n_left\n\n                # Verify the minimum child weight constraint\n                if n_left < w_min or n_right < w_min:\n                    continue\n\n                # Calculate misclassifications in the left child\n                y_left = y_train[left_mask]\n                misclass_left = min(np.sum(y_left), n_left - np.sum(y_left))\n                \n                # Calculate misclassifications in the right child\n                right_mask = ~left_mask\n                y_right = y_train[right_mask]\n                misclass_right = min(np.sum(y_right), n_right - np.sum(y_right))\n\n                current_misclass = misclass_left + misclass_right\n\n                # A split is chosen only if it strictly reduces the misclassification count.\n                # The loop order (j=0 then j=1; tau ascending) ensures that ties are\n                # broken by smaller feature index, then smaller threshold.\n                if current_misclass < best_misclass:\n                    best_misclass = current_misclass\n                    best_split = {'feature': j, 'threshold': tau}\n        \n        selected_feature = best_split['feature']\n        \n        # The split is considered non-spurious if the selected feature is not x_1 (index 1).\n        # This includes cases where x_0 is chosen or no split is made (feature = -1).\n        avoids_spurious = (selected_feature != 1)\n        results.append(avoids_spurious)\n\n    # Print the final list of booleans in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112969"}, {"introduction": "The standard CART algorithm builds trees using a greedy, best-first strategy, which finds a locally optimal split at each step but offers no guarantee of global optimality. This advanced practice delves into the potential pitfalls of such an approach by creating a scenario with a misleading \"distractor\" feature [@problem_id:3113028]. You will analyze whether a greedy tree, with its limited split budget, can uncover the true additive structure of the data or if it becomes trapped by the immediate, but ultimately misleading, impurity reduction offered by the distractor.", "problem": "You are to design and analyze a synthetic regression dataset with an additive hierarchical structure and evaluate whether a greedy Classification and Regression Tree (CART) procedure with a small split budget recovers the intended structure or instead favors misleading local impurity gains. Work in the setting of decision trees for regression where empirical risk is measured by the mean squared error.\n\nConstruct a dataset with features $x_1$, $x_2$, and a distractor feature $z$, and response $Y$, according to the following generative model. For a given sample size $n$, draw $x_1 \\sim \\mathrm{Uniform}[0,1]$ and $x_2 \\sim \\mathrm{Uniform}[0,1]$ independently. Fix thresholds $a \\in (0,1)$ and $b \\in (0,1)$, and define indicator functions $\\mathbb{I}\\{x_1 > a\\}$ and $\\mathbb{I}\\{x_2 > b\\}$. Let $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and $\\xi \\sim \\mathcal{N}(0,\\sigma_z^2)$ be independent noise. Define the response and distractor as\n$$\nY = \\alpha \\,\\mathbb{I}\\{x_1 > a\\} + \\beta \\,\\mathbb{I}\\{x_2 > b\\} + \\varepsilon,\n\\quad\nz = \\lambda \\left(\\mathbb{I}\\{x_1 > a\\} + \\mathbb{I}\\{x_2 > b\\}\\right) + \\xi,\n$$\nwhere $\\alpha$, $\\beta$, $\\lambda$, $\\sigma$, and $\\sigma_z$ are fixed real parameters. The dataset consists of $n$ independent observations of $(x_1, x_2, z, Y)$.\n\nImplement a greedy best-first CART regression tree that uses at most $S$ binary, axis-aligned splits and obeys a minimum leaf size of $m$. The impurity at a node with responses $\\{y_i\\}_{i \\in \\mathcal{I}}$ is the empirical sum of squared deviations from the node mean, that is\n$$\n\\mathrm{SSE}(\\mathcal{I}) = \\sum_{i \\in \\mathcal{I}} \\left(y_i - \\bar{y}_{\\mathcal{I}}\\right)^2,\n\\quad \\text{where } \\bar{y}_{\\mathcal{I}} = \\frac{1}{|\\mathcal{I}|}\\sum_{i \\in \\mathcal{I}} y_i.\n$$\nAt each step, among all current leaves and all valid splits that respect the minimum leaf size $m$, choose the split that yields the largest reduction in $\\mathrm{SSE}$, i.e., the largest difference between the parent node’s $\\mathrm{SSE}$ and the sum of the children’s $\\mathrm{SSE}$. Apply the split to that leaf. Repeat until either $S$ splits have been made or no valid split yields a positive reduction.\n\nDefine “recovering the hierarchy” as follows: With a split budget of $S = 2$, the greedy tree “recovers the hierarchy” if and only if the two splits chosen by the algorithm use both $x_1$ and $x_2$ (in any order), and do not exclusively use the distractor $z$. If one or both splits use $z$ exclusively (and the set of split features does not contain both $x_1$ and $x_2$), then the greedy procedure is considered “trapped by misleading local impurity gains.”\n\nFor additional evaluation, also compute the best achievable $\\mathrm{SSE}$ with exactly two splits constrained to use each of $x_1$ and $x_2$ exactly once (in any order), and applied so that one of the two children of the root is split by the other feature, always respecting the minimum leaf size $m$. This constrained best value represents the best “structured two-split” tree aligned with the intended additive hierarchy.\n\nYour program must:\n- Generate the dataset for each test case with the specified parameters using a fixed random seed for reproducibility.\n- Build the greedy best-first CART tree with at most $S = 2$ splits.\n- Determine whether the greedy tree “recovers the hierarchy” as defined.\n- Optionally compute and compare the greedy $\\mathrm{SSE}$ to the best “structured two-split” $\\mathrm{SSE}$, but the required output is the recovery decision.\n\nTest Suite:\nProvide results for the following four test cases. In each case, report whether the greedy procedure “recovers the hierarchy” as a boolean. Use the listed parameters and random seeds.\n\n- Case 1 (happy path): $n = 2000$, $\\alpha = 1.0$, $\\beta = 1.0$, $a = 0.5$, $b = 0.5$, $\\sigma = 0.1$, $\\lambda = 0.2$, $\\sigma_z = 0.5$, $S = 2$, $m = 25$, seed $= 0$.\n- Case 2 (strong distractor trap): $n = 2000$, $\\alpha = 1.0$, $\\beta = 1.0$, $a = 0.5$, $b = 0.5$, $\\sigma = 0.1$, $\\lambda = 2.0$, $\\sigma_z = 0.05$, $S = 2$, $m = 25$, seed $= 1$.\n- Case 3 (skewed thresholds): $n = 2000$, $\\alpha = 1.5$, $\\beta = 0.5$, $a = 0.85$, $b = 0.2$, $\\sigma = 0.1$, $\\lambda = 0.8$, $\\sigma_z = 0.2$, $S = 2$, $m = 20$, seed $= 2$.\n- Case 4 (noise dominated): $n = 2000$, $\\alpha = 1.0$, $\\beta = 1.0$, $a = 0.5$, $b = 0.5$, $\\sigma = 1.0$, $\\lambda = 0.5$, $\\sigma_z = 0.5$, $S = 2$, $m = 25$, seed $= 3$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list of booleans enclosed in square brackets (e.g., [True,False,True,False]). No additional output is permitted.", "solution": "We construct a dataset based on the additive hierarchical structure by sampling $x_1 \\sim \\mathrm{Uniform}[0,1]$ and $x_2 \\sim \\mathrm{Uniform}[0,1]$, setting thresholds $a$ and $b$, and defining the response\n$$\nY = \\alpha \\,\\mathbb{I}\\{x_1 > a\\} + \\beta \\,\\mathbb{I}\\{x_2 > b\\} + \\varepsilon,\n\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2).\n$$\nThis is an additive model in the indicator features $\\mathbb{I}\\{x_1 > a\\}$ and $\\mathbb{I}\\{x_2 > b\\}$, producing four regions with piecewise-constant means that differ by $\\alpha$ and $\\beta$. To create a feature that can mislead greedy splitting, we add a distractor\n$$\nz = \\lambda \\left(\\mathbb{I}\\{x_1 > a\\} + \\mathbb{I}\\{x_2 > b\\}\\right) + \\xi,\n\\quad \\xi \\sim \\mathcal{N}(0,\\sigma_z^2),\n$$\nwhich is correlated with the latent sum of indicators. If $\\lambda$ is large and $\\sigma_z$ is small, a split on $z$ at the root can have a large immediate reduction in empirical sum of squared errors (SSE), potentially exceeding the reduction obtained by the true structural splits on $x_1$ or $x_2$. However, with a split budget limited to $S = 2$, expending the first split on $z$ can limit the tree’s ability to represent the additive hierarchical structure because only one additional split remains, while fully separating both $x_1$ and $x_2$ contributions across all regions ideally requires splitting on both signals at or near the top of the tree and typically benefits from splitting both children, which exceeds the two-split budget.\n\nThe greedy best-first CART procedure is defined by the empirical risk minimization principle for squared error: at any node with index set $\\mathcal{I}$, the impurity is \n$$\n\\mathrm{SSE}(\\mathcal{I}) = \\sum_{i \\in \\mathcal{I}} \\left(y_i - \\bar{y}_{\\mathcal{I}}\\right)^2,\n\\quad \\bar{y}_{\\mathcal{I}} = \\frac{1}{|\\mathcal{I}|}\\sum_{i \\in \\mathcal{I}} y_i.\n$$\nGiven a candidate split of $\\mathcal{I}$ into left and right children $\\mathcal{I}_L$ and $\\mathcal{I}_R$, the reduction in impurity is\n$$\n\\Delta = \\mathrm{SSE}(\\mathcal{I}) - \\left( \\mathrm{SSE}(\\mathcal{I}_L) + \\mathrm{SSE}(\\mathcal{I}_R) \\right).\n$$\nBest-first greedy tree growth repeatedly chooses, among all current leaves and all valid axis-aligned thresholds that respect the minimum leaf size $m$, the split that maximizes $\\Delta$. This aligns with the principle that, for squared loss, the piecewise-constant fit minimizing empirical risk in each leaf is the leaf mean, and the optimal split is the one that yields the largest reduction in total within-leaf squared error.\n\nTo implement this efficiently and deterministically, we:\n- Use fixed random seeds per test case to generate $(x_1,x_2)$, noise, and hence $(z,Y)$.\n- For any node and feature, sort the node’s samples by the feature value and evaluate candidate thresholds between consecutive unique values, constrained to produce child leaves with at least $m$ samples. For each split point, we compute left and right $\\mathrm{SSE}$ via cumulative sums and cumulative sums of squares. The best split at a node is the one with the largest $\\Delta$.\n- In the best-first scheme, at each iteration we scan all current leaves, compute each leaf’s best split, pick the leaf and split achieving the largest $\\Delta$, and apply it. We record the feature used at each chosen split. We stop after at most $S = 2$ splits or if no valid split yields positive $\\Delta$.\n\nTo assess whether the tree “recovers the hierarchy,” we check the set of features used by the greedy algorithm across its up to two splits. If both $x_1$ and $x_2$ are used (in any order), we declare success. If the set of split features does not include both $x_1$ and $x_2$, we declare that the greedy procedure is “trapped by misleading local impurity gains.”\n\nFor additional perspective, we also compute the best two-split structured tree that is constrained to use each of $x_1$ and $x_2$ exactly once. This is obtained by enumerating all possible root splits on one of $\\{x_1,x_2\\}$ and then, for each root choice and threshold, enumerating the best valid second split on the other feature applied to either the left or right child, respecting $m$, and choosing the combination minimizing total $\\mathrm{SSE}$. This constrained optimum represents the best two-split approximation that adheres to the intended additive structure.\n\nTest cases are designed to probe:\n- A “happy path” where the distractor is weak, so greedy should select $x_1$ and $x_2$.\n- A “trap” where the distractor is strong and low-noise, encouraging greedy to split on $z$ and possibly again on $z$ with only $S = 2$ splits.\n- Skewed thresholds where region sizes differ substantially, testing the interaction of minimum leaf size and split gains.\n- A noise-dominated regime where gains are muted and split choices may be unstable.\n\nThe program computes and prints a single line list of four booleans corresponding to the recovery decision for each case, in the order specified. This output directly satisfies the requirement to deliver quantifiable, testable results that reflect whether greedy CART recovers the intended hierarchy or is misled by local impurity reductions due to the distractor feature.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sse_of_indices(y, idx):\n    if idx.size == 0:\n        return 0.0\n    y_sub = y[idx]\n    mean = y_sub.mean()\n    return float(((y_sub - mean) ** 2).sum())\n\ndef best_split_on_feature(X_col, y, idx, min_leaf):\n    # Returns (best_gain, threshold, left_idx, right_idx, thr_value_at_split)\n    # Using efficient cumulative sums on the subset defined by idx.\n    if idx.size < 2 * min_leaf:\n        return (0.0, None, None, None, None)\n    # Sort by feature values\n    order = np.argsort(X_col[idx], kind='mergesort')  # stable\n    idx_sorted = idx[order]\n    x_sorted = X_col[idx_sorted]\n    y_sorted = y[idx_sorted]\n    n = idx_sorted.size\n\n    # Precompute cumulative sums for SSE computation\n    csum = np.cumsum(y_sorted)\n    csum2 = np.cumsum(y_sorted ** 2)\n\n    total_sum = csum[-1]\n    total_sum2 = csum2[-1]\n\n    best_gain = 0.0\n    best_pos = None\n    # Evaluate splits between positions pos-1 and pos, where each side has >= min_leaf\n    # Positions are 1..n-1, we require min_leaf <= pos <= n - min_leaf\n    start = min_leaf\n    end = n - min_leaf\n    if end <= start - 1:\n        return (0.0, None, None, None, None)\n\n    # To avoid splitting at identical feature values that lead to empty child, we skip positions where x_sorted[pos-1] == x_sorted[pos]\n    parent_sse = float(total_sum2 - (total_sum ** 2) / n)\n\n    for pos in range(start, end + 1):\n        if x_sorted[pos - 1] == x_sorted[pos]:\n            continue\n        left_n = pos\n        right_n = n - pos\n\n        left_sum = csum[pos - 1]\n        right_sum = total_sum - left_sum\n\n        left_sum2 = csum2[pos - 1]\n        right_sum2 = total_sum2 - left_sum2\n\n        left_sse = float(left_sum2 - (left_sum ** 2) / left_n)\n        right_sse = float(right_sum2 - (right_sum ** 2) / right_n)\n\n        gain = parent_sse - (left_sse + right_sse)\n        if gain > best_gain:\n            best_gain = gain\n            best_pos = pos\n\n    if best_pos is None:\n        return (0.0, None, None, None, None)\n\n    # Threshold as midpoint between consecutive feature values\n    thr = 0.5 * (x_sorted[best_pos - 1] + x_sorted[best_pos])\n    left_mask = X_col[idx] <= thr\n    left_idx = idx[left_mask]\n    right_idx = idx[~left_mask]\n    return (best_gain, thr, left_idx, right_idx, thr)\n\ndef best_split_for_leaf(X, y, idx, min_leaf):\n    # Iterate over all features to find best split on this leaf\n    n_features = X.shape[1]\n    parent_sse = sse_of_indices(y, idx)\n    best = {\n        \"gain\": 0.0,\n        \"feature\": None,\n        \"threshold\": None,\n        \"left_idx\": None,\n        \"right_idx\": None,\n    }\n    for j in range(n_features):\n        gain, thr, left_idx, right_idx, _ = best_split_on_feature(X[:, j], y, idx, min_leaf)\n        if gain > best[\"gain\"]:\n            best[\"gain\"] = gain\n            best[\"feature\"] = j\n            best[\"threshold\"] = thr\n            best[\"left_idx\"] = left_idx\n            best[\"right_idx\"] = right_idx\n    return best\n\ndef greedy_best_first_cart_two_splits(X, y, split_budget, min_leaf):\n    # Best-first: at each iteration, evaluate best split on each current leaf, pick the best overall.\n    # Track which features were used in performed splits.\n    n = X.shape[0]\n    initial_idx = np.arange(n, dtype=int)\n    leaves = [initial_idx]\n    features_used = []\n    splits_done = 0\n\n    while splits_done < split_budget:\n        best_overall = None\n        best_leaf_pos = None\n        # Evaluate best split for each leaf\n        for pos, idx in enumerate(leaves):\n            candidate = best_split_for_leaf(X, y, idx, min_leaf)\n            if candidate[\"gain\"] <= 1e-12:\n                continue\n            if best_overall is None or candidate[\"gain\"] > best_overall[\"gain\"]:\n                best_overall = candidate\n                best_leaf_pos = pos\n        if best_overall is None:\n            break  # no positive gain split available\n        # Apply the best split\n        features_used.append(best_overall[\"feature\"])\n        left_idx = best_overall[\"left_idx\"]\n        right_idx = best_overall[\"right_idx\"]\n        # Replace the split leaf with its two children\n        leaves.pop(best_leaf_pos)\n        leaves.append(left_idx)\n        leaves.append(right_idx)\n        splits_done += 1\n\n    # Compute final SSE of the greedy tree\n    total_sse = 0.0\n    for idx in leaves:\n        total_sse += sse_of_indices(y, idx)\n    return features_used, total_sse\n\ndef best_structured_two_split_sse(X, y, min_leaf, feature_indices):\n    # Constrained: exactly two splits, use each of the two specified features exactly once.\n    # Enumerate: choose root feature and threshold; then split one child on the other feature with best possible threshold.\n    # Return minimal SSE achievable under these constraints.\n    n = X.shape[0]\n    all_idx = np.arange(n, dtype=int)\n    best_total_sse = sse_of_indices(y, all_idx)\n\n    f1, f2 = feature_indices\n    for root_feat in [f1, f2]:\n        other_feat = f2 if root_feat == f1 else f1\n        # Enumerate root splits on root_feat\n        # Use same best_split_on_feature logic but we need to scan all thresholds manually to evaluate second split too.\n        # We'll generate all valid root thresholds (midpoints) and then for each compute best child split on other_feat.\n        # Prepare sorted indices by root feature\n        Xr = X[:, root_feat]\n        order = np.argsort(Xr[all_idx], kind='mergesort')\n        idx_sorted = all_idx[order]\n        xr_sorted = Xr[idx_sorted]\n        y_sorted = y[idx_sorted]\n        n_all = idx_sorted.size\n\n        # Candidate split positions\n        start = min_leaf\n        end = n_all - min_leaf\n        if end <= start - 1:\n            continue\n\n        # Generate possible positions where feature value changes\n        valid_positions = []\n        for pos in range(start, end + 1):\n            if xr_sorted[pos - 1] != xr_sorted[pos]:\n                valid_positions.append(pos)\n        if not valid_positions:\n            continue\n\n        for pos in valid_positions:\n            thr_root = 0.5 * (xr_sorted[pos - 1] + xr_sorted[pos])\n            left_mask = Xr[all_idx] <= thr_root\n            left_idx = all_idx[left_mask]\n            right_idx = all_idx[~left_mask]\n\n            # For each child, compute best split on other_feat; choose the better of splitting left or right.\n            # Option A: split left child by other_feat\n            gain_left, thr_left, left_left_idx, left_right_idx, _ = best_split_on_feature(X[:, other_feat], y, left_idx, min_leaf)\n            if gain_left <= 0.0:\n                sse_left_after = sse_of_indices(y, left_idx)\n            else:\n                sse_left_after = sse_of_indices(y, left_left_idx) + sse_of_indices(y, left_right_idx)\n            sse_right_after = sse_of_indices(y, right_idx)\n            total_sse_a = sse_left_after + sse_right_after\n\n            # Option B: split right child by other_feat\n            gain_right, thr_right, right_left_idx, right_right_idx, _ = best_split_on_feature(X[:, other_feat], y, right_idx, min_leaf)\n            sse_right_after_b = sse_of_indices(y, right_idx) if gain_right <= 0.0 else (sse_of_indices(y, right_left_idx) + sse_of_indices(y, right_right_idx))\n            sse_left_after_b = sse_of_indices(y, left_idx)\n            total_sse_b = sse_left_after_b + sse_right_after_b\n\n            best_total_sse = min(best_total_sse, total_sse_a, total_sse_b)\n\n    return best_total_sse\n\ndef generate_data(n, alpha, beta, a, b, sigma, lam, sigma_z, seed):\n    rng = np.random.default_rng(seed)\n    x1 = rng.uniform(0.0, 1.0, size=n)\n    x2 = rng.uniform(0.0, 1.0, size=n)\n    i1 = (x1 > a).astype(float)\n    i2 = (x2 > b).astype(float)\n    eps = rng.normal(0.0, sigma, size=n)\n    xi = rng.normal(0.0, sigma_z, size=n)\n    y = alpha * i1 + beta * i2 + eps\n    z = lam * (i1 + i2) + xi\n    # Features in order: x1, x2, z\n    X = np.column_stack([x1, x2, z])\n    return X, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a dict of parameters.\n    test_cases = [\n        # Case 1: happy path\n        dict(n=2000, alpha=1.0, beta=1.0, a=0.5, b=0.5, sigma=0.1, lam=0.2, sigma_z=0.5, S=2, m=25, seed=0),\n        # Case 2: strong distractor trap\n        dict(n=2000, alpha=1.0, beta=1.0, a=0.5, b=0.5, sigma=0.1, lam=2.0, sigma_z=0.05, S=2, m=25, seed=1),\n        # Case 3: skewed thresholds\n        dict(n=2000, alpha=1.5, beta=0.5, a=0.85, b=0.2, sigma=0.1, lam=0.8, sigma_z=0.2, S=2, m=20, seed=2),\n        # Case 4: noise dominated\n        dict(n=2000, alpha=1.0, beta=1.0, a=0.5, b=0.5, sigma=1.0, lam=0.5, sigma_z=0.5, S=2, m=25, seed=3),\n    ]\n\n    results = []\n    for params in test_cases:\n        X, y = generate_data(\n            n=params[\"n\"],\n            alpha=params[\"alpha\"],\n            beta=params[\"beta\"],\n            a=params[\"a\"],\n            b=params[\"b\"],\n            sigma=params[\"sigma\"],\n            lam=params[\"lam\"],\n            sigma_z=params[\"sigma_z\"],\n            seed=params[\"seed\"],\n        )\n        # Greedy best-first CART with at most S splits\n        features_used, greedy_sse = greedy_best_first_cart_two_splits(\n            X, y, split_budget=params[\"S\"], min_leaf=params[\"m\"]\n        )\n        # Determine recovery: features_used must include both x1 (index 0) and x2 (index 1)\n        used_set = set(features_used)\n        recovered = (0 in used_set) and (1 in used_set)\n        results.append(recovered)\n\n        # Optionally, compute structured best SSE (not used in printed results but can be kept for inspection if needed)\n        # structured_sse = best_structured_two_split_sse(X, y, min_leaf=params[\"m\"], feature_indices=(0, 1))\n        # diff = greedy_sse - structured_sse\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(['True' if r else 'False' for r in results])}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3113028"}]}