{"hands_on_practices": [{"introduction": "Decision trees, and by extension Random Forests, build themselves by recursively finding the \"best\" split in the data. This choice is governed by a split criterion, typically Gini impurity or Shannon entropy, which measures how well a feature separates the classes. This exercise [@problem_id:3166111] provides a hands-on, quantitative comparison of these two fundamental criteria. By deriving the impurity reduction formulas from first principles, you will gain a deeper intuition for how these core components of tree-based models respond differently to class imbalance, a crucial consideration in many real-world applications.", "problem": "You are given a binary classification setting with classes $\\{0,1\\}$. Let the class prior be $P(Y=1)=\\pi$ and $P(Y=0)=1-\\pi$, where $\\pi \\in [0,1]$. Consider a single binary candidate split based on a binary feature $X \\in \\{0,1\\}$, with class-conditional probabilities $P(X=1 \\mid Y=1)=a$ and $P(X=1 \\mid Y=0)=b$, where $a \\in [0,1]$ and $b \\in [0,1]$. The split partitions the parent node into two child nodes: the left child for $X=1$ and the right child for $X=0$.\n\nAs the fundamental base, use the following definitions:\n- The Gini impurity of a node with positive-class probability $p$ is $G(p)=2p(1-p)$.\n- The Shannon entropy (in bits) of a node with positive-class probability $p$ is $H(p)=-p\\log_2 p -(1-p)\\log_2(1-p)$, with the convention that $0\\log_2 0$ is taken to be $0$.\n- For any impurity function $I(\\cdot)$, the impurity reduction (also called information gain for entropy) when splitting a parent node with positive-class probability $p$ into two children is $I(p)-w_L I(p_L)-w_R I(p_R)$, where $w_L$ and $w_R$ are the proportions of samples going to the left and right child respectively, and $p_L$ and $p_R$ are the positive-class probabilities within the left and right child respectively.\n\nFrom these definitions, derive the expressions for $w_L$, $w_R$, $p_L$, and $p_R$ in terms of $\\pi$, $a$, and $b$, and then compute the impurity reduction for both the Gini impurity and the Shannon entropy. For a fixed pair $(a,b)$ and a given $\\pi$, denote the impurity reductions by $R_G(\\pi;a,b)$ and $R_H(\\pi;a,b)$ for Gini and entropy, respectively.\n\nYour task is to write a complete, runnable program that:\n- Uses the endpoints $\\pi=0.01$ and $\\pi=0.50$ to assess sensitivity of the split quality to rare classes, by computing the sensitivity ratios\n  $$S_G(a,b)=\\frac{R_G(0.01;a,b)}{R_G(0.50;a,b)} \\quad \\text{and} \\quad S_H(a,b)=\\frac{R_H(0.01;a,b)}{R_H(0.50;a,b)}.$$\n- If the denominator $R_G(0.50;a,b)$ is $0$, define $S_G(a,b)=0.0$. If the denominator $R_H(0.50;a,b)$ is $0$, define $S_H(a,b)=0.0$. This convention ensures well-defined outputs in degenerate cases.\n\nImplement the above using only the definitions provided, without invoking any other pre-derived results.\n\nTest suite:\n- Use the following five $(a,b)$ pairs as distinct test cases to probe different signal regimes and edge behavior:\n  - Case one (strong split): $(a,b)=(0.90,0.10)$.\n  - Case two (weak split): $(a,b)=(0.60,0.40)$.\n  - Case three (no signal boundary): $(a,b)=(0.50,0.50)$.\n  - Case four (high true positive and moderate false positive): $(a,b)=(0.99,0.49)$.\n  - Case five (rare trigger among negatives): $(a,b)=(0.30,0.01)$.\n\nRequired final output format:\n- Your program must output a single line containing a flat list of $10$ floating-point numbers in the order\n  $$[S_G(a_1,b_1), S_H(a_1,b_1), S_G(a_2,b_2), S_H(a_2,b_2), \\dots, S_G(a_5,b_5), S_H(a_5,b_5)],$$\n  where $(a_1,b_1),\\dots,(a_5,b_5)$ follow the test suite order above.\n- Each number must be printed in decimal form. There are no physical units or angles involved.\n- The list must be comma-separated and enclosed in square brackets, with no additional whitespace or text.\n\nYour program must be self-contained, require no input, and execute deterministically.", "solution": "The user wants me to solve a problem related to impurity reduction in a decision tree split.\n\nHere's my plan:\n1.  **Validate the Problem**: I'll meticulously check the problem statement against my validation criteria.\n    *   **Step 1: Extract Givens**: I'll list all the definitions and parameters provided.\n        *   Binary classification: classes $\\{0,1\\}$.\n        *   Class prior: $P(Y=1)=\\pi$, $P(Y=0)=1-\\pi$.\n        *   Binary feature: $X \\in \\{0,1\\}$.\n        *   Class-conditional probabilities: $P(X=1 \\mid Y=1)=a$, $P(X=1 \\mid Y=0)=b$.\n        *   Split rule: Left child if $X=1$, right child if $X=0$.\n        *   Gini impurity: $G(p)=2p(1-p)$.\n        *   Shannon entropy: $H(p)=-p\\log_2 p -(1-p)\\log_2(1-p)$.\n        *   Impurity reduction: $I(p)-w_L I(p_L)-w_R I(p_R)$.\n        *   Sensitivity ratios: $S_G(a,b)=\\frac{R_G(0.01;a,b)}{R_G(0.50;a,b)}$ and $S_H(a,b)=\\frac{R_H(0.01;a,b)}{R_H(0.50;a,b)}$.\n        *   Denominator zero convention: If denominator is $0$, ratio is $0.0$.\n        *   Test cases: $(0.90,0.10)$, $(0.60,0.40)$, $(0.50,0.50)$, $(0.99,0.49)$, $(0.30,0.01)$.\n        *   Pi values for sensitivity: $\\pi=0.01$ and $\\pi=0.50$.\n        *   Output format: A flat list of 10 floats.\n\n    *   **Step 2: Validate Using Extracted Givens**:\n        *   **Scientifically Grounded**: Yes, the problem is based on standard definitions from statistical learning and decision tree theory (Gini impurity, Shannon entropy, information gain). The definitions are correct. The given formula for Gini impurity $G(p)=2p(1-p)$ is a common variant; sometimes it is defined as $p(1-p)$. This is fine, as the scaling factor of 2 will cancel out in some contexts but will be present in the impurity reduction calculation, which is what the problem asks for. I must stick to the given definition.\n        *   **Well-Posed**: Yes, the problem asks for the derivation of specific quantities ($w_L, w_R, p_L, p_R$) and then the calculation of impurity reductions and their ratios. The steps are clearly defined. The special case for zero denominators makes the problem well-defined everywhere.\n        *   **Objective**: Yes, the language is precise and mathematical. There are no subjective elements.\n        *   **Flaw Checklist**:\n            1.  **Scientific/Factual Unsoundness**: No. The definitions are standard in machine learning.\n            2.  **Non-Formalizable/Irrelevant**: No. It's a formalizable problem directly related to random forests/decision trees.\n            3.  **Incomplete/Contradictory Setup**: No. All necessary information is provided.\n            4.  **Unrealistic/Infeasible**: No. The parameters are probabilities and are within valid ranges.\n            5.  **Ill-Posed/Poorly Structured**: No. A unique solution exists for each test case.\n            6.  **Pseudo-Profound/Trivial**: No. It requires careful derivation and calculation, testing understanding of the core concepts.\n            7.  **Outside Scientific Verifiability**: No. The results are mathematically derivable and verifiable.\n\n    *   **Step 3: Verdict and Action**: The problem is **VALID**. I will proceed to the solution.\n\n2.  **Solve the Problem**: I'll derive the required expressions and then implement the calculations.\n\n    *   **Derivation Phase**: I need to express $w_L, w_R, p_L, p_R$ in terms of $\\pi, a, b$. I'll use Bayes' theorem and the law of total probability.\n\n        *   **Parent node positive-class probability**: This is given directly as $p = P(Y=1) = \\pi$.\n\n        *   **Child node proportions ($w_L, w_R$)**: These are the marginal probabilities of the feature $X$.\n            *   $w_L$ is the proportion of samples in the left child, which corresponds to $X=1$.\n                $w_L = P(X=1)$.\n                Using the law of total probability:\n                $P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$\n                $w_L = a \\cdot \\pi + b \\cdot (1-\\pi)$.\n            *   $w_R$ is the proportion of samples in the right child, which corresponds to $X=0$.\n                $w_R = P(X=0) = 1 - P(X=1) = 1 - w_L$.\n                Alternatively, using the law of total probability:\n                $P(X=0) = P(X=0 \\mid Y=1)P(Y=1) + P(X=0 \\mid Y=0)P(Y=0)$\n                $P(X=0 \\mid Y=1) = 1 - P(X=1 \\mid Y=1) = 1-a$.\n                $P(X=0 \\mid Y=0) = 1 - P(X=1 \\mid Y=0) = 1-b$.\n                $w_R = (1-a)\\pi + (1-b)(1-\\pi)$.\n                Let's check if $w_L + w_R = 1$:\n                $(a\\pi + b(1-\\pi)) + ((1-a)\\pi + (1-b)(1-\\pi))$\n                $= \\pi(a + 1 - a) + (1-\\pi)(b + 1 - b)$\n                $= \\pi(1) + (1-\\pi)(1) = 1$. The derivation is consistent.\n\n        *   **Child node positive-class probabilities ($p_L, p_R$)**: These are posterior probabilities.\n            *   $p_L$ is the probability of class 1 in the left child, i.e., $P(Y=1 \\mid X=1)$.\n                Using Bayes' theorem:\n                $p_L = P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1)P(Y=1)}{P(X=1)}$\n                $p_L = \\frac{a \\pi}{w_L} = \\frac{a \\pi}{a\\pi + b(1-\\pi)}$.\n                Note: if $w_L=0$, then this node is empty, and its impurity doesn't contribute to the reduction. In this case, either $a\\pi=0$ and $b(1-\\pi)=0$. If $w_L=0$, then the term $w_L I(p_L)$ will be 0, so the value of $p_L$ itself does not need to be defined. The problem code should handle this division by zero. If $w_L=0$, the impurity reduction calculation should just have a $0$ for that term.\n            *   $p_R$ is the probability of class 1 in the right child, i.e., $P(Y=1 \\mid X=0)$.\n                Using Bayes' theorem:\n                $p_R = P(Y=1 \\mid X=0) = \\frac{P(X=0 \\mid Y=1)P(Y=1)}{P(X=0)}$\n                $p_R = \\frac{(1-a)\\pi}{w_R} = \\frac{(1-a)\\pi}{(1-a)\\pi + (1-b)(1-\\pi)}$.\n                Similarly, if $w_R=0$, the term $w_R I(p_R)$ will be 0.\n\n    *   **Impurity Reduction Calculation**: Now I can write the formulas for $R_G$ and $R_H$.\n        *   Parent impurity: $I(p) = I(\\pi)$.\n        *   Impurity Reduction: $R_I(\\pi; a, b) = I(\\pi) - [w_L I(p_L) + w_R I(p_R)]$.\n\n        *   **Gini Impurity Reduction ($R_G$)**:\n            $G(p) = 2p(1-p)$.\n            $G(\\pi) = 2\\pi(1-\\pi)$.\n            $G(p_L) = 2p_L(1-p_L)$.\n            $G(p_R) = 2p_R(1-p_R)$.\n            $R_G(\\pi; a, b) = 2\\pi(1-\\pi) - [w_L \\cdot 2p_L(1-p_L) + w_R \\cdot 2p_R(1-p_R)]$.\n            Let's simplify the child impurity part.\n            $w_L \\cdot 2p_L(1-p_L) = w_L \\cdot 2 \\frac{a\\pi}{w_L} (1 - \\frac{a\\pi}{w_L}) = 2a\\pi (1 - \\frac{a\\pi}{w_L}) = 2a\\pi - \\frac{2(a\\pi)^2}{w_L}$.\n            $w_R \\cdot 2p_R(1-p_R) = w_R \\cdot 2 \\frac{(1-a)\\pi}{w_R} (1 - \\frac{(1-a)\\pi}{w_R}) = 2(1-a)\\pi - \\frac{2((1-a)\\pi)^2}{w_R}$.\n            This seems unnecessarily complicated. Let's use the probabilities directly in the implementation. It's less prone to algebraic errors.\n\n            An alternative way to write the Gini reduction is through a weighted average of child impurities.\n            The term $p(1-p)$ is the variance of a Bernoulli trial with success probability $p$. The reduction in impurity is related to the explained variance.\n            Let's check a known formula for Gini gain:\n            Gain = $G(parent) - (w_L G(left) + w_R G(right))$. This matches the problem statement.\n            A simplified form is sometimes used: $w_L w_R (p_L - p_R)^2$ (scaled by 2).\n            Let's see if this is true for $G(p)=2p(1-p)$.\n            The weighted average of child probabilities is $w_L p_L + w_R p_R = P(X=1)P(Y=1|X=1) + P(X=0)P(Y=1|X=0) = P(X=1, Y=1) + P(X=0, Y=1) = P(Y=1) = \\pi$.\n            So $\\pi = w_L p_L + w_R p_R$. Also $1-\\pi = w_L(1-p_L) + w_R(1-p_R)$.\n            Parent Impurity: $2\\pi(1-\\pi) = 2(w_L p_L + w_R p_R)(w_L(1-p_L) + w_R(1-p_R))$.\n            This is getting messy. I'll stick to the direct definition:\n            $R_G(\\pi; a, b) = G(\\pi) - (w_L G(p_L) + w_R G(p_R))$.\n            This is the safest path.\n\n        *   **Shannon Entropy Reduction ($R_H$)**:\n            $H(p) = -p\\log_2 p -(1-p)\\log_2(1-p)$.\n            $H(\\pi) = -\\pi\\log_2 \\pi - (1-\\pi)\\log_2(1-\\pi)$.\n            $H(p_L) = -p_L\\log_2 p_L -(1-p_L)\\log_2(1-p_L)$.\n            $H(p_R) = -p_R\\log_2 p_R -(1-p_R)\\log_2(1-p_R)$.\n            $R_H(\\pi; a,b) = H(\\pi) - [w_L H(p_L) + w_R H(p_R)]$.\n            This is the standard definition of Information Gain. Again, I will implement this directly and not seek a simplified algebraic form, as it's easy to make mistakes and the direct form is computationally straightforward. I need to be careful with the $0 \\log 0 = 0$ convention. This means if $p=0$ or $p=1$, $H(p)=0$.\n\n    *   **Implementation Strategy**:\n        1.  Create a function `calculate_reductions(pi, a, b)` that returns a tuple `(R_G, R_H)`.\n        2.  Inside this function:\n            a. Calculate $w_L = a\\pi + b(1-\\pi)$.\n            b. Calculate $w_R = 1 - w_L$.\n            c. Calculate $p_L$. If $w_L > 0$, $p_L = a\\pi / w_L$. If $w_L=0$, this branch is empty; its impurity contribution is $0$.\n            d. Calculate $p_R$. If $w_R > 0$, $p_R = (1-a)\\pi / w_R$. If $w_R=0$, this branch is empty; its impurity contribution is $0$.\n            e. Implement helper functions for Gini `G(p)` and Entropy `H(p)`.\n                *   `G(p) = 2 * p * (1 - p)`\n                *   `H(p)` needs to handle $p=0$ and $p=1$. If $p=0$ or $p=1$, $H(p)=0$. Otherwise, $H(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$. `np.log2` will be useful here.\n            f. Calculate parent impurities $G(\\pi)$ and $H(\\pi)$.\n            g. Calculate weighted child impurities.\n                *   `gini_children = 0`. If `w_L > 0`, add `w_L * G(p_L)`. If `w_R > 0`, add `w_R * G(p_R)`.\n                *   `entropy_children = 0`. If `w_L > 0`, add `w_L * H(p_L)`. If `w_R > 0`, add `w_R * H(p_R)`.\n            h. Calculate reductions:\n                *   `R_G = G(pi) - gini_children`\n                *   `R_H = H(pi) - entropy_children`\n            i. Return `(R_G, R_H)`.\n        3.  Define the test cases: `test_cases = [(0.90, 0.10), ..., (0.30, 0.01)]`.\n        4.  Loop through the test cases. For each `(a,b)`:\n            a. Set $\\pi_1 = 0.01$ and $\\pi_2 = 0.50$.\n            b. Call `R_G1, R_H1 = calculate_reductions(pi_1, a, b)`.\n            c. Call `R_G2, R_H2 = calculate_reductions(pi_2, a, b)`.\n            d. Calculate the sensitivity ratios $S_G$ and $S_H$.\n                *   `S_G = 0.0 if R_G2 == 0 else R_G1 / R_G2`. Use a small tolerance for the zero check, `np.isclose(R_G2, 0)`.\n                *   `S_H = 0.0 if R_H2 == 0 else R_H1 / R_H2`. Use `np.isclose(R_H2, 0)`.\n            e. Append $S_G$ and $S_H$ to a results list.\n        5.  Print the results in the required format.\n\n    *   **Double-checking the logic**:\n        *   **Degenerate cases**:\n            *   When is $R(\\pi; a, b) = 0$? Impurity reduction is zero if the child nodes are not any \"purer\" than the parent. This happens if $p_L = p_R = \\pi$.\n                *   $p_L = \\pi \\implies \\frac{a\\pi}{a\\pi + b(1-\\pi)} = \\pi$.\n                  If $\\pi \\in (0,1)$, we can divide by $\\pi$: $\\frac{a}{a\\pi + b(1-\\pi)} = 1 \\implies a = a\\pi + b(1-\\pi) \\implies a(1-\\pi) = b(1-\\pi) \\implies a=b$.\n                *   So, if $a=b$, the feature $X$ is independent of the class $Y$, $P(X|Y)=P(X)$. The split provides no information.\n                *   $P(X=1|Y=1) = a$, $P(X=1|Y=0) = b$. If $a=b$, then $P(X=1|Y)$ does not depend on $Y$. Hence $X$ and $Y$ are independent.\n                *   In this case ($a=b$), $w_L = a\\pi+a(1-\\pi) = a$. $p_L = \\frac{a\\pi}{a} = \\pi$. $w_R = 1-a$. $p_R = \\frac{(1-a)\\pi}{1-a} = \\pi$.\n                *   So the child impurities are $I(p_L)=I(\\pi)$ and $I(p_R)=I(\\pi)$.\n                *   The reduction is $I(\\pi) - (w_L I(\\pi) + w_R I(\\pi)) = I(\\pi) - (w_L+w_R)I(\\pi) = I(\\pi) - I(\\pi) = 0$.\n                *   This confirms that if $a=b$, the reduction is zero. This will happen for the test case $(a,b)=(0.50, 0.50)$.\n                *   For this case, $R_G(0.50; 0.50, 0.50) = 0$ and $R_H(0.50; 0.50, 0.50) = 0$.\n                *   According to the problem, $S_G$ and $S_H$ should be defined as $0.0$. My implementation logic `S = 0.0 if R_denom == 0 else R_num / R_denom` handles this correctly.\n\n        *   **Floating point precision**: The problem states \"decimal form\". Standard float representation should be sufficient. I will use `np.isclose` for comparing to zero to be safe.\n\n3.  **Writing the Code**: I'll structure the code as planned.\n\n    *   Python's `math.log2` can be used, or `numpy.log2`. `numpy` is allowed, so I'll use it.\n    *   Need to import `numpy`. `scipy` is not necessary for this problem.\n\n    Let's walk through one calculation manually to get a feel for the numbers.\n    Case 1: $(a,b) = (0.90, 0.10)$.\n\n    *   At $\\pi = 0.50$:\n        *   $w_L = 0.90 \\cdot 0.50 + 0.10 \\cdot 0.50 = 0.45 + 0.05 = 0.50$.\n        *   $w_R = 1 - 0.50 = 0.50$.\n        *   $p = 0.50$.\n        *   $p_L = \\frac{0.90 \\cdot 0.50}{0.50} = 0.90$.\n        *   $p_R = \\frac{(1-0.90) \\cdot 0.50}{0.50} = \\frac{0.10 \\cdot 0.50}{0.50} = 0.10$.\n        *   Gini:\n            *   $G(p) = G(0.5) = 2 \\cdot 0.5 \\cdot 0.5 = 0.5$.\n            *   $G(p_L) = G(0.9) = 2 \\cdot 0.9 \\cdot 0.1 = 0.18$.\n            *   $G(p_R) = G(0.1) = 2 \\cdot 0.1 \\cdot 0.9 = 0.18$.\n            *   $R_G(0.50; 0.9, 0.1) = G(0.5) - [0.5 \\cdot G(0.9) + 0.5 \\cdot G(0.1)] = 0.5 - [0.5 \\cdot 0.18 + 0.5 \\cdot 0.18] = 0.5 - 0.18 = 0.32$.\n        *   Entropy:\n            *   $H(p) = H(0.5) = -0.5\\log_2(0.5) - 0.5\\log_2(0.5) = - \\log_2(0.5) = \\log_2(2) = 1$.\n            *   $H(p_L) = H(0.9) = -0.9\\log_2(0.9) - 0.1\\log_2(0.1) \\approx -0.9(-0.152) - 0.1(-3.322) \\approx 0.1368 + 0.3322 = 0.469$.\n            *   $H(p_R) = H(0.1) = H(0.9) \\approx 0.469$.\n            *   $R_H(0.50; 0.9, 0.1) = H(0.5) - [0.5 \\cdot H(0.9) + 0.5 \\cdot H(0.1)] \\approx 1 - [0.5 \\cdot 0.469 + 0.5 \\cdot 0.469] = 1 - 0.469 = 0.531$.\n\n    *   At $\\pi = 0.01$:\n        *   $w_L = 0.90 \\cdot 0.01 + 0.10 \\cdot 0.99 = 0.009 + 0.099 = 0.108$.\n        *   $w_R = 1 - 0.108 = 0.892$.\n        *   $p = 0.01$.\n        *   $p_L = \\frac{0.90 \\cdot 0.01}{0.108} = \\frac{0.009}{0.108} = \\frac{9}{108} = \\frac{1}{12} \\approx 0.08333$.\n        *   $p_R = \\frac{(1-0.90) \\cdot 0.01}{0.892} = \\frac{0.10 \\cdot 0.01}{0.892} = \\frac{0.001}{0.892} \\approx 0.00112$.\n        *   Gini:\n            *   $G(p) = G(0.01) = 2 \\cdot 0.01 \\cdot 0.99 = 0.0198$.\n            *   $G(p_L) = G(1/12) = 2 \\cdot (1/12) \\cdot (11/12) = 22/144 = 11/72 \\approx 0.15278$.\n            *   $G(p_R) = G(0.00112) \\approx 2 \\cdot 0.00112 \\cdot 0.99888 \\approx 0.002237$.\n            *   $R_G(0.01; 0.9, 0.1) \\approx 0.0198 - [0.108 \\cdot 0.15278 + 0.892 \\cdot 0.002237]$\n              $\\approx 0.0198 - [0.0165 + 0.001995] \\approx 0.0198 - 0.018495 \\approx 0.001305$.\n        *   Entropy:\n            *   $H(p) = H(0.01) = -0.01\\log_2(0.01) - 0.99\\log_2(0.99) \\approx 0.08079$.\n            *   $H(p_L) = H(1/12) \\approx H(0.0833) \\approx -(0.0833\\log_2(0.0833) + 0.9167\\log_2(0.9167)) \\approx 0.4138$.\n            *   $H(p_R) = H(0.00112) \\approx -(0.00112\\log_2(0.00112) + 0.99888\\log_2(0.99888)) \\approx 0.0101$.\n            *   $R_H(0.01; 0.9, 0.1) \\approx 0.08079 - [0.108 \\cdot 0.4138 + 0.892 \\cdot 0.0101]$\n              $\\approx 0.08079 - [0.04469 + 0.00901] \\approx 0.08079 - 0.0537 \\approx 0.02709$.\n\n    *   Sensitivity Ratios for Case 1:\n        *   $S_G(0.9, 0.1) = \\frac{R_G(0.01)}{R_G(0.50)} \\approx \\frac{0.001305}{0.32} \\approx 0.004078$.\n        *   $S_H(0.9, 0.1) = \\frac{R_H(0.01)}{R_H(0.50)} \\approx \\frac{0.02709}{0.531} \\approx 0.0510$.\n\nThe sensitivity is much lower for Gini impurity. This is a known property: Gini tends to favor splits that create large, pure nodes, while entropy can be more sensitive to smaller pure nodes, which are more likely to arise in imbalanced settings. Let me check the literature to confirm this intuition. Yes, Gini impurity is less sensitive to changes in node probabilities than entropy, especially for probabilities near 0 or 1. This is because the Gini function $G(p)=2p(1-p)$ has a derivative $G'(p) = 2(1-2p)$, which goes to $\\pm 2$ at the boundaries, while the entropy function $H(p)$ has derivative $H'(p) = -\\log_2(p) + \\log_2(1-p) = \\log_2(\\frac{1-p}{p})$, which goes to $\\pm\\infty$ as $p \\to 0$ or $p \\to 1$. A higher derivative magnitude means higher sensitivity. The calculation seems to reflect this expected behavior, which is a good sanity check.\n\nNow I will structure the final implementation. I will create a main `solve` function that orchestrates the entire process as required by the problem template. I will define the helper functions for Gini and entropy, and the main calculation function.\n\n```python\nimport numpy as np\n\n# Helper function for Shannon entropy\ndef shannon_entropy(p):\n    if p == 0 or p == 1:\n        return 0.0\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Helper function for Gini impurity\ndef gini_impurity(p):\n    return 2 * p * (1 - p)\n\n# Main calculation function\ndef calculate_reductions(pi, a, b):\n    # If a=b, the feature is independent of the class, so reduction is 0.\n    # This also prevents division by zero if pi=0 or pi=1 and a=b, as w_L becomes a or b.\n    if a == b:\n        return 0.0, 0.0\n\n    # Parent node probability for positive class\n    p_parent = pi\n\n    # Proportions of samples going to left/right children\n    # Left child: X=1, Right child: X=0\n    w_L = a * pi + b * (1 - pi)\n    w_R = (1 - a) * pi + (1 - b) * (1 - pi)\n    # Sanity check: w_R = 1 - w_L\n\n    # Impurity of the parent node\n    parent_gini = gini_impurity(p_parent)\n    parent_entropy = shannon_entropy(p_parent)\n\n    # Calculate weighted impurity of child nodes\n    gini_children = 0.0\n    entropy_children = 0.0\n\n    # Left child (X=1)\n    if w_L > 0:\n        p_L = (a * pi) / w_L\n        gini_children += w_L * gini_impurity(p_L)\n        entropy_children += w_L * shannon_entropy(p_L)\n\n    # Right child (X=0)\n    if w_R > 0:\n        p_R = ((1 - a) * pi) / w_R\n        gini_children += w_R * gini_impurity(p_R)\n        entropy_children += w_R * shannon_entropy(p_R)\n        \n    # Impurity reduction (Information Gain)\n    reduction_gini = parent_gini - gini_children\n    reduction_entropy = parent_entropy - entropy_children\n    \n    return reduction_gini, reduction_entropy\n\ndef solve():\n    test_cases = [\n        (0.90, 0.10),\n        (0.60, 0.40),\n        (0.50, 0.50),\n        (0.99, 0.49),\n        (0.30, 0.01),\n    ]\n\n    pi_rare = 0.01\n    pi_balanced = 0.50\n\n    results = []\n    for a, b in test_cases:\n        # Calculate reductions for the rare class scenario\n        R_G_rare, R_H_rare = calculate_reductions(pi_rare, a, b)\n        \n        # Calculate reductions for the balanced class scenario\n        R_G_balanced, R_H_balanced = calculate_reductions(pi_balanced, a, b)\n\n        # Calculate sensitivity ratios S_G and S_H\n        # Handle division by zero as per problem spec\n        S_G = 0.0 if np.isclose(R_G_balanced, 0) else R_G_rare / R_G_balanced\n        S_H = 0.0 if np.isclose(R_H_balanced, 0) else R_H_rare / R_H_balanced\n        \n        results.extend([S_G, S_H])\n\n    # Format output as required\n    print(f\"[{','.join(f'{r:.17f}'.rstrip('0').rstrip('.') for r in results)}]\")\n    # The formatting needs to be just decimal form. `str(r)` should be enough.\n    # The example output doesn't show a fixed precision. Let `str` handle it.\n\n    # Let me refine the output formatting. `{r}` should be fine for standard float representation.\n    # The requirement is \"decimal form\". This is what Python's `str()` on a float produces.\n    # The example output in the problem template is `[{','.join(map(str, results))}]`. This seems perfect.\n    \n    # Final check on the code logic. Looks sound.\n    # The case where `w_L` or `w_R` is zero is handled correctly by the `if` statements.\n    # For example, if pi=1 and a=0, then `w_L = b*0 = 0`, but `w_R = 1*1 + (1-b)*0 = 1`.\n    # Then `p_L` is not calculated, `p_R = (1*1)/1 = 1`.\n    # `parent_gini = G(1) = 0`. `parent_entropy = H(1) = 0`.\n    # `gini_children = w_R * G(p_R) = 1 * G(1) = 0`.\n    # `reduction = 0 - 0 = 0`. This is correct, as the parent node is already pure.\n    # The logic seems robust.\n\n    # Let's consider the case a=b=0.5.\n    # R_G_balanced, R_H_balanced will be 0 as a=b.\n    # S_G and S_H will be 0.0 by the rule. This is correct.\n    \n    # The problem asks for the output `[S_G(a1,b1), S_H(a1,b1), ...]`. My code does `results.extend([S_G, S_H])`, which produces this ordering.\n\n    # Let me write out the full `solve` part inside the template for the final answer.\n```\nThe logic seems solid. I will now write the solution description and the final code.\n\n**Derivation of intermediate quantities:**\n\nThe problem is defined within a standard Bayesian probability framework. Let $\\pi = P(Y=1)$ be the prior probability of the positive class. The feature $X$ is binary, and its class-conditional probabilities are given as $P(X=1 \\mid Y=1) = a$ and $P(X=1 \\mid Y=0) = b$. The split partitions data based on the value of $X$, with $X=1$ going to the left child and $X=0$ to the right child.\n\n1.  **Parent Node Positive-Class Probability, $p$**:\n    The parent node contains all samples before the split. Therefore, the probability of the positive class in the parent node is simply the overall prior probability of the positive class.\n    $$p = P(Y=1) = \\pi$$\n\n2.  **Child Node Proportions, $w_L$ and $w_R$**:\n    These are the marginal probabilities of observing $X=1$ (left child) and $X=0$ (right child). We find them using the law of total probability.\n    -   $w_L$ is the proportion of samples in the left child ($X=1$):\n        $$w_L = P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$$\n        Substituting the given values:\n        $$w_L = a\\pi + b(1-\\pi)$$\n    -   $w_R$ is the proportion of samples in the right child ($X=0$):\n        $$w_R = P(X=0) = 1 - P(X=1) = 1 - w_L$$\n        Alternatively, using the class-conditional probabilities for $X=0$, which are $P(X=0 \\mid Y=1) = 1-a$ and $P(X=0 \\mid Y=0) = 1-b$:\n        $$w_R = P(X=0 \\mid Y=1)P(Y=1) + P(X=0 \\mid Y=0)P(Y=0) = (1-a)\\pi + (1-b)(1-\\pi)$$\n        It is easy to verify that $w_L + w_R = (a\\pi + b(1-\\pi)) + ((1-a)\\pi + (1-b)(1-\\pi)) = \\pi(a+1-a) + (1-\\pi)(b+1-b) = \\pi + (1-\\pi) = 1$.\n\n3.  **Child Node Positive-Class Probabilities, $p_L$ and $p_R$**:\n    These are posterior probabilities, calculated using Bayes' theorem.\n    -   $p_L$ is the probability of the positive class in the left child, conditioned on $X=1.\n        $$p_L = P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1)P(Y=1)}{P(X=1)}$$\n        Substituting the previously derived expressions:\n        $$p_L = \\frac{a\\pi}{w_L} = \\frac{a\\pi}{a\\pi + b(1-\\pi)}$$\n        This is defined only if $w_L > 0$. If $w_L=0$, the left child is empty, and its impurity contribution to the reduction is zero.\n    -   $p_R$ is the probability of the positive class in the right child, conditioned on $X=0$.\n        $$p_R = P(Y=1 \\mid X=0) = \\frac{P(X=0 \\mid Y=1)P(Y=1)}{P(X=0)}$$\n        Substituting the previously derived expressions:\n        $$p_R = \\frac{(1-a)\\pi}{w_R} = \\frac{(1-a)\\pi}{(1-a)\\pi + (1-b)(1-\\pi)}$$\n        This is defined only if $w_R > 0$. If $w_R=0$, the right child is empty.\n\n**Calculation of Impurity Reduction:**\n\nWith these quantities, we can compute the impurity reduction for any impurity function $I(\\cdot)$ using the formula provided:\n$$R_I(\\pi;a,b) = I(p) - [w_L I(p_L) + w_R I(p_R)]$$\nwhere $p=\\pi$.\n\n-   **For Gini Impurity**:\n    The Gini impurity is $G(p) = 2p(1-p)$. The reduction is:\n    $$R_G(\\pi;a,b) = G(\\pi) - [w_L G(p_L) + w_R G(p_R)]$$\n    $$R_G(\\pi;a,b) = 2\\pi(1-\\pi) - \\left[ w_L \\cdot 2p_L(1-p_L) + w_R \\cdot 2p_R(1-p_R) \\right]$$\n\n-   **For Shannon Entropy**:\n    The Shannon entropy is $H(p) = -p\\log_2 p - (1-p)\\log_2(1-p)$. The reduction, also known as information gain, is:\n    $$R_H(\\pi;a,b) = H(\\pi) - [w_L H(p_L) + w_R H(p_R)]$$\n    $$R_H(\\pi;a,b) = \\left( -\\pi\\log_2 \\pi - (1-\\pi)\\log_2(1-\\pi) \\right) - \\left[ w_L H(p_L) + w_R H(p_R) \\right]$$\n\nThese expressions are implemented directly in the provided program. The program computes these reduction values for $\\pi=0.01$ and $\\pi=0.50$ for each $(a, b)$ pair in the test suite and then calculates the required sensitivity ratios $S_G$ and $S_H$. The special case where the denominator is zero (which occurs when $a=b$, rendering the split uninformative) is handled by setting the ratio to $0.0$, as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates sensitivity ratios for Gini and Entropy impurity reductions based on a binary split.\n    \"\"\"\n\n    def shannon_entropy(p):\n        \"\"\"\n        Calculates Shannon entropy H(p) = -p*log2(p) - (1-p)*log2(1-p).\n        Handles p=0 and p=1 cases where H(p)=0.\n        \"\"\"\n        if p <= 0 or p >= 1:\n            return 0.0\n        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n    def gini_impurity(p):\n        \"\"\"\n        Calculates Gini impurity G(p) = 2*p*(1-p).\n        \"\"\"\n        return 2 * p * (1 - p)\n\n    def calculate_reductions(pi, a, b):\n        \"\"\"\n        Calculates the impurity reduction for Gini and Shannon entropy.\n\n        Args:\n            pi (float): The prior probability of class 1, P(Y=1).\n            a (float): The class-conditional probability P(X=1 | Y=1).\n            b (float): The class-conditional probability P(X=1 | Y=0).\n\n        Returns:\n            tuple: A tuple containing (Gini reduction, Shannon entropy reduction).\n        \"\"\"\n        # If a=b, feature X is independent of class Y, so impurity reduction is 0.\n        if np.isclose(a, b):\n            return 0.0, 0.0\n        \n        # Parent node positive class probability is pi.\n        p_parent = pi\n\n        # Calculate proportions of samples in left (X=1) and right (X=0) children.\n        w_L = a * pi + b * (1 - pi)\n        w_R = 1.0 - w_L\n\n        # Calculate parent node impurity.\n        parent_gini = gini_impurity(p_parent)\n        parent_entropy = shannon_entropy(p_parent)\n\n        # Calculate weighted average of child node impurities.\n        gini_children = 0.0\n        entropy_children = 0.0\n\n        # Contribution from left child (X=1).\n        if w_L > 0:\n            p_L = (a * pi) / w_L\n            gini_children += w_L * gini_impurity(p_L)\n            entropy_children += w_L * shannon_entropy(p_L)\n\n        # Contribution from right child (X=0).\n        if w_R > 0:\n            p_R = ((1 - a) * pi) / w_R\n            gini_children += w_R * gini_impurity(p_R)\n            entropy_children += w_R * shannon_entropy(p_R)\n        \n        # Impurity reduction is parent impurity minus weighted child impurity.\n        reduction_gini = parent_gini - gini_children\n        reduction_entropy = parent_entropy - entropy_children\n        \n        return reduction_gini, reduction_entropy\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.90, 0.10), # Case one (strong split)\n        (0.60, 0.40), # Case two (weak split)\n        (0.50, 0.50), # Case three (no signal boundary)\n        (0.99, 0.49), # Case four (high true positive and moderate false positive)\n        (0.30, 0.01), # Case five (rare trigger among negatives)\n    ]\n\n    pi_rare = 0.01\n    pi_balanced = 0.50\n\n    results = []\n    for a, b in test_cases:\n        # Calculate reductions for the rare class (imbalanced) scenario.\n        R_G_rare, R_H_rare = calculate_reductions(pi_rare, a, b)\n        \n        # Calculate reductions for the balanced class scenario.\n        R_G_balanced, R_H_balanced = calculate_reductions(pi_balanced, a, b)\n\n        # Calculate sensitivity ratios S_G and S_H.\n        # Handle the case where the denominator is zero.\n        S_G = 0.0 if np.isclose(R_G_balanced, 0) else R_G_rare / R_G_balanced\n        S_H = 0.0 if np.isclose(R_H_balanced, 0) else R_H_rare / R_H_balanced\n        \n        results.extend([S_G, S_H])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3166111"}, {"introduction": "After understanding how a single split is made, the next step is to construct the full ensemble and interpret its results. A key output of a trained Random Forest is its feature importance scores, which rank predictors by their contribution to the model. In this practice [@problem_id:3166180], you will implement a Random Forest classifier from scratch to investigate how the standard Mean Decrease in Impurity (MDI) importance metric behaves in specific edge cases. By analyzing scenarios with constant (zero-variance) and perfectly duplicated features, you will develop a critical perspective on the interpretation and potential limitations of this widely used diagnostic tool.", "problem": "You are asked to implement, from first principles, a binary-classification Random Forest based on Classification and Regression Trees (CART), using the Gini impurity as the split criterion, and to compute normalized feature importance scores defined as the mean decrease in impurity. Your implementation must handle edge cases where some features are constant (zero variance). You will then run the implementation on three specified test cases and report a compact numerical summary.\n\nDefinitions and requirements:\n- A decision tree is grown recursively. At each internal node with data indices $\\mathcal{I}$, you must:\n  1) Randomly select a subset of features of size $m$ (without replacement) from the $d$ available features, independently at each node.\n  2) For each selected feature $j$, consider all candidate thresholds formed as midpoints between sorted unique values of that feature among the samples in $\\mathcal{I}$. If a feature has only a single unique value, it cannot be split at this node.\n  3) For a candidate threshold $\\tau$, define the left child as the set of indices with $x_{ij} \\le \\tau$ and the right child as the set of indices with $x_{ij} > \\tau$. Compute the weighted Gini impurity of this split and choose the feature and threshold that maximizes the impurity decrease. If the best impurity decrease is non-positive, make the node a leaf.\n- For any node with index set $\\mathcal{I}$ and class labels $y_i \\in \\{0,1\\}$, the Gini impurity is $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$, where $p_k$ is the class proportion in $\\mathcal{I}$.\n- For a split of $\\mathcal{I}$ into left $\\mathcal{L}$ and right $\\mathcal{R}$ using feature $j$ and threshold $\\tau$, the impurity decrease contributed by that split is\n$$\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R}).$$\n- A Random Forest with $T$ trees is constructed by training each tree on a bootstrap sample of size $N$ (sampling with replacement) from the original training set, and applying the node-wise random feature subspace selection of size $m$.\n- The feature importance for feature $j$ over one tree is the sum, over all internal nodes where the best split uses feature $j$, of $\\Delta(\\mathcal{I}, j, \\tau)$ weighted by the number of samples $|\\mathcal{I}|$ at that node. Aggregate this sum over all trees. Let $S$ denote the total across all features of these aggregated, sample-weighted decreases. If $S > 0$, define the normalized importance of feature $j$ as $I_j = \\frac{\\text{aggregated decrease for } j}{S}$ so that $\\sum_{j=1}^{d} I_j = 1$. If $S = 0$, define $I_j = 0$ for all $j$.\n- There is no restriction on tree depth other than the implicit stopping condition when no valid split yields positive impurity decrease.\n\nImplementation constraints:\n- You must implement the above procedure exactly as stated, using pure numerical operations. No external machine learning libraries are allowed.\n- All randomness must be made reproducible using the specified seeds.\n\nDatasets and test suite:\nYou will run your implementation on three cases. In each case, construct the feature matrix $X \\in \\mathbb{R}^{N \\times d}$ and labels $y \\in \\{0,1\\}^N$ as specified, then train a Random Forest with the specified parameters, and compute the normalized feature importances $\\{I_j\\}_{j=1}^d$.\n\nCase $1$ (constant and noise features with a single informative feature):\n- Data generation seed: $42$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = 0$ for all $i$ (a constant feature).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently for all $i$.\n- Define labels $y_i = 1$ if $x_{i1} > 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 2$, forest randomness seed $2024$.\n- Compute normalized importances $(I_1, I_2, I_3)$.\n\nCase $2$ (perfectly duplicated informative feature and one noise feature):\n- Data generation seed: $123$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = x_{i1}$ for all $i$ (a perfect duplicate).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently.\n- Define labels $y_i = 1$ if $x_{i1} > 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 1$, forest randomness seed $2025$.\n- Compute normalized importances $(I_1, I_2, I_3)$, and also compute the duplicate-share statistic $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$.\n\nCase $3$ (all features constant):\n- Data generation seed: $7$.\n- $N = 100$, $d = 2$.\n- Set $x_{i1} = 0$ and $x_{i2} = 3$ for all $i$ (both features constant).\n- Generate labels $y_i$ as independent Bernoulli with $P(y_i = 1) = 0.5$ using the given seed.\n- Random Forest parameters: number of trees $T = 10$, node-wise feature subset size $m = 2$, forest randomness seed $99$.\n- Compute normalized importances $(I_1, I_2)$ and their sum $Q = I_1 + I_2$.\n\nNumerical reporting:\n- For Case $1$, report the three floats $I_1$, $I_2$, $I_3$.\n- For Case $2$, report the four floats $S_{\\text{dup}}$, $I_1$, $I_2$, $I_3$.\n- For Case $3$, report the three floats $Q$, $I_1$, $I_2$.\n- Round each reported float to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing all the reported results as a comma-separated list enclosed in square brackets, in the following order:\n$[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, S_{\\text{dup}}^{(2)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, Q^{(3)}, I_1^{(3)}, I_2^{(3)}]$,\nwhere the superscripts indicate the case index. There are no physical units. Angles are not used. All proportions must be reported as decimals, not as percentages.", "solution": "The user has specified the task of implementing a Random Forest classifier for binary classification from first principles. The implementation must adhere to a precise set of definitions for Classification and Regression Trees (CART), Gini impurity, node splitting, and a specific method for calculating feature importance based on the mean decrease in impurity. The implementation will be validated against three distinct test cases, and a set of numerical results must be reported in a specific format.\n\n### Problem Validation\n\nFirst, I will validate the problem statement against the required criteria.\n\n**Step 1: Extract Givens**\n\n- **Algorithm:** A Random Forest composed of `T` Classification and Regression Trees (CART) for binary classification ($y \\in \\{0,1\\}$).\n- **Tree Construction (Recursion):**\n    - At each node $\\mathcal{I}$, randomly select `m` features (from `d` total) without replacement.\n    - For each selected feature $j$, find the optimal split.\n    - **Thresholds ($\\tau$):** Midpoints between sorted unique values of feature $j$ among samples in $\\mathcal{I}$.\n    - **Split Rule:** Left child $\\mathcal{L} = \\{i \\in \\mathcal{I} | x_{ij} \\le \\tau\\}$, Right child $\\mathcal{R} = \\{i \\in \\mathcal{I} | x_{ij} > \\tau\\}$.\n- **Stopping Conditions:** A node becomes a leaf if the best impurity decrease is non-positive, or if no valid splits can be made (e.g., all features at the node are constant).\n- **Split Criterion (Gini Impurity):**\n    - **Impurity of a node $\\mathcal{I}$:** $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$, where $p_k$ is the proportion of class $k$.\n    - **Impurity Decrease (Gini Gain):** $\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R})$. The split with maximum $\\Delta$ is chosen.\n- **Ensemble Method (Random Forest):**\n    - Each of the $T$ trees is trained on a bootstrap sample of size $N$ (sampling with replacement) from the training data of size $N$.\n- **Feature Importance:**\n    - For a single tree and feature $j$, importance is the sum of $|\\mathcal{I}| \\cdot \\Delta(\\mathcal{I}, j, \\tau)$ over all nodes where $j$ was the best split feature.\n    - Total importance for feature $j$ is the sum aggregated over all $T$ trees.\n    - **Normalization:** Let $S$ be the sum of aggregated importances over all features. Normalized importance $I_j$ is $\\frac{\\text{aggregated decrease for } j}{S}$ if $S > 0$, and $I_j = 0$ if $S = 0$.\n- **Test Cases & Parameters:**\n    - **Case 1:** Data seed=`42`, `N=200`, `d=3`. $x_1 \\sim \\mathcal{N}(0,1)$, $x_2=0$, $x_3 \\sim \\mathcal{N}(0,1)$. $y=1$ if $x_1 > 0$, else $0$. RF params: `T=50`, `m=2`, forest seed=`2024`. Report: $(I_1, I_2, I_3)$.\n    - **Case 2:** Data seed=`123`, `N=200`, `d=3`. $x_1 \\sim \\mathcal{N}(0,1)$, $x_2=x_1$, $x_3 \\sim \\mathcal{N}(0,1)$. $y=1$ if $x_1 > 0$, else $0$. RF params: `T=50`, `m=1`, forest seed=`2025`. Report: $(S_{\\text{dup}}, I_1, I_2, I_3)$ where $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$.\n    - **Case 3:** Data seed=`7`, `N=100`, `d=2`. $x_1=0$, $x_2=3$. $y \\sim \\text{Bernoulli}(0.5)$. RF params: `T=10`, `m=2`, forest seed=`99`. Report: $(Q, I_1, I_2)$ where $Q = I_1 + I_2$.\n- **Output:** A single comma-separated list of $10$ floating-point numbers rounded to six decimal places, enclosed in brackets.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem describes the standard Random Forest algorithm. The CART methodology, Gini impurity criterion, and mean decrease in impurity (MDI) for feature importance are all well-established concepts in statistical learning. The definitions provided are mathematically and algorithmically correct.\n- **Well-Posed:** The problem is meticulously specified. All parameters (`N`, `d`, `T`, `m`), data generation procedures, and random seeds are provided, which ensures that the result is unique, deterministic, and computable. The edge cases (constant features, no positive gain) are explicitly defined.\n- **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology.\n\nThe problem does not violate any of the invalidity criteria. It is a well-defined, scientifically sound computational task.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the implementation and solution.\n\n### Algorithmic Implementation\n\nThe solution will be structured as a set of functions that collectively implement the specified Random Forest algorithm.\n\n1.  **`gini_impurity(y)`**: A helper function to compute the Gini impurity for a given array of labels `y`.\n2.  **`find_best_split(X, y, idxs, feature_subset)`**: This function iterates through a given subset of features. For each feature, it evaluates all valid thresholds (midpoints of unique values) to find the split that maximizes the Gini gain. It returns a dictionary containing the details of the best split found (feature index, threshold, gain, and child indices) or `None` if no valid split is found.\n3.  **`grow_tree(X, y, idxs, m, d, rng, tree_importances)`**: A recursive function that builds a single decision tree. At each step (node), it checks for stopping conditions (e.g., pure node). If not a leaf, it randomly selects `m` features, calls `find_best_split` to determine the optimal split, records the contribution to feature importance ($|\\mathcal{I}| \\cdot \\Delta$), and then recursively calls itself for the left and right children.\n4.  **`run_rf_case(X, y, T, m, forest_seed)`**: This function orchestrates the training of the entire Random Forest for a single test case. It initializes a random number generator with the forest seed. It then iterates `T` times, each time creating a bootstrap sample of the data and growing a tree using `grow_tree`. It aggregates the feature importances from all trees and returns the final normalized importance scores.\n5.  **`solve()`**: The main function that prepares the data for each of the three test cases as specified, calls `run_rf_case` with the appropriate parameters, computes the required summary statistics ($S_{\\text{dup}}$, $Q$), and prints the final combined results in the specified format.\n\nRandomness is managed carefully using `numpy.random.default_rng` to ensure reproducibility for data generation and all stochastic aspects of the algorithm (bootstrapping and feature selection). Special attention is given to the edge cases, such as handling constant features (which cannot be split) and normalizing importances only if the total impurity decrease is positive.", "answer": "```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Computes the Gini impurity of a set of labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    proportions = counts / n_samples\n    return 1.0 - np.sum(proportions**2)\n\ndef find_best_split(X, y, idxs, feature_subset):\n    \"\"\"Finds the best split for a node by iterating through features and thresholds.\"\"\"\n    X_node, y_node = X[idxs], y[idxs]\n    n_node = len(y_node)\n    \n    if n_node <= 1:\n        return None\n\n    g_parent = gini_impurity(y_node)\n    best_gain = -1.0\n    best_split_info = None\n\n    for j in feature_subset:\n        feature_values = X_node[:, j]\n        unique_vals = np.unique(feature_values)\n\n        if len(unique_vals) <= 1:\n            continue\n\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n        for tau in thresholds:\n            left_mask = feature_values <= tau\n            right_mask = ~left_mask\n\n            y_left, y_right = y_node[left_mask], y_node[right_mask]\n\n            if len(y_left) == 0 or len(y_right) == 0:\n                continue\n\n            g_left = gini_impurity(y_left)\n            g_right = gini_impurity(y_right)\n            n_left, n_right = len(y_left), len(y_right)\n            \n            w_left = n_left / n_node\n            w_right = n_right / n_node\n            \n            gain = g_parent - (w_left * g_left + w_right * g_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = {\n                    'feature': j, \n                    'threshold': tau, \n                    'gain': gain,\n                    'left_idxs': idxs[left_mask],\n                    'right_idxs': idxs[right_mask]\n                }\n                \n    return best_split_info\n\ndef grow_tree(X, y, idxs, m, d, rng, tree_importances):\n    \"\"\"Recursively grows a single decision tree and accumulates feature importances.\"\"\"\n    if len(np.unique(y[idxs])) == 1:\n        return\n\n    m_node = min(m, d)\n    feature_subset = rng.choice(d, size=m_node, replace=False)\n    \n    best_split = find_best_split(X, y, idxs, feature_subset)\n    \n    if best_split is None or best_split['gain'] <= 0:\n        return\n\n    j = best_split['feature']\n    gain = best_split['gain']\n    n_samples = len(idxs)\n    tree_importances[j] += n_samples * gain\n    \n    left_idxs = best_split['left_idxs']\n    right_idxs = best_split['right_idxs']\n    \n    grow_tree(X, y, left_idxs, m, d, rng, tree_importances)\n    grow_tree(X, y, right_idxs, m, d, rng, tree_importances)\n\ndef run_rf_case(X, y, T, m, forest_seed):\n    \"\"\"Runs the Random Forest algorithm for a given case and computes feature importances.\"\"\"\n    N, d = X.shape\n    forest_rng = np.random.default_rng(forest_seed)\n    total_importances = np.zeros(d)\n\n    for _ in range(T):\n        bootstrap_idxs = forest_rng.choice(N, size=N, replace=True)\n        X_sample, y_sample = X[bootstrap_idxs], y[bootstrap_idxs]\n        \n        tree_importances = np.zeros(d)\n        \n        # Grow a tree on the bootstrap sample\n        initial_idxs = np.arange(len(y_sample))\n        grow_tree(X_sample, y_sample, initial_idxs, m, d, forest_rng, tree_importances)\n        \n        total_importances += tree_importances\n\n    S = np.sum(total_importances)\n    if S > 0:\n        normalized_importances = total_importances / S\n    else:\n        normalized_importances = np.zeros(d)\n        \n    return normalized_importances\n\ndef solve():\n    \"\"\"Generates data for three cases, runs the RF, and reports the results.\"\"\"\n    all_results = []\n\n    # Case 1\n    data_seed, N, d = 42, 200, 3\n    T, m, forest_seed = 50, 2, 2024\n    rng_case1 = np.random.default_rng(data_seed)\n    X1 = np.zeros((N, d))\n    X1[:, 0] = rng_case1.normal(size=N)\n    X1[:, 1] = 0.0\n    X1[:, 2] = rng_case1.normal(size=N)\n    y1 = (X1[:, 0] > 0).astype(int)\n    importances1 = run_rf_case(X1, y1, T, m, forest_seed)\n    all_results.extend(importances1)\n\n    # Case 2\n    data_seed, N, d = 123, 200, 3\n    T, m, forest_seed = 50, 1, 2025\n    rng_case2 = np.random.default_rng(data_seed)\n    X2 = np.zeros((N, d))\n    X2[:, 0] = rng_case2.normal(size=N)\n    X2[:, 1] = X2[:, 0]\n    X2[:, 2] = rng_case2.normal(size=N)\n    y2 = (X2[:, 0] > 0).astype(int)\n    importances2 = run_rf_case(X2, y2, T, m, forest_seed)\n    I1_2, I2_2, I3_2 = importances2\n    denom = I1_2 + I2_2 + I3_2\n    S_dup = (I1_2 + I2_2) / denom if denom > 0 else 0.0\n    all_results.extend([S_dup, I1_2, I2_2, I3_2])\n\n    # Case 3\n    data_seed, N, d = 7, 100, 2\n    T, m, forest_seed = 10, 2, 99\n    rng_case3 = np.random.default_rng(data_seed)\n    X3 = np.zeros((N, d))\n    X3[:, 0] = 0.0\n    X3[:, 1] = 3.0\n    y3 = rng_case3.integers(0, 2, size=N)\n    importances3 = run_rf_case(X3, y3, T, m, forest_seed)\n    I1_3, I2_3 = importances3\n    Q = I1_3 + I2_3\n    all_results.extend([Q, I1_3, I2_3])\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3166180"}, {"introduction": "In applied machine learning, building a powerful model is only half the battle; ensuring the integrity of your data is paramount. One of the most subtle and dangerous pitfalls is target leakage, where features are contaminated with information from the outcome itself. This exercise [@problem_id:2386893] simulates a realistic financial scenario to demonstrate how a Random Forest can be misled by such a flaw. You will see firsthand how the model can incorrectly identify a leaking variable as the most important predictor, highlighting the absolute necessity of careful feature engineering and a thorough understanding of your data's temporal structure.", "problem": "You are modeling binary loan default outcomes in a credit portfolio. For each observation $i \\in \\{1,\\dots,n\\}$, let the binary target be $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates default. You will generate synthetic covariates that are economically interpretable and then add a subtle post-outcome covariate that leaks information about $y_i$. You must then quantify how an ensemble of decision stumps (a one-split decision forest) ranks the covariates by importance and report, for each test case, the zero-based index of the most important covariate. Indices must be reported as integers.\n\nData-generating process:\n- Let the number of base covariates be $p_b = 5$. For each observation $i$, draw a base feature vector $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$ from a mean-zero multivariate normal distribution with covariance matrix $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$ defined by\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$,\nwhere $I_{p_b}$ is the identity matrix of size $p_b$ and $\\mathbf{1}$ is the $p_b$-dimensional vector of ones. The scalar $\\rho \\in (-\\frac{1}{p_b-1},1)$ controls the common correlation among base features.\n- Draw an idiosyncratic macro factor $m_i \\sim \\mathcal{N}(0,1)$ independently of $\\mathbf{x}_i$.\n- Define a latent score $s_i$ via a logistic index:\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\nwith coefficients fixed at $\\beta_0 = -0.5$, $\\beta_1 = 0.8$, $\\beta_2 = -1.0$, $\\beta_3 = 0.6$, $\\beta_4 = 0.0$, $\\beta_5 = 0.5$, and $\\gamma = 0.7$.\n- Define the default probability $p_i$ by the logistic function:\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$.\n- Draw the binary outcome $y_i \\sim \\text{Bernoulli}(p_i)$ independently across $i$.\n- Define a post-outcome covariate $z_i$ that leaks target information as\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$,\nwith $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ independent of everything else. The parameter $\\lambda \\in \\mathbb{R}$ controls the magnitude of leakage, and $\\sigma \\ge 0$ controls the amount of noise obscuring the leakage.\n- For modeling, you will form the feature vector $\\tilde{\\mathbf{x}}_i$ as follows. If the test case flag $\\text{include\\_leak} = 1$, then set\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\nwith $p = p_b + 1$ and the leaking covariate at zero-based index $p_b$. If $\\text{include\\_leak} = 0$, then set\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\nwith $p = p_b$.\n\nModel and importance:\n- Consider an ensemble of $T$ decision stumps (one-split decision trees) with $T = 200$. For each tree $t \\in \\{1,\\dots,T\\}$:\n  - Draw a bootstrap sample of size $n$ by sampling indices from $\\{1,\\dots,n\\}$ with replacement.\n  - Let $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ and select uniformly at random $m_{\\text{try}}$ distinct features from $\\{0,\\dots,p-1\\}$ to be considered for splitting.\n  - For each selected feature $j$, consider splits of the form $x_{j} \\le \\tau$ for thresholds $\\tau$ at midpoints between consecutive sorted observed values of that feature on the bootstrap sample, excluding thresholds that would yield an empty child node. Let $G(S)$ denote the Gini impurity of a set $S$ of binary labels:\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n  $$,\n  where $n_c$ is the count of class $c$ in $S$. For a parent node with label multiset $S$ and a split into left child $S_L$ and right child $S_R$, define the impurity decrease as\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right)\n  $$.\n  - Choose the feature $j^\\star$ and threshold $\\tau^\\star$ that maximize $\\Delta G$ among the considered features and thresholds. Grow a decision stump by splitting at $(j^\\star,\\tau^\\star)$.\n  - Attribute the realized impurity decrease $\\Delta G^\\star$ of the chosen split to feature $j^\\star$.\n- Define the importance of feature $j$ as the sum of attributed impurity decreases over the $T$ trees:\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$.\n- For each test case, compute the index $j_{\\max} \\in \\{0,\\dots,p-1\\}$ that maximizes $I_j$. In case of ties, take the smallest index achieving the maximum.\n\nTest suite:\nFor reproducibility, use an independent random seed $s$ per test case for both data generation and ensemble construction. Use the following four test cases, each specified by the tuple $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$:\n- Case A: $(3000, 0.1, 0.9, 0.2, 1, 11)$.\n- Case B: $(3000, 0.3, 0.6, 0.2, 1, 12)$.\n- Case C: $(1200, 0.1, 0.9, 0.2, 0, 13)$.\n- Case D: $(3000, 0.6, 0.4, 0.2, 1, 14)$.\n\nRequired program behavior and output:\n- For each test case, generate data according to the process above, train the ensemble described above, compute feature importances $\\{I_j\\}_{j=0}^{p-1}$, and return the zero-based index $j_{\\max}$ of the most important feature.\n- Your program must produce a single line of output containing the four indices as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example [$i_1$,$i_2$,$i_3$,$i_4$]. No additional text should be printed.", "solution": "The problem presented is a valid, well-posed exercise in computational statistics and machine learning, specifically concerning the evaluation of feature importance in tree-based ensemble models. The data-generating process is rigorously defined and scientifically grounded in standard models of financial econometrics. The task is to quantify feature importance using Gini impurity decrease in an ensemble of decision stumps and to identify the most impactful feature, particularly in the presence of a \"leaking\" covariate. The problem is objective, self-contained, and algorithmically specified, permitting a unique, reproducible solution.\n\nThe solution methodology proceeds in two main stages for each test case: data generation and model training with importance calculation. All mathematical entities, including variables, parameters, and numerical values, are represented in LaTeX as required.\n\n**1. Data Generation Process**\n\nFor each of the specified test cases, a synthetic dataset of size $n$ is generated according to the following stochastic process. A random seed $s$ is used to ensure reproducibility.\n\n- **Base Covariates**: A set of $p_b = 5$ base covariates, denoted by the vector $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$ for each observation $i \\in \\{1, \\dots, n\\}$, is drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$. The covariance matrix $\\Sigma(\\rho)$ is defined as\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\nwhere $I_{p_b}$ is the $p_b \\times p_b$ identity matrix and $\\mathbf{1}$ is a $p_b$-dimensional vector of ones. The parameter $\\rho$ controls the equicorrelation between these base features.\n\n- **Latent Score and Default Probability**: An idiosyncratic factor $m_i \\sim \\mathcal{N}(0,1)$ is drawn independently. A latent score $s_i$ is constructed as a linear combination of the base covariates and the macro factor:\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\nThe coefficients are fixed at $\\beta_0 = -0.5$, $\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$, and $\\gamma = 0.7$. Note that the coefficient for $x_{i,4}$ is $\\beta_4 = 0.0$, making this feature uninformative by construction with respect to the latent score. The latent score is then transformed into a probability of default, $p_i$, using the standard logistic function:\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **Binary Outcome**: The binary target variable, $y_i \\in \\{0, 1\\}$, indicating non-default ($0$) or default ($1$), is drawn from a Bernoulli distribution with the generated probability, $y_i \\sim \\text{Bernoulli}(p_i)$.\n\n- **Leaking Covariate**: A post-outcome covariate $z_i$ is generated to simulate information leakage from the target variable. Its definition is:\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\nwhere $\\delta_i$ is a noise term drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. The parameter $\\lambda$ controls the strength of the leakage, and $\\sigma$ controls the noise level. A high ratio of $|\\lambda|$ to $\\sigma$ implies a strong, easily detectable link between $z_i$ and $y_i$.\n\n- **Final Feature Matrix**: The complete feature matrix $\\tilde{\\mathbf{X}}$ is assembled. If the `include_leak` flag is $1$, the feature set is $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$. The leaking feature $z_i$ is located at the last position (zero-based index $5$). If the flag is $0$, only the base covariates are used, $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$. The number of features is denoted by $p$.\n\n**2. Feature Importance Quantification**\n\nThe importance of each feature is determined by training an ensemble of $T = 200$ decision stumps. A decision stump is a decision tree with only one split.\n\n- **Ensemble Construction**: For each of the $T$ stumps in the ensemble:\n    1. A bootstrap sample of size $n$ is created by sampling with replacement from the full dataset $(\\tilde{\\mathbf{X}}, \\mathbf{y})$.\n    2. A random subset of $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ distinct features is selected.\n    3. For each selected feature, the optimal split is found. A split is defined by a feature $j$ and a threshold $\\tau$. The quality of the split is measured by the Gini impurity decrease, $\\Delta G$. The Gini impurity of a set of labels $S$ is given by:\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    where $n_c$ is the count of class $c$ in the set $S$. The impurity decrease is the difference between the parent node's impurity and the weighted average of the two child nodes' impurities.\n    4. The feature $j^\\star$ and threshold $\\tau^\\star$ that yield the maximum impurity decrease $\\Delta G^\\star$ are chosen for the stump's split. Potential thresholds are the midpoints of consecutive unique sorted values of the feature in the bootstrap sample.\n\n- **Importance Aggregation**: The importance of a feature $j$, denoted $I_j$, is calculated as the sum of the impurity decreases it is responsible for across all trees in the ensemble:\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\n- **Result**: Finally, for each test case, the zero-based index of the feature with the highest importance score, $j_{\\max} = \\arg\\max_j I_j$, is identified. Ties are resolved by selecting the smallest index. This index is the output for the case. The procedure is implemented in Python, adhering to the specified libraries and random seeds to ensure verifiable results.", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples <= 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals) < 2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386893"}]}