{"hands_on_practices": [{"introduction": "To appreciate the power of polynomial kernels, we must first understand the limitations of simpler models. This exercise challenges you to construct a scenario where linear and quadratic models are fundamentally incapable of solving a problem that a cubic kernel can handle with ease. By working through this classic example based on a parity-like function, you will gain a concrete intuition for how higher-degree kernels capture essential feature interactions that would otherwise be missed [@problem_id:3158522].", "problem": "You are asked to construct a binary classification dataset in a way that highlights when a polynomial kernel of degree $3$ can capture interactions that kernels of degree $1$ and $2$ cannot. Consider inputs $x \\in \\{-1,+1\\}^3$ and labels defined by a parity-like rule on triple interactions. You train a kernel method with inhomogeneous polynomial kernels of degrees $1$, $2$, and $3$ on noiseless data. The training objective is empirical risk minimization with zero training error required. Which choice correctly specifies a dataset and the minimal sample size needed so that a degree $3$ kernel can exactly fit the data while degree $1$ and degree $2$ kernels cannot, for any algorithm that returns a solution in the span of the chosen kernel features?\n\nA. Use the dataset that contains all $8$ points in $\\{-1,+1\\}^3$, with labels $y = x_1 x_2 x_3$. The minimal sample size to reliably learn the cubic structure is $8$.\n\nB. Use a dataset of $6$ distinct points in $\\{-1,+1\\}^3$ chosen so that exactly half have $x_1 x_2 x_3 = +1$ and half have $x_1 x_2 x_3 = -1$. The minimal sample size is $6$.\n\nC. Use the dataset that contains all $8$ points in $\\{-1,+1\\}^3$, with labels $y = x_1 x_2 x_3$. The minimal sample size to reliably learn the cubic structure is $7$.\n\nD. Use a dataset of $4$ points in $\\{-1,+1\\}^3$ (for example, those with Hamming weight equal to $1$ or $2$), with labels $y = x_1 x_2 x_3$. The minimal sample size is $4$.", "solution": "The core of the problem lies in understanding the function spaces associated with polynomial kernels on the domain $x \\in \\{-1,+1\\}^3$. A function $f(x)$ can be learned with zero error on a training set $\\{(x_i, y_i)\\}_{i=1}^N$ by a given kernel method if there exists a function $f$ in the associated Reproducing Kernel Hilbert Space (RKHS) such that $f(x_i) = y_i$ for all $i=1, \\dots, N$.\n\nFor inputs $x \\in \\{-1,+1\\}^3$, any component $x_j$ satisfies $x_j^2 = 1$. This simplifies the set of unique monomials. The set of all possible functions on the domain $\\{-1,+1\\}^3$ forms an $8$-dimensional vector space, since the domain has $2^3=8$ points. A basis for this space is given by the set of all unique monomials:\n$$\n\\mathcal{B} = \\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3, x_1x_2x_3\\}\n$$\nThe target function is $g(x) = x_1 x_2 x_3$, which is the highest-degree monomial in this basis. The RKHS for an inhomogeneous polynomial kernel of degree $d$ is spanned by the monomials of degree up to $d$.\n- **Degree 1 kernel**: The RKHS is $\\mathcal{F}_1 = \\text{span}\\{1, x_1, x_2, x_3\\}$, a $4$-dimensional space.\n- **Degree 2 kernel**: The RKHS is $\\mathcal{F}_2 = \\text{span}\\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3\\}$, a $7$-dimensional space.\n- **Degree 3 kernel**: The RKHS is $\\mathcal{F}_3 = \\text{span}(\\mathcal{B})$, the full $8$-dimensional space.\n\nThe target function $g(x) = x_1 x_2 x_3$ is in $\\mathcal{F}_3$ but not in $\\mathcal{F}_1$ or $\\mathcal{F}_2$. A kernel method can fit the data if the target label vector (evaluated on the sample points) lies in the span of the kernel's basis functions (also evaluated on the sample points).\n\nConsider a sample of size $N \\le 7$. The $8$ basis functions of $\\mathcal{B}$, when evaluated on these $N$ points, become $8$ vectors in $\\mathbb{R}^N$. Since $N \\le 7$, these $8$ vectors must be linearly dependent. It is a known property of these basis functions (a Walsh-Hadamard system) that on any set of $N \\le 7$ points, the vector corresponding to $g(x)$ can be written as a linear combination of the 7 basis vectors for $\\mathcal{F}_2$. This means that for any dataset of size $N \\le 7$, a degree 2 kernel *can* achieve zero training error. To find a dataset that *cannot* be fit by a degree 2 kernel, we therefore need a sample size $N > 7$.\n\nThe minimal integer sample size is thus $N=8$. Let the dataset consist of all $8$ points in $\\{-1,+1\\}^3$.\n- A degree 3 kernel can fit this data because $g(x)$ is in its feature space $\\mathcal{F}_3$.\n- A degree 2 kernel cannot fit this data. On the full domain of 8 points, the 8 basis functions of $\\mathcal{B}$ are linearly independent (in fact, they are orthogonal). Therefore, the vector for $g(x)$ cannot be written as a linear combination of the 7 basis functions for $\\mathcal{F}_2$.\n\nThus, the minimal sample size required is $N=8$, using the full dataset of all 8 points. Option A correctly identifies this, whereas options B, C, and D propose sample sizes that are too small.", "answer": "$$\\boxed{A}$$", "id": "3158522"}, {"introduction": "Building on the conceptual understanding of why higher-degree features are necessary, this practice moves from theory to implementation. You will write a program to perform classification using Kernel Ridge Regression, a close cousin of the Support Vector Machine. This hands-on coding exercise not only requires you to translate the mathematical definition of the polynomial kernel into working code but also introduces the powerful idea of creating \"partial\" kernels by selecting only the most relevant feature degrees, providing a practical lesson in model selection and customization [@problem_id:3158464].", "problem": "You are given a binary classification task and asked to compare the performance of tailored partial polynomial kernels that include only selected homogeneous degrees against the full inhomogeneous polynomial kernel obtained by the complete binomial expansion. Your program must implement classification via Kernel Ridge Regression (KRR) and report, for each specified test case, the difference in test misclassification rates between a partial kernel and the corresponding full binomial kernel of the same total degree and offset.\n\nStart from the following core definitions and principles:\n- For vectors $x \\in \\mathbb{R}^p$ and $y \\in \\mathbb{R}^p$, the homogeneous degree-$m$ polynomial kernel is the symmetric tensor inner product given by $k_m(x,y) = (x^\\top y)^m$ for integer $m \\ge 0$.\n- Given an integer total degree $d \\ge 0$ and an offset $c \\ge 0$, the full inhomogeneous polynomial kernel of total degree $d$ is built by incorporating all homogeneous degrees from $0$ to $d$ with nonnegative weights induced by the binomial structure. A partial polynomial kernel selects a subset of the homogeneous degrees. Specifically, for a subset $S \\subseteq \\{0,1,\\dots,d\\}$, define\n$$\nK_S(x,y) \\;=\\; \\sum_{m \\in S} \\binom{d}{m}\\, c^{\\,d-m}\\, (x^\\top y)^m.\n$$\nThe full kernel is obtained by taking $S = \\{0,1,\\dots,d\\}$ and will be denoted by $K_{\\text{full}}(x,y)$.\n\nLearning rule:\n- Perform Kernel Ridge Regression (KRR) for classification with regularization parameter $\\lambda > 0$. Let the training data be $\\{(x_i, y_i)\\}_{i=1}^n$ with labels $y_i \\in \\{-1,+1\\}$. Let $K \\in \\mathbb{R}^{n \\times n}$ be the training Gram matrix with entries $K_{ij} = K(x_i,x_j)$ for a chosen kernel $K$. The coefficient vector $\\alpha \\in \\mathbb{R}^n$ is defined as the unique solution of the linear system\n$$\n\\big(K + n \\lambda I_n\\big)\\, \\alpha \\;=\\; y,\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix and $y = (y_1,\\dots,y_n)^\\top$. For a test point $z$, the prediction is\n$$\nf(z) \\;=\\; \\sum_{i=1}^n \\alpha_i\\, K(x_i, z),\n$$\nand the predicted label is $\\mathrm{sign}(f(z))$, with $\\mathrm{sign}(0)$ defined to be $+1$. The misclassification rate on a test set is the fraction of test points whose predicted labels disagree with their true labels, represented as a real number in $[0,1]$.\n\nTraining and test data:\n- Training inputs $X_{\\text{train}}$ in $\\mathbb{R}^{8 \\times 2}$ (each row is one input):\n  - $(\\,1.0,\\,1.0\\,)$\n  - $(\\,1.0,\\,-1.0\\,)$\n  - $(\\,-1.0,\\,1.0\\,)$\n  - $(\\,-1.0,\\,-1.0\\,)$\n  - $(\\,2.0,\\,0.5\\,)$\n  - $(\\,-2.0,\\,0.5\\,)$\n  - $(\\,0.5,\\,-2.0\\,)$\n  - $(\\,-0.5,\\,-2.0\\,)$\n- Training labels $y_{\\text{train}} \\in \\{-1,+1\\}^8$:\n  - $+1$, $-1$, $-1$, $+1$, $+1$, $-1$, $-1$, $+1$.\n- Test inputs $X_{\\text{test}}$ in $\\mathbb{R}^{8 \\times 2}$:\n  - $(\\,0.5,\\,0.5\\,)$\n  - $(\\,0.5,\\,-0.5\\,)$\n  - $(\\,-0.5,\\,0.5\\,)$\n  - $(\\,-0.5,\\,-0.5\\,)$\n  - $(\\,3.0,\\,0.1\\,)$\n  - $(\\,-3.0,\\,0.1\\,)$\n  - $(\\,0.1,\\,-3.0\\,)$\n  - $(\\,-0.1,\\,-3.0\\,)$\n- Test labels $y_{\\text{test}} \\in \\{-1,+1\\}^8$:\n  - $+1$, $-1$, $-1$, $+1$, $+1$, $-1$, $-1$, $+1$.\n\nTask:\n- For each test case below, compute two misclassification rates on the test set: one using the partial kernel $K_S$ and one using the full kernel $K_{\\text{full}}$ with the same $d$ and $c$. Report the difference\n$$\n\\Delta \\;=\\; \\text{error}(K_S) \\;-\\; \\text{error}(K_{\\text{full}}).\n$$\nA negative $\\Delta$ indicates the partial kernel achieved a lower error (an improvement), while a positive $\\Delta$ indicates the full kernel performed better.\n\nTest suite (each case is $(d, c, S, \\lambda)$ with $S$ listed explicitly):\n1. $(\\,3,\\, 1.0,\\, \\{2,3\\},\\, 10^{-2}\\,)$\n2. $(\\,3,\\, 1.0,\\, \\{0,1,2,3\\},\\, 10^{-2}\\,)$\n3. $(\\,3,\\, 1.0,\\, \\{2\\},\\, 10^{-2}\\,)$\n4. $(\\,4,\\, 0.5,\\, \\{4\\},\\, 10^{-2}\\,)$\n5. $(\\,2,\\, 1.0,\\, \\{2\\},\\, 10^{-3}\\,)$\n6. $(\\,3,\\, 1.0,\\, \\{0\\},\\, 10^{-2}\\,)$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers, rounded to six decimal places, enclosed in square brackets. For example, an output with three results should look like $[\\text{r}_1,\\text{r}_2,\\text{r}_3]$ where each $\\text{r}_i$ is a real number rounded to six decimals.\n\nScientific realism and constraints:\n- All computations are over the real numbers, and no physical units are involved.\n- Angles are not used.\n- Percentages must be represented as decimals in $[0,1]$.\n- Ensure numerical stability by using $\\lambda > 0$ as specified for each test, and by solving linear systems directly without iterative approximations.\n\nYour program must be a complete, runnable program that performs these computations and prints the single required output line. No user input is needed.", "solution": "The user has provided a problem in the domain of statistical learning, specifically concerning Kernel Ridge Regression (KRR) with polynomial kernels. The task is to compare the classification performance of partial polynomial kernels against full inhomogeneous polynomial kernels.\n\nThe problem statement has been validated and found to be self-contained, scientifically grounded in the principles of statistical learning theory, and mathematically well-posed. The definitions for the polynomial kernel, the KRR learning rule, and the evaluation metric (misclassification rate) are standard and unambiguous. The KRR objective function involves solving a linear system $(K + n \\lambda I_n) \\alpha = y$. Since the Gram matrix $K$ is positive semi-definite and the regularization parameter $\\lambda$ is strictly positive, the matrix $(K + n \\lambda I_n)$ is symmetric and positive definite, which guarantees the existence of a unique, stable solution for the coefficient vector $\\alpha$. All data and parameters are explicitly provided. Therefore, the problem is valid, and a solution will be constructed.\n\nThe solution proceeds systematically through the following steps for each test case:\n1.  **Define Kernels**: For each case specified by $(d, c, S, \\lambda)$, we define two kernels: the partial kernel $K_S(x,y)$ using the given set of degrees $S$, and the full inhomogeneous kernel $K_{\\text{full}}(x,y)$ using the complete set of degrees $S_{\\text{full}} = \\{0, 1, \\dots, d\\}$. The kernel function is given by:\n    $$\n    K(x,y) = \\sum_{m \\in \\text{degrees}} \\binom{d}{m} c^{d-m} (x^\\top y)^m\n    $$\n2.  **Compute Gram Matrices**: For each kernel, we must compute the training Gram matrix and the test Gram matrix.\n    *   The training Gram matrix, $G \\in \\mathbb{R}^{n \\times n}$, is computed on the training data $X_{\\text{train}}$. Its entries are $G_{ij} = K(x_i, x_j)$, where $x_i, x_j$ are training samples. For efficiency, we first compute a matrix of all dot products, $D_{\\text{train}} = X_{\\text{train}} X_{\\text{train}}^\\top$, and then apply the kernel-defining sum to this matrix element-wise.\n    *   The test Gram matrix, $G_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times n}$, is used for prediction. Its entries are $(G_{\\text{test}})_{ij} = K(z_i, x_j)$, where $z_i$ is a test sample and $x_j$ is a training sample. This is computed efficiently from the dot product matrix $D_{\\text{test}} = X_{\\text{test}} X_{\\text{train}}^\\top$.\n\n3.  **Solve for KRR Coefficients**: The core of KRR is solving for the coefficient vector $\\alpha \\in \\mathbb{R}^n$. For a given training Gram matrix $G$, regularization parameter $\\lambda$, training labels $y_{\\text{train}}$, and number of training samples $n$, we solve the linear system:\n    $$\n    (G + n \\lambda I_n) \\alpha = y_{\\text{train}}\n    $$\n    where $I_n$ is the $n \\times n$ identity matrix. This system is solved for $\\alpha$. This step is performed separately for the partial kernel (yielding $\\alpha_S$) and the full kernel (yielding $\\alpha_{\\text{full}}$).\n\n4.  **Predict and Evaluate**: Predictions for the test set $X_{\\text{test}}$ are generated using the computed coefficients and the test Gram matrix. The vector of predicted scores is $f_{\\text{test}} = G_{\\text{test}} \\alpha$. The predicted labels $\\hat{y}$ are obtained by taking the sign of these scores:\n    $$\n    \\hat{y}_i = \\mathrm{sign}(f_{\\text{test}, i})\n    $$\n    The problem specifies that $\\mathrm{sign}(0)$ should be mapped to $+1$. The misclassification rate is the fraction of test samples for which the predicted label does not match the true label $y_{\\text{test}}$:\n    $$\n    \\text{error} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\mathbb{I}(\\hat{y}_i \\neq y_{\\text{test}, i})\n    $$\n    where $\\mathbb{I}(\\cdot)$ is the indicator function. This rate is calculated for both the partial kernel, $\\text{error}(K_S)$, and the full kernel, $\\text{error}(K_{\\text{full}})$.\n\n5.  **Compute Final Difference**: For each test case, the final result is the difference in misclassification rates:\n    $$\n    \\Delta = \\text{error}(K_S) - \\text{error}(K_{\\text{full}})\n    $$\nThis value quantifies the change in performance due to restricting the set of homogeneous degrees in the polynomial kernel. A negative value indicates an improvement from using the partial kernel. Note that for test case $2$, the set $S$ is $\\{0,1,2,3\\}$, which is the full set for $d=3$. Therefore, $K_S = K_{\\text{full}}$, and the resulting difference $\\Delta$ must be exactly $0$. This serves as a useful sanity check for the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the kernel ridge regression problem for the specified test cases.\n    \"\"\"\n    # Define training and test data as specified in the problem statement.\n    X_train = np.array([\n        [1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0],\n        [2.0, 0.5], [-2.0, 0.5], [0.5, -2.0], [-0.5, -2.0]\n    ])\n    y_train = np.array([1, -1, -1, 1, 1, -1, -1, 1])\n\n    X_test = np.array([\n        [0.5, 0.5], [0.5, -0.5], [-0.5, 0.5], [-0.5, -0.5],\n        [3.0, 0.1], [-3.0, 0.1], [0.1, -3.0], [-0.1, -3.0]\n    ])\n    y_test = np.array([1, -1, -1, 1, 1, -1, -1, 1])\n    \n    n_train = X_train.shape[0]\n\n    # Pre-compute dot product matrices for efficiency.\n    dots_train = X_train @ X_train.T\n    dots_test = X_test @ X_train.T\n\n    # Define the test suite. S is represented as a set for efficient lookup.\n    test_cases = [\n        # (d, c, S, lambda)\n        (3, 1.0, {2, 3}, 1e-2),\n        (3, 1.0, {0, 1, 2, 3}, 1e-2),\n        (3, 1.0, {2}, 1e-2),\n        (4, 0.5, {4}, 1e-2),\n        (2, 1.0, {2}, 1e-3),\n        (3, 1.0, {0}, 1e-2)\n    ]\n\n    def gram_matrix_from_dots(dot_products_matrix, d, c, S):\n        \"\"\"\n        Computes the Gram matrix from a pre-computed matrix of dot products.\n        \"\"\"\n        gram = np.zeros_like(dot_products_matrix, dtype=np.float64)\n        for m in S:\n            if m  0 or m > d:\n                continue\n            # Binomial coefficient can be zero if c=0 and d-m>0.\n            # dot_product can be zero.\n            # Use np.power to handle element-wise exponentiation safely.\n            term_coeff = comb(d, m) * np.power(c, d - m, dtype=np.float64)\n            gram += term_coeff * np.power(dot_products_matrix, m, dtype=np.float64)\n        return gram\n\n    def calculate_misclassification_rate(d, c, S, lam):\n        \"\"\"\n        Performs KRR and returns the misclassification rate on the test set.\n        \"\"\"\n        # 1. Compute training Gram matrix\n        K_train = gram_matrix_from_dots(dots_train, d, c, S)\n        \n        # 2. Solve for alpha coefficients\n        A = K_train + n_train * lam * np.identity(n_train)\n        alpha = np.linalg.solve(A, y_train)\n        \n        # 3. Compute test Gram matrix for predictions\n        K_test = gram_matrix_from_dots(dots_test, d, c, S)\n        \n        # 4. Make predictions\n        f_test = K_test @ alpha\n        y_pred = np.sign(f_test)\n        y_pred[y_pred == 0] = 1.0 # As per rule sign(0) = +1\n        \n        # 5. Calculate misclassification rate\n        error_rate = np.mean(y_pred != y_test)\n        return error_rate\n\n    results = []\n    # Memoization for full kernel errors to avoid redundant calculations\n    full_kernel_errors = {}\n\n    for d, c, S, lam in test_cases:\n        # Calculate error for the partial kernel K_S\n        error_S = calculate_misclassification_rate(d, c, S, lam)\n        \n        # Check if the full kernel error for this (d, c, lam) is already computed\n        full_params_key = (d, c, lam)\n        if full_params_key not in full_kernel_errors:\n            S_full = set(range(d + 1))\n            error_full = calculate_misclassification_rate(d, c, S_full, lam)\n            full_kernel_errors[full_params_key] = error_full\n        else:\n            error_full = full_kernel_errors[full_params_key]\n        \n        # Compute the difference and append to results\n        delta = error_S - error_full\n        results.append(delta)\n\n    # Format the final output as specified\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3158464"}, {"introduction": "A robust machine learning model should be insensitive to arbitrary choices made during data preprocessing, such as feature scaling. This advanced exercise delves into the theoretical underpinnings of this stability for Support Vector Machines (SVMs) with polynomial kernels. You will derive how the kernel's offset parameter and the SVM's regularization term must be adjusted to make the learning outcome invariant to input scaling, a crucial insight for building reliable and predictable models in practice [@problem_id:3158558].", "problem": "Consider binary classification with a soft-margin Support Vector Machine (SVM) in a Reproducing Kernel Hilbert Space (RKHS) induced by the polynomial kernel $$k(\\mathbf{x},\\mathbf{y})=\\left(\\mathbf{x}^{\\top}\\mathbf{y}+c\\right)^{d},$$ where $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{p}$, $c\\ge 0$ is the kernel offset, and $d\\in\\mathbb{N}$ is the degree. The primal SVM objective uses hinge loss and RKHS norm regularization: for training examples $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ with labels $y_{i}\\in\\{-1,+1\\}$, it minimizes $$\\frac{\\lambda}{2}\\|w\\|_{\\mathcal{H}}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max\\{0,1-y_{i}f(\\mathbf{x}_{i})\\},$$ where $f(\\mathbf{x})=\\langle w,\\varphi(\\mathbf{x})\\rangle_{\\mathcal{H}}+b$ and $\\varphi$ is the feature map associated with $k$.\n\nSuppose the inputs are rescaled by a positive scalar $\\alpha0$, that is, $\\mathbf{x}\\mapsto\\alpha\\mathbf{x}$ for all inputs. Let the original kernel parameter be $c_{0}$ at $\\alpha=1$, and the regularization be $\\lambda_{0}$. Define the rescaled kernel as $$k_{\\alpha}(\\mathbf{x},\\mathbf{y})=\\left(\\alpha^{2}\\mathbf{x}^{\\top}\\mathbf{y}+c(\\alpha)\\right)^{d},$$ where $c(\\alpha)$ is the (to-be-chosen) offset used after rescaling the inputs.\n\nYou are asked to proceed from first principles as follows:\n\n- Using only the kernel definition and basic properties of RKHS feature maps (that is, $k(\\mathbf{x},\\mathbf{y})=\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{y})\\rangle_{\\mathcal{H}}$), impose the requirement that the rescaled kernel $k_{\\alpha}$ differ from the original kernel $k$ only by a global, input-independent multiplicative factor $s(\\alpha)^{2}0$, that is, $$k_{\\alpha}(\\mathbf{x},\\mathbf{y})=s(\\alpha)^{2}\\,k(\\mathbf{x},\\mathbf{y})\\quad\\text{for all }\\mathbf{x},\\mathbf{y}.$$ Derive the resulting expressions for $s(\\alpha)$ and the relationship between $c(\\alpha)$ and $c_{0}$.\n\n- Under the same requirement, determine how the SVM regularization parameter must transform, that is, find $\\lambda(\\alpha)$ such that if $(w^{\\star},b^{\\star})$ solves the SVM at $(\\alpha=1,c_{0},\\lambda_{0})$, then $(w^{\\star}/s(\\alpha),b^{\\star})$ solves the SVM with rescaled data and parameters $(\\alpha,c(\\alpha),\\lambda(\\alpha))$ and achieves the same objective value and functional margins on the original inputs.\n\n- Briefly interpret the scaling of the geometric margin in the feature space when $(w,b)$ is held fixed versus when it is adjusted as above.\n\nFor the purposes of this problem, define “stabilize training under feature rescaling” to mean meeting the global multiplicative equivalence requirement $k_{\\alpha}=s(\\alpha)^{2}k$ so that the optimization problem can be made invariant by an appropriate rescaling of the regularization parameter. Provide your final answer as a single closed-form expression for $c(\\alpha)$ in terms of $\\alpha$ and $c_{0}$. No numerical evaluation is required, and no rounding is needed. Express your final answer without units.", "solution": "The problem asks us to find conditions under which an SVM with a polynomial kernel produces an equivalent result after the input data is scaled. This is achieved by adjusting the kernel offset $c$ and the regularization parameter $\\lambda$.\n\nFirst, we derive the relationship between the original offset $c_0$ and the new offset $c(\\alpha)$ by enforcing the equivalence condition on the kernels. The requirement is that the new kernel $k_{\\alpha}$ is just a scaled version of the original kernel $k$:\n$$k_{\\alpha}(\\mathbf{x},\\mathbf{y})=s(\\alpha)^{2}\\,k(\\mathbf{x},\\mathbf{y})$$\nSubstituting the definitions of the polynomial kernels, we get:\n$$\\left(\\alpha^{2}\\mathbf{x}^{\\top}\\mathbf{y}+c(\\alpha)\\right)^{d} = s(\\alpha)^{2}\\,\\left(\\mathbf{x}^{\\top}\\mathbf{y}+c_{0}\\right)^{d}$$\nTaking the $d$-th root of both sides gives:\n$$\\alpha^{2}\\mathbf{x}^{\\top}\\mathbf{y}+c(\\alpha) = (s(\\alpha)^2)^{1/d} \\left(\\mathbf{x}^{\\top}\\mathbf{y}+c_{0}\\right)$$\nThis equation must hold for all input vectors $\\mathbf{x}$ and $\\mathbf{y}$. We can treat it as a polynomial identity in the variable $z = \\mathbf{x}^{\\top}\\mathbf{y}$. For the two linear functions of $z$ to be identical, their coefficients must match.\n- Matching the coefficient of $z$: $\\alpha^2 = (s(\\alpha)^2)^{1/d}$\n- Matching the constant term: $c(\\alpha) = (s(\\alpha)^2)^{1/d} c_0$\n\nBy substituting the first identity into the second, we immediately find the required relationship for the offset parameter:\n$$c(\\alpha) = \\alpha^2 c_0$$\nFrom the first identity, we can also find the scaling factor $s(\\alpha)$ by raising both sides to the power of $d/2$, which gives $s(\\alpha) = \\alpha^d$.\n\nNext, we determine how the regularization parameter $\\lambda$ must scale. The solution to the SVM is found by solving the dual optimization problem, which for this SVM formulation is:\n$$\\max_{\\boldsymbol{\\alpha}} \\left( \\sum_i \\alpha_i - \\frac{1}{2\\lambda} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j k(\\mathbf{x}_i, \\mathbf{x}_j) \\right)$$\nsubject to $\\sum \\alpha_i y_i = 0$ and $0 \\le \\alpha_i \\le 1/n$.\nFor the solution (the optimal dual variables $\\boldsymbol{\\alpha}^\\star$) to be invariant, the dual objective functions for the original and scaled problems must be equivalent. Let's compare the quadratic terms. For the original problem, the coefficient of the quadratic part is proportional to $1/\\lambda_0$. For the new problem with kernel $k_\\alpha = s(\\alpha)^2 k_0$, the coefficient is proportional to $s(\\alpha)^2 / \\lambda(\\alpha)$. For the optimization problems to be identical, these coefficients must be equal:\n$$\\frac{1}{\\lambda_0} = \\frac{s(\\alpha)^2}{\\lambda(\\alpha)} \\implies \\lambda(\\alpha) = s(\\alpha)^2 \\lambda_0 = \\alpha^{2d} \\lambda_0$$\nWith these adjustments, the dual problem remains unchanged, yielding the same optimal $\\boldsymbol{\\alpha}^\\star$. This means the resulting decision function $f^\\star(\\mathbf{x}) + b^\\star$ is also identical.\n\nFinally, we interpret the geometric margin, $1/\\|w^\\star\\|_{\\mathcal{H}}$. The RKHS norm of the optimal function $f_0^\\star$ scales as $\\|f_0^\\star\\|_{\\mathcal{H}_\\alpha} = \\|f_0^\\star\\|_{\\mathcal{H}_0} / s(\\alpha)$. Since the optimal function is the same for both problems, the new primal weight norm $\\|w_\\alpha^\\star\\|_{\\mathcal{H}_\\alpha}$ is related to the old one by $\\|w_\\alpha^\\star\\|_{\\mathcal{H}_\\alpha} = \\|w_0^\\star\\|_{\\mathcal{H}_0} / s(\\alpha)$. Therefore, the geometric margin scales as $1/\\|w_\\alpha^\\star\\|_{\\mathcal{H}_\\alpha} = s(\\alpha)/\\|w_0^\\star\\|_{\\mathcal{H}_0} = s(\\alpha) \\times (\\text{original margin})$. The margin is scaled by the factor $s(\\alpha) = \\alpha^d$. While the decision boundary and support vectors are invariant, the geometry of the feature space itself is rescaled to absorb the input scaling, which manifests as a change in the margin width.\n\nThe question asks for the closed-form expression for $c(\\alpha)$, which we derived as the primary result.", "answer": "$$\n\\boxed{c(\\alpha) = \\alpha^{2}c_{0}}\n$$", "id": "3158558"}]}