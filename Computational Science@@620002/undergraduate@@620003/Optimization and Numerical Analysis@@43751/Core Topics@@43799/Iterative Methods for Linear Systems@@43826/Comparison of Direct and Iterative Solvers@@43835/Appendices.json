{"hands_on_practices": [{"introduction": "While iterative solvers can be remarkably efficient for certain large, sparse systems, their convergence is not always guaranteed. This first exercise provides a crucial lesson by presenting a system where the popular Jacobi method fails to converge, compelling us to use a reliable direct method to find the correct solution. This practice underscores the importance of understanding the limitations of iterative techniques and knowing when a direct solver is the necessary, robust choice [@problem_id:2160102].", "problem": "In numerical analysis, linear systems of the form $A\\mathbf{x} = \\mathbf{b}$ can be solved using either direct methods (like Gaussian elimination) or iterative methods (like the Jacobi method). While iterative methods can be very efficient for large, sparse systems, their convergence is not guaranteed for all matrices. A sufficient, but not necessary, condition for the Jacobi method to converge is that the matrix $A$ must be strictly diagonally dominant.\n\nConsider the following $2 \\times 2$ linear system, which is not strictly diagonally dominant:\n$$\n\\begin{pmatrix} 1  2 \\\\ 3  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix}\n$$\nAn attempt to solve this system using the Jacobi method with an initial guess $\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ reveals that the iterates diverge rapidly. To obtain the correct solution, one must use a direct method.\n\nDetermine the exact solution vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ for this system. Express your answer as a row matrix $\\begin{pmatrix} x_1  x_2 \\end{pmatrix}$.", "solution": "We solve the linear system $A\\mathbf{x}=\\mathbf{b}$ with $A=\\begin{pmatrix}1  2 \\\\ 3  1\\end{pmatrix}$ and $\\mathbf{b}=\\begin{pmatrix}5 \\\\ 7\\end{pmatrix}$ by a direct method (Gaussian elimination).\n\nWrite the system as equations:\n$$\n\\begin{cases}\nx_{1}+2x_{2}=5, \\\\\n3x_{1}+x_{2}=7.\n\\end{cases}\n$$\nEliminate $x_{1}$ by multiplying the first equation by $3$ and subtracting the second equation:\n$$\n(3x_{1}+6x_{2})-(3x_{1}+x_{2})=15-7 \\;\\;\\Rightarrow\\;\\; 5x_{2}=8 \\;\\;\\Rightarrow\\;\\; x_{2}=\\frac{8}{5}.\n$$\nSubstitute $x_{2}=\\frac{8}{5}$ into the first equation:\n$$\nx_{1}+2\\left(\\frac{8}{5}\\right)=5 \\;\\;\\Rightarrow\\;\\; x_{1}+\\frac{16}{5}=5 \\;\\;\\Rightarrow\\;\\; x_{1}=5-\\frac{16}{5}=\\frac{9}{5}.\n$$\nTherefore, the exact solution vector is $\\begin{pmatrix}\\frac{9}{5} \\\\ \\frac{8}{5}\\end{pmatrix}$. For completeness, this agrees with the matrix inverse method: $\\det(A)=1\\cdot 1-2\\cdot 3=-5$, $\\operatorname{adj}(A)=\\begin{pmatrix}1  -2 \\\\ -3  1\\end{pmatrix}$, so\n$$\n\\mathbf{x}=A^{-1}\\mathbf{b}=\\frac{1}{-5}\\begin{pmatrix}1  -2 \\\\ -3  1\\end{pmatrix}\\begin{pmatrix}5 \\\\ 7\\end{pmatrix}=\\frac{1}{-5}\\begin{pmatrix}-9 \\\\ -8\\end{pmatrix}=\\begin{pmatrix}\\frac{9}{5} \\\\ \\frac{8}{5}\\end{pmatrix}.\n$$\nExpressed as a row matrix, this is $\\begin{pmatrix}\\frac{9}{5}  \\frac{8}{5}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{9}{5}  \\frac{8}{5}\\end{pmatrix}}$$", "id": "2160102"}, {"introduction": "Why do some iterative methods converge while others, like the one in our previous practice, diverge? The answer lies in the mathematical properties of the system's matrix, specifically the spectral radius, $\\rho$, of the iteration matrix. This exercise moves from empirical observation to theory, guiding you to calculate this critical value for a system where the Jacobi method is guaranteed to succeed, as $\\rho  1$. Mastering this concept provides a quantitative tool to predict and understand the behavior of iterative solvers before committing computational resources [@problem_id:2160047].", "problem": "In the field of numerical linear algebra, iterative methods provide an alternative to direct methods for solving linear systems of the form $A\\mathbf{x} = \\mathbf{b}$. One of the simplest iterative schemes is the Jacobi method. The convergence of the Jacobi method is determined by the spectral radius of its iteration matrix.\n\nFor a given $n \\times n$ matrix $A$, we can decompose it as $A = D - L - U$, where $D$ is a diagonal matrix containing the diagonal entries of $A$, $-L$ is the strictly lower triangular part of $A$, and $-U$ is the strictly upper triangular part of $A$. The Jacobi iteration matrix, denoted $T_J$, is then defined as $T_J = D^{-1}(L+U)$. An iterative process based on this matrix converges for any initial guess if and only if its spectral radius, $\\rho(T_J)$, is less than 1. The spectral radius is the maximum absolute value of the eigenvalues of $T_J$.\n\nConsider the specific matrix:\n$$\nA = \\begin{pmatrix} 4  1 \\\\ -2  5 \\end{pmatrix}\n$$\nThis matrix is strictly diagonally dominant, which guarantees the convergence of the Jacobi method. Your task is to explicitly verify this theoretical guarantee by direct calculation.\n\nCalculate the exact spectral radius, $\\rho(T_J)$, for the given matrix $A$. Express your answer as a single, simplified analytical expression.", "solution": "We are given $A=\\begin{pmatrix}4  1 \\\\ -2  5\\end{pmatrix}$ with the decomposition $A=D-L-U$, where $D$ is the diagonal of $A$, $-L$ is the strictly lower triangular part of $A$, and $-U$ is the strictly upper triangular part of $A$. Thus\n$$\nD=\\begin{pmatrix}4  0 \\\\ 0  5\\end{pmatrix},\\quad -L=\\begin{pmatrix}0  0 \\\\ -2  0\\end{pmatrix}\\;\\Rightarrow\\;L=\\begin{pmatrix}0  0 \\\\ 2  0\\end{pmatrix},\\quad -U=\\begin{pmatrix}0  1 \\\\ 0  0\\end{pmatrix}\\;\\Rightarrow\\;U=\\begin{pmatrix}0  -1 \\\\ 0  0\\end{pmatrix}.\n$$\nThe Jacobi iteration matrix is\n$$\nT_{J}=D^{-1}(L+U).\n$$\nCompute $L+U$ and $D^{-1}$:\n$$\nL+U=\\begin{pmatrix}0  -1 \\\\ 2  0\\end{pmatrix},\\qquad D^{-1}=\\begin{pmatrix}\\frac{1}{4}  0 \\\\ 0  \\frac{1}{5}\\end{pmatrix}.\n$$\nTherefore\n$$\nT_{J}=D^{-1}(L+U)=\\begin{pmatrix}\\frac{1}{4}  0 \\\\ 0  \\frac{1}{5}\\end{pmatrix}\\begin{pmatrix}0  -1 \\\\ 2  0\\end{pmatrix}=\\begin{pmatrix}0  -\\frac{1}{4} \\\\ \\frac{2}{5}  0\\end{pmatrix}.\n$$\nTo find the spectral radius, compute the eigenvalues of $T_{J}$. The characteristic polynomial is\n$$\n\\det(\\lambda I - T_{J})=\\det\\begin{pmatrix}\\lambda  \\frac{1}{4} \\\\ -\\frac{2}{5}  \\lambda\\end{pmatrix}=\\lambda^{2}+\\frac{1}{10}.\n$$\nThus the eigenvalues satisfy $\\lambda^{2}=-\\frac{1}{10}$, giving\n$$\n\\lambda=\\pm i\\,\\frac{1}{\\sqrt{10}}.\n$$\nThe spectral radius is the maximum absolute value of the eigenvalues, hence\n$$\n\\rho(T_{J})=\\frac{1}{\\sqrt{10}}.\n$$\nThis is less than $1$, confirming convergence, and provides the exact spectral radius.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{10}}}$$", "id": "2160047"}, {"introduction": "The choice between a direct and an iterative solver often comes down to a trade-off between robustness and computational efficiency. This final practice explores this trade-off by modeling the cost of two different solution schemes for a hypothetical physics problem. By analyzing how the cost of each method scales with the problem's resolution parameter $N$, you will determine the 'crossover point' where one approach becomes more efficient than the other, a fundamental analysis skill in applied mathematics and computational science [@problem_id:2160054].", "problem": "A computational scientist is evaluating two different numerical schemes to solve a linear system of equations arising from a physical model. The problem's resolution is controlled by a single positive integer parameter $N$. The choice between the schemes depends on their computational cost, estimated in floating-point operations (flops).\n\n**Scheme 1: Direct Solver on a Sparse Matrix**\nThis scheme discretizes the entire domain, resulting in a large, sparse linear system of size $n_1 \\times n_1$. The matrix size is related to the resolution parameter by $n_1 = N^2$. The system is solved using a direct sparse LU factorization. The computational cost for this solver is well-approximated by the formula:\n$$F_1 = \\gamma_D n_1^{3/2}$$\nwhere $\\gamma_D$ is a constant related to the solver's efficiency and the matrix structure.\n\n**Scheme 2: Iterative Solver on a Dense Matrix**\nThis scheme uses a different formulation that only requires discretizing the boundary of the domain. This results in a smaller, but dense, linear system of size $n_2 \\times n_2$. The matrix size for this scheme is $n_2 = \\gamma_S N$, where $\\gamma_S$ is a scaling constant. The resulting dense matrix is ill-conditioned, with its condition number $\\kappa$ scaling with the resolution as $\\kappa = \\gamma_K N$, where $\\gamma_K$ is another constant. An iterative solver is employed, and its total computational cost is estimated by:\n$$F_2 = \\gamma_I n_2^2 \\sqrt{\\kappa}$$\nwhere $\\gamma_I$ is a constant reflecting the properties of the iterative method.\n\nAll constants $\\gamma_D, \\gamma_S, \\gamma_K$, and $\\gamma_I$ are positive real numbers.\n\nDetermine the analytical expression for the critical resolution parameter, $N_{\\text{crit}}$, at which the computational costs of both schemes are identical. Your answer should be expressed in terms of the constants $\\gamma_D, \\gamma_S, \\gamma_K$, and $\\gamma_I$.", "solution": "We are given two computational cost models as functions of the resolution parameter $N$. For Scheme 1, the matrix size is $n_1 = N^2$ and the cost is\n$$\nF_1 = \\gamma_D n_1^{3/2}.\n$$\nSubstituting $n_1 = N^2$, we obtain\n$$\nF_1 = \\gamma_D (N^2)^{3/2} = \\gamma_D N^3.\n$$\n\nFor Scheme 2, the matrix size is $n_2 = \\gamma_S N$ and the condition number is $\\kappa = \\gamma_K N$. The cost model is\n$$\nF_2 = \\gamma_I n_2^2 \\sqrt{\\kappa}.\n$$\nSubstituting $n_2 = \\gamma_S N$ and $\\kappa = \\gamma_K N$, we get\n$$\nF_2 = \\gamma_I (\\gamma_S N)^2 \\sqrt{\\gamma_K N} = \\gamma_I \\gamma_S^2 N^2 \\gamma_K^{1/2} N^{1/2} = \\gamma_I \\gamma_S^2 \\gamma_K^{1/2} N^{5/2}.\n$$\n\nThe critical resolution $N_{\\text{crit}}$ is defined by $F_1 = F_2$. Setting the two costs equal,\n$$\n\\gamma_D N^3 = \\gamma_I \\gamma_S^2 \\gamma_K^{1/2} N^{5/2}.\n$$\nSince $N > 0$, we divide both sides by $N^{5/2}$ to obtain\n$$\n\\gamma_D N^{1/2} = \\gamma_I \\gamma_S^2 \\gamma_K^{1/2}.\n$$\nSolving for $N^{1/2}$ and squaring both sides yields\n$$\nN_{\\text{crit}} = \\frac{\\gamma_I^2 \\gamma_S^4 \\gamma_K}{\\gamma_D^2}.\n$$\nAll constants are positive, so $N_{\\text{crit}}$ is positive as required.", "answer": "$$\\boxed{\\frac{\\gamma_{I}^{2} \\gamma_{S}^{4} \\gamma_{K}}{\\gamma_{D}^{2}}}$$", "id": "2160054"}]}