{"hands_on_practices": [{"introduction": "The choice between a direct and an iterative method often depends on the fundamental properties of the linear system itself. This first practice problem explores a critical scenario: a singular system, which does not have a unique solution. By applying both Gaussian elimination and the Jacobi method, you will see firsthand how these two classes of algorithms behave differently, revealing fundamental insights into their underlying design and how they report on the nature of the solution set [@problem_id:2180013].", "problem": "Consider the following system of linear equations $A\\mathbf{x} = \\mathbf{b}$:\n$$\n\\begin{align*}\n2x_1 - x_2 + 3x_3 &= 4 \\\\\n-4x_1 + 2x_2 - 6x_3 &= -8 \\\\\nx_1 + x_2 + x_3 &= 6\n\\end{align*}\n$$\nTwo different numerical methods are proposed to solve this system.\nMethod 1: A direct method, specifically Gaussian elimination, which systematically transforms the augmented matrix $[A|\\mathbf{b}]$ into an upper triangular form to solve for the variables.\nMethod 2: A stationary iterative method, specifically the Jacobi method, which generates a sequence of approximations $\\mathbf{x}^{(k)}$ starting from an initial guess $\\mathbf{x}^{(0)} = [0, 0, 0]^T$.\n\nWhich of the following statements most accurately describes the expected outcome when applying these two methods to the given linear system?\n\nA. Both methods will fail. Gaussian elimination will halt due to a division-by-zero error, and the Jacobi method will produce a sequence of vectors that diverges to infinity.\n\nB. Gaussian elimination will correctly identify that the system has a unique solution. The Jacobi method will also converge to this unique solution.\n\nC. Gaussian elimination will terminate by revealing that the system is inconsistent and has no solution. The Jacobi method will fail to converge.\n\nD. Both methods will successfully find a solution. Gaussian elimination will provide the general form of the infinite solutions, and the Jacobi method will converge to one particular solution from this infinite set.\n\nE. Gaussian elimination will terminate by revealing that the system has infinitely many solutions. The Jacobi method will fail to converge to a specific, unique solution vector.", "solution": "Write the system as $A\\mathbf{x}=\\mathbf{b}$ with\n$$\nA=\\begin{pmatrix}\n2 & -1 & 3\\\\\n-4 & 2 & -6\\\\\n1 & 1 & 1\n\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}\n4\\\\\n-8\\\\\n6\n\\end{pmatrix}.\n$$\nObserve that the second equation is exactly $-2$ times the first equation:\n$$\n-2\\,(2x_{1}-x_{2}+3x_{3})=-4x_{1}+2x_{2}-6x_{3}=-8,\n$$\nso the second row is linearly dependent on the first, and the right-hand sides satisfy the same dependence, hence no inconsistency arises from the first two equations. The third equation is not a scalar multiple of the first, so it is independent. Thus $\\operatorname{rank}(A)=2$ and $\\operatorname{rank}([A|\\mathbf{b}])=2<3$, implying there are infinitely many solutions. Performing Gaussian elimination confirms this and yields one free variable. For instance, solving\n$$\n\\begin{cases}\n2x_{1}-x_{2}+3x_{3}=4,\\\\\nx_{1}+x_{2}+x_{3}=6\n\\end{cases}\n$$\nfor $x_{1},x_{2}$ in terms of the free parameter $x_{3}=t$ gives\n$$\n3x_{1}+4x_{3}=10\\;\\Rightarrow\\;x_{1}=\\frac{10-4t}{3},\\qquad\nx_{2}=6-x_{1}-x_{3}=6-\\frac{10-4t}{3}-t=\\frac{8+t}{3},\n$$\nso Gaussian elimination will produce the general solution set with infinitely many solutions. Therefore, the direct method succeeds and does not report inconsistency or uniqueness.\n\nFor the Jacobi method, write the Jacobi iteration in the standard splitting $A=D+L+U$, where\n$$\nD=\\operatorname{diag}(2,2,1),\\qquad L+U=A-D=\\begin{pmatrix}\n0 & -1 & 3\\\\\n-4 & 0 & -6\\\\\n1 & 1 & 0\n\\end{pmatrix}.\n$$\nThe Jacobi iteration is\n$$\n\\mathbf{x}^{(k+1)}=D^{-1}\\big(\\mathbf{b}-(L+U)\\mathbf{x}^{(k)}\\big)\n= D^{-1}\\mathbf{b} - D^{-1}(L+U)\\mathbf{x}^{(k)}\\equiv \\mathbf{c}+T\\,\\mathbf{x}^{(k)},\n$$\nwith iteration matrix\n$$\nT=-D^{-1}(L+U)=-\\begin{pmatrix}\n\\frac{1}{2} & 0 & 0\\\\\n0 & \\frac{1}{2} & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & -1 & 3\\\\\n-4 & 0 & -6\\\\\n1 & 1 & 0\n\\end{pmatrix}\n=\\begin{pmatrix}\n0 & \\frac{1}{2} & -\\frac{3}{2}\\\\\n2 & 0 & 3\\\\\n-1 & -1 & 0\n\\end{pmatrix}.\n$$\nConvergence of the Jacobi method requires the spectral radius $\\rho(T)<1$. Compute the characteristic polynomial of $T$:\n$$\n\\det(T-\\lambda I)=\\det\\begin{pmatrix}\n-\\lambda & \\frac{1}{2} & -\\frac{3}{2}\\\\\n2 & -\\lambda & 3\\\\\n-1 & -1 & -\\lambda\n\\end{pmatrix}\n=-\\lambda^{3}-\\frac{1}{2}\\lambda+\\frac{3}{2},\n$$\nso the eigenvalues satisfy\n$$\n2\\lambda^{3}+\\lambda-3=0.\n$$\nOne root is $\\lambda=1$ since $2\\cdot 1+1-3=0$. Factoring gives\n$$\n2\\lambda^{3}+\\lambda-3=(\\lambda-1)\\big(2\\lambda^{2}+2\\lambda+3\\big),\n$$\nand the remaining eigenvalues are\n$$\n\\lambda=\\frac{-2\\pm \\sqrt{-20}}{4}=\\frac{-1\\pm i\\sqrt{5}}{2},\n$$\nwhose modulus is\n$$\n\\left|\\frac{-1\\pm i\\sqrt{5}}{2}\\right|=\\sqrt{\\frac{1}{4}+\\frac{5}{4}}=\\sqrt{\\frac{3}{2}}>1.\n$$\nTherefore\n$$\n\\rho(T)=\\max\\left\\{1,\\sqrt{\\frac{3}{2}}\\right\\}=\\sqrt{\\frac{3}{2}}>1,\n$$\nso the Jacobi iteration does not converge (from the given initial guess $\\mathbf{x}^{(0)}=\\mathbf{0}$ or in general) to a solution.\n\nCombining these facts: Gaussian elimination will show the system has infinitely many solutions, while the Jacobi method will fail to converge to a specific, unique solution vector. The most accurate choice is E.", "answer": "$$\\boxed{E}$$", "id": "2180013"}, {"introduction": "For large systems, particularly those that are sparse (containing many zeros), direct methods face a unique challenge. This exercise investigates the \"fill-in\" phenomenon, where the process of Gaussian elimination can turn zero entries into non-zero ones, dramatically increasing memory requirements and computational cost. By analyzing a carefully constructed sparse matrix, you will discover that this fill-in is not random but depends critically on the matrix's structure, a key reason why iterative methods are often favored for large-scale applications [@problem_id:2180037].", "problem": "In the field of numerical linear algebra, solving systems of linear equations $Ax=b$ is a fundamental task. When the matrix $A$ is large and sparse (contains many zero entries), direct methods like Gaussian elimination can be computationally expensive due to a phenomenon called \"fill-in,\" where initially zero entries become non-zero during the procedure. Understanding the pattern of fill-in is crucial for choosing efficient solution strategies.\n\nConsider a $5 \\times 5$ matrix $A$ with entries denoted by $A_{ij}$. The matrix is sparse, and its non-zero entries are located *only* on the main diagonal (where $i=j$) and in the first column (where $j=1$). All other entries of the matrix are zero. You may assume that all these specified non-zero entries are indeed numerically different from zero.\n\nThe first step of Gaussian elimination (without pivoting) is performed on this matrix to introduce zeros into the first column below the main diagonal. This is accomplished by applying the row operations $R_i \\leftarrow R_i - m_{i1} R_1$ for each row $i$ from 2 to 5. Here, $R_i$ represents the $i$-th row of the matrix, and the multiplier $m_{i1}$ is calculated as the ratio $A_{i1}/A_{11}$.\n\nWhich of the following options correctly identifies the complete set of coordinate pairs $(i,j)$ corresponding to entries that were initially zero but become non-zero during this first step of elimination?\n\nA. The set of all $(i,j)$ such that $i > 1, j > 1$.\n\nB. The set of all $(i,j)$ such that $i > 1, j > 1,$ and $i \\neq j$.\n\nC. The set containing the coordinate pairs $(2,1), (3,1), (4,1), (5,1)$.\n\nD. The empty set; no fill-in occurs.\n\nE. The set of all $(i,j)$ such that $j=2, 3, 4, 5$ and $i \\neq j$.", "solution": "The problem asks us to identify the \"fill-in\" entries after the first step of Gaussian elimination on a specific sparse matrix. Fill-in occurs at a position $(i,j)$ if the entry $A_{ij}$ is initially zero but becomes non-zero after the operation.\n\nFirst, let's establish the structure of the $5 \\times 5$ matrix $A$. The non-zero entries are specified to be only on the main diagonal ($A_{ii}$ for $i=1, \\dots, 5$) and in the first column ($A_{i1}$ for $i=1, \\dots, 5$). Note that the entry $A_{11}$ is included in both descriptions. We can represent the structure of the matrix, using 'x' for a non-zero entry and '0' for a zero entry:\n\n$$\nA = \\begin{pmatrix}\n\\text{x} & 0 & 0 & 0 & 0 \\\\\n\\text{x} & \\text{x} & 0 & 0 & 0 \\\\\n\\text{x} & 0 & \\text{x} & 0 & 0 \\\\\n\\text{x} & 0 & 0 & \\text{x} & 0 \\\\\n\\text{x} & 0 & 0 & 0 & \\text{x}\n\\end{pmatrix}\n$$\n\nThe first step of Gaussian elimination involves row operations to zero out the elements of the first column below the diagonal. The operation for each row $i$ from 2 to 5 is given by $R_i \\leftarrow R_i - m_{i1} R_1$, where $m_{i1} = A_{i1}/A_{11}$. Let the matrix after this step be $A'$. The new entry $A'_{ij}$ is calculated from the old entries as:\n\n$$\nA'_{ij} = A_{ij} - m_{i1} A_{1j} = A_{ij} - \\left(\\frac{A_{i1}}{A_{11}}\\right) A_{1j}\n$$\n\nThis update is performed for $i \\in \\{2, 3, 4, 5\\}$ and for all columns $j \\in \\{1, 2, 3, 4, 5\\}$. The first row remains unchanged, so $A'_{1j} = A_{1j}$.\n\nWe are looking for fill-in, which means we are interested in positions $(i,j)$ where $A_{ij}=0$ but $A'_{ij} \\neq 0$. The operations are applied to rows $i=2, 3, 4, 5$.\n\nLet's analyze the update formula for these rows: $A'_{ij} = A_{ij} - (A_{i1}/A_{11}) A_{1j}$.\nFor fill-in to occur at $(i,j)$, we must have $A_{ij}=0$ and the update term $(A_{i1}/A_{11}) A_{1j}$ must be non-zero. A non-zero update term requires $A_{i1} \\neq 0$, $A_{11} \\neq 0$, and $A_{1j} \\neq 0$.\n\nFrom the problem description, we know $A_{11}$ is non-zero. For $i \\in \\{2, 3, 4, 5\\}$, the elements $A_{i1}$ are in the first column and thus are also non-zero. So the multipliers $m_{i1}$ are all non-zero.\n\nThe critical part is the term $A_{1j}$, the element from the pivot row (the first row). Let's examine the first row of matrix $A$:\n$R_1 = (A_{11}, A_{12}, A_{13}, A_{14}, A_{15})$.\nAccording to the matrix structure, the only non-zero element in the first row is $A_{11}$ (as it's on the main diagonal). All other elements $A_{12}, A_{13}, A_{14}, A_{15}$ are zero.\n\nNow, we can analyze the new entries $A'_{ij}$ for $i \\in \\{2,3,4,5\\}$ for each column $j$:\n\nCase 1: $j=1$\n$A'_{i1} = A_{i1} - (A_{i1}/A_{11}) A_{11} = A_{i1} - A_{i1} = 0$.\nThe initial entries $A_{i1}$ for $i>1$ were non-zero, and they become zero. This is the goal of elimination, not fill-in. So, no fill-in occurs in the first column.\n\nCase 2: $j \\in \\{2, 3, 4, 5\\}$\nFor any of these columns, the element in the first row is $A_{1j}=0$.\nThe update formula becomes:\n$A'_{ij} = A_{ij} - (A_{i1}/A_{11}) A_{1j} = A_{ij} - (A_{i1}/A_{11}) \\cdot 0 = A_{ij} - 0 = A_{ij}$.\n\nThis result shows that for all rows $i \\in \\{2, 3, 4, 5\\}$ and all columns $j \\in \\{2, 3, 4, 5\\}$, the matrix entries do not change: $A'_{ij} = A_{ij}$.\n\nSince the entries in this entire submatrix do not change, any entry that was initially zero will remain zero. Specifically, for an entry $(i, j)$ to experience fill-in, it must have been zero initially ($A_{ij}=0$). Our analysis shows that if $A_{ij}=0$ (and $j>1$), then $A'_{ij}$ will also be $0$.\n\nTherefore, no zero entries become non-zero during the first step of Gaussian elimination. The set of coordinates experiencing fill-in is the empty set.\n\nComparing this conclusion with the given options:\nA. The set of all $(i,j)$ such that $i > 1, j > 1$. Incorrect.\nB. The set of all $(i,j)$ such that $i > 1, j > 1,$ and $i \\neq j$. This would be a large fill-in, but it's incorrect.\nC. The set containing the coordinate pairs $(2,1), (3,1), (4,1), (5,1)$. These entries become zero, they don't fill-in. Incorrect.\nD. The empty set; no fill-in occurs. Correct.\nE. The set of all $(i,j)$ such that $j=2, 3, 4, 5$ and $i \\neq j$. Incorrect.", "answer": "$$\\boxed{D}$$", "id": "2180037"}, {"introduction": "When an iterative method is viable, its efficiency is paramount. The central question then becomes: how quickly does it converge? This final problem moves from qualitative differences to a quantitative comparison of performance by examining the *rate of convergence*. You will translate the abstract notion of an error constant into a concrete number of required iterations, providing a practical understanding of how to evaluate and compare the speed of different iterative algorithms [@problem_id:2180012].", "problem": "In the field of orbital mechanics, engineers often use iterative numerical methods to solve complex, non-linear equations for satellite positioning. Consider one such problem where two different algorithms, Method A and Method B, are proposed to find a solution. Both methods are known to exhibit linear convergence near the true solution, meaning the error at a given iteration is proportional to the error from the previous iteration.\n\nFor Method A, the error bound is described by the inequality $|e_{k+1}| \\le C_A |e_k|$, where $C_A = \\exp(-\\alpha)$ is the asymptotic error constant. Here, $\\alpha$ is a dimensionless parameter related to the algorithm's damping strategy.\n\nFor Method B, the error bound is given by $|e_{k+1}| \\le C_B |e_k|$, with an error constant $C_B = \\frac{1}{1 + \\beta^2}$. Here, $\\beta$ is a dimensionless parameter representing the stiffness of the system of equations being solved.\n\nAssume a particular scenario where the parameters are $\\alpha = 0.25$ and $\\beta = 3$. Both methods start with the same initial guess, which has an error magnitude of $|e_0| = 0.8$. The goal is to reach a tolerance where the error magnitude is less than or equal to $\\epsilon = 1.0 \\times 10^{-7}$.\n\nCalculate how many more full iterations the slower iterative method requires to reach the desired tolerance compared to the faster method. The number of iterations must be an integer.", "solution": "Given linear convergence with bound $|e_{k+1}| \\le C|e_{k}|$ and $0<C<1$, by induction we have the $k$-step bound\n$$\n|e_{k}| \\le C^{k}|e_{0}|.\n$$\nTo achieve $|e_{k}| \\le \\epsilon$, it suffices that\n$$\nC^{k}|e_{0}| \\le \\epsilon \\quad \\Longleftrightarrow \\quad k \\ge \\frac{\\ln(\\epsilon/|e_{0}|)}{\\ln C}.\n$$\nThe minimal integer number of iterations is $k_{\\min}=\\lceil \\frac{\\ln(\\epsilon/|e_{0}|)}{\\ln C} \\rceil$.\n\nMethod A: $C_{A}=\\exp(-\\alpha)$ with $\\alpha=0.25=\\frac{1}{4}$. Then $\\ln C_{A}=\\ln(\\exp(-\\tfrac{1}{4}))=-\\tfrac{1}{4}$, so\n$$\nk_{A} \\ge \\frac{\\ln(\\epsilon/|e_{0}|)}{-\\tfrac{1}{4}} = -4\\,\\ln\\!\\left(\\frac{\\epsilon}{|e_{0}|}\\right) = 4\\,\\ln\\!\\left(\\frac{|e_{0}|}{\\epsilon}\\right).\n$$\nWith $|e_{0}|=0.8$ and $\\epsilon=1.0 \\times 10^{-7}$,\n$$\n\\frac{|e_{0}|}{\\epsilon}=\\frac{0.8}{1.0 \\times 10^{-7}}=0.8 \\times 10^{7}=8 \\times 10^{6},\n$$\nhence\n$$\nk_{A} \\ge 4\\,\\ln(8 \\times 10^{6})=4\\,[\\ln 8 + 6 \\ln 10].\n$$\nNumerically, $\\ln 8 \\approx 2.079441542$ and $\\ln 10 \\approx 2.302585093$, so\n$$\nk_{A} \\ge 4\\,(2.079441542 + 6 \\times 2.302585093) \\approx 4 \\times 15.894952100 \\approx 63.579808400,\n$$\nand therefore $k_{A}=\\lceil 63.579808400 \\rceil=64$.\n\nMethod B: $C_{B}=\\frac{1}{1+\\beta^{2}}$ with $\\beta=3$, so $C_{B}=\\frac{1}{10}=10^{-1}$ and $\\ln C_{B}=-\\ln 10$. Then\n$$\nk_{B} \\ge \\frac{\\ln(\\epsilon/|e_{0}|)}{-\\ln 10} = \\frac{\\ln(|e_{0}|/\\epsilon)}{\\ln 10} = \\log_{10}\\!\\left(\\frac{|e_{0}|}{\\epsilon}\\right) = \\log_{10}(8 \\times 10^{6}) = 6 + \\log_{10} 8.\n$$\nSince $\\log_{10} 8 \\approx 0.903089987$, we get\n$$\nk_{B} \\ge 6.903089987 \\quad \\Longrightarrow \\quad k_{B}=\\lceil 6.903089987 \\rceil=7.\n$$\n\nMethod B is faster because $C_{B}<C_{A}$. The slower method (Method A) therefore requires $k_{A}-k_{B}=64-7=57$ more full iterations to reach the tolerance.", "answer": "$$\\boxed{57}$$", "id": "2180012"}]}