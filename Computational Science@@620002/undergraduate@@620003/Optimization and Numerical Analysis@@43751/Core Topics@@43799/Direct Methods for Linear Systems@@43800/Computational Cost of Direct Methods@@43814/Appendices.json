{"hands_on_practices": [{"introduction": "Understanding the computational cost of an algorithm is not just an academic exercise; it is a practical necessity for predicting performance and managing resources. This first practice demonstrates how to use the dominant scaling behavior of a direct method, often expressed in Big-O notation, to estimate how changes in problem size will affect the required computation time. By working through this scenario, you will gain an intuitive feel for the significant impact of cubic complexity, a common characteristic of solvers for dense linear systems.", "problem": "An engineering student is performing a structural analysis using a custom-built Finite Element Method (FEM) software. The core of the simulation involves solving a large, dense system of linear equations, $A\\mathbf{x} = \\mathbf{b}$, where the size of the square matrix $A$ is $N \\times N$. The software uses a direct solver, and for such methods, the computational time required is known to be proportional to the cube of the matrix dimension, $N$.\n\nFor a preliminary analysis, the student runs a coarse model with $N_1 = 8,000$ degrees of freedom. The solver takes $T_1 = 10.0$ seconds to complete on her workstation. To obtain more accurate results for her final report, she must run a refined model with $N_2 = 12,000$ degrees of freedom.\n\nAssuming the computational performance is exclusively determined by this relationship and all other factors are negligible, estimate the time the solver will take for the refined model. Express your answer in seconds, rounded to three significant figures.", "solution": "The problem states that the computational time, $T$, is proportional to the cube of the matrix dimension, $N$. We can express this relationship mathematically as:\n$$T = C N^{3}$$\nwhere $C$ is a constant of proportionality that depends on the specific hardware and software implementation.\n\nWe are given two scenarios: a coarse model and a refined model. Let's denote the parameters for the coarse model with subscript 1 and for the refined model with subscript 2.\n\nFor the coarse model, we have:\n$$T_1 = C N_1^{3}$$\n\nFor the refined model, we have:\n$$T_2 = C N_2^{3}$$\n\nWe are given $T_1 = 10.0$ s, $N_1 = 8,000$, and $N_2 = 12,000$. We need to find $T_2$. To eliminate the unknown constant $C$, we can take the ratio of the two equations:\n$$\\frac{T_2}{T_1} = \\frac{C N_2^{3}}{C N_1^{3}} = \\left(\\frac{N_2}{N_1}\\right)^{3}$$\n\nNow, we can solve for $T_2$:\n$$T_2 = T_1 \\left(\\frac{N_2}{N_1}\\right)^{3}$$\n\nLet's substitute the given values into this equation. First, we compute the ratio of the dimensions:\n$$\\frac{N_2}{N_1} = \\frac{12,000}{8,000} = \\frac{12}{8} = \\frac{3}{2} = 1.5$$\n\nNow, we substitute this ratio and the value of $T_1$ into the equation for $T_2$:\n$$T_2 = 10.0 \\times (1.5)^{3}$$\n\nWe calculate the value of $(1.5)^{3}$:\n$$(1.5)^{3} = 1.5 \\times 1.5 \\times 1.5 = 2.25 \\times 1.5 = 3.375$$\n\nFinally, we compute $T_2$:\n$$T_2 = 10.0 \\times 3.375 = 33.75$$\n\nThe problem asks for the answer to be rounded to three significant figures. The calculated value is $33.75$. The first three significant figures are 3, 3, and 7. The next digit is 5, so we round up the last significant digit.\n$$T_2 \\approx 33.8$$\nThe estimated time for the refined model is 33.8 seconds.", "answer": "$$\\boxed{33.8}$$", "id": "2160731"}, {"introduction": "While many scientific problems lead to dense matrices, a vast number exhibit special structures that can be exploited for tremendous computational savings. This exercise moves from high-level scaling to a detailed analysis of an algorithm designed for tridiagonal systems, which appear frequently in applications like solving differential equations. By meticulously counting the arithmetic operations, you will see firsthand how leveraging matrix structure can reduce the computational cost from an $O(n^3)$ dependence to a highly efficient $O(n)$ one.", "problem": "Consider the problem of solving a linear system of equations $A\\mathbf{x} = \\mathbf{d}$, where $A$ is a non-singular $n \\times n$ tridiagonal matrix and $\\mathbf{x}$ and $\\mathbf{d}$ are column vectors of size $n$. A tridiagonal matrix is one where the only non-zero elements are on the main diagonal, the first subdiagonal (the one directly below the main diagonal), and the first superdiagonal (the one directly above the main diagonal).\n\nA highly efficient algorithm for solving such a system consists of two stages:\n\n1.  **Forward Elimination:** This stage transforms the matrix $A$ into an upper bidiagonal matrix (non-zero elements only on the main diagonal and the superdiagonal). This is achieved by iterating from the second row ($i=2$) up to the last row ($i=n$). For each row $i$, the algorithm performs a row operation to eliminate the subdiagonal element $A_{i, i-1}$. This is done by calculating a multiplier based on the element $A_{i, i-1}$ and the (potentially modified) main diagonal element of the previous row, $A_{i-1, i-1}$, and then subtracting that multiple of row $i-1$ from row $i$. This operation modifies both the main diagonal element $A_{i,i}$ and the corresponding element $d_i$ in the right-hand side vector. The superdiagonal elements are not modified in this process.\n\n2.  **Backward Substitution:** The resulting upper bidiagonal system is then solved for the vector $\\mathbf{x}$. This is accomplished by first calculating the last component, $x_n$, and then iterating backwards from $i = n-1$ down to $1$ to find the remaining components of $\\mathbf{x}$.\n\nYour task is to determine the exact total number of fundamental arithmetic operations (i.e., additions, subtractions, multiplications, and divisions) required to solve the system using this specific algorithm. Assume that no divisions by zero are encountered.\n\nProvide your answer as a single closed-form analytic expression in terms of $n$.", "solution": "Let the tridiagonal matrix have subdiagonal entries $\\{a_{i}\\}_{i=2}^{n}$, main diagonal entries $\\{b_{i}\\}_{i=1}^{n}$, and superdiagonal entries $\\{c_{i}\\}_{i=1}^{n-1}$, and let the right-hand side be $\\{d_{i}\\}_{i=1}^{n}$.\n\nForward elimination proceeds for $i=2,3,\\dots,n$ as follows. First compute the multiplier\n$$\nm_{i}=\\frac{a_{i}}{b_{i-1}},\n$$\nwhich is one division. Then update the main diagonal entry and right-hand side:\n$$\nb_{i}\\leftarrow b_{i}-m_{i}c_{i-1},\\qquad d_{i}\\leftarrow d_{i}-m_{i}d_{i-1}.\n$$\nEach of these is one multiplication followed by one subtraction. The superdiagonal entries $c_{i}$ are not modified. Therefore, for each $i$ there are $1$ division, $2$ multiplications, and $2$ subtractions, totaling $5$ operations. Since this loop runs for $i=2$ to $n$, the forward elimination cost is\n$$\n5(n-1).\n$$\n\nBackward substitution solves the resulting upper bidiagonal system. First compute\n$$\nx_{n}=\\frac{d_{n}}{b_{n}},\n$$\nwhich is one division. Then for $i=n-1,n-2,\\dots,1$ compute\n$$\nx_{i}=\\frac{d_{i}-c_{i}x_{i+1}}{b_{i}},\n$$\nwhich requires one multiplication ($c_{i}x_{i+1}$), one subtraction ($d_{i}-c_{i}x_{i+1}$), and one division by $b_{i}$, i.e., $3$ operations per index. This backward pass therefore costs\n$$\n1+3(n-1)=3n-2\n$$\noperations.\n\nAdding the two stages yields the exact total number of fundamental arithmetic operations:\n$$\n5(n-1)+(3n-2)=8n-7.\n$$\n\nNo additions occur explicitly in this algorithm; subtractions, multiplications, and divisions are counted individually as above. The count is valid for all $n\\geq 1$ and matches edge cases (e.g., for $n=1$, only one division is performed).", "answer": "$$\\boxed{8n-7}$$", "id": "2160762"}, {"introduction": "This final practice synthesizes your skills by analyzing the cost of a complete, multi-step algorithm used in a real-world application. You will deconstruct the solution process for ridge regression, a common technique in machine learning, into its fundamental linear algebra operationsâ€”matrix products, Cholesky factorization, and triangular solves. By building a complete cost model in terms of the problem's dimensions, $m$ and $n$, you will learn how cost analysis is used to understand and compare practical computational strategies in data science and engineering.", "problem": "In the field of machine learning, ridge regression is a common technique used to address overfitting in linear models. It is mathematically equivalent to solving a Tikhonov-regularized least squares problem. For a given overdetermined system of linear equations $Ax=b$, where $A$ is a dense $m \\times n$ matrix of input features (with $m$ samples and $n$ features, where $m > n$), and $b$ is an $m \\times 1$ vector of observations, the regularized solution for the parameter vector $x$ can be found by solving the normal equations:\n$$\n(A^T A + \\lambda I)x = A^T b\n$$\nHere, $\\lambda$ is a positive scalar regularization parameter and $I$ is the $n \\times n$ identity matrix.\n\nConsider a direct method for solving this system that proceeds in the following sequence:\n1.  Compute the matrix $M = A^T A$.\n2.  Compute the vector $c = A^T b$.\n3.  Form the matrix $B = M + \\lambda I$.\n4.  Solve the linear system $Bx = c$ by first computing the Cholesky factorization of $B$ (i.e., $B = L L^T$), and then solving for $x$ via one forward and one backward substitution.\n\nYour task is to determine the total computational cost of this entire procedure. For your calculation, you are given the following standard costs for basic operations, where a \"flop\" is defined as a single floating-point addition, subtraction, multiplication, or division.\n\n-   The cost of multiplying a $p \\times q$ matrix by a $q \\times r$ matrix is $2pqr$ flops. However, if the resulting $p \\times r$ matrix is known to be symmetric and $p=r$, the cost can be reduced to $pqr$ flops by computing only the unique elements.\n-   The cost of computing the Cholesky factorization of a symmetric positive-definite $k \\times k$ matrix is approximately $\\frac{1}{3}k^3$ flops.\n-   The cost of solving a $k \\times k$ triangular system (i.e., forward or backward substitution) is approximately $k^2$ flops.\n-   You may neglect the computational cost of matrix addition, such as forming $B = M + \\lambda I$.\n\nExpress the total number of flops as a polynomial expression in terms of $m$ and $n$.", "solution": "We compute the total flop count by summing the costs of each step, using the provided cost model and exploiting symmetry where applicable.\n\nFirst, compute $M = A^{T}A$. Here, $A^{T}$ is $n \\times m$ and $A$ is $m \\times n$, so the naive matrix multiplication cost would be $2 n m n = 2 m n^{2}$ flops. However, since $A^{T}A$ is symmetric and the result is $n \\times n$, we can reduce the cost to $n m n = m n^{2}$ flops by computing only the unique elements.\n\nSecond, compute $c = A^{T} b$. This is an $n \\times m$ matrix times an $m \\times 1$ vector, costing $2 n m = 2 m n$ flops.\n\nThird, form $B = M + \\lambda I$. The problem statement allows us to neglect the cost of this matrix addition, so we count $0$ flops for this step.\n\nFourth, perform the Cholesky factorization $B = L L^{T}$, where $B$ is $n \\times n$ symmetric positive definite. The cost is approximately $\\frac{1}{3} n^{3}$ flops.\n\nFifth, solve the two triangular systems $L y = c$ (forward substitution) and $L^{T} x = y$ (backward substitution). Each triangular solve costs approximately $n^{2}$ flops, for a total of $2 n^{2}$ flops.\n\nSumming all contributions gives the total computational cost:\n$$\nm n^{2} + 2 m n + \\frac{1}{3} n^{3} + 2 n^{2}.\n$$\nThis is a polynomial in $m$ and $n$ as required.", "answer": "$$\\boxed{\\frac{1}{3}n^{3}+mn^{2}+2n^{2}+2mn}$$", "id": "2160711"}]}