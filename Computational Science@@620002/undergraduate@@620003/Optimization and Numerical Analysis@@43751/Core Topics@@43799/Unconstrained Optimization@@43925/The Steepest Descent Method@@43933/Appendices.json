{"hands_on_practices": [{"introduction": "The steepest descent method is an iterative process, and every iteration begins with a crucial decision: in which direction should we move to achieve the fastest decrease in the function's value? This exercise focuses on this fundamental first step. By calculating the negative gradient for the renowned Rosenbrock function, a standard benchmark for optimization algorithms, you will determine the optimal initial search direction from a given starting point. [@problem_id:2221567]", "problem": "In the field of numerical optimization, the performance of new algorithms is often benchmarked using a set of standard test functions. One such function is the Rosenbrock function, which is challenging to minimize due to its narrow, parabolic valley.\n\nConsider a two-dimensional version of the Rosenbrock function given by\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\nwhere $a$ and $b$ are positive real constants.\n\nAn iterative minimization algorithm is initiated at the point $(x_0, y_0) = (0, 0)$. The first step of this algorithm involves determining the initial search direction. This direction is defined as the vector along which the function's value decreases most rapidly from the starting point.\n\nDetermine this initial search direction vector, $\\mathbf{d}_0$. Express your answer as a column vector in terms of the constants $a$ and $b$.", "solution": "The direction along which $f$ decreases most rapidly at a point is the negative gradient, so the initial search direction from $(x_{0},y_{0})=(0,0)$ is $\\mathbf{d}_{0}=-\\nabla f(0,0)$.\n\nCompute the gradient of $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$:\n- The partial derivative with respect to $x$ is\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- The partial derivative with respect to $y$ is\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\nEvaluate these at $(0,0)$:\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\nHence,\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\\[4pt]0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\\[4pt]0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$", "id": "2221567"}, {"introduction": "Once a direction of descent is established, the next question is \"how far should we travel along it?\". This practice isolates this critical component of the algorithm by guiding you through an exact line search. You will calculate the optimal step size, denoted by $\\alpha$, which minimizes the objective function along the chosen steepest descent path, thereby ensuring the maximum possible improvement for that iteration. [@problem_id:2170908]", "problem": "In the field of numerical optimization, the steepest descent method is a fundamental algorithm for finding a local minimum of a function. A crucial part of this method is the line search, which determines how far to move along the search direction in each iteration.\n\nConsider the task of minimizing the quadratic function $f(x_1, x_2) = 2x_1^2 + x_2^2 + x_1 x_2$. The first step of the steepest descent algorithm starts at an initial point $\\mathbf{x}_0 = (3, 0)$ and moves along the direction of the negative gradient evaluated at that point, which we denote as $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$.\n\nAn exact line search is performed to find the optimal step size $\\alpha > 0$ that minimizes the function along this direction. This involves finding the value of $\\alpha$ that minimizes the one-dimensional function $\\phi(\\alpha) = f(\\mathbf{x}_0 + \\alpha \\mathbf{p}_0)$.\n\nYour task is to determine this optimal step size $\\alpha$. Express your answer as an exact fraction in its simplest form.", "solution": "We minimize $f(x_{1},x_{2})=2x_{1}^{2}+x_{2}^{2}+x_{1}x_{2}$ using steepest descent with exact line search starting at $\\mathbf{x}_{0}=(3,0)$. The gradient is\n$$\n\\nabla f(x_{1},x_{2})=\\begin{pmatrix}4x_{1}+x_{2}\\\\ x_{1}+2x_{2}\\end{pmatrix}.\n$$\nEvaluated at $\\mathbf{x}_{0}$, this gives\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=\\begin{pmatrix}4\\cdot 3+0\\\\ 3+2\\cdot 0\\end{pmatrix}=\\begin{pmatrix}12\\\\ 3\\end{pmatrix},\n$$\nso the steepest descent direction is $\\mathbf{p}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}-12\\\\ -3\\end{pmatrix}$.\n\nDefine the line $x(\\alpha)=\\mathbf{x}_{0}+\\alpha \\mathbf{p}_{0}=(3-12\\alpha,-3\\alpha)$ and the one-dimensional function $\\phi(\\alpha)=f(x(\\alpha))$. Then\n$$\n\\phi(\\alpha)=2(3-12\\alpha)^{2}+(-3\\alpha)^{2}+(3-12\\alpha)(-3\\alpha).\n$$\nExpand each term:\n$$\n(3-12\\alpha)^{2}=9-72\\alpha+144\\alpha^{2},\\quad 2(3-12\\alpha)^{2}=18-144\\alpha+288\\alpha^{2},\n$$\n$$\n(-3\\alpha)^{2}=9\\alpha^{2},\\quad (3-12\\alpha)(-3\\alpha)=-9\\alpha+36\\alpha^{2}.\n$$\nSumming,\n$$\n\\phi(\\alpha)=18-144\\alpha+288\\alpha^{2}+9\\alpha^{2}-9\\alpha+36\\alpha^{2}\n=18-153\\alpha+333\\alpha^{2}.\n$$\nFor exact line search, set the derivative to zero:\n$$\n\\phi'(\\alpha)=-153+666\\alpha=0 \\quad \\Rightarrow \\quad \\alpha=\\frac{153}{666}.\n$$\nSimplify the fraction:\n$$\n\\frac{153}{666}=\\frac{51}{222}=\\frac{17}{74}.\n$$\nSince $\\phi''(\\alpha)=666>0$, this $\\alpha$ indeed minimizes $\\phi$.", "answer": "$$\\boxed{\\frac{17}{74}}$$", "id": "2170908"}, {"introduction": "Now it's time to combine the core components and observe the algorithm's behavior over multiple steps. This problem requires you to perform two complete iterations of the steepest descent method, applying your skills in both finding the descent direction and computing the corresponding optimal step size via exact line search. Working with an elliptical quadratic function like this one provides valuable, first-hand insight into the method's characteristic \"zigzagging\" convergence pattern, a key phenomenon in understanding its practical performance. [@problem_id:2221576]", "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x, y) = 10x^2 + y^2$. The optimization process is initiated at the point $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$.\n\nThe steepest descent algorithm is to be used. For each iteration $k$, the step size, denoted by $\\alpha_k > 0$, is determined via an exact line search. This means that for a given point $\\mathbf{x}_k$ and descent direction $\\mathbf{p}_k$, the step size $\\alpha_k$ is chosen to find the global minimum of the single-variable function $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$.\n\nCalculate the coordinates of the point $\\mathbf{x}_2 = (x_2, y_2)$ after two complete iterations of this method. The coordinates in your final answer should be expressed as exact fractions in their simplest form.", "solution": "We minimize $f(x,y)=10x^{2}+y^{2}$ starting at $\\mathbf{x}_{0}=(1,1)$ using steepest descent with exact line search. The gradient and Hessian are\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}.\n$$\nAt iteration $k$, with $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$ and direction $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$, the exact line search minimizes $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$. Since $f$ is a quadratic function, the optimal step size has a closed-form solution. The derivative of $g(\\alpha)$ is\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\nSetting $g'(\\alpha)=0$ yields the exact step size:\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\n**Iteration 0:** Start at $\\mathbf{x}_{0}=(1,1)$. The gradient is:\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix}\n$$\nWe compute the terms for $\\alpha_0$:\n$$\n\\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0} = 20^2 + 2^2 = 404\n$$\n$$\n\\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0} = \\begin{pmatrix}20 & 2\\end{pmatrix}\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}\\begin{pmatrix}20\\\\2\\end{pmatrix} = \\begin{pmatrix}20 & 2\\end{pmatrix}\\begin{pmatrix}400\\\\4\\end{pmatrix} = 20(400) + 2(4) = 8008\n$$\nThe step size is:\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\nThe new point $\\mathbf{x}_1$ is:\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1\\\\1\\end{pmatrix}-\\frac{101}{2002}\\begin{pmatrix}20\\\\2\\end{pmatrix}=\\begin{pmatrix}1-\\frac{2020}{2002}\\\\1-\\frac{202}{2002}\\end{pmatrix}=\\begin{pmatrix}-\\frac{18}{2002}\\\\\\frac{1800}{2002}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\n**Iteration 1:** Start at $\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$. The gradient is:\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\nWe compute the terms for $\\alpha_1$:\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1} = \\left(-\\frac{180}{1001}\\right)^2 + \\left(\\frac{1800}{1001}\\right)^2 = \\frac{180^2(1^2 + 10^2)}{1001^2} = \\frac{180^2 \\cdot 101}{1001^2}\n$$\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1} = 20\\left(-\\frac{180}{1001}\\right)^2 + 2\\left(\\frac{1800}{1001}\\right)^2 = \\frac{180^2(20 \\cdot 1^2 + 2 \\cdot 10^2)}{1001^2} = \\frac{180^2 \\cdot 220}{1001^2}\n$$\nThe step size is:\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}} = \\frac{180^2 \\cdot 101 / 1001^2}{180^2 \\cdot 220 / 1001^2} = \\frac{101}{220}.\n$$\nThe new point $\\mathbf{x}_2$ is:\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}\\\\[4pt]\\frac{900}{1001}\\end{pmatrix}-\\frac{101}{220}\\begin{pmatrix}-\\frac{180}{1001}\\\\[4pt]\\frac{1800}{1001}\\end{pmatrix} = \\begin{pmatrix}-\\frac{9}{1001} + \\frac{101 \\cdot 180}{220 \\cdot 1001}\\\\[4pt]\\frac{900}{1001} - \\frac{101 \\cdot 1800}{220 \\cdot 1001}\\end{pmatrix} = \\begin{pmatrix}-\\frac{9}{1001} + \\frac{909}{11011}\\\\[4pt]\\frac{900}{1001} - \\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\\[4pt]\\frac{810}{11011}\\end{pmatrix}.\n$$\nThe fractions are already in simplest terms because $\\gcd(810,11011)=1$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} \\\\ \\frac{810}{11011} \\end{pmatrix}}$$", "id": "2221576"}]}