{"hands_on_practices": [{"introduction": "To understand the behavior of any algorithm, it's often best to begin with the simplest possible scenario. This first exercise explores the performance of the steepest descent method on a one-dimensional quadratic function. You will demonstrate that when combined with an exact line search, the algorithm can find the precise minimum in just a single iteration, providing a crucial baseline for its ideal performance [@problem_id:2162624].", "problem": "An optimization specialist is using the steepest descent algorithm to find the minimum of a one-dimensional objective function. The function is given by:\n$f(x) = 3x^2 - 7x + 11$\n\nThe iterative process of the steepest descent algorithm is defined by the update rule:\n$x_{k+1} = x_k - \\alpha_k f'(x_k)$\nwhere $x_k$ is the current position at iteration $k$, $f'(x_k)$ is the derivative of the function, and $\\alpha_k$ is the step size.\n\nThe specialist uses an 'exact line search' to determine the optimal step size $\\alpha_k$ at each iteration. This method involves choosing $\\alpha_k$ such that it minimizes the function $f$ along the search direction starting from $x_k$. In other words, for a given $x_k$, the value $\\alpha_k$ is chosen to minimize the new function of the step size, $g(\\alpha) = f(x_k - \\alpha f'(x_k))$.\n\nStarting from the initial point $x_0 = 5$, calculate the position $x_1$ after a single iteration of the steepest descent algorithm with an exact line search. Express your answer as a fraction.", "solution": "The problem asks for the position $x_1$ after one iteration of the steepest descent algorithm, starting from $x_0 = 5$. The function to be minimized is $f(x) = 3x^2 - 7x + 11$.\n\nThe update rule for the steepest descent algorithm is given by:\n$x_{k+1} = x_k - \\alpha_k f'(x_k)$\n\nFor the first iteration (from $k=0$ to $k=1$), the rule is:\n$x_1 = x_0 - \\alpha_0 f'(x_0)$\n\nFirst, we need to compute the derivative of the function $f(x)$:\n$f'(x) = \\frac{d}{dx}(3x^2 - 7x + 11) = 6x - 7$\n\nNext, we evaluate this derivative at the starting point $x_0 = 5$:\n$f'(x_0) = f'(5) = 6(5) - 7 = 30 - 7 = 23$\n\nNow we have the update rule in terms of the unknown step size $\\alpha_0$:\n$x_1 = 5 - \\alpha_0 (23)$\n\nThe problem states that an exact line search is used to find $\\alpha_0$. This means we must choose $\\alpha_0$ to minimize the function $g(\\alpha) = f(x_0 - \\alpha f'(x_0))$. Substituting the known values:\n$g(\\alpha) = f(5 - 23\\alpha)$\n\nTo find the value of $\\alpha$ that minimizes $g(\\alpha)$, we take the derivative of $g(\\alpha)$ with respect to $\\alpha$ and set it to zero. We use the chain rule for differentiation:\n$g'(\\alpha) = \\frac{d}{d\\alpha} f(5 - 23\\alpha) = f'(5 - 23\\alpha) \\cdot \\frac{d}{d\\alpha}(5 - 23\\alpha)$\n$g'(\\alpha) = f'(5 - 23\\alpha) \\cdot (-23)$\n\nSetting $g'(\\alpha_0) = 0$ to find the optimal step size $\\alpha_0$:\n$f'(5 - 23\\alpha_0) \\cdot (-23) = 0$\n\nSince $-23 \\neq 0$, this condition simplifies to:\n$f'(5 - 23\\alpha_0) = 0$\n\nThe expression inside the derivative, $5 - 23\\alpha_0$, is precisely the formula for our target point $x_1$. Thus, the condition for the optimal step size implies:\n$f'(x_1) = 0$\n\nThis means that the point $x_1$ found after one iteration with an exact line search is the point where the derivative of the original function $f(x)$ is zero. For a convex quadratic function, this point is the global minimum.\n\nWe can now solve for $x_1$ using the expression for the derivative $f'(x) = 6x - 7$:\n$f'(x_1) = 6x_1 - 7 = 0$\n$6x_1 = 7$\n$x_1 = \\frac{7}{6}$\n\nThus, a single iteration of the steepest descent method with an exact line search, starting from $x_0=5$, leads directly to the minimum of the function at $x_1 = 7/6$.", "answer": "$$\\boxed{\\frac{7}{6}}$$", "id": "2162624"}, {"introduction": "While steepest descent performs perfectly on 1D quadratics, its behavior changes dramatically in higher dimensions, especially for ill-conditioned functions whose level sets are elongated ellipses. This practice problem illuminates the method's most famous characteristic: its slow, zigzagging path toward the minimum within a narrow valley [@problem_id:2162600]. By analyzing the geometry of the iterates, you will gain a visual and mathematical understanding of why convergence can be frustratingly inefficient.", "problem": "Consider the optimization problem of minimizing the quadratic function $f(x, y) = \\frac{1}{2}(x^2 + \\gamma y^2)$, where $\\gamma$ is a positive real constant. We use the steepest descent algorithm to find the minimum of this function. The algorithm generates a sequence of points $\\{\\mathbf{x}_k\\}$, starting from an initial point $\\mathbf{x}_0$. The update rule is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k$, where $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ is the gradient of $f$ at $\\mathbf{x}_k$. The step size $\\alpha_k$ is chosen at each step to minimize $f(\\mathbf{x}_{k+1})$; this method is known as an exact line search.\n\nFor a quadratic function of the form $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x}$, where $Q$ is a positive definite matrix, the optimal step size $\\alpha_k$ at iteration $k$ is given by the formula:\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^T \\mathbf{g}_k}{\\mathbf{g}_k^T Q \\mathbf{g}_k}\n$$\nSuppose we set $\\gamma = 49$ and start the algorithm from the point $\\mathbf{x}_0 = (49, 1)^T$. It can be shown that for this specific setup, the iterates with even indices ($ \\mathbf{x}_0, \\mathbf{x}_2, \\mathbf{x}_4, \\dots $) lie on one straight line passing through the origin, and the iterates with odd indices ($ \\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_5, \\dots $) lie on another straight line also passing through the origin. This sequence of points forms a 'zigzag' path converging to the minimum at $(0, 0)$.\n\nDetermine the cosine of the acute angle $\\theta$ between these two lines.", "solution": "We have $f(x,y)=\\frac{1}{2}(x^{2}+\\gamma y^{2})$ with $Q=\\mathrm{diag}(1,\\gamma)$, so for $\\mathbf{x}_{k}=(x_{k},y_{k})^{T}$ the gradient is $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})=(x_{k},\\gamma y_{k})^{T}$. The exact line-search step size is\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{T}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{T}Q\\mathbf{g}_{k}}=\\frac{x_{k}^{2}+\\gamma^{2}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}.\n$$\nThe steepest descent update $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha_{k}\\mathbf{g}_{k}$ gives componentwise\n$$\nx_{k+1}=(1-\\alpha_{k})x_{k},\\qquad y_{k+1}=(1-\\alpha_{k}\\gamma)y_{k}.\n$$\nDefine the slope $m_{k}=y_{k}/x_{k}$ (line through the origin). Then\n$$\nm_{k+1}=\\frac{y_{k+1}}{x_{k+1}}=\\frac{1-\\alpha_{k}\\gamma}{1-\\alpha_{k}}\\,m_{k}.\n$$\nUsing the expression for $\\alpha_{k}$,\n$$\n1-\\alpha_{k}=\\frac{\\gamma^{3}y_{k}^{2}-\\gamma^{2}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}=\\frac{\\gamma^{2}(\\gamma-1)y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}},\n$$\n$$\n1-\\alpha_{k}\\gamma=\\frac{x_{k}^{2}+\\gamma^{3}y_{k}^{2}-\\gamma x_{k}^{2}-\\gamma^{3}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}=\\frac{(1-\\gamma)x_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}.\n$$\nTherefore\n$$\n\\frac{1-\\alpha_{k}\\gamma}{1-\\alpha_{k}}=\\frac{(1-\\gamma)x_{k}^{2}}{\\gamma^{2}(\\gamma-1)y_{k}^{2}}=-\\frac{x_{k}^{2}}{\\gamma^{2}y_{k}^{2}},\n$$\nand hence\n$$\nm_{k+1}=-\\frac{x_{k}^{2}}{\\gamma^{2}y_{k}^{2}}\\,m_{k}=-\\frac{1}{\\gamma^{2}m_{k}}.\n$$\nIt follows that $m_{k+2}=-\\frac{1}{\\gamma^{2}m_{k+1}}=-\\frac{1}{\\gamma^{2}\\left(-\\frac{1}{\\gamma^{2}m_{k}}\\right)}=m_{k}$, so all even slopes are equal and all odd slopes are equal, confirming the two lines.\n\nWith $\\gamma=49$ and $\\mathbf{x}_{0}=(49,1)^{T}$, we have $m_{0}=y_{0}/x_{0}=1/49=1/\\gamma$, so\n$$\nm_{1}=-\\frac{1}{\\gamma^{2}m_{0}}=-\\frac{1}{\\gamma^{2}\\cdot(1/\\gamma)}=-\\frac{1}{\\gamma}=-\\frac{1}{49}.\n$$\nThus the two lines have slopes $m$ and $-m$ with $m=1/49$. The cosine of the acute angle $\\theta$ between lines with slopes $m$ and $-m$ equals\n$$\n\\cos\\theta=\\frac{(1,m)\\cdot(1,-m)}{\\|(1,m)\\|\\,\\|(1,-m)\\|}=\\frac{1-m^{2}}{1+m^{2}}.\n$$\nSubstituting $m=1/49$,\n$$\n\\cos\\theta=\\frac{1-\\frac{1}{49^{2}}}{1+\\frac{1}{49^{2}}}=\\frac{49^{2}-1}{49^{2}+1}=\\frac{2400}{2402}=\\frac{1200}{1201}.\n$$", "answer": "$$\\boxed{\\frac{1200}{1201}}$$", "id": "2162600"}, {"introduction": "The slow convergence demonstrated in the previous practice leads to significant practical challenges, particularly in deciding when to stop the algorithm. This final exercise serves as a critical warning: a small gradient norm, a commonly used termination criterion, does not guarantee that you are close to the true minimum. You will quantify the maximum possible error for a point with a small gradient, revealing how ill-conditioning can create an illusion of convergence [@problem_id:2162662].", "problem": "An optimization algorithm is employed to find the minimum of the convex quadratic function $f(x_1, x_2) = \\frac{1}{2} (x_1^2 + \\lambda x_2^2)$, where $\\lambda$ is a small positive constant representing the conditioning of the problem. The unique global minimum of this function is located at $x^* = (0, 0)$. The algorithm uses a standard termination criterion: it stops iterating when the Euclidean norm of the gradient, $\\|\\nabla f(x_k)\\|$, falls below a predefined tolerance $\\epsilon$.\n\nIn a specific application, the function parameter is $\\lambda = 10^{-8}$ and the termination tolerance is set to $\\epsilon = 10^{-5}$. The algorithm stops at a point $x_{term} = (x_1, x_2)$ where the condition is just met, meaning $\\|\\nabla f(x_{term})\\| = \\epsilon$. This scenario corresponds to the boundary of the termination region.\n\nThis small gradient norm does not guarantee that the point $x_{term}$ is close to the true minimum $x^*$. Your task is to quantify this worst-case error. Calculate the maximum possible Euclidean distance, $\\|x_{term} - x^*\\|$, for a point that satisfies the termination condition $\\|\\nabla f(x_{term})\\| = \\epsilon$.", "solution": "We are given the convex quadratic function $f(x_{1}, x_{2}) = \\frac{1}{2}\\left(x_{1}^{2} + \\lambda x_{2}^{2}\\right)$ with unique minimizer at $x^{*} = (0, 0)$. Its gradient is\n$$\n\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} x_{1} \\\\ \\lambda x_{2} \\end{pmatrix}.\n$$\nThe termination condition is $\\|\\nabla f(x_{term})\\| = \\epsilon$, which explicitly imposes the constraint\n$$\nx_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}.\n$$\nWe are asked to compute the maximum possible Euclidean distance to the minimizer,\n$$\n\\|x_{term} - x^{*}\\| = \\sqrt{x_{1}^{2} + x_{2}^{2}},\n$$\nover all $(x_{1}, x_{2})$ satisfying the constraint above. This is the constrained optimization problem\n$$\n\\max_{x_{1}, x_{2}} \\; x_{1}^{2} + x_{2}^{2} \\quad \\text{subject to} \\quad x_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}.\n$$\nUsing the method of Lagrange multipliers, define\n$$\n\\mathcal{L}(x_{1}, x_{2}, \\mu) = x_{1}^{2} + x_{2}^{2} - \\mu \\left(x_{1}^{2} + \\lambda^{2} x_{2}^{2} - \\epsilon^{2}\\right).\n$$\nStationarity conditions are\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{1}} = 2 x_{1} - 2 \\mu x_{1} = 2 x_{1} (1 - \\mu) = 0,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{2}} = 2 x_{2} - 2 \\mu \\lambda^{2} x_{2} = 2 x_{2} (1 - \\mu \\lambda^{2}) = 0,\n$$\ntogether with the constraint $x_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}$.\n\nFrom the stationarity equations, either $x_{1} = 0$ or $\\mu = 1$, and either $x_{2} = 0$ or $\\mu = \\frac{1}{\\lambda^{2}}$. Since $\\lambda \\neq 1$, we cannot have both $\\mu = 1$ and $\\mu = \\frac{1}{\\lambda^{2}}$ simultaneously, so the extrema occur when one coordinate is zero.\n\n- If $x_{2} = 0$, then the constraint gives $x_{1}^{2} = \\epsilon^{2}$ and the objective value is $x_{1}^{2} + x_{2}^{2} = \\epsilon^{2}$, hence the distance is $\\epsilon$.\n- If $x_{1} = 0$, then the constraint gives $\\lambda^{2} x_{2}^{2} = \\epsilon^{2}$, so $x_{2}^{2} = \\frac{\\epsilon^{2}}{\\lambda^{2}}$ and the objective value is $x_{1}^{2} + x_{2}^{2} = \\frac{\\epsilon^{2}}{\\lambda^{2}}$, hence the distance is $\\frac{\\epsilon}{\\lambda}$.\n\nBetween these two candidates, the maximum occurs at $x_{1} = 0$ because $\\lambda \\in (0, 1)$, yielding the worst-case distance\n$$\n\\|x_{term} - x^{*}\\|_{\\max} = \\frac{\\epsilon}{\\lambda}.\n$$\nSubstituting the given values $\\lambda = 10^{-8}$ and $\\epsilon = 10^{-5}$,\n$$\n\\|x_{term} - x^{*}\\|_{\\max} = \\frac{10^{-5}}{10^{-8}} = 10^{3} = 1 \\times 10^{3}.\n$$", "answer": "$$\\boxed{1 \\times 10^{3}}$$", "id": "2162662"}]}