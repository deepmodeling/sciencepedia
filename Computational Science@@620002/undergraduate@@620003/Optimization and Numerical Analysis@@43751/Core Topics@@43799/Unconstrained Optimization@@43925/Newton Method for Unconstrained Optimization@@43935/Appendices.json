{"hands_on_practices": [{"introduction": "To truly master Newton's method, it is essential to work through a complete iteration by hand. This first practice problem provides a concrete objective function, similar to those found in engineering design, and asks you to perform a full Newton step. By calculating the gradient and Hessian, solving the resulting linear system, and updating your position, you will execute the entire mechanical workflow of the algorithm [@problem_id:2190727]. This exercise reinforces the interplay between multivariable calculus and linear algebra that is at the heart of the method.", "problem": "A team of engineers is working to minimize the operational cost of a new robotic arm. The cost is modeled by a function $f(x, y)$ of two dimensionless design parameters $x$ and $y$. The function is given by:\n$$f(x, y) = (y - x^2)^2 + (1-x)^2$$\nTo find the optimal parameters that minimize this cost, the team decides to use Newton's method for unconstrained optimization. They start from an initial design guess of $(x_0, y_0) = (2, 3)$.\n\nCalculate the coordinates of the next iterate, $(x_1, y_1)$, that results from applying one step of Newton's method. Express your answer as an ordered pair of exact fractions.", "solution": "For unconstrained optimization in two variables, one step of Newton's method updates the iterate $\\mathbf{z} = (x, y)$ by\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_{k} - H(\\mathbf{z}_{k})^{-1} \\nabla f(\\mathbf{z}_{k}),\n$$\nwhere $\\nabla f$ is the gradient and $H$ is the Hessian of $f$.\n\nGiven $f(x, y) = (y - x^{2})^{2} + (1 - x)^{2}$, compute the gradient:\n$$\n\\frac{\\partial f}{\\partial x} = 2(y - x^{2})(-2x) + 2(1 - x)(-1) = -4xy + 4x^{3} - 2 + 2x,\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(y - x^{2}) = 2y - 2x^{2}.\n$$\nThus,\n$$\n\\nabla f(x, y) = \\begin{pmatrix} -4xy + 4x^{3} - 2 + 2x \\\\ 2y - 2x^{2} \\end{pmatrix}.\n$$\n\nCompute the Hessian:\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4y + 12x^{2} + 2,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4x,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}\\left(2y - 2x^{2}\\right) = 2.\n$$\nTherefore,\n$$\nH(x, y) = \\begin{pmatrix} 12x^{2} - 4y + 2 & -4x \\\\ -4x & 2 \\end{pmatrix}.\n$$\n\nEvaluate at $(x_{0}, y_{0}) = (2, 3)$:\n$$\n\\nabla f(2, 3) = \\begin{pmatrix} -4\\cdot 2 \\cdot 3 + 4\\cdot 2^{3} - 2 + 2\\cdot 2 \\\\ 2\\cdot 3 - 2\\cdot 2^{2} \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -2 \\end{pmatrix},\n$$\n$$\nH(2, 3) = \\begin{pmatrix} 12\\cdot 2^{2} - 4\\cdot 3 + 2 & -4\\cdot 2 \\\\ -4\\cdot 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 38 & -8 \\\\ -8 & 2 \\end{pmatrix}.\n$$\n\nThe Newton step $\\mathbf{s}$ solves $H(2, 3)\\,\\mathbf{s} = -\\nabla f(2, 3)$:\n$$\n\\begin{pmatrix} 38 & -8 \\\\ -8 & 2 \\end{pmatrix} \\begin{pmatrix} s_{1} \\\\ s_{2} \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 2 \\end{pmatrix}.\n$$\nFrom the system\n$$\n38 s_{1} - 8 s_{2} = -10, \\quad -8 s_{1} + 2 s_{2} = 2,\n$$\nmultiply the second equation by $4$ to get $-32 s_{1} + 8 s_{2} = 8$ and add to the first to obtain $6 s_{1} = -2$, hence $s_{1} = -\\frac{1}{3}$. Substituting into $-8 s_{1} + 2 s_{2} = 2$ gives $8/3 + 2 s_{2} = 2$, so $2 s_{2} = -2/3$ and $s_{2} = -\\frac{1}{3}$.\n\nThus,\n$$\n(x_{1}, y_{1}) = (x_{0}, y_{0}) + \\mathbf{s} = \\left(2 - \\frac{1}{3},\\, 3 - \\frac{1}{3}\\right) = \\left(\\frac{5}{3},\\, \\frac{8}{3}\\right).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{3} \\\\ \\frac{8}{3} \\end{pmatrix}}$$", "id": "2190727"}, {"introduction": "Newton's method relies on inverting the Hessian matrix, $H$, to find the search direction. But what happens if this matrix is not invertible, or in the one-dimensional case, when the second derivative is zero? This exercise explores this critical failure scenario by tasking you with finding an an initial point that causes the algorithm to fail on the very first step [@problem_id:2190705]. Understanding this limitation is crucial, as it reveals the mathematical conditions under which the quadratic model, and thus the entire method, breaks down.", "problem": "A classic algorithm for unconstrained optimization is Newton's method. Consider the problem of minimizing the one-dimensional objective function $f(x) = (x^2 - A)^2$, where $A$ is a given positive real constant. The iterative scheme of Newton's method starts from an initial guess $x_0$ and generates a sequence of points intended to converge to a local minimum. However, for certain choices of $x_0$, the algorithm can fail at the very first step because the update cannot be computed. Determine the positive value of the initial guess, $x_0$, for which this failure occurs. Your answer should be a symbolic expression in terms of $A$.", "solution": "Newton's method for unconstrained minimization in one dimension updates according to\n$$\nx_{k+1} = x_{k} - \\frac{f'(x_{k})}{f''(x_{k})}.\n$$\nThe update cannot be computed at the first step if and only if the denominator is zero, i.e., when $f''(x_{0}) = 0$.\n\nGiven $f(x) = (x^{2} - A)^{2}$ with $A > 0$, compute the first and second derivatives:\n$$\nf'(x) = 2(x^{2} - A)\\cdot 2x = 4x(x^{2} - A),\n$$\n$$\nf''(x) = 4(x^{2} - A) + 8x^{2} = 12x^{2} - 4A = 4(3x^{2} - A).\n$$\nSet $f''(x_{0}) = 0$ to find the problematic initial guesses:\n$$\n4(3x_{0}^{2} - A) = 0 \\;\\Longrightarrow\\; 3x_{0}^{2} - A = 0 \\;\\Longrightarrow\\; x_{0}^{2} = \\frac{A}{3}.\n$$\nThus the positive initial guess that makes the Newton step undefined is\n$$\nx_{0} = \\sqrt{\\frac{A}{3}}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{A}{3}}}$$", "id": "2190705"}, {"introduction": "An effective optimization algorithm should, at each step, move closer to a solution. One might intuitively assume that Newton's method always moves \"downhill\" toward a minimum. This final practice problem challenges that assumption with a carefully constructed hypothetical scenario [@problem_id:2190682]. Here, you will find that even when a Newton step is perfectly well-defined, it can result in an increase in the function's value. This counterintuitive result is vital for appreciating why practical implementations of Newton's method are rarely \"pure\" and almost always include safeguards like line searches or trust regions.", "problem": "In an engineering system, a cost function is used to quantify the performance based on two adjustable parameters, $p$ and $q$. The goal is to find the pair of parameters $(p, q)$ that minimizes this cost. The cost function is given by:\n$$C(p, q) = (pq - 2)^2$$\nAn optimization algorithm is initialized at the parameter setting $(p_0, q_0) = (1, 1)$. To find the optimal parameters, one iteration of Newton's method for unconstrained optimization is performed. Calculate the new parameter setting $(p_1, q_1)$ after this single step.", "solution": "We want one Newton step for minimizing $C(p,q)=(pq-2)^{2}$ starting at $(p_{0},q_{0})=(1,1)$. Newton's update for unconstrained optimization is\n$$\n\\begin{pmatrix} p_{1} \\\\ q_{1} \\end{pmatrix}\n=\n\\begin{pmatrix} p_{0} \\\\ q_{0} \\end{pmatrix}\n-\n\\left[\\nabla^{2} C(p_{0},q_{0})\\right]^{-1}\\nabla C(p_{0},q_{0}).\n$$\nFirst compute the gradient using the chain rule. Let $r=pq-2$. Then $C=r^{2}$, so\n$$\n\\frac{\\partial C}{\\partial p}=2r\\frac{\\partial r}{\\partial p}=2r\\,q=2q(pq-2),\\qquad\n\\frac{\\partial C}{\\partial q}=2r\\frac{\\partial r}{\\partial q}=2r\\,p=2p(pq-2).\n$$\nThus\n$$\n\\nabla C(p,q)=\\begin{pmatrix} 2q(pq-2) \\\\ 2p(pq-2) \\end{pmatrix}.\n$$\nNext compute the Hessian. Differentiate the gradient components:\n$$\n\\frac{\\partial^{2} C}{\\partial p^{2}}=\\frac{\\partial}{\\partial p}\\left(2q(pq-2)\\right)=2q\\frac{\\partial}{\\partial p}(pq-2)=2q^{2},\n$$\n$$\n\\frac{\\partial^{2} C}{\\partial q^{2}}=\\frac{\\partial}{\\partial q}\\left(2p(pq-2)\\right)=2p\\frac{\\partial}{\\partial q}(pq-2)=2p^{2},\n$$\n$$\n\\frac{\\partial^{2} C}{\\partial p\\,\\partial q}=\\frac{\\partial}{\\partial q}\\left(2q(pq-2)\\right)=2(pq-2)+2pq=4pq-4.\n$$\nTherefore\n$$\n\\nabla^{2} C(p,q)=\\begin{pmatrix} 2q^{2} & 4pq-4 \\\\ 4pq-4 & 2p^{2} \\end{pmatrix}.\n$$\nEvaluate at $(p_{0},q_{0})=(1,1)$. Here $pq-2=-1$, so\n$$\n\\nabla C(1,1)=\\begin{pmatrix} 2\\cdot 1 \\cdot (-1) \\\\ 2\\cdot 1 \\cdot (-1) \\end{pmatrix}=\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix},\n\\qquad\n\\nabla^{2} C(1,1)=\\begin{pmatrix} 2\\cdot 1^{2} & 4\\cdot 1\\cdot 1 - 4 \\\\ 4\\cdot 1\\cdot 1 - 4 & 2\\cdot 1^{2} \\end{pmatrix}\n=\\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nThe inverse Hessian at $(1,1)$ is\n$$\n\\left[\\nabla^{2} C(1,1)\\right]^{-1}=\\frac{1}{2}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nApply the Newton update:\n$$\n\\begin{pmatrix} p_{1} \\\\ q_{1} \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n-\n\\frac{1}{2}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n-\n\\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}.\n$$\nThus, after one Newton step, the parameters are $(p_{1},q_{1})=(2,2)$.", "answer": "$$\\boxed{\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}}$$", "id": "2190682"}]}