{"hands_on_practices": [{"introduction": "The first step in mastering the backtracking line search is understanding its core component: the Armijo condition. This exercise provides a focused opportunity to practice the fundamental calculation. By plugging values into the sufficient decrease inequality, you can determine if a proposed step size is acceptable, grounding the abstract formula in a concrete example. [@problem_id:2154929]", "problem": "In the field of robotics, an engineer is fine-tuning a parameter $x$ that controls the stability of a walking gait. The instability of the gait is quantified by a cost function $J(x) = 5(x-100)^2$, where $x$ is a dimensionless parameter. The goal is to find the value of $x$ that minimizes $J(x)$. The optimization process starts at an initial guess of $x_k = 101$.\n\nA gradient descent algorithm is employed. At $x_k=101$, the algorithm computes a search direction $p_k = -1$. To determine how far to step in this direction, a backtracking line search is used with an initial trial step size of $\\alpha = 0.1$. A step is considered acceptable if it satisfies the Armijo condition for sufficient decrease:\n$$J(x_k + \\alpha p_k) \\le J(x_k) + c_1 \\alpha \\nabla J(x_k) \\cdot p_k$$\nThe constant for this criterion is set to $c_1 = 0.5$.\n\nBased on these values, which of the following statements is correct?\n\nA. The Armijo condition is not satisfied, so the step size $\\alpha$ must be decreased.\n\nB. The Armijo condition is satisfied, and the new cost function value is $J(x_{k+1}) = 4.05$.\n\nC. The Armijo condition is satisfied, and the new cost function value is $J(x_{k+1}) = 4.50$.\n\nD. The Armijo condition is not satisfied, but the cost function value still decreases.\n\nE. The search direction $p_k=-1$ is not a descent direction at $x_k=101$.", "solution": "We are given the cost function $J(x) = 5(x-100)^{2}$, the current point $x_{k} = 101$, the search direction $p_{k} = -1$, the initial trial step size $\\alpha = 0.1$, and the Armijo condition\n$$\nJ(x_{k} + \\alpha p_{k}) \\le J(x_{k}) + c_{1}\\alpha \\nabla J(x_{k}) \\cdot p_{k},\n$$\nwith $c_{1} = 0.5$.\n\nFirst, compute the gradient of $J$. Since $J(x) = 5(x-100)^{2}$, by differentiation we have\n$$\n\\nabla J(x) = \\frac{d}{dx} \\left[5(x-100)^{2}\\right] = 10(x-100).\n$$\nAt $x_{k} = 101$, this gives\n$$\n\\nabla J(x_{k}) = 10(101 - 100) = 10.\n$$\nCheck that $p_{k}$ is a descent direction:\n$$\n\\nabla J(x_{k}) \\cdot p_{k} = 10 \\cdot (-1) = -10 < 0,\n$$\nso $p_{k}$ is a descent direction, ruling out option E.\n\nEvaluate the left-hand side of the Armijo condition:\n$$\nx_{k} + \\alpha p_{k} = 101 + 0.1(-1) = 100.9,\n$$\n$$\nJ(x_{k} + \\alpha p_{k}) = J(100.9) = 5(100.9 - 100)^{2} = 5(0.9)^{2} = 5 \\cdot 0.81 = 4.05.\n$$\n\nEvaluate the right-hand side:\n$$\nJ(x_{k}) = 5(101 - 100)^{2} = 5(1)^{2} = 5,\n$$\n$$\nJ(x_{k}) + c_{1}\\alpha \\nabla J(x_{k}) \\cdot p_{k} = 5 + 0.5 \\cdot 0.1 \\cdot (-10) = 5 - 0.5 = 4.5.\n$$\n\nCompare both sides:\n$$\nJ(x_{k} + \\alpha p_{k}) = 4.05 \\le 4.5 = J(x_{k}) + c_{1}\\alpha \\nabla J(x_{k}) \\cdot p_{k}.\n$$\nTherefore, the Armijo condition is satisfied with the trial step size $\\alpha = 0.1$, and the new cost function value is $J(x_{k+1}) = 4.05$.\n\nThis confirms that option B is correct. Options A and D are false because the Armijo condition is satisfied; option C gives an incorrect new cost value; and option E is false because $p_{k}$ is a descent direction.", "answer": "$$\\boxed{B}$$", "id": "2154929"}, {"introduction": "While checking the condition for a single step size is crucial, the real power of the method lies in its iterative nature. This practice expands on the previous exercise by challenging you to execute the complete backtracking loop. You will start with an initial guess for the step size and systematically reduce it until the Armijo condition is satisfied, simulating how an optimization solver automatically finds a viable step. [@problem_id:2154887]", "problem": "In the field of numerical optimization, the method of steepest descent is an iterative algorithm used to find a local minimum of a function. At each iteration $k$, the next point $x_{k+1}$ is determined from the current point $x_k$ by moving a certain distance along the direction of the negative gradient. The update rule is given by $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k > 0$ is the step size.\n\nA common strategy to determine an appropriate step size $\\alpha_k$ is the backtracking line search. This procedure starts with an initial guess for the step size, $\\bar{\\alpha}$, and iteratively reduces it until a sufficient decrease in the function value is achieved. This is formalized by the Armijo condition.\n\nConsider the task of minimizing the function $f(x, y) = x^2 + 4y^2$. We are at the point $x_k = (3, 1)$ and wish to find the next iterate using the steepest descent direction. The search direction $p_k$ is therefore given by $p_k = -\\nabla f(x_k)$.\n\nTo find the step size $\\alpha_k$, you will perform a backtracking line search with the following parameters: an initial step size $\\bar{\\alpha} = 1$, a contraction factor $\\rho = 0.5$, and a sufficient decrease constant $c_1 = 0.25$. The process is as follows:\n1. Start with $\\alpha = \\bar{\\alpha}$.\n2. Check if the Armijo condition is satisfied: $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k$.\n3. If the condition is met, the accepted step size is $\\alpha_k = \\alpha$.\n4. If the condition is not met, update the step size by setting $\\alpha \\leftarrow \\rho \\alpha$ and repeat from step 2.\n\nDetermine the accepted step size $\\alpha_k$ resulting from this procedure.", "solution": "We minimize $f(x,y)=x^{2}+4y^{2}$. Its gradient is $\\nabla f(x,y)=(2x,8y)$. At $x_{k}=(3,1)$,\n$$\n\\nabla f(x_{k})=(6,8), \\quad p_{k}=-\\nabla f(x_{k})=(-6,-8).\n$$\nThe Armijo condition with parameters $c_{1}=\\frac{1}{4}$ and step $\\alpha>0$ is\n$$\nf(x_{k}+\\alpha p_{k}) \\le f(x_{k}) + c_{1}\\,\\alpha\\,\\nabla f(x_{k})^{T}p_{k}.\n$$\nCompute the needed quantities. First,\n$$\nf(x_{k})=3^{2}+4\\cdot 1^{2}=13, \\quad \\nabla f(x_{k})^{T}p_{k}=(6,8)\\cdot(-6,-8)=-100.\n$$\nNext, parameterize along the search direction:\n$$\nx_{k}+\\alpha p_{k}=(3-6\\alpha,\\,1-8\\alpha),\n$$\nso\n$$\nf(x_{k}+\\alpha p_{k})=(3-6\\alpha)^{2}+4(1-8\\alpha)^{2}\n=9-36\\alpha+36\\alpha^{2}+4-64\\alpha+256\\alpha^{2}\n=13-100\\alpha+292\\alpha^{2}.\n$$\nThus the Armijo inequality becomes\n$$\n13-100\\alpha+292\\alpha^{2} \\le 13 + \\frac{1}{4}\\alpha(-100) = 13-25\\alpha,\n$$\nwhich simplifies to\n$$\n-100\\alpha+292\\alpha^{2} \\le -25\\alpha \\;\\;\\Longleftrightarrow\\;\\; 292\\alpha^{2} \\le 75\\alpha.\n$$\nFor $\\alpha>0$, this is equivalent to\n$$\n\\alpha \\le \\frac{75}{292}.\n$$\nThe backtracking line search starts with $\\bar{\\alpha}=1$ and $\\rho=0.5$. Test sequentially:\n- $\\alpha=1$: since $1>\\frac{75}{292}$, the condition is not satisfied; reduce to $\\alpha=0.5$.\n- $\\alpha=0.5$: since $0.5>\\frac{75}{292}$, the condition is not satisfied; reduce to $\\alpha=0.25$.\n- $\\alpha=0.25$: since $\\frac{1}{4}\\le \\frac{75}{292}$ (as $0.25 \\le 0.2568...$), the Armijo condition holds.\nThe final accepted step size is $\\alpha_k = 0.25$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "2154887"}, {"introduction": "The constants in an algorithm often have deep theoretical implications, and the Armijo condition's parameter $c_1$ is no exception. This final practice moves from computation to a more theoretical exploration of how the choice of $c_1$ impacts the line search's behavior. By analyzing the condition for a special and important class of functions, you will uncover a fundamental limit on $c_1$ that ensures the line search can accept an 'optimal' step, providing insight into why practical implementations favor certain parameter ranges. [@problem_id:2154908]", "problem": "In the field of numerical optimization, a backtracking line search is a common method for choosing a step size $\\alpha$ that provides sufficient decrease in an objective function. Consider the task of minimizing a general strongly convex quadratic function of the form $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$, where $\\mathbf{x} \\in \\mathbb{R}^n$, $Q$ is a symmetric positive definite $n \\times n$ matrix, and $\\mathbf{b} \\in \\mathbb{R}^n$.\n\nWe are at a point $\\mathbf{x}_k$ which is not the minimizer, and we use the steepest descent direction for our search, defined as $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$. A step size $\\alpha > 0$ is considered acceptable if it satisfies the Armijo condition for sufficient decrease:\n$$f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}_k$$\nwhere the parameter $c_1$ is a constant in the interval $(0, 1)$.\n\nLet $\\alpha^*$ be the unique step size that corresponds to the exact minimum of the function $f$ along the search direction $\\mathbf{p}_k$. This exact step size $\\alpha^*$ is guaranteed to be accepted by the Armijo condition (i.e., satisfy the inequality) for any valid choice of $Q$ and $\\mathbf{b}$ (provided $\\mathbf{x}_k$ is not the true minimizer) if and only if the parameter $c_1$ is less than or equal to a certain threshold value, which we will call $C_{max}$.\n\nDetermine the value of this threshold $C_{max}$.", "solution": "We define $g_{k} = \\nabla f(\\mathbf{x}_{k}) = Q\\mathbf{x}_{k} - \\mathbf{b}$ and choose the steepest descent direction $\\mathbf{p}_{k} = -g_{k}$. Consider the univariate function along the line $\\phi(\\alpha) = f(\\mathbf{x}_{k} + \\alpha \\mathbf{p}_{k})$. Using the quadratic form,\n$$\n\\phi(\\alpha) = f(\\mathbf{x}_{k}) + \\alpha g_{k}^{T}\\mathbf{p}_{k} + \\frac{1}{2}\\alpha^{2}\\mathbf{p}_{k}^{T}Q\\mathbf{p}_{k}.\n$$\nDifferentiating, $\\phi'(\\alpha) = g_{k}^{T}\\mathbf{p}_{k} + \\alpha\\,\\mathbf{p}_{k}^{T}Q\\mathbf{p}_{k}$. The exact minimizer along the line satisfies $\\phi'(\\alpha^{*}) = 0$, which gives\n$$\n\\alpha^{*} = -\\frac{g_{k}^{T}\\mathbf{p}_{k}}{\\mathbf{p}_{k}^{T}Q\\mathbf{p}_{k}}.\n$$\nWith $\\mathbf{p}_{k} = -g_{k}$, we have $g_{k}^{T}\\mathbf{p}_{k} = -\\|g_{k}\\|^{2} < 0$ and $\\mathbf{p}_{k}^{T}Q\\mathbf{p}_{k} = g_{k}^{T}Qg_{k} > 0$, so $\\alpha^{*} > 0$. The minimized value along the line is\n$$\n\\phi(\\alpha^{*}) = f(\\mathbf{x}_{k}) + \\frac{1}{2} \\alpha^{*} g_{k}^{T}\\mathbf{p}_{k} = f(\\mathbf{x}_{k}) - \\frac{\\left(g_{k}^{T}\\mathbf{p}_{k}\\right)^{2}}{2\\,\\mathbf{p}_{k}^{T}Q\\mathbf{p}_{k}}.\n$$\nThe Armijo condition at $\\alpha^{*}$ requires\n$$\n\\phi(\\alpha^{*}) \\le f(\\mathbf{x}_{k}) + c_{1}\\alpha^{*} g_{k}^{T}\\mathbf{p}_{k}.\n$$\nSubstituting the expressions for $\\phi(\\alpha^{*})$ and $\\alpha^{*}$, we obtain\n$$\nf(\\mathbf{x}_{k}) + \\frac{1}{2}\\alpha^{*} g_{k}^{T}\\mathbf{p}_{k} \\le f(\\mathbf{x}_{k}) + c_{1}\\alpha^{*} g_{k}^{T}\\mathbf{p}_{k}.\n$$\nAfter canceling $f(\\mathbf{x}_{k})$, the inequality becomes $\\frac{1}{2}\\alpha^{*} g_{k}^{T}\\mathbf{p}_{k} \\le c_{1}\\alpha^{*} g_{k}^{T}\\mathbf{p}_{k}$. Since $\\mathbf{x}_k$ is not the minimizer, $g_k \\neq \\mathbf{0}$, $\\mathbf{p}_k \\neq \\mathbf{0}$, and $\\alpha^* > 0$. Also, $g_{k}^{T}\\mathbf{p}_{k} < 0$ for a descent direction. We can divide by the negative quantity $\\alpha^{*} g_{k}^{T}\\mathbf{p}_{k}$, which reverses the inequality sign:\n$$\n\\frac{1}{2} \\ge c_{1}.\n$$\nThus the exact step $\\alpha^{*}$ satisfies the Armijo condition if and only if $c_{1} \\le \\frac{1}{2}$, independent of $Q$ and $\\mathbf{b}$ (for $g_{k} \\neq \\mathbf{0}$). Therefore the threshold is $C_{\\max} = \\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2154908"}]}