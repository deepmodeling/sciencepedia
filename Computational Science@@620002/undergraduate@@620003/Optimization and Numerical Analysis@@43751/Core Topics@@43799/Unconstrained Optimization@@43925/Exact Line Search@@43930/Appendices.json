{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will tackle a foundational problem in numerical optimization. This exercise [@problem_id:2170891] walks you through a single, complete step of the steepest descent algorithm, calculating the optimal step size $\\alpha$ using an exact line search. By working with a simple quadratic function, you will master the core mechanics of finding the descent direction and then minimizing the function along that line.", "problem": "Consider the optimization problem of minimizing the quadratic objective function $f(x_1, x_2) = (x_1 + x_2 - 2)^2$. We will perform one step of the steepest descent algorithm, beginning from the initial point $\\mathbf{x}_0 = (0, 0)$. The search direction for this step is the steepest descent direction, defined as $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$, where $\\nabla f$ is the gradient of the function.\n\nTo find the optimal distance to move along this direction, an exact line search is performed. This procedure involves solving a one-dimensional minimization problem to find the step size $\\alpha > 0$ that minimizes the function $g(\\alpha) = f(\\mathbf{x}_0 + \\alpha\\mathbf{p}_0)$.\n\nDetermine the exact optimal step size $\\alpha^*$ that results from this procedure.", "solution": "We are given the quadratic function $f(x_1, x_2) = (x_1 + x_2 - 2)^2$ and the initial point $\\mathbf{x}_0 = (0, 0)$. The steepest descent direction is defined as $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$, where the gradient is computed by differentiating:\n$$\n\\nabla f(x_1, x_2) = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}\\right).\n$$\nSince $f(x_1, x_2) = (x_1 + x_2 - 2)^2$, by the chain rule,\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 + x_2 - 2)\\cdot \\frac{\\partial}{\\partial x_1}(x_1 + x_2 - 2) = 2(x_1 + x_2 - 2),\n$$\nand similarly,\n$$\n\\frac{\\partial f}{\\partial x_2} = 2(x_1 + x_2 - 2).\n$$\nTherefore,\n$$\n\\nabla f(x_1, x_2) = 2(x_1 + x_2 - 2)\\,(1, 1).\n$$\nEvaluating at $\\mathbf{x}_0 = (0, 0)$ gives\n$$\n\\nabla f(\\mathbf{x}_0) = 2(0 + 0 - 2)\\,(1, 1) = -4\\,(1, 1) = (-4, -4),\n$$\nso the steepest descent direction is\n$$\n\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0) = (4, 4).\n$$\nFor the exact line search, define the univariate function\n$$\ng(\\alpha) = f(\\mathbf{x}_0 + \\alpha \\mathbf{p}_0) = f\\big((0, 0) + \\alpha(4, 4)\\big) = f(4\\alpha, 4\\alpha).\n$$\nSubstituting into $f$,\n$$\ng(\\alpha) = (4\\alpha + 4\\alpha - 2)^2 = (8\\alpha - 2)^2.\n$$\nTo minimize $g(\\alpha)$ over $\\alpha > 0$, set its derivative to zero:\n$$\ng'(\\alpha) = 2(8\\alpha - 2)\\cdot 8 = 16(8\\alpha - 2).\n$$\nSolve $g'(\\alpha) = 0$:\n$$\n16(8\\alpha - 2) = 0 \\;\\;\\Longrightarrow\\;\\; 8\\alpha - 2 = 0 \\;\\;\\Longrightarrow\\;\\; \\alpha^{*} = \\frac{1}{4}.\n$$\nThis satisfies $\\alpha^{*} > 0$ and yields the exact minimizer along the search direction.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "2170891"}, {"introduction": "Optimization methods are not limited to simple polynomial functions; they are powerful tools for a wide range of problems. This next practice [@problem_id:2170899] challenges you to apply the exact line search technique to a trigonometric function, which introduces the possibility of multiple local minima along your search direction. This scenario highlights the importance of carefully analyzing the one-dimensional subproblem to identify the correct step size.", "problem": "Consider the process of minimizing a single-variable objective function $f(x)$ using an iterative optimization algorithm. A key step in such algorithms is the line search, where, starting from a point $x_k$, we find a step size $\\alpha$ that minimizes the function along a given search direction $p_k$. The next point in the sequence is then determined by $x_{k+1} = x_k + \\alpha p_k$.\n\nLet the objective function be $f(x) = \\sin(x) + \\cos(x)$, where the argument $x$ is in radians. Suppose we are at the initial point $x_0 = 0$. The search direction is chosen to be the direction of steepest descent, which for a single-variable function is given by $p_0 = -f'(x_0)$, where $f'(x_0)$ is the derivative of $f(x)$ evaluated at $x_0$.\n\nDetermine the smallest positive step size, $\\alpha > 0$, that minimizes the function along this direction. That is, find the smallest positive $\\alpha$ that minimizes the new function $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$.\n\nProvide the exact value of $\\alpha$ as a closed-form analytic expression.", "solution": "We are given $f(x) = \\sin(x) + \\cos(x)$ and $x_{0} = 0$. The steepest descent direction is $p_{0} = -f'(x_{0})$. Compute the derivative:\n$$\nf'(x) = \\cos(x) - \\sin(x).\n$$\nEvaluate at $x_{0} = 0$:\n$$\nf'(0) = \\cos(0) - \\sin(0) = 1 - 0 = 1,\n$$\nso\n$$\np_{0} = -f'(0) = -1.\n$$\nDefine the line-search objective $\\phi(\\alpha) = f(x_{0} + \\alpha p_{0})$:\n$$\n\\phi(\\alpha) = f(0 + \\alpha(-1)) = f(-\\alpha) = \\sin(-\\alpha) + \\cos(-\\alpha) = -\\sin(\\alpha) + \\cos(\\alpha).\n$$\nTo minimize $\\phi(\\alpha)$ over $\\alpha > 0$, differentiate and set to zero:\n$$\n\\phi'(\\alpha) = -\\cos(\\alpha) - \\sin(\\alpha) = 0 \\quad \\Longleftrightarrow \\quad \\cos(\\alpha) + \\sin(\\alpha) = 0.\n$$\nThis gives\n$$\n\\tan(\\alpha) = -1 \\quad \\text{with} \\quad \\cos(\\alpha) \\neq 0 \\quad \\Longrightarrow \\quad \\alpha = -\\frac{\\pi}{4} + n\\pi, \\quad n \\in \\mathbb{Z}.\n$$\nAmong positive solutions, the smallest is obtained with $n=1$:\n$$\n\\alpha = -\\frac{\\pi}{4} + \\pi = \\frac{3\\pi}{4}.\n$$\nVerify it is a minimum via the second derivative:\n$$\n\\phi''(\\alpha) = \\sin(\\alpha) - \\cos(\\alpha).\n$$\nAt $\\alpha = \\frac{3\\pi}{4}$, $\\sin\\left(\\frac{3\\pi}{4}\\right) > 0$ and $\\cos\\left(\\frac{3\\pi}{4}\\right) < 0$, hence $\\phi''\\left(\\frac{3\\pi}{4}\\right) > 0$, confirming a local minimum. Since $\\phi$ is periodic with period $2\\pi$ and all minima occur at $\\alpha = \\frac{3\\pi}{4} + 2\\pi k$, $k \\in \\mathbb{Z}$, the smallest positive minimizer is $\\alpha = \\frac{3\\pi}{4}$.", "answer": "$$\\boxed{\\frac{3\\pi}{4}}$$", "id": "2170899"}, {"introduction": "Our final exercise moves from direct calculation to a more conceptual analysis, connecting an algorithm's behavior to the underlying structure of the objective function. Based on a hypothetical observation about the path of the steepest descent method, you are asked to deduce a fundamental property of the function itself [@problem_id:2170940]. This practice will deepen your intuition for how the geometry of a function, particularly its curvature, dictates the performance and trajectory of optimization algorithms.", "problem": "Consider a quadratic objective function $f(x_1, x_2) = \\frac{A}{2} x_1^2 + \\frac{B}{2} x_2^2 - C x_2$, where $A, B, C$ are positive real constants. The method of steepest descent with exact line search is used to find the minimum of this function. It is observed that for any starting point $\\mathbf{x}_0 = (x_{0,1}, 0)$ with $x_{0,1} \\neq 0$, the first iterate of the algorithm, $\\mathbf{x}_1$, always lies on the $x_2$-axis. Based on this observation, determine the value of the ratio $B/A$.", "solution": "We minimize the quadratic function $f(x_1,x_2)=\\frac{A}{2}x_1^2+\\frac{B}{2}x_2^2-C x_2$ with $A>0$, $B>0$, $C>0$ by steepest descent with exact line search. Its gradient and Hessian are\n$$\n\\nabla f(x_1,x_2)=\\begin{pmatrix}A x_1\\\\ B x_2-C\\end{pmatrix},\\qquad H=\\begin{pmatrix}A&0\\\\0&B\\end{pmatrix}.\n$$\nLet the starting point be $\\mathbf{x}_0=(x_{0,1},0)^T$ with $x_{0,1}\\neq 0$. Then the initial gradient is\n$$\n\\mathbf{g}_0=\\nabla f(\\mathbf{x}_0)=\\begin{pmatrix}A x_{0,1}\\\\ -C\\end{pmatrix}.\n$$\nThe steepest descent direction is $\\mathbf{d}_0=-\\mathbf{g}_0$, and with exact line search the step size $\\alpha_0$ minimizes $\\phi(\\alpha)=f(\\mathbf{x}_0+\\alpha \\mathbf{d}_0)$. For a quadratic, using $\\nabla f(\\mathbf{x}_0+\\alpha \\mathbf{d}_0)=\\nabla f(\\mathbf{x}_0)+\\alpha H \\mathbf{d}_0$, we have\n$$\n\\phi'(\\alpha)=\\nabla f(\\mathbf{x}_0+\\alpha \\mathbf{d}_0)^{\\top} \\mathbf{d}_0\n=\\left(\\mathbf{g}_0+\\alpha H \\mathbf{d}_0\\right)^{\\top} \\mathbf{d}_0\n=\\mathbf{g}_0^{\\top} \\mathbf{d}_0+\\alpha \\mathbf{d}_0^{\\top} H \\mathbf{d}_0.\n$$\nSetting $\\phi'(\\alpha_0)=0$ and using $\\mathbf{d}_0 = -\\mathbf{g}_0$ yields\n$$\n\\alpha_0=\\frac{\\mathbf{g}_0^{\\top} \\mathbf{g}_0}{\\mathbf{g}_0^{\\top} H \\mathbf{g}_0}.\n$$\nCompute the needed quantities:\n$$\n\\mathbf{g}_0^{\\top} \\mathbf{g}_0=(A x_{0,1})^2+C^2=A^2x_{0,1}^2+C^2,\n$$\n$$\nH \\mathbf{g}_0=\\begin{pmatrix}A&0\\\\0&B\\end{pmatrix}\\begin{pmatrix}A x_{0,1}\\\\ -C\\end{pmatrix}\n=\\begin{pmatrix}A^2 x_{0,1}\\\\ -B C\\end{pmatrix},\n$$\n$$\n\\mathbf{g}_0^{\\top} H \\mathbf{g}_0=(A x_{0,1})(A^2 x_{0,1})+(-C)(-B C)=A^3 x_{0,1}^2+B C^2.\n$$\nHence\n$$\n\\alpha_0=\\frac{A^2 x_{0,1}^2+C^2}{A^3 x_{0,1}^2+B C^2}.\n$$\nThe first iterate is\n$$\n\\mathbf{x}_1=\\mathbf{x}_0-\\alpha_0 \\mathbf{g}_0=\\begin{pmatrix}x_{0,1}-\\alpha_0 A x_{0,1}\\\\ 0-\\alpha_0(-C)\\end{pmatrix},\n$$\nso its first component is\n$$\nx_{1,1}=x_{0,1}\\left(1-\\alpha_0 A\\right).\n$$\nThe observation states $\\mathbf{x}_1$ always lies on the $x_2$-axis for any $x_{0,1}\\neq 0$, thus $x_{1,1}=0$ for all such $x_{0,1}$. Therefore $1-\\alpha_0 A=0$ must hold for all $x_{0,1}$, i.e.,\n$$\n\\alpha_0=\\frac{1}{A}.\n$$\nEquating this with the exact line-search expression gives\n$$\n\\frac{A^2 x_{0,1}^2+C^2}{A^3 x_{0,1}^2+B C^2}=\\frac{1}{A}.\n$$\nCross-multiplying and simplifying,\n$$\nA\\left(A^2 x_{0,1}^2+C^2\\right)=A^3 x_{0,1}^2+B C^2\n\\;\\;\\Longrightarrow\\;\\;\nA^3 x_{0,1}^2+A C^2=A^3 x_{0,1}^2+B C^2\n\\;\\;\\Longrightarrow\\;\\;\n(A-B) C^2=0.\n$$\nSince $C>0$, it follows that $A=B$, hence\n$$\n\\frac{B}{A}=1.\n$$\nThis condition is also sufficient, because if $A=B$ then $\\alpha_0=(A^2 x_{0,1}^2+C^2)/(A^3 x_{0,1}^2+A C^2)=1/A$ for every $x_{0,1}$, yielding $x_{1,1}=x_{0,1}(1-\\alpha_0 A)=0$.", "answer": "$$\\boxed{1}$$", "id": "2170940"}]}