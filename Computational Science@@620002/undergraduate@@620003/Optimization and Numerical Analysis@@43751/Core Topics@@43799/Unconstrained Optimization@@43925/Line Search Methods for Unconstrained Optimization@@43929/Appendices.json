{"hands_on_practices": [{"introduction": "Before diving into sophisticated line search algorithms, it is crucial to understand why they are necessary. This first exercise provides a concrete comparison between using a simple, fixed step size and performing an exact line search to find the optimal step size. By working through this problem [@problem_id:2184823], you will quantify the performance difference and build a strong intuition for the value of optimizing the step length $\\alpha$ at each iteration.", "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x_1, x_2) = x_1^2 + 4x_2^2$. We are interested in finding the minimum of this function using an iterative method. Starting from the point $\\mathbf{x}_0 = (4, 1)^T$, a new iterate $\\mathbf{x}_1$ is generated according to the update rule $\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0)$, where $\\nabla f(\\mathbf{x}_0)$ is the gradient of $f$ evaluated at $\\mathbf{x}_0$, and $\\alpha > 0$ is a scalar known as the step size.\n\nTwo different strategies for choosing the step size are to be compared.\n\nStrategy A: A fixed step size of $\\alpha_A = 0.1$ is used. Let the resulting iterate be $\\mathbf{x}_{1,A}$.\nStrategy B: An exact line search is performed to find the optimal step size, $\\alpha_B$, which is the value of $\\alpha$ that minimizes the one-dimensional function $g(\\alpha) = f(\\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0))$. Let the resulting iterate be $\\mathbf{x}_{1,B}$.\n\nCalculate the ratio $R = \\frac{f(\\mathbf{x}_{1,A})}{f(\\mathbf{x}_{1,B})}$. Report your answer for $R$ rounded to three significant figures.", "solution": "We are minimizing $f(x_{1},x_{2})=x_{1}^{2}+4x_{2}^{2}$ with gradient $\\nabla f(x_{1},x_{2})=(2x_{1},8x_{2})^{T}$. At $\\mathbf{x}_{0}=(4,1)^{T}$, the gradient is\n$$\n\\nabla f(\\mathbf{x}_{0})=(2\\cdot 4,\\,8\\cdot 1)^{T}=(8,8)^{T}.\n$$\nThe update rule is $\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha \\nabla f(\\mathbf{x}_{0})$.\n\nStrategy A: With $\\alpha_{A}=0.1=\\frac{1}{10}$,\n$$\n\\mathbf{x}_{1,A}=(4,1)^{T}-\\tfrac{1}{10}(8,8)^{T}=\\left(4-\\tfrac{8}{10},\\,1-\\tfrac{8}{10}\\right)^{T}=\\left(\\tfrac{16}{5},\\,\\tfrac{1}{5}\\right)^{T}.\n$$\nThen\n$$\nf(\\mathbf{x}_{1,A})=\\left(\\tfrac{16}{5}\\right)^{2}+4\\left(\\tfrac{1}{5}\\right)^{2}=\\tfrac{256}{25}+\\tfrac{4}{25}=\\tfrac{260}{25}=\\tfrac{52}{5}.\n$$\n\nStrategy B: Perform exact line search on $g(\\alpha)=f(\\mathbf{x}_{0}-\\alpha \\nabla f(\\mathbf{x}_{0}))$. Using $\\mathbf{x}_{0}=(4,1)$ and $\\nabla f(\\mathbf{x}_{0})=(8,8)$,\n$$\ng(\\alpha)=f(4-8\\alpha,\\,1-8\\alpha)=(4-8\\alpha)^{2}+4(1-8\\alpha)^{2}.\n$$\nExpand:\n$$\ng(\\alpha)=(16-64\\alpha+64\\alpha^{2})+4(1-16\\alpha+64\\alpha^{2})=20-128\\alpha+320\\alpha^{2}.\n$$\nDifferentiate and set to zero:\n$$\ng'(\\alpha)=-128+640\\alpha=0 \\;\\Rightarrow\\; \\alpha_{B}=\\tfrac{128}{640}=\\tfrac{1}{5}.\n$$\nThus\n$$\n\\mathbf{x}_{1,B}=(4,1)^{T}-\\tfrac{1}{5}(8,8)^{T}=\\left(4-\\tfrac{8}{5},\\,1-\\tfrac{8}{5}\\right)^{T}=\\left(\\tfrac{12}{5},\\,-\\tfrac{3}{5}\\right)^{T},\n$$\nand\n$$\nf(\\mathbf{x}_{1,B})=\\left(\\tfrac{12}{5}\\right)^{2}+4\\left(-\\tfrac{3}{5}\\right)^{2}=\\tfrac{144}{25}+\\tfrac{36}{25}=\\tfrac{180}{25}=\\tfrac{36}{5}.\n$$\n\nCompute the ratio\n$$\nR=\\frac{f(\\mathbf{x}_{1,A})}{f(\\mathbf{x}_{1,B})}=\\frac{\\tfrac{52}{5}}{\\tfrac{36}{5}}=\\frac{52}{36}=\\frac{13}{9}\\approx 1.444\\ldots\n$$\nRounded to three significant figures, $R=1.44$.", "answer": "$$\\boxed{1.44}$$", "id": "2184823"}, {"introduction": "In practice, performing an exact line search is often computationally prohibitive. This leads to the development of inexact line search methods that find an \"good enough\" step size efficiently. This exercise provides hands-on practice with the Armijo sufficient decrease condition, a cornerstone of modern line search algorithms, within a backtracking framework [@problem_id:2184835]. The choice of a non-smooth function, $f(x)=|x|$, demonstrates how these conditions are applied and underscores their robustness even when the function's derivative is not defined everywhere.", "problem": "An optimization algorithm is used to find the minimum of the function $f(x) = |x|$ for a real variable $x$. At the current iteration, the point is $x_k = 2$. The search direction is chosen as $p_k = -1$.\n\nTo find the step size $\\alpha$, a backtracking line search algorithm is employed. The algorithm starts with an initial step size and iteratively reduces it until a sufficient decrease condition is met. The accepted step size is the first value in the sequence $\\alpha_{init}, \\rho\\alpha_{init}, \\rho^2\\alpha_{init}, \\dots$ that satisfies the condition:\n$$f(x_k + \\alpha p_k) \\leq f(x_k) + c \\alpha f'(x_k) p_k$$\nwhere $f'(x_k)$ is the derivative of $f(x)$ at $x_k$. The parameters for the algorithm are:\n- Initial step size $\\alpha_{init} = 3.0$\n- Reduction factor $\\rho = 0.5$\n- Sufficient decrease constant $c = 0.8$\n\nCalculate the value of the step size $\\alpha$ that is accepted by this algorithm. Give your answer as a real number.", "solution": "We seek the first step size in the backtracking sequence that satisfies the Armijo sufficient decrease condition for $f(x) = |x|$ at $x_{k} = 2$ with search direction $p_{k} = -1$. The derivative of $f$ exists for $x \\neq 0$ and is $f'(x) = 1$ for $x > 0$ and $f'(x) = -1$ for $x  0$. Since $x_{k} = 2 > 0$, we have $f'(x_{k}) = 1$.\n\nThe Armijo condition is\n$$\nf(x_{k} + \\alpha p_{k}) \\leq f(x_{k}) + c\\,\\alpha\\,f'(x_{k})\\,p_{k}.\n$$\nSubstituting $x_{k} = 2$, $p_{k} = -1$, and $f'(x_{k}) = 1$ gives\n$$\n|2 - \\alpha| \\leq 2 - c\\alpha.\n$$\nWe solve this inequality by cases on $\\alpha$.\n\nFor $0 \\leq \\alpha \\leq 2$, we have $|2 - \\alpha| = 2 - \\alpha$, so\n$$\n2 - \\alpha \\leq 2 - c\\alpha \\;\\;\\Longleftrightarrow\\;\\; -\\alpha \\leq -c\\alpha \\;\\;\\Longleftrightarrow\\;\\; \\alpha \\geq c\\alpha \\;\\;\\Longleftrightarrow\\;\\; (1 - c)\\alpha \\geq 0,\n$$\nwhich holds for all $\\alpha \\geq 0$ since $c \\in (0,1)$ in backtracking line search.\n\nFor $\\alpha > 2$, we have $|2 - \\alpha| = \\alpha - 2$, so\n$$\n\\alpha - 2 \\leq 2 - c\\alpha \\;\\;\\Longleftrightarrow\\;\\; (1 + c)\\alpha \\leq 4 \\;\\;\\Longleftrightarrow\\;\\; \\alpha \\leq \\frac{4}{1 + c}.\n$$\nTherefore the Armijo condition holds for all $\\alpha \\in [0, \\frac{4}{1 + c}]$.\n\nWith $c = 0.8$, this threshold is\n$$\n\\frac{4}{1 + c} = \\frac{4}{1.8} = \\frac{20}{9}.\n$$\nThe backtracking candidates are $\\alpha_{0} = \\alpha_{\\text{init}} = 3.0$, $\\alpha_{1} = \\rho \\alpha_{0} = 0.5 \\times 3.0 = 1.5$, $\\alpha_{2} = 0.75$, etc. Since $3.0 > \\frac{20}{9}$, $\\alpha_{0}$ fails the condition. Next, $\\alpha_{1} = 1.5 \\leq 2$, which lies in the range where the condition always holds, so it is accepted. As a check,\n$$\n|2 - 1.5| = 0.5 \\leq 2 - 0.8 \\times 1.5 = 0.8.\n$$\nThus the accepted step size is $\\alpha = 1.5$.", "answer": "$$\\boxed{1.5}$$", "id": "2184835"}, {"introduction": "While simple backtracking strategies, such as repeatedly halving the step size, can find a point satisfying the Wolfe conditions, they can be inefficient. This practice problem introduces a more advanced and powerful technique used in professional-grade optimization software: polynomial interpolation. You will use function and derivative information to build a local cubic model of the objective function and then find the minimum of this model to propose a much more accurate trial step size [@problem_id:2184822]. This exercise showcases how building a better local model can significantly accelerate the search for a suitable step length.", "problem": "In a line search procedure for an unconstrained optimization problem, the goal is to find a step size $\\alpha > 0$ that minimizes a function $\\phi(\\alpha)$ along a given search direction. A common strategy is to approximate $\\phi(\\alpha)$ using a simpler model based on available data.\n\nConsider a case where we model $\\phi(\\alpha)$ with a cubic polynomial, $h(\\alpha)$. We have evaluated two trial step sizes, $\\alpha_a$ and $\\alpha_b$, and obtained the corresponding function values, $\\phi(\\alpha_a)$ and $\\phi(\\alpha_b)$, as well as the derivatives, $\\phi'(\\alpha_a)$ and $\\phi'(\\alpha_b)$. The cubic model $h(\\alpha)$ is constructed to be the unique cubic polynomial that interpolates this data, meaning it satisfies $h(\\alpha_a) = \\phi(\\alpha_a)$, $h'(\\alpha_a) = \\phi'(\\alpha_a)$, $h(\\alpha_b) = \\phi(\\alpha_b)$, and $h'(\\alpha_b) = \\phi'(\\alpha_b)$.\n\nSuppose the two trial steps are at $\\alpha_a = 1$ and $\\alpha_b = 4$. The evaluated data are as follows:\n- $\\phi(\\alpha_a) = 5$\n- $\\phi'(\\alpha_a) = -7$\n- $\\phi(\\alpha_b) = 8$\n- $\\phi'(\\alpha_b) = 2$\n\nBased on this information, determine the new trial step size $\\alpha_*$ that corresponds to the local minimum of the interpolating cubic polynomial $h(\\alpha)$ within the interval $[\\alpha_a, \\alpha_b]$.\n\nRound your final answer to four significant figures.", "solution": "We model $\\phi(\\alpha)$ by a cubic polynomial $h(\\alpha)$ that interpolates the function and derivative at $\\alpha_{a}=1$ and $\\alpha_{b}=4$. Introduce $t=\\alpha-1$ so that $t\\in[0,3]$. Let\n$$\nh(\\alpha)=p(t)=a t^{3}+b t^{2}+c t+d.\n$$\nThe Hermite interpolation conditions are:\n$$\np(0)=\\phi(1)=5,\\quad p'(0)=\\phi'(1)=-7,\\quad p(3)=\\phi(4)=8,\\quad p'(3)=\\phi'(4)=2.\n$$\nFrom $p(0)=d=5$ and $p'(0)=c=-7$. Using $p(3)=27a+9b+3c+d=8$ and $p'(3)=27a+6b+c=2$, substitute $c=-7$ and $d=5$ to obtain the linear system\n$$\n27a+9b=24,\\quad 27a+6b=9.\n$$\nSubtracting the second from the first gives $3b=15$, hence $b=5$. Then $27a+6\\cdot 5=9$ implies $27a=-21$, so $a=-\\frac{7}{9}$. Therefore,\n$$\np(t)=-\\frac{7}{9}t^{3}+5t^{2}-7t+5,\\quad p'(t)=-\\frac{7}{3}t^{2}+10t-7.\n$$\nThe stationary points satisfy $p'(t)=0$:\n$$\n-\\frac{7}{3}t^{2}+10t-7=0\\;\\;\\Longleftrightarrow\\;\\;7t^{2}-30t+21=0.\n$$\nSolving gives\n$$\nt=\\frac{30\\pm\\sqrt{900-588}}{14}=\\frac{30\\pm\\sqrt{312}}{14}=\\frac{15\\pm\\sqrt{78}}{7}.\n$$\nOver $t\\in[0,3]$, only $t_{*}=\\frac{15-\\sqrt{78}}{7}$ lies in the interval. The second derivative is\n$$\np''(t)=-\\frac{14}{3}t+10,\n$$\nand $p''(t_{*})>0$, confirming a local minimum. Hence\n$$\n\\alpha_{*}=1+t_{*}=1+\\frac{15-\\sqrt{78}}{7}=\\frac{22-\\sqrt{78}}{7}\\approx 1.881177\\ldots\n$$\nRounded to four significant figures, $\\alpha_{*}=1.881$.", "answer": "$$\\boxed{1.881}$$", "id": "2184822"}]}