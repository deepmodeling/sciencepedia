{"hands_on_practices": [{"introduction": "The BFGS algorithm iteratively refines its search for a function's minimum. Each iteration provides new information that the algorithm uses to update its model of the function's curvature. This exercise [@problem_id:2208670] focuses on computing the two fundamental pieces of information gathered in a single step: the step vector $s_k$, which records the change in position, and the gradient difference vector $y_k$, which records the change in the function's slope. Mastering this first step is essential as these vectors form the basis of the entire BFGS update.", "problem": "In numerical optimization, quasi-Newton methods are a class of algorithms used to find local minima or maxima of functions. A prominent member of this class is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, which iteratively approximates the inverse Hessian matrix of the objective function. A key step in this process involves calculating two vectors from one iteration to the next: the step vector, $s_k = x_{k+1} - x_k$, and the gradient difference vector, $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n\nConsider the optimization of a two-variable function $f(x_1, x_2)$ defined as:\n$$\nf(x_1, x_2) = x_1^2 \\sin(x_2) + x_2^2 \\cos(x_1)\n$$\nAn optimization algorithm is currently at iterate $k$, and the two most recent points in the sequence are given by the vectors $x_k = (0, \\pi)$ and $x_{k+1} = (\\frac{\\pi}{2}, 0)$.\n\nCalculate the components of the step vector $s_k = (s_{k,1}, s_{k,2})$ and the gradient difference vector $y_k = (y_{k,1}, y_{k,2})$. Present your final answer as a single row matrix containing the four components in the order $s_{k,1}, s_{k,2}, y_{k,1}, y_{k,2}$.", "solution": "The goal is to compute the components of the step vector $s_k = x_{k+1} - x_k$ and the gradient difference vector $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ for the given function and points.\n\nFirst, we calculate the step vector $s_k$. The points are given as $x_k = (0, \\pi)$ and $x_{k+1} = (\\frac{\\pi}{2}, 0)$.\nThe components of $s_k$ are found by subtracting the components of $x_k$ from $x_{k+1}$:\n$$\ns_k = x_{k+1} - x_k = \\begin{pmatrix} \\frac{\\pi}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\pi \\end{pmatrix} = \\begin{pmatrix} \\frac{\\pi}{2} - 0 \\\\ 0 - \\pi \\end{pmatrix} = \\begin{pmatrix} \\frac{\\pi}{2} \\\\ -\\pi \\end{pmatrix}\n$$\nSo, $s_{k,1} = \\frac{\\pi}{2}$ and $s_{k,2} = -\\pi$.\n\nNext, we calculate the gradient difference vector $y_k$. This requires finding the gradient of the function $f(x_1, x_2)$ and evaluating it at the points $x_k$ and $x_{k+1}$.\nThe function is $f(x_1, x_2) = x_1^2 \\sin(x_2) + x_2^2 \\cos(x_1)$.\nThe gradient, $\\nabla f$, is a vector of partial derivatives:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\nThe partial derivative with respect to $x_1$ is:\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (x_1^2 \\sin(x_2) + x_2^2 \\cos(x_1)) = 2x_1 \\sin(x_2) - x_2^2 \\sin(x_1)\n$$\nThe partial derivative with respect to $x_2$ is:\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (x_1^2 \\sin(x_2) + x_2^2 \\cos(x_1)) = x_1^2 \\cos(x_2) + 2x_2 \\cos(x_1)\n$$\nSo the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} 2x_1 \\sin(x_2) - x_2^2 \\sin(x_1) \\\\ x_1^2 \\cos(x_2) + 2x_2 \\cos(x_1) \\end{pmatrix}\n$$\nNow, we evaluate the gradient at $x_k = (0, \\pi)$:\n$$\n\\nabla f(x_k) = \\nabla f(0, \\pi) = \\begin{pmatrix} 2(0) \\sin(\\pi) - \\pi^2 \\sin(0) \\\\ 0^2 \\cos(\\pi) + 2\\pi \\cos(0) \\end{pmatrix} = \\begin{pmatrix} 0 - \\pi^2(0) \\\\ 0(-1) + 2\\pi(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2\\pi \\end{pmatrix}\n$$\nNext, we evaluate the gradient at $x_{k+1} = (\\frac{\\pi}{2}, 0)$:\n$$\n\\nabla f(x_{k+1}) = \\nabla f(\\frac{\\pi}{2}, 0) = \\begin{pmatrix} 2(\\frac{\\pi}{2}) \\sin(0) - 0^2 \\sin(\\frac{\\pi}{2}) \\\\ (\\frac{\\pi}{2})^2 \\cos(0) + 2(0) \\cos(\\frac{\\pi}{2}) \\end{pmatrix} = \\begin{pmatrix} \\pi(0) - 0(1) \\\\ \\frac{\\pi^2}{4}(1) + 0(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{\\pi^2}{4} \\end{pmatrix}\n$$\nFinally, we compute the gradient difference vector $y_k$:\n$$\ny_k = \\nabla f(x_{k+1}) - \\nabla f(x_k) = \\begin{pmatrix} 0 \\\\ \\frac{\\pi^2}{4} \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 2\\pi \\end{pmatrix} = \\begin{pmatrix} 0 - 0 \\\\ \\frac{\\pi^2}{4} - 2\\pi \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{\\pi^2}{4} - 2\\pi \\end{pmatrix}\n$$\nSo, $y_{k,1} = 0$ and $y_{k,2} = \\frac{\\pi^2}{4} - 2\\pi$.\n\nThe four components are $s_{k,1} = \\frac{\\pi}{2}$, $s_{k,2} = -\\pi$, $y_{k,1} = 0$, and $y_{k,2} = \\frac{\\pi^2}{4} - 2\\pi$.\nAssembling these into a single row matrix in the specified order gives:\n$$\n\\begin{pmatrix} \\frac{\\pi}{2}  -\\pi  0  \\frac{\\pi^2}{4} - 2\\pi \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\pi}{2}  -\\pi  0  \\frac{\\pi^{2}}{4} - 2\\pi \\end{pmatrix}}\n$$", "id": "2208670"}, {"introduction": "For the BFGS update to be successful and numerically stable, a critical prerequisite known as the curvature condition must be met. This condition, expressed as $y_k^T s_k \\gt 0$, ensures that the step taken moves into a region where the function's gradient is changing in a way consistent with minimization geometry. This practice [@problem_id:2208660] will guide you through calculating this crucial value, highlighting the link between the algorithm's mechanics and the underlying optimization landscape.", "problem": "In the context of unconstrained optimization, quasi-Newton methods are a class of popular iterative algorithms for finding local minima. A key algorithm in this family is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. The BFGS update for the approximate Hessian matrix requires that a specific condition, known as the curvature condition, is satisfied at each step to ensure the updated matrix remains positive definite.\n\nConsider the optimization of a function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(x_1, x_2) = x_1^4 + x_1 x_2 + (1+x_2)^2$$\n\nSuppose an iterative optimization process is at the iterate $x_k = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and moves to the next iterate $x_{k+1} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}$.\n\nThe curvature condition is characterized by the sign of the product $y_k^T s_k$. The vectors $s_k$ and $y_k$ are defined as:\n- The step vector: $s_k = x_{k+1} - x_k$\n- The change in gradient: $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$\n\nCalculate the value of the expression $y_k^T s_k$ for the given iterates. Round your final answer to three significant figures.", "solution": "We first compute the gradient of $f(x_{1},x_{2})=x_{1}^{4}+x_{1}x_{2}+(1+x_{2})^{2}$. The partial derivatives are\n$$\n\\frac{\\partial f}{\\partial x_{1}}=4x_{1}^{3}+x_{2}, \\qquad \\frac{\\partial f}{\\partial x_{2}}=x_{1}+2(1+x_{2}).\n$$\nHence,\n$$\n\\nabla f(x_{1},x_{2})=\\begin{pmatrix}4x_{1}^{3}+x_{2}\\\\ x_{1}+2(1+x_{2})\\end{pmatrix}.\n$$\n\nWith $x_{k}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$ and $x_{k+1}=\\begin{pmatrix}\\frac{1}{2}\\\\ 1\\end{pmatrix}$, the step is\n$$\ns_{k}=x_{k+1}-x_{k}=\\begin{pmatrix}\\frac{1}{2}-1\\\\ 1-1\\end{pmatrix}=\\begin{pmatrix}-\\frac{1}{2}\\\\ 0\\end{pmatrix}.\n$$\n\nEvaluate the gradients:\n$$\n\\nabla f(x_{k})=\\begin{pmatrix}4\\cdot 1^{3}+1\\\\ 1+2(1+1)\\end{pmatrix}=\\begin{pmatrix}5\\\\ 5\\end{pmatrix}, \\qquad\n\\nabla f(x_{k+1})=\\begin{pmatrix}4\\left(\\frac{1}{2}\\right)^{3}+1\\\\ \\frac{1}{2}+2(1+1)\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{2}\\\\ \\frac{9}{2}\\end{pmatrix}.\n$$\n\nThus,\n$$\ny_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})=\\begin{pmatrix}\\frac{3}{2}-5\\\\ \\frac{9}{2}-5\\end{pmatrix}=\\begin{pmatrix}-\\frac{7}{2}\\\\ -\\frac{1}{2}\\end{pmatrix}.\n$$\n\nThe curvature quantity is\n$$\ny_{k}^{T}s_{k}=\\begin{pmatrix}-\\frac{7}{2}  -\\frac{1}{2}\\end{pmatrix}\\begin{pmatrix}-\\frac{1}{2}\\\\ 0\\end{pmatrix}=\\left(-\\frac{7}{2}\\right)\\left(-\\frac{1}{2}\\right)+\\left(-\\frac{1}{2}\\right)\\cdot 0=\\frac{7}{4}=1.75.\n$$\n\nRounded to three significant figures, the value is $1.75$.", "answer": "$$\\boxed{1.75}$$", "id": "2208660"}, {"introduction": "The elegance of the BFGS formula lies not in its complexity, but in the fundamental property it is designed to satisfy: the secant condition. This condition, $H_{k+1} y_k = s_k$, ensures that the new inverse Hessian approximation $H_{k+1}$ is consistent with the most recently observed information from the function. In this final practice [@problem_id:2208659], you will not only perform the full BFGS update but also verify that the resulting matrix satisfies the secant condition, revealing the core purpose behind the formula's structure.", "problem": "In the field of numerical optimization, quasi-Newton methods are popular for finding local minima of functions. These methods build an approximation of the inverse Hessian matrix at each iteration. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) update is one of the most effective algorithms for this purpose.\n\nThe BFGS formula for updating the approximate inverse Hessian, $H_k$, to $H_{k+1}$ is given by:\n$$H_{k+1} = \\left(I - \\frac{s_k y_k^T}{y_k^T s_k}\\right) H_k \\left(I - \\frac{y_k s_k^T}{y_k^T s_k}\\right) + \\frac{s_k s_k^T}{y_k^T s_k}$$\nwhere $I$ is the identity matrix, $s_k = x_{k+1} - x_k$ is the step in position, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient. The term $v^T$ denotes the transpose of a column vector $v$.\n\nConsider a single step of a quasi-Newton optimization algorithm in a two-dimensional space. The relevant quantities are given as:\n- Initial approximate inverse Hessian: $H_k = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\n- Step vector: $s_k = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$\n- Change in gradient: $y_k = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n\nYour task is to calculate the vector $V$ that results from the matrix-vector product $V = H_{k+1} y_k$.\n\nPresent your final answer as a row matrix. For example, a column vector $\\begin{pmatrix} a \\\\ b \\end{pmatrix}$ should be written as $\\begin{pmatrix} a  b \\end{pmatrix}$.", "solution": "We use the BFGS inverse-Hessian update\n$$H_{k+1}=\\left(I-\\frac{s_{k}y_{k}^{T}}{y_{k}^{T}s_{k}}\\right)H_{k}\\left(I-\\frac{y_{k}s_{k}^{T}}{y_{k}^{T}s_{k}}\\right)+\\frac{s_{k}s_{k}^{T}}{y_{k}^{T}s_{k}},$$\nwith $H_{k}=I$, $s_{k}=\\begin{pmatrix}1\\\\3\\end{pmatrix}$, and $y_{k}=\\begin{pmatrix}2\\\\1\\end{pmatrix}$. First compute the scalar\n$$y_{k}^{T}s_{k}=2\\cdot 1+1\\cdot 3=5,$$\nso $\\rho=\\frac{1}{y_{k}^{T}s_{k}}=\\frac{1}{5}$. Next,\n$$s_{k}y_{k}^{T}=\\begin{pmatrix}1\\\\3\\end{pmatrix}\\begin{pmatrix}2  1\\end{pmatrix}=\\begin{pmatrix}2  1\\\\6  3\\end{pmatrix},\\quad\ny_{k}s_{k}^{T}=\\begin{pmatrix}2\\\\1\\end{pmatrix}\\begin{pmatrix}1  3\\end{pmatrix}=\\begin{pmatrix}2  6\\\\1  3\\end{pmatrix}.$$\nThus\n$$I-\\rho s_{k}y_{k}^{T}=\\begin{pmatrix}1  0\\\\0  1\\end{pmatrix}-\\frac{1}{5}\\begin{pmatrix}2  1\\\\6  3\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{5}  -\\frac{1}{5}\\\\ -\\frac{6}{5}  \\frac{2}{5}\\end{pmatrix},$$\n$$I-\\rho y_{k}s_{k}^{T}=\\begin{pmatrix}1  0\\\\0  1\\end{pmatrix}-\\frac{1}{5}\\begin{pmatrix}2  6\\\\1  3\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{5}  -\\frac{6}{5}\\\\ -\\frac{1}{5}  \\frac{2}{5}\\end{pmatrix}.$$\nWith $H_{k}=I$, we have\n$$H_{k+1}=\\left(I-\\rho s_{k}y_{k}^{T}\\right)\\left(I-\\rho y_{k}s_{k}^{T}\\right)+\\rho s_{k}s_{k}^{T}.$$\nCompute the product\n$$\\left(I-\\rho s_{k}y_{k}^{T}\\right)\\left(I-\\rho y_{k}s_{k}^{T}\\right)=\\begin{pmatrix}\\frac{3}{5}  -\\frac{1}{5}\\\\ -\\frac{6}{5}  \\frac{2}{5}\\end{pmatrix}\\begin{pmatrix}\\frac{3}{5}  -\\frac{6}{5}\\\\ -\\frac{1}{5}  \\frac{2}{5}\\end{pmatrix}=\\begin{pmatrix}\\frac{2}{5}  -\\frac{4}{5}\\\\ -\\frac{4}{5}  \\frac{8}{5}\\end{pmatrix}.$$\nAlso,\n$$\\rho s_{k}s_{k}^{T}=\\frac{1}{5}\\begin{pmatrix}1\\\\3\\end{pmatrix}\\begin{pmatrix}1  3\\end{pmatrix}=\\frac{1}{5}\\begin{pmatrix}1  3\\\\3  9\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{5}  \\frac{3}{5}\\\\ \\frac{3}{5}  \\frac{9}{5}\\end{pmatrix}.$$\nTherefore\n$$H_{k+1}=\\begin{pmatrix}\\frac{2}{5}  -\\frac{4}{5}\\\\ -\\frac{4}{5}  \\frac{8}{5}\\end{pmatrix}+\\begin{pmatrix}\\frac{1}{5}  \\frac{3}{5}\\\\ \\frac{3}{5}  \\frac{9}{5}\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{5}  -\\frac{1}{5}\\\\ -\\frac{1}{5}  \\frac{17}{5}\\end{pmatrix}.$$\nNow compute $V=H_{k+1}y_{k}$:\n$$V=\\begin{pmatrix}\\frac{3}{5}  -\\frac{1}{5}\\\\ -\\frac{1}{5}  \\frac{17}{5}\\end{pmatrix}\\begin{pmatrix}2\\\\1\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{5}\\cdot 2-\\frac{1}{5}\\cdot 1\\\\ -\\frac{1}{5}\\cdot 2+\\frac{17}{5}\\cdot 1\\end{pmatrix}=\\begin{pmatrix}1\\\\3\\end{pmatrix}.$$\nThis also reflects the secant condition $H_{k+1}y_{k}=s_{k}$ that BFGS enforces. Hence the requested row matrix is $\\begin{pmatrix}1  3\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1  3 \\end{pmatrix}}$$", "id": "2208659"}]}