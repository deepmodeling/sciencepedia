{"hands_on_practices": [{"introduction": "Embarking on an optimization journey with a quasi-Newton method begins with a single, crucial step: determining the initial search direction. This foundational exercise solidifies your understanding of how the process starts, using the gradient of the function and an initial approximation for the Hessian matrix. By working through this first calculation [@problem_id:2195903], you will see how the choice of the identity matrix for the initial Hessian approximation, $B_0=I$, simplifies the problem to taking a step in the direction of the negative gradient, linking quasi-Newton methods to the familiar concept of steepest descent.", "problem": "An iterative optimization algorithm is employed to find a local minimum of the two-variable function $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$. The algorithm is initialized at the point $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$. At the first step (iteration $k=0$), a search direction $p_0$ is computed by solving the linear system of equations $B_0 p_0 = -\\nabla f(x_0)$, where $\\nabla f(x_0)$ is the gradient of $f$ evaluated at $x_0$, and $B_0$ is an approximation of the Hessian matrix. For this procedure, the initial approximation is chosen as the $2 \\times 2$ identity matrix, $I$. Determine the components of the initial search direction vector $p_0$.", "solution": "The problem asks for the initial search direction vector $p_0$, which is the solution to the linear system $B_0 p_0 = -\\nabla f(x_0)$. We can solve this problem by following three main steps: first, compute the gradient of the function $f(x_1, x_2)$; second, evaluate this gradient at the initial point $x_0$; and third, solve the given linear system for $p_0$.\n\nStep 1: Compute the gradient of the function.\nThe function is given by $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$.\nThe gradient of $f$, denoted by $\\nabla f$, is a vector of its partial derivatives:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\nThe partial derivative with respect to $x_1$ is:\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\nThe partial derivative with respect to $x_2$ is:\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\nTherefore, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\nStep 2: Evaluate the gradient at the initial point $x_0$.\nThe initial point is given as $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$. We substitute these values into the gradient expression:\n$$\n\\nabla f(x_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\nWe evaluate each component. The cosine of 0 is:\n$$\n\\cos(0) = 1\n$$\nThe hyperbolic sine function is defined as $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$. For $y = \\ln(2)$:\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\nSo, the gradient vector at the initial point is:\n$$\n\\nabla f(x_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\nStep 3: Solve the linear system for $p_0$.\nThe system to solve is $B_0 p_0 = -\\nabla f(x_0)$. We are given that $B_0$ is the $2 \\times 2$ identity matrix, $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe system becomes:\n$$\nI p_0 = -\\nabla f(x_0)\n$$\nMultiplying any vector by the identity matrix leaves the vector unchanged, so $p_0 = -\\nabla f(x_0)$.\nSubstituting the value of the gradient we found:\n$$\np_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\nThus, the components of the initial search direction vector $p_0$ are $-1$ and $-\\frac{3}{4}$.", "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$", "id": "2195903"}, {"introduction": "Having taken the first step, we now turn to the engine of a quasi-Newton method: the iterative update of the Hessian approximation. This practice guides you through one complete cycle of the popular Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, from calculating the search direction to applying the rank-two update formula. By meticulously working through the calculations in this problem [@problem_id:2195901], you will gain a concrete understanding of how the algorithm uses gradient information to build a better quadratic model of the function, paving the way for more efficient subsequent steps.", "problem": "An optimization routine is used to find the minimum of a quadratic function $f(x)$ defined for $x \\in \\mathbb{R}^2$ as:\n$$f(x) = \\frac{1}{2} x^T A x - b^T x$$\nwhere the matrix $A$ and vector $b$ are given by:\n$$A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nThe routine employs a quasi-Newton method, which iteratively refines an approximation of the Hessian matrix. The method starts at an initial point $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with an initial Hessian approximation $B_0 = I$, where $I$ is the $2 \\times 2$ identity matrix.\n\nAt each iteration $k$, the search direction $p_k$ is computed by solving the linear system $B_k p_k = -\\nabla f(x_k)$. The next point is then found using a fixed unit step length, i.e., $x_{k+1} = x_k + p_k$.\n\nAfter determining the new point $x_{k+1}$, the Hessian approximation is updated from $B_k$ to $B_{k+1}$ using the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update formula:\n$$B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n\nYour task is to perform the first full iteration of this algorithm to determine the search direction for the second step, $p_1$. Calculate the value of the first component of the vector $p_1$. Round your final answer to four significant figures.", "solution": "We are given the quadratic function $f(x) = \\frac{1}{2} x^{T} A x - b^{T} x$ with $A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Its gradient is $\\nabla f(x) = A x - b$.\n\nAt the initial point $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and with $B_{0} = I$, compute the first search direction $p_{0}$ by solving\n$$\nB_{0} p_{0} = -\\nabla f(x_{0}).\n$$\nSince $\\nabla f(x_{0}) = A x_{0} - b = -b = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$ and $B_{0} = I$, we have\n$$\np_{0} = -\\nabla f(x_{0}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nWith unit step, the next point is\n$$\nx_{1} = x_{0} + p_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nDefine $s_{0} = x_{1} - x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Compute the gradients:\n$$\n\\nabla f(x_{1}) = A x_{1} - b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\nabla f(x_{0}) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\nThus\n$$\ny_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0}) = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nUpdate the Hessian approximation using the BFGS formula\n$$\nB_{1} = B_{0} - \\frac{B_{0} s_{0} s_{0}^{T} B_{0}}{s_{0}^{T} B_{0} s_{0}} + \\frac{y_{0} y_{0}^{T}}{y_{0}^{T} s_{0}}.\n$$\nWith $B_{0} = I$, $s_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{0} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, we have\n$$\ns_{0}^{T} s_{0} = 1, \\quad s_{0} s_{0}^{T} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad y_{0} y_{0}^{T} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 1 \\end{pmatrix}, \\quad y_{0}^{T} s_{0} = 3,\n$$\nso\n$$\nB_{1} = I - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 9 & 3 \\\\ 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & \\frac{4}{3} \\end{pmatrix}.\n$$\nThe search direction for the second step $p_{1}$ solves\n$$\nB_{1} p_{1} = -\\nabla f(x_{1}) = -\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n$$\nLet $p_{1} = \\begin{pmatrix} p \\\\ q \\end{pmatrix}$. Then\n$$\n\\begin{cases}\n3p + q = -2, \\\\\np + \\frac{4}{3} q = -1.\n\\end{cases}\n$$\nFrom the first equation $q = -2 - 3p$. Substitute into the second:\n$$\np + \\frac{4}{3}(-2 - 3p) = -1 \\;\\Rightarrow\\; p - \\frac{8}{3} - 4p = -1 \\;\\Rightarrow\\; -3p - \\frac{8}{3} = -1 \\;\\Rightarrow\\; -3p = \\frac{5}{3} \\;\\Rightarrow\\; p = -\\frac{5}{9}.\n$$\nTherefore, the first component of $p_{1}$ is $-\\frac{5}{9}$, which to four significant figures is $-0.5556$.", "answer": "$$\\boxed{-0.5556}$$", "id": "2195901"}, {"introduction": "The ultimate test of understanding is to build a working system from first principles. This final practice moves beyond manual calculations to the realm of computational implementation, challenging you to program a robust optimizer using the Symmetric Rank-One (SR1) method. By writing the code for this algorithm [@problem_id:2417336], you will directly engage with practical issues such as ensuring descent directions, handling numerical instabilities via update skipping, and globalizing the search with a line search, providing deep insight into the design of modern optimization software.", "problem": "You are asked to write a complete program that minimizes given twice continuously differentiable objective functions by iteratively approximating the Hessian matrix using a symmetric rank-one update. The method must maintain a symmetric matrix sequence $\\{B_k\\}_{k \\ge 0}$, build steps $\\{s_k\\}$ using directions computed from $B_k$, and apply a skipping criterion that avoids the rank-one update when the associated denominator is too small. The method should terminate when the gradient norm falls below a prescribed tolerance or when a maximum number of iterations is reached.\n\nLet $f:\\mathbb{R}^n \\to \\mathbb{R}$ be the objective function, let $\\nabla f(x)$ denote its gradient, and let $B_k \\in \\mathbb{R}^{n \\times n}$ be a symmetric approximation of the Hessian at iteration $k$. Given a current iterate $x_k \\in \\mathbb{R}^n$ with gradient $g_k = \\nabla f(x_k)$ and a direction $p_k$ that solves $B_k p_k = -g_k$, define the step $s_k = \\alpha_k p_k$ for some step length $\\alpha_k \\in (0,1]$, the next iterate $x_{k+1} = x_k + s_k$, and the gradient displacement $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. Define $u_k = y_k - B_k s_k$. The symmetric rank-one update is\n$$\nB_{k+1} = B_k + \\frac{u_k u_k^\\top}{s_k^\\top u_k}.\n$$\nIntroduce a skipping mechanism parameterized by a tolerance $\\tau > 0$ by applying the update only when\n$$\n|s_k^\\top u_k| > \\tau \\,\\|s_k\\|_2 \\,\\|u_k\\|_2,\n$$\nand otherwise set $B_{k+1} = B_k$ (skip the update). Start with $B_0 = I$.\n\nYour program must, for each test case below, produce the following outputs:\n- the total number of iterations $k_{\\text{end}}$ performed (an integer),\n- the final objective value $f(x_{k_{\\text{end}}})$ (a float),\n- the final gradient norm $\\|\\nabla f(x_{k_{\\text{end}}})\\|_2$ (a float),\n- the number of skipped updates (an integer),\n- the number of times the search direction $p_k$ failed to be a descent direction (that is, when $g_k^\\top p_k \\ge 0$) and a fallback direction was used (an integer).\n\nIf solving $B_k p_k = -g_k$ is not possible or produces a non-descent direction, you must fall back to the steepest descent direction $p_k = -g_k$ for that iteration. Use a backtracking line search that enforces the Armijo condition\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\, g_k^\\top p_k,\n$$\nwith fixed parameters $c_1 = 10^{-4}$ and reduction factor $\\beta = \\tfrac{1}{2}$, starting from $\\alpha_k = 1$ and reducing by factors of $\\beta$ until the condition holds or until $\\alpha_k$ becomes smaller than $10^{-16}$, in which case the step may be taken as is. Terminate when $\\|\\nabla f(x_k)\\|_2 \\le 10^{-6}$ or when $k$ reaches the maximum iteration budget.\n\nTest suite. Apply the method to each of the following six test cases. In all cases, use initial Hessian approximation $B_0 = I$, tolerance $\\|\\nabla f(x_k)\\|_2 \\le 10^{-6}$, and maximum iterations $200$.\n- Case A (happy path, nonconvex narrow valley):\n  - Objective: the Rosenbrock function in dimension $2$, \n    $$\n    f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n    $$\n  - Initial point: $x_0 = (-1.2,\\, 1.0)$.\n  - Skipping tolerance: $\\tau = 10^{-8}$.\n- Case B (same as Case A but more aggressive skipping):\n  - Objective: same as Case A.\n  - Initial point: $x_0 = (-1.2,\\, 1.0)$.\n  - Skipping tolerance: $\\tau = 10^{-2}$.\n- Case C (ill-conditioned strictly convex quadratic):\n  - Objective:\n    $$\n    f(x) = \\tfrac{1}{2}\\left(1\\cdot x_1^2 + 1000\\cdot x_2^2\\right).\n    $$\n  - Initial point: $x_0 = (1.0,\\, 1.0)$.\n  - Skipping tolerance: $\\tau = 10^{-8}$.\n- Case D (same as Case C but more aggressive skipping):\n  - Objective: same as Case C.\n  - Initial point: $x_0 = (1.0,\\, 1.0)$.\n  - Skipping tolerance: $\\tau = 10^{-2}$.\n- Case E (nonconvex with saddle structure):\n  - Objective:\n    $$\n    f(x) = x_1^4 - x_1^2 + x_2^2.\n    $$\n  - Initial point: $x_0 = (0.5,\\, 0.5)$.\n  - Skipping tolerance: $\\tau = 10^{-8}$.\n- Case F (same as Case E but more aggressive skipping):\n  - Objective: same as Case E.\n  - Initial point: $x_0 = (0.5,\\, 0.5)$.\n  - Skipping tolerance: $\\tau = 10^{-2}$.\n\nAngle units are not involved. No physical units appear. Your program must implement the above mathematical specifications faithfully.\n\nFinal output format. Your program should produce a single line of output containing a list of six lists, each inner list corresponding to a test case in the order A through F, with the structure\n$$\n[\\;k_{\\text{end}},\\; f(x_{k_{\\text{end}}}),\\; \\|\\nabla f(x_{k_{\\text{end}}})\\|_2,\\; \\text{skipped},\\; \\text{fallback}\\;],\n$$\nwhere the two floating-point values must be rounded to exactly six decimal places. For example, the final printout must look like\n$$\n\\big[\\,[k_1, f_1, g_1, s_1, b_1],\\; [k_2, f_2, g_2, s_2, b_2],\\; \\dots,\\; [k_6, f_6, g_6, s_6, b_6]\\,\\big].\n$$", "solution": "We formalize the iterative procedure by combining the symmetric rank-one update with a robust step computation and a sufficient decrease line search. The goal is to examine how the skipping threshold parameter influences stability and speed, using a consistent termination and line search policy across test problems.\n\nPrinciples. Consider an objective function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ with gradient $\\nabla f(x)$. Newton’s method uses the Hessian $\\nabla^2 f(x_k)$ to define a local quadratic model and the step solves $\\nabla^2 f(x_k) p_k = -\\nabla f(x_k)$. Quasi-Newton methods replace the exact Hessian with an approximation $B_k$ that is updated using information from gradients differences to satisfy a secant-like condition. The Symmetric Rank-One (SR1) update is derived by imposing symmetry, minimal change in the Frobenius norm, and the secant condition along $s_k$, namely $B_{k+1} s_k = y_k$, where $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. The SR1 update that enforces this condition is\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k) (y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k},\n$$\nwhenever the denominator is nonzero. Define $u_k = y_k - B_k s_k$, and denote the denominator by $d_k = s_k^\\top u_k$.\n\nStability mechanism. When $|d_k|$ is small, the SR1 correction becomes very large in norm, which can inject severe instability (large changes in curvature estimates, loss of descent, and potential numerical singularity). A common safeguard is to skip the update when $|d_k|$ is small relative to the product of the norms of $s_k$ and $u_k$. We adopt the criterion\n$$\n|s_k^\\top u_k| > \\tau \\,\\|s_k\\|_2 \\,\\|u_k\\|_2,\n$$\nwith $\\tau > 0$. If this holds, we apply the rank-one update; otherwise we set $B_{k+1} = B_k$ (skipping).\n\nDirection computation and fallback. At each iteration, we seek $p_k$ by solving $B_k p_k = -g_k$ with $g_k = \\nabla f(x_k)$. Since SR1 does not preserve positive definiteness, $B_k$ can be indefinite or singular. If a linear solve fails or if $g_k^\\top p_k \\ge 0$ (not a descent direction), we fall back to the steepest descent direction $p_k = -g_k$. This ensures we always have a viable direction satisfying $g_k^\\top p_k \\le 0$.\n\nGlobalization by sufficient decrease. To ensure stability and decrease in the objective, we use backtracking with the Armijo condition:\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\, g_k^\\top p_k,\n$$\nwith fixed $c_1 = 10^{-4}$ and reduction factor $\\beta = \\tfrac{1}{2}$. We start from $\\alpha = 1$ and reduce $\\alpha \\leftarrow \\beta \\alpha$ until the condition holds or $\\alpha$ becomes extremely small (below $10^{-16}$). The chosen $\\alpha_k$ defines $s_k = \\alpha_k p_k$ and yields $x_{k+1} = x_k + s_k$.\n\nTermination and metrics. We terminate when $\\|\\nabla f(x_k)\\|_2 \\le 10^{-6}$ or when $k$ reaches the maximum iteration budget ($200$). For each test, we return:\n- the number of iterations $k_{\\text{end}}$,\n- $f(x_{k_{\\text{end}}})$,\n- $\\|\\nabla f(x_{k_{\\text{end}}})\\|_2$,\n- the count of skipped SR1 updates, and\n- the count of fallback uses for non-descent or failed solves.\n\nTest suite and coverage. We use six cases:\n- Rosenbrock in dimension $2$ from $x_0 = (-1.2, 1.0)$ with $\\tau = 10^{-8}$ and $\\tau = 10^{-2}$. This probes a nonconvex valley, testing curvature learning and the impact of skipping on speed.\n- Ill-conditioned convex quadratic $f(x) = \\tfrac{1}{2} (x_1^2 + 1000 x_2^2)$ from $x_0 = (1,1)$ with $\\tau = 10^{-8}$ and $\\tau = 10^{-2}$. This tests sensitivity to conditioning and the benefit of curvature updates versus skipping.\n- Nonconvex quartic $f(x) = x_1^4 - x_1^2 + x_2^2$ from $x_0 = (0.5, 0.5)$ with $\\tau = 10^{-8}$ and $\\tau = 10^{-2}$, including a saddle direction, probing descent fallback and stability.\n\nTrade-off analysis. With smaller $\\tau$ (for example, $\\tau = 10^{-8}$), the method applies the SR1 correction more often. This tends to accelerate convergence because curvature information is updated frequently, but it also increases the risk of large corrections when $|s_k^\\top u_k|$ is small yet not below the threshold, potentially causing indefiniteness or unstable steps, which we mitigate via fallback and line search. With larger $\\tau$ (for example, $\\tau = 10^{-2}$), the method skips more updates. This stabilizes the sequence $\\{B_k\\}$ by avoiding corrections deemed ill-conditioned, but slows curvature learning and can increase the number of iterations or the reliance on steepest descent behavior. The program quantifies this trade-off via the iteration counts, final gradient norms, the number of skipped updates, and the number of fallback events, allowing direct comparison between $\\tau = 10^{-8}$ and $\\tau = 10^{-2}$ across the three objectives.\n\nCorrectness and first principles. The method follows directly from:\n- the secant condition $B_{k+1} s_k = y_k$,\n- the SR1 minimal-change derivation,\n- Armijo’s sufficient decrease rule, and\n- the definition of a descent direction via $g_k^\\top p_k < 0$.\nNo problem-specific heuristics are used beyond the norm-relative skipping threshold, and all quantities are computed from the definitions of $f$, $\\nabla f$, and the update rule.", "answer": "```python\nimport numpy as np\n\ndef rosenbrock(x: np.ndarray) -> float:\n    # f(x1, x2) = 100 (x2 - x1^2)^2 + (1 - x1)^2\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef rosenbrock_grad(x: np.ndarray) -> np.ndarray:\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * (x2 - x1**2) * x1 - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2], dtype=float)\n\ndef quad_ic(x: np.ndarray) -> float:\n    # f(x) = 1/2 (1*x1^2 + 1000*x2^2)\n    x1, x2 = x[0], x[1]\n    return 0.5 * (1.0 * x1**2 + 1000.0 * x2**2)\n\ndef quad_ic_grad(x: np.ndarray) -> np.ndarray:\n    x1, x2 = x[0], x[1]\n    return np.array([1.0 * x1, 1000.0 * x2], dtype=float)\n\ndef quartic_saddle(x: np.ndarray) -> float:\n    # f(x) = x1^4 - x1^2 + x2^2\n    x1, x2 = x[0], x[1]\n    return x1**4 - x1**2 + x2**2\n\ndef quartic_saddle_grad(x: np.ndarray) -> np.ndarray:\n    x1, x2 = x[0], x[1]\n    return np.array([4.0 * x1**3 - 2.0 * x1, 2.0 * x2], dtype=float)\n\ndef armijo_backtracking(f, grad, x, p, gTp, c1=1e-4, beta=0.5, alpha0=1.0, min_alpha=1e-16):\n    alpha = alpha0\n    fx = f(x)\n    while True:\n        xn = x + alpha * p\n        if f(xn) <= fx + c1 * alpha * gTp:\n            break\n        alpha *= beta\n        if alpha < min_alpha:\n            break\n    return alpha\n\ndef sr1_optimize(f, grad, x0, tau, max_iter=200, tol=1e-6):\n    n = len(x0)\n    x = x0.copy()\n    B = np.eye(n)\n    skipped_updates = 0\n    fallback_count = 0\n    k = 0\n\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm <= tol:\n            break\n\n        # Attempt to solve B p = -g\n        descent_ok = False\n        try:\n            p = np.linalg.solve(B, -g)\n            gTp = float(np.dot(g, p))\n            if gTp < 0.0 and np.all(np.isfinite(p)):\n                descent_ok = True\n        except np.linalg.LinAlgError:\n            descent_ok = False\n\n        if not descent_ok:\n            p = -g\n            gTp = -float(np.dot(g, g))\n            fallback_count += 1\n\n        # Armijo backtracking line search\n        alpha = armijo_backtracking(f, grad, x, p, gTp, c1=1e-4, beta=0.5, alpha0=1.0, min_alpha=1e-16)\n\n        s = alpha * p\n        x_new = x + s\n        g_new = grad(x_new)\n        y = g_new - g\n\n        # SR1 update with skipping\n        u = y - B.dot(s)\n        denom = float(np.dot(s, u))\n        norm_s = np.linalg.norm(s)\n        norm_u = np.linalg.norm(u)\n\n        # Apply skip criterion only if norms are finite and nonzero\n        apply_update = True\n        if norm_s == 0.0 or not np.isfinite(norm_s) or not np.isfinite(norm_u):\n            apply_update = False\n        else:\n            if abs(denom) <= tau * norm_s * norm_u:\n                apply_update = False\n\n        if apply_update:\n            # Rank-one update\n            B = B + np.outer(u, u) / denom\n        else:\n            skipped_updates += 1\n\n        x = x_new\n\n    # Final metrics\n    final_f = float(f(x))\n    final_gnorm = float(np.linalg.norm(grad(x)))\n    return k, final_f, final_gnorm, skipped_updates, fallback_count\n\ndef solve():\n    # Define test cases A-F\n    test_cases = [\n        # (name, f, grad, x0, tau)\n        (\"A_rosen_tau1e-8\", rosenbrock, rosenbrock_grad, np.array([-1.2, 1.0], dtype=float), 1e-8),\n        (\"B_rosen_tau1e-2\", rosenbrock, rosenbrock_grad, np.array([-1.2, 1.0], dtype=float), 1e-2),\n        (\"C_quad_tau1e-8\", quad_ic, quad_ic_grad, np.array([1.0, 1.0], dtype=float), 1e-8),\n        (\"D_quad_tau1e-2\", quad_ic, quad_ic_grad, np.array([1.0, 1.0], dtype=float), 1e-2),\n        (\"E_quartic_tau1e-8\", quartic_saddle, quartic_saddle_grad, np.array([0.5, 0.5], dtype=float), 1e-8),\n        (\"F_quartic_tau1e-2\", quartic_saddle, quartic_saddle_grad, np.array([0.5, 0.5], dtype=float), 1e-2),\n    ]\n\n    results = []\n    for _, f, grad, x0, tau in test_cases:\n        k, fval, gnorm, skipped, fallback = sr1_optimize(f, grad, x0, tau, max_iter=200, tol=1e-6)\n        # Round floats to exactly six decimals as required\n        results.append([int(k), float(f\"{fval:.6f}\"), float(f\"{gnorm:.6f}\"), int(skipped), int(fallback)])\n\n    # Print in the exact required format: a single line with the list of lists\n    # Ensure no extra text\n    def format_inner(lst):\n        # lst: [k, f, g, s, b]\n        return f\"[{lst[0]},{lst[1]:.6f},{lst[2]:.6f},{lst[3]},{lst[4]}]\"\n    print(f\"[{','.join(format_inner(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2417336"}]}