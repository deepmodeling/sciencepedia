{"hands_on_practices": [{"introduction": "Newton's method is celebrated for its quadratic convergence, which promises a rapid approach to the solution. However, this impressive speed is not unconditional. This exercise explores a scenario where this core assumption fails, revealing a fundamental limitation of the method. You will investigate the optimization of a function whose minimum is \"flat,\" meaning the second derivative $f''(x)$ is zero at the solution, and discover how this degeneracy slows the convergence from quadratic to merely linear [@problem_id:2167211].", "problem": "An optimization algorithm is used to find the value of $x$ that minimizes the function $f(x) = \\alpha x^4$, where $\\alpha$ is a positive constant. The algorithm uses Newton's method for optimization, which generates a sequence of estimates $\\{x_k\\}$ for $k=0, 1, 2, \\dots$ starting from an initial guess $x_0$. The update rule is given by:\n$$x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}$$\nwhere $f'(x)$ and $f''(x)$ are the first and second derivatives of $f(x)$ with respect to $x$.\n\nAssume the initial guess $x_0$ is not equal to zero. The algorithm is known to converge to the minimum at $x=0$. To characterize the speed of this convergence, determine the exact value of the limit:\n$$L = \\lim_{k \\to \\infty} \\frac{|x_{k+1}|}{|x_k|}$$\nYour final answer should be provided as an exact fraction.", "solution": "We are given the function $f(x) = \\alpha x^{4}$ with $\\alpha > 0$. Its first and second derivatives are computed using standard differentiation rules:\n$$f'(x) = 4\\alpha x^{3}, \\quad f''(x) = 12\\alpha x^{2}.$$\nNewton's method for optimization updates $x_{k}$ by\n$$x_{k+1} = x_{k} - \\frac{f'(x_{k})}{f''(x_{k})}.$$\nSubstituting the derivatives into the update rule gives, for $x_{k} \\neq 0$,\n$$x_{k+1} = x_{k} - \\frac{4\\alpha x_{k}^{3}}{12\\alpha x_{k}^{2}} = x_{k} - \\frac{1}{3}x_{k} = \\frac{2}{3}x_{k}.$$\nTherefore, at each iteration,\n$$\\frac{|x_{k+1}|}{|x_{k}|} = \\left|\\frac{2}{3}\\right| = \\frac{2}{3}.$$\nThis quantity is constant for all $k$ (as long as $x_{k} \\neq 0$), and the sequence converges linearly to $x=0$ with rate $\\frac{2}{3}$. Hence the limit is\n$$L = \\lim_{k \\to \\infty} \\frac{|x_{k+1}|}{|x_{k}|} = \\frac{2}{3}.$$", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "2167211"}, {"introduction": "Beyond a reduction in speed, a more dramatic failure of Newton's method is outright divergence. The method's guarantee of convergence is local, meaning it is only assured if the initial guess is \"sufficiently close\" to the true minimum. This exercise provides a striking hands-on example where, for a simple convex function, choosing a starting point just beyond a certain threshold causes the iterates to move catastrophically away from the solution, demonstrating the critical importance of the initial guess [@problem_id:2167231].", "problem": "In a computational optics design tool, an algorithm seeks to find the position $x$ on a sensor that minimizes a cost function related to optical aberrations. A simplified model for this cost is given by the function $f(x) = \\sqrt{L^2 + x^2}$, where $L$ is a fixed positive characteristic length of the optical system.\n\nThe algorithm uses Newton's method for optimization to find the minimum of $f(x)$. This method starts with an initial guess $x_0$ and generates a sequence of estimates using the iterative formula:\n$$x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}$$\nwhere $f'(x)$ and $f''(x)$ are the first and second derivatives of $f(x)$ with respect to $x$.\n\nA test is run with an initial guess of $x_0 = \\sqrt{2}L$. Calculate the fourth estimate, $x_4$, generated by this algorithm. Express your answer in terms of $L$.", "solution": "We are given $f(x) = \\sqrt{L^{2} + x^{2}}$ with $L>0$ and Newtonâ€™s method for optimization\n$$\nx_{n+1} = x_{n} - \\frac{f'(x_{n})}{f''(x_{n})}.\n$$\nFirst compute derivatives:\n$$\nf'(x) = \\frac{x}{\\sqrt{L^{2} + x^{2}}},\n$$\n$$\nf''(x) = \\frac{d}{dx}\\left(x\\left(L^{2}+x^{2}\\right)^{-\\frac{1}{2}}\\right) = \\left(L^{2}+x^{2}\\right)^{-\\frac{1}{2}} - x^{2}\\left(L^{2}+x^{2}\\right)^{-\\frac{3}{2}} = \\frac{L^{2}}{\\left(L^{2}+x^{2}\\right)^{\\frac{3}{2}}}.\n$$\nThus\n$$\n\\frac{f'(x)}{f''(x)} = \\frac{\\frac{x}{\\sqrt{L^{2}+x^{2}}}}{\\frac{L^{2}}{(L^{2}+x^{2})^{\\frac{3}{2}}}} = x\\cdot\\frac{L^{2}+x^{2}}{L^{2}}.\n$$\nTherefore the Newton update simplifies to\n$$\nx_{n+1} = x_{n} - x_{n}\\frac{L^{2}+x_{n}^{2}}{L^{2}} = -\\frac{x_{n}^{3}}{L^{2}}.\n$$\nStarting from $x_{0} = \\sqrt{2}\\,L$, iterate:\n$$\nx_{1} = -\\frac{x_{0}^{3}}{L^{2}} = -\\frac{\\left(\\sqrt{2}\\,L\\right)^{3}}{L^{2}} = -2^{\\frac{3}{2}}L,\n$$\n$$\nx_{2} = -\\frac{x_{1}^{3}}{L^{2}} = -\\frac{\\left(-2^{\\frac{3}{2}}L\\right)^{3}}{L^{2}} = 2^{\\frac{9}{2}}L,\n$$\n$$\nx_{3} = -\\frac{x_{2}^{3}}{L^{2}} = -\\frac{\\left(2^{\\frac{9}{2}}L\\right)^{3}}{L^{2}} = -2^{\\frac{27}{2}}L,\n$$\n$$\nx_{4} = -\\frac{x_{3}^{3}}{L^{2}} = -\\frac{\\left(-2^{\\frac{27}{2}}L\\right)^{3}}{L^{2}} = 2^{\\frac{81}{2}}L.\n$$\nThus the fourth estimate is $x_{4} = 2^{\\frac{81}{2}}L$.", "answer": "$$\\boxed{2^{\\frac{81}{2}}L}$$", "id": "2167231"}, {"introduction": "Moving from theoretical limitations to practical realities, we consider the impact of finite-precision computer arithmetic. In theory, Newton's method finds the minimum of a simple quadratic function in a single step. This problem [@problem_id:2167170] simulates a computer with limited precision to demonstrate a subtle but crucial drawback: due to rounding or truncation, the algorithm may stall in a \"stalling interval\" around the true minimum, where the machine-computed gradient becomes zero, thus halting progress prematurely.", "problem": "In theory, Newton's method for optimization is remarkably efficient for quadratic functions. For a general one-dimensional quadratic function $f(x) = \\frac{1}{2}ax^2 + bx + c$ (with $a>0$), the method converges to the exact minimizer in a single iterative step, regardless of the starting point.\n\nHowever, in practice, the finite precision of computer arithmetic can introduce artifacts that prevent the algorithm from performing as expected. Consider the task of minimizing the function $g(x) = \\frac{1}{2}x^2 - (\\ln 10) x$ using a hypothetical computer. The true minimum of this function occurs at $x^* = \\ln 10$.\n\nThis computer has the following specifications: it represents all numbers and performs all arithmetic using a base-10 floating-point system that maintains 5 significant figures. To handle numbers that require more than 5 significant figures, the computer uses 'chopping' (i.e., it simply truncates any digits beyond the fifth significant figure). For this problem, use the approximation $\\ln 10 \\approx 2.30258509...$.\n\nThe optimization program, an implementation of Newton's method, is designed to stop when the machine-computed value of the gradient, $g'(x)$, is exactly zero. Due to the chopping behavior, there exists a continuous interval of real numbers $I = [x_{\\text{low}}, x_{\\text{high}})$ such that for any point $x \\in I$, the floating-point representation of $x$ is identical to the floating-point representation of the true minimum, $x^*$. This causes the machine-computed gradient to be zero for any $x$ in this interval, stalling the algorithm prematurely.\n\nCalculate the width of this \"stalling interval\", defined as $x_{\\text{high}} - x_{\\text{low}}$. Provide your answer as a single numerical value.", "solution": "We consider the function $g(x) = \\frac{1}{2}x^{2} - (\\ln 10)\\,x$ with derivative $g'(x) = x - \\ln 10$. On the given computer, all numbers are represented in base-10 floating-point with $t=5$ significant digits using chopping. Let $\\operatorname{fl}(\\cdot)$ denote the floating-point mapping by chopping to $t=5$ significant digits.\n\nThe optimization program stops when the machine-computed gradient is exactly zero. With floating-point operations, $g'(x)$ is computed as $\\operatorname{fl}(\\operatorname{fl}(x) - \\operatorname{fl}(\\ln 10))$. In base-10 floating-point without exponent limits and with chopping, the only way this equals exactly zero is when the two operands of the subtraction are exactly equal in floating-point, that is,\n$$\n\\operatorname{fl}(x) = \\operatorname{fl}(\\ln 10).\n$$\n\nTherefore, the stalling interval $I$ is precisely the preimage under $\\operatorname{fl}$ of the single floating-point number $\\operatorname{fl}(\\ln 10)$. For a positive normalized floating-point number $y$ with $t$ significant digits in base $10$, written as $y = m \\times 10^{e}$ with $1 \\le m  10$, the set of real numbers that chop to $y$ is the half-open interval\n$$\n[y, y + \\operatorname{ulp}(y)),\n$$\nwhere the spacing to the next representable number is\n$$\n\\operatorname{ulp}(y) = 10^{e - (t-1)}.\n$$\nThis follows because increasing the last (the $t$-th) significant digit of $m$ by one changes $y$ by $10^{e-(t-1)}$.\n\nWe compute $\\operatorname{fl}(\\ln 10)$ using $t=5$ digits and chopping. Since $\\ln 10 \\approx 2.30258509\\ldots$ and chopping to $5$ significant digits yields\n$$\n\\operatorname{fl}(\\ln 10) = 2.3025 \\times 10^{0},\n$$\nwe have $e=0$. Hence,\n$$\n\\operatorname{ulp}(\\operatorname{fl}(\\ln 10)) = 10^{0 - (5-1)} = 10^{-4}.\n$$\nTherefore,\n$$\nI = [2.3025, 2.3025 + 10^{-4})\n= [2.3025, 2.3026),\n$$\nand the width of the stalling interval is\n$$x_{\\text{high}} - x_{\\text{low}} = 10^{-4}.$$", "answer": "$$\\boxed{1 \\times 10^{-4}}$$", "id": "2167170"}]}