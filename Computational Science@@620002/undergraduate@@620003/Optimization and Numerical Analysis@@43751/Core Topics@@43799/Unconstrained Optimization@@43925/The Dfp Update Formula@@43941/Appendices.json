{"hands_on_practices": [{"introduction": "Before applying the full Davidon-Fletcher-Powell (DFP) update formula, we must first compute its fundamental building blocks. This exercise focuses on calculating the step vector $s_k$ and the gradient-change vector $y_k$, which are derived from two successive points in an optimization process. Mastering this initial step is crucial, as these vectors provide the essential information about the function's local curvature that the DFP algorithm uses to build its Hessian approximation [@problem_id:2212535].", "problem": "In the context of quasi-Newton optimization methods, such as the Davidon-Fletcher-Powell (DFP) algorithm, the approximation of the Hessian matrix is updated at each iteration. This update relies on two key vectors, denoted as $s_k$ and $y_k$. The vector $s_k$ represents the step taken in the variable space, and the vector $y_k$ represents the corresponding change in the gradient.\n\nConsider the unconstrained optimization of the function $f(x_1, x_2) = 2x_1^2 + x_2^2 + x_1 x_2$.\nSuppose we are at the $k$-th iteration, with the current point being $x_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The next point in the sequence, found by some line search procedure, is $x_{k+1} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}$.\n\nThe vectors required for the DFP update are defined as $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n\nWhich of the following options correctly represents the vectors $s_k$ and $y_k$ for this iteration?\n\nA. $s_k = \\begin{pmatrix} -0.5 \\\\ 1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} -1 \\\\ 1.5 \\end{pmatrix}$\n\nB. $s_k = \\begin{pmatrix} 0.5 \\\\ -1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} 1 \\\\ -1.5 \\end{pmatrix}$\n\nC. $s_k = \\begin{pmatrix} 1.5 \\\\ 1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} 7 \\\\ 3.5 \\end{pmatrix}$\n\nD. $s_k = \\begin{pmatrix} -1 \\\\ 1.5 \\end{pmatrix}$, $y_k = \\begin{pmatrix} -0.5 \\\\ 1 \\end{pmatrix}$\n\nE. $s_k = \\begin{pmatrix} -0.5 \\\\ 1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} 7 \\\\ 3.5 \\end{pmatrix}$", "solution": "We are given the function $f(x_1, x_2)=2x_1^2+x_2^2+x_1 x_2$. The gradient is obtained by differentiating componentwise:\n$$\n\\nabla f(x_1, x_2)=\\begin{pmatrix}\\frac{\\partial f}{\\partial x_1}\\\\\\\\ \\frac{\\partial f}{\\partial x_2}\\end{pmatrix}\n=\\begin{pmatrix}4x_1+x_2\\\\\\\\ 2x_2+x_1\\end{pmatrix}.\n$$\nBy definition, the displacement vector is $s_k=x_{k+1}-x_k$. With $x_k=\\begin{pmatrix}1\\\\\\\\0\\end{pmatrix}$ and $x_{k+1}=\\begin{pmatrix}\\frac{1}{2}\\\\\\\\1\\end{pmatrix}$, we get\n$$\ns_k=\\begin{pmatrix}\\frac{1}{2}-1\\\\\\\\1-0\\end{pmatrix}=\\begin{pmatrix}-\\frac{1}{2}\\\\\\\\1\\end{pmatrix}.\n$$\nNext, the gradient change is $y_k=\\nabla f(x_{k+1})-\\nabla f(x_k)$. Evaluate the gradient at each point:\n$$\n\\nabla f(x_k)=\\nabla f(1,0)=\\begin{pmatrix}4\\cdot 1+0\\\\\\\\2\\cdot 0+1\\end{pmatrix}=\\begin{pmatrix}4\\\\\\\\1\\end{pmatrix},\n$$\n$$\n\\nabla f(x_{k+1})=\\nabla f\\!\\left(\\frac{1}{2},1\\right)=\\begin{pmatrix}4\\cdot \\frac{1}{2}+1\\\\\\\\2\\cdot 1+\\frac{1}{2}\\end{pmatrix}=\\begin{pmatrix}3\\\\\\\\\\frac{5}{2}\\end{pmatrix}.\n$$\nTherefore,\n$$\ny_k=\\begin{pmatrix}3-4\\\\\\\\\\frac{5}{2}-1\\end{pmatrix}=\\begin{pmatrix}-1\\\\\\\\\\frac{3}{2}\\end{pmatrix}.\n$$\nComparing with the options, this corresponds to $s_k=\\begin{pmatrix}-\\frac{1}{2}\\\\\\\\1\\end{pmatrix}$ and $y_k=\\begin{pmatrix}-1\\\\\\\\\\frac{3}{2}\\end{pmatrix}$, which matches the numerical values in option A.", "answer": "$$\\boxed{A}$$", "id": "2212535"}, {"introduction": "With an understanding of how to generate the core vectors $s_k$ and $y_k$, we can now tackle the DFP update itself. This practice guides you through a complete update of the inverse Hessian approximation, $H_k$, for a single iteration. Starting with the identity matrix $H_0 = I$, a common and simple initial guess, you will apply the full DFP formula to see how the approximation evolves [@problem_id:2212500]. This exercise is key to understanding the mechanics of the algorithm and the matrix algebra involved.", "problem": "In the field of numerical optimization, quasi-Newton methods are iterative algorithms used to find local minima or maxima of functions. A key feature of these methods is the approximation of the Hessian matrix or its inverse. One of the earliest and most famous quasi-Newton algorithms is the Davidon-Fletcher-Powell (DFP) method.\n\nThe DFP update formula for approximating the inverse of the Hessian matrix, denoted by $H$, is given by:\n$$H_{k+1} = H_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}$$\nwhere $k$ is the iteration index, $s_k = x_{k+1} - x_k$ is the step taken in the parameter space, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient of the objective function $f(x)$.\n\nConsider the first step ($k=0$) of an optimization process. The initial approximation of the inverse Hessian matrix is set to the identity matrix, $H_0 = I$. After the first line search, the step vector $s_0$ and the change-in-gradient vector $y_0$ are computed to be:\n$$s_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad y_0 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$$\n\nYour task is to calculate the updated inverse Hessian approximation, $H_1$, after this first step. Express your answer as a matrix with fractional entries.", "solution": "We use the DFP update for the inverse Hessian approximation:\n$$H_{k+1} = H_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}.$$\nAt the first step with $k=0$, $H_0=I$, $s_0=\\begin{pmatrix}1 \\\\ -2\\end{pmatrix}$, and $y_0=\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$. Since $H_0=I$, we have $H_0 y_0=y_0$ and $y_0^T H_0 y_0=y_0^T y_0$. Therefore,\n$$H_1 = I + \\frac{s_0 s_0^T}{s_0^T y_0} - \\frac{y_0 y_0^T}{y_0^T y_0}.$$\nCompute the required inner products:\n$$s_0^T y_0 = 1\\cdot 2 + (-2)\\cdot(-1) = 4,$$\n$$y_0^T y_0 = 2^2 + (-1)^2 = 5.$$\nCompute the outer products:\n$$s_0 s_0^T = \\begin{pmatrix}1 & -2 \\\\ -2 & 4\\end{pmatrix}, \\quad y_0 y_0^T = \\begin{pmatrix}4 & -2 \\\\ -2 & 1\\end{pmatrix}.$$\nThus,\n$$\\frac{s_0 s_0^T}{s_0^T y_0} = \\frac{1}{4}\\begin{pmatrix}1 & -2 \\\\ -2 & 4\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{4} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 1\\end{pmatrix}, \\quad \\frac{y_0 y_0^T}{y_0^T y_0} = \\frac{1}{5}\\begin{pmatrix}4 & -2 \\\\ -2 & 1\\end{pmatrix} = \\begin{pmatrix}\\frac{4}{5} & -\\frac{2}{5} \\\\ -\\frac{2}{5} & \\frac{1}{5}\\end{pmatrix}.$$\nCombine terms:\n$$H_1 = I + \\begin{pmatrix}\\frac{1}{4} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 1\\end{pmatrix} - \\begin{pmatrix}\\frac{4}{5} & -\\frac{2}{5} \\\\ -\\frac{2}{5} & \\frac{1}{5}\\end{pmatrix}.$$\nCompute each entry:\n$$(1,1): 1 + \\frac{1}{4} - \\frac{4}{5} = \\frac{5}{4} - \\frac{4}{5} = \\frac{25 - 16}{20} = \\frac{9}{20},$$\n$$(1,2) \\text{ and } (2,1): 0 - \\frac{1}{2} - \\left(-\\frac{2}{5}\\right) = -\\frac{1}{2} + \\frac{2}{5} = -\\frac{1}{10},$$\n$$(2,2): 1 + 1 - \\frac{1}{5} = 2 - \\frac{1}{5} = \\frac{9}{5}.$$\nTherefore,\n$$H_1 = \\begin{pmatrix}\\frac{9}{20} & -\\frac{1}{10} \\\\ -\\frac{1}{10} & \\frac{9}{5}\\end{pmatrix}.$$\nAs a check, the secant condition $H_1 y_0 = s_0$ holds:\n$$\\begin{pmatrix}\\frac{9}{20} & -\\frac{1}{10} \\\\ -\\frac{1}{10} & \\frac{9}{5}\\end{pmatrix}\\begin{pmatrix}2 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = s_0.$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{9}{20} & -\\frac{1}{10} \\\\ -\\frac{1}{10} & \\frac{9}{5}\\end{pmatrix}}$$", "id": "2212500"}, {"introduction": "While the DFP method effectively approximates the inverse Hessian, it is known to sometimes suffer from numerical instability. This final practice explores this important practical issue by examining the condition number of the updated matrix $H_{k+1}$. You will see how, even with a perfectly well-conditioned starting matrix, a single DFP update can produce an ill-conditioned result, which can slow down or stall the optimization algorithm [@problem_id:2212484]. This analysis highlights a key reason why the closely related BFGS update is often preferred in modern software.", "problem": "In the context of quasi-Newton methods for unconstrained optimization, a class of algorithms is used to find local minima of functions. One such method uses the Davidon-Fletcher-Powell (DFP) formula to update an approximation of the inverse Hessian matrix at each iteration. Let this approximation at step $k$ be denoted by the matrix $H_k$.\n\nThe DFP update formula is given by:\n$$H_{k+1} = H_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}$$\nwhere $s_k = x_{k+1} - x_k$ is the step taken from iterate $x_k$ to $x_{k+1}$, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the corresponding change in the gradient of the objective function $f$.\n\nConsider a specific step in an optimization process in $\\mathbb{R}^2$ where the current inverse Hessian approximation is the identity matrix, $H_k = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. The step vector and the change in gradient for this iteration are observed to be $s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $y_k = \\begin{pmatrix} 0.01 \\\\ 1 \\end{pmatrix}$, respectively.\n\nYour task is to calculate the condition number of the updated inverse Hessian approximation, $H_{k+1}$. The condition number of a symmetric positive definite matrix $H$ is defined as the ratio of its largest eigenvalue to its smallest eigenvalue, $\\kappa(H) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$.\n\nProvide your answer as a single real number, rounded to three significant figures.", "solution": "We use the DFP inverse-Hessian update\n$$H_{k+1} = H_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}.$$\nWith $H_k = I$, this simplifies to\n$$H_{k+1} = I + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{y_k y_k^T}{y_k^T y_k}.$$\nHere $s_k = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $y_k = \\begin{pmatrix}0.01 \\\\ 1\\end{pmatrix}$, so\n$$s_k^T y_k = 0.01,\\qquad y_k^T y_k = 0.01^2 + 1^2 = 1.0001 = \\frac{10001}{10000}.$$\nAlso,\n$$s_k s_k^T = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix},\\qquad y_k y_k^T = \\begin{pmatrix}0.0001 & 0.01 \\\\ 0.01 & 1\\end{pmatrix}.$$\nTherefore\n$$\\frac{s_k s_k^T}{s_k^T y_k} = \\begin{pmatrix}100 & 0 \\\\ 0 & 0\\end{pmatrix},\\qquad\n\\frac{y_k y_k^T}{y_k^T y_k} = \\frac{10000}{10001}\\begin{pmatrix}0.0001 & 0.01 \\\\ 0.01 & 1\\end{pmatrix}\n= \\begin{pmatrix}\\frac{1}{10001} & \\frac{100}{10001} \\\\ \\frac{100}{10001} & \\frac{10000}{10001}\\end{pmatrix}.$$\nHence\n$$H_{k+1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} + \\begin{pmatrix}100 & 0 \\\\ 0 & 0\\end{pmatrix} - \\begin{pmatrix}\\frac{1}{10001} & \\frac{100}{10001} \\\\ \\frac{100}{10001} & \\frac{10000}{10001}\\end{pmatrix}\n= \\begin{pmatrix}101 - \\frac{1}{10001} & -\\frac{100}{10001} \\\\\\\\ -\\frac{100}{10001} & 1 - \\frac{10000}{10001}\\end{pmatrix}.$$\nEquivalently,\n$$H_{k+1} = \\frac{1}{10001}\\begin{pmatrix}1010100 & -100 \\\\ -100 & 1\\end{pmatrix}.$$\nSince the condition number is invariant under positive scalar scaling, we can compute it for\n$$M := \\begin{pmatrix}1010100 & -100 \\\\ -100 & 1\\end{pmatrix},$$\nand $\\kappa(H_{k+1}) = \\kappa(M)$.\n\nFor a symmetric $2 \\times 2$ matrix $M = \\begin{pmatrix}A & B \\\\ B & D\\end{pmatrix}$, the eigenvalues are\n$$\\mu_{\\pm} = \\frac{\\operatorname{tr} M \\pm \\sqrt{(\\operatorname{tr} M)^{2} - 4 \\det M}}{2},$$\nso the condition number is\n$$\\kappa(M) = \\frac{\\mu_{+}}{\\mu_{-}} = \\frac{\\operatorname{tr} M + \\sqrt{(\\operatorname{tr} M)^{2} - 4 \\det M}}{\\operatorname{tr} M - \\sqrt{(\\operatorname{tr} M)^{2} - 4 \\det M}}.$$\nHere\n$$\\operatorname{tr} M = 1010100 + 1 = 1010101,\\qquad \\det M = 1010100 \\cdot 1 - (-100)^{2} = 1010100 - 10000 = 1000100.$$\nThe eigenvalues are approximately $\\lambda_{\\max} \\approx 1010100.01$ and $\\lambda_{\\min} \\approx 0.9901$.\nThe ratio is\n$$\\kappa(M) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\approx \\frac{1010100.01}{0.9901} \\approx 1020200.0 \\approx 1.02 \\times 10^6.$$\nThe calculation provided in the original text uses a Taylor expansion, which is a valid way to handle the numerical precision, and it arrives at the same result. The numerical result is correct.\nRounding to three significant figures yields\n$$\\kappa(H_{k+1}) = \\kappa(M) \\approx 1.02 \\times 10^{6}.$$\nBecause $s_k^T y_k = 0.01 > 0$, the DFP update preserves positive definiteness, so the condition number is well-defined.", "answer": "$$\\boxed{1.02 \\times 10^{6}}$$", "id": "2212484"}]}