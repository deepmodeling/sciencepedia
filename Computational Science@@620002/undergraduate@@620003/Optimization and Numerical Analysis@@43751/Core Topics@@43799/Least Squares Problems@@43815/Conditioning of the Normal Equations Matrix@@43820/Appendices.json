{"hands_on_practices": [{"introduction": "The most intuitive way to understand ill-conditioning in the normal equations is through geometry. The columns of the matrix $A$ can be viewed as vectors, and if these vectors are nearly parallel (i.e., almost linearly dependent), the matrix $A^TA$ becomes nearly singular. This exercise [@problem_id:2162074] provides a direct and elegant illustration of this principle by defining the columns as two vectors separated by a small angle $\\theta$, allowing you to derive how the condition number explodes as the vectors align.", "problem": "In numerical linear algebra, the conditioning of a matrix is a critical measure of its sensitivity to errors in input data. For a linear least-squares problem $A\\mathbf{x} \\approx \\mathbf{b}$, the sensitivity is governed by the condition number of the normal equations matrix, $A^TA$. Poor conditioning often arises when the columns of the matrix $A$ are nearly linearly dependent.\n\nConsider a simple $2 \\times 2$ matrix $A$ whose columns represent two basis vectors in a plane. The first column is the vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the second column is the vector $\\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$, where $\\theta$, in radians, is the angle between the two vectors. Assume $\\theta$ is a small positive angle. The matrix $A$ is thus given by $A = \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}$.\n\nDetermine the leading-order asymptotic expression for the 2-norm condition number of the associated normal equations matrix, $A^TA$, in the limit as $\\theta \\to 0^+$. Your answer should be a simple expression in terms of $\\theta$ that captures the dominant behavior for small angles.", "solution": "We are asked for the 2-norm condition number of the normal equations matrix $A^{T}A$ for $A=\\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}$, in the limit $\\theta \\to 0^{+}$. For a symmetric positive definite matrix $M$, the 2-norm condition number is $\\kappa_{2}(M)=\\frac{\\lambda_{\\max}(M)}{\\lambda_{\\min}(M)}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $M$.\n\nCompute $A^{T}A$:\n$$\nA^{T}=\\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix}, \\quad\nA^{T}A=\\begin{pmatrix} 1  \\cos\\theta \\\\ \\cos\\theta  1 \\end{pmatrix}.\n$$\nThe matrix $\\begin{pmatrix} 1  \\cos\\theta \\\\ \\cos\\theta  1 \\end{pmatrix}$ has eigenvalues\n$$\n\\lambda_{\\pm}=1 \\pm \\cos\\theta.\n$$\nTherefore,\n$$\n\\kappa_{2}(A^{T}A)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{1+\\cos\\theta}{1-\\cos\\theta}.\n$$\nFor small $\\theta$, use the Taylor expansion $\\cos\\theta=1-\\frac{\\theta^{2}}{2}+O(\\theta^{4})$. Hence\n$$\n1-\\cos\\theta=\\frac{\\theta^{2}}{2}+O(\\theta^{4}), \\quad 1+\\cos\\theta=2-\\frac{\\theta^{2}}{2}+O(\\theta^{4}).\n$$\nThus\n$$\n\\kappa_{2}(A^{T}A)=\\frac{2-\\frac{\\theta^{2}}{2}+O(\\theta^{4})}{\\frac{\\theta^{2}}{2}+O(\\theta^{4})}\n=\\frac{4}{\\theta^{2}}+O(1),\n$$\nso the leading-order asymptotic behavior as $\\theta \\to 0^{+}$ is\n$$\n\\kappa_{2}(A^{T}A)\\sim \\frac{4}{\\theta^{2}}.\n$$", "answer": "$$\\boxed{\\frac{4}{\\theta^{2}}}$$", "id": "2162074"}, {"introduction": "Building on the geometric intuition, let's explore how near-collinearity manifests in a common application: polynomial regression. When we attempt to fit a model to data points that are clustered very close to one another, the columns of the underlying design matrix $A$ become nearly linearly dependent. This practice [@problem_id:2162115] simulates this scenario by considering two data points separated by a small distance $\\epsilon$, providing a concrete link between the spacing of data and the numerical stability of the least-squares solution.", "problem": "Consider a linear least squares problem for fitting a model $y(x) = c_1 + c_2 x$ to two data points. The x-coordinates of these data points are given by $x_1 = 1$ and $x_2 = 1 + \\epsilon$, where $\\epsilon$ is a small, positive real number. The design matrix $A$ for this problem has elements $A_{ij} = x_i^{j-1}$ for $i,j \\in \\{1,2\\}$. The stability of solving the associated normal equations, $A^T A \\mathbf{c} = A^T \\mathbf{y}$, is related to the condition number of the matrix $B(\\epsilon) = A^T A(\\epsilon)$.\n\nThe 2-norm condition number of the symmetric positive-definite matrix $B(\\epsilon)$ is defined as the ratio of its largest to smallest eigenvalue, $\\kappa_2(B(\\epsilon)) = \\frac{\\lambda_{\\max}(B(\\epsilon))}{\\lambda_{\\min}(B(\\epsilon))}$. For small $\\epsilon$, this condition number can be approximated by an asymptotic formula of the form:\n$$ \\kappa_2(B(\\epsilon)) \\approx \\frac{C}{\\epsilon^n} $$\nwhere $C$ is a positive constant and $n$ is a positive integer.\n\nDetermine the values of the constant $C$ and the integer $n$. Present your answer as a 1-by-2 row matrix with elements $C$ and $n$ in that order.", "solution": "We form the Vandermonde-type design matrix for two points with $x_{1}=1$ and $x_{2}=1+\\epsilon$:\n$$\nA=\\begin{pmatrix}\n1  x_{1} \\\\\n1  x_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  1 \\\\\n1  1+\\epsilon\n\\end{pmatrix}.\n$$\nThe normal matrix is $B(\\epsilon)=A^{T}A$, which is symmetric positive-definite for $\\epsilon0$ (since the two rows of $A$ are linearly independent when $x_{1}\\neq x_{2}$). Compute $B(\\epsilon)$ explicitly:\n$$\nB(\\epsilon)=A^{T}A=\n\\begin{pmatrix}\n\\sum_{i=1}^{2}1^{2}  \\sum_{i=1}^{2}1\\cdot x_{i} \\\\\n\\sum_{i=1}^{2}x_{i}\\cdot 1  \\sum_{i=1}^{2}x_{i}^{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2  2+\\epsilon \\\\\n2+\\epsilon  2+2\\epsilon+\\epsilon^{2}\n\\end{pmatrix}.\n$$\nFor a $2\\times 2$ symmetric matrix $\\begin{pmatrix}a  b \\\\ b  c\\end{pmatrix}$, the eigenvalues satisfy $\\lambda_{\\max}+\\lambda_{\\min}=\\operatorname{tr}=a+c$ and $\\lambda_{\\max}\\lambda_{\\min}=\\det=ac-b^{2}$. Here,\n$$\na=2,\\quad b=2+\\epsilon,\\quad c=2+2\\epsilon+\\epsilon^{2}.\n$$\nHence the determinant is\n$$\n\\det B(\\epsilon)=ac-b^{2}=2(2+2\\epsilon+\\epsilon^{2})-(2+\\epsilon)^{2}=(4+4\\epsilon+2\\epsilon^{2})-(4+4\\epsilon+\\epsilon^{2})=\\epsilon^{2}.\n$$\nAt $\\epsilon=0$, we have\n$$\nB(0)=\\begin{pmatrix}2  2 \\\\ 2  2\\end{pmatrix},\n$$\nwhose eigenvalues are $4$ and $0$. Therefore, for small $\\epsilon0$, by continuity,\n$$\n\\lambda_{\\max}(B(\\epsilon))=4+O(\\epsilon).\n$$\nUsing the identity $\\lambda_{\\min}=\\det/\\lambda_{\\max}$,\n$$\n\\lambda_{\\min}(B(\\epsilon))=\\frac{\\det B(\\epsilon)}{\\lambda_{\\max}(B(\\epsilon))}=\\frac{\\epsilon^{2}}{4+O(\\epsilon)}=\\frac{1}{4}\\epsilon^{2}+O(\\epsilon^{3}).\n$$\nTherefore, the $2$-norm condition number is\n$$\n\\kappa_{2}(B(\\epsilon))=\\frac{\\lambda_{\\max}(B(\\epsilon))}{\\lambda_{\\min}(B(\\epsilon))}=\\frac{4+O(\\epsilon)}{\\frac{1}{4}\\epsilon^{2}+O(\\epsilon^{3})}=\\frac{16}{\\epsilon^{2}}+O\\left(\\frac{1}{\\epsilon}\\right).\n$$\nThus the asymptotic form $\\kappa_{2}(B(\\epsilon))\\approx \\frac{C}{\\epsilon^{n}}$ has\n$$\nC=16,\\quad n=2.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}16  2\\end{pmatrix}}$$", "id": "2162115"}, {"introduction": "While near-collinearity is a primary cause of ill-conditioning, it is not the only one. Another significant source of numerical instability arises from poor scaling among the features, where the columns of the data matrix $A$ have vastly different magnitudes. This exercise [@problem_id:2162068] guides you through an analysis of how rescaling one feature by a large factor $\\alpha$ impacts the condition number of $A^TA$, highlighting the critical importance of data preprocessing and feature scaling in practical machine learning and statistical applications.", "problem": "In the context of linear least squares problems, the normal equations matrix $A^T A$ must be inverted. The numerical stability of this inversion is related to the condition number of $A^T A$. Consider a data matrix $A$ with two column vectors, $A = [v_1, v_2]$, where $v_1, v_2 \\in \\mathbb{R}^m$ for some large integer $m$. The columns represent two different features measured in an experiment. The vectors are normalized such that their Euclidean norms are one, which means $v_1^T v_1 = 1$ and $v_2^T v_2 = 1$. The features are correlated, with the inner product of the column vectors being $v_1^T v_2 = c$.\n\nSuppose we rescale the second feature by a large factor $\\alpha  0$, leading to a new data matrix $A_\\alpha = [v_1, \\alpha v_2]$. This is a common operation in data preprocessing, for example, when changing measurement units. This scaling can significantly affect the conditioning of the corresponding normal equations matrix, $B_\\alpha = A_\\alpha^T A_\\alpha$.\n\nFor a symmetric positive-definite matrix, its 2-norm condition number is defined as the ratio of its largest eigenvalue to its smallest eigenvalue. For large values of $\\alpha$, it is found that the 2-norm condition number of $B_\\alpha$, denoted $\\kappa_2(B_\\alpha)$, grows quadratically with $\\alpha$. That is, $\\kappa_2(B_\\alpha) \\approx K \\alpha^2$ for some constant $K$.\n\nGiven that the correlation constant is $c = 1/3$, determine the exact value of the proportionality constant $K$.", "solution": "We are given $A_{\\alpha} = [v_{1}, \\alpha v_{2}]$ with $v_{1}^{T} v_{1} = 1$, $v_{2}^{T} v_{2} = 1$, and $v_{1}^{T} v_{2} = c$. The normal equations matrix is the Gram matrix\n$$\nB_{\\alpha} = A_{\\alpha}^{T} A_{\\alpha} = \\begin{pmatrix} 1  \\alpha c \\\\ \\alpha c  \\alpha^{2} \\end{pmatrix}.\n$$\nFor a symmetric positive-definite matrix, the $2$-norm condition number is the ratio of the largest to the smallest eigenvalue. The eigenvalues of a $2 \\times 2$ symmetric matrix can be expressed in terms of its trace and determinant. Here\n$$\n\\operatorname{tr}(B_{\\alpha}) = 1 + \\alpha^{2}, \\qquad \\det(B_{\\alpha}) = \\alpha^{2}(1 - c^{2}).\n$$\nThus the eigenvalues are\n$$\n\\lambda_{\\pm} = \\frac{1 + \\alpha^{2} \\pm \\sqrt{(1 + \\alpha^{2})^{2} - 4 \\alpha^{2}(1 - c^{2})}}{2}\n= \\frac{1 + \\alpha^{2} \\pm \\sqrt{(\\alpha^{2} - 1)^{2} + 4 \\alpha^{2} c^{2}}}{2}.\n$$\nTherefore the condition number is\n$$\n\\kappa_{2}(B_{\\alpha}) = \\frac{\\lambda_{+}}{\\lambda_{-}} = \\frac{1 + \\alpha^{2} + \\sqrt{(\\alpha^{2} - 1)^{2} + 4 \\alpha^{2} c^{2}}}{1 + \\alpha^{2} - \\sqrt{(\\alpha^{2} - 1)^{2} + 4 \\alpha^{2} c^{2}}}.\n$$\n\nTo extract the leading dependence for large $\\alpha$, factor $\\alpha^{2}$ inside the square root. Set\n$$\nS_{\\alpha} = \\sqrt{(\\alpha^{2} - 1)^{2} + 4 \\alpha^{2} c^{2}} = \\sqrt{\\alpha^{4} + \\alpha^{2}(-2 + 4 c^{2}) + 1}\n= \\alpha^{2} \\sqrt{1 + \\frac{-2 + 4 c^{2}}{\\alpha^{2}} + \\frac{1}{\\alpha^{4}}}.\n$$\nUsing the expansion $\\sqrt{1 + x} = 1 + \\frac{1}{2} x + O(x^{2})$ with $x = \\frac{-2 + 4 c^{2}}{\\alpha^{2}} + \\frac{1}{\\alpha^{4}}$ yields\n$$\nS_{\\alpha} = \\alpha^{2} \\left(1 + \\frac{-2 + 4 c^{2}}{2 \\alpha^{2}} + O\\left(\\frac{1}{\\alpha^{4}}\\right)\\right)\n= \\alpha^{2} + \\frac{-2 + 4 c^{2}}{2} + O\\left(\\frac{1}{\\alpha^{2}}\\right).\n$$\nThen\n$$\n1 + \\alpha^{2} + S_{\\alpha} = 2 \\alpha^{2} + \\frac{-2 + 4 c^{2}}{2} + 1 + O\\left(\\frac{1}{\\alpha^{2}}\\right),\n$$\n$$\n1 + \\alpha^{2} - S_{\\alpha} = 1 - \\frac{-2 + 4 c^{2}}{2} + O\\left(\\frac{1}{\\alpha^{2}}\\right) = 2 - 2 c^{2} + O\\left(\\frac{1}{\\alpha^{2}}\\right).\n$$\nHence\n$$\n\\kappa_{2}(B_{\\alpha}) = \\frac{2 \\alpha^{2} + O(1)}{2 - 2 c^{2} + O\\left(\\frac{1}{\\alpha^{2}}\\right)}\n= \\frac{\\alpha^{2}}{1 - c^{2}} + O(1) \\quad \\text{as } \\alpha \\to \\infty.\n$$\nTherefore the proportionality constant is\n$$\nK = \\frac{1}{1 - c^{2}}.\n$$\nWith $c = \\frac{1}{3}$, we obtain\n$$\nK = \\frac{1}{1 - \\left(\\frac{1}{3}\\right)^{2}} = \\frac{1}{1 - \\frac{1}{9}} = \\frac{1}{\\frac{8}{9}} = \\frac{9}{8}.\n$$", "answer": "$$\\boxed{\\frac{9}{8}}$$", "id": "2162068"}]}