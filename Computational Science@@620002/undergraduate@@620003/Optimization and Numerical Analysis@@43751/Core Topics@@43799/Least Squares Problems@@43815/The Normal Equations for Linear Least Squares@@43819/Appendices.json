{"hands_on_practices": [{"introduction": "The most common application of the normal equations is finding the \"line of best fit\" for a set of experimental data. This process, known as linear regression, is fundamental in science and engineering for modeling relationships between variables. This first practice [@problem_id:2218047] walks you through a classic scenario of sensor calibration, where you will not only determine the model parameters by setting up and solving the normal equations but also quantify the \"goodness of fit\" by calculating the norm of the residual vector.", "problem": "An engineer is calibrating a novel thermal sensor. The sensor's output voltage, $V$, is assumed to be a linear function of the ambient temperature, $T$. The relationship is modeled by the equation $V(T) = c_0 + c_1 T$, where $c_0$ and $c_1$ are the calibration constants to be determined. To find these constants, four measurements are taken in a controlled environment:\n\n*   At a temperature of $T=10$ degrees Celsius, the measured voltage is $V=2.6$ volts.\n*   At a temperature of $T=20$ degrees Celsius, the measured voltage is $V=3.4$ volts.\n*   At a temperature of $T=30$ degrees Celsius, the measured voltage is $V=4.7$ volts.\n*   At a temperature of $T=40$ degrees Celsius, the measured voltage is $V=5.4$ volts.\n\nThe parameters $c_0$ and $c_1$ are to be determined such that the sum of the squared differences between the measured voltages and the voltages predicted by the linear model is minimized. Let the resulting best-fit line be $\\hat{V}(T) = \\hat{c}_0 + \\hat{c}_1 T$.\n\nYour task is to calculate the Euclidean norm of the residual vector, where the components of the residual vector are the differences between the individually measured voltages and the corresponding voltages predicted by this best-fit line.\n\nExpress your final answer in volts, rounded to three significant figures.", "solution": "We model the voltage as a linear function of temperature, $V(T)=c_{0}+c_{1}T$, and determine $(\\hat{c}_{0},\\hat{c}_{1})$ by least squares using the four measurements $(T_{i},V_{i})=(10,2.6),(20,3.4),(30,4.7),(40,5.4)$. Let the design matrix be $X=\\begin{pmatrix}1 & 10 \\\\ 1 & 20 \\\\ 1 & 30 \\\\ 1 & 40\\end{pmatrix}$ and the observation vector be $\\boldsymbol{V}=\\begin{pmatrix}2.6 \\\\ 3.4 \\\\ 4.7 \\\\ 5.4\\end{pmatrix}$. The least-squares estimate satisfies\n$$\n\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=(X^{T}X)^{-1}X^{T}\\boldsymbol{V},\n$$\nequivalently the normal equations\n$$\n\\begin{pmatrix}n & \\sum T_{i} \\\\ \\sum T_{i} & \\sum T_{i}^{2}\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}\\sum V_{i} \\\\ \\sum T_{i}V_{i}\\end{pmatrix}.\n$$\nCompute the sums: $n=4$, $\\sum T_{i}=10+20+30+40=100$, $\\sum T_{i}^{2}=10^{2}+20^{2}+30^{2}+40^{2}=3000$, $\\sum V_{i}=2.6+3.4+4.7+5.4=16.1$, and $\\sum T_{i}V_{i}=10\\cdot 2.6+20\\cdot 3.4+30\\cdot 4.7+40\\cdot 5.4=451$. Thus we solve\n$$\n\\begin{pmatrix}4 & 100 \\\\ 100 & 3000\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}16.1 \\\\ 451\\end{pmatrix}.\n$$\nThe determinant is $4\\cdot 3000-100\\cdot 100=2000$, so\n$$\n\\hat{c}_{0}=\\frac{3000\\cdot 16.1-100\\cdot 451}{2000}=1.6,\\quad \\hat{c}_{1}=\\frac{-100\\cdot 16.1+4\\cdot 451}{2000}=0.097.\n$$\nHence the best-fit line is $\\hat{V}(T)=1.6+0.097\\,T$.\n\nCompute the residuals $r_{i}=V_{i}-\\hat{V}(T_{i})$ at the four temperatures:\n$$\n\\hat{V}(10)=1.6+0.097\\cdot 10=2.57,\\quad r_{1}=2.6-2.57=0.03,\n$$\n$$\n\\hat{V}(20)=1.6+0.097\\cdot 20=3.54,\\quad r_{2}=3.4-3.54=-0.14,\n$$\n$$\n\\hat{V}(30)=1.6+0.097\\cdot 30=4.51,\\quad r_{3}=4.7-4.51=0.19,\n$$\n$$\n\\hat{V}(40)=1.6+0.097\\cdot 40=5.48,\\quad r_{4}=5.4-5.48=-0.08.\n$$\nThe Euclidean norm of the residual vector $\\boldsymbol{r}$ is\n$$\n\\|\\boldsymbol{r}\\|_{2}=\\sqrt{\\sum_{i=1}^{4}r_{i}^{2}}=\\sqrt{(0.03)^{2}+(-0.14)^{2}+(0.19)^{2}+(-0.08)^{2}}=\\sqrt{0.063}.\n$$\nEvaluating the square root and rounding to three significant figures gives\n$$\n\\|\\boldsymbol{r}\\|_{2}\\approx 0.251.\n$$\nThis value is in volts because each residual is a voltage difference.", "answer": "$$\\boxed{0.251}$$", "id": "2218047"}, {"introduction": "The least squares method is not limited to straight lines; its power lies in its adaptability to any model that is linear in its coefficients. This practice [@problem_id:2218043] challenges you to fit a quadratic curve to experimental data, demonstrating how the framework naturally extends to polynomial regression. The underlying principle remains identical: you will construct a design matrix $A$ based on your polynomial model and solve the normal equations $A^T A \\mathbf{c} = A^T \\mathbf{b}$ to find the optimal coefficients.", "problem": "An engineer is calibrating a novel cryogenic temperature sensor. The sensor's electrical resistance, $R$, is hypothesized to vary quadratically with the absolute temperature, $T$. The proposed model is $R(T) = c_{0} + c_{1} T + c_{2} T^{2}$, where $c_0$, $c_1$, and $c_2$ are the calibration coefficients to be determined. To find these coefficients, a series of measurements are taken, yielding the following four data points $(T_i, R_i)$:\n\n- $(10.0 \\text{ K}, 125.0 \\text{ } \\Omega)$\n- $(20.0 \\text{ K}, 180.0 \\text{ } \\Omega)$\n- $(30.0 \\text{ K}, 250.0 \\text{ } \\Omega)$\n- $(40.0 \\text{ K}, 350.0 \\text{ } \\Omega)$\n\nYour task is to determine the best-fit values for the coefficients by solving the linear least squares problem that minimizes the sum of the squared differences between the measured resistances and the model's predictions.\n\nFind the value of the linear coefficient, $c_1$. Express your answer in units of $\\Omega/\\text{K}$, rounded to four significant figures.", "solution": "We model the resistance as $R(T)=c_{0}+c_{1}T+c_{2}T^{2}$ and determine $c_{0},c_{1},c_{2}$ in the linear least squares sense. Using the normal equations for least squares with design matrix rows $[1,\\,T_{i},\\,T_{i}^{2}]$ and data $R_{i}$, we form $(X^{\\top}X)\\mathbf{c}=X^{\\top}\\mathbf{y}$. Define the sums $S_{k}=\\sum_{i}T_{i}^{k}$ for $k=0,1,2,3,4$ and $b_{0}=\\sum_{i}R_{i}$, $b_{1}=\\sum_{i}T_{i}R_{i}$, $b_{2}=\\sum_{i}T_{i}^{2}R_{i}$. For the data $(T_{i},R_{i})\\in\\{(10,125),(20,180),(30,250),(40,350)\\}$, these are\n$$\nS_{0}=4,\\quad S_{1}=100,\\quad S_{2}=3000,\\quad S_{3}=100000,\\quad S_{4}=3540000,\n$$\n$$\nb_{0}=905,\\quad b_{1}=26350,\\quad b_{2}=869500.\n$$\nThus the normal equations are\n$$\n\\begin{bmatrix}\n4 & 100 & 3000\\\\\n100 & 3000 & 100000\\\\\n3000 & 100000 & 3540000\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{0}\\\\\nc_{1}\\\\\nc_{2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n905\\\\\n26350\\\\\n869500\n\\end{bmatrix}.\n$$\nWriting these componentwise,\n$$\n\\begin{aligned}\n&4c_{0}+100c_{1}+3000c_{2}=905\\quad\\text{(1)},\\\\\n&100c_{0}+3000c_{1}+100000c_{2}=26350\\quad\\text{(2)},\\\\\n&3000c_{0}+100000c_{1}+3540000c_{2}=869500\\quad\\text{(3)}.\n\\end{aligned}\n$$\nFrom (1), solve for $c_{0}$:\n$$\nc_{0}=\\frac{905-100c_{1}-3000c_{2}}{4}=226.25-25c_{1}-750c_{2}.\n$$\nSubstitute this into (2) and (3). For (2):\n$$\n100(226.25-25c_{1}-750c_{2})+3000c_{1}+100000c_{2}=26350,\n$$\nwhich simplifies to\n$$\n500c_{1}+25000c_{2}=3725\\;\\Rightarrow\\;20c_{1}+1000c_{2}=149.\\quad\\text{(A)}\n$$\nFor (3):\n$$\n3000(226.25-25c_{1}-750c_{2})+100000c_{1}+3540000c_{2}=869500,\n$$\nwhich simplifies to\n$$\n25000c_{1}+1290000c_{2}=190750\\;\\Rightarrow\\;100c_{1}+5160c_{2}=763.\\quad\\text{(B)}\n$$\nSolve (A) and (B). Multiply (A) by $5$ and subtract from (B):\n$$\n\\bigl(100c_{1}+5160c_{2}\\bigr)-\\bigl(100c_{1}+5000c_{2}\\bigr)=763-745\\;\\Rightarrow\\;160c_{2}=18,\n$$\nso\n$$\nc_{2}=\\frac{18}{160}=\\frac{9}{80}=0.1125.\n$$\nSubstitute into (A):\n$$\n20c_{1}+1000\\cdot\\frac{9}{80}=149\\;\\Rightarrow\\;20c_{1}+112.5=149\\;\\Rightarrow\\;20c_{1}=36.5\\;\\Rightarrow\\;c_{1}=\\frac{73}{40}=1.825.\n$$\nRounded to four significant figures in the required units, the linear coefficient is $1.825$.", "answer": "$$\\boxed{1.825}$$", "id": "2218043"}, {"introduction": "Beyond fitting curves to data, the method of least squares has a profound geometric interpretation that is crucial for a deeper understanding. This final exercise [@problem_id:2218040] frames the problem as finding the closest point in a vector subspace to an external pointâ€”a task equivalent to computing an orthogonal projection. By solving the normal equations, you are geometrically projecting your observation vector onto the subspace spanned by your model's basis vectors, a core concept with applications in robotics, signal processing, and machine learning.", "problem": "You are a control systems engineer working on a simple 2-Degree-Of-Freedom (2-DOF) robotic manipulator. The robot's end-effector moves in 3D space, and its position relative to its base is described by a coordinate vector $\\mathbf{p} = [x, y, z]^T$. The robot's motion is generated by two independent actuators. The first actuator moves the end-effector along the direction vector $\\mathbf{u}_1 = [1, 0, 1]^T$, and the second actuator moves it along the direction vector $\\mathbf{u}_2 = [0, 1, 1]^T$. Consequently, any reachable position $\\mathbf{p}_{reach}$ must be a linear combination of these two vectors: $\\mathbf{p}_{reach} = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2$, for some scalar coefficients $c_1$ and $c_2$ representing the actuator displacements. This means the set of all reachable points forms a plane passing through the origin.\n\nA task requires the robot's end-effector to move to a target position $\\mathbf{b} = [1, 2, 2]^T$. This point may not lie within the robot's reachable plane. The control system's objective is to find the point $\\mathbf{p}^*$ within the reachable plane that is geometrically closest to the target point $\\mathbf{b}$. This optimal point is the orthogonal projection of $\\mathbf{b}$ onto the subspace spanned by the actuator direction vectors.\n\nUsing the method of normal equations, determine the coordinate vector of this optimal reachable point $\\mathbf{p}^*$. Express your answer as a column vector with exact fractional components.", "solution": "The problem asks for the orthogonal projection of the vector $\\mathbf{b}$ onto the subspace spanned by the vectors $\\mathbf{u}_1$ and $\\mathbf{u}_2$. This subspace is the column space of the matrix $A$ whose columns are $\\mathbf{u}_1$ and $\\mathbf{u}_2$. The projection $\\mathbf{p}^*$ is the vector in the column space of $A$ that is closest to $\\mathbf{b}$.\n\nFirst, we define the matrix $A$ and the vector $\\mathbf{b}$:\n$$ A = \\begin{bmatrix} \\mathbf{u}_1 & \\mathbf{u}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} $$\n\nThe projection $\\mathbf{p}^*$ can be expressed as a linear combination of the columns of $A$, i.e., $\\mathbf{p}^* = A\\mathbf{c}^*$, where $\\mathbf{c}^* = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}$ is the vector of coefficients that minimizes the squared Euclidean distance $\\|A\\mathbf{c} - \\mathbf{b}\\|^2$. This is a linear least squares problem. The solution $\\mathbf{c}^*$ is found by solving the normal equations:\n$$ A^T A \\mathbf{c}^* = A^T \\mathbf{b} $$\n\nWe proceed by computing the components of this equation.\n\nStep 1: Compute the matrix $A^T A$.\nThe transpose of $A$ is:\n$$ A^T = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} $$\nNow, we compute the product $A^T A$:\n$$ A^T A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(0)+(1)(1) & (1)(0)+(0)(1)+(1)(1) \\\\ (0)(1)+(1)(0)+(1)(1) & (0)(0)+(1)(1)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $$\n\nStep 2: Compute the vector $A^T \\mathbf{b}$.\n$$ A^T \\mathbf{b} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(2)+(1)(2) \\\\ (0)(1)+(1)(2)+(1)(2) \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$\n\nStep 3: Solve the normal equations for $\\mathbf{c}^*$.\nThe system of equations is:\n$$ \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$\nThis corresponds to the two linear equations:\n1) $2c_1 + c_2 = 3$\n2) $c_1 + 2c_2 = 4$\n\nFrom equation (1), we can express $c_2$ in terms of $c_1$:\n$c_2 = 3 - 2c_1$\n\nSubstitute this into equation (2):\n$c_1 + 2(3 - 2c_1) = 4$\n$c_1 + 6 - 4c_1 = 4$\n$-3c_1 = -2$\n$c_1 = \\frac{2}{3}$\n\nNow, substitute the value of $c_1$ back to find $c_2$:\n$c_2 = 3 - 2\\left(\\frac{2}{3}\\right) = 3 - \\frac{4}{3} = \\frac{9}{3} - \\frac{4}{3} = \\frac{5}{3}$\nSo, the coefficient vector is $\\mathbf{c}^* = \\begin{bmatrix} 2/3 \\\\ 5/3 \\end{bmatrix}$.\n\nStep 4: Compute the projection vector $\\mathbf{p}^*$.\nThe optimal reachable point $\\mathbf{p}^*$ is given by $A\\mathbf{c}^*$:\n$$ \\mathbf{p}^* = A\\mathbf{c}^* = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\end{bmatrix} = \\begin{bmatrix} (1)\\left(\\frac{2}{3}\\right) + (0)\\left(\\frac{5}{3}\\right) \\\\ (0)\\left(\\frac{2}{3}\\right) + (1)\\left(\\frac{5}{3}\\right) \\\\ (1)\\left(\\frac{2}{3}\\right) + (1)\\left(\\frac{5}{3}\\right) \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\\\ \\frac{2}{3} + \\frac{5}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\\\ \\frac{7}{3} \\end{bmatrix} $$\n\nThe coordinate vector of the optimal reachable point is $\\mathbf{p}^* = [2/3, 5/3, 7/3]^T$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} \\\\\n\\frac{5}{3} \\\\\n\\frac{7}{3}\n\\end{pmatrix}\n}\n$$", "id": "2218040"}]}