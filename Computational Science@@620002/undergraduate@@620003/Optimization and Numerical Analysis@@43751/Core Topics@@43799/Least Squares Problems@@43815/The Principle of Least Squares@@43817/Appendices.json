{"hands_on_practices": [{"introduction": "The principle of least squares is a powerful tool for finding the best-fit model for a set of data. We will begin with the simplest possible case: finding a single constant value that best represents a series of measurements. This exercise [@problem_id:2218976] demonstrates that the least squares estimate in this scenario is a familiar statistical quantity, the arithmetic mean, providing a strong intuitive foundation for the method.", "problem": "An experiment is conducted to determine a constant physical parameter. Due to random experimental errors, multiple measurements yield slightly different values. The data collected consists of three measurement pairs $(x_i, y_i)$, where $x_i$ represents an independent variable (like time) and $y_i$ is the measured value of the parameter. The observed data points are $(1, 3)$, $(2, 5)$, and $(3, 4)$.\n\nAssuming the true value of the parameter is a constant, we want to find the best estimate for this constant. This can be achieved by fitting the data to a horizontal line of the form $y = c$.\n\nUsing the principle of least squares, determine the value of the constant $c$ that provides the best fit to the given data points. Express your answer as an exact number.", "solution": "We model the measurements by a constant $c$ so that each observed $y_{i}$ is approximated by $c$. By the least squares principle, we choose $c$ to minimize the sum of squared residuals\n$$\nS(c)=\\sum_{i=1}^{3}\\left(y_{i}-c\\right)^{2}.\n$$\nDifferentiate with respect to $c$ and set the derivative to zero:\n$$\n\\frac{dS}{dc}=\\sum_{i=1}^{3}2\\left(c-y_{i}\\right)=2\\left(3c-\\sum_{i=1}^{3}y_{i}\\right)=0.\n$$\nThis yields the normal equation\n$$\n3c=\\sum_{i=1}^{3}y_{i}\\quad\\Longrightarrow\\quad c=\\frac{1}{3}\\sum_{i=1}^{3}y_{i}.\n$$\nSubstituting the data $y_{1}=3$, $y_{2}=5$, $y_{3}=4$ gives\n$$\nc=\\frac{1}{3}(3+5+4)=\\frac{12}{3}=4.\n$$\nTo verify it is a minimum, note\n$$\n\\frac{d^{2}S}{dc^{2}}=\\sum_{i=1}^{3}2=6>0,\n$$\nso $c=4$ minimizes $S(c)$ and is the least squares estimate (the sample mean of the $y_{i}$ values).", "answer": "$$\\boxed{4}$$", "id": "2218976"}, {"introduction": "Many phenomena in science and engineering exhibit a direct proportional relationship, which can be modeled by a line passing through the origin, $y=mx$. This next practice [@problem_id:2219020] challenges you to apply the principle of least squares to derive the general formula for the slope of such a best-fit line. Understanding this derivation is key to seeing how calculus is used to find optimal model parameters.", "problem": "In many physical systems, a proportional relationship is expected between two quantities, which can be modeled by an equation of the form $y = mx$. Consider an experiment that yields a set of $N$ data points $(x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)$. We wish to find the \"best-fit\" line that passes through the origin for this dataset.\n\nAccording to the principle of least squares, the best fit is the one that minimizes the sum of the squared vertical distances between the observed data points and the model line. Let this sum of squared errors be denoted by $S$.\n\nDetermine the analytic expression for the slope $m$ that minimizes $S$. Your answer should be expressed in terms of the given data coordinates $x_i$ and $y_i$.", "solution": "We assume the model is $y=m x$ (a line constrained to pass through the origin). For data $\\{(x_{i},y_{i})\\}_{i=1}^{N}$, the least-squares objective (sum of squared vertical residuals) as a function of $m$ is\n$$\nS(m)=\\sum_{i=1}^{N}\\left(y_{i}-m x_{i}\\right)^{2}.\n$$\nTo find the minimizing slope, differentiate $S(m)$ with respect to $m$ and set the derivative to zero (first-order optimality condition):\n$$\n\\frac{dS}{dm}=\\sum_{i=1}^{N}2\\left(y_{i}-m x_{i}\\right)\\left(-x_{i}\\right)=-2\\sum_{i=1}^{N}x_{i}y_{i}+2m\\sum_{i=1}^{N}x_{i}^{2}.\n$$\nSet $\\frac{dS}{dm}=0$:\n$$\n-2\\sum_{i=1}^{N}x_{i}y_{i}+2m\\sum_{i=1}^{N}x_{i}^{2}=0.\n$$\nSolve for $m$:\n$$\nm\\sum_{i=1}^{N}x_{i}^{2}=\\sum_{i=1}^{N}x_{i}y_{i}\\quad\\Rightarrow\\quad m=\\frac{\\sum_{i=1}^{N}x_{i}y_{i}}{\\sum_{i=1}^{N}x_{i}^{2}}.\n$$\nTo confirm this is a minimum, compute the second derivative:\n$$\n\\frac{d^{2}S}{dm^{2}}=2\\sum_{i=1}^{N}x_{i}^{2}\\ge 0,\n$$\nwith strict positivity whenever at least one $x_{i}\\neq 0$, ensuring a unique minimizer. If $\\sum_{i=1}^{N}x_{i}^{2}=0$ (i.e., all $x_{i}=0$), then $S(m)=\\sum_{i=1}^{N}y_{i}^{2}$ is independent of $m$, so the slope is not identifiable. Otherwise, the minimizing slope is the expression above.", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{N} x_{i} y_{i}}{\\sum_{i=1}^{N} x_{i}^{2}}}$$", "id": "2219020"}, {"introduction": "While single-parameter models are useful, most real-world problems involve multiple variables, which are best handled using the language of linear algebra. In this exercise [@problem_id:2218998], you will solve an overdetermined system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$ by setting up and solving the normal equations. This is a cornerstone technique for finding approximate solutions in data analysis and machine learning.", "problem": "In many data analysis applications, we encounter overdetermined linear systems of the form $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ has more rows than columns. Such systems typically have no exact solution. The principle of least squares provides a way to find an approximate solution, $\\hat{\\mathbf{x}}$, by minimizing the squared Euclidean norm of the residual vector, $r = A\\mathbf{x} - \\mathbf{b}$.\n\nConsider the specific overdetermined system $A\\mathbf{x} = \\mathbf{b}$ where the matrix $A$ and the vector $\\mathbf{b}$ are given by:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\n$$\nFind the vector $\\hat{\\mathbf{x}} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ that is the least squares solution to this system. Your answer should be the vector $\\hat{\\mathbf{x}}$.", "solution": "We seek the least squares solution $\\hat{\\mathbf{x}}$ that minimizes $\\|A\\mathbf{x}-\\mathbf{b}\\|^{2}$. The necessary condition for a minimizer is given by the normal equations\n$$\nA^{\\top}A\\,\\hat{\\mathbf{x}}=A^{\\top}\\mathbf{b}.\n$$\nWith\n$$\nA=\\begin{pmatrix}1&0\\\\0&1\\\\1&1\\end{pmatrix},\\quad \\mathbf{b}=\\begin{pmatrix}2\\\\3\\\\5\\end{pmatrix},\n$$\ncompute\n$$\nA^{\\top}=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix},\n$$\nso\n$$\nA^{\\top}A=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}\\begin{pmatrix}1&0\\\\0&1\\\\1&1\\end{pmatrix}\n=\\begin{pmatrix}2&1\\\\1&2\\end{pmatrix},\n$$\nand\n$$\nA^{\\top}\\mathbf{b}=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}\\begin{pmatrix}2\\\\3\\\\5\\end{pmatrix}\n=\\begin{pmatrix}7\\\\8\\end{pmatrix}.\n$$\nThus $\\hat{\\mathbf{x}}$ satisfies\n$$\n\\begin{pmatrix}2&1\\\\1&2\\end{pmatrix}\\begin{pmatrix}x_{1}\\\\x_{2}\\end{pmatrix}=\\begin{pmatrix}7\\\\8\\end{pmatrix}.\n$$\nSolving,\n$$\n\\begin{cases}\n2x_{1}+x_{2}=7,\\\\\nx_{1}+2x_{2}=8,\n\\end{cases}\n\\quad\\Rightarrow\\quad x_{2}=7-2x_{1},\n$$\nsubstitute into the second equation:\n$$\nx_{1}+2(7-2x_{1})=8\\;\\Rightarrow\\;-3x_{1}=-6\\;\\Rightarrow\\;x_{1}=2,\n$$\nand then\n$$\nx_{2}=7-2\\cdot 2=3.\n$$\nTherefore,\n$$\n\\hat{\\mathbf{x}}=\\begin{pmatrix}2\\\\3\\end{pmatrix}.\n$$\nNotably, $A\\hat{\\mathbf{x}}=\\begin{pmatrix}2\\\\3\\\\5\\end{pmatrix}=\\mathbf{b}$, so the residual is zero and the system is consistent.", "answer": "$$\\boxed{\\begin{pmatrix}2\\\\3\\end{pmatrix}}$$", "id": "2218998"}]}