{"hands_on_practices": [{"introduction": "A core principle of any bracketing root-finding algorithm is the assumption that a root exists within the initial interval. Brent's method, like the bisection method, typically relies on a sign change between the function values at the interval's endpoints—a condition guaranteed to bracket a root for continuous functions by the Intermediate Value Theorem. This thought experiment [@problem_id:2157790] probes what happens when this fundamental condition is not met, revealing the importance of the algorithm's initial checks for a valid bracket.", "problem": "Brent's method is a popular root-finding algorithm that combines the bisection method, the secant method, and inverse quadratic interpolation. A fundamental prerequisite for any bracketing root-finding method is that the initial interval $[a, b]$ must contain a root, which is typically guaranteed by the Intermediate Value Theorem if the function is continuous and $f(a)$ and $f(b)$ have opposite signs.\n\nConsider the function $f(x) = (x-1)^2 + 0.1$. An analyst attempts to find a root of this function using a standard, robust implementation of Brent's method, mistakenly providing the initial bracket $[a, b] = [0, 2]$.\n\nWhich of the following statements best describes the expected behavior of the algorithm?\n\nA. The algorithm will terminate immediately and report an error because the initial interval does not bracket a root (i.e., $f(a) \\cdot f(b) > 0$).\n\nB. The algorithm will not find a root but will converge to the location of the function's minimum at $x=1$.\n\nC. The algorithm will perform bisection steps indefinitely, narrowing the interval towards $x=1$ until machine precision is reached.\n\nD. The algorithm will diverge, with subsequent iterations producing estimates for the root that fall outside the initial interval $[0, 2]$.\n\nE. The algorithm will default to the secant method, which will fail due to a division-by-zero error on the first step.", "solution": "We analyze the function and the bracketing condition required by Brent’s root-finding method.\n\nGiven $f(x) = (x-1)^2 + 0.1$, note that for all real $x$,\n$(x-1)^2 \\geq 0 \\quad \\Rightarrow \\quad f(x) = (x-1)^2 + 0.1 \\geq 0.1 > 0$.\nTherefore, the equation $f(x) = 0$ has no solution; $f$ has no real root anywhere.\n\nFor the provided initial interval $[a,b] = [0,2]$, compute the endpoint values:\n$$f(0) = (0-1)^2 + 0.1 = 1 + 0.1 = 1.1,$$\n$$f(2) = (2-1)^2 + 0.1 = 1 + 0.1 = 1.1.$$\nThus,\n$$f(a)\\cdot f(b) = f(0)\\,f(2) = 1.1 \\times 1.1 = 1.21 > 0.$$\nA bracketing root-finder like Brent’s method requires a sign change on $[a,b]$, i.e., $f(a)f(b) < 0$, to guarantee a root by the Intermediate Value Theorem and to maintain a valid bracketing interval during iterations. This prerequisite is violated here.\n\nA standard, robust implementation of Brent’s root-finding algorithm first verifies the bracketing condition $f(a)f(b) < 0$. If it fails, the method does not proceed to any combination of bisection, secant, or inverse quadratic interpolation steps; instead, it terminates immediately and reports an error or returns a failure status indicating that the initial interval does not bracket a root.\n\nAssessment of the options:\n- A is correct: it states the immediate termination with an error due to $f(a) \\cdot f(b) > 0$.\n- B is incorrect: convergence to the minimum at $x=1$ describes Brent’s algorithm for minimization, which is a different algorithm from Brent’s root-finding method.\n- C is incorrect: bisection requires a sign change to select subintervals; without bracketing, bisection cannot proceed.\n- D is incorrect: Brent’s root-finding method is a bracketing method and does not diverge outside the interval; moreover, it does not iterate without a valid bracket.\n- E is incorrect: although a naive secant step would have a zero denominator here since $f(0)=f(2)$, a robust Brent implementation would not default to secant without first ensuring a valid bracketing interval; it would already have terminated due to the failed bracket condition.\n\nTherefore, the expected behavior is immediate termination with an error due to the lack of a bracketing interval.", "answer": "$$\\boxed{A}$$", "id": "2157790"}, {"introduction": "Having established the importance of a valid starting interval, we now turn to the mechanics of the algorithm itself. This exercise [@problem_id:2157795] provides a hands-on opportunity to perform a few iterations of Brent's method, tracing its hybrid logic on the well-known Bessel function. You will see firsthand how the method attempts a fast interpolation step and then decides whether to accept it or fall back to the slower but guaranteed bisection method, demonstrating the algorithm's intelligent balance of speed and reliability.", "problem": "Brent's method is a popular root-finding algorithm that combines the bisection method, the secant method, and inverse quadratic interpolation. Consider the function $f(x) = J_0(x)$, where $J_0(x)$ is the zeroth-order Bessel function of the first kind. We want to find the first positive root of this function.\n\nYou are given an initial bracketing interval for the root $[a_0, b_0] = [2.0, 3.0]$. Your task is to apply Brent's method for two full iterations to find a more accurate estimate of the root.\n\nThe logic of the method you must follow is:\n1.  At each step, with a current bracketing interval $[a, b]$ and a third point $c$ (the previous value of $b$), first attempt to use inverse quadratic interpolation with the points $(a, f(a))$, $(b, f(b))$, and $(c, f(c))$ to find a new estimate, $s$. This is only possible if $f(a)$, $f(b)$, and $f(c)$ are distinct.\n2.  If inverse quadratic interpolation is not possible or its result is not accepted, fall back to the secant method using the points $(a, f(a))$ and $(b, f(b))$ to find the estimate $s$.\n3.  The new estimate $s$ from either interpolation or the secant method is only accepted if it lies between $\\frac{3a+b}{4}$ and $b$.\n4.  If the estimate $s$ is not accepted, the bisection method is used instead, where the new estimate is the midpoint $s = \\frac{a+b}{2}$.\n5.  After finding the new estimate $s$, the interval $[a, b]$ and the point $c$ are updated for the next iteration based on the signs of the function values. The new interval must always bracket the root.\n\nTo perform the calculation, use the following pre-computed function values:\n- $J_0(2.0) = 0.22389$\n- $J_0(3.0) = -0.26005$\n- $J_0(2.46264) = -0.02495$\n- $J_0(2.41146) = -0.00282$\n\nCalculate the value of the best root estimate after two full iterations of the method have been completed. The best estimate is the endpoint of the final bracketing interval whose function value is closer to zero. Report your final answer rounded to four significant figures.", "solution": "We apply Brent’s method exactly as specified, using only the provided function values.\n\nLet $f(x)=J_0(x)$. Initially, $a_0=2.0$, $b_0=3.0$, with $f(a_0)=0.22389>0$ and $f(b_0)=-0.26005<0$, so the interval $[a_0,b_0]$ brackets a root. For the first iteration, we take $c_0=a_0$ (so inverse quadratic interpolation is not possible because $a_0=c_0$).\n\nIteration 1:\n- Since inverse quadratic interpolation is not possible, use the secant method with $(a_0,f(a_0))$ and $(b_0,f(b_0))$:\n$$\ns_1\n= b_0 - f(b_0)\\frac{b_0-a_0}{f(b_0)-f(a_0)}\n= 3.0 - \\left(-0.26005\\right)\\frac{3.0-2.0}{-0.26005-0.22389}.\n$$\nCompute the denominator $f(b_0)-f(a_0)=-0.48394$, so\n$$\ns_1=3.0 - \\frac{-0.26005}{-0.48394}=3.0-0.53736=2.46264.\n$$\n- Acceptance check: $s_1$ must satisfy $\\frac{3a_0+b_0}{4} \\leq s_1 \\leq b_0$. Here, $\\frac{3\\cdot 2.0 + 3.0}{4}=2.25$, and $2.25 \\leq 2.46264 \\leq 3.0$, so $s_1$ is accepted.\n- Update the bracketing interval using signs: $f(a_0)\\cdot f(s_1)=0.22389\\cdot(-0.02495)<0$, so the new bracket is $[a_1,b_1]=[a_0,s_1]=[2.0,2.46264]$. Set $c_1=b_0=3.0$. The provided value $f(b_1)=f(2.46264)=-0.02495$ will be used.\n\nIteration 2:\n- Attempt inverse quadratic interpolation with $(a_1,f(a_1))=(2.0,0.22389)$, $(b_1,f(b_1))=(2.46264,-0.02495)$, and $(c_1,f(c_1))=(3.0,-0.26005)$, which have distinct function values. Using inverse quadratic interpolation (interpolating $x$ as a quadratic function of $y=f(x)$ and evaluating at $y=0$), the formula is\n$$\ns_2\n= x_{0}\\frac{f_{1}f_{2}}{(f_{0}-f_{1})(f_{0}-f_{2})}\n+ x_{1}\\frac{f_{0}f_{2}}{(f_{1}-f_{0})(f_{1}-f_{2})}\n+ x_{2}\\frac{f_{0}f_{1}}{(f_{2}-f_{0})(f_{2}-f_{1})},\n$$\nwith $(x_0,f_0)=(a_1,f(a_1))=(2.0,0.22389)$, $(x_1,f_1)=(b_1,f(b_1))=(2.46264,-0.02495)$, and $(x_2,f_2)=(c_1,f(c_1))=(3.0,-0.26005)$.\nUsing the provided values (and arithmetic consistent with them), this yields\n$$\ns_2=2.41146.\n$$\n- Acceptance check: $s_2$ must satisfy $\\frac{3a_1+b_1}{4} \\leq s_2 \\leq b_1$. Here, $\\frac{3\\cdot 2.0 + 2.46264}{4}=\\frac{8.46264}{4}=2.11566$, and $2.11566 \\leq 2.41146 \\leq 2.46264$, so $s_2$ is accepted.\n- Update the bracketing interval using signs: $f(a_1)\\cdot f(s_2)=0.22389\\cdot(-0.00282)<0$, so the new bracket is $[a_2,b_2]=[a_1,s_2]=[2.0,2.41146]$. The provided value $f(b_2)=f(2.41146)=-0.00282$ will be used.\n\nAfter two full iterations, the final bracketing interval is $[2.0,2.41146]$ with function values $f(2.0)=0.22389$ and $f(2.41146)=-0.00282$. The best estimate is the endpoint whose function value is closer to zero, namely $b_2=2.41146$. Rounding to four significant figures gives $2.411$.", "answer": "$$\\boxed{2.411}$$", "id": "2157795"}, {"introduction": "The true elegance of Brent's method lies in its robustness—its ability to succeed even with difficult functions where simpler methods might fail or converge slowly. This problem [@problem_id:2157793] explores such a case, where the function has a vertical tangent at its root, a feature that violates the smoothness assumptions underlying fast interpolation techniques. By analyzing this scenario, you will gain a deeper appreciation for why the bisection fallback is a crucial component of the algorithm's design, ensuring convergence even when the faster methods struggle.", "problem": "Brent's method is a popular root-finding algorithm that combines the reliability of the bisection method with the speed of interpolation-based techniques, namely the secant method and inverse quadratic interpolation. The algorithm attempts to use the faster interpolation methods when possible but falls back to the bisection method if the interpolation steps are not satisfactory.\n\nConsider the function $f(x) = \\text{sign}(x-2) \\sqrt{|x-2|}$, which has a root at $x=2$. When applying Brent's method to find this root, the interpolation-based steps are observed to perform poorly, causing the algorithm to rely heavily on the bisection fallback and thus converge much more slowly than expected.\n\nWhich of the following statements provides the most accurate mathematical reason for the poor performance of the interpolation-based steps (secant method and inverse quadratic interpolation) for this specific function near its root?\n\nA. The function is discontinuous at the root $x=2$.\n\nB. The function has an odd symmetry with respect to the root, i.e., $f(2+h) = -f(2-h)$, which forces the algorithm to take symmetric but slow steps.\n\nC. The function's first derivative is undefined at the root, approaching infinity as $x$ approaches the root.\n\nD. The function's second derivative is zero at the root, indicating an inflection point that slows down convergence.\n\nE. The algorithm fails because it can only be applied to polynomials, but the function involves a square root term.", "solution": "The goal is to understand why the interpolation-based components of Brent's method (secant method and inverse quadratic interpolation) perform poorly for the function $f(x) = \\text{sign}(x-2) \\sqrt{|x-2|}$ near its root $x=2$.\n\nFirst, let's analyze the function $f(x)$. The function is defined using the sign function and the absolute value. We can express it in a piecewise form to better understand its behavior around the root $x=2$.\n- If $x > 2$, then $x-2 > 0$, so $\\text{sign}(x-2) = 1$ and $|x-2| = x-2$. Thus, $f(x) = \\sqrt{x-2}$.\n- If $x < 2$, then $x-2 < 0$, so $\\text{sign}(x-2) = -1$ and $|x-2| = -(x-2) = 2-x$. Thus, $f(x) = -\\sqrt{2-x}$.\n- If $x = 2$, then $f(2) = \\text{sign}(0)\\sqrt{0} = 0 \\times 0 = 0$.\n\nThe interpolation methods used in Brent's algorithm, the secant method and inverse quadratic interpolation, achieve their fast convergence rates by assuming that the function is locally smooth and can be well-approximated by a low-degree polynomial (a line for the secant method, a quadratic for inverse quadratic interpolation). A key indicator of this smoothness is the behavior of the function's derivatives near the root.\n\nLet's compute the first derivative, $f'(x)$, for $x \\neq 2$.\n- For $x > 2$, $f'(x) = \\frac{d}{dx}(\\sqrt{x-2}) = \\frac{d}{dx}((x-2)^{1/2}) = \\frac{1}{2}(x-2)^{-1/2} = \\frac{1}{2\\sqrt{x-2}}$.\n- For $x < 2$, $f'(x) = \\frac{d}{dx}(-\\sqrt{2-x}) = -\\frac{1}{2}(2-x)^{-1/2}(-1) = \\frac{1}{2\\sqrt{2-x}}$.\n\nNow, we examine the limit of the derivative as $x$ approaches the root at $2$.\n- As $x \\to 2$ from the right ($x \\to 2^+$), the denominator $2\\sqrt{x-2} \\to 0^+$. Thus, $\\lim_{x\\to 2^+} f'(x) = +\\infty$.\n- As $x \\to 2$ from the left ($x \\to 2^-$), the denominator $2\\sqrt{2-x} \\to 0^+$. Thus, $\\lim_{x\\to 2^-} f'(x) = +\\infty$.\n\nSince the limit of the derivative approaches infinity from both sides, the derivative $f'(2)$ is undefined. Geometrically, this means the function has a vertical tangent at its root $x=2$.\n\nThis vertical tangent is the primary reason for the failure of the interpolation methods:\n1.  **Secant Method:** This method approximates the function with a secant line between two points $(x_n, f(x_n))$ and $(x_{n-1}, f(x_{n-1}))$. When these points are close to the root at $x=2$, the function is nearly vertical. The secant line will also be nearly vertical, and its intersection with the x-axis (the next root estimate) can be highly sensitive to small changes in the points, leading to inaccurate or unstable steps. The fundamental assumption of local linearity is violated.\n2.  **Inverse Quadratic Interpolation (IQI):** This method fits a quadratic polynomial to the *inverse* function, $x = f^{-1}(y)$. If the original function $f(x)$ has an infinite derivative at the root, the inverse function will have a derivative of zero. The derivative of the inverse is $(f^{-1})'(y) = 1/f'(x)$. As $x \\to 2$, $f'(x) \\to \\infty$, so $(f^{-1})'(0) = 0$. Let's look at the second derivative of the inverse. The inverse function is $x = 2+y|y|$. Its first derivative is $x'(y) = 2|y|$, and its second derivative is $x''(y) = 2 \\text{sign}(y)$, which has a jump discontinuity at $y=0$. A function with a discontinuous second derivative cannot be well-approximated by a single quadratic polynomial near that point.\n\nBecause both interpolation schemes fail to produce reliable and fast-converging estimates, Brent's method's logic will repeatedly reject their outputs and fall back to the guaranteed, but slow, bisection method.\n\nNow we evaluate the given options:\nA. The function is discontinuous at the root $x=2$. This is false. As shown above, $\\lim_{x\\to 2} f(x) = 0 = f(2)$, so the function is continuous.\nB. The function has an odd symmetry with respect to the root. This statement is true, as $f(2+h) = \\text{sign}(h)\\sqrt{|h|}$ and $-f(2-h) = -(\\text{sign}(-h)\\sqrt{|-h|}) = -(-\\text{sign}(h)\\sqrt{|h|}) = \\text{sign}(h)\\sqrt{|h|}$. However, this is not the cause of the poor performance. For example, $g(x) = x-2$ also has this symmetry, and Brent's method works perfectly.\nC. The function's first derivative is undefined at the root, approaching infinity as $x$ approaches the root. This is true, as demonstrated by our calculation. This infinite derivative (vertical tangent) violates the smoothness assumptions underlying the interpolation methods, causing them to perform poorly. This is the correct explanation.\nD. The function's second derivative is zero at the root. This is false. The first derivative is not even defined at the root, so the second derivative cannot be. Furthermore, for $x>2$, $f''(x) = -\\frac{1}{4}(x-2)^{-3/2}$, which approaches $-\\infty$ as $x \\to 2^+$.\nE. The algorithm fails because it can only be applied to polynomials. This is false. Brent's method is a general-purpose root-finder applicable to a wide range of continuous functions.\n\nTherefore, the most accurate reason for the poor performance is the infinite derivative at the root.", "answer": "$$\\boxed{C}$$", "id": "2157793"}]}