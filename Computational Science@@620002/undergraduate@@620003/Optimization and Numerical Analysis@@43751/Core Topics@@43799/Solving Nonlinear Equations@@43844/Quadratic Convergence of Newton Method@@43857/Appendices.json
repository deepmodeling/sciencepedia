{"hands_on_practices": [{"introduction": "To truly appreciate the power of Newton's method, we must move beyond theory and see its rapid convergence in action. This first exercise provides a direct, hands-on calculation of the quantities that define quadratic convergence. By performing a single iteration on a simple function and computing the ratio of the new error to the square of the old error, you will obtain a concrete value for the asymptotic error constant, offering a tangible glimpse into why the number of correct digits can roughly double with each step. [@problem_id:2195730]", "problem": "In numerical analysis, Newton's method is an iterative procedure for finding successively better approximations to the roots (or zeroes) of a real-valued function. For a function $f(x)$ with a simple root $r$ (i.e., $f(r)=0$ and $f'(r) \\neq 0$), the error at iteration $n$, defined as $e_n = |x_n - r|$, often satisfies the relationship $e_{n+1} \\approx C e_n^2$ for some constant $C$, a property known as quadratic convergence. This implies that the ratio $K_n = \\frac{e_{n+1}}{e_n^2}$ should approach a constant value as the number of iterations increases.\n\nConsider the task of finding the real root of the function $f(x) = x^3 - 8$. The exact real root of this function is $r=2$. We will analyze the convergence behavior of Newton's method starting from an initial guess $x_0 = 3$.\n\nCalculate the specific value of the ratio $K_0 = \\frac{e_1}{e_0^2}$, where $e_0$ is the initial error and $e_1$ is the error after the first complete iteration of Newton's method. Report your answer as a decimal rounded to four significant figures.", "solution": "The goal is to calculate the ratio $K_0 = \\frac{e_1}{e_0^2}$. To do this, we need to find the initial error $e_0$ and the error after the first iteration, $e_1$.\n\nFirst, we define the necessary components for Newton's method. The iterative formula is:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\nThe given function is $f(x) = x^3 - 8$.\nThe derivative of the function is $f'(x) = 3x^2$.\n\nNext, we calculate the initial error, $e_0$.\nThe problem states the initial guess is $x_0 = 3$ and the true root is $r = 2$.\nThe initial error $e_0$ is defined as the absolute difference between the initial guess and the true root:\n$$e_0 = |x_0 - r| = |3 - 2| = 1$$\n\nNow, we must perform one iteration of Newton's method to find $x_1$. We use the iterative formula with $n=0$:\n$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\nWe evaluate the function and its derivative at the initial guess $x_0 = 3$:\n$$f(x_0) = f(3) = (3)^3 - 8 = 27 - 8 = 19$$\n$$f'(x_0) = f'(3) = 3(3)^2 = 3(9) = 27$$\nSubstituting these values into the formula for $x_1$:\n$$x_1 = 3 - \\frac{19}{27} = \\frac{3 \\times 27}{27} - \\frac{19}{27} = \\frac{81 - 19}{27} = \\frac{62}{27}$$\n\nWith the value of $x_1$, we can now calculate the error after the first iteration, $e_1$.\nThe error $e_1$ is defined as the absolute difference between the first iterate and the true root:\n$$e_1 = |x_1 - r| = \\left|\\frac{62}{27} - 2\\right| = \\left|\\frac{62}{27} - \\frac{2 \\times 27}{27}\\right| = \\left|\\frac{62 - 54}{27}\\right| = \\frac{8}{27}$$\n\nFinally, we calculate the desired ratio $K_0 = \\frac{e_1}{e_0^2}$.\nSubstituting the values we found for $e_0$ and $e_1$:\n$$K_0 = \\frac{e_1}{e_0^2} = \\frac{8/27}{(1)^2} = \\frac{8}{27}$$\n\nThe problem asks for the answer as a decimal rounded to four significant figures. We convert the fraction to a decimal:\n$$K_0 = \\frac{8}{27} \\approx 0.296296296...$$\nRounding this to four significant figures gives:\n$$K_0 \\approx 0.2963$$", "answer": "$$\\boxed{0.2963}$$", "id": "2195730"}, {"introduction": "While Newton's method is famous for its speed, quadratic convergence is not a universal guarantee; it depends critically on the nature of the function at its root. This problem challenges you to act as a diagnostician, examining three different functions to determine which will support quadratic convergence. The key lies in investigating the derivative of the function at the root ($f'(r) \\neq 0$), a condition that separates simple roots from multiple roots and governs the ultimate speed of the method. [@problem_id:2195658]", "problem": "Newton's method is an iterative procedure used to find successively better approximations to the roots (or zeros) of a real-valued function. The convergence rate of this method is sensitive to the nature of the root. A key performance metric is quadratic convergence, where the number of correct digits roughly doubles with each iteration, provided the initial guess is sufficiently close to the root.\n\nConsider the following three functions, each of which has a root at $x=0$:\n1.  $f_1(x) = x^{3}$\n2.  $f_2(x) = \\cos(x) - 1$\n3.  $f_3(x) = \\exp(x) - 1$\n\nAssuming an initial guess is chosen appropriately close to the root at $x=0$, for which of these functions will Newton's method exhibit quadratic convergence?\n\nA. Function $f_1(x)$ only\n\nB. Function $f_2(x)$ only\n\nC. Function $f_3(x)$ only\n\nD. Functions $f_1(x)$ and $f_2(x)$\n\nE. All three functions", "solution": "Newton’s method for solving $f(x)=0$ is defined by the iteration\n$$\nx_{k+1}=x_{k}-\\frac{f(x_{k})}{f'(x_{k})}.\n$$\nA standard local convergence result states: if $f$ is twice continuously differentiable near a root $\\alpha$ and $f'(\\alpha)\\neq 0$, then for initial $x_{0}$ sufficiently close to $\\alpha$, Newton’s method converges quadratically to $\\alpha$. If instead $f'(\\alpha)=0$ (i.e., the root has multiplicity greater than $1$), then Newton’s method does not have quadratic convergence; for a root of multiplicity $m>1$, the convergence is typically linear with asymptotic error reduction factor $(m-1)/m$.\n\nWe analyze each function at the root $\\alpha=0$.\n\n1) For $f_{1}(x)=x^{3}$, we have $f_{1}'(x)=3x^{2}$. The Newton iterate is\n$$\nx_{k+1}=x_{k}-\\frac{x_{k}^{3}}{3x_{k}^{2}}=x_{k}-\\frac{x_{k}}{3}=\\frac{2}{3}x_{k}.\n$$\nLet $e_{k}=x_{k}-0=x_{k}$. Then\n$$\ne_{k+1}=\\frac{2}{3}e_{k},\n$$\nwhich is linear convergence (error is reduced by a constant factor), not quadratic. This aligns with the fact that $f_{1}'(0)=0$ and the root has multiplicity $3$.\n\n2) For $f_{2}(x)=\\cos(x)-1$, we have $f_{2}'(x)=-\\sin(x)$. The Newton iterate is\n$$\nx_{k+1}=x_{k}-\\frac{\\cos(x_{k})-1}{-\\sin(x_{k})}=x_{k}+\\frac{\\cos(x_{k})-1}{\\sin(x_{k})}.\n$$\nUsing trigonometric identities,\n$$\n\\cos(x)-1=-2\\sin^{2}\\!\\left(\\frac{x}{2}\\right),\\quad \\sin(x)=2\\sin\\!\\left(\\frac{x}{2}\\right)\\cos\\!\\left(\\frac{x}{2}\\right),\n$$\nso\n$$\n\\frac{\\cos(x)-1}{\\sin(x)}=-\\tan\\!\\left(\\frac{x}{2}\\right).\n$$\nTherefore,\n$$\nx_{k+1}=x_{k}-\\tan\\!\\left(\\frac{x_{k}}{2}\\right).\n$$\nFor small $x_{k}$, the Taylor expansion $\\tan(y)=y+\\frac{y^{3}}{3}+O(y^{5})$ yields\n$$\n\\tan\\!\\left(\\frac{x_{k}}{2}\\right)=\\frac{x_{k}}{2}+\\frac{x_{k}^{3}}{24}+O(x_{k}^{5}),\n$$\nhence\n$$\nx_{k+1}=x_{k}-\\left(\\frac{x_{k}}{2}+\\frac{x_{k}^{3}}{24}+O(x_{k}^{5})\\right)=\\frac{1}{2}x_{k}-\\frac{1}{24}x_{k}^{3}+O(x_{k}^{5}).\n$$\nWith $e_{k}=x_{k}$, this gives\n$$\ne_{k+1}=\\frac{1}{2}e_{k}+O(e_{k}^{3}),\n$$\nwhich is linear convergence, not quadratic. This is consistent with $f_{2}'(0)=0$ and the root’s effective multiplicity $2$ (since $f_{2}''(0)=-1\\neq 0$).\n\n3) For $f_{3}(x)=\\exp(x)-1$, we have $f_{3}'(x)=\\exp(x)$, so $f_{3}'(0)=1\\neq 0$. The Newton iterate is\n$$\nx_{k+1}=x_{k}-\\frac{\\exp(x_{k})-1}{\\exp(x_{k})}=x_{k}-\\left(1-\\exp(-x_{k})\\right)=x_{k}-1+\\exp(-x_{k}).\n$$\nUsing the Taylor expansion $\\exp(-x)=1-x+\\frac{x^{2}}{2}-\\frac{x^{3}}{6}+O(x^{4})$, we obtain\n$$\nx_{k+1}=x_{k}-1+\\left(1-x_{k}+\\frac{x_{k}^{2}}{2}-\\frac{x_{k}^{3}}{6}+O(x_{k}^{4})\\right)=\\frac{x_{k}^{2}}{2}-\\frac{x_{k}^{3}}{6}+O(x_{k}^{4}).\n$$\nLetting $e_{k}=x_{k}$, we have\n$$\ne_{k+1}=\\frac{1}{2}e_{k}^{2}+O(e_{k}^{3}),\n$$\nwhich demonstrates quadratic convergence. This matches the general criterion since $f_{3}'(0)\\neq 0$ and $f_{3}$ is smooth.\n\nTherefore, among the given functions, only $f_{3}(x)=\\exp(x)-1$ yields quadratic convergence of Newton’s method near $x=0$.", "answer": "$$\\boxed{C}$$", "id": "2195658"}, {"introduction": "Understanding the conditions for quadratic convergence leads to a deeper question: what happens if those conditions are altered? This problem explores a practical thought experiment where the Newton iteration is \"simplified\" by fixing the derivative value instead of recalculating it at each step. By analyzing the convergence rate of this modified method, you will gain a profound appreciation for why the original formulation is so powerful and see firsthand how a seemingly minor change degrades the convergence from quadratic to linear. [@problem_id:2195669]", "problem": "A student is implementing an algorithm to compute the square root of a positive number $A$. They start with the function $f(x) = x^2 - A$, for which the positive root is $\\alpha = \\sqrt{A}$. To find this root, they decide to use an iterative method. Recalling Newton's method, $x_{n+1} = x_n - f(x_n)/f'(x_n)$, they realize that computing the derivative $f'(x_n) = 2x_n$ at every step might be computationally expensive.\n\nTo optimize their code, they propose a modified method where the derivative is calculated only once at the beginning of the iteration, using their initial guess $x_0$, and then treated as a constant throughout. The resulting iterative scheme is:\n$$x_{n+1} = x_n - \\frac{x_n^2 - A}{f'(x_0)}$$\nwhere $x_0$ is the initial guess, and it is assumed that $x_0 > 0$ and $x_0 \\neq \\sqrt{A}$.\n\nThis defines a fixed-point iteration $x_{n+1} = g(x_n)$. Assuming the sequence of iterates $x_n$ converges to the root $\\alpha = \\sqrt{A}$, determine the order of convergence, $p$, and the asymptotic error constant, $\\lambda$. Present your answer as a row matrix containing the pair $(p, \\lambda)$.", "solution": "We are given $f(x) = x^{2} - A$ with the positive root $\\alpha = \\sqrt{A}$ and the modified iteration\n$$\nx_{n+1} = x_{n} - \\frac{x_{n}^{2} - A}{f'(x_{0})} = x_{n} - \\frac{x_{n}^{2} - A}{2 x_{0}},\n$$\nwhich defines a fixed-point iteration $x_{n+1} = g(x_{n})$ with\n$$\ng(x) = x - \\frac{x^{2} - A}{2 x_{0}}.\n$$\nFirst, verify that $\\alpha$ is a fixed point:\n$$\ng(\\alpha) = \\alpha - \\frac{\\alpha^{2} - A}{2 x_{0}} = \\alpha.\n$$\nLet $e_{n} = x_{n} - \\alpha$ denote the error. Using the algebraic identity $x_{n}^{2} - \\alpha^{2} = (x_{n} - \\alpha)(x_{n} + \\alpha)$, we obtain an exact error recursion:\n$$\ne_{n+1} = x_{n+1} - \\alpha = x_{n} - \\frac{x_{n}^{2} - A}{2 x_{0}} - \\alpha = e_{n} - \\frac{(x_{n} - \\alpha)(x_{n} + \\alpha)}{2 x_{0}} = e_{n}\\left(1 - \\frac{x_{n} + \\alpha}{2 x_{0}}\\right).\n$$\nSince $x_{n} \\to \\alpha$ by assumption, we write $x_{n} = \\alpha + e_{n}$ and substitute:\n$$\ne_{n+1} = e_{n}\\left(1 - \\frac{2\\alpha + e_{n}}{2 x_{0}}\\right) = \\left(1 - \\frac{\\alpha}{x_{0}}\\right)e_{n} - \\frac{1}{2 x_{0}} e_{n}^{2}.\n$$\nAs $n \\to \\infty$, the linear term dominates because $x_{0} \\neq \\alpha$ implies $1 - \\alpha/x_{0} \\neq 0$. Therefore, the convergence is linear with order $p = 1$, and the asymptotic error constant is\n$$\n\\lambda = \\lim_{n \\to \\infty} \\frac{|e_{n+1}|}{|e_{n}|} = \\left|1 - \\frac{\\alpha}{x_{0}}\\right| = \\left|1 - \\frac{\\sqrt{A}}{x_{0}}\\right|.\n$$\nEquivalently, this conclusion follows from the fixed-point derivative $g'(x) = 1 - x/x_{0}$, so $g'(\\alpha) = 1 - \\alpha/x_{0} \\neq 0$, which yields linear convergence with $\\lambda = |g'(\\alpha)|$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & \\left|1 - \\frac{\\sqrt{A}}{x_{0}}\\right| \\end{pmatrix}}$$", "id": "2195669"}]}