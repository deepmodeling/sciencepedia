{"hands_on_practices": [{"introduction": "Let's begin with a classic textbook example to build our intuition for the Fixed-Point Theorem. This practice involves verifying the convergence conditions for the iteration $x_{k+1} = \\cos(x_k)$, a simple-looking but non-trivial problem [@problem_id:2162898]. By methodically checking that the function $g(x)=\\cos(x)$ maps the interval $[0, 1]$ to itself and that it is a contraction on this interval, you will solidify your understanding of the two core requirements for guaranteed convergence.", "problem": "A numerical analyst is tasked with finding the root of the equation $f(x) = x - \\cos(x) = 0$. They decide to use a fixed-point iteration method by rearranging the equation into the form $x = g(x)$. The chosen iteration scheme is defined as $x_{k+1} = \\cos(x_k)$, with an initial guess $x_0$ chosen from the interval $[0, 1]$. All calculations involving trigonometric functions assume the angles are measured in radians.\n\nWhich of the following statements correctly analyzes whether this iterative scheme is guaranteed to converge to the unique fixed point within the interval $[0, 1]$ for any arbitrary choice of $x_0 \\in [0, 1]$?\n\nA. Convergence is guaranteed because the function $g(x) = \\cos(x)$ maps the interval $[0, 1]$ to itself, and the magnitude of its derivative is strictly less than 1 for all points within this interval.\n\nB. Convergence is not guaranteed because the function $g(x) = \\cos(x)$ does not map the interval $[0, 1]$ to itself; for some values of $x$ in the interval, $g(x)$ falls outside of $[0, 1]$.\n\nC. Convergence is not guaranteed because the condition on the derivative is not met; there exists at least one point in the interval $[0, 1]$ where the magnitude of the derivative of $g(x) = \\cos(x)$ is equal to or greater than 1.\n\nD. Convergence is only guaranteed for specific starting points like $x_0=0.5$, but not for all starting points in the interval $[0, 1]$, as the conditions for global convergence on the interval are not fully satisfied.\n\nE. Convergence is guaranteed because the function $g(x) = \\cos(x)$ is continuous on the interval $[0, 1]$. The derivative condition is only relevant for determining the rate of convergence, not for guaranteeing it.", "solution": "We consider the fixed-point iteration $x_{k+1}=g(x_{k})$ with $g(x)=\\cos(x)$ on the interval $[0,1]$. To guarantee convergence to a unique fixed point for any $x_{0}\\in[0,1]$, we verify the conditions of the Banach fixed-point theorem: (i) $g$ maps $[0,1]$ into itself, and (ii) $g$ is a contraction on $[0,1]$.\n\nFirst, we show $g([0,1])\\subseteq[0,1]$. Since $g'(x)=-\\sin(x)\\leq 0$ on $[0,1]$, $g$ is nonincreasing on $[0,1]$. Therefore,\n$$\ng([0,1])=[g(1),g(0)]=[\\cos(1),\\cos(0)]=[\\cos(1),1].\n$$\nBecause $1 \\lt \\frac{\\pi}{2}$, we have $\\cos(1) \\gt 0$, hence $[\\cos(1),1]\\subseteq[0,1]$. Thus, $g$ maps the interval to itself.\n\nSecond, we show $g$ is a contraction on $[0,1]$. The derivative is $g'(x)=-\\sin(x)$, so\n$$\n|g'(x)|=|\\sin(x)|\\leq \\sin(1) \\lt 1 \\quad \\text{for all } x\\in[0,1],\n$$\nsince $0 \\leq x \\leq 1 \\lt \\frac{\\pi}{2}$ and $\\sin$ is strictly increasing on $[0, \\frac{\\pi}{2}]$ with $\\sin\\!\\left(\\frac{\\pi}{2}\\right)=1$. By the mean value theorem, for all $x,y\\in[0,1]$,\n$$\n|g(x)-g(y)|\\leq \\sup_{z\\in[0,1]}|g'(z)|\\,|x-y|\\leq \\sin(1)\\,|x-y|,\n$$\nwith a Lipschitz constant $L=\\sin(1)$ satisfying $0 \\leq L \\lt 1$. Therefore, $g$ is a contraction on $[0,1]$.\n\nBy the Banach fixed-point theorem on the complete metric space $([0,1],|\\cdot|)$, there exists a unique fixed point $x^{\\ast}\\in[0,1]$ such that $x^{\\ast}=\\cos(x^{\\ast})$, and for any $x_{0}\\in[0,1]$, the sequence defined by $x_{k+1}=\\cos(x_{k})$ converges to $x^{\\ast}$.\n\nTherefore, the correct statement is that convergence is guaranteed because $g$ maps $[0,1]$ into itself and satisfies $|g'(x)| \\lt 1$ on $[0,1]$. This corresponds to option A. The alternatives are incorrect: B is false since $g([0,1])\\subseteq[0,1]$; C is false because $\\sup_{[0,1]}|g'|=\\sin(1) \\lt 1$; D is false because the contraction property ensures convergence for all $x_{0}\\in[0,1]$; E is false because continuity alone does not guarantee convergence without a contraction condition.", "answer": "$$\\boxed{A}$$", "id": "2162898"}, {"introduction": "Not all fixed points are created equal; some attract nearby iterations while others repel them, and the convergence of an iteration is often a local property. This exercise explores this crucial distinction by analyzing an iteration with two different fixed points [@problem_id:2162900]. Your task is to determine for which specific interval the conditions for convergence are met, highlighting that a careful choice of the domain is paramount for ensuring the iteration will succeed.", "problem": "In a simplified model of a reversible chemical process, the dimensionless equilibrium concentration, $c$, of a certain product can be found by solving for the fixed points of the function $g(c) = \\frac{1}{3}(c^2+1)$. A numerical approach to find this concentration is the fixed-point iteration method, defined by the sequence $c_{n+1} = g(c_n)$ for $n \\ge 0$, starting with an initial guess $c_0$.\n\nThe convergence of this iteration to a unique fixed point within an interval $[a, b]$ is guaranteed by the Fixed-Point Theorem if two conditions are met:\n1.  For all $c \\in [a, b]$, the value of $g(c)$ is also in $[a, b]$ (i.e., $g$ maps the interval into itself).\n2.  There exists a constant $k \\lt 1$ such that the absolute value of the derivative, $|g'(c)|$, is less than or equal to $k$ for all $c \\in [a, b]$.\n\nThe function $g(c)$ has two distinct positive fixed points. Your task is to identify which one of the following intervals satisfies both conditions of the Fixed-Point Theorem, thereby guaranteeing convergence to the fixed point contained within that interval.\n\nWhich of the following intervals $[a, b]$ satisfies both conditions for convergence?\n\nA. $[0, 1.2]$\n\nB. $[2.5, 2.7]$\n\nC. $[1.4, 1.6]$\n\nD. $[-1, 2]$", "solution": "We are given the fixed-point iteration $c_{n+1} = g(c_{n})$ with $g(c) = \\frac{1}{3}\\left(c^{2} + 1\\right)$. The Fixed-Point (Banach) Contraction Theorem guarantees existence, uniqueness, and convergence to the fixed point in an interval $[a,b]$ if:\n1) $g([a,b]) \\subseteq [a,b]$ (interval is mapped into itself), and\n2) $\\sup_{c \\in [a,b]} |g'(c)| \\le k \\lt 1$ (contraction on $[a,b]$).\n\nCompute the derivative:\n$$\ng'(c) = \\frac{2}{3}c.\n$$\nHence, on any interval $[a,b]$, we have\n$$\n\\sup_{c \\in [a,b]} |g'(c)| = \\frac{2}{3}\\max\\{|a|,|b|\\}.\n$$\nThus the contraction condition holds if and only if $\\max\\{|a|,|b|\\} \\lt \\frac{3}{2}$.\n\nWe now check each option for both conditions.\n\nOption A: $[0,1.2] = \\left[0,\\frac{6}{5}\\right]$.\n- Contraction: \n$$\n\\sup_{c \\in [0,6/5]} |g'(c)| = \\frac{2}{3}\\cdot \\frac{6}{5} = \\frac{4}{5} \\lt 1,\n$$\nso the contraction condition holds.\n- Mapping into itself: Since $g$ is even and increasing on $[0,\\infty)$,\n$$\ng([0,6/5]) = [g(0),g(6/5)].\n$$\nCompute the endpoints:\n$$\ng(0) = \\frac{1}{3} \\ge 0,\\qquad g\\!\\left(\\frac{6}{5}\\right) = \\frac{1}{3}\\left(1 + \\left(\\frac{6}{5}\\right)^{2}\\right) = \\frac{1}{3}\\left(1 + \\frac{36}{25}\\right) = \\frac{61}{75}.\n$$\nSince $0 \\le \\frac{1}{3} \\le \\frac{61}{75} \\lt \\frac{6}{5}$ (because $61 \\lt 90$), it follows that\n$$\ng([0,6/5]) \\subseteq [0,6/5].\n$$\nBoth conditions are satisfied for A.\n\nOption B: $[2.5,2.7] = \\left[\\frac{5}{2},\\frac{27}{10}\\right]$.\n- Contraction:\n$$\n\\sup_{c \\in [5/2,27/10]} |g'(c)| = \\frac{2}{3}\\cdot \\frac{27}{10} = \\frac{27}{15} \\gt 1,\n$$\nso the contraction condition fails.\n- Mapping into itself also fails, since\n$$\ng\\!\\left(\\frac{5}{2}\\right) = \\frac{1}{3}\\left(1 + \\frac{25}{4}\\right) = \\frac{29}{12} \\lt \\frac{5}{2},\\quad\ng\\!\\left(\\frac{27}{10}\\right) = \\frac{1}{3}\\left(1 + \\frac{729}{100}\\right) = \\frac{829}{300} \\gt \\frac{27}{10},\n$$\nhence $g([5/2,27/10]) \\not\\subseteq [5/2,27/10]$.\n\nOption C: $[1.4,1.6] = \\left[\\frac{7}{5},\\frac{8}{5}\\right]$.\n- Contraction:\n$$\n\\sup_{c \\in [7/5,8/5]} |g'(c)| = \\frac{2}{3}\\cdot \\frac{8}{5} = \\frac{16}{15} \\gt 1,\n$$\nso the contraction condition fails.\n- Mapping into itself also fails since\n$$\ng\\!\\left(\\frac{7}{5}\\right) = \\frac{1}{3}\\left(1 + \\frac{49}{25}\\right) = \\frac{74}{75} \\lt \\frac{7}{5},\n$$\nhence $g([7/5,8/5]) \\not\\subseteq [7/5,8/5]$.\n\nOption D: $[-1,2]$.\n- Contraction:\n$$\n\\sup_{c \\in [-1,2]} |g'(c)| = \\frac{2}{3}\\cdot 2 = \\frac{4}{3} \\gt 1,\n$$\nso the contraction condition fails.\n- Mapping into itself holds because $g$ is even and increasing in $|c|$:\n$$\ng([-1,2]) = [g(0),g(2)] = \\left[\\frac{1}{3},\\frac{1}{3}(1+4)\\right] = \\left[\\frac{1}{3},\\frac{5}{3}\\right] \\subseteq [-1,2],\n$$\nbut without contraction this option does not satisfy the theoremâ€™s conditions.\n\nTherefore, only option A satisfies both the mapping and contraction conditions, guaranteeing convergence to the fixed point in that interval.", "answer": "$$\\boxed{A}$$", "id": "2162900"}, {"introduction": "Having learned to verify convergence, we now shift our focus to optimizing it. This practice challenges you to design an iteration scheme by selecting a parameter $c$ that ensures the fastest possible local convergence to a root [@problem_id:2162905]. By manipulating the derivative of the iteration function $g(x)$ to be zero at the fixed point, you will discover the principle behind creating more efficient, quadratically convergent methods, which form the bedrock of many powerful numerical algorithms.", "problem": "A numerical procedure for finding a root of an equation $f(x)=0$ involves creating a fixed-point iteration scheme of the form $x_{k+1} = g(x_k)$, where the root is a fixed point of $g(x)$.\nConsider the quadratic equation $x^2 - 6x - 16 = 0$. An iteration scheme is proposed using the function $g(x) = x + c(x^2 - 6x - 16)$, where $c$ is a constant parameter. The choice of $c$ critically affects the performance of the algorithm.\n\nDetermine the exact value of the constant $c$ that provides the fastest possible local convergence of the iteration to the *positive* root of the equation.\n\nExpress your answer as a final numerical value.", "solution": "We seek a constant-step fixed-point iteration of the form $x_{k+1}=g(x_{k})$ with\n$$\ng(x)=x+c f(x), \\quad f(x)=x^{2}-6x-16,\n$$\nto converge as fast as possible to the positive root of $f(x)=0$.\n\nFirst, determine the positive root $r$ of $f(x)=0$. Solving the quadratic equation,\n$$\nx^{2}-6x-16=0 \\;\\;\\Rightarrow\\;\\; x=\\frac{6\\pm\\sqrt{36+64}}{2}=\\frac{6\\pm 10}{2}\\in\\{8,-2\\}.\n$$\nHence the positive root is $r=8$.\n\nFor a fixed-point iteration $x_{k+1}=g(x_{k})$ to converge locally to $r$, it is necessary that $g(r)=r$ and sufficient that $|g'(r)| \\lt 1$. The local error satisfies, for $e_{k}=x_{k}-r$,\n$$\ne_{k+1}=g(r+e_{k})-g(r)=g'(r)e_{k}+\\frac{1}{2}g''(r)e_{k}^{2}+O(e_{k}^{3}).\n$$\nThus the asymptotic linear convergence factor is $|g'(r)|$, and the fastest local convergence is achieved by minimizing $|g'(r)|$. The optimal choice is to enforce $g'(r)=0$, which yields at least quadratic convergence.\n\nCompute $g'(x)$:\n$$\ng'(x)=1+c f'(x), \\quad f'(x)=2x-6.\n$$\nImpose $g'(r)=0$:\n$$\n0=1+c f'(r) \\;\\;\\Rightarrow\\;\\; c=-\\frac{1}{f'(r)}.\n$$\nEvaluate $f'(r)$ at $r=8$:\n$$\nf'(8)=2\\cdot 8-6=10.\n$$\nTherefore,\n$$\nc=-\\frac{1}{10}.\n$$\nThis choice makes $g'(r)=0$ and yields the fastest possible local convergence (quadratic order) for this family $g(x)=x+c f(x)$ with constant $c$.", "answer": "$$\\boxed{-\\frac{1}{10}}$$", "id": "2162905"}]}