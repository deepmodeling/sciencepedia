{"hands_on_practices": [{"introduction": "An equation can often be rearranged into multiple fixed-point forms, $x = g(x)$, but not all forms are created equal. This exercise provides hands-on practice in evaluating the quality of different iteration schemes by calculating their rates of convergence [@problem_id:2214069]. You will see how the magnitude of the derivative, $|g'(\\alpha)|$, at the fixed point $\\alpha$ determines whether an iteration converges quickly, slowly, or not at all.", "problem": "The equation $f(x) = e^x - 2x - 1 = 0$ has a trivial root at $x=0$ and a unique positive root, which we denote by $\\alpha$. To numerically approximate this positive root $\\alpha$, two different fixed-point iteration schemes are proposed.\n\nScheme A is defined by the iteration function $g_A(x) = \\frac{e^x - 1}{2}$.\nScheme B is defined by the iteration function $g_B(x) = \\ln(2x+1)$.\n\nThe local behavior of a fixed-point iteration method, whether it converges to or diverges from a root, is characterized by its asymptotic rate constant. For an iteration function $g(x)$ and a root $\\alpha$, this constant is given by $C = |g'(\\alpha)|$. A value of $C < 1$ indicates convergence, while $C > 1$ indicates divergence.\n\nLet the rate constants for Scheme A and Scheme B at the root $\\alpha$ be $C_A = |g'_A(\\alpha)|$ and $C_B = |g'_B(\\alpha)|$, respectively.\n\nGiven that the positive root is $\\alpha \\approx 1.256431$, calculate the numerical value of the ratio $R = \\frac{C_A}{C_B}$.\n\nRound your final answer to four significant figures.", "solution": "We are given $f(x)=\\exp(x)-2x-1$ with a positive root $\\alpha$. Two fixed-point iterations are defined by $g_{A}(x)=\\frac{\\exp(x)-1}{2}$ and $g_{B}(x)=\\ln(2x+1)$. For a fixed-point iteration $x_{n+1}=g(x_{n})$ converging to $\\alpha$, the asymptotic rate constant is $C=|g'(\\alpha)|$.\n\nCompute derivatives:\n$$\ng_{A}'(x)=\\frac{1}{2}\\exp(x), \\quad g_{B}'(x)=\\frac{2}{2x+1}.\n$$\nTherefore,\n$$\nC_{A}=|g_{A}'(\\alpha)|=\\frac{1}{2}\\exp(\\alpha), \\quad C_{B}=|g_{B}'(\\alpha)|=\\frac{2}{2\\alpha+1}.\n$$\nThe ratio is\n$$\nR=\\frac{C_{A}}{C_{B}}=\\frac{\\frac{1}{2}\\exp(\\alpha)}{\\frac{2}{2\\alpha+1}}=\\frac{\\exp(\\alpha)\\,(2\\alpha+1)}{4}.\n$$\nSince $\\alpha$ satisfies $f(\\alpha)=0$, we have $\\exp(\\alpha)-2\\alpha-1=0$, hence $\\exp(\\alpha)=2\\alpha+1$. Substituting,\n$$\nR=\\frac{(2\\alpha+1)^{2}}{4}.\n$$\nWith $\\alpha\\approx 1.256431$,\n$$\n2\\alpha+1=2(1.256431)+1=3.512862,\n$$\n$$\n(2\\alpha+1)^{2}=(3.512862)^{2}=12.340199431044,\n$$\n$$\nR\\approx \\frac{12.340199431044}{4}=3.085049857761.\n$$\nRounding to four significant figures gives $R\\approx 3.085$.", "answer": "$$\\boxed{3.085}$$", "id": "2214069"}, {"introduction": "When a fixed-point iteration converges, but does so slowly, we can often accelerate it. This problem challenges you to think algorithmically about implementing Aitken's $\\Delta^2$ method, a classic technique for accelerating linearly convergent sequences [@problem_id:2214048]. By analyzing different pseudocode implementations, you'll develop a deeper understanding of how to translate a mathematical formula into a working computational procedure.", "problem": "In numerical analysis, a fixed-point iteration for solving an equation of the form $x = g(x)$ generates a sequence $\\{x_k\\}_{k=0}^\\infty$ using the recurrence relation $x_{k+1} = g(x_k)$, starting from an an initial guess $x_0$. If this sequence converges to the fixed point $p$, and the convergence is linear, its rate can often be improved.\n\nAitken's $\\Delta^2$ method is a sequence acceleration technique that transforms a linearly convergent sequence $\\{x_k\\}$ into a new sequence $\\{\\hat{x}_k\\}$ that converges faster to the same limit. The formula for the accelerated sequence is given by:\n$$\n\\hat{x}_k = x_k - \\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k}\n$$\nThis formula requires three consecutive terms from the original sequence ($x_k, x_{k+1}, x_{k+2}$) to compute a single term of the accelerated sequence ($\\hat{x}_k$).\n\nConsider an iterative algorithm designed to find a fixed point of $g(x)$ using Aitken's method. The algorithm takes an initial guess `x0`, a tolerance `TOL`, and a maximum number of iterations `N`. In each step, it uses a sliding window of the three most recent points from the sequence $x_{k+1}=g(x_k)$ to compute an accelerated approximation `x_hat`, and then updates the points for the next iteration.\n\nWhich of the following pseudocode blocks correctly implements this iterative Aitken's acceleration algorithm?\n\nA.\n```\nINPUT x0, TOL, N\nx1 = g(x0)\nx2 = g(x1)\ni = 1\nWHILE i = N DO\n    den = x2 - 2*x1 + x0\n    IF den == 0 THEN OUTPUT('Failure: Zero denominator'); STOP\n    x_hat = x0 - (x1 - x0)^2 / den\n    IF |x_hat - x2|  TOL THEN OUTPUT(x_hat); STOP\n    x0 = x1\n    x1 = x2\n    x2 = g(x1)\n    i = i + 1\nEND WHILE\nOUTPUT('Failure: Max iterations reached')\n```\n\nB.\n```\nINPUT x0, TOL, N\ni = 1\nWHILE i = N DO\n    x1 = g(x0)\n    x2 = g(x1)\n    den = x2 - 2*x1 + x0\n    IF den == 0 THEN OUTPUT('Failure: Zero denominator'); STOP\n    x_hat = x0 - (x1 - x0)^2 / den\n    IF |x_hat - x0|  TOL THEN OUTPUT(x_hat); STOP\n    x0 = x_hat\n    i = i + 1\nEND WHILE\nOUTPUT('Failure: Max iterations reached')\n```\n\nC.\n```\nINPUT x0, TOL, N\nx1 = g(x0)\nx2 = g(x1)\ni = 1\nWHILE i = N DO\n    den = x2 - 2*x1 + x0\n    IF den == 0 THEN OUTPUT('Failure: Zero denominator'); STOP\n    x_hat = x2 - (x2 - x1)^2 / den\n    IF |x_hat - x2|  TOL THEN OUTPUT(x_hat); STOP\n    x0 = x1\n    x1 = x2\n    x2 = g(x1)\n    i = i + 1\nEND WHILE\nOUTPUT('Failure: Max iterations reached')\n```\n\nD.\n```\nINPUT x0, TOL, N\nx1 = g(x0)\nx2 = g(x1)\ni = 1\nWHILE i = N DO\n    den = x2 - 2*x1 + x0\n    IF den == 0 THEN OUTPUT('Failure: Zero denominator'); STOP\n    x_hat = x0 - (x1 - x0)^2 / den\n    IF |x_hat - x2|  TOL THEN OUTPUT(x_hat); STOP\n    x2 = g(x1)\n    x0 = x1\n    x1 = x2\n    i = i + 1\nEND WHILE\nOUTPUT('Failure: Max iterations reached')\n```", "solution": "We are given a fixed-point iteration $x_{k+1}=g(x_{k})$ and Aitkenâ€™s $\\Delta^{2}$ acceleration\n$$\n\\hat{x}_{k}=x_{k}-\\frac{(x_{k+1}-x_{k})^{2}}{x_{k+2}-2x_{k+1}+x_{k}}.\n$$\nTo implement an iterative algorithm that, at each step, uses a sliding window of the three most recent points from the original sequence and computes $\\hat{x}_{k}$, we must follow these steps:\n1) Initialize three consecutive iterates from the original sequence: set $x_{1}=g(x_{0})$ and $x_{2}=g(x_{1})$ so that $(x_{0},x_{1},x_{2})=(x_{k},x_{k+1},x_{k+2})$.\n2) Compute the denominator $\\Delta^{2}x_{k}=x_{2}-2x_{1}+x_{0}$; if it equals zero, terminate with failure to avoid division by zero.\n3) Compute the accelerated value using exactly\n$$\nx_{\\text{hat}}=x_{0}-\\frac{(x_{1}-x_{0})^{2}}{x_{2}-2x_{1}+x_{0}}.\n$$\n4) Check a reasonable stopping criterion using available quantities from the same window, for example $|x_{\\text{hat}}-x_{2}|\\text{TOL}$.\n5) Advance the sliding window to the next triple of consecutive original iterates, which corresponds to incrementing $k$ to $k+1$:\n$$\nx_{0}\\leftarrow x_{1},\\quad x_{1}\\leftarrow x_{2},\\quad x_{2}\\leftarrow g(x_{1}).\n$$\nThis maintains the invariant that $(x_{0},x_{1},x_{2})=(x_{k},x_{k+1},x_{k+2})$ at the start of each loop.\n\nNow compare with the options:\n- Option A initializes $x_{1}=g(x_{0})$, $x_{2}=g(x_{1})$, computes $\\text{den}=x_{2}-2x_{1}+x_{0}$, uses\n$$\nx_{\\text{hat}}=x_{0}-\\frac{(x_{1}-x_{0})^{2}}{\\text{den}},\n$$\nchecks $|x_{\\text{hat}}-x_{2}|\\text{TOL}$, and updates by\n$$\nx_{0}=x_{1},\\quad x_{1}=x_{2},\\quad x_{2}=g(x_{1}),\n$$\nwhich exactly matches the required formula and the correct sliding window update.\n- Option B replaces $x_{0}$ by $x_{\\text{hat}}$ and recomputes $x_{1}=g(x_{0})$, $x_{2}=g(x_{1})$ each iteration. This does not maintain a sliding window of the original sequence; it feeds the accelerated estimate back into the generator, which is not what is described.\n- Option C uses\n$$\nx_{\\text{hat}}=x_{2}-\\frac{(x_{2}-x_{1})^{2}}{x_{2}-2x_{1}+x_{0}},\n$$\nwhich does not match the Aitken formula $\\hat{x}_{k}=x_{k}-\\frac{(x_{k+1}-x_{k})^{2}}{x_{k+2}-2x_{k+1}+x_{k}}$.\n- Option D computes $x_{2}=g(x_{1})$ before shifting $x_{0}$ and $x_{1}$, so after the shift $x_{2}$ is not advanced to $x_{k+3}$; this breaks the sliding window advancement.\n\nTherefore, only Option A correctly implements the iterative Aitken acceleration with a sliding window on the original fixed-point sequence.", "answer": "$$\\boxed{A}$$", "id": "2214048"}, {"introduction": "The power of fixed-point iteration extends beyond single equations to solving systems of nonlinear equations, which model complex interacting phenomena. This practice guides you through applying the Contraction Mapping Theorem in a multi-dimensional setting [@problem_id:2214053]. You will use the Jacobian matrix of the iterative map to establish a condition that guarantees convergence to a unique solution within a specified domain.", "problem": "A key step in modeling certain coupled physical systems is finding the equilibrium points, which often requires solving a system of nonlinear equations. Consider the system described by:\n$$\n\\begin{align*}\nx = \\alpha \\cos(y) \\\\\ny = \\alpha \\sin(x)\n\\end{align*}\n$$\nwhere $(x,y)$ are state variables and $\\alpha$ is a positive real parameter characterizing the coupling strength.\n\nTo find a solution to this system, we can use a fixed-point iteration method defined by the map $F: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$, where $F(x,y) = (\\alpha\\cos(y), \\alpha\\sin(x))$. An iteration is defined as $(x_{k+1}, y_{k+1}) = F(x_k, y_k)$.\n\nFor the iteration to be guaranteed to converge to a unique fixed point within a closed set $D$ (provided certain conditions like $F(D) \\subseteq D$ are met), the map $F$ must be a contraction on $D$. A sufficient condition for $F$ to be a contraction on a convex domain $D$ is that there exists a constant $L  1$ such that $\\|J_F(v)\\| \\le L$ for all $v \\in D$, where $J_F$ is the Jacobian matrix of $F$ and $\\|\\cdot\\|$ is a matrix norm.\n\nLet the domain of interest be the rectangular region $D = \\left[ \\frac{\\pi}{4}, \\frac{\\pi}{2} \\right] \\times \\left[ -\\frac{\\pi}{6}, \\frac{\\pi}{6} \\right]$. Using the matrix infinity norm, defined for a $2 \\times 2$ matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ as $\\|A\\|_{\\infty} = \\max(|a|+|b|, |c|+|d|)$, determine the supremum of all positive values of $\\alpha$ for which the map $F$ is a contraction on the domain $D$.", "solution": "We use the sufficient contraction criterion on a convex domain via the Jacobian norm: if $D$ is convex and $\\sup_{v \\in D}\\|J_{F}(v)\\|_{\\infty} \\le L  1$, then $F$ is a contraction on $D$ with Lipschitz constant $L$.\n\nThe map is $F(x,y)=(\\alpha\\cos(y),\\alpha\\sin(x))$. Its Jacobian is\n$$\nJ_{F}(x,y)\n=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x}\\left(\\alpha\\cos y\\right)  \\frac{\\partial}{\\partial y}\\left(\\alpha\\cos y\\right) \\\\\n\\frac{\\partial}{\\partial x}\\left(\\alpha\\sin x\\right)  \\frac{\\partial}{\\partial y}\\left(\\alpha\\sin x\\right)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  -\\alpha\\sin y\\\\\n\\alpha\\cos x  0\n\\end{pmatrix}.\n$$\nUsing the matrix infinity norm $\\|A\\|_{\\infty}=\\max(|a|+|b|,|c|+|d|)$ for $A=\\begin{pmatrix}a  b\\\\ c  d\\end{pmatrix}$, we get\n$$\n\\|J_{F}(x,y)\\|_{\\infty}=\\max\\left(|0|+|-\\alpha\\sin y|,\\;|\\alpha\\cos x|+|0|\\right)=\\max\\left(\\alpha|\\sin y|,\\;\\alpha|\\cos x|\\right).\n$$\nOn the domain $D=\\left[\\frac{\\pi}{4},\\frac{\\pi}{2}\\right]\\times\\left[-\\frac{\\pi}{6},\\frac{\\pi}{6}\\right]$, we have\n$$\n\\sup_{y\\in\\left[-\\frac{\\pi}{6},\\frac{\\pi}{6}\\right]}|\\sin y|=\\sin\\!\\left(\\frac{\\pi}{6}\\right)=\\frac{1}{2},\\qquad\n\\sup_{x\\in\\left[\\frac{\\pi}{4},\\frac{\\pi}{2}\\right]}|\\cos x|=\\cos\\!\\left(\\frac{\\pi}{4}\\right)=\\frac{\\sqrt{2}}{2}.\n$$\nHence\n$$\n\\sup_{(x,y)\\in D}\\|J_{F}(x,y)\\|_{\\infty}\n=\n\\alpha\\;\\sup_{(x,y)\\in D}\\max\\left(|\\sin y|,|\\cos x|\\right)\n=\n\\alpha\\;\\max\\left(\\frac{1}{2},\\frac{\\sqrt{2}}{2}\\right)\n=\n\\alpha\\;\\frac{\\sqrt{2}}{2}.\n$$\nFor $F$ to be a contraction on $D$ (in the sense that there exists $L1$ with $\\|J_{F}(v)\\|_{\\infty}\\le L$ for all $v\\in D$), it suffices that\n$$\n\\alpha\\;\\frac{\\sqrt{2}}{2}1,\n$$\nthat is,\n$$\n\\alpha\\sqrt{2}.\n$$\nTherefore, the supremum of all positive $\\alpha$ for which $F$ is a contraction on $D$ (with respect to the infinity norm bound) is $\\sqrt{2}$.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "2214053"}]}