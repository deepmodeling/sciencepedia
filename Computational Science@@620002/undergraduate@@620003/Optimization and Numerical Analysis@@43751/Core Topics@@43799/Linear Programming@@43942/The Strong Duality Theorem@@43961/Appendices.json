{"hands_on_practices": [{"introduction": "To begin our exploration of strong duality, we start with a classic and intuitive geometric problem: finding the point on a line that is closest to the origin. This scenario can be formulated as a convex optimization problem with a quadratic objective function and a single linear equality constraint. By solving for both the primal and dual optimal values, you will directly verify that they are equal, providing a concrete demonstration of the Strong Duality Theorem in action [@problem_id:2221847].", "problem": "Consider an optimization problem defined in $\\mathbb{R}^2$. The objective is to find a point $(x, y)$ that minimizes its squared Euclidean distance from the origin, given by the function $f_0(x, y) = x^2 + y^2$. The minimization is subject to a linear equality constraint, $ax + by = c$, where $a, b,$ and $c$ are non-zero real constants. This formulation is referred to as the primal problem. Let the optimal value of this primal problem be denoted by $p^*$.\n\nThe corresponding Lagrange dual problem involves maximizing the Lagrange dual function, $g(\\nu)$. Let the optimal value of this dual problem be denoted by $d^*$.\n\nYour task is to determine the analytical expressions for both the primal optimal value, $p^*$, and the dual optimal value, $d^*$, in terms of the constants $a, b,$ and $c$.\n\nPresent your final answer as a $1 \\times 2$ row matrix containing the value of $p^*$ as the first element and $d^*$ as the second element.", "solution": "We are minimizing the convex quadratic $f_{0}(x,y)=x^{2}+y^{2}$ subject to the affine equality constraint $ax+by=c$, with $a\\neq 0$ and $b\\neq 0$, so the feasible set is nonempty and KKT conditions are necessary and sufficient.\n\nForm the Lagrangian with dual variable $\\nu\\in\\mathbb{R}$:\n$$\nL(x,y,\\nu)=x^{2}+y^{2}+\\nu(ax+by-c).\n$$\nStationarity with respect to $x$ and $y$ requires\n$$\n\\frac{\\partial L}{\\partial x}=2x+\\nu a=0,\\qquad \\frac{\\partial L}{\\partial y}=2y+\\nu b=0,\n$$\nwhich gives\n$$\nx=-\\frac{\\nu a}{2},\\qquad y=-\\frac{\\nu b}{2}.\n$$\nImposing primal feasibility $ax+by=c$ yields\n$$\na\\left(-\\frac{\\nu a}{2}\\right)+b\\left(-\\frac{\\nu b}{2}\\right)=c\n\\;\\;\\Longrightarrow\\;\\;\n-\\frac{\\nu}{2}(a^{2}+b^{2})=c\n\\;\\;\\Longrightarrow\\;\\;\n\\nu^{\\star}=-\\frac{2c}{a^{2}+b^{2}}.\n$$\nHence the primal optimizer is\n$$\nx^{\\star}=\\frac{ac}{a^{2}+b^{2}},\\qquad y^{\\star}=\\frac{bc}{a^{2}+b^{2}},\n$$\nand the primal optimal value is\n$$\np^{\\star}=(x^{\\star})^{2}+(y^{\\star})^{2}\n=\\frac{a^{2}c^{2}+b^{2}c^{2}}{(a^{2}+b^{2})^{2}}\n=\\frac{c^{2}}{a^{2}+b^{2}}.\n$$\n\nFor the dual, the dual function is $g(\\nu)=\\inf_{x,y}L(x,y,\\nu)$, obtained by substituting the stationary $x=-\\nu a/2$ and $y=-\\nu b/2$:\n$$\nx^{2}+y^{2}=\\frac{\\nu^{2}}{4}(a^{2}+b^{2}),\\qquad\nax+by=-\\frac{\\nu}{2}(a^{2}+b^{2}),\n$$\nso\n$$\ng(\\nu)=\\frac{\\nu^{2}}{4}(a^{2}+b^{2})+\\nu\\left(-\\frac{\\nu}{2}(a^{2}+b^{2})-c\\right)\n=-\\frac{a^{2}+b^{2}}{4}\\nu^{2}-c\\nu.\n$$\nThe dual problem is $\\max_{\\nu\\in\\mathbb{R}}g(\\nu)$; differentiate and set to zero:\n$$\ng'(\\nu)=-\\frac{a^{2}+b^{2}}{2}\\nu-c=0\n\\;\\;\\Longrightarrow\\;\\;\n\\nu^{\\star}=-\\frac{2c}{a^{2}+b^{2}}.\n$$\nEvaluating the dual optimal value,\n$$\nd^{\\star}=g(\\nu^{\\star})\n=-\\frac{a^{2}+b^{2}}{4}\\left(\\frac{4c^{2}}{(a^{2}+b^{2})^{2}}\\right)-c\\left(-\\frac{2c}{a^{2}+b^{2}}\\right)\n=-\\frac{c^{2}}{a^{2}+b^{2}}+\\frac{2c^{2}}{a^{2}+b^{2}}\n=\\frac{c^{2}}{a^{2}+b^{2}}.\n$$\nSince the problem is convex with an affine equality constraint and a feasible point exists, strong duality holds, so $p^{\\star}=d^{\\star}=\\frac{c^{2}}{a^{2}+b^{2}}$.\n\nThus the requested $1\\times 2$ row matrix is $\\begin{pmatrix}p^{\\star}  d^{\\star}\\end{pmatrix}=\\begin{pmatrix}\\frac{c^{2}}{a^{2}+b^{2}}  \\frac{c^{2}}{a^{2}+b^{2}}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{c^{2}}{a^{2}+b^{2}}  \\frac{c^{2}}{a^{2}+b^{2}}\\end{pmatrix}}$$", "id": "2221847"}, {"introduction": "Building on the previous example, we now introduce inequality constraints by considering a finite line segment instead of an infinite line. This practice requires using the Karush-Kuhn-Tucker (KKT) conditions, which generalize the method of Lagrange multipliers to problems with inequalities. The key insight you will gain here is the interpretive power of the dual variables, which act as indicators telling us whether the optimal point lies in the interior of the segment or at one of its endpoints [@problem_id:2221842].", "problem": "A point robot must travel along a straight line segment path in a two-dimensional Cartesian plane. The path begins at point $A=(1, 5)$ and ends at point $B=(5, 3)$. For a monitoring task, it is necessary to find the location on this path that is closest to a stationary sensor located at the origin $(0,0)$.\n\nThis geometric problem can be cast as a constrained optimization problem. A point on the segment can be parameterized by $P(t) = (1-t)A + tB$, where the parameter $t$ is constrained to the interval $[0, 1]$. The objective is to minimize the squared Euclidean distance from the origin to the point $P(t)$.\n\nLet $d_{min}^2$ be the minimum squared distance. In the standard formulation for such problems, we introduce Lagrange multipliers (dual variables) for the inequality constraints on $t$. Let $\\lambda_1$ be the Lagrange multiplier for the constraint $t \\ge 0$ (which can be written as $-t \\le 0$) and $\\lambda_2$ be the Lagrange multiplier for the constraint $t \\le 1$ (which can be written as $t-1 \\le 0$).\n\nDetermine the numerical values for the minimum squared distance $d_{min}^2$, and the optimal values of the corresponding Lagrange multipliers, $\\lambda_1$ and $\\lambda_2$. Provide your answer as three numbers for the tuple ($d_{min}^2$, $\\lambda_1$, $\\lambda_2$).", "solution": "We parameterize the point on the segment by $P(t)=(1-t)A+tB$ with $A=(1,5)$ and $B=(5,3)$. Thus\n$$\nP(t)=(1,5)+t\\big((5,3)-(1,5)\\big)=(1,5)+t(4,-2)=(1+4t,\\,5-2t),\n$$\nwith the constraint $t\\in[0,1]$.\n\nThe squared Euclidean distance from the origin is the objective function\n$$\nf(t)=\\|P(t)\\|^{2}=(1+4t)^{2}+(5-2t)^{2}.\n$$\nExpanding,\n$$\n(1+4t)^{2}=1+8t+16t^{2},\\qquad (5-2t)^{2}=25-20t+4t^{2},\n$$\nso\n$$\nf(t)=\\big(1+8t+16t^{2}\\big)+\\big(25-20t+4t^{2}\\big)=20t^{2}-12t+26.\n$$\n\nWe cast this as a constrained optimization with inequality constraints $g_{1}(t)=-t\\le 0$ and $g_{2}(t)=t-1\\le 0$. The Lagrangian is\n$$\n\\mathcal{L}(t,\\lambda_{1},\\lambda_{2})=20t^{2}-12t+26+\\lambda_{1}(-t)+\\lambda_{2}(t-1),\n$$\nwith $\\lambda_{1}\\ge 0$ and $\\lambda_{2}\\ge 0$.\n\nThe Karush–Kuhn–Tucker conditions are:\n- Stationarity:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial t}=40t-12-\\lambda_{1}+\\lambda_{2}=0.\n$$\n- Primal feasibility: $0\\le t\\le 1$.\n- Dual feasibility: $\\lambda_{1}\\ge 0$, $\\lambda_{2}\\ge 0$.\n- Complementary slackness:\n$$\n\\lambda_{1}(-t)=0\\quad\\Longleftrightarrow\\quad \\lambda_{1}t=0,\\qquad \\lambda_{2}(t-1)=0.\n$$\n\nFirst find the unconstrained minimizer of $f(t)$ by setting $f'(t)=0$:\n$$\nf'(t)=40t-12=0\\quad\\Longrightarrow\\quad t^{*}=\\frac{12}{40}=\\frac{3}{10}.\n$$\nSince $t^{*}=\\frac{3}{10}\\in(0,1)$, both inequality constraints are inactive (strict): $-t^{*}0$ and $t^{*}-10$. By complementary slackness, this implies\n$$\n\\lambda_{1}^{*}=0,\\qquad \\lambda_{2}^{*}=0.\n$$\nWith these, the stationarity condition reduces to $40t-12=0$, confirming $t^{*}=\\frac{3}{10}$.\n\nEvaluate the minimum squared distance:\n$$\nd_{\\min}^{2}=f\\!\\left(\\frac{3}{10}\\right)=20\\left(\\frac{3}{10}\\right)^{2}-12\\left(\\frac{3}{10}\\right)+26=20\\cdot \\frac{9}{100}-\\frac{36}{10}+26=\\frac{9}{5}-\\frac{18}{5}+26=26-\\frac{9}{5}=\\frac{121}{5}.\n$$\n\nTherefore, the optimal tuple is $\\left(d_{\\min}^{2},\\lambda_{1}^{*},\\lambda_{2}^{*}\\right)=\\left(\\frac{121}{5},0,0\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{121}{5}  0  0\\end{pmatrix}}$$", "id": "2221842"}, {"introduction": "This final practice takes us from the familiar territory of vector spaces into the more abstract world of matrix optimization. You will tackle a problem common in machine learning and statistics: finding the closest positive semidefinite (PSD) matrix to a given symmetric matrix. This exercise provides a glimpse into semidefinite programming (SDP) and reveals a beautiful connection between optimization and linear algebra, where the solution is found by simply adjusting the eigenvalues of the original matrix [@problem_id:2221816].", "problem": "In many fields, such as finance and machine learning, it is often necessary to find the \"closest\" valid model to an empirically estimated one that may not satisfy theoretical constraints. A common example is finding the nearest valid covariance matrix. This problem can be formalized as an optimization problem.\n\nConsider the task of finding a symmetric matrix $X$ that is closest to a given symmetric matrix $M$, under the constraint that $X$ must be a Positive Semidefinite (PSD) matrix. A symmetric matrix is PSD if all its eigenvalues are non-negative. The \"closeness\" is measured using the squared Frobenius norm, defined as $\\|A\\|_F^2 = \\operatorname{tr}(A^T A) = \\sum_{i,j} A_{ij}^2$.\n\nYou are asked to solve the following optimization problem for a specific matrix $M$:\n$$ \\text{minimize } \\frac{1}{2}\\|X - M\\|_F^2 \\quad \\text{subject to } X \\succeq 0 $$\nwhere $X \\succeq 0$ denotes that $X$ is a symmetric positive semidefinite matrix.\n\nLet the given matrix $M$ be:\n$$ M = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} $$\nFind the optimal matrix $X$. Express your answer as a 2x2 matrix, with each entry rounded to three significant figures.", "solution": "We are asked to solve the projection problem onto the cone of symmetric positive semidefinite matrices:\n$$\\min_{X} \\;\\frac{1}{2}\\|X-M\\|_{F}^{2} \\quad \\text{subject to } X \\succeq 0.$$\nBecause the feasible set $\\{X: X \\succeq 0\\}$ is a closed convex cone and the Frobenius norm is unitarily invariant, the optimizer is the orthogonal projection of $M$ onto the PSD cone. For a symmetric $M$ with eigen-decomposition $M=Q\\Lambda Q^{T}$, where $\\Lambda=\\operatorname{diag}(\\lambda_{1},\\lambda_{2})$, the projection is\n$$X^{\\star}=Q\\,\\Lambda_{+}\\,Q^{T}, \\quad \\Lambda_{+}=\\operatorname{diag}(\\max\\{\\lambda_{1},0\\},\\max\\{\\lambda_{2},0\\}).$$\nThis follows from the fact that the PSD cone is a spectral cone and the Frobenius norm depends only on the eigenvalues in an orthonormal basis, so the closest PSD matrix is obtained by zeroing out the negative eigenvalues.\n\nFor the given matrix\n$$M=\\begin{pmatrix}1  2 \\\\ 2  1\\end{pmatrix},$$\nwe compute its eigenvalues from the characteristic polynomial:\n$$\\det(M-\\lambda I)=\\det\\begin{pmatrix}1-\\lambda  2 \\\\ 2  1-\\lambda\\end{pmatrix}=(1-\\lambda)^{2}-4=\\lambda^{2}-2\\lambda-3=0.$$\nSolving yields\n$$\\lambda_{1}=3,\\quad \\lambda_{2}=-1.$$\nCorresponding (unnormalized) eigenvectors are $v_{1}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ for $\\lambda_{1}=3$ and $v_{2}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$ for $\\lambda_{2}=-1$. Normalizing gives\n$$ q_{1}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad q_{2}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ -1\\end{pmatrix},\\quad Q=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}\\end{pmatrix} $$\nProjecting the eigenvalues by zeroing the negative one yields\n$$\\Lambda_{+}=\\operatorname{diag}(3,0).$$\nThus,\n$$X^{\\star}=Q\\,\\Lambda_{+}\\,Q^{T}=3\\,q_{1}q_{1}^{T}=3\\,\\frac{1}{2}\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}=\\frac{3}{2}\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{2}  \\frac{3}{2} \\\\ \\frac{3}{2}  \\frac{3}{2}\\end{pmatrix}.$$\nRounding each entry to three significant figures gives\n$$\\begin{pmatrix}1.50  1.50 \\\\ 1.50  1.50\\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}1.50  1.50 \\\\ 1.50  1.50\\end{pmatrix}}$$", "id": "2221816"}]}