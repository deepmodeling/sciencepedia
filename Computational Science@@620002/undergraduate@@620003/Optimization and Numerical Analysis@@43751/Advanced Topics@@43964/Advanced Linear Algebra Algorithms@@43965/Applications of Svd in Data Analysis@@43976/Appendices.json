{"hands_on_practices": [{"introduction": "A primary application of Singular Value Decomposition in data analysis is to understand the intrinsic structure of a dataset. The singular values of a data matrix act as quantitative measures of variance along its principal axes. This exercise provides a foundational understanding of how these values can diagnose relationships between features, specifically by demonstrating that a zero singular value indicates perfect collinearity, a critical insight for feature selection and dimensionality reduction. [@problem_id:2154133]", "problem": "A data scientist is analyzing a dataset containing measurements of two properties, Feature A and Feature B, for four different samples. The dataset is provided below:\n\n- Sample 1: Feature A = 3, Feature B = 3\n- Sample 2: Feature A = 5, Feature B = 7\n- Sample 3: Feature A = 2, Feature B = 1\n- Sample 4: Feature A = 6, Feature B = 9\n\nTo analyze the structure of the data, the scientist decides to investigate its singular values. First, create a centered data matrix $X$ of size $4 \\times 2$, where each column corresponds to a feature and each row corresponds to a sample. Centering is performed by subtracting the mean of each feature (column) from all values in that feature's column.\n\nThe singular values of a matrix, denoted by $\\sigma_i$, are a measure of the data's variance along its principal axes and are defined as the non-negative square roots of the eigenvalues of the matrix $X^T X$. A singular value of zero has a specific implication for the relationship between the matrix's columns.\n\nAfter calculating the singular values of the centered data matrix $X$, determine which of the following statements accurately describes the relationship between Feature A and Feature B.\n\nA. The features are linearly independent.\n\nB. The features are perfectly collinear.\n\nC. The features exhibit a strong but not perfect positive correlation.\n\nD. The features exhibit a strong but not perfect negative correlation.\n\nE. The data is insufficient to determine the relationship.", "solution": "Let the uncentered data matrix with rows as samples and columns as features be\n$$\nX_{\\text{raw}}=\\begin{pmatrix}\n3 & 3\\\\\n5 & 7\\\\\n2 & 1\\\\\n6 & 9\n\\end{pmatrix}.\n$$\nCompute the column means\n$$\n\\mu_{A}=\\frac{3+5+2+6}{4}=4,\\qquad \\mu_{B}=\\frac{3+7+1+9}{4}=5.\n$$\nCenter each column by subtracting its mean to obtain the centered columns\n$$\na=\\begin{pmatrix}3-4\\\\5-4\\\\2-4\\\\6-4\\end{pmatrix}=\\begin{pmatrix}-1\\\\1\\\\-2\\\\2\\end{pmatrix},\\qquad\nb=\\begin{pmatrix}3-5\\\\7-5\\\\1-5\\\\9-5\\end{pmatrix}=\\begin{pmatrix}-2\\\\2\\\\-4\\\\4\\end{pmatrix}.\n$$\nObserve that\n$$\nb=2a,\n$$\nso the centered columns are exactly collinear. The centered data matrix is\n$$\nX=\\begin{pmatrix}\n-1 & -2\\\\\n\\phantom{-}1 & \\phantom{-}2\\\\\n-2 & -4\\\\\n\\phantom{-}2 & \\phantom{-}4\n\\end{pmatrix}.\n$$\nCompute\n$$\nX^{T}X=\\begin{pmatrix}\na^{T}a & a^{T}b\\\\\nb^{T}a & b^{T}b\n\\end{pmatrix}\n=\\begin{pmatrix}\n10 & 20\\\\\n20 & 40\n\\end{pmatrix},\n$$\nsince\n$$\na^{T}a=(-1)^{2}+1^{2}+(-2)^{2}+2^{2}=10,\\quad b^{T}b=(-2)^{2}+2^{2}+(-4)^{2}+4^{2}=40,\\quad a^{T}b=(-1)(-2)+1\\cdot 2+(-2)(-4)+2\\cdot 4=20.\n$$\nThe eigenvalues of $X^{T}X$ solve\n$$\n\\det\\begin{pmatrix}10-\\lambda & 20\\\\ 20 & 40-\\lambda\\end{pmatrix}=(10-\\lambda)(40-\\lambda)-20^{2}=0,\n$$\nwhich simplifies to\n$$\n\\lambda^{2}-50\\lambda=0 \\quad\\Rightarrow\\quad \\lambda\\in\\{0,50\\}.\n$$\nHence the singular values are\n$$\n\\sigma_{1}=\\sqrt{50},\\qquad \\sigma_{2}=0.\n$$\nA zero singular value implies the columns of $X$ are linearly dependent, i.e., the features are perfectly collinear. Therefore, the correct statement is option B.", "answer": "$$\\boxed{B}$$", "id": "2154133"}, {"introduction": "Moving from diagnosing data to solving common analytical problems, this practice explores how SVD can be used to find the best-fit solution to an overdetermined system of equations, a scenario that is ubiquitous in linear regression. You will use SVD to compute the Moore-Penrose pseudoinverse, a powerful generalization of the matrix inverse that provides a robust method for solving linear least-squares problems. This technique is fundamental for fitting models to real-world data where measurements outnumber model parameters. [@problem_id:2154101]", "problem": "A data scientist is attempting to model a relationship between a feature $x$ and a target variable $y$ using a simple linear model of the form $y = mx + c$, where $m$ is the slope and $c$ is the y-intercept. To determine the optimal values for $m$ and $c$, the scientist has collected the following four data points $(x, y)$: $(-2, -1)$, $(-1, 1)$, $(1, 2)$, and $(2, 4)$.\n\nEach data point provides a linear equation relating $m$ and $c$. Since there are more data points (equations) than unknown parameters, this forms an overdetermined system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where the solution vector is $\\mathbf{x} = \\begin{pmatrix} m \\\\ c \\end{pmatrix}$. The goal is to find the least-squares solution for $\\mathbf{x}$, which minimizes the sum of squared differences between the model's predictions and the actual $y$ values.\n\nYour task is to find this least-squares solution. You must follow a specific procedure: first, construct the matrix $A$ and the vector $\\mathbf{b}$ from the given data. Then, compute the Singular Value Decomposition (SVD) of $A$. Using the SVD, find the Moore-Penrose pseudoinverse $A^\\dagger$. Finally, calculate the least-squares solution vector $\\mathbf{x} = A^\\dagger\\mathbf{b}$.\n\nProvide the values for the slope $m$ and the y-intercept $c$ in the form of a single closed-form analytic expression.", "solution": "We model $y$ as $y = mx + c$. For each data point $(x_{i},y_{i})$, this gives $m x_{i} + c = y_{i}$. With the given four points, the matrix $A$ and vector $\\mathbf{b}$ are\n$$\nA = \\begin{pmatrix}\n-2 & 1 \\\\\n-1 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{pmatrix},\\quad\n\\mathbf{b} = \\begin{pmatrix}\n-1 \\\\\n1 \\\\\n2 \\\\\n4\n\\end{pmatrix}.\n$$\nWe compute the singular value decomposition $A = U \\Sigma V^{T}$. Start from $A^{T}A$:\n$$\nA^{T}A = \\begin{pmatrix}\n\\sum x_{i}^{2} & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n10 & 0 \\\\\n0 & 4\n\\end{pmatrix}.\n$$\nThus the eigenvalues of $A^{T}A$ are $10$ and $4$ with eigenvectors given by the standard basis, so we can take\n$$\nV = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix},\\qquad\n\\Sigma_{r} = \\begin{pmatrix}\n\\sqrt{10} & 0 \\\\\n0 & 2\n\\end{pmatrix}.\n$$\nWith the thin SVD, $U_{r}$ has columns $u_{1}$ and $u_{2}$ given by normalizing the columns of $A$:\n$$\nu_{1} = \\frac{1}{\\sqrt{10}}\\begin{pmatrix}-2\\\\-1\\\\1\\\\2\\end{pmatrix},\\qquad\nu_{2} = \\frac{1}{2}\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix},\n$$\nso $U_{r} = \\begin{pmatrix}u_{1} & u_{2}\\end{pmatrix}$. Therefore, the Moore-Penrose pseudoinverse is\n$$\nA^{\\dagger} = V \\Sigma_{r}^{-1} U_{r}^{T} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{10}} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{1}^{T} \\\\\nu_{2}^{T}\n\\end{pmatrix}.\n$$\nThe least-squares solution is $\\mathbf{x} = A^{\\dagger}\\mathbf{b} = V \\Sigma_{r}^{-1} U_{r}^{T}\\mathbf{b} = \\Sigma_{r}^{-1} U_{r}^{T}\\mathbf{b}$. Compute the projections:\n$$\nu_{1}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{10}}\\big((-2)(-1)+(-1)(1)+(1)(2)+(2)(4)\\big) = \\frac{11}{\\sqrt{10}},\n$$\n$$\nu_{2}^{T}\\mathbf{b} = \\frac{1}{2}\\big((-1)+1+2+4\\big) = 3.\n$$\nMultiply by $\\Sigma_{r}^{-1}$:\n$$\n\\mathbf{x} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{10}} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{11}{\\sqrt{10}} \\\\\n3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{11}{10} \\\\\n\\frac{3}{2}\n\\end{pmatrix}.\n$$\nThus the least-squares slope and intercept are $m = \\frac{11}{10}$ and $c = \\frac{3}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{11}{10} & \\frac{3}{2}\\end{pmatrix}}$$", "id": "2154101"}, {"introduction": "SVD's ability to distill complex data into its most significant components makes it invaluable for modern applications like recommendation systems. This final practice demonstrates how to predict missing data by leveraging the dominant pattern within a dataset. By constructing the best rank-one approximation from the SVD of a user-item rating matrix, you will see how the most powerful singular value and its corresponding vectors can be used to infer unknown information, a core principle behind matrix completion and collaborative filtering. [@problem_id:2154142]", "problem": "In the field of data analysis, particularly for recommendation systems, one common task is to predict a user's potential rating for an item they have not yet reviewed. This can be modeled by filling in missing entries in a user-item matrix.\n\nConsider a small user-item rating matrix $R$ representing the ratings of two users (rows) for three different items (columns). One of the ratings is unknown and is denoted by a question mark.\n\n$$\nR = \\begin{pmatrix}\n1 & 1 & ? \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\n\nA standard method to estimate the missing rating involves using the dominant structure of the known data. The procedure is as follows:\n1.  A new matrix $A$ is created by taking the matrix $R$ and replacing the missing entry '?' with the value 0.\n2.  The Singular Value Decomposition (SVD) of the matrix $A$ is determined.\n3.  A predictive model is constructed using only the most significant component of the SVD. This model is known as the best rank-one approximation of $A$.\n\nFollowing this procedure, determine the predicted numerical value for the missing rating. Present your answer as a real number rounded to three significant figures.", "solution": "The known-entry matrix with the missing entry set to zero is\n$$\nA=\\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nThe best rank-one approximation of $A$ from the SVD is $\\sigma_{1} u_{1} v_{1}^{T}$, where $\\sigma_{1}$ is the largest singular value and $u_{1},v_{1}$ are the corresponding left and right singular vectors.\n\nCompute $A A^{T}$:\n$$\nA A^{T}=\n\\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0\\\\\n1 & 1\\\\\n0 & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 1\\\\\n1 & 2\n\\end{pmatrix}.\n$$\nFind its eigenvalues from\n$$\n\\det\\!\\left(\\begin{pmatrix}2 & 1\\\\ 1 & 2\\end{pmatrix}-\\lambda I\\right)\n=\\det\\!\\begin{pmatrix}2-\\lambda & 1\\\\ 1 & 2-\\lambda\\end{pmatrix}\n=(2-\\lambda)^{2}-1=\\lambda^{2}-4\\lambda+3=0,\n$$\nwhich gives $\\lambda_{1}=3$ and $\\lambda_{2}=1$. Hence the singular values are $\\sigma_{1}=\\sqrt{3}$ and $\\sigma_{2}=1$.\n\nFor $\\lambda_{1}=3$, an eigenvector of $A A^{T}$ satisfies\n$$\n\\begin{pmatrix}-1 & 1\\\\ 1 & -1\\end{pmatrix}u=0 \\quad\\Rightarrow\\quad u\\propto \\begin{pmatrix}1\\\\ 1\\end{pmatrix}.\n$$\nNormalize to obtain\n$$\nu_{1}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\ 1\\end{pmatrix}.\n$$\nThe corresponding right singular vector is\n$$\nv_{1}=\\frac{1}{\\sigma_{1}}A^{T}u_{1}\n=\\frac{1}{\\sqrt{3}}\n\\begin{pmatrix}\n1 & 0\\\\\n1 & 1\\\\\n0 & 1\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\ 1\\end{pmatrix}\n=\\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\ 2\\\\ 1\\end{pmatrix}.\n$$\nTherefore, the best rank-one approximation is\n$$\nA_{1}=\\sigma_{1} u_{1} v_{1}^{T}=\\sqrt{3}\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\ 1\\end{pmatrix}\\right)\\left(\\frac{1}{\\sqrt{6}}\\begin{pmatrix}1 & 2 & 1\\end{pmatrix}\\right)\n=\\frac{1}{2}\\begin{pmatrix}1 & 2 & 1\\\\ 1 & 2 & 1\\end{pmatrix}.\n$$\nThe predicted missing entry is the $(1,3)$ element of $A_{1}$:\n$$\n(A_{1})_{1,3}=\\sigma_{1}\\,u_{1,1}\\,v_{1,3}\n=\\sqrt{3}\\cdot \\frac{1}{\\sqrt{2}}\\cdot \\frac{1}{\\sqrt{6}}\n=\\frac{\\sqrt{3}}{\\sqrt{12}}=\\frac{1}{2}=0.5.\n$$\nRounded to three significant figures, the prediction is $0.500$.", "answer": "$$\\boxed{0.500}$$", "id": "2154142"}]}