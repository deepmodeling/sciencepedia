{"hands_on_practices": [{"introduction": "The power of Krylov subspace methods comes from their ability to approximate solutions to large-scale problems by solving a much smaller, simpler problem. For symmetric matrices, the Lanczos algorithm is the engine that drives this process. It iteratively builds an orthonormal basis for the Krylov subspace and simultaneously generates a tridiagonal matrix $T$ which represents the original matrix $A$ projected onto this subspace. This hands-on exercise walks you through the very first step of the Lanczos algorithm, giving you a feel for how the essential components of the tridiagonal matrix are generated. [@problem_id:2183327]", "problem": "The Lanczos algorithm is an iterative method that transforms a symmetric matrix $A$ into a symmetric tridiagonal matrix $T$. The elements of $T$ consist of diagonal entries $\\alpha_j$ and off-diagonal entries $\\beta_j$. The algorithm starts with an initial vector $b$ and generates a sequence of orthonormal vectors $q_j$ known as Lanczos vectors.\n\nLet the symmetric matrix $A$ and the starting vector $b$ be defined as:\n$$A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\nPerform the first iteration of the Lanczos algorithm to find the first diagonal element, $\\alpha_1$, and the first off-diagonal element, $\\beta_1$, of the corresponding tridiagonal matrix $T$. The process begins with the normalization of $b$ to obtain the first Lanczos vector, $q_1$.\n\nReport your answer as an ordered pair for $(\\alpha_1, \\beta_1)$.", "solution": "The Lanczos algorithm generates a sequence of orthonormal vectors $q_j$ and a symmetric tridiagonal matrix $T$. We are asked to find the first entries of this matrix, $\\alpha_1$ and $\\beta_1$.\n\n**Step 1: Initialization**\n\nThe first step is to normalize the starting vector $b$ to obtain the first Lanczos vector $q_1$.\nThe given starting vector is $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe calculate its Euclidean norm, $\\|b\\|_2$:\n$$\\|b\\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\nNow, we find $q_1$ by dividing $b$ by its norm:\n$$q_1 = \\frac{b}{\\|b\\|_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n**Step 2: First Iteration (j=1)**\n\nThe core of the Lanczos iteration involves computing the elements $\\alpha_j$ and $\\beta_j$. For the first iteration ($j=1$), the algorithm proceeds as follows.\n\nFirst, we compute the vector $v$ by applying the matrix $A$ to $q_1$:\n$$v = A q_1 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2(1) + 1(1) \\\\ 1(1) + 3(1) \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n\nNext, we compute the first diagonal element, $\\alpha_1$. This is found by taking the inner product of $v$ with $q_1$:\n$$\\alpha_1 = q_1^T v$$\nSubstituting the expressions for $q_1$ and $v$:\n$$\\alpha_1 = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} \\right) = \\frac{1}{(\\sqrt{2})^2} (1 \\cdot 3 + 1 \\cdot 4) = \\frac{1}{2}(7) = \\frac{7}{2}$$\n\nAfter finding $\\alpha_1$, we compute the unnormalized residual vector, $r_1$, which will be used to find $\\beta_1$ and $q_2$. This vector is made orthogonal to $q_1$ by subtracting the projection of $v$ onto $q_1$:\n$$r_1 = v - \\alpha_1 q_1$$\nSubstituting the known values:\n$$r_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} - \\frac{7}{2} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right)$$\n$$r_1 = \\frac{1}{\\sqrt{2}} \\left[ \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} - \\frac{7}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right] = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 - \\frac{7}{2} \\\\ 4 - \\frac{7}{2} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\nFinally, the first off-diagonal element, $\\beta_1$, is the Euclidean norm of this residual vector $r_1$:\n$$\\beta_1 = \\|r_1\\|_2$$\n$$\\beta_1 = \\left\\| \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{2}} \\left\\| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{2}} \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2}$$\n$$\\beta_1 = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{2}{4}} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} \\frac{1}{\\sqrt{2}} = \\frac{1}{2}$$\n\nThe first iteration is now complete. The requested values are $\\alpha_1 = \\frac{7}{2}$ and $\\beta_1 = \\frac{1}{2}$.", "answer": "$$\\boxed{(\\frac{7}{2}, \\frac{1}{2})}$$", "id": "2183327"}, {"introduction": "When dealing with non-symmetric systems, the Arnoldi process (a generalization of Lanczos) is used to build the Krylov subspace. Once this subspace is constructed, we must decide how to extract the best approximate solution from it. This practice compares two fundamental approaches: the Full Orthogonalization Method (FOM), which imposes a Galerkin orthogonality condition on the residual, and the Generalized Minimal Residual Method (GMRES), which explicitly minimizes the norm of the residual over the subspace. By performing the calculations for both, you will gain a direct, quantitative understanding of the key optimality property that makes GMRES so powerful and widely used. [@problem_id:2183323]", "problem": "Consider the linear system $Ax=b$, where\n$$ A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}. $$\nWe want to find an approximate solution using two-step ($m=2$) Krylov subspace methods starting with an initial guess $x_0 = \\vec{0}$. The two methods are the Full Orthogonalization Method (FOM) and the Generalized Minimal Residual Method (GMRES).\n\nBoth methods start by constructing an orthonormal basis $\\{v_1, v_2\\}$ for the Krylov subspace $\\mathcal{K}_2(A, r_0)$, where $r_0 = b - Ax_0$, using the Arnoldi process. This process yields the relation $AV_2 = V_3 \\tilde{H}_2$, where $V_k = [v_1, \\dots, v_k]$ is the matrix with the basis vectors as columns, and $\\tilde{H}_2$ is a $3 \\times 2$ upper Hessenberg matrix. Let $H_2$ be the $2 \\times 2$ matrix obtained by removing the last row of $\\tilde{H}_2$.\n\nThe approximate solution after two steps is given by $x_2 = x_0 + V_2 y_2$, where $y_2$ is a vector in $\\mathbb{R}^2$.\n- For FOM(2), the vector $y_2^{\\text{FOM}}$ is found by solving the system $H_2 y = \\|r_0\\|_2 e_1$, where $e_1 = [1, 0]^T$.\n- For GMRES(2), the vector $y_2^{\\text{GMRES}}$ is found by solving the least-squares problem $\\min_{y \\in \\mathbb{R}^2} \\| \\tilde{H}_2 y - \\|r_0\\|_2 e_1 \\|_2$, where $e_1 = [1, 0, 0]^T$.\n\nThe final residual vectors are $r_2^{\\text{FOM}} = b - Ax_2^{\\text{FOM}}$ and $r_2^{\\text{GMRES}} = b - Ax_2^{\\text{GMRES}}$.\n\nCalculate the 2-norms of the final residual vectors for both methods, $\\|r_2^{\\text{FOM}}\\|_2$ and $\\|r_2^{\\text{GMRES}}\\|_2$. Express your answer as a pair of exact analytical values $(\\|r_2^{\\text{FOM}}\\|_2, \\|r_2^{\\text{GMRES}}\\|_2)$.", "solution": "We have the linear system $Ax=b$ with\n$$\nA=\\begin{pmatrix}1&1&0\\\\0&1&1\\\\1&0&1\\end{pmatrix},\\quad b=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix},\n$$\ninitial guess $x_{0}=\\vec{0}$, and $r_{0}=b-Ax_{0}=b$. Hence $\\beta=\\|r_{0}\\|_{2}=1$ and $v_{1}=r_{0}/\\beta=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$.\n\nWe run two steps of the Arnoldi process to build an orthonormal basis $\\{v_{1},v_{2}\\}$ of $\\mathcal{K}_{2}(A,r_{0})$ and obtain the relation $AV_{2}=V_{3}\\tilde{H}_{2}$ with $V_{k}=[v_{1},\\dots,v_{k}]$.\n\nStep 1:\n$$\nw=A v_{1}=A\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix},\\quad h_{1,1}=v_{1}^{T}w=1,\n$$\n$$\nw:=w-h_{1,1}v_{1}=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix},\\quad h_{2,1}=\\|w\\|_{2}=1,\\quad v_{2}=w/h_{2,1}=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}.\n$$\n\nStep 2:\n$$\nw=A v_{2}=A\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix},\\quad h_{1,2}=v_{1}^{T}w=0,\\quad w:=w-h_{1,2}v_{1}=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix},\n$$\n$$\nh_{2,2}=v_{2}^{T}w=1,\\quad w:=w-h_{2,2}v_{2}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix},\\quad h_{3,2}=\\|w\\|_{2}=1,\\quad v_{3}=w/h_{3,2}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}.\n$$\n\nTherefore,\n$$\n\\tilde{H}_{2}=\\begin{pmatrix}h_{1,1}&h_{1,2}\\\\ h_{2,1}&h_{2,2}\\\\ 0&h_{3,2}\\end{pmatrix}\n=\\begin{pmatrix}1&0\\\\ 1&1\\\\ 0&1\\end{pmatrix},\\qquad\nH_{2}=\\begin{pmatrix}1&0\\\\ 1&1\\end{pmatrix}.\n$$\n\nThe two-step approximations have the form $x_{2}=x_{0}+V_{2}y$ with $y\\in\\mathbb{R}^{2}$.\n\nFor FOM(2), $y^{\\text{FOM}}$ solves $H_{2}y=\\beta e_{1}$ with $e_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$:\n$$\n\\begin{pmatrix}1&0\\\\1&1\\end{pmatrix}\\begin{pmatrix}y_{1}\\\\y_{2}\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\end{pmatrix}\n\\;\\Rightarrow\\; y_{1}=1,\\; y_{2}=-1.\n$$\nHence $x_{2}^{\\text{FOM}}=V_{2}y^{\\text{FOM}}=v_{1}\\cdot 1+v_{2}\\cdot(-1)=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$, and the residual is\n$$\nr_{2}^{\\text{FOM}}=b-Ax_{2}^{\\text{FOM}}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}-A\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}\n=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}-\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}\n=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix},\n$$\nso $\\|r_{2}^{\\text{FOM}}\\|_{2}=1$.\n\nFor GMRES(2), $y^{\\text{GMRES}}$ minimizes $\\|\\tilde{H}_{2}y-\\beta e_{1}\\|_{2}$ with $e_{1}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$. Let $y=\\begin{pmatrix}y_{1}\\\\y_{2}\\end{pmatrix}$. Then\n$$\n\\tilde{H}_{2}y-\\beta e_{1}=\\begin{pmatrix}y_{1}-1\\\\ y_{1}+y_{2}\\\\ y_{2}\\end{pmatrix},\n$$\nand the squared norm to minimize is\n$$\nf(y_{1},y_{2})=(y_{1}-1)^{2}+(y_{1}+y_{2})^{2}+y_{2}^{2}.\n$$\nSetting the gradient to zero,\n$$\n\\frac{\\partial f}{\\partial y_{1}}=2(y_{1}-1)+2(y_{1}+y_{2})=0\\;\\Rightarrow\\;2y_{1}+y_{2}-1=0,\n$$\n$$\n\\frac{\\partial f}{\\partial y_{2}}=2(y_{1}+y_{2})+2y_{2}=0\\;\\Rightarrow\\;y_{1}+2y_{2}=0.\n$$\nSolving, $y_{2}=-\\frac{1}{3}$ and $y_{1}=\\frac{2}{3}$. The minimized residual norm is\n$$\n\\|\\tilde{H}_{2}y^{\\text{GMRES}}-\\beta e_{1}\\|_{2}\n=\\left\\|\\begin{pmatrix}\\frac{2}{3}-1\\\\ \\frac{2}{3}-\\frac{1}{3}\\\\ -\\frac{1}{3}\\end{pmatrix}\\right\\|_{2}\n=\\left\\|\\begin{pmatrix}-\\frac{1}{3}\\\\ \\frac{1}{3}\\\\ -\\frac{1}{3}\\end{pmatrix}\\right\\|_{2}\n=\\sqrt{\\frac{1}{9}+\\frac{1}{9}+\\frac{1}{9}}=\\sqrt{\\frac{1}{3}}.\n$$\nBecause $r_{2}^{\\text{GMRES}}=V_{3}(\\beta e_{1}-\\tilde{H}_{2}y^{\\text{GMRES}})$ and $V_{3}$ is orthonormal, we have $\\|r_{2}^{\\text{GMRES}}\\|_{2}=\\|\\tilde{H}_{2}y^{\\text{GMRES}}-\\beta e_{1}\\|_{2}=\\sqrt{\\frac{1}{3}}$.\n\nTherefore, the requested pair of residual norms is $(\\|r_{2}^{\\text{FOM}}\\|_{2},\\|r_{2}^{\\text{GMRES}}\\|_{2})=\\left(1,\\sqrt{\\frac{1}{3}}\\right)$.", "answer": "$$\\boxed{(1, \\sqrt{\\frac{1}{3}})}$$", "id": "2183323"}, {"introduction": "While full GMRES is guaranteed to find the exact solution in at most $N$ steps (for an $N \\times N$ matrix, in exact arithmetic), its memory requirements can be prohibitive for large problems. A common practical solution is \"restarted\" GMRES, or GMRES($m$), which resets the underlying Arnoldi process every $m$ iterations. However, this practicality comes at a cost: convergence is no longer guaranteed. This exercise explores a classic example of a non-normal linear system where restarted GMRES fails to make any progress, a phenomenon known as stagnation, providing critical insight into the algorithm's real-world behavior. [@problem_id:2183305]", "problem": "The Generalized Minimal Residual Method (GMRES) is an iterative algorithm used to solve large, sparse, non-symmetric systems of linear equations of the form $Ax=b$. A common variant is the restarted GMRES algorithm, denoted as GMRES(m), which performs $m$ iterations of the Arnoldi process to build a Krylov subspace and find an approximate solution. This solution then becomes the initial guess for the next \"cycle\" of $m$ iterations. While full GMRES (where $m$ is the dimension of the matrix) is guaranteed to converge in at most $m$ steps in exact arithmetic, the behavior of restarted GMRES can be more complex, especially for non-normal matrices (where $A^T A \\neq A A^T$).\n\nConsider the 4-dimensional linear system $Ax=b$ defined by the matrix $A$ and vector $b$ as follows:\n$$\nA = \\begin{pmatrix} 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}, \\quad b = \\begin{pmatrix} \\sqrt{3} \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nStarting with an initial guess $x_0 = \\mathbf{0}$, where $\\mathbf{0}$ is the 4-dimensional zero vector, the GMRES(2) algorithm is applied to approximate the solution.\n\nCalculate the Euclidean norm (2-norm) of the residual vector, $r_k = b - Ax_k$, after 5 full cycles of the algorithm (i.e., at iteration $k=10$). Round your final answer to four significant figures.", "solution": "Let $e_{1},e_{2},e_{3},e_{4}$ denote the standard basis of $\\mathbb{R}^{4}$. The matrix $A$ is the cyclic shift:\n$$\nA e_{1}=e_{2},\\quad A e_{2}=e_{3},\\quad A e_{3}=e_{4},\\quad A e_{4}=e_{1}.\n$$\nStart with $x_{0}=\\mathbf{0}$. The initial residual is\n$$\nr_{0}=b-Ax_{0}=b=\\sqrt{3}\\,e_{1},\\qquad \\|r_{0}\\|_{2}=\\sqrt{3},\\qquad v_{1}=\\frac{r_{0}}{\\|r_{0}\\|_{2}}=e_{1}.\n$$\nApply one GMRES(2) cycle (two-step Arnoldi) starting from $v_{1}$.\n\nStep 1: $w=A v_{1}=e_{2}$, $h_{11}=v_{1}^{T}w=0$, $w\\leftarrow w-h_{11}v_{1}=e_{2}$, $h_{21}=\\|w\\|_{2}=1$, $v_{2}=w/h_{21}=e_{2}$.\n\nStep 2: $w=A v_{2}=e_{3}$, $h_{12}=v_{1}^{T}w=0$, $w\\leftarrow w-h_{12}v_{1}=e_{3}$, $h_{22}=v_{2}^{T}w=0$, $w\\leftarrow w-h_{22}v_{2}=e_{3}$, $h_{32}=\\|w\\|_{2}=1$.\n\nThus the Arnoldi relation with $V_{2}=[v_{1},v_{2}]=[e_{1},e_{2}]$ and $\\widetilde{H}_{2}\\in\\mathbb{R}^{3\\times 2}$ is\n$$\n\\widetilde{H}_{2}=\\begin{pmatrix}0 & 0\\\\ 1 & 0\\\\ 0 & 1\\end{pmatrix},\\qquad A V_{2}=[A e_{1},A e_{2}]=[e_{2},e_{3}].\n$$\nGMRES(2) computes $y\\in\\mathbb{R}^{2}$ minimizing\n$$\n\\left\\|\\beta e_{1}-\\widetilde{H}_{2}y\\right\\|_{2},\\qquad \\beta=\\|r_{0}\\|_{2}=\\sqrt{3}.\n$$\nSince $\\widetilde{H}_{2}y=[0,\\,y_{1},\\,y_{2}]^{T}$, the objective is\n$$\n\\left\\|\\begin{pmatrix}\\sqrt{3}\\\\ 0\\\\ 0\\end{pmatrix}-\\begin{pmatrix}0\\\\ y_{1}\\\\ y_{2}\\end{pmatrix}\\right\\|_{2}^{2}=\\left(\\sqrt{3}\\right)^{2}+y_{1}^{2}+y_{2}^{2}.\n$$\nThis is minimized at $y_{1}=0$, $y_{2}=0$, yielding $y=\\mathbf{0}$, so the GMRES(2) update is\n$$\nx_{1}=x_{0}+V_{2}y=x_{0},\\qquad r_{1}=b-Ax_{1}=r_{0},\\qquad \\|r_{1}\\|_{2}=\\sqrt{3}.\n$$\nTherefore, after the first full GMRES(2) cycle, the residual is unchanged. Restarting with $x_{1}$ yields the same initial residual and the same Arnoldi quantities, so each subsequent GMRES(2) cycle again produces $y=\\mathbf{0}$ and no change. Hence, after any number of full cycles, the residual norm remains $\\sqrt{3}$.\n\nAfter $5$ full cycles (i.e., $k=10$ iterations), we have\n$$\n\\|r_{10}\\|_{2}=\\sqrt{3}\\approx 1.7320508075688772\\ldots,\n$$\nwhich rounded to four significant figures is $1.732$.", "answer": "$$\\boxed{1.732}$$", "id": "2183305"}]}