{"hands_on_practices": [{"introduction": "The Dogleg Method constructs a path towards an optimal solution by cleverly combining two key vectors. The first, and most ambitious, is the Newton step, which represents the unconstrained minimizer of the quadratic model of the objective function. This exercise [@problem_id:2212755] provides practice in calculating this fundamental step, which serves as the ultimate destination on the dogleg path.", "problem": "In a trust-region optimization algorithm, at the current iterate, the objective function is approximated by a quadratic model of the form $m(p) = f + g^T p + \\frac{1}{2} p^T B p$. This model describes the change in the objective function for a step $p$ from the current point. A key component in many trust-region strategies, such as the dogleg method, is the Newton step, which is the unconstrained minimizer of this quadratic model.\n\nGiven a gradient vector $g = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}$ and a positive-definite Hessian approximation matrix $B = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}$, calculate the Newton step $p_N$ for this quadratic model.", "solution": "The Newton step is the unconstrained minimizer of the quadratic model $m(p) = f + g^{T} p + \\frac{1}{2} p^{T} B p$. For a symmetric positive-definite $B$, the first-order optimality condition is obtained by setting the gradient with respect to $p$ to zero:\n$$\n\\nabla m(p) = g + B p = 0.\n$$\nThis yields the linear system\n$$\nB p_{N} = -g,\n$$\nso the Newton step is\n$$\np_{N} = -B^{-1} g.\n$$\nWith $g = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}$ and $B = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}$, the inverse is\n$$\nB^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix}.\n$$\nTherefore,\n$$\nB^{-1} g = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 2 \\end{pmatrix},\n$$\nand\n$$\np_{N} = - \\begin{pmatrix} -2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}}$$", "id": "2212755"}, {"introduction": "While the Newton step points to the model's minimum, it may lie far outside our trusted area, making it too aggressive. The Dogleg Method's second key component is the Cauchy point, a more conservative step that finds the minimum along the steepest descent direction within the trust region's bounds. This practice [@problem_id:2212704] hones your ability to compute this 'safe' step, which forms the first segment of the dogleg path and ensures progress.", "problem": "In the field of numerical optimization, trust-region methods are a class of algorithms for solving nonlinear programming problems. These methods iteratively find a step $p$ by minimizing a model function $m(p)$ of the true objective function within a \"trust region\" of radius $\\Delta$ around the current point $x_k$. The model is typically a quadratic approximation:\n$$m(p) = f(x_k) + g^T p + \\frac{1}{2} p^T B p$$\nwhere $g$ is the gradient of the objective function at $x_k$ and $B$ is an approximation to the Hessian matrix.\n\nA fundamental component of many trust-region algorithms (like the dogleg method) is the calculation of the Cauchy point, $p_U$. The Cauchy point is defined as the vector that minimizes the quadratic model $m(p)$ along the direction of steepest descent, subject to the constraint that the step lies within the trust region, i.e., $\\|p\\| \\le \\Delta$.\n\nConsider an optimization problem where, at the current iterate, the gradient is given by $g = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ and the Hessian approximation is the identity matrix, $B = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. The trust-region radius is set to $\\Delta = 4$.\n\nCalculate the Cauchy point $p_U$ for this scenario. Your answer should be a single 2x1 column vector containing exact fractions.", "solution": "The problem asks for the Cauchy point, $p_U$, which is the solution to the following constrained optimization problem:\n$$ \\min_{p} \\quad m(p) = f(x_k) + g^T p + \\frac{1}{2} p^T B p $$\n$$ \\text{subject to} \\quad p = -\\tau g \\text{ for some } \\tau \\ge 0, \\quad \\text{and} \\quad \\|p\\| \\le \\Delta $$\n\nThe direction of search is the steepest descent direction, $d = -g$. Any point along this direction can be parameterized as $p(\\tau) = -\\tau g$ for a scalar $\\tau \\ge 0$.\n\nFirst, we substitute this into the quadratic model $m(p)$ to obtain a function of a single variable, $\\tau$:\n$$ \\phi(\\tau) = m(-\\tau g) = f(x_k) + g^T(-\\tau g) + \\frac{1}{2} (-\\tau g)^T B (-\\tau g) $$\n$$ \\phi(\\tau) = f(x_k) - \\tau (g^T g) + \\frac{1}{2} \\tau^2 (g^T B g) $$\nThe constant term $f(x_k)$ does not affect the minimizer, so we can focus on minimizing the $\\tau$-dependent part.\n\nNext, we translate the trust-region constraint $\\|p\\| \\le \\Delta$ into a constraint on $\\tau$:\n$$ \\|p(\\tau)\\| = \\|-\\tau g\\| = |\\tau| \\|g\\| = \\tau \\|g\\| \\le \\Delta $$\nSince we are seeking a step in the descent direction, $\\tau \\ge 0$. Thus, the constraint on $\\tau$ is:\n$$ 0 \\le \\tau \\le \\frac{\\Delta}{\\|g\\|} $$\n\nNow, let's find the unconstrained minimizer of the quadratic $\\phi(\\tau)$ by taking its derivative with respect to $\\tau$ and setting it to zero. This will give us the optimal step length, $\\tau^*$, in the absence of the trust-region constraint.\n$$ \\frac{d\\phi}{d\\tau} = -g^T g + \\tau (g^T B g) = 0 $$\nSolving for $\\tau$, we get:\n$$ \\tau^* = \\frac{g^T g}{g^T B g} $$\nThe second derivative is $\\frac{d^2\\phi}{d\\tau^2} = g^T B g$. If this is positive, we have found a minimum.\n\nLet's compute the required quantities using the given values:\n$g = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$, $B = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, and $\\Delta = 4$.\n\n1.  Calculate $g^T g$:\n    $g^T g = (3)(3) + (-4)(-4) = 9 + 16 = 25$.\n\n2.  Calculate $\\|g\\|$:\n    $\\|g\\| = \\sqrt{g^T g} = \\sqrt{25} = 5$.\n\n3.  Calculate $g^T B g$:\n    Since $B=I$, $g^T B g = g^T I g = g^T g = 25$.\n\nThe second derivative $g^T B g = 25 > 0$, so $\\phi(\\tau)$ is a convex parabola, and its unconstrained minimum is indeed at $\\tau^*$.\n\n4.  Calculate the unconstrained optimal step length $\\tau^*$:\n    $$ \\tau^* = \\frac{g^T g}{g^T B g} = \\frac{25}{25} = 1 $$\n\n5.  Determine the feasible interval for $\\tau$:\n    The constraint is $0 \\le \\tau \\le \\frac{\\Delta}{\\|g\\|}$.\n    $$ \\frac{\\Delta}{\\|g\\|} = \\frac{4}{5} = 0.8 $$\n    So, the feasible interval for $\\tau$ is $[0, 0.8]$.\n\n6.  Find the constrained optimal step length, $\\tau_U$. The unconstrained minimizer $\\tau^* = 1$ lies outside the feasible interval $[0, 0.8]$. Since $\\phi(\\tau)$ is a convex parabola with its minimum at $\\tau=1$, the minimum value of $\\phi(\\tau)$ over the interval $[0, 0.8]$ must occur at the boundary point closest to $\\tau=1$. This point is $\\tau = 0.8$.\n    Therefore, the step length for the Cauchy point is $\\tau_U = \\frac{\\Delta}{\\|g\\|} = \\frac{4}{5}$.\n\n7.  Calculate the Cauchy point $p_U$:\n    $$ p_U = -\\tau_U g = -\\frac{4}{5} \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} -12/5 \\\\ 16/5 \\end{pmatrix} $$\n\nThe Cauchy point is $p_U = \\begin{pmatrix} -12/5 \\\\ 16/5 \\end{pmatrix}$. Let's verify its norm: $\\|p_U\\| = \\sqrt{(-12/5)^2 + (16/5)^2} = \\sqrt{\\frac{144+256}{25}} = \\sqrt{\\frac{400}{25}} = \\sqrt{16} = 4$, which is exactly equal to the trust-region radius $\\Delta$, as expected.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{16}{5} \\end{pmatrix}\n}\n$$", "id": "2212704"}, {"introduction": "Having mastered the calculation of the two primary components of the dogleg path, we now put them together to find the final trial step. When the trust region is too small to contain the full Newton step but large enough to extend beyond the Cauchy point, the optimal step lies on the 'dogleg' segment connecting the two. This exercise [@problem_id:2212758] provides hands-on experience in interpolating between these points to find the final step on the trust-region boundary, which is the core of the method's ingenuity.", "problem": "In the context of solving a trust-region subproblem in numerical optimization, the dogleg method is a popular technique for finding an approximate solution. This method constructs a trial step $p$ by following a path defined by two key vectors: the Cauchy point $p_U$ (the minimizer along the steepest descent direction) and the Newton point $p_N$ (the unconstrained minimizer of the quadratic model). The path consists of a line segment from the origin to $p_U$, followed by a line segment from $p_U$ to $p_N$. The final dogleg step $p$ is the point along this path that is farthest from the origin but still lies within the trust region, a sphere of radius $\\Delta$.\n\nConsider a specific iteration of an optimization algorithm where the Cauchy point and the Newton point are given by the vectors $p_U = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $p_N = \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix}$, respectively. The trust-region radius for this iteration is $\\Delta = 3$.\n\nDetermine the dogleg step vector $p$. Present your answer as a column vector with exact components.", "solution": "We are given $p_{U}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, $p_{N}=\\begin{pmatrix}4 \\\\ 4\\end{pmatrix}$, and trust-region radius $\\Delta=3$. Compute the norms:\n$$\n\\|p_{U}\\|=\\sqrt{1^{2}+1^{2}}=\\sqrt{2},\\quad \\|p_{N}\\|=\\sqrt{4^{2}+4^{2}}=4\\sqrt{2}.\n$$\nSince $\\|p_{U}\\|<\\Delta<\\|p_{N}\\|$, the dogleg step lies on the segment from $p_{U}$ to $p_{N}$ and has norm exactly $\\Delta$. Parameterize the second segment by\n$$\np(\\tau)=p_{U}+\\tau\\,(p_{N}-p_{U}),\\quad \\tau\\in[0,1],\n$$\nwith $p_{N}-p_{U}=\\begin{pmatrix}3 \\\\ 3\\end{pmatrix}$. Then\n$$\np(\\tau)=\\begin{pmatrix}1+3\\tau \\\\ 1+3\\tau\\end{pmatrix},\\quad \\|p(\\tau)\\|^{2}=2\\,(1+3\\tau)^{2}.\n$$\nImpose the trust-region boundary condition $\\|p(\\tau)\\|=\\Delta$:\n$$\n2\\,(1+3\\tau)^{2}=9\\;\\Rightarrow\\;(1+3\\tau)^{2}=\\frac{9}{2}\\;\\Rightarrow\\;1+3\\tau=\\frac{3}{\\sqrt{2}},\n$$\nwhere the positive root is taken because $\\tau\\geq 0$. Hence\n$$\n3\\tau=\\frac{3}{\\sqrt{2}}-1\\;\\Rightarrow\\;\\tau=\\frac{1}{\\sqrt{2}}-\\frac{1}{3},\n$$\nwhich lies in $[0,1]$. Therefore\n$$\np=p(\\tau)=\\begin{pmatrix}1+3\\tau \\\\ 1+3\\tau\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{\\sqrt{2}} \\\\ \\frac{3}{\\sqrt{2}}\\end{pmatrix}.\n$$\nEquivalently, rationalizing denominators gives $p=\\begin{pmatrix}\\frac{3\\sqrt{2}}{2} \\\\ \\frac{3\\sqrt{2}}{2}\\end{pmatrix}$. Both are exact forms.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3\\sqrt{2}}{2} \\\\ \\frac{3\\sqrt{2}}{2}\\end{pmatrix}}$$", "id": "2212758"}]}