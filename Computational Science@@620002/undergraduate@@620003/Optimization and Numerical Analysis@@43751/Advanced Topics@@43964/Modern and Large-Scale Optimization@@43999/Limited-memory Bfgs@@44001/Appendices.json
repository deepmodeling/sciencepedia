{"hands_on_practices": [{"introduction": "The efficiency of the L-BFGS algorithm stems from its clever use of a limited memory of past iterations to approximate the problem's curvature. This memory consists of vector pairs $(s_k, y_k)$, which track changes in position and gradient. This first exercise [@problem_id:2184596] will guide you through the fundamental process of computing these history vectors from a sequence of optimizer steps, solidifying your understanding of the raw data that powers the algorithm.", "problem": "You are analyzing the behavior of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm, a popular quasi-Newton method for unconstrained optimization. The algorithm builds an approximation of the inverse Hessian matrix by storing the $m$ most recent pairs of vectors $(s_k, y_k)$. Here, $x_k$ is the iterate at step $k$, $s_k = x_{k+1} - x_k$ is the displacement vector, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient vector for some objective function $f(x)$.\n\nConsider the optimization of the two-dimensional convex quadratic function $f(x) = f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. An optimization routine has produced the following sequence of three iterates (position vectors):\n$$\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix}\n$$\nCalculate the two pairs of history vectors, $(s_0, y_0)$ and $(s_1, y_1)$, that an L-BFGS algorithm would store based on this sequence of iterates.\n\nExpress your answer as a single $2 \\times 4$ matrix where the columns represent the vectors $s_0$, $y_0$, $s_1$, and $y_1$ in that specific order. Use fractions for any non-integer values.", "solution": "The goal is to compute the displacement vectors $s_k = x_{k+1} - x_k$ and the gradient difference vectors $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ for $k=0$ and $k=1$.\n\nFirst, we need to find the gradient of the objective function $f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 - 2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 3 \\cdot 2(x_2 + 1) = 6(x_2 + 1)\n$$\nSo, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} 2(x_1 - 2) \\\\ 6(x_2 + 1) \\end{pmatrix}\n$$\n\nNext, we evaluate the gradient at each of the given iterates $x_0$, $x_1$, and $x_2$. Let's denote these gradients as $g_0$, $g_1$, and $g_2$.\n\nFor $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$\ng_0 = \\nabla f(0, 0) = \\begin{pmatrix} 2(0 - 2) \\\\ 6(0 + 1) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}\n$$\n\nFor $x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$:\n$$\ng_1 = \\nabla f(1, -2) = \\begin{pmatrix} 2(1 - 2) \\\\ 6(-2 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(-1) \\\\ 6(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix}\n$$\n\nFor $x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\end{pmatrix}$:\n$$\ng_2 = \\nabla f(2, -1.5) = \\begin{pmatrix} 2(2 - 2) \\\\ 6(-1.5 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(0) \\\\ 6(-0.5) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}\n$$\n\nNow we can compute the displacement vectors $s_0$ and $s_1$.\n\nFor $k=0$:\n$$\ns_0 = x_1 - x_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\ns_1 = x_2 - x_1 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1.5 - (-2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n\nNext, we compute the gradient difference vectors $y_0$ and $y_1$.\n\nFor $k=0$:\n$$\ny_0 = g_1 - g_0 = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} - \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} -2 - (-4) \\\\ -6 - 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\ny_1 = g_2 - g_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 0 - (-2) \\\\ -3 - (-6) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n\nFinally, we assemble the results into a $2 \\times 4$ matrix where the columns are $s_0, y_0, s_1, y_1$.\n$$\ns_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad y_0 = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}, \\quad s_1 = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad y_1 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nThe resulting matrix is:\n$$\n\\begin{pmatrix} 1 & 2 & 1 & 2 \\\\ -2 & -12 & \\frac{1}{2} & 3 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2 & 1 & 2 \\\\ -2 & -12 & \\frac{1}{2} & 3 \\end{pmatrix}}\n$$", "id": "2184596"}, {"introduction": "With the history vectors $(s_k, y_k)$ in hand, the next step is to use them to generate an improved search direction. The L-BFGS algorithm achieves this without ever forming a large matrix, using a clever procedure known as the two-loop recursion. In this practice [@problem_id:2184578], you will manually execute this recursion, providing a clear view into the algorithm's engine room and demystifying how it implicitly applies an inverse Hessian approximation.", "problem": "The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm is a popular quasi-Newton method for unconstrained optimization. In each iteration $k$, the algorithm computes a search direction $p_k$ by applying an approximation of the inverse Hessian matrix to the negative of the current gradient, $g_k = \\nabla f(x_k)$. This approximation is constructed implicitly using a limited history of the $m$ most recent steps.\n\nThe history is stored as pairs of vectors $(s_i, y_i)$ for $i=k-m, \\dots, k-1$, where $s_i = x_{i+1} - x_i$ is the change in position and $y_i = g_{i+1} - g_i$ is the change in the gradient. The search direction $p_k$ is then found by a procedure known as the L-BFGS two-loop recursion.\n\nConsider an L-BFGS update at step $k$ with a memory size of $m=2$. The relevant data available from previous steps are:\n- Current gradient: $g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- History from step $k-1$: $s_{k-1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{k-1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- History from step $k-2$: $s_{k-2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{k-2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n\nYour task is to compute the search direction vector $p_k$ for this iteration. Express your answer as a $2 \\times 1$ column vector with exact rational components.", "solution": "The L-BFGS search direction $p_k$ is computed by approximating the product $-H_k g_k$, where $H_k$ is the inverse Hessian approximation. This is achieved efficiently using the two-loop recursion algorithm. We are given $m=2$, the gradient $g_k$, and the history vectors $(s_{k-1}, y_{k-1})$ and $(s_{k-2}, y_{k-2})$.\n\nThe algorithm is as follows:\n\n1.  Initialize a vector $q$ with the current gradient:\n    $q = g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\n2.  **First Loop (backward pass):** This loop iterates from $i = k-1$ down to $i = k-m$. In our case, $i$ goes from $k-1$ to $k-2$.\n    We first pre-calculate the scalars $\\rho_i = \\frac{1}{y_i^T s_i}$.\n    For $i = k-1$:\n    $y_{k-1}^T s_{k-1} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (1)(0) = 1$.\n    So, $\\rho_{k-1} = \\frac{1}{1} = 1$.\n\n    For $i = k-2$:\n    $y_{k-2}^T s_{k-2} = \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (-1)(0) + (2)(1) = 2$.\n    So, $\\rho_{k-2} = \\frac{1}{2}$.\n\n    Now, we perform the loop updates. We will also store the computed $\\alpha_i$ values, as they are needed in the second loop.\n    -   **For $i = k-1$**:\n        $\\alpha_{k-1} = \\rho_{k-1} s_{k-1}^T q = (1) \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = (1)((1)(1) + (0)(-2)) = 1$.\n        $q \\leftarrow q - \\alpha_{k-1} y_{k-1} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - (1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ -2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n\n    -   **For $i = k-2$**:\n        $\\alpha_{k-2} = \\rho_{k-2} s_{k-2}^T q = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((0)(0) + (1)(-3)) = -\\frac{3}{2}$.\n        $q \\leftarrow q - \\alpha_{k-2} y_{k-2} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\left(-\\frac{3}{2}\\right) \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} + \\frac{3}{2}\\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} -3/2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -3/2 \\\\ 0 \\end{pmatrix}$.\n\n3.  **Initial Hessian Scaling:** The initial inverse Hessian approximation $H_k^0$ is a diagonal matrix $\\gamma_k I$, where $\\gamma_k = \\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}}$. We initialize our result vector $r$ by multiplying this scaled identity matrix with the current $q$.\n    $s_{k-1}^T y_{k-1} = (1)(1) + (0)(1) = 1$.\n    $y_{k-1}^T y_{k-1} = (1)^2 + (1)^2 = 2$.\n    $\\gamma_k = \\frac{1}{2}$.\n    $r = \\gamma_k q = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix}$.\n\n4.  **Second Loop (forward pass):** This loop iterates from $i = k-m$ up to $i = k-1$. In our case, $i$ goes from $k-2$ to $k-1$.\n    -   **For $i = k-2$**:\n        $\\beta = \\rho_{k-2} y_{k-2}^T r = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((-1)(-\\frac{3}{4}) + (2)(0)) = \\frac{3}{8}$.\n        $r \\leftarrow r + s_{k-2} (\\alpha_{k-2} - \\beta) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{3}{2} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{12}{8} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n    -   **For $i = k-1$**:\n        $\\beta = \\rho_{k-1} y_{k-1}^T r = (1) \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = (1)(-\\frac{6}{8}) + (1)(-\\frac{15}{8}) = -\\frac{21}{8}$.\n        $r \\leftarrow r + s_{k-1} (\\alpha_{k-1} - \\beta) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(1 - \\left(-\\frac{21}{8}\\right)\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(\\frac{8}{8} + \\frac{21}{8}\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} \\frac{29}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n5.  The final result of the two-loop recursion is the vector $r = H_k g_k$. The search direction is $p_k = -r$.\n    $p_k = - \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}}$$", "id": "2184578"}, {"introduction": "The theoretical advantage of a quasi-Newton method lies in its ability to generate search directions that are better adapted to the local geometry of the function than the simple negative gradient. This final practice problem [@problem_id:2184555] makes this concept concrete by asking you to compute and compare the search direction from L-BFGS with that from the steepest descent method. By analyzing the difference between these two vectors, you will develop a tangible intuition for how L-BFGS uses curvature information to accelerate convergence.", "problem": "In the field of numerical optimization, quasi-Newton methods are popular for finding the minimum of a function. One such method is the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, which approximates the inverse Hessian matrix using information from a limited number of previous iterations.\n\nConsider the task of minimizing the quadratic objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(x_1, x_2) = x_1^2 + 4x_2^2$$\nAn optimization algorithm is currently at the iterate $\\mathbf{x}_1 = (2, -0.5)^T$. The previous iterate was $\\mathbf{x}_0 = (4, 1)^T$. We wish to compare the search direction proposed by two different methods at point $\\mathbf{x}_1$.\n\nThe first method is the method of steepest descent, where the search direction $\\mathbf{p}_{\\text{SD}}$ is simply the negative of the gradient at the current point, i.e., $\\mathbf{p}_{\\text{SD}} = -\\nabla f(\\mathbf{x}_1)$.\n\nThe second method is L-BFGS with a memory of $m=1$. The search direction $\\mathbf{p}_{\\text{L-BFGS}}$ at the current iterate $\\mathbf{x}_1$ is calculated as $\\mathbf{p}_{\\text{L-BFGS}} = -\\mathbf{r}$, where the vector $\\mathbf{r}$ is the result of the following procedure (known as the two-loop recursion):\nLet $\\mathbf{g}_1 = \\nabla f(\\mathbf{x}_1)$ be the current gradient. From the previous step, we have the vectors $\\mathbf{s}_0 = \\mathbf{x}_1 - \\mathbf{x}_0$ and $\\mathbf{y}_0 = \\nabla f(\\mathbf{x}_1) - \\nabla f(\\mathbf{x}_0)$.\n1.  Initialize a temporary vector $\\mathbf{q} \\leftarrow \\mathbf{g}_1$.\n2.  Compute the scalar $\\rho_0 = \\frac{1}{\\mathbf{y}_0^T \\mathbf{s}_0}$.\n3.  Compute the scalar $\\alpha_0 = \\rho_0 (\\mathbf{s}_0^T \\mathbf{q})$.\n4.  Update the temporary vector: $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_0 \\mathbf{y}_0$.\n5.  Compute the initial Hessian scaling factor $\\gamma_0 = \\frac{\\mathbf{s}_0^T \\mathbf{y}_0}{\\mathbf{y}_0^T \\mathbf{y}_0}$.\n6.  Initialize the result vector $\\mathbf{r} \\leftarrow \\gamma_0 \\mathbf{q}$.\n7.  Compute the scalar $\\beta_0 = \\rho_0 (\\mathbf{y}_0^T \\mathbf{r})$.\n8.  Update the result vector: $\\mathbf{r} \\leftarrow \\mathbf{r} + (\\alpha_0 - \\beta_0) \\mathbf{s}_0$.\n\nYour task is to calculate the cosine of the angle $\\theta$ between the steepest descent search direction $\\mathbf{p}_{\\text{SD}}$ and the L-BFGS search direction $\\mathbf{p}_{\\text{L-BFGS}}$ at the point $\\mathbf{x}_1$. Report your answer as a numerical value rounded to four significant figures.", "solution": "The objective is $f(x_{1},x_{2})=x_{1}^{2}+4x_{2}^{2}$, so the gradient is $\\nabla f(x_{1},x_{2})=(2x_{1},\\,8x_{2})^{T}$.\n\nAt $\\mathbf{x}_{0}=(4,1)^{T}$, the gradient is $\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=(8,8)^{T}$. At $\\mathbf{x}_{1}=(2,-0.5)^{T}$, the gradient is $\\mathbf{g}_{1}=\\nabla f(\\mathbf{x}_{1})=(4,-4)^{T}$.\n\nThe steepest descent direction at $\\mathbf{x}_{1}$ is\n$$\n\\mathbf{p}_{\\text{SD}}=-\\mathbf{g}_{1}=(-4,\\,4)^{T}.\n$$\n\nFor L-BFGS with $m=1$, define $\\mathbf{s}_{0}=\\mathbf{x}_{1}-\\mathbf{x}_{0}=(-2,\\,-\\tfrac{3}{2})^{T}$ and $\\mathbf{y}_{0}=\\mathbf{g}_{1}-\\mathbf{g}_{0}=(-4,\\,-12)^{T}$. Compute\n$$\n\\mathbf{y}_{0}^{T}\\mathbf{s}_{0}=(-4)(-2)+(-12)\\!\\left(-\\tfrac{3}{2}\\right)=26,\\qquad \\rho_{0}=\\frac{1}{\\mathbf{y}_{0}^{T}\\mathbf{s}_{0}}=\\frac{1}{26}.\n$$\nInitialize $\\mathbf{q}\\leftarrow\\mathbf{g}_{1}=(4,-4)^{T}$ and compute\n$$\n\\alpha_{0}=\\rho_{0}(\\mathbf{s}_{0}^{T}\\mathbf{q})=\\frac{1}{26}\\big[(-2)(4)+\\left(-\\tfrac{3}{2}\\right)(-4)\\big]=-\\frac{1}{13}.\n$$\nUpdate\n$$\n\\mathbf{q}\\leftarrow \\mathbf{q}-\\alpha_{0}\\mathbf{y}_{0}=(4,-4)^{T}-\\left(-\\frac{1}{13}\\right)(-4,-12)^{T}=\\left(\\frac{48}{13},\\,-\\frac{64}{13}\\right)^{T}.\n$$\nCompute the scaling\n$$\n\\gamma_{0}=\\frac{\\mathbf{s}_{0}^{T}\\mathbf{y}_{0}}{\\mathbf{y}_{0}^{T}\\mathbf{y}_{0}}=\\frac{26}{16+144}=\\frac{13}{80},\n$$\nand initialize\n$$\n\\mathbf{r}\\leftarrow \\gamma_{0}\\mathbf{q}=\\frac{13}{80}\\left(\\frac{48}{13},\\,-\\frac{64}{13}\\right)^{T}=\\left(\\frac{3}{5},\\,-\\frac{4}{5}\\right)^{T}.\n$$\nThen\n$$\n\\beta_{0}=\\rho_{0}(\\mathbf{y}_{0}^{T}\\mathbf{r})=\\frac{1}{26}\\left[(-4)\\!\\left(\\frac{3}{5}\\right)+(-12)\\!\\left(-\\frac{4}{5}\\right)\\right]=\\frac{18}{65}.\n$$\nUpdate\n$$\n\\mathbf{r}\\leftarrow \\mathbf{r}+(\\alpha_{0}-\\beta_{0})\\mathbf{s}_{0}=\\left(\\frac{3}{5},-\\frac{4}{5}\\right)^{T}+\\left(-\\frac{1}{13}-\\frac{18}{65}\\right)\\!\\left(-2,-\\frac{3}{2}\\right)^{T}=\\left(\\frac{17}{13},-\\frac{7}{26}\\right)^{T}.\n$$\nHence the L-BFGS direction is\n$$\n\\mathbf{p}_{\\text{L-BFGS}}=-\\mathbf{r}=\\left(-\\frac{17}{13},\\,\\frac{7}{26}\\right)^{T}.\n$$\n\nThe cosine of the angle between $\\mathbf{p}_{\\text{SD}}$ and $\\mathbf{p}_{\\text{L-BFGS}}$ is\n$$\n\\cos\\theta=\\frac{\\mathbf{p}_{\\text{SD}}^{T}\\mathbf{p}_{\\text{L-BFGS}}}{\\|\\mathbf{p}_{\\text{SD}}\\|\\,\\|\\mathbf{p}_{\\text{L-BFGS}}\\|}.\n$$\nCompute the numerator and norms:\n$$\n\\mathbf{p}_{\\text{SD}}^{T}\\mathbf{p}_{\\text{L-BFGS}}=(-4,4)\\cdot\\left(-\\frac{17}{13},\\frac{7}{26}\\right)=\\frac{82}{13},\n$$\n$$\n\\|\\mathbf{p}_{\\text{SD}}\\|=\\sqrt{(-4)^{2}+4^{2}}=4\\sqrt{2},\\qquad\n\\|\\mathbf{p}_{\\text{L-BFGS}}\\|=\\sqrt{\\left(\\frac{17}{13}\\right)^{2}+\\left(\\frac{7}{26}\\right)^{2}}=\\frac{\\sqrt{1205}}{26}.\n$$\nTherefore\n$$\n\\cos\\theta=\\frac{\\frac{82}{13}}{4\\sqrt{2}\\,\\frac{\\sqrt{1205}}{26}}\n=\\frac{41}{\\sqrt{2410}}.\n$$\nNumerically, $\\cos\\theta=\\frac{41}{\\sqrt{2410}}\\approx 0.8352$ when rounded to four significant figures.", "answer": "$$\\boxed{0.8352}$$", "id": "2184555"}]}