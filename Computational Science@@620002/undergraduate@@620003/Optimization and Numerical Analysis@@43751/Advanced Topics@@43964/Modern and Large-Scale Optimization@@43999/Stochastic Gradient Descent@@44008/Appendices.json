{"hands_on_practices": [{"introduction": "Understanding an algorithm often begins with mastering its most fundamental operation. For Stochastic Gradient Descent (SGD), this is the single update step, where we adjust our model's parameters based on the error from just one data point. This exercise [@problem_id:2206637] walks you through a concrete calculation of this step, providing a solid foundation for how SGD iteratively learns from data.", "problem": "An iterative optimization algorithm is used to find a parameter $x$ that minimizes a cost function. The total cost function is an average of several component functions: $F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$. In this specific case, the component functions are quadratic and given by $f_i(x) = (x - c_i)^2$, where the constants are $c_i = i$ for $i = 1, 2, \\dots, 10$, and thus $N=10$.\n\nThe optimization process starts with an initial guess for the parameter, $x_0$. At each step, a new estimate, $x_{k+1}$, is calculated from the current estimate, $x_k$, by using only a single, randomly chosen component function, $f_j(x)$. The update rule is defined as:\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\nwhere $\\eta$ is a constant known as the learning rate.\n\nGiven an initial parameter value of $x_0 = 10.0$ and a learning rate of $\\eta = 0.1$, compute the value of the parameter $x_1$ after one update step. For this first step, the component function used is $f_j(x)$ with the index $j=5$.", "solution": "We are given component functions of the form $f_{i}(x) = (x - c_{i})^{2}$ with $c_{i} = i$. For the first update, the chosen index is $j=5$, so $f_{5}(x) = (x - 5)^{2}$.\n\nThe update rule is\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\nUsing the power rule and chain rule, the derivative of the chosen component is\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\nEvaluating at the current iterate $x_{0} = 10$ gives\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\nWith learning rate $\\eta = 0.1$, the update becomes\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\nThus, after one update step using $f_{5}$, the parameter value is $x_{1} = 9$.", "answer": "$$\\boxed{9}$$", "id": "2206637"}, {"introduction": "Why is Stochastic Gradient Descent 'stochastic'? It's because the gradient calculated from a single data point is only a noisy estimate of the true gradient over the entire dataset. This exercise [@problem_id:2206620] delves into this core concept by having you calculate the variance of the gradient estimator, which is a direct measure of this 'noise'. Understanding this variance is key to grasping the behavior and trade-offs of SGD.", "problem": "In many machine learning problems, the goal is to minimize a loss function $F(x)$ that is structured as an average over a dataset. A common form is $F(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x)$, where $f_i(x)$ is the loss associated with the $i$-th data point and $x$ is a model parameter.\n\nConsider a simplified one-dimensional problem where we want to find the parameter $x$ that minimizes a loss function. The dataset consists of just two data points ($N=2$), leading to the following component loss functions:\n$$f_1(x) = (x-2)^2$$\n$$f_2(x) = (x+2)^2$$\nThe total loss function is therefore $F(x) = \\frac{1}{2} (f_1(x) + f_2(x))$.\n\nStochastic Gradient Descent (SGD) is an iterative optimization algorithm that approximates the true gradient of $F(x)$ at each step. In its simplest form, a stochastic gradient estimator, which we will denote as $g(x)$, is computed by first selecting an index $i$ uniformly at random from $\\{1, 2\\}$, and then calculating the gradient of the corresponding component function, $g(x) = \\nabla f_i(x)$. In this one-dimensional case, the gradient operator $\\nabla$ is simply the derivative with respect to $x$, i.e., $\\frac{d}{dx}$.\n\nCalculate the variance of the stochastic gradient estimator $g(x)$ at the specific point $x = 1$.", "solution": "We are given $f_{1}(x)=(x-2)^{2}$ and $f_{2}(x)=(x+2)^{2}$, and the stochastic gradient estimator $g(x)$ is defined by choosing $i$ uniformly from $\\{1,2\\}$ and setting $g(x)=\\frac{d}{dx}f_{i}(x)$. First compute the component gradients:\n$$\n\\frac{d}{dx}f_{1}(x)=2(x-2), \\quad \\frac{d}{dx}f_{2}(x)=2(x+2).\n$$\nAt $x=1$, $g(1)$ takes the values\n$$\ng(1)=2(1-2)=-2 \\quad \\text{with probability } \\frac{1}{2}, \\quad g(1)=2(1+2)=6 \\quad \\text{with probability } \\frac{1}{2}.\n$$\nCompute the mean of $g(1)$:\n$$\n\\mathbb{E}[g(1)]=\\frac{1}{2}(-2)+\\frac{1}{2}(6)=2.\n$$\nThis equals the true gradient of $F$ at $x=1$, since\n$$\nF'(x)=\\frac{1}{2}\\left(2(x-2)+2(x+2)\\right)=2x \\implies F'(1)=2.\n$$\nCompute the second moment:\n$$\n\\mathbb{E}[g(1)^{2}]=\\frac{1}{2}\\left((-2)^{2}+6^{2}\\right)=\\frac{1}{2}(4+36)=20.\n$$\nTherefore, the variance is\n$$\n\\operatorname{Var}(g(1))=\\mathbb{E}[g(1)^{2}]-\\left(\\mathbb{E}[g(1)]\\right)^{2}=20-2^{2}=16.\n$$\nHence, the variance of the stochastic gradient estimator at $x=1$ is $16$.", "answer": "$$\\boxed{16}$$", "id": "2206620"}, {"introduction": "While a standard Gradient Descent step is guaranteed to decrease the loss function (for a small enough step size), the same is not true for SGD. A single step, guided by only a fraction of the data, can sometimes lead to an increase in the overall loss. This practice [@problem_id:2206653] provides a striking example of this phenomenon, helping to build intuition for SGD's characteristic 'zig-zag' convergence path and its ability to explore the parameter space.", "problem": "In the field of machine learning, optimization algorithms are used to train model parameters by minimizing a loss function. Consider a simple model with a single scalar parameter, $w$. The goal is to minimize a total loss function, $F(w)$, which is defined as the average of the individual loss functions over a small dataset of two data points. The total loss function is given by:\n\n$$F(w) = \\frac{1}{2}\\left[f_1(w) + f_2(w)\\right]$$\n\nThe individual loss functions associated with the two data points are:\n\n$$f_1(w) = \\frac{1}{2}(w - 2)^2$$\n$$f_2(w) = \\frac{1}{2}(w - 10)^2$$\n\nThe training process begins with an initial parameter value of $w_0 = 3$. A single update step is performed using the Stochastic Gradient Descent (SGD) algorithm with a learning rate of $\\eta = 2$. For this specific update, the gradient is computed using only the loss function of the first data point, $f_1(w)$.\n\nCalculate the change in the value of the total loss function, $F(w_1) - F(w_0)$, that results from this single SGD update. Round your final answer to three significant figures.", "solution": "We are given $F(w)=\\frac{1}{2}\\left[f_{1}(w)+f_{2}(w)\\right]$ with $f_{1}(w)=\\frac{1}{2}(w-2)^{2}$ and $f_{2}(w)=\\frac{1}{2}(w-10)^{2}$. The initial parameter is $w_{0}=3$. A single SGD step with learning rate $\\eta=2$ uses only the gradient of $f_{1}$.\n\nThe SGD update rule in one dimension is\n$$\nw_{1}=w_{0}-\\eta\\,\\frac{d f_{1}}{d w}\\bigg|_{w=w_{0}}.\n$$\nCompute the derivative:\n$$\n\\frac{d f_{1}}{d w}=\\frac{d}{d w}\\left[\\frac{1}{2}(w-2)^{2}\\right]=(w-2).\n$$\nEvaluate at $w_{0}=3$:\n$$\n\\frac{d f_{1}}{d w}\\bigg|_{w=3}=3-2=1.\n$$\nThus the updated parameter is\n$$\nw_{1}=3-2\\cdot 1=1.\n$$\n\nNow compute $F(w_{0})$:\n$$\nf_{1}(3)=\\frac{1}{2}(3-2)^{2}=\\frac{1}{2},\\quad f_{2}(3)=\\frac{1}{2}(3-10)^{2}=\\frac{1}{2}\\cdot 49=\\frac{49}{2},\n$$\n$$\nF(3)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{49}{2}\\right)=\\frac{1}{2}\\cdot\\frac{50}{2}=\\frac{1}{2}\\cdot 25=\\frac{25}{2}.\n$$\n\nCompute $F(w_{1})$:\n$$\nf_{1}(1)=\\frac{1}{2}(1-2)^{2}=\\frac{1}{2},\\quad f_{2}(1)=\\frac{1}{2}(1-10)^{2}=\\frac{1}{2}\\cdot 81=\\frac{81}{2},\n$$\n$$\nF(1)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{81}{2}\\right)=\\frac{1}{2}\\cdot\\frac{82}{2}=\\frac{1}{2}\\cdot 41=\\frac{41}{2}.\n$$\n\nTherefore, the change in the total loss is\n$$\nF(w_{1})-F(w_{0})=\\frac{41}{2}-\\frac{25}{2}=\\frac{16}{2}=8.\n$$\nRounded to three significant figures, this is $8.00$.", "answer": "$$\\boxed{8.00}$$", "id": "2206653"}]}