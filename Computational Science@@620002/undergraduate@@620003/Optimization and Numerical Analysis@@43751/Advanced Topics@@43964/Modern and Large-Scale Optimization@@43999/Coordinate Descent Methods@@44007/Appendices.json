{"hands_on_practices": [{"introduction": "To truly understand an algorithm, there's no substitute for applying it yourself. This first practice problem invites you to perform one complete cycle of coordinate descent on a simple quadratic function [@problem_id:2164480]. By minimizing the function first along one axis and then along the next, you will gain hands-on experience with the fundamental mechanics of the algorithm and observe its deliberate, axis-aligned progression toward the optimum.", "problem": "Consider the function of two variables $f(x,y) = 2x^2 + y^2 + xy - 6x - 5y$. An optimization routine is employed to find the minimum of this function using the coordinate descent algorithm. The process starts from the initial point $(x_0, y_0) = (0, 0)$.\n\nYou are asked to perform exactly one full cycle of coordinate descent. The update order within the cycle is as follows: first, minimize the function with respect to the $x$-coordinate, and then, using the newly found $x$-coordinate, minimize the function with respect to the $y$-coordinate.\n\nDetermine the coordinates $(x_1, y_1)$ of the point after this single cycle. Your answer should consist of the two coordinate values, expressed as exact fractions.", "solution": "We perform one full cycle of coordinate descent on the quadratic function $f(x,y) = 2x^{2} + y^{2} + xy - 6x - 5y$ starting from $(x_{0}, y_{0}) = (0, 0)$, updating first $x$ and then $y$. For a fixed coordinate, minimizing a differentiable function with respect to that coordinate is done by setting the corresponding partial derivative to zero; since $f$ is quadratic and strictly convex in each coordinate (as verified by positive second partial derivatives), this yields the unique minimizer in that coordinate.\n\nFirst, minimize with respect to $x$ holding $y = y_{0} = 0$ fixed. Compute the partial derivative with respect to $x$:\n$$\n\\frac{\\partial f}{\\partial x} = 4x + y - 6.\n$$\nSet it to zero at $y=0$:\n$$\n4x + 0 - 6 = 0 \\quad \\Longrightarrow \\quad 4x = 6 \\quad \\Longrightarrow \\quad x = \\frac{3}{2}.\n$$\nSince $\\frac{\\partial^{2} f}{\\partial x^{2}} = 4 > 0$, this is the unique minimizer in $x$ for fixed $y=0$. Thus, after the $x$-update we have $x_{1} = \\frac{3}{2}$ and $y$ remains $0$.\n\nNext, minimize with respect to $y$ holding $x = x_{1} = \\frac{3}{2}$ fixed. Compute the partial derivative with respect to $y$:\n$$\n\\frac{\\partial f}{\\partial y} = 2y + x - 5.\n$$\nSet it to zero at $x=\\frac{3}{2}$:\n$$\n2y + \\frac{3}{2} - 5 = 0 \\quad \\Longrightarrow \\quad 2y - \\frac{7}{2} = 0 \\quad \\Longrightarrow \\quad 2y = \\frac{7}{2} \\quad \\Longrightarrow \\quad y = \\frac{7}{4}.\n$$\nSince $\\frac{\\partial^{2} f}{\\partial y^{2}} = 2 > 0$, this is the unique minimizer in $y$ for fixed $x=\\frac{3}{2}$. Therefore, after one full cycle, the updated point is $\\left(x_{1}, y_{1}\\right) = \\left(\\frac{3}{2}, \\frac{7}{4}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{2} & \\frac{7}{4}\\end{pmatrix}}$$", "id": "2164480"}, {"introduction": "An effective way to grasp the essence of an optimization method is to compare it with other foundational techniques. This exercise challenges you to trace the first step of both Coordinate Descent and Gradient Descent from the same starting point [@problem_id:2164428]. By calculating the resulting points from each algorithm, you will directly observe the fundamental difference in their strategies: coordinate descent's constrained movement along axes versus gradient descent's path in the direction of steepest descent.", "problem": "Consider the optimization problem of minimizing the function $f(x, y) = 2x^2 + y^2 - xy + x - 4y$. We will explore the first step of two different iterative optimization algorithms, both starting from the initial point $(x_0, y_0) = (2, 3)$.\n\n1.  **Coordinate Descent (CD):** Perform one full cycle of coordinate descent, starting with an update to the $x$-coordinate. This involves two sequential steps. First, keeping $y$ fixed at its initial value $y_0 = 3$, find the value of $x$ that minimizes the function; let this be the updated $x$-coordinate. Second, using this new $x$-coordinate, find the value of $y$ that minimizes the function. Let the resulting point after this cycle be $(x_{CD}, y_{CD})$.\n\n2.  **Gradient Descent (GD):** Perform one step of gradient descent with a fixed learning rate of $\\alpha = 0.1$. The update to a point $\\mathbf{x}_k$ is given by the rule $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)$. Let the resulting point be $(x_{GD}, y_{GD})$.\n\nDetermine the coordinates of these two points. Express your answer as a single row matrix containing the four values in the order $x_{CD}, y_{CD}, x_{GD}, y_{GD}$. The values should be provided as exact integers or fractions.", "solution": "We minimize the quadratic function $f(x,y)=2x^{2}+y^{2}-xy+x-4y$.\n\nFor coordinate descent, start at $(x_{0},y_{0})=(2,3)$ and first minimize in $x$ with $y$ fixed at $y_{0}=3$. The univariate function is\n$$\nf(x,3)=2x^{2}+3^{2}-x\\cdot 3+x-4\\cdot 3=2x^{2}-2x-3.\n$$\nDifferentiate with respect to $x$ and set to zero:\n$$\n\\frac{\\partial}{\\partial x}f(x,3)=4x-2=0 \\quad\\Longrightarrow\\quad x=\\frac{1}{2},\n$$\nand since $\\frac{\\partial^{2}}{\\partial x^{2}}f(x,3)=4>0$, this is the minimizer. Next, with $x=\\frac{1}{2}$ fixed, minimize in $y$:\n$$\nf\\!\\left(\\frac{1}{2},y\\right)=2\\left(\\frac{1}{2}\\right)^{2}+y^{2}-\\frac{1}{2}y+\\frac{1}{2}-4y=y^{2}-\\frac{9}{2}y+1.\n$$\nDifferentiate with respect to $y$ and set to zero:\n$$\n\\frac{\\partial}{\\partial y}f\\!\\left(\\frac{1}{2},y\\right)=2y-\\frac{9}{2}=0 \\quad\\Longrightarrow\\quad y=\\frac{9}{4},\n$$\nwith $\\frac{\\partial^{2}}{\\partial y^{2}}f=2>0$ confirming a minimizer. Thus $(x_{CD},y_{CD})=\\left(\\frac{1}{2},\\frac{9}{4}\\right)$.\n\nFor one step of gradient descent with learning rate $\\alpha=\\frac{1}{10}$, use the update $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha\\nabla f(\\mathbf{x}_{k})$. The gradient is\n$$\n\\nabla f(x,y)=\\bigl(4x-y+1,\\;2y-x-4\\bigr).\n$$\nAt $(2,3)$,\n$$\n\\nabla f(2,3)=\\bigl(4\\cdot 2-3+1,\\;2\\cdot 3-2-4\\bigr)=(6,0).\n$$\nHence\n$$\n(x_{GD},y_{GD})=(2,3)-\\frac{1}{10}(6,0)=\\left(2-\\frac{6}{10},\\,3\\right)=\\left(\\frac{7}{5},\\,3\\right).\n$$\n\nTherefore, the requested row matrix in the order $x_{CD}, y_{CD}, x_{GD}, y_{GD}$ is $\\begin{pmatrix}\\frac{1}{2} & \\frac{9}{4} & \\frac{7}{5} & 3\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{9}{4} & \\frac{7}{5} & 3\\end{pmatrix}}$$", "id": "2164428"}, {"introduction": "In modern large-scale optimization, randomized coordinate descent is a powerful tool, but how should we choose which coordinate to update at each step? This advanced problem guides you through the process of optimizing the optimizer itself [@problem_id:2164478]. Your task is to derive the optimal probability distribution for selecting coordinates, a strategy that maximizes the guaranteed progress. This exercise bridges theory and practice, showing how a deeper understanding of the function's geometry can lead to a demonstrably more efficient algorithm.", "problem": "Consider the problem of minimizing a convex, differentiable function $f: \\mathbb{R}^n \\to \\mathbb{R}$. A popular method for large-scale problems is the Randomized Coordinate Descent (RCD) algorithm. At each step, starting from a point $x$, the algorithm selects and updates a single coordinate.\n\nThe function $f$ is known to have a coordinate-wise Lipschitz continuous gradient. This means that for each coordinate $i \\in \\{1, \\dots, n\\}$, there exists a positive constant $L_i > 0$, known as the coordinate-wise Lipschitz constant, such that for any point $x \\in \\mathbb{R}^n$ and any scalar $t \\in \\mathbb{R}$, the following inequality holds:\n$$\n|\\nabla_i f(x + t e_i) - \\nabla_i f(x)| \\leq L_i |t|\n$$\nwhere $\\nabla_i f(x)$ is the partial derivative of $f$ with respect to the $i$-th coordinate evaluated at $x$, and $e_i$ is the $i$-th standard basis vector. A direct consequence of this property is the following quadratic upper bound, often called the Descent Lemma:\n$$\nf(x + t e_i) \\leq f(x) + t \\nabla_i f(x) + \\frac{L_i}{2} t^2\n$$\nIn one step of RCD, we first choose a coordinate $i$ according to a probability distribution $P = \\{p_1, p_2, \\dots, p_n\\}$, where $p_i > 0$ for all $i$ and $\\sum_{i=1}^n p_i = 1$. Then, we update $x$ to $x_{\\text{new}} = x + t_i e_i$. The step $t_i$ is chosen to precisely minimize the quadratic upper bound on the right-hand side of the Descent Lemma. This procedure guarantees a single-step function decrease, $\\Delta_i(x) = f(x) - f(x_{\\text{new}})$, which is lower-bounded by:\n$$\n\\Delta_i(x) \\ge \\frac{(\\nabla_i f(x))^2}{2 L_i}\n$$\nYou can assume both the Descent Lemma and the resulting lower bound on $\\Delta_i(x)$ without proof.\n\nThe overall performance of the RCD algorithm depends crucially on the choice of the probability distribution $P$. A robust sampling strategy should provide a good guaranteed rate of convergence, independent of the particular characteristics of the gradient at any point. The quality of a sampling distribution $P$ can be quantified by the guaranteed ratio of the expected single-step progress bound to the squared norm of the full gradient, $\\|\\nabla f(x)\\|^2 = \\sum_{j=1}^n (\\nabla_j f(x))^2$.\n\nYour task is to find the optimal probability distribution $P^* = \\{p_1^*, \\dots, p_n^*\\}$ that maximizes this worst-case performance ratio. Specifically, you must solve the following maximin optimization problem for the probability vector $P$:\n$$\n\\text{maximize}_{P} \\left( \\min_{\\nabla f(x) \\neq 0} \\frac{\\mathbb{E}_i\\left[\\frac{(\\nabla_i f(x))^2}{2L_i}\\right]}{\\|\\nabla f(x)\\|^2} \\right)\n$$\nwhere the expectation $\\mathbb{E}_i[\\cdot]$ is taken over the choice of coordinate $i$, which is sampled according to the distribution $P$. The maximization is over all valid probability distributions $P$.\n\nExpress the optimal probability $p_j^*$ for an arbitrary coordinate $j \\in \\{1, \\dots, n\\}$ in terms of the Lipschitz constants $L_1, \\dots, L_n$.", "solution": "We denote by $g=\\nabla f(x)\\in\\mathbb{R}^{n}$ the full gradient at the current point $x$. By the given single-step lower bound, when coordinate $i$ is selected and updated using the step that minimizes the quadratic upper bound, the guaranteed decrease satisfies\n$$\n\\Delta_{i}(x)\\geq \\frac{g_{i}^{2}}{2L_{i}}.\n$$\nIf the coordinate $i$ is sampled according to $P=\\{p_{1},\\dots,p_{n}\\}$, the expected guaranteed decrease is\n$$\n\\mathbb{E}_{i}\\left[\\Delta_{i}(x)\\right]\\geq \\sum_{i=1}^{n}p_{i}\\frac{g_{i}^{2}}{2L_{i}}.\n$$\nThe performance ratio for a given $P$ and nonzero $g$ is therefore\n$$\n\\rho(P;g)\\coloneqq \\frac{\\mathbb{E}_{i}\\left[\\frac{g_{i}^{2}}{2L_{i}}\\right]}{\\|g\\|^{2}}=\\frac{1}{2}\\cdot \\frac{\\sum_{i=1}^{n}p_{i}\\frac{g_{i}^{2}}{L_{i}}}{\\sum_{i=1}^{n}g_{i}^{2}}.\n$$\nIntroduce the weights\n$$\ns_{i}\\coloneqq \\frac{g_{i}^{2}}{\\|g\\|^{2}},\\quad i=1,\\dots,n,\n$$\nso that $s_{i}\\geq 0$ and $\\sum_{i=1}^{n}s_{i}=1$. Then\n$$\n\\rho(P;g)=\\frac{1}{2}\\sum_{i=1}^{n}s_{i}\\frac{p_{i}}{L_{i}}.\n$$\nFor fixed $P$, the worst-case ratio over all nonzero $g$ is the minimum of a linear function of $s$ over the probability simplex, hence it is attained at an extreme point of the simplex. Therefore\n$$\n\\min_{g\\neq 0}\\rho(P;g)=\\frac{1}{2}\\min_{1\\leq i\\leq n}\\frac{p_{i}}{L_{i}}.\n$$\nThe maximin problem thus reduces to choosing $P$ to maximize $\\min_{i}p_{i}/L_{i}$ subject to $p_{i}>0$ and $\\sum_{i=1}^{n}p_{i}=1$. Let $\\tau\\coloneqq \\min_{i}p_{i}/L_{i}$. Then the constraints imply\n$$\np_{i}\\geq \\tau L_{i}\\quad\\text{for all }i,\\qquad \\sum_{i=1}^{n}p_{i}=1\\ \\Longrightarrow\\ 1=\\sum_{i=1}^{n}p_{i}\\geq \\tau\\sum_{i=1}^{n}L_{i},\n$$\nhence any feasible $\\tau$ must satisfy\n$$\n\\tau\\leq \\frac{1}{\\sum_{k=1}^{n}L_{k}}.\n$$\nThis upper bound on $\\tau$ is achievable by setting\n$$\np_{i}=\\tau L_{i}\\quad\\text{with}\\quad \\tau=\\frac{1}{\\sum_{k=1}^{n}L_{k}},\n$$\nwhich yields $p_{i}=\\frac{L_{i}}{\\sum_{k=1}^{n}L_{k}}$ and equalizes all ratios $p_{i}/L_{i}$. Therefore this $P$ maximizes the worst-case ratio. Consequently, for any coordinate $j$,\n$$\np_{j}^{*}=\\frac{L_{j}}{\\sum_{k=1}^{n}L_{k}}.\n$$", "answer": "$$\\boxed{\\frac{L_{j}}{\\sum_{k=1}^{n}L_{k}}}$$", "id": "2164478"}]}