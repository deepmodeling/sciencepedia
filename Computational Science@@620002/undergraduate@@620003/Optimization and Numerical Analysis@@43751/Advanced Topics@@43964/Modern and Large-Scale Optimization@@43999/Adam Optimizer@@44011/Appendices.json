{"hands_on_practices": [{"introduction": "The Adam optimizer's power lies in its adaptive nature, which begins with tracking the \"memory\" of past gradients. This first practice isolates the fundamental calculation of the first moment estimate ($m_t$, the moving average of the gradient) and the second moment estimate ($v_t$, the moving average of the squared gradient) for a single step. Mastering this initial computation is the key to understanding how Adam builds its adaptive learning rate for each parameter. [@problem_id:2152288]", "problem": "In the field of machine learning, optimizers are algorithms used to adjust the parameters of a model to minimize a loss function. The Adam (Adaptive Moment Estimation) optimizer is a popular choice that computes adaptive learning rates for each parameter from an estimate of the first and second moments of the gradients.\n\nThe update rules for the first moment estimate (a moving average of the gradient, $m$) and the second moment estimate (a moving average of the squared gradient, $v$) at a time step $t$ are given by:\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\nwhere $g_t$ is the gradient of the loss function with respect to the model parameters at step $t$, and the square operation $g_t^2$ is performed element-wise. The hyperparameters $\\beta_1$ and $\\beta_2$ are decay rates for the moving averages.\n\nConsider a simple two-parameter model. At the beginning of the optimization process ($t=1$), the initial moment estimates are initialized to zero vectors, i.e., $m_0 = [0, 0]^T$ and $v_0 = [0, 0]^T$. The hyperparameters are set to their commonly used values: $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. For the first training step, the computed gradient of the loss function is $g_1 = [2.0, -4.0]^T$.\n\nCalculate the updated first moment vector $m_1 = [m_{1,1}, m_{1,2}]^T$ and the updated second moment vector $v_1 = [v_{1,1}, v_{1,2}]^T$. Your final answer should be a row matrix containing the four numerical components in the specific order $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$.", "solution": "We use the Adam moment update equations at step $t=1$:\n$$m_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2},$$\nwith $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $\\beta_{1}=0.9$, $\\beta_{2}=0.999$, and $g_{1}=\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}$. The square $g_{1}^{2}$ is taken element-wise.\n\nFirst moment:\nSince $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, the term $\\beta_{1}m_{0}$ is $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$. Thus,\n$$m_{1}=(1-\\beta_{1})g_{1}=0.1\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}=\\begin{bmatrix}0.2 \\\\ -0.4\\end{bmatrix}.$$\nTherefore, $m_{1,1}=0.2$ and $m_{1,2}=-0.4$.\n\nSecond moment:\nCompute the element-wise square $g_{1}^{2}$:\n$$g_{1}^{2}=\\begin{bmatrix}(2.0)^{2} \\\\ (-4.0)^{2}\\end{bmatrix}=\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}.$$\nSince $v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, the term $\\beta_{2}v_{0}$ is $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$. Thus,\n$$v_{1}=(1-\\beta_{2})g_{1}^{2}=0.001\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}=\\begin{bmatrix}0.004 \\\\ 0.016\\end{bmatrix}.$$\nTherefore, $v_{1,1}=0.004$ and $v_{1,2}=0.016$.\n\nThe requested row matrix in the order $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$ is $\\begin{pmatrix}0.2 & -0.4 & 0.004 & 0.016\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}0.2 & -0.4 & 0.004 & 0.016\\end{pmatrix}}$$", "id": "2152288"}, {"introduction": "Building on our understanding of moment estimates, we will now perform a complete, end-to-end update step of the Adam optimizer. This exercise guides you through the full process of minimizing a simple function: from calculating the gradient, updating the moments, applying the crucial bias-correction, to finally computing the new parameter value. This practice connects all the individual components of the Adam algorithm into a single, concrete action. [@problem_id:2152250]", "problem": "In the field of numerical optimization, the Adam algorithm is a widely used method for finding the minimum of a function. Consider the one-dimensional cost function $f(x) = 5x^2$. We wish to find the value of $x$ that minimizes this function, starting from an initial guess of $x_0 = 2$.\n\nCalculate the value of the parameter after one update step, denoted as $x_1$, using the Adam algorithm. The algorithm is configured with the following hyperparameters:\n- Learning rate, $\\alpha = 0.1$\n- Exponential decay rate for the first moment estimate, $\\beta_1 = 0.9$\n- Exponential decay rate for the second moment estimate, $\\beta_2 = 0.999$\n- A small constant for numerical stability, $\\epsilon = 10^{-8}$\n\nThe initial first and second moment estimates, $m_0$ and $v_0$, are both initialized to zero.\n\nRound your final answer to five significant figures.", "solution": "We minimize $f(x)=5x^{2}$ using Adam starting from $x_{0}=2$. The gradient is $\\nabla f(x)=10x$. At the first step ($t=1$), the gradient evaluated at $x_{0}$ is\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam moment updates are\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\nWith $m_{0}=0$ and $v_{0}=0$, $\\beta_{1}=0.9$, and $\\beta_{2}=0.999$,\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\nBias-corrected estimates are\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nThe Adam update is\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\nEvaluating the fraction,\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\nso\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\nRounded to five significant figures, this is $1.9000$.", "answer": "$$\\boxed{1.9000}$$", "id": "2152250"}, {"introduction": "Having practiced single-step calculations, we now explore the long-term behavior of Adam's momentum component to build deeper intuition about its stability. This thought experiment investigates how the first moment estimate responds to a hypothetical, persistently oscillating gradient. By analyzing this scenario, you will gain insight into the powerful smoothing effect of the exponential moving average, a key feature that makes Adam robust even when faced with noisy or erratic gradient signals. [@problem_id:2152247]", "problem": "In the field of machine learning, the Adam optimizer is a popular algorithm for training deep neural networks. A key component of Adam is the first moment estimate, often called the momentum, which is an exponential moving average of the gradients. For a single model parameter, the momentum $m_t$ at timestep $t$ is updated using the rule:\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$$\nHere, $g_t$ is the gradient of the loss function with respect to the parameter at timestep $t$, and $\\beta_1$ is the exponential decay rate for the first moment estimates, a constant satisfying $0 < \\beta_1 < 1$. The momentum is typically initialized to zero, i.e., $m_0 = 0$.\n\nConsider a specific, pathological scenario designed to test the behavior of the momentum update. Suppose the gradient sequence does not converge but instead oscillates with a constant magnitude. For all timesteps $t \\ge 1$, the gradient is given by the sequence $g_t = g_0 (-1)^t$, where $g_0$ is a positive constant.\n\nDetermine the limiting value of the first moment estimate $m_t$ as $t$ approaches infinity through even integers. In other words, calculate the value of $\\lim_{n \\to \\infty} m_{2n}$. Express your answer as a symbolic expression in terms of $g_0$ and $\\beta_1$.", "solution": "We define $a \\equiv \\beta_{1}$ and $b \\equiv 1 - \\beta_{1}$ with $0<a<1$, and note the alternating input $g_{2n} = g_{0}$ and $g_{2n+1} = -g_{0}$.\n\nThe update is $m_{t} = a m_{t-1} + b g_{t}$. For even indices,\n$$\nm_{2n+1} = a m_{2n} + b(-g_{0}) = a m_{2n} - b g_{0},\n$$\n$$\nm_{2n+2} = a m_{2n+1} + b g_{0} = a\\left(a m_{2n} - b g_{0}\\right) + b g_{0} = a^{2} m_{2n} + b g_{0}(1 - a).\n$$\nThus the even subsequence satisfies the affine contraction\n$$\nm_{2n+2} = a^{2} m_{2n} + b g_{0}(1 - a).\n$$\nSince $0<a<1$, the map $x \\mapsto a^{2} x + b g_{0}(1 - a)$ is a contraction and $m_{2n}$ converges to its unique fixed point $m^{\\star}$ solving\n$$\nm^{\\star} = a^{2} m^{\\star} + b g_{0}(1 - a).\n$$\nRearranging,\n$$\n(1 - a^{2}) m^{\\star} = b g_{0}(1 - a).\n$$\nUsing $1 - a^{2} = (1 - a)(1 + a)$,\n$$\nm^{\\star} = \\frac{b}{1 + a} g_{0} = \\frac{1 - \\beta_{1}}{1 + \\beta_{1}} g_{0}.\n$$\nTherefore,\n$$\n\\lim_{n \\to \\infty} m_{2n} = \\frac{1 - \\beta_{1}}{1 + \\beta_{1}} g_{0}.\n$$", "answer": "$$\\boxed{\\frac{1 - \\beta_{1}}{1 + \\beta_{1}}\\,g_{0}}$$", "id": "2152247"}]}