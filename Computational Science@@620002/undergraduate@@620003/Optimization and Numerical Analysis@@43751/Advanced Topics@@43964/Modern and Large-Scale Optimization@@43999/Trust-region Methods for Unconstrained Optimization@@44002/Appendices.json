{"hands_on_practices": [{"introduction": "The foundation of any trust-region method is the creation of a simpler, local model of the objective function. This model, typically a quadratic, serves as a proxy that we can easily optimize. This exercise [@problem_id:2224506] provides direct practice in constructing this quadratic model using a second-order Taylor expansion, a fundamental skill for understanding and implementing these algorithms.", "problem": "In the field of unconstrained optimization, trust-region methods are a class of iterative algorithms used to solve nonlinear programming problems. At each iteration $k$, these methods construct a simpler model function $m_k(p)$ that approximates the true objective function $f(x)$ in a neighborhood (the \"trust region\") around the current point $x_k$. The step $p$ is then found by minimizing this model within the trust region. A common choice for the model is a quadratic function derived from the second-order Taylor expansion of $f(x)$ at $x_k$.\n\nConsider the objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(x_1, x_2) = \\sin(x_1) + x_2^2$$\nDetermine the quadratic model $m_k(p)$ for this function at the point $x_k = (0, 1)$, where $p = (p_1, p_2)$ is the step from $x_k$. Express your answer as a polynomial in terms of $p_1$ and $p_2$.", "solution": "The quadratic trust-region model based on the second-order Taylor expansion of $f$ at $x_{k}$ is\n$$m_{k}(p) = f(x_{k}) + \\nabla f(x_{k})^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^{2} f(x_{k}) p,$$\nwhere $p = (p_{1}, p_{2})$.\n\nGiven $f(x_{1}, x_{2}) = \\sin(x_{1}) + x_{2}^{2}$, compute the gradient and Hessian:\n$$\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} \\cos(x_{1}) \\\\ 2 x_{2} \\end{pmatrix}, \\quad \\nabla^{2} f(x_{1}, x_{2}) = \\begin{pmatrix} -\\sin(x_{1})  0 \\\\ 0  2 \\end{pmatrix}.$$\n\nEvaluate these at $x_{k} = (0, 1)$:\n$$f(x_{k}) = \\sin(0) + 1^{2} = 1,$$\n$$\\nabla f(x_{k}) = \\begin{pmatrix} \\cos(0) \\\\ 2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad \\nabla^{2} f(x_{k}) = \\begin{pmatrix} -\\sin(0)  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix}.$$\n\nWith $p = (p_{1}, p_{2})$, compute the linear and quadratic terms:\n$$\\nabla f(x_{k})^{\\top} p = 1 \\cdot p_{1} + 2 \\cdot p_{2} = p_{1} + 2 p_{2},$$\n$$p^{\\top} \\nabla^{2} f(x_{k}) p = \\begin{pmatrix} p_{1}  p_{2} \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} p_{1} \\\\ p_{2} \\end{pmatrix} = 2 p_{2}^{2}.$$\n\nTherefore,\n$$m_{k}(p) = 1 + \\left(p_{1} + 2 p_{2}\\right) + \\frac{1}{2} \\left(2 p_{2}^{2}\\right) = 1 + p_{1} + 2 p_{2} + p_{2}^{2}.$$", "answer": "$$\\boxed{1 + p_{1} + 2 p_{2} + p_{2}^{2}}$$", "id": "2224506"}, {"introduction": "Once a quadratic model is built, the next task is to find the optimal step $p$ by minimizing the model. However, this minimization is confined to a 'trust region' where the model is deemed reliable. This exercise [@problem_id:2224504] delves into solving this core trust-region subproblem, highlighting the crucial scenario where the step is limited by the trust-region boundary, not by the model's unconstrained minimizer.", "problem": "In the context of unconstrained optimization, trust-region methods iteratively approximate a complex function with a simpler model function $m(p)$ around the current point. The next step $p$ is then determined by solving the trust-region subproblem, which involves minimizing this model within a \"trust region\" of radius $\\Delta  0$, where the model is believed to be a reliable approximation of the original function. The subproblem is formally stated as:\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad |p| \\le \\Delta\n$$\nConsider a one-dimensional optimization scenario where the model function for a step $p \\in \\mathbb{R}$ is a quadratic given by:\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\nwith a gradient term $g = 2$, a Hessian term $H = 6$, and an arbitrary constant $c$. The step is constrained by a trust-region radius of $\\Delta = 0.1$.\n\nDetermine the optimal step $p$ that solves this one-dimensional trust-region subproblem. Provide your answer as an exact decimal number.", "solution": "We must minimize the quadratic model $m(p) = g p + \\frac{1}{2} H p^{2} + c$ subject to the trust-region constraint $|p| \\le \\Delta$, with given values $g=2$, $H=6$, and $\\Delta=0.1$. The constant $c$ does not affect the minimizer and can be ignored.\n\nConsider the Lagrangian for the trust-region subproblem:\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\nwith $\\lambda \\ge 0$. The Karush-Kuhn-Tucker conditions are:\n1. Stationarity: \n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. Primal feasibility: $|p| \\le \\Delta$.\n3. Dual feasibility: $\\lambda \\ge 0$.\n4. Complementary slackness: $\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$.\n\nCase 1 (interior solution): If $|p|  \\Delta$, then $\\lambda = 0$ and stationarity gives\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\nFor $g=2$ and $H=6$, this gives $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$. Check feasibility: $|p_{u}| = \\frac{1}{3}  0.1 = \\Delta$, so the interior solution is infeasible.\n\nCase 2 (boundary solution): Then $p^{2} = \\Delta^{2}$, so $p = \\pm \\Delta$. Stationarity becomes\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\nSince $H + 2 \\lambda \\ge 0$, the sign of $p$ must match the sign of $-g$. With $g = 2  0$, we must take $p = -\\Delta = -0.1$. To verify dual feasibility, solve for $\\lambda$:\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\nSubstituting $g=2$, $\\Delta=0.1$, and $H=6$ gives\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\nwhich satisfies dual feasibility and complementary slackness.\n\nTherefore, the optimal trust-region step is the boundary step in the negative gradient direction:\n$$\np^{\\star} = -\\Delta = -0.1.\n$$", "answer": "$$\\boxed{-0.1}$$", "id": "2224504"}, {"introduction": "A powerful feature of trust-region methods is their robustness, even when the quadratic model is not a well-behaved convex function. In cases where the model is concave, its unconstrained minimum is not well-defined, but a sensible step can still be found. This exercise [@problem_id:2224539] explores such a non-convex scenario, demonstrating how the trust-region constraint ensures a well-defined solution by forcing the step to the boundary of the trusted area.", "problem": "In the context of unconstrained optimization, a trust-region method iteratively builds a quadratic model $m_k(p)$ of the objective function $f(x)$ around the current iterate $x_k$. The next step $p_k$ is found by minimizing this model within a \"trust region\" of radius $\\Delta_k$. The model is defined in one dimension as:\n$$m_k(p) = f(x_k) + f'(x_k)p + \\frac{1}{2}f''(x_k)p^2$$\nwhere $p$ is the step from $x_k$. The step $p_k$ is the solution to the subproblem:\n$$\\min_{p} m_k(p) \\quad \\text{subject to} \\quad |p| \\le \\Delta_k$$\n\nConsider the one-dimensional objective function $f(x) = -\\frac{1}{3}x^3 + 2x^2 - x$. Suppose we are at the iterate $x_k = 5$ and the trust-region radius is $\\Delta_k = 2$.\n\nDetermine the value of the step $p_k$ that solves the trust-region subproblem for this iteration.", "solution": "We form the one-dimensional quadratic model at $x_{k}=5$:\n$$m_{k}(p)=f(x_{k})+f'(x_{k})p+\\frac{1}{2}f''(x_{k})p^{2}.$$\nGiven $f(x)=-\\frac{1}{3}x^{3}+2x^{2}-x$, its derivatives are\n$$f'(x)=-x^{2}+4x-1,\\qquad f''(x)=-2x+4.$$\nEvaluating at $x_{k}=5$ gives\n$$g_{k}=f'(5)=-25+20-1=-6,\\qquad B_{k}=f''(5)=-10+4=-6.$$\nHence the model simplifies to\n$$m_{k}(p)=f(x_{k})-6p+\\frac{1}{2}(-6)p^{2}=f(x_{k})-6p-3p^{2}.$$\nThe trust-region subproblem is\n$$\\min_{p} \\, m_{k}(p)\\quad \\text{subject to}\\quad |p|\\le \\Delta_{k}=2.$$\nSince $B_{k}0$, the quadratic $m_{k}(p)$ is concave in $p$. Its stationary point solves $m_{k}'(p)=g_{k}+B_{k}p=0$, giving\n$$p^{\\ast}=-\\frac{g_{k}}{B_{k}}=-\\frac{-6}{-6}=-1,$$\nwhich is a maximizer due to concavity. Therefore, the minimum over the closed interval $\\{|p|\\le 2\\}$ occurs at one of the boundary points $p=\\pm 2$. Evaluate the model at the endpoints:\n$$m_{k}(2)=f(x_{k})-12-12=f(x_{k})-24,\\qquad m_{k}(-2)=f(x_{k})+12-12=f(x_{k}).$$\nThus $m_{k}(2)m_{k}(-2)$, so the minimizing step is\n$$p_{k}=2.$$", "answer": "$$\\boxed{2}$$", "id": "2224539"}]}