{"hands_on_practices": [{"introduction": "To begin, we will ground the abstract concept of a finite difference Jacobian in a familiar context: the transformation from polar to Cartesian coordinates. This exercise provides a foundational a-hands-on experience in applying the forward difference formula, allowing you to practice the fundamental mechanics of assembling a Jacobian approximation one partial derivative at a time. Mastering this process is the first step toward using Jacobian matrices in more complex numerical problems. [@problem_id:2171159]", "problem": "A two-dimensional vector function $F: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ models the transformation from a polar coordinate system $(r, \\theta)$ to a Cartesian coordinate system $(x, y)$. The function is defined as:\n$$\nF(r, \\theta) = \\begin{pmatrix} x(r, \\theta) \\\\ y(r, \\theta) \\end{pmatrix} = \\begin{pmatrix} r\\cos\\theta \\\\ r\\sin\\theta \\end{pmatrix}\n$$\nYour task is to find the approximation of the Jacobian matrix of $F$ at a general point $(r, \\theta)$ using the forward finite difference method for each partial derivative. Let $h$ represent the small, positive step size used in the finite difference calculation. The angle $\\theta$ is measured in radians.\n\nProvide your answer as a 2x2 matrix, with its entries expressed in terms of $r$, $\\theta$, and $h$.", "solution": "We are given $F(r,\\theta)=(x(r,\\theta),y(r,\\theta))$ with $x(r,\\theta)=r\\cos\\theta$ and $y(r,\\theta)=r\\sin\\theta$. The Jacobian matrix is\n$$\nJ_{F}(r,\\theta)=\\begin{pmatrix}\n\\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial \\theta} \\\\\n\\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial \\theta}\n\\end{pmatrix}.\n$$\nUsing the forward finite difference approximation for a scalar function $g(r,\\theta)$,\n$$\n\\frac{\\partial g}{\\partial r}(r,\\theta)\\approx \\frac{g(r+h,\\theta)-g(r,\\theta)}{h},\\qquad\n\\frac{\\partial g}{\\partial \\theta}(r,\\theta)\\approx \\frac{g(r,\\theta+h)-g(r,\\theta)}{h},\n$$\nwe compute each entry.\n\nFor $x(r,\\theta)=r\\cos\\theta$:\n$$\n\\frac{\\partial x}{\\partial r}(r,\\theta)\\approx \\frac{(r+h)\\cos\\theta-r\\cos\\theta}{h}=\\cos\\theta,\n$$\n$$\n\\frac{\\partial x}{\\partial \\theta}(r,\\theta)\\approx \\frac{r\\cos(\\theta+h)-r\\cos\\theta}{h}\n=\\frac{r\\left(\\cos(\\theta+h)-\\cos\\theta\\right)}{h}.\n$$\n\nFor $y(r,\\theta)=r\\sin\\theta$:\n$$\n\\frac{\\partial y}{\\partial r}(r,\\theta)\\approx \\frac{(r+h)\\sin\\theta-r\\sin\\theta}{h}=\\sin\\theta,\n$$\n$$\n\\frac{\\partial y}{\\partial \\theta}(r,\\theta)\\approx \\frac{r\\sin(\\theta+h)-r\\sin\\theta}{h}\n=\\frac{r\\left(\\sin(\\theta+h)-\\sin\\theta\\right)}{h}.\n$$\n\nTherefore, the forward finite difference approximation of the Jacobian matrix at $(r,\\theta)$ is\n$$\n\\begin{pmatrix}\n\\cos\\theta & \\frac{r\\left(\\cos(\\theta+h)-\\cos\\theta\\right)}{h} \\\\\n\\sin\\theta & \\frac{r\\left(\\sin(\\theta+h)-\\sin\\theta\\right)}{h}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\n\\cos\\theta & \\frac{r\\left(\\cos(\\theta+h)-\\cos\\theta\\right)}{h} \\\\\n\\sin\\theta & \\frac{r\\left(\\sin(\\theta+h)-\\sin\\theta\\right)}{h}\n\\end{pmatrix}}$$", "id": "2171159"}, {"introduction": "In practical applications, functions are not always perfectly smooth. This practice explores how different finite difference schemes, specifically the forward and backward difference methods, behave when a function is not differentiable at the point of evaluation. By analyzing a vector function that includes a non-differentiable absolute value component, you will gain crucial insight into the potential asymmetries and limitations of one-sided derivative approximations. [@problem_id:2171184]", "problem": "In numerical optimization, the Jacobian matrix of a vector-valued function is often approximated using finite differences, especially when analytic derivatives are difficult to compute. Consider the vector-valued function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined as:\n$$\nF(\\mathbf{x}) = F(x, y) = \\begin{pmatrix} f_1(x, y) \\\\ f_2(x, y) \\end{pmatrix} = \\begin{pmatrix} \\sqrt{x^2 + 1} \\\\ |y| \\end{pmatrix}\n$$\nWe want to approximate the Jacobian of this function at the point $\\mathbf{x}_0 = (0, 0)$.\n\nThe $(i,j)$-th entry of the forward difference Jacobian approximation, $J_F$, is given by:\n$$\n[J_F]_{ij} = \\frac{f_i(\\mathbf{x}_0 + h \\mathbf{e}_j) - f_i(\\mathbf{x}_0)}{h}\n$$\nThe $(i,j)$-th entry of the backward difference Jacobian approximation, $J_B$, is given by:\n$$\n[J_B]_{ij} = \\frac{f_i(\\mathbf{x}_0) - f_i(\\mathbf{x}_0 - h \\mathbf{e}_j)}{h}\n$$\nHere, $h$ is a small positive scalar, and $\\mathbf{e}_1 = (1, 0)$ and $\\mathbf{e}_2 = (0, 1)$ are the standard basis vectors in $\\mathbb{R}^2$.\n\nWhich of the following pairs correctly represents the forward difference Jacobian ($J_F$) and the backward difference Jacobian ($J_B$) for the function $F$ at $\\mathbf{x}_0 = (0,0)$?\n\nA. $J_F = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad J_B = \\begin{pmatrix} 0 & 0 \\\\ 0 & -1 \\end{pmatrix}$\n\nB. $J_F = \\begin{pmatrix} \\frac{\\sqrt{h^2+1}-1}{h} & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad J_B = \\begin{pmatrix} -\\frac{\\sqrt{h^2+1}-1}{h} & 0 \\\\ 0 & -1 \\end{pmatrix}$\n\nC. $J_F = \\begin{pmatrix} \\frac{\\sqrt{h^2+1}-1}{h} & 0 \\\\ 0 & -1 \\end{pmatrix}, \\quad J_B = \\begin{pmatrix} -\\frac{\\sqrt{h^2+1}-1}{h} & 0 \\\\ 0 & 1 \\end{pmatrix}$\n\nD. $J_F = \\begin{pmatrix} 0 & \\frac{\\sqrt{h^2+1}-1}{h} \\\\ 1 & 0 \\end{pmatrix}, \\quad J_B = \\begin{pmatrix} 0 & -\\frac{\\sqrt{h^2+1}-1}{h} \\\\ -1 & 0 \\end{pmatrix}$\n\nE. $J_F = \\begin{pmatrix} \\frac{\\sqrt{h^2+1}}{h} & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad J_B = \\begin{pmatrix} -\\frac{\\sqrt{h^2+1}}{h} & 0 \\\\ 0 & -1 \\end{pmatrix}$", "solution": "We use the forward and backward difference definitions componentwise at $\\mathbf{x}_{0}=(0,0)$ with step $h>0$ and standard basis vectors $\\mathbf{e}_{1}=(1,0)$ and $\\mathbf{e}_{2}=(0,1)$:\n$$\n[J_{F}]_{ij}=\\frac{f_{i}(\\mathbf{x}_{0}+h\\mathbf{e}_{j})-f_{i}(\\mathbf{x}_{0})}{h},\\qquad\n[J_{B}]_{ij}=\\frac{f_{i}(\\mathbf{x}_{0})-f_{i}(\\mathbf{x}_{0}-h\\mathbf{e}_{j})}{h}.\n$$\nThe function is $F(x,y)=\\begin{pmatrix}\\sqrt{x^{2}+1}\\\\ |y|\\end{pmatrix}$, so $f_{1}(x,y)=\\sqrt{x^{2}+1}$ depends only on $x$, and $f_{2}(x,y)=|y|$ depends only on $y$. Evaluate needed values:\n- At the base point: $f_{1}(0,0)=\\sqrt{0+1}=1$, $f_{2}(0,0)=|0|=0$.\n- Along $+\\mathbf{e}_{1}$: $f_{1}(h,0)=\\sqrt{h^{2}+1}$, $f_{2}(h,0)=|0|=0$.\n- Along $+\\mathbf{e}_{2}$: $f_{1}(0,h)=\\sqrt{0+1}=1$, $f_{2}(0,h)=|h|=h$.\n- Along $-\\mathbf{e}_{1}$: $f_{1}(-h,0)=\\sqrt{(-h)^{2}+1}=\\sqrt{h^{2}+1}$, $f_{2}(-h,0)=|0|=0$.\n- Along $-\\mathbf{e}_{2}$: $f_{1}(0,-h)=\\sqrt{0+1}=1$, $f_{2}(0,-h)=|-h|=h$.\n\nCompute forward difference entries:\n$$\n[J_{F}]_{11}=\\frac{\\sqrt{h^{2}+1}-1}{h},\\quad [J_{F}]_{12}=\\frac{1-1}{h}=0,\n$$\n$$\n[J_{F}]_{21}=\\frac{0-0}{h}=0,\\quad [J_{F}]_{22}=\\frac{h-0}{h}=1.\n$$\nThus\n$$\nJ_{F}=\\begin{pmatrix}\n\\frac{\\sqrt{h^{2}+1}-1}{h} & 0\\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\nCompute backward difference entries:\n$$\n[J_{B}]_{11}=\\frac{1-\\sqrt{h^{2}+1}}{h}=-\\frac{\\sqrt{h^{2}+1}-1}{h},\\quad [J_{B}]_{12}=\\frac{1-1}{h}=0,\n$$\n$$\n[J_{B}]_{21}=\\frac{0-0}{h}=0,\\quad [J_{B}]_{22}=\\frac{0-h}{h}=-1.\n$$\nThus\n$$\nJ_{B}=\\begin{pmatrix}\n-\\frac{\\sqrt{h^{2}+1}-1}{h} & 0\\\\\n0 & -1\n\\end{pmatrix}.\n$$\n\nComparing with the options, this corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "2171184"}, {"introduction": "A powerful numerical method is characterized not just by its ability to produce an answer, but also by the predictability of its accuracy. This exercise shifts our focus from computation to analysis, examining the convergence properties of the highly accurate central difference method. Through a theoretical analysis rooted in Taylor expansions, you will determine how the approximation error scales as the step size $h$ decreases, thereby quantifying the method's second-order accuracyâ€”a key reason for its widespread use. [@problem_id:2171195]", "problem": "In a simplified model for a novel two-degree-of-freedom actuator, the output state vector is related to the input control vector $(q_1, q_2)$ by the function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$, defined as:\n$$ F(q_1, q_2) = \\begin{pmatrix} F_1(q_1, q_2) \\\\ F_2(q_1, q_2) \\end{pmatrix} = \\begin{pmatrix} \\exp(q_1 + q_2) \\\\ \\cos(q_1 - q_2) \\end{pmatrix} $$\nThe system's control relies on its Jacobian matrix, $J(q_1, q_2)$, which has elements $J_{ij} = \\frac{\\partial F_i}{\\partial q_j}$.\n\nIn practical applications, the Jacobian is approximated numerically. The central difference approximation of the Jacobian, denoted $J_h(q_1, q_2)$, is constructed by approximating each partial derivative using the formula:\n$$ \\frac{\\partial F_i}{\\partial q_j} \\approx \\frac{F_i(\\mathbf{q} + h\\mathbf{e}_j) - F_i(\\mathbf{q} - h\\mathbf{e}_j)}{2h} $$\nwhere $\\mathbf{q} = (q_1, q_2)$ is the point of evaluation, $h$ is a small step size, and $\\mathbf{e}_j$ is the $j$-th standard basis vector (e.g., $\\mathbf{e}_1 = (1, 0)$).\n\nThe accuracy of this approximation is measured by the error matrix $E_h = J(q_1, q_2) - J_h(q_1, q_2)$. The magnitude of this error is quantified using the Frobenius norm, defined for a general $m \\times n$ matrix $A$ as $\\|A\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |A_{ij}|^2}$. Let $\\mathcal{E}(h) = \\|E_h\\|_F$ denote the error for a given step size $h$.\n\nTo understand the convergence behavior of the method, we analyze how the error changes as the step size is reduced. Calculate the theoretical ratio of the approximation errors as the step size is halved. Specifically, for any state $(q_1, q_2)$ where the Jacobian is well-defined and non-zero, compute the value of the following limit:\n$$ R = \\lim_{h \\to 0} \\frac{\\mathcal{E}(h)}{\\mathcal{E}(h/2)} $$\nExpress your final answer as an exact integer or fraction.", "solution": "We are given $F(q_{1},q_{2}) = \\begin{pmatrix} \\exp(q_{1}+q_{2}) \\\\ \\cos(q_{1}-q_{2}) \\end{pmatrix}$ and its Jacobian $J(q_{1},q_{2})$ with entries $J_{ij} = \\partial_{q_{j}} F_{i}$. The central difference approximation of $\\partial_{q_{j}} F_{i}$ at $\\mathbf{q}=(q_{1},q_{2})$ with stepsize $h$ is\n$$\n\\frac{F_{i}(\\mathbf{q}+h\\mathbf{e}_{j}) - F_{i}(\\mathbf{q}-h\\mathbf{e}_{j})}{2h}.\n$$\nFix $i$ and $j$ and consider the one-variable function $\\varphi(t) = F_{i}(\\mathbf{q} + t \\mathbf{e}_{j})$. A Taylor expansion about $t=0$ gives\n$$\nF_{i}(\\mathbf{q} + h \\mathbf{e}_{j}) = F_{i}(\\mathbf{q}) + h\\,\\partial_{q_{j}} F_{i}(\\mathbf{q}) + \\frac{h^{2}}{2}\\,\\partial_{q_{j}q_{j}} F_{i}(\\mathbf{q}) + \\frac{h^{3}}{6}\\,\\partial_{q_{j}q_{j}q_{j}} F_{i}(\\mathbf{q}) + O(h^{4}),\n$$\n$$\nF_{i}(\\mathbf{q} - h \\mathbf{e}_{j}) = F_{i}(\\mathbf{q}) - h\\,\\partial_{q_{j}} F_{i}(\\mathbf{q}) + \\frac{h^{2}}{2}\\,\\partial_{q_{j}q_{j}} F_{i}(\\mathbf{q}) - \\frac{h^{3}}{6}\\,\\partial_{q_{j}q_{j}q_{j}} F_{i}(\\mathbf{q}) + O(h^{4}).\n$$\nSubtracting and dividing by $2h$ yields the central difference formula with truncation error\n$$\n\\frac{F_{i}(\\mathbf{q}+h\\mathbf{e}_{j}) - F_{i}(\\mathbf{q}-h\\mathbf{e}_{j})}{2h} = \\partial_{q_{j}} F_{i}(\\mathbf{q}) + \\frac{h^{2}}{6}\\,\\partial_{q_{j}q_{j}q_{j}} F_{i}(\\mathbf{q}) + O(h^{4}).\n$$\nTherefore, each Jacobian entry error is\n$$\nE_{ij}(h) = J_{ij} - (J_{h})_{ij} = -\\frac{h^{2}}{6}\\,\\partial_{q_{j}q_{j}q_{j}} F_{i}(\\mathbf{q}) + O(h^{4}).\n$$\nIn matrix form, there exists a constant matrix $A$ with entries $A_{ij} = -\\tfrac{1}{6}\\,\\partial_{q_{j}q_{j}q_{j}} F_{i}(\\mathbf{q})$ such that\n$$\nE_{h} = h^{2} A + O(h^{4}).\n$$\nConsequently, by homogeneity and continuity of the Frobenius norm,\n$$\n\\mathcal{E}(h) = \\|E_{h}\\|_{F} = \\|h^{2}A + O(h^{4})\\|_{F} = h^{2}\\|A\\|_{F} + O(h^{4}).\n$$\nFor the given $F$, the third directional partial derivatives along coordinate directions are\n$$\n\\partial_{q_{1}q_{1}q_{1}} F_{1} = \\exp(q_{1}+q_{2}), \\quad \\partial_{q_{2}q_{2}q_{2}} F_{1} = \\exp(q_{1}+q_{2}),\n$$\n$$\n\\partial_{q_{1}q_{1}q_{1}} F_{2} = \\sin(q_{1}-q_{2}), \\quad \\partial_{q_{2}q_{2}q_{2}} F_{2} = -\\sin(q_{1}-q_{2}).\n$$\nThus $A$ is nonzero for all $(q_{1},q_{2})$ because $\\exp(q_{1}+q_{2}) \\neq 0$, ensuring $\\|A\\|_{F} > 0$ and establishing the leading-order behavior $\\mathcal{E}(h) = C h^{2} + O(h^{4})$ with $C = \\|A\\|_{F} > 0$.\n\nNow compute the ratio as $h \\to 0$:\n$$\n\\frac{\\mathcal{E}(h)}{\\mathcal{E}(h/2)} = \\frac{h^{2}\\|A\\|_{F} + O(h^{4})}{(h/2)^{2}\\|A\\|_{F} + O(h^{4})} = \\frac{h^{2}\\|A\\|_{F}\\left(1 + O(h^{2})\\right)}{\\tfrac{h^{2}}{4}\\|A\\|_{F}\\left(1 + O(h^{2})\\right)} \\to 4 \\quad \\text{as } h \\to 0.\n$$\nTherefore, the theoretical ratio is $R=4$, reflecting the second-order accuracy of the central difference Jacobian approximation in $h$.", "answer": "$$\\boxed{4}$$", "id": "2171195"}]}