{"hands_on_practices": [{"introduction": "One of the most intuitive constrained optimization problems is finding the shortest distance from a point to a plane, which is equivalent to projecting the point onto the plane. This first exercise applies the quadratic penalty method to this classic geometric problem, providing a concrete foundation for the technique. By working through this problem, you will master the core mechanics of constructing a penalized objective function and finding its minimizer as a function of the penalty parameter $\\mu$.", "problem": "Consider a constrained optimization problem where the objective is to minimize the function $f(x_1, x_2) = (x_1 - a)^2 + (x_2 - b)^2$ subject to the linear constraint $h(x_1, x_2) = x_1 + x_2 - c = 0$. Here, $a, b,$ and $c$ are real-valued constants.\n\nThis type of problem can be approached using the quadratic penalty method. The method involves creating an unconstrained objective function, called the penalty function, defined as $P(x_1, x_2; \\mu) = f(x_1, x_2) + \\frac{\\mu}{2}[h(x_1, x_2)]^2$, where $\\mu > 0$ is a large positive constant known as the penalty parameter. The solution to the original constrained problem can be approximated by finding the minimizer of $P(x_1, x_2; \\mu)$. As $\\mu$ approaches infinity, this minimizer converges to the true solution of the constrained problem.\n\nDetermine the minimizer $(x_1^*(\\mu), x_2^*(\\mu))$ of the unconstrained penalty function $P(x_1, x_2; \\mu)$. Express your answer as a vector of two components, where each component is an analytical expression in terms of the parameters $a, b, c,$ and $\\mu$.", "solution": "We are given the penalty function\n$$\nP(x_{1},x_{2};\\mu)= (x_{1}-a)^{2}+(x_{2}-b)^{2}+\\frac{\\mu}{2}\\left(x_{1}+x_{2}-c\\right)^{2},\n$$\nwith $\\mu>0$. To find its unconstrained minimizer, set the gradient to zero. The partial derivatives are\n$$\n\\frac{\\partial P}{\\partial x_{1}}=2(x_{1}-a)+\\mu(x_{1}+x_{2}-c),\n$$\n$$\n\\frac{\\partial P}{\\partial x_{2}}=2(x_{2}-b)+\\mu(x_{1}+x_{2}-c).\n$$\nSetting these equal to zero gives the linear system\n$$\n2(x_{1}-a)+\\mu(x_{1}+x_{2}-c)=0,\\qquad 2(x_{2}-b)+\\mu(x_{1}+x_{2}-c)=0.\n$$\nEquivalently,\n$$\n(2+\\mu)x_{1}+\\mu x_{2}=2a+\\mu c,\\qquad \\mu x_{1}+(2+\\mu)x_{2}=2b+\\mu c.\n$$\nIn matrix form,\n$$\n\\begin{pmatrix}\n2+\\mu & \\mu\\\\\n\\mu & 2+\\mu\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1}\\\\\nx_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2a+\\mu c\\\\\n2b+\\mu c\n\\end{pmatrix}.\n$$\nThe determinant of the coefficient matrix is $(2+\\mu)^{2}-\\mu^{2}=4+4\\mu=4(1+\\mu)$, which is positive for $\\mu>0$, so the solution is unique. Using Cramer’s rule,\n$$\nx_{1}=\\frac{(2a+\\mu c)(2+\\mu)-\\mu(2b+\\mu c)}{4(1+\\mu)}=\\frac{4a+2\\mu(a-b+c)}{4(1+\\mu)}=\\frac{2a+\\mu(a-b+c)}{2(1+\\mu)},\n$$\n$$\nx_{2}=\\frac{(2+\\mu)(2b+\\mu c)-\\mu(2a+\\mu c)}{4(1+\\mu)}=\\frac{4b+2\\mu(b-a+c)}{4(1+\\mu)}=\\frac{2b+\\mu(b-a+c)}{2(1+\\mu)}.\n$$\nAs $\\mu\\to\\infty$, these converge to the exact constrained solution $(\\frac{a-b+c}{2},\\frac{b-a+c}{2})$, consistent with the projection onto the line $x_{1}+x_{2}=c$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{2 a+\\mu\\left(a-b+c\\right)}{2\\left(1+\\mu\\right)} & \\frac{2 b+\\mu\\left(b-a+c\\right)}{2\\left(1+\\mu\\right)}\\end{pmatrix}}$$", "id": "2193291"}, {"introduction": "What happens if the unconstrained minimum of an objective function already happens to satisfy the problem's constraints? This practice explores this elegant special case, which reveals a deeper insight into how penalty methods function. You will demonstrate that when the unconstrained optimum is already feasible, the penalty term becomes zero at the solution, and the penalized solution is exact and independent of the penalty parameter. Note that this exercise uses a penalty coefficient of $\\frac{1}{2\\mu}$, so a large penalty corresponds to a small $\\mu$, but the underlying principle remains universal.", "problem": "Consider the application of the quadratic penalty method to an optimization problem. The goal is to find the minimizer $(x_{1,\\mu}, x_{2,\\mu})$ of a penalized objective function $P(x_1, x_2; \\mu)$ for a generic penalty parameter $\\mu > 0$. The function is defined as:\n\n$$P(x_1, x_2; \\mu) = (x_1 - 1)^2 + (x_2 - 2)^2 + \\frac{1}{2\\mu} (x_1 + x_2 - 3)^2$$\n\nwhere $x_1$ and $x_2$ are real variables.\n\nFind the coordinates of the point $(x_{1,\\mu}, x_{2,\\mu})$ that minimizes this function. Your answer should be a point with constant coordinates that holds for any choice of $\\mu > 0$. Express your final answer as a $1 \\times 2$ row matrix.", "solution": "We minimize the quadratic function $P(x_{1},x_{2};\\mu)=(x_{1}-1)^{2}+(x_{2}-2)^{2}+\\frac{1}{2\\mu}(x_{1}+x_{2}-3)^{2}$ for $\\mu>0$. Since $P$ is a strictly convex quadratic in $(x_{1},x_{2})$ (its Hessian is the symmetric matrix\n$$\nH=\\begin{pmatrix}2+\\frac{1}{\\mu} & \\frac{1}{\\mu} \\\\ \\frac{1}{\\mu} & 2+\\frac{1}{\\mu}\\end{pmatrix},\n$$\nwhich is strictly diagonally dominant and hence positive definite for $\\mu>0$), the unique minimizer is characterized by the first-order optimality conditions $\\nabla P=0$.\n\nCompute the partial derivatives:\n$$\n\\frac{\\partial P}{\\partial x_{1}}=2(x_{1}-1)+\\frac{1}{\\mu}(x_{1}+x_{2}-3),\n$$\n$$\n\\frac{\\partial P}{\\partial x_{2}}=2(x_{2}-2)+\\frac{1}{\\mu}(x_{1}+x_{2}-3).\n$$\nSetting these to zero gives the linear system\n$$\n2(x_{1}-1)+\\frac{1}{\\mu}(x_{1}+x_{2}-3)=0,\\qquad 2(x_{2}-2)+\\frac{1}{\\mu}(x_{1}+x_{2}-3)=0.\n$$\nEquivalently,\n$$\n\\left(2+\\frac{1}{\\mu}\\right)x_{1}+\\frac{1}{\\mu}x_{2}=2+\\frac{3}{\\mu},\\qquad \\frac{1}{\\mu}x_{1}+\\left(2+\\frac{1}{\\mu}\\right)x_{2}=4+\\frac{3}{\\mu}.\n$$\nSubtracting the first equation from the second yields\n$$\n-2x_{1}+2x_{2}=2 \\;\\;\\Rightarrow\\;\\; x_{2}=x_{1}+1.\n$$\nSubstituting into the first equation,\n$$\n\\left(2+\\frac{1}{\\mu}\\right)x_{1}+\\frac{1}{\\mu}(x_{1}+1)=2+\\frac{3}{\\mu}\n\\;\\;\\Rightarrow\\;\\;\n\\left(2+\\frac{2}{\\mu}\\right)x_{1}+\\frac{1}{\\mu}=2+\\frac{3}{\\mu},\n$$\nso\n$$\n\\left(2+\\frac{2}{\\mu}\\right)x_{1}=2+\\frac{2}{\\mu}\n\\;\\;\\Rightarrow\\;\\;\nx_{1}=1.\n$$\nThen $x_{2}=x_{1}+1=2$. This solution is independent of $\\mu>0$ and thus holds for any penalty parameter.", "answer": "$$\\boxed{\\begin{pmatrix}1 & 2\\end{pmatrix}}$$", "id": "2193301"}, {"introduction": "Theoretically, we can find the exact solution to a constrained problem by letting the penalty parameter $\\mu$ approach infinity. In practice, however, this leads to numerical difficulties that can make the problem challenging to solve accurately. This advanced exercise investigates the root of this issue—the ill-conditioning of the Hessian matrix $\\nabla^2 P$ of the penalized objective function. By analyzing how the condition number $\\kappa(\\mu)$ grows with $\\mu$, you will gain a crucial understanding of the practical limitations of penalty methods, bridging the gap between abstract theory and computational reality.", "problem": "A materials science research team is designing a new alloy. The stability of the alloy depends on two concentration parameters, $x_1$ and $x_2$. The team aims to minimize a cost function given by $f(x_1, x_2) = x_1^2 + 4x_2^2$. For the alloy to be viable, the concentrations must satisfy the strict linear constraint $g(x_1, x_2) = x_1 + x_2 - 1 = 0$.\n\nTo solve this constrained optimization problem, the team explores an unconventional penalty method. They define a penalized objective function $P(x_1, x_2; \\mu)$ which depends on a positive penalty parameter $\\mu$:\n$$ P(x_1, x_2; \\mu) = f(x_1, x_2) + \\frac{\\mu}{4} [g(x_1, x_2)]^4 $$\nFor any given $\\mu > 0$, they find the unique minimizer $x(\\mu) = (x_1(\\mu), x_2(\\mu))$ of $P(x_1, x_2; \\mu)$.\n\nA key concern in numerical optimization is the conditioning of the problem. It is known that as $\\mu$ becomes very large, the condition number, $\\kappa(\\mu)$, of the Hessian matrix $\\nabla^2 P$ evaluated at the minimizer $x(\\mu)$, often grows, potentially leading to numerical instability. Theoretical analysis suggests that for large $\\mu$, this growth follows a power-law relationship: $\\kappa(\\mu) \\approx C \\mu^{\\gamma}$, where $C$ and $\\gamma$ are positive constants.\n\nDetermine the exact value of the exponent $\\gamma$ for this specific problem. Express your answer as a fraction.", "solution": "We are given $f(x_{1},x_{2})=x_{1}^{2}+4x_{2}^{2}$ and $g(x_{1},x_{2})=x_{1}+x_{2}-1$, with penalized objective\n$$\nP(x_{1},x_{2};\\mu)=x_{1}^{2}+4x_{2}^{2}+\\frac{\\mu}{4}\\left(x_{1}+x_{2}-1\\right)^{4},\n$$\nwhere $\\mu>0$. Let $\\phi:=g(x_{1},x_{2})=x_{1}+x_{2}-1$ for brevity.\n\nFirst, compute the gradient and Hessian of $P$. Since $\\nabla f=[2x_{1},\\,8x_{2}]^{\\top}$ and $\\nabla\\left(\\frac{\\mu}{4}\\phi^{4}\\right)=\\mu\\phi^{3}\\nabla\\phi=\\mu\\phi^{3}[1,\\,1]^{\\top}$, the stationarity conditions at the minimizer $x(\\mu)=(x_{1}(\\mu),x_{2}(\\mu))$ are\n$$\n\\begin{cases}\n2x_{1}+\\mu\\phi^{3}=0,\\\\\n8x_{2}+\\mu\\phi^{3}=0.\n\\end{cases}\n$$\nSubtracting gives $8x_{2}-2x_{1}=0$, hence $x_{1}=4x_{2}$. Using $\\phi=x_{1}+x_{2}-1=5x_{2}-1$, we obtain $x_{2}=(\\phi+1)/5$ and $x_{1}=4(\\phi+1)/5$. Substituting into $2x_{1}+\\mu\\phi^{3}=0$ yields the scalar equation for $\\phi$:\n$$\n\\mu\\phi^{3}+\\frac{8}{5}(\\phi+1)=0.\n$$\nFor large $\\mu$, the solution satisfies $\\phi\\to 0$ with the dominant balance $\\mu\\phi^{3}+\\frac{8}{5}\\approx 0$, giving the asymptotic scaling\n$$\n\\phi(\\mu)\\sim -\\left(\\frac{8}{5}\\right)^{\\frac{1}{3}}\\mu^{-\\frac{1}{3}} \\quad \\text{as} \\quad \\mu\\to\\infty.\n$$\n\nNext, compute the Hessian. Since $\\nabla^{2}f=\\mathrm{diag}(2,8)$ and for the penalty term\n$$\n\\nabla^{2}\\left(\\frac{\\mu}{4}\\phi^{4}\\right)=\\mu\\cdot 3\\phi^{2}\\,(\\nabla\\phi)(\\nabla\\phi)^{\\top}=3\\mu\\phi^{2}\\begin{bmatrix}1&1\\\\1&1\\end{bmatrix},\n$$\nthe Hessian at $x(\\mu)$ is\n$$\nH(\\mu):=\\nabla^{2}P(x(\\mu);\\mu)=\\begin{bmatrix}2&0\\\\0&8\\end{bmatrix}+t(\\mu)\\begin{bmatrix}1&1\\\\1&1\\end{bmatrix},\n$$\nwhere $t(\\mu):=3\\mu\\phi(\\mu)^{2}$. Using the scaling of $\\phi(\\mu)$, we have\n$$\nt(\\mu)=3\\mu\\phi(\\mu)^{2}\\sim 3\\left(\\frac{8}{5}\\right)^{\\frac{2}{3}}\\mu^{\\frac{1}{3}} \\quad \\text{as} \\quad \\mu\\to\\infty.\n$$\n\nThe eigenvalues of $H(\\mu)$ can be obtained exactly. Writing $A=\\mathrm{diag}(2,8)$ and $J=\\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}$, we have $H=A+tJ$. Its characteristic polynomial is\n$$\n\\det\\!\\left(\\begin{bmatrix}2+t-\\lambda & t\\\\t & 8+t-\\lambda\\end{bmatrix}\\right)=0\n\\;\\Longleftrightarrow\\;\n\\lambda^{2}-(10+2t)\\lambda+(16+10t)=0,\n$$\nso the eigenvalues are\n$$\n\\lambda_{\\pm}(t)=5+t\\pm\\sqrt{9+t^{2}}.\n$$\nFor large $t$,\n$$\n\\sqrt{9+t^{2}}=t\\sqrt{1+\\frac{9}{t^{2}}}=t+\\frac{9}{2t}+O(t^{-3}),\n$$\nhence\n$$\n\\lambda_{+}(t)=5+2t+\\frac{9}{2t}+O(t^{-3}),\\qquad\n\\lambda_{-}(t)=5-\\frac{9}{2t}+O(t^{-3}).\n$$\nTherefore, the spectral condition number in the $2$-norm is\n$$\n\\kappa(\\mu)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{\\lambda_{+}(t(\\mu))}{\\lambda_{-}(t(\\mu))}\n=\\frac{5+2t(\\mu)+\\frac{9}{2t(\\mu)}+O(t(\\mu)^{-3})}{5-\\frac{9}{2t(\\mu)}+O(t(\\mu)^{-3})}\n=\\frac{2}{5}t(\\mu)+O\\!\\left(\\frac{1}{t(\\mu)}\\right).\n$$\nSince $t(\\mu)\\sim \\text{const}\\cdot \\mu^{\\frac{1}{3}}$, it follows that\n$$\n\\kappa(\\mu)\\sim C\\,\\mu^{\\frac{1}{3}} \\quad \\text{as} \\quad \\mu\\to\\infty,\n$$\nfor some positive constant $C$. Thus the exponent is\n$$\n\\gamma=\\frac{1}{3}.\n$$", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "2193316"}]}