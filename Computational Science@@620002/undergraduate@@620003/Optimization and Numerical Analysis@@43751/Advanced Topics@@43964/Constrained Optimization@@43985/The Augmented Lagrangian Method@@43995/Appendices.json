{"hands_on_practices": [{"introduction": "To truly understand the Augmented Lagrangian Method, we must start with its fundamental building block: a single iteration. This practice exercise guides you through the two core steps of the algorithmâ€”solving an unconstrained minimization subproblem and subsequently updating the Lagrange multiplier. Mastering this process is the first step toward applying the method to more complex optimization challenges. [@problem_id:2208360]", "problem": "Consider the constrained optimization problem of minimizing the objective function $f(x) = x^2$ subject to the equality constraint $h(x) = x - 3 = 0$.\n\nThe augmented Lagrangian method is an iterative algorithm for solving such problems. The augmented Lagrangian function is defined as:\n$$L_A(x, \\lambda; \\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2} [h(x)]^2$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\rho > 0$ is the penalty parameter.\n\nA single iteration of the method, starting from an estimate $\\lambda_k$, consists of two main steps:\n1.  Find the next iterate $x_{k+1}$ by solving the unconstrained minimization problem:\n    $$x_{k+1} = \\arg\\min_{x} L_A(x, \\lambda_k; \\rho)$$\n2.  Update the Lagrange multiplier using the formula:\n    $$\\lambda_{k+1} = \\lambda_k + \\rho h(x_{k+1})$$\n\nPerform one full iteration of the augmented Lagrangian method starting with an initial multiplier estimate $\\lambda_0 = 1$ and using a penalty parameter $\\rho = 2$. Determine the resulting values for the new iterate $x_1$ and the updated multiplier $\\lambda_1$.\n\nExpress your answer as a row matrix $\\begin{pmatrix} x_1 & \\lambda_1 \\end{pmatrix}$ using exact fractions.", "solution": "We are given $f(x) = x^{2}$ and $h(x) = x - 3$, with augmented Lagrangian\n$$L_{A}(x,\\lambda;\\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^{2}.$$\nWith $\\lambda_{0} = 1$ and $\\rho = 2$, this becomes\n$$L_{A}(x,1;2) = x^{2} + 1\\cdot(x-3) + \\frac{2}{2}(x-3)^{2} = x^{2} + x - 3 + (x-3)^{2}.$$\nTo obtain $x_{1}$, solve the unconstrained minimization:\n$$x_{1} = \\arg\\min_{x} L_{A}(x,1;2).$$\nDifferentiate and set the derivative to zero:\n$$\\frac{d}{dx}L_{A}(x,1;2) = 2x + 1 + 2(x-3) = 4x - 5,$$\n$$4x - 5 = 0 \\implies x_{1} = \\frac{5}{4}.$$\nThe second derivative is\n$$\\frac{d^{2}}{dx^{2}}L_{A}(x,1;2) = 4 > 0,$$\nso $x_{1} = \\frac{5}{4}$ is the unique minimizer.\n\nUpdate the multiplier using $\\lambda_{1} = \\lambda_{0} + \\rho h(x_{1})$:\n$$h(x_{1}) = \\frac{5}{4} - 3 = -\\frac{7}{4},$$\n$$\\lambda_{1} = 1 + 2\\left(-\\frac{7}{4}\\right) = 1 - \\frac{7}{2} = -\\frac{5}{2}.$$\n\nThus, the resulting values are $x_{1} = \\frac{5}{4}$ and $\\lambda_{1} = -\\frac{5}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{4} & -\\frac{5}{2} \\end{pmatrix}}$$", "id": "2208360"}, {"introduction": "Having executed a single iteration, we now turn our attention to one of the method's most critical components: the penalty parameter, $\\rho$. This parameter governs the extent to which the constraint is enforced at each step. This problem allows us to derive an analytical solution, offering a clear glimpse into how $\\rho$ influences the iterate and balances the pull between the original objective and the constraint. [@problem_id:2208363]", "problem": "An engineer is using the augmented Lagrangian method to solve a simple one-dimensional constrained optimization problem. The goal is to minimize the objective function $f(x) = \\frac{1}{2} \\beta x^2$ subject to the linear equality constraint $c(x) = x - a = 0$. Here, $\\beta$ and $a$ are given positive real constants.\n\nThe augmented Lagrangian function for this problem is defined as $L_A(x, \\lambda; \\rho) = f(x) + \\lambda c(x) + \\frac{\\rho}{2} [c(x)]^2$, where $\\lambda$ is the Lagrange multiplier and $\\rho > 0$ is the penalty parameter.\n\nThe first step of the algorithm involves minimizing $L_A(x, \\lambda_0; \\rho)$ with respect to $x$ to find the first primal iterate, $x_0$. This step is performed using an initial guess for the Lagrange multiplier, $\\lambda_0$. Assume the engineer uses an initial guess of $\\lambda_0 = 0$ and a fixed penalty parameter $\\rho$.\n\nDetermine the resulting first primal iterate $x_0$. Express your answer as a single closed-form analytic expression in terms of $a$, $\\beta$, and $\\rho$.", "solution": "We are given the objective function $f(x) = \\frac{1}{2}\\beta x^{2}$ and the constraint $c(x) = x - a = 0$. The augmented Lagrangian is\n$$\nL_{A}(x,\\lambda;\\rho) = f(x) + \\lambda c(x) + \\frac{\\rho}{2}[c(x)]^{2}.\n$$\nWith the initial multiplier $\\lambda_{0} = 0$, we minimize with respect to $x$ the function\n$$\nL_{A}(x,0;\\rho) = \\frac{1}{2}\\beta x^{2} + \\frac{\\rho}{2}(x - a)^{2}.\n$$\nTo find the minimizer, take the derivative with respect to $x$ and set it to zero:\n$$\n\\frac{d}{dx}L_{A}(x,0;\\rho) = \\beta x + \\rho(x - a) = (\\beta + \\rho)x - \\rho a = 0.\n$$\nSolving for $x$ yields\n$$\nx = \\frac{\\rho a}{\\beta + \\rho}.\n$$\nSince the second derivative is\n$$\n\\frac{d^{2}}{dx^{2}}L_{A}(x,0;\\rho) = \\beta + \\rho > 0,\n$$\nthis critical point is the unique minimizer. Therefore, the first primal iterate is\n$$\nx_{0} = \\frac{\\rho a}{\\beta + \\rho}.\n$$", "answer": "$$\\boxed{\\frac{\\rho a}{\\beta + \\rho}}$$", "id": "2208363"}, {"introduction": "The real power of the Augmented Lagrangian Method is revealed not in a single step, but in the sequence of operations. This final practice illuminates the iterative nature of the algorithm, showing how the sequence of solutions to the subproblems, $\\{x_k\\}$, converges toward the true constrained optimum. By computing the first few iterates from a hypothetical scenario, you will witness the method's powerful convergence property firsthand. [@problem_id:2208359]", "problem": "The method of multipliers, also known as the augmented Lagrangian method, is an iterative algorithm for solving constrained optimization problems. Consider a one-dimensional optimization problem of the form:\nMinimize $f(x)$ subject to the equality constraint $h(x) = 0$.\n\nThe augmented Lagrangian function $L_\\rho(x, \\lambda)$ is defined as:\n$$L_\\rho(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2} (h(x))^2$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\rho > 0$ is a constant penalty parameter.\n\nThe method of multipliers proceeds via the following iterative scheme starting from an initial multiplier $\\lambda_0$:\nFor $k = 0, 1, 2, \\dots$\n1. Find the next iterate $x_{k+1}$ by minimizing the augmented Lagrangian with the current multiplier estimate $\\lambda_k$:\n   $$x_{k+1} = \\arg\\min_x L_\\rho(x, \\lambda_k)$$\n2. Update the Lagrange multiplier:\n   $$\\lambda_{k+1} = \\lambda_k + \\rho h(x_{k+1})$$\n\nApply the first three iterations of this method to the problem of minimizing the function $f(x) = x^2$ subject to the constraint $x - 1 = 0$.\nUse a fixed penalty parameter $\\rho > 0$ and start with an initial multiplier estimate of $\\lambda_0 = 0$.\n\nDetermine the analytical expressions for the first three minimizers, $x_1, x_2$, and $x_3$, in terms of the penalty parameter $\\rho$. Present your answers for $x_1, x_2,$ and $x_3$ in order.", "solution": "We are asked to minimize $f(x)=x^{2}$ subject to $h(x)=x-1=0$ using the method of multipliers with augmented Lagrangian\n$$\nL_{\\rho}(x,\\lambda)=x^{2}+\\lambda(x-1)+\\frac{\\rho}{2}(x-1)^{2},\n$$\na fixed $\\rho>0$, and initial multiplier $\\lambda_{0}=0$. The iterations are:\n$$\nx_{k+1}=\\arg\\min_{x}L_{\\rho}(x,\\lambda_{k}),\\quad \\lambda_{k+1}=\\lambda_{k}+\\rho\\,h(x_{k+1})=\\lambda_{k}+\\rho(x_{k+1}-1).\n$$\nTo find $x_{k+1}$, differentiate $L_{\\rho}(x,\\lambda_{k})$ with respect to $x$ and set the derivative to zero:\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x}=2x+\\lambda_{k}+\\rho(x-1)=0.\n$$\nThis gives a linear equation in $x$:\n$$\n(2+\\rho)x+\\lambda_{k}-\\rho=0 \\quad\\Longrightarrow\\quad x_{k+1}=\\frac{\\rho-\\lambda_{k}}{2+\\rho}.\n$$\nUpdate $\\lambda$ using the rule:\n$$\n\\lambda_{k+1}=\\lambda_{k}+\\rho\\left(\\frac{\\rho-\\lambda_{k}}{2+\\rho}-1\\right)=\\lambda_{k}-\\frac{\\rho(\\lambda_{k}+2)}{2+\\rho}.\n$$\nThis simplifies to the affine recursion\n$$\n\\lambda_{k+1}=\\frac{2}{2+\\rho}\\lambda_{k}-\\frac{2\\rho}{2+\\rho}.\n$$\nDefine $a=\\frac{2}{2+\\rho}$. With $\\lambda_{0}=0$, the solution of the recursion is\n$$\n\\lambda_{k}=-2\\sum_{i=0}^{k-1}a^{i}=-2\\frac{1-a^{k}}{1-a}=2a^{k}-2.\n$$\nSubstitute into $x_{k+1}=\\frac{\\rho-\\lambda_{k}}{2+\\rho}$ to obtain\n$$\nx_{k+1}=\\frac{\\rho-(2a^{k}-2)}{2+\\rho}=\\frac{\\rho+2-2a^{k}}{2+\\rho}=1-\\frac{2a^{k}}{2+\\rho}\n=1-\\frac{2^{k+1}}{(2+\\rho)^{k+1}}.\n$$\nNow compute the first three minimizers:\n- For $k=0$,\n$$\nx_{1}=1-\\frac{2}{2+\\rho}=\\frac{\\rho}{2+\\rho}.\n$$\n- For $k=1$,\n$$\nx_{2}=1-\\frac{2^{2}}{(2+\\rho)^{2}}=\\frac{(2+\\rho)^{2}-4}{(2+\\rho)^{2}}=\\frac{\\rho(\\rho+4)}{(2+\\rho)^{2}}.\n$$\n- For $k=2$,\n$$\nx_{3}=1-\\frac{2^{3}}{(2+\\rho)^{3}}=\\frac{(2+\\rho)^{3}-8}{(2+\\rho)^{3}}=\\frac{\\rho(\\rho^{2}+6\\rho+12)}{(2+\\rho)^{3}}.\n$$\nTherefore, the requested expressions for $x_{1},x_{2},x_{3}$ in terms of $\\rho$ are as above.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{2+\\rho} & \\frac{\\rho(\\rho+4)}{(2+\\rho)^{2}} & \\frac{\\rho(\\rho^{2}+6\\rho+12)}{(2+\\rho)^{3}}\\end{pmatrix}}$$", "id": "2208359"}]}