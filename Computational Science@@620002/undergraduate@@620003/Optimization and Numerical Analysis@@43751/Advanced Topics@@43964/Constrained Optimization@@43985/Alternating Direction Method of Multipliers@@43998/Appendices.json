{"hands_on_practices": [{"introduction": "The Alternating Direction Method of Multipliers shines when it is used to decompose complex optimization problems into a sequence of simpler steps. This practice explores a powerful heuristic for sparse linear regression, where the goal is to find a model that fits the data well while using as few features as possible. You will derive the update step for the regression coefficients, which, as you will see, reduces to solving a familiar ridge regression-like problem. This exercise [@problem_id:2153785] demonstrates how ADMM can isolate a quadratic objective into a manageable subproblem solvable with standard linear algebra.", "problem": "In sparse linear regression, the goal is to find a model that explains a set of observations using a minimal number of explanatory variables. One formulation for this problem seeks to minimize the sum of squared errors subject to a cardinality constraint on the model's coefficient vector. This problem is computationally difficult due to the non-convex nature of the cardinality constraint.\n\nConsider the following optimization problem:\n$$\n\\text{minimize} \\quad \\frac{1}{2}\\|y - Ax\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_0 \\le K\n$$\nHere, $y \\in \\mathbb{R}^m$ is the vector of observations, $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix, and $x \\in \\mathbb{R}^n$ is the coefficient vector we wish to find. The term $\\|x\\|_0$ denotes the $\\ell_0$-norm, which counts the number of non-zero elements in $x$, and $K$ is a positive integer representing the maximum desired number of non-zero coefficients.\n\nTo develop a heuristic algorithm for this problem, we can use the Alternating Direction Method of Multipliers (ADMM). The problem is first reformulated by introducing an auxiliary variable $z \\in \\mathbb{R}^n$ and an equality constraint:\n$$\n\\text{minimize} \\quad f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0\n$$\nwhere $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$ and $g(z)$ is the indicator function for the non-convex set $C = \\{z \\in \\mathbb{R}^n \\mid \\|z\\|_0 \\le K\\}$.\n\nThe scaled-form augmented Lagrangian for this problem is given by:\n$$\nL_\\rho(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2}\\|x - z + u\\|_2^2\n$$\nwhere $u$ is the scaled dual variable and $\\rho > 0$ is a penalty parameter. The ADMM algorithm proceeds via the following iterations at step $k$:\n1.  $x^{k+1} := \\arg\\min_x L_\\rho(x, z^k, u^k)$\n2.  $z^{k+1} := \\arg\\min_z L_\\rho(x^{k+1}, z, u^k)$\n3.  $u^{k+1} := u^k + x^{k+1} - z^{k+1}$\n\nAssuming that the matrix $A^TA + \\rho I$ is invertible, derive the closed-form expression for the $x$-update step, $x^{k+1}$, in terms of $A, y, \\rho, z^k,$ and $u^k$.", "solution": "We consider the $x$-update at iteration $k$, which minimizes $L_{\\rho}(x,z^{k},u^{k})$ with respect to $x$. Since $g(z^{k})$ does not depend on $x$, the $x$-subproblem is\n$$\n\\min_{x}\\ \\frac{1}{2}\\|y-Ax\\|_{2}^{2}+\\frac{\\rho}{2}\\|x-z^{k}+u^{k}\\|_{2}^{2}.\n$$\nDefine the objective\n$$\n\\phi(x)=\\frac{1}{2}(y-Ax)^{T}(y-Ax)+\\frac{\\rho}{2}(x-z^{k}+u^{k})^{T}(x-z^{k}+u^{k}).\n$$\nCompute the gradient with respect to $x$:\n$$\n\\nabla\\phi(x)=-A^{T}(y-Ax)+\\rho(x-z^{k}+u^{k})= -A^{T}y+A^{T}Ax+\\rho x-\\rho z^{k}+\\rho u^{k}.\n$$\nSet the gradient to zero for optimality:\n$$\n-A^{T}y+A^{T}Ax+\\rho x-\\rho z^{k}+\\rho u^{k}=0.\n$$\nRearrange to collect terms in $x$:\n$$\n(A^{T}A+\\rho I)x=A^{T}y+\\rho(z^{k}-u^{k}).\n$$\nBy the assumption that $A^{T}A+\\rho I$ is invertible, the unique minimizer is\n$$\nx^{k+1}=(A^{T}A+\\rho I)^{-1}\\left(A^{T}y+\\rho(z^{k}-u^{k})\\right).\n$$", "answer": "$$\\boxed{(A^{T}A+\\rho I)^{-1}\\left(A^{T}y+\\rho\\left(z^{k}-u^{k}\\right)\\right)}$$", "id": "2153785"}, {"introduction": "A key feature of ADMM is its ability to elegantly handle constraints by encoding them within one of the functions to be minimized, often using an \"indicator function\" that restricts a variable to a specific feasible set. In this exercise [@problem_id:2153782], you will discover that the ADMM update step for such a variable becomes a simple and intuitive geometric operation: a projection onto the feasible set. This provides a powerful visual understanding of how ADMM enforces constraints in an iterative fashion.", "problem": "Let's consider an optimization problem of the form:\n$$\n\\begin{aligned}\n& \\underset{x, z}{\\text{minimize}}\n& & f(x) + g(z) \\\\\n& \\text{subject to}\n& & x - z = 0\n\\end{aligned}\n$$\nwhere $f$ and $g$ are convex functions. This problem can be solved using the Alternating Direction Method of Multipliers (ADMM). The scaled-form augmented Lagrangian for this problem is given by:\n$$L_{\\rho}(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2} \\|x - z + u\\|_2^2$$\nwhere $\\rho > 0$ is a penalty parameter and $u$ is the scaled dual variable. The ADMM algorithm proceeds via the following iterative updates:\n1. $x^{k+1} := \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2} \\|x - z^k + u^k\\|_2^2 \\right)$\n2. $z^{k+1} := \\arg\\min_{z} \\left( g(z) + \\frac{\\rho}{2} \\|x^{k+1} - z + u^k\\|_2^2 \\right)$\n3. $u^{k+1} := u^k + x^{k+1} - z^{k+1}$\n\nSuppose the function $g(z)$ is the indicator function of a non-empty closed convex set $\\mathcal{C}$. The indicator function, $I_{\\mathcal{C}}(z)$, is defined as:\n$$\nI_{\\mathcal{C}}(z) = \\begin{cases} 0 & \\text{if } z \\in \\mathcal{C} \\\\ \\infty & \\text{if } z \\notin \\mathcal{C} \\end{cases}\n$$\nDetermine the closed-form expression for the $z$-update, $z^{k+1}$. Your answer should be expressed in terms of the projection operator $\\Pi_{\\mathcal{C}}$, the variable $x^{k+1}$, and the scaled dual variable $u^k$. The projection of a vector $v$ onto the set $\\mathcal{C}$ is defined as $\\Pi_{\\mathcal{C}}(v) = \\arg\\min_{w \\in \\mathcal{C}} \\|w - v\\|_2^2$.", "solution": "We start from the $z$-update in the scaled ADMM iterations:\n$$\nz^{k+1} := \\arg\\min_{z} \\left( g(z) + \\frac{\\rho}{2} \\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right).\n$$\nWith $g(z)=I_{\\mathcal{C}}(z)$, the objective is infinite whenever $z \\notin \\mathcal{C}$ and equals the quadratic term when $z \\in \\mathcal{C}$. Therefore, the minimization is equivalent to\n$$\nz^{k+1} = \\arg\\min_{z \\in \\mathcal{C}} \\frac{\\rho}{2} \\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nSince $\\rho>0$ is a positive scalar, multiplying the objective by a positive constant does not change the minimizer, so we can drop the factor $\\rho/2$ without affecting the argmin:\n$$\nz^{k+1} = \\arg\\min_{z \\in \\mathcal{C}} \\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nUsing the symmetry of the Euclidean norm, we rewrite the argument as\n$$\n\\|x^{k+1} - z + u^{k}\\|_{2}^{2} = \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nThus,\n$$\nz^{k+1} = \\arg\\min_{z \\in \\mathcal{C}} \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nBy the definition of the projection operator $\\Pi_{\\mathcal{C}}(v) = \\arg\\min_{w \\in \\mathcal{C}} \\|w - v\\|_{2}^{2}$, we identify $v = x^{k+1} + u^{k}$ and obtain the closed-form update\n$$\nz^{k+1} = \\Pi_{\\mathcal{C}}(x^{k+1} + u^{k}).\n$$\nThis is the projection of $x^{k+1} + u^{k}$ onto the set $\\mathcal{C}$.", "answer": "$$\\boxed{\\Pi_{\\mathcal{C}}(x^{k+1}+u^{k})}$$", "id": "2153782"}, {"introduction": "Theory and derivation are essential, but true understanding is often cemented through concrete application. This hands-on practice [@problem_id:2153755] moves from abstract formulas to a numerical calculation, asking you to compute the result of a single ADMM iteration for a resource allocation problem. By working with specific matrices and vectors, you will solidify your grasp of the algorithm's mechanics and gain a tangible feel for how the variables evolve as the algorithm proceeds.", "problem": "Consider a simplified resource allocation problem where we want to determine a state vector $x \\in \\mathbb{R}^n$ that is as close as possible to a given target state $c \\in \\mathbb{R}^n$, subject to a set of $k$ linear constraints given by $Bx = z_{target}$, where $B \\in \\mathbb{R}^{k \\times n}$ and $z_{target} \\in \\mathbb{R}^k$. The optimization problem is formulated as:\n$$ \\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|x-c\\|_2^2 \\quad \\text{subject to} \\quad Bx = z_{target} $$\n\nTo solve this, we can use the Alternating Direction Method of Multipliers (ADMM). The problem is first cast into the ADMM standard form by introducing an auxiliary variable $z \\in \\mathbb{R}^k$:\n$$ \\min_{x, z} f(x) + g(z) \\quad \\text{subject to} \\quad Bx - z = 0 $$\nwhere $f(x) = \\frac{1}{2}\\|x-c\\|_2^2$ and $g(z)$ is the indicator function for the point $z_{target}$, i.e., $g(z) = 0$ if $z=z_{target}$ and $g(z) = \\infty$ otherwise.\n\nThe ADMM algorithm iterates the following steps for an iteration counter $j=0, 1, 2, \\dots$:\n1. $x^{(j+1)} := \\arg\\min_x \\left( f(x) + \\frac{\\rho}{2}\\|Bx - z^{(j)} + u^{(j)}\\|_2^2 \\right)$\n2. $z^{(j+1)} := \\arg\\min_z \\left( g(z) + \\frac{\\rho}{2}\\|Bx^{(j+1)} - z + u^{(j)}\\|_2^2 \\right)$\n3. $u^{(j+1)} := u^{(j)} + Bx^{(j+1)} - z^{(j+1)}$\n\nHere, $u$ is the scaled dual variable and $\\rho > 0$ is a given penalty parameter.\n\nYou are given the following specific values for the problem:\n- Dimension of the state vector, $n=4$.\n- Number of linear constraints, $k=2$.\n- The constraint matrix $B = \\begin{pmatrix} 1 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix}$.\n- The target state vector $c = \\begin{pmatrix} 10 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}$.\n- The target for the constraints $z_{target} = \\begin{pmatrix} 12 \\\\ 13 \\end{pmatrix}$.\n- The penalty parameter $\\rho = 3$.\n- The initial auxiliary variable $z^{(0)} = \\begin{pmatrix} 12 \\\\ 13 \\end{pmatrix}$.\n- The initial scaled dual variable $u^{(0)} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nYour task is to compute the state vector $x^{(1)}$ after the first iteration ($j=0$). Specifically, calculate the first component of the vector $x^{(1)}$, denoted as $x_1^{(1)}$. Give your answer as a real number rounded to four significant figures.", "solution": "We use the ADMM $x$-update at $j=0$:\n$$\nx^{(1)}=\\arg\\min_{x}\\left(\\frac{1}{2}\\|x-c\\|_{2}^{2}+\\frac{\\rho}{2}\\|Bx-z^{(0)}+u^{(0)}\\|_{2}^{2}\\right).\n$$\nThis is a strictly convex quadratic; setting the gradient to zero gives the normal equations\n$$\n(x-c)+\\rho B^{\\top}(Bx-z^{(0)}+u^{(0)})=0 \\;\\;\\Longrightarrow\\;\\; (I+\\rho B^{\\top}B)x=c+\\rho B^{\\top}(z^{(0)}-u^{(0)}).\n$$\nWith the given data,\n$$\nB=\\begin{pmatrix}1&1&1&0\\\\0&1&0&1\\end{pmatrix},\\quad \\rho=3,\\quad c=\\begin{pmatrix}10\\\\2\\\\5\\\\7\\end{pmatrix},\\quad z^{(0)}=\\begin{pmatrix}12\\\\13\\end{pmatrix},\\quad u^{(0)}=\\begin{pmatrix}1\\\\-1\\end{pmatrix}.\n$$\nCompute\n$$\nB^{\\top}=\\begin{pmatrix}1&0\\\\1&1\\\\1&0\\\\0&1\\end{pmatrix},\\quad B^{\\top}B=\\begin{pmatrix}1&1&1&0\\\\1&2&1&1\\\\1&1&1&0\\\\0&1&0&1\\end{pmatrix},\n$$\nhence\n$$\nA:=I+\\rho B^{\\top}B=\\begin{pmatrix}4&3&3&0\\\\3&7&3&3\\\\3&3&4&0\\\\0&3&0&4\\end{pmatrix}.\n$$\nNext,\n$$\nd:=z^{(0)}-u^{(0)}=\\begin{pmatrix}11\\\\14\\end{pmatrix},\\quad B^{\\top}d=\\begin{pmatrix}11\\\\25\\\\11\\\\14\\end{pmatrix},\\quad \\rho B^{\\top}d=\\begin{pmatrix}33\\\\75\\\\33\\\\42\\end{pmatrix},\n$$\nso\n$$\nb:=c+\\rho B^{\\top}(z^{(0)}-u^{(0)})=\\begin{pmatrix}43\\\\77\\\\38\\\\49\\end{pmatrix}.\n$$\nWe solve $Ax=b$, i.e.,\n$$\n\\begin{cases}\n4x_{1}+3x_{2}+3x_{3}=43,\\\\\n3x_{1}+7x_{2}+3x_{3}+3x_{4}=77,\\\\\n3x_{1}+3x_{2}+4x_{3}=38,\\\\\n3x_{2}+4x_{4}=49.\n\\end{cases}\n$$\nFrom the first and third equations, subtracting gives $x_{1}-x_{3}=5$, hence $x_{1}=x_{3}+5$. Using the third equation,\n$$\n3(x_{3}+5)+3x_{2}+4x_{3}=38 \\;\\;\\Longrightarrow\\;\\; 3x_{2}+7x_{3}=23.\n$$\nFrom the fourth equation, $x_{4}=(49-3x_{2})/4$. Substitute $x_{1}=x_{3}+5$ and $x_{4}$ into the second equation:\n$$\n3(x_{3}+5)+7x_{2}+3x_{3}+3\\frac{49-3x_{2}}{4}=77,\n$$\nwhich simplifies to\n$$\n19x_{2}+24x_{3}=101.\n$$\nSolving the linear system\n$$\n\\begin{cases}\n3x_{2}+7x_{3}=23,\\\\\n19x_{2}+24x_{3}=101,\n\\end{cases}\n$$\ngives $x_{3}=\\frac{134}{61}$ and $x_{2}=\\frac{155}{61}$. Therefore,\n$$\nx_{1}=x_{3}+5=\\frac{134}{61}+5=\\frac{439}{61}.\n$$\nThus the first component is $x_{1}^{(1)}=\\frac{439}{61}\\approx 7.196721\\ldots$, which rounded to four significant figures is $7.197$.", "answer": "$$\\boxed{7.197}$$", "id": "2153755"}]}