{"hands_on_practices": [{"introduction": "The first step in any curve-fitting optimization is to define what a \"best fit\" means. The Levenberg-Marquardt algorithm, like other least-squares methods, quantifies the misfit between a model's prediction and the observed data through a residual vector, $\\mathbf{r}$. This exercise [@problem_id:2217008] provides foundational practice in calculating these residuals for a concrete geometric problem, which represents the very quantity the algorithm seeks to minimize.", "problem": "A common task in optimization is fitting a geometric model to a set of data points. Consider the problem of fitting a circle to a set of points in a 2D plane. The circle is defined by a parameter vector $\\mathbf{p} = [x_c, y_c, R]^T$, where $(x_c, y_c)$ is the center of the circle and $R$ is its radius.\n\nFor a set of $n$ data points $(x_i, y_i)$, the goal of a non-linear least squares fit is to find the parameter vector $\\mathbf{p}$ that minimizes the sum of squared residuals. The residual for the $i$-th data point, $r_i(\\mathbf{p})$, is defined as the difference between the point's distance to the proposed center $(x_c, y_c)$ and the proposed radius $R$.\n\nYou are given three data points representing measurements in a Cartesian coordinate system where all coordinates are in meters:\n$P_1 = (1.0, 7.0)$\n$P_2 = (6.0, 2.0)$\n$P_3 = (9.0, 8.0)$\n\nAn iterative optimization algorithm, such as the Levenberg-Marquardt algorithm, begins with an initial guess for the parameters. The initial guess for the circle's parameters is given as $\\mathbf{p}_0 = [x_{c,0}, y_{c,0}, R_0]^T = [5.0, 5.0, 4.0]^T$.\n\nCalculate the residual vector $\\mathbf{r}(\\mathbf{p}_0) = [r_1(\\mathbf{p}_0), r_2(\\mathbf{p}_0), r_3(\\mathbf{p}_0)]^T$ for this initial guess. Express each component of the resulting vector in meters, rounded to four significant figures. Present your final answer as a single row matrix.", "solution": "For a circle with parameters $\\mathbf{p} = [x_{c}, y_{c}, R]^{T}$ and a data point $(x_{i}, y_{i})$, the residual is defined as the difference between the Euclidean distance from the point to the center and the radius:\n$$\nr_{i}(\\mathbf{p}) = \\sqrt{(x_{i} - x_{c})^{2} + (y_{i} - y_{c})^{2}} - R.\n$$\nWith the initial guess $\\mathbf{p}_{0} = [5.0, 5.0, 4.0]^{T}$, compute each residual.\n\nFor $P_{1} = (1.0, 7.0)$:\n$$\nd_{1} = \\sqrt{(1.0 - 5.0)^{2} + (7.0 - 5.0)^{2}} = \\sqrt{(-4.0)^{2} + (2.0)^{2}} = \\sqrt{16 + 4} = \\sqrt{20},\n$$\n$$\nr_{1}(\\mathbf{p}_{0}) = \\sqrt{20} - 4.0 \\approx 0.4721 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{2} = (6.0, 2.0)$:\n$$\nd_{2} = \\sqrt{(6.0 - 5.0)^{2} + (2.0 - 5.0)^{2}} = \\sqrt{(1.0)^{2} + (-3.0)^{2}} = \\sqrt{1 + 9} = \\sqrt{10},\n$$\n$$\nr_{2}(\\mathbf{p}_{0}) = \\sqrt{10} - 4.0 \\approx -0.8377 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{3} = (9.0, 8.0)$:\n$$\nd_{3} = \\sqrt{(9.0 - 5.0)^{2} + (8.0 - 5.0)^{2}} = \\sqrt{(4.0)^{2} + (3.0)^{2}} = \\sqrt{16 + 9} = \\sqrt{25} = 5.0,\n$$\n$$\nr_{3}(\\mathbf{p}_{0}) = 5.0 - 4.0 = 1.000 \\text{ (to four significant figures)}.\n$$\n\nThus the residual vector, as a row matrix, is:\n$$\n\\begin{pmatrix}\n0.4721 & -0.8377 & 1.000\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.4721 & -0.8377 & 1.000 \\end{pmatrix}}$$", "id": "2217008"}, {"introduction": "Once we have quantified the error, the next question is how to adjust our model's parameters to reduce it. The Jacobian matrix, $\\mathbf{J}$, is the key, as it maps how sensitive each residual is to a small change in each parameter. This practice [@problem_id:2217052] will guide you through the process of calculating the Jacobian for a non-linear model, a critical skill for assembling the core equation of the LM update step.", "problem": "In the field of non-linear optimization, algorithms like the Levenberg-Marquardt algorithm are used to fit a model function to a set of data points by minimizing the sum of the squares of the residuals. A key component in this process is the Jacobian matrix of the model function.\n\nConsider a model used to describe the concentration of a substance over time, given by the two-parameter rational function:\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\nwhere $t$ is the independent variable (e.g., time), and $\\mathbf{p} = [a, b]^T$ is the vector of parameters to be determined.\n\nSuppose we have collected the following three data points $(t_i, y_i)$:\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\nThe Jacobian matrix $\\mathbf{J}$ for this fitting problem is an $m \\times n$ matrix, where $m$ is the number of data points and $n$ is the number of parameters. Its elements are defined by $J_{ij} = \\frac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$.\n\nCalculate the numerical value of the Jacobian matrix $\\mathbf{J}$ evaluated at the initial parameter guess $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$. All given numerical values are dimensionless.\n\nExpress your final answer as a $3 \\times 2$ matrix, with each element rounded to three significant figures.", "solution": "We are given the model function $f(t; a, b) = \\dfrac{a}{1 + bt}$ and the Jacobian matrix defined by $J_{ij} = \\dfrac{\\partial f(t_{i}; \\mathbf{p})}{\\partial p_{j}}$ for the parameter vector $\\mathbf{p} = [a, b]^{T}$. Thus, each row of $\\mathbf{J}$ corresponds to a data point $t_{i}$, and the two columns correspond to the derivatives with respect to $a$ and $b$.\n\nFirst, compute the partial derivatives symbolically. Write $f(t; a, b) = a(1 + bt)^{-1}$. Then, using the power rule and chain rule:\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\nTherefore, for each data point $t_{i}$, the Jacobian row is\n$$\n\\left[\\frac{1}{1 + b t_{i}},\\ -\\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\nEvaluate at the initial guess $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ and the given $t$ values.\n\nFor $t_{1} = 1$:\n$$\n1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = \\frac{3}{2},\\quad \\frac{\\partial f}{\\partial a} = \\frac{2}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 1}{(3/2)^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\nFor $t_{2} = 2$:\n$$\n1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{2},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 2}{2^{2}} = -\\frac{6}{4} = -\\frac{3}{2}.\n$$\n\nFor $t_{3} = 4$:\n$$\n1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 4}{3^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\nAssemble the Jacobian and round each element to three significant figures:\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n0.667 & -1.33 \\\\\n0.500 & -1.50 \\\\\n0.333 & -1.33\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.667 & -1.33 \\\\ 0.500 & -1.50 \\\\ 0.333 & -1.33\\end{pmatrix}}$$", "id": "2217052"}, {"introduction": "The true power of the Levenberg-Marquardt algorithm lies in its adaptive strategy, which is controlled by the damping parameter $\\lambda$. This parameter intelligently blends the fast convergence of the Gauss-Newton method with the robustness of gradient descent. This exercise [@problem_id:2216991] focuses on the crucial decision-making logic of the algorithm: how to adjust $\\lambda$ when a proposed step fails to improve the fit, ensuring the optimization process remains stable and moves progressively toward a solution.", "problem": "The Levenberg-Marquardt algorithm is an iterative method used to solve non-linear least squares problems. At each iteration $k$, the algorithm attempts to find a parameter update step $\\boldsymbol{\\delta}_k$ that minimizes a sum of squared errors function, $S(\\boldsymbol{\\beta})$. The update is calculated from the current parameter estimate $\\boldsymbol{\\beta}_k$. A key feature of the algorithm is a damping parameter, $\\lambda$, which adaptively blends the fast-converging Gauss-Newton method with the robust gradient descent method.\n\nConsider a single iteration of this algorithm. The current parameters are $\\boldsymbol{\\beta}_{\\text{old}}$ with a corresponding sum of squared errors $S(\\boldsymbol{\\beta}_{\\text{old}})$. A candidate step $\\boldsymbol{\\delta}$ is calculated, leading to a new parameter estimate $\\boldsymbol{\\beta}_{\\text{new}} = \\boldsymbol{\\beta}_{\\text{old}} + \\boldsymbol{\\delta}$. However, upon evaluation, it is found that the new estimate is worse than the old one, i.e., $S(\\boldsymbol{\\beta}_{\\text{new}}) \\ge S(\\boldsymbol{\\beta}_{\\text{old}})$.\n\nIn this situation, the algorithm must reject the step (i.e., set $\\boldsymbol{\\beta}_{k+1} = \\boldsymbol{\\beta}_{\\text{old}})$ and adjust the damping parameter $\\lambda$ before re-calculating a new step from $\\boldsymbol{\\beta}_{\\text{old}}$. A common strategy is to update $\\lambda$ using a multiplicative factor $\\nu > 1$.\n\nWhich of the following update rules for the damping parameter $\\lambda$ is the most appropriate and standard response to this failed iteration?\n\nA. $\\lambda_{\\text{new}} = \\lambda_{\\text{old}} / \\nu$\n\nB. $\\lambda_{\\text{new}} = \\lambda_{\\text{old}} \\times \\nu$\n\nC. $\\lambda_{\\text{new}} = \\lambda_{\\text{old}} + (S(\\boldsymbol{\\beta}_{\\text{new}}) - S(\\boldsymbol{\\beta}_{\\text{old}}))$\n\nD. $\\lambda_{\\text{new}} = 0$\n\nE. $\\lambda_{\\text{new}} = \\lambda_{\\text{old}} - \\nu$", "solution": "We consider a nonlinear least squares objective $S(\\boldsymbol{\\beta})$ with residual vector $\\boldsymbol{r}(\\boldsymbol{\\beta})$ and Jacobian $J(\\boldsymbol{\\beta}) = \\partial \\boldsymbol{r} / \\partial \\boldsymbol{\\beta}^{\\top}$. In the Levenberg-Marquardt (LM) algorithm, at a current iterate $\\boldsymbol{\\beta}_{\\text{old}}$, with $J = J(\\boldsymbol{\\beta}_{\\text{old}})$ and $\\boldsymbol{r} = \\boldsymbol{r}(\\boldsymbol{\\beta}_{\\text{old}})$, the trial step $\\boldsymbol{\\delta}$ is defined by the linear system\n$$\n\\left(J^{\\top}J + \\lambda I\\right)\\boldsymbol{\\delta} = - J^{\\top}\\boldsymbol{r},\n$$\nand the trial update is $\\boldsymbol{\\beta}_{\\text{new}} = \\boldsymbol{\\beta}_{\\text{old}} + \\boldsymbol{\\delta}$.\n\nThe damping parameter $\\lambda$ controls the blend between Gauss-Newton and gradient descent:\n- As $\\lambda \\to 0$, the system becomes $J^{\\top}J \\boldsymbol{\\delta} = - J^{\\top}\\boldsymbol{r}$, i.e., the Gauss-Newton step.\n- As $\\lambda$ becomes large, using $(J^{\\top}J + \\lambda I)^{-1} \\approx \\lambda^{-1} I$ gives $\\boldsymbol{\\delta} \\approx - \\lambda^{-1} J^{\\top}\\boldsymbol{r}$, i.e., a scaled gradient descent step.\n\nWhen a trial step produces $S(\\boldsymbol{\\beta}_{\\text{new}}) \\ge S(\\boldsymbol{\\beta}_{\\text{old}})$, the step is rejected and the algorithm should move toward a more conservative, gradient-descent-like behavior by increasing the damping. A standard rule is to multiply $\\lambda$ by a factor $\\nu>1$:\n$$\n\\lambda_{\\text{new}} = \\nu \\,\\lambda_{\\text{old}}.\n$$\nThis increases damping, shrinks the next step, and biases it toward the descent direction.\n\nExamining the options:\n- A divides $\\lambda$ after a failed step, reducing damping, which is the opposite of the intended behavior.\n- B multiplies $\\lambda$ by $\\nu>1$, which is the standard LM response to a failed iteration.\n- C ties $\\lambda$ to the objective difference, which is not standard and can yield nonpositive or unstable values.\n- D sets $\\lambda=0$, reverting to Gauss-Newton, which is inappropriate immediately after a failed step.\n- E subtracts $\\nu$, which can make $\\lambda$ negative; LM requires $\\lambda \\ge 0$.\n\nTherefore, the appropriate update rule is $\\lambda_{\\text{new}} = \\lambda_{\\text{old}} \\times \\nu$.", "answer": "$$\\boxed{B}$$", "id": "2216991"}]}