{"hands_on_practices": [{"introduction": "The foundation of any least-squares fitting procedure is the concept of the residual, which measures the discrepancy between your model's prediction and the observed data. Before we can minimize the sum of squared residuals, we must first be able to calculate them accurately for a given set of parameters. This introductory exercise [@problem_id:2214281] provides essential practice in computing the residual vector, a crucial first step in setting up the Gauss-Newton method.", "problem": "In the context of non-linear least squares fitting, a common task is to minimize the sum of the squares of residuals. Consider a model that describes a certain physical phenomenon, given by the function $f(x, \\beta) = \\beta_1 \\sqrt{x} + \\beta_2$, where $\\beta = (\\beta_1, \\beta_2)^T$ is the vector of parameters to be determined.\n\nAn experiment yields two data points $(x_i, y_i)$: the first point is $(4, 5)$ and the second point is $(9, 7)$.\n\nThe residual for the $i$-th data point is defined as $r_i(\\beta) = y_i - f(x_i, \\beta)$. The residual vector $r(\\beta)$ is a column vector whose components are the individual residuals $r_i(\\beta)$.\n\nGiven an initial estimate for the parameters $\\beta^{(0)} = (1, 3)^T$, calculate the corresponding residual vector $r(\\beta^{(0)})$. Present your final answer as a row matrix with two elements, corresponding to the components of the residual vector.", "solution": "We are given the model $f(x,\\beta)=\\beta_{1}\\sqrt{x}+\\beta_{2}$, residuals $r_{i}(\\beta)=y_{i}-f(x_{i},\\beta)$, data points $(x_{1},y_{1})=(4,5)$ and $(x_{2},y_{2})=(9,7)$, and the initial estimate $\\beta^{(0)}=(\\beta_{1}^{(0)},\\beta_{2}^{(0)})^{T}=(1,3)^{T}$.\n\nCompute the model predictions at the given $x_{i}$ using $\\beta^{(0)}$:\n$$\nf(x_{1},\\beta^{(0)})=\\beta_{1}^{(0)}\\sqrt{x_{1}}+\\beta_{2}^{(0)}=1\\cdot\\sqrt{4}+3=2+3=5,\n$$\n$$\nf(x_{2},\\beta^{(0)})=\\beta_{1}^{(0)}\\sqrt{x_{2}}+\\beta_{2}^{(0)}=1\\cdot\\sqrt{9}+3=3+3=6.\n$$\n\nCompute the residuals:\n$$\nr_{1}(\\beta^{(0)})=y_{1}-f(x_{1},\\beta^{(0)})=5-5=0,\n$$\n$$\nr_{2}(\\beta^{(0)})=y_{2}-f(x_{2},\\beta^{(0)})=7-6=1.\n$$\n\nThus the residual vector has components $0$ and $1$. Presented as a row matrix with two elements, it is $\\begin{pmatrix}0 & 1\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}0 & 1\\end{pmatrix}}$$", "id": "2214281"}, {"introduction": "The Gauss-Newton method iteratively improves parameter estimates by approximating the non-linear model with a linear one at each step. This practice [@problem_id:2214282] takes you through the mechanics of a single, complete iteration for a one-parameter model. By calculating the Jacobian, the residual vector, and solving for the parameter update, you will gain a concrete understanding of how the algorithm works to refine a solution.", "problem": "In an experimental study, a certain physical process is modeled by the function $y(x) = \\frac{x}{1+ax}$, where $a$ is an unknown parameter to be determined. A researcher has collected two data points $(x_i, y_i)$: the first point is $(1, 0.5)$ and the second point is $(2, 0.8)$.\n\nTo find the optimal value of the parameter $a$ that best fits the data in a least-squares sense, the researcher decides to use the Gauss-Newton method. Starting with an initial guess of $a_0 = 1$, perform exactly one iteration of the Gauss-Newton method to find the updated estimate for the parameter, denoted as $a_1$.\n\nExpress your answer for $a_1$ as an exact fraction in simplest form.", "solution": "Let the model be $f(x;a)=\\dfrac{x}{1+a x}$. Following the convention in the main text, the residuals are $r_{i}(a)=y_{i}-f(x_{i};a)$. The Gauss-Newton update for a single parameter $a$ from $a_{0}$ is\n$$\n\\Delta a=-(J^{\\top}J)^{-1}J^{\\top}r,\n$$\nwhere the Jacobian components are $J_{i}=\\dfrac{\\partial r_{i}}{\\partial a}=-\\dfrac{\\partial f(x_{i};a)}{\\partial a}$ evaluated at $a_{0}$, and $r$ is the residual vector evaluated at $a_{0}$. Then $a_{1}=a_{0}+\\Delta a$.\n\nFirst, compute the derivative of the model with respect to $a$. Writing $f(x;a)=x(1+a x)^{-1}$, we obtain\n$$\n\\frac{\\partial f}{\\partial a}=-x^{2}(1+a x)^{-2}.\n$$\n\nWith data points $(x_{1},y_{1})=(1,\\tfrac{1}{2})$ and $(x_{2},y_{2})=(2,\\tfrac{4}{5})$, and initial guess $a_{0}=1$, the Jacobian entries are $J_i = -(\\frac{\\partial f}{\\partial a}) = \\frac{x_i^2}{(1+ax_i)^2}$:\n$$\nJ_{1}=\\left.\\frac{x_{1}^{2}}{(1+a x_{1})^{2}}\\right|_{a=1}=\\frac{1}{(1+1)^{2}}=\\frac{1}{4},\\quad\nJ_{2}=\\left.\\frac{x_{2}^{2}}{(1+a x_{2})^{2}}\\right|_{a=1}=\\frac{4}{(1+2)^{2}}=\\frac{4}{9}.\n$$\n\nThe residuals at $a_{0}=1$ are\n$$\nr_{1}=y_1-f(1;1)=\\frac{1}{2}-\\frac{1}{2}=0,\\quad\nr_{2}=y_2-f(2;1)=\\frac{4}{5}-\\frac{2}{3}=\\frac{12-10}{15}=\\frac{2}{15}.\n$$\n\nCompute the scalar quantities $J^{\\top}r$ and $J^{\\top}J$:\n$$\nJ^{\\top}r=J_{1}r_{1}+J_{2}r_{2}=0+\\left(\\frac{4}{9}\\right)\\left(\\frac{2}{15}\\right)=\\frac{8}{135},\n$$\n$$\nJ^{\\top}J=J_{1}^{2}+J_{2}^{2}=\\left(\\frac{1}{4}\\right)^{2}+\\left(\\frac{4}{9}\\right)^{2}=\\frac{1}{16}+\\frac{16}{81}=\\frac{81+256}{1296}=\\frac{337}{1296}.\n$$\n\nThus,\n$$\n\\Delta a=-\\frac{J^{\\top}r}{J^{\\top}J}=-\\frac{\\frac{8}{135}}{\\frac{337}{1296}}=-\\frac{8}{135}\\cdot\\frac{1296}{337}=-\\frac{8\\cdot 48}{5\\cdot 337}=-\\frac{384}{1685}.\n$$\n\nTherefore, the updated estimate is\n$$\na_{1}=a_{0}+\\Delta a=1-\\frac{384}{1685}=\\frac{1685-384}{1685}=\\frac{1301}{1685}.\n$$", "answer": "$$\\boxed{\\frac{1301}{1685}}$$", "id": "2214282"}, {"introduction": "While powerful, the Gauss-Newton method is based on a linear approximation and is not guaranteed to converge, especially if the initial guess is poor or the problem is highly non-linear. This exercise [@problem_id:2214263] presents a fascinating thought experiment where the method fails to find the true minimum, instead becoming trapped in a perpetual oscillation. By analyzing this failure mode, you will develop a deeper appreciation for the algorithm's limitations and the rationale behind more advanced techniques.", "problem": "An engineer is using the Gauss-Newton algorithm to solve a simple non-linear least squares problem. The goal is to fit a model $g(t; x) = A \\arctan(x t)$ to a single data point $(t_1, y_1) = (1, 0)$. The parameter to be determined is $x$, and the constant $A$ is fixed at $A=1$. The objective is to find the value of $x$ that minimizes the sum of squared residuals, defined as $F(x) = \\frac{1}{2} [g(t_1; x) - y_1]^2$. The true minimum for this problem is clearly at $x = 0$.\n\nHowever, when the engineer applies the Gauss-Newton method starting from a specific initial guess $x_0 > 0$, they observe that the algorithm fails to converge to the minimum. Instead, the iterates become trapped in a stable 2-cycle, oscillating perpetually between the initial guess $x_0$ and its negative, $-x_0$.\n\nDetermine the value of this specific initial guess $x_0$ that causes the Gauss-Newton method to immediately enter this 2-cycle. Express your answer as a number rounded to four significant figures.", "solution": "The model is $g(t; x) = \\arctan(x t)$ with $A=1$, and the single datum is $(t_{1}, y_{1}) = (1, 0)$. The residual is therefore\n$$\nr(x) = g(1; x) - y_{1} = \\arctan(x).\n$$\nThe scalar Gauss-Newton step for minimizing $F(x) = \\frac{1}{2} r(x)^{2}$ solves the linearized least squares $r(x) + J(x) p \\approx 0$, where $J(x) = \\frac{dr}{dx}$. Hence\n$$\np = -\\frac{r(x)}{J(x)}, \\quad x_{+} = x + p = x - \\frac{r(x)}{J(x)}.\n$$\nHere $J(x) = \\frac{d}{dx}\\arctan(x) = \\frac{1}{1 + x^{2}}$, so the Gauss-Newton iteration map is\n$$\nT(x) = x - \\frac{\\arctan(x)}{\\frac{1}{1 + x^{2}}} = x - (1 + x^{2}) \\arctan(x).\n$$\nA 2-cycle $\\{x_{0}, -x_{0}\\}$ occurs immediately when $T(x_{0}) = -x_{0}$, which gives\n$$\n-x_{0} = x_{0} - (1 + x_{0}^{2}) \\arctan(x_{0}) \\;\\;\\Longrightarrow\\;\\; (1 + x_{0}^{2}) \\arctan(x_{0}) = 2 x_{0}.\n$$\nLet $x_{0} = \\tan(\\theta)$ with $\\theta \\in (0, \\frac{\\pi}{2})$. Using $1 + \\tan^{2}(\\theta) = \\sec^{2}(\\theta)$ and $\\arctan(\\tan(\\theta)) = \\theta$, the equation becomes\n$$\n\\sec^{2}(\\theta)\\,\\theta = 2 \\tan(\\theta) \\;\\;\\Longrightarrow\\;\\; \\theta = 2 \\sin(\\theta)\\cos(\\theta) = \\sin(2\\theta).\n$$\nBesides the trivial solution $\\theta = 0$ (which corresponds to $x=0$), the unique nonzero solution in $(0, \\frac{\\pi}{2})$ satisfies $\\theta = \\sin(2\\theta)$. Solving numerically yields\n$$\n\\theta \\approx 0.947745,\n$$\nso\n$$\nx_{0} = \\tan(\\theta) \\approx 1.391740\\ldots\n$$\nRounded to four significant figures, this gives the required initial guess.", "answer": "$$\\boxed{1.392}$$", "id": "2214263"}]}