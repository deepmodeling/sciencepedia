{"hands_on_practices": [{"introduction": "Let's begin with a foundational exercise that puts the core principle of Monte Carlo integration into practice. This problem demonstrates how to estimate a multi-dimensional integral, a key area where these methods outperform traditional numerical techniques. By working through this example [@problem_id:2188188], you will calculate a Monte Carlo estimate from a given set of random samples, solidifying your understanding of the basic estimation formula $I \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(\\mathbf{x}_i)$.", "problem": "A novel parallel computing architecture consists of four independent processing units. For a specific class of problems, the overall performance of the system is determined by the output of the fastest unit. The outputs of the units, when normalized, can be modeled as independent random variables $x_1, x_2, x_3, x_4$, each uniformly distributed over the interval $[0, 1]$. The expected system performance is therefore given by the integral $I$:\n\n$$\nI = \\int_0^1 \\int_0^1 \\int_0^1 \\int_0^1 \\max(x_1, x_2, x_3, x_4) dx_1 dx_2 dx_3 dx_4\n$$\n\nAn engineer runs a simulation to estimate this value and obtains the following five 4-dimensional sample points, with each coordinate drawn from the uniform distribution on $[0, 1]$:\n*   $P_1 = (0.12, 0.83, 0.55, 0.41)$\n*   $P_2 = (0.91, 0.22, 0.67, 0.35)$\n*   $P_3 = (0.48, 0.76, 0.09, 0.88)$\n*   $P_4 = (0.62, 0.59, 0.95, 0.71)$\n*   $P_5 = (0.33, 0.81, 0.24, 0.50)$\n\nUsing only this set of five sample points, calculate the Monte Carlo estimate for the expected system performance, $I$. Round your final answer to three significant figures.", "solution": "The integral $I$ is the expected value of $f(x_{1},x_{2},x_{3},x_{4})=\\max(x_{1},x_{2},x_{3},x_{4})$ when $(x_{1},x_{2},x_{3},x_{4})$ is uniformly distributed over $[0,1]^{4}$. A Monte Carlo estimator for $I$ using $N$ independent samples $P_{k}=(x_{1}^{(k)},x_{2}^{(k)},x_{3}^{(k)},x_{4}^{(k)})$ is\n$$\n\\hat{I}_{N}=\\frac{1}{N}\\sum_{k=1}^{N}\\max\\bigl(x_{1}^{(k)},x_{2}^{(k)},x_{3}^{(k)},x_{4}^{(k)}\\bigr).\n$$\nWith the given five samples, $N=5$. Compute the maxima for each sample:\n$$\n\\max(P_{1})=\\max(0.12,0.83,0.55,0.41)=0.83,\n$$\n$$\n\\max(P_{2})=\\max(0.91,0.22,0.67,0.35)=0.91,\n$$\n$$\n\\max(P_{3})=\\max(0.48,0.76,0.09,0.88)=0.88,\n$$\n$$\n\\max(P_{4})=\\max(0.62,0.59,0.95,0.71)=0.95,\n$$\n$$\n\\max(P_{5})=\\max(0.33,0.81,0.24,0.50)=0.81.\n$$\nSum and average:\n$$\n\\hat{I}_{5}=\\frac{0.83+0.91+0.88+0.95+0.81}{5}=\\frac{4.38}{5}=0.876.\n$$\nRounded to three significant figures, the Monte Carlo estimate is $0.876$.", "answer": "$$\\boxed{0.876}$$", "id": "2188188"}, {"introduction": "While the basic Monte Carlo method is powerful, its efficiency can often be dramatically improved using variance reduction techniques. This practice introduces antithetic variates, a strategy that is particularly effective for monotonic functions by introducing a negative correlation between samples. You will analytically compare the variance of a standard estimator with an antithetic variates estimator, quantifying the significant gain in precision this clever sampling strategy can provide [@problem_id:2188199].", "problem": "In computational science, Monte Carlo methods are frequently used to approximate the value of a definite integral, $I = \\int_{0}^{1} f(x) \\,dx$. A key aspect of these methods is the reduction of the estimator's variance to improve accuracy.\n\nConsider the integral of the function $f(x) = (1+x)^2$ over the interval $[0, 1]$. We will compare two Monte Carlo estimators for this integral, both based on a total of $N$ function evaluations, where $N$ is an even integer.\n\n1.  **Standard Monte Carlo Estimator ($\\hat{I}_{\\text{std}}$):**\n    This estimator is formed by drawing $N$ independent and identically distributed random numbers $X_1, X_2, \\ldots, X_N$ from a Uniform(0,1) distribution. The estimator is given by:\n    $$ \\hat{I}_{\\text{std}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i) $$\n\n2.  **Antithetic Variates Estimator ($\\hat{I}_{\\text{anti}}$):**\n    This estimator is formed by drawing $M = N/2$ independent random numbers $U_1, U_2, \\ldots, U_M$ from a Uniform(0,1) distribution. For each $U_i$, we form a pair of 'antithetic' samples, $(U_i, 1-U_i)$. The estimator is given by:\n    $$ \\hat{I}_{\\text{anti}} = \\frac{1}{M} \\sum_{i=1}^{M} \\frac{f(U_i) + f(1-U_i)}{2} $$\n\nYour task is to analytically determine the theoretical variance reduction achieved by using antithetic variates for this specific function. Calculate the exact value of the ratio $R = \\frac{\\text{Var}(\\hat{I}_{\\text{anti}})}{\\text{Var}(\\hat{I}_{\\text{std}})}$.", "solution": "Let $X\\sim \\text{Uniform}(0,1)$ and $f(x)=(1+x)^{2}=1+2x+x^{2}$. The standard Monte Carlo estimator with $N$ IID samples has variance\n$$\n\\text{Var}(\\hat{I}_{\\text{std}})=\\frac{\\text{Var}(f(X))}{N}.\n$$\nCompute $\\text{Var}(f(X))$ by evaluating $E[f(X)]$ and $E[f(X)^{2}]$. Using $E[X]=\\frac{1}{2}$, $E[X^{2}]=\\frac{1}{3}$, $E[X^{3}]=\\frac{1}{4}$, $E[X^{4}]=\\frac{1}{5}$,\n$$\nE[f(X)]=E[1+2X+X^{2}]=1+2\\cdot \\frac{1}{2}+\\frac{1}{3}=\\frac{7}{3},\n$$\nand\n$$\nf(X)^{2}=(1+2X+X^{2})^{2}=1+4X+6X^{2}+4X^{3}+X^{4},\n$$\nso\n$$\nE[f(X)^{2}]=1+4\\cdot \\frac{1}{2}+6\\cdot \\frac{1}{3}+4\\cdot \\frac{1}{4}+\\frac{1}{5}=\\frac{31}{5}.\n$$\nThus\n$$\n\\text{Var}(f(X))=E[f(X)^{2}]-\\left(E[f(X)]\\right)^{2}=\\frac{31}{5}-\\frac{49}{9}=\\frac{34}{45}.\n$$\n\nFor the antithetic estimator, define $M=\\frac{N}{2}$ and $Y=\\frac{f(U)+f(1-U)}{2}$ with $U\\sim \\text{Uniform}(0,1)$. Then\n$$\n\\text{Var}(\\hat{I}_{\\text{anti}})=\\frac{\\text{Var}(Y)}{M}.\n$$\nUse\n$$\n\\text{Var}(Y)=\\frac{1}{4}\\text{Var}\\big(f(U)+f(1-U)\\big)=\\frac{1}{4}\\left(2\\,\\text{Var}(f(X))+2\\,\\text{Cov}(f(U),f(1-U))\\right)\n=\\frac{1}{2}\\left(\\text{Var}(f(X))+\\text{Cov}(f(U),f(1-U))\\right).\n$$\nTherefore the variance ratio is\n$$\nR=\\frac{\\text{Var}(\\hat{I}_{\\text{anti}})}{\\text{Var}(\\hat{I}_{\\text{std}})}\n=\\frac{\\frac{\\text{Var}(Y)}{M}}{\\frac{\\text{Var}(f(X))}{N}}\n=\\frac{\\text{Var}(Y)}{\\text{Var}(f(X))}\\cdot \\frac{N}{M}\n=\\frac{\\frac{1}{2}\\left(\\text{Var}(f(X))+\\text{Cov}(f(U),f(1-U))\\right)}{\\text{Var}(f(X))}\\cdot 2\n=1+\\frac{\\text{Cov}(f(U),f(1-U))}{\\text{Var}(f(X))}.\n$$\n\nIt remains to compute $\\text{Cov}(f(U),f(1-U))$. Since $f(1-U)=(2-U)^{2}=4-4U+U^{2}$,\n$$\nf(U)f(1-U)=(1+2U+U^{2})(4-4U+U^{2})=4+4U-3U^{2}-2U^{3}+U^{4}.\n$$\nHence\n$$\nE[f(U)f(1-U)]=4+4\\cdot \\frac{1}{2}-3\\cdot \\frac{1}{3}-2\\cdot \\frac{1}{4}+\\frac{1}{5}=\\frac{47}{10},\n$$\nand with $E[f(U)]=\\frac{7}{3}$,\n$$\n\\text{Cov}(f(U),f(1-U))=E[f(U)f(1-U)]-\\left(E[f(U)]\\right)^{2}=\\frac{47}{10}-\\frac{49}{9}=-\\frac{67}{90}.\n$$\nTherefore,\n$$\nR=1+\\frac{-\\frac{67}{90}}{\\frac{34}{45}}=1-\\frac{67}{90}\\cdot \\frac{45}{34}=1-\\frac{67}{68}=\\frac{1}{68}.\n$$\nThus the antithetic variates estimator achieves a variance that is a factor $\\frac{1}{68}$ of the standard Monte Carlo estimator for this $f$.", "answer": "$$\\boxed{\\frac{1}{68}}$$", "id": "2188199"}, {"introduction": "A naive application of Monte Carlo integration can sometimes fail, especially when dealing with improper integrals where the integrand has a singularity. This exercise explores such a case, where the standard estimator has an infinite variance and thus fails to converge reliably according to the Central Limit Theorem. You will discover how importance sampling [@problem_id:2188184], an advanced method, resolves this by concentrating samples in the most 'important' regions of the domain, yielding a well-behaved estimator with finite variance.", "problem": "A numerical analyst is tasked with estimating the value of the improper integral $I = \\int_0^1 x^{-2/3} dx$ using Monte Carlo methods. Two different approaches are considered.\n\nLet the integrand be denoted by $f(x) = x^{-2/3}$.\n\n**Method 1: Naive Monte Carlo**\nThis method involves generating $N$ independent random samples $U_1, U_2, \\dots, U_N$ from the standard uniform distribution on $(0, 1)$, denoted $U \\sim \\text{Uniform}(0,1)$. The integral is then estimated using the sample mean:\n$$ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N f(U_i) $$\nThe theoretical variance of a single sample for this estimator is $V_N = \\text{Var}(f(U))$.\n\n**Method 2: Importance Sampling**\nThis method involves generating $N$ independent random samples $Y_1, Y_2, \\dots, Y_N$ from a non-uniform probability distribution with a probability density function (PDF) $p(x)$ defined on $(0,1]$. The chosen PDF is $p(x) = \\frac{1}{2\\sqrt{x}}$. The integral is then estimated using the weighted sample mean:\n$$ \\hat{I}_{IS} = \\frac{1}{N} \\sum_{i=1}^N \\frac{f(Y_i)}{p(Y_i)} $$\nThe theoretical variance of a single sample for this estimator is $V_{IS} = \\text{Var}\\left(\\frac{f(Y)}{p(Y)}\\right)$.\n\nYour task is to analyze the theoretical statistical properties of the estimators $\\hat{I}_N$ and $\\hat{I}_{IS}$, specifically their variances. Based on your analysis, which of the following statements is correct?\n\nA. Both $V_N$ and $V_{IS}$ are finite, but $V_N > V_{IS}$, indicating importance sampling is more efficient.\n\nB. $V_N$ is finite, but $V_{IS}$ is infinite, indicating that the chosen importance sampling distribution $p(x)$ is unsuitable.\n\nC. The integral $I$ is divergent, so both Monte Carlo methods will fail to produce a meaningful estimate.\n\nD. $V_N$ is infinite, while $V_{IS}$ is finite. This implies that the naive Monte Carlo estimator does not satisfy the conditions of the Central Limit Theorem, whereas the importance sampling estimator does.\n\nE. Both estimators are biased, meaning their expected values are not equal to $I$, but the importance sampling estimator has a smaller bias.", "solution": "We first evaluate the improper integral to confirm finiteness:\n$$\nI=\\int_{0}^{1} x^{-2/3}\\,dx=\\left[\\frac{x^{1/3}}{1/3}\\right]_{0}^{1}=3,\n$$\nso the integral converges.\n\nNaive Monte Carlo with $U\\sim\\text{Uniform}(0,1)$ uses $f(U)=U^{-2/3}$. Its mean is\n$$\n\\mathbb{E}[f(U)]=\\int_{0}^{1} x^{-2/3}\\,dx=3,\n$$\nso the estimator is unbiased. Its variance is\n$$\nV_{N}=\\operatorname{Var}(f(U))=\\mathbb{E}[f(U)^{2}]-\\left(\\mathbb{E}[f(U)]\\right)^{2}.\n$$\nCompute the second moment:\n$$\n\\mathbb{E}[f(U)^{2}]=\\int_{0}^{1} x^{-4/3}\\,dx=\\lim_{\\varepsilon\\to 0^{+}}\\int_{\\varepsilon}^{1} x^{-4/3}\\,dx\n=\\lim_{\\varepsilon\\to 0^{+}}\\left[-3 x^{-1/3}\\right]_{\\varepsilon}^{1}\n=\\lim_{\\varepsilon\\to 0^{+}}\\left(-3+3\\varepsilon^{-1/3}\\right)=\\infty.\n$$\nHence $V_{N}=\\infty$.\n\nFor importance sampling, the proposal has density $p(x)=\\frac{1}{2\\sqrt{x}}$ on $(0,1]$, which is a valid pdf since\n$$\n\\int_{0}^{1} p(x)\\,dx=\\int_{0}^{1} \\frac{1}{2\\sqrt{x}}\\,dx=\\left[\\sqrt{x}\\right]_{0}^{1}=1.\n$$\nLet $Y\\sim p$. The single-sample IS weight is\n$$\nW=\\frac{f(Y)}{p(Y)}=\\frac{Y^{-2/3}}{1/(2\\sqrt{Y})}=2\\,Y^{-1/6}.\n$$\nIts mean is\n$$\n\\mathbb{E}[W]=\\int_{0}^{1} \\frac{f(x)}{p(x)}\\,p(x)\\,dx=\\int_{0}^{1} f(x)\\,dx=I=3,\n$$\nso the IS estimator is unbiased. Its variance is\n$$\nV_{IS}=\\operatorname{Var}(W)=\\mathbb{E}[W^{2}]-\\left(\\mathbb{E}[W]\\right)^{2}.\n$$\nCompute the second moment:\n$$\n\\mathbb{E}[W^{2}]=\\int_{0}^{1} \\left(\\frac{f(x)}{p(x)}\\right)^{2} p(x)\\,dx\n=\\int_{0}^{1} \\frac{f(x)^{2}}{p(x)}\\,dx\n=\\int_{0}^{1} \\frac{x^{-4/3}}{1/(2\\sqrt{x})}\\,dx\n=\\int_{0}^{1} 2\\,x^{-5/6}\\,dx\n=2\\left[\\frac{x^{1/6}}{1/6}\\right]_{0}^{1}=12.\n$$\nTherefore,\n$$\nV_{IS}=12-3^{2}=3<\\infty.\n$$\n\nConclusions:\n- $I$ is finite, so C is false.\n- Naive MC has infinite variance, so A and B are false.\n- Both estimators are unbiased, so E is false.\n- $V_{N}=\\infty$ while $V_{IS}=3<\\infty$; thus the naive estimator fails the usual Central Limit Theorem conditions (finite variance), whereas the IS estimator satisfies them.\n\nThe correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "2188184"}]}