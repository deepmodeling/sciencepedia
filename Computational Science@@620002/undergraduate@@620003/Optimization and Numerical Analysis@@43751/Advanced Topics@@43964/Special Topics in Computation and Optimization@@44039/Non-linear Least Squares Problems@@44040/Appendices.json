{"hands_on_practices": [{"introduction": "The journey into solving any optimization problem begins with a clear understanding of the objective functionâ€”the very quantity we aim to minimize. In non-linear least squares, this is the sum of squared errors, $S(\\beta)$, which quantifies the total discrepancy between our model's predictions and the observed data. This foundational exercise [@problem_id:2214255] provides direct practice in calculating this value, a critical first step before any iterative algorithm can be applied to find the optimal parameters $\\beta$.", "problem": "In the field of data analysis and numerical optimization, non-linear least squares is a fundamental technique used to fit a model to a set of observed data points. The goal is to find the model parameters that minimize the sum of the squared differences between the observed data and the values predicted by the model.\n\nConsider a model intended to describe a saturation process, given by the function:\n$$f(x, \\beta) = \\frac{\\beta_1}{1 + x^{\\beta_2}}$$\nwhere $\\beta = (\\beta_1, \\beta_2)$ is a vector of the model's parameters.\n\nSuppose an experiment yields two data points: $(x_1, y_1) = (1, 1)$ and $(x_2, y_2) = (2, 0.5)$.\n\nThe quality of the fit for a particular choice of parameters is quantified by the sum-of-squared-errors objective function, $S(\\beta)$, defined as the sum of the squared residuals:\n$$S(\\beta) = \\sum_{i=1}^{2} (y_i - f(x_i, \\beta))^2$$\n\nAs a first step in an optimization procedure like the Gauss-Newton method, one must evaluate the objective function at an initial guess for the parameters. Calculate the value of $S(\\beta)$ for the specific parameter vector $\\beta = (2, 1)$.\n\nExpress your final answer as an exact fraction in its simplest form.", "solution": "We are given $f(x,\\beta)=\\dfrac{\\beta_{1}}{1+x^{\\beta_{2}}}$ and $S(\\beta)=\\sum_{i=1}^{2}\\left(y_{i}-f(x_{i},\\beta)\\right)^{2}$ with data $(x_{1},y_{1})=(1,1)$ and $(x_{2},y_{2})=(2,\\dfrac{1}{2})$. For $\\beta=(2,1)$, the model evaluates to\n$$\nf(x,(2,1))=\\frac{2}{1+x^{1}}=\\frac{2}{1+x}.\n$$\nAt $x_{1}=1$, we have\n$$\nf(1,(2,1))=\\frac{2}{1+1}=\\frac{2}{2}=1,\n$$\nso the residual is $r_{1}=y_{1}-f(1,(2,1))=1-1=0$, giving $r_{1}^{2}=0$.\nAt $x_{2}=2$, we have\n$$\nf(2,(2,1))=\\frac{2}{1+2}=\\frac{2}{3},\n$$\nso the residual is\n$$\nr_{2}=y_{2}-f(2,(2,1))=\\frac{1}{2}-\\frac{2}{3}=\\frac{3-4}{6}=-\\frac{1}{6},\n$$\ngiving $r_{2}^{2}=\\left(-\\frac{1}{6}\\right)^{2}=\\frac{1}{36}$.\nTherefore,\n$$\nS(\\beta)=r_{1}^{2}+r_{2}^{2}=0+\\frac{1}{36}=\\frac{1}{36}.\n$$", "answer": "$$\\boxed{\\frac{1}{36}}$$", "id": "2214255"}, {"introduction": "With an understanding of how to measure error, we can now explore *how* to minimize it using the Gauss-Newton algorithm. This method's efficiency stems from a clever approximation of the error surface's curvature (the Hessian matrix), replacing it with the simpler term $J^T J$, which avoids computing complex second derivatives. This insightful exercise [@problem_id:2214286] isolates the core of this approximation, asking you to directly compare the Gauss-Newton Hessian to the true Hessian to understand the method's mechanics and assumptions.", "problem": "Consider a single-parameter nonlinear model used to describe a growth process, given by the function $f(x; \\beta) = \\exp(\\beta x)$, where $\\beta$ is the model parameter. We want to fit this model to a single experimental data point $(x_1, y_1) = (1, 3)$.\n\nThe quality of the fit for a given $\\beta$ is measured by the least-squares objective function, which for a single data point is the squared residual:\n$$ S(\\beta) = (f(x_1; \\beta) - y_1)^2 $$\n\nIn optimization, the second derivative (Hessian) of the objective function provides information about the curvature of the error surface. The true Hessian of $S(\\beta)$ is the scalar $H = \\frac{d^2S}{d\\beta^2}$. The Gauss-Newton algorithm, a popular method for solving nonlinear least squares problems, uses an approximation for the Hessian given by $H_{GN} = J^T J$, where $J$ is the Jacobian of the residual function $r(\\beta) = f(x_1; \\beta) - y_1$. Since there is only one parameter and one data point, the Jacobian $J$ is a $1 \\times 1$ matrix.\n\nCalculate the exact value of the discrepancy between the true Hessian and the Gauss-Newton approximation, $\\Delta H = H - H_{GN}$, evaluated at the specific parameter value $\\beta=1$.", "solution": "We have a single-parameter model $f(x;\\beta)=\\exp(\\beta x)$ and one data point $(x_{1},y_{1})=(1,3)$. The residual is defined by $r(\\beta)=f(x_{1};\\beta)-y_{1}=\\exp(\\beta)-3$. The least-squares objective for one data point is $S(\\beta)=r(\\beta)^{2}$.\n\nTo compute the true Hessian, differentiate $S(\\beta)=r(\\beta)^{2}$. Using the chain rule,\n$$\n\\frac{dS}{d\\beta}=2\\,r(\\beta)\\,r'(\\beta),\n$$\nand differentiating again gives\n$$\nH=\\frac{d^{2}S}{d\\beta^{2}}=2\\left[(r'(\\beta))^{2}+r(\\beta)\\,r''(\\beta)\\right].\n$$\nFor $r(\\beta)=\\exp(\\beta)-3$, we have $r'(\\beta)=\\exp(\\beta)$ and $r''(\\beta)=\\exp(\\beta)$. Substituting,\n$$\nH=2\\left[\\exp(2\\beta)+(\\exp(\\beta)-3)\\exp(\\beta)\\right]\n=2\\left[\\exp(2\\beta)+\\exp(2\\beta)-3\\exp(\\beta)\\right]\n=4\\exp(2\\beta)-6\\exp(\\beta).\n$$\n\nThe Gauss-Newton approximation uses $H_{GN}=J^{T}J$, where $J$ is the Jacobian of $r(\\beta)$ with respect to $\\beta$. Since $J=r'(\\beta)=\\exp(\\beta)$ in this scalar case, we have\n$$\nH_{GN}=J^{T}J=(\\exp(\\beta))^{2}=\\exp(2\\beta).\n$$\n\nThe discrepancy is\n$$\n\\Delta H=H-H_{GN}=\\left(4\\exp(2\\beta)-6\\exp(\\beta)\\right)-\\exp(2\\beta)\n=3\\exp(2\\beta)-6\\exp(\\beta).\n$$\nEvaluating at $\\beta=1$ yields\n$$\n\\Delta H=3\\exp(2)-6\\exp(1)=3\\exp(1)\\left(\\exp(1)-2\\right).\n$$", "answer": "$$\\boxed{3\\exp(2)-6\\exp(1)}$$", "id": "2214286"}, {"introduction": "Now it's time to put theory into practice by tackling a realistic scenario from the field of signal processing. This problem [@problem_id:2191258] challenges you to apply the Gauss-Newton algorithm to determine the location of a sound source based on time-difference-of-arrival data, a technique fundamental to systems like GPS and underwater acoustics. By performing a full iteration, you will synthesize the concepts of residuals, Jacobians, and the normal equations to tangibly refine a position estimate and see the method in action.", "problem": "An acoustic localization system is designed to find the position of a sound source in a two-dimensional plane. The system consists of three microphones located at positions $\\mathbf{m}_1 = (0, 0)$, $\\mathbf{m}_2 = (L, 0)$, and $\\mathbf{m}_3 = (0, L)$. The source is at an unknown position $\\mathbf{p} = (x, y)$.\n\nThe system operates by measuring the Time-Difference-of-Arrival (TDOA) of a sound signal at different microphones. The time differences are measured with respect to the signal's arrival at the reference microphone $\\mathbf{m}_1$. The theoretical model for the time difference between microphone $\\mathbf{m}_i$ and $\\mathbf{m}_1$ is given by $\\Delta t_{i1}(\\mathbf{p}) = \\frac{\\|\\mathbf{p} - \\mathbf{m}_i\\| - \\|\\mathbf{p} - \\mathbf{m}_1\\|}{v_s}$, where $\\|\\cdot\\|$ denotes the Euclidean distance and $v_s$ is the speed of sound.\n\nThe position $\\mathbf{p}$ is estimated by finding the coordinates $(x, y)$ that minimize the sum of squared differences between the measured TDOAs and the model's predictions. This non-linear least squares problem is to be solved using the Gauss-Newton algorithm. The algorithm iteratively refines an estimate $\\mathbf{p}^{(k)}$ to a new estimate $\\mathbf{p}^{(k+1)}$ using the update rule:\n$$ \\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p} $$\nwhere the update step $\\Delta\\mathbf{p}$ is the solution to the linear system $(J^T J) \\Delta\\mathbf{p} = J^T \\mathbf{r}$. Here, $\\mathbf{r}$ is the vector of residuals (measured values minus model predictions) and $J$ is the Jacobian matrix of the model functions, both evaluated at the current estimate $\\mathbf{p}^{(k)}$.\n\nYou are given the following parameters:\n- Microphone configuration constant: $L = 100 \\text{ m}$\n- Speed of sound: $v_s = 340 \\text{ m/s}$\n- Measured TDOAs: $\\tau_{21} = \\frac{5(1-\\sqrt{2})}{17} \\text{ s}$ and $\\tau_{31} = \\frac{5(1-\\sqrt{2})}{17} \\text{ s}$\n- Initial guess for the source position: $\\mathbf{p}^{(0)} = (L/2, L/2)$\n\nYour task is to calculate the next position estimate $\\mathbf{p}^{(1)} = (x_1, y_1)$ by performing a single step of the Gauss-Newton algorithm. Provide the coordinates $(x_1, y_1)$ as an analytic expression.", "solution": "We model the TDOA for microphones $\\mathbf{m}_{2}$ and $\\mathbf{m}_{3}$ relative to $\\mathbf{m}_{1}$ by\n$$\nh_{i}(\\mathbf{p})=\\Delta t_{i1}(\\mathbf{p})=\\frac{\\|\\mathbf{p}-\\mathbf{m}_{i}\\|-\\|\\mathbf{p}-\\mathbf{m}_{1}\\|}{v_{s}},\\quad i\\in\\{2,3\\}.\n$$\nGiven measured TDOAs $\\tau_{21}$ and $\\tau_{31}$, the residual vector at a current estimate $\\mathbf{p}^{(k)}$ is\n$$\n\\mathbf{r}=\\begin{pmatrix}\\tau_{21}-h_{2}(\\mathbf{p}^{(k)})\\\\ \\tau_{31}-h_{3}(\\mathbf{p}^{(k)})\\end{pmatrix}.\n$$\nThe Gauss-Newton step solves $(J^{T}J)\\,\\Delta\\mathbf{p}=J^{T}\\mathbf{r}$, where $J$ is the Jacobian of $(h_{2},h_{3})$ at $\\mathbf{p}^{(k)}$. Using $\\nabla_{\\mathbf{p}}\\|\\mathbf{p}-\\mathbf{a}\\|=\\frac{\\mathbf{p}-\\mathbf{a}}{\\|\\mathbf{p}-\\mathbf{a}\\|}$ for $\\mathbf{p}\\neq\\mathbf{a}$, we obtain\n$$\n\\nabla h_{i}(\\mathbf{p})=\\frac{1}{v_{s}}\\left(\\frac{\\mathbf{p}-\\mathbf{m}_{i}}{\\|\\mathbf{p}-\\mathbf{m}_{i}\\|}-\\frac{\\mathbf{p}-\\mathbf{m}_{1}}{\\|\\mathbf{p}-\\mathbf{m}_{1}\\|}\\right),\\quad i\\in\\{2,3\\}.\n$$\n\nEvaluate at the initial guess $\\mathbf{p}^{(0)}=\\left(\\frac{L}{2},\\frac{L}{2}\\right)$ with $\\mathbf{m}_{1}=(0,0)$, $\\mathbf{m}_{2}=(L,0)$, $\\mathbf{m}_{3}=(0,L)$. Then\n$$\n\\mathbf{p}^{(0)}-\\mathbf{m}_{1}=\\left(\\frac{L}{2},\\frac{L}{2}\\right),\\quad\n\\mathbf{p}^{(0)}-\\mathbf{m}_{2}=\\left(-\\frac{L}{2},\\frac{L}{2}\\right),\\quad\n\\mathbf{p}^{(0)}-\\mathbf{m}_{3}=\\left(\\frac{L}{2},-\\frac{L}{2}\\right),\n$$\nand their norms are all equal:\n$$\n\\left\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{1}\\right\\|=\\left\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{2}\\right\\|=\\left\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{3}\\right\\|=\\frac{L}{\\sqrt{2}}.\n$$\nTherefore the unit vectors are\n$$\n\\frac{\\mathbf{p}^{(0)}-\\mathbf{m}_{1}}{\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{1}\\|}=\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right),\\quad\n\\frac{\\mathbf{p}^{(0)}-\\mathbf{m}_{2}}{\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{2}\\|}=\\left(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right),\\quad\n\\frac{\\mathbf{p}^{(0)}-\\mathbf{m}_{3}}{\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{3}\\|}=\\left(\\frac{1}{\\sqrt{2}},-\\frac{1}{\\sqrt{2}}\\right).\n$$\nHence the Jacobian rows at $\\mathbf{p}^{(0)}$ are\n$$\n\\nabla h_{2}(\\mathbf{p}^{(0)})=\\frac{1}{v_{s}}\\left(\\left(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right)-\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right)\\right)=\\left(-\\frac{\\sqrt{2}}{v_{s}},\\,0\\right),\n$$\n$$\n\\nabla h_{3}(\\mathbf{p}^{(0)})=\\frac{1}{v_{s}}\\left(\\left(\\frac{1}{\\sqrt{2}},-\\frac{1}{\\sqrt{2}}\\right)-\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right)\\right)=\\left(0,\\,-\\frac{\\sqrt{2}}{v_{s}}\\right).\n$$\nThus\n$$\nJ=\\begin{pmatrix}-\\frac{\\sqrt{2}}{v_{s}} & 0\\\\ 0 & -\\frac{\\sqrt{2}}{v_{s}}\\end{pmatrix},\\qquad\nJ^{T}J=\\begin{pmatrix}\\frac{2}{v_{s}^{2}} & 0\\\\ 0 & \\frac{2}{v_{s}^{2}}\\end{pmatrix},\\qquad\n(J^{T}J)^{-1}=\\begin{pmatrix}\\frac{v_{s}^{2}}{2} & 0\\\\ 0 & \\frac{v_{s}^{2}}{2}\\end{pmatrix}.\n$$\n\nAt $\\mathbf{p}^{(0)}$, the model predictions are $h_{2}(\\mathbf{p}^{(0)})=h_{3}(\\mathbf{p}^{(0)})=0$ (equal distances), so with $\\tau_{21}=\\tau_{31}=\\tau$,\n$$\n\\mathbf{r}=\\begin{pmatrix}\\tau\\\\ \\tau\\end{pmatrix},\\qquad\nJ^{T}\\mathbf{r}=\\begin{pmatrix}-\\frac{\\sqrt{2}}{v_{s}}\\,\\tau\\\\ -\\frac{\\sqrt{2}}{v_{s}}\\,\\tau\\end{pmatrix}.\n$$\nTherefore the Gauss-Newton step is\n$$\n\\Delta\\mathbf{p}=(J^{T}J)^{-1}J^{T}\\mathbf{r}=\\begin{pmatrix}-\\frac{v_{s}\\,\\tau}{\\sqrt{2}}\\\\ -\\frac{v_{s}\\,\\tau}{\\sqrt{2}}\\end{pmatrix}.\n$$\nThe update yields\n$$\n\\mathbf{p}^{(1)}=\\mathbf{p}^{(0)}+\\Delta\\mathbf{p}=\\left(\\frac{L}{2}-\\frac{v_{s}\\,\\tau}{\\sqrt{2}},\\,\\frac{L}{2}-\\frac{v_{s}\\,\\tau}{\\sqrt{2}}\\right).\n$$\n\nWith the given values $L=100$, $v_{s}=340$, and $\\tau=\\frac{5(1-\\sqrt{2})}{17}$,\n$$\n-\\frac{v_{s}\\,\\tau}{\\sqrt{2}}=-\\frac{340\\cdot \\frac{5(1-\\sqrt{2})}{17}}{\\sqrt{2}}=-\\frac{100(1-\\sqrt{2})}{\\sqrt{2}}=100\\left(1-\\frac{1}{\\sqrt{2}}\\right),\n$$\nso\n$$\n\\mathbf{p}^{(1)}=\\left(\\frac{L}{2}+100\\left(1-\\frac{1}{\\sqrt{2}}\\right),\\,\\frac{L}{2}+100\\left(1-\\frac{1}{\\sqrt{2}}\\right)\\right)=\\left(150-\\frac{100}{\\sqrt{2}},\\,150-\\frac{100}{\\sqrt{2}}\\right).\n$$", "answer": "$$\\boxed{(150 - \\frac{100}{\\sqrt{2}}, 150 - \\frac{100}{\\sqrt{2}})}$$", "id": "2191258"}]}