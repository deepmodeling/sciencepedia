{"hands_on_practices": [{"introduction": "Before we can use the Hessian matrix to unlock insights about a function's behavior, we must first master the fundamental skill of computing it. This first practice is a straightforward but essential exercise in multivariable differentiation. By working through the calculation for a polynomial function of three variables, you will solidify your understanding of how the Hessian is constructed from second-order partial derivatives [@problem_id:2215330].", "problem": "In multivariable calculus and optimization, the Hessian matrix is a square matrix composed of the second-order partial derivatives of a scalar-valued function. It provides crucial information about the function's local curvature.\n\nConsider a scalar-valued function $f$ of three variables $x$, $y$, and $z$, defined as:\n$$f(x, y, z) = 3x^{2}z - y^{3} + 2xyz^{2} + 15$$\nYour task is to compute the Hessian matrix of this function. The elements of the Hessian matrix are defined as $H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$, where the variables are indexed as $(x_1, x_2, x_3) = (x, y, z)$. Present your answer as a 3x3 matrix.", "solution": "We seek the Hessian matrix $H$ of $f(x,y,z)=3x^{2}z-y^{3}+2xyz^{2}+15$, with variables ordered as $(x_{1},x_{2},x_{3})=(x,y,z)$. By definition, $H_{ij}=\\frac{\\partial^{2}f}{\\partial x_{i}\\partial x_{j}}$.\n\nFirst compute the first-order partial derivatives using standard differentiation rules:\n- With respect to $x$:\n$$\nf_{x}=\\frac{\\partial}{\\partial x}\\left(3x^{2}z\\right)+\\frac{\\partial}{\\partial x}\\left(-y^{3}\\right)+\\frac{\\partial}{\\partial x}\\left(2xyz^{2}\\right)+\\frac{\\partial}{\\partial x}(15)\n=6xz+2yz^{2}.\n$$\n- With respect to $y$:\n$$\nf_{y}=\\frac{\\partial}{\\partial y}\\left(3x^{2}z\\right)+\\frac{\\partial}{\\partial y}\\left(-y^{3}\\right)+\\frac{\\partial}{\\partial y}\\left(2xyz^{2}\\right)+\\frac{\\partial}{\\partial y}(15)\n=-3y^{2}+2xz^{2}.\n$$\n- With respect to $z$:\n$$\nf_{z}=\\frac{\\partial}{\\partial z}\\left(3x^{2}z\\right)+\\frac{\\partial}{\\partial z}\\left(-y^{3}\\right)+\\frac{\\partial}{\\partial z}\\left(2xyz^{2}\\right)+\\frac{\\partial}{\\partial z}(15)\n=3x^{2}+4xyz.\n$$\n\nNow compute all second-order partial derivatives:\n\n- Second derivatives with respect to $x$:\n$$\nf_{xx}=\\frac{\\partial}{\\partial x}(6xz+2yz^{2})=6z,\\quad\nf_{xy}=\\frac{\\partial}{\\partial y}(6xz+2yz^{2})=2z^{2},\\quad\nf_{xz}=\\frac{\\partial}{\\partial z}(6xz+2yz^{2})=6x+4yz.\n$$\n\n- Second derivatives with respect to $y$:\n$$\nf_{yx}=\\frac{\\partial}{\\partial x}(-3y^{2}+2xz^{2})=2z^{2},\\quad\nf_{yy}=\\frac{\\partial}{\\partial y}(-3y^{2}+2xz^{2})=-6y,\\quad\nf_{yz}=\\frac{\\partial}{\\partial z}(-3y^{2}+2xz^{2})=4xz.\n$$\n\n- Second derivatives with respect to $z$:\n$$\nf_{zx}=\\frac{\\partial}{\\partial x}(3x^{2}+4xyz)=6x+4yz,\\quad\nf_{zy}=\\frac{\\partial}{\\partial y}(3x^{2}+4xyz)=4xz,\\quad\nf_{zz}=\\frac{\\partial}{\\partial z}(3x^{2}+4xyz)=4xy.\n$$\n\nAssembling these into the Hessian matrix in the order $(x,y,z)$ gives\n$$\nH=\\begin{pmatrix}\nf_{xx} & f_{xy} & f_{xz} \\\\\nf_{yx} & f_{yy} & f_{yz} \\\\\nf_{zx} & f_{zy} & f_{zz}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6z & 2z^{2} & 6x+4yz \\\\\n2z^{2} & -6y & 4xz \\\\\n6x+4yz & 4xz & 4xy\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\n6z & 2z^{2} & 6x+4yz \\\\\n2z^{2} & -6y & 4xz \\\\\n6x+4yz & 4xz & 4xy\n\\end{pmatrix}}$$", "id": "2215330"}, {"introduction": "Having mastered the mechanics of its calculation, we now turn to a deeper conceptual question: what information does the Hessian matrix truly capture? This exercise explores the relationship between a function's Hessian and its linear components. By analyzing how adding a linear term affects the Hessian, you will discover a fundamental principle about what the Hessian measures—and what it does not [@problem_id:2215291].", "problem": "Let $\\mathbf{x} = (x_1, x_2)^T$ be a vector in $\\mathbb{R}^2$. Consider the two scalar-valued functions $f: \\mathbb{R}^2 \\to \\mathbb{R}$ and $g: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(\\mathbf{x}) = 3x_1^3 - 2x_1^2 x_2 + 4x_1 x_2^2 + x_2^3$$\n$$g(\\mathbf{x}) = f(\\mathbf{x}) - 5x_1 + 12x_2$$\nLet $\\mathbf{H}_f(\\mathbf{x})$ and $\\mathbf{H}_g(\\mathbf{x})$ denote the Hessian matrices of the functions $f$ and $g$ with respect to $\\mathbf{x}$, respectively. Which of the following statements correctly describes the relationship between $\\mathbf{H}_f(\\mathbf{x})$ and $\\mathbf{H}_g(\\mathbf{x})$?\n\nA. $\\mathbf{H}_f(\\mathbf{x}) = \\mathbf{H}_g(\\mathbf{x})$ for all $\\mathbf{x} \\in \\mathbb{R}^2$.\n\nB. $\\mathbf{H}_g(\\mathbf{x}) = \\mathbf{H}_f(\\mathbf{x}) + \\begin{pmatrix} -5 & 0 \\\\ 0 & 12 \\end{pmatrix}$.\n\nC. $\\mathbf{H}_g(\\mathbf{x}) = \\mathbf{H}_f(\\mathbf{x}) + \\begin{pmatrix} 0 & 3.5 \\\\ 3.5 & 0 \\end{pmatrix}$.\n\nD. $\\mathbf{H}_g(\\mathbf{x}) = 2\\mathbf{H}_f(\\mathbf{x})$.\n\nE. $\\mathbf{H}_f(\\mathbf{x})$ is a zero matrix, but $\\mathbf{H}_g(\\mathbf{x})$ is not.", "solution": "The Hessian of a scalar function is the matrix of second-order partial derivatives. For $f(x_{1},x_{2}) = 3x_{1}^{3} - 2x_{1}^{2}x_{2} + 4x_{1}x_{2}^{2} + x_{2}^{3}$, compute first the gradients:\n$$\n\\frac{\\partial f}{\\partial x_{1}} = 9x_{1}^{2} - 4x_{1}x_{2} + 4x_{2}^{2}, \\quad\n\\frac{\\partial f}{\\partial x_{2}} = -2x_{1}^{2} + 8x_{1}x_{2} + 3x_{2}^{2}.\n$$\nThen the Hessian entries are\n$$\n\\frac{\\partial^{2} f}{\\partial x_{1}^{2}} = 18x_{1} - 4x_{2}, \\quad\n\\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}} = -4x_{1} + 8x_{2}, \\quad\n\\frac{\\partial^{2} f}{\\partial x_{2}^{2}} = 8x_{1} + 6x_{2}.\n$$\nThus\n$$\n\\mathbf{H}_{f}(\\mathbf{x}) =\n\\begin{pmatrix}\n18x_{1} - 4x_{2} & -4x_{1} + 8x_{2} \\\\\n-4x_{1} + 8x_{2} & 8x_{1} + 6x_{2}\n\\end{pmatrix}.\n$$\nNow $g(\\mathbf{x}) = f(\\mathbf{x}) - 5x_{1} + 12x_{2}$. The gradient is\n$$\n\\nabla g = \\nabla f + \\begin{pmatrix} -5 \\\\ 12 \\end{pmatrix},\n$$\nso taking second derivatives, the contributions from the linear terms $-5x_{1}$ and $12x_{2}$ vanish, because the second derivatives of any linear function are zero. Therefore,\n$$\n\\mathbf{H}_{g}(\\mathbf{x}) = \\mathbf{H}_{f}(\\mathbf{x}) \\quad \\text{for all } \\mathbf{x} \\in \\mathbb{R}^{2}.\n$$\nThis matches option A, while options B, C, D, and E are incorrect since linear terms do not alter the Hessian and $\\mathbf{H}_{f}$ is not the zero matrix.", "answer": "$$\\boxed{A}$$", "id": "2215291"}, {"introduction": "One of the Hessian matrix's most powerful applications is in determining the convexity of a function, a property that is central to the field of optimization. A convex function has a shape like a bowl, guaranteeing that any local minimum is also a global minimum. This final practice challenges you to use the Hessian and an important test known as Sylvester's criterion to find the conditions under which a quadratic function is strictly convex [@problem_id:2215312].", "problem": "A function $f(x,y,z)$ is defined by the expression $f(x, y, z) = 3x^2 + 2y^2 + \\alpha z^2 + 4xy + 6yz$, where the domain is $\\mathbb{R}^3$ and $\\alpha$ is a real-valued parameter. Find the infimum of the set of values for $\\alpha$ for which this function is strictly convex everywhere. Your answer should be a single real number.", "solution": "A twice continuously differentiable function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ is strictly convex everywhere if and only if its Hessian matrix is positive definite at every point. For the quadratic function\n$$\nf(x,y,z)=3x^{2}+2y^{2}+\\alpha z^{2}+4xy+6yz,\n$$\nthe Hessian is constant and given by\n$$\nH=\\begin{pmatrix}\n\\frac{\\partial^{2}f}{\\partial x^{2}} & \\frac{\\partial^{2}f}{\\partial x \\partial y} & \\frac{\\partial^{2}f}{\\partial x \\partial z} \\\\\n\\frac{\\partial^{2}f}{\\partial y \\partial x} & \\frac{\\partial^{2}f}{\\partial y^{2}} & \\frac{\\partial^{2}f}{\\partial y \\partial z} \\\\\n\\frac{\\partial^{2}f}{\\partial z \\partial x} & \\frac{\\partial^{2}f}{\\partial z \\partial y} & \\frac{\\partial^{2}f}{\\partial z^{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 & 4 & 0 \\\\\n4 & 4 & 6 \\\\\n0 & 6 & 2\\alpha\n\\end{pmatrix}.\n$$\nBy Sylvester’s criterion, $H$ is positive definite if and only if all leading principal minors are positive. Compute them:\n- First minor: $\\Delta_{1}=6>0$.\n- Second minor:\n$$\n\\Delta_{2}=\\det\\begin{pmatrix}6 & 4 \\\\ 4 & 4\\end{pmatrix}=6\\cdot 4-4\\cdot 4=8>0.\n$$\n- Third minor (the determinant of $H$):\n$$\n\\Delta_{3}=\\det\\begin{pmatrix}\n6 & 4 & 0 \\\\\n4 & 4 & 6 \\\\\n0 & 6 & 2\\alpha\n\\end{pmatrix}\n=6\\det\\begin{pmatrix}4 & 6 \\\\ 6 & 2\\alpha\\end{pmatrix}-4\\det\\begin{pmatrix}4 & 6 \\\\ 0 & 2\\alpha\\end{pmatrix}\n=6(8\\alpha-36)-4(8\\alpha)\n=48\\alpha-216-32\\alpha\n=16\\alpha-216.\n$$\nFor strict convexity we require $\\Delta_{3}>0$, i.e.,\n$$\n16\\alpha-216>0 \\quad \\Longleftrightarrow \\quad \\alpha>\\frac{216}{16}=\\frac{27}{2}.\n$$\nTherefore, the set of $\\alpha$ for which $f$ is strictly convex is $(\\frac{27}{2},\\infty)$, whose infimum is $\\frac{27}{2}$.", "answer": "$$\\boxed{\\frac{27}{2}}$$", "id": "2215312"}]}