{"hands_on_practices": [{"introduction": "To build intuition, we begin by simulating how a computer handles the subtraction of two very close numbers. This exercise [@problem_id:2158264] places you in the context of the bisection method, where an interval $[a, b]$ has become very narrow. You will see firsthand how finite-precision storage affects these values before they are subtracted and quantify the dramatic increase in relative error that defines catastrophic cancellation.", "problem": "In the field of computational science, understanding the limitations of floating-point arithmetic is crucial for writing robust numerical algorithms. Consider a hypothetical computer that represents all numbers using a normalized decimal floating-point format which stores a mantissa of exactly 7 significant decimal digits. When a number with more than 7 significant digits is stored, it is rounded to the nearest representable number. The rounding rule is as follows: if the first digit to be discarded is 5 or greater, the last remaining digit is rounded up; otherwise, it is left unchanged.\n\nA student is implementing the bisection method to find a root of a function. In one of the final iterations, the search interval has been narrowed down to $[a, b]$, where the exact theoretical values are $a = 9876.5432$ and $b = 9876.5441$. Before any calculations are performed, the machine first stores the values of $a$ and $b$ in its 7-digit floating-point format. Then, it computes the width of the interval, $w = b - a$, using these stored approximations.\n\nYour task is to determine the impact of this finite-precision arithmetic. Calculate the relative error of the computed width with respect to the true width. The relative error is defined as $\\frac{|\\text{computed value} - \\text{true value}|}{|\\text{true value}|}$. Express your final answer as a decimal rounded to three significant figures.", "solution": "The machine stores numbers with exactly 7 significant decimal digits, rounding to nearest with the rule that if the first discarded digit is at least 5, the last kept digit is increased by 1; otherwise, it is unchanged.\n\nFor $a=9876.5432$, keeping 7 significant digits retains $9876.543$ and the first discarded digit is $2<5$, so the stored value is\n$$\n\\hat{a}=9876.543.\n$$\nFor $b=9876.5441$, keeping 7 significant digits retains $9876.544$ and the first discarded digit is $1<5$, so the stored value is\n$$\n\\hat{b}=9876.544.\n$$\nThe computed width using stored values is\n$$\nw_{\\text{comp}}=\\hat{b}-\\hat{a}=9876.544-9876.543=0.001.\n$$\nThe true width is\n$$\nw_{\\text{true}}=b-a=9876.5441-9876.5432=0.0009.\n$$\nThe relative error is\n$$\n\\frac{|w_{\\text{comp}}-w_{\\text{true}}|}{|w_{\\text{true}}|}=\\frac{|0.001-0.0009|}{0.0009}=\\frac{0.0001}{0.0009}=\\frac{1}{9}.\n$$\nAs a decimal rounded to three significant figures, this is $0.111$.", "answer": "$$\\boxed{0.111}$$", "id": "2158264"}, {"introduction": "Once we can identify catastrophic cancellation, the next step is to eliminate it. This practice [@problem_id:2375840] moves from diagnosis to cure, presenting a function that is numerically unstable for large values of $x$. Your task is to apply a classic algebraic technique—multiplying by the conjugate—to derive an equivalent expression that is stable for all positive $x$, thereby turning a problematic subtraction into a benign addition.", "problem": "Let $f(x) = \\sqrt{x^{2}+1} - x$ for real $x > 0$. In a standard floating-point arithmetic model with unit roundoff $u$, direct evaluation of $f(x)$ for very large $x$ can suffer from catastrophic cancellation. By reasoning from first principles, derive an algebraically equivalent, numerically stable closed-form expression for $f(x)$ that avoids subtracting nearly equal quantities for very large $x$. Express your final answer as a single analytic expression in terms of $x$ only. No rounding is required.", "solution": "The source of numerical instability in the expression $f(x) = \\sqrt{x^{2}+1} - x$ for large $x > 0$ is the subtraction of two quantities of similar magnitude. In floating-point arithmetic, if two numbers $a$ and $b$ are very close, their difference $a-b$ will have a significantly smaller number of correct significant digits than either $a$ or $b$ individually. This loss of relative precision is known as catastrophic cancellation.\n\nTo derive a numerically stable expression, we must eliminate this subtraction. A standard algebraic technique for expressions involving a difference with a square root, of the form $\\sqrt{A} - B$, is to multiply and divide by the conjugate expression, which is $\\sqrt{A} + B$. This technique is often referred to as \"rationalizing the numerator\".\n\nThe given function is $f(x) = \\sqrt{x^{2}+1} - x$. Its conjugate expression is $\\sqrt{x^{2}+1} + x$. We multiply and divide $f(x)$ by this conjugate:\n\n$$ f(x) = (\\sqrt{x^{2}+1} - x) \\cdot \\frac{\\sqrt{x^{2}+1} + x}{\\sqrt{x^{2}+1} + x} $$\n\nThe numerator is now of the form $(a-b)(a+b)$, which simplifies to $a^{2}-b^{2}$. Here, $a = \\sqrt{x^{2}+1}$ and $b = x$.\n\n$$ f(x) = \\frac{(\\sqrt{x^{2}+1})^{2} - x^{2}}{\\sqrt{x^{2}+1} + x} $$\n\nSimplifying the numerator:\n\n$$ (\\sqrt{x^{2}+1})^{2} - x^{2} = (x^{2}+1) - x^{2} = 1 $$\n\nSubstituting this result back into the expression for $f(x)$:\n\n$$ f(x) = \\frac{1}{\\sqrt{x^{2}+1} + x} $$\n\nThis new expression is algebraically equivalent to the original for all $x$ in the domain. We must now verify its numerical stability. The denominator consists of the sum of two positive quantities, $\\sqrt{x^{2}+1}$ and $x$. In floating-point arithmetic, the addition of two numbers of the same sign is a benign operation that does not cause catastrophic cancellation. The subsequent division by this sum is also a numerically stable operation.\n\nFor large $x$, the denominator becomes large, and the result correctly approaches zero without any loss of relative precision. This transformed expression is therefore numerically stable for all $x > 0$, especially for large values of $x$ where the original expression failed.", "answer": "$$ \\boxed{\\frac{1}{\\sqrt{x^{2}+1} + x}} $$", "id": "2375840"}, {"introduction": "We conclude by tackling one of the most iconic examples of numerical instability: the quadratic formula. In this capstone exercise [@problem_id:2389875], you will move beyond simple expressions to design a robust algorithm that correctly computes the roots of a quadratic equation even when cancellation is a risk. This involves not only implementing a stable formula but also using Vieta's formulas to cleverly find the second root, synthesizing analysis, algebraic manipulation, and programming skills into a complete solution.", "problem": "Design and implement a program that quantitatively models numerical instability in computing the roots of the quadratic equation $a x^2 + b x + c = 0$ when $b^2 \\gg 4 a c$. Your program must compare a direct, naive evaluation of the quadratic formula against a numerically stable evaluation derived from first principles, and must measure forward relative errors using a high-precision reference.\n\nFundamental base and modeling assumptions:\n- Use the standard floating-point rounding model for basic operations, namely that each elementary arithmetic operation in floating-point arithmetic satisfies $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\le \\varepsilon$, where $\\varepsilon$ is the machine epsilon and $\\circ$ denotes $+$, $-$, $\\times$, or $\\div$.\n- Recognize that subtracting nearly equal floating-point numbers can lose significant digits (catastrophic cancellation), especially when computing $-b + \\sqrt{b^2 - 4 a c}$ for $b^2 \\gg 4 a c$.\n- Use relationships between polynomial coefficients and roots (Vieta’s formulas) as well-tested facts: if $x_1$ and $x_2$ are the roots, then $x_1 + x_2 = -b/a$ and $x_1 x_2 = c/a$.\n\nProgram requirements:\n1. Implement two double-precision computations of the roots:\n   - A naive method that directly evaluates both roots via the quadratic formula using $x = (-b \\pm \\sqrt{b^2 - 4 a c})/(2 a)$ in standard double precision.\n   - A numerically stable method that avoids subtracting nearly equal quantities by algebraic rearrangement justified from first principles and the relationships among coefficients and roots. You must ensure that the implementation minimizes cancellation for the root that would otherwise be unstable when $b^2 \\gg 4 a c$.\n2. Implement a high-precision reference computation of the roots using base-$10$ arbitrary precision, set to at least $p = 80$ decimal digits, to approximate the true values. Use this reference to assess forward errors.\n3. For each test case below, compute the two reference roots $x_{\\text{true},1}$ and $x_{\\text{true},2}$ at high precision and label them by magnitude as the “small” root $x_{\\text{true},s}$ (the one with smaller $|x|$) and the “large” root $x_{\\text{true},L}$ (the one with larger $|x|$). For each method (naive and stable), also label its two computed roots by their magnitudes as “small” and “large” to align comparisons by scale.\n4. For a computed root $\\hat{x}$ and a reference root $x_{\\text{true}}$, define the forward relative error as $E = |\\hat{x} - x_{\\text{true}}|/|x_{\\text{true}}|$ when $x_{\\text{true}} \\ne 0$, and define $E = |\\hat{x}|$ when $x_{\\text{true}} = 0$.\n5. For each test case, produce a list of four floating-point numbers in the order $[E_{\\text{naive},s}, E_{\\text{naive},L}, E_{\\text{stable},s}, E_{\\text{stable},L}]$.\n\nTest suite:\nUse the following five test cases, each specified as $(a,b,c)$:\n- $(1, 10^8, 1)$\n- $(1, -10^8, 1)$\n- $(1, 3, 1)$\n- $(1, 10^{16}, 1)$\n- $(10^{-8}, 1, 10^{-16})$\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of lists with no spaces, one inner list per test case, in the same order as listed above. Each floating-point number must be formatted in scientific notation with twelve digits after the decimal point. For example, a line like `[[1.234000000000e-12,5.678000000000e+03,...],...]`.\n- The output elements must be ordered for each test case exactly as $[E_{\\text{naive},s}, E_{\\text{naive},L}, E_{\\text{stable},s}, E_{\\text{stable},L}]$.", "solution": "The core of the problem is to solve the quadratic equation $a x^2 + b x + c = 0$ and analyze the numerical stability of its solution. The standard formula for the roots is:\n\n$$\nx_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\n\nWe will analyze this formula's behavior and derive a more stable alternative.\n\n**1. Analysis of Numerical Instability in the Naive Method**\n\nThe naive method directly implements the formula above for both roots. The instability arises under the condition $b^2 \\gg 4ac$. In this case, the discriminant $\\Delta = b^2 - 4ac$ is very close to $b^2$, and thus $\\sqrt{\\Delta}$ is very close to $|b|$.\n\nLet us analyze the numerator $-b \\pm \\sqrt{\\Delta}$. The floating-point error model states that the result of an operation is the true result multiplied by $(1+\\delta)$, where $|\\delta| \\le \\varepsilon$, the machine epsilon. When we subtract two nearly equal numbers, the relative error of the result can be very large. This phenomenon is known as catastrophic cancellation.\n\nConsider the term $\\sqrt{b^2 - 4ac}$. Using a binomial or Taylor expansion for small $z = \\frac{4ac}{b^2}$:\n$$\n\\sqrt{b^2 - 4ac} = |b|\\sqrt{1 - \\frac{4ac}{b^2}} = |b|\\left(1 - \\frac{1}{2}\\frac{4ac}{b^2} - \\frac{1}{8}\\left(\\frac{4ac}{b^2}\\right)^2 - \\dots\\right) \\approx |b|\\left(1 - \\frac{2ac}{b^2}\\right)\n$$\nThe calculation of one of the roots will involve the subtraction of $\\sqrt{\\Delta}$ from $-b$ (or vice-versa).\n- If $b > 0$, then $|b|=b$. The root $x_1 = (-b + \\sqrt{b^2-4ac})/(2a)$ involves the subtraction:\n$$\n-b + \\sqrt{b^2-4ac} \\approx -b + b\\left(1 - \\frac{2ac}{b^2}\\right) = -b + b - \\frac{2ac}{b} = -\\frac{2ac}{b}\n$$\nThe operation is $-b + (\\approx b)$, which is a subtraction of nearly equal numbers, leading to a loss of significant digits. This root is the one with the smaller magnitude. The other root, $x_2 = (-b - \\sqrt{b^2-4ac})/(2a)$, involves the summation of two large negative numbers, which is numerically stable. This root has the larger magnitude.\n\n- If $b < 0$, then $|b|=-b$. The root $x_2 = (-b - \\sqrt{b^2-4ac})/(2a)$ is unstable, as $-b > 0$ and $\\sqrt{b^2-4ac} \\approx \\sqrt{b^2} = |b| = -b$. The operation is $(\\approx -b) - (-b)$. The stable root is $x_1 = (-b + \\sqrt{b^2-4ac})/(2a)$, which involves adding two large positive numbers.\n\nIn general, the unstable calculation is for the root involving $-b + \\operatorname{sgn}(b)\\sqrt{\\Delta}$, which corresponds to the root with the smaller absolute value. The stable calculation is for the root involving $-b - \\operatorname{sgn}(b)\\sqrt{\\Delta}$, which is the root with the larger absolute value.\n\n**2. Derivation of the Numerically Stable Method**\n\nTo avoid catastrophic cancellation, a stable algorithm must be designed. The strategy is to:\na. Calculate the root with the larger magnitude, which we denote $\\hat{x}_L$, using the numerically stable version of the quadratic formula:\n$$\n\\hat{x}_L = \\frac{-b - \\operatorname{sgn}(b)\\sqrt{b^2 - 4ac}}{2a}\n$$\nHere, the term $\\operatorname{sgn}(b)\\sqrt{b^2 - 4ac}$ has the same sign as $b$, so $-b$ and $-\\operatorname{sgn}(b)\\sqrt{b^2-4ac}$ have the same sign. Their sum is therefore numerically stable.\n\nb. To find the second root (the one with smaller magnitude, $\\hat{x}_s$), we use Vieta's formulas, which relate the roots $x_1, x_2$ to the coefficients of the polynomial:\n$$\nx_1 + x_2 = -\\frac{b}{a} \\quad \\text{and} \\quad x_1 x_2 = \\frac{c}{a}\n$$\nUsing the product relation, we can find the small root from the large root:\n$$\n\\hat{x}_s = \\frac{c/a}{\\hat{x}_L} = \\frac{c}{a\\hat{x}_L}\n$$\nThis calculation is stable because $\\hat{x}_L$ has been computed accurately and, being the large-magnitude root, is not close to zero. Attempting to use the sum relation, $\\hat{x}_s = -b/a - \\hat{x}_L$, would be a mistake, as it would reintroduce catastrophic cancellation since $\\hat{x}_L \\approx -b/a$.\n\n**3. Implementation and Error Analysis**\n\nThe program will implement three procedures for finding the roots for each test case $(a, b, c)$:\n\n- **Reference Computation:** The roots $x_{\\text{true},1}$ and $x_{\\text{true},2}$ are computed using the standard quadratic formula but with arbitrary-precision arithmetic. A precision of $p = 85$ decimal digits is used to ensure these values act as a reliable ground truth. The resulting roots are sorted by their absolute values to identify the \"small\" true root, $x_{\\text{true},s}$, and the \"large\" true root, $x_{\\text{true},L}$.\n\n- **Naive Computation:** The roots $\\hat{x}_{\\text{naive},1}$ and $\\hat{x}_{\\text{naive},2}$ are computed using the standard quadratic formula in double-precision floating-point arithmetic. These are then sorted by magnitude to yield $\\hat{x}_{\\text{naive},s}$ and $\\hat{x}_{\\text{naive},L}$.\n\n- **Stable Computation:** The roots are computed in double-precision using the stable algorithm derived above. The large root $\\hat{x}_{\\text{stable},L}$ is calculated first, and then the small root $\\hat{x}_{\\text{stable},s}$ is found using Vieta's formula. This method inherently produces the roots separated by magnitude.\n\nFinally, for each method (naive and stable) and each root type (small and large), the forward relative error is computed with respect to the high-precision reference value. The error $E$ for a computed root $\\hat{x}$ and a true root $x_{\\text{true}}$ is given by:\n$$\nE = \\frac{|\\hat{x} - x_{\\text{true}}|}{|x_{\\text{true}}|} \\quad (\\text{for } x_{\\text{true}} \\neq 0)\n$$\nThe calculation $|\\hat{x} - x_{\\text{true}}|$ is performed at high precision to avoid losing accuracy from the true value during the comparison itself. The final program iterates through the test suite, computes the four required error values for each case, and formats the output as specified.", "answer": "```python\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical stability of naive and stable algorithms\n    for solving the quadratic equation ax^2 + bx + c = 0, particularly when b^2 >> 4ac.\n    \"\"\"\n    # Set precision for high-precision reference calculations.\n    # p=80 decimal digits is required, 85 is used for safety.\n    decimal.getcontext().prec = 85\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0e8, 1.0),\n        (1.0, -1.0e8, 1.0),\n        (1.0, 3.0, 1.0),\n        (1.0, 1.0e16, 1.0),\n        (1.0e-8, 1.0, 1.0e-16),\n    ]\n\n    all_case_results = []\n\n    def compute_relative_error(computed_val, true_val_hp):\n        \"\"\"\n        Calculates the forward relative error using a high-precision true value.\n        E = |computed - true| / |true|\n        \"\"\"\n        if true_val_hp == 0:\n            return abs(computed_val)\n        \n        # Promote the computed float to Decimal for accurate error calculation\n        error = abs(decimal.Decimal(computed_val) - true_val_hp) / abs(true_val_hp)\n        return float(error)\n\n    for a, b, c in test_cases:\n        # --- 1. High-Precision Reference Calculation ---\n        a_hp = decimal.Decimal(a)\n        b_hp = decimal.Decimal(b)\n        c_hp = decimal.Decimal(c)\n\n        delta_hp = (b_hp**2 - 4 * a_hp * c_hp).sqrt()\n        x_true1 = (-b_hp + delta_hp) / (2 * a_hp)\n        x_true2 = (-b_hp - delta_hp) / (2 * a_hp)\n\n        # Label true roots by magnitude: small (s) and large (L)\n        if abs(x_true1) < abs(x_true2):\n            x_true_s, x_true_L = x_true1, x_true2\n        else:\n            x_true_s, x_true_L = x_true2, x_true1\n\n        # --- 2. Naive Double-Precision Calculation ---\n        a_dp, b_dp, c_dp = a, b, c\n        \n        # Handle potential negative discriminant for real roots\n        discriminant_val = b_dp**2 - 4 * a_dp * c_dp\n        if discriminant_val < 0:\n            # Although problem context implies real roots, this is robust\n            discriminant_val = 0\n            \n        delta_dp = np.sqrt(discriminant_val)\n        \n        x_naive1 = (-b_dp + delta_dp) / (2 * a_dp)\n        x_naive2 = (-b_dp - delta_dp) / (2 * a_dp)\n        \n        # Label naive roots by magnitude\n        if abs(x_naive1) < abs(x_naive2):\n            x_naive_s, x_naive_L = x_naive1, x_naive2\n        else:\n            x_naive_s, x_naive_L = x_naive2, x_naive1\n\n        # --- 3. Stable Double-Precision Calculation ---\n        # The sign of b is used to determine the stable formula for the large root.\n        # np.copysign is used to handle b=0 robustly.\n        sign_b = np.copysign(1.0, b_dp if b_dp != 0 else 1.0)\n        \n        # Calculate the large root first, avoiding cancellation\n        x_stable_L = (-b_dp - sign_b * delta_dp) / (2 * a_dp)\n        \n        # Calculate the small root using Vieta's formula\n        x_stable_s = (c_dp / a_dp) / x_stable_L\n\n        # --- 4. Compute Forward Relative Errors ---\n        E_naive_s = compute_relative_error(x_naive_s, x_true_s)\n        E_naive_L = compute_relative_error(x_naive_L, x_true_L)\n        \n        E_stable_s = compute_relative_error(x_stable_s, x_true_s)\n        E_stable_L = compute_relative_error(x_stable_L, x_true_L)\n\n        case_errors = [E_naive_s, E_naive_L, E_stable_s, E_stable_L]\n        all_case_results.append(case_errors)\n\n    # --- 5. Format and Print Final Output ---\n    output_parts = []\n    for errors in all_case_results:\n        formatted_errors = [f\"{e:.12e}\" for e in errors]\n        output_parts.append(f\"[{','.join(formatted_errors)}]\")\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "2389875"}]}