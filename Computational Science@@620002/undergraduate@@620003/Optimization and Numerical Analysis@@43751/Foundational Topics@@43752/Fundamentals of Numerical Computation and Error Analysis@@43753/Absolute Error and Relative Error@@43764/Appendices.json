{"hands_on_practices": [{"introduction": "Understanding error begins with being able to calculate it. In numerical analysis, we frequently replace complex functions or operations with simpler approximations, a process that inherently introduces what's known as truncation error. This first exercise [@problem_id:2152036] provides a classic example of this by using a forward difference formula to approximate a derivative. By comparing the approximated value to the exact analytical solution, you will practice the fundamental skill of calculating relative error, a crucial first step in evaluating the accuracy of any numerical method.", "problem": "A numerical analyst is tasked with evaluating the accuracy of the forward difference approximation for computing the first derivative of a function. The function under consideration is $f(x) = 2x^3 + x^2 - 4x + 1$. The analyst decides to approximate the derivative at the specific point $x_0 = 1$ using a step size of $h = 0.2$.\n\nThe forward difference formula for the approximation is given by:\n$$\nf'_{\\text{approx}}(x_0) = \\frac{f(x_0+h) - f(x_0)}{h}\n$$\nThis approximation is to be compared against the true value of the derivative, $f'_{\\text{true}}(x_0)$, which is found using analytical differentiation.\n\nCalculate the relative error of this approximation. The relative error, $E_{\\text{rel}}$, is defined as the absolute value of the difference between the approximate and true values, divided by the absolute value of the true value.\n\nRound your final answer to three significant figures.", "solution": "Given $f(x)=2x^{3}+x^{2}-4x+1$, differentiate analytically to obtain the true derivative:\n$$\nf'(x)=6x^{2}+2x-4.\n$$\nAt $x_{0}=1$,\n$$\nf'_{\\text{true}}(1)=6\\cdot 1^{2}+2\\cdot 1-4=4.\n$$\nUsing the forward difference with $h=0.2$, compute\n$$\nf(1)=2\\cdot 1^{3}+1^{2}-4\\cdot 1+1=0,\n$$\n$$\nf(1.2)=2\\cdot (1.2)^{3}+(1.2)^{2}-4\\cdot 1.2+1=2\\cdot 1.728+1.44-4.8+1=3.456+1.44-4.8+1=1.096.\n$$\nThus the forward difference approximation is\n$$\nf'_{\\text{approx}}(1)=\\frac{f(1.2)-f(1)}{0.2}=\\frac{1.096-0}{0.2}=5.48.\n$$\nThe relative error is\n$$\nE_{\\text{rel}}=\\frac{|f'_{\\text{approx}}(1)-f'_{\\text{true}}(1)|}{|f'_{\\text{true}}(1)|}=\\frac{|5.48-4|}{4}=\\frac{1.48}{4}=0.37.\n$$\nRounded to three significant figures, this is $0.370$.", "answer": "$$\\boxed{0.370}$$", "id": "2152036"}, {"introduction": "Simply calculating an error value is only half the battle; interpreting that value correctly is just as important. For example, is an error of $0.01$ better when measuring a 10-meter beam, or is an error of $10^{-6}$ better when measuring a component of size $10^{-6}$ meters? This thought experiment [@problem_id:2198986] explores this very question by examining the approximation of polynomial roots of vastly different magnitudes. You will be challenged to decide whether absolute or relative error provides a more meaningful assessment of accuracy, reinforcing the critical concept that relative error is often indispensable for comparisons across different scales.", "problem": "In the analysis of a damped oscillatory system, the characteristic polynomial is found to be $P(x) = x^2 - (10 + 10^{-6})x + 10^{-5} = 0$. The two exact roots of this equation, representing characteristic frequencies of the system, are a large root, $\\beta = 10$, and a very small root, $\\alpha = 10^{-6}$.\n\nTwo different numerical algorithms are used to find approximations of these roots.\n-   Algorithm A is used to approximate the small root $\\alpha$, and it returns the value $\\tilde{\\alpha} = 2 \\times 10^{-6}$.\n-   Algorithm B is used to approximate the large root $\\beta$, and it returns the value $\\tilde{\\beta} = 10.01$.\n\nGiven these results, which of the following statements provides the most meaningful assessment of the accuracy of the two algorithms?\n\nA. Algorithm A is more accurate because its absolute error is significantly smaller than the absolute error of Algorithm B.\n\nB. Algorithm B is more accurate because its relative error is significantly smaller than the relative error of Algorithm A.\n\nC. Both algorithms exhibit poor accuracy, as both approximations result in relative errors greater than 0.0001.\n\nD. The accuracy of the two algorithms cannot be meaningfully compared, as they are approximating roots of vastly different magnitudes.\n\nE. Both algorithms have comparable accuracy because the absolute error of Algorithm A is on the same order of magnitude as the relative error of Algorithm B.", "solution": "We are given the quadratic $P(x) = x^{2} - (10 + 10^{-6})x + 10^{-5} = 0$ with exact roots $\\alpha = 10^{-6}$ and $\\beta = 10$. The numerical approximations are $\\tilde{\\alpha} = 2 \\times 10^{-6}$ and $\\tilde{\\beta} = 10.01$.\n\nTo assess accuracy, we compute both absolute and relative errors for each algorithm. For the small root:\n$$|\\tilde{\\alpha} - \\alpha| = |2 \\times 10^{-6} - 10^{-6}| = 10^{-6},$$\n$$\\text{relative error for } \\alpha = \\frac{|\\tilde{\\alpha} - \\alpha|}{|\\alpha|} = \\frac{10^{-6}}{10^{-6}} = 1.$$\n\nFor the large root:\n$$|\\tilde{\\beta} - \\beta| = |10.01 - 10| = 0.01,$$\n$$\\text{relative error for } \\beta = \\frac{|\\tilde{\\beta} - \\beta|}{|\\beta|} = \\frac{0.01}{10} = 0.001.$$\n\nWhen comparing approximations of quantities with vastly different magnitudes, the relative error is the meaningful metric. Algorithm A has relative error $1$, while Algorithm B has relative error $0.001$. Therefore, Algorithm B is more accurate because its relative error is significantly smaller than that of Algorithm A.\n\nEvaluating the options:\n- A is incorrect because it relies on absolute error, which is not appropriate across different scales, and in fact Algorithm A has much worse relative error.\n- B is correct: Algorithm B is more accurate due to a much smaller relative error.\n- C is not the most meaningful assessment; while both relative errors exceed $0.0001$, declaring both poor without a specified tolerance is arbitrary and ignores the large difference in relative accuracy.\n- D is incorrect; relative error allows meaningful comparison across scales.\n- E is incorrect; $10^{-6}$ and $0.001$ are not of the same order, and mixing absolute and relative errors is not meaningful.\n\nThus the most meaningful assessment is given by B.", "answer": "$$\\boxed{B}$$", "id": "2198986"}, {"introduction": "The ultimate goal of error analysis is not just to measure error, but to control and minimize it. This advanced practice [@problem_id:2370392] moves from analysis to synthesis, challenging you to tackle a famous pitfall in numerical computation known as \"catastrophic cancellation\". While the quadratic formula is simple in exact arithmetic, its direct implementation using floating-point numbers can lead to a disastrous loss of precision. This exercise requires you to not only quantify this large relative error but also to derive and implement a more robust, numerically stable algorithm from first principles, demonstrating how a deep understanding of error sources empowers us to write better code.", "problem": "A quadratic equation with real coefficients has the form $a x^2 + b x + c = 0$ with $a \\neq 0$. The exact real roots exist when the discriminant $D = b^2 - 4ac \\ge 0$. In floating-point (FP) arithmetic, direct evaluation of the quadratic formula can suffer catastrophic cancellation when $b^2 \\gg 4ac$, because one root is obtained by subtracting two nearly equal numbers. This problem asks you to quantify absolute and relative error for that cancellation-prone root and to mitigate the error by an algebraically equivalent reformulation derived from first principles.\n\nFundamental bases you may use:\n- Vieta’s formulas for quadratic equations: if $r_1$ and $r_2$ are the exact roots, then $r_1 + r_2 = -\\frac{b}{a}$ and $r_1 r_2 = \\frac{c}{a}$.\n- The quadratic formula expressing the exact roots in real arithmetic when $D \\ge 0$.\n- The definitions of absolute error and relative error. For an approximation $\\tilde{x}$ to a nonzero exact value $x$, the absolute error is $|\\tilde{x} - x|$ and the relative error is $\\frac{|\\tilde{x} - x|}{|x|}$. If $x = 0$, define the relative error to be the absolute error.\n\nYour tasks:\n1. For each test case below, compute both floating-point approximations to the two roots by directly applying the quadratic formula in standard double precision (that is, evaluate $\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ in the most straightforward way). Identify the cancellation-prone root as follows: if $b > 0$, then the cancellation-prone root is computed using the $+$ sign; if $b  0$, it is computed using the $-$ sign. This identification reflects which formula subtracts two nearly equal quantities when $b^2 \\gg 4ac$.\n2. Starting strictly from Vieta’s formulas and basic algebra, derive a numerically stable reformulation that avoids subtracting nearly equal numbers for the cancellation-prone root, and implement it. Your implementation must not rely on any “given” stabilized formula; it must follow from your derivation. You may, however, compute the other root in any stable manner implied by your derivation and then recover the cancellation-prone root using $r_1 r_2 = \\frac{c}{a}$.\n3. For ground truth, compute high-precision approximations to the exact roots using real arithmetic with at least $80$ decimal digits of precision, then cast to double precision for comparison. For each test case, choose the exact root that corresponds to the cancellation-prone one (the one of smaller magnitude when $b^2 \\gg 4ac$) and compute both its absolute error and its relative error for the naive evaluation (Task $1$) and for your stable reformulation (Task $2$).\n4. Your program must aggregate the results for all test cases into a single flat list of floating-point numbers in the order, per test case: $[\\text{abs\\_err\\_naive}, \\text{rel\\_err\\_naive}, \\text{abs\\_err\\_stable}, \\text{rel\\_err\\_stable}]$, concatenated across test cases in the same order as listed below.\n\nTest suite (each triplet is $(a,b,c)$):\n- Case $1$: $(a,b,c) = (1, 10^8, 1)$.\n- Case $2$: $(a,b,c) = (1, -10^8, 1)$.\n- Case $3$: $(a,b,c) = (1, 3, 10^{-3})$.\n- Case $4$: $(a,b,c) = (10^{-3}, 10^5, 10^{-3})$.\n\nNumerical and output requirements:\n- Use radians for any angular computations if they arise, although none are expected here.\n- No physical units are involved.\n- All outputs must be raw numbers; do not use a percentage sign. Relative error must be a pure number (for example, output $0.001$ for one-tenth of one percent).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots]$). The final list must contain exactly $16$ floating-point numbers corresponding to the $4$ values per test case over $4$ cases, in the same order as the test suite.", "solution": "The problem presented is a classical exercise in numerical analysis, concerning the loss of precision when solving a quadratic equation $ax^2 + bx + c = 0$ using floating-point arithmetic. The analysis of the problem's validity confirms it is scientifically grounded, well-posed, and objective. We shall proceed with a rigorous solution.\n\nThe standard quadratic formula provides the two exact roots, $r_1$ and $r_2$, as:\n$$\nr_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\nThe problem focuses on the condition where $b^2 \\gg 4ac$. In this regime, the discriminant $D = b^2 - 4ac$ is dominated by the $b^2$ term. Consequently, the square root $\\sqrt{b^2 - 4ac}$ can be approximated by a Taylor series expansion:\n$$\n\\sqrt{b^2 - 4ac} = |b| \\sqrt{1 - \\frac{4ac}{b^2}} \\approx |b| \\left(1 - \\frac{2ac}{b^2}\\right) = |b| - \\frac{2ac}{|b|}\n$$\nThis approximation reveals that $\\sqrt{b^2 - 4ac}$ is a quantity very close to $|b|$.\n\nThis proximity is the source of numerical instability. The phenomenon is known as catastrophic cancellation.\n\\begin{itemize}\n    \\item If $b > 0$, the numerator for one of the roots is $-b + \\sqrt{b^2-4ac}$. Since $\\sqrt{b^2-4ac} \\approx b$, this expression involves the subtraction of two nearly equal numbers. This operation results in a significant loss of relative precision in the computed result. The root affected is $r_1 = \\frac{-b + \\sqrt{b^2-4ac}}{2a}$.\n    \\item If $b  0$, then $-b$ is a positive quantity. The term $\\sqrt{b^2-4ac} \\approx \\sqrt{b^2} = |b| = -b$. The numerator for one root becomes $-b - \\sqrt{b^2-4ac}$, which is again a subtraction of two nearly equal positive numbers (e.g., if $b=-10^8$, $-b=10^8$ and $\\sqrt{D} \\approx 10^8$). This affects the root $r_2 = \\frac{-b - \\sqrt{b^2-4ac}}{2a}$.\n\\end{itemize}\nIn both cases, the root with the smaller magnitude is the one susceptible to this catastrophic cancellation. The other root, which involves an addition of terms of the same sign ($-b$ and $-\\sqrt{D}$ if $b>0$, or $-b$ and $+\\sqrt{D}$ if $b0$), is computed stably.\n\nTo mitigate this numerical error, we must derive an alternative formulation. The problem requires this derivation to start from first principles, specifically Vieta's formulas. The strategy is to first compute the \"stable\" root accurately and then use it to find the \"unstable\" (cancellation-prone) root.\n\nLet $r_{\\text{stable}}$ be the root that is computed without subtractive cancellation. It can be expressed in a unified manner using the sign function, $\\text{sgn}(b)$:\n$$\nr_{\\text{stable}} = \\frac{-b - \\text{sgn}(b)\\sqrt{b^2-4ac}}{2a}\n$$\nThis formula ensures that the two terms in the numerator, $-b$ and $-\\text{sgn}(b)\\sqrt{D}$, have the same sign, and their sum is computed robustly. For the non-trivial cases where $b^2 \\gg 4ac$, $b$ is non-zero, so $\\text{sgn}(b)$ is either $1$ or $-1$.\n\nHaving obtained an accurate value for $r_{\\text{stable}}$, we employ Vieta's formula for the product of roots, $r_1 r_2 = c/a$. If $r_{\\text{prone}}$ is the other root (the one susceptible to cancellation), then:\n$$\nr_{\\text{prone}} \\cdot r_{\\text{stable}} = \\frac{c}{a}\n$$\nFrom this, we can solve for $r_{\\text{prone}}$:\n$$\nr_{\\text{prone}} = \\frac{c/a}{r_{\\text{stable}}} = \\frac{c}{a \\cdot r_{\\text{stable}}}\n$$\nThis expression for $r_{\\text{prone}}$ involves only division and multiplication, which are numerically stable operations in this context, thus avoiding the catastrophic cancellation inherent in the naive formula.\n\nThe computational procedure to solve the problem is as follows:\n\\begin{enumerate}\n    \\item For each test case $(a, b, c)$, we first establish a ground truth value for the cancellation-prone root. This is achieved by computing the roots using the standard quadratic formula but with high-precision arithmetic (at least $80$ decimal digits, using Python's `decimal` module). The root corresponding to the subtraction of nearly equal terms is identified as the exact value, $x_{\\text{exact}}$, and cast to a standard double-precision float.\n\n    \\item We compute the cancellation-prone root, $\\tilde{x}_{\\text{naive}}$, using the direct quadratic formula in standard double-precision floating-point arithmetic.\n\n    \\item We compute the cancellation-prone root, $\\tilde{x}_{\\text{stable}}$, using the derived stable method. First, the stable root $\\tilde{r}_{\\text{stable}}$ is computed. Then, the prone root is found via $\\tilde{x}_{\\text{stable}} = (c/a) / \\tilde{r}_{\\text{stable}}$.\n\n    \\item For both the naive and stable methods, we calculate the absolute error, $|\\tilde{x} - x_{\\text{exact}}|$, and the relative error, $\\frac{|\\tilde{x} - x_{\\text{exact}}|}{|x_{\\text{exact}}|}$. If $x_{\\text{exact}}=0$, the relative error is taken to be the absolute error.\n\\end{enumerate}\nThese four error metrics are then collected for each test case and aggregated into a single list for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves for the roots of a quadratic equation using naive and stable methods,\n    and calculates the absolute and relative errors for the cancellation-prone root.\n    \"\"\"\n    # Set the precision for the decimal module to 85, which is more than the required 80 digits,\n    # to ensure accuracy of the ground truth calculations.\n    getcontext().prec = 85\n\n    # Define the test cases from the problem statement. Each tuple is (a, b, c).\n    test_cases = [\n        (1.0, 1e8, 1.0),\n        (1.0, -1e8, 1.0),\n        (1.0, 3.0, 1e-3),\n        (1e-3, 1e5, 1e-3),\n    ]\n\n    all_results = []\n    for a, b, c in test_cases:\n        # Use standard double-precision floats for coefficients in FP calculations\n        a_fp, b_fp, c_fp = float(a), float(b), float(c)\n\n        # Task 3: Ground truth using high-precision arithmetic\n        a_d, b_d, c_d = Decimal(a), Decimal(b), Decimal(c)\n        discriminant_d = (b_d**2 - 4 * a_d * c_d).sqrt()\n\n        # Identify the exact cancellation-prone root based on the sign of b.\n        # This is the root with the smaller magnitude.\n        if b_fp > 0:\n            # The root from (-b + sqrt(D)) suffers cancellation\n            x_exact_d = (-b_d + discriminant_d) / (2 * a_d)\n        else: # b = 0\n            # The root from (-b - sqrt(D)) suffers cancellation\n            x_exact_d = (-b_d - discriminant_d) / (2 * a_d)\n        \n        # Cast the high-precision ground truth to a standard float for comparison\n        x_exact = float(x_exact_d)\n\n        # Task 1: Naive evaluation using the standard quadratic formula in double precision\n        discriminant_fp = np.sqrt(b_fp**2 - 4 * a_fp * c_fp)\n        \n        # Identify the cancellation-prone root from the naive calculation\n        if b_fp > 0:\n            x_naive_prone = (-b_fp + discriminant_fp) / (2 * a_fp)\n        else:\n            x_naive_prone = (-b_fp - discriminant_fp) / (2 * a_fp)\n\n        # Task 2: Stable reformulation derived from first principles\n        # First, compute the stable root (the one without subtractive cancellation)\n        # np.copysign is robust and correctly applies the sign of b.\n        # It's equivalent to sgn(b) for b!=0.\n        stable_root = (-b_fp - np.copysign(1.0, b_fp) * discriminant_fp) / (2 * a_fp)\n        \n        # Then, use Vieta's formula (r1 * r2 = c/a) to find the cancellation-prone root.\n        # This avoids the subtraction of nearly equal numbers.\n        if stable_root == 0:\n            # Handle the unlikely case of the stable root being exactly zero\n            # This would imply c=0, in which case the prone root is also 0.\n            x_stable_prone = 0.0\n        else:\n            x_stable_prone = (c_fp / a_fp) / stable_root\n\n        # Task 4: Compute absolute and relative errors for both methods\n        # Naive method errors\n        abs_err_naive = abs(x_naive_prone - x_exact)\n        # Per problem spec, if x_exact is 0, relative error is absolute error\n        if x_exact != 0:\n            rel_err_naive = abs_err_naive / abs(x_exact)\n        else:\n            rel_err_naive = abs_err_naive\n        \n        # Stable method errors\n        abs_err_stable = abs(x_stable_prone - x_exact)\n        if x_exact != 0:\n            rel_err_stable = abs_err_stable / abs(x_exact)\n        else:\n            rel_err_stable = abs_err_stable\n\n        # Aggregate the results for this test case\n        all_results.extend([abs_err_naive, rel_err_naive, abs_err_stable, rel_err_stable])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.17e}' for r in all_results)}]\")\n\nsolve()\n```", "id": "2370392"}]}