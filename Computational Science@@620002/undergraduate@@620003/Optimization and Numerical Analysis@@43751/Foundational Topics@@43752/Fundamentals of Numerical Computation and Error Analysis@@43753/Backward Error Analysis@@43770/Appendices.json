{"hands_on_practices": [{"introduction": "Backward error analysis offers a powerful alternative perspective on errors in modeling and computation. Instead of asking how wrong our answer is for a given input, we ask: \"What is the smallest change to the input that would make our answer exactly correct?\" This first practice [@problem_id:2155431] grounds this abstract idea in a simple, geometric context, challenging you to find the minimal perturbation needed for a data point to fit a linear model perfectly. This exercise provides a concrete and visualizable foundation for the concept before diving into the more abstract world of floating-point arithmetic.", "problem": "In the field of numerical analysis, backward error analysis is a powerful tool for understanding the discrepancy between a mathematical model and observed data. Instead of asking \"how large is the error in the model's output for the given input?\", it asks \"what is the smallest change to the input data that would make the model's output exact?\"\n\nConsider a simple linear model given by the equation $y = mx + c$. An experimental data point $(x_i, y_i)$ is collected, which does not lie exactly on this line. We wish to find the backward error associated with this data point and model.\n\nThe backward error is defined as the magnitude of the smallest perturbation to the data point, $(\\delta x_i, \\delta y_i)$, such that the new point $(x_i + \\delta x_i, y_i + \\delta y_i)$ falls perfectly on the model line. The magnitude of the perturbation is measured by the standard Euclidean distance, i.e., we seek to minimize the value of $\\sqrt{(\\delta x_i)^2 + (\\delta y_i)^2}$.\n\nFind a symbolic expression for this minimum perturbation magnitude, representing the backward error, in terms of the model parameters $m$ and $c$, and the data coordinates $x_i$ and $y_i$.", "solution": "We seek the smallest perturbation $(\\delta x_{i}, \\delta y_{i})$ such that the perturbed point $(x_{i} + \\delta x_{i}, y_{i} + \\delta y_{i})$ lies exactly on the line $y = m x + c$. This requirement imposes the constraint\n$$\ny_{i} + \\delta y_{i} = m \\left(x_{i} + \\delta x_{i}\\right) + c,\n$$\nwhich can be rewritten as\n$$\n\\delta y_{i} - m\\,\\delta x_{i} = m x_{i} + c - y_{i}.\n$$\nThe backward error is the minimum value of the Euclidean norm\n$$\n\\sqrt{(\\delta x_{i})^{2} + (\\delta y_{i})^{2}},\n$$\nsubject to the above linear constraint. Because the square root is monotone, we equivalently minimize\n$$\nJ(\\delta x_{i}, \\delta y_{i}) = (\\delta x_{i})^{2} + (\\delta y_{i})^{2}\n$$\nsubject to\n$$\ng(\\delta x_{i}, \\delta y_{i}) = \\delta y_{i} - m\\,\\delta x_{i} - (m x_{i} + c - y_{i}) = 0.\n$$\nIntroduce a Lagrange multiplier $\\lambda$ and form\n$$\n\\mathcal{L} = (\\delta x_{i})^{2} + (\\delta y_{i})^{2} + \\lambda\\left(\\delta y_{i} - m\\,\\delta x_{i} - (m x_{i} + c - y_{i})\\right).\n$$\nSet partial derivatives to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\delta x_{i}} = 2\\,\\delta x_{i} - \\lambda m = 0 \\quad \\Rightarrow \\quad \\delta x_{i} = \\frac{\\lambda m}{2},\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\delta y_{i}} = 2\\,\\delta y_{i} + \\lambda = 0 \\quad \\Rightarrow \\quad \\delta y_{i} = -\\frac{\\lambda}{2}.\n$$\nImpose the constraint:\n$$\n\\delta y_{i} - m\\,\\delta x_{i} = m x_{i} + c - y_{i}.\n$$\nSubstitute the expressions for $\\delta x_{i}$ and $\\delta y_{i}$:\n$$\n-\\frac{\\lambda}{2} - m\\left(\\frac{\\lambda m}{2}\\right) = m x_{i} + c - y_{i}\n\\quad \\Rightarrow \\quad\n-\\frac{\\lambda}{2}(1 + m^{2}) = m x_{i} + c - y_{i}.\n$$\nHence\n$$\n\\lambda = \\frac{2\\,(y_{i} - m x_{i} - c)}{1 + m^{2}}.\n$$\nTherefore,\n$$\n\\delta x_{i} = \\frac{\\lambda m}{2} = \\frac{m\\,(y_{i} - m x_{i} - c)}{1 + m^{2}}, \n\\quad\n\\delta y_{i} = -\\frac{\\lambda}{2} = -\\frac{y_{i} - m x_{i} - c}{1 + m^{2}}.\n$$\nThe minimum squared magnitude is\n$$\n(\\delta x_{i})^{2} + (\\delta y_{i})^{2} \n= \\frac{m^{2}(y_{i} - m x_{i} - c)^{2} + (y_{i} - m x_{i} - c)^{2}}{(1 + m^{2})^{2}}\n= \\frac{(y_{i} - m x_{i} - c)^{2}}{1 + m^{2}}.\n$$\nThus the minimum perturbation magnitude (the backward error) is\n$$\n\\frac{|y_{i} - m x_{i} - c|}{\\sqrt{1 + m^{2}}}.\n$$\nThis is the perpendicular distance from $(x_{i}, y_{i})$ to the line $y = m x + c$.", "answer": "$$\\boxed{\\frac{|y_{i} - m x_{i} - c|}{\\sqrt{1 + m^{2}}}}$$", "id": "2155431"}, {"introduction": "We now move from the geometric interpretation to a primary application of backward error analysis: understanding the effects of finite-precision computer arithmetic. Every floating-point operation introduces a tiny error, and this exercise [@problem_id:2155424] demonstrates how to trace these errors through a compound calculation. By working through this problem, you will see how a computed result can be viewed as the exact result of the same calculation performed on slightly modified inputs, a fundamental skill for analyzing numerical stability.", "problem": "Consider a computer that performs floating-point arithmetic according to the standard model. For any two floating-point numbers $a$ and $b$, and any elementary arithmetic operation $\\circ \\in \\{+, -, \\times, \\div\\}$, the computed result is given by $\\mathrm{fl}(a \\circ b) = (a \\circ b)(1+\\delta)$, where $\\delta$ is the relative error of the operation and its magnitude is bounded by the machine epsilon, $|\\delta| \\le u$.\n\nAn analyst is computing the quantity $z = \\frac{x+y}{w}$ for given floating-point numbers $x, y, w$. The computation is performed as two consecutive operations: first, the sum $s=x+y$ is computed, and then the quotient $z=s/w$ is evaluated. Let the computed result be $\\hat{z}$.\n\nBackward error analysis provides a framework to interpret this computational error by stating that the computed value $\\hat{z}$ is the *exact* result for a problem with slightly perturbed input data. Specifically, we can find relative perturbations $\\epsilon_x$, $\\epsilon_y$, and $\\epsilon_w$ such that the following equation holds exactly:\n$$ \\hat{z} = \\frac{x(1+\\epsilon_x) + y(1+\\epsilon_y)}{w(1+\\epsilon_w)} $$\nAssume the relative error from the addition step is $\\delta_1$ and the relative error from the division step is $\\delta_2$. The choice of perturbations is not unique. For this problem, you are to follow a specific attribution scheme: the error from the sum $(x+y)$ is to be associated entirely and symmetrically with the individual terms $x$ and $y$, while the error from the subsequent division is to be associated entirely with the term $w$.\n\nFollowing this error attribution scheme, determine the expressions for the relative backward errors $\\epsilon_x$, $\\epsilon_y$, and $\\epsilon_w$ in terms of $\\delta_1$ and $\\delta_2$. Choose the correct set of expressions from the options below.\n\nA. $\\epsilon_x = \\delta_1$, $\\epsilon_y = \\delta_1$, $\\epsilon_w = \\delta_2$\n\nB. $\\epsilon_x = \\delta_1+\\delta_2$, $\\epsilon_y = \\delta_1+\\delta_2$, $\\epsilon_w = 0$\n\nC. $\\epsilon_x = \\delta_1$, $\\epsilon_y = \\delta_1$, $\\epsilon_w = -\\delta_2$\n\nD. $\\epsilon_x = \\delta_1$, $\\epsilon_y = \\delta_1$, $\\epsilon_w = \\frac{-\\delta_2}{1+\\delta_2}$\n\nE. $\\epsilon_x = \\delta_1+\\delta_2$, $\\epsilon_y = \\delta_1+\\delta_2$, $\\epsilon_w = -\\delta_1-\\delta_2$\n\nF. $\\epsilon_x = \\delta_1(1+\\delta_2)$, $\\epsilon_y = \\delta_1(1+\\delta_2)$, $\\epsilon_w = 0$", "solution": "We use the standard floating-point model: for any elementary operation, $fl(a \\circ b) = (a \\circ b)(1+\\delta)$ with $|\\delta| \\leq u$. The computation proceeds in two steps.\n\nFirst, the sum is computed with relative error $\\delta_{1}$:\n$$\n\\hat{s} = fl(x+y) = (x+y)(1+\\delta_{1}).\n$$\nSecond, the division is computed with relative error $\\delta_{2}$:\n$$\n\\hat{z} = fl\\left(\\frac{\\hat{s}}{w}\\right) = \\left(\\frac{\\hat{s}}{w}\\right)(1+\\delta_{2}) = \\frac{(x+y)(1+\\delta_{1})}{w}(1+\\delta_{2}).\n$$\n\nBackward error analysis asks for $\\epsilon_{x}$, $\\epsilon_{y}$, $\\epsilon_{w}$ such that\n$$\n\\hat{z} = \\frac{x(1+\\epsilon_{x}) + y(1+\\epsilon_{y})}{w(1+\\epsilon_{w})}.\n$$\nAccording to the attribution scheme, the error from the sum is to be associated entirely and symmetrically with $x$ and $y$, and the error from the division entirely with $w$.\n\nTo represent the sum’s error symmetrically, choose\n$$\n\\epsilon_{x} = \\delta_{1}, \\quad \\epsilon_{y} = \\delta_{1},\n$$\nwhich yields\n$$\nx(1+\\epsilon_{x}) + y(1+\\epsilon_{y}) = x(1+\\delta_{1}) + y(1+\\delta_{1}) = (x+y)(1+\\delta_{1}).\n$$\n\nTo represent the division’s error entirely in $w$, we require\n$$\n\\frac{(x+y)(1+\\delta_{1})}{w(1+\\epsilon_{w})} = \\frac{(x+y)(1+\\delta_{1})}{w}(1+\\delta_{2}),\n$$\nwhich reduces (after canceling the common factor $(x+y)(1+\\delta_{1})$) to\n$$\n\\frac{1}{1+\\epsilon_{w}} = 1+\\delta_{2}.\n$$\nSolving for $\\epsilon_{w}$ gives\n$$\n1+\\epsilon_{w} = \\frac{1}{1+\\delta_{2}}, \\quad \\epsilon_{w} = \\frac{-\\delta_{2}}{1+\\delta_{2}}.\n$$\n\nThus, the required perturbations are\n$$\n\\epsilon_{x} = \\delta_{1}, \\quad \\epsilon_{y} = \\delta_{1}, \\quad \\epsilon_{w} = \\frac{-\\delta_{2}}{1+\\delta_{2}},\n$$\nwhich corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "2155424"}, {"introduction": "Building on the analysis of single operations, we can now assess the stability of entire algorithms. An algorithm is considered \"backward stable\" if its computed output is the exact solution to a slightly perturbed version of the original problem. This practice [@problem_id:2155426] applies this concept to polynomial evaluation, guiding you to show that a computed 'machine root' is, in fact, the exact root of a polynomial with slightly different coefficients, thereby demonstrating the stability of the computational method.", "problem": "In numerical analysis, backward error analysis investigates the stability of an algorithm by showing that the computed result is the exact solution to a nearby problem. Consider the evaluation of a quadratic polynomial $p(x) = ax^2 + bx + c$ for a given value $\\hat{x}$. The evaluation is performed in floating-point arithmetic using Horner's method, which corresponds to the operational sequence for $((a\\hat{x} + b)\\hat{x} + c)$.\n\nWe adopt a simplified model for floating-point arithmetic where each elementary binary operation (`+`, `*`) introduces a relative error. That is, for any two numbers $z_1$ and $z_2$, the floating-point result is given by $\\mathrm{fl}(z_1 \\text{ op } z_2) = (z_1 \\text{ op } z_2)(1 + \\delta)$, where $\\delta$ is a small relative error term unique to that specific operation.\n\nThe sequence of floating-point operations to evaluate $p(\\hat{x})$ is defined as follows:\n1.  A multiplication of $a$ and $\\hat{x}$, introducing a relative error $\\delta_1$.\n2.  An addition of the result of step 1 with $b$, introducing a relative error $\\delta_2$.\n3.  A multiplication of the result of step 2 with $\\hat{x}$, introducing a relative error $\\delta_3$.\n4.  An addition of the result of step 3 with $c$, introducing a relative error $\\delta_4$.\n\nA numerical root-finding algorithm has produced a value $\\hat{x}$ such that the final computed result of this four-step sequence is exactly zero. This means $\\hat{x}$ can be considered a \"machine root\" of the polynomial. Your task is to apply backward error analysis to find the coefficients $(\\hat{a}, \\hat{b}, \\hat{c})$ of a perturbed polynomial $\\hat{p}(x) = \\hat{a}x^2 + \\hat{b}x + \\hat{c}$ for which $\\hat{x}$ is an *exact* root. In other words, the equation $\\hat{a}\\hat{x}^2 + \\hat{b}\\hat{x} + \\hat{c} = 0$ must hold true in exact arithmetic.\n\nExpress the coefficients $\\hat{a}$, $\\hat{b}$, and $\\hat{c}$ in terms of the original coefficients $a, b, c$ and the error terms $\\delta_1, \\delta_2, \\delta_3, \\delta_4$.", "solution": "We model each floating-point operation as an exact operation times a multiplicative perturbation. Let $\\hat{x}$ be the returned “machine root,” and let the Horner evaluation proceed as:\n1) $y_{1}=\\mathrm{fl}(a\\hat{x})=a\\hat{x}(1+\\delta_{1})$ by the floating-point multiplication model.\n2) $y_{2}=\\mathrm{fl}(y_{1}+b)=(y_{1}+b)(1+\\delta_{2})=\\big(a\\hat{x}(1+\\delta_{1})+b\\big)(1+\\delta_{2})$ by the floating-point addition model.\n3) $y_{3}=\\mathrm{fl}(y_{2}\\hat{x})=y_{2}\\hat{x}(1+\\delta_{3})=\\big(a\\hat{x}(1+\\delta_{1})+b\\big)(1+\\delta_{2})\\hat{x}(1+\\delta_{3})$ by the floating-point multiplication model.\n4) $y_{4}=\\mathrm{fl}(y_{3}+c)=(y_{3}+c)(1+\\delta_{4})$ by the floating-point addition model.\n\nBy hypothesis, the computed result is exactly zero, so $y_{4}=0$. Since the relative error terms are small, $1+\\delta_{4}\\neq 0$, hence\n$$\ny_{3}+c=0.\n$$\nSubstituting $y_{3}$ from step 3 gives\n$$\n\\big(a\\hat{x}(1+\\delta_{1})+b\\big)(1+\\delta_{2})\\hat{x}(1+\\delta_{3})+c=0.\n$$\nDistributing and collecting powers of $\\hat{x}$ yields\n$$\na\\hat{x}^{2}(1+\\delta_{1})(1+\\delta_{2})(1+\\delta_{3})+b\\hat{x}(1+\\delta_{2})(1+\\delta_{3})+c=0.\n$$\nDefine the perturbed coefficients\n$$\n\\hat{a}=a(1+\\delta_{1})(1+\\delta_{2})(1+\\delta_{3}),\\quad\n\\hat{b}=b(1+\\delta_{2})(1+\\delta_{3}),\\quad\n\\hat{c}=c.\n$$\nThen the above equation becomes\n$$\n\\hat{a}\\hat{x}^{2}+\\hat{b}\\hat{x}+\\hat{c}=0,\n$$\nso $\\hat{x}$ is an exact root of the perturbed polynomial $\\hat{p}(x)=\\hat{a}x^{2}+\\hat{b}x+\\hat{c}$. Note that $\\delta_{4}$ does not appear in $\\hat{a},\\hat{b},\\hat{c}$ because $y_{4}=0$ forces $y_{3}+c=0$ independently of the final multiplicative factor $(1+\\delta_{4})$.", "answer": "$$\\boxed{\\begin{pmatrix} a(1+\\delta_{1})(1+\\delta_{2})(1+\\delta_{3}) & b(1+\\delta_{2})(1+\\delta_{3}) & c \\end{pmatrix}}$$", "id": "2155426"}]}