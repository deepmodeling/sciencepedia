{"hands_on_practices": [{"introduction": "A fundamental task in programming is iterating a variable until it reaches a specific target. While this is straightforward with integers, this exercise demonstrates why the same logic can fail catastrophically with floating-point numbers. By simulating a simple arithmetic sequence, you will directly observe the consequences of representation error and understand why exact equality checks are unreliable in numerical computations. [@problem_id:2447428]", "problem": "You are given real scalars $s$, $h$, and $t$, and a positive integer $M$. Consider the sequence $\\{x_n\\}_{n\\ge 0}$ defined by $x_0 = s$ and $x_{n+1} = x_n + h$ for $n \\ge 0$. A naive termination condition that checks whether $x_n = t$ may be unreliable when $x_n$ is computed in binary floating-point arithmetic due to representation and rounding effects. Your task is to write a program that, for each provided test case, simulates the floating-point sequence and reports whether the equality $x_n = t$ ever holds within a bounded number of simulated iterations.\n\nFor each test case, perform the following steps:\n1. Initialize $x_0 = s$ in double-precision floating-point arithmetic.\n2. For $n = 0,1,2,\\dots$, check whether $x_n = t$ exactly in floating-point arithmetic. If equality holds for some $n \\le M$, record the smallest such $n$ and terminate the simulation for that test case.\n3. If no equality occurs for $0 \\le n \\le M$, terminate the simulation after $M$ increments.\n4. Let $n_{\\mathrm{exec}}$ denote the number of increments actually performed by your simulation for the test case, where $n_{\\mathrm{exec}}$ equals the smallest $n$ for which equality is observed (if any), or $M$ otherwise.\n5. Compute the theoretical exact value $x_{\\mathrm{exact}} = s + n_{\\mathrm{exec}}\\,h$ using exact real arithmetic, not using floating-point rounding at intermediate steps.\n6. Let $x_{\\mathrm{float}}$ denote the floating-point value obtained by your simulation after $n_{\\mathrm{exec}}$ increments. Compute the absolute error $e = |x_{\\mathrm{float}} - x_{\\mathrm{exact}}|$ as a real number.\n\nFor each test case, your program must output a list with the following fields in order:\n- A boolean indicating whether there exists an $n \\in \\{0,1,\\dots,M\\}$ such that $x_n = t$ in floating-point arithmetic.\n- The smallest such $n$ if it exists; otherwise the integer $-1$.\n- The integer $n_{\\mathrm{exec}}$.\n- The floating-point value $x_{\\mathrm{float}}$ after $n_{\\mathrm{exec}}$ increments.\n- The theoretical exact value $x_{\\mathrm{exact}}$ converted to a floating-point number for reporting.\n- The absolute error $e$ as a floating-point number.\n\nUse the following five test cases. In all test cases, angles do not appear, and there are no physical units. The real numbers written in decimal (such as $0.1$) denote exact real values in base ten, and expressions involving powers of two (such as $2^{-55}$) denote exact real values.\n\n- Test case 1 (canonical failure of equality with decimal step):\n  $s = 0.0$, $h = 0.1$, $t = 1.0$, $M = 12$.\n- Test case 2 (exactly representable step and reachable target):\n  $s = 0.0$, $h = 0.125$, $t = 1.0$, $M = 8$.\n- Test case 3 (step does not subdivide the interval):\n  $s = 0.0$, $h = 0.3$, $t = 1.0$, $M = 10$.\n- Test case 4 (negative step toward zero with decimal step):\n  $s = 1.0$, $h = -0.1$, $t = 0.0$, $M = 12$.\n- Test case 5 (increments below one unit in the last place until accumulation):\n  $s = 1.0$, $h = 2^{-55}$, $t = 1 + 2^{-52}$, $M = 8$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one test case in the same order as above. For example, the output must have the form\n$[r_1,r_2,r_3,r_4,r_5]$\nwhere each $r_i$ is the list corresponding to test case $i$, and there must be no spaces anywhere in the line. Each list $r_i$ must be of the form\n$[\\text{hit}, n_{\\min}, n_{\\mathrm{exec}}, x_{\\mathrm{float}}, x_{\\mathrm{exact}}, e]$\nwith the field types defined above. All numeric outputs must be represented as base-ten floats or integers on this line.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the principles of numerical computation, well-posed with a clear and deterministic procedure, and objective in its formulation. It presents a standard but fundamental problem in computational engineering concerning the limitations of floating-point arithmetic. We shall now proceed with the solution.\n\nThe core of this problem lies in the fundamental distinction between exact real arithmetic and computer-based floating-point arithmetic, governed by the IEEE $754$ standard for double-precision numbers. When simulating the sequence $x_{n+1} = x_n + h$, two primary sources of error arise:\n\n$1$. **Representation Error**: Many decimal fractions, such as $0.1$ or $0.3$, do not have an exact finite representation in binary. They are stored as the nearest representable binary floating-point number. For example, the decimal $0.1$ is an infinite repeating fraction in binary ($0.0001100110011\\dots_2$), which must be truncated to fit the $52$-bit significand of a double-precision float. This introduces an initial error before any computation begins. Conversely, numbers that are finite sums of powers of two, such as $0.125 = 1/8 = 2^{-3}$, are represented exactly.\n\n$2$. **Round-off Error**: Each arithmetic operation, in this case, addition, is performed with finite precision. The mathematically exact result of $x_n + h$ is rounded to the nearest representable floating-point number. This small error, introduced at each step, can accumulate over many iterations, causing the computed sequence $x_n$ to diverge from its theoretical path.\n\nThe provided problem requires a simulation that demonstrates these effects. The methodology is as follows:\n\nFirst, we must meticulously handle the input values. The provided test case parameters $s$, $h$, and $t$ are defined as exact real numbers. To compute the theoretical value $x_{\\mathrm{exact}}$, we must use a high-precision arithmetic library that avoids standard floating-point inaccuracies. For this purpose, Python's `decimal` module is employed, configured with a sufficiently high precision to handle all calculations as if they were exact. The inputs for each test case are converted to these high-precision objects.\n\nSecond, a simulation is performed for each test case using standard double-precision floating-point arithmetic, which is represented in Python by the `float` type or `numpy.float64`. The simulation adheres to the following algorithm:\n$1$. Initialize the floating-point sequence value $x_{\\mathrm{float}} \\leftarrow \\text{float}(s)$. Initialize `hit` to `False` and `n_min` to $-1$.\n$2$. Iterate with an index $n$ from $0$ to $M$, inclusive. In each iteration, the current value $x_n$ of the sequence is represented by $x_{\\mathrm{float}}$.\n$3$. At each step $n$, perform an exact equality check: if $x_{\\mathrm{float}} == \\text{float}(t)$.\n$4$. If equality holds, the target has been hit. We set `hit` to `True`, record the current index as $n_{\\mathrm{min}} = n$, set the number of increments performed as $n_{\\mathrm{exec}} = n$, store the current value of $x_{\\mathrm{float}}$ as the final one, and terminate the simulation loop for this test case.\n$5$. If equality does not hold and $n  M$, the sequence is advanced by one step: $x_{\\mathrm{float}} \\leftarrow x_{\\mathrm{float}} + \\text{float}(h)$.\n$6$. If the loop completes without finding an equality (i.e., for all $n \\in \\{0, 1, \\dots, M\\}$), we set $n_{\\mathrm{exec}} = M$. The final value of $x_{\\mathrm{float}}$ is the result after $M$ increments.\n\nThird, after the simulation determines the values of $n_{\\mathrm{exec}}$ and the final $x_{\\mathrm{float}}$, we calculate the theoretical quantities.\n$1$. The exact final value is computed using the high-precision `Decimal` objects: $x_{\\mathrm{exact}} = s_{\\mathrm{exact}} + n_{\\mathrm{exec}} \\cdot h_{\\mathrm{exact}}$.\n$2$. The absolute error is then $e = |x_{\\mathrm{float}} - \\text{float}(x_{\\mathrm{exact}})|$.\n\nFinally, the six required output fields (`hit`, $n_{\\mathrm{min}}$, $n_{\\mathrm{exec}}$, $x_{\\mathrm{float}}$, $\\text{float}(x_{\\mathrm{exact}})$, and $e$) are collected into a list for each test case.\n\nThe specific test cases are designed to illustrate different behaviors:\n- **Cases $1$, $3$, and $4$**: These use step sizes ($h = \\pm 0.1$, $h = 0.3$) that are not exactly representable in binary. The accumulation of representation and round-off errors will cause the floating-point sequence to miss the target value $t$ exactly. Thus, we expect `hit` to be `False`.\n- **Case $2$**: Here, $s, h, t$ (`0.0`, `0.125`, `1.0`) are all exactly representable, as $h=2^{-3}$. All additions are exact. The sequence will exactly hit the target $t=1.0$ at $n=8$. Thus, we expect `hit` to be `True`.\n- **Case $5$**: This case is subtle. The start value is $s=1.0$. The unit in the last place (ULP) for $1.0$ is $2^{-52}$. The increment is $h = 2^{-55}$, which is smaller than half the ULP of $1.0$. Due to round-to-nearest-even rules in IEEE $754$, the operation $1.0 + 2^{-55}$ rounds back to $1.0$. The simulated value of $x_n$ will therefore remain stuck at $1.0$ and will never reach the target $t = 1.0 + 2^{-52}$. However, the exact sum does accumulate. This demonstrates a significant truncation error where small increments are lost. We expect `hit` to be `False`, and a non-zero error $e$ equal to the sum of the lost increments.", "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Simulates a floating-point sequence and compares it with exact arithmetic.\n    \"\"\"\n    # Set a high precision for the Decimal module for \"exact\" calculations.\n    getcontext().prec = 100\n\n    def run_case_simulation(s_exact, h_exact, t_exact, M):\n        \"\"\"\n        Runs the simulation for a single test case.\n        \"\"\"\n        # Convert exact Decimal inputs to double-precision floats for simulation.\n        s_f = np.float64(s_exact)\n        h_f = np.float64(h_exact)\n        t_f = np.float64(t_exact)\n\n        x_n_float = s_f\n        hit = False\n        n_min = -1\n        \n        # Loop from n=0 to M, checking the sequence value x_n at each step.\n        for n in range(M + 1):\n            # Per problem, check for exact floating-point equality.\n            if x_n_float == t_f:\n                hit = True\n                n_min = n\n                n_exec = n\n                x_float_final = x_n_float\n                break\n            \n            # If not hit and not the last iteration, perform one increment.\n            if n  M:\n                x_n_float += h_f\n        else:  # This 'else' clause executes if the 'for' loop completes without a 'break'.\n            n_exec = M\n            # The final value is the result after M increments.\n            x_float_final = x_n_float\n\n        # Calculate the theoretical exact value and the absolute error.\n        x_exact_val = s_exact + Decimal(n_exec) * h_exact\n        error = np.abs(x_float_final - np.float64(x_exact_val))\n\n        return [hit, n_min, n_exec, x_float_final, np.float64(x_exact_val), error]\n\n    # Define test cases using Decimal for exact representation of inputs.\n    test_cases = [\n        # Test case 1: Canonical failure with decimal step.\n        (Decimal('0.0'), Decimal('0.1'), Decimal('1.0'), 12),\n        # Test case 2: Exactly representable step and reachable target.\n        (Decimal('0.0'), Decimal('0.125'), Decimal('1.0'), 8),\n        # Test case 3: Step does not subdivide the interval.\n        (Decimal('0.0'), Decimal('0.3'), Decimal('1.0'), 10),\n        # Test case 4: Negative step toward zero with decimal step.\n        (Decimal('1.0'), Decimal('-0.1'), Decimal('0.0'), 12),\n        # Test case 5: Increments below one ULP until accumulation.\n        (Decimal('1.0'), Decimal(1) / (Decimal(2)**55), Decimal(1) + Decimal(1) / (Decimal(2)**52), 8)\n    ]\n    \n    all_results = []\n    for case_params in test_cases:\n        s, h, t, M = case_params\n        result = run_case_simulation(s, h, t, M)\n        all_results.append(result)\n\n    # Format the output string to be a list of lists with no spaces.\n    # Each inner list is manually formatted to avoid spaces from default str(list).\n    result_strings = []\n    for res in all_results:\n        # Note: res[0] is a boolean, str(res[0]) is 'True' or 'False'.\n        res_str = f\"[{str(res[0]).lower()},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447428"}, {"introduction": "In mathematics, addition is associative: $(a+b)+c = a+(b+c)$. This exercise challenges that intuition in the context of finite-precision arithmetic, revealing that the order of summation can dramatically alter the final result. You will investigate how summing a list of numbers from smallest-to-largest versus largest-to-smallest magnitude affects accuracy, uncovering the phenomenon of \"swamping\" and learning a crucial technique for error mitigation. [@problem_id:2447450]", "problem": "You must write a complete, runnable program that demonstrates how finite-precision rounding in floating-point arithmetic causes the result of summing real numbers to depend on the order of operations. The investigation must be grounded in the standard rounding model of floating-point arithmetic. In this model, a floating-point addition of two real numbers is represented as $\\,\\mathrm{fl}(x + y) = (x + y)(1 + \\delta)\\,$ with $\\,|\\delta| \\leq u\\,$, where $\\,u\\,$ is the unit roundoff (for Institute of Electrical and Electronics Engineers (IEEE) base-$2$ double precision, $\\,u = 2^{-53}\\,$). The program must use IEEE double precision ($\\,64\\,$-bit) as implemented by the host programming language for all floating-point operations.\n\nDefinitions and requirements:\n- Let $\\,\\{a_i\\}_{i=1}^n\\,$ be a finite list of real numbers. The exact real sum is $\\,S = \\sum_{i=1}^n a_i\\,$. A naive floating-point summation computes $\\,\\hat{S}\\,$ by iterating the binary addition $\\,s \\leftarrow \\mathrm{fl}(s + a_i)\\,$ from an initial $\\,s = 0\\,$ in some order.\n- You will compare two orders:\n  1. Ascending-by-magnitude order: sort by $\\,|a_i|\\,$ in ascending order using a stable sort; then sum from smallest magnitude to largest. Denote the result by $\\,\\hat{S}_{\\mathrm{asc}}\\,$.\n  2. Descending-by-magnitude order: sort by $\\,|a_i|\\,$ in descending order using a stable sort; then sum from largest magnitude to smallest. Denote the result by $\\,\\hat{S}_{\\mathrm{desc}}\\,$.\n- As a high-accuracy reference, compute $\\,S_{\\mathrm{ref}}\\,$ using arbitrary precision decimal arithmetic with at least $\\,80\\,$ decimal digits of precision, and treat the inputs as exact decimal rationals. This reference is used to quantify the absolute errors $\\,|\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|\\,$ and $\\,|\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|\\,$. To compare values in a single arithmetic domain, convert the floating-point results to exact decimal rationals before subtraction; that is, convert IEEE double $\\,x\\,$ to its exactly represented decimal $\\,\\mathrm{Decimal}(x)\\,$ and then subtract from $\\,S_{\\mathrm{ref}}\\,$.\n\nYour program must implement the above, and produce a single line of output with the results for the following four test cases (test suite). For each test case, output the triple $\\,\\big[\\hat{S}_{\\mathrm{desc}} - \\hat{S}_{\\mathrm{asc}},\\, |\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|,\\, |\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|\\big]\\,$ as native floating-point numbers.\n\nTest suite (lists are to be interpreted as exact decimal inputs; a stable sort must preserve the original order for ties in $\\,|a_i|\\,$):\n1. Case A (small numbers swamped by a large one): $\\,\\{1.0,\\, 10 \\text{ copies of } 10^{-16}\\}\\,$, i.e., the list $\\,\\{1.0,\\, 10^{-16},\\, 10^{-16},\\, \\dots,\\, 10^{-16}\\}\\,$ with $\\,10\\,$ copies of $\\,10^{-16}\\,$.\n2. Case B (catastrophic cancellation with three terms): $\\,\\{10^{16},\\, 1.0,\\, -10^{16}\\}\\,$.\n3. Case C (large cancellation plus moderate terms): $\\,\\{10^{16},\\, -10^{16},\\, 3.14,\\, 2.71\\}\\,$.\n4. Case D (harmonic sum, $\\,n = 10{,}000\\,$): $\\,\\left\\{ \\frac{1}{k} \\,:\\, k = 1, 2, \\dots, 10{,}000 \\right\\}\\,$.\n\nImplementation details:\n- All floating-point summations for $\\,\\hat{S}_{\\mathrm{asc}}\\,$ and $\\,\\hat{S}_{\\mathrm{desc}}\\,$ must use IEEE double precision ($\\,64\\,$-bit) additions. Use a stable sort by absolute value for the two required orders.\n- The reference $\\,S_{\\mathrm{ref}}\\,$ must be computed using arbitrary precision decimal arithmetic with at least $\\,80\\,$ decimal digits. Each input $\\,a_i\\,$ must be interpreted as an exact decimal rational before summation in this high-precision arithmetic.\n- For absolute errors $\\,|\\hat{S} - S_{\\mathrm{ref}}|\\,$, first convert $\\,\\hat{S}\\,$ to its exact decimal representation, then subtract $\\,S_{\\mathrm{ref}}\\,$ in the high-precision decimal domain, and finally report the magnitude as a floating-point number.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a length-$3$ list for one test case. The exact format is\n$\\,\\big[\\,[d_1, e^{\\mathrm{asc}}_1, e^{\\mathrm{desc}}_1],\\,[d_2, e^{\\mathrm{asc}}_2, e^{\\mathrm{desc}}_2],\\,[d_3, e^{\\mathrm{asc}}_3, e^{\\mathrm{desc}}_3],\\,[d_4, e^{\\mathrm{asc}}_4, e^{\\mathrm{desc}}_4]\\,\\big]\\,$,\nwhere $\\,d_i = \\hat{S}_{\\mathrm{desc}} - \\hat{S}_{\\mathrm{asc}}\\,$, $\\,e^{\\mathrm{asc}}_i = |\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|\\,$, and $\\,e^{\\mathrm{desc}}_i = |\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|\\,$. Units do not apply, and all angles, if any, are in radians by default (none are used here).", "solution": "The problem statement is valid. It is scientifically sound, well-posed, and provides a clear, quantitative task rooted in the fundamental principles of numerical analysis, specifically the study of round-off errors in floating-point arithmetic. The problem requires a demonstration of the non-associativity of floating-point addition.\n\nThe core of the problem is to compute the sum of a list of numbers, $\\{a_i\\}_{i=1}^n$, using three different methods and to compare the results. The first two methods use standard IEEE $754$ double-precision ($64$-bit) floating-point arithmetic but differ in the order of summation. The third method uses high-precision arithmetic to establish a reference value, considered here as the \"exact\" sum.\n\nThe methodological approach is structured as follows:\n\n1.  **High-Precision Reference Summation ($S_{\\mathrm{ref}}$)**: To establish a ground truth for the sum, we first compute it using arbitrary-precision decimal arithmetic. The precision is set to $100$ decimal digits, which exceeds the requirement of at least $80$ digits. Each input number $a_i$ is treated as an exact decimal rational. For terms like $1/k$, the division is performed within the high-precision context to maintain accuracy. The sum of these high-precision numbers yields the reference sum, $S_{\\mathrm{ref}}$.\n\n2.  **Floating-Point Summation ($\\hat{S}_{\\mathrm{asc}}$ and $\\hat{S}_{\\mathrm{desc}}$)**: The summations in finite precision are performed using standard double-precision floating-point numbers. The problem specifies two orderings based on the magnitude of the terms:\n    *   **Ascending Order ($\\hat{S}_{\\mathrm{asc}}$)**: The list of numbers $\\{a_i\\}$ is sorted according to their absolute values, $|a_i|$, in ascending order. A stable sorting algorithm is used, which ensures that numbers with equal magnitudes maintain their original relative order. The sum is then computed by iterating through this sorted list, accumulating the total with a naive sequential summation: $s_0 = 0$, $s_j = \\mathrm{fl}(s_{j-1} + a'_j)$, where $\\{a'_j\\}$ is the sorted list. The final result is $\\hat{S}_{\\mathrm{asc}} = s_n$.\n    *   **Descending Order ($\\hat{S}_{\\mathrm{desc}}$)**: The procedure is identical, except the list is sorted by $|a_i|$ in descending order. The resulting sum is denoted $\\hat{S}_{\\mathrm{desc}}$.\n\n3.  **Error Analysis**: The analysis involves computing three quantities for each test case.\n    *   The discrepancy between the two floating-point results, $d = \\hat{S}_{\\mathrm{desc}} - \\hat{S}_{\\mathrm{asc}}$. This subtraction is performed in standard double-precision arithmetic.\n    *   The absolute error of the ascending-order sum, $e_{\\mathrm{asc}} = |\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|$.\n    *   The absolute error of the descending-order sum, $e_{\\mathrm{desc}} = |\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|$.\n    To compute these errors accurately, the floating-point results $\\hat{S}_{\\mathrm{asc}}$ and $\\hat{S}_{\\mathrm{desc}}$ are first converted into their exact high-precision decimal representations. The subtraction and absolute value operations are then carried out in the high-precision domain before the final error value is converted back to a standard double-precision float for output.\n\nThis methodology is applied to four distinct test cases designed to highlight specific phenomena of floating-point arithmetic.\n*   **Case A** demonstrates **swamping**, where adding a small number to a very large number results in the loss of the small number's value. Summing in ascending order ($small \\to large$) is expected to be more accurate because the small numbers are first accumulated into a sum that is large enough not to be entirely lost when added to the largest number.\n*   **Case B** and **Case C** involve **catastrophic cancellation**, the subtraction of two nearly equal large numbers. This operation can lead to a dramatic loss of significant digits. The accuracy depends on when the cancellation occurs relative to the addition of other, smaller terms. If the cancellation happens first (as in the descending-magnitude sort), the subsequent addition of small terms to a result near zero is accurate. If a small term is added to a large number before cancellation (as in the ascending sort), its value is swamped and lost.\n*   **Case D**, the partial sum of the harmonic series $\\sum_{k=1}^{10000} \\frac{1}{k}$, involves summing a large number of terms with a wide range of magnitudes. The general heuristic for minimizing accumulated round-off error in such sums is to add the numbers from smallest to largest. Therefore, $\\hat{S}_{\\mathrm{asc}}$ is expected to be significantly more accurate than $\\hat{S}_{\\mathrm{desc}}$.\n\nThe program implements this logic, processing each test case to compute the triple $[d, e_{\\mathrm{asc}}, e_{\\mathrm{desc}}]$ and formats the final output as a list of these results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing summation order effects in floating-point arithmetic.\n    \"\"\"\n\n    def analyze_summation(float_numbers, decimal_numbers):\n        \"\"\"\n        Computes sums in different orders and compares them to a high-precision reference.\n\n        Args:\n            float_numbers (list): A list of numbers as standard Python floats (IEEE 754 double).\n            decimal_numbers (list): A list of numbers as high-precision Decimal objects.\n\n        Returns:\n            list: A list containing [d, e_asc, e_desc] as floats.\n        \"\"\"\n        # 1. Compute the high-precision reference sum S_ref.\n        # The sum is initialized with Decimal(0) to ensure high-precision accumulation.\n        s_ref = sum(decimal_numbers, decimal.Decimal(0))\n\n        # 2. Compute floating-point sums S_asc and S_desc.\n        # The problem requires a stable sort by absolute value. Python's sorted() is stable.\n\n        # Ascending-by-magnitude summation\n        asc_sorted_floats = sorted(float_numbers, key=abs)\n        # Use np.sum for sequential summation as specified by environment.\n        s_asc_float = np.sum(np.array(asc_sorted_floats, dtype=np.float64))\n        \n        # Descending-by-magnitude summation\n        desc_sorted_floats = sorted(float_numbers, key=abs, reverse=True)\n        s_desc_float = np.sum(np.array(desc_sorted_floats, dtype=np.float64))\n\n        # 3. Calculate the required output quantities.\n        # d = S_desc - S_asc, computed in standard floating-point arithmetic.\n        d = s_desc_float - s_asc_float\n\n        # To calculate errors accurately, convert float sums to their exact Decimal representation.\n        # Decimal(float_value) creates a Decimal that exactly represents the binary float.\n        s_asc_decimal = decimal.Decimal(s_asc_float)\n        s_desc_decimal = decimal.Decimal(s_desc_float)\n        \n        # Calculate absolute errors in the high-precision domain.\n        e_asc_decimal = abs(s_asc_decimal - s_ref)\n        e_desc_decimal = abs(s_desc_decimal - s_ref)\n        \n        # Convert final error values back to float for output.\n        return [d, float(e_asc_decimal), float(e_desc_decimal)]\n\n    # Set precision for decimal calculations. 100 digits is safely above the required 80.\n    decimal.getcontext().prec = 100\n    \n    all_results = []\n    \n    # Test Case A: small numbers swamped by a large one\n    inputs_a_str = ['1.0'] + ['1e-16'] * 10\n    floats_a = [float(s) for s in inputs_a_str]\n    decimals_a = [decimal.Decimal(s) for s in inputs_a_str]\n    all_results.append(analyze_summation(floats_a, decimals_a))\n    \n    # Test Case B: catastrophic cancellation with three terms\n    inputs_b_str = ['1e16', '1.0', '-1e16']\n    floats_b = [float(s) for s in inputs_b_str]\n    decimals_b = [decimal.Decimal(s) for s in inputs_b_str]\n    all_results.append(analyze_summation(floats_b, decimals_b))\n\n    # Test Case C: large cancellation plus moderate terms\n    inputs_c_str = ['1e16', '-1e16', '3.14', '2.71']\n    floats_c = [float(s) for s in inputs_c_str]\n    decimals_c = [decimal.Decimal(s) for s in inputs_c_str]\n    all_results.append(analyze_summation(floats_c, decimals_c))\n\n    # Test Case D: harmonic sum, n = 10,000\n    n = 10000\n    # The float numbers for standard summation\n    floats_d = [1.0 / k for k in range(1, n + 1)]\n    # The high-precision decimal numbers for the reference sum.\n    # Division is performed in the high-precision context.\n    decimals_d = [decimal.Decimal(1) / decimal.Decimal(k) for k in range(1, n + 1)]\n    all_results.append(analyze_summation(floats_d, decimals_d))\n    \n    # Final print statement in the exact required format.\n    result_strings = []\n    for res in all_results:\n        res_str = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_strings.append(res_str)\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2447450"}, {"introduction": "While reordering terms can improve summation accuracy, some applications demand an even higher degree of precision without the overhead of sorting. This practice introduces the Kahan summation algorithm, a classic and powerful method for compensated summation that drastically reduces accumulated round-off error. By implementing this algorithm, you will see how to systematically track and correct for the small errors that are lost in each step of a naive summation, providing a robust tool for scientific computation. [@problem_id:2447409]", "problem": "You must write a complete, runnable program that evaluates round-off error in summing sequences of real numbers and demonstrates error reduction using the Kahan summation algorithm. All computations are to be carried out in standard double-precision binary floating-point arithmetic. For each test case, compute two sums of the same sequence: a naive left-to-right floating-point sum and a compensated sum using the Kahan summation algorithm. For each sum, compute the absolute error with respect to a high-accuracy reference sum. Your program must then output a single line containing all absolute errors for all test cases in a specified order and format.\n\nDefine absolute error for a computed sum $\\hat{S}$ relative to a reference value $S^{\\star}$ as $E = \\lvert \\hat{S} - S^{\\star} \\rvert$.\n\nThe test suite consists of the following four sequences:\n\n- Test case $1$ (many tiny increments added to a large baseline):\n  - Sequence $S_1$ has length $N_1 + 1$, where $N_1 = 10^{6}$. The first term is $s^{(1)}_0 = 1$, and the remaining $N_1$ terms are $s^{(1)}_k = 10^{-16}$ for $1 \\le k \\le N_1$.\n\n- Test case $2$ (repeated catastrophic cancellation triplets):\n  - Let $M = 2 \\cdot 10^{5}$. Sequence $S_2$ is the concatenation of $M$ blocks of three terms $(1, 10^{-16}, -1)$.\n\n- Test case $3$ (deterministic pseudo-random small-magnitude values with slight bias):\n  - Let the modulus $m = 2^{64}$, multiplier $a = 6364136223846793005$, increment $c = 1442695040888963407$, and seed $x_0 = 123456789123456789$. Define a linear congruential generator by $x_{k+1} \\equiv a x_k + c \\pmod{m}$ for $k \\ge 0$. Let $N_3 = 5 \\cdot 10^{4}$. For $k = 1, 2, \\dots, N_3$, define $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ and the sequence term $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$. The sequence $S_3$ consists of these $N_3$ terms.\n\n- Test case $4$ (dynamic range and cancellation in a short sequence):\n  - Sequence $S_4$ has five terms: $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$.\n\nFor each test case $i \\in \\{1,2,3,4\\}$, compute:\n- The naive sum $\\hat{S}^{\\text{naive}}_i$ by left-to-right accumulation in double-precision floating-point arithmetic.\n- The Kahan-compensated sum $\\hat{S}^{\\text{Kahan}}_i$ using the Kahan summation algorithm in double-precision floating-point arithmetic.\n- A high-accuracy reference $S^{\\star}_i$ computed from the mathematical definition of the sequence using exact arithmetic where possible or using base-$10$ arbitrary precision arithmetic with at least $50$ correct decimal digits such that rounding in double-precision does not contaminate $S^{\\star}_i$.\n\nFor each test case $i$, compute the absolute errors $E^{\\text{naive}}_i = \\lvert \\hat{S}^{\\text{naive}}_i - S^{\\star}_i \\rvert$ and $E^{\\text{Kahan}}_i = \\lvert \\hat{S}^{\\text{Kahan}}_i - S^{\\star}_i \\rvert$.\n\nFinal output format:\n- Produce a single line of output containing a comma-separated list enclosed in square brackets. The list must contain the $8$ numbers in the following order:\n  - $E^{\\text{naive}}_1, E^{\\text{Kahan}}_1, E^{\\text{naive}}_2, E^{\\text{Kahan}}_2, E^{\\text{naive}}_3, E^{\\text{Kahan}}_3, E^{\\text{naive}}_4, E^{\\text{Kahan}}_4$.\n- Each number must be rounded to $12$ significant digits, expressed as a decimal (scientific notation is acceptable).\n- Example of the required single-line format (illustrative only): $[e_1,e_2,e_3,e_4,e_5,e_6,e_7,e_8]$.\n\nNo physical units or angle units are involved in this problem. The program must be self-contained and must not require any user input or external files. The results must be reproducible exactly from the definitions above in any modern programming language that adheres to standard double-precision floating-point semantics.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of numerical analysis, specifically concerning floating-point arithmetic and round-off error. The problem is well-posed, with all necessary data and definitions provided to compute a unique, verifiable solution. It is objective and free from ambiguity.\n\nThe core of this problem is to demonstrate and quantify the loss of precision that occurs during the summation of floating-point numbers of disparate magnitudes and the mitigation of this error using a compensated summation algorithm.\n\nAll calculations are performed using standard double-precision floating-point arithmetic, which corresponds to the IEEE 754 $64$-bit format. This format has approximately $15$ to $17$ decimal digits of precision. The machine epsilon, $\\epsilon$, which is the smallest number such that $1.0 + \\epsilon > 1.0$, is approximately $2.22 \\times 10^{-16}$ for this format. When two numbers of vastly different magnitudes are added, the smaller number may be partially or completely lost. This phenomenon is known as swamping.\n\nThe first method of summation is naive, left-to-right accumulation. For a sequence $s_0, s_1, \\dots, s_N$, the sum $\\hat{S}^{\\text{naive}}$ is computed as $(\\dots((s_0 + s_1) + s_2) + \\dots + s_N)$. This method is highly susceptible to round-off error.\n\nThe second method is the Kahan summation algorithm, a method of compensated summation. It significantly reduces the numerical error in the total obtained by adding a sequence of finite-precision floating-point numbers. The algorithm maintains a running compensation variable, $c$, which accumulates the error that would otherwise be lost. For each term $s_k$ in the sequence, the update rules are:\n$$y_k = s_k - c_{k-1}$$\n$$t_k = \\text{sum}_{k-1} + y_k$$\n$$c_k = (t_k - \\text{sum}_{k-1}) - y_k$$\n$$\\text{sum}_k = t_k$$\nHere, $\\text{sum}_0 = 0$ and $c_0 = 0$. The term $(t_k - \\text{sum}_{k-1})$ recovers the high-order part of $y_k$, and subtracting $y_k$ from this isolates the low-order part (the round-off error), which is stored in $c_k$ and subtracted from the next term $s_{k+1}$.\n\nThe absolute error is defined as $E = \\lvert \\hat{S} - S^{\\star} \\rvert$, where $\\hat{S}$ is a computed sum and $S^{\\star}$ is a high-accuracy reference sum.\n\nAnalysis of Test Cases:\n\nTest Case $1$:\nThe sequence is $s^{(1)}_0 = 1$ followed by $N_1 = 10^6$ terms of $s^{(1)}_k = 10^{-16}$ for $k \\ge 1$.\nThe exact sum is $S^{\\star}_1 = 1 + 10^6 \\times 10^{-16} = 1 + 10^{-10}$.\nIn naive summation, we compute $1 + 10^{-16} + 10^{-16} + \\dots$. The term $10^{-16}$ is very close to the machine epsilon relative to $1.0$. The operation $1.0 + 10^{-16}$ in double-precision arithmetic will suffer from swamping; the result is likely to be rounded back to $1.0$. Thus, most of the small terms will be lost, and $\\hat{S}^{\\text{naive}}_1$ is expected to be very close to $1.0$, resulting in an error close to $10^{-10}$.\nThe Kahan algorithm will capture the lost part $10^{-16}$ in the compensation variable $c$ at each step and reintroduce it, yielding a result $\\hat{S}^{\\text{Kahan}}_1$ that is extremely close to $S^{\\star}_1$. The error $E^{\\text{Kahan}}_1$ should be near machine precision.\n\nTest Case $2$:\nThe sequence consists of $M = 2 \\cdot 10^5$ blocks of $(1, 10^{-16}, -1)$.\nThe exact sum of one block is $1 + 10^{-16} - 1 = 10^{-16}$. The total exact sum is $S^{\\star}_2 = 2 \\cdot 10^5 \\times 10^{-16} = 2 \\cdot 10^{-11}$.\nNaive summation will compute $(1 + 10^{-16}) - 1$. As in the first case, $1 + 10^{-16}$ will likely round to $1.0$, and thus $(1 + 10^{-16}) - 1$ evaluates to $0$. Repeating this for all blocks, $\\hat{S}^{\\text{naive}}_2$ is expected to be $0.0$, leading to an error $E^{\\text{naive}}_2$ of exactly $2 \\cdot 10^{-11}$.\nThe Kahan algorithm will prevent this cancellation error, producing a sum $\\hat{S}^{\\text{Kahan}}_2$ very close to $S^{\\star}_2$, and a much smaller error $E^{\\text{Kahan}}_2$.\n\nTest Case $3$:\nThe sequence consists of $N_3 = 5 \\cdot 10^4$ terms $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$, where $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ and $x_k$ is from an LCG. The values of $u_k$ are pseudo-random in $[-0.5, 0.5)$. The terms $s^{(3)}_k$ are small, with a small positive bias of $10^{-16}$.\nThe exact sum is $S^{\\star}_3 = \\sum_{k=1}^{N_3} (10^{-12} u_k + 10^{-16}) = 10^{-12} \\sum_{k=1}^{N_3} u_k + N_3 \\cdot 10^{-16}$.\nThis sum must be computed using high-precision arithmetic to serve as the reference $S^{\\star}_3$. The LCG states $x_{k+1} \\equiv a x_k + c \\pmod{m}$ are computed using $64$-bit integer arithmetic. The sum $\\sum x_k$ is computed using arbitrary-precision integers, and the final expression for $S^{\\star}_3$ is evaluated using high-precision decimal arithmetic.\nNaive summation will accumulate small round-off errors over the $5 \\cdot 10^4$ additions. The Kahan algorithm is expected to minimize this accumulation, leading to $E^{\\text{Kahan}}_3 \\ll E^{\\text{naive}}_3$.\n\nTest Case $4$:\nThe sequence is $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$.\nThe exact sum is $S^{\\star}_4 = (10^{16} - 10^{16}) + (1 + 3) + 4 \\cdot 10^{-16} = 4 + 4 \\cdot 10^{-16}$.\nNaive left-to-right summation calculates step-by-step:\n1. $10^{16} + 1 = 10^{16}$ (swamping, since $1$ is less than the unit in the last place of $10^{16}$).\n2. $10^{16} - 10^{16} = 0$.\n3. $0 + 3 = 3$.\n4. $3 + 4 \\cdot 10^{-16} = 3$ (swamping, since $4 \\cdot 10^{-16}$ is smaller than machine epsilon relative to $3$).\nSo, $\\hat{S}^{\\text{naive}}_4 = 3$. The error is $E^{\\text{naive}}_4 = \\lvert 3 - (4 + 4 \\cdot 10^{-16}) \\rvert \\approx 1$.\nThe Kahan summation algorithm is designed to handle this. The loss of $1$ in the first step will be captured by the compensation variable. The final sum $\\hat{S}^{\\text{Kahan}}_4$ should be very close to the true sum $S^{\\star}_4$, resulting in a very small error $E^{\\text{Kahan}}_4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Computes and prints round-off errors for naive and Kahan summations\n    for four specific test cases, adhering to the problem specification.\n    \"\"\"\n\n    def naive_sum(sequence):\n        \"\"\"Computes the naive left-to-right sum of a sequence.\"\"\"\n        s = 0.0\n        for x in sequence:\n            s += x\n        return s\n\n    def kahan_sum(sequence):\n        \"\"\"Computes the sum of a sequence using Kahan's algorithm.\"\"\"\n        s = 0.0\n        c = 0.0\n        for x in sequence:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def generate_test_cases():\n        \"\"\"Generates the sequences for all four test cases.\"\"\"\n        # Test Case 1: Many tiny increments\n        N1 = 10**6\n        seq1 = np.full(N1, 1e-16, dtype=np.float64)\n        seq1 = np.insert(seq1, 0, 1.0)\n        \n        # Test Case 2: Repeated catastrophic cancellation\n        M = 2 * 10**5\n        block = np.array([1.0, 1e-16, -1.0], dtype=np.float64)\n        seq2 = np.tile(block, M)\n        \n        # Test Case 3: LCG-based sequence\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        seq3 = np.zeros(N3, dtype=np.float64)\n        x_current = x0\n        for k in range(N3):\n            x_current = (a * x_current + c) % m\n            u_k = x_current / m - 0.5\n            seq3[k] = 1e-12 * u_k + 1e-16\n\n        # Test Case 4: Dynamic range and cancellation\n        seq4 = np.array([1e16, 1.0, -1e16, 3.0, 4e-16], dtype=np.float64)\n        \n        return [seq1, seq2, seq3, seq4]\n\n    def get_reference_sums():\n        \"\"\"Computes high-accuracy reference sums for all test cases.\"\"\"\n        # Set precision for Decimal calculations\n        decimal.getcontext().prec = 100\n\n        # Reference Sum 1\n        N1 = 10**6\n        s_star_1 = decimal.Decimal(1) + decimal.Decimal(N1) * decimal.Decimal('1e-16')\n\n        # Reference Sum 2\n        M = 2 * 10**5\n        s_star_2 = decimal.Decimal(M) * decimal.Decimal('1e-16')\n\n        # Reference Sum 3\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        sum_x = 0\n        x_current = x0\n        for _ in range(N3):\n            x_current = (a * x_current + c) % m\n            sum_x += x_current\n        \n        D_sum_x = decimal.Decimal(sum_x)\n        D_m = decimal.Decimal(m)\n        D_N3 = decimal.Decimal(N3)\n        D_1e_12 = decimal.Decimal('1e-12')\n        D_1e_16 = decimal.Decimal('1e-16')\n        D_half = decimal.Decimal('0.5')\n        \n        sum_u = D_sum_x / D_m - D_N3 * D_half\n        s_star_3 = D_1e_12 * sum_u + D_N3 * D_1e_16\n\n        # Reference Sum 4\n        s_star_4 = decimal.Decimal('4') + decimal.Decimal('4e-16')\n        \n        return [float(s_star_1), float(s_star_2), float(s_star_3), float(s_star_4)]\n\n    sequences = generate_test_cases()\n    reference_sums = get_reference_sums()\n    \n    results = []\n    \n    for i in range(4):\n        seq = sequences[i]\n        s_star = reference_sums[i]\n        \n        # Naive sum and its error\n        s_naive = naive_sum(seq)\n        e_naive = abs(s_naive - s_star)\n        \n        # Kahan sum and its error\n        s_kahan = kahan_sum(seq)\n        e_kahan = abs(s_kahan - s_star)\n        \n        results.extend([e_naive, e_kahan])\n\n    # Format output to 12 significant digits and print\n    formatted_results = [f\"{res:.12g}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2447409"}]}