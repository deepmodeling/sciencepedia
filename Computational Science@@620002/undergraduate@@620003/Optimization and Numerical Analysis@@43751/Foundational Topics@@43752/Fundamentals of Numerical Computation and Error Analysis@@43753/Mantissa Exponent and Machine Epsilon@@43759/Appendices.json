{"hands_on_practices": [{"introduction": "To truly grasp the concept of machine precision, it's helpful to move from abstract definitions to concrete calculations. This first practice invites you to dissect a simplified, hypothetical floating-point systemâ€”much like the ones used in early or specialized computing devices. By calculating the machine epsilon for this system by hand, you will build a foundational intuition for how the finite length of the mantissa directly limits the precision of representable numbers [@problem_id:2204331].", "problem": "Consider a hypothetical 8-bit floating-point number system designed for a low-power embedded controller. In this system, each number is represented using 1 bit for the sign ($S$), 4 bits for a biased exponent ($E$), and 3 bits for the fraction part of the mantissa ($M$).\n\nThe value of a number is given by the formula:\n$$V = (-1)^S \\times (1.M)_2 \\times 2^{E - \\text{bias}}$$\n\nThe mantissa is normalized, meaning it is of the form $(1.M)_2$, which represents the binary number $1$ followed by the 3-bit fraction $M$. The exponent bias is defined as $\\text{bias} = 2^{k-1} - 1$, where $k$ is the number of bits in the exponent field.\n\nIn any floating-point system, there is a fundamental limitation on precision known as machine epsilon ($\\epsilon_m$). It is defined as the smallest positive number that, when added to 1, results in a value greater than 1 in that system's representation.\n\nCalculate the value of the machine epsilon for this 8-bit floating-point system. Express your answer as a single real number in decimal form.", "solution": "We have an 8-bit floating-point format with 1 sign bit, 4 exponent bits, and 3 fraction bits in the mantissa. The value is given by\n$$\nV = (-1)^{S} (1.M)_{2} 2^{E - \\text{bias}},\n$$\nwith normalized mantissa and bias defined by $\\text{bias} = 2^{k-1} - 1$ for $k$ exponent bits.\n\nFirst compute the bias for $k=4$:\n$$\n\\text{bias} = 2^{4-1} - 1 = 2^{3} - 1 = 7.\n$$\nThe number $1$ is represented by choosing $S=0$, $E=\\text{bias}$, and $M=000$, giving\n$$\nV_{1} = (1.000)_{2} \\times 2^{7-7} = 1 \\times 2^{0} = 1.\n$$\nThe next representable number greater than $1$ at the same exponent has the smallest positive mantissa increment, namely $M=001$. Its value is\n$$\nV_{\\text{next}} = (1.001)_{2} \\times 2^{7-7} = \\left(1 + 2^{-3}\\right) \\times 2^{0} = 1 + 2^{-3}.\n$$\nBy the definition of machine epsilon $\\epsilon_{m}$ as the smallest positive number such that $1 + \\epsilon_{m} > 1$ in this system, we have\n$$\n\\epsilon_{m} = V_{\\text{next}} - V_{1} = \\left(1 + 2^{-3}\\right) - 1 = 2^{-3} = \\frac{1}{8} = 0.125.\n$$\nEquivalently, since the precision is $p=1+3=4$ significant bits (including the implicit leading $1$), the general formula $\\epsilon_{m} = 2^{1-p}$ yields\n$$\n\\epsilon_{m} = 2^{1-4} = 2^{-3} = 0.125,\n$$\nconsistent with the direct construction.", "answer": "$$\\boxed{0.125}$$", "id": "2204331"}, {"introduction": "Having explored the mechanics of machine epsilon in a theoretical model, we now turn to the real world of scientific computing. This hands-on coding exercise challenges you to write a program that empirically measures machine epsilon for standard single and double-precision numbers, validating your findings against the formal IEEE 754 standard [@problem_id:2395229]. This practice bridges the gap between theory and the practical realities of programming, allowing you to \"discover\" a fundamental constant of your own computer.", "problem": "Write a complete program that empirically determines the machine epsilon for both single precision and double precision binary floating-point numbers, and validates the result against theoretical expectations from the Institute of Electrical and Electronics Engineers (IEEE) Standard $754$. In this context, define the machine epsilon as the smallest positive number $\\epsilon$ such that $1 + \\epsilon$ is representable and strictly greater than $1$ in the given precision. Use only deterministic computations; do not read any input.\n\nBackground and foundational base: Floating-point numbers in IEEE Standard $754$ are represented in base $b = 2$. Any normalized number can be written as $x = m \\times b^{e}$, where the significand $m$ satisfies $1 \\le m < 2$ and $e$ is an integer exponent. For normalized numbers, the gap between adjacent representable values depends only on $b$ and the number of significand bits $p$. Single precision has $p = 24$ and double precision has $p = 53$. All rounding adheres to round-to-nearest, ties-to-even.\n\nTasks your program must perform:\n- Implement, for a given binary floating-point type, a purely empirical procedure to estimate the machine epsilon by iterative halving. Begin with $\\epsilon = 1$ in that type, and repeatedly halve until adding it to $1$ no longer changes the value in that type. Return the last $\\epsilon$ that did change the value. This must be performed separately for single precision ($32$-bit) and double precision ($64$-bit).\n- Using only the base facts above, compute the theoretical spacing between $1$ and the next larger representable number for each precision from $b$ and $p$.\n- For each precision, compute the relative error between the empirical epsilon $\\epsilon_{\\mathrm{emp}}$ and the theoretical spacing $\\epsilon_{\\mathrm{theory}}$ as $\\left|\\epsilon_{\\mathrm{emp}} - \\epsilon_{\\mathrm{theory}}\\right| / \\epsilon_{\\mathrm{theory}}$.\n- For each precision, verify the boundary property that $1 + \\epsilon_{\\mathrm{emp}} > 1$ while $1 + \\epsilon_{\\mathrm{emp}}/2 = 1$, evaluated in that precision, and report the result as a boolean.\n\nTest suite and parameters:\n- Use the following type-parameter pairs:\n  - Single precision ($32$-bit) with $p = 24$.\n  - Double precision ($64$-bit) with $p = 53$.\nThese two cases together constitute the test suite and must both be processed by your program.\n\nRequired final output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets with the following six entries in this exact order:\n  - $[\\epsilon_{\\mathrm{single}}, \\epsilon_{\\mathrm{double}}, r_{\\mathrm{single}}, r_{\\mathrm{double}}, \\mathrm{ok}_{\\mathrm{single}}, \\mathrm{ok}_{\\mathrm{double}}]$,\nwhere $\\epsilon_{\\mathrm{single}}$ and $\\epsilon_{\\mathrm{double}}$ are the empirical epsilons for single and double precision respectively, $r_{\\mathrm{single}}$ and $r_{\\mathrm{double}}$ are the corresponding relative errors (nonnegative real numbers), and $\\mathrm{ok}_{\\mathrm{single}}$ and $\\mathrm{ok}_{\\mathrm{double}}$ are boolean values for the boundary property. The numbers must be printed as standard decimal numerals. For example, the output must look like $[x_1,x_2,x_3,x_4,True,True]$ with the actual computed values substituted.\n\nAngle units and physical units are not applicable for this problem. All numerical values must be reported as unitless real numbers or booleans. The program must be self-contained and must not require any input. The result must be reproducible across compliant implementations of IEEE Standard $754$ arithmetic.", "solution": "The problem requires the empirical and theoretical determination of machine epsilon for single-precision and double-precision floating-point numbers as specified by the Institute of Electrical and Electronics Engineers (IEEE) Standard $754$. The validation of the problem statement confirms its scientific soundness, consistency, and completeness. We may therefore proceed with a formal solution.\n\nThe solution is grounded in the principles of binary floating-point representation. According to the IEEE $754$ standard, a normalized floating-point number $x$ is represented in base $b=2$ as:\n$$x = (-1)^s \\times m \\times 2^e$$\nwhere $s$ is the sign bit, $e$ is the exponent, and $m$ is the significand (or mantissa). The significand is normalized to be in the range $1 \\le m < 2$ and is stored in the form $m = 1.f$, where the leading bit $1$ is implicit and $f$ is the fractional part stored explicitly. The total number of bits of precision in the significand is denoted by $p$. This includes the implicit leading bit. Thus, the fractional part $f$ is represented by $p-1$ bits.\n\nFor single-precision numbers, the parameter is $p=24$. For double-precision numbers, the parameter is $p=53$.\n\n**Theoretical Machine Epsilon**\n\nThe problem requires us to compute the theoretical spacing between the number $1$ and the next larger representable number. The number $1$ is exactly representable in binary floating-point arithmetic. Its normalized representation is:\n$$1.0 = (1.000...0)_2 \\times 2^0$$\nHere, the exponent is $e=0$, and the significand is $m=1$. The $p-1$ fractional bits are all zero.\n\nTo find the next representable floating-point number greater than $1$, we must make the smallest possible change. This is achieved by incrementing the least significant bit (LSB) of the significand's fractional part. The LSB corresponds to a value of $2^{-(p-1)}$. Thus, the significand of the next number is:\n$$m' = (1.000...01)_2 = 1 + 2^{-(p-1)}$$\nThe next representable number, $x_{\\text{next}}$, is therefore:\n$$x_{\\text{next}} = (1 + 2^{-(p-1)}) \\times 2^0 = 1 + 2^{-(p-1)}$$\nThe theoretical spacing, which we will call $\\epsilon_{\\mathrm{theory}}$, is the difference between $x_{\\text{next}}$ and $1$:\n$$\\epsilon_{\\mathrm{theory}} = x_{\\text{next}} - 1 = (1 + 2^{-(p-1)}) - 1 = 2^{-(p-1)}$$\n\nUsing the given parameters:\n- For single precision ($p=24$):\n  $$\\epsilon_{\\mathrm{theory, single}} = 2^{-(24-1)} = 2^{-23}$$\n- For double precision ($p=53$):\n  $$\\epsilon_{\\mathrm{theory, double}} = 2^{-(53-1)} = 2^{-52}$$\n\n**Empirical Estimation Procedure**\n\nThe problem specifies an iterative algorithm to find the machine epsilon empirically. We start with $\\epsilon = 1$ and repeatedly divide it by $2$ until the condition $(1 + \\epsilon)_{FP} = 1$ is met, where the subscript $FP$ denotes a floating-point operation. The algorithm is to return the last value of $\\epsilon$ for which $(1 + \\epsilon)_{FP} > 1$.\n\nLet us analyze this procedure. We are testing values $\\epsilon_k = 2^{-k}$ for $k=0, 1, 2, \\dots$. The operation is the floating-point addition of $1$ and $\\epsilon_k$. The exact mathematical sum is $1 + 2^{-k}$. To perform the addition, the operands' exponents must be aligned. The number $1$ is $1.0 \\times 2^0$, and $\\epsilon_k$ is $1.0 \\times 2^{-k}$. The sum is performed by right-shifting the significand of the smaller number:\n$$1.0 \\times 2^0 + 1.0 \\times 2^{-k} = (1.00...0_2 + 0.00...1_2) \\times 2^0$$\nwhere the $1$ in the second term is at the $k$-th position after the binary point. The result's significand has $p-1$ fractional bits for storage.\n\n- If $k < p-1$, the bit at position $k$ is within the representable part of the significand. The sum $1+2^{-k}$ is exactly representable, and the result is greater than $1$.\n- If $k = p-1$, the sum is $1+2^{-(p-1)}$. This is the next representable number after $1$, as derived earlier. So, $(1+2^{-(p-1)})_{FP} = 1+2^{-(p-1)} > 1$.\n- If $k=p$, the mathematical sum is $1+2^{-p}$. This value lies exactly halfway between the two consecutive representable numbers $N_1=1$ and $N_2=1+2^{-(p-1)}$.\n  $$1+2^{-p} = 1 + \\frac{1}{2}(2^{-(p-1)}) = \\frac{N_1 + N_2}{2}$$\n  The IEEE $754$ standard specifies \"round to nearest, ties to even\". We must round to the neighbor whose significand has a least significant bit of $0$.\n  The significand of $N_1=1$ is $(1.00...0)_2$. Its LSB is $0$.\n  The significand of $N_2=1+2^{-(p-1)}$ is $(1.00...1)_2$. Its LSB is $1$.\n  Therefore, the tie is rounded down to $N_1=1$. So, $(1+2^{-p})_{FP} = 1$.\n- If $k > p$, the sum $1+2^{-k}$ is closer to $1$ than to $1+2^{-(p-1)}$, so it also rounds to $1$.\n\nThe iterative procedure specified in the problem, \"repeatedly halve until adding it to $1$ no longer changes the value...return the last $\\epsilon$ that did change the value,\" will stop when it tests $\\epsilon_k = 2^{-p}$. At this point, $(1+\\epsilon_k)_{FP} = 1$. The algorithm returns the previous value of epsilon, which was $\\epsilon_{p-1}=2^{-(p-1)}$. Therefore, the empirically determined epsilon is:\n$$\\epsilon_{\\mathrm{emp}} = 2^{-(p-1)}$$\n\n**Verification and Error Calculation**\n\nGiven the derivations above, we have $\\epsilon_{\\mathrm{emp}} = \\epsilon_{\\mathrm{theory}} = 2^{-(p-1)}$.\nThe relative error is therefore:\n$$r = \\frac{|\\epsilon_{\\mathrm{emp}} - \\epsilon_{\\mathrm{theory}}|}{\\epsilon_{\\mathrm{theory}}} = \\frac{|2^{-(p-1)} - 2^{-(p-1)}|}{2^{-(p-1)}} = 0$$\nThis must hold true for both single and double precision.\n\nThe boundary property verification requires checking two conditions:\n$1$. $(1 + \\epsilon_{\\mathrm{emp}})_{FP} > 1$:\n    This is $(1 + 2^{-(p-1)})_{FP} > 1$. As $1+2^{-(p-1)}$ is exactly representable and the next value after $1$, this is true.\n$2$. $(1 + \\epsilon_{\\mathrm{emp}}/2)_{FP} = 1$:\n    This is $(1 + 2^{-(p-1)}/2)_{FP} = (1 + 2^{-p})_{FP}$. As shown by the rounding rule analysis, this expression evaluates to $1$. This is also true.\n\nConsequently, the boolean checks for the boundary property should both yield true.\n\n**Implementation Strategy**\n\nThe solution will be implemented in Python using the `numpy` library to access `np.float32` (single precision) and `np.float64` (double precision) types. A function will perform the empirical calculation for a given floating-point type by implementing an iterative halving loop. The main program will execute this for both `np.float32` and `np.float64` types. The theoretical values, relative errors, and boundary property booleans will be computed as derived. All six required values ($\\epsilon_{\\mathrm{single}}$, $\\epsilon_{\\mathrm{double}}$, $r_{\\mathrm{single}}$, $r_{\\mathrm{double}}$, $\\mathrm{ok}_{\\mathrm{single}}$, $\\mathrm{ok}_{\\mathrm{double}}$) will be collected and printed in the specified format. The calculations must ensure that intermediate floating-point types are handled correctly, for instance by using `np.float32(1)` when performing single-precision tests.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically determines and theoretically validates machine epsilon for single\n    and double precision floating-point numbers as per IEEE 754.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (numpy_type, significand_bits_p, name)\n    test_cases = [\n        (np.float32, 24, \"single\"),\n        (np.float64, 53, \"double\"),\n    ]\n\n    results_dict = {}\n\n    for precision_type, p, name in test_cases:\n        # Task 1: Empirically determine machine epsilon.\n        # The specified procedure is to start with epsilon = 1 and repeatedly\n        # halve it. The last epsilon for which 1 + epsilon > 1 is the result.\n        # A more direct loop finds the smallest representable power of 2, epsilon,\n        # such that 1 + epsilon/2 == 1, which confirms that epsilon is the smallest\n        # representable number that makes a difference when added to 1.\n        one = precision_type(1)\n        two = precision_type(2)\n        epsilon = precision_type(1)\n        \n        while one + (epsilon / two) != one:\n            epsilon = epsilon / two\n        \n        epsilon_emp = epsilon\n        \n        # Task 2: Compute the theoretical spacing.\n        # As derived from IEEE 754 principles, the theoretical spacing between 1\n        # and the next representable number is 2^-(p-1).\n        # We use Python's native float (double precision) for this calculation\n        # to ensure high accuracy for the reference value.\n        epsilon_theory = 2.0**(-(p - 1))\n\n        # Task 3: Compute the relative error.\n        # The empirical value should be identical to the theoretical one.\n        # The relative error is computed with high-precision values.\n        relative_error = np.abs(float(epsilon_emp) - epsilon_theory) / epsilon_theory\n        \n        # Task 4: Verify the boundary property.\n        # This confirms our understanding of the epsilon value. We must check that\n        # 1 + epsilon_emp > 1 but 1 + epsilon_emp/2 = 1, all in the target precision.\n        boundary_ok = (one + epsilon_emp > one) and \\\n                      (one + epsilon_emp / two == one)\n\n        # Store results for this precision.\n        results_dict[name] = {\n            \"epsilon_emp\": epsilon_emp,\n            \"relative_error\": relative_error,\n            \"boundary_ok\": boundary_ok\n        }\n\n    # Assemble the final list of results in the specified order.\n    # [eps_single, eps_double, r_single, r_double, ok_single, ok_double]\n    final_results = [\n        results_dict[\"single\"][\"epsilon_emp\"],\n        results_dict[\"double\"][\"epsilon_emp\"],\n        results_dict[\"single\"][\"relative_error\"],\n        results_dict[\"double\"][\"relative_error\"],\n        results_dict[\"single\"][\"boundary_ok\"],\n        results_dict[\"double\"][\"boundary_ok\"]\n    ]\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) function correctly converts floats and booleans to their\n    # standard string representations (e.g., \"True\", \"False\").\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2395229"}, {"introduction": "The existence of machine epsilon is not just a theoretical curiosity; it has profound consequences for the accuracy of numerical algorithms. This final exercise demonstrates one of the most classic and important principles in numerical analysis: the order of operations matters. By computing a series sum in both forward and reverse order, you will witness firsthand how the accumulation of small rounding errors can lead to dramatically different results, revealing why thoughtful algorithm design is crucial for reliable scientific computation [@problem_id:2393710].", "problem": "You are given the finite alternating harmonic sum defined by $S_N = \\sum_{k=1}^{N} \\frac{(-1)^k}{k}$ for a positive integer $N$. Consider computing $S_N$ using the standard double-precision floating-point arithmetic provided by your programming language. Define two summation orders: (i) forward order, which accumulates from $k=1$ to $k=N$, and (ii) reverse order, which accumulates from $k=N$ to $k=1$. For each computation, measure the absolute rounding error by comparing the double-precision result with a reference value that approximates the exact finite sum $S_N$ to within an absolute tolerance smaller than $10^{-30}$.\n\nYour program must, for each test value of $N$ specified below, compute:\n- the absolute error of the forward-order sum, denoted $E_{\\mathrm{fwd}}(N) = \\left| \\widehat{S}^{\\mathrm{fwd}}_N - S_N \\right|$,\n- the absolute error of the reverse-order sum, denoted $E_{\\mathrm{rev}}(N) = \\left| \\widehat{S}^{\\mathrm{rev}}_N - S_N \\right|$,\n- a boolean indicator $B(N)$ that is $\\mathrm{True}$ if $E_{\\mathrm{rev}}(N) &lt; E_{\\mathrm{fwd}}(N)$ and $\\mathrm{False}$ otherwise.\n\nThe reference value used for $S_N$ must match the exact finite sum within an absolute error strictly less than $10^{-30}$. Angles are not involved. No physical units are involved.\n\nTest suite (four cases):\n- $N = 1$,\n- $N = 2$,\n- $N = 10000$,\n- $N = 100000$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the test suite as a comma-separated list of lists, each inner list having the form $[E_{\\mathrm{fwd}}(N),E_{\\mathrm{rev}}(N),B(N)]$, enclosed in a single pair of square brackets. For example, the required structure is $[[e_1,e_2,b_1],[e_3,e_4,b_2],\\dots]$, where each $e_i$ is a floating-point decimal number and each $b_i$ is a boolean literal $\\mathrm{True}$ or $\\mathrm{False}$. There must be no additional text in the output line.", "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed problem in numerical analysis, grounded in the established principles of floating-point arithmetic. It is objective, self-contained, and computationally feasible. We proceed with the solution.\n\nThe problem requires a comparison of the numerical accuracy of two different summation orders for the finite alternating harmonic sum $S_N = \\sum_{k=1}^{N} \\frac{(-1)^k}{k}$. The two orders are forward, from $k=1$ to $k=N$, and reverse, from $k=N$ to $k=1$. The analysis hinges on the nature of floating-point arithmetic and the accumulation of round-off errors.\n\n**Fundamental Principle: Floating-Point Arithmetic and Round-off Error**\n\nStandard double-precision floating-point numbers, as defined by the IEEE $754$ standard, represent real numbers using a finite number of bits (a $52$-bit mantissa, an $11$-bit exponent, and a $1$-bit sign). This finite representation means that most real numbers cannot be stored exactly. The relative error of this representation is bounded by the machine epsilon, $\\epsilon_{\\mathrm{mach}}$, which for double-precision is approximately $2.22 \\times 10^{-16}$.\n\nA critical source of error in numerical computation is the addition of two numbers of vastly different magnitudes. Let us consider adding a small number $y$ to a large number $x$. The operation is performed as $\\mathrm{fl}(x+y)$, where $\\mathrm{fl}(\\cdot)$ denotes the floating-point representation. To perform the addition, the exponents of $x$ and $y$ must be aligned. This involves right-shifting the mantissa of the number with the smaller exponent. If $|y| \\ll |x|$, the mantissa of $y$ is shifted so far to the right that its least significant bits are lost. In the extreme case where $|y| < \\epsilon_{\\mathrm{mach}}|x|$, the addition yields $\\mathrm{fl}(x+y) = x$, and the information contained in $y$ is completely lost. This phenomenon is a form of loss of significance.\n\n**Analysis of Summation Orders**\n\nThe series in question is composed of terms $t_k = \\frac{(-1)^k}{k}$. The absolute values of these terms, $|t_k| = \\frac{1}{k}$, form a strictly decreasing sequence for $k \\ge 1$.\n\n1.  **Forward Summation ($\\widehat{S}^{\\mathrm{fwd}}_N$):** The summation proceeds as $S_{\\mathrm{fwd}} = ( \\dots ((t_1 + t_2) + t_3) + \\dots + t_N)$. The first term is $t_1 = -1$. The partial sum $s_m = \\sum_{k=1}^{m} t_k$ rapidly approaches the limit of the infinite series, which is $-\\ln(2) \\approx -0.693$. For any subsequent step $m+1$ where $m$ is large, we are adding a term $t_{m+1}$ with small magnitude (i.e., $|t_{m+1}| = \\frac{1}{m+1}$) to a partial sum $s_m$ of significantly larger magnitude. This is precisely the scenario described above where loss of significance occurs. The precision of each small term $t_k$ for large $k$ is progressively eroded as it is added to the growing sum. The accumulated round-off error is therefore expected to be substantial for large $N$.\n\n2.  **Reverse Summation ($\\widehat{S}^{\\mathrm{rev}}_N$):** The summation proceeds as $S_{\\mathrm{rev}} = ( \\dots ((t_N + t_{N-1}) + t_{N-2}) + \\dots + t_1)$. This order adheres to a fundamental principle of accurate numerical summation: sum the numbers in increasing order of their absolute values. Here, we begin by adding the terms with the smallest magnitudes, $t_N$ and $t_{N-1}$. The partial sum grows slowly in magnitude. At each step, a new term is added to a partial sum of a more comparable magnitude than in the forward case. This procedure minimizes the loss of significance at each step because the mantissas of the numbers being added are better aligned. Consequently, the reverse summation order is expected to yield a much more accurate result with a smaller accumulated round-off error.\n\n**High-Precision Reference Value ($S_N$)**\n\nTo quantify the errors $E_{\\mathrm{fwd}}(N)$ and $E_{\\mathrm{rev}}(N)$, we require a reference value for $S_N$ that is significantly more accurate than the double-precision results. The problem specifies that the reference must be accurate to within an absolute tolerance of $10^{-30}$. Standard double-precision arithmetic is insufficient for this. We must use arbitrary-precision arithmetic. By setting the precision to a high level, for example $50$ decimal digits, we can compute the sum $S_N = \\sum_{k=1}^{N} \\frac{(-1)^k}{k}$ such that the computational error in the reference value is negligible (e.g., on the order of $N \\times 10^{-50}$), satisfying the problem's accuracy requirement.\n\n**Expected Results**\n\n-   For small values of $N$, such as $N=1$ and $N=2$, the terms ($t_1=-1$, $t_2=0.5$) are exactly representable in binary floating-point. The sums are trivial and exact. Therefore, we expect $E_{\\mathrm{fwd}}(N) = E_{\\mathrm{rev}}(N) = 0$, leading to $B(N)=\\mathrm{False}$.\n-   For large values of $N$, such as $N=10000$ and $N=100000$, the accumulated round-off errors will become significant. Based on the analysis above, the error in the reverse sum should be smaller than the error in the forward sum. We expect $E_{\\mathrm{rev}}(N) < E_{\\mathrm{fwd}}(N)$, which means $B(N)$ should be $\\mathrm{True}$.\n\nThe implementation will consist of three functions: one to compute the high-precision reference sum, and two to compute the forward and reverse sums using standard floating-point arithmetic. The errors are then calculated by taking the absolute difference between the floating-point results and the high-precision reference.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares the round-off error for forward and reverse summation\n    of the alternating harmonic series.\n    \"\"\"\n    # Set the precision for the Decimal context to ensure the reference value\n    # is accurate to well below the required 1e-30 tolerance. 50 digits is sufficient.\n    getcontext().prec = 50\n\n    def compute_reference_sum(n_val: int) -> Decimal:\n        \"\"\"\n        Computes the sum S_N using high-precision Decimal arithmetic to serve as\n        a reference value.\n        \"\"\"\n        s = Decimal(0)\n        for k in range(1, n_val + 1):\n            s += Decimal(-1)**k / Decimal(k)\n        return s\n\n    def compute_forward_sum(n_val: int) -> float:\n        \"\"\"\n        Computes the sum S_N in forward order (k=1 to N) using standard\n        double-precision floats. A Python loop is used to strictly enforce\n        the accumulation order.\n        \"\"\"\n        s = 0.0\n        for k in range(1, n_val + 1):\n            s += ((-1)**k) / k\n        return s\n\n    def compute_reverse_sum(n_val: int) -> float:\n        \"\"\"\n        Computes the sum S_N in reverse order (k=N to 1) using standard\n        double-precision floats.\n        \"\"\"\n        s = 0.0\n        for k in range(n_val, 0, -1):\n            s += ((-1)**k) / k\n        return s\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1,\n        2,\n        10000,\n        100000,\n    ]\n\n    results = []\n    for n in test_cases:\n        # 1. Compute the high-precision reference sum.\n        s_ref = compute_reference_sum(n)\n\n        # 2. Compute the forward sum and its absolute error.\n        s_fwd = compute_forward_sum(n)\n        # For accurate error calculation, convert the float to Decimal before subtraction.\n        e_fwd = abs(Decimal(s_fwd) - s_ref)\n\n        # 3. Compute the reverse sum and its absolute error.\n        s_rev = compute_reverse_sum(n)\n        e_rev = abs(Decimal(s_rev) - s_ref)\n\n        # 4. Determine the boolean indicator: True if reverse error is less than forward error.\n        b_n = e_rev < e_fwd\n\n        # 5. Store the results, converting Decimal errors to float for the output.\n        #    The output format calls for floating-point decimal numbers.\n        results.append([float(e_fwd), float(e_rev), b_n])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) is used to correctly format boolean literals and floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2393710"}]}