{"hands_on_practices": [{"introduction": "Let's begin with a very direct and tangible example of an ill-posed problem. This exercise [@problem_id:2197153] demonstrates how a seemingly minor change in input data—akin to a small measurement error—can lead to dramatically different results. By solving a simple $2 \\times 2$ linear system, you will witness firsthand the practical consequences of instability, a failure of Hadamard's third criterion for well-posedness.", "problem": "In many scientific and engineering applications, we encounter systems of linear equations of the form $Ax = b$, where we need to find the vector $x$ given a matrix $A$ and a vector $b$. The vector $b$ often represents measurements, which are subject to small errors. An ill-conditioned system is one where small errors in $b$ can lead to large errors in the solution $x$.\n\nConsider the following ill-conditioned linear system:\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{pmatrix}\n$$\nLet's first consider the case where the measurement vector is exactly:\n$$\nb_{orig} = \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}\n$$\nAnd the corresponding solution vector is $x_{orig}$, satisfying $A x_{orig} = b_{orig}$.\n\nNow, suppose a small measurement error occurs, leading to a new, perturbed measurement vector:\n$$\nb_{pert} = \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}\n$$\nThe new solution vector is $x_{pert}$, which satisfies $A x_{pert} = b_{pert}$.\n\nYour task is to quantify the effect of this small perturbation in the measurement vector. Calculate the magnitude of the change in the solution vector, which is given by the Euclidean norm of the difference, $\\| x_{pert} - x_{orig} \\|_2$. Round your final answer to three significant figures.", "solution": "We solve both linear systems using $x = A^{-1} b$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Here,\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{pmatrix},\n\\quad \\det(A) = 1 \\cdot 1.001 - 1 \\cdot 1 = 0.001.\n$$\nThus,\n$$\nA^{-1} = \\frac{1}{0.001} \\begin{pmatrix} 1.001 & -1 \\\\ -1 & 1 \\end{pmatrix} = 1000 \\begin{pmatrix} 1.001 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix}.\n$$\nFor $b_{orig} = \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}$,\n$$\nx_{orig} = A^{-1} b_{orig} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}\n= \\begin{pmatrix} 1001 \\cdot 2 - 1000 \\cdot 2.001 \\\\ -1000 \\cdot 2 + 1000 \\cdot 2.001 \\end{pmatrix}\n= \\begin{pmatrix} 2002 - 2001 \\\\ -2000 + 2001 \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nFor $b_{pert} = \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}$,\n$$\nx_{pert} = A^{-1} b_{pert} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}\n= \\begin{pmatrix} 1001 \\cdot 2 - 1000 \\cdot 2.002 \\\\ -1000 \\cdot 2 + 1000 \\cdot 2.002 \\end{pmatrix}\n= \\begin{pmatrix} 2002 - 2002 \\\\ -2000 + 2002 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}.\n$$\nThe change in the solution is\n$$\nx_{pert} - x_{orig} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix},\n$$\nand its Euclidean norm is\n$$\n\\|x_{pert} - x_{orig}\\|_{2} = \\sqrt{(-1)^{2} + 1^{2}} = \\sqrt{2}.\n$$\nRounding $\\sqrt{2}$ to three significant figures gives $1.41$.", "answer": "$$\\boxed{1.41}$$", "id": "2197153"}, {"introduction": "Building on the numerical demonstration of instability, this next problem [@problem_id:2225884] challenges you to generalize the concept. Instead of using specific numbers, you will derive a symbolic 'amplification factor' that quantifies how sensitive a solution is to input errors. This practice connects the geometric intuition of nearly-parallel lines to a precise mathematical measure of ill-conditioning.", "problem": "A manufacturing process uses two independent laser-based sensors to locate a critical point on a component. The component is placed on a 2D Cartesian plane. Sensor 1 reports that the point lies along the line $L_1$ with the equation $y = (m_0 + \\epsilon)x + c_1$. Sensor 2 reports that the point lies along the line $L_2$ with the equation $y = m_0 x + c_2$. In these equations, $m_0, c_1, c_2$ are known parameters, and $\\epsilon$ is a very small, non-zero parameter representing a slight angular misalignment of Sensor 1. The calculated position of the critical point is the intersection of lines $L_1$ and $L_2$.\n\nDue to thermal fluctuations, the intercept measurement of Sensor 2 is subject to a small error. The new reported line is $L'_2: y = m_0 x + (c_2 + \\delta)$, where $\\delta$ represents this small measurement error. This error leads to a change in the calculated x-coordinate of the critical point.\n\nWe are interested in the sensitivity of the system's x-coordinate calculation to this measurement error. This is quantified by the amplification factor, $K$, defined as the ratio of the absolute magnitude of the change in the calculated x-coordinate to the absolute magnitude of the input error.\n\nWhat is the expression for the amplification factor $K$?\n\nA. $K = |\\epsilon|$\n\nB. $K = 1$\n\nC. $K = \\frac{1}{|\\epsilon|}$\n\nD. $K = \\frac{|\\delta|}{|\\epsilon|}$\n\nE. $K = \\frac{|c_1 - c_2|}{|\\epsilon|^2}$", "solution": "The x-coordinate of the intersection of two lines is obtained by equating their y-values. For the original lines $L_{1}: y = (m_{0} + \\epsilon)x + c_{1}$ and $L_{2}: y = m_{0}x + c_{2}$, set the right-hand sides equal:\n$$(m_{0} + \\epsilon)x + c_{1} = m_{0}x + c_{2}.$$\nSubtract $m_{0}x$ from both sides and isolate $x$:\n$$\\epsilon x + c_{1} = c_{2} \\quad \\Rightarrow \\quad \\epsilon x = c_{2} - c_{1} \\quad \\Rightarrow \\quad x = \\frac{c_{2} - c_{1}}{\\epsilon}.$$\nWith the perturbed line $L'_{2}: y = m_{0}x + (c_{2} + \\delta)$, repeat the intersection calculation:\n$$(m_{0} + \\epsilon)x' + c_{1} = m_{0}x' + c_{2} + \\delta.$$\nAgain subtract $m_{0}x'$ and isolate $x'$:\n$$\\epsilon x' + c_{1} = c_{2} + \\delta \\quad \\Rightarrow \\quad \\epsilon x' = c_{2} - c_{1} + \\delta \\quad \\Rightarrow \\quad x' = \\frac{c_{2} - c_{1}}{\\epsilon} + \\frac{\\delta}{\\epsilon}.$$\nThe change in the computed x-coordinate due to the input error $\\delta$ is\n$$\\Delta x = x' - x = \\frac{\\delta}{\\epsilon}.$$\nThe amplification factor is defined as\n$$K = \\frac{|\\Delta x|}{|\\delta|} = \\frac{\\left|\\frac{\\delta}{\\epsilon}\\right|}{|\\delta|} = \\frac{1}{|\\epsilon|}.$$\nThis corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "2225884"}, {"introduction": "Ill-posed problems are not only about sensitivity to noise; sometimes, the issue is more fundamental. This final practice [@problem_id:2225888] moves into the common application of data fitting and explores a different kind of ill-posedness: the failure to produce a unique solution. You will investigate a physical model and discover how a poor choice of basis functions can make it impossible to uniquely determine the model's parameters, a violation of Hadamard's second criterion.", "problem": "A physicist is analyzing the behavior of an electromagnetic field mode within a resonant cavity. The amplitude of the mode, $V(t)$, is measured at various distinct times. Theoretical analysis suggests that the amplitude can be modeled by the function:\n$V(t) = c_1 \\phi_1(t) + c_2 \\phi_2(t) + c_3 \\phi_3(t)$\nwhere the basis functions are $\\phi_1(t) = 1$, $\\phi_2(t) = \\sin^2(\\omega t)$, and $\\phi_3(t) = \\cos(2\\omega t)$. Here, $\\omega$ is a known, non-zero constant angular frequency, and $c_1, c_2, c_3$ are the unknown real coefficients to be determined.\n\nTo find these coefficients, the physicist takes $m=10$ measurements $(t_i, V_i)$ at 10 distinct non-zero times $t_1, t_2, \\dots, t_{10}$. This experimental setup leads to an overdetermined linear system $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = [c_1, c_2, c_3]^T$ is the vector of coefficients, $\\mathbf{b} = [V_1, V_2, \\dots, V_{10}]^T$ is the vector of measured amplitudes, and $A$ is the corresponding $10 \\times 3$ coefficient matrix. The physicist decides to find the coefficients by computing the least-squares solution $\\mathbf{x}_{LS}$, which is the vector $\\mathbf{x}$ that minimizes the sum of squared residuals, $\\|A\\mathbf{x} - \\mathbf{b}\\|_2^2$.\n\nWhich of the following statements most accurately describes the problem of finding this least-squares solution $\\mathbf{x}_{LS}$?\n\nA. The problem is ill-posed because a least-squares solution does not exist for an overdetermined system.\n\nB. The problem is well-posed, as a unique least-squares solution exists because all measurement times $t_i$ are distinct.\n\nC. The problem is ill-posed because there are infinitely many least-squares solutions.\n\nD. The problem is well-posed, but it is ill-conditioned because the basis functions are periodic.\n\nE. Whether a unique least-squares solution exists depends on the specific numerical values of the measured amplitudes $V_i$ in the vector $\\mathbf{b}$.", "solution": "The problem asks us to determine the nature of the least-squares problem for finding the coefficients $\\mathbf{x} = [c_1, c_2, c_3]^T$ in the model $V(t) = c_1 + c_2 \\sin^2(\\omega t) + c_3 \\cos(2\\omega t)$. The problem is set up as an overdetermined linear system $A\\mathbf{x} = \\mathbf{b}$, where the least-squares solution $\\mathbf{x}_{LS}$ minimizes $\\|A\\mathbf{x} - \\mathbf{b}\\|_2^2$.\n\nA problem is considered well-posed (in the sense of Hadamard) if it satisfies three criteria:\n1.  A solution exists.\n2.  The solution is unique.\n3.  The solution's behavior changes continuously with the initial conditions (stability).\n\nLet's analyze the given least-squares problem with respect to these criteria, focusing on existence and uniqueness.\n\nFirst, let's construct the matrix $A$. The system of equations is formed by evaluating the model at each measurement time $t_i$:\n$c_1 \\phi_1(t_i) + c_2 \\phi_2(t_i) + c_3 \\phi_3(t_i) = V_i$ for $i=1, \\dots, 10$.\nThis translates to the matrix equation $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ has entries $A_{ij} = \\phi_j(t_i)$.\n$$\nA = \\begin{pmatrix}\n\\phi_1(t_1) & \\phi_2(t_1) & \\phi_3(t_1) \\\\\n\\phi_1(t_2) & \\phi_2(t_2) & \\phi_3(t_2) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\phi_1(t_{10}) & \\phi_2(t_{10}) & \\phi_3(t_{10})\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & \\sin^2(\\omega t_1) & \\cos(2\\omega t_1) \\\\\n1 & \\sin^2(\\omega t_2) & \\cos(2\\omega t_2) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & \\sin^2(\\omega t_{10}) & \\cos(2\\omega t_{10})\n\\end{pmatrix}\n$$\n\nNow, we check for existence. A least-squares solution $\\mathbf{x}_{LS}$ always exists for any $m \\times n$ matrix $A$ and any vector $\\mathbf{b}$ in $\\mathbb{R}^m$. The solution corresponds to finding the vector in the column space of $A$ that is closest to $\\mathbf{b}$, which is the orthogonal projection of $\\mathbf{b}$ onto the column space. This projection always exists and is unique. Since a vector in the column space can be written as $A\\mathbf{x}$ for some $\\mathbf{x}$, at least one such $\\mathbf{x}$ (a least-squares solution) is guaranteed to exist. Thus, the existence criterion is met. This immediately shows that option A is incorrect.\n\nNext, we check for uniqueness. The least-squares solution $\\mathbf{x}_{LS}$ is unique if and only if the columns of the matrix $A$ are linearly independent. If the columns are linearly independent, the matrix $A^T A$ is invertible, and the unique solution is given by the normal equations: $\\mathbf{x}_{LS} = (A^T A)^{-1} A^T \\mathbf{b}$. If the columns are linearly dependent, $A^T A$ is singular, and there are infinitely many solutions.\n\nLet's examine the columns of $A$. Let $\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3$ be the three columns.\n$\\mathbf{a}_1$ corresponds to $\\phi_1(t) = 1$.\n$\\mathbf{a}_2$ corresponds to $\\phi_2(t) = \\sin^2(\\omega t)$.\n$\\mathbf{a}_3$ corresponds to $\\phi_3(t) = \\cos(2\\omega t)$.\n\nWe recall the double-angle trigonometric identity: $\\cos(2\\theta) = 1 - 2\\sin^2(\\theta)$.\nApplying this identity to our basis functions with $\\theta = \\omega t$, we get:\n$\\cos(2\\omega t) = 1 - 2\\sin^2(\\omega t)$\nThis can be written in terms of our basis functions as:\n$\\phi_3(t) = \\phi_1(t) - 2\\phi_2(t)$\n\nThis relationship holds for any value of $t$, and therefore it holds for all measurement times $t_1, t_2, \\dots, t_{10}$. For the columns of matrix $A$, this implies a linear relationship:\n$\\mathbf{a}_3 = \\mathbf{a}_1 - 2\\mathbf{a}_2$\nThis can be rewritten as:\n$1 \\cdot \\mathbf{a}_1 - 2 \\cdot \\mathbf{a}_2 - 1 \\cdot \\mathbf{a}_3 = \\mathbf{0}$\nThis is a linear combination of the columns of $A$ with non-zero coefficients $(1, -2, -1)$ that results in the zero vector. Therefore, the columns of $A$ are linearly dependent.\n\nSince the columns of $A$ are linearly dependent, the matrix $A$ is not of full column rank. This means the null space of $A$ is non-trivial. If $\\mathbf{x}_{LS}$ is one particular solution to the least-squares problem, then for any non-zero vector $\\mathbf{z}$ in the null space of $A$ (e.g., $\\mathbf{z} = [1, -2, -1]^T$), the vector $\\mathbf{x}' = \\mathbf{x}_{LS} + k\\mathbf{z}$ for any scalar $k$ is also a least-squares solution. This is because $\\|A\\mathbf{x}' - \\mathbf{b}\\|_2 = \\|A(\\mathbf{x}_{LS} + k\\mathbf{z}) - \\mathbf{b}\\|_2 = \\|(A\\mathbf{x}_{LS} - \\mathbf{b}) + k(A\\mathbf{z})\\|_2 = \\|A\\mathbf{x}_{LS} - \\mathbf{b}\\|_2$, since $A\\mathbf{z}=\\mathbf{0}$. Because there are infinitely many such vectors (one for each real number $k$), the least-squares solution is not unique.\n\nThe failure of the uniqueness criterion means the problem is ill-posed.\n\nLet's evaluate the given options based on this analysis:\nA. Incorrect. A least-squares solution always exists.\nB. Incorrect. The solution is not unique, even though the times $t_i$ are distinct. The linear dependence arises from the choice of basis functions, not the sampling points.\nC. Correct. The problem is ill-posed because a solution exists, but it is not unique, leading to infinitely many solutions.\nD. Incorrect. The problem is fundamentally ill-posed due to non-uniqueness. An ill-conditioned but well-posed problem would have a unique but sensitive solution. Here, uniqueness itself fails.\nE. Incorrect. The uniqueness of the least-squares solution is a property of the matrix $A$ alone, determined by the linear independence of its columns. It does not depend on the measurement vector $\\mathbf{b}$.\n\nTherefore, the problem is ill-posed because there are infinitely many least-squares solutions.", "answer": "$$\\boxed{C}$$", "id": "2225888"}]}