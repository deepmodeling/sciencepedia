{"hands_on_practices": [{"introduction": "The efficiency of a fixed-point iteration $x_{n+1} = g(x_n)$ depends critically on the choice of the function $g(x)$. This first practice delves into the design of an iteration that converges quadratically, a significant improvement over linear convergence. You will explore how to enforce the condition $g'(r) = 0$ at a root $r$ to achieve this speed-up, providing a bridge between the general theory of fixed-point methods and the development of powerful algorithms like Newton's method [@problem_id:2393364]. This exercise is fundamental to understanding how the local behavior of the iteration function dictates its performance.", "problem": "In computational engineering, fixed-point iterations are fundamental for solving nonlinear algebraic equations that arise in discretizations of governing equations. Consider the scalar nonlinear equation\n$$\nf(x) = x^{3} - 6x^{2} + 11x - 6 = 0,\n$$\nwhich has three simple real roots at $x = 1$, $x = 2$, and $x = 3$. For each root $r_{k} \\in \\{1, 2, 3\\}$, construct a fixed-point iteration of the relaxation form\n$$\nx_{n+1} = g_{k}(x_{n}), \\quad g_{k}(x) = x - \\alpha_{k}\\, f(x),\n$$\nwith a constant parameter $\\alpha_{k}$ chosen so that the iteration is locally of at least quadratic order about the target root $r_{k}$. Then determine the exact values of $\\alpha_{1}$, $\\alpha_{2}$, and $\\alpha_{3}$ corresponding to $r_{1} = 1$, $r_{2} = 2$, and $r_{3} = 3$, respectively, and compute\n$$\nS = \\alpha_{1} + \\alpha_{2} + \\alpha_{3}.\n$$\nProvide $S$ as a single exact value. No rounding is required.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\nStep 1: Extract Givens.\nThe provided information is as follows:\n- The nonlinear scalar equation is $f(x) = x^{3} - 6x^{2} + 11x - 6 = 0$.\n- The equation has three simple real roots: $r_{1} = 1$, $r_{2} = 2$, and $r_{3} = 3$.\n- A fixed-point iteration scheme is given for each root $r_{k}$: $x_{n+1} = g_{k}(x_{n})$.\n- The iteration function is defined in a relaxation form: $g_{k}(x) = x - \\alpha_{k}\\, f(x)$.\n- The parameter $\\alpha_{k}$ must be chosen such that the iteration is locally of at least quadratic order about the corresponding root $r_{k}$.\n- The objective is to compute the sum $S = \\alpha_{1} + \\alpha_{2} + \\alpha_{3}$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is assessed for validity:\n- **Scientifically Grounded:** The problem is rooted in the fundamental theory of numerical methods, specifically fixed-point iteration and its convergence criteria. The concept of convergence order (linear, quadratic, etc.) is a standard topic in computational science and engineering. The problem is scientifically sound.\n- **Well-Posed:** The problem provides a clear objective and sufficient information. The condition of \"at least quadratic order\" imposes a specific mathematical constraint on the iteration function's derivative, which allows for the unique determination of each parameter $\\alpha_{k}$. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical language, free from ambiguity, subjectivity, or opinion.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard, formalizable problem in numerical analysis.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A solution will be derived.\n\nThe core of the problem lies in the analysis of the convergence rate of a fixed-point iteration $x_{n+1} = g(x_{n})$. A fixed-point iteration converges to a root $r$ if $r$ is a fixed point of $g(x)$, i.e., $g(r) = r$, and if the iteration function is a contraction mapping in a neighborhood of $r$. The order of convergence is determined by the derivatives of $g(x)$ evaluated at the fixed point $r$.\n\nFor an iteration to be locally of at least quadratic order, the first derivative of the iteration function must be zero at the fixed point. That is, $g'(r) = 0$. If this condition is met, the Taylor series expansion of $g(x)$ about $r$ is:\n$$\ng(x) = g(r) + g'(r)(x-r) + \\frac{g''(r)}{2!}(x-r)^{2} + O((x-r)^{3})\n$$\nWith $g(r)=r$ and $g'(r)=0$, this becomes:\n$$\ng(x) - r = \\frac{g''(r)}{2}(x-r)^{2} + O((x-r)^{3})\n$$\nLet $e_{n} = x_{n} - r$ be the error at iteration $n$. Then $x_{n} = r + e_{n}$, and $x_{n+1} = g(x_{n})$.\n$$\ne_{n+1} = x_{n+1} - r = g(x_{n}) - r = g(r + e_{n}) - r = \\frac{g''(r)}{2} e_{n}^{2} + O(e_{n}^{3})\n$$\nThis relationship, where the error at step $n+1$ is proportional to the square of the error at step $n$, is the definition of quadratic convergence, provided that $g''(r) \\neq 0$. The condition \"at least quadratic\" requires $g'(r)=0$.\n\nFor each root $r_k$, the iteration function is $g_{k}(x) = x - \\alpha_{k} f(x)$. First, we confirm that each root $r_k$ is a fixed point of $g_k(x)$. By definition, $f(r_k) = 0$, so $g_{k}(r_{k}) = r_{k} - \\alpha_{k} f(r_{k}) = r_{k} - \\alpha_{k}(0) = r_{k}$. This is satisfied for any choice of $\\alpha_k$.\n\nTo ensure at least quadratic convergence, we must enforce the condition $g_{k}'(r_{k}) = 0$. We first compute the derivative of $g_{k}(x)$:\n$$\ng_{k}'(x) = \\frac{d}{dx} \\left( x - \\alpha_{k} f(x) \\right) = 1 - \\alpha_{k} f'(x)\n$$\nEvaluating this at the root $r_{k}$ and setting it to zero yields:\n$$\ng_{k}'(r_{k}) = 1 - \\alpha_{k} f'(r_{k}) = 0\n$$\nSolving for $\\alpha_{k}$, we find that the required value is:\n$$\n\\alpha_{k} = \\frac{1}{f'(r_{k})}\n$$\nThis assumes $f'(r_k) \\neq 0$, which is true for a simple root, as stated in the problem.\n\nNext, we must compute the derivative of the given function $f(x) = x^{3} - 6x^{2} + 11x - 6$:\n$$\nf'(x) = \\frac{d}{dx} \\left( x^{3} - 6x^{2} + 11x - 6 \\right) = 3x^{2} - 12x + 11\n$$\nNow we can compute the value of $\\alpha_{k}$ for each of the three roots.\n\nFor the root $r_{1} = 1$:\n$$\nf'(1) = 3(1)^{2} - 12(1) + 11 = 3 - 12 + 11 = 2\n$$\nThus, the parameter $\\alpha_{1}$ is:\n$$\n\\alpha_{1} = \\frac{1}{f'(1)} = \\frac{1}{2}\n$$\n\nFor the root $r_{2} = 2$:\n$$\nf'(2) = 3(2)^{2} - 12(2) + 11 = 3(4) - 24 + 11 = 12 - 24 + 11 = -1\n$$\nThus, the parameter $\\alpha_{2}$ is:\n$$\n\\alpha_{2} = \\frac{1}{f'(2)} = \\frac{1}{-1} = -1\n$$\n\nFor the root $r_{3} = 3$:\n$$\nf'(3) = 3(3)^{2} - 12(3) + 11 = 3(9) - 36 + 11 = 27 - 36 + 11 = 2\n$$\nThus, the parameter $\\alpha_{3}$ is:\n$$\n\\alpha_{3} = \\frac{1}{f'(3)} = \\frac{1}{2}\n$$\n\nFinally, the problem requires the computation of the sum $S = \\alpha_{1} + \\alpha_{2} + \\alpha_{3}$:\n$$\nS = \\frac{1}{2} + (-1) + \\frac{1}{2} = 1 - 1 = 0\n$$\nThe value of $S$ is exactly $0$.", "answer": "$$\\boxed{0}$$", "id": "2393364"}, {"introduction": "Having seen how to achieve quadratic convergence, a natural question arises: can we do even better? This practice challenges you to construct an iteration with a cubic order of convergence, where the error at each step shrinks by a power of three. To achieve this remarkable speed, the iteration function $g(x)$ must satisfy both $g'(\\alpha) = 0$ and $g''(\\alpha) = 0$ at the root $\\alpha$. By implementing what is known as Halley's method [@problem_id:2394865], you will gain direct experience with higher-order root-finding techniques and the trade-offs involved, such as the need for higher derivatives of your target function.", "problem": "You are to construct and apply a fixed-point iteration scheme with cubic convergence to approximate the real root of a nonlinear equation. Let a real-valued function be denoted by $f(x)$, with $x \\in \\mathbb{R}$. A fixed-point iteration is any mapping of the form $x_{n+1} = g(x_n)$ for some function $g(x)$. Your task is to devise a function $g(x)$ that yields a fixed-point iteration with local order of convergence equal to $3$ for solving $f(x) = 0$ when the root is simple, and then apply it to the test suite below.\n\nAssumptions for correctness and convergence analysis: $f$ is at least three times continuously differentiable in a neighborhood of the sought root $\\alpha$, with $f(\\alpha) = 0$ and $f'(\\alpha) \\neq 0$.\n\nTermination requirements: Your iteration must stop when either $|f(x_n)| \\leq 10^{-12}$ or $|x_{n+1} - x_n| \\leq 10^{-12}$, or after at most $50$ iterations, whichever occurs first. If your iteration reaches the maximum iteration count without meeting the accuracy threshold, return the last iterate obtained.\n\nAngles must be in radians for all trigonometric and exponential evaluations.\n\nTest suite: For each case, use the specified $f(x)$ with its analytical derivatives $f'(x)$ and $f''(x)$, the given initial guess $x_0$, and the termination criteria above.\n- Case $1$ (“happy path”): $f_1(x) = \\cos(x) - x$, $f_1'(x) = -\\sin(x) - 1$, $f_1''(x) = -\\cos(x)$, with $x_0 = 0.5$.\n- Case $2$ (far initial guess): $f_2(x) = x^3 - 2$, $f_2'(x) = 3 x^2$, $f_2''(x) = 6 x$, with $x_0 = 10.0$.\n- Case $3$ (moderate nonlinearity): $f_3(x) = e^x - 3 x$, $f_3'(x) = e^x - 3$, $f_3''(x) = e^x$, with $x_0 = 0.0$.\n\nNumerical output requirements: Your program must compute an approximate root for each case that satisfies the termination requirements, and then output all three approximations in a single line as a comma-separated list enclosed in square brackets. Each approximation must be rounded to $10$ decimal places. For example, your output format must be exactly of the form\n$[r_1,r_2,r_3]$\nwhere each $r_k$ is a decimal numeral with exactly $10$ digits after the decimal point.\n\nYour program must not read any input. It must produce exactly one line of output in the specified format containing the results for the three cases in the order listed above.", "solution": "The problem requires the construction and application of a fixed-point iteration scheme, $x_{n+1} = g(x_n)$, with a third-order (cubic) rate of convergence for finding a simple root $\\alpha$ of a nonlinear equation $f(x)=0$. A simple root is defined by the conditions $f(\\alpha)=0$ and $f'(\\alpha) \\neq 0$.\n\nFirst, we must derive the iteration function $g(x)$. The order of convergence of a fixed-point method is determined by the derivatives of the iteration function $g(x)$ at the fixed point $\\alpha$. For cubic convergence, we require that $g(\\alpha)=\\alpha$, and its first two derivatives vanish at the root, while the third does not:\n$$g'(\\alpha) = 0$$\n$$g''(\\alpha) = 0$$\n$$g'''(\\alpha) \\neq 0$$\n\nA well-known family of high-order root-finding methods is Householder's methods. The method for third-order convergence is commonly known as Halley's method. We can derive its formula by considering the Taylor series expansion of $f(x)$ around the current iterate $x_n$, and retaining terms up to the second order to approximate the correction step $\\delta = \\alpha - x_n$.\nThe root $\\alpha$ satisfies $f(\\alpha)=0$. Expanding $f$ around $x_n$:\n$$f(\\alpha) = 0 \\approx f(x_n) + f'(x_n)(\\alpha - x_n) + \\frac{f''(x_n)}{2}(\\alpha - x_n)^2$$\nLet $\\delta = \\alpha - x_n$. The equation becomes a quadratic in $\\delta$:\n$$f(x_n) + f'(x_n)\\delta + \\frac{f''(x_n)}{2}\\delta^2 \\approx 0$$\nTo obtain a refined approximation for $\\delta$, we can substitute the first-order approximation from Newton's method, $\\delta \\approx -f(x_n)/f'(x_n)$, into the second-order term:\n$$f(x_n) + f'(x_n)\\delta + \\frac{f''(x_n)}{2} \\left( -\\frac{f(x_n)}{f'(x_n)} \\right)^2 \\approx 0$$\nSolving for $\\delta$:\n$$f'(x_n)\\delta \\approx -f(x_n) - \\frac{f''(x_n) [f(x_n)]^2}{2 [f'(x_n)]^2}$$\n$$\\delta \\approx -\\frac{f(x_n)}{f'(x_n)} - \\frac{f''(x_n) [f(x_n)]^2}{2 [f'(x_n)]^3}$$\nThis yields an iterative formula. However, a more common and numerically stable form, Halley's method, is derived by rewriting the Taylor expansion as an approximation for $\\delta$:\n$$f'(x_n)\\delta \\approx -f(x_n) - \\frac{f''(x_n)}{2}\\delta^2$$\nDividing by $f'(x_n)$ gives $\\delta \\approx -\\frac{f(x_n)}{f'(x_n)} - \\frac{f''(x_n)}{2f'(x_n)}\\delta^2$. This is an implicit equation for $\\delta$. An iterative approach to this equation is to start with $\\delta_0=0$ and compute $\\delta_1 = -f(x_n)/f'(x_n)$, which is Newton's step. Then $\\delta_2 = \\frac{-f(x_n)}{f'(x_n) - \\frac{f''(x_n)}{2}\\delta_1}$. This is one of a few ways to motivate the structure.\n\nThe standard form of Halley's iteration is:\n$$x_{n+1} = x_n - \\frac{2 f(x_n) f'(x_n)}{2 [f'(x_n)]^2 - f(x_n) f''(x_n)}$$\nThis defines the iteration function $g(x)$:\n$$g(x) = x - \\frac{2 f(x) f'(x)}{2 [f'(x)]^2 - f(x) f''(x)}$$\nIt can be formally proven through Taylor expansion of the error term $\\epsilon_{n+1} = x_{n+1} - \\alpha$ that this method satisfies $\\epsilon_{n+1} \\propto \\epsilon_n^3$, confirming its cubic convergence for simple roots, provided $f$ is sufficiently smooth (at least $C^3$).\n\nThe algorithm for finding the root of each function is as follows:\n1.  Initialize the iteration with $n=0$ and the given starting value $x_0$.\n2.  For $n = 0, 1, 2, \\dots$ up to a maximum of $49$ iterations:\n    a. Let the current iterate be $x_n$.\n    b. Calculate $f(x_n)$, $f'(x_n)$, and $f''(x_n)$.\n    c. Check the first termination criterion: if $|f(x_n)| \\leq 10^{-12}$, the iteration is successful. The result is $x_n$. Terminate.\n    d. Calculate the denominator of the correction term: $D_n = 2 [f'(x_n)]^2 - f(x_n) f''(x_n)$. If $D_n$ is close to zero, the method may fail; however, the problem's assumptions preclude this near a simple root.\n    e. Calculate the next iterate: $x_{n+1} = x_n - \\frac{2 f(x_n) f'(x_n)}{D_n}$.\n    f. Check the second termination criterion: if $|x_{n+1} - x_n| \\leq 10^{-12}$, the iteration is successful. The result is $x_{n+1}$. Terminate.\n    g. Update for the next iteration by setting $x_n \\leftarrow x_{n+1}$.\n3.  If the loop completes after $50$ iterations ($n=49$ is the last step calculating $x_{50}$) without meeting either termination criterion, the process stops. The result is the last computed iterate, $x_{50}$.\n\nThis procedure is applied to each of the three test cases specified in the problem statement. The final numerical results are rounded to $10$ decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a cubically convergent fixed-point iteration scheme (Halley's method)\n    to find the roots of three specified nonlinear equations.\n    \"\"\"\n\n    def solve_with_halley(f, fp, fpp, x0, tol=1e-12, max_iter=50):\n        \"\"\"\n        Applies Halley's method to find a root of f(x)=0.\n\n        Args:\n            f: The function f(x).\n            fp: The first derivative f'(x).\n            fpp: The second derivative f''(x).\n            x0: The initial guess.\n            tol: The tolerance for termination.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The approximate root.\n        \"\"\"\n        x_n = float(x0)\n\n        for _ in range(max_iter):\n            f_val = f(x_n)\n            \n            # Termination criterion 1: |f(x_n)| = tol\n            if abs(f_val) = tol:\n                return x_n\n\n            fp_val = fp(x_n)\n            fpp_val = fpp(x_n)\n\n            # Denominator of Halley's method correction term\n            denominator = 2.0 * fp_val**2 - f_val * fpp_val\n\n            # Prevent division by zero, though unlikely for these problems\n            if abs(denominator)  1e-15:\n                # Method fails, return the last valid iterate\n                return x_n\n\n            # Halley's iteration formula\n            x_n_plus_1 = x_n - (2.0 * f_val * fp_val) / denominator\n\n            # Termination criterion 2: |x_{n+1} - x_n| = tol\n            if abs(x_n_plus_1 - x_n) = tol:\n                return x_n_plus_1\n\n            x_n = x_n_plus_1\n        \n        # Return the last iterate if max_iter is reached\n        return x_n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda x: np.cos(x) - x,\n            \"fp\": lambda x: -np.sin(x) - 1.0,\n            \"fpp\": lambda x: -np.cos(x),\n            \"x0\": 0.5\n        },\n        {\n            \"f\": lambda x: x**3 - 2.0,\n            \"fp\": lambda x: 3.0 * x**2,\n            \"fpp\": lambda x: 6.0 * x,\n            \"x0\": 10.0\n        },\n        {\n            \"f\": lambda x: np.exp(x) - 3.0 * x,\n            \"fp\": lambda x: np.exp(x) - 3.0,\n            \"fpp\": lambda x: np.exp(x),\n            \"x0\": 0.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        root = solve_with_halley(\n            f=case[\"f\"],\n            fp=case[\"fp\"],\n            fpp=case[\"fpp\"],\n            x0=case[\"x0\"],\n            tol=1e-12,\n            max_iter=50\n        )\n        results.append(root)\n\n    # Format results to 10 decimal places and create the final output string.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2394865"}, {"introduction": "Instead of designing a new high-order method from scratch, we can often accelerate the convergence of an existing, simpler iteration. This exercise introduces Aitken's $\\Delta^2$ method, a powerful technique for transforming a slowly, linearly converging sequence into one that converges much more rapidly. By assuming a geometric model for the error, you will derive the acceleration formula from first principles and apply it to a practical example [@problem_id:2394839]. This hands-on task demonstrates the crucial computational concept of sequence transformation, offering a practical tool to significantly improve algorithmic efficiency.", "problem": "You are tasked with constructing and analyzing a fixed-point iteration that converges to the natural logarithm of two, denoted $\\,\\ln(2)\\,$, and then accelerating the convergence using Aitken’s delta-squared (denoted $\\Delta^2$) method. Begin from the fixed-point form of the equation $\\,e^{x} = 2\\,$, which implies that a fixed point of a mapping $\\,g(x)\\,$ is the desired value $\\,x^\\star = \\ln(2)\\,$. Consider the family of fixed-point iterations parameterized by $\\,\\lambda\\,$,\n$$\ng_{\\lambda}(x) = x - \\lambda\\,(e^{x} - 2),\n$$\nand the iterative process $\\,x_{n+1} = g_{\\lambda}(x_n)\\,$ for $\\,n \\ge 0\\,$. Assume the contraction condition near the fixed point $\\,x^\\star\\,$ holds, specifically $|g'_{\\lambda}(x^\\star)|  1$, and leverage this to justify linear error behavior as the starting point for designing an acceleration.\n\nYour tasks are:\n- Derive the Aitken’s delta-squared ($\\Delta^2$) acceleration formula for a linearly convergent sequence $\\,\\{s_n\\}\\,$ based on a geometric error model assumption $\\,s_n = s + C r^n\\,$ with $\\,|r|1\\,$. Do not use or quote any pre-derived acceleration formula; instead, eliminate the nuisance parameters to obtain an explicit accelerated estimator in terms of three consecutive iterates. Clearly state any conditions under which the transform is well-defined.\n- Design a robust algorithm that:\n  1. Generates the fixed-point sequence $\\,\\{x_n\\}\\,$ for the mapping $\\,g_{\\lambda}(x)\\,$.\n  2. Forms one accelerated estimate from the last three available iterates using your derived $\\Delta^2$ expression.\n  3. Detects and safely handles the edge case where the $\\Delta^2$ denominator approaches zero in magnitude (e.g., smaller than a tolerance), in which case your algorithm must return the unaccelerated latest iterate as the accelerated output.\n  4. Quantifies accuracy by the absolute error $\\,|x - \\ln(2)|\\,$, which is unitless. All error values in the final output must be rounded to exactly $\\,12\\,$ digits after the decimal point.\n\nImplementation requirements:\n- You must implement a complete, runnable program that performs the above steps for a prescribed test suite. No user input is permitted.\n- For each test case, the iteration must run for exactly $\\,N+2\\,$ steps so that the last three iterates $\\,x_N, x_{N+1}, x_{N+2}\\,$ are available to construct a single $\\Delta^2$-accelerated estimate corresponding to index $\\,N\\,$.\n- For each test case, compute two floats:\n  1. The absolute error of the unaccelerated estimate using $\\,x_{N+2}\\,$.\n  2. The absolute error of the $\\Delta^2$-accelerated estimate built from $\\,x_{N}, x_{N+1}, x_{N+2}\\,$.\n- Final output format: Your program should produce a single line of output containing all results in a single flat list, as a comma-separated list enclosed in square brackets. The entries must appear in the exact order\n  $$[E_{1,\\mathrm{plain}},E_{1,\\mathrm{acc}},E_{2,\\mathrm{plain}},E_{2,\\mathrm{acc}},E_{3,\\mathrm{plain}},E_{3,\\mathrm{acc}},E_{4,\\mathrm{plain}},E_{4,\\mathrm{acc}}],$$\n  where $\\,E_{i,\\mathrm{plain}}\\,$ is the unaccelerated absolute error and $\\,E_{i,\\mathrm{acc}}\\,$ is the accelerated absolute error for test $\\,i\\,$. Each entry must be rounded to exactly $\\,12\\,$ digits after the decimal point.\n\nTest suite:\n- Use $\\,\\ln(2)\\,$ as the reference value, computed by the natural logarithm function in radians.\n- Use the following $\\,(\\lambda, x_0, N)\\,$ triplets:\n  1. $\\,(\\,0.05,\\; 0.0,\\; 50\\,)\\,$\n  2. $\\,(\\,0.01,\\; 0.0,\\; 200\\,)\\,$\n  3. $\\,(\\,0.49,\\; 0.0,\\; 10\\,)\\,$\n  4. $\\,(\\,0.99,\\; 0.0,\\; 200\\,)\\,$\n\nScientific and numerical considerations:\n- Start from the fixed-point convergence condition $|g'_{\\lambda}(x^\\star)|1$ to justify linear convergence and motivate the geometric error model, then derive the acceleration rule from first principles by eliminating unknowns.\n- No physical units are involved in this problem. All angles, where relevant due to exponential and logarithm functions, are in radians by definition of the natural logarithm and exponential.\n- Ensure your algorithm is numerically stable by incorporating a small positive tolerance $\\,\\tau\\,$ for the $\\Delta^2$ denominator check, where a value such as $\\,\\tau = 10^{-14}\\,$ is acceptable.\n\nYour final program must compute and print the single required line in the exact format described above.", "solution": "The problem, upon inspection, is a standard exercise in the numerical analysis of iterative methods. It is valid, self-contained, and possesses a unique, well-defined solution path. We shall proceed with the required derivations and algorithmic construction.\n\nThe problem is to find the root of the equation $f(x) = e^x - 2 = 0$, which is $x^\\star = \\ln(2)$. We are given a family of fixed-point mappings,\n$$\ng_{\\lambda}(x) = x - \\lambda(e^x - 2)\n$$\nThe iterative sequence is defined by $x_{n+1} = g_{\\lambda}(x_n)$ for $n \\ge 0$. A fixed point $x^\\star$ of this mapping satisfies $x^\\star = g_{\\lambda}(x^\\star)$, which implies $x^\\star = x^\\star - \\lambda(e^{x^\\star} - 2)$, and thus $e^{x^\\star} - 2 = 0$. The fixed points of $g_{\\lambda}(x)$ are indeed the roots of $f(x)$, provided $\\lambda \\neq 0$.\n\nFor the iteration to converge to a fixed point $x^\\star$, the mapping $g_{\\lambda}(x)$ must be a contraction in a neighborhood of $x^\\star$. The condition for convergence is given by the Contraction Mapping Theorem, which requires $|g'_{\\lambda}(x^\\star)|  1$. The derivative of $g_{\\lambda}(x)$ is:\n$$\ng'_{\\lambda}(x) = \\frac{d}{dx} \\left( x - \\lambda(e^x - 2) \\right) = 1 - \\lambda e^x\n$$\nEvaluating this derivative at the fixed point $x^\\star = \\ln(2)$:\n$$\ng'_{\\lambda}(x^\\star) = 1 - \\lambda e^{\\ln(2)} = 1 - 2\\lambda\n$$\nThe convergence condition is therefore $|1 - 2\\lambda|  1$. This inequality is equivalent to $-1  1 - 2\\lambda  1$, which simplifies to $-2  -2\\lambda  0$. Dividing by $-2$ and reversing the inequalities gives $0  \\lambda  1$. All test cases provided use $\\lambda$ values within this range, ensuring convergence.\n\nWhen $g'_{\\lambda}(x^\\star) \\neq 0$, the convergence is linear. The error at step $n+1$, denoted $e_{n+1} = x_{n+1} - x^\\star$, is related to the error at step $n$, $e_n = x_n - x^\\star$, by a Taylor expansion of $g_{\\lambda}(x_n)$ around $x^\\star$:\n$$\nx_{n+1} = g_{\\lambda}(x_n) \\approx g_{\\lambda}(x^\\star) + g'_{\\lambda}(x^\\star)(x_n - x^\\star)\n$$\n$$\nx_{n+1} - x^\\star \\approx g'_{\\lambda}(x^\\star)(x_n - x^\\star) \\implies e_{n+1} \\approx r \\cdot e_n\n$$\nwhere $r = g'_{\\lambda}(x^\\star) = 1 - 2\\lambda$ is the asymptotic rate of convergence. This relation implies that for large $n$, the error behaves geometrically: $e_n \\approx C r^n$ for some constant $C$. This justifies the geometric error model used as the foundation for Aitken's $\\Delta^2$ method.\n\nWe now derive the Aitken's $\\Delta^2$ acceleration formula. Let $\\{s_n\\}$ be a sequence that converges to a limit $s$. We model the sequence for large $n$ as:\n$$\ns_n = s + C r^n, \\quad |r|1\n$$\nWe write this for three consecutive terms, $s_n$, $s_{n+1}$, and $s_{n+2}$:\n$$\ns_n - s = C r^n \\quad (1)\n$$\n$$\ns_{n+1} - s = C r^{n+1} \\quad (2)\n$$\n$$\ns_{n+2} - s = C r^{n+2} \\quad (3)\n$$\nOur goal is to eliminate the unknown parameters $C$ and $r$ to find an improved estimate for $s$. We introduce the forward difference operator $\\Delta s_k = s_{k+1} - s_k$.\n$$\n\\Delta s_n = s_{n+1} - s_n = (s_{n+1} - s) - (s_n - s) = C r^{n+1} - C r^n = C r^n (r - 1)\n$$\n$$\n\\Delta s_{n+1} = s_{n+2} - s_{n+1} = (s_{n+2} - s) - (s_{n+1} - s) = C r^{n+2} - C r^{n+1} = C r^{n+1} (r - 1)\n$$\nTaking the ratio of these differences eliminates $C$:\n$$\n\\frac{\\Delta s_{n+1}}{\\Delta s_n} = \\frac{C r^{n+1} (r - 1)}{C r^n (r - 1)} = r\n$$\nThis provides an estimate for the convergence rate $r$. Now, from equations $(1)$ and $(2)$, we have $s_{n+1} - s = r(s_n - s)$. Substituting our expression for $r$:\n$$\ns_{n+1} - s = \\left(\\frac{s_{n+2} - s_{n+1}}{s_{n+1} - s_n}\\right) (s_n - s)\n$$\nThis is an equation for the unknown limit $s$. We solve for $s$:\n$$\n(s_{n+1} - s)(s_{n+1} - s_n) = (s_{n+2} - s_{n+1})(s_n - s)\n$$\n$$\ns_{n+1}(s_{n+1} - s_n) - s(s_{n+1} - s_n) = s_n(s_{n+2} - s_{n+1}) - s(s_{n+2} - s_{n+1})\n$$\n$$\ns \\left( (s_{n+2} - s_{n+1}) - (s_{n+1} - s_n) \\right) = s_n(s_{n+2} - s_{n+1}) - s_{n+1}(s_{n+1} - s_n)\n$$\n$$\ns (s_{n+2} - 2s_{n+1} + s_n) = s_n s_{n+2} - s_n s_{n+1} - s_{n+1}^2 + s_n s_{n+1} = s_n s_{n+2} - s_{n+1}^2\n$$\nProvided the denominator is non-zero, the accelerated estimate for $s$ is:\n$$\ns = \\frac{s_n s_{n+2} - s_{n+1}^2}{s_{n+2} - 2s_{n+1} + s_n}\n$$\nThis formula is susceptible to subtractive cancellation if $s_n$ is large. A more numerically robust form is obtained by rewriting it as:\n$$\ns = s_n - \\frac{(s_{n+1} - s_n)^2}{s_{n+2} - 2s_{n+1} + s_n} = s_n - \\frac{(\\Delta s_n)^2}{\\Delta^2 s_n}\n$$\nwhere $\\Delta^2 s_n = \\Delta(\\Delta s_n) = \\Delta s_{n+1} - \\Delta s_n$ is the second-order forward difference. This is the Aitken's $\\Delta^2$ formula. We apply this to the sequence $\\{x_n\\}$ by setting $s_k = x_k$. The accelerated estimate, which we denote $x'_{N}$, is formed from the iterates $x_N$, $x_{N+1}$, and $x_{N+2}$:\n$$\nx'_{N} = x_N - \\frac{(x_{N+1} - x_N)^2}{x_{N+2} - 2x_{N+1} + x_N}\n$$\nThis expression is well-defined if and only if the denominator $x_{N+2} - 2x_{N+1} + x_N \\neq 0$. In floating-point arithmetic, we must check if the absolute value of the denominator is smaller than some tolerance $\\tau > 0$, such as $\\tau = 10^{-14}$. If $|x_{N+2} - 2x_{N+1} + x_N|  \\tau$, the transformation is numerically unstable or ill-defined, and we shall return the latest unaccelerated iterate, $x_{N+2}$, as the result.\n\nThe algorithm is as follows:\nFor each given test case $(\\lambda, x_0, N)$:\n1. Set the fixed reference value $x^\\star = \\ln(2)$.\n2. Initialize a sequence of iterates with $x_0$.\n3. Generate $N+2$ subsequent iterates, $x_1, \\dots, x_{N+2}$, using the mapping $x_{n+1} = x_n - \\lambda(e^{x_n} - 2)$. Store all iterates up to $x_{N+2}$.\n4. Calculate the unaccelerated absolute error: $E_{\\mathrm{plain}} = |x_{N+2} - x^\\star|$.\n5. Retrieve the last three iterates required for acceleration: $x_N, x_{N+1}, x_{N+2}$.\n6. Calculate the denominator of the Aitken formula: $d = x_{N+2} - 2x_{N+1} + x_N$.\n7. If $|d|  10^{-14}$, set the accelerated estimate $x_{\\mathrm{acc}} = x_{N+2}$.\n8. Otherwise, calculate the accelerated estimate $x_{\\mathrm{acc}} = x_N - (x_{N+1} - x_N)^2 / d$.\n9. Calculate the accelerated absolute error: $E_{\\mathrm{acc}} = |x_{\\mathrm{acc}} - x^\\star|$.\n10. Round both error values, $E_{\\mathrm{plain}}$ and $E_{\\mathrm{acc}}$, to $12$ decimal digits and store them.\nFinally, all computed error values from all test cases are compiled into a single flat list and printed in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes a fixed-point iteration for ln(2), accelerates it\n    using Aitken's delta-squared method, and computes the errors for a suite\n    of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, x0, N)\n        (0.05, 0.0, 50),\n        (0.01, 0.0, 200),\n        (0.49, 0.0, 10),\n        (0.99, 0.0, 200),\n    ]\n\n    # The exact value of the fixed point, ln(2)\n    x_star = np.log(2)\n    \n    # Tolerance for the denominator in Aitken's formula\n    tau = 1.0e-14\n\n    results = []\n    \n    for case in test_cases:\n        lambda_val, x0, N = case\n        \n        # 1. Generate the fixed-point sequence {x_n}\n        # We need N+3 points in the sequence (x_0, x_1, ..., x_{N+2})\n        # This requires N+2 iteration steps.\n        iterates = [x0]\n        current_x = x0\n        for _ in range(N + 2):\n            # Fixed-point iteration: x_{n+1} = g(x_n) = x_n - lambda * (exp(x_n) - 2)\n            current_x = current_x - lambda_val * (np.exp(current_x) - 2)\n            iterates.append(current_x)\n            \n        # The last three iterates for acceleration are x_N, x_{N+1}, x_{N+2}\n        x_N = iterates[N]\n        x_N_plus_1 = iterates[N + 1]\n        x_N_plus_2 = iterates[N + 2]\n        \n        # 2. Compute the absolute error of the unaccelerated estimate\n        # The latest unaccelerated estimate is x_{N+2}\n        error_plain = abs(x_N_plus_2 - x_star)\n        \n        # 3. Form the accelerated estimate using Aitken's delta-squared method\n        denominator = x_N_plus_2 - 2 * x_N_plus_1 + x_N\n        \n        # 4. Detect and handle the edge case of a small denominator\n        if abs(denominator)  tau:\n            # If denominator is too small, use the unaccelerated estimate\n            x_accelerated = x_N_plus_2\n        else:\n            numerator = (x_N_plus_1 - x_N)**2\n            x_accelerated = x_N - numerator / denominator\n            \n        # 5. Compute the absolute error of the accelerated estimate\n        error_accelerated = abs(x_accelerated - x_star)\n        \n        # 6. Format results to exactly 12 decimal places\n        results.append(f\"{error_plain:.12f}\")\n        results.append(f\"{error_accelerated:.12f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2394839"}]}