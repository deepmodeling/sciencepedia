{"hands_on_practices": [{"introduction": "The foundation of any numerical method is confidence in the tools we use. While we can derive the formal order of accuracy for a finite difference formula from a Taylor series expansion, verifying this behavior in practice is a cornerstone of robust scientific computing. This exercise provides a direct, hands-on opportunity to bridge the gap between theoretical error analysis and practical implementation [@problem_id:2391581]. By systematically measuring the approximation error for decreasing step sizes $h$, you will empirically confirm that the popular central difference schemes for the first and second derivatives are indeed second-order accurate, with an error that scales as $\\mathcal{O}(h^2)$. This practice is essential for building skills in code verification and developing an intuitive grasp of convergence.", "problem": "Design and implement a fully reproducible numerical experiment to empirically verify the order of accuracy of symmetric finite difference approximations for derivatives. Use only the following foundational base: Taylor series expansions of a sufficiently smooth function about a point, and the definitions of the first and second derivatives as limits. Your tasks are:\n\n1. Construct symmetric, centered finite difference approximations for the first derivative and the second derivative of a smooth function $f(x)$ at a point $x_0$ using a step size $h$. The approximations must use values of $f$ evaluated at $x_0 \\pm h$ (and optionally $x_0$ for the second derivative). Do not assume or quote any pre-existing error formulas; instead, base your reasoning on Taylor series expansions about $x_0$ to justify the expected truncation error behavior.\n\n2. For a given function $f(x)$ with known derivatives, define the true first derivative $f'(x)$ and the true second derivative $f''(x)$ at $x_0$. For a sequence of step sizes $h_k = h_0/2^k$, $k = 0,1,\\dots,n-1$, compute the absolute error $e_k = \\lvert A(h_k) - T \\rvert$ where $A(h_k)$ is the numerical approximation of the target derivative at $x_0$ using step size $h_k$ and $T$ is the corresponding true derivative value. For each consecutive pair of errors, compute the observed order of accuracy\n$$\np_k = \\log_2\\left( \\frac{e_k}{e_{k+1}} \\right).\n$$\nReport as the scalar result for the test case the last available $p_k$ in the refinement sequence (i.e., the one corresponding to the two smallest step sizes), which empirically approximates the asymptotic order.\n\n3. Implement the above as a complete program that runs the following test suite. Each test case specifies a target derivative order, a function $f(x)$, an evaluation point $x_0$, an initial step size $h_0$, and a number of refinements $n$. The functions and their true derivatives must be treated exactly as specified below.\n\n- Test case 1 (happy path, first derivative): Target derivative is $f'(x)$ with $f(x) = \\sin(x)$, $x_0 = 0.37$ (in radians), $h_0 = 0.2$, $n = 5$.\n- Test case 2 (different smoothness profile, first derivative): Target derivative is $f'(x)$ with $f(x) = \\exp(\\sin(x))$, $x_0 = -0.8$ (in radians), $h_0 = 0.2$, $n = 5$. Use the exact identity $f'(x) = \\exp(\\sin(x)) \\cos(x)$.\n- Test case 3 (happy path, second derivative): Target derivative is $f''(x)$ with $f(x) = \\sin(x)$, $x_0 = 0.37$ (in radians), $h_0 = 0.2$, $n = 5$. Use the exact identity $f''(x) = -\\sin(x)$.\n- Test case 4 (nontrivial rational function, second derivative): Target derivative is $f''(x)$ with $f(x) = \\frac{1}{1+x^2}$, $x_0 = 0.9$, $h_0 = 0.2$, $n = 5$. Use the exact identity $f''(x) = \\frac{6x^2 - 2}{(1+x^2)^3}$.\n- Test case 5 (edge case with large magnitude function values, first derivative): Target derivative is $f'(x)$ with $f(x) = \\exp(x)$, $x_0 = 5.0$, $h_0 = 0.4$, $n = 5$. Use the exact identity $f'(x) = \\exp(x)$.\n\n4. Your implementation must:\n- For target $f'(x)$, use a symmetric, centered stencil based only on $x_0 \\pm h$.\n- For target $f''(x)$, use a symmetric, centered stencil based only on $x_0 \\pm h$ and $x_0$.\n- Use the refinement strategy $h_k = h_0/2^k$ for $k = 0,1,\\dots,n-1$.\n- Compute absolute errors and the sequence of observed orders $p_k$.\n- Return the final scalar result per test case as the last $p_k$ in the sequence.\n- If an error difference happens to be zero at the smallest scales, use the most recent nonzero neighboring pair to compute $p_k$.\n\n5. Final output format:\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets (for example, \"[r1,r2,r3,r4,r5]\"). Each number must be the final observed order for the corresponding test case, rounded to six decimal places.\n\nAngles must be interpreted in radians. No physical units are involved in this problem. The expected outcome is that the observed orders for both the first-derivative and second-derivative symmetric schemes are close to $2$, consistent with a truncation error proportional to $h^2$.", "solution": "The problem statement is a valid, well-posed exercise in numerical analysis. It is scientifically grounded in the theory of Taylor series and finite difference methods, contains no contradictions, and provides all necessary information to construct a unique, verifiable solution. We will therefore proceed with the derivation and implementation.\n\nThe objective is to derive symmetric finite difference formulas for the first and second derivatives of a sufficiently smooth function $f(x)$, analyze their truncation error, and empirically verify the theoretical order of accuracy through a numerical experiment.\n\nFirst, we derive the finite difference approximation for the first derivative, $f'(x)$. Let $f(x)$ be at least three times continuously differentiable. The Taylor series expansions of $f(x)$ around a point $x_0$ for steps $+h$ and $-h$ are given by:\n$$\nf(x_0 + h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2!} f''(x_0) + \\frac{h^3}{3!} f'''(x_0) + O(h^4) \\quad (1)\n$$\n$$\nf(x_0 - h) = f(x_0) - h f'(x_0) + \\frac{h^2}{2!} f''(x_0) - \\frac{h^3}{3!} f'''(x_0) + O(h^4) \\quad (2)\n$$\nSubtracting equation $(2)$ from equation $(1)$ eliminates the terms with even powers of $h$:\n$$\nf(x_0 + h) - f(x_0 - h) = 2h f'(x_0) + \\frac{2h^3}{6} f'''(x_0) + O(h^5)\n$$\nSolving for $f'(x_0)$ yields:\n$$\nf'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{h^2}{6} f'''(x_0) + O(h^4)\n$$\nThis gives the symmetric centered finite difference approximation for the first derivative:\n$$\nA_{f'}(h) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}\n$$\nThe truncation error, defined as the difference between the approximation and the true value, $E_{trunc} = A_{f'}(h) - f'(x_0)$, is therefore:\n$$\nE_{trunc} = -\\frac{h^2}{6} f'''(x_0) + O(h^4)\n$$\nThe leading term of the error is proportional to $h^2$, which means the approximation is second-order accurate.\n\nNext, we derive the approximation for the second derivative, $f''(x)$. Let $f(x)$ be at least four times continuously differentiable. Adding equations $(1)$ and $(2)$ eliminates the terms with odd powers of $h$:\n$$\nf(x_0 + h) + f(x_0 - h) = 2f(x_0) + h^2 f''(x_0) + \\frac{2h^4}{24} f''''(x_0) + O(h^6)\n$$\nSolving for $f''(x_0)$ yields:\n$$\nf''(x_0) = \\frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2} - \\frac{h^2}{12} f''''(x_0) + O(h^4)\n$$\nThis gives the symmetric centered finite difference approximation for the second derivative:\n$$\nA_{f''}(h) = \\frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2}\n$$\nThe truncation error for this approximation is:\n$$\nE_{trunc} = -\\frac{h^2}{12} f''''(x_0) + O(h^4)\n$$\nThe leading term of the error is again proportional to $h^2$, so this approximation is also second-order accurate.\n\nTo empirically verify the order of accuracy, we analyze the behavior of the absolute error as the step size $h$ is refined. For an approximation of order $p$, the absolute error $e(h) = |A(h) - T|$, where $T$ is the true derivative value, behaves as $e(h) \\approx C h^p$ for some constant $C$ and sufficiently small $h$.\nConsider a sequence of step sizes $h_k = h_0 / 2^k$ for $k=0, 1, 2, \\dots$. The corresponding errors are $e_k = e(h_k)$ and $e_{k+1} = e(h_{k+1})$. The ratio of consecutive errors is:\n$$\n\\frac{e_k}{e_{k+1}} \\approx \\frac{C h_k^p}{C h_{k+1}^p} = \\frac{C (h_0/2^k)^p}{C (h_0/2^{k+1})^p} = \\frac{(1/2^k)^p}{(1/2^{k+1})^p} = \\left(\\frac{2^{k+1}}{2^k}\\right)^p = 2^p\n$$\nBy taking the base-$2$ logarithm, we can compute the observed order of accuracy, $p_k$:\n$$\np_k = \\log_2\\left(\\frac{e_k}{e_{k+1}}\\right)\n$$\nAs $k$ increases, $h_k$ decreases, and $p_k$ should converge to the theoretical order $p$. For the derived second-order schemes, we expect $p_k \\to 2$.\n\nThe numerical experiment is implemented as follows. For each test case:\n1.  A sequence of $n$ step sizes is generated: $h_k = h_0 / 2^k$ for $k=0, \\dots, n-1$.\n2.  The true value of the derivative, $T$, is computed at the point $x_0$.\n3.  For each step size $h_k$, the corresponding numerical approximation $A(h_k)$ is calculated using either the first- or second-derivative formula.\n4.  A sequence of absolute errors is computed: $e_k = |A(h_k) - T|$.\n5.  A sequence of observed orders is computed: $p_k = \\log_2(e_k / e_{k+1})$ for $k=0, \\dots, n-2$.\n6.  The result is the last validly computed order, $p_{n-2}$, which corresponds to the comparison between the two smallest step sizes, $h_{n-2}$ and $h_{n-1}$. This provides an empirical estimate of the asymptotic order of accuracy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and implements a numerical experiment to verify the order of accuracy\n    of symmetric finite difference approximations for first and second derivatives.\n    \"\"\"\n\n    # Define the functions, their true derivatives, and parameters for each test case.\n    test_cases = [\n        {\n            # Test case 1: f'(x) for sin(x)\n            \"target_order\": 1,\n            \"f\": lambda x: np.sin(x),\n            \"true_deriv_func\": lambda x: np.cos(x),\n            \"x0\": 0.37,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 2: f'(x) for exp(sin(x))\n            \"target_order\": 1,\n            \"f\": lambda x: np.exp(np.sin(x)),\n            \"true_deriv_func\": lambda x: np.exp(np.sin(x)) * np.cos(x),\n            \"x0\": -0.8,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 3: f''(x) for sin(x)\n            \"target_order\": 2,\n            \"f\": lambda x: np.sin(x),\n            \"true_deriv_func\": lambda x: -np.sin(x),\n            \"x0\": 0.37,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 4: f''(x) for 1/(1+x^2)\n            \"target_order\": 2,\n            \"f\": lambda x: 1.0 / (1.0 + x**2),\n            \"true_deriv_func\": lambda x: (6.0 * x**2 - 2.0) / (1.0 + x**2)**3,\n            \"x0\": 0.9,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 5: f'(x) for exp(x) with large function magnitude\n            \"target_order\": 1,\n            \"f\": lambda x: np.exp(x),\n            \"true_deriv_func\": lambda x: np.exp(x),\n            \"x0\": 5.0,\n            \"h0\": 0.4,\n            \"n\": 5,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        target_order = case[\"target_order\"]\n        f = case[\"f\"]\n        true_deriv_func = case[\"true_deriv_func\"]\n        x0 = case[\"x0\"]\n        h0 = case[\"h0\"]\n        n = case[\"n\"]\n\n        # Calculate the true value of the derivative\n        true_value = true_deriv_func(x0)\n\n        # Generate step sizes and compute errors\n        errors = []\n        for k in range(n):\n            h = h0 / (2**k)\n            \n            if target_order == 1:\n                # Symmetric centered difference for the first derivative\n                approx_val = (f(x0 + h) - f(x0 - h)) / (2.0 * h)\n            elif target_order == 2:\n                # Symmetric centered difference for the second derivative\n                approx_val = (f(x0 + h) - 2.0 * f(x0) + f(x0 - h)) / (h**2)\n            else:\n                # This case should not be reached based on the problem statement\n                raise ValueError(\"Invalid target derivative order.\")\n\n            # Compute and store the absolute error\n            errors.append(np.abs(approx_val - true_value))\n\n        # Compute observed orders of accuracy\n        observed_orders = []\n        for k in range(n - 1):\n            e_k = errors[k]\n            e_k_plus_1 = errors[k + 1]\n            \n            # Ensure errors are non-zero to avoid division by zero\n            # or log of zero. This also handles the specified edge case.\n            if e_k > 0 and e_k_plus_1 > 0:\n                p_k = np.log2(e_k / e_k_plus_1)\n                observed_orders.append(p_k)\n\n        # The result for the test case is the last available observed order\n        if observed_orders:\n            final_p = observed_orders[-1]\n        else:\n            # Handle case where no valid order could be computed (e.g., all errors are zero)\n            # For this problem set, this fallback is not expected to be needed.\n            final_p = np.nan\n            \n        results.append(final_p)\n\n    # Format the final output as a comma-separated list of numbers rounded to six decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2391581"}, {"introduction": "When a finite difference scheme is applied to a grid of points, it can be viewed as a linear operator represented by a matrix. The properties of this matrix, particularly its eigenvalues (its spectrum), reveal profound insights into the behavior of the numerical method when used to solve differential equations. This practice asks you to construct the matrix for the first derivative using a central difference scheme on a periodic domain and to analyze its eigenvalues [@problem_id:2392396]. You will numerically verify the fundamental property that for this scheme, the differentiation matrix is skew-symmetric, leading to purely imaginary eigenvalues. This spectral knowledge is the key to understanding the stability of time-evolution schemes for partial differential equations like the wave or advection equations.", "problem": "You are to construct a linear finite difference operator that approximates the first spatial derivative on a one-dimensional periodic domain and to verify a key spectral property numerically. Work in purely mathematical terms on a uniform grid, and treat all variables as dimensionless. No angles are involved; any occurrences of $2\\pi$ should be interpreted as a length, not an angle.\n\nStarting point and modeling assumptions:\n- Consider a smooth function $f(x)$ defined on a periodic interval of length $L$, that is, $f(x + L) = f(x)$ for all $x$.\n- Use a uniform grid of $N$ points on $[0, L)$ with spacing $\\Delta x = L/N$, and grid points $x_j = j \\,\\Delta x$ for $j = 0, 1, \\dots, N-1$.\n- Use the definition of the derivative and the Taylor expansion about a point to derive an approximation that is consistent for the first derivative with a truncation error that scales like $\\mathcal{O}(\\Delta x^2)$.\n- Impose periodic boundary conditions by wrapping indices modulo $N$.\n\nTasks:\n1. From the definition of the derivative and Taylor expansion, derive a second-order accurate linear finite difference approximation for the first derivative on the uniform grid defined above. Do not assume any particular stencil without derivation, and do not introduce any non-uniform spacing.\n2. Assemble the corresponding $N \\times N$ matrix $D$ that linearly maps the vector $u = \\bigl(f(x_0), f(x_1), \\dots, f(x_{N-1})\\bigr)^\\top$ to an approximation of the derivative values at the grid points. Enforce periodic boundary conditions by wrapping indices modulo $N$.\n3. Show numerically that all eigenvalues of $D$ are purely imaginary by computing all eigenvalues and reporting, for each test case, the maximum absolute value of the real parts. This maximum should be near zero in floating-point arithmetic.\n\nImplementation and numerical details:\n- Use double-precision floating-point arithmetic for all computations.\n- For a given pair $(N, L)$, construct $\\Delta x = L/N$, build the matrix $D$, compute all eigenvalues, and determine the maximum absolute value of their real parts.\n- No units are required; report raw floating-point values.\n\nTest suite:\nEvaluate your implementation on the following four parameter sets:\n- Case $1$: $N = 8$, $L = 2\\pi$.\n- Case $2$: $N = 9$, $L = 1$.\n- Case $3$: $N = 3$, $L = 5$.\n- Case $4$: $N = 64$, $L = 7.5$.\n\nAnswer specification:\n- For each test case, compute a single float equal to $\\max_k \\lvert \\operatorname{Re}(\\lambda_k) \\rvert$, where $\\lambda_k$ are the eigenvalues of $D$ for that case.\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. Each entry must be a floating-point number.\n\nYour final answer must be a complete, runnable program that carries out all computations and prints the results in exactly the format specified above, with no additional text. The program must not read any input.", "solution": "The problem set before us is a standard exercise in numerical analysis, concerning the construction and spectral analysis of a finite difference operator. It is a valid, well-posed problem grounded in established mathematical principles. We shall proceed with its systematic solution.\n\nThe primary objective is to construct a linear operator, represented by a matrix $D$, that approximates the first spatial derivative $\\frac{d}{dx}$ on a one-dimensional periodic domain of length $L$. The domain is discretized into a uniform grid of $N$ points, $x_j = j \\Delta x$ for $j = 0, 1, \\dots, N-1$, where the grid spacing is $\\Delta x = L/N$. We must numerically verify a spectral property of this operator.\n\nFirst, we derive the finite difference scheme required to be accurate to second order in the grid spacing, $\\mathcal{O}(\\Delta x^2)$. We begin with the Taylor series expansion of a smooth function $f(x)$ around a grid point $x_j$. The expansions for the function values at the neighboring points, $x_{j+1} = x_j + \\Delta x$ and $x_{j-1} = x_j - \\Delta x$, are:\n$$f(x_{j+1}) = f(x_j + \\Delta x) = f(x_j) + f'(x_j)\\Delta x + \\frac{1}{2}f''(x_j)\\Delta x^2 + \\frac{1}{6}f'''(x_j)\\Delta x^3 + \\mathcal{O}(\\Delta x^4)$$\n$$f(x_{j-1}) = f(x_j - \\Delta x) = f(x_j) - f'(x_j)\\Delta x + \\frac{1}{2}f''(x_j)\\Delta x^2 - \\frac{1}{6}f'''(x_j)\\Delta x^3 + \\mathcal{O}(\\Delta x^4)$$\nTo isolate the first derivative term, $f'(x_j)$, we subtract the second expansion from the first. This subtraction conveniently eliminates all even-power terms in $\\Delta x$, including the constant term $f(x_j)$ and the second-derivative term $f''(x_j)$.\n$$f(x_{j+1}) - f(x_{j-1}) = 2f'(x_j)\\Delta x + \\frac{1}{3}f'''(x_j)\\Delta x^3 + \\mathcal{O}(\\Delta x^5)$$\nSolving for $f'(x_j)$ yields:\n$$f'(x_j) = \\frac{f(x_{j+1}) - f(x_{j-1})}{2\\Delta x} - \\frac{1}{6}f'''(x_j)\\Delta x^2 + \\mathcal{O}(\\Delta x^4)$$\nFrom this expression, we define our finite difference approximation for the first derivative at $x_j$ as:\n$$f'(x_j) \\approx \\frac{f(x_{j+1}) - f(x_{j-1})}{2\\Delta x}$$\nThis is the second-order accurate central difference scheme. The leading term of the truncation error is $-\\frac{1}{6}f'''(x_j)\\Delta x^2$, which scales as $\\mathcal{O}(\\Delta x^2)$, satisfying the requirement of the problem.\n\nNext, we assemble the $N \\times N$ differentiation matrix $D$. Let $u$ be the column vector of function values at the grid points, $u = [f(x_0), f(x_1), \\dots, f(x_{N-1})]^\\top$. The operation of differentiation is approximated by the matrix-vector multiplication $u' = Du$, where $u'$ is the vector of approximate derivative values. The $j$-th row of this matrix equation corresponds to the derivative approximation at point $x_j$:\n$$(Du)_j \\approx \\frac{u_{j+1} - u_{j-1}}{2\\Delta x}$$\nThe problem specifies periodic boundary conditions, which are enforced by taking all indices modulo $N$. For example, at index $j=0$, the stencil requires values at $u_1$ and $u_{-1}$. Periodicity implies $u_{-1} = u_{N-1}$. Similarly, for $j=N-1$, the stencil requires $u_N$ and $u_{N-2}$, where periodicity gives $u_N = u_0$.\nThe general form for the $j$-th row of the matrix $D$ is such that it has non-zero values only in the columns corresponding to the stencil points $j-1$ and $j+1$ (modulo $N$). Specifically, for each row $j \\in \\{0, 1, \\dots, N-1\\}$:\n$$D_{j, (j+1) \\pmod N} = \\frac{1}{2\\Delta x}$$\n$$D_{j, (j-1) \\pmod N} = -\\frac{1}{2\\Delta x}$$\nAll other entries $D_{j,k}$ are zero. This structure defines a circulant matrix. For example, for $N=4$, the matrix is:\n$$D = \\frac{1}{2\\Delta x} \\begin{pmatrix} 0 & 1 & 0 & -1 \\\\ -1 & 0 & 1 & 0 \\\\ 0 & -1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\end{pmatrix}$$\nIt is immediately apparent that this matrix is skew-symmetric, meaning $D^\\top = -D$. This is a crucial property. A fundamental theorem of linear algebra states that any real skew-symmetric matrix has purely imaginary eigenvalues.\n\nFinally, we must numerically verify this spectral property. For each given test case $(N, L)$, we perform the following steps:\n1.  Calculate the grid spacing $\\Delta x = L/N$.\n2.  Construct the $N \\times N$ matrix $D$ as described above. A computationally efficient way to do this is to define the first row of the circulant matrix and use a library function to generate the full matrix. The first row (for $j=0$) has element $D_{0,1} = 1/(2\\Delta x)$, element $D_{0,N-1} = -1/(2\\Delta x)$, and zeros elsewhere.\n3.  Compute all $N$ eigenvalues, $\\lambda_k$ for $k = 0, \\dots, N-1$, of the matrix $D$ using a standard numerical eigenvalue solver.\n4.  As a consequence of floating-point arithmetic, the real parts of the numerically computed eigenvalues will not be exactly zero but should be on the order of machine precision. We quantify this by calculating the maximum absolute value of the real parts of all eigenvalues: $\\max_k |\\operatorname{Re}(\\lambda_k)|$.\nThis value serves as our numerical evidence. A result close to zero confirms that the eigenvalues of $D$ are, for all practical purposes, purely imaginary, as predicted by theory. The implementation will proceed exactly along these lines for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import circulant\n\ndef solve():\n    \"\"\"\n    Constructs a second-order central difference matrix for the first derivative\n    on a periodic domain, and numerically verifies that its eigenvalues are purely\n    imaginary by computing the maximum absolute real part of its eigenvalues.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (N, L), where N is the number of grid points\n    # and L is the length of the periodic interval.\n    test_cases = [\n        (8, 2 * np.pi),\n        (9, 1.0),\n        (3, 5.0),\n        (64, 7.5),\n    ]\n\n    results = []\n    for N, L in test_cases:\n        # Calculate grid spacing Delta x\n        delta_x = L / N\n\n        # The differentiation matrix D is circulant. We define its first row.\n        # For the central difference scheme f'(j) ~ (f(j+1) - f(j-1)) / (2*dx),\n        # the j-th row of D has non-zero entries at columns (j-1) and (j+1).\n        # For the first row (j=0), the columns are (N-1) and 1.\n        first_row = np.zeros(N, dtype=np.float64)\n        \n        # Coefficient for the f(j+1) term, at index 1\n        first_row[1] = 1.0 / (2.0 * delta_x)\n        \n        # Coefficient for the f(j-1) term, at index N-1 due to periodicity\n        if N > 1:\n            first_row[N - 1] = -1.0 / (2.0 * delta_x)\n\n        # Construct the N x N circulant differentiation matrix D\n        # The resulting matrix is real and skew-symmetric (D^T = -D).\n        D = circulant(first_row)\n\n        # Compute all eigenvalues of the matrix D.\n        # For a real skew-symmetric matrix, eigenvalues are purely imaginary.\n        eigenvalues = np.linalg.eigvals(D)\n\n        # Due to floating-point inaccuracies, the real parts will be very small\n        # non-zero numbers. We find the maximum absolute value of these real parts\n        # to verify the theoretical property.\n        max_abs_real_part = np.max(np.abs(np.real(eigenvalues)))\n        \n        results.append(max_abs_real_part)\n\n    # Final print statement in the exact required format.\n    # The output is a list of floating-point numbers, representing\n    # max|Re(lambda)| for each test case.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2392396"}, {"introduction": "The choice of a finite difference scheme is not merely a matter of formal accuracy; it has critical implications for the qualitative behavior of the solution. This is nowhere more apparent than in the simulation of transport phenomena, governed by the advection-diffusion equation. This exercise challenges you to compare the performance of three different approximations for the advection term ($\\partial_x u$): the non-dissipative central difference scheme and the dissipative upwind and downwind schemes [@problem_id:2392370]. By implementing these methods, you will witness firsthand the trade-offs between the unphysical oscillations (numerical dispersion) often produced by central differencing and the excessive smearing (numerical diffusion) introduced by one-sided schemes. This experience is vital for learning how to select a numerical method that is not only accurate but also stable and physically faithful to the system being modeled.", "problem": "Consider the one-dimensional linear advection-diffusion Partial Differential Equation (PDE)\n$$\n\\partial_t u(x,t) + a\\,\\partial_x u(x,t) = \\nu\\,\\partial_{xx} u(x,t),\n$$\non the periodic domain $x \\in [0,1)$ with period $1$, where $a>0$ is a constant advection speed and $\\nu \\ge 0$ is a constant diffusion coefficient. Let the initial condition be\n$$\nu(x,0) = \\sin\\!\\big(2\\pi k\\,x\\big),\n$$\nwith integer wavenumber $k \\ge 1$, and angles measured in radians.\n\nDiscretize space with a uniform periodic grid of $N$ points, grid spacing $\\Delta x = 1/N$, and grid nodes $x_i = i\\,\\Delta x$ for $i \\in \\{0,1,\\dots,N-1\\}$. For a grid function $\\mathbf{u} = (u_0,\\dots,u_{N-1})^\\top$, define periodic finite difference operators for the first and second spatial derivatives by\n$$\n(\\mathrm{D}^{\\mathrm{C}} \\mathbf{u})_i = \\frac{u_{i+1} - u_{i-1}}{2\\,\\Delta x},\\quad\n(\\mathrm{D}^{\\mathrm{U}} \\mathbf{u})_i = \\frac{u_i - u_{i-1}}{\\Delta x},\\quad\n(\\mathrm{D}^{\\mathrm{D}} \\mathbf{u})_i = \\frac{u_{i+1} - u_i}{\\Delta x},\n$$\nand\n$$\n(\\mathrm{D}^{(2)} \\mathbf{u})_i = \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2},\n$$\nwith periodic indexing so that $u_{-1} \\equiv u_{N-1}$ and $u_{N} \\equiv u_0$. For each advection approximation $\\mathrm{D}^{\\bullet} \\in \\{\\mathrm{D}^{\\mathrm{C}}, \\mathrm{D}^{\\mathrm{U}}, \\mathrm{D}^{\\mathrm{D}}\\}$, define the corresponding semi-discrete spatial operator\n$$\n\\mathbf{L}^{(\\bullet)} = -a\\,\\mathrm{D}^{\\bullet} + \\nu\\,\\mathrm{D}^{(2)}.\n$$\nEvolve the semi-discrete system in time exactly by the matrix exponential so that\n$$\n\\mathbf{u}^{(\\bullet)}(T) = \\exp\\!\\big( T\\,\\mathbf{L}^{(\\bullet)} \\big)\\,\\mathbf{u}(0),\n$$\nwhere $\\mathbf{u}(0)$ is the sampled initial condition $\\big(\\sin(2\\pi k x_i)\\big)_{i=0}^{N-1}$ and $\\exp$ denotes the matrix exponential. The exact solution of the PDE at grid nodes is\n$$\nu_{\\text{exact}}(x_i,T) = \\exp\\!\\big( -\\nu\\,(2\\pi k)^2\\,T \\big)\\,\\sin\\!\\big( 2\\pi k\\,(x_i - a\\,T) \\big).\n$$\nFor each advection approximation, compute the discrete $\\ell^2$ error at time $T$,\n$$\nE^{(\\bullet)} = \\left( \\Delta x \\sum_{i=0}^{N-1} \\big(u^{(\\bullet)}_i(T) - u_{\\text{exact}}(x_i,T)\\big)^2 \\right)^{1/2}.\n$$\n\nYour program must evaluate the triple $\\big(E^{(\\mathrm{C})}, E^{(\\mathrm{U})}, E^{(\\mathrm{D})}\\big)$ for each of the following four test cases (with angles in radians):\n\n- Case $1$: $N=64$, $a=1$, $\\nu=0.01$, $T=0.1$, $k=1$.\n- Case $2$: $N=64$, $a=1$, $\\nu=0.0005$, $T=0.05$, $k=1$.\n- Case $3$: $N=32$, $a=1$, $\\nu=0.1$, $T=0.1$, $k=1$.\n- Case $4$: $N=64$, $a=1$, $\\nu=0.01$, $T=0.1$, $k=3$.\n\nThe required final output format is a single line containing a flat list of $12$ floating-point numbers, ordered as\n$$\n\\big[E^{(\\mathrm{C})}_{1}, E^{(\\mathrm{U})}_{1}, E^{(\\mathrm{D})}_{1}, E^{(\\mathrm{C})}_{2}, E^{(\\mathrm{U})}_{2}, E^{(\\mathrm{D})}_{2}, E^{(\\mathrm{C})}_{3}, E^{(\\mathrm{U})}_{3}, E^{(\\mathrm{D})}_{3}, E^{(\\mathrm{C})}_{4}, E^{(\\mathrm{U})}_{4}, E^{(\\mathrmD})}_{4}\\big],\n$$\nwhere the subscript indicates the test case index. Each number must be rounded to exactly $8$ digits after the decimal point, expressed as a base-$10$ decimal. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\big[0.12345678,0.00000000,\\dots\\big]$).", "solution": "The problem statement has been critically examined and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of computational physics concerning the numerical solution of a partial differential equation. All constants, variables, boundary conditions, and methods are defined with precision and are mutually consistent.\n\nThe problem requires the calculation of the numerical error for three different finite difference approximations to the advection term in the one-dimensional linear advection-diffusion equation:\n$$\n\\partial_t u(x,t) + a\\,\\partial_x u(x,t) = \\nu\\,\\partial_{xx} u(x,t)\n$$\nThe spatial domain is $x \\in [0,1)$ with periodic boundary conditions. The solution is found by first discretizing the spatial derivatives on a uniform grid of $N$ points $x_i = i\\,\\Delta x$ for $i \\in \\{0, 1, \\dots, N-1\\}$, where $\\Delta x = 1/N$. This semi-discretization transforms the partial differential equation into a system of $N$ coupled ordinary differential equations (ODEs):\n$$\n\\frac{d\\mathbf{u}}{dt} = \\mathbf{L}^{(\\bullet)}\\mathbf{u}(t)\n$$\nwhere $\\mathbf{u}(t)$ is the vector of solution values at the grid points, and $\\mathbf{L}^{(\\bullet)}$ is the matrix representation of the spatial operator for a given advection scheme $(\\bullet) \\in \\{\\mathrm{C}, \\mathrm{U}, \\mathrm{D}\\}$. The problem requires exact time integration of this ODE system, which is achieved by using the matrix exponential:\n$$\n\\mathbf{u}(T) = \\exp(T\\,\\mathbf{L}^{(\\bullet)})\\,\\mathbf{u}(0)\n$$\nThe core of the task is to construct the matrices for the operators, compute the solution at time $T$, and evaluate the discrete $\\ell^2$ error against the provided analytical solution.\n\nThe process for each test case is as follows:\n\n1.  **Matrix Representation of Operators**: The finite difference operators are represented as $N \\times N$ matrices that act on the grid function vector $\\mathbf{u} = (u_0, \\dots, u_{N-1})^\\top$. Due to the periodic boundary conditions, these are circulant matrices.\n\n    - The second-derivative operator, $(\\mathrm{D}^{(2)} \\mathbf{u})_i = \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}$, is represented by a matrix $\\mathbf{M}^{(2)}$ with entries:\n      $$\n      \\mathbf{M}^{(2)}_{i,j} = \\frac{1}{\\Delta x^2} \\begin{cases} -2 & \\text{if } j=i \\\\ 1 & \\text{if } j = (i \\pm 1) \\pmod N \\\\ 0 & \\text{otherwise} \\end{cases}\n      $$\n\n    - The central-difference first-derivative operator, $(\\mathrm{D}^{\\mathrm{C}} \\mathbf{u})_i = \\frac{u_{i+1} - u_{i-1}}{2\\Delta x}$, is represented by a matrix $\\mathbf{M}^{(\\mathrm{C})}$:\n      $$\n      \\mathbf{M}^{(\\mathrm{C})}_{i,j} = \\frac{1}{2\\Delta x} \\begin{cases} 1 & \\text{if } j = (i+1) \\pmod N \\\\ -1 & \\text{if } j = (i-1) \\pmod N \\\\ 0 & \\text{otherwise} \\end{cases}\n      $$\n\n    - The upwind-difference operator, $(\\mathrm{D}^{\\mathrm{U}} \\mathbf{u})_i = \\frac{u_i - u_{i-1}}{\\Delta x}$, is represented by a matrix $\\mathbf{M}^{(\\mathrm{U})}$:\n      $$\n      \\mathbf{M}^{(\\mathrm{U})}_{i,j} = \\frac{1}{\\Delta x} \\begin{cases} 1 & \\text{if } j=i \\\\ -1 & \\text{if } j = (i-1) \\pmod N \\\\ 0 & \\text{otherwise} \\end{cases}\n      $$\n\n    - The downwind-difference operator, $(\\mathrm{D}^{\\mathrm{D}} \\mathbf{u})_i = \\frac{u_{i+1} - u_i}{\\Delta x}$, is represented by a matrix $\\mathbf{M}^{(\\mathrm{D})}$:\n      $$\n      \\mathbf{M}^{(\\mathrm{D})}_{i,j} = \\frac{1}{\\Delta x} \\begin{cases} -1 & \\text{if } j=i \\\\ 1 & \\text{if } j = (i+1) \\pmod N \\\\ 0 & \\text{otherwise} \\end{cases}\n      $$\n\n2.  **Semi-Discrete System Construction**: For each advection scheme $(\\bullet)$, the full semi-discrete operator matrix $\\mathbf{L}^{(\\bullet)}$ is assembled:\n    $$\n    \\mathbf{L}^{(\\bullet)} = -a\\,\\mathbf{M}^{(\\bullet)} + \\nu\\,\\mathbf{M}^{(2)}\n    $$\n    where $\\mathbf{M}^{(\\bullet)}$ is one of $\\mathbf{M}^{(\\mathrm{C})}$, $\\mathbf{M}^{(\\mathrm{U})}$, or $\\mathbf{M}^{(\\mathrm{D})}$.\n\n3.  **Numerical Solution and Error Calculation**: For each test case defined by parameters $(N, a, \\nu, T, k)$:\n    - The grid points $x_i = i/N$ for $i \\in \\{0, \\dots, N-1\\}$ are defined.\n    - The initial condition vector $\\mathbf{u}(0)$ is formed from the initial function $u(x,0) = \\sin(2\\pi k x)$, with elements $(\\mathbf{u}(0))_i = \\sin(2\\pi k x_i)$.\n    - The numerical solution vector at time $T$ is computed by $\\mathbf{u}^{(\\bullet)}(T) = \\exp(T\\,\\mathbf{L}^{(\\bullet)})\\,\\mathbf{u}(0)$. The matrix exponential is computed using a robust numerical algorithm, such as the one available in `scipy.linalg.expm`.\n    - The exact solution vector $\\mathbf{u}_{\\text{exact}}(T)$ is computed at grid points: $(\\mathbf{u}_{\\text{exact}}(T))_i = \\exp(-\\nu(2\\pi k)^2 T) \\sin(2\\pi k(x_i - aT))$.\n    - The discrete $\\ell^2$ error is then calculated according to the given formula:\n      $$\n      E^{(\\bullet)} = \\left( \\Delta x \\sum_{i=0}^{N-1} \\big(u^{(\\bullet)}_i(T) - u_{\\text{exact}}(x_i,T)\\big)^2 \\right)^{1/2}\n      $$\n\nThis procedure is systematically applied to all four test cases, and the resulting twelve error values are collected and formatted as specified. The implementation will utilize the `numpy` library for efficient array and matrix operations and `scipy` for the matrix exponential.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the 1D advection-diffusion equation using finite differences\n    and matrix exponentiation for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: N=64, a=1, nu=0.01, T=0.1, k=1\n        (64, 1.0, 0.01, 0.1, 1),\n        # Case 2: N=64, a=1, nu=0.0005, T=0.05, k=1\n        (64, 1.0, 0.0005, 0.05, 1),\n        # Case 3: N=32, a=1, nu=0.1, T=0.1, k=1\n        (32, 1.0, 0.1, 0.1, 1),\n        # Case 4: N=64, a=1, nu=0.01, T=0.1, k=3\n        (64, 1.0, 0.01, 0.1, 3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, a, nu, T, k = case\n        \n        # Grid setup\n        dx = 1.0 / N\n        x = np.arange(N) * dx\n        \n        # Initial condition vector\n        u0 = np.sin(2 * np.pi * k * x)\n        \n        # Exact solution at time T\n        decay_factor = np.exp(-nu * (2 * np.pi * k)**2 * T)\n        u_exact_T = decay_factor * np.sin(2 * np.pi * k * (x - a * T))\n        \n        # Construct matrix for the second derivative (D^2)\n        dx2 = dx**2\n        M2 = (np.diag(np.full(N, -2.0)) + \n              np.diag(np.full(N - 1, 1.0), 1) + \n              np.diag(np.full(N - 1, 1.0), -1)) / dx2\n        M2[0, -1] = 1.0 / dx2\n        M2[-1, 0] = 1.0 / dx2\n\n        # Construct matrices for the first derivative (D^C, D^U, D^D)\n        # Central difference (D^C)\n        M1_C = (np.diag(np.full(N - 1, 1.0), 1) + \n                np.diag(np.full(N - 1, -1.0), -1)) / (2 * dx)\n        M1_C[0, -1] = -1.0 / (2 * dx)\n        M1_C[-1, 0] = 1.0 / (2 * dx)\n\n        # Upwind difference (D^U)\n        M1_U = (np.diag(np.full(N, 1.0)) + \n                np.diag(np.full(N - 1, -1.0), -1)) / dx\n        M1_U[0, -1] = -1.0 / dx\n        \n        # Downwind difference (D^D)\n        M1_D = (np.diag(np.full(N, -1.0)) + \n                np.diag(np.full(N - 1, 1.0), 1)) / dx\n        M1_D[-1, 0] = 1.0 / dx\n        \n        M1_schemes = {'C': M1_C, 'U': M1_U, 'D': M1_D}\n        case_errors = []\n        \n        for scheme_key in ['C', 'U', 'D']:\n            M1 = M1_schemes[scheme_key]\n            \n            # Assemble the semi-discrete operator matrix L\n            L = -a * M1 + nu * M2\n            \n            # Compute numerical solution using matrix exponential\n            u_T_numerical = expm(T * L) @ u0\n            \n            # Compute the discrete l^2 error\n            error = np.sqrt(dx * np.sum((u_T_numerical - u_exact_T)**2))\n            case_errors.append(error)\n\n        results.extend(case_errors)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.8f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2392370"}]}