{"hands_on_practices": [{"introduction": "Different root-finding methods leave unique \"fingerprints\" in the sequences of estimates they generate. This first practice invites you to become a numerical detective. By carefully analyzing a sequence of iterates for a given function, you can deduce the underlying algorithm and pinpoint the exact moment it switches from a safe, bracketing strategy to a faster, open method. This exercise sharpens your intuition for the distinct behaviors of methods like bisection and Newton's method, moving from abstract formulas to concrete numerical signatures [@problem_id:2402250].", "problem": "Consider the nonlinear equation $f(x)=x^3-x-1=0$, which has a unique real root in the interval $[a_0,b_0]=[1,2]$. A black-box solver is run with the initial bracketing interval $[1,2]$, and it reports the following sequence of root estimates $\\{x_k\\}$:\n$x_0=1.5,\\;\\; x_1=1.25,\\;\\; x_2=1.375,\\;\\; x_3=1.3125,\\;\\; x_4=1.32486,\\;\\; x_5=1.32472.$\nNo other internal state is revealed by the solver. Based solely on the observed iterates and the definition of $f(x)$, select the most plausible hybrid strategy and the iteration index at which the method switched its update rule. For this question, the “switching iteration” is the index $k$ at which the first iterate consistent with a non-bracketing step appears.\n\nWhich option best explains the observed sequence?\n\nA. A hybrid that performs bisection for $k=0,1,2,3$, then switches at $k=4$ to Newton’s method applied to $f(x)$, accepting a Newton step only if it lies within the current bracketing interval.\n\nB. A hybrid that performs the classical regula falsi (false-position) method for $k=0,1,2,3$, then switches at $k=4$ to the secant method.\n\nC. A pure secant method from the start, with initial guesses $x_0=1$ and $x_1=2$.\n\nD. Pure bisection with no switching; all $x_k$ are midpoints of the current bracketing interval.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- Nonlinear equation: $f(x) = x^3 - x - 1 = 0$.\n- Initial bracketing interval: $[a_0, b_0] = [1, 2]$.\n- Sequence of root estimates: $\\{x_k\\}$.\n  $x_0 = 1.5$\n  $x_1 = 1.25$\n  $x_2 = 1.375$\n  $x_3 = 1.3125$\n  $x_4 = 1.32486$\n  $x_5 = 1.32472$\n- Definition of \"switching iteration\": The index $k$ at which the first iterate consistent with a non-bracketing step appears.\n- Task: Identify the most plausible hybrid strategy and switching index from the given options.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem uses standard numerical root-finding algorithms (bisection, Newton's method, regula falsi, secant method) applied to a simple polynomial function. These topics are fundamental to computational physics and numerical analysis. The premise is scientifically sound.\n- **Well-Posed:** The problem provides a sequence of numerical results and asks for the identification of the generating algorithm from a fixed set of choices. This is a well-defined deductive problem.\n- **Objective:** The problem is stated with precise numerical data and standard terminology. It is free of ambiguity and subjective claims.\n- The problem is self-contained, consistent, and scientifically verifiable through calculation. No flaws are identified.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\nThe governing equation is $f(x) = x^3 - x - 1 = 0$. The derivative is $f'(x) = 3x^2 - 1$.\nThe initial interval is $[a_0, b_0] = [1, 2]$. We evaluate the function at the endpoints:\n$f(1) = 1^3 - 1 - 1 = -1$.\n$f(2) = 2^3 - 2 - 1 = 5$.\nSince $f(1)  0$ and $f(2)  0$, there is at least one root in the interval $[1, 2]$. As $f'(x) = 3x^2 - 1  0$ for all $x \\in [1, 2]$, the function is monotonically increasing on this interval, so the root is unique.\n\nWe will now analyze the sequence of iterates $\\{x_k\\}$ to determine the algorithm used.\n\nAnalysis of iterates for $k=0, 1, 2, 3$:\n1.  **k=0**: The first iterate is $x_0 = 1.5$. This is the midpoint of the initial interval $[1, 2]$, since $\\frac{1+2}{2} = 1.5$. This is consistent with the bisection method.\n    We find the new interval for a bracketing method. $f(x_0) = f(1.5) = (1.5)^3 - 1.5 - 1 = 3.375 - 2.5 = 0.875$. Since $f(1)  0$ and $f(1.5)  0$, the new interval is $[a_1, b_1] = [1, 1.5]$.\n\n2.  **k=1**: The second iterate is $x_1 = 1.25$. This is the midpoint of the new interval $[1, 1.5]$, since $\\frac{1+1.5}{2} = 1.25$. This is also consistent with the bisection method.\n    We find the next interval. $f(x_1) = f(1.25) = (1.25)^3 - 1.25 - 1 = 1.953125 - 2.25 = -0.296875$. Since $f(1.25)  0$ and $f(1.5)  0$, the new interval is $[a_2, b_2] = [1.25, 1.5]$.\n\n3.  **k=2**: The third iterate is $x_2 = 1.375$. This is the midpoint of the interval $[1.25, 1.5]$, since $\\frac{1.25+1.5}{2} = 1.375$. This is consistent with the bisection method.\n    We find the next interval. $f(x_2) = f(1.375) = (1.375)^3 - 1.375 - 1 \\approx 2.599609 - 2.375 = 0.224609$. Since $f(1.25)  0$ and $f(1.375)  0$, the new interval is $[a_3, b_3] = [1.25, 1.375]$.\n\n4.  **k=3**: The fourth iterate is $x_3 = 1.3125$. This is the midpoint of the interval $[1.25, 1.375]$, since $\\frac{1.25+1.375}{2} = 1.3125$. This is also consistent with the bisection method.\n    We find the next interval. $f(x_3) = f(1.3125) = (1.3125)^3 - 1.3125 - 1 \\approx 2.260986 - 2.3125 = -0.051514$. Since $f(1.3125)  0$ and $f(1.375)  0$, the new interval is $[a_4, b_4] = [1.3125, 1.375]$.\n\nThe iterates $x_0, x_1, x_2, x_3$ are perfectly explained by the bisection method.\n\nNow we evaluate each option.\n\n**D. Pure bisection with no switching; all $x_k$ are midpoints of the current bracketing interval.**\nTo obtain $x_4$ via bisection, we would take the midpoint of the last interval, $[1.3125, 1.375]$.\n$x_4^{\\text{bisect}} = \\frac{1.3125 + 1.375}{2} = 1.34375$.\nThe given iterate is $x_4 = 1.32486$. Since $1.34375 \\neq 1.32486$, the method is not pure bisection.\nVerdict: **Incorrect**.\n\n**C. A pure secant method from the start, with initial guesses $x_0=1$ and $x_1=2$.**\nThe secant method formula for the next iterate $x_{n+1}$ based on $x_n$ and $x_{n-1}$ is $x_{n+1} = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}$.\nWith initial guesses $p_0=1$ and $p_1=2$, the first computed iterate is:\n$p_2 = p_1 - f(p_1) \\frac{p_1 - p_0}{f(p_1) - f(p_0)} = 2 - f(2) \\frac{2-1}{f(2)-f(1)} = 2 - 5 \\frac{1}{5 - (-1)} = 2 - \\frac{5}{6} = \\frac{7}{6} \\approx 1.16667$.\nThe first iterate in the given sequence is $x_0=1.5$. These do not match.\nVerdict: **Incorrect**.\n\n**B. A hybrid that performs the classical regula falsi (false-position) method for $k=0,1,2,3$, then switches at $k=4$ to the secant method.**\nThe regula falsi method computes the next iterate as the root of the secant line connecting the endpoints of the current bracketing interval $[a_k, b_k]$: $x_{k+1} = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}$.\nFor the first step, $[a_0, b_0] = [1, 2]$. $f(1)=-1, f(2)=5$.\n$x_0^{\\text{RF}} = \\frac{1 \\cdot 5 - 2 \\cdot (-1)}{5 - (-1)} = \\frac{5+2}{6} = \\frac{7}{6} \\approx 1.16667$.\nThe first iterate in the given sequence is $x_0=1.5$. These do not match.\nVerdict: **Incorrect**.\n\n**A. A hybrid that performs bisection for $k=0,1,2,3$, then switches at $k=4$ to Newton’s method applied to $f(x)$, accepting a Newton step only if it lies within the current bracketing interval.**\nAs established, the iterates for $k=0,1,2,3$ are perfectly consistent with the bisection method. This means the first part of the statement is correct.\nThe option proposes a switch at $k=4$. This means the iterate $x_4$ is computed using a new rule, Newton's method. The Newton's method update rule is $x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$.\nA common strategy in hybrid methods is to launch the Newton step from the best current estimate, which is $x_3 = 1.3125$.\nLet's compute $x_4$ using one step of Newton's method starting from $x_3$:\n$x_4^{\\text{Newton}} = x_3 - \\frac{f(x_3)}{f'(x_3)} = 1.3125 - \\frac{(1.3125)^3 - 1.3125 - 1}{3(1.3125)^2 - 1}$.\n$f(1.3125) = -0.051513671875$.\n$f'(1.3125) = 3(1.72265625) - 1 = 5.16796875 - 1 = 4.16796875$.\n$x_4^{\\text{Newton}} = 1.3125 - \\frac{-0.051513671875}{4.16796875} \\approx 1.3125 + 0.012359434 \\approx 1.324859434$.\nRounding this result to $5$ decimal places gives $1.32486$, which is an exact match for the given $x_4$.\nAccording to the problem definition, the switch happens at $k=4$, which is the index of the first iterate ($x_4$) that does not follow the initial (bisection) rule. This is consistent.\nThe option also states a condition: the Newton step is accepted only if it lies within the current bracketing interval. The bracketing interval before computing $x_4$ was $[a_4, b_4] = [1.3125, 1.375]$. The computed iterate $x_4 \\approx 1.32486$ is indeed within this interval, so this condition is met.\nNow let's check the next iterate, $x_5$. If the method continues with Newton's method, the next step would be:\n$x_5^{\\text{Newton}} = x_4 - \\frac{f(x_4)}{f'(x_4)}$, starting from $x_4 = 1.32486$.\n$f(1.32486) \\approx (1.32486)^3 - 1.32486 - 1 \\approx 2.325376 - 1.32486 - 1 = 0.000516$.\n$f'(1.32486) \\approx 3(1.32486)^2 - 1 \\approx 3(1.75525) - 1 = 4.26575$.\n$x_5^{\\text{Newton}} \\approx 1.32486 - \\frac{0.000516}{4.26575} \\approx 1.32486 - 0.000121 \\approx 1.324739$.\nThe given value is $x_5 = 1.32472$. Our calculated value is $1.32474$ (when rounded). This is a very small discrepancy, most likely attributable to rounding in the problem's provided sequence. The given $x_5$ is in fact a better approximation to the true root ($r \\approx 1.324717957...$) than the one calculated from the rounded $x_4$.\nGiven the perfect match for the first $5$ iterates ($x_0$ to $x_4$) and the plausible mechanism of switching from the safe bisection method to the fast-converging Newton's method once the root is localized, this option is by far the most plausible explanation. The other options fail at the first step.\nVerdict: **Correct**.", "answer": "$$\\boxed{A}$$", "id": "2402250"}, {"introduction": "Real-world measurements are rarely perfect and are often contaminated with noise. This practice moves beyond idealized scenarios to explore the critical impact of noisy function evaluations on the logic of a hybrid algorithm [@problem_id:2402215]. You will analyze how even small amounts of random error can corrupt sign-based bracket maintenance and mislead the criteria for accepting a fast, open step. Understanding these failure modes is fundamental to designing solvers that are not just fast, but genuinely robust and reliable in practical applications.", "problem": "Consider a continuously differentiable function $f:\\mathbb{R}\\to\\mathbb{R}$ with a simple root at $r$, i.e., $f(r)=0$ and $f'(r)\\neq 0$. A hybrid Newton–bisection method is used to locate $r$ while maintaining a bracket $[a_k,b_k]$ that initially satisfies $f(a_0)f(b_0)0$. The algorithm proceeds as follows at iteration $k$:\n\n$1.$ The available function evaluation is noisy: for any $x$ one observes $f_{\\text{noisy}}(x)=f(x)+\\delta$, where $\\delta$ is an independent random variable drawn from a uniform distribution on $[-\\eta,\\eta]$ with known $\\eta0$.\n\n$2.$ The derivative $f'(x)$ is available exactly. From the current point $x_k\\in[a_k,b_k]$, form a Newton trial $x_N = x_k - f_{\\text{noisy}}(x_k)/f'(x_k)$.\n\n$3.$ Accept the Newton trial, setting $x_{k+1}=x_N$, if $x_N\\in[a_k,b_k]$ and the sufficient-decrease test $|f_{\\text{noisy}}(x_N)| \\le q\\,|f_{\\text{noisy}}(x_k)|$ holds for a fixed $q\\in(0,1)$. Otherwise, fall back to the bisection update $x_{k+1}=(a_k+b_k)/2$.\n\n$4.$ Update the bracket by replacing the endpoint that has the same measured sign as $f_{\\text{noisy}}(x_{k+1})$; that is, if $\\operatorname{sign}(f_{\\text{noisy}}(a_k))=\\operatorname{sign}(f_{\\text{noisy}}(x_{k+1}))$, set $a_{k+1}=x_{k+1}$ and $b_{k+1}=b_k$, otherwise set $a_{k+1}=a_k$ and $b_{k+1}=x_{k+1}$.\n\nAssume $f$ is monotone in a neighborhood of $r$ and $|f'(x)|$ is bounded away from $0$ there. Analyze the impact of the small random noise on the switching logic and bracket maintenance. Which of the following statements are correct? Select all that apply.\n\nA. When $|f(x_k)|\\ll \\eta$, the probability that $\\operatorname{sign}(f_{\\text{noisy}}(x_k))$ matches the true $\\operatorname{sign}(f(x_k))$ is close to $1/2$, making sign-based bracket updates unreliable and potentially eliminating the true root from the maintained bracket.\n\nB. Because the sufficient-decrease test compares two independent noisy evaluations, the observed ratio $|f_{\\text{noisy}}(x_N)|/|f_{\\text{noisy}}(x_k)|$ can exceed $q$ even if the true ratio $|f(x_N)|/|f(x_k)|$ is below $q$, and the frequency of such false rejections increases with $\\eta$.\n\nC. Introducing a nonzero hysteresis threshold $T$ for sign-based updates—only updating the bracket when $|f_{\\text{noisy}}(a_k)|\\ge T$, $|f_{\\text{noisy}}(b_k)|\\ge T$, and $\\operatorname{sign}(f_{\\text{noisy}}(a_k))\\neq \\operatorname{sign}(f_{\\text{noisy}}(b_k))$ with $T$ chosen on the order of $\\eta$—reduces the chance of spurious bracket errors at the cost of potentially slower progress.\n\nD. Since the noise $\\delta$ has zero mean, the expected sign $\\mathbb{E}[\\operatorname{sign}(f_{\\text{noisy}}(x))]$ equals $\\operatorname{sign}(f(x))$, so a true bracket is preserved with probability $1$ at every iteration, independent of $\\eta$.\n\nE. A mitigation strategy is to average $m$ independent evaluations at the same point, $\\bar{f}_{\\text{noisy}}(x)=\\frac{1}{m}\\sum_{i=1}^m (f(x)+\\delta_i)$, before applying switching and sign tests; this reduces the noise standard deviation by a factor of $\\sqrt{m}$ and improves both the acceptance of good Newton steps and reliability of bracket updates.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The function $f:\\mathbb{R}\\to\\mathbb{R}$ is continuously differentiable.\n- There exists a simple root $r$ such that $f(r)=0$ and $f'(r)\\neq 0$.\n- A hybrid Newton-bisection algorithm is considered.\n- An initial bracket $[a_0, b_k]$ satisfies $f(a_0)f(b_0)0$.\n- Function evaluation is noisy: $f_{\\text{noisy}}(x)=f(x)+\\delta$, where $\\delta$ is a random variable from a uniform distribution $U([-\\eta,\\eta])$ for a known $\\eta0$. Each evaluation draws an independent $\\delta$.\n- The derivative $f'(x)$ is known exactly.\n- At iteration $k$, from $x_k \\in [a_k, b_k]$, a Newton trial point is computed: $x_N = x_k - f_{\\text{noisy}}(x_k)/f'(x_k)$.\n- The Newton step is accepted, setting $x_{k+1}=x_N$, if two conditions are met: $x_N\\in[a_k,b_k]$ and $|f_{\\text{noisy}}(x_N)| \\le q\\,|f_{\\text{noisy}}(x_k)|$ for a fixed $q\\in(0,1)$.\n- If the Newton step is not accepted, a bisection step is taken: $x_{k+1}=(a_k+b_k)/2$.\n- The bracket $[a_k, b_k]$ is updated to $[a_{k+1}, b_{k+1}]$ using the rule: if $\\operatorname{sign}(f_{\\text{noisy}}(a_k))=\\operatorname{sign}(f_{\\text{noisy}}(x_{k+1}))$, then $a_{k+1}=x_{k+1}$ and $b_{k+1}=b_k$; otherwise, $a_{k+1}=a_k$ and $b_{k+1}=x_{k+1}$.\n- It is assumed that $f$ is monotone in a neighborhood of $r$ and $|f'(x)|$ is bounded away from $0$ in that neighborhood.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically grounded. It presents a realistic scenario in computational science where a numerical algorithm must operate with noisy data. The algorithm described is a plausible, though perhaps not ideal, hybrid method. The noise model is standard. The question asks for an analysis of the algorithm's behavior under the specified conditions, which is a legitimate and formalizable problem in numerical analysis and probability theory. The components of the problem (function, noise, algorithm steps) are specified with sufficient clarity to permit a rigorous analysis of the proposed statements. There are no contradictions, requests for infeasible computations, or violations of scientific principles. The potential ambiguity in the bracket update rule for the case when $\\operatorname{sign}(f_{\\text{noisy}}(a_k)) = \\operatorname{sign}(f_{\\text{noisy}}(b_k))$ is not a flaw in the problem statement, but rather a characteristic of the specified algorithm whose consequences are to be analyzed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed to a full solution.\n\n**Analysis of Options**\n\nLet $y = f(x)$ be the true function value and $Y = f_{\\text{noisy}}(x) = y + \\delta$ be the noisy observation, where $\\delta \\sim U(-\\eta, \\eta)$. The probability density function of $\\delta$ is $p(\\delta) = 1/(2\\eta)$ for $\\delta \\in [-\\eta, \\eta]$ and $0$ otherwise.\n\n**A. When $|f(x_k)|\\ll \\eta$, the probability that $\\operatorname{sign}(f_{\\text{noisy}}(x_k))$ matches the true $\\operatorname{sign}(f(x_k))$ is close to $1/2$, making sign-based bracket updates unreliable and potentially eliminating the true root from the maintained bracket.**\n\nLet us compute the probability of a sign error. Assume the true sign is positive, so $y = f(x_k)  0$. An error occurs if the measured value is negative, $Y  0$, which means $y + \\delta  0$, or $\\delta  -y$. The probability of this event is\n$$ P(\\delta  -y) = \\int_{-\\infty}^{-y} p(\\delta) \\,d\\delta $$\nGiven $\\delta \\sim U(-\\eta, \\eta)$, this probability is non-zero only if $-y  -\\eta$, which is equivalent to $y  \\eta$. If $y \\ge \\eta$, the probability of a sign error is $0$. If $0  y  \\eta$, the probability of an error is\n$$ P(\\delta  -y) = \\int_{-\\eta}^{-y} \\frac{1}{2\\eta} \\,d\\delta = \\frac{-y - (-\\eta)}{2\\eta} = \\frac{\\eta - y}{2\\eta} = \\frac{1}{2} - \\frac{y}{2\\eta} $$\nThe probability that the sign is correct is $1 - (\\frac{1}{2} - \\frac{y}{2\\eta}) = \\frac{1}{2} + \\frac{y}{2\\eta}$.\nAs $y \\to 0$ (i.e., $|f(x_k)| \\ll \\eta$), this probability approaches $1/2$. A symmetric argument holds for $y  0$. Thus, when the true function value is much smaller than the noise amplitude, the measured sign is correct with a probability close to $1/2$. This is equivalent to a random guess.\n\nSuch unreliability can corrupt the bracketing. Suppose the true root $r$ is in $[a_k, b_k]$, and $f$ is increasing, so $f(a_k)  0$ and $f(b_k)  0$. Let $x_{k+1}$ be a new point with $a_k  r  x_{k+1}  b_k$, so $f(x_{k+1})  0$. If $|f(a_k)|  \\eta$, it is possible to measure $f_{\\text{noisy}}(a_k)  0$. It is also likely that $f_{\\text{noisy}}(x_{k+1})  0$. According to the update rule, since $\\operatorname{sign}(f_{\\text{noisy}}(a_k)) = \\operatorname{sign}(f_{\\text{noisy}}(x_{k+1}))$, the new bracket becomes $[a_{k+1}, b_{k+1}] = [x_{k+1}, b_k]$. The root $r$ is no longer in this bracket. This confirms that sign-based updates can fail catastrophically.\n\nVerdict: **Correct**.\n\n**B. Because the sufficient-decrease test compares two independent noisy evaluations, the observed ratio $|f_{\\text{noisy}}(x_N)|/|f_{\\text{noisy}}(x_k)|$ can exceed $q$ even if the true ratio $|f(x_N)|/|f(x_k)|$ is below $q$, and the frequency of such false rejections increases with $\\eta$.**\n\nThe sufficient-decrease test is $|Y_N| \\le q |Y_k|$, where $Y_N = f(x_N) + \\delta_N$ and $Y_k = f(x_k) + \\delta_k$. The $\\delta_N$ and $\\delta_k$ are independent random variables.\nA \"false rejection\" occurs when the condition is violated, $|Y_N|  q|Y_k|$, despite the true values satisfying the condition, $|f(x_N)| \\le q|f(x_k)|$.\nLet us denote $y_N = f(x_N)$ and $y_k = f(x_k)$. The condition for false rejection is $|y_N + \\delta_N|  q|y_k + \\delta_k|$.\nThe term $|y_N + \\delta_N|$ can be significantly larger than $|y_N|$ if $\\delta_N$ is large and has the same sign as $y_N$. Conversely, $|y_k + \\delta_k|$ can be smaller than $|y_k|$ if $\\delta_k$ has the opposite sign of $y_k$. The combination of these effects can easily make the observed ratio larger than the true ratio, and thus larger than $q$.\nThe magnitude of the random fluctuations is governed by $\\eta$. As $\\eta$ increases, the support of the distribution of $\\delta$ widens. This increases the variance of $Y_N$ and $Y_k$. A larger variance implies that more extreme deviations from the true values $y_N$ and $y_k$ become more probable. Therefore, the event $|Y_N|  q|Y_k|$ due to random noise, despite $|y_N| \\le q|y_k|$, becomes more frequent as $\\eta$ increases. This leads to a higher rate of false rejections of perfectly good Newton steps, forcing more frequent, and slower, bisection fallbacks.\n\nVerdict: **Correct**.\n\n**C. Introducing a nonzero hysteresis threshold $T$ for sign-based updates—only updating the bracket when $|f_{\\text{noisy}}(a_k)|\\ge T$, $|f_{\\text{noisy}}(b_k)|\\ge T$, and $\\operatorname{sign}(f_{\\text{noisy}}(a_k))\\neq \\operatorname{sign}(f_{\\text{noisy}}(b_k))$ with $T$ chosen on the order of $\\eta$—reduces the chance of spurious bracket errors at the cost of potentially slower progress.**\n\nThis proposed modification is a form of safeguard. By requiring the magnitudes of the noisy function values at the endpoints to be larger than a threshold $T$ (comparable to the noise level $\\eta$), we ensure that we only trust the signs of measurements where the signal-to-noise ratio is sufficiently high. As shown in the analysis of A, sign errors are highly probable when $|f(x)| \\lesssim \\eta$. Setting a threshold $T \\approx \\eta$ effectively filters out these unreliable data points from the bracketing decision. This directly mitigates the risk of spurious bracket updates, like the one described in A where the root is eliminated. So, it reduces errors.\n\nThe cost is that the algorithm may refuse to update the bracket even if a valid smaller bracket is found. As the algorithm converges, the bracket $[a_k, b_k]$ shrinks around $r$. Consequently, both $|f(a_k)|$ and $|f(b_k)|$ will decrease and eventually become smaller than $\\eta$. In this regime, it will become increasingly difficult to find endpoints that satisfy $|f_{\\text{noisy}}(x)| \\ge T$. The bracketing mechanism will stall, halting the convergence of the bracket size. Therefore, this strategy trades guaranteed progress for increased reliability. The statement correctly identifies this trade-off.\n\nVerdict: **Correct**.\n\n**D. Since the noise $\\delta$ has zero mean, the expected sign $\\mathbb{E}[\\operatorname{sign}(f_{\\text{noisy}}(x))]$ equals $\\operatorname{sign}(f(x))$, so a true bracket is preserved with probability $1$ at every iteration, independent of $\\eta$.**\n\nThis statement is fundamentally flawed. The expectation of a non-linear function of a random variable is not generally the function of the expectation. Here, $\\operatorname{sign}(\\cdot)$ is a non-linear function.\nLet $y=f(x)$ and $Y=f(x)+\\delta$. The expected sign is $\\mathbb{E}[\\operatorname{sign}(Y)] = 1 \\cdot P(Y0) + (-1) \\cdot P(Y0)$.\nConsider the case where $0  y \\le \\eta$. We found in A that $P(Y0) = \\frac{1}{2} + \\frac{y}{2\\eta}$ and $P(Y0) = \\frac{1}{2} - \\frac{y}{2\\eta}$.\nThe expectation is:\n$$ \\mathbb{E}[\\operatorname{sign}(Y)] = \\left(\\frac{1}{2} + \\frac{y}{2\\eta}\\right) - \\left(\\frac{1}{2} - \\frac{y}{2\\eta}\\right) = \\frac{y}{\\eta} $$\nThe true sign is $\\operatorname{sign}(y) = 1$. Since $0  y \\le \\eta$, we have $0  y/\\eta \\le 1$. Clearly, $\\mathbb{E}[\\operatorname{sign}(Y)] = y/\\eta \\neq 1 = \\operatorname{sign}(y)$ in this regime. The premise of the statement is false.\nThe conclusion that a true bracket is preserved with probability $1$ is also false, as demonstrated in the analysis for A. The probability of bracket corruption is non-zero and highly dependent on $\\eta$.\n\nVerdict: **Incorrect**.\n\n**E. A mitigation strategy is to average $m$ independent evaluations at the same point, $\\bar{f}_{\\text noisy}(x)=\\frac{1}{m}\\sum_{i=1}^m (f(x)+\\delta_i)$, before applying switching and sign tests; this reduces the noise standard deviation by a factor of $\\sqrt{m}$ and improves both the acceptance of good Newton steps and reliability of bracket updates.**\n\nLet $\\bar{f}_{\\text{noisy}}(x)$ be the averaged measurement. We can write this as:\n$$ \\bar{f}_{\\text{noisy}}(x) = \\frac{1}{m} \\sum_{i=1}^m (f(x) + \\delta_i) = f(x) + \\frac{1}{m} \\sum_{i=1}^m \\delta_i = f(x) + \\bar{\\delta} $$\nwhere $\\bar{\\delta}$ is the average of $m$ independent and identically distributed random variables $\\delta_i \\sim U(-\\eta, \\eta)$.\nThe variance of a single noise sample $\\delta_i$ is $\\operatorname{Var}(\\delta_i) = \\frac{(\\eta - (-\\eta))^2}{12} = \\frac{\\eta^2}{3}$.\nThe variance of the average of $m$ i.i.d. variables is\n$$ \\operatorname{Var}(\\bar{\\delta}) = \\frac{1}{m^2} \\sum_{i=1}^m \\operatorname{Var}(\\delta_i) = \\frac{1}{m^2} (m \\cdot \\operatorname{Var}(\\delta_i)) = \\frac{\\operatorname{Var}(\\delta_i)}{m} = \\frac{\\eta^2}{3m} $$\nThe standard deviation of the average noise is $\\sigma_{\\bar{\\delta}} = \\sqrt{\\frac{\\eta^2}{3m}} = \\frac{\\eta}{\\sqrt{3m}}$. The standard deviation of the original noise is $\\sigma_\\delta = \\frac{\\eta}{\\sqrt{3}}$.\nThe ratio of the standard deviations is $\\frac{\\sigma_{\\bar{\\delta}}}{\\sigma_\\delta} = \\frac{1}{\\sqrt{m}}$. So, averaging reduces the noise standard deviation by a factor of $\\sqrt{m}$. This part of the statement is correct.\n\nBy reducing the effective noise level, the signal-to-noise ratio is improved for any given point $x$. A smaller effective $\\eta$ means that the conditions analyzed in A, B, and C are ameliorated.\n- The probability of a sign error (A) decreases because the effective noise amplitude is smaller.\n- The probability of false rejection of a Newton step (B) decreases because the random fluctuations in the measured values are smaller.\n- The reliability of all decisions is improved. This is a standard and fundamental technique in experimental science. The cost is an increase in computational/measurement effort by a factor of $m$ for each evaluation. The statement correctly identifies the statistical benefit and its positive consequences on the algorithm.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ABCE}$$", "id": "2402215"}, {"introduction": "A method with a higher order of convergence is not always the most efficient choice overall. This final challenge asks you to quantify this trade-off by implementing and comparing two sophisticated hybrid solvers: one based on the second-order Newton's method and another using the third-order Halley's method [@problem_id:2402194]. By deriving the higher-order method from first principles and performing a cost-benefit analysis based on the computational expense of function and derivative evaluations, you will gain practical insight into how to select the optimal strategy for a given problem.", "problem": "You are tasked with designing and evaluating a globally convergent hybrid root-finding algorithm in which the open component is a third-order method. The core setting is to find a simple root $x^\\star$ of a smooth nonlinear scalar function $f(x)$ given a guaranteed bracketing interval $[a,b]$ with $f(a)f(b)  0$. The foundation for your design must be based on the following widely accepted principles: (i) the Intermediate Value Theorem, which guarantees the existence of at least one root in $[a,b]$ if $f(a)f(b)  0$, and (ii) Taylor expansions of smooth functions, which justify local high-order open-method steps near the root. The angle unit for trigonometric functions must be radians.\n\nYour tasks are:\n\n- Derive, from Taylor expansions and without using any pre-provided iteration formula, an open-step update that attains third-order local convergence for a simple root under standard smoothness and nondegeneracy conditions. This open step must only use $f(x)$, $f'(x)$, and $f''(x)$ evaluated at the current iterate.\n- Implement a robust hybrid method that:\n  - Maintains a bracketing interval $[a_k,b_k]$ at each iteration $k$ with $f(a_k)f(b_k) \\le 0$.\n  - Proposes an open step from the current point $x_k$ using your third-order update.\n  - Accepts the open step only if it remains inside $(a_k,b_k)$ and strictly decreases the residual magnitude. Otherwise, it must fall back to a bisection step.\n  - Updates the bracketing interval using the sign of the function at the accepted new point.\n  - Terminates when either the absolute residual is below a prescribed tolerance or the bracket width is sufficiently small.\n- For comparison, also implement a baseline hybrid method where the open step is the classical Newton update (second-order local convergence), with the same safeguards and fallbacks as above.\n- Model computational cost as a weighted sum of evaluations, where the cost of one evaluation of $f(x)$ is $c_0$, of $f'(x)$ is $c_1$, and of $f''(x)$ is $c_2$. Count every evaluation performed by your algorithm and compute the total weighted cost for each method.\n\nUse absolute residual tolerance $|f(x)| \\le \\varepsilon$ with $\\varepsilon = 10^{-12}$ and a bracket-width tolerance $|b-a| \\le \\varepsilon$, and enforce a maximum of $100$ iterations. Angles must be in radians. There are no physical units involved in the final answers.\n\nTest Suite Specification. Implement $f(x)$, $f'(x)$, and $f''(x)$ analytically for each of the following test cases, each of which includes the bracketing interval $[a,b]$ and evaluation costs $(c_0,c_1,c_2)$:\n\n- Case $1$: $f(x) = \\cos(x) - x$, $[a,b] = [0,1]$, $(c_0,c_1,c_2) = (1.0,1.0,1.0)$.\n- Case $2$: $f(x) = \\cos(x) - x$, $[a,b] = [0,1]$, $(c_0,c_1,c_2) = (1.0,1.0,10.0)$.\n- Case $3$: $f(x) = x^3 - 2x - 5$, $[a,b] = [2,3]$, $(c_0,c_1,c_2) = (1.0,1.0,1.0)$.\n- Case $4$: $f(x) = \\mathrm{e}^x - 3$, $[a,b] = [0,2]$, $(c_0,c_1,c_2) = (1.0,1.0,5.0)$.\n- Case $5$: $f(x) = \\tanh(x) - 0.5$, $[a,b] = [0,2]$, $(c_0,c_1,c_2) = (1.0,2.0,4.0)$.\n- Case $6$: $f(x) = x^5 - x - 1$, $[a,b] = [1,2]$, $(c_0,c_1,c_2) = (1.0,1.0,3.0)$.\n\nFor each case, run both hybrid solvers (the third-order one you derived and the Newton-based baseline) from the interval midpoint $x_0 = (a+b)/2$, apply the same acceptance criteria and fallbacks, and compute the total weighted cost of function and derivative evaluations.\n\nFinal Output Requirement. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, a valid output might be of the form `[True,False,True,True,False,True]`.", "solution": "The problem presented is a well-defined exercise in numerical analysis, concerning the design and comparative evaluation of hybrid root-finding algorithms. The premises are scientifically sound, the objectives are clear and quantitative, and all necessary data and constraints are provided. The problem is therefore deemed valid and a full solution shall be presented.\n\nThe task is to construct a globally convergent root-finding algorithm by combining a robust, bracketing method (bisection) with a fast, open method of third-order convergence. This hybrid strategy aims to leverage the rapid local convergence of the open method while retaining the guaranteed convergence of the bracketing method.\n\nFirst, the derivation of the third-order open-step update is required, based on Taylor series expansions and using only the function $f(x)$ and its first two derivatives, $f'(x)$ and $f''(x)$. Let $x_k$ be the current approximation to a simple root $x^\\star$, where $f(x^\\star) = 0$. The Taylor series expansion of $f(x)$ around $x_k$ is:\n$$ f(x^\\star) = f(x_k) + (x^\\star - x_k)f'(x_k) + \\frac{(x^\\star-x_k)^2}{2!}f''(x_k) + O\\left((x^\\star-x_k)^3\\right) $$\nSetting $f(x^\\star) = 0$ and letting $h = x_{k+1} - x_k$ be the step to the next iterate, we use $x_{k+1}$ as an improved approximation for $x^\\star$. Thus, we have the approximation $x^\\star - x_k \\approx h$. Substituting this into the truncated series gives a quadratic equation for the step $h$:\n$$ 0 \\approx f(x_k) + h f'(x_k) + \\frac{h^2}{2} f''(x_k) $$\nTo avoid solving a quadratic equation, we can linearize this expression by using a lower-order approximation for one of the $h$ factors in the quadratic term. The first-order Taylor approximation, which forms the basis of Newton's method, provides the estimate $h \\approx -f(x_k)/f'(x_k)$. Substituting this into the equation above yields:\n$$ 0 \\approx f(x_k) + h f'(x_k) + \\frac{h}{2} \\left( -\\frac{f(x_k)}{f'(x_k)} \\right) f''(x_k) $$\nSolving this equation for $h$ gives the update step:\n$$ h \\left( f'(x_k) - \\frac{f(x_k) f''(x_k)}{2 f'(x_k)} \\right) \\approx -f(x_k) $$\n$$ h = -\\frac{f(x_k)}{f'(x_k) - \\frac{f(x_k) f''(x_k)}{2 f'(x_k)}} $$\nThe next iterate is then $x_{k+1} = x_k + h$. This formula is known as Halley's method, and it exhibits cubic (third-order) local convergence for a simple root under standard smoothness conditions.\n\nFor comparison, a baseline hybrid method using the classical second-order Newton's method is required. The Newton step is derived from the first-order Taylor expansion:\n$$ 0 \\approx f(x_k) + (x^\\star - x_k)f'(x_k) $$\nThis directly yields the step $h = x_{k+1} - x_k \\approx -f(x_k)/f'(x_k)$, and the update rule:\n$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\n\nThe hybrid algorithm integrating these open steps is designed as follows. It starts with a bracketing interval $[a_0, b_0]$ where $f(a_0)f(b_0)  0$ and an initial guess $x_0 = (a_0+b_0)/2$. At each iteration $k$, given the current bracket $[a_k, b_k]$ and iterate $x_k$:\n$1$. An open step is proposed from $x_k$ to a new point $x_{open}$, calculated using either Halley's method or Newton's method. This step itself requires evaluations of $f(x_k)$, $f'(x_k)$, and for Halley's method, $f''(x_k)$.\n$2$. The proposed step is validated against two safeguarding conditions:\n   a. Bracket condition: The point $x_{open}$ must lie strictly within the current bracketing interval, i.e., $x_{open} \\in (a_k, b_k)$.\n   b. Progress condition: The absolute value of the function at the new point must be strictly less than at the current point, i.e., $|f(x_{open})|  |f(x_k)|$. This requires an additional evaluation of $f$ at $x_{open}$.\n$3$. If both safeguards are passed, the open step is accepted, and the next iterate is set to $x_{k+1} = x_{open}$.\n$4$. If the open step is rejected (due to failing a safeguard, or due to a numerical issue like division by zero), the algorithm falls back to a bisection step. The next iterate is set to the midpoint of the bracket, $x_{k+1} = (a_k + b_k) / 2$.\n$5$. The bracketing interval is updated. Depending on the sign of $f(x_{k+1})$, the new interval $[a_{k+1}, b_{k+1}]$ becomes either $[a_k, x_{k+1}]$ or $[x_{k+1}, b_k]$ to preserve the property $f(a_{k+1})f(b_{k+1}) \\le 0$.\n$6$. The process stops when either the bracket width $|b_k - a_k|$ or the absolute function residual $|f(x_k)|$ is less than or equal to a tolerance $\\varepsilon = 10^{-12}$, or after a maximum of $100$ iterations.\n\nThe computational cost is evaluated using a weighted sum of the number of function and derivative evaluations. If an algorithm completes with $N_0$ evaluations of $f(x)$, $N_1$ evaluations of $f'(x)$, and $N_2$ evaluations of $f''(x)$, with per-evaluation costs $c_0, c_1, c_2$ respectively, the total cost $C$ is calculated as:\n$$ C = N_0 c_0 + N_1 c_1 + N_2 c_2 $$\nThis cost model enables a fair comparison between the second-order and third-order hybrid methods. The question is whether the potential reduction in the number of iterations for the third-order method is sufficient to offset the additional cost of evaluating the second derivative $f''(x)$ at each open-step attempt.", "answer": "```python\nimport numpy as np\n\nclass FuncWithCounter:\n    \"\"\"A wrapper class for a function and its derivatives to count evaluations.\"\"\"\n    def __init__(self, f_def, df_def, d2f_def, costs):\n        self._f = f_def\n        self._df = df_def\n        self._d2f = d2f_def\n        self.costs = costs\n        self.f_evals = 0\n        self.df_evals = 0\n        self.d2f_evals = 0\n\n    def f(self, x):\n        self.f_evals += 1\n        return self._f(x)\n\n    def df(self, x):\n        self.df_evals += 1\n        return self._df(x)\n\n    def d2f(self, x):\n        self.d2f_evals += 1\n        return self._d2f(x)\n\n    def reset(self):\n        \"\"\"Resets all evaluation counters to zero.\"\"\"\n        self.f_evals = 0\n        self.df_evals = 0\n        self.d2f_evals = 0\n\n    def total_cost(self):\n        \"\"\"Computes the total weighted cost of all evaluations.\"\"\"\n        c0, c1, c2 = self.costs\n        return self.f_evals * c0 + self.df_evals * c1 + self.d2f_evals * c2\n\ndef hybrid_solver(f_obj, a_start, b_start, tol, max_iter, open_step_type):\n    \"\"\"\n    Globally convergent hybrid root-finding algorithm.\n\n    Args:\n        f_obj (FuncWithCounter): The function object with evaluation counters.\n        a_start (float): The start of the bracketing interval.\n        b_start (float): The end of the bracketing interval.\n        tol (float): The tolerance for termination.\n        max_iter (int): The maximum number of iterations.\n        open_step_type (str): The type of open step ('newton' or 'halley').\n\n    Returns:\n        float: The total computational cost.\n    \"\"\"\n    a, b = float(a_start), float(b_start)\n    f_obj.reset()\n\n    f_a = f_obj.f(a)\n    f_b = f_obj.f(b)\n\n    if np.sign(f_a) == np.sign(f_b):\n        return float('inf')\n\n    x_cur = (a + b) / 2.0\n    f_cur = f_obj.f(x_cur)\n\n    for _ in range(max_iter):\n        if abs(f_cur) = tol or (b - a) = tol:\n            break\n\n        # Propose an open step from the current point\n        open_step_valid = False\n        try:\n            df_cur = f_obj.df(x_cur)\n            if abs(df_cur)  1e-15: # Avoid division by zero\n                raise ValueError(\"Derivative is too small.\")\n            \n            if open_step_type == 'newton':\n                x_open = x_cur - f_cur / df_cur\n                open_step_valid = True\n            elif open_step_type == 'halley':\n                d2f_cur = f_obj.d2f(x_cur)\n                denom = df_cur - (f_cur * d2f_cur) / (2.0 * df_cur)\n                if abs(denom)  1e-15: # Avoid division by zero\n                    raise ValueError(\"Halley denominator is too small.\")\n                x_open = x_cur - f_cur / denom\n                open_step_valid = True\n        except (ValueError, ZeroDivisionError):\n            open_step_valid = False\n\n        accepted = False\n        if open_step_valid and (a  x_open  b):\n            f_open = f_obj.f(x_open)\n            if abs(f_open)  abs(f_cur):\n                x_next = x_open\n                f_next = f_open\n                accepted = True\n\n        if not accepted:\n            x_next = (a + b) / 2.0\n            if x_next == x_cur: # Interval is at machine precision limit\n                f_next = f_cur\n            else:\n                f_next = f_obj.f(x_next)\n\n        # Update current point and function value\n        x_cur = x_next\n        f_cur = f_next\n        \n        # Update bracket\n        if np.sign(f_cur) == np.sign(f_a):\n            a = x_cur\n            f_a = f_cur\n        else:\n            b = x_cur\n            f_b = f_cur\n\n    return f_obj.total_cost()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'func': lambda x: np.cos(x) - x, 'df': lambda x: -np.sin(x) - 1.0, 'd2f': lambda x: -np.cos(x),\n         'interval': (0.0, 1.0), 'costs': (1.0, 1.0, 1.0)},\n        {'func': lambda x: np.cos(x) - x, 'df': lambda x: -np.sin(x) - 1.0, 'd2f': lambda x: -np.cos(x),\n         'interval': (0.0, 1.0), 'costs': (1.0, 1.0, 10.0)},\n        {'func': lambda x: x**3 - 2.0*x - 5.0, 'df': lambda x: 3.0*x**2 - 2.0, 'd2f': lambda x: 6.0*x,\n         'interval': (2.0, 3.0), 'costs': (1.0, 1.0, 1.0)},\n        {'func': lambda x: np.exp(x) - 3.0, 'df': lambda x: np.exp(x), 'd2f': lambda x: np.exp(x),\n         'interval': (0.0, 2.0), 'costs': (1.0, 1.0, 5.0)},\n        {'func': lambda x: np.tanh(x) - 0.5, 'df': lambda x: 1.0 / (np.cosh(x)**2), 'd2f': lambda x: -2.0 * np.tanh(x) / (np.cosh(x)**2),\n         'interval': (0.0, 2.0), 'costs': (1.0, 2.0, 4.0)},\n        {'func': lambda x: x**5 - x - 1.0, 'df': lambda x: 5.0*x**4 - 1.0, 'd2f': lambda x: 20.0*x**3,\n         'interval': (1.0, 2.0), 'costs': (1.0, 1.0, 3.0)}\n    ]\n\n    results = []\n    TOL = 1e-12\n    MAX_ITER = 100\n\n    for case in test_cases:\n        a, b = case['interval']\n        f_obj = FuncWithCounter(case['func'], case['df'], case['d2f'], case['costs'])\n\n        cost_newton = hybrid_solver(f_obj, a, b, TOL, MAX_ITER, 'newton')\n        cost_halley = hybrid_solver(f_obj, a, b, TOL, MAX_ITER, 'halley')\n        \n        results.append(cost_halley  cost_newton)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2402194"}]}