{"hands_on_practices": [{"introduction": "A primary application of interpolation in the physical sciences is to estimate function values between discrete data points obtained from experiments. This exercise provides a hands-on scenario from particle physics, where you will use Newton's divided differences to model the differential cross-section $\\sigma(\\theta)$ from a few measurements. By constructing and evaluating an interpolating polynomial, you'll practice the core workflow of turning sparse experimental data into a continuous model. [@problem_id:2428275]", "problem": "A scattering experiment reports discrete measurements of the differential cross-section $\\sigma(\\theta)$ at several polar angles $\\theta$ (angle unit: radians), expressed in barns per steradian (barn/sr). Assuming that $\\sigma(\\theta)$ is sufficiently smooth over the angular interval spanned by the given data in each case, determine an interpolated estimate of $\\sigma(\\pi/2)$ by evaluating the unique polynomial of minimal degree that exactly fits the provided data points for each case. Angles must be treated and computed in radians. The final interpolated values must be expressed in barns per steradian and rounded to $10$ decimal places.\n\nUse the following three independent test cases. In each case, the input consists of a strictly increasing list of angles $\\{\\theta_i\\}$ in radians and the corresponding measured cross-sections $\\{\\sigma_i\\}$ in barns per steradian. For each case, compute the interpolated estimate of $\\sigma(\\pi/2)$ in barns per steradian.\n\nTest Case A (general interior interpolation):\n- Angles $\\{\\theta_i\\}$: $\\{0.5,\\,1.2,\\,2.0\\}$ radians.\n- Cross-sections $\\{\\sigma_i\\}$: $\\{12.2,\\,10.1,\\,7.3\\}$ barn/sr.\n\nTest Case B (includes the target angle as a data node for an exact match in exact arithmetic):\n- Angles $\\{\\theta_i\\}$: $\\{0.2,\\,1.0,\\,1.5707963267948966,\\,2.4\\}$ radians.\n- Cross-sections $\\{\\sigma_i\\}$: $\\{11.0,\\,10.0,\\,9.87654321,\\,7.0\\}$ barn/sr.\n\nTest Case C (nonuniform spacing with more nodes):\n- Angles $\\{\\theta_i\\}$: $\\{0.1,\\,0.9,\\,1.3,\\,1.8,\\,2.5\\}$ radians.\n- Cross-sections $\\{\\sigma_i\\}$: $\\{15.0,\\,11.0,\\,9.5,\\,8.2,\\,6.0\\}$ barn/sr.\n\nYour program must:\n- For each case, construct the unique polynomial of degree at most $n-1$ that interpolates the $n$ provided points $(\\theta_i,\\sigma_i)$.\n- Evaluate that polynomial at $\\theta=\\pi/2$ radians.\n- Round each resulting value to $10$ decimal places.\n- Produce a single line of output containing the results for the three cases in order A, B, C as a comma-separated list enclosed in square brackets, for example $[\\text{result}_A,\\text{result}_B,\\text{result}_C]$.\n\nAngle unit: radians. Cross-sections must be reported in barns per steradian (barn/sr), rounded to $10$ decimal places. The final output must be a single line exactly in the format described above.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in computational physics, mathematically well-posed, and an objectively defined problem of polynomial interpolation. All necessary data and conditions for each of the three test cases are provided, and there are no internal contradictions. Therefore, I will proceed with a full solution.\n\nThe objective is to estimate the differential cross-section $\\sigma(\\theta)$ at the specific polar angle $\\theta = \\pi/2$ radians for three independent data sets. The prescribed method is to use the unique interpolating polynomial of minimal degree that passes through the given data points $(\\theta_i, \\sigma_i)$. For a set of $n$ distinct points, this corresponds to the unique polynomial of degree at most $n-1$.\n\nA numerically robust and computationally efficient method for this task is the Newton form of the interpolating polynomial, which utilizes divided differences. This approach is particularly suitable as it avoids the ill-conditioning often associated with solving for the polynomial's coefficients in the monomial basis via the Vandermonde matrix.\n\nGiven a set of $n$ data points $(x_0, y_0), (x_1, y_1), \\dots, (x_{n-1}, y_{n-1})$, the Newton form of the interpolating polynomial $P(x)$ is given by:\n$$ P(x) = \\sum_{k=0}^{n-1} c_k \\prod_{j=0}^{k-1} (x - x_j) $$\nwhere the coefficients $c_k$ are the divided differences, defined as $c_k = f[x_0, x_1, \\dots, x_k]$. These are computed recursively:\n$$ f[x_i] = y_i $$\n$$ f[x_i, \\dots, x_{i+j}] = \\frac{f[x_{i+1}, \\dots, x_{i+j}] - f[x_i, \\dots, x_{i+j-1}]}{x_{i+j} - x_i} $$\nThe coefficients $c_k$ are the top diagonal entries of the divided differences table. Once these coefficients are determined, the polynomial can be efficiently evaluated at a point $x_{\\text{eval}}$ using Horner's method. This is evaluated from the inside out, or, more commonly in implementation, iteratively:\nLet $val = c_{n-1}$.\nFor $i$ from $n-2$ down to $0$: $val = val \\cdot (x_{\\text{eval}} - x_i) + c_i$.\nThe final $val$ is $P(x_{\\text{eval}})$.\n\nWe will now apply this procedure to each test case, with $\\theta$ corresponding to $x$ and $\\sigma(\\theta)$ corresponding to $y$. The target angle is $\\theta_{\\text{eval}} = \\pi/2$.\n\nTest Case A:\nThe data provided are $n=3$ points:\n- Angles $\\{\\theta_i\\}: \\{0.5, 1.2, 2.0\\}$\n- Cross-sections $\\{\\sigma_i\\}: \\{12.2, 10.1, 7.3\\}$\nThe interpolating polynomial is of degree at most $3-1 = 2$. We construct the divided differences table:\n- Zeroth-order differences: $f[\\theta_0] = 12.2$, $f[\\theta_1] = 10.1$, $f[\\theta_2] = 7.3$\n- First-order differences:\n  - $f[\\theta_0, \\theta_1] = \\frac{10.1 - 12.2}{1.2 - 0.5} = \\frac{-2.1}{0.7} = -3.0$\n  - $f[\\theta_1, \\theta_2] = \\frac{7.3 - 10.1}{2.0 - 1.2} = \\frac{-2.8}{0.8} = -3.5$\n- Second-order difference:\n  - $f[\\theta_0, \\theta_1, \\theta_2] = \\frac{-3.5 - (-3.0)}{2.0 - 0.5} = \\frac{-0.5}{1.5} = -1/3$\nThe coefficients for the Newton polynomial are $c_0 = 12.2$, $c_1 = -3.0$, and $c_2 = -1/3$. We evaluate the polynomial at $\\theta = \\pi/2$:\n$P(\\pi/2) = c_2 \\cdot (\\pi/2 - \\theta_1) + c_1$\n$P(\\pi/2) = (P(\\pi/2)) \\cdot (\\pi/2 - \\theta_0) + c_0$\nNumerically, this yields:\n$P(\\pi/2) = (-1/3) \\cdot (\\pi/2 - 1.2) - 3.0$\n$P(\\pi/2) = ((-1/3) \\cdot (\\pi/2 - 1.2) - 3.0) \\cdot (\\pi/2 - 0.5) + 12.2 \\approx 8.8552689974$.\n\nTest Case B:\nThe data provided are $n=4$ points:\n- Angles $\\{\\theta_i\\}: \\{0.2, 1.0, 1.5707963267948966, 2.4\\}$\n- Cross-sections $\\{\\sigma_i\\}: \\{11.0, 10.0, 9.87654321, 7.0\\}$\nThe target angle of interpolation is $\\theta = \\pi/2$. We observe that the third data node $\\theta_2 = 1.5707963267948966$ is the floating-point representation of $\\pi/2$. By definition, the interpolating polynomial must pass through all given data points exactly. Therefore, $P(\\theta_i) = \\sigma_i$ for all $i = 0, 1, 2, 3$.\nConsequently, for $\\theta = \\theta_2 = \\pi/2$, the value of the polynomial is simply the corresponding cross-section $\\sigma_2$.\n$P(\\pi/2) = \\sigma_2 = 9.87654321$. No further computation is required.\n\nTest Case C:\nThe data provided are $n=5$ points:\n- Angles $\\{\\theta_i\\}: \\{0.1, 0.9, 1.3, 1.8, 2.5\\}$\n- Cross-sections $\\{\\sigma_i\\}: \\{15.0, 11.0, 9.5, 8.2, 6.0\\}$\nThe interpolating polynomial is of degree at most $5-1=4$. The procedure is identical to that of Case A, but for a larger set of points. We compute the divided differences to find the Newton coefficients $c_0, c_1, c_2, c_3, c_4$.\n- $c_0 = f[\\theta_0] = 15.0$\n- $c_1 = f[\\theta_0, \\theta_1] = -5.0$\n- $c_2 = f[\\theta_0, \\theta_1, \\theta_2] \\approx 1.0416666667$\n- $c_3 = f[\\theta_0, \\theta_1, \\theta_2, \\theta_3] \\approx 0.1388888889$\n- $c_4 = f[\\theta_0, \\theta_1, \\theta_2, \\theta_3, \\theta_4] \\approx -0.50843254$\nUsing these coefficients and Horner's method to evaluate $P(\\pi/2)$:\n$P(\\pi/2) = ((((c_4(\\pi/2 - \\theta_3) + c_3)(\\pi/2 - \\theta_2) + c_2)(\\pi/2 - \\theta_1) + c_1)(\\pi/2 - \\theta_0) + c_0)$\nThis calculation yields the value $P(\\pi/2) \\approx 8.9410183358$.\n\nThe final results for Cases A, B, and C, rounded to $10$ decimal places as required, are $8.8552689974$, $9.87654321$, and $8.9410183358$ respectively.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the polynomial interpolation problem for three test cases.\n    \"\"\"\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case A: General interior interpolation\n        (np.array([0.5, 1.2, 2.0]), np.array([12.2, 10.1, 7.3])),\n        # Case B: Target angle is a data node\n        (np.array([0.2, 1.0, 1.5707963267948966, 2.4]), np.array([11.0, 10.0, 9.87654321, 7.0])),\n        # Case C: Nonuniform spacing with more nodes\n        (np.array([0.1, 0.9, 1.3, 1.8, 2.5]), np.array([15.0, 11.0, 9.5, 8.2, 6.0])),\n    ]\n\n    target_angle = np.pi / 2\n    results = []\n\n    def get_newton_coeffs(x_nodes: np.ndarray, y_nodes: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the coefficients of the Newton form of the interpolating polynomial.\n        The algorithm computes the divided differences table in-place.\n        \"\"\"\n        n = len(x_nodes)\n        coeffs = np.copy(y_nodes).astype(float)\n        for k in range(1, n):\n            # To compute the k-th order differences, we need x distance of k\n            # coeffs[i] becomes f[x_{i-k}, ..., x_i]\n            for i in range(n - 1, k - 1, -1):\n                coeffs[i] = (coeffs[i] - coeffs[i - 1]) / (x_nodes[i] - x_nodes[i - k])\n        return coeffs\n\n    def eval_newton_poly(x_nodes: np.ndarray, coeffs: np.ndarray, x_eval: float) -> float:\n        \"\"\"\n        Evaluates the Newton polynomial at a given point using Horner's method.\n        \"\"\"\n        n = len(coeffs)\n        # Start from the highest order coefficient\n        result = coeffs[n - 1]\n        # Apply Horner's method\n        for i in range(n - 2, -1, -1):\n            result = result * (x_eval - x_nodes[i]) + coeffs[i]\n        return result\n\n    for x_nodes, y_nodes in test_cases:\n        # Check if the target angle is one of the nodes\n        # Use np.isclose for robust floating-point comparison\n        is_node, indices = np.isclose(x_nodes, target_angle), np.where(np.isclose(x_nodes, target_angle))\n        if np.any(is_node):\n            # If the target is a node, the interpolated value is the node's y-value\n            result = y_nodes[indices[0][0]]\n        else:\n            # Otherwise, perform Newton interpolation\n            coeffs = get_newton_coeffs(x_nodes, y_nodes)\n            result = eval_newton_poly(x_nodes, coeffs, target_angle)\n        \n        # Round the result to 10 decimal places\n        results.append(round(result, 10))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2428275"}, {"introduction": "An interpolating polynomial is not merely a curve that connects points; it is a smooth function that locally approximates the underlying function's geometry. This practice extends interpolation beyond simple value estimation, guiding you to extract derivatives from the interpolant to calculate the curvature of a curve. By estimating the osculating circle from just three points, you will connect the algebraic tool of divided differences to the geometric concept of curvature and explore its sensitivity to noise. [@problem_id:2428257]", "problem": "You are given three distinct sample points $(x_0,y_0)$, $(x_1,y_1)$, and $(x_2,y_2)$ that lie on a smooth planar curve which, in the vicinity of $x_1$, can be represented as a single-valued function $y=y(x)$. Assume the underlying curve is a circle of unknown center $(a,b)$ and unknown radius $R$. Your task is to use polynomial interpolation, constructed by Newton's divided differences, to estimate the circle's center and radius from just these three points, and then to examine how sensitive these estimates are to observation noise in the $y$-coordinates.\n\nFundamental base and goals:\n- Start from the definition of Newton's divided differences and the construction of the second-degree interpolating polynomial through $(x_0,y_0)$, $(x_1,y_1)$, and $(x_2,y_2)$.\n- Using only these foundations and core geometric facts, derive from first principles how to obtain the first and second derivatives of the quadratic interpolant at $x_1$ without resorting to any result not derivable from the definitions.\n- From the geometric relationship between curvature of a plane curve and the osculating circle, derive how the curvature at $x_1$ determines the radius of the osculating circle and the location of its center relative to $(x_1,y_1)$ and the local tangent and normal directions.\n- Implement this method in code and evaluate it on a small test suite that includes noise-free and noisy data, symmetric and asymmetric node placement, and a nearly straight arc (large radius) case. All quantities are treated as dimensionless; no physical units are involved.\n\nDetailed requirements:\n1. Construct the unique quadratic interpolant $p_2(x)$ that passes through the three given points by using Newton's divided differences. Express $p_2(x)$ in the Newton form based on the nodes $x_0$, $x_1$, and $x_2$. Then, by differentiating this polynomial, obtain $p_2'(x)$ and $p_2''(x)$ and hence compute $p_2'(x_1)$ and $p_2''(x_1)$ directly from the divided differences. Do not assume any special formulas beyond what follows from the definitions of divided differences and polynomial differentiation.\n2. From geometric principles, the curvature $\\kappa$ of a planar curve given as $y=y(x)$ relates to its first and second derivatives at a point. Use this relation to connect the curvature at $x_1$ to the radius $R$ of the osculating circle. Additionally, determine the center $(a,b)$ of the osculating circle by moving from $(x_1,y_1)$ along the appropriate unit normal direction by a distance equal to the radius. Use the sign of the local concavity to choose the correct normal direction. In your derivation, do not assume any closed-form shortcut beyond basic differential geometry definitions and vector normalization.\n3. Implement a program that, for each provided test case, does the following:\n   - Generates three points $(x_i,y_i)$ on a circle with given true parameters $(a,b,R)$ by using $y_i=b+s\\sqrt{R^2-(x_i-a)^2}$ for a specified branch sign $s \\in \\{+1,-1\\}$. Use $s=+1$ in all cases below.\n   - Optionally adds independent, zero-mean Gaussian noise with a specified standard deviation $\\sigma$ to each $y_i$, leaving $x_i$ unchanged, to obtain observed values $\\tilde{y}_i$.\n   - Constructs the quadratic Newton interpolant through $(x_i,\\tilde{y}_i)$, evaluates the first and second derivatives at $x_1$, and from these estimates computes the osculating circle center $(\\hat{a},\\hat{b})$ and radius $\\hat{R}$.\n   - Reports the absolute errors $(|\\hat{a}-a|,|\\hat{b}-b|,|\\hat{R}-R|)$ for the case.\n\nNumerical and formatting requirements:\n- All computations are dimensionless.\n- If noise is used, it is applied as independent samples from a normal distribution with zero mean and the specified standard deviation $\\sigma$ added to each $y_i$. Use the provided random seeds for reproducibility. Angles, if any are implicitly involved in derivations, need not be reported and no angular unit is required.\n- For each test case, report a list of three floating-point numbers corresponding to $(|\\hat{a}-a|,|\\hat{b}-b|,|\\hat{R}-R|)$. Round each reported float to $12$ decimal places before printing.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case's triple enclosed in its own square brackets and no spaces. For example, a valid format is $[[0.0,0.0,0.0],[1.0,2.0,3.0]]$.\n\nTest suite:\n- Case $1$ (noise-free, symmetric nodes, moderate curvature):\n  - True parameters: $(a,b,R)=(0.75,-0.5,1.9)$.\n  - Nodes: $x_0=a-0.5$, $x_1=a$, $x_2=a+0.5$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=0$.\n- Case $2$ (noise-free, asymmetric nodes, moderate curvature):\n  - True parameters: $(a,b,R)=(0.75,-0.5,1.9)$.\n  - Nodes: $x_0=a-0.6$, $x_1=a+0.1$, $x_2=a+0.7$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=0$.\n- Case $3$ (noisy, symmetric nodes, moderate curvature):\n  - True parameters: $(a,b,R)=(0.75,-0.5,1.9)$.\n  - Nodes: $x_0=a-0.5$, $x_1=a$, $x_2=a+0.5$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=10^{-5}$, random seed $12345$.\n- Case $4$ (noise-free, symmetric nodes, nearly straight arc):\n  - True parameters: $(a,b,R)=(10.0,-20.0,1000.0)$.\n  - Nodes: $x_0=a-0.1$, $x_1=a$, $x_2=a+0.1$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=0$.\n- Case $5$ (noisy, symmetric nodes, nearly straight arc):\n  - True parameters: $(a,b,R)=(10.0,-20.0,1000.0)$.\n  - Nodes: $x_0=a-0.1$, $x_1=a$, $x_2=a+0.1$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=10^{-8}$, random seed $24680$.\n\nFinal output format:\n- A single line containing a list of the five error triples, in the order of Cases $1$ through $5$, as a single comma-separated list enclosed in square brackets, with no spaces, and each triple rounded to $12$ decimal places. For example: $[[e_{1a},e_{1b},e_{1R}],[e_{2a},e_{2b},e_{2R}],[e_{3a},e_{3b},e_{3R}],[e_{4a},e_{4b},e_{4R}],[e_{5a},e_{5b},e_{5R}]]$ where each $e$ denotes the corresponding absolute error.", "solution": "The problem statement presented is valid. It is scientifically grounded in the principles of numerical analysis and differential geometry, is well-posed with a clear objective, and provides all necessary information to proceed with a unique solution. The task is to estimate the parameters of a circle from three sample points by employing a quadratic interpolant constructed via Newton's divided differences and then using the geometric properties of its osculating circle at the central point. This is a standard and verifiable problem in computational physics.\n\nWe shall proceed with the derivation and solution, following the specified requirements meticulously.\n\n**Part 1: The Quadratic Interpolant and its Derivatives**\n\nGiven three distinct points $(x_0, y_0)$, $(x_1, y_1)$, and $(x_2, y_2)$ with distinct abscissas, there exists a unique polynomial of degree at most two that passes through them. We construct this polynomial, $p_2(x)$, using the Newton form.\n\nFirst, we define the necessary divided differences:\nThe zeroth-order divided difference is simply the function value:\n$$f[x_i] = y_i$$\nThe first-order divided difference is the slope of the secant line between two points:\n$$f[x_i, x_j] = \\frac{f[x_j] - f[x_i]}{x_j - x_i}$$\nThe second-order divided difference is the difference of first-order differences:\n$$f[x_i, x_j, x_k] = \\frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}$$\n\nThe Newton form of the quadratic interpolating polynomial passing through $(x_0, y_0)$, $(x_1, y_1)$, and $(x_2, y_2)$ is given by:\n$$p_2(x) = f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1)$$\nLet us denote the coefficients, which are the divided differences, as $c_0 = f[x_0]$, $c_1 = f[x_0, x_1]$, and $c_2 = f[x_0, x_1, x_2]$. The polynomial is then:\n$$p_2(x) = c_0 + c_1(x - x_0) + c_2(x - x_0)(x - x_1)$$\n\nTo find the first and second derivatives of the interpolating polynomial, we differentiate $p_2(x)$ with respect to $x$.\nThe first derivative, $p_2'(x)$, is:\n$$p_2'(x) = \\frac{d}{dx} \\left[ c_0 + c_1(x - x_0) + c_2(x^2 - (x_0 + x_1)x + x_0x_1) \\right]$$\n$$p_2'(x) = 0 + c_1 + c_2(2x - (x_0 + x_1))$$\n$$p_2'(x) = c_1 + c_2((x - x_0) + (x - x_1))$$\nWe evaluate this at the central point $x_1$:\n$$p_2'(x_1) = c_1 + c_2((x_1 - x_0) + (x_1 - x_1)) = c_1 + c_2(x_1 - x_0)$$\nSubstituting the definitions of the divided differences $c_1$ and $c_2$:\n$$p_2'(x_1) = f[x_0, x_1] + f[x_0, x_1, x_2](x_1 - x_0)$$\n\nThe second derivative, $p_2''(x)$, is found by differentiating $p_2'(x)$:\n$$p_2''(x) = \\frac{d}{dx} \\left[ c_1 + c_2(2x - x_0 - x_1) \\right] = 2c_2$$\nThe second derivative of a quadratic polynomial is constant. Therefore, at $x_1$:\n$$p_2''(x_1) = 2c_2 = 2f[x_0, x_1, x_2]$$\nThese expressions for $p_2'(x_1)$ and $p_2''(x_1)$ are derived directly from the definition of the Newton polynomial and are what we will use for our computation.\n\n**Part 2: Osculating Circle Parameters**\n\nThe osculating circle at a point on a curve is the circle that best approximates the curve at that point. Its radius is the reciprocal of the curvature, and its center lies along the normal line.\n\nFor a curve defined by $y = y(x)$, the curvature $\\kappa$ is given by the formula:\n$$\\kappa(x) = \\frac{|y''(x)|}{(1 + [y'(x)]^2)^{3/2}}$$\nWe approximate the curvature of the underlying curve at $(x_1, y_1)$ by the curvature of the interpolating polynomial $p_2(x)$ at $x=x_1$. Let this be $\\hat{\\kappa}$.\n$$\\hat{\\kappa} = \\frac{|p_2''(x_1)|}{(1 + [p_2'(x_1)]^2)^{3/2}}$$\nThe estimated radius of the osculating circle, $\\hat{R}$, is the reciprocal of this curvature:\n$$\\hat{R} = \\frac{1}{\\hat{\\kappa}} = \\frac{(1 + [p_2'(x_1)]^2)^{3/2}}{|p_2''(x_1)|}$$\n\nThe center of the osculating circle, $(\\hat{a}, \\hat{b})$, is located at a distance $\\hat{R}$ from the point $(x_1, y_1)$ along the unit normal vector. The tangent vector to the curve $y=p_2(x)$ at $x_1$ is proportional to $\\langle 1, p_2'(x_1) \\rangle$. A normal vector is therefore proportional to $\\langle -p_2'(x_1), 1 \\rangle$. The unit normal vector $\\vec{n}$ is:\n$$\\vec{n} = \\frac{\\langle -p_2'(x_1), 1 \\rangle}{\\sqrt{(-p_2'(x_1))^2 + 1^2}} = \\frac{\\langle -p_2'(x_1), 1 \\rangle}{\\sqrt{1 + [p_2'(x_1)]^2}}$$\nThis vector points \"upwards\" (positive $y$-component). The center of curvature lies in this direction if the curve is concave up ($p_2''(x_1) > 0$), and in the opposite direction if the curve is concave down ($p_2''(x_1) < 0$). We can thus express the displacement vector from $(x_1, y_1)$ to $(\\hat{a}, \\hat{b})$ as $\\hat{R} \\cdot \\text{sign}(p_2''(x_1)) \\cdot \\vec{n}$.\n\nThe position vector of the center is:\n$$\\langle \\hat{a}, \\hat{b} \\rangle = \\langle x_1, y_1 \\rangle + \\hat{R} \\cdot \\text{sign}(p_2''(x_1)) \\cdot \\vec{n}$$\nSubstituting the expressions for $\\hat{R}$ and $\\vec{n}$:\n$$\\langle \\hat{a}, \\hat{b} \\rangle = \\langle x_1, y_1 \\rangle + \\frac{(1 + [p_2'(x_1)]^2)^{3/2}}{|p_2''(x_1)|} \\cdot \\text{sign}(p_2''(x_1)) \\cdot \\frac{\\langle -p_2'(x_1), 1 \\rangle}{\\sqrt{1 + [p_2'(x_1)]^2}}$$\nUsing the identity $|z| \\cdot \\text{sign}(z) = z$, let $z = p_2''(x_1)$, we simplify the scalar part:\n$$\\frac{(1 + [p_2'(x_1)]^2)^{3/2}}{p_2''(x_1)} \\cdot \\frac{1}{\\sqrt{1 + [p_2'(x_1)]^2}} = \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)}$$\nThe vector equation for the center becomes:\n$$\\langle \\hat{a}, \\hat{b} \\rangle = \\langle x_1, y_1 \\rangle + \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)} \\langle -p_2'(x_1), 1 \\rangle$$\nFrom this, we extract the coordinates of the estimated center:\n$$\\hat{a} = x_1 - p_2'(x_1) \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)}$$\n$$\\hat{b} = y_1 + \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)}$$\nNote that the $y$-coordinate used in the formula for $\\hat{b}$ is the observed (and potentially noisy) value $\\tilde{y}_1$, as this is the point on the interpolating polynomial through which the osculating circle is constructed.\n\nThese derived formulas are implemented to solve for the estimated parameters and their errors for each test case. The sensitivity to noise is particularly apparent in cases with small curvature (large radius), where $p_2''(x_1)$ is close to zero, making the expressions for $\\hat{a}$, $\\hat{b}$, and $\\hat{R}$ numerically unstable.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the estimated circle parameters using a quadratic interpolant\n    and calculates the absolute errors for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: noise-free, symmetric, moderate curvature\n        {'true_params': (0.75, -0.5, 1.9), 'a_offset': -0.5, 'c_offset': 0.5,\n         'noise_sigma': 0.0, 'seed': None},\n        # Case 2: noise-free, asymmetric, moderate curvature\n        {'true_params': (0.75, -0.5, 1.9), 'a_offset': -0.6, 'c_offset': 0.7,\n         'x1_offset': 0.1, 'noise_sigma': 0.0, 'seed': None},\n        # Case 3: noisy, symmetric, moderate curvature\n        {'true_params': (0.75, -0.5, 1.9), 'a_offset': -0.5, 'c_offset': 0.5,\n         'noise_sigma': 1e-5, 'seed': 12345},\n        # Case 4: noise-free, symmetric, large radius\n        {'true_params': (10.0, -20.0, 1000.0), 'a_offset': -0.1, 'c_offset': 0.1,\n         'noise_sigma': 0.0, 'seed': None},\n        # Case 5: noisy, symmetric, large radius\n        {'true_params': (10.0, -20.0, 1000.0), 'a_offset': -0.1, 'c_offset': 0.1,\n         'noise_sigma': 1e-8, 'seed': 24680},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a_true, b_true, R_true = case['true_params']\n        sigma = case['noise_sigma']\n        seed = case['seed']\n        \n        # Define nodes\n        x0 = a_true + case['a_offset']\n        x1 = a_true + case.get('x1_offset', 0.0) # a_true if symmetric\n        x2 = a_true + case['c_offset']\n        \n        xs = np.array([x0, x1, x2])\n        \n        # Generate true y-values on the upper branch (s=+1)\n        ys_true = b_true + np.sqrt(R_true**2 - (xs - a_true)**2)\n        \n        # Add noise if specified\n        ys_observed = ys_true.copy()\n        if sigma > 0:\n            rng = np.random.default_rng(seed)\n            noise = rng.normal(0, sigma, 3)\n            ys_observed += noise\n        \n        y0_obs, y1_obs, y2_obs = ys_observed\n        \n        # Calculate divided differences\n        # c0 = f[x0]\n        # c1 = f[x0, x1]\n        # c2 = f[x0, x1, x2]\n        c0 = y0_obs\n        \n        # First order differences\n        f_x0_x1 = (y1_obs - y0_obs) / (x1 - x0)\n        f_x1_x2 = (y2_obs - y1_obs) / (x2 - x1)\n        \n        c1 = f_x0_x1\n        \n        # Second order difference\n        c2 = (f_x1_x2 - f_x0_x1) / (x2 - x0)\n        \n        # Calculate derivatives of the interpolant p2(x) at x1\n        p2_prime_x1 = c1 + c2 * (x1 - x0)\n        p2_double_prime_x1 = 2 * c2\n        \n        # Check for near-zero second derivative (collinear points)\n        if abs(p2_double_prime_x1) < 1e-15:\n            # For collinear points, radius is infinite, center is at infinity.\n            # This problem's setup avoids this, but it's good practice.\n            # Report large error.\n            errors = [np.inf, np.inf, np.inf]\n            results.append(errors)\n            continue\n            \n        # Estimate parameters of the osculating circle\n        y_prime_sq = p2_prime_x1**2\n        common_term = (1 + y_prime_sq) / p2_double_prime_x1\n        \n        a_hat = x1 - p2_prime_x1 * common_term\n        b_hat = y1_obs + common_term\n        R_hat = np.abs((1 + y_prime_sq)**(3/2) / p2_double_prime_x1)\n\n        # Calculate absolute errors\n        error_a = abs(a_hat - a_true)\n        error_b = abs(b_hat - b_true)\n        error_R = abs(R_hat - R_true)\n        \n        results.append([error_a, error_b, error_R])\n\n    # Format the final output string as per requirements\n    formatted_results = []\n    for res_triple in results:\n        formatted_triple = f\"[{res_triple[0]:.12f},{res_triple[1]:.12f},{res_triple[2]:.12f}]\"\n        formatted_results.append(formatted_triple)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2428257"}, {"introduction": "Beyond constructing approximations, the theory of divided differences provides a powerful tool for data analysis. A key theorem states that the $(n+1)$-th order divided difference of data sampled from a polynomial of degree $n$ is identically zero. This exercise challenges you to implement this idea as a diagnostic test to determine if a noisy dataset could have plausibly originated from a cubic polynomial, showcasing how divided differences can be used to probe the underlying nature of your data. [@problem_id:2428325]", "problem": "You are given a mathematical diagnostic for assessing whether a set of scalar data points $\\{(x_i,y_i)\\}_{i=0}^{n-1}$ could plausibly have been generated by a cubic polynomial in the presence of small measurement noise. Let the $k$-th order Newton divided difference of the sequence of $k+1$ points $(x_j,y_j), (x_{j+1},y_{j+1}), \\dots, (x_{j+k},y_{j+k})$ be defined recursively by\n$$[x_j; y] = y_j,$$\n$$[x_j,x_{j+1},\\dots,x_{j+k}; y] = \\frac{[x_{j+1},\\dots,x_{j+k}; y] - [x_j,\\dots,x_{j+k-1}; y]}{x_{j+k} - x_j},$$\nfor all integers $j \\ge 0$ and $k \\ge 1$, where all $x_i$ are distinct. For data generated exactly by a polynomial $p(x)$ of degree at most $3$, every fourth-order divided difference vanishes identically.\n\nDefine the diagnostic\n$$D_4(x,y) = \\begin{cases}\n\\max\\limits_{0 \\le i \\le n-5} \\left| [x_i,x_{i+1},x_{i+2},x_{i+3},x_{i+4}; y] \\right|, & \\text{if } n \\ge 5,\\\\\n0, & \\text{if } n < 5,\n\\end{cases}$$\nthat is, the maximum absolute value of the fourth-order divided differences computed over all consecutive groups of five points. Given a nonnegative tolerance $\\varepsilon$, the data are to be classified as \"plausibly cubic\" if and only if $D_4(x,y) \\le \\varepsilon$.\n\nWrite a complete program that, for each of the following test cases with strictly increasing abscissae $x_i$ and corresponding ordinates $y_i$, computes $D_4(x,y)$ according to the definition above and outputs a boolean classification for each case using the criterion $D_4(x,y) \\le \\varepsilon$. Use the same tolerance $\\varepsilon$ for all test cases, namely $\\varepsilon = 10^{-4}$.\n\nTest Suite (each case is independent and self-contained, and all numbers are real-valued):\n1. Case A: $x = [\\,0,\\,1,\\,2,\\,3,\\,4,\\,5\\,]$, $y = [\\,1,\\,0,\\,5,\\,22,\\,57,\\,116\\,]$, $\\varepsilon = 10^{-4}$.\n2. Case B: $x = [\\,0,\\,1,\\,2,\\,3,\\,4,\\,5\\,]$, $y = [\\,1 + 10^{-6},\\,0 - 10^{-6},\\,5 + 5 \\cdot 10^{-7},\\,22 - 7 \\cdot 10^{-7},\\,57 + 2 \\cdot 10^{-6},\\,116 - 1.5 \\cdot 10^{-6}\\,]$, $\\varepsilon = 10^{-4}$.\n3. Case C: $x = [\\,0,\\,1,\\,2,\\,3,\\,4,\\,5\\,]$, $y = [\\,1,\\,0,\\,13,\\,76,\\,249,\\,616\\,]$, $\\varepsilon = 10^{-4}$.\n4. Case D: $x = [\\,-2,\\,-1,\\,0,\\,1,\\,2\\,]$, $y = [\\,-3,\\,2,\\,1,\\,0,\\,5\\,]$, $\\varepsilon = 10^{-4}$.\n5. Case E: $x = [\\,0,\\,1,\\,2,\\,3\\,]$, $y = [\\,1,\\,0,\\,13,\\,76\\,]$, $\\varepsilon = 10^{-4}$.\n6. Case F: $x = [\\,0,\\,0.5,\\,1.7,\\,2.0,\\,2.5,\\,4.0\\,]$, $y = [\\,1 + 2 \\cdot 10^{-7},\\,0.125 - 3 \\cdot 10^{-7},\\,2.513 + 10^{-6},\\,5 - 4 \\cdot 10^{-7},\\,11.625 + 5 \\cdot 10^{-7},\\,57 - 2 \\cdot 10^{-7}\\,]$, $\\varepsilon = 10^{-4}$.\n\nInterpretation rules to apply uniformly:\n- If $n < 5$, then by definition set $D_4(x,y) = 0$ and classify as \"plausibly cubic\".\n- If $n \\ge 5$, evaluate all consecutive five-point fourth-order divided differences and set $D_4(x,y)$ to the maximum absolute value among them; classify as \"plausibly cubic\" if and only if $D_4(x,y) \\le \\varepsilon$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the boolean classifications for Cases A through F, in order, as a comma-separated list enclosed in square brackets, for example $[\\,\\text{True},\\text{False},\\dots\\,]$, but with no whitespace characters. The output line must be exactly of the form \"[resultA,resultB,resultC,resultD,resultE,resultF]\" where each entry is either \"True\" or \"False\".", "solution": "The problem requires the implementation of a diagnostic, denoted as $D_4(x,y)$, to test whether a given set of data points $\\{(x_i, y_i)\\}_{i=0}^{n-1}$ could plausibly originate from a cubic polynomial curve in the presence of small measurement noise. The validation of the problem statement finds it to be scientifically sound, well-posed, and an objective task within the field of computational science. We may therefore proceed with a solution.\n\nThe theoretical foundation of this problem lies in the properties of Newton's divided differences. For a set of $k+1$ distinct points $(x_0, y_0), \\dots, (x_k, y_k)$, the $k$-th order divided difference is a symmetric function of its arguments. A fundamental theorem of numerical analysis states that if the data points $(x_i, y_i)$ are sampled from a polynomial $p(x)$ of degree at most $m$, then any divided difference of order $m+1$ or higher is identically zero. That is, $[x_i, x_{i+1}, \\dots, x_{i+m+1}; y] = 0$ for all valid $i$.\n\nThe problem specifies that we are testing for a \"plausibly cubic\" relationship, which corresponds to a polynomial of degree at most $3$. Therefore, we must examine the fourth-order divided differences. If the data were generated *exactly* by a cubic polynomial, all fourth-order divided differences would be exactly $0$. However, real-world data often contains noise. If $y_i = p(x_i) + \\delta_i$, where $p(x)$ is a cubic polynomial and $\\delta_i$ represents a small noise term, then the fourth-order divided differences will no longer be zero. Instead, they will be small values proportional to the magnitude of the noise.\n\nThe diagnostic $D_4(x,y)$ is defined as the maximum absolute value of all fourth-order divided differences computed over consecutive blocks of five points. The recursive definition of the divided difference is given as:\n$$[x_j; y] = y_j$$\n$$[x_j, \\dots, x_{j+k}; y] = \\frac{[x_{j+1}, \\dots, x_{j+k}; y] - [x_j, \\dots, x_{j+k-1}; y]}{x_{j+k} - x_j}$$\nFor the diagnostic $D_4(x,y)$, we are interested in the case where the order $k=4$. The problem statement defines:\n$$D_4(x,y) = \\begin{cases}\n\\max\\limits_{0 \\le i \\le n-5} \\left| [x_i,x_{i+1},x_{i+2},x_{i+3},x_{i+4}; y] \\right|, & \\text{if } n \\ge 5,\\\\\n0, & \\text{if } n < 5,\n\\end{cases}$$\nThe data are classified as \"plausibly cubic\" if $D_4(x,y) \\le \\varepsilon$ for a given tolerance $\\varepsilon = 10^{-4}$.\n\nTo compute $D_4(x,y)$, we must first calculate all relevant fourth-order divided differences. For a dataset of $n$ points, there are $n-4$ such differences to be computed, indexed from $i=0$ to $i=n-5$. A direct and systematic algorithm to achieve this is to iteratively compute the tables of divided differences for orders $k=1, 2, 3, 4$.\n\nLet $f^{(k)}_i$ denote the $k$-th order divided difference starting at index $i$, which is $f^{(k)}_i = [x_i, \\dots, x_{i+k}; y]$.\nThe initial values are the zeroth-order differences, given by the data itself: $f^{(0)}_i = y_i$ for $i=0, \\dots, n-1$.\nThe higher-order differences are computed iteratively using the recursive definition. For each order $k \\in \\{1, 2, 3, 4\\}$, we compute a vector of coefficients $f^{(k)}$ from the vector of the previous order $f^{(k-1)}$:\n$$ f^{(k)}_i = \\frac{f^{(k-1)}_{i+1} - f^{(k-1)}_i}{x_{i+k} - x_i} $$\nThis calculation is performed for all valid indices $i$. For order $k$, the index $i$ ranges from $0$ to $n-1-k$.\n\nThe algorithm proceeds as follows:\n1. Initialize a vector of coefficients with the given $y_i$ values. This is the vector of zeroth-order differences, $\\{f^{(0)}_i\\}$.\n2. For $k=1$ to $4$:\n   Compute the vector of $k$-th order differences, $\\{f^{(k)}_i\\}$, from the vector $\\{f^{(k-1)}_i\\}$ using the formula above. The length of the vector shrinks by one at each step.\n3. After the loop completes for $k=4$, the resulting vector contains all fourth-order divided differences, $\\{f^{(4)}_i\\}_{i=0}^{n-5}$.\n4. If this vector is empty (which happens if $n < 5$), $D_4(x,y)$ is $0$. Otherwise, $D_4(x,y)$ is the maximum of the absolute values of the elements in this vector.\n5. Finally, the boolean classification is determined by comparing $D_4(x,y)$ with the tolerance $\\varepsilon = 10^{-4}$.\n\nThis procedure will be applied to each test case. For cases with $n < 5$ points, the diagnostic $D_4(x,y)$ is defined to be $0$, leading to a classification of \"plausibly cubic\". For cases with $n \\ge 5$, the full calculation is performed. We will use floating-point arithmetic with sufficient precision (e.g., 64-bit floats) to handle the small numerical values involved, especially in cases with added noise.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the D4 diagnostic for each test case and\n    classifying the data as \"plausibly cubic\" based on a given tolerance.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        (np.array([0, 1, 2, 3, 4, 5], dtype=np.float64),\n         np.array([1, 0, 5, 22, 57, 116], dtype=np.float64)),\n        # Case B\n        (np.array([0, 1, 2, 3, 4, 5], dtype=np.float64),\n         np.array([1 + 1e-6, 0 - 1e-6, 5 + 5e-7, 22 - 7e-7, 57 + 2e-6, 116 - 1.5e-6], dtype=np.float64)),\n        # Case C\n        (np.array([0, 1, 2, 3, 4, 5], dtype=np.float64),\n         np.array([1, 0, 13, 76, 249, 616], dtype=np.float64)),\n        # Case D\n        (np.array([-2, -1, 0, 1, 2], dtype=np.float64),\n         np.array([-3, 2, 1, 0, 5], dtype=np.float64)),\n        # Case E\n        (np.array([0, 1, 2, 3], dtype=np.float64),\n         np.array([1, 0, 13, 76], dtype=np.float64)),\n        # Case F\n        (np.array([0, 0.5, 1.7, 2.0, 2.5, 4.0], dtype=np.float64),\n         np.array([1 + 2e-7, 0.125 - 3e-7, 2.513 + 1e-6, 5 - 4e-7, 11.625 + 5e-7, 57 - 2e-7], dtype=np.float64))\n    ]\n\n    epsilon = 1e-4\n    results = []\n\n    def compute_d4(x, y):\n        \"\"\"\n        Computes the D4 diagnostic for a given set of data points (x, y).\n        \n        Args:\n            x (np.ndarray): The abscissae of the data points.\n            y (np.ndarray): The ordinates of the data points.\n        \n        Returns:\n            float: The value of the D4 diagnostic.\n        \"\"\"\n        n = len(x)\n        if n < 5:\n            return 0.0\n\n        # Start with 0-th order divided differences (y values)\n        dd_coeffs = np.copy(y)\n\n        # Iteratively compute higher-order divided differences up to order 4\n        for k in range(1, 5):  # k is the order of the difference\n            # The number of coefficients of order k-1 is n - (k-1)\n            # The temporary array will hold coefficients of order k.\n            # Its size is (n - (k-1)) - 1 = n - k.\n            temp_coeffs = np.zeros(n - k)\n            for i in range(n - k):\n                numerator = dd_coeffs[i + 1] - dd_coeffs[i]\n                denominator = x[i + k] - x[i]\n                if denominator == 0:\n                    # This should not happen given the problem statement\n                    # that x_i are distinct (strictly increasing).\n                    # Handle defensively.\n                    return np.inf\n                temp_coeffs[i] = numerator / denominator\n            dd_coeffs = temp_coeffs\n        \n        # After the loop, dd_coeffs contains all 4th-order divided differences.\n        # The number of such differences is n-4.\n        if len(dd_coeffs) == 0:\n            return 0.0\n        \n        return np.max(np.abs(dd_coeffs))\n\n    for x_vals, y_vals in test_cases:\n        d4_value = compute_d4(x_vals, y_vals)\n        is_plausibly_cubic = d4_value <= epsilon\n        results.append(is_plausibly_cubic)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2428325"}]}