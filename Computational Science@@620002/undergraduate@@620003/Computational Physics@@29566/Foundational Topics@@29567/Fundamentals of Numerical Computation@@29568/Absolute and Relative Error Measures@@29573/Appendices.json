{"hands_on_practices": [{"introduction": "Our journey into the practical application of error analysis begins with a fundamental concept: error propagation. When a physical quantity is calculated using a formula that relies on measured values, the uncertainties inherent in those measurements propagate to the final result. This exercise [@problem_id:2370476] uses the well-known Stefan-Boltzmann law of thermal radiation to provide a concrete example of how a small relative error in a temperature measurement can be significantly amplified when calculating the radiated power, illustrating a critical consideration for both experimental and computational scientists.", "problem": "A thermal radiation experiment measures the total radiated power from an ideal black surface using the Stefan–Boltzmann law, which states that the radiated power is given by $P=\\sigma A T^{4}$, where $P$ is the power, $\\sigma$ is the Stefan–Boltzmann constant, $A$ is the emitting area, and $T$ is the absolute temperature. Suppose the emitting area $A$ and the Stefan–Boltzmann constant $\\sigma$ are known exactly and the only source of measurement uncertainty is the temperature $T$. If the measured temperature $T$ has a relative error of $1/100$, determine the resulting relative error in the computed radiated power $P$. Express your final answer as a decimal number with no units. Do not use a percent sign. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The governing equation is the Stefan–Boltzmann law: $P = \\sigma A T^{4}$.\n- The variable $P$ is the radiated power.\n- The constant $\\sigma$ is the Stefan–Boltzmann constant, which is known exactly.\n- The variable $A$ is the emitting area, which is known exactly.\n- The variable $T$ is the absolute temperature, and it is the only source of measurement uncertainty.\n- The relative error in the measured temperature $T$ is given as $\\frac{1}{100}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it is based on the Stefan–Boltzmann law, a fundamental principle of physics. It is a well-posed problem in error propagation analysis. The problem is formulated with objective and precise language. It contains all necessary information to derive a unique solution, specifying that uncertainties in $\\sigma$ and $A$ are zero. The setup is self-contained and free of contradictions or vagueness.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be provided.\n\nThe relationship between the radiated power $P$ and the absolute temperature $T$ is given by the function:\n$$P(T) = \\sigma A T^{4}$$\nHere, the Stefan–Boltzmann constant $\\sigma$ and the area $A$ are considered exact, meaning their associated uncertainties are zero. The uncertainty in $P$ arises solely from the uncertainty in the measurement of $T$.\n\nTo determine the propagation of relative error, we employ the method of logarithmic differentiation, which is most efficient for power-law relationships. We take the natural logarithm of both sides of the equation:\n$$\\ln(P) = \\ln(\\sigma A T^{4})$$\nUsing the properties of logarithms, we can expand the right side:\n$$\\ln(P) = \\ln(\\sigma) + \\ln(A) + \\ln(T^{4})$$\n$$\\ln(P) = \\ln(\\sigma) + \\ln(A) + 4\\ln(T)$$\nNext, we find the total differential of this expression to relate infinitesimal changes in the variables:\n$$d(\\ln(P)) = d(\\ln(\\sigma)) + d(\\ln(A)) + d(4\\ln(T))$$\nThis gives:\n$$\\frac{dP}{P} = \\frac{d\\sigma}{\\sigma} + \\frac{dA}{A} + 4\\frac{dT}{T}$$\nFor small, finite uncertainties, we can approximate these differentials ($\\mathit{d}$) with finite differences ($\\delta$):\n$$\\frac{\\delta P}{P} \\approx \\frac{\\delta \\sigma}{\\sigma} + \\frac{\\delta A}{A} + 4\\frac{\\delta T}{T}$$\nThis equation expresses the relative error in $P$ in terms of the relative errors of the quantities it depends on.\n\nAccording to the problem statement, $\\sigma$ and $A$ are known exactly. This implies that their uncertainties are zero: $\\delta \\sigma = 0$ and $\\delta A = 0$. Consequently, their relative errors are also zero:\n$$\\frac{\\delta \\sigma}{\\sigma} = 0$$\n$$\\frac{\\delta A}{A} = 0$$\nSubstituting these into the error propagation formula simplifies it to:\n$$\\frac{\\delta P}{P} \\approx 4\\frac{\\delta T}{T}$$\nThis shows that the relative error in the computed power $P$ is four times the relative error in the measured temperature $T$. We are interested in the magnitude of this error. Let $\\epsilon_X = \\left| \\frac{\\delta X}{X} \\right|$ denote the magnitude of the relative error for a quantity $X$. Then:\n$$\\epsilon_P = \\left| \\frac{\\delta P}{P} \\right| \\approx 4 \\left| \\frac{\\delta T}{T} \\right| = 4 \\epsilon_T$$\nThe problem states that the relative error in temperature is $\\frac{1}{100}$. So, $\\epsilon_T = \\frac{1}{100}$.\nSubstituting this value, we find the relative error in power:\n$$\\epsilon_P \\approx 4 \\times \\frac{1}{100} = \\frac{4}{100}$$\nAs requested, we express this result as a decimal number:\n$$\\epsilon_P = 0.04$$\nThis is the resulting relative error in the computed radiated power $P$.", "answer": "$$\n\\boxed{0.04}\n$$", "id": "2370476"}, {"introduction": "Moving from measurement uncertainty to the inherent errors within digital computation, we confront a classic pitfall of floating-point arithmetic known as \"catastrophic cancellation.\" This hands-on coding exercise [@problem_id:2370414] directly demonstrates how a mathematically sound formula can produce highly inaccurate results when implemented naively due to the subtraction of two nearly equal numbers. By comparing a naive implementation with a numerically stable, algebraically equivalent version, you will observe and quantify the dramatic loss of relative precision, a vital lesson in writing robust and reliable scientific software.", "problem": "Consider the real-valued function $f(a) = \\sqrt{a^2 + 1} - a$ for $a \\in \\mathbb{R}_{0}$. For large $a$, the subtraction in $f(a)$ involves two nearly equal positive numbers, which can lead to catastrophic cancellation in standard floating-point arithmetic. Define the absolute error of an approximation $\\tilde{y}$ to a reference value $y$ as $| \\tilde{y} - y |$, and define the relative error as $| \\tilde{y} - y | / | y |$.\n\nYour task is to write a complete, runnable program that, for each specified test value of $a$, evaluates:\n1. A naive direct evaluation $f_{\\text{naive}}(a) = \\sqrt{a^2 + 1} - a$ using standard double-precision floating-point arithmetic.\n2. A mathematically equivalent reference value $f_{\\text{ref}}(a) = \\dfrac{1}{\\sqrt{a^2 + 1} + a}$ using the same arithmetic.\n\nThen compute, for each $a$, the absolute error $E_{\\text{abs}}(a) = \\left| f_{\\text{naive}}(a) - f_{\\text{ref}}(a) \\right|$ and the relative error $E_{\\text{rel}}(a) = \\left| f_{\\text{naive}}(a) - f_{\\text{ref}}(a) \\right| \\big/ \\left| f_{\\text{ref}}(a) \\right|$. Express all numerical results as dimensionless numbers.\n\nUse the following test suite for $a$:\n- $a = 1$\n- $a = 10^{4}$\n- $a = 10^{8}$\n- $a = 10^{16}$\n- $a = 10^{-8}$\n\nFinal output format: Your program should produce a single line of output containing a comma-separated list enclosed in square brackets, in the same order as the test suite. Each element must itself be a two-element list $[E_{\\text{abs}}(a), E_{\\text{rel}}(a)]$. For example, an output with three test cases would look like `[[x_1,y_1],[x_2,y_2],[x_3,y_3]]` where each $x_i$ and $y_i$ are decimal numbers.", "solution": "The problem requires an analysis of numerical error when evaluating the function $f(a) = \\sqrt{a^2 + 1} - a$ for positive real numbers $a \\in \\mathbb{R}_{0}$. Specifically, we must compare a naive computational approach with a mathematically equivalent but numerically superior one, and quantify the discrepancy using absolute and relative error measures.\n\nThe problem statement has been validated and is scientifically sound, well-posed, and objective. It presents a classic example of catastrophic cancellation in floating-point arithmetic, a fundamental topic in computational science. All required definitions, data, and constraints are provided.\n\nThe core of the problem lies in the behavior of the function for large values of $a$. As $a$ becomes large, the term $\\sqrt{a^2 + 1}$ becomes very close to $\\sqrt{a^2} = a$. For instance, for $a=10^8$, we have $a^2=10^{16}$. Using a binomial expansion, $\\sqrt{a^2+1} = a\\sqrt{1+1/a^2} \\approx a(1 + \\frac{1}{2a^2}) = a + \\frac{1}{2a}$. For $a=10^8$, this is $10^8 + 0.5 \\times 10^{-8}$. The subtraction $f(a) = (a + \\frac{1}{2a}) - a$ involves two nearly identical numbers. Standard double-precision floating-point arithmetic has finite precision (approximately $15$–$17$ decimal digits). When two nearly equal numbers are subtracted, the leading, most significant digits cancel out, leaving fewer significant digits in the result. This loss of precision is known as catastrophic cancellation. For a sufficiently large $a$, the value of $a^2+1$ may not even be distinguishable from $a^2$ in finite precision, causing $\\sqrt{a^2+1}$ to be computed as exactly $a$, leading to a result of $0$.\n\nTo mitigate this, we can reformulate the expression algebraically. By multiplying and dividing by the conjugate expression, $\\sqrt{a^2+1} + a$, we obtain a numerically stable form:\n$$\nf(a) = \\sqrt{a^2 + 1} - a = (\\sqrt{a^2 + 1} - a) \\times \\frac{\\sqrt{a^2 + 1} + a}{\\sqrt{a^2 + 1} + a}\n$$\n$$\nf(a) = \\frac{(\\sqrt{a^2+1})^2 - a^2}{\\sqrt{a^2 + 1} + a} = \\frac{(a^2+1) - a^2}{\\sqrt{a^2 + 1} + a} = \\frac{1}{\\sqrt{a^2 + 1} + a}\n$$\nThis revised expression, which we denote $f_{\\text{ref}}(a)$, involves only additions and a final division. These operations are numerically stable. The subtraction of nearly equal numbers has been eliminated. Therefore, $f_{\\text{ref}}(a)$ computed using floating-point arithmetic will yield a result much closer to the true mathematical value than the naive evaluation, $f_{\\text{naive}}(a) = \\sqrt{a^2+1} - a$.\n\nThe problem requires us to compute the absolute and relative errors of the naive formulation with respect to the stable one. We treat the result from $f_{\\text{ref}}(a)$ as the reference value, $y$, and the result from $f_{\\text{naive}}(a)$ as the approximation, $\\tilde{y}$.\n\nThe absolute error is defined as:\n$$\nE_{\\text{abs}}(a) = | f_{\\text{naive}}(a) - f_{\\text{ref}}(a) |\n$$\nThe relative error is defined as:\n$$\nE_{\\text{rel}}(a) = \\frac{| f_{\\text{naive}}(a) - f_{\\text{ref}}(a) |}{| f_{\\text{ref}}(a) |}\n$$\nThe computational procedure will be as follows for each test value of $a$ ($1$, $10^4$, $10^8$, $10^{16}$, and $10^{-8}$):\n$1$. Define the value of $a$ using double-precision floating-point representation.\n$2$. Calculate $f_{\\text{naive}}(a) = \\sqrt{a^2 + 1} - a$.\n$3$. Calculate $f_{\\text{ref}}(a) = \\frac{1}{\\sqrt{a^2 + 1} + a}$.\n$4$. Compute $E_{\\text{abs}}(a)$ and $E_{\\text{rel}}(a)$ using the results from steps $2$ and $3$.\n\nFor small values of $a$ (e.g., $a=1, a=10^{-8}$), the terms $\\sqrt{a^2+1}$ and $a$ are not nearly equal, so catastrophic cancellation is not a significant factor. The errors are expected to be on the order of machine precision. For large values of $a$ (e.g., $a=10^4, a=10^8, a=10^{16}$), we expect the errors, particularly the relative error, to grow substantially as the effect of catastrophic cancellation becomes more pronounced. For $a=10^8$ and larger, $a^2+1$ will be rounded to $a^2$ in standard double precision, causing $f_{\\text{naive}}(a)$ to evaluate to $0$, resulting in a relative error of $1$, or $100\\%$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the absolute and relative errors for the naive evaluation\n    of f(a) = sqrt(a^2 + 1) - a against a more numerically stable form.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1.0,\n        1.0e4,\n        1.0e8,\n        1.0e16,\n        1.0e-8,\n    ]\n\n    results = []\n    for a_val in test_cases:\n        # Ensure the value is treated as a double-precision float (numpy's default).\n        a = np.float64(a_val)\n        \n        # 1. Naive direct evaluation, prone to catastrophic cancellation for large a.\n        # f_naive(a) = sqrt(a^2 + 1) - a\n        # a**2 can cause overflow for very large a, but a=1e16 is fine.\n        # The key issue is sqrt(a^2+1) becoming indistinguishable from 'a'.\n        f_naive = np.sqrt(a**2 + 1) - a\n        \n        # 2. Mathematically equivalent reference value, numerically stable.\n        # f_ref(a) = 1 / (sqrt(a^2 + 1) + a)\n        # This form avoids subtracting nearly equal numbers.\n        f_ref = 1 / (np.sqrt(a**2 + 1) + a)\n        \n        # In the edge case where the reference value is exactly zero, which does not\n        # happen for a > 0, we would define the relative error as zero\n        # to avoid division by zero.\n        if f_ref == 0.0:\n            abs_error = np.abs(f_naive - f_ref)\n            rel_error = 0.0 if f_naive == 0.0 else np.inf\n        else:\n            # 3. Compute absolute error.\n            abs_error = np.abs(f_naive - f_ref)\n            \n            # 4. Compute relative error.\n            rel_error = abs_error / np.abs(f_ref)\n            \n        results.append([abs_error, rel_error])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) will convert each inner list [x, y] to its string\n    # representation, e.g., '[1.23, 4.56]'.\n    # ','.join(...) will create the comma-separated list of these strings.\n    # The outer f-string adds the final brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2370414"}, {"introduction": "We conclude by applying our understanding of error to the dynamic world of physical simulation, exploring how approximation errors accumulate over time. This practice [@problem_id:2370478] challenges you to integrate the equations of motion for a simple harmonic oscillator using two distinct numerical schemes: the elementary explicit Euler method and the more sophisticated fourth-order Runge-Kutta (RK4) method. By tracking the growth of relative error against the known analytical solution, you will gain crucial insight into how the choice of a numerical algorithm profoundly impacts the long-term stability and fidelity of a simulation.", "problem": "You will write a complete, runnable program that compares how two explicit time-stepping schemes propagate error when integrating the motion of a one-dimensional simple harmonic oscillator. The system of interest is governed by the ordinary differential equation (ODE) $$\\frac{d^{2}x}{dt^{2}} + \\omega^{2} x = 0,$$ where $x$ is the displacement as a function of time $t$ and $\\omega$ is the angular frequency. Introduce the first-order system by defining the state vector $y(t) = \\begin{bmatrix} x(t) \\\\ v(t) \\end{bmatrix}$ with $v(t) = \\frac{dx}{dt}$. The ODE becomes $$\\frac{d}{dt}\\begin{bmatrix} x \\\\ v \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\omega^{2} x \\end{bmatrix}.$$ Initial conditions are given as $x(0)=x_{0}$ and $v(0)=v_{0}$. Time is measured in seconds, displacement in meters, and velocity in meters per second. The angular frequency $\\omega$ is in radians per second.\n\nFundamental base and requirements:\n- Use the defining ODE above, conservation properties of the simple harmonic oscillator, and standard algorithmic definitions of explicit Euler and classical fourth-order Runge–Kutta (RK4) time-stepping methods. Do not assume any unstated stability properties; quantify error directly from definitions.\n- The exact solution for the simple harmonic oscillator follows from solving the linear constant-coefficient ODE with the given initial conditions. You must derive and implement it to compute reference values at each time.\n- Define the scaled state $s(t) = \\begin{bmatrix} x(t) \\\\ v(t)/\\omega \\end{bmatrix}$. This scaling renders the exact energy norm $\\|s(t)\\|_{2}$ constant for the simple harmonic oscillator when $\\omega$ is constant.\n- At each discrete time $t_{n} = n h$ with step size $h$, define the absolute error of a numerical method as $$E_{\\mathrm{abs}}(t_{n}) = \\left\\| s_{\\mathrm{num}}(t_{n}) - s_{\\mathrm{exact}}(t_{n}) \\right\\|_{2},$$ and the relative error as $$E_{\\mathrm{rel}}(t_{n}) = \\frac{E_{\\mathrm{abs}}(t_{n})}{\\left\\| s_{\\mathrm{exact}}(t_{n}) \\right\\|_{2}}.$$ Here $\\|\\cdot\\|_{2}$ is the Euclidean norm. For the simple harmonic oscillator with constant $\\omega$, $\\left\\| s_{\\mathrm{exact}}(t_{n}) \\right\\|_{2}$ equals the constant amplitude $\\sqrt{x_{0}^{2} + \\left(v_{0}/\\omega\\right)^{2}}$ provided not both $x_{0}$ and $v_{0}$ are zero.\n- Implement two integrators for the first-order system: explicit Euler and classical Runge–Kutta of order four (RK4). For each test, track the full time history of $E_{\\mathrm{rel}}(t_{n})$ for $n = 0, 1, \\dots, N$ where $N = T/h$ and $T$ is the final time. Then report both the final-time relative error $E_{\\mathrm{rel}}(T)$ and the maximum relative error over the trajectory $\\max_{0 \\le n \\le N} E_{\\mathrm{rel}}(t_{n})$ for each method.\n\nUnits and angle specifications:\n- Time $t$ must be in seconds, displacement $x$ in meters, velocity $v$ in meters per second, and angular frequency $\\omega$ in radians per second.\n- Trigonometric functions for the exact solution must interpret angles in radians.\n\nTest suite and parameters:\nFor each test case, you are given a quintuple $(\\omega, x_{0}, v_{0}, h, T)$ with the meanings defined above. Use exactly the following four test cases:\n- Test case $1$: $(\\omega, x_{0}, v_{0}, h, T) = (\\,1.0,\\, 1.0,\\, 0.0,\\, 0.01,\\, 10.0\\,)$.\n- Test case $2$: $(\\omega, x_{0}, v_{0}, h, T) = (\\,1.0,\\, 1.0,\\, 0.0,\\, 0.1,\\, 50.0\\,)$.\n- Test case $3$: $(\\omega, x_{0}, v_{0}, h, T) = (\\,2.0,\\, 1.0,\\, 0.0,\\, 0.05,\\, 25.0\\,)$.\n- Test case $4$: $(\\omega, x_{0}, v_{0}, h, T) = (\\,1.0,\\, 1.0,\\, 1.0,\\, 0.25,\\, 20.0\\,)$.\n\nOutput specification:\n- For each test case, compute four floats: $E_{\\mathrm{rel}}^{\\mathrm{Euler}}(T)$, $E_{\\mathrm{rel}}^{\\mathrm{RK4}}(T)$, $\\max_{0 \\le n \\le N} E_{\\mathrm{rel}}^{\\mathrm{Euler}}(t_{n})$, and $\\max_{0 \\le n \\le N} E_{\\mathrm{rel}}^{\\mathrm{RK4}}(t_{n})$.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a list of the four floats in the order specified above. The final format must be\n  `[[a_1, b_1, c_1, d_1], [a_2, b_2, c_2, d_2], [a_3, b_3, c_3, d_3], [a_4, b_4, c_4, d_4]]`.\n- Each floating-point value must be printed in scientific notation, rounded to $10$ significant digits.\n\nScientific realism and constraints:\n- All scenarios are physically and numerically plausible for a simple harmonic oscillator with the given parameters. The target quantities are dimensionless relative errors. The program must be self-contained, require no input, and follow the output format exactly.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded exercise in computational physics that requests the comparison of two standard numerical integration schemes, the explicit Euler method and the classical fourth-order Runge-Kutta method, for the simple harmonic oscillator. All parameters, definitions, and requirements are provided in a clear and unambiguous manner. The problem is self-contained and free from logical contradictions or factual inaccuracies.\n\nThe solution proceeds in a sequence of logical steps: first, we derive the exact analytical solution of the system, which serves as the ground truth. Second, we formulate the iterative update rules for the two numerical methods. Third, we define the error metrics used for comparison. Finally, we describe the computational algorithm to generate the required output for the given test cases.\n\nThe system is a one-dimensional simple harmonic oscillator, governed by the second-order ordinary differential equation (ODE):\n$$\n\\frac{d^{2}x}{dt^{2}} + \\omega^{2} x = 0\n$$\nwhere $x$ is the displacement, $t$ is time, and $\\omega$ is the angular frequency. This is converted into a system of two first-order ODEs by defining the state vector $\\mathbf{y}(t) = \\begin{bmatrix} x(t) \\\\ v(t) \\end{bmatrix}$, where $v(t) = \\frac{dx}{dt}$ is the velocity. The system dynamics are then expressed as:\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y}) = \\begin{bmatrix} v \\\\ -\\omega^{2} x \\end{bmatrix}\n$$\nwith initial conditions $\\mathbf{y}(0) = \\begin{bmatrix} x(0) \\\\ v(0) \\end{bmatrix} = \\begin{bmatrix} x_{0} \\\\ v_{0} \\end{bmatrix}$.\n\nThe exact solution to this linear constant-coefficient ODE is found by assuming a solution of the form $x(t) = A \\cos(\\omega t) + B \\sin(\\omega t)$. Applying the initial conditions:\n$x(0) = A = x_{0}$.\nThe velocity is $v(t) = \\frac{dx}{dt} = -A\\omega \\sin(\\omega t) + B\\omega \\cos(\\omega t)$.\n$v(0) = B\\omega = v_{0}$, which implies $B = \\frac{v_{0}}{\\omega}$.\nThus, the exact solution for the state vector components is:\n$$\nx_{\\mathrm{exact}}(t) = x_{0} \\cos(\\omega t) + \\frac{v_{0}}{\\omega} \\sin(\\omega t)\n$$\n$$\nv_{\\mathrm{exact}}(t) = -x_{0} \\omega \\sin(\\omega t) + v_{0} \\cos(\\omega t)\n$$\nThese functions provide the reference values against which the numerical approximations are compared.\n\nThe problem requires the implementation of two numerical integration schemes for the system $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y})$. For a discrete time step $h$, we denote the state at time $t_{n} = n h$ as $\\mathbf{y}_{n}$.\n\n$1.$ **Explicit Euler Method**: This is a first-order method. The state at the next time step, $\\mathbf{y}_{n+1}$, is found by a simple forward extrapolation:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_{n} + h \\mathbf{f}(\\mathbf{y}_{n})\n$$\nIn component form, this becomes:\n$$\nx_{n+1} = x_{n} + h v_{n}\n$$\n$$\nv_{n+1} = v_{n} - h \\omega^{2} x_{n}\n$$\n\n$2.$ **Classical Fourth-Order Runge-Kutta (RK4) Method**: This is a fourth-order method known for its balance of accuracy and computational cost. The update rule is:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_{n} + \\frac{h}{6} (\\mathbf{k}_{1} + 2\\mathbf{k}_{2} + 2\\mathbf{k}_{3} + \\mathbf{k}_{4})\n$$\nwhere the intermediate slopes $\\mathbf{k}_{i}$ are calculated as:\n$$\n\\begin{aligned}\n\\mathbf{k}_{1} = \\mathbf{f}(\\mathbf{y}_{n}) \\\\\n\\mathbf{k}_{2} = \\mathbf{f}(\\mathbf{y}_{n} + \\frac{h}{2}\\mathbf{k}_{1}) \\\\\n\\mathbf{k}_{3} = \\mathbf{f}(\\mathbf{y}_{n} + \\frac{h}{2}\\mathbf{k}_{2}) \\\\\n\\mathbf{k}_{4} = \\mathbf{f}(\\mathbf{y}_{n} + h\\mathbf{k}_{3})\n\\end{aligned}\n$$\n\nTo quantify the error, we use the scaled state vector $\\mathbf{s}(t) = \\begin{bmatrix} x(t) \\\\ v(t)/\\omega \\end{bmatrix}$. For the exact solution, the squared Euclidean norm of this vector, $\\|\\mathbf{s}_{\\mathrm{exact}}(t)\\|_{2}^{2} = x_{\\mathrm{exact}}(t)^2 + (v_{\\mathrm{exact}}(t)/\\omega)^2$, is a conserved quantity equal to the constant value $x_{0}^{2} + (v_{0}/\\omega)^{2}$. This provides a stable normalization factor for defining relative error.\n\nAt each discrete time $t_{n}$, the absolute error is the Euclidean distance between the numerical and exact scaled states:\n$$\nE_{\\mathrm{abs}}(t_{n}) = \\| \\mathbf{s}_{\\mathrm{num}}(t_{n}) - \\mathbf{s}_{\\mathrm{exact}}(t_{n}) \\|_{2}\n$$\nThe relative error is then defined as:\n$$\nE_{\\mathrm{rel}}(t_{n}) = \\frac{E_{\\mathrm{abs}}(t_{n})}{\\| \\mathbf{s}_{\\mathrm{exact}}(t_{0}) \\|_{2}} = \\frac{\\| \\mathbf{s}_{\\mathrm{num}}(t_{n}) - \\mathbf{s}_{\\mathrm{exact}}(t_{n}) \\|_{2}}{\\sqrt{x_{0}^{2} + (v_{0}/\\omega)^{2}}}\n$$\nThe denominator is constant throughout the simulation, provided the initial state is non-trivial ($x_0$ and $v_0$ not both zero).\n\nThe computational procedure for each test case is as follows:\n$1.$ Set the parameters $(\\omega, x_{0}, v_{0}, h, T)$.\n$2.$ Determine the number of steps $N = T/h$.\n$3.$ Create an array of time points $t_{n} = n h$ for $n = 0, 1, \\dots, N$.\n$4.$ Calculate the exact solution $\\mathbf{y}_{\\mathrm{exact}}(t_{n})$ for all time points.\n$5.$ Initialize the numerical solutions $\\mathbf{y}^{\\mathrm{Euler}}_{0}$ and $\\mathbf{y}^{\\mathrm{RK4}}_{0}$ with the initial condition $\\begin{bmatrix} x_{0} \\\\ v_{0} \\end{bmatrix}$.\n$6.$ Initialize histories for relative errors $E_{\\mathrm{rel}}^{\\mathrm{Euler}}$ and $E_{\\mathrm{rel}}^{\\mathrm{RK4}}$, starting with an error of $0$ at $t=0$.\n$7.$ Loop for $n$ from $0$ to $N-1$:\n    a. Compute $\\mathbf{y}^{\\mathrm{Euler}}_{n+1}$ and $\\mathbf{y}^{\\mathrm{RK4}}_{n+1}$ using their respective update rules.\n    b. At $t_{n+1}$, form the scaled state vectors $\\mathbf{s}_{\\mathrm{num}}(t_{n+1})$ for both methods and $\\mathbf{s}_{\\mathrm{exact}}(t_{n+1})$.\n    c. Calculate $E_{\\mathrm{rel}}(t_{n+1})$ for both methods and record the values.\n$8.$ After the loop completes, extract the final relative errors at $t_N = T$, which are $E_{\\mathrm{rel}}^{\\mathrm{Euler}}(T)$ and $E_{\\mathrm{rel}}^{\\mathrm{RK4}}(T)$.\n$9.$ Determine the maximum relative errors over the entire trajectory, $\\max_{0 \\le n \\le N} E_{\\mathrm{rel}}^{\\mathrm{Euler}}(t_{n})$ and $\\max_{0 \\le n \\le N} E_{\\mathrm{rel}}^{\\mathrm{RK4}}(t_{n})$.\n$10.$ Report these four computed values, formatted as specified.\n\nThis procedure is repeated for all provided test cases. The explicit Euler method is expected to show error that grows, often linearly with time, and is of order $O(h)$. The RK4 method, being of higher order, is expected to maintain a much smaller error, of order $O(h^4)$, for a significantly longer duration.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing Euler and RK4 integrators for a simple\n    harmonic oscillator, as specified in the problem statement.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (omega, x0, v0, h, T)\n        (1.0, 1.0, 0.0, 0.01, 10.0),\n        (1.0, 1.0, 0.0, 0.1, 50.0),\n        (2.0, 1.0, 0.0, 0.05, 25.0),\n        (1.0, 1.0, 1.0, 0.25, 20.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        omega, x0, v0, h, T = case\n\n        # Ensure N is an integer to avoid floating-point issues in loop ranges\n        N = int(round(T / h))\n        times = np.linspace(0, T, N + 1)\n\n        # --- Define the ODE system ---\n        def f(y, omega_val):\n            \"\"\"RHS of the ODE system: dy/dt = f(y)\"\"\"\n            x, v = y\n            return np.array([v, -omega_val**2 * x])\n\n        # --- Exact Solution ---\n        # Vectorized calculation for all time points\n        x_exact = x0 * np.cos(omega * times) + (v0 / omega) * np.sin(omega * times)\n        v_exact = -x0 * omega * np.sin(omega * times) + v0 * np.cos(omega * times)\n        s_exact = np.vstack((x_exact, v_exact / omega)).T\n\n        # --- Normalization factor for relative error ---\n        norm = np.sqrt(x0**2 + (v0 / omega)**2)\n        if norm == 0:\n            # Handle trivial case, though not present in test suite\n            all_results.append([0.0, 0.0, 0.0, 0.0])\n            continue\n\n        # --- Numerical Integration ---\n        # Initialize states\n        y_euler = np.array([x0, v0])\n        y_rk4 = np.array([x0, v0])\n        \n        # Histories for relative errors (N+1 points, from t=0 to t=T)\n        rel_err_euler_hist = [0.0]\n        rel_err_rk4_hist = [0.0]\n\n        for i in range(N):\n            # Euler step\n            y_euler = y_euler + h * f(y_euler, omega)\n\n            # RK4 step\n            k1 = f(y_rk4, omega)\n            k2 = f(y_rk4 + 0.5 * h * k1, omega)\n            k3 = f(y_rk4 + 0.5 * h * k2, omega)\n            k4 = f(y_rk4 + h * k3, omega)\n            y_rk4 = y_rk4 + (h / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n\n            # --- Error Calculation at step i+1 ---\n            # Scaled numerical states\n            s_euler_num = np.array([y_euler[0], y_euler[1] / omega])\n            s_rk4_num = np.array([y_rk4[0], y_rk4[1] / omega])\n            \n            # Exact scaled state at this time\n            s_exact_current = s_exact[i + 1]\n\n            # Absolute errors\n            abs_err_euler = np.linalg.norm(s_euler_num - s_exact_current)\n            abs_err_rk4 = np.linalg.norm(s_rk4_num - s_exact_current)\n\n            # Relative errors\n            rel_err_euler = abs_err_euler / norm\n            rel_err_rk4 = abs_err_rk4 / norm\n\n            rel_err_euler_hist.append(rel_err_euler)\n            rel_err_rk4_hist.append(rel_err_rk4)\n\n        # --- Collect results for this case ---\n        # Final-time relative errors\n        e_rel_euler_final = rel_err_euler_hist[-1]\n        e_rel_rk4_final = rel_err_rk4_hist[-1]\n        \n        # Maximum relative errors over the trajectory\n        max_e_rel_euler = max(rel_err_euler_hist)\n        max_e_rel_rk4 = max(rel_err_rk4_hist)\n\n        all_results.append([\n            e_rel_euler_final,\n            e_rel_rk4_final,\n            max_e_rel_euler,\n            max_e_rel_rk4\n        ])\n\n    # --- Final Output Formatting ---\n    # Format each number and build the final string representation\n    # Use {:.9e} to get 10 significant digits in scientific notation.\n    case_strings = []\n    for result_set in all_results:\n        formatted_nums = [f\"{num:.9e}\" for num in result_set]\n        case_strings.append(f\"[{','.join(formatted_nums)}]\")\n\n    final_output_str = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "2370478"}]}