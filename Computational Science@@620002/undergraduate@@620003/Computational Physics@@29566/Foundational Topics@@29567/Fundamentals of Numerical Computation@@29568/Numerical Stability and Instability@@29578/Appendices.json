{"hands_on_practices": [{"introduction": "We begin with a classic case where numerical instability arises in a seemingly simple task: finding the roots of a quadratic equation. This exercise demonstrates how catastrophic cancellation—the loss of significant digits when subtracting nearly-equal numbers—can corrupt a result when one term in the standard formula becomes negligible compared to another, such as when $b^2 \\gg 4ac$. By deriving and implementing a stable alternative, you will gain a fundamental appreciation for how the algebraic form of an equation impacts its numerical evaluation [@problem_id:2421654].", "problem": "You are to investigate numerical stability and instability in computing the roots of a quadratic polynomial using floating-point arithmetic. Consider the real roots of the quadratic equation $a x^2 + b x + c = 0$, which in exact arithmetic are given by the quadratic formula. In finite-precision arithmetic, subtractive cancellation severely degrades accuracy when $b^2 \\gg 4ac$. Your task is to analyze this situation from first principles and implement a program that quantifies the resulting error for a carefully chosen test suite.\n\nStarting point and goals:\n1) Begin from the exact definition of the roots of the quadratic equation: the roots are the values $x$ that satisfy $a x^2 + b x + c = 0$ for given real coefficients $a$, $b$, and $c$. In exact arithmetic, using the quadratic formula and the normal rules of real numbers yields the exact two roots. Define the discriminant $D = b^2 - 4ac$ and assume $D \\ge 0$ so both roots are real. In floating-point arithmetic, rounding and especially subtractive cancellation (the loss of significant digits when subtracting nearly equal numbers) must be considered.\n2) Analyze the instability: when $b^2 \\gg 4ac$, the quantity $\\sqrt{b^2 - 4ac}$ is close to $|b|$. Using the form\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\none of the two signs induces a subtraction of nearly equal large numbers in the numerator, which leads to catastrophic cancellation and large relative error in that root when computed in floating-point arithmetic. Using standard algebraic identities for quadratic equations (such as Vieta’s formulas: the sum of roots $r_1 + r_2 = -b/a$ and the product $r_1 r_2 = c/a$), re-express the computation to avoid this subtraction. Do not present memorized “fix” formulas; instead, reason from these identities and exact algebra to formulate an alternative that is algebraically equivalent in exact arithmetic but avoids subtracting nearly equal quantities in the floating-point computation.\n3) Quantify error: for a computed approximation $\\hat{x}$ of a true root $x^{\\star}$, define the relative error as $|\\hat{x} - x^{\\star}| / |x^{\\star}|$. You will use an arbitrarily high precision arithmetic to obtain a ground-truth $x^{\\star}$ against which to compare floating-point results.\n\nProgramming requirements:\n- Implement two algorithms for the roots of $a x^2 + b x + c = 0$:\n  1) A direct computation using $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ in standard double-precision floating-point arithmetic.\n  2) A numerically stable alternative that you derive by algebraic manipulation from the exact identities, designed to avoid subtractive cancellation when $b^2 \\gg 4ac$; then compute the second root using the exact product relationship $r_1 r_2 = c/a$ to avoid cancellation in the second root as well.\n- For the purpose of error assessment, compute a high-precision reference using arbitrary-precision floating-point arithmetic with at least $80$ decimal digits of precision. Use the same algebraic approach that avoids subtractive cancellation in the high-precision computation so that the reference is as accurate as possible.\n- For each test case, identify the “smaller-magnitude root” as the root with the smaller absolute value among the two true roots $x^{\\star}_{\\text{small}} = \\operatorname{argmin}_{r \\in \\{r_1, r_2\\}} |r|$. For each algorithm, select the computed root with smaller magnitude among its two outputs and compare it to $x^{\\star}_{\\text{small}}$ by the relative error $|\\hat{x} - x^{\\star}_{\\text{small}}| / |x^{\\star}_{\\text{small}}|$.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a two-element list `[e_naive, e_stable]`, where `e_naive` is the relative error for the smaller-magnitude root from the direct quadratic formula and `e_stable` is the relative error for the smaller-magnitude root from your numerically stable method. Therefore, if there are $N$ test cases, the output is a single list of $N$ two-element lists.\n\nTest suite:\nUse the following six test cases, each given as $(a,b,c)$, chosen to exercise different stability regimes. In all expressions, all numbers are in pure real number units (no physical units), and angles are not involved.\n- Case 1: $(a,b,c) = (1, 10^8 + 1, 1)$, i.e., $(1, 100000001, 1)$.\n- Case 2: $(a,b,c) = (1, 10^{16}, 1)$.\n- Case 3: $(a,b,c) = (1, -10^8 - 1, 1)$, i.e., $(1, -100000001, 1)$.\n- Case 4: $(a,b,c) = (1, 2, 1)$, which has a repeated real root since $D = 0$.\n- Case 5: $(a,b,c) = (1, 2, 10^{-16})$.\n- Case 6: $(a,b,c) = (10^{-16}, 1, 10^{-16})$.\n\nFinal output specification:\n- The program must print exactly one line: a single Python-style list of lists, in the same order as the test suite above. Each inner list contains two floating-point numbers `[e_naive, e_stable]` representing the relative error of the smaller-magnitude root for the naive and stable computations, respectively. For example, a valid output format with two hypothetical test cases would be: `[[0.1,1e-12],[0.5,2.3e-16]]`.", "solution": "The problem at hand is to analyze, from first principles, the numerical instability encountered when computing the roots of a quadratic equation $a x^2 + b x + c = 0$ using the standard quadratic formula in finite-precision arithmetic, and to implement a numerically stable alternative.\n\nA problem is valid if it is scientifically sound, well-posed, and objective. This problem is a classic exercise in numerical analysis, grounded in the established principles of floating-point arithmetic and error analysis. The givens are complete, the objective is clearly defined, and the premises are factually correct within the domain of computational science. The problem is therefore deemed valid.\n\nWe begin with the analytical solution. The roots of the quadratic equation $a x^2 + b x + c = 0$, for real coefficients $a$, $b$, and $c$, where $a \\ne 0$, are given by the quadratic formula:\n$$\nx = \\frac{-b \\pm \\sqrt{D}}{2a}\n$$\nwhere the discriminant is $D = b^2 - 4ac$. We are given that $D \\ge 0$, ensuring the roots are real. Let the two roots be denoted by $r_1$ and $r_2$.\n\n**Analysis of Numerical Instability**\n\nThe source of instability arises when using finite-precision floating-point arithmetic, particularly in the case where $b^2 \\gg 4ac$. This condition implies that the discriminant $D = b^2 - 4ac$ is very close to $b^2$. Consequently, $\\sqrt{D}$ is very close to $\\sqrt{b^2} = |b|$.\n\nLet us analyze the numerator of the quadratic formula, $-b \\pm \\sqrt{D}$.\nThe computation of one of the roots will involve the term $-b + \\sqrt{D}$ or $-b - \\sqrt{D}$.\nConsider the case when $\\operatorname{sgn}(b) = +1$, so $b > 0$. Then $|b|=b$.\nThe term $\\sqrt{D} = \\sqrt{b^2 - 4ac}$ is slightly less than $b$.\nThe two roots are computed via:\n$$\nx_1 = \\frac{-b + \\sqrt{D}}{2a} \\quad \\text{and} \\quad x_2 = \\frac{-b - \\sqrt{D}}{2a}\n$$\nIn the expression for $x_1$, the numerator involves the subtraction of two nearly equal positive numbers, $-b$ and $\\sqrt{D}$. This operation is known as **catastrophic cancellation**. In floating-point arithmetic, if $\\sqrt{D}$ is sufficiently close to $b$, their difference may have very few or even zero significant digits, leading to a large relative error in the computed value of $x_1$. The root $x_2$, conversely, involves summing two large negative numbers, which is a numerically stable operation.\n\nSimilarly, if $\\operatorname{sgn}(b) = -1$, so $b < 0$, then $|b| = -b$.\nIn this case, $\\sqrt{D}$ is close to $|b| = -b$. The two roots are:\n$$\nx_1' = \\frac{-b - \\sqrt{D}}{2a} \\quad \\text{and} \\quad x_2' = \\frac{-b + \\sqrt{D}}{2a}\n$$\nHere, the numerator of $x_1'$ involves subtracting two nearly equal numbers (since $\\sqrt{D} \\approx -b$), leading to catastrophic cancellation. The calculation of $x_2'$, involving the sum of two large positive numbers, is stable.\n\nIn general, the root computation is unstable when the sign of the radical is chosen to be opposite to the sign of $-b$, which is equivalent to being the same as the sign of $b$. This unstable computation always yields the root with the smaller absolute magnitude.\n\n**Derivation of a Stable Algorithm**\n\nTo remedy this, we must formulate an alternative expression that is algebraically equivalent but avoids the subtractive cancellation. The problem directs us to use Vieta's formulas, which relate the roots of a polynomial to its coefficients. For our quadratic equation, they are:\n$$\nr_1 + r_2 = -\\frac{b}{a} \\quad \\text{(sum of roots)}\n$$\n$$\nr_1 r_2 = \\frac{c}{a} \\quad \\text{(product of roots)}\n$$\nThe strategy is to first compute the root that does *not* suffer from cancellation (the larger-magnitude root) using the standard formula, and then use Vieta's formulas to find the second root (the smaller-magnitude root) accurately.\n\nThe stable calculation for one root involves the addition of quantities of like sign in the numerator. This corresponds to the expression:\n$$\nr_{\\text{large}} = \\frac{-b - \\operatorname{sgn}(b)\\sqrt{b^2 - 4ac}}{2a}\n$$\nHere, $\\operatorname{sgn}(b)$ is the sign function, which is $+1$ if $b \\ge 0$ and $-1$ if $b < 0$ (the choice for $b=0$ is arbitrary but must be consistent; this formula works regardless). This expression robustly computes the root with the larger absolute value, as it avoids subtracting nearly equal quantities.\n\nOnce $r_{\\text{large}}$ is computed accurately, we can use the product identity $r_{\\text{large}} \\cdot r_{\\text{small}} = c/a$ to find the other root, $r_{\\text{small}}$:\n$$\nr_{\\text{small}} = \\frac{c/a}{r_{\\text{large}}} = \\frac{c}{a \\cdot r_{\\text{large}}}\n$$\nSubstituting the expression for $r_{\\text{large}}$, we get:\n$$\nr_{\\text{small}} = \\frac{c}{a \\left( \\frac{-b - \\operatorname{sgn}(b)\\sqrt{b^2 - 4ac}}{2a} \\right)} = \\frac{2c}{-b - \\operatorname{sgn}(b)\\sqrt{b^2-4ac}}\n$$\nThis form for $r_{\\text{small}}$ avoids the cancellation that plagued the naive formula because division is a well-behaved operation in floating-point arithmetic (provided the denominator is not near zero, which is not the case here as $|-b - \\operatorname{sgn}(b)\\sqrt{D}| \\approx 2|b|$).\n\nThe resulting stable algorithm is as follows:\n1.  Compute the discriminant $D = b^2 - 4ac$.\n2.  Calculate one root, $r_1$, using the stable form: $r_1 = \\frac{-b - \\operatorname{sgn}(b)\\sqrt{D}}{2a}$.\n3.  Calculate the second root, $r_2$, using Vieta's product formula: $r_2 = \\frac{c}{a \\cdot r_1}$.\n\n**Implementation and Error Quantification**\n\nThe implementation will consist of three parts for each test case $(a,b,c)$:\n1.  **Reference Computation:** We use Python's `decimal` library with a precision of $85$ digits to compute the \"true\" roots $x^{\\star}_1$ and $x^{\\star}_2$ using the stable algorithm. The smaller-magnitude root, $x^{\\star}_{\\text{small}}$, is identified.\n2.  **Naive Computation:** We use standard double-precision floating-point arithmetic (`float`) to compute roots $\\hat{x}_{n1}, \\hat{x}_{n2}$ using the direct quadratic formula $x = \\frac{-b \\pm \\sqrt{D}}{2a}$. We identify the smaller-magnitude root from this pair, $\\hat{x}_{n, \\text{small}}$.\n3.  **Stable Computation:** We use `float` arithmetic to compute roots $\\hat{x}_{s1}, \\hat{x}_{s2}$ using the stable algorithm derived above. We identify the smaller-magnitude root, $\\hat{x}_{s, \\text{small}}$.\n\nFor both the naive and stable algorithms, the relative error of the computed smaller-magnitude root is calculated against the high-precision reference:\n$$\ne = \\frac{|\\hat{x}_{\\text{small}} - x^{\\star}_{\\text{small}}|}{|x^{\\star}_{\\text{small}}|}\n$$\nThis error quantification will demonstrate the failure of the naive method and the success of the stable method in the regime where $b^2 \\gg 4ac$. For cases where this condition does not hold (e.g., $D=0$), both methods are expected to perform well.", "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Analyzes numerical stability in solving quadratic equations and quantifies the error\n    of a naive vs. a stable algorithm for a given test suite.\n    \"\"\"\n    \n    # Set precision for high-accuracy reference calculations.\n    # At least 80 decimal digits are required; 85 provides a margin.\n    getcontext().prec = 85\n\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        (1, 10**8 + 1, 1),\n        (1, 10**16, 1),\n        (1, -(10**8 + 1), 1),\n        (1, 2, 1),\n        (1, 2, 1e-16),\n        (1e-16, 1, 1e-16)\n    ]\n\n    results = []\n\n    for a_val, b_val, c_val in test_cases:\n        # --- 1. High-Precision Reference Calculation ---\n        a_hp, b_hp, c_hp = Decimal(a_val), Decimal(b_val), Decimal(str(c_val))\n\n        d_hp = b_hp**2 - 4 * a_hp * c_hp\n        \n        # Stable algorithm for high-precision reference roots\n        if d_hp < 0:\n            # Problem constraints ensure D >= 0, but as a safeguard.\n            # For this problem, we can assume this branch is not taken.\n            ref_r1, ref_r2 = float('nan'), float('nan')\n        else:\n            sqrt_d_hp = d_hp.sqrt()\n            # Use np.copysign on the float value of b_hp to determine the sign.\n            # This is robust for b=0.\n            r1_hp = (-b_hp - Decimal(np.copysign(1, b_val)) * sqrt_d_hp) / (2 * a_hp)\n            if r1_hp == 0:\n                # This case happens if c=0, so one root is 0. The other is -b/a.\n                # The stable algorithm for r2 would fail.\n                # However, test cases do not have c=0.\n                r2_hp = -b_hp / a_hp\n            else:\n                r2_hp = c_hp / (a_hp * r1_hp)\n            ref_r1, ref_r2 = r1_hp, r2_hp\n\n        # Identify the smaller-magnitude reference root\n        if abs(ref_r1) < abs(ref_r2):\n            x_star_small = ref_r1\n        else:\n            x_star_small = ref_r2\n            \n        # --- 2. Standard Double-Precision Calculations ---\n        a_f, b_f, c_f = float(a_val), float(b_val), float(c_val)\n        d_f = b_f**2 - 4 * a_f * c_f\n        \n        if d_f < 0:\n            # Handle cases where floating point error makes D negative\n            sqrt_d_f = 0.0\n        else:\n            sqrt_d_f = np.sqrt(d_f)\n\n        # 2a. Naive algorithm\n        naive_r1 = (-b_f + sqrt_d_f) / (2 * a_f)\n        naive_r2 = (-b_f - sqrt_d_f) / (2 * a_f)\n\n        # Identify smaller-magnitude root from naive method\n        x_hat_n_small = naive_r1 if abs(naive_r1) < abs(naive_r2) else naive_r2\n\n        # 2b. Stable algorithm\n        stable_r1 = (-b_f - np.copysign(sqrt_d_f, b_f)) / (2 * a_f)\n        if stable_r1 == 0:\n             # As before, this is for c=0 case not present in tests.\n             stable_r2 = -b_f / a_f\n        else:\n            stable_r2 = c_f / (a_f * stable_r1)\n        \n        # Identify smaller-magnitude root from stable method\n        # Note: The way the stable method is structured, stable_r2 is derived\n        # to be the smaller magnitude root when b^2 >> 4ac. Comparing magnitudes\n        # is a more general approach that is always correct.\n        if abs(stable_r1) < abs(stable_r2):\n            x_hat_s_small = stable_r1\n        else:\n            x_hat_s_small = stable_r2\n\n        # --- 3. Error Calculation ---\n        # Perform error calculation in high precision to avoid further errors.\n        if x_star_small == 0:\n            # If the true root is 0, relative error is 0 if computed is 0, else infinite.\n            # Test cases do not have zero roots.\n            e_naive = 0.0 if x_hat_n_small == 0.0 else float('inf')\n            e_stable = 0.0 if x_hat_s_small == 0.0 else float('inf')\n        else:\n            e_naive = float(abs(Decimal(x_hat_n_small) - x_star_small) / abs(x_star_small))\n            e_stable = float(abs(Decimal(x_hat_s_small) - x_star_small) / abs(x_star_small))\n        \n        results.append([e_naive, e_stable])\n\n    # Final print statement in the exact required format.\n    print(f\"{results}\")\n\nsolve()\n```", "id": "2421654"}, {"introduction": "Building on the concept of round-off error, this practice explores the crucial trade-off between it and \"truncation error\" in the context of numerical differentiation. You will empirically discover that making the step size $h$ infinitesimally small is not the path to perfect accuracy, as a smaller $h$ reduces the truncation error of the approximation but amplifies the round-off error. This exercise highlights the existence of an optimal step size that balances these two competing error sources, a fundamental challenge in many numerical methods [@problem_id:2421640].", "problem": "You are to write a complete program that, for a finite set of step sizes, determines the step size $h$ that minimizes the absolute error of a finite-difference approximation to the derivative of $\\sin(x)$ at a specified point $x_0$, under prescribed difference schemes. Angles must be treated in radians.\n\nUse the following fully specified setup.\n\n1. Function and exact derivative:\n   - Target function: $f(x) = \\sin(x)$.\n   - Exact derivative: $f'(x) = \\cos(x)$.\n\n2. Finite-difference schemes to be evaluated:\n   - Forward difference: $$D_{\\mathrm{fwd}}(x_0;h) = \\frac{\\sin(x_0 + h) - \\sin(x_0)}{h}$$.\n   - Central difference: $$D_{\\mathrm{cen}}(x_0;h) = \\frac{\\sin(x_0 + h) - \\sin(x_0 - h)}{2h}$$.\n\n3. Error metric to be minimized for a given $x_0$ and scheme:\n   - Absolute error: $$E(h) = \\left| D(x_0;h) - \\cos(x_0) \\right|$$.\n\n4. Candidate step sizes:\n   - Consider the geometric sequence $h_k = 10^{a + k\\Delta}$ with $a=-16$, $\\Delta=10^{-2}$, and $k \\in \\{0,1,2,\\dots,K\\}$ where $K=1500$. This yields the finite set $\\{h_k\\}$ spanning from $10^{-16}$ to $10^{-1}$ inclusive.\n\n5. Selection rule:\n   - For each test case, select $h^\\star$ from the above candidate set that minimizes $E(h)$. If multiple values of $h$ attain the same minimal error value, select the largest such $h$.\n\n6. Test suite (all angles in radians):\n   - Case 1: $x_0 = 1.0$, scheme = central.\n   - Case 2: $x_0 = 1.0$, scheme = forward.\n   - Case 3: $x_0 = \\frac{\\pi}{2}$, scheme = central.\n   - Case 4: $x_0 = \\frac{\\pi}{2}$, scheme = forward.\n   - Case 5: $x_0 = 0.0$, scheme = central.\n   - Case 6: $x_0 = 0.123$, scheme = forward.\n\nYour program must compute, for each of the six cases, the minimizing step size $h^\\star$ according to items 1–5 above. The final output must be a single line containing a comma-separated list of the six selected $h^\\star$ values, in the order of the cases listed, enclosed in square brackets. Express each returned step size using scientific notation with exactly six digits after the decimal point, for example $1.234567\\mathrm{e}{-08}$. The required final output format is thus a single line:\n[$h^\\star_1, h^\\star_2, h^\\star_3, h^\\star_4, h^\\star_5, h^\\star_6$].", "solution": "The problem requires us to determine the optimal step size $h^\\star$ that minimizes the absolute error of a finite-difference approximation for the derivative of $f(x) = \\sin(x)$ at several points $x_0$. This is a classic problem in numerical analysis, illustrating the fundamental trade-off between truncation error and round-off error.\n\nWe begin by analyzing the sources of error in numerical differentiation. The total absolute error $E(h)$ for a given step size $h$ is the magnitude of the difference between the numerical approximation $D(x_0;h)$ and the true derivative $f'(x_0)$:\n$$\nE(h) = |D(x_0;h) - f'(x_0)|\n$$\nThis total error can be viewed as the sum of two principal components: truncation error $E_{\\text{trunc}}(h)$ and round-off error $E_{\\text{round}}(h)$.\n\n1.  **Truncation Error**: This error is inherent to the approximation formula itself and arises from truncating the Taylor series expansion.\n    -   For the **forward-difference scheme**, $D_{\\mathrm{fwd}}(x_0;h) = \\frac{f(x_0+h) - f(x_0)}{h}$, we expand $f(x_0+h)$ around $x_0$:\n        $$\n        f(x_0+h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2} f''(x_0) + O(h^3)\n        $$\n        Substituting this into the formula for $D_{\\mathrm{fwd}}$ yields:\n        $$\n        D_{\\mathrm{fwd}}(x_0;h) = f'(x_0) + \\frac{h}{2} f''(x_0) + O(h^2)\n        $$\n        The leading-order truncation error is therefore proportional to $h$:\n        $$\n        E_{\\text{trunc, fwd}}(h) = \\left| \\frac{h}{2} f''(x_0) \\right|\n        $$\n        This is a first-order accurate method. For $f(x)=\\sin(x)$, we have $f''(x)=-\\sin(x)$, so $E_{\\text{trunc, fwd}}(h) = \\frac{h}{2} |\\sin(x_0)|$.\n\n    -   For the **central-difference scheme**, $D_{\\mathrm{cen}}(x_0;h) = \\frac{f(x_0+h) - f(x_0-h)}{2h}$, we use Taylor expansions for both $f(x_0+h)$ and $f(x_0-h)$:\n        $$\n        f(x_0 \\pm h) = f(x_0) \\pm h f'(x_0) + \\frac{h^2}{2} f''(x_0) \\pm \\frac{h^3}{6} f'''(x_0) + O(h^4)\n        $$\n        Subtracting the expansion for $f(x_0-h)$ from that of $f(x_0+h)$ gives:\n        $$\n        f(x_0+h) - f(x_0-h) = 2h f'(x_0) + \\frac{h^3}{3} f'''(x_0) + O(h^5)\n        $$\n        Dividing by $2h$, we obtain:\n        $$\n        D_{\\mathrm{cen}}(x_0;h) = f'(x_0) + \\frac{h^2}{6} f'''(x_0) + O(h^4)\n        $$\n        The leading-order truncation error is proportional to $h^2$:\n        $$\n        E_{\\text{trunc, cen}}(h) = \\left| \\frac{h^2}{6} f'''(x_0) \\right|\n        $$\n        This is a second-order accurate method. For $f(x)=\\sin(x)$, we have $f'''(x)=-\\cos(x)$, so $E_{\\text{trunc, cen}}(h) = \\frac{h^2}{6} |\\cos(x_0)|$.\n\n    In both cases, the truncation error decreases as $h$ becomes smaller.\n\n2.  **Round-off Error**: This error arises from the finite precision of floating-point arithmetic in a computer. When $h$ is very small, the subtraction in the numerator, e.g., $f(x_0+h) - f(x_0)$, involves two nearly equal numbers. This leads to a loss of significant figures, an effect known as catastrophic cancellation. The error in evaluating $f(x)$ is roughly proportional to the machine epsilon, $\\epsilon_{\\text{mach}}$, and the magnitude of $f(x)$. The error in the numerator is thus on the order of $\\epsilon_{\\text{mach}} |f(x_0)|$. This small absolute error is then divided by a very small number $h$, amplifying the round-off error.\n    $$\n    E_{\\text{round}}(h) \\approx \\frac{\\epsilon_{\\text{mach}} |f(x_0)|}{h}\n    $$\n    Unlike truncation error, round-off error *increases* as $h$ decreases.\n\nThe total error $E(h) \\approx E_{\\text{trunc}}(h) + E_{\\text{round}}(h)$ is a sum of a decreasing function of $h$ and an increasing function of $1/h$. This sum exhibits a minimum for some optimal step size $h^\\star$. For $h > h^\\star$, the error is dominated by truncation; for $h < h^\\star$, it is dominated by round-off.\n\nThe problem requires an empirical search for $h^\\star$ over a discrete, finite set of candidate step sizes, $\\{h_k\\}$, where $h_k = 10^{-16 + k \\cdot 10^{-2}}$ for $k \\in \\{0, 1, \\dots, 1500\\}$. The algorithm proceeds as follows for each test case:\n\n1.  Define the point $x_0$ and the numerical differentiation scheme (forward or central).\n2.  Compute the exact derivative, $d_{\\text{true}} = \\cos(x_0)$.\n3.  Generate the ordered set of candidate step sizes $\\{h_k\\}$ from smallest to largest.\n4.  Initialize a minimum error, $E_{\\min}$, to infinity and the optimal step size, $h^\\star$, to a placeholder value.\n5.  Iterate through each candidate step size $h_k$ in the set.\n    a. Calculate the numerical derivative $D(x_0; h_k)$ using the specified scheme.\n    b. Compute the absolute error $E(h_k) = |D(x_0; h_k) - d_{\\text{true}}|$.\n    c. Compare $E(h_k)$ with $E_{\\min}$. If $E(h_k) \\le E_{\\min}$, update $E_{\\min} = E(h_k)$ and $h^\\star = h_k$. This update rule ensures that if multiple $h_k$ values yield the same minimum error, the one chosen will be the largest, as the iteration proceeds from smaller to larger $h_k$. This directly satisfies the tie-breaking rule specified in the problem statement.\n6.  After iterating through all $h_k$, the final value of $h^\\star$ is the desired result for the test case.\n\nThis procedure is repeated for all six specified test cases. The resulting optimal step sizes are then collected and formatted according to the output specification. A special case arises for the central difference at $x_0 = \\pi/2$. Since $\\sin(\\pi/2+h) = \\cos(h)$ and $\\sin(\\pi/2-h) = \\cos(h)$, the numerator $f(x_0+h) - f(x_0-h)$ is mathematically zero. The numerical approximation yields an error close to machine precision for a wide range of $h$ values. The selection rule to choose the largest $h$ in case of a tie becomes critical here.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal finite-difference step size h by minimizing absolute error.\n\n    The function iterates through a set of candidate step sizes for six different\n    test cases involving forward and central difference approximations of the\n    derivative of sin(x). It finds the step size h that minimizes the\n    absolute error |D(x0;h) - cos(x0)|.\n    \"\"\"\n    # Define the six test cases as per the problem statement.\n    # Each case is a tuple: (x0, 'scheme_name')\n    test_cases = [\n        (1.0, 'central'),\n        (1.0, 'forward'),\n        (np.pi / 2, 'central'),\n        (np.pi / 2, 'forward'),\n        (0.0, 'central'),\n        (0.123, 'forward'),\n    ]\n\n    # Generate the candidate step sizes h_k = 10^(a + k*Delta)\n    a = -16.0\n    delta = 1e-2\n    K = 1500\n    exponents = a + np.arange(K + 1) * delta\n    h_values = np.power(10.0, exponents)\n\n    optimal_h_results = []\n\n    for x0, scheme in test_cases:\n        true_derivative = np.cos(x0)\n        \n        min_error = np.inf\n        optimal_h = -1.0\n\n        # Iterate through all candidate step sizes to find the minimum error.\n        # h_values are sorted in increasing order.\n        for h in h_values:\n            numerical_derivative = 0.0\n            if scheme == 'forward':\n                # Forward difference: (f(x+h) - f(x)) / h\n                numerical_derivative = (np.sin(x0 + h) - np.sin(x0)) / h\n            elif scheme == 'central':\n                # Central difference: (f(x+h) - f(x-h)) / (2h)\n                numerical_derivative = (np.sin(x0 + h) - np.sin(x0 - h)) / (2 * h)\n\n            error = np.abs(numerical_derivative - true_derivative)\n\n            # If the current error is less than or equal to the minimum error found so far,\n            # update the minimum error and the optimal h.\n            # Because we iterate from smallest h to largest h, this correctly implements\n            # the tie-breaking rule: \"select the largest such h\".\n            if error <= min_error:\n                min_error = error\n                optimal_h = h\n        \n        optimal_h_results.append(optimal_h)\n\n    # Format the results as specified: scientific notation with 6 decimal places.\n    formatted_results = [f\"{h:.6e}\" for h in optimal_h_results]\n    \n    # Print the final output in the required single-line format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2421640"}, {"introduction": "Our final exercise moves from static calculations to the dynamics of a physical system, the simple pendulum, which is governed by a system of ordinary differential equations. You will compare a straightforward but non-symplectic integrator (Forward Euler) with a structure-preserving symplectic integrator (Velocity Verlet) to observe their long-term behavior. This powerful demonstration reveals how the choice of algorithm can determine the physical realism of a simulation, showing that some methods lead to an unphysical, systematic gain in energy while others conserve a related quantity over long periods [@problem_id:2421691].", "problem": "You are to write a complete, runnable program that compares the long-term energy behavior of a simple pendulum evolved with a non-symplectic time integrator and with a symplectic time integrator. Use the non-dimensional simple pendulum with unit length, unit mass, and unit gravitational acceleration, so that the equations of motion are\n$$\n\\frac{d\\theta}{dt}=\\omega,\\quad \\frac{d\\omega}{dt}=-\\sin(\\theta),\n$$\nwhere $\\theta$ is the angular displacement and $\\omega$ is the angular velocity. The total mechanical energy is\n$$\nE(\\theta,\\omega)=\\frac{1}{2}\\,\\omega^2 + \\left(1-\\cos\\theta\\right).\n$$\nAngles must be measured in radians. All quantities are dimensionless. Implement one non-symplectic integrator (the forward explicit Euler scheme) and one symplectic integrator (the velocity Verlet scheme). For each simulation, compute the energy at every time step over a long time horizon and quantify the long-term energy drift.\n\nFor each test case, starting from initial state $(\\theta(0),\\omega(0))$, constant time step $\\Delta t$, and total time $T$, you must:\n- Simulate the system using the forward explicit Euler method over the interval $[0,T]$ with step size $\\Delta t$.\n- Simulate the system using the velocity Verlet method over the same interval and step size.\n- For each method, compute the following metrics using the discrete energy time series $\\{E_n\\}_{n=0}^N$ at times $t_n=n\\,\\Delta t$ with $N=T/\\Delta t$:\n  1. The final relative energy error,\n  $$\n  \\frac{E_N - E_0}{E_0}.\n  $$\n  2. The linear least-squares drift rate of $E_n$ versus $t_n$ divided by $E_0$ (this is the slope of the best-fit line to $(t_n,E_n)$, divided by $E_0$), which is a dimensionless drift rate per unit time.\n  3. The maximum absolute relative energy deviation,\n  $$\n  \\max_{0\\le n\\le N}\\frac{|E_n - E_0|}{E_0}.\n  $$\n\nUse the following test suite of initial conditions and numerical parameters; all angles are in radians:\n- Case 1: $\\theta(0)=0.1$, $\\omega(0)=0.0$, $\\Delta t=0.01$, $T=100.0$.\n- Case 2: $\\theta(0)=1.0$, $\\omega(0)=0.0$, $\\Delta t=0.05$, $T=200.0$.\n- Case 3: $\\theta(0)=0.5$, $\\omega(0)=0.0$, $\\Delta t=0.2$, $T=100.0$.\n- Case 4: $\\theta(0)=3.0$, $\\omega(0)=0.0$, $\\Delta t=0.01$, $T=200.0$.\n\nYour program must produce, for each case, a list of six real numbers in the order\n$$\n\\left[\\text{Euler final relative error, Verlet final relative error, Euler relative drift rate, Verlet relative drift rate, Euler max relative deviation, Verlet max relative deviation}\\right].\n$$\nAggregate the results for the four cases into a single list of lists, preserving the case order given above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[a,b,c,d,e,f],[...],...]\"). All outputs are dimensionless real numbers. No plots are required, and no other output should be printed.", "solution": "The problem requires a comparison of a non-symplectic integrator, the forward explicit Euler method, and a symplectic integrator, the velocity Verlet method, by applying them to the non-dimensional simple pendulum. The objective is to quantify and contrast their long-term energy conservation properties. The analysis will proceed by first validating the problem statement, then defining the physical system and the numerical schemes, and finally outlining the implementation that computes the required metrics.\n\nThe problem is determined to be valid. It is scientifically grounded in the principles of classical mechanics and numerical analysis. The system of equations, the energy function, the numerical methods, and the evaluation metrics are all standard and well-defined. The problem is self-contained, with all necessary parameters provided for each test case. There are no contradictions, ambiguities, or factual inaccuracies. Therefore, we proceed to the solution.\n\nThe dynamics of the non-dimensional simple pendulum are described by a system of two first-order ordinary differential equations:\n$$\n\\frac{d\\theta}{dt} = \\omega\n$$\n$$\n\\frac{d\\omega}{dt} = -\\sin(\\theta)\n$$\nHere, $\\theta$ is the angular displacement from the vertical and $\\omega$ is the angular velocity. The total mechanical energy $E$ of the system is the sum of its kinetic and potential energies, given by:\n$$\nE(\\theta, \\omega) = \\frac{1}{2}\\omega^2 + (1 - \\cos\\theta)\n$$\nwhere the potential energy is zero at the stable equilibrium point $\\theta=0$. For an isolated physical system, this energy must be conserved.\n\nWe will now define the two numerical integrators used to approximate the solution $(\\theta(t), \\omega(t))$ over a discrete set of time steps $t_n = n \\Delta t$.\n\n**1. Forward Explicit Euler Method:**\nThe forward Euler method is a first-order, non-symplectic integrator. It advances the state $(\\theta_n, \\omega_n)$ at time $t_n$ to the state $(\\theta_{n+1}, \\omega_{n+1})$ at time $t_{n+1} = t_n + \\Delta t$ using the following update rules:\n$$\n\\theta_{n+1} = \\theta_n + \\Delta t \\cdot \\omega_n\n$$\n$$\n\\omega_{n+1} = \\omega_n - \\Delta t \\cdot \\sin(\\theta_n)\n$$\nBeing non-symplectic, this method does not preserve the geometric structure of the phase space flow for a Hamiltonian system. The local truncation error, which is of order $O(\\Delta t^2)$, accumulates systematically over many steps. For an oscillatory system like the pendulum, this typically leads to a secular, or long-term, drift in the computed energy, usually an increase. The energy error is expected to grow linearly with time $T$.\n\n**2. Velocity Verlet Method:**\nThe velocity Verlet method is a second-order, symplectic integrator. It is specifically designed for systems of the form $\\ddot{x} = a(x)$, which is the case for the pendulum where $\\ddot{\\theta} = -\\sin(\\theta)$. The update is performed in three steps:\nFirst, the velocity is advanced by a half-step:\n$$\n\\omega_{n+1/2} = \\omega_n - \\sin(\\theta_n) \\frac{\\Delta t}{2}\n$$\nSecond, the position is advanced by a full step using this intermediate velocity:\n$$\n\\theta_{n+1} = \\theta_n + \\omega_{n+1/2} \\Delta t\n$$\nFinally, the velocity is advanced by the remaining half-step using the acceleration at the new position:\n$$\n\\omega_{n+1} = \\omega_{n+1/2} - \\sin(\\theta_{n+1}) \\frac{\\Delta t}{2}\n$$\nSymplectic integrators, by design, conserve a \"shadow Hamiltonian\" which is very close to the true Hamiltonian of the system. Consequently, they do not exhibit secular energy drift. The computed energy $E_n$ will oscillate around the true initial energy $E_0$, but the error $|E_n - E_0|$ remains bounded over very long, often exponentially long, simulation times.\n\nTo quantify these behaviors, we will compute three metrics from the discrete energy time series $\\{E_n\\}_{n=0}^N$, where $N=T/\\Delta t$:\n1.  **Final Relative Energy Error, $\\frac{E_N - E_0}{E_0}$**: This metric measures the net energy change over the entire simulation interval. For Euler, it is expected to be significantly non-zero, whereas for Verlet, it should be close to zero.\n2.  **Relative Drift Rate**: This is the slope $m$ of the ordinary least-squares line fitted to the points $(t_n, E_n)$, normalized by the initial energy $E_0$. The slope is computed via the formula $m = \\frac{\\sum_{n=0}^{N} (t_n - \\bar{t})(E_n - \\bar{E})}{\\sum_{n=0}^{N} (t_n - \\bar{t})^2}$, where $\\bar{t}$ and $\\bar{E}$ are the mean values. This metric directly quantifies the average rate of secular energy drift. We expect a significant positive value for Euler and a value close to zero for Verlet.\n3.  **Maximum Absolute Relative Energy Deviation, $\\max_{0\\le n\\le N}\\frac{|E_n - E_0|}{E_0}$**: This metric captures the maximum amplitude of energy fluctuation. For Euler, this will be dominated by the secular drift. For Verlet, it measures the size of the bounded energy oscillations.\n\nThe implementation will consist of a main procedure that iterates through each of the four specified test cases. For each case, separate simulations are executed using the Euler and Verlet integrators. The full history of states $(\\theta_n, \\omega_n)$ is stored for each run. From these histories, the corresponding energy time series $\\{E_n\\}$ are calculated. Then, the three metrics are computed for each integrator. The six resulting values are collected and formatted as required. This systematic approach allows for a direct and quantitative comparison of the integrators, which will demonstrate the superior long-term stability of the symplectic Velocity Verlet method.", "answer": "```python\nimport numpy as np\nimport json\n\ndef energy(theta, omega):\n    \"\"\"\n    Calculates the non-dimensional energy of the simple pendulum.\n    \"\"\"\n    return 0.5 * omega**2 + (1.0 - np.cos(theta))\n\ndef euler_step(theta, omega, dt):\n    \"\"\"\n    Performs a single step of the forward explicit Euler method.\n    \"\"\"\n    theta_next = theta + dt * omega\n    omega_next = omega - dt * np.sin(theta)\n    return theta_next, omega_next\n\ndef verlet_step(theta, omega, dt):\n    \"\"\"\n    Performs a single step of the velocity Verlet method.\n    \"\"\"\n    # Force (acceleration) function\n    def acceleration(th):\n        return -np.sin(th)\n\n    omega_half = omega + acceleration(theta) * (dt / 2.0)\n    theta_next = theta + omega_half * dt\n    omega_next = omega_half + acceleration(theta_next) * (dt / 2.0)\n    return theta_next, omega_next\n\ndef run_simulation(integrator_step_func, theta0, omega0, dt, T):\n    \"\"\"\n    Runs a simulation for the pendulum dynamics using a given integrator.\n    \"\"\"\n    if dt <= 0:\n        raise ValueError(\"Time step dt must be positive.\")\n    N = int(round(T / dt))\n    num_points = N + 1\n    \n    theta_hist = np.zeros(num_points)\n    omega_hist = np.zeros(num_points)\n    \n    theta_hist[0] = theta0\n    omega_hist[0] = omega0\n    \n    theta_n, omega_n = theta0, omega0\n    for n in range(N):\n        theta_n, omega_n = integrator_step_func(theta_n, omega_n, dt)\n        theta_hist[n + 1] = theta_n\n        omega_hist[n + 1] = omega_n\n        \n    return theta_hist, omega_hist\n\ndef calculate_metrics(T, dt, theta_hist, omega_hist):\n    \"\"\"\n    Calculates the three required energy metrics from a simulation history.\n    \"\"\"\n    num_points = len(theta_hist)\n    times = np.linspace(0, T, num_points)\n    \n    energies = energy(theta_hist, omega_hist)\n    E0 = energies[0]\n    EN = energies[-1]\n\n    if E0 == 0:\n        # This case is avoided by the problem's initial conditions,\n        # but as a safeguard, return NaNs.\n        return np.nan, np.nan, np.nan\n\n    # Metric 1: Final relative energy error\n    final_rel_err = (EN - E0) / E0\n\n    # Metric 2: Linear least-squares drift rate divided by E0\n    # The slope of the best-fit line to (times, energies) is the first\n    # coefficient returned by np.polyfit with deg=1.\n    slope = np.polyfit(times, energies, deg=1)[0]\n    rel_drift_rate = slope / E0\n\n    # Metric 3: Maximum absolute relative energy deviation\n    max_abs_rel_dev = np.max(np.abs((energies - E0) / E0))\n    \n    return final_rel_err, rel_drift_rate, max_abs_rel_dev\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (theta(0), omega(0), dt, T)\n        (0.1, 0.0, 0.01, 100.0),\n        (1.0, 0.0, 0.05, 200.0),\n        (0.5, 0.0, 0.2, 100.0),\n        (3.0, 0.0, 0.01, 200.0),\n    ]\n\n    all_results = []\n    for theta0, omega0, dt, T in test_cases:\n        # Run simulation with Forward Euler\n        euler_theta, euler_omega = run_simulation(euler_step, theta0, omega0, dt, T)\n        euler_metrics = calculate_metrics(T, dt, euler_theta, euler_omega)\n\n        # Run simulation with Velocity Verlet\n        verlet_theta, verlet_omega = run_simulation(verlet_step, theta0, omega0, dt, T)\n        verlet_metrics = calculate_metrics(T, dt, verlet_theta, verlet_omega)\n        \n        # Assemble the 6 metrics for the current case in the specified order\n        case_results = [\n            euler_metrics[0],  # Euler final relative error\n            verlet_metrics[0], # Verlet final relative error\n            euler_metrics[1],  # Euler relative drift rate\n            verlet_metrics[1], # Verlet relative drift rate\n            euler_metrics[2],  # Euler max relative deviation\n            verlet_metrics[2]  # Verlet max relative deviation\n        ]\n        all_results.append(case_results)\n\n    # Print the final result in the exact specified format `[[...],...]`\n    # Using json.dumps and replacing spaces ensures a compact, correct format.\n    print(json.dumps(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2421691"}]}