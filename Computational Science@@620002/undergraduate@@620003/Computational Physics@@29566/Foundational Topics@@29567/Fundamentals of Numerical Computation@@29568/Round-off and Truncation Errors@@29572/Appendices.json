{"hands_on_practices": [{"introduction": "Theory becomes tangible when a simple loop doesn't do what you expect. This first practice explores a fundamental pitfall of floating-point arithmetic: iterating with a fractional step size. By simulating a sequence and checking for exact equality with a target value, you will directly observe why seemingly straightforward code can fail, and more importantly, understand the underlying reasons related to binary representation and accumulating round-off error [@problem_id:2447428]. This foundational exercise is crucial for developing robust numerical code and avoiding common bugs.", "problem": "You are given real scalars $s$, $h$, and $t$, and a positive integer $M$. Consider the sequence $\\{x_n\\}_{n\\ge 0}$ defined by $x_0 = s$ and $x_{n+1} = x_n + h$ for $n \\ge 0$. A naive termination condition that checks whether $x_n = t$ may be unreliable when $x_n$ is computed in binary floating-point arithmetic due to representation and rounding effects. Your task is to write a program that, for each provided test case, simulates the floating-point sequence and reports whether the equality $x_n = t$ ever holds within a bounded number of simulated iterations.\n\nFor each test case, perform the following steps:\n1. Initialize $x_0 = s$ in double-precision floating-point arithmetic.\n2. For $n = 0,1,2,\\dots$, check whether $x_n = t$ exactly in floating-point arithmetic. If equality holds for some $n \\le M$, record the smallest such $n$ and terminate the simulation for that test case.\n3. If no equality occurs for $0 \\le n \\le M$, terminate the simulation after $M$ increments.\n4. Let $n_{\\mathrm{exec}}$ denote the number of increments actually performed by your simulation for the test case, where $n_{\\mathrm{exec}}$ equals the smallest $n$ for which equality is observed (if any), or $M$ otherwise.\n5. Compute the theoretical exact value $x_{\\mathrm{exact}} = s + n_{\\mathrm{exec}}\\,h$ using exact real arithmetic, not using floating-point rounding at intermediate steps.\n6. Let $x_{\\mathrm{float}}$ denote the floating-point value obtained by your simulation after $n_{\\mathrm{exec}}$ increments. Compute the absolute error $e = |x_{\\mathrm{float}} - x_{\\mathrm{exact}}|$ as a real number.\n\nFor each test case, your program must output a list with the following fields in order:\n- A boolean indicating whether there exists an $n \\in \\{0,1,\\dots,M\\}$ such that $x_n = t$ in floating-point arithmetic.\n- The smallest such $n$ if it exists; otherwise the integer $-1$.\n- The integer $n_{\\mathrm{exec}}$.\n- The floating-point value $x_{\\mathrm{float}}$ after $n_{\\mathrm{exec}}$ increments.\n- The theoretical exact value $x_{\\mathrm{exact}}$ converted to a floating-point number for reporting.\n- The absolute error $e$ as a floating-point number.\n\nUse the following five test cases. In all test cases, angles do not appear, and there are no physical units. The real numbers written in decimal (such as $0.1$) denote exact real values in base ten, and expressions involving powers of two (such as $2^{-55}$) denote exact real values.\n\n- Test case 1 (canonical failure of equality with decimal step):\n  $s = 0.0$, $h = 0.1$, $t = 1.0$, $M = 12$.\n- Test case 2 (exactly representable step and reachable target):\n  $s = 0.0$, $h = 0.125$, $t = 1.0$, $M = 8$.\n- Test case 3 (step does not subdivide the interval):\n  $s = 0.0$, $h = 0.3$, $t = 1.0$, $M = 10$.\n- Test case 4 (negative step toward zero with decimal step):\n  $s = 1.0$, $h = -0.1$, $t = 0.0$, $M = 12$.\n- Test case 5 (increments below one unit in the last place until accumulation):\n  $s = 1.0$, $h = 2^{-55}$, $t = 1 + 2^{-52}$, $M = 8$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one test case in the same order as above. For example, the output must have the form\n$[r_1,r_2,r_3,r_4,r_5]$\nwhere each $r_i$ is the list corresponding to test case $i$, and there must be no spaces anywhere in the line. Each list $r_i$ must be of the form\n$[\\text{hit}, n_{\\min}, n_{\\mathrm{exec}}, x_{\\mathrm{float}}, x_{\\mathrm{exact}}, e]$\nwith the field types defined above. All numeric outputs must be represented as base-ten floats or integers on this line.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the principles of numerical computation, well-posed with a clear and deterministic procedure, and objective in its formulation. It presents a standard but fundamental problem in computational engineering concerning the limitations of floating-point arithmetic. We shall now proceed with the solution.\n\nThe core of this problem lies in the fundamental distinction between exact real arithmetic and computer-based floating-point arithmetic, governed by the IEEE 754 standard for double-precision numbers. When simulating the sequence $x_{n+1} = x_n + h$, two primary sources of error arise:\n\n1.  **Representation Error**: Many decimal fractions, such as $0.1$ or $0.3$, do not have an exact finite representation in binary. They are stored as the nearest representable binary floating-point number. For example, the decimal $0.1$ is an infinite repeating fraction in binary ($0.0001100110011\\dots_2$), which must be truncated to fit the 52-bit significand of a double-precision float. This introduces an initial error before any computation begins. Conversely, numbers that are finite sums of powers of two, such as $0.125 = 1/8 = 2^{-3}$, are represented exactly.\n\n2.  **Round-off Error**: Each arithmetic operation, in this case, addition, is performed with finite precision. The mathematically exact result of $x_n + h$ is rounded to the nearest representable floating-point number. This small error, introduced at each step, can accumulate over many iterations, causing the computed sequence $x_n$ to diverge from its theoretical path.\n\nThe provided problem requires a simulation that demonstrates these effects. The methodology is as follows:\n\nFirst, we must meticulously handle the input values. The provided test case parameters $s$, $h$, and $t$ are defined as exact real numbers. To compute the theoretical value $x_{\\mathrm{exact}}$, we must use a high-precision arithmetic library that avoids standard floating-point inaccuracies. For this purpose, Python's `decimal` module is employed, configured with a sufficiently high precision to handle all calculations as if they were exact. The inputs for each test case are converted to these high-precision objects.\n\nSecond, a simulation is performed for each test case using standard double-precision floating-point arithmetic, which is represented in Python by the `float` type or `numpy.float64`. The simulation adheres to the following algorithm:\n1.  Initialize the floating-point sequence value $x_{\\mathrm{float}} \\leftarrow \\text{float}(s)$. Initialize `hit` to `False` and `n_min` to $-1$.\n2.  Iterate with an index $n$ from $0$ to $M$, inclusive. In each iteration, the current value $x_n$ of the sequence is represented by $x_{\\mathrm{float}}$.\n3.  At each step $n$, perform an exact equality check: if $x_{\\mathrm{float}} == \\text{float}(t)$.\n4.  If equality holds, the target has been hit. We set `hit` to `True`, record the current index as $n_{\\mathrm{min}} = n$, set the number of increments performed as $n_{\\mathrm{exec}} = n$, store the current value of $x_{\\mathrm{float}}$ as the final one, and terminate the simulation loop for this test case.\n5.  If equality does not hold and $n  M$, the sequence is advanced by one step: $x_{\\mathrm{float}} \\leftarrow x_{\\mathrm{float}} + \\text{float}(h)$.\n6.  If the loop completes without finding an equality (i.e., for all $n \\in \\{0, 1, \\dots, M\\}$), we set $n_{\\mathrm{exec}} = M$. The final value of $x_{\\mathrm{float}}$ is the result after $M$ increments.\n\nThird, after the simulation determines the values of $n_{\\mathrm{exec}}$ and the final $x_{\\mathrm{float}}$, we calculate the theoretical quantities.\n1.  The exact final value is computed using the high-precision `Decimal` objects: $x_{\\mathrm{exact}} = s_{\\mathrm{exact}} + n_{\\mathrm{exec}} \\cdot h_{\\mathrm{exact}}$.\n2.  The absolute error is then $e = |x_{\\mathrm{float}} - \\text{float}(x_{\\mathrm{exact}})|$.\n\nFinally, the six required output fields (`hit`, $n_{\\mathrm{min}}$, $n_{\\mathrm{exec}}$, $x_{\\mathrm{float}}$, $\\text{float}(x_{\\mathrm{exact}})$, and $e$) are collected into a list for each test case.\n\nThe specific test cases are designed to illustrate different behaviors:\n- **Cases $1$, $3$, and $4$**: These use step sizes ($h = \\pm 0.1$, $h = 0.3$) that are not exactly representable in binary. The accumulation of representation and round-off errors will cause the floating-point sequence to miss the target value $t$ exactly. Thus, we expect `hit` to be `False`.\n- **Case $2$**: Here, $s, h, t$ (`0.0`, `0.125`, `1.0`) are all exactly representable, as $h=2^{-3}$. All additions are exact. The sequence will exactly hit the target $t=1.0$ at $n=8$. Thus, we expect `hit` to be `True`.\n- **Case $5$**: This case is subtle. The start value is $s=1.0$. The unit in the last place (ULP) for $1.0$ is $2^{-52}$. The increment is $h = 2^{-55}$, which is smaller than half the ULP of $1.0$. Due to round-to-nearest-even rules in IEEE 754, the operation $1.0 + 2^{-55}$ rounds back to $1.0$. The simulated value of $x_n$ will therefore remain stuck at $1.0$ and will never reach the target $t = 1.0 + 2^{-52}$. However, the exact sum does accumulate. This demonstrates a significant truncation error where small increments are lost. We expect `hit` to be `False`, and a non-zero error $e$ equal to the sum of the lost increments.", "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Simulates a floating-point sequence and compares it with exact arithmetic.\n    \"\"\"\n    # Set a high precision for the Decimal module for \"exact\" calculations.\n    getcontext().prec = 100\n\n    def run_case_simulation(s_exact, h_exact, t_exact, M):\n        \"\"\"\n        Runs the simulation for a single test case.\n        \"\"\"\n        # Convert exact Decimal inputs to double-precision floats for simulation.\n        s_f = np.float64(s_exact)\n        h_f = np.float64(h_exact)\n        t_f = np.float64(t_exact)\n\n        x_n_float = s_f\n        hit = False\n        n_min = -1\n        \n        # Loop from n=0 to M, checking the sequence value x_n at each step.\n        for n in range(M + 1):\n            # Per problem, check for exact floating-point equality.\n            if x_n_float == t_f:\n                hit = True\n                n_min = n\n                n_exec = n\n                x_float_final = x_n_float\n                break\n            \n            # If not hit and not the last iteration, perform one increment.\n            if n  M:\n                x_n_float += h_f\n        else:  # This 'else' clause executes if the 'for' loop completes without a 'break'.\n            n_exec = M\n            # The final value is the result after M increments.\n            x_float_final = x_n_float\n\n        # Calculate the theoretical exact value and the absolute error.\n        x_exact_val = s_exact + Decimal(n_exec) * h_exact\n        error = np.abs(x_float_final - np.float64(x_exact_val))\n\n        return [hit, n_min, n_exec, x_float_final, np.float64(x_exact_val), error]\n\n    # Define test cases using Decimal for exact representation of inputs.\n    test_cases = [\n        # Test case 1: Canonical failure with decimal step.\n        (Decimal('0.0'), Decimal('0.1'), Decimal('1.0'), 12),\n        # Test case 2: Exactly representable step and reachable target.\n        (Decimal('0.0'), Decimal('0.125'), Decimal('1.0'), 8),\n        # Test case 3: Step does not subdivide the interval.\n        (Decimal('0.0'), Decimal('0.3'), Decimal('1.0'), 10),\n        # Test case 4: Negative step toward zero with decimal step.\n        (Decimal('1.0'), Decimal('-0.1'), Decimal('0.0'), 12),\n        # Test case 5: Increments below one ULP until accumulation.\n        (Decimal('1.0'), Decimal(1) / (Decimal(2)**55), Decimal(1) + Decimal(1) / (Decimal(2)**52), 8)\n    ]\n    \n    all_results = []\n    for case_params in test_cases:\n        s, h, t, M = case_params\n        result = run_case_simulation(s, h, t, M)\n        all_results.append(result)\n\n    # Format the output string to be a list of lists with no spaces.\n    # Each inner list is manually formatted to avoid spaces from default str(list).\n    result_strings = []\n    for res in all_results:\n        # Note: res[0] is a boolean, str(res[0]) is 'True' or 'False'.\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447428"}, {"introduction": "Often, there are several ways to compute the same mathematical quantity, but not all are created equal in the world of finite precision. This exercise focuses on calculating the variance of a dataset, a common task in data analysis, using two mathematically equivalent formulas [@problem_id:2447454]. You will implement both a \"one-pass\" and a \"two-pass\" algorithm and discover the dramatic failure of the former due to \"catastrophic cancellation,\" a phenomenon where subtracting nearly equal large numbers obliterates significant digits. This practice highlights the critical importance of choosing numerically stable algorithms in scientific computation.", "problem": "You are to implement a complete, runnable program that demonstrates truncation and round-off error in the computation of variance for datasets with a large mean and small deviations, comparing two computational formulas. The foundational base is the definition of variance as the second central moment of a real-valued random variable and the standard floating-point rounding model. Use the following facts as the starting point:\n- The variance of a real-valued random variable $X$ with finite second moment is defined by the second central moment: $\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big]$, where $\\mu = \\mathbb{E}[X]$.\n- For real numbers, the second raw moment satisfies $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + \\mu^2$.\n- Floating-point arithmetic in the Institute of Electrical and Electronics Engineers (IEEE) binary64 format (often called double precision) approximately obeys the rounding model $ \\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ with $|\\delta| \\le \\epsilon_{\\text{mach}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon, and $\\circ$ is an arithmetic operation. Subtracting nearly equal numbers can cause catastrophic cancellation, whereby significant digits are lost.\n\nYour program must:\n- Construct specified datasets whose elements have a large mean and small deviations.\n- Compute the variance using two methods in IEEE binary64 arithmetic:\n  1. The raw-moment one-pass form: compute $\\mathbb{E}[X]$ and $\\mathbb{E}[X^2]$ and then form $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n  2. The centered two-pass form: first compute $\\mu = \\mathbb{E}[X]$, then compute $\\mathbb{E}[(X-\\mu)^2]$ in a second pass.\n- Compute a high-precision reference variance using decimal arithmetic with sufficiently high precision so that round-off in the operations is negligible compared to IEEE binary64. Use the central-moment definition $\\mathbb{E}[(X-\\mu)^2]$ in high precision.\n- Quantify the absolute error of each floating-point method relative to the high-precision reference.\n- Produce a single line of output containing, for each dataset in the test suite and in order, a list of five values: the one-pass variance, the two-pass variance, the high-precision reference variance, the absolute error of the one-pass result, and the absolute error of the two-pass result.\n\nAll datasets are purely numeric; there are no physical units in this problem. Angles are not involved.\n\nTest suite to cover the happy path, boundary emphasis, and edge cases for catastrophic cancellation:\n- Test $1$ (symmetric small deviations around a large mean, integers to avoid input quantization): Let $M = 10^{8}$ and $D = \\{-3,-1,0,1,3\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals the average of squared deviations, which is $4$.\n- Test $2$ (non-symmetric small deviations with nonzero mean of deviations): Let $M = 10^{8}$ and $D = \\{0,1,2,3,4\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals $2$.\n- Test $3$ (larger sample with tiny, smoothly varying deviations): Let $M = 10^{8}$ and $D = \\left\\{ \\frac{k-500}{1000} \\;\\middle|\\; k=0,1,\\dots,999 \\right\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals is the population variance of this arithmetic grid; it is close to $1/12$ minus the square of the small mean of deviations and must be computed exactly by your high-precision routine.\n- Test $4$ (extreme small deviations near the limit of resolution relative to the mean): Let $M = 10^{8}$ and $D = \\{10^{-8},-10^{-8}\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals $10^{-16}$.\n\nHigh-precision reference requirement:\n- Build the reference using base-ten decimal arithmetic with at least $p = 100$ digits of precision. Construct the dataset using exact decimal values for $M$ and $D$ as specified above (e.g., use $M = 100000000$ exactly and rational deviations such as $(k-500)/1000$ as exact decimals). Compute the population variance using the two-pass central-moment definition $\\mathbb{E}[(X-\\mu)^2]$, where $\\mathbb{E}[\\cdot]$ is the arithmetic mean over the finite set.\n\nFloating-point computations requirement:\n- Use IEEE binary64 (double precision) via standard arrays in a numerical library to compute the one-pass raw-moment and two-pass centered-moment population variances. Do not apply any compensated summation or numerically stabilized tricks; use straightforward means and sums in the obvious ways so that truncation and round-off errors are visible.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-like list of four inner lists, one per test case in the order Tests $1$ through $4$. Each inner list must have the five floats: $[\\text{var\\_one\\_pass}, \\text{var\\_two\\_pass}, \\text{var\\_ref}, \\text{abs\\_err\\_one}, \\text{abs\\_err\\_two}]$. For example, a syntactically valid output line looks like $[[v_{11},v_{12},v_{13},e_{11},e_{12}],[v_{21},v_{22},v_{23},e_{21},e_{22}],\\dots]$ with numeric values filled in by your program.\n\nThere must be no user input and no external files. The program must fully determine the test data, perform the computations, and print the single required output line. The outputs must be floating-point numbers.", "solution": "The user has presented a problem in computational engineering that requires an analysis of numerical stability in variance calculations. The problem is valid, well-posed, and scientifically grounded. It addresses a fundamental issue in numerical methods: the loss of precision due to catastrophic cancellation in floating-point arithmetic.\n\nThe core task is to compare two formulas for computing the population variance of a dataset $X = \\{x_1, x_2, \\dots, x_N\\}$:\n\n1.  **The one-pass (or raw-moment) formula:** This method is derived from the algebraic identity $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Computationally, it involves a single pass through the data to compute the sum of values and the sum of squares, from which the means are calculated. The formula is:\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i\\right)^2 $$\n    While mathematically exact for real numbers, this formula is numerically unstable when the standard deviation $\\sigma$ is small compared to the mean $\\mu = \\mathbb{E}[X]$. The two terms, $\\mathbb{E}[X^2]$ and $(\\mathbb{E}[X])^2$, become very close to each other. Specifically, $\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2$, so the formula amounts to computing $\\sigma^2 = (\\sigma^2 + \\mu^2) - \\mu^2$. When evaluated using finite-precision floating-point arithmetic, such as IEEE binary64, this involves the subtraction of two very large, nearly identical numbers. This operation is a classic example of catastrophic cancellation. The leading digits of the two numbers cancel out, resulting in a loss of most or all significant digits of the small difference. The rounding errors in the computation of $\\mathbb{E}[X^2]$ and $(\\mathbb{E}[X])^2$, which are on the order of $\\mu^2 \\epsilon_{\\text{mach}}$, become the dominant part of the final result, potentially yielding a variance that is highly inaccurate, or even negative.\n\n2.  **The two-pass (or centered-moment) formula:** This method adheres more closely to the definition of variance as the mean of the squared deviations from the mean. It requires two passes over the data.\n    $$ \\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i $$\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2 $$\n    In the first pass, the mean $\\mu$ is computed. In the second pass, this computed mean is used to find the squared deviations $(x_i - \\mu)^2$, which are then averaged. This method is far more numerically robust. The subtraction $x_i - \\mu$ is still performed, but the result is a set of small numbers (the deviations). Squaring and summing these small numbers does not involve the subtraction of large quantities. The primary source of error is the initial computation of $\\mu$. The error in the computed mean, $\\hat{\\mu}$, propagates to the deviations. However, this error is typically small. For data with a large mean $M$ and small deviations, the error in the computed mean is on the order of $M\\epsilon_{\\text{mach}}$. As long as this error is small relative to the magnitude of the true deviations, the two-pass algorithm yields an accurate result.\n\nFor this problem, a high-precision reference calculation is also required. This will be performed using the Python `decimal` module, with a precision of $p=100$ digits. At this level of precision, the rounding errors are negligible compared to those in standard binary64 floating-point arithmetic, providing a \"ground truth\" against which the two other methods can be compared.\n\nThe program will be structured to process four specific test cases. Each case uses a dataset with a large mean ($M=10^8$) and small deviations, designed to expose the numerical flaws of the one-pass formula.\n\n-   **Test 1  2:** Deviations are small integers. The two-pass method is expected to be highly accurate. The one-pass method is expected to fail due to catastrophic cancellation.\n-   **Test 3:** A larger dataset with smoothly varying, small rational deviations. Similar behavior is expected.\n-   **Test 4:** Deviations are extremely small ($d = \\pm 10^{-8}$), near the limit of resolution of binary64 relative to the mean. For $x = M+d$, the value of $d$ is on the same order of magnitude as the spacing between representable floating-point numbers around $M$. When converted to float, the exact value $M+d$ is rounded to the nearest available machine number. This quantization introduces representation error before any calculation begins. While the two-pass method will compute a variance based on these rounded inputs, the one-pass method is still expected to fail catastrophically and yield a highly inaccurate result.\n\nThe implementation will proceed by defining a function that takes a dataset, computes the variance using the three methods (one-pass float, two-pass float, and high-precision reference), calculates the absolute errors for the float methods, and returns the results. This function will be called for each test case, and the collected results will be formatted into the specified output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares variance using three different methods to demonstrate\n    truncation and round-off errors.\n    \"\"\"\n    # Set the precision for decimal arithmetic to 100 digits, as required.\n    getcontext().prec = 100\n\n    def analyze_dataset(M_str: str, D_list: list):\n        \"\"\"\n        Performs the full analysis for a given dataset definition.\n\n        Args:\n            M_str: The large mean component as a string for exact Decimal conversion.\n            D_list: A list of deviation values (as Decimal objects).\n\n        Returns:\n            A list containing the five required values:\n            [var_one_pass, var_two_pass, var_ref, abs_err_one, abs_err_two]\n        \"\"\"\n        # 1. High-precision reference calculation using the decimal module.\n        # This serves as the ground truth.\n        M_dec = Decimal(M_str)\n        X_dec = [M_dec + d for d in D_list]\n        N_dec = Decimal(len(X_dec))\n        \n        # Use two-pass formula for the reference calculation.\n        mu_dec = sum(X_dec) / N_dec\n        var_ref = sum([(x - mu_dec)**2 for x in X_dec]) / N_dec\n\n        # 2. Floating-point calculations using numpy (IEEE binary64).\n        # Construct the dataset using standard float64.\n        # Note: float() conversion from Decimal can introduce small errors,\n        # but the dominant error source is the variance algorithm itself.\n        X_fp = np.array([float(x) for x in X_dec], dtype=np.float64)\n        \n        # Method 1: One-pass (raw-moment) formula. Prone to catastrophic cancellation.\n        # sigma^2 = E[X^2] - (E[X])^2\n        mean_of_squares = np.mean(X_fp**2)\n        square_of_mean = np.mean(X_fp)**2\n        var_one_pass = mean_of_squares - square_of_mean\n        \n        # Method 2: Two-pass (centered-moment) formula. More numerically stable.\n        # sigma^2 = E[(X - E[X])^2]\n        mu_fp = np.mean(X_fp)\n        var_two_pass = np.mean((X_fp - mu_fp)**2)\n        \n        # 3. Quantify absolute errors.\n        abs_err_one = abs(var_one_pass - float(var_ref))\n        abs_err_two = abs(var_two_pass - float(var_ref))\n        \n        return [var_one_pass, var_two_pass, float(var_ref), abs_err_one, abs_err_two]\n\n    # --- Define and run all test cases ---\n    test_cases_defs = [\n        # Test 1: Symmetric small integer deviations. True Var = 4.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['-3', '-1', '0', '1', '3']]},\n        \n        # Test 2: Non-symmetric small integer deviations. True Var = 2.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['0', '1', '2', '3', '4']]},\n        \n        # Test 3: Larger sample with small rational deviations.\n        {'M': '1e8', 'D': [(Decimal(k) - Decimal(500)) / Decimal(1000) for k in range(1000)]},\n        \n        # Test 4: Extremely small deviations at the limit of float64 resolution. True Var = 1e-16.\n        {'M': '1e8', 'D': [Decimal('1e-8'), Decimal('-1e-8')]}\n    ]\n\n    all_results = []\n    for case in test_cases_defs:\n        result = analyze_dataset(case['M'], case['D'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[v1,v2,...],[v1,v2,...]]\n    formatted_inner_lists = []\n    for res_list in all_results:\n        # Format each inner list as \"[v1,v2,v3,e1,e2]\"\n        formatted_list_str = f\"[{','.join(map(str, res_list))}]\"\n        formatted_inner_lists.append(formatted_list_str)\n    \n    # Join the inner lists into the final output format \"[ [...], [...], ... ]\"\n    final_output = f\"[{','.join(formatted_inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447454"}, {"introduction": "Many important functions in physics, like Legendre polynomials, are defined by recurrence relations, which seem to offer an efficient computational path. This practice investigates the stability of the standard three-term recurrence for Legendre polynomials, $P_n(x)$ [@problem_id:2435686]. By implementing the forward recurrence and comparing your results to a high-precision benchmark, you will discover that for certain values of $x$, the method becomes hopelessly inaccurate as $n$ increases. This will introduce you to the powerful concepts of \"minimal\" and \"dominant\" solutions, explaining why some recurrence algorithms are inherently unstable and must be avoided.", "problem": "You are asked to study the numerical propagation of round-off error when evaluating the Legendre polynomials $P_n(x)$ by the three-term recurrence in floating-point arithmetic. The Legendre polynomials are defined for all real $x$ by the initial conditions $P_0(x)=1$, $P_1(x)=x$, and the recurrence\n$$(n+1)P_{n+1}(x)=(2n+1)\\,x\\,P_n(x)-n\\,P_{n-1}(x),\\quad n\\ge 1.$$\nLet a program, using standard double-precision floating-point arithmetic, compute $\\widehat{P}_n(x)$ by the forward use of the recurrence starting from $P_0(x)$ and $P_1(x)$. Define the relative error at degree $n$ by\n$$E(n;x)=\\frac{\\left|\\widehat{P}_n(x)-P_n(x)\\right|}{\\max\\{1,\\left|P_n(x)\\right|\\}}.$$\nA pair $(x,n)$ is to be reported as unstable if $E(n;x)\\tau$ for a fixed tolerance $\\tau$. In this task, take $\\tau=10^{-12}$.\n\nYour program must, for each input pair $(x,N_{\\max})$, compute the smallest degree $n\\in\\{0,1,\\dots,N_{\\max}\\}$ such that $E(n;x)\\tau$. If no such $n$ exists, the program must return $N_{\\max}+1$ for that pair. All computations are to be performed in dimensionless units.\n\nUse the following test suite of input pairs $(x,N_{\\max})$:\n- $(0.0,100)$,\n- $(0.5,150)$,\n- $(0.99,200)$,\n- $(1.0,300)$,\n- $(-1.0,300)$,\n- $(1.1,50)$,\n- $(-1.2,50)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite (for example, $[r_1,r_2,\\dots,r_7]$), where each $r_i$ is the required integer for the corresponding test pair. No other output is permitted.", "solution": "The problem requires an analysis of the numerical stability of the three-term forward recurrence relation for Legendre polynomials, $P_n(x)$. This is a standard topic in numerical analysis, and its resolution rests on the mathematical theory of linear difference equations.\n\nThe governing recurrence is given by\n$$ (n+1)P_{n+1}(x)=(2n+1)xP_n(x) - nP_{n-1}(x), \\quad n \\ge 1 $$\nwith initial conditions $P_0(x)=1$ and $P_1(x)=x$. This is a second-order linear homogeneous difference equation. As such, it possesses two linearly independent solutions for a given $x$. The stability of a numerical computation of one solution depends critically on the asymptotic behavior of both solutions for large $n$.\n\nLet the two fundamental solutions be $f_n$ and $g_n$. If, for large $n$, one solution grows in magnitude while the other decays or grows more slowly (i.e., $\\lim_{n \\to \\infty} f_n/g_n = 0$), then $f_n$ is termed the *minimal* (or recessive) solution and $g_n$ is the *dominant* solution.\n- Forward recurrence is stable only for computing the dominant solution. Any round-off error, which inevitably introduces a small component of the dominant solution into the calculation, will be amplified in subsequent steps if we are attempting to compute the minimal solution. This parasitic component will eventually overwhelm the desired minimal solution, leading to a loss of numerical accuracy.\n- Conversely, backward recurrence is the stable method for computing a minimal solution.\n\nThe two fundamental solutions to the Legendre recurrence are the Legendre polynomials, $P_n(x)$, and the Legendre functions of the second kind, $Q_n(x)$. Their asymptotic behavior dictates the stability of the given forward recurrence:\n1.  For $|x|  1$: The polynomials $P_n(x)$ grow exponentially with $n$, behaving as $P_n(x) \\sim (x+\\sqrt{x^2-1})^n$. The functions $Q_n(x)$ decay to zero. Thus, $P_n(x)$ is the dominant solution. The forward recurrence is expected to be numerically stable for computing $P_n(x)$.\n2.  For $|x|  1$: The polynomials are bounded, $|P_n(x)| \\le 1$. The functions $Q_n(x)$, however, are unbounded as $n \\to \\infty$. In this regime, $P_n(x)$ is the minimal solution. Forward recurrence is therefore an unstable method for computing $P_n(x)$. We anticipate that the numerically computed sequence, $\\widehat{P}_n(x)$, will diverge from the true sequence $P_n(x)$ as $n$ increases.\n3.  For $|x| = 1$: At $x=1$, we have $P_n(1)=1$ for all $n$. At $x=-1$, $P_n(-1)=(-1)^n$. The recurrence is exactly satisfied by these constant-magnitude sequences. Analysis of the propagation of a small error $\\epsilon$ shows that the error growth is, at worst, algebraic (linear or quadratic) in $n$. Such slow growth is unlikely to cause the error to exceed the small tolerance $\\tau=10^{-12}$ for the given range of $n \\le 300$.\n4.  For $x=0$: The recurrence simplifies to $(n+1)P_{n+1}(0) = -nP_{n-1}(0)$. Starting with $P_0(0)=1$ and $P_1(0)=0$, the computation involves simple rational numbers and is expected to be stable.\n\nTo perform the required validation, the numerically computed value $\\widehat{P}_n(x)$ must be compared to the true value $P_n(x)$. The \"true\" value is obtained using a high-fidelity, stable algorithm. The `scipy.special.eval_legendre` function provides such a reference by employing methods like backward recurrence or asymptotic series, which are stable for the respective domains.\n\nThe algorithm to solve the problem for each input pair $(x, N_{\\text{max}})$ is as follows:\n- Set tolerance $\\tau = 10^{-12}$.\n- Initialize the numerically computed sequence with $\\widehat{P}_0(x)=1.0$ and $\\widehat{P}_1(x)=x$.\n- For each degree $n$ from $0$ to $N_{\\text{max}}$:\n    1. Compute $\\widehat{P}_n(x)$ using the forward recurrence. For $n \\ge 2$, this is $\\widehat{P}_n(x) = \\frac{(2n-1)x\\widehat{P}_{n-1}(x) - (n-1)\\widehat{P}_{n-2}(x)}{n}$.\n    2. Obtain the reference value $P_n(x)$ from `scipy.special.eval_legendre(n, x)`.\n    3. Calculate the error $E(n;x) = \\frac{|\\widehat{P}_n(x) - P_n(x)|}{\\max\\{1, |P_n(x)|\\}}$.\n    4. If $E(n;x)  \\tau$, then $n$ is the first degree at which instability is observed. The value $n$ is returned, and the process for this $(x, N_{\\text{max}})$ pair terminates.\n- If the loop completes without the error condition being met for any $n \\in \\{0, 1, \\dots, N_{\\text{max}}\\}$, the computation is deemed stable for the given range. In this case, $N_{\\text{max}}+1$ is returned.\n\nThis procedure is implemented in a Python program, using standard double-precision floating-point arithmetic. The analysis predicts that instability will be found for the cases where $|x|  1$ (i.e., $x=0.5$ and $x=0.99$) and not for the other cases within the specified limits.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import special\n\ndef solve():\n    \"\"\"\n    Computes the smallest degree n for which the forward recurrence of\n    Legendre polynomials becomes unstable for given (x, N_max) pairs.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 100),\n        (0.5, 150),\n        (0.99, 200),\n        (1.0, 300),\n        (-1.0, 300),\n        (1.1, 50),\n        (-1.2, 50),\n    ]\n\n    results = []\n    for x_val, N_max in test_cases:\n        result = find_unstable_n(float(x_val), N_max)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_unstable_n(x, N_max):\n    \"\"\"\n    For a given x and N_max, find the smallest n where the numerical\n    computation of P_n(x) via forward recurrence becomes unstable.\n    \"\"\"\n    tau = 1e-12\n\n    # We compute P_hat_n and compare with P_true_n for n=0, 1, ..., N_max.\n    # The true value is obtained from scipy's stable implementation.\n\n    # Case n = 0\n    P_hat_0 = 1.0\n    P_true_0 = special.eval_legendre(0, x)\n    error_0 = abs(P_hat_0 - P_true_0) / max(1.0, abs(P_true_0))\n    if error_0 > tau:\n        return 0\n    \n    if N_max == 0:\n        return N_max + 1\n\n    # Case n = 1\n    P_hat_1 = x\n    P_true_1 = special.eval_legendre(1, x)\n    error_1 = abs(P_hat_1 - P_true_1) / max(1.0, abs(P_true_1))\n    if error_1 > tau:\n        return 1\n\n    # Setup for recurrence loop\n    # P_hat_prev corresponds to P_{n-2} and P_hat_curr to P_{n-1} when computing P_n\n    P_hat_prev = P_hat_0\n    P_hat_curr = P_hat_1\n\n    # Loop for degree n = 2, 3, ..., N_max.\n    # The recurrence is (k+1)P_{k+1} = (2k+1)xP_k - kP_{k-1}.\n    # To compute P_hat_n, we use k=n-1.\n    for n in range(2, N_max + 1):\n        k = n - 1\n        \n        # Forward recurrence computation\n        P_hat_next = ((2 * k + 1) * x * P_hat_curr - k * P_hat_prev) / (k + 1)\n\n        # Get the \"true\" value for comparison\n        P_true_next = special.eval_legendre(n, x)\n\n        # Calculate the relative error\n        denominator = max(1.0, abs(P_true_next))\n        error = abs(P_hat_next - P_true_next) / denominator\n\n        # Check for instability\n        if error > tau:\n            return n\n\n        # Update values for the next iteration\n        P_hat_prev = P_hat_curr\n        P_hat_curr = P_hat_next\n\n    # If the loop completes, no instability was found up to N_max.\n    return N_max + 1\n\nsolve()\n```", "id": "2435686"}]}