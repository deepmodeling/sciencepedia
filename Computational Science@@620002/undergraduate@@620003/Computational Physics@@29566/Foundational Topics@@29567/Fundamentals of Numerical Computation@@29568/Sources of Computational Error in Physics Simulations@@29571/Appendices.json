{"hands_on_practices": [{"introduction": "In many physical problems, the quantity of interest is a small difference between two large, nearly-equal values. A classic example is the tidal acceleration, which arises from the differential gravitational pull across an extended body. This practice [@problem_id:2439869] challenges you to compute this physical effect, demonstrating how a naive subtraction of force vectors can suffer from catastrophic cancellation and yield a result dominated by numerical noise. You will then develop a more robust method based on a linear approximation, illustrating how analytical reformulation is a key strategy for creating stable numerical code.", "problem": "Consider a point-mass gravitational field generated by a star of gravitational parameter $G M$ acting on a moon orbiting a planet. Let the position of the planet relative to the star be the vector $\\mathbf{R} \\in \\mathbb{R}^3$ and the position of the moon relative to the planet be the vector $\\mathbf{r} \\in \\mathbb{R}^3$. The gravitational acceleration field at position $\\mathbf{x}$ is given by Newton’s law of universal gravitation as $\\mathbf{g}(\\mathbf{x}) = - G M \\, \\mathbf{x} / \\lVert \\mathbf{x} \\rVert^{3}$. The tidal acceleration of the moon due to the star, relative to the planet’s center, is the vector difference $\\Delta \\mathbf{g} = \\mathbf{g}(\\mathbf{R}+\\mathbf{r}) - \\mathbf{g}(\\mathbf{R})$. When $\\lVert \\mathbf{r} \\rVert \\ll \\lVert \\mathbf{R} \\rVert$, the two vectors $\\mathbf{g}(\\mathbf{R}+\\mathbf{r})$ and $\\mathbf{g}(\\mathbf{R})$ are large and nearly equal, so directly subtracting them in finite-precision arithmetic can suffer catastrophic cancellation.\n\nYour task is to write a complete program that, for a given test suite of physically plausible cases, computes and compares the relative error of two numerical strategies for evaluating $\\Delta \\mathbf{g}$:\n\n1. A naive subtraction strategy that evaluates $\\mathbf{g}(\\mathbf{R}+\\mathbf{r})$ and $\\mathbf{g}(\\mathbf{R})$ in standard double-precision arithmetic and subtracts the results componentwise.\n2. A linearized, cancellation-avoiding strategy based on the first-order Taylor expansion of $\\mathbf{g}(\\mathbf{x})$ about $\\mathbf{x}=\\mathbf{R}$, using only quantities at $\\mathbf{R}$ and the displacement $\\mathbf{r}$, without explicitly forming and subtracting the two large vectors.\n\nTo assess accuracy, treat a high-precision computation using arbitrary-precision decimal arithmetic as the reference. Define the relative error for an approximation $\\widehat{\\Delta \\mathbf{g}}$ to the exact high-precision $\\Delta \\mathbf{g}_{\\mathrm{ref}}$ by\n$$\n\\varepsilon = \n\\begin{cases}\n\\frac{\\lVert \\widehat{\\Delta \\mathbf{g}} - \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2}{\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2},  \\text{if } \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2 \\neq 0,\\\\\n0,  \\text{if } \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2 = 0 \\text{ and } \\widehat{\\Delta \\mathbf{g}}=\\mathbf{0},\\\\\n1,  \\text{if } \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2 = 0 \\text{ and } \\widehat{\\Delta \\mathbf{g}}\\neq\\mathbf{0}.\n\\end{cases}\n$$\nAlso report the magnitude $\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2$ in $\\mathrm{m/s^2}$. All computations should assume the International System of Units (SI units): distances in $\\mathrm{m}$ and accelerations in $\\mathrm{m/s^2}$. Angles, if any arise, are to be treated implicitly via vector components; no explicit angle units are needed. Report all floats as decimals rounded to $12$ significant digits.\n\nYou must implement both strategies and compare their errors against the high-precision reference on the following test suite, which uses Sun–Earth–Moon-like scales as well as extreme and boundary conditions. For all tests, use $G M = 1.32712440018\\times 10^{20}\\ \\mathrm{m^3/s^2}$.\n\n- Test A (collinear, typical scale): $\\mathbf{R} = (1.495978707\\times 10^{11}, 0, 0)\\ \\mathrm{m}$, $\\mathbf{r} = (3.844\\times 10^{8}, 0, 0)\\ \\mathrm{m}$.\n- Test B (perpendicular, typical scale): $\\mathbf{R} = (1.495978707\\times 10^{11}, 0, 0)\\ \\mathrm{m}$, $\\mathbf{r} = (0, 3.844\\times 10^{8}, 0)\\ \\mathrm{m}$.\n- Test C (extreme cancellation): $\\mathbf{R} = (1.0\\times 10^{13}, 0, 0)\\ \\mathrm{m}$, $\\mathbf{r} = (1.0, 0, 0)\\ \\mathrm{m}$.\n- Test D (boundary, zero displacement): $\\mathbf{R} = (1.495978707\\times 10^{11}, 0, 0)\\ \\mathrm{m}$, $\\mathbf{r} = (0, 0, 0)\\ \\mathrm{m}$.\n\nYour program must:\n- Compute $\\Delta \\mathbf{g}_{\\mathrm{ref}}$ using arbitrary-precision decimal arithmetic with sufficiently high precision to act as a reference for all tests in this suite, and compute its Euclidean norm $\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2$ in $\\mathrm{m/s^2}$.\n- Compute the naive subtraction approximation and its relative error $\\varepsilon_{\\mathrm{naive}}$.\n- Compute the linearized, cancellation-avoiding approximation via the first-order Taylor expansion at $\\mathbf{R}$ and its relative error $\\varepsilon_{\\mathrm{lin}}$.\n- For each test, output the three floats in the order $\\varepsilon_{\\mathrm{naive}}$, $\\varepsilon_{\\mathrm{lin}}$, $\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated flat list enclosed in square brackets, in the following fixed order that concatenates the triplets for Tests A, B, C, and D: $[\\varepsilon_{\\mathrm{naive,A}}, \\varepsilon_{\\mathrm{lin,A}}, \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref,A}} \\rVert_2, \\varepsilon_{\\mathrm{naive,B}}, \\varepsilon_{\\mathrm{lin,B}}, \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref,B}} \\rVert_2, \\varepsilon_{\\mathrm{naive,C}}, \\varepsilon_{\\mathrm{lin,C}}, \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref,C}} \\rVert_2, \\varepsilon_{\\mathrm{naive,D}}, \\varepsilon_{\\mathrm{lin,D}}, \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref,D}} \\rVert_2]$. All floats must be rounded to $12$ significant digits. No other text should be printed.", "solution": "The problem requires an analysis of numerical error in the computation of tidal gravitational acceleration. Specifically, it contrasts a direct but numerically unstable subtraction method with a mathematically reformulated, stable method based on a linear approximation. The validity of each method is assessed by comparing its results against a high-precision reference computation.\n\nThe tidal acceleration $\\Delta \\mathbf{g}$ on a moon at position $\\mathbf{r}$ relative to its planet, caused by a distant star at position $-\\mathbf{R}$ relative to the planet, is the difference in the star's gravitational pull at the moon's location and at the planet's center. Given the star's gravitational acceleration field $\\mathbf{g}(\\mathbf{x}) = - G M \\, \\mathbf{x} / \\lVert \\mathbf{x} \\rVert^{3}$, the tidal acceleration is:\n$$\n\\Delta \\mathbf{g} = \\mathbf{g}(\\mathbf{R}+\\mathbf{r}) - \\mathbf{g}(\\mathbf{R})\n$$\nThe core issue arises when the moon-planet distance is much smaller than the planet-star distance, i.e., $\\lVert \\mathbf{r} \\rVert \\ll \\lVert \\mathbf{R} \\rVert$. In this regime, the two vectors being subtracted are large and nearly equal, which is a classic scenario for catastrophic cancellation in finite-precision arithmetic.\n\nThe analysis proceeds by implementing and comparing three computational strategies.\n\n**1. High-Precision Reference Calculation ($\\Delta \\mathbf{g}_{\\mathrm{ref}}$)**\nTo establish a \"ground truth\" against which numerical errors can be measured, the exact formula for $\\Delta \\mathbf{g}$ is computed using arbitrary-precision decimal arithmetic. This approach, by employing a sufficiently high number of significant digits (e.g., $50$), effectively sidesteps the floating-point precision limits of standard hardware, yielding a result that can be treated as exact for the purposes of this problem. The calculation is performed as:\n$$\n\\Delta \\mathbf{g}_{\\mathrm{ref}} = \\left( - G M \\frac{\\mathbf{R}+\\mathbf{r}}{\\lVert \\mathbf{R}+\\mathbf{r} \\rVert^3} \\right) - \\left( - G M \\frac{\\mathbf{R}}{\\lVert \\mathbf{R} \\rVert^3} \\right)\n$$\nAll vector operations (addition, norm, scaling) are executed using high-precision data types. The Euclidean norm of this reference vector, $\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2$, is also computed in high precision and serves as the reference magnitude.\n\n**2. Naive Subtraction Strategy ($\\Delta \\mathbf{g}_{\\mathrm{naive}}$)**\nThis strategy is a direct implementation of the definition of $\\Delta \\mathbf{g}$ using standard double-precision floating-point numbers (float64). While straightforward, this method is expected to be inaccurate when $\\lVert \\mathbf{r} \\rVert / \\lVert \\mathbf{R} \\rVert$ is small. The subtraction of two nearly identical floating-point numbers results in a significant loss of relative precision, as the leading, identical digits cancel out, leaving a result dominated by rounding errors.\n\n**3. Linearized, Cancellation-Avoiding Strategy ($\\Delta \\mathbf{g}_{\\mathrm{lin}}$)**\nTo avoid catastrophic cancellation, the expression for $\\Delta \\mathbf{g}$ is reformulated. This is achieved by applying a first-order Taylor expansion to the function $\\mathbf{g}(\\mathbf{x})$ around the point $\\mathbf{x}=\\mathbf{R}$. The expansion is given by:\n$$\n\\mathbf{g}(\\mathbf{R}+\\mathbf{r}) \\approx \\mathbf{g}(\\mathbf{R}) + J_{\\mathbf{g}}(\\mathbf{R})\\mathbf{r}\n$$\nwhere $J_{\\mathbf{g}}(\\mathbf{R})$ is the Jacobian matrix of the vector field $\\mathbf{g}$ evaluated at $\\mathbf{R}$. The tidal acceleration is then approximated by:\n$$\n\\Delta \\mathbf{g} = \\mathbf{g}(\\mathbf{R}+\\mathbf{r}) - \\mathbf{g}(\\mathbf{R}) \\approx J_{\\mathbf{g}}(\\mathbf{R})\\mathbf{r} \\equiv \\Delta \\mathbf{g}_{\\mathrm{lin}}\n$$\nTo find the Jacobian matrix, we differentiate the components of $\\mathbf{g}(\\mathbf{x})$, $g_i(\\mathbf{x}) = -G M \\, x_i / \\lVert \\mathbf{x} \\rVert^3$. The partial derivative $\\partial g_i / \\partial x_j$ is:\n$$\nJ_{ij}(\\mathbf{x}) = \\frac{\\partial g_i}{\\partial x_j} = -G M \\frac{\\partial}{\\partial x_j} \\left( \\frac{x_i}{\\lVert\\mathbf{x}\\rVert^3} \\right) = -G M \\left( \\frac{\\delta_{ij} \\lVert\\mathbf{x}\\rVert^3 - x_i (3 \\lVert\\mathbf{x}\\rVert^2 \\frac{x_j}{\\lVert\\mathbf{x}\\rVert})}{\\lVert\\mathbf{x}\\rVert^6} \\right)\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. This simplifies to the tidal tensor:\n$$\nJ_{ij}(\\mathbf{x}) = \\frac{G M}{\\lVert\\mathbf{x}\\rVert^5} \\left( 3 x_i x_j - \\delta_{ij} \\lVert\\mathbf{x}\\rVert^2 \\right)\n$$\nThe linearized approximation for the tidal acceleration vector is the product of this tensor (evaluated at $\\mathbf{R}$) with the displacement vector $\\mathbf{r}$:\n$$\n(\\Delta \\mathbf{g}_{\\mathrm{lin}})_i = \\sum_{j=1}^{3} J_{ij}(\\mathbf{R}) r_j = \\frac{G M}{\\lVert\\mathbf{R}\\rVert^5} \\sum_{j=1}^{3} \\left( 3 R_i R_j - \\delta_{ij} \\lVert\\mathbf{R}\\rVert^2 \\right) r_j\n$$\nBy evaluating the sum, we obtain the expression for the $i$-th component:\n$$\n(\\Delta \\mathbf{g}_{\\mathrm{lin}})_i = \\frac{G M}{\\lVert\\mathbf{R}\\rVert^5} \\left( 3 R_i (\\mathbf{R} \\cdot \\mathbf{r}) - \\lVert\\mathbf{R}\\rVert^2 r_i \\right)\n$$\nIn vector form, the final, numerically stable formula is:\n$$\n\\Delta \\mathbf{g}_{\\mathrm{lin}} = G M \\left( \\frac{3(\\mathbf{R} \\cdot \\mathbf{r})\\mathbf{R}}{\\lVert\\mathbf{R}\\rVert^5} - \\frac{\\mathbf{r}}{\\lVert\\mathbf{R}\\rVert^3} \\right)\n$$\nThis expression is computed using standard double-precision arithmetic. It avoids direct subtraction of large vectors and is expected to be significantly more accurate than the naive method, with its error primarily determined by the neglected higher-order terms of the Taylor expansion, which are of the order $O((\\lVert \\mathbf{r} \\rVert / \\lVert \\mathbf{R} \\rVert)^2)$.\n\n**Error Evaluation**\nThe relative error $\\varepsilon$ for each approximation $\\widehat{\\Delta \\mathbf{g}}$ (either $\\Delta \\mathbf{g}_{\\mathrm{naive}}$ or $\\Delta \\mathbf{g}_{\\mathrm{lin}}$) is calculated with respect to the high-precision reference $\\Delta \\mathbf{g}_{\\mathrm{ref}}$ using the provided formula:\n$$\n\\varepsilon = \n\\begin{cases}\n\\frac{\\lVert \\widehat{\\Delta \\mathbf{g}} - \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2}{\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2},  \\text{if } \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2 \\neq 0,\\\\\n0,  \\text{if } \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2 = 0 \\text{ and } \\widehat{\\Delta \\mathbf{g}}=\\mathbf{0},\\\\\n1,  \\text{if } \\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2 = 0 \\text{ and } \\widehat{\\Delta \\mathbf{g}}\\neq\\mathbf{0}.\n\\end{cases}\n$$\nThe program systematically applies these steps to each test case, computing $\\varepsilon_{\\mathrm{naive}}$, $\\varepsilon_{\\mathrm{lin}}$, and $\\lVert \\Delta \\mathbf{g}_{\\mathrm{ref}} \\rVert_2$.", "answer": "```python\nimport numpy as np\nimport decimal\nfrom decimal import Decimal\n\ndef solve():\n    \"\"\"\n    Computes and compares the relative error of two numerical strategies\n    for evaluating tidal gravitational acceleration.\n    \"\"\"\n    # Set precision for high-precision reference calculation.\n    decimal.getcontext().prec = 60\n\n    # Gravitational parameter GM_sun in m^3/s^2.\n    GM_str = \"1.32712440018e20\"\n    GM_dec = Decimal(GM_str)\n    GM_float = float(GM_str)\n\n    # Test suite with physically plausible cases.\n    # R and r vectors are given as tuples of strings to preserve precision for Decimal.\n    test_cases = [\n        # Test A: collinear, typical scale\n        ((\"1.495978707e11\", \"0\", \"0\"), (\"3.844e8\", \"0\", \"0\")),\n        # Test B: perpendicular, typical scale\n        ((\"1.495978707e11\", \"0\", \"0\"), (\"0\", \"3.844e8\", \"0\")),\n        # Test C: extreme cancellation\n        ((\"1.0e13\", \"0\", \"0\"), (\"1.0\", \"0\", \"0\")),\n        # Test D: boundary, zero displacement\n        ((\"1.495978707e11\", \"0\", \"0\"), (\"0\", \"0\", \"0\")),\n    ]\n\n    # --- Helper Functions ---\n\n    def format_val(value):\n        \"\"\"Formats a float to 12 significant digits.\"\"\"\n        if value == 0.0:\n            return \"0.0\"\n        return f\"{value:.12g}\"\n\n    def g_func_dec(x_vec, gm):\n        \"\"\"High-precision gravitational acceleration g(x).\"\"\"\n        norm_sq = sum(c * c for c in x_vec)\n        if norm_sq == 0:\n            return [Decimal(0)] * 3\n        norm = norm_sq.sqrt()\n        norm_cubed = norm * norm * norm\n        return [-gm * c / norm_cubed for c in x_vec]\n\n    def norm_dec(vec):\n        \"\"\"High-precision Euclidean norm.\"\"\"\n        return (sum(c * c for c in vec)).sqrt()\n\n    def g_func_np(x_vec, gm):\n        \"\"\"Double-precision gravitational acceleration g(x).\"\"\"\n        norm = np.linalg.norm(x_vec)\n        if norm == 0:\n            return np.zeros(3)\n        return -gm * x_vec / (norm**3)\n\n    results = []\n    for r_star_str, r_moon_str in test_cases:\n        # --- Convert inputs to appropriate types ---\n        R_dec = [Decimal(c) for c in r_star_str]\n        r_dec = [Decimal(c) for c in r_moon_str]\n        \n        R_np = np.array([float(c) for c in r_star_str])\n        r_np = np.array([float(c) for c in r_moon_str])\n        \n        # --- 1. High-Precision Reference Calculation ---\n        R_plus_r_dec = [R_dec[i] + r_dec[i] for i in range(3)]\n        g_R_plus_r_ref = g_func_dec(R_plus_r_dec, GM_dec)\n        g_R_ref = g_func_dec(R_dec, GM_dec)\n        delta_g_ref = [g_R_plus_r_ref[i] - g_R_ref[i] for i in range(3)]\n        \n        norm_delta_g_ref = norm_dec(delta_g_ref)\n        delta_g_ref_np = np.array([float(c) for c in delta_g_ref])\n        norm_delta_g_ref_float = float(norm_delta_g_ref)\n\n        # --- 2. Naive Subtraction (Double Precision) ---\n        delta_g_naive = g_func_np(R_np + r_np, GM_float) - g_func_np(R_np, GM_float)\n\n        # --- 3. Linearized Strategy (Double Precision) ---\n        norm_R_np = np.linalg.norm(R_np)\n        if norm_R_np == 0:\n            delta_g_lin = np.zeros(3)\n        else:\n            R_dot_r = np.dot(R_np, r_np)\n            term1 = (3 * R_dot_r * R_np) / (norm_R_np**5)\n            term2 = r_np / (norm_R_np**3)\n            delta_g_lin = GM_float * (term1 - term2)\n            \n        # --- 4. Error Calculation ---\n        def calculate_relative_error(approx_vec, ref_vec, ref_norm):\n            if ref_norm != 0:\n                return np.linalg.norm(approx_vec - ref_vec) / ref_norm\n            elif np.all(approx_vec == 0):\n                return 0.0\n            else:\n                return 1.0\n\n        eps_naive = calculate_relative_error(delta_g_naive, delta_g_ref_np, norm_delta_g_ref_float)\n        eps_lin = calculate_relative_error(delta_g_lin, delta_g_ref_np, norm_delta_g_ref_float)\n\n        # Append formatted results for the current test case\n        results.append(format_val(eps_naive))\n        results.append(format_val(eps_lin))\n        results.append(format_val(norm_delta_g_ref_float))\n\n    # --- Final Output ---\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2439869"}, {"introduction": "While the previous exercise demonstrates how numerical errors can corrupt a single calculation, this problem explores their cumulative effect on the evolution of a dynamical system. An inverted pendulum, perfectly balanced in theory, provides an ideal testbed for observing how numerical noise interacts with an unstable equilibrium [@problem_id:2439859]. Your task is to simulate this system and discover how minute floating-point and integrator errors provide the inevitable \"kick\" that causes the pendulum to fall, highlighting the profound impact of computational precision on stability analysis.", "problem": "Consider a planar simple pendulum of point mass $m$ attached to a massless rigid rod of length $l$ under a uniform gravitational field of magnitude $g$. Let $\\theta(t)$ denote the angular position measured from the downward vertical, so the upright (inverted) equilibrium is at $\\theta = \\pi$. Let $\\omega(t) = d\\theta/dt$ be the angular velocity. Start from Newton’s second law for rotation, $\\sum \\tau = I\\, d^2\\theta/dt^2$, where the moment of inertia is $I = m l^2$ for a point mass at radius $l$. Include a linear viscous torque with damping coefficient $c$ opposing motion. The governing ordinary differential equation and initial conditions are therefore\n$$\nm l^2 \\,\\frac{d^2\\theta}{dt^2} + c\\,\\frac{d\\theta}{dt} + m g l\\,\\sin(\\theta) = 0,\\quad \\theta(0)=\\pi,\\quad \\omega(0)=0.\n$$\nEquivalently,\n$$\n\\frac{d\\theta}{dt} = \\omega,\\qquad\n\\frac{d\\omega}{dt} = -\\frac{c}{m l^2}\\,\\omega - \\frac{g}{l}\\,\\sin(\\theta).\n$$\nIn exact arithmetic with the above initial conditions, the solution remains at the unstable equilibrium $\\theta(t) \\equiv \\pi$ for all $t \\ge 0$. However, in floating-point computation, round-off (for example, $\\sin(\\pi)$ not being represented as exactly zero) and truncation error from time discretization can act as an effective perturbation, causing the system to deviate and eventually tip away from the upright position.\n\nYour task is to write a program that numerically integrates the system and reports the time to tip for several prescribed test cases that differ by numerical precision, time step size, and integrator. Define the tip time $t_{\\mathrm{tip}}$ as the first time $t \\ge 0$ such that the wrapped angular deviation from the nearest upright configuration satisfies\n$$\nd_{\\mathrm{upright}}(\\theta(t)) \\equiv \\min_{k \\in \\mathbb{Z}} \\left| \\theta(t) - (2k+1)\\pi \\right| \\ge \\varphi_{\\mathrm{tip}},\n$$\nwith $\\varphi_{\\mathrm{tip}}$ a fixed threshold. In code, this can be implemented by mapping $\\theta$ to its closest representative relative to $\\pi$ via the transformation\n$$\n\\phi = \\left((\\theta - \\pi) + \\pi \\bmod 2\\pi\\right) - \\pi,\n$$\nand then taking $d_{\\mathrm{upright}}(\\theta) = |\\phi|$. Use the following physical parameters: $m = 1\\,\\mathrm{kg}$, $l = 1\\,\\mathrm{m}$, $g = 9.81\\,\\mathrm{m/s}^2$, $c = 0\\,\\mathrm{kg\\,m^2/s}$. Angles must be in radians, and time must be in seconds. Use the initial conditions $\\theta(0)=\\pi$ and $\\omega(0)=0$ exactly in the chosen floating-point precision of each test case. Set the tipping threshold to $\\varphi_{\\mathrm{tip}} = 0.1$ and a finite horizon $T_{\\max} = 10.0$; if tipping does not occur before $T_{\\max}$, report exactly $T_{\\max}$ for that case.\n\nImplement three one-step explicit time integrators:\n- Forward (explicit) Euler derived from the finite-difference approximation of $\\frac{d\\mathbf{y}}{dt}$, where $\\mathbf{y} = (\\theta,\\omega)$ and step size $\\Delta t$:\n  $$\n  \\theta_{n+1} = \\theta_{n} + \\Delta t\\,\\omega_{n},\\qquad\n  \\omega_{n+1} = \\omega_{n} + \\Delta t\\left(-\\frac{c}{m l^2}\\,\\omega_{n} - \\frac{g}{l}\\,\\sin(\\theta_n)\\right).\n  $$\n- Symplectic (semi-implicit) Euler for the undamped case $c=0$:\n  $$\n  \\omega_{n+1} = \\omega_{n} + \\Delta t\\left(- \\frac{g}{l}\\,\\sin(\\theta_n)\\right),\\qquad\n  \\theta_{n+1} = \\theta_{n} + \\Delta t\\,\\omega_{n+1}.\n  $$\n- Classical fourth-order Runge–Kutta (order $4$) applied to the system $\\frac{d\\theta}{dt}=\\omega$, $\\frac{d\\omega}{dt} = -\\frac{c}{m l^2}\\,\\omega - \\frac{g}{l}\\,\\sin(\\theta)$.\n\nFor each integrator, march forward in steps of size $\\Delta t$ until either tipping is detected or the time reaches $T_{\\max}$. Use floating-point arithmetic of the specified precision throughout each simulation.\n\nTest Suite. Use the following four cases to probe different sources of computational error:\n- Case $1$: Integrator $=$ Runge–Kutta $4$, precision $=$ $64$-bit floating point, $\\Delta t = 0.001$.\n- Case $2$: Integrator $=$ Runge–Kutta $4$, precision $=$ $32$-bit floating point, $\\Delta t = 0.001$.\n- Case $3$: Integrator $=$ Forward Euler, precision $=$ $32$-bit floating point, $\\Delta t = 0.005$.\n- Case $4$: Integrator $=$ Symplectic Euler, precision $=$ $64$-bit floating point, $\\Delta t = 0.01$.\n\nAll other parameters are identical across cases: $m = 1$, $l = 1$, $g = 9.81$, $c = 0$, $\\varphi_{\\mathrm{tip}} = 0.1$, $T_{\\max} = 10.0$. Angles are in radians; time is in seconds.\n\nRequired final output. Your program should produce a single line of output containing the four tip times for the above cases, in seconds, each rounded to $6$ decimal places, aggregated into a single Python-style list with comma separation and no spaces, in the same order as the cases above. For example, an output line has the form\n$[\\text{t1},\\text{t2},\\text{t3},\\text{t4}]$,\nwhere each $\\text{t\\#}$ is a floating-point number in seconds. If a case does not tip before $T_{\\max}$, output exactly $10.000000$ for that entry.", "solution": "The problem presented is valid. It is a well-posed initial value problem in computational physics that is scientifically grounded in classical mechanics and numerical analysis. All necessary parameters, conditions, and definitions for a unique and meaningful solution are provided. The task is to investigate the effects of computational errors—specifically round-off and truncation errors—on the stability of a numerical simulation of a pendulum at an unstable equilibrium.\n\nThe governing equation for the simple pendulum is given as a second-order ordinary differential equation (ODE), which is converted into a system of two first-order ODEs for numerical integration. Let the state vector be $\\mathbf{y}(t) = [\\theta(t), \\omega(t)]^T$. The system dynamics are described by $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y})$, where:\n$$\n\\mathbf{f}(\\mathbf{y}) = \\begin{pmatrix} \\omega \\\\ -\\frac{g}{l}\\sin(\\theta) \\end{pmatrix}\n$$\nThe problem specifies the initial condition $\\mathbf{y}(0) = [\\pi, 0]^T$, which corresponds to the pendulum being perfectly balanced at its highest point. In a perfect mathematical world with exact arithmetic, the system would remain in this unstable equilibrium state indefinitely, as $\\mathbf{f}([\\pi, 0]^T) = [0, -(g/l)\\sin(\\pi)]^T = [0, 0]^T$.\n\nHowever, digital computers cannot perform exact arithmetic. The deviation from the exact solution is driven by two primary sources of computational error:\n\n$1$. **Round-off Error**: Computers represent real numbers using a finite number of bits, leading to floating-point approximations. For example, the transcendental number $\\pi$ cannot be represented exactly. When we initialize $\\theta(0) = \\pi$, we are in fact setting it to a nearby machine-representable number, $\\pi_{fp}$. Consequently, the term $\\sin(\\theta(0))$ does not evaluate to exactly zero. For IEEE $754$ double precision ($64$-bit), $\\sin(\\pi_{64}) \\approx 1.22 \\times 10^{-16}$, while for single precision ($32$-bit), $\\sin(\\pi_{32}) \\approx -8.74 \\times 10^{-8}$. This non-zero value acts as an initial perturbation, providing a non-zero angular acceleration that pushes the system away from equilibrium. The magnitude of this initial round-off error is significantly larger for lower-precision arithmetic.\n\n$2$. **Truncation Error**: Numerical integration schemes approximate the continuous evolution of the system with discrete time steps of size $\\Delta t$. This discretization introduces an error at each step, known as the local truncation error. The global error, accumulated over many steps, depends on the order of the integrator. For a first-order method like Forward or Symplectic Euler, the global error is proportional to $\\mathcal{O}(\\Delta t)$. For a fourth-order method like classical Runge-Kutta (RK$4$), the global error is much smaller, proportional to $\\mathcal{O}(\\Delta t^4)$. This error acts as a persistent perturbation at each time step.\n\nNear the unstable equilibrium $\\theta = \\pi$, let $\\theta(t) = \\pi + \\epsilon(t)$, where $\\epsilon(t)$ is a small angular deviation. Linearizing the equation of motion gives $\\sin(\\theta) = \\sin(\\pi + \\epsilon) = -\\sin(\\epsilon) \\approx -\\epsilon$. The ODE for the perturbation becomes:\n$$\n\\frac{d^2\\epsilon}{dt^2} - \\frac{g}{l}\\epsilon = 0\n$$\nThe general solution to this equation is $\\epsilon(t) = C_1 e^{\\sqrt{g/l} t} + C_2 e^{-\\sqrt{g/l} t}$. The term with the positive exponent indicates that any initial perturbation, $\\epsilon_{eff}$, will grow exponentially over time. The time to tip, $t_{\\mathrm{tip}}$, when the deviation reaches a threshold $\\varphi_{\\mathrm{tip}}$, can be estimated as:\n$$\nt_{\\mathrm{tip}} \\approx \\frac{1}{\\sqrt{g/l}} \\ln\\left(\\frac{\\varphi_{\\mathrm{tip}}}{\\epsilon_{eff}}\\right)\n$$\nThis shows that the tipping time is inversely related to the logarithm of the effective initial perturbation, $\\epsilon_{eff}$, which is an aggregate of round-off and truncation errors.\n\nThe algorithmic approach is to implement a general-purpose numerical simulation framework. The state of the pendulum is stored in a $2$-element NumPy array, $\\mathbf{y} = [\\theta, \\omega]$. The core of the algorithm is a time-stepping loop that simulates the dynamics from $t=0$ to $T_{\\max}=10.0$. For each of the four test cases, we configure the simulation with a specific integrator, floating-point precision, and time step $\\Delta t$.\n\n**Precision Control**: The floating-point precision ($32$-bit or $64$-bit) is managed by specifying the `dtype` for the state vector and all physical and numerical constants used in the calculations (e.g., $g, l, \\pi, \\Delta t$). This ensures that all arithmetic operations within the simulation for a given case are performed at the specified precision, correctly modeling the intended source of round-off error.\n\n**Integrator Implementation**: Three explicit one-step integrators are implemented as functions that take the current state $\\mathbf{y}_n$ and return the state at the next time step, $\\mathbf{y}_{n+1}$:\n- **Forward Euler**: A simple, first-order method: $\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t \\, \\mathbf{f}(\\mathbf{y}_n)$.\n- **Symplectic Euler**: A first-order method that has favorable long-term energy conservation properties for Hamiltonian systems. For our system, the update is:\n  $\\omega_{n+1} = \\omega_n + \\Delta t f_{\\omega}(\\theta_n)$, followed by $\\theta_{n+1} = \\theta_n + \\Delta t f_{\\theta}(\\omega_{n+1}) = \\theta_n + \\Delta t \\, \\omega_{n+1}$.\n- **Runge-Kutta $4$ (RK$4$)**: A fourth-order method providing higher accuracy: $\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)$, where the $\\mathbf{k}_i$ are intermediate evaluations of the derivative function $\\mathbf{f}$.\n\n**Tipping Condition**: After each time step, the angular deviation from the nearest upright equilibrium, $d_{\\mathrm{upright}}(\\theta)$, is calculated. The provided transformation $\\phi = \\left((\\theta - \\pi) + \\pi \\bmod 2\\pi\\right) - \\pi$ correctly maps the total angle $\\theta$ to the deviation from the closest equilibrium of the form $(2k+1)\\pi$. The simulation for a case stops when $|\\phi| \\ge \\varphi_{\\mathrm{tip}} = 0.1$, and the current time is reported as $t_{\\mathrm{tip}}$. If the condition is not met by $T_{\\max}$, then $T_{\\max}$ is reported.\n\nThe relative tipping times are expected to follow from the magnitude of the errors. Case $3$ (Forward Euler, $32$-bit) combines large round-off error with large truncation error, and is expected to tip fastest. Case $2$ (RK$4$, $32$-bit) has large round-off error but small truncation error, tipping slower than Case $3$. Case $4$ (Symplectic Euler, $64$-bit) has small round-off error but a larger truncation error due to its first-order nature and larger $\\Delta t$, tipping slower than the $32$-bit cases. Case $1$ (RK$4$, $64$-bit) has both small round-off and very small truncation error, and is expected to be the most stable, likely not tipping within the time horizon.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the pendulum tipping problem for the four specified test cases.\n    \"\"\"\n\n    test_cases = [\n        {'integrator': 'Runge-Kutta 4', 'precision': 64, 'dt': 0.001},\n        {'integrator': 'Runge-Kutta 4', 'precision': 32, 'dt': 0.001},\n        {'integrator': 'Forward Euler', 'precision': 32, 'dt': 0.005},\n        {'integrator': 'Symplectic Euler', 'precision': 64, 'dt': 0.01},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Set up precision for the current case\n        dtype = np.float32 if case['precision'] == 32 else np.float64\n\n        # Define numerical and physical parameters with the specified precision\n        g = dtype(9.81)\n        l = dtype(1.0)\n        phi_tip = dtype(0.1)\n        T_max = 10.0  # Loop control, does not need to be typed\n        dt = dtype(case['dt'])\n        pi = dtype(np.pi)\n        \n        g_over_l = g / l\n\n        # Initial state vector [theta, omega]\n        state = np.array([pi, dtype(0.0)], dtype=dtype)\n        \n        # --- Integrator definitions ---\n        \n        def f_ode(y_vec, g_l_const):\n            \"\"\"ODE function dy/dt = f(y).\"\"\"\n            theta, omega = y_vec[0], y_vec[1]\n            return np.array([omega, -g_l_const * np.sin(theta)], dtype=dtype)\n\n        def rk4_step(y, dt_val, g_l_const):\n            \"\"\"Performs one step of the RK4 method.\"\"\"\n            dt_half = dt_val / dtype(2.0)\n            dt_sixth = dt_val / dtype(6.0)\n            \n            k1 = f_ode(y, g_l_const)\n            k2 = f_ode(y + dt_half * k1, g_l_const)\n            k3 = f_ode(y + dt_half * k2, g_l_const)\n            k4 = f_ode(y + dt_val * k3, g_l_const)\n            \n            return y + dt_sixth * (k1 + dtype(2.0) * k2 + dtype(2.0) * k3 + k4)\n\n        def forward_euler_step(y, dt_val, g_l_const):\n            \"\"\"Performs one step of the Forward Euler method.\"\"\"\n            return y + dt_val * f_ode(y, g_l_const)\n        \n        def symplectic_euler_step(y, dt_val, g_l_const):\n            \"\"\"Performs one step of the Symplectic Euler method.\"\"\"\n            theta, omega = y[0], y[1]\n            # Update omega first using old theta\n            omega_next = omega - dt_val * g_l_const * np.sin(theta)\n            # Update theta using new omega\n            theta_next = theta + dt_val * omega_next\n            return np.array([theta_next, omega_next], dtype=dtype)\n\n        integrators = {\n            'Runge-Kutta 4': rk4_step,\n            'Forward Euler': forward_euler_step,\n            'Symplectic Euler': symplectic_euler_step,\n        }\n        integrator_func = integrators[case['integrator']]\n\n        # --- Simulation loop ---\n        \n        tip_time = T_max\n        num_steps = int(np.ceil(T_max / case['dt']))\n        \n        for i in range(num_steps):\n            state = integrator_func(state, dt, g_over_l)\n            current_time = (i + 1) * case['dt']\n            \n            # Check for tipping condition\n            theta = state[0]\n            # Calculate deviation from the nearest upright equilibrium\n            # The formula phi = ( (theta - pi) + pi ) mod 2*pi - pi can be implemented as:\n            dev_from_initial_pi = theta - pi\n            phi = np.mod(dev_from_initial_pi + pi, dtype(2.0) * pi) - pi\n            \n            if np.abs(phi) >= phi_tip:\n                tip_time = current_time\n                break\n        \n        results.append(tip_time)\n\n    # Format and print the final output\n    formatted_results = [f\"{t:.6f}\" for t in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2439859"}, {"introduction": "Sometimes, numerical instability stems not from a flaw in the algorithm itself, but from an intrinsic property of the physical system known as stiffness. Stiff systems, common in fields from chemical kinetics to circuit simulation, contain processes that evolve on vastly different time scales. This practice [@problem_id:2439902] uses a model of a gene regulatory network to explore this concept. You will investigate how the stability of a simple explicit solver is dictated by the fastest-evolving component, revealing why stiff problems demand specialized integration techniques for efficient and accurate simulation.", "problem": "Consider a three-gene inhibitory ring described by the system of ordinary differential equations\n$$\n\\frac{dx}{dt} \\;=\\; \\frac{\\alpha}{1 + z^n} \\;-\\; \\beta_x x,\\quad\n\\frac{dy}{dt} \\;=\\; \\frac{\\alpha}{1 + x^n} \\;-\\; \\beta_y y,\\quad\n\\frac{dz}{dt} \\;=\\; \\frac{\\alpha}{1 + y^n} \\;-\\; \\beta_z z,\n$$\nwhere $x$, $y$, and $z$ denote protein concentrations, $\\alpha$ is a production rate, $n$ is a Hill coefficient, and $\\beta_x$, $\\beta_y$, and $\\beta_z$ are degradation rates. All concentrations are nonnegative. The parameters $\\beta_x$, $\\beta_y$, and $\\beta_z$ are positive and measured in inverse seconds, denoted $\\mathrm{s}^{-1}$. Let a uniform discrete-time update be defined by\n$$\n\\mathbf{u}^{k+1} \\;=\\; \\mathbf{u}^k \\;+\\; h\\,\\mathbf{f}(\\mathbf{u}^k),\n$$\nwith constant time step $h$ in seconds, where $\\mathbf{u}^k = (x^k,y^k,z^k)$ and $\\mathbf{f} = (f_x,f_y,f_z)$ is the right-hand side of the system above. Focus on the effect of the linear degradation terms on the stability of this discrete-time update in the presence of disparate degradation rates.\n\nFor each parameter set, define the following three quantities:\n1) A stability flag $s$ equal to $1$ if the discrete-time update is linearly asymptotically stable for all three decoupled linear test equations $\\frac{du}{dt} = -\\beta_i u$ with $i \\in \\{x,y,z\\}$ under the same constant step $h$ (interpreting stability as asymptotic decay of perturbations), and equal to $0$ otherwise. This stability flag is unitless and must be reported as an integer.\n2) The smallest positive integer $N_{\\min}$ such that the choice of a constant step size $h' = T/N$ with any integer $N \\ge N_{\\min}$ guarantees linear asymptotic stability of the same discrete-time update over a total simulated duration $T$ in seconds. Report $N_{\\min}$ as an integer.\n3) The stiffness ratio $S = \\max(\\beta_x,\\beta_y,\\beta_z)\\big/\\min(\\beta_x,\\beta_y,\\beta_z)$, reported as a floating-point number (dimensionless).\n\nUse the following test suite, where each case specifies $(\\beta_x,\\beta_y,\\beta_z)$ in $\\mathrm{s}^{-1}$, the constant step $h$ in $\\mathrm{s}$, and the total simulated duration $T$ in $\\mathrm{s}$:\n- Case $1$: $(\\beta_x,\\beta_y,\\beta_z) = (\\,1,\\,10,\\,1000\\,)$, $h = 0.001$, $T = 1.0$.\n- Case $2$: $(\\beta_x,\\beta_y,\\beta_z) = (\\,1,\\,10,\\,1000\\,)$, $h = 0.005$, $T = 1.0$.\n- Case $3$: $(\\beta_x,\\beta_y,\\beta_z) = (\\,2,\\,2,\\,2\\,)$, $h = 1.0$, $T = 10.0$.\n- Case $4$: $(\\beta_x,\\beta_y,\\beta_z) = (\\,100,\\,10000,\\,10^6\\,)$, $h = 10^{-6}$, $T = 0.1$.\n- Case $5$: $(\\beta_x,\\beta_y,\\beta_z) = (\\,0.1,\\,0.1,\\,0.1\\,)$, $h = 8.0$, $T = 1.0$.\n\nYour program must compute, for each case, the triplet $[\\,s,\\,N_{\\min},\\,S\\,]$ in the order listed above. The final output must be a single line containing a comma-separated list of these triplets, each triplet enclosed in square brackets, and the whole list enclosed in square brackets. For example, an output with two generic cases should look like\n`[[s_1,N_min_1,S_1],[s_2,N_min_2,S_2]]`.\nAll physical quantities must be handled in the units specified above, and every $N_{\\min}$ must be an integer. Angles are not involved. Percentages are not to be used; ratios must be reported as floating-point numbers without a percentage sign.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. The problem presents a standard task in computational physics: the stability analysis of a numerical method for solving ordinary differential equations. The simplification to decoupled linear test equations is a standard technique for analyzing the role of stiff terms, making the problem self-contained and solvable without the need for the nonlinear parameters $\\alpha$ and $n$.\n\nThe core of the problem lies in determining the stability of the Forward Euler method for a simple linear ordinary differential equation. The problem directs analysis to the decoupled linear test equations $\\frac{du}{dt} = -\\beta_i u$ for each degradation rate $\\beta_i$. The stability of the full system's numerical integration is constrained by the most restrictive of these conditions.\n\nLet us consider the canonical test equation:\n$$ \\frac{du}{dt} = -\\beta u $$\nwhere $\\beta  0$ is a constant. The discrete-time update rule provided is the Forward Euler method:\n$$ u^{k+1} = u^k + hf(u^k, t^k) $$\nFor our test equation, $f(u,t) = -\\beta u$. Substituting this into the update rule gives:\n$$ u^{k+1} = u^k + h(-\\beta u^k) = (1 - h\\beta) u^k $$\nThis is a geometric recurrence relation. The solution at step $k$ is $u^k = (1 - h\\beta)^k u^0$. For the solution to be linearly asymptotically stable, which is interpreted as the asymptotic decay of perturbations, the magnitude of the amplification factor, $\\lambda = 1 - h\\beta$, must be strictly less than $1$.\n$$ |\\lambda|  1 \\implies |1 - h\\beta|  1 $$\nThis inequality is equivalent to the two-sided inequality:\n$$ -1  1 - h\\beta  1 $$\nAnalyzing the two parts:\n1.  The right side: $1 - h\\beta  1 \\implies -h\\beta  0$. Since both the time step $h$ and the degradation rate $\\beta$ are defined as positive, this condition is always satisfied.\n2.  The left side: $-1  1 - h\\beta \\implies h\\beta  2$.\n\nThus, the necessary and sufficient condition for asymptotic stability of the Forward Euler method applied to $\\frac{du}{dt} = -\\beta u$ is:\n$$ h  \\frac{2}{\\beta} \\quad \\text{or equivalently} \\quad h\\beta  2 $$\nThis condition must hold for all three decoupled linear systems, corresponding to $\\beta_x$, $\\beta_y$, and $\\beta_z$. The overall stability is determined by the most stringent constraint, which comes from the largest degradation rate. Let $\\beta_{\\max} = \\max(\\beta_x, \\beta_y, \\beta_z)$. The stability condition for the set of three equations is:\n$$ h  \\frac{2}{\\beta_{\\max}} $$\n\nWith this principal result, we can formulate the expressions for the three required quantities.\n\n1.  **Stability flag $s$**: This flag is $1$ if the given time step $h$ satisfies the stability condition for all three rates, and $0$ otherwise. This is determined by the largest rate $\\beta_{\\max}$.\n    $$ s = \\begin{cases} 1  \\text{if } h \\cdot \\beta_{\\max}  2 \\\\ 0  \\text{if } h \\cdot \\beta_{\\max} \\ge 2 \\end{cases} $$\n    The strict inequality is crucial for *asymptotic* decay. If $h\\beta_{\\max} = 2$, the amplification factor is $-1$, and perturbations oscillate with constant amplitude, which is not asymptotic stability.\n\n2.  **Minimum steps $N_{\\min}$**: This is the smallest positive integer $N$ such that a time step $h' = T/N$ guarantees stability for any integer greater than or equal to $N$. We apply the stability condition to $h'$:\n    $$ h'  \\frac{2}{\\beta_{\\max}} \\implies \\frac{T}{N}  \\frac{2}{\\beta_{\\max}} $$\n    Solving for $N$ yields:\n    $$ N  \\frac{T \\cdot \\beta_{\\max}}{2} $$\n    Since $N$ must be an integer, the smallest integer $N$ satisfying this strict inequality is $\\lfloor \\frac{T \\cdot \\beta_{\\max}}{2} \\rfloor + 1$. Therefore:\n    $$ N_{\\min} = \\left\\lfloor \\frac{T \\cdot \\beta_{\\max}}{2} \\right\\rfloor + 1 $$\n\n3.  **Stiffness ratio $S$**: This is explicitly defined in the problem statement. Letting $\\beta_{\\min} = \\min(\\beta_x, \\beta_y, \\beta_z)$, the stiffness ratio is:\n    $$ S = \\frac{\\beta_{\\max}}{\\beta_{\\min}} $$\n    This ratio quantifies the disparity in time scales of the system's components, which is a primary cause of numerical difficulty for explicit methods.\n\nWe now apply these formulas to each test case.\n\n**Case 1**: $(\\beta_x,\\beta_y,\\beta_z) = (1, 10, 1000)$, $h=0.001$, $T=1.0$.\n$\\beta_{\\max} = 1000$, $\\beta_{\\min} = 1$.\n$s$: $h \\cdot \\beta_{\\max} = 0.001 \\cdot 1000 = 1$. Since $1  2$, $s=1$.\n$N_{\\min} = \\lfloor \\frac{1.0 \\cdot 1000}{2} \\rfloor + 1 = \\lfloor 500 \\rfloor + 1 = 501$.\n$S = 1000 / 1 = 1000.0$.\nResult: $[1, 501, 1000.0]$.\n\n**Case 2**: $(\\beta_x,\\beta_y,\\beta_z) = (1, 10, 1000)$, $h=0.005$, $T=1.0$.\n$\\beta_{\\max} = 1000$, $\\beta_{\\min} = 1$.\n$s$: $h \\cdot \\beta_{\\max} = 0.005 \\cdot 1000 = 5$. Since $5 \\ge 2$, $s=0$.\n$N_{\\min} = \\lfloor \\frac{1.0 \\cdot 1000}{2} \\rfloor + 1 = 501$.\n$S = 1000 / 1 = 1000.0$.\nResult: $[0, 501, 1000.0]$.\n\n**Case 3**: $(\\beta_x,\\beta_y,\\beta_z) = (2, 2, 2)$, $h=1.0$, $T=10.0$.\n$\\beta_{\\max} = 2$, $\\beta_{\\min} = 2$.\n$s$: $h \\cdot \\beta_{\\max} = 1.0 \\cdot 2 = 2$. Since $2 \\not 2$, $s=0$.\n$N_{\\min} = \\lfloor \\frac{10.0 \\cdot 2}{2} \\rfloor + 1 = \\lfloor 10 \\rfloor + 1 = 11$.\n$S = 2 / 2 = 1.0$.\nResult: $[0, 11, 1.0]$.\n\n**Case 4**: $(\\beta_x,\\beta_y,\\beta_z) = (100, 10000, 10^6)$, $h=10^{-6}$, $T=0.1$.\n$\\beta_{\\max} = 10^6$, $\\beta_{\\min} = 100$.\n$s$: $h \\cdot \\beta_{\\max} = 10^{-6} \\cdot 10^6 = 1$. Since $1  2$, $s=1$.\n$N_{\\min} = \\lfloor \\frac{0.1 \\cdot 10^6}{2} \\rfloor + 1 = \\lfloor 50000 \\rfloor + 1 = 50001$.\n$S = 10^6 / 100 = 10000.0$.\nResult: $[1, 50001, 10000.0]$.\n\n**Case 5**: $(\\beta_x,\\beta_y,\\beta_z) = (0.1, 0.1, 0.1)$, $h=8.0$, $T=1.0$.\n$\\beta_{\\max} = 0.1$, $\\beta_{\\min} = 0.1$.\n$s$: $h \\cdot \\beta_{\\max} = 8.0 \\cdot 0.1 = 0.8$. Since $0.8  2$, $s=1$.\n$N_{\\min} = \\lfloor \\frac{1.0 \\cdot 0.1}{2} \\rfloor + 1 = \\lfloor 0.05 \\rfloor + 1 = 1$.\n$S = 0.1 / 0.1 = 1.0$.\nResult: $[1, 1, 1.0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes stability metrics for a discretized system based on its linear\n    degradation terms.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: ((beta_x, beta_y, beta_z), h, T)\n    test_cases = [\n        ((1.0, 10.0, 1000.0), 0.001, 1.0),\n        ((1.0, 10.0, 1000.0), 0.005, 1.0),\n        ((2.0, 2.0, 2.0), 1.0, 10.0),\n        ((100.0, 10000.0, 1e6), 1e-6, 0.1),\n        ((0.1, 0.1, 0.1), 8.0, 1.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        betas, h, T = case\n        \n        # Find the maximum and minimum degradation rates.\n        beta_max = np.max(betas)\n        beta_min = np.min(betas)\n\n        # 1. Stability flag 's'\n        # The Forward Euler method for du/dt = -beta*u is asymptotically stable\n        # if and only if |1 - h*beta|  1, which simplifies to h*beta  2.\n        # This must hold for the largest beta to ensure stability for all\n        # decoupled equations.\n        is_stable = h * beta_max  2\n        s = 1 if is_stable else 0\n\n        # 2. Minimum steps 'N_min'\n        # To guarantee stability, the step size h' = T/N must satisfy h' * beta_max  2.\n        # This means T/N  2/beta_max, or N  T * beta_max / 2.\n        # The smallest integer N satisfying this strict inequality is floor(C) + 1,\n        # where C = T * beta_max / 2.\n        critical_value = (T * beta_max) / 2\n        N_min = int(np.floor(critical_value) + 1)\n        \n        # 3. Stiffness ratio 'S'\n        # Defined as the ratio of the largest to the smallest degradation rate.\n        S = float(beta_max / beta_min)\n\n        # Format the triplet as a string for the final output.\n        results.append(f\"[{s},{N_min},{S}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2439902"}]}