{"hands_on_practices": [{"introduction": "While convolution can be computed directly from its summation definition, this approach is often too slow for large signals. The convolution theorem offers a much more efficient alternative by transforming the problem into the frequency domain using the Fast Fourier Transform (FFT). This practice [@problem_id:2419128] guides you through implementing this efficient method, which requires a careful understanding of how to use zero-padding to compute a linear convolution using an algorithm that naturally produces a circular one. Mastering this technique is fundamental for a wide range of applications in signal processing and computational science.", "problem": "Implement a program that computes the linear convolution of two finite-length discrete-time real sequences by using the Fast Fourier Transform (FFT). Starting from the definitions of discrete-time convolution and the Discrete Fourier Transform (DFT), you will implement linear convolution numerically by zero-padding each sequence so that circular aliasing is avoided. You must implement the convolution through the following general plan without using built-in linear convolution routines: zero-pad both sequences to a length that guarantees that the circular convolution equals the linear convolution, compute the DFTs via the FFT, multiply pointwise in the frequency domain, and then compute the inverse DFT to return to the time domain. The final result must be the linear convolution. The algorithm must be valid for arbitrary real-valued input sequences of finite length. You may assume all inputs are real and finite length. You must also quantify the numerical accuracy by comparing your FFT-based result to a direct time-domain reference computed by a reliable library routine that implements the definition of linear convolution.\n\nYour program must perform the following tasks:\n1) Implement a function that, given two one-dimensional arrays representing sequences $x[n]$ and $h[n]$ of lengths $N$ and $M$, computes their linear convolution $y[k]$ by:\n$$y[k] = \\sum_{n=-\\infty}^{\\infty} x[n]\\,h[k-n]$$\n- Zero-padding both sequences to a length $L$ that is at least $N+M-1$.\n- Computing the DFTs (using the FFT) of the zero-padded sequences.\n- Multiplying the DFTs pointwise.\n- Computing the inverse DFT and returning the first $L$ samples as the linear convolution.\nYou must not use any built-in linear convolution routine in this step. You may, however, use a direct convolution routine only for validating numerical accuracy in the test suite (see below).\n\n2) For validation, compute a reference linear convolution using a direct time-domain method and then report, for each test, the maximum absolute error between the FFT-based result and the direct result.\n\n3) Use the following test suite of input sequences. All angles are in radians. Each list denotes a finite sequence in increasing index order.\n- Test A (general small case): $x = [\\,1,\\,2,\\,3\\,]$, $h = [\\,0,\\,1,\\,0.5\\,]$.\n- Test B (impulse-like input): $x = [\\,1,\\,0,\\,0,\\,0\\,]$, $h = [\\,-1,\\,2,\\,-1\\,]$.\n- Test C (sinusoid smoothed by a short kernel): $x[n] = \\sin\\!\\big(2\\pi n/8\\big)$ for $n \\in \\{\\,0,\\,1,\\,2,\\,3,\\,4,\\,5,\\,6,\\,7\\,\\}$, and $h = [\\,0.25,\\,0.5,\\,0.25\\,]$.\n- Test D (length-one edge case): $x = [\\,2.5\\,]$, $h = [\\,-3.0,\\,4.0\\,]$.\n- Test E (alternating-sign sequence with a short decaying kernel): $x = [\\,-1.0,\\,2.0,\\,-3.0,\\,4.0,\\,-5.0\\,]$, $h = [\\,0.5,\\,-0.25,\\,0.125,\\,-0.0625\\,]$.\n\n4) For each test, compute the maximum absolute error $e = \\max_k \\big| y_{\\text{FFT}}[k] - y_{\\text{direct}}[k] \\big|$. Aggregate the errors for Tests A through E into a single list in order.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, $[r_1,r_2,r_3,r_4,r_5]$.\n- Each $r_i$ must be a decimal floating-point number representing the maximum absolute error for the corresponding test, formatted with at most $12$ significant digits.\n\nAll computations should be performed in a unitless, purely mathematical setting. The implementation must be fully deterministic and must not require any user input. The result of each test is a float. The program must be self-contained and runnable as is.", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The task is to implement the well-established numerical method of computing linear convolution using the Fast Fourier Transform (FFT), based on the convolution theorem. The problem is a standard exercise in computational physics and signal processing.\n\nThe solution proceeds by first establishing the theoretical principles connecting linear convolution, circular convolution, and the Discrete Fourier Transform (DFT). Subsequently, an algorithm is designed based on these principles, and its implementation is validated against a direct computational method for a given set of test cases.\n\nThe discrete linear convolution $y[k]$ of two finite-length, real-valued sequences, $x[n]$ of length $N$ and $h[n]$ of length $M$, is defined as:\n$$\ny[k] = (x * h)[k] = \\sum_{n=-\\infty}^{\\infty} x[n] h[k-n]\n$$\nFor causal sequences that are non-zero only for indices $n \\in [0, N-1]$ for $x[n]$ and $n \\in [0, M-1]$ for $h[n]$, this sum is non-zero over a finite range. The resulting sequence $y[k]$ has a length of $L = N+M-1$, with indices $k \\in [0, N+M-2]$.\n\nThe Discrete Fourier Transform (DFT) of a sequence $x[n]$ of length $L$ is given by:\n$$\nX[k] = \\sum_{n=0}^{L-1} x[n] e^{-i 2\\pi nk/L} \\quad \\text{for } k \\in [0, L-1]\n$$\nAnd its corresponding Inverse Discrete Fourier Transform (IDFT) is:\n$$\nx[n] = \\frac{1}{L} \\sum_{k=0}^{L-1} X[k] e^{i 2\\pi nk/L} \\quad \\text{for } n \\in [0, L-1]\n$$\nThe Fast Fourier Transform (FFT) is an efficient algorithm for computing the DFT.\n\nThe convolution theorem for the DFT states that the circular convolution of two sequences of length $L$ can be calculated by taking the IDFT of the element-wise product of their DFTs:\n$$\n(x_L \\circledast h_L)[n] = \\text{IDFT}(\\text{DFT}(x_L) \\cdot \\text{DFT}(h_L))\n$$\nwhere $\\cdot$ denotes element-wise multiplication. Circular convolution is not the same as linear convolution. However, linear convolution of sequences with lengths $N$ and $M$ can be correctly computed using circular convolution if the sequences are first zero-padded to a sufficient length.\n\nTo avoid time-domain aliasing, where the result of the circular convolution wraps around and contaminates itself, we must pad both sequences with zeros to a length $L \\ge N+M-1$. Let $x_{pad}$ and $h_{pad}$ be the sequences $x$ and $h$ padded with zeros to length $L$. The circular convolution of $x_{pad}$ and $h_{pad}$ will then be numerically identical to the linear convolution of the original sequences $x$ and $h$.\n\nThis leads to the following algorithm for computing linear convolution using the FFT:\n1.  Given input sequences $x[n]$ of length $N$ and $h[n]$ of length $M$.\n2.  Determine the minimum required length for padding: $L = N+M-1$.\n3.  Create two new sequences, $x_{pad}$ and $h_{pad}$, both of length $L$. This is done by appending $M-1$ zeros to $x[n]$ and $N-1$ zeros to $h[n]$.\n4.  Compute the DFTs of the padded sequences using the FFT algorithm:\n    $$\n    X_{pad}[k] = \\text{FFT}(x_{pad})\n    $$\n    $$\n    H_{pad}[k] = \\text{FFT}(h_{pad})\n    $$\n5.  Multiply the DFTs element-wise in the frequency domain:\n    $$\n    Y_{pad}[k] = X_{pad}[k] \\cdot H_{pad}[k]\n    $$\n6.  Compute the inverse DFT of the product using the inverse FFT (IFFT) algorithm:\n    $$\n    y_{fft}[n] = \\text{IFFT}(Y_{pad}[k])\n    $$\nThe resulting sequence $y_{fft}[n]$ is the linear convolution of $x[n]$ and $h[n]$. Since the input sequences are real-valued, the theoretical result of the convolution is also real. The imaginary part of the computed $y_{fft}[n]$ will be non-zero due to finite-precision arithmetic but should be negligible. We take the real part of the result, $\\text{Re}(y_{fft}[n])$, as the final answer.\n\nTo validate the numerical accuracy of this FFT-based method, we compare its result, $y_{FFT}$, to a reference result, $y_{direct}$, computed using a direct, time-domain convolution algorithm. The error is quantified by the maximum absolute difference over all elements of the resulting sequences:\n$$\ne = \\max_{k} | y_{FFT}[k] - y_{direct}[k] |\n$$\nThis error $e$ is calculated for each of the specified test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.fft import fft, ifft\n\ndef solve():\n    \"\"\"\n    Computes linear convolution using FFT and validates against a direct method.\n    \"\"\"\n\n    def compute_convolution_fft(x, h):\n        \"\"\"\n        Computes the linear convolution of two 1D sequences using the FFT method.\n        \n        Args:\n            x (np.ndarray): The first sequence.\n            h (np.ndarray): The second sequence (kernel).\n        \n        Returns:\n            np.ndarray: The linear convolution of x and h.\n        \"\"\"\n        N = len(x)\n        M = len(h)\n        # 1. Determine padding length L >= N + M - 1.\n        L = N + M - 1\n\n        # 2. Zero-pad both sequences to length L.\n        x_pad = np.zeros(L)\n        x_pad[:N] = x\n        \n        h_pad = np.zeros(L)\n        h_pad[:M] = h\n\n        # 3. Compute DFTs using FFT.\n        X_pad = fft(x_pad)\n        H_pad = fft(h_pad)\n\n        # 4. Multiply pointwise in the frequency domain.\n        Y_pad = X_pad * H_pad\n\n        # 5. Compute inverse DFT using IFFT.\n        y_fft = ifft(Y_pad)\n\n        # The result must be real since inputs are real.\n        # The length of the result is L.\n        return np.real(y_fft)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test A\n        (np.array([1.0, 2.0, 3.0]), np.array([0.0, 1.0, 0.5])),\n        # Test B\n        (np.array([1.0, 0.0, 0.0, 0.0]), np.array([-1.0, 2.0, -1.0])),\n        # Test C\n        (np.sin(2 * np.pi * np.arange(8) / 8.0), np.array([0.25, 0.5, 0.25])),\n        # Test D\n        (np.array([2.5]), np.array([-3.0, 4.0])),\n        # Test E\n        (np.array([-1.0, 2.0, -3.0, 4.0, -5.0]), np.array([0.5, -0.25, 0.125, -0.0625]))\n    ]\n\n    errors = []\n    for x_test, h_test in test_cases:\n        # Compute convolution using the implemented FFT method.\n        y_fft_result = compute_convolution_fft(x_test, h_test)\n        \n        # Compute reference convolution using a direct, trusted method.\n        # The 'full' mode computes the linear convolution of length N+M-1.\n        y_direct_result = np.convolve(x_test, h_test, mode='full')\n        \n        # Ensure both results have the same length for comparison.\n        L = len(x_test) + len(h_test) - 1\n        # The FFT-based result will have length L. The direct one will as well.\n        if len(y_fft_result) != L or len(y_direct_result) != L:\n            # This check is for robusteness but shouldn't fail with correct logic.\n            raise ValueError(\"Convolution result lengths do not match.\")\n\n        # Calculate the maximum absolute error.\n        max_abs_error = np.max(np.abs(y_fft_result - y_direct_result))\n        errors.append(max_abs_error)\n\n    # Format the results according to the specification.\n    # The \"{:.12g}\" format specifier ensures at most 12 significant digits.\n    formatted_errors = [f\"{err:.12g}\" for err in errors]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_errors)}]\")\n\nsolve()\n```", "id": "2419128"}, {"introduction": "In pure mathematics, convolution is a commutative operation, meaning the order of the two signals does not affect the result ($f * g = g * f$). This hands-on practice [@problem_id:2419015] challenges you to investigate whether this theoretical property holds true in the world of finite-precision computer arithmetic. By implementing convolution directly via its summation definition, you will explore how the non-associativity of floating-point addition can lead to subtle yet significant differences, a crucial lesson for any computational scientist.", "problem": "You are asked to design and implement a numerical experiment that starts from the fundamental definition of discrete convolution of two finite sequences and uses it to assess, in computation, the commutativity of convolution. Work entirely in pure mathematics and numerical analysis terms; no physical units are involved.\n\nLet two finite sequences be given by $f = \\{f_0, f_1, \\dots, f_{N-1}\\}$ and $g = \\{g_0, g_1, \\dots, g_{M-1}\\}$. The discrete linear convolution of $f$ and $g$, denoted by $(f * g)$, is defined for each integer index $n$ by\n$$(f * g)[n] \\equiv \\sum_{k=-\\infty}^{\\infty} f[k]\\, g[n - k],$$\nwith the convention that $f[k] = 0$ for $k < 0$ or $k \\ge N$ and $g[k] = 0$ for $k < 0$ or $k \\ge M$. Hence, $(f * g)[n]$ is nonzero only for $n \\in \\{0, 1, \\dots, N + M - 2\\}$.\n\nYour tasks are:\n\n1) Starting from the above definition (no transform-domain or spectral methods are allowed), implement a deterministic algorithm that computes $(f * g)$ for arbitrary finite real-valued sequences $f$ and $g$. The algorithm must explicitly perform the finite sum $\\sum_{k} f[k]\\, g[n - k]$ at each output index $n$ in a fixed, increasing order of the summation index. Do not call any built-in convolution routine or any function that implicitly uses the convolution theorem.\n\n2) Using the same algorithm, compute $(g * f)$ and quantitatively compare $(f * g)$ with $(g * f)$ under IEEE $754$ double-precision floating-point arithmetic. Define the following diagnostic for any pair of computed outputs $y_{fg}$ and $y_{gf}$:\n$$\\Delta_\\infty \\equiv \\frac{\\max_n \\left| y_{fg}[n] - y_{gf}[n] \\right|}{\\max\\left(1, \\max_n |y_{fg}[n]|, \\max_n |y_{gf}[n]|\\right)}.$$\nThis is the relative error in the infinity norm, with a floor of $1$ in the denominator to avoid division by very small numbers.\n\n3) Evaluate $\\Delta_\\infty$ for the following three carefully chosen test cases that together act as a test suite. Each sequence entry must be treated as a real number in double precision.\n\n- Test case A (exact-arithmetic regime within range): $f_A = [\\,1, 2, 3, 4\\,]$, $g_A = [\\,5, 6, 7\\,]$. For these integer values, all intermediate products and sums remain below $2^{53}$, so addition and multiplication of these integers are exactly representable in double precision. This case probes a regime where commutativity should hold to machine exactness.\n\n- Test case B (smooth, well-conditioned signals): Let $N = 64$, define the grid $x_n = -3 + 6\\, n/(N - 1)$ for $n \\in \\{0, 1, \\dots, N - 1\\}$, and set $f_B[n] = \\exp\\!\\left(-\\tfrac{1}{2} x_n^2\\right)$. Let $g_B$ be a normalized boxcar of length $9$, i.e., $g_B[m] = 1/9$ for $m \\in \\{0, 1, \\dots, 8\\}$. This case probes typical “happy path” numerical behavior.\n\n- Test case C (ill-conditioned dynamic range): Generate $f_C$ and $g_C$ independently as follows. Let $N = 256$ and $M = 257$. For $f_C$, draw $N$ independent exponents $e_k$ uniformly from $[-100, 100]$ and independent signs $s_k \\in \\{-1, +1\\}$ with equal probability, and set $f_C[k] = s_k \\cdot 10^{e_k}$. For $g_C$, draw $M$ independent exponents and signs by the same rule and set $g_C[m] = s_m \\cdot 10^{e_m}$. Use a fixed random seed so that the sequences are reproducible. This case probes floating-point non-associativity in the presence of very large and very small magnitudes.\n\n4) For each of the three test cases, compute $\\Delta_\\infty$ as defined above. Your program must produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, in scientific notation with exactly $12$ digits after the decimal point (e.g., $[1.234567890123e-04,5.000000000000e-01,3.210000000000e-16]$). The three outputs must be in the order A, B, C and must be floats.\n\nNotes:\n\n- You must implement the summations in increasing index order for each output sample $n$ so that the finite addition order is explicitly defined. Do not reorder terms by magnitude or use compensated summation.\n- You must not call any function that computes convolution for you. You may use basic array operations to access elements and loops to perform the sums.\n- Do not rely on any properties of the Fast Fourier Transform (FFT; Fast Fourier Transform), as frequency-domain methods are not permitted in this task.", "solution": "The problem requires the design and implementation of a numerical experiment to assess the commutativity of discrete convolution under finite-precision floating-point arithmetic. The analysis is to be performed starting from the fundamental definition of discrete linear convolution, without recourse to transform-domain methods such as the Fast Fourier Transform (FFT).\n\nThe discrete linear convolution of two finite sequences $f = \\{f_0, f_1, \\dots, f_{N-1}\\}$ and $g = \\{g_0, g_1, \\dots, g_{M-1}\\}$ is a sequence $h = (f * g)$ whose $n$-th term is given by\n$$h[n] = (f * g)[n] \\equiv \\sum_{k=-\\infty}^{\\infty} f[k]\\, g[n - k].$$\nGiven that the sequences $f$ and $g$ have finite support, i.e., $f[k] = 0$ for $k \\notin [0, N-1]$ and $g[m] = 0$ for $m \\notin [0, M-1]$, the summation is non-zero only over a finite range of indices $k$. For a given output index $n$, the term $f[k]g[n-k]$ is potentially non-zero only if both $k \\in [0, N-1]$ and $n-k \\in [0, M-1]$. The second condition is equivalent to $n - (M-1) \\le k \\le n$. Combining these constraints, the summation index $k$ must fall within the range\n$$ \\max(0, n - M + 1) \\le k \\le \\min(n, N-1). $$\nThe resulting convolution sequence $h$ has a length of $N + M - 1$, with indices $n$ from $0$ to $N + M - 2$.\n\nThe first task is to construct a deterministic algorithm to compute $h[n]$ for each $n$ by explicitly evaluating the sum, with the summation index $k$ strictly increasing. The algorithm proceeds as follows:\n1. Initialize a result array $h$ of length $L = N + M - 1$ with zeros. All computations will use IEEE $754$ double-precision floating-point numbers.\n2. Iterate through each output index $n$ from $0$ to $L-1$.\n3. For each $n$, compute the range of the summation index $k$ as $k_{\\text{start}} = \\max(0, n - M + 1)$ and $k_{\\text{end}} = \\min(n, N-1)$.\n4. Initialize a temporary sum variable, $S_n = 0.0$.\n5. Iterate $k$ from $k_{\\text{start}}$ to $k_{\\text{end}}$ in increasing order. In each step, compute the product $f[k] \\cdot g[n-k]$ and add it to $S_n$. This enforces the specified summation order.\n6. After the inner loop completes, assign $h[n] = S_n$.\nThis procedure directly implements the convolution definition as required.\n\nThe second task is to investigate the commutativity property, which states that $(f * g) = (g * f)$. In exact arithmetic, this property holds. However, floating-point arithmetic is not associative, meaning $(a+b)+c$ is not necessarily equal to $a+(b+c)$. Since the computation of $(f * g)$ and $(g * f)$ involves summing the same set of product terms but in a different order, their numerical results may differ.\n\nLet $y_{fg}$ be the computed result of $(f * g)$ and $y_{gf}$ be the computed result of $(g * f)$ using the algorithm described above. To quantify the discrepancy, we use the specified diagnostic, a relative error in the infinity norm:\n$$ \\Delta_\\infty \\equiv \\frac{\\max_n \\left| y_{fg}[n] - y_{gf}[n] \\right|}{\\max\\left(1, \\max_n |y_{fg}[n]|, \\max_n |y_{gf}[n]|\\right)}. $$\nThe denominator is floored at $1$ to maintain numerical stability and provide a sensible scale for the error, particularly if the convolved signals have maximal amplitudes less than $1$ or are identically zero.\n\nThe third and fourth tasks involve applying this analysis to three test cases.\n\nTest Case A: $f_A = [\\,1, 2, 3, 4\\,]$ and $g_A = [\\,5, 6, 7\\,]$. All inputs are small integers. The intermediate products (e.g., $4 \\times 7 = 28$) and the final sums (e.g., $(f_A * g_A)[3] = 52$) are also integers small enough to be represented exactly by a `float64` mantissa, which can hold all integers up to $2^{53}$. As all arithmetic operations are exact, the non-associativity of floating-point addition does not manifest. Therefore, we predict that $y_{fg}$ and $y_{gf}$ will be bit-for-bit identical, yielding $\\Delta_\\infty = 0$.\n\nTest Case B: $f_B$ is a sampled Gaussian function, a smooth and well-behaved signal. $g_B$ is a normalized boxcar function. This represents a common scenario in signal processing, such as applying a moving average filter. The components of $f_B$ are all positive and of similar magnitude. Summing these values in a different order, as happens in $(f*g)$ versus $(g*f)$, will introduce discrepancies due to floating-point rounding. The expected error should be on the order of machine epsilon for double precision, which is approximately $2.22 \\times 10^{-16}$.\n\nTest Case C: The sequences $f_C$ and $g_C$ are constructed with elements whose magnitudes are drawn from a uniform distribution on a logarithmic scale, spanning many orders of magnitude (from roughly $10^{-100}$ to $10^{100}$). This creates an ill-conditioned problem for summation. When summing numbers of vastly different magnitudes, catastrophic cancellation and absorption errors are prominent. For example, if $A$ is very large and $b$ is very small, $A+b$ may evaluate to $A$. The order of summation becomes critical. Since $(f * g)$ and $(g * f)$ will sum the same products but in different orders, the final results are expected to differ significantly. A substantial value for $\\Delta_\\infty$ is anticipated. To ensure reproducibility, a fixed seed must be used for the random number generator. We select a seed of $0$.\n\nThe implementation will strictly follow these principles, generating the sequences, performing the convolutions via the direct summation algorithm, and computing $\\Delta_\\infty$ for each case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical convolution problem by direct implementation and assesses commutativity.\n    \"\"\"\n\n    def convolve_direct(f: np.ndarray, g: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the discrete linear convolution of two 1D sequences f and g\n        using direct summation as per the definition.\n\n        Args:\n            f: The first sequence, a 1D NumPy array of float64.\n            g: The second sequence, a 1D NumPy array of float64.\n\n        Returns:\n            The convolved sequence, a 1D NumPy array of float64.\n        \"\"\"\n        N = len(f)\n        M = len(g)\n        if N == 0 or M == 0:\n            return np.array([], dtype=np.float64)\n            \n        L = N + M - 1\n        h = np.zeros(L, dtype=np.float64)\n\n        for n in range(L):\n            s = 0.0\n            k_start = max(0, n - M + 1)\n            k_end = min(n, N - 1)\n            for k in range(k_start, k_end + 1):\n                s += f[k] * g[n - k]\n            h[n] = s\n        return h\n\n    def calculate_delta_inf(y_fg: np.ndarray, y_gf: np.ndarray) -> float:\n        \"\"\"\n        Calculates the relative error in the infinity norm between two sequences.\n\n        Args:\n            y_fg: The result of (f * g).\n            y_gf: The result of (g * f).\n\n        Returns:\n            The diagnostic value Delta_infinity.\n        \"\"\"\n        if y_fg.size == 0 and y_gf.size == 0:\n            return 0.0\n\n        max_abs_diff = np.max(np.abs(y_fg - y_gf))\n        \n        norm_fg = np.max(np.abs(y_fg)) if y_fg.size > 0 else 0.0\n        norm_gf = np.max(np.abs(y_gf)) if y_gf.size > 0 else 0.0\n\n        denominator = max(1.0, norm_fg, norm_gf)\n        \n        return max_abs_diff / denominator\n\n    results = []\n\n    # Test Case A: Exact integer arithmetic regime\n    f_A = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float64)\n    g_A = np.array([5.0, 6.0, 7.0], dtype=np.float64)\n    \n    y_fg_A = convolve_direct(f_A, g_A)\n    y_gf_A = convolve_direct(g_A, f_A)\n    delta_A = calculate_delta_inf(y_fg_A, y_gf_A)\n    results.append(delta_A)\n\n    # Test Case B: Smooth, well-conditioned signals\n    N_B = 64\n    x_n = -3.0 + 6.0 * np.arange(N_B) / (N_B - 1)\n    f_B = np.exp(-0.5 * x_n**2, dtype=np.float64)\n    g_B = np.full(9, 1.0/9.0, dtype=np.float64)\n    \n    y_fg_B = convolve_direct(f_B, g_B)\n    y_gf_B = convolve_direct(g_B, f_B)\n    delta_B = calculate_delta_inf(y_fg_B, y_gf_B)\n    results.append(delta_B)\n\n    # Test Case C: Ill-conditioned dynamic range\n    np.random.seed(0) # For reproducibility\n    N_C = 256\n    M_C = 257\n    \n    # Generate f_C\n    exponents_f = np.random.uniform(-100, 100, N_C)\n    signs_f = np.random.choice([-1.0, 1.0], N_C)\n    f_C = signs_f * (10.0**exponents_f)\n    f_C = f_C.astype(np.float64)\n\n    # Generate g_C\n    exponents_g = np.random.uniform(-100, 100, M_C)\n    signs_g = np.random.choice([-1.0, 1.0], M_C)\n    g_C = signs_g * (10.0**exponents_g)\n    g_C = g_C.astype(np.float64)\n\n    y_fg_C = convolve_direct(f_C, g_C)\n    y_gf_C = convolve_direct(g_C, f_C)\n    delta_C = calculate_delta_inf(y_fg_C, y_gf_C)\n    results.append(delta_C)\n\n    # Format and print the final output\n    output_str = \",\".join([f\"{r:.12e}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2419015"}, {"introduction": "After exploring the forward problem of convolution, we now tackle the more challenging inverse problem: deconvolution. The goal is to recover an original signal that has been blurred or filtered, a common task in image restoration and data analysis. This practice [@problem_id:2419120] demonstrates how a naive inversion is unstable in the presence of noise and introduces Tikhonov regularization as a powerful method for obtaining a stable and physically meaningful solution. You will implement this technique and learn the critical skill of selecting an optimal regularization parameter to balance noise suppression and signal fidelity.", "problem": "You are asked to implement a complete, deterministic program that performs stable deconvolution of one-dimensional signals corrupted by additive white noise using Tikhonov regularization. The implementation must proceed from the fundamental definitions of discrete convolution and the Discrete Fourier Transform (DFT), and apply the convolution theorem to derive an algorithm. Your program must evaluate a fixed test suite and output a single formatted line with numerical results.\n\nThe fundamental base for this task is:\n- The discrete circular convolution of signals of length $N$ defined by $$(h \\circledast x)[n] = \\sum_{m=0}^{N-1} h[m]\\, x[(n-m) \\bmod N].$$\n- The Discrete Fourier Transform (DFT) and its inverse,\n$$X[k] = \\sum_{n=0}^{N-1} x[n]\\, e^{-i 2\\pi kn/N}, \\quad x[n] = \\frac{1}{N}\\sum_{k=0}^{N-1} X[k]\\, e^{i 2\\pi kn/N}.$$\n- The convolution theorem, which states that for circular convolution, $$\\mathcal{F}\\{h \\circledast x\\}[k] = H[k]\\, X[k],$$ where $\\mathcal{F}$ denotes the DFT, and $H[k], X[k]$ are the DFTs of $h[n]$ and $x[n]$.\n- The Tikhonov-regularized least-squares objective for deconvolution (zero-order, with the identity as the regularization operator) given by\n$$J(x) = \\lVert h \\circledast x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2,$$\nwhere $y[n]$ is the observed noisy, blurred signal, $\\lambda \\ge 0$ is the regularization parameter, and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n\nYour implementation must:\n1. Use circular convolution of length $N$ for both forward modeling (to generate the observed signal) and for deconvolution.\n2. Take the DFT-based pointwise solution for the Tikhonov-regularized normal equations to compute the deconvolution estimate $\\hat{x}[n]$:\n   - Work in the DFT domain. For each DFT bin $k$, compute\n     $$\\hat{X}[k] = \\frac{H^*[k]}{|H[k]|^2 + \\lambda}\\, Y[k],$$\n     where $H^*[k]$ is the complex conjugate of $H[k]$, $|H[k]|^2 = H^*[k] H[k]$, and $Y[k]$ is the DFT of $y[n]$. Then apply the inverse DFT to obtain $\\hat{x}[n]$.\n   - For the special case $\\lambda = 0$, handle numerical stability by replacing the denominator $|H[k]|^2$ with $|H[k]|^2 + \\varepsilon$ for a fixed $\\varepsilon = 10^{-12}$ to avoid division by zero.\n3. Generate additive white Gaussian noise with zero mean and specified standard deviation in the time domain. Use a fixed random seed for reproducibility.\n4. Evaluate a fixed list of candidate regularization parameters $\\lambda \\in \\{0, 10^{-8}, 10^{-6}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$ for each test case and select the $\\lambda$ value that minimizes the root-mean-square error (RMSE) between the deconvolved estimate and the known ground truth signal. The RMSE is defined as\n   $$\\mathrm{RMSE}(\\hat{x}, x) = \\sqrt{\\frac{1}{N}\\sum_{n=0}^{N-1} \\left(\\hat{x}[n] - x[n]\\right)^2}.$$\n5. For each test case, also compute the RMSE of the naive inverse filter corresponding to $\\lambda = 0$ with the $\\varepsilon$ safeguard as specified above.\n\nAngle unit requirement: All trigonometric functions must take angles in radians. No physical units are involved in this problem.\n\nTest suite specification:\n- Shared parameters:\n  - Signal length $N = 512$.\n  - Use circular convolution of length $N$ everywhere.\n  - Random number generator seed must be set to $12345$ once at the start.\n  - Candidate regularization parameters: the seven values listed in item 4 above.\n  - For all kernels, represent them as length-$N$ sequences by placing their nonzero samples at the start of the array and zero-padding to length $N$.\n\n- Test case 1 (moderate Gaussian blur, composite smooth signal):\n  - True signal $x_1[n]$ for $n = 0,1,\\dots,N-1$:\n    $$x_1[n] = \\exp\\!\\left(-\\frac{(n-100)^2}{2\\cdot 8^2}\\right) + 0.6\\, \\exp\\!\\left(-\\frac{(n-300)^2}{2\\cdot 12^2}\\right) + 0.1\\, \\sin\\!\\left(2\\pi \\cdot 0.05 \\cdot n\\right).$$\n    The sine argument is in radians.\n  - Blur kernel $h_1[n]$ is a discrete Gaussian with standard deviation $\\sigma_k = 2.0$ samples, sampled at integer indices and normalized to unit sum. Take the symmetric taps over $m \\in \\{-\\lceil 6\\sigma_k \\rceil,\\dots,\\lceil 6\\sigma_k \\rceil\\}$, i.e., include sufficient support to approximate the Gaussian, then place these taps at the start of the length-$N$ array in increasing $m$ order and zero-pad the rest.\n  - Additive white Gaussian noise standard deviation: $\\sigma_{\\text{noise}} = 0.02$.\n  - Observed data: $y_1 = h_1 \\circledast x_1 + \\eta$, where $\\eta$ is the noise sequence.\n\n- Test case 2 (uniform motion blur, step-like signal):\n  - True signal $x_2[n]$:\n    $$x_2[n] = \\begin{cases}1, & 120 \\le n < 220\\\\ 0, & \\text{otherwise}\\end{cases} + 0.1\\, \\sin\\!\\left(2\\pi \\cdot 0.03 \\cdot n\\right).$$\n  - Blur kernel $h_2[n]$ is a uniform boxcar of length $L = 21$ samples with values $1/L$ on its support and zero elsewhere, then zero-padded to length $N$ as above.\n  - Additive white Gaussian noise standard deviation: $\\sigma_{\\text{noise}} = 0.03$.\n  - Observed data: $y_2 = h_2 \\circledast x_2 + \\eta$.\n\n- Test case 3 (identity kernel boundary case, sparse impulses with background):\n  - True signal $x_3[n]$:\n    $$x_3[n] = 1.0\\cdot \\delta[n-50] + 0.7\\cdot \\delta[n-200] + 1.2\\cdot \\delta[n-350] + 0.05\\, \\exp\\!\\left(-\\frac{(n-256)^2}{2\\cdot 20^2}\\right),$$\n    where $\\delta[\\cdot]$ is the Kronecker delta on the integer index.\n  - Blur kernel $h_3[n]$ is the identity (discrete delta) with $h_3[0]=1$ and $h_3[n]=0$ for $n\\neq 0$.\n  - Additive white Gaussian noise standard deviation: $\\sigma_{\\text{noise}} = 0.02$.\n  - Observed data: $y_3 = h_3 \\circledast x_3 + \\eta$.\n\nRequired outputs:\n- For each test case $j \\in \\{1,2,3\\}$, compute:\n  1. The value $\\lambda^{\\star}_j$ in the candidate set that minimizes the RMSE between the Tikhonov deconvolution estimate and the true signal.\n  2. The minimal RMSE $\\mathrm{RMSE}_{\\min, j}$ achieved at $\\lambda^{\\star}_j$.\n  3. The RMSE of the naive inverse filter $\\mathrm{RMSE}_{\\text{naive}, j}$ defined by the $\\lambda=0$ case with the $\\varepsilon$ safeguard.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n  $$[\\lambda^{\\star}_1, \\mathrm{RMSE}_{\\min,1}, \\mathrm{RMSE}_{\\text{naive},1}, \\lambda^{\\star}_2, \\mathrm{RMSE}_{\\min,2}, \\mathrm{RMSE}_{\\text{naive},2}, \\lambda^{\\star}_3, \\mathrm{RMSE}_{\\min,3}, \\mathrm{RMSE}_{\\text{naive},3}],$$\n  where all entries are printed as floating-point numbers. No additional text should be printed.", "solution": "The problem statement has been critically examined and is determined to be valid. It is a well-posed, scientifically sound, and self-contained problem in computational physics, specifically in the domain of signal processing and inverse problems. All constants, functions, and procedures are defined with sufficient clarity and precision to permit a unique, deterministic solution. We shall therefore proceed with the derivation and implementation of the required algorithm.\n\nThe core of the problem is to recover an original signal $x[n]$ from a degraded observation $y[n]$, which is modeled as the circular convolution of $x[n]$ with a known blur kernel $h[n]$, further corrupted by additive white Gaussian noise $\\eta[n]$. The model is thus $y = h \\circledast x + \\eta$. A naive attempt to solve for $x$ by inverting the convolution in the frequency domain, i.e., $\\hat{X}[k] = Y[k]/H[k]$, is notoriously unstable because the blur kernel's Discrete Fourier Transform (DFT), $H[k]$, often has values near zero. This amplifies noise in the corresponding frequency components of $Y[k]$.\n\nTo address this instability, we employ Tikhonov regularization. The goal is to find a signal estimate $\\hat{x}$ that minimizes the regularized least-squares objective function:\n$$\nJ(x) = \\lVert h \\circledast x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2\n$$\nHere, $\\lambda \\ge 0$ is the regularization parameter, which controls the trade-off between fidelity to the data (the first term) and the smoothness or size of the solution (the second term). By leveraging the convolution theorem and Parseval's theorem, this minimization can be efficiently performed in the frequency domain. The objective function becomes a sum of independent minimization problems for each frequency component $X[k]$ of the DFT of $x[n]$:\n$$\nJ(X) = \\frac{1}{N} \\sum_{k=0}^{N-1} \\left( |H[k]X[k] - Y[k]|^2 + \\lambda|X[k]|^2 \\right)\n$$\nSolving for the optimal $X[k]$ by setting the gradient with respect to $X[k]$ to zero yields the Tikhonov-regularized deconvolution filter:\n$$\n\\hat{X}[k] = \\frac{H^*[k] Y[k]}{|H[k]|^2 + \\lambda}\n$$\nwhere $H^*[k]$ is the complex conjugate of $H[k]$. The estimated signal in the time domain, $\\hat{x}[n]$, is then recovered by applying the inverse DFT to $\\hat{X}[k]$.\n\nThe implementation will proceed as follows:\n\n1.  **System Setup**: We define shared parameters: signal length $N=512$, a list of candidate regularization parameters $\\lambda \\in \\{0, 10^{-8}, 10^{-6}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$, and a small constant $\\varepsilon = 10^{-12}$ for numerical stability in the $\\lambda=0$ case. A random number generator is seeded with $12345$ for reproducibility.\n\n2.  **Test Case Generation**: For each of the three test cases, we will programmatically generate the ground truth signal $x[n]$ and the blur kernel $h[n]$ as arrays of length $N$ according to their specified mathematical definitions.\n    - For the Gaussian kernel $h_1[n]$, we first compute the unnormalized taps for integer indices $m$ over the range $[-\\lceil 6\\sigma_k \\rceil, \\lceil 6\\sigma_k \\rceil]$, where $\\sigma_k = 2.0$. These taps are then normalized to have a unit sum. The resulting sequence of taps is placed at the beginning of the length-$N$ kernel array.\n    - For the boxcar kernel $h_2[n]$, an array of length $L=21$ is filled with the value $1/21$ and placed at the beginning of the length-$N$ kernel array.\n    - The identity kernel $h_3[n]$ is a discrete delta function, with $h_3[0]=1$ and all other elements zero.\n\n3.  **Forward Model Simulation**: The observed signal $y[n]$ is simulated for each test case.\n    - The circular convolution $h \\circledast x$ is computed efficiently using the DFT via the convolution theorem: $h \\circledast x = \\mathcal{F}^{-1}\\{\\mathcal{F}\\{h\\} \\cdot \\mathcal{F}\\{x\\}\\}$.\n    - An additive white Gaussian noise sequence $\\eta[n]$ is generated with zero mean and the specified standard deviation $\\sigma_{\\text{noise}}$.\n    - The final observed signal is $y[n] = (h \\circledast x)[n] + \\eta[n]$.\n\n4.  **Deconvolution and Parameter Selection**: For each test case, we perform the following procedure:\n    - We compute the DFTs of the observed signal $y[n]$ and the kernel $h[n]$, yielding $Y[k]$ and $H[k]$.\n    - We iterate through each candidate value of $\\lambda$. For each $\\lambda$, we compute the estimated signal's DFT, $\\hat{X}[k]$, using the Tikhonov filter formula. For $\\lambda=0$, the denominator $|H[k]|^2$ is replaced by $|H[k]|^2 + \\varepsilon$ as specified.\n    - The estimated signal $\\hat{x}[n]$ is obtained via the inverse DFT of $\\hat{X}[k]$. We take the real part of the result to discard negligible imaginary components arising from numerical precision errors.\n    - The Root-Mean-Square Error (RMSE) between the estimate $\\hat{x}[n]$ and the true signal $x[n]$ is calculated as:\n      $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=0}^{N-1} (\\hat{x}[n] - x[n])^2}$$\n    - The $\\lambda$ value that results in the minimum RMSE is identified as the optimal parameter, $\\lambda^\\star$. The corresponding minimum RMSE, $\\mathrm{RMSE}_{\\min}$, is recorded. Additionally, the RMSE for the naive inverse filter ($\\lambda=0$ case), $\\mathrm{RMSE}_{\\text{naive}}$, is stored.\n\n5.  **Output Formatting**: The nine resulting numerical values ($\\lambda^\\star_j, \\mathrm{RMSE}_{\\min,j}, \\mathrm{RMSE}_{\\text{naive},j}$ for $j=1,2,3$) are collected and formatted into a single-line string as required.\n\nThis structured approach ensures that the solution is a direct and correct implementation of the principles and specifications outlined in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the deconvolution test suite.\n    \"\"\"\n    # Shared parameters\n    N = 512\n    lambda_candidates = [0.0, 1e-8, 1e-6, 1e-4, 1e-3, 1e-2, 1e-1]\n    epsilon = 1e-12\n    rng = np.random.default_rng(12345)\n\n    # --- Test Case 1 ---\n    # True signal x1\n    n = np.arange(N)\n    x1 = (np.exp(-(n - 100)**2 / (2 * 8**2)) +\n          0.6 * np.exp(-(n - 300)**2 / (2 * 12**2)) +\n          0.1 * np.sin(2 * np.pi * 0.05 * n))\n\n    # Blur kernel h1\n    sigma_k = 2.0\n    support_half_width = int(np.ceil(6 * sigma_k))\n    m = np.arange(-support_half_width, support_half_width + 1)\n    h1_taps = np.exp(-m**2 / (2 * sigma_k**2))\n    h1_taps /= np.sum(h1_taps)\n    h1 = np.zeros(N)\n    h1[0:len(h1_taps)] = h1_taps\n    \n    # Noise and observation y1\n    noise_std1 = 0.02\n    \n    # --- Test Case 2 ---\n    # True signal x2\n    x2 = np.zeros(N)\n    x2[120:220] = 1.0\n    x2 += 0.1 * np.sin(2 * np.pi * 0.03 * n)\n\n    # Blur kernel h2\n    L = 21\n    h2 = np.zeros(N)\n    h2[0:L] = 1.0 / L\n\n    # Noise and observation y2\n    noise_std2 = 0.03\n\n    # --- Test Case 3 ---\n    # True signal x3\n    x3 = np.zeros(N)\n    x3[50] = 1.0\n    x3[200] = 0.7\n    x3[350] = 1.2\n    x3 += 0.05 * np.exp(-(n - 256)**2 / (2 * 20**2))\n    \n    # Blur kernel h3\n    h3 = np.zeros(N)\n    h3[0] = 1.0\n    \n    # Noise and observation y3\n    noise_std3 = 0.02\n\n    test_cases = [\n        (x1, h1, noise_std1),\n        (x2, h2, noise_std2),\n        (x3, h3, noise_std3),\n    ]\n\n    results = []\n    for x_true, h, noise_std in test_cases:\n        # Generate observed signal y = h * x + noise\n        noise = rng.normal(loc=0.0, scale=noise_std, size=N)\n        y = np.real(np.fft.ifft(np.fft.fft(h) * np.fft.fft(x_true))) + noise\n\n        # Pre-compute DFTs\n        Y = np.fft.fft(y)\n        H = np.fft.fft(h)\n        H_conj = np.conj(H)\n        H_mag_sq = np.abs(H)**2\n\n        min_rmse = float('inf')\n        best_lambda = -1.0\n        naive_rmse = -1.0\n\n        for lam in lambda_candidates:\n            # Apply Tikhonov filter in frequency domain\n            if lam == 0:\n                denominator = H_mag_sq + epsilon\n            else:\n                denominator = H_mag_sq + lam\n            \n            X_hat = (H_conj * Y) / denominator\n            \n            # Inverse FFT to get signal estimate\n            x_hat = np.real(np.fft.ifft(X_hat))\n            \n            # Calculate RMSE\n            current_rmse = np.sqrt(np.mean((x_hat - x_true)**2))\n            \n            if lam == 0:\n                naive_rmse = current_rmse\n            \n            if current_rmse < min_rmse:\n                min_rmse = current_rmse\n                best_lambda = lam\n        \n        results.extend([best_lambda, min_rmse, naive_rmse])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2419120"}]}