{"hands_on_practices": [{"introduction": "A fundamental test of any numerical integration scheme is to verify its order of convergence, which tells us how quickly the global error decreases as we refine the step size $h$. This practice will guide you through the process of numerically calculating the convergence order for a simple predictor-corrector method, confirming that it behaves as theoretically expected. By applying the method to an ODE with a known singularity [@problem_id:2429754], you will also gain insight into how a method's performance can be affected when the solution is not perfectly smooth.", "problem": "Consider the initial-value problem for the autonomous ordinary differential equation (ODE) $y' = y^2$ with initial condition $y(0) = 1$. The unique analytical solution is $y(t) = \\dfrac{1}{1 - t}$ for $t \\in [0,1)$, which exhibits a finite-time singularity at $t = 1$. Let $f(t,y) = y^2$. Consider the following two-stage predictorâ€“corrector time-stepping scheme applied with a constant step size $h > 0$ on a uniform grid $t_n = t_0 + n h$ for $n = 0,1,\\dots,N$, where $N h = t_{\\text{end}} - t_0$ and $t_0 = 0$:\n1. Predictor (explicit Euler): $y^{\\text{pred}}_{n+1} = y_n + h\\, f(t_n, y_n)$.\n2. Corrector (trapezoidal update using the predictor): $y_{n+1} = y_n + \\dfrac{h}{2}\\,\\left[f(t_n, y_n) + f(t_{n+1}, y^{\\text{pred}}_{n+1})\\right]$.\nFor each test case below, you must:\n- Integrate from $t = 0$ to $t = t_{\\text{end}}$ using the above scheme and the specified list of step sizes $h$ (each chosen so that $t_{\\text{end}} / h$ is a positive integer).\n- For each $h$ in the list, compute the absolute global error at $t = t_{\\text{end}}$, defined as $E(h) = \\left|y_{\\text{num}}(t_{\\text{end}};h) - \\dfrac{1}{1 - t_{\\text{end}}}\\right|$, where $y_{\\text{num}}(t_{\\text{end}};h)$ is the numerical approximation produced by the scheme with step size $h$.\n- Using all $(h, E(h))$ pairs within a test case, estimate the observed convergence order $p$ as the slope of the least-squares linear fit to the points $\\left(\\log(h), \\log(E(h))\\right)$, that is, fit $\\log(E(h)) \\approx C + p \\log(h)$ and report $p$ for the test case.\n- Round each reported $p$ to three decimals and report dimensionless values.\n\nTest suite:\n- Case A: $t_{\\text{end}} = 0.1$ with $h \\in \\{0.02, 0.01, 0.005, 0.0025\\}$.\n- Case B: $t_{\\text{end}} = 0.5$ with $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n- Case C: $t_{\\text{end}} = 0.9$ with $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n- Case D: $t_{\\text{end}} = 0.95$ with $h \\in \\{0.05, 0.025, 0.0125\\}$.\n\nYour program should produce a single line of output containing the four rounded estimates $[p_A, p_B, p_C, p_D]$ as a comma-separated list enclosed in square brackets, for example, $[2.000,2.000,1.950,1.700]$. No additional text should be printed. All angles, units, and quantities are dimensionless; report the final four values as decimals rounded to three digits after the decimal point.", "solution": "The supplied problem is a well-defined exercise in computational physics concerning the numerical solution of an ordinary differential equation (ODE). It is scientifically grounded, logically consistent, and contains all necessary information for a unique solution. Therefore, the problem is valid, and we proceed with its resolution.\n\nThe problem requires the analysis of a numerical method applied to the initial value problem (IVP) given by the autonomous ODE:\n$$\ny'(t) = y(t)^2, \\quad y(0) = 1\n$$\nThis equation is a specific case of a Riccati equation. The analytical solution can be found by separation of variables:\n$$\n\\frac{dy}{y^2} = dt \\implies \\int \\frac{dy}{y^2} = \\int dt \\implies -\\frac{1}{y} = t + C\n$$\nUsing the initial condition $y(0)=1$, we find the constant of integration $C$:\n$$\n-\\frac{1}{1} = 0 + C \\implies C = -1\n$$\nThus, the unique analytical solution is:\n$$\ny(t) = \\frac{1}{1 - t}\n$$\nThis solution exists for $t \\in [0, 1)$ and exhibits a finite-time singularity at $t=1$. The function to be integrated, as defined in the problem, is $f(t,y) = y^2$. Since the ODE is autonomous, the function $f$ does not explicitly depend on $t$.\n\nThe numerical scheme to be used is the following two-stage predictor-corrector method:\n1.  **Predictor (Explicit Euler):** A predicted value $y^{\\text{pred}}_{n+1}$ at time $t_{n+1} = t_n + h$ is computed using the forward Euler method:\n    $$\n    y^{\\text{pred}}_{n+1} = y_n + h\\, f(t_n, y_n) = y_n + h\\, y_n^2\n    $$\n2.  **Corrector (Trapezoidal Update):** The final value $y_{n+1}$ is computed by averaging the slope at the beginning of the interval, $f(t_n, y_n)$, and the slope at the end of the interval, where the function is evaluated at the predicted value, $f(t_{n+1}, y^{\\text{pred}}_{n+1})$:\n    $$\n    y_{n+1} = y_n + \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y^{\\text{pred}}_{n+1}) \\right] = y_n + \\frac{h}{2} \\left[ y_n^2 + (y^{\\text{pred}}_{n+1})^2 \\right]\n    $$\nThis scheme is widely known as Heun's method or the improved Euler method. It belongs to the family of second-order Runge-Kutta methods. For a sufficiently smooth solution, the local truncation error of this method is of order $\\mathcal{O}(h^3)$, which results in a global error $E(h)$ at a fixed time $t_{\\text{end}}$ that is of order $\\mathcal{O}(h^2)$.\n\nThe problem requires estimating the observed order of convergence, $p$. For a method with global error $E(h) \\approx K h^p$ for some constant $K$, we can take the natural logarithm of both sides:\n$$\n\\log(E(h)) \\approx \\log(K) + p \\log(h)\n$$\nThis equation has the form of a linear relationship $Y = C + pX$, where $Y = \\log(E(h))$, $X = \\log(h)$, and the intercept is $C = \\log(K)$. The order of convergence $p$ is the slope of this line. We compute $p$ by performing a linear least-squares regression on the set of data points $(\\log(h_i), \\log(E_i))$, where $E_i = E(h_i)$ is the absolute global error at $t=t_{\\text{end}}$ for each step size $h_i$ in a given test case.\n\nThe algorithm to obtain the result for each test case is as follows:\n1.  Define the integration endpoint $t_{\\text{end}}$ and the set of step sizes $\\{h_i\\}$.\n2.  Calculate the exact solution at the endpoint, $y_{\\text{exact}} = (1 - t_{\\text{end}})^{-1}$.\n3.  For each step size $h_i$ in the set:\n    a. Determine the number of steps, $N_i = \\text{integer}(t_{\\text{end}} / h_i)$.\n    b. Initialize the numerical solution at $t_0=0$ with $y_0=1$.\n    c. Iterate $N_i$ times using the predictor-corrector formulas to find the numerical solution $y_{\\text{num}}(t_{\\text{end}}; h_i) = y_{N_i}$.\n    d. Compute the absolute global error $E_i = |y_{\\text{num}}(t_{\\text{end}}; h_i) - y_{\\text{exact}}|$.\n    e. Store the pair $(\\log(h_i), \\log(E_i))$.\n4.  Using all stored pairs for the test case, compute the slope $p$ of the best-fit line through the points.\n5.  Report the value of $p$, rounded to three decimal places.\n\nThe theoretical order of convergence for Heun's method is $p=2$. This is expected to be observed in test cases where $t_{\\text{end}}$ is far from the singularity at $t=1$. As $t_{\\text{end}}$ approaches $1$, the higher-order derivatives of the solution $y(t)$ become very large, which increases the magnitude of the leading error terms. This can lead to a degradation of the method's performance, and the observed order of convergence $p$ may fall below the theoretical value of $2$ for the given range of step sizes.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given problem by calculating the observed convergence order\n    for a predictor-corrector method on a specific ODE for four test cases.\n    \"\"\"\n    \n    # Define the four test cases from the problem statement.\n    test_cases = [\n        {\"t_end\": 0.1, \"h_values\": [0.02, 0.01, 0.005, 0.0025]},\n        {\"t_end\": 0.5, \"h_values\": [0.1, 0.05, 0.025, 0.0125]},\n        {\"t_end\": 0.9, \"h_values\": [0.1, 0.05, 0.025, 0.0125]},\n        {\"t_end\": 0.95, \"h_values\": [0.05, 0.025, 0.0125]}\n    ]\n\n    # The ODE function y' = f(t, y) = y^2. It is autonomous.\n    def f_ode(y):\n        return y * y\n\n    def integrate_ode(t_end, h):\n        \"\"\"\n        Integrates the ODE y'=y^2 from t=0 to t_end using the specified\n        predictor-corrector scheme (Heun's method).\n        \n        Args:\n            t_end (float): The final integration time.\n            h (float): The constant step size.\n\n        Returns:\n            float: The numerical approximation of y(t_end).\n        \"\"\"\n        y = 1.0  # Initial condition y(0) = 1\n        num_steps = int(round(t_end / h))\n\n        for _ in range(num_steps):\n            # Predictor step (Explicit Euler)\n            f_yn = f_ode(y)\n            y_pred = y + h * f_yn\n            \n            # Corrector step (Trapezoidal update)\n            f_y_pred = f_ode(y_pred)\n            y = y + 0.5 * h * (f_yn + f_y_pred)\n            \n        return y\n\n    results = []\n    for case in test_cases:\n        t_end = case[\"t_end\"]\n        h_values = case[\"h_values\"]\n        \n        # Calculate the exact solution at t_end\n        y_exact = 1.0 / (1.0 - t_end)\n        \n        log_h_vals = []\n        log_E_vals = []\n        \n        for h in h_values:\n            # Compute numerical solution\n            y_num = integrate_ode(t_end, h)\n            \n            # Compute absolute global error\n            error = np.abs(y_num - y_exact)\n            \n            # Store log of step size and log of error\n            log_h_vals.append(np.log(h))\n            log_E_vals.append(np.log(error))\n            \n        # Perform a linear least-squares fit to find the convergence order p (the slope)\n        # log(E) = p * log(h) + C\n        p, _ = np.polyfit(log_h_vals, log_E_vals, 1)\n        \n        results.append(f\"{p:.3f}\")\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2429754"}, {"introduction": "Beyond quantitative accuracy, it is crucial that a numerical method preserves the qualitative features of the underlying dynamical system. This exercise explores a fascinating and important pitfall: the creation of spurious, non-physical fixed points by the discretization process itself. Through a careful analysis of a simple linear ODE [@problem_id:2429768], you will discover the conditions under which a predictor-corrector scheme can misleadingly report stationary behavior where none exists, a critical lesson in numerical stability.", "problem": "Design a self-contained computational physics task that demonstrates how a standard predictorâ€“corrector method can introduce spurious fixed points that are not present in the original continuous-time system. Use only the following fundamental bases: the definition of an ordinary differential equation, the definition of an equilibrium (fixed point) of a continuous dynamical system, and the explicit formula of a one-step predictorâ€“corrector method constructed from the explicit Euler predictor and the trapezoidal-rule corrector.\n\nConsider the scalar ordinary differential equation in one dimension given by $\\,\\dfrac{dx}{dt} = f(x)\\,$, where $\\,x \\in \\mathbb{R}\\,$ and $\\,t \\in \\mathbb{R}\\,$ denotes time. An equilibrium (fixed point) of the continuous system is any $\\,x^\\ast\\,$ such that $\\,f(x^\\ast)=0\\,$. A one-step predictorâ€“corrector method with explicit Euler prediction and trapezoidal-rule correction advances from $\\,x_n\\,$ at time $\\,t_n\\,$ to $\\,x_{n+1}\\,$ at time $\\,t_{n+1}=t_n+h\\,$ using the following two stages:\n- Predictor: $\\,x_{\\mathrm{p}} = x_n + h\\, f(x_n)\\,$.\n- Corrector: $\\,x_{n+1} = x_n + \\dfrac{h}{2}\\,\\big(f(x_n) + f(x_{\\mathrm{p}})\\big)\\,$.\n\nYour tasks are:\n1. Construct a concrete example by taking $\\,f(x)=\\lambda\\,x\\,$ with a constant parameter $\\,\\lambda \\in \\mathbb{R}\\,$. Interpret $\\,\\lambda\\,$ as having units of inverse time, that is $\\,\\lambda\\,$ is measured in $\\,\\mathrm{s}^{-1}\\,$, and use a uniform time step $\\,h>0\\,$ measured in $\\,\\mathrm{s}\\,$. Derive from first principles the discrete one-step map $\\,x_{n+1} = \\Phi_{h,\\lambda}(x_n)\\,$ produced by the above predictorâ€“corrector method for this linear $\\,f(x)\\,$.\n2. Using only the definitions of a fixed point of a map and an equilibrium of a differential equation, determine the condition on the parameters $\\,(\\lambda,h)\\,$ under which the discrete map $\\,\\Phi_{h,\\lambda}\\,$ possesses nonzero fixed points $\\,x^\\ast \\neq 0\\,$ even though the continuous system has only the equilibrium $\\,x^\\ast=0\\,$ for $\\,\\lambda \\neq 0\\,$. Explain why such nonzero fixed points of the discrete map are spurious and non-physical relative to the original continuous system.\n3. Implement a program that, for each provided test case $\\,(\\lambda,h)\\,$, decides whether the discrete map $\\,\\Phi_{h,\\lambda}\\,$ has spurious nonzero fixed points. You must use a numerical tolerance of $\\,\\varepsilon=10^{-12}\\,$ when comparing real numbers to determine equality conditions. The decision should be reported as a Boolean: return $\\text{True}$ if there exist spurious nonzero fixed points and $\\text{False}$ otherwise, under the assumption $\\,\\lambda \\neq 0\\,$. If $\\,\\lambda=0\\,$, recall that the continuous system already has every $\\,x\\,$ as an equilibrium, so no spurious non-physical fixed points are introduced; in that case your decision must be $\\text{False}$.\n\nPhysical units:\n- Time step $\\,h\\,$ must be specified in $\\,\\mathrm{s}\\,$.\n- The parameter $\\,\\lambda\\,$ must be specified in $\\,\\mathrm{s}^{-1}\\,$.\n- There is no angle quantity in this problem.\n- There is no requirement to output units; the outputs are Booleans.\n\nTest suite:\nEvaluate your decision function on the following parameter pairs $\\,(\\lambda,h)\\,$:\n- Case $\\,1\\,$: $\\,(\\lambda,h)=(-1,\\;0.1)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,2\\,$: $\\,(\\lambda,h)=(-1,\\;2.0)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,3\\,$: $\\,(\\lambda,h)=(-4,\\;0.5)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,4\\,$: $\\,(\\lambda,h)=(3,\\;0.5)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,5\\,$: $\\,(\\lambda,h)=(-2.5,\\;0.8)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of Booleans enclosed in square brackets with no spaces, in the order of the test suite above. For example, a valid output looks like $\\,\\big[\\text{False},\\text{True},\\ldots\\big]\\,$. Your program must not read any input and must not print anything else.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical analysis and dynamical systems, is well-posed, and all its components are objectively and rigorously defined. We shall proceed with the solution.\n\nThe task is to analyze how a specific predictor-corrector numerical integration scheme can introduce spurious, non-physical fixed points that are absent in the original continuous-time dynamical system. We consider the scalar ordinary differential equation (ODE) given by $\\frac{dx}{dt} = f(x)$, with the specific linear form $f(x) = \\lambda x$, where $\\lambda \\in \\mathbb{R}$ is a constant parameter.\n\nFirst, we identify the equilibrium points of the continuous system. An equilibrium, or fixed point, $x^\\ast$ is a state where the system does not evolve, which mathematically corresponds to $\\frac{dx}{dt} = 0$. For our system, this condition is $f(x^\\ast) = \\lambda x^\\ast = 0$. For any nonzero parameter $\\lambda \\neq 0$, the one and only equilibrium of the continuous system is the trivial one, $x^\\ast = 0$. If $\\lambda=0$, the ODE becomes $\\frac{dx}{dt} = 0$, and every point $x \\in \\mathbb{R}$ is an equilibrium.\n\nNext, we derive the discrete one-step map, $x_{n+1} = \\Phi_{h,\\lambda}(x_n)$, that is generated by applying the specified predictor-corrector method to this ODE. The method uses a time step $h > 0$.\n\nThe predictor step is the explicit Euler method:\n$$x_{\\mathrm{p}} = x_n + h f(x_n)$$\nSubstituting $f(x_n) = \\lambda x_n$, we obtain the predicted value $x_{\\mathrm{p}}$:\n$$x_{\\mathrm{p}} = x_n + h (\\lambda x_n) = x_n(1 + h\\lambda)$$\n\nThe corrector step is based on the trapezoidal rule, which averages the slope at the beginning and the predicted end of the interval:\n$$x_{n+1} = x_n + \\frac{h}{2} \\big(f(x_n) + f(x_{\\mathrm{p}})\\big)$$\nSubstituting $f(x_n) = \\lambda x_n$ and $f(x_{\\mathrm{p}}) = \\lambda x_{\\mathrm{p}}$:\n$$x_{n+1} = x_n + \\frac{h}{2} (\\lambda x_n + \\lambda x_{\\mathrm{p}})$$\nNow, we replace $x_{\\mathrm{p}}$ with the expression from the predictor step:\n$$x_{n+1} = x_n + \\frac{h\\lambda}{2} \\big(x_n + x_n(1 + h\\lambda)\\big)$$\nFactoring out $x_n$ from the bracketed term gives:\n$$x_{n+1} = x_n + \\frac{h\\lambda}{2} \\big(x_n(1 + (1 + h\\lambda))\\big) = x_n + \\frac{h\\lambda}{2} \\big(x_n(2 + h\\lambda)\\big)$$\nFinally, factoring out $x_n$ from the entire expression yields the discrete map:\n$$x_{n+1} = x_n \\left(1 + \\frac{h\\lambda}{2}(2 + h\\lambda)\\right) = x_n \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right)$$\nSo, the one-step map is $\\Phi_{h,\\lambda}(x_n) = x_n \\left(1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2\\right)$. This is a linear map where the state at step $n+1$ is simply a multiple of the state at step $n$.\n\nWe now seek the fixed points of this discrete map. A fixed point $x^\\ast$ of the map must satisfy the condition $x^\\ast = \\Phi_{h,\\lambda}(x^\\ast)$:\n$$x^\\ast = x^\\ast \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right)$$\nTo find the values of $x^\\ast$ that solve this equation, we rearrange it:\n$$x^\\ast - x^\\ast \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right) = 0$$\n$$x^\\ast \\left(1 - \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right)\\right) = 0$$\n$$x^\\ast \\left(-h\\lambda - \\frac{h^2\\lambda^2}{2}\\right) = 0$$\n$$-x^\\ast h\\lambda \\left(1 + \\frac{h\\lambda}{2}\\right) = 0$$\nThis equation has solutions if either of its factors is zero.\n$1$. The first possibility is $x^\\ast = 0$. This fixed point exists for all values of $h$ and $\\lambda$ and corresponds to the true equilibrium of the original ODE.\n$2$. The second possibility is that the other factor is zero: $-h\\lambda \\left(1 + \\frac{h\\lambda}{2}\\right) = 0$. Since we are given $h > 0$ and are searching for spurious points which only make sense when $\\lambda \\neq 0$, the term $-h\\lambda$ is non-zero. Thus, this condition simplifies to:\n$$1 + \\frac{h\\lambda}{2} = 0$$\n$$h\\lambda = -2$$\n\nIf the condition $h\\lambda = -2$ is met, the equation for the fixed points becomes $-x^\\ast \\cdot 0 = 0$, which is true for *any* value of $x^\\ast \\in \\mathbb{R}$. This means that under the condition $h\\lambda = -2$, every point on the real line is a fixed point of the discrete map. The map itself becomes the identity map: $x_{n+1} = x_n(1 - 2 + \\frac{(-2)^2}{2}) = x_n(1 - 2 + 2) = x_n$.\n\nThese fixed points are deemed spurious because for $\\lambda \\neq 0$, the continuous system possesses only one equilibrium, $x^\\ast = 0$. The numerical method, for the specific combination of parameters satisfying $h\\lambda = -2$, incorrectly implies that *any* initial state $x_0$ is a stationary solution, since $x_n = x_0$ for all $n \\ge 0$. This is a severe qualitative error. For example, if $\\lambda < 0$, the true solution $x(t) = x_0 e^{\\lambda t}$ decays exponentially to $0$. The numerical method with $h\\lambda=-2$ falsely predicts a static system. These infinitely many nonzero fixed points are therefore non-physical artifacts of the discretization.\n\nBased on this analysis, the criterion for the existence of spurious nonzero fixed points (for $\\lambda \\neq 0$) is precisely $h\\lambda = -2$. The case $\\lambda = 0$ is excluded, as the problem statement correctly notes that both the continuous system and the discrete map have all points as equilibria/fixed points, so no spurious fixed points are introduced.\n\nFor implementation, we must use a numerical tolerance $\\varepsilon = 10^{-12}$. The decision logic is as follows:\n- If $|\\lambda| < \\varepsilon$, we consider $\\lambda=0$. The result is $\\text{False}$.\n- Otherwise, we calculate the product $p = h\\lambda$.\n- Spurious nonzero fixed points exist if and only if $|p - (-2)| < \\varepsilon$, which is $|p + 2| < \\varepsilon$. If this condition holds, the result is $\\text{True}$; otherwise, it is $\\text{False}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef has_spurious_fixed_points(lambda_val: float, h: float, epsilon: float = 1e-12) -> bool:\n    \"\"\"\n    Determines if the discrete map has spurious nonzero fixed points.\n\n    Args:\n        lambda_val: The parameter lambda from the ODE dx/dt = lambda * x.\n        h: The time step for the numerical method.\n        epsilon: The numerical tolerance for equality checks.\n\n    Returns:\n        True if spurious nonzero fixed points exist, False otherwise.\n    \"\"\"\n    # According to the problem statement, if lambda is 0, the continuous system\n    # already has every point as an equilibrium. Thus, the fixed points of the\n    # discrete map are not considered spurious.\n    if abs(lambda_val) < epsilon:\n        return False\n\n    # The condition for the existence of spurious nonzero fixed points is derived\n    # to be h * lambda = -2. We check this condition using the provided tolerance.\n    product = h * lambda_val\n    if abs(product + 2.0) < epsilon:\n        return True\n    else:\n        return False\n\ndef solve():\n    \"\"\"\n    Runs the analysis for the test cases provided in the problem statement\n    and prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (lambda, h).\n    test_cases = [\n        (-1.0, 0.1),    # Case 1\n        (-1.0, 2.0),    # Case 2\n        (-4.0, 0.5),    # Case 3\n        (3.0, 0.5),     # Case 4\n        (-2.5, 0.8)     # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_val, h = case\n        result = has_spurious_fixed_points(lambda_val, h)\n        results.append(result)\n\n    # Final print statement in the exact required format: [Boolean,Boolean,...]\n    # The str() of a Python bool ('True', 'False') matches the implicitly desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver.\nsolve()\n```", "id": "2429768"}, {"introduction": "A key strength of predictor-corrector methods is their natural capacity for error estimation, which enables powerful adaptive algorithms. In this practice, you will harness the difference between the predicted and corrected states to build an adaptive step-size controller for the famous Lorenz system [@problem_id:2429776]. This task moves beyond simple analysis and into the realm of computational engineering, creating a robust solver that can efficiently navigate the complex, chaotic dynamics of a challenging problem.", "problem": "Write a complete, runnable program that advances the solution of the Lorenz initial value problem using a single-step predictorâ€“corrector method with adaptive step-size control based on the difference between the predicted and corrected states. The Lorenz system is the three-dimensional autonomous system\n$$\n\\frac{d}{dt}\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} = \\begin{bmatrix}\n\\sigma \\left(y - x\\right) \\\\\nx \\left(\\rho - z\\right) - y \\\\\nx y - \\beta z\n\\end{bmatrix},\n$$\nwhere $\\sigma$, $\\rho$, and $\\beta$ are dimensionless parameters, $t$ is dimensionless time, and the state vector is $\\vec{y}(t) = \\begin{bmatrix}x(t) \\\\ y(t) \\\\ z(t)\\end{bmatrix}$. All quantities in this problem are dimensionless.\n\nAt each step from $\\left(t_n,\\vec{y}_n\\right)$ with step size $h$, compute a predicted state $\\vec{y}_p$ and a corrected state $\\vec{y}_c$ using the following explicit predictorâ€“corrector pair:\n$$\n\\vec{y}_p = \\vec{y}_n + h\\,\\vec{f}\\!\\left(\\vec{y}_n\\right), \\quad\n\\vec{y}_c = \\vec{y}_n + \\frac{h}{2}\\left(\\vec{f}\\!\\left(\\vec{y}_n\\right) + \\vec{f}\\!\\left(\\vec{y}_p\\right)\\right),\n$$\nwhere $\\vec{f}(\\vec{y})$ denotes the right-hand side of the Lorenz system. Define the local discrepancy\n$$\ne = \\left\\|\\vec{y}_c - \\vec{y}_p\\right\\|_2.\n$$\nUse the following adaptive step-size control:\n- Accept the step if $e \\le \\text{tol}$, in which case advance to $\\left(t_{n+1},\\vec{y}_{n+1}\\right) = \\left(t_n + h,\\vec{y}_c\\right)$.\n- If the step is rejected $(e > \\text{tol})$, do not advance time and recompute the step with a smaller $h$.\n- On each attempt (whether the previous attempt was accepted or rejected), update the trial step size according to\n$$\nh \\leftarrow h \\cdot \\operatorname{clip}\\!\\left(s \\left(\\frac{\\text{tol}}{\\max(e,\\epsilon)}\\right)^{1/2}, g_{\\min}, g_{\\max}\\right),\n$$\nwhere $s$ is a safety factor, $g_{\\min}$ and $g_{\\max}$ bound the multiplicative change per attempt, $\\epsilon$ is a positive number used to avoid division by zero, and $\\operatorname{clip}(u,a,b)$ denotes the value of $u$ limited to the interval $\\left[a,b\\right]$. On acceptance, enforce $h \\le h_{\\max}$. On all attempts, enforce $h \\ge h_{\\min}$. If $t + h > t_f$, then replace $h$ by $t_f - t$ for that attempt to end exactly at $t_f$.\n\nImplement the method above to advance from $t_0$ to $t_f$ for each test case below, starting with the given initial step size $h_0$, and count only the accepted steps. Use the Euclidean norm $\\left\\|\\cdot\\right\\|_2$ for $e$. Use $\\epsilon = 10^{-30}$ in the step-size update. All computations must be performed in double precision.\n\nTest Suite:\n- Case A (typical chaotic parameters, moderate tolerance):\n  - $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}$; $t_0 = 0$, $t_f = 2$; $\\text{tol} = 10^{-5}$; $h_0 = 10^{-2}$; $h_{\\min} = 10^{-6}$; $h_{\\max} = 5 \\times 10^{-2}$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n- Case B (looser tolerance, larger maximum step):\n  - $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}$; $t_0 = 0$, $t_f = 2$; $\\text{tol} = 10^{-2}$; $h_0 = 10^{-2}$; $h_{\\min} = 10^{-6}$; $h_{\\max} = 1/2$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n- Case C (tighter tolerance, different initial state, shorter horizon):\n  - $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}0 \\\\ 1 \\\\ 1\\end{bmatrix}$; $t_0 = 0$, $t_f = 1/2$; $\\text{tol} = 10^{-7}$; $h_0 = 10^{-3}$; $h_{\\min} = 10^{-7}$; $h_{\\max} = 5 \\times 10^{-3}$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n- Case D (non-chaotic parameter, convergence toward the origin):\n  - $\\sigma = 10$, $\\rho = 0$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}5 \\\\ 5 \\\\ 5\\end{bmatrix}$; $t_0 = 0$, $t_f = 1$; $\\text{tol} = 10^{-6}$; $h_0 = 10^{-2}$; $h_{\\min} = 10^{-7}$; $h_{\\max} = 10^{-1}$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n\nRequired final output format:\n- For each case in the order A, B, C, D, produce the three components of the final state $\\vec{y}(t_f) = \\begin{bmatrix}x(t_f) \\\\ y(t_f) \\\\ z(t_f)\\end{bmatrix}$ rounded to six digits after the decimal point, followed by the integer number of accepted steps used to reach $t_f$.\n- Aggregate all results into a single line as a comma-separated list enclosed in square brackets, in the order\n$$\n\\left[x_A, y_A, z_A, N_A, x_B, y_B, z_B, N_B, x_C, y_C, z_C, N_C, x_D, y_D, z_D, N_D\\right],\n$$\nwhere $x_\\bullet$, $y_\\bullet$, $z_\\bullet$ are floats with six digits after the decimal point and $N_\\bullet$ are integers. Print exactly this single line as the only output of the program.", "solution": "The Lorenz system is a first-order autonomous system defined by $\\vec{f}(\\vec{y}) = \\begin{bmatrix}\\sigma(y-x) \\\\ x(\\rho - z) - y \\\\ xy - \\beta z\\end{bmatrix}$, with state $\\vec{y}(t) = \\begin{bmatrix}x(t) \\\\ y(t) \\\\ z(t)\\end{bmatrix}$. To perform time integration from $t_0$ to $t_f$ with adaptive step-size control using a predictorâ€“corrector method, we use the explicit Euler step as a predictor paired with the explicit trapezoidal rule (also called the improved Euler or Heun method) as the corrector. At each step, given $\\vec{y}_n$ at time $t_n$ and a tentative step size $h$, the predictor and corrector are defined by\n$$\n\\vec{y}_p = \\vec{y}_n + h\\,\\vec{f}(\\vec{y}_n), \\qquad\n\\vec{y}_c = \\vec{y}_n + \\frac{h}{2}\\big(\\vec{f}(\\vec{y}_n) + \\vec{f}(\\vec{y}_p)\\big).\n$$\nA step-size controller requires a measure of local error. For one-step methods, a standard approach is to use an embedded estimate derived from the difference between two approximations of different orders applied to the same step. Here, the difference $\\vec{y}_c - \\vec{y}_p$ provides an estimate with magnitude proportional to the local truncation error. To see this, expand $\\vec{y}(t_n + h)$ in a Taylor series:\n$$\n\\vec{y}(t_n + h) = \\vec{y}_n + h \\vec{f}(\\vec{y}_n) + \\frac{h^2}{2} \\vec{J}(\\vec{y}_n)\\,\\vec{f}(\\vec{y}_n) + \\mathcal{O}(h^3),\n$$\nwhere $\\vec{J}(\\vec{y})$ is the Jacobian of $\\vec{f}$. The explicit Euler predictor $\\vec{y}_p$ matches terms up to $\\mathcal{O}(h)$, while the trapezoidal corrector $\\vec{y}_c$ matches terms up to $\\mathcal{O}(h^2)$, making the corrected value second-order accurate. The difference $\\vec{y}_c - \\vec{y}_p$ scales like $\\mathcal{O}(h^2)$, hence its Euclidean norm\n$$\ne = \\left\\|\\vec{y}_c - \\vec{y}_p\\right\\|_2\n$$\nserves as a proxy for the local error.\n\nTo keep the local error controlled relative to a user-defined tolerance $\\text{tol}$, we accept a step when $e \\le \\text{tol}$ and reject it otherwise. Classical step-size control uses the relation $e \\propto h^2$ to derive an update for the next trial step size $h_{\\text{new}}$:\n$$\nh_{\\text{new}} = h \\cdot \\left(\\frac{\\text{tol}}{e}\\right)^{1/2}.\n$$\nTo make this robust in practice, we add a safety factor $s \\in (0,1)$ and bound the multiplicative change by $g_{\\min} \\le \\frac{h_{\\text{new}}}{h} \\le g_{\\max}$, giving\n$$\nh \\leftarrow h \\cdot \\operatorname{clip}\\!\\left(s \\left(\\frac{\\text{tol}}{\\max(e,\\epsilon)}\\right)^{1/2}, g_{\\min}, g_{\\max}\\right),\n$$\nwith a very small $\\epsilon > 0$ to avoid division by zero if $e=0$. On each attempt, we enforce $h \\ge h_{\\min}$, and we do not exceed $h_{\\max}$ after acceptance. We also ensure that the final step lands exactly at $t_f$ by shortening the last attempt if $t + h > t_f$.\n\nAlgorithmically, for each case:\n1. Initialize $t \\leftarrow t_0$, $\\vec{y} \\leftarrow \\vec{y}_0$, $h \\leftarrow h_0$, and an accepted-step counter $N \\leftarrow 0$.\n2. While $t < t_f$:\n   - Set $h \\leftarrow \\min\\left(h, t_f - t\\right)$ to avoid overshooting the final time.\n   - Compute the predictor $\\vec{y}_p = \\vec{y} + h\\,\\vec{f}(\\vec{y})$.\n   - Compute the corrector $\\vec{y}_c = \\vec{y} + \\frac{h}{2}\\left(\\vec{f}(\\vec{y}) + \\vec{f}(\\vec{y}_p)\\right)$.\n   - Compute $e = \\left\\|\\vec{y}_c - \\vec{y}_p\\right\\|_2$.\n   - Compute a multiplicative factor $\\phi = s \\left(\\frac{\\text{tol}}{\\max(e,\\epsilon)}\\right)^{1/2}$, then bound it: $\\phi \\leftarrow \\operatorname{clip}(\\phi, g_{\\min}, g_{\\max})$.\n   - If $e \\le \\text{tol}$:\n     - Accept: set $\\vec{y} \\leftarrow \\vec{y}_c$, $t \\leftarrow t + h$, $N \\leftarrow N + 1$.\n     - Update $h \\leftarrow \\min\\left(h \\cdot \\phi, h_{\\max}\\right)$ and enforce $h \\ge h_{\\min}$.\n   - Else:\n     - Reject: update $h \\leftarrow \\max\\left(h \\cdot \\phi, h_{\\min}\\right)$ and retry without advancing $t$ or $\\vec{y}$.\n3. After termination, report $\\vec{y}(t_f)$ and $N$.\n\nThe Lorenz right-hand side is evaluated as\n$$\n\\vec{f}\\!\\left(\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix}\\right) = \\begin{bmatrix}\n\\sigma\\left(y - x\\right) \\\\\nx\\left(\\rho - z\\right) - y \\\\\nx y - \\beta z\n\\end{bmatrix}.\n$$\nAll cases in the test suite use this $\\vec{f}$ with specified parameters, initial conditions, tolerances, and step-size controller constants. The outputs required are the components of the final state at $t_f$ rounded to six digits after the decimal point, followed by the integer number of accepted steps, concatenated for the four cases in order. The program prints these values as a single comma-separated list enclosed in square brackets.\n\nThis approach is grounded in the local error behavior of the predictorâ€“corrector pair and provides a systematic means to adapt the step size to meet the specified tolerance while respecting lower and upper bounds on the step and ensuring exact termination at $t_f$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lorenz_rhs(y, sigma, rho, beta):\n    x, yv, z = y\n    return np.array([\n        sigma * (yv - x),\n        x * (rho - z) - yv,\n        x * yv - beta * z\n    ], dtype=np.float64)\n\ndef integrate_lorenz_pc_adaptive(y0, t0, tf, sigma, rho, beta,\n                                 tol, h0, hmin, hmax,\n                                 safety=0.9, gmin=0.2, gmax=2.0,\n                                 eps=1e-30):\n    y = np.array(y0, dtype=np.float64)\n    t = float(t0)\n    h = float(h0)\n    accepted_steps = 0\n\n    # Ensure bounds are sensible\n    hmin = float(hmin)\n    hmax = float(hmax)\n    safety = float(safety)\n    gmin = float(gmin)\n    gmax = float(gmax)\n\n    # Main adaptive loop\n    while t < tf:\n        # Prevent overshooting the final time\n        if t + h > tf:\n            h = tf - t\n        # Enforce min/max bounds on h for this attempt\n        if h < hmin:\n            h = hmin\n        if h > hmax:\n            h = hmax\n\n        f_n = lorenz_rhs(y, sigma, rho, beta)\n        y_pred = y + h * f_n\n        f_pred = lorenz_rhs(y_pred, sigma, rho, beta)\n        y_corr = y + 0.5 * h * (f_n + f_pred)\n\n        # Error estimate: Euclidean norm of difference\n        err = np.linalg.norm(y_corr - y_pred, ord=2)\n\n        # Compute multiplicative factor for next h\n        # Avoid division by zero using eps\n        ratio = tol / max(err, eps)\n        phi = safety * (ratio ** 0.5)\n        # Clip by growth/shrinkage limits\n        if phi < gmin:\n            phi = gmin\n        elif phi > gmax:\n            phi = gmax\n\n        if err <= tol:\n            # Accept step\n            y = y_corr\n            t = t + h\n            accepted_steps += 1\n            # Update h for next attempt, enforce bounds\n            h = h * phi\n            if h > hmax:\n                h = hmax\n            if h < hmin:\n                h = hmin\n        else:\n            # Reject step: shrink h and retry\n            h = h * phi\n            if h < hmin:\n                h = hmin\n            # Do not advance t or y\n\n        # Safety net to avoid pathological infinite loops:\n        # if h becomes effectively zero and cannot advance time, break.\n        if h <= 0.0:\n            break\n\n    return y, accepted_steps\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (sigma, rho, beta, y0, t0, tf, tol, h0, hmin, hmax, safety, gmin, gmax)\n    test_cases = [\n        # Case A\n        (10.0, 28.0, 8.0/3.0, (1.0, 1.0, 1.0), 0.0, 2.0, 1e-5, 1e-2, 1e-6, 5e-2, 0.9, 0.2, 2.0),\n        # Case B\n        (10.0, 28.0, 8.0/3.0, (1.0, 1.0, 1.0), 0.0, 2.0, 1e-2, 1e-2, 1e-6, 5e-1, 0.9, 0.2, 2.0),\n        # Case C\n        (10.0, 28.0, 8.0/3.0, (0.0, 1.0, 1.0), 0.0, 0.5, 1e-7, 1e-3, 1e-7, 5e-3, 0.9, 0.2, 2.0),\n        # Case D\n        (10.0, 0.0, 8.0/3.0, (5.0, 5.0, 5.0), 0.0, 1.0, 1e-6, 1e-2, 1e-7, 1e-1, 0.9, 0.2, 2.0),\n    ]\n\n    results_str = []\n    for case in test_cases:\n        sigma, rho, beta, y0, t0, tf, tol, h0, hmin, hmax, safety, gmin, gmax = case\n        y_final, n_acc = integrate_lorenz_pc_adaptive(\n            y0=y0, t0=t0, tf=tf,\n            sigma=sigma, rho=rho, beta=beta,\n            tol=tol, h0=h0, hmin=hmin, hmax=hmax,\n            safety=safety, gmin=gmin, gmax=gmax, eps=1e-30\n        )\n        # Append formatted floats and integer as specified\n        results_str.append(f\"{y_final[0]:.6f}\")\n        results_str.append(f\"{y_final[1]:.6f}\")\n        results_str.append(f\"{y_final[2]:.6f}\")\n        results_str.append(str(int(n_acc)))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2429776"}]}