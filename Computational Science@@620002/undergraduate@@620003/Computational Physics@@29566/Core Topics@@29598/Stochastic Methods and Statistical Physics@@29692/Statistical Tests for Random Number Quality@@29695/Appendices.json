{"hands_on_practices": [{"introduction": "A credible random number generator must produce sequences whose distribution is statistically indistinguishable from uniform. This first exercise allows you to apply two of the most fundamental goodness-of-fit tests—the chi-squared ($\\chi^2$) test and the Kolmogorov-Smirnov (KS) test—to assess this property. By analyzing a deterministic sequence from number theory whose properties are well-understood, you will build a foundational intuition for how these statistical tools quantify the concept of uniformity. [@problem_id:2442653]", "problem": "You are given the deterministic sequence defined by the fractional part map. For a real number $\\alpha$ and a positive integer $N$, define the sequence $\\{x_n\\}_{n=1}^N$ by\n$$\nx_n = \\{\\alpha n^2\\} = \\alpha n^2 - \\lfloor \\alpha n^2 \\rfloor \\in [0,1),\n$$\nwhere $\\{\\cdot\\}$ denotes the fractional part and $\\lfloor \\cdot \\rfloor$ denotes the floor function. Consider the null hypothesis that the sample $\\{x_n\\}_{n=1}^N$ is drawn from the continuous Uniform distribution on $[0,1)$.\n\nFor each parameter set in the test suite below, evaluate the following two goodness-of-fit tests of uniformity for the sample $\\{x_n\\}_{n=1}^N$:\n\n1. The chi-squared goodness-of-fit test with $K$ equal-width bins partitioning $[0,1)$ into intervals $I_j = [\\frac{j-1}{K}, \\frac{j}{K})$ for $j = 1, 2, \\dots, K$. Let $O_j$ be the observed count in bin $I_j$, and $E_j = \\frac{N}{K}$ the expected count under the null hypothesis. The chi-squared statistic is\n$$\n\\chi^2 = \\sum_{j=1}^{K} \\frac{(O_j - E_j)^2}{E_j},\n$$\nwhich under the null hypothesis is asymptotically distributed as a chi-squared distribution with $K-1$ degrees of freedom. Compute the corresponding p-value.\n\n2. The one-sample Kolmogorov-Smirnov (KS) test, with statistic\n$$\nD_N = \\sup_{u \\in [0,1]} \\left| F_N(u) - u \\right|,\n$$\nwhere $F_N(u)$ is the empirical cumulative distribution function of $\\{x_n\\}_{n=1}^N$. Under the null hypothesis, the distribution of $D_N$ is the Kolmogorov distribution, which determines the p-value.\n\nFor each test, determine whether to reject the null hypothesis at significance level $\\delta$ using the decision rule: reject if and only if the p-value is less than $\\delta$.\n\nTest suite:\n- Case $1$: $\\alpha = \\sqrt{2}$, $N = 10000$, $K = 50$, $\\delta = 0.001$.\n- Case $2$: $\\alpha = \\sqrt{3}$, $N = 200$, $K = 10$, $\\delta = 0.001$.\n- Case $3$ (control, rational $\\alpha$): $\\alpha = \\frac{1}{2}$, $N = 1000$, $K = 20$, $\\delta = 0.001$.\n\nYour program must, for each case, generate the sequence $\\{x_n\\}_{n=1}^N$, compute the two p-values, and output for each case a list containing four entries in the order\n$$\n[\\text{p\\_chi2}, \\text{p\\_KS}, I_{\\chi^2}, I_{\\mathrm{KS}}],\n$$\nwhere $\\text{p\\_chi2}$ is the chi-squared p-value, $\\text{p\\_KS}$ is the Kolmogorov-Smirnov p-value, $I_{\\chi^2}$ equals $1$ if the chi-squared test rejects the null hypothesis at level $\\delta$ and $0$ otherwise, and $I_{\\mathrm{KS}}$ equals $1$ if the Kolmogorov-Smirnov test rejects at level $\\delta$ and $0$ otherwise.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the three case results, each case result formatted as a bracketed, comma-separated list as described above. The overall output must be a single bracketed list, for example,\n$$\n[[\\text{p1\\_chi2},\\text{p1\\_KS},I1_{\\chi^2},I1_{\\mathrm{KS}}],[\\text{p2\\_chi2},\\text{p2\\_KS},I2_{\\chi^2},I2_{\\mathrm{KS}}],[\\text{p3\\_chi2},\\text{p3\\_KS},I3_{\\chi^2},I3_{\\mathrm{KS}}]].\n$$\nNumbers must be printed without units. No user input is required or permitted. The answer must be computed and printed by the program using the specified test suite only.", "solution": "The problem requires the application of two standard goodness-of-fit statistical tests—the chi-squared test and the Kolmogorov-Smirnov test—to a deterministically generated sequence to assess its uniformity. We must first validate the premise and then, if valid, construct a systematic solution.\n\nThe problem statement is valid. It is scientifically grounded in the mathematical field of number theory, specifically the theory of uniform distribution of sequences, and in the statistical methodology for testing pseudorandom number generators. All parameters and definitions are provided, making the problem well-posed and objective. The conditions for the applicability of the chi-squared test, namely that expected frequencies should not be too small, are met in all test cases. For instance, in the most constrained case (Case 2), the expected count per bin is $E_j = N/K = 200/10 = 20$, which is well above the typical minimum requirement of $E_j \\ge 5$.\n\nThe core of the problem lies in testing the null hypothesis, $H_0$, that the sequence $\\{x_n\\}_{n=1}^{N}$ defined by $x_n = \\{\\alpha n^2\\} = \\alpha n^2 - \\lfloor \\alpha n^2 \\rfloor$ behaves as a sample from the continuous uniform distribution on the interval $[0,1)$. The behavior of such sequences is well-understood in number theory. According to Weyl's criterion, the sequence $\\{P(n)\\alpha\\}_{n=1}^{\\infty}$ is uniformly distributed in $[0,1)$ for any polynomial $P(n)$ with at least one irrational coefficient other than the constant term. For the polynomial $P(n) = n^2$, the sequence $\\{n^2 \\alpha\\}$ is uniformly distributed if and only if $\\alpha$ is irrational.\n\nTherefore, for Case 1 ($\\alpha = \\sqrt{2}$) and Case 2 ($\\alpha = \\sqrt{3}$), where $\\alpha$ is irrational, we expect the sequence to be uniformly distributed for large $N$. Consequently, the statistical tests should fail to reject the null hypothesis, yielding high p-values. Conversely, for Case 3 ($\\alpha = 1/2$), where $\\alpha$ is rational, the sequence is not uniformly distributed. The values of $x_n$ are confined to a finite set; specifically, $x_n = 0$ if $n$ is even and $x_n = 0.5$ if $n$ is odd. This drastic departure from uniformity should be easily detected by both tests, resulting in extremely low p-values and a rejection of the null hypothesis.\n\nThe solution proceeds by implementing these tests for each specified parameter set.\n\n**1. Sequence Generation**\nFor each case, we generate the sequence $\\{x_n\\}_{n=1}^{N}$ by computing $x_n = (\\alpha \\cdot n^2) \\pmod 1$ for $n = 1, 2, \\ldots, N$. High-precision floating-point arithmetic is essential to minimize numerical errors.\n\n**2. Chi-Squared ($\\chi^2$) Goodness-of-Fit Test**\nThis test discretizes the sample space. The interval $[0,1)$ is divided into $K$ disjoint, equal-width bins, $I_j = [\\frac{j-1}{K}, \\frac{j}{K})$ for $j=1, \\dots, K$.\nThe number of observed data points, $O_j$, falling into each bin $I_j$ is counted.\nUnder the null hypothesis of uniformity, the expected number of points in each bin is $E_j = N/K$.\nThe chi-squared statistic is then calculated as:\n$$\n\\chi^2 = \\sum_{j=1}^{K} \\frac{(O_j - E_j)^2}{E_j}\n$$\nUnder $H_0$, this statistic follows a chi-squared distribution with $K-1$ degrees of freedom. The p-value, which is the probability of observing a test statistic at least as extreme as the one computed, is given by $P(\\chi^2_{K-1} \\ge \\chi^2_{obs})$. This is calculated using the survival function (1 - CDF) of the chi-squared distribution.\n\n**3. Kolmogorov-Smirnov (KS) Test**\nThis test compares the empirical cumulative distribution function (ECDF) of the sample, $F_N(u)$, with the CDF of the reference distribution, which for $U[0,1)$ is $F(u)=u$. The ECDF is defined as:\n$$\nF_N(u) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}_{x_n \\le u}\n$$\nwhere $\\mathbf{1}$ is the indicator function. The test statistic is the maximum absolute difference between these two functions over the entire domain:\n$$\nD_N = \\sup_{u \\in [0,1]} |F_N(u) - u|\n$$\nThe distribution of the $D_N$ statistic under $H_0$ is known as the Kolmogorov distribution, which is used to compute the corresponding p-value. This test is generally more powerful than the chi-squared test as it does not lose information through binning.\n\n**4. Decision Rule**\nFor each test, the computed p-value is compared against the given significance level $\\delta$. The null hypothesis $H_0$ is rejected if and only if the p-value is less than $\\delta$. An indicator variable, $I_{\\chi^2}$ or $I_{\\mathrm{KS}}$, is set to $1$ for rejection and $0$ for failure to reject.\n\nThe implementation will utilize the `numpy` library for numerical operations and sequence generation, and the `scipy.stats` module for the statistical test implementations (`chisquare` and `kstest`), which provide direct computation of the p-values. The final output will be formatted into a nested list structure as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing chi-squared and Kolmogorov-Smirnov tests\n    on a deterministic sequence for multiple test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, N, K, delta)\n        (np.sqrt(2), 10000, 50, 0.001),  # Case 1\n        (np.sqrt(3), 200, 10, 0.001),    # Case 2\n        (0.5, 1000, 20, 0.001),         # Case 3\n    ]\n\n    all_results = []\n    for alpha, N, K, delta in test_cases:\n        # Generate the sequence x_n = {alpha * n^2} for n = 1, ..., N\n        # Ensure high precision by using float64 where intermediate products\n        # could become large. n**2 for n=10000 is 1e8, well within int64.\n        # The multiplication with float alpha will promote the result to float64.\n        n = np.arange(1, N + 1)\n        x_n = np.mod(alpha * n**2, 1.0)\n\n        # 1. Chi-squared goodness-of-fit test\n        # Get observed counts in K equal-width bins\n        observed_counts, _ = np.histogram(x_n, bins=K, range=(0, 1))\n        \n        # scipy.stats.chisquare with no f_exp assumes uniform expected frequencies.\n        # The expected frequency is N / K.\n        # The degrees of freedom is K - 1.\n        chi2_stat, p_chi2 = stats.chisquare(f_obs=observed_counts)\n        \n        # Determine if we reject the null hypothesis\n        I_chi2 = 1 if p_chi2 < delta else 0\n\n        # 2. One-sample Kolmogorov-Smirnov test\n        # Test against the continuous uniform distribution on [0, 1]\n        ks_result = stats.kstest(x_n, 'uniform')\n        p_ks = ks_result.pvalue\n        \n        # Determine if we reject the null hypothesis\n        I_ks = 1 if p_ks < delta else 0\n\n        # Collate results for the current case\n        case_result = [p_chi2, p_ks, I_chi2, I_ks]\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    # Example: [[p1, p2, i1, i2],[p3, p4, i3, i4]]\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2442653"}, {"introduction": "Often, physical sources of randomness are imperfect and exhibit some bias. This practice introduces a classic and elegant technique, the von Neumann extractor, to transform a biased stream of bits into a perfectly unbiased one. Your task is to implement this extractor and then verify the quality of its output, not just for bias but for other subtle defects, by applying a battery of statistical tests including the frequency, serial, and runs tests. [@problem_id:2442648]", "problem": "You are given an infinite sequence of independent and identically distributed coin flips, where each flip is Heads with probability $p$ and Tails with probability $1 - p$. Represent Heads as $1$ and Tails as $0$. Define the von Neumann extractor as follows: partition the raw flips into disjoint consecutive pairs and map each pair according to the rule\n- $(1, 0) \\mapsto 1$,\n- $(0, 1) \\mapsto 0$,\n- $(0, 0)$ and $(1, 1)$ produce no output,\ncontinuing until exactly $N$ output bits have been produced.\n\nUsing only the assumption of independence for the raw flips, generate the von Neumann extracted output sequence of length $N$ from the biased source and assess the statistical quality of the resulting output using the following three tests, each producing a two-sided $p$-value:\n1. Frequency (monobit) test: If $k$ of the $N$ output bits are equal to $1$, compute the exact two-sided binomial $p$-value under the null hypothesis $H_0\\!:\\, \\Pr(1)=0.5$.\n2. Serial test on non-overlapping adjacent pairs: Using the first $2 \\lfloor N/2 \\rfloor$ bits of the output, form $\\lfloor N/2 \\rfloor$ non-overlapping pairs and count the occurrences of $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$. Under the null hypothesis of independent fair bits, the expected proportion of each pair is $1/4$. Compute the chi-squared goodness-of-fit $p$-value with $3$ degrees of freedom.\n3. Runs test for independence in a binary sequence with $\\Pr(1)=\\Pr(0)=1/2$: Let $n_1$ and $n_0$ denote the counts of ones and zeros, respectively, in the $N$ output bits, and let $R$ be the total number of runs of identical bits. Under the null hypothesis of independence with fixed $n_1$ and $n_0$, use the mean\n$$\\mu_R \\,=\\, 1 \\,+\\, \\frac{2 n_1 n_0}{N}$$\nand variance\n$$\\sigma_R^2 \\,=\\, \\frac{2 n_1 n_0 \\left(2 n_1 n_0 - N\\right)}{N^2 (N-1)}$$\nto form the normal approximation statistic\n$$Z \\,=\\, \\frac{R - \\mu_R}{\\sigma_R},$$\nand compute the corresponding two-sided $p$-value.\n\nFor reproducibility, use a fixed integer seed $s$ for the pseudorandom number generator that produces the raw biased flips. For test case index $i$ (starting from $0$), use the seed $s + i$.\n\nTest Suite. For each parameter pair $(p, N)$ below, generate the von Neumann extracted output and compute the three $p$-values described above:\n- Case $0$: $(p, N) = (0.30, 8000)$,\n- Case $1$: $(p, N) = (0.49, 12000)$,\n- Case $2$: $(p, N) = (0.90, 6000)$,\n- Case $3$: $(p, N) = (0.50, 10000)$,\nwith base seed $s = 20240913$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list of four items enclosed in square brackets. Each item corresponds to one test case and is itself a list of three decimal numbers equal to the frequency-test $p$-value, the serial-test $p$-value, and the runs-test $p$-value, each rounded to six decimal places. For example, the overall format must be\n$[[p_{0,1},p_{0,2},p_{0,3}],[p_{1,1},p_{1,2},p_{1,3}],[p_{2,1},p_{2,2},p_{2,3}],[p_{3,1},p_{3,2},p_{3,3}]]$,\nwhere $p_{i,j}$ denotes the $j$-th test’s $p$-value for case $i$. No additional text should be printed.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, and objective. The task is a standard exercise in computational statistics, specifically concerning the quality assessment of random numbers generated via a well-established randomness extraction technique. All parameters, methods, and test statistics are clearly and correctly defined.\n\nThe solution is developed in three stages: first, the generation of a debiased binary sequence using the von Neumann extractor; second, the application of three distinct statistical tests to this sequence; and third, the implementation of this process for the specified test cases.\n\n**1. The Von Neumann Extractor**\n\nThe foundation of this problem is the von Neumann randomness extractor. It takes a sequence of independent and identically distributed (i.i.d.) biased bits, $X_i$, where the probability of a one is $\\Pr(X_i=1)=p$ and a zero is $\\Pr(X_i=0)=1-p$, with $p \\in (0,1)$. The extractor processes the input stream by taking non-overlapping pairs of bits, $(X_{2j-1}, X_{2j})$.\n\nThe operational rules are:\n-   If the pair is $(1,0)$, output a $1$.\n-   If the pair is $(0,1)$, output a $0$.\n-   If the pair is $(0,0)$ or $(1,1)$, produce no output and proceed to the next pair.\n\nThe theoretical justification for this method's effectiveness is based on simple probability. The probabilities of the generating pairs are $\\Pr(1,0) = p(1-p)$ and $\\Pr(0,1) = (1-p)p$. These probabilities are identical. The probability of a pair being discarded is $\\Pr(0,0) + \\Pr(1,1) = (1-p)^2 + p^2$.\n\nThe conditional probability of producing a $1$, given that an output bit is generated, is:\n$$ \\Pr(\\text{output}=1 | \\text{output is generated}) = \\frac{\\Pr(1,0)}{\\Pr(1,0) + \\Pr(0,1)} = \\frac{p(1-p)}{p(1-p) + (1-p)p} = \\frac{1}{2} $$\nSimilarly, the conditional probability of producing a $0$ is also $1/2$. Thus, the output sequence is perfectly unbiased. Further, because the input bits are i.i.d., the non-overlapping pairs are independent, which ensures that the resulting output bits are also independent of each other. The procedure therefore transforms a biased i.i.d. source into an unbiased i.i.d. source, which should pass statistical tests for randomness.\n\nThe simulation generates these output bits by creating a stream of pseudorandom bits with bias $p$, processing them in pairs according to the rules, and collecting the output until a sequence of the desired length $N$ is formed. For reproducibility, the pseudorandom number generator is seeded with $s+i$ for test case index $i$, where the base seed is $s=20240913$.\n\n**2. Statistical Quality Assessment**\n\nThe quality of the generated $N$-bit sequence is assessed using three standard statistical tests. Each test evaluates a specific null hypothesis ($H_0$) related to ideal randomness and yields a $p$-value. The $p$-value represents the probability of observing a result at least as extreme as the one measured, assuming $H_0$ is true.\n\n**2.1. Frequency (Monobit) Test**\nThis test checks for the most fundamental property of an unbiased binary sequence: an equal number of zeros and ones on average.\n-   $H_0$: The probability of a one is $1/2$.\n-   Let $k$ be the number of ones in the $N$-bit sequence. Under $H_0$, $k$ follows a binomial distribution, $B(N, 0.5)$.\n-   The two-sided $p$-value is the probability of observing a count of ones as far or farther from the expected mean $N/2$ than $k$. This is calculated using the exact binomial probability mass function.\n\n**2.2. Serial Test on Pairs**\nThis test examines for short-range correlations by checking if adjacent bits are independent.\n-   $H_0$: The bits are independent and fair. This implies that each of the four possible non-overlapping pairs—$(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$—should occur with equal probability, $1/4$.\n-   The first $2 \\lfloor N/2 \\rfloor$ bits are formed into $M = \\lfloor N/2 \\rfloor$ pairs. The number of occurrences of each pair type ($c_{00}, c_{01}, c_{10}, c_{11}$) is counted.\n-   A chi-squared ($\\chi^2$) goodness-of-fit test is performed to compare the observed counts to the expected count of $M/4$ for each pair type. The $\\chi^2$ statistic is:\n$$ \\chi^2 = \\sum_{ij \\in \\{00,01,10,11\\}} \\frac{(c_{ij} - M/4)^2}{M/4} $$\n-   This statistic is compared against a $\\chi^2$ distribution with $4-1=3$ degrees of freedom to obtain the $p$-value.\n\n**2.3. Runs Test**\nThis test checks for independence by analyzing the number of \"runs\" in the sequence, where a run is a maximal consecutive subsequence of identical values. An unusual number of runs can indicate clustering (too few runs) or rapid oscillation (too many runs).\n-   $H_0$: The sequence is independent, with fixed counts of ones ($n_1$) and zeros ($n_0$).\n-   Let $R$ be the total observed number of runs. For large $N$, the distribution of $R$ can be approximated by a normal distribution with the specified mean $\\mu_R = 1 + \\frac{2 n_1 n_0}{N}$ and variance $\\sigma_R^2 = \\frac{2 n_1 n_0 (2 n_1 n_0 - N)}{N^2 (N-1)}$.\n-   The test statistic $Z = (R - \\mu_R) / \\sigma_R$ is computed.\n-   The two-sided $p$-value is derived from the standard normal distribution as $2 \\times \\Phi(-|Z|)$, where $\\Phi$ is the standard normal cumulative distribution function.\n\nThe implementation of these three tests will produce three $p$-values for each test case, which are then formatted as required.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import binomtest, chisquare, norm\n\ndef generate_von_neumann_sequence(p, N, seed):\n    \"\"\"\n    Generates a sequence of N unbiased bits from a biased source using the\n    von Neumann extractor.\n\n    Args:\n        p (float): The probability of '1' in the source sequence.\n        N (int): The desired length of the output sequence.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        np.ndarray: A numpy array of length N containing the unbiased bits.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    output_list = []\n    \n    # Heuristic for chunk size to generate raw bits.\n    # A larger chunk is more efficient than many small ones.\n    # 20000 raw bits = 10000 pairs.\n    chunk_size = 20000\n\n    while len(output_list) < N:\n        # Generate a chunk of biased bits (True for 1, False for 0).\n        raw_bits = rng.random(size=chunk_size) < p\n        \n        for i in range(0, chunk_size, 2):\n            b1, b2 = raw_bits[i], raw_bits[i+1]\n            \n            # Check for (1,0) or (0,1) pairs.\n            if b1 != b2:\n                # Map (1,0) to 1 and (0,1) to 0.\n                output_list.append(1 if b1 else 0)\n                if len(output_list) == N:\n                    break\n    \n    return np.array(output_list, dtype=np.int8)\n\ndef frequency_test(sequence):\n    \"\"\"\n    Performs the frequency (monobit) test.\n    \n    Args:\n        sequence (np.ndarray): The binary sequence to test.\n        \n    Returns:\n        float: The two-sided p-value.\n    \"\"\"\n    N = len(sequence)\n    if N == 0:\n        return 1.0\n    k = np.sum(sequence)\n    # The binomtest function computes the exact two-sided binomial test p-value.\n    result = binomtest(k, n=N, p=0.5, alternative='two-sided')\n    return result.pvalue\n\ndef serial_test(sequence):\n    \"\"\"\n    Performs the serial test on non-overlapping adjacent pairs.\n    \n    Args:\n        sequence (np.ndarray): The binary sequence to test.\n        \n    Returns:\n        float: The chi-squared p-value.\n    \"\"\"\n    N = len(sequence)\n    M = N // 2\n    if M == 0:\n        return 1.0 # Not enough data for pairs.\n\n    # Form non-overlapping pairs from the first 2*M bits.\n    seq_pairs = sequence[:2*M].reshape(M, 2)\n    \n    # A mapping to count pairs: (0,0)->0, (0,1)->1, (1,0)->2, (1,1)->3\n    pair_values = 2 * seq_pairs[:, 0] + seq_pairs[:, 1]\n    observed_counts = np.bincount(pair_values, minlength=4)\n    \n    # Under H0, expected count for each pair is M/4.\n    expected_count = M / 4.0\n    \n    if expected_count == 0:\n        return 1.0\n        \n    # The chisquare test compares observed vs expected frequencies.\n    # Degrees of freedom is k-1 = 4-1 = 3 by default.\n    _, p_value = chisquare(f_obs=observed_counts, f_exp=[expected_count]*4)\n    return p_value\n\ndef runs_test(sequence):\n    \"\"\"\n    Performs the runs test for independence.\n    \n    Args:\n        sequence (np.ndarray): The binary sequence to test.\n        \n    Returns:\n        float: The normal approximation p-value.\n    \"\"\"\n    N = len(sequence)\n    n1 = np.sum(sequence)\n    n0 = N - n1\n\n    # If the sequence is monolithic, the test is not applicable.\n    # This implies extreme non-randomness. p-value should be 0.\n    if n1 == 0 or n0 == 0:\n        return 0.0\n\n    # Count the number of runs R.\n    R = np.sum(sequence[:-1] != sequence[1:]) + 1\n    \n    # Calculate mean and variance under the null hypothesis.\n    mu_R = 1 + (2.0 * n1 * n0) / N\n    \n    numerator_sigma2 = 2.0 * n1 * n0 * (2.0 * n1 * n0 - N)\n    denominator_sigma2 = float(N**2) * (N - 1)\n    \n    # Variance can be non-positive only in extreme cases not expected here.\n    if denominator_sigma2 == 0 or numerator_sigma2 <= 0:\n        return 0.0\n        \n    sigma2_R = numerator_sigma2 / denominator_sigma2\n    sigma_R = np.sqrt(sigma2_R)\n    \n    # Calculate Z-score and two-sided p-value from standard normal distribution.\n    Z = (R - mu_R) / sigma_R\n    p_value = 2 * norm.sf(abs(Z)) # sf is the survival function, 1-cdf\n    \n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (0.30, 8000),\n        (0.49, 12000),\n        (0.90, 6000),\n        (0.50, 10000),\n    ]\n    base_seed = 20240913\n\n    all_results = []\n    for i, (p, N) in enumerate(test_cases):\n        seed = base_seed + i\n        \n        # 1. Generate the sequence\n        sequence = generate_von_neumann_sequence(p, N, seed)\n        \n        # 2. Run the statistical tests\n        p_freq = frequency_test(sequence)\n        p_serial = serial_test(sequence)\n        p_runs = runs_test(sequence)\n        \n        all_results.append([p_freq, p_serial, p_runs])\n\n    # 3. Format the output as specified\n    formatted_cases = []\n    for pvals in all_results:\n        formatted_pvals = [f\"{p:.6f}\" for p in pvals]\n        formatted_cases.append(f'[{\",\".join(formatted_pvals)}]')\n    \n    print(f'[{\",\".join(formatted_cases)}]')\n\nsolve()\n```", "id": "2442648"}, {"introduction": "Can a generator pass a suite of standard statistical tests and still be catastrophically flawed for a physics simulation? This final, challenging practice demonstrates that the answer is a resounding yes. You will construct a pseudo-random number generator that is deliberately engineered to have a perfect one-dimensional uniform distribution but a fatal correlation in two dimensions, a flaw that will cause a simple Monte Carlo estimation of $ \\pi $ to fail dramatically. This exercise delivers a crucial lesson on the limitations of one-dimensional tests and the critical importance of multidimensional uniformity for reliable scientific simulations. [@problem_id:2442681]", "problem": "You are to construct and analyze a deterministic Pseudorandom Number Generator (PRNG) that, for each provided seed, produces a finite sequence of real numbers in the open interval $(0,1)$ which passes a specified statistical battery while producing a systematically incorrect outcome in a simple Monte Carlo (MC) physical simulation. The program you write must implement the generator, apply the tests, run the simulation, and report a boolean result for each test case. All definitions and thresholds are given below.\n\nA PRNG with seed $s$ produces a sequence $\\{x_n\\}_{n=1}^N$ with each $x_n \\in (0,1)$. For a fixed seed $s$, define three hypothesis tests under the null hypothesis $\\mathcal{H}_0$ that the sequence is independent and identically distributed (i.i.d.) with common distribution $\\mathrm{Uniform}(0,1)$:\n\n- One-sample Kolmogorov–Smirnov (KS) test: Let $F_N(x)$ denote the empirical distribution function of $\\{x_n\\}_{n=1}^N$ and $F(x)=x$ denote the cumulative distribution function of $\\mathrm{Uniform}(0,1)$. Define the KS statistic $D_N=\\sup_{x\\in[0,1]} \\left| F_N(x) - F(x) \\right|$. The associated $p$-value is computed under $\\mathcal{H}_0$.\n\n- Chi-square test for equal-probability bins: Partition $(0,1)$ into $B$ equal-width bins. Let $O_j$ be the observed count in bin $j$ for $j=1,\\dots,B$, and let $E_j=N/B$ be the expected count under $\\mathcal{H}_0$. The chi-square statistic is $\\chi^2=\\sum_{j=1}^B \\frac{(O_j-E_j)^2}{E_j}$. The associated $p$-value is computed under a chi-square distribution with $B-1$ degrees of freedom under $\\mathcal{H}_0$.\n\n- Sample mean test: Let $\\bar{X}=\\frac{1}{N}\\sum_{n=1}^N x_n$. Under $\\mathcal{H}_0$, by the Central Limit Theorem, $Z=\\sqrt{12N}\\,(\\bar{X}-\\tfrac{1}{2})$ is approximately standard normal. The two-sided $p$-value is computed from the standard normal distribution.\n\nDefine a pass criterion threshold $\\varepsilon$ with $0<\\varepsilon<\\tfrac{1}{2}$, and say that the statistical battery passes if and only if all three $p$-values lie strictly within the open interval $(\\varepsilon,1-\\varepsilon)$.\n\nIndependently of the above tests, define a physics-motivated MC estimator of $\\pi$ via the classical dart-throwing method on the unit square. Using the same PRNG and the same seed $s$, form $M$ consecutive pairs from a stream $(x_1,x_2,\\dots)$ as $(x_{2k-1},x_{2k})$ for $k=1,\\dots,M$. Define\n$$\n\\widehat{\\pi} = 4\\, \\frac{1}{M}\\sum_{k=1}^M \\mathbf{1}\\!\\left\\{ x_{2k-1}^2 + x_{2k}^2 \\le 1 \\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Given an absolute error tolerance $\\tau>0$, declare that the MC simulation fails if and only if $|\\widehat{\\pi} - \\pi| > \\tau$. All angles involved are implicitly in radians through the use of $\\pi$; no explicit angles are computed.\n\nYour program must implement a single deterministic PRNG design of your choosing and evaluate it on the following test suite. For each test case, the program must compute a boolean value that is true if and only if the PRNG passes the statistical battery and fails the MC simulation, and false otherwise.\n\nThe test suite consists of the following parameter sets $(s, N, B, M, \\varepsilon, \\tau)$:\n\n- Case $1$: $(s, N, B, M, \\varepsilon, \\tau) = (123456789, 100000, 100, 50000, 10^{-9}, 10^{-2})$.\n- Case $2$: $(s, N, B, M, \\varepsilon, \\tau) = (0, 50000, 50, 20000, 10^{-9}, 10^{-2})$.\n- Case $3$: $(s, N, B, M, \\varepsilon, \\tau) = (987654321098765432, 120000, 200, 60000, 10^{-9}, 10^{-2})$.\n\nYour program should produce a single line of output containing the three boolean results for the cases above as a comma-separated list enclosed in square brackets (for example, `[True,False,True]`). No other output should be produced. All reported values are unitless; no physical units are required in the output. All internal computations must be deterministic and require no user input.", "solution": "The problem as stated is valid. It is a well-posed and scientifically grounded exercise in computational physics that probes the critical distinction between one-dimensional and multi-dimensional properties of pseudorandom number sequences. An unsophisticated battery of one-dimensional statistical tests is insufficient to certify a generator for multi-dimensional applications, such as Monte Carlo simulations. We shall construct a generator that is deliberately flawed in two dimensions, yet whose one-dimensional marginal distribution remains uniform, thereby satisfying the stated criteria.\n\nThe core of the problem is to design a single deterministic Pseudorandom Number Generator (PRNG) that, for a given seed $s$, produces a sequence $\\{x_n\\}$ that simultaneously:\n$1$. Passes a statistical battery of three tests (Kolmogorov-Smirnov, Chi-square, sample mean), meaning the $p$-value for each test falls within the interval $(\\varepsilon, 1-\\varepsilon)$.\n$2$. Fails a Monte Carlo (MC) estimation of $\\pi$, meaning the estimated value $\\widehat{\\pi}$ differs from the true value $\\pi$ by more than a tolerance $\\tau$.\n\nWe will construct a PRNG with a specific, fatal correlation between adjacent numbers. Let a high-quality, cryptographically secure PRNG, such as the PCG64 generator provided by the `numpy` library, be used as a base source of randomness. Let this base generator be seeded with the given integer seed $s$ to produce a sequence of i.i.d. values $\\{u_k\\}$ from a $\\mathrm{Uniform}(0,1)$ distribution.\n\nOur flawed PRNG will then construct the target sequence $\\{x_n\\}$ by forming pairs:\n$$ x_{2k-1} = u_k $$\n$$ x_{2k} = 1 - u_k $$\nfor $k = 1, 2, 3, \\dots$.\n\nLet us analyze the properties of this sequence $\\{x_n\\}$.\n\nFirst, consider the one-dimensional statistical properties. The sequence $\\{x_n\\}$ is a mixture of two sequences, $\\{u_k\\}$ and $\\{1-u_k\\}$. If $u_k \\sim \\mathrm{Uniform}(0,1)$, then it is a basic result of probability theory that the transformed variable $1-u_k$ is also distributed as $\\mathrm{Uniform}(0,1)$. The combined sequence $\\{x_n\\}$ therefore has a marginal cumulative distribution function that is an equal-weighted average of the uniform CDF with itself, which is simply the uniform CDF. Thus, the marginal distribution of $\\{x_n\\}$ is $\\mathrm{Uniform}(0,1)$. Consequently, it is expected to appear uniform to any one-dimensional test.\n- The **Kolmogorov-Smirnov test** compares the empirical CDF of $\\{x_n\\}_{n=1}^N$ to the uniform CDF $F(x)=x$. As the marginal distribution is uniform, the empirical CDF will converge to the true CDF, and the test is expected to pass with a high $p$-value.\n- The **Chi-square test** examines the frequency of numbers falling into $B$ bins. A uniform marginal distribution implies that counts should be evenly distributed, so this test is also expected to pass.\n- The **Sample mean test** relies on the mean of the sequence. The expectation is $E[x_n] = \\frac{1}{2} E[u_k] + \\frac{1}{2} E[1-u_k] = \\frac{1}{2}(\\frac{1}{2}) + \\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{2}$. The sample mean $\\bar{X}$ will converge to $\\frac{1}{2}$, and the test statistic $Z=\\sqrt{12N}\\,(\\bar{X}-\\tfrac{1}{2})$ will be close to $0$, yielding a $p$-value close to $1$.\nGiven the extremely permissive pass-fail threshold of $\\varepsilon=10^{-9}$, it is virtually certain that all three statistical tests will pass.\n\nSecond, consider the two-dimensional Monte Carlo simulation. The method estimates $\\pi$ by sampling points $(x,y)$ in the unit square and finding the fraction that falls within the unit circle. Our generator produces pairs $(x_{2k-1}, x_{2k}) = (u_k, 1-u_k)$. These points are not uniformly distributed in the unit square; they all lie perfectly on the line segment $y = 1-x$ for $x \\in (0,1)$. This is a catastrophic failure of two-dimensional uniformity.\n\nThe MC estimator is $\\widehat{\\pi} = 4\\, \\frac{1}{M}\\sum_{k=1}^M \\mathbf{1}\\!\\left\\{ x_{2k-1}^2 + x_{2k}^2 \\le 1 \\right\\}$. Substituting our correlated pairs, the condition inside the indicator function becomes:\n$$ u_k^2 + (1-u_k)^2 \\le 1 $$\nExpanding the expression gives:\n$$ u_k^2 + 1 - 2u_k + u_k^2 \\le 1 $$\n$$ 2u_k^2 - 2u_k \\le 0 $$\n$$ 2u_k(u_k - 1) \\le 0 $$\nSince the base generator produces $u_k \\in [0,1)$, the term $u_k$ is non-negative and the term $(u_k-1)$ is non-positive. Their product is therefore always non-positive, and the inequality is always satisfied.\n\nEvery single pair $(x_{2k-1}, x_{2k})$ generated will be counted as a \"hit\" inside the circle. The sum in the estimator becomes $\\sum_{k=1}^M 1 = M$. The estimator for $\\pi$ will therefore be:\n$$ \\widehat{\\pi} = 4 \\frac{M}{M} = 4 $$\nThe MC simulation fails if $|\\widehat{\\pi} - \\pi| > \\tau$. In our case, this is $|4 - \\pi| > 10^{-2}$. Since $\\pi \\approx 3.14159$, the absolute error is $|4 - \\pi| \\approx 0.8584$, which is significantly larger than the tolerance $\\tau=10^{-2}$. The MC simulation is thus guaranteed to fail.\n\nThe final boolean result for each test case is true if and only if the statistical battery passes (which we expect) and the MC simulation fails (which is guaranteed). The implementation will proceed by generating the sequence as described, applying the specified statistical tests using functions from the `scipy.stats` library, and calculating the MC simulation result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest, chisquare, norm\n\ndef solve():\n    \"\"\"\n    Implements a flawed PRNG, evaluates it against a statistical battery and a\n    Monte Carlo simulation, and reports the combined outcome for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (s, N, B, M, epsilon, tau)\n        (123456789, 100000, 100, 50000, 1e-9, 1e-2),\n        (0, 50000, 50, 20000, 1e-9, 1e-2),\n        (987654321098765432, 120000, 200, 60000, 1e-9, 1e-2),\n    ]\n\n    results = []\n    for s, N, B, M, epsilon, tau in test_cases:\n        # Step 0: Determine total sequence length required.\n        # Length must be sufficient for both tests and MC simulation, and be even for pairing.\n        L = max(N, 2 * M)\n        if L % 2 != 0:\n            L += 1\n\n        # Step 1: Implement the PRNG with a 2D flaw.\n        # Use numpy's default PRNG (PCG64) as the base generator.\n        rng = np.random.default_rng(s)\n        # Generate L/2 base numbers.\n        base_rands = rng.random(size=L // 2)\n        # Construct the flawed sequence x_n where x_2k = 1 - x_{2k-1}.\n        x_sequence = np.empty(L, dtype=np.float64)\n        x_sequence[0::2] = base_rands\n        x_sequence[1::2] = 1.0 - base_rands\n\n        # Step 2: Perform the statistical battery on the first N numbers.\n        x_for_tests = x_sequence[:N]\n\n        # 2a: One-sample Kolmogorov–Smirnov test.\n        # Test against the standard uniform distribution on (0,1).\n        ks_result = kstest(x_for_tests, 'uniform')\n        p_ks = ks_result.pvalue\n\n        # 2b: Chi-square test for equal-probability bins.\n        observed_counts, _ = np.histogram(x_for_tests, bins=B, range=(0.0, 1.0))\n        # With f_exp not provided, chisquare computes it as sum(f_obs)/len(f_obs),\n        # which is N/B, as required.\n        chi2_result = chisquare(f_obs=observed_counts)\n        p_chi2 = chi2_result.pvalue\n\n        # 2c: Sample mean test.\n        mean_val = np.mean(x_for_tests)\n        z_stat = np.sqrt(12.0 * N) * (mean_val - 0.5)\n        # Two-sided p-value from standard normal survival function.\n        p_mean = 2.0 * norm.sf(np.abs(z_stat))\n\n        # Check if the statistical battery passes.\n        stat_pass = (p_ks > epsilon and p_ks < 1.0 - epsilon) and \\\n                    (p_chi2 > epsilon and p_chi2 < 1.0 - epsilon) and \\\n                    (p_mean > epsilon and p_mean < 1.0 - epsilon)\n\n        # Step 3: Perform the Monte Carlo simulation on the first 2M numbers.\n        x_for_mc = x_sequence[:(2 * M)]\n        x_coords = x_for_mc[0::2]\n        y_coords = x_for_mc[1::2]\n\n        # Count \"hits\" inside the unit circle.\n        hits = np.sum(x_coords**2 + y_coords**2 <= 1.0)\n        # Calculate pi estimate.\n        pi_estimate = 4.0 * hits / M\n\n        # Check if the MC simulation fails.\n        mc_fail = np.abs(pi_estimate - np.pi) > tau\n\n        # Step 4: Determine the final boolean result for the case.\n        # The result is True iff the battery passes AND the simulation fails.\n        final_result = stat_pass and mc_fail\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2442681"}]}