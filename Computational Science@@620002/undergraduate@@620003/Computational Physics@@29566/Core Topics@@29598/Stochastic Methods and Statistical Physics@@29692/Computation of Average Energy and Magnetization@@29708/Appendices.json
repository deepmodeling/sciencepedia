{"hands_on_practices": [{"introduction": "This first exercise provides foundational practice in computing the macroscopic properties of a system from its microscopic details. By performing an exact enumeration of all possible states for a small 2D Ising model, you will calculate the average energy and magnetization directly from the partition function. This \"brute-force\" method, while only feasible for small systems, is a crucial pedagogical tool for understanding the core principles of the canonical ensemble and provides an exact benchmark for more advanced simulation techniques.[@problem_id:2380966]", "problem": "Consider a two-dimensional Ising model of $N=L_x L_y$ spins $s_i \\in \\{+1,-1\\}$ on a rectangular lattice of size $L_x \\times L_y$ with periodic boundary conditions in both directions. The Hamiltonian is defined by\n$$\nE(\\mathbf{s}) \\equiv -J \\sum_{\\langle i,j\\rangle} s_i s_j \\;-\\; \\sum_{i=1}^{N} h(x_i) s_i,\n$$\nwhere $\\langle i,j\\rangle$ denotes nearest-neighbor pairs with each bond counted exactly once, $J0$ is the ferromagnetic exchange constant, and the site-dependent external magnetic field is\n$$\nh(x) \\equiv H_0 \\,\\sin\\!\\left(\\frac{2\\pi x}{L_x}\\right),\n$$\nwith $x\\in \\{0,1,\\dots,L_x-1\\}$ the integer $x$-coordinate of a site. The argument of the sine function must be taken in radians. Use dimensionless units with Boltzmann constant set to $k_{\\mathrm{B}}=1$, energies measured in units of $J$, and temperature $T$ measured in units of $J$. The canonical ensemble at temperature $T$ defines the partition function\n$$\nZ \\equiv \\sum_{\\mathbf{s}} e^{-\\beta E(\\mathbf{s})}, \\quad \\beta \\equiv \\frac{1}{T},\n$$\nand ensemble averages\n$$\n\\langle E \\rangle \\equiv \\frac{1}{Z}\\sum_{\\mathbf{s}} E(\\mathbf{s})\\, e^{-\\beta E(\\mathbf{s})}, \\qquad\n\\langle M \\rangle \\equiv \\frac{1}{Z}\\sum_{\\mathbf{s}} \\left(\\sum_{i=1}^{N} s_i\\right)\\, e^{-\\beta E(\\mathbf{s})}.\n$$\nCompute, for each specified parameter set, the average energy per spin $\\langle E\\rangle/N$ and the average total magnetization per spin $\\langle M\\rangle/N$ as defined above.\n\nAll quantities are dimensionless, so no physical units are required beyond the above specification. The angle in $h(x)$ must be interpreted in radians. Your program must produce numerical values rounded to six decimal places.\n\nTest suite (each case is a tuple $(L_x,L_y,J,H_0,T)$):\n- Case A: $(L_x,L_y,J,H_0,T)=(\\,3,\\,3,\\,1,\\,0,\\,2.5\\,)$.\n- Case B: $(L_x,L_y,J,H_0,T)=(\\,4,\\,3,\\,1,\\,0.8,\\,1.8\\,)$.\n- Case C: $(L_x,L_y,J,H_0,T)=(\\,2,\\,3,\\,1,\\,5.0,\\,1.0\\,)$.\n- Case D: $(L_x,L_y,J,H_0,T)=(\\,3,\\,3,\\,1,\\,0.7,\\,\\infty\\,)$, where $T=\\infty$ denotes the limit $\\beta=0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case, output the two floats in the following order: first $\\langle E\\rangle/N$, then $\\langle M\\rangle/N$, each rounded to six decimal places.\n- Concatenate the results for Cases A, B, C, and D in that order into a single flat list. For example, the output format must be\n$$\n[\\langle E\\rangle/N\\text{(A)},\\langle M\\rangle/N\\text{(A)},\\langle E\\rangle/N\\text{(B)},\\langle M\\rangle/N\\text{(B)},\\langle E\\rangle/N\\text{(C)},\\langle M\\rangle/N\\text{(C)},\\langle E\\rangle/N\\text{(D)},\\langle M\\rangle/N\\text{(D)}].\n$$", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It constitutes a standard computational problem in statistical physics.\n\nThe solution proceeds by direct computation of the canonical ensemble averages using the method of exact enumeration. The system size $N = L_x L_y$ for all test cases is small enough (maximum $N=12$) to make the summation over all $2^N$ microstates computationally feasible.\n\nThe foundation of the calculation lies in the principles of the canonical ensemble in statistical mechanics. The probability of finding the system in a specific microstate $\\mathbf{s}$ (a configuration of spins) with energy $E(\\mathbf{s})$ is given by the Boltzmann distribution:\n$$\nP(\\mathbf{s}) = \\frac{e^{-\\beta E(\\mathbf{s})}}{Z}\n$$\nwhere $\\beta = 1/T$ is the inverse temperature (with Boltzmann constant $k_B=1$) and $Z$ is the partition function, which acts as a normalization constant ensuring that probabilities sum to $1$. It is defined as the sum over all possible microstates:\n$$\nZ = \\sum_{\\mathbf{s}} e^{-\\beta E(\\mathbf{s})}\n$$\nThe thermal average (or expectation value) of any observable quantity $O(\\mathbf{s})$ is then calculated as:\n$$\n\\langle O \\rangle = \\sum_{\\mathbf{s}} O(\\mathbf{s}) P(\\mathbf{s}) = \\frac{1}{Z} \\sum_{\\mathbf{s}} O(\\mathbf{s}) e^{-\\beta E(\\mathbf{s})}\n$$\nThe specific quantities required are the average energy $\\langle E \\rangle$ and the average total magnetization $\\langle M \\rangle = \\langle \\sum_i s_i \\rangle$.\n\nThe Hamiltonian $E(\\mathbf{s})$ for the two-dimensional Ising model on an $L_x \\times L_y$ lattice with periodic boundary conditions is given as:\n$$\nE(\\mathbf{s}) = -J \\sum_{\\langle i,j\\rangle} s_i s_j - \\sum_{i=1}^{N} h(x_i) s_i\n$$\nThe first term is the ferromagnetic interaction energy, where $J0$ is the coupling constant and the sum is over all unique nearest-neighbor pairs $\\langle i,j \\rangle$. The second term is the energy due to a site-dependent external magnetic field $h(x) = H_0 \\sin(2\\pi x/L_x)$, where $x \\in \\{0, 1, \\dots, L_x-1\\}$ is the integer x-coordinate of a site.\n\nThe computational algorithm is as follows:\n$1$. For each test case defined by the parameters $(L_x, L_y, J, H_0, T)$, the total number of spins is $N = L_x L_y$. We also compute the inverse temperature $\\beta = 1/T$.\n$2$. We iterate through all $2^N$ possible spin configurations. Each configuration $\\mathbf{s}$ can be uniquely mapped to an integer from $0$ to $2^N-1$. The binary representation of the integer is used to set the spin values $s_i \\in \\{+1, -1\\}$.\n$3$. For each configuration $\\mathbf{s}$:\n    a. The energy $E(\\mathbf{s})$ is calculated. The interaction term $\\sum_{\\langle i,j \\rangle} s_i s_j$ is computed efficiently by summing over each site $i$ the products of its spin $s_i$ with the spins of its 'right' and 'down' neighbors, which accounts for the periodic boundary conditions and ensures each bond is counted exactly once. The external field energy is computed by summing $-h(x_i)s_i$ over all sites.\n    b. The total magnetization $M(\\mathbf{s}) = \\sum_i s_i$ is calculated.\n    c. The Boltzmann factor $w = e^{-\\beta E(\\mathbf{s})}$ is determined.\n    d. Three sums are updated: $S_Z = \\sum w$, $S_E = \\sum E(\\mathbf{s})w$, and $S_M = \\sum M(\\mathbf{s})w$, where the sums are over all configurations visited so far.\n$4$. After iterating through all $2^N$ configurations, the ensemble averages are computed:\n$$\n\\langle E \\rangle = \\frac{S_E}{S_Z} \\qquad \\text{and} \\qquad \\langle M \\rangle = \\frac{S_M}{S_Z}\n$$\nFinally, these are converted to per-spin quantities by dividing by the total number of spins, $N$.\n\nA special case arises for $T \\to \\infty$ (Case D), which corresponds to $\\beta \\to 0$. In this limit, the Boltzmann factor $e^{-\\beta E(\\mathbf{s})} \\to e^0 = 1$ for all configurations, regardless of their energy. All $2^N$ states become equally probable. Due to the symmetry of the spin states ($s_i = \\pm 1$), the thermal average of any single spin is zero: $\\langle s_i \\rangle = (1/2)(+1) + (1/2)(-1) = 0$. Similarly, for any distinct pair of spins, $\\langle s_i s_j \\rangle = \\langle s_i \\rangle \\langle s_j \\rangle = 0$.\nThe average magnetization is $\\langle M \\rangle = \\sum_i \\langle s_i \\rangle = 0$.\nThe average energy is $\\langle E \\rangle = -J \\sum_{\\langle i,j\\rangle} \\langle s_i s_j \\rangle - \\sum_i h(x_i) \\langle s_i \\rangle = 0$.\nThus, for Case D, both the average energy per spin and the average magnetization per spin are exactly $0$. This analytical result is used directly.\n\nThe final implementation translates this exact enumeration algorithm into a Python script utilizing the `numpy` library for efficient array manipulations, particularly for the energy calculation with periodic boundaries. For each case, the computed values of $\\langle E \\rangle/N$ and $\\langle M \\rangle/N$ are rounded to six decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the average energy and magnetization per spin for a 2D Ising model\n    on a rectangular lattice with periodic boundary conditions using exact enumeration.\n    \"\"\"\n    # Test suite: (Lx, Ly, J, H0, T)\n    test_cases = [\n        (3, 3, 1.0, 0.0, 2.5),  # Case A\n        (4, 3, 1.0, 0.8, 1.8),  # Case B\n        (2, 3, 1.0, 5.0, 1.0),  # Case C\n        (3, 3, 1.0, 0.7, float('inf')),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        Lx, Ly, J, H0, T = case\n        N = Lx * Ly\n\n        # Handle the infinite temperature limit analytically\n        if T == float('inf'):\n            E_per_spin = 0.0\n            M_per_spin = 0.0\n            results.extend([E_per_spin, M_per_spin])\n            continue\n\n        beta = 1.0 / T\n\n        # Pre-calculate the site-dependent magnetic field grid\n        x_coords = np.arange(Lx)\n        h_vals = H0 * np.sin(2.0 * np.pi * x_coords / Lx)\n        H_grid = np.tile(h_vals, (Ly, 1))\n\n        # Initialize sums for partition function and observables\n        Z_sum = 0.0\n        E_times_Z_sum = 0.0\n        M_times_Z_sum = 0.0\n\n        # Iterate through all 2^N spin configurations\n        for i in range(2**N):\n            # Generate the spin configuration from the integer's binary representation\n            # '0' bit maps to spin -1, '1' bit maps to spin +1\n            bits = bin(i)[2:].zfill(N)\n            spins_1d = np.array([int(b) * 2 - 1 for b in bits])\n            s = spins_1d.reshape((Ly, Lx))\n\n            # Calculate the total energy E(s) for the current configuration\n            # Interaction energy: sum over right and down neighbors to count each bond once\n            E_int = -J * np.sum(s * np.roll(s, -1, axis=1) + s * np.roll(s, -1, axis=0))\n            \n            # External field energy\n            E_field = -np.sum(s * H_grid)\n            \n            E = E_int + E_field\n\n            # Calculate the total magnetization M(s)\n            M = np.sum(s)\n\n            # Calculate the Boltzmann weight and update the sums\n            # Use a high-precision float to avoid overflow/underflow issues\n            boltzmann_weight = np.exp(-beta * E, dtype=np.float64)\n            \n            Z_sum += boltzmann_weight\n            E_times_Z_sum += E * boltzmann_weight\n            M_times_Z_sum += M * boltzmann_weight\n\n        # Calculate the ensemble averages\n        avg_E = E_times_Z_sum / Z_sum\n        avg_M = M_times_Z_sum / Z_sum\n\n        # Normalize to get per-spin quantities\n        E_per_spin = avg_E / N\n        M_per_spin = avg_M / N\n\n        results.append(round(E_per_spin, 6))\n        results.append(round(M_per_spin, 6))\n\n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2380966"}, {"introduction": "The power of statistical physics models extends far beyond traditional physics, and this practice demonstrates this by reframing an Ising-like system as a model for consensus-building in a distributed network. You will simulate the system's evolution over time, where individual 'nodes' update their 'decision' based on local information, a process analogous to spin dynamics.[@problem_id:2380973] This exercise introduces asynchronous dynamics and time-averaging, highlighting how these simple models can provide insights into complex collective behaviors in various fields.", "problem": "You are given a network of nodes, each with a binary state representing a decision, denoted by spins $s_i(t) \\in \\{-1,+1\\}$ for node index $i$ at discrete time $t$. The network is modeled as an undirected simple graph with node set $\\{0,1,\\dots,N-1\\}$ and edge set $\\mathcal{E} \\subset \\{(i,j) \\mid 0 \\le i  j \\le N-1\\}$. The microscopic configuration of the system at time $t$ is the vector $\\mathbf{s}(t) = (s_0(t),\\dots,s_{N-1}(t))$. The instantaneous energy $E(t)$ of the configuration is defined as the number of disagreeing connections (edges whose endpoints have opposite spins), given by\n$$\nE(t) = \\sum_{(i,j) \\in \\mathcal{E}} \\frac{1 - s_i(t) s_j(t)}{2}.\n$$\nThe instantaneous magnetization $m(t)$ is defined as the arithmetic mean of the spins,\n$$\nm(t) = \\frac{1}{N} \\sum_{i=0}^{N-1} s_i(t).\n$$\nThe dynamics of the system is specified as follows. At each discrete update step $t = 1,2,\\dots$, one node index $i$ is selected uniformly at random from $\\{0,1,\\dots,N-1\\}$. Let $N(i)$ be the set of neighbors of node $i$ in the graph. Define the local neighbor sum\n$$\nh_i(t) = \\sum_{j \\in N(i)} s_j(t-1).\n$$\nThe updated spin $s_i(t)$ is set by the rule\n$$\ns_i(t) = \n\\begin{cases}\n+1,  \\text{if } h_i(t)  0,\\\\\n-1,  \\text{if } h_i(t)  0,\\\\\n\\xi_t,  \\text{if } h_i(t) = 0,\n\\end{cases}\n$$\nwhere $\\xi_t$ is an independent random variable taking values $+1$ and $-1$ with equal probability $1/2$. All other spins remain unchanged at time $t$, i.e., $s_k(t) = s_k(t-1)$ for all $k \\ne i$. The update schedule is asynchronous, meaning exactly one node is updated per discrete time step.\n\nFor a given finite number of burn-in steps $T_{\\mathrm{burn}}$ and measurement steps $T_{\\mathrm{meas}}$, define the time-averaged energy $\\overline{E}$ and time-averaged magnetization $\\overline{m}$ over the measurement window as\n$$\n\\overline{E} = \\frac{1}{T_{\\mathrm{meas}}} \\sum_{t=T_{\\mathrm{burn}}+1}^{T_{\\mathrm{burn}}+T_{\\mathrm{meas}}} E(t), \\quad\n\\overline{m} = \\frac{1}{T_{\\mathrm{meas}}} \\sum_{t=T_{\\mathrm{burn}}+1}^{T_{\\mathrm{burn}}+T_{\\mathrm{meas}}} m(t).\n$$\nAll random choices (node selection and tie-breaking when $h_i(t)=0$) must be generated by a pseudorandom number generator initialized with the specified nonnegative integer seed $S$ for reproducibility.\n\nYour task is to write a complete program that, for each test case below, initializes the system at time $t=0$ with a specified spin vector $\\mathbf{s}(0)$, then performs $T_{\\mathrm{burn}} + T_{\\mathrm{meas}}$ asynchronous update steps according to the above rule, computes $E(t)$ and $m(t)$ after each step $t \\ge 1$, and finally reports $\\overline{E}$ and $\\overline{m}$ for that test case.\n\nTest suite (each case specifies $N$, $\\mathcal{E}$, $\\mathbf{s}(0)$, $T_{\\mathrm{burn}}$, $T_{\\mathrm{meas}}$, and $S$):\n- Case $1$ (complete graph on $4$ nodes):\n  - $N = 4$,\n  - $\\mathcal{E} = \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\\}$,\n  - $\\mathbf{s}(0) = [ +1, -1, +1, -1 ]$,\n  - $T_{\\mathrm{burn}} = 20$,\n  - $T_{\\mathrm{meas}} = 50$,\n  - $S = 1$.\n- Case $2$ (ring on $10$ nodes):\n  - $N = 10$,\n  - $\\mathcal{E} = \\{(i,(i+1) \\bmod 10) \\mid i \\in \\{0,1,\\dots,9\\}\\}$,\n  - $\\mathbf{s}(0) = [ +1, -1, +1, -1, +1, -1, +1, -1, +1, -1 ]$,\n  - $T_{\\mathrm{burn}} = 200$,\n  - $T_{\\mathrm{meas}} = 400$,\n  - $S = 2$.\n- Case $3$ (star on $6$ nodes with center at node $0$):\n  - $N = 6$,\n  - $\\mathcal{E} = \\{(0,1),(0,2),(0,3),(0,4),(0,5)\\}$,\n  - $\\mathbf{s}(0) = [ +1, +1, -1, -1, -1, -1 ]$,\n  - $T_{\\mathrm{burn}} = 100$,\n  - $T_{\\mathrm{meas}} = 200$,\n  - $S = 3$.\n- Case $4$ (path on $5$ nodes):\n  - $N = 5$,\n  - $\\mathcal{E} = \\{(0,1),(1,2),(2,3),(3,4)\\}$,\n  - $\\mathbf{s}(0) = [ +1, +1, +1, +1, +1 ]$,\n  - $T_{\\mathrm{burn}} = 10$,\n  - $T_{\\mathrm{meas}} = 20$,\n  - $S = 4$.\n\nProgram output specification:\n- For each case, compute $\\overline{E}$ and $\\overline{m}$ as defined above.\n- Round each reported value to $6$ decimal places.\n- The final program output must be a single line containing a comma-separated list of the $8$ numbers in the order $[\\overline{E}_1,\\overline{m}_1,\\overline{E}_2,\\overline{m}_2,\\overline{E}_3,\\overline{m}_3,\\overline{E}_4,\\overline{m}_4]$, enclosed in square brackets and with no spaces.", "solution": "The problem presented is a well-defined computational exercise in statistical physics, specifically the simulation of a discrete-time stochastic process on a graph. The model is a variant of the voter model, where nodes (spins) adopt the state of the local majority. Its dynamics are related to a zero-temperature update algorithm for an Ising-like system. The problem is scientifically grounded, internally consistent, and requires a direct simulation to find the required time-averaged observables.\n\nThe system is defined by a set of $N$ nodes with binary spins $s_i \\in \\{-1, +1\\}$, structured as an undirected graph $G = (V, \\mathcal{E})$ where $V = \\{0, 1, \\dots, N-1\\}$. The configuration of the system at a discrete time $t$ is the vector $\\mathbf{s}(t) = (s_0(t), \\dots, s_{N-1}(t))$.\n\nThe instantaneous energy $E(t)$ is defined as the count of edges connecting nodes with opposing spins:\n$$\nE(t) = \\sum_{(i,j) \\in \\mathcal{E}} \\frac{1 - s_i(t) s_j(t)}{2}\n$$\nThis expression evaluates to $1$ if $s_i(t) \\ne s_j(t)$ and $0$ otherwise, thus correctly counting the number of 'disagreeing' or 'antiferromagnetic' links. The instantaneous magnetization $m(t)$ is the average spin value across the network:\n$$\nm(t) = \\frac{1}{N} \\sum_{i=0}^{N-1} s_i(t)\n$$\n\nThe system evolves according to an asynchronous update schedule. At each time step $t \\ge 1$, a single node $i$ is chosen uniformly at random for a potential state change. The update rule is deterministic, conditioned on the states of the node's neighbors, $N(i)$, at the previous time step, $t-1$. The new state $s_i(t)$ is determined by the sign of the local neighbor sum, $h_i(t)$:\n$$\nh_i(t) = \\sum_{j \\in N(i)} s_j(t-1)\n$$\nThe spin $s_i(t)$ is updated as follows:\n$$\ns_i(t) = \n\\begin{cases}\n+1,  \\text{if } h_i(t)  0 \\\\\n-1,  \\text{if } h_i(t)  0 \\\\\n\\xi_t,  \\text{if } h_i(t) = 0\n\\end{cases}\n$$\nwhere $\\xi_t$ is a random variable drawn from $\\{-1, +1\\}$ with equal probability, $P(\\xi_t = 1) = P(\\xi_t = -1) = 1/2$. All other spins remain unchanged: $s_k(t) = s_k(t-1)$ for $k \\ne i$. This update rule causes a spin to align with the local majority opinion, which corresponds to a local energy minimization step. The stochasticity enters through the random selection of the node to be updated and the tie-breaking rule when $h_i(t) = 0$.\n\nThe task is to compute the time-averaged energy $\\overline{E}$ and magnetization $\\overline{m}$ over a specified measurement window. The simulation runs for a total of $T_{\\mathrm{burn}} + T_{\\mathrm{meas}}$ steps. The first $T_{\\mathrm{burn}}$ steps constitute the \"burn-in\" or thermalization phase, during which the system evolves towards a statistical steady state and no measurements are taken. The subsequent $T_{\\mathrm{meas}}$ steps form the measurement phase. The averages are defined as:\n$$\n\\overline{E} = \\frac{1}{T_{\\mathrm{meas}}} \\sum_{t=T_{\\mathrm{burn}}+1}^{T_{\\mathrm{burn}}+T_{\\mathrm{meas}}} E(t), \\quad\n\\overline{m} = \\frac{1}{T_{\\mathrm{meas}}} \\sum_{t=T_{\\mathrm{burn}}+1}^{T_{\\mathrm{burn}}+T_{\\mathrm{meas}}} m(t)\n$$\nAll random elements of the simulation must be controlled by a pseudorandom number generator (PRNG) initialized with a specific seed $S$ to ensure reproducibility.\n\nThe computational procedure for each test case is as follows:\n$1$. Initialize the PRNG with the given seed $S$.\n$2$. Represent the graph using an adjacency list, which is efficient for accessing neighbors. The initial spin configuration $\\mathbf{s}(0)$ is stored in a numerical array.\n$3$. Initialize cumulative sums for energy and magnetization, $\\Sigma E = 0$ and $\\Sigma m = 0$.\n$4$. Execute a loop for $t$ from $1$ to $T_{\\mathrm{burn}} + T_{\\mathrm{meas}}$:\n    a. Select a node index $i_{\\mathrm{update}}$ from $\\{0, \\dots, N-1\\}$ uniformly at random using the PRNG.\n    b. Calculate the local neighbor sum $h_{i_{\\mathrm{update}}}(t)$ using the spin states from time $t-1$.\n    c. Update the spin $s_{i_{\\mathrm{update}}}(t)$ according to the sign of $h_{i_{\\mathrm{update}}}(t)$, using the PRNG for tie-breaking if $h_{i_{\\mathrm{update}}}(t)=0$. The spin array is modified in-place to reflect the state $\\mathbf{s}(t)$.\n    d. If the current step $t$ is in the measurement window (i.e., $t  T_{\\mathrm{burn}}$), compute the instantaneous $E(t)$ and $m(t)$ from the newly updated state $\\mathbf{s}(t)$.\n    e. Add these instantaneous values to the respective cumulative sums, $\\Sigma E$ and $\\Sigma m$.\n$5$. After the loop completes, calculate the averages: $\\overline{E} = \\Sigma E / T_{\\mathrm{meas}}$ and $\\overline{m} = \\Sigma m / T_{\\mathrm{meas}}$.\n$6$. Round the final averages to $6$ decimal places as required.\n\nThis algorithm is implemented for each provided test case, and the results are aggregated into the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n\n    def solve_case(N, edges, s0, T_burn, T_meas, S):\n        \"\"\"\n        Solves a single test case of the spin network simulation.\n\n        Args:\n            N (int): Number of nodes.\n            edges (list of tuples): List of edges in the graph.\n            s0 (list of int): Initial spin configuration.\n            T_burn (int): Number of burn-in steps.\n            T_meas (int): Number of measurement steps.\n            S (int): Seed for the random number generator.\n\n        Returns:\n            tuple: A tuple containing the time-averaged energy and magnetization,\n                   each rounded to 6 decimal places.\n        \"\"\"\n        # 1. Initialize the PRNG with the given seed S.\n        rng = np.random.default_rng(seed=S)\n\n        # 2. Represent the graph using an adjacency list.\n        adj = [[] for _ in range(N)]\n        for i, j in edges:\n            adj[i].append(j)\n            adj[j].append(i)\n\n        # The spin configuration is stored in a NumPy array.\n        s = np.array(s0, dtype=np.int8)\n\n        # 3. Initialize cumulative sums for energy and magnetization.\n        total_E = 0.0\n        total_m = 0.0\n        \n        T_total = T_burn + T_meas\n\n        # 4. Execute the simulation loop.\n        for t in range(1, T_total + 1):\n            # a. Select a node to update uniformly at random.\n            i_update = rng.integers(N)\n\n            # b. Calculate the local neighbor sum h_i.\n            h_i = 0\n            for neighbor in adj[i_update]:\n                h_i += s[neighbor]\n\n            # c. Update the spin s_i based on the sign of h_i.\n            if h_i  0:\n                s[i_update] = 1\n            elif h_i  0:\n                s[i_update] = -1\n            else:  # h_i == 0, tie-breaking\n                s[i_update] = rng.choice([-1, 1])\n\n            # d. If in measurement phase, compute and accumulate observables.\n            if t  T_burn:\n                # Calculate instantaneous energy E(t): number of disagreeing connections.\n                # The formula (1 - s_i*s_j)/2 is equivalent to counting disagreeing pairs.\n                current_E = 0.5 * sum(1 - s[u] * s[v] for u, v in edges)\n                \n                # Calculate instantaneous magnetization m(t): mean of spins.\n                current_m = np.mean(s)\n\n                # e. Add to cumulative sums.\n                total_E += current_E\n                total_m += current_m\n\n        # 5. Calculate final averages.\n        avg_E = total_E / T_meas if T_meas  0 else 0\n        avg_m = total_m / T_meas if T_meas  0 else 0\n\n        # 6. Round results.\n        return round(avg_E, 6), round(avg_m, 6)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: complete graph on 4 nodes\n        {\n            \"N\": 4,\n            \"edges\": [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)],\n            \"s0\": [+1, -1, +1, -1],\n            \"T_burn\": 20,\n            \"T_meas\": 50,\n            \"S\": 1\n        },\n        # Case 2: ring on 10 nodes\n        {\n            \"N\": 10,\n            # Normalize edges to (i, j) with i  j\n            \"edges\": list(sorted({tuple(sorted(t)) for t in [(i, (i + 1) % 10) for i in range(10)]})),\n            \"s0\": [+1, -1, +1, -1, +1, -1, +1, -1, +1, -1],\n            \"T_burn\": 200,\n            \"T_meas\": 400,\n            \"S\": 2\n        },\n        # Case 3: star on 6 nodes\n        {\n            \"N\": 6,\n            \"edges\": [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)],\n            \"s0\": [+1, +1, -1, -1, -1, -1],\n            \"T_burn\": 100,\n            \"T_meas\": 200,\n            \"S\": 3\n        },\n        # Case 4: path on 5 nodes\n        {\n            \"N\": 5,\n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 4)],\n            \"s0\": [+1, +1, +1, +1, +1],\n            \"T_burn\": 10,\n            \"T_meas\": 20,\n            \"S\": 4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        E_avg, m_avg = solve_case(**case)\n        results.append(E_avg)\n        results.append(m_avg)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2380973"}, {"introduction": "To truly understand phase transitions, we must look beyond simple averages and analyze the fluctuations of thermodynamic quantities. This final practice introduces a powerful statistical tool, the fourth-order energy cumulant $\\kappa_4$, to probe the shape of the energy distribution. By applying this technique to synthetic data that mimics different types of transitions, you will learn how the sign of $\\kappa_4$ can serve as a potent indicator to distinguish between first-order and continuous (second-order) phase transitions.[@problem_id:2380998]", "problem": "Consider a lattice spin system with microstates indexed by an integer $i \\in \\{1,\\dots,N\\}$, where each microstate has an energy $E_i$ and a total magnetization $M_i$. For a set of $N$ statistically independent samples $\\{(E_i,M_i)\\}_{i=1}^N$, define the sample averages $\\langle E \\rangle_N = \\frac{1}{N} \\sum_{i=1}^N E_i$ and $\\langle |M| \\rangle_N = \\frac{1}{N} \\sum_{i=1}^N |M_i|$. The energy fluctuations are quantified by the central moments $\\mu_k = \\frac{1}{N}\\sum_{i=1}^N (E_i - \\langle E \\rangle_N)^k$ for integer $k \\ge 1$. The cumulant generating function of the energy is $K_E(t) = \\ln \\langle e^{t E} \\rangle$, and the $n$-th energy cumulant is defined by $\\kappa_n = \\left.\\frac{d^n}{dt^n} K_E(t) \\right|_{t=0}$. Your task is to derive a computable expression for the fourth-order energy cumulant $\\kappa_4$ purely in terms of empirical central moments up to order $4$, and then implement it numerically on synthetic datasets that mimic unimodal and bimodal energy distributions.\n\nStarting from the fundamental definitions above and without using any pre-provided shortcut formulas, do the following:\n- Derive a formula that expresses the fourth-order energy cumulant $\\kappa_4$ in terms of the central moments $\\mu_2$ and $\\mu_4$ computed from the samples $\\{E_i\\}_{i=1}^N$. Justify each step using the definition of the cumulant generating function and properties of moments and cumulants.\n- Explain, using first principles of probability, why a symmetric bimodal energy distribution near a phase coexistence point (typical of a first-order transition) differs in the sign of $\\kappa_4$ from a unimodal, approximately Gaussian energy distribution near a continuous critical point (typical of a second-order transition).\n- Implement a program that:\n  - Generates independent samples $(E_i,M_i)$ for each test case specified below.\n  - Computes the empirical averages $\\langle E \\rangle_N$ and $\\langle |M| \\rangle_N$.\n  - Computes the empirical central moments $\\mu_2$ and $\\mu_4$ for the energy and then the fourth-order energy cumulant $\\kappa_4$ from your derived expression.\n  - Classifies each case as “first-order-like” if $\\kappa_4  0$ and “second-order-like” otherwise. Encode this classification as an integer label $1$ for first-order-like and $2$ for second-order-like.\n  - Uses a fixed pseudorandom number generator seed equal to $12345$ for reproducibility.\n\nYou must implement the three synthetic test cases below. All distributions are to be sampled using independent Gaussian components, and all mixture weights must be equal to $1/2$ for the two-component mixtures. All numbers below are dimensionless and must be used exactly as given.\n\nTest Suite:\n- Case A (unimodal, “second-order-like” baseline): draw $N = 50000$ pairs $(E,M)$ with $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$ and $M \\sim \\mathcal{N}(\\mu_M, \\sigma_M^2)$, where $\\mu_E = 0.0$, $\\sigma_E = 1.0$, $\\mu_M = 0.0$, $\\sigma_M = 1.0$.\n- Case B (bimodal, “first-order-like” clear separation): draw $N = 50000$ pairs $(E,M)$ from an equal-weight mixture of two independent Gaussians for both $E$ and $M$. For $E$, the components are $\\mathcal{N}(-2.0, 0.2^2)$ and $\\mathcal{N}(+2.0, 0.2^2)$. For $M$, the components are $\\mathcal{N}(-1.0, 0.1^2)$ and $\\mathcal{N}(+1.0, 0.1^2)$.\n- Case C (bimodal, “first-order-like” finite-sample edge): draw $N = 1000$ pairs $(E,M)$ from an equal-weight mixture of two independent Gaussians for both $E$ and $M$. For $E$, the components are $\\mathcal{N}(-1.5, 0.5^2)$ and $\\mathcal{N}(+1.5, 0.5^2)$. For $M$, the components are $\\mathcal{N}(-0.5, 0.2^2)$ and $\\mathcal{N}(+0.5, 0.2^2)$.\n\nFor each case, your program must output a list of four quantities in the order: $[\\kappa_4, \\langle E \\rangle_N, \\langle |M| \\rangle_N, \\text{label}]$, where $\\kappa_4$, $\\langle E \\rangle_N$, and $\\langle |M| \\rangle_N$ are real numbers and $\\text{label}$ is an integer defined above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of lists enclosed in square brackets, exactly:\n[[k4_A,meanE_A,meanAbsM_A,label_A],[k4_B,meanE_B,meanAbsM_B,label_B],[k4_C,meanE_C,meanAbsM_C,label_C]]\nNo other text must be printed. No physical units are required; report all numerical results as pure real numbers without units and without percentage signs.", "solution": "The problem presents three tasks: first, the derivation of an expression for the fourth-order energy cumulant $\\kappa_4$ in terms of empirical central moments; second, a theoretical explanation for the sign of $\\kappa_4$ in distinguishing unimodal and bimodal energy distributions; and third, the numerical implementation of these concepts on synthetic datasets. The problem is well-posed and scientifically grounded in statistical mechanics and probability theory. We proceed with the solution.\n\n### Part 1: Derivation of the Fourth-Order Cumulant $\\kappa_4$\n\nThe relationship between cumulants and moments is derived from their respective generating functions. The moment generating function (MGF) of a random variable $E$ is defined as $M_E(t) = \\langle e^{tE} \\rangle$, where $\\langle \\cdot \\rangle$ denotes the expectation value. Expanding the exponential in a Taylor series and taking the expectation term-wise yields the MGF in terms of the raw moments $m_k = \\langle E^k \\rangle$:\n$$\nM_E(t) = \\left\\langle \\sum_{k=0}^{\\infty} \\frac{(tE)^k}{k!} \\right\\rangle = \\sum_{k=0}^{\\infty} \\frac{\\langle E^k \\rangle}{k!} t^k = \\sum_{k=0}^{\\infty} \\frac{m_k}{k!} t^k\n$$\nBy definition, $m_0 = 1$. The cumulant generating function (CGF) is defined as $K_E(t) = \\ln M_E(t)$. The $n$-th cumulant, $\\kappa_n$, is the coefficient of $\\frac{t^n}{n!}$ in the Taylor series expansion of $K_E(t)$ about $t=0$:\n$$\nK_E(t) = \\sum_{n=1}^{\\infty} \\frac{\\kappa_n}{n!} t^n\n$$\nTo derive the expression for $\\kappa_4$, we relate it to the central moments, $\\mu_k = \\langle (E - m_1)^k \\rangle$. A key property of cumulants is their shift-invariance for orders $n \\ge 2$. For a shifted variable $E' = E - c$, its CGF is $K_{E'}(t) = K_E(t) - ct$. Consequently, $\\kappa_1(E') = \\kappa_1(E) - c$, and $\\kappa_n(E') = \\kappa_n(E)$ for all $n \\ge 2$. This allows us to perform the derivation for a zero-mean variable $E' = E - m_1$, for which the raw moments are identical to the central moments of $E$, i.e., $\\langle(E')^k\\rangle = \\mu_k$. For this variable, $m_1' = \\mu_1 = 0$.\n\nThe MGF for the zero-mean variable $E'$ is:\n$$\nM_{E'}(t) = 1 + \\mu_1 \\cdot t + \\frac{\\mu_2}{2!} t^2 + \\frac{\\mu_3}{3!} t^3 + \\frac{\\mu_4}{4!} t^4 + O(t^5)\n$$\nSince $\\mu_1 = 0$, we have:\n$$\nM_{E'}(t) = 1 + \\left( \\frac{\\mu_2}{2} t^2 + \\frac{\\mu_3}{6} t^3 + \\frac{\\mu_4}{24} t^4 + \\dots \\right)\n$$\nThe CGF is $K_{E'}(t) = \\ln M_{E'}(t)$. Using the Taylor expansion $\\ln(1+x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\dots$, with $x = \\frac{\\mu_2}{2} t^2 + \\frac{\\mu_3}{6} t^3 + \\frac{\\mu_4}{24} t^4 + \\dots$, we expand $K_{E'}(t)$:\n$$\nK_{E'}(t) = \\left( \\frac{\\mu_2}{2} t^2 + \\frac{\\mu_3}{6} t^3 + \\frac{\\mu_4}{24} t^4 + \\dots \\right) - \\frac{1}{2} \\left( \\frac{\\mu_2}{2} t^2 + \\frac{\\mu_3}{6} t^3 + \\dots \\right)^2 + O(t^6)\n$$\nWe collect terms up to order $t^4$:\nThe linear term in the expansion of $K_{E'}(t)$ is zero, so $\\kappa_1(E') = 0$, as expected.\nThe $t^2$ term is $\\frac{\\mu_2}{2}t^2$. Comparing this to $\\frac{\\kappa_2}{2!}t^2$, we find $\\kappa_2 = \\mu_2$.\nThe $t^3$ term is $\\frac{\\mu_3}{6}t^3$. Comparing this to $\\frac{\\kappa_3}{3!}t^3$, we find $\\kappa_3 = \\mu_3$.\nThe $t^4$ term comes from two contributions: $\\frac{\\mu_4}{24}t^4$ from the first parenthesis, and $-\\frac{1}{2}(\\frac{\\mu_2}{2}t^2)^2 = -\\frac{\\mu_2^2}{8}t^4$ from the second parenthesis. All other terms are of higher order in $t$. The total coefficient of $t^4$ is thus $\\frac{\\mu_4}{24} - \\frac{\\mu_2^2}{8}$.\nWe equate this to the coefficient of $t^4$ in the definition of the CGF, $\\frac{\\kappa_4}{4!} = \\frac{\\kappa_4}{24}$:\n$$\n\\frac{\\kappa_4}{24} = \\frac{\\mu_4}{24} - \\frac{\\mu_2^2}{8} = \\frac{\\mu_4 - 3\\mu_2^2}{24}\n$$\nThis yields the required expression for the fourth-order cumulant:\n$$\n\\kappa_4 = \\mu_4 - 3\\mu_2^2\n$$\nThis formula is expressed purely in terms of the central moments $\\mu_2$ and $\\mu_4$, which can be estimated from the sample $\\{E_i\\}_{i=1}^N$.\n\n### Part 2: Physical Interpretation of the Sign of $\\kappa_4$\n\nThe sign of $\\kappa_4$ provides information about the shape of the probability distribution $P(E)$, specifically its deviation from a Gaussian form. The quantity is directly related to the excess kurtosis, $\\gamma_2 = \\frac{\\mu_4}{\\mu_2^2} - 3 = \\frac{\\kappa_4}{\\mu_2^2}$. Since $\\mu_2 = \\sigma^2$ is non-negative, the sign of $\\kappa_4$ is identical to the sign of the excess kurtosis.\n\nA unimodal, approximately Gaussian energy distribution is characteristic of a system near a second-order (continuous) phase transition. For a perfect Gaussian distribution, the central moments are $\\mu_2 = \\sigma^2$ and $\\mu_4 = 3\\sigma^4$. Substituting these into our derived formula gives:\n$$\n\\kappa_4 = (3\\sigma^4) - 3(\\sigma^2)^2 = 0\n$$\nTherefore, for a system with Gaussian energy fluctuations (Case A), we expect $\\kappa_4 \\approx 0$, where any deviation from zero is due to finite sampling error. The problem defines such cases as \"second-order-like\" and assigns them a label of $2$ if $\\kappa_4 \\ge 0$.\n\nA symmetric bimodal energy distribution is characteristic of a system near a first-order phase transition, where two distinct thermodynamic phases (e.g., high and low energy states) coexist. This leads to a double-peaked probability distribution for the energy. Consider an idealized model of such a distribution consisting of two symmetric Dirac delta functions: $P(E) = \\frac{1}{2}\\delta(E-a) + \\frac{1}{2}\\delta(E+a)$ for some $a  0$. The mean energy is $\\langle E \\rangle = 0$. The central moments are easily computed:\n$$\n\\mu_2 = \\langle E^2 \\rangle = \\frac{1}{2}(-a)^2 + \\frac{1}{2}(a)^2 = a^2\n$$\n$$\n\\mu_4 = \\langle E^4 \\rangle = \\frac{1}{2}(-a)^4 + \\frac{1}{2}(a)^4 = a^4\n$$\nThe fourth cumulant is then:\n$$\n\\kappa_4 = \\mu_4 - 3\\mu_2^2 = a^4 - 3(a^2)^2 = -2a^4\n$$\nSince $a  0$, $\\kappa_4$ is strictly negative. This distribution is platykurtic (flatter than Gaussian). For the mixture of two Gaussians specified in Cases B and C, as long as the separation between the means is significant compared to their standard deviations, the distribution is strongly bimodal and the same conclusion holds: probability mass is concentrated in two peaks away from the mean, which makes the fourth moment $\\mu_4$ \"small\" relative to the square of the variance $\\mu_2^2$, leading to a negative $\\kappa_4$. This negative sign is a classic signature of first-order phase coexistence, and such cases are labeled as \"first-order-like\" (label $1$).\n\n### Part 3: Algorithmic Design and Implementation\n\nThe numerical part of the problem requires generating synthetic data and computing the specified quantities. The implementation proceeds as follows:\n\n_1. Data Generation:_ A single pseudorandom number generator is initialized with the specified seed, `12345`, to ensure reproducibility.\n- For Case A (unimodal), $N=50000$ independent samples for $E$ and $M$ are drawn from their respective Gaussian distributions $\\mathcal{N}(0.0, 1.0^2)$.\n- For Cases B and C (bimodal), the data represents a mixture of two components. The problem states \"draw $N$ pairs $(E,M)$ from an equal-weight mixture of two independent Gaussians for both $E$ and $M$\". This implies that for each pair $(E_i, M_i)$, a single random choice determines whether it is drawn from component $1$ or component $2$. This creates a correlation between $E$ and $M$, which is physically representative of phase coexistence. To implement this, we generate $N/2$ pairs from the first component's bivariate distribution (where $E$ and $M$ are independent) and $N/2$ pairs from the second component's bivariate distribution, then concatenate them.\n  For Case B ($N=50000$):\n  - Component 1: $E \\sim \\mathcal{N}(-2.0, 0.2^2)$, $M \\sim \\mathcal{N}(-1.0, 0.1^2)$\n  - Component 2: $E \\sim \\mathcal{N}(+2.0, 0.2^2)$, $M \\sim \\mathcal{N}(+1.0, 0.1^2)$\n  For Case C ($N=1000$), the principle is the same with its specified parameters.\n\n_2. Computation of Observables:_ For each generated dataset $\\{E_i, M_i\\}_{i=1}^N$:\n- The sample mean energy is computed as $\\langle E \\rangle_N = \\frac{1}{N} \\sum_{i=1}^N E_i$.\n- The sample mean absolute magnetization is $\\langle |M| \\rangle_N = \\frac{1}{N} \\sum_{i=1}^N |M_i|$.\n- The second and fourth central moments of energy are computed using their definitions:\n  $$ \\mu_2 = \\frac{1}{N}\\sum_{i=1}^N (E_i - \\langle E \\rangle_N)^2 $$\n  $$ \\mu_4 = \\frac{1}{N}\\sum_{i=1}^N (E_i - \\langle E \\rangle_N)^4 $$\n- The fourth-order cumulant is then calculated using the derived formula: $\\kappa_4 = \\mu_4 - 3\\mu_2^2$.\n\n_3. Classification:_ Each case is classified based on the sign of the computed $\\kappa_4$:\n- If $\\kappa_4  0$, the label is $1$ (\"first-order-like\").\n- If $\\kappa_4 \\ge 0$, the label is $2$ (\"second-order-like\").\n\nThe final output is a list containing the results $[\\kappa_4, \\langle E \\rangle_N, \\langle |M| \\rangle_N, \\text{label}]$ for each of the three cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating synthetic data for three test cases,\n    computing the specified physical quantities including the fourth-order\n    energy cumulant, and classifying the cases based on its sign.\n    \"\"\"\n    # Use a fixed seed for the pseudorandom number generator for reproducibility.\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    # Define the parameters for the three test cases.\n    test_cases_params = [\n        # Case A: Unimodal, \"second-order-like\"\n        {\n            \"N\": 50000,\n            \"type\": \"unimodal\",\n            \"E_params\": {\"mu\": 0.0, \"sigma\": 1.0},\n            \"M_params\": {\"mu\": 0.0, \"sigma\": 1.0},\n        },\n        # Case B: Bimodal, \"first-order-like\", clear separation\n        {\n            \"N\": 50000,\n            \"type\": \"bimodal\",\n            \"E_params\": [{\"mu\": -2.0, \"sigma\": 0.2}, {\"mu\": 2.0, \"sigma\": 0.2}],\n            \"M_params\": [{\"mu\": -1.0, \"sigma\": 0.1}, {\"mu\": 1.0, \"sigma\": 0.1}],\n        },\n        # Case C: Bimodal, \"first-order-like\", finite-sample edge\n        {\n            \"N\": 1000,\n            \"type\": \"bimodal\",\n            \"E_params\": [{\"mu\": -1.5, \"sigma\": 0.5}, {\"mu\": 1.5, \"sigma\": 0.5}],\n            \"M_params\": [{\"mu\": -0.5, \"sigma\": 0.2}, {\"mu\": 0.5, \"sigma\": 0.2}],\n        },\n    ]\n\n    results = []\n    for params in test_cases_params:\n        N = params[\"N\"]\n        \n        # Generate synthetic data (E, M) pairs\n        if params[\"type\"] == \"unimodal\":\n            E = rng.normal(loc=params[\"E_params\"][\"mu\"], scale=params[\"E_params\"][\"sigma\"], size=N)\n            M = rng.normal(loc=params[\"M_params\"][\"mu\"], scale=params[\"M_params\"][\"sigma\"], size=N)\n        elif params[\"type\"] == \"bimodal\":\n            # For bimodal cases, generate N/2 samples from each component,\n            # ensuring correlation between E and M by picking from the same\n            # component for each (E, M) pair.\n            n1 = N // 2\n            n2 = N - n1\n            \n            E1 = rng.normal(loc=params[\"E_params\"][0][\"mu\"], scale=params[\"E_params\"][0][\"sigma\"], size=n1)\n            E2 = rng.normal(loc=params[\"E_params\"][1][\"mu\"], scale=params[\"E_params\"][1][\"sigma\"], size=n2)\n            E = np.concatenate((E1, E2))\n            \n            M1 = rng.normal(loc=params[\"M_params\"][0][\"mu\"], scale=params[\"M_params\"][0][\"sigma\"], size=n1)\n            M2 = rng.normal(loc=params[\"M_params\"][1][\"mu\"], scale=params[\"M_params\"][1][\"sigma\"], size=n2)\n            M = np.concatenate((M1, M2))\n\n            # Shuffle the pairs to mix the components\n            p = rng.permutation(N)\n            E = E[p]\n            M = M[p]\n\n        # Compute empirical averages\n        mean_E = np.mean(E)\n        mean_abs_M = np.mean(np.abs(M))\n\n        # Compute empirical central moments of energy\n        # The sample central moments are defined with a 1/N normalization factor.\n        # np.var uses ddof=0 by default, which is equivalent to 1/N.\n        mu_2 = np.var(E)\n        mu_4 = np.mean((E - mean_E)**4)\n\n        # Compute the fourth-order energy cumulant using the derived formula\n        kappa_4 = mu_4 - 3 * (mu_2**2)\n\n        # Classify the case based on the sign of kappa_4\n        # Label 1: first-order-like (k4  0)\n        # Label 2: second-order-like (k4 >= 0)\n        label = 1 if kappa_4  0 else 2\n        \n        results.append([kappa_4, mean_E, mean_abs_M, label])\n\n    # Format the results into the required string format: [[...],[...],[...]]\n    results_str_list = [\n        f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results\n    ]\n    final_output_str = f\"[{','.join(results_str_list)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "2380998"}]}