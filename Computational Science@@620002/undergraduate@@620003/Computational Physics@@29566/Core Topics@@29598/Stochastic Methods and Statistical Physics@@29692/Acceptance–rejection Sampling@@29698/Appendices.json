{"hands_on_practices": [{"introduction": "The power of acceptance-rejection sampling hinges on a critical requirement: the proposal distribution $g(x)$ must have \"heavier\" tails than the target distribution $f(x)$, or at least tails that decay no faster. This ensures that the proposal can fully envelop the target. This exercise [@problem_id:2403911] provides a crucial cautionary tale by exploring a scenario where this rule is violated, attempting to sample a heavy-tailed Cauchy distribution using a light-tailed Gaussian proposal. By analyzing this case, you will gain a firm understanding of why this \"heavy-tail\" condition is not just a technicality, but a fundamental prerequisite for the method to be valid and computationally feasible.", "problem": "Consider the task of generating independent and identically distributed samples from the target probability density function (PDF) $f(x)$ on $\\mathbb{R}$, where $f(x)$ is the standard Cauchy distribution given by\n$$\nf(x)=\\frac{1}{\\pi\\left(1+x^{2}\\right)}.\n$$\nAssume two generic sampling strategies:\n\n$1.$ Inverse transform sampling using the cumulative distribution function $F(x)=\\frac{1}{2}+\\frac{1}{\\pi}\\arctan x$, which is strictly increasing and admits the analytic inverse $F^{-1}(u)$ for $u\\in(0,1)$.\n\n$2.$ Rejection sampling with a univariate Gaussian proposal density\n$$\ng_{\\sigma}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right),\n$$\nwhere the proposal is narrow in the sense that $\\sigma\\in(0,1)$, and with a putative envelope constant $M>0$ satisfying $f(x)\\le M\\,g_{\\sigma}(x)$ for all $x\\in\\mathbb{R}$ if such an $M$ exists.\n\nCompare the computational efficiency of these two approaches for this target–proposal pair, in terms of the existence of a valid rejection scheme and the expected computational effort per accepted sample. Which of the following statements is/are correct?\n\nA. Inverse transform sampling produces one exact Cauchy sample per uniform variate (up to a constant-cost evaluation of $F^{-1}$), whereas no finite envelope constant $M$ exists for the given narrow Gaussian proposal, so rejection sampling cannot be made valid; equivalently, its acceptance probability is $0$ and the expected number of proposals per accepted sample is infinite.\n\nB. Because $g_{\\sigma}(x)$ concentrates near $x=0$ where $f(x)$ attains its maximum, rejection sampling has a high acceptance probability and outperforms inverse transform sampling, whose cost scales with repeated target evaluations.\n\nC. A finite envelope constant exists with $M$ scaling like $\\exp\\!\\left(\\frac{1}{2\\sigma^{2}}\\right)$, so rejection sampling remains valid though moderately inefficient; therefore both methods have finite and comparable computational costs.\n\nD. The cost of inverse transform sampling is dominated by generating Gaussian proposals, so rejection sampling is preferable when $\\sigma$ is very small.", "solution": "The problem statement is scientifically and mathematically sound, well-posed, and objective. It poses a standard question in computational physics and statistics concerning the comparison of two fundamental sampling algorithms. The definitions of the probability density functions and sampling strategies are correct and self-contained. Therefore, a full analysis is warranted.\n\nThe problem requires a comparison of the computational efficiency of two methods for sampling from the standard Cauchy distribution with probability density function (PDF) $f(x) = \\frac{1}{\\pi(1+x^2)}$.\n\n**Method 1: Inverse Transform Sampling (ITS)**\n\nThis method relies on the cumulative distribution function (CDF) $F(x)$ and its inverse $F^{-1}(u)$. The CDF is given as $F(x) = \\frac{1}{2} + \\frac{1}{\\pi} \\arctan x$. The algorithm proceeds by generating a random number $u$ from a uniform distribution on the interval $(0, 1)$, and then calculating the sample as $x = F^{-1}(u)$.\n\nTo find the inverse function, we set $u = F(x)$ and solve for $x$:\n$$\nu = \\frac{1}{2} + \\frac{1}{\\pi} \\arctan x\n$$\n$$\nu - \\frac{1}{2} = \\frac{1}{\\pi} \\arctan x\n$$\n$$\n\\pi \\left( u - \\frac{1}{2} \\right) = \\arctan x\n$$\n$$\nx = \\tan\\left(\\pi \\left( u - \\frac{1}{2} \\right)\\right)\n$$\nThus, the inverse function is $F^{-1}(u) = \\tan(\\pi(u - 1/2))$. This is a well-defined analytical expression for any $u \\in (0,1)$.\n\nThe computational cost of this method consists of two steps:\n$1.$ Generating one uniform random variate $u \\sim U(0,1)$.\n$2.$ Evaluating the function $F^{-1}(u)$, which involves a few basic arithmetic operations and the evaluation of the tangent function.\nThis entire process has a small, constant computational cost. For every uniform variate $u$ generated, one exact sample $x$ from the target distribution $f(x)$ is produced. This method is, therefore, extremely efficient.\n\n**Method 2: Rejection Sampling (RS)**\n\nThis method requires a proposal distribution $g_{\\sigma}(x)$ and a constant $M > 0$ such that $f(x) \\le M g_{\\sigma}(x)$ for all $x \\in \\mathbb{R}$. The proposal is a Gaussian PDF:\n$$\ng_{\\sigma}(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)\n$$\nwith $\\sigma \\in (0,1)$.\n\nFor this scheme to be valid, the ratio $h(x) = \\frac{f(x)}{g_{\\sigma}(x)}$ must be bounded from above over the entire real line $\\mathbb{R}$. The smallest possible value for the envelope constant is $M = \\sup_{x \\in \\mathbb{R}} h(x)$. Let us analyze this ratio:\n$$\nh(x) = \\frac{f(x)}{g_{\\sigma}(x)} = \\frac{\\frac{1}{\\pi(1+x^2)}}{\\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)} = \\frac{\\sigma\\sqrt{2\\pi}}{\\pi(1+x^2)} \\exp\\left(\\frac{x^2}{2\\sigma^2}\\right)\n$$\nWe must examine the behavior of $h(x)$ as $|x| \\to \\infty$. The term in the denominator, $(1+x^2)$, is a polynomial in $x$. The term in the numerator, $\\exp\\left(\\frac{x^2}{2\\sigma^2}\\right)$, is an exponential function of $x^2$. It is a fundamental result of calculus that exponential growth is asymptotically faster than any polynomial growth.\nFormally, let's consider the limit:\n$$\n\\lim_{|x| \\to \\infty} h(x) = \\frac{\\sigma\\sqrt{2\\pi}}{\\pi} \\lim_{|x| \\to \\infty} \\frac{\\exp\\left(\\frac{x^2}{2\\sigma^2}\\right)}{1+x^2}\n$$\nBy applying L'Hôpital's rule to the limit with respect to $y=x^2$ as $y \\to \\infty$:\n$$\n\\lim_{y \\to \\infty} \\frac{\\exp\\left(\\frac{y}{2\\sigma^2}\\right)}{1+y} = \\lim_{y \\to \\infty} \\frac{\\frac{1}{2\\sigma^2}\\exp\\left(\\frac{y}{2\\sigma^2}\\right)}{1} = \\infty\n$$\nSince the limit of $h(x)$ as $|x| \\to \\infty$ is infinite, the function $h(x)$ is unbounded. This means that no finite constant $M$ exists that satisfies the condition $f(x) \\le M g_{\\sigma}(x)$ for all $x \\in \\mathbb{R}$.\n\nThis is a classic case of attempting to use a \"thin-tailed\" proposal distribution (the Gaussian, which decays exponentially) to sample from a \"fat-tailed\" target distribution (the Cauchy, which decays polynomially like $1/x^2$). The tails of the proposal decay to zero much faster than the tails of the target, causing their ratio to diverge.\n\nThe consequence for rejection sampling is severe. The acceptance probability of a proposed sample is $1/M$. Since a finite $M$ does not exist, we can consider the formal limit $M \\to \\infty$, which gives an acceptance probability of $0$. The expected number of proposals required to obtain one accepted sample is $M$, which is infinite. Therefore, the rejection sampling scheme as described is not valid and has infinite computational cost.\n\n**Evaluation of the Options**\n\n**A. Inverse transform sampling produces one exact Cauchy sample per uniform variate (up to a constant-cost evaluation of $F^{-1}$), whereas no finite envelope constant $M$ exists for the given narrow Gaussian proposal, so rejection sampling cannot be made valid; equivalently, its acceptance probability is $0$ and the expected number of proposals per accepted sample is infinite.**\nThis statement accurately summarizes our findings. The ITS part is correct: one uniform variate yields one exact sample at a small, constant computational cost. The rejection sampling part is also correct: the ratio $f(x)/g_{\\sigma}(x)$ is unbounded, so no finite $M$ exists, rendering the method invalid with an acceptance probability of $0$ and infinite expected cost.\n**Verdict: Correct.**\n\n**B. Because $g_{\\sigma}(x)$ concentrates near $x=0$ where $f(x)$ attains its maximum, rejection sampling has a high acceptance probability and outperforms inverse transform sampling, whose cost scales with repeated target evaluations.**\nThis statement is incorrect on multiple accounts. While it is true that both functions have their mode at $x=0$, the efficiency of rejection sampling depends on the global behavior of the ratio $f(x)/g_{\\sigma}(x)$, not just its behavior near the mode. The claim of high acceptance probability is false; the probability is $0$. The claim that it \"outperforms\" ITS is consequently false. Finally, the description of ITS is wrong; its cost is constant per sample and does not involve \"repeated target evaluations\" in the sense of a trial-and-error process.\n**Verdict: Incorrect.**\n\n**C. A finite envelope constant exists with $M$ scaling like $\\exp\\!\\left(\\frac{1}{2\\sigma^{2}}\\right)$, so rejection sampling remains valid though moderately inefficient; therefore both methods have finite and comparable computational costs.**\nThe premise that a finite envelope constant $M$ exists is false, as proven by the analysis of the limit of $h(x)$. Therefore, the entire statement collapses. Rejection sampling is not valid, and its cost is not finite, let alone comparable to the highly efficient ITS method.\n**Verdict: Incorrect.**\n\n**D. The cost of inverse transform sampling is dominated by generating Gaussian proposals, so rejection sampling is preferable when $\\sigma$ is very small.**\nThis statement makes a nonsensical claim. Inverse transform sampling uses uniform random variates, not Gaussian proposals. It fundamentally confuses the machinery of the two distinct algorithms. The premise is false, and the conclusion derived from it is baseless.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2403911"}, {"introduction": "While the ultimate goal of acceptance-rejection sampling is to produce accepted samples, a deeper understanding of the algorithm comes from analyzing all its components, including the samples that are \"thrown away\". This practice problem [@problem_id:760197] shifts the focus to the rejected samples, challenging you to calculate their expected value. By working through this derivation, you will uncover the non-uniform nature of the rejection process and see firsthand how the probability of rejection depends on the proposed value $X$, providing a more nuanced insight into the mechanics of the algorithm.", "problem": "The Acceptance-Rejection method is a technique for generating random samples from a target probability distribution $f(x)$ by using a simpler proposal distribution $g(x)$. The method requires a constant $M$ such that $f(x) \\le M g(x)$ for all $x$ in the support of $f(x)$, and the support of $g(x)$ must contain the support of $f(x)$.\n\nThe sampling algorithm is as follows:\n1.  Draw a sample $X$ from the proposal distribution $g(x)$.\n2.  Draw a sample $U$ from the uniform distribution $U(0, 1)$.\n3.  The sample $X$ is \"accepted\" if $U < \\frac{f(X)}{M g(X)}$; otherwise, it is \"rejected\". This process is repeated until a sample is accepted.\n\nConsider a target probability density function (PDF) $f(x)$ defined on the interval $[0, 1]$ given by a linear function:\n$$f(x) = C(1 + \\alpha x)$$\nwhere $C$ is the normalization constant and $\\alpha$ is a parameter satisfying $0 < \\alpha \\le 1$.\n\nThe proposal distribution is the uniform distribution on the interval $[0, 1]$, i.e., $g(x) = 1$ for $x \\in [0, 1]$. The constant $M$ is chosen to be the smallest possible value that satisfies the condition $f(x) \\le M g(x)$.\n\nLet $X$ be a random variate drawn from the proposal distribution $g(x)$. Derive the expected value of $X$, conditioned on the event that $X$ is rejected by the acceptance-rejection algorithm, i.e., find $E[X | \\text{X is rejected}]$.", "solution": "1. Normalize $f(x)$ on $[0,1]$:\n$$\\int_0^1 C(1+\\alpha x)\\,dx\n= C\\Bigl[1+\\tfrac{\\alpha}{2}\\Bigr]\n=1\n\\;\\Longrightarrow\\;\nC=\\frac{1}{1+\\tfrac{\\alpha}{2}}.$$\n\n2. Find the smallest envelope constant $M$:\n$$M=\\max_{x\\in[0,1]}\\frac{f(x)}{g(x)}\n=\\max_{x}(C(1+\\alpha x))\n=C(1+\\alpha)\n=\\frac{1+\\alpha}{1+\\tfrac{\\alpha}{2}}.$$\n\n3. Rejection probability at $X=x$:\n$$P(\\text{reject}\\mid X=x)\n=1-\\frac{f(x)}{M\\,g(x)}\n=1-\\frac{C(1+\\alpha x)}{M}\n=\\frac{\\alpha(1-x)}{1+\\alpha}.$$\n\n4. Joint density of $(X,\\text{reject})$:\n$$h(x)=g(x)\\,P(\\text{reject}\\mid X=x)\n=\\frac{\\alpha(1-x)}{1+\\alpha},\\quad x\\in[0,1].$$\n\n5. Total rejection probability:\n$$P(\\text{reject})\n=\\int_0^1 h(x)\\,dx\n=\\frac{\\alpha}{1+\\alpha}\\int_0^1(1-x)\\,dx\n=\\frac{\\alpha}{2(1+\\alpha)}.$$\n\n6. Conditional density given rejection:\n$$f_{X\\mid\\text{reject}}(x)\n=\\frac{h(x)}{P(\\text{reject})}\n=\\frac{\\alpha(1-x)/(1+\\alpha)}{\\alpha/[2(1+\\alpha)]}\n=2(1-x),\\quad x\\in[0,1].$$\n\n7. Compute $E[X\\mid\\text{reject}]$:\n$$E[X\\mid\\text{reject}]\n=\\int_0^1 x\\cdot2(1-x)\\,dx\n=2\\int_0^1(x-x^2)\\,dx\n=2\\Bigl[\\tfrac12-\\tfrac13\\Bigr]\n=\\tfrac13.$$", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "760197"}, {"introduction": "Moving from theory to practice, one of the most common challenges in computational science is sampling from distributions with multiple modes, which represent different states or peaks of interest. A naive proposal distribution may be terribly inefficient in this scenario. This hands-on computational exercise [@problem_id:2370829] guides you through comparing two strategies for sampling a bimodal target: a simple, single-mode Gaussian proposal versus a more sophisticated mixture proposal designed to mirror the target's structure. By quantifying the dramatic improvement in efficiency, you will learn the practical value of thoughtful proposal design and the power of using mixture models to tackle complex distributions.", "problem": "You will compare the efficiency of acceptance–rejection sampling (ARS) for drawing samples from a one-dimensional bimodal target probability density function using two distinct proposal strategies. The target distribution is a mixture of two normal distributions. The efficiency metric is the acceptance rate, which for a valid ARS setup equals the reciprocal of the enveloping constant.\n\nFundamental base and definitions:\n- A probability density function $f(x)$ is a nonnegative function with $\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1$.\n- In acceptance–rejection sampling (ARS), one selects a proposal density $g(x)$ with $\\int g(x)\\,dx = 1$ and a constant $M \\ge 1$ such that $M\\,g(x) \\ge f(x)$ for all $x \\in \\mathbb{R}$. The ARS procedure draws $X \\sim g$ and $U \\sim \\text{Uniform}(0,1)$ and accepts $X$ if $U \\le \\frac{f(X)}{M g(X)}$. This guarantees that accepted $X$ values are distributed according to $f$.\n- The acceptance rate is the expected value $\\mathbb{E}\\left[\\frac{f(X)}{M g(X)}\\right]$ with $X \\sim g$. Under the ARS conditions and $f$ normalized to integrate to $1$, the acceptance rate equals $\\frac{1}{M}$.\n\nTarget and proposals to be studied:\n- Target density $f(x)$: a mixture of two normal components with common standard deviation $\\sigma_t > 0$, means $\\mu_1$ and $\\mu_2$, and mixing weight $w \\in (0,1)$ on the first mode,\n$$\nf(x) \\;=\\; w\\,\\phi(x;\\mu_1,\\sigma_t) \\;+\\; (1-w)\\,\\phi(x;\\mu_2,\\sigma_t),\n$$\nwhere $\\phi(x;\\mu,\\sigma)$ is the normal density\n$$\n\\phi(x;\\mu,\\sigma) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n$$\n- Proposal A (single unimodal proposal) $g_s(x)$: a single normal density with mean chosen as the mixture mean $\\mu_p = w\\,\\mu_1 + (1-w)\\,\\mu_2$ and standard deviation chosen as the square root of the mixture variance,\n$$\n\\sigma_p \\;=\\; \\sqrt{\\sigma_t^2 + w(1-w)(\\mu_2-\\mu_1)^2}, \\qquad g_s(x) = \\phi(x;\\mu_p,\\sigma_p).\n$$\n- Proposal B (mixture of two strategically placed unimodal proposals) $g_m(x)$: a mixture of two normal components centered at the two modes with the same standard deviation as the target and the same weights,\n$$\ng_m(x) \\;=\\; w\\,\\phi(x;\\mu_1,\\sigma_t) \\;+\\; (1-w)\\,\\phi(x;\\mu_2,\\sigma_t).\n$$\n\nComputational task:\n- For each test case in the suite below, compute a valid enveloping constant for each proposal,\n$$\nM_s \\;\\ge\\; \\sup_{x \\in \\mathbb{R}} \\frac{f(x)}{g_s(x)}, \\qquad\nM_m \\;\\ge\\; \\sup_{x \\in \\mathbb{R}} \\frac{f(x)}{g_m(x)}.\n$$\n- Because the exact supremum may not be obtainable in closed form, compute a numerically valid upper bound by evaluating the ratio on a sufficiently wide and dense grid and then multiplying the maximum observed ratio by a small safety factor to guarantee validity. Use the domain\n$$\n[x_{\\min}, x_{\\max}] \\;=\\; \\big[\\min(\\mu_1,\\mu_2) - L,\\; \\max(\\mu_1,\\mu_2) + L\\big], \\quad L \\;=\\; 8\\,\\sigma_t + |\\mu_2-\\mu_1|,\n$$\nand a uniform grid of $N_{\\text{grid}} = 200001$ points on this interval. Let $\\widehat{M}$ denote the observed maximum ratio on the grid. Define the certified constants\n$$\nM \\;=\\; \\gamma\\,\\widehat{M}, \\quad \\text{with safety factor } \\gamma \\;=\\; 1.0005.\n$$\n- Compute the acceptance rates $a_s = 1/M_s$ and $a_m = 1/M_m$ for proposals $g_s$ and $g_m$, respectively.\n- For each test case, report the improvement factor $I = a_m/a_s$ as a decimal number.\n\nTest suite:\n- Case 1 (balanced, well-separated modes): $\\mu_1 = -3$, $\\mu_2 = 3$, $\\sigma_t = 1$, $w = 0.5$.\n- Case 2 (balanced, moderately overlapping modes): $\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma_t = 1$, $w = 0.5$.\n- Case 3 (unequal weights, separated modes): $\\mu_1 = 0$, $\\mu_2 = 8$, $\\sigma_t = 1$, $w = 0.9$.\n- Case 4 (balanced, extremely separated modes): $\\mu_1 = -6$, $\\mu_2 = 6$, $\\sigma_t = 1$, $w = 0.5$.\n\nAnswer specification:\n- For each test case, the answer is a single float $I$ representing the improvement factor $a_m/a_s$.\n- Your program must produce a single line of output containing the results as a comma-separated list of the improvement factors for the test suite, in the same order as above, enclosed in square brackets. Each float must be rounded to six decimal places (use decimal form, not a percentage). For example, a valid output format is $[1.234000,2.000000, \\dots]$.", "solution": "The problem requires a comparative analysis of the efficiency of two different proposal strategies for acceptance-rejection sampling (ARS) from a bimodal target probability density function. The efficiency is quantified by the acceptance rate, which is the reciprocal of the enveloping constant $M$. The improvement factor is defined as the ratio of the acceptance rates of the two proposals.\n\nThe target probability density function, $f(x)$, is a mixture of two normal distributions:\n$$\nf(x) = w\\,\\phi(x;\\mu_1,\\sigma_t) + (1-w)\\,\\phi(x;\\mu_2,\\sigma_t)\n$$\nwhere $\\phi(x;\\mu,\\sigma)$ is the normal probability density function:\n$$\n\\phi(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\nThe parameters are the means $\\mu_1$ and $\\mu_2$, a common standard deviation $\\sigma_t > 0$, and a mixing weight $w \\in (0,1)$.\n\nTwo proposal densities are specified for analysis.\n\nProposal A, a single unimodal proposal $g_s(x)$, is a normal density whose mean and variance are matched to the mean and variance of the target mixture distribution $f(x)$. The mean of the mixture is $\\mu_p = w\\,\\mu_1 + (1-w)\\,\\mu_2$, and the variance is $\\sigma_p^2 = \\sigma_t^2 + w(1-w)(\\mu_2-\\mu_1)^2$. Thus, the proposal is:\n$$\ng_s(x) = \\phi(x; \\mu_p, \\sigma_p) \\quad \\text{with} \\quad \\mu_p = w\\,\\mu_1 + (1-w)\\,\\mu_2, \\quad \\sigma_p = \\sqrt{\\sigma_t^2 + w(1-w)(\\mu_2-\\mu_1)^2}\n$$\nThis strategy attempts to approximate the bimodal target with a single, broader normal distribution.\n\nProposal B, a mixture proposal $g_m(x)$, is constructed to be identical to the target density:\n$$\ng_m(x) = w\\,\\phi(x;\\mu_1,\\sigma_t) + (1-w)\\,\\phi(x;\\mu_2,\\sigma_t)\n$$\nThis represents a 'perfect' proposal, as it has the same form as the target distribution.\n\nThe core of acceptance-rejection sampling relies on an enveloping constant $M$ such that $f(x) \\le M\\,g(x)$ for all $x$. The minimal such constant is $M = \\sup_{x \\in \\mathbb{R}} \\frac{f(x)}{g(x)}$. The acceptance probability, or efficiency, is given by $a = 1/M$. We must compute the enveloping constants $M_s$ and $M_m$ for proposals $g_s(x)$ and $g_m(x)$, respectively.\n\nFor Proposal B, since $g_m(x) = f(x)$, the ratio is identically one:\n$$\n\\frac{f(x)}{g_m(x)} = 1, \\quad \\forall x \\in \\mathbb{R}\n$$\nTherefore, the theoretical supremum is $\\sup_{x} \\frac{f(x)}{g_m(x)} = 1$. The numerically observed maximum on any grid, $\\widehat{M}_m$, will be $1$ (up to floating-point precision). According to the problem specification, the certified constant is $M_m = \\gamma\\,\\widehat{M}_m = \\gamma \\cdot 1 = \\gamma$, where $\\gamma = 1.0005$ is the safety factor. The corresponding acceptance rate is $a_m = 1/M_m = 1/\\gamma$.\n\nFor Proposal A, the ratio $r_s(x) = f(x)/g_s(x)$ is not constant. An analytical determination of its supremum is non-trivial. The problem mandates a numerical approach: we find the maximum value of this ratio on a specified dense grid, $\\widehat{M}_s = \\max_{x \\in \\text{grid}} r_s(x)$. The certified constant is then $M_s = \\gamma\\,\\widehat{M}_s$. The acceptance rate for this proposal is $a_s = 1/M_s$.\n\nThe objective is to compute the improvement factor $I$, defined as the ratio of the acceptance rates:\n$$\nI = \\frac{a_m}{a_s}\n$$\nSubstituting the expressions for the acceptance rates in terms of the certified constants, we obtain:\n$$\nI = \\frac{1/M_m}{1/M_s} = \\frac{M_s}{M_m}\n$$\nUsing the definitions $M_s = \\gamma\\,\\widehat{M}_s$ and $M_m = \\gamma$, the expression for the improvement factor simplifies:\n$$\nI = \\frac{\\gamma\\,\\widehat{M}_s}{\\gamma} = \\widehat{M}_s\n$$\nThus, the required improvement factor is precisely the maximum value of the ratio $f(x)/g_s(x)$ observed on the numerical grid, before the application of the safety factor. This is the quantity we must compute for each test case.\n\nThe computational procedure for each test case is as follows:\n1.  Given the parameters $\\mu_1$, $\\mu_2$, $\\sigma_t$, and $w$.\n2.  Define the target density function $f(x) = w\\,\\phi(x;\\mu_1,\\sigma_t) + (1-w)\\,\\phi(x;\\mu_2,\\sigma_t)$.\n3.  Calculate the parameters for the single proposal $g_s(x)$:\n    -   Mean: $\\mu_p = w\\,\\mu_1 + (1-w)\\,\\mu_2$\n    -   Standard deviation: $\\sigma_p = \\sqrt{\\sigma_t^2 + w(1-w)(\\mu_2-\\mu_1)^2}$\n4.  Construct the numerical grid. The domain is $[x_{\\min}, x_{\\max}]$, where:\n    -   $L = 8\\,\\sigma_t + |\\mu_2-\\mu_1|$\n    -   $x_{\\min} = \\min(\\mu_1,\\mu_2) - L$\n    -   $x_{\\max} = \\max(\\mu_1,\\mu_2) + L$\n    -   The grid consists of $N_{\\text{grid}} = 200001$ uniformly spaced points on this domain.\n5.  Evaluate the functions $f(x)$ and $g_s(x)$ at each point on the grid.\n6.  Compute the ratio $r_s(x) = f(x)/g_s(x)$ for all grid points. Special care must be taken if $g_s(x)$ approaches zero, but given the wide grid and the nature of normal distributions, both numerator and denominator will tend to zero in the tails, and their ratio will be well-behaved.\n7.  The improvement factor $I$ is the maximum value found in the array of ratios: $I = \\max(r_s(x))$.\n8.  This process is repeated for all four test cases provided in the suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the improvement factor of a mixture proposal over a single \n    unimodal proposal for acceptance-rejection sampling.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (balanced, well-separated modes)\n        {'mu1': -3.0, 'mu2': 3.0, 'sigma_t': 1.0, 'w': 0.5},\n        # Case 2 (balanced, moderately overlapping modes)\n        {'mu1': -1.0, 'mu2': 1.0, 'sigma_t': 1.0, 'w': 0.5},\n        # Case 3 (unequal weights, separated modes)\n        {'mu1': 0.0, 'mu2': 8.0, 'sigma_t': 1.0, 'w': 0.9},\n        # Case 4 (balanced, extremely separated modes)\n        {'mu1': -6.0, 'mu2': 6.0, 'sigma_t': 1.0, 'w': 0.5},\n    ]\n\n    # Constants from the problem statement\n    N_grid = 200001\n    \n    results = []\n\n    for case in test_cases:\n        mu1 = case['mu1']\n        mu2 = case['mu2']\n        sigma_t = case['sigma_t']\n        w = case['w']\n\n        # 1. Define the target density function f(x)\n        def f(x_vals):\n            term1 = w * norm.pdf(x_vals, loc=mu1, scale=sigma_t)\n            term2 = (1.0 - w) * norm.pdf(x_vals, loc=mu2, scale=sigma_t)\n            return term1 + term2\n\n        # 2. Calculate parameters for the single proposal g_s(x)\n        mu_p = w * mu1 + (1.0 - w) * mu2\n        var_p = sigma_t**2 + w * (1.0 - w) * (mu2 - mu1)**2\n        sigma_p = np.sqrt(var_p)\n\n        # 3. Define the single proposal density g_s(x)\n        def g_s(x_vals):\n            return norm.pdf(x_vals, loc=mu_p, scale=sigma_p)\n\n        # 4. Construct the numerical grid\n        L = 8.0 * sigma_t + np.abs(mu2 - mu1)\n        x_min = min(mu1, mu2) - L\n        x_max = max(mu1, mu2) + L\n        x_grid = np.linspace(x_min, x_max, N_grid)\n\n        # 5. Evaluate f(x) and g_s(x) on the grid\n        f_vals = f(x_grid)\n        g_s_vals = g_s(x_grid)\n\n        # 6. Compute the ratio r_s(x) = f(x) / g_s(x)\n        # To avoid division by zero or NaN, we handle cases where g_s is very small.\n        # If g_s(x) is near zero, f(x) must also be near zero for the ratio to be finite.\n        # In the extreme tails, both are effectively zero, and the ratio tends to zero.\n        # We can safely replace g_s_vals=0 with a very small number, or filter them.\n        # Here, we only divide where g_s_vals is non-zero.\n        # The result of f(x)/g(x) approaches 0 where g(x) goes to 0 faster than f(x),\n        # which is the case in the tails.\n        ratio_s = np.zeros_like(f_vals)\n        non_zero_mask = g_s_vals > 0\n        ratio_s[non_zero_mask] = f_vals[non_zero_mask] / g_s_vals[non_zero_mask]\n\n        # 7. The improvement factor I is the max of the ratio\n        # I = M_s / M_m = (gamma * M_hat_s) / (gamma * M_hat_m)\n        # M_hat_m = 1 because g_m(x) = f(x). So, I = M_hat_s.\n        improvement_factor = np.max(ratio_s)\n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2370829"}]}