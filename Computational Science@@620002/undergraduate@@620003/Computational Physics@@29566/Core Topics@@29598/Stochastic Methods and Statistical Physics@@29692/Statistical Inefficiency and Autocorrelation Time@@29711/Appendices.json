{"hands_on_practices": [{"introduction": "Before we dive into complex simulations, it is crucial to build our intuition with a simple, exactly solvable model. This exercise explores the autoregressive process of order one, or AR(1), which serves as a cornerstone for modeling correlated time series data [@problem_id:2442417]. By analytically deriving the relationship between the statistical inefficiency $g$ and the exponential autocorrelation time $\\tau_{\\mathrm{exp}}$, you will gain a deep understanding of how these different metrics quantify the \"memory\" within a sequence of correlated measurements.", "problem": "Consider a stationary Gaussian autoregressive of order one (AR(1)) time series defined by\n$$\nX_{t+1}=\\phi X_t+\\varepsilon_t,\n$$\nwhere $\\{\\varepsilon_t\\}$ are independent and identically distributed with $\\mathbb{E}[\\varepsilon_t]=0$, $\\mathrm{Var}[\\varepsilon_t]=\\sigma_{\\varepsilon}^{2}$, and the autoregressive coefficient satisfies $\\phi\\in(0,1)$. Let the autocovariance function be defined by\n$$\nC(k)\\equiv \\mathrm{Cov}(X_t,X_{t+k}),\n$$\nand the normalized autocorrelation function be\n$$\n\\rho(k)\\equiv \\frac{C(k)}{C(0)}.\n$$\nDefine the exponential autocorrelation time $\\tau_{\\mathrm{exp}}$ as the positive constant characterizing exponential decay of the normalized autocorrelation function via\n$$\n\\rho(k)=\\exp\\!\\left(-\\frac{k}{\\tau_{\\mathrm{exp}}}\\right),\n$$\nand define the statistical inefficiency\n$$\ng \\equiv 1+2\\sum_{k=1}^{\\infty}\\rho(k),\n$$\nwith the understanding that the sum converges for $\\phi\\in(0,1)$. Derive, from these definitions and the given process, the exact ratio\n$$\nR(\\phi)\\equiv \\frac{g}{\\tau_{\\mathrm{exp}}}\n$$\nas a closed-form analytic expression in terms of $\\phi$ only. Express your final answer as a single simplified analytic expression. No numerical evaluation or units are required.", "solution": "The problem is to determine the ratio $R(\\phi) \\equiv g/\\tau_{\\mathrm{exp}}$ for a stationary Gaussian autoregressive process of order one (AR(1)). The process is defined by the stochastic difference equation\n$$\nX_{t+1} = \\phi X_t + \\varepsilon_t,\n$$\nwhere $\\phi \\in (0, 1)$, and $\\{\\varepsilon_t\\}$ is a sequence of independent and identically distributed random variables with mean $\\mathbb{E}[\\varepsilon_t] = 0$ and variance $\\mathrm{Var}[\\varepsilon_t] = \\sigma_{\\varepsilon}^{2}$.\n\nFirst, we determine the normalized autocorrelation function, $\\rho(k)$, for this process. Since the process is stationary, its mean $\\mu = \\mathbb{E}[X_t]$ must be constant. Taking the expectation of the defining equation gives $\\mathbb{E}[X_{t+1}] = \\phi \\mathbb{E}[X_t] + \\mathbb{E}[\\varepsilon_t]$, which implies $\\mu = \\phi \\mu + 0$. As $\\phi \\neq 1$, this forces the mean to be $\\mu = 0$.\n\nNext, we find the variance, $C(0) = \\mathrm{Var}(X_t)$. Due to stationarity, $\\mathrm{Var}(X_{t+1}) = \\mathrm{Var}(X_t) = C(0)$. The variance of the defining equation is\n$$\n\\mathrm{Var}(X_{t+1}) = \\mathrm{Var}(\\phi X_t + \\varepsilon_t).\n$$\nSince $X_t$ is a function of past innovations $\\{\\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots \\}$, it is uncorrelated with the present innovation $\\varepsilon_t$. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}(X_{t+1}) = \\phi^2 \\mathrm{Var}(X_t) + \\mathrm{Var}(\\varepsilon_t).\n$$\nSubstituting $C(0)$ and $\\sigma_{\\varepsilon}^{2}$:\n$$\nC(0) = \\phi^2 C(0) + \\sigma_{\\varepsilon}^{2}.\n$$\nSolving for $C(0)$ yields\n$$\nC(0) = \\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^2},\n$$\nwhich is well-defined and positive since $\\phi \\in (0,1)$ implies $|\\phi|<1$.\n\nNow, we compute the autocovariance function $C(k) = \\mathrm{Cov}(X_t, X_{t+k})$ for $k > 0$. Since the mean is zero, $C(k) = \\mathbb{E}[X_t X_{t+k}]$.\n$$\nC(k) = \\mathbb{E}[X_t (\\phi X_{t+k-1} + \\varepsilon_{t+k-1})] = \\phi \\mathbb{E}[X_t X_{t+k-1}] + \\mathbb{E}[X_t \\varepsilon_{t+k-1}].\n$$\nThe term $\\mathbb{E}[X_t \\varepsilon_{t+k-1}]$ is zero for $k \\ge 1$ because $X_t$ depends only on innovations up to time $t-1$. This gives a recurrence relation:\n$$\nC(k) = \\phi C(k-1).\n$$\nBy repeated application, we find $C(k) = \\phi^k C(0)$.\n\nThe normalized autocorrelation function, $\\rho(k)$, is then\n$$\n\\rho(k) = \\frac{C(k)}{C(0)} = \\frac{\\phi^k C(0)}{C(0)} = \\phi^k.\n$$\n\nWith $\\rho(k)$ established, we can calculate the two quantities. First, the exponential autocorrelation time, $\\tau_{\\mathrm{exp}}$, is defined by the model $\\rho(k) = \\exp(-k/\\tau_{\\mathrm{exp}})$. Equating this with our derived expression for $\\rho(k)$:\n$$\n\\exp\\left(-\\frac{k}{\\tau_{\\mathrm{exp}}}\\right) = \\phi^k.\n$$\nTaking the natural logarithm of both sides gives\n$$\n-\\frac{k}{\\tau_{\\mathrm{exp}}} = k \\ln(\\phi).\n$$\nFor $k \\neq 0$, we can solve for $\\tau_{\\mathrm{exp}}$:\n$$\n\\tau_{\\mathrm{exp}} = -\\frac{1}{\\ln(\\phi)}.\n$$\nSince $\\phi \\in (0, 1)$, $\\ln(\\phi)$ is negative, so $\\tau_{\\mathrm{exp}}$ is positive, as required.\n\nSecond, the statistical inefficiency, $g$, is defined as\n$$\ng = 1 + 2 \\sum_{k=1}^{\\infty} \\rho(k).\n$$\nSubstituting $\\rho(k) = \\phi^k$:\n$$\ng = 1 + 2 \\sum_{k=1}^{\\infty} \\phi^k.\n$$\nThe summation is a geometric series. For $|\\phi| < 1$, the sum converges to\n$$\n\\sum_{k=1}^{\\infty} \\phi^k = \\frac{\\phi}{1 - \\phi}.\n$$\nTherefore,\n$$\ng = 1 + 2 \\left(\\frac{\\phi}{1 - \\phi}\\right) = \\frac{1 - \\phi + 2\\phi}{1 - \\phi} = \\frac{1 + \\phi}{1 - \\phi}.\n$$\n\nFinally, we compute the desired ratio, $R(\\phi) = g/\\tau_{\\mathrm{exp}}$:\n$$\nR(\\phi) = \\frac{g}{\\tau_{\\mathrm{exp}}} = \\frac{\\frac{1+\\phi}{1-\\phi}}{-\\frac{1}{\\ln(\\phi)}} = -\\frac{(1+\\phi)\\ln(\\phi)}{1-\\phi}.\n$$\nThis is the final closed-form expression for the ratio in terms of $\\phi$.", "answer": "$$\n\\boxed{-\\frac{(1+\\phi)\\ln(\\phi)}{1-\\phi}}\n$$", "id": "2442417"}, {"introduction": "Now we transition from an abstract stochastic model to a concrete physical system, bridging the gap between statistical theory and computational practice. This problem challenges you to simulate the dynamics of a simplified microcanonical ensemble—a set of harmonic oscillators—and compute the autocorrelation of its instantaneous temperature [@problem_id:2442392]. This hands-on task involves generating time series data from first principles and then applying the full analysis pipeline to calculate the statistical inefficiency $g$ and the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, essential skills for any computational physicist.", "problem": "You are asked to construct a self-contained program that generates and analyzes deterministic time series for the instantaneous temperature of a microcanonical (constant Number, Volume, and Energy) ensemble (NVE) molecular dynamics (MD) system. Consider a system modeled as $M$ noninteracting, classical, one-dimensional harmonic oscillators with unit mass. For oscillator $j \\in \\{1,\\dots,M\\}$, the angular frequency is $\\omega_j$, the initial phase is $\\phi_j$, and the total energy is $E_j$. The position and velocity satisfy the equations of motion $\\ddot{x}_j(t) + \\omega_j^2 x_j(t) = 0$ with $m_j = 1$. The instantaneous kinetic energy of oscillator $j$ is $K_j(t) = \\tfrac{1}{2} v_j(t)^2$, and for harmonic motion this yields $K_j(t) = E_j \\sin^2(\\omega_j t + \\phi_j)$. The total kinetic energy is $K(t) = \\sum_{j=1}^M K_j(t)$.\n\nDefine the instantaneous temperature time series by\n$$\nT_{\\mathrm{inst}}(t) = \\frac{2}{f k_{\\mathrm{B}}}\\,K(t),\n$$\nwhere $f$ is the number of velocity degrees of freedom and $k_{\\mathrm{B}}$ is the Boltzmann constant. Use dimensionless units with $k_{\\mathrm{B}} = 1$ and $f = M$. Let the discrete sampling times be $t_n = n\\,\\Delta t$ for $n \\in \\{0,1,\\dots,N-1\\}$.\n\nFor a given discrete time series $\\{T_{\\mathrm{inst}}(t_n)\\}_{n=0}^{N-1}$, define the sample mean $\\mu = \\frac{1}{N} \\sum_{n=0}^{N-1} T_{\\mathrm{inst}}(t_n)$, the discrete autocovariance\n$$\n\\Gamma(\\ell) = \\frac{1}{N - \\ell} \\sum_{n=0}^{N-1-\\ell} \\big(T_{\\mathrm{inst}}(t_n) - \\mu\\big)\\,\\big(T_{\\mathrm{inst}}(t_{n+\\ell}) - \\mu\\big), \\quad \\ell \\in \\{0,1,\\dots,N-1\\},\n$$\nand the normalized autocorrelation\n$$\n\\rho(\\ell) = \\frac{\\Gamma(\\ell)}{\\Gamma(0)}.\n$$\nDefine the statistical inefficiency\n$$\ng = 1 + 2 \\sum_{\\ell=1}^{L^\\star} \\rho(\\ell),\n$$\nwhere $L^\\star$ is the largest nonnegative integer such that $\\rho(\\ell) > 0$ for all $\\ell \\in \\{1,2,\\dots,L^\\star\\}$ and either $L^\\star = N-1$ or $\\rho(L^\\star+1) \\le 0$. Define the integrated autocorrelation time by\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\Delta t}{2}\\, g,\n$$\nso that the variance of the sample mean scales as in the standard continuous-time relation.\n\nYour program must, for each test case below, generate $T_{\\mathrm{inst}}(t_n)$ from first principles using the model above, compute $\\rho(\\ell)$ until the first nonpositive value, and then compute $g$ and $\\tau_{\\mathrm{int}}$ using the definitions above.\n\nTest suite (all quantities are dimensionless):\n- Case $\\mathcal{A}$:\n  - $M = 1$, $N = 4096$, $\\Delta t = 0.01$,\n  - $\\omega_1 = 1.5$, $\\phi_1 = 0.37$, $E_1 = 1.0$.\n- Case $\\mathcal{B}$:\n  - $M = 2$, $N = 4096$, $\\Delta t = 0.01$,\n  - $(\\omega_1,\\omega_2) = \\big(1.0, \\sqrt{2}\\big)$,\n  - $(\\phi_1,\\phi_2) = (0.1, 1.3)$,\n  - $(E_1,E_2) = (1.0, 0.6)$.\n- Case $\\mathcal{C}$:\n  - $M = 10$, $N = 8192$, $\\Delta t = 0.005$,\n  - $\\omega_j$ linearly spaced over the closed interval $[0.5, 2.0]$ for $j \\in \\{1,\\dots,10\\}$,\n  - $\\phi_j = \\big(j \\pi / 7\\big) \\bmod (2\\pi)$ for $j \\in \\{1,\\dots,10\\}$,\n  - $E_j = 1.0$ for all $j \\in \\{1,\\dots,10\\}$.\n\nFor each case, report the pair $\\big[\\tau_{\\mathrm{int}}, g\\big]$. Express $\\tau_{\\mathrm{int}}$ in the same time unit as $t$ and $\\Delta t$. Your program should produce a single line of output containing the results as a comma-separated outer list of inner lists with no spaces, in the format $[[\\tau_{\\mathrm{int}}^{(\\mathcal{A})},g^{(\\mathcal{A})}],[\\tau_{\\mathrm{int}}^{(\\mathcal{B})},g^{(\\mathcal{B})}],[\\tau_{\\mathrm{int}}^{(\\mathcal{C})},g^{(\\mathcal{C})}]]$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, mathematically well-posed, and provides a complete and consistent set of definitions and data required for a unique solution. We may therefore proceed with the derivation and implementation of the solution.\n\nThe problem requires the computation of the statistical inefficiency, $g$, and the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for a time series of instantaneous temperature, $T_{\\mathrm{inst}}(t)$. This time series is generated from a deterministic model of $M$ non-interacting, one-dimensional classical harmonic oscillators.\n\nThe instantaneous temperature is defined in dimensionless units ($k_{\\mathrm{B}}=1$) with $f=M$ degrees of freedom as:\n$$\nT_{\\mathrm{inst}}(t) = \\frac{2}{M} K(t)\n$$\nwhere $K(t)$ is the total kinetic energy of the system. For a system of $M$ harmonic oscillators with unit mass ($m_j=1$), initial parameters $(\\omega_j, \\phi_j, E_j)$, the total kinetic energy is given by the sum of individual kinetic energies:\n$$\nK(t) = \\sum_{j=1}^{M} K_j(t) = \\sum_{j=1}^{M} E_j \\sin^2(\\omega_j t + \\phi_j)\n$$\nThus, the expression for the instantaneous temperature becomes:\n$$\nT_{\\mathrm{inst}}(t) = \\frac{2}{M} \\sum_{j=1}^{M} E_j \\sin^2(\\omega_j t + \\phi_j)\n$$\nThis function is sampled at discrete times $t_n = n\\,\\Delta t$ for $n \\in \\{0, 1, \\dots, N-1\\}$, yielding a discrete time series $\\{T_n\\} \\equiv \\{T_{\\mathrm{inst}}(t_n)\\}$.\n\nThe subsequent analysis of this time series follows a standard statistical procedure. First, we compute the sample mean of the series:\n$$\n\\mu = \\frac{1}{N} \\sum_{n=0}^{N-1} T_n\n$$\nNext, we compute the discrete, unbiased autocovariance function $\\Gamma(\\ell)$ for a time lag of $\\ell$ steps:\n$$\n\\Gamma(\\ell) = \\frac{1}{N - \\ell} \\sum_{n=0}^{N-1-\\ell} \\big(T_n - \\mu\\big)\\,\\big(T_{n+\\ell} - \\mu\\big)\n$$\nThis is normalized by the variance, $\\Gamma(0)$, to obtain the normalized autocorrelation function, $\\rho(\\ell)$:\n$$\n\\rho(\\ell) = \\frac{\\Gamma(\\ell)}{\\Gamma(0)}\n$$\nThe statistical inefficiency, $g$, quantifies the number of simulation steps required to obtain an independent sample. It is calculated by integrating the autocorrelation function:\n$$\ng = 1 + 2 \\sum_{\\ell=1}^{L^\\star} \\rho(\\ell)\n$$\nThe summation cutoff, $L^\\star$, is defined as the largest integer for which the autocorrelation $\\rho(\\ell)$ remains positive for all preceding lags, i.e., $\\rho(\\ell) > 0$ for all $\\ell \\in \\{1, 2, \\dots, L^\\star\\}$. The summation must terminate at the first lag where the autocorrelation function becomes non-positive. If $\\rho(\\ell) > 0$ for all $\\ell \\in \\{1, \\dots, N-1\\}$, then $L^\\star = N-1$.\n\nFinally, the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, is determined from the statistical inefficiency and the simulation time step, $\\Delta t$:\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\Delta t}{2}\\, g\n$$\nThis value represents the characteristic time over which the system's \"memory\" persists.\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n1.  Initialize the parameters for the specific case: $M, N, \\Delta t$, and the sets of oscillator parameters $\\{\\omega_j\\}$, $\\{\\phi_j\\}$, and $\\{E_j\\}$.\n2.  Generate the discrete time vector $t_n = n\\Delta t$ for $n = 0, \\dots, N-1$.\n3.  Construct the instantaneous temperature time series $\\{T_n\\}$ by evaluating the analytical expression for $T_{\\mathrm{inst}}(t_n)$. This is a vectorized operation for efficiency.\n4.  Calculate the sample mean $\\mu$ of the time series $\\{T_n\\}$.\n5.  Center the time series by subtracting the mean: $T'_n = T_n - \\mu$.\n6.  Calculate the variance $\\Gamma(0) = \\frac{1}{N}\\sum_{n=0}^{N-1} (T'_n)^2$.\n7.  Iterate for lag $\\ell$ from $1$ to $N-1$. In each step:\n    a. Compute the autocovariance sum $\\sum_{n=0}^{N-1-\\ell} T'_n T'_{n+\\ell}$.\n    b. Calculate $\\Gamma(\\ell)$ by dividing by the normalization factor $N-\\ell$.\n    c. Calculate $\\rho(\\ell) = \\Gamma(\\ell) / \\Gamma(0)$.\n    d. If $\\rho(\\ell) \\le 0$, the condition for termination is met. Set $L^\\star = \\ell-1$ and break the loop. The sum for $g$ is complete.\n    e. If $\\rho(\\ell) > 0$, add it to a running sum for the calculation of $g$. If the loop completes without breaking, $L^\\star=N-1$.\n8.  Compute the statistical inefficiency $g = 1 + 2 \\sum_{\\ell=1}^{L^\\star} \\rho(\\ell)$.\n9.  Compute the integrated autocorrelation time $\\tau_{\\mathrm{int}} = \\frac{\\Delta t}{2} g$.\n10. Store the resulting pair $[\\tau_{\\mathrm{int}}, g]$.\n\nThis procedure is implemented for each of the three test cases provided. The numerical implementation relies on the `numpy` library for efficient array manipulations. The final results are then formatted into a single string as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"M\": 1, \"N\": 4096, \"dt\": 0.01,\n            \"omegas\": np.array([1.5]),\n            \"phis\": np.array([0.37]),\n            \"Es\": np.array([1.0]),\n        },\n        # Case B\n        {\n            \"M\": 2, \"N\": 4096, \"dt\": 0.01,\n            \"omegas\": np.array([1.0, np.sqrt(2)]),\n            \"phis\": np.array([0.1, 1.3]),\n            \"Es\": np.array([1.0, 0.6]),\n        },\n        # Case C\n        {\n            \"M\": 10, \"N\": 8192, \"dt\": 0.005,\n            \"omegas\": np.linspace(0.5, 2.0, 10),\n            \"phis\": (np.arange(1, 11) * np.pi / 7) % (2 * np.pi),\n            \"Es\": np.ones(10),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_autocorrelation_properties(case)\n        results.append(result)\n\n    # Format the output string to be exactly as required, without spaces within lists.\n    output_str = f\"[{','.join(f'[{tau},{g}]' for tau, g in results)}]\"\n    print(output_str)\n\ndef calculate_autocorrelation_properties(params):\n    \"\"\"\n    Calculates tau_int and g for a single test case.\n\n    Args:\n        params (dict): A dictionary containing all parameters for the case.\n\n    Returns:\n        tuple: A tuple containing (tau_int, g).\n    \"\"\"\n    M = params[\"M\"]\n    N = params[\"N\"]\n    dt = params[\"dt\"]\n    omegas = params[\"omegas\"]\n    phis = params[\"phis\"]\n    Es = params[\"Es\"]\n\n    # 1. Generate the time series for instantaneous temperature\n    t = np.arange(N) * dt\n    T_inst = np.zeros(N)\n\n    # Sum contributions from each oscillator\n    for j in range(M):\n        arg = omegas[j] * t + phis[j]\n        T_inst += Es[j] * np.sin(arg)**2\n    \n    T_inst *= (2.0 / M)\n    \n    # 2. Calculate sample mean and center the series\n    mu = np.mean(T_inst)\n    T_prime = T_inst - mu\n\n    # 3. Calculate variance Gamma(0)\n    # The problem defines Gamma(l) with 1/(N-l) prefactor.\n    # For l=0, Gamma(0) = (1/N) * sum((T_n - mu)^2).\n    # np.var calculates (1/N) * sum((x-mean(x))^2), which matches.\n    gamma_0 = np.var(T_inst)\n\n    if gamma_0 == 0:\n        # If variance is zero, the series is constant.\n        # Autocorrelation is undefined, but g=1 and tau_int = dt/2 is a reasonable limit.\n        g = 1.0\n        tau_int = 0.5 * dt * g\n        return tau_int, g\n\n    # 4. Find L_star and sum rho(l)\n    sum_rho = 0.0\n    # L_star is largest l <= N-2 s.t. rho(1...l) > 0\n    # but the problem formulation uses l in {1,...,L_star}.\n    # We sum rho(l) for l > 0 as long as rho(l) is positive.\n    for l in range(1, N):\n        # Calculate autocovariance Gamma(l)\n        # Gamma(l) = 1/(N-l) * sum_{n=0}^{N-l-1} (T_n-mu)(T_{n+l}-mu)\n        numerator = np.dot(T_prime[0:N-l], T_prime[l:N])\n        gamma_l = numerator / (N - l)\n        \n        # Calculate normalized autocorrelation rho(l)\n        rho_l = gamma_l / gamma_0\n        \n        if rho_l > 0:\n            sum_rho += rho_l\n        else:\n            # First non-positive rho(l) found, stop summation.\n            break\n            \n    # 5. Calculate g and tau_int\n    # g = 1 + 2 * sum_{l=1}^{L_star} rho(l)\n    g = 1.0 + 2.0 * sum_rho\n    \n    # tau_int = (dt/2) * g\n    tau_int = 0.5 * dt * g\n    \n    return tau_int, g\n\nsolve()\n```", "id": "2442392"}, {"introduction": "Real-world simulation data is rarely as clean as our ideal models; a common and critical challenge is ensuring the system has reached equilibrium before collecting statistics. This advanced practice introduces the powerful blocking method for error estimation and asks you to tackle the problem of non-stationarity head-on [@problem_id:2442379]. By developing a quantitative criterion to detect a lack of equilibrium from the blocking curve itself, you will learn a vital diagnostic skill for validating Markov Chain Monte Carlo results and ensuring the reliability of your scientific conclusions.", "problem": "You are given the task of implementing the blocking (also called binning) method to estimate the standard error of the sample mean from correlated data produced by a Markov Chain Monte Carlo (MCMC) simulation and to design a quantitative criterion, based solely on the blocking curve, to detect lack of equilibrium (non-stationarity). The fundamental base you may assume includes: the definition of the autocovariance function of a stationary time series, the fact that the variance of the sample mean of independent and identically distributed (i.i.d.) data is the population variance divided by the sample size, and the empirical observation that the blocking method estimates the variance of the sample mean by treating block averages as approximately independent when the block size is larger than the autocorrelation time. You must not assume any shortcut formulas beyond these core definitions.\n\nDefinitions to use as a starting point:\n- Let $\\{X_t\\}_{t=1}^N$ be a scalar time series. For a stationary process with mean $\\mu$ and autocovariance function $\\gamma(k)=\\mathbb{E}[(X_t-\\mu)(X_{t+k}-\\mu)]$, the statistical inefficiency is defined as $g=1+2\\sum_{k=1}^{\\infty}\\rho(k)$ where $\\rho(k)=\\gamma(k)/\\gamma(0)$ is the autocorrelation function. The integrated autocorrelation time is $\\tau_{\\mathrm{int}} = g/2$. Under stationarity and mild mixing assumptions, the variance of the sample mean $\\bar{X}=\\frac{1}{N}\\sum_{t=1}^N X_t$ behaves as $\\mathrm{Var}(\\bar{X})\\approx \\frac{\\sigma^2 g}{N}$ where $\\sigma^2=\\gamma(0)$.\n- The blocking method partitions the data into $m$ non-overlapping consecutive blocks of equal size $b$ (so that $m=\\lfloor N/b\\rfloor$) and computes the block averages. For sufficiently large $b$, the block averages are approximately uncorrelated and their sample variance, divided by $m$, estimates the variance of the overall mean.\n\nYour program must:\n1) Construct synthetic MCMC-like time series according to the following rules. All chains are real-valued autoregressive processes of order one (AR(1)) with parameter $\\phi$ and time-dependent mean $\\mu_t$ given by the recursion $X_{t+1}=\\phi X_t + (1-\\phi)\\mu_t + \\eta_{t+1}$ where $\\eta_{t}$ are independent Gaussian random variables with zero mean and variance chosen such that the stationary variance (when $\\mu_t$ is constant and $|\\phi|<1$) is approximately unity. Explicitly, you must use $\\mathrm{Var}(\\eta_t)=1-\\phi^2$ so that for constant $\\mu_t$, the process has stationary variance close to $1$.\n2) For a given series $\\{X_t\\}_{t=1}^N$, implement the blocking estimator as follows. For block sizes $b=2^k$ with $k\\in\\{0,1,2,\\dots\\}$, include only those $b$ such that the number of full blocks $m=\\lfloor N/b\\rfloor$ is at least $8$. For each such $b$, compute the $m$ consecutive block averages and then compute the unbiased sample variance of the block averages. The estimated variance of the overall mean at block size $b$ is then the sample variance of the block averages divided by $m$. The corresponding standard error is the square root of that variance. This produces a sequence of standard error estimates as a function of $b$ (the blocking curve).\n3) Devise a quantitative criterion, based exclusively on the shape of the blocking curve at the largest accessible block sizes, to decide whether the chain has reached equilibrium. You must formalize the criterion using precise, testable inequalities. For this problem, define a “plateau” at the end of the blocking curve as follows: take the last $K$ valid block sizes (use $K=4$, or all available if fewer than $4$ exist), compute the median of their standard error estimates and the relative range, defined as $(\\max-\\min)/\\text{median}$ over those $K$ points. Declare the chain equilibrated if and only if the relative range over these tail points is less than or equal to a threshold $R_\\star$, with $R_\\star=0.15$, and there are at least $3$ tail points. Otherwise, declare it non-equilibrated.\n4) Using the plateau standard error estimate (taken as the median over the last $K$ block sizes), together with the sample variance $s^2$ of the raw series, construct an estimator of the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ by comparing the plateau variance of the mean to the i.i.d. variance-of-the-mean baseline. Your estimator must be a function of $N$, the plateau standard error, and $s^2$ only, and must be consistent with the fundamental definitions above.\n\nTest suite. You must run your code on the following four test cases, each defined by $(N,\\phi,\\mu_t,\\text{seed},X_1)$:\n- Case A (stationary, long chain): $N=65536$, $\\phi=0.9$, constant mean $\\mu_t\\equiv 0$, seed $=101$, initial value $X_1=0$.\n- Case B (slow relaxation, short chain, far-from-equilibrium start): $N=8192$, $\\phi=0.9995$, constant mean $\\mu_t\\equiv 0$, seed $=202$, initial value $X_1=20$.\n- Case C (stationary, short chain): $N=2048$, $\\phi=0.5$, constant mean $\\mu_t\\equiv 0$, seed $=303$, initial value $X_1=0$.\n- Case D (piecewise-constant mean shift): $N=65536$, $\\phi=0.9$, mean $\\mu_t=0$ for $t\\le N/2$ and $\\mu_t=1$ for $t> N/2$, seed $=404$, initial value $X_1=0$.\n\nFor each case, your program must output:\n- A boolean indicating whether the chain is declared equilibrated by your plateau criterion.\n- A floating-point estimate of $\\tau_{\\mathrm{int}}$ constructed from the plateau standard error and the raw sample variance as specified in item $4$.\n- A floating-point plateau standard error estimate as specified in item $3$.\n\nNumerical reporting requirements:\n- All floating-point outputs must be rounded to six decimal places.\n- There are no physical or angular units in this problem.\n- The final output must be a single line containing a JSON array of four elements, one per test case, where each element is a JSON array of the form $[\\text{equilibrated}, \\tau_{\\mathrm{int}}, \\text{SE}]$, with the boolean as a JSON boolean and the two floating-point numbers as decimals rounded to six places. For example, a valid output format is $[[\\text{true},1.234000,0.056000],[\\dots],\\dots]$.\n\nYour program should produce a single line of output containing the results as a JSON array of arrays, in the format described above, with no additional text. The program must be completely deterministic given the seeds above and must not require any user input.", "solution": "The problem is well-posed, scientifically sound, and provides a complete, self-contained specification for a computational task. It is therefore valid. I will proceed with the solution.\n\nThe task requires the implementation of the blocking method to analyze correlated time series data, specifically to estimate the standard error of the sample mean and to devise a criterion for detecting non-stationarity. Furthermore, an estimator for the integrated autocorrelation time must be derived and implemented.\n\nFirst, we address the generation of synthetic time series. The problem specifies an autoregressive process of order one, or AR(1), defined by the recurrence relation:\n$$X_{t+1} = \\phi X_t + (1-\\phi)\\mu_t + \\eta_{t+1}$$\nHere, $\\{X_t\\}_{t=1}^N$ is the time series of length $N$, $\\phi$ is the autoregressive parameter, $\\mu_t$ is a potentially time-dependent mean, and $\\eta_{t+1}$ are independent and identically distributed random variables drawn from a Gaussian distribution with mean zero, $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$. The variance of the noise is specified as $\\sigma_\\eta^2 = 1-\\phi^2$. This choice is deliberate; for a stationary process where $\\mu_t$ is a constant $\\mu$ and $|\\phi|<1$, the centered process $Y_t = X_t - \\mu$ follows $Y_t = \\phi Y_{t-1} + \\eta_t$. Its stationary variance is $\\mathrm{Var}(Y_t) = \\mathrm{Var}(\\eta_t) / (1-\\phi^2)$. With our choice of $\\mathrm{Var}(\\eta_t) = 1-\\phi^2$, the stationary variance of the process becomes $\\sigma^2 = \\mathrm{Var}(X_t) = 1$. This normalizes the intrinsic scale of the fluctuations.\n\nNext, we implement the blocking method. For a stationary time series $\\{X_t\\}_{t=1}^N$, the sample mean is $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^N X_t$. The variance of this sample mean is not simply $\\sigma^2/N$ as for independent data, but is amplified by correlations. The correct asymptotic formula is:\n$$\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2 g}{N}$$\nwhere $\\sigma^2$ is the process variance and $g$ is the statistical inefficiency. The blocking method provides an estimate of $\\mathrm{Var}(\\bar{X})$ directly. The data is partitioned into $m = \\lfloor N/b \\rfloor$ non-overlapping blocks of size $b$. For each block $j \\in \\{1, \\dots, m\\}$, we compute the block average:\n$$\\bar{X}_j^{(b)} = \\frac{1}{b} \\sum_{i=1}^{b} X_{(j-1)b+i}$$\nWhen the block size $b$ is significantly larger than the integrated autocorrelation time, the block averages $\\{\\bar{X}_j^{(b)}\\}_{j=1}^m$ become approximately independent and identically distributed. We can thus treat them as a set of $m$ independent samples. The variance of the mean of these samples, which is an estimator for $\\mathrm{Var}(\\bar{X})$, is given by the standard formula for the variance of a sample mean:\n$$V_b = \\frac{s^2_{\\text{blocks}}(b)}{m}$$\nwhere $s^2_{\\text{blocks}}(b)$ is the unbiased sample variance of the block averages:\n$$s^2_{\\text{blocks}}(b) = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(\\bar{X}_j^{(b)} - \\left(\\frac{1}{m}\\sum_{l=1}^m \\bar{X}_l^{(b)}\\right)\\right)^2$$\nThe standard error of the mean for block size $b$ is then $SE_b = \\sqrt{V_b}$. This procedure is performed for block sizes $b=2^k$ for $k=0, 1, 2, \\dots$, so long as the number of blocks $m$ is at least $8$. This generates the \"blocking curve\", a plot of $SE_b$ versus $b$.\n\nThe shape of the blocking curve is used to diagnose non-stationarity. For a stationary process, as $b$ increases, $SE_b$ initially rises (as short-range correlations are averaged out) and then converges to a plateau when $b \\gg \\tau_{\\mathrm{int}}$. For a non-stationary process, such as one with a drift or trend, the variance between blocks continues to increase with $b$, and $SE_b$ does not plateau. We formalize a plateau criterion as follows:\n1.  Consider the last $K$ valid block sizes, where $K$ is the minimum of $4$ and the total number of valid block sizes.\n2.  Let the corresponding standard error estimates be $\\{S_1, S_2, \\dots, S_K\\}$.\n3.  Compute the median, $S_{\\text{median}} = \\text{median}(S_1, \\dots, S_K)$.\n4.  Compute the relative range, $R = \\frac{\\max(S_1, \\dots, S_K) - \\min(S_1, \\dots, S_K)}{S_{\\text{median}}}$. A check for $S_{\\text{median}}>0$ is prudent.\n5.  The process is declared \"equilibrated\" (stationary) if and only if $R \\le R_\\star = 0.15$ and the number of tail points $K$ is at least $3$.\nThe plateau standard error estimate, $SE_{\\text{plateau}}$, is defined as $S_{\\text{median}}$ regardless of whether the equilibrium criterion is met.\n\nFinally, we construct an estimator for the integrated autocorrelation time, $\\hat{\\tau}_{\\mathrm{int}}$. We start from the fundamental relation $\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2 g}{N}$. We have two estimators from our data:\n- An estimate of $\\mathrm{Var}(\\bar{X})$ from the blocking method's plateau: $(SE_{\\text{plateau}})^2$.\n- An estimate of the process variance $\\sigma^2$ from the raw data's sample variance: $s^2 = \\frac{1}{N-1}\\sum_{t=1}^N (X_t - \\bar{X})^2$.\nSubstituting these into the relation gives:\n$$(SE_{\\text{plateau}})^2 \\approx \\frac{s^2 \\hat{g}}{N}$$\nSolving for our estimator $\\hat{g}$ yields:\n$$\\hat{g} = \\frac{N \\cdot (SE_{\\text{plateau}})^2}{s^2}$$\nThis formula estimates the statistical inefficiency $g$. The integrated autocorrelation time $\\tau_{\\mathrm{int}}$ is related by $g \\approx 2\\tau_{\\mathrm{int}}$ for a stationary series. Therefore, our estimator for $\\tau_{\\mathrm{int}}$ is:\n$$\\hat{\\tau}_{\\mathrm{int}} = \\frac{\\hat{g}}{2} = \\frac{N \\cdot (SE_{\\text{plateau}})^2}{2s^2}$$\nThis formula connects the macroscopic estimate of the mean's variance with the microscopic sample variance to extract the integrated autocorrelation time.\n\nThe overall algorithm proceeds by generating the time series for each test case, applying the blocking analysis to obtain the blocking curve, using the tail of this curve to test for equilibrium and find $SE_{\\text{plateau}}$, and finally combining this with the raw sample variance to compute $\\hat{\\tau}_{\\mathrm{int}}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the blocking method to estimate standard error and autocorrelation time\n    for AR(1) processes and applies a quantitative criterion for non-stationarity.\n    \"\"\"\n\n    def generate_ar1_series(N, phi, mu_func, seed, X1):\n        \"\"\"Generates an AR(1) time series.\"\"\"\n        rng = np.random.default_rng(seed)\n        noise_std = np.sqrt(1 - phi**2)\n        # Generate all noise values at once for efficiency\n        eta = rng.normal(loc=0.0, scale=noise_std, size=N - 1)\n        \n        X = np.zeros(N)\n        X[0] = X1\n        \n        for t in range(N - 1):\n            # mu_t influences the state at t+1\n            mu_t_val = mu_func(t)\n            X[t+1] = phi * X[t] + (1 - phi) * mu_t_val + eta[t]\n            \n        return X\n\n    test_cases = [\n        # Case A: Stationary, long chain\n        {'N': 65536, 'phi': 0.9, 'mu_func': lambda t: 0.0, 'seed': 101, 'X1': 0.0},\n        # Case B: Slow relaxation, far-from-equilibrium start\n        {'N': 8192, 'phi': 0.9995, 'mu_func': lambda t: 0.0, 'seed': 202, 'X1': 20.0},\n        # Case C: Stationary, short chain\n        {'N': 2048, 'phi': 0.5, 'mu_func': lambda t: 0.0, 'seed': 303, 'X1': 0.0},\n        # Case D: Piecewise-constant mean shift (non-stationary)\n        {'N': 65536, 'phi': 0.9, 'mu_func': lambda t, N_val=65536: 1.0 if (t + 1) > N_val / 2 else 0.0, 'seed': 404, 'X1': 0.0}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        phi = case['phi']\n        mu_func = case['mu_func']\n        seed = case['seed']\n        X1 = case['X1']\n\n        # 1. Generate time series\n        series = generate_ar1_series(N, phi, mu_func, seed, X1)\n        \n        # 2. Implement blocking estimator\n        se_values = []\n        k = 0\n        while True:\n            b = 2**k\n            m = N // b\n            if m < 8:\n                break\n            \n            # Truncate data to fit full blocks\n            data_to_block = series[:m*b]\n            # Reshape into m blocks of size b\n            blocks = data_to_block.reshape((m, b))\n            # Compute block averages\n            block_averages = blocks.mean(axis=1)\n            \n            # Unbiased sample variance of block averages\n            var_of_block_averages = np.var(block_averages, ddof=1)\n            \n            # Estimated variance of the overall mean\n            var_of_mean = var_of_block_averages / m\n            se = np.sqrt(var_of_mean)\n            se_values.append(se)\n            \n            k += 1\n\n        # 3. Apply equilibrium criterion\n        num_points = len(se_values)\n        K = min(4, num_points)\n        \n        is_equilibrated = False\n        se_plateau = 0.0\n        \n        if K > 0:\n            tail_points = np.array(se_values[-K:])\n            se_plateau = np.median(tail_points)\n\n            if K >= 3:\n                R_star = 0.15\n                se_min = np.min(tail_points)\n                se_max = np.max(tail_points)\n                \n                relative_range = 0.0\n                if se_plateau > 0:\n                    relative_range = (se_max - se_min) / se_plateau\n                \n                if relative_range <= R_star:\n                    is_equilibrated = True\n\n        # 4. Estimate tau_int\n        raw_sample_var = np.var(series, ddof=1)\n        tau_int = 0.0\n        if raw_sample_var > 0:\n            g_hat = (N * se_plateau**2) / raw_sample_var\n            # From g = 2*tau_int, we get tau_int = g/2\n            tau_int = g_hat / 2.0\n        \n        results.append(f\"[{str(is_equilibrated).lower()},{tau_int:.6f},{se_plateau:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2442379"}]}