{"hands_on_practices": [{"introduction": "The exponential distribution is fundamental in physics and statistics, often modeling waiting times or radioactive decay. This first exercise [@problem_id:2403697] serves as a perfect introduction to inverse transform sampling. You will walk through the entire process, from deriving the simple, elegant analytical formula for the inverse cumulative distribution function to using a statistical test to verify that the generated samples indeed follow the target distribution.", "problem": "You are asked to formalize, implement, and validate sampling from the exponential distribution using first principles. Consider the exponential distribution with rate parameter $\\lambda \\in (0,\\infty)$ and cumulative distribution function $F(x) = \\mathbb{P}(X \\le x)$.\n\nTask:\n1. Using the inverse transform principle, generate $n$ independent samples from the exponential distribution with rate $\\lambda$ from independent draws of a variable $U$ that is uniformly distributed on the unit interval. You must rely on the property that if $U$ is uniformly distributed on the unit interval, then $F^{-1}(U)$ has cumulative distribution function $F$.\n2. For each parameter triple $(\\lambda,n,\\alpha)$, with $\\lambda \\in (0,\\infty)$, $n \\in \\mathbb{N}$, and $\\alpha \\in (0,1)$ as a significance level, conduct a one-sample distributional goodness-of-fit test for the exponential distribution with rate $\\lambda$ based on the $n$ samples you generated. Report the $p$-value and a boolean decision that is $\\text{True}$ if you do not reject the null hypothesis that the data follow an exponential distribution with rate $\\lambda$ at significance level $\\alpha$, and $\\text{False}$ otherwise. The $p$-value must be reported as a real number.\n3. For reproducibility, initialize your pseudo-random number generator once at the beginning with the fixed integer seed $s=123456789$.\n\nTest suite:\n- Case $1$: $(\\lambda,n,\\alpha) = (0.7,1000,0.05)$\n- Case $2$: $(\\lambda,n,\\alpha) = (2.3,200,0.01)$\n- Case $3$: $(\\lambda,n,\\alpha) = (0.1,500,0.10)$\n- Case $4$: $(\\lambda,n,\\alpha) = (10.0,100,0.05)$\n- Case $5$: $(\\lambda,n,\\alpha) = (0.5,5,0.20)$\n\nYour program must:\n- Implement the sampler described in item $1$ to generate the $n$ samples for each case.\n- Implement the test described in item $2$ and compute the $p$-value for each case.\n- For each case $i \\in \\{1,2,3,4,5\\}$, output two values in order: the $p$-value rounded to six decimal places and the boolean decision as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets. The list must have length $10$ and be ordered as $[p_1,d_1,p_2,d_2,p_3,d_3,p_4,d_4,p_5,d_5]$, where $p_i$ is the $p$-value (rounded to six decimal places) for case $i$ and $d_i$ is the corresponding boolean decision.", "solution": "The problem requires the generation of random samples from an exponential distribution using the inverse transform method, followed by a statistical validation of these samples using a goodness-of-fit test. The solution is developed in two parts: first, the derivation of the sampling algorithm, and second, the specification of the statistical test.\n\n**1. Inverse Transform Sampling for the Exponential Distribution**\n\nThe exponential distribution is characterized by a rate parameter $\\lambda > 0$. Its probability density function (PDF) for a random variable $X$ is given by:\n$$\nf(x; \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} & \\text{if } x \\ge 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n$$\nThe cumulative distribution function (CDF), $F(x)$, represents the probability that the random variable $X$ takes a value less than or equal to $x$. It is obtained by integrating the PDF from $0$ to $x$:\n$$\nF(x) = \\mathbb{P}(X \\le x) = \\int_0^x \\lambda e^{-\\lambda t} dt = \\left[ -e^{-\\lambda t} \\right]_0^x = 1 - e^{-\\lambda x} \\quad \\text{for } x \\ge 0\n$$\nThe inverse transform sampling method is founded on the principle that if $U$ is a random variable drawn from a standard uniform distribution on the interval $(0, 1)$, then the random variable $X = F^{-1}(U)$ will have the distribution characterized by the CDF $F$.\n\nTo apply this method, we must first find the inverse of the CDF, denoted $F^{-1}(u)$. We set $u = F(x)$ and solve for $x$:\n$$\nu = 1 - e^{-\\lambda x}\n$$\nRearranging the terms to isolate $x$:\n$$\ne^{-\\lambda x} = 1 - u\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\lambda x = \\ln(1 - u)\n$$\nFinally, solving for $x$ yields the inverse CDF:\n$$\nx = F^{-1}(u) = -\\frac{1}{\\lambda} \\ln(1 - u)\n$$\nTherefore, to generate a sample $x$ from an exponential distribution with rate $\\lambda$, one generates a random number $u$ from $\\text{Uniform}(0,1)$ and applies the transformation $x = -\\frac{1}{\\lambda} \\ln(1 - u)$.\n\nA useful simplification arises from the property that if $U \\sim \\text{Uniform}(0,1)$, then the random variable $V = 1 - U$ is also distributed as $\\text{Uniform}(0,1)$. This allows us to replace $1 - u$ with $u$ in the formula, leading to the computationally equivalent and more direct expression:\n$$\nx = -\\frac{1}{\\lambda} \\ln(u)\n$$\nThis formula will be implemented to generate the $n$ samples for each test case. The procedure commences by initializing a pseudo-random number generator with the specified seed $s=123456789$ to ensure reproducibility. Then, for each test case $(\\lambda, n, \\alpha)$, we draw $n$ independent values $u_1, u_2, \\dots, u_n$ from $\\text{Uniform}(0,1)$ and compute the corresponding exponential samples $x_i = -\\frac{1}{\\lambda} \\ln(u_i)$.\n\n**2. Goodness-of-Fit Testing**\n\nAfter generating the sample data $\\{x_1, \\dots, x_n\\}$, a one-sample distributional goodness-of-fit test must be performed. The null hypothesis, $H_0$, is that the data are drawn from an exponential distribution with the specified rate parameter $\\lambda$. The alternative hypothesis, $H_1$, is that the data are not from this distribution.\n\nA standard and appropriate test for this scenario is the one-sample Kolmogorov-Smirnov (K-S) test. This test compares the empirical cumulative distribution function (ECDF) of the sample data, $F_n(x)$, with the theoretical CDF of the hypothesized distribution, $F(x) = 1 - e^{-\\lambda x}$. The ECDF is defined as:\n$$\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(x_i \\le x)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The K-S test statistic, $D_n$, is the maximum absolute difference between the ECDF and the theoretical CDF over all possible values of $x$:\n$$\nD_n = \\sup_x | F_n(x) - F(x) |\n$$\nThe distribution of the $D_n$ statistic under the null hypothesis is known and is independent of the specific continuous distribution being tested. This property allows for the calculation of a $p$-value, defined as the probability of observing a test statistic as extreme as, or more extreme than, the one computed from the data, assuming $H_0$ is true.\n\nThe decision rule is based on the significance level $\\alpha$:\n- If the $p$-value is less than $\\alpha$, we reject the null hypothesis $H_0$.\n- If the $p$-value is greater than or equal to $\\alpha$, we do not reject the null hypothesis $H_0$.\n\nThe problem requires a boolean decision that is $\\text{True}$ if we do not reject $H_0$. Thus, the decision for a given test case will be the result of the comparison $p\\text{-value} \\ge \\alpha$. For implementation, we will use the `scipy.stats.kstest` function. The exponential distribution implementation in `scipy` is parameterized by a location `loc` and a `scale` parameter. To match our target PDF $\\lambda e^{-\\lambda x}$, we must set `loc`$=0$ and `scale`$=1/\\lambda$.\n\nThe complete algorithm for each test case $(\\lambda, n, \\alpha)$ is:\n1. Generate $n$ uniform random numbers $u_1, \\dots, u_n$ using a generator seeded with $s=123456789$.\n2. Transform these into exponential samples $x_i = -\\frac{1}{\\lambda} \\ln(u_i)$.\n3. Perform the K-S test on the samples $\\{x_i\\}$ against the 'expon' distribution with parameters `loc=0` and `scale=1/\\lambda`.\n4. Obtain the $p$-value from the test result.\n5. Compute the boolean decision as $p\\text{-value} \\ge \\alpha$.\n6. Report the $p$-value rounded to six decimal places and the boolean decision.\n\nThis procedure will be executed for all five specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating exponential samples via inverse transform\n    and validating them with a Kolmogorov-Smirnov goodness-of-fit test.\n\n    The process adheres to the problem's requirements for validation,\n    reproducibility with a fixed seed, and specific output formatting.\n    \"\"\"\n    \n    # Global seed for reproducibility, as per problem statement.\n    # The modern `default_rng` is used for generating pseudo-random numbers.\n    seed = 123456789\n    rng = np.random.default_rng(seed)\n\n    # Test suite: each tuple is (lambda, n, alpha)\n    test_cases = [\n        (0.7, 1000, 0.05),\n        (2.3, 200, 0.01),\n        (0.1, 500, 0.10),\n        (10.0, 100, 0.05),\n        (0.5, 5, 0.20),\n    ]\n\n    results = []\n    for lambda_val, n, alpha in test_cases:\n        # Task 1: Generate n samples from the exponential distribution with rate lambda.\n        # This is implemented using the inverse transform method.\n        # If U is a random variable from Uniform(0, 1), then X = -ln(U)/lambda\n        # is a random variable from Exponential(lambda).\n        \n        # Generate n uniform random numbers in the interval [0.0, 1.0).\n        uniform_samples = rng.uniform(size=n)\n        \n        # Apply the inverse CDF transformation.\n        exponential_samples = -np.log(uniform_samples) / lambda_val\n\n        # Task 2: Conduct a one-sample Kolmogorov-Smirnov test.\n        # The null hypothesis (H0) is that the generated data follows an\n        # exponential distribution with the given rate lambda_val.\n        # The `scipy.stats.expon` distribution is parameterized by `loc` and `scale`.\n        # For a rate lambda, the scale parameter is 1/lambda and loc is 0.\n        scale_param = 1.0 / lambda_val\n        ks_statistic, p_value = stats.kstest(exponential_samples, 'expon', args=(0, scale_param))\n        \n        # The decision is True if we do not reject H0 at significance level alpha.\n        # This occurs when the p-value is greater than or equal to alpha.\n        decision = p_value >= alpha\n        \n        # Format results as specified in the problem statement.\n        p_value_rounded = round(p_value, 6)\n        \n        results.append(p_value_rounded)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) function correctly converts boolean values to \"True\"/\"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function to solve the problem and print the output.\nsolve()\n```", "id": "2403697"}, {"introduction": "Often, physical quantities are known to be constrained within a specific interval, making truncated distributions essential for realistic modeling. This practice [@problem_id:2403656] moves beyond simple distributions to tackle the truncated normal distribution. The key insight you will develop here is a powerful and general technique for handling truncation: instead of inverting the complex cumulative distribution function of the truncated distribution itself, we rescale the uniform variate to the correct probability mass and then use the inverse cumulative distribution function of the original, untruncated distribution.", "problem": "Consider the truncated normal distribution used in Monte Carlo simulation of risk-limited returns in computational economics and finance. Let $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$ be a normal random variable with mean $\\mu$ and standard deviation $\\sigma$, and define its truncation to the closed interval $[a,b]$ with $a \\le b$. The resulting truncated random variable $Y$ has support $[a,b]$ and probability density function\n$$\nf_Y(x) \\;=\\; \\begin{cases}\n\\dfrac{\\varphi\\!\\left(\\dfrac{x-\\mu}{\\sigma}\\right)}{\\sigma\\left[\\Phi\\!\\left(\\dfrac{b-\\mu}{\\sigma}\\right)-\\Phi\\!\\left(\\dfrac{a-\\mu}{\\sigma}\\right)\\right]} & \\text{for } x \\in [a,b],\\\\\n0 & \\text{otherwise,}\n\\end{cases}\n$$\nwhere $\\varphi(z) = \\dfrac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$ is the standard normal probability density function and $\\Phi(z) = \\displaystyle \\int_{-\\infty}^{z} \\varphi(t)\\,dt$ is the standard normal cumulative distribution function. Assume $\\sigma > 0$ and finite $a,b$.\n\nYour task is to write a complete, self-contained program that, for each specified parameter set, generates $n$ independent draws from the truncated distribution $Y$ defined above, using a pseudorandom number generator initialized with the provided seed for reproducibility, and then computes the sample mean of these draws. If $a=b$, interpret $Y$ as a degenerate distribution concentrated at $a$ and generate $n$ identical draws equal to $a$.\n\nAll quantities are unitless. Angles are not involved.\n\nInput is not provided to your program; instead, it must internally compute the sample means for the following test suite of parameter sets, each given as a tuple $(\\mu,\\sigma,a,b,n,\\text{seed})$:\n- Test $1$: $(\\mu,\\sigma,a,b,n,\\text{seed}) = (0,1,-1,2,10000,12345)$.\n- Test $2$: $(\\mu,\\sigma,a,b,n,\\text{seed}) = (0,1,3,4,20000,2021)$.\n- Test $3$: $(\\mu,\\sigma,a,b,n,\\text{seed}) = (1,0.5,1,1,7,999)$.\n- Test $4$: $(\\mu,\\sigma,a,b,n,\\text{seed}) = (-0.5,0.25,-0.6,-0.4,50000,314159)$.\n\nFor each test, compute the sample mean of the $n$ generated draws as a floating-point number. Your program should produce a single line of output containing the four sample means in the order of the tests above, formatted as a comma-separated list enclosed in square brackets, with no spaces. For example, an output with placeholder values would look like $[m_1,m_2,m_3,m_4]$, where each $m_i$ is a floating-point number. The final output must follow exactly this single-line format.", "solution": "The problem is subjected to validation.\n\nStep 1: Extract Givens.\n- The random variable $Y$ follows a truncated normal distribution on the support $[a,b]$.\n- Its probability density function is $f_Y(x) = \\dfrac{\\varphi(\\frac{x-\\mu}{\\sigma})}{\\sigma[\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma})]}$ for $x \\in [a,b]$, and $0$ otherwise.\n- $\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$ is the standard normal probability density function.\n- $\\Phi(z) = \\int_{-\\infty}^{z} \\varphi(t)\\,dt$ is the standard normal cumulative distribution function.\n- Constraints: $\\sigma > 0$, $a \\le b$, with $a$ and $b$ being finite.\n- Special case: If $a=b$, $Y$ is a degenerate distribution concentrated at $a$.\n- Task: Generate $n$ independent draws from $Y$ for given parameter sets, using a specified seed, and compute the sample mean.\n- Test Cases:\n    1. $(\\mu,\\sigma,a,b,n,\\text{seed}) = (0,1,-1,2,10000,12345)$\n    2. $(\\mu,\\sigma,a,b,n,\\text{seed}) = (0,1,3,4,20000,2021)$\n    3. $(\\mu,\\sigma,a,b,n,\\text{seed}) = (1,0.5,1,1,7,999)$\n    4. $(\\mu,\\sigma,a,b,n,\\text{seed}) = (-0.5,0.25,-0.6,-0.4,50000,314159)$\n- Output format: A single line containing a comma-separated list of the four sample means, enclosed in square brackets. `[m_1,m_2,m_3,m_4]`.\n\nStep 2: Validate Using Extracted Givens.\n- **Scientifically Grounded**: The problem is based on the well-established theory of probability distributions, specifically the truncated normal distribution, which is a standard tool in statistics and its applications, including computational finance. The provided PDF is correct. The problem is scientifically sound.\n- **Well-Posed**: The problem asks for the computation of a sample mean from a simulated dataset. The parameters for the simulation are fully specified, including a random seed for reproducibility. This constitutes a well-posed computational problem.\n- **Objective**: The problem is stated using precise mathematical language and definitions. It is entirely objective and free of subjective elements.\n- The problem is fully specified, internally consistent, and requires a formalizable solution based on standard statistical simulation techniques. It does not violate any of the specified invalidity criteria.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be developed.\n\nThe most efficient and robust method for generating random variates from this distribution is inverse transform sampling. This method relies on the inverse of the cumulative distribution function (CDF), also known as the quantile function. It is superior to alternatives like rejection sampling, whose efficiency can degrade severely if the truncation interval lies in the tails of the underlying normal distribution.\n\nFirst, we derive the CDF of the truncated random variable $Y$. For any $x \\in [a, b]$, the CDF $F_Y(x)$ is given by:\n$$ F_Y(x) = P(Y \\le x) = \\int_a^x f_Y(t) \\, dt = \\int_a^x \\frac{\\varphi\\left(\\frac{t-\\mu}{\\sigma}\\right)}{\\sigma\\left[\\Phi\\left(\\frac{b-\\mu}{\\sigma}\\right)-\\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)\\right]} \\, dt $$\nLet us define the standardized bounds $\\alpha = \\frac{a-\\mu}{\\sigma}$ and $\\beta = \\frac{b-\\mu}{\\sigma}$. The denominator is then $\\sigma[\\Phi(\\beta) - \\Phi(\\alpha)]$.\nFor the integral, we perform a change of variables: let $z = \\frac{t-\\mu}{\\sigma}$, which implies $dt = \\sigma \\, dz$. The integration limits change from $t=a$ to $z=\\alpha$, and from $t=x$ to $z=\\frac{x-\\mu}{\\sigma}$.\n$$ F_Y(x) = \\frac{1}{\\sigma[\\Phi(\\beta) - \\Phi(\\alpha)]} \\int_{\\alpha}^{\\frac{x-\\mu}{\\sigma}} \\varphi(z) (\\sigma \\, dz) = \\frac{1}{\\Phi(\\beta) - \\Phi(\\alpha)} \\int_{\\alpha}^{\\frac{x-\\mu}{\\sigma}} \\varphi(z) \\, dz $$\nBy definition of the standard normal CDF, $\\Phi(z)$, the integral is $\\Phi(\\frac{x-\\mu}{\\sigma}) - \\Phi(\\alpha)$. Thus, the CDF is:\n$$ F_Y(x) = \\frac{\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right) - \\Phi(\\alpha)}{\\Phi(\\beta) - \\Phi(\\alpha)} $$\nThe inverse transform sampling method states that if $U$ is a random variable uniformly distributed on $[0,1]$, then $X = F^{-1}(U)$ is a random variable with CDF $F$. We apply this principle. Let $u$ be a realization from a $U(0,1)$ distribution. We set $F_Y(y) = u$ and solve for $y$:\n$$ u = \\frac{\\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) - \\Phi(\\alpha)}{\\Phi(\\beta) - \\Phi(\\alpha)} $$\nRearranging the terms:\n$$ u[\\Phi(\\beta) - \\Phi(\\alpha)] + \\Phi(\\alpha) = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) $$\nTo isolate $y$, we apply the inverse standard normal CDF, $\\Phi^{-1}$:\n$$ \\frac{y-\\mu}{\\sigma} = \\Phi^{-1}\\left( u[\\Phi(\\beta) - \\Phi(\\alpha)] + \\Phi(\\alpha) \\right) $$\nFinally, solving for $y$ yields the sampling formula:\n$$ y = \\mu + \\sigma \\Phi^{-1}\\left( \\Phi(\\alpha) + u[\\Phi(\\beta) - \\Phi(\\alpha)] \\right) $$\nThe term $\\Phi(\\alpha) + u[\\Phi(\\beta) - \\Phi(\\alpha)]$ is simply a linear transformation of a uniform variate on $[0,1]$ to a uniform variate on $[\\Phi(\\alpha), \\Phi(\\beta)]$.\n\nThe algorithm for each test case is as follows:\n1.  Parse the parameters $(\\mu, \\sigma, a, b, n, \\text{seed})$.\n2.  Handle the degenerate case where $a=b$. The distribution is a point mass at $a$. All $n$ draws are equal to $a$, and the sample mean is therefore exactly $a$.\n3.  For the non-degenerate case ($a < b$):\n    a. Initialize a pseudorandom number generator with the given `seed`.\n    b. Generate $n$ independent random variates $u_1, u_2, \\ldots, u_n$ from the uniform distribution $U(0,1)$.\n    c. Compute the standardized bounds $\\alpha = (a-\\mu)/\\sigma$ and $\\beta = (b-\\mu)/\\sigma$.\n    d. Evaluate the standard normal CDF at these bounds: $C_a = \\Phi(\\alpha)$ and $C_b = \\Phi(\\beta)$.\n    e. For each $u_i$, compute the transformed variate $v_i = C_a + u_i(C_b - C_a)$.\n    f. Apply the inverse standard normal CDF (quantile function) to each $v_i$ to get a standard normal variate $z_i = \\Phi^{-1}(v_i)$.\n    g. Transform each $z_i$ back to the scale of the original truncated distribution to get the final samples: $y_i = \\mu + \\sigma z_i$.\n    h. Compute the sample mean of the generated draws: $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$.\n\nThis procedure will be implemented for each of the four test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of generating samples from a truncated normal distribution\n    and calculating their sample mean for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu, sigma, a, b, n, seed)\n        (0.0, 1.0, -1.0, 2.0, 10000, 12345),\n        (0.0, 1.0, 3.0, 4.0, 20000, 2021),\n        (1.0, 0.5, 1.0, 1.0, 7, 999),\n        (-0.5, 0.25, -0.6, -0.4, 50000, 314159)\n    ]\n\n    results = []\n    for mu, sigma, a, b, n, seed in test_cases:\n        # Handle the degenerate case where a = b.\n        # The distribution is a point mass at 'a'.\n        if a == b:\n            sample_mean = a\n            results.append(sample_mean)\n            continue\n\n        # Initialize the random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate n uniform random numbers in the interval [0, 1].\n        uniform_samples = rng.uniform(0.0, 1.0, n)\n\n        # Standardize the truncation bounds a and b.\n        alpha = (a - mu) / sigma\n        beta = (b - mu) / sigma\n\n        # Evaluate the standard normal CDF at the standardized bounds.\n        # norm.cdf is the implementation of the standard normal CDF, Phi.\n        cdf_a = norm.cdf(alpha)\n        cdf_b = norm.cdf(beta)\n\n        # Transform the uniform samples to be uniform on [cdf_a, cdf_b].\n        # This corresponds to the term Phi(alpha) + u * (Phi(beta) - Phi(alpha)).\n        transformed_samples = cdf_a + uniform_samples * (cdf_b - cdf_a)\n\n        # Apply the inverse standard normal CDF (percent point function or quantile function).\n        # norm.ppf is the implementation of the inverse standard normal CDF, Phi^-1.\n        standard_normal_variates = norm.ppf(transformed_samples)\n        \n        # Scale and shift the standard normal variates to obtain samples\n        # from the target truncated normal distribution.\n        truncated_normal_samples = mu + sigma * standard_normal_variates\n\n        # Compute the sample mean of the generated draws.\n        sample_mean = np.mean(truncated_normal_samples)\n        results.append(sample_mean)\n\n    # Final print statement in the exact required format.\n    # The format is a comma-separated list of floats enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2403656"}, {"introduction": "Our final practice connects inverse transform sampling directly to a cornerstone of quantum mechanics: the particle in a one-dimensional box. This problem [@problem_id:2403893] illustrates the full generality of the inverse transform method, as the cumulative distribution function for the particle's position cannot be inverted analytically. You will see how to bridge this gap by combining the sampling principle with numerical root-finding algorithms, a common and powerful workflow in computational science for exploring complex physical systems.", "problem": "A non-relativistic quantum particle is confined in a one-dimensional infinite potential well on the interval $[0,L]$ with perfectly rigid walls. The normalized stationary state wavefunctions are given by $\\psi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\!\\left(\\frac{n\\pi x}{L}\\right)$ for $x \\in [0,L]$ and integer $n \\ge 1$. Consider the first excited state with $n = 2$, so that the probability density on $[0,L]$ is $f(x) = \\lvert \\psi_2(x) \\rvert^2$. All trigonometric arguments must be interpreted in radians.\n\nTask:\n1. Starting from first principles, derive the cumulative distribution function $F(x)$ associated with $f(x)$, defined by $F(x) = \\int_{0}^{x} f(t)\\,dt$ for $x \\in [0,L]$. Prove that $F(x)$ is strictly increasing on $[0,L]$ and hence has a unique inverse $G(u)$ on $u \\in [0,1]$ such that $F(G(u)) = u$.\n2. Construct a program that, for each test case listed below, performs the following:\n   - Computes the function $F(x)$ and its inverse $G(u)$ such that for any $u \\in [0,1]$, the value $x = G(u)$ satisfies $\\lvert F(x) - u \\rvert \\le 10^{-12}$.\n   - Generates $N$ independent pseudorandom variates $u_1,\\dots,u_N$ that are uniformly distributed on the open interval $(0,1)$ using a pseudorandom number generator initialized with the specified integer seed for that test case.\n   - Transforms the uniform variates to samples $x_i = G(u_i)$ that follow the distribution with density $f(x)$ on $[0,L]$.\n   - Computes the following three summary statistics for the samples:\n     - The sample mean $m_N = \\frac{1}{N}\\sum_{i=1}^{N} x_i$ (express your answer in the same length unit as $L$).\n     - The sample second moment $s_N^{(2)} = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2$ (express your answer in the square of the length unit of $L$).\n     - The Kolmogorov–Smirnov statistic $D_N$ for the target cumulative distribution function $F$, defined by\n       $$D_N = \\max\\!\\left(\\max_{1\\le i\\le N}\\left(\\frac{i}{N} - F(x_{(i)})\\right), \\max_{1\\le i\\le N}\\left(F(x_{(i)}) - \\frac{i-1}{N}\\right)\\right)$$\n       where $x_{(1)} \\le \\cdots \\le x_{(N)}$ is the non-decreasing ordering of the samples. The statistic $D_N$ is dimensionless.\n3. Your program must produce a single line of output containing the results for all test cases as a comma-separated list of sublists, where each sublist contains the three values $[m_N, s_N^{(2)}, D_N]$ in that order. Each floating-point result in the output must be rounded to $6$ decimal places. For example, the required overall format is\n   $$\\big[\\,[m_{N,1}, s^{(2)}_{N,1}, D_{N,1}], [m_{N,2}, s^{(2)}_{N,2}, D_{N,2}], \\ldots \\big]$$\n   and the program must print exactly one line containing this list with the brackets and commas as shown.\n\nTest Suite:\n- Case $1$: $L = 1.0$, $N = 1000$, seed $= 12345$.\n- Case $2$: $L = 5.0$, $N = 2000$, seed $= 20231102$.\n- Case $3$: $L = 2.0$, $N = 1$, seed $= 42$.\n\nAngle unit specification: all angles appearing inside trigonometric functions must be in radians.\n\nPhysical units: report $m_N$ in the same unit as $L$, $s_N^{(2)}$ in the square of that unit, and $D_N$ as a unitless number.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list of sublists, each sublist of the form $[m_N,s_N^{(2)},D_N]$, enclosed in square brackets, for example: $[[a,b,c],[d,e,f],[g,h,i]]$.", "solution": "The problem requires the application of the inverse transform sampling method to generate random variates from a distribution defined by the quantum mechanical probability density of a particle in a one-dimensional infinite potential well. The steps involve analytical derivation of the probability and cumulative distribution functions, followed by a numerical implementation to generate samples and compute specified statistics. The problem is scientifically grounded and well-posed.\n\nThe system is a particle in an infinite potential well on the interval $[0, L]$. The stationary state wavefunctions are $\\psi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{n\\pi x}{L}\\right)$ for an integer quantum number $n \\ge 1$. We are concerned with the first excited state, which corresponds to $n=2$.\n\n**1. Probability Density Function (PDF)**\n\nThe probability density function $f(x)$ is given by the squared modulus of the wavefunction, $f(x) = |\\psi_2(x)|^2$.\nFor $n=2$, the wavefunction is:\n$$ \\psi_2(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{2\\pi x}{L}\\right) $$\nThe corresponding PDF for $x \\in [0, L]$ is:\n$$ f(x) = \\left| \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{2\\pi x}{L}\\right) \\right|^2 = \\frac{2}{L} \\sin^2\\left(\\frac{2\\pi x}{L}\\right) $$\nUsing the trigonometric power-reduction identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$, we can rewrite the PDF as:\n$$ f(x) = \\frac{2}{L} \\left( \\frac{1 - \\cos\\left(2 \\cdot \\frac{2\\pi x}{L}\\right)}{2} \\right) = \\frac{1}{L} \\left( 1 - \\cos\\left(\\frac{4\\pi x}{L}\\right) \\right) $$\nThis form is more convenient for integration.\n\n**2. Cumulative Distribution Function (CDF)**\n\nThe cumulative distribution function $F(x)$ is defined as the integral of the PDF from $0$ to $x$:\n$$ F(x) = \\int_{0}^{x} f(t) \\,dt = \\int_{0}^{x} \\frac{1}{L} \\left( 1 - \\cos\\left(\\frac{4\\pi t}{L}\\right) \\right) dt $$\nWe perform the integration term by term:\n$$ F(x) = \\frac{1}{L} \\left[ \\int_{0}^{x} 1 \\,dt - \\int_{0}^{x} \\cos\\left(\\frac{4\\pi t}{L}\\right) dt \\right] $$\n$$ F(x) = \\frac{1}{L} \\left[ t - \\frac{L}{4\\pi} \\sin\\left(\\frac{4\\pi t}{L}\\right) \\right]_{0}^{x} $$\nEvaluating at the limits gives:\n$$ F(x) = \\frac{1}{L} \\left[ \\left( x - \\frac{L}{4\\pi} \\sin\\left(\\frac{4\\pi x}{L}\\right) \\right) - \\left( 0 - \\frac{L}{4\\pi} \\sin(0) \\right) \\right] $$\n$$ F(x) = \\frac{x}{L} - \\frac{1}{4\\pi} \\sin\\left(\\frac{4\\pi x}{L}\\right) $$\nAs a check, we evaluate the CDF at the boundaries: $F(0) = 0$ and $F(L) = \\frac{L}{L} - \\frac{1}{4\\pi}\\sin(4\\pi) = 1 - 0 = 1$. This confirms the normalization.\n\n**3. Invertibility of the CDF**\n\nTo apply inverse transform sampling, the CDF $F(x)$ must have a unique inverse. A continuous function has a unique inverse if it is strictly monotonic. We can prove that $F(x)$ is strictly increasing on $[0, L]$ by examining its derivative. By the Fundamental Theorem of Calculus, $F'(x) = f(x)$.\n$$ F'(x) = f(x) = \\frac{2}{L} \\sin^2\\left(\\frac{2\\pi x}{L}\\right) $$\nThe term $\\frac{2}{L}$ is positive. The term $\\sin^2(\\theta)$ is non-negative for all real $\\theta$. It is equal to $0$ only when $\\sin(\\theta)=0$. For $\\theta = \\frac{2\\pi x}{L}$ with $x \\in [0, L]$, this occurs at $x=0$, $x=L/2$, and $x=L$.\nSince $F'(x) \\ge 0$ for all $x \\in [0, L]$ and $F'(x)$ is zero only at a finite number of isolated points, the function $F(x)$ is strictly increasing over the entire interval $[0, L]$. Therefore, a unique inverse function $G(u) = F^{-1}(u)$ exists for $u \\in [0, 1]$.\n\n**4. The Inverse Transform Sampling Algorithm**\n\nThe inverse transform method consists of two steps:\n1. Generate a random variate $u$ from the uniform distribution on $[0, 1]$, denoted $u \\sim U(0, 1)$.\n2. Compute the sample $x$ by applying the inverse CDF: $x = G(u) = F^{-1}(u)$.\n\nThe equation we must solve for $x$ is $F(x) = u$, or:\n$$ \\frac{x}{L} - \\frac{1}{4\\pi} \\sin\\left(\\frac{4\\pi x}{L}\\right) = u $$\nThis is a transcendental equation that cannot be solved for $x$ in a simple closed form. We must employ a numerical root-finding algorithm. For a given $u$, we seek the root $x$ of the function $h(x) = F(x) - u = 0$ on the interval $[0, L]$. Since $h(0) = F(0) - u = -u < 0$ and $h(L) = F(L) - u = 1 - u > 0$ for $u \\in (0, 1)$, the root is bracketed in $[0, L]$. A robust and efficient algorithm like Brent's method is suitable for finding this root to the required precision.\n\n**5. Computation of Summary Statistics**\n\nFor a set of $N$ samples $\\{x_1, x_2, \\ldots, x_N\\}$ generated by this method, we compute the following statistics:\n- **Sample Mean ($m_N$)**: The arithmetic average of the samples.\n$$ m_N = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\n- **Sample Second Moment ($s_N^{(2)}$)**: The arithmetic average of the squares of the samples.\n$$ s_N^{(2)} = \\frac{1}{N} \\sum_{i=1}^{N} x_i^2 $$\n- **Kolmogorov–Smirnov (KS) Statistic ($D_N$)**: This statistic measures the maximum distance between the empirical distribution function (EDF) of the samples and the theoretical cumulative distribution function $F(x)$. For a sorted sample set $x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(N)}$, the statistic is defined as:\n$$ D_N = \\max\\left( \\max_{1 \\le i \\le N}\\left(\\frac{i}{N} - F(x_{(i)})\\right), \\max_{1 \\le i \\le N}\\left(F(x_{(i)}) - \\frac{i-1}{N}\\right) \\right) $$\nThe algorithm will be implemented for each test case, generating $N$ samples using the specified seed, calculating these three statistics, and reporting the results in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the inverse transform sampling problem for a particle in a box.\n    \"\"\"\n    test_cases = [\n        # (L, N, seed)\n        (1.0, 1000, 12345),\n        (5.0, 2000, 20231102),\n        (2.0, 1, 42),\n    ]\n\n    def F_cdf(x, L):\n        \"\"\"\n        Computes the theoretical cumulative distribution function (CDF) F(x).\n        F(x) = x/L - sin(4*pi*x/L) / (4*pi)\n        \"\"\"\n        arg = 4.0 * np.pi * x / L\n        return (x / L) - (np.sin(arg) / (4.0 * np.pi))\n\n    all_results = []\n\n    for L, N, seed in test_cases:\n        # 1. Initialize the pseudorandom number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n\n        # 2. Generate N independent pseudorandom variates uniformly distributed on (0,1).\n        u_samples = rng.uniform(0.0, 1.0, size=N)\n\n        # 3. For each uniform variate u, find the corresponding x by inverting the CDF.\n        # This requires solving F(x) - u = 0 for x.\n        x_samples = np.zeros(N)\n        for i, u in enumerate(u_samples):\n            # The function to find the root of.\n            func_to_solve = lambda x: F_cdf(x, L) - u\n            \n            # Use Brent's method to find the root in the interval [0, L].\n            # The root is guaranteed to be in this interval as F(0)=0 and F(L)=1.\n            # h(0) = -u < 0 and h(L) = 1-u > 0 for u in (0,1).\n            x_samples[i] = brentq(func_to_solve, 0.0, L)\n\n        # 4. Compute the required summary statistics.\n        # Sample mean\n        m_N = np.mean(x_samples)\n\n        # Sample second moment\n        s2_N = np.mean(np.square(x_samples))\n\n        # Kolmogorov-Smirnov statistic\n        if N > 0:\n            x_sorted = np.sort(x_samples)\n            F_values = F_cdf(x_sorted, L)\n            \n            # i/N for i=1,...,N\n            i_over_N = np.arange(1, N + 1) / N\n            # (i-1)/N for i=1,...,N\n            i_minus_1_over_N = np.arange(0, N) / N\n            \n            # D_N = max(max(i/N - F(x_i)), max(F(x_i) - (i-1)/N))\n            max1 = np.max(i_over_N - F_values)\n            max2 = np.max(F_values - i_minus_1_over_N)\n            D_N = np.max([max1, max2])\n        else:\n            # Although not in the test suite, handle the N=0 case gracefully.\n            m_N, s2_N, D_N = 0.0, 0.0, 0.0\n\n        all_results.append([m_N, s2_N, D_N])\n\n    # 5. Format the final output string exactly as specified.\n    # Each sublist is formatted, then joined by commas, then enclosed in brackets.\n    sublist_strs = [f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\" for res in all_results]\n    final_output_str = f\"[{','.join(sublist_strs)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "2403893"}]}