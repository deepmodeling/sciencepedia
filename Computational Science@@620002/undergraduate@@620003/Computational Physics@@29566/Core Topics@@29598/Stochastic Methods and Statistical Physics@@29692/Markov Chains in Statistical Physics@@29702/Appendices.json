{"hands_on_practices": [{"introduction": "The random walk is a cornerstone of statistical physics, providing the microscopic foundation for understanding diffusion. This exercise connects the simple, memoryless steps of a single particle to the macroscopic diffusion coefficient. By deriving this relationship from first principles and then simulating the walk on two different lattice geometries—square and honeycomb—you will directly test the universality of the diffusion law and build a foundational skill set in both analytical theory and computational modeling [@problem_id:2445720].", "problem": "You will study unbiased discrete-time nearest-neighbor random walks on two infinite two-dimensional lattices: the square lattice and the honeycomb lattice. Each walker starts at the origin at time step $0$, takes steps of fixed length $a$ (in meters) at fixed time interval $\\tau$ (in seconds), and at each step chooses uniformly among the nearest neighbors allowed by the lattice geometry. The honeycomb lattice is modeled with three bond directions at mutual angles of $120^\\circ$; the square lattice is modeled with four cardinal directions. Your objective is to quantify and compare diffusion on these two lattices from first principles and to confirm or refute any lattice-dependent difference in the long-time diffusion coefficient. All numerical results must be expressed in $\\mathrm{m}^2/\\mathrm{s}$.\n\nRequired derivation and implementation tasks:\n- Starting from the definition of the random walk displacement $\\mathbf{r}_n = \\sum_{k=1}^{n} \\mathbf{s}_k$ after $n$ steps and the defining relation for diffusion in two spatial dimensions, derive an estimator for the diffusion coefficient $D$ in terms of the mean squared displacement $\\langle r_n^2 \\rangle$ accumulated over $n$ steps during a total physical time $t = n \\tau$. Your derivation must rely only on the core properties that the increments are independent given the current state, have zero mean by lattice symmetry at each step, and have fixed length $a$. Do not assume any lattice-specific shortcut formulas.\n- Implement two independent simulators:\n  1. Square lattice: at each step choose uniformly among the $4$ unit vectors $(\\pm 1, 0)$, $(0, \\pm 1)$ scaled by $a$.\n  2. Honeycomb lattice: use the three bond vectors of length $a$, $\\mathbf{b}_1 = a (1, 0)$, $\\mathbf{b}_2 = a (-\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2})$, $\\mathbf{b}_3 = a (-\\tfrac{1}{2}, -\\tfrac{\\sqrt{3}}{2})$, such that from one sublattice the outgoing choices are $\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}$ and from the other sublattice they are $\\{-\\mathbf{b}_1, -\\mathbf{b}_2, -\\mathbf{b}_3\\}$. Start at the origin on one sublattice and alternate sublattices with each step.\n- For each test case below, simulate $M$ independent walkers for $n$ steps on each lattice, compute the mean squared displacement $\\langle r_n^2 \\rangle$ over the $M$ walkers for each lattice separately, and convert it into an estimate $\\hat{D}$ of the diffusion coefficient using your derived estimator. Use the same $M$, $n$, $a$, and $\\tau$ for both lattices in a given test case. If a random seed is provided, use it to produce reproducible results.\n\nPhysical units and output requirements:\n- Report every diffusion coefficient in $\\mathrm{m}^2/\\mathrm{s}$ as a floating-point number. Do not print unit strings; only print the numeric values.\n- Angles, where relevant in the honeycomb geometry, must be understood in radians internally; the algorithm as specified above already encodes the directions, so you do not need to handle angles explicitly in the program output.\n\nTest suite (each tuple is $(n, a, \\tau, M, \\text{seed})$ and all quantities are to be interpreted in the International System of Units):\n- Case A (happy path): $(20000, 1.0 \\times 10^{-9}, 1.0 \\times 10^{-12}, 200, 12345)$.\n- Case B (boundary condition, single step): $(1, 5.0 \\times 10^{-10}, 2.0 \\times 10^{-13}, 1000, 54321)$.\n- Case C (short walk, many trials): $(2000, 2.0 \\times 10^{-10}, 1.0 \\times 10^{-13}, 500, 202311)$.\n- Case D (different scales): $(5000, 3.0 \\times 10^{-10}, 5.0 \\times 10^{-13}, 300, 777)$.\n\nFinal output format:\n- Your program must produce a single line of output containing a list of results, one per test case, in the order A, B, C, D. For each test case, output a list of three floating-point numbers $[\\hat{D}_{\\text{square}}, \\hat{D}_{\\text{honeycomb}}, D_{\\text{theory}}]$, where $D_{\\text{theory}}$ is the diffusion coefficient predicted by your derivation under the stated assumptions. The full output must therefore be a list of lists, for example, $[[x_1, y_1, z_1], [x_2, y_2, z_2], [x_3, y_3, z_3], [x_4, y_4, z_4]]$, with all values in $\\mathrm{m}^2/\\mathrm{s}$. Only this single-line list must be printed.", "solution": "The problem requires a first-principles derivation of an estimator for the diffusion coefficient, $D$, for a two-dimensional discrete random walk, and its subsequent numerical verification on square and honeycomb lattices. The validation of the problem statement finds it to be scientifically sound, well-posed, and complete. We proceed with the solution.\n\nThe primary task is to establish a relationship between the microscopic parameters of the random walk and the macroscopic diffusion coefficient. The position of a walker after $n$ steps, $\\mathbf{r}_n$, is the sum of individual step vectors $\\mathbf{s}_k$:\n$$ \\mathbf{r}_n = \\sum_{k=1}^{n} \\mathbf{s}_k $$\nThe quantity of interest is the mean squared displacement (MSD), $\\langle r_n^2 \\rangle$, where the average $\\langle \\cdot \\rangle$ is taken over an ensemble of independent walkers. The squared displacement is the dot product of the position vector with itself:\n$$ r_n^2 = \\mathbf{r}_n \\cdot \\mathbf{r}_n = \\left( \\sum_{i=1}^{n} \\mathbf{s}_i \\right) \\cdot \\left( \\sum_{j=1}^{n} \\mathbf{s}_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\mathbf{s}_i \\cdot \\mathbf{s}_j $$\nBy linearity of the expectation operator, the MSD is:\n$$ \\langle r_n^2 \\rangle = \\left\\langle \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\mathbf{s}_i \\cdot \\mathbf{s}_j \\right\\rangle = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle $$\nWe decompose this double summation into terms where the indices are equal ($i=j$) and terms where they are not ($i \\neq j$):\n$$ \\langle r_n^2 \\rangle = \\sum_{i=1}^{n} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_i \\rangle + \\sum_{i \\neq j} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle $$\nThe problem states that the step increments are independent. Therefore, for $i \\neq j$, the expectation of the product is the product of the expectations: $\\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle = \\langle \\mathbf{s}_i \\rangle \\cdot \\langle \\mathbf{s}_j \\rangle$. The problem also specifies that the walk is unbiased, meaning the choice of step direction is from a symmetric set, which results in a zero mean for any step vector: $\\langle \\mathbf{s}_k \\rangle = \\mathbf{0}$ for all $k$. This holds true for both the square and honeycomb lattices as defined. Consequently, all cross-terms ($i \\neq j$) vanish:\n$$ \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle = \\mathbf{0} \\cdot \\mathbf{0} = 0 \\quad \\text{for } i \\neq j $$\nThe MSD expression simplifies to the sum of the diagonal terms:\n$$ \\langle r_n^2 \\rangle = \\sum_{i=1}^{n} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_i \\rangle = \\sum_{i=1}^{n} \\langle |\\mathbf{s}_i|^2 \\rangle $$\nThe step length is given as a fixed constant, $|\\mathbf{s}_k| = a$. Thus, its square is also constant, $|\\mathbf{s}_k|^2 = a^2$, and its expectation is simply $a^2$. The sum becomes:\n$$ \\langle r_n^2 \\rangle = \\sum_{i=1}^{n} a^2 = n a^2 $$\nThis is the theoretical MSD for an unbiased random walk with fixed step length after $n$ steps.\nIn the continuous limit, diffusion in two dimensions is described by the Einstein relation, which connects the MSD to the diffusion coefficient $D$ and time $t$:\n$$ \\langle r^2(t) \\rangle = 4 D t $$\nWe relate our discrete model to this continuous description by setting the total time $t$ to be the number of steps $n$ multiplied by the time per step $\\tau$, i.e., $t = n \\tau$. Equating the two expressions for MSD gives:\n$$ n a^2 = 4 D (n \\tau) $$\nSolving for $D$ yields the theoretical diffusion coefficient, which we denote as $D_{\\text{theory}}$:\n$$ D_{\\text{theory}} = \\frac{a^2}{4 \\tau} $$\nThis theoretical result is general for any lattice that satisfies the initial assumptions and is, notably, independent of the lattice coordination number or geometry. This derivation refutes the idea of a lattice-dependent diffusion coefficient under these model conditions.\n\nFor the numerical part of the problem, we must estimate $D$ from simulation data. A simulation of $M$ walkers for $n$ steps produces a set of final squared displacements $\\{r_{n,1}^2, r_{n,2}^2, \\dots, r_{n,M}^2\\}$. The simulated MSD, $\\langle r_n^2 \\rangle_{\\text{sim}}$, is the sample mean of these values. We use this empirical result in the diffusion relation to find our estimator, $\\hat{D}$:\n$$ \\langle r_n^2 \\rangle_{\\text{sim}} = 4 \\hat{D} (n \\tau) \\implies \\hat{D} = \\frac{\\langle r_n^2 \\rangle_{\\text{sim}}}{4 n \\tau} $$\nThis is the estimator to be implemented.\n\nThe implementation will consist of two independent simulators. Both will track a population of $M$ walkers, represented by a NumPy array of shape $(M, 2)$, initialized to zeros. For each of the $n$ time steps, a random step vector is chosen for each walker and added to its current position. This process is vectorized for efficiency.\n\nFor the square lattice, the set of possible step vectors is $\\{\\,(a, 0), (-a, 0), (0, a), (0, -a)\\,\\}$. At each step, one of these $4$ vectors is chosen with uniform probability $p=1/4$.\n\nFor the honeycomb lattice, the structure is bipartite. The set of step vectors depends on the sublattice the walker currently occupies. Let the sublattices be $A$ and $B$. A walker starting on $A$ must step to $B$, and from $B$ must step to $A$. We define the basis bond vectors as $\\mathbf{b}_1 = a(1, 0)$, $\\mathbf{b}_2 = a(-\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2})$, and $\\mathbf{b}_3 = a(-\\tfrac{1}{2}, -\\tfrac{\\sqrt{3}}{2})$.\n- From sublattice $A$ (e.g., at even-numbered steps $0, 2, \\dots$), the allowed steps are $\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}$, each with probability $p=1/3$.\n- From sublattice $B$ (e.g., at odd-numbered steps $1, 3, \\dots$), the allowed steps are $\\{-\\mathbf{b}_1, -\\mathbf{b}_2, -\\mathbf{b}_3\\}$, each with probability $p=1/3$.\n\nAfter $n$ steps, the final squared displacement for each walker is computed, the mean is taken over all $M$ walkers to get $\\langle r_n^2 \\rangle_{\\text{sim}}$, and this is used to calculate $\\hat{D}_{\\text{square}}$ and $\\hat{D}_{\\text{honeycomb}}$. These simulated values will be compared against the derived $D_{\\text{theory}} = a^2 / (4\\tau)$. Due to statistical fluctuations inherent in a finite simulation ($M < \\infty$), the estimated values $\\hat{D}$ will deviate slightly from $D_{\\text{theory}}$, but should converge to it as $M$ and $n$ become large.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_square(n, a, tau, M, seed):\n    \"\"\"\n    Simulates a random walk on a 2D square lattice.\n\n    Args:\n        n (int): Number of steps.\n        a (float): Step length in meters.\n        tau (float): Time interval per step in seconds.\n        M (int): Number of independent walkers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: Estimated diffusion coefficient in m^2/s.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Initialize all M walkers at the origin (0, 0).\n    positions = np.zeros((M, 2), dtype=np.float64)\n    # Define the four possible step vectors.\n    steps_set = a * np.array([[1.0, 0.0], [-1.0, 0.0], [0.0, 1.0], [0.0, -1.0]])\n\n    for _ in range(n):\n        # For each of the M walkers, choose one of the 4 steps randomly.\n        choices = rng.integers(0, 4, size=M)\n        # Get the corresponding displacement vectors for all walkers.\n        displacements = steps_set[choices]\n        # Update all positions simultaneously.\n        positions += displacements\n    \n    # For n=0, division by zero occurs. Problem cases have n>=1.\n    if n == 0:\n        return 0.0\n\n    # Calculate the squared displacement from the origin for each walker.\n    squared_displacements = np.sum(positions**2, axis=1)\n    # Compute the mean squared displacement over all walkers.\n    mean_squared_displacement = np.mean(squared_displacements)\n    \n    # Use the derived estimator for the diffusion coefficient D.\n    D_hat = mean_squared_displacement / (4.0 * n * tau)\n    return D_hat\n\ndef simulate_honeycomb(n, a, tau, M, seed):\n    \"\"\"\n    Simulates a random walk on a 2D honeycomb lattice.\n\n    Args:\n        n (int): Number of steps.\n        a (float): Step length in meters.\n        tau (float): Time interval per step in seconds.\n        M (int): Number of independent walkers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: Estimated diffusion coefficient in m^2/s.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Initialize all M walkers at the origin (0, 0) on sublattice A.\n    positions = np.zeros((M, 2), dtype=np.float64)\n    \n    sqrt3_div_2 = np.sqrt(3.0) / 2.0\n    # Define bond vectors for sublattice A -> B transitions.\n    steps_A = a * np.array([[1.0, 0.0], [-0.5, sqrt3_div_2], [-0.5, -sqrt3_div_2]])\n    # Bond vectors for sublattice B -> A are the negative of A -> B.\n    steps_B = -steps_A\n\n    for i in range(n):\n        # For each of the M walkers, choose one of the 3 steps randomly.\n        choices = rng.integers(0, 3, size=M)\n        # Select step set based on sublattice (even/odd step number).\n        if i % 2 == 0:  # Walker is on sublattice A\n            displacements = steps_A[choices]\n        else:  # Walker is on sublattice B\n            displacements = steps_B[choices]\n        # Update all positions simultaneously.\n        positions += displacements\n\n    if n == 0:\n        return 0.0\n\n    # Calculate and average the squared displacements.\n    squared_displacements = np.sum(positions**2, axis=1)\n    mean_squared_displacement = np.mean(squared_displacements)\n    \n    # Estimate D.\n    D_hat = mean_squared_displacement / (4.0 * n * tau)\n    return D_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a, tau, M, seed)\n        (20000, 1.0e-9, 1.0e-12, 200, 12345),  # Case A\n        (1, 5.0e-10, 2.0e-13, 1000, 54321),    # Case B\n        (2000, 2.0e-10, 1.0e-13, 500, 202311), # Case C\n        (5000, 3.0e-10, 5.0e-13, 300, 777),      # Case D\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, a, tau, M, seed = case\n        \n        # Run simulation for the square lattice.\n        D_square = simulate_square(n, a, tau, M, seed)\n        \n        # Run simulation for the honeycomb lattice.\n        D_honeycomb = simulate_honeycomb(n, a, tau, M, seed)\n        \n        # Calculate the theoretical diffusion coefficient.\n        D_theory = (a**2) / (4.0 * tau)\n        \n        all_results.append([D_square, D_honeycomb, D_theory])\n\n    # The str() representation of a list of lists matches the required format\n    # with spaces after commas.\n    print(str(all_results))\n\nsolve()\n```", "id": "2445720"}, {"introduction": "We now move from a single, non-interacting particle to a system of many interacting units, a central theme in statistical mechanics. The Ising model is a paradigm for studying collective behaviors like phase transitions, and this practice challenges you to simulate it using Gibbs sampling, a powerful Markov Chain Monte Carlo (MCMC) method [@problem_id:2411722]. You will implement the algorithm to sample configurations from the Boltzmann distribution and compute key physical observables to numerically investigate the celebrated order-disorder transition.", "problem": "You will implement a Markov Chain Monte Carlo simulator using Gibbs sampling (also called the heat-bath algorithm) for a two-dimensional binary alloy modeled as an Ising-like lattice system. Consider a square lattice with periodic boundary conditions of linear size $L$, so the number of lattice sites is $N = L^2$. Each site $i$ carries a binary variable $s_i \\in \\{-1, +1\\}$, representing one of two atomic species. The energy of a configuration is given by the Ising Hamiltonian\n$$\n\\mathcal{H}(s) = - J \\sum_{\\langle i,j \\rangle} s_i s_j,\n$$\nwhere the sum is over nearest-neighbor pairs on the square lattice, and $J$ is a coupling constant. You will use Gibbs sampling to construct a Markov chain that is ergodic and satisfies detailed balance with respect to the Boltzmann distribution at inverse temperature $\\beta = 1/T$ with Boltzmann constant $k_B$ set to $1$ (that is, temperatures are expressed in units of $J/k_B$ with $k_B=1$). You must explicitly implement periodic boundary conditions and use a checkerboard (red-black) update scheme so that all sites of one sublattice are updated simultaneously using the current values of their neighbors.\n\nStarting from the fundamental definition that equilibrium probabilities are proportional to the Boltzmann weight $e^{-\\beta \\mathcal{H}(s)}$, derive the single-site conditional probability that underlies the Gibbs update used to resample a site $s_i$ given its neighbors at fixed temperature $T$ and coupling $J$. Use that conditional to implement a single-site Gibbs sampler that updates the entire lattice by alternating sublattice updates. Initialize spins randomly and allow a burn-in period before collecting measurements.\n\nFor each state of the Markov chain after burn-in, compute the following observables:\n- The instantaneous magnetization per site $m = \\frac{1}{N} \\sum_i s_i$ and its absolute value $|m|$.\n- The instantaneous energy per site $e = \\frac{1}{N} \\mathcal{H}(s)$, computed with each nearest-neighbor pair counted exactly once.\n- The Binder cumulant $U_4 = 1 - \\frac{\\langle m^4 \\rangle}{3 \\langle m^2 \\rangle^2}$, where angle brackets denote the time average over measurement samples of the Markov chain at fixed temperature and lattice size.\n\nDesign your simulator so that, for a fixed random number generator seed, it produces reproducible results. Your program must run the following test suite of parameter sets and report, for each test case, the time-averaged absolute magnetization $\\langle |m| \\rangle$, the time-averaged energy per site $\\langle e \\rangle$, and the Binder cumulant $U_4$. All temperatures are in units where $k_B=1$, so $T$ is expressed in units of $J$. Use periodic boundary conditions in both directions.\n\nTest suite (each tuple is $(L, J, T, n_{\\text{therm}}, n_{\\text{meas}})$):\n- Happy path, ordered phase: $(40, 1.0, 1.8, 800, 800)$.\n- Near the two-dimensional Ising critical point: $(40, 1.0, 2.269, 1500, 1500)$.\n- Disordered phase: $(40, 1.0, 3.5, 800, 800)$.\n- Boundary case (no interactions): $(40, 0.0, 1.0, 500, 1000)$.\n\nYour program must compute the three requested observables for each test case, average them over the $n_{\\text{meas}}$ measurement sweeps, and round each of the three outputs to $4$ decimal places for reporting. There are no physical units in the final output because all quantities are dimensionless in the chosen units with $k_B=1$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result corresponding to each test case must itself be a bracketed, comma-separated list of the three rounded floats $[\\langle |m| \\rangle, \\langle e \\rangle, U_4]$, in the same order as the test suite above. For example, the output should look like\n$[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$\nwith each $x_i, y_i, z_i$ rounded to $4$ decimal places and no other text printed.", "solution": "The supplied problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **System Model**: A two-dimensional square lattice of linear size $L$, with $N = L^2$ sites. Each site $i$ has a spin variable $s_i \\in \\{-1, +1\\}$.\n- **Hamiltonian**: The energy of a configuration $s$ is given by $\\mathcal{H}(s) = -J \\sum_{\\langle i,j \\rangle} s_i s_j$, where the sum is over nearest-neighbor pairs.\n- **Thermodynamics**: The system is in equilibrium with a heat bath at inverse temperature $\\beta = 1/T$, with the Boltzmann constant $k_B$ set to $1$. Probabilities follow the Boltzmann distribution, $P(s) \\propto e^{-\\beta \\mathcal{H}(s)}$.\n- **Algorithm**: Markov Chain Monte Carlo simulation using Gibbs sampling (heat-bath algorithm).\n- **Update Scheme**: A checkerboard (red-black) update scheme is mandated, where all sites of one sublattice are updated simultaneously.\n- **Boundary Conditions**: Periodic boundary conditions in both lattice directions.\n- **Simulation Protocol**: Start from a random spin configuration, perform $n_{\\text{therm}}$ thermalization sweeps, followed by $n_{\\text{meas}}$ measurement sweeps.\n- **Observables to Compute**:\n    - Instantaneous magnetization per site: $m = \\frac{1}{N} \\sum_i s_i$.\n    - Instantaneous absolute magnetization per site: $|m|$.\n    - Instantaneous energy per site: $e = \\frac{1}{N} \\mathcal{H}(s)$.\n    - Binder cumulant: $U_4 = 1 - \\frac{\\langle m^4 \\rangle}{3 \\langle m^2 \\rangle^2}$, where $\\langle \\cdot \\rangle$ denotes the average over measurement samples.\n- **Reproducibility**: The simulation must be reproducible for a fixed random number generator seed.\n- **Test Suite**: A list of test cases, each defined by the tuple $(L, J, T, n_{\\text{therm}}, n_{\\text{meas}})$:\n    1. $(40, 1.0, 1.8, 800, 800)$\n    2. $(40, 1.0, 2.269, 1500, 1500)$\n    3. $(40, 1.0, 3.5, 800, 800)$\n    4. $(40, 0.0, 1.0, 500, 1000)$\n- **Output Format**: For each test case, report a list containing the time-averaged absolute magnetization $\\langle |m| \\rangle$, time-averaged energy per site $\\langle e \\rangle$, and the Binder cumulant $U_4$, with each value rounded to $4$ decimal places. The final output must be a single line: `[[...],[...],[...],[...]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is fundamentally sound. The Ising model, Gibbs sampling, and the specified observables are central topics in statistical and computational physics. The setup uses established principles and models.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary information: the physical model, the simulation algorithm, specific parameters for each run, and a clear definition of the required outputs. With a fixed random seed, the algorithm is deterministic and will produce a unique solution.\n- **Objectivity**: The problem statement is objective and uses precise, unambiguous scientific language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard, well-defined problem in computational statistical mechanics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution\nThe objective is to simulate a two-dimensional Ising model using a specific Markov Chain Monte Carlo method, namely Gibbs sampling, and to compute several key statistical observables.\n\n**1. Gibbs Sampling and Conditional Probability**\n\nThe Gibbs sampling algorithm, also known as the heat-bath algorithm, updates the state of a single component of the system, $s_i$, by drawing a new value from its conditional probability distribution, given the current state of all other components. For the Ising model, the state of a spin $s_i$ only depends on its immediate neighbors due to the nearest-neighbor nature of the Hamiltonian $\\mathcal{H}$.\n\nThe conditional probability of finding spin $i$ in state $s_i$, given the fixed configuration of its neighbors, is proportional to the Boltzmann factor associated with the energy contribution from that spin:\n$$ P(s_i | \\{s_j\\}_{j \\in \\text{nn}(i)}) \\propto e^{-\\beta E_i} $$\nwhere $E_i$ is the part of the total energy that depends on $s_i$. This energy is $E_i = -J s_i \\sum_{j \\in \\text{nn}(i)} s_j$. Let us define the local field $h_i = \\sum_{j \\in \\text{nn}(i)} s_j$, which is the sum of the four neighboring spins. Then, $E_i = -J s_i h_i$.\n\nThe conditional probability for $s_i$ to be in the state $+1$ is given by normalizing over the two possible states, $s_i = +1$ and $s_i = -1$:\n$$ P(s_i = +1 | h_i) = \\frac{e^{-\\beta (-J(+1)h_i)}}{e^{-\\beta (-J(+1)h_i)} + e^{-\\beta (-J(-1)h_i)}} = \\frac{e^{\\beta J h_i}}{e^{\\beta J h_i} + e^{-\\beta J h_i}} $$\nThis expression can be rewritten in a numerically stable form using the hyperbolic tangent or, more conveniently for implementation, as:\n$$ P(s_i = +1 | h_i) = \\frac{1}{1 + e^{-2\\beta J h_i}} $$\nThe probability for $s_i$ to be $-1$ is simply $P(s_i = -1 | h_i) = 1 - P(s_i = +1 | h_i)$.\nThe Gibbs update rule for a single spin $s_i$ is as follows:\n1. Calculate the sum of its neighbors, $h_i$.\n2. Calculate the probability $p_+ = P(s_i = +1 | h_i)$.\n3. Draw a uniform random number $r \\in [0, 1)$.\n4. If $r < p_+$, set $s_i = +1$; otherwise, set $s_i = -1$.\n\n**2. Checkerboard Update Scheme**\n\nTo parallelize the updates and ensure correctness, a checkerboard (or red-black) update scheme is employed. The lattice sites are partitioned into two sublattices, 'red' and 'black', like squares on a chessboard. A site $(i,j)$ is 'red' if $i+j$ is even and 'black' if $i+j$ is odd. The crucial property is that all neighbors of a red site are black, and vice-versa.\n\nA full sweep over the lattice consists of two steps:\n1.  **Update Red Sublattice**: Simultaneously update all spins on the red sublattice. For each red spin, its neighbors are all on the black sublattice. The update for each red spin depends only on the current state of the black spins, so these updates are independent of each other.\n2.  **Update Black Sublattice**: Subsequently, simultaneously update all spins on the black sublattice. The neighbors of these spins are on the red sublattice, so their update will use the newly computed values of the red spins from step $1$.\n\nThis scheme guarantees that the detailed balance condition is met for the sweep as a whole and allows for efficient, vectorized implementation.\n\n**3. Observables and Their Computation**\n\nAfter a thermalization period of $n_{\\text{therm}}$ sweeps to allow the system to reach equilibrium, measurements are taken over $n_{\\text{meas}}$ sweeps.\n\n- **Magnetization per site ($m$)**: For a given spin configuration, this is the average spin value: $m = \\frac{1}{N} \\sum_{i=1}^N s_i$. We will need its absolute value $|m|$, as well as $m^2$ and $m^4$ for the Binder cumulant.\n\n- **Energy per site ($e$)**: The total energy is $\\mathcal{H} = -J \\sum_{\\langle i,j \\rangle} s_i s_j$. To avoid double-counting pairs, we sum over each site's interaction with its neighbor to the \"right\" and \"down\" (with periodic boundaries).\n$$ \\mathcal{H} = -J \\sum_{i,j} s_{i,j} (s_{i,j+1} + s_{i+1,j}) $$\nwhere indices are taken modulo $L$. The energy per site is $e = \\mathcal{H} / N$.\n\n- **Binder Cumulant ($U_4$)**: This is a higher-order moment ratio used to locate phase transitions. It is defined as:\n$$ U_4 = 1 - \\frac{\\langle m^4 \\rangle}{3 \\langle m^2 \\rangle^2} $$\nwhere $\\langle m^2 \\rangle$ and $\\langle m^4 \\rangle$ are the means of the squared and fourth power of the instantaneous magnetization, averaged over the measurement sweeps. This quantity has the useful property of being approximately independent of system size at the critical temperature. It approaches $2/3$ in the ordered phase and $0$ in the disordered phase. For the non-interacting case ($J=0$), for a finite system, $U_4 = 2/(3N)$, which is close to $0$.\n\n**4. Algorithmic Implementation**\n\nThe simulation will proceed as follows for each test case $(L, J, T, n_{\\text{therm}}, n_{\\text{meas}})$:\n1.  Initialize an $L \\times L$ lattice with random spins ($+1$ or $-1$ with equal probability).\n2.  Define the red and black sublattice masks.\n3.  Set $\\beta = 1/T$ (if $T > 0$).\n4.  **Thermalization**: Perform $n_{\\text{therm}}$ full checkerboard sweeps without taking measurements.\n5.  **Measurement**: Perform $n_{\\text{meas}}$ sweeps. After each sweep:\n    a. Calculate instantaneous magnetization $m$ and energy per site $e$.\n    b. Store $|m|$, $e$, $m^2$, and $m^4$.\n6.  **Averaging**: After the measurement loop, compute the averages of the stored quantities: $\\langle |m| \\rangle$, $\\langle e \\rangle$, $\\langle m^2 \\rangle$, and $\\langle m^4 \\rangle$.\n7.  Calculate the Binder cumulant $U_4$ from the averaged moments.\n8.  Return the final three observables, rounded as specified.\n\nThe use of `numpy` allows for efficient vectorization of the neighbor sum calculation (using `numpy.roll`) and the probabilistic updates across an entire sublattice at once.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(L, J, T, n_therm, n_meas, rng):\n    \"\"\"\n    Runs a Markov Chain Monte Carlo simulation for the 2D Ising model.\n\n    Args:\n        L (int): Linear size of the square lattice.\n        J (float): Coupling constant.\n        T (float): Temperature.\n        n_therm (int): Number of thermalization sweeps.\n        n_meas (int): Number of measurement sweeps.\n        rng (np.random.Generator): Random number generator instance.\n\n    Returns:\n        tuple: A tuple containing the averaged absolute magnetization,\n               averaged energy per site, and the Binder cumulant.\n    \"\"\"\n    N = L * L\n    \n    # Initialize spins randomly\n    spins = rng.choice([-1, 1], size=(L, L))\n    \n    # Create checkerboard masks\n    x, y = np.meshgrid(range(L), range(L))\n    red_mask = (x + y) % 2 == 0\n    black_mask = ~red_mask\n    \n    beta = 1.0 / T if T > 0 else float('inf')\n\n    # Simulation sweeps\n    for _ in range(n_therm):\n        # Gibbs sweep\n        for mask in [red_mask, black_mask]:\n            # Sum of neighbors using np.roll for periodic boundaries\n            neighbors_sum = (np.roll(spins, 1, axis=0) +\n                             np.roll(spins, -1, axis=0) +\n                             np.roll(spins, 1, axis=1) +\n                             np.roll(spins, -1, axis=1))\n            \n            # Argument for the exponential in the probability calculation\n            # For J=0, delta_E_arg is 0, prob is 0.5, which is correct.\n            delta_E_arg = 2.0 * J * neighbors_sum * beta\n            \n            # Probability to be in the +1 state, numerically stable form\n            prob_plus_one = 1.0 / (1.0 + np.exp(-delta_E_arg))\n            \n            # Generate random numbers and update spins\n            rand_flips = rng.random(size=(L,L))\n            new_spins = 2 * (rand_flips  prob_plus_one) - 1\n            \n            # Apply update only to the current sublattice\n            spins[mask] = new_spins[mask]\n\n    # Measurement phase\n    m_vals = []\n    e_vals = []\n    m2_vals = []\n    m4_vals = []\n\n    for _ in range(n_meas):\n        # Gibbs sweep\n        for mask in [red_mask, black_mask]:\n            neighbors_sum = (np.roll(spins, 1, axis=0) +\n                             np.roll(spins, -1, axis=0) +\n                             np.roll(spins, 1, axis=1) +\n                             np.roll(spins, -1, axis=1))\n            \n            delta_E_arg = 2.0 * J * neighbors_sum * beta\n            prob_plus_one = 1.0 / (1.0 + np.exp(-delta_E_arg))\n            \n            rand_flips = rng.random(size=(L,L))\n            new_spins = 2 * (rand_flips  prob_plus_one) - 1\n            spins[mask] = new_spins[mask]\n\n        # Calculate observables\n        m = np.mean(spins)\n        \n        # Energy calculation (each bond counted once)\n        energy_total = -J * np.sum(spins * (np.roll(spins, 1, axis=0) + np.roll(spins, 1, axis=1)))\n        e = energy_total / N\n        \n        # Store values for averaging\n        m_vals.append(np.abs(m))\n        e_vals.append(e)\n        m2_vals.append(m**2)\n        m4_vals.append(m**4)\n        \n    # Calculate final averages\n    avg_abs_m = np.mean(m_vals)\n    avg_e = np.mean(e_vals)\n    avg_m2 = np.mean(m2_vals)\n    avg_m4 = np.mean(m4_vals)\n    \n    # Calculate Binder cumulant, handle division by zero\n    if avg_m2 > 1e-12:\n        binder_cumulant = 1.0 - (avg_m4 / (3.0 * avg_m2**2))\n    else:\n        # Occurs if m is always zero (highly unlikely in finite simulation)\n        # or for J=0 case where avg_m2 is small. The theoretical value is 0.\n        binder_cumulant = 0.0\n\n    return avg_abs_m, avg_e, binder_cumulant\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, J, T, n_therm, n_meas)\n        (40, 1.0, 1.8, 800, 800),\n        (40, 1.0, 2.269, 1500, 1500),\n        (40, 1.0, 3.5, 800, 800),\n        (40, 0.0, 1.0, 500, 1000),\n    ]\n\n    # For reproducibility\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for case in test_cases:\n        L, J, T, n_therm, n_meas = case\n        avg_abs_m, avg_e, binder_cumulant = run_simulation(L, J, T, n_therm, n_meas, rng)\n        \n        # Round results to 4 decimal places\n        result_tuple = [\n            round(avg_abs_m, 4),\n            round(avg_e, 4),\n            round(binder_cumulant, 4)\n        ]\n        results.append(result_tuple)\n\n    # Manually construct the string to match the no-space format [[v1,v2,v3],[v4,v5,v6]]\n    inner_strings = []\n    for res_list in results:\n        inner_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2411722"}, {"introduction": "The principles of Markov chains and the Metropolis algorithm extend beyond simulating physical systems in equilibrium; they provide a powerful framework for optimization. This exercise introduces simulated annealing, a technique that mimics the slow cooling of a physical system to guide a search for the lowest-energy state, or optimal solution, of a complex problem [@problem_id:2411642]. By framing a creative task—synthesizing a mosaic—in terms of an 'energy' function, you will see how concepts from statistical physics can be harnessed to solve practical challenges in fields like image processing and combinatorial optimization.", "problem": "You will implement a simulated annealing solver for a mosaic synthesis task that is modeled as a discrete Markov chain on a finite state space. The goal is to place colored tiles on a grid so that the resulting image approximates a given target while respecting local color smoothness constraints. The design must be grounded in the foundations of statistical physics. You must formalize the state space, define an energy function inspired by interactions in lattice models, justify a transition mechanism that forms a Markov chain, and implement an annealing schedule that drives the system toward low-energy configurations.\n\nFundamental base to use and build upon:\n- The Boltzmann distribution in equilibrium statistical physics: a probability measure over configurations $X$ proportional to $\\exp(-E(X)/T)$ at temperature $T$, where $E(X)$ is an energy function.\n- The Metropolis method (a special case of the Metropolis–Hastings algorithm): a Markov chain Monte Carlo construction that enforces detailed balance with respect to a target distribution by accepting or rejecting proposed moves based on an energy difference and a temperature.\n- Basic Markov chain definitions: a stochastic process with the memoryless property, transition probabilities that depend only on the current state, and detailed balance as a sufficient condition for stationarity with a specified invariant distribution.\n\nProblem setup:\n- Grid and palette. Let the grid be of size $N \\times M$ with integer indices $(i,j)$ for $i \\in \\{0,\\dots,N-1\\}$ and $j \\in \\{0,\\dots,M-1\\}$. Let the finite color palette be $P=\\{p_{0},p_{1},\\dots,p_{K-1}\\}$ where each $p_{k}$ is a grayscale intensity in the set $\\{0,\\dots,255\\}$. A mosaic configuration $X$ is an $N \\times M$ array of palette indices $x_{ij} \\in \\{0,\\dots,K-1\\}$; the rendered intensity at site $(i,j)$ is $R_{ij}=p_{x_{ij}}$.\n- Target image. The target is an $N \\times M$ grid $I$ of grayscale intensities $I_{ij} \\in \\{0,\\dots,255\\}$.\n- Energy function. Define the total energy\n$$\nE(X) \\;=\\; \\sum_{i=0}^{N-1} \\sum_{j=0}^{M-1} \\big(R_{ij}-I_{ij}\\big)^{2}\n\\;+\\;\n\\lambda \\sum_{i=0}^{N-1} \\sum_{j=0}^{M-1} \\Big( \\mathbf{1}_{i+1N} \\cdot \\big(R_{ij}-R_{i+1,j}\\big)^{2} + \\mathbf{1}_{j+1M} \\cdot \\big(R_{ij}-R_{i,j+1}\\big)^{2} \\Big),\n$$\nwhere $\\lambda \\ge 0$ is a smoothness weight and $\\mathbf{1}_{\\cdot}$ is an indicator that is $1$ when the neighbor exists and $0$ otherwise. The first term enforces fidelity to the target image, the second term is a local color constraint that penalizes large differences between adjacent tiles to encourage smoothness.\n\nAlgorithmic requirements to be derived and implemented:\n- State space and proposals. The state space is the set of all $N \\times M$ assignments of palette indices. A single update proposal is to select one site $(i,j)$ uniformly at random and propose changing $x_{ij}$ to a different palette index $x'_{ij} \\in \\{0,\\dots,K-1\\} \\setminus \\{x_{ij}\\}$, drawn uniformly at random. This defines a proposal kernel that is symmetric over the discrete choices.\n- Acceptance rule. Design an acceptance rule based on the Metropolis method that satisfies detailed balance with respect to the Boltzmann distribution $\\pi_{T}(X) \\propto \\exp(-E(X)/T)$ at a given temperature $T$. Express the acceptance probability in terms of the energy difference $\\Delta E$ between the proposed and current configuration and the temperature $T$.\n- Temperature schedule. Use a geometric cooling schedule $T_{t}=T_{0}\\,\\alpha^{t}$ with $T_{0}0$ and $\\alpha \\in (0,1)$ over a total of $S$ proposals, where $t \\in \\{0,1,\\dots,S-1\\}$ counts proposals. Justify why such a schedule is a computational physics heuristic for simulated annealing. There is no requirement to prove convergence.\n- Local energy update. To ensure computational efficiency, compute the energy difference $\\Delta E$ for a single-site change using only the affected terms (the data term at $(i,j)$ and its interactions with up to four neighbors), not by recomputing the full $E(X)$.\n\nImplementation details:\n- Initialization. Initialize $X$ by mapping each target intensity $I_{ij}$ to the nearest palette value in $P$ by absolute difference, that is, choose $x_{ij} \\in \\operatorname*{arg\\,min}_{k \\in \\{0,\\dots,K-1\\}} |p_{k}-I_{ij}|$.\n- Randomness. Use a fixed pseudorandom seed per test case to ensure deterministic outputs.\n- Output. For each test case, run the annealing for precisely $S$ proposals. Report the final total energy $E(X)$ as a floating-point number rounded to three digits after the decimal point.\n\nTest suite:\nProvide a program that runs the following four cases and prints the four results in order as a single line with comma-separated values enclosed in brackets.\n\n- Case $1$ (general case): $N=M=4$, $K=3$, $P=[0,128,255]$, $\\lambda=0.1$, $T_{0}=50.0$, $\\alpha=0.995$, $S=6000$, seed $=12345$. Target $I$:\n  $\n  \\begin{bmatrix}\n  0  64  128  192\\\\\n  16  80  144  208\\\\\n  32  96  160  224\\\\\n  48  112  176  240\n  \\end{bmatrix}\n  $.\n- Case $2$ (small grid, strong smoothness): $N=M=2$, $K=3$, $P=[0,128,255]$, $\\lambda=2.0$, $T_{0}=10.0$, $\\alpha=0.99$, $S=2000$, seed $=7$. Target $I$:\n  $\n  \\begin{bmatrix}\n  200  50\\\\\n  60  210\n  \\end{bmatrix}\n  $.\n- Case $3$ (no smoothness): $N=M=3$, $K=4$, $P=[0,85,170,255]$, $\\lambda=0.0$, $T_{0}=1.0$, $\\alpha=0.9$, $S=1000$, seed $=99$. Target $I$:\n  $\n  \\begin{bmatrix}\n  0  0  255\\\\\n  0  128  255\\\\\n  0  255  255\n  \\end{bmatrix}\n  $.\n- Case $4$ (checkerboard conflict, binary palette): $N=M=4$, $K=2$, $P=[0,255]$, $\\lambda=1.0$, $T_{0}=5.0$, $\\alpha=0.997$, $S=8000$, seed $=202$. Target $I$:\n  $\n  \\begin{bmatrix}\n  0  255  0  255\\\\\n  255  0  255  0\\\\\n  0  255  0  255\\\\\n  255  0  255  0\n  \\end{bmatrix}\n  $.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases, with each value rounded to three digits after the decimal point. For example, a valid output line has the form $[\\text{r}_{1},\\text{r}_{2},\\text{r}_{3},\\text{r}_{4}]$ where each $\\text{r}_{i}$ is a floating-point number with exactly three digits after the decimal point and no additional text is printed.", "solution": "The problem presented is a valid and well-posed formulation of a combinatorial optimization task, solvable by the method of simulated annealing. It is grounded in the principles of statistical physics, specifically the correspondence between optimization and finding the ground state energy of a physical system. We shall construct the solution by formalizing the components of the algorithm as required.\n\nThe objective is to synthesize a mosaic configuration $X$ that approximates a target image $I$. This is framed as an energy minimization problem. The configuration space is vast, necessitating a stochastic search algorithm, for which simulated annealing is a physically-motivated and effective choice.\n\nFirst, we formalize the system. The state space, $\\mathcal{S}$, is the set of all possible mosaic configurations. A configuration, or state, $X$ is an $N \\times M$ grid of palette indices, where each site $(i,j)$ is assigned an index $x_{ij} \\in \\{0, 1, \\dots, K-1\\}$. The total number of states is finite, equal to $K^{NM}$. The rendered intensity at site $(i,j)$ of a configuration $X$ is $R_{ij} = p_{x_{ij}}$, where $P=\\{p_0, p_1, \\dots, p_{K-1}\\}$ is the palette of available grayscale values.\n\nThe quality of a configuration $X$ is quantified by the energy function $E(X)$, which we seek to minimize. The given function is:\n$$\nE(X) = \\underbrace{\\sum_{i=0}^{N-1} \\sum_{j=0}^{M-1} (R_{ij} - I_{ij})^2}_{\\text{Data Fidelity Term}} + \\underbrace{\\lambda \\sum_{i=0}^{N-1} \\sum_{j=0}^{M-1} \\left( \\mathbf{1}_{i+1N} (R_{ij} - R_{i+1,j})^2 + \\mathbf{1}_{j+1M} (R_{ij} - R_{i,j+1})^2 \\right)}_{\\text{Smoothness Regularization Term}}\n$$\nHere, $I_{ij}$ are the intensities of the target image, $\\lambda \\ge 0$ is a weighting coefficient, and $\\mathbf{1}_{\\cdot}$ is the indicator function. The first term, the data fidelity or external field term, penalizes deviations from the target image. The second term, the smoothness or interaction term, penalizes large intensity differences between adjacent tiles, promoting local homogeneity. This energy function is analogous to Hamiltonians used in lattice models like the Ising or Potts models, where states have site-specific energies (from an external field) and interaction energies with neighbors.\n\nTo find a low-energy state, we employ a Markov chain Monte Carlo (MCMC) method. We construct a Markov chain whose states are the configurations $X \\in \\mathcal{S}$ and whose stationary distribution is the Boltzmann distribution, $\\pi_T(X)$:\n$$\n\\pi_T(X) = \\frac{1}{Z(T)} \\exp\\left(-\\frac{E(X)}{T}\\right)\n$$\nwhere $T$ is a parameter analogous to temperature and $Z(T) = \\sum_{X' \\in \\mathcal{S}} \\exp(-E(X')/T)$ is the partition function. At low temperatures ($T \\to 0$), this distribution concentrates probability on the states with the lowest energy $E(X)$.\n\nThe Markov chain evolves via a sequence of proposed moves, which are accepted or rejected. The problem specifies the proposal mechanism:\n$1$. Select a site $(i,j)$ uniformly at random from the $N \\times M$ grid.\n$2$. Propose a new configuration $X'$ by changing the palette index at this site from its current value $x_{ij}$ to a new index $x'_{ij}$, chosen uniformly at random from the set $\\{0, \\dots, K-1\\} \\setminus \\{x_{ij}\\}$.\n\nLet the proposal probability of moving from state $X$ to $X'$ be $g(X'|X)$. A specific site is chosen with probability $1/(NM)$, and a specific new index is chosen with probability $1/(K-1)$. Thus, $g(X'|X) = 1/(NM(K-1))$ if $X$ and $X'$ differ at exactly one site. The reverse proposal, from $X'$ to $X$, involves choosing the same site $(i,j)$ and the specific original index $x_{ij}$ from the $K-1$ alternatives. Therefore, $g(X|X') = 1/(NM(K-1))$, and the proposal kernel is symmetric: $g(X'|X) = g(X|X')$.\n\nTo ensure the chain converges to $\\pi_T(X)$, we enforce the detailed balance condition: $\\pi_T(X) P(X \\to X') = \\pi_T(X') P(X' \\to X)$, where $P(X \\to X') = g(X'|X) A(X \\to X')$ is the total transition probability and $A(X \\to X')$ is the acceptance probability. Due to the symmetry of $g$, detailed balance simplifies to $\\pi_T(X) A(X \\to X') = \\pi_T(X') A(X' \\to X)$. A standard choice for the acceptance probability that satisfies this is the Metropolis rule:\n$$\nA(X \\to X') = \\min\\left(1, \\frac{\\pi_T(X')}{\\pi_T(X)}\\right) = \\min\\left(1, \\exp\\left(-\\frac{E(X') - E(X)}{T}\\right)\\right)\n$$\nLetting $\\Delta E = E(X') - E(X)$ be the change in energy, the acceptance probability is $A = \\min(1, \\exp(-\\Delta E/T))$. A proposed move is always accepted if it lowers the energy ($\\Delta E  0$). If it increases the energy ($\\Delta E  0$), it is accepted with a probability that decreases as $\\Delta E$ increases or as $T$ decreases. This allows the system to escape local energy minima.\n\nSimulated annealing is a heuristic that leverages this MCMC procedure. It begins at a high initial temperature $T_0$, where many uphill moves are accepted, allowing for a broad exploration of the state space. The temperature is then gradually lowered according to a cooling schedule. The specified schedule is geometric cooling:\n$$\nT_t = T_0 \\alpha^t\n$$\nfor $t \\in \\{0, 1, \\dots, S-1\\}$, where $t$ is the step (proposal) number, $T_0$ is the initial temperature, and $\\alpha \\in (0,1)$ is the cooling rate. As $T$ decreases, the acceptance criterion becomes stricter, and the system preferentially settles into a low-energy state. While this heuristic does not guarantee finding the global minimum in a finite number of steps, it is a widely used and effective practical method.\n\nA crucial aspect of implementation is computational efficiency. Recomputing the entire energy $E(X')$ for each proposal is prohibitive. Since a proposal changes only one site $(i,j)$, the change in energy $\\Delta E$ can be calculated locally. Let the proposed change be from old index $x_{ij}$ to new index $x'_{ij}$, with corresponding rendered colors $R_o = p_{x_{ij}}$ and $R_n = p_{x'_{ij}}$. The change in the data fidelity term occurs only at site $(i,j)$:\n$$\n\\Delta E_{\\text{data}} = (R_n - I_{ij})^2 - (R_o - I_{ij})^2\n$$\nThe change in the smoothness term involves only the interactions between site $(i,j)$ and its immediate neighbors. Let $\\mathcal{N}(i,j)$ be the set of existing neighbor coordinates of $(i,j)$. For each neighbor $(u,v) \\in \\mathcal{N}(i,j)$, the interaction term $(R_{ij} - R_{uv})^2$ changes. The total change in smoothness energy is:\n$$\n\\Delta E_{\\text{smooth}} = \\lambda \\sum_{(u,v) \\in \\mathcal{N}(i,j)} \\left[ (R_n - R_{uv})^2 - (R_o - R_{uv})^2 \\right]\n$$\nThe total energy change is $\\Delta E = \\Delta E_{\\text{data}} + \\Delta E_{\\text{smooth}}$. This local update is significantly faster than a full re-computation.\n\nThe algorithm proceeds as follows:\n$1$. Initialize the configuration $X$. For each site $(i,j)$, set the palette index $x_{ij}$ to $\\operatorname*{arg\\,min}_{k} |p_k - I_{ij}|$.\n$2$. Set the pseudorandom generator seed for reproducibility.\n$3$. For $t$ from $0$ to $S-1$:\n    a. Calculate the current temperature $T_t = T_0 \\alpha^t$.\n    b. Propose a new state $X'$ by randomly selecting a site and a new index.\n    c. Compute $\\Delta E$ locally.\n    d. Generate a uniform random number $u \\in [0,1)$.\n    e. If $u  \\min(1, \\exp(-\\Delta E/T_t))$, accept the move: update the configuration $X$ to $X'$.\n$4$. After $S$ steps, compute the total energy $E(X)$ of the final configuration using the full energy formula and report the result.", "answer": "```python\nimport numpy as np\n\ndef calculate_total_energy(palette_indices, palette, target_image, lam):\n    \"\"\"Calculates the total energy of a given configuration from scratch.\"\"\"\n    rendered_image = palette[palette_indices]\n    N, M = rendered_image.shape\n\n    # Data fidelity term\n    data_energy = np.sum((rendered_image - target_image)**2)\n\n    # Smoothness term\n    smoothness_energy = 0\n    # Horizontal differences\n    if M > 1:\n        smoothness_energy += np.sum((rendered_image[:, :-1] - rendered_image[:, 1:])**2)\n    # Vertical differences\n    if N > 1:\n        smoothness_energy += np.sum((rendered_image[:-1, :] - rendered_image[1:, :])**2)\n        \n    return data_energy + lam * smoothness_energy\n\ndef run_annealing(N, M, K, P, lam, T0, alpha, S, seed, I):\n    \"\"\"Runs the simulated annealing algorithm for a single test case.\"\"\"\n    \n    # Set seed for reproducibility\n    np.random.seed(seed)\n    \n    # Convert inputs to numpy arrays\n    palette = np.array(P, dtype=np.float64)\n    target_image = np.array(I, dtype=np.float64)\n\n    # Initialization\n    # Find the closest palette color for each pixel in the target image\n    palette_indices = np.zeros((N, M), dtype=int)\n    for i in range(N):\n        for j in range(M):\n            palette_indices[i, j] = np.argmin(np.abs(palette - target_image[i, j]))\n\n    # Annealing loop\n    all_indices = list(range(K))\n    for t in range(S):\n        T = T0 * (alpha ** t)\n\n        # Propose a move: pick a random site and a random new color\n        i, j = np.random.randint(N), np.random.randint(M)\n        \n        current_idx = palette_indices[i, j]\n        options = all_indices[:current_idx] + all_indices[current_idx+1:]\n        new_idx = np.random.choice(options)\n        \n        current_color = palette[current_idx]\n        new_color = palette[new_idx]\n\n        # Efficiently calculate energy difference (Delta E)\n        # Data fidelity term change\n        delta_e = (new_color - target_image[i, j])**2 - (current_color - target_image[i, j])**2\n        \n        # Smoothness term change\n        # Neighbors: (i-1, j), (i+1, j), (i, j-1), (i, j+1)\n        # Since (a-b)^2 = (b-a)^2, the calculation is symmetric\n        for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            ni, nj = i + di, j + dj\n            if 0 = ni  N and 0 = nj  M:\n                neighbor_idx = palette_indices[ni, nj]\n                neighbor_color = palette[neighbor_idx]\n                delta_e += lam * ((new_color - neighbor_color)**2 - (current_color - neighbor_color)**2)\n\n        # Metropolis acceptance criterion\n        if delta_e  0 or (T > 1e-9 and np.random.rand()  np.exp(-delta_e / T)):\n            palette_indices[i, j] = new_idx\n            \n    # Calculate final energy of the final configuration\n    final_energy = calculate_total_energy(palette_indices, palette, target_image, lam)\n    \n    return final_energy\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the simulated annealing solver for each,\n    printing the results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 4, \"M\": 4, \"K\": 3, \"P\": [0, 128, 255], \"lam\": 0.1, \n            \"T0\": 50.0, \"alpha\": 0.995, \"S\": 6000, \"seed\": 12345,\n            \"I\": [\n                [0, 64, 128, 192],\n                [16, 80, 144, 208],\n                [32, 96, 160, 224],\n                [48, 112, 176, 240]\n            ]\n        },\n        {\n            \"N\": 2, \"M\": 2, \"K\": 3, \"P\": [0, 128, 255], \"lam\": 2.0, \n            \"T0\": 10.0, \"alpha\": 0.99, \"S\": 2000, \"seed\": 7,\n            \"I\": [\n                [200, 50],\n                [60, 210]\n            ]\n        },\n        {\n            \"N\": 3, \"M\": 3, \"K\": 4, \"P\": [0, 85, 170, 255], \"lam\": 0.0, \n            \"T0\": 1.0, \"alpha\": 0.9, \"S\": 1000, \"seed\": 99,\n            \"I\": [\n                [0, 0, 255],\n                [0, 128, 255],\n                [0, 255, 255]\n            ]\n        },\n        {\n            \"N\": 4, \"M\": 4, \"K\": 2, \"P\": [0, 255], \"lam\": 1.0,\n            \"T0\": 5.0, \"alpha\": 0.997, \"S\": 8000, \"seed\": 202,\n            \"I\": [\n                [0, 255, 0, 255],\n                [255, 0, 255, 0],\n                [0, 255, 0, 255],\n                [255, 0, 255, 0]\n            ]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Added a check to avoid division by zero when T becomes very small in the exp\n        if \"alpha\" in case and case[\"alpha\"]  1.0:\n            min_T = case[\"T0\"] * (case[\"alpha\"] ** (case[\"S\"]-1))\n            if min_T  1e-9:\n                # Add a small check in run_annealing to handle this\n                pass\n\n        result = run_annealing(\n            case[\"N\"], case[\"M\"], case[\"K\"], case[\"P\"], case[\"lam\"],\n            case[\"T0\"], case[\"alpha\"], case[\"S\"], case[\"seed\"], case[\"I\"]\n        )\n        results.append(result)\n\n    # Format the output as a comma-separated list of floats with 3 decimal places, no spaces.\n    output_str = \",\".join([f\"{r:.3f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "2411642"}]}