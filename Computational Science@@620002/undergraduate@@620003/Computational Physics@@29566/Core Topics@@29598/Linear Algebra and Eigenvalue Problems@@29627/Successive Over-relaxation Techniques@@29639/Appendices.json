{"hands_on_practices": [{"introduction": "To begin, we will apply the Successive Over-Relaxation (SOR) method to a fundamental problem in physics: the steady-state heat distribution along a rod. This exercise focuses on a single iterative step, allowing you to master the core SOR update formula in a concrete and physically intuitive setting [@problem_id:1369790]. Understanding this basic mechanical step is the foundation for tackling more complex systems.", "problem": "A simplified steady-state heat distribution model for a thin, insulated rod is being analyzed. The rod is discretized into five equally spaced points, $P_0, P_1, P_2, P_3, P_4$. The temperatures at the two endpoints are held constant at $T_0 = 25.0^\\circ\\text{C}$ and $T_4 = 100.0^\\circ\\text{C}$. The temperatures at the three interior points, denoted by $T_1, T_2,$ and $T_3$, are unknown.\n\nIn the steady state, the temperature at each interior point is the arithmetic mean of the temperatures of its two immediate neighbors. This physical principle leads to a system of linear equations for the unknown temperatures $T_1, T_2,$ and $T_3$.\n\nTo solve this system, the Successive Over-Relaxation (SOR) iterative method is employed. The relaxation parameter is chosen as $\\omega = 1.15$. The iteration starts with an initial guess of zero for all unknown temperatures, i.e., $T_1^{(0)} = T_2^{(0)} = T_3^{(0)} = 0$.\n\nCalculate the value of the temperature at the first interior point, $T_1$, after the first full iteration, denoted as $T_1^{(1)}$. Express your answer in degrees Celsius, rounded to three significant figures.", "solution": "The steady-state condition on the 1D grid imposes, for each interior node, the mean relation\n$$\nT_{i}=\\frac{T_{i-1}+T_{i+1}}{2}.\n$$\nFor the three unknowns, this yields the linear system\n$$\n\\begin{aligned}\n2T_{1}-T_{2}=T_{0},\\\\\n-T_{1}+2T_{2}-T_{3}=0,\\\\\n-T_{2}+2T_{3}=T_{4}.\n\\end{aligned}\n$$\nUsing the SOR iteration for a system $A\\mathbf{T}=\\mathbf{b}$,\n$$\nT_{i}^{(k+1)}=(1-\\omega)T_{i}^{(k)}+\\frac{\\omega}{a_{ii}}\\left(b_{i}-\\sum_{ji}a_{ij}T_{j}^{(k+1)}-\\sum_{ji}a_{ij}T_{j}^{(k)}\\right),\n$$\nthe update for $T_{1}$ (with $a_{11}=2$, $a_{12}=-1$, $b_{1}=T_{0}$) is\n$$\nT_{1}^{(1)}=(1-\\omega)T_{1}^{(0)}+\\frac{\\omega}{2}\\left(T_{0}-(-1)T_{2}^{(0)}\\right).\n$$\nWith the initial guess $T_{1}^{(0)}=T_{2}^{(0)}=0$,\n$$\nT_{1}^{(1)}=\\frac{\\omega}{2}T_{0}.\n$$\nSubstituting $\\omega=1.15$ and $T_{0}=25.0$ gives\n$$\nT_{1}^{(1)}=\\frac{1.15}{2}\\times 25.0=14.375.\n$$\nRounded to three significant figures, this is $14.4$ in degrees Celsius.", "answer": "$$\\boxed{14.4}$$", "id": "1369790"}, {"introduction": "Next, we move from a single calculation to a full numerical simulation by solving the two-dimensional Laplace equation, a classic problem in electrostatics and heat transfer. This practice requires you to implement and compare the performance of the Jacobi, Gauss-Seidel, and SOR methods, providing a dramatic, quantitative illustration of the accelerated convergence that gives SOR its name [@problem_id:2406769]. Through this coding exercise, you will gain firsthand experience in the practical application and efficiency of these iterative solvers.", "problem": "Consider the two-dimensional Laplace equation $\\nabla^2 u = 0$ on the square domain $\\Omega = (0,1)\\times(0,1)$ with Dirichlet boundary conditions. Let the boundary data be $u(x,0) = \\sin(\\pi x)$, $u(x,1) = 0$, $u(0,y)=0$, and $u(1,y)=0$, where angles are in radians. Discretize the domain using a uniform grid with $N$ interior points in each spatial direction (so that the grid spacing is $h = \\frac{1}{N+1}$ and there are $(N+2)\\times(N+2)$ total grid points including the boundary). Use the standard second-order centered finite-difference approximation for the Laplacian to obtain a linear system for the interior unknowns. Starting from the discrete Laplace operator definition derived from the continuous equation and central differences, implement three stationary iterative methods to solve the discrete equations: the Jacobi method, the Gauss–Seidel method, and the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega$ satisfying $0  \\omega  2$. For each method, use the following fundamental base and definitions:\n\n- The discrete Laplace equation at an interior grid point $(i,j)$ is obtained from the central-difference approximation to $\\nabla^2 u = 0$:\n$$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0,$$\nwhich is algebraically equivalent to the stencil equation\n$$-u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} + 4 u_{i,j} = 0.$$\n- From this, define the discrete residual on the interior as\n$$r_{i,j} = 4u_{i,j} - \\left(u_{i+1,j}+u_{i-1,j}+u_{i,j+1}+u_{i,j-1}\\right),$$\nand its infinity norm as $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$.\n- Use the stopping rule $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol}\\cdot \\|r^{(0)}\\|_{\\infty}$, where $\\text{tol}$ is a given tolerance and $k$ is the iteration index, with the initial guess $u^{(0)}$ equal to zero on all interior points and fixed boundary values on the boundary points.\n\nYour program must:\n- Implement the Jacobi iteration that updates all interior values using only the previous iteration’s values.\n- Implement the Gauss–Seidel iteration in a way that uses the most recently available neighbor values (you may use a red–black ordering to achieve this).\n- Implement the SOR iteration using red–black ordering with relaxation parameter $\\omega$ via\n$$u^{(k+1)}_{i,j} = u^{(k)}_{i,j} + \\omega\\left(\\frac{1}{4}\\left(u^{(*)}_{i+1,j}+u^{(*)}_{i-1,j}+u^{(*)}_{i,j+1}+u^{(*)}_{i,j-1}\\right) - u^{(k)}_{i,j}\\right),$$\nwhere $u^{(*)}$ denotes the most up-to-date values consistent with Gauss–Seidel ordering on each color. Take $\\omega = 1$ to recover Gauss–Seidel.\n\nYour task is to compare the convergence rates of the three methods quantitatively by reporting, for each test case, the number of iterations required by each method to satisfy the stopping rule. Use the same discretization, boundary conditions, and stopping criterion for all methods, and report iteration counts as integers.\n\nTest suite. Run your program on the following parameter sets, where each test case is a triple $(N, \\text{tol}, \\omega)$:\n- Test $1$: $(20, 10^{-5}, 1.5)$.\n- Test $2$: $(20, 10^{-5}, 1.0)$.\n- Test $3$: $(10, 10^{-8}, 1.8)$.\n- Test $4$: $(40, 10^{-4}, 1.9)$.\n\nFor each test case, your program must produce a list $[n_J, n_{GS}, n_{SOR}]$ containing the iteration counts required by the Jacobi, Gauss–Seidel, and SOR methods, respectively, to meet the stopping rule. Aggregate the results of all tests into a single line as a comma-separated list of these lists, with no spaces, enclosed in square brackets. For example, your output must look exactly like\n$[[n_J^{(1)},n_{GS}^{(1)},n_{SOR}^{(1)}],[n_J^{(2)},n_{GS}^{(2)},n_{SOR}^{(2)}],[n_J^{(3)},n_{GS}^{(3)},n_{SOR}^{(3)}],[n_J^{(4)},n_{GS}^{(4)},n_{SOR}^{(4)}]]$,\nprinted as a single line. No physical units are involved. Angles in the sine function must be interpreted in radians.", "solution": "The problem as stated is a standard, well-posed exercise in the numerical solution of elliptic partial differential equations. It is scientifically sound, self-contained, and algorithmically specified. There are no contradictions, ambiguities, or factual errors. Therefore, we proceed directly to the solution.\n\nThe problem requires solving the two-dimensional Laplace equation $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$ on a unit square domain $\\Omega = (0,1)\\times(0,1)$. The potential $u(x,y)$ is subject to Dirichlet boundary conditions: $u(x,0) = \\sin(\\pi x)$, and $u=0$ on the other three boundaries.\n\nThe first step is to discretize the continuous problem. The domain is covered by a uniform grid with $(N+2) \\times (N+2)$ points, where $N$ is the number of interior points in each direction. The grid spacing is $h = \\frac{1}{N+1}$. A grid point is denoted by $(x_i, y_j) = (ih, jh)$ for indices $i,j \\in \\{0, 1, \\dots, N+1\\}$. The value of the potential at this point is $u_{i,j} \\approx u(x_i, y_j)$.\n\nThe Laplacian operator $\\nabla^2$ is approximated at each interior grid point $(i,j)$ using the second-order central difference formula:\n$$ \\nabla^2 u \\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} $$\nSetting this approximation to $0$ gives the discrete Laplace equation for the interior points ($1 \\le i,j \\le N$):\n$$ u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0 $$\nThis five-point stencil equation can be rearranged to express $u_{i,j}$ as the average of its four neighbors:\n$$ u_{i,j} = \\frac{1}{4} \\left( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\right) $$\nThis set of $N^2$ linear equations for the $N^2$ interior unknown values forms a large, sparse linear system of the form $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the unknowns $u_{i,j}$ and the matrix $A$ represents the discrete Laplacian operator. The right-hand side vector $\\mathbf{b}$ incorporates the fixed boundary values. Such systems are well-suited for solution by iterative methods.\n\nWe are tasked to implement three classical stationary iterative methods: Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR). These methods start with an initial guess $u^{(0)}$ and generate a sequence of approximations $u^{(k)}$ that converges to the true solution.\n\nThe Jacobi method is the simplest iterative scheme. For each point $(i,j)$, the new value $u_{i,j}^{(k+1)}$ is computed using only the values from the previous iteration, $u^{(k)}$. The update rule is a direct application of the averaging formula:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} \\right) $$\nThis update can be performed for all interior points simultaneously (or in any order), as the calculation for each point is independent of the others within the same iteration. In a vectorized implementation, a full copy of the grid from iteration $k$ is required to compute the grid for iteration $k+1$.\n\nThe Gauss-Seidel method improves upon Jacobi by using the most recently computed values within the current iteration. The update rule is:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(*)} + u_{i-1,j}^{(*)} + u_{i,j+1}^{(*)} + u_{i,j-1}^{(*)} \\right) $$\nwhere $u^{(*)}$ denotes the most up-to-date value available. For example, in a lexicographical ordering (row by row, column by column), the calculation for $u_{i,j}^{(k+1)}$ would use $u_{i-1,j}^{(k+1)}$ and $u_{i,j-1}^{(k+1)}$ from the current iteration $k+1$, and $u_{i+1,j}^{(k)}$ and $u_{i,j+1}^{(k)}$ from the previous iteration $k$. This dependency on the update order makes parallelization complex. The red-black ordering scheme circumvents this. The grid points are colored like a checkerboard. All \"red\" points are updated first, using values from their \"black\" neighbors (from the previous iteration). Then, all \"black\" points are updated, using the newly computed values from their \"red\" neighbors. Each of the two stages (red update, black update) can be fully vectorized.\n\nThe Successive Over-Relaxation (SOR) method is an extrapolation of the Gauss-Seidel method, designed to accelerate convergence. It computes the Gauss-Seidel update, and then pushes the solution further in that direction, controlled by a relaxation parameter $\\omega$. The update formula is:\n$$ u_{i,j}^{(k+1)} = u_{i,j}^{(k)} + \\omega \\left( u_{i,j}^{\\text{GS}} - u_{i,j}^{(k)} \\right) = (1-\\omega)u_{i,j}^{(k)} + \\omega u_{i,j}^{\\text{GS}} $$\nwhere $u_{i,j}^{\\text{GS}}$ is the value that would be computed by the Gauss-Seidel step at that point. Like Gauss-Seidel, SOR is implemented using the red-black ordering to efficiently use the most recent values. When $\\omega=1$, the SOR method reduces exactly to the Gauss-Seidel method. For Laplace-type problems, choosing an optimal $\\omega$ in the range $1  \\omega  2$ (over-relaxation) typically leads to a significant speedup in convergence.\n\nThe stopping criterion is based on the infinity norm of the discrete residual, defined as $\\|r^{(k)}\\|_{\\infty} = \\max_{i,j} |r_{i,j}^{(k)}|$, where $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})$. The iteration stops when $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol} \\cdot \\|r^{(0)}\\|_{\\infty}$, where $\\text{tol}$ is a given tolerance and $\\|r^{(0)}\\|_{\\infty}$ is the residual norm of the initial guess ($u^{(0)}=0$ on the interior). This relative criterion ensures a fair comparison between different problem setups.\n\nThe implementation consists of three main functions. One function sets up the $(N+2) \\times (N+2)$ grid, initializing the interior to $0$ and setting the boundary conditions. A second function implements the Jacobi iteration. A third function implements the SOR iteration with red-black ordering, which is also used for the Gauss-Seidel method by setting $\\omega=1$. A helper function calculates the residual norm at each step. The main program iterates through the test cases, calls the appropriate solver functions, and records the number of iterations required for convergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_initial_state(N):\n    \"\"\"\n    Initializes the grid with boundary conditions and zero interior.\n\n    Args:\n        N (int): Number of interior points in each direction.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The (N+2)x(N+2) grid `u`.\n            - float: The grid spacing `h`.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    u = np.zeros((N + 2, N + 2))\n    \n    # Set boundary condition u(x,0) = sin(pi*x)\n    # The j=0 row corresponds to y=0.\n    x_coords = np.linspace(0, 1, N + 2)\n    u[0, :] = np.sin(np.pi * x_coords)\n    \n    # Other boundaries u(x,1)=0, u(0,y)=0, u(1,y)=0 are already zero.\n    return u, h\n\ndef calculate_residual_norm(u, N):\n    \"\"\"\n    Calculates the infinity norm of the residual on the interior grid.\n\n    Args:\n        u (np.ndarray): The full (N+2)x(N+2) grid.\n        N (int): Number of interior grid points.\n\n    Returns:\n        float: The infinity norm of the residual.\n    \"\"\"\n    interior = u[1:N + 1, 1:N + 1]\n    neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                     u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n    residual = 4 * interior - neighbors_sum\n    return np.max(np.abs(residual))\n\ndef solve_jacobi(N, tol):\n    \"\"\"\n    Solves the Laplace equation using the Jacobi method.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n    \n    threshold = tol * r0_norm\n    \n    k = 0\n    while True:\n        k += 1\n        \n        u_old = u.copy()\n        \n        neighbors_sum = (u_old[1:N + 1, 2:N + 2] + u_old[1:N + 1, 0:N] +\n                         u_old[2:N + 2, 1:N + 1] + u_old[0:N, 1:N + 1])\n        u[1:N + 1, 1:N + 1] = 0.25 * neighbors_sum\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve_sor(N, tol, omega):\n    \"\"\"\n    Solves the Laplace equation using SOR with red-black ordering.\n    Recovers Gauss-Seidel for omega=1.0.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n        omega (float): Relaxation parameter.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n        \n    threshold = tol * r0_norm\n    \n    # Create red-black masks for the interior (N x N) grid.\n    # (j, i) indices for the interior part start from 0.\n    # Grid point (j_grid, i_grid) where j_grid, i_grid in [1,N]\n    # corresponds to mask point (j_grid-1, i_grid-1).\n    # Color depends on (j_grid + i_grid). (j_grid-1) + (i_grid-1) has same parity.\n    I, J = np.meshgrid(np.arange(N), np.arange(N))\n    red_mask = (I + J) % 2 == 0\n    black_mask = ~red_mask\n    \n    k = 0\n    while True:\n        k += 1\n        \n        # Keep a copy of the interior from the start of the iteration\n        # for the (1-omega) term.\n        u_old_interior = u[1:N + 1, 1:N + 1].copy()\n\n        # Update red points. Neighbors are black, use values from start of iteration.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][red_mask] = (1 - omega) * u_old_interior[red_mask] + \\\n                                      omega * gs_update[red_mask]\n\n        # Update black points. Neighbors are red, use newly updated values.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][black_mask] = (1 - omega) * u_old_interior[black_mask] + \\\n                                        omega * gs_update[black_mask]\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (20, 1e-5, 1.5),\n        (20, 1e-5, 1.0),\n        (10, 1e-8, 1.8),\n        (40, 1e-4, 1.9),\n    ]\n\n    results = []\n    for N, tol, omega_sor in test_cases:\n        # Calculate iterations for Jacobi\n        n_J = solve_jacobi(N, tol)\n        \n        # Calculate iterations for Gauss-Seidel (SOR with omega=1.0)\n        n_GS = solve_sor(N, tol, 1.0)\n        \n        # Calculate iterations for SOR with the specified omega\n        n_SOR = solve_sor(N, tol, omega_sor)\n        \n        results.append([n_J, n_GS, n_SOR])\n\n    # Format the output string as specified: [[r1,r2,r3],[...],...]\n    formatted_results = [f'[{\",\".join(map(str, r))}]' for r in results]\n    print(f\"[[{','.join(formatted_results)}]]\")\n\nsolve()\n```", "id": "2406769"}, {"introduction": "Finally, to solidify your theoretical understanding, we will analyze the heart of the SOR method's power: the relaxation parameter $\\omega$. By examining a simple $2 \\times 2$ system, you will derive the optimal value, $\\omega_{opt}$, that guarantees the fastest possible convergence [@problem_id:1394844]. This analytical exercise demystifies the choice of $\\omega$ and connects the algorithm's performance directly to the spectral properties of its iteration matrix.", "problem": "Consider a simplified model of a physical system with two interacting states, whose equilibrium positions $x_1$ and $x_2$ are described by a system of linear equations $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = (x_1, x_2)^T$. The interaction is characterized by the matrix\n$$\nA = \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix}\n$$\nHere, $a$ is a real-valued coupling constant such that $0  a  1$.\n\nTo solve this system numerically, one can employ the Successive Over-Relaxation (SOR) method, which is an iterative technique that depends on a relaxation parameter, $\\omega$. The rate of convergence of the SOR method is critically dependent on the choice of $\\omega$. For a given matrix $A$, there exists an optimal value, $\\omega_{opt}$, that maximizes this rate of convergence.\n\nYour task is to determine the optimal relaxation parameter for this system. Derive an expression for $\\omega_{opt}$ solely in terms of the coupling constant $a$.", "solution": "We solve the linear system with SOR using the standard splitting $A = D + L + U$, where for\n$$\nA=\\begin{pmatrix}1  a \\\\ a  1\\end{pmatrix},\n$$\nwe have $D=\\operatorname{diag}(1,1)$, $L=\\begin{pmatrix}0  0 \\\\ a  0\\end{pmatrix}$, and $U=\\begin{pmatrix}0  a \\\\ 0  0\\end{pmatrix}$. The SOR iteration is\n$$\n(D+\\omega L)\\,\\mathbf{x}^{k+1}=\\omega \\mathbf{b} - \\big(\\omega U + (\\omega - 1)D\\big)\\,\\mathbf{x}^{k},\n$$\nso the iteration matrix is\n$$\nT_{\\omega}=-(D+\\omega L)^{-1}\\big(\\omega U + (\\omega - 1)D\\big).\n$$\nSince\n$$\nD+\\omega L=\\begin{pmatrix}1  0 \\\\ \\omega a  1\\end{pmatrix},\\quad (D+\\omega L)^{-1}=\\begin{pmatrix}1  0 \\\\ -\\omega a  1\\end{pmatrix},\n$$\nand\n$$\n\\omega U + (\\omega - 1)D=\\begin{pmatrix}\\omega - 1  \\omega a \\\\ 0  \\omega - 1\\end{pmatrix},\n$$\nwe obtain\n$$\nT_{\\omega}\n=-\\begin{pmatrix}1  0 \\\\ -\\omega a  1\\end{pmatrix}\n\\begin{pmatrix}\\omega - 1  \\omega a \\\\ 0  \\omega - 1\\end{pmatrix}\n=\\begin{pmatrix}\n-(\\omega - 1)  -\\omega a \\\\\n\\omega a(\\omega - 1)  \\omega^{2}a^{2} - (\\omega - 1)\n\\end{pmatrix}.\n$$\nLet $s=\\omega - 1$. The trace and determinant of $T_{\\omega}$ are\n$$\nt=\\operatorname{tr}(T_{\\omega})=\\omega^{2}a^{2}-2s,\\qquad d=\\det(T_{\\omega})=s^{2}.\n$$\nThe characteristic polynomial is\n$$\n\\lambda^{2}-t\\lambda + s^{2}=0,\n$$\nso the eigenvalues are\n$$\n\\lambda_{1,2}=\\frac{t\\pm\\sqrt{t^{2}-4s^{2}}}{2}.\n$$\nThe discriminant simplifies to\n$$\nt^{2}-4s^{2}=(\\omega^{2}a^{2}-2s)^{2}-4s^{2}=\\omega^{2}a^{2}\\big(\\omega^{2}a^{2}-4s\\big).\n$$\nSince the product of the eigenvalues is $s^{2}$, for a given $s$ the maximal modulus of the eigenvalues is minimized when the two eigenvalues have equal modulus. This occurs at the boundary between complex-conjugate and distinct-real eigenvalues, namely when the discriminant vanishes:\n$$\n\\omega^{2}a^{2}-4s=0\\quad\\Longleftrightarrow\\quad a^{2}\\omega^{2}-4\\omega+4=0.\n$$\nSolving the quadratic for $\\omega$ gives\n$$\n\\omega=\\frac{4\\pm 4\\sqrt{1-a^{2}}}{2a^{2}}=\\frac{2\\pm 2\\sqrt{1-a^{2}}}{a^{2}}.\n$$\nFor $0a1$, the smaller root lies in the admissible range and yields optimal convergence. Simplifying it,\n$$\n\\omega_{\\mathrm{opt}}=\\frac{2-2\\sqrt{1-a^{2}}}{a^{2}}\n=\\frac{2(1-\\sqrt{1-a^2})}{a^2} \\cdot \\frac{1+\\sqrt{1-a^2}}{1+\\sqrt{1-a^2}} = \\frac{2(1-(1-a^2))}{a^2(1+\\sqrt{1-a^2})}\n=\\frac{2}{1+\\sqrt{1-a^{2}}}.\n$$\nTherefore, the optimal relaxation parameter expressed in terms of $a$ is\n$$\n\\omega_{\\mathrm{opt}}=\\frac{2}{1+\\sqrt{1-a^{2}}}.\n$$", "answer": "$$\\boxed{\\frac{2}{1+\\sqrt{1-a^{2}}}}$$", "id": "1394844"}]}