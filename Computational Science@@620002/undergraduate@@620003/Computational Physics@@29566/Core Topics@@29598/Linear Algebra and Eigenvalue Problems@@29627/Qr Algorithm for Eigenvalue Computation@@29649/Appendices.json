{"hands_on_practices": [{"introduction": "Theory comes to life through implementation. This first practice challenges you to build a basic unshifted QR algorithm from the ground up and use it to numerically verify the fundamental theorem stating that the determinant of a matrix equals the product of its eigenvalues. This exercise not only familiarizes you with the core iterative loop but also reinforces the deep connection between different matrix properties [@problem_id:2431464].", "problem": "Implement a complete program that numerically validates, across a small test suite of real matrices, the equivalence between two independently computed scalar quantities derived from a square matrix: the determinant computed directly from the matrix and the product computed from the eigenvalues approximated by an iterative orthogonal-triangular (QR) algorithm. Your implementation must adhere to the following requirements.\n\nFundamental base for the task:\n- Use the definition of eigenvalues via the characteristic equation $ \\det(A - \\lambda I) = 0 $ for a real square matrix $ A \\in \\mathbb{R}^{n \\times n} $.\n- Use basic properties of determinants and similarity transformations, including that if $ A = Q R $ is an orthogonal-triangular (QR) factorization with $ Q^\\mathsf{T} Q = I $, then the orthogonal similarity $ A_{k+1} = R Q $ preserves the spectrum of $ A $ while improving triangularity in the iterative orthogonal-triangular algorithm.\n- Use standard numerical linear algebra facts: orthogonal transformations are norm-preserving and generally well-conditioned; upper triangular matrices have eigenvalues equal to their diagonal entries.\n\nAlgorithmic constraints:\n- Implement the orthogonal-triangular (QR) algorithm without calling any built-in eigenvalue solver. Specifically, for a real square matrix $ A $, iterate $ A_{k+1} = R_k Q_k $ where $ A_k = Q_k R_k $ is a QR factorization at iteration $ k $. Use a stopping criterion based on the norm of the strictly off-diagonal entries for symmetric inputs. For matrices that are exactly upper triangular within numerical tolerance, read the eigenvalues directly from the diagonal.\n- For symmetric inputs, apply the unshifted orthogonal-triangular iteration and declare convergence when the Frobenius norm of strictly off-diagonal entries is less than $ \\varepsilon \\, \\|A\\|_F $, where $ \\varepsilon = 10^{-12} $.\n- Do not use any specialized shift strategies; unshifted iteration is acceptable for the small matrices in this test suite.\n\nError metric and pass/fail criterion:\n- For each test matrix $ A $, compute $ P $, the product of the approximate eigenvalues returned by your orthogonal-triangular algorithm, and compute $ D = \\det(A) $ using a stable direct routine.\n- Define the error as follows. If $ |D| \\ge 10^{-12} $, use the relative error $ e = |P - D| / |D| $. Otherwise, use the absolute error $ e = |P - D| $.\n- A test passes if $ e \\le 10^{-8} $.\n\nAngle unit requirement:\n- Any angles used to construct test matrices must be in radians.\n\nTest suite:\nConstruct the following five real matrices. Let $ \\cos(\\cdot) $ and $ \\sin(\\cdot) $ denote cosine and sine with inputs in radians. Define the $ 3 \\times 3 $ rotation matrices\n$$\nR_z(\\theta) =\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta & 0 \\\\\n\\sin\\theta & \\cos\\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}, \\quad\nR_y(\\phi) =\n\\begin{bmatrix}\n\\cos\\phi & 0 & \\sin\\phi \\\\\n0 & 1 & 0 \\\\\n-\\sin\\phi & 0 & \\cos\\phi\n\\end{bmatrix}, \\quad\nR_x(\\psi) =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos\\psi & -\\sin\\psi \\\\\n0 & \\sin\\psi & \\cos\\psi\n\\end{bmatrix}.\n$$\nUse the following five cases:\n- Case $ 1 $ (symmetric positive definite, dense): Let $ \\theta = \\pi/6 $, $ \\phi = \\pi/5 $, $ Q_1 = R_z(\\theta) R_y(\\phi) $, and $ D_1 = \\mathrm{diag}(2.5, 1.2, 0.7) $. Set $ A_1 = Q_1 D_1 Q_1^\\mathsf{T} $.\n- Case $ 2 $ (symmetric indefinite, dense): Let $ \\psi = \\pi/7 $, $ \\varphi = \\pi/8 $, $ Q_2 = R_x(\\psi) R_z(\\varphi) $, and $ D_2 = \\mathrm{diag}(3.0, -1.0, 0.5) $. Set $ A_2 = Q_2 D_2 Q_2^\\mathsf{T} $.\n- Case $ 3 $ (upper triangular, non-symmetric): Define the $ 4 \\times 4 $ matrix\n$$\nA_3 =\n\\begin{bmatrix}\n1.0 & 0.3 & -0.2 & 0.1 \\\\\n0.0 & 2.0 & -0.1 & 0.25 \\\\\n0.0 & 0.0 & 3.0 & 0.05 \\\\\n0.0 & 0.0 & 0.0 & 4.0\n\\end{bmatrix}.\n$$\n- Case $ 4 $ (diagonal, nearly singular): Let $ A_4 = \\mathrm{diag}(10^{-8}, 1.0, 2.0) $.\n- Case $ 5 $ (scalar boundary): Let $ A_5 = [5.0] $.\n\nProgram output format:\n- For each case $ i \\in \\{1,2,3,4,5\\} $, compute the pass/fail boolean as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $ [\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5] $.\n\nAdditional constraints:\n- Your program must be self-contained, must not read any input, and must not access external files or networks.\n- Do not call any built-in eigenvalue solvers; an implementation based on repeated orthogonal-triangular factorizations is required.", "solution": "We begin from fundamental definitions and facts. For a real square matrix $ A \\in \\mathbb{R}^{n \\times n} $, a scalar $ \\lambda \\in \\mathbb{C} $ is an eigenvalue if there exists a nonzero vector $ v \\in \\mathbb{C}^n $ such that $ A v = \\lambda v $. Equivalently, $ \\lambda $ is a root of the characteristic polynomial $ p_A(\\lambda) = \\det(A - \\lambda I) $. The determinant $ \\det(A) $ is a scalar functional on square matrices that is multiplicative over matrix products and invariant under similarity transformations. For upper triangular matrices, the determinant equals the product of diagonal entries, and their eigenvalues coincide with those diagonal entries.\n\nThe orthogonal-triangular (QR) algorithm is an iterative method to approximate eigenvalues, rooted in the observation that orthogonal similarity transformations preserve the spectrum while improving triangular structure. At iteration $ k $, one computes a QR factorization $ A_k = Q_k R_k $ with $ Q_k^\\mathsf{T} Q_k = I $ and $ R_k $ upper triangular, then forms the orthogonal similarity\n$$\nA_{k+1} = R_k Q_k = Q_k^\\mathsf{T} A_k Q_k.\n$$\nBecause $ A_{k+1} $ is orthogonally similar to $ A_k $, they share the same eigenvalues. Repeating this process tends to drive $ A_k $ towards an upper triangular (Schur) form, and when $ A $ is symmetric, towards a diagonal form. In the symmetric case, convergence of the off-diagonal entries to zero is a standard and robust behavior for the unshifted algorithm on small matrices, due to the fact that symmetric matrices are orthogonally diagonalizable.\n\nTo design a numerically robust stopping criterion, we monitor the Frobenius norm of strictly off-diagonal entries. Let $ \\mathrm{off}(A) $ denote the matrix equal to $ A $ with its diagonal entries set to zero. We declare convergence when\n$$\n\\|\\mathrm{off}(A_k)\\|_F \\le \\varepsilon \\, \\|A_k\\|_F,\n$$\nwith $ \\varepsilon = 10^{-12} $. For matrices that are exactly upper triangular within numerical tolerance, their eigenvalues can be read directly from the diagonal, since an upper triangular matrix is already in real Schur form. For a $ 1 \\times 1 $ matrix $ [\\alpha] $, the sole eigenvalue is $ \\alpha $.\n\nHaving approximated the eigenvalues $ \\{\\lambda_i\\}_{i=1}^n $ of a test matrix $ A $ using the orthogonal-triangular iteration, we compute the scalar\n$$\nP = \\prod_{i=1}^n \\lambda_i,\n$$\nand independently compute the determinant $ D = \\det(A) $ using a stable direct routine. To quantify numerical agreement between these two independently computed quantities, we use a scale-sensitive error definition:\n- If $ |D| \\ge 10^{-12} $, use the relative error $ e = \\frac{|P - D|}{|D|} $.\n- Otherwise, use the absolute error $ e = |P - D| $.\nWe then declare success for that matrix if $ e \\le 10^{-8} $.\n\nWe construct a test suite to probe several facets: a dense symmetric positive definite matrix $ A_1 $, a dense symmetric indefinite matrix $ A_2 $, an upper triangular non-symmetric matrix $ A_3 $, a nearly singular diagonal matrix $ A_4 $, and a scalar matrix $ A_5 $. The dense symmetric cases are formed via orthogonal similarity $ A = Q D Q^\\mathsf{T} $ with orthogonal $ Q $ given as products of rotations about coordinate axes:\n$$\nR_z(\\theta) =\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta & 0 \\\\\n\\sin\\theta & \\cos\\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}, \\quad\nR_y(\\phi) =\n\\begin{bmatrix}\n\\cos\\phi & 0 & \\sin\\phi \\\\\n0 & 1 & 0 \\\\\n-\\sin\\phi & 0 & \\cos\\phi\n\\end{bmatrix}, \\quad\nR_x(\\psi) =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos\\psi & -\\sin\\psi \\\\\n0 & \\sin\\psi & \\cos\\psi\n\\end{bmatrix},\n$$\nwith angles in radians. Specifically, we use:\n- $ A_1 = Q_1 D_1 Q_1^\\mathsf{T} $ with $ Q_1 = R_z(\\pi/6) R_y(\\pi/5) $ and $ D_1 = \\mathrm{diag}(2.5, 1.2, 0.7) $.\n- $ A_2 = Q_2 D_2 Q_2^\\mathsf{T} $ with $ Q_2 = R_x(\\pi/7) R_z(\\pi/8) $ and $ D_2 = \\mathrm{diag}(3.0, -1.0, 0.5) $.\n- $ A_3 $ is an explicitly given $ 4 \\times 4 $ upper triangular matrix with positive diagonal entries, so its eigenvalues are exactly its diagonal entries.\n- $ A_4 $ is diagonal with a very small entry $ 10^{-8} $ to test near-singularity handling.\n- $ A_5 $ is a $ 1 \\times 1 $ matrix to test the boundary case.\n\nImplementation details:\n- For symmetric matrices, we apply unshifted orthogonal-triangular iterations until the off-diagonal norm criterion is met or a maximum iteration cap is reached (e.g., $ 10^4 $ iterations), which is ample for the small sizes used here.\n- For upper triangular matrices detected by $ \\|\\mathrm{strict\\_lower}(A)\\|_{\\max} \\le 10^{-14} $, we return the diagonal entries as the eigenvalues.\n- We compute the determinant using a standard direct routine, then compare to the product of the approximated eigenvalues using the error metric above.\n\nThe final program constructs the five matrices, runs the orthogonal-triangular eigenvalue approximation as specified, computes $ P $ and $ D $, evaluates pass/fail booleans for each case, and prints a single line with the list $ [\\text{bool}_1,\\text{bool}_2,\\text{bool}_3,\\text{bool}_4,\\text{bool}_5] $. Provided the implementation adheres to the algorithmic and numerical specifications, all five cases will pass with the stated tolerance, thereby demonstrating numerically that the product computed from eigenvalues obtained via the orthogonal-triangular algorithm agrees with the determinant computed directly, within the prescribed error bounds.", "answer": "```python\nimport numpy as np\n\ndef rotation_matrices():\n    # Define rotation matrices about z, y, x axes with angle in radians\n    def Rz(theta):\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s, 0.0],\n                         [s,  c, 0.0],\n                         [0.0, 0.0, 1.0]], dtype=float)\n\n    def Ry(phi):\n        c, s = np.cos(phi), np.sin(phi)\n        return np.array([[ c, 0.0,  s],\n                         [0.0, 1.0, 0.0],\n                         [-s, 0.0,  c]], dtype=float)\n\n    def Rx(psi):\n        c, s = np.cos(psi), np.sin(psi)\n        return np.array([[1.0, 0.0, 0.0],\n                         [0.0,  c, -s],\n                         [0.0,  s,  c]], dtype=float)\n    return Rz, Ry, Rx\n\ndef is_symmetric(a, tol=1e-14):\n    return np.allclose(a, a.T, atol=tol, rtol=0.0)\n\ndef is_upper_triangular(a, tol=1e-14):\n    return np.all(np.abs(np.tril(a, -1)) <= tol)\n\ndef qr_eigvals_symmetric(a, tol=1e-12, max_iter=10000):\n    \"\"\"\n    Unshifted QR iteration for symmetric matrices.\n    Returns approximate eigenvalues as the diagonal of the iterated matrix.\n    \"\"\"\n    n = a.shape[0]\n    ak = a.copy().astype(float)\n    # Iterate until off-diagonal Frobenius norm is small\n    for _ in range(max_iter):\n        # Compute off-diagonal Frobenius norm\n        off = ak - np.diag(np.diag(ak))\n        off_norm = np.linalg.norm(off, ord='fro')\n        total_norm = np.linalg.norm(ak, ord='fro')\n        # Avoid division by zero in case of zero matrix\n        scale = max(1.0, total_norm)\n        if off_norm <= tol * scale:\n            break\n        q, r = np.linalg.qr(ak)\n        ak = r @ q\n    return np.diag(ak)\n\ndef eigenvalues_via_qr(a):\n    \"\"\"\n    Compute eigenvalues via QR algorithm under the constraints:\n    - If symmetric (within tolerance), use unshifted QR iteration until convergence.\n    - If upper triangular (within tolerance), return diagonal entries.\n    - If 1x1, return the single entry.\n    For this test suite, these branches suffice.\n    \"\"\"\n    n, m = a.shape\n    if n != m:\n        raise ValueError(\"Matrix must be square.\")\n    if n == 1:\n        return np.array([a[0, 0]], dtype=float)\n    if is_symmetric(a):\n        return qr_eigvals_symmetric(a)\n    if is_upper_triangular(a):\n        return np.diag(a).copy()\n    # Fallback: for robustness (not used in provided tests), attempt generic unshifted QR\n    # without claiming convergence to diagonal; will return diagonal of iterates.\n    ak = a.copy().astype(float)\n    for _ in range(5000):\n        q, r = np.linalg.qr(ak)\n        ak = r @ q\n        # If subdiagonal is sufficiently small, we can read diagonal as eigenvalues\n        if np.all(np.abs(np.tril(ak, -1)) <= 1e-12):\n            break\n    return np.diag(ak)\n\ndef build_test_matrices():\n    Rz, Ry, Rx = rotation_matrices()\n    # Case 1: Symmetric positive definite (3x3), dense\n    theta = np.pi / 6.0\n    phi = np.pi / 5.0\n    Q1 = Rz(theta) @ Ry(phi)\n    D1 = np.diag([2.5, 1.2, 0.7])\n    A1 = Q1 @ D1 @ Q1.T\n\n    # Case 2: Symmetric indefinite (3x3), dense\n    psi = np.pi / 7.0\n    varphi = np.pi / 8.0\n    Q2 = Rx(psi) @ Rz(varphi)\n    D2 = np.diag([3.0, -1.0, 0.5])\n    A2 = Q2 @ D2 @ Q2.T\n\n    # Case 3: Upper triangular (4x4), non-symmetric\n    A3 = np.array([\n        [1.0, 0.3, -0.2, 0.1],\n        [0.0, 2.0, -0.1, 0.25],\n        [0.0, 0.0, 3.0, 0.05],\n        [0.0, 0.0, 0.0, 4.0]\n    ], dtype=float)\n\n    # Case 4: Diagonal nearly singular (3x3)\n    A4 = np.diag([1e-8, 1.0, 2.0])\n\n    # Case 5: Scalar boundary (1x1)\n    A5 = np.array([[5.0]], dtype=float)\n\n    return [A1, A2, A3, A4, A5]\n\ndef compare_product_vs_det(a, rel_tol=1e-8, abs_thresh=1e-12):\n    eigs = eigenvalues_via_qr(a)\n    P = float(np.prod(eigs))\n    D = float(np.linalg.det(a))\n    diff = abs(P - D)\n    if abs(D) >= abs_thresh:\n        err = diff / abs(D)\n    else:\n        err = diff\n    return err <= rel_tol\n\ndef solve():\n    test_matrices = build_test_matrices()\n    results = [compare_product_vs_det(A) for A in test_matrices]\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2431464"}, {"introduction": "A robust algorithm must be understood not just by when it works, but also by when it fails. This exercise explores the behavior of the unshifted QR iteration on matrices that challenge its convergence criteria, revealing how eigenvalues with equal magnitudes can prevent convergence to a triangular form. Understanding this limitation [@problem_id:2431496] is the crucial first step toward appreciating the need for the sophisticated shift strategies used in modern solvers.", "problem": "You are given real square matrices and the following iterative map based on the orthogonal-upper triangular (QR) factorization. For a given real matrix $A^{(0)} \\in \\mathbb{R}^{n \\times n}$, define a sequence $\\{A^{(k)}\\}_{k=0}^{\\infty}$ by the rule: for each iteration index $k \\ge 0$, compute the orthogonal-upper triangular (QR) factorization $A^{(k)} = Q^{(k)} R^{(k)}$ where $Q^{(k)}$ is orthogonal and $R^{(k)}$ is upper triangular with positive diagonal entries, and then set\n$$\nA^{(k+1)} = R^{(k)} Q^{(k)}.\n$$\nConsider the following decision problem for $2 \\times 2$ real matrices: run the above iteration with tolerance $\\tau = 10^{-12}$ and maximum number of iterations $k_{\\max} = 100$. Classify the behavior of the sequence using the following mutually exclusive rules:\n- Converged to upper triangular: if the Euclidean norm of the strictly lower-triangular part of $A^{(k)}$ satisfies $\\lVert \\operatorname{tril}(A^{(k)}, -1) \\rVert_{F} < \\tau$ for some $k \\le k_{\\max}$, then report the integer code $0$.\n- Detected a cycle of length $1$: if for some $k \\le k_{\\max}$ one has $\\lVert A^{(k)} - A^{(k-1)} \\rVert_{F} < \\tau$, then report the integer code $1$.\n- Detected a cycle of length $2$: if for some $k \\le k_{\\max}$ one has $\\lVert A^{(k)} - A^{(k-2)} \\rVert_{F} < \\tau$, then report the integer code $2$.\n- Otherwise: if none of the above conditions is met within $k_{\\max}$ iterations, report the integer code $3$.\n\nHere $\\lVert \\cdot \\rVert_{F}$ denotes the Frobenius norm, $\\operatorname{tril}(\\cdot,-1)$ extracts the strictly lower-triangular part, and the diagonal positivity constraint on $R^{(k)}$ must be enforced at each step.\n\nUse this decision problem to investigate the behavior of the iteration on the following test suite of matrices:\n- Test $1$ (flip matrix): \n$$\nA^{(1)} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n$$\n- Test $2$ (symmetric with distinct eigenvalues): \n$$\nA^{(2)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}.\n$$\n- Test $3$ (identity): \n$$\nA^{(3)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n$$\n- Test $4$ (upper triangular with positive diagonal): \n$\nA^{(4)} = \\begin{bmatrix} 1 & 10 \\\\ 0 & 1 \\end{bmatrix}.\n$\n\nFor each of the above $4$ tests, your program must run the iteration starting at $A^{(0)} = A^{(j)}$ (for $j \\in \\{1,2,3,4\\}$) and return the corresponding integer code according to the rules above.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of the $4$ integer codes enclosed in square brackets (e.g., $[c_1,c_2,c_3,c_4]$). No other text should be printed. No physical units or angles are involved in this problem; all outputs are unitless integers.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Iterative Map**: For a real matrix $A^{(0)} \\in \\mathbb{R}^{n \\times n}$, a sequence $\\{A^{(k)}\\}_{k=0}^{\\infty}$ is defined by:\n  1. $A^{(k)} = Q^{(k)} R^{(k)}$, where $Q^{(k)}$ is orthogonal and $R^{(k)}$ is upper triangular with positive diagonal entries.\n  2. $A^{(k+1)} = R^{(k)} Q^{(k)}$.\n- **Decision Problem Parameters for $2 \\times 2$ matrices**:\n  - Tolerance: $\\tau = 10^{-12}$.\n  - Maximum iterations: $k_{\\max} = 100$.\n- **Classification Rules**:\n  1.  **Code 0 (Converged to upper triangular)**: $\\lVert \\operatorname{tril}(A^{(k)}, -1) \\rVert_{F} < \\tau$ for some $k \\in [0, k_{\\max}]$.\n  2.  **Code 1 (Cycle of length 1)**: $\\lVert A^{(k)} - A^{(k-1)} \\rVert_{F} < \\tau$ for some $k \\in [1, k_{\\max}]$.\n  3.  **Code 2 (Cycle of length 2)**: $\\lVert A^{(k)} - A^{(k-2)} \\rVert_{F} < \\tau$ for some $k \\in [2, k_{\\max}]$.\n  4.  **Code 3 (Otherwise)**: None of the above conditions are met.\n- **Definitions**:\n  - $\\lVert \\cdot \\rVert_{F}$ is the Frobenius norm.\n  - $\\operatorname{tril}(\\cdot, -1)$ is the strictly lower-triangular part of a matrix.\n- **Test Matrices ($A^{(0)}$)**:\n  1.  $A^{(1)} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$\n  2.  $A^{(2)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n  3.  $A^{(3)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n  4.  $A^{(4)} = \\begin{bmatrix} 1 & 10 \\\\ 0 & 1 \\end{bmatrix}$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem describes the pure (unshifted) QR algorithm, a fundamental and well-established method in numerical linear algebra for eigenvalue computation. The described behavior and classification rules are standard tests for analyzing the convergence of such iterative methods. The problem is scientifically sound.\n- **Well-Posedness**: The iterative map is deterministic. The condition that the upper triangular matrix $R^{(k)}$ must have positive diagonal entries ensures that the QR factorization is unique for any non-singular matrix $A^{(k)}$. All initial test matrices are non-singular, and since the iteration $A^{(k+1)} = (Q^{(k)})^T A^{(k)} Q^{(k)}$ is a similarity transformation, all subsequent matrices $A^{(k)}$ will also be non-singular. The classification rules are conditional checks to be performed at each iteration. To ensure the \"mutually exclusive\" nature of the rules, they must be checked in a specific order of precedence. The natural and logical order is the one presented: $0, 1, 2, 3$. This makes the problem well-posed.\n- **Objectivity**: The problem is stated using precise mathematical language, with clearly defined quantities, conditions, and numerical values. It is free from ambiguity and subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be provided.\n\nThe QR iteration is defined by the sequence $A^{(k+1)} = R^{(k)}Q^{(k)}$, where $A^{(k)} = Q^{(k)}R^{(k)}$. Substituting $R^{(k)} = (Q^{(k)})^T A^{(k)}$ (since $Q^{(k)}$ is orthogonal) into the update rule gives $A^{(k+1)} = (Q^{(k)})^T A^{(k)} Q^{(k)}$. This reveals that each $A^{(k)}$ is orthogonally similar to its predecessor, and thus all matrices in the sequence $\\{A^{(k)}\\}$ share the same eigenvalues as the initial matrix $A^{(0)}$.\n\nThe algorithm is known to converge to an upper triangular (or quasi-triangular for complex eigenvalues) Schur form under certain conditions. Specifically, for a real matrix with eigenvalues $\\{\\lambda_i\\}$ of distinct magnitudes, $|\\lambda_1| > |\\lambda_2| > \\dots > |\\lambda_n|$, the sequence $A^{(k)}$ converges to an upper triangular matrix whose diagonal entries are the eigenvalues of $A^{(0)}$. The classification rules are designed to test for this convergence or identify alternative behaviors like fixed points or cycles, which can occur when the eigenvalue magnitude condition is not met. The rules are checked in the order $0, 1, 2$ at each iteration $k$ from $0$ to $k_{max}$.\n\n**Analysis of Test Cases:**\n\n**Test 1:** $A^{(0)} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$.\nThis matrix is symmetric. Its characteristic equation is $\\lambda^2 - 1 = 0$, yielding eigenvalues $\\lambda_1 = 1$ and $\\lambda_2 = -1$. The magnitudes of the eigenvalues are equal, $|\\lambda_1| = |\\lambda_2| = 1$, which violates the standard convergence condition for the simple QR algorithm.\nTo determine the behavior, we compute the first iteration. We require the unique QR factorization $A^{(0)} = Q^{(0)}R^{(0)}$ where $R^{(0)}$ has positive diagonal entries. For $A^{(0)}$, this decomposition is $Q^{(0)} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$ and $R^{(0)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I$. Both $Q^{(0)}$ being orthogonal and $R^{(0)}$ being upper triangular with positive diagonals are satisfied.\nThe next iterate is $A^{(1)} = R^{(0)}Q^{(0)} = I Q^{(0)} = Q^{(0)} = A^{(0)}$.\nThe sequence is constant: $A^{(k)} = A^{(0)}$ for all $k \\ge 0$. This is a fixed point of the iteration.\nWe now apply the classification rules:\n- At $k=0$: Is $A^{(0)}$ upper triangular? No, because the entry $A_{21}^{(0)} = 1 \\neq 0$. The norm $\\lVert \\operatorname{tril}(A^{(0)},-1)\\rVert_F = 1$, which is not less than $\\tau = 10^{-12}$. Rule $0$ is not met.\n- At $k=1$: We have $A^{(1)} = A^{(0)}$.\n  - Rule $0$: Is $A^{(1)}$ upper triangular? No.\n  - Rule $1$: Is $\\lVert A^{(1)} - A^{(0)} \\rVert_F < \\tau$? Yes, because $\\lVert A^{(0)} - A^{(0)} \\rVert_F = 0 < \\tau$.\nThe process terminates and reports code $1$.\n\n**Test 2:** $A^{(0)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$.\nThis is a symmetric matrix. Its characteristic equation is $(2-\\lambda)^2 - 1 = 0$, giving eigenvalues $\\lambda_1 = 3$ and $\\lambda_2 = 1$. The eigenvalues are real and have distinct magnitudes, $|3| > |1|$. The theory of the QR algorithm predicts that for such a matrix, the sequence $A^{(k)}$ will converge to a diagonal matrix containing the eigenvalues, ordered by magnitude: $A^{(k)} \\to \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}$. A diagonal matrix is a special case of an upper triangular matrix. Therefore, the strictly lower-triangular part of $A^{(k)}$ will converge to zero. The condition $\\lVert \\operatorname{tril}(A^{(k)},-1)\\rVert_F < \\tau$ will be satisfied for some sufficiently large $k \\le k_{\\max}$.\nThe classification will be code $0$.\n\n**Test 3:** $A^{(0)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$.\nThe initial matrix is the identity matrix. It is already in upper triangular (in fact, diagonal) form.\nWe apply the classification rules at the very first step, $k=0$:\n- Rule $0$: Is $\\lVert \\operatorname{tril}(A^{(0)},-1)\\rVert_F < \\tau$? The strictly lower-triangular part of the identity matrix is the zero matrix, so its Frobenius norm is $0$. The condition $0 < \\tau$ is met.\nThe process terminates immediately and reports code $0$.\n\n**Test 4:** $A^{(0)} = \\begin{bmatrix} 1 & 10 \\\\ 0 & 1 \\end{bmatrix}$.\nThe initial matrix is already in upper triangular form. This is a non-diagonalizable Jordan block with repeated eigenvalues $\\lambda_1=\\lambda_2=1$.\nSimilar to Test 3, we check the condition at $k=0$:\n- Rule $0$: Is $\\lVert \\operatorname{tril}(A^{(0)},-1)\\rVert_F < \\tau$? The matrix is upper triangular, so its strictly lower-triangular part is zero. The norm is $0$. The condition $0 < \\tau$ is met.\nThe process terminates and reports code $0$.\nIt is also a fixed point of the iteration, as $A^{(0)}$ being upper triangular with positive diagonals gives $Q^{(0)}=I$ and $R^{(0)}=A^{(0)}$, so $A^{(1)} = R^{(0)}Q^{(0)} = A^{(0)}I = A^{(0)}$. However, the rule for convergence to upper triangular form has precedence and is satisfied at $k=0$.\n\nThe predicted classification codes are $[1, 0, 0, 0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the QR iteration classification problem for a suite of test matrices.\n    \"\"\"\n\n    def classify_qr_behavior(A0, k_max, tau):\n        \"\"\"\n        Runs the QR iteration and classifies the behavior of the sequence A^(k).\n\n        Args:\n            A0 (np.ndarray): The initial 2x2 real matrix.\n            k_max (int): The maximum number of iterations.\n            tau (float): The tolerance for convergence/cycle detection.\n\n        Returns:\n            int: The integer classification code (0, 1, 2, or 3).\n        \"\"\"\n        # A sliding window of the last three matrices in the sequence: [A^(k-2), A^(k-1), A^(k)]\n        A_hist = [\n            np.full_like(A0, np.nan, dtype=float),\n            np.full_like(A0, np.nan, dtype=float),\n            A0.astype(float)\n        ]\n\n        # Default result if no condition is met within k_max iterations.\n        result_code = 3\n\n        for k in range(k_max + 1):\n            A_k = A_hist[2]\n\n            # Rule 0: Converged to upper triangular.\n            # This check has the highest precedence.\n            if np.linalg.norm(np.tril(A_k, -1), 'fro') < tau:\n                result_code = 0\n                break\n\n            # Rule 1: Detected a cycle of length 1 (fixed point).\n            # This check requires k >= 1 to have a previous matrix A^(k-1).\n            if k >= 1:\n                if np.linalg.norm(A_k - A_hist[1], 'fro') < tau:\n                    result_code = 1\n                    break\n            \n            # Rule 2: Detected a cycle of length 2.\n            # This check requires k >= 2 to have A^(k-2).\n            if k >= 2:\n                if np.linalg.norm(A_k - A_hist[0], 'fro') < tau:\n                    result_code = 2\n                    break\n            \n            # If the loop reaches its final iteration, break and return the default code 3.\n            if k == k_max:\n                break\n\n            # Compute the next matrix in the sequence, A^(k+1).\n            Q, R = np.linalg.qr(A_k)\n            \n            # Enforce the problem constraint: R must have positive diagonal entries.\n            # The QR decomposition is unique for non-singular matrices under this constraint.\n            signs = np.sign(np.diag(R))\n            # Handle potential zero on the diagonal (though not expected for these test cases).\n            signs[signs == 0] = 1.0\n            S = np.diag(signs)\n            \n            Q_corr = Q @ S\n            R_corr = S @ R\n            \n            A_next = R_corr @ Q_corr\n\n            # Update the history by sliding the window.\n            A_hist = [A_hist[1], A_hist[2], A_next]\n            \n        return result_code\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([[0., 1.], [1., 0.]]),    # Test 1\n        np.array([[2., 1.], [1., 2.]]),    # Test 2\n        np.array([[1., 0.], [0., 1.]]),    # Test 3\n        np.array([[1., 10.], [0., 1.]])   # Test 4\n    ]\n    \n    # Parameters for the decision problem.\n    tau = 1e-12\n    k_max = 100\n\n    results = []\n    for A0 in test_cases:\n        code = classify_qr_behavior(A0, k_max, tau)\n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431496"}, {"introduction": "Having understood the need for shifts, we now tackle the elegant solution: the Francis implicit double-shift QR step. This advanced practice guides you through implementing this cornerstone of modern eigenvalue solvers, using only real arithmetic to handle complex conjugate pairs efficiently [@problem_id:2431491]. Mastering this \"bulge-chasing\" technique provides a deep insight into the efficiency and stability of production-grade numerical software.", "problem": "You must implement, test, and run a single implicit double-shift Orthogonal-Triangular (QR) step for real matrices in upper Hessenberg form, using only real arithmetic, to handle cases where the trailing block has a complex conjugate eigenvalue pair. Your program must be a complete, runnable program. The objective is to realize one Francis implicit double-shift step as an orthogonal similarity transformation on a real upper Hessenberg matrix. The implementation must avoid any complex number arithmetic by working with a real quadratic shift constructed from the trailing $2\\times 2$ block.\n\nStart from fundamental definitions and facts:\n- An eigenpair of a matrix $A$ is a scalar $\\,\\lambda\\,$ and nonzero vector $\\,v\\,$ such that $\\,A v = \\lambda v\\,$.\n- A similarity transformation $\\,\\widetilde{A} = S^{-1} A S\\,$ preserves the characteristic polynomial and thus preserves all eigenvalues. If $\\,S\\,$ is orthogonal, then $\\,\\widetilde{A} = S^{\\mathsf{T}} A S\\,$ preserves inner products and the Frobenius norm.\n- Any square matrix $\\,A\\in\\mathbb{R}^{n\\times n}\\,$ can be reduced to (upper) Hessenberg form $\\,H\\,$ by orthogonal similarity, where $\\,H_{ij} = 0\\,$ for all $\\,i > j+1\\,$.\n- A double-shift QR step constructs a quadratic polynomial shift using the trailing $\\,2\\times 2\\,$ block of an upper Hessenberg matrix. If the trailing block is $\\,T = \\begin{bmatrix} t_{11} & t_{12}\\\\ t_{21} & t_{22}\\end{bmatrix}\\,$, then the shift polynomial is $\\,q(x) = x^2 - s x + p\\,$ with $\\,s = t_{11} + t_{22}\\,$ and $\\,p = t_{11} t_{22} - t_{12} t_{21}\\,$. This polynomial equals $\\, (x - \\mu_1)(x - \\mu_2)\\,$ where $\\,\\mu_1, \\mu_2\\,$ are the (possibly complex) eigenvalues of $\\,T\\,$, ensuring a real-coefficient polynomial. By the implicit Orthogonal-Triangular theorem, a sequence of orthogonal transformations constructed from $\\,q(H)\\,$ can be applied to effect the same result as two successive single-shift steps, without explicitly forming $\\,q(H)\\,$.\n\nYour task:\n- Implement a function that performs one real implicit double-shift QR step on a real upper Hessenberg matrix $\\,H \\in \\mathbb{R}^{n\\times n}\\,$, producing $\\,\\widehat{H} = Q^{\\mathsf{T}} H Q\\,$ with $\\,Q\\,$ orthogonal. You must:\n  - Use only real arithmetic throughout (no complex types).\n  - Use the quadratic shift coefficients $\\,s\\,$ and $\\,p\\,$ from the trailing $\\,2\\times 2\\,$ block as described above.\n  - Initialize the transformation via the leading vector of $\\,\\left(H^2 - s H + p I\\right) e_1\\,$, where $\\,e_1\\,$ is the first canonical basis vector, and then apply bulge-chasing using Householder reflectors to maintain the Hessenberg structure.\n  - Preserve orthogonality by performing left and right applications of each Householder reflector.\n  - Maintain upper Hessenberg form by annihilating fill-in elements below the first subdiagonal via the orthogonal transformations, and zeroing any lingering entries smaller than a numerically reasonable tolerance.\n\n- You must not use any prepackaged eigenvalue routines, complex arithmetic, or black-box functions that perform the QR algorithm. Linear algebra primitives such as matrix-vector products and Householder reflectors are allowed, but they must be constructed from first principles of orthogonal transformations.\n\nTest suite:\nApply your implementation to the following four real upper Hessenberg matrices (all entries are real numbers, and each matrix is given explicitly):\n1) Case A ($\\,4\\times 4\\,$):\n$$\nH_A =\n\\begin{bmatrix}\n1.0 & 2.0 & 0.0 & 0.0\\\\\n3.0 & 4.0 & 5.0 & 0.0\\\\\n0.0 & 6.0 & 0.5 & 2.0\\\\\n0.0 & 0.0 & -3.0 & 0.5\n\\end{bmatrix}.\n$$\n2) Case B ($\\,5\\times 5\\,$):\n$$\nH_B =\n\\begin{bmatrix}\n2.0 & -1.0 & 4.0 & 0.0 & 0.0\\\\\n5.0 & 3.0 & -2.0 & 1.0 & 0.0\\\\\n0.0 & -3.0 & 1.0 & 2.0 & 4.0\\\\\n0.0 & 0.0 & 1.5 & 0.0 & 2.0\\\\\n0.0 & 0.0 & 0.0 & -2.5 & 0.0\n\\end{bmatrix}.\n$$\n3) Case C ($\\,3\\times 3\\,$):\n$$\nH_C =\n\\begin{bmatrix}\n0.0 & 2.0 & 1.0\\\\\n-10.0 & 0.0 & 3.0\\\\\n0.0 & -4.0 & 1.0\n\\end{bmatrix}.\n$$\n4) Case D ($\\,2\\times 2\\,$):\n$$\nH_D =\n\\begin{bmatrix}\n0.0 & 5.0\\\\\n-4.0 & 0.0\n\\end{bmatrix}.\n$$\n\nVerification metrics to compute for each case after one double-shift step $\\,H \\mapsto \\widehat{H}\\,$:\n- $\\,b_1\\,$: upper Hessenberg structure preserved in $\\,\\widehat{H}\\,$ (boolean; true if all $\\,\\widehat{H}_{ij} = 0\\,$ for $\\,i > j+1\\,$ within a tolerance).\n- $\\,b_2\\,$: trace preserved, i.e., $\\,|\\mathrm{tr}(\\widehat{H}) - \\mathrm{tr}(H)|\\,$ is within a specified small tolerance (boolean).\n- $\\,b_3\\,$: Frobenius norm preserved, i.e., $\\,\\| \\widehat{H} \\|_F\\,$ equals $\\,\\|H\\|_F\\,$ within a small tolerance (boolean).\n- $\\,b_4\\,$: real arithmetic respected (boolean; true if $\\,\\widehat{H}\\,$ has no complex entries).\n\nNumerical tolerances:\n- Use a relative tolerance of $\\,10^{-10}\\,$ for trace preservation.\n- Use a relative tolerance of $\\,10^{-12}\\,$ for Frobenius norm preservation.\n- When checking the Hessenberg structure, treat entries $\\,\\widehat{H}_{ij}\\,$ with $\\,i > j+1\\,$ whose absolute values are less than $\\,10^{-12} \\cdot \\left(|\\widehat{H}_{i-1,i-1}| + |\\widehat{H}_{ii}|\\right)\\,$ as zero.\n\nFinal output format:\n- Your program must run the four cases in order $\\,\\{H_A,H_B,H_C,H_D\\}\\,$, compute the booleans $\\,b_1, b_2, b_3, b_4\\,$ for each case, and produce a single line of output containing the $\\,16\\,$ booleans as a comma-separated list enclosed in square brackets, in the order\n$$\n[b_{1,A}, b_{2,A}, b_{3,A}, b_{4,A},\\; b_{1,B}, b_{2,B}, b_{3,B}, b_{4,B},\\; b_{1,C}, b_{2,C}, b_{3,C}, b_{4,C},\\; b_{1,D}, b_{2,D}, b_{3,D}, b_{4,D}].\n$$\nNo angles or physical units are involved. The output elements are booleans. The program must be self-contained and require no user input.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the well-established theory of numerical linear algebra, specifically the Francis implicit double-shift QR algorithm for computing eigenvalues. The problem is well-posed, providing all necessary data, constraints, and verification metrics. It is objective and free from ambiguity or contradiction. We may therefore proceed with a solution.\n\nThe task is to implement a single step of the Francis implicit double-shift QR algorithm for a real upper Hessenberg matrix $\\,H \\in \\mathbb{R}^{n\\times n}\\,$. This algorithm is a cornerstone of nonsymmetric eigenvalue solvers, designed to find eigenvalues, including complex conjugate pairs, using only real arithmetic.\n\nA fundamental principle of eigenvalue computation is the use of similarity transformations, $\\,\\hat{H} = Q^{-1}HQ\\,$, which preserve the eigenvalues of $\\,H\\,$. When $\\,Q\\,$ is an orthogonal matrix ($\\,Q^{-1}=Q^{\\mathsf{T}}\\,$), the transformation is numerically stable. The QR algorithm iteratively applies such transformations to drive the matrix towards a form from which eigenvalues are easily extracted, typically an upper triangular (Schur) form.\n\nA simple QR step with a real shift $\\,\\mu\\,$ involves factoring $\\,H - \\mu I = QR\\,$ and forming $\\,\\hat{H} = R Q + \\mu I = Q^{\\mathsf{T}}HQ\\,$. However, if $\\,H\\,$ has complex conjugate eigenvalues, convergence can be slow or fail with only real shifts. A more robust strategy is to use two shifts, $\\,\\mu_1\\,$ and $\\,\\mu_2\\,$, corresponding to a complex conjugate pair of eigenvalues of a trailing submatrix. The direct application of two QR steps with complex shifts $\\,\\mu_1\\,$ and $\\,\\mu_2\\,$ would require complex arithmetic.\n\nThe implicit double-shift method, or Francis step, circumvents this by applying the two steps simultaneously using only real arithmetic. The two shifts give rise to a real quadratic polynomial $\\,q(x) = (x-\\mu_1)(x-\\mu_2) = x^2 - (\\mu_1+\\mu_2)x + \\mu_1\\mu_2 = x^2 - sx + p\\,$. The coefficients $\\,s\\,$ (trace) and $\\,p\\,$ (determinant) are calculated from the trailing $\\,2 \\times 2\\,$ submatrix of $\\,H\\,$, whose eigenvalues are used as the shifts $\\,\\mu_1, \\mu_2\\,$. This ensures $\\,s\\,$ and $\\,p\\,$ are real.\n\nThe algorithm then proceeds implicitly, leveraging the Implicit Q Theorem. This theorem essentially states that if an orthogonal similarity transformation $\\,Q^{\\mathsf{T}}HQ\\,$ reduces $\\,H\\,$ to an unreduced upper Hessenberg matrix $\\,\\hat{H}\\,$, and the first column of $\\,Q\\,$ is fixed, then the matrices $\\,Q\\,$ and $\\,\\hat{H}\\,$ are uniquely determined. We need only to construct the correct first column for our total transformation matrix $\\,Q\\,$ and the rest follows by enforcing the Hessenberg structure.\n\nThe first column of the desired transformation matrix $\\,Q\\,$ is proportional to the first column of the matrix $\\,(H^2 - sH + pI)\\,$ evaluated at $\\,H\\,$. Let $\\,\\mathbf{v} = (H^2 - sH + pI)\\mathbf{e}_1\\,$, where $\\,\\mathbf{e}_1\\,$ is the first canonical basis vector. For an upper Hessenberg $\\,H\\,$, this vector $\\,\\mathbf{v}\\,$ has only its first three components, $\\,v_0, v_1, v_2\\,$, as potentially non-zero.\n\nThe algorithm is as follows:\n\n1.  **Shift Calculation**: For a matrix $\\,H\\,$ of size $\\,n \\times n\\,$ with $\\,n \\geq 3\\,$, compute the trace $\\,s\\,$ and determinant $\\,p\\,$ of the trailing $\\,2 \\times 2\\,$ block, $\\,H[n-2:n, n-2:n]\\,$.\n\n2.  **Initiate the Bulge**: Compute the first three components of $\\,\\mathbf{v} = (H^2 - sH + pI)\\mathbf{e}_1\\,$. Construct a Householder reflector $\\,P_0\\,$ that acts on the first three rows/columns and maps this three-component vector to a multiple of $\\,\\mathbf{e}_1\\,$. The similarity transformation $\\,H \\leftarrow P_0 H P_0\\,$ creates a \"bulge\"—unwanted non-zero entries that break the Hessenberg structure, typically starting at position $\\,(2,0)\\,$ or $\\,(3,1)\\,$.\n\n3.  **Bulge Chasing**: A sequence of Householder reflectors $\\,P_1, P_2, \\ldots, P_{n-3}\\,$ is generated to chase the bulge down the subdiagonal and out of the matrix. Each reflector $\\,P_k\\,$ is designed to operate on rows and columns $\\,k+1\\,$ to $\\,k+3\\,$ to restore the Hessenberg structure in column $\\,k\\,$. This is done by forming a reflector from the column vector $\\,H[k+1:k+4, k]\\,$ to zero out elements below the sub-diagonal. Applying the similarity transformation $\\,H \\leftarrow P_k H P_k\\,$ restores column $\\,k\\,$ but creates a new bulge in column $\\,k+1\\,$. This process is repeated until the bulge is chased off the matrix.\n\n4.  **Finalization**: After the loop terminates, the matrix $\\,\\hat{H}\\,$ is returned to upper Hessenberg form up to floating-point precision. Small spurious entries below the subdiagonal must be set to zero based on a numerical tolerance. This step, however, is an approximation. The verification of trace and Frobenius norm preservation (metrics $\\,b_2, b_3\\,$) must be done on the matrix *before* this zeroing, as the pure orthogonal transformation preserves these quantities exactly. The check for Hessenberg structure ($\\,b_1\\,$) is performed on the cleaned matrix. The check for real arithmetic ($\\,b_4\\,$) is passed by construction.\n\nFor matrices with $\\,n < 3\\,$, a double-shift step is not applicable; the matrix is returned unchanged.\n\nThe implementation will construct a function that performs these steps. Householder reflectors are built using the standard numerically stable method. Left and right matrix multiplications are applied in sequence for each reflector to effect the similarity transformation.\n\nThe verification metrics are then computed:\n- $\\,b_1\\,$: Checks if $\\,\\hat{H}_{ij} = 0\\,$ for $\\,i > j+1\\,$ after applying the specified cleaning tolerance.\n- $\\,b_2\\,$: Confirms preservation of trace, $\\,|\\mathrm{tr}(\\hat{H}) - \\mathrm{tr}(H)| \\le 10^{-10} |\\mathrm{tr}(H)|\\,$.\n- $\\,b_3\\,$: Confirms preservation of the Frobenius norm, $\\,\\|\\hat{H}\\|_F\\,$ and $\\,\\|H\\|_F\\,$ are equal within a relative tolerance of $\\,10^{-12}\\,$.\n- $\\,b_4\\,$: Confirms the resulting matrix entries are real numbers.", "answer": "```python\nimport numpy as np\n\ndef _create_householder_vector(x):\n    \"\"\"\n    Creates a Householder vector u from a given vector x.\n    The reflector P = I - 2*u*u.T transforms x into [alpha, 0, ..., 0].T.\n    Returns the vector u and the scalar alpha.\n    \"\"\"\n    if len(x) == 0:\n        return np.array([]), 0.0\n    \n    # Use copysign for numerical stability\n    alpha = np.linalg.norm(x)\n    signed_alpha = np.copysign(alpha, x[0])\n    \n    u = x.copy()\n    u[0] += signed_alpha\n    \n    norm_u = np.linalg.norm(u)\n    if norm_u < 1e-15:  # Vector is already close to zero or e_1 aligned\n        return np.zeros_like(u), -signed_alpha\n        \n    u /= norm_u\n    return u, -signed_alpha\n\ndef francis_double_shift_qr_step(H):\n    \"\"\"\n    Performs a single implicit double-shift QR step on a real upper Hessenberg matrix H.\n    \"\"\"\n    n = H.shape[0]\n    if n < 3:\n        return H.copy()\n\n    H_new = H.copy()\n\n    # 1. Calculate shift parameters s (trace) and p (determinant)\n    # from the trailing 2x2 block.\n    s = H_new[n-2, n-2] + H_new[n-1, n-1]\n    p = H_new[n-2, n-2] * H_new[n-1, n-1] - H_new[n-2, n-1] * H_new[n-1, n-2]\n\n    # 2. Compute the first column of (H^2 - sH + pI).\n    # This vector determines the first Householder transformation.\n    # Only the first 3 components are non-zero for a Hessenberg matrix.\n    h00, h10, h01 = H_new[0, 0], H_new[1, 0], H_new[0, 1]\n    h11, h21 = H_new[1, 1], H_new[2, 1]\n\n    v0 = h00 * (h00 - s) + h01 * h10 + p\n    v1 = h10 * (h00 + h11 - s)\n    v2 = h10 * h21\n    \n    v = np.array([v0, v1, v2])\n\n    # 3. Bulge-chasing loop\n    for k in range(n - 2):\n        # Determine the size of the reflector and the part of H to operate on.\n        # Window size is 3, but shrinks near the end of the matrix.\n        m = min(k + 3, n)\n        \n        # Get the vector for the Householder reflector.\n        # For k=0, it's the initial vector v calculated above.\n        # For k>0, it's part of the k-1 column of the current matrix,\n        # which contains the bulge from the previous step.\n        if k == 0:\n            sub_vector = v\n        else:\n            sub_vector = H_new[k:m, k-1]\n\n        # Create Householder reflector P = I - 2*u*u.T\n        if np.linalg.norm(sub_vector) < 1e-15:\n            continue\n        \n        u, _ = _create_householder_vector(sub_vector)\n        \n        if np.linalg.norm(u) < 1e-15:\n            continue\n\n        # Apply the transformation P from the left: H_new <- P H_new\n        # This acts on rows k to m-1.\n        sub_matrix_left = H_new[k:m, k:]\n        tau_left = 2 * (u.T @ sub_matrix_left)\n        H_new[k:m, k:] -= np.outer(u, tau_left)\n\n        # Apply the transformation P from the right: H_new <- H_new P\n        # This acts on columns k to m-1.\n        sub_matrix_right = H_new[:, k:m]\n        tau_right = 2 * (sub_matrix_right @ u)\n        H_new[:, k:m] -= np.outer(tau_right, u)\n\n    return H_new\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [1.0, 2.0, 0.0, 0.0],\n            [3.0, 4.0, 5.0, 0.0],\n            [0.0, 6.0, 0.5, 2.0],\n            [0.0, 0.0, -3.0, 0.5]\n        ]),\n        np.array([\n            [2.0, -1.0, 4.0, 0.0, 0.0],\n            [5.0, 3.0, -2.0, 1.0, 0.0],\n            [0.0, -3.0, 1.0, 2.0, 4.0],\n            [0.0, 0.0, 1.5, 0.0, 2.0],\n            [0.0, 0.0, 0.0, -2.5, 0.0]\n        ]),\n        np.array([\n            [0.0, 2.0, 1.0],\n            [-10.0, 0.0, 3.0],\n            [0.0, -4.0, 1.0]\n        ]),\n        np.array([\n            [0.0, 5.0],\n            [-4.0, 0.0]\n        ])\n    ]\n\n    results = []\n    \n    for H_orig in test_cases:\n        n = H_orig.shape[0]\n        \n        # Run one step of the algorithm\n        H_raw = francis_double_shift_qr_step(H_orig)\n\n        # Create a cleaned version for the Hessenberg structure check\n        H_clean = H_raw.copy()\n        for i in range(n):\n            for j in range(n):\n                if i > j + 1:\n                    # Tolerance from problem description\n                    tol_clean = 1e-12 * (np.abs(H_clean[i-1, i-1]) + np.abs(H_clean[i, i]))\n                    if np.abs(H_clean[i, j]) < tol_clean:\n                        H_clean[i, j] = 0.0\n        \n        # --- Verification Metrics ---\n        # b1: Upper Hessenberg structure preserved\n        b1 = True\n        for i in range(n):\n            for j in range(n):\n                if i > j + 1 and H_clean[i, j] != 0.0:\n                    b1 = False\n                    break\n            if not b1:\n                break\n\n        # b2: Trace preserved (use raw matrix)\n        trace_orig = np.trace(H_orig)\n        trace_new = np.trace(H_raw)\n        b2 = np.isclose(trace_orig, trace_new, rtol=1e-10, atol=1e-12)\n\n        # b3: Frobenius norm preserved (use raw matrix)\n        norm_orig = np.linalg.norm(H_orig, 'fro')\n        norm_new = np.linalg.norm(H_raw, 'fro')\n        b3 = np.isclose(norm_orig, norm_new, rtol=1e-12, atol=1e-14)\n\n        # b4: Real arithmetic respected\n        b4 = not np.iscomplexobj(H_raw)\n\n        results.extend([b1, b2, b3, b4])\n    \n    # Format the final output as a comma-separated list of booleans\n    # Python's str(True) is 'True', which is a standard representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    output_str = output_str.replace(\"True\", \"true\").replace(\"False\", \"false\") # Match lowercase example\n    print(output_str)\n\nsolve()\n```", "id": "2431491"}]}