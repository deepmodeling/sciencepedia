{"hands_on_practices": [{"introduction": "Before diving into complex implementations, it is crucial to build a strong intuition for the mechanics of iterative methods. This first exercise provides a conceptual \"pen-and-paper\" opportunity to analyze the behavior of the power method on a special and highly intuitive matrixâ€”an orthogonal projector [@problem_id:2427094]. By examining a single iteration, you will gain a clear, fundamental insight into how and why the algorithm isolates the dominant eigenvector, revealing the core principle of the method in its simplest form.", "problem": "Let $v \\in \\mathbb{R}^{n}$ be a nonzero vector and define the orthogonal projector onto the one-dimensional subspace spanned by $v$ by $P = \\dfrac{v v^{T}}{v^{T} v} \\in \\mathbb{R}^{n \\times n}$. Consider the power iteration with normalization applied to $P$, given an initial vector $x_{0} \\in \\mathbb{R}^{n}$ satisfying $v^{T} x_{0} \\neq 0$. One iteration produces $y_{1} = P x_{0}$ and the normalized iterate $x_{1} = \\dfrac{y_{1}}{\\|y_{1}\\|_{2}}$. Define the Rayleigh quotient of $x_{1}$ with respect to $P$ by $r_{1} = \\dfrac{x_{1}^{T} P x_{1}}{x_{1}^{T} x_{1}}$. Determine the exact value of $r_{1}$. The final answer must be a single real number. No rounding is required.", "solution": "The problem requires the calculation of the Rayleigh quotient $r_{1}$ for the first iterate of the power method applied to an orthogonal projector matrix $P$.\n\nFirst, we must validate the problem statement.\nThe givens are:\n- A nonzero vector $v \\in \\mathbb{R}^{n}$.\n- The orthogonal projector matrix $P = \\dfrac{v v^{T}}{v^{T} v} \\in \\mathbb{R}^{n \\times n}$.\n- An initial vector $x_{0} \\in \\mathbb{R}^{n}$ with the property $v^{T} x_{0} \\neq 0$.\n- The first unnormalized iterate is $y_{1} = P x_{0}$.\n- The first normalized iterate is $x_{1} = \\dfrac{y_{1}}{\\|y_{1}\\|_{2}}$.\n- The Rayleigh quotient is $r_{1} = \\dfrac{x_{1}^{T} P x_{1}}{x_{1}^{T} x_{1}}$.\n\nThe problem is scientifically grounded in the principles of linear algebra and numerical analysis. The definitions of the orthogonal projector, power iteration, and Rayleigh quotient are standard. The problem is well-posed; the condition $v^{T} x_{0} \\neq 0$ ensures that the first iterate $y_{1}$ is not the zero vector, so its normalization $x_{1}$ is well-defined. The problem is objective and contains all necessary information for a unique solution. Therefore, the problem is deemed valid and a solution will be constructed.\n\nThe objective is to compute $r_{1}$. We begin by analyzing the structure of the first iterate $x_{1}$.\nThe unnormalized iterate $y_{1}$ is given by:\n$$y_{1} = P x_{0} = \\left(\\dfrac{v v^{T}}{v^{T} v}\\right) x_{0} = \\dfrac{v (v^{T} x_{0})}{v^{T} v}$$\nLet us define the scalar $\\alpha = v^{T} x_{0}$. By the problem statement, $\\alpha \\neq 0$. Also, let us denote the scalar $v^{T} v = \\|v\\|_{2}^{2}$. Since $v$ is a nonzero vector, $v^{T} v > 0$.\nWith these definitions, the expression for $y_{1}$ simplifies to:\n$$y_{1} = \\dfrac{\\alpha}{v^{T} v} v$$\nThis expression shows that $y_{1}$ is a non-zero scalar multiple of the vector $v$.\n\nNext, we normalize $y_{1}$ to obtain $x_{1}$. The normalization is with respect to the Euclidean norm, $\\| \\cdot \\|_{2}$.\nThe norm of $y_{1}$ is:\n$$\\|y_{1}\\|_{2} = \\left\\| \\dfrac{\\alpha}{v^{T} v} v \\right\\|_{2} = \\left| \\dfrac{\\alpha}{v^{T} v} \\right| \\|v\\|_{2} = \\dfrac{|\\alpha|}{v^{T} v} \\sqrt{v^{T} v} = \\dfrac{|\\alpha|}{\\sqrt{v^{T} v}}$$\nNow, we compute $x_{1}$:\n$$x_{1} = \\dfrac{y_{1}}{\\|y_{1}\\|_{2}} = \\dfrac{\\frac{\\alpha}{v^{T} v} v}{\\frac{|\\alpha|}{\\sqrt{v^{T} v}}} = \\dfrac{\\alpha}{|\\alpha|} \\dfrac{\\sqrt{v^{T} v}}{v^{T} v} v = \\dfrac{\\alpha}{|\\alpha|} \\dfrac{1}{\\sqrt{v^{T} v}} v$$\nRecognizing that $\\sqrt{v^{T} v} = \\|v\\|_{2}$, we can write $x_{1}$ as:\n$$x_{1} = \\text{sgn}(\\alpha) \\dfrac{v}{\\|v\\|_{2}}$$\nwhere $\\text{sgn}(\\alpha) = \\frac{\\alpha}{|\\alpha|}$ is the sign of $\\alpha$. This shows that $x_{1}$ is a unit vector pointing in the same or opposite direction as $v$. In other words, $x_{1}$ lies in the one-dimensional subspace spanned by $v$.\n\nNow we must evaluate the Rayleigh quotient $r_{1} = \\dfrac{x_{1}^{T} P x_{1}}{x_{1}^{T} x_{1}}$.\nBy definition of a normalized vector, the denominator is $x_{1}^{T} x_{1} = \\|x_{1}\\|_{2}^{2} = 1$.\nSo, the Rayleigh quotient simplifies to $r_{1} = x_{1}^{T} P x_{1}$.\n\nTo compute this, we first evaluate the action of $P$ on $x_{1}$.\n$$P x_{1} = P \\left( \\text{sgn}(\\alpha) \\dfrac{v}{\\|v\\|_{2}} \\right) = \\text{sgn}(\\alpha) \\dfrac{1}{\\|v\\|_{2}} (P v)$$\nLet us compute $P v$:\n$$P v = \\left(\\dfrac{v v^{T}}{v^{T} v}\\right) v = \\dfrac{v (v^{T} v)}{v^{T} v} = v$$\nThis confirms that $v$ is an eigenvector of the projector $P$ with a corresponding eigenvalue of $\\lambda = 1$.\nSubstituting this result back into the expression for $P x_{1}$:\n$$P x_{1} = \\text{sgn}(\\alpha) \\dfrac{1}{\\|v\\|_{2}} (v) = x_{1}$$\nThis shows that $x_{1}$ is also an eigenvector of $P$ corresponding to the eigenvalue $\\lambda = 1$. This is expected, as the power iteration has converged in a single step to an eigenvector associated with the dominant eigenvalue of $P$, which is $1$. The eigenvalues of $P$ are $1$ (with eigenvector $v$) and $0$ (with eigenspace being the orthogonal complement of the span of $v$).\n\nFinally, we calculate $r_{1}$:\n$$r_{1} = x_{1}^{T} (P x_{1}) = x_{1}^{T} x_{1}$$\nSince $x_{1}$ is a unit vector, $x_{1}^{T} x_{1} = 1$.\nTherefore, the value of the Rayleigh quotient is:\n$$r_{1} = 1$$\nThis result is independent of the choice of initial vector $x_{0}$, as long as $x_{0}$ has a non-zero component in the direction of $v$.", "answer": "$$\\boxed{1}$$", "id": "2427094"}, {"introduction": "With a solid conceptual foundation, we now move to a practical application central to computational physics and engineering: finding the extremal modes of a physical system. This practice involves implementing both the standard power iteration and the inverse power iteration to find the largest and smallest eigenvalues of a matrix representing the discrete two-dimensional Laplacian operator [@problem_id:2428639]. Completing this exercise will not only solidify your coding skills but also bridge the gap between abstract numerical algorithms and their tangible application in modeling physical phenomena like heat diffusion or quantum mechanics.", "problem": "You are given a family of real, symmetric, positive-definite matrices that arise from the finite-difference discretization of the negative two-dimensional Laplacian with homogeneous Dirichlet boundary conditions on a rectangle. For integers $n_x \\ge 1$ and $n_y \\ge 1$, and positive real side lengths $L_x > 0$ and $L_y > 0$, define the uniform grid spacings $h_x = L_x/(n_x+1)$ and $h_y = L_y/(n_y+1)$. Let $T_x \\in \\mathbb{R}^{n_x \\times n_x}$ and $T_y \\in \\mathbb{R}^{n_y \\times n_y}$ be the tridiagonal matrices\n$$\nT_x = \\frac{1}{h_x^2}\\,\\mathrm{tridiag}(-1,\\,2,\\,-1),\\qquad\nT_y = \\frac{1}{h_y^2}\\,\\mathrm{tridiag}(-1,\\,2,\\,-1),\n$$\nand let $I_x \\in \\mathbb{R}^{n_x \\times n_x}$ and $I_y \\in \\mathbb{R}^{n_y \\times n_y}$ be identity matrices. The discrete operator matrix $A \\in \\mathbb{R}^{(n_x n_y) \\times (n_x n_y)}$ is defined by the Kronecker sum\n$$\nA = I_y \\otimes T_x \\;+\\; T_y \\otimes I_x.\n$$\nThis represents the standard five-point stencil for the negative Laplacian on interior grid points with homogeneous Dirichlet boundary conditions on the boundary of the rectangle $[0,L_x]\\times[0,L_y]$.\n\nLet $n = n_x n_y$ and let $x_0 \\in \\mathbb{R}^{n}$ be the vector whose entries are all equal to $1$. Consider the following two sequences of unit vectors constructed from $A$ and $x_0$:\n\n1. Define $u_0 = x_0/\\|x_0\\|_2$ and, for an integer $t_{\\mathrm{up}} \\ge 1$, define $u_k$ recursively for $k=0,1,\\dots,t_{\\mathrm{up}}-1$ by\n$$\n\\tilde{u}_{k+1} = A u_k,\\qquad u_{k+1} = \\frac{\\tilde{u}_{k+1}}{\\|\\tilde{u}_{k+1}\\|_2}.\n$$\nLet $\\lambda_{\\mathrm{up}}$ be the Rayleigh quotient\n$$\n\\lambda_{\\mathrm{up}} = \\frac{u_{t_{\\mathrm{up}}}^\\top A u_{t_{\\mathrm{up}}}}{u_{t_{\\mathrm{up}}}^\\top u_{t_{\\mathrm{up}}}}.\n$$\n\n2. Define $w_0 = x_0/\\|x_0\\|_2$ and, for an integer $t_{\\mathrm{low}} \\ge 1$, define $w_k$ recursively for $k=0,1,\\dots,t_{\\mathrm{low}}-1$ by\n$$\n\\tilde{w}_{k+1} \\text{ is the unique solution of } A \\tilde{w}_{k+1} = w_k,\\qquad w_{k+1} = \\frac{\\tilde{w}_{k+1}}{\\|\\tilde{w}_{k+1}\\|_2}.\n$$\nLet $\\lambda_{\\mathrm{low}}$ be the Rayleigh quotient\n$$\n\\lambda_{\\mathrm{low}} = \\frac{w_{t_{\\mathrm{low}}}^\\top A w_{t_{\\mathrm{low}}}}{w_{t_{\\mathrm{low}}}^\\top w_{t_{\\mathrm{low}}}}.\n$$\n\nYour task is to write a program that, for each test case below, constructs $A$ exactly as defined, forms the two sequences above from the same initial vector $x_0$ and the specified iteration counts, and outputs the two scalars $\\lambda_{\\mathrm{up}}$ and $\\lambda_{\\mathrm{low}}$ computed as specified. All computations are purely numerical and dimensionless; no physical units are required. Angles, if any are introduced internally by your method, shall be in radians, though angles are not required by the problem.\n\nTest suite (each tuple is $(n_x,n_y,L_x,L_y,t_{\\mathrm{up}},t_{\\mathrm{low}})$):\n- Case 1: $(20,\\,20,\\,1.0,\\,1.0,\\,80,\\,80)$\n- Case 2: $(1,\\,1,\\,1.0,\\,1.0,\\,10,\\,10)$\n- Case 3: $(12,\\,8,\\,2.0,\\,1.0,\\,100,\\,100)$\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a two-element list $[\\lambda_{\\mathrm{up}},\\lambda_{\\mathrm{low}}]$ with both values rounded to exactly six digits after the decimal point. The overall output shall therefore be a list of three two-element lists in the order of the cases above, for example:\n$[[\\ell_{1,\\mathrm{up}},\\ell_{1,\\mathrm{low}}],[\\ell_{2,\\mathrm{up}},\\ell_{2,\\mathrm{low}}],[\\ell_{3,\\mathrm{up}},\\ell_{3,\\mathrm{low}}]]$ where each $\\ell$ is a decimal representation rounded to six digits after the decimal point.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded, well-posed, and provides all necessary information for a unique, verifiable solution. The problem requires the implementation of two fundamental numerical algorithms in linear algebraâ€”the power iteration and the inverse iteration methodsâ€”to find the extremal eigenvalues of a matrix representing a discrete physical operator.\n\nThe matrix $A$ represents the five-point finite-difference discretization of the negative two-dimensional Laplacian operator, $-\\nabla^2 = -(\\partial^2/\\partial x^2 + \\partial^2/\\partial y^2)$, on a rectangular domain $[0, L_x] \\times [0, L_y]$ with homogeneous Dirichlet boundary conditions. The domain is discretized into a grid of $(n_x+2) \\times (n_y+2)$ points, with $n_x n_y$ interior points. The matrix $A$ acts on a vector of function values at these interior points.\n\nThe construction of $A$ as a Kronecker sum, $A = I_y \\otimes T_x + T_y \\otimes I_x$, is the standard mathematical formulation for separable two-dimensional operators. The matrices $T_x$ and $T_y$ are scaled discretizations of the one-dimensional second derivative operator, $-d^2/dx^2$, on their respective axes. Specifically, the matrix $\\mathrm{tridiag}(-1, 2, -1)$ is a scaled version of the discrete one-dimensional Laplacian. As the sum of symmetric positive-definite matrices (since $T_x$ and $T_y$ are scaled diagonally dominant M-matrices), $A$ is guaranteed to be symmetric and positive-definite, possessing real, positive eigenvalues.\n\nThe first sequence for computing $\\lambda_{\\mathrm{up}}$ is the Power Iteration method. This iterative algorithm repeatedly applies a matrix $A$ to a vector. For a given starting vector $u_0$ that has a non-zero component in the direction of the eigenvector corresponding to the dominant eigenvalue (the eigenvalue with the largest magnitude), the sequence $u_k = A^k u_0 / \\|A^k u_0\\|_2$ converges to this eigenvector. For a positive-definite matrix $A$, the dominant eigenvalue is the largest eigenvalue, $\\lambda_{\\max}$. The core of the method is the recurrence relation $\\tilde{u}_{k+1} = A u_k$, followed by normalization $u_{k+1} = \\tilde{u}_{k+1} / \\|\\tilde{u}_{k+1}\\|_2$. After a sufficient number of iterations $t_{\\mathrm{up}}$, the vector $u_{t_{\\mathrm{up}}}$ is a good approximation of the corresponding eigenvector.\n\nThe second sequence for computing $\\lambda_{\\mathrm{low}}$ is the Inverse Power Iteration method. This is mathematically equivalent to applying the power method to the inverse matrix $A^{-1}$. The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$. Therefore, the largest eigenvalue of $A^{-1}$ corresponds to the smallest eigenvalue of $A$. The iteration step requires solving the linear system $A \\tilde{w}_{k+1} = w_k$ for $\\tilde{w}_{k+1}$, which is computationally more efficient than explicitly computing the inverse $A^{-1}$ and performing a matrix-vector multiplication. The subsequent normalization $w_{k+1} = \\tilde{w}_{k+1} / \\|\\tilde{w}_{k+1}\\|_2$ completes the iteration. After $t_{\\mathrm{low}}$ iterations, the vector $w_{t_{\\mathrm{low}}}$ converges to the eigenvector associated with the smallest eigenvalue of $A$, $\\lambda_{\\min}$.\n\nFinally, the Rayleigh quotient, $R_A(v) = (v^\\top A v) / (v^\\top v)$, provides an estimate for an eigenvalue of $A$ given an approximate eigenvector $v$. For a normalized vector $v$ (i.e., $v^\\top v = \\|v\\|_2^2 = 1$), this simplifies to $R_A(v) = v^\\top A v$. The problem correctly defines $\\lambda_{\\mathrm{up}}$ and $\\lambda_{\\mathrm{low}}$ as the Rayleigh quotients of the final iterated vectors $u_{t_{\\mathrm{up}}}$ and $w_{t_{\\mathrm{low}}}$, respectively. These will be highly accurate approximations of $\\lambda_{\\max}$ and $\\lambda_{\\min}$.\n\nThe algorithmic procedure is as follows:\n$1$. For each test case, define the parameters $n_x$, $n_y$, $L_x$, $L_y$, $t_{\\mathrm{up}}$, and $t_{\\mathrm{low}}$.\n$2$. Compute the grid spacings $h_x = L_x / (n_x+1)$ and $h_y = L_y / (n_y+1)$.\n$3$. Construct the one-dimensional operator matrices $T_x \\in \\mathbb{R}^{n_x \\times n_x}$ and $T_y \\in \\mathbb{R}^{n_y \\times n_y}$ as specified.\n$4$. Construct the full two-dimensional operator matrix $A \\in \\mathbb{R}^{(n_x n_y) \\times (n_x n_y)}$ using the Kronecker sum $A = I_y \\otimes T_x + T_y \\otimes I_x$.\n$5$. To compute $\\lambda_{\\mathrm{up}}$: initialize a unit vector $u_0$, and iterate $t_{\\mathrm{up}}$ times using the power method recursion $u_{k+1} \\propto A u_k$. Then compute the Rayleigh quotient $\\lambda_{\\mathrm{up}} = u_{t_{\\mathrm{up}}}^\\top A u_{t_{\\mathrm{up}}}$.\n$6$. To compute $\\lambda_{\\mathrm{low}}$: initialize a unit vector $w_0$, and iterate $t_{\\mathrm{low}}$ times by solving $A \\tilde{w}_{k+1} = w_k$ and normalizing $w_{k+1} = \\tilde{w}_{k+1} / \\|\\tilde{w}_{k+1}\\|_2$. Then compute the Rayleigh quotient $\\lambda_{\\mathrm{low}} = w_{t_{\\mathrm{low}}}^\\top A w_{t_{\\mathrm{low}}}$.\n$7$. The results are collected and formatted as required.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as sp_solve\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases by constructing the discrete\n    Laplacian matrix and applying power and inverse power iterations to find\n    approximations of the largest and smallest eigenvalues.\n    \"\"\"\n    test_cases = [\n        (20, 20, 1.0, 1.0, 80, 80),\n        (1, 1, 1.0, 1.0, 10, 10),\n        (12, 8, 2.0, 1.0, 100, 100),\n    ]\n\n    results = []\n    for params in test_cases:\n        nx, ny, Lx, Ly, t_up, t_low = params\n\n        # Grid spacings\n        hx = Lx / (nx + 1)\n        hy = Ly / (ny + 1)\n\n        # Construct 1D tridiagonal matrices\n        diag_x = 2 * np.ones(nx)\n        offdiag_x = -1 * np.ones(nx - 1)\n        Tx = (1 / hx**2) * (np.diag(diag_x) + np.diag(offdiag_x, k=1) + np.diag(offdiag_x, k=-1))\n\n        diag_y = 2 * np.ones(ny)\n        offdiag_y = -1 * np.ones(ny - 1)\n        Ty = (1 / hy**2) * (np.diag(diag_y) + np.diag(offdiag_y, k=1) + np.diag(offdiag_y, k=-1))\n\n        # Identity matrices\n        Ix = np.eye(nx)\n        Iy = np.eye(ny)\n\n        # Construct 2D operator matrix A using Kronecker sum\n        A = np.kron(Iy, Tx) + np.kron(Ty, Ix)\n\n        # Dimension of the system\n        n = nx * ny\n\n        # Initial vector of all ones\n        x0 = np.ones(n)\n\n        # --- Power Iteration for lambda_up ---\n        u = x0 / np.linalg.norm(x0)\n        for _ in range(t_up):\n            u_tilde = A @ u\n            u = u_tilde / np.linalg.norm(u_tilde)\n        \n        # Rayleigh quotient for lambda_up\n        # Since u is a unit vector, u.T @ u = 1\n        lambda_up = u.T @ (A @ u)\n\n        # --- Inverse Iteration for lambda_low ---\n        w = x0 / np.linalg.norm(x0)\n        for _ in range(t_low):\n            # Solve A * w_tilde = w\n            w_tilde = sp_solve(A, w, assume_a='pos')\n            w = w_tilde / np.linalg.norm(w_tilde)\n            \n        # Rayleigh quotient for lambda_low\n        # Since w is a unit vector, w.T @ w = 1\n        lambda_low = w.T @ (A @ w)\n        \n        results.append([lambda_up, lambda_low])\n\n    # Format output\n    results_str = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "2428639"}, {"introduction": "While the basic power and inverse iteration methods are effective, their efficiency can be dramatically enhanced, especially when high precision is required. This final hands-on practice challenges you to implement and compare the convergence performance of three distinct schemes: power iteration, inverse iteration with a fixed shift, and the powerful Rayleigh Quotient Iteration (RQI) with its adaptive shift [@problem_id:2427128]. By observing their behavior on carefully chosen test cases, you will gain a practical appreciation for algorithm selection and witness firsthand the remarkable speed of RQI's cubic convergence.", "problem": "Implement an algorithm to approximate eigenpairs of a real symmetric matrix using inverse iteration with a variable shift given by the Rayleigh quotient. Let the Rayleigh quotient for a nonzero vector $x \\in \\mathbb{R}^n$ be defined as $R(x) = \\dfrac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$. Consider the following three iterative schemes applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with a given nonzero initial vector $x_0 \\in \\mathbb{R}^n$ and tolerance $\\varepsilon > 0$:\n\n- Power iteration: $x_{k+1} \\leftarrow \\dfrac{A x_k}{\\lVert A x_k \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a fixed shift: with $\\sigma_0 = R(x_0)$ fixed, compute $x_{k+1}$ by solving $(A - \\sigma_0 I) y = x_k$ and setting $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a variable shift equal to the Rayleigh quotient (Rayleigh quotient iteration): at each iteration, compute $\\sigma_k = R(x_k)$, solve $(A - \\sigma_k I) y = x_k$, and set $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n\nFor each scheme, stop when the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$ or when a prescribed maximum number of iterations is reached. All vector norms are the Euclidean norm, and $I$ denotes the identity matrix of size $n$.\n\nUse the following test suite. In every case, set $n = 5$, the tolerance $\\varepsilon = 10^{-10}$, and the maximum number of iterations to $1000$.\n\n- Test case $\\#1$ (tri-diagonal symmetric positive definite matrix):\n  - Matrix $A_1 \\in \\mathbb{R}^{5 \\times 5}$:\n    $$\n    A_1 =\n    \\begin{bmatrix}\n    6 & 2 & 0 & 0 & 0 \\\\\n    2 & 5 & 2 & 0 & 0 \\\\\n    0 & 2 & 4 & 2 & 0 \\\\\n    0 & 0 & 2 & 3 & 2 \\\\\n    0 & 0 & 0 & 2 & 2\n    \\end{bmatrix}.\n    $$\n  - Initial vector $x_0^{(1)} = \\dfrac{1}{\\sqrt{5}} [1, 1, 1, 1, 1]^\\mathsf{T}$.\n\n- Test case $\\#2$ (symmetric matrix with two very close eigenvalues):\n  - Define diagonal matrix $D = \\mathrm{diag}(1, 1 + 10^{-6}, 2, 3, 4)$.\n  - Define the planar rotation with angle $\\theta$ such that $\\cos \\theta = \\dfrac{4}{5}$ and $\\sin \\theta = \\dfrac{3}{5}$, and set\n    $$\n    Q = \\begin{bmatrix}\n    \\cos \\theta & -\\sin \\theta & 0 & 0 & 0 \\\\\n    \\sin \\theta & \\phantom{-}\\cos \\theta & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 1\n    \\end{bmatrix}.\n    $$\n  - Matrix $A_2 = Q^\\mathsf{T} D Q$.\n  - Initial vector $x_0^{(2)} = [1, 0, 0, 0, 0]^\\mathsf{T}$.\n\n- Test case $\\#3$ (Hilbert matrix):\n  - Matrix $A_3 \\in \\mathbb{R}^{5 \\times 5}$ with entries $(A_3)_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, 3, 4, 5\\}$.\n  - Initial vector $x_0^{(3)} = \\dfrac{1}{\\sqrt{5}} [1, -1, 1, -1, 1]^\\mathsf{T}$.\n\nFor each test case, run the three schemes independently with the same $A$ and $x_0$ and record the smallest iteration count $k$ at which the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$. If convergence does not occur within the maximum number of iterations, record the maximum number of iterations.\n\nYour program must output a single line containing a comma-separated list of $9$ integers enclosed in square brackets, in the following order:\n$[k_{\\mathrm{RQI}}^{(1)}, k_{\\mathrm{fixed}}^{(1)}, k_{\\mathrm{power}}^{(1)}, k_{\\mathrm{RQI}}^{(2)}, k_{\\mathrm{fixed}}^{(2)}, k_{\\mathrm{power}}^{(2)}, k_{\\mathrm{RQI}}^{(3)}, k_{\\mathrm{fixed}}^{(3)}, k_{\\mathrm{power}}^{(3)}]$, where $k_{\\mathrm{RQI}}^{(i)}$ is the iteration count for Rayleigh quotient iteration on test case $i$, $k_{\\mathrm{fixed}}^{(i)}$ is the iteration count for inverse iteration with a fixed shift $\\sigma_0 = R(x_0^{(i)})$, and $k_{\\mathrm{power}}^{(i)}$ is the iteration count for power iteration. The output must be exactly one line in this format, with no additional characters or whitespace beyond those structurally required by the list representation.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically iterative methods for eigenvalue problems. The problem is well-posed, with all necessary parameters, matrices, initial conditions, and stopping criteria explicitly and unambiguously defined. The language is objective and formal. Therefore, a solution will be provided.\n\nThe problem requires the implementation and comparison of three iterative algorithms for approximating an eigenpair $(\\lambda, v)$ of a real symmetric matrix $A$, where $A v = \\lambda v$. An eigenpair consists of an eigenvalue $\\lambda$ and its corresponding eigenvector $v$. The methods under consideration are power iteration, inverse iteration with a fixed shift, and inverse iteration with a variable shift, also known as Rayleigh quotient iteration (RQI). For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, all its eigenvalues are real, and there exists an orthonormal basis of eigenvectors. The Rayleigh quotient, defined for a nonzero vector $x \\in \\mathbb{R}^n$ as $R(x) = \\frac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$, provides an estimate for an eigenvalue. If $x$ is an eigenvector, then $R(x)$ is the corresponding exact eigenvalue. For all algorithms, we start with an initial vector $x_0$ and generate a sequence of vectors $\\{x_k\\}$ that converges to an eigenvector, and a sequence of Rayleigh quotients $\\{\\lambda_k = R(x_k)\\}$ that converges to the corresponding eigenvalue.\n\n1.  **Power Iteration**\n\n    The power iteration method is the simplest algorithm for finding the dominant eigenpair of a matrix, i.e., the eigenpair $(\\lambda_1, v_1)$ where $|\\lambda_1|$ is the largest magnitude among all eigenvalues. The iterative step is defined as:\n    $$\n    x_{k+1} = \\frac{A x_k}{\\lVert A x_k \\rVert_2}\n    $$\n    Starting with an initial vector $x_0$ that has a non-zero component in the direction of the dominant eigenvector $v_1$, the sequence $x_k$ converges to $v_1$. The convergence is linear, with a rate determined by the ratio $|\\lambda_2 / \\lambda_1|$, where $\\lambda_2$ is the eigenvalue with the second-largest magnitude. If this ratio is close to $1$, convergence can be very slow. The eigenvalue is approximated at each step by the Rayleigh quotient, $\\lambda_k = R(x_k)$.\n\n2.  **Inverse Iteration with Fixed Shift**\n\n    Inverse iteration is a method to find the eigenpair corresponding to the eigenvalue closest to a given shift $\\sigma$. It applies the power method to the matrix $(A - \\sigma I)^{-1}$. The eigenvalues of $(A - \\sigma I)^{-1}$ are $(\\lambda_i - \\sigma)^{-1}$, where $\\lambda_i$ are the eigenvalues of $A$. The dominant eigenvalue of $(A - \\sigma I)^{-1}$ corresponds to the smallest value of $|\\lambda_i - \\sigma|$, which means $\\lambda_i$ is the eigenvalue of $A$ closest to $\\sigma$. The iterative step is:\n    $$\n    x_{k+1} = \\frac{(A - \\sigma I)^{-1} x_k}{\\lVert (A - \\sigma I)^{-1} x_k \\rVert_2}\n    $$\n    In practice, computing the matrix inverse is avoided. Instead, we solve the linear system $(A - \\sigma I) y_k = x_k$ for $y_k$, and then normalize:\n    $$\n    x_{k+1} = \\frac{y_k}{\\lVert y_k \\rVert_2}\n    $$\n    In this problem, a fixed shift $\\sigma_0 = R(x_0)$ is used throughout the process. Convergence is linear, but the rate is determined by the ratio of the two eigenvalues of $(A-\\sigma_0 I)^{-1}$ with largest magnitude. If $\\sigma_0$ is much closer to one eigenvalue $\\lambda_j$ than to any other, convergence to the eigenvector $v_j$ is very rapid.\n\n3.  **Rayleigh Quotient Iteration (RQI)**\n\n    Rayleigh quotient iteration is a powerful refinement of inverse iteration where the shift is updated at each step using the best current estimate for the eigenvalue: the Rayleigh quotient. The iterative process is defined by:\n    1.  Compute the shift: $\\sigma_k = R(x_k) = \\frac{x_k^\\mathsf{T} A x_k}{x_k^\\mathsf{T} x_k}$.\n    2.  Solve for $y_{k+1}$: $(A - \\sigma_k I) y_{k+1} = x_k$.\n    3.  Normalize: $x_{k+1} = \\frac{y_{k+1}}{\\lVert y_{k+1} \\rVert_2}$.\n\n    For a symmetric matrix, RQI exhibits cubic convergence once the iterate $x_k$ is sufficiently close to an eigenvector. This means that the number of correct digits in the approximation roughly triples with each iteration, leading to extremely fast convergence.\n\n**Stopping Criterion**\n\nFor all three methods, the iteration terminates when the norm of the residual vector, $\\lVert A x_k - \\lambda_k x_k \\rVert_2$, falls below a specified tolerance $\\varepsilon$, where $\\lambda_k = R(x_k)$. This residual measures how well the current approximate pair $( \\lambda_k, x_k )$ satisfies the eigenvalue equation. The iteration count $k$ at which this condition is first met is the desired output. If the condition is not met within a maximum number of iterations, that maximum number is recorded.\n\nThe implementation will proceed by defining three functions, one for each algorithm. Each function will iteratively generate the vector sequence and check the stopping criterion at each step, returning the iteration count. These functions will then be applied to the three specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates a dominant eigenpair using Power Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    for k in range(1, max_iter + 1):\n        # Calculate x_k\n        v = A @ x\n        x_k = v / np.linalg.norm(v)\n\n        # Check residual for x_k\n        # Since x_k is normalized, its L2 norm squared is 1.\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm <= tol:\n            return k\n\n        # Prepare for the next iteration\n        x = x_k\n\n    return max_iter\n\ndef inverse_iteration_fixed_shift(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using inverse iteration with a fixed shift.\n    The shift is the Rayleigh quotient of the initial vector.\n    \"\"\"\n    # Normalize initial vector for stability, although given vectors are normalized.\n    # The problem specifies sigma0 = R(x0), where x0 is the given initial vector.\n    # Since all given x0 are unit norm, x0.T @ x0 = 1.\n    sigma0 = x0.T @ A @ x0\n    \n    try:\n        M = A - sigma0 * np.eye(A.shape[0])\n    except np.linalg.LinAlgError:\n        return max_iter # Fails if shift is an exact eigenvalue\n\n    x = x0 / np.linalg.norm(x0) # Start iteration with normalized vector\n\n    for k in range(1, max_iter + 1):\n        try:\n            # Solve (A - sigma0*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # Shift is an eigenvalue or matrix is numerically singular\n            return max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm <= tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef rayleigh_quotient_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using Rayleigh Quotient Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    \n    for k in range(1, max_iter + 1):\n        # Update shift at each step using the Rayleigh quotient of x_{k-1}\n        sigma = x.T @ A @ x\n\n        try:\n            M = A - sigma * np.eye(A.shape[0])\n            # Solve (A - sigma_{k-1}*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # If the shift is an eigenvalue, the previous iterate was the eigenvector.\n            # Its residual should be zero or very small.\n            # The loop condition will have caught it in the previous iteration.\n            # This indicates a numerical breakdown or an exact hit.\n            return k-1 if k > 1 else max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n        \n        if residual_norm <= tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    n = 5\n    tol = 1e-10\n    max_iter = 1000\n\n    # Test Case 1\n    A1 = np.array([\n        [6, 2, 0, 0, 0],\n        [2, 5, 2, 0, 0],\n        [0, 2, 4, 2, 0],\n        [0, 0, 2, 3, 2],\n        [0, 0, 0, 2, 2]\n    ], dtype=float)\n    x0_1 = np.ones(n) / np.sqrt(n)\n\n    # Test Case 2\n    D = np.diag([1.0, 1.0 + 1e-6, 2.0, 3.0, 4.0])\n    cos_theta = 4.0 / 5.0\n    sin_theta = 3.0 / 5.0\n    Q = np.eye(n)\n    Q[0, 0] = cos_theta\n    Q[0, 1] = -sin_theta\n    Q[1, 0] = sin_theta\n    Q[1, 1] = cos_theta\n    A2 = Q.T @ D @ Q\n    x0_2 = np.zeros(n)\n    x0_2[0] = 1.0\n\n    # Test Case 3\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (n, n), dtype=int)\n    x0_3 = np.array([1, -1, 1, -1, 1]) / np.sqrt(n)\n    \n    test_cases = [\n        (A1, x0_1),\n        (A2, x0_2),\n        (A3, x0_3)\n    ]\n\n    results = []\n    for A, x0 in test_cases:\n        k_rqi = rayleigh_quotient_iteration(A, x0, tol, max_iter)\n        k_fixed = inverse_iteration_fixed_shift(A, x0, tol, max_iter)\n        k_power = power_iteration(A, x0, tol, max_iter)\n        \n        results.extend([k_rqi, k_fixed, k_power])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2427128"}]}