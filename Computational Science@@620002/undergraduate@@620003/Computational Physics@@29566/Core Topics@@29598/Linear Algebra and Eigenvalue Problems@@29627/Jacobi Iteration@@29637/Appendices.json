{"hands_on_practices": [{"introduction": "Understanding the Jacobi method begins with its fundamental convergence property. This first exercise provides a hands-on exploration of the connection between the theoretical condition for convergence and the practical performance of the iteration. By calculating the spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix for several distinct linear systems, you will directly test the principle that the iteration converges if and only if $\\rho(T_J)  1$. This practice [@problem_id:2404665] will build your intuition by demonstrating scenarios of guaranteed convergence, slow convergence, and divergence, solidifying the core mechanism of the Jacobi method.", "problem": "You are given square linear systems of the form $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$. Define the diagonal matrix $D = \\mathrm{diag}(A)$, the linear operator $T_J = I - D^{-1} A$, and the vector $c = D^{-1} b$. Consider the fixed-point iteration $x^{(k+1)} = T_J x^{(k)} + c$ with initial vector $x^{(0)} = 0$. For each system below, compute the spectral radius $\\rho(T_J)$, and assess convergence behavior by comparing iterates to the exact solution $x^\\star$ defined by $A x^\\star = b$.\n\nFor each system, perform the following steps in purely mathematical terms:\n1. Compute $\\rho(T_J)$, where $\\rho(T_J) = \\max_i |\\lambda_i(T_J)|$ and $\\{\\lambda_i(T_J)\\}$ are the eigenvalues of $T_J$.\n2. If $\\rho(T_J)  1$, generate the sequence $\\{x^{(k)}\\}_{k=0}^{\\infty}$ by $x^{(k+1)} = T_J x^{(k)} + c$ until the first index $k_{\\min}$ such that the relative error $\\|x^{(k)} - x^\\star\\|_2 / \\|x^\\star\\|_2 \\le \\varepsilon$ is achieved, where $\\varepsilon = 10^{-8}$ is the tolerance and $\\|\\cdot\\|_2$ is the Euclidean norm. Report this minimal index $k_{\\min}$, the final relative error $\\|x^{(k_{\\min})} - x^\\star\\|_2 / \\|x^\\star\\|_2$, and a boolean that is `True` if the tolerance was met within at most $K_{\\max}$ iterations and `False` otherwise. Use $K_{\\max} = 50000$ as the iteration cap.\n3. If $\\rho(T_J) \\ge 1$, do not iterate. In this case, report the boolean `False`, set the iteration count to $0$, and report the relative error of the initial guess $x^{(0)}$, namely $\\|x^{(0)} - x^\\star\\|_2 / \\|x^\\star\\|_2$.\n4. In all cases, the exact solution $x^\\star$ is the unique vector satisfying $A x^\\star = b$. All quantities are dimensionless.\n\nTest suite:\n- Case $1$ (trivial scalar system): $A_1 = [\\,3\\,] \\in \\mathbb{R}^{1 \\times 1}$, $b_1 = [\\,1\\,] \\in \\mathbb{R}^{1}$.\n- Case $2$ (one-dimensional discrete Laplacian with Dirichlet boundaries, size $n$): for $n = 50$, $A_2 \\in \\mathbb{R}^{n \\times n}$ has $2$ on the diagonal and $-1$ on the first sub- and super-diagonals, and zeros elsewhere; $b_2 = \\mathbf{1} \\in \\mathbb{R}^{n}$ with all entries equal to $1$.\n- Case $3$ (strictly diagonally dominant tridiagonal, size $n$): for $n = 50$, $A_3 \\in \\mathbb{R}^{n \\times n}$ has $5$ on the diagonal and $-1$ on the first sub- and super-diagonals, and zeros elsewhere; $b_3 = \\mathbf{1} \\in \\mathbb{R}^{n}$ with all entries equal to $1$.\n- Case $4$ (non-convergent under this iteration): $A_4 = \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2}$, $b_4 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^{2}$.\n\nRequired final output format:\n- Your program must produce a single line of output that aggregates the results for the four cases in order as a list of lists. For each case, output the quadruple $[\\rho(T_J), k, \\text{rel\\_err}, \\text{converged}]$, where $\\rho(T_J)$ and $\\text{rel\\_err}$ are floats rounded to six decimal places, $k$ is an integer, and $\\text{converged}$ is a boolean with values exactly $\\mathrm{True}$ or $\\mathrm{False}$.\n- The single-line output must be of the form\n  `[[\\rho_1,k_1,e_1,\\mathrm{flag}_1],[\\rho_2,k_2,e_2,\\mathrm{flag}_2],[\\rho_3,k_3,e_3,\\mathrm{flag}_3],[\\rho_4,k_4,e_4,\\mathrm{flag}_4]]`\n  with no spaces anywhere in the line.", "solution": "The user has submitted a problem concerning the analysis of the Jacobi iteration method for solving square linear systems of the form $A x = b$. The task requires validating the problem statement and, if valid, providing a comprehensive solution that includes theoretical analysis and a numerical implementation.\n\n### Step 1: Extract Givens\nThe problem provides the following definitions and data:\n- A square linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$.\n- The diagonal matrix $D = \\mathrm{diag}(A)$.\n- The Jacobi iteration operator $T_J = I - D^{-1} A$.\n- The vector $c = D^{-1} b$.\n- The Jacobi fixed-point iteration is defined as $x^{(k+1)} = T_J x^{(k)} + c$.\n- The initial vector is $x^{(0)} = \\mathbf{0}$.\n- The spectral radius of $T_J$ is $\\rho(T_J) = \\max_i |\\lambda_i(T_J)|$, where $\\{\\lambda_i(T_J)\\}$ are the eigenvalues of $T_J$.\n- The exact solution is $x^\\star$ such that $A x^\\star = b$.\n- The convergence tolerance for the relative error is $\\varepsilon = 10^{-8}$.\n- The relative error is defined as $\\|x^{(k)} - x^\\star\\|_2 / \\|x^\\star\\|_2$, where $\\|\\cdot\\|_2$ is the Euclidean norm.\n- The maximum number of iterations is $K_{\\max} = 50000$.\n- All quantities are dimensionless.\n\nFour test cases are specified:\n1.  **Case 1**: $A_1 = [\\,3\\,]$, $b_1 = [\\,1\\,]$.\n2.  **Case 2**: $n = 50$, $A_2$ is the $1$D discrete Laplacian matrix (diagonal entries $2$, first off-diagonals $-1$), $b_2 = \\mathbf{1}$.\n3.  **Case 3**: $n = 50$, $A_3$ is a strictly diagonally dominant tridiagonal matrix (diagonal entries $5$, first off-diagonals $-1$), $b_3 = \\mathbf{1}$.\n4.  **Case 4**: $A_4 = \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix}$, $b_4 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\nThe required output for each case is a quadruple $[\\rho(T_J), k, \\text{rel\\_err}, \\text{converged}]$. This includes the spectral radius, the number of iterations, the final relative error, and a boolean indicating if convergence was achieved within $K_{\\max}$ iterations.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is based on the Jacobi method, a fundamental iterative algorithm in numerical linear algebra and computational physics. The concepts of spectral radius, convergence criteria for fixed-point iterations, and matrix eigenvalues are standard and well-established mathematical principles. The test cases, particularly the discrete Laplacian, are canonical examples in the field. The problem is scientifically sound.\n-   **Well-Posed**: The problem is clearly defined. For each case, the matrices $A$ are invertible, guaranteeing a unique exact solution $x^\\star$. The diagonal matrices $D$ are also invertible, so the Jacobi iteration operator $T_J$ is well-defined. The conditions for iterating and for terminating are unambiguous. A unique, meaningful result exists for each case.\n-   **Objective**: The problem is stated using precise, objective mathematical language. There are no subjective claims or ambiguities.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, non-formalizability, incompleteness, contradiction, or infeasibility. It is a standard, verifiable computational problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Solution\nThe Jacobi method is a fixed-point iteration for solving the linear system $A x = b$. The matrix $A$ is decomposed as $A = D + L + U$, where $D$ is the diagonal of $A$, and $L$ and $U$ are the strictly lower and upper triangular parts of $A$, respectively. The equation $A x = b$ can be written as $(D+L+U)x=b$, which is rearranged to $Dx = -(L+U)x+b$. Assuming $D$ is invertible, this yields the iteration scheme:\n$$x^{(k+1)} = -D^{-1}(L+U)x^{(k)} + D^{-1}b$$\nThe iteration matrix is $T_J = -D^{-1}(L+U)$. Since $A = D+L+U$, we can write $L+U = A-D$, so $T_J = -D^{-1}(A-D) = -D^{-1}A + D^{-1}D = I - D^{-1}A$. The vector is $c = D^{-1}b$. The iteration is thus $x^{(k+1)} = T_J x^{(k)} + c$.\n\nA fundamental theorem of iterative methods states that this fixed-point iteration converges for any initial guess $x^{(0)}$ if and only if the spectral radius of the iteration matrix $T_J$ is less than $1$, i.e., $\\rho(T_J)  1$.\n\nThe overall procedure is as follows:\n1.  For each system $(A, b)$, construct the iteration matrix $T_J = I - D^{-1}A$.\n2.  Compute the eigenvalues of $T_J$ and determine the spectral radius $\\rho(T_J)$.\n3.  Calculate the exact solution $x^\\star = A^{-1}b$.\n4.  If $\\rho(T_J) \\ge 1$, the iteration diverges. We report the spectral radius, $k=0$ iterations, the initial relative error for $x^{(0)}=\\mathbf{0}$ (which is $\\|-\\|x^\\star\\|_2/\\|x^\\star\\|_2 = 1$ for $x^\\star \\neq \\mathbf{0}$), and a `False` convergence flag.\n5.  If $\\rho(T_J)  1$, the iteration converges. We start with $x^{(0)} = \\mathbf{0}$ and generate the sequence $x^{(k+1)} = T_J x^{(k)} + c$ for $k=0, 1, 2, \\dots$. At each step $k$, we compute the relative error $e_k = \\|x^{(k)} - x^\\star\\|_2 / \\|x^\\star\\|_2$. The process stops at the first iteration $k_{min}$ where $e_{k_{min}} \\le 10^{-8}$. If this does not occur by $k=K_{max}$, the process is terminated and reported as non-converged within the specified limit.\n\n**Case-by-case analysis:**\n\n**Case 1:** $A_1 = [\\,3\\,], b_1 = [\\,1\\,]$.\n- $D_1 = [\\,3\\,]$, $D_1^{-1} = [\\,1/3\\,]$.\n- $T_J = I - D_1^{-1}A_1 = [\\,1\\,] - [\\,1/3\\,][\\,3\\,] = [\\,0\\,]$.\n- The only eigenvalue is $0$, so $\\rho(T_J) = 0$.\n- Since $\\rho(T_J)  1$, the method converges.\n- $x^\\star = A_1^{-1}b_1 = [\\,1/3\\,]$.\n- $c = D_1^{-1}b_1 = [\\,1/3\\,]$.\n- $x^{(0)} = [\\,0\\,]$.\n- $x^{(1)} = T_J x^{(0)} + c = [\\,0\\,][\\,0\\,] + [\\,1/3\\,] = [\\,1/3\\,]$.\n- The iteration converges exactly in $k=1$ step. The relative error is $\\|x^{(1)} - x^\\star\\|_2 / \\|x^\\star\\|_2 = 0$.\n\n**Case 2:** $n=50$, $A_2$ is the $1$D discrete Laplacian.\n- $A_2$ is a tridiagonal matrix with $2$ on the diagonal and $-1$ on the sub- and super-diagonals.\n- $D_2 = 2I$, so $D_2^{-1} = (1/2)I$.\n- $T_J = I - (1/2)A_2$. This matrix has $0$ on the diagonal and $1/2$ on the sub- and super-diagonals.\n- The eigenvalues of such a matrix are known to be $\\lambda_j(T_J) = \\cos\\left(\\frac{j\\pi}{n+1}\\right)$ for $j=1, \\dots, n$.\n- For $n=50$, the spectral radius is $\\rho(T_J) = \\max_j|\\lambda_j| = \\cos\\left(\\frac{\\pi}{51}\\right)$.\n- Numerically, $\\rho(T_J) \\approx 0.998104$. Since $\\rho(T_J)  1$, the iteration converges, but the spectral radius is very close to $1$, indicating slow convergence. The numerical implementation will find the precise number of iterations required to meet the tolerance.\n\n**Case 3:** $n=50$, $A_3$ is a strictly diagonally dominant matrix.\n- $A_3$ is a tridiagonal matrix with $5$ on the diagonal and $-1$ on the sub- and super-diagonals.\n- $D_3 = 5I$, so $D_3^{-1} = (1/5)I$.\n- $T_J = I - (1/5)A_3$. This matrix has $0$ on the diagonal and $1/5$ on the sub- and super-diagonals.\n- Its eigenvalues are $\\lambda_j(T_J) = \\frac{2}{5}\\cos\\left(\\frac{j\\pi}{n+1}\\right)$ for $j=1, \\dots, n$.\n- For $n=50$, the spectral radius is $\\rho(T_J) = \\frac{2}{5}\\cos\\left(\\frac{\\pi}{51}\\right)$.\n- Numerically, $\\rho(T_J) \\approx 0.4 \\times 0.998104 \\approx 0.399242$. This value is significantly smaller than $1$, which guarantees fast convergence. The matrix $A_3$ is strictly diagonally dominant, which is a sufficient condition for the Jacobi method to converge.\n\n**Case 4:** $A_4 = \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix}$.\n- $D_4 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = I$.\n- $T_J = I - D_4^{-1}A_4 = I - A_4 = \\begin{pmatrix} 0  -2 \\\\ -2  0 \\end{pmatrix}$.\n- The characteristic equation is $\\det(T_J - \\lambda I) = \\lambda^2 - 4 = 0$, giving eigenvalues $\\lambda_1 = -2, \\lambda_2 = 2$.\n- The spectral radius is $\\rho(T_J) = \\max(|-2|, |2|) = 2$.\n- Since $\\rho(T_J) \\ge 1$, the Jacobi iteration does not converge for this system.\n- The exact solution is $x^\\star = A_4^{-1} b_4 = \\begin{pmatrix} -1/3  2/3 \\\\ 2/3  -1/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\end{pmatrix}$.\n- The initial guess is $x^{(0)} = \\mathbf{0}$. The relative error is $\\|x^{(0)} - x^\\star\\|_2 / \\|x^\\star\\|_2 = \\|-x^\\star\\|_2 / \\|x^\\star\\|_2 = 1.0$.\n\nThe following program implements this logic to compute the required results numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Jacobi iteration problem for four specified linear systems.\n    \"\"\"\n    K_max = 50000\n    epsilon = 1e-8\n\n    def solve_case(A, b, K_max, epsilon):\n        \"\"\"\n        Analyzes a single linear system with the Jacobi method.\n\n        Args:\n            A (np.ndarray): The coefficient matrix.\n            b (np.ndarray): The constant vector.\n            K_max (int): Maximum number of iterations.\n            epsilon (float): Convergence tolerance.\n\n        Returns:\n            tuple: A tuple containing (rho_T_J, k, rel_err, converged).\n        \"\"\"\n        n = A.shape[0]\n        \n        # Extract the diagonal of A to form D\n        diag_A = np.diag(A)\n        if np.any(diag_A == 0):\n            # This case is excluded by problem constraints but is a necessary check.\n            raise ValueError(\"Matrix has a zero on its diagonal; Jacobi method is not applicable.\")\n        \n        D = np.diag(diag_A)\n        D_inv = np.diag(1.0 / diag_A) # More efficient than np.linalg.inv for diagonal\n        \n        # Construct the Jacobi iteration matrix T_J\n        T_J = np.identity(n) - D_inv @ A\n        \n        # Compute the spectral radius of T_J\n        eigenvalues = np.linalg.eigvals(T_J)\n        rho_T_J = np.max(np.abs(eigenvalues))\n        \n        # Compute the exact solution x_star\n        x_star = np.linalg.solve(A, b)\n        norm_x_star = np.linalg.norm(x_star, 2)\n        \n        if norm_x_star == 0:\n            # If x_star is the zero vector, the initial guess is exact.\n            return rho_T_J, 0, 0.0, True\n            \n        # Case 1: Iteration is not guaranteed to converge\n        if rho_T_J = 1:\n            x0 = np.zeros_like(b)\n            # Initial relative error for x^(0) = 0\n            rel_err = np.linalg.norm(x0 - x_star, 2) / norm_x_star\n            return rho_T_J, 0, rel_err, False\n        \n        # Case 2: Iteration converges\n        else:\n            c = D_inv @ b\n            xk = np.zeros_like(b)\n            for k in range(K_max + 1):\n                rel_err = np.linalg.norm(xk - x_star, 2) / norm_x_star\n                \n                # Check for convergence\n                if rel_err = epsilon:\n                    return rho_T_J, k, rel_err, True\n                \n                # Check if max iterations reached without convergence\n                if k == K_max:\n                    return rho_T_J, k, rel_err, False\n                \n                # Update for next iteration\n                xk = T_J @ xk + c\n        \n        # This part of the code should be unreachable given the logic above.\n        return rho_T_J, K_max, np.nan, False\n\n    # Define the test cases from the problem statement.\n    test_cases = []\n\n    # Case 1: Trivial scalar system\n    A1 = np.array([[3.0]], dtype=float)\n    b1 = np.array([1.0], dtype=float)\n    test_cases.append((A1, b1))\n\n    # Case 2: 1D discrete Laplacian\n    n2 = 50\n    diag2 = np.full(n2, 2.0, dtype=float)\n    off_diag2 = np.full(n2 - 1, -1.0, dtype=float)\n    A2 = np.diag(diag2) + np.diag(off_diag2, k=1) + np.diag(off_diag2, k=-1)\n    b2 = np.ones(n2, dtype=float)\n    test_cases.append((A2, b2))\n\n    # Case 3: Strictly diagonally dominant tridiagonal\n    n3 = 50\n    diag3 = np.full(n3, 5.0, dtype=float)\n    off_diag3 = np.full(n3 - 1, -1.0, dtype=float)\n    A3 = np.diag(diag3) + np.diag(off_diag3, k=1) + np.diag(off_diag3, k=-1)\n    b3 = np.ones(n3, dtype=float)\n    test_cases.append((A3, b3))\n    \n    # Case 4: Non-convergent case\n    A4 = np.array([[1.0, 2.0], [2.0, 1.0]], dtype=float)\n    b4 = np.array([1.0, 1.0], dtype=float)\n    test_cases.append((A4, b4))\n\n    results = []\n    for A, b in test_cases:\n        result = solve_case(A, b, K_max, epsilon)\n        results.append(result)\n\n    # Format the results into the required single-line string.\n    formatted_results = []\n    for rho, k, err, conv in results:\n        # Each sub-list is formatted as a string \"[rho,k,err,conv]\"\n        formatted_results.append(f\"[{rho:.6f},{k},{err:.6f},{str(conv)}]\")\n\n    # The final output is a string representation of a list of these sub-lists.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2404665"}, {"introduction": "In practical applications, we do not have access to the true solution, so we cannot stop an iterative solver based on the true error. Instead, we must rely on computable proxies, with the most common being the norm of the residual vector, $\\lVert b - Ax^{(k)} \\rVert$. This exercise [@problem_id:2404697] challenges you to investigate the reliability of this stopping criterion. You will discover that a small residual does not always guarantee a small error, a crucial insight that hinges on the conditioning of the system matrix $A$ and has profound implications for the robust implementation of any iterative method.", "problem": "You are given the task of implementing a stopping criterion for the Jacobi iteration based on the residual norm, and to investigate when this criterion is a reliable proxy for the actual error of the iterate. Consider a linear system of equations $A x = b$ with a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^{n}$. The Jacobi iteration produces a sequence of approximations $x^{(k)}$ to the true solution $x^{\\star}$. The residual at iteration $k$ is defined by $r^{(k)} = b - A x^{(k)}$, and its Euclidean norm is $\\lVert r^{(k)} \\rVert_{2}$. The stopping criterion you must implement is to terminate as soon as $\\lVert r^{(k)} \\rVert_{2} \\le \\tau$, where $\\tau  0$ is a given tolerance, or when a specified maximum number of iterations has been reached. You must also compute the true error norm $\\lVert x^{(k)} - x^{\\star} \\rVert_{2}$ at termination (or at the iteration limit if no convergence occurs), where $x^{\\star}$ is known for the test cases below.\n\nStart from fundamental linear algebra definitions and facts, notably: the equation $A x = b$, the Euclidean norm $\\lVert \\cdot \\rVert_{2}$, and the residual definition $r^{(k)} = b - A x^{(k)}$. Do not assume any specialized shortcut results beyond these. You should implement the Jacobi iteration correctly for general nonsingular $A$ with nonzero diagonal entries, using an initial guess $x^{(0)} = 0$. Your implementation must be universal and work for each test case without hard-coding case-specific behavior.\n\nYour investigation must assess the reliability of using the residual norm as a proxy for the true error norm. For each test case, after termination, evaluate the boolean statement \"naively reliable\" defined as $\\lVert x^{(k)} - x^{\\star} \\rVert_{2} \\le \\tau$; that is, whether the actual error is no larger than the residual tolerance used as a stopping threshold. Also compute the spectral condition number in the Euclidean norm, $\\kappa_{2}(A)$, to contextualize reliability.\n\nTest suite:\n- Test case $1$ (well-conditioned, strictly diagonally dominant):\n  - $A_{1} = \\begin{bmatrix} 4  1  1 \\\\ 1  3  0 \\\\ 1  0  2 \\end{bmatrix}$,\n  - $x^{\\star}_{1} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$,\n  - $b_{1} = A_{1} x^{\\star}_{1}$,\n  - residual tolerance $\\tau_{1} = 10^{-10}$,\n  - maximum iterations $N_{1} = 1000$.\n- Test case $2$ (ill-conditioned but convergent Jacobi):\n  - $A_{2} = \\begin{bmatrix} 1  9 \\times 10^{-4} \\\\ 9 \\times 10^{-4}  10^{-6} \\end{bmatrix}$,\n  - $x^{\\star}_{2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $b_{2} = A_{2} x^{\\star}_{2}$,\n  - residual tolerance $\\tau_{2} = 10^{-8}$,\n  - maximum iterations $N_{2} = 5000$.\n- Test case $3$ (Jacobi diverges):\n  - $A_{3} = \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix}$,\n  - $x^{\\star}_{3} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$,\n  - $b_{3} = A_{3} x^{\\star}_{3}$,\n  - residual tolerance $\\tau_{3} = 10^{-8}$,\n  - maximum iterations $N_{3} = 200$.\n\nFor each test case, run the Jacobi iteration with $x^{(0)} = 0$ and the stopping rule $\\lVert r^{(k)} \\rVert_{2} \\le \\tau$ or hitting the iteration cap. At termination, compute:\n- a convergence boolean indicating whether $\\lVert r^{(k)} \\rVert_{2} \\le \\tau$ was achieved before the cap,\n- a naive-reliability boolean indicating whether $\\lVert x^{(k)} - x^{\\star} \\rVert_{2} \\le \\tau$,\n- the number of iterations performed (an integer),\n- the final residual norm $\\lVert r^{(k)} \\rVert_{2}$ (a float),\n- the final error norm $\\lVert x^{(k)} - x^{\\star} \\rVert_{2}$ (a float),\n- the spectral condition number $\\kappa_{2}(A)$ (a float),\n- the amplification factor $\\gamma = \\lVert x^{(k)} - x^{\\star} \\rVert_{2} / \\lVert r^{(k)} \\rVert_{2}$ (a float, define $\\gamma = 0$ if the denominator is $0$).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of per-case result lists, with no spaces, enclosed in square brackets. Each per-case result list must be in the exact order:\n$[$converged boolean, naive-reliability boolean, iterations integer, residual norm float, error norm float, condition number float, amplification factor float$]$.\nFor example, a valid overall output structure is `[[True,False,10,1.0,2.0,3.0,2.0],[\\dots],[\\dots]]`.\n\nAngles are not involved. No physical units are involved. All numerical results are dimensionless real numbers. The output must be a single line exactly as specified, with booleans and numbers only, using the canonical textual forms for these types.", "solution": "The problem presented is valid. It is a well-defined computational exercise in numerical linear algebra, based on established scientific principles and free of ambiguity or contradiction. We shall proceed with the solution.\n\nThe core task is to solve the linear system of equations $A x = b$ for a given square, nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^{n}$, using the Jacobi iterative method. We will then analyze the relationship between the residual norm, used as a stopping criterion, and the true error norm.\n\nFirst, we establish the Jacobi iteration formula. The matrix $A$ is decomposed into its diagonal part $D$, a strictly lower-triangular part $L$, and a strictly upper-triangular part $U$, such that $A = D + L + U$. The original equation $A x = b$ can be written as $(D + L + U) x = b$. Rearranging for an iterative solution gives:\n$$D x = b - (L + U) x$$\nAssuming the diagonal elements of $A$ are all non-zero, the diagonal matrix $D$ is invertible. We can thus define an iterative sequence $x^{(k)}$ that hopefully converges to the true solution $x^{\\star}$:\n$$x^{(k+1)} = D^{-1} (b - (L + U) x^{(k)})$$\nwhere $k$ is the iteration index, starting from an initial guess $x^{(0)}$. The problem specifies $x^{(0)} = 0$. In component-wise form, the update rule for the $i$-th element of the vector $x$ at iteration $k+1$ is:\n$$x_{i}^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)} \\right)$$\nThis formula highlights that the computation of each component of the new iterate $x^{(k+1)}$ depends only on the components of the previous iterate $x^{(k)}$.\n\nThe convergence of the Jacobi method is determined by the spectral radius of the Jacobi iteration matrix $T_J = -D^{-1}(L+U)$. The iteration is guaranteed to converge for any initial guess $x^{(0)}$ if and only if the spectral radius $\\rho(T_J)  1$. A sufficient, but not necessary, condition for convergence is that the matrix $A$ is strictly diagonally dominant.\n\nThe problem requires a stopping criterion based on the Euclidean norm of the residual vector. At each iteration $k$, the residual $r^{(k)}$ is defined as the difference between the right-hand side $b$ and the result of applying the matrix $A$ to the current approximation $x^{(k)}$:\n$$r^{(k)} = b - A x^{(k)}$$\nThe iteration terminates when $\\lVert r^{(k)} \\rVert_{2} \\le \\tau$, where $\\tau$ is a given tolerance, or when a maximum number of iterations $N$ is exceeded.\n\nA central part of this investigation is to understand the relationship between the residual $r^{(k)}$ and the true error $e^{(k)} = x^{(k)} - x^{\\star}$. Since $A x^{\\star} = b$ by definition of the true solution, we can write:\n$$r^{(k)} = A x^{\\star} - A x^{(k)} = A (x^{\\star} - x^{(k)}) = -A e^{(k)}$$\nSince $A$ is nonsingular, its inverse $A^{-1}$ exists. We can express the error in terms of the residual:\n$$e^{(k)} = -A^{-1} r^{(k)}$$\nBy taking the Euclidean norm of both sides and using the property of an induced matrix norm ($\\lVert M v \\rVert \\le \\lVert M \\rVert \\lVert v \\rVert$), we arrive at an important inequality:\n$$\\lVert e^{(k)} \\rVert_{2} = \\lVert A^{-1} r^{(k)} \\rVert_{2} \\le \\lVert A^{-1} \\rVert_{2} \\lVert r^{(k)} \\rVert_{2}$$\nThis inequality provides an upper bound on the error norm based on the residual norm. Symmetrically, from $r^{(k)} = -A e^{(k)}$, we have $\\lVert r^{(k)} \\rVert_{2} \\le \\lVert A \\rVert_{2} \\lVert e^{(k)} \\rVert_{2}$, which gives a lower bound on the error norm:\n$$\\lVert e^{(k)} \\rVert_{2} \\ge \\frac{\\lVert r^{(k)} \\rVert_{2}}{\\lVert A \\rVert_{2}}$$\nCombining these gives the full bounding relationship:\n$$\\frac{\\lVert r^{(k)} \\rVert_{2}}{\\lVert A \\rVert_{2}} \\le \\lVert e^{(k)} \\rVert_{2} \\le \\lVert A^{-1} \\rVert_{2} \\lVert r^{(k)} \\rVert_{2}$$\nThe reliability of using the residual norm $\\lVert r^{(k)} \\rVert_{2}$ as a proxy for the error norm $\\lVert e^{(k)} \\rVert_{2}$ depends critically on the values of $\\lVert A \\rVert_{2}$ and $\\lVert A^{-1} \\rVert_{2}$. The spectral condition number, $\\kappa_{2}(A) = \\lVert A \\rVert_{2} \\lVert A^{-1} \\rVert_{2}$, encapsulates this relationship. A large condition number, indicating an ill-conditioned matrix, implies that $\\lVert A^{-1} \\rVert_{2}$ can be large.\n\nIf the stopping criterion $\\lVert r^{(k)} \\rVert_{2} \\le \\tau$ is met, the error bound becomes:\n$$\\lVert e^{(k)} \\rVert_{2} \\le \\lVert A^{-1} \\rVert_{2} \\tau$$\nThe \"naively reliable\" criterion defined in the problem, $\\lVert e^{(k)} \\rVert_{2} \\le \\tau$, is only guaranteed if $\\lVert A^{-1} \\rVert_{2} \\le 1$. For an ill-conditioned matrix, $\\lVert A^{-1} \\rVert_{2}$ can be significantly larger than $1$, causing the true error to be much larger than the residual tolerance $\\tau$. The amplification factor $\\gamma = \\lVert e^{(k)} \\rVert_{2} / \\lVert r^{(k)} \\rVert_{2}$ quantifies this effect directly, and we expect $\\gamma$ to be related to $\\lVert A^{-1} \\rVert_{2}$.\n\nThe algorithm to be implemented will perform the following steps for each test case:\n1. Initialize the iteration counter $k=0$ and the solution vector $x^{(0)} = 0$.\n2. Loop for $k$ from $1$ to the maximum number of iterations $N$.\n3. In each iteration, calculate the next approximation $x^{(k)}$ using the Jacobi formula with $x^{(k-1)}$.\n4. Compute the residual $r^{(k)} = b - A x^{(k)}$ and its Euclidean norm $\\lVert r^{(k)} \\rVert_{2}$.\n5. Check if $\\lVert r^{(k)} \\rVert_{2} \\le \\tau$. If true, the iteration has converged. Terminate the loop.\n6. If the loop completes without meeting the tolerance, convergence was not achieved within $N$ iterations.\n7. Upon termination (either by convergence or by reaching the iteration limit), calculate the following quantities:\n    - `converged`: A boolean indicating if the tolerance was met.\n    - `naive-reliability`: A boolean for the test $\\lVert x^{(k)} - x^{\\star} \\rVert_{2} \\le \\tau$.\n    - `iterations`: The final iteration count, $k$.\n    - `residual_norm`: The final value of $\\lVert r^{(k)} \\rVert_{2}$.\n    - `error_norm`: The final value of $\\lVert x^{(k)} - x^{\\star} \\rVert_{2}$.\n    - `condition_number`: The spectral condition number of the matrix, $\\kappa_{2}(A)$.\n    - `amplification_factor`: The ratio $\\gamma = \\lVert e^{(k)} \\rVert_{2} / \\lVert r^{(k)} \\rVert_{2}$.\n\nThis procedure will be applied to the three provided test cases, which are designed to illustrate different behaviors: a well-conditioned convergent case, an ill-conditioned but still convergent case, and a divergent case. The results will provide empirical evidence for the theoretical relationship between residual, error, and conditioning.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed for this problem specification.\n\ndef run_jacobi_test(A, x_star, tau, N_max):\n    \"\"\"\n    Performs the Jacobi iteration for a given linear system Ax=b.\n\n    Args:\n        A (np.ndarray): The system matrix.\n        x_star (np.ndarray): The true solution vector.\n        tau (float): The residual norm tolerance for stopping.\n        N_max (int): The maximum number of iterations.\n\n    Returns:\n        list: A list containing the seven required result metrics.\n    \"\"\"\n    # Ensure inputs are numpy arrays with float type for precision\n    A = np.array(A, dtype=float)\n    x_star = np.array(x_star, dtype=float)\n    b = A @ x_star  # Calculate b from A and the known true solution\n\n    n = A.shape[0]\n    x_current = np.zeros(n, dtype=float)\n    \n    # Pre-calculate matrices for Jacobi iteration\n    diag_A = np.diag(A)\n    # The problem guarantees non-zero diagonal entries\n    D_inv = np.diag(1.0 / diag_A)\n    R = A - np.diag(diag_A)\n\n    num_iter = 0\n    converged = False\n\n    # The main iteration loop\n    for k in range(1, N_max + 1):\n        num_iter = k\n        \n        # Jacobi update: x_k = D^-1 * (b - (L+U) * x_{k-1})\n        x_next = D_inv @ (b - R @ x_current)\n        x_current = x_next\n        \n        # Calculate residual and its norm for the current iterate\n        r_current = b - A @ x_current\n        r_norm_current = np.linalg.norm(r_current, 2)\n        \n        # Check stopping criterion\n        if r_norm_current = tau:\n            converged = True\n            break\n    else:\n        # This block executes if the for loop completes without a 'break'\n        converged = False\n        r_current = b - A @ x_current\n        r_norm_current = np.linalg.norm(r_current, 2)\n\n    # Post-iteration calculations\n    error_current = x_current - x_star\n    error_norm_current = np.linalg.norm(error_current, 2)\n\n    naive_reliability = error_norm_current = tau\n    \n    cond_A = np.linalg.cond(A, 2)\n    \n    if r_norm_current == 0.0:\n        amplification_factor = 0.0\n    else:\n        amplification_factor = error_norm_current / r_norm_current\n        \n    return [\n        converged,\n        naive_reliability,\n        num_iter,\n        float(r_norm_current),\n        float(error_norm_current),\n        float(cond_A),\n        float(amplification_factor)\n    ]\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement and runs the analysis.\n    \"\"\"\n    # Test case 1 (well-conditioned, strictly diagonally dominant)\n    A1 = [[4, 1, 1], [1, 3, 0], [1, 0, 2]]\n    x_star1 = [1, -2, 3]\n    tau1 = 1e-10\n    N1 = 1000\n\n    # Test case 2 (ill-conditioned but convergent Jacobi)\n    A2 = [[1, 9e-4], [9e-4, 1e-6]]\n    x_star2 = [1, 1]\n    tau2 = 1e-8\n    N2 = 5000\n\n    # Test case 3 (Jacobi diverges)\n    A3 = [[1, 2], [2, 1]]\n    x_star3 = [1, -1]\n    tau3 = 1e-8\n    N3 = 200\n\n    test_cases = [\n        (A1, x_star1, tau1, N1),\n        (A2, x_star2, tau2, N2),\n        (A3, x_star3, tau3, N3),\n    ]\n\n    results = []\n    for case in test_cases:\n        A, x_star, tau, N_max = case\n        result_list = run_jacobi_test(A, x_star, tau, N_max)\n        results.append(result_list)\n\n    # Format the final output string exactly as specified\n    per_case_strings = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(per_case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2404697"}, {"introduction": "Why choose an iterative method like Jacobi over a direct method such as LU factorization? This final practice addresses this fundamental question by comparing the computational cost of both approaches. By analyzing a model of the 2D Poisson problem, you will use an operation-count model to determine the 'crossover point'—the problem size at which the Jacobi method becomes more efficient than a dense LU solver [@problem_id:2404653]. This investigation highlights the critical role of algorithmic complexity and scalability, demonstrating why iterative methods are indispensable tools for the large-scale problems that are common in computational physics.", "problem": "You will investigate the algorithmic trade-off between an iterative Jacobi method and a direct Lower–Upper (LU) factorization when solving a model linear system arising from a two-dimensional Poisson problem. Consider the boundary value problem with homogeneous Dirichlet boundary conditions, discretized on a uniform grid of size $N \\times N$ interior points, yielding a linear system of size $M \\times M$ with $M = N^2$. The coefficient matrix $A$ corresponds to the standard five-point stencil for the Laplacian, with entries $4$ on the diagonal and $-1$ on the four immediate neighbors, and the right-hand side vector corresponds to a constant source $f \\equiv 1$ discretized consistently. Angles in all trigonometric functions must be treated in radians.\n\nYou must:\n\n- Implement a Jacobi iteration solver for this problem that, given $N$ and a tolerance $\\varepsilon \\in (0,1)$, starts from the zero vector and iterates until the relative $\\ell_2$-norm of the residual $\\|r_k\\|_2 / \\|r_0\\|_2$ is less than or equal to $\\varepsilon$, where $r_k = b - A x_k$. The solver must be correct for any $N \\ge 2$ and any $\\varepsilon \\in (0,1)$, expressed with dimensionless quantities. No physical units are involved in this task.\n- Implement a direct dense LU decomposition solver using a standard routine for the same linear system (note: this is intentionally dense and not specialized to the sparse structure).\n- Using a first-principles operation-count model, determine, for various tolerances $\\varepsilon$, the smallest grid size $N$ for which Jacobi becomes faster than dense LU factorization when both are applied to the two-dimensional $N \\times N$ interior grid Poisson system described above. Define the cost model as follows:\n  - Let $M = N^2$ be the number of unknowns. Model the total cost of Jacobi as\n    $$\\mathcal{C}_{\\text{J}}(N,\\varepsilon) = M \\times K_{\\text{J}}(N,\\varepsilon),$$\n    where $K_{\\text{J}}(N,\\varepsilon)$ is the number of Jacobi iterations required to reduce the relative residual below $\\varepsilon$.\n  - Model the total cost of dense LU as\n    $$\\mathcal{C}_{\\text{LU}}(N; r) = r \\left(\\tfrac{2}{3} M^3 + 2 M^2 \\right),$$\n    where $r > 0$ is a dimensionless constant that captures relative implementation efficiency and machine-dependent effects (smaller $r$ means relatively faster LU per operation unit compared to Jacobi).\n  - For the purposes of this exercise, define the crossover $N^\\star(\\varepsilon,r)$ to be the smallest integer $N \\ge 2$ such that\n    $$\\mathcal{C}_{\\text{J}}(N,\\varepsilon) \\le \\mathcal{C}_{\\text{LU}}(N; r).$$\n- Your program must compute $N^\\star(\\varepsilon,r)$ by combining your Jacobi implementation insight with a mathematically justified estimate of $K_{\\text{J}}(N,\\varepsilon)$ derived from the spectral properties of the Jacobi iteration matrix for this model problem. You must reason from first principles. Do not time code; use the stated cost model.\n\nTest Suite:\nEvaluate the crossover grid size $N^\\star(\\varepsilon,r)$ for the following four parameter pairs $(\\varepsilon, r)$:\n- $(10^{-6}, 1.0)$,\n- $(10^{-8}, 0.5)$,\n- $(10^{-2}, 2.0)$,\n- $(10^{-12}, 0.25)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of four integers enclosed in square brackets in the same order as the test suite (e.g., \"[3,4,2,7]\"). No other text should be printed.\n\nNotes:\n- Angles must be in radians.\n- All quantities are dimensionless; no physical units are involved.\n- The answer for each test case is an integer.", "solution": "The problem as stated is a standard exercise in numerical analysis and is well-posed, scientifically grounded, and internally consistent. All provided data and definitions are sufficient for a unique solution. We shall therefore proceed directly to the derivation and solution.\n\nThe problem requires a comparison of computational costs for solving the linear system $A\\mathbf{x} = \\mathbf{b}$ derived from the two-dimensional Poisson equation, $\\nabla^2 u = f$, on a unit square with homogeneous Dirichlet boundary conditions. The domain is discretized into a uniform grid with $N \\times N$ interior points. The system size is thus $M = N^2$. The matrix $A$ represents the five-point finite difference stencil for the negative Laplacian, $-\\nabla^2$, which results in a matrix with $4$ on the diagonal and $-1$ on off-diagonal entries corresponding to the four nearest grid neighbors. The right-hand side vector $\\mathbf{b}$ corresponds to a constant source term $f \\equiv 1$.\n\nThe core of the problem is to find the crossover grid size $N^\\star$ at which the Jacobi iterative method becomes more computationally efficient than a direct dense LU factorization, according to a specified cost model. This requires deriving an analytical estimate for the number of Jacobi iterations, $K_{\\text{J}}$, needed to achieve a given tolerance $\\varepsilon$.\n\nFirst, we analyze the Jacobi method. The matrix $A$ is decomposed as $A = D - L - U$, where $D$ is the diagonal part of $A$, and $-L$ and $-U$ are the strictly lower and upper triangular parts, respectively. For our specific matrix, $D$ is a scalar matrix, $D = 4I_M$, where $I_M$ is the identity matrix of size $M$. The Jacobi iteration is defined by the recurrence:\n$$D\\mathbf{x}_{k+1} = (L+U)\\mathbf{x}_k + \\mathbf{b}$$\nThis can be rewritten as $\\mathbf{x}_{k+1} = D^{-1}(L+U)\\mathbf{x}_k + D^{-1}\\mathbf{b}$. The iteration matrix is $T_J = D^{-1}(L+U)$. Since $L+U = D-A$, we have $T_J = D^{-1}(D-A) = I_M - D^{-1}A$. Given $D = 4I_M$, this simplifies to:\n$$T_J = I_M - \\tfrac{1}{4}A$$\n\nThe convergence rate of the Jacobi method is governed by the spectral radius of the iteration matrix, $\\rho(T_J)$, which is the maximum absolute value of its eigenvalues. The eigenvalues of the matrix $A$ for the 2D discrete Poisson problem on an $N \\times N$ grid are well-known:\n$$\\lambda_{p,q}(A) = 4 - 2\\left(\\cos\\left(\\frac{p\\pi}{N+1}\\right) + \\cos\\left(\\frac{q\\pi}{N+1}\\right)\\right) \\quad \\text{for } p,q = 1, 2, \\ldots, N$$\nThe eigenvalues of the Jacobi matrix $T_J$ are therefore:\n$$\\lambda_{p,q}(T_J) = 1 - \\frac{1}{4}\\lambda_{p,q}(A) = 1 - \\frac{1}{4}\\left[4 - 2\\left(\\cos\\left(\\frac{p\\pi}{N+1}\\right) + \\cos\\left(\\frac{q\\pi}{N+1}\\right)\\right)\\right]$$\n$$\\lambda_{p,q}(T_J) = \\frac{1}{2}\\left(\\cos\\left(\\frac{p\\pi}{N+1}\\right) + \\cos\\left(\\frac{q\\pi}{N+1}\\right)\\right)$$\nThe spectral radius $\\rho(T_J)$ corresponds to the eigenvalue with the largest magnitude. Since the arguments to the cosine function lie in the interval $(0, \\pi)$, all eigenvalues are real and positive. The maximum value is obtained for the smallest arguments, i.e., $p=1$ and $q=1$:\n$$\\rho(T_J) = \\lambda_{1,1}(T_J) = \\frac{1}{2}\\left(\\cos\\left(\\frac{\\pi}{N+1}\\right) + \\cos\\left(\\frac{\\pi}{N+1}\\right)\\right) = \\cos\\left(\\frac{\\pi}{N+1}\\right)$$\n\nThe problem specifies a stopping criterion based on the relative residual norm: $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{r}_0\\|_2 \\le \\varepsilon$, where $\\mathbf{r}_k = \\mathbf{b} - A\\mathbf{x}_k$. The initial guess is $\\mathbf{x}_0 = \\mathbf{0}$, so $\\mathbf{r}_0 = \\mathbf{b}$. The residual updates as $\\mathbf{r}_{k+1} = T_J \\mathbf{r}_k$, leading to $\\mathbf{r}_k = T_J^k \\mathbf{r}_0$. The criterion becomes $\\|T_J^k \\mathbf{r}_0\\|_2 / \\|\\mathbf{r}_0\\|_2 \\le \\varepsilon$.\nA standard upper bound is established using the matrix norm: $\\|T_J^k \\mathbf{r}_0\\|_2 \\le \\|T_J^k\\|_2 \\|\\mathbf{r}_0\\|_2$. Since $T_J$ is symmetric, $\\|T_J\\|_2 = \\rho(T_J)$, and thus $\\|T_J^k\\|_2 = (\\rho(T_J))^k$. The condition is satisfied if $(\\rho(T_J))^k \\le \\varepsilon$. Solving for the number of iterations $k$, we take the logarithm:\n$$k \\ln(\\rho(T_J)) \\le \\ln(\\varepsilon)$$\nSince $\\rho(T_J)  1$ for any finite $N \\ge 1$, its logarithm is negative. We must therefore reverse the inequality when dividing:\n$$k \\ge \\frac{\\ln(\\varepsilon)}{\\ln(\\rho(T_J))}$$\nThe required number of iterations, $K_{\\text{J}}$, is the smallest integer satisfying this, which is the ceiling of the expression on the right-hand side.\n$$K_{\\text{J}}(N,\\varepsilon) = \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln\\left(\\cos\\left(\\frac{\\pi}{N+1}\\right)\\right)} \\right\\rceil$$\n\nNow we formulate the cost models. The cost for the Jacobi method is given as:\n$$\\mathcal{C}_{\\text{J}}(N,\\varepsilon) = M \\times K_{\\text{J}}(N,\\varepsilon) = N^2 \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln\\left(\\cos\\left(\\frac{\\pi}{N+1}\\right)\\right)} \\right\\rceil$$\nThe cost for the dense LU factorization is given as:\n$$\\mathcal{C}_{\\text{LU}}(N; r) = r \\left(\\frac{2}{3} M^3 + 2 M^2 \\right) = r \\left(\\frac{2}{3} (N^2)^3 + 2 (N^2)^2 \\right) = r \\left(\\frac{2}{3} N^6 + 2 N^4 \\right)$$\n\nThe crossover grid size, $N^\\star(\\varepsilon, r)$, is the smallest integer $N \\ge 2$ for which Jacobi becomes faster, i.e., $\\mathcal{C}_{\\text{J}}(N,\\varepsilon) \\le \\mathcal{C}_{\\text{LU}}(N; r)$. This gives the inequality:\n$$N^2 \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln\\left(\\cos\\left(\\frac{\\pi}{N+1}\\right)\\right)} \\right\\rceil \\le r \\left(\\frac{2}{3} N^6 + 2 N^4 \\right)$$\nFor $N \\ge 1$, we can divide by $N^2$:\n$$\\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln\\left(\\cos\\left(\\frac{\\pi}{N+1}\\right)\\right)} \\right\\rceil \\le r \\left(\\frac{2}{3} N^4 + 2 N^2 \\right)$$\nThe left-hand side, representing the number of iterations, grows approximately as $O(N^2)$, while the right-hand side term grows as $O(N^4)$. For small $N$, the Jacobi cost $\\mathcal{C}_{\\text{J}} \\approx O(N^4)$ will exceed the LU cost $\\mathcal{C}_{\\text{LU}} \\approx O(N^6)$, but because of the higher-order growth of the polynomial, the LU cost will eventually dominate. We are seeking the first integer $N \\ge 2$ where the inequality reverses and holds true.\nThis inequality is not readily solvable for $N$ in closed form due to the floor/ceiling function and transcendental terms. However, we can find the solution computationally by iterating through integer values of $N$ starting from $N=2$ and stopping at the first value that satisfies the condition. The functions are monotonic, ensuring a unique crossover point. We will implement this search for each parameter pair $(\\varepsilon, r)$ provided in the test suite.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the crossover grid size N* for which the Jacobi method becomes\n    more efficient than dense LU factorization for a 2D Poisson problem.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1e-6, 1.0),\n        (1e-8, 0.5),\n        (1e-2, 2.0),\n        (1e-12, 0.25),\n    ]\n\n    results = []\n    for epsilon, r_factor in test_cases:\n        n_star = find_crossover_n(epsilon, r_factor)\n        results.append(n_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_crossover_n(epsilon, r):\n    \"\"\"\n    Searches for the smallest integer N = 2 that satisfies the crossover condition.\n\n    Args:\n        epsilon (float): The tolerance for the Jacobi solver.\n        r (float): The relative efficiency factor for the LU solver.\n\n    Returns:\n        int: The smallest grid size N for which Jacobi is more efficient.\n    \"\"\"\n    n = 2\n    while True:\n        # Calculate the number of Jacobi iterations, K_J.\n        # The spectral radius of the Jacobi iteration matrix is rho_J = cos(pi / (N+1)).\n        arg_cos = np.pi / (n + 1)\n        \n        # For N = 2, 0  arg_cos  pi/2, so cos is in (0, 1) and its log is negative.\n        rho_j = np.cos(arg_cos)\n        k_j = np.ceil(np.log(epsilon) / np.log(rho_j))\n\n        # Calculate the computational cost for Jacobi, C_J.\n        # The system size is M = N*N. Cost model is M * K_J.\n        cost_j = (n**2) * k_j\n\n        # Calculate the computational cost for dense LU, C_LU.\n        # Cost model is r * (2/3 * M^3 + 2 * M^2).\n        m = n**2\n        cost_lu = r * ((2/3) * (m**3) + 2 * (m**2))\n\n        # Check for the crossover condition.\n        if cost_j = cost_lu:\n            return n\n        \n        n += 1\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2404653"}]}