{"hands_on_practices": [{"introduction": "Solving systems of linear equations is a cornerstone of computational science. While general systems can be complex, many problems in physics and engineering result in matrices with special structures that permit highly efficient solutions. This first practice focuses on upper-triangular systems, which can be solved elegantly using an algorithm known as `back substitution`. Mastering this technique provides a foundational skill and builds intuition for more advanced matrix decomposition methods, such as `LU factorization`, that transform general systems into these simpler forms. [@problem_id:14096]", "problem": "Consider a system of linear equations represented in matrix form as $U\\mathbf{x} = \\mathbf{b}$, where $U$ is a 3x3 upper-triangular matrix, and $\\mathbf{x}$ and $\\mathbf{b}$ are 3x1 column vectors.\n\nThe matrix $U$ and the vector $\\mathbf{b}$ are given by:\n$$\nU = \\begin{pmatrix} a & b & c \\\\ 0 & d & e \\\\ 0 & 0 & f \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} p \\\\ q \\\\ r \\end{pmatrix}\n$$\nThe vector of unknowns is $\\mathbf{x} = [x_1, x_2, x_3]^T$. The parameters $a, b, c, d, e, f, p, q, r$ are arbitrary constants, with the condition that the diagonal elements $a, d, f$ are all non-zero.\n\nUsing the method of back substitution, derive a symbolic expression for the component $x_1$ of the solution vector $\\mathbf{x}$. Your final expression should be in terms of the given constants.", "solution": "We solve the upper‐triangular system $U\\mathbf{x}=\\mathbf{b}$ by back‐substitution.  Writing out the equations,\n$$\n\\begin{cases}\na\\,x_1 + b\\,x_2 + c\\,x_3 = p,\\\\\nd\\,x_2 + e\\,x_3 = q,\\\\\nf\\,x_3 = r.\n\\end{cases}\n$$\nSince $f\\neq0$, from the third equation\n$$\nx_3 = \\frac{r}{f}.\n$$\nSubstitute into the second equation, noting $d\\neq0$,\n$$\nd\\,x_2 + e\\bigl(\\tfrac{r}{f}\\bigr) = q\n\\quad\\Longrightarrow\\quad\nx_2 = \\frac{q - \\frac{e\\,r}{f}}{d}\n= \\frac{q f - e\\,r}{d\\,f}.\n$$\nFinally substitute $x_2$ and $x_3$ into the first equation, with $a\\neq0$,\n$$\na\\,x_1\n= p - b\\,x_2 - c\\,x_3\n= p - b\\Bigl(\\frac{q f - e r}{d f}\\Bigr) - c\\Bigl(\\frac{r}{f}\\Bigr).\n$$\nCombine the terms over the common denominator $d f$,\n$$\np - b\\frac{qf - e r}{df} - c\\frac{r}{f}\n= \\frac{p\\,d\\,f - \\bigl(b(qf - e r) + c\\,r\\,d\\bigr)}{d\\,f}\n= \\frac{p\\,d\\,f - b\\,q\\,f + b\\,e\\,r - c\\,d\\,r}{d\\,f}.\n$$\nHence\n$$\nx_1 = \\frac{1}{a}\\;\\frac{p\\,d\\,f - b\\,q\\,f + b\\,e\\,r - c\\,d\\,r}{d\\,f}\n= \\frac{p\\,d\\,f - b\\,q\\,f + b\\,e\\,r - c\\,d\\,r}{a\\,d\\,f}.\n$$", "answer": "$$\\boxed{\\frac{p\\,d\\,f - b\\,q\\,f + b\\,e\\,r - c\\,d\\,r}{a\\,d\\,f}}$$", "id": "14096"}, {"introduction": "The power of matrix representations extends far beyond just solving a given set of equations; they serve as powerful descriptive models for physical systems. In this exercise, you will derive the matrix representation for a resistor network directly from Ohm's law and Kirchhoff's Current Law. This practice will allow you to see how the very structure of the matrix, known as the graph Laplacian, encodes the physical topology of the circuit. Furthermore, you will explore how abstract linear algebra concepts, such as matrix singularity and the null space, correspond to tangible physical principles like the necessity of a ground reference and the behavior of disconnected components. [@problem_id:2412372]", "problem": "Consider a passive resistor network with nodes labeled $1$ through $4$ and resistors placed only between directly connected nodes as follows: $R_{12}=2\\,\\Omega$, $R_{23}=1\\,\\Omega$, and $R_{34}=2\\,\\Omega$. There is no direct connection between nodes $1$ and $3$, between nodes $1$ and $4$, or between nodes $2$ and $4$. Let $V_i$ denote the electric potential at node $i$, and let $I_i$ denote the externally injected current at node $i$ (positive when injected into the node from an external source). Use Ohm’s law and Kirchhoff’s Current Law (KCL), which states that the sum of currents leaving any node equals the net current injection at that node, to set up the linear system that relates the vector of node potentials to the vector of current injections.\n\nStart from the definitions that the current flowing from node $i$ to node $j$ through a resistor $R_{ij}$ is $I_{ij} = (V_i - V_j)/R_{ij}$, and that KCL at node $i$ is $\\sum_{j \\in \\mathcal{N}(i)} (V_i - V_j)/R_{ij} = I_i$, where $\\mathcal{N}(i)$ denotes the set of nodes connected to $i$ by a resistor. From these principles, the nodal equations take the matrix form $L \\mathbf{V} = \\mathbf{I}$, where $L$ is the Kirchhoff (graph Laplacian) matrix assembled from conductances $G_{ij} = 1/R_{ij}$.\n\nAnswer the following by selecting all options that are correct.\n\nA. When node $4$ is chosen as the reference (ground) so that $V_4 = 0$, and the unknowns are the node potentials $(V_1, V_2, V_3)$, the KCL matrix $L_{\\mathrm{red}}$ in the reduced linear system $L_{\\mathrm{red}} \\begin{bmatrix} V_1 \\\\ V_2 \\\\ V_3 \\end{bmatrix} = \\begin{bmatrix} I_1 \\\\ I_2 \\\\ I_3 \\end{bmatrix}$ is\n$$\n\\begin{bmatrix}\n\\frac{1}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & -1 \\\\\n0 & -1 & \\frac{3}{2}\n\\end{bmatrix}.\n$$\n\nB. If node $4$ is not grounded and all $4$ node potentials $(V_1,V_2,V_3,V_4)$ are unknown, then the KCL matrix $L$ is singular for any choice of current injections $I$ satisfying $\\sum_{i=1}^4 I_i = 0$. Its null space is one-dimensional, spanned by the vector of all ones, and physically this expresses that adding a uniform constant offset to all node potentials does not change any branch current.\n\nC. In the ungrounded network of nodes $1$–$4$, if the resistor between nodes $2$ and $3$ is removed (opened), the network decomposes into two disconnected components $\\{1,2\\}$ and $\\{3,4\\}$, and the KCL matrix $L$ becomes more rank-deficient: its null space has dimension $2$, spanned by vectors that are constant on each connected component. Physically, this corresponds to independent arbitrary reference shifts on each component without inducing any branch currents.\n\nD. Null space vectors of the KCL matrix represent nonzero circulating loop currents that can flow even when all node voltage differences are zero; therefore, the null space encodes mesh currents rather than voltage gauge freedoms.\n\nSelect all correct options (there may be more than one).", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and complete.\n\n**Step 1: Extract Givens**\n- Node labels: $1, 2, 3, 4$.\n- Resistor values: $R_{12}=2\\,\\Omega$, $R_{23}=1\\,\\Omega$, $R_{34}=2\\,\\Omega$.\n- Network topology: No direct connections between nodes $1$ and $3$, $1$ and $4$, or $2$ and $4$.\n- $V_i$: Electric potential at node $i$.\n- $I_i$: Externally injected current at node $i$.\n- Current from node $i$ to $j$: $I_{ij} = (V_i - V_j)/R_{ij}$.\n- Kirchhoff’s Current Law (KCL) at node $i$: $\\sum_{j \\in \\mathcal{N}(i)} (V_i - V_j)/R_{ij} = I_i$.\n- Matrix representation: $L V = I$, where $L$ is the Kirchhoff (graph Laplacian) matrix.\n- Conductance: $G_{ij} = 1/R_{ij}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on Ohm's Law and Kirchhoff's Current Law, which are fundamental principles of circuit theory. The matrix formulation using the graph Laplacian (or nodal admittance matrix) is a standard technique in computational physics and electrical engineering for solving such problems. The principles are correct.\n- **Well-Posed:** The network topology and component values are explicitly given. The problem asks to evaluate specific, verifiable statements about the resulting mathematical model.\n- **Objective:** The problem is stated in precise, technical language with no ambiguity or subjective elements.\n- **Completeness and Consistency:** All necessary information to construct the KCL matrix is provided. There are no contradictions in the setup.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. A solution can be derived.\n\n**Derivation of the Full KCL Matrix**\n\nThe KCL matrix $L$, also known as the nodal admittance matrix or the graph Laplacian, is constructed from the conductances $G_{ij} = 1/R_{ij}$.\nThe given resistances are $R_{12} = 2\\,\\Omega$, $R_{23} = 1\\,\\Omega$, and $R_{34} = 2\\,\\Omega$.\nThe corresponding conductances are:\n- $G_{12} = 1/R_{12} = 1/2$.\n- $G_{23} = 1/R_{23} = 1/1 = 1$.\n- $G_{34} = 1/R_{34} = 1/2$.\n\nThe elements of the $4 \\times 4$ matrix $L$ are defined as:\n- $L_{ii} = \\sum_{j \\neq i, j \\text{ connected to } i} G_{ij}$ (sum of conductances connected to node $i$).\n- $L_{ij} = -G_{ij}$ for $i \\neq j$ and nodes $i, j$ are connected.\n- $L_{ij} = 0$ for $i \\neq j$ and nodes $i, j$ are not connected.\n\nLet's compute the elements:\n- $L_{11} = G_{12} = 1/2$.\n- $L_{22} = G_{21} + G_{23} = 1/2 + 1 = 3/2$.\n- $L_{33} = G_{32} + G_{34} = 1 + 1/2 = 3/2$.\n- $L_{44} = G_{43} = 1/2$.\n\nThe off-diagonal elements are:\n- $L_{12} = L_{21} = -G_{12} = -1/2$.\n- $L_{23} = L_{32} = -G_{23} = -1$.\n- $L_{34} = L_{43} = -G_{34} = -1/2$.\n- All other off-diagonal elements ($L_{13}, L_{14}, L_{24}$, etc.) are $0$ because there are no direct connections.\n\nThe full KCL matrix $L$ is:\n$$\nL = \\begin{bmatrix}\n\\frac{1}{2} & -\\frac{1}{2} & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & -1 & 0 \\\\\n0 & -1 & \\frac{3}{2} & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{1}{2} & \\frac{1}{2}\n\\end{bmatrix}\n$$\nThe system of equations is $L V = I$, where $V = [V_1, V_2, V_3, V_4]^T$ and $I = [I_1, I_2, I_3, I_4]^T$.\n\n**Option-by-Option Analysis**\n\n**A. Evaluation of Option A**\nThis option considers the case where node $4$ is the reference (ground), meaning $V_4 = 0$. The reduced system involves the unknown potentials $(V_1, V_2, V_3)$. The procedure to find the reduced matrix $L_{\\mathrm{red}}$ is to remove the row and column corresponding to the grounded node from the full matrix $L$. In this case, we remove the $4$-th row and $4$-th column.\nThe full system is:\n$$\n\\begin{bmatrix}\n\\frac{1}{2} & -\\frac{1}{2} & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & -1 & 0 \\\\\n0 & -1 & \\frac{3}{2} & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{1}{2} & \\frac{1}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nV_1 \\\\ V_2 \\\\ V_3 \\\\ V_4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nI_1 \\\\ I_2 \\\\ I_3 \\\\ I_4\n\\end{bmatrix}\n$$\nWe only need to solve for the first three potentials, using the first three equations. Setting $V_4=0$ in the first three equations gives:\n$$\n\\begin{cases}\n\\frac{1}{2} V_1 - \\frac{1}{2} V_2 = I_1 \\\\\n-\\frac{1}{2} V_1 + \\frac{3}{2} V_2 - 1 V_3 = I_2 \\\\\n-1 V_2 + \\frac{3}{2} V_3 = I_3\n\\end{cases}\n$$\nThis corresponds to the matrix equation $L_{\\mathrm{red}} \\begin{bmatrix} V_1 \\\\ V_2 \\\\ V_3 \\end{bmatrix} = \\begin{bmatrix} I_1 \\\\ I_2 \\\\ I_3 \\end{bmatrix}$ with:\n$$\nL_{\\mathrm{red}} = \\begin{bmatrix}\n\\frac{1}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & -1 \\\\\n0 & -1 & \\frac{3}{2}\n\\end{bmatrix}\n$$\nThis matches the matrix given in option A precisely.\n**Verdict for A: Correct.**\n\n**B. Evaluation of Option B**\nThis option concerns the properties of the full $4 \\times 4$ matrix $L$. First, we check if $L$ is singular. A property of any graph Laplacian is that its row sums are zero.\nRow 1: $\\frac{1}{2} - \\frac{1}{2} + 0 + 0 = 0$.\nRow 2: $-\\frac{1}{2} + \\frac{3}{2} - 1 + 0 = 0$.\nRow 3: $0 - 1 + \\frac{3}{2} - \\frac{1}{2} = 0$.\nRow 4: $0 + 0 - \\frac{1}{2} + \\frac{1}{2} = 0$.\nSince all row sums are zero, the vector $\\mathbf{v} = [1, 1, 1, 1]^T$ is a right eigenvector with eigenvalue $0$, i.e., $L\\mathbf{v} = \\mathbf{0}$. This means $\\mathbf{v}$ is in the null space of $L$, and thus $L$ is singular.\nAccording to algebraic graph theory, the dimension of the null space of a graph Laplacian is equal to the number of connected components in the graph. The network $1-2-3-4$ is connected, so there is one connected component. Therefore, the null space is one-dimensional and is spanned by the vector $[1, 1, 1, 1]^T$.\nThe system $LV=I$ has a solution if and only if the vector $I$ is in the column space of $L$. For a symmetric matrix like $L$, the column space is the orthogonal complement of the null space. Thus, $I$ must be orthogonal to the null space basis vector $\\mathbf{v} = [1, 1, 1, 1]^T$. This condition is $\\mathbf{v}^T I = 0$, which means $I_1+I_2+I_3+I_4=0$. This is the overall KCL for an isolated network (no current lost to a global ground).\nThe physical meaning of the null space vector is that if $V$ is a solution, then $V+c\\mathbf{v}$ is also a solution for any constant $c$, because $L(V+c\\mathbf{v}) = LV + cL\\mathbf{v} = LV + \\mathbf{0} = I$. Adding a constant $c$ to all potentials $V_i$ does not change the potential differences $(V_i+c)-(V_j+c) = V_i-V_j$, and thus does not change any branch currents. This is a gauge freedom in setting the reference potential.\n**Verdict for B: Correct.**\n\n**C. Evaluation of Option C**\nThis option considers removing resistor $R_{23}$, which means its conductance $G_{23}$ becomes $0$. The network decomposes into two separate components: $\\{1,2\\}$ connected by $R_{12}$, and $\\{3,4\\}$ connected by $R_{34}$.\nThe new KCL matrix, $L'$, is obtained by setting $G_{23}=0$ in the original formulation.\n$L'_{11} = G_{12} = 1/2$.\n$L'_{22} = G_{21} = 1/2$.\n$L'_{33} = G_{34} = 1/2$.\n$L'_{44} = G_{43} = 1/2$.\n$L'_{12}=L'_{21}=-G_{12}=-1/2$.\n$L'_{34}=L'_{43}=-G_{34}=-1/2$.\nAll other off-diagonal elements are $0$.\n$$\nL' = \\begin{bmatrix}\n\\frac{1}{2} & -\\frac{1}{2} & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{2} & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{1}{2} & \\frac{1}{2}\n\\end{bmatrix}\n$$\nAs stated for option B, the dimension of the null space equals the number of connected components. Here, there are two components, so the null space dimension is $2$.\nThe null space is spanned by vectors that are constant on each component. A basis for this null space is given by $\\mathbf{v}_1 = [1, 1, 0, 0]^T$ and $\\mathbf{v}_2 = [0, 0, 1, 1]^T$. Any vector in the null space has the form $c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 = [c_1, c_1, c_2, c_2]^T$.\nPhysically, this corresponds to the ability to set the potential reference for component $\\{1,2\\}$ independently of the reference for component $\\{3,4\\}$. Shifting all potentials in $\\{1,2\\}$ by $c_1$ and all potentials in $\\{3,4\\}$ by $c_2$ does not change any potential differences within either component, hence no branch currents are induced.\n**Verdict for C: Correct.**\n\n**D. Evaluation of Option D**\nThis option claims that null space vectors represent \"nonzero circulating loop currents\" and encode \"mesh currents\". This is fundamentally incorrect.\nThe null space of the KCL matrix $L$ consists of potential vectors $V_{\\text{null}}$ such that $L V_{\\text{null}} = \\mathbf{0}$. As defined, $L V = I$, so these are potential configurations that require zero external current injection at every node.\nThe current through a branch between nodes $i$ and $j$ is $I_{ij} = (V_i - V_j)/R_{ij}$. A vector in the null space of a connected graph's Laplacian is of the form $V_{\\text{null}} = [c, c, ..., c]^T$. For such a potential vector, the potential difference across any resistor is $V_i - V_j = c - c = 0$. Therefore, all branch currents are zero. This directly contradicts the idea of \"nonzero circulating loop currents\".\nThe concept of loop currents belongs to mesh analysis, which is the dual of nodal analysis. Mesh analysis is based on Kirchhoff's Voltage Law (KVL). The null space of the KCL (nodal admittance) matrix is related to the choice of voltage reference (gauge freedom), not mesh currents. The statement confuses the mathematical and physical meaning of the null spaces of different matrices used in circuit analysis.\n**Verdict for D: Incorrect.**", "answer": "$$\\boxed{ABC}$$", "id": "2412372"}, {"introduction": "A mathematically well-defined problem does not always guarantee a computationally stable solution. This final practice confronts the critical issue of ill-conditioning, where small errors in the input data or during computation can be amplified to catastrophic levels in the final answer. You will investigate this phenomenon firsthand by attempting to solve a linear system involving the Hilbert matrix, a canonical example of an ill-conditioned matrix. By implementing the solution in both single-precision ($32$-bit) and double-precision ($64$-bit) arithmetic, you will quantify the dramatic loss of accuracy and understand the vital role of the matrix condition number, $\\kappa(H)$, in predicting the reliability of numerical solutions. [@problem_id:2412354]", "problem": "Consider the linear system that arises from discretizing a moment inversion problem with a kernel that leads to a Hilbert matrix. For a given integer size $n \\ge 1$, define the Hilbert matrix $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ with entries\n$$\nH_{ij} = \\frac{1}{i + j - 1}, \\quad \\text{for } i,j \\in \\{1,2,\\dots,n\\}.\n$$\nLet the exact (unknown) physical parameters be collected in a vector $\\mathbf{x}^{\\ast} \\in \\mathbb{R}^n$ with components $x^{\\ast}_i = 1$ for all $i \\in \\{1,2,\\dots,n\\}$. Define the right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$ by\n$$\n\\mathbf{b} = \\mathbf{H} \\mathbf{x}^{\\ast}.\n$$\nFor each test size $n$, you must solve the linear system\n$$\n\\mathbf{H} \\mathbf{x} = \\mathbf{b}\n$$\nin two floating-point precisions: single precision ($32$-bit) and double precision ($64$-bit), and quantify the sensitivity to rounding by the relative $2$-norm error of the computed solution vectors. Specifically, for a computed solution $\\hat{\\mathbf{x}}$, define the relative error\n$$\n\\varepsilon(\\hat{\\mathbf{x}}) = \\frac{\\lVert \\hat{\\mathbf{x}} - \\mathbf{x}^{\\ast} \\rVert_2}{\\lVert \\mathbf{x}^{\\ast} \\rVert_2}.\n$$\nAdditionally, characterize the ill-conditioning of $\\mathbf{H}$ by its spectral condition number in the $2$-norm,\n$$\n\\kappa_2(\\mathbf{H}) = \\frac{\\sigma_{\\max}(\\mathbf{H})}{\\sigma_{\\min}(\\mathbf{H})},\n$$\nwhere $\\sigma_{\\max}(\\mathbf{H})$ and $\\sigma_{\\min}(\\mathbf{H})$ are the largest and smallest singular values of $\\mathbf{H}$, respectively, obtained via the Singular Value Decomposition (SVD).\n\nYour task is to write a complete, runnable program that, for each test case, constructs $\\mathbf{H}$, forms $\\mathbf{b}$ from $\\mathbf{x}^{\\ast}$, solves for $\\mathbf{x}$ in both $32$-bit and $64$-bit floating-point arithmetic, computes the relative errors $\\varepsilon_{32}$ and $\\varepsilon_{64}$, and computes $\\kappa_2(\\mathbf{H})$ in $64$-bit arithmetic. The final output should aggregate the results for all test cases in a single line.\n\nTest suite:\n- Sizes $n$ to evaluate: $n \\in \\{3, 8, 12\\}$.\n\nFor each test case size $n$, the required output is a list containing three floating-point numbers in this order:\n- $\\varepsilon_{32}$: the relative $2$-norm error using $32$-bit arithmetic,\n- $\\varepsilon_{64}$: the relative $2$-norm error using $64$-bit arithmetic,\n- $\\kappa_2(\\mathbf{H})$: the spectral condition number of $\\mathbf{H}$ computed in $64$-bit arithmetic.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of the three-number lists for the test cases, in the order $n = 3$, then $n = 8$, then $n = 12$. The line must be a single Python-style list literal with comma-separated entries and square brackets, for example: \"[[e32_case1,e64_case1,cond_case1],[e32_case2,e64_case2,cond_case2],[e32_case3,e64_case3,cond_case3]]\".\n- All values must be expressed as dimensionless floating-point numbers in decimal or scientific notation.", "solution": "The problem presented is a well-posed exercise in numerical linear algebra, designed to demonstrate the critical impact of matrix conditioning and finite-precision arithmetic on the solution of linear systems. The task is to solve the system $\\mathbf{H} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{H}$ is the Hilbert matrix, for several matrix sizes $n$ and to quantify the degradation of solution accuracy with increasing ill-conditioning and decreasing floating-point precision.\n\nFirst, let us define the components of the problem with mathematical rigor. The Hilbert matrix $\\mathbf{H}$ of size $n \\times n$ is a symmetric matrix with entries given by:\n$$\nH_{ij} = \\frac{1}{i + j - 1}, \\quad \\text{for } i, j \\in \\{1, 2, \\dots, n\\}.\n$$\nThese matrices are canonical examples of ill-conditioned systems. The problem specifies an exact solution vector $\\mathbf{x}^{\\ast} \\in \\mathbb{R}^n$ where every component is unity, i.e., $x^{\\ast}_i = 1$ for all $i$. The right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$ is constructed from the exact solution, ensuring consistency:\n$$\n\\mathbf{b} = \\mathbf{H} \\mathbf{x}^{\\ast}.\n$$\nThe components of $\\mathbf{b}$ are therefore $b_i = \\sum_{j=1}^n H_{ij} x^{\\ast}_j = \\sum_{j=1}^n \\frac{1}{i+j-1}$.\n\nThe central task is to numerically solve for $\\mathbf{x}$ in the system $\\mathbf{H} \\mathbf{x} = \\mathbf{b}$ and compare the computed solution, which we denote $\\hat{\\mathbf{x}}$, against the known true solution $\\mathbf{x}^{\\ast}$. The deviation from the true solution is caused by two compounding sources of error in numerical computation:\n$1$. **Representation Error**: The fractional entries of $\\mathbf{H}$ and $\\mathbf{b}$ cannot always be stored exactly in a finite-precision binary floating-point format (e.g., $32$-bit or $64$-bit).\n$2$. **Round-off Error Amplification**: During the execution of a linear solver algorithm (such as LU decomposition followed by substitution), further round-off errors are introduced at each arithmetic step. The ill-conditioning of the matrix $\\mathbf{H}$ dramatically amplifies these small initial and intermediate errors.\n\nThe degree of this amplification is quantified by the condition number of the matrix. The problem specifies the spectral or $2$-norm condition number, $\\kappa_2(\\mathbf{H})$, defined as:\n$$\n\\kappa_2(\\mathbf{H}) = \\lVert \\mathbf{H} \\rVert_2 \\lVert \\mathbf{H}^{-1} \\rVert_2 = \\frac{\\sigma_{\\max}(\\mathbf{H})}{\\sigma_{\\min}(\\mathbf{H})},\n$$\nwhere $\\sigma_{\\max}(\\mathbf{H})$ and $\\sigma_{\\min}(\\mathbf{H})$ are the largest and smallest singular values of $\\mathbf{H}$, respectively. A large value of $\\kappa_2(\\mathbf{H})$ indicates that small relative errors in the input data (vector $\\mathbf{b}$) can lead to large relative errors in the output solution (vector $\\mathbf{x}$). The condition number of the Hilbert matrix grows extremely rapidly with $n$; for example, $\\kappa_2(\\mathbf{H})$ is on the order of $10^{16}$ for $n=12$, which is approximately the inverse of machine epsilon for $64$-bit double-precision arithmetic. This implies a catastrophic loss of precision.\n\nWe will quantify the accuracy of a computed solution $\\hat{\\mathbf{x}}$ using the relative $2$-norm error, $\\varepsilon(\\hat{\\mathbf{x}})$:\n$$\n\\varepsilon(\\hat{\\mathbf{x}}) = \\frac{\\lVert \\hat{\\mathbf{x}} - \\mathbf{x}^{\\ast} \\rVert_2}{\\lVert \\mathbf{x}^{\\ast} \\rVert_2}.\n$$\nSince $\\mathbf{x}^{\\ast}$ is a vector of $n$ ones, its $2$-norm is $\\lVert \\mathbf{x}^{\\ast} \\rVert_2 = \\sqrt{\\sum_{i=1}^n 1^2} = \\sqrt{n}$. The error is thus computed as $\\varepsilon(\\hat{\\mathbf{x}}) = \\frac{1}{\\sqrt{n}} \\lVert \\hat{\\mathbf{x}} - \\mathbf{x}^{\\ast} \\rVert_2$.\n\nThe computational procedure for each test size $n \\in \\{3, 8, 12\\}$ is as follows:\n$1$. **Problem Construction**: Construct the matrix $\\mathbf{H}$ and the vector $\\mathbf{x}^{\\ast}$ using $64$-bit double-precision floating-point numbers for maximum fidelity. Compute the right-hand side vector $\\mathbf{b} = \\mathbf{H} \\mathbf{x}^{\\ast}$.\n$2$. **Condition Number Calculation**: Compute the singular values of the $64$-bit matrix $\\mathbf{H}$ to find $\\kappa_2(\\mathbf{H}) = \\sigma_{\\max}(\\mathbf{H}) / \\sigma_{\\min}(\\mathbf{H})$.\n$3$. **Single-Precision Solution**: Cast the matrix $\\mathbf{H}$ and vector $\\mathbf{b}$ to $32$-bit single-precision format ($H_{32}$ and $b_{32}$). Solve the linear system $H_{32} \\hat{\\mathbf{x}}_{32} = b_{32}$ to obtain the single-precision solution $\\hat{\\mathbf{x}}_{32}$. Calculate the corresponding relative error $\\varepsilon_{32}$.\n$4$. **Double-Precision Solution**: Solve the original $64$-bit system $\\mathbf{H} \\hat{\\mathbf{x}}_{64} = \\mathbf{b}$ to obtain the double-precision solution $\\hat{\\mathbf{x}}_{64}$. Calculate the relative error $\\varepsilon_{64}$.\n$5$. **Result Aggregation**: Collate the three computed values: the single-precision relative error $\\varepsilon_{32}$, the double-precision relative error $\\varepsilon_{64}$, and the condition number $\\kappa_2(\\mathbf{H})$.\n\nThis procedure will be executed for each specified size $n$, and the results will be presented as required, illustrating the stark difference in solution quality between $32$-bit and $64$-bit arithmetic and how this difference is governed by the enormous condition numbers of the Hilbert matrices. For larger $n$, we expect $\\varepsilon_{32}$ to be greater than $1$, indicating that the solution has no correct digits. Even $\\varepsilon_{64}$ will grow significantly, reflecting the theoretical limits of computation for such ill-posed problems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Hilbert matrix linear system problem for specified sizes,\n    calculating errors for single and double precision and the condition number.\n    \"\"\"\n    \n    # Test suite: sizes n to evaluate.\n    test_cases = [3, 8, 12]\n\n    results = []\n    \n    for n in test_cases:\n        # Step 1: Construct the problem in 64-bit precision (double).\n        # This provides the most accurate \"ground truth\" for the matrix H and vector b.\n        \n        # Create an array of indices from 1 to n for vectorized matrix construction.\n        i = np.arange(1, n + 1, dtype=np.float64).reshape(-1, 1)\n        j = np.arange(1, n + 1, dtype=np.float64)\n        \n        # Construct the Hilbert matrix H using broadcasting.\n        H_64 = 1.0 / (i + j - 1)\n        \n        # Define the exact solution x* (a vector of ones).\n        x_star_64 = np.ones(n, dtype=np.float64)\n        \n        # Compute the right-hand side vector b = H * x*.\n        b_64 = H_64 @ x_star_64\n        \n        # Step 2: Compute the spectral condition number of H in 64-bit precision.\n        # np.linalg.cond(H, 2) computes the 2-norm condition number, which is\n        # the ratio of the largest to smallest singular values.\n        kappa_H = np.linalg.cond(H_64, 2)\n        \n        # Step 3: Solve in 32-bit precision (single) and compute the error.\n        \n        # Cast the 64-bit matrix and vector to 32-bit.\n        H_32 = H_64.astype(np.float32)\n        b_32 = b_64.astype(np.float32)\n        \n        # Solve the linear system H_32 * x_hat_32 = b_32.\n        x_hat_32 = np.linalg.solve(H_32, b_32)\n        \n        # To accurately compute the error, cast the 32-bit solution back to 64-bit\n        # before subtracting it from the exact 64-bit solution vector.\n        error_vec_32 = x_hat_32.astype(np.float64) - x_star_64\n        \n        # Compute the relative 2-norm error.\n        norm_x_star = np.linalg.norm(x_star_64, 2)\n        e_32 = np.linalg.norm(error_vec_32, 2) / norm_x_star\n        \n        # Step 4: Solve in 64-bit precision (double) and compute the error.\n        \n        # Solve the linear system H_64 * x_hat_64 = b_64.\n        x_hat_64 = np.linalg.solve(H_64, b_64)\n        \n        # Compute the error vector.\n        error_vec_64 = x_hat_64 - x_star_64\n        \n        # Compute the relative 2-norm error.\n        e_64 = np.linalg.norm(error_vec_64, 2) / norm_x_star\n        \n        # Step 5: Collect results for this case.\n        results.append([e_32, e_64, kappa_H])\n\n    # Final print statement in the exact required format.\n    # The output is a string representation of a Python list of lists.\n    print(f\"[{','.join(f'[{e32},{e64},{k}]' for e32, e64, k in results)}]\")\n\nsolve()\n\n```", "id": "2412354"}]}