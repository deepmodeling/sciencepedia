{"hands_on_practices": [{"introduction": "To truly grasp the differences between stationary iterative methods, there is no substitute for implementing them. This first exercise guides you through solving the two-dimensional Laplace equation, a cornerstone problem in computational physics, using the Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) methods. By comparing the number of iterations required for each, you will gain a direct, quantitative understanding of their relative performance and see firsthand the dramatic acceleration offered by SOR.", "problem": "Consider the two-dimensional Laplace equation $\\nabla^2 u = 0$ on the square domain $\\Omega = (0,1)\\times(0,1)$ with Dirichlet boundary conditions. Let the boundary data be $u(x,0) = \\sin(\\pi x)$, $u(x,1) = 0$, $u(0,y)=0$, and $u(1,y)=0$, where angles are in radians. Discretize the domain using a uniform grid with $N$ interior points in each spatial direction (so that the grid spacing is $h = \\frac{1}{N+1}$ and there are $(N+2)\\times(N+2)$ total grid points including the boundary). Use the standard second-order centered finite-difference approximation for the Laplacian to obtain a linear system for the interior unknowns. Starting from the discrete Laplace operator definition derived from the continuous equation and central differences, implement three stationary iterative methods to solve the discrete equations: the Jacobi method, the Gauss–Seidel method, and the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega$ satisfying $0  \\omega  2$. For each method, use the following fundamental base and definitions:\n\n- The discrete Laplace equation at an interior grid point $(i,j)$ is obtained from the central-difference approximation to $\\nabla^2 u = 0$:\n$$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0,$$\nwhich is algebraically equivalent to the stencil equation\n$$-u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} + 4 u_{i,j} = 0.$$\n- From this, define the discrete residual on the interior as\n$$r_{i,j} = 4u_{i,j} - \\left(u_{i+1,j}+u_{i-1,j}+u_{i,j+1}+u_{i,j-1}\\right),$$\nand its infinity norm as $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$.\n- Use the stopping rule $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol}\\cdot \\|r^{(0)}\\|_{\\infty}$, where $\\text{tol}$ is a given tolerance and $k$ is the iteration index, with the initial guess $u^{(0)}$ equal to zero on all interior points and fixed boundary values on the boundary points.\n\nYour program must:\n- Implement the Jacobi iteration that updates all interior values using only the previous iteration’s values.\n- Implement the Gauss–Seidel iteration in a way that uses the most recently available neighbor values (you may use a red–black ordering to achieve this).\n- Implement the SOR iteration using red–black ordering with relaxation parameter $\\omega$ via\n$$u^{(k+1)}_{i,j} = u^{(k)}_{i,j} + \\omega\\left(\\frac{1}{4}\\left(u^{(*)}_{i+1,j}+u^{(*)}_{i-1,j}+u^{(*)}_{i,j+1}+u^{(*)}_{i,j-1}\\right) - u^{(k)}_{i,j}\\right),$$\nwhere $u^{(*)}$ denotes the most up-to-date values consistent with Gauss–Seidel ordering on each color. Take $\\omega = 1$ to recover Gauss–Seidel.\n\nYour task is to compare the convergence rates of the three methods quantitatively by reporting, for each test case, the number of iterations required by each method to satisfy the stopping rule. Use the same discretization, boundary conditions, and stopping criterion for all methods, and report iteration counts as integers.\n\nTest suite. Run your program on the following parameter sets, where each test case is a triple $(N, \\text{tol}, \\omega)$:\n- Test $1$: $(20, 10^{-5}, 1.5)$.\n- Test $2$: $(20, 10^{-5}, 1.0)$.\n- Test $3$: $(10, 10^{-8}, 1.8)$.\n- Test $4$: $(40, 10^{-4}, 1.9)$.\n\nFor each test case, your program must produce a list $[n_J, n_{GS}, n_{SOR}]$ containing the iteration counts required by the Jacobi, Gauss–Seidel, and SOR methods, respectively, to meet the stopping rule. Aggregate the results of all tests into a single line as a comma-separated list of these lists, with no spaces, enclosed in square brackets. For example, your output must look exactly like\n$[[n_J^{(1)},n_{GS}^{(1)},n_{SOR}^{(1)}],[n_J^{(2)},n_{GS}^{(2)},n_{SOR}^{(2)}],[n_J^{(3)},n_{GS}^{(3)},n_{SOR}^{(3)}],[n_J^{(4)},n_{GS}^{(4)},n_{SOR}^{(4)}]]$,\nprinted as a single line. No physical units are involved. Angles in the sine function must be interpreted in radians.", "solution": "The problem as stated is a standard, well-posed exercise in the numerical solution of elliptic partial differential equations. It is scientifically sound, self-contained, and algorithmically specified. There are no contradictions, ambiguities, or factual errors. Therefore, we proceed directly to the solution.\n\nThe problem requires solving the two-dimensional Laplace equation $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$ on a unit square domain $\\Omega = (0,1)\\times(0,1)$. The potential $u(x,y)$ is subject to Dirichlet boundary conditions: $u(x,0) = \\sin(\\pi x)$, and $u=0$ on the other three boundaries.\n\nThe first step is to discretize the continuous problem. The domain is covered by a uniform grid with $(N+2) \\times (N+2)$ points, where $N$ is the number of interior points in each direction. The grid spacing is $h = \\frac{1}{N+1}$. A grid point is denoted by $(x_i, y_j) = (ih, jh)$ for indices $i,j \\in \\{0, 1, \\dots, N+1\\}$. The value of the potential at this point is $u_{i,j} \\approx u(x_i, y_j)$.\n\nThe Laplacian operator $\\nabla^2$ is approximated at each interior grid point $(i,j)$ using the second-order central difference formula:\n$$ \\nabla^2 u \\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} $$\nSetting this approximation to $0$ gives the discrete Laplace equation for the interior points ($1 \\le i,j \\le N$):\n$$ u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0 $$\nThis five-point stencil equation can be rearranged to express $u_{i,j}$ as the average of its four neighbors:\n$$ u_{i,j} = \\frac{1}{4} \\left( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\right) $$\nThis set of $N^2$ linear equations for the $N^2$ interior unknown values forms a large, sparse linear system of the form $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the unknowns $u_{i,j}$ and the matrix $A$ represents the discrete Laplacian operator. The right-hand side vector $\\mathbf{b}$ incorporates the fixed boundary values. Such systems are well-suited for solution by iterative methods.\n\nWe are tasked to implement three classical stationary iterative methods: Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR). These methods start with an initial guess $u^{(0)}$ and generate a sequence of approximations $u^{(k)}$ that converges to the true solution.\n\nThe Jacobi method is the simplest iterative scheme. For each point $(i,j)$, the new value $u_{i,j}^{(k+1)}$ is computed using only the values from the previous iteration, $u^{(k)}$. The update rule is a direct application of the averaging formula:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} \\right) $$\nThis update can be performed for all interior points simultaneously (or in any order), as the calculation for each point is independent of the others within the same iteration. In a vectorized implementation, a full copy of the grid from iteration $k$ is required to compute the grid for iteration $k+1$.\n\nThe Gauss-Seidel method improves upon Jacobi by using the most recently computed values within the current iteration. The update rule is:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(*)} + u_{i-1,j}^{(*)} + u_{i,j+1}^{(*)} + u_{i,j-1}^{(*)} \\right) $$\nwhere $u^{(*)}$ denotes the most up-to-date value available. For example, in a lexicographical ordering (row by row, column by column), the calculation for $u_{i,j}^{(k+1)}$ would use $u_{i-1,j}^{(k+1)}$ and $u_{i,j-1}^{(k+1)}$ from the current iteration $k+1$, and $u_{i+1,j}^{(k)}$ and $u_{i,j+1}^{(k)}$ from the previous iteration $k$. This dependency on the update order makes parallelization complex. The red-black ordering scheme circumvents this. The grid points are colored like a checkerboard. All \"red\" points are updated first, using values from their \"black\" neighbors (from the previous iteration). Then, all \"black\" points are updated, using the newly computed values from their \"red\" neighbors. Each of the two stages (red update, black update) can be fully vectorized.\n\nThe Successive Over-Relaxation (SOR) method is an extrapolation of the Gauss-Seidel method, designed to accelerate convergence. It computes the Gauss-Seidel update, and then pushes the solution further in that direction, controlled by a relaxation parameter $\\omega$. The update formula is:\n$$ u_{i,j}^{(k+1)} = u_{i,j}^{(k)} + \\omega \\left( u_{i,j}^{\\text{GS}} - u_{i,j}^{(k)} \\right) = (1-\\omega)u_{i,j}^{(k)} + \\omega u_{i,j}^{\\text{GS}} $$\nwhere $u_{i,j}^{\\text{GS}}$ is the value that would be computed by the Gauss-Seidel step at that point. Like Gauss-Seidel, SOR is implemented using the red-black ordering to efficiently use the most recent values. When $\\omega=1$, the SOR method reduces exactly to the Gauss-Seidel method. For Laplacetype problems, choosing an optimal $\\omega$ in the range $1  \\omega  2$ (over-relaxation) typically leads to a significant speedup in convergence.\n\nThe stopping criterion is based on the infinity norm of the discrete residual, defined as $\\|r^{(k)}\\|_{\\infty} = \\max_{i,j} |r_{i,j}^{(k)}|$, where $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})$. The iteration stops when $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol} \\cdot \\|r^{(0)}\\|_{\\infty}$, where $\\text{tol}$ is a given tolerance and $\\|r^{(0)}\\|_{\\infty}$ is the residual norm of the initial guess ($u^{(0)}=0$ on the interior). This relative criterion ensures a fair comparison between different problem setups.\n\nThe implementation consists of three main functions. One function sets up the $(N+2) \\times (N+2)$ grid, initializing the interior to $0$ and setting the boundary conditions. A second function implements the Jacobi iteration. A third function implements the SOR iteration with red-black ordering, which is also used for the Gauss-Seidel method by setting $\\omega=1$. A helper function calculates the residual norm at each step. The main program iterates through the test cases, calls the appropriate solver functions, and records the number of iterations required for convergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_initial_state(N):\n    \"\"\"\n    Initializes the grid with boundary conditions and zero interior.\n\n    Args:\n        N (int): Number of interior points in each direction.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The (N+2)x(N+2) grid `u`.\n            - float: The grid spacing `h`.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    u = np.zeros((N + 2, N + 2))\n    \n    # Set boundary condition u(x,0) = sin(pi*x)\n    # The j=0 row corresponds to y=0.\n    x_coords = np.linspace(0, 1, N + 2)\n    u[0, :] = np.sin(np.pi * x_coords)\n    \n    # Other boundaries u(x,1)=0, u(0,y)=0, u(1,y)=0 are already zero.\n    return u, h\n\ndef calculate_residual_norm(u, N):\n    \"\"\"\n    Calculates the infinity norm of the residual on the interior grid.\n\n    Args:\n        u (np.ndarray): The full (N+2)x(N+2) grid.\n        N (int): Number of interior grid points.\n\n    Returns:\n        float: The infinity norm of the residual.\n    \"\"\"\n    interior = u[1:N + 1, 1:N + 1]\n    neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                     u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n    residual = 4 * interior - neighbors_sum\n    return np.max(np.abs(residual))\n\ndef solve_jacobi(N, tol):\n    \"\"\"\n    Solves the Laplace equation using the Jacobi method.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n    \n    threshold = tol * r0_norm\n    \n    k = 0\n    while True:\n        k += 1\n        \n        u_old = u.copy()\n        \n        neighbors_sum = (u_old[1:N + 1, 2:N + 2] + u_old[1:N + 1, 0:N] +\n                         u_old[2:N + 2, 1:N + 1] + u_old[0:N, 1:N + 1])\n        u[1:N + 1, 1:N + 1] = 0.25 * neighbors_sum\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve_sor(N, tol, omega):\n    \"\"\"\n    Solves the Laplace equation using SOR with red-black ordering.\n    Recovers Gauss-Seidel for omega=1.0.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n        omega (float): Relaxation parameter.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n        \n    threshold = tol * r0_norm\n    \n    # Create red-black masks for the interior (N x N) grid.\n    # (j, i) indices for the interior part start from 0.\n    # Grid point (j_grid, i_grid) where j_grid, i_grid in [1,N]\n    # corresponds to mask point (j_grid-1, i_grid-1).\n    # Color depends on (j_grid + i_grid). (j_grid-1) + (i_grid-1) has same parity.\n    I, J = np.meshgrid(np.arange(N), np.arange(N))\n    red_mask = (I + J) % 2 == 0\n    black_mask = ~red_mask\n    \n    k = 0\n    while True:\n        k += 1\n        \n        # Keep a copy of the interior from the start of the iteration\n        # for the (1-omega) term.\n        u_old_interior = u[1:N + 1, 1:N + 1].copy()\n\n        # Update red points. Neighbors are black, use values from start of iteration.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][red_mask] = (1 - omega) * u_old_interior[red_mask] + \\\n                                      omega * gs_update[red_mask]\n\n        # Update black points. Neighbors are red, use newly updated values.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][black_mask] = (1 - omega) * u_old_interior[black_mask] + \\\n                                        omega * gs_update[black_mask]\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (20, 1e-5, 1.5),\n        (20, 1e-5, 1.0),\n        (10, 1e-8, 1.8),\n        (40, 1e-4, 1.9),\n    ]\n\n    results = []\n    for N, tol, omega_sor in test_cases:\n        # Calculate iterations for Jacobi\n        n_J = solve_jacobi(N, tol)\n        \n        # Calculate iterations for Gauss-Seidel (SOR with omega=1.0)\n        n_GS = solve_sor(N, tol, 1.0)\n        \n        # Calculate iterations for SOR with the specified omega\n        n_SOR = solve_sor(N, tol, omega_sor)\n        \n        results.append([n_J, n_GS, n_SOR])\n\n    # Format the output string as specified: [[r1,r2,r3],[...],...]\n    formatted_results = [f'[{\",\".join(map(str, r))}]' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406769"}, {"introduction": "After observing that some methods converge faster than others [@problem_id:2406769], we now investigate the theoretical reason for this behavior. This practice connects the abstract concept of the spectral radius, $\\rho(T)$, to the concrete performance of an iterative solver. By testing the Jacobi and Gauss-Seidel methods on matrices with varying degrees of diagonal dominance, you will explore how matrix properties influence the spectral radius and, consequently, dictate the speed of convergence.", "problem": "You must write a complete, runnable program that evaluates linear stationary iterations for specified linear systems. Consider square, real, symmetric matrices of size $n \\times n$ with right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$. For each system, starting from the zero vector $\\mathbf{x}^{(0)} = \\mathbf{0}$, iterate until the infinity norm of the residual $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A \\mathbf{x}^{(k)}\\|_{\\infty}$ is less than or equal to a tolerance $\\tau$, or until a maximum number of iterations $k_{\\max}$ is reached. Use the standard definitions of the Jacobi method and the Gauss–Seidel method. For each specified matrix, report the minimal iteration counts required by the Jacobi and Gauss–Seidel methods to satisfy the residual tolerance, and the spectral radius of the Jacobi iteration matrix. All computations are purely numerical, and no physical units are involved.\n\nUse the following parameters, which constitute the test suite:\n\n- Dimension $n = 6$.\n- Right-hand side $\\mathbf{b} = [1,2,3,4,5,6]^T$.\n- Initial guess $\\mathbf{x}^{(0)} = \\mathbf{0}$.\n- Residual tolerance $\\tau = 10^{-8}$.\n- Maximum iterations $k_{\\max} = 20000$.\n\nDefine three matrices $A \\in \\mathbb{R}^{6 \\times 6}$ as follows:\n\n1) Strongly diagonally dominant, dense with constant off-diagonals:\n- Parameters $\\alpha_{\\mathrm{s}} = 20.0$, $\\gamma = -1.0$.\n- Entries:\n  - $A_{ii} = \\alpha_{\\mathrm{s}}$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{ij} = \\gamma$ for all $i \\neq j$.\n\n2) Just-barely diagonally dominant, dense with constant off-diagonals:\n- Parameters $\\alpha_{\\mathrm{w}} = 5.1$, $\\gamma = -1.0$.\n- Entries:\n  - $A_{ii} = \\alpha_{\\mathrm{w}}$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{ij} = \\gamma$ for all $i \\neq j$.\n\n3) Edge case, symmetric tridiagonal (one-dimensional discrete Laplacian form):\n- Entries:\n  - $A_{ii} = 2.0$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{i,i+1} = A_{i+1,i} = -1.0$ for all $i \\in \\{1,\\dots,5\\}$.\n  - All other off-diagonal entries are $0.0$.\n\nFor each of the three matrices, do the following:\n- Using the Jacobi method, determine the minimal iteration count $k_{\\mathrm{J}}$ such that $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{J}})}\\|_{\\infty} \\le \\tau$, or return $k_{\\max}$ if such $k_{\\mathrm{J}}$ is not achieved within $k_{\\max}$ iterations.\n- Using the Gauss–Seidel method, determine the minimal iteration count $k_{\\mathrm{GS}}$ such that $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{GS}})}\\|_{\\infty} \\le \\tau$, or return $k_{\\max}$ if such $k_{\\mathrm{GS}}$ is not achieved within $k_{\\max}$ iterations.\n- For the Jacobi method, compute the spectral radius $\\rho_{\\mathrm{J}}$ of its iteration matrix. Report $\\rho_{\\mathrm{J}}$ rounded to six decimal places.\n\nYour program should produce a single line of output containing the nine results in the following order:\n- For the strongly diagonally dominant matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n- For the just-barely diagonally dominant matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n- For the tridiagonal edge-case matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n\nThe final output format must be a single line that is a comma-separated list enclosed in square brackets, for example\n$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$,\nwhere $k_{\\mathrm{J}}$ and $k_{\\mathrm{GS}}$ are integers and each $\\rho_{\\mathrm{J}}$ is a floating-point number rounded to six decimal places.", "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically grounded in the field of numerical linear algebra, well-posed with all necessary parameters defined, and objective in its formulation. The problem asks for the implementation and evaluation of two fundamental linear stationary iterative methods, Jacobi and Gauss–Seidel, for solving a system of linear equations $A\\mathbf{x} = \\mathbf{b}$.\n\nThese methods are based on splitting the matrix $A$ into its constituent parts. A square matrix $A$ can be decomposed as $A = D + L + U$, where $D$ is a diagonal matrix containing the diagonal elements of $A$, $L$ is a strictly lower triangular matrix, and $U$ is a strictly upper triangular matrix. The system $A\\mathbf{x} = \\mathbf{b}$ can thus be written as $(D+L+U)\\mathbf{x} = \\mathbf{b}$.\n\n**Jacobi Method**\n\nThe Jacobi method rearranges the system as $D\\mathbf{x} = \\mathbf{b} - (L+U)\\mathbf{x}$. This leads to the iterative scheme:\n$$ D\\mathbf{x}^{(k+1)} = \\mathbf{b} - (L+U)\\mathbf{x}^{(k)} $$\nAssuming $D$ is invertible (i.e., no zero diagonal elements, which is true for all matrices in this problem), we obtain the iteration formula:\n$$ \\mathbf{x}^{(k+1)} = D^{-1}(\\mathbf{b} - (L+U)\\mathbf{x}^{(k)}) $$\nThis can be computed component-wise for each element $i$ of the vector $\\mathbf{x}^{(k+1)}$:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^{n} A_{ij} x_j^{(k)} \\right) $$\nA key characteristic of the Jacobi method is that the computation of each component $x_i^{(k+1)}$ depends only on the components of the vector from the previous iteration, $\\mathbf{x}^{(k)}$. This allows for parallel computation of the new vector components.\n\n**Gauss–Seidel Method**\n\nThe Gauss–Seidel method aims to improve the convergence rate by using the most up-to-date information available. It rearranges the system as $(D+L)\\mathbf{x} = \\mathbf{b} - U\\mathbf{x}$, leading to the iterative scheme:\n$$ (D+L)\\mathbf{x}^{(k+1)} = \\mathbf{b} - U\\mathbf{x}^{(k)} $$\nThis yields the iteration formula:\n$$ \\mathbf{x}^{(k+1)} = (D+L)^{-1}(\\mathbf{b} - U\\mathbf{x}^{(k)}) $$\nIn practice, this is implemented as a forward substitution. The component-wise formula is:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} A_{ij} x_j^{(k)} \\right) $$\nNotice that for computing $x_i^{(k+1)}$, we use the newly computed components $x_j^{(k+1)}$ for $j  i$ from the current iteration $k+1$, and the old components $x_j^{(k)}$ for $j  i$ from the previous iteration $k$. This sequential dependency means the components must be updated in order.\n\n**Convergence and Spectral Radius**\n\nAny linear stationary iteration can be written in the form $\\mathbf{x}^{(k+1)} = T \\mathbf{x}^{(k)} + \\mathbf{c}$, where $T$ is the iteration matrix. The method is guaranteed to converge for any initial guess $\\mathbf{x}^{(0)}$ if and only if the spectral radius of the iteration matrix, $\\rho(T)$, is strictly less than $1$. The spectral radius is defined as the maximum absolute value of the eigenvalues of $T$, i.e., $\\rho(T) = \\max_i |\\lambda_i(T)|$.\n\nFor the Jacobi method, the iteration matrix $T_J$ is given by:\n$$ T_J = -D^{-1}(L+U) = I - D^{-1}A $$\nThe spectral radius $\\rho(T_J)$ dictates the convergence of the Jacobi method. A smaller spectral radius implies a faster asymptotic rate of convergence. The problem requires the calculation of this value.\n\n**Implementation Strategy**\n\nThe solution will be implemented in Python using the `numpy` library.\n1.  **Matrix Construction**: The three specified matrices, $A_1$ (strongly diagonally dominant), $A_2$ (just-barely diagonally dominant), and $A_3$ (tridiagonal), will be constructed as `numpy` arrays. The problem parameters $n=6$, $\\mathbf{b}=[1,2,3,4,5,6]^T$, $\\mathbf{x}^{(0)}=\\mathbf{0}$, $\\tau=10^{-8}$, and $k_{\\max}=20000$ will be defined.\n2.  **Iterative Solvers**: Functions for the Jacobi and Gauss–Seidel methods will be implemented. Each function will take a matrix $A$, vector $\\mathbf{b}$, initial guess $\\mathbf{x}^{(0)}$, tolerance $\\tau$, and maximum iterations $k_{\\max}$ as input. The loop will run from $k=1$ to $k_{\\max}$, updating the solution vector $\\mathbf{x}$ at each step. After each update, the infinity norm of the residual, $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\|_{\\infty}$, will be checked against the tolerance $\\tau$. If the condition is met, the current iteration count $k$ is returned. If the loop completes without convergence, $k_{\\max}$ is returned. An initial check for $k=0$ is also performed.\n3.  **Spectral Radius Calculation**: A function will compute the Jacobi iteration matrix $T_J = I - D^{-1}A$. The eigenvalues of $T_J$ will be found using `numpy.linalg.eigvals`, and the spectral radius will be the maximum of their absolute values.\n4.  **Execution and Output**: The main part of the program will iterate through the three test cases (matrices). For each case, it will call the solver functions to get the iteration counts $k_J$ and $k_{GS}$, and the spectral radius function for $\\rho_J$. The results will be collected and formatted into a single string as specified in the problem statement.", "answer": "```python\nimport numpy as np\n\ndef jacobi(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) - int:\n    \"\"\"\n    Solves the system Ax=b using the Jacobi method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    D = np.diag(A)\n    R = A - np.diag(D)  # R = L + U\n\n    for k in range(1, k_max + 1):\n        x_new = (b - R @ x) / D\n        x = x_new\n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n    \n    return k_max\n\ndef gauss_seidel(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) - int:\n    \"\"\"\n    Solves the system Ax=b using the Gauss-Seidel method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    for k in range(1, k_max + 1):\n        x_old = x.copy()\n        for i in range(n):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x_old[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n        \n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n            \n    return k_max\n\ndef get_spectral_radius_J(A: np.ndarray) - float:\n    \"\"\"\n    Computes the spectral radius of the Jacobi iteration matrix.\n\n    Args:\n        A: The n x n coefficient matrix.\n\n    Returns:\n        The spectral radius of the Jacobi matrix T_J.\n    \"\"\"\n    D = np.diag(np.diag(A))\n    D_inv = np.linalg.inv(D)\n    T_J = np.eye(A.shape[0]) - D_inv @ A\n    eigenvalues = np.linalg.eigvals(T_J)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters\n    n = 6\n    b = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    x0 = np.zeros(n)\n    tol = 1e-8\n    k_max = 20000\n\n    # Define the three matrices\n    test_cases = []\n\n    # Case 1: Strongly diagonally dominant\n    alpha_s = 20.0\n    gamma1 = -1.0\n    A1 = np.full((n, n), gamma1)\n    np.fill_diagonal(A1, alpha_s)\n    test_cases.append(A1)\n\n    # Case 2: Just-barely diagonally dominant\n    alpha_w = 5.1\n    gamma2 = -1.0\n    A2 = np.full((n, n), gamma2)\n    np.fill_diagonal(A2, alpha_w)\n    test_cases.append(A2)\n\n    # Case 3: Tridiagonal edge case\n    A3 = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n    test_cases.append(A3)\n\n    results = []\n    for A in test_cases:\n        k_J = jacobi(A, b, x0, tol, k_max)\n        k_GS = gauss_seidel(A, b, x0, tol, k_max)\n        rho_J = get_spectral_radius_J(A)\n\n        results.append(k_J)\n        results.append(k_GS)\n        results.append(round(rho_J, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406932"}, {"introduction": "The SOR method's power is unlocked by choosing its relaxation parameter, $\\omega$, wisely. This final exercise elevates your understanding from using the method to optimizing it. You will numerically determine the optimal parameter, $\\omega_{\\text{opt}}$, by finding the value that minimizes the spectral radius of the SOR iteration matrix, a crucial technique for tuning solver performance in practical scientific computing applications.", "problem": "You are asked to write a complete, runnable program that numerically estimates the optimal Successive Over-Relaxation (SOR) parameter for solving a finite-difference discretization of the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions. The goal is to determine, for several grid sizes, the value of the over-relaxation parameter that minimizes the spectral radius of the SOR iteration matrix.\n\nConsider the linear system arising from the standard five-point finite-difference discretization of the Laplace operator on the interior of a square grid with homogeneous Dirichlet boundary conditions. Let there be $N$ interior points along each spatial dimension, with lexicographic ordering of the unknowns. The discrete operator leads to a sparse, symmetric positive-definite matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n = N^2$, having diagonal entries equal to $4$ and off-diagonal entries equal to $-1$ for immediate neighbors in the four cardinal directions, and $0$ otherwise.\n\nA stationary iterative method for solving $A x = b$ proceeds by splitting $A$ into $A = D - L - U$, where $D$ is the diagonal of $A$, and $L$ and $U$ are, respectively, the strict lower and strict upper triangular parts of $-A$ so that $L$ and $U$ have nonnegative entries. The Successive Over-Relaxation (SOR) iteration with relaxation parameter $\\omega \\in (0,2)$ is defined by\n$$\nx^{(k+1)} = (D - \\omega L)^{-1} \\big( (1 - \\omega) D + \\omega U \\big) x^{(k)} + \\omega (D - \\omega L)^{-1} b .\n$$\nThe corresponding iteration matrix is\n$$\nT_{\\mathrm{SOR}}(\\omega) = (D - \\omega L)^{-1} \\big( (1 - \\omega) D + \\omega U \\big) .\n$$\nConvergence of the iteration for any initial guess $x^{(0)}$ is governed by the spectral radius of the iteration matrix, denoted $\\rho(T_{\\mathrm{SOR}}(\\omega))$, where\n$$\n\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } M \\} .\n$$\nThe smaller $\\rho(T_{\\mathrm{SOR}}(\\omega))$ is, the faster the linear iteration converges.\n\nYour task is to:\n- Construct the matrix $A$ corresponding to the five-point stencil on an $N \\times N$ interior grid with homogeneous Dirichlet boundary conditions, using the lexicographic ordering of unknowns.\n- Form the splitting $A = D - L - U$ with $D$ diagonal and $L, U$ strictly lower and strictly upper triangular parts of $-A$ so that $A = D - L - U$ holds exactly.\n- For a given $\\omega \\in (0,2)$, compute the SOR iteration matrix $T_{\\mathrm{SOR}}(\\omega)$ and its spectral radius $\\rho(T_{\\mathrm{SOR}}(\\omega))$ by computing all eigenvalues and taking the maximum modulus.\n- For each test case listed below, perform a grid search over $\\omega$ to approximate the minimizer of $\\rho(T_{\\mathrm{SOR}}(\\omega))$. Use a two-stage search: a coarse scan with step size $\\Delta \\omega = 0.05$ over $\\omega \\in [0.2,1.95]$, followed by a refinement scan with step size $\\Delta \\omega = 0.01$ over an interval of width $0.10$ centered at the best coarse value, truncated to stay within the interval $(0.05,1.95)$.\n\nTest suite:\n- Use $N \\in \\{ 3, 6, 8, 10 \\}$.\n\nFor each $N$ in the test suite, your program must output the approximate minimizer $\\omega_{\\mathrm{opt}}$ of $\\rho(T_{\\mathrm{SOR}}(\\omega))$ found by the described search. Report each $\\omega_{\\mathrm{opt}}$ rounded to exactly four decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite. For example, if there are four results $w_1, w_2, w_3, w_4$, print the single line \"[w1,w2,w3,w4]\" where each $w_i$ is a decimal string rounded to exactly four places. There are no physical units or angles involved in this problem; all reported values are dimensionless.", "solution": "The problem presented is a standard exercise in numerical linear algebra and computational physics. It asks for the numerical determination of the optimal over-relaxation parameter, $\\omega_{\\mathrm{opt}}$, for the Successive Over-Relaxation (SOR) method applied to the linear system derived from a finite-difference discretization of the two-dimensional Poisson equation, $-\\nabla^2 u = f$, on a square domain with homogeneous Dirichlet boundary conditions. The problem is scientifically sound, well-posed, and all necessary parameters for its solution are provided. We therefore proceed with the solution.\n\nThe core of the problem lies in understanding that the convergence rate of a stationary iterative method is governed by the spectral radius of its iteration matrix. For the SOR method, this matrix is denoted as $T_{\\mathrm{SOR}}(\\omega)$, and our objective is to find the value of the relaxation parameter $\\omega$ in the interval $(0, 2)$ that minimizes its spectral radius, $\\rho(T_{\\mathrm{SOR}}(\\omega))$.\n\nFirst, we must construct the coefficient matrix $A$ for the linear system $Ax=b$. The five-point stencil on an $N \\times N$ interior grid results in a system of $n = N^2$ linear equations. With lexicographic ordering of the grid points, where a point with grid coordinates $(i, j)$ ($1 \\le i, j \\le N$) is mapped to a single index $k = (i-1)N + j-1$, the discrete Laplace operator at this point is:\n$$\n4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = h^2 f_{i,j}\n$$\nwhere $h$ is the grid spacing. This structure defines the matrix $A \\in \\mathbb{R}^{n \\times n}$. $A$ is a block tridiagonal matrix:\n$$\nA = \\begin{pmatrix}\nB  -I  0  \\cdots  0 \\\\\n-I  B  -I  \\ddots  \\vdots \\\\\n0  \\ddots  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots -I  B  -I \\\\\n0  \\cdots  0  -I  B\n\\end{pmatrix}\n$$\nwhere each block is an $N \\times N$ matrix. The block $I$ is the identity matrix, and the block $B$ is a tridiagonal matrix representing the connections within a single grid row:\n$$\nB = \\begin{pmatrix}\n4  -1  0  \\cdots  0 \\\\\n-1  4  -1  \\ddots  \\vdots \\\\\n0  \\ddots  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  -1  4  -1 \\\\\n0  \\cdots  0  -1  4\n\\end{pmatrix}\n$$\nThe matrix $A$ is sparse, symmetric, and positive-definite.\n\nThe SOR method is based on splitting the matrix $A$ as $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strictly lower triangular part of $A$, and $-U$ is the strictly upper triangular part of $A$. Based on the structure of $A$, $D$ is a scalar matrix $4I_n$, where $I_n$ is the $n \\times n$ identity matrix. The matrices $L$ and $U$ are strictly lower and upper triangular, respectively, composed of entries $0$ and $1$.\n\nThe SOR iteration is defined by the recurrence:\n$$\nx^{(k+1)} = T_{\\mathrm{SOR}}(\\omega) x^{(k)} + c\n$$\nwhere the SOR iteration matrix $T_{\\mathrm{SOR}}(\\omega)$ is given by:\n$$\nT_{\\mathrm{SOR}}(\\omega) = (D - \\omega L)^{-1} \\big( (1 - \\omega) D + \\omega U \\big)\n$$\nThe rate of convergence is determined by the spectral radius $\\rho(T_{\\mathrm{SOR}}(\\omega)) = \\max_i |\\lambda_i|$, where $\\{\\lambda_i\\}$ are the eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$. We must find $\\omega_{\\mathrm{opt}} = \\arg\\min_{\\omega \\in (0,2)} \\rho(T_{\\mathrm{SOR}}(\\omega))$.\n\nThe computational procedure for a given grid size $N$ is as follows:\n1.  Construct the $n \\times n$ matrix $A$, where $n=N^2$, according to the block tridiagonal structure described above.\n2.  Decompose $A$ into its diagonal part $D$, strictly lower part $-L$, and strictly upper part $-U$. Note that $D = \\text{diag}(A)$, $L = -\\text{tril}(A, -1)$, and $U = -\\text{triu}(A, 1)$.\n3.  Implement a function that, for a given value of $\\omega$, computes the spectral radius $\\rho(T_{\\mathrm{SOR}}(\\omega))$. This involves:\n    a. Forming the matrices $M = D - \\omega L$ and $P = (1 - \\omega) D + \\omega U$.\n    b. Computing the SOR matrix $T_{\\mathrm{SOR}}(\\omega) = M^{-1} P$. Since $M$ is a lower triangular matrix, its inverse is well-defined and can be computed numerically.\n    c. Calculating all eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$.\n    d. Finding the maximum of the absolute values of the eigenvalues, which is the spectral radius.\n\n4.  Execute a two-stage grid search to find the optimal $\\omega$.\n    a.  **Coarse Search:** Iterate over $\\omega$ from $0.20$ to $1.95$ with a step of $\\Delta\\omega = 0.05$. For each $\\omega$, compute $\\rho(T_{\\mathrm{SOR}}(\\omega))$ and identify the value $\\omega_{\\text{coarse\\_opt}}$ that yields the minimum spectral radius.\n    b.  **Refined Search:** Define a new search interval $[\\omega_{\\text{coarse\\_opt}} - 0.05, \\omega_{\\text{coarse\\_opt}} + 0.05]$, truncated to lie within $(0.05, 1.95)$. Perform a search over this new interval with a finer step of $\\Delta\\omega = 0.01$. The value of $\\omega$ that minimizes the spectral radius in this refined search is our estimate for $\\omega_{\\mathrm{opt}}$.\n\nThis entire process is repeated for each value of $N$ in the test suite, which is $N \\in \\{ 3, 6, 8, 10 \\}$. The final results are the estimated optimal parameters $\\omega_{\\mathrm{opt}}$ for each grid size.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_poisson_matrix(N):\n    \"\"\"\n    Constructs the matrix for the 2D Poisson problem on an N x N grid.\n    \n    Args:\n        N (int): The number of interior grid points in one dimension.\n        \n    Returns:\n        numpy.ndarray: The (N*N) x (N*N) matrix A.\n    \"\"\"\n    n = N * N\n    \n    # Create the main diagonal block B\n    B = np.diag([4] * N) + np.diag([-1] * (N - 1), k=1) + np.diag([-1] * (N - 1), k=-1)\n    \n    # Create the full matrix A using Kronecker products\n    A = np.kron(np.eye(N), B)\n    \n    # Create identity matrices for off-diagonal blocks\n    off_diag_block = -np.eye(N)\n    \n    # Add the off-diagonal blocks\n    A += np.kron(np.diag(np.ones(N - 1), k=1), off_diag_block)\n    A += np.kron(np.diag(np.ones(N - 1), k=-1), off_diag_block)\n    \n    return A\n\ndef calculate_sor_spectral_radius(omega, D, L, U):\n    \"\"\"\n    Calculates the spectral radius of the SOR iteration matrix for a given omega.\n    \n    Args:\n        omega (float): The relaxation parameter.\n        D (numpy.ndarray): The diagonal part of matrix A.\n        L (numpy.ndarray): The strictly lower triangular part of -A.\n        U (numpy.ndarray): The strictly upper triangular part of -A.\n        \n    Returns:\n        float: The spectral radius of the SOR iteration matrix.\n    \"\"\"\n    # Form the matrix (D - omega*L)\n    D_minus_omega_L = D - omega * L\n    \n    # Form the matrix ((1-omega)*D + omega*U)\n    rhs_matrix = (1 - omega) * D + omega * U\n    \n    # Compute the SOR iteration matrix T_SOR = (D - omega*L)^-1 * (...)\n    # Since D_minus_omega_L is triangular, linalg.solve is more stable and efficient\n    # than computing the inverse explicitly, but for small dense matrices, inv is fine.\n    inv_D_minus_omega_L = np.linalg.inv(D_minus_omega_L)\n    T_sor = inv_D_minus_omega_L @ rhs_matrix\n    \n    # Calculate eigenvalues and spectral radius\n    eigenvalues = np.linalg.eigvals(T_sor)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    \n    return spectral_radius\n\ndef find_optimal_omega(N):\n    \"\"\"\n    Finds the optimal SOR parameter for a given grid size N using a two-stage search.\n    \n    Args:\n        N (int): The number of interior grid points in one dimension.\n        \n    Returns:\n        float: The estimated optimal omega.\n    \"\"\"\n    A = construct_poisson_matrix(N)\n    D = np.diag(np.diag(A))\n    L = -np.tril(A, k=-1)\n    U = -np.triu(A, k=1)\n    \n    # Stage 1: Coarse search\n    coarse_omegas = np.linspace(0.2, 1.95, int(round((1.95 - 0.2) / 0.05)) + 1)\n    min_rho_coarse = float('inf')\n    omega_coarse_opt = 0.0\n    \n    for omega in coarse_omegas:\n        rho = calculate_sor_spectral_radius(omega, D, L, U)\n        if rho  min_rho_coarse:\n            min_rho_coarse = rho\n            omega_coarse_opt = omega\n            \n    # Stage 2: Refined search\n    # Define search interval, truncated to (0.05, 1.95)\n    fine_start = max(0.051, omega_coarse_opt - 0.05) # Use 0.051 to avoid floating point issues at boundary\n    fine_end = min(1.949, omega_coarse_opt + 0.05)   # Use 1.949 for the same reason\n    \n    num_points = int(round((fine_end - fine_start) / 0.01)) + 1\n    fine_omegas = np.linspace(fine_start, fine_end, num_points)\n    \n    min_rho_fine = float('inf')\n    omega_opt = 0.0\n    \n    for omega in fine_omegas:\n        rho = calculate_sor_spectral_radius(omega, D, L, U)\n        if rho  min_rho_fine:\n            min_rho_fine = rho\n            omega_opt = omega\n            \n    return omega_opt\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [3, 6, 8, 10]\n\n    results = []\n    for N in test_cases:\n        optimal_omega = find_optimal_omega(N)\n        results.append(optimal_omega)\n\n    # Format the final output as specified\n    formatted_results = [f\"{r:.4f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2442097"}]}