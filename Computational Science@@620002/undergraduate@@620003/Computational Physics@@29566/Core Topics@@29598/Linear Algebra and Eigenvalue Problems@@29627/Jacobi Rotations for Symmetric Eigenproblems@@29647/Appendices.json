{"hands_on_practices": [{"introduction": "Before diving into code, it is essential to grasp the fundamental nature of the Jacobi method. A common pitfall is to assume that since a rotation can zero out one off-diagonal element, a sequence of such rotations can directly diagonalize the matrix in one pass. This exercise [@problem_id:2405322] confronts this misconception, compelling you to think critically about how a rotation in one plane affects elements in others and clarifying why the method must be iterative to converge.", "problem": "In the Jacobi method for the symmetric eigenproblem, one applies successive plane rotations (Givens rotations) to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ as orthogonal similarity transformations $A \\leftarrow G^{\\mathsf{T}} A G$, where $G$ is an identity matrix modified in the $(p,q)$-plane by a $2 \\times 2$ rotation. Each such rotation can be chosen to annihilate a single off-diagonal entry $a_{pq}$ of the current matrix. Consider the specific case $n = 3$.\n\nStarting from the fundamental facts that any real symmetric matrix admits an orthonormal eigenbasis (hence an orthogonal diagonalization $A = Q D Q^{\\mathsf{T}}$ with $Q \\in \\mathbb{R}^{3 \\times 3}$ orthogonal and $D$ diagonal), and that a Jacobi rotation acting in the $(p,q)$-plane changes only the rows and columns $p$ and $q$ (thereby generally altering multiple off-diagonal entries), evaluate the following statements about the minimum number of Jacobi rotations needed to guarantee diagonalization of an arbitrary real symmetric $3 \\times 3$ matrix:\n\nChoose all statements that are true.\n\nA. Exactly $3$ Jacobi rotations always suffice, because there are exactly $3$ distinct off-diagonal entries in a $3 \\times 3$ symmetric matrix and each rotation can annihilate one of them.\n\nB. Three Jacobi rotations are not guaranteed to diagonalize an arbitrary $3 \\times 3$ symmetric matrix when each rotation is chosen to annihilate the current value of a single off-diagonal entry; subsequent rotations can reintroduce previously zeroed entries, so convergence typically requires iterative sweeps.\n\nC. There exists, for any real symmetric $3 \\times 3$ matrix, a choice of three plane rotations whose product $Q$ exactly diagonalizes $A$ by $Q^{\\mathsf{T}} A Q = D$, but the angles of these rotations are not, in general, the same as those obtained by the Jacobi rule of successively annihilating currently nonzero off-diagonal entries.\n\nD. The minimum number of Jacobi rotations that guarantees diagonalization of any real symmetric $3 \\times 3$ matrix is $5$.\n\nE. Without special structure in $A$, there is no fixed finite number $k$ such that performing $k$ Jacobi annihilations (according to the rule “choose an angle that zeros the current $a_{pq}$”) is guaranteed to produce an exactly diagonal matrix for all real symmetric $3 \\times 3$ inputs; rather, the method converges asymptotically over sweeps to a chosen tolerance.", "solution": "The problem statement is scientifically sound and well-posed. It concerns the standard Jacobi eigenvalue algorithm for real symmetric matrices, a fundamental topic in numerical linear algebra. The premises, including the existence of an orthogonal diagonalization and the effect of a plane rotation, are correct. The problem is valid for analysis.\n\nThe analysis hinges on understanding the effect of a single Jacobi rotation and the distinction between the iterative Jacobi method and direct diagonalization. A Jacobi rotation, represented by an orthogonal matrix $G$ (a Givens rotation), acts on a symmetric matrix $A$ via a similarity transformation, $A' = G^{\\mathsf{T}}AG$. For a rotation in the $(p,q)$-plane, the matrix $G$ is the identity matrix except for the entries $g_{pp} = g_{qq} = \\cos\\theta$ and $g_{pq} = -g_{qp} = \\sin\\theta$ for some angle $\\theta$. The angle $\\theta$ in the standard Jacobi method is chosen specifically to make the new off-diagonal element $a'_{pq}$ equal to zero. This is achieved when $\\cot(2\\theta) = \\frac{a_{pp} - a_{qq}}{2a_{pq}}$.\n\nThe key issue is that this transformation, while zeroing out $a_{pq}$, alters all other elements in rows $p$ and $q$ and columns $p$ and $q$. Specifically, for any $k \\neq p, q$, the new off-diagonal entries $a'_{pk}$ and $a'_{qk}$ are linear combinations of the old entries $a_{pk}$ and $a_{qk}$:\n$$ a'_{pk} = a_{pk} \\cos\\theta + a_{qk} \\sin\\theta $$\n$$ a'_{qk} = -a_{pk} \\sin\\theta + a_{qk} \\cos\\theta $$\nThis means that a subsequent Jacobi rotation, say to annihilate $a'_{pk}$, will generally destroy the zero that was just created at the $(p,q)$ position. This behavior is central to the analysis of the options.\n\nLet us consider the specific case of a $3 \\times 3$ real symmetric matrix $A$:\n$$ A = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{12} & a_{22} & a_{23} \\\\ a_{13} & a_{23} & a_{33} \\end{pmatrix} $$\nThe distinct off-diagonal entries are $a_{12}$, $a_{13}$, and $a_{23}$.\n\nA. Exactly $3$ Jacobi rotations always suffice, because there are exactly $3$ distinct off-diagonal entries in a $3 \\times 3$ symmetric matrix and each rotation can annihilate one of them.\n\nThis statement is **Incorrect**. The reasoning is naive. Suppose we perform three successive annihilations for $a_{12}$, $a_{13}$, and $a_{23}$.\n1. Annihilate $a_{12}$ using a rotation $G_{12}$. The new matrix $A' = G_{12}^{\\mathsf{T}} A G_{12}$ has $a'_{12} = 0$. However, the entries $a'_{13}$ and $a'_{23}$ are now different from the original $a_{13}$ and $a_{23}$.\n2. Annihilate $a'_{13}$ using a rotation $G_{13}$. The new matrix $A'' = G_{13}^{\\mathsf{T}} A' G_{13}$ has $a''_{13} = 0$. But this rotation acts on rows/columns $1$ and $3$. It will alter the $(1,2)$ entry, which was previously zeroed. The new entry $a''_{12}$ is given by $a''_{12} = a'_{12} \\cos\\theta_{13} + a'_{32} \\sin\\theta_{13} = 0 \\cdot \\cos\\theta_{13} + a'_{23} \\sin\\theta_{13}$. In general, $a'_{23} \\neq 0$ and $\\sin\\theta_{13} \\neq 0$, so $a''_{12}$ becomes non-zero.\nThe process of zeroing one off-diagonal element generally \"un-zeros\" others. Therefore, one cycle (or \"sweep\") of $3$ rotations is not sufficient for diagonalization.\n\nB. Three Jacobi rotations are not guaranteed to diagonalize an arbitrary $3 \\times 3$ symmetric matrix when each rotation is chosen to annihilate the current value of a single off-diagonal entry; subsequent rotations can reintroduce previously zeroed entries, so convergence typically requires iterative sweeps.\n\nThis statement is **Correct**. As demonstrated in the analysis of option A, the standard iterative Jacobi procedure, where each step annihilates a single off-diagonal element, suffers from the \"whack-a-mole\" effect. Zeroing an element in one step can (and generally does) create non-zero values in positions that were previously zeroed. For this reason, the Jacobi method is not a direct method that terminates in a fixed number of steps. Instead, it is an iterative algorithm that converges to a diagonal matrix over multiple \"sweeps,\" where a sweep consists of rotating all off-diagonal pairs.\n\nC. There exists, for any real symmetric $3 \\times 3$ matrix, a choice of three plane rotations whose product $Q$ exactly diagonalizes $A$ by $Q^{\\mathsf{T}} A Q = D$, but the angles of these rotations are not, in general, the same as those obtained by the Jacobi rule of successively annihilating currently nonzero off-diagonal entries.\n\nThis statement is **Correct**. It makes two distinct claims.\n1. Existence: The spectral theorem for real symmetric matrices guarantees the existence of an orthogonal matrix $Q$ such that $Q^{\\mathsf{T}}AQ = D$, where $D$ is diagonal. For $n=3$, any such orthogonal matrix $Q \\in O(3)$ can be decomposed into a product of at most three plane (Givens) rotations. This is a known result from linear algebra related to Euler angles and other similar rotation sequences (e.g., a Yaw-Pitch-Roll decomposition). Thus, a set of three rotations exists that can directly diagonalize $A$.\n2. Distinction from Jacobi method: The angles of these three rotations must be calculated simultaneously based on the *global* properties of the matrix $A$ (i.e., its eigenvectors). This is a difficult, non-linear problem. The standard Jacobi method, by contrast, uses a simple, *local* formula to compute each rotation angle based only on three elements ($a_{pp}, a_{qq}, a_{pq}$) of the *current* matrix. This greedy, local choice of angle is not the same as the globally determined angles required for direct diagonalization in three steps. Hence, the angles are different in general.\n\nD. The minimum number of Jacobi rotations that guarantees diagonalization of any real symmetric $3 \\times 3$ matrix is $5$.\n\nThis statement is **Incorrect**. There is no such fixed finite number for the standard iterative Jacobi method. The method is proven to converge, meaning the sum of the squares of the off-diagonal elements approaches zero as the number of rotations increases. However, for a general matrix, it will not reach exactly zero in a predetermined number of steps. The number of steps required to reach a certain tolerance depends on the matrix itself. The number $5$ is arbitrary and has no theoretical backing in this context.\n\nE. Without special structure in $A$, there is no fixed finite number $k$ such that performing $k$ Jacobi annihilations (according to the rule “choose an angle that zeros the current $a_{pq}$”) is guaranteed to produce an exactly diagonal matrix for all real symmetric $3 \\times 3$ inputs; rather, the method converges asymptotically over sweeps to a chosen tolerance.\n\nThis statement is **Correct**. It accurately describes the fundamental nature of the Jacobi method when applied to general matrices. It is an iterative, not a direct, method. While each step $A \\to G^{\\mathsf{T}}AG$ reduces the sum of squares of the off-diagonal elements, it does not reduce it by a fixed fraction, and certainly not to zero in a fixed number of steps. The process is one of asymptotic convergence. One performs rotations until the magnitude of all off-diagonal elements is smaller than a pre-defined tolerance $\\epsilon > 0$. Achieving an *exactly* diagonal matrix (tolerance $\\epsilon=0$) would, in general, require an infinite number of steps. Therefore, no finite $k$ can guarantee exact diagonalization for all possible input matrices.\n\nIn summary, statements B, C, and E are all true.\n- B correctly points out the flaw in assuming one sweep is enough.\n- E correctly generalizes this to any finite number of steps, describing the method's asymptotic nature.\n- C correctly introduces the subtle but important distinction between the *existence* of a 3-rotation direct solution and the *procedure* of the iterative Jacobi algorithm.", "answer": "$$\\boxed{BCE}$$", "id": "2405322"}, {"introduction": "To build a solid understanding of the spectral decomposition $A = Q \\Lambda Q^{\\mathsf{T}}$, we will work backward from the solution. This reverse problem [@problem_id:2405315] provides the final diagonal matrix $\\Lambda$ and the sequence of rotations that constitute the orthogonal matrix $Q$. Your task is to reconstruct the original dense symmetric matrix $A$, an exercise that reinforces the geometric meaning of the transformation and provides excellent practice in constructing and manipulating rotation matrices.", "problem": "You are given a reverse formulation of the classical Jacobi method for symmetric eigenproblems. In the forward Jacobi process, a real symmetric matrix is diagonalized by a sequence of orthogonal plane rotations (Jacobi rotations). In this reverse problem, you are given a diagonal matrix and a specified sequence of Jacobi rotations, and you must reconstruct the dense symmetric matrix that would be diagonalized by the forward application of those rotations.\n\nFundamental base:\n- A real square matrix $Q$ is orthogonal if $Q^{\\mathsf{T}}Q=I$, where $I$ is the identity matrix and ${\\mathsf{T}}$ denotes transpose. Orthogonal similarity transformations map symmetric matrices to symmetric matrices and preserve eigenvalues.\n- A Jacobi rotation in the coordinate plane spanned by standard basis vectors indexed by $p$ and $q$ is an orthogonal matrix identical to the identity except in its $2\\times 2$ sub-block on rows and columns $(p,q)$, which equals a rotation by angle $\\theta$.\n\nDefinitions and conventions for this problem:\n- Angles are specified in radians.\n- Indices $p$ and $q$ are zero-based integers with $0\\le p&lt;q&lt;n$, where $n$ is the matrix dimension.\n- For a given $n$, the elementary Jacobi rotation $G(p,q,\\theta)\\in\\mathbb{R}^{n\\times n}$ is defined as the identity matrix except for the entries\n  $$\n  G_{pp}=\\cos\\theta,\\quad G_{qq}=\\cos\\theta,\\quad G_{pq}=\\sin\\theta,\\quad G_{qp}=-\\sin\\theta,\n  $$\n  and $G_{ij}=0$ for all other off-diagonal entries in rows or columns $p$ and $q$. This specifies the standard plane rotation embedded in $\\mathbb{R}^n$.\n- Given a sequence of $k$ rotations $\\{(p_1,q_1,\\theta_1),\\ldots,(p_k,q_k,\\theta_k)\\}$, form the orthogonal matrix\n  $$\n  Q \\equiv G(p_1,q_1,\\theta_1)\\,G(p_2,q_2,\\theta_2)\\,\\cdots\\,G(p_k,q_k,\\theta_k).\n  $$\n- Let $\\Lambda=\\mathrm{diag}(\\lambda_0,\\ldots,\\lambda_{n-1})$ be a real diagonal matrix. Your task is to reconstruct the symmetric matrix $A$ that would be diagonalized by the forward Jacobi process using the given rotations, solely from $\\Lambda$ and the rotation sequence. You must derive from first principles how to compute $A$ using only the definitions above and general properties of orthogonal similarity transformations.\n\nProgram requirements:\n- Implement a program that, for each test case below, constructs the specified $Q$ from the rotation sequence and returns the upper-triangular entries of the reconstructed matrix $A$ in row-major order, including the diagonal. For example, for $n=3$, return the list $[A_{00},A_{01},A_{02},A_{11},A_{12},A_{22}]$.\n- For numerical reporting, round each returned number to $10^{-6}$ and format it with exactly six digits after the decimal point.\n- The final output must be a single line containing a single list that concatenates the per-test-case upper-triangular outputs, as a comma-separated list enclosed in square brackets, with no spaces.\n\nTest suite (angles in radians, indices zero-based):\n- Test case $1$ (boundary: empty rotation sequence): $n=3$, $\\Lambda=\\mathrm{diag}(1.5,-0.5,2.5)$, rotation sequence $\\varnothing$.\n- Test case $2$ (single rotation): $n=3$, $\\Lambda=\\mathrm{diag}(1,2,4)$, rotation sequence $\\{(0,1,\\pi/6)\\}$.\n- Test case $3$ (multiple rotations with shared indices): $n=4$, $\\Lambda=\\mathrm{diag}(0.5,1.5,2.5,3.5)$, rotation sequence $\\{(0,2,\\pi/4),(1,3,\\pi/3),(0,1,-\\pi/8)\\}$.\n- Test case $4$ (angles including zeros): $n=3$, $\\Lambda=\\mathrm{diag}(3,1,2)$, rotation sequence $\\{(0,2,0),(0,2,\\pi/2),(1,2,0)\\}$.\n\nYour program should produce a single line of output containing the concatenated results as a comma-separated list enclosed in square brackets (for example, $\\big[$result$1,$result$2,\\ldots\\big]$). No additional text should be printed.", "solution": "The problem requires the reconstruction of a real symmetric matrix $A$ from its diagonal form $\\Lambda$ and a sequence of Jacobi rotations that produce this diagonalization. This is the inverse of the Jacobi eigenvalue algorithm. We shall first derive the reconstruction formula from fundamental principles of linear algebra and then develop an algorithm to compute it.\n\nA fundamental result from spectral theory states that any real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ can be diagonalized by an orthogonal similarity transformation. That is, there exists an orthogonal matrix $P \\in \\mathbb{R}^{n \\times n}$ and a diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that:\n$$ \\Lambda = P^{\\mathsf{T}} A P $$\nHere, the diagonal entries of $\\Lambda$ are the eigenvalues of $A$, and the columns of $P$ are the corresponding orthonormal eigenvectors. This relation can be inverted to express $A$ in terms of its eigenvalues and eigenvectors. Since $P$ is orthogonal, its transpose is its inverse, i.e., $P^{\\mathsf{T}} = P^{-1}$. By pre-multiplying by $P$ and post-multiplying by $P^{\\mathsf{T}}$, we obtain:\n$$ P \\Lambda P^{\\mathsf{T}} = P (P^{\\mathsf{T}} A P) P^{\\mathsf{T}} = (P P^{\\mathsf{T}}) A (P P^{\\mathsf{T}}) = I A I = A $$\nThus, the reconstruction formula is $A = P \\Lambda P^{\\mathsf{T}}$.\n\nThe problem states that the forward Jacobi process diagonalizes $A$ using a sequence of $k$ elementary rotations, $\\{(p_1,q_1,\\theta_1), \\ldots, (p_k,q_k,\\theta_k)\\}$. An elementary Jacobi rotation is an orthogonal transformation represented by a matrix $G(p,q,\\theta)$. The forward process applies these rotations as a sequence of similarity transformations. Let $A_0 = A$. The sequence is:\n$$ A_1 = G(p_1,q_1,\\theta_1)^{\\mathsf{T}} A_0 G(p_1,q_1,\\theta_1) $$\n$$ A_2 = G(p_2,q_2,\\theta_2)^{\\mathsf{T}} A_1 G(p_2,q_2,\\theta_2) $$\n$$ \\vdots $$\n$$ \\Lambda = A_k = G(p_k,q_k,\\theta_k)^{\\mathsf{T}} A_{k-1} G(p_k,q_k,\\theta_k) $$\nSubstituting recursively, we find the relation between $A$ and $\\Lambda$:\n$$ \\Lambda = \\left(G_k^{\\mathsf{T}} \\cdots G_2^{\\mathsf{T}} G_1^{\\mathsf{T}}\\right) A \\left(G_1 G_2 \\cdots G_k\\right) $$\nwhere we use the shorthand $G_i = G(p_i,q_i,\\theta_i)$. Using the property $(AB)^{\\mathsf{T}} = B^{\\mathsf{T}}A^{\\mathsf{T}}$, this simplifies to:\n$$ \\Lambda = (G_1 G_2 \\cdots G_k)^{\\mathsf{T}} A (G_1 G_2 \\cdots G_k) $$\nThe problem defines the total transformation matrix $Q$ as the ordered product of the elementary rotations:\n$$ Q \\equiv G_1 G_2 \\cdots G_k $$\nComparing this definition with the expression for $\\Lambda$, we see that the transformation matrix $P$ in the standard spectral decomposition corresponds exactly to $Q$.\n$$ \\Lambda = Q^{\\mathsf{T}} A Q $$\nTherefore, the formula to reconstruct $A$ is:\n$$ A = Q \\Lambda Q^{\\mathsf{T}} $$\nSince each $G_i$ is an orthogonal matrix, their product $Q$ is also an orthogonal matrix, validating this derivation.\n\nThe algorithm to solve the problem is as follows:\n$1$. For each test case, given the dimension $n$, the list of eigenvalues $(\\lambda_0, \\ldots, \\lambda_{n-1})$, and the rotation sequence $\\{(p_1,q_1,\\theta_1), \\ldots, (p_k,q_k,\\theta_k)\\}$.\n$2$. Construct the diagonal matrix $\\Lambda = \\mathrm{diag}(\\lambda_0, \\ldots, \\lambda_{n-1})$.\n$3$. Construct the total rotation matrix $Q$. This is done by first initializing a matrix and then multiplying it by each elementary Jacobi rotation matrix $G_i = G(p_i,q_i,\\theta_i)$ in the specified order.\n    - If the rotation sequence is empty ($k=0$), $Q$ is the identity matrix $I$ of size $n \\times n$.\n    - If the sequence is not empty, compute $Q = G_1 G_2 \\cdots G_k$. This can be done iteratively: start with $Q=G_1$, then compute $Q \\leftarrow Q G_2$, and so on, until $Q \\leftarrow Q G_k$. An elementary rotation matrix $G(p,q,\\theta)$ is an $n \\times n$ identity matrix except for four entries:\n    $$ G_{pp}=\\cos\\theta, \\quad G_{qq}=\\cos\\theta, \\quad G_{pq}=\\sin\\theta, \\quad G_{qp}=-\\sin\\theta $$\n$4$. Compute the symmetric matrix $A$ using the derived formula $A = Q \\Lambda Q^{\\mathsf{T}}$.\n$5$. Extract the upper-triangular elements of $A$, including the main diagonal, in row-major order. These are the elements $A_{ij}$ where $0 \\le i \\le j < n$.\n$6$. Format each extracted element by rounding to $10^{-6}$ and presenting it with exactly six digits after the decimal point. Concatenate all results from all test cases into a single list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reverse Jacobi rotation problem for a suite of test cases.\n    \"\"\"\n    # Test suite (angles in radians, indices zero-based):\n    # - Case 1: n=3, Lambda=diag(1.5,-0.5,2.5), sequence []\n    # - Case 2: n=3, Lambda=diag(1,2,4), sequence [(0,1,pi/6)]\n    # - Case 3: n=4, Lambda=diag(0.5,1.5,2.5,3.5), sequence [(0,2,pi/4),(1,3,pi/3),(0,1,-pi/8)]\n    # - Case 4: n=3, Lambda=diag(3,1,2), sequence [(0,2,0),(0,2,pi/2),(1,2,0)]\n    test_cases = [\n        {'n': 3, 'lambdas': [1.5, -0.5, 2.5], 'rotations': []},\n        {'n': 3, 'lambdas': [1, 2, 4], 'rotations': [(0, 1, np.pi / 6)]},\n        {'n': 4, 'lambdas': [0.5, 1.5, 2.5, 3.5], 'rotations': [(0, 2, np.pi / 4), (1, 3, np.pi / 3), (0, 1, -np.pi / 8)]},\n        {'n': 3, 'lambdas': [3, 1, 2], 'rotations': [(0, 2, 0), (0, 2, np.pi / 2), (1, 2, 0)]}\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case['n']\n        lambdas = case['lambdas']\n        rotations = case['rotations']\n\n        # Step 1: Construct the diagonal matrix Lambda\n        Lambda_mat = np.diag(lambdas)\n\n        # Step 2: Construct the orthogonal matrix Q from the sequence of rotations\n        if not rotations:\n            Q_mat = np.identity(n)\n        else:\n            # Initialize Q with the first rotation matrix G_1\n            p, q, theta = rotations[0]\n            c, s = np.cos(theta), np.sin(theta)\n            G = np.identity(n)\n            G[p, p], G[q, q] = c, c\n            G[p, q], G[q, p] = s, -s\n            Q_mat = G\n\n            # Accumulate the product Q = G_1 * G_2 * ... * G_k\n            for i in range(1, len(rotations)):\n                p, q, theta = rotations[i]\n                c, s = np.cos(theta), np.sin(theta)\n                G = np.identity(n)\n                G[p, p], G[q, q] = c, c\n                G[p, q], G[q, p] = s, -s\n                Q_mat = Q_mat @ G\n        \n        # Step 3: Reconstruct matrix A using A = Q * Lambda * Q^T\n        A_mat = Q_mat @ Lambda_mat @ Q_mat.T\n\n        # Step 4: Extract the upper-triangular entries in row-major order\n        case_results = []\n        for i in range(n):\n            for j in range(i, n):\n                # Format to 6 decimal places, including trailing zeros\n                formatted_val = f\"{A_mat[i, j]:.6f}\"\n                case_results.append(formatted_val)\n        \n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "2405315"}, {"introduction": "This practice serves as a capstone exercise, bringing together the conceptual and practical aspects of the Jacobi method. You will implement the complete algorithm to diagonalize a randomly generated symmetric matrix from scratch [@problem_id:2405358]. The core of this task is not just to obtain a result, but to verify it against the predictions of the spectral theorem by quantitatively measuring the diagonality, reconstruction accuracy, and orthogonality of your computed solution.", "problem": "You are to write a complete program that verifies the spectral theorem for real symmetric matrices using Jacobi rotations, and quantitatively checks the orthogonality of the resulting eigenvectors. For each test case, construct a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ as follows: given an integer seed $s$ and real bounds $a_{\\min}$ and $a_{\\max}$, use a pseudorandom number generator initialized with seed $s$ to draw an $n \\times n$ matrix $M$ with entries independently sampled from the continuous uniform distribution on the interval $\\left[a_{\\min}, a_{\\max}\\right]$. Then define $A = \\dfrac{1}{2}\\left(M + M^{\\mathsf{T}}\\right)$ so that $A$ is real and symmetric. Your task is to compute an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ and a diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that $A \\approx Q \\Lambda Q^{\\mathsf{T}}$. Use a convergence tolerance $\\varepsilon = 10^{-10}$ with respect to the Frobenius norm of the off-diagonal part of $Q^{\\mathsf{T}} A Q$, and enforce a hard cap of $10^{6}$ similarity rotations. If any rotation angles are computed internally, they must be in radians. For each test case, report the following three real numbers:\n- $r_{\\mathrm{off}} = \\left\\| \\mathrm{offdiag}\\left(Q^{\\mathsf{T}} A Q\\right) \\right\\|_{F}$, where $\\mathrm{offdiag}(X)$ denotes the matrix $X$ with its diagonal entries set to zero and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm,\n- $r_{\\mathrm{rec}} = \\left\\| A - Q \\Lambda Q^{\\mathsf{T}} \\right\\|_{F}$,\n- $r_{\\mathrm{orth}} = \\left\\| Q^{\\mathsf{T}} Q - I \\right\\|_{F}$, where $I$ is the identity matrix in $\\mathbb{R}^{n \\times n}$.\n\nAll three quantities must be reported as floating-point numbers formatted in scientific notation with exactly $10$ digits after the decimal point.\n\nThe test suite consists of the following parameter sets $(n, s, a_{\\min}, a_{\\max})$:\n- $(n, s, a_{\\min}, a_{\\max}) = (1, 1, -1, 1)$,\n- $(n, s, a_{\\min}, a_{\\max}) = (2, 2, -1, 1)$,\n- $(n, s, a_{\\min}, a_{\\max}) = (5, 7, -1, 1)$,\n- $(n, s, a_{\\min}, a_{\\max}) = (6, 11, -10, 10)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-test-case triplets, each triplet ordered as $\\left[r_{\\mathrm{off}}, r_{\\mathrm{rec}}, r_{\\mathrm{orth}}\\right]$ and the entire collection enclosed in square brackets. For example, an output with two cases would look like $[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}]]$, where each symbol $x_{i}, y_{i}, z_{i}$ denotes a floating-point number in the specified format. Ensure your exact output format is a single line with no additional text, using scientific notation for each number with exactly $10$ digits after the decimal point.", "solution": "The problem requires the numerical verification of the spectral theorem for real symmetric matrices using the Jacobi eigenvalue algorithm. The spectral theorem states that for any real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, there exists an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ and a real diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that $A = Q \\Lambda Q^{\\mathsf{T}}$. The columns of $Q$ are the orthonormal eigenvectors of $A$, and the diagonal entries of $\\Lambda$ are the corresponding real eigenvalues.\n\nOur approach is to implement the Jacobi rotation method to compute the matrices $Q$ and $\\Lambda$ for a given symmetric matrix $A$. The matrix $A$ is constructed from a pseudorandom matrix $M$ with entries sampled from a uniform distribution $U(a_{\\min}, a_{\\max})$, ensuring its symmetry by defining $A = \\frac{1}{2}(M + M^{\\mathsf{T}})$.\n\nThe Jacobi method is an iterative algorithm that progressively reduces the magnitude of the off-diagonal elements of $A$ to zero through a sequence of similarity transformations. Each transformation is a Givens rotation, designed to annihilate a specific off-diagonal element $a_{pq}$.\nA single Jacobi rotation is a similarity transformation of the form $A' = J^{\\mathsf{T}} A J$, where $J \\equiv J(p, q, \\theta)$ is a rotation matrix in the $(p, q)$-plane. The matrix $J$ is an identity matrix except for four entries:\n$$\nJ_{pp} = \\cos\\theta, \\quad J_{qq} = \\cos\\theta \\\\\nJ_{pq} = \\sin\\theta, \\quad J_{qp} = -\\sin\\theta\n$$\nThe rotation angle $\\theta$ is chosen such that the element $a'_{pq}$ of the transformed matrix $A'$ becomes zero. This condition leads to the equation:\n$$\n\\cot(2\\theta) = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}\n$$\nFor numerical stability and efficiency, we avoid directly computing $\\theta$. Instead, we compute $t = \\tan\\theta$. Let $\\tau = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}$. The equation for $t$ is $t^2 + 2\\tau t - 1 = 0$. We choose the root with the smaller magnitude to ensure $|\\theta| \\le \\pi/4$, which enhances stability. This root is given by:\n$$\nt = \\frac{\\operatorname{sgn}(\\tau)}{|\\tau| + \\sqrt{1 + \\tau^2}}\n$$\nfor $\\tau \\neq 0$. If $\\tau = 0$ (i.e., $a_{pp} = a_{qq}$), then $\\theta = \\pi/4$ and $t=1$. From $t$, we find $c = \\cos\\theta = 1/\\sqrt{1+t^2}$ and $s = \\sin\\theta = t \\cdot c$.\n\nThe algorithm proceeds as follows:\n1.  Initialize the matrix of eigenvectors $Q$ as the identity matrix $I \\in \\mathbb{R}^{n \\times n}$.\n2.  Iteratively apply rotations. We use a cyclic Jacobi approach, where in each \"sweep,\" we iterate through all unique off-diagonal pairs $(p, q)$ with $p < q$.\n3.  For each pair $(p, q)$, if $a_{pq}$ is not numerically zero, we compute $c$ and $s$ and update the matrices $A$ and $Q$:\n    -   $A \\leftarrow J^{\\mathsf{T}} A J$\n    -   $Q \\leftarrow Q J$\n    These updates only affect rows and columns $p$ and $q$ of the respective matrices. The eigenvector matrix $Q$ accumulates the product of all rotation matrices.\n4.  The process terminates when the Frobenius norm of the off-diagonal part of $A$, defined as $S = \\sqrt{\\sum_{i \\neq j} a_{ij}^2}$, falls below a specified tolerance $\\varepsilon = 10^{-10}$, or when a maximum number of rotations ($10^6$) is reached.\n\nUpon convergence, the matrix $A$ has been transformed into an approximately diagonal matrix $\\Lambda_{\\text{approx}}$, whose diagonal entries are the eigenvalues. The final matrix $Q$ contains the corresponding eigenvectors as its columns.\n\nTo verify the computed decomposition, we calculate three error metrics:\n1.  The off-diagonal error, $r_{\\mathrm{off}} = \\left\\| \\mathrm{offdiag}(Q^{\\mathsf{T}} A Q) \\right\\|_{F}$. This measures how close the transformed matrix is to being perfectly diagonal.\n2.  The reconstruction error, $r_{\\mathrm{rec}} = \\left\\| A - Q \\Lambda Q^{\\mathsf{T}} \\right\\|_{F}$, where $\\Lambda$ is the purely diagonal matrix formed from the diagonal of $Q^{\\mathsf{T}} A Q$. This quantifies how well the computed eigenvalues and eigenvectors reconstruct the original matrix $A$.\n3.  The orthogonality error, $r_{\\mathrm{orth}} = \\left\\| Q^{\\mathsf{T}} Q - I \\right\\|_{F}$. This measures how close the computed eigenvector matrix $Q$ is to being orthogonal.\n\nFor the trivial case $n=1$, the matrix $A = [a_{11}]$ is already diagonal. We have $Q=[1]$ and $\\Lambda=[a_{11}]$, and all three error metrics are exactly $0$. For all other cases, the Jacobi algorithm is applied.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the eigenvalues and eigenvectors of real symmetric matrices\n    using the Jacobi rotation method and verifies the spectral theorem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, s, a_min, a_max)\n        (1, 1, -1, 1),\n        (2, 2, -1, 1),\n        (5, 7, -1, 1),\n        (6, 11, -10, 10),\n    ]\n    \n    TOLERANCE = 1e-10\n    MAX_ROTATIONS = 1_000_000\n\n    results_all_cases = []\n    for n, s, a_min, a_max in test_cases:\n        \n        # Handle the trivial case n=1\n        if n == 1:\n            results_all_cases.append([0.0, 0.0, 0.0])\n            continue\n            \n        # 1. Construct the real symmetric matrix A\n        rng = np.random.default_rng(seed=s)\n        M = rng.uniform(low=a_min, high=a_max, size=(n, n))\n        A = 0.5 * (M + M.T)\n        A_orig = A.copy()\n        \n        # Initialize eigenvector matrix Q to identity\n        Q = np.identity(n, dtype=np.float64)\n\n        # 2. Jacobi Iteration\n        rotation_count = 0\n        max_sweeps = 100 # A generous limit for sweeps\n        \n        for sweep in range(max_sweeps):\n            # Calculate sum of squares of off-diagonal elements\n            off_diag_sq_sum = np.sum(A**2) - np.sum(np.diag(A)**2)\n            \n            # Check for convergence\n            if np.sqrt(off_diag_sq_sum) < TOLERANCE:\n                break\n            \n            # Perform a sweep through all off-diagonal elements\n            for p in range(n):\n                for q in range(p + 1, n):\n                    if rotation_count >= MAX_ROTATIONS:\n                        break\n\n                    apq = A[p, q]\n                    # Skip rotation if element is already numerically zero\n                    if abs(apq) < 1e-20:\n                        continue\n\n                    # Calculate rotation parameters c and s\n                    app = A[p, p]\n                    aqq = A[q, q]\n                    tau = (aqq - app) / (2.0 * apq)\n                    \n                    if tau == 0:\n                        t = 1.0\n                    else:\n                        t = np.sign(tau) / (abs(tau) + np.sqrt(1.0 + tau**2))\n                    \n                    c = 1.0 / np.sqrt(1.0 + t**2)\n                    s = c * t\n\n                    # Apply rotation to A (transform A -> J^T * A * J)\n                    app_old = A[p, p]\n                    aqq_old = A[q, q]\n                    A[p, p] = c*c*app_old - 2*c*s*apq + s*s*aqq_old\n                    A[q, q] = s*s*app_old + 2*c*s*apq + c*c*aqq_old\n                    A[p, q] = 0.0\n                    A[q, p] = 0.0\n\n                    for i in range(n):\n                        if i != p and i != q:\n                            aip_old = A[i, p]\n                            aiq_old = A[i, q]\n                            A[i, p] = c * aip_old - s * aiq_old\n                            A[p, i] = A[i, p]\n                            A[i, q] = s * aip_old + c * aiq_old\n                            A[q, i] = A[i, q]\n\n                    # Apply rotation to Q (transform Q -> Q * J)\n                    Q_col_p = Q[:, p].copy()\n                    Q_col_q = Q[:, q].copy()\n                    Q[:, p] = c * Q_col_p - s * Q_col_q\n                    Q[:, q] = s * Q_col_p + c * Q_col_q\n                    \n                    rotation_count += 1\n\n                if rotation_count >= MAX_ROTATIONS:\n                    break\n            \n            if rotation_count >= MAX_ROTATIONS:\n                break\n\n        # 3. Calculate final metrics\n        A_diag = A\n        \n        # r_off: Frobenius norm of the off-diagonal part of Q^T A Q\n        off_diag_part = A_diag - np.diag(np.diag(A_diag))\n        r_off = np.linalg.norm(off_diag_part, 'fro')\n\n        # r_rec: Frobenius norm of A - Q Lambda Q^T\n        Lambda = np.diag(np.diag(A_diag))\n        reconstruction = Q @ Lambda @ Q.T\n        r_rec = np.linalg.norm(A_orig - reconstruction, 'fro')\n        \n        # r_orth: Frobenius norm of Q^T Q - I\n        identity = np.identity(n, dtype=np.float64)\n        ortho_check = Q.T @ Q - identity\n        r_orth = np.linalg.norm(ortho_check, 'fro')\n        \n        results_all_cases.append([r_off, r_rec, r_orth])\n    \n    # Format and print the final results\n    output_str = \"[\"\n    for i, case_results in enumerate(results_all_cases):\n        res_str = f\"[{case_results[0]:.10e},{case_results[1]:.10e},{case_results[2]:.10e}]\"\n        output_str += res_str\n        if i < len(results_all_cases) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "2405358"}]}