{"hands_on_practices": [{"introduction": "The Cholesky decomposition provides more than just a method for solving linear systems; it serves as a definitive test for whether a symmetric matrix is positive definite. This practice focuses on the critical moment the algorithm fails: when it encounters a non-positive pivot during factorization, which violates the condition for positive definiteness. By exploring a parametric matrix family, you will gain hands-on experience with how small changes to a matrix can cross the boundary from being positive definite to non-positive definite, a key concept for understanding numerical stability and matrix properties [@problem_id:2376407].", "problem": "You are to implement and analyze the Cholesky factorization for symmetric positive definite (SPD) systems from first principles. An $n \\times n$ real matrix $A$ is symmetric positive definite (SPD) if $A = A^{\\mathsf{T}}$ and $x^{\\mathsf{T}} A x > 0$ for all nonzero vectors $x \\in \\mathbb{R}^n$. A well-tested mathematical fact is that an SPD matrix admits a unique Cholesky factorization $A = L L^{\\mathsf{T}}$ with $L$ lower triangular and all diagonal entries of $L$ strictly positive. Another well-tested fact (Sylvester’s criterion) is that a symmetric matrix is SPD if and only if all leading principal minors are positive.\n\nYour tasks:\n\n1. Implement a routine that attempts to compute the Cholesky factorization $A = L L^{\\mathsf{T}}$ for a given symmetric matrix $A$ without any pivoting. The routine must explicitly check the positivity of each computed squared diagonal pivot, and declare failure if a nonpositive value arises at any step. The implementation must not rely on black-box SPD checks; it should detect failure based purely on the computed intermediate quantities.\n\n2. Use the following parametric family of symmetric matrices:\n   $$ A(r) = \\begin{bmatrix} 1 & r \\\\ r & 1 \\end{bmatrix}, $$\n   where $r \\in \\mathbb{R}$. This family contains matrices that move from SPD to non-SPD with small changes in the single off-diagonal element $r$. Evaluate your routine on the test suite of parameter values\n   $$ r \\in \\{\\, 0.9,\\; 0.999999,\\; 1.0,\\; 1.000001 \\,\\}. $$\n   Record a boolean for each test indicating whether the Cholesky routine succeeds (return $\\,\\text{True}\\,$) or fails (return $\\,\\text{False}\\,$).\n\n3. As an additional \"happy path\" coverage case, test the following fixed $3 \\times 3$ symmetric matrix known to be SPD by Sylvester’s criterion:\n   $$ B = \\begin{bmatrix} 4 & 1 & 1 \\\\ 1 & 3 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}. $$\n   Record a boolean indicating whether the Cholesky routine succeeds on $B$.\n\n4. Output specification. Your program must produce a single line of output containing a Python-style list literal with the booleans for all tests in the following order: the four cases $A(r)$ with $r$ equal to $0.9$, $0.999999$, $1.0$, $1.000001$, followed by the single case $B$. For example, the required format is:\n   $$ [\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5] $$\n   where each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$.\n\nNotes and constraints:\n\n- Base your reasoning on the definitions of SPD matrices and the existence condition for the Cholesky factorization stated above. Do not assume any unstated properties.\n- There are no physical units involved.\n- Angle units do not apply.\n- The only accepted output is the specified single-line list of booleans in the exact order described.", "solution": "The problem as stated is well-defined, self-contained, and scientifically sound, resting on established principles of numerical linear algebra. It is therefore valid, and we proceed to a full solution. The task requires the implementation of a Cholesky factorization routine and its application to a specified set of test matrices.\n\nThe fundamental principle is the factorization of a symmetric positive-definite (SPD) matrix $A$ into the product $A = LL^{\\mathsf{T}}$, where $L$ is a lower triangular matrix. The elements of $L$, denoted $L_{ij}$, can be computed directly from this equation. For an $n \\times n$ matrix $A$, the element $A_{ij}$ is given by the dot product of the $i$-th row of $L$ and the $j$-th column of $L^{\\mathsf{T}}$ (which is the $j$-th row of $L$):\n$$ A_{ij} = \\sum_{k=1}^{j} L_{ik} L_{jk} \\quad \\text{for } i \\ge j $$\nSince $A$ is symmetric, $A_{ij} = A_{ji}$, and we only need to compute $L_{ij}$ for $i \\ge j$.\n\nThe algorithm computes the columns of $L$ sequentially, from $j=1$ to $j=n$. For each column $j$, we first compute the diagonal element $L_{jj}$ and then the off-diagonal elements $L_{ij}$ for $i > j$.\n\nConsider the diagonal element $A_{jj}$:\n$$ A_{jj} = \\sum_{k=1}^{j} L_{jk}^2 = \\left( \\sum_{k=1}^{j-1} L_{jk}^2 \\right) + L_{jj}^2 $$\nFrom this, we solve for the squared diagonal element of $L$:\n$$ L_{jj}^2 = A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2 $$\nFor the Cholesky factorization to exist with a real-valued matrix $L$ having positive diagonal entries, the term $L_{jj}^2$ must be strictly positive at every step $j$. If $L_{jj}^2 \\le 0$ for any $j$, the matrix is not positive-definite, and the factorization fails. This is the condition our routine must check. If $L_{jj}^2 > 0$, we have $L_{jj} = \\sqrt{A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2}$.\n\nNext, consider the off-diagonal elements $A_{ij}$ for $i > j$:\n$$ A_{ij} = \\sum_{k=1}^{j} L_{ik} L_{jk} = \\left( \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right) + L_{ij} L_{jj} $$\nSolving for $L_{ij}$, we get:\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right) $$\nThis calculation is possible provided $L_{jj} \\ne 0$, which is guaranteed if $L_{jj}^2 > 0$.\n\nWe now analyze the specified test cases based on this algorithm.\n\n1.  **Parametric Family $A(r) = \\begin{bmatrix} 1 & r \\\\ r & 1 \\end{bmatrix}$**\n\n    We apply the algorithm for this $2 \\times 2$ matrix.\n    \n    For column $j=1$:\n    The squared diagonal is $L_{11}^2 = A_{11} = 1$. Since $1 > 0$, this step succeeds, and we find $L_{11} = 1$.\n    The off-diagonal element is $L_{21} = \\frac{1}{L_{11}} (A_{21}) = \\frac{r}{1} = r$.\n    \n    For column $j=2$:\n    The squared diagonal is $L_{22}^2 = A_{22} - L_{21}^2 = 1 - r^2$.\n    The factorization succeeds if and only if this quantity is strictly positive: $1 - r^2 > 0$, which is equivalent to $r^2  1$, or $|r|  1$.\n\n    We evaluate the specified values of $r$:\n    - For $r = 0.9$: $L_{22}^2 = 1 - (0.9)^2 = 1 - 0.81 = 0.19 > 0$. The routine succeeds. Result: $\\text{True}$.\n    - For $r = 0.999999$: $L_{22}^2 = 1 - (0.999999)^2 > 0$. The value is positive, although small. The routine succeeds. Result: $\\text{True}$.\n    - For $r = 1.0$: $L_{22}^2 = 1 - (1.0)^2 = 0$. This is not strictly positive. The routine must fail. Result: $\\text{False}$.\n    - For $r = 1.000001$: $L_{22}^2 = 1 - (1.000001)^2  0$. The routine must fail. Result: $\\text{False}$.\n\n2.  **Fixed Matrix $B = \\begin{bmatrix} 4  1  1 \\\\ 1  3  1 \\\\ 1  1  2 \\end{bmatrix}$**\n\n    We apply the algorithm to this $3 \\times 3$ matrix.\n\n    For column $j=1$:\n    $L_{11}^2 = B_{11} = 4$. Since $4 > 0$, we proceed. $L_{11} = 2$.\n    $L_{21} = B_{21} / L_{11} = 1/2$.\n    $L_{31} = B_{31} / L_{11} = 1/2$.\n\n    For column $j=2$:\n    $L_{22}^2 = B_{22} - L_{21}^2 = 3 - (1/2)^2 = 3 - 1/4 = 11/4$. Since $11/4 > 0$, we proceed. $L_{22} = \\sqrt{11}/2$.\n    $L_{32} = \\frac{1}{L_{22}}(B_{32} - L_{31}L_{21}) = \\frac{1}{\\sqrt{11}/2}(1 - (1/2)(1/2)) = \\frac{2}{\\sqrt{11}}(3/4) = \\frac{3}{2\\sqrt{11}}$.\n\n    For column $j=3$:\n    $L_{33}^2 = B_{33} - (L_{31}^2 + L_{32}^2) = 2 - \\left( (1/2)^2 + \\left(\\frac{3}{2\\sqrt{11}}\\right)^2 \\right) = 2 - \\left( \\frac{1}{4} + \\frac{9}{44} \\right) = 2 - \\left( \\frac{11}{44} + \\frac{9}{44} \\right) = 2 - \\frac{20}{44} = 2 - \\frac{5}{11} = \\frac{17}{11}$.\n    Since $17/11 > 0$, the final step succeeds.\n\n    As all squared diagonal pivots are strictly positive, the Cholesky factorization for matrix $B$ succeeds. Result: $\\text{True}$.\n\nThe sequence of boolean results for the five test cases in the specified order is therefore: $\\text{True}$, $\\text{True}$, $\\text{False}$, $\\text{False}$, $\\text{True}$. The program in the final answer will implement this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Cholesky factorization from first principles,\n    tests it on several matrices, and prints the results.\n    \"\"\"\n\n    def cholesky_factorization_attempt(A: np.ndarray) - bool:\n        \"\"\"\n        Attempts to compute the Cholesky factorization of a symmetric matrix A.\n\n        The routine follows the standard column-wise algorithm. It explicitly\n        checks for the strict positivity of the squared diagonal pivots (L_jj^2).\n        If a non-positive pivot is encountered, the matrix is not positive-definite,\n        and the factorization fails.\n\n        Args:\n            A (np.ndarray): The n x n symmetric matrix to factorize.\n\n        Returns:\n            bool: True if the factorization succeeds, False otherwise.\n        \"\"\"\n        n = A.shape[0]\n        L = np.zeros_like(A, dtype=float)\n\n        for j in range(n):\n            # Compute the sum of squares of elements in the j-th row of L up to column j-1.\n            # This corresponds to sum_{k=0}^{j-1} L[j, k]^2\n            s1 = np.dot(L[j, :j], L[j, :j])\n\n            # Compute the squared diagonal element L[j, j]^2.\n            squared_pivot = A[j, j] - s1\n\n            # The core condition for positive-definiteness in Cholesky factorization:\n            # The pivot must be strictly positive.\n            if squared_pivot = 0:\n                return False\n\n            L[j, j] = np.sqrt(squared_pivot)\n\n            # Compute the elements in the j-th column below the diagonal.\n            if j  n - 1:\n                # This corresponds to the sum sum_{k=0}^{j-1} L[i, k] * L[j, k] for each i  j.\n                s2 = np.dot(L[j + 1:n, :j], L[j, :j])\n                L[j + 1:n, j] = (A[j + 1:n, j] - s2) / L[j, j]\n\n        return True\n\n    # 1. Define the parametric test cases A(r).\n    r_values = [0.9, 0.999999, 1.0, 1.000001]\n    test_matrices = [np.array([[1.0, r], [r, 1.0]]) for r in r_values]\n\n    # 2. Define the fixed 3x3 test case B.\n    B_matrix = np.array([\n        [4.0, 1.0, 1.0],\n        [1.0, 3.0, 1.0],\n        [1.0, 1.0, 2.0]\n    ])\n    test_matrices.append(B_matrix)\n\n    # 3. Evaluate the routine on all test cases.\n    results = []\n    for matrix in test_matrices:\n        success = cholesky_factorization_attempt(matrix)\n        results.append(success)\n\n    # 4. Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2376407"}, {"introduction": "Solving linear systems of the form $A x = b$ is a cornerstone of computational physics, especially when the matrix $A$ is symmetric and positive definite (SPD). This exercise guides you through building a complete Cholesky solver from scratch, combining the factorization $A = L L^{\\mathsf{T}}$ with forward and backward substitution to find the solution $x$. By implementing and testing your solver against a diverse set of matrices, including those that are ill-conditioned or not positive definite, you will develop a robust understanding of the algorithm's practical application and its numerical strengths [@problem_id:2379908].", "problem": "You will implement and test a program that solves symmetric positive definite linear systems using the Cholesky decomposition. In computational physics, many elliptic partial differential equation discretizations lead to linear systems of the form $A x = b$ where $A$ is symmetric positive definite. A fundamental characterization is that $A$ is symmetric positive definite if and only if $x^{\\mathsf{T}} A x > 0$ for every nonzero vector $x$. Under this condition, the quadratic energy $E(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ is strictly convex and has a unique minimizer that satisfies the normal equations $A x = b$. Your task is to build a solver from first principles that exploits this structure.\n\nImplement a function that, given a symmetric positive definite matrix $A$ and a vector $b$, returns the solution $x$ by:\n- Computing a Cholesky factorization $A = L L^{\\mathsf{T}}$ with $L$ lower triangular and strictly positive diagonal entries, without calling any library routine for Cholesky factorization.\n- Solving the lower-triangular system $L y = b$ by forward substitution.\n- Solving the upper-triangular system $L^{\\mathsf{T}} x = y$ by back substitution.\n\nRequirements for the implementation:\n- Input $A$ must be real, square, and symmetric. Your implementation must check symmetry numerically.\n- If $A$ is not positive definite, your factorization should detect this via a nonpositive pivot and signal an error.\n- Use double-precision floating point arithmetic.\n- Do not call any built-in Cholesky solver; implement the factorization and the triangular solves using basic operations.\n\nTest suite:\nYour program must run the solver on the following seven test cases and aggregate the results.\n\n- Case $1$ (small, well-conditioned):\n  $$A_1 = \\begin{bmatrix} 4  1  1 \\\\ 1  3  0 \\\\ 1  0  2 \\end{bmatrix}, \\quad x^{\\star}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad b_1 = A_1 x^{\\star}_1.$$\n  Report the maximum absolute error $e_1 = \\|x_1 - x^{\\star}_1\\|_{\\infty}$ as a floating-point number.\n\n- Case $2$ (identity):\n  $$A_2 = I_{5}, \\quad x^{\\star}_2 = \\begin{bmatrix} -2 \\\\ 0.5 \\\\ 3.5 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\quad b_2 = x^{\\star}_2.$$\n  Report $e_2 = \\|x_2 - x^{\\star}_2\\|_{\\infty}$.\n\n- Case $3$ (diagonal with wide scaling):\n  $$A_3 = \\operatorname{diag}\\!\\left(10^{-3},\\,10^{-1},\\,1,\\,10,\\,10^{3}\\right), \\quad x^{\\star}_3 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ -2 \\\\ 0.5 \\end{bmatrix}, \\quad b_3 = A_3 x^{\\star}_3.$$\n  Report $e_3 = \\|x_3 - x^{\\star}_3\\|_{\\infty}$.\n\n- Case $4$ (one-dimensional Poisson operator with Dirichlet boundaries, banded symmetric positive definite):\n  For $n = 6$, let $A_4 \\in \\mathbb{R}^{n \\times n}$ be tridiagonal with main diagonal equal to $2$ and first sub- and super-diagonals equal to $-1$, i.e.,\n  $$A_4 = \\begin{bmatrix}\n  2  -1  0  0  0  0 \\\\\n  -1  2  -1  0  0  0 \\\\\n  0  -1  2  -1  0  0 \\\\\n  0  0  -1  2  -1  0 \\\\\n  0  0  0  -1  2  -1 \\\\\n  0  0  0  0  -1  2\n  \\end{bmatrix}.$$\n  Take $x^{\\star}_4 = \\mathbf{1} \\in \\mathbb{R}^{6}$ (all ones), and $b_4 = A_4 x^{\\star}_4$. Report $e_4 = \\|x_4 - x^{\\star}_4\\|_{\\infty}$.\n\n- Case $5$ (random symmetric positive definite):\n  Construct $A_5$ as follows. Set a deterministic seed, generate $M \\in \\mathbb{R}^{6 \\times 6}$ with independent standard normal entries, and let\n  $$A_5 = M^{\\mathsf{T}} M + 10^{-3} I_{6}.$$\n  Choose $x^{\\star}_5 \\in \\mathbb{R}^{6}$ with independent standard normal entries, and set $b_5 = A_5 x^{\\star}_5$. Report $e_5 = \\|x_5 - x^{\\star}_5\\|_{\\infty}$.\n\n- Case $6$ (ill-conditioned symmetric positive definite, Hilbert matrix):\n  Let $A_6 \\in \\mathbb{R}^{5 \\times 5}$ be the Hilbert matrix with entries $(A_6)_{ij} = 1/(i+j+1)$ for zero-based indices $i,j \\in \\{0,1,2,3,4\\}$. Let\n  $$x^{\\star}_6 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\quad b_6 = A_6 x^{\\star}_6.$$\n  Report $e_6 = \\|x_6 - x^{\\star}_6\\|_{\\infty}$.\n\n- Case $7$ (symmetric but not positive definite):\n  $$A_7 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}, \\quad b_7 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.$$\n  Run your solver and report a boolean $r_7$ that is $\\text{True}$ if your solver correctly detects non-positive definiteness and signals an error, and $\\text{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries in the order of the cases above, namely $[e_1,e_2,e_3,e_4,e_5,e_6,r_7]$. For example, a schematic format is [$r_1,r_2,r_3,r_4,r_5,r_6,r_7$], where $r_k$ denotes the result for case $k$.\n\nThe final outputs for cases $1$ through $6$ are floating-point numbers. The final output for case $7$ is a boolean. No angles or physical units are involved in this problem; all quantities are dimensionless real numbers.", "solution": "We begin from the definition of a symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$: $A$ is symmetric ($A = A^{\\mathsf{T}}$) and satisfies $x^{\\mathsf{T}} A x > 0$ for all nonzero $x \\in \\mathbb{R}^{n}$. In computational physics, such matrices arise as stiffness matrices for discretized elliptic operators. Consider the quadratic energy\n$$\nE(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x.\n$$\nSince $A$ is positive definite, $E$ is strictly convex and coercive. The unique minimizer $x^{\\star}$ satisfies the first-order optimality condition $\\nabla E(x) = A x - b = 0$, which is equivalent to the linear system $A x = b$.\n\nThe Cholesky factorization leverages positive definiteness to guarantee the existence and uniqueness of a factorization\n$$\nA = L L^{\\mathsf{T}},\n$$\nwhere $L$ is lower triangular with strictly positive diagonal entries. This factorization can be computed via a constructive algorithm that processes $A$ one column at a time. The core identities come from expanding $A = L L^{\\mathsf{T}}$ componentwise. For $k \\in \\{0,\\dots,n-1\\}$, write\n$$\nA_{kk} = \\sum_{j=0}^{k} L_{kj}^{2} \\quad \\text{and} \\quad A_{ik} = \\sum_{j=0}^{k} L_{ij} L_{kj} \\quad \\text{for } i > k.\n$$\nSubtracting the contribution from previously computed entries yields the recursive formulas\n$$\nL_{kk} = \\sqrt{A_{kk} - \\sum_{j=0}^{k-1} L_{kj}^{2}}, \\quad\nL_{ik} = \\frac{A_{ik} - \\sum_{j=0}^{k-1} L_{ij} L_{kj}}{L_{kk}} \\quad \\text{for } i = k+1,\\dots,n-1.\n$$\nThe term under the square root is strictly positive by positive definiteness, which ensures well-defined real square roots and strictly positive pivots.\n\nOnce $L$ is computed, solving $A x = b$ reduces to two triangular solves:\n- Forward substitution solves $L y = b$. For $i = 0,\\dots,n-1$,\n$$\ny_i = \\frac{1}{L_{ii}} \\left( b_i - \\sum_{j=0}^{i-1} L_{ij} y_j \\right).\n$$\n- Back substitution solves $L^{\\mathsf{T}} x = y$. For $i = n-1,\\dots,0$,\n$$\nx_i = \\frac{1}{L_{ii}} \\left( y_i - \\sum_{j=i+1}^{n-1} L_{ji} x_j \\right).\n$$\n\nAlgorithmic design:\n- Validate that $A$ is square and numerically symmetric; a practical check is that $\\|A - A^{\\mathsf{T}}\\|_{\\infty}$ is below a small multiple of machine precision times $\\|A\\|_{\\infty}$.\n- Perform the Cholesky factorization using the recursive formulas above. If at any step the computed pivot $A_{kk} - \\sum_{j=0}^{k-1} L_{kj}^{2}$ is nonpositive (e.g., less than or equal to zero within a numerical tolerance), declare that $A$ is not positive definite and signal an error.\n- Carry out forward and back substitutions using the formulas above.\n- For each symmetric positive definite test case, compute the maximum absolute error $e = \\|x - x^{\\star}\\|_{\\infty} = \\max_i |x_i - x^{\\star}_i|$.\n- For the symmetric but not positive definite test case, catch the signaled error and report a boolean indicating successful detection.\n\nRationale and numerical considerations:\n- No pivoting is required: for symmetric positive definite matrices, Cholesky factorization without pivoting is backward stable in floating-point arithmetic and guarantees positive pivots in exact arithmetic.\n- The work complexity is $\\mathcal{O}(n^{3}/3)$ floating-point operations for the factorization and $\\mathcal{O}(n^{2})$ for the triangular solves.\n- The test suite spans well-conditioned small systems, trivial identity, widely scaled diagonal systems, banded systems typical of elliptic operators, random symmetric positive definite systems, and an ill-conditioned Hilbert matrix to probe numerical robustness. The final test ensures that the solver properly detects the violation of positive definiteness.\n\nThe program implements these steps without calling a library Cholesky routine, constructs each test case exactly as specified, computes the error metrics $e_1,\\dots,e_6$ and the boolean $r_7$, and prints a single line with the list $[e_1,e_2,e_3,e_4,e_5,e_6,r_7]$.", "answer": "```python\nimport numpy as np\n\ndef cholesky_solve(A: np.ndarray, b: np.ndarray, symmetry_tol: float = 1e-12) - np.ndarray:\n    \"\"\"\n    Solve A x = b for symmetric positive definite A using Cholesky factorization A = L L^T,\n    followed by forward and back substitution. Raises ValueError if A is not SPD.\n\n    Parameters:\n        A (np.ndarray): Real symmetric positive definite matrix of shape (n, n).\n        b (np.ndarray): Right-hand side vector of shape (n,).\n        symmetry_tol (float): Tolerance for numerical symmetry check.\n\n    Returns:\n        x (np.ndarray): Solution vector of shape (n,).\n    \"\"\"\n    A = np.array(A, dtype=np.float64, copy=True)\n    b = np.array(b, dtype=np.float64, copy=False)\n    n = A.shape[0]\n\n    if A.ndim != 2 or A.shape[1] != n:\n        raise ValueError(\"Matrix A must be square.\")\n    if b.shape != (n,):\n        raise ValueError(\"Vector b must have shape (n,).\")\n\n    # Symmetry check\n    if not np.allclose(A, A.T, atol=symmetry_tol, rtol=0.0):\n        raise ValueError(\"Matrix A is not symmetric within the given tolerance.\")\n\n    # Cholesky factorization: compute lower-triangular L such that A = L L^T\n    L = np.zeros_like(A)\n    for k in range(n):\n        # Compute diagonal element\n        if k == 0:\n            sumsq = 0.0\n        else:\n            sumsq = float(np.dot(L[k, :k], L[k, :k]))\n        diag = A[k, k] - sumsq\n        if not np.isfinite(diag) or diag = 0.0:\n            raise ValueError(\"Matrix A is not positive definite (nonpositive pivot encountered).\")\n        Lkk = np.sqrt(diag)\n        L[k, k] = Lkk\n\n        # Compute subdiagonal elements in column k\n        if k  n - 1:\n            # Vectorized computation of the remaining entries in column k\n            for i in range(k + 1, n):\n                if k == 0:\n                    cross = 0.0\n                else:\n                    cross = float(np.dot(L[i, :k], L[k, :k]))\n                L[i, k] = (A[i, k] - cross) / Lkk\n\n    # Forward substitution: solve L y = b\n    y = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        if i == 0:\n            acc = 0.0\n        else:\n            acc = float(np.dot(L[i, :i], y[:i]))\n        y[i] = (b[i] - acc) / L[i, i]\n\n    # Back substitution: solve L^T x = y\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        if i == n - 1:\n            acc = 0.0\n        else:\n            acc = float(np.dot(L[i + 1:, i], x[i + 1:]))\n        x[i] = (y[i] - acc) / L[i, i]\n\n    return x\n\n\ndef make_tridiagonal_poisson(n: int) - np.ndarray:\n    \"\"\"Construct the 1D Poisson tridiagonal SPD matrix with 2 on diagonal and -1 on off-diagonals.\"\"\"\n    A = np.zeros((n, n), dtype=np.float64)\n    diag = 2.0\n    off = -1.0\n    for i in range(n):\n        A[i, i] = diag\n        if i + 1  n:\n            A[i, i + 1] = off\n            A[i + 1, i] = off\n    return A\n\n\ndef make_hilbert(n: int) - np.ndarray:\n    \"\"\"Construct the Hilbert matrix of size n with zero-based indexing: A[i,j] = 1 / (i + j + 1).\"\"\"\n    i = np.arange(n, dtype=np.float64).reshape(-1, 1)\n    j = np.arange(n, dtype=np.float64).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\n\ndef solve():\n    results = []\n\n    # Case 1\n    A1 = np.array([[4.0, 1.0, 1.0],\n                   [1.0, 3.0, 0.0],\n                   [1.0, 0.0, 2.0]], dtype=np.float64)\n    x1_true = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n    b1 = A1 @ x1_true\n    x1 = cholesky_solve(A1, b1)\n    e1 = float(np.max(np.abs(x1 - x1_true)))\n    results.append(e1)\n\n    # Case 2\n    A2 = np.eye(5, dtype=np.float64)\n    x2_true = np.array([-2.0, 0.5, 3.5, 0.0, -1.0], dtype=np.float64)\n    b2 = x2_true.copy()\n    x2 = cholesky_solve(A2, b2)\n    e2 = float(np.max(np.abs(x2 - x2_true)))\n    results.append(e2)\n\n    # Case 3\n    A3 = np.diag([1e-3, 1e-1, 1.0, 10.0, 1e3]).astype(np.float64)\n    x3_true = np.array([1.0, -1.0, 2.0, -2.0, 0.5], dtype=np.float64)\n    b3 = A3 @ x3_true\n    x3 = cholesky_solve(A3, b3)\n    e3 = float(np.max(np.abs(x3 - x3_true)))\n    results.append(e3)\n\n    # Case 4\n    n4 = 6\n    A4 = make_tridiagonal_poisson(n4)\n    x4_true = np.ones(n4, dtype=np.float64)\n    b4 = A4 @ x4_true\n    x4 = cholesky_solve(A4, b4)\n    e4 = float(np.max(np.abs(x4 - x4_true)))\n    results.append(e4)\n\n    # Case 5\n    np.random.seed(7)\n    M = np.random.randn(6, 6).astype(np.float64)\n    A5 = M.T @ M + 1e-3 * np.eye(6, dtype=np.float64)\n    x5_true = np.random.randn(6).astype(np.float64)\n    b5 = A5 @ x5_true\n    x5 = cholesky_solve(A5, b5)\n    e5 = float(np.max(np.abs(x5 - x5_true)))\n    results.append(e5)\n\n    # Case 6\n    A6 = make_hilbert(5)\n    x6_true = np.array([1.0, -1.0, 1.0, -1.0, 1.0], dtype=np.float64)\n    b6 = A6 @ x6_true\n    x6 = cholesky_solve(A6, b6)\n    e6 = float(np.max(np.abs(x6 - x6_true)))\n    results.append(e6)\n\n    # Case 7 (non-SPD detection)\n    A7 = np.array([[0.0, 1.0],\n                   [1.0, 0.0]], dtype=np.float64)\n    b7 = np.array([1.0, 2.0], dtype=np.float64)\n    detected = False\n    try:\n        _ = cholesky_solve(A7, b7)\n        detected = False\n    except ValueError:\n        detected = True\n    results.append(detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2379908"}, {"introduction": "In many real-world applications, matrices are not static; they are updated as new data becomes available. Recomputing a full Cholesky factorization after a small change is computationally wasteful. This advanced practice introduces an elegant and efficient $\\mathcal{O}(n^2)$ algorithm to update an existing Cholesky factorization $L L^{\\mathsf{T}}$ when the original matrix undergoes a rank-1 update of the form $A \\rightarrow A + v v^{\\mathsf{T}}$ [@problem_id:2379848]. Mastering this technique provides insight into the sophisticated methods used to maintain computational efficiency in dynamic and iterative numerical problems.", "problem": "You are given a symmetric positive definite matrix factorization in lower-triangular form, namely a matrix $L \\in \\mathbb{R}^{n \\times n}$ with strictly positive diagonal such that $A = L L^{\\mathsf{T}}$, where $A$ is symmetric positive definite. Consider a rank-1 update of the form $A \\rightarrow A + v v^{\\mathsf{T}}$ for a nonzero vector $v \\in \\mathbb{R}^{n}$. Your task is to implement an algorithm that updates $L$ to a new lower-triangular factor $L^{\\prime}$ with positive diagonal, satisfying $A + v v^{\\mathsf{T}} = L^{\\prime} {L^{\\prime}}^{\\mathsf{T}}$, using only $\\mathcal{O}(n^2)$ operations and without recomputing a Cholesky factorization from scratch.\n\nFundamental base and definitions to use:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite if and only if $x^{\\mathsf{T}} A x > 0$ for all nonzero $x \\in \\mathbb{R}^{n}$, and if and only if there exists a unique lower-triangular $L$ with positive diagonal such that $A = L L^{\\mathsf{T}}$ (Cholesky factorization).\n- For any $v \\in \\mathbb{R}^{n}$, the matrix $A + v v^{\\mathsf{T}}$ is symmetric positive definite if $A$ is symmetric positive definite.\n\nYour program must:\n- Implement an in-place style rank-1 update routine operating on a lower-triangular factor $L$ and a vector $v$ to produce $L^{\\prime}$ such that $L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} = L L^{\\mathsf{T}} + v v^{\\mathsf{T}}$, using only $\\mathcal{O}(n^2)$ arithmetic.\n- Verify correctness for each test by computing the Frobenius norm of the difference between the target matrix $A + v v^{\\mathsf{T}}$ and the reconstructed matrix $L^{\\prime} {L^{\\prime}}^{\\mathsf{T}}$, i.e., compute $\\| L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} - (A + v v^{\\mathsf{T}}) \\|_{F}$.\n- Return these norms as floating-point numbers.\n\nTest suite:\nUse the following five test cases. In each case, $L$ is lower-triangular with positive diagonal. All entries are real-valued.\n\n1) Happy-path case ($n = 4$):\n- $L = \\begin{bmatrix}\n2.0  0  0  0 \\\\\n0.5  1.8  0  0 \\\\\n1.2  -0.3  1.5  0 \\\\\n-0.7  0.4  0.6  1.1\n\\end{bmatrix}$\n- $v = \\begin{bmatrix} 0.8 \\\\ -1.2 \\\\ 0.3 \\\\ 0.5 \\end{bmatrix}$\n\n2) Boundary case ($n = 1$):\n- $L = \\begin{bmatrix} 2.0 \\end{bmatrix}$\n- $v = \\begin{bmatrix} 3.0 \\end{bmatrix}$\n\n3) Zero-update edge case ($n = 3$):\n- $L = \\begin{bmatrix}\n1.2  0  0 \\\\\n0.1  1.1  0 \\\\\n-0.2  0.05  0.9\n\\end{bmatrix}$\n- $v = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$\n\n4) Small-perturbation case ($n = 3$):\n- $L = \\begin{bmatrix}\n3.0  0  0 \\\\\n1.0  2.5  0 \\\\\n-1.2  0.7  2.2\n\\end{bmatrix}$\n- $v = \\begin{bmatrix} 1\\times 10^{-8} \\\\ -2\\times 10^{-8} \\\\ 3\\times 10^{-8} \\end{bmatrix}$\n\n5) Ill-conditioned but valid case ($n = 5$):\n- $L = \\begin{bmatrix}\n1\\times 10^{-6}  0  0  0  0 \\\\\n2\\times 10^{-7}  1\\times 10^{-3}  0  0  0 \\\\\n-1\\times 10^{-7}  3\\times 10^{-4}  1\\times 10^{-1}  0  0 \\\\\n5\\times 10^{-8}  -2\\times 10^{-4}  4\\times 10^{-2}  1.0  0 \\\\\n1\\times 10^{-8}  1\\times 10^{-5}  -3\\times 10^{-3}  2\\times 10^{-1}  10.0\n\\end{bmatrix}$\n- $v = \\begin{bmatrix} 1\\times 10^{-6} \\\\ -1\\times 10^{-4} \\\\ 1\\times 10^{-2} \\\\ -1.0 \\\\ 2.0 \\end{bmatrix}$\n\nYour program should:\n- For each test case, compute $A = L L^{\\mathsf{T}}$, compute $L^{\\prime}$ via your rank-1 update routine using the given $L$ and $v$, then compute the Frobenius norm $\\| L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} - (A + v v^{\\mathsf{T}}) \\|_{F}$ as a floating-point value.\n- Produce a single line of output containing the five results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4},r_{5}]$).\n\nNotes:\n- No physical units are involved in this problem.\n- Do not call a generic full Cholesky decomposition on $A + v v^{\\mathsf{T}}$ to obtain $L^{\\prime}$. You must implement the $\\mathcal{O}(n^2)$ rank-1 update algorithm that operates directly on the factor $L$ and the vector $v$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the domain of numerical linear algebra, well-posed, objective, and contains all necessary information for a unique and verifiable solution. We may therefore proceed with the derivation and implementation of the required algorithm.\n\nThe problem requires the computation of an updated Cholesky factorization following a rank-1 update to a symmetric positive definite (SPD) matrix. Given a lower-triangular matrix $L \\in \\mathbb{R}^{n \\times n}$ with positive diagonal elements such that $A = L L^{\\mathsf{T}}$, and a vector $v \\in \\mathbb{R}^{n}$, we seek a new lower-triangular matrix $L^{\\prime} \\in \\mathbb{R}^{n \\times n}$, also with positive diagonal elements, such that $L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} = A + v v^{\\mathsf{T}}$. A naive approach of first computing $A' = A + v v^{\\mathsf{T}}$ and then performing a full Cholesky decomposition of $A'$ would cost $\\mathcal{O}(n^3)$ operations. The requirement is to devise an algorithm with complexity $\\mathcal{O}(n^2)$.\n\nThe fundamental relationship to be satisfied is:\n$$\nL^{\\prime} {L^{\\prime}}^{\\mathsf{T}} = L L^{\\mathsf{T}} + v v^{\\mathsf{T}}\n$$\nThis equation can be expressed in terms of matrix products. Consider the augmented matrix $\\begin{bmatrix} L  v \\end{bmatrix} \\in \\mathbb{R}^{n \\times (n+1)}$. Then, the updated matrix $A' = A + v v^{\\mathsf{T}}$ can be written as:\n$$\nA' = \\begin{bmatrix} L  v \\end{bmatrix} \\begin{bmatrix} L^{\\mathsf{T}} \\\\ v^{\\mathsf{T}} \\end{bmatrix}\n$$\nThis representation does not immediately lead to a triangular factor. A more productive formulation is to consider the transpose, where we have an $(n+1) \\times n$ matrix $M$:\n$$\nM = \\begin{bmatrix} L^{\\mathsf{T}} \\\\ v^{\\mathsf{T}} \\end{bmatrix}\n$$\nThen, the updated matrix is $A' = M^{\\mathsf{T}} M = (L^{\\mathsf{T}})^{\\mathsf{T}} L^{\\mathsf{T}} + (v^{\\mathsf{T}})^{\\mathsf{T}} v^{\\mathsf{T}} = L L^{\\mathsf{T}} + v v^{\\mathsf{T}}$.\nWe are seeking a factorization $A' = L' {L'}^{\\mathsf{T}}$. By identifying $L'^{\\mathsf{T}}$ with an upper-triangular matrix $R$, we have $A' = R^{\\mathsf{T}} R$. This suggests that $R$ is the upper-triangular factor from a QR decomposition of the matrix $M$. That is, if we find an orthogonal matrix $Q \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ such that:\n$$\nQ M = \\begin{bmatrix} R \\\\ 0 \\end{bmatrix}\n$$\nwhere $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular, then $M = Q^{\\mathsf{T}} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix}$. Consequently,\n$$\nA' = M^{\\mathsf{T}} M = \\left( Q^{\\mathsf{T}} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\right)^{\\mathsf{T}} \\left( Q^{\\mathsf{T}} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} R^{\\mathsf{T}}  0 \\end{bmatrix} Q Q^{\\mathsf{T}} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} R^{\\mathsf{T}}  0 \\end{bmatrix} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} = R^{\\mathsf{T}} R\n$$\nWe can therefore identify $L'^{\\mathsf{T}} = R$. The matrix $M$ has a special structure: its top $n \\times n$ block, $L^{\\mathsf{T}}$, is already upper triangular.\n$$\nM = \\begin{bmatrix}\nL^{\\mathsf{T}} \\\\ v^{\\mathsf{T}}\n\\end{bmatrix} = \\begin{bmatrix}\nl_{11}  l_{21}  \\dots  l_{n1} \\\\\n0  l_{22}  \\dots  l_{n2} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\dots  l_{nn} \\\\\nv_1  v_2  \\dots  v_n\n\\end{bmatrix}\n$$\nThis structure is \"almost\" upper triangular, except for the last row. We can introduce zeros into this last row, one element at a time, using a sequence of $n$ Givens rotations. For each column $j=1, \\dots, n$, we apply a rotation in the plane of row $j$ and row $n+1$ to zero out the element $(n+1, j)$, which is $v_j$. This process must be done sequentially from $j=1$ to $n$.\n\nFor the sake of direct implementation on the lower-triangular matrix $L$, we can derive an equivalent column-wise algorithm. This avoids explicit formation of $L^{\\mathsf{T}}$. The algorithm proceeds by updating one column of $L$ at a time, from $j=1$ to $n$. Let $p$ be an auxiliary vector, initialized to $v$. At each step $j$, we use a Givens rotation to combine the diagonal element $L_{jj}$ with the current $j$-th element of $p$, denoted $p_j$. This rotation updates $L_{jj}$ and annihilates $p_j$. The same rotation must then be applied to the rest of the $j$-th column of $L$ (elements $L_{ij}$ for $i > j$) and the corresponding elements of $p$ (elements $p_i$ for $i > j$).\n\nThe algorithm is as follows:\nInitialize an auxiliary vector $p \\leftarrow v$. The matrix $L'$ is to be computed, which can be done in-place on a copy of $L$.\n\nFor $j = 1, 2, \\dots, n$:\n1.  Extract the diagonal element $\\alpha = L_{jj}$ and the vector element $\\beta = p_j$.\n2.  Compute the new diagonal element of $L'$, which is the Euclidean norm:\n    $$\n    L'_{jj} = \\sqrt{\\alpha^2 + \\beta^2}\n    $$\n    Since $L_{jj} > 0$ by definition of the Cholesky factor, $L'_{jj}$ will also be positive (unless both $\\alpha$ and $\\beta$ were zero, which is not the case).\n3.  Define the cosine and sine for the Givens rotation:\n    $$\n    c = \\frac{\\alpha}{L'_{jj}}, \\quad s = \\frac{\\beta}{L'_{jj}}\n    $$\n    These satisfy $c^2 + s^2 = 1$. The transformation $\\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} L'_{jj} \\\\ 0 \\end{pmatrix}$ effectively incorporates $\\beta$ into the diagonal and zeroes the targeted element of $p$.\n4.  Apply this same rotation to the remaining elements of the $j$-th column of $L$ and the vector $p$. For each $i = j+1, \\dots, n$:\n    $$\n    \\begin{pmatrix} L'_{ij} \\\\ p'_{i} \\end{pmatrix} = \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} L_{ij} \\\\ p_{i} \\end{pmatrix}\n    $$\n    This translates to the updates:\n    $$\n    L'_{ij} \\leftarrow c \\cdot L_{ij} + s \\cdot p_i\n    $$\n    $$\n    p_i \\leftarrow -s \\cdot L_{ij} + c \\cdot p_i\n    $$\n    These updates must use the values of $L_{ij}$ and $p_i$ from before the start of step $j$. When implementing in-place, one must be careful to use the \"old\" values for both updates.\n\nThis process is repeated for all columns. The total number of arithmetic operations is dominated by step 4. For each $j$, the update loop for $i$ runs $n-j$ times, each time performing a constant number of multiplications and additions. The total complexity is therefore proportional to $\\sum_{j=1}^{n-1} (n-j) \\approx n^2/2$, which is $\\mathcal{O}(n^2)$ as required. The resulting matrix $L'$ is the desired lower-triangular Cholesky factor of $A + v v^{\\mathsf{T}}$.", "answer": "```python\nimport numpy as np\n\ndef chol_rank1_update(L: np.ndarray, v: np.ndarray) - np.ndarray:\n    \"\"\"\n    Performs a rank-1 update of a Cholesky factorization.\n\n    Given a lower-triangular matrix L and a vector v, computes the \n    lower-triangular matrix L_prime such that:\n    L_prime @ L_prime.T = L @ L.T + v @ v.T\n\n    The algorithm has O(n^2) complexity.\n\n    Args:\n        L: An (n, n) lower-triangular numpy array with a positive diagonal.\n        v: An (n,) numpy array representing the update vector.\n\n    Returns:\n        An (n, n) lower-triangular numpy array L_prime.\n    \"\"\"\n    n = L.shape[0]\n    # Make copies to avoid modifying the original inputs\n    L_prime = L.copy()\n    p = v.copy().astype(float) # Ensure p is float for computations\n\n    for j in range(n):\n        # Extract the diagonal element of L and the j-th element of p\n        alpha = L_prime[j, j]\n        beta = p[j]\n\n        # Compute the new diagonal element for L_prime\n        # np.hypot calculates sqrt(alpha^2 + beta^2) robustly\n        L_prime_jj_new = np.hypot(alpha, beta)\n\n        # Calculate Givens rotation parameters\n        if L_prime_jj_new == 0.0:\n            # This case occurs if alpha and beta are both zero. Since L has a\n            # positive diagonal, alpha  0, so this branch is unlikely.\n            c = 1.0\n            s = 0.0\n        else:\n            c = alpha / L_prime_jj_new\n            s = beta / L_prime_jj_new\n        \n        # Update the diagonal element of L_prime\n        L_prime[j, j] = L_prime_jj_new\n\n        # Apply the rotation to the rest of the column j and vector p\n        if j  n - 1:\n            # Select the sub-column of L and sub-vector of p to be updated\n            L_subcol = L_prime[j+1:n, j]\n            p_subvec = p[j+1:n]\n            \n            # The update for the new p sub-vector must use the OLD L sub-column.\n            # The update for the new L sub-column must use the OLD p sub-vector.\n            # We compute the new p sub-vector first before updating L_prime's column.\n            new_p_subvec = -s * L_subcol + c * p_subvec\n            L_prime[j+1:n, j] = c * L_subcol + s * p_subvec\n            p[j+1:n] = new_p_subvec\n            \n    return L_prime\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the Cholesky rank-1 update validation.\n    \"\"\"\n    test_cases = [\n        # 1) Happy-path case (n=4)\n        (np.array([[2.0, 0, 0, 0], \n                   [0.5, 1.8, 0, 0], \n                   [1.2, -0.3, 1.5, 0], \n                   [-0.7, 0.4, 0.6, 1.1]]), \n         np.array([0.8, -1.2, 0.3, 0.5])),\n        # 2) Boundary case (n=1)\n        (np.array([[2.0]]), \n         np.array([3.0])),\n        # 3) Zero-update edge case (n=3)\n        (np.array([[1.2, 0, 0], \n                   [0.1, 1.1, 0], \n                   [-0.2, 0.05, 0.9]]), \n         np.array([0.0, 0.0, 0.0])),\n        # 4) Small-perturbation case (n=3)\n        (np.array([[3.0, 0, 0], \n                   [1.0, 2.5, 0], \n                   [-1.2, 0.7, 2.2]]), \n         np.array([1e-8, -2e-8, 3e-8])),\n        # 5) Ill-conditioned but valid case (n=5)\n        (np.array([[1e-6, 0, 0, 0, 0],\n                   [2e-7, 1e-3, 0, 0, 0],\n                   [-1e-7, 3e-4, 1e-1, 0, 0],\n                   [5e-8, -2e-4, 4e-2, 1.0, 0],\n                   [1e-8, 1e-5, -3e-3, 2e-1, 10.0]]),\n         np.array([1e-6, -1e-4, 1e-2, -1.0, 2.0])),\n    ]\n\n    results = []\n    for L, v in test_cases:\n        # Compute the target matrix A_prime = L L^T + v v^T\n        A = L @ L.T\n        v_col = v[:, np.newaxis]\n        A_prime_target = A + v_col @ v_col.T\n\n        # Get the updated Cholesky factor L_prime using the O(n^2) algorithm\n        L_prime = chol_rank1_update(L, v)\n        \n        # Reconstruct the matrix from the updated factor\n        A_prime_reconstructed = L_prime @ L_prime.T\n\n        # Compute the Frobenius norm of the difference to verify correctness\n        error_norm = np.linalg.norm(A_prime_reconstructed - A_prime_target, 'fro')\n        results.append(error_norm)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2379848"}]}