{"hands_on_practices": [{"introduction": "In our first practice, we will emulate one of the great achievements in the history of science: the discovery of Kepler's Third Law. This exercise [@problem_id:2410557] shows how a simple data analysis technique can function as a form of symbolic regression to uncover physical laws. By linearizing a hypothesized power-law relationship, $P \\propto a^{n}$, and fitting a model to simulated astronomical data, you will precisely extract the governing exponent and, in a sense, rediscover the law from raw observations.", "problem": "You are tasked with constructing a minimal symbolic regression pipeline to recover a power-law relationship between orbital period and semi-major axis from simulated astronomical data, using only standard numerical tools. The physical setting is a two-body system in which a small body orbits a central mass under Newtonian gravity. The fundamental basis you must use consists of Newton's law of universal gravitation and the definition of centripetal acceleration, combined with kinematic relations linking orbital speed and period for circular motion. From these, a power-law linking the orbital period and semi-major axis follows. You must derive the form of this power-law from these principles and then implement a machine learning model that discovers the exponent of the power-law directly from noisy simulated data.\n\nYou must perform the following tasks:\n\n1. Derive, from first principles, the relationship between the orbital period $P$ in seconds and the semi-major axis $a$ in meters for a circular orbit around a point mass $M$ in kilograms, using Newton's law of universal gravitation with gravitational constant $G$ in SI units. The derivation must start from the equality of gravitational and centripetal acceleration and well-tested kinematic facts. Do not assume any pre-known target formula relating $P$ and $a$.\n\n2. Design a symbolic regression model restricted to the hypothesis class of power laws of the form $P = C a^n$, where $C$ and $n$ are constants. Show how to transform this into a linear model by taking natural logarithms so that you can estimate $n$ via ordinary least squares on the transformed variables. Assume a multiplicative noise model in $P$ that becomes additive, zero-mean noise in $\\ln P$.\n\n3. Implement a complete, runnable program that:\n   - Simulates datasets according to the physical model with the following details:\n     - Use the exact formula for the orbital period $P$ implied by the derivation in task $1$.\n     - Sample $a$ values log-uniformly between specified bounds $a_{\\min}$ and $a_{\\max}$ for each dataset.\n     - Generate noisy observations by adding zero-mean Gaussian noise with standard deviation $\\sigma$ to $\\ln P$ (equivalently, multiplicative log-normal noise on $P$). To ensure determinism and eliminate finite-sample bias in the estimated slope, the noise must be constructed to have zero sample covariance with $\\ln a$. To achieve this, if $\\epsilon_{\\text{raw}} \\sim \\mathcal{N}(0,\\sigma^2)$ denotes the initial noise vector and $x=\\ln a$ with $x_c = x - \\overline{x}$, use\n       $$\\epsilon \\leftarrow \\epsilon_{\\text{raw}} - \\frac{x_c^\\top \\epsilon_{\\text{raw}}}{x_c^\\top x_c} x_c,$$\n       and then set $\\ln P_{\\text{obs}} = \\ln P_{\\text{true}} + \\epsilon$ and $P_{\\text{obs}} = \\exp(\\ln P_{\\text{obs}})$.\n     - Use the following physical constants (in SI units): gravitational constant $G = 6.67430 \\times 10^{-11}\\ \\text{m}^3\\ \\text{kg}^{-1}\\ \\text{s}^{-2}$, Solar mass $M_\\odot = 1.98847 \\times 10^{30}\\ \\text{kg}$, Jupiter mass $M_J = 1.89813 \\times 10^{27}\\ \\text{kg}$, astronomical unit $\\mathrm{AU} = 1.495978707 \\times 10^{11}\\ \\text{m}$.\n     - Use a fixed random seed so that results are reproducible.\n   - Fits the model $\\ln P = \\beta_0 + n \\ln a$ by ordinary least squares and returns the estimated exponent $n$ for each dataset.\n   - Rounds each reported exponent to three decimal places.\n   - Produces the final output as a single line containing a comma-separated list of the exponents for all test cases enclosed in square brackets.\n\nTest Suite:\nSimulate four datasets with the following parameter sets. In all cases, report the discovered exponent $n$ (dimensionless) rounded to three decimal places.\n\n- Case $1$ (happy path, Solar mass, broad dynamic range, low noise):\n  - $M = 1.0 \\times M_\\odot$\n  - $a_{\\min} = 0.3 \\times \\mathrm{AU}$, $a_{\\max} = 5.0 \\times \\mathrm{AU}$\n  - Number of samples $N = 64$\n  - Log-noise standard deviation $\\sigma = 0.02$\n\n- Case $2$ (heavier star, very broad dynamic range, higher noise):\n  - $M = 5.0 \\times M_\\odot$\n  - $a_{\\min} = 0.1 \\times \\mathrm{AU}$, $a_{\\max} = 10.0 \\times \\mathrm{AU}$\n  - Number of samples $N = 128$\n  - Log-noise standard deviation $\\sigma = 0.05$\n\n- Case $3$ (lighter star, narrow dynamic range, low noise):\n  - $M = 0.1 \\times M_\\odot$\n  - $a_{\\min} = 0.5 \\times \\mathrm{AU}$, $a_{\\max} = 1.0 \\times \\mathrm{AU}$\n  - Number of samples $N = 40$\n  - Log-noise standard deviation $\\sigma = 0.02$\n\n- Case $4$ (Jupiter as central mass, meter-scale inputs):\n  - $M = 1.0 \\times M_J$\n  - $a_{\\min} = 1.0 \\times 10^{7}\\ \\text{m}$, $a_{\\max} = 1.0 \\times 10^{9}\\ \\text{m}$\n  - Number of samples $N = 50$\n  - Log-noise standard deviation $\\sigma = 0.03$\n\nFinal Output Format:\nYour program should produce a single line of output containing the four rounded exponents in order for cases $1$ through $4$, as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like\n[1.500,1.500,1.500,1.500]\nwith each value rounded to three decimal places.\n\nAll physical quantities must be handled in SI units internally. The final reported exponents are dimensionless; no physical units are required for the final outputs. Angles do not appear in this task. The final outputs are floats rounded to three decimal places and aggregated into a single list on one line as specified.", "solution": "The problem has been analyzed and is determined to be valid. It is scientifically grounded in Newtonian mechanics, well-posed with a clear objective and methodology, and free from ambiguity or contradiction. We shall proceed with a complete solution.\n\nThe task is to derive the physical law governing orbital motion from first principles and then use this understanding to construct a computational model that recovers the law's parameters from simulated, noisy data. This process is a microcosm of the scientific method itself, bridging theoretical physics with data analysis.\n\n### Part 1: Derivation of the Orbital Period-Radius Relationship\n\nWe begin from fundamental principles of classical mechanics to derive the relationship between the orbital period $P$ and the semi-major axis $a$ for a body in a circular orbit around a central mass $M$. For a circular orbit, the semi-major axis is simply the constant orbital radius, $r = a$.\n\n1.  **Force Balance**: An object in a stable circular orbit is subject to a constant gravitational force, which provides the necessary centripetal force to maintain the circular path. We equate the expressions for Newton's law of universal gravitation, $F_g$, and the centripetal force, $F_c$.\n    $$F_g = F_c$$\n    Let $m$ be the mass of the orbiting body, $M$ be the mass of the central body, $v$ be the orbital velocity, and $a$ be the orbital radius. The gravitational constant is $G$.\n    $$G \\frac{M m}{a^2} = \\frac{m v^2}{a}$$\n\n2.  **Isolating Velocity**: The mass of the orbiting body, $m$, cancels, which is a manifestation of the equivalence principle. We can then rearrange the equation to solve for the square of the orbital velocity, $v^2$.\n    $$v^2 = \\frac{G M}{a}$$\n\n3.  **Kinematic Relation**: The orbital velocity $v$ is the circumference of the orbit, $2 \\pi a$, divided by the time it takes to complete one orbit, the period $P$.\n    $$v = \\frac{2 \\pi a}{P}$$\n\n4.  **Substitution and Final Form**: We substitute this kinematic expression for $v$ back into the equation derived from the force balance.\n    $$\\left(\\frac{2 \\pi a}{P}\\right)^2 = \\frac{G M}{a}$$\n    $$\\frac{4 \\pi^2 a^2}{P^2} = \\frac{G M}{a}$$\n    We now rearrange the algebra to isolate the period $P$. First, we solve for $P^2$.\n    $$P^2 = \\left(\\frac{4 \\pi^2}{G M}\\right) a^3$$\n    Finally, taking the square root of both sides gives the explicit relationship between $P$ and $a$:\n    $$P = \\sqrt{\\frac{4 \\pi^2}{G M}} a^{3/2}$$\n    This is Kepler's Third Law for circular orbits. This equation is of the form $P = C a^n$, where the constant coefficient is $C = \\sqrt{4 \\pi^2 / (G M)}$ and the exponent is $n = 3/2 = 1.5$. This theoretically derived exponent $n=1.5$ is the quantity our machine learning model must recover.\n\n### Part 2: Symbolic Regression Model via Linearization\n\nThe task requires a symbolic regression model restricted to the hypothesis class of power laws, $P = C a^n$. While more complex symbolic regression might involve genetic algorithms to search a space of mathematical expressions, for this restricted class, a simpler and more direct method is available through linearization.\n\n1.  **Logarithmic Transformation**: We apply the natural logarithm, $\\ln$, to both sides of the power-law equation.\n    $$\\ln(P) = \\ln(C a^n)$$\n    Using the properties of logarithms, $\\ln(xy) = \\ln(x) + \\ln(y)$ and $\\ln(x^k) = k \\ln(x)$, we transform the equation:\n    $$\\ln(P) = \\ln(C) + \\ln(a^n) = \\ln(C) + n \\ln(a)$$\n\n2.  **Linear Model Formulation**: This transformed equation is a linear equation. Let us define new variables: $y = \\ln(P)$, $x = \\ln(a)$, the intercept $\\beta_0 = \\ln(C)$, and the slope $\\beta_1 = n$. The model becomes:\n    $$y = \\beta_0 + \\beta_1 x$$\n    This is a simple linear regression model. The original exponent $n$ is now the slope of the line in log-log space.\n\n3.  **Noise Model**: The problem specifies a multiplicative noise model for $P$, which is common for physical quantities that are strictly positive. An observed period, $P_{obs}$, is related to the true period, $P_{true}$, by $P_{obs} = P_{true} \\cdot e^{\\epsilon_{raw}}$, where $\\epsilon_{raw}$ is a random variable drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. Applying the logarithm to the observation gives:\n    $$\\ln(P_{obs}) = \\ln(P_{true} \\cdot e^{\\epsilon_{raw}}) = \\ln(P_{true}) + \\ln(e^{\\epsilon_{raw}}) = \\ln(P_{true}) + \\epsilon_{raw}$$\n    Thus, a multiplicative log-normal noise model in the original space becomes a simple additive Gaussian noise model in the log-transformed space.\n\n4.  **Parameter Estimation**: The unknown parameters $\\beta_0$ and $\\beta_1$ (our exponent $n$) can be estimated using ordinary least squares (OLS). The OLS estimator for the slope $\\beta_1$ for a set of $N$ data points $(x_i, y_i)$ is given by:\n    $$\\hat{\\beta_1} = n_{est} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}$$\n    The problem specifies a non-standard but deterministic noise generation procedure. The noise vector $\\epsilon$ added to $\\ln(P_{true})$ is made to be orthogonal to the centered predictor vector $x_c = x - \\bar{x}$. This means the sample covariance between $x$ and $\\epsilon$ is exactly zero. As shown in the derivation of the OLS estimator, the error term in the estimate of the slope is proportional to this sample covariance. By forcing this covariance to zero, the OLS estimator for the slope will recover the true parameter $n = 1.5$ exactly, irrespective of the noise variance $\\sigma$ or the sample size $N$, provided the computations are done with sufficient numerical precision. The noise will only affect the estimate of the intercept $\\beta_0$. This construction provides a precise analytical test of the implementation's correctness.\n\n### Part 3: Implementation Outline\n\nThe program will execute the following steps for each test case:\n1.  Define all necessary physical constants in SI units: $G$, $M_\\odot$, $M_J$, $\\mathrm{AU}$.\n2.  Set a fixed random seed for reproducibility.\n3.  For each case, establish the parameters: central mass $M$, semi-major axis bounds $a_{min}$ and $a_{max}$, number of samples $N$, and noise standard deviation $\\sigma$. All inputs, such as AU, are converted to SI units (meters).\n4.  Generate $N$ samples of $a$ log-uniformly between $a_{min}$ and $a_{max}$. This is achieved by creating a geometric progression using `numpy.logspace`.\n5.  Calculate the \"true\" orbital period $P_{true}$ for each $a$ using the derived formula $P = \\sqrt{4\\pi^2 / (GM)} a^{3/2}$.\n6.  Transform the data into log-space: $x = \\ln(a)$ and $y_{true} = \\ln(P_{true})$.\n7.  Generate the noise vector $\\epsilon$. First, a raw noise vector $\\epsilon_{raw}$ is drawn from $\\mathcal{N}(0, \\sigma^2)$. Then, it is orthogonalized with respect to the centered predictor vector $x_c = x - \\bar{x}$ using the provided formula: $\\epsilon \\leftarrow \\epsilon_{raw} - \\frac{x_c^\\top \\epsilon_{raw}}{x_c^\\top x_c} x_c$.\n8.  Create the \"observed\" log-period data: $y_{obs} = y_{true} + \\epsilon$.\n9.  Calculate the slope of the best-fit line for the data $(x, y_{obs})$ using the OLS formula. This slope is the estimated exponent $n$.\n10. Round the estimated $n$ to three decimal places and store it.\n11. After processing all cases, format the list of exponents into the required string `\"[n1,n2,n3,n4]\"`.\nThis procedure will be encapsulated in a Python program using the `numpy` library.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of recovering the power-law exponent in Kepler's Third Law\n    from simulated data using a linearized model and ordinary least squares.\n    \"\"\"\n\n    # Physical constants in SI units\n    G = 6.67430e-11  # m^3 kg^-1 s^-2\n    M_SOLAR = 1.98847e30  # kg\n    M_JUPITER = 1.89813e27  # kg\n    AU = 1.495978707e11  # m\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"M\": 1.0 * M_SOLAR,\n            \"a_min\": 0.3 * AU,\n            \"a_max\": 5.0 * AU,\n            \"N\": 64,\n            \"sigma\": 0.02,\n        },\n        {\n            \"M\": 5.0 * M_SOLAR,\n            \"a_min\": 0.1 * AU,\n            \"a_max\": 10.0 * AU,\n            \"N\": 128,\n            \"sigma\": 0.05,\n        },\n        {\n            \"M\": 0.1 * M_SOLAR,\n            \"a_min\": 0.5 * AU,\n            \"a_max\": 1.0 * AU,\n            \"N\": 40,\n            \"sigma\": 0.02,\n        },\n        {\n            \"M\": 1.0 * M_JUPITER,\n            \"a_min\": 1.0e7, # Already in meters\n            \"a_max\": 1.0e9, # Already in meters\n            \"N\": 50,\n            \"sigma\": 0.03,\n        },\n    ]\n\n    # Set a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    results = []\n\n    for case in test_cases:\n        M = case[\"M\"]\n        a_min = case[\"a_min\"]\n        a_max = case[\"a_max\"]\n        N = case[\"N\"]\n        sigma = case[\"sigma\"]\n\n        # 1. Simulate dataset: sample semi-major axis `a` log-uniformly\n        a_samples = np.logspace(np.log10(a_min), np.log10(a_max), N)\n\n        # 2. Calculate true orbital period `P` using the derived formula\n        # P^2 = (4 * pi^2 / (G * M)) * a^3\n        C_squared = (4 * np.pi**2) / (G * M)\n        p_true_samples = np.sqrt(C_squared * a_samples**3)\n\n        # 3. Transform to log-space for linear regression\n        # ln(P) = ln(C_sqrt) + (3/2) * ln(a)\n        x = np.log(a_samples)  # ln(a)\n        y_true = np.log(p_true_samples) # ln(P_true)\n\n        # 4. Generate and orthogonalize noise\n        # Generate raw Gaussian noise\n        eps_raw = np.random.normal(loc=0.0, scale=sigma, size=N)\n        \n        # Center the predictor variable x = ln(a)\n        x_centered = x - np.mean(x)\n\n        # Orthogonalize the noise vector with respect to the centered predictor vector\n        # This ensures the sample covariance between x and the final noise is zero.\n        # eps_ortho = eps_raw - proj_of_eps_raw_onto_x_centered\n        # projection = (x_c.T @ eps_raw / x_c.T @ x_c) * x_c\n        dot_product_xc_eps = np.dot(x_centered, eps_raw)\n        dot_product_xc_xc = np.dot(x_centered, x_centered)\n        \n        # Handle case where x_centered has zero variance (e.g., N=1 or all x are same)\n        if dot_product_xc_xc == 0:\n            eps_ortho = eps_raw\n        else:\n            projection_scalar = dot_product_xc_eps / dot_product_xc_xc\n            eps_ortho = eps_raw - projection_scalar * x_centered\n\n        # 5. Create the observed log-period data with the orthogonalized noise\n        y_obs = y_true + eps_ortho\n\n        # 6. Fit the model ln(P) = beta_0 + n * ln(a) using Ordinary Least Squares\n        # We only need the slope 'n' (beta_1).\n        # The OLS formula for the slope is Cov(x, y) / Var(x).\n        # We use ddof=0 for sample covariance/variance, not unbiased estimates.\n        cov_matrix = np.cov(x, y_obs, ddof=0)\n        # The slope is cov(x,y) / var(x)\n        estimated_n = cov_matrix[0, 1] / cov_matrix[0, 0]\n\n        # 7. Round the result to three decimal places\n        rounded_n = round(estimated_n, 3)\n        results.append(str(rounded_n))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2410557"}, {"introduction": "We now advance from discovering a law's form to learning the parameters of a known, complex model. This practice [@problem_id:2410513] uses the Semi-Empirical Mass Formula (SEMF) from nuclear physics to demonstrate the power of physics-informed feature engineering. You will train a model to predict nuclear binding energies, discovering that when the correct physical features are provided, a simple linear model can learn the underlying theory with near-perfect accuracy, showcasing a powerful synergy between domain knowledge and machine learning.", "problem": "You are tasked with constructing a complete, runnable program that learns to predict the nuclear binding energy from the proton number and neutron number using a data-driven approach grounded in first principles. The target quantity is the total binding energy in mega-electronvolts (MeV) of a nucleus with proton number $Z$ and neutron number $N$. The physical ground-truth mapping for training and evaluation is defined by the Semi-Empirical Mass Formula (SEMF), also known as the Weizsäcker formula, with specified coefficients. The objective is to train a model solely from data generated by this formula and then produce predictions for a prescribed test suite of nuclei.\n\nPhysics definition of the target:\nFor a nucleus with proton number $Z$, neutron number $N$, and mass number $A=Z+N$, the total binding energy $B(Z,N)$ in mega-electronvolts is given by\n$$\nB(Z,N) \\;=\\; a_v A \\;-\\; a_s A^{2/3} \\;-\\; a_c \\frac{Z(Z-1)}{A^{1/3}} \\;-\\; a_a \\frac{(A-2Z)^2}{A} \\;+\\; \\delta(A,Z,N),\n$$\nwhere the pairing term is\n$$\n\\delta(A,Z,N) \\;=\\; \n\\begin{cases}\n+\\dfrac{a_p}{\\sqrt{A}}, & \\text{$Z$ and $N$ even},\\\\[6pt]\n-\\dfrac{a_p}{\\sqrt{A}}, & \\text{$Z$ and $N$ odd},\\\\[6pt]\n0, & \\text{$A$ odd}.\n\\end{cases}\n$$\nUse the following SEMF coefficients (all in mega-electronvolts): $a_v=15.8$, $a_s=18.3$, $a_c=0.714$, $a_a=23.2$, $a_p=12.0$. All outputs must be expressed in mega-electronvolts (MeV).\n\nData generation for training:\n- Construct a training set by evaluating $B(Z,N)$ on the grid with $Z \\in \\{2,3,\\dots,60\\}$ and $N \\in \\{2,3,\\dots,90\\}$. The values of $Z$ and $N$ are integers.\n- Each training example consists of an input pair $(Z,N)$ and the corresponding target $B(Z,N)$ computed from the formula above, with $A=Z+N$, using the given coefficients.\n\nLearning task:\n- Train a model (you may choose any architecture) to approximate the mapping $(Z,N)\\mapsto B(Z,N)$ from the training data generated exactly as specified.\n\nTest suite to evaluate the trained model:\nProvide predictions for the following eight nuclei, each given as a pair $(Z,N)$:\n- $(1,1)$\n- $(2,2)$\n- $(3,3)$\n- $(8,9)$\n- $(26,30)$\n- $(50,70)$\n- $(82,126)$\n- $(92,146)$\n\nAnswer specification and units:\n- For each test case, output the model’s predicted total binding energy $B(Z,N)$ in mega-electronvolts (MeV).\n- Express each result as a floating-point decimal number rounded to three decimal places.\n- Angles are not involved in this problem, so no angle unit applies.\n\nFinal output format:\n- Your program must produce a single line of output containing the eight results as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,\\dots,x_8]$, where each $x_i$ is the rounded floating-point result in MeV corresponding to the $i$-th test case in the order listed above.\n\nDesign for coverage in the test suite:\n- The set includes light nuclei (e.g., $(1,1)$ and $(2,2)$), an odd-odd even-mass nucleus $(3,3)$, an odd-mass nucleus $(8,9)$ where the pairing term vanishes, mid-mass $(26,30)$, and heavy magic-number and near-magic-number nuclei $(82,126)$ and $(92,146)$, as well as a mid-heavy case $(50,70)$. This ensures coverage of different pairing regimes, mass scales, and structural regimes.\n\nYour submission must be a complete program that performs training on the specified grid and prints the predictions for the test suite in the exact format required. No user input is allowed at runtime. All computations must be in mega-electronvolts (MeV), and the final printed numbers must be rounded to three decimal places.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. It is based on the well-established Semi-Empirical Mass Formula (SEMF) from nuclear physics and poses a clear, solvable computational task.\n\nThe problem requires the construction of a model to predict the nuclear binding energy $B(Z, N)$ for a nucleus with proton number $Z$ and neutron number $N$. The ground truth for this learning task is explicitly defined by the SEMF, also known as the Weizsäcker formula. The binding energy $B$ is given in units of mega-electronvolts (MeV) by:\n$$\nB(Z,N) = a_v A - a_s A^{2/3} - a_c \\frac{Z(Z-1)}{A^{1/3}} - a_a \\frac{(A-2Z)^2}{A} + \\delta(A,Z,N)\n$$\nwhere $A = Z+N$ is the mass number. The coefficients are provided as $a_v=15.8$, $a_s=18.3$, $a_c=0.714$, $a_a=23.2$, and $a_p=12.0$, all in units of MeV. The pairing term $\\delta(A,Z,N)$ is defined as:\n$$\n\\delta(A,Z,N) = \n\\begin{cases}\n+a_p/\\sqrt{A}, & \\text{if $Z$ and $N$ are even} \\\\\n-a_p/\\sqrt{A}, & \\text{if $Z$ and $N$ are odd} \\\\\n0, & \\text{if $A$ is odd}\n\\end{cases}\n$$\n\nThe task is framed as a machine learning problem: to train a model on data generated from this formula and then use it for prediction. The most rigorous and appropriate \"architecture\" for this task is a linear regression model, as the SEMF is inherently a linear combination of physically motivated basis functions (features) derived from $Z$ and $N$.\n\nLet us define a feature vector $\\boldsymbol{x}$ with $5$ components, corresponding to the five terms of the SEMF:\n1.  **Volume Term Feature**: $x_1 = A$\n2.  **Surface Term Feature**: $x_2 = A^{2/3}$\n3.  **Coulomb Term Feature**: $x_3 = \\frac{Z(Z-1)}{A^{1/3}}$\n4.  **Asymmetry Term Feature**: $x_4 = \\frac{(A-2Z)^2}{A}$\n5.  **Pairing Term Feature**: $x_5 = p(Z,N)A^{-1/2}$, where $p(Z,N) = +1$ for even-even nuclei, $-1$ for odd-odd nuclei, and $0$ for odd-A nuclei.\n\nWith these features, the binding energy can be expressed as a linear model:\n$$\nB(\\boldsymbol{x}) = \\boldsymbol{w}^T \\boldsymbol{x} = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5\n$$\nThe ideal weight vector $\\boldsymbol{w}_{\\text{ideal}}$ corresponding to the SEMF is $\\boldsymbol{w}_{\\text{ideal}} = [a_v, -a_s, -a_c, -a_a, a_p]^T = [15.8, -18.3, -0.714, -23.2, 12.0]^T$.\n\nThe procedure is as follows:\n1.  **Data Generation**: We generate a training set of input-output pairs. The inputs are pairs of integers $(Z,N)$ from the grid defined by $Z \\in \\{2, 3, \\dots, 60\\}$ and $N \\in \\{2, 3, \\dots, 90\\}$. For each pair $(Z_i, N_i)$, we construct the feature vector $\\boldsymbol{x}_i$ and compute the corresponding \"true\" binding energy $y_i = B(Z_i, N_i)$ using the given SEMF formula and coefficients.\n\n2.  **Model Training**: The training process consists of finding the optimal weight vector $\\boldsymbol{w}$ that best fits the training data. Given the feature matrix $\\boldsymbol{X}$ (where each row is a feature vector $\\boldsymbol{x}_i^T$) and the target vector $\\boldsymbol{y}$ (containing the values $y_i$), we solve the ordinary least squares problem. The solution is given by the normal equation:\n$$\n\\boldsymbol{w} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n$$\nThis linear system is solved to obtain the learned model parameters $\\boldsymbol{w}$. A numerically stable method, such as one provided by `numpy.linalg.lstsq`, is used for this computation.\n\n3.  **Prediction**: Once the model is trained (i.e., the weight vector $\\boldsymbol{w}$ is determined), we can predict the binding energy for any nucleus $(Z_{\\text{test}}, N_{\\text{test}})$. We first construct the feature vector $\\boldsymbol{x}_{\\text{test}}$ for the test nucleus. The predicted binding energy $\\hat{B}$ is then calculated as the dot product:\n$$\n\\hat{B} = \\boldsymbol{w}^T \\boldsymbol{x}_{\\text{test}}\n$$\nThis procedure is applied to each of the eight specified test nuclei. The resulting predictions are rounded to three decimal places as required. Since the chosen model architecture perfectly matches the form of the data-generating function and the data is noise-free, the learned weights $\\boldsymbol{w}$ will be nearly identical to $\\boldsymbol{w}_{\\text{ideal}}$, and the model's predictions will faithfully reproduce the output of the SEMF. This holds true even for test nuclei outside the training range, demonstrating the generalization capability of a model grounded in correct physical principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a model to predict nuclear binding energy based on the Semi-Empirical Mass Formula (SEMF).\n    The program generates training data from the SEMF, trains a linear model, and predicts the binding\n    energy for a specified set of test nuclei.\n    \"\"\"\n\n    # SEMF coefficients (in MeV)\n    COEFFS = {\n        'a_v': 15.8,\n        'a_s': 18.3,\n        'a_c': 0.714,\n        'a_a': 23.2,\n        'a_p': 12.0\n    }\n\n    def get_pairing_factor(Z, N):\n        \"\"\"Calculates the sign of the pairing term.\"\"\"\n        if Z % 2 == 0 and N % 2 == 0:\n            return 1.0  # even-even\n        if Z % 2 != 0 and N % 2 != 0:\n            return -1.0  # odd-odd\n        return 0.0  # odd-A\n\n    def get_semf_features(Z, N):\n        \"\"\"\n        Calculates the 5 feature terms of the SEMF for a given nucleus (Z, N).\n        \"\"\"\n        if Z < 0 or N < 0 or (Z == 0 and N == 0):\n            return np.zeros(5)\n\n        A = float(Z + N)\n\n        # Handle cases where A is zero to avoid division by zero, though not expected here.\n        if A == 0:\n            return np.zeros(5)\n\n        # 1. Volume term feature\n        f1 = A\n        # 2. Surface term feature\n        f2 = A**(2/3)\n        # 3. Coulomb term feature\n        f3 = Z * (Z - 1) / (A**(1/3))\n        # 4. Asymmetry term feature\n        f4 = (A - 2 * Z)**2 / A\n        # 5. Pairing term feature\n        f5 = get_pairing_factor(Z, N) / np.sqrt(A)\n\n        return np.array([f1, f2, f3, f4, f5])\n\n    def get_semf_binding_energy(Z, N, coeffs):\n        \"\"\"\n        Calculates the ground-truth binding energy using the SEMF formula.\n        \"\"\"\n        features = get_semf_features(Z, N)\n        \n        # The ideal weights include the signs from the formula\n        ideal_weights = np.array([\n            coeffs['a_v'],\n            -coeffs['a_s'],\n            -coeffs['a_c'],\n            -coeffs['a_a'],\n            coeffs['a_p']\n        ])\n        \n        return np.dot(features, ideal_weights)\n\n    # --- 1. Data Generation ---\n    # Generate training data from the specified grid.\n    Z_range = range(2, 61)  # Z from 2 to 60\n    N_range = range(2, 91)  # N from 2 to 90\n    \n    X_train_list = []\n    y_train_list = []\n\n    for Z_val in Z_range:\n        for N_val in N_range:\n            features = get_semf_features(Z_val, N_val)\n            target = get_semf_binding_energy(Z_val, N_val, COEFFS)\n            X_train_list.append(features)\n            y_train_list.append(target)\n\n    X_train = np.array(X_train_list)\n    y_train = np.array(y_train_list)\n\n    # --- 2. Model Training ---\n    # Train a linear regression model by solving for the weights.\n    # weights = (X^T X)^-1 X^T y\n    # np.linalg.lstsq is a numerically stable way to do this.\n    weights, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n    # --- 3. Prediction ---\n    # Test suite of nuclei (Z, N)\n    test_cases = [\n        (1, 1),\n        (2, 2),\n        (3, 3),\n        (8, 9),\n        (26, 30),\n        (50, 70),\n        (82, 126),\n        (92, 146)\n    ]\n    \n    results = []\n    for Z_test, N_test in test_cases:\n        test_features = get_semf_features(Z_test, N_test)\n        # Predict binding energy using the trained model (learned weights)\n        prediction = np.dot(test_features, weights)\n        # Round the result to three decimal places\n        rounded_prediction = round(prediction, 3)\n        results.append(rounded_prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2410513"}, {"introduction": "Our final practice takes us to the cutting edge of computational physics, where machine learning is used not just to analyze data but to represent complex physical objects. In this exercise [@problem_id:2410566], you will implement a \"Neural Quantum State\" to find the ground state of a quantum spin system. By using a neural network as a flexible variational ansatz for the system's wavefunction and minimizing its energy according to the variational principle, you will apply a powerful technique from modern research to solve a fundamental problem in quantum mechanics.", "problem": "Design and implement a complete, runnable program that approximates the ground state of a quantum spin system by minimizing the variational energy over a neural network ansatz, and reports the minimized energies for a specified test suite. Consider a one-dimensional ring of $N$ spin-$\\tfrac{1}{2}$ degrees of freedom in the computational basis $\\{ | s \\rangle \\}$, where $s = (s_1,\\dots,s_N)$ with $s_i \\in \\{-1,+1\\}$ denotes the eigenvalues of $\\hat{\\sigma}_i^z$. The Hamiltonian is\n$$\n\\hat{H} = -J \\sum_{i=1}^{N} \\hat{\\sigma}_i^z \\hat{\\sigma}_{i+1}^z - h \\sum_{i=1}^{N} \\hat{\\sigma}_i^x \\,,\n$$\nwith periodic boundary conditions $\\hat{\\sigma}_{N+1}^z \\equiv \\hat{\\sigma}_1^z$. Use a real-valued Artificial Neural Network (ANN) variational ansatz for the wavefunction amplitude in the computational basis,\n$$\n\\Psi_\\theta(s) = \\exp\\!\\Big( w_2^\\top \\tanh(W_1 s + b_1) + b_2 \\Big) \\,,\n$$\nwhere $W_1 \\in \\mathbb{R}^{H \\times N}$, $b_1 \\in \\mathbb{R}^{H}$, $w_2 \\in \\mathbb{R}^{H}$, $b_2 \\in \\mathbb{R}$ are variational parameters collected in $\\theta$, $H \\in \\mathbb{N}$ is the number of hidden units, and $\\tanh(\\cdot)$ is applied elementwise. The variational energy to be minimized is the Rayleigh quotient\n$$\nE(\\theta) = \\frac{\\langle \\Psi_\\theta | \\hat{H} | \\Psi_\\theta \\rangle}{\\langle \\Psi_\\theta | \\Psi_\\theta \\rangle} \\,,\n$$\nwith the inner products taken over the full Hilbert space. In the computational basis, the numerator and denominator can be written as\n$$\n\\langle \\Psi_\\theta | \\Psi_\\theta \\rangle = \\sum_{s} \\Psi_\\theta(s)^2 \\,,\n$$\n$$\n\\langle \\Psi_\\theta | \\hat{H} | \\Psi_\\theta \\rangle = \\sum_{s} \\Psi_\\theta(s)^2 \\left( -J \\sum_{i=1}^{N} s_i s_{i+1} \\right) + \\sum_{s} \\sum_{i=1}^{N} \\left( -h \\right) \\Psi_\\theta(s) \\Psi_\\theta\\!\\big(s^{(i)}\\big) \\,,\n$$\nwhere $s^{(i)}$ denotes the configuration obtained from $s$ by flipping the $i$-th spin, $s_i \\mapsto -s_i$, and indices are taken modulo $N$ in the bond term $s_{i+1}$. All quantities are dimensionless in this model; report energies as dimensionless real numbers.\n\nYour program must search over $\\theta$ to approximate the minimum of $E(\\theta)$ for each test case below, using the same fixed architecture size $H = 8$ for all cases, and then output only the minimized energies in the precise requested format. The Hilbert space is finite, and all sums must be taken exactly over all basis configurations; no stochastic sampling is permitted.\n\nTest suite:\n- Case $1$: $N = 3$, $J = 1.0$, $h = 0.5$.\n- Case $2$: $N = 4$, $J = 1.0$, $h = 1.0$.\n- Case $3$: $N = 5$, $J = 1.0$, $h = 0.0$.\n- Case $4$: $N = 6$, $J = 0.0$, $h = 1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite (for Cases $1$ through $4$). Each entry must be a decimal floating-point number rounded to six digits after the decimal point. For example, a valid output looks like\n[\\dots,\\dots,\\dots,\\dots]\nNo additional text or lines are allowed in the output.", "solution": "The problem as stated is valid. It presents a well-posed and scientifically grounded task in the field of computational physics, specifically the application of machine learning methods to quantum many-body problems. The objective is to find an approximation to the ground state energy of the one-dimensional transverse-field Ising model using a neural network variational ansatz. All necessary components—the Hamiltonian, the form of the ansatz, the definition of the variational energy, and the parameters for the test cases—are provided with mathematical precision. The constraint of performing an exact summation over the entire Hilbert space is computationally feasible for the specified system sizes ($N \\le 6$, Hilbert space dimension $2^N \\le 64$), making the problem self-contained and solvable without recourse to stochastic sampling methods.\n\nThe solution proceeds by minimizing the variational energy, which is the expectation value of the Hamiltonian with respect to the variational state, normalized by the state's norm. This is an application of the Rayleigh-Ritz variational principle, which guarantees that the calculated energy $E(\\theta)$ is an upper bound to the true ground state energy $E_0$, i.e., $E(\\theta) \\ge E_0$. The optimal approximation within the given ansatz family is found by numerically minimizing $E(\\theta)$ with respect to the network parameters $\\theta$.\n\nThe overall procedure is as follows:\n\n1.  **Hilbert Space Construction**: For a system of $N$ spins, the computational basis is comprised of $2^N$ states. Each basis state is a configuration $s = (s_1, \\dots, s_N)$, where $s_i \\in \\{-1, +1\\}$. These $2^N$ configurations are explicitly generated and stored.\n\n2.  **Variational Wavefunction Ansatz**: The problem specifies a real-valued neural network ansatz for the wavefunction amplitude:\n    $$\n    \\Psi_\\theta(s) = \\exp\\!\\Big( w_2^\\top \\tanh(W_1 s + b_1) + b_2 \\Big)\n    $$\n    Here, $\\theta = \\{W_1, b_1, w_2, b_2\\}$ are the variational parameters, with $W_1 \\in \\mathbb{R}^{H \\times N}$, $b_1 \\in \\mathbb{R}^{H}$, $w_2 \\in \\mathbb{R}^{H}$, and $b_2 \\in \\mathbb{R}$. For a fixed number of hidden units $H=8$, the total number of parameters is $H(N+2)+1$. These parameters are flattened into a single vector to be used with a standard numerical optimizer. A function is implemented to calculate the wavefunction amplitude $\\Psi_\\theta(s)$ for any given configuration $s$ and parameter vector $\\theta$.\n\n3.  **Variational Energy Calculation**: The core of the task is the evaluation of the objective function, the variational energy $E(\\theta)$, given by the Rayleigh quotient:\n    $$\n    E(\\theta) = \\frac{\\langle \\Psi_\\theta | \\hat{H} | \\Psi_\\theta \\rangle}{\\langle \\Psi_\\theta | \\Psi_\\theta \\rangle}\n    $$\n    The denominator is the squared norm of the wavefunction, $\\langle \\Psi_\\theta | \\Psi_\\theta \\rangle = \\sum_s \\Psi_\\theta(s)^2$, where the sum is over all $2^N$ configurations.\n\n    The numerator, the expectation value of the Hamiltonian $\\hat{H} = -J \\sum_{i} \\hat{\\sigma}_i^z \\hat{\\sigma}_{i+1}^z - h \\sum_{i} \\hat{\\sigma}_i^x$, is computed as two separate terms:\n    \n    a.  **Diagonal (Ising) Term**: The operator $\\hat{\\sigma}_i^z \\hat{\\sigma}_{i+1}^z$ is diagonal in the computational basis. Its expectation value is:\n    $$\n    E_{\\text{diag}} = \\left\\langle -J \\sum_{i=1}^{N} \\hat{\\sigma}_i^z \\hat{\\sigma}_{i+1}^z \\right\\rangle = \\frac{\\sum_s \\Psi_\\theta(s)^2 \\left( -J \\sum_{i=1}^{N} s_i s_{i+1} \\right)}{\\sum_s \\Psi_\\theta(s)^2}\n    $$\n    This is computed by evaluating the classical energy $-J \\sum_i s_i s_{i+1}$ for each configuration $s$, weighting it by $\\Psi_\\theta(s)^2$, and summing the results.\n\n    b.  **Off-Diagonal (Field) Term**: The operator $\\hat{\\sigma}_i^x$ is off-diagonal. It flips the $i$-th spin, i.e., $\\hat{\\sigma}_i^x |s\\rangle = |s^{(i)}\\rangle$. Its expectation value is:\n    $$\n    E_{\\text{off-diag}} = \\left\\langle -h \\sum_{i=1}^{N} \\hat{\\sigma}_i^x \\right\\rangle = \\frac{\\sum_{s} \\sum_{i=1}^{N} (-h) \\Psi_\\theta(s) \\Psi_\\theta(s^{(i)})}{\\sum_s \\Psi_\\theta(s)^2}\n    $$\n    To compute this efficiently, a mapping from each spin configuration to its corresponding index in the state vector is pre-calculated. This allows for rapid retrieval of the amplitude $\\Psi_\\theta(s^{(i)})$ for any flipped configuration.\n\n    The total energy is $E(\\theta) = E_{\\text{diag}} + E_{\\text{off-diag}}$.\n\n4.  **Numerical Optimization**: The `L-BFGS-B` algorithm, a quasi-Newton method available in `scipy.optimize.minimize`, is employed to find the parameter vector $\\theta$ that minimizes the energy function $E(\\theta)$. The optimization is initialized with a small, randomly generated parameter vector to break symmetry. The algorithm iteratively adjusts the parameters to descend the energy landscape until convergence criteria for the gradient and function value are met.\n\nThis entire procedure is encapsulated and executed for each of the four test cases provided, yielding the minimized variational energy for each set of physical parameters $(N, J, h)$. The final results are rounded to six decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nimport itertools\n\ndef solve():\n    \"\"\"\n    Calculates the ground state energy of the 1D transverse-field Ising model\n    using a neural network variational ansatz and exact summation.\n    \"\"\"\n    \n    H = 8 # Fixed number of hidden units\n\n    def get_spin_configs(N):\n        \"\"\"Generates all 2**N spin configurations for a chain of length N.\"\"\"\n        # Use float64 for compatibility with matrix multiplication\n        return np.array(list(itertools.product([-1.0, 1.0], repeat=N)), dtype=np.float64)\n\n    def unpack_params(params_flat, N, H):\n        \"\"\"Unflattens the parameter vector into network weights and biases.\"\"\"\n        idx = 0\n        W1 = params_flat[idx : idx + H * N].reshape((H, N))\n        idx += H * N\n        b1 = params_flat[idx : idx + H]\n        idx += H\n        w2 = params_flat[idx : idx + H]\n        idx += H\n        b2 = params_flat[idx]\n        return W1, b1, w2, b2\n\n    def psi_theta(params_flat, spins, N, H):\n        \"\"\"Computes the ANN wavefunction amplitude for a batch of spin configurations.\"\"\"\n        W1, b1, w2, b2 = unpack_params(params_flat, N, H)\n        # spins shape: (num_configs, N), W1.T shape: (N, H)\n        hidden_input = spins @ W1.T + b1\n        hidden_output = np.tanh(hidden_input)\n        # hidden_output shape: (num_configs, H), w2 shape: (H,)\n        log_psi = hidden_output @ w2 + b2\n        return np.exp(log_psi)\n\n    def create_objective_function(N, J, h, H, spin_configs, config_to_index_map):\n        \"\"\"\n        Factory to create the energy function for a given set of physical parameters.\n        This function will be the target for the numerical optimizer.\n        \"\"\"\n        def energy_function(params_flat):\n            # Calculate wavefunction amplitudes for all 2**N states\n            psi = psi_theta(params_flat, spin_configs, N, H)\n            psi_sq = np.square(psi)\n            norm_sq = np.sum(psi_sq)\n\n            # Defensive check for numerical stability\n            if norm_sq < 1e-12:\n                return 0.0\n\n            # 1. Diagonal part of H (Ising term)\n            # Roll spin configs to get neighbors for periodic boundary conditions\n            s_ip1 = np.roll(spin_configs, shift=-1, axis=1)\n            # Sum over bonds for each configuration\n            ising_interaction = np.sum(spin_configs * s_ip1, axis=1)\n            E_diag = np.sum(psi_sq * (-J * ising_interaction))\n\n            # 2. Off-diagonal part of H (Transverse-field term)\n            E_offdiag = 0.0\n            num_configs = 2**N\n            # Iterate over each configuration s\n            for k in range(num_configs):\n                config_s = spin_configs[k]\n                psi_s = psi[k]\n                # Iterate over each spin site i to flip\n                for i in range(N):\n                    s_flipped = config_s.copy()\n                    s_flipped[i] *= -1.0\n                    # Find the index of the flipped config using the pre-built map\n                    k_flipped = config_to_index_map[tuple(s_flipped)]\n                    psi_s_flipped = psi[k_flipped]\n                    E_offdiag += -h * psi_s * psi_s_flipped\n            \n            # Total energy is the sum of parts divided by the norm\n            total_energy = (E_diag + E_offdiag) / norm_sq\n            return total_energy\n\n        return energy_function\n\n    def solve_case(N, J, h):\n        \"\"\"\n        Sets up and runs the optimization for a single test case.\n        \"\"\"\n        # 1. Generate all basis states and a lookup map for efficiency\n        spin_configs = get_spin_configs(N)\n        config_to_index_map = {tuple(config): i for i, config in enumerate(spin_configs)}\n        \n        # 2. Create the specific objective function for this case\n        objective = create_objective_function(N, J, h, H, spin_configs, config_to_index_map)\n        \n        # 3. Perform the optimization\n        num_params = H * (N + 2) + 1\n        # Use a fixed random seed for reproducibility of the initial guess\n        np.random.seed(42)\n        initial_params = np.random.randn(num_params) * 0.1\n        \n        # L-BFGS-B is a good choice for this type of unconstrained optimization\n        res = minimize(\n            objective, \n            initial_params, \n            method='L-BFGS-B', \n            options={'maxiter': 2000, 'ftol': 1e-12, 'gtol': 1e-9}\n        )\n        \n        return res.fun\n\n    test_cases = [\n        (3, 1.0, 0.5), # Case 1\n        (4, 1.0, 1.0), # Case 2\n        (5, 1.0, 0.0), # Case 3\n        (6, 0.0, 1.0), # Case 4\n    ]\n\n    results = []\n    for N, J, h in test_cases:\n        minimized_energy = solve_case(N, J, h)\n        results.append(f\"{minimized_energy:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2410566"}]}