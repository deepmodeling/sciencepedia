{"hands_on_practices": [{"introduction": "The heart of a Physics-Informed Neural Network is its loss function, which serves as the bridge between the abstract world of neural network parameters and the concrete laws of physics. This exercise provides a foundational walkthrough of how to construct this crucial component. By working through the classic example of the Poisson equation, you will learn how to translate a partial differential equation and its boundary conditions into a mathematical objective that a PINN can minimize, effectively teaching the network the underlying physics of an electrostatic system. [@problem_id:2126324]", "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.", "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "Real-world physical phenomena are often described by non-linear, time-evolving equations, and a robust computational method must be able to handle this complexity. This practice demonstrates the power and flexibility of the PINN framework by extending the loss function concept to the inviscid Burgers' equation, a model famous for developing shock waves from smooth initial states. You will construct a loss function that handles non-linearity, time dependence, and periodic boundary conditions, showcasing how PINNs can be adapted to capture complex dynamic behaviors. [@problem_id:2126315]", "problem": "A Physics-Informed Neural Network (PINN) is a type of neural network that is trained to solve supervised learning tasks while respecting given laws of physics, which are described by general nonlinear Partial Differential Equations (PDEs). We aim to construct the training objective, known as the loss function, for a PINN designed to model the formation of a shock wave.\n\nThe physical system is governed by the one-dimensional, time-dependent, inviscid Burgers' equation:\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0\n$$\nThe solution $u(t, x)$ is sought within the spatio-temporal domain defined by $x \\in [-1, 1]$ and $t \\in [0, T]$ for some final time $T > 0$.\n\nThe system evolves from a smooth initial condition given by:\n$$\nu(0, x) = -\\sin(\\pi x) \\quad \\text{for } x \\in [-1, 1]\n$$\nThe system is subject to periodic boundary conditions:\n$$\nu(t, -1) = u(t, 1) \\quad \\text{for } t \\in [0, T]\n$$\n\nA neural network, denoted by $\\hat{u}(t, x; \\theta)$, is used to approximate the true solution $u(t, x)$. Here, $\\theta$ represents all the trainable parameters (weights and biases) of the network. The training process involves minimizing a loss function $\\mathcal{L}(\\theta)$ that is evaluated on discrete sets of sample points:\n1.  **Initial Condition Points**: A set $S_{IC} = \\{(0, x_i)\\}_{i=1}^{N_{IC}}$ of $N_{IC}$ points along the initial time slice.\n2.  **Boundary Condition Points**: A set $S_{BC} = \\{(t_j, -1), (t_j, 1)\\}_{j=1}^{N_{BC}}$ of $N_{BC}$ pairs of points on the spatial boundaries at various times.\n3.  **Collocation Points**: A set $S_{PDE} = \\{(t_k, x_k)\\}_{k=1}^{N_{PDE}}$ of $N_{PDE}$ points distributed within the interior of the domain $(t,x) \\in (0, T] \\times (-1, 1)$.\n\nYour task is to construct the total loss function $\\mathcal{L}(\\theta)$. The loss function should be formulated as the sum of the mean squared errors corresponding to the initial condition, the boundary conditions, and the PDE residual. Assume all weighting factors for these three components of the loss are equal to one. Provide the final expression for $\\mathcal{L}(\\theta)$.", "solution": "We approximate the solution $u(t,x)$ of the inviscid Burgers' equation $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0$ with the neural network $\\hat{u}(t,x;\\theta)$. The physics is enforced by driving to zero the residual obtained by substituting $\\hat{u}$ into the PDE, while the data constraints are enforced by matching the initial and boundary conditions.\n\nThe PDE residual at any point $(t,x)$ is defined by substituting $\\hat{u}$ into the governing equation:\n$$\nr_{\\theta}(t,x) \\equiv \\frac{\\partial \\hat{u}}{\\partial t}(t,x;\\theta) + \\hat{u}(t,x;\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t,x;\\theta).\n$$\nAt the collocation points $S_{PDE} = \\{(t_{k},x_{k})\\}_{k=1}^{N_{PDE}}$, the mean squared residual is\n$$\n\\mathcal{L}_{PDE}(\\theta) = \\frac{1}{N_{PDE}} \\sum_{k=1}^{N_{PDE}} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta) + \\hat{u}(t_{k},x_{k};\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta) \\right)^{2}.\n$$\n\nThe initial condition is $u(0,x) = -\\sin(\\pi x)$. At the initial-condition points $S_{IC} = \\{(0,x_{i})\\}_{i=1}^{N_{IC}}$, the mean squared initial-condition error is\n$$\n\\mathcal{L}_{IC}(\\theta) = \\frac{1}{N_{IC}} \\sum_{i=1}^{N_{IC}} \\left( \\hat{u}(0,x_{i};\\theta) + \\sin(\\pi x_{i}) \\right)^{2}.\n$$\n\nThe periodic boundary condition is $u(t,-1) = u(t,1)$. At the boundary points $S_{BC} = \\{(t_{j},-1),(t_{j},1)\\}_{j=1}^{N_{BC}}$, the mean squared boundary-condition error is\n$$\n\\mathcal{L}_{BC}(\\theta) = \\frac{1}{N_{BC}} \\sum_{j=1}^{N_{BC}} \\left( \\hat{u}(t_{j},-1;\\theta) - \\hat{u}(t_{j},1;\\theta) \\right)^{2}.\n$$\n\nWith unit weights for all components, the total loss function is the sum\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{IC}(\\theta) + \\mathcal{L}_{BC}(\\theta) + \\mathcal{L}_{PDE}(\\theta).\n$$\nSubstituting the component definitions gives the explicit expression\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N_{IC}} \\sum_{i=1}^{N_{IC}} \\left( \\hat{u}(0,x_{i};\\theta) + \\sin(\\pi x_{i}) \\right)^{2} + \\frac{1}{N_{BC}} \\sum_{j=1}^{N_{BC}} \\left( \\hat{u}(t_{j},-1;\\theta) - \\hat{u}(t_{j},1;\\theta) \\right)^{2} + \\frac{1}{N_{PDE}} \\sum_{k=1}^{N_{PDE}} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta) + \\hat{u}(t_{k},x_{k};\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta) \\right)^{2}.\n$$", "answer": "$$\\boxed{\\mathcal{L}(\\theta)=\\frac{1}{N_{IC}}\\sum_{i=1}^{N_{IC}}\\left(\\hat{u}(0,x_{i};\\theta)+\\sin(\\pi x_{i})\\right)^{2}+\\frac{1}{N_{BC}}\\sum_{j=1}^{N_{BC}}\\left(\\hat{u}(t_{j},-1;\\theta)-\\hat{u}(t_{j},1;\\theta)\\right)^{2}+\\frac{1}{N_{PDE}}\\sum_{k=1}^{N_{PDE}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta)+\\hat{u}(t_{k},x_{k};\\theta)\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta)\\right)^{2}}$$", "id": "2126315"}, {"introduction": "While including boundary conditions in the loss function is a general strategy, it is not always the most effective. This exercise introduces a more elegant and often superior technique: enforcing boundary conditions by construction. You will devise a simple mathematical transformation that modifies the neural network's output, guaranteeing that it satisfies the specified Dirichlet boundary conditions exactly, regardless of the network's learned parameters. This \"hard constraint\" approach can lead to faster and more stable training, making it a vital tool in the PINN practitioner's toolkit. [@problem_id:2126300]", "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.", "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.", "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$", "id": "2126300"}]}