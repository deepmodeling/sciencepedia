{"hands_on_practices": [{"introduction": "A cornerstone of experimental physics is testing theoretical predictions against measured data. This first practice provides a hands-on application of this principle by fitting simulated experimental data to the Stefan-Boltzmann law, $P = A \\sigma T^4$. You will implement a weighted least-squares fit to determine the best estimate for the radiating area $A$ and, more importantly, use the reduced chi-squared statistic, $\\chi^2_{\\nu}$, and its corresponding $p$-value to quantitatively assess whether the model is a \"good fit\" to the data [@problem_id:2379497]. This exercise builds the fundamental skill of not just fitting a model, but also rigorously evaluating its validity.", "problem": "A black body of unknown effective radiating area $A$ is measured at several absolute temperatures $T$ to obtain the total radiated power $P$. The physical model is the Stefan–Boltzmann law $P = A\\,\\sigma\\,T^{4}$, where the Stefan–Boltzmann constant is $\\sigma = 5.670\\,374\\,419\\times 10^{-8}\\ \\mathrm{W\\,m^{-2}\\,K^{-4}}$. Assume all temperatures are known exactly, and each power measurement $P_i$ has an independent, known standard uncertainty $u_i$.\n\nFor each dataset, treat $A$ as the only unknown model parameter. Using the provided triplets $(T_i,P_i,u_i)$, determine the value $\\hat{A}$ that minimizes the weighted sum of squared residuals (the chi-squared),\n$$\n\\chi^{2}(A) = \\sum_{i=1}^{N}\\frac{\\left[P_i - A\\,\\sigma\\,T_i^{4}\\right]^2}{u_i^{2}},\n$$\nand then compute:\n- the minimized chi-squared $\\chi^{2}_{\\min}$,\n- the reduced chi-squared $\\chi^{2}_{\\nu} = \\chi^{2}_{\\min}/\\nu$ with degrees of freedom $\\nu = N - 1$,\n- the goodness-of-fit $p$-value $p = \\Pr\\!\\left(\\chi^{2}_{\\nu} \\ge \\chi^{2}_{\\min}\\right)$ from the chi-squared distribution with $\\nu$ degrees of freedom,\n- a boolean decision $\\mathrm{accept}$ that is $\\mathrm{True}$ if $p \\ge 0.05$ and $\\mathrm{False}$ otherwise.\n\nAll temperatures $T$ must be treated in kelvins, all powers $P$ and uncertainties $u$ in watts, and the area estimate $\\hat{A}$ in square meters. The statistical significance level is $0.05$. Report $\\hat{A}$ in square meters, $\\chi^{2}_{\\nu}$ dimensionlessly, and $p$ as a decimal number.\n\nUse the following test suite of $3$ datasets:\n\n- Dataset $1$ (noise consistent with the model):\n  - $T$ in kelvins: $[300, 400, 500, 600, 700, 800]$.\n  - $P$ in watts: $[6.92, 21.50, 53.60, 109.40, 204.90, 347.20]$.\n  - $u$ in watts: $[0.30, 0.50, 0.70, 1.00, 1.20, 1.50]$.\n\n- Dataset $2$ (systematic offset relative to the model):\n  - $T$ in kelvins: $[300, 400, 500, 600, 700, 800]$.\n  - $P$ in watts: $[11.90, 26.80, 58.20, 115.40, 208.80, 353.00]$.\n  - $u$ in watts: $[0.50, 0.50, 0.50, 0.70, 0.80, 1.00]$.\n\n- Dataset $3$ (near-perfect data, small uncertainties):\n  - $T$ in kelvins: $[250, 500, 750]$.\n  - $P$ in watts: $[2.21499, 35.43984, 179.41419]$.\n  - $u$ in watts: $[0.10, 0.10, 0.10]$.\n\nYour program must process all datasets in order and produce, for each dataset, a result list of the form $[\\hat{A}, \\chi^{2}_{\\nu}, p, \\mathrm{accept}]$. The final output must be a single line containing a list of these per-dataset lists, with elements separated by commas and enclosed in square brackets. For example, the output format should be like $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$, where each $a_k$ is a float in square meters, each $b_k$ is a float, each $c_k$ is a float, and each $d_k$ is a boolean.", "solution": "The problem requires performing a chi-squared goodness-of-fit analysis for the Stefan-Boltzmann law, $P = A\\,\\sigma\\,T^{4}$, against several experimental datasets. For each dataset, which consists of $N$ measurements $(T_i, P_i, u_i)$, we must find the best-fit value of the effective radiating area, $\\hat{A}$, and evaluate the quality of the fit. The model is linear in the single unknown parameter $A$. This problem is a classic case of weighted linear least-squares regression.\n\nThe quantity to be minimized is the chi-squared statistic, defined as the weighted sum of squared residuals:\n$$\n\\chi^{2}(A) = \\sum_{i=1}^{N}\\frac{\\left[P_i - P_{\\text{model}}(T_i; A)\\right]^2}{u_i^{2}}\n$$\nHere, $P_i$ is the measured power at temperature $T_i$, $u_i$ is the standard uncertainty of the measurement $P_i$, and $P_{\\text{model}}(T_i; A) = A\\,\\sigma\\,T_i^{4}$ is the power predicted by the model. The weights for each data point are implicitly defined as $w_i = 1/u_i^2$.\n\nTo find the value of $A$, denoted $\\hat{A}$, that minimizes $\\chi^{2}(A)$, we must solve the equation $\\frac{d\\chi^2}{dA} = 0$. The derivative is:\n$$\n\\frac{d\\chi^{2}}{dA} = \\frac{d}{dA} \\sum_{i=1}^{N}\\frac{\\left[P_i - A\\,\\sigma\\,T_i^{4}\\right]^2}{u_i^{2}} = \\sum_{i=1}^{N} \\frac{2\\left[P_i - A\\,\\sigma\\,T_i^{4}\\right](-\\sigma\\,T_i^{4})}{u_i^{2}}\n$$\nSetting the derivative to zero and solving for $\\hat{A}$:\n$$\n\\sum_{i=1}^{N} \\frac{\\left[P_i - \\hat{A}\\,\\sigma\\,T_i^{4}\\right](\\sigma\\,T_i^{4})}{u_i^{2}} = 0\n$$\n$$\n\\sum_{i=1}^{N} \\frac{P_i\\,\\sigma\\,T_i^{4}}{u_i^{2}} - \\sum_{i=1}^{N} \\frac{\\hat{A}\\,(\\sigma\\,T_i^{4})^2}{u_i^{2}} = 0\n$$\n$$\n\\hat{A} \\sum_{i=1}^{N} \\frac{(\\sigma\\,T_i^{4})^2}{u_i^{2}} = \\sum_{i=1}^{N} \\frac{P_i\\,\\sigma\\,T_i^{4}}{u_i^{2}}\n$$\nThis gives the analytical expression for the best-fit parameter $\\hat{A}$:\n$$\n\\hat{A} = \\frac{\\sum_{i=1}^{N} (P_i \\sigma T_i^4 / u_i^2)}{\\sum_{i=1}^{N} (\\sigma T_i^4 / u_i)^2}\n$$\n\nOnce $\\hat{A}$ is determined, we can calculate the minimized chi-squared, $\\chi^{2}_{\\min}$, by substituting $\\hat{A}$ back into the original expression:\n$$\n\\chi^{2}_{\\min} = \\chi^{2}(\\hat{A}) = \\sum_{i=1}^{N}\\frac{\\left[P_i - \\hat{A}\\,\\sigma\\,T_i^{4}\\right]^2}{u_i^{2}}\n$$\n\nThe number of degrees of freedom, $\\nu$, is the number of data points, $N$, minus the number of fitted parameters, $M$. In this case, $M=1$ (the parameter $A$), so $\\nu = N - 1$.\n\nThe reduced chi-squared, $\\chi^{2}_{\\nu}$, provides a measure of the goodness-of-fit, normalized by the degrees of freedom:\n$$\n\\chi^{2}_{\\nu} = \\frac{\\chi^{2}_{\\min}}{\\nu}\n$$\nFor a good fit where the model is correct and the uncertainties $u_i$ are accurately estimated, we expect $\\chi^{2}_{\\nu} \\approx 1$. A value $\\chi^{2}_{\\nu} \\gg 1$ suggests a poor fit or underestimated uncertainties, while $\\chi^{2}_{\\nu} \\ll 1$ suggests an overly good fit, possibly due to overestimated uncertainties.\n\nTo formalize the goodness-of-fit test, we calculate the $p$-value. The $p$-value is the probability of obtaining a chi-squared statistic at least as large as the observed $\\chi^{2}_{\\min}$, assuming the null hypothesis (that the model is correct) is true. This probability is calculated from the survival function (1 - CDF) of the chi-squared distribution with $\\nu$ degrees of freedom:\n$$\np = \\Pr(\\chi^2_{\\text{dist}} \\ge \\chi^{2}_{\\min} \\mid \\nu) = \\int_{\\chi^{2}_{\\min}}^{\\infty} f(x; \\nu) dx\n$$\nwhere $f(x; \\nu)$ is the probability density function of the $\\chi^2$ distribution with $\\nu$ degrees of freedom.\n\nThe final decision is made by comparing the $p$-value to a pre-defined significance level, $\\alpha = 0.05$.\n- If $p \\ge 0.05$, the observed deviation from the model is not statistically significant. We accept the model as a plausible description of the data. The boolean `accept` is $\\mathrm{True}$.\n- If $p < 0.05$, the deviation is statistically significant, making it unlikely that the discrepancy is due to random chance alone. We reject the model. The boolean `accept` is $\\mathrm{False}$.\n\nThe procedure will be applied to each of the three datasets provided. For each dataset, we will calculate the tuple $(\\hat{A}, \\chi^{2}_{\\nu}, p, \\mathrm{accept})$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs chi-squared analysis for the Stefan-Boltzmann law on given datasets.\n    \"\"\"\n    # Stefan-Boltzmann constant in W m^-2 K^-4\n    SIGMA = 5.670374419e-8\n\n    # Define the datasets as provided in the problem statement.\n    test_cases = [\n        {\n            \"T\": np.array([300, 400, 500, 600, 700, 800]),  # kelvins\n            \"P\": np.array([6.92, 21.50, 53.60, 109.40, 204.90, 347.20]),  # watts\n            \"u\": np.array([0.30, 0.50, 0.70, 1.00, 1.20, 1.50]),  # watts\n        },\n        {\n            \"T\": np.array([300, 400, 500, 600, 700, 800]),\n            \"P\": np.array([11.90, 26.80, 58.20, 115.40, 208.80, 353.00]),\n            \"u\": np.array([0.50, 0.50, 0.50, 0.70, 0.80, 1.00]),\n        },\n        {\n            \"T\": np.array([250, 500, 750]),\n            \"P\": np.array([2.21499, 35.43984, 179.41419]),\n            \"u\": np.array([0.10, 0.10, 0.10]),\n        },\n    ]\n\n    results = []\n    significance_level = 0.05\n\n    for case in test_cases:\n        T, P, u = case[\"T\"], case[\"P\"], case[\"u\"]\n\n        # Number of data points\n        N = len(T)\n        \n        # Degrees of freedom (N data points - 1 fitted parameter)\n        nu = N - 1\n\n        # Model value for P with A=1, this is sigma * T^4\n        model_base = SIGMA * T**4\n\n        # Calculate the best-fit parameter A_hat using the derived analytical formula\n        # A_hat = sum(P_i * sigma * T_i^4 / u_i^2) / sum((sigma * T_i^4)^2 / u_i^2)\n        numerator = np.sum(P * model_base / u**2)\n        denominator = np.sum(model_base**2 / u**2)\n        A_hat = numerator / denominator\n\n        # Calculate the minimized chi-squared value\n        residuals = P - A_hat * model_base\n        chi2_min = np.sum((residuals / u)**2)\n\n        # Calculate the reduced chi-squared\n        chi2_nu = chi2_min / nu if nu > 0 else 0.0\n\n        # Calculate the p-value (goodness-of-fit)\n        # It's the probability of getting a chi2 value >= chi2_min\n        p_value = chi2.sf(chi2_min, nu)\n\n        # Make the decision based on the significance level\n        accept = p_value >= significance_level\n        \n        # Store results for this dataset\n        results.append([A_hat, chi2_nu, p_value, accept])\n\n    # Format the final output string according to the problem specification\n    # to avoid spaces inside the lists.\n    list_of_strings = []\n    for res in results:\n        # str(True) -> 'True', str(False) -> 'False'\n        # which is the correct boolean representation in this context.\n        list_of_strings.append(f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\")\n    \n    final_output = f\"[{','.join(list_of_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2379497"}, {"introduction": "Scientific inquiry often involves choosing between multiple competing theories that could explain a given phenomenon. This exercise simulates such a scenario, where you must decide whether a dataset is better described by an exponential decay model or a power-law model [@problem_id:2379487]. By performing non-linear chi-squared minimization for both models and comparing their best-fit reduced chi-squared values, you will practice the essential skill of quantitative model selection. This moves beyond simply testing one model to making a principled decision between alternative hypotheses.", "problem": "You are given three independent datasets consisting of triplets $\\{(x_i,y_i,\\sigma_i)\\}_{i=1}^{N}$, where $x_i$ are positive, dimensionless abscissas, $y_i$ are positive, dimensionless ordinates, and $\\sigma_i$ are known, positive, dimensionless standard deviations of independent Gaussian measurement noise for each $y_i$. For each dataset, consider two competing parametric models for a strictly positive, monotonically decreasing signal: the exponential decay model $y(x)=A\\,e^{-\\lambda x}$ with parameters $A>0$ and $\\lambda\\ge 0$, and the power-law model $y(x)=C\\,x^{-\\alpha}$ with parameters $C>0$ and $\\alpha>0$. For each model and each dataset, define the chi-squared statistic as\n$$\n\\chi^2(A,\\lambda)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Ae^{-\\lambda x_i}}{\\sigma_i}\\right)^2,\\qquad\n\\chi^2(C,\\alpha)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Cx_i^{-\\alpha}}{\\sigma_i}\\right)^2,\n$$\nand define the best-fit parameters as those that minimize the corresponding $\\chi^2$ over the indicated parameter domains. Let $p=2$ be the number of free parameters in each model, let $\\nu=N-p$ be the degrees of freedom, and define the reduced chi-squared for each model by $\\chi^2_\\nu=\\chi^2/\\nu$ whenever $\\nu>0$. For each dataset with $\\nu\\le 0$, treat the model comparison as inconclusive. For each dataset with $\\nu>0$, decide the preferred model using the following rule: if $\\lvert \\chi^2_{\\nu,\\mathrm{pow}}-\\chi^2_{\\nu,\\mathrm{exp}}\\rvert \\le \\varepsilon$, declare the result inconclusive; otherwise, prefer the model with the smaller reduced chi-squared. Use the tolerance $\\varepsilon=10^{-2}$. Your program must process the following test suite of datasets and produce the required decisions.\n\nTest Suite (dimensionless):\n- Dataset $\\mathrm{D1}$ (exponential-generated, heteroscedastic):\n  - Abscissas: $x_i=i$ for $i\\in\\{1,2,3,4,5,6,7,8\\}$, so $N=8$.\n  - Ordinates: $y_i = A\\,e^{-\\lambda x_i}$ with $A=2.5$ and $\\lambda=0.5$.\n  - Standard deviations: $\\sigma_i = 0.05 + 0.01\\,y_i$.\n- Dataset $\\mathrm{D2}$ (power-law-generated, homoscedastic):\n  - Abscissas: $x_i=i$ for $i\\in\\{1,2,3,4,5,6,7,8\\}$, so $N=8$.\n  - Ordinates: $y_i = C\\,x_i^{-\\alpha}$ with $C=1.8$ and $\\alpha=1.4$.\n  - Standard deviations: $\\sigma_i = 0.06$ for all $i$.\n- Dataset $\\mathrm{D3}$ (degenerate edge case):\n  - Abscissas: $x_1=1.0$, $x_2=3.0$, so $N=2$.\n  - Ordinates: $y_1=2.0$, $y_2=0.4$.\n  - Standard deviations: $\\sigma_1=0.05$, $\\sigma_2=0.05$.\n\nYour task is to, for each dataset, compute the best-fit parameters that minimize the chi-squared for each model, compute the reduced chi-squared values $\\chi^2_{\\nu,\\mathrm{exp}}$ and $\\chi^2_{\\nu,\\mathrm{pow}}$ when defined, and then, using the decision rule with tolerance $\\varepsilon=10^{-2}$, output for each dataset an integer code: output $0$ if the exponential model is preferred, output $1$ if the power-law model is preferred, and output $-1$ if the comparison is inconclusive (either because $\\nu\\le 0$ for at least one model or because the reduced chi-squared values differ by at most $\\varepsilon$). All quantities are unitless. Your program should produce a single line of output containing the three integer decision codes for $\\mathrm{D1}$, $\\mathrm{D2}$, and $\\mathrm{D3}$ respectively, as a comma-separated list enclosed in square brackets (for example, $[0,1,-1]$).", "solution": "The problem statement is critically validated and found to be valid. It is a well-posed problem in computational physics, specifically in the domain of data analysis and model comparison. The problem is scientifically grounded, objective, and provides a complete and consistent set of data and rules for its resolution. No scientific or logical flaws are present.\n\nThe central task is to compare the goodness-of-fit for two competing models, an exponential decay model $y(x)=A\\,e^{-\\lambda x}$ and a power-law model $y(x)=C\\,x^{-\\alpha}$, for three distinct datasets. The comparison metric is the reduced chi-squared statistic, $\\chi^2_{\\nu}$, defined as $\\chi^2_{\\nu} = \\chi^2 / \\nu$, where $\\nu = N - p$ represents the degrees of freedom. Here, $N$ is the number of data points and $p=2$ is the number of free parameters in each model.\n\nThe foundation of this analysis is the minimization of the chi-squared function for each model and dataset. The chi-squared statistic for the exponential model is given by\n$$\n\\chi^2(A,\\lambda)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Ae^{-\\lambda x_i}}{\\sigma_i}\\right)^2\n$$\nand for the power-law model by\n$$\n\\chi^2(C,\\alpha)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Cx_i^{-\\alpha}}{\\sigma_i}\\right)^2.\n$$\nThe best-fit parameters $(A, \\lambda)$ and $(C, \\alpha)$ are those that minimize these respective $\\chi^2$ values, subject to the constraints $A>0$, $\\lambda\\ge 0$, and $C>0$, $\\alpha>0$.\n\nAs both models are non-linear in their parameters, finding the minimum of the $\\chi^2$ function requires numerical optimization. A quasi-Newton method, specifically the L-BFGS-B algorithm, is appropriate for this task as it can handle the boundary constraints on the parameters. The success of such iterative optimization algorithms is heavily dependent on the provision of a reasonable initial guess for the parameters. An effective strategy for generating such initial guesses is to linearize the models.\n\nFor the exponential model, taking the natural logarithm yields $\\ln(y) = \\ln(A) - \\lambda x$. This is a linear relationship between $\\ln(y)$ and $x$. A weighted linear least-squares fit of $\\ln(y_i)$ versus $x_i$ can provide initial estimates for $\\ln(A)$ (the intercept) and $-\\lambda$ (the slope). The weights must account for the propagation of uncertainty, where the variance of $\\ln(y_i)$ is approximately $(\\sigma_i/y_i)^2$.\n\nSimilarly, for the power-law model, the logarithmic transformation $\\ln(y) = \\ln(C) - \\alpha \\ln(x)$ establishes a linear relationship between $\\ln(y)$ and $\\ln(x)$. A weighted linear least-squares fit of $\\ln(y_i)$ versus $\\ln(x_i)$ provides initial estimates for $\\ln(C)$ and $-\\alpha$, using the same error propagation for the weights.\n\nThe procedure for each dataset is as follows:\n1.  Determine the number of data points, $N$, and calculate the degrees of freedom, $\\nu = N - p = N - 2$.\n2.  If $\\nu \\le 0$, the problem statement dictates that the comparison is inconclusive. This applies to Dataset $\\mathrm{D3}$, for which $N=2$ and thus $\\nu=0$.\n3.  If $\\nu > 0$, proceed with the fitting procedure for both models.\n    a. For each model, generate an initial parameter guess using the linearized weighted least-squares method.\n    b. Use the L-BFGS-B algorithm to find the parameters that minimize the non-linear $\\chi^2$ function, starting from the initial guess and respecting the parameter bounds.\n    c. Record the minimized chi-squared value, $\\chi^2_{\\mathrm{min}}$.\n    d. Calculate the reduced chi-squared, $\\chi^2_\\nu = \\chi^2_{\\mathrm{min}} / \\nu$.\n4.  Apply the decision rule: If $\\lvert \\chi^2_{\\nu,\\mathrm{pow}}-\\chi^2_{\\nu,\\mathrm{exp}}\\rvert \\le \\varepsilon$, where the tolerance is $\\varepsilon=10^{-2}$, the result is inconclusive. Otherwise, the model with the smaller $\\chi^2_\\nu$ value is preferred.\n\nBased on this rigorous procedure, integer codes are assigned: $0$ for the exponential model, $1$ for the power-law model, and $-1$ for an inconclusive result. The datasets are synthetic and generated without stochastic noise, meaning the correct model for Dataset $\\mathrm{D1}$ (exponential) and Dataset $\\mathrm{D2}$ (power-law) should yield a $\\chi^2$ value that is numerically close to zero, ensuring a decisive comparison.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for three datasets by performing\n    chi-squared minimization and applying a decision rule.\n    \"\"\"\n    \n    # Define the decision rule tolerance\n    EPSILON = 1e-2\n    # Number of free parameters in each model\n    P_PARAMS = 2\n\n    # --- Dataset Definitions ---\n    # Dataset D1 (exponential-generated, heteroscedastic)\n    x1 = np.arange(1, 9, dtype=float)\n    A_true_d1, lambda_true_d1 = 2.5, 0.5\n    y1 = A_true_d1 * np.exp(-lambda_true_d1 * x1)\n    sigma1 = 0.05 + 0.01 * y1\n    N1 = len(x1)\n    dataset1 = (x1, y1, sigma1, N1)\n\n    # Dataset D2 (power-law-generated, homoscedastic)\n    x2 = np.arange(1, 9, dtype=float)\n    C_true_d2, alpha_true_d2 = 1.8, 1.4\n    y2 = C_true_d2 * x2**(-alpha_true_d2)\n    sigma2 = np.full_like(x2, 0.06)\n    N2 = len(x2)\n    dataset2 = (x2, y2, sigma2, N2)\n\n    # Dataset D3 (degenerate edge case)\n    x3 = np.array([1.0, 3.0])\n    y3 = np.array([2.0, 0.4])\n    sigma3 = np.array([0.05, 0.05])\n    N3 = len(x3)\n    dataset3 = (x3, y3, sigma3, N3)\n\n    test_cases = [dataset1, dataset2, dataset3]\n\n    # --- Fitting and Analysis Functions ---\n    def get_initial_guess(x, y, sigma, model_type):\n        \"\"\"\n        Calculates initial parameter guesses by linearizing the model\n        and performing a weighted linear least-squares fit.\n        \"\"\"\n        # Filter out non-positive y values for log transformation\n        valid_indices = y > 0\n        if not np.any(valid_indices):\n            return [1.0, 1.0] # Default guess if no valid data\n        x_f, y_f, sigma_f = x[valid_indices], y[valid_indices], sigma[valid_indices]\n\n        weights = (y_f / sigma_f)**2\n\n        if model_type == 'exp':\n            # Linear model: log(y) = log(A) - lambda * x\n            fit_x = x_f\n            fit_y = np.log(y_f)\n            m, b = np.polyfit(fit_x, fit_y, 1, w=weights)\n            A0, lambda0 = np.exp(b), -m\n            return [A0, lambda0]\n        elif model_type == 'pow':\n            # Linear model: log(y) = log(C) - alpha * log(x)\n            fit_x = np.log(x_f)\n            fit_y = np.log(y_f)\n            m, b = np.polyfit(fit_x, fit_y, 1, w=weights)\n            C0, alpha0 = np.exp(b), -m\n            return [C0, alpha0]\n        return [1.0, 1.0]\n\n    def fit_model_and_get_chi2(x, y, sigma, model_type):\n        \"\"\"\n        Performs chi-squared minimization for a given model and dataset.\n        Returns the minimized chi-squared value.\n        \"\"\"\n        if model_type == 'exp':\n            model_func = lambda p, x_d: p[0] * np.exp(-p[1] * x_d)\n            bounds = [(1e-9, None), (0.0, None)]  # A > 0, lambda >= 0\n        elif model_type == 'pow':\n            model_func = lambda p, x_d: p[0] * x_d**(-p[1])\n            bounds = [(1e-9, None), (1e-9, None)]  # C > 0, alpha > 0\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        chi2_func = lambda p: np.sum(((y - model_func(p, x)) / sigma)**2)\n        \n        initial_guess = get_initial_guess(x, y, sigma, model_type)\n        # Ensure initial guess is within bounds\n        initial_guess[0] = max(bounds[0][0], initial_guess[0]) if bounds[0][1] is None else min(bounds[0][1], max(bounds[0][0], initial_guess[0]))\n        initial_guess[1] = max(bounds[1][0], initial_guess[1]) if bounds[1][1] is None else min(bounds[1][1], max(bounds[1][0], initial_guess[1]))\n\n        result = minimize(chi2_func, initial_guess, method='L-BFGS-B', bounds=bounds)\n        return result.fun\n\n    # --- Main Loop for Processing Test Cases ---\n    results = []\n    for x_data, y_data, sigma_data, N in test_cases:\n        nu = N - P_PARAMS\n\n        if nu = 0:\n            results.append(-1)\n            continue\n\n        chi2_min_exp = fit_model_and_get_chi2(x_data, y_data, sigma_data, 'exp')\n        chi2_min_pow = fit_model_and_get_chi2(x_data, y_data, sigma_data, 'pow')\n\n        chi2_nu_exp = chi2_min_exp / nu\n        chi2_nu_pow = chi2_min_pow / nu\n\n        if abs(chi2_nu_pow - chi2_nu_exp) = EPSILON:\n            results.append(-1)\n        elif chi2_nu_exp  chi2_nu_pow:\n            results.append(0)  # Exponential model preferred\n        else:\n            results.append(1)  # Power-law model preferred\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2379487"}, {"introduction": "When searching for a new signal in a large dataset, a common pitfall is the \"look-elsewhere effect\": the more places you look, the more likely you are to find a statistical fluctuation that mimics a real signal. This advanced practice explores this critical concept through simulation, demonstrating how the distribution of your test statistic changes when you search for a signal peak across many possible locations [@problem_id:2379498]. By observing this effect firsthand, you will gain a crucial, intuitive understanding of multiple hypothesis testing and why simply finding a low $\\chi^2$ value is not always sufficient proof of a discovery.", "problem": "You will study the statistical impact of searching for a localized signal at many possible locations on the distribution of best-fit chi-squared values. Consider a one-dimensional array of $N$ sampled points at positions $x_j$ for $j \\in \\{0,1,\\dots,N-1\\}$ with $x_j = j/(N-1)$. For each synthetic trial, generate independent noise-only data $y_j$ with $y_j \\sim \\mathcal{N}(0,\\sigma^2)$ and $\\sigma = 1$. For a fixed candidate center $\\mu$ and a fixed width $s  0$, define the template $g_j(\\mu) = \\exp\\!\\big(-\\tfrac{(x_j - \\mu)^2}{2 s^2}\\big)$. The signal-plus-noise model for the trial data is $y_j \\approx A\\, g_j(\\mu)$, where $A$ is an unknown amplitude. For each candidate $\\mu$, the goodness-of-fit is quantified by the chi-squared\n$$\n\\chi^2(\\mu) = \\sum_{j=0}^{N-1} \\frac{\\big(y_j - A\\, g_j(\\mu)\\big)^2}{\\sigma^2},\n$$\nand the best-fit chi-squared at that $\\mu$ is the minimum of $\\chi^2(\\mu)$ with respect to $A$. When searching over a finite set of $M$ candidate locations $\\{\\mu_i\\}_{i=0}^{M-1}$, the search-wise best-fit chi-squared for the trial is\n$$\n\\chi^2_{\\min} = \\min_{0 \\le i \\le M-1} \\left[ \\min_{A \\in \\mathbb{R}} \\chi^2(\\mu_i) \\right].\n$$\nAll trials are to be independent, and all candidate centers must be chosen deterministically as $M$ equally spaced points in $[0,1]$, that is $\\mu_i = i/(M-1)$ for $i \\in \\{0,1,\\dots,M-1\\}$; when $M=1$, take $\\mu_0=0$. The noise variance $\\sigma^2$ must be treated as known and equal to $1$.\n\nFor each test case below, run $T$ independent trials constructed as described. In each trial compute $\\chi^2_{\\min}$. Across the $T$ trials, compute the following three quantities:\n- The empirical mean of $\\chi^2_{\\min}$.\n- The empirical standard deviation of $\\chi^2_{\\min}$.\n- The empirical fraction of trials in which $\\chi^2_{\\min}$ is less than or equal to the $5$th percentile of a chi-squared distribution with $N-1$ degrees of freedom (degrees of freedom is the number of independent pieces of information, abbreviated as DoF). This fraction must be expressed as a decimal number, not as a percentage.\n\nUse a single pseudo-random number generator initialized once at the start of the program with seed equal to $123456789$, and do not reseed between test cases. The template width $s$ is fixed within each test case as specified below. All arithmetic is dimensionless; no physical units are involved.\n\nTest suite (each tuple is $(N,M,s,T)$):\n- Case $1$: $(64, 1, 0.08, 5000)$.\n- Case $2$: $(64, 16, 0.08, 5000)$.\n- Case $3$: $(64, 64, \\tfrac{1}{2\\,(64-1)}, 5000)$.\n\nYour program must produce a single line of output containing the results for Cases $1$, $2$, and $3$ in that order as a comma-separated list of three-element lists, where within each inner list the three numbers are, respectively, the empirical mean, the empirical standard deviation, and the empirical fraction as defined above. Each number must be rounded to exactly six decimal places. The required final output format is `[[mean1,std1,frac1],[mean2,std2,frac2],[mean3,std3,frac3]]`. For example, `[[12.345678,9.876543,0.042000],[...],[...]]`.", "solution": "The central object is the chi-squared function for a single candidate location $\\mu$,\n$$\n\\chi^2(\\mu;A) \\equiv \\sum_{j=0}^{N-1} \\frac{\\big(y_j - A\\, g_j(\\mu)\\big)^2}{\\sigma^2},\n$$\nwith known noise variance $\\sigma^2 = 1$, trial data $y_j$, and template $g_j(\\mu) = \\exp\\!\\big(-\\tfrac{(x_j - \\mu)^2}{2 s^2}\\big)$ defined on the grid $x_j = j/(N-1)$. The best-fit amplitude at fixed $\\mu$ minimizes $\\chi^2(\\mu;A)$ with respect to $A$. Taking the derivative with respect to $A$ and setting it to zero yields the normal equation\n$$\n\\frac{\\partial \\chi^2(\\mu;A)}{\\partial A} = -2 \\sum_{j=0}^{N-1} \\frac{g_j(\\mu)}{\\sigma^2} \\left(y_j - A\\, g_j(\\mu)\\right) = 0,\n$$\nwhich solves to\n$$\n\\widehat{A}(\\mu) = \\frac{\\sum_{j=0}^{N-1} g_j(\\mu)\\, y_j}{\\sum_{j=0}^{N-1} \\big(g_j(\\mu)\\big)^2}.\n$$\nSubstituting $\\widehat{A}(\\mu)$ back into $\\chi^2(\\mu;A)$, the minimized chi-squared for that $\\mu$ can be written in closed form as\n$$\n\\chi^2_{\\min}(\\mu) = \\sum_{j=0}^{N-1} \\frac{y_j^2}{\\sigma^2} - \\frac{\\left(\\sum_{j=0}^{N-1} g_j(\\mu)\\, y_j\\right)^2}{\\sigma^2 \\sum_{j=0}^{N-1} \\big(g_j(\\mu)\\big)^2}.\n$$\nWith $\\sigma^2=1$ this simplifies to\n$$\n\\chi^2_{\\min}(\\mu) = \\sum_{j=0}^{N-1} y_j^2 - \\frac{\\left(\\sum_{j=0}^{N-1} g_j(\\mu)\\, y_j\\right)^2}{\\sum_{j=0}^{N-1} \\big(g_j(\\mu)\\big)^2}.\n$$\nThis expression has a clear geometric interpretation: Let $\\mathbf{y}\\in\\mathbb{R}^N$ be the data vector and $\\mathbf{g}(\\mu)\\in\\mathbb{R}^N$ the template vector. The least-squares fit removes the component of $\\mathbf{y}$ parallel to $\\mathbf{g}(\\mu)$, leaving the squared norm of the orthogonal residual. That is,\n$$\n\\chi^2_{\\min}(\\mu) = \\|\\mathbf{y}\\|_2^2 - \\frac{\\langle \\mathbf{g}(\\mu), \\mathbf{y}\\rangle^2}{\\|\\mathbf{g}(\\mu)\\|_2^2}.\n$$\nUnder the noise-only null hypothesis with $\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$ and fixed $\\mu$, the minimized chi-squared $\\chi^2_{\\min}(\\mu)$ follows a chi-squared distribution with $N-1$ degrees of freedom (DoF), because one linear parameter has been fitted and thus one dimension (the span of $\\mathbf{g}(\\mu)$) is removed from the $N$-dimensional Gaussian, leaving $N-1$ independent Gaussian components contributing to the sum of squares. In particular, its mean is $N-1$ and its variance is $2(N-1)$.\n\nThe look-elsewhere effect arises when we search over multiple candidate locations $\\{\\mu_i\\}_{i=0}^{M-1}$ and take the minimum over the set $\\{\\chi^2_{\\min}(\\mu_i)\\}$. Because we select the smallest of several correlated random variables, the distribution of $\\chi^2_{\\min} \\equiv \\min_i \\chi^2_{\\min}(\\mu_i)$ is shifted toward smaller values compared to the single-location case. As a consequence, the probability that $\\chi^2_{\\min}$ falls below a fixed quantile (for example, the $5$th percentile of the $\\chi^2_{N-1}$ distribution) increases with $M$.\n\nAlgorithmically, for each test case with specified $(N,M,s,T)$ and a fixed grid $x_j = j/(N-1)$ and candidate centers $\\mu_i = i/(M-1)$ (with $\\mu_0=0$ if $M=1$), the following steps compute the required statistics:\n1. Precompute the $M\\times N$ template matrix $G$ with entries $G_{i j} = g_j(\\mu_i)$ and the vector $S\\in\\mathbb{R}^M$ with entries $S_i = \\sum_{j=0}^{N-1} G_{i j}^2$.\n2. For each of the $T$ trials, draw $\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, compute $q = \\|\\mathbf{y}\\|_2^2$, compute the $M$ inner products $\\mathbf{h} = G\\,\\mathbf{y}$, and obtain the $M$ minimized chi-squared values via $\\chi^2_{\\min}(\\mu_i) = q - h_i^2/S_i$.\n3. Record $\\chi^2_{\\min} = \\min_i \\chi^2_{\\min}(\\mu_i)$ for the trial.\n4. After $T$ trials, compute the empirical mean $\\overline{\\chi^2_{\\min}}$, the empirical standard deviation, and the empirical fraction of trials with $\\chi^2_{\\min} \\le q_{0.05}$, where $q_{0.05}$ is the $5$th percentile of the $\\chi^2_{N-1}$ distribution. The $5$th percentile $q_{0.05}$ is defined by the cumulative distribution function (CDF) $F_{\\chi^2_{N-1}}(q_{0.05}) = 0.05$.\n\nFor Case $1$ with $(N,M,s,T)=(64,1,0.08,5000)$, the distribution of $\\chi^2_{\\min}$ matches $\\chi^2_{63}$ exactly in the noise-only setting, so the theoretical mean is $64-1=63$ and the theoretical standard deviation is $\\sqrt{2(64-1)} = \\sqrt{126}$. The empirical estimates from the simulation will be close to these values given $T=5000$. For Cases $2$ and $3$, where $M=16$ and $M=64$ respectively, the empirical mean will be strictly less than $63$, and the empirical fraction falling below the $5$th percentile of $\\chi^2_{63}$ will be larger than $0.05$, illustrating the look-elsewhere effect. The program computes these quantities exactly as specified, using a single pseudo-random number generator initialized once with seed $123456789$, and outputs the three requested values for each case rounded to six decimal places in the required single-line format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef simulate_case(N, M, s, T, rng):\n    # Grid of sample positions x_j in [0,1]\n    x = np.linspace(0.0, 1.0, N)\n    # Candidate centers mu_i, equally spaced in [0,1]; if M==1, mu=[0.0]\n    if M == 1:\n        mu = np.array([0.0])\n    else:\n        mu = np.linspace(0.0, 1.0, M)\n    # Build template matrix G of shape (M, N)\n    # G[i, j] = exp( - (x_j - mu_i)^2 / (2 s^2) )\n    # Handle very small s robustly: s > 0 by construction in test suite.\n    G = np.exp(-0.5 * ((x[None, :] - mu[:, None]) / s) ** 2)\n    # Precompute S_i = sum_j G[i,j]^2\n    S = np.sum(G * G, axis=1)  # shape (M,)\n    # Draw all trials at once: Y shape (T, N), y_j ~ N(0,1)\n    Y = rng.normal(loc=0.0, scale=1.0, size=(T, N))\n    # q_t = sum_j Y[t,j]^2\n    q = np.einsum('ij,ij->i', Y, Y)  # shape (T,)\n    # Compute H = G @ Y^T, shape (M, T)\n    H = G @ Y.T\n    # Compute chi2 for each (i,t): chi2(i,t) = q_t - H[i,t]^2 / S[i]\n    # Broadcast S over trials\n    chi2_all = q[None, :] - (H * H) / S[:, None]\n    # Minimum across candidate locations for each trial\n    chi2_min = np.min(chi2_all, axis=0)  # shape (T,)\n    # Empirical statistics\n    mean_val = float(np.mean(chi2_min))\n    std_val = float(np.std(chi2_min, ddof=0))\n    # Threshold at 5th percentile of chi2 with df = N - 1\n    df = N - 1\n    thresh = chi2.ppf(0.05, df=df)\n    frac_low = float(np.mean(chi2_min = thresh))\n    return mean_val, std_val, frac_low\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (N, M, s, T)\n    test_cases = [\n        (64, 1, 0.08, 5000),\n        (64, 16, 0.08, 5000),\n        (64, 64, 1.0 / (2.0 * (64 - 1)), 5000),\n    ]\n\n    # Initialize a single RNG with fixed seed and do not reseed between cases\n    rng = np.random.default_rng(123456789)\n\n    results = []\n    for N, M, s, T in test_cases:\n        mean_val, std_val, frac_low = simulate_case(N, M, s, T, rng)\n        results.append((mean_val, std_val, frac_low))\n\n    # Build the exact output string: list of lists with numbers rounded to 6 decimals, no extra spaces\n    formatted_cases = []\n    for mean_val, std_val, frac_low in results:\n        formatted = f\"[{mean_val:.6f},{std_val:.6f},{frac_low:.6f}]\"\n        formatted_cases.append(formatted)\n    output = \"[\" + \",\".join(formatted_cases) + \"]\"\n\n    # Final print statement in the exact required format.\n    print(output)\n\nsolve()\n```", "id": "2379498"}]}