{"hands_on_practices": [{"introduction": "We begin with a foundational task in computational physics: accurately representing a physical probability distribution. This exercise challenges you to use a simple generative model, a normalizing flow, to transform a basic Gaussian distribution into the Boltzmann distribution for a hypothetical harmonic system. Mastering this technique is a key first step in understanding how generative models can efficiently sample states from complex energy landscapes, a common and critical task in statistical mechanics and molecular dynamics. [@problem_id:2398415]", "problem": "Implement a complete, runnable program that constructs a normalizing flow to transform a simple Gaussian base distribution into the Boltzmann distribution for a one-dimensional, two-particle system with harmonic interaction in reduced units where the Boltzmann constant equals one. Consider two particles with positions $x_1$ and $x_2$ in one spatial dimension. The potential energy is given by\n$$\nU(x_1,x_2) = \\frac{1}{2} k \\, (x_1 - x_2)^2 + \\frac{1}{2} k_0 \\, (x_1^2 + x_2^2),\n$$\nwith stiffness constants $k$ and $k_0$. The Boltzmann distribution at temperature $T$ over the state vector $x = (x_1,x_2)^\\top \\in \\mathbb{R}^2$ is\n$$\np(x) = \\frac{1}{Z} \\exp\\!\\left(-\\frac{U(x)}{T}\\right),\n$$\nwhere $Z$ is the partition function and all quantities are in reduced, dimensionless units with the Boltzmann constant equal to $1$. The base distribution is the standard normal distribution\n$$\nq_0(z) = \\mathcal{N}(0, I_2)\n$$\nfor $z \\in \\mathbb{R}^2$. Implement a normalizing flow that is a bijective, differentiable mapping $f_\\theta : \\mathbb{R}^2 \\to \\mathbb{R}^2$ that transforms $z \\sim q_0$ into $x = f_\\theta(z)$ with model density\n$$\nq_\\theta(x) = q_0\\!\\big(f_\\theta^{-1}(x)\\big)\\,\\left|\\det J_{f_\\theta^{-1}}(x)\\right|,\n$$\nwhere $J_{f_\\theta^{-1}}(x)$ is the Jacobian of the inverse mapping. Constrain the flow to be linear and invertible, $x = L z + b$, where $L \\in \\mathbb{R}^{2\\times 2}$ is invertible and $b \\in \\mathbb{R}^2$.\n\nFrom first principles, express the Boltzmann distribution for the specified potential as a multivariate normal distribution with explicit mean and covariance, and choose $L$ and $b$ such that the pushforward $q_\\theta$ equals the Boltzmann distribution exactly. Then, for each test case below, compute the Kullback–Leibler divergence in closed form,\n$$\nD_{\\mathrm{KL}}\\!\\big(q_\\theta \\,\\|\\, p\\big) = \\frac{1}{2}\\left(\\operatorname{tr}\\!\\big(\\Sigma_p^{-1} \\Sigma_\\theta\\big) + (\\mu_p - \\mu_\\theta)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_\\theta) - d + \\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_\\theta}\\right),\n$$\nwhere $d = 2$, and $\\mu_\\theta$, $\\Sigma_\\theta$ are the mean and covariance of $q_\\theta$, while $\\mu_p$, $\\Sigma_p$ are the mean and covariance of $p$. Report one real number per test case, namely the computed $D_{\\mathrm{KL}}$ as a decimal.\n\nTest suite (each tuple is $(k, k_0, T)$):\n- Case $1$: $(1.0, 1.0, 1.0)$\n- Case $2$: $(0.0, 1.0, 0.5)$\n- Case $3$: $(0.5, 5.0, 2.0)$\n- Case $4$: $(5.0, 0.1, 1.5)$\n\nFinal output format: Your program should produce a single line of output containing a comma-separated list of the four $D_{\\mathrm{KL}}$ values enclosed in square brackets. For example, an output with four values should look like\n$$\n[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4],\n$$\nwhere each $\\alpha_i$ is a decimal number. No other output or text is permitted.", "solution": "The problem requires the construction of a linear normalizing flow to transform a standard bivariate Gaussian base distribution into a target Boltzmann distribution for a two-particle system. The validity of the problem statement must first be established.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Potential Energy: $U(x_1,x_2) = \\frac{1}{2} k \\, (x_1 - x_2)^2 + \\frac{1}{2} k_0 \\, (x_1^2 + x_2^2)$\n- State Vector: $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$\n- Boltzmann Distribution (Target): $p(x) = \\frac{1}{Z} \\exp(-\\frac{U(x)}{T})$\n- Base Distribution: $q_0(z) = \\mathcal{N}(0, I_2)$\n- Normalizing Flow Map: $x = f_\\theta(z) = L z + b$, where $L \\in \\mathbb{R}^{2\\times 2}$ is invertible and $b \\in \\mathbb{R}^2$.\n- Model Distribution: $q_\\theta(x) = q_0(f_\\theta^{-1}(x))\\,|\\det J_{f_\\theta^{-1}}(x)|$\n- Constraint: Choose $L$ and $b$ such that $q_\\theta(x) = p(x)$ exactly.\n- Kullback–Leibler Divergence Formula: $D_{\\mathrm{KL}}(q_\\theta \\,\\|\\, p) = \\frac{1}{2}\\left(\\operatorname{tr}(\\Sigma_p^{-1} \\Sigma_\\theta) + (\\mu_p - \\mu_\\theta)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_\\theta) - d + \\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_\\theta}\\right)$, with $d = 2$.\n- Test Cases $(k, k_0, T)$: $(1.0, 1.0, 1.0)$, $(0.0, 1.0, 0.5)$, $(0.5, 5.0, 2.0)$, $(5.0, 0.1, 1.5)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in statistical mechanics (Boltzmann distribution), linear algebra, and probability theory (multivariate normal distributions, change of variables formula for probability densities, Kullback-Leibler divergence). The application of normalizing flows to model physical distributions is a standard technique in computational physics. All provided formulas are correct and standard.\n- **Well-Posed:** The problem is well-posed. It requires demonstrating that the target distribution is a multivariate Gaussian and then finding the parameters of a linear transformation that maps a standard Gaussian to this target. This is a solvable problem with a unique solution for the parameters of the resulting distribution. The request to compute the KL divergence is also well-defined.\n- **Objective:** The problem is stated using precise, objective mathematical language, with no subjective or ambiguous terms.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, and objective. A complete solution will be provided.\n\n**Principle-Based Solution**\n\nThe objective is to construct a linear normalizing flow $x = Lz + b$ that transforms a base distribution $q_0(z) = \\mathcal{N}(z | 0, I_2)$ into a model distribution $q_\\theta(x)$ that is identical to the target Boltzmann distribution $p(x)$. We will first characterize $p(x)$ and $q_\\theta(x)$ as multivariate normal distributions and then match their parameters.\n\n**1. Characterization of the Target Distribution $p(x)$**\n\nThe potential energy $U(x)$ is a quadratic form of the coordinates $x_1$ and $x_2$. We express it in matrix form.\n$$\nU(x_1,x_2) = \\frac{1}{2} k (x_1^2 - 2x_1x_2 + x_2^2) + \\frac{1}{2} k_0 (x_1^2 + x_2^2) = \\frac{1}{2} (k+k_0)x_1^2 - k x_1 x_2 + \\frac{1}{2} (k+k_0)x_2^2\n$$\nThis can be written as $U(x) = \\frac{1}{2} x^\\top H x$, where $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ and $H$ is the Hessian matrix of the potential, given by:\n$$\nH = \\begin{pmatrix} k+k_0 & -k \\\\ -k & k+k_0 \\end{pmatrix}\n$$\nThe target Boltzmann distribution is $p(x) = \\frac{1}{Z} \\exp(-\\frac{U(x)}{T}) = \\frac{1}{Z} \\exp(-\\frac{1}{2T} x^\\top H x)$. This is the probability density function of a multivariate normal distribution $\\mathcal{N}(\\mu_p, \\Sigma_p)$, which is generally given by:\n$$\np(x) = \\frac{1}{\\sqrt{(2\\pi)^d \\det \\Sigma_p}} \\exp\\left(-\\frac{1}{2} (x - \\mu_p)^\\top \\Sigma_p^{-1} (x - \\mu_p)\\right)\n$$\nBy comparing the exponents, we can identify the mean $\\mu_p$ and the inverse covariance (precision) matrix $\\Sigma_p^{-1}$. The absence of a linear term in $x$ in our exponent implies that the mean vector is zero:\n$$\n\\mu_p = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe precision matrix is identified as:\n$$\n\\Sigma_p^{-1} = \\frac{1}{T} H = \\frac{1}{T} \\begin{pmatrix} k+k_0 & -k \\\\ -k & k+k_0 \\end{pmatrix}\n$$\nThe covariance matrix is the inverse of the precision matrix, $\\Sigma_p = ( \\Sigma_p^{-1} )^{-1} = T H^{-1}$. For the given test cases, $H$ is invertible. Thus, the target distribution $p(x)$ is a zero-mean multivariate normal distribution, $p(x) = \\mathcal{N}(x | 0, \\Sigma_p)$.\n\n**2. Characterization of the Model Distribution $q_\\theta(x)$**\n\nThe base random variable $z$ follows a standard normal distribution, $z \\sim \\mathcal{N}(0, I_2)$. The normalizing flow defines a new random variable $x = Lz + b$. This is an affine transformation of a Gaussian variable, so $x$ is also Gaussian.\nThe mean of the model distribution $q_\\theta(x)$ is:\n$$\n\\mu_\\theta = \\mathbb{E}[x] = \\mathbb{E}[Lz + b] = L\\mathbb{E}[z] + b = L \\cdot 0 + b = b\n$$\nThe covariance of the model distribution is:\n$$\n\\Sigma_\\theta = \\text{Cov}(x) = \\text{Cov}(Lz + b) = L \\, \\text{Cov}(z) \\, L^\\top = L I_2 L^\\top = LL^\\top\n$$\nThus, the model distribution is $q_\\theta(x) = \\mathcal{N}(x | b, LL^\\top)$.\n\n**3. Matching the Distributions and Computing KL Divergence**\n\nThe problem demands that we choose the flow parameters $L$ and $b$ such that $q_\\theta(x)$ is identical to $p(x)$. This requires matching their respective means and covariance matrices:\n$$\n\\mu_\\theta = \\mu_p \\implies b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\Sigma_\\theta = \\Sigma_p \\implies LL^\\top = \\Sigma_p\n$$\nSince $\\Sigma_p = T H^{-1}$ is a symmetric positive-definite matrix for the given parameters (as $T>0$ and $H$ is positive-definite), a suitable matrix $L$ can always be found, for example, via Cholesky decomposition.\n\nWith this choice of parameters, the model distribution is identical to the target distribution: $q_\\theta(x) = p(x)$. The Kullback-Leibler divergence measures the dissimilarity between two distributions. For any distribution $P$, the KL divergence of $P$ from itself is zero: $D_{\\mathrm{KL}}(P \\,\\|\\, P) = \\int P(x) \\log\\frac{P(x)}{P(x)} dx = \\int P(x) \\log(1) dx = 0$.\n\nWe can verify this using the provided formula for two multivariate normal distributions $q_\\theta = \\mathcal{N}(\\mu_\\theta, \\Sigma_\\theta)$ and $p = \\mathcal{N}(\\mu_p, \\Sigma_p)$:\n$$\nD_{\\mathrm{KL}}(q_\\theta \\,\\|\\, p) = \\frac{1}{2}\\left(\\operatorname{tr}(\\Sigma_p^{-1} \\Sigma_\\theta) + (\\mu_p - \\mu_\\theta)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_\\theta) - d + \\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_\\theta}\\right)\n$$\nSubstituting our matched parameters $\\mu_\\theta = \\mu_p$ and $\\Sigma_\\theta = \\Sigma_p$:\n- The trace term becomes $\\operatorname{tr}(\\Sigma_p^{-1} \\Sigma_p) = \\operatorname{tr}(I_d) = d$.\n- The mean difference term becomes $(\\mu_p - \\mu_p)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_p) = 0$.\n- The log-determinant term becomes $\\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_p} = \\ln(1) = 0$.\n\nSubstituting these into the formula with $d=2$:\n$$\nD_{\\mathrm{KL}}(q_\\theta \\,\\|\\, p) = \\frac{1}{2}(d + 0 - d + 0) = \\frac{1}{2}(2 - 2) = 0\n$$\nThis result is independent of the specific values of $k$, $k_0$, and $T$, as long as they define a valid (non-degenerate) Gaussian distribution. Since the problem explicitly states to construct the flow such that the distributions match exactly, the KL divergence must be $0$ for all test cases. The implementation will perform the calculation numerically to confirm this analytical conclusion.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the KL divergence for each test case.\n\n    The analytical derivation shows that if the normalizing flow parameters are\n    chosen to perfectly match the target Boltzmann distribution, the KL divergence\n    must be exactly zero. This program confirms this result numerically for\n    the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0, 1.0),  # Case 1: (k, k_0, T)\n        (0.0, 1.0, 0.5),  # Case 2\n        (0.5, 5.0, 2.0),  # Case 3\n        (5.0, 0.1, 1.5),  # Case 4\n    ]\n\n    results = []\n    d = 2  # Dimensionality of the system\n\n    for case in test_cases:\n        k, k0, T = case\n\n        # 1. Define the parameters of the target Boltzmann distribution p(x).\n        # The potential is U(x) = 1/2 * x^T H x.\n        # The distribution p(x) is a multivariate normal N(mu_p, Sigma_p).\n        \n        # The mean is zero due to the form of the potential.\n        mu_p = np.array([0.0, 0.0])\n\n        # The inverse covariance matrix is Sigma_p^-1 = (1/T) * H, where H is the Hessian.\n        H = np.array([[k + k0, -k], \n                      [-k, k + k0]])\n        \n        # Check for non-invertible Hessian (though not expected for test cases)\n        if np.linalg.det(H) == 0:\n            # This would indicate a non-physical or degenerate system.\n            # For the given cases, det(H) = k0 * (2k + k0), which is > 0.\n            # Handle as an error, though we expect it not to occur.\n            results.append(np.nan) \n            continue\n\n        Sigma_p_inv = (1.0 / T) * H\n        \n        # The covariance matrix is the inverse of the precision matrix.\n        Sigma_p = np.linalg.inv(Sigma_p_inv)\n\n        # 2. Define the parameters of the model distribution q_theta(x).\n        # The problem states to choose the linear flow x = Lz + b such that\n        # the model distribution q_theta(x) exactly equals the target p(x).\n        # This means we must match their means and covariances.\n        mu_theta = mu_p\n        Sigma_theta = Sigma_p\n\n        # 3. Compute the KL divergence using the provided formula.\n        # D_KL(q_theta || p) = 0.5 * ( tr(Sigma_p^-1 * Sigma_theta) + \n        #                              (mu_p - mu_theta)^T * Sigma_p^-1 * (mu_p - mu_theta) -\n        #                              d + log(det(Sigma_p) / det(Sigma_theta)) )\n\n        # Since mu_theta = mu_p and Sigma_theta = Sigma_p, the terms simplify:\n        # trace_term = tr(Sigma_p^-1 * Sigma_p) = tr(I) = d\n        # mean_term = (0)^T * ... * (0) = 0\n        # log_det_term = log(det(Sigma_p) / det(Sigma_p)) = log(1) = 0\n        # D_KL = 0.5 * (d + 0 - d + 0) = 0\n        \n        # We perform the full numerical computation to verify.\n        \n        trace_term = np.trace(Sigma_p_inv @ Sigma_theta)\n        \n        mu_diff = mu_p - mu_theta\n        mean_term = mu_diff.T @ Sigma_p_inv @ mu_diff\n        \n        det_p = np.linalg.det(Sigma_p)\n        det_theta = np.linalg.det(Sigma_theta)\n        \n        # Handle potential division by zero or log of non-positive, though not expected here.\n        if det_theta <= 0 or det_p <= 0:\n            log_det_term = np.nan\n        else:\n            log_det_term = np.log(det_p / det_theta)\n        \n        dkl = 0.5 * (trace_term + mean_term - d + log_det_term)\n        \n        # The result should be numerically very close to 0.0.\n        results.append(dkl)\n\n    # Final print statement in the exact required format.\n    # We format the numbers to represent them as standard decimals.\n    print(f\"[{','.join(map(lambda x: f'{x:.1f}', results))}]\")\n\nsolve()\n\n```", "id": "2398415"}, {"introduction": "Can a purely data-driven model learn the fundamental symmetries of a physical system without being explicitly taught them? This practice explores that very question by having you train a simple linear model on snapshots from an $N$-body gravitational simulation. Your task is to investigate if this model, learning only from observed positions and velocities, can preserve the total angular momentum vector $\\vec{L}$ during its own rollouts. This exercise powerfully illustrates both the capabilities and the potential pitfalls of \"black-box\" dynamical models in physics. [@problem_id:2398389]", "problem": "You are given the task of assessing whether a data-driven generative model, trained only on snapshots of Newtonian gravitational $N$-body dynamics, can learn to preserve the conservation of the total angular momentum vector $\\vec{L}$. You must write a complete program that (i) generates physically realistic $N$-body training data using a symplectic integrator, (ii) learns a linear time-advance generative model from snapshots, (iii) rolls out the learned model on held-out initial conditions, and (iv) tests conservation of $\\vec{L}$ in the generated rollouts against a specified tolerance.\n\nUse the following fundamental base. Newton’s second law states that for particle $i$ of mass $m_i$, the equation of motion is $m_i \\,\\mathrm{d}^2\\vec{r}_i/\\mathrm{d}t^2=\\vec{F}_i$, where $\\vec{F}_i$ is the net force. For Newtonian gravity with gravitational constant $G$, the force on particle $i$ due to particle $j$ is $\\vec{F}_{ij}=-G\\,m_i m_j\\,(\\vec{r}_i-\\vec{r}_j)/\\lVert \\vec{r}_i-\\vec{r}_j\\rVert^3$. By Newton’s third law, $\\vec{F}_{ij}=-\\vec{F}_{ji}$, and because these forces are central, the total internal torque is zero, which implies $\\mathrm{d}\\vec{L}/\\mathrm{d}t=\\vec{0}$ in the absence of external torques. The total angular momentum is defined by $\\vec{L}=\\sum_{i=1}^{N} m_i\\,\\vec{r}_i\\times\\vec{v}_i$, where $\\vec{v}_i=\\mathrm{d}\\vec{r}_i/\\mathrm{d}t$.\n\nThe program you write must implement the following tasks.\n\n- Data generation: For each test case, simulate the true gravitational $N$-body system using the velocity-Verlet method (a symplectic integrator) with gravitational softening to avoid singularities. Use gravitational constant $G=1$ in nondimensional units. Given masses $\\{m_i\\}_{i=1}^N$, positions $\\{\\vec{r}_i\\}_{i=1}^N$, and velocities $\\{\\vec{v}_i\\}_{i=1}^N$ at time $t$, compute gravitational accelerations $\\{\\vec{a}_i\\}_{i=1}^N$ with softened pairwise interactions using a softening length squared $\\epsilon^2>0$. Advance positions and velocities by a time step $\\Delta t$ using velocity-Verlet. Construct a snapshot vector $\\vec{s}_t$ by stacking all positions and velocities into a single vector in a fixed order at each time step $t$.\n\n- Generative model training: Learn a linear time-advance model $\\mathbf{A}$ from snapshot pairs by minimizing the mean squared error $\\sum_t \\lVert \\vec{s}_{t+1}-\\mathbf{A}\\vec{s}_t\\rVert^2$ over the training set. Do not introduce any external physics constraints into the model; it must learn only from the data.\n\n- Rollout generation: Starting from the initial snapshot $\\vec{s}_0$ of each test case, generate a rollout of length $T_{\\mathrm{eval}}$ using the learned model via $\\vec{s}_{t+1}=\\mathbf{A}\\vec{s}_t$. At each generated step, compute the total angular momentum $\\vec{L}(t)$ of the generated state.\n\n- Conservation test: Define the initial angular momentum magnitude $L_0=\\lVert \\vec{L}(0)\\rVert_2$. For a given tolerance pair $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})$, declare that angular momentum is preserved in the generated rollout if either $L_0>\\tau_0$ and $\\max_{0\\le t\\le T_{\\mathrm{eval}}}\\lVert \\vec{L}(t)-\\vec{L}(0)\\rVert_2/L_0\\le\\varepsilon_{\\mathrm{rel}}$, or $L_0\\le\\tau_0$ and $\\max_{0\\le t\\le T_{\\mathrm{eval}}}\\lVert \\vec{L}(t)\\rVert_2\\le\\varepsilon_{\\mathrm{abs}}$, where $\\tau_0$ is a small threshold. All comparisons are dimensionless because they use relative or absolute norms of $\\vec{L}$.\n\nImplement the above and evaluate the boolean result for each of the following test cases (the “test suite”). In all cases, use $G=1$ and the velocity-Verlet integrator with softening. The initial conditions and parameters are as follows; all vectors are three-dimensional with the third component $z$ initialized to $0$.\n\n- Case $1$ (two-body circular orbit, equal masses):\n  - $N=2$, masses $(m_1,m_2)=(1,1)$.\n  - Initial positions $(\\vec{r}_1,\\vec{r}_2)=((-1/2,0,0),(+1/2,0,0))$.\n  - For a circular orbit with separation $a=1$ and total mass $M=2$, the angular frequency satisfies $\\omega^2=GM/a^3=2$. Use initial velocities $(\\vec{v}_1,\\vec{v}_2)=((0,+\\tfrac{1}{2}\\sqrt{2},0),(0,-\\tfrac{1}{2}\\sqrt{2},0))$.\n  - Time step $\\Delta t=10^{-2}$, softening length squared $\\epsilon^2=10^{-6}$.\n  - Training steps $T_{\\mathrm{train}}=400$, evaluation steps $T_{\\mathrm{eval}}=200$.\n  - Tolerances $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.15,10^{-3})$, with $\\tau_0=10^{-6}$.\n\n- Case $2$ (two-body head-on approach with zero angular momentum):\n  - $N=2$, masses $(m_1,m_2)=(1,1)$.\n  - Initial positions $(\\vec{r}_1,\\vec{r}_2)=((0,-1/2,0),(0,+1/2,0))$.\n  - Initial velocities along the line of centers and opposite: $(\\vec{v}_1,\\vec{v}_2)=((0,+0.5,0),(0,-0.5,0))$.\n  - $\\Delta t=10^{-2}$, $\\epsilon^2=10^{-4}$.\n  - $T_{\\mathrm{train}}=400$, $T_{\\mathrm{eval}}=200$.\n  - $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.15,5\\times 10^{-3})$, with $\\tau_0=10^{-6}$.\n\n- Case $3$ (three-body equal masses in an equilateral rotating configuration):\n  - $N=3$, masses $(1,1,1)$.\n  - Place particles at the vertices of an equilateral triangle of circumradius $R=1$: $\\vec{r}_1=(1,0,0)$, $\\vec{r}_2=(-1/2,\\sqrt{3}/2,0)$, $\\vec{r}_3=(-1/2,-\\sqrt{3}/2,0)$.\n  - For uniform rotation with angular speed $\\omega$ satisfying $\\omega^2=1/(\\sqrt{3} R^3)$, initialize tangential velocities $\\vec{v}_i=\\omega\\,\\hat{\\boldsymbol{t}}_i\\,R$, with $\\hat{\\boldsymbol{t}}_i$ perpendicular to $\\vec{r}_i$ in the counterclockwise sense. Numerically, use $\\omega=\\sqrt{1/\\sqrt{3}}$ and $R=1$.\n  - $\\Delta t=10^{-2}$, $\\epsilon^2=10^{-6}$.\n  - $T_{\\mathrm{train}}=600$, $T_{\\mathrm{eval}}=300$.\n  - $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.20,10^{-3})$, with $\\tau_0=10^{-6}$.\n\n- Case $4$ (three-body, unequal masses, noncircular, near-encounter):\n  - $N=3$, masses $(1.5,1.0,0.1)$.\n  - Initial positions $\\vec{r}_1=(-0.7,0,0)$, $\\vec{r}_2=(+0.7,0,0)$, $\\vec{r}_3=(0,0.2,0)$.\n  - Initial velocities chosen approximately bound and adjusted to zero total linear momentum: start with $\\vec{v}_1=(0,0.4,0)$, $\\vec{v}_2=(0,-0.6,0)$, $\\vec{v}_3=(0.2,0,0)$, then subtract the mass-weighted average velocity from each to enforce $\\sum_i m_i \\vec{v}_i=\\vec{0}$.\n  - $\\Delta t=5\\times 10^{-3}$, $\\epsilon^2=10^{-4}$.\n  - $T_{\\mathrm{train}}=800$, $T_{\\mathrm{eval}}=300$.\n  - $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.15,10^{-3})$, with $\\tau_0=10^{-6}$.\n\nYour program must, for each case, train the linear generator on the training snapshots, generate the evaluation rollout from the initial snapshot using the learned generator, compute the angular momentum deviations as defined above, and produce a boolean indicating whether conservation is satisfied under the learned model for that case. All computations are in nondimensional units; the final boolean outputs are dimensionless.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\") corresponding to Cases $1$ through $4$ in order.", "solution": "The user has provided a problem that is scientifically grounded, well-posed, and objective. It outlines a computational experiment to evaluate whether a data-driven linear generative model can learn to preserve the physical conservation of angular momentum in an $N$-body gravitational system. The problem supplies all necessary physical laws, numerical methods, model specifications, evaluation criteria, and concrete test cases. It is a valid problem in the field of computational physics and scientific machine learning. Therefore, I will proceed with a full solution.\n\nThe fundamental task is to assess a linear model's ability to respect a nonlinear conservation law. The true dynamics of an $N$-body system are governed by a set of coupled, nonlinear ordinary differential equations. The time-evolution operator, or flow map $\\Phi_{\\Delta t}$, that advances the state of the system $\\vec{s}(t)$ to $\\vec{s}(t+\\Delta t)$ is a complex nonlinear function. The problem proposes to approximate this map with a single linear operator $\\mathbf{A}$, such that $\\vec{s}_{t+1} \\approx \\mathbf{A}\\vec{s}_t$. This matrix $\\mathbf{A}$ is learned from data by minimizing the least-squares error, with no explicit physical constraints imposed. The central question is whether the symmetries of the true dynamics that lead to conservation laws are implicitly captured by this linear approximation.\n\nThe total angular momentum of an isolated $N$-body system, $\\vec{L} = \\sum_{i=1}^{N} m_i \\vec{r}_i \\times \\vec{v}_i$, is a conserved quantity. This conservation arises from the rotational symmetry of the Hamiltonian. A linear model $\\mathbf{A}$ will preserve $\\vec{L}$ if and only if the structure of $\\mathbf{A}$ respects this symmetry for all possible states. For a general time-varying state, this is highly unlikely. The matrix $\\mathbf{A}$ is not guaranteed to be symplectic, which would be a necessary condition to preserve the geometric structure of Hamiltonian dynamics. However, for a sufficiently small time step $\\Delta t$ and for trajectories that are regular (e.g., periodic or quasi-periodic), the nonlinear flow map $\\Phi_{\\Delta t}$ may be well-approximated by a linear map in the local region of phase space occupied by the trajectory. In such cases, the learned model might exhibit approximate conservation. For chaotic trajectories, this approximation is expected to fail rapidly.\n\nThe solution is implemented through a series of computational steps for each test case.\n\n1.  **Data Generation**:\n    The state of the system at any time $t$ is represented by a snapshot vector $\\vec{s}_t \\in \\mathbb{R}^{6N}$, which is a flattened concatenation of the position vectors $\\{\\vec{r}_i \\in \\mathbb{R}^3\\}_{i=1}^N$ and velocity vectors $\\{\\vec{v}_i \\in \\mathbb{R}^3\\}_{i=1}^N$. We generate a time series of these snapshots, $\\vec{s}_0, \\vec{s}_1, \\ldots, \\vec{s}_{T_{\\mathrm{train}}}$, by numerically integrating the equations of motion using the velocity-Verlet method. This is a second-order symplectic integrator known for its good long-term stability and conservation properties in Hamiltonian systems. The gravitational acceleration on particle $i$ is computed with a softening term $\\epsilon^2$ to prevent singularities during close encounters:\n    $$ \\vec{a}_i = \\sum_{j \\neq i} \\frac{G m_j (\\vec{r}_j - \\vec{r}_i)}{(\\lVert \\vec{r}_i - \\vec{r}_j \\rVert_2^2 + \\epsilon^2)^{3/2}} $$\n    The velocity-Verlet algorithm advances the state over a time step $\\Delta t$ as follows:\n    \\begin{enumerate}\n        \\item $\\vec{v}_i(t + \\frac{1}{2}\\Delta t) = \\vec{v}_i(t) + \\frac{1}{2}\\vec{a}_i(t)\\Delta t$\n        \\item $\\vec{r}_i(t + \\Delta t) = \\vec{r}_i(t) + \\vec{v}_i(t + \\frac{1}{2}\\Delta t)\\Delta t$\n        \\item Compute $\\vec{a}_i(t + \\Delta t)$ using the new positions $\\vec{r}_i(t + \\Delta t)$.\n        \\item $\\vec{v}_i(t + \\Delta t) = \\vec{v}_i(t + \\frac{1}{2}\\Delta t) + \\frac{1}{2}\\vec{a}_i(t + \\Delta t)\\Delta t$\n    \\end{enumerate}\n    This process is repeated for $T_{\\mathrm{train}}$ steps to create the training dataset.\n\n2.  **Generative Model Training**:\n    The purely data-driven model is a linear map $\\mathbf{A}$ of size $6N \\times 6N$. We seek the matrix $\\mathbf{A}$ that best approximates the relationship $\\vec{s}_{t+1} = \\mathbf{A}\\vec{s}_t$. We form two matrices from the training data: $S_{\\mathrm{in}}$, whose rows are the snapshots $\\vec{s}_0, \\ldots, \\vec{s}_{T_{\\mathrm{train}}-1}$, and $S_{\\mathrm{out}}$, whose rows are $\\vec{s}_1, \\ldots, \\vec{s}_{T_{\\mathrm{train}}}$. We wish to find a matrix $\\mathbf{M}$ such that $S_{\\mathrm{out}} \\approx S_{\\mathrm{in}}\\mathbf{M}$. This is a standard linear least-squares problem, solved for $\\mathbf{M}$. If we define state vectors as row vectors, then this matrix $\\mathbf{M}$ is the transpose of the desired operator $\\mathbf{A}$. The solution is found efficiently using numerical linear algebra routines, such as those provided by `numpy.linalg.lstsq`. The learned matrix, which we will call $\\mathbf{A}_{\\text{op}}$, defines the update rule $\\vec{s}_{t+1}^T = \\vec{s}_t^T \\mathbf{A}_{\\text{op}}$.\n\n3.  **Rollout and Conservation Test**:\n    Using the learned operator $\\mathbf{A}_{\\text{op}}$, we generate a new sequence of states (a \"rollout\") starting from the initial condition $\\vec{s}_0$ of the test case. The sequence is generated recursively for $T_{\\mathrm{eval}}$ steps:\n    $$ \\vec{s}_{t+1}^{\\text{gen},T} = \\vec{s}_{t}^{\\text{gen},T} \\mathbf{A}_{\\text{op}} $$\n    For each generated snapshot $\\vec{s}_t^{\\text{gen}}$, we reconstruct the positions $\\{\\vec{r}_i(t)\\}$ and velocities $\\{\\vec{v}_i(t)\\}$ and compute the total angular momentum vector $\\vec{L}(t) = \\sum_{i=1}^{N} m_i (\\vec{r}_i(t) \\times \\vec{v}_i(t))$. We then compare the evolution of $\\vec{L}(t)$ to its initial value $\\vec{L}(0)$.\n    The conservation test is performed according to the specified criteria. Let $L_0 = \\lVert \\vec{L}(0) \\rVert_2$.\n    \\begin{itemize}\n        \\item If $L_0 > \\tau_0$, we calculate the maximum relative deviation: $\\max_{0 \\le t \\le T_{\\mathrm{eval}}} \\frac{\\lVert \\vec{L}(t) - \\vec{L}(0) \\rVert_2}{L_0}$. The test passes if this value is less than or equal to the relative tolerance $\\varepsilon_{\\mathrm{rel}}$.\n        \\item If $L_0 \\le \\tau_0$, we calculate the maximum absolute magnitude: $\\max_{0 \\le t \\le T_{\\mathrm{eval}}} \\lVert \\vec{L}(t) \\rVert_2$. The test passes if this value is less than or equal to the absolute tolerance $\\varepsilon_{\\mathrm{abs}}$.\n    \\end{itemize}\n    This procedure is carried out for each of the four specified test cases, and a boolean result is recorded for each.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    # Gravitational constant G=1 everywhere.\n    G = 1.0\n    \n    # Threshold for L0 magnitude in conservation test\n    tau0 = 1e-6\n\n    # Test cases defined as a list of dictionaries.\n    test_cases = [\n        {\n            \"name\": \"Case 1: Two-body circular orbit\",\n            \"N\": 2,\n            \"m\": np.array([1.0, 1.0]),\n            \"r0\": np.array([[-0.5, 0.0, 0.0], [0.5, 0.0, 0.0]]),\n            \"v0\": np.array([[0.0, 0.5 * np.sqrt(2), 0.0], [0.0, -0.5 * np.sqrt(2), 0.0]]),\n            \"dt\": 1e-2,\n            \"eps2\": 1e-6,\n            \"T_train\": 400,\n            \"T_eval\": 200,\n            \"eps_rel\": 0.15,\n            \"eps_abs\": 1e-3,\n        },\n        {\n            \"name\": \"Case 2: Two-body head-on\",\n            \"N\": 2,\n            \"m\": np.array([1.0, 1.0]),\n            \"r0\": np.array([[0.0, -0.5, 0.0], [0.0, 0.5, 0.0]]),\n            \"v0\": np.array([[0.0, 0.5, 0.0], [0.0, -0.5, 0.0]]),\n            \"dt\": 1e-2,\n            \"eps2\": 1e-4,\n            \"T_train\": 400,\n            \"T_eval\": 200,\n            \"eps_rel\": 0.15,\n            \"eps_abs\": 5e-3,\n        },\n        {\n            \"name\": \"Case 3: Three-body equilateral\",\n            \"N\": 3,\n            \"m\": np.array([1.0, 1.0, 1.0]),\n            \"r0\": np.array([\n                [1.0, 0.0, 0.0],\n                [-0.5, np.sqrt(3)/2.0, 0.0],\n                [-0.5, -np.sqrt(3)/2.0, 0.0]\n            ]),\n            \"v0\": None, # Will be computed\n            \"dt\": 1e-2,\n            \"eps2\": 1e-6,\n            \"T_train\": 600,\n            \"T_eval\": 300,\n            \"eps_rel\": 0.20,\n            \"eps_abs\": 1e-3,\n        },\n        {\n            \"name\": \"Case 4: Three-body near-encounter\",\n            \"N\": 3,\n            \"m\": np.array([1.5, 1.0, 0.1]),\n            \"r0\": np.array([[-0.7, 0.0, 0.0], [0.7, 0.0, 0.0], [0.0, 0.2, 0.0]]),\n            \"v0\": None, # Will be computed\n            \"dt\": 5e-3,\n            \"eps2\": 1e-4,\n            \"T_train\": 800,\n            \"T_eval\": 300,\n            \"eps_rel\": 0.15,\n            \"eps_abs\": 1e-3,\n        },\n    ]\n\n    # Special initial condition calculations\n    # Case 3 ICs\n    R_c3 = 1.0\n    omega_c3_from_prob = np.sqrt(1.0 / np.sqrt(3.0)) # For R=1\n    r0_c3 = test_cases[2]['r0']\n    v0_c3 = np.zeros_like(r0_c3)\n    for i in range(3):\n        tangent_vec = np.cross([0, 0, 1], r0_c3[i, :])\n        v0_c3[i, :] = omega_c3_from_prob * R_c3 * (tangent_vec / np.linalg.norm(tangent_vec))\n    test_cases[2]['v0'] = v0_c3\n\n    # Case 4 ICs\n    m_c4 = test_cases[3]['m']\n    v0_prime_c4 = np.array([[0.0, 0.4, 0.0], [0.0, -0.6, 0.0], [0.2, 0.0, 0.0]])\n    total_momentum = (m_c4[:, np.newaxis] * v0_prime_c4).sum(axis=0)\n    total_mass = m_c4.sum()\n    v_com = total_momentum / total_mass\n    test_cases[3]['v0'] = v0_prime_c4 - v_com\n\n    results = []\n    for case in test_cases:\n        result = run_case(G=G, tau0=tau0, **case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_accel(r, m, G, eps2):\n    \"\"\"Calculates gravitational acceleration for all particles.\"\"\"\n    N = r.shape[0]\n    accel = np.zeros_like(r)\n    for i in range(N):\n        for j in range(N):\n            if i == j:\n                continue\n            r_ij = r[j] - r[i]\n            r_ij_norm_sq = np.sum(r_ij**2)\n            denominator = (r_ij_norm_sq + eps2)**1.5\n            accel[i] += G * m[j] * r_ij / denominator\n    return accel\n\ndef verlet_step(r, v, m, G, dt, eps2):\n    \"\"\"Performs a single Velocity-Verlet integration step.\"\"\"\n    a_t = get_accel(r, m, G, eps2)\n    v_half = v + 0.5 * dt * a_t\n    r_new = r + dt * v_half\n    a_new = get_accel(r_new, m, G, eps2)\n    v_new = v_half + 0.5 * dt * a_new\n    return r_new, v_new\n\ndef get_angular_momentum(r, v, m):\n    \"\"\"Calculates the total angular momentum of the system.\"\"\"\n    L_per_particle = np.cross(r, v) * m[:, np.newaxis]\n    return np.sum(L_per_particle, axis=0)\n\ndef run_case(name, N, m, r0, v0, dt, eps2, T_train, T_eval, eps_rel, eps_abs, G, tau0):\n    \"\"\"Runs a single test case from simulation to evaluation.\"\"\"\n    \n    # 1. Data Generation\n    dim = 3 * N\n    snapshots = np.zeros((T_train + 1, 2 * dim))\n    r, v = np.copy(r0), np.copy(v0)\n    \n    snapshots[0] = np.concatenate((r.flatten(), v.flatten()))\n    for t_idx in range(T_train):\n        r, v = verlet_step(r, v, m, G, dt, eps2)\n        snapshots[t_idx + 1] = np.concatenate((r.flatten(), v.flatten()))\n\n    # 2. Generative Model Training\n    S_in = snapshots[:-1]\n    S_out = snapshots[1:]\n    \n    # Solve S_in @ A_op = S_out for A_op\n    A_op = np.linalg.lstsq(S_in, S_out, rcond=None)[0]\n\n    # 3. Rollout Generation\n    rollout_snapshots = np.zeros((T_eval + 1, 2 * dim))\n    current_s = snapshots[0]\n    rollout_snapshots[0] = current_s\n\n    for t_idx in range(T_eval):\n        # Update using s_new = s_old @ A_op\n        current_s = current_s @ A_op\n        rollout_snapshots[t_idx + 1] = current_s\n        \n    # 4. Conservation Test\n    r_init = rollout_snapshots[0, :dim].reshape((N, 3))\n    v_init = rollout_snapshots[0, dim:].reshape((N, 3))\n    L0_vec = get_angular_momentum(r_init, v_init, m)\n    L0_mag = np.linalg.norm(L0_vec)\n\n    deviations = []\n    L_mags = []\n\n    for t_idx in range(T_eval + 1):\n        s_t = rollout_snapshots[t_idx]\n        r_t = s_t[:dim].reshape((N, 3))\n        v_t = s_t[dim:].reshape((N, 3))\n        L_t_vec = get_angular_momentum(r_t, v_t, m)\n        \n        if L0_mag > tau0:\n            dev = np.linalg.norm(L_t_vec - L0_vec)\n            deviations.append(dev)\n        else: # L0 is effectively zero\n            mag = np.linalg.norm(L_t_vec)\n            L_mags.append(mag)\n            \n    is_conserved = False\n    if L0_mag > tau0:\n        max_rel_dev = np.max(deviations) / L0_mag\n        if max_rel_dev <= eps_rel:\n            is_conserved = True\n    else:\n        # Handle the case where L_mags is empty (T_eval=0), though not in these tests\n        if not L_mags: L_mags.append(0)\n        max_abs_mag = np.max(L_mags)\n        if max_abs_mag <= eps_abs:\n            is_conserved = True\n            \n    return is_conserved\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2398389"}, {"introduction": "Building on the previous practices, we now advance to a more sophisticated, non-linear generative model to tackle a classic problem in physics. In this exercise, you will construct a Conditional Variational Autoencoder (cVAE) to model the outcomes of a classical scattering experiment. You will train the model to generate the probable final position of a scattered particle, conditioned on its initial energy $E$ and impact parameter $b$. This demonstrates how modern generative models can learn the complex, conditional, and often stochastic relationships between the initial conditions and final outcomes of a physical process. [@problem_id:2398395]", "problem": "You will build, train, and evaluate a conditional generative model for two-dimensional classical scattering of a charged particle by a fixed repulsive Coulomb center. The goal is to use a Conditional Variational Autoencoder (cVAE) to model the conditional distribution of final detector-plane positions given initial beam energy and impact parameter. All computations must be in a consistent set of physical units, and the final program must output a single, machine-checkable line aggregating the results for a fixed test suite.\n\nTask overview:\n- Physics: A classical point particle of mass $m$ and charge $q$ approaches from $x=-\\infty$ along the $+x$ direction with initial kinetic energy $E$ and an impact parameter $b$ in the $y$ direction. It scatters from a fixed target with charge $Q$ located at the origin under a repulsive inverse-square Coulomb force. A circular detector of radius $R$ centered at the origin records the asymptotic outgoing direction as a position on the circle.\n- Data generation: Synthesize training data by sampling pairs $(E,b)$ and mapping them to noisily observed final positions $(x_{\\text{det}},y_{\\text{det}})$ on the detector. You must base this mapping on fundamental mechanics for inverse-square repulsive scattering that follow from Newton’s Second Law, conservation of energy, and conservation of angular momentum. Include a small, physically plausible additive Gaussian measurement noise on $(x_{\\text{det}},y_{\\text{det}})$ to represent detector uncertainty.\n- Model: Train a Conditional Variational Autoencoder (cVAE) to learn $p_\\theta(\\mathbf{x}\\mid \\mathbf{c})$ where $\\mathbf{x}=\\left[x_{\\text{det}},y_{\\text{det}}\\right]$ and $\\mathbf{c}=\\left[E,b\\right]$, using the reparameterization trick and an evidence lower bound objective. Use a low-dimensional latent variable $\\mathbf{z}$, with a standard normal prior $p(\\mathbf{z})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$. Condition both the encoder and decoder on $\\mathbf{c}$. The reconstruction term must be a Gaussian negative log-likelihood or a mean-squared error surrogate, and the Kullback–Leibler divergence must regularize the approximate posterior to the standard normal prior.\n- Generation: After training, condition on test $(E,b)$ and generate the mean final position by decoding with $\\mathbf{z}=\\mathbf{0}$, which corresponds to the prior mean and yields a deterministic prediction for evaluation.\n- Evaluation: For each specified test case, compute the Euclidean distance (in meters) between the generated position and the physically correct ground-truth position on the detector circle.\n\nFundamental base you must use:\n- Newton’s Second Law in central fields, conservation of angular momentum, and conservation of mechanical energy for a Coulomb central potential $U(r)=\\frac{\\kappa qQ}{r}$ with $\\kappa=\\frac{1}{4\\pi\\varepsilon_0}$.\n- Classical hyperbolic scattering kinematics implied by the above, without using any shortcut scattering formulas stated without derivation.\n\nPhysical and numerical setup:\n- Mass $m = 1.67262192369\\times 10^{-27}\\,\\mathrm{kg}$.\n- Elementary charge magnitude $e = 1.602176634\\times 10^{-19}\\,\\mathrm{C}$.\n- Fixed charges $q=+e$, $Q=+e$ (repulsive).\n- Coulomb constant $\\kappa = 8.9875517923\\times 10^{9}\\,\\mathrm{N\\,m^2/C^2}$, so $\\alpha \\equiv \\kappa qQ$ has units $\\mathrm{J\\,m}$ when combined with $\\mathrm{C^2}$.\n- Detector radius $R = 1.0\\times 10^{-9}\\,\\mathrm{m}$.\n- Energies to be expressed in joules in all computations; if you internally generate energies in kilo-electron-volts, ensure you convert to joules using $1\\,\\mathrm{eV}=1.602176634\\times 10^{-19}\\,\\mathrm{J}$ and $1\\,\\mathrm{keV}=10^{3}\\,\\mathrm{eV}$.\n- Impact parameter $b$ in meters.\n- Additive isotropic Gaussian detector noise $\\mathcal{N}(\\mathbf{0},\\sigma_{\\text{det}}^2\\mathbf{I})$ on $(x_{\\text{det}},y_{\\text{det}})$ with $\\sigma_{\\text{det}}=1.0\\times 10^{-11}\\,\\mathrm{m}$.\n\nProgram requirements:\n- Synthesize a training set by sampling $(E,b)$ pairs over a physically reasonable range (e.g., $E$ in the kiloelectron-volt scale converted to joules, and $|b|$ up to picometers) and mapping them to noisy detector positions $(x_{\\text{det}},y_{\\text{det}})$ via the physically derived scattering relation. Normalize inputs and outputs to stabilize training, but report evaluation in physical units.\n- Implement a Conditional Variational Autoencoder (cVAE) with:\n  - A standard normal prior on the latent variable $\\mathbf{z}$.\n  - An encoder $q_\\phi(\\mathbf{z}\\mid \\mathbf{x},\\mathbf{c})$ and decoder $p_\\theta(\\mathbf{x}\\mid \\mathbf{z},\\mathbf{c})$ that are differentiable functions implemented with basic fully connected layers. Use the reparameterization trick $\\mathbf{z}=\\boldsymbol{\\mu}_z+\\boldsymbol{\\sigma}_z\\odot \\boldsymbol{\\epsilon}$ with $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$.\n  - A training objective comprising a reconstruction term and a Kullback–Leibler term between $q_\\phi(\\mathbf{z}\\mid \\mathbf{x},\\mathbf{c})$ and the prior.\n- Train until convergence sufficient to give small Euclidean errors on the test suite. Keep model sizes and dataset sizes modest so that the program runs quickly.\n- For generation on the test suite, decode with $\\mathbf{z}=\\mathbf{0}$ to obtain a single deterministic predicted position.\n\nTest suite:\nUse the following five test cases with energies in kiloelectron-volts (to be converted to joules in the program) and impact parameters in meters. For each pair, compute the ground-truth final position on the detector using the physically derived relation and compare with the cVAE prediction as specified above. The final outputs to be printed are the Euclidean errors in meters as floats.\n\n- Case $1$: $E = 1.0\\,\\mathrm{keV}$, $b = 0.0\\,\\mathrm{m}$.\n- Case $2$: $E = 2.0\\,\\mathrm{keV}$, $b = 1.0\\times 10^{-12}\\,\\mathrm{m}$.\n- Case $3$: $E = 0.8\\,\\mathrm{keV}$, $b = 2.0\\times 10^{-12}\\,\\mathrm{m}$.\n- Case $4$: $E = 3.0\\,\\mathrm{keV}$, $b = -1.5\\times 10^{-12}\\,\\mathrm{m}$.\n- Case $5$: $E = 1.5\\,\\mathrm{keV}$, $b = 2.0\\times 10^{-12}\\,\\mathrm{m}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5]\"), where each $r_i$ is the Euclidean error in meters (a float) for Case $i$, in the order given above.\n\nAngle units:\n- Any internal angular computations must use radians.\n\nPhysical units:\n- All final distances must be reported in meters, and energies must be in joules for the physics calculations, with any internally specified kiloelectron-volts correctly converted to joules.\n\nConstraints:\n- The final answer must be a complete runnable program that performs all steps: data synthesis, cVAE training, conditional generation, and evaluation on the test suite, followed by printing the single required output line. The program must not require any user input.", "solution": "The problem statement is subjected to a rigorous validation process before any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Physical System**: A point particle of mass $m=1.67262192369\\times 10^{-27}\\,\\mathrm{kg}$ and charge $q=+e$ scatters from a fixed charge $Q=+e$ at the origin. The elementary charge is $e = 1.602176634\\times 10^{-19}\\,\\mathrm{C}$.\n- **Interaction**: Repulsive Coulomb potential $U(r) = \\frac{\\alpha}{r}$, where $\\alpha = \\kappa qQ$ and $\\kappa = 8.9875517923\\times 10^{9}\\,\\mathrm{N\\,m^2/C^2}$.\n- **Initial Conditions**: The particle has initial kinetic energy $E$ and impact parameter $b$.\n- **Detector**: A circular detector of radius $R = 1.0\\times 10^{-9}\\,\\mathrm{m}$ centered at the origin records the final position $(x_{\\text{det}}, y_{\\text{det}})$.\n- **Measurement Noise**: Additive isotropic Gaussian noise $\\mathcal{N}(\\mathbf{0}, \\sigma_{\\text{det}}^2\\mathbf{I})$ with $\\sigma_{\\text{det}} = 1.0\\times 10^{-11}\\,\\mathrm{m}$ is added to the detector position for training data.\n- **Model**: A Conditional Variational Autoencoder (cVAE) is to be trained to learn the conditional probability distribution $p_{\\theta}(\\mathbf{x}|\\mathbf{c})$, where the data is $\\mathbf{x} = [x_{\\text{det}}, y_{\\text{det}}]$ and the condition is $\\mathbf{c} = [E, b]$.\n- **cVAE Specification**: The model uses a standard normal prior $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ on the latent variable $\\mathbf{z}$. The training objective is the Evidence Lower Bound (ELBO) using the reparameterization trick.\n- **Generation**: A prediction for a given condition $\\mathbf{c}$ is generated by decoding the mean of the prior, i.e., using $\\mathbf{z}=\\mathbf{0}$.\n- **Evaluation**: The performance is measured by the Euclidean distance between the model's prediction and the noiseless ground-truth position on the detector.\n- **Energy Conversion**: $1\\,\\mathrm{keV} = 10^3\\,\\mathrm{eV} = 1.602176634\\times 10^{-16}\\,\\mathrm{J}$.\n- **Test Suite**: Five pairs of $(E, b)$ are provided for final evaluation.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is critically assessed against the validation criteria.\n\n- **Scientifically Grounded**: The problem is founded on the principles of classical mechanics and electromagnetism, specifically the well-understood phenomenon of Rutherford (or Coulomb) scattering. All physical constants provided are accurate, and the scenario is a standard textbook example. This criterion is met.\n- **Well-Posed**: For any non-pathological initial conditions $(E,b)$ with $E>0$, there exists a unique, stable scattering trajectory. The relationship between initial conditions and final scattering angle is a continuous function. The task of learning this function with a generative model, including the effects of noise, is a well-posed problem in machine learning. This criterion is met.\n- **Objective**: The problem is specified with quantitative precision. All parameters, constants, model requirements, and evaluation metrics are defined unambiguously. There is no subjective or speculative content. This criterion is met.\n\nAll other criteria (completeness, non-contradiction, feasibility) are also satisfied. The problem is a standard exercise in computational physics that combines classical theory with a modern machine learning technique.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be constructed as per the requirements.\n\n### Solution Derivation\n\nThe solution is developed by first establishing the physical ground truth, then synthesizing a dataset, and finally constructing and training the cVAE model to learn the underlying physical process.\n\n**1. Ground Truth from Classical Scattering Theory**\nThe relationship between the initial conditions $(E, b)$ and the final state is derived from the conservation of energy and angular momentum for the repulsive Coulomb potential $U(r) = \\alpha/r$, where $\\alpha = \\kappa q Q$.\n\nThe total energy $E$ and angular momentum $L$ are constants of motion:\n$$ E = \\frac{1}{2}mv_0^2 $$\n$$ L = v_0 b m $$\nwhere $v_0$ is the initial speed of the particle at infinity.\n\nThe scattering angle $\\theta_s$, which is the angle between the initial and final velocity vectors, can be found by solving the orbit equation. The standard result for hyperbolic trajectories under a $1/r^2$ force relates the impact parameter $b$ and the scattering angle $\\theta_s$ as:\n$$ \\cot\\left(\\frac{\\theta_s}{2}\\right) = \\frac{2 E b}{\\alpha} $$\nThis can be solved for $\\theta_s$. However, to capture the direction of deflection correctly for positive and negative impact parameters, it is more convenient to define the final asymptotic polar angle $\\phi_{\\text{final}}$ of the particle's trajectory. This angle depends on the sign of $b$:\n$$ \\tan\\left(\\frac{\\phi_{\\text{final}}}{2}\\right) = \\frac{\\alpha}{2 E b} $$\nThis can be solved for $\\phi_{\\text{final}}$:\n$$ \\phi_{\\text{final}} = 2 \\arctan\\left(\\frac{\\alpha}{2 E b}\\right) $$\nThis expression correctly gives $\\phi_{\\text{final}} \\to \\pi$ as $b \\to 0^+$ (head-on collision, particle scatters back) and $\\phi_{\\text{final}} \\to -\\pi$ as $b \\to 0^-$. For the special case $b=0$, the deflection is exactly backwards, thus $\\phi_{\\text{final}} = \\pi$. As $|b| \\to \\infty$, $\\phi_{\\text{final}} \\to 0$, representing no deflection.\n\nThe ground-truth position on the detector of radius $R$ is then given by projecting this final angle onto the detector circle:\n$$ \\mathbf{x}_{\\text{true}} = (x_{\\text{det}}, y_{\\text{det}}) = (R \\cos(\\phi_{\\text{final}}), R \\sin(\\phi_{\\text{final}})) $$\nThis physical model serves as the oracle for generating training data and for evaluating the final trained cVAE.\n\n**2. Data Synthesis and Preprocessing**\nA synthetic dataset is generated to train the cVAE. Pairs of conditions $\\mathbf{c} = (E, b)$ are sampled from uniform distributions over physically reasonable ranges ($E \\in [0.5, 3.5]\\,\\mathrm{keV}$, $b \\in [-3 \\times 10^{-12}, 3 \\times 10^{-12}]\\,\\mathrm{m}$). For each sampled pair, the ground-truth detector position $\\mathbf{x}_{\\text{true}}$ is calculated using the formula above. To simulate experimental uncertainty, additive Gaussian noise is introduced:\n$$ \\mathbf{x}_{\\text{noisy}} = \\mathbf{x}_{\\text{true}} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\text{det}}^2\\mathbf{I}) $$\nThe training set consists of pairs $(\\mathbf{c}, \\mathbf{x}_{\\text{noisy}})$. To ensure stable training of the neural network, both the conditions $\\mathbf{c}$ and the positions $\\mathbf{x}$ are normalized. The conditions are standardized to have zero mean and unit variance based on the training set statistics. The positions $\\mathbf{x}$ are normalized by dividing by the detector radius $R$, mapping them to a circle of radius $1$.\n\n**3. Conditional Variational Autoencoder (cVAE)**\nThe cVAE is designed to learn the conditional distribution $p(\\mathbf{x}|\\mathbf{c})$. It consists of two main components: an encoder and a decoder, both implemented as multi-layer perceptrons (MLPs).\n\n- **Encoder $q_{\\phi}(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})$**: This network takes the concatenated normalized position $\\mathbf{x}_{\\text{norm}}$ and condition $\\mathbf{c}_{\\text{norm}}$ as input. It outputs the parameters of the approximate posterior distribution over the latent variable $\\mathbf{z}$: the mean vector $\\boldsymbol{\\mu}_z$ and the log-variance vector $\\log(\\boldsymbol{\\sigma}_z^2)$. This posterior is a diagonal Gaussian.\n\n- **Decoder $p_{\\theta}(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})$**: This network takes the concatenated latent vector $\\mathbf{z}$ and condition $\\mathbf{c}_{\\text{norm}}$ as input. It outputs the reconstructed normalized position $\\hat{\\mathbf{x}}_{\\text{norm}}$.\n\n**4. Training Objective and Procedure**\nThe cVAE is trained by maximizing the ELBO, which is equivalent to minimizing a loss function $L$:\n$$ L = L_{\\text{recon}} + L_{\\text{KL}} $$\n- **Reconstruction Loss $L_{\\text{recon}}$**: This term encourages the decoder to accurately reconstruct the input data. We use the Mean Squared Error (MSE) between the original normalized position $\\mathbf{x}_{\\text{norm}}$ and the reconstructed one $\\hat{\\mathbf{x}}_{\\text{norm}}$:\n$$ L_{\\text{recon}} = \\|\\mathbf{x}_{\\text{norm}} - \\hat{\\mathbf{x}}_{\\text{norm}}\\|^2 $$\n- **Kullback-Leibler (KL) Divergence $L_{\\text{KL}}$**: This term acts as a regularizer, forcing the approximate posterior $q_{\\phi}(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})$ to be close to the standard normal prior $p(\\mathbf{z})$. For diagonal Gaussians, it has a closed-form expression:\n$$ L_{\\text{KL}} = D_{KL}(q_{\\phi} || p) = \\frac{1}{2} \\sum_{i=1}^{d_z} \\left(\\sigma_{z,i}^2 + \\mu_{z,i}^2 - \\log(\\sigma_{z,i}^2) - 1\\right) $$\nwhere $d_z$ is the dimension of the latent space.\n\nTraining proceeds using mini-batch stochastic gradient descent with the Adam optimizer. The reparameterization trick, $\\mathbf{z} = \\boldsymbol{\\mu}_z + \\boldsymbol{\\sigma}_z \\odot \\boldsymbol{\\epsilon}$ with $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, is used to allow gradients to flow from the reconstruction loss back through the sampling process to the encoder parameters.\n\n**5. Generation and Evaluation**\nAfter training, the model can generate a predicted detector position for any new condition $\\mathbf{c}_{\\text{test}}$. As specified, this is done by setting the latent variable to the mean of its prior, $\\mathbf{z}=\\mathbf{0}$, and passing it along with the normalized condition $\\mathbf{c}_{\\text{test,norm}}$ to the decoder. This yields a deterministic prediction $\\hat{\\mathbf{x}}_{\\text{norm,pred}}$. This prediction is denormalized by multiplying by $R$. The final evaluation is the Euclidean distance in meters between this predicted position $\\hat{\\mathbf{x}}_{\\text{pred}}$ and the noiseless ground-truth position $\\mathbf{x}_{\\text{true}}$ for each case in the test suite.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the CVAE-based modeling of Coulomb scattering.\n    It performs data synthesis, model training, and evaluation on the test suite.\n    \"\"\"\n    # For reproducibility of the neural network training\n    np.random.seed(42)\n\n    # --- Physical Constants ---\n    M_P = 1.67262192369e-27  # kg (proton mass)\n    E_CHARGE = 1.602176634e-19  # C (elementary charge)\n    K_COULOMB = 8.9875517923e9  # N m^2 / C^2\n    ALPHA = K_COULOMB * E_CHARGE**2  # J*m\n    R_DET = 1.0e-9  # m (detector radius)\n    SIGMA_DET = 1.0e-11  # m (detector noise standard deviation)\n    EV_TO_J = 1.602176634e-19  # J/eV\n\n    # --- Ground Truth Calculation ---\n    def get_ground_truth(E_J, b_m):\n        \"\"\"\n        Calculates the ground-truth detector position for given energy and impact parameter.\n        E_J: Energy in Joules.\n        b_m: Impact parameter in meters.\n        \"\"\"\n        positions = np.zeros((len(E_J), 2))\n        \n        # Handle non-zero impact parameter case\n        nonzero_b = b_m != 0\n        if np.any(nonzero_b):\n            arg = ALPHA / (2 * E_J[nonzero_b] * b_m[nonzero_b])\n            phi_final = 2 * np.arctan(arg)\n            positions[nonzero_b, 0] = R_DET * np.cos(phi_final)\n            positions[nonzero_b, 1] = R_DET * np.sin(phi_final)\n        \n        # Handle zero impact parameter (head-on collision)\n        zero_b = ~nonzero_b\n        if np.any(zero_b):\n            positions[zero_b, 0] = -R_DET\n            positions[zero_b, 1] = 0.0\n\n        return positions\n\n    # --- Data Synthesis ---\n    def generate_dataset(n_samples):\n        # Sample initial conditions\n        E_keV = np.random.uniform(0.5, 3.5, n_samples)\n        E_J = E_keV * 1000 * EV_TO_J\n        b_m = np.random.uniform(-3e-12, 3e-12, n_samples)\n        \n        # Conditions c = (E, b)\n        c = np.vstack([E_J, b_m]).T\n        \n        # Get ground truth positions\n        x_true = get_ground_truth(E_J, b_m)\n        \n        # Add detector noise\n        noise = np.random.normal(0, SIGMA_DET, x_true.shape)\n        x_noisy = x_true + noise\n        \n        return c, x_noisy\n\n    # --- Normalization ---\n    class Normalizer:\n        def __init__(self):\n            self.mean = None\n            self.std = None\n\n        def fit(self, data):\n            self.mean = np.mean(data, axis=0)\n            self.std = np.std(data, axis=0)\n            # Prevent division by zero if a feature is constant\n            self.std[self.std == 0] = 1.0\n\n        def transform(self, data):\n            return (data - self.mean) / self.std\n\n        def inverse_transform(self, data):\n            return data * self.std + self.mean\n\n    # --- Neural Network Implementation ---\n    def relu(x):\n        return np.maximum(0, x)\n\n    def relu_backward(dA, Z):\n        # dA is gradient of loss wrt output of relu\n        # Z is input to relu\n        dZ = dA.copy()\n        dZ[Z <= 0] = 0\n        return dZ\n\n    class CVAE:\n        def __init__(self, input_dim, cond_dim, latent_dim, hidden_dim, learning_rate=1e-3):\n            self.input_dim = input_dim\n            self.cond_dim = cond_dim\n            self.latent_dim = latent_dim\n            self.hidden_dim = hidden_dim\n            self.lr = learning_rate\n            self.params = {}\n            self.caches = {}\n            self.grads = {}\n\n            # Xavier initialization\n            # Encoder\n            self.params['W_e1'] = np.random.randn(input_dim + cond_dim, hidden_dim) * np.sqrt(2 / (input_dim + cond_dim))\n            self.params['b_e1'] = np.zeros((1, hidden_dim))\n            self.params['W_e2'] = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_e2'] = np.zeros((1, hidden_dim))\n            self.params['W_mu'] = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_mu'] = np.zeros((1, latent_dim))\n            self.params['W_lv'] = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_lv'] = np.zeros((1, latent_dim))\n\n            # Decoder\n            self.params['W_d1'] = np.random.randn(latent_dim + cond_dim, hidden_dim) * np.sqrt(2 / (latent_dim + cond_dim))\n            self.params['b_d1'] = np.zeros((1, hidden_dim))\n            self.params['W_d2'] = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_d2'] = np.zeros((1, hidden_dim))\n            self.params['W_out'] = np.random.randn(hidden_dim, input_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_out'] = np.zeros((1, input_dim))\n            \n            # Adam optimizer parameters\n            self.m = {k: np.zeros_like(v) for k, v in self.params.items()}\n            self.v = {k: np.zeros_like(v) for k, v in self.params.items()}\n            self.t = 0\n            self.beta1 = 0.9\n            self.beta2 = 0.999\n            self.epsilon = 1e-8\n\n\n        def forward(self, x, c):\n            # --- Encoder ---\n            enc_input = np.hstack([x, c])\n            self.caches['Z_e1'] = enc_input @ self.params['W_e1'] + self.params['b_e1']\n            self.caches['A_e1'] = relu(self.caches['Z_e1'])\n            self.caches['Z_e2'] = self.caches['A_e1'] @ self.params['W_e2'] + self.params['b_e2']\n            self.caches['A_e2'] = relu(self.caches['Z_e2'])\n            \n            mu = self.caches['A_e2'] @ self.params['W_mu'] + self.params['b_mu']\n            log_var = self.caches['A_e2'] @ self.params['W_lv'] + self.params['b_lv']\n            \n            # --- Reparameterization ---\n            std = np.exp(0.5 * log_var)\n            epsilon = np.random.randn(*mu.shape)\n            z = mu + std * epsilon\n            \n            self.caches['x'] = x\n            self.caches['c'] = c\n            self.caches['enc_input'] = enc_input\n            self.caches['mu'] = mu\n            self.caches['log_var'] = log_var\n            self.caches['std'] = std\n            self.caches['epsilon'] = epsilon\n\n            # --- Decoder ---\n            dec_input = np.hstack([z, c])\n            self.caches['Z_d1'] = dec_input @ self.params['W_d1'] + self.params['b_d1']\n            self.caches['A_d1'] = relu(self.caches['Z_d1'])\n            self.caches['Z_d2'] = self.caches['A_d1'] @ self.params['W_d2'] + self.params['b_d2']\n            self.caches['A_d2'] = relu(self.caches['Z_d2'])\n            \n            x_hat = self.caches['A_d2'] @ self.params['W_out'] + self.params['b_out']\n            self.caches['dec_input'] = dec_input\n            \n            return x_hat\n\n        def loss(self, x_hat):\n            batch_size = x_hat.shape[0]\n            # Reconstruction Loss (MSE)\n            recon_loss = np.mean((self.caches['x'] - x_hat)**2)\n            \n            # KL Divergence\n            kl_loss = -0.5 * np.sum(1 + self.caches['log_var'] - self.caches['mu']**2 - np.exp(self.caches['log_var']))\n            kl_loss /= batch_size\n            \n            return recon_loss + kl_loss\n\n        def backward(self, x_hat):\n            batch_size = x_hat.shape[0]\n            \n            # --- Gradient of Loss ---\n            d_recon_loss = (2 / batch_size) * (x_hat - self.caches['x'])\n            d_kl_mu = self.caches['mu'] / batch_size\n            d_kl_log_var = 0.5 * (np.exp(self.caches['log_var']) - 1) / batch_size\n            \n            # --- Decoder Backward ---\n            d_x_hat = d_recon_loss\n            dZ_out = d_x_hat\n            self.grads['W_out'] = self.caches['A_d2'].T @ dZ_out\n            self.grads['b_out'] = np.sum(dZ_out, axis=0, keepdims=True)\n            dA_d2 = dZ_out @ self.params['W_out'].T\n            \n            dZ_d2 = relu_backward(dA_d2, self.caches['Z_d2'])\n            self.grads['W_d2'] = self.caches['A_d1'].T @ dZ_d2\n            self.grads['b_d2'] = np.sum(dZ_d2, axis=0, keepdims=True)\n            dA_d1= dZ_d2 @ self.params['W_d2'].T\n            \n            dZ_d1 = relu_backward(dA_d1, self.caches['Z_d1'])\n            self.grads['W_d1'] = self.caches['dec_input'].T @ dZ_d1\n            self.grads['b_d1'] = np.sum(dZ_d1, axis=0, keepdims=True)\n            d_dec_input = dZ_d1 @ self.params['W_d1'].T\n\n            # --- Reparameterization Backward ---\n            d_z = d_dec_input[:, :self.latent_dim]\n            d_mu_from_recon = d_z\n            d_std = d_z * self.caches['epsilon']\n            d_log_var_from_recon = d_std * 0.5 * self.caches['std']\n            \n            # --- Encoder Backward ---\n            d_mu = d_mu_from_recon + d_kl_mu\n            d_log_var = d_log_var_from_recon + d_kl_log_var\n            \n            dZ_mu = d_mu\n            self.grads['W_mu'] = self.caches['A_e2'].T @ dZ_mu\n            self.grads['b_mu'] = np.sum(dZ_mu, axis=0, keepdims=True)\n            \n            dZ_lv = d_log_var\n            self.grads['W_lv'] = self.caches['A_e2'].T @ dZ_lv\n            self.grads['b_lv'] = np.sum(dZ_lv, axis=0, keepdims=True)\n            \n            dA_e2 = (dZ_mu @ self.params['W_mu'].T) + (dZ_lv @ self.params['W_lv'].T)\n            \n            dZ_e2 = relu_backward(dA_e2, self.caches['Z_e2'])\n            self.grads['W_e2'] = self.caches['A_e1'].T @ dZ_e2\n            self.grads['b_e2'] = np.sum(dZ_e2, axis=0, keepdims=True)\n            dA_e1 = dZ_e2 @ self.params['W_e2'].T\n            \n            dZ_e1 = relu_backward(dA_e1, self.caches['Z_e1'])\n            self.grads['W_e1'] = self.caches['enc_input'].T @ dZ_e1\n            self.grads['b_e1'] = np.sum(dZ_e1, axis=0, keepdims=True)\n        \n        def update_params(self):\n            self.t += 1\n            for key in self.params:\n                # Adam update\n                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * self.grads[key]\n                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (self.grads[key]**2)\n                m_hat = self.m[key] / (1 - self.beta1**self.t)\n                v_hat = self.v[key] / (1 - self.beta2**self.t)\n                self.params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n\n        def predict(self, c_norm):\n            z = np.zeros((c_norm.shape[0], self.latent_dim))\n            dec_input = np.hstack([z, c_norm])\n            \n            Z_d1 = dec_input @ self.params['W_d1'] + self.params['b_d1']\n            A_d1 = relu(Z_d1)\n            Z_d2 = A_d1 @ self.params['W_d2'] + self.params['b_d2']\n            A_d2 = relu(Z_d2)\n            x_hat_norm = A_d2 @ self.params['W_out'] + self.params['b_out']\n            \n            return x_hat_norm * R_DET\n\n    # --- Training Configuration ---\n    N_SAMPLES = 10000\n    EPOCHS = 100\n    BATCH_SIZE = 64\n    LEARNING_RATE = 1e-3\n    LATENT_DIM = 2\n    HIDDEN_DIM = 32\n\n    # --- Main Execution ---\n    \n    # 1. Prepare Data\n    c_train, x_train = generate_dataset(N_SAMPLES)\n    \n    c_normalizer = Normalizer()\n    c_normalizer.fit(c_train)\n    c_train_norm = c_normalizer.transform(c_train)\n    \n    x_train_norm = x_train / R_DET\n    \n    # 2. Initialize Model\n    model = CVAE(input_dim=2, cond_dim=2, latent_dim=LATENT_DIM, \n                 hidden_dim=HIDDEN_DIM, learning_rate=LEARNING_RATE)\n\n    # 3. Train Model\n    n_batches = N_SAMPLES // BATCH_SIZE\n    for epoch in range(EPOCHS):\n        permutation = np.random.permutation(N_SAMPLES)\n        c_train_norm_shuffled = c_train_norm[permutation]\n        x_train_norm_shuffled = x_train_norm[permutation]\n        \n        for i in range(n_batches):\n            start = i * BATCH_SIZE\n            end = start + BATCH_SIZE\n            c_batch = c_train_norm_shuffled[start:end]\n            x_batch = x_train_norm_shuffled[start:end]\n            \n            # Forward pass\n            x_hat_batch = model.forward(x_batch, c_batch)\n            \n            # Loss calculation\n            loss = model.loss(x_hat_batch)\n            \n            # Backward pass\n            model.backward(x_hat_batch)\n            \n            # Update parameters\n            model.update_params()\n\n    # 4. Evaluation\n    test_cases = [\n        (1.0, 0.0),                      # E in keV, b in m\n        (2.0, 1.0e-12),\n        (0.8, 2.0e-12),\n        (3.0, -1.5e-12),\n        (1.5, 2.0e-12),\n    ]\n\n    E_test_keV = np.array([c[0] for c in test_cases])\n    b_test_m = np.array([c[1] for c in test_cases])\n    \n    E_test_J = E_test_keV * 1000 * EV_TO_J\n    c_test = np.vstack([E_test_J, b_test_m]).T\n    \n    # Get ground truth for test cases\n    x_true_test = get_ground_truth(E_test_J, b_test_m)\n    \n    # Get model predictions for test cases\n    c_test_norm = c_normalizer.transform(c_test)\n    x_pred_test = model.predict(c_test_norm)\n    \n    # Calculate Euclidean errors\n    errors = np.linalg.norm(x_true_test - x_pred_test, axis=1)\n    \n    # 5. Print Final Output\n    print(f\"[{','.join(map(str, errors))}]\")\n\n\nsolve()\n```", "id": "2398395"}]}