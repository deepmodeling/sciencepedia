{"hands_on_practices": [{"introduction": "This practice draws a powerful analogy between the training of a neural network and the physical process of simulated annealing. You will explore how adding noise to backpropagation, akin to thermal fluctuations in a system described by Langevin dynamics, can help the model escape from suboptimal local minima and find a better global solution [@problem_id:2373958]. This exercise builds physical intuition for why stochasticity is a powerful tool in complex optimization and demonstrates its practical implementation.", "problem": "You will implement and analyze a computational experiment that connects artificial neural network training by backpropagation to thermal-noise-assisted escape from local minima, drawing a parallel to simulated annealing. Consider a scalar artificial neural network with a single parameter $w \\in \\mathbb{R}$ whose output is $y(w) = w$ for a single fixed input. Define a nonconvex training objective (interpreted as an energy landscape) by\n$$\nU(w) = (w^2 - 1)^2 + \\alpha\\, w + \\beta \\cos(k\\, w),\n$$\nwhere $\\alpha$, $\\beta$, and $k$ are fixed, known constants. Assume that the scalar network is trained by minimizing $U(w)$ via backpropagation in the sense that the training dynamics are driven by the gradient $\\frac{dU}{dw}$ with respect to $w$. You will incorporate thermal noise into the gradient signal to model thermal fluctuations, in direct analogy with overdamped Langevin dynamics and simulated annealing.\n\nFoundational bases you must use:\n- The definition of gradient-based training for a scalar parameter: the backpropagated gradient for the scalar parameter is $\\frac{dU}{dw}$.\n- The overdamped Langevin dynamics for a coordinate $w$ in an energy landscape $U(w)$ at temperature $T$ has the continuous-time form $dw/dt = -\\mu \\frac{dU}{dw} + \\sqrt{2 D}\\,\\xi(t)$ with mobility $\\mu$ and diffusion constant $D$ related by the Einstein relation $D = \\mu k_{\\mathrm{B}} T$, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $\\xi(t)$ is standard white noise. In discrete time with step size $\\Delta t$ and identifying the learning rate with $\\eta = \\mu \\Delta t$, this yields a time-discretized update with a thermally scaled Gaussian noise term.\n\nYour tasks:\n1. Starting from the above bases, derive the discrete-time update for $w_{t+1}$ from $w_t$ that models gradient-descent-like backpropagation with an additive thermal noise term at a (possibly time-dependent) temperature $T(t)$. Use a constant learning rate $\\eta > 0$. Clearly justify the exact variance of the Gaussian noise term in terms of $\\eta$ and $T(t)$.\n2. Using your derived update, implement a simulation that starts the parameter at $w_0 = 0.95$ and performs a fixed number of update steps $N_{\\mathrm{steps}}$. Define a successful escape event as ending in the deeper, global well near $w \\approx -1$, which you must operationalize as the condition $w_{N_{\\mathrm{steps}}} \\le -0.9$.\n3. For each experiment, run $N_{\\mathrm{trials}}$ independent trials with different random seeds and report the fraction of successful escapes as a floating-point number in $[0,1]$. Each trial must use the same initial condition $w_0 = 0.95$ and the same schedule $T(t)$, but independent realizations of the Gaussian thermal noise. Use the following fixed constants: $\\alpha = 0.3$, $\\beta = 0.1$, $k = 5.0$, $\\eta = 0.01$, $N_{\\mathrm{steps}} = 4000$, $N_{\\mathrm{trials}} = 64$.\n4. Consider the following temperature schedules $T(t)$ that emulate different training regimes. For each schedule, compute and report the escape fraction as described.\n   - Case A (no thermal noise): $T(t) = 0$ for all $t$.\n   - Case B (constant thermal bath): $T(t) = 0.02$ for all $t$.\n   - Case C (exponential simulated annealing, slow): $T(t) = T_0 \\exp(-t/\\tau)$ with $T_0 = 0.05$ and $\\tau = 1500$.\n   - Case D (exponential simulated annealing, hot and fast): $T(t) = T_0 \\exp(-t/\\tau)$ with $T_0 = 0.15$ and $\\tau = 300$.\n5. Your program must compute the escape fractions for Cases Aâ€“D in this order and output them on a single line as a comma-separated list enclosed in square brackets. Each value must be rounded to exactly three decimal places. For example, an output for four cases should look like: [0.000,0.312,0.516,0.203].\n\nAdditional instructions:\n- Angles in trigonometric functions are dimensionless radians.\n- There are no physical units required in the output; report pure numbers.\n- The random number generation must be reproducible: fix a master seed of your choice and then derive distinct seeds for each independent trial.\n- Implement the gradient $\\frac{dU}{dw}$ exactly from the given $U(w)$. Do not approximate derivatives numerically.\n- Your code must be self-contained and must not read any input.\n\nTest suite specification:\n- Use the exact constants and schedules listed above. These constitute the test cases to be run by your program.\n- Output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and rounded to three decimals, in the order [CaseA,CaseB,CaseC,CaseD].\n- Each of the four numbers must be of type float and must lie in $[0,1]$.", "solution": "The problem requires deriving a discrete-time update rule for a parameter $w$ undergoing training analogous to overdamped Langevin dynamics, and then using this rule to simulate escape from a local minimum of an energy landscape $U(w)$.\n\n**Part 1: Derivation of the Discrete-Time Update Rule**\n\nWe begin with the continuous-time overdamped Langevin equation for the parameter $w$ in the potential $U(w)$ at temperature $T$:\n$$\n\\frac{dw}{dt} = -\\mu \\frac{dU}{dw} + \\sqrt{2 D}\\,\\xi(t)\n$$\nHere, $\\mu$ is the mobility, $D$ is the diffusion constant, and $\\xi(t)$ is standard Gaussian white noise with $\\langle \\xi(t) \\rangle = 0$ and $\\langle \\xi(t)\\xi(t') \\rangle = \\delta(t-t')$. The mobility and diffusion are related by the Einstein relation, which we take as $D = \\mu T$ by absorbing the Boltzmann constant $k_B$ into the definition of temperature $T$ (treating $T$ as having units of energy).\n\nTo obtain a discrete-time update rule, we employ the Euler-Maruyama method to integrate this stochastic differential equation over a small time step $\\Delta t$. The change in $w$ from time $t$ to $t+\\Delta t$ is approximately:\n$$\nw_{t+1} - w_t = \\Delta w \\approx \\int_{t}^{t+\\Delta t} \\left( -\\mu \\frac{dU}{dw}\\bigg|_{w_s} + \\sqrt{2 D}\\,\\xi(s) \\right) ds\n$$\nAssuming $\\Delta t$ is small, the gradient term can be treated as constant over the interval, evaluated at $w_t = w(t)$. The integral of the noise term is a Wiener process increment, $\\Delta W_t = \\int_{t}^{t+\\Delta t} \\xi(s) ds$. This increment is a Gaussian random variable with mean $0$ and variance $\\Delta t$. We can write $\\Delta W_t = \\sqrt{\\Delta t} Z_t$, where $Z_t$ is a random variable drawn from the standard normal distribution $\\mathcal{N}(0, 1)$.\n\nThe discretized update equation thus becomes:\n$$\nw_{t+1} = w_t - \\mu \\Delta t \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 D} \\sqrt{\\Delta t} Z_t\n$$\nThe problem statement provides an analogy between the learning rate $\\eta$ and the physical parameters via $\\eta = \\mu \\Delta t$. Substituting this and the relation $D = \\mu T$ into the equation gives:\n$$\nw_{t+1} = w_t - \\eta \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 (\\mu T)} \\sqrt{\\Delta t} Z_t\n$$\nTo express the noise term purely in terms of $\\eta$ and $T$, we rearrange $\\eta = \\mu \\Delta t$ to find $\\mu = \\eta / \\Delta t$. Substituting this into the square root:\n$$\n\\sqrt{2 (\\mu T) \\Delta t} = \\sqrt{2 \\left(\\frac{\\eta}{\\Delta t} T\\right) \\Delta t} = \\sqrt{2 \\eta T}\n$$\nTherefore, the final discrete-time update rule for $w_t$ is:\n$$\nw_{t+1} = w_t - \\eta \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 \\eta T(t)} Z_t\n$$\nwhere $Z_t \\sim \\mathcal{N}(0, 1)$ and the temperature $T(t)$ can be time-dependent. The first term, $-\\eta \\frac{dU}{dw}$, corresponds to a standard gradient descent update in machine learning. The second term, $\\sqrt{2 \\eta T(t)} Z_t$, is the additive thermal noise. This term is a Gaussian random variable with mean $0$ and variance $(\\sqrt{2\\eta T(t)})^2 = 2\\eta T(t)$.\n\n**Part 2: Gradient Calculation**\n\nThe energy landscape (or training objective) is given by:\n$$\nU(w) = (w^2 - 1)^2 + \\alpha w + \\beta \\cos(k w)\n$$\nTo implement the update rule, we must compute the analytical gradient of $U(w)$ with respect to $w$:\n$$\n\\frac{dU}{dw} = \\frac{d}{dw} \\left( w^4 - 2w^2 + 1 + \\alpha w + \\beta \\cos(k w) \\right)\n$$\nApplying the rules of differentiation, we get:\n$$\n\\frac{dU}{dw} = 4w^3 - 4w + \\alpha - \\beta k \\sin(k w)\n$$\nThis expression is used directly in the simulation with the provided constants $\\alpha=0.3$, $\\beta=0.1$, and $k=5.0$.\n\n**Part 3: Simulation Procedure**\n\nThe simulation aims to calculate the fraction of times the parameter $w$, starting near a local minimum at $w \\approx 1$, escapes to the global minimum basin near $w \\approx -1$.\n\nFor each of the four specified temperature schedules $T(t)$, we perform the following procedure:\n1.  Initialize a counter for successful escapes to $0$.\n2.  Run $N_{\\mathrm{trials}} = 64$ independent trials. For each trial:\n    a. Initialize the parameter $w$ to its starting value, $w_0 = 0.95$.\n    b. Generate an independent sequence of random numbers for the thermal noise. This is ensured by seeding a random number generator for each trial with a unique seed derived from a master generator to ensure reproducibility across and independence between trials.\n    c. Perform $N_{\\mathrm{steps}} = 4000$ update steps. For each step $t$ from $0$ to $N_{\\mathrm{steps}}-1$:\n        i.  Calculate the temperature $T_t = T(t)$ according to the given schedule.\n        ii. Calculate the gradient $g_t = \\frac{dU}{dw}\\big|_{w_t}$ using the formula derived above.\n        iii. Calculate the standard deviation of the noise term, $\\sigma_t = \\sqrt{2 \\eta T_t}$. If $T_t = 0$, then $\\sigma_t=0$.\n        iv. Draw a random sample $Z_t$ from the standard normal distribution $\\mathcal{N}(0, 1)$.\n        v.  Update the parameter: $w_{t+1} = w_t - \\eta g_t + \\sigma_t Z_t$.\n    d. After $N_{\\mathrm{steps}}$, check the final state of the parameter, $w_{N_{\\mathrm{steps}}}$. If $w_{N_{\\mathrm{steps}}} \\le -0.9$, the trial is counted as a successful escape.\n3.  The final escape fraction for the temperature schedule is the total number of successful escapes divided by $N_{\\mathrm{trials}}$.\n\nThis procedure is repeated for all four cases:\n- Case A (No Noise): $T(t) = 0$.\n- Case B (Constant Temperature): $T(t) = 0.02$.\n- Case C (Slow Annealing): $T(t) = 0.05 \\exp(-t/1500)$.\n- Case D (Fast Annealing): $T(t) = 0.15 \\exp(-t/300)$.\n\nThe implementation will directly follow this logic, using the provided constants: $\\alpha = 0.3$, $\\beta = 0.1$, $k = 5.0$, and $\\eta = 0.01$. The final results are rounded to three decimal places and presented in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from the problem statement\nALPHA = 0.3\nBETA = 0.1\nK = 5.0\nETA = 0.01\nW0 = 0.95\nN_STEPS = 4000\nN_TRIALS = 64\nSUCCESS_THRESHOLD = -0.9\nMASTER_SEED = 42 # For reproducible results\n\ndef gradient_U(w):\n    \"\"\"\n    Computes the analytical gradient of the potential U(w).\n    dU/dw = 4*w^3 - 4*w + alpha - beta*k*sin(k*w)\n    \"\"\"\n    return 4 * w**3 - 4 * w + ALPHA - BETA * K * np.sin(K * w)\n\ndef calculate_escape_fraction(T_func, master_rng):\n    \"\"\"\n    Runs N_trials simulations for a given temperature schedule T_func.\n    Returns the fraction of successful escapes.\n    \"\"\"\n    success_count = 0\n    for i in range(N_TRIALS):\n        # Derive a distinct seed for each trial from the master RNG for independence\n        trial_seed = master_rng.integers(2**32)\n        trial_rng = np.random.default_rng(seed=trial_seed)\n\n        w = W0\n        for t in range(N_STEPS):\n            # Calculate gradient\n            grad = gradient_U(w)\n\n            # Calculate temperature and noise term\n            temp = T_func(t)\n            noise = 0.0\n            if temp > 0:\n                # Variance of noise is 2*eta*T, so std_dev is sqrt(2*eta*T)\n                std_dev = np.sqrt(2 * ETA * temp)\n                noise = trial_rng.normal(0.0, std_dev)\n\n            # Update w using the derived discrete-time Langevin equation\n            # w_{t+1} = w_t - eta * dU/dw + noise\n            w = w - ETA * grad + noise\n        \n        # Check for successful escape\n        if w = SUCCESS_THRESHOLD:\n            success_count += 1\n\n    return success_count / N_TRIALS\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run simulations, and print results.\n    \"\"\"\n    # Master RNG for generating trial seeds, ensuring independence between cases A, B, C, D\n    master_rng = np.random.default_rng(seed=MASTER_SEED)\n\n    # Define the four temperature schedules (test cases)\n    # Case A: No thermal noise\n    T_A = lambda t: 0.0\n    # Case B: Constant thermal bath\n    T_B = lambda t: 0.02\n    # Case C: Exponential simulated annealing, slow\n    T0_C, TAU_C = 0.05, 1500.0\n    T_C = lambda t: T0_C * np.exp(-t / TAU_C)\n    # Case D: Exponential simulated annealing, hot and fast\n    T0_D, TAU_D = 0.15, 300.0\n    T_D = lambda t: T0_D * np.exp(-t / TAU_D)\n\n    schedules = [T_A, T_B, T_C, T_D]\n\n    results = []\n    for schedule in schedules:\n        fraction = calculate_escape_fraction(schedule, master_rng)\n        results.append(fraction)\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n\n```", "id": "2373958"}, {"introduction": "Here, we move beyond the standard assumption of a \"flat\" Euclidean space for model parameters and venture into the realm of information geometry. This practice introduces the Fisher Information Metric, which defines a more natural geometry based on the statistical properties of the model itself [@problem_id:2373930]. By implementing Natural Gradient Descent, you will learn to navigate the loss landscape using steps that are invariant to parameterization, often resulting in significantly faster and more stable convergence.", "problem": "Implement, analyze, and compare two first-order optimization methods for training a single-layer Artificial Neural Network (ANN) classifier (logistic regression) on small, deterministic datasets: (i) standard Euclidean gradient descent and (ii) a Riemannian steepest-descent method that follows the geodesic direction induced by the Fisher Information Metric (FIM) manifold. Your derivation and implementation must start from first principles: the definitions of the model, the negative log-likelihood, the Kullbackâ€“Leibler (KL) divergence, and the Fisher information. No pre-derived update rules may be assumed. You must: (a) derive the appropriate Riemannian gradient direction from these definitions, (b) implement both methods, and (c) evaluate them on the provided test suite. There are no physical units in this problem. All angles, if any, are to be considered dimensionless real numbers. All constants in this problem are real numbers.\n\nModel and data. Consider a binary classifier with inputs $x \\in \\mathbb{R}^d$, binary label $y \\in \\{0,1\\}$, and parameter vector $\\theta \\in \\mathbb{R}^d$. The model is the logistic regression ANN given by\n$$\np_\\theta(y=1 \\mid x) = \\sigma(\\theta^\\top x), \\quad \\text{where } \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\nThe average negative log-likelihood over a dataset $\\{(x_i,y_i)\\}_{i=1}^N$ is\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^N \\left[ y_i \\log p_\\theta(y_i=1 \\mid x_i) + (1-y_i)\\log\\left(1 - p_\\theta(y_i=1 \\mid x_i)\\right) \\right].\n$$\n\nFoundational definitions. Use only the following as your starting point for derivations:\n- The definition of the Kullbackâ€“Leibler (KL) divergence between two distributions $P$ and $Q$,\n$$\n\\mathrm{KL}(P \\,\\|\\, Q) = \\mathbb{E}_P\\!\\left[ \\log \\frac{dP}{dQ} \\right].\n$$\n- The Fisher Information Metric (FIM) tensor at parameters $\\theta$,\n$$\nF(\\theta) = \\mathbb{E}\\!\\left[ \\nabla_\\theta \\log p_\\theta(Y \\mid X) \\, \\nabla_\\theta \\log p_\\theta(Y \\mid X)^\\top \\right],\n$$\nwith expectation over the data-generating process for $(X,Y)$.\n\nTasks.\n1. From the definitions above and a local second-order expansion of the Kullbackâ€“Leibler divergence, derive the Riemannian steepest-descent direction at $\\theta$ induced by the Fisher Information Metric (FIM) for minimizing $\\mathcal{L}(\\theta)$. Express the resulting update in terms of $\\nabla_\\theta \\mathcal{L}(\\theta)$ and $F(\\theta)$ without assuming any prior shortcut formulas.\n2. Specialize the above to the logistic regression model, obtaining explicit expressions for the average Euclidean gradient $\\nabla_\\theta \\mathcal{L}(\\theta)$ and for the Fisher Information Matrix $F(\\theta)$ in terms of the dataset $\\{(x_i,y_i)\\}$ and $\\theta$.\n3. Implement two training procedures that each perform $T$ steps starting from an initial vector $\\theta_0$:\n   - Euclidean gradient descent with step size $\\alpha_{\\mathrm{E}}$.\n   - Fisherâ€“Riemannian steepest descent using a step size $\\alpha_{\\mathrm{N}}$ and a Tikhonov damping parameter $\\lambda  0$ applied to the metric, meaning the metric tensor used in the step is $F(\\theta) + \\lambda I$, where $I$ is the identity matrix.\n4. For numerical stability in implementation, you must ensure a stable computation of the sigmoid and the logarithms in $\\mathcal{L}(\\theta)$, and you must use a linear solver or a numerically stable inverse for the metric system.\n5. For each test case below, return the final average negative log-likelihoods after training, for both methods. Your program must output a single line that is a list of $6$ floating-point numbers rounded to six decimal places, ordered as $[\\mathcal{L}^{(1)}_{\\mathrm{E,final}}, \\mathcal{L}^{(1)}_{\\mathrm{N,final}}, \\mathcal{L}^{(2)}_{\\mathrm{E,final}}, \\mathcal{L}^{(2)}_{\\mathrm{N,final}}, \\mathcal{L}^{(3)}_{\\mathrm{E,final}}, \\mathcal{L}^{(3)}_{\\mathrm{N,final}}]$.\n\nTest suite. Use exactly the following datasets, initializations, and hyperparameters. In all cases, use $d=2$. For each test, use $T$ steps, the stated learning rates, and the stated damping factor $\\lambda$.\n- Test $1$ (balanced, moderately scaled):\n$$\nX^{(1)} = \\begin{bmatrix}\n2  1 \\\\\n2  -1 \\\\\n-2  1 \\\\\n-2  -1 \\\\\n1  2 \\\\\n-1  -2\n\\end{bmatrix}, \\quad\ny^{(1)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\n\\theta^{(1)}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad\nT^{(1)} = 100, \\quad\n\\alpha^{(1)}_{\\mathrm{E}} = 0.1, \\quad\n\\alpha^{(1)}_{\\mathrm{N}} = 0.1, \\quad\n\\lambda^{(1)} = 10^{-2}.\n$$\n- Test $2$ (collinear features; nearly singular metric, requires damping):\n$$\nX^{(2)} = \\begin{bmatrix}\n1  2 \\\\\n2  4 \\\\\n-1  -2 \\\\\n-2  -4\n\\end{bmatrix}, \\quad\ny^{(2)} = \\begin{bmatrix}\n1 \\\\ 1 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad\n\\theta^{(2)}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad\nT^{(2)} = 100, \\quad\n\\alpha^{(2)}_{\\mathrm{E}} = 0.1, \\quad\n\\alpha^{(2)}_{\\mathrm{N}} = 0.1, \\quad\n\\lambda^{(2)} = 10^{-2}.\n$$\n- Test $3$ (saturated initialization; tiny Fisher factors $p(1-p)$):\n$$\nX^{(3)} = \\begin{bmatrix}\n3  1 \\\\\n-3  -1 \\\\\n1  3 \\\\\n-1  -3\n\\end{bmatrix}, \\quad\ny^{(3)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\n\\theta^{(3)}_0 = \\begin{bmatrix} 20 \\\\ -20 \\end{bmatrix}, \\quad\nT^{(3)} = 50, \\quad\n\\alpha^{(3)}_{\\mathrm{E}} = 0.05, \\quad\n\\alpha^{(3)}_{\\mathrm{N}} = 0.05, \\quad\n\\lambda^{(3)} = 10^{-2}.\n$$\n\nBackpropagation note. For the Euclidean gradient, compute $\\nabla_\\theta \\mathcal{L}(\\theta)$ by applying the chain rule (which, in the context of ANNs, is backpropagation). For the Fisherâ€“Riemannian method, compute the Fisher Information Matrix using the model and data, apply the damping $\\lambda I$, and update along the geodesic steepest-descent direction you derive.\n\nOutput format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be\n$$\n\\left[\n\\mathcal{L}^{(1)}_{\\mathrm{E,final}}, \\mathcal{L}^{(1)}_{\\mathrm{N,final}}, \\mathcal{L}^{(2)}_{\\mathrm{E,final}}, \\mathcal{L}^{(2)}_{\\mathrm{N,final}}, \\mathcal{L}^{(3)}_{\\mathrm{E,final}}, \\mathcal{L}^{(3)}_{\\mathrm{N,final}}\n\\right],\n$$\nwith each entry rounded to six decimal places. No other output is permitted.", "solution": "The problem requires the derivation, implementation, and comparison of two optimization methods for training a logistic regression model: standard Euclidean gradient descent and a Riemannian steepest-descent method based on the Fisher Information Metric (FIM). The derivation must begin from first principles.\n\nFirst, the derivation of the Riemannian steepest-descent direction is addressed. The objective is to find a parameter update $\\delta\\theta$ that produces the maximum decrease in the loss function $\\mathcal{L}(\\theta)$. A first-order Taylor expansion of the loss gives the change $\\Delta \\mathcal{L} \\approx \\nabla_\\theta \\mathcal{L}(\\theta)^\\top \\delta\\theta$. To find the direction of steepest descent, this change must be minimized subject to a constraint on the \"size\" of the step $\\delta\\theta$. In a standard Euclidean setting, this constraint is on the squared Euclidean norm $||\\delta\\theta||_2^2$, leading to the familiar gradient descent update $\\delta\\theta \\propto -\\nabla_\\theta \\mathcal{L}(\\theta)$.\n\nIn the context of probabilistic models, the geometry of the parameter space is more naturally described by how much the probability distribution $p_\\theta$ changes. A fundamental measure of the dissimilarity between two distributions $p_\\theta$ and $p_{\\theta'}$ is the Kullbackâ€“Leibler (KL) divergence, $\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta'})$. For an infinitesimal step $\\delta\\theta$, so that $\\theta' = \\theta + \\delta\\theta$, the KL-divergence provides a local metric for the distance between parameters. We expand $\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta})$ for small $\\delta\\theta$.\nThe definition of KL-divergence is $\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta}) = \\mathbb{E}_{Y,X \\sim p_\\theta}[\\log p_\\theta(Y|X) - \\log p_{\\theta+\\delta\\theta}(Y|X)]$.\nWe perform a second-order Taylor expansion of the term $\\log p_{\\theta+\\delta\\theta}(Y|X)$ around $\\theta$:\n$$\n\\log p_{\\theta+\\delta\\theta}(Y|X) \\approx \\log p_\\theta(Y|X) + (\\nabla_\\theta \\log p_\\theta(Y|X))^\\top \\delta\\theta + \\frac{1}{2} \\delta\\theta^\\top (\\nabla^2_\\theta \\log p_\\theta(Y|X)) \\delta\\theta\n$$\nSubstituting this into the KL-divergence expression gives:\n$$\n\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta}) \\approx -\\mathbb{E}[(\\nabla_\\theta \\log p_\\theta)^\\top \\delta\\theta] - \\frac{1}{2} \\mathbb{E}[\\delta\\theta^\\top (\\nabla^2_\\theta \\log p_\\theta) \\delta\\theta]\n$$\nTwo key identities of information theory are now employed. First, the expectation of the score function (the gradient of the log-likelihood) is zero: $\\mathbb{E}[\\nabla_\\theta \\log p_\\theta] = 0$. Second, the Fisher Information Matrix (FIM), $F(\\theta)$, is given by the information matrix equality: $F(\\theta) = \\mathbb{E}[(\\nabla_\\theta \\log p_\\theta)(\\nabla_\\theta \\log p_\\theta)^\\top] = -\\mathbb{E}[\\nabla^2_\\theta \\log p_\\theta]$.\nApplying these identities simplifies the KL-divergence to:\n$$\n\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta}) \\approx \\frac{1}{2} \\delta\\theta^\\top F(\\theta) \\delta\\theta\n$$\nThis quadratic form defines a Riemannian metric on the parameter manifold, where the metric tensor is the FIM. The Riemannian steepest-descent direction is the one that minimizes $\\nabla_\\theta \\mathcal{L}(\\theta)^\\top \\delta\\theta$ subject to a fixed step size in this geometry, i.e., $\\frac{1}{2} \\delta\\theta^\\top F(\\theta) \\delta\\theta = \\text{constant}$. Using the method of Lagrange multipliers, we minimize $\\mathcal{J}(\\delta\\theta) = \\nabla_\\theta \\mathcal{L}(\\theta)^\\top \\delta\\theta + \\mu (\\frac{1}{2} \\delta\\theta^\\top F(\\theta) \\delta\\theta)$. Differentiating with respect to $\\delta\\theta$ and setting to zero yields $\\nabla_\\theta \\mathcal{L}(\\theta) + \\mu F(\\theta) \\delta\\theta = 0$. This gives the optimal step direction $\\delta\\theta \\propto -F(\\theta)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta)$. The vector $d_N = -F(\\theta)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta)$ is known as the natural gradient. The update rule for the Fisher-Riemannian method is $\\theta_{t+1} = \\theta_t - \\alpha_N F(\\theta_t)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta_t)$. Including the Tikhonov damping term $\\lambda I$ for numerical stability, the update becomes:\n$$\n\\theta_{t+1} = \\theta_t - \\alpha_N (F(\\theta_t) + \\lambda I)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta_t)\n$$\n\nSecond, we specialize the required quantities for the given logistic regression model. The model prediction is $p_i = p_\\theta(y=1|x_i) = \\sigma(\\theta^\\top x_i)$, where $\\sigma(z) = (1+e^{-z})^{-1}$. The derivative of the sigmoid function is $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$.\nThe Euclidean gradient of the average negative log-likelihood $\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_i [y_i \\log p_i + (1-y_i)\\log(1-p_i)]$ is found by applying the chain rule. For a single data point $(x_i, y_i)$, the gradient of its contribution to the sum inside $\\mathcal{L}$ is:\n$$\n\\nabla_\\theta [y_i \\log p_i + (1-y_i)\\log(1-p_i)] = \\left(\\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i}\\right)\\nabla_\\theta p_i = \\frac{y_i - p_i}{p_i(1-p_i)} (p_i(1-p_i)x_i) = (y_i - p_i)x_i\n$$\nThe full gradient is therefore $\\nabla_\\theta \\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^N (y_i - p_i)x_i = \\frac{1}{N}\\sum_{i=1}^N (p_i - y_i)x_i$. In matrix notation, this is $\\nabla_\\theta \\mathcal{L}(\\theta) = \\frac{1}{N}X^\\top(p-y)$.\n\nThe FIM for a single data point $x_i$, denoted $F_{x_i}(\\theta)$, is computed by taking the expectation over the model's predicted label distribution $Y_i \\sim \\text{Bernoulli}(p_i)$. The gradient of the log-likelihood for one sample is $\\nabla_\\theta \\log p_\\theta(y_i|x_i) = (y_i-p_i)x_i$.\n$$\nF_{x_i}(\\theta) = \\mathbb{E}_{Y_i \\sim p_i}[ (Y_i-p_i)^2 x_i x_i^\\top ] = x_i x_i^\\top \\mathbb{E}[(Y_i-p_i)^2]\n$$\nThe expectation term is the variance of a Bernoulli random variable, which is $p_i(1-p_i)$. Thus, $F_{x_i}(\\theta) = p_i(1-p_i)x_i x_i^\\top$. The total FIM for the dataset is the average, known as the empirical FIM:\n$$\nF(\\theta) = \\frac{1}{N}\\sum_{i=1}^N p_i(1-p_i) x_i x_i^\\top\n$$\nIn matrix notation, this is $F(\\theta) = \\frac{1}{N}X^\\top W X$, where $W$ is a diagonal matrix with entries $W_{ii} = p_i(1-p_i)$.\n\nFinally, the implementation proceeds by setting up two optimization loops for each test case, one for Euclidean gradient descent and one for the Fisher-Riemannian method. In each step of the Riemannian method, the Euclidean gradient $\\nabla_\\theta \\mathcal{L}(\\theta)$ and the empirical FIM $F(\\theta)$ are computed. The damped metric $M = F(\\theta) + \\lambda I$ is formed, and the linear system $Mv = \\nabla_\\theta \\mathcal{L}(\\theta)$ is solved for the natural gradient direction $v$. The parameters are then updated using this direction and the step size $\\alpha_N$. For numerical stability, the loss calculation clips probability values away from exactly $0$ and $1$. The final negative log-likelihood values are calculated after $T$ iterations for both methods and reported for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements, analyzes, and compares Euclidean gradient descent and\n    Fisher-Riemannian steepest descent for training a logistic regression model.\n    \"\"\"\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        # The standard implementation is sufficient due to numpy's handling of large exponents.\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def calculate_loss(y, p_hat):\n        \"\"\"\n        Calculates the average negative log-likelihood (binary cross-entropy).\n        Clips probabilities to avoid log(0).\n        \"\"\"\n        epsilon = 1e-15\n        p_hat = np.clip(p_hat, epsilon, 1.0 - epsilon)\n        return -np.mean(y * np.log(p_hat) + (1.0 - y) * np.log(1.0 - p_hat))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([[2., 1.], [2., -1.], [-2., 1.], [-2., -1.], [1., 2.], [-1., -2.]]),\n            \"y\": np.array([[1.], [0.], [1.], [0.], [1.], [0.]]),\n            \"theta0\": np.array([[0.], [0.]]),\n            \"T\": 100,\n            \"alpha_E\": 0.1,\n            \"alpha_N\": 0.1,\n            \"lambda\": 1e-2,\n        },\n        {\n            \"X\": np.array([[1., 2.], [2., 4.], [-1., -2.], [-2., -4.]]),\n            \"y\": np.array([[1.], [1.], [0.], [0.]]),\n            \"theta0\": np.array([[0.], [0.]]),\n            \"T\": 100,\n            \"alpha_E\": 0.1,\n            \"alpha_N\": 0.1,\n            \"lambda\": 1e-2,\n        },\n        {\n            \"X\": np.array([[3., 1.], [-3., -1.], [1., 3.], [-1., -3.]]),\n            \"y\": np.array([[1.], [0.], [1.], [0.]]),\n            \"theta0\": np.array([[20.], [-20.]]),\n            \"T\": 50,\n            \"alpha_E\": 0.05,\n            \"alpha_N\": 0.05,\n            \"lambda\": 1e-2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, theta0 = case[\"X\"], case[\"y\"], case[\"theta0\"]\n        T, alpha_E, alpha_N, lam = case[\"T\"], case[\"alpha_E\"], case[\"alpha_N\"], case[\"lambda\"]\n        N, d = X.shape\n\n        # --- Euclidean Gradient Descent ---\n        theta_E = theta0.copy()\n        for _ in range(T):\n            z_E = X @ theta_E\n            p_E = sigmoid(z_E)\n            grad_E = (1.0 / N) * X.T @ (p_E - y)\n            theta_E -= alpha_E * grad_E\n\n        p_final_E = sigmoid(X @ theta_E)\n        loss_E = calculate_loss(y, p_final_E)\n        results.append(loss_E)\n\n        # --- Fisher-Riemannian Steepest Descent (Natural Gradient) ---\n        theta_N = theta0.copy()\n        I = np.identity(d)\n        for _ in range(T):\n            z_N = X @ theta_N\n            p_N = sigmoid(z_N)\n\n            # Euclidean gradient\n            grad_N = (1.0 / N) * X.T @ (p_N - y)\n\n            # Fisher Information Matrix (empirical)\n            weights = p_N * (1.0 - p_N)\n            W = np.diag(weights.flatten())\n            F = (1.0 / N) * (X.T @ W @ X)\n\n            # Damped metric and natural gradient direction calculation\n            M = F + lam * I\n            # Solve Mv = grad_N for v, where v is the natural gradient direction\n            v = np.linalg.solve(M, grad_N)\n            \n            theta_N -= alpha_N * v\n        \n        p_final_N = sigmoid(X @ theta_N)\n        loss_N = calculate_loss(y, p_final_N)\n        results.append(loss_N)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2373930"}, {"introduction": "This exercise bridges the gap between abstract optimization theory and its implementation on physical hardware, where numerical precision is finite. You will investigate the effects of quantizing gradients during backpropagation, systematically reducing the bit-depth and observing its impact on learning [@problem_id:2373947]. This reveals a critical threshold where training breaks down, analogous to a phase transition, and provides crucial insights into the trade-offs involved in designing efficient, low-precision algorithms for deep learning.", "problem": "Construct a quantitative experiment to study learning under finite-precision backpropagation by training a single linear neuron with quantized gradients. Consider a dataset with input matrix $X \\in \\mathbb{R}^{N \\times d}$ and output vector $y \\in \\mathbb{R}^{N}$, where $N=d$ and $X$ is the identity matrix $I_{d}$. Let the target parameter vector be $w^{\\star} \\in \\mathbb{R}^{d}$ with entries $w^{\\star} = [1,-2,3,-4,5]^{\\top}$, and let the initial parameter vector be $w_{0}=\\mathbf{0} \\in \\mathbb{R}^{d}$. Define the empirical mean-squared error loss\n$$\nL(w) = \\frac{1}{N}\\,\\lVert Xw - y \\rVert_{2}^{2}.\n$$\nPerform full-batch gradient descent with a fixed learning rate $\\eta$ using a quantized gradient $Q_{b}(\\nabla L(w))$ that has exactly $b$ signed bits of precision (including the sign bit) applied componentwise. Let $N=5$, $d=5$, $X = I_{5}$, $y = w^{\\star}$, $w_{0}=\\mathbf{0}$, and $\\eta=1$. Train for exactly $T$ iterations with $T=2000$.\n\nThe quantizer is defined as follows. Compute the initial full-precision gradient $g_{0} = \\nabla L(w_{0})$ and set the fixed dynamic range $s_{\\mathrm{fixed}} = \\max_{j} |(g_{0})_{j}|$. For $b \\ge 2$, define the uniform quantization step\n$$\n\\Delta_{b} = \\frac{s_{\\mathrm{fixed}}}{2^{b-1}-1},\n$$\nand apply componentwise rounding-to-zero quantization\n$$\n\\left[Q_{b}(g)\\right]_{j} = \\operatorname{sign}(g_{j}) \\cdot \\Delta_{b}\\,\\left\\lfloor \\frac{|g_{j}|}{\\Delta_{b}} \\right\\rfloor,\n$$\nwith the convention that $\\operatorname{sign}(0)=0$. For the edge case $b=1$, use one-bit sign quantization\n$$\n\\left[Q_{1}(g)\\right]_{j} = s_{\\mathrm{fixed}} \\cdot \\operatorname{sign}(g_{j}),\n$$\nwith $\\operatorname{sign}(0)=0$. The training update is\n$$\nw_{t+1} = w_{t} - \\eta \\, Q_{b}\\big(\\nabla L(w_{t})\\big),\n$$\napplied for $t=0,1,\\dots,T-1$.\n\nDefine the learning breakdown criterion as failure to achieve a target loss threshold $\\tau$ by iteration $T$, that is, breakdown occurs if $L(w_{T})  \\tau$. Use the fixed threshold $\\tau = 0.002$.\n\nTest suite and required output:\n- Use the bit-precision test cases $b \\in \\{32,16,8,7,5,3,2,1\\}$.\n- For each $b$ in this set, run the above training and record a boolean indicating breakdown: output $\\mathrm{True}$ if $L(w_{T})  \\tau$ and $\\mathrm{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[False,False,False,True,True,True,True,True]\") in the exact order of the test cases $[32,16,8,7,5,3,2,1]$.\n\nAll quantities are unitless real numbers. Angles are not used. The required final output format is a single line containing the list of booleans as specified, with no additional text.", "solution": "The task is to simulate the training of a single linear neuron using full-batch gradient descent where the gradients are quantized. We will analyze the behavior for different bit precisions and determine when the training process breaks down, defined as failing to reach a specific loss threshold.\n\nFirst, we formulate the loss function and its gradient. The model is a linear neuron with output $Xw$ for an input matrix $X$ and weight vector $w$. The loss is the empirical mean-squared error:\n$$\nL(w) = \\frac{1}{N} \\lVert Xw - y \\rVert_2^2\n$$\nThe problem specifies $N=5$, $d=5$, $X = I_5$ (the $5 \\times 5$ identity matrix), and the target outputs $y$ are set equal to the target weights $w^{\\star}$. Thus, the loss function simplifies to:\n$$\nL(w) = \\frac{1}{5} \\lVert I_5 w - w^\\star \\rVert_2^2 = \\frac{1}{5} \\lVert w - w^\\star \\rVert_2^2\n$$\nThe global minimum of this loss is $L=0$ at $w = w^\\star$. The gradient of the loss function with respect to $w$ is:\n$$\n\\nabla L(w) = \\frac{2}{N} X^{\\top}(Xw - y) = \\frac{2}{5} I_5^{\\top}(I_5 w - w^\\star) = \\frac{2}{5} (w - w^\\star)\n$$\n\nThe training process starts with the initial parameter vector $w_0 = \\mathbf{0}$. The initial gradient is therefore:\n$$\ng_0 = \\nabla L(w_0) = \\frac{2}{5} (w_0 - w^\\star) = -\\frac{2}{5} w^\\star\n$$\nGiven $w^{\\star} = [1, -2, 3, -4, 5]^{\\top}$, we compute $g_0$:\n$$\ng_0 = -\\frac{2}{5} [1, -2, 3, -4, 5]^{\\top} = [-0.4, 0.8, -1.2, 1.6, -2.0]^{\\top}\n$$\nThe quantization scheme uses a fixed dynamic range $s_{\\mathrm{fixed}}$ determined by the maximum magnitude of the initial gradient components:\n$$\ns_{\\mathrm{fixed}} = \\max_{j} |(g_0)_j| = \\max\\{0.4, 0.8, 1.2, 1.6, 2.0\\} = 2.0\n$$\n\nThe training proceeds for $T=2000$ iterations using the update rule:\n$$\nw_{t+1} = w_t - \\eta \\, Q_b(\\nabla L(w_t))\n$$\nwhere the learning rate is $\\eta=1$, and $Q_b$ is the quantization operator for $b$ bits of precision.\n\nFor $b \\ge 2$, the uniform quantization step $\\Delta_b$ is defined as:\n$$\n\\Delta_b = \\frac{s_{\\mathrm{fixed}}}{2^{b-1}-1} = \\frac{2.0}{2^{b-1}-1}\n$$\nThe component-wise quantization function is:\n$$\n[Q_b(g)]_j = \\operatorname{sign}(g_j) \\cdot \\Delta_b \\cdot \\left\\lfloor \\frac{|g_j|}{\\Delta_b} \\right\\rfloor\n$$\nThis function maps any gradient component $g_j$ to the nearest multiple of $\\Delta_b$ that is closer to zero. A \"dead zone\" exists around zero: if $|g_j|  \\Delta_b$, then $[Q_b(g)]_j = 0$.\n\nFor the edge case $b=1$, the quantization is:\n$$\n[Q_1(g)]_j = s_{\\mathrm{fixed}} \\cdot \\operatorname{sign}(g_j) = 2.0 \\cdot \\operatorname{sign}(g_j)\n$$\nThis maps any non-zero gradient component to either $+2.0$ or $-2.0$.\n\nThe simulation will be run for each bit precision $b \\in \\{32, 16, 8, 7, 5, 3, 2, 1\\}$. After $T=2000$ iterations, we calculate the final loss $L(w_T)$ and check for breakdown, which occurs if $L(w_T)  \\tau=0.002$.\n\nA key cause of breakdown is the quantization dead zone. If the magnitude of an initial gradient component $|(g_0)_j|$ is smaller than the quantization step $\\Delta_b$, that component will be quantized to zero from the start. The corresponding weight $w_j$ will never be updated from its initial value of $0$. The smallest magnitude of a component in $g_0$ is $|(g_0)_0| = 0.4$. Breakdown is guaranteed if $\\Delta_b  0.4$:\n$$\n\\frac{2.0}{2^{b-1}-1}  0.4 \\implies 5  2^{b-1}-1 \\implies 6  2^{b-1} \\implies b-1  \\log_2(6) \\approx 2.585 \\implies b  3.585\n$$\nThus, for $b=3, 2, 1$, we anticipate breakdown because at least one weight component will fail to learn.\n\nFor higher values of $b$, where $\\Delta_b$ is small enough for all components to begin learning, the quantization error still limits the final precision. The training effectively stalls when the gradient magnitude for each component, $|\\nabla L(w_t)_j|$, falls below $\\Delta_b$. This leaves a residual error in the weights, $w_T - w^\\star$, where each component is roughly bounded by $\\frac{N}{2}\\Delta_b$. The final loss is approximately proportional to $\\Delta_b^2$. If this minimum achievable loss is greater than the threshold $\\tau$, breakdown occurs. We expect this to happen for intermediate values of $b$ such as $7$ and $5$.\n\nFor large $b$ (e.g., $32, 16$), $\\Delta_b$ is very small, the quantization error is negligible, and the algorithm converges well, resulting in a final loss below $\\tau$. The provided program implements this simulation precisely to find the breakdown point.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates training a single linear neuron with quantized gradients\n    to study learning under finite precision.\n    \"\"\"\n    # Define problem parameters\n    N = 5.0\n    d = 5\n    w_star = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    w0 = np.zeros(d)\n    eta = 1.0\n    T = 2000\n    tau = 0.002\n\n    # Define the bit-precision test cases\n    b_cases = [32, 16, 8, 7, 5, 3, 2, 1]\n    \n    results = []\n\n    # Calculate the fixed dynamic range s_fixed from the initial gradient.\n    # This is constant for all test cases.\n    g0 = (2.0 / N) * (w0 - w_star)\n    s_fixed = np.max(np.abs(g0))\n\n    # Iterate over each bit-precision case\n    for b in b_cases:\n        # Initialize weights for the current simulation\n        w = w0.copy()\n\n        # Define the quantizer for the current bit precision b\n        def quantize_gradient(g: np.ndarray, b_val: int) -> np.ndarray:\n            \"\"\"\n            Applies component-wise quantization to the gradient vector.\n            \"\"\"\n            # Case b=1: Sign quantization\n            if b_val == 1:\n                return s_fixed * np.sign(g)\n\n            # Case b >= 2: Uniform quantization\n            # The denominator 2**(b-1) - 1 is greater than 0 for b >= 2.\n            denominator = (2**(b_val - 1)) - 1\n            \n            # For very large b like 32, delta_b is very small but non-zero.\n            # If hypothetically b was so large that denominator overflows,\n            # delta_b would tend to 0, meaning no quantization.\n            if denominator == 0:  # Should not happen for b>=2\n                return g\n            \n            delta_b = s_fixed / denominator\n\n            # The quantizer function is Q_b(g)_j = sign(g_j) * delta_b * floor(|g_j|/delta_b)\n            # This is equivalent to rounding towards zero (truncation).\n            sign_g = np.sign(g)\n            abs_g = np.abs(g)\n            \n            # np.floor implements the rounding-to-zero logic when combined with sign.\n            quantized_magnitude = delta_b * np.floor(abs_g / delta_b)\n            \n            return sign_g * quantized_magnitude\n\n        # Main training loop\n        for _ in range(T):\n            # 1. Calculate the full-precision gradient\n            gradient = (2.0 / N) * (w - w_star)\n            \n            # 2. Quantize the gradient\n            quantized_grad = quantize_gradient(gradient, b)\n            \n            # 3. Update the weights\n            w = w - eta * quantized_grad\n            \n        # After training, calculate the final loss\n        # L(w) = (1/N) * ||w - w_star||^2\n        final_loss = (1.0 / N) * np.linalg.norm(w - w_star)**2\n        \n        # Determine if a breakdown occurred based on the threshold tau\n        breakdown = final_loss > tau\n        results.append(breakdown)\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a Python list of booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2373947"}]}