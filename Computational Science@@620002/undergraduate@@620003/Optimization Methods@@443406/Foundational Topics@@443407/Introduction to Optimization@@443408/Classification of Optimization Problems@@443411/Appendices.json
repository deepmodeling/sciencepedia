{"hands_on_practices": [{"introduction": "This first practice problem tackles a foundational task in optimization: the least-squares problem with bounds. By analyzing the structure of the objective function, $\\|Ax - b\\|_2^2$, you will see how it fits the definition of a Quadratic Program (QP). This exercise reinforces the critical link between the properties of the data matrix $A$, the Hessian of the objective, and the convexity of the problem itselfâ€”a key determinant of its solvability.", "problem": "Consider the optimization problem\nminimize over $x \\in \\mathbb{R}^n$: $f(x) = \\lVert A x - b \\rVert_2^2$ subject to $l \\le x \\le u$,\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $l, u \\in \\mathbb{R}^n$ with $l_i \\le u_i$ for all $i \\in \\{1,\\dots,n\\}$. Using only the core definitions that a Quadratic Program (QP) is an optimization problem with a quadratic objective and linear constraints, that a Linear Program (LP) has a linear objective and linear constraints, that a Quadratically Constrained Quadratic Program (QCQP) has at least one quadratic constraint, and that for a twice continuously differentiable function $f$, convexity corresponds to a positive semidefinite Hessian and strict convexity corresponds to a positive definite Hessian on the domain, classify the problem and determine how strict convexity of $f$ depends on $A$. Select the single correct statement.\n\nA. The problem is a convex Quadratic Program (QP) because the objective is quadratic and the box constraints are linear; the objective $f$ is strictly convex if and only if $A$ has full column rank, equivalently $A^\\top A \\succ 0$, and otherwise $f$ is merely convex.\n\nB. The problem is a nonconvex Quadratic Program because $A$ may be rectangular; the objective $f$ is strictly convex if $A$ has full row rank.\n\nC. The problem is a Linear Program (LP) because the constraints are linear; the objective $f$ is never strictly convex regardless of $A$.\n\nD. The problem is a convex Quadratically Constrained Quadratic Program (QCQP) because the norm involves a quadratic expression; the objective $f$ is strictly convex for any nonzero $A$.\n\nE. The problem is a convex Quadratic Program (QP); the objective $f$ is strictly convex if and only if $A$ is square and invertible.", "solution": "The problem statement is a valid, well-posed question in optimization theory. It asks to classify the bounded least-squares problem and determine the condition for strict convexity.\n\n### Step-by-Step Derivation\n\n1.  **Analyze the Objective Function**: The objective function is $f(x) = \\lVert A x - b \\rVert_2^2$. We can expand this expression:\n    $$ f(x) = (Ax - b)^\\top (Ax - b) = x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b $$\n    This is a quadratic function of the variable $x \\in \\mathbb{R}^n$, in the form $x^\\top Q x + c^\\top x + d$ with $Q = A^\\top A$, $c = -2A^\\top b$, and $d = b^\\top b$.\n\n2.  **Analyze the Constraints**: The constraints are given by $l \\le x \\le u$. These are known as box constraints and are a set of $2n$ linear inequalities: $l_i \\le x_i$ and $x_i \\le u_i$ for $i \\in \\{1, \\dots, n\\}$. These are linear constraints.\n\n3.  **Classify the Problem**: According to the provided definitions, a problem with a quadratic objective function and linear constraints is a Quadratic Program (QP). Therefore, this problem is a QP.\n\n4.  **Determine Convexity**: For a twice-differentiable function, convexity is determined by its Hessian matrix, $\\nabla^2 f(x)$. First, we find the gradient:\n    $$ \\nabla f(x) = 2 A^\\top A x - 2 A^\\top b = 2A^\\top(Ax-b) $$\n    The Hessian is the Jacobian of the gradient:\n    $$ \\nabla^2 f(x) = 2 A^\\top A $$\n    A function is convex if its Hessian is positive semidefinite (PSD). For any vector $v \\in \\mathbb{R}^n$, we have:\n    $$ v^\\top (2 A^\\top A) v = 2 (Av)^\\top (Av) = 2 \\lVert Av \\rVert_2^2 \\ge 0 $$\n    Since the result is always non-negative, the Hessian $2 A^\\top A$ is always PSD. This means the objective function $f(x)$ is always convex, and the problem is a **convex QP**.\n\n5.  **Determine Strict Convexity**: A function is strictly convex if its Hessian is positive definite (PD), meaning $v^\\top H v > 0$ for all non-zero vectors $v$. From our previous calculation, we need $2 \\lVert Av \\rVert_2^2 > 0$ for all $v \\neq 0$. This is equivalent to requiring that $Av \\neq 0$ for all $v \\neq 0$. This condition means that the null space of $A$ contains only the zero vector, which is the definition of $A$ having linearly independent columns, i.e., having **full column rank**. The condition that the Hessian $2A^\\top A$ is positive definite ($A^\\top A \\succ 0$) is equivalent to $A$ having full column rank. If $A$ does not have full column rank, the function is convex but not strictly so.\n\n### Analysis of Options\n\n*   **A**: This statement correctly identifies the problem as a convex QP. It also correctly states that the objective is strictly convex if and only if $A$ has full column rank (equivalent to $A^\\top A \\succ 0$), and is otherwise merely convex. This matches our derivation perfectly.\n*   **B**: This is incorrect. The problem is always a convex QP. The condition for strict convexity is full *column* rank, not full *row* rank.\n*   **C**: This is incorrect. The objective is quadratic, so it is not an LP. The objective can be strictly convex.\n*   **D**: This is incorrect. The constraints are linear, not quadratic, so it is not a QCQP. The condition for strict convexity is also incorrect; a nonzero matrix $A$ without full column rank will not yield a strictly convex objective.\n*   **E**: This is incorrect. While it correctly identifies the problem as a convex QP, the condition for strict convexity (\"square and invertible\") is sufficient but not necessary. A tall matrix ($m > n$) can have full column rank.\n\nTherefore, option A is the only correct and complete statement.", "answer": "$$\\boxed{A}$$", "id": "3108413"}, {"introduction": "Not all problems fit neatly into a standard form at first glance. This exercise introduces the powerful technique of reformulation by tackling the minimization of the infinity norm, $\\|Ax - b\\|_{\\infty}$, a non-differentiable objective. You will learn how to use an auxiliary variable to transform this \"minimax\" problem into an equivalent Linear Program (LP), a classic trick that makes a seemingly difficult problem computationally tractable.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$ be given, and consider the unconstrained regression problem of minimizing the infinity norm of the residual,\n$$\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{\\infty}.$$\nUsing only the definition of the infinity norm and the definition of a Linear Programming (LP) problem (linear objective and linear constraints), determine the correct classification of this problem and select the formulation that is equivalent to the original problem. You may introduce auxiliary variables where appropriate. Let $\\mathbf{1} \\in \\mathbb{R}^{m}$ denote the vector of all ones, and interpret vector inequalities componentwise.\n\nWhich option is correct?\n\nA. Linear Programming (LP): Introduce a scalar $t \\in \\mathbb{R}$ and solve\n$$\\min_{x \\in \\mathbb{R}^{n}, \\ t \\in \\mathbb{R}} \\ t \\quad \\text{subject to} \\quad -t \\,\\mathbf{1} \\le A x - b \\le t \\,\\mathbf{1}, \\quad t \\ge 0.$$\n\nB. Quadratic Programming (QP): Replace the objective by the squared two-norm and solve\n$$\\min_{x \\in \\mathbb{R}^{n}} \\ \\tfrac{1}{2}\\|A x - b\\|_{2}^{2},$$\nbecause minimizing $\\|A x - b\\|_{\\infty}$ is equivalent to minimizing $\\|A x - b\\|_{2}$ when $A$ has full column rank.\n\nC. Second-Order Cone Program (SOCP): Introduce a scalar $t \\in \\mathbb{R}$ and solve\n$$\\min_{x \\in \\mathbb{R}^{n}, \\ t \\in \\mathbb{R}} \\ t \\quad \\text{subject to} \\quad \\|A x - b\\|_{2} \\le t,$$\nbecause the infinity norm can be represented by a second-order cone.\n\nD. Linear Programming (LP) with per-component slacks: Introduce $t \\in \\mathbb{R}^{m}$ and solve\n$$\\min_{x \\in \\mathbb{R}^{n}, \\ t \\in \\mathbb{R}^{m}} \\ \\mathbf{1}^{\\top} t \\quad \\text{subject to} \\quad -t \\le A x - b \\le t, \\quad t \\ge 0,$$\nbecause minimizing $\\mathbf{1}^{\\top} t$ enforces uniform control of the residual components.\n\nE. Nonlinear and nonconvex program: The absolute values inside $\\|A x - b\\|_{\\infty}$ make the problem nonconvex and it cannot be represented as an LP even with auxiliary variables.", "solution": "The problem statement is valid and asks to classify and reformulate the $\\ell_\\infty$-norm minimization problem. This is a classic problem in optimization.\n\n### Step-by-Step Derivation\n\n1.  **Understand the Objective**: The problem is to minimize $\\|A x - b\\|_{\\infty}$. The infinity norm of a vector $v \\in \\mathbb{R}^m$ is defined as $\\|v\\|_{\\infty} = \\max_{i=1,\\dots,m} |v_i|$. So, the problem is:\n    $$ \\min_{x \\in \\mathbb{R}^{n}} \\max_{i=1,\\dots,m} |(Ax - b)_i| $$\n    The objective function is non-linear and non-differentiable.\n\n2.  **Epigraph Reformulation**: We can transform this \"minimax\" problem into a constrained problem by introducing a single auxiliary scalar variable, $t$. Minimizing $\\max(\\dots)$ is equivalent to minimizing an upper bound $t$ on that expression. This gives:\n    $$ \\begin{array}{ll}\n    \\min_{x \\in \\mathbb{R}^{n}, t \\in \\mathbb{R}} & t \\\\\n    \\text{subject to} & \\|A x - b\\|_{\\infty} \\le t\n    \\end{array} $$\n\n3.  **Unpack the Constraint**: The constraint $\\|A x - b\\|_{\\infty} \\le t$ means that the maximum absolute value of any component of the residual vector $Ax - b$ must be less than or equal to $t$. This is equivalent to requiring this condition for every component:\n    $$ |(Ax - b)_i| \\le t \\quad \\text{for all } i = 1, \\dots, m $$\n\n4.  **Linearize the Constraint**: An inequality involving an absolute value, $|z| \\le c$ (for non-negative $c$), is equivalent to the pair of linear inequalities $-c \\le z \\le c$. Applying this to our set of constraints, we get:\n    $$ -t \\le (Ax - b)_i \\le t \\quad \\text{for all } i = 1, \\dots, m $$\n    This can be written compactly using vector notation, where $\\mathbf{1}$ is a vector of all ones:\n    $$ -t \\mathbf{1} \\le A x - b \\le t \\mathbf{1} $$\n\n5.  **Final Formulation and Classification**: The original problem is equivalent to the following:\n    $$\n    \\begin{array}{ll}\n    \\min_{x \\in \\mathbb{R}^{n}, t \\in \\mathbb{R}} & t \\\\\n    \\text{subject to} & Ax - b \\le t \\mathbf{1} \\\\\n    & -(Ax - b) \\le t \\mathbf{1}\n    \\end{array}\n    $$\n    The objective function is linear in the optimization variables $(x, t)$. The constraints are also linear inequalities in $x$ and $t$. Therefore, the problem is a **Linear Program (LP)**. Note that the constraint $t \\ge 0$ is often added; it is redundant because $t$ is constrained to be greater than or equal to a norm, which is non-negative, but its inclusion does not change the problem.\n\n### Analysis of Options\n\n*   **A**: This correctly identifies the problem as an LP and provides the exact formulation derived above.\n*   **B**: This is incorrect. It suggests solving a different problem (least squares, or $\\ell_2$-norm minimization), which gives a different solution. Minimizing the $\\ell_\\infty$-norm is not equivalent to minimizing the $\\ell_2$-norm.\n*   **C**: This is incorrect. The formulation provided, with the constraint $\\|A x - b\\|_{2} \\le t$, is the correct way to reformulate the minimization of the $\\ell_2$-norm, not the $\\ell_\\infty$-norm.\n*   **D**: This is incorrect. This formulation introduces a vector of slack variables, $t \\in \\mathbb{R}^m$, and minimizes their sum, $\\sum t_i$. This is the standard reformulation for minimizing the $\\ell_1$-norm, $\\|Ax-b\\|_1 = \\sum |(Ax-b)_i|$, not the $\\ell_\\infty$-norm.\n*   **E**: This is incorrect. The objective function $\\|Ax-b\\|_\\infty$ is convex (as it is a norm composed with an affine function), not nonconvex. Furthermore, as shown in A, it can be reformulated as an LP.", "answer": "$$\\boxed{A}$$", "id": "3108400"}, {"introduction": "Why is the distinction between convex and non-convex problems so important? This final practice problem illustrates the divide by comparing two seemingly similar tasks: finding a solution to $Bx=c$ with the smallest Euclidean norm ($\\|x\\|_2$) versus one with the fewest non-zero elements ($\\|x\\|_0$). You will see how one problem [@problem_id:3108380] is a well-behaved convex problem (a Second-Order Cone Program), while the other is a non-convex, combinatorial problem that is generally intractable, highlighting why classification is a crucial first step in any optimization workflow.", "problem": "Consider two optimization problems defined on the Euclidean space $\\mathbb{R}^{n}$, with a given matrix $B \\in \\mathbb{R}^{m \\times n}$ and vector $c \\in \\mathbb{R}^{m}$:\n- $\\mathcal{P}_{2}$: minimize $f(x) = \\|x\\|_{2}$ subject to $Bx = c$.\n- $\\mathcal{P}_{0}$: minimize $g(x) = \\|x\\|_{0}$ subject to $Bx = c$, where $\\|x\\|_{0}$ denotes the number of nonzero components of $x$.\n\nUsing only foundational definitions and facts about convexity, affine sets, norms, and standard conic models, decide which of the following statements are true. Acronyms are spelled out on first appearance: Second-Order Cone Programming (SOCP), Linear Programming (LP), Mixed-Integer Linear Programming (MILP), and Non-deterministic Polynomial time (NP).\n\nChoose all that apply.\n\nA. $\\mathcal{P}_{2}$ is a convex optimization problem, and it can be equivalently reformulated as a Second-Order Cone Programming (SOCP) problem by introducing a scalar $t \\in \\mathbb{R}$ and using the second-order cone constraint $\\|x\\|_{2} \\le t$, then minimizing $t$ subject to $Bx = c$.\n\nB. $\\mathcal{P}_{0}$ is a convex optimization problem because $\\|\\cdot\\|_{0}$ is a norm that satisfies the triangle inequality, and therefore it can be solved as a Linear Programming (LP) problem.\n\nC. Since the feasible set $\\{x \\in \\mathbb{R}^{n} : Bx = c\\}$ is convex, any objective minimized over it yields a convex optimization problem.\n\nD. $\\mathcal{P}_{2}$ can always be solved as a Linear Programming (LP) problem without any nonlinearity because the Euclidean norm is linear on $\\mathbb{R}^{n}$.\n\nE. $\\mathcal{P}_{0}$ can be encoded as a Mixed-Integer Linear Programming (MILP) problem by introducing binary variables to count nonzero entries, and in general it is combinatorial and nonconvex (NP-hard in the worst case).", "solution": "The problem statement is valid. It presents two fundamental optimization problems and asks to evaluate five statements about their classification and properties based on foundational principles.\n\n### Analysis of the Options\n\n**A. $\\mathcal{P}_{2}$ is a convex optimization problem, and it can be equivalently reformulated as a Second-Order Cone Programming (SOCP) problem...**\n\n1.  **Convexity of $\\mathcal{P}_{2}$**: An optimization problem is convex if its objective function is convex and its feasible set is convex.\n    *   The objective function is $f(x) = \\|x\\|_{2}$. The Euclidean norm (and any norm) is a convex function.\n    *   The feasible set is $S = \\{x \\in \\mathbb{R}^{n} : Bx = c\\}$. This is an affine set, which is a convex set.\n    *   Since both conditions are met, $\\mathcal{P}_{2}$ is a convex optimization problem.\n2.  **SOCP Reformulation**: The problem $\\min_{x} \\|x\\|_2$ subject to $Bx=c$ can be rewritten using an auxiliary variable $t \\in \\mathbb{R}$. This leads to the epigraph form: $\\min_{x,t} t$ subject to $\\|x\\|_2 \\le t$ and $Bx=c$. This is the standard definition of a Second-Order Cone Programming (SOCP) problem, as it has a linear objective and constraints that are linear equalities and a second-order cone inequality.\n**Verdict: Correct.**\n\n**B. $\\mathcal{P}_{0}$ is a convex optimization problem because $\\|\\cdot\\|_{0}$ is a norm...**\n\n1.  **Is $\\|\\cdot\\|_{0}$ a norm?**: The function $g(x) = \\|x\\|_{0}$ is not a norm. A key property of a norm is absolute homogeneity: $\\|\\alpha x\\| = |\\alpha| \\|x\\|$. Let $x = [1, 1, 0, \\dots]^T$ and $\\alpha=2$. Then $\\|x\\|_0 = 2$, but $\\|\\alpha x\\|_0 = \\|[2, 2, 0, \\dots]^T\\|_0 = 2$. Since $\\|\\alpha x\\|_0 \\ne |\\alpha| \\|x\\|_0$ (i.e., $2 \\ne 2 \\cdot 2 = 4$), it is not a norm. It is properly called a pseudo-norm.\n2.  **Is $\\|\\cdot\\|_{0}$ a convex function?**: A function $g$ is convex if $g(\\alpha x_1 + (1-\\alpha)x_2) \\le \\alpha g(x_1) + (1-\\alpha)g(x_2)$. Let $x_1=[1,0]^T$ and $x_2=[0,1]^T$. Let $\\alpha = 0.5$. Then $g(x_1)=1$ and $g(x_2)=1$. The right side of the inequality is $0.5(1) + 0.5(1) = 1$. The left side is $g(0.5x_1 + 0.5x_2) = g([0.5, 0.5]^T) = 2$. Since $2 \\not\\le 1$, the function is not convex.\n3.  Therefore, $\\mathcal{P}_{0}$ is not a convex optimization problem and cannot be an LP.\n**Verdict: Incorrect.**\n\n**C. Since the feasible set $\\{x \\in \\mathbb{R}^{n} : Bx = c\\}$ is convex, any objective minimized over it yields a convex optimization problem.**\n\nThis is a fundamental misunderstanding of the definition. For an optimization problem to be convex, **both** the feasible set and the objective function must be convex. A convex feasible set is a necessary but not sufficient condition. Problem $\\mathcal{P}_{0}$ is a perfect counterexample, as it has a convex feasible set but a non-convex objective.\n**Verdict: Incorrect.**\n\n**D. $\\mathcal{P}_{2}$ can always be solved as a Linear Programming (LP) problem... because the Euclidean norm is linear.**\n\nThe premise is false. The Euclidean norm $f(x) = \\|x\\|_2$ is not a linear function. For example, $\\|[1,0]^T + [0,1]^T\\|_2 = \\sqrt{2}$, which is not equal to $\\|[1,0]^T\\|_2 + \\|[0,1]^T\\|_2 = 1+1=2$. Since the objective is non-linear, the problem cannot be an LP, which requires a linear objective. As established in option A, it is an SOCP.\n**Verdict: Incorrect.**\n\n**E. $\\mathcal{P}_{0}$ can be encoded as a Mixed-Integer Linear Programming (MILP) problem... and in general it is combinatorial and nonconvex (NP-hard in the worst case).**\n\n1.  **MILP Formulation**: The non-convex problem $\\mathcal{P}_{0}$ can be exactly formulated as an MILP. We introduce a binary variable $z_i \\in \\{0, 1\\}$ for each component $x_i$, where $z_i=1$ if $x_i \\ne 0$. The objective becomes $\\min \\sum z_i$. This logic is enforced using \"big-M\" constraints: $-Mz_i \\le x_i \\le Mz_i$, where $M$ is a sufficiently large constant. The resulting problem has a linear objective, linear constraints, and both continuous ($x_i$) and integer ($z_i$) variables, which is the definition of an MILP.\n2.  **Complexity**: The problem of finding the sparsest solution to a system of equations ($Bx=c$) is a classic combinatorial optimization problem. It is known to be NP-hard, meaning it is computationally intractable in the worst case. The statement accurately describes the nature of $\\mathcal{P}_0$.\n**Verdict: Correct.**", "answer": "$$\\boxed{AE}$$", "id": "3108380"}]}