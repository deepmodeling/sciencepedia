{"hands_on_practices": [{"introduction": "To begin our practical exploration, we will investigate how different optimization algorithms respond to a simple, uniform scaling of the problem variables. In this exercise, you will apply gradient descent, the conjugate gradient method, and Newton's method to a quadratic problem under various scaling factors. This practice is valuable because it reveals a crucial lesson: sensitivity to scaling is not a universal property but depends heavily on an algorithm's underlying mechanics, highlighting the importance of choosing the right tool for the job. [@problem_id:3158924]", "problem": "You will study how uniform variable scaling affects the numerical behavior of first- and second-order optimization methods on a strictly convex quadratic objective. Consider the quadratic function defined by $f(\\boldsymbol{x}) = \\tfrac{1}{2}\\,\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$, where $\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite and $\\boldsymbol{b}\\in\\mathbb{R}^{n}$. Define a family of scaled problems by the change of variables $\\boldsymbol{x} = s\\,\\boldsymbol{y}$ with $s = 10^{k}$ for integer $k$, and let $g_{k}(\\boldsymbol{y}) = f(s\\,\\boldsymbol{y})$. You will implement three methods on $g_{k}$: gradient descent (GD) with a fixed step size, the Conjugate Gradient method (CG) for symmetric positive definite systems applied to the quadratic form, and Newton's method with unit step. Your program must empirically count iterations required to reach a prescribed stationarity tolerance measured in the original variable $\\boldsymbol{x}$.\n\nFundamental base and definitions to be used:\n- A symmetric positive definite matrix has real positive eigenvalues; the gradient of $f$ is $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}$, and the Hessian is $\\nabla^{2} f(\\boldsymbol{x}) = \\boldsymbol{Q}$.\n- The spectral condition number in the matrix $2$-norm is $\\kappa_{2}(\\boldsymbol{Q}) = \\lambda_{\\max}(\\boldsymbol{Q})/\\lambda_{\\min}(\\boldsymbol{Q})$ where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues.\n- For the scaled problem with $\\boldsymbol{x} = s\\,\\boldsymbol{y}$, the gradient in $\\boldsymbol{y}$ is $\\nabla g_{k}(\\boldsymbol{y}) = s\\,\\nabla f(s\\,\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}\\boldsymbol{y} - s\\,\\boldsymbol{b}$ and the Hessian is $\\nabla^{2} g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}$.\n\nImplementation requirements:\n- Use dimension $n=10$ with $\\boldsymbol{Q}$ diagonal, having entries $\\lambda_{i} = 10^{4i/9}$ for $i = 0,1,2,\\dots,9$ along the diagonal. Set $\\boldsymbol{b} = \\boldsymbol{1}$ (the vector of all ones).\n- Use the scaling factors $s_{k} = 10^{k}$ for the test suite $k \\in \\{-6,-3,0,3,6\\}$.\n- Initialize all methods at $\\boldsymbol{y}_{0} = \\boldsymbol{0}$. For each method and each $k$, iterate until the original gradient norm satisfies $\\|\\nabla f(\\boldsymbol{x})\\|_{2} \\le \\varepsilon$ with $\\varepsilon = 10^{-8}$, where $\\boldsymbol{x} = s_{k}\\,\\boldsymbol{y}$. All norms are Euclidean norms.\n- Gradient Descent (GD): Apply to $g_{k}$ with a fixed step size $\\alpha = 10^{-4}$, i.e., $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha\\,\\nabla g_{k}(\\boldsymbol{y}_{t})$. Set a cap of $N_{\\max}^{\\mathrm{GD}} = 20000$ iterations. To avoid numerical overflow during divergence, if at any point $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} > 10^{12}\\,\\|\\nabla f(\\boldsymbol{x}_{0})\\|_{2}$ or becomes not finite, terminate GD early and report $N_{\\max}^{\\mathrm{GD}}$ as the iteration count for that $k$.\n- Conjugate Gradient (CG): Apply the standard linear Conjugate Gradient method to the normal equations for $g_{k}$, i.e., solve $s_{k}^{2}\\boldsymbol{Q}\\boldsymbol{y} = s_{k}\\boldsymbol{b}$ starting from $\\boldsymbol{y}_{0} = \\boldsymbol{0}$. After each CG iteration, check the original gradient norm criterion by noting that the residual $\\boldsymbol{r}_{t} = s_{k}\\boldsymbol{b} - s_{k}^{2}\\boldsymbol{Q}\\boldsymbol{y}_{t}$ satisfies $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} = \\|\\boldsymbol{r}_{t}\\|_{2}/s_{k}$ with $\\boldsymbol{x}_{t} = s_{k}\\boldsymbol{y}_{t}$. Use a maximum of $N_{\\max}^{\\mathrm{CG}} = n$ iterations.\n- Newton's method: Apply to $g_{k}$ with unit step size, i.e., $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\left(\\nabla^{2} g_{k}(\\boldsymbol{y}_{t})\\right)^{-1}\\nabla g_{k}(\\boldsymbol{y}_{t})$. On a strictly convex quadratic, this converges in one iteration from any starting point. You must implement one Newton iteration from $\\boldsymbol{y}_{0} = \\boldsymbol{0}$ and count $1$ iteration for each $k$.\n\nTest suite and output:\n- Test suite is the set of scalings $k \\in \\{-6,-3,0,3,6\\}$ with the fixed parameters $n=10$, $\\varepsilon=10^{-8}$, $\\alpha=10^{-4}$, $N_{\\max}^{\\mathrm{GD}}=20000$, and $N_{\\max}^{\\mathrm{CG}}=n$.\n- For each $k$ in the given order, compute the triple of iteration counts $\\left(N_{\\mathrm{GD}}(k), N_{\\mathrm{CG}}(k), N_{\\mathrm{Newton}}(k)\\right)$, where each entry is an integer. If GD terminates early due to the divergence guard or fails to reach the tolerance within the cap, report $N_{\\mathrm{GD}}(k)=N_{\\max}^{\\mathrm{GD}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list of the triples, enclosed in square brackets. The format must be a valid Python list literal of lists, for example, $[\\,[3,2,1],[4,2,1]\\,]$ for two test cases, but with the actual computed integers for this problem and with five triples in the specified order $k \\in \\{-6,-3,0,3,6\\}$.\n\nYour implementation must be a complete, runnable program that performs all computations and prints exactly one line in the specified format, with no extra text. No user input is required or permitted.", "solution": "The user's request is to analyze the effect of uniform variable scaling on the performance of three standard optimization algorithms: Gradient Descent (GD), Conjugate Gradient (CG), and Newton's method. The analysis is performed on a strictly convex quadratic objective function.\n\n### Problem Validation\nThe problem statement is validated according to the specified criteria.\n\n**Step 1: Extract Givens**\n-   Objective function: $f(\\boldsymbol{x}) = \\tfrac{1}{2}\\,\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$, where $\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite (SPD) and $\\boldsymbol{b}\\in\\mathbb{R}^{n}$.\n-   Scaled variables: $\\boldsymbol{x} = s\\,\\boldsymbol{y}$ with $s = 10^{k}$ for integer $k$.\n-   Scaled objective function: $g_{k}(\\boldsymbol{y}) = f(s\\,\\boldsymbol{y})$.\n-   Gradient and Hessian of $f$: $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}$, $\\nabla^{2} f(\\boldsymbol{x}) = \\boldsymbol{Q}$.\n-   Gradient and Hessian of $g_k$: $\\nabla g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}\\boldsymbol{y} - s\\,\\boldsymbol{b}$, $\\nabla^{2} g_{k}(\\boldsymbol{y}) = s^{2}\\boldsymbol{Q}$.\n-   Parameters: $n=10$, $\\boldsymbol{Q} = \\mathrm{diag}(10^{4i/9})$ for $i \\in \\{0, \\dots, 9\\}$, $\\boldsymbol{b} = \\boldsymbol{1}$.\n-   Test suite: $k \\in \\{-6, -3, 0, 3, 6\\}$.\n-   Initial point: $\\boldsymbol{y}_{0} = \\boldsymbol{0}$.\n-   Stopping criterion: $\\|\\nabla f(\\boldsymbol{x})\\|_{2} \\le \\varepsilon$ with $\\varepsilon = 10^{-8}$.\n-   Gradient Descent (GD): update $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha\\,\\nabla g_{k}(\\boldsymbol{y}_{t})$ with $\\alpha = 10^{-4}$. Max iterations $N_{\\max}^{\\mathrm{GD}} = 20000$. Divergence check $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} > 10^{12}\\,\\|\\nabla f(\\boldsymbol{x}_{0})\\|_{2}$ or non-finite norm results in reporting $N_{\\max}^{\\mathrm{GD}}$.\n-   Conjugate Gradient (CG): applied to solve $s_{k}^{2}\\boldsymbol{Q}\\boldsymbol{y} = s_{k}\\boldsymbol{b}$. Max iterations $N_{\\max}^{\\mathrm{CG}} = n = 10$. Stopping criterion on original gradient norm $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} = \\|\\boldsymbol{r}_{t}\\|_{2}/s_{k} \\le \\varepsilon$.\n-   Newton's Method: one iteration from $\\boldsymbol{y}_{0} = \\boldsymbol{0}$ is required, counting as $1$ iteration.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the established theory of numerical optimization. It is well-posed, with all parameters and conditions explicitly defined to ensure a unique, computable result. The language is objective and formal. The problem is a standard exercise in analyzing the numerical properties of optimization algorithms and is entirely self-contained and consistent. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A reasoned solution is provided below.\n\n### Principle-Based Solution\n\nThe problem investigates the effect of a scaling transformation $\\boldsymbol{x} = s\\boldsymbol{y}$ on the convergence of optimization algorithms. The original objective is $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$. The scaled objective, in terms of $\\boldsymbol{y}$, is $g_k(\\boldsymbol{y}) = \\frac{s^2}{2}\\boldsymbol{y}^{\\top}\\boldsymbol{Q}\\boldsymbol{y} - s\\boldsymbol{b}^{\\top}\\boldsymbol{y}$. The Hessian of this scaled problem is $\\nabla^2 g_k(\\boldsymbol{y}) = s^2\\boldsymbol{Q}$. The behavior of the algorithms depends critically on the properties of this scaled Hessian.\n\nThe specified matrix $\\boldsymbol{Q}$ is diagonal with eigenvalues $\\lambda_i = 10^{4i/9}$ for $i=0, \\dots, 9$.\nThe smallest eigenvalue is $\\lambda_{\\min}(\\boldsymbol{Q}) = 10^{0} = 1$.\nThe largest eigenvalue is $\\lambda_{\\max}(\\boldsymbol{Q}) = 10^{4\\cdot9/9} = 10^4$.\nThe condition number of the original Hessian is $\\kappa_2(\\boldsymbol{Q}) = \\lambda_{\\max}(\\boldsymbol{Q}) / \\lambda_{\\min}(\\boldsymbol{Q}) = 10^4$.\n\n**1. Gradient Descent (GD)**\nThe GD update rule for the scaled problem is $\\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha \\nabla g_k(\\boldsymbol{y}_t)$. For a quadratic function, this is a linear iteration:\n$$ \\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - \\alpha (s^2\\boldsymbol{Q}\\boldsymbol{y}_t - s\\boldsymbol{b}) = (\\boldsymbol{I} - \\alpha s^2\\boldsymbol{Q})\\boldsymbol{y}_t + \\alpha s\\boldsymbol{b} $$\nThis iteration converges if and only if the spectral radius of the iteration matrix $\\boldsymbol{I} - \\alpha s^2\\boldsymbol{Q}$ is less than $1$. The eigenvalues of this matrix are $1 - \\alpha s^2 \\lambda_i(\\boldsymbol{Q})$. The condition is $|1 - \\alpha s^2 \\lambda_i(\\boldsymbol{Q})| < 1$ for all $i$, which is equivalent to $0 < \\alpha s^2 \\lambda_i(\\boldsymbol{Q}) < 2$. Since $\\alpha > 0$ and $\\lambda_i > 0$, we only need to check the upper bound for the largest eigenvalue:\n$$ \\alpha s^2 \\lambda_{\\max}(\\boldsymbol{Q}) < 2 $$\nGiven $\\alpha = 10^{-4}$ and $\\lambda_{\\max}(\\boldsymbol{Q}) = 10^4$, the condition becomes $10^{-4} \\cdot s^2 \\cdot 10^4 < 2$, which simplifies to $s^2 < 2$.\nLet's examine this for each $k$:\n-   $k \\in \\{-6,-3,0\\}$: $s \\in \\{10^{-6}, 10^{-3}, 1\\}$. In these cases, $s^2 \\in \\{10^{-12}, 10^{-6}, 1\\}$, all of which are less than $2$. GD is guaranteed to converge. However, the convergence rate is determined by $\\rho(\\boldsymbol{I} - \\alpha s^2\\boldsymbol{Q}) = \\max_i |1 - \\alpha s^2 \\lambda_i|$. For small $s$, $\\alpha s^2$ is very small, making the spectral radius extremely close to $1$. This implies exceedingly slow convergence. Given the iteration limit of $N_{\\max}^{\\mathrm{GD}}=20000$, it is expected that the algorithm will not reach the tolerance $\\varepsilon=10^{-8}$ and will terminate at the iteration cap.\n-   $k \\in \\{3,6\\}$: $s \\in \\{10^3, 10^6\\}$. In these cases, $s^2 \\in \\{10^6, 10^{12}\\}$, which are both greater than $2$. The convergence condition is violated, and the GD iteration will diverge. The divergence guard, $\\|\\nabla f(\\boldsymbol{x}_{t})\\|_{2} > 10^{12}\\,\\|\\nabla f(\\boldsymbol{x}_{0})\\|_{2}$, will be triggered, and the reported iteration count will be $N_{\\max}^{\\mathrm{GD}}$.\nTherefore, for all values of $k$, the expected iteration count for GD is $N_{\\max}^{\\mathrm{GD}} = 20000$.\n\n**2. Conjugate Gradient (CG) Method**\nThe CG method is applied to solve the linear system arising from the optimality condition $\\nabla g_k(\\boldsymbol{y}) = \\boldsymbol{0}$, which is $s^2\\boldsymbol{Q}\\boldsymbol{y} = s\\boldsymbol{b}$. The system matrix is $A_k = s^2\\boldsymbol{Q}$. The convergence rate of CG depends on the condition number of the system matrix, $\\kappa_2(A_k)$.\n$$ \\kappa_2(A_k) = \\kappa_2(s^2\\boldsymbol{Q}) = \\frac{\\lambda_{\\max}(s^2\\boldsymbol{Q})}{\\lambda_{\\min}(s^2\\boldsymbol{Q})} = \\frac{s^2\\lambda_{\\max}(\\boldsymbol{Q})}{s^2\\lambda_{\\min}(\\boldsymbol{Q})} = \\frac{\\lambda_{\\max}(\\boldsymbol{Q})}{\\lambda_{\\min}(\\boldsymbol{Q})} = \\kappa_2(\\boldsymbol{Q}) $$\nThe condition number is invariant to the scaling factor $s$. Furthermore, the stopping criterion is $\\|\\nabla f(\\boldsymbol{x}_t)\\|_2 \\le \\varepsilon$. The residual of the linear system for CG is $\\boldsymbol{r}_t = s\\boldsymbol{b} - s^2\\boldsymbol{Q}\\boldsymbol{y}_t$. The gradient of the original function is $\\nabla f(\\boldsymbol{x}_t) = \\boldsymbol{Q}\\boldsymbol{x}_t - \\boldsymbol{b} = \\boldsymbol{Q}(s\\boldsymbol{y}_t) - \\boldsymbol{b} = s\\boldsymbol{Q}\\boldsymbol{y}_t - \\boldsymbol{b}$. We can see that $\\boldsymbol{r}_t = -s(s\\boldsymbol{Q}\\boldsymbol{y}_t - \\boldsymbol{b}) = -s\\nabla f(\\boldsymbol{x}_t)$. Thus, $\\|\\boldsymbol{r}_t\\|_2 = s\\|\\nabla f(\\boldsymbol{x}_t)\\|_2$.\nThe stopping criterion $\\|\\nabla f(\\boldsymbol{x}_t)\\|_2 \\le \\varepsilon$ is equivalent to $\\|\\boldsymbol{r}_t\\|_2/s \\le \\varepsilon$. Let's analyze the relative residual reduction. The initial residual is $\\boldsymbol{r}_0 = s\\boldsymbol{b} - A_k\\boldsymbol{y}_0 = s\\boldsymbol{b}$. The condition can be expressed as:\n$$ \\frac{\\|\\boldsymbol{r}_t\\|_2}{\\|\\boldsymbol{r}_0\\|_2} \\le \\frac{s\\varepsilon}{\\|s\\boldsymbol{b}\\|_2} = \\frac{s\\varepsilon}{s\\|\\boldsymbol{b}\\|_2} = \\frac{\\varepsilon}{\\|\\boldsymbol{b}\\|_2} $$\nSince both the condition number of the system matrix and the required relative reduction in the residual are independent of the scaling factor $s$, the number of CG iterations required to meet the criterion will be constant for all tested values of $k$. With $n=10$ distinct eigenvalues, CG is guaranteed to converge in at most $10$ iterations in exact arithmetic.\n\n**3. Newton's Method**\nNewton's method for minimizing $g_k(\\boldsymbol{y})$ uses the update rule:\n$$ \\boldsymbol{y}_{t+1} = \\boldsymbol{y}_{t} - (\\nabla^2 g_k(\\boldsymbol{y}_t))^{-1} \\nabla g_k(\\boldsymbol{y}_t) $$\nFor our quadratic problem, the Hessian $\\nabla^2 g_k(\\boldsymbol{y}) = s^2\\boldsymbol{Q}$ is constant. Starting from $\\boldsymbol{y}_0 = \\boldsymbol{0}$, the first iteration is:\n$$ \\boldsymbol{y}_1 = \\boldsymbol{y}_0 - (s^2\\boldsymbol{Q})^{-1} (s^2\\boldsymbol{Q}\\boldsymbol{y}_0 - s\\boldsymbol{b}) = \\boldsymbol{0} - (s^2\\boldsymbol{Q})^{-1} (-s\\boldsymbol{b}) = s^{-2}\\boldsymbol{Q}^{-1}s\\boldsymbol{b} = s^{-1}\\boldsymbol{Q}^{-1}\\boldsymbol{b} $$\nThe exact minimizer $\\boldsymbol{y}^*$ of $g_k(\\boldsymbol{y})$ is found by solving $\\nabla g_k(\\boldsymbol{y}^*) = s^2\\boldsymbol{Q}\\boldsymbol{y}^* - s\\boldsymbol{b} = \\boldsymbol{0}$, which gives $\\boldsymbol{y}^* = s^{-1}\\boldsymbol{Q}^{-1}\\boldsymbol{b}$.\nThus, Newton's method finds the exact solution in a single iteration from any starting point. This property is affine-invariant and thus unaffected by the scaling $s$. The iteration count is $1$ for all $k$, as stated in the problem.\n\nThis analysis shows the varying effects of scaling: it is detrimental to a poorly-tuned GD, irrelevant to CG (a scale-invariant method for this problem structure), and irrelevant to Newton's method (an affine-invariant method).", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for different scaling factors and reports\n    the number of iterations for Gradient Descent, Conjugate Gradient, and\n    Newton's method.\n    \"\"\"\n    # Problem Parameters\n    n = 10\n    epsilon = 1e-8\n    alpha_gd = 1e-4\n    N_max_GD = 20000\n    N_max_CG = n\n    k_values = [-6, -3, 0, 3, 6]\n\n    # Problem Setup: Q matrix and b vector\n    i = np.arange(n, dtype=float)\n    lambdas = 10.0**(4.0 * i / 9.0)\n    Q = np.diag(lambdas)\n    b = np.ones(n)\n\n    all_results = []\n    \n    # Pre-calculate initial gradient norm for GD divergence check\n    # x0 = 0, so grad_f(x0) = -b\n    grad_f_x0_norm = np.linalg.norm(b)\n\n    for k in k_values:\n        s = 10.0**k\n        iter_counts_for_k = []\n\n        # --- 1. Gradient Descent (GD) ---\n        y = np.zeros(n)\n        gd_iters = N_max_GD\n        divergence_threshold = 1e12 * grad_f_x0_norm\n\n        for t in range(N_max_GD):\n            x = s * y\n            grad_f = Q @ x - b\n            grad_f_norm = np.linalg.norm(grad_f)\n\n            if grad_f_norm <= epsilon:\n                gd_iters = t\n                break\n            \n            if grad_f_norm > divergence_threshold or not np.isfinite(grad_f_norm):\n                # As per instructions, report N_max_GD on divergence\n                gd_iters = N_max_GD\n                break\n            \n            # GD step on the scaled problem g_k(y)\n            grad_g = s*s * (Q @ y) - s * b\n            y = y - alpha_gd * grad_g\n        \n        iter_counts_for_k.append(gd_iters)\n\n        # --- 2. Conjugate Gradient (CG) ---\n        # Solve A*y = c where A = s^2*Q and c = s*b\n        A = (s**2) * Q\n        c = s * b\n        \n        y = np.zeros(n)\n        r = c - A @ y  # Initial residual r_0\n        p = r          # Initial search direction p_0\n        rs_old = np.dot(r, r)\n        \n        cg_iters = N_max_CG\n        \n        # Check an edge case where r_0 is already small, though not for y_0=0\n        if np.linalg.norm(r) / s <= epsilon:\n             cg_iters = 0\n        else:\n            for t in range(N_max_CG):\n                Ap = A @ p\n                alpha_cg = rs_old / np.dot(p, Ap)\n                \n                y = y + alpha_cg * p\n                r = r - alpha_cg * Ap\n                \n                # Check stopping criterion on the original gradient norm\n                # ||grad_f(x_t)||_2 = ||r_t||_2 / s\n                if np.linalg.norm(r) / s <= epsilon:\n                    cg_iters = t + 1\n                    break\n                \n                rs_new = np.dot(r, r)\n                # Update search direction\n                p = r + (rs_new / rs_old) * p\n                rs_old = rs_new\n\n        iter_counts_for_k.append(cg_iters)\n\n        # --- 3. Newton's Method ---\n        # Converges in 1 step for a quadratic function\n        newton_iters = 1\n        iter_counts_for_k.append(newton_iters)\n\n        all_results.append(iter_counts_for_k)\n\n    # Final print statement must match the specified format\n    print(f\"{all_results}\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3158924"}, {"introduction": "Building on our understanding that naive scaling can have unpredictable effects, we now turn to a more structured approach known as preconditioning. This exercise introduces Jacobi scaling, a popular technique that attempts to balance the problem by normalizing the diagonal elements of the Hessian matrix. By comparing the performance of gradient descent and coordinate descent, you will see how this 'smarter' scaling can be particularly beneficial for methods that operate along coordinate axes. [@problem_id:3158937]", "problem": "Consider the unconstrained quadratic optimization problem in dimension $3$ given by minimizing the function $f(\\boldsymbol{x}) = \\tfrac{1}{2}\\,\\boldsymbol{x}^{\\top}\\mathbf{A}\\,\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$, where $\\mathbf{A}\\in\\mathbb{R}^{3\\times 3}$ is symmetric positive definite and $\\boldsymbol{b}\\in\\mathbb{R}^{3}$. The exact solution $\\boldsymbol{x}^{\\star}$ is the unique vector satisfying $\\mathbf{A}\\,\\boldsymbol{x}^{\\star}=\\boldsymbol{b}$. You will compare two first-order optimization methods under variable rescalings (changes of variables) and quantify how scaling affects their convergence behavior, with particular attention to when coordinates are balanced. The two methods are: gradient descent (GD) and cyclic coordinate descent (CD). Define gradient descent (GD) as the method that repeatedly takes a step along the negative gradient with an exact line search at each iteration. Define cyclic coordinate descent (CD) as the method that repeatedly sweeps through coordinates $j=1,2,3$ in fixed order, and for each $j$ sets the $j$-th coordinate to the exact minimizer with all other coordinates fixed. Do not assume any specific shortcut formulas in your implementation; you must derive each step from core definitions: gradients for GD and one-dimensional minimization along a line for both GD and each coordinate update of CD.\n\nYou will examine the effects of rescaling variables via an invertible diagonal matrix $\\mathbf{S}\\in\\mathbb{R}^{3\\times 3}$, using the change of variables $\\boldsymbol{x}=\\mathbf{S}\\,\\boldsymbol{z}$. Under this change, the same objective can be written in the $\\boldsymbol{z}$-variables as $f(\\mathbf{S}\\boldsymbol{z})=\\tfrac{1}{2}\\,\\boldsymbol{z}^{\\top}\\mathbf{H}\\,\\boldsymbol{z}-\\boldsymbol{c}^{\\top}\\boldsymbol{z}$ up to an additive constant that does not affect minimization, where $\\mathbf{H}=\\mathbf{S}^{\\top}\\mathbf{A}\\mathbf{S}$ and $\\boldsymbol{c}=\\mathbf{S}^{\\top}\\boldsymbol{b}$. You will run both GD and CD in the $\\boldsymbol{z}$-coordinates, starting from $\\boldsymbol{z}_{0}=\\mathbf{S}^{-1}\\boldsymbol{x}_{0}$, and then report progress in the original variables by mapping back $\\boldsymbol{x}_{k}=\\mathbf{S}\\boldsymbol{z}_{k}$. The error metric is the suboptimality in the original objective, $f(\\boldsymbol{x}_{k})-f(\\boldsymbol{x}^{\\star})$.\n\nFundamental base you must use:\n- Core definition of the gradient $\\nabla f(\\boldsymbol{x})$ and the negative gradient direction for steepest descent in the Euclidean norm.\n- Exact line search along a given direction defined as minimizing a univariate quadratic function obtained by restricting $f$ to a line.\n- For CD, exact one-dimensional minimization with respect to a single coordinate holding the others fixed.\n\nConstruction of the test problem:\n- Let $\\mathbf{D}=\\operatorname{diag}(50,\\,5,\\,0.5)$.\n- Let the ZYX (yaw–pitch–roll) rotation matrix $\\mathbf{R}$ be $\\mathbf{R}=\\mathbf{R}_{z}(\\psi)\\,\\mathbf{R}_{y}(\\theta)\\,\\mathbf{R}_{x}(\\phi)$, with $\\phi=0.4$, $\\theta=0.7$, $\\psi=-0.5$ in radians. Here\n  $$\\mathbf{R}_{x}(\\phi)=\\begin{bmatrix}1&0&0\\\\0&\\cos\\phi&-\\sin\\phi\\\\0&\\sin\\phi&\\cos\\phi\\end{bmatrix},\\quad\n  \\mathbf{R}_{y}(\\theta)=\\begin{bmatrix}\\cos\\theta&0&\\sin\\theta\\\\0&1&0\\\\-\\sin\\theta&0&\\cos\\theta\\end{bmatrix},\\quad\n  \\mathbf{R}_{z}(\\psi)=\\begin{bmatrix}\\cos\\psi&-\\sin\\psi&0\\\\\\sin\\psi&\\cos\\psi&0\\\\0&0&1\\end{bmatrix}.$$\n- Define $\\mathbf{A}=\\mathbf{R}^{\\top}\\mathbf{D}\\,\\mathbf{R}$ and $\\boldsymbol{b}=[\\,1,\\,-2,\\,3\\,]^{\\top}$.\n- Define the initial point $\\boldsymbol{x}_{0}=[\\,0,\\,0,\\,0\\,]^{\\top}$.\n- Angles $\\phi$, $\\theta$, and $\\psi$ are specified in radians, and all trigonometric functions use radians.\n\nScaling matrices to test (test suite):\n- Case $1$: $\\mathbf{S}_{1}=\\operatorname{diag}(1,\\,1,\\,1)$.\n- Case $2$: $\\mathbf{S}_{2}=\\operatorname{diag}(100,\\,1,\\,0.01)$.\n- Case $3$: $\\mathbf{S}_{3}=\\operatorname{diag}\\big(1/\\sqrt{[\\mathbf{A}]_{11}},\\,1/\\sqrt{[\\mathbf{A}]_{22}},\\,1/\\sqrt{[\\mathbf{A}]_{33}}\\big)$, that is, the diagonal scaling that normalizes the diagonal of $\\mathbf{A}$ to unity (a Jacobi-style balancing).\n- Case $4$: $\\mathbf{S}_{4}=\\operatorname{diag}(10,\\,0.1,\\,1)$.\n\nStopping criterion and counting rules:\n- Define $f^{\\star}=f(\\boldsymbol{x}^{\\star})$ with $\\boldsymbol{x}^{\\star}$ solving $\\mathbf{A}\\boldsymbol{x}^{\\star}=\\boldsymbol{b}$.\n- For GD: perform iterations in $\\boldsymbol{z}$ with exact line search along the negative gradient until $f(\\boldsymbol{x}_{k})-f^{\\star}\\le 10^{-9}$, where $\\boldsymbol{x}_{k}=\\mathbf{S}\\boldsymbol{z}_{k}$, counting the number of GD iterations $I_{\\mathrm{GD}}$ as the number of line searches executed.\n- For CD: perform cyclic sweeps over coordinates $j=1,2,3$ in $\\boldsymbol{z}$, at each coordinate setting the exact minimizer holding other coordinates fixed, and after each completed sweep (epoch) check $f(\\boldsymbol{x}_{k})-f^{\\star}\\le 10^{-9}$ with $\\boldsymbol{x}_{k}=\\mathbf{S}\\boldsymbol{z}_{k}$. Count the number of epochs $E_{\\mathrm{CD}}$ as the number of completed full sweeps required to satisfy the stopping criterion.\n- Use a maximum of $10000$ iterations for GD and $10000$ epochs for CD to prevent infinite loops in pathological cases. If the stopping criterion is not met within these limits, use the counts at termination.\n\nRequired program output:\n- For each case $i\\in\\{1,2,3,4\\}$, compute the ratio $r_{i}=E_{\\mathrm{CD}}/I_{\\mathrm{GD}}$.\n- Your program should produce a single line of output containing $4$ floating-point numbers rounded to $6$ decimals, as a Python-style list in order $[\\,r_{1},r_{2},r_{3},r_{4}\\,]$.\n- No physical units are involved. Angles are in radians as stated above. Do not print any additional text.\n\nDesign intent and coverage:\n- Case $1$ is a baseline without rescaling.\n- Cases $2$ and $4$ introduce imbalanced coordinates that distort relative curvature across axes.\n- Case $3$ balances coordinates by normalizing the diagonal of $\\mathbf{A}$; cyclic coordinate descent is expected to benefit when coordinates are balanced.", "solution": "### Step 1: Problem Validation\n\n#### Givens Extraction\nThe problem provides the following data and definitions:\n1.  **Objective Function**: Minimize $f(\\boldsymbol{x}) = \\tfrac{1}{2}\\,\\boldsymbol{x}^{\\top}\\mathbf{A}\\,\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}$ for $\\boldsymbol{x} \\in \\mathbb{R}^3$.\n2.  **Matrix and Vector Definitions**:\n    -   $\\mathbf{A} \\in \\mathbb{R}^{3\\times 3}$ is a symmetric positive definite matrix.\n    -   $\\boldsymbol{b} \\in \\mathbb{R}^3$.\n    -   Exact solution $\\boldsymbol{x}^{\\star}$ satisfies $\\mathbf{A}\\,\\boldsymbol{x}^{\\star}=\\boldsymbol{b}$.\n3.  **Optimization Methods**:\n    -   **Gradient Descent (GD)**: Iterative steps along the negative gradient with an exact line search.\n    -   **Cyclic Coordinate Descent (CD)**: Iterative sweeps through coordinates $j=1,2,3$, setting each $z_j$ to its exact minimizer while holding other coordinates fixed.\n4.  **Variable Rescaling**:\n    -   Change of variables: $\\boldsymbol{x}=\\mathbf{S}\\,\\boldsymbol{z}$, where $\\mathbf{S}$ is an invertible diagonal matrix.\n    -   Transformed objective in $\\boldsymbol{z}$: $f(\\mathbf{S}\\boldsymbol{z}) = \\tfrac{1}{2}\\,\\boldsymbol{z}^{\\top}\\mathbf{H}\\,\\boldsymbol{z}-\\boldsymbol{c}^{\\top}\\boldsymbol{z}$, where $\\mathbf{H}=\\mathbf{S}^{\\top}\\mathbf{A}\\mathbf{S}$ and $\\boldsymbol{c}=\\mathbf{S}^{\\top}\\boldsymbol{b}$.\n    -   Initial point in transformed coordinates: $\\boldsymbol{z}_{0}=\\mathbf{S}^{-1}\\boldsymbol{x}_{0}$.\n5.  **Test Problem Construction**:\n    -   Rotation angles (radians): $\\phi=0.4$, $\\theta=0.7$, $\\psi=-0.5$.\n    -   Rotation matrix definitions for $\\mathbf{R}_{x}(\\phi)$, $\\mathbf{R}_{y}(\\theta)$, $\\mathbf{R}_{z}(\\psi)$.\n    -   Total rotation: $\\mathbf{R}=\\mathbf{R}_{z}(\\psi)\\,\\mathbf{R}_{y}(\\theta)\\,\\mathbf{R}_{x}(\\phi)$.\n    -   Diagonal matrix: $\\mathbf{D}=\\operatorname{diag}(50,\\,5,\\,0.5)$.\n    -   Hessian matrix: $\\mathbf{A}=\\mathbf{R}^{\\top}\\mathbf{D}\\,\\mathbf{R}$.\n    -   Linear term vector: $\\boldsymbol{b}=[\\,1,\\,-2,\\,3\\,]^{\\top}$.\n    -   Initial point: $\\boldsymbol{x}_{0}=[\\,0,\\,0,\\,0\\,]^{\\top}$.\n6.  **Test Suite of Scaling Matrices**:\n    -   Case 1: $\\mathbf{S}_{1}=\\operatorname{diag}(1,\\,1,\\,1)$.\n    -   Case 2: $\\mathbf{S}_{2}=\\operatorname{diag}(100,\\,1,\\,0.01)$.\n    -   Case 3: $\\mathbf{S}_{3}=\\operatorname{diag}\\big(1/\\sqrt{[\\mathbf{A}]_{11}},\\,1/\\sqrt{[\\mathbf{A}]_{22}},\\,1/\\sqrt{[\\mathbf{A}]_{33}}\\big)$.\n    -   Case 4: $\\mathbf{S}_{4}=\\operatorname{diag}(10,\\,0.1,\\,1)$.\n7.  **Stopping Criterion and Counters**:\n    -   Error metric: $f(\\boldsymbol{x}_{k})-f^{\\star} \\le 10^{-9}$, where $f^{\\star}=f(\\boldsymbol{x}^{\\star})$ and $\\boldsymbol{x}_{k}=\\mathbf{S}\\boldsymbol{z}_{k}$.\n    -   GD counter $I_{\\mathrm{GD}}$: Number of line searches.\n    -   CD counter $E_{\\mathrm{CD}}$: Number of completed full sweeps (epochs).\n    -   Maximum limits: $10000$ iterations for GD, $10000$ epochs for CD.\n8.  **Required Output**: A single line containing a list of ratios $[\\,r_{1},r_{2},r_{3},r_{4}\\,]$, where $r_{i}=E_{\\mathrm{CD}}/I_{\\mathrm{GD}}$, with each number rounded to $6$ decimals.\n\n#### Validation Analysis\n1.  **Scientific or Factual Soundness**: The problem is grounded in the well-established field of numerical optimization, specifically concerning unconstrained quadratic programming and the effects of preconditioning/scaling on first-order methods. The matrix $\\mathbf{A}=\\mathbf{R}^{\\top}\\mathbf{D}\\,\\mathbf{R}$ is constructed to be symmetric and positive definite (SPD). Symmetry holds as $(\\mathbf{R}^{\\top}\\mathbf{D}\\,\\mathbf{R})^{\\top} = \\mathbf{R}^{\\top}\\mathbf{D}^{\\top}\\mathbf{R} = \\mathbf{R}^{\\top}\\mathbf{D}\\,\\mathbf{R}$ since $\\mathbf{D}$ is diagonal. Positive definiteness holds because the eigenvalues of $\\mathbf{A}$ are the same as those of $\\mathbf{D}$ (i.e., $50$, $5$, $0.5$), which are all positive. The existence of a unique minimizer for the strictly convex quadratic function $f(\\boldsymbol{x})$ is thereby guaranteed. The mathematical setup is logically sound.\n2.  **Well-Posedness**: The problem is well-posed. The SPD nature of $\\mathbf{A}$ ensures a unique solution $\\boldsymbol{x}^{\\star}$ exists. The algorithms (GD with exact line search, CD) are standard and guaranteed to converge for SPD quadratic problems. The stopping criterion is well-defined.\n3.  **Objectivity**: The language is precise, and the definitions are formal and unambiguous. There are no subjective elements.\n4.  **Completeness**: All necessary values ($\\phi, \\theta, \\psi, \\mathbf{D}, \\mathbf{b}, \\mathbf{x}_0$), matrix definitions, algorithm descriptions, stopping criteria, and output formats are specified. The problem is self-contained. The instruction to derive algorithmic steps from first principles is a-\nprocedural detail, not a lack of specification.\n\n#### Verdict\nThe problem is valid. It is scientifically grounded, well-posed, objective, and complete.\n\n### Step 2: Solution Derivation\n\nThe problem requires a comparison of Gradient Descent (GD) and Cyclic Coordinate Descent (CD) on a quadratic objective function under four different variable scalings. The core task is to implement the algorithms from first principles for a generalized quadratic objective $g(\\boldsymbol{z}) = \\tfrac{1}{2}\\,\\boldsymbol{z}^{\\top}\\mathbf{H}\\,\\boldsymbol{z} - \\boldsymbol{c}^{\\top}\\boldsymbol{z}$ and track convergence using the original objective $f(\\boldsymbol{x})$.\n\n**1. Problem Setup**\nFirst, we construct the matrix $\\mathbf{A}$ and vector $\\boldsymbol{b}$ as specified.\n- The rotation matrices $\\mathbf{R}_x(\\phi), \\mathbf{R}_y(\\theta), \\mathbf{R}_z(\\psi)$ are formed using the given angles.\n- The total rotation is $\\mathbf{R} = \\mathbf{R}_z(\\psi)\\mathbf{R}_y(\\theta)\\mathbf{R}_x(\\phi)$.\n- The Hessian of the original problem is $\\mathbf{A} = \\mathbf{R}^{\\top}\\mathbf{D}\\mathbf{R}$, where $\\mathbf{D} = \\operatorname{diag}(50, 5, 0.5)$.\n- The linear term is $\\boldsymbol{b} = [1, -2, 3]^{\\top}$.\n- The exact solution $\\boldsymbol{x}^{\\star}$ is computed by solving the linear system $\\mathbf{A}\\boldsymbol{x}^{\\star} = \\boldsymbol{b}$.\n- The minimum objective value is $f^{\\star} = f(\\boldsymbol{x}^{\\star}) = \\tfrac{1}{2}(\\boldsymbol{x}^{\\star})^{\\top}\\mathbf{A}\\boldsymbol{x}^{\\star} - \\boldsymbol{b}^{\\top}\\boldsymbol{x}^{\\star} = \\tfrac{1}{2}(\\boldsymbol{x}^{\\star})^{\\top}\\boldsymbol{b} - (\\boldsymbol{x}^{\\star})^{\\top}\\boldsymbol{b} = -\\tfrac{1}{2}\\boldsymbol{b}^{\\top}\\boldsymbol{x}^{\\star}$.\n- For each test case, the scaling matrix $\\mathbf{S}$ defines the transformed problem with Hessian $\\mathbf{H} = \\mathbf{S}^{\\top}\\mathbf{A}\\mathbf{S}$ and linear term $\\boldsymbol{c} = \\mathbf{S}^{\\top}\\boldsymbol{b}$. The initial point $\\boldsymbol{x}_0 = \\mathbf{0}$ maps to $\\boldsymbol{z}_0 = \\mathbf{S}^{-1}\\boldsymbol{x}_0 = \\mathbf{0}$.\n\n**2. Gradient Descent (GD) with Exact Line Search**\nFor the objective $g(\\boldsymbol{z})$, the GD update rule is $\\boldsymbol{z}_{k+1} = \\boldsymbol{z}_k + \\alpha_k \\boldsymbol{p}_k$, where the search direction is the negative gradient, $\\boldsymbol{p}_k = - \\nabla g(\\boldsymbol{z}_k)$, and $\\alpha_k$ is the step size from an exact line search.\n- The gradient is $\\nabla g(\\boldsymbol{z}_k) = \\mathbf{H}\\boldsymbol{z}_k - \\boldsymbol{c}$. Let this be $\\boldsymbol{g}_k$.\n- The search direction is $\\boldsymbol{p}_k = -\\boldsymbol{g}_k = \\boldsymbol{c} - \\mathbf{H}\\boldsymbol{z}_k$.\n- To find the optimal step size $\\alpha_k$, we minimize the univariate function $\\phi(\\alpha) = g(\\boldsymbol{z}_k + \\alpha \\boldsymbol{p}_k)$ with respect to $\\alpha$.\n$$ \\phi(\\alpha) = \\tfrac{1}{2}(\\boldsymbol{z}_k + \\alpha \\boldsymbol{p}_k)^{\\top}\\mathbf{H}(\\boldsymbol{z}_k + \\alpha \\boldsymbol{p}_k) - \\boldsymbol{c}^{\\top}(\\boldsymbol{z}_k + \\alpha \\boldsymbol{p}_k) $$\n- The minimum is found by setting the derivative $\\phi'(\\alpha)$ to zero:\n$$ \\phi'(\\alpha) = \\boldsymbol{p}_k^{\\top}\\mathbf{H}(\\boldsymbol{z}_k + \\alpha \\boldsymbol{p}_k) - \\boldsymbol{p}_k^{\\top}\\boldsymbol{c} = \\boldsymbol{p}_k^{\\top}\\mathbf{H}\\boldsymbol{z}_k + \\alpha \\boldsymbol{p}_k^{\\top}\\mathbf{H}\\boldsymbol{p}_k - \\boldsymbol{p}_k^{\\top}\\boldsymbol{c} = 0 $$\n- Solving for $\\alpha$:\n$$ \\alpha_k = \\frac{\\boldsymbol{p}_k^{\\top}(\\boldsymbol{c} - \\mathbf{H}\\boldsymbol{z}_k)}{\\boldsymbol{p}_k^{\\top}\\mathbf{H}\\boldsymbol{p}_k} = \\frac{\\boldsymbol{p}_k^{\\top}\\boldsymbol{p}_k}{\\boldsymbol{p}_k^{\\top}\\mathbf{H}\\boldsymbol{p}_k} $$\n- Substituting $\\boldsymbol{p}_k = -\\boldsymbol{g}_k$, we get the well-known formula for exact line search in quadratic minimization:\n$$ \\alpha_k = \\frac{\\boldsymbol{g}_k^{\\top}\\boldsymbol{g}_k}{\\boldsymbol{g}_k^{\\top}\\mathbf{H}\\boldsymbol{g}_k} $$\n- The algorithm proceeds by initializing $\\boldsymbol{z}_0 = \\mathbf{0}$, and for each iteration $k=0, 1, 2, \\dots$, computing $\\boldsymbol{g}_k$, $\\alpha_k$, and updating $\\boldsymbol{z}_{k+1} = \\boldsymbol{z}_k - \\alpha_k \\boldsymbol{g}_k$. The loop terminates when $f(\\mathbf{S}\\boldsymbol{z}_{k})-f^{\\star} \\le 10^{-9}$.\n\n**3. Cyclic Coordinate Descent (CD)**\nIn CD, we iterate through the coordinates $j=1, 2, 3$ and update each component $z_j$ to minimize $g(\\boldsymbol{z})$ while keeping all other components $z_l$ ($l\\neq j$) fixed.\n- To find the optimal update for $z_j$, we set the partial derivative $\\frac{\\partial g}{\\partial z_j}$ to zero. The partial derivative is the $j$-th component of the gradient:\n$$ \\frac{\\partial g}{\\partial z_j} = (\\nabla g(\\boldsymbol{z}))_j = (\\mathbf{H}\\boldsymbol{z} - \\boldsymbol{c})_j = \\sum_{l=1}^{3} H_{jl}z_l - c_j = 0 $$\n- We separate the term with a new $z_j^{\\text{new}}$:\n$$ H_{jj} z_j^{\\text{new}} + \\sum_{l \\neq j} H_{jl}z_l = c_j $$\n- Solving for $z_j^{\\text{new}}$:\n$$ z_j^{\\text{new}} = \\frac{1}{H_{jj}} \\left( c_j - \\sum_{l \\neq j} H_{jl}z_l \\right) $$\n- The cyclic (Gauss-Seidel) nature of the algorithm means that when updating $z_j$, the new values for $z_l$ with $l<j$ from the current sweep are used.\n- One full sweep, or epoch, consists of updating $z_1, z_2, z_3$ in order. The stopping criterion is checked after each completed epoch.\n\n**4. Implementation Plan**\nThe solution will be implemented in a single Python function.\n1.  Define helper functions for GD and CD solvers. These functions will take the transformed problem parameters $(\\mathbf{H}, \\boldsymbol{c})$, the scaling matrix $\\mathbf{S}$, the original problem parameters $(\\mathbf{A}, \\boldsymbol{b})$, and the optimal value $f^{\\star}$ as input. They will return the number of iterations/epochs.\n2.  The main part of the script will:\n    a. Construct $\\mathbf{A}$, $\\boldsymbol{b}$.\n    b. Calculate the exact solution $\\boldsymbol{x}^{\\star}$ and $f^{\\star}$.\n    c. Define the four scaling matrices $\\mathbf{S}_1, \\dots, \\mathbf{S}_4$.\n    d. Loop through each $\\mathbf{S}_i$:\n        i.  Compute the transformed problem matrices $\\mathbf{H}_i$ and $\\boldsymbol{c}_i$.\n        ii. Call the GD and CD solvers to get $I_{\\mathrm{GD}}$ and $E_{\\mathrm{CD}}$.\n        iii.Compute the ratio $r_i = E_{\\mathrm{CD}} / I_{\\mathrm{GD}}$.\n    e. Print the final list of ratios in the specified format.\n\nThe Jacobi scaling (Case 3) is expected to balance the problem for coordinate-wise methods, likely making CD more efficient relative to GD compared to the unscaled case. The other non-identity scalings (Cases 2 and 4) are arbitrary and may degrade the performance of both methods by distorting the geometry of the problem and increasing the condition number of the Hessian $\\mathbf{H}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quadratic optimization problem with GD and CD under different scalings,\n    and computes the ratio of CD epochs to GD iterations.\n    \"\"\"\n    # Define problem constants\n    phi = 0.4\n    theta = 0.7\n    psi = -0.5\n    b_vec = np.array([1.0, -2.0, 3.0])\n    tol = 1e-9\n    max_iter_gd = 10000\n    max_epochs_cd = 10000\n\n    # -------- Helper Functions --------\n\n    def objective_function(x, A, b):\n        \"\"\"Calculates the value of the original objective function f(x).\"\"\"\n        return 0.5 * x.T @ A @ x - b.T @ x\n\n    def run_gd(H, c, S, A, b, f_star):\n        \"\"\"\n        Solves the scaled quadratic problem using gradient descent with exact line search.\n        Returns the number of iterations.\n        \"\"\"\n        z = np.zeros(3)  # z0 = S^-1 * x0 = 0\n\n        # Check initial point (0 iterations)\n        x_check = S @ z\n        if objective_function(x_check, A, b) - f_star <= tol:\n            return 0\n\n        for i in range(1, max_iter_gd + 1):\n            grad = H @ z - c\n            \n            grad_norm_sq = np.dot(grad, grad)\n\n            if grad_norm_sq < 1e-30:\n                break # Reached optimum or stalled\n\n            alpha_den = grad.T @ H @ grad\n            if alpha_den < 1e-30:\n                break # Stalled\n\n            alpha = grad_norm_sq / alpha_den\n            z = z - alpha * grad\n            \n            x = S @ z\n            if objective_function(x, A, b) - f_star <= tol:\n                return i\n        \n        return max_iter_gd\n\n    def run_cd(H, c, S, A, b, f_star):\n        \"\"\"\n        Solves the scaled quadratic problem using cyclic coordinate descent.\n        Returns the number of epochs (full sweeps).\n        \"\"\"\n        z = np.zeros(3) # z0 = S^-1 * x0 = 0\n\n        # Check initial point (0 epochs)\n        x_check = S @ z\n        if objective_function(x_check, A, b) - f_star <= tol:\n            return 0\n\n        H_diag = np.diag(H)\n        \n        for k in range(1, max_epochs_cd + 1):\n            # Perform one full sweep (epoch) using Gauss-Seidel updates\n            for j in range(3):\n                # z_j_new = (c_j - sum_{l!=j} H_jl z_l) / H_jj\n                numerator = c[j] - (np.dot(H[j, :], z) - H[j, j] * z[j])\n                z[j] = numerator / H_diag[j]\n\n            x = S @ z\n            if objective_function(x, A, b) - f_star <= tol:\n                return k\n\n        return max_epochs_cd\n\n    # -------- Main Logic --------\n\n    # 1. Construct the problem matrix A\n    c_phi, s_phi = np.cos(phi), np.sin(phi)\n    c_theta, s_theta = np.cos(theta), np.sin(theta)\n    c_psi, s_psi = np.cos(psi), np.sin(psi)\n\n    Rx = np.array([[1.0, 0.0, 0.0], [0.0, c_phi, -s_phi], [0.0, s_phi, c_phi]])\n    Ry = np.array([[c_theta, 0.0, s_theta], [0.0, 1.0, 0.0], [-s_theta, 0.0, c_theta]])\n    Rz = np.array([[c_psi, -s_psi, 0.0], [s_psi, c_psi, 0.0], [0.0, 0.0, 1.0]])\n\n    R = Rz @ Ry @ Rx\n    D = np.diag([50.0, 5.0, 0.5])\n    A_mat = R.T @ D @ R\n    \n    # 2. Compute the exact solution and optimal value\n    x_star = np.linalg.solve(A_mat, b_vec)\n    f_star = objective_function(x_star, A_mat, b_vec)\n    \n    # 3. Define the scaling matrices for test cases\n    S1 = np.diag([1.0, 1.0, 1.0])\n    S2 = np.diag([100.0, 1.0, 0.01])\n    S3 = np.diag(1.0 / np.sqrt(np.diag(A_mat)))\n    S4 = np.diag([10.0, 0.1, 1.0])\n\n    test_cases = [S1, S2, S3, S4]\n    results = []\n\n    # 4. Run simulations for each test case\n    for S_mat in test_cases:\n        H_mat = S_mat.T @ A_mat @ S_mat\n        c_vec = S_mat.T @ b_vec\n\n        I_gd = run_gd(H_mat, c_vec, S_mat, A_mat, b_vec, f_star)\n        E_cd = run_cd(H_mat, c_vec, S_mat, A_mat, b_vec, f_star)\n        \n        # Initial error is non-trivial, so I_gd will not be 0\n        ratio = float(E_cd) / float(I_gd)\n        results.append(ratio)\n        \n    # 5. Print results in the required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3158937"}, {"introduction": "Our final practice provides a glimpse into the theoretical ideal of preconditioning. Here, we will construct and apply a 'perfect' preconditioner derived from the spectral decomposition of the problem's Hessian matrix. This transformation will reshape an ill-conditioned problem into a perfectly conditioned one where the level sets are spherical, allowing the steepest descent method to find the solution in a single step. This exercise serves as a conceptual benchmark, illustrating the ultimate goal that practical preconditioning methods strive to approximate. [@problem_id:3158940]", "problem": "You are asked to write a complete, runnable program that evaluates the effect of numerical scaling on the conditioning of quadratic optimization problems and its impact on the iteration count of steepest descent with exact line search. Consider the unconstrained quadratic minimization problem with objective function $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\mathsf{T}}\\mathbf{H}\\mathbf{x} - \\mathbf{b}^{\\mathsf{T}}\\mathbf{x}$, where $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite and $\\mathbf{b}\\in\\mathbb{R}^{n}$. Implement steepest descent with exact line search in two parameterizations: the original variable $\\mathbf{x}$ and a scaled variable $\\mathbf{z}=\\mathbf{P}\\mathbf{x}$, where $\\mathbf{P}$ is chosen to flatten the spectrum of $\\mathbf{H}$. Specifically, compute an eigendecomposition $\\mathbf{H}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}$ with orthonormal $\\mathbf{Q}$ and diagonal $\\mathbf{\\Lambda}=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ with strictly positive entries, and let $\\mathbf{P}=\\mathbf{Q}\\mathbf{\\Lambda}^{1/2}$, where $\\mathbf{\\Lambda}^{1/2}=\\operatorname{diag}(\\sqrt{\\lambda_{1}},\\dots,\\sqrt{\\lambda_{n}})$. This choice transforms the objective into a problem whose Hessian in $\\mathbf{z}$-coordinates is the identity matrix, thereby flattening eigenvalues.\n\nAlgorithmic requirements that your program must follow:\n- Implement steepest descent with exact line search in $\\mathbf{x}$-coordinates as follows: starting from $\\mathbf{x}_{0}$, iterate $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha_{k}\\nabla f(\\mathbf{x}_{k})$, where $\\nabla f(\\mathbf{x})=\\mathbf{H}\\mathbf{x}-\\mathbf{b}$ and the step length $\\alpha_{k}$ minimizes the one-dimensional function $\\phi(\\alpha)=f(\\mathbf{x}_{k}-\\alpha\\nabla f(\\mathbf{x}_{k}))$ over $\\alpha\\in\\mathbb{R}$. Terminate when $\\|\\nabla f(\\mathbf{x}_{k})\\|_{2}\\leq \\varepsilon \\|\\nabla f(\\mathbf{x}_{0})\\|_{2}$ or when a maximum number of iterations is reached.\n- Implement the same method in $\\mathbf{z}$-coordinates for the transformed objective $f_{t}(\\mathbf{z})=\\tfrac{1}{2}\\mathbf{z}^{\\mathsf{T}}\\mathbf{I}\\mathbf{z}-\\mathbf{c}^{\\mathsf{T}}\\mathbf{z}$, where $\\mathbf{c}=\\mathbf{P}^{-\\mathsf{T}}\\mathbf{b}$, starting from $\\mathbf{z}_{0}=\\mathbf{P}\\mathbf{x}_{0}$, using exact line search along the negative gradient direction $\\nabla f_{t}(\\mathbf{z})=\\mathbf{z}-\\mathbf{c}$, and terminating when $\\|\\nabla f_{t}(\\mathbf{z}_{k})\\|_{2}\\leq \\varepsilon \\|\\nabla f_{t}(\\mathbf{z}_{0})\\|_{2}$ or when a maximum number of iterations is reached.\n\nUse the following test suite of three cases. For each case, the tolerance is $\\varepsilon=10^{-8}$ and the maximum number of iterations is $10^{6}$. All vectors are column vectors, but you may represent them as one-dimensional arrays in code.\n\n- Case $1$ (isotropic Hessian, boundary case):\n  - Dimension $n=4$.\n  - $\\mathbf{H}_{1}=\\mu\\mathbf{I}$ with $\\mu=7$.\n  - $\\mathbf{b}_{1}=[2,\\,-1,\\,3,\\,0.5]^{\\mathsf{T}}$.\n  - $\\mathbf{x}_{0,1}=\\mathbf{0}$.\n\n- Case $2$ (axis-aligned but ill-conditioned):\n  - Dimension $n=4$.\n  - $\\mathbf{H}_{2}=\\operatorname{diag}(0.1,\\,1,\\,10,\\,100)$.\n  - $\\mathbf{b}_{2}=[1,\\,1,\\,1,\\,1]^{\\mathsf{T}}$.\n  - $\\mathbf{x}_{0,2}=\\mathbf{0}$.\n\n- Case $3$ (rotated, strongly ill-conditioned):\n  - Dimension $n=6$.\n  - Let $\\mathbf{A}_{3}$ be\n    $\n    \\begin{bmatrix}\n    1 & 2 & 3 & 4 & 5 & 6\\\\\n    2 & 1 & 4 & 3 & 6 & 5\\\\\n    3 & 4 & 1 & 2 & 5 & 6\\\\\n    4 & 3 & 2 & 1 & 6 & 5\\\\\n    5 & 6 & 5 & 6 & 1 & 2\\\\\n    6 & 5 & 6 & 5 & 2 & 1\n    \\end{bmatrix}\n    $.\n    Compute the thin $\\mathbf{Q}\\mathbf{R}$ factorization of $\\mathbf{A}_{3}$ and take the orthonormal factor $\\mathbf{Q}_{3}$.\n  - Define $\\mathbf{\\Lambda}_{3}=\\operatorname{diag}(1,\\,2,\\,5,\\,20,\\,50,\\,1000)$ and $\\mathbf{H}_{3}=\\mathbf{Q}_{3}\\mathbf{\\Lambda}_{3}\\mathbf{Q}_{3}^{\\mathsf{T}}$.\n  - $\\mathbf{b}_{3}=[1,\\,-2,\\,3,\\,-4,\\,5,\\,-6]^{\\mathsf{T}}$.\n  - $\\mathbf{x}_{0,3}=\\mathbf{0}$.\n\nFor each case $i\\in\\{1,2,3\\}$, your program must compute:\n- $N_{x}^{(i)}$, the number of iterations taken by steepest descent with exact line search in $\\mathbf{x}$-coordinates to meet the termination condition.\n- $N_{z}^{(i)}$, the number of iterations taken by steepest descent with exact line search in $\\mathbf{z}$-coordinates to meet the termination condition.\n- $\\Delta^{(i)}=N_{x}^{(i)}-N_{z}^{(i)}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, in the order $[N_{x}^{(1)},N_{z}^{(1)},\\Delta^{(1)},N_{x}^{(2)},N_{z}^{(2)},\\Delta^{(2)},N_{x}^{(3)},N_{z}^{(3)},\\Delta^{(3)}]$.\n- All outputs must be integers.\n\nYour implementation must be self-contained and must not require any external input. There are no physical units involved in this problem. Angles are not used. Express all thresholds and constants as pure numbers in your code, and ensure the execution finishes under the specified iteration cap for all cases. The program must adhere to the runtime environment described in the final answer specification.", "solution": "The user-provided problem is assessed to be valid. It is a well-posed problem in numerical optimization that is scientifically grounded, self-contained, and objective. It asks for the implementation and comparison of the steepest descent method on a quadratic objective function with and without preconditioning.\n\nThe core problem is the unconstrained minimization of a quadratic function $f(\\mathbf{x}): \\mathbb{R}^{n} \\to \\mathbb{R}$:\n$$f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\mathsf{T}}\\mathbf{H}\\mathbf{x} - \\mathbf{b}^{\\mathsf{T}}\\mathbf{x}$$\nwhere $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix and $\\mathbf{b} \\in \\mathbb{R}^{n}$. The gradient of this function is $\\nabla f(\\mathbf{x}) = \\mathbf{H}\\mathbf{x} - \\mathbf{b}$.\n\n### Steepest Descent in Original Coordinates ($\\mathbf{x}$)\n\nThe steepest descent method iteratively updates the current solution $\\mathbf{x}_k$ by moving in the direction of the negative gradient, which is the direction of steepest descent. The update rule is:\n$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_k \\mathbf{p}_k = \\mathbf{x}_{k} - \\alpha_k \\nabla f(\\mathbf{x}_k)$$\nwhere $\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)$ is the search direction and $\\alpha_k > 0$ is the step length.\n\nThe problem specifies an exact line search to determine the optimal step length $\\alpha_k$. This involves minimizing the one-dimensional function $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))$ with respect to $\\alpha$. We find the minimum by setting the derivative $\\frac{d\\phi}{d\\alpha}$ to zero.\nLet $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$.\n$$\\phi(\\alpha) = \\frac{1}{2}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^{\\mathsf{T}}\\mathbf{H}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b}^{\\mathsf{T}}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$$\n$$\\frac{d\\phi}{d\\alpha} = -\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) + \\mathbf{g}_k^{\\mathsf{T}}\\mathbf{b} = -\\mathbf{g}_k^{\\mathsf{T}}(\\mathbf{H}\\mathbf{x}_k - \\mathbf{b}) + \\alpha\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}\\mathbf{g}_k$$\nUsing the definition of the gradient $\\mathbf{g}_k = \\mathbf{H}\\mathbf{x}_k - \\mathbf{b}$, we get:\n$$\\frac{d\\phi}{d\\alpha} = -\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{g}_k + \\alpha\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}\\mathbf{g}_k$$\nSetting this to $0$ and solving for $\\alpha$ gives the optimal step length $\\alpha_k$:\n$$\\alpha_k = \\frac{\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{g}_k}{\\mathbf{g}_k^{\\mathsf{T}}\\mathbf{H}\\mathbf{g}_k}$$\nThe iteration proceeds until the termination condition $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon \\|\\nabla f(\\mathbf{x}_0)\\|_2$ is met or a maximum number of iterations is reached.\n\n### Steepest Descent in Scaled Coordinates ($\\mathbf{z}$)\n\nThe convergence rate of steepest descent is limited by the condition number $\\kappa(\\mathbf{H}) = \\lambda_{\\max}/\\lambda_{\\min}$ of the Hessian matrix $\\mathbf{H}$. If $\\kappa(\\mathbf{H}) \\gg 1$, the problem is ill-conditioned, and convergence can be very slow. Preconditioning aims to transform the problem into a better-conditioned one.\n\nWe use a change of variables $\\mathbf{z} = \\mathbf{P}\\mathbf{x}$, which means $\\mathbf{x} = \\mathbf{P}^{-1}\\mathbf{z}$. The scaling matrix $\\mathbf{P}$ is chosen to \"whiten\" the Hessian. Based on the eigendecomposition of the SPD matrix $\\mathbf{H} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}$, where $\\mathbf{Q}$ is orthonormal and $\\mathbf{\\Lambda}$ is a diagonal matrix of positive eigenvalues, we choose $\\mathbf{P} = \\mathbf{Q}\\mathbf{\\Lambda}^{1/2}$.\n\nThe objective function in terms of $\\mathbf{z}$ is $f_t(\\mathbf{z}) = f(\\mathbf{P}^{-1}\\mathbf{z})$:\n$$f_t(\\mathbf{z}) = \\frac{1}{2}(\\mathbf{P}^{-1}\\mathbf{z})^{\\mathsf{T}}\\mathbf{H}(\\mathbf{P}^{-1}\\mathbf{z}) - \\mathbf{b}^{\\mathsf{T}}(\\mathbf{P}^{-1}\\mathbf{z})$$\n$$f_t(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^{\\mathsf{T}}(\\mathbf{P}^{-1})^{\\mathsf{T}}\\mathbf{H}\\mathbf{P}^{-1}\\mathbf{z} - (\\mathbf{P}^{-\\mathsf{T}}\\mathbf{b})^{\\mathsf{T}}\\mathbf{z}$$\nThe new Hessian is $\\mathbf{H}_t = (\\mathbf{P}^{-1})^{\\mathsf{T}}\\mathbf{H}\\mathbf{P}^{-1}$. Substituting $\\mathbf{P}$ and $\\mathbf{H}$:\n$$\\mathbf{H}_t = ((\\mathbf{Q}\\mathbf{\\Lambda}^{1/2})^{-1})^{\\mathsf{T}} (\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}) ((\\mathbf{Q}\\mathbf{\\Lambda}^{1/2})^{-1}) = (\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}})^{\\mathsf{T}} (\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}) (\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}})$$\n$$\\mathbf{H}_t = (\\mathbf{Q}\\mathbf{\\Lambda}^{-1/2}) (\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\mathsf{T}}) (\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}}) = \\mathbf{Q}\\mathbf{\\Lambda}^{-1/2}\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{\\mathsf{T}} = \\mathbf{Q}\\mathbf{I}\\mathbf{Q}^{\\mathsf{T}} = \\mathbf{I}$$\nThe transformed Hessian is the identity matrix, which has a condition number of $\\kappa(\\mathbf{I})=1$. The problem is perfectly conditioned. The transformed objective is:\n$$f_t(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^{\\mathsf{T}}\\mathbf{I}\\mathbf{z} - \\mathbf{c}^{\\mathsf{T}}\\mathbf{z}, \\quad \\text{where} \\quad \\mathbf{c} = \\mathbf{P}^{-\\mathsf{T}}\\mathbf{b} = \\mathbf{Q}\\mathbf{\\Lambda}^{-1/2}\\mathbf{b}$$\nThe gradient is $\\nabla f_t(\\mathbf{z}) = \\mathbf{z} - \\mathbf{c}$. The minimum occurs where $\\nabla f_t(\\mathbf{z}) = \\mathbf{0}$, i.e., at $\\mathbf{z}^* = \\mathbf{c}$.\n\nApplying steepest descent with exact line search to $f_t(\\mathbf{z})$:\n- The search direction is $\\mathbf{p}_k = - \\nabla f_t(\\mathbf{z}_k) = -(\\mathbf{z}_k - \\mathbf{c})$.\n- The step length formula gives $\\alpha_k = \\frac{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\nabla f_t(\\mathbf{z}_k)}{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\mathbf{H}_t\\nabla f_t(\\mathbf{z}_k)} = \\frac{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\nabla f_t(\\mathbf{z}_k)}{\\nabla f_t(\\mathbf{z}_k)^{\\mathsf{T}}\\mathbf{I}\\nabla f_t(\\mathbf{z}_k)} = 1$.\nThe step length is always $\\alpha_k=1$. The update rule becomes:\n$$\\mathbf{z}_{k+1} = \\mathbf{z}_k - 1 \\cdot \\nabla f_t(\\mathbf{z}_k) = \\mathbf{z}_k - (\\mathbf{z}_k - \\mathbf{c}) = \\mathbf{c}$$\nThus, starting from any $\\mathbf{z}_0 \\neq \\mathbf{c}$, the algorithm converges to the exact solution $\\mathbf{z}^*=\\mathbf{c}$ in a single step. For the given test cases, $\\mathbf{x}_0 = \\mathbf{0}$, so $\\mathbf{z}_0 = \\mathbf{P}\\mathbf{x}_0 = \\mathbf{0}$. The initial gradient $\\nabla f_t(\\mathbf{z}_0) = -\\mathbf{c}$ is non-zero, so the algorithm will take exactly one iteration. Therefore, $N_z^{(i)}=1$ for all cases $i=1, 2, 3$.\n\nFor Case $1$, the Hessian $\\mathbf{H}_1=\\mu\\mathbf{I}$ is already perfectly conditioned with $\\kappa(\\mathbf{H}_1)=1$. The steepest descent method in original coordinates is also expected to converge in one step. Thus we predict $N_x^{(1)}=1$.\n\nFor Cases $2$ and $3$, the Hessians are constructed to be ill-conditioned, with $\\kappa(\\mathbf{H}) = 1000$. This will lead to slow convergence, and we expect $N_x^{(2)}$ and $N_x^{(3)}$ to be significantly larger than $1$.\n\n### Implementation\n\nTwo functions are implemented: one for steepest descent in $\\mathbf{x}$-coordinates (`steepest_descent_x`) and another that first computes the transformation and then performs steepest descent in $\\mathbf{z}$-coordinates (`steepest_descent_z`). The main `solve` function constructs the matrices for each of the three test cases and calls these two functions to compute the number of iterations $N_x$ and $N_z$. The difference $\\Delta = N_x - N_z$ is then calculated for each case. The results are aggregated and printed in the required format.", "answer": "```python\nimport numpy as np\n\ndef steepest_descent_x(H, b, x0, epsilon, max_iter):\n    \"\"\"\n    Performs steepest descent with exact line search in x-coordinates.\n    \"\"\"\n    x = x0.copy()\n    g = H @ x - b\n    g0_norm = np.linalg.norm(g)\n\n    if g0_norm == 0:\n        return 0\n\n    term_thresh = epsilon * g0_norm\n    \n    k = 0\n    while k < max_iter:\n        g_norm = np.linalg.norm(g)\n        if g_norm <= term_thresh:\n            break\n        \n        # Exact line search step size\n        alpha = (g.T @ g) / (g.T @ H @ g)\n        \n        # Update step\n        x = x - alpha * g\n        \n        # Update gradient\n        g = H @ x - b\n        \n        k += 1\n        \n    return k\n\ndef steepest_descent_z(H, b, x0, epsilon, max_iter):\n    \"\"\"\n    Performs steepest descent with exact line search in z-coordinates.\n    \"\"\"\n    # Eigendecomposition of H = Q Lambda Q.T\n    # eigh is for symmetric/hermitian matrices\n    eigvals, Q = np.linalg.eigh(H)\n    \n    # Transformation matrices\n    # P = Q * Lambda^(1/2)\n    # P_inv_T = Q * Lambda^(-1/2) (derived in solution explanation)\n    sqrt_eigvals = np.sqrt(eigvals)\n    inv_sqrt_eigvals = 1.0 / sqrt_eigvals\n    \n    P = Q @ np.diag(sqrt_eigvals)\n    P_inv_T = Q @ np.diag(inv_sqrt_eigvals)\n    \n    # Transformed problem parameters\n    c = P_inv_T @ b\n    z0 = P @ x0\n    \n    z = z0.copy()\n    g_t = z - c # Gradient in z-space\n    g0_norm_t = np.linalg.norm(g_t)\n\n    if g0_norm_t == 0:\n        return 0\n        \n    term_thresh_t = epsilon * g0_norm_t\n\n    k = 0\n    while k < max_iter:\n        g_t_norm = np.linalg.norm(g_t)\n        if g_t_norm <= term_thresh_t:\n            break\n            \n        # Exact line search step size is always 1 for z-space problem\n        alpha = 1.0\n        \n        # Update step\n        z = z - alpha * g_t\n        \n        # Update gradient\n        g_t = z - c\n        \n        k += 1\n        \n    return k\n\ndef solve():\n    \"\"\"\n    Sets up and solves the three test cases as specified in the problem.\n    \"\"\"\n    epsilon = 1e-8\n    max_iter = 10**6\n    results = []\n\n    # Case 1\n    n1 = 4\n    mu = 7.0\n    H1 = np.diag([mu] * n1)\n    b1 = np.array([2.0, -1.0, 3.0, 0.5])\n    x01 = np.zeros(n1)\n    \n    Nx1 = steepest_descent_x(H1, b1, x01, epsilon, max_iter)\n    Nz1 = steepest_descent_z(H1, b1, x01, epsilon, max_iter)\n    D1 = Nx1 - Nz1\n    results.extend([Nx1, Nz1, D1])\n\n    # Case 2\n    n2 = 4\n    H2 = np.diag([0.1, 1.0, 10.0, 100.0])\n    b2 = np.ones(n2)\n    x02 = np.zeros(n2)\n\n    Nx2 = steepest_descent_x(H2, b2, x02, epsilon, max_iter)\n    Nz2 = steepest_descent_z(H2, b2, x02, epsilon, max_iter)\n    D2 = Nx2 - Nz2\n    results.extend([Nx2, Nz2, D2])\n\n    # Case 3\n    n3 = 6\n    A3 = np.array([\n        [1, 2, 3, 4, 5, 6],\n        [2, 1, 4, 3, 6, 5],\n        [3, 4, 1, 2, 5, 6],\n        [4, 3, 2, 1, 6, 5],\n        [5, 6, 5, 6, 1, 2],\n        [6, 5, 6, 5, 2, 1]\n    ], dtype=float)\n    \n    Q3, _ = np.linalg.qr(A3, mode='reduced')\n    Lambda3_diag = np.array([1.0, 2.0, 5.0, 20.0, 50.0, 1000.0])\n    Lambda3 = np.diag(Lambda3_diag)\n    H3 = Q3 @ Lambda3 @ Q3.T\n    \n    b3 = np.array([1.0, -2.0, 3.0, -4.0, 5.0, -6.0])\n    x03 = np.zeros(n3)\n\n    Nx3 = steepest_descent_x(H3, b3, x03, epsilon, max_iter)\n    Nz3 = steepest_descent_z(H3, b3, x03, epsilon, max_iter)\n    D3 = Nx3 - Nz3\n    results.extend([Nx3, Nz3, D3])\n    \n    # Cast to integer as per output format specification\n    results_int = [int(r) for r in results]\n    \n    # Final print statement\n    print(f\"[{','.join(map(str, results_int))}]\")\n\nsolve()\n\n```", "id": "3158940"}]}