{"hands_on_practices": [{"introduction": "The concept of separating a point from a convex set is a cornerstone of optimization theory. This practice will guide you through deriving this fundamental principle from the ground up, starting with the definition of a projection. By working through a concrete example, you will solidify your understanding of how the vector connecting a point to its closest point in a convex set naturally defines a separating hyperplane.", "problem": "Let $C \\subset \\mathbb{R}^{3}$ be a nonempty, closed, and convex set, and let $x_{0} \\in \\mathbb{R}^{3}$ be a point with $x_{0} \\notin C$. Denote by $p = \\operatorname{proj}_{C}(x_{0})$ the Euclidean projection of $x_{0}$ onto $C$, defined as the unique point in $C$ that minimizes the squared Euclidean distance to $x_{0}$. Starting from first principles of convex optimization and the definition of projection, derive a hyperplane that supports $C$ at $p$ and strictly separates $x_{0}$ from $C$ using the vector $x_{0} - p$. Your derivation must begin from the definition of projection as the solution to a convex minimization problem and proceed by establishing the necessary optimality condition that characterizes $p$.\n\nThen, instantiate your construction for the specific closed convex set\n$$\nC = \\{ x \\in \\mathbb{R}^{3} : 0 \\leq x_{1} \\leq 1,\\; 0 \\leq x_{2} \\leq 1,\\; 0 \\leq x_{3} \\leq 1 \\}\n$$\nand the point\n$$\nx_{0} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}.\n$$\nCompute the projection $p = \\operatorname{proj}_{C}(x_{0})$, and from your derivation, compute the normal vector $w \\in \\mathbb{R}^{3}$ and offset $b \\in \\mathbb{R}$ of the separating hyperplane written in the form $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x = b \\}$.\n\nExpress your final answer as a row matrix $(w_{1}\\; w_{2}\\; w_{3}\\; b)$ using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The problem is valid. It is a well-posed problem in convex analysis that is scientifically grounded and objectively stated. It requires the derivation of a standard result concerning projections onto convex sets and its application to a specific case. All necessary information is provided.\n\n**Part 1: General Derivation**\n\nLet $C \\subset \\mathbb{R}^{3}$ be a nonempty, closed, and convex set, and let $x_{0} \\in \\mathbb{R}^{3}$ be a point such that $x_{0} \\notin C$. The Euclidean projection of $x_{0}$ onto $C$, denoted by $p = \\operatorname{proj}_{C}(x_{0})$, is defined as the unique point in $C$ that minimizes the squared Euclidean distance to $x_{0}$. This can be formulated as a convex optimization problem:\n$$\np = \\arg\\min_{z \\in C} f(z)\n$$\nwhere the objective function is $f(z) = \\frac{1}{2} \\|z - x_{0}\\|^{2}$. The function $f(z)$ is strictly convex, and the constraint set $C$ is convex. Therefore, a unique minimizer $p$ exists.\n\nThe first-order necessary and sufficient condition for a point $p \\in C$ to be the minimizer of a differentiable convex function $f(z)$ over a convex set $C$ is given by the variational inequality:\n$$\n\\nabla f(p)^{\\top} (z - p) \\geq 0, \\quad \\forall z \\in C\n$$\nThis condition states that the negative gradient $-\\nabla f(p)$ at the optimal point $p$ defines a supporting hyperplane to the feasible set $C$ at $p$.\n\nFor our specific objective function, $f(z) = \\frac{1}{2} \\|z - x_{0}\\|^{2} = \\frac{1}{2} (z - x_{0})^{\\top}(z - x_{0})$, the gradient with respect to $z$ is:\n$$\n\\nabla f(z) = z - x_{0}\n$$\nEvaluating the gradient at the optimal point $p$, we have $\\nabla f(p) = p - x_{0}$. Substituting this into the optimality condition yields:\n$$\n(p - x_{0})^{\\top} (z - p) \\geq 0, \\quad \\forall z \\in C\n$$\nThis inequality is the fundamental characterization of the projection $p$. We can rearrange it by multiplying by $-1$:\n$$\n(x_{0} - p)^{\\top} (z - p) \\leq 0\n$$\n$$\n(x_{0} - p)^{\\top} z - (x_{0} - p)^{\\top} p \\leq 0\n$$\n$$\n(x_{0} - p)^{\\top} z \\leq (x_{0} - p)^{\\top} p, \\quad \\forall z \\in C\n$$\nNow, we define a hyperplane using the vector $w = x_{0} - p$. Since $x_{0} \\notin C$ and $p \\in C$, we have $x_{0} \\neq p$, so $w \\neq 0$. Let the hyperplane be $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x = b \\}$. We set the offset $b$ to be $b = w^{\\top} p$.\nWith these definitions, the inequality becomes:\n$$\nw^{\\top} z \\leq b, \\quad \\forall z \\in C\n$$\nThis shows that the entire set $C$ is contained in the closed half-space $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x \\leq b \\}$. By definition, $w^{\\top} p = b$, so the point $p$ lies on the hyperplane. Since $p \\in C$, this confirms that the hyperplane $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x = b \\}$ supports the set $C$ at the point $p$.\n\nNext, we must show that this hyperplane strictly separates $x_{0}$ from $C$. We have shown that $w^\\top z \\leq b$ for all $z \\in C$. We need to show that $w^\\top x_0 > b$. Let's evaluate $w^{\\top} x_{0}$:\n$$\nw^{\\top} x_{0} = (x_{0} - p)^{\\top} x_{0}\n$$\nThe difference between this value and $b$ is:\n$$\nw^{\\top} x_{0} - b = (x_{0} - p)^{\\top} x_{0} - (x_{0} - p)^{\\top} p = (x_{0} - p)^{\\top} (x_{0} - p) = \\|x_{0} - p\\|^{2}\n$$\nSince $x_{0} \\notin C$, we know $x_{0} \\neq p$, and thus the squared Euclidean distance $\\|x_{0} - p\\|^{2}$ is strictly positive. Therefore:\n$$\nw^{\\top} x_{0} - b > 0 \\implies w^{\\top} x_{0} > b\n$$\nThis demonstrates that $x_{0}$ lies in the open half-space $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x > b \\}$, which is strictly separated from the set $C$.\n\n**Part 2: Specific Instance**\n\nWe are given the closed convex set\n$$\nC = \\{ x \\in \\mathbb{R}^{3} : 0 \\leq x_{1} \\leq 1,\\; 0 \\leq x_{2} \\leq 1,\\; 0 \\leq x_{3} \\leq 1 \\} = [0, 1]^{3}\n$$\nwhich is the unit cube in $\\mathbb{R}^{3}$. The point is given by\n$$\nx_{0} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}.\n$$\nFirst, we compute the projection $p = \\operatorname{proj}_{C}(x_{0})$. For a box-like set such as $C$, the projection is separable and can be computed component-wise. For each component $i \\in \\{1, 2, 3\\}$, the projection of $(x_{0})_{i}$ onto the interval $[0, 1]$ is given by:\n$$\np_{i} = \\operatorname{proj}_{[0,1]}((x_{0})_{i}) = \\max(0, \\min(1, (x_{0})_{i}))\n$$\nFor $i=1$: $p_{1} = \\operatorname{proj}_{[0,1]}(2) = 1$.\nFor $i=2$: $p_{2} = \\operatorname{proj}_{[0,1]}(-1) = 0$.\nFor $i=3$: $p_{3} = \\operatorname{proj}_{[0,1]}(3) = 1$.\nThus, the projection point is:\n$$\np = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nNext, we compute the normal vector $w$ of the separating hyperplane using the formula derived in the general part: $w = x_{0} - p$.\n$$\nw = x_{0} - p = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\nSo, the components of the normal vector are $w_{1} = 1$, $w_{2} = -1$, and $w_{3} = 2$.\n\nFinally, we compute the offset $b$ of the hyperplane using the formula $b = w^{\\top} p$.\n$$\nb = w^{\\top} p = \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = (1)(1) + (-1)(0) + (2)(1) = 1 + 0 + 2 = 3\n$$\nThe hyperplane is thus described by the equation $w^{\\top} x = b$, which is $x_{1} - x_{2} + 2x_{3} = 3$.\n\nThe final answer consists of the components of $w$ and the value of $b$.\n$w_{1} = 1$, $w_{2} = -1$, $w_{3} = 2$, $b = 3$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  -1  2  3\n\\end{pmatrix}\n}\n$$", "id": "3179785"}, {"introduction": "While smooth convex sets have a unique supporting hyperplane at each boundary point, the geometry becomes richer at non-smooth points, or \"corners.\" This exercise explores this scenario by examining the epigraph of a convex function at a point where it is not differentiable. You will discover the direct correspondence between the set of subgradients—the analytical generalization of the gradient—and the family of supporting hyperplanes at that point.", "problem": "Consider the convex function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x_{1},x_{2})=\\max\\{|x_{1}|,|x_{2}|\\}$. Let $\\operatorname{epi}(f)=\\{(x_{1},x_{2},t)\\in\\mathbb{R}^{3}:t\\geq f(x_{1},x_{2})\\}$ denote the epigraph of $f$. Using only the definitions of convexity, epigraph, subgradient, and supporting hyperplane, do the following at the point $(x_{1},x_{2})=(0,0)$:\n- Establish that $f$ is nondifferentiable at $(0,0)$ and characterize the set of subgradients $\\partial f(0,0)$ via the subgradient inequality.\n- Explain how each subgradient $g\\in\\partial f(0,0)$ yields a supporting hyperplane to $\\operatorname{epi}(f)$ at the point $(0,0,f(0,0))$, and describe the set of outward normal vectors to all such supporting hyperplanes.\n- Identify two extreme outward normal directions in this set corresponding to distinct subgradients that witness nondifferentiability, and compute the angle between these two extreme rays of outward normals.\n\nExpress the final angle in radians as an exact value. Your final answer must be a single closed-form analytic expression.", "solution": "The problem asks for an analysis of the convex function $f(x_{1},x_{2})=\\max\\{|x_{1}|,|x_{2}|\\}$ at the point $(x_{1},x_{2})=(0,0)$, focusing on its nondifferentiability, subgradients, and the geometry of the supporting hyperplanes to its epigraph.\n\nLet $x = (x_{1}, x_{2}) \\in \\mathbb{R}^{2}$. The function is $f(x) = \\max\\{|x_{1}|, |x_{2}|\\}$. The point of analysis is $x_0 = (0,0)$. At this point, $f(x_0) = f(0,0) = \\max\\{|0|,|0|\\} = 0$.\n\n**Part 1: Nondifferentiability and Subgradient Set**\n\nFirst, we establish that $f$ is nondifferentiable at $x_0 = (0,0)$. A function is differentiable at a point if its directional derivatives exist and are linear in the direction vector. The directional derivative of $f$ at $x_0$ in the direction $v=(v_{1},v_{2})$ is given by:\n$$ f'(x_0; v) = \\lim_{h\\to 0^{+}} \\frac{f(x_0+hv) - f(x_0)}{h} $$\nSubstituting $x_0=(0,0)$ and $f(0,0)=0$:\n$$ f'(0; v) = \\lim_{h\\to 0^{+}} \\frac{f(hv) - 0}{h} = \\lim_{h\\to 0^{+}} \\frac{\\max\\{|hv_{1}|, |hv_{2}|\\}}{h} $$\nSince $h>0$, we have $|h|=h$, so we can factor it out of the maximum function:\n$$ f'(0; v) = \\lim_{h\\to 0^{+}} \\frac{h\\max\\{|v_{1}|, |v_{2}|\\}}{h} = \\max\\{|v_{1}|, |v_{2}|\\} $$\nFor $f$ to be differentiable at $(0,0)$, there must exist a gradient vector $\\nabla f(0,0) = g \\in \\mathbb{R}^2$ such that $f'(0; v) = g \\cdot v$ for all $v \\in \\mathbb{R}^2$. This would require $\\max\\{|v_{1}|, |v_{2}|\\} = g_{1}v_{1} + g_{2}v_{2}$ for all $v_{1}, v_{2}$. Let's test this for two different directions:\n- For $v=(1,0)$, we get $\\max\\{|1|,|0|\\} = 1 = g_1(1) + g_2(0)$, so $g_1 = 1$.\n- For $v=(-1,0)$, we get $\\max\\{|-1|,|0|\\} = 1 = g_1(-1) + g_2(0)$, so $g_1 = -1$.\nSince $g_{1}$ cannot be simultaneously $1$ and $-1$, no such gradient vector $g$ exists. Therefore, $f$ is nondifferentiable at $(0,0)$.\n\nNext, we characterize the set of subgradients, $\\partial f(0,0)$. A vector $g=(g_1, g_2) \\in \\mathbb{R}^2$ is a subgradient of $f$ at $x_0=(0,0)$ if the following subgradient inequality holds for all $x=(x_1, x_2) \\in \\mathbb{R}^2$:\n$$ f(x) \\geq f(x_0) + g \\cdot (x - x_0) $$\nSubstituting $f(x)$, $x_0$, and $f(x_0)$:\n$$ \\max\\{|x_1|, |x_2|\\} \\geq g_1 x_1 + g_2 x_2 $$\nTo find the conditions on $g$, we must ensure this inequality holds for all $x$.\nLet's choose specific values for $x$ to derive necessary conditions on $g_1$ and $g_2$. Consider $x = (\\text{sgn}(g_1), \\text{sgn}(g_2))$. Note that $|\\text{sgn}(g_1)|$ is either $0$ or $1$, and similarly for $g_2$.\n$$ \\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\} \\geq g_1 \\text{sgn}(g_1) + g_2 \\text{sgn}(g_2) = |g_1| + |g_2| $$\nThe left side is $\\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\}$, which is $1$ if at least one of $g_1, g_2$ is non-zero, and $0$ if both are zero. In all cases, $\\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\} \\le 1$. The inequality requires $|g_1| + |g_2| \\leq \\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\}$. If we choose $x_1 = \\text{sgn}(g_1)$ and $x_2=0$, the inequality gives $|x_1| \\ge g_1 x_1 \\implies 1 \\ge |g_1|$, if $g_1 \\ne 0$. A better choice of $x$ is $x_1 = s \\cdot \\text{sgn}(g_1)$ and $x_2 = s \\cdot \\text{sgn}(g_2)$ for some $s>0$. Then the inequality becomes $s \\ge s(|g_1| + |g_2|)$, which implies $1 \\ge |g_1|+|g_2|$. This is a necessary condition.\n\nNow we show this condition is also sufficient. Assume $|g_1| + |g_2| \\le 1$. For any $x=(x_1, x_2)$, we have:\n$$ g_1 x_1 + g_2 x_2 \\le |g_1 x_1| + |g_2 x_2| = |g_1||x_1| + |g_2||x_2| $$\nLet $M = \\max\\{|x_1|, |x_2|\\}$. Then $|x_1| \\le M$ and $|x_2| \\le M$.\n$$ |g_1||x_1| + |g_2||x_2| \\le |g_1|M + |g_2|M = (|g_1| + |g_2|)M $$\nSince we assumed $|g_1| + |g_2| \\le 1$, we have $(|g_1| + |g_2|)M \\le 1 \\cdot M = M$.\nThus, $g_1 x_1 + g_2 x_2 \\le M = \\max\\{|x_1|, |x_2|\\}$, satisfying the subgradient inequality.\nSo, the set of subgradients is $\\partial f(0,0) = \\{ g=(g_1,g_2) \\in \\mathbb{R}^2 : |g_1| + |g_2| \\le 1 \\}$. This set is a square rotated by $45^\\circ$ with vertices at $(1,0)$, $(0,1)$, $(-1,0)$, and $(0,-1)$. Since this set is not a singleton, this confirms that $f$ is not differentiable at $(0,0)$.\n\n**Part 2: Supporting Hyperplanes to the Epigraph**\n\nThe epigraph of $f$ is the set $\\operatorname{epi}(f) = \\{ (x,t) \\in \\mathbb{R}^3 : t \\ge f(x) \\}$. The point on the boundary of the epigraph corresponding to $x_0=(0,0)$ is $(x_0, f(x_0)) = (0,0,0)$.\nA supporting hyperplane to $\\operatorname{epi}(f)$ at $(0,0,0)$ is a hyperplane passing through this point such that the entire set $\\operatorname{epi}(f)$ lies in one of the closed half-spaces defined by the hyperplane. Let the normal vector to the hyperplane be $a=(n_1, n_2, n_3) \\in \\mathbb{R}^3$, where $n=(n_1,n_2)$. The hyperplane equation is $n\\cdot x + n_3 t = 0$. The support condition is that for all $(x,t) \\in \\operatorname{epi}(f)$, we must have $n\\cdot x + n_3 t \\le 0$ (for an outward pointing normal).\n\nFor any $(x,t) \\in \\operatorname{epi}(f)$, we have $t \\ge f(x)$. Consider points of the form $(0,0,t)$ with $t \\ge f(0,0)=0$. For these points, the inequality becomes $n_3 t \\le 0$. Since this must hold for all $t \\ge 0$, we must have $n_3 \\le 0$. A non-trivial hyperplane requires a non-zero normal vector. If $n_3 = 0$, then $n \\cdot x \\le 0$ for all $x \\in \\mathbb{R}^2$, which implies $n=0$. Thus, for a non-trivial supporting hyperplane, $n_3  0$.\n\nLet's normalize the normal vector by setting $n_3 = -1$. The support condition becomes $n \\cdot x - t \\le 0$, which is $t \\ge n \\cdot x$. This must hold for all $(x,t)$ with $t \\ge f(x)$. This is true if and only if it holds for the smallest possible value of $t$, which is $t=f(x)$. So we require $f(x) \\ge n \\cdot x$ for all $x \\in \\mathbb{R}^2$.\nThis is precisely the subgradient inequality at $x_0=(0,0)$, where $n=g$.\nThus, for each subgradient $g \\in \\partial f(0,0)$, the vector $(g, -1)$ is an outward normal to a supporting hyperplane to $\\operatorname{epi}(f)$ at $(0,0,0)$. The set of all such outward normal vectors is:\n$$ N = \\{ (g_1, g_2, -1) \\in \\mathbb{R}^3 : |g_1| + |g_2| \\le 1 \\} $$\n\n**Part 3: Extreme Normals and Angle**\n\nThe set of subgradients $\\partial f(0,0)$ is a convex polygon. Its \"extreme\" points are its vertices. These give rise to the extreme supporting hyperplanes. The vertices of the subdifferential $\\partial f(0,0)$ are $g_A=(1,0)$, $g_B=(0,1)$, $g_C=(-1,0)$, and $g_D=(0,-1)$.\n\nThe problem asks to identify two extreme outward normal directions that witness nondifferentiability. The nondifferentiability of $f$ at $(0,0)$ can be demonstrated by showing the failure of linearity of the directional derivative $f'(0;v)$. A standard demonstration uses the standard basis vectors $v=e_1=(1,0)$ and $w=e_2=(0,1)$:\n$f'(0; e_1) = \\max\\{|1|,|0|\\} = 1$.\n$f'(0; e_2) = \\max\\{|0|,|1|\\} = 1$.\n$f'(0; e_1+e_2) = f'(0; (1,1)) = \\max\\{|1|,|1|\\} = 1$.\nIf $f$ were differentiable, we would have $f'(0; e_1+e_2) = f'(0; e_1)+f'(0; e_2) = 1+1=2$. Since $1 \\neq 2$, $f$ is nondifferentiable.\nThe directional derivative is also given by $f'(x; v) = \\max_{g \\in \\partial f(x)} g \\cdot v$. The subgradients that achieve the maximum for $v=e_1$ and $v=e_2$ are $g_A=(1,0)$ and $g_B=(0,1)$, respectively. These two extreme subgradients are thus natural \"witnesses\" to the nondifferentiability.\n\nWe therefore identify the two extreme subgradients as $g_A=(1,0)$ and $g_B=(0,1)$. The corresponding extreme outward normal vectors are:\n$n_A = (g_A, -1) = (1, 0, -1)$\n$n_B = (g_B, -1) = (0, 1, -1)$\n\nThe angle $\\theta$ between these two vectors is found using the dot product formula $\\cos\\theta = \\frac{n_A \\cdot n_B}{\\|n_A\\| \\|n_B\\|}$.\nThe dot product is:\n$$ n_A \\cdot n_B = (1)(0) + (0)(1) + (-1)(-1) = 0 + 0 + 1 = 1 $$\nThe magnitudes of the vectors are:\n$$ \\|n_A\\| = \\sqrt{1^2 + 0^2 + (-1)^2} = \\sqrt{2} $$\n$$ \\|n_B\\| = \\sqrt{0^2 + 1^2 + (-1)^2} = \\sqrt{2} $$\nThe cosine of the angle is:\n$$ \\cos\\theta = \\frac{1}{\\sqrt{2} \\cdot \\sqrt{2}} = \\frac{1}{2} $$\nThe angle $\\theta$ in radians is:\n$$ \\theta = \\arccos\\left(\\frac{1}{2}\\right) = \\frac{\\pi}{3} $$", "answer": "$$\\boxed{\\frac{\\pi}{3}}$$", "id": "3179819"}, {"introduction": "The theory of separating hyperplanes finds one of its most powerful applications in machine learning, specifically in the design of Support Vector Machines (SVMs). This practice will challenge you to prove the deep connection between finding a maximum-margin classifier and computing the shortest distance between the convex hulls of two sets of data points. By implementing the solution numerically, you will verify this remarkable equivalence and see how geometric principles directly translate into a practical data-science algorithm.", "problem": "Let $A,B \\subset \\mathbb{R}^n$ be two finite, disjoint point clouds. Consider the problem of finding a linear classifier that separates $A$ and $B$ with the largest possible margin. Begin from the following foundational definitions and facts:\n\n- The convex hull of a set $S \\subset \\mathbb{R}^n$, denoted $\\operatorname{conv}(S)$, is the set of all convex combinations of points in $S$, that is, $\\operatorname{conv}(S) = \\left\\{ \\sum_{i=1}^m \\alpha_i s_i \\,\\middle|\\, s_i \\in S, \\alpha_i \\ge 0, \\sum_{i=1}^m \\alpha_i = 1 \\right\\}$.\n- A hyperplane in $\\mathbb{R}^n$ is the set $\\{ x \\in \\mathbb{R}^n \\mid \\langle w, x \\rangle + b = 0 \\}$ for some $w \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product. The Euclidean norm of $w$ is $\\|w\\| = \\sqrt{\\langle w, w \\rangle}$.\n- The margin of a linear classifier $(w,b)$ separating labeled points $\\{(x_i, y_i)\\}$ with $y_i \\in \\{+1,-1\\}$ is the minimal distance from the hyperplane $\\{x \\mid \\langle w, x \\rangle + b = 0\\}$ to any training point, expressed in the units of the Euclidean metric. For hard-margin separation, one may scale $(w,b)$ so that $y_i (\\langle w, x_i \\rangle + b) \\ge 1$ for all $i$; under this scaling, the margin equals $1/\\|w\\|$.\n- If two nonempty, disjoint, closed, convex sets $C,D \\subset \\mathbb{R}^n$ are given, the hyperplane separation theorem ensures there exists a separating hyperplane, and the minimal Euclidean distance $\\operatorname{dist}(C,D) = \\inf\\{ \\|x - y\\| \\mid x \\in C, y \\in D \\}$ is attained when $C$ and $D$ are compact.\n\nTask 1 (Derivation): Using only the above foundational definitions and facts, derive why the hard-margin maximum-margin linear classifier for two disjoint finite sets $A$ and $B$ is equivalent to maximizing the Euclidean distance between $\\operatorname{conv}(A)$ and $\\operatorname{conv}(B)$. Specifically, show that the hyperplane achieving the maximum margin has its normal vector aligned with the line segment connecting the closest pair of points between $\\operatorname{conv}(A)$ and $\\operatorname{conv}(B)$, and that the margin equals half of the minimal distance between these two convex hulls. Your derivation must explicitly establish the relationship between the geometric margin and the separation of convex hulls, starting from the definitions of convex hull, hyperplane, margin, and Euclidean distance, and relying on well-tested facts such as hyperplane separation.\n\nTask 2 (Algorithm Design): Design two optimization-based computations grounded in convex optimization principles:\n\n- Compute the hard-margin maximum-margin classifier by minimizing the squared Euclidean norm $\\frac{1}{2}\\|w\\|^2$ subject to the separation constraints $y_i(\\langle w, x_i \\rangle + b) \\ge 1$ for all training points $x_i \\in A \\cup B$, where $y_i = +1$ for $x_i \\in A$ and $y_i = -1$ for $x_i \\in B$. Report the margin as $1/\\|w\\|$.\n- Compute the minimal Euclidean distance between $\\operatorname{conv}(A)$ and $\\operatorname{conv}(B)$ by solving the convex optimization problem\n$$\n\\min_{\\alpha \\in \\mathbb{R}^{|A|},\\, \\beta \\in \\mathbb{R}^{|B|}} \\left\\| \\sum_{i=1}^{|A|} \\alpha_i a_i - \\sum_{j=1}^{|B|} \\beta_j b_j \\right\\|^2 \\quad \\text{subject to} \\quad \\alpha_i \\ge 0, \\sum_{i=1}^{|A|} \\alpha_i = 1, \\ \\beta_j \\ge 0, \\sum_{j=1}^{|B|} \\beta_j = 1,\n$$\nwhere $\\{a_i\\}$ are the points of $A$ and $\\{b_j\\}$ are the points of $B$. Report the Euclidean distance as the square root of the optimal objective value.\n\nTask 3 (Numerical Experiment and Test Suite): Implement a single program that, for each of the following test cases, computes:\n- the margin $m$ from the hard-margin classifier as described above, and\n- the minimal distance $d$ between $\\operatorname{conv}(A)$ and $\\operatorname{conv}(B)$ as described above,\nthen returns, for each case, the absolute difference $|d - 2m|$ and a boolean indicating whether $|d - 2m|$ is less than or equal to a tolerance $\\tau = 10^{-4}$.\n\nUse the following test suite of point clouds (all coordinates are unitless):\n\n- Test Case 1 (two-dimensional, well separated):\n$A = \\{(0.0,0.0), (0.0,1.0), (0.8,0.2), (0.5,0.9)\\}$ and $B = \\{(3.0,0.1), (3.2,1.1), (3.8,0.4), (4.0,0.9)\\}$.\n- Test Case 2 (three-dimensional, small separation):\n$A = \\{(-0.2,-0.1,0.0), (0.0,0.2,-0.1), (0.1,-0.2,0.1), (-0.1,0.0,0.2)\\}$ and $B = \\{(0.6,0.1,0.0), (0.7,-0.2,0.1), (0.8,0.2,-0.1), (0.9,0.0,0.2)\\}$.\n- Test Case 3 (two-dimensional, multiple support vectors):\n$A = \\{(0.0,-0.2), (0.0,0.2), (1.0,-0.2), (1.0,0.2), (0.5,-0.2), (0.5,0.2), (0.25,0.2), (0.75,-0.2)\\}$ and $B = \\{(2.0,-0.25), (2.0,0.25), (3.0,-0.25), (3.0,0.25), (2.5,-0.25), (2.5,0.25), (2.25,0.25), (2.75,-0.25)\\}$.\n\nFinal Output Format: Your program should produce a single line of output containing the aggregated results in the following exact format:\n- A single nested list with two elements:\n1. A list of three floating-point numbers representing the absolute differences $|d - 2m|$ for the three test cases, in the order given.\n2. A list of three boolean values indicating whether each absolute difference is less than or equal to $\\tau$.\nFor example, the output must look like\n$[[e_1,e_2,e_3],[p_1,p_2,p_3]]$\nwith no extra text.\n\nAngles, physical units, or percentages do not apply to this problem; all quantities are unitless real numbers. The program must be self-contained, require no input, and follow the exact output format described above.", "solution": "We begin with the core definitions and facts. For finite disjoint point clouds $A,B \\subset \\mathbb{R}^n$, their convex hulls $\\operatorname{conv}(A)$ and $\\operatorname{conv}(B)$ are compact, convex subsets of $\\mathbb{R}^n$. Since they are disjoint, by the hyperplane separation theorem, there exist parallel supporting hyperplanes that separate these sets. The minimal Euclidean distance between two compact convex sets is attained by at least one pair $(p^\\star, q^\\star) \\in \\operatorname{conv}(A) \\times \\operatorname{conv}(B)$, and the line segment connecting $p^\\star$ and $q^\\star$ is orthogonal to a pair of parallel supporting hyperplanes at these points.\n\nWe define a linear classifier by $(w,b)$, with decision boundary $\\{x \\mid \\langle w, x \\rangle + b = 0\\}$. For hard-margin separation of two labeled sets, we can scale $(w,b)$ so that $y_i(\\langle w, x_i \\rangle + b) \\ge 1$ for all $i$ with $y_i \\in \\{+1,-1\\}$, where $y_i=+1$ for $x_i \\in A$ and $y_i=-1$ for $x_i \\in B$. Under this scaling, the geometric margin equals $1/\\|w\\|$, which is the Euclidean distance from the hyperplane to the closest training points in either class. The maximum-margin classifier minimizes $\\frac{1}{2}\\|w\\|^2$ subject to these separation constraints.\n\nWe now connect the hyperplane margin to the distance between convex hulls. Consider the optimization problem defining the minimal distance between the convex hulls:\n$$\n\\min_{\\alpha, \\beta} \\left\\| \\sum_{i=1}^{|A|} \\alpha_i a_i - \\sum_{j=1}^{|B|} \\beta_j b_j \\right\\|^2 \\quad \\text{subject to} \\quad \\alpha \\ge 0, \\sum_i \\alpha_i = 1, \\ \\beta \\ge 0, \\sum_j \\beta_j = 1,\n$$\nwith optimal solution $(\\alpha^\\star,\\beta^\\star)$ and corresponding points $p^\\star = \\sum_i \\alpha_i^\\star a_i \\in \\operatorname{conv}(A)$ and $q^\\star = \\sum_j \\beta_j^\\star b_j \\in \\operatorname{conv}(B)$. The vector $v^\\star = p^\\star - q^\\star$ realizes the minimal distance $d = \\|v^\\star\\|$. Since the sets are convex, the first-order optimality conditions for this convex problem imply that there exist parallel supporting hyperplanes orthogonal to $v^\\star$ that touch the sets at $p^\\star$ and $q^\\star$ respectively. Specifically, consider the pair of parallel hyperplanes orthogonal to $v^\\star$ placed so that they pass through $p^\\star$ and $q^\\star$. The midpoint hyperplane orthogonal to $v^\\star$ and bisecting the segment $[p^\\star,q^\\star]$ separates $\\operatorname{conv}(A)$ and $\\operatorname{conv}(B)$, and hence also $A$ and $B$, because the convex hulls are disjoint.\n\nChoose $w$ collinear with $v^\\star$, namely $w = \\lambda v^\\star$ for some $\\lambda  0$, and $b$ such that the two parallel planes $\\langle w, x \\rangle + b = \\pm 1$ pass through $p^\\star$ and $q^\\star$. The distance between these two planes, measured along the normal direction, equals $\\frac{2}{\\|w\\|}$ because the signed distance from the hyperplane $\\langle w, x \\rangle + b = 0$ to either plane is $\\frac{1}{\\|w\\|}$ under the scaling $y_i(\\langle w, x_i \\rangle + b) \\ge 1$. The distances from points $p^\\star$ and $q^\\star$ to the central hyperplane are equal in magnitude and opposite in sign. By construction, the segment connecting $p^\\star$ and $q^\\star$ is orthogonal to the hyperplanes, so the Euclidean distance $\\|p^\\star - q^\\star\\|$ equals the separation between the two margin-defining planes, which is $2/\\|w\\|$. Therefore, for the hyperplane normal aligned with $v^\\star$ and scaled to satisfy the hard-margin constraints, the margin is\n$$\nm = \\frac{1}{\\|w\\|} = \\frac{1}{2} \\|p^\\star - q^\\star\\| = \\frac{1}{2} \\operatorname{dist}(\\operatorname{conv}(A), \\operatorname{conv}(B)).\n$$\nIf we consider any other separating hyperplane $(\\tilde{w},\\tilde{b})$, its margin cannot exceed the half-distance between the convex hulls, because the closest feasible margin planes must be at least as far apart as the minimal separation realized by the closest points between the convex hulls. Maximizing the margin is thus equivalent to maximizing the separation between the convex hulls, but since the convex hulls are fixed, the maximum margin equals half of their minimal Euclidean separation. This establishes the equivalence.\n\nAlgorithmically, we compute the margin by solving the convex quadratic program for the hard-margin classifier:\n$$\n\\min_{w \\in \\mathbb{R}^n,\\, b \\in \\mathbb{R}} \\ \\frac{1}{2}\\|w\\|^2 \\quad \\text{subject to} \\quad y_i(\\langle w, x_i \\rangle + b) \\ge 1 \\ \\text{for all } i,\n$$\nwhich is convex because the objective is a strictly convex quadratic and the constraints are affine inequalities. The margin is $m = 1/\\|w^\\star\\|$ where $(w^\\star,b^\\star)$ is the optimizer.\n\nWe compute the minimal distance between convex hulls by solving\n$$\n\\min_{\\alpha \\in \\mathbb{R}^{|A|},\\, \\beta \\in \\mathbb{R}^{|B|}} \\left\\| \\sum_{i=1}^{|A|} \\alpha_i a_i - \\sum_{j=1}^{|B|} \\beta_j b_j \\right\\|^2\n$$\nsubject to the simplex constraints that $\\alpha$ and $\\beta$ are nonnegative and each sum to $1$. This is a convex quadratic program over the product of simplices, and hence has a unique minimizer when the quadratic form is strictly convex on the feasible set. The Euclidean distance is $d = \\sqrt{\\text{optimal objective}}$.\n\nFor each test case, we report the absolute difference $|d - 2m|$ and a boolean that checks whether this difference is within a tolerance $\\tau = 10^{-4}$. The numerical experiment is constructed to verify the derived equivalence across diverse configurations: two-dimensional well-separated clusters (general case), three-dimensional clusters with small separation (near-boundary case with small margin), and two-dimensional rectangles with multiple points on the supporting faces (edge case with multiple support vectors). All quantities are unitless, and no angles or physical units are involved.\n\nThe final program implements both convex optimization problems using Sequential Least Squares Quadratic Programming, computes $m$ and $d$, and prints a single nested list as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef hard_margin_svm_margin(A, B):\n    \"\"\"\n    Compute the hard-margin SVM margin for two disjoint point sets A (label +1) and B (label -1).\n    Solves: minimize 0.5 * ||w||^2 subject to y_i * (w^T x_i + b) = 1 for all i.\n    Returns: margin m = 1 / ||w||.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    n = A.shape[1]\n    # Build dataset\n    X_pos = A\n    X_neg = B\n    X = np.vstack([X_pos, X_neg])\n    y = np.hstack([np.ones(len(X_pos)), -np.ones(len(X_neg))])\n\n    # Objective: 0.5 * ||w||^2\n    def objective(vars_):\n        w = vars_[:-1]\n        return 0.5 * np.dot(w, w)\n\n    # Gradient of objective\n    def objective_jac(vars_):\n        w = vars_[:-1]\n        grad_w = w\n        grad_b = 0.0\n        return np.concatenate([grad_w, [grad_b]])\n\n    # Inequality constraints: y_i * (w^T x_i + b) - 1 = 0\n    constraints = []\n    for i in range(len(X)):\n        xi = X[i]\n        yi = y[i]\n        def fun_factory(xi, yi):\n            def fun(vars_):\n                w = vars_[:-1]\n                b = vars_[-1]\n                return yi * (np.dot(w, xi) + b) - 1.0\n            return fun\n        def jac_factory(xi, yi):\n            def jac(vars_):\n                # Gradient w.r.t. w is yi * xi, w.r.t. b is yi\n                return np.concatenate([yi * xi, [yi]])\n            return jac\n        constraints.append({'type': 'ineq', 'fun': fun_factory(xi, yi), 'jac': jac_factory(xi, yi)})\n\n    # Initial guess: normal pointing from class B mean to class A mean; b at mid-plane\n    muA = np.mean(A, axis=0)\n    muB = np.mean(B, axis=0)\n    w0 = muA - muB\n    norm_w0 = np.linalg.norm(w0)\n    if norm_w0  1e-8:\n        # Fallback to a canonical direction if means coincide\n        w0 = np.zeros(n)\n        w0[0] = 1.0\n    b0 = -0.5 * np.dot(w0, muA + muB)\n    x0 = np.concatenate([w0, [b0]])\n\n    res = minimize(objective, x0, method='SLSQP', jac=objective_jac,\n                   constraints=constraints, options={'ftol': 1e-12, 'maxiter': 1000, 'disp': False})\n    if not res.success:\n        raise RuntimeError(f\"SVM optimization failed: {res.message}\")\n    w_opt = res.x[:-1]\n    norm_w = np.linalg.norm(w_opt)\n    if norm_w = 0:\n        raise RuntimeError(\"Optimized w has zero norm, margin undefined.\")\n    margin = 1.0 / norm_w\n    return margin\n\ndef convex_hull_distance(A, B):\n    \"\"\"\n    Compute the minimal Euclidean distance between conv(A) and conv(B).\n    Solve QP over simplices:\n        min ||sum_i alpha_i a_i - sum_j beta_j b_j||^2\n        s.t. alpha = 0, sum(alpha) = 1; beta = 0, sum(beta) = 1.\n    Returns: distance d = 0.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    mA, nA = A.shape\n    mB, nB = B.shape\n    assert nA == nB, \"A and B must have same dimension.\"\n\n    # Precompute Gram matrices\n    AA = A @ A.T          # shape (mA, mA)\n    BB = B @ B.T          # shape (mB, mB)\n    AB = A @ B.T          # shape (mA, mB)\n\n    # Objective in terms of z = [alpha, beta]\n    def obj(z):\n        alpha = z[:mA]\n        beta = z[mA:]\n        # Quadratic form: ||A^T alpha - B^T beta||^2\n        term = alpha @ (AA @ alpha) - 2.0 * alpha @ (AB @ beta) + beta @ (BB @ beta)\n        return term\n\n    def obj_jac(z):\n        alpha = z[:mA]\n        beta = z[mA:]\n        grad_alpha = 2.0 * (AA @ alpha) - 2.0 * (AB @ beta)\n        grad_beta = 2.0 * (BB @ beta) - 2.0 * (AB.T @ alpha)\n        return np.concatenate([grad_alpha, grad_beta])\n\n    # Equality constraints: sum(alpha)=1, sum(beta)=1\n    def con_alpha_fun(z):\n        alpha = z[:mA]\n        return np.sum(alpha) - 1.0\n\n    def con_alpha_jac(z):\n        jac = np.zeros(mA + mB)\n        jac[:mA] = 1.0\n        return jac\n\n    def con_beta_fun(z):\n        beta = z[mA:]\n        return np.sum(beta) - 1.0\n\n    def con_beta_jac(z):\n        jac = np.zeros(mA + mB)\n        jac[mA:] = 1.0\n        return jac\n\n    constraints = [\n        {'type': 'eq', 'fun': con_alpha_fun, 'jac': con_alpha_jac},\n        {'type': 'eq', 'fun': con_beta_fun, 'jac': con_beta_jac},\n    ]\n    # Bounds: alpha_i = 0, beta_j = 0\n    bounds = [(0.0, None)] * (mA + mB)\n\n    # Initial guess: uniform weights on each simplex\n    alpha0 = np.full(mA, 1.0 / mA)\n    beta0 = np.full(mB, 1.0 / mB)\n    z0 = np.concatenate([alpha0, beta0])\n\n    res = minimize(obj, z0, method='SLSQP', jac=obj_jac,\n                   bounds=bounds, constraints=constraints,\n                   options={'ftol': 1e-12, 'maxiter': 2000, 'disp': False})\n    if not res.success:\n        raise RuntimeError(f\"Convex hull distance optimization failed: {res.message}\")\n    val = res.fun\n    if val  0:\n        # numerical guard\n        val = max(val, 0.0)\n    d = np.sqrt(val)\n    return d\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: 2D well separated\n        (\n            np.array([[0.0, 0.0],\n                      [0.0, 1.0],\n                      [0.8, 0.2],\n                      [0.5, 0.9]]),\n            np.array([[3.0, 0.1],\n                      [3.2, 1.1],\n                      [3.8, 0.4],\n                      [4.0, 0.9]])\n        ),\n        # Test Case 2: 3D small separation\n        (\n            np.array([[-0.2, -0.1, 0.0],\n                      [ 0.0,  0.2, -0.1],\n                      [ 0.1, -0.2,  0.1],\n                      [-0.1,  0.0,  0.2]]),\n            np.array([[0.6,  0.1,  0.0],\n                      [0.7, -0.2,  0.1],\n                      [0.8,  0.2, -0.1],\n                      [0.9,  0.0,  0.2]])\n        ),\n        # Test Case 3: 2D rectangles with multiple support vectors\n        (\n            np.array([[0.0, -0.2],\n                      [0.0,  0.2],\n                      [1.0, -0.2],\n                      [1.0,  0.2],\n                      [0.5, -0.2],\n                      [0.5,  0.2],\n                      [0.25, 0.2],\n                      [0.75, -0.2]]),\n            np.array([[2.0, -0.25],\n                      [2.0,  0.25],\n                      [3.0, -0.25],\n                      [3.0,  0.25],\n                      [2.5, -0.25],\n                      [2.5,  0.25],\n                      [2.25, 0.25],\n                      [2.75, -0.25]])\n        ),\n    ]\n\n    tau = 1e-4\n    errors = []\n    passes = []\n    for A, B in test_cases:\n        # Compute margin via hard-margin SVM\n        m = hard_margin_svm_margin(A, B)\n        # Compute distance between convex hulls\n        d = convex_hull_distance(A, B)\n        # Compare d and 2*m\n        err = abs(d - 2.0 * m)\n        errors.append(err)\n        passes.append(err = tau)\n\n    # Final print statement in the exact required format.\n    # Nested list: [[e1,e2,e3],[p1,p2,p3]]\n    # Use repr for booleans and floats consistency in join\n    errors_str = \",\".join(f\"{e:.10f}\" for e in errors)\n    passes_str = \",\".join(\"True\" if p else \"False\" for p in passes)\n    print(f\"[[{errors_str}],[{passes_str}]]\")\n\nsolve()\n```", "id": "3114075"}]}