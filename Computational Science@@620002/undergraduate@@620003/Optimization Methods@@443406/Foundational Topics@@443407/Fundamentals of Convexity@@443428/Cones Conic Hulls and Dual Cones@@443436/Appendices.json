{"hands_on_practices": [{"introduction": "The abstract definition of a conic hull can be made concrete through the lens of linear algebra and optimization. This practice guides you through formulating the geometric question of membership as a linear programming feasibility problem, providing an algorithmic test. Furthermore, it demonstrates that when a vector is not in the cone, duality theory gives us a powerful certificate of non-membership: a separating hyperplane. [@problem_id:3110862]", "problem": "You are given a finite family of rays in a real vector space, represented by vectors $\\{r_i\\}_{i=1}^m \\subset \\mathbb{R}^n$. The conic hull (also called the finitely generated cone) of these rays is defined from first principles as the set\n$$\n\\operatorname{cone}(\\{r_i\\}_{i=1}^m) \\;=\\; \\left\\{ \\sum_{i=1}^m \\lambda_i r_i \\;:\\; \\lambda_i \\in \\mathbb{R}_{\\ge 0} \\right\\}.\n$$\nGiven a target vector $y \\in \\mathbb{R}^n$, your task is to decide algorithmically whether $y \\in \\operatorname{cone}(\\{r_i\\})$, and to interpret a dual certificate when $y \\notin \\operatorname{cone}(\\{r_i\\})$.\n\nFundamental base and requirements:\n- Use the core definition of a conic hull and the standard linear programming feasibility viewpoint: membership $y \\in \\operatorname{cone}(\\{r_i\\})$ is equivalent to the existence of nonnegative coefficients $\\lambda \\in \\mathbb{R}^m_{\\ge 0}$ satisfying the linear equality $\\sum_{i=1}^m \\lambda_i r_i = y$.\n- Frame this as a linear program with decision variables $\\lambda \\in \\mathbb{R}^m$: minimize $0$ subject to $R \\lambda = y$, $\\lambda \\ge 0$, where $R \\in \\mathbb{R}^{n \\times m}$ has columns $r_1, \\dots, r_m$.\n- If the above problem is infeasible, use a dual-separation viewpoint grounded in the Separating Hyperplane Theorem or Farkas' Lemma to find a separating vector $w \\in \\mathbb{R}^n$ satisfying $w^\\top r_i \\ge 0$ for all $i$ and $w^\\top y < 0$. To obtain a concrete certificate via linear programming, solve a feasibility linear program in $w$:\n  - Find $w \\in \\mathbb{R}^n$ such that $R^\\top w \\le 0$ and $y^\\top w \\ge 1$; then set $w_{\\mathrm{sep}} := -w$. This ensures $w_{\\mathrm{sep}}^\\top r_i \\ge 0$ for all $i$ and $w_{\\mathrm{sep}}^\\top y \\le -1 < 0$, which is a strict separating hyperplane certificate.\n\nYour program must implement this approach using linear programming and produce the following for each test case:\n- A boolean indicating whether $y \\in \\operatorname{cone}(\\{r_i\\})$.\n- If the answer is true, provide one feasible vector of coefficients $\\lambda \\in \\mathbb{R}^m_{\\ge 0}$ and also report the Euclidean residual norm $\\lVert R \\lambda - y \\rVert_2$.\n- If the answer is false, provide one separating vector $w_{\\mathrm{sep}} \\in \\mathbb{R}^n$ and also report the minimum margin over generators $\\min_i \\{ w_{\\mathrm{sep}}^\\top r_i \\}$ and the value $w_{\\mathrm{sep}}^\\top y$.\n\nTest suite:\nWork in $\\mathbb{R}^2$ and use the following five test cases. Each case consists of a list of rays and a target vector $y$.\n1. Rays $\\{(1,0), (0,1)\\}$, target $y = (1,2)$.\n2. Rays $\\{(1,0), (0,1)\\}$, target $y = (-1,1)$.\n3. Rays $\\{(1,0), (0,1)\\}$, target $y = (0,0)$.\n4. Rays $\\{(1,1), (1,-1)\\}$, target $y = (2,0)$.\n5. Rays $\\{(1,1), (1,-1)\\}$, target $y = (1,1.2)$.\n\nAll vectors should be interpreted as elements of $\\mathbb{R}^2$ with no physical units. Angles do not appear, and percentages are not used.\n\nProgram output specification:\n- For each test case, output a list with the following structure:\n  - Case result format: $[\\text{is\\_member}, \\lambda\\_\\text{list}, w\\_\\text{sep\\_list}, \\text{verification\\_list}]$,\n    - If $\\text{is\\_member}$ is true, then $\\lambda\\_\\text{list}$ is a list of the coefficients $\\lambda_i$, $w\\_\\text{sep\\_list}$ is the empty list, and $\\text{verification\\_list}$ contains a single float equal to $\\lVert R \\lambda - y \\rVert_2$.\n    - If $\\text{is\\_member}$ is false, then $\\lambda\\_\\text{list}$ is the empty list, $w\\_\\text{sep\\_list}$ is one separating vector $w_{\\mathrm{sep}}$, and $\\text{verification\\_list}$ contains two floats: $\\min_i \\{ w_{\\mathrm{sep}}^\\top r_i \\}$ and $w_{\\mathrm{sep}}^\\top y$.\n- Your program should produce a single line of output containing the results for all five cases as a comma-separated list enclosed in square brackets. Concretely, the output must be a single line in the form:\n  $$\n  [\\text{case1\\_result},\\text{case2\\_result},\\text{case3\\_result},\\text{case4\\_result},\\text{case5\\_result}]\n  $$\nwhere each $\\text{caseX\\_result}$ is itself a list as specified above.\n\nDesign for coverage and edge cases:\n- Case 1 is a typical membership instance.\n- Case 2 is a typical non-membership instance with a clear axis-aligned separator.\n- Case 3 is a boundary case ($y = 0$).\n- Case 4 is membership in a non-orthant cone.\n- Case 5 is non-membership in a non-orthant cone with a nontrivial separating hyperplane.\n\nYour solution must rely only on fundamental definitions of conic hulls and the feasibility of linear equalities with nonnegativity constraints, and must obtain certificates using linear programming. No external inputs are allowed; all data are hard-coded as above. The final numerical outputs should be rounded to a reasonable number of decimal places to ensure readability while preserving correctness of signs and zero-versus-nonzero distinctions.", "solution": "The problem requires an algorithmic procedure to determine if a given vector $y \\in \\mathbb{R}^n$ lies within the conic hull of a finite set of vectors $\\{r_i\\}_{i=1}^m \\subset \\mathbb{R}^n$. The conic hull, denoted $\\operatorname{cone}(\\{r_i\\})$, is the set of all non-negative linear combinations of the generating vectors:\n$$\n\\operatorname{cone}(\\{r_i\\}_{i=1}^m) \\;=\\; \\left\\{ \\sum_{i=1}^m \\lambda_i r_i \\;:\\; \\lambda_i \\in \\mathbb{R}_{\\ge 0} \\right\\}\n$$\nA vector $y$ is in this cone if and only if there exist coefficients $\\lambda = (\\lambda_1, \\dots, \\lambda_m)^\\top$ such that:\n$$\n\\sum_{i=1}^m \\lambda_i r_i = y, \\quad \\text{with } \\lambda_i \\ge 0 \\text{ for all } i=1, \\dots, m\n$$\nThis can be expressed in matrix form as $R\\lambda = y$, where $R \\in \\mathbb{R}^{n \\times m}$ is the matrix whose columns are the vectors $r_i$. The condition for membership is thus the existence of a solution to a system of linear equations with non-negativity constraints on the variables.\n\nThis feasibility problem can be formulated as a linear program (LP). Specifically, we seek to find a vector $\\lambda \\in \\mathbb{R}^m$ that satisfies the constraints, where the choice of objective function is immaterial for a feasibility problem. A standard choice is to minimize a zero objective function, $c^\\top \\lambda = 0^\\top \\lambda = 0$:\n\n**Primal LP (Membership Problem):**\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & 0 \\\\\n\\text{subject to} \\quad & R \\lambda = y \\\\\n& \\lambda \\ge 0\n\\end{aligned}\n$$\nIf this LP has a feasible solution, then such a $\\lambda$ exists, and $y \\in \\operatorname{cone}(\\{r_i\\})$. The vector $\\lambda$ itself serves as a certificate of membership. The quality of this certificate can be checked by computing the Euclidean residual norm, $\\lVert R \\lambda - y \\rVert_2$, which should be nearly zero for a numerically accurate solution.\n\nIf the primal LP is infeasible, then no such non-negative $\\lambda$ exists, and $y \\notin \\operatorname{cone}(\\{r_i\\})$. By the principles of duality in linear programming, specifically Farkas' Lemma (a theorem of the alternative), the infeasibility of the primal system implies the feasibility of a related dual system. This leads to the concept of a separating hyperplane. A hyperplane defined by a normal vector $w_{\\mathrm{sep}} \\in \\mathbb{R}^n$ separates $y$ from the cone if $w_{\\mathrm{sep}}^\\top y < 0$ and $w_{\\mathrm{sep}}^\\top z \\ge 0$ for all $z \\in \\operatorname{cone}(\\{r_i\\})$. For the latter condition to hold, it is sufficient that $w_{\\mathrm{sep}}^\\top r_i \\ge 0$ for all generating vectors $r_i$.\n\nThe problem statement provides a specific LP formulation to find such a separating vector. We seek an intermediate vector $w \\in \\mathbb{R}^n$ that satisfies a set of inequalities, from which $w_{\\mathrm{sep}}$ is derived.\n\n**Separation LP (Non-Membership Certificate):**\n$$\n\\begin{aligned}\n\\text{find} \\quad & w \\in \\mathbb{R}^n \\\\\n\\text{subject to} \\quad & R^\\top w \\le 0 \\\\\n& y^\\top w \\ge 1\n\\end{aligned}\n$$\nThis is also a feasibility LP. We can rewrite the constraint $y^\\top w \\ge 1$ as $(-y)^\\top w \\le -1$ to fit the standard form $A_w w \\le b_w$. If a solution $w$ to this system exists, we define the separating vector as $w_{\\mathrm{sep}} := -w$. Let us verify that this $w_{\\mathrm{sep}}$ has the desired properties:\n$1$. For each generator $r_i$, the constraint is $r_i^\\top w \\le 0$. Then $w_{\\mathrm{sep}}^\\top r_i = (-w)^\\top r_i = -(w^\\top r_i) \\ge 0$. This confirms that $w_{\\mathrm{sep}}$ forms a non-negative inner product with all generators, and thus with all vectors in the cone.\n$2$. The constraint $y^\\top w \\ge 1$ implies $w_{\\mathrm{sep}}^\\top y = (-w)^\\top y = -(y^\\top w) \\le -1$. This is a strictly negative value, confirming that $y$ is on the opposite side of the hyperplane from the cone.\n\nThe overall algorithm is as follows:\n$1$. Formulate and attempt to solve the primal LP for the coefficients $\\lambda$.\n$2$. If the primal LP is feasible (successful), conclude that $y$ is in the cone. The solution vector $\\lambda$ is the membership certificate.\n$3$. If the primal LP is infeasible, conclude that $y$ is not in the cone. Formulate and solve the separation LP for the vector $w$. The derived vector $w_{\\mathrm{sep}} = -w$ is the non-membership certificate.\n\nThis procedure will be implemented for each test case using a numerical linear programming solver.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve_case(rays, y):\n    \"\"\"\n    Determines if a vector y is in the conic hull of a set of rays.\n\n    Args:\n        rays (list of tuples): A list of vectors representing the rays.\n        y (np.ndarray): The target vector.\n\n    Returns:\n        list: A result list in the format specified by the problem.\n    \"\"\"\n    R = np.array(rays).T\n    if R.ndim == 1: # Handle case of a single ray (1D vector)\n        R = R.reshape(-1, 1)\n        \n    n_dim, m_rays = R.shape\n\n    # --- Primal LP (Membership Test) ---\n    # Try to find lambda >= 0 such that R @ lambda = y.\n    # min 0 subject to R @ lambda = y, lambda >= 0\n    c_primal = np.zeros(m_rays)\n    # The bounds argument enforces lambda >= 0 by default when set to (0, None).\n    res_primal = linprog(\n        c=c_primal,\n        A_eq=R,\n        b_eq=y,\n        bounds=(0, None),\n        method='highs'\n    )\n\n    if res_primal.success:\n        # y is in the cone.\n        lambda_coeffs = res_primal.x\n        residual = np.linalg.norm(R @ lambda_coeffs - y)\n        return [\n            True,\n            list(np.round(lambda_coeffs, 5)),\n            [],\n            [round(residual, 5)]\n        ]\n    else:\n        # y is not in the cone. Find a separating hyperplane.\n        # --- Separation LP (Non-Membership Certificate) ---\n        # Find w such that R.T @ w = 0 and y.T @ w >= 1.\n        # We rewrite y.T @ w >= 1 as -y.T @ w = -1.\n        # This is equivalent to min 0 subject to A_ub @ w = b_ub.\n        c_dual = np.zeros(n_dim)\n        \n        # A_ub has shape (m_rays + 1, n_dim)\n        A_ub_dual = np.vstack([R.T, -y.reshape(1, -1)])\n        \n        # b_ub has shape (m_rays + 1,)\n        b_ub_dual = np.zeros(m_rays + 1)\n        b_ub_dual[-1] = -1.0\n        \n        res_dual = linprog(\n            c=c_dual,\n            A_ub=A_ub_dual,\n            b_ub=b_ub_dual,\n            bounds=(None, None), # w is unbounded\n            method='highs'\n        )\n\n        if res_dual.success:\n            w = res_dual.x\n            w_sep = -w\n            \n            # verification_list: [min_margin, w_sep_dot_y]\n            min_margin = np.min(w_sep @ R)\n            w_sep_dot_y = w_sep @ y\n            \n            return [\n                False,\n                [],\n                list(np.round(w_sep, 5)),\n                [round(min_margin, 5), round(w_sep_dot_y, 5)]\n            ]\n        else:\n            # According to Farkas' Lemma, this case should not be reached\n            # if the primal is infeasible. This indicates a numerical issue.\n            return [\"Error: Both primal and dual LPs are infeasible.\"]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # 1. Rays {(1,0), (0,1)}, target y = (1,2) -> In cone\n        ([(1, 0), (0, 1)], (1, 2)),\n        # 2. Rays {(1,0), (0,1)}, target y = (-1,1) -> Not in cone\n        ([(1, 0), (0, 1)], (-1, 1)),\n        # 3. Rays {(1,0), (0,1)}, target y = (0,0) -> In cone (boundary)\n        ([(1, 0), (0, 1)], (0, 0)),\n        # 4. Rays {(1,1), (1,-1)}, target y = (2,0) -> In cone\n        ([(1, 1), (1, -1)], (2, 0)),\n        # 5. Rays {(1,1), (1,-1)}, target y = (1,1.2) -> Not in cone\n        ([(1, 1), (1, -1)], (1, 1.2)),\n    ]\n\n    results = []\n    for rays, y in test_cases:\n        y_vec = np.array(y)\n        result = solve_case(rays, y_vec)\n        # The string representation of a Python list matches the required output format for a single case.\n        results.append(str(result))\n\n    # Join the string representations of each case result with commas and enclose in brackets.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3110862"}, {"introduction": "Beyond simple membership, cones serve as the fundamental building blocks for a powerful class of optimization problems. This exercise challenges you to construct and solve a conic optimization problem from first principles, deriving its dual via the Lagrangian. By contrasting a problem constrained to the non-negative orthant (a Linear Program) with one constrained to the second-order cone (an SOCP), you will discover how the cone's geometry directly dictates the structure and feasibility region of its dual problem. [@problem_id:3110866]", "problem": "Consider the conic optimization template with a cone constraint. Let the decision vector be $x \\in \\mathbb{R}^{3}$, the cost vector be $c \\in \\mathbb{R}^{3}$, and the linear constraint be $A x = b$ with $A \\in \\mathbb{R}^{1 \\times 3}$ and $b \\in \\mathbb{R}$. Work from first principles, beginning with the definitions of a cone, its dual cone, and the Lagrangian of a constrained optimization problem. Do not use any pre-stated duality formulas; instead, derive the dual problem by eliminating primal variables through the infimum over the Lagrangian.\n\nUse the following specific data: let $A = \\begin{bmatrix} 1  1  1 \\end{bmatrix}$, $b = 1$, and $c = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$. Consider two instances distinguished only by the cone $K$.\n\n1) First, take $K = \\mathbb{R}^{3}_{+}$. Construct the corresponding primal problem, derive its dual problem using the definition of the dual cone $K^{\\ast}$ and the Lagrangian, and solve for the optimal value. Interpret the geometry of the dual feasibility region in terms of basic geometric objects.\n\n2) Next, replace $K$ with the $3$-dimensional second-order cone $Q_{3} = \\{ (t,u_{1},u_{2}) \\in \\mathbb{R}^{3} \\mid t \\geq \\sqrt{u_{1}^{2} + u_{2}^{2}} \\}$. Construct the corresponding primal problem, derive its dual problem by the same first-principles route, and solve for the optimal value. Compare the geometry of the dual feasibility constraint you obtain here with that in part 1) in terms of how the feasible set is shaped.\n\nFinally, let $v_{+}$ denote the optimal objective value when $K = \\mathbb{R}^{3}_{+}$ and $v_{Q}$ denote the optimal objective value when $K = Q_{3}$. Report the exact value of $v_{Q} - v_{+}$. No rounding is required, and no units are involved. Your final answer must be a single closed-form expression.", "solution": "This problem asks for the derivation and solution of two conic optimization problems, differing only in the choice of the cone $K$. The final goal is to compute the difference between their optimal objective values. We will proceed from first principles as requested.\n\nA cone $K \\subseteq \\mathbb{R}^{n}$ is a set that is closed under non-negative scalar multiplication, i.e., for any $x \\in K$ and any $\\alpha \\geq 0$, it holds that $\\alpha x \\in K$.\nThe dual cone $K^{\\ast}$ of a cone $K$ is defined as $K^{\\ast} = \\{y \\in \\mathbb{R}^{n} \\mid y^{T}x \\geq 0 \\text{ for all } x \\in K\\}$.\n\nThe general form of the primal conic optimization problem is:\n$$\n\\begin{array}{ll}\n\\text{minimize}  c^{T}x \\\\\n\\text{subject to}  Ax = b \\\\\n x \\in K\n\\end{array}\n$$\nwhere $x \\in \\mathbb{R}^{n}$ is the decision vector, $c \\in \\mathbb{R}^{n}$ is the cost vector, $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$ define the linear equality constraints, and $K$ is a closed convex cone.\n\nTo derive the dual problem, we form the Lagrangian by associating a dual variable $\\nu \\in \\mathbb{R}^{m}$ with the equality constraint $Ax=b$:\n$$L(x, \\nu) = c^{T}x + \\nu^{T}(Ax - b) = (c + A^{T}\\nu)^{T}x - b^{T}\\nu$$\nThe dual function $g(\\nu)$ is the infimum of the Lagrangian over the primal variable $x$, subject to the remaining constraint $x \\in K$:\n$$g(\\nu) = \\inf_{x \\in K} L(x, \\nu) = \\inf_{x \\in K} \\left\\{ (c + A^{T}\\nu)^{T}x \\right\\} - b^{T}\\nu$$\nThe value of the infimum depends on whether the vector $s = c + A^{T}\\nu$ lies in the dual cone $K^{\\ast}$.\nIf $s \\in K^{\\ast}$, then by definition, $s^{T}x \\geq 0$ for all $x \\in K$. Since $0 \\in K$ (as $K$ is a cone), the infimum is achieved at $x=0$, and its value is $0$.\nIf $s \\notin K^{\\ast}$, there exists some $x_{0} \\in K$ such that $s^{T}x_{0}  0$. Since $K$ is a cone, $\\alpha x_{0} \\in K$ for any $\\alpha  0$. Then $s^{T}(\\alpha x_{0}) = \\alpha (s^{T}x_{0}) \\to -\\infty$ as $\\alpha \\to \\infty$. So the infimum is $-\\infty$.\nTherefore, the dual function is:\n$$\ng(\\nu) = \\begin{cases}\n-b^{T}\\nu  \\text{if } c + A^{T}\\nu \\in K^{\\ast} \\\\\n-\\infty  \\text{otherwise}\n\\end{cases}\n$$\nThe dual problem is to maximize the dual function $g(\\nu)$:\n$$\n\\begin{array}{ll}\n\\text{maximize}  -b^{T}\\nu \\\\\n\\text{subject to}  c + A^{T}\\nu \\in K^{\\ast}\n\\end{array}\n$$\nIn this specific problem, we have $n=3$ and $m=1$. The data are $c = \\begin{bmatrix} 1  0  0 \\end{bmatrix}^{T}$, $A = \\begin{bmatrix} 1  1  1 \\end{bmatrix}$, and $b=1$. The dual variable $\\nu$ is a scalar. The dual problem becomes:\n$$\n\\begin{array}{ll}\n\\text{maximize}  -\\nu \\\\\n\\text{subject to}  \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\nu \\in K^{\\ast}\n\\end{array}\n$$\n\n**1) Case 1: The non-negative orthant, $K = \\mathbb{R}^{3}_{+}$**\n\nFirst, we determine the dual cone $K^{\\ast} = (\\mathbb{R}^{3}_{+})^{\\ast}$. A vector $y = (y_1, y_2, y_3)^{T}$ is in $(\\mathbb{R}^{3}_{+})^{\\ast}$ if and only if $y^{T}x = y_1x_1 + y_2x_2 + y_3x_3 \\geq 0$ for all $x = (x_1, x_2, x_3)^{T}$ with $x_i \\geq 0$. By choosing $x = e_i$ (the standard basis vectors), which are in $\\mathbb{R}^{3}_{+}$, we find that we must have $y_i \\geq 0$ for $i=1,2,3$. This means $y \\in \\mathbb{R}^{3}_{+}$. Conversely, if $y \\in \\mathbb{R}^{3}_{+}$, then $y^{T}x \\geq 0$ for all $x \\in \\mathbb{R}^{3}_{+}$. Thus, $\\mathbb{R}^{3}_{+}$ is self-dual: $(\\mathbb{R}^{3}_{+})^{\\ast} = \\mathbb{R}^{3}_{+}$.\n\nThe primal problem $(P_{+})$ is:\n$$\n\\begin{array}{ll}\n\\text{minimize}  x_1 \\\\\n\\text{subject to}  x_1+x_2+x_3 = 1 \\\\\n x_1 \\geq 0, x_2 \\geq 0, x_3 \\geq 0\n\\end{array}\n$$\nTo minimize $x_1$ subject to the constraints, we should choose the smallest possible non-negative value for $x_1$, which is $x_1=0$. This is feasible; for instance, $x=(0, 1, 0)^{T}$ satisfies $0+1+0=1$ and all components are non-negative. The optimal objective value is $v_{+} = 0$.\n\nThe dual problem $(D_{+})$ is:\n$$\n\\begin{array}{ll}\n\\text{maximize}  -\\nu \\\\\n\\text{subject to}  \\begin{bmatrix} 1 + \\nu \\\\ \\nu \\\\ \\nu \\end{bmatrix} \\in \\mathbb{R}^{3}_{+}\n\\end{array}\n$$\nThe constraints are $1+\\nu \\geq 0$, $\\nu \\geq 0$, and $\\nu \\geq 0$. These simplify to $\\nu \\geq 0$. We want to maximize $-\\nu$ subject to $\\nu \\geq 0$. The maximum is achieved at the smallest possible value of $\\nu$, which is $\\nu=0$. The optimal dual value is $-0 = 0$, which matches the primal optimal value.\n\nThe dual feasibility region for $\\nu$ is the set $\\{\\nu \\in \\mathbb{R} \\mid \\nu \\geq 0\\}$, which is a closed, unbounded interval (a ray).\n\n**2) Case 2: The second-order cone, $K = Q_{3}$**\n\nLet $x=(x_1, x_2, x_3)^{T}$. The cone is $Q_{3} = \\{ x \\in \\mathbb{R}^{3} \\mid x_1 \\geq \\sqrt{x_2^2 + x_3^2} \\}$.\nThe second-order cone is also self-dual: $(Q_{3})^{\\ast} = Q_{3}$. To show this, consider $y \\in (Q_3)^*$. Then $y^T x \\ge 0$ for all $x \\in Q_3$. Let $x = (x_1, x_v)$ where $x_v=(x_2,x_3)$. The condition is $x_1 \\ge \\|x_v\\|_2$. We need $y_1 x_1 + y_v^T x_v \\ge 0$. This must hold for all $x_1 \\ge \\|x_v\\|_2$. To minimize the expression for a given $x_v$, we choose $x_1 = \\|x_v\\|_2$. So we need $y_1 \\|x_v\\|_2 + y_v^T x_v \\ge 0$ for all $x_v$. By Cauchy-Schwarz inequality, $y_v^T x_v \\ge -\\|y_v\\|_2 \\|x_v\\|_2$. Thus $y_1 \\|x_v\\|_2 + y_v^T x_v \\ge (y_1 - \\|y_v\\|_2)\\|x_v\\|_2$. For this to be non-negative for all $x_v$, we must have $y_1 - \\|y_v\\|_2 \\ge 0$, which is $y_1 \\ge \\sqrt{y_2^2+y_3^2}$. This is precisely the condition for $y \\in Q_3$.\n\nThe primal problem $(P_{Q})$ is:\n$$\n\\begin{array}{ll}\n\\text{minimize}  x_1 \\\\\n\\text{subject to}  x_1+x_2+x_3 = 1 \\\\\n x_1 \\geq \\sqrt{x_2^2 + x_3^2}\n\\end{array}\n$$\nFrom the equality constraint, $x_1 = 1 - x_2 - x_3$. Substituting into the cone constraint: $1-x_2-x_3 \\geq \\sqrt{x_2^2+x_3^2}$. Minimizing $x_1$ is equivalent to maximizing $x_2+x_3$.\nThe constraint requires $1-x_2-x_3 \\geq 0$, so we can square both sides: $(1-(x_2+x_3))^2 \\geq x_2^2+x_3^2$, which simplifies to $1-2(x_2+x_3)+2x_2x_3 \\geq 0$. We can rewrite this as $2(x_2-1)(x_3-1) \\geq 1$.\nThe problem reduces to maximizing $S=x_2+x_3$ subject to $(x_2-1)(x_3-1) \\geq \\frac{1}{2}$ and $x_2+x_3 \\leq 1$. The maximum of a symmetric linear function over a region with symmetric boundary occurs at a point where $x_2=x_3$. Substituting $x_2=x_3$ into the boundary condition $(x_2-1)(x_3-1) = \\frac{1}{2}$ gives $(x_2-1)^2 = \\frac{1}{2}$, so $x_2-1 = \\pm \\frac{1}{\\sqrt{2}}$. Since $x_2+x_3 \\leq 1$ implies $2x_2 \\leq 1$ or $x_2 \\leq 1/2$, we must choose the negative root: $x_2 = 1 - \\frac{1}{\\sqrt{2}}$. Then $x_3 = 1 - \\frac{1}{\\sqrt{2}}$.\nThe maximum value of $x_2+x_3$ is $(1 - \\frac{1}{\\sqrt{2}}) + (1-\\frac{1}{\\sqrt{2}}) = 2-\\frac{2}{\\sqrt{2}} = 2-\\sqrt{2}$.\nThe optimal primal value is $v_{Q} = \\min x_1 = 1 - \\max(x_2+x_3) = 1 - (2-\\sqrt{2}) = \\sqrt{2}-1$.\n\nThe dual problem $(D_{Q})$ is:\n$$\n\\begin{array}{ll}\n\\text{maximize}  -\\nu \\\\\n\\text{subject to}  \\begin{bmatrix} 1 + \\nu \\\\ \\nu \\\\ \\nu \\end{bmatrix} \\in Q_{3}\n\\end{array}\n$$\nThe constraint is $1+\\nu \\geq \\sqrt{\\nu^2+\\nu^2} = \\sqrt{2\\nu^2} = \\sqrt{2}|\\nu|$.\nAlso, from $1+\\nu \\geq \\dots \\geq 0$, we must have $1+\\nu \\geq 0$, so $\\nu \\geq -1$.\nCase 1: $\\nu \\geq 0$. The inequality is $1+\\nu \\geq \\sqrt{2}\\nu$, which means $1 \\geq (\\sqrt{2}-1)\\nu$, so $\\nu \\leq \\frac{1}{\\sqrt{2}-1} = \\sqrt{2}+1$. This gives $0 \\leq \\nu \\leq \\sqrt{2}+1$.\nCase 2: $-1 \\leq \\nu  0$. The inequality is $1+\\nu \\geq \\sqrt{2}(-\\nu) = -\\sqrt{2}\\nu$, which means $1 \\geq -(\\sqrt{2}+1)\\nu$, so $\\nu \\geq -\\frac{1}{\\sqrt{2}+1} = 1-\\sqrt{2}$. This gives $1-\\sqrt{2} \\leq \\nu  0$.\nCombining both cases, the feasible set for $\\nu$ is the closed interval $[1-\\sqrt{2}, \\sqrt{2}+1]$.\nWe want to maximize $-\\nu$, which means we must choose the minimum possible value for $\\nu$. The minimum feasible $\\nu$ is $1-\\sqrt{2}$.\nThe optimal dual value is $-(1-\\sqrt{2}) = \\sqrt{2}-1$, which again matches the primal optimal value.\n\nThe dual feasibility region for $\\nu$ is the set $\\{\\nu \\in \\mathbb{R} \\mid 1-\\sqrt{2} \\leq \\nu \\leq \\sqrt{2}+1\\}$, which is a closed, bounded interval (a line segment). This contrasts with the unbounded ray found in Part 1. The more restrictive cone ($Q_3 \\subset \\mathbb{R}^3_+$) in the primal problem leads to a larger feasible set for the dual variable.\n\nFinally, we calculate the required difference between the optimal values:\n$v_{Q} = \\sqrt{2}-1$\n$v_{+} = 0$\nThe difference is $v_{Q} - v_{+} = (\\sqrt{2}-1) - 0 = \\sqrt{2}-1$.", "answer": "$$\\boxed{\\sqrt{2}-1}$$", "id": "3110866"}, {"introduction": "The analytical tools we use often depend on the specific geometry of the cone in question. This practice explores this distinction by having you design a membership test based on separation from the dual cone for two canonical examples: the polyhedral non-negative orthant and the non-polyhedral cone of positive semidefinite matrices. Deriving the closed-form solutions reveals the fundamental difference between analyzing polyhedral versus curved cones, offering a glimpse into the worlds of linear and semidefinite programming. [@problem_id:3110877]", "problem": "Consider a closed, convex cone $K \\subset \\mathbb{R}^{n}$ with dual cone $K^{*} = \\{y \\in \\mathbb{R}^{n} \\mid \\langle y, x \\rangle \\ge 0 \\text{ for all } x \\in K\\}$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product. By the Hyperplane Separation Theorem (a well-tested fact in convex analysis), if $x \\notin K$, then there exists a hyperplane, specified by some $y \\in K^{*}$, that strictly separates $x$ from $K$ in the sense that $\\langle y, x \\rangle  0 \\le \\langle y, z \\rangle$ for all $z \\in K$.\n\nPart A (polyhedral cone). Let $K = \\mathbb{R}_{+}^{4}$ be the nonnegative orthant in $\\mathbb{R}^{4}$, and note that $K^{*} = \\mathbb{R}_{+}^{4}$. Design a test for membership in $K$ via strict separation from $K^{*}$ by considering the statistic\n$$\nv(x) = \\inf\\{\\langle y, x \\rangle \\mid y \\in K^{*}, \\ \\mathbf{1}^{\\top} y = 1\\},\n$$\nwhere $\\mathbf{1}$ is the vector in $\\mathbb{R}^{4}$ with all entries equal to $1$. Starting from the definitions above, derive a closed-form expression for $v(x)$ that applies to any $x \\in \\mathbb{R}^{4}$, and then evaluate this expression for\n$$\nx = (1, -2, 3, 0).\n$$\n\nPart B (curved cone). Now consider the cone of $2 \\times 2$ real symmetric positive semidefinite (PSD) matrices, denoted $S_{+}^{2}$, equipped with the Frobenius inner product $\\langle Y, X \\rangle = \\mathrm{Tr}(Y X)$, where $\\mathrm{Tr}(\\cdot)$ denotes the trace. The dual cone of $S_{+}^{2}$ under this inner product is $S_{+}^{2}$ itself. Design an analogous strict-separation-based membership test by defining\n$$\nw(X) = \\inf\\{\\langle Y, X \\rangle \\mid Y \\in S_{+}^{2}, \\ \\mathrm{Tr}(Y) = 1\\}.\n$$\nStarting from first principles, derive a closed-form expression for $w(X)$ that applies to any symmetric $X \\in \\mathbb{R}^{2 \\times 2}$, and then evaluate this expression for\n$$\nX = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}.\n$$\n\nFinally, present your answer as a single row matrix containing the two values $v(x)$ and $w(X)$, in that order. No rounding is required. Express the final answer without units.", "solution": "Part A: Analysis of $v(x)$ for the Nonnegative Orthant\n\nThe first part of the problem asks for a closed-form expression for the statistic $v(x)$, defined as:\n$$\nv(x) = \\inf\\{\\langle y, x \\rangle \\mid y \\in K^{*}, \\ \\mathbf{1}^{\\top} y = 1\\}\n$$\nwhere $K = \\mathbb{R}_{+}^{4}$ is the nonnegative orthant in $\\mathbb{R}^{4}$. It is given that the dual cone is $K^{*} = \\mathbb{R}_{+}^{4}$. The vector $\\mathbf{1}$ is a vector of all ones, so the condition $\\mathbf{1}^{\\top} y = 1$ is equivalent to $\\sum_{i=1}^{4} y_i = 1$. The inner product is the standard Euclidean inner product, so $\\langle y, x \\rangle = \\sum_{i=1}^{4} y_i x_i$.\n\nThe problem is thus a linear programming problem:\n$$\n\\begin{array}{ll}\n\\text{minimize}  \\sum_{i=1}^{4} y_i x_i \\\\\n\\text{subject to}  \\sum_{i=1}^{4} y_i = 1 \\\\\n y_i \\ge 0, \\quad \\text{for } i=1, 2, 3, 4.\n\\end{array}\n$$\nThe feasible region for $y = (y_1, y_2, y_3, y_4)$ is the standard $3$-simplex in $\\mathbb{R}^{4}$, which is a compact and convex set (a polytope). The objective function is linear in $y$. A fundamental theorem of linear programming states that the minimum of a linear function over a polytope is always attained at one of its vertices.\n\nThe vertices of the standard simplex are the standard basis vectors:\n$e_1 = (1, 0, 0, 0)$, $e_2 = (0, 1, 0, 0)$, $e_3 = (0, 0, 1, 0)$, and $e_4 = (0, 0, 0, 1)$.\n\nWe can find the minimum value by evaluating the objective function $\\langle y, x \\rangle$ at each of these vertices:\n\\begin{itemize}\n    \\item At $y = e_1$, the value is $\\langle e_1, x \\rangle = x_1$.\n    \\item At $y = e_2$, the value is $\\langle e_2, x \\rangle = x_2$.\n    \\item At $y = e_3$, the value is $\\langle e_3, x \\rangle = x_3$.\n    \\item At $y = e_4$, the value is $\\langle e_4, x \\rangle = x_4$.\n\\end{itemize}\nThe infimum (which is a minimum) is the smallest of these values. Therefore, the closed-form expression for $v(x)$ is:\n$$\nv(x) = \\min\\{x_1, x_2, x_3, x_4\\}\n$$\nAlternatively, let $x_{\\min} = \\min_{i} x_i$. For any feasible $y$ (i.e., $y_i \\ge 0$ and $\\sum y_i = 1$), we have:\n$$\n\\sum_{i=1}^{4} y_i x_i \\ge \\sum_{i=1}^{4} y_i x_{\\min} = x_{\\min} \\sum_{i=1}^{4} y_i = x_{\\min} \\cdot 1 = x_{\\min}\n$$\nThis shows that $x_{\\min}$ is a lower bound. This lower bound is achieved by setting $y = e_j$, where $j$ is any index for which $x_j = x_{\\min}$. This $y$ is feasible, and the objective value is $\\langle e_j, x \\rangle = x_j = x_{\\min}$.\n\nFor the specific vector $x = (1, -2, 3, 0)$, we evaluate this expression:\n$$\nv(x) = \\min\\{1, -2, 3, 0\\} = -2\n$$\n\nPart B: Analysis of $w(X)$ for the PSD Cone\n\nThe second part of the problem asks for a closed-form expression for the statistic $w(X)$:\n$$\nw(X) = \\inf\\{\\langle Y, X \\rangle \\mid Y \\in S_{+}^{2}, \\ \\mathrm{Tr}(Y) = 1\\}\n$$\nwhere $S_{+}^{2}$ is the cone of $2 \\times 2$ real symmetric positive semidefinite (PSD) matrices, and the inner product is the Frobenius inner product, $\\langle Y, X \\rangle = \\mathrm{Tr}(Y X)$. The matrix $X$ is a general $2 \\times 2$ real symmetric matrix.\n\nSince $X$ is symmetric, it admits a spectral decomposition $X = U \\Lambda U^{\\top}$, where $U$ is an orthogonal matrix ($U^{\\top}U=I$) whose columns are the eigenvectors of $X$, and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\lambda_2)$ is the diagonal matrix of the corresponding eigenvalues.\n\nUsing the cyclic property of the trace, the objective function can be rewritten as:\n$$\n\\mathrm{Tr}(YX) = \\mathrm{Tr}(Y U \\Lambda U^{\\top}) = \\mathrm{Tr}(U^{\\top} Y U \\Lambda)\n$$\nLet's introduce a change of variables $Y' = U^{\\top} Y U$. The set of feasible $Y$ is $\\{Y \\in S_+^2, \\mathrm{Tr}(Y)=1\\}$. Let's see how the properties of $Y$ transform to $Y'$:\n\\begin{enumerate}\n    \\item Symmetry: $(Y')^{\\top} = (U^{\\top} Y U)^{\\top} = U^{\\top} Y^{\\top} U = U^{\\top} Y U = Y'$. So $Y'$ is symmetric.\n    \\item Positive Semidefiniteness: For any $v \\in \\mathbb{R}^2$, $v^{\\top}Y'v = v^{\\top}U^{\\top}Y U v = (Uv)^{\\top}Y(Uv) \\ge 0$ since $Y \\in S_+^2$. So $Y' \\in S_+^2$.\n    \\item Trace: $\\mathrm{Tr}(Y') = \\mathrm{Tr}(U^{\\top} Y U) = \\mathrm{Tr}(Y U U^{\\top}) = \\mathrm{Tr}(Y)$. Thus, $\\mathrm{Tr}(Y')=1$.\n\\end{enumerate}\nThe transformation $Y \\mapsto Y'$ is a bijection from the feasible set to itself. So, the optimization problem is equivalent to:\n$$\nw(X) = \\inf\\{\\mathrm{Tr}(Y' \\Lambda) \\mid Y' \\in S_{+}^{2}, \\ \\mathrm{Tr}(Y')=1\\}\n$$\nLet $Y' = \\begin{pmatrix} y'_{11}  y'_{12} \\\\ y'_{12}  y'_{22} \\end{pmatrix}$. The objective function is $\\mathrm{Tr}(Y' \\Lambda) = \\lambda_1 y'_{11} + \\lambda_2 y'_{22}$.\nThe constraints are $Y' \\in S_+^2$ (i.e., $y'_{11} \\ge 0, y'_{22} \\ge 0, y'_{11}y'_{22} \\ge (y'_{12})^2$) and $\\mathrm{Tr}(Y') = y'_{11} + y'_{22}=1$.\n\nThe expression $\\lambda_1 y'_{11} + \\lambda_2 y'_{22}$ is a convex combination of the eigenvalues $\\lambda_1$ and $\\lambda_2$, since $y'_{11}, y'_{22} \\ge 0$ and they sum to $1$. To minimize this expression, we should put all the weight on the smallest eigenvalue.\nLet $\\lambda_{\\min}(X) = \\min\\{\\lambda_1, \\lambda_2\\}$. Then for any feasible $Y'$,\n$$\n\\lambda_1 y'_{11} + \\lambda_2 y'_{22} \\ge \\lambda_{\\min}(X) y'_{11} + \\lambda_{\\min}(X) y'_{22} = \\lambda_{\\min}(X) (y'_{11} + y'_{22}) = \\lambda_{\\min}(X)\n$$\nThis shows that $\\lambda_{\\min}(X)$ is a lower bound. This bound is attainable. Let $u$ be a unit eigenvector corresponding to $\\lambda_{\\min}(X)$. Let $Y = uu^{\\top}$. This $Y$ is in $S_+^2$ and has $\\mathrm{Tr}(Y) = \\mathrm{Tr}(uu^{\\top}) = u^{\\top}u=1$, so it is a feasible point. The value of the objective at this point is:\n$$\n\\mathrm{Tr}(YX) = \\mathrm{Tr}(uu^{\\top}X) = \\mathrm{Tr}(u^{\\top}Xu) = u^{\\top}Xu = u^{\\top}(\\lambda_{\\min}(X)u) = \\lambda_{\\min}(X)u^{\\top}u = \\lambda_{\\min}(X)\n$$\nThus, the minimum value is indeed the minimum eigenvalue of $X$. The closed-form expression for $w(X)$ is:\n$$\nw(X) = \\lambda_{\\min}(X)\n$$\nWe need to evaluate this for the specific matrix $X = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(X - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} 1-\\lambda  1 \\\\ 1  -1-\\lambda \\end{pmatrix} = (1-\\lambda)(-1-\\lambda) - 1 \\cdot 1 = 0\n$$\n$$\n-(1-\\lambda)(1+\\lambda) - 1 = 0\n$$\n$$\n-(1-\\lambda^2) - 1 = 0\n$$\n$$\n\\lambda^2 - 1 - 1 = 0\n$$\n$$\n\\lambda^2 = 2\n$$\nThe eigenvalues are $\\lambda = \\sqrt{2}$ and $\\lambda = -\\sqrt{2}$. The minimum eigenvalue is $\\lambda_{\\min}(X) = -\\sqrt{2}$.\nTherefore, for the given $X$, we have:\n$$\nw(X) = -\\sqrt{2}\n$$\nThe problem asks for the two values $v(x)$ and $w(X)$ in a single row matrix. The values are $v(x) = -2$ and $w(X) = -\\sqrt{2}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -2  -\\sqrt{2} \\end{pmatrix}}\n$$", "id": "3110877"}]}