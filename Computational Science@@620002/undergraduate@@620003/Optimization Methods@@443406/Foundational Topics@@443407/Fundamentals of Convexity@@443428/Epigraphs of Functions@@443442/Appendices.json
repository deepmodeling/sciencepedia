{"hands_on_practices": [{"introduction": "This first exercise tests your grasp of the fundamental definitions that connect the geometric world of sets with the analytic world of functions. By distinguishing between epigraphs and hypographs and linking them to convexity and concavity, you'll solidify the foundational language of optimization theory. This practice [@problem_id:3125625] is crucial for understanding how problems like maximizing a concave function can be transformed into a standard minimization format using the epigraph concept.", "problem": "Consider a real-valued function $g:\\mathbb{R}^n\\to \\mathbb{R}$ that is concave, and a nonempty feasible set $C\\subseteq \\mathbb{R}^n$. Recall the fundamental definitions: the epigraph of a function $f:\\mathbb{R}^n\\to \\mathbb{R}$ is the set $\\operatorname{epi} f := \\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid f(x)\\le t\\}$, and the hypograph of a function $h:\\mathbb{R}^n\\to \\mathbb{R}$ is the set $\\operatorname{hypo} h := \\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid h(x)\\ge t\\}$. Concavity of $g$ means that for all $x,y\\in \\mathbb{R}^n$ and all $\\theta\\in [0,1]$, one has $g(\\theta x+(1-\\theta)y)\\ge \\theta g(x)+(1-\\theta)g(y)$. Using only these definitions and standard logical reasoning, select all statements that are correct.\n\nA. The hypograph of $g$ is $\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid g(x)\\le t\\}$.\n\nB. If $g$ is concave, then the set $\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid -g(x)\\le t\\}=\\operatorname{epi}(-g)$ is convex.\n\nC. The maximization problem $\\max_{x\\in C} g(x)$ can be reformulated as the minimization problem $\\min_{x\\in C,\\,t\\in \\mathbb{R}} t$ subject to $-g(x)\\le t$, that is, minimize $t$ over $(x,t)\\in \\operatorname{epi}(-g)$ with $x\\in C$.\n\nD. For concave $g$, the hypograph $\\operatorname{hypo} g=\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid g(x)\\ge t\\}$ is a convex set.\n\nE. If $g$ is concave and not affine, then $\\operatorname{epi} g$ is convex.", "solution": "The user wants me to validate the problem statement and, if valid, provide a detailed solution.\n\n### Step 1: Extract Givens\n-   Function: $g:\\mathbb{R}^n\\to \\mathbb{R}$ is a concave function.\n-   Set: $C\\subseteq \\mathbb{R}^n$ is a nonempty feasible set.\n-   Definition of Epigraph: For a function $f:\\mathbb{R}^n\\to \\mathbb{R}$, its epigraph is $\\operatorname{epi} f := \\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid f(x)\\le t\\}$.\n-   Definition of Hypograph: For a function $h:\\mathbb{R}^n\\to \\mathbb{R}$, its hypograph is $\\operatorname{hypo} h := \\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid h(x)\\ge t\\}$.\n-   Definition of Concavity: For the function $g$, for all $x,y\\in \\mathbb{R}^n$ and all $\\theta\\in [0,1]$, the inequality $g(\\theta x+(1-\\theta)y)\\ge \\theta g(x)+(1-\\theta)g(y)$ holds.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard exercise in the field of convex analysis and optimization.\n-   **Scientifically Grounded:** All definitions and concepts (concave function, epigraph, hypograph) are standard, correct, and fundamental to mathematics and optimization theory.\n-   **Well-Posed:** The problem asks for the evaluation of several statements based on the provided definitions. This is a clear and well-posed task with a unique set of correct answers.\n-   **Objective:** The language is formal and mathematical, free of any subjectivity.\n\nThe problem does not exhibit any of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, ambiguity). All terms are defined correctly and are self-consistent. The set $C$ is only relevant for option C, and its non-emptiness is a standard and necessary assumption for the optimization problem to be well-defined.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed to the solution phase.\n\n### Derivation and Option-by-Option Analysis\n\nThe derivation will rely on the provided definitions and fundamental theorems of convex analysis that can be derived directly from them.\n\n**A. The hypograph of $g$ is $\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid g(x)\\le t\\}$.**\n\nThe problem statement explicitly defines the hypograph of a function $h$ as $\\operatorname{hypo} h := \\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid h(x)\\ge t\\}$. Applying this definition to the function $g$, its hypograph is $\\operatorname{hypo} g = \\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid g(x)\\ge t\\}$.\n\nThe set given in the option, $\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid g(x)\\le t\\}$, is the definition of the *epigraph* of $g$, denoted $\\operatorname{epi} g$. The statement incorrectly equates the hypograph with the epigraph.\n\nVerdict: **Incorrect**.\n\n**B. If $g$ is concave, then the set $\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid -g(x)\\le t\\}=\\operatorname{epi}(-g)$ is convex.**\n\nLet's define a new function $f(x) = -g(x)$. Since a function is convex if and only if its epigraph is a convex set, the statement is true if and only if the function $f(x) = -g(x)$ is convex.\n\nWe are given that $g$ is concave. By definition, for any $x, y \\in \\mathbb{R}^n$ and $\\theta \\in [0,1]$:\n$$g(\\theta x + (1-\\theta)y) \\ge \\theta g(x) + (1-\\theta)g(y)$$\nMultiplying both sides of this inequality by $-1$ reverses the inequality sign:\n$$-g(\\theta x + (1-\\theta)y) \\le -(\\theta g(x) + (1-\\theta)g(y))$$\n$$-g(\\theta x + (1-\\theta)y) \\le \\theta(-g(x)) + (1-\\theta)(-g(y))$$\nSubstituting $f(x) = -g(x)$, we get:\n$$f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y)$$\nThis is the definition of a convex function. Thus, if $g$ is concave, then $f(x) = -g(x)$ is convex.\n\nThe set given in the option is $\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid -g(x)\\le t\\}$, which is precisely the epigraph of the function $-g$, i.e., $\\operatorname{epi}(-g)$.\nA fundamental result in convex analysis states that a function is convex if and only if its epigraph is a convex set. Since we have shown that $-g$ is a convex function, its epigraph, $\\operatorname{epi}(-g)$, must be a convex set.\n\nVerdict: **Correct**.\n\n**C. The maximization problem $\\max_{x\\in C} g(x)$ can be reformulated as the minimization problem $\\min_{x\\in C,\\,t\\in \\mathbb{R}} t$ subject to $-g(x)\\le t$, that is, minimize $t$ over $(x,t)\\in \\operatorname{epi}(-g)$ with $x\\in C$.**\n\nConsider the original problem, $P_1: \\max_{x\\in C} g(x)$.\nMaximizing $g(x)$ is equivalent to minimizing $-g(x)$. So, an equivalent problem is $P'_1: \\min_{x\\in C} -g(x)$.\n\nNow consider the proposed reformulation, $P_2: \\min_{x\\in C,\\,t\\in \\mathbb{R}} t$ subject to the constraint $-g(x) \\le t$.\nThe feasible region for $P_2$ is the set of points $(x,t)$ where $x \\in C$ and $t \\ge -g(x)$. In this problem, we are minimizing the variable $t$. For any given $x \\in C$, to minimize $t$, we must choose the smallest possible value of $t$ that satisfies the constraint, which is $t = -g(x)$.\nSubstituting this into the objective function of $P_2$, the problem becomes equivalent to minimizing the expression $-g(x)$ over all $x \\in C$. That is, $P_2$ is equivalent to $\\min_{x\\in C} -g(x)$.\n\nSince $P'_1$ and $P_2$ are equivalent, and $P_1$ is equivalent to $P'_1$, it follows that $P_1$ can be reformulated as $P_2$. If $x^*$ is an optimal solution to $P_1$ (a maximizer of $g(x)$), then it is also an optimal solution to $P'_1$ (a minimizer of $-g(x)$), and $(x^*, -g(x^*))$ is an optimal solution to $P_2$. This is a standard epigraph reformulation technique in optimization.\n\nVerdict: **Correct**.\n\n**D. For concave $g$, the hypograph $\\operatorname{hypo} g=\\{(x,t)\\in \\mathbb{R}^n\\times \\mathbb{R}\\mid g(x)\\ge t\\}$ is a convex set.**\n\nThis statement posits that concavity of a function is equivalent to the convexity of its hypograph. We can prove this directly from the definitions.\nLet $(x_1, t_1)$ and $(x_2, t_2)$ be two arbitrary points in $\\operatorname{hypo} g$. By definition:\n$g(x_1) \\ge t_1$\n$g(x_2) \\ge t_2$\nTo show that $\\operatorname{hypo} g$ is a convex set, we must show that for any $\\theta \\in [0,1]$, the convex combination of these two points, which is $( \\theta x_1 + (1-\\theta)x_2, \\theta t_1 + (1-\\theta)t_2 )$, is also in $\\operatorname{hypo} g$.\nLet $x_\\theta = \\theta x_1 + (1-\\theta)x_2$ and $t_\\theta = \\theta t_1 + (1-\\theta)t_2$. We need to verify if $g(x_\\theta) \\ge t_\\theta$.\n\nSince $g$ is a concave function, we have:\n$$g(x_\\theta) = g(\\theta x_1 + (1-\\theta)x_2) \\ge \\theta g(x_1) + (1-\\theta)g(x_2)$$\nSince $g(x_1) \\ge t_1$ and $g(x_2) \\ge t_2$, and $\\theta, (1-\\theta) \\ge 0$, we have $\\theta g(x_1) \\ge \\theta t_1$ and $(1-\\theta)g(x_2) \\ge (1-\\theta)t_2$. Summing these gives:\n$$\\theta g(x_1) + (1-\\theta)g(x_2) \\ge \\theta t_1 + (1-\\theta)t_2 = t_\\theta$$\nCombining the two inequalities, we get:\n$$g(x_\\theta) \\ge \\theta g(x_1) + (1-\\theta)g(x_2) \\ge t_\\theta$$\nThus, $g(x_\\theta) \\ge t_\\theta$, which means the point $(x_\\theta, t_\\theta)$ is in $\\operatorname{hypo} g$. Since this holds for any two points in the set and any $\\theta \\in [0,1]$, the set $\\operatorname{hypo} g$ is convex.\n\nVerdict: **Correct**.\n\n**E. If $g$ is concave and not affine, then $\\operatorname{epi} g$ is convex.**\n\nA fundamental theorem of convex analysis states that a function $f$ is a convex function if and only if its epigraph, $\\operatorname{epi} f$, is a convex set.\nTherefore, the statement \"$\\operatorname{epi} g$ is convex\" is equivalent to the statement \"$g$ is a convex function\".\nThe option can be rephrased as: \"If $g$ is concave and not affine, then $g$ is a convex function.\"\n\nLet's analyze this implication. A function is defined as affine if it is both convex and concave. The problem assumes that $g$ is concave and *not* affine. This means that $g$ cannot be both convex and concave. Since we are given that $g$ is concave, it must be the case that $g$ is *not* convex.\nThus, the premise \"g is concave and not affine\" implies the conclusion \"g is not convex\".\n\nThe option, however, concludes that \"$g$ is convex\" (or, equivalently, that $\\operatorname{epi} g$ is convex). This is a direct contradiction.\nFor example, the function $g(x) = -x^2$ for $x \\in \\mathbb{R}$ is strictly concave, and therefore not affine. However, it is not a convex function. Consequently, its epigraph, $\\operatorname{epi} g = \\{(x,t) \\mid -x^2 \\le t\\}$, is not a convex set.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{BCD}$$", "id": "3125625"}, {"introduction": "Building upon the basics, this practice explores how the epigraph concept helps us analyze functions constructed from simpler pieces, a common scenario in modeling. You will work with the pointwise maximum of two functions and see how its epigraph is formed by the intersection of the individual epigraphs. This exercise [@problem_id:3125707] provides a powerful geometric intuition for locating points of non-differentiability, which often play a critical role in finding the solution to optimization problems.", "problem": "Consider the function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{x^{2},2|x|\\}$. Work from foundational definitions in optimization methods, including the definition of the epigraph of a function $\\operatorname{epi} f=\\{(x,t)\\in\\mathbb{R}^{2}\\mid t\\ge f(x)\\}$ and the definition of the pointwise maximum of functions. Using these, first express $\\operatorname{epi} f$ as the intersection of the epigraphs of two simpler functions. Then, by analyzing the active branches of the maximum, determine all points where $f$ fails to be differentiable and explain how these points arise from either the non-differentiability of a component function or a switch in the active branch. Finally, consider the unconstrained minimization problem $\\min_{x\\in\\mathbb{R}} f(x)$ and discuss which of the kink points are relevant to the optimizer.\n\nYour tasks:\n1. Derive $\\operatorname{epi} f$ explicitly as an intersection of epigraphs of two simpler functions and write this intersection in set-builder form.\n2. Identify all kink points (points of non-differentiability) of $f$ and justify your identification from first principles.\n3. State which kink point(s) are relevant for the unconstrained minimization of $f(x)$ by discussing the behavior of $f$ in neighborhoods of those points.\n\nAnswer specification:\n- Provide a concise, exact final answer consisting of the $x$-coordinates of the kink points in increasing order, written as a single row matrix.", "solution": "The problem is well-posed and scientifically grounded in real analysis and optimization theory. All terms are standard and well-defined, and the problem is self-contained. Therefore, a solution can be derived.\n\nThe function under consideration is $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{x^{2},2|x|\\}$. We can decompose this into two simpler functions, $f_1(x) = x^2$ and $f_2(x) = 2|x|$, such that $f(x) = \\max\\{f_1(x), f_2(x)\\}$.\n\n**1. Epigraph as an Intersection**\n\nBy definition, the epigraph of a function $g$ is the set $\\operatorname{epi} g = \\{(x,t) \\mid t \\ge g(x)\\}$. For the function $f(x)$, its epigraph is:\n$$ \\operatorname{epi} f = \\{(x,t) \\in \\mathbb{R}^{2} \\mid t \\ge f(x)\\} $$\nSubstituting the definition of $f(x)$, we have:\n$$ \\operatorname{epi} f = \\{(x,t) \\in \\mathbb{R}^{2} \\mid t \\ge \\max\\{x^{2}, 2|x|\\}\\} $$\nThe inequality $t \\ge \\max\\{a,b\\}$ holds if and only if $t \\ge a$ and $t \\ge b$. Applying this principle, the condition for $(x,t)$ to be in $\\operatorname{epi} f$ is equivalent to the system of two inequalities:\n$$ t \\ge x^{2} \\quad \\text{and} \\quad t \\ge 2|x| $$\nThe set of points satisfying $t \\ge x^2$ is precisely the epigraph of $f_1(x)=x^2$, which is $\\operatorname{epi} f_1 = \\{(x,t) \\in \\mathbb{R}^{2} \\mid t \\ge x^2\\}$.\nThe set of points satisfying $t \\ge 2|x|$ is the epigraph of $f_2(x)=2|x|$, which is $\\operatorname{epi} f_2 = \\{(x,t) \\in \\mathbb{R}^{2} \\mid t \\ge 2|x|\\}$.\nTherefore, $\\operatorname{epi} f$ is the intersection of these two sets:\n$$ \\operatorname{epi} f = \\operatorname{epi} f_1 \\cap \\operatorname{epi} f_2 $$\nIn set-builder notation, this intersection is explicitly written as:\n$$ \\operatorname{epi} f = \\{(x,t) \\in \\mathbb{R}^{2} \\mid t \\ge x^{2} \\text{ and } t \\ge 2|x|\\} $$\n\n**2. Kink Points (Points of Non-Differentiability)**\n\nA point of non-differentiability for $f(x) = \\max\\{f_1(x), f_2(x)\\}$ can occur where one of the component functions is not differentiable, or where the active branch of the maximum function switches.\n\nFirst, we analyze the differentiability of the component functions.\n-   $f_1(x) = x^2$ is a polynomial and is differentiable everywhere on $\\mathbb{R}$. Its derivative is $f_1'(x) = 2x$.\n-   $f_2(x) = 2|x|$ is not differentiable at $x=0$. The left-hand derivative is $\\lim_{h\\to 0^-}\\frac{2|h|-0}{h} = \\lim_{h\\to 0^-}\\frac{-2h}{h} = -2$, and the right-hand derivative is $\\lim_{h\\to 0^+}\\frac{2|h|-0}{h} = \\lim_{h\\to 0^+}\\frac{2h}{h} = 2$.\n\nThe points where the active branch might switch are found by solving $f_1(x) = f_2(x)$:\n$$ x^2 = 2|x| $$\nSince $x^2 = |x|^2$, the equation becomes $|x|^2 - 2|x| = 0$, which factors as $|x|(|x|-2)=0$. The solutions are $|x|=0$ (i.e., $x=0$) and $|x|=2$ (i.e., $x=2$ and $x=-2$).\nThe set of potential kink points is therefore $\\{-2, 0, 2\\}$.\n\nTo analyze differentiability at these points, we first express $f(x)$ in a piecewise form by determining which function is greater in the intervals defined by these points.\n-   For $|x| < 2$, we have $|x|-2 < 0$. Multiplying by $|x| \\ge 0$ gives $|x|^2 - 2|x| \\le 0$, so $x^2 \\le 2|x|$. Thus, $f(x)=2|x|$ for $x \\in [-2,2]$.\n-   For $|x| > 2$, we have $|x|-2 > 0$. Multiplying by $|x| > 0$ gives $|x|^2 - 2|x| > 0$, so $x^2 > 2|x|$. Thus, $f(x)=x^2$ for $|x| \\ge 2$.\n\nThis gives the piecewise definition:\n$$ f(x) = \\begin{cases} x^2 & \\text{if } x \\le -2 \\\\ -2x & \\text{if } -2 < x < 0 \\\\ 2x & \\text{if } 0 \\le x < 2 \\\\ x^2 & \\text{if } x \\ge 2 \\end{cases} $$\nNow we check the one-sided derivatives at each potential kink point:\n-   At $x=-2$:\n    -   Left-hand derivative: $\\lim_{x \\to -2^-} f'(x) = \\lim_{x \\to -2^-} (x^2)' = \\lim_{x \\to -2^-} 2x = 2(-2) = -4$.\n    -   Right-hand derivative: $\\lim_{x \\to -2^+} f'(x) = \\lim_{x \\to -2^+} (-2x)' = -2$.\n    Since $-4 \\neq -2$, $f$ is not differentiable at $x=-2$.\n\n-   At $x=0$:\n    -   Left-hand derivative: $\\lim_{x \\to 0^-} f'(x) = \\lim_{x \\to 0^-} (-2x)' = -2$.\n    -   Right-hand derivative: $\\lim_{x \\to 0^+} f'(x) = \\lim_{x \\to 0^+} (2x)' = 2$.\n    Since $-2 \\neq 2$, $f$ is not differentiable at $x=0$. This is expected, as $f(x) = 2|x|$ in a neighborhood of $x=0$.\n\n-   At $x=2$:\n    -   Left-hand derivative: $\\lim_{x \\to 2^-} f'(x) = \\lim_{x \\to 2^-} (2x)' = 2$.\n    -   Right-hand derivative: $\\lim_{x \\to 2^+} f'(x) = \\lim_{x \\to 2^+} (x^2)' = \\lim_{x \\to 2^+} 2x = 2(2) = 4$.\n    Since $2 \\neq 4$, $f$ is not differentiable at $x=2$.\n\nThe set of all points where $f$ fails to be differentiable is $\\{-2, 0, 2\\}$.\n\n**3. Relevance of Kink Points to Minimization**\n\nWe must solve the unconstrained minimization problem $\\min_{x\\in\\mathbb{R}} f(x)$.\nThe function is $f(x) = \\max\\{x^2, 2|x|\\}$. Since $x^2 \\ge 0$ and $2|x| \\ge 0$ for all $x \\in \\mathbb{R}$, their maximum $f(x)$ must also be non-negative, i.e., $f(x) \\ge 0$.\nThe value $f(x)=0$ is achieved if and only if both $x^2=0$ and $2|x|=0$, which occurs uniquely at $x=0$.\nAt this point, $f(0) = \\max\\{0^2, 2|0|\\} = 0$.\nSince $f(x) \\ge 0$ for all $x$ and $f(0)=0$, the global minimum of $f$ is $0$, attained at the optimizer $x^*=0$.\n\nThe optimizer $x^*=0$ is one of the kink points. Therefore, this kink point is centrally relevant to the minimization problem as it is the location of the solution.\nFor a convex function such as $f$ (being the pointwise maximum of convex functions), the global minimum occurs at a point $x^*$ where the subgradient contains zero, i.e., $0 \\in \\partial f(x^*)$.\n-   At $x=0$, the subgradient is the interval defined by the left and right derivatives, $\\partial f(0) = [-2, 2]$. Since $0 \\in [-2, 2]$, $x=0$ is a minimizer.\n-   At $x=2$, the subgradient is $\\partial f(2) = [2, 4]$. Since $0 \\notin [2, 4]$, $x=2$ is not a minimizer.\n-   At $x=-2$, the subgradient is $\\partial f(-2) = [-4, -2]$. Since $0 \\notin [-4, -2]$, $x=-2$ is not a minimizer.\n\nThus, the only kink point that corresponds to an optimizer is $x=0$. The analysis of kink points is essential because the minimum does not occur at a stationary point (where a derivative would be zero), but rather at a point of non-differentiability.\n\nThe final answer requires the set of all kink points. These are $x=-2$, $x=0$, and $x=2$.", "answer": "$$ \\boxed{\\begin{pmatrix} -2 & 0 & 2 \\end{pmatrix}} $$", "id": "3125707"}, {"introduction": "This final practice brings the theory of epigraphs into the realm of computation by guiding you through the implementation of a classic optimization algorithm. You will use the idea of supporting hyperplanes to the epigraph to iteratively build a piecewise-linear model of a convex function. This hands-on exercise [@problem_id:3125647] demonstrates how the geometric concept of an epigraph provides the foundation for powerful cutting-plane methods that can solve complex, \"black-box\" optimization problems.", "problem": "You are asked to implement, from first principles, a cutting-plane algorithm based on epigraphs to compute piecewise-linear lower bounding models of a convex function and use these models to approximate the minimum over a compact convex set. The method should construct a sequence of supporting hyperplanes to the epigraph that yields a monotonically nondecreasing lower bound on the optimal value. The algorithm must rely only on the following core definitions and facts:\n1) The epigraph of a function: for a function $f : \\mathbb{R}^n \\to \\mathbb{R}$, the epigraph is $\\operatorname{epi}(f) = \\{(x,t) \\in \\mathbb{R}^{n+1} : t \\ge f(x)\\}$.\n2) Convexity and subgradient inequality: if $f$ is convex, then for any $x \\in \\operatorname{dom}(f)$ and any subgradient $g \\in \\partial f(x)$, the inequality $f(y) \\ge f(x) + g^\\top (y - x)$ holds for all $y$ in the domain. This implies each $(x,g)$ defines a valid supporting halfspace $t \\ge g^\\top y + \\left(f(x) - g^\\top x\\right)$ for $\\operatorname{epi}(f)$.\n3) Minimizing a linear objective over the intersection of halfspaces and box constraints is a linear program.\n\nImplement the following algorithmic task:\n- Given a convex function $f$, a routine to compute one subgradient $g \\in \\partial f(x)$ at any queried point $x$, and a box domain $\\mathcal{X} = \\{x \\in \\mathbb{R}^n : \\ell \\le x \\le u \\}$ for given vectors $\\ell, u \\in \\mathbb{R}^n$, build an iterative cutting-plane method (often called Kelley’s method) to minimize an auxiliary epigraph variable $t$ subject to all accumulated cutting planes:\n  - Initialize with a single cut at the box center $x^{(0)} = \\frac{1}{2}(\\ell + u)$. Compute $f(x^{(0)})$ and a subgradient $g^{(0)} \\in \\partial f(x^{(0)})$. Form the affine minorant $\\phi_0(y) = g^{(0)\\top} y + \\big(f(x^{(0)}) - g^{(0)\\top} x^{(0)}\\big)$.\n  - At iteration $k \\in \\{1,2,\\dots\\}$, solve the linear program\n    minimize $t$ subject to $t \\ge \\phi_i(x)$ for all $i \\in \\{0,\\dots,k-1\\}$ and $\\ell \\le x \\le u$.\n    Let the optimizer be $(x^{(k)}_{\\text{LP}}, t^{(k)}_{\\text{LP}})$. Evaluate $f\\big(x^{(k)}_{\\text{LP}}\\big)$ and a subgradient $g^{(k)} \\in \\partial f\\big(x^{(k)}_{\\text{LP}}\\big)$. Add the new cut $\\phi_k(y) = g^{(k)\\top} y + \\big(f(x^{(k)}_{\\text{LP}}) - g^{(k)\\top} x^{(k)}_{\\text{LP}}\\big)$. Maintain the incumbent upper bound $U^{(k)} = \\min\\{U^{(k-1)}, f(x^{(k)}_{\\text{LP}})\\}$ and define the lower bound $L^{(k)} = t^{(k)}_{\\text{LP}}$.\n  - Use the gap stopping rule: if $U^{(k)} - L^{(k)} \\le \\varepsilon$, terminate and return $(L^{(k)}, U^{(k)}, U^{(k)} - L^{(k)})$ along with the number of iterations and a convergence flag. Otherwise continue until a maximum of $K_{\\max}$ iterations is reached.\n\nYour program must implement this method using a linear programming solver for the subproblem. The epigraph model must be explicitly constructed from accumulated cuts as described.\n\nTest Suite:\nImplement the above algorithm for the following four test cases, each with its own function, subgradient rule, and domain.\n1) Nondifferentiable univariate convex function on a symmetric interval:\n   - Dimension $n = 1$.\n   - Domain bounds $\\ell = [-3]$, $u = [3]$.\n   - Function $f(x) = |x_1| + \\tfrac{1}{2} x_1$, where $x = (x_1)$.\n   - Subgradient rule: if $x_1 > 0$, use $g = [\\tfrac{3}{2}]$; if $x_1 < 0$, use $g = [-\\tfrac{1}{2}]$; if $x_1 = 0$, use $g = [0]$.\n2) Smooth strongly convex quadratic on a box:\n   - Dimension $n = 2$.\n   - Domain bounds $\\ell = [-2,-2]$, $u = [2,2]$.\n   - Function $f(x) = x_1^2 + x_2^2 - 2 x_1 + x_2$.\n   - Gradient (which is a subgradient): $g(x) = [2 x_1 - 2,\\; 2 x_2 + 1]$.\n3) Convex piecewise-linear (maximum of affine functions) on a box:\n   - Dimension $n = 2$.\n   - Domain bounds $\\ell = [-2,-2]$, $u = [2,2]$.\n   - Function $f(x) = \\max\\{ [1,-1]^\\top x, [-2,1]^\\top x + 1, [\\frac{1}{2}, \\frac{1}{2}]^\\top x - 1 \\}$.\n   - Subgradient rule: select any maximizer’s coefficient vector as a subgradient; to make it deterministic, if multiple affine functions tie, choose the one with the smallest index.\n4) Smooth convex log-sum-exp on a box:\n   - Dimension $n = 2$.\n   - Domain bounds $\\ell = [-2,-2]$, $u = [2,2]$.\n   - Function $f(x) = \\log\\left(\\exp(x_1) + \\exp(x_2)\\right)$.\n   - Gradient (a subgradient): $g(x) = \\left[\\dfrac{\\exp(x_1)}{\\exp(x_1) + \\exp(x_2)},\\; \\dfrac{\\exp(x_2)}{\\exp(x_1) + \\exp(x_2)}\\right]$.\n\nAlgorithmic parameters and output:\n- Use tolerance $\\varepsilon = 10^{-5}$ and maximum iterations $K_{\\max} = 100$ for all tests.\n- Initialize the first cut at the box center $x^{(0)} = \\tfrac{1}{2}(\\ell + u)$.\n- For each test case, return a list of the form $[L, U, G, K, C]$, where $L$ is the final lower bound, $U$ is the final upper bound (the best function value found), $G = U - L$ is the final optimality gap, $K$ is the number of iterations performed (an integer), and $C$ is a boolean indicating whether $G \\le \\varepsilon$ at termination. For numerical reproducibility, round $L$, $U$, and $G$ to $6$ decimal places before output.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, i.e., of the form $[[L_1,U_1,G_1,K_1,C_1],[L_2,U_2,G_2,K_2,C_2],[L_3,U_3,G_3,K_3,C_3],[L_4,U_4,G_4,K_4,C_4]]$.\n\nNotes on coverage:\n- The four tests cover a nondifferentiable function with a minimizer at a kink, a smooth strongly convex function with an interior minimizer, a piecewise-linear convex function where the epigraph is polyhedral, and a smooth convex function whose minimizer is on the boundary of the box. This suite exercises correctness of cut generation, linear subproblem formulation, and the stopping rule based on the epigraph lower bound versus the incumbent upper bound.", "solution": "The problem requires the implementation of Kelley's cutting-plane method for minimizing a convex function $f: \\mathbb{R}^n \\to \\mathbb{R}$ over a compact box domain $\\mathcal{X} = \\{x \\in \\mathbb{R}^n : \\ell \\le x \\le u \\}$. The method is based on an epigraph formulation and iteratively builds a piecewise-linear lower model of the function.\n\nThe core principle is to transform the original optimization problem, $\\min_{x \\in \\mathcal{X}} f(x)$, into an equivalent problem involving the function's epigraph. The epigraph of $f$ is the set $\\operatorname{epi}(f) = \\{(x,t) \\in \\mathbb{R}^{n+1} : t \\ge f(x)\\}$. The minimization problem is then equivalent to finding the lowest point in the part of the epigraph that lies above the domain $\\mathcal{X}$, which can be stated as:\n$$\n\\begin{aligned}\n\\text{minimize} & \\quad t \\\\\n\\text{subject to} & \\quad t \\ge f(x) \\\\\n& \\quad x \\in \\mathcal{X}\n\\end{aligned}\n$$\nThe constraint $t \\ge f(x)$ is generally nonlinear and makes the problem difficult to solve directly. The cutting-plane method addresses this by replacing the single nonlinear constraint with an evolving set of linear inequalities, or \"cuts.\" This creates a polyhedral outer approximation of the epigraph.\n\nFor a convex function $f$, the foundation for these cuts is the subgradient inequality. At any point $x^{(i)} \\in \\operatorname{dom}(f)$, and for any subgradient $g^{(i)} \\in \\partial f(x^{(i)})$, we have the inequality:\n$$\nf(x) \\ge f(x^{(i)}) + g^{(i)\\top} (x - x^{(i)}) \\quad \\text{for all } x \\in \\operatorname{dom}(f)\n$$\nThis inequality defines a global affine underestimator for the function $f$. Since any point $(x, t)$ in the epigraph must satisfy $t \\ge f(x)$, it must also satisfy the weaker linear inequality:\n$$\nt \\ge f(x^{(i)}) + g^{(i)\\top} (x - x^{(i)})\n$$\nThis inequality defines a supporting halfspace to the epigraph at the boundary point $(x^{(i)}, f(x^{(i)}))$. The iterative algorithm proceeds as follows:\n\n1.  **Initialization**: The algorithm begins at an initial point, specified as the center of the box, $x^{(0)} = \\frac{1}{2}(\\ell + u)$. We evaluate the function $f(x^{(0)})$ and compute a subgradient $g^{(0)} \\in \\partial f(x^{(0)})$. This defines the first cut, $t \\ge f(x^{(0)}) + g^{(0)\\top} (x - x^{(0)})$. The initial best upper bound on the minimum value is set to $U^{(0)} = f(x^{(0)})$. The lower bound is initialized to $L^{(0)} = -\\infty$.\n\n2.  **Iterative Refinement**: At each iteration $k \\in \\{1, 2, \\dots\\}$, we have accumulated a set of cuts from points $x^{(0)}, x^{(1)}_{\\text{LP}}, \\dots, x^{(k-1)}_{\\text{LP}}$. These cuts define a polyhedral set $\\mathcal{P}^{(k-1)}$ that contains the epigraph of $f$. We solve the following linear program (LP) to find the minimum value of $t$ over this polyhedral approximation intersected with the domain constraints:\n    $$\n    \\begin{aligned}\n    (x^{(k)}_{\\text{LP}}, t^{(k)}_{\\text{LP}}) = \\quad \\operatorname{argmin}_{x,t} & \\quad t \\\\\n    \\text{subject to} & \\quad t \\ge f(x^{(i)}) + g^{(i)\\top} (x - x^{(i)}), \\quad i=0, \\dots, k-1 \\\\\n    & \\quad \\ell \\le x \\le u\n    \\end{aligned}\n    $$\n    The objective value of this LP, $t^{(k)}_{\\text{LP}}$, provides a new lower bound on the true minimum of $f$, so we set $L^{(k)} = t^{(k)}_{\\text{LP}}$. Since $\\mathcal{P}^{(k-1)}$ contains $\\operatorname{epi}(f)$, $L^{(k)}$ is guaranteed to be less than or equal to the true minimum. As more cuts are added, the approximation becomes tighter, and the sequence of lower bounds $\\{L^{(k)}\\}$ is monotonically non-decreasing.\n\n3.  **Updating Bounds and Generating New Cuts**: The spatial component of the LP solution, $x^{(k)}_{\\text{LP}}$, is a candidate for the true minimizer. We evaluate the function at this point, $f(x^{(k)}_{\\text{LP}})$, and update the incumbent (best-so-far) upper bound: $U^{(k)} = \\min(U^{(k-1)}, f(x^{(k)}_{\\text{LP}}))$. The sequence of upper bounds $\\{U^{(k)}\\}$ is monotonically non-increasing. A new cut is then generated at $x^{(k)}_{\\text{LP}}$ using a subgradient $g^{(k)} \\in \\partial f(x^{(k)}_{\\text{LP}})$. This new cut, $t \\ge f(x^{(k)}_{\\text{LP}}) + g^{(k)\\top} (x - x^{(k)}_{\\text{LP}})$, is added to the set for the next iteration, refining the polyhedral approximation.\n\n4.  **Termination**: The algorithm terminates when the gap between the upper and lower bounds is sufficiently small, i.e., $U^{(k)} - L^{(k)} \\le \\varepsilon$, for a given tolerance $\\varepsilon$. This condition ensures that the incumbent solution $U^{(k)}$ is $\\varepsilon$-optimal. A maximum number of iterations, $K_{\\max}$, is also used as a safeguard to ensure termination.\n\nThe implementation uses `scipy.optimize.linprog` to solve the LP subproblem at each iteration. The LP is formulated with $n+1$ variables $(x_1, \\dots, x_n, t)$. The objective is to minimize $t$, corresponding to an objective vector $c = [0, \\dots, 0, 1]^\\top$. Each cut constraint $t \\ge g^{(i)\\top}x + (f(x^{(i)}) - g^{(i)\\top}x^{(i)})$ is rewritten in the standard form $A_{ub}z \\le b_{ub}$ required by the solver, which for a variable vector $z = [x^\\top, t]^\\top$ becomes $g^{(i)\\top}x - t \\le g^{(i)\\top}x^{(i)} - f(x^{(i)})$. The box constraints $\\ell \\le x \\le u$ are passed directly to the solver's `bounds` parameter.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef cutting_plane_algorithm(f, g_oracle, l, u, eps, K_max):\n    \"\"\"\n    Implements Kelley's cutting-plane method using an epigraph formulation.\n\n    Args:\n        f (callable): The convex objective function.\n        g_oracle (callable): A function providing a subgradient of f.\n        l (np.ndarray): The lower bounds of the box domain.\n        u (np.ndarray): The upper bounds of the box domain.\n        eps (float): The tolerance for the stopping condition (U - L <= eps).\n        K_max (int): The maximum number of iterations.\n\n    Returns:\n        list: A list containing [L, U, G, K, C] where L is the final lower bound,\n              U is the final upper bound, G is the optimality gap, K is the number\n              of iterations, and C is the convergence flag.\n    \"\"\"\n    n = len(l)\n    \n    # Initialization (k=0)\n    x_k = 0.5 * (l + u)\n    f_k = f(x_k)\n    g_k = g_oracle(x_k)\n    \n    # Store cuts as tuples of (subgradient, function value, dot_product)\n    # The dot product is pre-calculated for efficiency in building the LP.\n    cuts = [(g_k, f_k, np.dot(g_k, x_k))]\n    \n    # Initial upper bound is the function value at the center. Lower bound is -inf.\n    U = f_k\n    L = -np.inf\n    \n    k_iter = 0\n    converged = False\n    final_gap = U - L\n\n    # Main loop for iterations k = 1, 2, ...\n    for k in range(1, K_max + 1):\n        k_iter = k\n        \n        # --- Construct and solve the Linear Program ---\n        # The variables of the LP are [x_1, ..., x_n, t].\n        c_obj = np.zeros(n + 1)\n        c_obj[n] = 1.0  # Objective: minimize t\n\n        # The constraints are t >= g_i^T x + (f_i - g_i^T x_i),\n        # which can be rewritten as g_i^T x - t <= g_i^T x_i - f_i.\n        num_cuts = len(cuts)\n        A_ub = np.zeros((num_cuts, n + 1))\n        b_ub = np.zeros(num_cuts)\n\n        for i in range(num_cuts):\n            g_i, f_i, g_dot_x_i = cuts[i]\n            A_ub[i, :n] = g_i\n            A_ub[i, n] = -1.0\n            b_ub[i] = g_dot_x_i - f_i\n        \n        # Bounds for the variables [x_1, ..., x_n, t]\n        bounds_x = list(zip(l, u))\n        bounds_t = (None, None)\n        all_bounds = bounds_x + [bounds_t]\n        \n        # Solve the LP subproblem using the 'highs' method.\n        res = linprog(c_obj, A_ub=A_ub, b_ub=b_ub, bounds=all_bounds, method='highs')\n        \n        if not res.success:\n            # If the LP solver fails, terminate and report the last valid state.\n            break\n\n        sol = res.x\n        x_lp_k = sol[:n]\n        t_lp_k = sol[n]\n        \n        # --- Update bounds and check for termination ---\n        L = t_lp_k\n        f_at_x_lp_k = f(x_lp_k)\n        U = min(U, f_at_x_lp_k)\n        \n        gap = U - L\n        final_gap = gap\n        if gap <= eps:\n            converged = True\n            break\n        \n        # --- Add a new cut for the next iteration ---\n        g_k_new = g_oracle(x_lp_k)\n        cuts.append((g_k_new, f_at_x_lp_k, np.dot(g_k_new, x_lp_k)))\n        \n    return [L, U, final_gap, k_iter, converged]\n\ndef solve():\n    # Define algorithmic parameters\n    eps = 1e-5\n    K_max = 100\n\n    # Test Case 1: Nondifferentiable univariate\n    f1 = lambda x: np.abs(x[0]) + 0.5 * x[0]\n    def g1(x):\n        if x[0] > 0: return np.array([1.5])\n        elif x[0] < 0: return np.array([-0.5])\n        else: return np.array([0.0])\n    l1 = np.array([-3.0])\n    u1 = np.array([3.0])\n\n    # Test Case 2: Smooth strongly convex quadratic\n    f2 = lambda x: x[0]**2 + x[1]**2 - 2*x[0] + x[1]\n    g2 = lambda x: np.array([2*x[0] - 2, 2*x[1] + 1])\n    l2 = np.array([-2.0, -2.0])\n    u2 = np.array([2.0, 2.0])\n\n    # Test Case 3: Convex piecewise-linear\n    components3 = [(np.array([1.0, -1.0]), 0.0), \n                   (np.array([-2.0, 1.0]), 1.0), \n                   (np.array([0.5, 0.5]), -1.0)]\n    def f3(x):\n        return np.max([np.dot(c, x) + b for c, b in components3])\n    def g3(x):\n        vals = [np.dot(c, x) + b for c, b in components3]\n        max_idx = np.argmax(vals) # argmax uses the first occurrence in case of a tie\n        return components3[max_idx][0]\n    l3 = np.array([-2.0, -2.0])\n    u3 = np.array([2.0, 2.0])\n\n    # Test Case 4: Smooth convex log-sum-exp\n    def f4(x):\n        return np.log(np.exp(x[0]) + np.exp(x[1]))\n    def g4(x):\n        exps = np.exp(x)\n        return exps / np.sum(exps)\n    l4 = np.array([-2.0, -2.0])\n    u4 = np.array([2.0, 2.0])\n\n    test_cases = [\n        (f1, g1, l1, u1),\n        (f2, g2, l2, u2),\n        (f3, g3, l3, u3),\n        (f4, g4, l4, u4),\n    ]\n\n    all_results = []\n    for f, g, l, u in test_cases:\n        result = cutting_plane_algorithm(f, g, l, u, eps, K_max)\n        all_results.append(result)\n\n    # Format output string exactly as required (list of lists, no spaces)\n    results_str_list = []\n    for res in all_results:\n        L, U, G, K, C = res\n        # Round L, U, G to 6 decimal places and format string representation\n        s = f\"[{round(L, 6):.6f},{round(U, 6):.6f},{round(G, 6):.6f},{K},{C}]\"\n        results_str_list.append(s)\n\n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3125647"}]}