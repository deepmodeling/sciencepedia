{"hands_on_practices": [{"introduction": "The definition of a convex set—that the line segment between any two points in the set must also be in the set—is simple, yet its consequences are profound. This first exercise provides a direct way to test this property. By analyzing a common non-convex constraint, $x_{1} x_{2} \\le 1$, you will use the fundamental definition of a convex combination to demonstrate its non-convexity, building concrete intuition for why certain sets fail this crucial test [@problem_id:3114525]. This practice also introduces convex relaxation, a powerful method for transforming non-convex problems.", "problem": "Let $S \\subset \\mathbb{R}^{2}$ be defined by the nonconvex inequality $x_{1} x_{2} \\leq 1$ together with bound constraints $x_{1} \\in \\left[\\tfrac{1}{4}, 2\\right]$ and $x_{2} \\in \\left[\\tfrac{1}{4}, 2\\right]$. Recall the definition: a set $C$ is convex if for any $x, y \\in C$ and any $t \\in [0,1]$, the convex combination $t x + (1-t) y \\in C$. Using this fundamental definition and the monotonicity of the natural logarithm, address the following.\n\n1) Consider the two feasible points $x^{A} = \\left(\\tfrac{1}{4}, 2\\right)$ and $x^{B} = \\left(2, \\tfrac{1}{4}\\right)$ in $S$. Parameterize their convex combinations as $x(t) = t x^{A} + (1-t) x^{B}$ for $t \\in [0,1]$. Starting from first principles, derive the expression for the product $P(t) = x_{1}(t)\\, x_{2}(t)$ and use it to demonstrate that some convex combinations of $x^{A}$ and $x^{B}$ are infeasible with respect to $x_{1} x_{2} \\leq 1$.\n\n2) Using only the above definition and standard calculus on polynomials, determine the exact maximum value of the violation of feasibility along the segment, defined as\n$$\nV^{\\star} \\triangleq \\max_{t \\in [0,1]} \\big(P(t) - 1\\big).\n$$\nProvide $V^{\\star}$ in exact form (no rounding).\n\n3) Propose a convex relaxation that is valid on the positive orthant by introducing the change of variables $y_{i} = \\ln(x_{i})$ for $i \\in \\{1,2\\}$, where $\\ln$ denotes the natural logarithm. Use the monotonicity of $\\ln(\\cdot)$ to transform the multiplicative constraint and the bound constraints, and justify why the resulting feasible region in $(y_{1}, y_{2})$ is convex.\n\nYour final answer should be the exact value of $V^{\\star}$ from part $2$, expressed as a single reduced fraction or a closed-form expression. Do not round.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The set $S \\subset \\mathbb{R}^{2}$ is defined by the inequality $x_{1} x_{2} \\leq 1$.\n- The variables are subject to bound constraints $x_{1} \\in \\left[\\tfrac{1}{4}, 2\\right]$ and $x_{2} \\in \\left[\\tfrac{1}{4}, 2\\right]$.\n- The definition of a convex set is provided: $C$ is convex if for any $x, y \\in C$ and any $t \\in [0,1]$, the point $t x + (1-t) y$ is also in $C$.\n- Two points in $S$ are given: $x^{A} = \\left(\\tfrac{1}{4}, 2\\right)$ and $x^{B} = \\left(2, \\tfrac{1}{4}\\right)$.\n- A convex combination of these points is parameterized as $x(t) = t x^{A} + (1-t) x^{B}$ for $t \\in [0,1]$.\n- A function $P(t) = x_{1}(t)\\, x_{2}(t)$ is defined.\n- The maximum violation of feasibility is defined as $V^{\\star} \\triangleq \\max_{t \\in [0,1]} \\big(P(t) - 1\\big)$.\n- A change of variables for a convex relaxation is proposed: $y_{i} = \\ln(x_{i})$ for $i \\in \\{1,2\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically well-defined and self-contained. The points $x^A$ and $x^B$ are verified to be in $S$:\n- For $x^A$: $x_1 x_2 = \\frac{1}{4} \\cdot 2 = \\frac{1}{2} \\leq 1$. The bounds $\\frac{1}{4} \\in [\\frac{1}{4}, 2]$ and $2 \\in [\\frac{1}{4}, 2]$ are satisfied.\n- For $x^B$: $x_1 x_2 = 2 \\cdot \\frac{1}{4} = \\frac{1}{2} \\leq 1$. The bounds $2 \\in [\\frac{1}{4}, 2]$ and $\\frac{1}{4} \\in [\\frac{1}{4}, 2]$ are satisfied.\nThe problem statement is grounded in the standard theory of convex analysis and optimization. The questions are precise and lead to a unique, meaningful solution based on standard calculus and algebraic manipulation. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution is provided below.\n\n### Part 1: Convex combinations and infeasibility\nWe are given two points $x^{A} = \\begin{pmatrix} \\frac{1}{4} \\\\ 2 \\end{pmatrix}$ and $x^{B} = \\begin{pmatrix} 2 \\\\ \\frac{1}{4} \\end{pmatrix}$. Their convex combination is $x(t) = t x^{A} + (1-t) x^{B}$ for $t \\in [0,1]$. The components of $x(t)$, denoted as $x_1(t)$ and $x_2(t)$, are:\n$$\nx_{1}(t) = t\\left(\\frac{1}{4}\\right) + (1-t)(2) = \\frac{1}{4}t + 2 - 2t = 2 - \\frac{7}{4}t\n$$\n$$\nx_{2}(t) = t(2) + (1-t)\\left(\\frac{1}{4}\\right) = 2t + \\frac{1}{4} - \\frac{1}{4}t = \\frac{1}{4} + \\frac{7}{4}t\n$$\nThe product $P(t) = x_{1}(t)\\, x_{2}(t)$ is derived by multiplying these two expressions:\n$$\nP(t) = \\left(2 - \\frac{7}{4}t\\right) \\left(\\frac{1}{4} + \\frac{7}{4}t\\right)\n$$\nExpanding the product:\n$$\nP(t) = 2\\left(\\frac{1}{4}\\right) + 2\\left(\\frac{7}{4}t\\right) - \\left(\\frac{7}{4}t\\right)\\left(\\frac{1}{4}\\right) - \\left(\\frac{7}{4}t\\right)\\left(\\frac{7}{4}t\\right)\n$$\n$$\nP(t) = \\frac{1}{2} + \\frac{14}{4}t - \\frac{7}{16}t - \\frac{49}{16}t^2\n$$\n$$\nP(t) = \\frac{1}{2} + \\left(\\frac{56}{16} - \\frac{7}{16}\\right)t - \\frac{49}{16}t^2\n$$\n$$\nP(t) = -\\frac{49}{16}t^2 + \\frac{49}{16}t + \\frac{1}{2}\n$$\nTo demonstrate that some convex combinations are infeasible, we need to show that $P(t) > 1$ for some $t \\in (0,1)$. Let's evaluate $P(t)$ at the midpoint of the interval, $t = \\frac{1}{2}$:\n$$\nP\\left(\\frac{1}{2}\\right) = -\\frac{49}{16}\\left(\\frac{1}{2}\\right)^2 + \\frac{49}{16}\\left(\\frac{1}{2}\\right) + \\frac{1}{2} = -\\frac{49}{64} + \\frac{49}{32} + \\frac{1}{2} = \\frac{-49 + 98 + 32}{64} = \\frac{81}{64}\n$$\nSince $81 > 64$, we have $P\\left(\\frac{1}{2}\\right) = \\frac{81}{64} > 1$. The point $x(\\frac{1}{2})$ is the midpoint of the segment connecting $x^A$ and $x^B$:\n$$\nx\\left(\\frac{1}{2}\\right) = \\frac{1}{2}x^A + \\frac{1}{2}x^B = \\frac{1}{2}\\begin{pmatrix} \\frac{1}{4}+2 \\\\ 2+\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{8} \\\\ \\frac{9}{8} \\end{pmatrix}\n$$\nFor this point, $x_1 x_2 = \\frac{9}{8} \\cdot \\frac{9}{8} = \\frac{81}{64} > 1$, which violates the constraint $x_1 x_2 \\leq 1$. This confirms that the set $S$ is not convex, as a convex combination of two points in $S$ does not lie in $S$.\n\n### Part 2: Maximum violation of feasibility\nWe need to find the maximum value of the violation, $V^{\\star} = \\max_{t \\in [0,1]} \\big(P(t) - 1\\big)$. This is equivalent to finding the maximum of $P(t)$ on the interval $[0,1]$ and then subtracting $1$.\nThe function to maximize is the quadratic polynomial $P(t) = -\\frac{49}{16}t^2 + \\frac{49}{16}t + \\frac{1}{2}$. Since the coefficient of the $t^2$ term is negative ($-\\frac{49}{16} < 0$), the graph of $P(t)$ is a parabola opening downwards. Its maximum occurs at its vertex.\nTo find the vertex, we compute the derivative of $P(t)$ with respect to $t$ and set it to $0$:\n$$\nP'(t) = \\frac{d}{dt}\\left(-\\frac{49}{16}t^2 + \\frac{49}{16}t + \\frac{1}{2}\\right) = -2\\left(\\frac{49}{16}\\right)t + \\frac{49}{16} = -\\frac{49}{8}t + \\frac{49}{16}\n$$\nSetting $P'(t) = 0$:\n$$\n-\\frac{49}{8}t + \\frac{49}{16} = 0 \\implies \\frac{49}{8}t = \\frac{49}{16} \\implies t = \\frac{49}{16} \\cdot \\frac{8}{49} = \\frac{1}{2}\n$$\nThe critical point $t = \\frac{1}{2}$ lies within the interval $[0,1]$. As this is the vertex of a downward-opening parabola, it corresponds to the global maximum of $P(t)$. The maximum value of $P(t)$ is $P(\\frac{1}{2})$, which we already calculated to be $\\frac{81}{64}$.\nThe maximum violation $V^{\\star}$ is therefore:\n$$\nV^{\\star} = P\\left(\\frac{1}{2}\\right) - 1 = \\frac{81}{64} - 1 = \\frac{81 - 64}{64} = \\frac{17}{64}\n$$\n\n### Part 3: Convex relaxation\nThe problem is defined for $x_1, x_2 > 0$ due to the bound constraints. We introduce the change of variables $y_i = \\ln(x_i)$ for $i \\in \\{1,2\\}$, which implies $x_i = \\exp(y_i)$.\nThe original constraints are:\n1. $x_1 x_2 \\leq 1$\n2. $\\frac{1}{4} \\leq x_1 \\leq 2$\n3. $\\frac{1}{4} \\leq x_2 \\leq 2$\n\nWe transform these constraints into the $(y_1, y_2)$ space.\nFor the first constraint, $x_1 x_2 \\leq 1$, we apply the natural logarithm function $\\ln(\\cdot)$ to both sides. Since $\\ln(\\cdot)$ is a monotonically increasing function, the direction of the inequality is preserved:\n$$\n\\ln(x_1 x_2) \\leq \\ln(1)\n$$\nUsing the logarithm property $\\ln(ab) = \\ln(a) + \\ln(b)$, we get:\n$$\n\\ln(x_1) + \\ln(x_2) \\leq 0\n$$\nSubstituting $y_1 = \\ln(x_1)$ and $y_2 = \\ln(x_2)$, the constraint becomes a linear inequality:\n$$\ny_1 + y_2 \\leq 0\n$$\nFor the bound constraints on $x_1$, we again apply the monotonic $\\ln(\\cdot)$ function:\n$$\n\\ln\\left(\\frac{1}{4}\\right) \\leq \\ln(x_1) \\leq \\ln(2)\n$$\n$$\n\\ln(4^{-1}) \\leq y_1 \\leq \\ln(2) \\implies -\\ln(4) \\leq y_1 \\leq \\ln(2)\n$$\nSimilarly, for the bound constraints on $x_2$:\n$$\n\\ln\\left(\\frac{1}{4}\\right) \\leq \\ln(x_2) \\leq \\ln(2) \\implies -\\ln(4) \\leq y_2 \\leq \\ln(2)\n$$\nThe feasible region in the $(y_1, y_2)$ space, let's call it $S_y$, is defined by the following system of linear inequalities:\n\\begin{align*}\ny_1 + y_2 &\\leq 0 \\\\\ny_1 &\\geq -\\ln(4) \\\\\ny_1 &\\leq \\ln(2) \\\\\ny_2 &\\geq -\\ln(4) \\\\\ny_2 &\\leq \\ln(2)\n\\end{align*}\nTo justify why the resulting set $S_y$ is convex, we note that each of these five inequalities defines a closed half-space in $\\mathbb{R}^2$. For example, $f(y_1, y_2) = y_1 + y_2$ is an affine function, and the set of points where $f(y_1, y_2) \\leq 0$ is a half-space. It is a fundamental theorem of convex analysis that any half-space is a convex set. The set $S_y$ is the intersection of these five convex sets. Another fundamental theorem states that the intersection of any number of convex sets is itself a convex set. Therefore, the feasible region $S_y$ in the $(y_1, y_2)$ variables is a convex set (specifically, a convex polygon). This transformation is a standard technique used in geometric programming to convert a certain class of non-convex problems into convex ones.", "answer": "$$\n\\boxed{\\frac{17}{64}}\n$$", "id": "3114525"}, {"introduction": "After exploring a non-convex set, we now turn to characterizing the structure of an important convex one. This practice focuses on the $\\ell_1$ ball, $B_1 = \\{x : \\|x\\|_1 \\le 1\\}$, a geometric object central to fields like compressed sensing and machine learning. You will prove that this entire set can be described as a convex combination of its \"corners\" (extreme points) and then leverage this structural insight to efficiently solve a linear optimization problem, demonstrating a foundational principle of convex optimization [@problem_id:3114531].", "problem": "Let $B_1 \\subset \\mathbb{R}^4$ denote the $\\ell_1$ ball defined by $B_1 = \\{ x \\in \\mathbb{R}^4 : \\|x\\|_1 \\le 1 \\}$, where $\\|x\\|_1 = \\sum_{i=1}^4 |x_i|$. Consider the linear optimization problem\n$$\n\\max_{x \\in \\mathbb{R}^4} \\;\\; a^{\\mathsf{T}} x \\quad \\text{subject to} \\quad \\|x\\|_1 \\le 1\n$$\nwith the given vector $a = \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\\\ \\frac{\\pi}{4} \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n\nUsing only the fundamental definitions of convex sets, convex combinations, extreme points, and the $\\ell_1$ norm, proceed as follows:\n\n1. Justify that every $x \\in B_1$ can be written as a convex combination of signed unit vectors and the origin by constructing explicit nonnegative weights $\\{w_i\\}_{i=0}^4$ summing to $1$ and vectors from $\\{0\\} \\cup \\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}$, where $e_i$ denotes the $i$-th standard basis vector. Then rigorously argue that the only extreme points of $B_1$ are the signed unit vectors $\\{\\pm e_i : i \\in \\{1,2,3,4\\}\\}$.\n\n2. Using your conclusion from part $1$ together with first-principles inequalities, compute the optimal value of the optimization problem above. Provide your final answer in exact form (no rounding).", "solution": "The problem is first validated to ensure it is well-posed, scientifically sound, and complete.\n\n### Step 1: Extract Givens\n- The set is the $\\ell_1$ ball in $\\mathbb{R}^4$: $B_1 = \\{ x \\in \\mathbb{R}^4 : \\|x\\|_1 \\le 1 \\}$.\n- The $\\ell_1$ norm is defined as $\\|x\\|_1 = \\sum_{i=1}^4 |x_i|$.\n- The optimization problem is $\\max_{x \\in \\mathbb{R}^4} \\;\\; a^{\\mathsf{T}} x$ subject to $x \\in B_1$.\n- The vector $a$ is given as $a = \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\\\ \\frac{\\pi}{4} \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n- The standard basis vectors in $\\mathbb{R}^4$ are denoted by $e_i$.\n- Part 1 requires justifying that any $x \\in B_1$ is a convex combination of $\\{0\\} \\cup \\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}$ and then arguing that the extreme points of $B_1$ are precisely $\\{\\pm e_i : i \\in \\{1,2,3,4\\}\\}$.\n- Part 2 requires computing the optimal value of the optimization problem using the conclusion from Part 1 and first-principles inequalities.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in convex analysis and linear optimization. The set $B_1$ is a compact and convex set (a convex polytope, specifically a cross-polytope). The objective function $a^{\\mathsf{T}}x$ is linear. By the Extreme Value Theorem, the maximum exists. The problem is well-defined, mathematically sound, and contains all necessary information. It is free of any scientific flaws or ambiguities. The problem is valid.\n\n### Step 3: Solution\n**Part 1: Characterization of the $\\ell_1$ ball $B_1$ and its extreme points.**\n\nFirst, we show that any point $x \\in B_1$ can be expressed as a convex combination of the vectors in the set $V = \\{0\\} \\cup \\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}$.\nLet $x = (x_1, x_2, x_3, x_4)^{\\mathsf{T}}$ be an arbitrary point in $B_1$. By definition, we have $\\|x\\|_1 = \\sum_{i=1}^4 |x_i| \\le 1$.\nWe can write the vector $x$ as a linear combination of the standard basis vectors: $x = \\sum_{i=1}^4 x_i e_i$.\nFor each non-zero component $x_i$, we can write $x_i = |x_i| \\text{sgn}(x_i)$, where $\\text{sgn}(x_i)$ is the sign function, which is $1$ if $x_i > 0$ and $-1$ if $x_i < 0$.\nSo, we can express $x$ as:\n$$x = \\sum_{i=1}^4 |x_i| (\\text{sgn}(x_i) e_i)$$\nwhere for any $i$ such that $x_i=0$, the corresponding term in the sum is zero. Let $v_i = \\text{sgn}(x_i) e_i$. Each $v_i$ is a signed standard basis vector, i.e., $v_i \\in \\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}$ if $x_i \\ne 0$.\n\nLet $S = \\|x\\|_1 = \\sum_{i=1}^4 |x_i|$. We know $0 \\le S \\le 1$. We can now construct a convex combination for $x$. Consider the following representation:\n$$x = \\left( \\sum_{i=1}^4 |x_i| v_i \\right) + (1 - S) \\cdot 0$$\nThis is a linear combination of the vectors $\\{v_1, v_2, v_3, v_4, 0\\}$, which are all in the set $V$. The coefficients (weights) are $w_i = |x_i|$ for $i \\in \\{1,2,3,4\\}$, and $w_0 = 1 - S$.\nThese weights are non-negative, since $|x_i| \\ge 0$ and $S \\le 1$ implies $1-S \\ge 0$.\nThe sum of the weights is:\n$$\\sum_{i=1}^4 w_i + w_0 = \\sum_{i=1}^4 |x_i| + (1 - S) = S + (1 - S) = 1$$\nThus, we have explicitly written any $x \\in B_1$ as a convex combination of vectors from the set $V$. This implies that $B_1$ is the convex hull of $V$, i.e., $B_1 = \\text{conv}(V)$.\n\nNext, we identify the extreme points of $B_1$. By the Krein-Milman theorem, the set of extreme points of a compact convex set must be a subset of any set of points whose convex hull generates the set. Therefore, the extreme points of $B_1$ must be a subset of $V = \\{0\\} \\cup \\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}$.\n\nLet's check which elements of $V$ are extreme points. An extreme point $p$ of a convex set $C$ cannot be written as $p = \\lambda y + (1-\\lambda)z$ for distinct points $y, z \\in C$ and $\\lambda \\in (0,1)$.\n\n- Is the origin $0$ an extreme point? No. We can write $0 = \\frac{1}{2}e_1 + \\frac{1}{2}(-e_1)$. Since $e_1 \\in B_1$ and $-e_1 \\in B_1$ are distinct points, $0$ is not an extreme point.\n\n- Are the signed unit vectors $\\{\\pm e_i\\}$ extreme points? Let's test $e_1$. Suppose $e_1 = \\lambda y + (1-\\lambda)z$ for some $y, z \\in B_1$ with $y \\ne z$ and $\\lambda \\in (0,1)$. The first component of this vector equation is $1 = \\lambda y_1 + (1-\\lambda)z_1$. Since $y, z \\in B_1$, we have $\\|y\\|_1 \\le 1$ and $\\|z\\|_1 \\le 1$, which implies $|y_1| \\le 1$ and $|z_1| \\le 1$. For a convex combination of two numbers not exceeding $1$ to be equal to $1$, both numbers must be $1$. Thus, $y_1=1$ and $z_1=1$.\nNow consider the norm of $y$: $\\|y\\|_1 = |y_1| + \\sum_{i=2}^4 |y_i| = 1 + \\sum_{i=2}^4 |y_i|$. Since $\\|y\\|_1 \\le 1$, we must have $\\sum_{i=2}^4 |y_i| \\le 0$. As the sum of non-negative terms, this forces $|y_2|=|y_3|=|y_4|=0$, so $y_2=y_3=y_4=0$. This means $y = (1,0,0,0)^{\\mathsf{T}} = e_1$. A similar argument for $z$ leads to $z = e_1$. This contradicts the assumption that $y \\ne z$. Therefore, $e_1$ must be an extreme point. The same argument holds for all other signed unit vectors $\\pm e_i$ for $i \\in \\{1,2,3,4\\}$.\n\nCombining these findings, the set of extreme points of $B_1$ is precisely $\\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}$.\n\n**Part 2: Solving the optimization problem.**\n\nThe problem is to find the maximum of the linear function $f(x) = a^{\\mathsf{T}}x$ over the compact convex set $B_1$. A fundamental result in convex optimization is that the maximum of a linear function over a compact convex set is always attained at one of its extreme points.\nFrom Part 1, the extreme points of $B_1$ are the $8$ vectors $\\{\\pm e_i : i=1,2,3,4\\}$. Therefore, the maximum value of $a^{\\mathsf{T}}x$ is the maximum value in the set $\\{a^{\\mathsf{T}}v : v \\in \\{\\pm e_1, \\pm e_2, \\pm e_3, \\pm e_4\\}\\}$.\n\nLet's evaluate $a^{\\mathsf{T}}v$ for these extreme points.\nFor any $i \\in \\{1,2,3,4\\}$, $a^{\\mathsf{T}}e_i = a_i$ and $a^{\\mathsf{T}}(-e_i) = -a_i$.\nThe maximum value is therefore:\n$$ \\max_{x \\in B_1} a^{\\mathsf{T}}x = \\max_{i \\in \\{1,2,3,4\\}} \\{a_i, -a_i\\} = \\max_{i \\in \\{1,2,3,4\\}} |a_i| $$\nThis maximum value is the $\\ell_\\infty$ norm of the vector $a$, denoted by $\\|a\\|_\\infty$.\n\nThis conclusion is also reachable via first-principles inequalities, as requested. For any $x \\in B_1$:\n$$ a^{\\mathsf{T}}x = \\sum_{i=1}^4 a_i x_i \\le \\sum_{i=1}^4 |a_i x_i| = \\sum_{i=1}^4 |a_i| |x_i| $$\nLet $\\|a\\|_\\infty = \\max_j |a_j|$. Then $|a_i| \\le \\|a\\|_\\infty$ for all $i$.\n$$ \\sum_{i=1}^4 |a_i| |x_i| \\le \\sum_{i=1}^4 \\|a\\|_\\infty |x_i| = \\|a\\|_\\infty \\sum_{i=1}^4 |x_i| = \\|a\\|_\\infty \\|x\\|_1 $$\nSince $x \\in B_1$, we have $\\|x\\|_1 \\le 1$. Thus, we have the upper bound:\n$$ a^{\\mathsf{T}}x \\le \\|a\\|_\\infty $$\nThis upper bound is attained if we can find an $x^* \\in B_1$ such that $a^{\\mathsf{T}}x^* = \\|a\\|_\\infty$. Let $k$ be an index such that $|a_k| = \\|a\\|_\\infty$. Choose $x^* = \\text{sgn}(a_k) e_k$.\nThen $\\|x^*\\|_1 = |\\text{sgn}(a_k)| = 1$, so $x^* \\in B_1$.\nAnd $a^{\\mathsf{T}}x^* = a^{\\mathsf{T}}(\\text{sgn}(a_k)e_k) = \\text{sgn}(a_k) a_k = |a_k| = \\|a\\|_\\infty$.\nThis confirms that the maximum value is indeed $\\|a\\|_\\infty$.\n\nNow we compute this value for the given vector $a = \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\\\ \\frac{\\pi}{4} \\\\ -\\frac{3}{5} \\end{pmatrix}$.\nWe need to find the maximum of the absolute values of its components:\n$$ \\|a\\|_\\infty = \\max \\left\\{ |1|, |-\\sqrt{2}|, \\left|\\frac{\\pi}{4}\\right|, \\left|-\\frac{3}{5}\\right| \\right\\} = \\max \\left\\{ 1, \\sqrt{2}, \\frac{\\pi}{4}, \\frac{3}{5} \\right\\} $$\nLet's compare these values:\n- $1$\n- $\\sqrt{2} \\approx 1.414$\n- $\\frac{\\pi}{4} \\approx \\frac{3.14159}{4} \\approx 0.785$\n- $\\frac{3}{5} = 0.6$\nThe maximum of these values is $\\sqrt{2}$.\n\nThe optimal value of the optimization problem is $\\sqrt{2}$. This maximum is achieved at $x^* = \\text{sgn}(a_2)e_2 = (-1)e_2 = (0,-1,0,0)^{\\mathsf{T}}$.", "answer": "$$\n\\boxed{\\sqrt{2}}\n$$", "id": "3114531"}, {"introduction": "Many practical optimization problems are non-convex, making them difficult to solve globally. A key strategy is to replace a non-convex feasible set with its convex hull, creating a \"relaxed\" problem that is easier to solve. This exercise demonstrates this powerful technique by focusing on the non-convex equality $z = x(1-x)$, which often appears in statistical models [@problem_id:3114079]. You will derive the convex hull of its graph and then solve a linear problem over this new convex set, illustrating how geometric understanding transforms an intractable problem into a solvable one.", "problem": "Let $S \\subset \\mathbb{R}^{2}$ be the set $S = \\{(x,z) : z = x(1-x),\\ x \\in [0,1]\\}$. The function $f(x) = x(1-x)$ arises in optimization when modeling a logistic-like variance surrogate, since if $p \\in [0,1]$ represents a probability (for example, the output of a logistic transformation), the nonlinearity $v = p(1-p)$ is a smooth, unimodal, nonconvex term. In optimization models, the equality constraint $z = x(1-x)$ is nonconvex in the pair $(x,z)$, so a common convexification strategy is to replace it by the convex hull of its graph.\n\nTasks:\n1) Starting only from the definitions of convexity, concavity, and convex hull, derive an explicit inequality description of the convex hull $\\operatorname{conv}(S)$, expressed as a set of constraints on $(x,z)$ over the domain $x \\in [0,1]$.\n2) Consider the convexified optimization problem that uses your description of $\\operatorname{conv}(S)$ to relax the nonconvex equality $z = x(1-x)$ in a simple linear objective that encourages large variance surrogate while modestly penalizing $x$:\nminimize $J(x,z) = x - 3 z$ subject to $(x,z) \\in \\operatorname{conv}(S)$.\nCompute the exact optimal objective value of this convex optimization problem. Give your final answer as a single exact number with no units. If any numerical approximation were required, you would be instructed to round to a specified number of significant figures; here, no rounding is required.", "solution": "The problem is divided into two parts. First, we must derive an explicit inequality description for the convex hull of the set $S = \\{(x,z) : z = x(1-x), x \\in [0,1]\\}$. Second, we must solve a linear optimization problem over this convex hull.\n\n### Part 1: Description of the Convex Hull $\\operatorname{conv}(S)$\n\nThe set $S$ is the graph of the function $f(x) = x(1-x)$ over the interval $[0,1]$. To find the convex hull, $\\operatorname{conv}(S)$, we first analyze the function $f(x)$. The second derivative is $f''(x) = -2$, which is negative for all $x$. Therefore, $f(x)$ is a strictly concave function on the interval $[0,1]$.\n\nThe endpoints of the graph are at $x=0$ and $x=1$.\nAt $x=0$, we have $z = f(0) = 0(1-0) = 0$, so the point is $(0,0)$.\nAt $x=1$, we have $z = f(1) = 1(1-1) = 0$, so the point is $(1,0)$.\n\nThe convex hull of the graph of a concave function over a closed interval is the set of points lying on or between the graph and the line segment connecting the graph's endpoints. Here, the line segment connecting $(0,0)$ and $(1,0)$ is the segment of the $x$-axis from $x=0$ to $x=1$, which is described by $z=0$ for $x \\in [0,1]$. Since $f(x) = x(1-x) \\ge 0$ for $x \\in [0,1]$, the graph lies above this line segment.\n\nThus, we propose that the convex hull is the set $K = \\{(x,z) \\in \\mathbb{R}^2 \\mid 0 \\le x \\le 1, 0 \\le z \\le x(1-x)\\}$.\nTo prove this rigorously, starting from the definitions, we must show that $K = \\operatorname{conv}(S)$.\n\n1.  **Show that $K$ is a convex set.**\n    Let $(x_1, z_1)$ and $(x_2, z_2)$ be two points in $K$. Let $\\lambda \\in [0,1]$. We must show that the point $(x_\\lambda, z_\\lambda) = \\lambda(x_1, z_1) + (1-\\lambda)(x_2, z_2)$ is also in $K$.\n    The coordinates are $x_\\lambda = \\lambda x_1 + (1-\\lambda) x_2$ and $z_\\lambda = \\lambda z_1 + (1-\\lambda) z_2$.\n    - Since $x_1, x_2 \\in [0,1]$, and $[0,1]$ is a convex set, $x_\\lambda \\in [0,1]$.\n    - Since $z_1 \\ge 0$ and $z_2 \\ge 0$, and $\\lambda, (1-\\lambda) \\ge 0$, it follows that $z_\\lambda = \\lambda z_1 + (1-\\lambda) z_2 \\ge 0$.\n    - We must show $z_\\lambda \\le f(x_\\lambda) = x_\\lambda(1-x_\\lambda)$. We know $z_1 \\le f(x_1)$ and $z_2 \\le f(x_2)$.\n    $$z_\\lambda = \\lambda z_1 + (1-\\lambda) z_2 \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)$$\n    By the definition of a concave function, for any $\\lambda \\in [0,1]$, $\\lambda f(x_1) + (1-\\lambda) f(x_2) \\le f(\\lambda x_1 + (1-\\lambda) x_2)$.\n    Therefore,\n    $$z_\\lambda \\le f(\\lambda x_1 + (1-\\lambda) x_2) = f(x_\\lambda)$$\n    This shows that $(x_\\lambda, z_\\lambda) \\in K$. Thus, $K$ is a convex set.\n\n2.  **Show that $\\operatorname{conv}(S) \\subseteq K$.**\n    Any point $(x,z) \\in S$ satisfies $z=x(1-x)$ and $x \\in [0,1]$. For $x \\in [0,1]$, we have $x(1-x) \\ge 0$, so $z \\ge 0$. This means any point in $S$ satisfies the inequalities defining $K$, so $S \\subseteq K$. Since $K$ is a convex set containing $S$, by the definition of a convex hull (as the smallest convex set containing $S$), we must have $\\operatorname{conv}(S) \\subseteq K$.\n\n3.  **Show that $K \\subseteq \\operatorname{conv}(S)$.**\n    Let $(x,z)$ be an arbitrary point in $K$. So, $0 \\le x \\le 1$ and $0 \\le z \\le x(1-x)$.\n    If $x=0$ or $x=1$, then $x(1-x)=0$, so the inequality $0 \\le z \\le x(1-x)$ implies $z=0$. The points are $(0,0)$ and $(1,0)$, which are in $S$, and therefore in $\\operatorname{conv}(S)$.\n    Now assume $x \\in (0,1)$, which implies $x(1-x) > 0$.\n    Consider the two points $P_1 = (x, x(1-x))$ and $P_2 = (x, 0)$. The point $P_1$ is in $S$. The point $P_2$ can be written as a convex combination of the endpoints of $S$: $P_2 = (x,0) = (1-x)(0,0) + x(1,0)$. Since $(0,0) \\in S$ and $(1,0) \\in S$, $P_2 \\in \\operatorname{conv}(S)$.\n    The point $(x,z)$ lies on the vertical line segment connecting $P_2$ and $P_1$. We can write it as a convex combination of these two points: $(x,z) = \\lambda P_1 + (1-\\lambda) P_2$ for some $\\lambda \\in [0,1]$.\n    Comparing the $z$-coordinates: $z = \\lambda (x(1-x)) + (1-\\lambda) \\cdot 0 = \\lambda x(1-x)$.\n    This gives $\\lambda = \\frac{z}{x(1-x)}$.\n    From the condition $(x,z) \\in K$, we have $0 \\le z \\le x(1-x)$. Since $x(1-x)>0$, dividing by it gives $0 \\le \\frac{z}{x(1-x)} \\le 1$, so $0 \\le \\lambda \\le 1$.\n    Thus, $(x,z)$ is a convex combination of $P_1$ and $P_2$. Since $P_1 \\in S \\subseteq \\operatorname{conv}(S)$ and $P_2 \\in \\operatorname{conv}(S)$, and $\\operatorname{conv}(S)$ is a convex set, any convex combination of its points must also be in $\\operatorname{conv}(S)$. Therefore, $(x,z) \\in \\operatorname{conv}(S)$.\n    This proves that $K \\subseteq \\operatorname{conv}(S)$.\n\nCombining the two inclusions, we conclude that $\\operatorname{conv}(S) = K$. The explicit inequality description is:\n$$ \\operatorname{conv}(S) = \\{(x,z) \\mid 0 \\le x \\le 1, \\, 0 \\le z, \\, z \\le x - x^2 \\} $$\n\n### Part 2: Solving the Optimization Problem\n\nWe are asked to solve the following optimization problem:\n$$ \\text{minimize } J(x,z) = x - 3z $$\n$$ \\text{subject to } (x,z) \\in \\operatorname{conv}(S) $$\nThe objective function $J(x,z)$ is linear. The feasible region $\\operatorname{conv}(S)$ is a compact convex set. A fundamental theorem of optimization states that the minimum of a linear function over a compact convex set is achieved at one of its extreme points.\n\nThe set of extreme points of the convex hull of a set $A$, denoted $\\operatorname{Ext}(\\operatorname{conv}(A))$, is always a subset of $A$. In this case, the set $S$ is the graph of a strictly concave function. The extreme points of $\\operatorname{conv}(S)$ are precisely the points in $S$.\nTherefore, minimizing $J(x,z)$ over $\\operatorname{conv}(S)$ is equivalent to minimizing $J(x,z)$ over $S$.\n$$ \\min_{(x,z) \\in \\operatorname{conv}(S)} (x - 3z) = \\min_{(x,z) \\in S} (x - 3z) $$\nThe constraint $(x,z) \\in S$ means $z = x(1-x)$ and $x \\in [0,1]$. We can substitute the expression for $z$ into the objective function to obtain a one-dimensional optimization problem in terms of $x$:\n$$ g(x) = J(x, x(1-x)) = x - 3(x(1-x)) = x - 3x + 3x^2 = 3x^2 - 2x $$\nWe need to find the minimum value of $g(x) = 3x^2 - 2x$ on the closed interval $x \\in [0,1]$.\n\nThe function $g(x)$ is a quadratic function, representing a parabola opening upwards. Its global minimum occurs at its vertex. To find the vertex, we find the derivative of $g(x)$ and set it to zero:\n$$ g'(x) = \\frac{d}{dx}(3x^2 - 2x) = 6x - 2 $$\nSetting $g'(x) = 0$ gives $6x - 2 = 0$, which solves to $x = \\frac{2}{6} = \\frac{1}{3}$.\nSince this value $x=\\frac{1}{3}$ lies within the interval $[0,1]$, it is the location of the minimum of $g(x)$ on this interval.\n\nTo find the minimum objective value, we evaluate $g(x)$ at $x = \\frac{1}{3}$:\n$$ g\\left(\\frac{1}{3}\\right) = 3\\left(\\frac{1}{3}\\right)^2 - 2\\left(\\frac{1}{3}\\right) = 3\\left(\\frac{1}{9}\\right) - \\frac{2}{3} = \\frac{1}{3} - \\frac{2}{3} = -\\frac{1}{3} $$\nFor completeness, we can check the values at the endpoints of the interval:\nAt $x=0$: $g(0) = 3(0)^2 - 2(0) = 0$.\nAt $x=1$: $g(1) = 3(1)^2 - 2(1) = 3 - 2 = 1$.\nComparing the values $\\{-\\frac{1}{3}, 0, 1\\}$, the minimum is indeed $-\\frac{1}{3}$.\n\nThe optimal objective value of the convex optimization problem is $-\\frac{1}{3}$.", "answer": "$$\n\\boxed{-\\frac{1}{3}}\n$$", "id": "3114079"}]}