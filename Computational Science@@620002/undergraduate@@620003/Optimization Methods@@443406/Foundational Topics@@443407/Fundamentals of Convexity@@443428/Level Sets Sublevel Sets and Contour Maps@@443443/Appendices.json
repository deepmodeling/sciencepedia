{"hands_on_practices": [{"introduction": "The shape of a function's level sets offers a profound glimpse into the behavior of optimization algorithms. This practice explores a classic scenario where this geometry poses a significant challenge: a function with a \"flat valley.\" By analyzing the simple function $f(x,y) = x^2$, we will visualize how its linear level sets can stall a gradient-based search, and then discover how a simple modification, known as regularization, reshapes these contours into ellipses, effectively carving a path to the minimum. [@problem_id:3141941]", "problem": "Consider the function $f:\\mathbb{R}^2\\to\\mathbb{R}$ defined by $f(x,y)=x^2$. Using only the definitions of a level set $\\{(x,y):f(x,y)=c\\}$, a sublevel set $\\{(x,y):f(x,y)\\le c\\}$ for a scalar $c\\in\\mathbb{R}$, and the gradient-based update $z_{k+1}=z_k-\\alpha\\nabla f(z_k)$ for a step size $\\alpha>0$ in gradient methods, determine which of the following statements are correct. Also consider adding a small quadratic regularization to obtain $g(x,y)=x^2+\\varepsilon y^2$ with $\\varepsilon>0$, and an absolute-value (also called $\\ell_1$) regularization to obtain $h(x,y)=x^2+\\lambda |y|$ with $\\lambda>0$.\n\nA. For $c>0$, the level set $\\{(x,y):f(x,y)=c\\}$ consists of the two vertical lines $x=\\pm\\sqrt{c}$, and for $c=0$ it is the $y$-axis.\n\nB. For $c\\ge 0$, the sublevel set $\\{(x,y):f(x,y)\\le c\\}$ is the vertical strip $\\{(x,y):|x|\\le \\sqrt{c}\\}$, which is unbounded in $y$.\n\nC. For gradient descent applied to $f$ with any fixed step size $\\alpha>0$ and any starting point $(x_0,y_0)$ with $y_0\\ne 0$, the $y$-component of the iterates changes and moves toward $0$ over iterations.\n\nD. For $g(x,y)=x^2+\\varepsilon y^2$ with $\\varepsilon>0$, each contour $\\{(x,y):g(x,y)=c\\}$ for $c>0$ is an ellipse, and the gradient has a nonzero $y$-component whenever $y\\ne 0$, enabling gradient methods with suitable step sizes to contract both $x$ and $y$ toward $0$.\n\nE. Replacing the quadratic term in $y$ by an absolute-value ($\\ell_1$) term, $h(x,y)=x^2+\\lambda |y|$ with $\\lambda>0$, produces smooth circular contours and a gradient with nonzero $y$-component for every $y$.", "solution": "The core functions and definitions are:\n- $f(x,y) = x^2$\n- $g(x,y) = x^2 + \\varepsilon y^2$ for $\\varepsilon > 0$\n- $h(x,y) = x^2 + \\lambda |y|$ for $\\lambda > 0$\n- Level set: $S_c = \\{(x,y) \\in \\mathbb{R}^2 : f(x,y) = c\\}$\n- Sublevel set: $K_c = \\{(x,y) \\in \\mathbb{R}^2 : f(x,y) \\le c\\}$\n- Gradient descent: $z_{k+1} = z_k - \\alpha \\nabla f(z_k)$, where $z_k = (x_k, y_k)^T$ and $\\alpha > 0$.\n\n**Analysis of Statement A**\n\nThis statement concerns the level sets of the function $f(x,y) = x^2$. A level set is defined by the equation $f(x,y)=c$ for some constant $c \\in \\mathbb{R}$. Here, the equation is $x^2 = c$.\n\n- Case 1: $c > 0$. The equation $x^2=c$ has two real solutions for $x$: $x = \\sqrt{c}$ and $x = -\\sqrt{c}$. Since the equation places no restriction on the variable $y$, the level set consists of all points $(x,y)$ where $x$ is one of these two values. These are precisely the two vertical lines defined by the equations $x=\\sqrt{c}$ and $x=-\\sqrt{c}$. This part of the statement is correct.\n- Case 2: $c = 0$. The equation becomes $x^2=0$, which has the unique solution $x=0$. Again, $y$ is unconstrained. The set of points $\\{(x,y) : x=0\\}$ is the definition of the $y$-axis. This part of the statement is also correct.\n- Case 3: $c < 0$. The equation $x^2=c$ has no real solutions for $x$, so the level set is the empty set $\\emptyset$. The statement does not address this case, but what it does state for $c \\ge 0$ is accurate.\n\nConclusion for A: The statement is a correct description of the level sets of $f(x,y)$ for $c \\ge 0$.\n**Verdict: Correct**\n\n**Analysis of Statement B**\n\nThis statement concerns the sublevel sets of $f(x,y) = x^2$. A sublevel set is defined by the inequality $f(x,y) \\le c$ for some constant $c \\in \\mathbb{R}$. The statement considers $c \\ge 0$. The inequality is $x^2 \\le c$.\n\nSince $c \\ge 0$, we can take the square root of both sides. The inequality $\\sqrt{x^2} \\le \\sqrt{c}$ is equivalent to $|x| \\le \\sqrt{c}$. This further expands to $-\\sqrt{c} \\le x \\le \\sqrt{c}$. Since there is no constraint on $y$, this inequality defines a region between the vertical lines $x=-\\sqrt{c}$ and $x=\\sqrt{c}$, inclusive. This region is a vertical strip that is infinite in the $y$-direction, i.e., unbounded in $y$. The statement's description, \"the vertical strip $\\{(x,y):|x|\\le \\sqrt{c}\\}$, which is unbounded in $y$\", is a precise and correct characterization of this set.\n\nConclusion for B: The statement is a correct description of the sublevel sets of $f(x,y)$ for $c \\ge 0$.\n**Verdict: Correct**\n\n**Analysis of Statement C**\n\nThis statement concerns the behavior of the gradient descent algorithm applied to $f(x,y) = x^2$. First, we compute the gradient of $f$:\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ 0 \\end{pmatrix} $$\nThe gradient descent update rule for a point $z_k = (x_k, y_k)^T$ is $z_{k+1} = z_k - \\alpha \\nabla f(z_k)$. In component form, this is:\n$$ x_{k+1} = x_k - \\alpha (2x_k) = (1 - 2\\alpha)x_k $$\n$$ y_{k+1} = y_k - \\alpha (0) = y_k $$\nThe update rule for the $y$-component is $y_{k+1} = y_k$. This means that for any step size $\\alpha$, the value of the $y$-component remains constant throughout all iterations: $y_k = y_{k-1} = \\dots = y_0$.\nThe statement claims that \"the $y$-component of the iterates changes and moves toward $0$\". Our derivation shows that the $y$-component does not change at all. It remains fixed at its initial value $y_0$. Therefore, the statement is false. The failure of the $y$-component to move towards the minimum (which is any point on the $y$-axis) is a key feature of optimizing this function.\n\nConclusion for C: The statement incorrectly describes the evolution of the $y$-component.\n**Verdict: Incorrect**\n\n**Analysis of Statement D**\n\nThis statement analyzes the regularized function $g(x,y) = x^2 + \\varepsilon y^2$ with $\\varepsilon > 0$.\n\n- Contours: A contour (level set) is given by $g(x,y) = c$ for a constant $c$. For $c > 0$, the equation is $x^2 + \\varepsilon y^2 = c$. Dividing by $c$, we get:\n$$ \\frac{x^2}{c} + \\frac{y^2}{c/\\varepsilon} = 1 \\quad \\text{or} \\quad \\frac{x^2}{(\\sqrt{c})^2} + \\frac{y^2}{(\\sqrt{c/\\varepsilon})^2} = 1 $$\nThis is the standard equation of an ellipse centered at the origin with semi-axes of length $\\sqrt{c}$ and $\\sqrt{c/\\varepsilon}$. The first part of the statement is correct.\n\n- Gradient: The gradient of $g(x,y)$ is:\n$$ \\nabla g(x,y) = \\begin{pmatrix} 2x \\\\ 2\\varepsilon y \\end{pmatrix} $$\nThe $y$-component of the gradient is $2\\varepsilon y$. Since $\\varepsilon > 0$, this component is non-zero whenever $y \\neq 0$. The second part of the statement is correct.\n\n- Gradient descent dynamics: The update rules are:\n$$ x_{k+1} = x_k - \\alpha(2x_k) = (1 - 2\\alpha)x_k $$\n$$ y_{k+1} = y_k - \\alpha(2\\varepsilon y_k) = (1 - 2\\alpha\\varepsilon)y_k $$\nFor the iterates to contract toward $0$, we require $|1 - 2\\alpha| < 1$ and $|1 - 2\\alpha\\varepsilon| < 1$.\nThe first inequality implies $-1 < 1 - 2\\alpha < 1$, which solves to $0 < \\alpha < 1$.\nThe second inequality implies $-1 < 1 - 2\\alpha\\varepsilon < 1$, which solves to $0 < \\alpha < 1/\\varepsilon$.\nA \"suitable step size\" $\\alpha$ that satisfies both conditions exists. For example, any $\\alpha$ such that $0 < \\alpha < \\min(1, 1/\\varepsilon)$ will cause both $|x_k|$ and $|y_k|$ to decrease geometrically towards $0$ (unless they start at $0$). Therefore, gradient methods can contract both $x$ and $y$ toward $0$. The final part of the statement is correct.\n\nConclusion for D: All parts of the statement are correct.\n**Verdict: Correct**\n\n**Analysis of Statement E**\n\nThis statement analyzes the function $h(x,y) = x^2 + \\lambda |y|$ with $\\lambda > 0$.\n\n- Contours: The contours are given by $x^2 + \\lambda |y| = c$. Let's analyze the shape. For a given c > 0:\n    - If $y \\ge 0$, the equation is $y = (c - x^2)/\\lambda$, which is a downward-opening parabola.\n    - If $y < 0$, the equation is $y = (x^2 - c)/\\lambda$, which is an upward-opening parabola.\nThe contour is formed by joining these two parabolic arcs along the $x$-axis (at $y=0$). The resulting shape is clearly not a circle. Furthermore, the function $|y|$ is not differentiable at $y=0$, which creates a \"crease\" or sharp corner in the level set along the line segment where $y=0$ and $|x| \\le \\sqrt{c}$. Therefore, the contours are not \"smooth.\" The claim that they are \"smooth circular contours\" is false.\n\n- Gradient: The function $h(x,y)$ is not differentiable with respect to $y$ at any point where $y=0$.\n    - For $y \\neq 0$, the partial derivative with respect to $y$ is $\\frac{\\partial h}{\\partial y} = \\lambda \\, \\text{sgn}(y)$, where $\\text{sgn}(y)$ is $+1$ for $y>0$ and $-1$ for $y<0$. In this case, the $y$-component of the gradient is $\\pm\\lambda$, which is non-zero.\n    - For $y = 0$, the gradient is not defined. In optimization, we would use the concept of a subgradient. The subdifferential of $|y|$ at $y=0$ is the interval $[-1, 1]$. Thus, the subdifferential of $h(x,y)$ with respect to $y$ at a point $(x,0)$ is the interval $[-\\lambda, \\lambda]$. This set contains $0$.\nThe statement claims the gradient has a non-zero $y$-component \"for every $y$\". This is false because the gradient is not defined for $y=0$. Even using the broader concept of subgradients, we see that $0$ is a possible value for the $y$-component of the subgradient at $y=0$.\n\nConclusion for E: The statement is incorrect on multiple grounds. The contours are neither smooth nor circular, and the gradient is not defined for all $y$, let alone having a universally non-zero $y$-component.\n**Verdict: Incorrect**\n\nSummary of verdicts:\n- A: Correct\n- B: Correct\n- C: Incorrect\n- D: Correct\n- E: Incorrect\n\nThe correct statements are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "3141941"}, {"introduction": "While the previous exercise showed how to improve the geometry of level sets by adding a term, a more powerful and systematic method exists for reshaping a problem's landscape. This practice introduces the concept of preconditioning as a geometric coordinate transformation. We will analyze a quadratic function with highly elongated (anisotropic) elliptical level sets—a common cause of slow convergence—and design a linear map that transforms them into perfect circles, making the optimization problem trivial for gradient-based methods. [@problem_id:3141910]", "problem": "Consider the twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(\\mathbf{x})=\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{H}\\,\\mathbf{x}$, where $\\mathbf{H}$ is the symmetric positive-definite matrix\n$$\n\\mathbf{H}=\\mathbf{R}(\\theta)^{\\top}\\begin{pmatrix}9 & 0 \\\\ 0 & 1\\end{pmatrix}\\mathbf{R}(\\theta),\n$$\nand $\\mathbf{R}(\\theta)$ is the rotation matrix\n$$\n\\mathbf{R}(\\theta)=\\begin{pmatrix}\\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta)\\end{pmatrix}\n$$\nwith $\\theta=\\frac{\\pi}{6}$. A linear reparameterization $ \\mathbf{y}=\\mathbf{T}\\mathbf{x} $ induces the transformed function $g(\\mathbf{y})=f(\\mathbf{T}^{-1}\\mathbf{y})$. Using only the foundational definitions of level sets, linear transformations, and the Euclidean norm, analyze how the level sets of $g$ depend on $\\mathbf{T}$ and $\\mathbf{H}$. Then, design an invertible linear map $\\mathbf{T}$ that reduces the anisotropy of the level sets of $g$ as much as possible in the Euclidean geometry, in the sense of minimizing the condition number (ratio of largest to smallest eigenvalue) of the Hessian of $g$.\n\nYour final task is to compute the condition number of the Hessian of $g$ under your designed $\\mathbf{T}$ in exact form. Express your final answer as a single real number. No rounding is required.", "solution": "The function $f(\\mathbf{x})$ is a quadratic form, and its Hessian matrix is $\\nabla^2 f(\\mathbf{x}) = \\mathbf{H}$. The level sets of $f$ are given by the equation $f(\\mathbf{x}) = c$ for some constant $c > 0$, which is equivalent to $\\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{H}\\mathbf{x} = c$. These level sets are ellipses centered at the origin. The shape and orientation of these ellipses are determined by the eigenvalues and eigenvectors of $\\mathbf{H}$.\n\nThe structure of $\\mathbf{H} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{D}\\mathbf{R}(\\theta)$ is its spectral decomposition, where $\\mathbf{D}$ is the diagonal matrix of eigenvalues and the columns of $\\mathbf{R}(\\theta)^{\\top}$ are the corresponding eigenvectors. The eigenvalues of $\\mathbf{H}$ are $\\lambda_{\\max}(\\mathbf{H}) = 9$ and $\\lambda_{\\min}(\\mathbf{H}) = 1$. The matrix $\\mathbf{D}$ is:\n$$ \\mathbf{D} = \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nThe condition number of $\\mathbf{H}$ is given by the ratio of its largest to smallest eigenvalue:\n$$\n\\kappa(\\mathbf{H}) = \\frac{\\lambda_{\\max}(\\mathbf{H})}{\\lambda_{\\min}(\\mathbf{H})} = \\frac{9}{1} = 9.\n$$\nThis condition number quantifies the anisotropy of the level sets of $f$. A value of $\\kappa(\\mathbf{H}) > 1$ indicates that the level sets are stretched ellipses rather than circles. The ratio of the lengths of the principal semi-axes of the ellipses is $\\sqrt{\\kappa(\\mathbf{H})} = \\sqrt{9} = 3$.\n\nNext, we analyze the transformed function $g(\\mathbf{y})$. The reparameterization is $\\mathbf{y} = \\mathbf{T}\\mathbf{x}$, which implies $\\mathbf{x} = \\mathbf{T}^{-1}\\mathbf{y}$ since $\\mathbf{T}$ is invertible. Substituting this into the definition of $f$ gives the expression for $g$:\n$$\ng(\\mathbf{y}) = f(\\mathbf{T}^{-1}\\mathbf{y}) = \\frac{1}{2}(\\mathbf{T}^{-1}\\mathbf{y})^{\\top}\\mathbf{H}(\\mathbf{T}^{-1}\\mathbf{y}) = \\frac{1}{2}\\mathbf{y}^{\\top}(\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1}\\mathbf{y}.\n$$\nThis shows that $g(\\mathbf{y})$ is also a quadratic form. Its Hessian matrix, which we denote as $\\mathbf{H}_g$, is:\n$$\n\\mathbf{H}_g = \\nabla^2 g(\\mathbf{y}) = (\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1}.\n$$\nThe problem asks to design the invertible linear map $\\mathbf{T}$ to reduce the anisotropy of the level sets of $g$ as much as possible. This is equivalent to minimizing the condition number of the Hessian of $g$, $\\kappa(\\mathbf{H}_g)$. The condition number of any symmetric positive-definite matrix is always greater than or equal to $1$. The minimum possible value is $\\kappa(\\mathbf{H}_g) = 1$, which occurs if and only if all eigenvalues of $\\mathbf{H}_g$ are equal. This means $\\mathbf{H}_g$ must be a scalar multiple of the identity matrix, i.e., $\\mathbf{H}_g = \\alpha\\mathbf{I}$ for some scalar $\\alpha > 0$.\n\nOur design goal is to find a $\\mathbf{T}$ such that $\\mathbf{H}_g = \\alpha\\mathbf{I}$. Let's choose the simplest case where $\\alpha=1$, so we aim to find $\\mathbf{T}$ such that $\\mathbf{H}_g = \\mathbf{I}$.\n$$\n(\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1} = \\mathbf{I}.\n$$\nMultiplying from the left by $\\mathbf{T}^{\\top}$ and from the right by $\\mathbf{T}$, we get:\n$$\n\\mathbf{H} = \\mathbf{T}^{\\top}\\mathbf{T}.\n$$\nThis equation suggests that we need to find a matrix \"square root\" of $\\mathbf{H}$. Since $\\mathbf{H}$ is symmetric and positive definite, such a matrix $\\mathbf{T}$ exists. A common and constructive choice for $\\mathbf{T}$ is the principal square root of $\\mathbf{H}$, denoted $\\mathbf{H}^{1/2}$. The principal square root is unique, symmetric, and positive definite.\n\nUsing the spectral decomposition of $\\mathbf{H} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{D}\\mathbf{R}(\\theta)$, we can define $\\mathbf{H}^{1/2}$ as:\n$$\n\\mathbf{H}^{1/2} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{D}^{1/2}\\mathbf{R}(\\theta),\n$$\nwhere:\n$$ \\mathbf{D}^{1/2} = \\begin{pmatrix} \\sqrt{9} & 0 \\\\ 0 & \\sqrt{1} \\end{pmatrix} = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nLet's design our transformation matrix $\\mathbf{T}$ to be this principal square root, $\\mathbf{T} = \\mathbf{H}^{1/2}$. Since $\\mathbf{H}^{1/2}$ is symmetric, $\\mathbf{T}^{\\top} = (\\mathbf{H}^{1/2})^{\\top} = \\mathbf{H}^{1/2} = \\mathbf{T}$.\nThen, we verify the condition $\\mathbf{H} = \\mathbf{T}^{\\top}\\mathbf{T}$:\n$$\n\\mathbf{T}^{\\top}\\mathbf{T} = (\\mathbf{H}^{1/2})^{\\top}\\mathbf{H}^{1/2} = \\mathbf{H}^{1/2}\\mathbf{H}^{1/2} = (\\mathbf{R}(\\theta)^{\\top}\\mathbf{D}^{1/2}\\mathbf{R}(\\theta))(\\mathbf{R}(\\theta)^{\\top}\\mathbf{D}^{1/2}\\mathbf{R}(\\theta)).\n$$\nSince $\\mathbf{R}(\\theta)$ is an orthogonal matrix, $\\mathbf{R}(\\theta)\\mathbf{R}(\\theta)^{\\top} = \\mathbf{I}$. The expression simplifies to:\n$$\n\\mathbf{T}^{\\top}\\mathbf{T} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{D}^{1/2}(\\mathbf{R}(\\theta)\\mathbf{R}(\\theta)^{\\top})\\mathbf{D}^{1/2}\\mathbf{R}(\\theta) = \\mathbf{R}(\\theta)^{\\top}\\mathbf{D}^{1/2}\\mathbf{I}\\mathbf{D}^{1/2}\\mathbf{R}(\\theta) = \\mathbf{R}(\\theta)^{\\top}(\\mathbf{D}^{1/2})^2\\mathbf{R}(\\theta) = \\mathbf{R}(\\theta)^{\\top}\\mathbf{D}\\mathbf{R}(\\theta) = \\mathbf{H}.\n$$\nThe choice $\\mathbf{T} = \\mathbf{H}^{1/2}$ is thus a valid design. Now we compute the Hessian $\\mathbf{H}_g$ for this choice of $\\mathbf{T}$.\n$$\n\\mathbf{H}_g = (\\mathbf{T}^{-1})^{\\top}\\mathbf{H}\\mathbf{T}^{-1} = ((\\mathbf{H}^{1/2})^{-1})^{\\top}\\mathbf{H}(\\mathbf{H}^{1/2})^{-1}.\n$$\nSince $\\mathbf{H}^{1/2}$ is symmetric, its inverse $(\\mathbf{H}^{1/2})^{-1}$ is also symmetric. Thus, $((\\mathbf{H}^{1/2})^{-1})^{\\top} = (\\mathbf{H}^{1/2})^{-1}$.\n$$\n\\mathbf{H}_g = (\\mathbf{H}^{1/2})^{-1}\\mathbf{H}(\\mathbf{H}^{1/2})^{-1} = (\\mathbf{H}^{1/2})^{-1}(\\mathbf{H}^{1/2}\\mathbf{H}^{1/2})(\\mathbf{H}^{1/2})^{-1}.\n$$\nBy associativity of matrix multiplication:\n$$\n\\mathbf{H}_g = ((\\mathbf{H}^{1/2})^{-1}\\mathbf{H}^{1/2})(\\mathbf{H}^{1/2}(\\mathbf{H}^{1/2})^{-1}) = \\mathbf{I} \\cdot \\mathbf{I} = \\mathbf{I}.\n$$\nThe Hessian of the transformed function $g$ is the identity matrix $\\mathbf{I}$. The eigenvalues of the identity matrix are all equal to $1$.\nTherefore, for this designed transformation $\\mathbf{T}$, the eigenvalues of $\\mathbf{H}_g$ are $\\lambda_{\\max}(\\mathbf{H}_g) = 1$ and $\\lambda_{\\min}(\\mathbf{H}_g) = 1$.\nThe condition number of the Hessian of $g$ is:\n$$\n\\kappa(\\mathbf{H}_g) = \\frac{\\lambda_{\\max}(\\mathbf{H}_g)}{\\lambda_{\\min}(\\mathbf{H}_g)} = \\frac{1}{1} = 1.\n$$\nThis is the minimum possible condition number for a positive-definite matrix, so our designed transformation $\\mathbf{T} = \\mathbf{H}^{1/2}$ optimally reduces the anisotropy, making the level sets of $g$ perfect circles. The specific value of $\\theta=\\frac{\\pi}{6}$ is not needed for the final numerical answer, as the optimal reparameterization adapts to any rotation.", "answer": "$$\n\\boxed{1}\n$$", "id": "3141910"}, {"introduction": "Our exploration of function landscapes so far has been confined to smooth surfaces. However, many optimization problems, particularly in fields like machine learning and logistics, involve functions with \"sharp edges\" or \"kinks\" where they are not differentiable. This practice delves into the geometry of such a function, built using the maximum of two parabolas, whose level sets form a distinctive \"peanut\" shape. By analyzing these contours, we can visually identify the line of non-differentiability and reinforce key properties of convex functions, such as the existence of a single global minimum despite the lack of smoothness. [@problem_id:3141952]", "problem": "Consider the function $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ defined by $f(x,y)=\\max\\{(x-1)^2+y^2,\\,(x+1)^2+y^2\\}$. Use only the foundational definitions of a level set $\\{(x,y)\\in\\mathbb{R}^2:\\,f(x,y)=c\\}$, a sublevel set $\\{(x,y)\\in\\mathbb{R}^2:\\,f(x,y)\\le c\\}$, convexity, and differentiability to analyze the geometry of the contours (level sets), the structure of sublevel sets, the location and nature of minimizers, and any basin boundary phenomena associated with gradient-based optimization. Select all statements that are true.\n\nA. For any $c\\ge 1$, the sublevel set $\\{(x,y):\\,f(x,y)\\le c\\}$ is exactly the intersection of two disks of radius $\\sqrt{c}$ centered at $(1,0)$ and $(-1,0)$, and it is empty for $c<1$.\n\nB. The function $f$ is convex and attains a unique global minimum at $(0,0)$ with minimum value $1$.\n\nC. The set of nondifferentiability of $f$ is precisely the vertical line $\\{(0,y):\\,y\\in\\mathbb{R}\\}$.\n\nD. For any $c>1$, the level set $\\{(x,y):\\,f(x,y)=c\\}$ consists of two circular arcs (one from the circle centered at $(1,0)$ of radius $\\sqrt{c}$ and one from the circle centered at $(-1,0)$ of radius $\\sqrt{c}$) meeting at the points $(0,\\pm\\sqrt{c-1})$, forming a “peanut”-shaped closed curve; for $c=1$ the level set is the single point $(0,0)$; for $c<1$ it is empty.\n\nE. Gradient descent initialized anywhere in $\\mathbb{R}^2$ exhibits two basins of attraction separated by the line $x=0$, converging to distinct local minima near $(\\pm 1,0)$.", "solution": "### Function Analysis\n\nLet us define $f_1(x,y) = (x-1)^2+y^2$ and $f_2(x,y) = (x+1)^2+y^2$. The function is given by $f(x,y) = \\max\\{f_1(x,y), f_2(x,y)\\}$. Geometrically, $f_1(x,y)$ is the squared Euclidean distance from the point $(x,y)$ to $(1,0)$, and $f_2(x,y)$ is the squared Euclidean distance from $(x,y)$ to $(-1,0)$.\n\nThe function $f(x,y)$ is equal to $f_1(x,y)$ or $f_2(x,y)$ depending on which of $(1,0)$ or $(-1,0)$ is farther from $(x,y)$. The set of points where the distances are equal is the perpendicular bisector of the segment connecting $(-1,0)$ and $(1,0)$. This bisector is the line $x=0$.\n\nLet's verify this algebraically:\n$$f_1(x,y) = f_2(x,y)$$\n$$(x-1)^2+y^2 = (x+1)^2+y^2$$\n$$x^2 - 2x + 1 = x^2 + 2x + 1$$\n$$-2x = 2x \\implies 4x = 0 \\implies x=0$$\n\n-   If $x>0$, the point $(x,y)$ is closer to $(1,0)$ than to $(-1,0)$. Thus, the distance to $(-1,0)$ is greater, meaning $f_2(x,y) > f_1(x,y)$. So, $f(x,y) = f_2(x,y) = (x+1)^2+y^2$.\n-   If $x<0$, the point $(x,y)$ is farther from $(1,0)$ than from $(-1,0)$. Thus, $f_1(x,y) > f_2(x,y)$. So, $f(x,y) = f_1(x,y) = (x-1)^2+y^2$.\n-   If $x=0$, $f_1(0,y) = f_2(0,y) = 1+y^2$.\n\nSo, the function can be expressed piecewise as:\n$$ f(x,y) = \\begin{cases} (x-1)^2+y^2 & \\text{if } x \\le 0 \\\\ (x+1)^2+y^2 & \\text{if } x > 0 \\end{cases} $$\n(The choice of where to put equality $x=0$ is arbitrary).\n\n### Option-by-Option Analysis\n\n**A. For any $c \\ge 1$, the sublevel set $\\{(x,y):\\,f(x,y)\\le c\\}$ is exactly the intersection of two disks of radius $\\sqrt{c}$ centered at $(1,0)$ and $(-1,0)$, and it is empty for $c<1$.**\n\nThe sublevel set $S_c$ is defined as $\\{(x,y) \\in \\mathbb{R}^2 : f(x,y) \\le c\\}$.\nBy definition of $f$, this is equivalent to $\\max\\{f_1(x,y), f_2(x,y)\\} \\le c$.\nThis inequality holds if and only if both $f_1(x,y) \\le c$ and $f_2(x,y) \\le c$ are satisfied.\n1.  $f_1(x,y) \\le c \\iff (x-1)^2+y^2 \\le c$. This inequality defines a closed disk $D_1$ centered at $(1,0)$ with radius $\\sqrt{c}$, provided $c \\ge 0$.\n2.  $f_2(x,y) \\le c \\iff (x+1)^2+y^2 \\le c$. This inequality defines a closed disk $D_2$ centered at $(-1,0)$ with radius $\\sqrt{c}$, provided $c \\ge 0$.\nTherefore, the sublevel set $S_c$ is the intersection of these two disks: $S_c = D_1 \\cap D_2$.\n\nNow, we must find the minimum value of $f(x,y)$. The function is non-negative.\nThe global minimum of $f(x,y)$ is $1$, achieved at $(0,0)$.\nThis implies that for any $c<1$, there is no $(x,y)$ such that $f(x,y) \\le c$. Thus, the sublevel set $S_c$ is empty for $c<1$. For $c \\ge 1$, the set is non-empty.\n\nThe statement asserts that for $c \\ge 1$, the sublevel set is the intersection of the two disks, and for $c<1$ it is empty. Our analysis confirms both parts of this statement.\n\nVerdict: **Correct**.\n\n**B. The function $f$ is convex and attains a unique global minimum at $(0,0)$ with minimum value $1$.**\n\nFirst, let's analyze the convexity. The functions $f_1(x,y) = (x-1)^2+y^2$ and $f_2(x,y) = (x+1)^2+y^2$ are convex. This can be verified by checking their Hessians. For both functions, the Hessian matrix is\n$$ \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} $$\nwhich is positive definite everywhere. A fundamental theorem of convex analysis states that the pointwise maximum of a collection of convex functions is also a convex function. Since $f = \\max\\{f_1, f_2\\}$, and $f_1, f_2$ are convex, $f$ is also convex.\n\nNext, let's analyze the minimum. As established in the analysis for Option A, the minimum value of $f$ is $1$, and it is attained at $(0,0)$. To check if this minimum is unique, we can examine the function's behavior away from $(0,0)$.\nIf $(x,y) \\ne (0,0)$:\n-   If $x \\ge 0$, $f(x,y)=(x+1)^2+y^2$. Since $(x,y) \\ne (0,0)$ and $x \\ge 0$, we have either $x>0$ or $y \\ne 0$ (or both). If $x>0$, then $(x+1)^2 > 1$, so $f(x,y) > 1$. If $x=0$ and $y \\ne 0$, then $f(0,y) = 1+y^2 > 1$.\n-   If $x < 0$, $f(x,y)=(x-1)^2+y^2$. Since $x<0$, $x-1 < -1$, so $(x-1)^2 > 1$, which means $f(x,y) > 1$.\nIn all cases where $(x,y) \\ne (0,0)$, we have $f(x,y) > 1 = f(0,0)$. Thus, $(0,0)$ is the unique global minimum.\n\nThe statement correctly identifies $f$ as convex and accurately describes its unique global minimum.\n\nVerdict: **Correct**.\n\n**C. The set of nondifferentiability of $f$ is precisely the vertical line $\\{(0,y):\\,y\\in\\mathbb{R}\\}$.**\n\nA function defined as the maximum of two differentiable functions, $f=\\max\\{f_1, f_2\\}$, is differentiable at all points $z$ where $f_1(z) \\ne f_2(z)$. At points $z_0$ where $f_1(z_0) = f_2(z_0)$, the function $f$ is differentiable if and only if $\\nabla f_1(z_0) = \\nabla f_2(z_0)$. Otherwise, it is not differentiable.\n\nWe have already established that $f_1(x,y) = f_2(x,y)$ precisely on the line $x=0$.\nLet's compute the gradients of $f_1$ and $f_2$:\n$\\nabla f_1(x,y) = [2(x-1), 2y]^T$\n$\\nabla f_2(x,y) = [2(x+1), 2y]^T$\n\nNow we evaluate these gradients on the line $x=0$, for any point $(0,y)$:\n$\\nabla f_1(0,y) = [-2, 2y]^T$\n$\\nabla f_2(0,y) = [2, 2y]^T$\n\nFor $f$ to be differentiable at $(0,y)$, we would need $\\nabla f_1(0,y) = \\nabla f_2(0,y)$. This would require $-2 = 2$, which is false. Therefore, the gradients are never equal at any point on the line $x=0$.\nConsequently, the function $f$ is not differentiable at any point $(0,y)$ where $y \\in \\mathbb{R}$.\nFor any point where $x \\ne 0$, $f$ is equal to either $f_1$ or $f_2$, both of which are polynomials and thus infinitely differentiable.\nSo, the set of nondifferentiability is precisely the line $x=0$.\n\nVerdict: **Correct**.\n\n**D. For any $c>1$, the level set $\\{(x,y):\\,f(x,y)=c\\}$ consists of two circular arcs (one from the circle centered at $(1,0)$ of radius $\\sqrt{c}$ and one from the circle centered at $(-1,0)$ of radius $\\sqrt{c}$) meeting at the points $(0,\\pm\\sqrt{c-1})$, forming a “peanut”-shaped closed curve; for $c=1$ the level set is the single point $(0,0)$; for $c<1$ it is empty.**\n\nLet $L_c = \\{(x,y) : f(x,y)=c\\}$.\n-   **Case $c<1$**: As $\\min f(x,y) = 1$, $L_c$ is the empty set. This part is correct.\n-   **Case $c=1$**: As $(0,0)$ is the unique global minimum with value $1$, $f(x,y)=1$ if and only if $(x,y)=(0,0)$. So $L_1 = \\{(0,0)\\}$. This part is correct.\n-   **Case $c>1$**: A point $(x,y)$ is in $L_c$ if $\\max\\{f_1(x,y), f_2(x,y)\\} = c$.\n    -   Consider points with $x \\ge 0$. For these points, $f(x,y) = f_2(x,y) = (x+1)^2+y^2$. The level set condition is $(x+1)^2+y^2 = c$. This is the equation of a circle centered at $(-1,0)$ with radius $\\sqrt{c}$. We are restricted to the region $x \\ge 0$, so this part of the level set is a circular arc of this circle.\n    -   Consider points with $x < 0$. For these points, $f(x,y) = f_1(x,y) = (x-1)^2+y^2$. The level set condition is $(x-1)^2+y^2 = c$. This is the equation of a circle centered at $(1,0)$ with radius $\\sqrt{c}$. We are restricted to the region $x < 0$, so this part of the level set is a circular arc of this circle.\n    -   These two arcs meet at the boundary line $x=0$. To find the meeting points, we set $x=0$ in either equation (e.g., in $(x+1)^2+y^2=c$):\n        $(0+1)^2+y^2 = c \\implies 1+y^2=c \\implies y^2 = c-1 \\implies y = \\pm\\sqrt{c-1}$.\n        Since we are in the case $c>1$, $c-1 > 0$, so these are two distinct real points, $(0, \\sqrt{c-1})$ and $(0, -\\sqrt{c-1})$.\n    The description provided in the option perfectly matches this derivation. The curve is formed by two arcs from different circles, meeting at the specified points.\n\nVerdict: **Correct**.\n\n**E. Gradient descent initialized anywhere in $\\mathbb{R}^2$ exhibits two basins of attraction separated by the line $x=0$, converging to distinct local minima near $(\\pm 1,0)$.**\n\nThis statement claims the existence of multiple local minima and corresponding basins of attraction.\nFrom our analysis in Option B, we know that $f$ is a convex function. A key property of convex functions is that any local minimum is also a global minimum. We found that $f$ has a **unique global minimum** at $(0,0)$. Therefore, there are no other local minima. The points $(\\pm 1, 0)$ are not minima of $f$. For instance, $f(1,0) = \\max\\{(1-1)^2+0^2, (1+1)^2+0^2\\} = \\max\\{0,4\\}=4$, which is clearly not the minimum value of $1$.\n\nFor a convex function with a unique minimum, a properly configured gradient descent (or subgradient descent, for non-differentiable functions) algorithm is guaranteed to converge to that unique minimum, regardless of the starting point. This means that the entire space $\\mathbb{R}^2$ constitutes a single basin of attraction for the global minimum at $(0,0)$.\n\nThe statement's claims of two basins of attraction and distinct local minima are therefore fundamentally incorrect. The behavior of gradient descent is to always move towards the single global minimizer $(0,0)$. For instance, if starting at $x_0 > 0$, the negative gradient $-\\nabla f(x_0,y_0) = -[2(x_0+1), 2y_0]^T$ points towards decreasing $x$ and $y$. If starting at $x_0<0$, the negative gradient $-\\nabla f(x_0,y_0) = -[2(x_0-1), 2y_0]^T$ points towards increasing $x$ (since $x_0-1$ is negative) and decreasing $y$. In both cases, the trajectory is driven towards the line $x=0$ and ultimately to the origin $(0,0)$.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ABCD}$$", "id": "3141952"}]}