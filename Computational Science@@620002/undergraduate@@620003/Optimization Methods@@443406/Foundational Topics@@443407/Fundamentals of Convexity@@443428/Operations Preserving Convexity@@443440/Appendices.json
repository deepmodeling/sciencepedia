{"hands_on_practices": [{"introduction": "In optimization, we often construct complex models by combining simpler, known-convex components. This exercise [@problem_id:3160286] explores fundamental building blocks, such as adding functions and composing them with linear maps, which are ubiquitous in fields like machine learning for defining loss functions with regularizers. By analyzing how these operations affect both standard convexity and strong convexity, you will develop the essential skill of recognizing and proving convexity in sophisticated, real-world objective functions.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a fixed matrix, let $g:\\mathbb{R}^m \\to \\mathbb{R}$ be any function, and let $h:\\mathbb{R}^n \\to \\mathbb{R}$ be any function. Consider the composite objective $f:\\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(x) = g(Ax) + h(x)$. This form models a data-fit term $g$ applied to the linear features $Ax$ together with a regularizer $h$ on the decision variable $x$. Using only the definitions of convexity and strong convexity and elementary properties of linear maps, determine which of the following statements are true about operations that preserve convexity and strong convexity in this setting.\n\nA. If $g$ is convex and $h$ is convex, then $f$ is convex for any choice of the matrix $A$.\n\nB. If $g$ is convex and $h$ is $\\mu$-strongly convex for some $\\mu>0$, then $f$ is $\\mu$-strongly convex for any choice of the matrix $A$.\n\nC. If $g$ is $m$-strongly convex for some $m>0$ and $A$ has full column rank, and $h$ is convex, then $f$ is $m\\,\\lambda_{\\min}(A^\\top A)$-strongly convex, where $\\lambda_{\\min}(A^\\top A)$ denotes the smallest eigenvalue of $A^\\top A$.\n\nD. Choosing $h(x)=\\lambda\\|x\\|_1$ with $\\lambda>0$ always makes $f$ strongly convex, even if $g$ is only convex.\n\nE. If $A$ does not have full column rank, then no choice of convex $h$ can make $f$ strongly convex.\n\nSelect all correct statements.", "solution": "The problem statement is a well-posed mathematical inquiry within the field of convex optimization. It is scientifically grounded, objective, and contains sufficient information to determine the truth value of the provided statements. I will proceed with the solution.\n\nFirst, let us state the definitions of convexity and strong convexity, as per the problem's instructions.\n\nA function $F: \\mathcal{D} \\to \\mathbb{R}$ defined on a convex domain $\\mathcal{D} \\subseteq \\mathbb{R}^k$ is said to be **convex** if for all $x, y \\in \\mathcal{D}$ and for all $\\theta \\in [0, 1]$, the following inequality holds:\n$$F(\\theta x + (1-\\theta)y) \\le \\theta F(x) + (1-\\theta)F(y)$$\nA function $F: \\mathcal{D} \\to \\mathbb{R}$ is **$\\sigma$-strongly convex** for some $\\sigma > 0$ if for all $x, y \\in \\mathcal{D}$ and for all $\\theta \\in [0, 1]$, the following inequality holds:\n$$F(\\theta x + (1-\\theta)y) \\le \\theta F(x) + (1-\\theta)F(y) - \\frac{\\sigma}{2}\\theta(1-\\theta)\\|x-y\\|_2^2$$\nAn equivalent and often more useful definition for $\\sigma$-strong convexity is that the function $x \\mapsto F(x) - \\frac{\\sigma}{2}\\|x\\|_2^2$ is convex.\n\nWe are given the composite function $f:\\mathbb{R}^n \\to \\mathbb{R}$ defined as $f(x) = g(Ax) + h(x)$, where $A \\in \\mathbb{R}^{m \\times n}$, $g:\\mathbb{R}^m \\to \\mathbb{R}$, and $h:\\mathbb{R}^n \\to \\mathbb{R}$. We will now analyze each statement.\n\n### A. If $g$ is convex and $h$ is convex, then $f$ is convex for any choice of the matrix $A$.\n\nLet's analyze the two terms of $f(x)$ separately.\n1.  The term $h(x)$ is given to be convex.\n2.  Consider the term $\\phi(x) = g(Ax)$. The function $x \\mapsto Ax$ is a linear map from $\\mathbb{R}^n$ to $\\mathbb{R}^m$. For any $x, y \\in \\mathbb{R}^n$ and $\\theta \\in [0, 1]$, we have:\n    $$\\phi(\\theta x + (1-\\theta)y) = g(A(\\theta x + (1-\\theta)y))$$\n    By the linearity of the matrix-vector product, this becomes:\n    $$\\phi(\\theta x + (1-\\theta)y) = g(\\theta(Ax) + (1-\\theta)(Ay))$$\n    Since $g$ is convex, and $Ax, Ay \\in \\mathbb{R}^m$, we can apply the definition of convexity to $g$:\n    $$g(\\theta(Ax) + (1-\\theta)(Ay)) \\le \\theta g(Ax) + (1-\\theta)g(Ay)$$\n    Substituting back $\\phi(x)$ and $\\phi(y)$, we get:\n    $$\\phi(\\theta x + (1-\\theta)y) \\le \\theta \\phi(x) + (1-\\theta)\\phi(y)$$\n    This shows that $\\phi(x) = g(Ax)$ is a convex function. This property holds for any matrix $A$.\n3.  The function $f(x)$ is the sum of two convex functions, $\\phi(x) = g(Ax)$ and $h(x)$. The sum of convex functions is convex. To demonstrate this from first principles:\n    $$f(\\theta x + (1-\\theta)y) = g(A(\\theta x + (1-\\theta)y)) + h(\\theta x + (1-\\theta)y)$$\n    Since both terms are convex:\n    $$f(\\theta x + (1-\\theta)y) \\le [\\theta g(Ax) + (1-\\theta)g(Ay)] + [\\theta h(x) + (1-\\theta)h(y)]$$\n    $$f(\\theta x + (1-\\theta)y) \\le \\theta(g(Ax) + h(x)) + (1-\\theta)(g(Ay) + h(y))$$\n    $$f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y)$$\n    Therefore, $f(x)$ is convex.\n\nTherefore, statement (A) is **Correct**.\n\n### B. If $g$ is convex and $h$ is $\\mu$-strongly convex for some $\\mu>0$, then $f$ is $\\mu$-strongly convex for any choice of the matrix $A$.\n\nTo check if $f(x)$ is $\\mu$-strongly convex, we examine the convexity of the function $F(x) = f(x) - \\frac{\\mu}{2}\\|x\\|_2^2$.\n$$F(x) = (g(Ax) + h(x)) - \\frac{\\mu}{2}\\|x\\|_2^2$$\nWe can rearrange the terms as:\n$$F(x) = g(Ax) + \\left(h(x) - \\frac{\\mu}{2}\\|x\\|_2^2\\right)$$\n1.  From the analysis of statement A, since $g$ is convex, the function $x \\mapsto g(Ax)$ is also convex for any matrix $A$.\n2.  By the definition of $\\mu$-strong convexity for $h$, the function $x \\mapsto h(x) - \\frac{\\mu}{2}\\|x\\|_2^2$ is convex.\n3.  $F(x)$ is the sum of two convex functions. As shown in the analysis for A, the sum of two convex functions is convex.\nSince $F(x) = f(x) - \\frac{\\mu}{2}\\|x\\|_2^2$ is convex, $f(x)$ is, by definition, $\\mu$-strongly convex. This result does not depend on the choice of $A$.\n\nTherefore, statement (B) is **Correct**.\n\n### C. If $g$ is $m$-strongly convex for some $m>0$ and $A$ has full column rank, and $h$ is convex, then $f$ is $m\\,\\lambda_{\\min}(A^\\top A)$-strongly convex, where $\\lambda_{\\min}(A^\\top A)$ denotes the smallest eigenvalue of $A^\\top A$.\n\nLet's analyze the strong convexity of $\\phi(x) = g(Ax)$. Since $g$ is $m$-strongly convex, for any $u, v \\in \\mathbb{R}^m$ and $\\theta \\in [0,1]$:\n$$g(\\theta u + (1-\\theta)v) \\le \\theta g(u) + (1-\\theta)g(v) - \\frac{m}{2}\\theta(1-\\theta)\\|u-v\\|_2^2$$\nLet $u=Ax$ and $v=Ay$. Then:\n$$\\phi(\\theta x + (1-\\theta)y) = g(\\theta Ax + (1-\\theta)Ay) \\le \\theta g(Ax) + (1-\\theta)g(Ay) - \\frac{m}{2}\\theta(1-\\theta)\\|Ax-Ay\\|_2^2$$\n$$\\phi(\\theta x + (1-\\theta)y) \\le \\theta \\phi(x) + (1-\\theta)\\phi(y) - \\frac{m}{2}\\theta(1-\\theta)\\|A(x-y)\\|_2^2$$\nWe can write the squared norm as a quadratic form: $\\|A(x-y)\\|_2^2 = (x-y)^\\top A^\\top A (x-y)$. The matrix $A^\\top A \\in \\mathbb{R}^{n \\times n}$ is always positive semidefinite. The statement specifies that $A$ has full column rank, which means $\\text{rank}(A) = n$. This implies that $A^\\top A$ is positive definite, and all its eigenvalues are strictly positive. The smallest eigenvalue, $\\lambda_{\\min}(A^\\top A)$, is therefore greater than $0$.\nFor any vector $z \\in \\mathbb{R}^n$, the Rayleigh-Ritz quotient gives the bound $z^\\top(A^\\top A)z \\ge \\lambda_{\\min}(A^\\top A)\\|z\\|_2^2$. Applying this with $z=x-y$:\n$$\\|A(x-y)\\|_2^2 \\ge \\lambda_{\\min}(A^\\top A)\\|x-y\\|_2^2$$\nSubstituting this into the inequality for $\\phi$:\n$$\\phi(\\theta x + (1-\\theta)y) \\le \\theta \\phi(x) + (1-\\theta)\\phi(y) - \\frac{m\\,\\lambda_{\\min}(A^\\top A)}{2}\\theta(1-\\theta)\\|x-y\\|_2^2$$\nThis is the definition of strong convexity for $\\phi(x)$ with modulus $\\sigma_\\phi = m\\,\\lambda_{\\min}(A^\\top A)$.\nNow, $f(x) = \\phi(x) + h(x)$, where $\\phi(x)$ is $\\sigma_\\phi$-strongly convex and $h(x)$ is convex. Let's check if $f(x)$ is $\\sigma_\\phi$-strongly convex. Consider the function $f(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2$:\n$$f(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2 = \\left(\\phi(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2\\right) + h(x)$$\nThe first term, $\\phi(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2$, is convex by the definition of $\\sigma_\\phi$-strong convexity of $\\phi$. The second term, $h(x)$, is given as convex. The sum of these two convex functions is convex. Thus, $f(x)$ is $\\sigma_\\phi = m\\,\\lambda_{\\min}(A^\\top A)$-strongly convex.\n\nTherefore, statement (C) is **Correct**.\n\n### D. Choosing $h(x)=\\lambda\\|x\\|_1$ with $\\lambda>0$ always makes $f$ strongly convex, even if $g$ is only convex.\n\nLet $f(x) = g(Ax) + \\lambda\\|x\\|_1$.\nWe are given that $g$ is convex. From the analysis for statement A, $g(Ax)$ is a convex function. The function $h(x) = \\lambda\\|x\\|_1$ is also convex for $\\lambda > 0$, as it is a positive scaling of the L$1$-norm. Thus, $f(x)$ is always convex. The question is whether it is always *strongly* convex.\nFor $f$ to be strongly convex, the function $f(x) - \\frac{\\mu}{2}\\|x\\|_2^2$ must be convex for some $\\mu>0$.\nLet's choose a simple counterexample. Let $g$ be the zero function, $g(z)=0$ for all $z \\in \\mathbb{R}^m$. The zero function is convex. In this case, $f(x) = \\lambda\\|x\\|_1$. Is $h(x) = \\lambda\\|x\\|_1$ strongly convex?\nLet's check the definition. Let $n=1$, so $x \\in \\mathbb{R}$. $h(x) = \\lambda|x|$.\nLet $x=c$ and $y=-c$ for some $c>0$. Let $\\theta=1/2$. The midpoint is $z=\\theta x + (1-\\theta)y = 0$.\nThe strong convexity condition is $h(z) \\le \\theta h(x) + (1-\\theta) h(y) - \\frac{\\mu}{2}\\theta(1-\\theta)\\|x-y\\|_2^2$.\n$h(0) \\le \\frac{1}{2}h(c) + \\frac{1}{2}h(-c) - \\frac{\\mu}{2}\\frac{1}{2}\\frac{1}{2}\\|c - (-c)\\|_2^2$\n$0 \\le \\frac{1}{2}(\\lambda c) + \\frac{1}{2}(\\lambda c) - \\frac{\\mu}{8}(2c)^2$\n$0 \\le \\lambda c - \\frac{\\mu}{8}(4c^2)$\n$0 \\le \\lambda c - \\frac{\\mu}{2}c^2$\n$\\frac{\\mu}{2}c^2 \\le \\lambda c$\nSince $c>0$, we can divide by it: $c \\le \\frac{2\\lambda}{\\mu}$.\nThis inequality must hold for all $c>0$ for the function to be strongly convex. However, we can clearly choose a value of $c$ that violates this, for example $c = 1 + \\frac{2\\lambda}{\\mu}$. Thus, $\\lambda\\|x\\|_1$ is not a strongly convex function. Since we found a valid choice of convex $g$ for which $f$ is not strongly convex, the statement that this choice of $h$ *always* makes $f$ strongly convex is false.\n\nTherefore, statement (D) is **Incorrect**.\n\n### E. If $A$ does not have full column rank, then no choice of convex $h$ can make $f$ strongly convex.\n\nThis statement claims that if $A$ does not have full column rank, then for *any* convex function $h$, the resulting function $f(x) = g(Ax) + h(x)$ cannot be strongly convex.\nTo disprove this, we need to find a counterexample: a matrix $A$ without full column rank and a convex function $h$ such that $f$ is strongly convex.\nLet's refer back to statement B: \"If $g$ is convex and $h$ is $\\mu$-strongly convex for some $\\mu>0$, then $f$ is $\\mu$-strongly convex for any choice of the matrix $A$.\"\nA strongly convex function is also a convex function. So, we can choose $h$ to be a $\\mu$-strongly convex function. According to statement B, which we have proven to be correct, this choice of $h$ will make $f$ be $\\mu$-strongly convex, regardless of the properties of $A$. This holds even if $A$ does not have full column rank.\nLet's provide a concrete counterexample.\nLet $n=2$, $m=1$. Let $A = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. This matrix does not have full column rank since its rank is $1$ while it has $2$ columns. Its null space is spanned by the vector $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nLet $g: \\mathbb{R} \\to \\mathbb{R}$ be $g(z)=0$, which is convex.\nLet $h: \\mathbb{R}^2 \\to \\mathbb{R}$ be $h(x) = \\frac{\\mu}{2}\\|x\\|_2^2 = \\frac{\\mu}{2}(x_1^2 + x_2^2)$ for some $\\mu>0$. The function $h$ is convex (it is even $\\mu$-strongly convex).\nThen $f(x) = g(Ax) + h(x) = 0 + \\frac{\\mu}{2}(x_1^2 + x_2^2) = \\frac{\\mu}{2}\\|x\\|_2^2$.\nThe function $f(x)$ is $\\mu$-strongly convex by definition.\nWe have found a matrix $A$ without full column rank and a convex function $h$ such that $f$ is strongly convex. This contradicts the statement.\n\nTherefore, statement (E) is **Incorrect**.\n\nFinal summary: Statements A, B, and C are correct, while D and E are incorrect.", "answer": "$$\\boxed{ABC}$$", "id": "3160286"}, {"introduction": "Convexity is fundamentally a geometric concept, and understanding how geometric operations affect convex sets provides deep insight into optimization problems. This practice [@problem_id:3160287] focuses on projection, a key operation for simplifying problems by eliminating variables. You will work with a tangible geometric object in $\\mathbb{R}^3$ to prove that its projection onto $\\mathbb{R}^2$ preserves convexity, a property that holds in general, and discover the subtle but crucial fact that extreme points of a set do not always project to extreme points of its image.", "problem": "Consider the following convex feasibility scenario in optimization methods. Let $n=2$ and $k=1$. Define the feasible region $S \\subset \\mathbb{R}^{n+k} = \\mathbb{R}^{3}$ by\n$$\nS \\;=\\; \\left\\{(x_{1},x_{2},y) \\in \\mathbb{R}^{3} \\;:\\; y \\ge 0,\\; y \\le 1,\\; x_{1}+y \\le 1,\\; -x_{1}+y \\le 1,\\; x_{2}+y \\le 1,\\; -x_{2}+y \\le 1 \\right\\}.\n$$\nLet the projection map $P:\\mathbb{R}^{3}\\to\\mathbb{R}^{2}$ be defined by $P(x_{1},x_{2},y)=(x_{1},x_{2})$. You may use only fundamental definitions such as the definition of convexity and the linearity of $P$, and general facts like “the image of a line segment under a linear map is a line segment,” but not any specialized theorems about projections.\n\nTasks:\n1. Using only the definition of convexity, show that $P(S)$ is convex.\n2. Determine the extreme points of $S$ and of $P(S)$, and explain in what specific way the projection $P$ changes extreme points in this example.\n3. Compute the exact area of $P(S)$ in the $x_1x_2$-plane. Provide your answer as an exact number with no rounding.", "solution": "The problem as stated is valid, self-contained, and mathematically well-posed. It is grounded in the fundamental principles of convex geometry and linear algebra. We shall proceed to solve the three tasks in the specified order.\n\nThe feasible region is the set $S \\subset \\mathbb{R}^{3}$ defined by six linear inequalities:\n$$ S = \\left\\{(x_{1},x_{2},y) \\in \\mathbb{R}^{3} \\;:\\; y \\ge 0,\\; y \\le 1,\\; x_{1}+y \\le 1,\\; -x_{1}+y \\le 1,\\; x_{2}+y \\le 1,\\; -x_{2}+y \\le 1 \\right\\} $$\nThese inequalities can be compactly written as $y \\in [0, 1]$, $|x_1| \\le 1-y$, and $|x_2| \\le 1-y$. For a point to exist for a given $y$, we must have $1-y \\ge 0$, which implies $y \\le 1$. This is consistent with the explicit constraint $y \\le 1$. The set $S$ is a polyhedron, being the intersection of six closed half-spaces.\n\n**Task 1: Show that $P(S)$ is convex**\n\nThe proof consists of two parts as required by the use of fundamental definitions. First, we must show that the set $S$ itself is convex. Second, we leverage the linearity of the projection map $P$ to demonstrate the convexity of its image, $P(S)$.\n\nTo prove that $S$ is a convex set, let $s_a = (x_{1a}, x_{2a}, y_a)$ and $s_b = (x_{1b}, x_{2b}, y_b)$ be any two points in $S$. For any scalar $\\lambda \\in [0, 1]$, we form the convex combination $s_c = \\lambda s_a + (1-\\lambda)s_b$. The coordinates of $s_c$ are:\n$$ s_c = (\\lambda x_{1a} + (1-\\lambda)x_{1b}, \\; \\lambda x_{2a} + (1-\\lambda)x_{2b}, \\; \\lambda y_a + (1-\\lambda)y_b) $$\nLet the coordinates of $s_c$ be denoted by $(x_{1c}, x_{2c}, y_c)$. We must verify that $s_c$ satisfies all six inequalities that define $S$.\nSince $s_a, s_b \\in S$, we know they satisfy the inequalities. For the coordinate $y_c$:\n$y_a \\ge 0$ and $y_b \\ge 0 \\implies y_c = \\lambda y_a + (1-\\lambda)y_b \\ge \\lambda(0) + (1-\\lambda)(0) = 0$.\n$y_a \\le 1$ and $y_b \\le 1 \\implies y_c = \\lambda y_a + (1-\\lambda)y_b \\le \\lambda(1) + (1-\\lambda)(1) = 1$.\nFor the other coordinates:\n$x_{1a} + y_a \\le 1$ and $x_{1b} + y_b \\le 1 \\implies x_{1c} + y_c = \\lambda(x_{1a}+y_a) + (1-\\lambda)(x_{1b}+y_b) \\le \\lambda(1) + (1-\\lambda)(1) = 1$.\n$-x_{1a} + y_a \\le 1$ and $-x_{1b} + y_b \\le 1 \\implies -x_{1c} + y_c = \\lambda(-x_{1a}+y_a) + (1-\\lambda)(-x_{1b}+y_b) \\le \\lambda(1) + (1-\\lambda)(1) = 1$.\n$x_{2a} + y_a \\le 1$ and $x_{2b} + y_b \\le 1 \\implies x_{2c} + y_c = \\lambda(x_{2a}+y_a) + (1-\\lambda)(x_{2b}+y_b) \\le \\lambda(1) + (1-\\lambda)(1) = 1$.\n$-x_{2a} + y_a \\le 1$ and $-x_{2b} + y_b \\le 1 \\implies -x_{2c} + y_c = \\lambda(-x_{2a}+y_a) + (1-\\lambda)(-x_{2b}+y_b) \\le \\lambda(1) + (1-\\lambda)(1) = 1$.\nSince $s_c$ satisfies all defining inequalities, $s_c \\in S$. Thus, $S$ is a convex set.\n\nNow, we prove that $P(S)$ is convex. Let $u$ and $v$ be two arbitrary points in $P(S)$. By definition of the set $P(S)$, there must exist points $s_u, s_v \\in S$ such that $P(s_u) = u$ and $P(s_v) = v$. To show $P(S)$ is convex, we must show that for any $\\lambda \\in [0, 1]$, the point $w = \\lambda u + (1-\\lambda)v$ is also in $P(S)$.\nConsider the point $s_w = \\lambda s_u + (1-\\lambda)s_v$. As established above, $S$ is a convex set, so $s_w \\in S$.\nThe projection map $P(x_1, x_2, y) = (x_1, x_2)$ is a linear transformation. Applying $P$ to $s_w$ and using linearity:\n$$ P(s_w) = P(\\lambda s_u + (1-\\lambda)s_v) = \\lambda P(s_u) + (1-\\lambda)P(s_v) $$\nBy substituting $P(s_u) = u$ and $P(s_v) = v$, we obtain:\n$$ P(s_w) = \\lambda u + (1-\\lambda)v = w $$\nWe have found a point $s_w \\in S$ such that $P(s_w) = w$. This signifies that $w \\in P(S)$. Since this holds for any pair of points $u, v \\in P(S)$ and any $\\lambda \\in [0, 1]$, we conclude that $P(S)$ is a convex set.\n\n**Task 2: Determine the extreme points of $S$ and of $P(S)$**\n\nAn extreme point of a polyhedron like $S$ in $\\mathbb{R}^3$ is a vertex, which is a feasible point lying at the intersection of at least $3$ of the boundary-defining planes. The equations of these planes are:\n1. $y=0$\n2. $y=1$\n3. $x_1+y=1$\n4. $-x_1+y=1$\n5. $x_2+y=1$\n6. $-x_2+y=1$\n\nWe find candidate points by solving systems of $3$ of these equations and checking feasibility with respect to the remaining inequalities.\nCase 1: Intersections involving the plane $y=0$. The inequalities reduce to $|x_1| \\le 1$ and $|x_2| \\le 1$. The vertices of this feasible square region at $y=0$ are extreme points of $S$.\n- Intersection of $y=0$, $x_1+y=1$ (i.e., $x_1=1$), and $x_2+y=1$ (i.e., $x_2=1$): This gives the point $(1, 1, 0)$, which satisfies all six original inequalities.\n- Intersection of $y=0$, $x_1+y=1$ ($x_1=1$), and $-x_2+y=1$ ($x_2=-1$): This gives $(1, -1, 0)$, which is feasible.\n- Intersection of $y=0$, $-x_1+y=1$ ($x_1=-1$), and $x_2+y=1$ ($x_2=1$): This gives $(-1, 1, 0)$, which is feasible.\n- Intersection of $y=0$, $-x_1+y=1$ ($x_1=-1$), and $-x_2+y=1$ ($x_2=-1$): This gives $(-1, -1, 0)$, which is feasible.\nThese four points form a square base in the $y=0$ plane.\n\nCase 2: Intersections involving the plane $y=1$.\nSubstituting $y=1$ into the other active inequalities gives $x_1+1 \\le 1 \\implies x_1 \\le 0$ and $-x_1+1 \\le 1 \\implies x_1 \\ge 0$, which forces $x_1=0$. Similarly, $x_2+1 \\le 1 \\implies x_2 \\le 0$ and $-x_2+1 \\le 1 \\implies x_2 \\ge 0$, forcing $x_2=0$. This yields a unique point $(0,0,1)$, which is feasible. This point is the simultaneous solution to $y=1$ and boundary equations $3,4,5,6$.\n\nThe set of extreme points of $S$, denoted $E_S$, is therefore:\n$$ E_S = \\left\\{ (1,1,0), (1,-1,0), (-1,1,0), (-1,-1,0), (0,0,1) \\right\\} $$\nGeometrically, $S$ is a right pyramid with its apex at $(0,0,1)$ and its base being a square on the $y=0$ plane with vertices $(\\pm 1, \\pm 1, 0)$.\n\nNext, we identify the set $P(S)$ and its extreme points. A point $(x_1, x_2)$ belongs to $P(S)$ if there exists a $y$ such that $(x_1, x_2, y) \\in S$. The constraints on $y$ are $y \\in [0,1]$, $y \\le 1-|x_1|$, and $y \\le 1-|x_2|$. For such a $y$ to exist, the lower bound on $y$ must be less than or equal to the upper bound.\n$$ 0 \\le \\min(1, 1-|x_1|, 1-|x_2|) $$\nThis requires $0 \\le 1$ (which is true), $0 \\le 1-|x_1| \\implies |x_1| \\le 1$, and $0 \\le 1-|x_2| \\implies |x_2| \\le 1$.\nThe projected set $P(S)$ is therefore the square in the $x_1x_2$-plane:\n$$ P(S) = \\left\\{ (x_1, x_2) \\in \\mathbb{R}^2 \\;:\\; -1 \\le x_1 \\le 1, \\; -1 \\le x_2 \\le 1 \\right\\} $$\nThis set is a convex polygon whose extreme points are its four vertices. Let $E_{P(S)}$ be the set of extreme points of $P(S)$:\n$$ E_{P(S)} = \\left\\{ (1,1), (1,-1), (-1,1), (-1,-1) \\right\\} $$\nTo explain how the projection $P$ changes the extreme points, we project each point in $E_S$:\n$P(1,1,0) = (1,1)$\n$P(1,-1,0) = (1,-1)$\n$P(-1,1,0) = (-1,1)$\n$P(-1,-1,0) = (-1,-1)$\n$P(0,0,1) = (0,0)$\nThe specific effects are:\n1. The four extreme points of $S$ that form its base on the plane $y=0$ are mapped one-to-one onto the four extreme points of $P(S)$.\n2. The fifth extreme point of $S$, the apex $(0,0,1)$, is mapped to the point $(0,0)$. This point is the geometric center of the square $P(S)$ and is an interior point of $P(S)$, not an extreme point.\nThis example demonstrates that the projection of an extreme point of a convex set is not necessarily an extreme point of the projected set.\n\n**Task 3: Compute the exact area of $P(S)$**\n\nAs found in Task 2, the set $P(S)$ is a square in the $x_1x_2$-plane defined by $-1 \\le x_1 \\le 1$ and $-1 \\le x_2 \\le 1$.\nThe side length of this square along the $x_1$-axis is the distance from $x_1=-1$ to $x_1=1$, which is $1 - (-1) = 2$.\nSimilarly, the side length along the $x_2$-axis is $1 - (-1) = 2$.\nThe area of a square is the product of its side lengths.\n$$ \\text{Area}(P(S)) = 2 \\times 2 = 4 $$\nThe exact area is $4$.", "answer": "$$\n\\boxed{4}\n$$", "id": "3160287"}, {"introduction": "While knowing that an operation preserves convexity is important, understanding the *robustness* of this property offers a deeper level of mastery. This exercise [@problem_id:3160294] challenges you to determine the precise amount of a non-convex term, specifically $-\\epsilon \\|x\\|^{2}$, that can be subtracted from a convex function before it loses its convexity. This threshold is directly related to the function's strong convexity modulus, a measure of its curvature, and by linking it to the eigenvalues of the Hessian matrix, you will gain a hands-on, quantitative understanding of the boundary between convex and non-convex functions.", "problem": "Consider the function $f : \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $f(x) = \\frac{1}{2} x^{\\top} A x + b^{\\top} x + c$, where $x \\in \\mathbb{R}^{2}$, $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$, and $c = 0$. Define the modified function $g_{\\epsilon}(x) = f(x) - \\epsilon \\|x\\|^{2}$, where $\\|x\\|^{2} = x^{\\top} x$ and $\\epsilon \\ge 0$ is a scalar.\n\nStart from the fundamental definition of convexity: a function $h$ is convex if for all $x, y \\in \\mathbb{R}^{2}$ and $t \\in [0,1]$, $h(tx + (1-t)y) \\le t h(x) + (1-t) h(y)$. Use the well-tested characterization that a twice continuously differentiable function is convex if and only if its Hessian is positive semidefinite (PSD) everywhere, where positive semidefinite (PSD) means all eigenvalues are nonnegative.\n\nDetermine the largest value $\\epsilon^{\\star}$ such that $g_{\\epsilon}$ is convex on $\\mathbb{R}^{2}$ if and only if $\\epsilon \\le \\epsilon^{\\star}$. Provide your final answer as a closed-form analytic expression. Do not round.", "solution": "The user wants to find the largest value of a non-negative scalar, denoted as $\\epsilon^{\\star}$, such that the function $g_{\\epsilon}(x)$ remains convex for all $\\epsilon$ in the range $0 \\le \\epsilon \\le \\epsilon^{\\star}$.\n\nThe problem begins with the definition of a quadratic function $f: \\mathbb{R}^{2} \\to \\mathbb{R}$:\n$$f(x) = \\frac{1}{2} x^{\\top} A x + b^{\\top} x + c$$\nwhere $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$, and $c=0$.\n\nA modified function $g_{\\epsilon}(x)$ is defined as:\n$$g_{\\epsilon}(x) = f(x) - \\epsilon \\|x\\|^{2}$$\nwhere $\\|x\\|^{2} = x^{\\top} x = x^{\\top} I x$, with $I$ being the $2 \\times 2$ identity matrix. The scalar $\\epsilon$ is given to be non-negative, i.e., $\\epsilon \\ge 0$.\n\nWe can substitute the expressions for $f(x)$ and $\\|x\\|^{2}$ into the definition of $g_{\\epsilon}(x)$ to obtain a consolidated quadratic form.\n$$g_{\\epsilon}(x) = \\left(\\frac{1}{2} x^{\\top} A x + b^{\\top} x + c\\right) - \\epsilon (x^{\\top} I x)$$\n$$g_{\\epsilon}(x) = \\frac{1}{2} x^{\\top} A x - \\epsilon x^{\\top} I x + b^{\\top} x + c$$\nBy factoring out the quadratic term, we get:\n$$g_{\\epsilon}(x) = \\frac{1}{2} x^{\\top} (A - 2\\epsilon I) x + b^{\\top} x + c$$\nThis expresses $g_{\\epsilon}(x)$ as a standard quadratic function with the associated matrix $A - 2\\epsilon I$.\n\nThe problem states that a twice continuously differentiable function is convex if and only if its Hessian matrix is positive semidefinite (PSD) everywhere. The function $g_{\\epsilon}(x)$ is a quadratic polynomial, so it is infinitely differentiable.\n\nThe Hessian of a general quadratic function $h(x) = \\frac{1}{2} x^{\\top} Q x + d^{\\top} x + k$, where $Q$ is a symmetric matrix, is given by $\\nabla^2 h(x) = Q$.\nIn our case, the matrix for $g_{\\epsilon}(x)$ is $Q_{\\epsilon} = A - 2\\epsilon I$. The matrix $A$ is symmetric, and the identity matrix $I$ is symmetric. Thus, $Q_{\\epsilon}$ is also symmetric.\nThe Hessian of $g_{\\epsilon}(x)$ is therefore constant and equal to this matrix:\n$$\\nabla^2 g_{\\epsilon}(x) = A - 2\\epsilon I$$\n\nFor $g_{\\epsilon}(x)$ to be convex, its Hessian must be positive semidefinite.\n$$\\nabla^2 g_{\\epsilon}(x) \\succeq 0 \\quad \\iff \\quad A - 2\\epsilon I \\succeq 0$$\nThe notation $\\succeq 0$ indicates that the matrix is positive semidefinite. A symmetric matrix is positive semidefinite if and only if all its eigenvalues are non-negative.\n\nLet $\\lambda$ be an eigenvalue of $A - 2\\epsilon I$ and $v$ be the corresponding eigenvector. Then:\n$$(A - 2\\epsilon I)v = \\lambda v$$\n$$Av - 2\\epsilon Iv = \\lambda v$$\n$$Av = (\\lambda + 2\\epsilon)v$$\nThis shows that $v$ is also an eigenvector of $A$, with eigenvalue $\\lambda_A = \\lambda + 2\\epsilon$.\nConversely, if $\\lambda_A$ is an eigenvalue of $A$, then the corresponding eigenvalue of $A - 2\\epsilon I$ is $\\lambda = \\lambda_A - 2\\epsilon$.\n\nThe condition that all eigenvalues of $A - 2\\epsilon I$ are non-negative translates to:\n$$\\lambda_A - 2\\epsilon \\ge 0 \\quad \\text{for all eigenvalues } \\lambda_A \\text{ of } A$$\nThis inequality can be rearranged to find a constraint on $\\epsilon$:\n$$2\\epsilon \\le \\lambda_A$$\n$$\\epsilon \\le \\frac{\\lambda_A}{2}$$\nThis must hold for all eigenvalues of $A$. Therefore, $\\epsilon$ must be less than or equal to half the minimum eigenvalue of $A$.\n$$\\epsilon \\le \\frac{1}{2} \\min(\\lambda_A)$$\nwhere $\\min(\\lambda_A)$ denotes the smallest eigenvalue of the matrix $A$.\n\nThe problem asks for the largest value $\\epsilon^{\\star}$ such that $g_{\\epsilon}$ is convex if and only if $\\epsilon \\le \\epsilon^{\\star}$. This value is precisely $\\frac{1}{2} \\min(\\lambda_A)$.\n$$\\epsilon^{\\star} = \\frac{1}{2} \\min(\\lambda_A)$$\n\nNext, we must compute the eigenvalues of the matrix $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$. The eigenvalues are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\\det \\begin{pmatrix} 4-\\lambda & 2 \\\\ 2 & 3-\\lambda \\end{pmatrix} = 0$$\n$$(4-\\lambda)(3-\\lambda) - (2)(2) = 0$$\n$$12 - 4\\lambda - 3\\lambda + \\lambda^2 - 4 = 0$$\n$$\\lambda^2 - 7\\lambda + 8 = 0$$\n\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\\lambda = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4(1)(8)}}{2(1)}$$\n$$\\lambda = \\frac{7 \\pm \\sqrt{49 - 32}}{2}$$\n$$\\lambda = \\frac{7 \\pm \\sqrt{17}}{2}$$\n\nThe two eigenvalues of $A$ are $\\lambda_1 = \\frac{7 + \\sqrt{17}}{2}$ and $\\lambda_2 = \\frac{7 - \\sqrt{17}}{2}$.\nThe smallest eigenvalue is:\n$$\\min(\\lambda_A) = \\frac{7 - \\sqrt{17}}{2}$$\nNote that since $\\sqrt{16}=4$ and $\\sqrt{25}=5$, $\\sqrt{17}$ is between $4$ and $5$. Thus, $7 - \\sqrt{17}$ is positive, and so is $\\min(\\lambda_A)$. This confirms that the original function $f(x)$ is strictly convex.\n\nFinally, we determine the value of $\\epsilon^{\\star}$:\n$$\\epsilon^{\\star} = \\frac{1}{2} \\min(\\lambda_A) = \\frac{1}{2} \\left( \\frac{7 - \\sqrt{17}}{2} \\right)$$\n$$\\epsilon^{\\star} = \\frac{7 - \\sqrt{17}}{4}$$\nThis is the largest value of $\\epsilon$ for which $g_{\\epsilon}(x)$ remains convex. The initial condition $\\epsilon \\ge 0$ is satisfied since $7 - \\sqrt{17} > 7 - 5 = 2 > 0$.", "answer": "$$\\boxed{\\frac{7 - \\sqrt{17}}{4}}$$", "id": "3160294"}]}