{"hands_on_practices": [{"introduction": "Understanding affine hulls begins with the fundamentals. This first practice grounds you in the core definition, guiding you through the essential process of verifying if points belong to a given affine set and then determining the dimension of the smallest affine space that contains them. You will use difference vectors to construct the underlying direction subspace, a key technique for analyzing the geometry of data and mastering the algebraic representation of affine sets [@problem_id:3096335].", "problem": "Consider the linear feasibility set defined by the system of linear equations $A x = b$, where\n$$\nA = \\begin{pmatrix}\n1 & -1 & 0 & 2 \\\\\n0 & 1 & 1 & -1\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n3 \\\\\n0\n\\end{pmatrix},\n$$\nand the dataset consisting of four points in $\\mathbb{R}^{4}$,\n$$\nx^{(1)} = \\begin{pmatrix} 4 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix},\\quad\nx^{(2)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix},\\quad\nx^{(3)} = \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\\\ 3 \\end{pmatrix},\\quad\nx^{(4)} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 3 \\\\ 2 \\end{pmatrix}.\n$$\nStarting from the core definitions of an affine set and affine hull, and using standard linear algebra facts, do the following:\n\n1. Determine whether each point $x^{(i)}$ lies in the affine set $\\{x \\in \\mathbb{R}^{4} : A x = b\\}$ by directly evaluating $A x^{(i)}$ and comparing it to $b$.\n2. Derive, from first principles, the smallest affine set that contains $\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\}$, and determine its dimension by appropriately analyzing difference vectors among the points.\n\nProvide a clear, principled derivation based on the definitions and linear algebra properties, not relying on pre-stated target formulas. Finally, report only the dimension of the affine hull of $\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\}$ as an exact integer. No rounding is required.", "solution": "The problem is analyzed in two parts as requested. First, we verify if the given points lie within the specified affine set. Second, we determine the affine hull of these points and its dimension from first principles.\n\nAn affine set is a subset of a vector space that is closed under affine combinations. A set $S$ is affine if for any $x, y \\in S$, the point $\\theta x + (1-\\theta)y$ is also in $S$ for any scalar $\\theta \\in \\mathbb{R}$. The set of solutions to a system of linear equations, $\\{x \\in \\mathbb{R}^n : Ax=b\\}$, constitutes an affine set.\n\nPart 1: Verification of points in the affine set $\\{x \\in \\mathbb{R}^{4} : A x = b\\}$.\n\nWe are given the matrix $A = \\begin{pmatrix} 1 & -1 & 0 & 2 \\\\ 0 & 1 & 1 & -1 \\end{pmatrix}$ and the vector $b = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$. We must verify if $A x^{(i)} = b$ for each point $i \\in \\{1, 2, 3, 4\\}$.\n\nFor $x^{(1)} = \\begin{pmatrix} 4 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$:\n$$A x^{(1)} = \\begin{pmatrix} 1 & -1 & 0 & 2 \\\\ 0 & 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(4) + (-1)(1) + (0)(-1) + (2)(0) \\\\ (0)(4) + (1)(1) + (1)(-1) + (-1)(0) \\end{pmatrix} = \\begin{pmatrix} 4 - 1 \\\\ 1 - 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}.$$\nSince $A x^{(1)} = b$, the point $x^{(1)}$ lies in the affine set.\n\nFor $x^{(2)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$A x^{(2)} = \\begin{pmatrix} 1 & -1 & 0 & 2 \\\\ 0 & 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (-1)(0) + (0)(1) + (2)(1) \\\\ (0)(1) + (1)(0) + (1)(1) + (-1)(1) \\end{pmatrix} = \\begin{pmatrix} 1 + 2 \\\\ 1 - 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}.$$\nSince $A x^{(2)} = b$, the point $x^{(2)}$ lies in the affine set.\n\nFor $x^{(3)} = \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\\\ 3 \\end{pmatrix}$:\n$$A x^{(3)} = \\begin{pmatrix} 1 & -1 & 0 & 2 \\\\ 0 & 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (-1)(2) + (0)(1) + (2)(3) \\\\ (0)(-1) + (1)(2) + (1)(1) + (-1)(3) \\end{pmatrix} = \\begin{pmatrix} -1 - 2 + 6 \\\\ 2 + 1 - 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}.$$\nSince $A x^{(3)} = b$, the point $x^{(3)}$ lies in the affine set.\n\nFor $x^{(4)} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 3 \\\\ 2 \\end{pmatrix}$:\n$$A x^{(4)} = \\begin{pmatrix} 1 & -1 & 0 & 2 \\\\ 0 & 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\\\ 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} (1)(-2) + (-1)(-1) + (0)(3) + (2)(2) \\\\ (0)(-2) + (1)(-1) + (1)(3) + (-1)(2) \\end{pmatrix} = \\begin{pmatrix} -2 + 1 + 4 \\\\ -1 + 3 - 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}.$$\nSince $A x^{(4)} = b$, the point $x^{(4)}$ lies in the affine set.\nAll four points $x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}$ are members of the given affine set.\n\nPart 2: Derivation of the affine hull and its dimension.\n\nThe affine hull of a set of points $C = \\{x^{(1)}, \\dots, x^{(k)}\\}$, denoted $\\text{aff}(C)$, is the set of all affine combinations of these points:\n$$ \\text{aff}(C) = \\left\\{ \\sum_{i=1}^{k} \\theta_i x^{(i)} \\;\\middle|\\; \\theta_i \\in \\mathbb{R}, \\sum_{i=1}^{k} \\theta_i = 1 \\right\\}. $$\nThis is the smallest affine set containing all points in $C$.\nAn affine set can be described as a translation of a vector subspace. That is, for any affine set $S$ containing a point $x_0$, the set $V = \\{x - x_0 \\mid x \\in S\\}$ is a vector subspace. The dimension of the affine set $S$ is defined as the dimension of this associated subspace $V$.\n\nTo find the dimension of the affine hull of $\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\}$, we first construct the associated vector subspace. We can choose any of the points as the origin of the translation, for instance $x^{(1)}$. The subspace $V$ is spanned by the difference vectors between $x^{(1)}$ and the other points:\n$$V = \\text{span}\\{x^{(2)} - x^{(1)}, x^{(3)} - x^{(1)}, x^{(4)} - x^{(1)}\\}.$$\nThe dimension of the affine hull is $\\dim(V)$. Let's compute these difference vectors:\n$$v^{(1)} = x^{(2)} - x^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -1 \\\\ 2 \\\\ 1 \\end{pmatrix}.$$\n$$v^{(2)} = x^{(3)} - x^{(1)} = \\begin{pmatrix} -1 \\\\ 2 \\\\ 1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -5 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix}.$$\n$$v^{(3)} = x^{(4)} - x^{(1)} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 3 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -2 \\\\ 4 \\\\ 2 \\end{pmatrix}.$$\nThe dimension of the affine hull is the dimension of the subspace spanned by $\\{v^{(1)}, v^{(2)}, v^{(3)}\\}$, which is equal to the number of linearly independent vectors in this set. We observe the relationship between $v^{(1)}$ and $v^{(3)}$:\n$$2 v^{(1)} = 2 \\begin{pmatrix} -3 \\\\ -1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -2 \\\\ 4 \\\\ 2 \\end{pmatrix} = v^{(3)}.$$\nSince $v^{(3)}$ is a scalar multiple of $v^{(1)}$, it is linearly dependent on $v^{(1)}$. Thus, the set of spanning vectors can be reduced to $\\{v^{(1)}, v^{(2)}\\}$:\n$$V = \\text{span}\\{v^{(1)}, v^{(2)}, v^{(3)}\\} = \\text{span}\\{v^{(1)}, v^{(2)}\\}.$$\nThe dimension of $V$ is at most $2$. To determine if it is exactly $2$, we must check if $v^{(1)}$ and $v^{(2)}$ are linearly independent. We seek scalars $c_1, c_2$ such that $c_1 v^{(1)} + c_2 v^{(2)} = 0$:\n$$c_1 \\begin{pmatrix} -3 \\\\ -1 \\\\ 2 \\\\ 1 \\end{pmatrix} + c_2 \\begin{pmatrix} -5 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.$$\nThis vector equation corresponds to a system of linear equations:\n\\begin{enumerate}\n    \\item $-3c_1 - 5c_2 = 0$\n    \\item $-c_1 + c_2 = 0$\n    \\item $2c_1 + 2c_2 = 0$\n    \\item $c_1 + 3c_2 = 0$\n\\end{enumerate}\nFrom equation (2), we have $c_2 = c_1$. Substituting this into equation (3) gives $2c_1 + 2c_1 = 4c_1 = 0$, which implies $c_1 = 0$. Since $c_2 = c_1$, we must also have $c_2 = 0$. This solution $(c_1, c_2)=(0,0)$ satisfies all four equations. Since the only solution is the trivial one, the vectors $v^{(1)}$ and $v^{(2)}$ are linearly independent.\n\nThe set $\\{v^{(1)}, v^{(2)}\\}$ forms a basis for the subspace $V$. The dimension of a vector space is the number of vectors in its basis. Therefore, $\\dim(V) = 2$.\nThe smallest affine set containing $\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\}$ is the affine hull, which can be expressed as:\n$$ \\text{aff}\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\} = x^{(1)} + V = \\left\\{ x^{(1)} + c_1 v^{(1)} + c_2 v^{(2)} \\mid c_1, c_2 \\in \\mathbb{R} \\right\\}. $$\nThe dimension of this affine hull is $\\dim(V)$.\nBased on our analysis, the dimension is $2$.\nThis is consistent with the fact that the affine hull of the points is a subset of the affine set $\\{x: Ax=b\\}$. The dimension of $\\{x: Ax=b\\}$ is the dimension of the null space of $A$, which is $4 - \\text{rank}(A) = 4 - 2 = 2$. Since the affine hull has dimension $2$ and is contained within an affine set of dimension $2$, the two sets must be identical.\n\nThe dimension of the affine hull of $\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\}$ is $2$.", "answer": "$$\\boxed{2}$$", "id": "3096335"}, {"introduction": "While manual calculations are insightful, real-world data requires robust computational methods. This exercise introduces a powerful technique using Principal Component Analysis (PCA) to determine the affine hull of a point cloud, bridging the gap between abstract geometry and practical data science. By implementing this method, you will learn how to algorithmically find the dimension and basis of the underlying subspace and compute the distance of any point to this hull, a fundamental task in dimensionality reduction and data modeling [@problem_id:3096329].", "problem": "You are given finite sets of sample points in a real Euclidean space and a single query point per set. Your task is to compute the smallest affine subspace that contains the sample points using Principal Component Analysis (PCA) and to assess how far the query point lies from that affine subspace. Use the following definitions as the fundamental base: an affine set in $\\mathbb{R}^n$ is any translation of a linear subspace; the affine hull of a set $S \\subset \\mathbb{R}^n$, denoted $\\mathrm{aff}(S)$, is the smallest affine set containing $S$. Principal Component Analysis (PCA) refers to identifying orthonormal directions that capture the maximal variance of centered data, obtainable from the singular value decomposition of the centered data matrix. The Euclidean norm is the norm induced by the standard inner product on $\\mathbb{R}^n$.\n\nImplement a program that, for each test case, performs the following:\n- Let the sample points be $\\{x_i\\}_{i=1}^m \\subset \\mathbb{R}^n$ and the query point be $y \\in \\mathbb{R}^n$. Compute the empirical center $x_c$ of the sample points and then the centered data matrix whose rows are $x_i - x_c$.\n- Apply Principal Component Analysis (PCA) via singular value decomposition to the centered data matrix to identify an orthonormal basis of the direction subspace for the affine hull of the sample points. Use the number of singular values strictly greater than a numerical threshold to determine the affine dimension. The threshold must be defined relative to the largest singular value to ensure scale invariance.\n- Let $V_r$ be an orthonormal basis of the direction subspace determined by PCA. Compute the Euclidean distance from $y$ to the affine hull, defined as the Euclidean norm of the residual when $y - x_c$ is orthogonally projected onto the span of the principal directions.\n\nYour program must output one result per test case. For each test case, output a two-element list $[d,\\ \\delta]$, where $d$ is the integer affine dimension of $\\mathrm{aff}(\\{x_i\\})$ and $\\delta$ is the Euclidean distance from $y$ to $\\mathrm{aff}(\\{x_i\\})$ expressed as a float rounded to six decimal places. Aggregate the results across all test cases into a single line of output containing a comma-separated list enclosed in square brackets, such as $\\left[\\left[d_1,\\ \\delta_1\\right],\\left[d_2,\\ \\delta_2\\right],\\dots\\right]$.\n\nUse the following test suite. Each test specifies the ambient dimension $n$, the list of sample points $\\{x_i\\}$, and the query point $y$.\n\n- Test Case $1$: Ambient dimension $n = 2$. Sample points $x_1 = \\left(0, 1\\right)$, $x_2 = \\left(1, 3\\right)$, $x_3 = \\left(2, 5\\right)$, $x_4 = \\left(3, 7\\right)$. Query point $y = \\left(4, 9\\right)$.\n- Test Case $2$: Ambient dimension $n = 3$. Sample points $x_1 = \\left(0, 0, 0\\right)$, $x_2 = \\left(1, 0, 1\\right)$, $x_3 = \\left(0, 1, 2\\right)$, $x_4 = \\left(2, -1, 0\\right)$. Query point $y = \\left(1, 1, 3\\right)$.\n- Test Case $3$: Ambient dimension $n = 4$. Sample points $x_1 = \\left(1, 1, 1, 1\\right)$, $x_2 = \\left(1, 1, 1, 1\\right)$, $x_3 = \\left(1, 1, 1, 1\\right)$, $x_4 = \\left(1, 1, 1, 1\\right)$. Query point $y = \\left(0, 0, 0, 0\\right)$.\n- Test Case $4$: Ambient dimension $n = 3$. Sample points $x_1 = \\left(0, 0, 0\\right)$, $x_2 = \\left(1, 0, 0\\right)$, $x_3 = \\left(0, 1, 0\\right)$. Query point $y = \\left(0, 0, 1\\right)$.\n- Test Case $5$: Ambient dimension $n = 3$. Sample points $x_1 = \\left(1, 2, 3\\right)$, $x_2 = \\left(2, 4, 6\\right)$, $x_3 = \\left(3, 6, 9\\right)$. Query point $y = \\left(1, 0, 0\\right)$.\n\nNumerical details to enforce:\n- Define the numerical threshold for singular values as $10^{-10}$ times the largest singular value (if the largest singular value is zero, the affine dimension is $0$).\n- Use the Euclidean distance (no physical units are involved). Report $\\delta$ rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each inner result is a two-element list $\\left[d,\\ \\delta\\right]$ with $\\delta$ rounded to six decimal places. For example, $\\left[\\left[d_1,\\ \\delta_1\\right],\\left[d_2,\\ \\delta_2\\right],\\left[d_3,\\ \\delta_3\\right]\\right]$.", "solution": "We start from the fundamental definitions in Euclidean space $\\mathbb{R}^n$. An affine set is any translate of a linear subspace. For a finite set $S = \\{x_i\\}_{i=1}^m \\subset \\mathbb{R}^n$, the affine hull $\\mathrm{aff}(S)$ is the smallest affine set containing $S$. A classical characterization is that $\\mathrm{aff}(S)$ equals $x_c + \\mathrm{span}\\{x_i - x_c : i = 1,\\dots,m\\}$ for any fixed $x_c \\in \\mathbb{R}^n$ (for instance, the empirical mean). This follows because any point in the affine hull can be written as an affine combination of the points, and differences relative to a fixed center generate the direction subspace.\n\nTo identify the direction subspace algorithmically, we use Principal Component Analysis (PCA). Principal Component Analysis (PCA) provides orthonormal directions capturing variance in centered data. Formally, form the centered data matrix $Z \\in \\mathbb{R}^{m \\times n}$ whose $i$-th row is $(x_i - x_c)^\\top$. Compute the singular value decomposition $Z = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$, $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has non-negative diagonal entries (the singular values), and $V \\in \\mathbb{R}^{n \\times n}$ is orthogonal. The right singular vectors (columns of $V$) associated with strictly positive singular values span the column space of $Z^\\top$, which is precisely $\\mathrm{span}\\{x_i - x_c\\}$. Therefore, the direction subspace of $\\mathrm{aff}(S)$ is spanned by the subset of columns of $V$ corresponding to singular values that are numerically deemed non-zero. If we let $r$ denote the number of singular values strictly greater than a threshold $\\tau$, then the affine dimension is $r$, and an orthonormal basis for the direction subspace is given by $V_r \\in \\mathbb{R}^{n \\times r}$, the matrix formed by the first $r$ columns of $V$.\n\nTo compute the distance from a query point $y \\in \\mathbb{R}^n$ to the affine hull, we note that $\\mathrm{aff}(S) = x_c + \\mathrm{span}(V_r)$, where $V_r$ has orthonormal columns. Consider the vector $v = y - x_c \\in \\mathbb{R}^n$. The orthogonal projection of $v$ onto $\\mathrm{span}(V_r)$ is $V_r (V_r^\\top v)$, because $V_r^\\top V_r = I_r$ when columns are orthonormal. Therefore, the projection of $y$ onto the affine hull is $x_c + V_r (V_r^\\top (y - x_c))$. The residual vector orthogonal to the affine hull is\n$$\nr_{\\perp} = (y - x_c) - V_r (V_r^\\top (y - x_c)),\n$$\nand the Euclidean distance from $y$ to $\\mathrm{aff}(S)$ is $\\|r_{\\perp}\\|_2$. In the special case when $r = 0$ (all singular values are zero), the direction subspace is trivial, and $\\mathrm{aff}(S)$ reduces to the single point $\\{x_c\\}$; the distance becomes $\\|y - x_c\\|_2$.\n\nNumerical rank determination requires a threshold to decide which singular values are effectively non-zero. To respect scale invariance, we set the threshold as\n$$\n\\tau = 10^{-10} \\cdot \\sigma_{\\max},\n$$\nwhere $\\sigma_{\\max}$ is the largest singular value. If $\\sigma_{\\max} = 0$, then all singular values are zero by definition, and the affine dimension is $0$.\n\nAlgorithmic steps for each test case:\n- Construct the sample matrix $X \\in \\mathbb{R}^{m \\times n}$, compute $x_c = \\frac{1}{m} \\sum_{i=1}^m x_i$, and form the centered matrix $Z$ with rows $x_i - x_c$.\n- Compute $U, \\Sigma, V^\\top = \\mathrm{svd}(Z)$ using singular value decomposition; extract singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots$.\n- Define $\\tau = 10^{-10} \\cdot \\sigma_{\\max}$ if $\\sigma_{\\max} > 0$ and otherwise set the affine dimension to $0$.\n- Let $r$ be the count of $\\sigma_j$ such that $\\sigma_j > \\tau$. Form $V_r$ by taking the first $r$ right singular vectors.\n- Compute $v = y - x_c$, residual $r_{\\perp} = v - V_r(V_r^\\top v)$ if $r > 0$, else $r_{\\perp} = v$.\n- The distance is $\\delta = \\|r_{\\perp}\\|_2$.\n- Output $[r,\\ \\delta]$ with $\\delta$ rounded to six decimal places.\n\nNow consider the provided test suite:\n- Test Case $1$ is in $\\mathbb{R}^2$ with points lying on a single affine line and a query point on the same line; the affine dimension should be $1$ and the distance should be $0$.\n- Test Case $2$ is in $\\mathbb{R}^3$ with points lying on a plane described by $z = x + 2y$ and a query point on that plane; the affine dimension should be $2$ and the distance should be $0$.\n- Test Case $3$ repeats the same point in $\\mathbb{R}^4$, making the affine hull a single point; the affine dimension should be $0$, and the distance equals the Euclidean norm from the query point to $x_c$, which is $\\sqrt{4} = 2$.\n- Test Case $4$ places points on the plane $z = 0$ in $\\mathbb{R}^3$ with a query point having $z = 1$; the affine dimension should be $2$ and the distance should be $1$.\n- Test Case $5$ places points on a line in $\\mathbb{R}^3$ with direction $\\left(1, 2, 3\\right)$ and a query point not on that line; the affine dimension should be $1$, and the computed distance will be a positive value determined by the orthogonal projection formula above.\n\nThe program implements these steps and prints a single line containing the list $\\left[\\left[d_1,\\ \\delta_1\\right],\\left[d_2,\\ \\delta_2\\right],\\left[d_3,\\ \\delta_3\\right],\\left[d_4,\\ \\delta_4\\right],\\left[d_5,\\ \\delta_5\\right]\\right]$ with each $\\delta_i$ rounded to six decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef affine_hull_pca_dimension_and_distance(points: np.ndarray, query: np.ndarray) -> tuple[int, float]:\n    \"\"\"\n    Compute the affine hull dimension via PCA and the Euclidean distance\n    from the query point to the affine hull.\n    \"\"\"\n    # Compute empirical center\n    xc = np.mean(points, axis=0)\n\n    # Center the data\n    Z = points - xc  # shape (m, n)\n\n    # SVD of the centered data matrix\n    # full_matrices=False ensures compact SVD with shapes:\n    # U: (m, min(m, n)), s: (min(m, n),), Vt: (min(m, n), n)\n    U, s, Vt = np.linalg.svd(Z, full_matrices=False)\n\n    # Numerical threshold relative to the largest singular value\n    if s.size == 0:\n        # No data cases, but with given tests m >= 1; handle gracefully\n        r = 0\n        Vr = np.zeros((points.shape[1], 0))\n    else:\n        s_max = s[0] if s.size > 0 else 0.0\n        if s_max > 0.0:\n            tau = 1e-10 * s_max\n            r = int(np.sum(s > tau))\n        else:\n            r = 0\n        # Basis of direction subspace from right singular vectors\n        Vr = Vt[:r, :].T if r > 0 else np.zeros((points.shape[1], 0))\n\n    # Distance from query to affine hull\n    v = query - xc\n    if r > 0:\n        # Orthogonal projection residual\n        # residual = v - Vr @ (Vr.T @ v)\n        proj_coeffs = Vr.T @ v\n        residual = v - (Vr @ proj_coeffs)\n    else:\n        residual = v\n    distance = float(np.linalg.norm(residual))\n\n    return r, distance\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (points_array, query_point)\n    test_cases = [\n        # Test Case 1: R^2, points on a line, query on the line\n        (np.array([[0.0, 1.0],\n                   [1.0, 3.0],\n                   [2.0, 5.0],\n                   [3.0, 7.0]]),\n         np.array([4.0, 9.0])),\n        # Test Case 2: R^3, points on a plane z = x + 2y, query on the plane\n        (np.array([[0.0, 0.0, 0.0],\n                   [1.0, 0.0, 1.0],\n                   [0.0, 1.0, 2.0],\n                   [2.0, -1.0, 0.0]]),\n         np.array([1.0, 1.0, 3.0])),\n        # Test Case 3: R^4, identical points (affine hull is a single point)\n        (np.array([[1.0, 1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0, 1.0]]),\n         np.array([0.0, 0.0, 0.0, 0.0])),\n        # Test Case 4: R^3, points on plane z=0, query with z=1\n        (np.array([[0.0, 0.0, 0.0],\n                   [1.0, 0.0, 0.0],\n                   [0.0, 1.0, 0.0]]),\n         np.array([0.0, 0.0, 1.0])),\n        # Test Case 5: R^3, points on a line direction (1,2,3), query off the line\n        (np.array([[1.0, 2.0, 3.0],\n                   [2.0, 4.0, 6.0],\n                   [3.0, 6.0, 9.0]]),\n         np.array([1.0, 0.0, 0.0])),\n    ]\n\n    results_str = []\n    for points, query in test_cases:\n        dim, dist = affine_hull_pca_dimension_and_distance(points, query)\n        # Format distance to six decimal places, ensure exact required format\n        results_str.append(f\"[{dim},{format(dist, '.6f')}]\")\n\n    # Final print statement in the exact required format: single line\n    print(f\"[{','.join(results_str)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3096329"}, {"introduction": "Why are affine hulls so important in optimization? This practice explores that question by contrasting the optimization of a linear function over a bounded convex hull versus its unbounded affine hull. By analyzing how the solution to an optimization problem changes, you will gain a crucial insight: relaxing inequality constraints to leave only affine equality constraints can fundamentally alter a problem's behavior, potentially making a bounded problem unbounded. This exercise illustrates a core principle in the formulation and analysis of optimization models [@problem_id:3096352].", "problem": "Consider a finite set of points $\\{x_i\\}_{i=1}^m \\subset \\mathbb{R}^n$, its convex hull $\\operatorname{conv}\\{x_i\\}$, and its affine hull $\\operatorname{aff}\\{x_i\\}$. Let $f(x) = c^\\top x$ be a linear functional with $c \\in \\mathbb{R}^n$. In the context of optimization methods, one is often interested in how the feasible region changes the behavior of solutions. In particular, affine feasibility (only equalities, no inequalities) can enlarge the feasible region from a bounded convex polytope to an unbounded affine subspace.\n\nWork with the specific instance in $\\mathbb{R}^2$ given by the points $x_1 = (0,0)$, $x_2 = (2,2)$, and $x_3 = (3,3)$. For these points, the convex hull is the line segment joining $x_1$ to $x_3$, and the affine hull is the entire line $\\{(t,t) : t \\in \\mathbb{R}\\}$. Consider two linear functionals:\n- $f_1(x) = c_1^\\top x$ with $c_1 = (1,-1)$,\n- $f_2(x) = c_2^\\top x$ with $c_2 = (1,0)$.\n\nSelect all statements that are correct about maximizing $f(x)$ over $\\operatorname{conv}\\{x_i\\}$ versus over $\\operatorname{aff}\\{x_i\\}$, and about how replacing a bounded convex feasible region with its affine hull (affine feasibility alone, without inequalities) changes solution behavior.\n\nA. For $f_1$, maximizing over $\\operatorname{aff}\\{x_i\\}$ yields the same optimal value as maximizing over $\\operatorname{conv}\\{x_i\\}$, and every point of $\\operatorname{aff}\\{x_i\\}$ is an optimizer when maximizing $f_1$ over $\\operatorname{aff}\\{x_i\\}$.\n\nB. For $f_2$, maximizing over $\\operatorname{aff}\\{x_i\\}$ is unbounded above, while maximizing over $\\operatorname{conv}\\{x_i\\}$ attains a finite maximum at $x_3$.\n\nC. For any linear functional $f(x) = c^\\top x$, the maximum over $\\operatorname{conv}\\{x_i\\}$ cannot occur at an extreme point; it must occur at a point in the relative interior of $\\operatorname{conv}\\{x_i\\}$.\n\nD. If $c$ is orthogonal to the direction subspace of $\\operatorname{aff}\\{x_i\\}$, then maximizing $c^\\top x$ over $\\operatorname{aff}\\{x_i\\}$ has a unique optimizer equal to the centroid (arithmetic mean) of the points $\\{x_i\\}$.", "solution": "The problem statement is first validated for soundness and completeness.\n\n**Step 1: Extract Givens**\n- A finite set of points $\\{x_i\\}_{i=1}^m \\subset \\mathbb{R}^n$.\n- The convex hull of these points, $\\operatorname{conv}\\{x_i\\}$.\n- The affine hull of these points, $\\operatorname{aff}\\{x_i\\}$.\n- A linear functional $f(x) = c^\\top x$ with $c \\in \\mathbb{R}^n$.\n- A specific instance in $\\mathbb{R}^2$ with points $x_1 = (0,0)$, $x_2 = (2,2)$, and $x_3 = (3,3)$.\n- For these points, the problem states that the convex hull is the line segment from $x_1$ to $x_3$, and the affine hull is the line $\\{(t,t) : t \\in \\mathbb{R}\\}$.\n- Two specific linear functionals are provided:\n    - $f_1(x) = c_1^\\top x$ with $c_1 = (1,-1)$.\n    - $f_2(x) = c_2^\\top x$ with $c_2 = (1,0)$.\n- The task is to evaluate statements about maximizing these functionals over the convex and affine hulls.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientific Grounding**: The concepts of convex hulls, affine hulls, and linear functionals are fundamental to convex analysis and optimization. They are well-defined mathematical objects. The problem is scientifically grounded.\n2.  **Well-Posedness**: The problem asks to evaluate the truth of four distinct mathematical statements based on a given setup. Each statement is a falsifiable claim, making the problem well-posed.\n3.  **Objectivity**: The language is precise and mathematical, free of subjective content.\n4.  **Consistency and Completeness**: The provided points $x_1=(0,0)$, $x_2=(2,2)$, and $x_3=(3,3)$ are collinear, lying on the line $y=x$.\n    - The convex hull of these points is the set of convex combinations. For collinear points, this is the line segment connecting the outermost points. Here, $x_1$ and $x_3$ are the outermost points, as $x_2 = \\frac{1}{3}x_1 + \\frac{2}{3}x_3$. Thus, $\\operatorname{conv}\\{x_1, x_2, x_3\\}$ is the line segment from $(0,0)$ to $(3,3)$. The problem statement is correct on this.\n    - The affine hull is the set of all affine combinations. For collinear points, this is the entire line passing through them. The line through $(0,0)$ and $(3,3)$ is indeed $\\{(t,t) : t \\in \\mathbb{R}\\}$. The problem statement is also correct on this.\n    The problem setup is self-consistent and provides all necessary information to analyze the options.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. The analysis may proceed.\n\n**Problem Analysis**\n\nLet the set of points be $S = \\{x_1, x_2, x_3\\}$ where $x_1 = (0,0)$, $x_2 = (2,2)$, and $x_3 = (3,3)$.\nThe convex hull of $S$ is $\\mathcal{C} = \\operatorname{conv}(S)$. As established, this is the line segment connecting $x_1$ and $x_3$. Any point $x \\in \\mathcal{C}$ can be written as $x = (t,t)$ for $t \\in [0,3]$.\nThe affine hull of $S$ is $\\mathcal{A} = \\operatorname{aff}(S)$. This is the line passing through the points. Any point $x \\in \\mathcal{A}$ can be written as $x = (t,t)$ for $t \\in \\mathbb{R}$.\n\nThe two linear functionals are $f_1(x) = c_1^\\top x$ with $c_1 = (1,-1)$ and $f_2(x) = c_2^\\top x$ with $c_2 = (1,0)$.\n\nLet's evaluate the functionals on a general point $x = (t,t) \\in \\mathcal{A}$:\n- For $f_1(x)$: $f_1((t,t)) = c_1^\\top (t,t) = (1, -1) \\cdot (t, t) = 1 \\cdot t + (-1) \\cdot t = t - t = 0$.\n- For $f_2(x)$: $f_2((t,t)) = c_2^\\top (t,t) = (1, 0) \\cdot (t, t) = 1 \\cdot t + 0 \\cdot t = t$.\n\nWe can now evaluate each statement.\n\n**A. For $f_1$, maximizing over $\\operatorname{aff}\\{x_i\\}$ yields the same optimal value as maximizing over $\\operatorname{conv}\\{x_i\\}$, and every point of $\\operatorname{aff}\\{x_i\\}$ is an optimizer when maximizing $f_1$ over $\\operatorname{aff}\\{x_i\\}$.**\n\n- **Maximizing $f_1$ over $\\mathcal{A} = \\operatorname{aff}\\{x_i\\}$**: As calculated above, $f_1(x) = 0$ for all $x \\in \\mathcal{A}$. The function is constant over the entire feasible set. The maximum value is $0$. Since the function value is the same everywhere, every point $x \\in \\mathcal{A}$ is a maximizer.\n- **Maximizing $f_1$ over $\\mathcal{C} = \\operatorname{conv}\\{x_i\\}$**: Since $\\mathcal{C} \\subset \\mathcal{A}$, the function $f_1(x)$ is also constant and equal to $0$ for all $x \\in \\mathcal{C}$. The maximum value is $0$.\n- **Comparison**: The optimal value for maximization over $\\mathcal{A}$ is $0$. The optimal value for maximization over $\\mathcal{C}$ is also $0$. They are the same. The second part of the statement, that every point of $\\operatorname{aff}\\{x_i\\}$ is an optimizer for the maximization over $\\operatorname{aff}\\{x_i\\}$, is also true.\n\nThus, statement A is **Correct**.\n\n**B. For $f_2$, maximizing over $\\operatorname{aff}\\{x_i\\}$ is unbounded above, while maximizing over $\\operatorname{conv}\\{x_i\\}$ attains a finite maximum at $x_3$.**\n\n- **Maximizing $f_2$ over $\\mathcal{A} = \\operatorname{aff}\\{x_i\\}$**: The problem is to maximize $f_2((t,t)) = t$ over the domain $t \\in \\mathbb{R}$. As $t$ can be arbitrarily large, the value of the function is not bounded above.\n- **Maximizing $f_2$ over $\\mathcal{C} = \\operatorname{conv}\\{x_i\\}$**: The problem is to maximize $f_2((t,t)) = t$ over the domain $t \\in [0,3]$. The maximum value is $3$, which is attained at $t=3$. The point corresponding to $t=3$ is $(3,3)$, which is the point $x_3$. So, the maximization attains a finite maximum at $x_3$.\n- Both parts of the statement are accurate.\n\nThus, statement B is **Correct**.\n\n**C. For any linear functional $f(x) = c^\\top x$, the maximum over $\\operatorname{conv}\\{x_i\\}$ cannot occur at an extreme point; it must occur at a point in the relative interior of $\\operatorname{conv}\\{x_i\\}$.**\n\nThis statement is a general claim about linear optimization over a convex hull. The fundamental theorem of linear programming states that the maximum (if it exists) of a linear function over a compact convex set (such as a polytope, which is the convex hull of a finite number of points) is always attained at one of its extreme points (vertices).\nIn our specific case, the convex hull $\\mathcal{C}$ is a line segment. Its extreme points are $x_1=(0,0)$ and $x_3=(3,3)$. The relative interior is the open segment between them, i.e., $\\{(t,t) : t \\in (0,3)\\}$.\n- As a counterexample, consider the maximization of $f_2(x)$ over $\\mathcal{C}$. We found in the analysis for B that the maximum occurs at $x_3$, which is an extreme point. This directly contradicts the statement.\n- Another counterexample: consider minimizing $f_2(x)$ over $\\mathcal{C}$. The minimum is $0$, attained at $x_1$, another extreme point.\nThe statement makes a claim that is the opposite of established theory.\n\nThus, statement C is **Incorrect**.\n\n**D. If $c$ is orthogonal to the direction subspace of $\\operatorname{aff}\\{x_i\\}$, then maximizing $c^\\top x$ over $\\operatorname{aff}\\{x_i\\}$ has a unique optimizer equal to the centroid (arithmetic mean) of the points $\\{x_i\\}$.**\n\nLet $\\mathcal{A} = \\operatorname{aff}\\{x_i\\}$ be an affine set. It can be written as $\\mathcal{A} = x_0 + V$, where $x_0 \\in \\mathcal{A}$ and $V$ is the direction subspace. The direction subspace for our problem is $V = \\operatorname{span}\\{(1,1)\\}$, since $\\mathcal{A}$ is the line passing through the origin with direction vector $(1,1)$.\nThe condition is that $c$ is orthogonal to $V$, which means $c^\\top v = 0$ for all $v \\in V$.\nLet's evaluate $f(x) = c^\\top x$ for any two points $x_a, x_b \\in \\mathcal{A}$. We can write $x_b = x_a + v$ for some $v \\in V$. Then, $f(x_b) = c^\\top x_b = c^\\top(x_a + v) = c^\\top x_a + c^\\top v$. Since $c^\\top v = 0$, we have $f(x_b) = c^\\top x_a$. This shows that the function $f(x)$ is constant on the entire affine hull $\\mathcal{A}$.\nIf the function is constant on $\\mathcal{A}$, then every point in $\\mathcal{A}$ is a maximizer. The set of optimizers is $\\mathcal{A}$ itself. An optimizer is unique only if the set $\\mathcal{A}$ consists of a single point, which is not true in this problem.\nThe centroid is $\\bar{x} = \\frac{1}{3}(x_1+x_2+x_3) = \\frac{1}{3}((0,0)+(2,2)+(3,3)) = (\\frac{5}{3}, \\frac{5}{3})$. This point is in $\\mathcal{A}$. It is an optimizer, but it is not unique. For instance, $x_1=(0,0)$ is also an optimizer.\nThe vector $c_1=(1,-1)$ from the problem statement is orthogonal to $V=\\operatorname{span}\\{(1,1)\\}$, since $c_1^\\top (1,1) = 1-1=0$. As shown in the analysis for A, $f_1(x)$ is constant on $\\mathcal{A}$, and every point is an optimizer. This provides a direct counterexample to the claim of uniqueness.\n\nThus, statement D is **Incorrect**.", "answer": "$$\\boxed{AB}$$", "id": "3096352"}]}