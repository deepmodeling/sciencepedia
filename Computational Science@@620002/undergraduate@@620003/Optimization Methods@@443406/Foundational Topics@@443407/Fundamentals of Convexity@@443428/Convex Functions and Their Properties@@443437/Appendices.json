{"hands_on_practices": [{"introduction": "A core tenet of convexity is a simple geometric idea: for a convex function, the line segment connecting any two points on its graph must lie on or above the graph itself. This practice asks you to explore this definition in a quantitative way for the function $f(x) = 1/x$, a classic example of a convex function for $x > 0$. By calculating the maximum vertical gap between the function and one of its secant lines, you will gain a deeper, more tangible understanding of what convexity means geometrically [@problem_id:2163733].", "problem": "Investigate the properties of the function $g(x) = 1/x$ for $x > 0$. Consider a straight line segment, known as a secant line, that connects two distinct points on the graph of $g(x)$. Let these points be $(a, g(a))$ and $(b, g(b))$, where the constants $a$ and $b$ are positive real numbers satisfying $0  a  b$. For any $x$ in the interval $[a, b]$, the secant line lies above or on the graph of $g(x)$. Let $h(x)$ represent the vertical distance between the secant line and the graph of $g(x)$ at the coordinate $x$. Determine the maximum possible value of $h(x)$ on the interval $[a,b]$.\n\nExpress your answer as a single closed-form analytic expression in terms of the parameters $a$ and $b$.", "solution": "Let $g(x)=\\frac{1}{x}$ for $x0$ and fix $0ab$. The secant line through the points $(a,g(a))=(a,\\frac{1}{a})$ and $(b,g(b))=(b,\\frac{1}{b})$ has slope\n$$\nm=\\frac{g(b)-g(a)}{b-a}=\\frac{\\frac{1}{b}-\\frac{1}{a}}{b-a}=\\frac{a-b}{ab(b-a)}=-\\frac{1}{ab}.\n$$\nAn equation of this secant line is\n$$\nL(x)=g(a)+m(x-a)=\\frac{1}{a}-\\frac{1}{ab}(x-a)=\\frac{1}{a}+\\frac{1}{b}-\\frac{x}{ab}=\\frac{a+b-x}{ab}.\n$$\nDefine the vertical distance between the secant line and the graph by\n$$\nh(x)=L(x)-g(x)=\\frac{a+b-x}{ab}-\\frac{1}{x}, \\quad x\\in[a,b].\n$$\nSince $g''(x)=\\frac{2}{x^{3}}0$ for $x0$, the function $g$ is convex, so $L(x)\\geq g(x)$ on $[a,b]$, and $h(x)\\geq 0$. To find the maximum of $h$ on $[a,b]$, compute its derivatives:\n$$\nh'(x)=-\\frac{1}{ab}+\\frac{1}{x^{2}}, \\qquad h''(x)=-\\frac{2}{x^{3}}0 \\text{ for } x0.\n$$\nSetting $h'(x)=0$ gives\n$$\n-\\frac{1}{ab}+\\frac{1}{x^{2}}=0 \\quad \\Longrightarrow \\quad x^{2}=ab \\quad \\Longrightarrow \\quad x=\\sqrt{ab},\n$$\nand since $h''(x)0$, this critical point is a strict maximum. Because $ab$, we have $a\\sqrt{ab}b$, so the maximizer lies in $[a,b]$. Evaluating $h$ at $x=\\sqrt{ab}$ gives\n$$\nh_{\\max}=h(\\sqrt{ab})=\\frac{a+b-\\sqrt{ab}}{ab}-\\frac{1}{\\sqrt{ab}}=\\frac{a+b-2\\sqrt{ab}}{ab}=\\frac{\\left(\\sqrt{b}-\\sqrt{a}\\right)^{2}}{ab}.\n$$\nThus, the maximum vertical distance between the secant line and the graph on $[a,b]$ equals $\\frac{(\\sqrt{b}-\\sqrt{a})^{2}}{ab}$.", "answer": "$$\\boxed{\\frac{\\left(\\sqrt{b}-\\sqrt{a}\\right)^{2}}{ab}}$$", "id": "2163733"}, {"introduction": "While the geometric picture is intuitive, it becomes difficult to apply in higher dimensions. For multivariable functions, we need a more robust algebraic tool: the Hessian matrix. A twice-differentiable function is convex if and only if its Hessian matrix is positive semidefinite everywhere in its domain. This exercise [@problem_id:2163677] provides a practical application of this powerful criterion, asking you to determine the stability of a simplified physical system by finding the conditions under which its potential energy function is convex.", "problem": "In the design of certain physical systems, the potential energy function must be convex to ensure that the system has a single stable equilibrium point. A simplified model of a two-particle system with interaction is described by a potential energy function $V(x_1, x_2)$, where $x_1$ and $x_2$ represent the positions of the two particles. The function is defined over the entire plane $\\mathbb{R}^2$ as:\n$$V(x_1, x_2) = x_1^2 + c x_1 x_2 + x_2^2$$\nIn this model, $c$ is a real-valued coupling constant that determines the strength and nature of the interaction between the particles. For the system to be stable in the sense that it possesses a unique global energy minimum, the potential energy function $V(x_1, x_2)$ must be convex.\n\nDetermine the complete set of values for the coupling constant $c$ for which the potential energy function $V(x_1, x_2)$ is convex. Select the correct option from the choices below.\n\nA. For all real numbers $c$.\n\nB. Only for $c=0$.\n\nC. The interval $-1 \\le c \\le 1$.\n\nD. The interval $-2 \\le c \\le 2$.\n\nE. The interval $c \\ge 0$.", "solution": "A twice-differentiable function of multiple variables is convex on a domain if and only if its Hessian matrix is positive semidefinite for all points in that domain. The given potential energy function is $V(x_1, x_2) = x_1^2 + c x_1 x_2 + x_2^2$. This function is a polynomial and is therefore infinitely differentiable everywhere on $\\mathbb{R}^2$.\n\nFirst, we compute the gradient of the function $V(x_1, x_2)$ by finding its first-order partial derivatives:\n$$ \\frac{\\partial V}{\\partial x_1} = 2x_1 + c x_2 $$\n$$ \\frac{\\partial V}{\\partial x_2} = c x_1 + 2x_2 $$\n\nNext, we compute the Hessian matrix, denoted by $H$, which consists of the second-order partial derivatives:\n$$ \\frac{\\partial^2 V}{\\partial x_1^2} = \\frac{\\partial}{\\partial x_1}(2x_1 + c x_2) = 2 $$\n$$ \\frac{\\partial^2 V}{\\partial x_2^2} = \\frac{\\partial}{\\partial x_2}(c x_1 + 2x_2) = 2 $$\n$$ \\frac{\\partial^2 V}{\\partial x_1 \\partial x_2} = \\frac{\\partial}{\\partial x_1}(c x_1 + 2x_2) = c $$\nBy Clairaut's theorem on the equality of mixed partials, we also have $\\frac{\\partial^2 V}{\\partial x_2 \\partial x_1} = c$.\n\nThe Hessian matrix $H$ is therefore given by:\n$$ H = \\begin{pmatrix} \\frac{\\partial^2 V}{\\partial x_1^2}  \\frac{\\partial^2 V}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 V}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 V}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2  c \\\\ c  2 \\end{pmatrix} $$\nNote that the Hessian matrix is constant and does not depend on the point $(x_1, x_2)$.\n\nFor the function $V(x_1, x_2)$ to be convex, the Hessian matrix $H$ must be positive semidefinite. A symmetric matrix is positive semidefinite if and only if all of its leading principal minors are non-negative.\nThe leading principal minors of the $2 \\times 2$ matrix $H$ are:\n1.  The first leading principal minor, $M_1$, which is the top-left element: $M_1 = 2$.\n2.  The second leading principal minor, $M_2$, which is the determinant of the matrix: $M_2 = \\det(H)$.\n\nWe must check the conditions for these minors to be non-negative.\nThe first condition is $M_1 \\ge 0$. In our case, $M_1 = 2$, so $2 \\ge 0$, which is always true.\n\nThe second condition is $M_2 \\ge 0$. We calculate the determinant of $H$:\n$$ \\det(H) = (2)(2) - (c)(c) = 4 - c^2 $$\nThe condition $M_2 \\ge 0$ thus becomes:\n$$ 4 - c^2 \\ge 0 $$\nRearranging the inequality, we get:\n$$ 4 \\ge c^2 $$\nThis is equivalent to $|c| \\le 2$, which can be written as the interval:\n$$ -2 \\le c \\le 2 $$\n\nSince both conditions for positive semidefiniteness must be satisfied, we find that the function $V(x_1, x_2)$ is convex if and only if $c$ lies within the interval $[-2, 2]$.\n\nComparing this result with the given choices:\nA. For all real numbers $c$. (Incorrect)\nB. Only for $c=0$. (Incorrect, this is just one point in the valid range)\nC. The interval $-1 \\le c \\le 1$. (Incorrect, this is a subset of the full range)\nD. The interval $-2 \\le c \\le 2$. (Correct)\nE. The interval $c \\ge 0$. (Incorrect)\n\nThus, the correct option is D.", "answer": "$$\\boxed{D}$$", "id": "2163677"}, {"introduction": "Building on the fundamentals, we can explore more advanced concepts that make convex functions so powerful in optimization. One such concept is the Fenchel conjugate, which is central to the theory of duality, allowing us to view optimization problems from a different and often more insightful perspective. This practice [@problem_id:2163745] guides you through the process of deriving the Fenchel conjugate for a general quadratic function, a foundational result that appears frequently in advanced optimization and machine learning algorithms.", "problem": "Let $x$ and $y$ be vectors in $\\mathbb{R}^n$. Consider a strongly convex quadratic function $f: \\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(x) = \\frac{1}{2}x^T A x$, where $A$ is a fixed $n \\times n$ symmetric and positive definite matrix.\n\nThe Fenchel conjugate of the function $f(x)$, denoted as $f^*(y)$, is defined by the following optimization problem:\n$$ f^*(y) = \\sup_{x \\in \\mathbb{R}^n} \\left( y^T x - f(x) \\right) $$\nwhere $\\sup_{x \\in \\mathbb{R}^n}$ indicates the supremum (or least upper bound) of the expression over all vectors $x$ in the domain $\\mathbb{R}^n$.\n\nDetermine the closed-form analytical expression for $f^*(y)$. Your final expression should be in terms of the vector $y$ and the matrix $A$.", "solution": "To find the Fenchel conjugate $f^*(y)$, we need to solve the maximization problem with respect to $x$ for a fixed $y$. Let's define the objective function inside the supremum as $g(x)$:\n$$ g(x) = y^T x - f(x) = y^T x - \\frac{1}{2}x^T A x $$\nWe want to find $\\sup_{x \\in \\mathbb{R}^n} g(x)$.\n\nThe function $f(x) = \\frac{1}{2}x^T A x$ is a convex function because its Hessian matrix is $\\nabla^2 f(x) = A$, which is a positive definite matrix by definition. Consequently, the function $-f(x)$ is a concave function. The function $g(x)$ is the sum of a linear function ($y^T x$) and a concave function ($-f(x)$), which makes $g(x)$ a concave function.\n\nFor a differentiable concave function defined on $\\mathbb{R}^n$, its supremum (which is also its maximum) occurs at a point where its gradient with respect to $x$ is the zero vector. Let's compute the gradient of $g(x)$:\n$$ \\nabla_x g(x) = \\nabla_x \\left( y^T x - \\frac{1}{2}x^T A x \\right) $$\nUsing standard matrix calculus identities, we have $\\nabla_x(y^T x) = y$. For the quadratic term, since $A$ is symmetric, the gradient $\\nabla_x(x^T A x) = 2Ax$. Therefore, $\\nabla_x(\\frac{1}{2}x^T A x) = Ax$.\nCombining these results, the gradient of $g(x)$ is:\n$$ \\nabla_x g(x) = y - Ax $$\nTo find the point $x^*$ that maximizes $g(x)$, we set the gradient to zero:\n$$ y - Ax^* = 0 $$\n$$ Ax^* = y $$\nSince the matrix $A$ is positive definite, it is invertible. We can solve for $x^*$ by multiplying by the inverse of $A$, denoted as $A^{-1}$:\n$$ x^* = A^{-1}y $$\nThis is the unique value of $x$ that maximizes $g(x)$. The value of the supremum, which is $f^*(y)$, is obtained by substituting this optimal $x^*$ back into the expression for $g(x)$:\n$$ f^*(y) = g(x^*) = y^T x^* - \\frac{1}{2}(x^*)^T A x^* $$\nNow, we substitute $x^* = A^{-1}y$ into this equation:\n$$ f^*(y) = y^T (A^{-1}y) - \\frac{1}{2}(A^{-1}y)^T A (A^{-1}y) $$\nLet's simplify the second term. Using the transpose property $(BC)^T = C^TB^T$, we get:\n$$ (A^{-1}y)^T = y^T(A^{-1})^T $$\nSince $A$ is a symmetric matrix, its inverse $A^{-1}$ is also symmetric. Thus, $(A^{-1})^T = A^{-1}$. The second term becomes:\n$$ \\frac{1}{2} y^T (A^{-1})^T A (A^{-1}y) = \\frac{1}{2} y^T A^{-1} A A^{-1} y $$\nSince $A^{-1}A = I$, where $I$ is the identity matrix, this simplifies to:\n$$ \\frac{1}{2} y^T A^{-1} I y = \\frac{1}{2} y^T A^{-1} y $$\nNow, we can substitute this back into the expression for $f^*(y)$:\n$$ f^*(y) = y^T A^{-1} y - \\frac{1}{2} y^T A^{-1} y $$\nCombining the terms, we arrive at the final expression for the Fenchel conjugate:\n$$ f^*(y) = \\frac{1}{2} y^T A^{-1} y $$", "answer": "$$\\boxed{\\frac{1}{2} y^{T} A^{-1} y}$$", "id": "2163745"}]}