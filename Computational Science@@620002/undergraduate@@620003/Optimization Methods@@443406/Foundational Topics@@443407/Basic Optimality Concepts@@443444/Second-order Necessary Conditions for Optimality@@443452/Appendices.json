{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of verifying the second-order necessary conditions for an equality-constrained problem. By systematically identifying the Karush-Kuhn-Tucker (KKT) point, constructing the tangent space, and evaluating the curvature of the Lagrangian, you will solidify your understanding of the core analytical procedure. This practice is essential for building the skills needed to analyze more complex optimization scenarios. [@problem_id:3175826]", "problem": "Let $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $f(x)=x_{1}^{2}-2x_{1}x_{2}+4x_{2}^{2}$ and consider the equality constraint $c(x)=x_{1}-3x_{2}=0$. Using only core definitions from equality-constrained optimization, do the following: identify the Karush-Kuhn-Tucker (KKT) point(s) by enforcing the first-order necessary conditions, construct the tangent space to the constraint at the KKT point, form the Hessian of the Lagrangian at that point, and verify the second-order necessary condition for a local constrained minimizer by evaluating the reduced curvature in the unit tangent direction. Provide as your final answer the exact value of this reduced curvature scalar. No rounding is required, and no units are involved.", "solution": "The user has provided a valid optimization problem. I will now proceed with the solution.\n\nThe problem is to find the reduced curvature for an equality-constrained optimization problem. The objective function is $f: \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $f(x) = x_{1}^{2} - 2x_{1}x_{2} + 4x_{2}^{2}$, where $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$. The equality constraint is $c(x) = x_{1} - 3x_{2} = 0$.\n\nThe solution process involves four main steps as requested: finding the Karush-Kuhn-Tucker (KKT) point, determining the tangent space, calculating the Hessian of the Lagrangian, and finally computing the reduced curvature.\n\nFirst, we define the Lagrangian function $\\mathcal{L}(x, \\lambda)$ for this problem, which is given by $\\mathcal{L}(x, \\lambda) = f(x) - \\lambda c(x)$.\n$$\n\\mathcal{L}(x_1, x_2, \\lambda) = (x_{1}^{2} - 2x_{1}x_{2} + 4x_{2}^{2}) - \\lambda(x_{1} - 3x_{2})\n$$\n\nThe first-order necessary conditions for a point $x^*$ to be a candidate for a local constrained extremum (a KKT point) are given by the system of equations $\\nabla_x \\mathcal{L}(x^*, \\lambda^*) = 0$ and $c(x^*) = 0$.\n\nLet's compute the gradient of the Lagrangian with respect to $x$:\n$$\n\\nabla_x \\mathcal{L}(x_1, x_2, \\lambda) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x_1} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 - 2x_2 - \\lambda \\\\ -2x_1 + 8x_2 + 3\\lambda \\end{pmatrix}\n$$\n\nSetting the gradient to the zero vector and including the constraint equation, we obtain the following system of three equations in variables $x_1$, $x_2$, and $\\lambda$:\n1. $2x_1 - 2x_2 - \\lambda = 0$\n2. $-2x_1 + 8x_2 + 3\\lambda = 0$\n3. $x_1 - 3x_2 = 0$\n\nFrom equation (3), we have $x_1 = 3x_2$. We substitute this into equations (1) and (2).\nSubstituting into (1):\n$$\n2(3x_2) - 2x_2 - \\lambda = 0 \\implies 6x_2 - 2x_2 = \\lambda \\implies 4x_2 = \\lambda\n$$\nSubstituting into (2):\n$$\n-2(3x_2) + 8x_2 + 3\\lambda = 0 \\implies -6x_2 + 8x_2 + 3\\lambda = 0 \\implies 2x_2 + 3\\lambda = 0\n$$\nNow we have a system of two linear equations for $x_2$ and $\\lambda$:\na. $\\lambda = 4x_2$\nb. $2x_2 + 3\\lambda = 0$\n\nSubstitute (a) into (b):\n$$\n2x_2 + 3(4x_2) = 0 \\implies 2x_2 + 12x_2 = 0 \\implies 14x_2 = 0 \\implies x_2 = 0\n$$\nWith $x_2 = 0$, we find $\\lambda$ from (a): $\\lambda = 4(0) = 0$.\nAnd we find $x_1$ from the constraint: $x_1 = 3(0) = 0$.\n\nThus, the unique KKT point is $x^* = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with the associated Lagrange multiplier $\\lambda^* = 0$.\n\nNext, we construct the tangent space to the constraint at the KKT point $x^*$. The tangent space $T(x^*)$ is the set of all vectors $z \\in \\mathbb{R}^2$ such that $\\nabla c(x^*)^T z = 0$.\nThe gradient of the constraint function $c(x) = x_1 - 3x_2$ is:\n$$\n\\nabla c(x) = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}\n$$\nThis gradient is constant, so $\\nabla c(x^*) = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$.\nThe condition for a vector $z = \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}$ to be in the tangent space is:\n$$\n\\nabla c(x^*)^T z = \\begin{pmatrix} 1  -3 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = z_1 - 3z_2 = 0\n$$\nThis implies $z_1 = 3z_2$. Any vector in the tangent space must be of the form $z = \\begin{pmatrix} 3z_2 \\\\ z_2 \\end{pmatrix} = z_2 \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$. This is a one-dimensional space spanned by the vector $v = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$.\nThe problem requires the unit tangent direction. We normalize the basis vector $v$:\n$$\n\\|v\\| = \\sqrt{3^2 + 1^2} = \\sqrt{9 + 1} = \\sqrt{10}\n$$\nThe unit tangent vector is $u = \\frac{v}{\\|v\\|} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$.\n\nNow, we form the Hessian of the Lagrangian with respect to $x$, denoted by $\\nabla_{xx}^2 \\mathcal{L}(x, \\lambda)$.\n$$\n\\nabla_{xx}^2 \\mathcal{L}(x, \\lambda) = \\begin{pmatrix} \\frac{\\partial^2 \\mathcal{L}}{\\partial x_1^2}  \\frac{\\partial^2 \\mathcal{L}}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 \\mathcal{L}}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 \\mathcal{L}}{\\partial x_2^2} \\end{pmatrix}\n$$\nThe second partial derivatives are:\n$\\frac{\\partial^2 \\mathcal{L}}{\\partial x_1^2} = \\frac{\\partial}{\\partial x_1}(2x_1 - 2x_2 - \\lambda) = 2$\n$\\frac{\\partial^2 \\mathcal{L}}{\\partial x_1 \\partial x_2} = \\frac{\\partial}{\\partial x_2}(2x_1 - 2x_2 - \\lambda) = -2$\n$\\frac{\\partial^2 \\mathcal{L}}{\\partial x_2^2} = \\frac{\\partial}{\\partial x_2}(-2x_1 + 8x_2 + 3\\lambda) = 8$\nThe Hessian is symmetric, so $\\frac{\\partial^2 \\mathcal{L}}{\\partial x_2 \\partial x_1} = -2$.\nThe Hessian matrix is:\n$$\n\\nabla_{xx}^2 \\mathcal{L}(x, \\lambda) = \\begin{pmatrix} 2  -2 \\\\ -2  8 \\end{pmatrix}\n$$\nThis Hessian is a constant matrix. At the KKT point $(x^*, \\lambda^*)$, the Hessian is $\\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*) = \\begin{pmatrix} 2  -2 \\\\ -2  8 \\end{pmatrix}$.\n\nFinally, we verify the second-order necessary condition and compute the reduced curvature. The second-order necessary condition for a constrained local minimum is that the curvature of the Lagrangian is non-negative on the tangent space, i.e., $z^T \\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*) z \\ge 0$ for all $z \\in T(x^*)$. The reduced curvature in the unit tangent direction $u$ is given by the quadratic form $u^T \\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*) u$.\n\nLet $H^* = \\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*)$. We compute the value:\n$$\nu^T H^* u = \\left( \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \\right)^T \\begin{pmatrix} 2  -2 \\\\ -2  8 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \\right)\n$$\n$$\nu^T H^* u = \\frac{1}{10} \\begin{pmatrix} 3  1 \\end{pmatrix} \\begin{pmatrix} 2  -2 \\\\ -2  8 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nFirst, we compute the matrix-vector product:\n$$\n\\begin{pmatrix} 2  -2 \\\\ -2  8 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2(3) + (-2)(1) \\\\ -2(3) + 8(1) \\end{pmatrix} = \\begin{pmatrix} 6 - 2 \\\\ -6 + 8 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\n$$\nNext, we compute the dot product:\n$$\n\\begin{pmatrix} 3  1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = (3)(4) + (1)(2) = 12 + 2 = 14\n$$\nFinally, we put it all together:\n$$\nu^T H^* u = \\frac{1}{10} (14) = \\frac{14}{10} = \\frac{7}{5}\n$$\nThe value of the reduced curvature is $\\frac{7}{5}$. Since this value is positive, the second-order necessary condition for a local constrained minimizer is satisfied. The problem asks for this specific scalar value.", "answer": "$$\\boxed{\\frac{7}{5}}$$", "id": "3175826"}, {"introduction": "Moving beyond a simple case, this problem introduces a mix of equality and inequality constraints and a more complex objective function. The main goal here is to explore a scenario where the second-order necessary conditions are satisfied, yet the candidate point is not a local minimum. This exercise is crucial for understanding the distinction between necessary and sufficient conditions for optimality, a subtle but vital concept in optimization theory. [@problem_id:3175817]", "problem": "Consider the constrained optimization problem of minimizing the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x)=x_{1}^{2}-x_{2}^{4}$ subject to the constraints $h(x)=x_{1}=0$ and $g(x)=-x_{2}\\le 0$ (equivalently $x_{2}\\ge 0$). Let $x^{\\star}=(0,0)$ be the candidate point of interest. Using only the fundamental definitions of gradient, Hessian, Lagrangian, tangent cone, and the Karush–Kuhn–Tucker (KKT) conditions, perform the following:\n\n1) Compute $\\nabla^{2} f(x^{\\star})$ from first principles and verify its structure.\n\n2) Determine the KKT multipliers $(\\lambda^{\\star},\\mu^{\\star})$ that satisfy stationarity, primal feasibility, dual feasibility, and complementary slackness at $x^{\\star}$.\n\n3) Form the Hessian of the Lagrangian at $x^{\\star}$, $\\nabla_{xx}^{2}L(x^{\\star},\\lambda^{\\star},\\mu^{\\star})$, and characterize the set of first-order feasible directions (the tangent cone) and the associated critical cone used in the second-order necessary conditions for optimality.\n\n4) Define $m$ as:\n$$ m = \\inf\\left\\{ d^{\\top}\\nabla_{xx}^{2}L(x^{\\star},\\lambda^{\\star},\\mu^{\\star})d : d \\text{ belongs to the critical cone at } x^{\\star}, \\|d\\|_{2}=1 \\right\\}. $$\nCompute the exact value of $m$.\n\nYour final answer must be the exact value of $m$ as a single real number. No rounding is required.", "solution": "The user has provided a constrained optimization problem and requested a series of calculations related to the second-order necessary conditions for optimality at the point $x^{\\star}=(0,0)$. The problem is mathematically well-defined and all required information is present. We will proceed with the solution by following the specified tasks.\n\nThe problem is to minimize $f(x) = x_1^2 - x_2^4$ for $x=(x_1, x_2) \\in \\mathbb{R}^2$, subject to an equality constraint $h(x) = x_1 = 0$ and an inequality constraint $g(x) = -x_2 \\le 0$. The point of interest is $x^{\\star}=(0,0)$.\n\n**1) Computation of $\\nabla^{2} f(x^{\\star})$**\n\nFirst, we compute the gradient of the function $f(x) = x_1^2 - x_2^4$. The first-order partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x_1} = 2x_1\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = -4x_2^3\n$$\nThe gradient vector is $\\nabla f(x) = \\begin{pmatrix} 2x_1 \\\\ -4x_2^3 \\end{pmatrix}$. At the point $x^{\\star}=(0,0)$, the gradient is $\\nabla f(x^{\\star}) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nNext, we compute the second-order partial derivatives to form the Hessian matrix:\n$$\n\\frac{\\partial^2 f}{\\partial x_1^2} = \\frac{\\partial}{\\partial x_1}(2x_1) = 2\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x_2^2} = \\frac{\\partial}{\\partial x_2}(-4x_2^3) = -12x_2^2\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} = \\frac{\\partial}{\\partial x_1}(-4x_2^3) = 0\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} = \\frac{\\partial}{\\partial x_2}(2x_1) = 0\n$$\nThe Hessian matrix of $f(x)$ is therefore:\n$$\n\\nabla^2 f(x) = \\begin{pmatrix} 2  0 \\\\ 0  -12x_2^2 \\end{pmatrix}\n$$\nEvaluating this at the candidate point $x^{\\star}=(0,0)$, we obtain:\n$$\n\\nabla^2 f(x^{\\star}) = \\begin{pmatrix} 2  0 \\\\ 0  -12(0)^2 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThis Hessian matrix is a diagonal matrix. It is positive semi-definite, as its eigenvalues are $2$ and $0$, which are non-negative.\n\n**2) Determination of KKT multipliers $(\\lambda^{\\star}, \\mu^{\\star})$**\n\nThe Lagrangian function for this problem is given by $L(x, \\lambda, \\mu) = f(x) + \\lambda h(x) + \\mu g(x)$, where $\\lambda \\in \\mathbb{R}$ is the multiplier for the equality constraint and $\\mu \\in \\mathbb{R}$ is the multiplier for the inequality constraint.\n$$\nL(x, \\lambda, \\mu) = x_1^2 - x_2^4 + \\lambda x_1 + \\mu(-x_2)\n$$\nThe Karush–Kuhn–Tucker (KKT) conditions at a regular point $x^{\\star}$ are:\n1.  **Stationarity:** $\\nabla_x L(x^{\\star}, \\lambda^{\\star}, \\mu^{\\star}) = 0$.\n2.  **Primal Feasibility:** $h(x^{\\star})=0$ and $g(x^{\\star}) \\le 0$.\n3.  **Dual Feasibility:** $\\mu^{\\star} \\ge 0$.\n4.  **Complementary Slackness:** $\\mu^{\\star} g(x^{\\star}) = 0$.\n\nWe verify these conditions for $x^{\\star}=(0,0)$:\n- **Primal Feasibility:** $h(x^{\\star}) = x_1^{\\star} = 0$, which holds. $g(x^{\\star}) = -x_2^{\\star} = 0$, which satisfies $g(x^{\\star}) \\le 0$. The inequality constraint is active at $x^{\\star}$.\n\n- **Stationarity:** The gradient of the Lagrangian with respect to $x$ is:\n$$\n\\nabla_x L(x, \\lambda, \\mu) = \\begin{pmatrix} 2x_1 + \\lambda \\\\ -4x_2^3 - \\mu \\end{pmatrix}\n$$\nAt $x^{\\star}=(0,0)$, the stationarity condition $\\nabla_x L(x^{\\star}, \\lambda^{\\star}, \\mu^{\\star}) = 0$ becomes:\n$$\n\\begin{pmatrix} 2(0) + \\lambda^{\\star} \\\\ -4(0)^3 - \\mu^{\\star} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies \\begin{pmatrix} \\lambda^{\\star} \\\\ -\\mu^{\\star} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the unique solution $\\lambda^{\\star}=0$ and $\\mu^{\\star}=0$.\n\n- **Dual Feasibility:** We must check if $\\mu^{\\star} \\ge 0$. Indeed, $\\mu^{\\star}=0$ satisfies this.\n\n- **Complementary Slackness:** We must check if $\\mu^{\\star}g(x^{\\star})=0$. We have $\\mu^{\\star}=0$ and $g(x^{\\star})=0$, so $(0)(0)=0$, which holds.\n\nAll KKT conditions are satisfied at $x^{\\star}=(0,0)$ with the unique multipliers $(\\lambda^{\\star}, \\mu^{\\star}) = (0,0)$.\n\n**3) Hessian of the Lagrangian, Tangent Cone, and Critical Cone**\n\nThe Hessian of the Lagrangian with respect to $x$ is obtained by differentiating $\\nabla_x L(x, \\lambda, \\mu)$:\n$$\n\\nabla_{xx}^2 L(x, \\lambda, \\mu) = \\begin{pmatrix} 2  0 \\\\ 0  -12x_2^2 \\end{pmatrix}\n$$\nEvaluating at $(x^{\\star}, \\lambda^{\\star}, \\mu^{\\star}) = ((0,0), 0, 0)$:\n$$\n\\nabla_{xx}^2 L(x^{\\star}, \\lambda^{\\star}, \\mu^{\\star}) = \\begin{pmatrix} 2  0 \\\\ 0  -12(0)^2 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThis is identical to $\\nabla^2 f(x^{\\star})$ because the Hessians of the linear constraints are zero matrices and the multipliers are also zero.\n\nTo find the **tangent cone**, we characterize the set of first-order feasible directions. The gradients of the constraints at $x^{\\star}$ are $\\nabla h(x^{\\star}) = (1,0)^{\\top}$ and $\\nabla g(x^{\\star}) = (0,-1)^{\\top}$. Since these vectors are linearly independent, the LICQ constraint qualification holds, and the tangent cone $T_{\\Omega}(x^{\\star})$ is equal to the set of linearized feasible directions. A direction $d=(d_1,d_2)$ is in this set if:\n1.  $\\nabla h(x^{\\star})^{\\top} d = 0 \\implies (1,0) \\cdot (d_1,d_2) = d_1 = 0$.\n2.  For the active inequality constraint $g(x)\\le 0$, we need $\\nabla g(x^{\\star})^{\\top} d \\le 0 \\implies (0,-1) \\cdot (d_1,d_2) = -d_2 \\le 0$, which implies $d_2 \\ge 0$.\nSo, the tangent cone is $T_{\\Omega}(x^{\\star}) = \\{d=(d_1,d_2) \\in \\mathbb{R}^2 \\mid d_1=0, d_2 \\ge 0\\}$.\n\nThe **critical cone** $C(x^{\\star}, \\mu^{\\star})$ is the set of directions $d$ from the tangent cone that are orthogonal to the gradient of the objective function, i.e., $C(x^{\\star}, \\mu^{\\star}) = \\{d \\in T_{\\Omega}(x^{\\star}) \\mid \\nabla f(x^{\\star})^{\\top} d = 0\\}$.\nWe found $\\nabla f(x^{\\star})=(0,0)^{\\top}$. Therefore, the condition $\\nabla f(x^{\\star})^{\\top} d = 0$ is $(0,0) \\cdot (d_1,d_2) = 0$, which is true for all $d \\in \\mathbb{R}^2$. This condition imposes no additional restrictions on the directions in the tangent cone.\nThus, the critical cone is identical to the tangent cone:\n$$\nC(x^{\\star}, \\mu^{\\star}) = T_{\\Omega}(x^{\\star}) = \\{d=(d_1,d_2) \\in \\mathbb{R}^2 \\mid d_1=0, d_2 \\ge 0\\}\n$$\n\n**4) Computation of $m$**\n\nWe are asked to compute the value of $m$:\n$$\nm = \\inf\\left\\{ d^{\\top}\\nabla_{xx}^{2}L(x^{\\star},\\lambda^{\\star},\\mu^{\\star})d : d \\in C(x^{\\star}, \\mu^{\\star}), \\|d\\|_{2}=1 \\right\\}\n$$\nThe set of vectors $d$ over which we take the infimum are those that belong to the critical cone and have a Euclidean norm of $1$. Let $d=(d_1,d_2)$. The conditions are:\n1.  $d_1 = 0$\n2.  $d_2 \\ge 0$\n3.  $\\|d\\|_2 = \\sqrt{d_1^2 + d_2^2} = 1$\n\nSubstituting $d_1=0$ into the norm condition gives $\\sqrt{0^2 + d_2^2} = 1$, which means $|d_2|=1$. Combined with the condition $d_2 \\ge 0$, we find that $d_2=1$.\nTherefore, there is only one vector $d$ that satisfies all conditions: $d=(0,1)$.\n\nThe set over which the infimum is taken is a singleton set containing only the vector $d = (0,1)^{\\top}$. Therefore, the infimum is simply the value of the quadratic form evaluated at this single vector.\nWe compute $d^{\\top}\\,\\nabla_{xx}^{2}L(x^{\\star},\\lambda^{\\star},\\mu^{\\star})\\,d$:\n$$\nd^{\\top}\\,\\nabla_{xx}^{2}L\\,d = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = (0)(0) + (1)(0) = 0\n$$\nSince this is the only possible value for the expression, the infimum is $0$.\n\nThus, the exact value of $m$ is $0$.", "answer": "$$\\boxed{0}$$", "id": "3175817"}, {"introduction": "In our final practice, we transition from analytical pen-and-paper calculations to computational implementation, a crucial step in applying optimization theory to real-world problems. You will develop an algorithm to numerically assess the second-order necessary conditions across a variety of scenarios, translating concepts like the tangent space and Rayleigh quotient into code. This hands-on programming exercise demonstrates how the theoretical framework you've learned underpins the numerical methods used in modern optimization software. [@problem_id:3175936]", "problem": "You are given a set of equality-constrained optimization scenarios of the form: minimize a twice continuously differentiable function at a candidate point, subject to linear equality constraints. The goal is to compute the smallest curvature direction in the tangent space using the Rayleigh quotient and to assess the second-order necessary condition for optimality numerically at each candidate point.\n\nFundamental base for the derivation and computation:\n- For an equality-constrained problem with constraints $A x = b$, the tangent space at a feasible point $x^\\star$ is the set of directions $T = \\{ d \\in \\mathbb{R}^n \\mid A d = 0 \\}$.\n- The Karush–Kuhn–Tucker (KKT) conditions (first-order necessary conditions) for equality constraints require primal feasibility $A x^\\star = b$ and stationarity $\\nabla_x f(x^\\star) + A^\\top \\lambda^\\star = 0$ for some Lagrange multipliers $\\lambda^\\star$.\n- The second-order necessary condition for optimality states that, under suitable regularity (such as linear independence of constraint gradients), the Hessian of the Lagrangian with respect to the decision variables, evaluated at $(x^\\star,\\lambda^\\star)$, is positive semidefinite on the tangent space $T$. For linear equality constraints, the Lagrangian Hessian with respect to $x$ is simply the Hessian of the objective function at $x^\\star$, denoted by $H = \\nabla_{xx}^2 f(x^\\star)$.\n\nWhat to compute:\n- The curvature along a direction $d \\in T$ with $\\|d\\|_2 = 1$ is $d^\\top H d$. The smallest curvature direction in the tangent space is the unit vector $d \\in T$ that minimizes the Rayleigh quotient $d^\\top H d$ subject to $A d = 0$ and $\\|d\\|_2 = 1$.\n- Numerically assess the second-order necessary condition by checking whether the minimum value of the Rayleigh quotient over $T$ is nonnegative and whether the KKT conditions hold.\n\nYour task:\n- For each scenario in the test suite below, implement an algorithm that:\n  1. Verifies primal feasibility by checking $A x^\\star = b$ numerically.\n  2. Verifies KKT stationarity by checking whether there exists $\\lambda$ such that $\\nabla_x f(x^\\star) + A^\\top \\lambda = 0$ numerically. This can be done by solving a least-squares system for $\\lambda$ to minimize $\\|\\nabla_x f(x^\\star) + A^\\top \\lambda\\|_2$.\n  3. Constructs an orthonormal basis of the tangent space $T$ from the null space of $A$.\n  4. Computes the minimum Rayleigh quotient value $\\min_{\\|d\\|_2=1,\\, d \\in T} d^\\top H d$ and the corresponding unit direction $d$ in $T$.\n  5. Outputs, for each scenario, the minimum curvature value rounded to six decimal places and a boolean indicating whether the second-order necessary condition is satisfied. The second-order necessary condition is considered satisfied if both KKT conditions hold and the minimum curvature is greater than or equal to $0$ within a numerical tolerance of $10^{-10}$.\n\nTest suite:\nEach scenario provides the Hessian matrix $H$, constraint matrix $A$, constraint vector $b$, candidate point $x^\\star$, and gradient vector $g = \\nabla_x f(x^\\star)$.\n\n- Scenario 1 (happy path, unconstrained positive definite Hessian):\n  - $H = \\begin{bmatrix}4  0  0 \\\\ 0  2  0 \\\\ 0  0  3\\end{bmatrix}$,\n  - $A$ is the $0 \\times 3$ matrix (no constraints),\n  - $b$ is the empty vector,\n  - $x^\\star = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$,\n  - $g = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$.\n\n- Scenario 2 (boundary case, unconstrained with a zero curvature direction):\n  - $H = \\begin{bmatrix}0  0  0 \\\\ 0  1  0 \\\\ 0  0  2\\end{bmatrix}$,\n  - $A$ is the $0 \\times 3$ matrix (no constraints),\n  - $b$ is the empty vector,\n  - $x^\\star = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$,\n  - $g = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$.\n\n- Scenario 3 (violation case, constrained with negative curvature allowed in tangent):\n  - $H = \\begin{bmatrix}-1  0  0 \\\\ 0  2  0 \\\\ 0  0  3\\end{bmatrix}$,\n  - $A = \\begin{bmatrix}0  1  0 \\\\ 0  0  1\\end{bmatrix}$ (constraints $y = 0$ and $z = 0$),\n  - $b = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n  - $x^\\star = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$,\n  - $g = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$.\n\n- Scenario 4 (constraints remove the negative curvature direction; nonzero gradient with stationarity satisfied via multipliers):\n  - $H = \\begin{bmatrix}-1  0  0 \\\\ 0  0  0 \\\\ 0  0  0\\end{bmatrix}$,\n  - $A = \\begin{bmatrix}1  0  0\\end{bmatrix}$ (constraint $x = 0$),\n  - $b = \\begin{bmatrix}0\\end{bmatrix}$,\n  - $x^\\star = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$,\n  - $g = \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix}$.\n\nNumerical details:\n- Use a tolerance of $10^{-10}$ to assess equality in feasibility and stationarity checks and to decide nonnegativity of the minimum curvature.\n- Construct the tangent space using a numerically stable null space computation.\n- All calculations are purely mathematical; no physical units are involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must alternate between the minimum curvature value (rounded to six decimal places) and the boolean second-order-necessity assessment for each scenario, in the given order. For example, an output with two scenarios would look like: $[\\text{curv}_1,\\text{bool}_1,\\text{curv}_2,\\text{bool}_2]$.", "solution": "We proceed from the fundamental definitions of equality-constrained optimization and the second-order necessary condition for optimality. Consider a twice continuously differentiable objective function with equality constraints $A x = b$ at a candidate point $x^\\star$. The first-order necessary conditions, known as the Karush–Kuhn–Tucker (KKT) conditions, assert the existence of Lagrange multipliers $\\lambda^\\star$ such that primal feasibility $A x^\\star = b$ and stationarity $\\nabla_x f(x^\\star) + A^\\top \\lambda^\\star = 0$ hold. Under regularity conditions like linear independence of constraint gradients, the second-order necessary condition requires the Hessian of the Lagrangian with respect to $x$, evaluated at $(x^\\star,\\lambda^\\star)$, to be positive semidefinite on the tangent space of feasible directions. For linear equality constraints, the Hessian of the Lagrangian with respect to $x$ equals the Hessian of the objective function at $x^\\star$, denoted by $H = \\nabla_{xx}^2 f(x^\\star)$.\n\nThe tangent space $T$ at $x^\\star$ for equality constraints is the set $T = \\{ d \\in \\mathbb{R}^n \\mid A d = 0 \\}$. To test positive semidefiniteness of $H$ on $T$, we analyze the Rayleigh quotient $d^\\top H d$ over unit directions $d \\in T$. The minimum curvature on $T$ is given by minimizing $d^\\top H d$ subject to $A d = 0$ and $\\|d\\|_2 = 1$. Algorithmically, the process can be carried out as follows:\n\n1. Construct an orthonormal basis of the tangent space. Compute an orthonormal basis matrix $Q_T \\in \\mathbb{R}^{n \\times k}$ whose columns span the null space of $A$. This can be obtained via a singular value decomposition of $A$, selecting right singular vectors corresponding to (numerically) zero singular values. The tangent space directions are then represented as $d = Q_T v$ with $\\|v\\|_2 = 1$.\n\n2. Reduce the Rayleigh quotient to the subspace. Substituting $d = Q_T v$, we have:\n   $$d^\\top H d = v^\\top (Q_T^\\top H Q_T) v,$$\n   with the constraint $\\|v\\|_2 = 1$. Therefore, minimizing the Rayleigh quotient over the tangent space is equivalent to finding the minimal eigenvalue of the symmetric matrix $H_T = Q_T^\\top H Q_T$, and the corresponding eigenvector $v_{\\min}$. The smallest-curvature tangent direction is then $d_{\\min} = Q_T v_{\\min}$, normalized to unit length.\n\n3. Check the KKT conditions numerically:\n   - Primal feasibility: verify that $\\|A x^\\star - b\\|_2 \\le \\varepsilon$ with a small tolerance $\\varepsilon$ (we use $\\varepsilon = 10^{-10}$).\n   - Stationarity: verify the existence of multipliers $\\lambda$ by solving the least-squares system $A^\\top \\lambda \\approx -\\nabla_x f(x^\\star)$ and check that the residual norm $\\|A^\\top \\lambda + \\nabla_x f(x^\\star)\\|_2 \\le \\varepsilon$. In the unconstrained case ($A$ has zero rows), stationarity reduces to $\\|\\nabla_x f(x^\\star)\\|_2 \\le \\varepsilon$.\n\n4. Assess the second-order necessary condition: compute the minimum curvature value $\\lambda_{\\min}$ as the smallest eigenvalue of $H_T$ and accept the condition as satisfied if $\\lambda_{\\min} \\ge -\\varepsilon$ and the KKT conditions hold.\n\nApplying this procedure to the test suite:\n- Scenario 1: With no constraints, the tangent space is the whole space. The minimum curvature is the smallest eigenvalue of $H = \\operatorname{diag}(4,2,3)$, namely $2$. KKT conditions hold since $g = 0$ and $A$ is empty. The second-order necessary condition is satisfied.\n- Scenario 2: With no constraints, the tangent space is the whole space. The minimum curvature is the smallest eigenvalue of $H = \\operatorname{diag}(0,1,2)$, namely $0$. KKT holds since $g = 0$. The second-order necessary condition is satisfied at the boundary of positive semidefiniteness.\n- Scenario 3: Constraints $y = 0$ and $z = 0$ reduce the tangent space to the $x$-axis. The curvature along the tangent is determined by the $(1,1)$ entry of $H$, which is $-1$. KKT holds since $g = 0$ and $x^\\star$ is feasible. The minimum curvature is negative, so the second-order necessary condition is not satisfied.\n- Scenario 4: Constraint $x = 0$ removes the negative curvature direction. Although $g \\neq 0$, stationarity holds with $\\lambda^\\star = -1$ solving $A^\\top \\lambda^\\star = -g$. The tangent space comprises directions in the $y$-$z$ plane. The restricted Hessian has zero eigenvalues there, so the minimum curvature is $0$. Both KKT and second-order necessary condition are satisfied.\n\nThe program implements:\n- A numerically stable null space computation via singular value decomposition.\n- Primal feasibility and stationarity checks with a tolerance of $10^{-10}$.\n- Eigenvalue computation on the reduced matrix $H_T$ to obtain the minimum Rayleigh quotient and the smallest-curvature direction (computed internally).\n- Final output as a single list alternating minimum curvature values (rounded to six decimals) and booleans indicating whether the second-order necessary condition is satisfied for each scenario.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef null_space(A: np.ndarray, rtol: float = 1e-12) - np.ndarray:\n    \"\"\"\n    Compute an orthonormal basis for the null space of A using SVD.\n    Returns a matrix Q whose columns form the basis.\n    \"\"\"\n    m, n = A.shape\n    if m == 0:\n        # No constraints: null space is entire R^n\n        return np.eye(n)\n    # SVD of A\n    U, S, Vh = np.linalg.svd(A, full_matrices=True)\n    if S.size == 0:\n        # A is 0xN, handled above, but keep safe guard\n        return np.eye(n)\n    tol = rtol * S.max() if S.size  0 else rtol\n    # Null space corresponds to singular values ~ 0\n    null_mask = (S = tol)\n    # If no singular values are below tol, still find null space via Vh rows beyond rank\n    rank = np.sum(S  tol)\n    # Vh shape is (n, n); null space basis columns are last n - rank rows of Vh (transposed)\n    Q = Vh[rank:].T\n    # Ensure orthonormality (Vh is orthonormal, so Q columns are orthonormal)\n    return Q\n\ndef smallest_curvature_on_tangent(H: np.ndarray, A: np.ndarray):\n    \"\"\"\n    Compute the minimal Rayleigh quotient d^T H d over unit d in null(A),\n    and the corresponding direction d. Returns (lambda_min, d_min).\n    \"\"\"\n    # Construct tangent basis\n    Q = null_space(A, rtol=1e-12)\n    # Project H to tangent subspace\n    H_t = Q.T @ H @ Q\n    if H_t.size == 0:\n        # Tangent is {0}; no directions to check. Return NaN and None.\n        return float('nan'), None\n    # Compute smallest eigenvalue and eigenvector\n    evals, evecs = np.linalg.eigh(H_t)\n    idx_min = np.argmin(evals)\n    lambda_min = float(evals[idx_min])\n    v_min = evecs[:, idx_min]\n    d_min = Q @ v_min\n    # Normalize direction to unit length (should already be unit if Q is orthonormal)\n    norm_d = np.linalg.norm(d_min)\n    if norm_d  0:\n        d_min = d_min / norm_d\n    else:\n        d_min = None\n    return lambda_min, d_min\n\ndef check_primal_feasibility(A: np.ndarray, b: np.ndarray, x: np.ndarray, tol: float = 1e-10) - bool:\n    if A.size == 0 and b.size == 0:\n        return True\n    residual = A @ x - b\n    return np.linalg.norm(residual) = tol\n\ndef check_stationarity(A: np.ndarray, g: np.ndarray, tol: float = 1e-10) - bool:\n    # Solve A^T lambda ≈ -g for lambda in least-squares sense\n    m, n = A.shape\n    if m == 0:\n        # Unconstrained: require g ≈ 0\n        return np.linalg.norm(g) = tol\n    # Least squares solve\n    lambda_hat, residuals, rank, s = np.linalg.lstsq(A.T, -g, rcond=None)\n    stationarity_residual = A.T @ lambda_hat + g\n    return np.linalg.norm(stationarity_residual) = tol\n\ndef assess_second_order_necessity(H: np.ndarray, A: np.ndarray, b: np.ndarray, x: np.ndarray, g: np.ndarray, tol: float = 1e-10):\n    feasible = check_primal_feasibility(A, b, x, tol=tol)\n    stationary = check_stationarity(A, g, tol=tol)\n    lambda_min, d_min = smallest_curvature_on_tangent(H, A)\n    # Condition: lambda_min = -tol and KKT holds\n    soc_ok = feasible and stationary and (lambda_min = -tol if not np.isnan(lambda_min) else True)\n    return lambda_min, soc_ok\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a dict with H, A, b, x, g\n    test_cases = [\n        {\n            \"H\": np.array([[4.0, 0.0, 0.0],\n                           [0.0, 2.0, 0.0],\n                           [0.0, 0.0, 3.0]]),\n            \"A\": np.zeros((0, 3)),\n            \"b\": np.zeros((0,)),\n            \"x\": np.array([0.0, 0.0, 0.0]),\n            \"g\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"H\": np.array([[0.0, 0.0, 0.0],\n                           [0.0, 1.0, 0.0],\n                           [0.0, 0.0, 2.0]]),\n            \"A\": np.zeros((0, 3)),\n            \"b\": np.zeros((0,)),\n            \"x\": np.array([0.0, 0.0, 0.0]),\n            \"g\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"H\": np.array([[-1.0, 0.0, 0.0],\n                           [0.0,  2.0, 0.0],\n                           [0.0,  0.0, 3.0]]),\n            \"A\": np.array([[0.0, 1.0, 0.0],\n                           [0.0, 0.0, 1.0]]),  # y = 0, z = 0\n            \"b\": np.array([0.0, 0.0]),\n            \"x\": np.array([0.0, 0.0, 0.0]),\n            \"g\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"H\": np.array([[-1.0, 0.0, 0.0],\n                           [0.0,  0.0, 0.0],\n                           [0.0,  0.0, 0.0]]),\n            \"A\": np.array([[1.0, 0.0, 0.0]]),  # x = 0\n            \"b\": np.array([0.0]),\n            \"x\": np.array([0.0, 0.0, 0.0]),\n            \"g\": np.array([1.0, 0.0, 0.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        H = case[\"H\"]\n        A = case[\"A\"]\n        b = case[\"b\"]\n        x = case[\"x\"]\n        g = case[\"g\"]\n        lambda_min, soc_ok = assess_second_order_necessity(H, A, b, x, g, tol=1e-10)\n        # Round curvature to 6 decimals and append alternating values\n        # Use formatting to ensure six decimal places\n        results.append(f\"{lambda_min:.6f}\")\n        results.append(str(bool(soc_ok)))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3175936"}]}