{"hands_on_practices": [{"introduction": "Optimization isn't always about smooth, differentiable functions. This exercise explores a \"minimax\" problem, where the goal is to minimize the maximum of several functions, a structure common in fields like engineering and economics for finding designs that perform best under worst-case scenarios. By analyzing this function [@problem_id:3098686], you will uncover a key principle: the optimal solution is often found at a point of equilibrium where competing objectives are perfectly balanced.", "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be defined by $f(x)=\\max\\{(x-1)^{2},\\,(x+1)^{2}\\}$. Using the core definition of the set of minimizers (the argument of the minimum, written $\\operatorname{argmin}$), namely $\\operatorname{argmin}_{x\\in\\mathbb{R}} f=\\{x\\in\\mathbb{R}:\\forall y\\in\\mathbb{R},\\, f(x)\\le f(y)\\}$, determine the exact $\\operatorname{argmin}$ set of $f$. Your reasoning should proceed from first principles: continuity and coercivity of the quadratic functions, the structure of the pointwise maximum of functions, and logical inequalities, rather than any specialized optimization shortcuts. In your explanation, explicitly justify why the minimizer must occur at a point where the two quadratic branches become equal, and clarify the role of equalizing the active branches at the minimizer. Provide the final answer as the exact numerical value $x^{\\star}$ of the unique minimizer (so that the $\\operatorname{argmin}$ set is $\\{x^{\\star}\\}$). No rounding is needed, and no units are required.", "solution": "The problem is to determine the set of minimizers, $\\operatorname{argmin}_{x\\in\\mathbb{R}} f$, for the function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{(x-1)^{2},\\,(x+1)^{2}\\}$. The analysis will be based on first principles as requested.\n\nLet us define two auxiliary functions, $g(x) = (x-1)^{2}$ and $h(x) = (x+1)^{2}$. The function in question is then $f(x) = \\max\\{g(x), h(x)\\}$. Both $g(x)$ and $h(x)$ are quadratic functions, which are continuous and coercive on $\\mathbb{R}$. A function is coercive if its value approaches $+\\infty$ as its argument approaches $\\pm\\infty$. Specifically, as $|x| \\to \\infty$, both $(x-1)^{2} \\to \\infty$ and $(x+1)^{2} \\to \\infty$. Consequently, their pointwise maximum, $f(x)$, is also continuous and coercive on $\\mathbb{R}$. A fundamental theorem in real analysis states that any continuous, coercive function on $\\mathbb{R}$ must attain a global minimum. This guarantees that the set $\\operatorname{argmin}_{x\\in\\mathbb{R}} f$ is non-empty.\n\nTo find the minimum, we must understand the structure of $f(x)$. The function $f(x)$ represents the upper envelope of the two parabolas defined by $g(x)$ and $h(x)$. The behavior of $f(x)$ changes depending on which of the two functions, $g(x)$ or $h(x)$, has a larger value. We determine the crossover points by setting the two functions equal to each other:\n$$g(x) = h(x)$$\n$$(x-1)^{2} = (x+1)^{2}$$\nExpanding both sides gives:\n$$x^{2} - 2x + 1 = x^{2} + 2x + 1$$\nSubtracting $x^{2}+1$ from both sides yields:\n$$-2x = 2x$$\n$$4x = 0$$\n$$x = 0$$\nThis indicates that the two parabolic branches intersect at a single point, $x=0$. At this point, the function value is $f(0) = \\max\\{(0-1)^{2}, (0+1)^{2}\\} = \\max\\{1, 1\\} = 1$.\n\nNow, we analyze which function is greater on either side of $x=0$.\nFor $x > 0$, we have $x+1 > x-1$. Also, $|x+1| > |x-1|$ since $x > 0$. Therefore, $(x+1)^2 > (x-1)^2$. Thus, for $x > 0$, $f(x) = (x+1)^{2}$.\nFor $x < 0$, we have $x-1 < x+1$. Since both are negative if $x < -1$, and one is negative and one is positive if $-1 < x < 0$, it is more robust to consider their squares: $|x-1| = 1-x$ and $|x+1|$ can be $1+x$ or $-1-x$. A simpler way is to analyze the sign of $g(x)-h(x) = -4x$. For $x<0$, $-4x > 0$, which implies $g(x) > h(x)$. Thus, for $x<0$, $f(x) = (x-1)^{2}$.\n\nWe can now express $f(x)$ in a piecewise form:\n$$\nf(x) = \\begin{cases}\n(x-1)^{2} & \\text{if } x < 0 \\\\\n1 & \\text{if } x = 0 \\\\\n(x+1)^{2} & \\text{if } x > 0\n\\end{cases}\n$$\nThis can be written more compactly as:\n$$\nf(x) = \\begin{cases}\n(x-1)^{2} & \\text{if } x \\le 0 \\\\\n(x+1)^{2} & \\text{if } x > 0\n\\end{cases}\n$$\nLet's find the minimum of this function.\nFor the domain $x > 0$, $f(x) = (x+1)^{2}$. This is part of a parabola with vertex at $x = -1$. Over the interval $(0, \\infty)$, this function is strictly increasing. Its values are always greater than its value at $x=0$, i.e., for any $y > 0$, $f(y) = (y+1)^{2} > (0+1)^{2} = 1 = f(0)$.\nFor the domain $x \\le 0$, $f(x) = (x-1)^{2}$. This is part of a parabola with vertex at $x = 1$. Over the interval $(-\\infty, 0]$, this function is strictly decreasing. Its minimum value on this interval is attained at the endpoint $x=0$, where $f(0) = (0-1)^{2} = 1$. For any $y < 0$, we have $y-1 < -1$, and since squaring a negative number reverses the inequality, $(y-1)^{2} > (-1)^{2} = 1$. Thus, for any $y < 0$, $f(y) > f(0)$.\n\nCombining these observations, for any $y \\in \\mathbb{R}$ with $y \\neq 0$, it holds that $f(y) > f(0)$. This satisfies the definition of a unique global minimum. Therefore, the minimizer is $x^{\\star} = 0$, and the set of minimizers is $\\{0\\}$.\n\nThe problem also requires an explicit justification for why the minimizer occurs at the point of equality, $x=0$. Let us assume that a minimizer $x^{\\star}$ exists at a point where $g(x^{\\star}) \\neq h(x^{\\star})$.\nCase 1: $f(x^{\\star}) = g(x^{\\star}) > h(x^{\\star})$. Since $g$ and $h$ are continuous, there exists a neighborhood around $x^{\\star}$ where $g(x) > h(x)$, and thus $f(x)=g(x)=(x-1)^2$ in this neighborhood. For $x^{\\star}$ to be a local minimizer of $f$, it must also be a local minimizer of $g$. The function $g(x)$ is a parabola with a unique global minimizer at its vertex, $x=1$. Therefore, if this case were to hold, we must have $x^{\\star}=1$. Let's evaluate $f(1) = \\max\\{(1-1)^2, (1+1)^2\\} = \\max\\{0, 4\\} = 4$.\nCase 2: $f(x^{\\star}) = h(x^{\\star}) > g(x^{\\star})$. By similar reasoning, $x^{\\star}$ must be the minimizer of $h(x)=(x+1)^2$, which occurs at $x=-1$. Let's evaluate $f(-1) = \\max\\{(-1-1)^2, (-1+1)^2\\} = \\max\\{4, 0\\} = 4$.\n\nWe have already found that $f(0)=1$. Since $1 < 4$, neither $x=1$ nor $x=-1$ can be the global minimizer. The assumption that the minimizer occurs where the two branches are unequal leads to a contradiction. Therefore, the minimizer must occur at the point where the branches are equal, i.e., $g(x^{\\star}) = h(x^{\\star})$, which we found to be $x^{\\star}=0$.\n\nThe role of equalizing the active branches is to find the optimal a trade-off. The function $f(x)$ is minimized by being simultaneously \"as close as possible\" to the vertices of both parabolas ($x=1$ and $x=-1$). Moving away from $x=0$ to the right decreases the value of $(x-1)^2$ (bringing it towards its minimum at $x=1$), but at the cost of increasing $(x+1)^2$, which becomes the dominant term and raises the overall value of $f(x)$. A symmetric argument applies when moving to the left of $x=0$. The point $x=0$ is the unique point of equilibrium where any movement in either direction increases the maximum of the two quadratic values. At $x=0$, both component functions have the same value, $1$. For any $x \\ne 0$, at least one of the component functions will have a value greater than $1$. To see this, $(|x|+1)^2 = |x|^2 + 2|x| + 1 > 1$ for $x \\ne 0$. And $f(x) = \\max\\{(x-1)^2, (x+1)^2\\} = \\max\\{(|x|-1)^2, (|x|+1)^2\\}$ if we exploit symmetry around $0$, which is clearly equal to $(|x|+1)^2$. Hence, $f(x) = (|x|+1)^2 \\ge (0+1)^2 = 1$, with equality if and only if $|x|=0$, i.e., $x=0$.\n\nThe unique minimizer is $x^{\\star} = 0$. The $\\operatorname{argmin}$ set is therefore $\\{0\\}$.", "answer": "$$\\boxed{0}$$", "id": "3098686"}, {"introduction": "While many simple optimization problems yield a single point as the solution, this is not always the case. This practice [@problem_id:3098641] challenges the intuition that a minimizer must be a unique point by exploring a function whose minimal value is achieved across an entire geometric shape. You will see how the algebraic properties of the function $f(x,y)=x^2y^2$ directly translate into a non-trivial, cross-shaped $\\operatorname{argmin}$ set, reinforcing the deep connection between algebraic expressions and the geometry of optimization.", "problem": "Let $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $f(x,y)=x^{2}y^{2}$ and let the feasible set be the closed square $D=[-1,1]^{2}=\\{(x,y)\\in\\mathbb{R}^{2}:-1\\leq x\\leq 1,\\,-1\\leq y\\leq 1\\}$. Using only core definitions from optimization, determine the $\\operatorname{argmin}$ set of $f$ over $D$, and interpret its geometry within $D$ based on first principles. Then, compute the total one-dimensional Euclidean length of this $\\operatorname{argmin}$ set, understood as the sum of the lengths of the line segments that comprise it, counting intersections only once. Express your final answer as an exact number.", "solution": "We begin by recalling the formal definition of the $\\operatorname{argmin}$ set. For a function $f$ defined on a set $D$, the $\\operatorname{argmin}$ of $f$ over $D$ is the set of points in $D$ at which $f$ attains its minimum value. Let $m = \\min_{(x,y) \\in D} f(x,y)$. Then, the $\\operatorname{argmin}$ set, which we denote as $S^*$, is given by:\n$$S^* = \\operatorname{argmin}_{(x,y) \\in D} f(x,y) = \\{(x,y) \\in D \\mid f(x,y) = m\\}$$\n\nThe first step is to determine the minimum value $m$ of the function $f(x,y) = x^2y^2$ on the feasible set $D = [-1,1]^2$. The function can be written as $f(x,y) = (xy)^2$. For any real numbers $x$ and $y$, the terms $x^2$ and $y^2$ are non-negative. That is, $x^2 \\geq 0$ and $y^2 \\geq 0$. Consequently, their product, $f(x,y) = x^2y^2$, must also be non-negative for all $(x,y) \\in \\mathbb{R}^2$, and therefore for all $(x,y) \\in D$.\n$$f(x,y) = x^2y^2 \\geq 0$$\nThe lowest possible value that a non-negative function can take is $0$. We must verify if this value can be attained by $f(x,y)$ for some point $(x,y)$ within the domain $D$. The condition $f(x,y) = 0$ is equivalent to:\n$$x^2y^2 = 0$$\nThis equation holds if and only if $x^2 = 0$ or $y^2 = 0$, which simplifies to $x=0$ or $y=0$.\n\nSo, the function $f(x,y)$ attains the value $0$ at any point $(x,y)$ where at least one of the coordinates is zero. Since we have established that $f(x,y) \\geq 0$ for all points and that the value $0$ is attainable within $D$ (for example, at $(0,0) \\in D$, $f(0,0)=0$), the global minimum value of $f$ over $D$ is $m=0$.\n\nNow we can determine the $\\operatorname{argmin}$ set $S^*$. This is the set of all points $(x,y)$ in the domain $D$ for which $f(x,y) = 0$.\n$$S^* = \\{(x,y) \\in D \\mid f(x,y) = 0\\}$$\nAs determined above, this condition is satisfied if and only if $x=0$ or $y=0$. So, we are looking for the set of points in the square $D = [-1,1]^2$ that have either their $x$-coordinate or their $y$-coordinate equal to zero.\n$$S^* = \\{(x,y) \\in [-1,1]^2 \\mid x=0 \\text{ or } y=0\\}$$\nThis set can be expressed as the union of two subsets:\n1.  The set of points where $x=0$: $S_x = \\{(0,y) \\mid -1 \\leq y \\leq 1\\}$.\n2.  The set of points where $y=0$: $S_y = \\{(x,0) \\mid -1 \\leq x \\leq 1\\}$.\nSo, the $\\operatorname{argmin}$ set is $S^* = S_x \\cup S_y$.\n\nNext, we interpret the geometry of this set. The domain $D$ is a square centered at the origin with side length $2$. The set $S_x$ is the vertical line segment on the $y$-axis from the point $(0,-1)$ to $(0,1)$. The set $S_y$ is the horizontal line segment on the $x$-axis from the point $(-1,0)$ to $(1,0)$. The union of these two segments, $S^*$, forms a cross shape \"+\" centered at the origin, with its arms extending to the boundaries of the square $D$. The two segments intersect at the origin, $(0,0)$.\n\nFinally, we compute the total one-dimensional Euclidean length of the set $S^*$. The problem specifies to sum the lengths of the line segments, counting intersections only once. The set $S^*$ is the union of the segment $S_x$ and the segment $S_y$.\nThe length of the vertical segment $S_x$ from $(0,-1)$ to $(0,1)$ is the Euclidean distance between its endpoints:\n$$L_x = \\sqrt{(0-0)^2 + (1 - (-1))^2} = \\sqrt{0^2 + 2^2} = 2$$\nThe length of the horizontal segment $S_y$ from $(-1,0)$ to $(1,0)$ is:\n$$L_y = \\sqrt{(1 - (-1))^2 + (0 - 0)^2} = \\sqrt{2^2 + 0^2} = 2$$\nThe total length of the union of two sets is the sum of their individual lengths minus the length of their intersection. The intersection of the two segments is the single point where both $x=0$ and $y=0$:\n$$S_x \\cap S_y = \\{(0,0)\\}$$\nA single point has a one-dimensional measure (length) of $0$. Therefore, the total length $L$ of $S^*$ is:\n$$L = L_x + L_y - \\text{Length}(S_x \\cap S_y) = 2 + 2 - 0 = 4$$\nAlternatively, we can view the cross shape as being composed of four distinct line segments of length $1$, all meeting at the origin: the segment from $(-1,0)$ to $(0,0)$, the segment from $(0,0)$ to $(1,0)$, the segment from $(0,-1)$ to $(0,0)$, and the segment from $(0,0)$ to $(0,1)$. The sum of their lengths is $1+1+1+1 = 4$. Both methods yield the same result. The total length is $4$.", "answer": "$$\n\\boxed{4}\n$$", "id": "3098641"}, {"introduction": "The nature of an optimal solution set is profoundly influenced by the geometry of the feasible region. This advanced practice [@problem_id:3098617] contrasts the maximization of a linear function over two fundamental domains: the $\\ell_1$ and $\\ell_2$ unit balls. By dissecting this problem, you will gain a deeper understanding of why $\\ell_1$ optimization promotes sparse solutions—a cornerstone concept in modern machine learning and signal processing—while $\\ell_2$ optimization does not.", "problem": "Let $n \\in \\mathbb{N}$ and let $a \\in \\mathbb{R}^{n}$ be a nonzero vector. Consider the two optimization problems of maximizing the same linear functional over two different unit balls:\n- $A_{1} \\coloneqq \\operatorname{argmax}\\{a^{\\top}x : \\|x\\|_{1} \\leq 1\\}$,\n- $A_{2} \\coloneqq \\operatorname{argmax}\\{a^{\\top}x : \\|x\\|_{2} \\leq 1\\}$,\nwhere $\\|\\cdot\\|_{1}$ denotes the $\\ell_{1}$ norm and $\\|\\cdot\\|_{2}$ denotes the Euclidean norm.\n\nStarting only from the definitions of $\\operatorname{argmax}$, $\\ell_{1}$ and $\\ell_{2}$ norms, the triangle inequality, and the Cauchy–Schwarz inequality, determine the exact forms of the sets $A_{1}$ and $A_{2}$, and identify their affine dimensions (that is, the dimension of the smallest affine subspace containing each set). Let $M \\coloneqq \\max_{1 \\leq i \\leq n} |a_{i}|$ and $J \\coloneqq \\{i \\in \\{1,\\dots,n\\} : |a_{i}| = M\\}$, and denote $k \\coloneqq |J|$. Assume $k \\geq 1$ since $a \\neq 0$.\n\nCompute the quantity $\\Delta \\coloneqq \\dim(A_{1}) - \\dim(A_{2})$ in closed form as a function of $k$. Your final answer must be a single analytic expression in $k$. No rounding is required.", "solution": "We first recall the definitions and fundamental inequalities to be used. For a set $S \\subset \\mathbb{R}^{n}$ and a function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$, $\\operatorname{argmax}_{x \\in S} f(x)$ denotes the set of points $x \\in S$ at which $f$ attains its maximum over $S$. The $\\ell_{1}$ norm is $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, and the $\\ell_{2}$ norm is $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$. We will use the triangle inequality $|u+v| \\leq |u| + |v|$ and the Cauchy–Schwarz inequality $|u^{\\top}v| \\leq \\|u\\|_{2}\\|v\\|_{2}$, together with the elementary bound $\\sum_{i=1}^{n} |a_{i}||x_{i}| \\leq \\left(\\max_{i} |a_{i}|\\right)\\sum_{i=1}^{n} |x_{i}|$.\n\nWe analyze the two problems separately and then compare the resulting argmax sets.\n\nStep $1$: Characterization of $A_{2}$ under the $\\ell_{2}$ constraint. For any $x$ with $\\|x\\|_{2} \\leq 1$, apply the Cauchy–Schwarz inequality:\n$$\na^{\\top}x \\leq \\|a\\|_{2}\\|x\\|_{2} \\leq \\|a\\|_{2}.\n$$\nHence, the optimal value is at most $\\|a\\|_{2}$. Equality in the Cauchy–Schwarz inequality holds if and only if $x$ is a scalar multiple of $a$, that is, $x = \\lambda a$ for some $\\lambda \\in \\mathbb{R}$. To also satisfy $\\|x\\|_{2} \\leq 1$ and achieve the upper bound $\\|a\\|_{2}$, we need $\\|x\\|_{2} = 1$ and $a^{\\top}x = \\|a\\|_{2}\\|x\\|_{2} = \\|a\\|_{2}$. This requires $x$ to be in the same direction as $a$, i.e., $x = a/\\|a\\|_{2}$. Thus,\n$$\nA_{2} = \\left\\{\\frac{a}{\\|a\\|_{2}}\\right\\}.\n$$\nThis set is a singleton, so its affine dimension is $0$, that is, $\\dim(A_{2}) = 0$.\n\nStep $2$: Characterization of $A_{1}$ under the $\\ell_{1}$ constraint. For any $x$ with $\\|x\\|_{1} \\leq 1$, we bound:\n$$\na^{\\top}x = \\sum_{i=1}^{n} a_{i} x_{i} \\leq \\sum_{i=1}^{n} |a_{i}||x_{i}| \\leq \\left(\\max_{1 \\leq i \\leq n} |a_{i}|\\right) \\sum_{i=1}^{n} |x_{i}| \\leq M.\n$$\nThus, the optimal value is at most $M$. We now characterize when equality holds simultaneously in the chain of inequalities:\n- For the first inequality, we require $a_{i}x_{i} = |a_{i}||x_{i}|$ for each $i$, which is equivalent to $x_{i}$ having the same sign as $a_{i}$ (or $x_{i} = 0$).\n- For the second inequality, we require that whenever $|x_{i}| > 0$, it must be that $|a_{i}| = M$; that is, the support of $x$ must be contained in $J = \\{i : |a_{i}| = M\\}$.\n- For the third inequality, we need $\\|x\\|_{1} = 1$.\n\nCombining these conditions, the equality case is exactly the set of $x \\in \\mathbb{R}^{n}$ such that:\n$$\nx_{i} = 0 \\text{ for } i \\notin J,\\quad x_{i} \\text{ has sign } \\operatorname{sign}(a_{i}) \\text{ for } i \\in J,\\quad \\sum_{i \\in J} |x_{i}| = 1.\n$$\nEquivalently, letting $t_{i} \\coloneqq |x_{i}| \\geq 0$ for $i \\in J$, the condition becomes\n$$\n\\sum_{i \\in J} t_{i} = 1,\\quad t_{i} \\geq 0 \\text{ for each } i \\in J,\n$$\nwith $x_{i} = \\operatorname{sign}(a_{i})\\,t_{i}$ for $i \\in J$ and $x_{i} = 0$ for $i \\notin J$. The set $\\{t \\in \\mathbb{R}^{k}_{\\ge 0} : \\sum_{i \\in J} t_{i} = 1\\}$ is the standard simplex of dimension $k-1$, and the map $t \\mapsto x$ that assigns the fixed signs and embeds into $\\mathbb{R}^{n}$ is an affine isomorphism onto $A_{1}$. Therefore,\n$$\n\\dim(A_{1}) = k - 1.\n$$\n\nStep $3$: Compute the difference in affine dimensions. From the above,\n$$\n\\Delta \\coloneqq \\dim(A_{1}) - \\dim(A_{2}) = (k - 1) - 0 = k - 1.\n$$\nThis formula is valid for any nonzero $a$ with $k = |J| \\geq 1$. In the special case $k = 1$, both argmax sets are singletons and $\\Delta = 0$; when $k > 1$, the $\\ell_{1}$ argmax set is a $(k-1)$-dimensional face, while the $\\ell_{2}$ argmax set remains a single point, yielding $\\Delta = k - 1$ as derived.", "answer": "$$\\boxed{k-1}$$", "id": "3098617"}]}