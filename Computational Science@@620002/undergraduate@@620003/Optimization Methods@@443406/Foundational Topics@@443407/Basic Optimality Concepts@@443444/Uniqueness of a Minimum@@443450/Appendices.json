{"hands_on_practices": [{"introduction": "Convexity is a powerful property, but it does not automatically guarantee a unique minimizer. This exercise explores the classic weighted median problem, where the minimum of a sum of weighted absolute differences can be a single point or an entire interval. By using the concept of subgradients on a non-differentiable function, you will derive the conditions for optimality and analyze precisely when the solution ceases to be unique [@problem_id:3196694].", "problem": "Consider the function $f(x) = \\sum_{i=1}^{n} w_i |x - a_i|$ on $\\mathbb{R}$, where the locations $a_1, a_2, \\dots, a_n \\in \\mathbb{R}$ are distinct and ordered so that $a_1  a_2  \\dots  a_n$, and the weights $w_1, w_2, \\dots, w_n$ are nonnegative real numbers. Work from the following foundations: the definition of convexity, the fact that the absolute value function is convex, that a nonnegative weighted sum of convex functions is convex, and the first-order optimality condition for convex functions stating that a point $x^{\\star}$ minimizes a convex function $f$ if and only if $0$ belongs to the subdifferential of $f$ at $x^{\\star}$. Do not assume or use any pre-derived shortcut formulas.\n\n1. Derive a necessary and sufficient condition for a point $x^{\\star}$ to minimize $f$ in terms of the cumulative weights strictly to the left of $x^{\\star}$, strictly to the right of $x^{\\star}$, and exactly at $x^{\\star}$. Express this condition so that it can be interpreted as a weighted median criterion.\n\n2. Specialize to the concrete data $a_1 = 0$, $a_2 = 1$, $a_3 = 2$, $a_4 = 3$ with weights $w_1 = 1$, $w_2 = t$, $w_3 = t$, $w_4 = 2$, where $t  0$ is a parameter. Using the condition from part 1, analyze the uniqueness of minimizers of $f$ as $t$ varies. Identify all $t  0$ for which the minimizer is unique and state where the unique minimizer occurs, and identify all $t  0$ for which the minimizers are not unique, specifying the entire interval of minimizers.\n\n3. Let $t$ be the smallest positive value for which the minimizers are not unique (as determined in part 2). For this value of $t$, compute the midpoint of the minimizer interval. Provide your final answer as a single real number. No rounding is required, and no units are involved.", "solution": "The problem asks for an analysis of the minimizers of the function $f(x) = \\sum_{i=1}^{n} w_i |x - a_i|$. This analysis will be performed in three parts: first, deriving a general optimality condition; second, applying this condition to a specific case to analyze the uniqueness of minimizers; and third, computing a specific value based on this analysis.\n\nThe function $f(x)$ is constructed as a nonnegative weighted sum of convex functions. The absolute value function, $g(u) = |u|$, is convex. Each term $|x - a_i|$ is a simple transformation of the absolute value function and is therefore also convex. Since the weights $w_i$ are nonnegative ($w_i \\ge 0$), the function $f(x) = \\sum_{i=1}^{n} w_i |x - a_i|$ is a nonnegative weighted sum of convex functions, which is itself a convex function.\n\nFor a convex function $f$, a point $x^{\\star}$ is a global minimizer if and only if $0$ is in the subdifferential of $f$ at $x^{\\star}$, denoted $0 \\in \\partial f(x^{\\star})$. This is the first-order optimality condition we are instructed to use.\n\n**1. Derivation of the Optimality Condition**\n\nTo find the subdifferential of $f(x)$, we use the sum rule for subdifferentials, which states that the subdifferential of a sum of convex functions is the Minkowski sum of their individual subdifferentials.\n$$ \\partial f(x) = \\partial \\left(\\sum_{i=1}^{n} w_i |x - a_i|\\right) = \\sum_{i=1}^{n} \\partial(w_i |x - a_i|) $$\nSince each $w_i \\ge 0$, we have $\\partial(w_i |x - a_i|) = w_i \\partial(|x - a_i|)$. The subdifferential of the absolute value function $|u|$ is given by:\n$$ \\partial|u| = \\begin{cases} \\{-1\\}  \\text{if } u  0 \\\\ [-1, 1]  \\text{if } u = 0 \\\\ \\{1\\}  \\text{if } u  0 \\end{cases} $$\nApplying this to each term $|x - a_i|$, we get:\n$$ \\partial|x - a_i| = \\begin{cases} \\{-1\\}  \\text{if } x  a_i \\\\ [-1, 1]  \\text{if } x = a_i \\\\ \\{1\\}  \\text{if } x  a_i \\end{cases} $$\nTo characterize $\\partial f(x^{\\star})$ at a point $x^{\\star}$, we partition the set of indices $\\{1, 2, \\dots, n\\}$ based on the relation between $x^{\\star}$ and $a_i$:\n- Let $L(x^{\\star}) = \\{i \\mid a_i  x^{\\star}\\}$ be the set of indices for points strictly to the left of $x^{\\star}$.\n- Let $R(x^{\\star}) = \\{i \\mid a_i  x^{\\star}\\}$ be the set of indices for points strictly to the right of $x^{\\star}$.\n- Let $C(x^{\\star}) = \\{i \\mid a_i = x^{\\star}\\}$ be the set of indices for points exactly at $x^{\\star}$.\n\nThe subdifferential $\\partial f(x^{\\star})$ is the Minkowski sum:\n$$ \\partial f(x^{\\star}) = \\sum_{i \\in L(x^{\\star})} \\{w_i\\} + \\sum_{i \\in R(x^{\\star})} \\{-w_i\\} + \\sum_{i \\in C(x^{\\star})} [-w_i, w_i] $$\nLet us define the cumulative weights for these sets:\n- $W_L(x^{\\star}) = \\sum_{i \\in L(x^{\\star})} w_i$\n- $W_R(x^{\\star}) = \\sum_{i \\in R(x^{\\star})} w_i$\n- $W_C(x^{\\star}) = \\sum_{i \\in C(x^{\\star})} w_i$\n\nThe Minkowski sum simplifies to an interval:\n$$ \\partial f(x^{\\star}) = [W_L(x^{\\star}) - W_R(x^{\\star}) - W_C(x^{\\star}), W_L(x^{\\star}) - W_R(x^{\\star}) + W_C(x^{\\star})] $$\nThe optimality condition $0 \\in \\partial f(x^{\\star})$ is satisfied if and only if the lower bound of this interval is less than or equal to $0$ and the upper bound is greater than or equal to $0$:\n$$ W_L(x^{\\star}) - W_R(x^{\\star}) - W_C(x^{\\star}) \\le 0 \\quad \\text{and} \\quad 0 \\le W_L(x^{\\star}) - W_R(x^{\\star}) + W_C(x^{\\star}) $$\nThese two inequalities can be combined into a single condition:\n$$ |W_L(x^{\\star}) - W_R(x^{\\star})| \\le W_C(x^{\\star}) $$\nThis is the necessary and sufficient condition for $x^{\\star}$ to be a minimizer. This is a weighted median criterion. If we let $W_{total} = \\sum_{i=1}^n w_i$, the condition is equivalent to requiring that the total weight of points to the left of $x^{\\star}$ is no more than half the total weight, and the total weight of points to the right of $x^{\\star}$ is also no more than half the total weight.\n\n**2. Analysis of the Specific Case**\n\nWe are given $a_1 = 0$, $a_2 = 1$, $a_3 = 2$, $a_4 = 3$ with weights $w_1 = 1$, $w_2 = t$, $w_3 = t$, $w_4 = 2$, where $t  0$. The total weight is $W = 1 + t + t + 2 = 3 + 2t$.\n\nWe use the subdifferential to find the minimizer(s) $x^{\\star}$. The subdifferential $\\partial f(x)$ is a constant value on each open interval $(a_k, a_{k+1})$ and is an interval at each point $a_k$.\nThe subgradient value for $x \\in (a_k, a_{k+1})$ is $g(x) = (\\sum_{i=1}^k w_i) - (\\sum_{i=k+1}^n w_i)$.\n- For $x \\in (-\\infty, 0)$: $g(x) = 0 - (1+t+t+2) = -3-2t$.\n- For $x \\in (0, 1)$: $g(x) = 1 - (t+t+2) = -1-2t$.\n- For $x \\in (1, 2)$: $g(x) = (1+t) - (t+2) = -1$.\n- For $x \\in (2, 3)$: $g(x) = (1+t+t) - 2 = 2t-1$.\n- For $x \\in (3, \\infty)$: $g(x) = (1+t+t+2) - 0 = 3+2t$.\n\nSince $t  0$, the subgradient is strictly negative for all $x  2$. Thus, the minimizer(s) must be at $x^{\\star} \\ge 2$. We analyze the subgradient for $x \\ge 2$.\n\nThe behavior of the function changes depending on the sign of $2t-1$.\nCase 1: $t  \\frac{1}{2}$.\nIn this case, $2t-1  0$. The subgradient is $g(x)=-1$ for $x \\in (1,2)$ and $g(x)=2t-1  0$ for $x \\in (2,3)$. The function decreases up to $x=2$ and increases after $x=2$. Therefore, the minimizer is unique and is located at $x^{\\star}=2$.\nTo verify, let's check the subdifferential at $x^{\\star}=2$: $\\partial f(2)$.\n$W_L(2) = w_1+w_2 = 1+t$. $W_R(2) = w_4=2$. $W_C(2) = w_3=t$.\n$\\partial f(2) = [(1+t)-2-t, (1+t)-2+t] = [-1, 2t-1]$.\nFor $t  \\frac{1}{2}$, $2t-10$, so $0 \\in [-1, 2t-1]$. The condition is satisfied. Since the derivative is strictly negative to the left of $2$ and strictly positive to the right, the minimizer at $x^{\\star}=2$ is unique.\n\nCase 2: $t  \\frac{1}{2}$.\nIn this case, $2t-1  0$. The subgradient is $g(x)=-1$ for $x \\in (1,2)$ and $g(x)=2t-1  0$ for $x \\in (2,3)$. The function is decreasing on the entire interval $(1,3)$. The subgradient becomes positive for $x  3$, where $g(x)=3+2t  0$. The sign change occurs across $x=3$. Thus, the unique minimizer is at $x^{\\star}=3$.\nTo verify, let's check the subdifferential at $x^{\\star}=3$: $\\partial f(3)$.\n$W_L(3) = w_1+w_2+w_3 = 1+2t$. $W_R(3) = 0$. $W_C(3) = w_4=2$.\n$\\partial f(3) = [(1+2t)-0-2, (1+2t)-0+2] = [2t-1, 3+2t]$.\nFor $t  \\frac{1}{2}$, $2t-10$, and for $t0$, $3+2t0$. So $0 \\in [2t-1, 3+2t]$. The condition is satisfied. Since the derivative is strictly negative to the left of $3$ and strictly positive to the right, the minimizer at $x^{\\star}=3$ is unique.\n\nCase 3: $t = \\frac{1}{2}$.\nIn this case, $2t-1=0$. The subgradient is $g(x)=0$ for all $x \\in (2,3)$. This means that the function $f(x)$ is constant on the interval $[2,3]$. Therefore, every point in the interval $[2,3]$ is a minimizer. The minimizers are not unique.\nLet's verify the endpoints.\nAt $x^{\\star}=2$: $\\partial f(2) = [-1, 2t-1] = [-1, 0]$. Since $0 \\in [-1,0]$, $x=2$ is a minimizer.\nAt $x^{\\star}=3$: $\\partial f(3) = [2t-1, 3+2t] = [0, 4]$. Since $0 \\in [0,4]$, $x=3$ is a minimizer.\nSince the derivative is $0$ on $(2,3)$ and the endpoints are minimizers, the entire interval $[2,3]$ is the set of minimizers.\n\nSummary of analysis:\n- For $t \\in (0, \\frac{1}{2})$, the minimizer is unique at $x^{\\star}=3$.\n- For $t = \\frac{1}{2}$, the minimizers are not unique; the set of minimizers is the interval $[2, 3]$.\n- For $t  \\frac{1}{2}$, the minimizer is unique at $x^{\\star}=2$.\n\n**3. Midpoint Calculation**\n\nThe problem asks for the smallest positive value of $t$ for which the minimizers are not unique. From our analysis, there is only one such value, $t=\\frac{1}{2}$. Thus, this is the smallest (and only) such value.\nFor this value $t=\\frac{1}{2}$, the set of minimizers is the interval $[2, 3]$.\nThe midpoint of a closed interval $[a, b]$ is given by the formula $\\frac{a+b}{2}$.\nThe midpoint of the minimizer interval $[2, 3]$ is therefore:\n$$ \\text{Midpoint} = \\frac{2+3}{2} = \\frac{5}{2} $$\nThe value is $2.5$.", "answer": "$$\\boxed{\\frac{5}{2}}$$", "id": "3196694"}, {"introduction": "After seeing how minima can be non-unique, a natural question is how to enforce uniqueness. This practice introduces strong convexity, a key property that guarantees a single global minimizer and improves algorithmic performance. You will start with a least-squares problem that has non-unique solutions and determine the minimal amount of quadratic regularization needed to make the function strongly convex, connecting algebraic analysis of the Hessian matrix to the geometric curvature of the function [@problem_id:3196690].", "problem": "Let $x \\in \\mathbb{R}^{3}$, let\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1\\\\\n0  1  0\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1\\\\\n-1\n\\end{pmatrix},\n$$\nand consider the objective\n$$\nf(x) \\;=\\; \\|A x - b\\|^{2} \\;+\\; \\sum_{i=1}^{3} \\psi_{i}(x_{i}).\n$$\nAssume the separable penalties are initially flat on coordinates $x_{1}$ and $x_{3}$, and quadratic on $x_{2}$:\n$$\n\\psi_{1}(x_{1}) \\equiv 0,\\qquad \\psi_{2}(x_{2}) \\;=\\; \\tfrac{1}{2} x_{2}^{2},\\qquad \\psi_{3}(x_{3}) \\equiv 0.\n$$\nTo guarantee a unique minimizer via strong convexity, you are allowed to add the smallest possible uniform quadratic curvature to the flat coordinates by replacing $\\psi_{1},\\psi_{3}$ with\n$$\n\\psi_{1}^{(t)}(x_{1}) \\;=\\; \\tfrac{t}{2}\\,x_{1}^{2},\\qquad \\psi_{3}^{(t)}(x_{3}) \\;=\\; \\tfrac{t}{2}\\,x_{3}^{2},\n$$\nfor some $t \\ge 0$, leaving $\\psi_{2}$ unchanged. The resulting objective is\n$$\nf_{t}(x) \\;=\\; \\|A x - b\\|^{2} \\;+\\; \\tfrac{t}{2}\\,x_{1}^{2} \\;+\\; \\tfrac{1}{2}\\,x_{2}^{2} \\;+\\; \\tfrac{t}{2}\\,x_{3}^{2}.\n$$\nUsing only fundamental definitions from convex optimization, determine the smallest real number $t \\ge 0$ such that $f_{t}$ is $1$-strongly convex on $\\mathbb{R}^{3}$, which in particular ensures a unique minimizer. Provide your answer as a single real number. No rounding is required.", "solution": "The problem asks for the smallest real number $t \\ge 0$ such that the objective function $f_t(x)$ is $1$-strongly convex.\n\nA twice-differentiable function $g: \\mathbb{R}^n \\to \\mathbb{R}$ is defined as $\\mu$-strongly convex if its Hessian matrix, $\\nabla^2 g(x)$, satisfies the condition $\\nabla^2 g(x) \\succeq \\mu I$ for all $x$ in its domain. Here, $I$ is the identity matrix and the inequality denotes that the matrix $\\nabla^2 g(x) - \\mu I$ is positive semi-definite. This is equivalent to the condition that the smallest eigenvalue of the Hessian, $\\lambda_{\\min}(\\nabla^2 g(x))$, is greater than or equal to $\\mu$.\n\nFor this problem, we must find the smallest $t \\ge 0$ such that $f_t(x)$ is $1$-strongly convex. This requires that the minimum eigenvalue of the Hessian of $f_t(x)$ is at least $1$.\nThe objective function is given by:\n$$f_{t}(x) \\;=\\; \\|A x - b\\|^{2} \\;+\\; \\tfrac{t}{2}\\,x_{1}^{2} \\;+\\; \\tfrac{1}{2}\\,x_{2}^{2} \\;+\\; \\tfrac{t}{2}\\,x_{3}^{2}$$\nThe term $\\|A x - b\\|^{2}$ can be written as $(Ax - b)^T(Ax-b)$, which expands to $x^T A^T A x - 2b^T A x + b^T b$. The function $f_t(x)$ can be written in quadratic form. Let us define a diagonal matrix $D_t$ that incorporates the penalty terms:\n$$D_t \\;=\\; \\begin{pmatrix} t  0  0 \\\\ 0  1  0 \\\\ 0  0  t \\end{pmatrix}$$\nThe term with the penalties can be expressed as $\\frac{1}{2}x^T D_t x$. Thus, the objective function is:\n$$f_t(x) \\;=\\; x^T A^T A x - 2b^T A x + b^T b + \\frac{1}{2}x^T D_t x$$\nThis is a quadratic function of $x$. For a quadratic function of the form $\\frac{1}{2}x^T Q x + c^T x + d$, the Hessian is the constant matrix $Q$. We can rewrite $f_t(x)$ to match this form:\n$$f_t(x) \\;=\\; \\frac{1}{2} x^T (2A^T A + D_t) x - 2b^T A x + b^T b$$\nThe Hessian matrix of $f_t(x)$, which we denote as $H_t$, is constant and given by:\n$$H_t = \\nabla^2 f_t(x) = 2A^T A + D_t$$\nFirst, we compute the matrix $A^T A$:\n$$A^T A \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  1\\\\ 0  1  0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix}$$\nNow, we construct the Hessian matrix $H_t$:\n$$H_t = 2 \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix} + \\begin{pmatrix} t  0  0 \\\\ 0  1  0 \\\\ 0  0  t \\end{pmatrix} \\;=\\; \\begin{pmatrix} 2  0  2 \\\\ 0  2  0 \\\\ 2  0  2 \\end{pmatrix} + \\begin{pmatrix} t  0  0 \\\\ 0  1  0 \\\\ 0  0  t \\end{pmatrix}$$\n$$H_t \\;=\\; \\begin{pmatrix} 2+t  0  2 \\\\ 0  3  0 \\\\ 2  0  2+t \\end{pmatrix}$$\nTo find the eigenvalues, we solve the characteristic equation $\\det(H_t - \\lambda I) = 0$:\n$$\\det \\begin{pmatrix} 2+t-\\lambda  0  2 \\\\ 0  3-\\lambda  0 \\\\ 2  0  2+t-\\lambda \\end{pmatrix} \\;=\\; 0$$\nWe can use cofactor expansion along the second row:\n$$(3-\\lambda) \\det \\begin{pmatrix} 2+t-\\lambda  2 \\\\ 2  2+t-\\lambda \\end{pmatrix} \\;=\\; 0$$\n$$(3-\\lambda) \\left[ (2+t-\\lambda)^2 - 2^2 \\right] \\;=\\; 0$$\nThis equation gives us the eigenvalues. One eigenvalue is clearly $\\lambda_1 = 3$. The others are found by solving:\n$$(2+t-\\lambda)^2 - 4 \\;=\\; 0$$\n$$(2+t-\\lambda)^2 \\;=\\; 4$$\n$$2+t-\\lambda \\;=\\; \\pm 2$$\nThis leads to two more eigenvalues:\n$$2+t-\\lambda = 2 \\implies \\lambda_2 = t$$\n$$2+t-\\lambda = -2 \\implies \\lambda_3 = t+4$$\nThe set of eigenvalues of $H_t$ is $\\{3, t, t+4\\}$.\n\nFor $f_t(x)$ to be $1$-strongly convex, all eigenvalues of its Hessian must be greater than or equal to $1$.\nThis gives us a system of inequalities:\n1. $3 \\ge 1$\n2. $t \\ge 1$\n3. $t+4 \\ge 1$\n\nThe first inequality is always true. The third inequality simplifies to $t \\ge -3$. The problem statement also specifies that $t \\ge 0$.\nWe must satisfy all these conditions simultaneously: $t \\ge 1$, $t \\ge -3$, and $t \\ge 0$.\nThe intersection of these conditions is $t \\ge 1$.\nThe problem asks for the smallest real number $t$ that satisfies this condition. The smallest value of $t$ in the set $\\{t \\in \\mathbb{R} \\mid t \\ge 1\\}$ is $1$.\nTherefore, the smallest value of $t$ that guarantees $1$-strong convexity is $1$.", "answer": "$$\\boxed{1}$$", "id": "3196690"}, {"introduction": "The conditions for a unique minimum depend critically on the structure of the objective function. This exercise challenges you to compare and contrast two of the most important problems in optimization and data science: minimizing the $\\ell_2$-norm squared versus the $\\ell_1$-norm of the residuals. By analyzing how linear algebra (the rank of a matrix) and geometry (the shape of level sets) interact, you will develop a deeper intuition for why uniqueness is guaranteed under simpler conditions in one case but is more subtle in the other [@problem_id:3196755].", "problem": "Consider the unconstrained minimization problems in $\\mathbb{R}^n$ with data matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$:\n- $f_1(x) = \\lVert Ax - b \\rVert_1$,\n- $f_2(x) = \\lVert Ax - b \\rVert_2^2$.\nYou are asked to compare conditions that ensure uniqueness of a minimizer for each problem, and to connect these conditions to geometric intuition via level sets. Rely only on fundamental facts such as definitions of convexity, strict convexity, positive definiteness, subgradients of norms, and basic linear algebra (e.g., column rank and null space). Select all statements that are correct.\n\nA. For $f_2(x) = \\lVert Ax - b \\rVert_2^2$, the minimizer is unique if and only if $A$ has full column rank.\n\nB. For $f_1(x) = \\lVert Ax - b \\rVert_1$, full column rank of $A$ is necessary for uniqueness of the minimizer, but it is not sufficient to guarantee uniqueness for every $b$.\n\nC. For $f_1(x) = \\lVert Ax - b \\rVert_1$, if there exists $x^\\star$ with $Ax^\\star = b$ and $A$ has full column rank, then $x^\\star$ is the unique minimizer.\n\nD. The level sets $\\{x : \\lVert Ax - b \\rVert_2^2 = c\\}$ are (possibly degenerate) ellipsoids in $x$; when $A$ has full column rank they are strictly convex, which geometrically supports uniqueness of the minimizer.\n\nE. The level sets $\\{x : \\lVert Ax - b \\rVert_1 = c\\}$ are strictly convex whenever $A$ has full column rank, hence uniqueness follows from full column rank in the $\\ell_1$ case as well.\n\nChoose all that apply.", "solution": "We begin from core definitions and linear algebraic facts.\n\n- A function $g:\\mathbb{R}^n \\to \\mathbb{R}$ is strictly convex if for any distinct $x,y$ and any $\\theta \\in (0,1)$, $g(\\theta x + (1-\\theta)y)  \\theta g(x) + (1-\\theta)g(y)$. A strictly convex function over a convex domain has at most one minimizer.\n- A twice differentiable function with Hessian $H(x) \\succeq 0$ everywhere is convex; if $H(x) \\succ 0$ on the entire domain, it is strictly (indeed strongly) convex, ensuring a unique minimizer.\n- If a function $g$ depends on $x$ only through $Ax$ and $\\operatorname{null}(A) \\neq \\{0\\}$, then $g(x) = g(x + d)$ for any $d \\in \\operatorname{null}(A)$; hence uniqueness is impossible.\n- The subdifferential of the $\\ell_1$ norm is given by $\\partial \\lVert r \\rVert_1 = \\{s \\in \\mathbb{R}^m : s_i = \\operatorname{sign}(r_i)$ if $r_i \\neq 0$, and $s_i \\in [-1,1]$ if $r_i = 0\\}$. For $g(x) = \\lVert Ax - b \\rVert_1$, first-order optimality is $0 \\in \\partial g(x) = A^\\top \\partial \\lVert Ax - b \\rVert_1$.\n\nNow we address each statement.\n\nAnalysis for $f_2(x) = \\lVert Ax - b \\rVert_2^2$ (Options A and D):\n- The function $f_2$ is differentiable with gradient $\\nabla f_2(x) = 2A^\\top(Ax - b)$ and Hessian $\\nabla^2 f_2(x) = 2A^\\top A$. The Hessian is positive semidefinite for any $A$, and is positive definite if and only if $A$ has full column rank (equivalently, $A^\\top A \\succ 0$). When $A$ has full column rank, $f_2$ is strictly (indeed strongly) convex, which guarantees a unique minimizer. Conversely, if $A$ is rank-deficient, there exists $d \\neq 0$ with $Ad = 0$, hence $f_2(x + d) = \\lVert A(x + d) - b \\rVert_2^2 = \\lVert Ax - b \\rVert_2^2 = f_2(x)$; therefore uniqueness cannot hold. This proves the “if and only if” in Option A.\n- The level sets of $f_2$ are $\\{x : \\lVert Ax - b \\rVert_2^2 = c\\}$, which are preimages of Euclidean spheres under the linear map $x \\mapsto Ax - b$. When $A$ has full column rank, these sets are ellipsoids (strictly convex closed surfaces); if $A$ is rank-deficient, the level sets become cylindrical (non-strictly convex) along directions in $\\operatorname{null}(A)$. This geometric picture aligns with the uniqueness characterization. Hence Option D is correct.\n\nAnalysis for $f_1(x) = \\lVert Ax - b \\rVert_1$ (Options B, C, E):\n- Necessity of full column rank for uniqueness: If $\\operatorname{null}(A) \\neq \\{0\\}$, then for any minimizer $x^\\star$ and any $d \\in \\operatorname{null}(A)$ with $d \\neq 0$, we have $f_1(x^\\star + d) = \\lVert A(x^\\star + d) - b \\rVert_1 = \\lVert Ax^\\star - b \\rVert_1 = f_1(x^\\star)$, so uniqueness is impossible. Thus full column rank is necessary. However, full column rank is not sufficient to guarantee uniqueness for every $b$. A concrete example shows this:\n  - Take $n = 1$, $m = 2$, $A = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ (which has full column rank), and $b = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Then $f_1(x) = |x - 0| + |x - 1|$. The set of minimizers is the interval $[0,1]$ (any median of the two points), so the minimizer is not unique. Therefore Option B is correct.\n- Sufficient condition ensuring uniqueness in a particular case: If there exists $x^\\star$ such that $Ax^\\star = b$ and $A$ has full column rank, then for any $x \\neq x^\\star$ we have $Ax \\neq b$ (injectivity of $A$ on $\\mathbb{R}^n$), and hence $\\lVert Ax - b \\rVert_1  0 = \\lVert Ax^\\star - b \\rVert_1$. Thus $x^\\star$ is the unique global minimizer of $f_1$. Therefore Option C is correct.\n- Level sets and lack of strict convexity for the $\\ell_1$ objective: The sets $\\{r : \\lVert r \\rVert_1 = c\\}$ in residual space are cross-polytopes (polytopes with flat faces and vertices), not strictly convex. Their preimages $\\{x : \\lVert Ax - b \\rVert_1 = c\\}$ in $x$ inherit flat features induced by $A$, regardless of full column rank. Therefore full column rank of $A$ does not make $\\ell_1$ level sets strictly convex, and it does not, by itself, ensure uniqueness. Hence Option E is incorrect.\n\nVerdicts:\n- Option A: Correct.\n- Option B: Correct.\n- Option C: Correct.\n- Option D: Correct.\n- Option E: Incorrect.\n\nGeometric intuition summary tying both problems together:\n- For $f_2$, when $A$ has full column rank, ellipsoidal (strictly convex) level sets intersect in at most one point at the minimum; if $A$ is rank-deficient, cylinders along $\\operatorname{null}(A)$ create flat directions and prevent uniqueness.\n- For $f_1$, level sets are polytopal (with flat faces) irrespective of the rank; even with full column rank, the intersection of a polytope’s faces with the affine image of $\\mathbb{R}^n$ can produce non-unique minimizers, unless additional problem-specific conditions (such as exact fit $Ax = b$ or suitable subgradient strict complementarity) hold.", "answer": "$$\\boxed{ABCD}$$", "id": "3196755"}]}