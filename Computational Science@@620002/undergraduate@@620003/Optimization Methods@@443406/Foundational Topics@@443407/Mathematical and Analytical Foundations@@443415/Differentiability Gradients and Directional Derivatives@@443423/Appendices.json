{"hands_on_practices": [{"introduction": "Many functions in optimization and machine learning are not smooth everywhere; they are often constructed piecewise, like the famous ReLU activation or hinge loss. This first exercise provides essential practice in handling such functions. By analyzing a function defined with a `max` operator, you will apply the fundamental definitions of the gradient and directional derivative to determine where the function is differentiable and what happens at the boundary between its different behaviors [@problem_id:3120157]. This will sharpen your skills in moving from a function's formula to a precise understanding of its local properties.", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{0, \\|x\\|-1\\}^{2}$, where $\\|x\\|$ denotes the Euclidean norm. Use only the core definitions from multivariable calculus and optimization methods: differentiability at a point $x_{0}$ means there exists a linear map $L$ such that $f(x_{0}+h)=f(x_{0})+L(h)+o(\\|h\\|)$ as $h\\to 0$, the gradient $\\nabla f(x)$ is the unique vector representing $L$ via $L(h)=\\nabla f(x)^{\\top}h$, and the directional derivative of $f$ at $x$ in direction $d$ is $D f(x;d)=\\lim_{t\\to 0}\\frac{f(x+td)-f(x)}{t}$ when this limit exists. Starting from these definitions and well-tested formulas (including the chain rule and the differentiability of the Euclidean norm at all $x\\neq 0$), investigate the differentiability of $f$ at the boundary points satisfying $\\|x\\|=1$, and derive explicit expressions for the gradient inside the unit ball ($\\|x\\|1$) and outside the unit ball ($\\|x\\|1$). Then compute the directional derivative at an arbitrary boundary point $x_{0}$ with $\\|x_{0}\\|=1$ in an arbitrary direction $d\\in\\mathbb{R}^{n}$. Provide your final results in the following order: (i) the gradient for $\\|x\\|1$, (ii) the gradient for $\\|x\\|1$, (iii) the directional derivative at a boundary point with $\\|x\\|=1$ in any direction $d$. The final answer must be an exact analytic expression; do not perform any numerical rounding.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Function definition: $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ is given by $f(x)=\\max\\{0, \\|x\\|-1\\}^{2}$.\n- Notation: $\\|x\\|$ represents the Euclidean norm of the vector $x \\in \\mathbb{R}^n$.\n- Core definitions:\n    - Differentiability at $x_{0}$: Existence of a linear map $L$ such that $f(x_{0}+h)=f(x_{0})+L(h)+o(\\|h\\|)$ as $h\\to 0$.\n    - Gradient $\\nabla f(x)$: The unique vector representing $L$ via the inner product, $L(h)=\\nabla f(x)^{\\top}h$.\n    - Directional derivative: $D f(x;d)=\\lim_{t\\to 0}\\frac{f(x+td)-f(x)}{t}$, when the limit exists.\n- Assumed knowledge: Chain rule and the differentiability of the Euclidean norm for all $x \\neq 0$.\n- Tasks:\n    1. Investigate the differentiability of $f$ at boundary points where $\\|x\\|=1$.\n    2. Derive the gradient for $\\|x\\|1$.\n    3. Derive the gradient for $\\|x\\|1$.\n    4. Compute the directional derivative at a point $x_0$ with $\\|x_0\\|=1$ in an arbitrary direction $d\\in\\mathbb{R}^{n}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard exercise in multivariable calculus and non-smooth optimization. The function $f(x)$ is a well-defined composition of standard functions (norm, subtraction, max, square). No scientific or factual unsoundness is present.\n- **Well-Posed:** The problem is clearly stated and asks for specific mathematical objects (gradients and a directional derivative) under well-defined conditions. A unique solution exists.\n- **Objective:** The language is formal and unambiguous. All terms are standard in mathematics.\n- **Completeness and Consistency:** The problem is self-contained. The provided definitions and assumed knowledge are sufficient for its resolution. There are no internal contradictions.\n- **No other flaws are identified.** The problem does not rely on metaphors, is relevant to the specified topic, is not trivial, and is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe function is $f(x) = \\max\\{0, \\|x\\|-1\\}^2$. We analyze the function in three distinct regions of $\\mathbb{R}^n$.\n\n**Part (i): Gradient for $\\|x\\|1$**\n\nFor any $x$ in the open unit ball, i.e., $\\|x\\|1$, the term $\\|x\\|-1$ is negative. The definition of the maximum function implies $\\max\\{0, \\|x\\|-1\\} = 0$.\nThus, for all $x$ such that $\\|x\\|1$, the function simplifies to:\n$$f(x) = 0^2 = 0$$\nThe function $f(x)$ is constant on the open set $\\{x \\in \\mathbb{R}^n \\mid \\|x\\|1\\}$. The derivative of a constant function is the zero map, and its corresponding gradient is the zero vector.\nTherefore, for $\\|x\\|1$, the gradient is:\n$$\\nabla f(x) = 0$$\n\n**Part (ii): Gradient for $\\|x\\|1$**\n\nFor any $x$ outside the closed unit ball, i.e., $\\|x\\|1$, the term $\\|x\\|-1$ is positive. The definition of the maximum function implies $\\max\\{0, \\|x\\|-1\\} = \\|x\\|-1$.\nThus, for all $x$ such that $\\|x\\|1$, the function simplifies to:\n$$f(x) = (\\|x\\|-1)^2$$\nThis function is a composition $f(x) = g(h(x))$, where $g(u) = u^2$ and $h(x) = \\|x\\|-1$. Both functions are differentiable in their respective domains (for $h(x)$, we require $x \\neq 0$, which is satisfied since $\\|x\\|1$). We apply the chain rule for gradients: $\\nabla f(x) = g'(h(x)) \\nabla h(x)$.\n\nThe derivative of $g(u)$ is $g'(u) = 2u$. Evaluated at $h(x)$, this gives $g'(h(x)) = 2(\\|x\\|-1)$.\nThe gradient of $h(x)$ is $\\nabla h(x) = \\nabla(\\|x\\|-1) = \\nabla(\\|x\\|)$. The gradient of the Euclidean norm is a standard result: $\\nabla(\\|x\\|) = \\frac{x}{\\|x\\|}$ for $x \\neq 0$.\n\nCombining these results, the gradient of $f(x)$ for $\\|x\\|1$ is:\n$$\\nabla f(x) = 2(\\|x\\|-1) \\frac{x}{\\|x\\|}$$\nThis can also be written as $\\nabla f(x) = 2\\left(1 - \\frac{1}{\\|x\\|}\\right)x$.\n\n**Part (iii): Differentiability and Directional Derivative for $\\|x_0\\|=1$**\n\nLet $x_0$ be a point on the boundary of the unit ball, so $\\|x_0\\|=1$. At this point, $f(x_0) = \\max\\{0, \\|x_0\\|-1\\}^2 = \\max\\{0, 1-1\\}^2 = 0$.\n\nFirst, we investigate differentiability. A function is differentiable at a point if its partial derivatives exist and are continuous in a neighborhood of that point.\nFrom parts (i) and (ii), the gradient is piecewise defined:\n$$ \\nabla f(x) = \\begin{cases} 0  \\text{if } \\|x\\|  1 \\\\ 2\\left(1 - \\frac{1}{\\|x\\|}\\right)x  \\text{if } \\|x\\|  1 \\end{cases} $$\nLet's check the limit of the gradient as $x \\to x_0$ where $\\|x_0\\|=1$.\nFrom the interior region ($\\|x\\|1$): $\\lim_{x\\to x_0, \\|x\\|1} \\nabla f(x) = \\lim_{x\\to x_0, \\|x\\|1} 0 = 0$.\nFrom the exterior region ($\\|x\\|1$): $\\lim_{x\\to x_0, \\|x\\|1} \\nabla f(x) = \\lim_{x\\to x_0, \\|x\\|1} 2\\left(1 - \\frac{1}{\\|x\\|}\\right)x = 2\\left(1 - \\frac{1}{\\|x_0\\|}\\right)x_0 = 2\\left(1-\\frac{1}{1}\\right)x_0 = 0$.\nSince the limits from both sides agree and are equal to the zero vector, we can define $\\nabla f(x_0) = 0$. The resulting gradient function $\\nabla f(x)$ is continuous for all $x \\neq 0$. The continuity of the partial derivatives implies that the function $f$ is continuously differentiable ($C^1$) for all $x \\neq 0$. This includes all points $x_0$ on the boundary $\\|x_0\\|=1$. Therefore, $f$ is differentiable at these points, and its gradient is $\\nabla f(x_0) = 0$.\n\nNow, we compute the directional derivative $D f(x_0; d)$ for an arbitrary direction $d \\in \\mathbb{R}^n$. Since $f$ is differentiable at $x_0$, the directional derivative is given by the dot product of the gradient and the direction vector:\n$$D f(x_0; d) = (\\nabla f(x_0))^{\\top}d$$\nSubstituting the gradient we found, $\\nabla f(x_0) = 0$:\n$$D f(x_0; d) = 0^{\\top}d = 0$$\nThis holds for any direction $d \\in \\mathbb{R}^n$.\n\nAs a verification, we can compute this directly from the limit definition:\n$D f(x_0; d) = \\lim_{t\\to 0} \\frac{f(x_0+td) - f(x_0)}{t} = \\lim_{t\\to 0} \\frac{\\max\\{0, \\|x_0+td\\|-1\\}^2}{t}$.\nWe use the expansion $\\|x_0+td\\| = \\sqrt{\\|x_0\\|^2 + 2t(x_0^\\top d) + t^2\\|d\\|^2} = \\sqrt{1 + 2t(x_0^\\top d) + t^2\\|d\\|^2}$.\nUsing the Taylor expansion $\\sqrt{1+u} = 1 + \\frac{1}{2}u + O(u^2)$ for small $u$, we have:\n$\\|x_0+td\\| = 1 + \\frac{1}{2}(2t(x_0^\\top d) + t^2\\|d\\|^2) + O(t^2) = 1 + t(x_0^\\top d) + O(t^2)$.\nSo, $\\|x_0+td\\|-1 = t(x_0^\\top d) + O(t^2)$.\nThe limit becomes:\n$D f(x_0; d) = \\lim_{t\\to 0} \\frac{\\max\\{0, t(x_0^\\top d) + O(t^2)\\}^2}{t}$.\nThe numerator is of order $O(t^2)$, as $\\max\\{0, \\dots\\}^2$ will be either $0$ or $(t(x_0^\\top d) + O(t^2))^2 = O(t^2)$.\nThe expression inside the limit is therefore $\\frac{O(t^2)}{t} = O(t)$.\nAs $t \\to 0$, $O(t) \\to 0$. This confirms that the directional derivative is $0$.\n\nThe final results are:\n(i) For $\\|x\\|1$, $\\nabla f(x) = 0$.\n(ii) For $\\|x\\|1$, $\\nabla f(x) = 2(\\|x\\|-1)\\frac{x}{\\|x\\|}$.\n(iii) At $\\|x_0\\|=1$, for any direction $d$, $D f(x_0; d) = 0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  2(\\|x\\|-1)\\frac{x}{\\|x\\|}  0\n\\end{pmatrix}\n}\n$$", "id": "3120157"}, {"introduction": "With a solid grasp of differentiability, we now turn to a cornerstone of optimization: the linear least squares problem. This exercise demonstrates that the gradient and directional derivatives are not just computational tools, but also powerful lenses for geometric analysis. By connecting the gradient's direction to the underlying structure of the problem via the Singular Value Decomposition (SVD), you will uncover why gradient-based methods can struggle with so-called \"ill-conditioned\" problems [@problem_id:3120177]. This practice offers a deep insight into the performance of one of the most fundamental optimization algorithms.", "problem": "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank and an observed vector $b \\in \\mathbb{R}^{m}$. Define the objective function $f:\\mathbb{R}^{n} \\to \\mathbb{R}$ by $f(x) = \\|A x - b\\|_{2}^{2}$. Let the singular value decomposition (SVD, singular value decomposition) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with strictly positive diagonal entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}  0$. Denote the right singular vectors by $v_{i} \\in \\mathbb{R}^{n}$ (the columns of $V$) and the left singular vectors by $u_{i} \\in \\mathbb{R}^{m}$ (the columns of $U$). Let $x_{0} \\in \\mathbb{R}^{n}$ be a current iterate and define the residual $r = A x_{0} - b$. Let $c = U^{\\top} r \\in \\mathbb{R}^{m}$ so that $c_{i} = u_{i}^{\\top} r$ for $i = 1, \\dots, m$. Assume $A$ is ill-conditioned, meaning $\\sigma_{1} / \\sigma_{n}$ is large.\n\n1) Starting from the definition of the directional derivative,\n$$\nD_{v} f(x_{0}) = \\lim_{t \\to 0} \\frac{f(x_{0} + t v) - f(x_{0})}{t},\n$$\nderive a formula for $D_{v_{i}} f(x_{0})$ for each $i \\in \\{1, \\dots, n\\}$ in terms of $\\sigma_{i}$ and $c_{i}$.\n\n2) Using only orthogonality properties and basic rules of vector calculus, derive the cosine of the angle between the steepest descent direction at $x_{0}$, which is the direction $- \\nabla f(x_{0})$, and a right singular vector $v_{k}$ for a fixed $k \\in \\{1, \\dots, n\\}$. Express your result in terms of $\\{\\sigma_{j}\\}_{j=1}^{n}$ and $\\{c_{j}\\}_{j=1}^{n}$.\n\n3) Based on your expression in part 2, briefly explain how ill-conditioning (that is, widely varying $\\sigma_{j}$) affects which right singular vectors $v_{k}$ the initial gradient descent trajectory aligns with most strongly, assuming $c_{j} \\neq 0$ for all $j$.\n\nFor your final answer, report your closed-form expression from part 2 for the cosine as a function of $k$, $\\{\\sigma_{j}\\}_{j=1}^{n}$, and $\\{c_{j}\\}_{j=1}^{n}$. No numerical evaluation is required.", "solution": "The problem statement has been rigorously validated and is found to be self-contained, mathematically sound, and well-posed. It presents a standard exercise in the analysis of optimization algorithms for linear least squares. We may proceed with the solution.\n\nThe problem asks for three related derivations concerning the objective function $f(x) = \\|A x - b\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ has SVD $A = U \\Sigma V^{\\top}$.\n\nFirst, we derive a formula for the directional derivative of $f$ at a point $x_{0}$ in the direction of a right singular vector $v_{i}$. The directional derivative $D_{v_{i}} f(x_{0})$ is defined as:\n$$\nD_{v_{i}} f(x_{0}) = \\lim_{t \\to 0} \\frac{f(x_{0} + t v_{i}) - f(x_{0})}{t}\n$$\nWe begin by expanding the term $f(x_{0} + t v_{i})$:\n$$\nf(x_{0} + t v_{i}) = \\|A(x_{0} + t v_{i}) - b\\|_{2}^{2} = \\|(A x_{0} - b) + t A v_{i}\\|_{2}^{2}\n$$\nUsing the definition of the residual, $r = A x_{0} - b$, we have:\n$$\nf(x_{0} + t v_{i}) = \\|r + t A v_{i}\\|_{2}^{2}\n$$\nExpanding the squared L2-norm, which is equivalent to the dot product of the vector with itself:\n$$\n\\|r + t A v_{i}\\|_{2}^{2} = (r + t A v_{i})^{\\top}(r + t A v_{i}) = r^{\\top}r + 2t r^{\\top}(A v_{i}) + t^{2} (A v_{i})^{\\top}(A v_{i})\n$$\nThis can be written in terms of norms as:\n$$\nf(x_{0} + t v_{i}) = \\|r\\|_{2}^{2} + 2t r^{\\top}(A v_{i}) + t^{2} \\|A v_{i}\\|_{2}^{2}\n$$\nGiven that $f(x_{0}) = \\|A x_{0} - b\\|_{2}^{2} = \\|r\\|_{2}^{2}$, we can form the difference quotient:\n$$\n\\frac{f(x_{0} + t v_{i}) - f(x_{0})}{t} = \\frac{(\\|r\\|_{2}^{2} + 2t r^{\\top}(A v_{i}) + t^{2} \\|A v_{i}\\|_{2}^{2}) - \\|r\\|_{2}^{2}}{t} = 2 r^{\\top}(A v_{i}) + t \\|A v_{i}\\|_{2}^{2}\n$$\nTaking the limit as $t \\to 0$:\n$$\nD_{v_{i}} f(x_{0}) = \\lim_{t \\to 0} (2 r^{\\top}(A v_{i}) + t \\|A v_{i}\\|_{2}^{2}) = 2 r^{\\top}(A v_{i})\n$$\nTo express this in the desired terms, we use the SVD of $A$. From $A = U \\Sigma V^{\\top}$, we can write $A V = U \\Sigma$. Since the columns of $V$ are the right singular vectors $v_{j}$, the $i$-th column of the matrix equation gives $A v_{i} = \\sigma_{i} u_{i}$, where $u_{i}$ is the $i$-th left singular vector (the $i$-th column of $U$) and $\\sigma_{i}$ is the corresponding singular value.\nSubstituting this into our expression for the directional derivative:\n$$\nD_{v_{i}} f(x_{0}) = 2 r^{\\top}(\\sigma_{i} u_{i}) = 2 \\sigma_{i} (r^{\\top} u_{i})\n$$\nBy definition, the component $c_{i}$ of the vector $c = U^{\\top} r$ is $c_{i} = u_{i}^{\\top} r$. Since the dot product is symmetric ($u_{i}^{\\top} r = r^{\\top} u_{i}$), we arrive at the final expression for the first part:\n$$\nD_{v_{i}} f(x_{0}) = 2 \\sigma_{i} c_{i}\n$$\n\nSecond, we derive the cosine of the angle between the steepest descent direction at $x_{0}$, which is $d = - \\nabla f(x_{0})$, and a right singular vector $v_{k}$. The cosine of the angle $\\theta_{k}$ between two non-zero vectors $p$ and $q$ is given by $\\cos(\\theta_{k}) = \\frac{p^{\\top}q}{\\|p\\|_{2}\\|q\\|_{2}}$. Here, $p = d = -\\nabla f(x_{0})$ and $q = v_{k}$.\n\nFirst, we find the gradient of $f(x) = \\|Ax - b\\|_{2}^{2} = (Ax-b)^{\\top}(Ax-b)$.\n$$\nf(x) = (x^{\\top}A^{\\top} - b^{\\top})(Ax-b) = x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b\n$$\nThe gradient of this quadratic form with respect to $x$ is:\n$$\n\\nabla f(x) = 2 A^{\\top} A x - 2 A^{\\top} b = 2 A^{\\top} (A x - b)\n$$\nAt the point $x_{0}$, using the residual $r=Ax_0-b$:\n$$\n\\nabla f(x_{0}) = 2 A^{\\top} r\n$$\nThe steepest descent direction is $d = - \\nabla f(x_{0}) = -2 A^{\\top} r$.\n\nNow we compute the terms for the cosine formula. The numerator is the dot product $d^{\\top}v_{k}$:\n$$\nd^{\\top}v_{k} = (-2 A^{\\top} r)^{\\top}v_{k} = -2 r^{\\top} A v_{k}\n$$\nUsing the relation $A v_{k} = \\sigma_{k} u_{k}$ and the definition $c_{k} = u_{k}^{\\top} r$:\n$$\nd^{\\top}v_{k} = -2 r^{\\top} (\\sigma_{k} u_{k}) = -2 \\sigma_{k} (r^{\\top} u_{k}) = -2 \\sigma_{k} c_{k}\n$$\nThe denominator involves the norms of the vectors. The vector $v_{k}$ is a column of the orthogonal matrix $V$, so it is a unit vector: $\\|v_{k}\\|_{2} = 1$. The norm of the steepest descent direction is:\n$$\n\\|d\\|_{2} = \\|-2 A^{\\top} r\\|_{2} = 2 \\|A^{\\top} r\\|_{2}\n$$\nWe must express $\\|A^{\\top} r\\|_{2}$ in terms of $\\{\\sigma_{j}\\}$ and $\\{c_{j}\\}$. We use the SVD for $A^{\\top} = (U \\Sigma V^{\\top})^{\\top} = V \\Sigma^{\\top} U^{\\top}$.\n$$\nA^{\\top}r = V \\Sigma^{\\top} (U^{\\top}r) = V \\Sigma^{\\top} c\n$$\nSince $V$ is an orthogonal matrix, it preserves the L2-norm, so $\\|A^{\\top}r\\|_{2} = \\|V \\Sigma^{\\top} c\\|_{2} = \\|\\Sigma^{\\top} c\\|_{2}$.\nThe matrix $\\Sigma^{\\top}$ is an $n \\times m$ diagonal matrix with entries $\\sigma_{1}, \\dots, \\sigma_{n}$ on its diagonal. The vector $c = U^{\\top}r \\in \\mathbb{R}^{m}$ has components $c_{j}$. The product $\\Sigma^{\\top}c$ is a vector in $\\mathbb{R}^{n}$ with components $(\\sigma_{j} c_{j})$ for $j = 1, \\dots, n$.\n$$\n\\Sigma^{\\top} c = \\begin{pmatrix} \\sigma_{1} c_{1} \\\\ \\sigma_{2} c_{2} \\\\ \\vdots \\\\ \\sigma_{n} c_{n} \\end{pmatrix}\n$$\nThe norm is therefore:\n$$\n\\|A^{\\top} r\\|_{2} = \\|\\Sigma^{\\top} c\\|_{2} = \\sqrt{\\sum_{j=1}^{n} (\\sigma_{j} c_{j})^{2}}\n$$\nSubstituting all parts into the cosine formula:\n$$\n\\cos(\\theta_{k}) = \\frac{d^{\\top}v_{k}}{\\|d\\|_{2} \\|v_{k}\\|_{2}} = \\frac{-2 \\sigma_{k} c_{k}}{(2 \\sqrt{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}})(1)} = \\frac{-\\sigma_{k} c_{k}}{\\sqrt{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}}}\n$$\nThis is the required expression.\n\nThird, we explain how ill-conditioning affects the alignment of the initial gradient descent trajectory with the right singular vectors $v_{k}$. The alignment is measured by $|\\cos(\\theta_{k})|$. Let us consider the squared cosine for simplicity:\n$$\n\\cos^{2}(\\theta_{k}) = \\frac{\\sigma_{k}^{2} c_{k}^{2}}{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}}\n$$\nThis expression shows how the squared-magnitude of the projection of the gradient direction onto $v_k$ is distributed. Ill-conditioning implies that the singular values vary over a large range, i.e., $\\sigma_{1} \\gg \\sigma_{n}$. Assuming the components $c_{j} = u_{j}^{\\top} r$ are non-zero and of comparable magnitude, the sum in the denominator, $\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}$, will be overwhelmingly dominated by the terms corresponding to the largest singular values.\nFor instance, the term $\\sigma_{1}^{2} c_{1}^{2}$ will be much larger than $\\sigma_{n}^{2} c_{n}^{2}$.\nConsequently, for a singular vector $v_{k}$ associated with a large singular value $\\sigma_{k}$ (e.g., $k=1$), the numerator $\\sigma_{k}^{2} c_{k}^{2}$ will be large, making $\\cos^{2}(\\theta_{k})$ close to $1$. The steepest descent direction $-\\nabla f(x_{0})$ is thus strongly aligned with $v_{k}$.\nConversely, for a singular vector $v_{k}$ associated with a small singular value $\\sigma_{k}$ (e.g., $k=n$), the numerator $\\sigma_{k}^{2} c_{k}^{2}$ will be very small compared to the denominator, making $\\cos^{2}(\\theta_{k})$ close to $0$. The steepest descent direction is nearly orthogonal to these singular vectors.\nTherefore, the initial trajectory of gradient descent, which follows $-\\nabla f(x_{0})$, aligns predominantly with the right singular vectors corresponding to the largest singular values. This leads to rapid initial reduction of the error components in these directions but extremely slow progress for error components associated with small singular values, which is the characteristic slow convergence of gradient descent on ill-conditioned problems.", "answer": "$$\n\\boxed{\\frac{-\\sigma_{k} c_{k}}{\\sqrt{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}}}}\n$$", "id": "3120177"}, {"introduction": "Our final practice brings theory into the realm of computation. While we often analyze gradients at points of differentiability, what happens when an algorithm like gradient descent encounters a non-smooth region? This exercise asks you to investigate a simple yet illustrative function with a non-differentiable \"ridge\" and simulate the behavior of gradient descent near it [@problem_id:3120201]. By observing the famous \"zig-zagging\" pattern and how it responds to different step-size strategies, you will gain a practical intuition for the challenges and dynamics of non-smooth optimization.", "problem": "Consider the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $f(x,y)=|x|+y^2$. This function is convex, has an anisotropic nonsmooth ridge along the line $\\{(x,y):x=0\\}$, and is differentiable for all points $(x,y)$ with $x\\neq 0$. Your task is to analyze differentiability, directional derivatives, descent directions, and the behavior of Gradient Descent (GD) near the origin $(0,0)$ starting from fundamental definitions. You must write a complete, runnable program that carries out the computations described below and produces the specified final output.\n\nFoundational starting points you must use:\n- The directional derivative of a function $f$ at a point $z$ in the direction $d$ is defined by $D f(z;d)=\\lim_{t\\to 0^{+}}\\frac{f(z+t d)-f(z)}{t}$, when the limit exists.\n- For points where $f$ is differentiable, its gradient $\\nabla f(z)$ is the unique vector such that $D f(z;d)=\\nabla f(z)^\\top d$ holds for every direction $d$.\n- A direction $d$ is a descent direction for $f$ at $z$ if $D f(z;d)0$.\n- At points where $f$ is not differentiable, use subgradient reasoning only as explicitly instructed below; do not assume differentiability at $x=0$.\n\nConventions to be followed in your program:\n- When simulating GD, use the direction $-\\nabla f(z)$ wherever $\\nabla f(z)$ exists. For the nondifferentiable component $|x|$ at $x=0$, adopt the subgradient choice $g_x=0$ at $x=0$ and $g_x=\\operatorname{sign}(x)$ for $x\\neq 0$, where $\\operatorname{sign}(x)=1$ if $x0$, $\\operatorname{sign}(x)=-1$ if $x0$, and $\\operatorname{sign}(0)=0$. For the smooth component, use $\\frac{\\partial}{\\partial y}y^2=2y$.\n- A sign flip in the $x$-coordinate during GD is counted if and only if $x_k\\cdot x_{k+1}0$. Transitions to or from $x=0$ do not count as sign flips.\n\nTasks and test suite:\n1) Compute one-sided directional derivatives at the origin and assess descent directions.\n   - Using only the definition of directional derivative, compute $D f((0,0);d)$ for each direction $d$ in the set $\\mathcal{D}=\\{(1,0),(-0.3,0.7),(0,1)\\}$.\n   - For each $d\\in\\mathcal{D}$, determine whether $d$ is a descent direction at $(0,0)$, that is, whether $D f((0,0);d)0$. Return three boolean values corresponding to the directions in the stated order.\n\n2) Assess $-\\nabla f$ as a descent direction near the origin where $f$ is differentiable.\n   - For each point $z$ in the set $\\mathcal{Z}=\\{(0.1,0),(-0.02,0.05)\\}$ with $x\\neq 0$, compute $\\nabla f(z)$ using the fundamental definitions, then evaluate $D f(z;-\\nabla f(z))$ using $D f(z;d)=\\nabla f(z)^\\top d$ at differentiable points.\n   - For each $z\\in\\mathcal{Z}$, return a boolean indicating whether $-\\nabla f(z)$ is a descent direction (i.e., $D f(z;-\\nabla f(z))0$). Return two boolean values corresponding to the points in the stated order.\n\n3) Simulate GD to reveal zig-zag behavior in the $x$-coordinate near the ridge.\n   - Implement GD iterations $(x_{k+1},y_{k+1})=(x_k,y_k)-\\alpha_k\\,(g_x(x_k),2y_k)$, where $g_x$ follows the convention stated above. Use two step-size schemes:\n     a) Constant step size: $\\alpha_k=\\alpha$.\n     b) Diminishing step size: $\\alpha_k=\\frac{\\alpha_0}{k+1}$.\n   - For each scenario below, starting from $(x_0,y_0)=(0.5,0.4)$, run $N$ GD iterations and report the integer count of sign flips in the $x$-coordinate:\n     i) Constant step size with $\\alpha=0.3$ and $N=50$.\n     ii) Constant step size with $\\alpha=0.05$ and $N=50$.\n     iii) Diminishing step size with $\\alpha_0=0.3$ and $N=50$.\n     iv) Boundary case: constant step size with $\\alpha=0.5$ and $N=50$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The nine entries must appear in the following order:\n  - Three boolean values for Task $1$ corresponding to directions $(1,0)$, $(-0.3,0.7)$, and $(0,1)$.\n  - Two boolean values for Task $2$ corresponding to points $(0.1,0)$ and $(-0.02,0.05)$.\n  - Four integers for Task $3$ corresponding to scenarios $(\\text{i}),(\\text{ii}),(\\text{iii}),(\\text{iv})$.\nFor example, the output should look like $[b_1,b_2,b_3,b_4,b_5,n_1,n_2,n_3,n_4]$, where each $b_i$ is a boolean and each $n_i$ is an integer. No other text should be printed.", "solution": "The problem requires an analysis of the function $f(x,y)=|x|+y^2$ with respect to its differentiability, directional derivatives, and the behavior of the Gradient Descent (GD) algorithm near the non-differentiable ridge at $x=0$. The analysis will be conducted in three distinct tasks.\n\n**Task 1: Directional Derivatives at the Origin**\n\nThe first task is to compute the one-sided directional derivative of $f$ at the origin $z=(0,0)$ for three specified directions and determine if they are descent directions. The definition of the directional derivative of a function $f$ at a point $z$ in a direction $d$ is given by:\n$$D f(z;d)=\\lim_{t\\to 0^{+}}\\frac{f(z+t d)-f(z)}{t}$$\nFor our function $f(x,y) = |x| + y^2$ and the point $z=(0,0)$, we have $f(z) = f(0,0) = |0| + 0^2 = 0$. Let the direction be $d=(d_x, d_y)$. The point $z+td$ is $(td_x, td_y)$. The value of the function at this point is:\n$$f(z+td) = f(td_x, td_y) = |td_x| + (td_y)^2$$\nSince the limit is for $t \\to 0^{+}$, $t$ is positive, so $|t|=t$. Therefore, we can write:\n$$f(td_x, td_y) = t|d_x| + t^2d_y^2$$\nSubstituting this into the definition of the directional derivative:\n$$D f((0,0);d) = \\lim_{t\\to 0^{+}}\\frac{(t|d_x| + t^2d_y^2) - 0}{t} = \\lim_{t\\to 0^{+}}\\left(\\frac{t|d_x|}{t} + \\frac{t^2d_y^2}{t}\\right) = \\lim_{t\\to 0^{+}}(|d_x| + td_y^2)$$\nAs $t \\to 0^{+}$, the term $td_y^2$ goes to $0$. The limit is thus:\n$$D f((0,0);(d_x, d_y)) = |d_x|$$\nA direction $d$ is a descent direction if $D f(z;d)  0$. We now apply this to each direction in the set $\\mathcal{D}=\\{(1,0),(-0.3,0.7),(0,1)\\}$.\n\n1.  For $d_1=(1,0)$: $D f((0,0);d_1) = |1| = 1$. Since $1 \\not 0$, this is not a descent direction.\n2.  For $d_2=(-0.3,0.7)$: $D f((0,0);d_2) = |-0.3| = 0.3$. Since $0.3 \\not 0$, this is not a descent direction.\n3.  For $d_3=(0,1)$: $D f((0,0);d_3) = |0| = 0$. Since $0 \\not 0$, this is not a descent direction.\n\nThe boolean results for Task 1 are (False, False, False).\n\n**Task 2: Assessment of the Negative Gradient as a Descent Direction**\n\nThe second task is to assess whether the negative gradient, $-\\nabla f(z)$, constitutes a descent direction at points where $f$ is differentiable. The points are taken from the set $\\mathcal{Z}=\\{(0.1,0),(-0.02,0.05)\\}$, where $x \\neq 0$.\n\nFor $x \\neq 0$, the function $f(x,y)=|x|+y^2$ is differentiable. Its partial derivatives are:\n$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} |x| = \\operatorname{sign}(x)$$\n$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} y^2 = 2y$$\nThe gradient vector is $\\nabla f(x,y) = (\\operatorname{sign}(x), 2y)^\\top$.\n\nAt a point $z$ where $f$ is differentiable, the directional derivative in a direction $d$ is given by the dot product $D f(z;d) = \\nabla f(z)^\\top d$. We are interested in the specific direction $d = -\\nabla f(z)$.\n$$D f(z;-\\nabla f(z)) = \\nabla f(z)^\\top (-\\nabla f(z)) = -(\\nabla f(z)^\\top \\nabla f(z)) = -\\|\\nabla f(z)\\|_2^2$$\nThis direction is a descent direction if $D f(z;-\\nabla f(z))  0$, which is equivalent to $-\\|\\nabla f(z)\\|_2^2  0$. This inequality holds if and only if the gradient vector $\\nabla f(z)$ is not the zero vector.\n\nWe now evaluate this for the two points in $\\mathcal{Z}$.\n\n1.  For $z_1=(0.1,0)$:\n    $\\nabla f(0.1,0) = (\\operatorname{sign}(0.1), 2 \\cdot 0)^\\top = (1, 0)^\\top$.\n    This gradient is non-zero. The directional derivative is $D f(z_1;-\\nabla f(z_1)) = -\\|(1,0)\\|_2^2 = -1  0$. Thus, $-\\nabla f(z_1)$ is a descent direction.\n2.  For $z_2=(-0.02,0.05)$:\n    $\\nabla f(-0.02,0.05) = (\\operatorname{sign}(-0.02), 2 \\cdot 0.05)^\\top = (-1, 0.1)^\\top$.\n    This gradient is non-zero. The directional derivative is $D f(z_2;-\\nabla f(z_2)) = -\\|(-1, 0.1)\\|_2^2 = -( (-1)^2 + 0.1^2 ) = -1.01  0$. Thus, $-\\nabla f(z_2)$ is a descent direction.\n\nThe boolean results for Task 2 are (True, True).\n\n**Task 3: Gradient Descent Simulation**\n\nThe third task involves simulating Gradient Descent and counting sign flips in the $x$-coordinate. The update rule is $(x_{k+1},y_{k+1})=(x_k,y_k)-\\alpha_k\\,(g_x(x_k),2y_k)$. The generalized gradient component $g_x(x)$ is defined as $\\operatorname{sign}(x)$ for $x \\neq 0$ and $g_x(0) = 0$. This is equivalent to $g_x(x) = \\operatorname{sign}(x)$ using the standard definition where $\\operatorname{sign}(0)=0$.\n\nThe update for the $x$-coordinate is:\n$$x_{k+1} = x_k - \\alpha_k \\operatorname{sign}(x_k)$$\nA sign flip is counted if $x_k \\cdot x_{k+1}  0$. This occurs if $x_k$ and $x_{k+1}$ are non-zero and have opposite signs. We start from $(x_0,y_0)=(0.5,0.4)$ and run for $N=50$ iterations.\n\ni) Constant step size $\\alpha=0.3$:\n$x_{k+1} = x_k - 0.3 \\operatorname{sign}(x_k)$.\n$x_0=0.5$. $x_1=0.5-0.3=0.2$. No flip.\n$x_2=0.2-0.3=-0.1$. Flip ($x_1 \\cdot x_2  0$).\n$x_3=-0.1+0.3=0.2$. Flip ($x_2 \\cdot x_3  0$).\nThe sequence for $x$ starting from $k=1$ oscillates between $0.2$ and $-0.1$. A flip occurs at every step from $k=1$ to $k=49$. The number of flips is $49 - 1 + 1 = 49$.\n\nii) Constant step size $\\alpha=0.05$:\n$x_{k+1} = x_k - 0.05 \\operatorname{sign}(x_k)$.\n$x_k$ decreases by $0.05$ at each step as long as $x_k0$.\n$x_0=0.5, x_1=0.45, \\dots, x_9=0.05$.\n$x_{10} = 0.05 - 0.05 = 0$.\nFor $k \\ge 10$, $x_k=0$, so $\\operatorname{sign}(x_k)=0$ and $x_{k+1}=x_k=0$.\nSince no iterate $x_{k+1}$ becomes negative, there are no instances of $x_k \\cdot x_{k+1}  0$. The flip count is $0$.\n\niii) Diminishing step size $\\alpha_k=\\frac{0.3}{k+1}$:\n$x_{k+1} = x_k - \\frac{0.3}{k+1}\\operatorname{sign}(x_k)$.\n$x_0=0.5$.\n$k=0$: $\\alpha_0=0.3$. $x_1=0.5-0.3=0.2$. No flip.\n$k=1$: $\\alpha_1=0.15$. $x_2=0.2-0.15=0.05$. No flip.\n$k=2$: $\\alpha_2=0.1$. $x_3=0.05-0.1=-0.05$. Flip ($x_2 \\cdot x_3  0$).\nA flip occurs at step $k$ if $|x_k|\\alpha_k$. For $k=2$, this condition is met. For subsequent steps, we find that the condition continues to be met, causing a sign flip at each step from $k=2$ to $k=49$.\nThe number of flips is $49 - 2 + 1 = 48$.\n\niv) Constant step size $\\alpha=0.5$:\n$x_{k+1} = x_k - 0.5 \\operatorname{sign}(x_k)$.\n$x_0=0.5$.\n$x_1=0.5-0.5=0$.\nFor $k \\ge 1$, $x_k=0$ and the sequence remains at $0$.\nThere are no instances of $x_k \\cdot x_{k+1}  0$. The flip count is $0$.\n\nThe integer results for Task 3 are (49, 0, 48, 0).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing all specified calculations for the three tasks.\n    \"\"\"\n    results = []\n\n    # Task 1: Compute one-sided directional derivatives at the origin and assess descent directions.\n    # The directional derivative at the origin (0,0) in direction d=(dx, dy) for f(x,y)=|x|+y^2\n    # is Df((0,0); d) = |dx|. A direction d is a descent direction if Df  0.\n    directions_task1 = [(1.0, 0.0), (-0.3, 0.7), (0.0, 1.0)]\n    for d in directions_task1:\n        dx, dy = d\n        df_val = abs(dx)\n        results.append(df_val  0)\n\n    # Task 2: Assess -∇f as a descent direction near the origin.\n    # For x != 0, f is differentiable. ∇f(x,y) = (sign(x), 2y).\n    # The directional derivative in the direction -∇f is Df(z; -∇f) = -||∇f||^2.\n    # This is  0 if and only if ∇f is not the zero vector.\n    points_task2 = [(0.1, 0.0), (-0.02, 0.05)]\n    for z in points_task2:\n        x, y = z\n        grad = np.array([np.sign(x), 2 * y])\n        # The dot product of a vector with itself is its squared L2 norm.\n        df_val = -np.dot(grad, grad)\n        results.append(df_val  0)\n\n    # Task 3: Simulate GD to reveal zig-zag behavior.\n    x0 = 0.5\n    N = 50\n\n    def run_gd_x_flips(x_start, N_iter, alpha_rule):\n        \"\"\"\n        Simulates the x-coordinate updates of Gradient Descent and counts sign flips.\n        A sign flip is counted if x_k * x_{k+1}  0.\n        \"\"\"\n        flips = 0\n        x = x_start\n        for k in range(N_iter):\n            x_prev = x\n            alpha_k = alpha_rule(k)\n            # Per problem spec, g_x(x) = sign(x) for x!=0, and g_x(0)=0.\n            # np.sign(0) is 0, so np.sign works for all cases.\n            gx = np.sign(x)\n            x = x - alpha_k * gx\n            if x_prev * x  0:\n                flips += 1\n        return flips\n\n    # Scenario i: Constant step size alpha = 0.3\n    alpha_i = 0.3\n    flips_i = run_gd_x_flips(x0, N, lambda k: alpha_i)\n    results.append(flips_i)\n\n    # Scenario ii: Constant step size alpha = 0.05\n    alpha_ii = 0.05\n    flips_ii = run_gd_x_flips(x0, N, lambda k: alpha_ii)\n    results.append(flips_ii)\n\n    # Scenario iii: Diminishing step size alpha_k = 0.3 / (k+1)\n    alpha0_iii = 0.3\n    flips_iii = run_gd_x_flips(x0, N, lambda k: alpha0_iii / (k + 1))\n    results.append(flips_iii)\n    \n    # Scenario iv: Boundary case, constant step size alpha = 0.5\n    alpha_iv = 0.5\n    flips_iv = run_gd_x_flips(x0, N, lambda k: alpha_iv)\n    results.append(flips_iv)\n\n    # Format the final output string as specified.\n    # Boolean values are converted to lowercase strings 'true' or 'false'.\n    output_str = ','.join(str(r).lower() if isinstance(r, bool) else str(r) for r in results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3120201"}]}