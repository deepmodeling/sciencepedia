{"hands_on_practices": [{"introduction": "Many powerful optimization algorithms require the objective function to be smooth, yet many real-world problems, like finding the maximum of a set of functions, are not. This exercise [@problem_id:3183315] introduces the widely-used \"log-sum-exp\" technique to create a smooth approximation and challenges you to compute its gradient's Lipschitz constant, $L$. This practice is key to understanding how we can adapt non-smooth problems to fit the requirements of first-order optimization methods.", "problem": "Let $m=3$ and $d=2$. Consider the convex function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by\n$$\nf(x)=\\max_{i\\in\\{1,2,3\\}} a_{i}^{\\top}x,\n$$\nwhere $a_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $a_{2}=\\begin{pmatrix}0\\\\2\\end{pmatrix}$, and $a_{3}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$. Work under the Euclidean norm $\\|\\cdot\\|_{2}$. Define the smooth approximation (log-sum-exp smoothing) $f_{\\gamma}:\\mathbb{R}^{2}\\to\\mathbb{R}$ for any $\\gamma>0$ by\n$$\nf_{\\gamma}(x)=\\gamma\\log\\!\\left(\\sum_{i=1}^{3}\\exp\\!\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right).\n$$\nStarting from the foundational definitions of Lipschitz continuity and gradient Lipschitz continuity, determine:\n1. The smallest Lipschitz constant of $f$ with respect to $\\|\\cdot\\|_{2}$.\n2. The smallest global Lipschitz constant of the gradient $\\nabla f_{\\gamma}(x)$ with respect to $\\|\\cdot\\|_{2}$, expressed as a closed-form analytic function $L(\\gamma)$ of $\\gamma$.\n\nProvide your final answer as a row matrix whose first entry is the Lipschitz constant of $f$ and whose second entry is $L(\\gamma)$. No rounding is required.", "solution": "The user wants me to solve a two-part problem related to a specific convex function $f(x)$ and its smooth approximation $f_\\gamma(x)$.\n\n**Part 1: Lipschitz Constant of $f(x)$**\n\nA function $f:\\mathbb{R}^{d}\\to\\mathbb{R}$ is said to be $L_f$-Lipschitz continuous with respect to a norm $\\|\\cdot\\|$ if there exists a constant $L_f \\ge 0$ such that for all $x, y \\in \\mathbb{R}^{d}$, the inequality holds:\n$$|f(x) - f(y)| \\le L_f \\|x - y\\|$$\nThe smallest such constant $L_f$ is the Lipschitz constant of $f$.\n\nFor a convex function, the Lipschitz constant is given by the largest possible norm of a subgradient of the function. The norm used for the subgradient is the dual norm of the norm used for the domain. Given that we are working with the Euclidean norm $\\|\\cdot\\|_{2}$, which is self-dual, the Lipschitz constant is:\n$$L_f = \\sup_{x \\in \\mathbb{R}^{d}} \\sup_{g \\in \\partial f(x)} \\|g\\|_{2}$$\nwhere $\\partial f(x)$ is the subdifferential of $f$ at $x$.\n\nThe given function is $f(x) = \\max_{i \\in \\{1, 2, 3\\}} a_{i}^{\\top}x$. This is the maximum of a finite number of linear functions, which is a well-known convex function. The subdifferential of $f(x)$ at a point $x$ is the convex hull of the gradients of the active linear functions, i.e., those that achieve the maximum at $x$:\n$$\\partial f(x) = \\text{conv}\\{a_i \\mid a_i^\\top x = f(x)\\}$$\nThe set of all possible subgradients over the entire domain is the convex hull of all the vectors $a_i$:\n$$\\bigcup_{x \\in \\mathbb{R}^{d}} \\partial f(x) = \\text{conv}\\{a_1, a_2, ..., a_m\\}$$\nIn our case, $m=3$, so this set is $\\text{conv}\\{a_1, a_2, a_3\\}$.\n\nThe Lipschitz constant is the supremum of the norm of any element in this set:\n$$L_f = \\sup_{g \\in \\text{conv}\\{a_1, a_2, a_3\\}} \\|g\\|_2$$\nSince the norm $\\|\\cdot\\|_2$ is a convex function, its maximum value over a compact convex set (the convex hull of a finite set of points is compact and convex) is attained at one of the extreme points of the set. The extreme points of $\\text{conv}\\{a_1, a_2, a_3\\}$ are a subset of $\\{a_1, a_2, a_3\\}$. Therefore, the problem simplifies to finding the maximum norm among the given vectors $a_i$:\n$$L_f = \\max_{i \\in \\{1, 2, 3\\}} \\|a_i\\|_2$$\nThe given vectors are $a_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $a_2=\\begin{pmatrix}0\\\\2\\end{pmatrix}$, and $a_3=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$. We compute their Euclidean norms:\n$$\\|a_1\\|_2 = \\sqrt{1^2 + 0^2} = \\sqrt{1} = 1$$\n$$\\|a_2\\|_2 = \\sqrt{0^2 + 2^2} = \\sqrt{4} = 2$$\n$$\\|a_3\\|_2 = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$$\nThe maximum of these values is:\n$$L_f = \\max\\{1, 2, \\sqrt{2}\\} = 2$$\n\n**Part 2: Gradient Lipschitz Constant of $f_\\gamma(x)$**\n\nA differentiable function $g:\\mathbb{R}^{d}\\to\\mathbb{R}$ has a Lipschitz continuous gradient with constant $L$ (also known as $L$-smoothness) if for all $x, y \\in \\mathbb{R}^{d}$:\n$$\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L \\|x - y\\|_2$$\nFor a twice-differentiable function, the smallest such constant $L$ is the supremum of the spectral norm (the largest eigenvalue for a symmetric positive semidefinite matrix) of its Hessian matrix over the domain:\n$$L = \\sup_{x \\in \\mathbb{R}^{d}} \\|\\nabla^2 g(x)\\|_2 = \\sup_{x \\in \\mathbb{R}^{d}} \\lambda_{\\max}(\\nabla^2 g(x))$$\nThe function is $f_{\\gamma}(x)=\\gamma\\log\\left(\\sum_{i=1}^{3}\\exp\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right)$. Let's compute its gradient and Hessian.\nLet $p_i(x) = \\frac{\\exp(a_i^\\top x / \\gamma)}{\\sum_{k=1}^3 \\exp(a_k^\\top x / \\gamma)}$. Note that $p_i(x) > 0$ and $\\sum_{i=1}^3 p_i(x) = 1$.\nThe gradient of $f_\\gamma(x)$ is:\n$$\\nabla f_\\gamma(x) = \\sum_{i=1}^3 p_i(x) a_i$$\nThe Hessian matrix $\\nabla^2 f_\\gamma(x)$ has entries $(\\nabla^2 f_\\gamma(x))_{jk} = \\frac{\\partial^2 f_\\gamma(x)}{\\partial x_j \\partial x_k}$. A standard calculation yields:\n$$\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\left( \\sum_{i=1}^3 p_i(x) a_i a_i^\\top - \\left(\\sum_{i=1}^3 p_i(x) a_i\\right) \\left(\\sum_{i=1}^3 p_i(x) a_i\\right)^\\top \\right)$$\nThis matrix represents the covariance matrix of the vector-valued random variable $A$ that takes value $a_i$ with probability $p_i(x)$, scaled by $\\frac{1}{\\gamma}$. Let $\\bar{a}(x) = \\mathbb{E}_{p(x)}[A] = \\sum_i p_i(x) a_i$. Then $\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\text{Cov}_{p(x)}(A) = \\frac{1}{\\gamma}(\\mathbb{E}_{p(x)}[AA^\\top] - \\bar{a}(x)\\bar{a}(x)^\\top)$.\n\nThe smallest global Lipschitz constant of the gradient, $L(\\gamma)$, is:\n$$L(\\gamma) = \\sup_{x \\in \\mathbb{R}^2} \\|\\nabla^2 f_\\gamma(x)\\|_2 = \\frac{1}{\\gamma} \\sup_{x \\in \\mathbb{R}^2} \\lambda_{\\max}(\\text{Cov}_{p(x)}(A))$$\nThe spectral norm is $\\lambda_{\\max}(M) = \\sup_{\\|v\\|_2=1} v^\\top M v$. For the covariance matrix, we have:\n$$v^\\top (\\text{Cov}_{p(x)}(A)) v = \\mathbb{E}_{p(x)}[(v^\\top A)^2] - (\\mathbb{E}_{p(x)}[v^\\top A])^2 = \\text{Var}_{p(x)}(a^\\top v)$$\nThe values $p(x)$ can be made arbitrarily close to any point in the probability simplex $\\Delta_3 = \\{p \\in \\mathbb{R}^3 \\mid p_i \\ge 0, \\sum p_i = 1\\}$. Thus, we need to find the supremum over $p \\in \\Delta_3$:\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{p \\in \\Delta_3} \\sup_{\\|v\\|_2=1} \\text{Var}_p(a^\\top v) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\sup_{p \\in \\Delta_3} \\text{Var}_p(a^\\top v)$$\nFor a fixed vector $v$, let $c_i = a_i^\\top v$. We want to find $\\sup_{p \\in \\Delta_3} \\text{Var}_p(c)$. The variance of a random variable with finite support is bounded by $\\frac{1}{4}(\\max(c) - \\min(c))^2$. This maximum is achieved when probability mass is split as $\\frac{1}{2}$ on the maximum value and $\\frac{1}{2}$ on the minimum value. Thus:\n$$\\sup_{p \\in \\Delta_3} \\text{Var}_p(c) = \\frac{1}{4} \\max_{i,j} (c_i - c_j)^2 = \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2$$\nSubstituting this back into the expression for $L(\\gamma)$:\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\left( \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2 \\right) = \\frac{1}{4\\gamma} \\max_{i,j} \\left( \\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 \\right)$$\nBy the definition of the dual norm and the Cauchy-Schwarz inequality, $\\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 = \\|a_i - a_j\\|_2^2$. Therefore, the smallest global Lipschitz constant is:\n$$L(\\gamma) = \\frac{1}{4\\gamma} \\max_{i,j \\in \\{1,2,3\\}} \\|a_i - a_j\\|_2^2$$\nWe now compute the squared norms of the differences between the vectors:\n$$a_1 - a_2 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} \\implies \\|a_1 - a_2\\|_2^2 = 1^2 + (-2)^2 = 5$$\n$$a_1 - a_3 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} \\implies \\|a_1 - a_3\\|_2^2 = 2^2 + 1^2 = 5$$\n$$a_2 - a_3 = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 3\\end{pmatrix} \\implies \\|a_2 - a_3\\|_2^2 = 1^2 + 3^2 = 10$$\nThe maximum of these values is $10$.\nSubstituting this into the formula for $L(\\gamma)$:\n$$L(\\gamma) = \\frac{10}{4\\gamma} = \\frac{5}{2\\gamma}$$\n\nThe final answer is a row matrix containing the Lipschitz constant of $f$, which is $2$, and the Lipschitz constant of $\\nabla f_\\gamma$, which is $\\frac{5}{2\\gamma}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & \\frac{5}{2\\gamma}\n\\end{pmatrix}\n}\n$$", "id": "3183315"}, {"introduction": "The convergence speed of many optimization methods depends critically on the condition number of the objective function, often represented by the ratio of its smoothness ($L$) to its strong convexity ($\\mu$). In this exercise [@problem_id:3183345], you will explore how adding a simple quadratic regularizer, a common technique in machine learning, allows you to control and improve this condition number. By analyzing this trade-off, you will learn to tune regularization parameters to optimize algorithm performance.", "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be a differentiable convex function whose gradient is $L$-Lipschitz continuous, meaning $|\\!|\\nabla f(x)-\\nabla f(y)|\\!|\\leq L\\,|\\!|x-y|\\!|$ for all $x,y\\in\\mathbb{R}^{n}$, and suppose $f$ is $\\mu_{0}$-strongly convex, meaning $f(y)\\geq f(x)+\\nabla f(x)^{\\top}(y-x)+\\frac{\\mu_{0}}{2}|\\!|y-x|\\!|^{2}$ for all $x,y\\in\\mathbb{R}^{n}$, with $L>\\mu_{0}\\geq 0$. Consider adding a quadratic regularizer to form\n$$\nF_{\\alpha}(x)\\;=\\;f(x)\\;+\\;\\frac{\\alpha}{2}\\,|\\!|x|\\!|^{2},\n$$\nwhere $\\alpha\\geq 0$ is a tunable parameter. Using only the fundamental definitions of Lipschitz continuity of the gradient and strong convexity, determine the smoothness constant $L_{\\alpha}$ and strong convexity modulus $\\mu_{\\alpha}$ of $F_{\\alpha}$.\n\nIn smooth strongly convex optimization, the iteration complexity of Nesterov’s Accelerated Gradient (NAG) method scales, up to multiplicative constants independent of $\\alpha$, like $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$ times a logarithmic factor in the target accuracy. To quantify the trade-off of increasing strong convexity “without changing $L$ too much,” impose the engineering constraint $L_{\\alpha}\\leq (1+\\rho)\\,L$ for a given parameter $0<\\rho<1$. Choose $\\alpha$ to minimize the leading condition-number factor $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$ subject to this constraint, and report the minimized factor as a single closed-form expression in terms of $L$, $\\mu_{0}$, and $\\rho$. Do not round your answer.", "solution": "The user wants me to solve a two-part problem in the context of optimization.\n\n**Part 1: Find the smoothness and strong convexity constants of a regularized function.**\n\n1.  **Analyze the function:** The new function is $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$.\n2.  **Define the properties:**\n    *   **Smoothness (L-smooth):** A function $g$ is $L_g$-smooth if its gradient is $L_g$-Lipschitz continuous, i.e., $||\\nabla g(x) - \\nabla g(y)|| \\leq L_g ||x - y||$.\n    *   **Strong Convexity ($\\mu$-strongly convex):** A differentiable function $g$ is $\\mu_g$-strongly convex if $g(y) \\geq g(x) + \\nabla g(x)^T(y-x) + \\frac{\\mu_g}{2} ||y-x||^2$.\n3.  **Apply these definitions to $F_{\\alpha}(x)$.**\n\n    *   **Smoothness of $F_{\\alpha}$:**\n        *   First, I need the gradient of $F_{\\alpha}(x)$.\n        *   The gradient of $f(x)$ is $\\nabla f(x)$.\n        *   The regularizer is $g(x) = \\frac{\\alpha}{2} ||x||^2 = \\frac{\\alpha}{2} \\sum_{i=1}^n x_i^2$.\n        *   The gradient of the regularizer is $\\nabla g(x) = \\alpha x$.\n        *   So, $\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$.\n        *   Now, I need to check the Lipschitz continuity of $\\nabla F_{\\alpha}(x)$.\n        *   $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| = ||(\\nabla f(x) + \\alpha x) - (\\nabla f(y) + \\alpha y)||$.\n        *   $= ||(\\nabla f(x) - \\nabla f(y)) + \\alpha(x - y)||$.\n        *   Using the triangle inequality: $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| \\leq ||\\nabla f(x) - \\nabla f(y)|| + ||\\alpha(x - y)||$.\n        *   $= ||\\nabla f(x) - \\nabla f(y)|| + |\\alpha| ||x - y||$.\n        *   Since $\\alpha \\geq 0$, this is $||\\nabla f(x) - \\nabla f(y)|| + \\alpha ||x - y||$.\n        *   We are given that $\\nabla f$ is $L$-Lipschitz, so $||\\nabla f(x) - \\nabla f(y)|| \\leq L ||x - y||$.\n        *   Substituting this in, we get $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| \\leq L ||x - y|| + \\alpha ||x - y|| = (L + \\alpha) ||x - y||$.\n        *   This means $\\nabla F_{\\alpha}$ is $(L+\\alpha)$-Lipschitz continuous.\n        *   Therefore, the smoothness constant of $F_{\\alpha}$ is $L_{\\alpha} = L + \\alpha$.\n\n    *   **Strong Convexity of $F_{\\alpha}$:**\n        *   I need to use the definition: $F_{\\alpha}(y) \\geq F_{\\alpha}(x) + \\nabla F_{\\alpha}(x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} ||y-x||^2$.\n        *   Let's expand the terms. $F_{\\alpha}(y) = f(y) + \\frac{\\alpha}{2} ||y||^2$.\n        *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$.\n        *   $\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$.\n        *   The inequality becomes:\n            $f(y) + \\frac{\\alpha}{2} ||y||^2 \\geq (f(x) + \\frac{\\alpha}{2} ||x||^2) + (\\nabla f(x) + \\alpha x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} ||y-x||^2$.\n        *   This looks complicated. Let's try another approach.\n        *   A function $g$ is $\\mu_g$-strongly convex if and only if the function $h(x) = g(x) - \\frac{\\mu_g}{2} ||x||^2$ is convex.\n        *   Let's use the definition directly but be more clever.\n        *   Let $g(x) = \\frac{\\alpha}{2} ||x||^2$. This is a quadratic function.\n        *   We know $f$ is $\\mu_0$-strongly convex:\n            $f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu_0}{2} ||y-x||^2$.\n        *   What about $g(x)$? $\\nabla g(x) = \\alpha x$.\n            $g(y) = \\frac{\\alpha}{2} ||y||^2$.\n            $g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} ||y-x||^2 = \\frac{\\alpha}{2} ||x||^2 + (\\alpha x)^T(y-x) + \\frac{\\alpha}{2} (||y||^2 - 2y^T x + ||x||^2)$.\n            $= \\frac{\\alpha}{2} ||x||^2 + \\alpha y^T x - \\alpha ||x||^2 + \\frac{\\alpha}{2} ||y||^2 - \\alpha y^T x + \\frac{\\alpha}{2} ||x||^2$.\n            $= \\frac{\\alpha}{2} ||y||^2 = g(y)$.\n        *   So, for $g(x) = \\frac{\\alpha}{2} ||x||^2$, we have the equality $g(y) = g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} ||y-x||^2$. This means $g(x)$ is exactly $\\alpha$-strongly convex.\n        *   Now consider $F_{\\alpha}(x) = f(x) + g(x)$.\n        *   The sum of a $\\mu_1$-strongly convex function and a $\\mu_2$-strongly convex function is $(\\mu_1 + \\mu_2)$-strongly convex.\n        *   Let's prove this. Let $h_1$ be $\\mu_1$-SC and $h_2$ be $\\mu_2$-SC.\n            $h_1(y) \\geq h_1(x) + \\nabla h_1(x)^T(y-x) + \\frac{\\mu_1}{2} ||y-x||^2$.\n            $h_2(y) \\geq h_2(x) + \\nabla h_2(x)^T(y-x) + \\frac{\\mu_2}{2} ||y-x||^2$.\n        *   Summing these two inequalities:\n            $(h_1(y) + h_2(y)) \\geq (h_1(x) + h_2(x)) + (\\nabla h_1(x) + \\nabla h_2(x))^T(y-x) + \\frac{(\\mu_1 + \\mu_2)}{2} ||y-x||^2$.\n        *   This is exactly the definition of strong convexity for $h_1+h_2$ with modulus $\\mu_1 + \\mu_2$.\n        *   In our case, $f(x)$ is $\\mu_0$-strongly convex and $g(x) = \\frac{\\alpha}{2} ||x||^2$ is $\\alpha$-strongly convex.\n        *   Therefore, $F_{\\alpha}(x) = f(x) + g(x)$ is $(\\mu_0 + \\alpha)$-strongly convex.\n        *   The strong convexity modulus is $\\mu_{\\alpha} = \\mu_0 + \\alpha$.\n\n    *   **Summary for Part 1:**\n        *   $L_{\\alpha} = L + \\alpha$.\n        *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$.\n        *   These seem correct and were derived using the fundamental definitions as requested.\n\n**Part 2: Optimize the condition number subject to a constraint.**\n\n1.  **State the objective:** Minimize $\\sqrt{\\frac{L_{\\alpha}}{\\mu_{\\alpha}}}$ with respect to $\\alpha \\geq 0$.\n2.  **State the quantities from Part 1:**\n    *   $L_{\\alpha} = L + \\alpha$.\n    *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$.\n3.  **State the constraint:**\n    *   $L_{\\alpha} \\leq (1+\\rho)L$.\n    *   Substituting $L_{\\alpha}$: $L + \\alpha \\leq (1+\\rho)L = L + \\rho L$.\n    *   This simplifies to $\\alpha \\leq \\rho L$.\n    *   We also have the implicit constraint $\\alpha \\geq 0$.\n    *   So, the feasible range for $\\alpha$ is $0 \\leq \\alpha \\leq \\rho L$.\n4.  **Formulate the optimization problem:**\n    *   Minimize $g(\\alpha) = \\sqrt{\\frac{L + \\alpha}{\\mu_0 + \\alpha}}$\n    *   Subject to $0 \\leq \\alpha \\leq \\rho L$.\n5.  **Analyze the objective function $g(\\alpha)$.**\n    *   Minimizing $\\sqrt{x}$ is equivalent to minimizing $x$ for $x \\geq 0$.\n    *   So, let's minimize $h(\\alpha) = \\frac{L + \\alpha}{\\mu_0 + \\alpha}$.\n    *   To find the behavior of $h(\\alpha)$, let's take its derivative with respect to $\\alpha$.\n    *   $h'(\\alpha) = \\frac{d}{d\\alpha} \\left( \\frac{L + \\alpha}{\\mu_0 + \\alpha} \\right)$.\n    *   Using the quotient rule:\n        $h'(\\alpha) = \\frac{(1)(\\mu_0 + \\alpha) - (L + \\alpha)(1)}{(\\mu_0 + \\alpha)^2}$.\n        $h'(\\alpha) = \\frac{\\mu_0 + \\alpha - L - \\alpha}{(\\mu_0 + \\alpha)^2} = \\frac{\\mu_0 - L}{(\\mu_0 + \\alpha)^2}$.\n6.  **Analyze the sign of the derivative.**\n    *   The denominator $(\\mu_0 + \\alpha)^2$ is always positive.\n    *   The sign of $h'(\\alpha)$ is determined by the sign of the numerator, $\\mu_0 - L$.\n    *   The problem statement says $L > \\mu_0 \\ge 0$.\n    *   Therefore, $\\mu_0 - L < 0$.\n    *   So, $h'(\\alpha) < 0$ for all $\\alpha$ in its domain.\n7.  **Interpret the result.**\n    *   Since the derivative of $h(\\alpha)$ is always negative, the function $h(\\alpha)$ is a strictly decreasing function of $\\alpha$.\n    *   The function $g(\\alpha) = \\sqrt{h(\\alpha)}$ is also strictly decreasing.\n    *   To minimize a strictly decreasing function over a closed interval $[a, b]$, we must choose the largest possible value in the interval.\n    *   The interval for $\\alpha$ is $[0, \\rho L]$.\n    *   The largest possible value is $\\alpha = \\rho L$.\n8.  **Calculate the minimized factor.**\n    *   The optimal value of $\\alpha$ is $\\alpha_{opt} = \\rho L$.\n    *   Now, substitute this value back into the expression for the condition number factor.\n    *   Minimized factor = $\\sqrt{\\frac{L + \\alpha_{opt}}{\\mu_0 + \\alpha_{opt}}}$.\n    *   $= \\sqrt{\\frac{L + \\rho L}{\\mu_0 + \\rho L}}$.\n    *   $= \\sqrt{\\frac{L(1 + \\rho)}{\\mu_0 + \\rho L}}$.\n9.  **Final check of the steps and constraints.**\n    *   Problem statement: $f$ is differentiable convex, $\\nabla f$ is $L$-Lipschitz, $f$ is $\\mu_0$-strongly convex.\n    *   $L > \\mu_0 \\geq 0$.\n    *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$, with $\\alpha \\geq 0$.\n    *   Constraint: $L_{\\alpha} \\leq (1+\\rho)L$ for $0 < \\rho < 1$.\n    *   The derivations for $L_{\\alpha} = L+\\alpha$ and $\\mu_{\\alpha} = \\mu_0+\\alpha$ are sound and follow from the definitions.\n    *   The constraint $\\alpha \\leq \\rho L$ is correct.\n    *   The analysis of the objective function $\\frac{L+\\alpha}{\\mu_0+\\alpha}$ is correct. The derivative is indeed $\\frac{\\mu_0 - L}{(\\mu_0+\\alpha)^2}$.\n    *   Since $L > \\mu_0$, the derivative is negative. The function is decreasing.\n    *   The minimum value is achieved at the maximum allowed value of $\\alpha$.\n    *   The maximum allowed value is $\\alpha = \\rho L$.\n    *   The resulting minimal value is $\\sqrt{\\frac{L(1+\\rho)}{\\mu_0 + \\rho L}}$.\n    *   This expression is in terms of $L$, $\\mu_0$, and $\\rho$ as requested. It is a single closed-form expression.\n    *   All steps seem logically sound and mathematically correct.", "answer": "$$\n\\boxed{\\sqrt{\\frac{L(1+\\rho)}{\\mu_{0}+\\rho L}}}\n$$", "id": "3183345"}, {"introduction": "The smoothness constant $L$ provides a powerful theoretical guarantee: for gradient descent to converge, the step size $\\alpha$ must be bounded, typically by $\\alpha \\lt 2/L$. This hands-on coding exercise [@problem_id:3183363] lets you put this theory to the test by designing a numerical experiment. You will observe the stark difference in algorithm behavior—from stable convergence to explosive divergence—as the step size crosses this critical threshold, building a strong intuition for the practical importance of smoothness.", "problem": "You are to design and implement a numerical experiment that examines the stability of gradient descent for unconstrained quadratic minimization through the lens of smoothness and Lipschitz continuity of the gradient. Work entirely in the setting of a quadratic objective of the form $$f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x,$$ where $A$ is a symmetric positive definite (SPD) matrix and $b$ is a vector. The gradient of $f$ is given by $$\\nabla f(x) = A x - b.$$ The gradient map is said to be Lipschitz continuous with constant $L$ if $$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|$$ for all $x,y$ in the domain, where $\\|\\cdot\\|$ denotes the Euclidean norm. In this quadratic setting with SPD $A$, the Lipschitz constant of the gradient equals the spectral norm of $A$, which coincides with the largest eigenvalue of $A$.\n\nStarting from the basic gradient descent iteration $$x_{k+1} = x_k - \\alpha \\nabla f(x_k),$$ where $\\alpha$ is a fixed step size, express the error $e_k = x_k - x^{\\star}$ relative to the minimizer $x^{\\star}$ (which satisfies $A x^{\\star} = b$) and analyze the linear error dynamics. The stability of the iteration can be characterized by the spectral radius of a certain matrix derived from $A$ and $\\alpha$. Based on this characterization, there is a critical step-size boundary associated with the Lipschitz constant of the gradient that separates stable behavior from oscillatory or divergent behavior. Your program must:\n\n- Compute the Lipschitz constant $L$ of the gradient as the largest eigenvalue of $A$.\n- Compute the theoretical stability/divergence classification by checking whether the spectral radius of the linear error propagation operator exceeds $1$.\n- Run gradient descent numerically on the error dynamics for a fixed number of iterations, starting from a specified initial point, and determine whether divergence is observed numerically. For this experiment, define “observed divergence” as occurring if both of the following hold:\n  1. The maximum error norm during the run exceeds the initial error norm by at least a multiplicative factor $r_{\\text{grow}}$.\n  2. The final error norm exceeds the initial error norm by at least a multiplicative factor $r_{\\text{final}}$.\n  \nUse $N$ iterations, with $N$ specified below, and fixed thresholds $r_{\\text{grow}}$ and $r_{\\text{final}}$ specified below. Your output for each test case must be a boolean indicating whether the observed divergence matches the theoretical divergence classification.\n\nFundamental base definitions to be used:\n- Lipschitz continuity of the gradient: $$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|.$$\n- Gradient descent update: $$x_{k+1} = x_k - \\alpha \\nabla f(x_k).$$\n- Error dynamics relative to the minimizer $$x^{\\star}$$ satisfying $$A x^{\\star} = b.$$\n\nImplement the following test suite. Each case specifies $A$, $b$, $x_0$, and $\\alpha$. Treat one-dimensional cases with $A$ as a $1 \\times 1$ matrix and vectors of length $1$. Use $N = 200$ iterations, $r_{\\text{grow}} = 1.5$, and $r_{\\text{final}} = 1.2$ for all cases.\n\n- Case $1$ (one-dimensional, step size below the boundary): $$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.30.$$\n- Case $2$ (one-dimensional, step size at the boundary): $$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.40.$$\n- Case $3$ (one-dimensional, step size above the boundary): $$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.41.$$\n- Case $4$ (two-dimensional, step size below the boundary): $$A = \\begin{bmatrix}3 & 1 \\\\ 1 & 3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.49.$$\n- Case $5$ (two-dimensional, step size at the boundary): $$A = \\begin{bmatrix}3 & 1 \\\\ 1 & 3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.50.$$\n- Case $6$ (two-dimensional, step size above the boundary): $$A = \\begin{bmatrix}3 & 1 \\\\ 1 & 3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.51.$$\n- Case $7$ (three-dimensional, step size below the boundary): $$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.28.$$\n- Case $8$ (three-dimensional, step size above the boundary): $$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.29.$$\n\nYour program must:\n- For each case, compute the largest eigenvalue $L$ of $A$.\n- Compute the theoretical divergence classification by evaluating the spectral radius of the linear error-propagation operator and checking whether it exceeds $1$.\n- Run the numerical gradient descent on the error dynamics for $N = 200$ iterations with the specified $\\alpha$, and classify whether divergence is observed using $r_{\\text{grow}} = 1.5$ and $r_{\\text{final}} = 1.2$ as defined above.\n- Produce one boolean per case indicating whether the observed divergence matches the theoretical classification.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $$[result_1,result_2,\\dots,result_8]$$). Each $$result_i$$ must be a boolean.", "solution": "The problem asks for a numerical validation of the stability theory for gradient descent applied to an unconstrained quadratic minimization problem. We are given the objective function $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, where $A$ is a symmetric positive definite (SPD) matrix. The gradient descent update rule is $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$, with a fixed step size $\\alpha > 0$.\n\nFirst, we establish the theoretical framework for stability by analyzing the error dynamics. The gradient of the objective function is $\\nabla f(x) = A x - b$. The unique minimizer, denoted by $x^{\\star}$, is the solution to the linear system $\\nabla f(x^{\\star}) = 0$, which gives $A x^{\\star} = b$.\n\nThe error at iteration $k$ is defined as $e_k = x_k - x^{\\star}$. To derive the evolution of the error, we substitute the gradient and the relation $b = A x^{\\star}$ into the gradient descent update rule:\n$$x_{k+1} = x_k - \\alpha (A x_k - b)$$\n$$x_{k+1} = x_k - \\alpha (A x_k - A x^{\\star})$$\nSubtracting $x^{\\star}$ from both sides yields the error at iteration $k+1$:\n$$x_{k+1} - x^{\\star} = (x_k - x^{\\star}) - \\alpha A (x_k - x^{\\star})$$\n$$e_{k+1} = e_k - \\alpha A e_k$$\nThis simplifies to a linear dynamical system for the error vector:\n$$e_{k+1} = (I - \\alpha A) e_k$$\nwhere $I$ is the identity matrix. Let $B = I - \\alpha A$ be the error propagation matrix. The error at iteration $k$ is given by $e_k = B^k e_0$, where $e_0$ is the initial error.\n\nThe stability of the iteration, which corresponds to the convergence of the error $e_k$ to zero as $k \\to \\infty$ for any initial error $e_0$, is determined by the spectral radius of the propagation matrix $B$. The spectral radius, $\\rho(B)$, is the maximum of the absolute values of the eigenvalues of $B$. The iteration is stable and converges if and only if $\\rho(B) < 1$. It diverges if $\\rho(B) > 1$. If $\\rho(B) = 1$, the error does not converge to zero (unless it is already zero), but its norm might not necessarily grow to infinity.\n\nSince $A$ is symmetric, it has real eigenvalues $\\lambda_i$. The eigenvalues of $B = I - \\alpha A$ are $\\mu_i = 1 - \\alpha \\lambda_i$. The spectral radius is therefore:\n$$\\rho(B) = \\max_i |1 - \\alpha \\lambda_i|$$\nSince $A$ is positive definite, all $\\lambda_i > 0$. The condition for convergence, $\\rho(B) < 1$, becomes equivalent to $-1 < 1 - \\alpha \\lambda_i < 1$ for all $i$.\nThe right-hand inequality, $1 - \\alpha \\lambda_i < 1$, simplifies to $-\\alpha \\lambda_i < 0$, which is always true for $\\alpha > 0$ and $\\lambda_i > 0$.\nThe left-hand inequality, $-1 < 1 - \\alpha \\lambda_i$, simplifies to $\\alpha \\lambda_i < 2$, or $\\alpha < 2/\\lambda_i$. This must hold for all eigenvalues $\\lambda_i$. The most restrictive condition is imposed by the largest eigenvalue, $\\lambda_{\\max}(A)$. Thus, the step size must satisfy:\n$$\\alpha < \\frac{2}{\\lambda_{\\max}(A)}$$\nThe problem states that the Lipschitz constant of the gradient, $L$, is equal to $\\lambda_{\\max}(A)$. The stability condition is $\\alpha < 2/L$.\n\nTheoretical divergence is classified as occurring if the stability condition is violated such that $\\rho(I - \\alpha A) > 1$.\n\nThe numerical experiment is designed to verify this theoretical prediction. For each test case, we perform the following steps:\n1.  **Theoretical Classification**: We compute the eigenvalues of the matrix $A$, find the spectral radius $\\rho(I - \\alpha A)$, and determine if it is greater than $1$. This gives the `theoretical_divergence` boolean classification.\n2.  **Numerical Simulation**: We simulate the error dynamics $e_{k+1} = (I - \\alpha A)e_k$ for $N=200$ iterations.\n    - First, the minimizer $x^{\\star}$ is found by solving $A x^{\\star} = b$.\n    - The initial error $e_0 = x_0 - x^{\\star}$ and its Euclidean norm $\\|e_0\\|$ are computed.\n    - We iterate to find $e_1, e_2, \\dots, e_N$ and their norms.\n    - **Observed Divergence**: We check if the simulation meets the specified criteria for divergence:\n      - (1) The maximum error norm over the run, $\\max_{0 \\le k \\le N} \\|e_k\\|$, is at least $r_{\\text{grow}} = 1.5$ times the initial error norm $\\|e_0\\|$.\n      - (2) The final error norm, $\\|e_N\\|$, is at least $r_{\\text{final}} = 1.2$ times the initial error norm $\\|e_0\\|$.\n    - If both conditions are met, the behavior is classified as `observed_divergence`.\n3.  **Comparison**: For each case, we compare the theoretical and observed classifications. The expected result is that they match, yielding a boolean `True`. When $\\rho(B) > 1$, the error component corresponding to the dominant eigenvalue of $B$ grows exponentially as $(\\rho(B))^k$. For a sufficiently large number of iterations $N$, this growth will inevitably surpass the fixed thresholds $r_{\\text{grow}}$ and $r_{\\text{final}}$. Conversely, when $\\rho(B) \\le 1$, the error norm will not grow, so the divergence criteria will not be met.\n\nThe implementation will process each of the eight test cases according to this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the stability of gradient descent for quadratic minimization problems,\n    comparing theoretical predictions with numerical simulations.\n    \"\"\"\n    # Use float for all numerical values to ensure consistency.\n    test_cases = [\n        # Case 1 (1D, stable)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.30},\n        # Case 2 (1D, boundary)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.40},\n        # Case 3 (1D, divergent)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.41},\n        # Case 4 (2D, stable)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.49},\n        # Case 5 (2D, boundary)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.50},\n        # Case 6 (2D, divergent)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.51},\n        # Case 7 (3D, stable)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.28},\n        # Case 8 (3D, divergent)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.29},\n    ]\n\n    results = []\n    \n    # Simulation parameters\n    N = 200\n    r_grow = 1.5\n    r_final = 1.2\n    \n    for case in test_cases:\n        A, b, x0, alpha = case['A'], case['b'], case['x0'], case['alpha']\n        \n        # 1. Theoretical Analysis\n        # Since A is symmetric, eigvalsh is appropriate and returns real eigenvalues.\n        eigenvalues_A = np.linalg.eigvalsh(A)\n        # The eigenvalues of the error propagation matrix B = I - alpha * A\n        eigenvalues_B = 1.0 - alpha * eigenvalues_A\n        # The spectral radius of B\n        rho_B = np.max(np.abs(eigenvalues_B))\n        # Theoretical divergence occurs if the spectral radius is > 1\n        theoretical_divergence = rho_B > 1.0\n        \n        # 2. Numerical Simulation\n        # Solve for the minimizer x_star from A * x_star = b\n        x_star = np.linalg.solve(A, b)\n        \n        # Initial error vector and its norm\n        e_k = x0 - x_star\n        initial_error_norm = np.linalg.norm(e_k)\n        \n        # Handle the edge case where the initial guess is the solution\n        if initial_error_norm < 1e-12:\n            observed_divergence = False\n        else:\n            max_error_norm = initial_error_norm\n            \n            # Error propagation matrix B = I - alpha * A\n            B = np.identity(A.shape[0]) - alpha * A\n            \n            # Run the simulation on the error dynamics\n            for _ in range(N):\n                e_k = B @ e_k\n                current_norm = np.linalg.norm(e_k)\n                if current_norm > max_error_norm:\n                    max_error_norm = current_norm\n            \n            final_error_norm = np.linalg.norm(e_k)\n            \n            # Check the conditions for observed divergence\n            cond1 = (max_error_norm >= r_grow * initial_error_norm)\n            cond2 = (final_error_norm >= r_final * initial_error_norm)\n            observed_divergence = cond1 and cond2\n\n        # 3. Compare theoretical prediction with numerical observation\n        # The result is True if they match, False otherwise.\n        match = (observed_divergence == theoretical_divergence)\n        results.append(match)\n\n    # Format the final output as a string representing a list of booleans\n    # The default string representation of Python booleans is 'True' or 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3183363"}]}