{"hands_on_practices": [{"introduction": "The definiteness of a Hessian matrix is not just a theoretical property; it directly dictates the landscape of the objective function. This first exercise explores the practical consequences of this connection by examining an exact line search, a fundamental component of many optimization algorithms. By contrasting the outcomes for a convex (positive definite) versus a non-convex (indefinite) quadratic, you will discover firsthand how the presence of negative curvature can cause descent methods to behave in unexpected and counter-intuitive ways [@problem_id:3163346].", "problem": "Consider the unconstrained quadratic objective $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ with $Q \\in \\mathbb{R}^{n \\times n}$ symmetric. A matrix $Q$ is called Positive Definite (PD) if $x^{\\top} Q x  0$ for all nonzero $x$, and Indefinite if there exist nonzero $x$ and $y$ such that $x^{\\top} Q x  0$ and $y^{\\top} Q y  0$. In gradient-based optimization, an exact line search along a search direction $p$ from a current point $x$ is defined as choosing a step length $\\alpha$ that minimizes the univariate function $\\phi(\\alpha) = f(x + \\alpha p)$ by solving $\\frac{d}{d\\alpha}\\phi(\\alpha) = 0$.\n\nYour task is to compare exact line search behavior for a PD versus an Indefinite $Q$ using the steepest descent direction $p = -\\nabla f(x)$ at a chosen starting point. Work with the following two cases:\n\n- PD case: $Q_{\\text{PD}} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$ and $x_{\\text{PD}} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n- Indefinite case: $Q_{\\text{indef}} = \\begin{pmatrix} 1  0 \\\\ 0  -3 \\end{pmatrix}$ and $x_{\\text{indef}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nStarting from the fundamental definitions of the gradient and the exact line search, and without invoking pre-memorized shortcut formulas, perform the following for each case:\n\n1. Compute the steepest descent direction $p = -\\nabla f(x)$ at the given $x$.\n2. Derive and compute the exact line search step length $\\alpha^{\\star}$ that satisfies $\\frac{d}{d\\alpha} f(x + \\alpha p) = 0$.\n3. Evaluate the change in the objective $\\Delta f = f(x + \\alpha^{\\star} p) - f(x)$.\n\nReturn your final answers as the row matrix $\\begin{pmatrix} \\alpha^{\\star}_{\\text{PD}}  \\alpha^{\\star}_{\\text{indef}}  \\Delta f_{\\text{PD}}  \\Delta f_{\\text{indef}} \\end{pmatrix}$ in exact form. No rounding is required, and no units apply.", "solution": "The problem requires an analysis of the exact line search procedure for an unconstrained quadratic objective function $f(x) = \\frac{1}{2} x^{\\top} Q x$ under two different scenarios for the symmetric matrix $Q$: one where $Q$ is positive definite (PD) and one where it is indefinite. The search direction is specified as the steepest descent direction, $p = -\\nabla f(x)$.\n\nFirst, we establish the general formulas for the quantities of interest. The objective function is $f(x) = \\frac{1}{2} x^{\\top} Q x$. The gradient of this function with respect to $x$ is given by:\n$$ \\nabla f(x) = \\frac{1}{2} (Q + Q^{\\top}) x $$\nSince $Q$ is symmetric, $Q = Q^{\\top}$, which simplifies the gradient to:\n$$ \\nabla f(x) = Qx $$\nThe steepest descent direction $p$ at a point $x$ is defined as the negative of the gradient:\n$$ p = -\\nabla f(x) = -Qx $$\nAn exact line search seeks to find a step length $\\alpha$ that minimizes the univariate function $\\phi(\\alpha) = f(x + \\alpha p)$. We first construct this function:\n$$ \\phi(\\alpha) = f(x + \\alpha p) = \\frac{1}{2} (x + \\alpha p)^{\\top} Q (x + \\alpha p) $$\nExpanding this expression, we get:\n$$ \\phi(\\alpha) = \\frac{1}{2} (x^{\\top} + \\alpha p^{\\top}) Q (x + \\alpha p) = \\frac{1}{2} (x^{\\top}Qx + \\alpha x^{\\top}Qp + \\alpha p^{\\top}Qx + \\alpha^2 p^{\\top}Qp) $$\nSince $x^{\\top}Qp$ is a scalar, it is equal to its transpose, $(x^{\\top}Qp)^{\\top} = p^{\\top}Q^{\\top}x$. Given $Q=Q^{\\top}$, this simplifies to $p^{\\top}Qx$. Thus, the two middle terms are identical.\n$$ \\phi(\\alpha) = \\frac{1}{2} x^{\\top}Qx + \\alpha p^{\\top}Qx + \\frac{1}{2} \\alpha^2 p^{\\top}Qp $$\nThe problem defines the procedure for finding the step length $\\alpha^{\\star}$ as solving $\\frac{d}{d\\alpha}\\phi(\\alpha) = 0$. We compute the derivative with respect to $\\alpha$:\n$$ \\frac{d\\phi}{d\\alpha} = p^{\\top}Qx + \\alpha p^{\\top}Qp $$\nSetting the derivative to zero yields the stationary point $\\alpha^{\\star}$:\n$$ p^{\\top}Qx + \\alpha^{\\star} p^{\\top}Qp = 0 \\implies \\alpha^{\\star} = -\\frac{p^{\\top}Qx}{p^{\\top}Qp} $$\nWe can simplify this expression by substituting $Qx = -p$:\n$$ \\alpha^{\\star} = -\\frac{p^{\\top}(-p)}{p^{\\top}Qp} = \\frac{p^{\\top}p}{p^{\\top}Qp} $$\nThis expression for $\\alpha^{\\star}$ is valid as long as $p^{\\top}Qp \\neq 0$.\n\nThe change in the objective function is $\\Delta f = f(x + \\alpha^{\\star} p) - f(x)$. Using the expression for $\\phi(\\alpha)$, this is $\\phi(\\alpha^{\\star}) - \\phi(0)$.\n$$ \\Delta f = \\left( \\frac{1}{2} x^{\\top}Qx + \\alpha^{\\star} p^{\\top}Qx + \\frac{1}{2} (\\alpha^{\\star})^2 p^{\\top}Qp \\right) - \\frac{1}{2} x^{\\top}Qx $$\n$$ \\Delta f = \\alpha^{\\star} p^{\\top}Qx + \\frac{1}{2} (\\alpha^{\\star})^2 p^{\\top}Qp $$\nFrom the first-order condition $p^{\\top}Qx + \\alpha^{\\star} p^{\\top}Qp = 0$, we have $p^{\\top}Qx = -\\alpha^{\\star} p^{\\top}Qp$. Substituting this into the expression for $\\Delta f$:\n$$ \\Delta f = \\alpha^{\\star} (-\\alpha^{\\star} p^{\\top}Qp) + \\frac{1}{2} (\\alpha^{\\star})^2 p^{\\top}Qp = -(\\alpha^{\\star})^2 p^{\\top}Qp + \\frac{1}{2} (\\alpha^{\\star})^2 p^{\\top}Qp = -\\frac{1}{2} (\\alpha^{\\star})^2 p^{\\top}Qp $$\nAlternatively, substituting $p^{\\top}Qp = \\frac{p^{\\top}p}{\\alpha^{\\star}}$ yields another useful form:\n$$ \\Delta f = -\\frac{1}{2} (\\alpha^{\\star})^2 \\left( \\frac{p^{\\top}p}{\\alpha^{\\star}} \\right) = -\\frac{1}{2} \\alpha^{\\star} p^{\\top}p $$\n\nWe now apply these general formulas to the two specified cases.\n\n**Case 1: Positive Definite Matrix**\nThe given matrix and starting point are:\n$Q_{\\text{PD}} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$ and $x_{\\text{PD}} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\n1.  Compute the steepest descent direction $p_{\\text{PD}}$:\n    $\\nabla f(x_{\\text{PD}}) = Q_{\\text{PD}} x_{\\text{PD}} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(-2) \\\\ 1(1) + 3(-2) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -5 \\end{pmatrix}$.\n    $p_{\\text{PD}} = -\\nabla f(x_{\\text{PD}}) = \\begin{pmatrix} -2 \\\\ 5 \\end{pmatrix}$.\n\n2.  Compute the exact line search step length $\\alpha^{\\star}_{\\text{PD}}$ using $\\alpha^{\\star} = \\frac{p^{\\top}p}{p^{\\top}Qp}$:\n    $p_{\\text{PD}}^{\\top} p_{\\text{PD}} = (-2)^2 + 5^2 = 4 + 25 = 29$.\n    $Q_{\\text{PD}} p_{\\text{PD}} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 4(-2) + 1(5) \\\\ 1(-2) + 3(5) \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 13 \\end{pmatrix}$.\n    $p_{\\text{PD}}^{\\top} Q_{\\text{PD}} p_{\\text{PD}} = \\begin{pmatrix} -2  5 \\end{pmatrix} \\begin{pmatrix} -3 \\\\ 13 \\end{pmatrix} = (-2)(-3) + 5(13) = 6 + 65 = 71$.\n    $\\alpha^{\\star}_{\\text{PD}} = \\frac{29}{71}$.\n    The second derivative of $\\phi(\\alpha)$ is $\\frac{d^2\\phi}{d\\alpha^2} = p_{\\text{PD}}^{\\top} Q_{\\text{PD}} p_{\\text{PD}} = 71  0$. Since $Q_{\\text{PD}}$ is positive definite, this is guaranteed for any non-zero $p$. This positive second derivative confirms that $\\alpha^{\\star}_{\\text{PD}}$ corresponds to a minimum.\n\n3.  Evaluate the change in the objective $\\Delta f_{\\text{PD}}$ using $\\Delta f = -\\frac{1}{2} \\alpha^{\\star} p^{\\top}p$:\n    $\\Delta f_{\\text{PD}} = -\\frac{1}{2} \\left( \\frac{29}{71} \\right) (29) = -\\frac{841}{142}$. The objective function decreases, as expected for a minimization step.\n\n**Case 2: Indefinite Matrix**\nThe given matrix and starting point are:\n$Q_{\\text{indef}} = \\begin{pmatrix} 1  0 \\\\ 0  -3 \\end{pmatrix}$ and $x_{\\text{indef}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n1.  Compute the steepest descent direction $p_{\\text{indef}}$:\n    $\\nabla f(x_{\\text{indef}}) = Q_{\\text{indef}} x_{\\text{indef}} = \\begin{pmatrix} 1  0 \\\\ 0  -3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(1) \\\\ 0(1) + (-3)(1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$.\n    $p_{\\text{indef}} = -\\nabla f(x_{\\text{indef}}) = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$.\n\n2.  Compute the exact line search step length $\\alpha^{\\star}_{\\text{indef}}$:\n    $p_{\\text{indef}}^{\\top} p_{\\text{indef}} = (-1)^2 + 3^2 = 1 + 9 = 10$.\n    $Q_{\\text{indef}} p_{\\text{indef}} = \\begin{pmatrix} 1  0 \\\\ 0  -3 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1(-1) + 0(3) \\\\ 0(-1) + (-3)(3) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -9 \\end{pmatrix}$.\n    $p_{\\text{indef}}^{\\top} Q_{\\text{indef}} p_{\\text{indef}} = \\begin{pmatrix} -1  3 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -9 \\end{pmatrix} = (-1)(-1) + 3(-9) = 1 - 27 = -26$.\n    $\\alpha^{\\star}_{\\text{indef}} = \\frac{p_{\\text{indef}}^{\\top} p_{\\text{indef}}}{p_{\\text{indef}}^{\\top} Q_{\\text{indef}} p_{\\text{indef}}} = \\frac{10}{-26} = -\\frac{5}{13}$.\n    The second derivative of $\\phi(\\alpha)$ is $\\frac{d^2\\phi}{d\\alpha^2} = p_{\\text{indef}}^{\\top} Q_{\\text{indef}} p_{\\text{indef}} = -26  0$. This negative value indicates that the stationary point found is a local maximum, not a minimum. The function $\\phi(\\alpha)$ is a downward-opening parabola and is unbounded below, meaning a true minimizer for the line search does not exist. The step $\\alpha^\\star$ is the result of applying the specified procedure, which fails to find a minimum. The negative sign for $\\alpha^\\star$ means the step is taken in the direction of the gradient, not against it.\n\n3.  Evaluate the change in the objective $\\Delta f_{\\text{indef}}$:\n    $\\Delta f_{\\text{indef}} = -\\frac{1}{2} \\alpha^{\\star}_{\\text{indef}} p_{\\text{indef}}^{\\top} p_{\\text{indef}} = -\\frac{1}{2} \\left( -\\frac{5}{13} \\right) (10) = \\frac{50}{26} = \\frac{25}{13}$.\n    The objective function increases, which is consistent with moving to a local maximum along the search direction.\n\nFinally, we assemble the four computed values into the requested row matrix.\n$$ \\begin{pmatrix} \\alpha^{\\star}_{\\text{PD}}  \\alpha^{\\star}_{\\text{indef}}  \\Delta f_{\\text{PD}}  \\Delta f_{\\text{indef}} \\end{pmatrix} = \\begin{pmatrix} \\frac{29}{71}  -\\frac{5}{13}  -\\frac{841}{142}  \\frac{25}{13} \\end{pmatrix} $$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{29}{71}  -\\frac{5}{13}  -\\frac{841}{142}  \\frac{25}{13} \\end{pmatrix}}\n$$", "id": "3163346"}, {"introduction": "While the previous practice highlighted the failure of simple line searches on non-convex problems, more sophisticated algorithms are designed to handle this exact challenge. The trust-region method is a prime example, which intelligently uses information about the Hessian's indefiniteness to find a minimizer. This exercise guides you through solving a trust-region subproblem by changing to the eigenvector basis, providing a clear illustration of how to leverage a direction of negative curvature to decrease the objective function [@problem_id:3163285].", "problem": "Consider the quadratic trust-region subproblem from unconstrained optimization:\nminimize $q(\\mathbf{p}) = \\mathbf{g}^{\\top}\\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^{\\top}\\mathbf{H}\\mathbf{p}$ subject to $\\|\\mathbf{p}\\|_{2} \\leq \\Delta$, where $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Let the symmetric matrix $\\mathbf{H}$ have the eigen-decomposition $\\mathbf{H} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top}$ with\n$$\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ -1  1 \\end{pmatrix}, \\quad \\mathbf{\\Lambda} = \\begin{pmatrix} -2  0 \\\\ 0  1 \\end{pmatrix}.$$\nLet the gradient vector be $\\mathbf{g} = \\mathbf{Q}\\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$ and the trust-region radius be $\\Delta = 2$. Using only first principles (definitions of eigen-decomposition, the Euclidean norm, and the necessary optimality conditions for constrained minimization), solve the trust-region subproblem explicitly via the eigen-decomposition. Identify the negative curvature direction and explain its influence on the optimizer. Then compute the exact minimal objective value $q(\\mathbf{p}^{\\star})$. Give your final answer as a single exact number (no rounding).", "solution": "The problem is to solve the quadratic trust-region subproblem:\n$$ \\text{minimize} \\quad q(\\mathbf{p}) = \\mathbf{g}^{\\top}\\mathbfp + \\tfrac{1}{2}\\mathbf{p}^{\\top}\\mathbf{H}\\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_{2} \\leq \\Delta $$\nwhere the Hessian $\\mathbf{H}$ has the given eigen-decomposition $\\mathbf{H} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top}$, the gradient is $\\mathbf{g}$, and the trust-region radius is $\\Delta$.\n\nFirst, we validate the problem statement. The givens are:\n$\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ -1  1 \\end{pmatrix}$, $\\mathbf{\\Lambda} = \\begin{pmatrix} -2  0 \\\\ 0  1 \\end{pmatrix}$, $\\mathbf{g} = \\mathbf{Q}\\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$, and $\\Delta = 2$.\nThe matrix $\\mathbf{Q}$ is orthogonal, as $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\frac{1}{2}\\begin{pmatrix} 1  -1 \\\\ 1  1 \\end{pmatrix}\\begin{pmatrix} 1  1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\mathbf{I}$. The eigenvalues of $\\mathbf{H}$ are the diagonal entries of $\\mathbf{\\Lambda}$, which are $\\lambda_1 = -2$ and $\\lambda_2 = 1$. Since one eigenvalue is negative, $\\mathbf{H}$ is an indefinite matrix. The problem is a standard, well-posed subproblem in optimization theory. The data are consistent and complete. Thus, the problem is valid.\n\nTo solve the problem, we perform a change of variables using the eigenvector basis of $\\mathbf{H}$. Let $\\mathbf{p} = \\mathbf{Q}\\mathbf{s}$, where $\\mathbf{s} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$. Since $\\mathbf{Q}$ is an orthogonal matrix, this transformation is a rotation and/or reflection which preserves the Euclidean norm:\n$$ \\|\\mathbf{p}\\|_{2} = \\|\\mathbf{Q}\\mathbf{s}\\|_{2} = \\sqrt{(\\mathbf{Q}\\mathbf{s})^{\\top}(\\mathbf{Q}\\mathbf{s})} = \\sqrt{\\mathbf{s}^{\\top}\\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{s}} = \\sqrt{\\mathbf{s}^{\\top}\\mathbf{I}\\mathbf{s}} = \\|\\mathbf{s}\\|_{2} $$\nThe constraint $\\|\\mathbf{p}\\|_{2} \\leq \\Delta$ becomes $\\|\\mathbf{s}\\|_{2} \\leq \\Delta$.\n\nWe now rewrite the objective function $q(\\mathbf{p})$ in terms of $\\mathbf{s}$:\n$$ q(\\mathbf{p}) = \\mathbf{g}^{\\top}\\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^{\\top}\\mathbf{H}\\mathbf{p} = (\\mathbf{Q}\\hat{\\mathbf{g}})^{\\top}(\\mathbf{Q}\\mathbf{s}) + \\tfrac{1}{2}(\\mathbf{Q}\\mathbf{s})^{\\top}(\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top})(\\mathbf{Q}\\mathbf{s}) $$\nwhere $\\hat{\\mathbf{g}} = \\mathbf{Q}^{\\top}\\mathbf{g} = \\mathbf{Q}^{\\top}\\left(\\mathbf{Q}\\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}\\right) = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$.\nUsing the orthogonality of $\\mathbf{Q}$, the objective function simplifies to:\n$$ \\hat{q}(\\mathbf{s}) = \\hat{\\mathbf{g}}^{\\top}\\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{s} = \\hat{\\mathbf{g}}^{\\top}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{\\Lambda}\\mathbf{s} $$\nThe problem is transformed into an equivalent problem in the variable $\\mathbf{s}$:\n$$ \\text{minimize} \\quad \\hat{q}(\\mathbf{s}) = \\hat{\\mathbf{g}}^{\\top}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{\\Lambda}\\mathbf{s} \\quad \\text{subject to} \\quad \\|\\mathbf{s}\\|_{2} \\leq \\Delta $$\nSubstituting the given values $\\hat{\\mathbf{g}} = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$, $\\mathbf{\\Lambda} = \\begin{pmatrix} -2  0 \\\\ 0  1 \\end{pmatrix}$, and $\\Delta=2$:\n$$ \\hat{q}(s_1, s_2) = \\begin{pmatrix} 0  3 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} + \\tfrac{1}{2} \\begin{pmatrix} s_1  s_2 \\end{pmatrix} \\begin{pmatrix} -2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = 3s_2 - s_1^2 + \\tfrac{1}{2}s_2^2 $$\nThe constraint is $s_1^2 + s_2^2 \\leq 2^2 = 4$.\n\nThe term $-s_1^2$ in the objective function corresponds to the negative eigenvalue $\\lambda_1 = -2$. This term indicates that the objective function decreases as $|s_1|$ increases. Therefore, an unconstrained minimum of $\\hat{q}(\\mathbf{s})$ does not exist, and the solution to the constrained problem must lie on the boundary of the trust region, i.e., $\\|\\mathbf{s}\\|_{2} = \\Delta = 2$.\nOn the boundary, we have $s_1^2 + s_2^2 = 4$, which implies $s_1^2 = 4 - s_2^2$. The domain for $s_2$ is $[-2, 2]$. We can substitute $s_1^2$ into the objective function to obtain a one-dimensional problem in $s_2$:\n$$ \\hat{q}(s_2) = 3s_2 - (4-s_2^2) + \\tfrac{1}{2}s_2^2 = \\tfrac{3}{2}s_2^2 + 3s_2 - 4 $$\nWe need to find the minimum of this quadratic function of $s_2$ on the interval $[-2, 2]$. The function $\\hat{q}(s_2)$ is a parabola opening upwards. Its global minimum occurs at the vertex. The vertex is found by setting the derivative with respect to $s_2$ to zero:\n$$ \\frac{d\\hat{q}}{ds_2} = 3s_2 + 3 = 0 \\implies s_2 = -1 $$\nSince $s_2 = -1$ is within the interval $[-2, 2]$, this is the value of $s_2$ that minimizes $\\hat{q}(s_2)$.\nWith $s_2 = -1$, we find the corresponding value for $s_1^2$:\n$$ s_1^2 = 4 - s_2^2 = 4 - (-1)^2 = 3 \\implies s_1 = \\pm\\sqrt{3} $$\nThus, there are two solutions in the $\\mathbf{s}$-space: $\\mathbf{s}^{\\star} = \\begin{pmatrix} \\sqrt{3} \\\\ -1 \\end{pmatrix}$ and $\\mathbf{s}^{\\star} = \\begin{pmatrix} -\\sqrt{3} \\\\ -1 \\end{pmatrix}$. Both yield the same minimal objective value.\n\nThe negative curvature direction is the eigenvector corresponding to the negative eigenvalue $\\lambda_1 = -2$. This is the first column of $\\mathbf{Q}$, let's call it $\\mathbf{q}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The component of the solution $\\mathbf{p}$ in this direction is given by $s_1$. The optimization drives the solution to have the largest possible magnitude for this component, $|s_1| = \\sqrt{3}$, as permitted by the trust-region constraint. The condition $\\hat{g}_1 = (\\mathbf{Q}^{\\top}\\mathbf{g})_1 = 0$ is a special situation known as the \"hard case\" because the gradient has no component along the direction of negative curvature. This requires the solution to take a step along the negative curvature direction to reach the trust-region boundary to find the minimum.\n\nFinally, we compute the exact minimal value of the objective function, $q(\\mathbf{p}^{\\star})$, which is equal to $\\hat{q}(\\mathbf{s}^{\\star})$. We can use either of the optimal $\\mathbf{s}^{\\star}$ vectors. Let's use $s_1 = \\sqrt{3}$ and $s_2 = -1$:\n$$ q(\\mathbf{p}^{\\star}) = \\hat{q}(\\sqrt{3}, -1) = 3(-1) - (\\sqrt{3})^2 + \\tfrac{1}{2}(-1)^2 = -3 - 3 + \\tfrac{1}{2} = -6 + \\tfrac{1}{2} = -5.5 $$\nAlternatively, using the 1D function $\\hat{q}(s_2)$:\n$$ q(\\mathbf{p}^{\\star}) = \\hat{q}(s_2=-1) = \\tfrac{3}{2}(-1)^2 + 3(-1) - 4 = \\tfrac{3}{2} - 3 - 4 = \\tfrac{3}{2} - 7 = \\tfrac{3-14}{2} = -\\tfrac{11}{2} $$\nThe minimal objective value is $-5.5$.", "answer": "$$\\boxed{-\\frac{11}{2}}$$", "id": "3163285"}, {"introduction": "An alternative to navigating a non-convex landscape is to approximate it with a convex one. This practice introduces a powerful technique for doing so: projecting an indefinite matrix onto the cone of positive semidefinite (PSD) matrices to find its 'closest' PSD counterpart. You will first derive this projection operator from fundamental principles and then analyze how this modification dramatically alters the solution to a constrained optimization problem, highlighting the stability benefits of ensuring convexity [@problem_id:3163344].", "problem": "Let $S^{n}$ denote the space of $n \\times n$ real symmetric matrices equipped with the Frobenius norm $\\|X\\|_{F} = \\sqrt{\\sum_{i,j} X_{ij}^{2}}$. The positive semidefinite (PSD) cone is $\\mathcal{S}_{+}^{n} = \\{X \\in S^{n} : X \\succeq 0\\}$. The orthogonal group is $\\mathcal{O}(n) = \\{U \\in \\mathbb{R}^{n \\times n} : U^{\\top} U = I\\}$. Consider the projection problem: given $A \\in S^{n}$, find $\\Pi_{\\mathcal{S}_{+}^{n}}(A) = \\arg\\min_{X \\in \\mathcal{S}_{+}^{n}} \\|X - A\\|_{F}^{2}$.\n\nStarting only from the following foundational facts: (i) the spectral theorem for real symmetric matrices (every $A \\in S^{n}$ can be written as $A = U \\Lambda U^{\\top}$ with $U \\in \\mathcal{O}(n)$ and diagonal $\\Lambda$ of real eigenvalues), and (ii) the orthogonal invariance of the Frobenius norm ($\\|U^{\\top} X U\\|_{F} = \\|X\\|_{F}$ for $U \\in \\mathcal{O}(n)$), do the following:\n\n1. Derive the form of $\\Pi_{\\mathcal{S}_{+}^{n}}(A)$ in terms of the eigenvalues and eigenvectors of $A$ without assuming any pre-memorized projection formula. Your derivation must rely only on the facts above and basic properties of convex minimization and the PSD cone.\n\n2. Apply your derivation to the explicit matrix $A \\in S^{3}$ given by\n$$\nA = \\begin{pmatrix}\n3  0  0 \\\\\n0  1  0 \\\\\n0  0  -0.1\n\\end{pmatrix}.\n$$\nCompute $P = \\Pi_{\\mathcal{S}_{+}^{3}}(A)$ and the Frobenius projection error $\\|A - P\\|_{F}^{2}$.\n\n3. Consider the constrained quadratic optimization problem over the unit ball\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\;\\; f_{M}(x) = \\frac{1}{2}\\, x^{\\top} M x + b^{\\top} x \\quad \\text{subject to} \\quad \\|x\\|_{2} \\leq 1,\n$$\nwith $b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0.001 \\end{pmatrix}$ and two choices of the quadratic matrix: $M = A$ and $M = P$. Compute the exact optimal values $v_{A}^{\\star}$ and $v_{P}^{\\star}$ of $f_{A}$ and $f_{P}$, respectively. Then form the scalar\n$$\nS \\;=\\; \\|A - P\\|_{F}^{2} \\;+\\; \\big|\\, v_{A}^{\\star} - v_{P}^{\\star} \\,\\big|.\n$$\n\nProvide your final answer as the exact value of $S$. If you choose to write $S$ as a decimal, ensure it is exact; no rounding is required. Your final answer must be a single real number.", "solution": "The problem is validated as being scientifically grounded, well-posed, and objective. It is a standard problem in matrix analysis and optimization.\n\nThe solution is presented in three parts as requested by the problem statement.\n\nPart 1: Derivation of the projection onto the positive semidefinite cone.\n\nThe problem is to find $\\Pi_{\\mathcal{S}_{+}^{n}}(A) = \\arg\\min_{X \\in \\mathcal{S}_{+}^{n}} \\|X - A\\|_{F}^{2}$ for a given symmetric matrix $A \\in S^{n}$. We are given two facts:\n(i) The spectral theorem for real symmetric matrices: any $A \\in S^{n}$ can be decomposed as $A = U \\Lambda U^{\\top}$, where $U \\in \\mathcal{O}(n)$ is an orthogonal matrix whose columns are the eigenvectors of $A$, and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ is a diagonal matrix of the corresponding real eigenvalues.\n(ii) The orthogonal invariance of the Frobenius norm: for any $X \\in S^{n}$ and any $Q \\in \\mathcal{O}(n)$, $\\|Q^{\\top} X Q\\|_{F} = \\|X\\|_{F}$.\n\nWe begin by substituting the spectral decomposition of $A$ into the objective function:\n$$ \\|X - A\\|_{F}^{2} = \\|X - U \\Lambda U^{\\top}\\|_{F}^{2} $$\nUsing the orthogonal invariance of the norm with $Q = U$, we can multiply the expression inside the norm by $U^{\\top}$ on the left and $U$ on the right without changing the value:\n$$ \\|X - U \\Lambda U^{\\top}\\|_{F}^{2} = \\|U^{\\top}(X - U \\Lambda U^{\\top})U\\|_{F}^{2} = \\|U^{\\top}XU - U^{\\top}U\\Lambda U^{\\top}U\\|_{F}^{2} = \\|U^{\\top}XU - \\Lambda\\|_{F}^{2} $$\nLet us define a new variable $\\tilde{X} = U^{\\top}XU$. Since $U$ is orthogonal, the map $X \\mapsto \\tilde{X}$ is a bijection from $S^n$ to $S^n$. We must check how the constraint $X \\in \\mathcal{S}_{+}^{n}$ transforms. A matrix $X$ is positive semidefinite if and only if all its eigenvalues are non-negative. Since $\\tilde{X}$ and $X$ are similar matrices, they have the same eigenvalues. Thus, $X \\succeq 0$ if and only if $\\tilde{X} \\succeq 0$. The optimization problem is therefore equivalent to:\n$$ \\min_{\\tilde{X} \\in \\mathcal{S}_{+}^{n}} \\|\\tilde{X} - \\Lambda\\|_{F}^{2} $$\nThe Frobenius norm squared is the sum of the squares of the matrix elements. Let $\\tilde{X}_{ij}$ be the elements of $\\tilde{X}$ and $\\lambda_i$ be the diagonal elements of $\\Lambda$.\n$$ \\|\\tilde{X} - \\Lambda\\|_{F}^{2} = \\sum_{i,j=1}^{n} (\\tilde{X}_{ij} - \\Lambda_{ij})^2 = \\sum_{i=1}^{n} (\\tilde{X}_{ii} - \\lambda_i)^2 + \\sum_{i \\neq j} \\tilde{X}_{ij}^2 $$\nTo minimize this sum of non-negative terms, we can minimize them independently. The terms $\\sum_{i \\neq j} \\tilde{X}_{ij}^2$ are minimized when $\\tilde{X}_{ij} = 0$ for all $i \\neq j$. This implies that the optimal matrix $\\tilde{X}$ must be diagonal.\nLet the optimal matrix be $\\tilde{X}^{\\star} = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$. For a diagonal matrix, the condition of being positive semidefinite simplifies to requiring all its diagonal entries to be non-negative, i.e., $d_i \\ge 0$ for all $i \\in \\{1, \\dots, n\\}$.\nThe problem thus reduces to solving $n$ independent scalar minimization problems:\n$$ \\min_{d_i \\ge 0} (d_i - \\lambda_i)^2 \\quad \\text{for } i = 1, \\dots, n $$\nThis is the problem of projecting a real number $\\lambda_i$ onto the non-negative real line $[0, \\infty)$. The solution is clear: if $\\lambda_i \\ge 0$, the minimum is achieved at $d_i = \\lambda_i$. If $\\lambda_i  0$, the minimum is achieved at $d_i = 0$. This can be written compactly as $d_i = \\max(0, \\lambda_i)$.\nLet us denote $\\Lambda_+ = \\mathrm{diag}(\\max(0, \\lambda_1), \\dots, \\max(0, \\lambda_n))$. The solution in the transformed coordinates is $\\tilde{X}^{\\star} = \\Lambda_+$.\nTo find the solution $P = \\Pi_{\\mathcal{S}_{+}^{n}}(A)$ in the original coordinates, we transform back:\n$$ P = U \\tilde{X}^{\\star} U^{\\top} = U \\Lambda_+ U^{\\top} $$\nThis derivation provides the explicit form of the projection: perform the spectral decomposition of $A$, set any negative eigenvalues to zero, and reconstruct the matrix.\n\nPart 2: Application to the specific matrix $A$.\n\nThe given matrix is $A = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  -0.1 \\end{pmatrix}$.\nThis matrix is already diagonal, so its spectral decomposition is trivial. The eigenvectors are the standard basis vectors, so $U = I$, the $3 \\times 3$ identity matrix. The eigenvalues are the diagonal entries: $\\lambda_1 = 3$, $\\lambda_2 = 1$, and $\\lambda_3 = -0.1$.\nFollowing the formula derived in Part 1, we form the matrix $\\Lambda_+$ by taking the positive part of each eigenvalue:\n$\\max(0, \\lambda_1) = \\max(0, 3) = 3$.\n$\\max(0, \\lambda_2) = \\max(0, 1) = 1$.\n$\\max(0, \\lambda_3) = \\max(0, -0.1) = 0$.\nSo, $\\Lambda_+ = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix}$.\nThe projection is $P = \\Pi_{\\mathcal{S}_{+}^{3}}(A) = U \\Lambda_+ U^{\\top} = I \\Lambda_+ I = \\Lambda_+$.\n$$ P = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} $$\nNext, we compute the Frobenius projection error, $\\|A - P\\|_{F}^{2}$.\n$$ A - P = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  -0.1 \\end{pmatrix} - \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  -0.1 \\end{pmatrix} $$\nThe squared Frobenius norm is the sum of the squares of the elements:\n$$ \\|A - P\\|_{F}^{2} = (0)^2 + (0)^2 + \\dots + (-0.1)^2 = 0.01 $$\n\nPart 3: Constrained quadratic optimization.\n\nWe need to solve $\\min_{x \\in \\mathbb{R}^{3}, \\|x\\|_{2} \\leq 1} f_{M}(x) = \\frac{1}{2}\\, x^{\\top} M x + b^{\\top} x$ for $M=A$ and $M=P$, with $b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0.001 \\end{pmatrix}$.\n\nCase 1: $M=A$.\nThe objective function is $f_{A}(x) = \\frac{1}{2} x^{\\top} A x + b^{\\top} x$. The Hessian of this quadratic function is $A = \\mathrm{diag}(3, 1, -0.1)$. Since $A$ has a negative eigenvalue ($-0.1$), $f_A(x)$ is non-convex, and its minimum over the unit ball must lie on the boundary, i.e., $\\|x\\|_{2}=1$.\nThis is a trust-region subproblem. The Karush-Kuhn-Tucker (KKT) conditions state that the solution $x^{\\star}$ must satisfy $(A + \\mu I)x^{\\star} = -b$ for some Lagrange multiplier $\\mu \\ge 0$. The second-order necessary condition requires that $A + \\mu I \\succeq 0$. This means all eigenvalues of $A+\\mu I$ must be non-negative: $3+\\mu \\ge 0$, $1+\\mu \\ge 0$, and $-0.1+\\mu \\ge 0$. The tightest of these for $\\mu \\ge 0$ is $\\mu \\ge 0.1$.\nWe solve for $x$: $x = -(A+\\mu I)^{-1}b$.\n$$ x = -\\begin{pmatrix} (3+\\mu)^{-1}  0  0 \\\\ 0  (1+\\mu)^{-1}  0 \\\\ 0  0  (-0.1+\\mu)^{-1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0.001 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -0.001 / (-0.1+\\mu) \\end{pmatrix} $$\nWe enforce the boundary condition $\\|x\\|_2=1$:\n$$ \\left( \\frac{-0.001}{-0.1+\\mu} \\right)^2 = 1 \\implies |-0.1+\\mu| = 0.001 $$\nSince we require $\\mu \\ge 0.1$, we must have $-0.1+\\mu  0$. Thus, $-0.1+\\mu = 0.001$, which gives $\\mu = 0.101$. This value satisfies the condition $\\mu \\ge 0.1$.\nThe optimal solution is $x_A^{\\star} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -0.001 / (-0.1+0.101) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix}$.\nThe optimal value is $v_{A}^{\\star} = f_{A}(x_A^{\\star})$:\n$$ v_{A}^{\\star} = \\frac{1}{2} (x_A^{\\star})^{\\top} A x_A^{\\star} + b^{\\top} x_A^{\\star} = \\frac{1}{2} \\begin{pmatrix} 0  0  -1 \\end{pmatrix} \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  -0.1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 0  0  0.001 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} $$\n$$ v_{A}^{\\star} = \\frac{1}{2}(-0.1) + (-0.001) = -0.05 - 0.001 = -0.051 $$\n\nCase 2: $M=P$.\nThe objective function is $f_{P}(x) = \\frac{1}{2} x^{\\top} P x + b^{\\top} x$. The Hessian is $P = \\mathrm{diag}(3, 1, 0)$, which is positive semidefinite. Thus $f_P(x)$ is convex, and this is a convex optimization problem.\n$$ f_P(x) = \\frac{1}{2}(3x_1^2 + x_2^2) + 0.001x_3 $$\nWe want to minimize this function subject to $x_1^2 + x_2^2 + x_3^2 \\le 1$. To minimize the sum, we should make the non-negative quadratic part as small as possible, which is achieved at $x_1=0$ and $x_2=0$. The problem reduces to minimizing $0.001x_3$ subject to $x_3^2 \\le 1$, or $-1 \\le x_3 \\le 1$.\nThe minimum value of $0.001x_3$ is achieved at the most negative value of $x_3$, so $x_3 = -1$.\nThe optimal solution is $x_P^{\\star} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\endpmatrix}$.\nThe optimal value is $v_{P}^{\\star} = f_{P}(x_P^{\\star})$:\n$$ v_{P}^{\\star} = \\frac{1}{2} (0) + 0.001(-1) = -0.001 $$\n\nFinal calculation of $S$.\nWe are asked to compute $S = \\|A - P\\|_{F}^{2} + |v_{A}^{\\star} - v_{P}^{\\star}|$.\nWe have $\\|A - P\\|_{F}^{2} = 0.01$.\nWe have $v_A^{\\star} = -0.051$ and $v_P^{\\star} = -0.001$.\n$$ |v_{A}^{\\star} - v_{P}^{\\star}| = |-0.051 - (-0.001)| = |-0.050| = 0.05 $$\nTherefore,\n$$ S = 0.01 + 0.05 = 0.06 $$\nThis is an exact decimal value.", "answer": "$$\\boxed{0.06}$$", "id": "3163344"}]}