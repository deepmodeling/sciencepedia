{"hands_on_practices": [{"introduction": "To begin, let's practice the fundamental skill of computing a Hessian matrix directly from a function. This exercise uses a hypothetical potential energy function, a common model in physics for describing the stability of a system near equilibrium. Calculating the Hessian will allow us to quantify the local curvature of this energy landscape, providing a concrete example of how this mathematical tool is applied in science [@problem_id:1643768].", "problem": "In solid-state physics, the potential energy of an atom in a crystal lattice, due to interactions with its neighbors, can often be approximated by a quadratic function of its displacement from a stable equilibrium position. This approximation is valid for small oscillations and is fundamental to understanding phonons and the thermal properties of solids.\n\nConsider a simplified two-dimensional model where a single particle's equilibrium position is at the origin $(0,0)$. The potential energy $U$ of the particle, when it is displaced by a small vector $\\mathbf{r} = (x, y)$, is described by the function:\n$$U(x, y) = \\alpha x^2 + \\beta y^2 + \\gamma xy$$\nHere, $\\alpha$, $\\beta$, and $\\gamma$ are real-valued constants that characterize the anisotropic nature of the potential well created by the surrounding crystal structure.\n\nThe local curvature of the potential energy surface at the equilibrium point is described by the Hessian matrix. This matrix is crucial for analyzing the stability of the equilibrium and determining the frequencies of small oscillations around it.\n\nYour task is to compute the Hessian matrix of the potential energy function $U(x, y)$, denoted by $H_U$, at the equilibrium point $(0,0)$. Express the matrix elements in terms of the constants $\\alpha$, $\\beta$, and $\\gamma$.", "solution": "We use the definition of the Hessian matrix for a scalar field $U(x,y)$:\n$$\nH_{U}(x,y)=\\begin{pmatrix}\n\\frac{\\partial^{2}U}{\\partial x^{2}} & \\frac{\\partial^{2}U}{\\partial x \\partial y} \\\\\n\\frac{\\partial^{2}U}{\\partial y \\partial x} & \\frac{\\partial^{2}U}{\\partial y^{2}}\n\\end{pmatrix}.\n$$\nGiven $U(x,y)=\\alpha x^{2}+\\beta y^{2}+\\gamma x y$, compute the first derivatives:\n$$\n\\frac{\\partial U}{\\partial x}=2\\alpha x+\\gamma y,\\qquad \\frac{\\partial U}{\\partial y}=2\\beta y+\\gamma x.\n$$\nThen the second derivatives are:\n$$\n\\frac{\\partial^{2}U}{\\partial x^{2}}=2\\alpha,\\qquad \\frac{\\partial^{2}U}{\\partial y^{2}}=2\\beta,\\qquad \\frac{\\partial^{2}U}{\\partial x \\partial y}=\\frac{\\partial^{2}U}{\\partial y \\partial x}=\\gamma.\n$$\nTherefore, the Hessian is constant in $(x,y)$ and at $(0,0)$ it is\n$$\nH_{U}(0,0)=\\begin{pmatrix}2\\alpha & \\gamma \\\\ \\gamma & 2\\beta\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2\\alpha & \\gamma \\\\ \\gamma & 2\\beta\\end{pmatrix}}$$", "id": "1643768"}, {"introduction": "Building on direct computation, we now explore the intimate relationship between a function's gradient and its Hessian. The Hessian matrix is precisely the Jacobian matrix of the gradient vector field. This exercise provides the gradient of a hypothetical function and asks you to find the Hessian, reinforcing your understanding of this connection and offering practice with more complex partial derivatives [@problem_id:1643797].", "problem": "Let $f: \\mathbb{R}^3 \\to \\mathbb{R}$ be a scalar function that is continuously twice-differentiable. The gradient of this function, $\\nabla f$, is known to be the vector field given by:\n$$\n\\nabla f(x, y, z) = \\left( y\\cos(x) + \\exp(z), \\sin(x) - z\\sin(y), \\cos(y) + x\\exp(z) \\right)\n$$\nYour task is to calculate the Hessian matrix of the function $f$, denoted by $H_f$, evaluated at the point $P = (0, \\frac{\\pi}{2}, 0)$.", "solution": "We are given $\\nabla f(x,y,z) = \\left(f_{x}, f_{y}, f_{z}\\right) = \\left(y\\cos(x) + \\exp(z), \\sin(x) - z\\sin(y), \\cos(y) + x\\exp(z)\\right)$. The Hessian matrix $H_{f}$ consists of all second-order partial derivatives:\n$$\nH_{f}(x,y,z)=\\begin{pmatrix}\nf_{xx} & f_{xy} & f_{xz}\\\\\nf_{yx} & f_{yy} & f_{yz}\\\\\nf_{zx} & f_{zy} & f_{zz}\n\\end{pmatrix}.\n$$\nCompute each second partial derivative by differentiating the components of the gradient:\n- From $f_{x}=y\\cos(x)+\\exp(z)$:\n$$\nf_{xx}=\\frac{\\partial}{\\partial x}(y\\cos(x)+\\exp(z))=-y\\sin(x),\\quad\nf_{xy}=\\frac{\\partial}{\\partial y}(y\\cos(x)+\\exp(z))=\\cos(x),\\quad\nf_{xz}=\\frac{\\partial}{\\partial z}(y\\cos(x)+\\exp(z))=\\exp(z).\n$$\n- From $f_{y}=\\sin(x)-z\\sin(y)$:\n$$\nf_{yx}=\\frac{\\partial}{\\partial x}(\\sin(x)-z\\sin(y))=\\cos(x),\\quad\nf_{yy}=\\frac{\\partial}{\\partial y}(\\sin(x)-z\\sin(y))=-z\\cos(y),\\quad\nf_{yz}=\\frac{\\partial}{\\partial z}(\\sin(x)-z\\sin(y))=-\\sin(y).\n$$\n- From $f_{z}=\\cos(y)+x\\exp(z)$:\n$$\nf_{zx}=\\frac{\\partial}{\\partial x}(\\cos(y)+x\\exp(z))=\\exp(z),\\quad\nf_{zy}=\\frac{\\partial}{\\partial y}(\\cos(y)+x\\exp(z))=-\\sin(y),\\quad\nf_{zz}=\\frac{\\partial}{\\partial z}(\\cos(y)+x\\exp(z))=x\\exp(z).\n$$\nThus, for general $(x,y,z)$,\n$$\nH_{f}(x,y,z)=\\begin{pmatrix}\n- y\\sin(x) & \\cos(x) & \\exp(z)\\\\\n\\cos(x) & - z\\cos(y) & - \\sin(y)\\\\\n\\exp(z) & - \\sin(y) & x\\exp(z)\n\\end{pmatrix}.\n$$\nEvaluate at $P=(0,\\frac{\\pi}{2},0)$ using $\\sin(0)=0$, $\\cos(0)=1$, $\\sin\\!\\left(\\frac{\\pi}{2}\\right)=1$, $\\cos\\!\\left(\\frac{\\pi}{2}\\right)=0$, and $\\exp(0)=1$:\n$$\nH_{f}(0,\\tfrac{\\pi}{2},0)=\\begin{pmatrix}\n- \\tfrac{\\pi}{2}\\cdot 0 & 1 & 1\\\\\n1 & - 0 \\cdot 0 & -1\\\\\n1 & -1 & 0 \\cdot 1\n\\end{pmatrix}\n=\\begin{pmatrix}\n0 & 1 & 1\\\\\n1 & 0 & -1\\\\\n1 & -1 & 0\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0 & 1 & 1 \\\\ 1 & 0 & -1 \\\\ 1 & -1 & 0\\end{pmatrix}}$$", "id": "1643797"}, {"introduction": "Moving from calculation to application, we now use the Hessian to analyze a function's behavior. The primary role of the Hessian in optimization is to classify critical points by describing the local geometry. This advanced exercise guides you through the complete process: finding a critical point, computing the Hessian's eigenvalues and eigenvectors, and using this information to identify a saddle point and a direction of negative curvature, a crucial skill for understanding optimization algorithms [@problem_id:3136114].", "problem": "Consider the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{2}-y^{2}$. Using only the core definitions of gradient and Hessian from multivariable calculus and the second-order characterization of curvature directions in unconstrained optimization, perform the following tasks:\n\n1) Determine the critical point(s) of $f$ by solving for $(x,y)$ such that the gradient $\\nabla f(x,y)$ is the zero vector.\n\n2) Construct the Hessian matrix $H(x,y)$ of $f$ from second-order partial derivatives, and evaluate it at the critical point found in part $1$.\n\n3) Compute the eigenvalues and a corresponding set of orthonormal eigenvectors of the Hessian at the critical point. Explain how the signs of the eigenvalues and the orientations of the eigenvectors characterize the local geometry of $f$ as a saddle.\n\n4) Based on the negative curvature indicated by the Hessian, identify a unit-length escape direction at the critical point that locally decreases $f$ to second order. For uniqueness, choose the unit eigenvector associated with the most negative eigenvalue whose second component is nonnegative.\n\nProvide as your final answer only the unit escape direction vector requested in part $4$, written as a row using the $\\text{pmatrix}$ environment. No numerical rounding is required.", "solution": "The function under consideration is $f:\\mathbb{R}^{2} \\to \\mathbb{R}$, defined by $f(x,y) = x^{2} - y^{2}$.\n\n1) First, we determine the critical points of $f$. A critical point $(x,y)$ is a point where the gradient of $f$, denoted $\\nabla f(x,y)$, is the zero vector. The gradient is the vector of first-order partial derivatives:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n$$\nWe compute the partial derivatives of $f(x,y) = x^{2} - y^{2}$:\n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^{2} - y^{2}) = 2x\n$$\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^{2} - y^{2}) = -2y\n$$\nThus, the gradient is:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix}\n$$\nTo find the critical points, we set the gradient to the zero vector, $\\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$\n\\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields a system of two linear equations:\n$2x = 0 \\implies x = 0$\n$-2y = 0 \\implies y = 0$\nThe only solution is $(x,y) = (0,0)$. Therefore, the function $f$ has a single critical point at the origin, $(0,0)$.\n\n2) Next, we construct the Hessian matrix, $H(x,y)$. The Hessian is the matrix of second-order partial derivatives:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix}\n$$\nWe compute the required second-order partial derivatives:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2x) = 2\n$$\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2y) = -2\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x}(-2y) = 0\n$$\nSince $f$ is a polynomial, its second-order partial derivatives are continuous, and by Clairaut's theorem, the mixed partial derivatives are equal: $\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 0$.\nThe Hessian matrix is therefore:\n$$\nH(x,y) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\nThe Hessian matrix is constant for all $(x,y) \\in \\mathbb{R}^{2}$. Evaluating it at the critical point $(0,0)$ gives:\n$$\nH(0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\n\n3) Now, we compute the eigenvalues and eigenvectors of the Hessian matrix $H = H(0,0)$. Since $H$ is a diagonal matrix, its eigenvalues are its diagonal entries.\nThe eigenvalues are $\\lambda_1 = 2$ and $\\lambda_2 = -2$.\n\nTo find the corresponding eigenvectors, we solve the equation $(H - \\lambda I)v = \\mathbf{0}$ for each eigenvalue $\\lambda$.\nFor $\\lambda_1 = 2$:\n$$\n(H - 2I)v = \\left( \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} - \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} \\right) \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & -4 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis equation simplifies to $-4v_y = 0$, which implies $v_y = 0$. The component $v_x$ is unconstrained. A corresponding eigenvector is of the form $\\begin{pmatrix} c \\\\ 0 \\end{pmatrix}$ for any $c \\ne 0$. A unit eigenvector is $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFor $\\lambda_2 = -2$:\n$$\n(H - (-2)I)v = \\left( \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} - \\begin{pmatrix} -2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\right) \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis equation simplifies to $4v_x = 0$, which implies $v_x = 0$. The component $v_y$ is unconstrained. A corresponding eigenvector is of the form $\\begin{pmatrix} 0 \\\\ c \\end{pmatrix}$ for any $c \\ne 0$. A unit eigenvector is $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nA set of orthonormal eigenvectors is $\\{v_1, v_2\\} = \\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}$.\n\nThe signs of the eigenvalues characterize the local geometry. The second-order Taylor expansion of $f$ around the critical point $x_c = (0,0)$ for a small displacement $d$ is given by $f(x_c + d) \\approx f(x_c) + \\nabla f(x_c)^T d + \\frac{1}{2} d^T H(x_c) d$. Since $\\nabla f(x_c) = \\mathbf{0}$, the change in function value is $\\Delta f \\approx \\frac{1}{2} d^T H d$. The term $d^T H d$ represents the local curvature.\n- The positive eigenvalue $\\lambda_1 = 2$ corresponds to the eigenvector $v_1 = (1,0)$, which is aligned with the $x$-axis. This indicates positive curvature in the $x$-direction. Moving away from the origin along the $x$-axis causes $f$ to increase, as shown by the cross-section $f(x,0) = x^2$.\n- The negative eigenvalue $\\lambda_2 = -2$ corresponds to the eigenvector $v_2 = (0,1)$, which is aligned with the $y$-axis. This indicates negative curvature in the $y$-direction. Moving away from the origin along the $y$-axis causes $f$ to decrease, as shown by the cross-section $f(0,y) = -y^2$.\nThe existence of eigenvalues of opposite signs (one positive, one negative) is the definitive second-order condition for a saddle point. The Hessian is indefinite, and the critical point $(0,0)$ is a strict saddle point.\n\n4) An escape direction is a direction $d$ along which the function locally decreases. This requires the quadratic form $d^T H d$ to be negative. The direction of most rapid decrease corresponds to the eigenvector associated with the most negative eigenvalue.\nThe most negative, and only negative, eigenvalue is $\\lambda_2 = -2$. The eigenspace associated with this eigenvalue is spanned by the vector $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The unit eigenvectors in this eigenspace are $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$.\nThe problem specifies a unique choice: \"choose the unit eigenvector associated with the most negative eigenvalue whose second component is nonnegative.\"\nComparing the two candidates:\n- For the vector $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, the second component is $1$, which is non-negative.\n- For the vector $\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$, the second component is $-1$, which is negative.\nTherefore, the required escape direction is $d = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe final answer is this unit vector, written as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 1\n\\end{pmatrix}\n}\n$$", "id": "3136114"}]}