{"hands_on_practices": [{"introduction": "The foundation of many powerful optimization algorithms lies in approximating a complex, nonlinear function with a simpler, local model. This first exercise provides practice in the fundamental skill of constructing a second-order Taylor polynomial, which serves as a quadratic approximation of a function near a specific point. Mastering this calculation [@problem_id:24079] is the first step toward understanding how methods like Newton's method or trust-region algorithms operate.", "problem": "The second-order Taylor polynomial of a twice-differentiable function $f(x, y)$ centered at the point $(a, b)$ provides a quadratic approximation of the function near that point. It is given by the formula:\n$$\nT_2(x, y) = f(a, b) + \\frac{\\partial f}{\\partial x}(a, b)(x - a) + \\frac{\\partial f}{\\partial y}(a, b)(y - b) + \\frac{1}{2!}\\left(\\frac{\\partial^2 f}{\\partial x^2}(a, b)(x - a)^2 + 2\\frac{\\partial^2 f}{\\partial x \\partial y}(a, b)(x - a)(y - b) + \\frac{\\partial^2 f}{\\partial y^2}(a, b)(y - b)^2\\right)\n$$\nConsider the function $f(x, y) = \\frac{x}{y+1}$. Derive the second-order Taylor polynomial for this function centered at the origin, $(a, b) = (0, 0)$.", "solution": "The problem asks for the second-order Taylor polynomial of the function $f(x, y) = \\frac{x}{y+1}$ centered at the point $(a, b) = (0, 0)$.\n\nThe general formula for the second-order Taylor polynomial at $(a, b)$ is:\n$$\nT_2(x, y) = f(a, b) + f_x(a, b)(x-a) + f_y(a, b)(y-b) + \\frac{1}{2} \\left[ f_{xx}(a, b)(x-a)^2 + 2f_{xy}(a, b)(x-a)(y-b) + f_{yy}(a, b)(y-b)^2 \\right]\n$$\nwhere the subscripts denote partial differentiation (e.g., $f_x = \\frac{\\partial f}{\\partial x}$ and $f_{xy} = \\frac{\\partial^2 f}{\\partial y \\partial x}$).\n\nSince we are centering the polynomial at the origin, $(a, b) = (0, 0)$, the formula simplifies to:\n$$\nT_2(x, y) = f(0, 0) + f_x(0, 0)x + f_y(0, 0)y + \\frac{1}{2} \\left[ f_{xx}(0, 0)x^2 + 2f_{xy}(0, 0)xy + f_{yy}(0, 0)y^2 \\right]\n$$\n\nWe must compute the function's value and its partial derivatives up to the second order, and then evaluate them at the point $(0, 0)$.\n\n**Step 1: Evaluate the function at (0, 0).**\n$$\nf(x, y) = \\frac{x}{y+1}\n$$\n$$\nf(0, 0) = \\frac{0}{0+1} = 0\n$$\n\n**Step 2: Compute the first-order partial derivatives and evaluate at (0, 0).**\n$$\nf_x(x, y) = \\frac{\\partial}{\\partial x}\\left(\\frac{x}{y+1}\\right) = \\frac{1}{y+1}\n$$\n$$\nf_x(0, 0) = \\frac{1}{0+1} = 1\n$$\n$$\nf_y(x, y) = \\frac{\\partial}{\\partial y}\\left(x(y+1)^{-1}\\right) = x(-1)(y+1)^{-2} = -\\frac{x}{(y+1)^2}\n$$\n$$\nf_y(0, 0) = -\\frac{0}{(0+1)^2} = 0\n$$\n\n**Step 3: Compute the second-order partial derivatives and evaluate at (0, 0).**\n$$\nf_{xx}(x, y) = \\frac{\\partial}{\\partial x}(f_x) = \\frac{\\partial}{\\partial x}\\left(\\frac{1}{y+1}\\right) = 0\n$$\n$$\nf_{xx}(0, 0) = 0\n$$\n$$\nf_{xy}(x, y) = \\frac{\\partial}{\\partial y}(f_x) = \\frac{\\partial}{\\partial y}\\left((y+1)^{-1}\\right) = -1(y+1)^{-2} = -\\frac{1}{(y+1)^2}\n$$\n$$\nf_{xy}(0, 0) = -\\frac{1}{(0+1)^2} = -1\n$$\n$$\nf_{yy}(x, y) = \\frac{\\partial}{\\partial y}(f_y) = \\frac{\\partial}{\\partial y}\\left(-\\frac{x}{(y+1)^2}\\right) = -x \\frac{\\partial}{\\partial y}\\left((y+1)^{-2}\\right) = -x(-2)(y+1)^{-3} = \\frac{2x}{(y+1)^3}\n$$\n$$\nf_{yy}(0, 0) = \\frac{2(0)}{(0+1)^3} = 0\n$$\n\n**Step 4: Substitute these values into the Taylor polynomial formula.**\n$$\nT_2(x, y) = f(0, 0) + f_x(0, 0)x + f_y(0, 0)y + \\frac{1}{2} \\left[ f_{xx}(0, 0)x^2 + 2f_{xy}(0, 0)xy + f_{yy}(0, 0)y^2 \\right]\n$$\n$$\nT_2(x, y) = 0 + (1)x + (0)y + \\frac{1}{2} \\left[ (0)x^2 + 2(-1)xy + (0)y^2 \\right]\n$$\n$$\nT_2(x, y) = x + \\frac{1}{2} (-2xy)\n$$\n$$\nT_2(x, y) = x - xy\n$$\nThe second-order Taylor polynomial for $f(x, y) = \\frac{x}{y+1}$ centered at the origin is $x - xy$.", "answer": "$$\\boxed{x - xy}$$", "id": "24079"}, {"introduction": "While the quadratic model is a powerful tool, it is crucial to remember that it is only a local approximation whose accuracy depends on higher-order derivatives. This practice problem [@problem_id:3191337] delves into the limitations of the second-order Taylor expansion by focusing on the remainder term, which captures the error between the true function and its model. By deriving a bound on this error, you will see how to make informed decisions in an optimization context, such as choosing a trust-region radius that ensures the model remains a reliable guide for finding a minimum.", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\|x\\|^{2}+\\alpha \\sin\\!\\big(\\beta\\,u^{\\top}x\\big)$, where $\\alpha>0$ and $\\beta>0$ are fixed scalars and $u\\in\\mathbb{R}^{n}$ is a fixed unit vector satisfying $\\|u\\|=1$. Let $m_{2}(s)$ denote the second-order Taylor model of $f$ around the center $x=0$, applied to a step $s\\in\\mathbb{R}^{n}$, which is the quadratic model used in trust-region (TR) methods in optimization. Using Taylor’s theorem for multivariate functions and only fundamental definitions and well-tested facts, derive a bound on the third-order remainder $R_{3}(s)$ that holds uniformly for all steps $s$ in the ball $\\{s\\in\\mathbb{R}^{n}:\\|s\\|\\leq \\Delta\\}$, by computing a Lipschitz constant for the Hessian of $f$ on that ball. Then, for a prescribed tolerance $\\tau>0$ measuring the maximum acceptable magnitude of the Taylor remainder, determine the largest trust-region radius $\\Delta=\\Delta(\\alpha,\\beta,\\tau)$ such that $|R_{3}(s)|\\leq\\tau$ for all steps $s$ with $\\|s\\|\\leq \\Delta$. Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\beta$, and $\\tau$. No numerical approximation or rounding is required.", "solution": "The problem is valid. It is a well-posed problem in multivariate calculus and numerical optimization, grounded in standard definitions and theorems. All necessary information is provided, and the objective is clearly stated. We proceed to the solution.\n\nThe problem requires us to find the largest trust-region radius $\\Delta$ for a function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$, defined as $f(x)=\\|x\\|^{2}+\\alpha \\sin(\\beta u^{\\top}x)$, where $\\alpha>0$, $\\beta>0$, and $u\\in\\mathbb{R}^{n}$ is a unit vector with $\\|u\\|=1$. The condition is that the third-order Taylor remainder, $R_{3}(s)$, around the center $x=0$, must satisfy $|R_{3}(s)| \\leq \\tau$ for a given tolerance $\\tau>0$, for all steps $s$ within the trust region, i.e., $\\|s\\|\\leq \\Delta$.\n\nAccording to Taylor's theorem for a function with a continuous third derivative, the expansion of $f$ around a point $x_c$ is given by:\n$$f(x_c+s) = f(x_c) + \\nabla f(x_c)^{\\top}s + \\frac{1}{2}s^{\\top}\\nabla^{2}f(x_c)s + R_{3}(s)$$\nwhere $s$ is the step vector, $\\nabla f$ is the gradient, and $\\nabla^{2}f$ is the Hessian matrix. The term $m_2(s) = f(x_c) + \\nabla f(x_c)^{\\top}s + \\frac{1}{2}s^{\\top}\\nabla^{2}f(x_c)s$ is the second-order Taylor model of $f$ around $x_c$. The remainder term $R_{3}(s)$ is of order $O(\\|s\\|^{3})$.\n\nA standard result from optimization theory provides a bound on this remainder. If the Hessian $\\nabla^2 f(x)$ is Lipschitz continuous with constant $L_H$ in a convex set containing $x_c$ and $x_c+s$, i.e., $\\|\\nabla^{2}f(y) - \\nabla^{2}f(z)\\| \\leq L_H\\|y-z\\|$ for all $y,z$ in the set, then the remainder is bounded by:\n$$|R_{3}(s)| = |f(x_c+s) - m_2(s)| \\leq \\frac{L_H}{6}\\|s\\|^{3}$$\nThe Lipschitz constant $L_H$ can be taken as the supremum of the norm of the third-order derivative tensor, $L_H = \\sup_x \\|\\nabla^3 f(x)\\|$.\n\nOur expansion is centered at $x_c=0$. First, we compute the necessary derivatives of $f(x) = x^{\\top}x + \\alpha \\sin(\\beta u^{\\top}x)$.\n\nThe gradient of $f(x)$ is:\n$$\\nabla f(x) = \\frac{\\partial}{\\partial x} \\left(x^{\\top}x + \\alpha \\sin(\\beta u^{\\top}x)\\right) = 2x + \\alpha \\cos(\\beta u^{\\top}x) (\\beta u) = 2x + \\alpha\\beta\\cos(\\beta u^{\\top}x)u$$\n\nThe Hessian matrix of $f(x)$ is the matrix of second partial derivatives:\n$$\\nabla^{2}f(x) = \\frac{\\partial}{\\partial x^{\\top}} \\left(2x + \\alpha\\beta\\cos(\\beta u^{\\top}x)u\\right) = 2I + \\alpha\\beta(-\\sin(\\beta u^{\\top}x) (\\beta u^{\\top}))u = 2I - \\alpha\\beta^{2}\\sin(\\beta u^{\\top}x)uu^{\\top}$$\nwhere $I$ is the $n\\times n$ identity matrix and $uu^{\\top}$ is an $n\\times n$ matrix.\n\nTo find the Lipschitz constant for the Hessian, we compute the third-order derivative of $f(x)$. This is a third-order tensor whose components are $\\frac{\\partial^3 f(x)}{\\partial x_i \\partial x_j \\partial x_k}$. Differentiating the Hessian:\n$$\\frac{\\partial}{\\partial x_k}\\left(\\nabla^{2}f(x)\\right)_{ij} = \\frac{\\partial}{\\partial x_k}\\left(2\\delta_{ij} - \\alpha\\beta^{2}\\sin(\\beta u^{\\top}x)u_{i}u_{j}\\right) = -\\alpha\\beta^{2}\\cos(\\beta u^{\\top}x)(\\beta u_{k})u_{i}u_{j} = -\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)u_{i}u_{j}u_{k}$$\nThe action of the third derivative tensor on three vectors $v_1, v_2, v_3 \\in \\mathbb{R}^n$ is:\n$$D^{3}f(x)[v_1,v_2,v_3] = \\sum_{i,j,k=1}^{n} \\frac{\\partial^3 f(x)}{\\partial x_i \\partial x_j \\partial x_k} (v_1)_i (v_2)_j (v_3)_k = -\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)\\sum_{i,j,k=1}^{n} u_{i}u_{j}u_{k}(v_1)_i (v_2)_j (v_3)_k$$\nThis simplifies to:\n$$D^{3}f(x)[v_1,v_2,v_3] = -\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)(u^{\\top}v_1)(u^{\\top}v_2)(u^{\\top}v_3)$$\n\nThe norm of this tensor at a point $x$ is defined as $\\|\\nabla^3 f(x)\\| = \\sup_{\\|v_1\\|=\\|v_2\\|=\\|v_3\\|=1} |D^{3}f(x)[v_1,v_2,v_3]|$.\n$$\\|\\nabla^3 f(x)\\| = \\sup_{\\|v_1\\|=\\|v_2\\|=\\|v_3\\|=1} |-\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)(u^{\\top}v_1)(u^{\\top}v_2)(u^{\\top}v_3)|$$\nSince $\\alpha>0$ and $\\beta>0$:\n$$\\|\\nabla^3 f(x)\\| = \\alpha\\beta^{3}|\\cos(\\beta u^{\\top}x)| \\sup_{\\|v_1\\|=\\|v_2\\|=\\|v_3\\|=1} |(u^{\\top}v_1)(u^{\\top}v_2)(u^{\\top}v_3)|$$\nBy the Cauchy-Schwarz inequality, $|u^{\\top}v| \\leq \\|u\\|\\|v\\|$. Since $\\|u\\|=1$ and we take $\\|v\\|=1$, we have $|u^{\\top}v| \\leq 1$. The supremum is achieved when $v_1, v_2, v_3$ are all chosen to be $u$ or $-u$, for which $|u^{\\top}v|=1$. Thus, the supremum is $1$.\n$$\\|\\nabla^3 f(x)\\| = \\alpha\\beta^{3}|\\cos(\\beta u^{\\top}x)|$$\n\nThe problem asks for a bound that holds uniformly for all steps $s$ in the ball $\\|s\\|\\leq \\Delta$. The remainder bound uses a Lipschitz constant $L_H$ for the Hessian valid on the ball of radius $\\Delta$ around $x=0$, which is the set $\\{x \\in \\mathbb{R}^n : \\|x\\| \\leq \\Delta\\}$.\n$$L_H = \\sup_{\\|x\\|\\leq\\Delta} \\|\\nabla^3 f(x)\\| = \\sup_{\\|x\\|\\leq\\Delta} \\alpha\\beta^{3}|\\cos(\\beta u^{\\top}x)|$$\nThe maximum value of $|\\cos(z)|$ is $1$, which occurs when $z$ is an integer multiple of $\\pi$. For any $\\Delta > 0$, the range of the argument $\\beta u^{\\top}x$ for $\\|x\\|\\le\\Delta$ is $[-\\beta \\Delta, \\beta \\Delta]$. This interval always contains $0$, and $\\cos(0)=1$. Hence:\n$$\\sup_{\\|x\\|\\leq\\Delta} |\\cos(\\beta u^{\\top}x)| = 1$$\nTherefore, the Lipschitz constant for the Hessian is $L_H = \\alpha\\beta^{3}$.\n\nWith this constant, the bound on the remainder $R_{3}(s)$ for any step $s$ is:\n$$|R_{3}(s)| \\leq \\frac{L_H}{6}\\|s\\|^{3} = \\frac{\\alpha\\beta^{3}}{6}\\|s\\|^{3}$$\n\nWe need to find the largest radius $\\Delta$ such that $|R_{3}(s)| \\leq \\tau$ for all steps $s$ with $\\|s\\| \\leq \\Delta$. This requires the upper bound on $|R_3(s)|$ to be no more than $\\tau$ throughout this region.\n$$\\frac{\\alpha\\beta^{3}}{6}\\|s\\|^{3} \\leq \\tau \\quad \\forall s \\text{ such that } \\|s\\|\\leq\\Delta$$\nThe left-hand side is a monotonically increasing function of $\\|s\\|$. Thus, the condition is satisfied if it holds for the maximum value of $\\|s\\|$, which is $\\Delta$.\n$$\\frac{\\alpha\\beta^{3}}{6}\\Delta^{3} \\leq \\tau$$\nTo find the largest $\\Delta$ that satisfies this inequality, we solve the corresponding equality:\n$$\\frac{\\alpha\\beta^{3}}{6}\\Delta^{3} = \\tau$$\n$$\\Delta^{3} = \\frac{6\\tau}{\\alpha\\beta^{3}}$$\nSolving for $\\Delta$, we obtain the largest possible radius for the trust region:\n$$\\Delta = \\left(\\frac{6\\tau}{\\alpha\\beta^{3}}\\right)^{\\frac{1}{3}}$$\nThis provides the required closed-form analytic expression for $\\Delta$ in terms of the given parameters $\\alpha$, $\\beta$, and $\\tau$.", "answer": "$$ \\boxed{\\left(\\frac{6\\tau}{\\alpha\\beta^{3}}\\right)^{\\frac{1}{3}}} $$", "id": "3191337"}, {"introduction": "The second-derivative test is a standard tool for classifying stationary points, but it is inconclusive when the Hessian matrix provides no information, such as when it is a zero matrix. This exercise [@problem_id:3191338] explores such a degenerate case, demonstrating that Taylor's theorem provides a systematic way to analyze the point using higher-order terms. By examining the cubic part of the expansion, you will not only classify the stationary point but also design a more sophisticated \"cubic-aware\" optimization step, illustrating the principles behind advanced regularization methods.", "problem": "Consider the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{3}+3\\,x\\,y^{2}$. Let $x^{\\star}=(0,0)$.\n\nTasks:\n- Verify that $x^{\\star}$ is a stationary point and that the Hessian of $f$ at $x^{\\star}$ is the zero matrix, while some third-order partial derivatives at $x^{\\star}$ are nonzero.\n- Using Taylor’s theorem for multivariate functions centered at $x^{\\star}$, classify the stationary point $x^{\\star}$.\n- In the spirit of trust-region methods, design a “cubic-aware” optimization step by minimizing the third-order Taylor model of $f$ centered at $x^{\\star}$ over the sphere $\\{s\\in\\mathbb{R}^{2}:\\|s\\|_{2}=r\\}$, where $r>0$ is given. Among all minimizers, choose the one with $y>0$. Provide the explicit optimizer step $s^{\\star}(r)$ as a single row vector.\n\nYour final answer must be the explicit expression of $s^{\\star}(r)$ in terms of $r$ only, written as a row matrix. No rounding is required.", "solution": "The problem asks us to analyze the function $f(x,y)=x^{3}+3\\,x\\,y^{2}$ at the point $x^{\\star}=(0,0)$, classify this stationary point, and then find an optimal step $s^{\\star}(r)$ by minimizing the third-order Taylor model of $f$ on a sphere of radius $r$.\n\nFirst, we validate that $x^{\\star}=(0,0)$ is a stationary point and that its Hessian is the zero matrix. We compute the gradient of $f$, denoted by $\\nabla f$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x} = 3x^{2}+3y^{2}\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 6xy\n$$\nThe gradient is $\\nabla f(x,y) = \\begin{pmatrix} 3x^{2}+3y^{2} \\\\ 6xy \\end{pmatrix}$.\nEvaluating at $x^{\\star}=(0,0)$, we get:\n$$\n\\nabla f(0,0) = \\begin{pmatrix} 3(0)^{2}+3(0)^{2} \\\\ 6(0)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nSince the gradient at $x^{\\star}=(0,0)$ is the zero vector, $x^{\\star}$ is a stationary point of $f$.\n\nNext, we compute the Hessian matrix of $f$, denoted by $\\nabla^{2}f$. The second-order partial derivatives are:\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}(3x^{2}+3y^{2}) = 6x\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}(6xy) = 6x\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(3x^{2}+3y^{2}) = 6y\n$$\nThe Hessian matrix is $\\nabla^{2}f(x,y) = \\begin{pmatrix} 6x & 6y \\\\ 6y & 6x \\end{pmatrix}$.\nEvaluating at $x^{\\star}=(0,0)$:\n$$\n\\nabla^{2}f(0,0) = \\begin{pmatrix} 6(0) & 6(0) \\\\ 6(0) & 6(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis confirms that the Hessian of $f$ at $x^{\\star}$ is the zero matrix. The second derivative test is inconclusive for classifying the stationary point.\n\nWe now compute the third-order partial derivatives at $x^{\\star}=(0,0)$:\n$$\n\\frac{\\partial^{3} f}{\\partial x^{3}} = 6 \\implies \\frac{\\partial^{3} f}{\\partial x^{3}}(0,0) = 6\n$$\n$$\n\\frac{\\partial^{3} f}{\\partial y^{3}} = 0 \\implies \\frac{\\partial^{3} f}{\\partial y^{3}}(0,0) = 0\n$$\n$$\n\\frac{\\partial^{3} f}{\\partial x^{2} \\partial y} = 0 \\implies \\frac{\\partial^{3} f}{\\partial x^{2} \\partial y}(0,0) = 0\n$$\n$$\n\\frac{\\partial^{3} f}{\\partial x \\partial y^{2}} = 6 \\implies \\frac{\\partial^{3} f}{\\partial x \\partial y^{2}}(0,0) = 6\n$$\nWe see that some third-order partial derivatives at $x^{\\star}$ are indeed nonzero.\n\nTo classify the stationary point, we use the Taylor expansion of $f$ around $x^{\\star}=(0,0)$. For a vector $s=(s_x, s_y)$, the expansion is:\n$f(s_x, s_y) = f(0,0) + \\nabla f(0,0)^{T}s + \\frac{1}{2}s^{T}\\nabla^{2}f(0,0)s + \\dots$\nSince $f(0,0)=0$, $\\nabla f(0,0)=0$, and $\\nabla^{2}f(0,0)=0$, the lowest-order non-vanishing term is the third-order term. The function $f(x,y)=x^3+3xy^2$ is a homogeneous polynomial of degree $3$. Its Taylor expansion about the origin is the function itself.\nThus, near the origin, the behavior of $f$ is given by $f(x,y) = x^3 + 3xy^2 = x(x^2 + 3y^2)$.\nIn any arbitrarily small neighborhood of $(0,0)$:\n- If we choose a point with $x > 0$, then $x^2+3y^2 > 0$ (unless $(x,y)=(0,0)$), so $f(x,y) > 0$.\n- If we choose a point with $x < 0$, then $f(x,y) < 0$.\nSince $f(0,0)=0$ and the function takes both positive and negative values in any neighborhood of $(0,0)$, the point $x^{\\star}=(0,0)$ is a saddle point (specifically, a degenerate or higher-order saddle point).\n\nFinally, we design the \"cubic-aware\" optimization step by minimizing the third-order Taylor model of $f$ centered at $x^{\\star}$ over the sphere $\\{s\\in\\mathbb{R}^{2}:\\|s\\|_{2}=r\\}$. Let $s = (s_x, s_y)$. The third-order Taylor model $m(s)$ is:\n$$\nm(s) = f(0,0) + \\nabla f(0,0)^{T}s + \\frac{1}{2}s^{T}\\nabla^{2}f(0,0)s + \\frac{1}{3!}\\sum_{i,j,k=1}^{2} \\frac{\\partial^3 f(0,0)}{\\partial x_i \\partial x_j \\partial x_k}s_i s_j s_k\n$$\nAs previously established, the first three terms are zero, and the third-order expansion matches the function itself. So we want to minimize $m(s_x, s_y) = s_x^3 + 3s_x s_y^2$ subject to the constraint $s_x^2 + s_y^2 = r^2$, where $r>0$.\n\nWe can parameterize the constraint using polar coordinates: $s_x = r\\cos\\theta$ and $s_y = r\\sin\\theta$ for $\\theta \\in [0, 2\\pi)$. Substituting this into the objective function gives a function of $\\theta$:\n$$\nh(\\theta) = (r\\cos\\theta)^{3} + 3(r\\cos\\theta)(r\\sin\\theta)^{2}\n$$\n$$\nh(\\theta) = r^{3}\\cos^{3}\\theta + 3r^{3}\\cos\\theta\\sin^{2}\\theta = r^{3}\\cos\\theta(\\cos^{2}\\theta + 3\\sin^{2}\\theta)\n$$\nUsing the identity $\\cos^{2}\\theta = 1 - \\sin^{2}\\theta$:\n$$\nh(\\theta) = r^{3}\\cos\\theta(1 - \\sin^{2}\\theta + 3\\sin^{2}\\theta) = r^{3}\\cos\\theta(1 + 2\\sin^{2}\\theta)\n$$\nTo find the minimum, we compute the derivative with respect to $\\theta$ and set it to zero:\n$$\n\\frac{dh}{d\\theta} = r^{3}[(-\\sin\\theta)(1+2\\sin^{2}\\theta) + (\\cos\\theta)(4\\sin\\theta\\cos\\theta)]\n$$\n$$\n\\frac{dh}{d\\theta} = r^{3}[-\\sin\\theta - 2\\sin^{3}\\theta + 4\\sin\\theta\\cos^{2}\\theta] = r^{3}\\sin\\theta[-1 - 2\\sin^{2}\\theta + 4\\cos^{2}\\theta]\n$$\nSetting $\\frac{dh}{d\\theta}=0$ (and since $r>0$), we have two cases:\nCase 1: $\\sin\\theta = 0$. This implies $\\theta=0$ or $\\theta=\\pi$.\n- If $\\theta=0$, $h(0)=r^{3}\\cos(0)(1+0) = r^{3}$.\n- If $\\theta=\\pi$, $h(\\pi)=r^{3}\\cos(\\pi)(1+0) = -r^{3}$.\n\nCase 2: $-1 - 2\\sin^{2}\\theta + 4\\cos^{2}\\theta = 0$. Using $\\cos^{2}\\theta = 1-\\sin^{2}\\theta$:\n$$\n-1 - 2\\sin^{2}\\theta + 4(1-\\sin^{2}\\theta) = 0 \\implies 3 - 6\\sin^{2}\\theta = 0 \\implies \\sin^{2}\\theta = \\frac{1}{2}\n$$\nThis also implies $\\cos^{2}\\theta = 1 - \\frac{1}{2} = \\frac{1}{2}$. The value of the objective function is $h(\\theta) = r^{3}\\cos\\theta(1 + 2(\\frac{1}{2})) = 2r^{3}\\cos\\theta$.\n- If $\\cos\\theta = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}$, then $h(\\theta) = 2r^{3}(\\frac{1}{\\sqrt{2}}) = \\sqrt{2}r^{3}$.\n- If $\\cos\\theta = -\\sqrt{\\frac{1}{2}} = -\\frac{1}{\\sqrt{2}}$, then $h(\\theta) = 2r^{3}(-\\frac{1}{\\sqrt{2}}) = -\\sqrt{2}r^{3}$.\n\nComparing all values $\\{r^3, -r^3, \\sqrt{2}r^3, -\\sqrt{2}r^3\\}$, the minimum value is $-\\sqrt{2}r^{3}$. This occurs when $\\cos\\theta = -1/\\sqrt{2}$ and $\\sin^2\\theta = 1/2$. The possible angles are $\\theta = \\frac{3\\pi}{4}$ and $\\theta = \\frac{5\\pi}{4}$.\nThe corresponding minimizers $s=(s_x,s_y)$ are:\n1. For $\\theta = \\frac{3\\pi}{4}$: $s_x = r\\cos(\\frac{3\\pi}{4}) = -\\frac{r}{\\sqrt{2}}$, $s_y = r\\sin(\\frac{3\\pi}{4}) = \\frac{r}{\\sqrt{2}}$. The minimizer is $(-\\frac{r}{\\sqrt{2}}, \\frac{r}{\\sqrt{2}})$.\n2. For $\\theta = \\frac{5\\pi}{4}$: $s_x = r\\cos(\\frac{5\\pi}{4}) = -\\frac{r}{\\sqrt{2}}$, $s_y = r\\sin(\\frac{5\\pi}{4}) = -\\frac{r}{\\sqrt{2}}$. The minimizer is $(-\\frac{r}{\\sqrt{2}}, -\\frac{r}{\\sqrt{2}})$.\n\nThe problem asks to choose the minimizer with $y > 0$, which corresponds to $s_y > 0$. This is the first case.\nThus, the explicit optimizer step is $s^{\\star}(r) = (-\\frac{r}{\\sqrt{2}}, \\frac{r}{\\sqrt{2}})$. Rationalizing the denominator gives $s^{\\star}(r) = (-\\frac{\\sqrt{2}}{2}r, \\frac{\\sqrt{2}}{2}r)$.\nWe express this as a row matrix.\n$$\ns^{\\star}(r) = \\begin{pmatrix} -\\frac{\\sqrt{2}}{2}r & \\frac{\\sqrt{2}}{2}r \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{\\sqrt{2}}{2}r & \\frac{\\sqrt{2}}{2}r\n\\end{pmatrix}\n}\n$$", "id": "3191338"}]}