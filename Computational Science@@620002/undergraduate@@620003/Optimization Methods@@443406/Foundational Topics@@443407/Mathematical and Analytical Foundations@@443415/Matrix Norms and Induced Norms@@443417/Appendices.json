{"hands_on_practices": [{"introduction": "An induced matrix norm measures the maximum \"stretching\" a matrix applies to a vector. Before finding this maximum, it's helpful to see the effect on a single vector. This exercise [@problem_id:2179377] provides a concrete, intuitive feel for this stretching by asking you to compute the amplification ratio $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$ for a specific matrix $A$ and vector $x$.", "problem": "In numerical linear algebra, vector norms and their corresponding induced matrix norms are fundamental tools for analyzing the behavior of linear transformations. The 1-norm (or Manhattan norm) for a vector $v \\in \\mathbb{R}^n$ is defined as $\\|v\\|_1 = \\sum_{i=1}^n |v_i|$. The induced matrix 1-norm for a matrix $M \\in \\mathbb{R}^{m \\times n}$ is the maximum absolute column sum, defined as $\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^m |M_{ij}|$.\n\nConsider the matrix $A$ and the vector $x$ given by:\n$$\nA = \\begin{pmatrix} -3  5 \\\\ 2  -1 \\end{pmatrix}, \\quad x = \\begin{pmatrix} 2 \\\\ -3 \\end{pmatrix}\n$$\nThe induced matrix norm $\\|A\\|_1$ provides an upper bound on the \"stretching\" effect of the matrix $A$ on any vector $x$ when measured with the 1-norm, according to the inequality $\\|Ax\\|_1 \\le \\|A\\|_1 \\|x\\|_1$.\n\nCalculate the specific \"stretching factor\" for the given vector $x$, which is the ratio $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$. Report your answer as a decimal rounded to three significant figures.", "solution": "We use the definition of the induced inequality for the 1-norm: for any matrix $A$ and vector $x$, the stretching factor for $x$ is $\\frac{\\|Ax\\|_{1}}{\\|x\\|_{1}}$. Compute $Ax$:\n$$\nAx=\\begin{pmatrix}-3  5 \\\\ 2  -1\\end{pmatrix}\\begin{pmatrix}2 \\\\ -3\\end{pmatrix}\n=\\begin{pmatrix}-3\\cdot 2+5\\cdot(-3) \\\\ 2\\cdot 2+(-1)\\cdot(-3)\\end{pmatrix}\n=\\begin{pmatrix}-6-15 \\\\ 4+3\\end{pmatrix}\n=\\begin{pmatrix}-21 \\\\ 7\\end{pmatrix}.\n$$\nCompute the 1-norms:\n$$\n\\|x\\|_{1}=|2|+|-3|=2+3=5,\\qquad \\|Ax\\|_{1}=|-21|+|7|=21+7=28.\n$$\nTherefore, the stretching factor is\n$$\n\\frac{\\|Ax\\|_{1}}{\\|x\\|_{1}}=\\frac{28}{5}=5.6.\n$$\nRounding to three significant figures gives $5.60$.", "answer": "$$\\boxed{5.60}$$", "id": "2179377"}, {"introduction": "Building on the idea of a stretching factor, the next logical step is to determine the maximum possible stretch a matrix can apply, which is the definition of the induced norm. This practice [@problem_id:3148401] guides you through a formal derivation of the closed-form expression for the induced matrix 1-norm, $\\|A\\|_1$. You will then apply this formula to a dataset matrix to see how imbalanced feature scales can dominate the norm, a critical consideration in machine learning.", "problem": "Consider a linear model used in optimization for machine learning, where a dataset is represented by a matrix $A \\in \\mathbb{R}^{m \\times n}$. Each column of $A$ corresponds to a feature, and each row corresponds to a sample. Assume the model uses coordinate-wise updates whose stability is influenced by how the linear map $x \\mapsto A x$ amplifies perturbations measured by the vector $1$-norm. Use only the following fundamental definitions: for a vector $x \\in \\mathbb{R}^{n}$, the $1$-norm is defined by $\\|x\\|_{1} = \\sum_{j=1}^{n} |x_{j}|$, and the induced matrix $1$-norm of $A$ is defined by\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}.\n$$\nStarting from these definitions and standard properties of absolute values and inequalities, derive a closed-form expression for $\\|A\\|_{1}$ in terms of the entries of $A$ and then evaluate it for the following dataset matrix (constructed to reflect a situation in which one feature is much more heavily scaled than the others):\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}.\n$$\nExplain, in words, how the derived expression connects to the sensitivity of feature-wise scaling in optimization, specifically why an imbalanced feature scale can dominate the induced matrix $1$-norm and thus the worst-case amplification of perturbations under the map $x \\mapsto A x$. Finally, report the exact numerical value of $\\|A\\|_{1}$ for the given matrix $A$. No rounding is required; provide the exact value.", "solution": "The problem asks for a derivation of the closed-form expression for the induced matrix $1$-norm, its evaluation for a given matrix $A$, and a conceptual explanation of its relevance to feature scaling in machine learning models. The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution.\n\nFirst, we will derive the formula for the induced matrix $1$-norm, denoted $\\|A\\|_1$, for a matrix $A \\in \\mathbb{R}^{m \\times n}$. The definition provided is:\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}\n$$\nwhere $x \\in \\mathbb{R}^{n}$ and the vector $1$-norm is $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$.\n\nLet $y = Ax$. The $i$-th component of the vector $y \\in \\mathbb{R}^m$ is given by $y_i = \\sum_{j=1}^{n} a_{ij} x_j$, where $a_{ij}$ are the entries of $A$. The $1$-norm of $Ax$ is then:\n$$\n\\|Ax\\|_1 = \\|y\\|_1 = \\sum_{i=1}^{m} |y_i| = \\sum_{i=1}^{m} \\left| \\sum_{j=1}^{n} a_{ij} x_j \\right|\n$$\nApplying the triangle inequality for absolute values, which states that $|\\sum_k z_k| \\le \\sum_k |z_k|$, to the inner sum, we get:\n$$\n\\left| \\sum_{j=1}^{n} a_{ij} x_j \\right| \\le \\sum_{j=1}^{n} |a_{ij} x_j| = \\sum_{j=1}^{n} |a_{ij}| |x_j|\n$$\nSubstituting this inequality back into the expression for $\\|Ax\\|_1$:\n$$\n\\|Ax\\|_1 \\le \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} |a_{ij}| |x_j| \\right)\n$$\nSince all terms are non-negative, we can interchange the order of summation:\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} \\sum_{i=1}^{m} |a_{ij}| |x_j| = \\sum_{j=1}^{n} \\left( |x_j| \\sum_{i=1}^{m} |a_{ij}| \\right)\n$$\nLet us define $C_j = \\sum_{i=1}^{m} |a_{ij}|$ as the sum of the absolute values of the entries in the $j$-th column of $A$. The inequality becomes:\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} C_j |x_j|\n$$\nNow, let $C_{\\max} = \\max_{1 \\le j \\le n} C_j = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|$. Since $C_j \\le C_{\\max}$ for all $j$, we can write:\n$$\n\\sum_{j=1}^{n} C_j |x_j| \\le \\sum_{j=1}^{n} C_{\\max} |x_j| = C_{\\max} \\sum_{j=1}^{n} |x_j| = C_{\\max} \\|x\\|_1\n$$\nCombining these inequalities, we have established an upper bound for the ratio $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$:\n$$\n\\|Ax\\|_1 \\le C_{\\max} \\|x\\|_1 \\implies \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max} \\quad \\text{for all } x \\neq 0\n$$\nThis implies that the supremum of the ratio is also less than or equal to $C_{\\max}$:\n$$\n\\|A\\|_1 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max}\n$$\nTo complete the derivation, we must show that this upper bound is attainable. That is, we must find a specific non-zero vector $x_0$ for which the equality $\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = C_{\\max}$ holds.\n\nLet $k$ be the index of the column for which the maximum absolute column sum is achieved, such that $C_k = C_{\\max}$. Consider the vector $x_0 = e_k$, where $e_k$ is the $k$-th standard basis vector in $\\mathbb{R}^n$. This vector has a $1$ in the $k$-th position and $0$s elsewhere.\nThe $1$-norm of this vector is $\\|x_0\\|_1 = \\|e_k\\|_1 = 1$.\nThe product $Ax_0 = Ae_k$ is the $k$-th column of the matrix $A$. Let's denote this column vector as $a_k$.\nThe $1$-norm of $Ax_0$ is therefore:\n$$\n\\|Ax_0\\|_1 = \\|a_k\\|_1 = \\sum_{i=1}^{m} |a_{ik}|\n$$\nBy our definition of the index $k$, this sum is exactly $C_k = C_{\\max}$.\nFor this specific choice of $x_0 = e_k$, the ratio becomes:\n$$\n\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = \\frac{C_{\\max}}{1} = C_{\\max}\n$$\nSince we have found a vector $x_0$ for which the ratio equals $C_{\\max}$, and we have also shown that the ratio can never exceed $C_{\\max}$, we conclude that the supremum must be exactly $C_{\\max}$. Thus, the closed-form expression for the induced matrix $1$-norm is the maximum absolute column sum:\n$$\n\\|A\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|\n$$\n\nNext, we evaluate this expression for the given matrix:\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}\n$$\nHere, $m=5$ and $n=3$. We calculate the absolute column sums $C_1$, $C_2$, and $C_3$.\nFor the first column ($j=1$):\n$$\nC_1 = |7.5| + |8.3| + |6.9| + |9.1| + |8.7| = 7.5 + 8.3 + 6.9 + 9.1 + 8.7 = 40.5\n$$\nFor the second column ($j=2$):\n$$\nC_2 = |0.2| + |-0.15| + |0.1| + |-0.05| + |0.25| = 0.2 + 0.15 + 0.1 + 0.05 + 0.25 = 0.75\n$$\nFor the third column ($j=3$):\n$$\nC_3 = |-0.1| + |0.05| + |0.2| + |-0.15| + |0.05| = 0.1 + 0.05 + 0.2 + 0.15 + 0.05 = 0.55\n$$\nThe induced matrix $1$-norm is the maximum of these sums:\n$$\n\\|A\\|_{1} = \\max(C_1, C_2, C_3) = \\max(40.5, 0.75, 0.55) = 40.5\n$$\n\nFinally, we explain the connection to feature scaling. The derived formula, $\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|$, reveals that the induced $1$-norm is determined solely by the column with the largest sum of absolute values. In the context given, each column of $A$ represents a feature, and its entries are the values of that feature across different samples. The sum $\\sum_i |a_{ij}|$ can be interpreted as a measure of the total magnitude or scale of the $j$-th feature in the dataset.\n\nThe induced norm $\\|A\\|_1$ quantifies the maximum amplification of a vector's $1$-norm under the linear transformation $x \\mapsto Ax$. It represents a worst-case sensitivity metric: any input perturbation $\\delta x$ to a vector $x$ will result in a perturbation $\\delta y = A\\delta x$ in the output, bounded by $\\|\\delta y\\|_1 \\le \\|A\\|_1 \\|\\delta x\\|_1$.\n\nThe given matrix $A$ was constructed to have one feature (the first column) with a much larger scale than the others. As our calculation shows, this imbalanced scaling causes the absolute sum of the first column ($C_1=40.5$) to completely dominate the sums of the other columns ($C_2=0.75$, $C_3=0.55$). Consequently, the overall norm $\\|A\\|_1$ is determined entirely by this single, heavily scaled feature. This implies that the stability and behavior of any optimization algorithm sensitive to this norm (like certain coordinate-wise methods) will be dictated by the worst-scaled feature. Perturbations aligned with this feature (i.e., concentrated in the first component of the input vector $x$) are amplified by a factor of $40.5$, while perturbations along other feature directions would be amplified by much smaller factors. Such a large, imbalanced amplification factor can lead to poor conditioning and instability in numerical methods, which is why feature scaling (e.g., standardizing columns to have a similar scale) is a critical preprocessing step for many optimization and machine learning algorithms.\n\nThe exact numerical value of $\\|A\\|_1$ is $40.5$.", "answer": "$$\n\\boxed{40.5}\n$$", "id": "3148401"}, {"introduction": "One of the most important applications of matrix norms is in numerical analysis, where they are used to quantify the sensitivity of a problem to errors in its input data. This advanced exercise [@problem_id:3148445] delves into the concept of the condition number, $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$, which provides a bound on error amplification when solving linear systems. You will construct a special matrix to demonstrate how the perceived stability of a problem can change dramatically depending on whether you measure error using the 1-norm or the $\\infty$-norm.", "problem": "Consider a square matrix of dimension $n \\geq 2$. Define the matrix $S \\in \\mathbb{R}^{n \\times n}$ by $s_{1j} = 1$ for $j = 2, 3, \\dots, n$ and $s_{ij} = 0$ otherwise. Construct the matrix $A = I - S$, where $I$ is the identity matrix. Use this explicit construction to analyze the sensitivity of solving the linear system $A\\mathbf{x} = \\mathbf{b}$ under perturbations of the data vector $\\mathbf{b}$.\n\nTasks:\n- Show that $A$ is invertible and find $A^{-1}$ using only fundamental properties of matrices and nilpotent operators.\n- Compute $\\|A^{-1}\\|_{1}$ and $\\|A^{-1}\\|_{\\infty}$ for this $A$.\n- Compute $\\|A\\|_{1}$ and $\\|A\\|_{\\infty}$ and then the corresponding condition numbers $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$, where $\\kappa_{p}(A) = \\|A\\|_{p} \\|A^{-1}\\|_{p}$ for an induced matrix norm.\n- For a perturbation $\\mathbf{b} \\mapsto \\mathbf{b} + \\delta \\mathbf{b}$, derive the worst-case upper bound on the relative solution error $\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|_{p} / \\|\\mathbf{x}\\|_{p}$ for $p \\in \\{1, \\infty\\}$ using only the definition of induced norms and submultiplicativity. Here, $\\mathbf{x}$ is the exact solution to $A\\mathbf{x} = \\mathbf{b}$ and $\\hat{\\mathbf{x}}$ solves $A\\hat{\\mathbf{x}} = \\mathbf{b} + \\delta \\mathbf{b}$.\n- Under the additional assumption that the relative data error is identical in both norms, i.e., $\\|\\delta \\mathbf{b}\\|_{1} / \\|\\mathbf{b}\\|_{1} = \\|\\delta \\mathbf{b}\\|_{\\infty} / \\|\\mathbf{b}\\|_{\\infty}$, determine the ratio of the two worst-case relative error bounds (in the $\\infty$-norm versus in the $1$-norm) as a function of $n$.\n\nProvide your final answer as a single analytic expression in $n$.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and complete. It represents a standard exercise in numerical linear algebra. The problem is valid and a solution will be provided.\n\nThe problem asks for a multi-step analysis of a specific matrix $A = I - S$. Let's proceed through each task.\n\nFirst, we define the matrix $S \\in \\mathbb{R}^{n \\times n}$ for $n \\ge 2$. Its elements are given by $s_{1j} = 1$ for $j \\in \\{2, 3, \\dots, n\\}$ and $s_{ij} = 0$ for all other pairs $(i, j)$. This means the only non-zero entries of $S$ are in the first row, starting from the second column.\nThe matrix $S$ has the form:\n$$S = \\begin{pmatrix}\n0  1  1  \\dots  1 \\\\\n0  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  0\n\\end{pmatrix}$$\n\n**Task 1: Show that $A$ is invertible and find $A^{-1}$.**\n\nThe matrix $A$ is defined as $A = I - S$. To show it is invertible and find its inverse, we first analyze the properties of $S$. Let's compute $S^2$. The element $(i, k)$ of $S^2$ is given by $(S^2)_{ik} = \\sum_{j=1}^{n} s_{ij} s_{jk}$.\n- If $i  1$, then $s_{ij} = 0$ for all $j$, so $(S^2)_{ik} = 0$.\n- If $i = 1$, then $(S^2)_{1k} = \\sum_{j=1}^{n} s_{1j} s_{jk}$. The only non-zero terms $s_{1j}$ are for $j \\in \\{2, 3, \\dots, n\\}$. Thus, the sum becomes $(S^2)_{1k} = \\sum_{j=2}^{n} s_{1j} s_{jk} = \\sum_{j=2}^{n} (1) s_{jk}$.\nFor $s_{jk}$ to be non-zero, we must have $j=1$. However, the sum runs from $j=2$ to $n$. Therefore, for every $j$ in the sum, $s_{jk} = 0$.\nAs a result, $(S^2)_{1k} = \\sum_{j=2}^{n} 0 = 0$ for all $k$.\nSince all elements of $S^2$ are zero, $S^2 = O$, where $O$ is the $n \\times n$ zero matrix. This shows that $S$ is a nilpotent matrix of order $2$.\n\nThe matrix $A$ is given by $A = I - S$. Because $S$ is nilpotent, the geometric series (Neumann series) for the inverse of $A$ is finite:\n$$(I-S)^{-1} = I + S + S^2 + S^3 + \\dots$$\nSince $S^k = O$ for all $k \\ge 2$, the series terminates.\n$$A^{-1} = (I - S)^{-1} = I + S$$\nThe existence of the inverse $A^{-1}$ proves that $A$ is invertible.\nThe explicit forms of $A$ and $A^{-1}$ are:\n$$A = I - S = \\begin{pmatrix}\n1  -1  -1  \\dots  -1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n$$A^{-1} = I + S = \\begin{pmatrix}\n1  1  1  \\dots  1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n\n**Task 2: Compute $\\|A^{-1}\\|_{1}$ and $\\|A^{-1}\\|_{\\infty}$.**\n\nThe induced $1$-norm of a matrix is the maximum absolute column sum, $\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |m_{ij}|$.\nFor $A^{-1} = I + S$:\n- The first column ($j=1$) has a sum of absolute values of $|1| = 1$.\n- For any other column $j \\in \\{2, \\dots, n\\}$, the non-zero entries are $(A^{-1})_{1j}=1$ and $(A^{-1})_{jj}=1$. The sum of absolute values is $|1| + |1| = 2$.\nThus, $\\|A^{-1}\\|_1 = \\max(1, 2, \\dots, 2) = 2$.\n\nThe induced $\\infty$-norm of a matrix is the maximum absolute row sum, $\\|M\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |m_{ij}|$.\nFor $A^{-1} = I + S$:\n- The first row ($i=1$) has $n$ entries equal to $1$. The sum of absolute values is $\\sum_{j=1}^{n} |1| = n$.\n- For any other row $i \\in \\{2, \\dots, n\\}$, the only non-zero entry is $(A^{-1})_{ii}=1$. The sum of absolute values is $|1| = 1$.\nThus, $\\|A^{-1}\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$.\n\n**Task 3: Compute $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$ and the condition numbers.**\n\nWe first compute the norms of $A = I - S$.\nFor the $1$-norm of $A$:\n- The first column ($j=1$) has a sum of absolute values of $|1| = 1$.\n- For any other column $j \\in \\{2, \\dots, n\\}$, the non-zero entries are $a_{1j}=-1$ and $a_{jj}=1$. The sum of absolute values is $|-1| + |1| = 2$.\nThus, $\\|A\\|_1 = \\max(1, 2, \\dots, 2) = 2$.\n\nFor the $\\infty$-norm of $A$:\n- The first row ($i=1$) has entries $1, -1, \\dots, -1$. The sum of absolute values is $|1| + \\sum_{j=2}^{n} |-1| = 1 + (n-1) = n$.\n- For any other row $i \\in \\{2, \\dots, n\\}$, the only non-zero entry is $a_{ii}=1$. The sum of absolute values is $|1|=1$.\nThus, $\\|A\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$.\n\nThe condition number $\\kappa_p(A)$ is defined as $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$.\n- For $p=1$: $\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1 = 2 \\cdot 2 = 4$.\n- For $p=\\infty$: $\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = n \\cdot n = n^2$.\n\n**Task 4: Derive the worst-case upper bound on the relative solution error.**\n\nLet $\\mathbf{x}$ be the solution to $A\\mathbf{x}=\\mathbf{b}$ and $\\hat{\\mathbf{x}}$ be the solution to the perturbed system $A\\hat{\\mathbf{x}} = \\mathbf{b} + \\delta \\mathbf{b}$.\nThe error in the solution is $\\delta \\mathbf{x} = \\hat{\\mathbf{x}} - \\mathbf{x}$.\nSubtracting the first equation from the second gives:\n$A\\hat{\\mathbf{x}} - A\\mathbf{x} = (\\mathbf{b} + \\delta \\mathbf{b}) - \\mathbf{b} \\implies A(\\hat{\\mathbf{x}} - \\mathbf{x}) = \\delta \\mathbf{b} \\implies A \\delta \\mathbf{x} = \\delta \\mathbf{b}$.\nSince $A$ is invertible, we can write $\\delta \\mathbf{x} = A^{-1} \\delta \\mathbf{b}$.\nTaking the $p$-norm of both sides and using the property of induced norms $\\|M\\mathbf{v}\\|_p \\le \\|M\\|_p \\|\\mathbf{v}\\|_p$, we get:\n$\\|\\delta \\mathbf{x}\\|_p = \\|A^{-1} \\delta \\mathbf{b}\\|_p \\le \\|A^{-1}\\|_p \\|\\delta \\mathbf{b}\\|_p$.\n\nTo find the relative error $\\|\\delta \\mathbf{x}\\|_p / \\|\\mathbf{x}\\|_p$, we need a bound involving $\\|\\mathbf{x}\\|_p$. From the original equation $A\\mathbf{x}=\\mathbf{b}$, we have $\\|\\mathbf{b}\\|_p = \\|A\\mathbf{x}\\|_p \\le \\|A\\|_p \\|\\mathbf{x}\\|_p$. Assuming $\\mathbf{b} \\ne 0$ (and thus $\\mathbf{x} \\ne 0$), we can write $1/\\|\\mathbf{x}\\|_p \\le \\|A\\|_p/\\|\\mathbf{b}\\|_p$.\n\nCombining these two inequalities:\n$$\\frac{\\|\\delta \\mathbf{x}\\|_p}{\\|\\mathbf{x}\\|_p} \\le \\frac{\\|A^{-1}\\|_p \\|\\delta \\mathbf{b}\\|_p}{\\|\\mathbf{x}\\|_p} \\le \\|A^{-1}\\|_p \\|\\delta \\mathbf{b}\\|_p \\left( \\frac{\\|A\\|_p}{\\|\\mathbf{b}\\|_p} \\right)$$\nRearranging terms, we obtain the standard worst-case relative error bound:\n$$\\frac{\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|_p}{\\|\\mathbf{x}\\|_p} \\le \\|A\\|_p \\|A^{-1}\\|_p \\frac{\\|\\delta \\mathbf{b}\\|_p}{\\|\\mathbf{b}\\|_p} = \\kappa_p(A) \\frac{\\|\\delta \\mathbf{b}\\|_p}{\\|\\mathbf{b}\\|_p}$$\nThis bound is considered \"worst-case\" because for any matrix $A$, there exist vectors $\\mathbf{b}$ and $\\delta \\mathbf{b}$ for which this inequality becomes an equality.\n\n**Task 5: Determine the ratio of the two worst-case relative error bounds.**\n\nThe worst-case upper bound for the relative error in the $\\infty$-norm is:\n$$E_{\\infty} = \\kappa_{\\infty}(A) \\frac{\\|\\delta \\mathbf{b}\\|_{\\infty}}{\\|\\mathbf{b}\\|_{\\infty}} = n^2 \\frac{\\|\\delta \\mathbf{b}\\|_{\\infty}}{\\|\\mathbf{b}\\|_{\\infty}}$$\nThe worst-case upper bound for the relative error in the $1$-norm is:\n$$E_{1} = \\kappa_{1}(A) \\frac{\\|\\delta \\mathbf{b}\\|_{1}}{\\|\\mathbf{b}\\|_{1}} = 4 \\frac{\\|\\delta \\mathbf{b}\\|_{1}}{\\|\\mathbf{b}\\|_{1}}$$\nWe are given the additional assumption that the relative data error is identical in both norms:\n$$\\frac{\\|\\delta \\mathbf{b}\\|_{1}}{\\|\\mathbf{b}\\|_{1}} = \\frac{\\|\\delta \\mathbf{b}\\|_{\\infty}}{\\|\\mathbf{b}\\|_{\\infty}}$$\nLet this common value be denoted by $\\epsilon$. The error bounds become $E_{\\infty} = n^2 \\epsilon$ and $E_1 = 4 \\epsilon$.\nThe problem asks for the ratio of the two worst-case relative error bounds, in the $\\infty$-norm versus in the $1$-norm. This ratio is:\n$$\\frac{E_{\\infty}}{E_{1}} = \\frac{n^2 \\epsilon}{4 \\epsilon} = \\frac{n^2}{4}$$\nThis result is valid for $n \\ge 2$.", "answer": "$$\\boxed{\\frac{n^2}{4}}$$", "id": "3148445"}]}