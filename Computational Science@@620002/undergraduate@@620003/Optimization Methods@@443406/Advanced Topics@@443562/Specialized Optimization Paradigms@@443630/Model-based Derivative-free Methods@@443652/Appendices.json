{"hands_on_practices": [{"introduction": "This first practice grounds us in a fundamental task of derivative-free optimization: approximating a function's gradient using a local linear model. By fitting a model to a known quadratic function, you will directly observe how the accuracy of your gradient estimate depends on the geometry and density of your sample points. This exercise [@problem_id:3153344] is crucial for developing an intuition for \"poise,\" the quality of a sample set that ensures a well-determined and accurate model.", "problem": "Consider a deterministic experiment for model-based derivative-free approximation of gradients using a local linear model. Let the dimension be $n=2$. Define the twice continuously differentiable objective function $f:\\mathbb{R}^2\\to\\mathbb{R}$ by the quadratic model\n$$\nf(\\mathbf{x})=\\tfrac{1}{2}\\,\\mathbf{x}^{\\top}Q\\,\\mathbf{x}+ \\mathbf{c}^{\\top}\\mathbf{x},\n$$\nwhere\n$$\nQ=\\begin{bmatrix}31\\\\\\\\12\\end{bmatrix},\\quad \\mathbf{c}=\\begin{bmatrix}1\\\\\\\\-2\\end{bmatrix}.\n$$\nLet the modeling center be\n$$\n\\mathbf{x}_c=\\begin{bmatrix}0.5\\\\\\\\-0.3\\end{bmatrix}.\n$$\nThe true gradient at $\\mathbf{x}_c$ is known from first principles of multivariable calculus as the Jacobian (which, for a scalar-valued function, is the gradient)\n$$\n\\nabla f(\\mathbf{x}_c)=Q\\,\\mathbf{x}_c+\\mathbf{c}.\n$$\nA standard model-based derivative-free approach builds an affine model $m(\\mathbf{x})=a+\\mathbf{g}^{\\top}(\\mathbf{x}-\\mathbf{x}_c)$ from function values only, by solving a least-squares fit over sample points $\\{\\mathbf{y}_i\\}_{i=1}^m$ near $\\mathbf{x}_c$, where $\\mathbf{s}_i=\\mathbf{y}_i-\\mathbf{x}_c$ are small displacements. The estimated gradient is the vector $\\widehat{\\mathbf{g}}$ obtained from the fit. Scientific realism requires that the sample set geometry affects identifiability and accuracy, and that the sampling radius controls the truncation error introduced by ignoring curvature in a linear model.\n\nStarting from fundamental definitions of least squares and the first-order Taylor expansion of $f$ around $\\mathbf{x}_c$, design a program that, for each specified test case, constructs the sample set, fits the affine model using linear least squares, and returns the Euclidean norm of the gradient estimation error\n$$\n\\left\\|\\widehat{\\mathbf{g}}-\\nabla f(\\mathbf{x}_c)\\right\\|_2.\n$$\nAngles used to place points on circles must be expressed in radians. No physical units apply in this purely mathematical setting.\n\nImplement the following test suite, which varies sample density via the sampling radius and geometry quality via the arrangement of points. In all cases, set the center at $\\mathbf{x}_c$ and evaluate $f$ exactly at each sampled point:\n\n- Case A (well-spread geometry, larger radius): a circle of radius $r=0.5$ around $\\mathbf{x}_c$ with $m=12$ equally spaced points. Angles are $2\\pi i/m$ for $i=0,1,\\dots,m-1$ in radians.\n- Case B (well-spread geometry, smaller radius): a circle of radius $r=0.01$ around $\\mathbf{x}_c$ with $m=12$ equally spaced points as above.\n- Case C (nearly collinear geometry, larger radius): points along the line direction $\\mathbf{v}=\\begin{bmatrix}1\\\\\\\\\\varepsilon\\end{bmatrix}$ with $\\varepsilon=10^{-3}$, scaled to radius $r=0.5$, with $m=12$ offsets $t_i$ equally spaced in the interval $[-1,1]$; $\\mathbf{s}_i=r\\,t_i\\,\\mathbf{v}/\\|\\mathbf{v}\\|_2$.\n- Case D (nearly collinear geometry, smaller radius): same as Case C but with radius $r=0.01$.\n- Case E (degenerate geometry): $m=12$ points all at $\\mathbf{x}_c$ (that is, $\\mathbf{s}_i=\\mathbf{0}$ for all $i$).\n- Case F (minimal well-spread set): $m=3$ points on a circle of radius $r=0.1$ at angles $0$, $2\\pi/3$, and $4\\pi/3$ radians.\n- Case G (minimal nearly collinear set): $m=3$ points with $\\mathbf{v}=\\begin{bmatrix}1\\\\\\\\\\varepsilon\\end{bmatrix}$ for $\\varepsilon=10^{-6}$, offsets $t\\in\\{-1,0,1\\}$, and radius $r=0.1$; $\\mathbf{s}_i=r\\,t_i\\,\\mathbf{v}/\\|\\mathbf{v}\\|_2$.\n\nFor each case, form the design matrix with a column of ones and the components of $\\mathbf{s}_i$, solve the linear least-squares problem for $(a,\\widehat{\\mathbf{g}})$, and report the scalar error $\\left\\|\\widehat{\\mathbf{g}}-\\nabla f(\\mathbf{x}_c)\\right\\|_2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\") ordered as Cases A through G.", "solution": "The problem requires the design of a numerical experiment to evaluate the accuracy of a model-based derivative-free gradient estimation method. We will first validate the problem statement and then develop a solution from fundamental principles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Objective Function:** $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}Q\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x}$, where $\\mathbf{x} \\in \\mathbb{R}^2$.\n- **Matrix Q:** $Q=\\begin{bmatrix}31\\\\12\\end{bmatrix}$.\n- **Vector c:** $\\mathbf{c}=\\begin{bmatrix}1\\\\-2\\end{bmatrix}$.\n- **Modeling Center:** $\\mathbf{x}_c=\\begin{bmatrix}0.5\\\\-0.3\\end{bmatrix}$.\n- **True Gradient:** $\\nabla f(\\mathbf{x}_c) = Q\\mathbf{x}_c + \\mathbf{c}$.\n- **Affine Model:** $m(\\mathbf{x}) = a + \\mathbf{g}^{\\top}(\\mathbf{x}-\\mathbf{x}_c)$. The parameters to be estimated are the scalar intercept $a$ and the gradient vector $\\widehat{\\mathbf{g}}$.\n- **Sample Points:** Displacements $\\mathbf{s}_i = \\mathbf{y}_i - \\mathbf{x}_c$ are used to generate sample points $\\mathbf{y}_i$.\n- **Method:** Solve a linear least-squares problem over the sample set $\\{\\mathbf{y}_i\\}_{i=1}^m$ to find $(a, \\widehat{\\mathbf{g}})$.\n- **Output Metric:** The Euclidean norm of the gradient estimation error, $\\|\\widehat{\\mathbf{g}} - \\nabla f(\\mathbf{x}_c)\\|_2$.\n- **Test Cases:**\n    - **A:** $r=0.5, m=12$, circle geometry.\n    - **B:** $r=0.01, m=12$, circle geometry.\n    - **C:** $r=0.5, m=12$, nearly collinear geometry with $\\mathbf{v}=\\begin{bmatrix}1\\\\10^{-3}\\end{bmatrix}$.\n    - **D:** $r=0.01, m=12$, nearly collinear geometry as in C.\n    - **E:** $r$ undefined (all points at center), $m=12$, degenerate geometry with $\\mathbf{s}_i=\\mathbf{0}$.\n    - **F:** $r=0.1, m=3$, minimal well-spread circle geometry.\n    - **G:** $r=0.1, m=3$, minimal nearly collinear geometry with $\\mathbf{v}=\\begin{bmatrix}1\\\\10^{-6}\\end{bmatrix}$ and offsets $t\\in\\{-1,0,1\\}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly grounded in the field of numerical optimization, specifically in the analysis of derivative-free methods. Using a quadratic test function and linear models is a standard technique for analyzing algorithmic properties. The concepts of least-squares fitting, model poise, and the influence of sample geometry are central to this field. All mathematical formulations are correct.\n- **Well-Posedness:** The problem is well-posed. The least-squares problem has a unique solution whenever the design matrix has full column rank. The test cases are designed to explore scenarios where this condition holds with varying degrees of stability (Cases A, B, C, D, F) and scenarios where it fails (Cases E, G). The failure cases are themselves an important part of the analysis and lead to predictable outcomes from standard numerical linear algebra routines.\n- **Objectivity:** The problem statement is precise, quantitative, and free of subjective elements.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It is a valid and instructive exercise in numerical analysis. We will proceed with a full solution.\n\n### Principle-Based Solution Design\n\nThe core of the task is to approximate the gradient of a function $f:\\mathbb{R}^n \\to \\mathbb{R}$ at a point $\\mathbf{x}_c$ by fitting a local affine model $m(\\mathbf{x}) = a + \\mathbf{g}^{\\top}(\\mathbf{x}-\\mathbf{x}_c)$ using only function values evaluated at a set of sample points $\\{\\mathbf{y}_i\\}_{i=1}^m$.\n\n**1. Theoretical Formulation**\n\nWe seek to find the parameters $a \\in \\mathbb{R}$ and $\\widehat{\\mathbf{g}} \\in \\mathbb{R}^n$ that minimize the sum of squared differences between the model's predictions and the true function values at the sample points. Let the displacement vectors be $\\mathbf{s}_i = \\mathbf{y}_i - \\mathbf{x}_c$. The model evaluated at $\\mathbf{y}_i$ is $m(\\mathbf{y}_i) = a + \\widehat{\\mathbf{g}}^{\\top}\\mathbf{s}_i$. The least-squares problem is:\n$$ \\min_{a, \\widehat{\\mathbf{g}}} \\sum_{i=1}^m \\left( (a + \\widehat{\\mathbf{g}}^{\\top}\\mathbf{s}_i) - f(\\mathbf{y}_i) \\right)^2 $$\nThis is a standard linear least-squares problem. Let the parameter vector be $\\boldsymbol{\\theta} = \\begin{bmatrix} a \\\\ \\widehat{\\mathbf{g}} \\end{bmatrix} \\in \\mathbb{R}^{1+n}$. For our problem, $n=2$, so $\\boldsymbol{\\theta} \\in \\mathbb{R}^3$. We can express the system in matrix form as $M\\boldsymbol{\\theta} \\approx \\mathbf{f}_{vals}$, where:\n-   $M$ is the $m \\times (1+n)$ design matrix. Each row corresponds to a sample point:\n    $$ M = \\begin{bmatrix} 1  \\mathbf{s}_1^{\\top} \\\\ 1  \\mathbf{s}_2^{\\top} \\\\ \\vdots  \\vdots \\\\ 1  \\mathbf{s}_m^{\\top} \\end{bmatrix} = \\begin{bmatrix} 1  s_{1,1}  s_{1,2} \\\\ 1  s_{2,1}  s_{2,2} \\\\ \\vdots  \\vdots  \\vdots \\\\ 1  s_{m,1}  s_{m,2} \\end{bmatrix} $$\n-   $\\mathbf{f}_{vals}$ is the $m \\times 1$ vector of function evaluations:\n    $$ \\mathbf{f}_{vals} = \\begin{bmatrix} f(\\mathbf{y}_1) \\\\ f(\\mathbf{y}_2) \\\\ \\vdots \\\\ f(\\mathbf{y}_m) \\end{bmatrix} $$\nThe least-squares solution $\\boldsymbol{\\theta}^*$ that minimizes $\\|M\\boldsymbol{\\theta} - \\mathbf{f}_{vals}\\|_2^2$ is found by solving the normal equations $M^{\\top}M\\boldsymbol{\\theta} = M^{\\top}\\mathbf{f}_{vals}$. Numerically stable algorithms, such as those based on QR decomposition or Singular Value Decomposition (SVD), are used in practice. The estimated gradient $\\widehat{\\mathbf{g}}$ consists of the last $n=2$ components of the solution vector $\\boldsymbol{\\theta}^*$.\n\n**2. Error Analysis for Quadratic Functions**\n\nThe given objective function $f(\\mathbf{x})$ is a quadratic. Its Taylor series expansion around $\\mathbf{x}_c$ is exact and terminates at the second-order term:\n$$ f(\\mathbf{x}_c + \\mathbf{s}) = f(\\mathbf{x}_c) + \\nabla f(\\mathbf{x}_c)^{\\top}\\mathbf{s} + \\frac{1}{2}\\mathbf{s}^{\\top}\\nabla^2 f(\\mathbf{x}_c)\\mathbf{s} $$\nFor the given $f$, the gradient is $\\nabla f(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c}$ and the Hessian is constant, $\\nabla^2 f(\\mathbf{x}) = Q$. Thus, the vector of function values can be written as:\n$$ \\mathbf{f}_{vals, i} = f(\\mathbf{x}_c) + \\nabla f(\\mathbf{x}_c)^{\\top}\\mathbf{s}_i + \\frac{1}{2}\\mathbf{s}_i^{\\top}Q\\mathbf{s}_i $$\nThe least-squares solver is effectively trying to fit a linear model to this quadratic data. The discrepancy arises from the quadratic term $\\frac{1}{2}\\mathbf{s}_i^{\\top}Q\\mathbf{s}_i$, which is the truncation error of the affine model. The error in the estimated gradient, $\\Delta\\mathbf{g} = \\widehat{\\mathbf{g}} - \\nabla f(\\mathbf{x}_c)$, is directly attributable to how the geometry of the sample set (encoded in $M$) maps these quadratic terms into the solution. It can be shown that the error in the parameter vector is $\\Delta\\boldsymbol{\\theta} = (M^{\\top}M)^{-1}M^{\\top}\\mathbf{b}$, where $b_i = \\frac{1}{2}\\mathbf{s}_i^{\\top}Q\\mathbf{s}_i$. The quality of the gradient estimate $\\widehat{\\mathbf{g}}$ thus critically depends on the properties of $M$, a concept known as the \"poise\" of the sample set.\n\n**3. Implementation Procedure**\n\nWe will implement a procedure that follows these steps for each test case.\n\n**Step A: Initialization**\nFirst, we define the constant problem parameters $Q$, $\\mathbf{c}$, and $\\mathbf{x}_c$. Then we compute the true gradient, which serves as our benchmark:\n$$ \\nabla f(\\mathbf{x}_c) = Q\\mathbf{x}_c + \\mathbf{c} = \\begin{bmatrix}31\\\\12\\end{bmatrix}\\begin{bmatrix}0.5\\\\-0.3\\end{bmatrix} + \\begin{bmatrix}1\\\\-2\\end{bmatrix} = \\begin{bmatrix}1.2\\\\-0.1\\end{bmatrix} + \\begin{bmatrix}1\\\\-2\\end{bmatrix} = \\begin{bmatrix}2.2\\\\-2.1\\end{bmatrix} $$\n\n**Step B: Per-Case Calculation Loop**\nFor each of the cases A through G, we perform the following:\n1.  **Generate Sample Displacements:** An $m \\times 2$ matrix $S$ is created, where each row is a displacement vector $\\mathbf{s}_i^{\\top}$ as specified by the case (e.g., points on a circle, points on a line).\n2.  **Evaluate Function:** Sample points are formed as $\\mathbf{y}_i = \\mathbf{x}_c + \\mathbf{s}_i$. The function $f(\\mathbf{y}_i) = \\tfrac{1}{2}\\mathbf{y}_i^{\\top}Q\\mathbf{y}_i + \\mathbf{c}^{\\top}\\mathbf{y}_i$ is evaluated for each $\\mathbf{y}_i$ to create the vector $\\mathbf{f}_{vals}$.\n3.  **Construct Design Matrix:** The $m \\times 3$ design matrix $M$ is constructed by augmenting the displacement matrix $S$ with a leading column of ones.\n4.  **Solve Least-Squares Problem:** A numerical linear least-squares solver is used to find $\\boldsymbol{\\theta}^* = [a, \\widehat{g}_1, \\widehat{g}_2]^{\\top}$ that minimizes $\\|M\\boldsymbol{\\theta} - \\mathbf{f}_{vals}\\|_2$. This handles rank-deficient cases (E and G) robustly by providing a minimum-norm solution.\n5.  **Extract Gradient and Compute Error:** The estimated gradient $\\widehat{\\mathbf{g}} = [\\widehat{g}_1, \\widehat{g}_2]^{\\top}$ is extracted from $\\boldsymbol{\\theta}^*$. The final error is calculated as the Euclidean norm of the difference: $\\|\\widehat{\\mathbf{g}} - \\nabla f(\\mathbf{x}_c)\\|_2$.\n\nThis systematic process allows for a direct comparison of how sampling radius and geometric arrangement affect the accuracy of the gradient approximation. For instance, we expect the symmetric geometries in Cases A and B to yield highly accurate results (error near machine precision) due to cancellation of error terms, while the poorly conditioned geometries in C, D, and G will result in larger errors. Case E represents a complete failure of identifiability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the gradient estimation error in a model-based derivative-free\n    method for a series of test cases.\n    \"\"\"\n    \n    # Define problem constants\n    Q = np.array([[3.0, 1.0], [1.0, 2.0]])\n    c = np.array([1.0, -2.0])\n    xc = np.array([0.5, -0.3])\n    \n    # Define the quadratic objective function\n    def f(x):\n        return 0.5 * x.T @ Q @ x + c.T @ x\n        \n    # Calculate the true gradient at xc\n    grad_true = Q @ xc + c\n    \n    # Define the test suite\n    test_cases = [\n        # Case A: well-spread, larger radius\n        {'name': 'A', 'r': 0.5, 'm': 12, 'type': 'circle'},\n        # Case B: well-spread, smaller radius\n        {'name': 'B', 'r': 0.01, 'm': 12, 'type': 'circle'},\n        # Case C: nearly collinear, larger radius\n        {'name': 'C', 'r': 0.5, 'm': 12, 'type': 'line', 'eps': 1e-3},\n        # Case D: nearly collinear, smaller radius\n        {'name': 'D', 'r': 0.01, 'm': 12, 'type': 'line', 'eps': 1e-3},\n        # Case E: degenerate geometry\n        {'name': 'E', 'm': 12, 'type': 'degenerate'},\n        # Case F: minimal well-spread set\n        {'name': 'F', 'r': 0.1, 'm': 3, 'type': 'circle'},\n        # Case G: minimal nearly collinear set\n        {'name': 'G', 'r': 0.1, 'm': 3, 'type': 'line', 'eps': 1e-6, 't_offsets': [-1, 0, 1]},\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        m = case['m']\n        \n        # 1. Generate sample displacements S\n        S = np.zeros((m, 2))\n        \n        if case['type'] == 'circle':\n            r = case['r']\n            if case['name'] == 'F':\n                angles = np.array([0, 2 * np.pi / 3, 4 * np.pi / 3])\n            else: # Cases A, B\n                angles = np.linspace(0, 2 * np.pi, m, endpoint=False)\n            S[:, 0] = r * np.cos(angles)\n            S[:, 1] = r * np.sin(angles)\n            \n        elif case['type'] == 'line':\n            r = case['r']\n            eps = case['eps']\n            v = np.array([1.0, eps])\n            v_norm = v / np.linalg.norm(v)\n            if 't_offsets' in case: # Case G\n                t = np.array(case['t_offsets'])\n            else: # Cases C, D\n                t = np.linspace(-1.0, 1.0, m)\n            S = t[:, np.newaxis] * v_norm\n            \n        elif case['type'] == 'degenerate': # Case E\n            # S is already initialized to zeros\n            pass\n\n        # 2. Evaluate function\n        Y = xc + S\n        f_vals = np.array([f(y) for y in Y])\n        \n        # 3. Construct design matrix M\n        M = np.hstack([np.ones((m, 1)), S])\n        \n        # 4. Solve least-squares problem\n        # theta = [a, g_hat_1, g_hat_2]\n        # Use rcond=None to use machine precision for rank detection\n        theta, _, _, _ = np.linalg.lstsq(M, f_vals, rcond=None)\n        \n        # 5. Extract gradient and compute error\n        g_hat = theta[1:]\n        error = np.linalg.norm(g_hat - grad_true)\n        results.append(error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{res:.12e}' for res in results)}]\")\n\nsolve()\n```", "id": "3153344"}, {"introduction": "While local models are powerful, they are not infallible, and understanding their failure modes is key to using them effectively. This exercise [@problem_id:3153302] presents a classic scenario where a quadratic model is built over a region containing two distinct minima. You will see firsthand how an overly large trust region can cause the model to \"average\" the landscape's features, leading to a misleading and counterproductive step.", "problem": "Consider a one-dimensional objective function $f:\\mathbb{R} \\to \\mathbb{R}$ defined by $f(x) = \\left(x^{2} - a^{2}\\right)^{2}$ with parameter $a = 0.2$. This $f(x)$ is smooth and has two nearby local minima at $x = \\pm a$. In a model-based derivative-free optimization (DFO) trust-region framework, suppose the current iterate is $x_{k} = 0.19$ and a large trust-region radius $\\Delta = 0.5$ is used. A quadratic regression model $m(x) = c_{0} + c_{1} x + c_{2} x^{2}$ is built by least-squares fitting to the function values at the five sampling points $x \\in \\{-0.3,\\,-0.15,\\,0,\\,0.15,\\,0.3\\}$.\n\nUsing only the stated definitions of least-squares fitting and trust-region modeling, determine the trial step $s_{k}$ produced by minimizing the model within the trust region, that is,\n$$\ns_{k} = \\underset{|x - x_{k}| \\le \\Delta}{\\arg\\min}\\, m(x) \\;-\\; x_{k}.\n$$\nProvide the value of $s_{k}$ as a single real number. Do not round your answer.", "solution": "We start from the foundational definitions. In model-based derivative-free optimization (DFO), one constructs a surrogate model $m(x)$ that approximates $f(x)$ in a neighborhood of the current point and then minimizes $m(x)$ within a trust region. For a quadratic model $m(x) = c_{0} + c_{1} x + c_{2} x^{2}$ built by least-squares regression over given data $\\{(x_{i}, f(x_{i}))\\}$, the coefficients $(c_{0}, c_{1}, c_{2})$ solve the normal equations associated with the basis $\\{1, x, x^{2}\\}$:\n$$\n\\begin{pmatrix}\n\\sum 1  \\sum x_{i}  \\sum x_{i}^{2} \\\\\n\\sum x_{i}  \\sum x_{i}^{2}  \\sum x_{i}^{3} \\\\\n\\sum x_{i}^{2}  \\sum x_{i}^{3}  \\sum x_{i}^{4}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_{0} \\\\ c_{1} \\\\ c_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum f(x_{i}) \\\\ \\sum x_{i} f(x_{i}) \\\\ \\sum x_{i}^{2} f(x_{i})\n\\end{pmatrix}.\n$$\nHere the sampling points are $x \\in \\{-0.3,\\,-0.15,\\,0,\\,0.15,\\,0.3\\}$, which are symmetric about $x=0$. The objective is $f(x) = (x^{2} - a^{2})^{2}$ with $a = 0.2$, so\n$$\nf(x) = x^{4} - 2 a^{2} x^{2} + a^{4} = x^{4} - 0.08 x^{2} + 0.0016.\n$$\nBy symmetry of the sample set about $0$, we have $\\sum x_{i} = 0$ and $\\sum x_{i}^{3} = 0$. Moreover, $f(x)$ is an even function, so $\\sum x_{i} f(x_{i}) = 0$. The normal equations decouple, giving directly\n$$\n\\left(\\sum x_{i}^{2}\\right) c_{1} = \\sum x_{i} f(x_{i}) = 0 \\quad \\Rightarrow \\quad c_{1} = 0.\n$$\nThus the least-squares quadratic model has the form $m(x) = c_{0} + c_{2} x^{2}$. Its minimizer is at $x_{m} = -\\frac{c_{1}}{2 c_{2}} = 0$ provided $c_{2}  0$. We now verify $c_{2}  0$ by solving the reduced $2 \\times 2$ normal equations for $(c_{0}, c_{2})$:\n$$\n\\begin{pmatrix}\nN  \\sum x_{i}^{2} \\\\\n\\sum x_{i}^{2}  \\sum x_{i}^{4}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_{0} \\\\ c_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum f(x_{i}) \\\\ \\sum x_{i}^{2} f(x_{i})\n\\end{pmatrix},\n$$\nwhere $N = 5$. We compute the necessary sums using the symmetry and explicit powers at $x \\in \\{-0.3,\\,-0.15,\\,0,\\,0.15,\\,0.3\\}$:\n- $\\sum x_{i}^{2} = 2\\,(0.09 + 0.0225) + 0 = 0.225$,\n- $\\sum x_{i}^{4} = 2\\,(0.0081 + 0.00050625) + 0 = 0.0172125$,\n- $\\sum f(x_{i}) = \\sum \\left(x^{4} - 0.08 x^{2} + 0.0016\\right) = \\sum x^{4} - 0.08 \\sum x^{2} + N \\cdot 0.0016 = 0.0172125 - 0.08 \\cdot 0.225 + 0.008 = 0.0072125$,\n- $\\sum x_{i}^{2} f(x_{i}) = \\sum \\left(x^{6} - 0.08 x^{4} + 0.0016 x^{2}\\right)$.\nTo evaluate $\\sum x_{i}^{6}$, note $x^{6} = x^{2} x^{4}$; at $x = \\pm 0.3$, $x^{6} = 0.09 \\cdot 0.0081 = 0.000729$, and at $x = \\pm 0.15$, $x^{6} = 0.0225 \\cdot 0.00050625 = 0.000011390625$. Therefore,\n$$\n\\sum x_{i}^{6} = 2\\,(0.000729 + 0.000011390625) + 0 = 0.00148078125,\n$$\nand thus\n$$\n\\sum x_{i}^{2} f(x_{i}) = 0.00148078125 - 0.08 \\cdot 0.0172125 + 0.0016 \\cdot 0.225 = 0.00046378125.\n$$\nWe solve\n$$\n\\begin{pmatrix}\n5  0.225 \\\\\n0.225  0.0172125\n\\end{pmatrix}\n\\begin{pmatrix}\nc_{0} \\\\ c_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.0072125 \\\\ 0.00046378125\n\\end{pmatrix}.\n$$\nThe determinant is\n$$\nD = 5 \\cdot 0.0172125 - (0.225)^{2} = 0.0860625 - 0.050625 = 0.0354375  0,\n$$\nand\n$$\nc_{2} = \\frac{5 \\cdot 0.00046378125 - 0.225 \\cdot 0.0072125}{D} = \\frac{0.00231890625 - 0.0016228125}{0.0354375} = \\frac{0.00069609375}{0.0354375} \\approx 0.01964  0.\n$$\nHence $m(x)$ is strictly convex with $c_{1} = 0$, and its unique minimizer is\n$$\nx_{m} = 0.\n$$\nThe trust region is centered at $x_{k} = 0.19$ with radius $\\Delta = 0.5$, so the feasible interval is $[x_{k} - \\Delta,\\, x_{k} + \\Delta] = [-0.31,\\, 0.69]$. Since $x_{m} = 0 \\in [-0.31,\\, 0.69]$, the trust-region constrained minimizer of $m(x)$ is $x_{m}$ itself. Therefore, the trial step is\n$$\ns_{k} = x_{m} - x_{k} = 0 - 0.19 = -0.19.\n$$\nThis explicit calculation shows that using a too-large trust region that spans both wells leads the quadratic regression model to average the two minima and place its minimizer at the midpoint $x=0$, thereby producing the step $s_{k} = -0.19$ away from the nearby true minimum at $x \\approx 0.2$.", "answer": "$$\\boxed{-0.19}$$", "id": "3153302"}, {"introduction": "Real-world optimization problems often feature complex, non-quadratic landscapes where a simple polynomial model may be inadequate. This advanced practice [@problem_id:3153247] challenges you to implement a principled method for choosing between a standard quadratic model and a more flexible Radial Basis Function (RBF) model. By using leave-one-out cross-validation, you will learn a robust, data-driven technique for model selection, a critical skill in modern optimization and machine learning.", "problem": "You are given the task of constructing a principled, programmatic test to decide when a Radial Basis Function (RBF) model should be preferred over a quadratic polynomial model for building a surrogate in model-based derivative-free optimization, based on cross-validated prediction error, on landscapes that may be multimodal. The problem must be solved using fundamental principles and definitions: model-based derivative-free optimization relies on surrogate modeling to approximate an objective function without using derivatives; cross-validation estimates generalization error by leaving out data points; RBF kernel ridge regression and quadratic polynomial ridge regression are linear smoothers, for which leave-one-out predictions can be computed without repeated refitting via the smoothing matrix identity.\n\nDefine the following models and principles:\n- A surrogate model is a function $\\hat{f}(x)$ designed to approximate an unknown objective $f(x)$ using only function evaluations at $x$. In model-based derivative-free optimization, surrogate models are fitted to sampled data $\\{x_i, y_i\\}_{i=1}^m$, where $y_i = f(x_i)$.\n- In kernel ridge regression with a Gaussian Radial Basis Function kernel, the kernel is $$K(x, x') = \\exp\\left(-(\\varepsilon \\lVert x - x' \\rVert_2)^2\\right),$$ where $\\varepsilon gt; 0$ is the shape parameter. Given the kernel matrix $K \\in \\mathbb{R}^{m \\times m}$ with entries $K_{ij} = K(x_i, x_j)$ and regularization parameter $\\lambda_r gt; 0$, the fitted values at the training points are $$\\hat{\\mathbf{y}} = S_r \\mathbf{y}, \\quad S_r = K (K + \\lambda_r I)^{-1},$$ where $I$ is the identity matrix and $\\mathbf{y} \\in \\mathbb{R}^m$ is the vector of $y_i$.\n- In quadratic polynomial ridge regression, the design matrix $X \\in \\mathbb{R}^{m \\times p}$ gathers the basis functions up to degree two in $d$ dimensions: constant term, linear terms $x_k$, pure quadratic terms $x_k^2$, and pairwise cross terms $x_k x_\\ell$ for $1 \\le k lt; \\ell \\le d$. With regularization parameter $\\lambda_q gt; 0$, the fitted values at training points are $$\\hat{\\mathbf{y}} = S_q \\mathbf{y}, \\quad S_q = X \\left(X^\\top X + \\lambda_q I_p\\right)^{-1} X^\\top,$$ where $I_p$ is the $p \\times p$ identity matrix.\n- For any linear smoother with $\\hat{\\mathbf{y}} = S \\mathbf{y}$, the leave-one-out cross-validation (LOOCV) prediction at $x_i$ can be computed without refitting using $$\\hat{y}^{(-i)} = \\frac{\\hat{y}_i - S_{ii} y_i}{1 - S_{ii}},$$ provided $1 - S_{ii} \\neq 0$. The LOOCV root mean squared error (RMSE) is $$\\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m \\left(\\hat{y}^{(-i)} - y_i\\right)^2}.$$\n- Propose the following switching rule based on cross-validated error and a margin motivated by sampling variability: compute $\\mathrm{RMSE}_{r}$ for the RBF kernel ridge model and $\\mathrm{RMSE}_{q}$ for the quadratic ridge model, and let $$\\gamma = \\alpha \\frac{s_y}{\\sqrt{m}},$$ where $s_y$ is the sample standard deviation of $\\{y_i\\}$ with Bessel's correction (dividing by $m-1$), and $\\alpha$ is a fixed constant. Choose the RBF model if $$\\mathrm{RMSE}_{r} + \\gamma lt; \\mathrm{RMSE}_{q},$$ otherwise choose the quadratic model. This rule prefers the more flexible RBF only when it is sufficiently better by a margin that shrinks as the sample size grows.\n\nYour program must:\n1. Implement both models with the above specifications, using ridge regularization for stability.\n2. Compute the LOOCV RMSE for both models using the smoothing matrix identity, not by refitting $m$ times.\n3. Implement the switching rule using the margin $\\gamma$ defined above with a specified $\\alpha$ per test case.\n4. For the RBF kernel, set the shape parameter $\\varepsilon$ to the reciprocal of the median pairwise distance among the sampled points, that is $$\\varepsilon = \\frac{1}{\\mathrm{median}\\left\\{\\lVert x_i - x_j \\rVert_2 : 1 \\le i  j \\le m\\right\\}}.$$ If the median distance is zero, fall back to a small positive default value. For all trigonometric functions, use radians.\n\nConstruct and evaluate the following test suite. In every case, sample $m$ points independently and uniformly from the specified hyperrectangle, using the given random seed for reproducibility, and compute $y_i = f(x_i)$ without added noise. Use the provided regularization parameters and margin constant $\\alpha$.\n\n- Test Case 1 (Multimodal, two-dimensional Rastrigin):\n    - Dimension: $d = 2$\n    - Objective: $$f(x) = A d + \\sum_{k=1}^d \\left(x_k^2 - A \\cos(2 \\pi x_k)\\right), \\quad A = 10$$\n    - Domain: $x \\in [-5.12, 5.12]^2$\n    - Sample size: $m = 60$\n    - Random seed: $42$\n    - Regularization: $\\lambda_r = 10^{-3}$, $\\lambda_q = 10^{-6}$\n    - Margin parameter: $\\alpha = 0.1$\n\n- Test Case 2 (Unimodal quadratic bowl in two dimensions):\n    - Dimension: $d = 2$\n    - Objective: $$f(x) = (x_1 - 1)^2 + 2(x_2 + 0.5)^2 + 3$$\n    - Domain: $x \\in [-2, 2]^2$\n    - Sample size: $m = 40$\n    - Random seed: $123$\n    - Regularization: $\\lambda_r = 10^{-3}$, $\\lambda_q = 10^{-6}$\n    - Margin parameter: $\\alpha = 0.1$\n\n- Test Case 3 (Multimodal Ackley in three dimensions):\n    - Dimension: $d = 3$\n    - Objective: $$f(x) = -20 \\exp\\left(-0.2 \\sqrt{\\frac{1}{d} \\sum_{k=1}^d x_k^2}\\right) - \\exp\\left(\\frac{1}{d} \\sum_{k=1}^d \\cos(2 \\pi x_k)\\right) + 20 + e$$\n    - Domain: $x \\in [-2, 2]^3$\n    - Sample size: $m = 80$\n    - Random seed: $2024$\n    - Regularization: $\\lambda_r = 10^{-3}$, $\\lambda_q = 10^{-6}$\n    - Margin parameter: $\\alpha = 0.1$\n\n- Test Case 4 (Quadratic with mild sinusoidal perturbation in two dimensions):\n    - Dimension: $d = 2$\n    - Objective: $$f(x) = x_1^2 + x_2^2 + 0.1 \\sin(3 x_1) \\sin(3 x_2)$$\n    - Domain: $x \\in [-1.5, 1.5]^2$\n    - Sample size: $m = 25$\n    - Random seed: $7$\n    - Regularization: $\\lambda_r = 10^{-3}$, $\\lambda_q = 10^{-6}$\n    - Margin parameter: $\\alpha = 0.1$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case produces the list $[b, \\mathrm{RMSE}_r, \\mathrm{RMSE}_q]$ with $b$ an integer indicator equal to $1$ if the RBF model is selected by the switching rule and $0$ otherwise. Thus the final output is a list of four lists of length three, for the four test cases, for example: \"[[b1,rmse_r1,rmse_q1],[b2,rmse_r2,rmse_q2],[b3,rmse_r3,rmse_q3],[b4,rmse_r4,rmse_q4]]\".", "solution": "The problem requires the creation and implementation of a quantitative test to decide between two types of surrogate models in the context of model-based derivative-free optimization: a more flexible Radial Basis Function (RBF) model and a simpler quadratic polynomial model. The decision is based on comparing their respective prediction errors, estimated via leave-one-out cross-validation (LOOCV), with a penalty against the more complex RBF model.\n\nThe overall procedure for each defined test case is as follows. First, a dataset composed of $m$ input-output pairs, $\\{x_i, y_i\\}_{i=1}^m$, is generated by sampling $m$ points $x_i$ uniformly from a specified $d$-dimensional domain and evaluating a given objective function $f$ such that $y_i = f(x_i)$. This dataset serves as the basis for training and validating both models.\n\nThe first model is a quadratic polynomial ridge regression model. For this model, we construct a design matrix $X \\in \\mathbb{R}^{m \\times p}$. Each row of this matrix corresponds to a sample point $x_i$ and is populated with the values of a basis of polynomials up to degree two. For a point $x = (x_1, \\dots, x_d)^\\top \\in \\mathbb{R}^d$, the basis includes a constant term ($1$), linear terms ($x_k$ for $k=1, \\dots, d$), pure quadratic terms ($x_k^2$ for $k=1, \\dots, d$), and cross-product terms ($x_k x_\\ell$ for $1 \\le k  \\ell \\le d$). The total number of basis functions is $p = \\frac{(d+1)(d+2)}{2}$. The vector of fitted values, $\\hat{\\mathbf{y}}_q$, is obtained using a linear smoother, $\\hat{\\mathbf{y}}_q = S_q \\mathbf{y}$, where $\\mathbf{y} = (y_1, \\dots, y_m)^\\top$ is the vector of observed function values. The smoothing matrix $S_q$ is defined as:\n$$S_q = X (X^\\top X + \\lambda_q I_p)^{-1} X^\\top$$\nIn this expression, $\\lambda_q  0$ represents the regularization parameter, and $I_p$ is the $p \\times p$ identity matrix.\n\nThe second model is a kernel ridge regression model using a Gaussian Radial Basis Function (RBF) kernel. The RBF kernel is given by the function:\n$$K(x, x') = \\exp\\left(-(\\varepsilon \\lVert x - x' \\rVert_2)^2\\right)$$\nwhere $\\varepsilon  0$ is a shape parameter that controls the \"width\" of the basis functions. The value of $\\varepsilon$ is determined from the data using a common heuristic: it is set to the reciprocal of the median of all pairwise Euclidean distances between the sample points $\\{x_i\\}$.\n$$\\varepsilon = \\frac{1}{\\mathrm{median}\\left\\{\\lVert x_i - x_j \\rVert_2 : 1 \\le i  j \\le m\\right\\}}$$\nIf this median distance is zero (a rare event with continuous sampling), a small positive default value is used. The $m \\times m$ kernel matrix $K$ is then assembled, with entries $K_{ij} = K(x_i, x_j)$. The fitted values, $\\hat{\\mathbf{y}}_r$, are also the result of a linear smoother, $\\hat{\\mathbf{y}}_r = S_r \\mathbf{y}$, where the smoothing matrix $S_r$ is:\n$$S_r = K (K + \\lambda_r I)^{-1}$$\nHere, $\\lambda_r  0$ is the regularization parameter for the RBF model, and $I$ is the $m \\times m$ identity matrix.\n\nTo compare the generalization performance of the two models, we employ leave-one-out cross-validation (LOOCV). A key insight for linear smoothers of the form $\\hat{\\mathbf{y}} = S\\mathbf{y}$ is that the LOOCV residuals can be computed efficiently without refitting the model $m$ times. The error of the LOOCV prediction for the $i$-th data point, $\\hat{y}^{(-i)}$, is given by the formula:\n$$y_i - \\hat{y}^{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - S_{ii}}$$\nwhere $\\hat{y}_i$ is the original fitted value for point $i$, and $S_{ii}$ is the $i$-th diagonal element of the corresponding smoothing matrix $S$. This allows for the rapid computation of the LOOCV root mean squared error (RMSE) for each model:\n$$\\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m \\left(\\frac{y_i - \\hat{y}_i}{1 - S_{ii}}\\right)^2}$$\nThis formula is used to calculate $\\mathrm{RMSE}_{q}$ for the quadratic model and $\\mathrm{RMSE}_{r}$ for the RBF model.\n\nThe final step is to apply the specified decision rule. To avoid overfitting with the more flexible RBF model, a penalty is incorporated. The quadratic model is considered the default, and the RBF model is chosen only if its performance is superior by a sufficient margin. The rule is: choose the RBF model if\n$$\\mathrm{RMSE}_{r} + \\gamma  \\mathrm{RMSE}_{q}$$\nand choose the quadratic model otherwise. The margin term, $\\gamma$, is defined as:\n$$\\gamma = \\alpha \\frac{s_y}{\\sqrt{m}}$$\nwhere $\\alpha$ is a specified constant, $s_y$ is the sample standard deviation of the observed values $\\{y_i\\}$ (computed with Bessel's correction, i.e., division by $m-1$), and $m$ is the sample size. This margin penalizes the RBF model's complexity and shrinks as more data becomes available, reflecting increased confidence in the estimated errors.\n\nThe programmatic implementation will execute this entire sequence for each test case, reporting a binary indicator of the chosen model ($1$ for RBF, $0$ for quadratic), along with the computed $\\mathrm{RMSE}_{r}$ and $\\mathrm{RMSE}_{q}$.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n\n    def rastrigin_fn(x, A=10):\n        d = len(x)\n        return A * d + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n\n    def quadratic_bowl_fn(x):\n        return (x[0] - 1)**2 + 2 * (x[1] + 0.5)**2 + 3\n\n    def ackley_fn(x):\n        d = len(x)\n        sum_sq = np.sum(x**2)\n        sum_cos = np.sum(np.cos(2 * np.pi * x))\n        term1 = -20 * np.exp(-0.2 * np.sqrt(sum_sq / d))\n        term2 = -np.exp(sum_cos / d)\n        return term1 + term2 + 20 + np.e\n\n    def perturbed_quadratic_fn(x):\n        return x[0]**2 + x[1]**2 + 0.1 * np.sin(3 * x[0]) * np.sin(3 * x[1])\n\n    test_cases = [\n        {\n            \"name\": \"Rastrigin_2D\",\n            \"d\": 2,\n            \"f\": rastrigin_fn,\n            \"domain\": [-5.12, 5.12],\n            \"m\": 60,\n            \"seed\": 42,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1\n        },\n        {\n            \"name\": \"Quadratic_Bowl_2D\",\n            \"d\": 2,\n            \"f\": quadratic_bowl_fn,\n            \"domain\": [-2, 2],\n            \"m\": 40,\n            \"seed\": 123,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1\n        },\n        {\n            \"name\": \"Ackley_3D\",\n            \"d\": 3,\n            \"f\": ackley_fn,\n            \"domain\": [-2, 2],\n            \"m\": 80,\n            \"seed\": 2024,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1\n        },\n        {\n            \"name\": \"Perturbed_Quadratic_2D\",\n            \"d\": 2,\n            \"f\": perturbed_quadratic_fn,\n            \"domain\": [-1.5, 1.5],\n            \"m\": 25,\n            \"seed\": 7,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1,\n        }\n    ]\n\n    def _compute_loocv_rmse(y, y_hat, S_diag):\n        \"\"\"Computes LOOCV RMSE using the GCV trick.\"\"\"\n        # Handle cases where 1 - S_ii is very close to zero\n        # to prevent division by zero or large numerical errors.\n        # For ridge-type problems, 0  S_ii  1, so this is a safeguard.\n        denominator = 1 - S_diag\n        safe_denominator = np.where(np.abs(denominator)  1e-12, 1e-12, denominator)\n        loocv_errors = (y - y_hat) / safe_denominator\n        return np.sqrt(np.mean(loocv_errors**2))\n\n    def evaluate_rbf(X_pts, y, lambda_r):\n        \"\"\"Evaluates the RBF model and computes its LOOCV RMSE.\"\"\"\n        pairwise_dists = pdist(X_pts, 'euclidean')\n        median_dist = np.median(pairwise_dists)\n        \n        if median_dist  1e-8:\n            epsilon = 1.0 / 1e-8\n        else:\n            epsilon = 1.0 / median_dist\n        \n        sq_dists = squareform(pairwise_dists**2)\n        K = np.exp(-(epsilon**2) * sq_dists)\n        \n        m = X_pts.shape[0]\n        Im = np.eye(m)\n        \n        # S_r = K @ inv(K + lambda_r * I)\n        A = K + lambda_r * Im\n        A_inv = np.linalg.inv(A)\n        S_r = K @ A_inv\n        \n        y_hat_r = S_r @ y\n        S_r_diag = np.diag(S_r)\n        \n        return _compute_loocv_rmse(y, y_hat_r, S_r_diag)\n\n    def evaluate_quadratic(X_pts, y, d, lambda_q):\n        \"\"\"Evaluates the quadratic model and computes its LOOCV RMSE.\"\"\"\n        m = X_pts.shape[0]\n        \n        # Build design matrix X\n        num_basis_funcs = (d + 1) * (d + 2) // 2\n        X_design = np.zeros((m, num_basis_funcs))\n        \n        for i in range(m):\n            pt = X_pts[i]\n            features = [1.0]\n            features.extend(pt)\n            features.extend(pt**2)\n            for j in range(d):\n                for k in range(j + 1, d):\n                    features.append(pt[j] * pt[k])\n            X_design[i, :] = features\n\n        p = X_design.shape[1]\n        Ip = np.eye(p)\n        \n        # S_q = X @ inv(X.T @ X + lambda_q * I) @ X.T\n        B = X_design.T @ X_design + lambda_q * Ip\n        B_inv = np.linalg.inv(B)\n        \n        # Fitted values y_hat_q\n        y_hat_q = X_design @ (B_inv @ (X_design.T @ y))\n        \n        # Diagonal of S_q, S_q_ii = X_i @ inv(B) @ X_i.T\n        S_q_diag = np.sum((X_design @ B_inv) * X_design, axis=1)\n        \n        return _compute_loocv_rmse(y, y_hat_q, S_q_diag)\n    \n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        m = case[\"m\"]\n        domain = case[\"domain\"]\n        \n        # Generate sample points\n        rng = np.random.default_rng(case[\"seed\"])\n        X_pts = rng.uniform(domain[0], domain[1], size=(m, d))\n        y = np.array([case[\"f\"](x) for x in X_pts])\n        \n        # Evaluate models\n        rmse_r = evaluate_rbf(X_pts, y, case[\"lambda_r\"])\n        rmse_q = evaluate_quadratic(X_pts, y, d, case[\"lambda_q\"])\n        \n        # Apply switching rule\n        s_y = np.std(y, ddof=1) if m  1 else 0\n        gamma = case[\"alpha\"] * s_y / np.sqrt(m)\n        \n        b = 1 if rmse_r + gamma  rmse_q else 0\n        \n        results.append([b, rmse_r, rmse_q])\n\n    # Format the final output string exactly as required\n    result_strings = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3153247"}]}