{"hands_on_practices": [{"introduction": "A core task in multi-objective optimization is to not just find the set of optimal trade-offs (the Pareto front), but also to identify a single \"best\" compromise solution. This exercise provides a foundational walkthrough of this process, starting from first principles with a simple, analytically tractable problem. You will use the weighted-sum scalarization method to trace the Pareto front and then apply a geometric concept—curvature—to pinpoint the \"knee,\" which represents the point of maximum trade-off. [@problem_id:3160527]", "problem": "Consider a two-objective Multi-Objective Optimization (MOO) problem with decision variable $x \\in [0,1]$ and objective functions $f_{1}(x)=x^{2}$ and $f_{2}(x)=(x-1)^{2}$. Use the standard definitions of Pareto dominance and the Pareto front: a decision $x^{\\ast}$ is Pareto optimal if there is no $x$ such that $f_{i}(x) \\leq f_{i}(x^{\\ast})$ for all $i \\in \\{1,2\\}$ with at least one strict inequality, and the Pareto front is the image of the set of Pareto optimal decisions under the mapping $x \\mapsto (f_{1}(x),f_{2}(x))$.\n\nGround your derivations in the following well-tested facts:\n- For differentiable scalarization $J_{\\lambda}(x)$ on a convex set, the minimizers are characterized by first-order conditions where interior stationary points satisfy $\\frac{\\mathrm{d}}{\\mathrm{d}x}J_{\\lambda}(x)=0$, and boundary points are checked when interior solutions lie outside the feasible set.\n- The curvature of a twice-differentiable plane curve $r(s)=(u(s),v(s))$ is $\\kappa(s)=\\frac{|u'(s)v''(s)-v'(s)u''(s)|}{\\left(u'(s)^{2}+v'(s)^{2}\\right)^{3/2}}$, and a “knee” on the Pareto front can be operationally defined as the point of maximal curvature along the parametric image of the decision set.\n\nTasks:\n- Derive, from first principles, the structure of the Pareto optimal set in the decision space $[0,1]$ and its Pareto front in objective space $\\mathbb{R}^{2}$.\n- Introduce the weighted-sum scalarization $J_{\\lambda}(x)=\\lambda f_{1}(x)+(1-\\lambda)f_{2}(x)$ with $\\lambda \\in [0,1]$. Compute analytically the $\\lambda$-parametrized minimizer $x^{\\ast}(\\lambda)$ on $[0,1]$, and map it to the Pareto front.\n- Treat the image of $x \\mapsto (f_{1}(x),f_{2}(x))$ as a parametric plane curve. Using the curvature formula, compute the decision value $x_{\\text{knee}}$ that maximizes curvature and then determine the corresponding $\\lambda_{\\text{knee}}$ via the $\\lambda$-parametrization.\n- Extend to $\\mathbb{R}^{n}$ by fixing a nonzero vector $\\mathbf{a} \\in \\mathbb{R}^{n}$ and restricting the decision set to the line segment $\\{\\mathbf{x}=t\\mathbf{a}:t \\in [0,1]\\}$. Consider $F_{1}(t)=\\|\\mathbf{x}\\|^{2}=\\|t\\mathbf{a}\\|^{2}$ and $F_{2}(t)=\\|\\mathbf{x}-\\mathbf{a}\\|^{2}=\\|t\\mathbf{a}-\\mathbf{a}\\|^{2}$, apply the weighted-sum scalarization $J_{\\lambda}(t)=\\lambda F_{1}(t)+(1-\\lambda)F_{2}(t)$, and repeat the curvature-based knee computation. Show how the knee, expressed as a value of $\\lambda$, depends (or does not depend) on the dimension $n$ and the choice of $\\mathbf{a}$.\n\nAnswer specification:\nReport only the unique value of $\\lambda \\in [0,1]$ at which the knee occurs. The answer must be a single exact number. No rounding is required, and no units are involved.", "solution": "The problem asks for the unique value of the weighted-sum parameter $\\lambda \\in [0,1]$ that corresponds to the \"knee\" of the Pareto front, where the knee is defined as the point of maximum curvature. We will solve this by following the sequence of tasks outlined.\n\nFirst, we analyze the one-dimensional problem with objective functions $f_{1}(x)=x^{2}$ and $f_{2}(x)=(x-1)^{2}$ for a decision variable $x \\in [0,1]$.\n\nThe first step is to characterize the Pareto optimal set. The function $f_{1}(x)=x^{2}$ is strictly increasing on the interval $[0,1]$, while $f_{2}(x)=(x-1)^{2}$ is strictly decreasing on $[0,1]$. For any two distinct decision variables $x_{a}, x_{b} \\in [0,1]$ such that $x_{a}  x_{b}$, we have $f_{1}(x_{a})  f_{1}(x_{b})$ and $f_{2}(x_{a}) > f_{2}(x_{b})$. This means that any improvement in one objective function is necessarily accompanied by a degradation in the other. Consequently, no point in the decision space is dominated by another. Therefore, the entire decision set $x \\in [0,1]$ is the Pareto optimal set.\n\nNext, we employ the weighted-sum scalarization method to find the minimizers of the composite objective function $J_{\\lambda}(x)$, which provides a parameterization of the Pareto front. The scalarized function is given by:\n$$J_{\\lambda}(x) = \\lambda f_{1}(x) + (1-\\lambda)f_{2}(x) = \\lambda x^{2} + (1-\\lambda)(x-1)^{2}$$\nwhere $\\lambda \\in [0,1]$. To find the minimizer $x^{\\ast}(\\lambda)$, we apply the first-order necessary condition for an extremum, $\\frac{\\mathrm{d}J_{\\lambda}}{\\mathrm{d}x} = 0$, for interior points.\n$$\\frac{\\mathrm{d}J_{\\lambda}}{\\mathrm{d}x} = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\lambda x^{2} + (1-\\lambda)(x^{2} - 2x + 1) \\right) = 2\\lambda x + 2(1-\\lambda)(x-1)$$\nSetting the derivative to zero yields:\n$$2\\lambda x + 2(1-\\lambda)x - 2(1-\\lambda) = 0$$\n$$2x(\\lambda + 1 - \\lambda) - 2(1-\\lambda) = 0$$\n$$2x - 2(1-\\lambda) = 0 \\implies x = 1-\\lambda$$\nSince $\\lambda \\in [0,1]$, the solution $x^{\\ast}(\\lambda) = 1-\\lambda$ is always within the feasible domain $[0,1]$. The second derivative is $\\frac{\\mathrm{d}^{2}J_{\\lambda}}{\\mathrm{d}x^{2}} = 2\\lambda + 2(1-\\lambda) = 2 > 0$, confirming that $x^{\\ast}(\\lambda)$ is a unique minimizer for any $\\lambda \\in [0,1]$. This gives us a mapping from the weight $\\lambda$ to the Pareto optimal solution $x^{\\ast}$.\n\nNow, we determine the knee of the Pareto front, defined as the point of maximum curvature. The Pareto front is the set of points $(f_{1}(x), f_{2}(x))$ for $x \\in [0,1]$. We can treat this as a parametric plane curve $r(x) = (u(x), v(x))$ with parameter $x$, where $u(x) = f_{1}(x) = x^{2}$ and $v(x) = f_{2}(x) = (x-1)^{2}$. The derivatives with respect to $x$ are:\n$$u'(x) = 2x, \\quad v'(x) = 2(x-1)$$\n$$u''(x) = 2, \\quad v''(x) = 2$$\nUsing the given formula for curvature $\\kappa(x)$:\n$$\\kappa(x) = \\frac{|u'(x)v''(x) - v'(x)u''(x)|}{\\left(u'(x)^{2} + v'(x)^{2}\\right)^{3/2}}$$\nThe numerator of the curvature expression is:\n$$|u'v'' - v'u''| = |(2x)(2) - (2(x-1))(2)| = |4x - 4x + 4| = 4$$\nThe denominator is:\n$$\\left(u'(x)^{2} + v'(x)^{2}\\right)^{3/2} = \\left((2x)^{2} + (2(x-1))^{2}\\right)^{3/2} = \\left(4x^{2} + 4(x^{2}-2x+1)\\right)^{3/2}$$\n$$= \\left(8x^{2} - 8x + 4\\right)^{3/2} = \\left(4(2x^{2} - 2x + 1)\\right)^{3/2} = 8\\left(2x^{2} - 2x + 1\\right)^{3/2}$$\nThus, the curvature is:\n$$\\kappa(x) = \\frac{4}{8\\left(2x^{2} - 2x + 1\\right)^{3/2}} = \\frac{1}{2\\left(2x^{2} - 2x + 1\\right)^{3/2}}$$\nTo maximize $\\kappa(x)$, we must minimize the denominator, which is equivalent to minimizing the quadratic function $g(x) = 2x^{2} - 2x + 1$ over $x \\in [0,1]$. The vertex of this upward-opening parabola occurs at $x = -\\frac{-2}{2(2)} = \\frac{1}{2}$. Since this value lies within the domain $[0,1]$, it is the location of the minimum of $g(x)$.\nTherefore, the knee of the Pareto front corresponds to the decision variable $x_{\\text{knee}} = \\frac{1}{2}$.\nWe now find the value of $\\lambda_{\\text{knee}}$ that yields this solution using the relation $x^{\\ast}(\\lambda) = 1-\\lambda$:\n$$x_{\\text{knee}} = 1 - \\lambda_{\\text{knee}} \\implies \\frac{1}{2} = 1 - \\lambda_{\\text{knee}} \\implies \\lambda_{\\text{knee}} = \\frac{1}{2}$$\n\nFinally, we extend the analysis to the $n$-dimensional case. The decision set is $\\{\\mathbf{x} = t\\mathbf{a} : t \\in [0,1]\\}$ for a nonzero vector $\\mathbf{a} \\in \\mathbb{R}^{n}$. The decision variable is $t$. The objective functions are:\n$$F_{1}(t) = \\|\\mathbf{x}\\|^{2} = \\|t\\mathbf{a}\\|^{2} = t^{2}\\|\\mathbf{a}\\|^{2}$$\n$$F_{2}(t) = \\|\\mathbf{x}-\\mathbf{a}\\|^{2} = \\|t\\mathbf{a}-\\mathbf{a}\\|^{2} = \\|(t-1)\\mathbf{a}\\|^{2} = (t-1)^{2}\\|\\mathbf{a}\\|^{2}$$\nLet $C = \\|\\mathbf{a}\\|^{2}$. Since $\\mathbf{a}$ is nonzero, $C > 0$. The objective functions are $F_{1}(t) = Ct^{2}$ and $F_{2}(t) = C(t-1)^{2}$. This problem is structurally identical to the one-dimensional case, with the objectives scaled by a constant factor $C$.\n\nThe scalarized function is $J_{\\lambda}(t) = \\lambda F_{1}(t) + (1-\\lambda)F_{2}(t) = C \\left( \\lambda t^{2} + (1-\\lambda)(t-1)^{2} \\right)$. Minimizing $J_{\\lambda}(t)$ with respect to $t$ is equivalent to minimizing the term in the parenthesis, which has the same form as the 1D case. The minimizer is thus $t^{\\ast}(\\lambda) = 1-\\lambda$.\n\nThe Pareto front is parameterized by $t \\in [0,1]$ as $r(t) = (Ct^{2}, C(t-1)^{2})$. The derivatives are $u'(t)=2Ct, v'(t)=2C(t-1), u''(t)=2C, v''(t)=2C$.\nThe curvature is:\n$$\\kappa(t) = \\frac{|(2Ct)(2C) - (2C(t-1))(2C)|}{\\left((2Ct)^{2} + (2C(t-1))^{2}\\right)^{3/2}} = \\frac{|4C^{2}|}{\\left(4C^{2}(t^{2} + (t-1)^{2})\\right)^{3/2}}$$\n$$\\kappa(t) = \\frac{4C^{2}}{8C^{3}\\left(2t^{2}-2t+1\\right)^{3/2}} = \\frac{1}{2C\\left(2t^{2}-2t+1\\right)^{3/2}}$$\nMaximizing $\\kappa(t)$ requires minimizing $g(t) = 2t^{2}-2t+1$, which again occurs at $t_{\\text{knee}} = \\frac{1}{2}$.\nUsing the correspondence $t^{\\ast}(\\lambda)=1-\\lambda$, we find:\n$$t_{\\text{knee}} = 1 - \\lambda_{\\text{knee}} \\implies \\frac{1}{2} = 1 - \\lambda_{\\text{knee}} \\implies \\lambda_{\\text{knee}} = \\frac{1}{2}$$\nThis result is independent of the dimension $n$ and the specific choice of the non-zero vector $\\mathbf{a}$. The unique value of $\\lambda$ at which the knee occurs is $\\frac{1}{2}$.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3160527"}, {"introduction": "The weighted-sum method is a powerful tool, but it is crucial to understand its limitations, especially in non-convex problems. This practice explores a scenario where this standard approach fails to identify all Pareto optimal solutions. By working with a specially designed non-convex feasible set, you will discover the existence of \"unsupported\" Pareto points, providing critical insight into why more advanced solution techniques are sometimes necessary. [@problem_id:3160625]", "problem": "Consider a bi-objective minimization problem in the plane, where both objectives are to be minimized simultaneously in the sense of Pareto optimality. Use the fundamentals of multi-objective optimization: pointwise dominance, Pareto optimality, Pareto front, and the weighted-sum scalarization. Let the feasible set be the nonconvex arc\n$$\nX \\;=\\; \\big\\{\\, (x_1,x_2) = \\big(t,\\; 1 - t + \\alpha \\big(1 - |2t - 1|\\big)\\big) \\;\\big|\\; t \\in [0,1] \\,\\big\\},\n$$\nwith parameter $\\alpha$ chosen so that $0  \\alpha  \\frac{1}{2}$. Define the convex objectives\n$$\nf_1(x_1,x_2) \\;=\\; x_1, \n\\qquad \nf_2(x_1,x_2) \\;=\\; x_2,\n$$\nand the weighted-sum scalarization\n$$\nF_{\\lambda}(x_1,x_2) \\;=\\; \\lambda f_1(x_1,x_2) + (1-\\lambda) f_2(x_1,x_2),\n\\qquad \\lambda \\in [0,1].\n$$\nStarting only from the core definitions of Pareto dominance (coordinatewise comparison), Pareto optimality (no feasible dominator), and supported Pareto points (minimizers of $F_{\\lambda}$ for some $\\lambda \\in [0,1]$), do the following:\n- Establish that every point in $X$ is Pareto optimal and identify which Pareto points are unsupported (i.e., cannot be recovered as a minimizer of $F_{\\lambda}$ for any $\\lambda \\in [0,1]$).\n- Argue whether the minimizers of $F_{\\lambda}$ on $X$ occur at the endpoints of $X$ or in its interior, and explain your conclusion using only the stated fundamentals.\nFinally, determine the unique weight $\\lambda^{\\star} \\in [0,1]$ at which the two endpoints of $X$, namely $(0,1)$ and $(1,0)$, tie as minimizers of $F_{\\lambda}$ on $X$. Provide your answer as a single exact expression. No rounding is required.", "solution": "The problem is a bi-objective minimization where the objective vector is $\\mathbf{f}(x_1, x_2) = (f_1(x_1,x_2), f_2(x_1,x_2)) = (x_1, x_2)$. Minimizing this vector means finding points in the feasible set $X$ that are Pareto optimal.\n\nFirst, let's analyze the feasible set $X$, which is an arc parameterized by $t \\in [0,1]$. The coordinates are $x_1(t) = t$ and $x_2(t) = 1 - t + \\alpha(1-|2t-1|)$. We can express $x_2(t)$ piecewise:\n- For $t \\in [0, 1/2]$, $|2t-1| = 1-2t$. So, $x_2(t) = 1 - t + \\alpha(1 - (1-2t)) = 1 - t + 2\\alpha t = 1 + (2\\alpha-1)t$.\n- For $t \\in [1/2, 1]$, $|2t-1| = 2t-1$. So, $x_2(t) = 1 - t + \\alpha(1 - (2t-1)) = 1 - t + \\alpha(2-2t) = (1+2\\alpha) - (1+2\\alpha)t$.\n\nThe set $X$ is composed of two line segments connecting $(0,1)$ to $(1/2, 1/2 + \\alpha)$ and then to $(1,0)$.\n\n**1. Pareto Optimality of Points in $X$**\nA point $\\mathbf{p} \\in X$ is Pareto optimal if no other point $\\mathbf{q} \\in X$ dominates it. Let's analyze the derivatives. $x_1'(t) = 1 > 0$ for all $t$. For $x_2(t)$, the derivatives are:\n- For $t \\in (0, 1/2)$, $x_2'(t) = 2\\alpha-1$. Since $0  \\alpha  1/2$, we have $2\\alpha-1  0$.\n- For $t \\in (1/2, 1)$, $x_2'(t) = -(1+2\\alpha)$. Since $\\alpha>0$, we have $-(1+2\\alpha)  0$.\nSince $x_1(t)$ is strictly increasing and $x_2(t)$ is strictly decreasing over $[0,1]$, for any two points corresponding to $s  t$, we have $x_1(s)  x_1(t)$ and $x_2(s) > x_2(t)$. This means neither point can dominate the other. Therefore, every point in $X$ is Pareto optimal.\n\n**2. Supported and Unsupported Pareto Points**\nA supported Pareto point minimizes the weighted-sum scalarization $F_{\\lambda}$ for some $\\lambda \\in [0,1]$. We express $F_{\\lambda}$ as a function of $t$:\n$g(t) = F_{\\lambda}(x_1(t), x_2(t)) = \\lambda t + (1-\\lambda)[1-t+\\alpha(1-|2t-1|)]$.\nThis function $g(t)$ is continuous and piecewise linear. Its global minimum on $[0,1]$ must occur at an endpoint ($t=0, t=1$) or a point of non-differentiability ($t=1/2$).\nLet's evaluate $g(t)$ at these points:\n- $g(0) = \\lambda(0) + (1-\\lambda)x_2(0) = 1-\\lambda$.\n- $g(1) = \\lambda(1) + (1-\\lambda)x_2(1) = \\lambda$.\n- $g(1/2) = \\lambda(1/2) + (1-\\lambda)x_2(1/2) = \\frac{\\lambda}{2} + (1-\\lambda)(\\frac{1}{2}+\\alpha) = \\frac{1}{2} + \\alpha(1-\\lambda)$.\n\nFor the point at $t=1/2$ to be a minimizer, its value must be less than or equal to the values at the endpoints:\n1) $g(1/2) \\le g(0) \\implies \\frac{1}{2} + \\alpha(1-\\lambda) \\le 1-\\lambda \\implies \\lambda(1-\\alpha) \\le \\frac{1}{2}-\\alpha \\implies \\lambda \\le \\frac{1-2\\alpha}{2(1-\\alpha)}$.\n2) $g(1/2) \\le g(1) \\implies \\frac{1}{2} + \\alpha(1-\\lambda) \\le \\lambda \\implies \\frac{1}{2} + \\alpha \\le \\lambda(1+\\alpha) \\implies \\lambda \\ge \\frac{1+2\\alpha}{2(1+\\alpha)}$.\n\nFor such a $\\lambda$ to exist, we require $\\frac{1+2\\alpha}{2(1+\\alpha)} \\le \\frac{1-2\\alpha}{2(1-\\alpha)}$. Since $\\alpha > 0$ and $\\alpha  1/2$, both denominators are positive. Cross-multiplying gives:\n$(1+2\\alpha)(1-\\alpha) \\le (1-2\\alpha)(1+\\alpha)$\n$1+\\alpha-2\\alpha^2 \\le 1-\\alpha-2\\alpha^2 \\implies \\alpha \\le -\\alpha \\implies 2\\alpha \\le 0 \\implies \\alpha \\le 0$.\nThis contradicts the given condition $\\alpha > 0$. Thus, there is no $\\lambda \\in [0,1]$ for which the point at $t=1/2$ is a minimizer. The minimum of $g(t)$ always occurs at the endpoints $t=0$ or $t=1$.\nThis means the only supported Pareto optimal points are the endpoints of $X$, corresponding to $t=0$ and $t=1$: $\\{(0,1), (1,0)\\}$. All other points in $X$, the open arc for $t \\in (0,1)$, are unsupported Pareto optimal points.\n\n**3. Unique Weight $\\lambda^{\\star}$**\nWe seek the weight $\\lambda^{\\star}$ where the two endpoints tie as minimizers. This occurs when the value of the objective function is the same for both points:\n$g(0) = g(1)$\n$1 - \\lambda = \\lambda$\n$1 = 2\\lambda$\n$\\lambda^{\\star} = \\frac{1}{2}$.\nThis value is unique and lies in $[0,1]$. For $\\lambda=1/2$, the objective value at both endpoints is $1/2$. The value at $t=1/2$ is $g(1/2) = 1/2 + \\alpha(1-1/2) = 1/2 + \\alpha/2$, which is strictly greater than $1/2$ since $\\alpha>0$. Thus, for $\\lambda=1/2$, the endpoints are indeed the joint minimizers.\nThe unique weight is $\\lambda^{\\star} = 1/2$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3160625"}, {"introduction": "Many real-world design problems involve choosing from a finite set of options rather than a continuous space of possibilities. This exercise shifts our focus to such a discrete setting, demonstrating how a simple \"greedy\" decision-making strategy can lead to suboptimal outcomes when faced with multiple conflicting objectives. You will compare the result of a naive heuristic to the \"knee\" of the discrete Pareto front, quantifying the performance gap and building an intuition for why a holistic view of the trade-off space is essential for effective decision-making. [@problem_id:3160572]", "problem": "Consider a bi-objective minimization problem in multiobjective optimization (MOO) with objectives $f_1$ and $f_2$. A solution $x$ is said to Pareto dominate a solution $y$ if $f_i(x) \\le f_i(y)$ for all $i \\in \\{1,2\\}$ and $f_j(x)  f_j(y)$ for at least one $j$. The collection of all undominated solutions is called the Pareto front.\n\nYou are given a discrete feasible set of designs $\\{x_A, x_B, x_C, x_D, x_E, x_F\\}$ with objective vectors\n$$\nf(x_A) = (1, 16),\\quad\nf(x_B) = (3, 9.5),\\quad\nf(x_C) = (5, 6.5),\\quad\nf(x_D) = (8, 5.5),\\quad\nf(x_E) = (12, 4.8),\\quad\nf(x_F) = (16, 1).\n$$\nAll objectives are to be minimized.\n\nA greedy procedure “by $f_1$ then $f_2$” selects the design with the smallest $f_1$-value; ties, if any, are broken by choosing the smallest $f_2$-value among those tied.\n\nDefine the “knee” of the Pareto front as the Pareto-efficient design that maximizes the Euclidean perpendicular distance in the objective space from the straight line passing through the two extreme Pareto designs with minimal $f_1$ and minimal $f_2$ respectively. This definition captures the point of greatest trade-off curvature between $f_1$ and $f_2$ based on the extremes.\n\nTasks:\n- Using the definition of Pareto dominance, identify the two extreme Pareto designs corresponding to the smallest $f_1$ and smallest $f_2$.\n- Identify the design selected by the greedy “by $f_1$ then $f_2$” procedure.\n- Identify the knee design according to the definition above.\n- Compute, in exact closed form, the magnitude of the gap in the objective space, defined as the Euclidean distance between the greedy-selected design’s objective vector and the knee design’s objective vector.\n\nYour final answer must be a single closed-form analytic expression. Do not round. No units are required.", "solution": "Let's denote the objective vectors as points in $\\mathbb{R}^2$: $P_A=(1, 16)$, $P_B=(3, 9.5)$, $P_C=(5, 6.5)$, $P_D=(8, 5.5)$, $P_E=(12, 4.8)$, $P_F=(16, 1)$.\n\nFirst, we identify the set of non-dominated solutions, which forms the Pareto front. The $f_1$ components of the objective vectors are $1, 3, 5, 8, 12, 16$, which is a strictly increasing sequence. The $f_2$ components are $16, 9.5, 6.5, 5.5, 4.8, 1$, which is a strictly decreasing sequence. For any two distinct points $P_i$ and $P_j$, if the first component of $P_i$ is smaller than that of $P_j$, its second component is larger. This means that neither point can dominate the other. Consequently, all six designs are non-dominated (i.e., Pareto-efficient), and the set of their objective vectors constitutes the Pareto front.\n\nNow, we proceed with the specified tasks.\n\n**1. Identify the extreme Pareto designs.**\nThe extreme Pareto designs are those with the minimal $f_1$ and minimal $f_2$ values, respectively.\n- The minimum value of $f_1$ is $1$, which corresponds to design $x_A$ with objective vector $P_A = (1, 16)$.\n- The minimum value of $f_2$ is $1$, which corresponds to design $x_F$ with objective vector $P_F = (16, 1)$.\nThe two extreme Pareto designs are $x_A$ and $x_F$.\n\n**2. Identify the design selected by the greedy procedure.**\nThe greedy procedure selects the design with the smallest $f_1$-value. This is design $x_A$, with $f_1=1$. Since there are no ties for the minimum $f_1$-value, the greedy-selected design is $x_A$, with objective vector $P_A = (1, 16)$.\n\n**3. Identify the knee design.**\nThe knee is the Pareto-efficient design that maximizes the perpendicular distance to the line passing through the two extreme points, $P_A=(1, 16)$ and $P_F=(16, 1)$.\nThe equation of the line passing through $(1, 16)$ and $(16, 1)$ has a slope $m = \\frac{1 - 16}{16 - 1} = -1$. The equation is $f_2 - 16 = -1(f_1 - 1)$, which simplifies to $f_1 + f_2 - 17 = 0$.\nThe perpendicular distance $d$ from a point $(f_{1,0}, f_{2,0})$ to this line is given by $d = \\frac{|f_{1,0} + f_{2,0} - 17|}{\\sqrt{1^2 + 1^2}} = \\frac{|f_{1,0} + f_{2,0} - 17|}{\\sqrt{2}}$.\nWe calculate this distance for all Pareto-efficient points:\n- For $P_A=(1, 16)$: $d_A = \\frac{|1 + 16 - 17|}{\\sqrt{2}} = 0$.\n- For $P_B=(3, 9.5)$: $d_B = \\frac{|3 + 9.5 - 17|}{\\sqrt{2}} = \\frac{|-4.5|}{\\sqrt{2}} = \\frac{4.5}{\\sqrt{2}}$.\n- For $P_C=(5, 6.5)$: $d_C = \\frac{|5 + 6.5 - 17|}{\\sqrt{2}} = \\frac{|-5.5|}{\\sqrt{2}} = \\frac{5.5}{\\sqrt{2}}$.\n- For $P_D=(8, 5.5)$: $d_D = \\frac{|8 + 5.5 - 17|}{\\sqrt{2}} = \\frac{|-3.5|}{\\sqrt{2}} = \\frac{3.5}{\\sqrt{2}}$.\n- For $P_E=(12, 4.8)$: $d_E = \\frac{|12 + 4.8 - 17|}{\\sqrt{2}} = \\frac{|-0.2|}{\\sqrt{2}} = \\frac{0.2}{\\sqrt{2}}$.\n- For $P_F=(16, 1)$: $d_F = \\frac{|16 + 1 - 17|}{\\sqrt{2}} = 0$.\nThe maximum distance is $d_C = \\frac{5.5}{\\sqrt{2}}$, which corresponds to design $x_C$.\nThe knee design is $x_C$, with objective vector $P_C = (5, 6.5)$.\n\n**4. Compute the magnitude of the gap.**\nThis is the Euclidean distance between the greedy-selected design's objective vector ($P_A$) and the knee design's objective vector ($P_C$).\n- Greedy objective vector: $P_{greedy} = P_A = (1, 16)$.\n- Knee objective vector: $P_{knee} = P_C = (5, 6.5)$.\nThe Euclidean distance $D$ is $D = \\sqrt{(f_{1,knee} - f_{1,greedy})^2 + (f_{2,knee} - f_{2,greedy})^2}$.\n$D = \\sqrt{(5 - 1)^2 + (6.5 - 16)^2}$\n$D = \\sqrt{4^2 + (-9.5)^2}$\nUsing fractions for an exact form, $9.5 = \\frac{19}{2}$.\n$D = \\sqrt{16 + \\left(-\\frac{19}{2}\\right)^2} = \\sqrt{16 + \\frac{361}{4}} = \\sqrt{\\frac{64}{4} + \\frac{361}{4}} = \\sqrt{\\frac{425}{4}}$\n$D = \\frac{\\sqrt{425}}{2}$\nTo simplify $\\sqrt{425}$, we find its prime factors: $425 = 25 \\times 17 = 5^2 \\times 17$.\nSo, $\\sqrt{425} = 5\\sqrt{17}$.\nThe distance is $D = \\frac{5\\sqrt{17}}{2}$.", "answer": "$$\\boxed{\\frac{5\\sqrt{17}}{2}}$$", "id": "3160572"}]}