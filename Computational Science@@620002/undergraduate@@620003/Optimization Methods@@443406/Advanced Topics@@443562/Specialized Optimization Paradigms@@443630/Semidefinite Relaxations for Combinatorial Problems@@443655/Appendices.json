{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise designed to build your core understanding of the Goemans-Williamson algorithm. By applying the semidefinite programming (SDP) relaxation to two elementary graphs—the triangle ($K_3$) and the square ($C_4$)—you will compute the exact relaxed values and compare them to the true maximum cut values. This hands-on calculation [@problem_id:3177813] reveals the concept of the integrality gap and provides crucial intuition for why the relaxation is tight for bipartite graphs but not for others.", "problem": "Consider the Maximum Cut problem on two simple undirected unit-weight graphs: the triangle with $3$ vertices (denoted $K_3$) and the square cycle with $4$ vertices (denoted $C_4$). The Maximum Cut objective can be written using sign variables $s_i \\in \\{-1, +1\\}$ assigned to vertices $i \\in V$ and edge weights $w_{ij} \\ge 0$ as the sum over edges of an indicator that the endpoints receive opposite signs. The Goemans–Williamson (GW) semidefinite programming (SDP) relaxation replaces these signs by unit vectors in a Euclidean space.\n\nYour tasks are:\n1. Formulate the GW SDP relaxation for Maximum Cut in terms of unit vectors $\\{v_i\\}_{i \\in V}$ with $\\|v_i\\| = 1$, and express the relaxed objective in terms of pairwise inner products $v_i^{\\top} v_j$ over edges.\n2. For $K_3$ and for $C_4$, compute the exact optimal SDP objective value by explicit construction or by a first-principles argument.\n3. Apply the GW random hyperplane rounding to an optimal SDP vector solution in each case, and compute the exact expected cut value produced by this rounding (expressed in terms of the edge angles determined by the inner products).\n4. For each graph, define the integrality gap bound as the ratio $\\rho = \\dfrac{\\text{expected rounded cut value}}{\\text{optimal SDP value}}$ and compute this ratio exactly.\n\nProvide your final answer as a single row matrix containing the two ratios for $K_3$ and $C_4$, in that order, in exact fractional form. No rounding is required, and no units are to be used.", "solution": "The analysis proceeds in four parts as required by the problem statement. First, we formulate the Goemans-Williamson (GW) Semidefinite Programming (SDP) relaxation for the Maximum Cut problem. Second and third, we apply this framework to the graphs $K_3$ and $C_4$ to compute their optimal SDP values and the expected values from the GW rounding procedure. Fourth, we compute the integrality gap bound for each case.\n\n**1. Formulation of the Goemans-Williamson SDP Relaxation**\n\nThe Maximum Cut (Max-Cut) problem on an undirected graph $G=(V, E)$ with non-negative edge weights $w_{ij}$ seeks a partition of the vertex set $V$ into two disjoint sets, say $S$ and $V \\setminus S$, such that the sum of weights of the edges crossing the partition is maximized.\n\nThis can be formulated as an integer quadratic program. Let a variable $s_i \\in \\{-1, +1\\}$ be associated with each vertex $i \\in V$, where $s_i = +1$ if $i \\in S$ and $s_i = -1$ if $i \\in V \\setminus S$. An edge $(i,j)$ is in the cut if and only if $s_i$ and $s_j$ have opposite signs, i.e., $s_i s_j = -1$. The term $\\frac{1}{2}(1 - s_i s_j)$ is $1$ if the edge is cut and $0$ otherwise. The Max-Cut objective is to maximize the total weight of the cut edges:\n$$ \\text{MAX-CUT} = \\max_{s_i \\in \\{-1, +1\\}} \\sum_{(i,j) \\in E} \\frac{w_{ij}}{2} (1 - s_i s_j) $$\nFor the unit-weight graphs specified, $w_{ij}=1$ for all $(i,j) \\in E$. The problem is equivalent to:\n$$ \\max \\sum_{(i,j) \\in E} \\frac{1}{2} (1 - s_i s_j) \\quad \\text{subject to} \\quad s_i^2 = 1, \\quad \\forall i \\in V $$\nThe Goemans-Williamson SDP relaxation lifts this problem from scalars to vectors. Each scalar variable $s_i$ is replaced by a a unit vector $v_i$ in a higher-dimensional Euclidean space, typically $\\mathbb{R}^{|V|}$. The quadratic constraint $s_i^2=1$ becomes the vector-norm constraint $\\|v_i\\| = 1$. The product $s_i s_j$ is replaced by the inner product $v_i^\\top v_j$.\n\nThe GW SDP relaxation is thus formulated as:\n$$ Z_{SDP} = \\max \\sum_{(i,j) \\in E} \\frac{1}{2} (1 - v_i^\\top v_j) \\quad \\text{subject to} \\quad \\|v_i\\| = 1, \\quad \\forall i \\in V $$\nHere, the vectors $v_i$ are optimization variables. This completes the first task.\n\n**2. Analysis for the Triangle Graph ($K_3$)**\n\nThe graph $K_3$ has $V = \\{1, 2, 3\\}$ and $E = \\{(1,2), (2,3), (3,1)\\}$.\n\n**Optimal SDP Value for $K_3$:**\nThe objective function for $K_3$ is:\n$$ Z(K_3) = \\frac{1}{2}(1 - v_1^\\top v_2) + \\frac{1}{2}(1 - v_2^\\top v_3) + \\frac{1}{2}(1 - v_3^\\top v_1) = \\frac{3}{2} - \\frac{1}{2}(v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1) $$\nTo maximize $Z(K_3)$, we must minimize the sum of the inner products $v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1$. A fundamental inequality for three unit vectors is derived from the non-negativity of the squared norm of their sum:\n$$ \\|v_1 + v_2 + v_3\\|^2 \\ge 0 $$\n$$ (v_1 + v_2 + v_3)^\\top(v_1 + v_2 + v_3) = \\sum_{i=1}^3 \\|v_i\\|^2 + 2\\sum_{1 \\le i < j \\le 3} v_i^\\top v_j \\ge 0 $$\nSince $\\|v_i\\|=1$ for $i=1,2,3$, this becomes:\n$$ 3 + 2(v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1) \\ge 0 $$\n$$ v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1 \\ge -\\frac{3}{2} $$\nThis lower bound is achievable. Let the vectors $v_1, v_2, v_3$ be coplanar in $\\mathbb{R}^2$, separated by angles of $2\\pi/3$ radians ($120^\\circ$). For instance:\n$v_1 = (1, 0)$, $v_2 = (\\cos(2\\pi/3), \\sin(2\\pi/3)) = (-1/2, \\sqrt{3}/2)$, $v_3 = (\\cos(4\\pi/3), \\sin(4\\pi/3)) = (-1/2, -\\sqrt{3}/2)$.\nFor this configuration, $v_1+v_2+v_3 = 0$, and the inner products are $v_i^\\top v_j = \\cos(2\\pi/3) = -1/2$ for all $i \\ne j$.\nSubstituting this minimal sum into the objective function gives the optimal SDP value:\n$$ Z_{SDP}(K_3) = \\frac{3}{2} - \\frac{1}{2}\\left(-\\frac{3}{2}\\right) = \\frac{3}{2} + \\frac{3}{4} = \\frac{9}{4} $$\n\n**Expected Rounded Cut Value for $K_3$:**\nThe GW rounding algorithm selects a random hyperplane and partitions vertices based on which side their corresponding vectors lie. The probability that an edge $(i, j)$ is cut is given by $\\frac{\\theta_{ij}}{\\pi}$, where $\\theta_{ij} = \\arccos(v_i^\\top v_j)$ is the angle between vectors $v_i$ and $v_j$. The expected value of the cut is the sum of these probabilities over all edges (since weights are $1$).\nFor the optimal SDP solution for $K_3$, $v_i^\\top v_j = -1/2$ for all edges. The angle is:\n$$ \\theta_{ij} = \\arccos(-1/2) = \\frac{2\\pi}{3} $$\nThe expected cut value is the sum over the three edges:\n$$ E[\\text{cut}(K_3)] = \\sum_{(i,j) \\in E} \\frac{\\arccos(v_i^\\top v_j)}{\\pi} = 3 \\times \\frac{2\\pi/3}{\\pi} = 2 $$\n\n**Integrality Gap Bound for $K_3$:**\nThe ratio $\\rho$ is:\n$$ \\rho(K_3) = \\frac{E[\\text{cut}(K_3)]}{Z_{SDP}(K_3)} = \\frac{2}{9/4} = \\frac{8}{9} $$\n\n**3. Analysis for the Square Cycle Graph ($C_4$)**\n\nThe graph $C_4$ has $V = \\{1, 2, 3, 4\\}$ and $E = \\{(1,2), (2,3), (3,4), (4,1)\\}$.\n\n**Optimal SDP Value for $C_4$:**\nThe objective function for $C_4$ is:\n$$ Z(C_4) = \\frac{1}{2}(1 - v_1^\\top v_2) + \\frac{1}{2}(1 - v_2^\\top v_3) + \\frac{1}{2}(1 - v_3^\\top v_4) + \\frac{1}{2}(1 - v_4^\\top v_1) $$\nThe graph $C_4$ is bipartite, with partitions $S_1 = \\{1, 3\\}$ and $S_2 = \\{2, 4\\}$. The maximum cut places $S_1$ and $S_2$ on opposite sides, cutting all $4$ edges. The optimal integer value is $4$.\nSince the SDP is a relaxation, its optimal value must be at least the integer optimum: $Z_{SDP}(C_4) \\ge 4$.\nWe can construct a feasible SDP solution that achieves this value. Let $v_1 = v_3$ and $v_2 = v_4$, with $v_1 = -v_2$. For instance, in $\\mathbb{R}^1$, let $v_1=v_3=1$ and $v_2=v_4=-1$. These are unit vectors.\nThe inner products for the edges are:\n$v_1^\\top v_2 = (1)(-1) = -1$.\n$v_2^\\top v_3 = (-1)(1) = -1$.\n$v_3^\\top v_4 = (1)(-1) = -1$.\n$v_4^\\top v_1 = (-1)(1) = -1$.\nSubstituting these values:\n$$ Z_{SDP}(C_4) = \\frac{1}{2}(1 - (-1)) + \\frac{1}{2}(1 - (-1)) + \\frac{1}{2}(1 - (-1)) + \\frac{1}{2}(1 - (-1)) = 4 \\times \\frac{2}{2} = 4 $$\nSince we found a feasible solution with value $4$ and we know $Z_{SDP}(C_4) \\ge 4$, the optimal SDP value is exactly $4$.\n\n**Expected Rounded Cut Value for $C_4$:**\nFor the optimal SDP solution found above, the inner product for every edge is $v_i^\\top v_j = -1$.\nThe angle for each edge is:\n$$ \\theta_{ij} = \\arccos(-1) = \\pi $$\nThe expected cut value is the sum over the four edges:\n$$ E[\\text{cut}(C_4)] = \\sum_{(i,j) \\in E} \\frac{\\arccos(v_i^\\top v_j)}{\\pi} = 4 \\times \\frac{\\pi}{\\pi} = 4 $$\n\n**Integrality Gap Bound for $C_4$:**\nThe ratio $\\rho$ is:\n$$ \\rho(C_4) = \\frac{E[\\text{cut}(C_4)]}{Z_{SDP}(C_4)} = \\frac{4}{4} = 1 $$\n\n**4. Final Answer**\n\nThe ratios for $K_3$ and $C_4$ are $\\rho(K_3) = 8/9$ and $\\rho(C_4) = 1$. The required final answer is the row matrix containing these two values in order.", "answer": "$$ \\boxed{\n\\begin{pmatrix}\n\\frac{8}{9} & 1\n\\end{pmatrix}\n} $$", "id": "3177813"}, {"introduction": "The power of SDP relaxation lies not just in solving the relaxed problem but also in effectively 'rounding' the continuous solution back to a discrete one. This exercise [@problem_id:3177821] explores the geometry of the rounding step through a carefully constructed hypothetical scenario where the standard random hyperplane method may not perform well. By comparing this with an adaptive, data-aware rounding strategy, you will develop a deeper insight into how the configuration of the solution vectors influences the final outcome.", "problem": "Consider the standard semidefinite programming (SDP) relaxation for the Maximum Cut (MAX-CUT) problem on a weighted undirected graph with vertex set $\\{1,2,3,4,5,6\\}$ and symmetric weight matrix $W \\in \\mathbb{R}^{6 \\times 6}$. The SDP introduces unit vectors $v_i \\in \\mathbb{R}^2$ for each vertex $i$ and maximizes an objective of the form\n$$\n\\sum_{1 \\leq i < j \\leq 6} w_{ij} \\cdot \\frac{1 - v_i^{\\top} v_j}{2},\n$$\nsubject to $\\|v_i\\|_2 = 1$ for all $i$. A standard randomized rounding maps the vectors $v_i$ to binary labels by sampling a random hyperplane through the origin and assigning $x_i = \\operatorname{sign}(g^{\\top} v_i)$, where $g \\in \\mathbb{R}^2$ is a random direction, thus producing a cut.\n\nIn this problem, analyze a case where the random hyperplane rounding is ineffective due to near-collinearity of many $v_i$, and then use an adaptive, data-dependent hyperplane whose orientation is chosen using the weight matrix $W$.\n\nThe instance is specified as follows.\n\n- The graph has two vertex groups $A = \\{1,2,3\\}$ and $B = \\{4,5,6\\}$. The weights are $w_{ij} = 1$ if $i \\in A$ and $j \\in B$ (or vice versa), and $w_{ij} = 0$ otherwise. Thus only the edges across $A$ and $B$ are weighted, and there are $9$ such edges.\n\n- The SDP vectors are unit vectors in $\\mathbb{R}^2$ represented by their angles (in radians) on the unit circle. For $A$,\n$$\n\\angle(v_1) = -0.01,\\quad \\angle(v_2) = 0,\\quad \\angle(v_3) = 0.01,\n$$\nand for $B$,\n$$\n\\angle(v_4) = 0.19,\\quad \\angle(v_5) = 0.20,\\quad \\angle(v_6) = 0.21.\n$$\nAll angles are in radians.\n\n- Consider the following two rounding schemes:\n  1) Standard random hyperplane rounding as described above.\n  2) An adaptive hyperplane rounding that chooses a deterministic normal\n  $$\n  u \\propto \\left(\\sum_{i \\in A} v_i\\right) - \\left(\\sum_{j \\in B} v_j\\right),\n  $$\n  and then assigns $x_i = \\operatorname{sign}(u^{\\top} v_i)$ to produce a cut. The proportionality constant is irrelevant, as only the direction of $u$ matters.\n\nTasks:\n- Using only the foundational geometry of random hyperplanes and unit vectors on the circle, compute the expected cut value under the standard random hyperplane rounding for this instance.\n- Compute the cut value achieved by the adaptive hyperplane rounding for this instance.\n- Let $r$ denote the ratio of the adaptive cut value to the expected cut value from random hyperplane rounding. Compute $r$ as an exact, simplified expression. No rounding is required, and no units are involved. Your final answer must be a single analytic expression for $r$.", "solution": "The solution is derived in three parts as required.\n\n**Part 1: Expected Cut Value from Standard Random Hyperplane Rounding**\n\nThe value of a cut is $C = \\sum_{1 \\leq i < j \\leq 6} w_{ij} \\frac{1 - x_i x_j}{2}$. By linearity of expectation, the expected cut value $E_{rand}$ is:\n$$ E_{rand} = \\sum_{1 \\leq i < j \\leq 6} w_{ij} P(x_i \\neq x_j) $$\nwhere $P(x_i \\neq x_j)$ is the probability that vertices $i$ and $j$ are separated by the random hyperplane. For unit vectors, this probability is given by $\\frac{\\theta_{ij}}{\\pi}$, where $\\theta_{ij} = |\\angle(v_i) - \\angle(v_j)|$ is the angle between the vectors.\nThe weights $w_{ij}$ are 1 only for edges between group $A = \\{1,2,3\\}$ and group $B = \\{4,5,6\\}$. Thus, the sum is over these $3 \\times 3 = 9$ edges:\n$$ E_{rand} = \\frac{1}{\\pi} \\sum_{i \\in A, j \\in B} |\\angle(v_i) - \\angle(v_j)| $$\nThe angles are: $\\angle(v_A) = \\{-0.01, 0, 0.01\\}$ and $\\angle(v_B) = \\{0.19, 0.20, 0.21\\}$. For any pair $(i, j)$ with $i \\in A$ and $j \\in B$, $\\angle(v_j) > \\angle(v_i)$, so the absolute value is simply the difference.\n$$ \\sum_{i \\in A, j \\in B} (\\angle(v_j) - \\angle(v_i)) = \\sum_{j \\in B} \\sum_{i \\in A} (\\angle(v_j) - \\angle(v_i)) = \\sum_{j \\in B} (3\\angle(v_j) - \\sum_{i \\in A} \\angle(v_i)) $$\nThe sums of the angles are:\n$$ \\sum_{i \\in A} \\angle(v_i) = -0.01 + 0 + 0.01 = 0 $$\n$$ \\sum_{j \\in B} \\angle(v_j) = 0.19 + 0.20 + 0.21 = 0.60 $$\nThe total sum of angles is $3(0.60) - 3(0) = 1.8$.\nThe expected cut value is:\n$$ E_{rand} = \\frac{1.8}{\\pi} $$\n\n**Part 2: Cut Value from Adaptive Hyperplane Rounding**\n\nThe adaptive rounding scheme uses a deterministic hyperplane normal vector $u \\propto V_A - V_B$, where $V_A = \\sum_{i \\in A} v_i$ and $V_B = \\sum_{j \\in B} v_j$.\nThe vectors for group $A$ are clustered around angle 0. By symmetry, their sum $V_A$ points along the direction of angle 0.\nThe vectors for group $B$ are clustered around angle 0.20. By symmetry, their sum $V_B$ points along the direction of angle 0.20.\nThe normal vector $u = V_A - V_B$ has an angle that bisects the angle between $-V_B$ (angle $0.20+\\pi$) and $V_A$ (angle $0$). The separating hyperplane is orthogonal to $u$. By symmetry, this hyperplane passes through the origin at an angle that bisects the angles of $V_A$ and $V_B$.\nThe separating line is at angle $\\theta_{sep} = \\frac{0 + 0.20}{2} = 0.10$ radians.\nThe cut assigns $x_i = \\operatorname{sign}(\\langle v_i, u \\rangle)$. This is equivalent to partitioning vertices based on which side of the separating line they fall.\n-   The angles for group A, $\\{-0.01, 0, 0.01\\}$, are all less than $0.10$.\n-   The angles for group B, $\\{0.19, 0.20, 0.21\\}$, are all greater than $0.10$.\nThus, the adaptive rounding perfectly separates group A from group B.\nThe value of this cut, $C_{adapt}$, is the sum of weights of edges between $A$ and $B$. Since there are 9 such edges, each with weight 1:\n$$ C_{adapt} = 9 $$\n\n**Part 3: Ratio of Cut Values**\n\nThe ratio $r$ is:\n$$ r = \\frac{C_{adapt}}{E_{rand}} = \\frac{9}{1.8 / \\pi} = \\frac{9 \\pi}{1.8} = 5\\pi $$\nThe final expression is $5\\pi$.", "answer": "$$ \\boxed{5\\pi} $$", "id": "3177821"}, {"introduction": "While analytical solutions are invaluable for understanding core principles, many real-world problems require computational approaches. This final practice [@problem_id:3177869] bridges the gap between theory and application by tasking you with a computational experiment. You will implement a numerical solver for the SDP relaxation and apply it to graphs with a known community structure, empirically measuring the algorithm's ability to recover this underlying structure. This exercise demonstrates the practical power of SDP relaxations in fields like data science and machine learning.", "problem": "You are tasked to design and implement a self-contained computational experiment to examine how Semidefinite Programming (SDP) relaxations behave on a combinatorial partitioning problem with a block-structured weight matrix. The experiment must be reproducible, fully specified, and must report quantitative outcomes for a small test suite.\n\nLet there be a weighted, undirected graph with $n$ vertices, represented by a symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with zero diagonal. Consider a ground-truth binary partition encoded by a label vector $s \\in \\{-1, +1\\}^n$, where $s_i = +1$ indicates membership in block $A$ and $s_i = -1$ indicates membership in block $B$. The graph weights are generated from a signed block model:\n- For pairs $(i, j)$ within the same block, the weight $W_{ij}$ is drawn independently from a normal distribution with mean $-\\alpha$ and standard deviation $\\sigma$.\n- For pairs $(i, j)$ in different blocks, the weight $W_{ij}$ is drawn independently from a normal distribution with mean $+\\beta$ and standard deviation $\\sigma$.\n- The diagonal entries satisfy $W_{ii} = 0$, and the matrix is made symmetric by setting $W_{ji} = W_{ij}$ for $i < j$.\n\nThe combinatorial objective we consider uses the $\\{-1, +1\\}$ encoding for a cut: Given a partition $s \\in \\{-1, +1\\}^n$, define the signed cut objective\n$$\nJ(s) \\triangleq \\frac{1}{4} \\sum_{i=1}^n \\sum_{j=1}^n W_{ij} \\left(1 - s_i s_j\\right).\n$$\nThis favors separating vertices connected by positive weights and aligning vertices connected by negative weights. Directly optimizing over $s \\in \\{-1, +1\\}^n$ is combinatorial. A Semidefinite Programming (SDP) relaxation replaces the rank-one matrix $s s^\\top$ with a positive semidefinite matrix $X \\in \\mathbb{S}_+^n$ subject to $\\operatorname{diag}(X) = \\mathbf{1}$, and optimizes a linear functional in $X$. In this experiment, use a low-rank factorization $X = Y Y^\\top$ with $Y \\in \\mathbb{R}^{n \\times r}$ and enforce the constraint that each row of $Y$ has unit Euclidean norm. Optimize the linear functional consistent with the relaxation by performing projected gradient descent on $Y$ for a fixed number of steps, using a step size scaled by the spectral norm of $W$. Concretely:\n- Initialize $Y$ with independent, standard normal entries and project each row to have unit norm.\n- At each iteration, update $Y$ by a gradient step on the factorized objective followed by projection of each row back to unit norm.\n- Use a step size of the form $\\eta = c / (\\lambda_{\\max} + 10^{-8})$, where $\\lambda_{\\max}$ is the largest absolute eigenvalue of $W$, and $c$ is a dimensionless coefficient specified per test case.\n\nAfter optimization, obtain a randomized rounding of the SDP solution using a random hyperplane: draw a random vector $g \\in \\mathbb{R}^r$ with independent, standard normal entries and set\n$$\n\\hat{s}_i \\triangleq \\operatorname{sign}\\left(\\langle y_i, g \\rangle\\right),\n$$\nwhere $y_i$ denotes the $i$-th row of $Y$ and $\\operatorname{sign}(0)$ is defined to be $+1$.\n\nMeasure and report:\n1. An alignment score quantifying how much the optimized vectors align with the ground-truth blocks:\n   - Compute the matrix $S \\triangleq Y Y^\\top$.\n   - Compute the mean of $S_{ij}$ over all pairs $(i, j)$ with $i \\neq j$ that satisfy $s_i = s_j$ (same block), and the mean of $S_{ij}$ over all pairs with $s_i \\neq s_j$ (different blocks).\n   - Define the alignment score as the difference between these two means (same-block mean minus different-block mean).\n2. The rounding classification accuracy as a decimal in $[0,1]$:\n   - Compute the fraction of indices $i$ for which $\\hat{s}_i = s_i$ and the fraction for which $-\\hat{s}_i = s_i$; report the maximum of these two fractions.\n3. The ratio of cut objective values $J(\\hat{s}) / J(s)$, using the same $J(\\cdot)$ defined above.\n\nImplement the above using the following test suite of parameter sets, each specifying $(n, \\text{block sizes}, \\alpha, \\beta, \\sigma, r, T, c)$:\n- Test case $1$ (happy path): $(n = 40, \\text{block sizes} = (20, 20), \\alpha = 1.0, \\beta = 1.0, \\sigma = 0.2, r = 5, T = 600, c = 0.9)$.\n- Test case $2$ (low-noise boundary): $(n = 30, \\text{block sizes} = (15, 15), \\alpha = 1.5, \\beta = 1.0, \\sigma = 0.05, r = 5, T = 400, c = 0.9)$.\n- Test case $3$ (high-noise edge): $(n = 30, \\text{block sizes} = (15, 15), \\alpha = 0.8, \\beta = 1.0, \\sigma = 0.7, r = 5, T = 800, c = 0.7)$.\n\nYour program must:\n- Construct $W$ for each test case as specified.\n- Run the projected gradient factorization-based SDP relaxation for $T$ iterations with step size $\\eta = c / (\\lambda_{\\max} + 10^{-8})$, where $\\lambda_{\\max}$ is the largest absolute eigenvalue of $W$.\n- Compute the alignment score, rounding classification accuracy, and the ratio $J(\\hat{s}) / J(s)$.\n- Produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case’s result is a sublist of three floats, ordered as $[\\text{alignment score}, \\text{accuracy}, \\text{ratio}]$. For example: $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$.\n\nNo physical units or angle units are involved. All reported quantities must be pure numerical values. Percentages must be expressed as decimals in $[0,1]$.", "solution": "The problem statement is assessed to be **valid**. It presents a self-contained, scientifically grounded, and well-posed computational experiment in the domain of optimization and combinatorial problems. The problem provides a complete specification of the model, the algorithm, the parameters for test cases, and the required output metrics. The methodology is based on established techniques, namely the signed block model for graph generation, semidefinite programming (SDP) relaxation for a MAX-CUT-like problem, the Burer-Monteiro factorization approach for solving the SDP using projected gradient descent, and randomized hyperplane rounding. The task is to implement this computational experiment and report the results for a given set of test cases.\n\nThe core task is to find a binary partition of a graph, represented by a vector $s \\in \\{-1, +1\\}^n$, that maximizes the signed cut objective:\n$$\nJ(s) = \\frac{1}{4} \\sum_{i=1}^n \\sum_{j=1}^n W_{ij} (1 - s_i s_j)\n$$\nMaximizing $J(s)$ is equivalent to minimizing the quadratic form $s^\\top W s = \\sum_{i,j} W_{ij} s_i s_j$, since $\\sum_{i,j} W_{ij}$ is a constant. We can write $s^\\top W s = \\operatorname{Tr}(W s s^\\top)$. The combinatorial constraint is $s_i \\in \\{-1, +1\\}$, which is equivalent to $s_i^2 = 1$. The SDP relaxation lifts this problem by replacing the rank-$1$ matrix $X = ss^\\top$ with a general positive semidefinite matrix $X \\in \\mathbb{S}_+^n$. The constraint $s_i^2 = 1$ becomes $\\operatorname{diag}(X) = \\mathbf{1}$. The relaxed optimization problem is:\n$$\n\\min_{X \\in \\mathbb{S}_+^n} \\operatorname{Tr}(WX) \\quad \\text{subject to} \\quad \\operatorname{diag}(X) = \\mathbf{1}\n$$\nThe problem specifies solving this using a low-rank factorization approach. We set $X = YY^\\top$ where $Y \\in \\mathbb{R}^{n \\times r}$. The constraint $\\operatorname{diag}(X)=\\mathbf{1}$ translates to requiring each row $y_i$ of $Y$ to be a unit vector, i.e., $\\|y_i\\|_2 = 1$. The objective becomes minimizing $F(Y) = \\operatorname{Tr}(WYY^\\top)$ over the manifold of such matrices $Y$. This non-convex problem is solved using projected gradient descent.\n\nThe methodology for the computational experiment is as follows:\n\n**1. Data Generation**\nFor each test case with parameters $(n, \\text{block sizes}, \\alpha, \\beta, \\sigma, r, T, c)$:\n- A ground-truth partition vector $s \\in \\{-1, +1\\}^n$ is constructed based on the specified block sizes. For example, for sizes $(k, n-k)$, $s_i = +1$ for $i \\in \\{1, \\dots, k\\}$ and $s_i = -1$ for $i \\in \\{k+1, \\dots, n\\}$.\n- A symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with a zero diagonal is generated. For each pair of indices $(i, j)$ with $i < j$:\n    - If $s_i = s_j$ (intra-block), $W_{ij}$ is drawn from a normal distribution $\\mathcal{N}(-\\alpha, \\sigma^2)$.\n    - If $s_i \\neq s_j$ (inter-block), $W_{ij}$ is drawn from $\\mathcal{N}(+\\beta, \\sigma^2)$.\n- Symmetry is enforced by setting $W_{ji} = W_{ij}$. To ensure reproducibility of the experiment, all random number generation processes are seeded with a fixed value.\n\n**2. Optimization via Projected Gradient Descent**\nThe objective function to be minimized is $F(Y) = \\operatorname{Tr}(WYY^\\top)$.\n- **Initialization**: An initial matrix $Y_0 \\in \\mathbb{R}^{n \\times r}$ is generated with entries drawn from the standard normal distribution $\\mathcal{N}(0, 1)$. Its rows are then normalized to have unit Euclidean norm.\n- **Gradient Calculation**: The gradient of $F(Y)$ with respect to $Y$ is $\\nabla_Y F(Y) = 2WY$.\n- **Step Size**: The step size $\\eta$ is set to $\\eta = c / (\\lambda_{\\max} + 10^{-8})$, where $\\lambda_{\\max} = \\max_i |\\lambda_i(W)|$ is the largest absolute eigenvalue (spectral norm) of the symmetric matrix $W$.\n- **Iteration**: For $k=0, 1, \\dots, T-1$, the following update is performed:\n    1.  **Gradient Step**: A temporary matrix $Y'$ is computed by taking a step in the negative gradient direction: $Y' = Y_k - \\eta (2 W Y_k)$.\n    2.  **Projection**: Each row $y'_i$ of $Y'$ is projected back onto the unit sphere: $y_{i, k+1} = y'_i / \\|y'_i\\|_2$. The resulting matrix is $Y_{k+1}$.\n\n**3. Randomized Rounding**\nAfter $T$ iterations, the final matrix $Y$ provides an approximate solution to the SDP. A discrete partition $\\hat{s} \\in \\{-1, +1\\}^n$ is recovered using randomized hyperplane rounding:\n- A random vector $g \\in \\mathbb{R}^r$ is drawn, with each component sampled independently from $\\mathcal{N}(0, 1)$.\n- The estimated partition is computed as $\\hat{s}_i = \\operatorname{sign}(\\langle y_i, g \\rangle)$, where $y_i$ is the $i$-th row of $Y$. The convention $\\operatorname{sign}(0) = +1$ is used.\n\n**4. Performance Evaluation**\nThree quantitative metrics are computed to evaluate the outcome:\n- **Alignment Score**: This measures how well the geometry of the solution vectors $\\{y_i\\}$ reflects the ground-truth partition. Let $S = YY^\\top$. The score is the difference between the average inner product of vectors in the same block and the average inner product of vectors in different blocks:\n  $$ \\text{score} = \\frac{\\sum_{i<j, s_i=s_j} S_{ij}}{|\\{(i,j) \\mid i<j, s_i=s_j\\}|} - \\frac{\\sum_{i<j, s_i \\neq s_j} S_{ij}}{|\\{(i,j) \\mid i<j, s_i \\neq s_j\\}|} $$\n- **Rounding Classification Accuracy**: Since the labels $\\{+1, -1\\}$ are arbitrary, the accuracy of $\\hat{s}$ is measured against both $s$ and $-s$, and the better of the two is reported:\n  $$ \\text{accuracy} = \\max\\left( \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}(\\hat{s}_i = s_i), \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}(\\hat{s}_i = -s_i) \\right) $$\n  where $\\mathbb{I}(\\cdot)$ is the indicator function.\n- **Ratio of Cut Objectives**: This compares the quality of the recovered partition $\\hat{s}$ to the ground-truth partition $s$ in terms of the original objective function $J(\\cdot)$. The ratio $J(\\hat{s}) / J(s)$ is computed. Based on the problem formulation, $J(s)$ is expected to be positive.\n\nThe implementation will follow these steps precisely for each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... # Scipy is not required for this implementation.\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment for all test cases.\n    \"\"\"\n    \n    # Test cases defined as (n, block_sizes, alpha, beta, sigma, r, T, c)\n    test_cases = [\n        # Test case 1 (happy path)\n        (40, (20, 20), 1.0, 1.0, 0.2, 5, 600, 0.9),\n        # Test case 2 (low-noise boundary)\n        (30, (15, 15), 1.5, 1.0, 0.05, 5, 400, 0.9),\n        # Test case 3 (high-noise edge)\n        (30, (15, 15), 0.8, 1.0, 0.7, 5, 800, 0.7),\n    ]\n\n    # Use a fixed seed for reproducibility of the entire experiment.\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n    for params in test_cases:\n        results.append(run_experiment(params, rng))\n\n    # Format the final output string as specified.\n    # The str() representation of a list is '[item1, item2, ...]'\n    # Joining these with ',' and enclosing in '[]' gives '[[...],[...]]'\n    final_output = f\"[{','.join(map(str, results))}]\"\n    print(final_output)\n\ndef run_experiment(params, rng):\n    \"\"\"\n    Executes a single instance of the experiment for a given parameter set.\n    \"\"\"\n    n, block_sizes, alpha, beta, sigma, r, T, c = params\n    \n    # 1. Generate Ground Truth and Weight Matrix\n    s = np.ones(n, dtype=int)\n    s[block_sizes[0]:] = -1\n    \n    W = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            if s[i] == s[j]: # Same block\n                weight = rng.normal(loc=-alpha, scale=sigma)\n            else: # Different blocks\n                weight = rng.normal(loc=beta, scale=sigma)\n            W[i, j] = weight\n            W[j, i] = weight\n            \n    # 2. Optimization via Projected Gradient Descent\n    # Calculate step size\n    # np.linalg.eigvalsh is used for real symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(W)\n    lambda_max_abs = np.max(np.abs(eigenvalues))\n    eta = c / (lambda_max_abs + 1e-8)\n    \n    # Initialize Y\n    Y = rng.normal(loc=0.0, scale=1.0, size=(n, r))\n    # Project rows to unit norm\n    row_norms = np.linalg.norm(Y, axis=1, keepdims=True)\n    Y /= row_norms\n    \n    # Run gradient descent\n    for _ in range(T):\n        # Gradient step\n        grad_Y = 2 * (W @ Y)\n        Y_temp = Y - eta * grad_Y\n        \n        # Projection step\n        row_norms = np.linalg.norm(Y_temp, axis=1, keepdims=True)\n        # Avoid division by zero for potential zero-norm rows\n        row_norms[row_norms == 0] = 1.0\n        Y = Y_temp / row_norms\n        \n    # 3. Randomized Rounding\n    g = rng.normal(loc=0.0, scale=1.0, size=r)\n    s_hat = np.sign(Y @ g)\n    s_hat[s_hat == 0] = 1 # As per problem spec: sign(0) = +1\n    \n    # 4. Performance Evaluation\n    # Metric 1: Alignment Score\n    S = Y @ Y.T\n    s_outer = np.outer(s, s)\n    off_diagonal_mask = ~np.eye(n, dtype=bool)\n    \n    same_block_mask = (s_outer == 1) & off_diagonal_mask\n    diff_block_mask = (s_outer == -1) & off_diagonal_mask\n    \n    mean_same = np.mean(S[same_block_mask]) if np.any(same_block_mask) else 0.0\n    mean_diff = np.mean(S[diff_block_mask]) if np.any(diff_block_mask) else 0.0\n    \n    alignment_score = mean_same - mean_diff\n    \n    # Metric 2: Rounding Classification Accuracy\n    acc1 = np.mean(s_hat == s)\n    acc2 = np.mean(-s_hat == s)\n    accuracy = max(acc1, acc2)\n    \n    # Metric 3: Ratio of Cut Objectives\n    def compute_J(W_mat, s_vec):\n        s_outer_mat = np.outer(s_vec, s_vec)\n        return 0.25 * np.sum(W_mat * (1 - s_outer_mat))\n    \n    J_s = compute_J(W, s)\n    J_s_hat = compute_J(W, s_hat)\n    \n    # Handle case where J(s) might be zero or negative, although unlikely.\n    ratio = J_s_hat / J_s if J_s != 0 else 0.0\n\n    # Ensure results are standard Python floats for consistent printing\n    return [float(alignment_score), float(accuracy), float(ratio)]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3177869"}]}