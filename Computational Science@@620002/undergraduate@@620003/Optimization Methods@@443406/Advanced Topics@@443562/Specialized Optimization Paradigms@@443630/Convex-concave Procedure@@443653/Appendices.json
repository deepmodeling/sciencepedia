{"hands_on_practices": [{"introduction": "The best way to grasp the Convex-Concave Procedure is to see it in action on a simple, visualizable problem. This first practice invites you to minimize a one-dimensional function, $f(x) = \\sin(x) + \\alpha x^2$, which is non-convex and has many local minima. By implementing a CCP algorithm for this function [@problem_id:3114687], you will perform a DC decomposition using the standard curvature-shifting technique and derive the explicit iterative update, giving you a foundational understanding of how CCP navigates a complex energy landscape.", "problem": "You are asked to implement the convex-concave procedure (CCP) for a univariate difference-of-convex (DC) minimization of the toy objective $f(x) = \\sin(x) + \\alpha x^2$, where $\\alpha \\ge 0$ is a parameter. The assignment has three parts: construct a valid DC splitting, derive the CCP update, and run the algorithm from different initializations to study convergence behavior in relation to the initial curvature. All angles used in trigonometric functions must be in radians.\n\nStart from the following foundational base:\n- The second derivative test: a twice-differentiable function $q(x)$ is convex if and only if $q''(x) \\ge 0$ for all $x$ in its domain.\n- The gradient of $\\sin(x)$ is $\\cos(x)$, and the second derivative is $-\\sin(x)$.\n- If a differentiable function $r(x)$ has a gradient that is $L$-Lipschitz continuous, i.e., $\\|\\nabla r(x) - \\nabla r(y)\\| \\le L \\|x-y\\|$ for all $x,y$, then the function $x \\mapsto \\frac{L}{2}x^2 - r(x)$ is convex, because its second derivative is bounded below by $0$.\n\nTasks to be completed by your program:\n1) Construct a DC splitting $f(x) = g(x) - h(x)$ with $g(x)$ convex and $h(x)$ convex. Use only the above foundational facts to justify your construction.\n2) Using only the definition of CCP for minimizing a DC function—namely, at iteration $k$, minimize the convex surrogate obtained by linearizing the concave part—derive the explicit iterative update for the univariate case and implement it. Your implementation must be numerically stable with a stopping criterion based on a tolerance in the decision variable, and must cap the number of iterations by a maximum iteration count to avoid non-termination.\n3) For each test case described below, run CCP and output a summary that relates the observed convergence to the initial curvature of $f$ at the starting point. Specifically, for each case, compute:\n   - The computed stationary point $x^\\star$ found by CCP.\n   - The objective value $f(x^\\star)$.\n   - A boolean indicating whether the second derivative $f''(x^\\star) > 0$ (interpreted here as a local minimum certificate).\n   - A boolean indicating whether the algorithm converged under the stopping criterion.\n   - The initial curvature $f''(x_0)$ at the starting point $x_0$.\n   - A boolean indicating whether the initial curvature $f''(x_0) > 0$.\n   - The number of iterations used.\n\nDefinition of quantities your program must use:\n- The function is $f(x) = \\sin(x) + \\alpha x^2$.\n- The initial curvature is $f''(x_0) = -\\sin(x_0) + 2\\alpha$.\n- The local minimum certificate is the boolean value of $f''(x^\\star) > 0$.\n- The CCP iterate is obtained by solving the convex subproblem produced by the linearization of the concave part at each iteration.\n\nAngle unit requirement:\n- All trigonometric computations must use radians.\n\nTest suite:\nUse the following test cases, each specified by $(\\alpha, x_0)$, with the curvature-related quantities and CCP run computed as described.\n- Case $1$: $\\alpha = 0.1$, $x_0 = 0.1$.\n- Case $2$: $\\alpha = 0.1$, $x_0 = 2.5$.\n- Case $3$: $\\alpha = 0.6$, $x_0 = -10.0$.\n- Case $4$: $\\alpha = 0.3$, $x_0 = 4.0$.\n- Case $5$: $\\alpha = 0.5$, $x_0 = \\frac{\\pi}{2}$.\n\nAlgorithmic details to implement:\n- Use the smallest valid curvature-shifting constant $L$ that makes your DC splitting valid for all $x \\in \\mathbb{R}$.\n- Use a stopping tolerance $\\varepsilon = 10^{-10}$ on the absolute change in $x$ between iterations.\n- Use a maximum iteration cap of $10000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order listed above. Each element must itself be a list containing the $7$ quantities in the order: $x^\\star$, $f(x^\\star)$, local minimum certificate, convergence boolean, $f''(x_0)$, initial curvature positive boolean, number of iterations. For example, the overall output must have the form\n$[ [x^\\star_1,f(x^\\star_1),\\text{min}_1,\\text{conv}_1,f''(x_0^{(1)}),\\text{icurvpos}_1,\\text{iters}_1], \\ldots, [x^\\star_5,f(x^\\star_5),\\text{min}_5,\\text{conv}_5,f''(x_0^{(5)}),\\text{icurvpos}_5,\\text{iters}_5] ]$.", "solution": "The user-provided problem is valid as it is scientifically grounded in the principles of optimization theory, well-posed with a clear objective and constraints, and free of any contradictions or ambiguities. We will proceed with a full solution.\n\nThe problem asks for the implementation and analysis of the Convex-Concave Procedure (CCP) to minimize the univariate objective function $f(x) = \\sin(x) + \\alpha x^2$ for a given parameter $\\alpha \\ge 0$. The process involves three main tasks: constructing a valid Difference-of-Convex (DC) decomposition, deriving the CCP iterative update, and implementing the algorithm to study its convergence behavior on specified test cases.\n\n### Part 1: Construction of a Valid DC Splitting\n\nThe objective function is $f(x) = \\sin(x) + \\alpha x^2$. A DC splitting represents $f(x)$ as a difference of two convex functions, $f(x) = g(x) - h(x)$.\n\nThe term $\\alpha x^2$ is convex for $\\alpha \\ge 0$, as its second derivative is $2\\alpha \\ge 0$. The term $\\sin(x)$ is not convex, as its second derivative, $-\\sin(x)$, can be negative. We must decompose the function to handle this non-convex part.\n\nA standard method for creating a DC decomposition is to add and subtract a sufficiently large quadratic term, $\\frac{L}{2}x^2$, to make the components convex. We can decompose $f(x)$ as:\n$$f(x) = \\left( \\alpha x^2 + \\frac{L}{2}x^2 \\right) - \\left( \\frac{L}{2}x^2 - \\sin(x) \\right)$$\nLet us define:\n$g(x) = (\\alpha + \\frac{L}{2})x^2$\n$h(x) = \\frac{L}{2}x^2 - \\sin(x)$\n\nFor this to be a valid DC splitting, both $g(x)$ and $h(x)$ must be convex.\n\n1.  **Convexity of $g(x)$:**\n    The second derivative of $g(x)$ is $g''(x) = 2(\\alpha + \\frac{L}{2}) = 2\\alpha + L$. For $g(x)$ to be convex, we need $g''(x) \\ge 0$. Since $\\alpha \\ge 0$ and we will choose a non-negative constant $L$, this condition will be satisfied.\n\n2.  **Convexity of $h(x)$:**\n    The second derivative of $h(x)$ is $h''(x) = L + \\sin(x)$. According to the second derivative test, $h(x)$ is convex if and only if $h''(x) \\ge 0$ for all $x \\in \\mathbb{R}$. This implies we must have $L + \\sin(x) \\ge 0$ for all $x$. Since the minimum value of $\\sin(x)$ is $-1$, the condition becomes $L - 1 \\ge 0$, which means $L \\ge 1$.\n\nThe problem requires using the smallest valid constant $L$. Therefore, we choose $L = 1$.\n\nWith $L=1$:\n- $g(x) = (\\alpha + \\frac{1}{2})x^2$ is convex, since its second derivative $2\\alpha + 1 \\ge 1 > 0$ for $\\alpha \\ge 0$.\n- $h(x) = \\frac{1}{2}x^2 - \\sin(x)$ is convex, since its second derivative $1 + \\sin(x) \\ge 1 - 1 = 0$ for all $x$.\n\nThus, a valid DC splitting is $f(x) = g(x) - h(x)$ with the functions defined above and $L=1$.\n\n### Part 2: Derivation of the CCP Update Rule\n\nThe CCP algorithm solves the non-convex problem $\\min_x \\{g(x) - h(x)\\}$ by iteratively solving a sequence of convex subproblems. At each iteration $k$, given the current iterate $x_k$, the concave part of the objective, $-h(x)$, is replaced by its first-order Taylor approximation around $x_k$. This yields a convex surrogate function to be minimized.\n\nThe linearization of $h(x)$ at $x_k$ is $h(x_k) + \\nabla h(x_k)(x - x_k)$.\nThe CCP subproblem at iteration $k+1$ is:\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - \\left[ h(x_k) + \\nabla h(x_k)(x - x_k) \\right] \\right\\}$$\nThe terms $h(x_k)$ and $-\\nabla h(x_k)(-x_k)$ are constant with respect to the optimization variable $x$, so we can simplify the problem to:\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - x \\nabla h(x_k) \\right\\}$$\nWe need the gradient of $h(x)$:\n$\\nabla h(x) = \\frac{d}{dx} \\left(\\frac{1}{2}x^2 - \\sin(x)\\right) = x - \\cos(x)$.\nSo, $\\nabla h(x_k) = x_k - \\cos(x_k)$.\n\nThe objective for the subproblem is:\n$$J(x) = g(x) - x \\nabla h(x_k) = \\left(\\alpha + \\frac{1}{2}\\right)x^2 - (x_k - \\cos(x_k))x$$\nThis is a convex quadratic function of $x$. To find its minimizer, we set its derivative with respect to $x$ to zero:\n$$\\nabla J(x) = 2\\left(\\alpha + \\frac{1}{2}\\right)x - (x_k - \\cos(x_k)) = 0$$\n$$(2\\alpha + 1)x = x_k - \\cos(x_k)$$\nSolving for $x$ gives the explicit iterative update rule for $x_{k+1}$:\n$$x_{k+1} = \\frac{x_k - \\cos(x_k)}{2\\alpha + 1}$$\nSince $\\alpha \\ge 0$, the denominator $2\\alpha+1$ is always greater than or equal to $1$, so the update is well-defined.\n\n### Part 3: Implementation and Analysis\n\nThe derived update rule is implemented as an iterative algorithm. Starting from an initial point $x_0$, the sequence $\\{x_k\\}$ is generated until the absolute difference between successive iterates, $|x_{k+1} - x_k|$, falls below a specified tolerance $\\varepsilon = 10^{-10}$, or a maximum of $10000$ iterations is reached.\n\nFor each test case defined by a pair $(\\alpha, x_0)$, the algorithm is run to find a stationary point $x^\\star$. A stationary point of the CCP iteration is a fixed point, which must satisfy $x = \\frac{x - \\cos(x)}{2\\alpha+1}$. This simplifies to $(2\\alpha+1)x = x-\\cos(x)$, or $2\\alpha x + \\cos(x) = 0$. This condition is identical to setting the gradient of the original function, $f'(x) = 2\\alpha x + \\cos(x)$, to zero. Thus, the CCP algorithm correctly converges to the stationary points of $f(x)$.\n\nAfter convergence, the following quantities are calculated and reported for each test case:\n- The computed stationary point, $x^\\star$.\n- The objective value at the stationary point, $f(x^\\star) = \\sin(x^\\star) + \\alpha(x^\\star)^2$.\n- A boolean certificate for being a local minimum, evaluated as $f''(x^\\star) > 0$. The second derivative is $f''(x) = -\\sin(x) + 2\\alpha$.\n- A boolean flag indicating if the algorithm converged within the maximum number of iterations.\n- The initial curvature at the starting point, $f''(x_0) = -\\sin(x_0) + 2\\alpha$.\n- A boolean flag indicating if the initial curvature at $x_0$ is positive.\n- The number of iterations performed.\n\nThe implementation will apply this procedure to all specified test cases and format the results as requested.", "answer": "```python\nimport numpy as np\n\ndef run_ccp(alpha, x0, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Implements the Convex-Concave Procedure (CCP) for the given objective.\n    \n    Args:\n        alpha (float): The parameter for the objective function.\n        x0 (float): The initial starting point.\n        tol (float): The tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n        \n    Returns:\n        list: A list containing the 7 required output quantities.\n    \"\"\"\n    x_k = x0\n    converged = False\n    iters = 0\n\n    for i in range(max_iter):\n        x_k_plus_1 = (x_k - np.cos(x_k)) / (2 * alpha + 1)\n        iters = i + 1\n\n        if np.abs(x_k_plus_1 - x_k) < tol:\n            converged = True\n            x_k = x_k_plus_1\n            break\n        \n        x_k = x_k_plus_1\n    \n    # In case of non-convergence, x_k is the last computed value.\n    if not converged:\n        iters = max_iter\n\n    x_star = x_k\n\n    # Calculate required metrics\n    f_x_star = np.sin(x_star) + alpha * x_star**2\n    f_double_prime_x_star = -np.sin(x_star) + 2 * alpha\n    is_local_min = bool(f_double_prime_x_star > 0)\n\n    f_double_prime_x0 = -np.sin(x0) + 2 * alpha\n    is_initial_curvature_pos = bool(f_double_prime_x0 > 0)\n\n    return [\n        x_star,\n        f_x_star,\n        is_local_min,\n        converged,\n        f_double_prime_x0,\n        is_initial_curvature_pos,\n        iters\n    ]\n\ndef solve():\n    \"\"\"\n    Runs the CCP solver for all test cases and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        (0.1, 0.1),\n        (0.1, 2.5),\n        (0.6, -10.0),\n        (0.3, 4.0),\n        (0.5, np.pi / 2.0)\n    ]\n\n    results = []\n    for alpha, x0 in test_cases:\n        result = run_ccp(alpha, x0)\n        # Ensure booleans are lowercase 'true'/'false' for perfect format matching if needed,\n        # but Python's default `str(bool)` is 'True'/'False' and is acceptable.\n        # Example: list_str = f\"[{','.join(str(item).lower() if isinstance(item, bool) else str(item) for item in result)}]\"\n        results.append(result)\n\n    # Format the final output string according to the strict specification.\n    # The format requires a string representation of a list of lists with no spaces.\n    formatted_results = []\n    for case_result in results:\n        # Convert each item in the sublist to a string\n        str_items = [str(item) for item in case_result]\n        # Join them with commas\n        list_str = ','.join(str_items)\n        # Enclose in brackets\n        formatted_results.append(f\"[{list_str}]\")\n    \n    # Join the formatted sublists with commas\n    main_content = ','.join(formatted_results)\n    \n    # Enclose the entire string in brackets\n    final_output = f\"[{main_content}]\"\n    \n    print(final_output)\n\n# Run the solver\nsolve()\n\n```", "id": "3114687"}, {"introduction": "Now that you have a grasp of the basics, let's apply CCP to a practical problem in machine learning: weight quantization. This exercise [@problem_id:3114735] tackles the challenge of encouraging a model's parameters to 'snap' to discrete values like $\\{-1, 1\\}$, a key step in building computationally efficient models. You will discover the power of algebraic insight by reformulating a complex piecewise penalty into a clean difference-of-convex structure, transforming a difficult problem into a series of solvable linear systems.", "problem": "Consider the following penalized least-squares problem in the context of optimization methods. You are given a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda \\in \\mathbb{R}_{+}$. Define the objective function\n$$\nF(w) \\triangleq \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\min\\{(w_i - 1)^2,\\,(w_i + 1)^2\\},\n$$\nwhere $w \\in \\mathbb{R}^{n}$ is the decision variable. The scalar regularization encourages each coordinate $w_i$ to align with the quantization set $\\{-1,\\,1\\}$ by penalizing the squared distance to the closest of the two points $-1$ and $1$.\n\nYou must solve this problem using the Convex-Concave Procedure (CCP), which is a standard method for minimizing a difference-of-convex (DC) function. The fundamental bases required are: the definition of convexity and subgradients, the concept of DC decomposition (writing a nonconvex function as the difference of two convex functions), and the first-order optimality conditions for convex functions. Explicit formulas specialized to this problem are not provided and must be derived by you from first principles.\n\nTasks:\n- Starting from the definition of convexity and the absolute value function, derive a valid difference-of-convex decomposition of each term $\\min\\{(w_i - 1)^2,\\,(w_i + 1)^2\\}$ and use it to construct a DC decomposition of the full objective $F(w)$.\n- Using first-order optimality conditions and the subgradient of the convex component in the DC decomposition, derive the convex subproblem that is solved at each CCP iteration. Express the iteration update rule for $w$ in terms of the subgradient evaluated at the current iterate.\n- Implement the CCP algorithm that starts from a specified initial $w^{(0)}$, chooses a subgradient consistent with the absolute value function (assigning the subgradient $0$ at coordinates equal to $0$), and solves the convex subproblem exactly at each iteration. Use the stopping rule: terminate when the relative change $\\|w^{(t+1)} - w^{(t)}\\|_{2} / \\max\\{\\|w^{(t)}\\|_{2},\\,1\\}$ is strictly less than $10^{-6}$, or when the number of iterations reaches $100$, whichever occurs first.\n- Define that a coordinate $w_i$ is considered “snapped” to the quantization set if $| |w_i| - 1 | \\le 10^{-3}$.\n- For each test case, produce three outputs: the integer count of snapped coordinates, the final objective value $F(w)$ rounded to $6$ decimal places, and a boolean indicating whether the algorithm met the stopping criterion before reaching the iteration limit.\n\nYou must implement a complete, standalone program that performs this computation for the test suite specified below and produces a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of the form $[k,f,\\text{converged}]$ with $k$ an integer, $f$ a float rounded to $6$ decimals, and $\\text{converged}$ a boolean.\n\nTest Suite:\n- Case $1$ (happy path): $m=8$, $n=4$, \n  $A=\\begin{bmatrix}\n  0.5 & -0.2 & 0.1 & 0.0 \\\\\n  1.0 & 0.3 & -0.4 & 0.2 \\\\\n  -0.7 & 0.5 & 0.0 & -0.1 \\\\\n  0.0 & -0.6 & 0.8 & 0.3 \\\\\n  0.2 & 0.1 & -0.5 & -0.9 \\\\\n  0.3 & 0.7 & 0.9 & -0.4 \\\\\n  -0.4 & -0.2 & 0.6 & 0.5 \\\\\n  0.6 & -0.1 & -0.3 & 0.7\n  \\end{bmatrix}$,\n  $b=\\begin{bmatrix}0.1 \\\\ -0.5 \\\\ 0.3 \\\\ -0.2 \\\\ 0.8 \\\\ -0.9 \\\\ 0.0 \\\\ 0.4\\end{bmatrix}$, $\\lambda=1.0$, $w^{(0)}=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$.\n- Case $2$ (strong quantization pressure): same $A$ and $b$ as Case $1$, $\\lambda=100.0$, $w^{(0)}=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$.\n- Case $3$ (zero data term, immediate snapping behavior): $m=6$, $n=3$, $A$ is the $6 \\times 3$ zero matrix, $b=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$, $\\lambda=0.5$, $w^{(0)}=\\begin{bmatrix}0.2 \\\\ -0.3 \\\\ 0.8\\end{bmatrix}$.\n- Case $4$ (weak quantization pressure): same $A$ and $b$ as Case $1$, $\\lambda=0.01$, $w^{(0)}=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$.\n- Case $5$ (one-dimensional boundary case): $m=5$, $n=1$, $A=\\begin{bmatrix}1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.7 \\\\ -0.2\\end{bmatrix}$, $b=\\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.0 \\\\ 1.0 \\\\ -0.5\\end{bmatrix}$, $\\lambda=0.5$, $w^{(0)}=\\begin{bmatrix}0.0\\end{bmatrix}$.\n\nAngle units are not applicable. No physical units are involved. The final program output must be a single line string of the form\n$[\\,[k_1,f_1,\\text{converged}_1],\\,[k_2,f_2,\\text{converged}_2],\\,\\ldots\\,]$,\nwith no spaces inside the inner lists. The float $f_i$ must be rounded to exactly $6$ decimal places.", "solution": "The problem requires the minimization of a nonconvex objective function using the Convex-Concave Procedure (CCP). The derivation and implementation steps are outlined below.\n\n### 1. Difference-of-Convex (DC) Decomposition\n\nThe objective function is given by\n$$ F(w) \\triangleq \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\min\\{(w_i - 1)^2,\\,(w_i + 1)^2\\} $$\nTo apply CCP, we first need to express $F(w)$ as a difference of two convex functions, $F(w) = G(w) - H(w)$.\n\nLet's analyze the penalty term $P(w_i) = \\min\\{(w_i - 1)^2, (w_i + 1)^2\\}$. We determine which of the two squared terms is smaller by examining the sign of their difference:\n$$ (w_i - 1)^2 - (w_i + 1)^2 = (w_i^2 - 2w_i + 1) - (w_i^2 + 2w_i + 1) = -4w_i $$\n- If $w_i \\ge 0$, then $-4w_i \\le 0$, which implies $(w_i - 1)^2 \\le (w_i + 1)^2$.\n- If $w_i < 0$, then $-4w_i > 0$, which implies $(w_i - 1)^2 > (w_i + 1)^2$.\n\nTherefore, the penalty term can be written piecewise as:\n$$ P(w_i) = \\begin{cases} (w_i - 1)^2 & \\text{if } w_i \\ge 0 \\\\ (w_i + 1)^2 & \\text{if } w_i < 0 \\end{cases} $$\nThis piecewise function can be represented by a single expression using the absolute value function, $|w_i|$. Consider the expression $w_i^2 - 2|w_i| + 1$:\n- If $w_i \\ge 0$, it becomes $w_i^2 - 2w_i + 1 = (w_i - 1)^2$.\n- If $w_i < 0$, it becomes $w_i^2 - 2(-w_i) + 1 = w_i^2 + 2w_i + 1 = (w_i + 1)^2$.\nThis matches the piecewise definition of $P(w_i)$. Thus, we have the identity $P(w_i) = w_i^2 - 2|w_i| + 1$.\n\nSubstituting this back into the objective function $F(w)$:\n$$ F(w) = \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} (w_i^2 - 2|w_i| + 1) $$\nWe can rearrange and group terms to separate the convex and concave parts:\n$$ F(w) = \\left( \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} (w_i^2 + 1) \\right) - \\left( 2\\lambda \\sum_{i=1}^{n} |w_i| \\right) $$\nThis provides a DC decomposition $F(w) = G(w) - H(w)$, where:\n- $G(w) = \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2} + n\\lambda$.\n- $H(w) = 2\\lambda \\|w\\|_{1}$.\n\nThe function $G(w)$ is the sum of a convex quadratic term ($\\frac{1}{2}\\|A w - b\\|_{2}^{2}$), another convex quadratic term ($\\lambda \\|w\\|_{2}^{2}$), and a constant. The sum of convex functions is convex. The Hessian matrix of $G(w)$ is $\\nabla^2 G(w) = A^T A + 2\\lambda I$. Since $A^T A$ is positive semi-definite and, for $\\lambda > 0$, $2\\lambda I$ is positive definite, their sum is positive definite. Hence, $G(w)$ is strictly convex.\nThe function $H(w)$ is a positively scaled $\\ell_1$-norm of $w$, which is a standard convex function.\n\n### 2. CCP Iteration Update Rule\n\nThe CCP algorithm minimizes $F(w) = G(w) - H(w)$ by iteratively solving a convex majorization of the objective. At iteration $t+1$, the function $H(w)$ is replaced by its first-order Taylor approximation around the current iterate $w^{(t)}$. Since $H(w)$ is non-differentiable at points where some $w_i=0$, we use a subgradient $v^{(t)} \\in \\partial H(w^{(t)})$. The convex majorization of $F(w)$ at $w^{(t)}$ is:\n$$ \\hat{F}(w; w^{(t)}) = G(w) - \\left( H(w^{(t)}) + \\langle v^{(t)}, w - w^{(t)} \\rangle \\right) $$\nThe next iterate $w^{(t+1)}$ is found by minimizing this convex function:\n$$ w^{(t+1)} = \\arg\\min_w \\hat{F}(w; w^{(t)}) = \\arg\\min_w \\{ G(w) - \\langle v^{(t)}, w \\rangle \\} $$\nThe subgradient of $H(w) = 2\\lambda \\sum_{i=1}^n |w_i|$ is a vector $v$ with components $v_i \\in \\partial(2\\lambda|w_i|) = 2\\lambda \\cdot \\partial|w_i|$. The subdifferential of the absolute value function is $\\partial|x| = \\{\\text{sgn}(x)\\}$ if $x \\neq 0$ and the interval $[-1, 1]$ if $x=0$. As per the problem statement, we must choose the subgradient value of $0$ at coordinates equal to $0$, which corresponds to defining $\\text{sgn}(0)=0$. Thus, the specific subgradient vector $v^{(t)}$ has components $v_i^{(t)} = 2\\lambda \\cdot \\text{sgn}(w^{(t)}_i)$.\n\nThe minimization subproblem at each iteration is:\n$$ \\min_w \\left\\{ \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda\\|w\\|_2^2 - (v^{(t)})^T w \\right\\} $$\nThis is an unconstrained convex optimization problem. We find the minimum by setting the gradient of the objective with respect to $w$ to zero. Let the objective of the subproblem be $J(w)$.\n$$ J(w) = \\frac{1}{2}w^T A^T A w - b^T A w + \\frac{1}{2}b^T b + \\lambda w^T w - (v^{(t)})^T w $$\nThe gradient is:\n$$ \\nabla_w J(w) = A^T A w - A^T b + 2\\lambda w - v^{(t)} $$\nSetting the gradient to zero yields the linear system:\n$$ (A^T A + 2\\lambda I) w = A^T b + v^{(t)} $$\nSince $A^T A + 2\\lambda I$ is positive definite for $\\lambda > 0$, it is invertible. The update rule for $w^{(t+1)}$ is the solution to this system:\n$$ w^{(t+1)} = (A^T A + 2\\lambda I)^{-1} (A^T b + v^{(t)}) $$\nwhere $v^{(t)}$ is the vector with components $v_i^{(t)} = 2\\lambda \\cdot \\text{sgn}(w_i^{(t)})$. This equation forms the core of the iterative algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of penalized least-squares problems using the\n    Convex-Concave Procedure (CCP).\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"A\": np.array([\n                [0.5, -0.2, 0.1, 0.0],\n                [1.0, 0.3, -0.4, 0.2],\n                [-0.7, 0.5, 0.0, -0.1],\n                [0.0, -0.6, 0.8, 0.3],\n                [0.2, 0.1, -0.5, -0.9],\n                [0.3, 0.7, 0.9, -0.4],\n                [-0.4, -0.2, 0.6, 0.5],\n                [0.6, -0.1, -0.3, 0.7]\n            ]),\n            \"b\": np.array([0.1, -0.5, 0.3, -0.2, 0.8, -0.9, 0.0, 0.4]),\n            \"lambda\": 1.0,\n            \"w0\": np.array([0.0, 0.0, 0.0, 0.0])\n        },\n        # Case 2 (strong quantization pressure)\n        {\n            \"A\": np.array([\n                [0.5, -0.2, 0.1, 0.0],\n                [1.0, 0.3, -0.4, 0.2],\n                [-0.7, 0.5, 0.0, -0.1],\n                [0.0, -0.6, 0.8, 0.3],\n                [0.2, 0.1, -0.5, -0.9],\n                [0.3, 0.7, 0.9, -0.4],\n                [-0.4, -0.2, 0.6, 0.5],\n                [0.6, -0.1, -0.3, 0.7]\n            ]),\n            \"b\": np.array([0.1, -0.5, 0.3, -0.2, 0.8, -0.9, 0.0, 0.4]),\n            \"lambda\": 100.0,\n            \"w0\": np.array([0.0, 0.0, 0.0, 0.0])\n        },\n        # Case 3 (zero data term)\n        {\n            \"A\": np.zeros((6, 3)),\n            \"b\": np.zeros(6),\n            \"lambda\": 0.5,\n            \"w0\": np.array([0.2, -0.3, 0.8])\n        },\n        # Case 4 (weak quantization pressure)\n        {\n            \"A\": np.array([\n                [0.5, -0.2, 0.1, 0.0],\n                [1.0, 0.3, -0.4, 0.2],\n                [-0.7, 0.5, 0.0, -0.1],\n                [0.0, -0.6, 0.8, 0.3],\n                [0.2, 0.1, -0.5, -0.9],\n                [0.3, 0.7, 0.9, -0.4],\n                [-0.4, -0.2, 0.6, 0.5],\n                [0.6, -0.1, -0.3, 0.7]\n            ]),\n            \"b\": np.array([0.1, -0.5, 0.3, -0.2, 0.8, -0.9, 0.0, 0.4]),\n            \"lambda\": 0.01,\n            \"w0\": np.array([0.0, 0.0, 0.0, 0.0])\n        },\n        # Case 5 (one-dimensional)\n        {\n            \"A\": np.array([1.0, -0.5, 0.3, 0.7, -0.2]).reshape(-1, 1),\n            \"b\": np.array([0.2, -0.1, 0.0, 1.0, -0.5]),\n            \"lambda\": 0.5,\n            \"w0\": np.array([0.0])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        lmbda = case[\"lambda\"]\n        w = case[\"w0\"].copy()\n        m, n = A.shape\n        \n        # Algorithm parameters\n        max_iter = 100\n        tolerance = 1e-6\n        converged = False\n\n        # Pre-compute matrices for the linear system in the update rule\n        # M_inv = (A^T A + 2*lambda*I)^(-1)\n        AtA = A.T @ A\n        I = np.eye(n)\n        M_inv = np.linalg.inv(AtA + 2 * lmbda * I)\n        \n        # A^T b\n        Atb = A.T @ b\n\n        for t in range(max_iter):\n            w_old = w.copy()\n            \n            # Compute subgradient v of H(w) = 2*lambda*||w||_1 at w_old\n            # Using sgn(0)=0 as specified\n            v = 2 * lmbda * np.sign(w_old)\n            \n            # Solve the linear system for the next iterate\n            # w = M_inv * (A^T*b + v)\n            rhs = Atb + v\n            w = M_inv @ rhs\n            \n            # Check for convergence\n            # Use max(||w_old||_2, 1) as the denominator for numerical stability\n            w_old_norm = np.linalg.norm(w_old)\n            rel_change = np.linalg.norm(w - w_old) / max(w_old_norm, 1.0)\n            \n            if rel_change < tolerance:\n                converged = True\n                break\n        \n        # --- Post-processing and output calculation ---\n\n        # 1. Count snapped coordinates\n        # A coordinate w_i is \"snapped\" if | |w_i| - 1| <= 1e-3\n        num_snapped = np.sum(np.abs(np.abs(w) - 1.0) <= 1e-3)\n        \n        # 2. Calculate final objective value F(w)\n        # F(w) = 0.5 * ||Aw - b||^2 + lambda * sum(min((w_i - 1)^2, (w_i + 1)^2))\n        least_squares_term = 0.5 * np.linalg.norm(A @ w - b)**2\n        penalty_term_per_coord = np.minimum((w - 1)**2, (w + 1)**2)\n        penalty_term = lmbda * np.sum(penalty_term_per_coord)\n        final_objective_value = least_squares_term + penalty_term\n        \n        results.append(\n            f\"[{num_snapped},{final_objective_value:.6f},{str(converged).lower()}]\"\n        )\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3114735"}, {"introduction": "Our final practice delves into the strategic aspect of CCP and its connection to sparsity, a central concept in modern data science. We will explore an objective where the choice of DC decomposition is not obvious and can be tailored to reveal connections to other well-known methods like the LASSO. This problem [@problem_id:3114720] will challenge you to think beyond a single fixed recipe for DC splitting and to analyze how your modeling choices influence the behavior of the algorithm, particularly the sparsity patterns in the solution iterates.", "problem": "Consider the nonconvex objective $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$ where $A \\in \\mathbb{R}^{m \\times n}$ and $\\mu > 0$. You will use the framework of Difference-of-Convex programming (DC programming) and the Convex-Concave Procedure (CCP) to construct a convex surrogate subproblem that is solved at each iteration.\n\nStarting from the core definitions of DC programming and subgradients of the $\\ell_1$ norm, first construct a valid DC decomposition of $f$ by introducing an auxiliary parameter $\\lambda > 0$ and expressing $f$ as the difference of two convex functions. Then, using the definition of the Convex-Concave Procedure (CCP), linearize the concave part at a current iterate $x^{k}$ via a subgradient in order to derive the explicit form of the convex surrogate that is minimized to produce $x^{k+1}$. Show that the surrogate subproblem has the structure of a Least Absolute Shrinkage and Selection Operator (LASSO)-like objective (quadratic loss plus an $\\ell_1$ penalty) with an additional linear term that depends on the chosen subgradient.\n\nFinally, for the specific instance with\n- $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$,\n- $\\mu > 0$, $\\lambda > 0$,\n- initial point $x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- and the subgradient selection $s^{0} \\in \\partial \\|x^{0}\\|_{1}$ chosen as $s^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n\ncompute the exact minimizer $x^{1}$ of the first CCP surrogate in closed form. Express your final answer as a single two-entry row matrix in terms of $\\mu$. No rounding is required, and no units are involved.\n\nIn addition to the computation, explain, based on the derived surrogate, how the extra linear term affects sparsity patterns in the iterates relative to a standard LASSO objective (without that linear term). Your explanation must be grounded in the first-principles derivation and should not rely on unintroduced shortcut formulas.", "solution": "The user wants to analyze the nonconvex objective function $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$ using the Convex-Concave Procedure (CCP). This involves three main parts:\n1.  Deriving the general form of the CCP surrogate subproblem.\n2.  Solving for the first iterate $x^1$ given a specific initial point $x^0$ and subgradient choice.\n3.  Explaining the role of the additional linear term in the surrogate on the solution's sparsity.\n\n### Problem Validation\nThe problem statement is analyzed first.\n\n**Step 1: Extract Givens**\n- Objective function: $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$\n- Matrix $A \\in \\mathbb{R}^{m \\times n}$\n- Constant $\\mu > 0$\n- Auxiliary parameter $\\lambda > 0$\n- Iterates: $x^k$ (current), $x^{k+1}$ (next)\n- Specific instance:\n    - $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$\n    - $\\mu > 0$, $\\lambda > 0$\n    - Initial point $x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n    - Subgradient choice $s^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\in \\partial \\|x^{0}\\|_{1}$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on established principles of convex analysis and optimization, specifically Difference-of-Convex (DC) programming and the Convex-Concave Procedure (CCP). These are standard techniques. The functions involved, the squared $\\ell_2$-norm and the $\\ell_1$-norm, are fundamental in mathematics and its applications.\n- **Well-Posed:** The problem is clearly stated. It asks for the derivation of a surrogate function and the computation of the next iterate for a specific, fully defined instance. The subproblem to be derived is convex, ensuring that a minimizer exists.\n- **Objective:** The problem is formulated in precise mathematical language, free of ambiguity or subjective claims.\n\nThe subgradient choice must be validated. The subgradient of $\\|x\\|_1$ at a point $x$ is the set of vectors $s$ with components $s_i = \\text{sign}(x_i)$ if $x_i \\neq 0$ and $s_i \\in [-1, 1]$ if $x_i = 0$. For $x^0 = (1, 0)^T$, we have $x^0_1 = 1$ and $x^0_2 = 0$. A valid subgradient $s^0$ must have $s^0_1 = \\text{sign}(1) = 1$ and $s^0_2 \\in [-1, 1]$. The provided choice $s^0 = (1, 0)^T$ satisfies these conditions, as $s^0_1=1$ and $s^0_2=0 \\in [-1, 1]$.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. We may proceed with the solution.\n\n### Part 1: Derivation of the CCP Surrogate Subproblem\n\nThe core of the problem is to formulate the optimization of $f(x)$ within the DC programming framework, which requires expressing $f(x)$ as a difference of two convex functions, $f(x) = g(x) - h(x)$. The problem specifies constructing such a decomposition by introducing an auxiliary parameter $\\lambda > 0$, and requires the resulting subproblem to be a LASSO-like objective with an additional linear term.\n\nThe standard decomposition $g(x) = \\|Ax\\|_2^2$ and $h(x) = \\mu\\|x\\|_1$ does not yield the required structure. We must find a decomposition where the $\\ell_1$-norm remains in the convex part $g(x)$ that is not linearized.\n\nWe introduce $\\lambda > 0$ by adding and subtracting the term $\\lambda\\|x\\|_1$ from the objective function:\n$$f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1} = (\\|A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}) - (\\lambda + \\mu)\\|x\\|_{1}$$\nLet us define two functions, $g(x)$ and $h(x)$:\n- $g(x) = \\|A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$\n- $h(x) = (\\lambda + \\mu)\\|x\\|_{1}$\n\nFor this to be a valid DC decomposition, both $g(x)$ and $h(x)$ must be convex.\n- The function $\\|A x\\|_{2}^{2}$ is convex. Since $\\lambda > 0$, the function $\\lambda \\|x\\|_{1}$ is also convex. The sum of two convex functions is convex, so $g(x)$ is convex.\n- Since $\\lambda > 0$ and $\\mu > 0$, the coefficient $(\\lambda + \\mu)$ is positive. A positive scaling of a convex function ($\\|x\\|_1$) is convex, so $h(x)$ is convex.\n\nWe have successfully expressed $f(x) = g(x) - h(x)$ as a difference of two convex functions.\n\nThe Convex-Concave Procedure (CCP) generates a sequence of iterates $\\{x^k\\}$ by minimizing a convex surrogate of $f(x)$ at each step. The surrogate is constructed by linearizing the second convex function, $h(x)$, around the current iterate $x^k$. By the definition of convexity, we have the following inequality for any $x$ and a subgradient $s_h(x^k) \\in \\partial h(x^k)$:\n$$h(x) \\ge h(x^k) + \\langle s_h(x^k), x - x^k \\rangle$$\nThis implies $-h(x) \\le -h(x^k) - \\langle s_h(x^k), x - x^k \\rangle$.\nSubstituting this into the expression for $f(x)$ gives an upper bound (the surrogate function):\n$$f(x) = g(x) - h(x) \\le g(x) - (h(x^k) + \\langle s_h(x^k), x - x^k \\rangle) =: \\hat{f}(x; x^k)$$\nThe next iterate $x^{k+1}$ is found by minimizing this surrogate:\n$$x^{k+1} = \\arg\\min_x \\hat{f}(x; x^k)$$\nThe terms $h(x^k)$ and $\\langle s_h(x^k), x^k \\rangle$ are constant with respect to $x$, so they can be dropped from the minimization. The problem becomes:\n$$x^{k+1} = \\arg\\min_x \\{ g(x) - \\langle s_h(x^k), x \\rangle \\}$$\nWe need the subgradient of $h(x) = (\\lambda+\\mu)\\|x\\|_1$. Using the scaling rule for subgradients:\n$$\\partial h(x) = (\\lambda+\\mu) \\partial\\|x\\|_1$$\nLet $s(x^k)$ be a chosen subgradient from $\\partial\\|x\\|_1$ at $x^k$. Then we can set $s_h(x^k) = (\\lambda+\\mu)s(x^k)$.\nSubstituting the expressions for $g(x)$ and $s_h(x^k)$ into the minimization problem, we get the explicit form of the surrogate objective to be minimized:\n$$x^{k+1} = \\arg\\min_x \\left\\{ \\|Ax\\|_2^2 + \\lambda \\|x\\|_1 - \\langle (\\lambda+\\mu)s(x^k), x \\rangle \\right\\}$$\nThis objective has the structure of a LASSO problem (a quadratic loss $\\|Ax\\|_2^2$ plus an $\\ell_1$ penalty $\\lambda\\|x\\|_1$) with an additional linear term $-\\left((\\lambda+\\mu)s(x^k)\\right)^T x$.\n\n### Part 2: Computation of $x^1$\n\nWe are given the specific instance:\n- $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$, $x^0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $s^0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The parameters $\\mu$ and $\\lambda$ are positive constants.\n\nWe need to compute $x^1 = \\arg\\min_x \\{ \\|Ax\\|_2^2 + \\lambda \\|x\\|_1 - ((\\lambda+\\mu)s^0)^T x \\}$. Let $x = (x_1, x_2)^T$.\n\nThe objective function $L(x)$ to be minimized is:\n$$L(x_1, x_2) = \\left\\| \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right\\|_2^2 + \\lambda \\left\\| \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right\\|_1 - (\\lambda+\\mu) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^T \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\nExpanding the terms:\n- $\\|Ax\\|_2^2 = x_1^2 + (2x_2)^2 = x_1^2 + 4x_2^2$.\n- $\\lambda\\|x\\|_1 = \\lambda(|x_1| + |x_2|)$.\n- $((\\lambda+\\mu)s^0)^T x = (\\lambda+\\mu)(1 \\cdot x_1 + 0 \\cdot x_2) = (\\lambda+\\mu)x_1$.\n\nThe objective function separates with respect to $x_1$ and $x_2$:\n$$L(x_1, x_2) = \\left( x_1^2 + \\lambda|x_1| - (\\lambda+\\mu)x_1 \\right) + \\left( 4x_2^2 + \\lambda|x_2| \\right)$$\nWe can minimize the two parts independently.\n\n**Minimization for $x_1$**:\nLet $F_1(x_1) = x_1^2 - (\\lambda+\\mu)x_1 + \\lambda|x_1|$. We find the minimum using the subgradient optimality condition, $0 \\in \\partial F_1(x_1)$.\nThe subgradient is $\\partial F_1(x_1) = 2x_1 - (\\lambda+\\mu) + \\lambda \\partial|x_1|$.\n- If $x_1 > 0$, $\\partial|x_1| = \\{1\\}$. The derivative is $2x_1 - (\\lambda+\\mu) + \\lambda = 2x_1 - \\mu$. Setting it to zero gives $x_1 = \\frac{\\mu}{2}$. Since $\\mu > 0$, this solution is in the domain $x_1 > 0$ and is a candidate.\n- If $x_1 < 0$, $\\partial|x_1| = \\{-1\\}$. The derivative is $2x_1 - (\\lambda+\\mu) - \\lambda = 2x_1 - (2\\lambda+\\mu)$. Setting it to zero gives $x_1 = \\frac{2\\lambda+\\mu}{2}$. Since $\\lambda, \\mu > 0$, this value is positive, which contradicts the assumption $x_1 < 0$. There is no solution in this domain.\n- If $x_1 = 0$, $\\partial|x_1| = [-1, 1]$. The subgradient set is $\\partial F_1(0) = -(\\lambda+\\mu) + \\lambda[-1, 1] = [-(\\lambda+\\mu)-\\lambda, -(\\lambda+\\mu)+\\lambda] = [-(2\\lambda+\\mu), -\\mu]$. Since $\\mu > 0$, this interval does not contain $0$. So $x_1=0$ is not the minimizer.\nThe unique minimizer is $x_1 = \\frac{\\mu}{2}$.\n\n**Minimization for $x_2$**:\nLet $F_2(x_2) = 4x_2^2 + \\lambda|x_2|$. We find the minimum where $0 \\in \\partial F_2(x_2)$.\nThe subgradient is $\\partial F_2(x_2) = 8x_2 + \\lambda \\partial|x_2|$.\n- If $x_2 > 0$, the derivative is $8x_2 + \\lambda$. Setting to zero gives $x_2 = -\\frac{\\lambda}{8}$, which contradicts $x_2 > 0$ (since $\\lambda>0$).\n- If $x_2 < 0$, the derivative is $8x_2 - \\lambda$. Setting to zero gives $x_2 = \\frac{\\lambda}{8}$, which contradicts $x_2 < 0$.\n- If $x_2 = 0$, the subgradient set is $\\partial F_2(0) = 8(0) + \\lambda[-1, 1] = [-\\lambda, \\lambda]$. Since $\\lambda > 0$, this interval contains $0$.\nThus, the unique minimizer is $x_2 = 0$.\n\nCombining the results, the minimizer is $x^1 = \\begin{pmatrix} \\frac{\\mu}{2} \\\\ 0 \\end{pmatrix}$.\n\n### Part 3: Effect of the Linear Term on Sparsity\n\nThe surrogate subproblem is to minimize $J(x) = \\|Ax\\|_2^2 + \\lambda\\|x\\|_1 - c^T x$, where $c = (\\lambda+\\mu)s(x^k)$. A standard LASSO objective lacks the linear term $-c^T x$. We analyze how this term affects sparsity.\n\nThe first-order optimality condition for the minimizer $x$ is $0 \\in \\partial J(x)$:\n$$0 \\in 2A^TAx + \\lambda \\partial\\|x\\|_1 - c$$\nThis can be rewritten as $c \\in 2A^TAx + \\lambda \\partial\\|x\\|_1$.\nLet's examine the condition for a component $x_i$ to be zero at the solution. If $x_i=0$, then the $i$-th component of $\\partial\\|x\\|_1$ is the interval $[-1, 1]$. The $i$-th component of the optimality condition becomes:\n$$c_i \\in (2A^TAx)_i + \\lambda[-1, 1] \\quad \\text{where } x_i=0$$\nThis is equivalent to $|(2A^TAx)_i - c_i| \\le \\lambda$. In this expression, the term $(2A^TAx)_i$ represents the influence of the other non-zero components of the solution. For comparison, the sparsity condition for standard LASSO (where $c=0$) is $|(2A^TAx)_i| \\le \\lambda$.\n\nThe vector $c$ is defined as $c = (\\lambda+\\mu)s(x^k)$, where $s(x^k) \\in \\partial\\|x\\|_1|_{x^k}$.\n- For a component $i$ that was non-zero at the previous iterate ($x_i^k \\ne 0$), we have $s_i(x^k) = \\text{sign}(x_i^k)$. Therefore, $c_i = (\\lambda+\\mu)\\text{sign}(x_i^k)$. This is a non-zero value with a magnitude of $\\lambda+\\mu$. The condition for $x_i$ to become zero in the next step is $|(2A^TAx)_i - (\\lambda+\\mu)\\text{sign}(x_i^k)| \\le \\lambda$. Since $|\\lambda+\\mu| > \\lambda$, the term $(2A^TAx)_i$ must take on a specific non-zero value to satisfy the condition, making it much less likely for this component to become zero compared to the standard LASSO case. This introduces an \"inertia\" or \"memory\" effect, where non-zero components are encouraged to remain non-zero.\n\n- For a component $j$ that was zero at the previous iterate ($x_j^k = 0$), we can choose any $s_j(x^k) \\in [-1, 1]$. A common choice is $s_j(x^k)=0$. In this case, $c_j=0$, and the condition for $x_j$ to remain zero is $|(2A^TAx)_j| \\le \\lambda$, which is identical to the standard LASSO condition.\n\nIn summary, the linear term $-c^T x$ makes it more difficult for previously active (non-zero) variables to become inactive (zero). It incentivizes the new iterate $x^{k+1}$ to maintain the sign pattern of the previous subgradient $s(x^k)$. This \"inertia\" can lead to iterates that are less sparse than what would be obtained by solving a standard LASSO problem, as it promotes the persistence of the active set across iterations.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\mu}{2} & 0\n\\end{pmatrix}\n}\n$$", "id": "3114720"}]}