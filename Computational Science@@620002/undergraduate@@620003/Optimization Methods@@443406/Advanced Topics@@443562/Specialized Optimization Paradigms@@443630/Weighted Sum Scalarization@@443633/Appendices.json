{"hands_on_practices": [{"introduction": "The first step in understanding weighted sum scalarization is to see how the optimal solution responds as we adjust the balance between objectives. This exercise provides a foundational, analytical look at this relationship using a simple bi-objective problem. By working with basic quadratic functions, you will derive a closed-form expression for the optimal decision, $x^*(\\lambda)$, and directly observe how it maps the space of weights to the set of Pareto optimal solutions [@problem_id:3198566].", "problem": "Consider a bi-objective unconstrained optimization problem with decision variable $x \\in \\mathbb{R}$ and objectives $f_{1}(x) = x^{2}$ and $f_{2}(x) = (x - 1)^{2}$. A standard approach to multiobjective optimization is the weighted sum scalarization, which forms a single-objective problem by using nonnegative weights that sum to $1$. Specifically, for a given weight $\\lambda \\in [0, 1]$, define the scalarized objective $g_{\\lambda}(x) = \\lambda f_{1}(x) + (1 - \\lambda) f_{2}(x)$ and consider minimizing $g_{\\lambda}(x)$ over $x \\in \\mathbb{R}$. Using only core definitions of convexity, differentiability, and first-order optimality conditions for unconstrained convex optimization, derive the optimizer $x^{\\ast}(\\lambda)$ in closed form and justify its uniqueness for each $\\lambda \\in [0, 1]$. Then, analyze qualitatively how $x^{\\ast}(\\lambda)$ varies with $\\lambda$ and determine whether this dependence is monotone and Lipschitz continuous with respect to $\\lambda$.\n\nProvide as your final answer the single closed-form expression for $x^{\\ast}(\\lambda)$ as a function of $\\lambda$. No numerical rounding is required. No units are involved.", "solution": "The problem requires finding the unique minimizer of a scalarized objective function formed by a weighted sum of two objectives, and then analyzing the properties of this minimizer as a function of the weight.\n\nThe problem is a bi-objective unconstrained optimization problem with decision variable $x \\in \\mathbb{R}$. The two objective functions are given as $f_{1}(x) = x^{2}$ and $f_{2}(x) = (x - 1)^{2}$.\n\nThe weighted sum scalarization method is used to combine these into a single-objective problem. For a weight $\\lambda \\in [0, 1]$, the scalarized objective function $g_{\\lambda}(x)$ is defined as:\n$$g_{\\lambda}(x) = \\lambda f_{1}(x) + (1 - \\lambda) f_{2}(x)$$\nSubstituting the given functions:\n$$g_{\\lambda}(x) = \\lambda x^{2} + (1 - \\lambda) (x - 1)^{2}$$\nThe task is to find the minimizer $x^{\\ast}(\\lambda)$ for the problem $\\min_{x \\in \\mathbb{R}} g_{\\lambda}(x)$.\n\nFirst, to justify the existence and uniqueness of the minimizer, we will analyze the convexity of the function $g_{\\lambda}(x)$. A twice-differentiable function is strictly convex if its second derivative is strictly positive.\n\nThe individual objective functions $f_{1}(x)$ and $f_{2}(x)$ are twice-differentiable. Let's compute their second derivatives:\nFor $f_{1}(x) = x^{2}$, the first derivative is $f_{1}'(x) = 2x$ and the second derivative is $f_{1}''(x) = 2$.\nFor $f_{2}(x) = (x - 1)^{2}$, the first derivative is $f_{2}'(x) = 2(x - 1)$ and the second derivative is $f_{2}''(x) = 2$.\nSince $f_{1}''(x) = 2 > 0$ and $f_{2}''(x) = 2 > 0$ for all $x \\in \\mathbb{R}$, both $f_{1}(x)$ and $f_{2}(x)$ are strictly convex functions.\n\nThe scalarized objective $g_{\\lambda}(x)$ is a linear combination of $f_{1}(x)$ and $f_{2}(x)$. Its second derivative is:\n$$g_{\\lambda}''(x) = \\frac{d^{2}}{dx^{2}} \\left[ \\lambda f_{1}(x) + (1 - \\lambda) f_{2}(x) \\right] = \\lambda f_{1}''(x) + (1 - \\lambda) f_{2}''(x)$$\nSubstituting the values of the second derivatives:\n$$g_{\\lambda}''(x) = \\lambda (2) + (1 - \\lambda) (2) = 2\\lambda + 2 - 2\\lambda = 2$$\nSince $g_{\\lambda}''(x) = 2 > 0$ for all $x \\in \\mathbb{R}$ and for any choice of $\\lambda \\in [0, 1]$, the function $g_{\\lambda}(x)$ is strictly convex. A strictly convex function defined on a convex domain (in this case, $\\mathbb{R}$) has at most one global minimizer. Since $g_{\\lambda}(x)$ is a quadratic function with a positive leading coefficient, it is coercive (i.e., $g_{\\lambda}(x) \\to \\infty$ as $|x| \\to \\infty$), which guarantees that a global minimum exists. Therefore, for each $\\lambda \\in [0, 1]$, there is a unique global minimizer, which we denote as $x^{\\ast}(\\lambda)$.\n\nTo find this unique minimizer, we apply the first-order necessary and sufficient condition for optimality for an unconstrained convex optimization problem. This condition states that the global minimizer $x^{\\ast}$ is the point where the first derivative of the objective function is zero.\n$$g_{\\lambda}'(x^{\\ast}) = 0$$\nTo facilitate differentiation, we can expand the expression for $g_{\\lambda}(x)$:\n$$g_{\\lambda}(x) = \\lambda x^{2} + (1 - \\lambda)(x^{2} - 2x + 1) = \\lambda x^{2} + x^{2} - 2x + 1 - \\lambda x^{2} + 2\\lambda x - \\lambda$$\n$$g_{\\lambda}(x) = x^{2} - 2(1 - \\lambda)x + (1 - \\lambda)$$\nNow, we compute the first derivative with respect to $x$:\n$$g_{\\lambda}'(x) = \\frac{d}{dx} \\left[ x^{2} - 2(1 - \\lambda)x + (1 - \\lambda) \\right] = 2x - 2(1 - \\lambda)$$\nSetting the derivative to zero to find the optimizer $x^{\\ast}(\\lambda)$:\n$$2x^{\\ast}(\\lambda) - 2(1 - \\lambda) = 0$$\n$$2x^{\\ast}(\\lambda) = 2(1 - \\lambda)$$\n$$x^{\\ast}(\\lambda) = 1 - \\lambda$$\nThis is the closed-form expression for the unique optimizer as a function of the weight $\\lambda$.\n\nNext, we analyze the properties of $x^{\\ast}(\\lambda) = 1 - \\lambda$ as a function of $\\lambda$ on the interval $[0, 1]$.\nAs $\\lambda$ varies from $0$ to $1$, the optimizer $x^{\\ast}(\\lambda)$ takes values from $x^{\\ast}(0) = 1 - 0 = 1$ to $x^{\\ast}(1) = 1 - 1 = 0$. The set of all possible optimizers, known as the Pareto front in the decision space, is the interval $[0, 1]$.\n\nFor monotonicity, we examine the derivative of $x^{\\ast}(\\lambda)$ with respect to $\\lambda$:\n$$\\frac{d x^{\\ast}}{d \\lambda} = \\frac{d}{d\\lambda}(1 - \\lambda) = -1$$\nSince the derivative is a constant $-1$, which is strictly negative, the function $x^{\\ast}(\\lambda)$ is strictly monotonically decreasing with respect to $\\lambda$ over its domain $[0, 1]$.\n\nFor Lipschitz continuity, we must show that there exists a constant $L \\ge 0$ such that for any $\\lambda_{1}, \\lambda_{2} \\in [0, 1]$, the inequality $|x^{\\ast}(\\lambda_{1}) - x^{\\ast}(\\lambda_{2})| \\le L |\\lambda_{1} - \\lambda_{2}|$ holds.\nLet's compute the absolute difference:\n$$|x^{\\ast}(\\lambda_{1}) - x^{\\ast}(\\lambda_{2})| = |(1 - \\lambda_{1}) - (1 - \\lambda_{2})| = |-\\lambda_{1} + \\lambda_{2}| = |\\lambda_{2} - \\lambda_{1}| = 1 \\cdot |\\lambda_{1} - \\lambda_{2}|$$\nThis identity shows that $x^{\\ast}(\\lambda)$ is Lipschitz continuous with a Lipschitz constant of $L=1$. This is also confirmed by the fact that its first derivative is bounded, with $\\sup_{\\lambda \\in [0, 1]} |\\frac{d x^{\\ast}}{d \\lambda}| = |-1| = 1$.", "answer": "$$\\boxed{1 - \\lambda}$$", "id": "3198566"}, {"introduction": "Building on the smooth case, we now explore how weighted sum scalarization performs in Linear Programming (LP), where feasible sets are polyhedra and Pareto fronts are often piecewise-linear. This practice connects the algebraic scalarization to the geometric intuition of finding optimal vertices of the feasible region. You will determine the specific weight value, or \"breakpoint,\" at which the optimal solution discontinuously jumps from one vertex to another, a key feature of multi-objective LPs [@problem_id:3198555].", "problem": "Consider the bi-objective Linear Programming (LP) problem with decision vector $x = (x_{1}, x_{2})$ and feasible set given by the polyhedron\n$$\n\\mathcal{P} = \\{x \\in \\mathbb{R}^{2} \\mid x_{1} + x_{2} \\leq 4,\\; 2x_{1} + x_{2} \\leq 5,\\; x_{1} \\geq 0,\\; x_{2} \\geq 0\\}.\n$$\nThe two objectives to be minimized are\n$$\nf_{1}(x) = -x_{1}, \\qquad f_{2}(x) = -x_{2}.\n$$\nFor a weight parameter $\\lambda \\in [0,1]$, the weighted-sum scalarization constructs the single-objective problem\n$$\n\\min_{x \\in \\mathcal{P}} \\; g_{\\lambda}(x) \\equiv \\lambda f_{1}(x) + (1-\\lambda) f_{2}(x).\n$$\nUsing only the following fundamental bases:\n- the fundamental theorem of Linear Programming (LP) that a linear objective over a nonempty bounded polyhedron attains an optimum at an extreme point,\n- convexity of linear images of convex sets and the supporting-hyperplane viewpoint for linear objectives,\n\nfirst argue why, as $\\lambda$ varies in $[0,1]$, the minimizers of $g_{\\lambda}$ lie at extreme points of $\\mathcal{P}$ except for values of $\\lambda$ where the level sets of $g_{\\lambda}$ are parallel to an edge of $\\mathcal{P}$, in which case any point on that edge is optimal. Then, focusing on the efficient frontier traced by the upper boundary of $\\mathcal{P}$, determine the exact value of the breakpoint $\\lambda^{\\star} \\in (0,1)$ at which the optimizer of $g_{\\lambda}$ switches from the vertex $(0,4)$ to the vertex $(1,3)$ as $\\lambda$ increases from $0$ to $1$.\n\nGive your answer as a single reduced fraction. No rounding is required. The final answer must be a single number with no units.", "solution": "The problem asks for two things: first, to provide a justification for why the minimizers of the weighted-sum objective function are extreme points of the feasible set, and second, to calculate the specific weight $\\lambda^{\\star}$ at which the optimal solution transitions between two given vertices.\n\nFirst, we address the qualitative argument. The feasible set is the polyhedron $\\mathcal{P} = \\{x \\in \\mathbb{R}^{2} \\mid x_{1} + x_{2} \\leq 4,\\; 2x_{1} + x_{2} \\leq 5,\\; x_{1} \\geq 0,\\; x_{2} \\geq 0\\}$. This set is defined by the intersection of four closed linear half-spaces. Since the variables $x_1$ and $x_2$ are non-negative and bounded above (e.g., $x_1 \\leq 5/2$ and $x_2 \\leq 4$), the set $\\mathcal{P}$ is a non-empty, bounded, and convex set, specifically a convex polytope in $\\mathbb{R}^2$. The objective functions to be minimized are $f_{1}(x) = -x_{1}$ and $f_{2}(x) = -x_{2}$.\n\nThe weighted-sum scalarization forms a single-objective problem with the objective function $g_{\\lambda}(x) = \\lambda f_{1}(x) + (1-\\lambda) f_{2}(x)$ for a weight parameter $\\lambda \\in [0,1]$. Substituting the given functions, we have:\n$$\ng_{\\lambda}(x) = \\lambda (-x_{1}) + (1-\\lambda) (-x_{2}) = -\\lambda x_{1} - (1-\\lambda) x_{2}\n$$\nFor any fixed $\\lambda \\in [0,1]$, $g_{\\lambda}(x)$ is a linear function of $x = (x_1, x_2)$. Therefore, the problem $\\min_{x \\in \\mathcal{P}} g_{\\lambda}(x)$ is a Linear Programming (LP) problem.\n\nThe problem statement requires using two fundamental principles:\n\n1.  **The fundamental theorem of Linear Programming**: This theorem states that for a linear program with a non-empty feasible region that has at least one extreme point, if there is an optimal solution, then at least one extreme point (vertex) of the feasible region is optimal. Since our feasible set $\\mathcal{P}$ is a non-empty bounded polyhedron, it is a compact set, and the continuous linear function $g_{\\lambda}(x)$ must attain its minimum on $\\mathcal{P}$. By the theorem, this minimum value must be achieved at one or more of the extreme points of $\\mathcal{P}$.\n\n2.  **Convexity of linear images and the supporting-hyperplane viewpoint**: The feasible set $\\mathcal{P}$ is a convex set. The level sets of the objective function $g_{\\lambda}(x)$ are lines of the form $-\\lambda x_{1} - (1-\\lambda) x_{2} = c$, where $c$ is a constant. Minimizing $g_{\\lambda}(x)$ over $\\mathcal{P}$ is geometrically equivalent to finding the smallest value of $c$ for which this line has a non-empty intersection with $\\mathcal{P}$. This line is a supporting hyperplane to the convex set $\\mathcal{P}$.\n    -   Typically, a supporting hyperplane will touch a convex polytope at a single point, which must be a vertex (an extreme point). This occurs when the gradient vector of the objective function, $\\nabla g_{\\lambda} = (-\\lambda, -(1-\\lambda))$, is not parallel to the normal of any of the bounding hyperplanes of $\\mathcal{P}$.\n    -   The exception, where the set of optimal solutions is larger than a single point, happens when the level sets of $g_{\\lambda}(x)$ are parallel to one of the edges of the polyhedron $\\mathcal{P}$. This corresponds to the gradient $\\nabla g_{\\lambda}$ being orthogonal to the direction vector of that edge. In this situation, the supporting hyperplane coincides with the entire edge. Consequently, every point on that edge, including its two endpoint vertices, is an optimal solution. This occurs for specific, non-generic values of $\\lambda$.\n\nThis completes the argument that minimizers of $g_{\\lambda}(x)$ are always found at the extreme points of $\\mathcal{P}$, and for certain values of $\\lambda$, the optimal set can extend to an entire edge.\n\nNext, we determine the breakpoint value $\\lambda^{\\star}$. The problem specifies that as $\\lambda$ increases from $0$ to $1$, the optimizer of $g_{\\lambda}$ switches from the vertex $(0,4)$ to the vertex $(1,3)$. Let's denote these vertices as $V_1 = (0,4)$ and $V_2 = (1,3)$. We can verify they are indeed extreme points of $\\mathcal{P}$.\n-   For $V_1 = (0,4)$: $0+4 = 4 \\leq 4$; $2(0)+4=4 \\leq 5$; $0 \\geq 0$; $4 \\geq 0$. All constraints are satisfied. It's the intersection of $x_1=0$ and $x_1+x_2=4$.\n-   For $V_2 = (1,3)$: $1+3 = 4 \\leq 4$; $2(1)+3=5 \\leq 5$; $1 \\geq 0$; $3 \\geq 0$. All constraints are satisfied. It's the intersection of $x_1+x_2=4$ and $2x_1+x_2=5$.\n\nThe transition between two vertices as the optimal solution occurs at the precise value of $\\lambda$ where both vertices are simultaneously optimal. This means they must yield the same objective function value. This is the condition for the supporting hyperplane to be parallel to the edge connecting $V_1$ and $V_2$. We set $g_{\\lambda^{\\star}}(V_1) = g_{\\lambda^{\\star}}(V_2)$ to find the breakpoint $\\lambda^{\\star}$.\n\nLet's evaluate the objective function $g_{\\lambda}(x) = -\\lambda x_{1} - (1-\\lambda) x_{2}$ at each vertex:\n-   At $V_1 = (0,4)$:\n    $$ g_{\\lambda}(0,4) = -\\lambda(0) - (1-\\lambda)(4) = -4(1-\\lambda) = 4\\lambda - 4 $$\n-   At $V_2 = (1,3)$:\n    $$ g_{\\lambda}(1,3) = -\\lambda(1) - (1-\\lambda)(3) = -\\lambda - 3 + 3\\lambda = 2\\lambda - 3 $$\n\nNow, we set the values equal to find $\\lambda^{\\star}$:\n$$ g_{\\lambda^{\\star}}(0,4) = g_{\\lambda^{\\star}}(1,3) $$\n$$ 4\\lambda^{\\star} - 4 = 2\\lambda^{\\star} - 3 $$\n$$ 4\\lambda^{\\star} - 2\\lambda^{\\star} = 4 - 3 $$\n$$ 2\\lambda^{\\star} = 1 $$\n$$ \\lambda^{\\star} = \\frac{1}{2} $$\n\nThis value $\\lambda^{\\star}=\\frac{1}{2}$ is in the interval $(0,1)$. For $\\lambda < 1/2$, we have $2\\lambda < 1$, which implies $4\\lambda - 4 < 2\\lambda - 3$, so $g_{\\lambda}(0,4) < g_{\\lambda}(1,3)$, making $(0,4)$ the unique minimizer. For $\\lambda > 1/2$, we have $2\\lambda > 1$, which implies $4\\lambda - 4 > 2\\lambda - 3$, so $g_{\\lambda}(0,4) > g_{\\lambda}(1,3)$, making $(1,3)$ the minimizer (assuming it's better than other vertices, which a full analysis confirms). Thus, $\\lambda^{\\star} = 1/2$ is indeed the breakpoint where the optimizer switches from $(0,4)$ to $(1,3)$ as $\\lambda$ increases. The answer is required as a single reduced fraction, which is $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3198555"}, {"introduction": "A method is only truly mastered when its limitations are also understood. This crucial exercise demonstrates a scenario where weighted sum scalarization fails to identify the entire Pareto front due to non-convexity. By analyzing a problem whose objective space has a concave boundary, you will discover the existence of \"unsupported\" efficient points—solutions that are truly Pareto optimal but are invisible to the weighted sum method, highlighting the need for more advanced techniques in certain situations [@problem_id:3154202].", "problem": "Consider a bi-objective minimization problem with a single decision variable $x \\in [0,1]$ and two objective functions $f_1(x)$ and $f_2(x)$ defined by\n$$\nf_1(x) = \\sqrt{1 - x^2}, \\qquad f_2(x) = x.\n$$\nThe feasible set in objective space is the image $\\{(f_1(x), f_2(x)) : x \\in [0,1]\\}$, which is the quarter of a unit circle arc in the first quadrant. The bi-objective goal is to simultaneously minimize $f_1$ and $f_2$.\n\nGround your reasoning in the core definitions of Pareto efficiency for minimization, the concept of weighted sum scalarization $w_1 f_1(x) + w_2 f_2(x)$ with $w_1 > 0$ and $w_2 > 0$, and basic properties of differentiable functions and convexity/concavity. Do not invoke any pre-packaged theorems beyond these fundamentals.\n\nTasks:\n1. Use the definition of Pareto efficiency to characterize the Pareto-efficient set for this problem. Explain why the entire arc $\\{(f_1(x), f_2(x)) : x \\in [0,1]\\}$ consists of nondominated points under minimization.\n2. Analyze the weighted sum scalarization $g_{w}(x) = w_1 f_1(x) + w_2 f_2(x)$ and show that for any strictly positive weights $w_1$ and $w_2$, every minimizer of $g_{w}$ over $[0,1]$ occurs at $x = 0$ or $x = 1$, and never at an interior point $x \\in (0,1)$. Conclude that interior points on the arc are efficient but unsupported by weighted sums.\n3. Identify one explicit unsupported efficient point $x^{\\star} \\in (0,1)$ and compute its objective vector $(f_1(x^{\\star}), f_2(x^{\\star}))$ in exact closed form. Report this objective vector as your final answer using a single row matrix. No rounding is required, and no units apply.", "solution": "The problem asks for a three-part analysis of a bi-objective minimization problem defined by the objective functions $f_1(x) = \\sqrt{1 - x^2}$ and $f_2(x) = x$ over the decision space $x \\in [0,1]$.\n\n**1. Characterization of the Pareto-Efficient Set**\n\nA solution $x^* \\in [0,1]$ is Pareto-efficient if there exists no other solution $x \\in [0,1]$ that dominates it. For a minimization problem, a solution $x$ dominates $x^*$ if $f_1(x) \\le f_1(x^*)$ and $f_2(x) \\le f_2(x^*)$, with at least one of these inequalities being strict.\n\nLet us test an arbitrary point $x_0 \\in [0,1]$ for Pareto efficiency. Suppose there exists another point $x \\in [0,1]$ that dominates $x_0$. According to the definition of dominance, the following two conditions must hold simultaneously:\n1. $f_2(x) \\le f_2(x_0) \\implies x \\le x_0$\n2. $f_1(x) \\le f_1(x_0) \\implies \\sqrt{1 - x^2} \\le \\sqrt{1 - x_0^2}$\n\nSince the square root function is strictly increasing for non-negative arguments, the second inequality is equivalent to:\n$1 - x^2 \\le 1 - x_0^2$\n$-x^2 \\le -x_0^2$\n$x^2 \\ge x_0^2$\n\nBecause the decision variable $x$ is restricted to the non-negative interval $[0,1]$, taking the square root of both sides gives:\n$x \\ge x_0$\n\nFor a point $x$ to dominate $x_0$, we must have both $x \\le x_0$ and $x \\ge x_0$. This is only possible if $x = x_0$.\nIf $x = x_0$, then $f_1(x) = f_1(x_0)$ and $f_2(x) = f_2(x_0)$. In this case, both inequalities in the definition of dominance are equalities, and the condition that at least one inequality must be strict is not met.\n\nTherefore, no feasible solution $x \\in [0,1]$ can be dominated by any other feasible solution. This means that every point in the decision space $x \\in [0,1]$ is a Pareto-efficient solution. Consequently, the entire feasible set in the objective space, which is the arc $\\{(f_1(x), f_2(x)) : x \\in [0,1]\\}$, consists of non-dominated points.\n\n**2. Analysis of the Weighted Sum Scalarization**\n\nThe weighted sum scalarization method transforms the bi-objective problem into a single-objective problem by minimizing the function $g_w(x) = w_1 f_1(x) + w_2 f_2(x)$ for strictly positive weights $w_1 > 0$ and $w_2 > 0$. The function to minimize over $x \\in [0,1]$ is:\n$$\ng_w(x) = w_1 \\sqrt{1 - x^2} + w_2 x\n$$\nTo find the minimizer of $g_w(x)$ on the closed interval $[0,1]$, we first analyze its behavior on the open interval $(0,1)$ by examining its derivative. The first derivative of $g_w(x)$ with respect to $x$ is:\n$$\ng_w'(x) = \\frac{d}{dx} \\left( w_1 (1 - x^2)^{1/2} + w_2 x \\right) = w_1 \\left( \\frac{1}{2} (1-x^2)^{-1/2} (-2x) \\right) + w_2 = -\\frac{w_1 x}{\\sqrt{1-x^2}} + w_2\n$$\nSetting $g_w'(x) = 0$ to find critical points in $(0,1)$:\n$$\n-\\frac{w_1 x}{\\sqrt{1-x^2}} + w_2 = 0 \\implies w_2 = \\frac{w_1 x}{\\sqrt{1-x^2}}\n$$\nSince $w_1, w_2 > 0$ and $x \\in (0,1)$, we can square both sides:\n$$\nw_2^2 = \\frac{w_1^2 x^2}{1-x^2} \\implies w_2^2(1-x^2) = w_1^2 x^2 \\implies w_2^2 = (w_1^2 + w_2^2)x^2\n$$\nThis gives $x^2 = \\frac{w_2^2}{w_1^2 + w_2^2}$, and since $x>0$, the unique critical point is $x_c = \\frac{w_2}{\\sqrt{w_1^2 + w_2^2}}$. For any $w_1, w_2 > 0$, we have $0 < x_c < 1$.\n\nTo determine if this critical point is a minimum or a maximum, we compute the second derivative:\n$$\ng_w''(x) = \\frac{d}{dx} \\left( -w_1 x (1 - x^2)^{-1/2} + w_2 \\right)\n$$\nUsing the quotient rule on the first term:\n$$\ng_w''(x) = -w_1 \\left[ \\frac{(1)\\sqrt{1-x^2} - x(-x/\\sqrt{1-x^2})}{1-x^2} \\right] = -w_1 \\left[ \\frac{\\sqrt{1-x^2} + x^2/\\sqrt{1-x^2}}{1-x^2} \\right]\n$$\nMultiplying the numerator and denominator by $\\sqrt{1-x^2}$:\n$$\ng_w''(x) = -w_1 \\left[ \\frac{(1-x^2) + x^2}{(1-x^2)^{3/2}} \\right] = -\\frac{w_1}{(1-x^2)^{3/2}}\n$$\nFor any $x \\in (0,1)$, the denominator $(1-x^2)^{3/2}$ is positive. Since $w_1 > 0$, it follows that $g_w''(x) < 0$ for all $x \\in (0,1)$. A negative second derivative indicates that the function $g_w(x)$ is strictly concave on the interval $(0,1)$.\n\nFor a continuous function on a closed interval that is strictly concave on the interior, its minimum value must be attained at one of the endpoints of the interval. Therefore, the minimizer of $g_w(x)$ over $[0,1]$ must be either $x=0$ or $x=1$.\n\nThis means that for any pair of strictly positive weights $(w_1, w_2)$, the weighted sum method will only ever identify the solutions $x=0$ (objective vector $(1,0)$) or $x=1$ (objective vector $(0,1)$) as optimal. The efficient solutions corresponding to $x \\in (0,1)$ cannot be found by this method. Such points are known as unsupported efficient points, because they do not lie on the boundary of the convex hull of the feasible objective set.\n\n**3. Identification of an Unsupported Efficient Point**\n\nFrom the analysis above, any solution $x^{\\star} \\in (0,1)$ is an unsupported efficient point. We must identify one such point and its corresponding objective vector. A convenient choice is $x^{\\star} = \\frac{1}{\\sqrt{2}}$. This value is clearly in the interval $(0,1)$.\n\nWe now compute the objective vector $(f_1(x^{\\star}), f_2(x^{\\star}))$ for this choice of $x^{\\star}$:\n$$\nf_1(x^{\\star}) = f_1\\left(\\frac{1}{\\sqrt{2}}\\right) = \\sqrt{1 - \\left(\\frac{1}{\\sqrt{2}}\\right)^2} = \\sqrt{1 - \\frac{1}{2}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}\n$$\n$$\nf_2(x^{\\star}) = f_2\\left(\\frac{1}{\\sqrt{2}}\\right) = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}\n$$\nThe explicit unsupported efficient point corresponds to the objective vector $\\left(\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2} \\end{pmatrix}}$$", "id": "3154202"}]}