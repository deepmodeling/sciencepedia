{"hands_on_practices": [{"introduction": "Real-world systems often exhibit time delays, which can complicate controller design. This exercise [@problem_id:3121144] demonstrates how to handle this common challenge by applying the powerful Linear Quadratic Regulator (LQR) framework to a system with an input delay. By constructing an augmented state vector, you will learn a fundamental technique to transform the problem into a standard form and solve it using the discrete-time Riccati recursion.", "problem": "Consider the discrete-time linear system with one-step input delay given by $x_{k+1} = a x_{k} + b u_{k-1}$, where $x_{k} \\in \\mathbb{R}$ is the state and $u_{k} \\in \\mathbb{R}$ is the control input. The delay is $d=1$. You are asked to convert this delayed system into an equivalent non-delayed augmented system and derive the optimal control via the Riccati recursion.\n\nLet the finite-horizon performance index be\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f},$$\nwith nonnegative scalars $q$, $r$, and $q_{f}$, and horizon length $N \\in \\mathbb{N}$. Assume the optimal control is computed using the Linear Quadratic Regulator (LQR), derived by the principle of dynamic programming.\n\nTasks:\n1. Introduce the augmented state $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$ and write the non-delayed augmented dynamics in the form $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$, identifying $\\bar{A}$ and $\\bar{B}$ in terms of $a$ and $b$.\n2. Starting from the dynamic programming principle, show that the optimal cost-to-go is quadratic in the augmented state, $J_{k}^{\\star}(z_{k}) = z_{k}^{\\top} P_{k} z_{k}$, and derive the Riccati recursion for $P_{k}$ together with the corresponding optimal state-feedback control law $u_{k}^{\\star} = -K_{k} z_{k}$ in terms of $\\bar{A}$, $\\bar{B}$, $Q_{z}$, and $R$, where $Q_{z}$ is the augmented-state penalty matrix consistent with the given performance index.\n3. For the specific system parameters $a = 2$, $b = 1$, $q = 1$, $r = 1$, $q_{f} = 1$, horizon $N = 2$, and initial conditions $x_{0} = 2$, $u_{-1} = 1$, compute the optimal control input $u_{0}^{\\star}$. Your final answer must be a single real number. No rounding is required.", "solution": "The problem statement is a well-defined and standard problem in discrete-time optimal control theory. It asks for the derivation and application of the Linear Quadratic Regulator (LQR) for a system with a single-step input delay. The method of state augmentation is a standard technique for handling such delays. The problem is scientifically grounded, self-contained, and all necessary parameters for a unique solution are provided. Therefore, the problem is valid.\n\n### Task 1: Augmented System Formulation\n\nThe given discrete-time linear system with input delay is:\n$$x_{k+1} = a x_{k} + b u_{k-1}$$\nwhere $x_k \\in \\mathbb{R}$ is the state and $u_k \\in \\mathbb{R}$ is the control input at time step $k$. The augmented state vector is defined as $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$.\n\nWe want to find the augmented system dynamics in the form $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$. Let's construct the state vector at time $k+1$:\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{(k+1)-1} \\end{pmatrix} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix}$$\nThe first component, $x_{k+1}$, is given by the system dynamics: $x_{k+1} = a x_{k} + b u_{k-1}$. In terms of the components of $z_k$, this is $x_{k+1} = a x_{k} + b u_{k-1} + 0 \\cdot u_k$.\nThe second component, $u_k$, can be written as a trivial identity: $u_k = 0 \\cdot x_k + 0 \\cdot u_{k-1} + 1 \\cdot u_k$.\n\nCombining these two equations in matrix form, we have:\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} u_{k}$$\nThis matches the desired form $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ with the augmented system matrices:\n$$\\bar{A} = \\begin{pmatrix} a & b \\\\ 0 & 0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n\n### Task 2: Riccati Recursion Derivation\n\nThe performance index is given by:\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f}$$\nWe rewrite this cost function in terms of the augmented state $z_k$.\nThe stage cost is $L(x_k, u_k) = x_k^2 q + u_k^2 r$.\nSince $x_k = \\begin{pmatrix} 1 & 0 \\end{pmatrix} z_k$, we have $x_k^2 q = \\left(\\begin{pmatrix} 1 & 0 \\end{pmatrix} z_k\\right)^2 q = z_k^\\top \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} q \\begin{pmatrix} 1 & 0 \\end{pmatrix} z_k = z_k^\\top \\begin{pmatrix} q & 0 \\\\ 0 & 0 \\end{pmatrix} z_k$.\nThus, the stage cost can be written as $L(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k$, where $Q_z = \\begin{pmatrix} q & 0 \\\\ 0 & 0 \\end{pmatrix}$ and $R = r$.\n\nThe terminal cost is $x_N^2 q_f$. Similarly, this becomes $z_N^\\top P_N z_N$ with $P_N = \\begin{pmatrix} q_f & 0 \\\\ 0 & 0 \\end{pmatrix}$.\nThe augmented LQR problem is to minimize $J = \\sum_{k=0}^{N-1} (z_k^\\top Q_z z_k + u_k^\\top R u_k) + z_N^\\top P_N z_N$ subject to $z_{k+1} = \\bar{A} z_k + \\bar{B} u_k$.\n\nWe use dynamic programming. Let $J_k^\\star(z_k)$ be the optimal cost-to-go from state $z_k$ at time $k$. The Bellman equation is:\n$$J_k^\\star(z_k) = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + J_{k+1}^\\star(z_{k+1}) \\right]$$\nWe hypothesize a quadratic form for the cost-to-go: $J_k^\\star(z_k) = z_k^\\top P_k z_k$, where $P_k$ is a symmetric positive semi-definite matrix. The terminal condition is $J_N^\\star(z_N) = z_N^\\top P_N z_N$, so we have $P_N = \\begin{pmatrix} q_f & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n\nSubstituting into the Bellman equation:\n$$z_k^\\top P_k z_k = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + (\\bar{A} z_k + \\bar{B} u_k)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k) \\right]$$\nLet's expand the term inside the minimization, which we call $\\mathcal{L}_k(z_k, u_k)$:\n$$\\mathcal{L}_k(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k + z_k^\\top \\bar{A}^\\top P_{k+1} \\bar{A} z_k + 2 u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{A} z_k + u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{B} u_k$$\nTo find the optimal control $u_k^\\star$, we take the gradient of $\\mathcal{L}_k$ with respect to $u_k$ and set it to zero:\n$$\\frac{\\partial \\mathcal{L}_k}{\\partial u_k} = 2 R u_k + 2 \\bar{B}^\\top P_{k+1} \\bar{A} z_k + 2 \\bar{B}^\\top P_{k+1} \\bar{B} u_k = 0$$\nSolving for $u_k$:\n$$(R + \\bar{B}^\\top P_{k+1} \\bar{B}) u_k = - \\bar{B}^\\top P_{k+1} \\bar{A} z_k$$\n$$u_k^\\star = - (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A}) z_k$$\nThis is a state-feedback control law $u_k^\\star = -K_k z_k$, with the time-varying gain matrix:\n$$K_k = (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} \\bar{B}^\\top P_{k+1} \\bar{A}$$\nSubstituting $u_k^\\star$ back into the Bellman equation yields the Riccati recursion for $P_k$:\n$$z_k^\\top P_k z_k = z_k^\\top Q_z z_k + (u_k^\\star)^\\top R u_k^\\star + (\\bar{A} z_k + \\bar{B} u_k^\\star)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k^\\star)$$\nAfter algebraic manipulation, this simplifies to:\n$$P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - (\\bar{A}^\\top P_{k+1} \\bar{B}) (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A})$$\nThis equation is solved backwards in time from the terminal condition $P_N$. An equivalent form is $P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - K_k^\\top (R + \\bar{B}^\\top P_{k+1} \\bar{B}) K_k$.\n\n### Task 3: Numerical Computation\n\nThe given parameters are $a = 2$, $b = 1$, $q = 1$, $r = 1$, $q_f = 1$, and $N = 2$.\nThe augmented system matrices are:\n$$\\bar{A} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nThe cost matrices are:\n$$Q_z = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad R = 1$$\nWe need to find $u_0^\\star$, which requires computing $K_0$. This requires $P_1$, which in turn is computed from $P_2$.\n\n**Step 1: Compute $P_2$ (at $k=N=2$)**\nThe terminal cost matrix is:\n$$P_2 = P_N = \\begin{pmatrix} q_f & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$$\n\n**Step 2: Compute $P_1$ (at $k=N-1=1$)**\nWe use the Riccati recursion for $k=1$:\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} - (\\bar{A}^\\top P_2 \\bar{B}) (R + \\bar{B}^\\top P_2 \\bar{B})^{-1} (\\bar{B}^\\top P_2 \\bar{A})$$\nLet's compute the terms:\n$$\\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 2 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\\\ 2 & 1 \\end{pmatrix}$$\n$$\\bar{B}^\\top P_2 \\bar{B} = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0$$\n$$\\bar{A}^\\top P_2 \\bar{B} = \\begin{pmatrix} 2 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe correction term is zero. Therefore:\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 4 & 2 \\\\ 2 & 1 \\end{pmatrix} = \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix}$$\n\n**Step 3: Compute $K_0$ (at $k=0$)**\nThe gain $K_0$ is given by:\n$$K_0 = (R + \\bar{B}^\\top P_1 \\bar{B})^{-1} \\bar{B}^\\top P_1 \\bar{A}$$\nLet's compute the terms:\n$$\\bar{B}^\\top P_1 \\bar{B} = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1$$\n$$R + \\bar{B}^\\top P_1 \\bar{B} = 1 + 1 = 2$$\n$$\\bar{B}^\\top P_1 \\bar{A} = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\end{pmatrix}$$\nNow, we can compute $K_0$:\n$$K_0 = (2)^{-1} \\begin{pmatrix} 4 & 2 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix}$$\n\n**Step 4: Compute the optimal control $u_0^\\star$**\nThe optimal control is $u_0^\\star = -K_0 z_0$.\nThe initial conditions are $x_0 = 2$ and $u_{-1} = 1$. The initial augmented state is:\n$$z_0 = \\begin{pmatrix} x_0 \\\\ u_{-1} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nFinally, we compute $u_0^\\star$:\n$$u_0^\\star = -K_0 z_0 = -\\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -(2 \\cdot 2 + 1 \\cdot 1) = -(4+1) = -5$$\nThe optimal control input at $k=0$ is $-5$.", "answer": "$$\\boxed{-5}$$", "id": "3121144"}, {"introduction": "While the LQR framework provides an optimal controller, it's crucial to understand what \"optimal\" truly means in a practical context. This practice [@problem_id:3121205] challenges you to think critically about design choices by comparing the LQR approach with the direct method of eigenvalue assignment. You will analyze the trade-offs between minimizing a quadratic cost, regulating state variance, and ensuring robustness to parameter uncertainty, revealing that different design goals can lead to different controllers.", "problem": "Consider the scalar discrete-time linear system with process noise\n$$x_{k+1} = a\\,x_k + b\\,u_k + w_k,$$\nwhere $a=1.2$, $b=1$, and $\\{w_k\\}$ is a zero-mean independent and identically distributed process with variance $\\sigma_w^2=0.04$. The state $x_k$ is perfectly measured. You will compare two static state-feedback designs of the form $u_k=-K\\,x_k$.\n\nController EA (eigenvalue assignment): choose $K$ to place the nominal closed-loop eigenvalue at $\\lambda_d=0.3$.\n\nController LQR (Linear Quadratic Regulator): choose $K$ to minimize the infinite-horizon average quadratic cost\n$$J=\\lim_{N\\to\\infty}\\frac{1}{N}\\,\\mathbb{E}\\left[\\sum_{k=0}^{N-1}\\left(Q\\,x_k^2 + R\\,u_k^2\\right)\\right]$$\nfor the nominal system with $Q=1$ and $R=2$.\n\nAdditionally, consider an additive parametric uncertainty in $a$ of the form $\\tilde a = a + \\delta$ with $|\\delta|\\le \\Delta$. Closed-loop stability in discrete time means that the magnitude of the closed-loop eigenvalue is strictly less than $1$.\n\nWhich of the following statements is correct for this system and data?\n\nA. For the given data, the Linear Quadratic Regulator (LQR) feedback achieves a lower steady-state average cost than the eigenvalue-assignment controller, but it tolerates a smaller maximum additive uncertainty $|\\delta|\\le \\Delta$ in $a$ while preserving closed-loop stability.\n\nB. Because both designs stabilize the system, their steady-state average costs are equal for the given $Q$ and $R$.\n\nC. Under the stated noise, the steady-state variance of the state $x_k$ is larger for eigenvalue assignment than for Linear Quadratic Regulator (LQR).\n\nD. The eigenvalue-assignment controller guarantees a smaller robust stability margin to additive uncertainty in $a$ than the Linear Quadratic Regulator (LQR).", "solution": "The user wants me to act as a meticulous STEM professor and solve a problem in the field of discrete-time optimal control. I am required to first validate the problem statement and then, if it is valid, proceed to a detailed solution and evaluation of the given options.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified procedure.\n\n**Step 1: Extract Givens**\n\n*   System dynamics: $x_{k+1} = a\\,x_k + b\\,u_k + w_k$.\n*   System parameters: $a=1.2$, $b=1$.\n*   Process noise: $\\{w_k\\}$ is a zero-mean independent and identically distributed (i.i.d.) process with variance $\\sigma_w^2=0.04$.\n*   State measurement: $x_k$ is perfectly measured.\n*   Control law: $u_k=-K\\,x_k$.\n*   Controller EA (eigenvalue assignment): $K$ is chosen to place the nominal closed-loop eigenvalue at $\\lambda_d=0.3$.\n*   Controller LQR (Linear Quadratic Regulator): $K$ is chosen to minimize the infinite-horizon average quadratic cost $J=\\lim_{N\\to\\infty}\\frac{1}{N}\\,\\mathbb{E}\\left[\\sum_{k=0}^{N-1}\\left(Q\\,x_k^2 + R\\,u_k^2\\right)\\right]$ for the nominal system.\n*   LQR cost weights: $Q=1$, $R=2$.\n*   Parametric uncertainty: $\\tilde a = a + \\delta$ with $|\\delta|\\le \\Delta$.\n*   Stability condition: For discrete time, the magnitude of the closed-loop eigenvalue must be strictly less than $1$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding:** The problem is a standard exercise in modern control theory, involving the comparison of two fundamental controller design techniques: pole placement (eigenvalue assignment) and Linear Quadratic Regulator (LQR). The concepts of state-space representation, feedback control, stochastic systems, cost functions, and robust stability are all well-established principles in engineering and applied mathematics. The problem is scientifically sound.\n2.  **Well-Posed:** The problem provides all necessary numerical values ($a$, $b$, $\\sigma_w^2$), design specifications ($\\lambda_d$), and cost function parameters ($Q$, $R$) to uniquely determine the feedback gains $K$ for both controllers. The metrics for comparison (average cost, state variance, stability margin) are clearly defined and calculable. The problem is well-posed.\n3.  **Objective:** The problem statement is formulated using precise, unambiguous mathematical language and standard terminology from control theory. It is free from subjective or opinion-based content.\n4.  **Flaw Checklist:**\n    *   The problem does not violate any scientific principles.\n    *   It is a formalizable problem central to the topic of optimal control.\n    *   The setup is complete and consistent; the open-loop system is unstable ($|a|=1.2 > 1$), which is a valid and common scenario for control design.\n    *   The problem is purely mathematical, so physical infeasibility is not applicable.\n    *   A unique and meaningful solution can be derived.\n    *   The problem requires non-trivial calculations and an understanding of control concepts, so it is not trivial or tautological.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed with the detailed derivation and analysis.\n\n### Solution Derivation\n\nFirst, we determine the closed-loop system dynamics. Substituting the control law $u_k = -K x_k$ into the system equation yields:\n$$x_{k+1} = a x_k + b(-K x_k) + w_k = (a - bK) x_k + w_k$$\nThe nominal closed-loop eigenvalue is $\\lambda_{cl} = a - bK$. With the given parameters $a=1.2$ and $b=1$, this becomes $\\lambda_{cl} = 1.2 - K$.\n\n**Controller EA (Eigenvalue Assignment)**\n\nThe goal is to place the closed-loop eigenvalue at $\\lambda_d = 0.3$.\n$$\\lambda_{EA} = a - b K_{EA} = 1.2 - (1)K_{EA} = 0.3$$\nSolving for the feedback gain $K_{EA}$:\n$$K_{EA} = 1.2 - 0.3 = 0.9$$\n\n**Controller LQR (Linear Quadratic Regulator)**\n\nThe LQR gain $K_{LQR}$ minimizes the given cost function. For the scalar discrete-time system, the gain is found via the solution $P$ to the Discrete Algebraic Riccati Equation (DARE). The DARE is:\n$$P = a^T P a - (a^T P b)(R + b^T P b)^{-1}(b^T P a) + Q$$\nSubstituting the scalar values $a=1.2$, $b=1$, $Q=1$, and $R=2$:\n$$P = (1.2)^2 P - ((1.2)P(1))(2 + (1)^2 P)^{-1}((1)P(1.2)) + 1$$\n$$P = 1.44 P - \\frac{1.44 P^2}{2+P} + 1$$\nWe must solve for $P$. Since the pair $(a, \\sqrt{Q})$ is detectable (here, $a=1.2 \\ne 0$) and the pair $(a, b)$ is stabilizable (here, $a=1.2, b=1$ is controllable, which is stronger), a unique positive definite solution $P > 0$ exists.\n$$P - 1 = 1.44 P - \\frac{1.44 P^2}{2+P}$$\n$$(P - 1)(2+P) = 1.44 P (2+P) - 1.44 P^2$$\n$$2P + P^2 - 2 - P = 2.88 P + 1.44 P^2 - 1.44 P^2$$\n$$P^2 + P - 2 = 2.88 P$$\n$$P^2 - 1.88 P - 2 = 0$$\nUsing the quadratic formula, $P = \\frac{-(-1.88) \\pm \\sqrt{(-1.88)^2 - 4(1)(-2)}}{2(1)}$:\n$$P = \\frac{1.88 \\pm \\sqrt{3.5344 + 8}}{2} = \\frac{1.88 \\pm \\sqrt{11.5344}}{2}$$\nAs $P$ must be positive, we take the positive root:\n$$P \\approx \\frac{1.88 + 3.39623}{2} = \\frac{5.27623}{2} \\approx 2.6381$$\nThe LQR gain $K_{LQR}$ is given by:\n$$K_{LQR} = (R + b^T P b)^{-1}(b^T P a) = \\frac{abP}{R+b^2P}$$\n$$K_{LQR} \\approx \\frac{(1.2)(1)(2.6381)}{2+(1)^2(2.6381)} = \\frac{3.1657}{4.6381} \\approx 0.6826$$\nThe corresponding closed-loop eigenvalue for the LQR controller is:\n$$\\lambda_{LQR} = a - b K_{LQR} \\approx 1.2 - (1)(0.6826) = 0.5174$$\n\nNow we can analyze the properties of the two designs.\n\n**1. Steady-State Average Cost ($J$) and State Variance ($\\text{Var}(x_k)$)**\n\nThe closed-loop system is $x_{k+1} = \\lambda_{cl} x_k + w_k$. At steady state, let the state variance be $V_x = \\mathbb{E}[x_k^2]$ (since $\\mathbb{E}[x_k]=0$).\n$$V_x = \\text{Var}(x_{k+1}) = \\text{Var}(\\lambda_{cl} x_k + w_k)$$\nSince $x_k$ depends on past noise terms $w_{k-1}, w_{k-2}, \\dots$, it is uncorrelated with the current noise $w_k$. Thus:\n$$V_x = \\lambda_{cl}^2 \\text{Var}(x_k) + \\text{Var}(w_k) = \\lambda_{cl}^2 V_x + \\sigma_w^2$$\n$$V_x(1 - \\lambda_{cl}^2) = \\sigma_w^2 \\implies V_x = \\frac{\\sigma_w^2}{1 - \\lambda_{cl}^2}$$\nThe steady-state average cost is $J = \\mathbb{E}[Q x_k^2 + R u_k^2] = \\mathbb{E}[Q x_k^2 + R(-K x_k)^2]$.\n$$J = (Q + R K^2) \\mathbb{E}[x_k^2] = (Q + R K^2) V_x = (Q + R K^2) \\frac{\\sigma_w^2}{1 - \\lambda_{cl}^2}$$\n\n*   **For Controller EA:**\n    *   $K_{EA} = 0.9$, $\\lambda_{EA} = 0.3$.\n    *   State variance: $V_{x,EA} = \\frac{0.04}{1 - (0.3)^2} = \\frac{0.04}{0.91} \\approx 0.04396$.\n    *   Average cost: $J_{EA} = (1 + 2(0.9)^2) V_{x,EA} = (1 + 1.62) (0.04396) = 2.62 \\times 0.04396 \\approx 0.1152$.\n\n*   **For Controller LQR:**\n    *   $K_{LQR} \\approx 0.6826$, $\\lambda_{LQR} \\approx 0.5174$.\n    *   State variance: $V_{x,LQR} = \\frac{0.04}{1 - (0.5174)^2} \\approx \\frac{0.04}{1 - 0.2677} = \\frac{0.04}{0.7323} \\approx 0.05462$.\n    *   Average cost: $J_{LQR} = (1 + 2(0.6826)^2) V_{x,LQR} \\approx (1 + 2(0.466)) (0.05462) = 1.932 \\times 0.05462 \\approx 0.1055$.\n\n**Comparison:**\n*   Cost: $J_{LQR} \\approx 0.1055 < J_{EA} \\approx 0.1152$. The LQR controller yields a lower cost, as expected by definition.\n*   Variance: $V_{x,LQR} \\approx 0.05462 > V_{x,EA} \\approx 0.04396$. The EA controller results in a smaller steady-state state variance.\n\n**2. Robust Stability Margin ($\\Delta$)**\n\nThe perturbed closed-loop eigenvalue is $\\lambda_{pert} = \\tilde{a} - bK = (a+\\delta) - bK = (a-bK) + \\delta = \\lambda_{cl} + \\delta$.\nFor stability, we require $|\\lambda_{pert}| < 1$, which means $|\\lambda_{cl} + \\delta| < 1$.\nThis expands to $-1 < \\lambda_{cl} + \\delta < 1$, or $-1 - \\lambda_{cl} < \\delta < 1 - \\lambda_{cl}$.\nThe controller is robustly stable for all $|\\delta| \\le \\Delta$ if the interval $[-\\Delta, \\Delta]$ is contained within $(-1 - \\lambda_{cl}, 1 - \\lambda_{cl})$. This requires:\n$$ \\Delta < 1 - \\lambda_{cl} \\quad \\text{and} \\quad \\Delta < 1 + \\lambda_{cl}$$\nThe maximum tolerable uncertainty is therefore $\\Delta_{max} = \\min(1-\\lambda_{cl}, 1+\\lambda_{cl})$.\nSince both $\\lambda_{EA}$ and $\\lambda_{LQR}$ are positive, the limit is governed by $1 - \\lambda_{cl}$.\n*   **For Controller EA:** $\\Delta_{max,EA} = 1 - \\lambda_{EA} = 1 - 0.3 = 0.7$.\n*   **For Controller LQR:** $\\Delta_{max,LQR} \\approx 1 - \\lambda_{LQR} = 1 - 0.5174 = 0.4826$.\n\n**Comparison:**\n*   Stability Margin: $\\Delta_{max,LQR} \\approx 0.4826 < \\Delta_{max,EA} = 0.7$. The LQR controller tolerates a smaller uncertainty range in the parameter $a$.\n\n### Option-by-Option Analysis\n\n**A. For the given data, the Linear Quadratic Regulator (LQR) feedback achieves a lower steady-state average cost than the eigenvalue-assignment controller, but it tolerates a smaller maximum additive uncertainty $|\\delta|\\le \\Delta$ in $a$ while preserving closed-loop stability.**\n*   Our calculations show $J_{LQR} \\approx 0.1055$ and $J_{EA} \\approx 0.1152$, so $J_{LQR} < J_{EA}$. This part is true.\n*   Our calculations show $\\Delta_{max,LQR} \\approx 0.4826$ and $\\Delta_{max,EA} = 0.7$, so the LQR controller tolerates a smaller maximum uncertainty. This part is also true.\n*   The entire statement is therefore correct.\n**Verdict: Correct**\n\n**B. Because both designs stabilize the system, their steady-state average costs are equal for the given $Q$ and $R$.**\n*   Our calculations show $J_{LQR} \\approx 0.1055$ and $J_{EA} \\approx 0.1152$. These costs are not equal. The cost depends not just on stability but on the specific closed-loop dynamics and control effort.\n**Verdict: Incorrect**\n\n**C. Under the stated noise, the steady-state variance of the state $x_k$ is larger for eigenvalue assignment than for Linear Quadratic Regulator (LQR).**\n*   Our calculations show $V_{x,EA} \\approx 0.04396$ and $V_{x,LQR} \\approx 0.05462$.\n*   The variance for eigenvalue assignment is *smaller* than for LQR ($V_{x,EA} < V_{x,LQR}$). The statement claims the opposite.\n**Verdict: Incorrect**\n\n**D. The eigenvalue-assignment controller guarantees a smaller robust stability margin to additive uncertainty in $a$ than the Linear Quadratic Regulator (LQR).**\n*   Our calculations show $\\Delta_{max,EA} = 0.7$ and $\\Delta_{max,LQR} \\approx 0.4826$.\n*   The margin for the eigenvalue-assignment controller is *larger* than for the LQR controller. The statement claims the opposite.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3121205"}, {"introduction": "Many practical optimization problems feature costs that are more complex than the simple quadratic functions used in standard LQR. This hands-on coding exercise [@problem_id:3121214] guides you through solving an optimal control problem with a non-convex cost, a scenario where analytical solutions are not readily available. By implementing the dynamic programming algorithm, you will directly experience its power to find a globally optimal policy by planning ahead, successfully avoiding the \"local minimum traps\" that simpler, myopic strategies often fall into.", "problem": "Consider a deterministic, finite-horizon, discrete-time optimal control problem with scalar state and control. The system evolves according to the dynamics $x_{k+1} = x_k + u_k$ for integer time indices $k = 0, 1, \\dots, N-1$, with given initial state $x_0 \\in \\mathbb{R}$ and control $u_k$ constrained to a fixed, finite grid of admissible values $\\mathcal{U} \\subset \\mathbb{R}$. The stage cost is $c(x_k,u_k) = q \\, x_k^2 + \\ell(u_k)$, where the nonconvex control cost is\n$$\n\\ell(u) = \\min\\{(u - a)^2,\\,(u + b)^2\\},\n$$\nwith parameters $a > 0$ and $b > 0$. The terminal cost is $\\phi(x_N) = p \\, x_N^2$, with $p > 0$. The total cost to be minimized is the sum of stage costs and the terminal cost over the horizon.\n\nStarting only from the definition of the control problem and the principle of optimality for dynamic programming, derive how to compute the optimal cost-to-go and policy. Implement a complete algorithm that:\n- Constructs a uniform grid $\\mathcal{X}$ over the state space sufficient to contain all reachable states from $x_0$ under $\\mathcal{U}$ across the horizon $N$.\n- Computes the optimal policy over the state grid by backward dynamic programming and applies it forward from $x_0$ to produce the realized optimal control sequence $(u_0^{\\star},\\dots,u_{N-1}^{\\star})$ and state trajectory $(x_0,\\dots,x_N)$.\n- Computes the realized optimal total cost $J^{\\star}$ along this trajectory.\n- Computes a greedy myopic policy that, at each time step, chooses a control $u_k$ minimizing $\\ell(u_k)$ alone (ties must be broken by choosing the control with the smallest absolute value, and if still tied, the smallest numerical value), and applies it forward from $x_0$ to produce the greedy total cost $J^{\\mathrm{greedy}}$.\n- Identifies policy switching by counting how many times the chosen active branch of $\\ell(u)$ changes along the realized optimal control sequence. For a control value $u$, define the active branch index as $0$ if $(u-a)^2 \\le (u+b)^2$ and $1$ otherwise. The number of switching events is the count of indices $k$ where the branch index changes from $k-1$ to $k$.\n- Detects a local-minimum trap indicator by the following rule: return a boolean that is true if and only if the greedy policy uses a single branch for all time steps (no switching), the dynamic programming policy has at least one switch, and $J^{\\mathrm{greedy}} > J^{\\star}$.\n\nYour implementation must treat state evolution on the grid by mapping $x_{k+1}$ to the nearest state grid point. If $x_{k+1}$ is outside the grid, it must be clipped to the nearest boundary grid point. You must define a tie-breaking rule for dynamic programming when multiple controls attain the same minimal value: choose the control with the smallest absolute value, and if still tied, the smallest numerical value.\n\nTest Suite. For each of the following parameter sets, run the algorithm and collect the four-tuple of results for that test case: $[J^{\\star},\\,S^{\\star},\\,\\text{is\\_greedy\\_suboptimal},\\,\\text{is\\_trap}]$, where $J^{\\star}$ is the realized optimal total cost as a float, $S^{\\star}$ is the integer number of branch switches along the realized optimal policy, $\\text{is\\_greedy\\_suboptimal}$ is a boolean indicating whether $J^{\\mathrm{greedy}} > J^{\\star}$, and $\\text{is\\_trap}$ is the boolean local-minimum trap indicator defined above. Use a state-grid spacing of $\\Delta x = 0.25$.\n\n- Case 1 (general case): $N = 6$, $q = 1.0$, $p = 5.0$, $a = 1.0$, $b = 1.0$, $x_0 = 2.5$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$.\n- Case 2 (boundary horizon): $N = 1$, $q = 1.0$, $p = 2.0$, $a = 1.0$, $b = 1.0$, $x_0 = -0.75$, $\\mathcal{U} = \\{-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0\\}$.\n- Case 3 (asymmetric wells): $N = 5$, $q = 0.5$, $p = 10.0$, $a = 2.0$, $b = 0.5$, $x_0 = -3.0$, $\\mathcal{U} = \\{-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0\\}$.\n- Case 4 (strong terminal penalty): $N = 8$, $q = 0.1$, $p = 20.0$, $a = 1.5$, $b = 1.5$, $x_0 = 3.0$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$.\n\nFinal Output Format. Your program should produce a single line of output containing the results for all test cases aggregated as a comma-separated list enclosed in square brackets, where each element itself is the comma-separated four-tuple for a test case enclosed in square brackets. For example:\n$[ [J_1^{\\star},S_1^{\\star},\\text{flag}_{1},\\text{flag}_{1}], [J_2^{\\star},S_2^{\\star},\\text{flag}_{2},\\text{flag}_{2}], \\dots ]$.\nNo physical units or angle units are involved. All booleans must be printed as capitalized Python booleans ($\\text{True}$ or $\\text{False}$). All floats and integers must be printed in standard Python notation.", "solution": "The user-provided problem is a valid deterministic, finite-horizon, discrete-time optimal control problem. It is well-posed, scientifically grounded in the principles of optimal control theory, and all parameters, constraints, and objectives are clearly defined. The core challenge lies in the non-convex nature of the control cost function $\\ell(u)$, which makes dynamic programming (DP) a suitable solution method. The solution will be derived from the Principle of Optimality.\n\n**1. Problem Formulation and Principle of Optimality**\n\nThe objective is to minimize the total cost function $J$ given by:\n$$J(x_0, u_0, \\dots, u_{N-1}) = \\sum_{k=0}^{N-1} c(x_k, u_k) + \\phi(x_N)$$\nwhere the stage cost is $c(x_k, u_k) = qx_k^2 + \\ell(u_k)$ with $\\ell(u) = \\min\\{(u - a)^2, (u + b)^2\\}$, and the terminal cost is $\\phi(x_N) = px_N^2$. The system dynamics are $x_{k+1} = x_k + u_k$, with a given initial state $x_0$. The control $u_k$ must be chosen from a finite set of admissible controls $\\mathcal{U}$.\n\nAccording to Bellman's Principle of Optimality, an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. This principle gives rise to the dynamic programming recursion.\n\nLet $J_k^{\\star}(x)$ be the optimal cost-to-go from state $x$ at time $k$ to the end of the horizon. The recursion begins at the terminal time $N$:\n$$J_N^{\\star}(x) = \\phi(x) = p x^2$$\nFor the preceding time steps, $k = N-1, N-2, \\dots, 0$, the Bellman equation relates the cost-to-go at time $k$ to the cost-to-go at time $k+1$:\n$$J_k^{\\star}(x) = \\min_{u \\in \\mathcal{U}} \\left\\{ c(x, u) + J_{k+1}^{\\star}(x + u) \\right\\}$$\nThe term inside the minimum represents the cost incurred by choosing control $u$ at state $x$ and time $k$, and then proceeding optimally thereafter.\n\n**2. Numerical Solution via Discretization**\n\nSince the state $x$ is continuous, we cannot directly tabulate $J_k^{\\star}(x)$. The problem specifies a numerical approach using state-space discretization.\n\nFirst, we construct a uniform state grid $\\mathcal{X}$ that is guaranteed to contain all possible trajectories. The minimum and maximum reachable states are $x_{\\min} = x_0 + N \\cdot \\min(\\mathcal{U})$ and $x_{\\max} = x_0 + N \\cdot \\max(\\mathcal{U})$, respectively. The grid $\\mathcal{X}$ is constructed over the interval $[x_{\\min}, x_{\\max}]$ with a specified spacing $\\Delta x$. Let the grid points be $\\{x_i\\}$.\n\nThe DP algorithm then proceeds backward in time on this grid:\n\n**Step 1: Backward Pass (Policy and Value Function Computation)**\n- **Initialization ($k=N$):** For each state $x_i \\in \\mathcal{X}$, compute and store the terminal cost: $J_N(x_i) = p x_i^2$.\n- **Backward Iteration ($k=N-1, \\dots, 0$):** For each state $x_i \\in \\mathcal{X}$:\n  - For each admissible control $u_j \\in \\mathcal{U}$, calculate the cost-to-go for that specific choice:\n    1. Compute the next state: $x_{\\text{next}} = x_i + u_j$.\n    2. Map this next state to the grid. As per the problem, this involves clipping $x_{\\text{next}}$ to the grid boundaries and then finding the nearest grid point, let's call it $x'_{\\text{next}}$.\n    3. Retrieve the already computed optimal cost-to-go from this next grid state: $J_{k+1}(x'_{\\text{next}})$.\n    4. The total cost for this $(x_i, u_j)$ pair is $V(u_j) = q x_i^2 + \\ell(u_j) + J_{k+1}(x'_{\\text{next}})$.\n  - Find the minimum cost over all controls: $J_k(x_i) = \\min_{u_j \\in \\mathcal{U}} V(u_j)$.\n  - Identify the set of controls that achieve this minimum. Apply the specified tie-breaking rule (smallest absolute value, then smallest numerical value) to select the unique optimal control $\\mu_k^{\\star}(x_i)$.\n  - Store the optimal cost-to-go $J_k(x_i)$ and the optimal control $\\mu_k^{\\star}(x_i)$.\n\nAfter this pass, we have the optimal policy $\\mu_k^{\\star}(x_i)$ and the optimal value function $J_k(x_i)$ tabulated for all grid states $x_i$ and time steps $k$.\n\n**Step 2: Forward Pass (Trajectory Realization)**\nWith the optimal policy $\\mu_k^{\\star}$ computed, we simulate the system forward from the initial state $x_0$ to find the realized optimal trajectory and calculate the associated costs and metrics.\n\n- Initialize the state trajectory with $x_0^{\\star} = x_0$.\n- For $k=0, \\dots, N-1$:\n  1. Take the current true state $x_k^{\\star}$.\n  2. Map $x_k^{\\star}$ to its nearest grid point, $x'_k = \\text{nearest}(x_k^{\\star}, \\mathcal{X})$.\n  3. Look up the optimal control from the policy table: $u_k^{\\star} = \\mu_k^{\\star}(x'_k)$.\n  4. Evolve the true state: $x_{k+1}^{\\star} = x_k^{\\star} + u_k^{\\star}$.\n  5. Store $u_k^{\\star}$ and $x_{k+1}^{\\star}$.\n\n**3. Calculation of Required Metrics**\n\n- **Optimal Cost ($J^{\\star}$):** Using the realized optimal state trajectory $(x_0^{\\star}, \\dots, x_N^{\\star})$ and control sequence $(u_0^{\\star}, \\dots, u_{N-1}^{\\star})$, the total cost is calculated as:\n  $$J^{\\star} = \\sum_{k=0}^{N-1} (q(x_k^{\\star})^2 + \\ell(u_k^{\\star})) + p(x_N^{\\star})^2$$\n\n- **Greedy Policy Cost ($J^{\\mathrm{greedy}}$):** A myopic greedy policy is one that minimizes the immediate control cost $\\ell(u)$ at every step, ignoring the state cost and future consequences. The greedy control $u^{\\mathrm{greedy}}$ is found by $\\arg\\min_{u \\in \\mathcal{U}} \\ell(u)$, with ties broken as specified. Since $\\ell(u)$ is independent of state and time, this control is constant for all $k$. The greedy trajectory is simulated starting from $x_0$, and $J^{\\mathrm{greedy}}$ is calculated similarly.\n\n- **Policy Switching ($S^{\\star}$):** The control cost $\\ell(u) = \\min\\{(u-a)^2, (u+b)^2\\}$ has two branches. The active branch for a control $u$ is determined by whether $(u-a)^2 \\le (u+b)^2$. This simplifies to checking if $u \\ge (a-b)/2$. We define a branch index function:\n  $$B(u) = \\begin{cases} 0 & \\text{if } u \\ge (a-b)/2 \\\\ 1 & \\text{if } u < (a-b)/2 \\end{cases}$$\n  We apply this function to the optimal control sequence $u_0^{\\star}, \\dots, u_{N-1}^{\\star}$ to get a sequence of branch indices. The number of switches $S^{\\star}$ is the count of times the branch index changes between consecutive time steps.\n\n- **Trap Indicators:** The boolean flags `is_greedy_suboptimal` ($J^{\\mathrm{greedy}} > J^{\\star}$) and `is_trap` (greedy has no switches, DP has at least one, and greedy is suboptimal) are computed based on these results. The \"trap\" signifies a situation where a simple greedy approach gets stuck in a locally attractive control mode, while the optimal DP policy correctly plans ahead and switches modes to achieve a lower overall cost.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'N': 6, 'q': 1.0, 'p': 5.0, 'a': 1.0, 'b': 1.0, 'x0': 2.5, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n        {'N': 1, 'q': 1.0, 'p': 2.0, 'a': 1.0, 'b': 1.0, 'x0': -0.75, 'U': [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]},\n        {'N': 5, 'q': 0.5, 'p': 10.0, 'a': 2.0, 'b': 0.5, 'x0': -3.0, 'U': [-3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]},\n        {'N': 8, 'q': 0.1, 'p': 20.0, 'a': 1.5, 'b': 1.5, 'x0': 3.0, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n    ]\n    dx = 0.25\n\n    results = []\n    for params in test_cases:\n        result = _solve_case(_dx=dx, **params)\n        results.append(result)\n    \n    # Format the final output string\n    s_results = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    print(f\"[{','.join(s_results)}]\")\n\ndef _solve_case(N, q, p, a, b, x0, U, _dx):\n    \"\"\"\n    Solves a single optimal control problem instance.\n    \"\"\"\n\n    # --- Helper Functions ---\n    def l_cost(u, a_param, b_param):\n        return min((u - a_param)**2, (u + b_param)**2)\n\n    def branch_index(u, a_param, b_param):\n        # branch 0 if (u-a)^2 <= (u+b)^2, else 1. This simplifies to u >= (a-b)/2.\n        return 0 if u >= (a_param - b_param) / 2.0 else 1\n\n    def solve_tiebreak(controls):\n        # Sort by |u|, then by u\n        return sorted(controls, key=lambda u_val: (abs(u_val), u_val))[0]\n\n    # --- 1. State Grid Construction ---\n    u_min_val, u_max_val = min(U), max(U)\n    x_min_reach = x0 + N * u_min_val\n    x_max_reach = x0 + N * u_max_val\n    # Use arange; add a small tolerance to include the endpoint\n    X = np.arange(x_min_reach, x_max_reach + _dx / 2.0, _dx)\n    num_states = len(X)\n    x_grid_min_val, x_grid_max_val = X[0], X[-1]\n    \n    def map_to_index(x):\n        clipped_x = np.clip(x, x_grid_min_val, x_grid_max_val)\n        return np.argmin(np.abs(X - clipped_x))\n\n    # --- 2. Dynamic Programming Backward Pass ---\n    J = np.zeros((N + 1, num_states))\n    policy = np.zeros((N, num_states))\n\n    # Terminal cost\n    J[N, :] = p * X**2\n\n    # Iterate backwards from k = N-1 to 0\n    for k in range(N - 1, -1, -1):\n        for i in range(num_states):\n            x = X[i]\n            stage_cost_x = q * x**2\n            \n            costs_for_u = []\n            for u in U:\n                control_cost_l = l_cost(u, a, b)\n                x_next_raw = x + u\n                next_idx = map_to_index(x_next_raw)\n                cost_to_go_next = J[k + 1, next_idx]\n                total_cost_for_u = stage_cost_x + control_cost_l + cost_to_go_next\n                costs_for_u.append(total_cost_for_u)\n                \n            min_cost = min(costs_for_u)\n            J[k, i] = min_cost\n            \n            candidate_us = [u for u, cost in zip(U, costs_for_u) if np.isclose(cost, min_cost)]\n            policy[k, i] = solve_tiebreak(candidate_us)\n\n    # --- 3. Forward Pass (Optimal Trajectory) ---\n    x_traj_opt = np.zeros(N + 1)\n    u_traj_opt = np.zeros(N)\n    x_traj_opt[0] = x0\n\n    for k in range(N):\n        current_x = x_traj_opt[k]\n        current_idx = map_to_index(current_x)\n        u_star_k = policy[k, current_idx]\n        u_traj_opt[k] = u_star_k\n        x_traj_opt[k + 1] = current_x + u_star_k\n    \n    # --- 4. Calculate Optimal Cost (J_star) ---\n    J_star = 0.0\n    for k in range(N):\n        J_star += q * x_traj_opt[k]**2 + l_cost(u_traj_opt[k], a, b)\n    J_star += p * x_traj_opt[N]**2\n\n    # --- 5. Calculate Policy Switches (S_star) ---\n    branch_indices_opt = [branch_index(u, a, b) for u in u_traj_opt]\n    S_star = 0\n    if N > 0:\n        for k in range(1, N):\n            if branch_indices_opt[k] != branch_indices_opt[k-1]:\n                S_star += 1\n\n    # --- 6. Greedy Policy Simulation ---\n    l_vals = [l_cost(u, a, b) for u in U]\n    min_l = min(l_vals)\n    candidate_greedy_us = [u for u, l_val in zip(U, l_vals) if np.isclose(l_val, min_l)]\n    u_greedy = solve_tiebreak(candidate_greedy_us)\n\n    x_traj_greedy = np.zeros(N + 1)\n    x_traj_greedy[0] = x0\n    for k in range(N):\n        x_traj_greedy[k + 1] = x_traj_greedy[k] + u_greedy\n    \n    J_greedy = 0.0\n    for k in range(N):\n        J_greedy += q * x_traj_greedy[k]**2 + l_cost(u_greedy, a, b)\n    J_greedy += p * x_traj_greedy[N]**2\n        \n    # --- 7. Final Indicators ---\n    is_greedy_suboptimal = J_greedy > J_star\n    \n    # Greedy policy uses constant control, so it has 0 switches.\n    greedy_switches = 0\n    \n    # A trap occurs if greedy stays on one branch, DP switches, and DP is better.\n    is_trap = (greedy_switches == 0) and (S_star >= 1) and is_greedy_suboptimal\n\n    return [J_star, S_star, is_greedy_suboptimal, is_trap]\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3121214"}]}