{"hands_on_practices": [{"introduction": "The power of the Bellman equation rests on the Markov property, where future outcomes depend only on the present state. This exercise tackles a common scenario where this property appears to be violated due to a penalty on switching controls, which makes the cost at time $t$ dependent on the action at $t-1$. You will learn the fundamental technique of state augmentation—expanding the definition of the state to include necessary historical information—to restore the Markov property and correctly formulate the Bellman recursion. [@problem_id:3101515]", "problem": "Consider a deterministic, discrete-time control problem over a finite horizon with time indices $t \\in \\{0,1,2\\}$. The state is $x_t \\in \\mathbb{R}$ and the control is $u_t \\in \\mathcal{U}$, where $\\mathcal{U} = \\{-1, 0, 1\\}$. The dynamics are given by $x_{t+1} = x_t + u_t$. The stage cost at time $t \\in \\{0,1\\}$ is\n$$\n\\ell(x_t,u_t,u_{t-1}) = x_t^2 + u_t^2 + k \\,\\mathbf{1}\\{u_t \\neq u_{t-1}\\},\n$$\nwhere $k > 0$ is a given constant and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The terminal cost at time $t = 2$ is zero.\n\n- Explain why, with the switching penalty $k \\,\\mathbf{1}\\{u_t \\neq u_{t-1}\\}$, the process with state $x_t$ alone is not Markov, and define an augmented state $s_t = (x_t, u_{t-1})$ that restores the Markov property. Using the principle of optimality, construct the dynamic programming recursion (Bellman equation) for the value function $V_t(x,\\bar{u})$ over the horizon $t \\in \\{0,1,2\\}$.\n\n- For the concrete instance with $k = 1$, initial state $x_0 = 1$, and initial previous control $u_{-1} = 0$, compute the optimal total cost-to-go $V_0(1,0)$.\n\n- Briefly discuss the computational implications of augmenting the state with the previous control in terms of the number of states evaluated per dynamic programming backup, assuming a discretization of the $x$-space of size $|\\mathcal{X}|$.\n\nYour final numeric answer should be the optimal total cost $V_0(1,0)$ for the specified instance. No rounding is required.", "solution": "This problem is a deterministic finite-horizon optimal control problem. The solution requires applying the principle of optimality via dynamic programming. The analysis proceeds in three parts as requested.\n\nFirst, we address the non-Markovian nature of the problem and formulate the appropriate dynamic programming recursion.\nA stochastic process is Markovian if the conditional probability distribution of future states, given the present and past states, depends only upon the present state. In the context of optimal control, the decision-making process is Markovian if the optimal control at time $t$ depends only on the current state $x_t$. The total cost to be minimized is the sum of stage costs, $\\sum_{t=0}^{1} \\ell(x_t, u_t, u_{t-1})$. The stage cost at time $t$, $\\ell(x_t, u_t, u_{t-1}) = x_t^2 + u_t^2 + k \\,\\mathbf{1}\\{u_t \\neq u_{t-1}\\}$, explicitly depends on the previous control, $u_{t-1}$. To choose the optimal control $u_t$, a decision-maker at time $t$ must know both the state $x_t$ and the previous control $u_{t-1}$ to correctly evaluate the switching penalty. Since $u_{t-1}$ is not part of the state $x_t$, the information contained in $x_t$ alone is insufficient to make an optimal decision for the future. Therefore, the decision process with state $x_t$ is not Markovian.\n\nTo restore the Markov property, we must augment the state to include all information necessary for decision-making. We define an augmented state $s_t = (x_t, u_{t-1})$ for $t \\in \\{0, 1, 2\\}$. With this definition, the state at time $t$ is $s_t = (x_t, u_{t-1})$, and the control is $u_t$. The state transition is given by $s_{t+1} = (x_{t+1}, u_t) = (x_t + u_t, u_t)$. The new state $s_{t+1}$ depends only on the current state $s_t$ and the current control $u_t$. The stage cost $\\ell(x_t, u_t, u_{t-1})$ can be written as a function $\\tilde{\\ell}(s_t, u_t)$, which depends only on the augmented state and the control. This structure ensures the process is a Markov Decision Process.\n\nThe value function, $V_t(s_t)$, represents the optimal cost-to-go from state $s_t$ at time $t$. We write it as $V_t(x_t, u_{t-1})$. The Bellman equation, derived from the principle of optimality, is:\n$$V_t(x_t, u_{t-1}) = \\min_{u_t \\in \\mathcal{U}} \\left\\{ \\ell(x_t, u_t, u_{t-1}) + V_{t+1}(x_{t+1}, u_t) \\right\\}$$\nSubstituting the problem's specifics, for $t \\in \\{0, 1\\}$, the recursion is:\n$$V_t(x_t, u_{t-1}) = \\min_{u_t \\in \\{-1, 0, 1\\}} \\left\\{ x_t^2 + u_t^2 + k \\, \\mathbf{1}\\{u_t \\neq u_{t-1}\\} + V_{t+1}(x_t + u_t, u_t) \\right\\}$$\nThe terminal condition is that the cost at time $t=2$ is zero, which means $V_2(x_2, u_1) = 0$ for all possible $x_2$ and $u_1$.\n\nSecond, we compute the optimal cost $V_0(1, 0)$ for the specific instance where $k=1$, $x_0=1$, and $u_{-1}=0$. We work backward in time from $t=2$.\n\n**Time $t=2$**: The terminal value function is given as zero.\n$$V_2(x_2, u_1) = 0$$\n\n**Time $t=1$**: We compute $V_1(x_1, u_0)$.\n$$V_1(x_1, u_0) = \\min_{u_1 \\in \\{-1, 0, 1\\}} \\left\\{ x_1^2 + u_1^2 + 1 \\cdot \\mathbf{1}\\{u_1 \\neq u_0\\} + V_2(x_1 + u_1, u_1) \\right\\}$$\nSince $V_2(\\cdot, \\cdot) = 0$, this simplifies to:\n$$V_1(x_1, u_0) = x_1^2 + \\min_{u_1 \\in \\{-1, 0, 1\\}} \\left\\{ u_1^2 + \\mathbf{1}\\{u_1 \\neq u_0\\} \\right\\}$$\nThe minimization depends on the value of $u_0 \\in \\{-1, 0, 1\\}$.\n- If $u_0 = 0$: The term to minimize is $\\min \\{(-1)^2 + 1, 0^2 + 0, 1^2 + 1\\} = \\min\\{2, 0, 2\\} = 0$. Thus, $V_1(x_1, 0) = x_1^2$.\n- If $u_0 = 1$: The term to minimize is $\\min \\{(-1)^2 + 1, 0^2 + 1, 1^2 + 0\\} = \\min\\{2, 1, 1\\} = 1$. Thus, $V_1(x_1, 1) = x_1^2 + 1$.\n- If $u_0 = -1$: The term to minimize is $\\min \\{(-1)^2 + 0, 0^2 + 1, 1^2 + 1\\} = \\min\\{1, 1, 2\\} = 1$. Thus, $V_1(x_1, -1) = x_1^2 + 1$.\n\n**Time $t=0$**: We compute $V_0(1, 0)$.\n$$V_0(1, 0) = \\min_{u_0 \\in \\{-1, 0, 1\\}} \\left\\{ 1^2 + u_0^2 + 1 \\cdot \\mathbf{1}\\{u_0 \\neq 0\\} + V_1(1 + u_0, u_0) \\right\\}$$\nWe evaluate the expression in the braces for each possible control $u_0$:\n- For $u_0 = -1$: The cost is $1^2 + (-1)^2 + \\mathbf{1}\\{-1 \\neq 0\\} + V_1(1 - 1, -1) = 1 + 1 + 1 + V_1(0, -1)$. Using our derived function, $V_1(0, -1) = 0^2 + 1 = 1$. The total cost is $1+1+1+1 = 4$.\n- For $u_0 = 0$: The cost is $1^2 + 0^2 + \\mathbf{1}\\{0 \\neq 0\\} + V_1(1 + 0, 0) = 1 + 0 + 0 + V_1(1, 0)$. Using our derived function, $V_1(1, 0) = 1^2 = 1$. The total cost is $1+0+0+1 = 2$.\n- For $u_0 = 1$: The cost is $1^2 + 1^2 + \\mathbf{1}\\{1 \\neq 0\\} + V_1(1 + 1, 1) = 1 + 1 + 1 + V_1(2, 1)$. Using our derived function, $V_1(2, 1) = 2^2 + 1 = 5$. The total cost is $1+1+1+5 = 8$.\n\nComparing the three outcomes, the minimum cost is $2$, which occurs when $u_0=0$.\nTherefore, $V_0(1, 0) = 2$.\n\nThird, we discuss the computational implications of the state augmentation.\nIn a typical dynamic programming setting with a continuous state space like $\\mathbb{R}$, we must discretize the space to apply the algorithm numerically. Let the discretized space for the $x$ component be $\\mathcal{X}$, with size $|\\mathcal{X}|$. The control space $\\mathcal{U}$ is already discrete with size $|\\mathcal{U}|=3$.\n\nWithout state augmentation, the (hypothetical, incorrect) state space would just be $\\mathcal{X}$, with size $|\\mathcal{X}|$. A DP backup for each state would involve iterating through $|\\mathcal{U}|$ controls. The total complexity of one time-step update would be proportional to $|\\mathcal{X}| \\times |\\mathcal{U}|$.\n\nWith the augmented state $s_t = (x_t, u_{t-1})$, the state space becomes the Cartesian product of the discretized $x$-space and the control space, $\\mathcal{X} \\times \\mathcal{U}$. The size of this augmented state space is $|\\mathcal{X}| \\times |\\mathcal{U}|$. To compute the value function for each of these states at time $t$, we must minimize over all possible controls $u_t \\in \\mathcal{U}$. Thus, for each of the $|\\mathcal{X}| \\times |\\mathcal{U}|$ states, we perform $|\\mathcal{U}|$ evaluations. The total complexity of one time-step update (a full DP backup) becomes proportional to $(|\\mathcal{X}| \\times |\\mathcal{U}|) \\times |\\mathcal{U}| = |\\mathcal{X}| \\times |\\mathcal{U}|^2$.\n\nThe augmentation increases the size of the state space that must be evaluated by a factor of $|\\mathcal{U}|$. Consequently, the computational complexity of each DP backup is also increased by a factor of $|\\mathcal{U}|$. In this specific problem, $|\\mathcal{U}| = 3$, so the state space size and per-step computational effort are both tripled compared to a naive (and incorrect) model that ignores the switching cost's dependency on past actions. This is a manifestation of the \"curse of dimensionality,\" where expanding the state vector leads to exponential growth in the state space volume and computational cost.", "answer": "$$\n\\boxed{2}\n$$", "id": "3101515"}, {"introduction": "Dynamic programming is not limited to purely physical states; it can also model strategic decisions and commitments. This problem explores a scenario involving a \"sunk cost,\" where an early decision irreversibly alters the landscape of future options and costs. By incorporating the commitment status into the state variable, you will practice applying the Bellman principle to a stochastic environment, learning how to correctly handle expectations over random future events like customer demand. [@problem_id:3101502]", "problem": "Consider a finite-horizon, two-stage decision problem to be modeled as a Markov decision process (MDP). Time is indexed by $t \\in \\{1,2\\}$. There is a sunk-cost commitment option at stage $t=1$ that irreversibly changes the set of available actions at stage $t=2$. Demand $D \\in \\{0,1\\}$ for a single unit of a needed service is realized after the stage-$t=1$ decision and before the stage-$t=2$ decision. The initial state at $t=1$ is $s_1 = (z_1)$ with $z_1 = 0$ indicating that no sunk-cost commitment has yet been made. The stage-$t=1$ control $u_1 \\in \\{\\text{commit}, \\text{not commit}\\}$ evolves the sunk-cost status according to the rule: if $u_1=\\text{commit}$, a sunk cost of $K=5$ is incurred immediately and the next state has $z_2=1$; if $u_1=\\text{not commit}$, no cost is incurred at $t=1$ and the next state has $z_2=0$. The random demand $D$ is independent of the control and of $z_t$, and satisfies $\\mathbb{P}(D=1)=\\frac{2}{3}$ and $\\mathbb{P}(D=0)=\\frac{1}{3}$.\n\nAt stage $t=2$, the control set and stage cost depend on the sunk-cost status $z_2$ and the realized demand $D$:\n- If $z_2=1$ (the sunk-cost commitment was made at $t=1$), then:\n  - If $D=1$, the only allowed action is in-house production, with stage cost $c_2=4$.\n  - If $D=0$, the allowed action is to do nothing, with stage cost $c_2=0$.\n- If $z_2=0$ (no sunk-cost commitment was made at $t=1$), then:\n  - If $D=1$, the allowed actions are external purchase, with stage cost $c_2=9$, or emergency rental incurring a setup cost $H=3$ and a per-unit cost $3$; for $D=1$ the rental’s total stage cost is $H+3=6$. The decision at $t=2$ should minimize the stage cost given the realized demand.\n  - If $D=0$, the allowed action is to do nothing, with stage cost $c_2=0$.\n\nUsing Bellman’s principle of optimality, treat the sunk-cost status as part of the state. Derive the stage-$t=2$ value function $V_2(s_2)$ for each possible state $s_2=(z_2,D)$ and use it to write the stage-$t=1$ Bellman equation $V_1(s_1)$ explicitly as a minimization over the stage-$t=1$ control, with an expectation over the demand realization at $t=2$. Then compute the optimal expected total cost starting from the initial state $s_1=(z_1=0)$.\n\nExpress your final answer as a single exact real number. No rounding is required and no units are associated with the answer.", "solution": "The problem is a finite-horizon dynamic programming problem that can be modeled as a Markov Decision Process (MDP). The objective is to find the optimal expected total cost from a given initial state. We will solve this using backward induction, as dictated by Bellman's principle of optimality.\n\nThe problem involves two stages, indexed by $t \\in \\{1, 2\\}$.\nThe state at stage $t=1$ is $s_1 = (z_1)$, where $z_1 \\in \\{0, 1\\}$ represents the sunk-cost commitment status. The initial state is given as $s_1 = (z_1=0)$.\nThe action at stage $t=1$ is $u_1 \\in \\{\\text{commit}, \\text{not commit}\\}$.\nThe state at stage $t=2$ is $s_2 = (z_2, D)$, where $z_2 \\in \\{0, 1\\}$ is determined by the action $u_1$, and $D \\in \\{0, 1\\}$ is a random demand variable with probabilities $\\mathbb{P}(D=1) = \\frac{2}{3}$ and $\\mathbb{P}(D=0) = \\frac{1}{3}$.\n\nThe solution proceeds by first analyzing the final stage ($t=2$) and then working backward to the initial stage ($t=1$).\n\n**Stage $t=2$ Analysis: Derivation of the Value Function $V_2(s_2)$**\n\nThe value function at stage $t=2$, denoted by $V_2(s_2)$, is the minimum cost incurred from state $s_2$ to the end of the horizon. As stage $2$ is the terminal stage, $V_2(s_2)$ is simply the minimum possible cost at this stage, given by $V_2(s_2) = \\min_{u_2} c_2(s_2, u_2)$, where $c_2$ is the stage-$2$ cost. We must evaluate this for all possible states $s_2=(z_2, D)$.\n\n1.  State $s_2 = (z_2=1, D=1)$: Sunk-cost commitment was made ($z_2=1$) and demand is high ($D=1$).\n    The only allowed action is \"in-house production,\" with a cost of $c_2=4$.\n    Therefore, the value function for this state is $V_2(1,1) = 4$.\n\n2.  State $s_2 = (z_2=1, D=0)$: Sunk-cost commitment was made ($z_2=1$) and demand is low ($D=0$).\n    The only allowed action is \"do nothing,\" with a cost of $c_2=0$.\n    Therefore, the value function for this state is $V_2(1,0) = 0$.\n\n3.  State $s_2 = (z_2=0, D=1)$: No sunk-cost commitment was made ($z_2=0$) and demand is high ($D=1$).\n    The allowed actions are \"external purchase\" with cost $c_2=9$, or \"emergency rental\" with cost $H+3=6$.\n    The problem specifies that the decision taken minimizes the stage cost.\n    Thus, the cost incurred is $\\min\\{9, 6\\} = 6$.\n    Therefore, the value function for this state is $V_2(0,1) = 6$.\n\n4.  State $s_2 = (z_2=0, D=0)$: No sunk-cost commitment was made ($z_2=0$) and demand is low ($D=0$).\n    The only allowed action is \"do nothing,\" with a cost of $c_2=0$.\n    Therefore, the value function for this state is $V_2(0,0) = 0$.\n\nIn summary, the stage-$2$ value function is:\n$$ V_2(z_2, D) = \\begin{cases} 4 & \\text{if } z_2=1, D=1 \\\\ 0 & \\text{if } z_2=1, D=0 \\\\ 6 & \\text{if } z_2=0, D=1 \\\\ 0 & \\text{if } z_2=0, D=0 \\end{cases} $$\n\n**Stage $t=1$ Analysis: The Bellman Equation and Optimal Cost**\n\nNow we step back to stage $t=1$. The initial state is $s_1=(z_1=0)$. The Bellman equation for the value function $V_1(s_1)$ is:\n$$V_1(s_1) = \\min_{u_1 \\in A_1(s_1)} \\left\\{ c_1(s_1, u_1) + \\mathbb{E}_{D}[V_2(s_2(s_1, u_1, D))] \\right\\}$$\nwhere $c_1(s_1, u_1)$ is the immediate cost of action $u_1$ and the expectation is over the random demand $D$. The action set at $s_1=(z_1=0)$ is $A_1(0) = \\{\\text{commit}, \\text{not commit}\\}$.\n\nWe evaluate the total expected cost for each possible action $u_1$:\n\n**Case 1: $u_1 = \\text{commit}$**\n- The immediate cost is the sunk cost, $c_1((z_1=0), \\text{commit}) = K = 5$.\n- This action sets the sunk-cost status for the next stage to $z_2=1$.\n- The expected future cost is the expected value of $V_2(1, D)$ over the demand $D$:\n$$ \\mathbb{E}_{D}[V_2(1, D)] = V_2(1,1) \\cdot \\mathbb{P}(D=1) + V_2(1,0) \\cdot \\mathbb{P}(D=0) $$\n$$ \\mathbb{E}_{D}[V_2(1, D)] = (4) \\cdot \\left(\\frac{2}{3}\\right) + (0) \\cdot \\left(\\frac{1}{3}\\right) = \\frac{8}{3} $$\n- The total expected cost for this action is the sum of the immediate and expected future costs:\n$$ \\text{Cost}(\\text{commit}) = 5 + \\frac{8}{3} = \\frac{15}{3} + \\frac{8}{3} = \\frac{23}{3} $$\n\n**Case 2: $u_1 = \\text{not commit}$**\n- The immediate cost is $c_1((z_1=0), \\text{not commit}) = 0$.\n- This action sets the sunk-cost status for the next stage to $z_2=0$.\n- The expected future cost is the expected value of $V_2(0, D)$ over the demand $D$:\n$$ \\mathbb{E}_{D}[V_2(0, D)] = V_2(0,1) \\cdot \\mathbb{P}(D=1) + V_2(0,0) \\cdot \\mathbb{P}(D=0) $$\n$$ \\mathbb{E}_{D}[V_2(0, D)] = (6) \\cdot \\left(\\frac{2}{3}\\right) + (0) \\cdot \\left(\\frac{1}{3}\\right) = \\frac{12}{3} = 4 $$\n- The total expected cost for this action is:\n$$ \\text{Cost}(\\text{not commit}) = 0 + 4 = 4 $$\n\nThe problem asks for the stage-$t=1$ Bellman equation explicitly. For the initial state $s_1=(z_1=0)$, this is:\n$$ V_1(0) = \\min \\left\\{ 5 + \\mathbb{E}_{D}[V_2(1, D)], 0 + \\mathbb{E}_{D}[V_2(0, D)] \\right\\} $$\nSubstituting the computed values and expectations:\n$$ V_1(0) = \\min \\left\\{ 5 + \\left(4 \\cdot \\frac{2}{3} + 0 \\cdot \\frac{1}{3}\\right), 0 + \\left(6 \\cdot \\frac{2}{3} + 0 \\cdot \\frac{1}{3}\\right) \\right\\} $$\n$$ V_1(0) = \\min \\left\\{ 5 + \\frac{8}{3}, 4 \\right\\} = \\min \\left\\{ \\frac{23}{3}, 4 \\right\\} $$\n\nFinally, to compute the optimal expected total cost, we find the minimum of the two values. We can express $4$ as $\\frac{12}{3}$.\n$$ V_1(0) = \\min \\left\\{ \\frac{23}{3}, \\frac{12}{3} \\right\\} $$\nSince $12 < 23$, the minimum value is $\\frac{12}{3} = 4$.\n\nThus, the optimal expected total cost starting from the initial state $s_1=(z_1=0)$ is $4$. The optimal policy at stage $t=1$ is to choose the action \"not commit\".", "answer": "$$\n\\boxed{4}\n$$", "id": "3101502"}, {"introduction": "Many real-world optimization problems, from vehicle routing to network management, involve finding the best path while managing a finite resource like fuel, battery, or budget. This exercise challenges you to model such a problem by augmenting the state to track both location and resource level, allowing for decisions like recharging. Furthermore, you will go beyond simply formulating the Bellman equation to analyze its properties, deriving a \"dominance rule\" that allows for the pruning of suboptimal states, a key concept for developing efficient solution algorithms. [@problem_id:3101499]", "problem": "Consider a directed network with nodes $s$, $A$, $B$, $C$, $D$, $t$. An agent starts at node $s$ with an energy resource of $E_0 = 3$ units and a battery capacity of $E_{\\max} = 5$ units. Traversing a directed arc $(i,j)$ incurs a nonnegative travel time cost $d_{ij}$ and consumes energy $c_{ij}$; the agent may traverse $(i,j)$ only if its current energy $e$ satisfies $e \\ge c_{ij}$. Some nodes host recharging stations: at node $A$ a recharge to full capacity costs $r_A = 6$ units of time, and at node $C$ a recharge to full capacity costs $r_C = 3$ units of time. A recharge at node $i \\in \\{A,C\\}$ is available only if $e < E_{\\max}$ and instantaneously sets the energy to $E_{\\max}$. All costs are dimensionless scalars.\n\nThe directed arcs, with their travel times and energy consumptions, are as follows:\n- From $s$ to $A$: $d_{sA} = 4$, $c_{sA} = 2$.\n- From $s$ to $B$: $d_{sB} = 3$, $c_{sB} = 3$.\n- From $s$ to $C$: $d_{sC} = 5$, $c_{sC} = 2$.\n- From $A$ to $C$: $d_{AC} = 2$, $c_{AC} = 2$.\n- From $A$ to $D$: $d_{AD} = 5$, $c_{AD} = 4$.\n- From $A$ to $t$: $d_{At} = 9$, $c_{At} = 5$.\n- From $B$ to $C$: $d_{BC} = 1$, $c_{BC} = 2$.\n- From $B$ to $D$: $d_{BD} = 4$, $c_{BD} = 1$.\n- From $B$ to $t$: $d_{Bt} = 7$, $c_{Bt} = 4$.\n- From $C$ to $D$: $d_{CD} = 2$, $c_{CD} = 1$.\n- From $C$ to $t$: $d_{Ct} = 6$, $c_{Ct} = 4$.\n- From $D$ to $t$: $d_{Dt} = 3$, $c_{Dt} = 2$.\n\nYour tasks:\n1. Using Bellman’s principle of optimality, augment the state by the resource level and define a value function $V(i,e)$ that represents the minimal remaining travel time to reach node $t$ from node $i$ with current energy $e \\in \\{0,1,2,3,4,5\\}$. Clearly state the boundary condition(s) and derive the Bellman recursion for $V(i,e)$ that incorporates both traversal and recharging decisions.\n2. Analyze dominance rules for label pruning in a forward Dynamic Programming (DP) label-setting interpretation: given two labels at the same node $i$ with pairs $(\\ell_1, e_1)$ and $(\\ell_2, e_2)$, where $\\ell_k$ is the cumulative travel time to reach $i$ and $e_k$ is the remaining energy upon arrival, state a sufficient condition under which one label dominates the other and justify it using Bellman’s principle of optimality. Illustrate the rule with one explicit comparison drawn from this instance.\n3. Compute the minimal total travel time from $s$ to $t$ under the given constraints, starting with energy $E_0 = 3$, assuming optimal recharging decisions. Report the final answer as an exact integer (no rounding required).", "solution": "The problem is a shortest path problem on a directed graph with a resource constraint (energy). The state of the agent is defined by its current location (node) and its current energy level. The objective is to find the path from a source node $s$ to a terminal node $t$ with the minimum total travel time. This can be solved using dynamic programming.\n\nFirst, we formalize the problem parameters. The set of nodes is $\\mathcal{V} = \\{s, A, B, C, D, t\\}$. The initial energy is $E_0 = 3$ and the maximum energy capacity is $E_{\\max} = 5$. The energy level $e$ is an integer, so $e \\in \\{0, 1, 2, 3, 4, 5\\}$. For each arc $(i,j)$ in the network, there is an associated travel time $d_{ij} \\ge 0$ and an energy consumption $c_{ij} \\ge 0$. At nodes $A$ and $C$, recharging is possible. The time cost for recharging at $A$ is $r_A = 6$, and at $C$ is $r_C = 3$. Recharging replenishes the energy to $E_{\\max}$.\n\n### 1. Bellman’s Principle and Recursion\n\nIn accordance with Bellman's principle of optimality, any subpath of an optimal path must itself be optimal. To apply this, we define the state by the agent's current node $i \\in \\mathcal{V}$ and current energy level $e \\in \\{0, 1, \\dots, E_{\\max}\\}$.\n\nLet $V(i, e)$ be the value function representing the minimum possible cumulative travel time from the current state $(i, e)$ to the destination node $t$. This is a \"cost-to-go\" function.\n\nThe boundary condition is defined at the destination node $t$. The time to reach $t$ from $t$ is $0$, regardless of the energy level upon arrival.\n$$V(t, e) = 0, \\quad \\forall e \\in \\{0, 1, 2, 3, 4, 5\\}$$\n\nFor any other node $i \\neq t$, the value function $V(i, e)$ is determined by taking the minimum over the costs of all possible actions from state $(i, e)$. The possible actions are traversing to an adjacent node or recharging (if at a recharge station).\n\nLet $\\mathcal{N}(i)$ denote the set of nodes $j$ for which a directed arc $(i,j)$ exists. The cost of traversing an arc $(i, j)$ is the sum of the travel time $d_{ij}$ and the minimum future cost from the new state $(j, e - c_{ij})$. This action is only feasible if the current energy is sufficient, i.e., $e \\ge c_{ij}$.\n\nThe Bellman recursion can be formulated based on the type of node $i$:\n\n**Case 1: Node $i$ is not a recharging station ($i \\in \\{s, B, D\\}$)**\nThe only possible actions are to traverse to an adjacent node $j \\in \\mathcal{N}(i)$.\n$$V(i, e) = \\min_{j \\in \\mathcal{N}(i) \\text{ s.t. } e \\ge c_{ij}} \\{d_{ij} + V(j, e - c_{ij})\\}$$\nIf no such moves are possible (i.e., the set of feasible next nodes is empty), the cost is infinite, $V(i, e) = \\infty$, as $t$ is unreachable from this state.\n\n**Case 2: Node $i$ is a recharging station ($i \\in \\{A, C\\}$)**\nIn addition to traversing, the agent can choose to recharge. This action is available only if $e < E_{\\max}$. Recharging takes time $r_i$ and sets the energy level to $E_{\\max}$. The subsequent cost is $V(i, E_{\\max})$.\n\nIf $e < E_{\\max}$:\n$$V(i, e) = \\min \\left( r_i + V(i, E_{\\max}), \\quad \\min_{j \\in \\mathcal{N}(i) \\text{ s.t. } e \\ge c_{ij}} \\{d_{ij} + V(j, e - c_{ij})\\} \\right)$$\nwhere $r_i$ is $r_A=6$ for $i=A$ and $r_C=3$ for $i=C$.\n\nIf $e = E_{\\max}$:\nThe agent cannot recharge further. The equation reduces to that of a non-recharging node:\n$$V(i, E_{\\max}) = \\min_{j \\in \\mathcal{N}(i) \\text{ s.t. } E_{\\max} \\ge c_{ij}} \\{d_{ij} + V(j, E_{\\max} - c_{ij})\\}$$\n\nThe objective is to find the minimal time from the starting state $(s, E_0)$, which is $V(s, 3)$.\n\n### 2. Dominance Rule for Label Pruning\n\nIn a forward dynamic programming algorithm (such as Dijkstra's algorithm adapted for this problem), we explore paths starting from the source node $s$. We maintain a set of non-dominated \"labels\" at each node. A label for node $i$ is a pair $(\\ell, e)$ representing a path from $s$ to $i$ with a total travel time of $\\ell$ and a remaining energy of $e$.\n\n**Dominance Condition:**\nConsider two labels at the same node $i$: $L_1 = (\\ell_1, e_1)$ and $L_2 = (\\ell_2, e_2)$. Label $L_1$ is said to dominate label $L_2$ if it is at least as good in one dimension (cost or energy) and strictly better in the other, or better in both. A sufficient condition for $L_1$ to dominate $L_2$ is:\n$$\\ell_1 \\le \\ell_2 \\quad \\text{and} \\quad e_1 \\ge e_2$$\nIf both inequalities hold and at least one is strict, then $L_1$ strictly dominates $L_2$, and the path corresponding to $L_2$ can be pruned.\n\n**Justification:**\nThe justification is rooted in Bellman's principle. The total time to reach the destination $t$ via the path that generated label $L_k$ is $\\ell_k + V(i, e_k)$, where $V(i, e_k)$ is the optimal cost-to-go from state $(i, e_k)$.\nThe value function $V(i, e)$ must be a non-increasing function of energy $e$. This is because having more energy can never be a disadvantage; it expands the set of feasible future actions, and the minimum over a wider set of choices can only be smaller or equal, i.e., if $e_1 \\ge e_2$, then $V(i, e_1) \\le V(i, e_2)$.\nGiven the dominance condition $\\ell_1 \\le \\ell_2$ and $e_1 \\ge e_2$, it follows that $V(i, e_1) \\le V(i, e_2)$. Therefore:\n$$\\ell_1 + V(i, e_1) \\le \\ell_2 + V(i, e_2)$$\nThis means the best possible path to $t$ continuing from the state represented by $L_1$ is guaranteed to be no worse than the best path from $L_2$. Thus, $L_2$ is redundant and can be discarded.\n\n**Illustration:**\nLet's find two paths from $s$ to node $D$ to illustrate dominance.\nPath 1: $s \\to A \\to \\text{recharge} \\to D$\n- $s \\to A$: Arrive at $A$ with time $\\ell = d_{sA} = 4$ and energy $e = 3 - c_{sA} = 3-2=1$.\n- Recharge at $A$: Time becomes $\\ell = 4 + r_A = 4+6=10$. Energy becomes $e = E_{\\max} = 5$.\n- $A \\to D$: Arrive at $D$ with time $\\ell = 10 + d_{AD} = 10+5=15$ and energy $e = 5 - c_{AD} = 5-4=1$. This gives label $L_1 = (15, 1)$ at node $D$.\n\nPath 2: $s \\to C \\to \\text{recharge} \\to D$\n- $s \\to C$: Arrive at $C$ with time $\\ell = d_{sC} = 5$ and energy $e = 3 - c_{sC} = 3-2=1$.\n- Recharge at $C$: Time becomes $\\ell = 5 + r_C = 5+3=8$. Energy becomes $e = E_{\\max} = 5$.\n- $C \\to D$: Arrive at $D$ with time $\\ell = 8 + d_{CD} = 8+2=10$ and energy $e = 5 - c_{CD} = 5-1=4$. This gives label $L_2 = (10, 4)$ at node $D$.\n\nComparing $L_1 = (\\ell_1=15, e_1=1)$ and $L_2 = (\\ell_2=10, e_2=4)$ at node $D$:\nWe observe that $\\ell_2 < \\ell_1$ ($10 < 15$) and $e_2 > e_1$ ($4 > 1$). Both conditions for dominance are met in a strict sense. Thus, label $L_2$ strictly dominates label $L_1$. The path corresponding to $L_1$ is inferior and can be pruned.\n\n### 3. Computation of Minimal Total Travel Time\n\nWe apply a forward search algorithm (a variant of Dijkstra's) starting from state $(s, 3)$ with cost $0$. We maintain a priority queue of labels $(\\ell, e, i)$ prioritized by the travel time $\\ell$.\n\n1.  Initialize priority queue `PQ` with `(0, 3, s)`. Initialize cost to $t$ as $\\infty$.\n2.  Pop `(0, 3, s)`.\n    - $s \\to A$: Feasible ($3 \\ge c_{sA}=2$). New label at $A$: time $0+4=4$, energy $3-2=1$. Push `(4, 1, A)`.\n    - $s \\to B$: Feasible ($3 \\ge c_{sB}=3$). New label at $B$: time $0+3=3$, energy $3-3=0$. Push `(3, 0, B)`.\n    - $s \\to C$: Feasible ($3 \\ge c_{sC}=2$). New label at $C$: time $0+5=5$, energy $3-2=1$. Push `(5, 1, C)`.\n3.  Pop `(3, 0, B)`. Node $B$ with energy $0$. All outgoing arcs from $B$ require energy $>0$. This path is a dead end.\n4.  Pop `(4, 1, A)`. At node $A$ with energy $1$.\n    - Moves: Arcs to $C, D, t$ require energy $2, 4, 5$ respectively. All are infeasible.\n    - Recharge: Possible since $1 < E_{\\max}$. Cost $r_A=6$. New label at $A$: time $4+6=10$, energy $5$. Push `(10, 5, A)`.\n5.  Pop `(5, 1, C)`. At node $C$ with energy $1$.\n    - Moves:\n        - $C \\to D$: Feasible ($1 \\ge c_{CD}=1$). New label at $D$: time $5+2=7$, energy $1-1=0$. Push `(7, 0, D)`.\n        - $C \\to t$: Infeasible ($1 < c_{Ct}=4$).\n    - Recharge: Possible ($1 < E_{\\max}$). cost $r_C=3$. New label at $C$: time $5+3=8$, energy $5$. Push `(8, 5, C)`.\n6.  Pop `(7, 0, D)`. At node $D$ with energy $0$.\n    - Move $D \\to t$: Infeasible ($0 < c_{Dt}=2$). Dead end.\n7.  Pop `(8, 5, C)`. At node $C$ with energy $5$.\n    - Moves:\n        - $C \\to D$: Feasible ($5 \\ge c_{CD}=1$). New label at $D$: time $8+2=10$, energy $5-1=4$. Check dominance at $D$: current label is $(7,0)$. Neither $(10,4)$ nor $(7,0)$ dominates the other. Push `(10, 4, D)`.\n        - $C \\to t$: Feasible ($5 \\ge c_{Ct}=4$). Arrive at $t$ at time $8+6=14$. This is a valid path. Update cost to $t$ to $14$.\n    - Recharge: Not possible (energy is at max).\n8.  Pop `(10, 4, D)`. At node $D$ with energy $4$.\n    - Move $D \\to t$: Feasible ($4 \\ge c_{Dt}=2$). Arrive at $t$ at time $10+3=13$. This is better than the previous path. Update cost to $t$ to $13$.\n9.  Pop `(10, 5, A)`. At node $A$ with energy $5$.\n    - Moves:\n        - $A \\to C$: Feasible ($5 \\ge c_{AC}=2$). New label at $C$: time $10+2=12$, energy $5-2=3$. Check dominance at $C$: existing labels are $(5,1)$ and $(8,5)$. The label $(8,5)$ dominates $(12,3)$ since $8<12$ and $5>3$. Prune this path.\n        - $A \\to D$: Feasible ($5 \\ge c_{AD}=4$). New label at $D$: time $10+5=15$, energy $5-4=1$. Check dominance at $D$: existing labels are $(7,0)$ and $(10,4)$. Label $(10,4)$ dominates $(15,1)$ since $10<15$ and $4>1$. Prune this path.\n        - $A \\to t$: Feasible ($5 \\ge c_{At}=5$). Arrive at $t$ at time $10+9=19$. This is worse than the current minimum of $13$. Ignore.\n10. The priority queue is now empty. The algorithm terminates.\n\nThe minimal total travel time is $13$. The optimal path is $s \\to C \\to \\text{recharge at } C \\to D \\to t$.\n- $s \\to C$: time $5$, energy upon arrival is $3-2=1$.\n- Recharge at $C$: time cost $3$, total time is $5+3=8$, energy is $5$.\n- $C \\to D$: time cost $2$, total time is $8+2=10$, energy upon arrival is $5-1=4$.\n- $D \\to t$: time cost $3$, total time is $10+3=13$, energy upon arrival is $4-2=2$.\nThe total time is $5+3+2+3=13$.", "answer": "$$\\boxed{13}$$", "id": "3101499"}]}