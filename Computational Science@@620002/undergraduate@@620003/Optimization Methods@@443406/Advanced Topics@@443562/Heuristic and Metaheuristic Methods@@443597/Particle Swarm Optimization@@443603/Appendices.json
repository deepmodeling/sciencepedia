{"hands_on_practices": [{"introduction": "The heart of the Particle Swarm Optimization (PSO) algorithm is its elegant velocity update equation, which mathematically models the social and cognitive behavior of a swarm. This first practice provides a concrete, step-by-step calculation to ensure you have a solid grasp of this fundamental mechanism [@problem_id:2166499]. By manually computing a particle's new velocity, you will see exactly how the inertia ($w$), cognitive ($c_1$), and social ($c_2$) components combine to guide the search process.", "problem": "An autonomous drone is part of a swarm searching for the location of a maximum intensity radio signal in a large, open field, which is modeled as a 2D Cartesian plane. The search is guided by the Particle Swarm Optimization (PSO) algorithm. At each time step, every drone updates its velocity based on its current velocity, its own best-found position, and the best-found position by any drone in the entire swarm.\n\nThe velocity update for a single particle (drone) at time step $t+1$ is given by the equation:\n$$ \\vec{v}(t+1) = \\omega \\vec{v}(t) + c_1 r_1 (\\vec{p} - \\vec{x}(t)) + c_2 r_2 (\\vec{g} - \\vec{x}(t)) $$\nwhere:\n- $\\vec{v}(t)$ is the drone's current velocity vector.\n- $\\vec{x}(t)$ is the drone's current position vector.\n- $\\vec{p}$ is the drone's personal best position found so far.\n- $\\vec{g}$ is the global best position found by the entire swarm so far.\n- $\\omega$ is the inertia weight, controlling the influence of the previous velocity.\n- $c_1$ and $c_2$ are the cognitive and social coefficients, respectively, which weight the influence of the personal and global best positions.\n- $r_1$ and $r_2$ are random numbers uniformly distributed in $[0, 1]$.\n\nConsider a specific drone at a particular time step $t$. The state of this drone and the swarm parameters are given as follows:\n- Current position: $\\vec{x}(t) = \\begin{pmatrix} 8.0 \\\\ 14.0 \\end{pmatrix}$\n- Current velocity: $\\vec{v}(t) = \\begin{pmatrix} -1.0 \\\\ 2.0 \\end{pmatrix}$\n- Personal best position: $\\vec{p} = \\begin{pmatrix} 10.0 \\\\ 12.0 \\end{pmatrix}$\n- Global best position: $\\vec{g} = \\begin{pmatrix} 11.0 \\\\ 10.0 \\end{pmatrix}$\n- Inertia weight: $\\omega = 0.7$\n- Cognitive coefficient: $c_1 = 1.5$\n- Social coefficient: $c_2 = 1.5$\nFor this specific update step, the generated random numbers are $r_1 = 0.4$ and $r_2 = 0.9$.\n\nCalculate the drone's new velocity vector, $\\vec{v}(t+1)$. All positions are given in meters and velocities in meters per second (m/s). Express your answer as a 2-element row matrix $[v_x, v_y]$ in m/s. Round each component of the vector to three significant figures.", "solution": "We apply the PSO velocity update rule\n$$\\vec{v}(t+1)=\\omega \\vec{v}(t)+c_{1}r_{1}\\left(\\vec{p}-\\vec{x}(t)\\right)+c_{2}r_{2}\\left(\\vec{g}-\\vec{x}(t)\\right).$$\nCompute the displacement vectors:\n$$\\vec{p}-\\vec{x}(t)=\\begin{pmatrix}10.0-8.0 \\\\ 12.0-14.0\\end{pmatrix}=\\begin{pmatrix}2 \\\\ -2\\end{pmatrix},\\quad \\vec{g}-\\vec{x}(t)=\\begin{pmatrix}11.0-8.0 \\\\ 10.0-14.0\\end{pmatrix}=\\begin{pmatrix}3 \\\\ -4\\end{pmatrix}.$$\nCompute each term:\n$$\\omega \\vec{v}(t)=0.7\\begin{pmatrix}-1.0 \\\\ 2.0\\end{pmatrix}=\\begin{pmatrix}-0.7 \\\\ 1.4\\end{pmatrix},$$\n$$c_{1}r_{1}\\left(\\vec{p}-\\vec{x}(t)\\right)=1.5\\cdot 0.4\\begin{pmatrix}2 \\\\ -2\\end{pmatrix}=0.6\\begin{pmatrix}2 \\\\ -2\\end{pmatrix}=\\begin{pmatrix}1.2 \\\\ -1.2\\end{pmatrix},$$\n$$c_{2}r_{2}\\left(\\vec{g}-\\vec{x}(t)\\right)=1.5\\cdot 0.9\\begin{pmatrix}3 \\\\ -4\\end{pmatrix}=1.35\\begin{pmatrix}3 \\\\ -4\\end{pmatrix}=\\begin{pmatrix}4.05 \\\\ -5.4\\end{pmatrix}.$$\nSum the contributions to obtain\n$$\\vec{v}(t+1)=\\begin{pmatrix}-0.7 \\\\ 1.4\\end{pmatrix}+\\begin{pmatrix}1.2 \\\\ -1.2\\end{pmatrix}+\\begin{pmatrix}4.05 \\\\ -5.4\\end{pmatrix}=\\begin{pmatrix}4.55 \\\\ -5.2\\end{pmatrix}.$$\nRounding each component to three significant figures gives\n$$\\vec{v}(t+1)=\\begin{pmatrix}4.55 \\\\ -5.20\\end{pmatrix}.$$\nExpressed as a row matrix, this is $\\begin{pmatrix}4.55 & -5.20\\end{pmatrix}$ in meters per second.", "answer": "$$\\boxed{\\begin{pmatrix}4.55 & -5.20\\end{pmatrix}}$$", "id": "2166499"}, {"introduction": "While the basic update rule drives the swarm, in practice, it can lead to instability, with particles developing excessively high velocities and \"exploding\" out of the search space. This exercise introduces velocity clamping, a crucial technique for controlling swarm dynamics and balancing global exploration with local exploitation [@problem_id:3161041]. You will implement a complete PSO algorithm and experiment with the velocity limit, $v^{\\max}$, to observe firsthand how this hyperparameter influences the algorithm's convergence and overall performance.", "problem": "You are to study the effect of velocity clamping in Particle Swarm Optimization (PSO). Particle Swarm Optimization (PSO) is a population-based stochastic search method that iteratively updates particle positions using velocities influenced by personal and social information. Consider a swarm of $N$ particles, each with position $x_j \\in \\mathbb{R}^d$ and velocity $v_j \\in \\mathbb{R}^d$, where $j \\in \\{1,\\dots,N\\}$ indexes particles and $d$ is the dimension. Let $p_j$ denote the personal best position found by particle $j$ so far and let $g$ denote the global best position found by the swarm. Velocity clamping enforces a bound $\\lVert v_j \\rVert_{\\infty} \\le v^{\\max}$ component-wise at every iteration, which means each component satisfies $\\lvert v_{j,k} \\rvert \\le v^{\\max}$ for all $k \\in \\{1,\\dots,d\\}$.\n\nFundamental base. Start from the core definitions of discrete-time iterative optimization methods and the well-tested facts of stochastic search:\n- Iterative methods update a state using a rule that combines inertia of the previous state and a corrective feedback proportional to an error signal. In PSO, the inertia is the previous velocity and the corrective signals are the differences to $p_j$ and $g$, modulated by random coefficients.\n- The objective function is real-valued, continuous, and bounded below. You will use the sphere function $f(x) = \\sum_{k=1}^d x_k^2$, which is convex with a unique global minimum at $x^\\star = 0$ and $f(x^\\star) = 0$.\n\nYour tasks:\n1. Derive, from the fundamental base above (without using shortcut formulas), the discrete-time update equations that embody inertia and corrective feedback to $p_j$ and $g$, and incorporate the velocity clamping operator that limits each component of the velocity by $v^{\\max}$. Explain qualitatively and quantitatively how velocity clamping affects the convergence rate toward the minimizer for $f(x)$ on a bounded domain.\n2. Implement a PSO variant that uses the derived velocity clamping to minimize $f(x) = \\sum_{k=1}^d x_k^2$ over the box $[-5,5]^d$. Use the following fixed hyperparameters for all test cases: dimension $d = 5$, swarm size $N = 25$, inertia weight $w = 0.7$, number of iterations $T = 100$, and position clamping to keep $x_j$ in $[-5,5]^d$ at all times. At each iteration, draw fresh independent random variables $r_{1,j,k}, r_{2,j,k} \\sim \\mathcal{U}(0,1)$ for all particles $j$ and dimensions $k$, where $\\mathcal{U}(0,1)$ denotes the uniform distribution on $[0,1]$.\n3. For reproducibility, use deterministic pseudorandom seeds. For test case with index $i$ (starting at $i=0$) and replicate index $r \\in \\{0,\\dots,R-1\\}$, seed the random number generator with $s = 1000 + 100 \\cdot i + r$, and run $R = 5$ independent replicates per test case. Initialize positions $x_j$ uniformly in $[-5,5]^d$, set initial personal bests $p_j$ to these positions, initialize the global best $g$ accordingly, and initialize velocities $v_j$ uniformly in $[-v^{\\max}, v^{\\max}]^d$.\n4. For each test case, after $T$ iterations, compute the average best-found value $\\bar{f}(g)$ across the $R$ replicates, where $f(g)$ is evaluated at the final global best for each replicate. Your program must output a single line containing these averages for all test cases as a comma-separated list enclosed in square brackets, like $[a_1,a_2,\\dots,a_m]$, where each $a_i$ is a floating-point number.\n\nTest suite. Evaluate the following $m = 6$ parameter sets, each given as $(c_1,c_2,v^{\\max})$:\n- Case $0$: $(c_1,c_2,v^{\\max}) = (2.0, 2.0, 0.0)$, a boundary case with no movement.\n- Case $1$: $(c_1,c_2,v^{\\max}) = (2.0, 2.0, 0.5)$, a moderate clamp with symmetric cognitive and social terms.\n- Case $2$: $(c_1,c_2,v^{\\max}) = (2.0, 2.0, 2.0)$, a larger clamp with symmetric cognitive and social terms.\n- Case $3$: $(c_1,c_2,v^{\\max}) = (3.0, 0.5, 1.0)$, cognitive-dominated regime with moderate clamp.\n- Case $4$: $(c_1,c_2,v^{\\max}) = (0.5, 3.0, 1.0)$, social-dominated regime with moderate clamp.\n- Case $5$: $(c_1,c_2,v^{\\max}) = (1.0, 1.0, 1.0)$, weak symmetric influence.\n\nAnswer specification. For each case, the computed average best-found value $\\bar{f}(g)$ is a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_0,r_1,r_2,r_3,r_4,r_5]$). No other output is permitted.", "solution": "The problem requires the derivation and implementation of a Particle Swarm Optimization (PSO) algorithm with velocity clamping, followed by an analysis of the clamp's effect on convergence. The validation confirms the problem is scientifically grounded, well-posed, and complete. We can thus proceed with a rigorous solution.\n\n### 1. Derivation of the PSO Update Equations\n\nWe begin from the fundamental principles of discrete-time iterative optimization methods. The state of each particle $j$ in the swarm at a discrete time step $t$ is described by its position vector $x_j(t) \\in \\mathbb{R}^d$ and its velocity vector $v_j(t) \\in \\mathbb{R}^d$. The goal is to iteratively update these states to find the minimum of an objective function $f(x)$.\n\n**Velocity Update Rule:**\nThe problem states that the update rule for velocity must combine inertia and corrective feedback. The velocity at the next time step, $v_j(t+1)$, is constructed from three components:\n\n1.  **Inertia:** A particle tends to continue in its current direction. This is modeled as a fraction of its previous velocity, controlled by an inertia weight $w$. The inertia term is $w v_j(t)$.\n\n2.  **Cognitive Corrective Feedback:** Each particle is drawn toward its own best-found position, denoted as $p_j(t)$. This represents the particle's individual experience. The \"error signal\" is the vector difference $(p_j(t) - x_j(t))$. The corrective term is proportional to this difference, modulated by a cognitive acceleration coefficient $c_1$ and a stochastic element $r_{1,j}(t)$ to introduce exploration. This term is $c_1 r_{1,j}(t) \\odot (p_j(t) - x_j(t))$, where $\\odot$ denotes element-wise multiplication and each component of $r_{1,j}(t)$ is drawn from a uniform distribution $\\mathcal{U}(0,1)$.\n\n3.  **Social Corrective Feedback:** Each particle is also drawn toward the best-found position by any particle in the entire swarm, denoted as $g(t)$. This represents the collective experience of the swarm. The error signal is $(g(t) - x_j(t))$. The corrective term is proportional to this, modulated by a social acceleration coefficient $c_2$ and a stochastic element $r_{2,j}(t) \\sim \\mathcal{U}(0,1)$. This term is $c_2 r_{2,j}(t) \\odot (g(t) - x_j(t))$.\n\nCombining these three components yields the equation for the pre-clamped velocity vector, which we denote $v'_j(t+1)$:\n$$\nv'_j(t+1) = w v_j(t) + c_1 r_{1,j}(t) \\odot (p_j(t) - x_j(t)) + c_2 r_{2,j}(t) \\odot (g(t) - x_j(t))\n$$\nIn component-wise form for each dimension $k \\in \\{1, \\dots, d\\}$:\n$$\nv'_{j,k}(t+1) = w v_{j,k}(t) + c_1 r_{1,j,k}(t) (p_{j,k}(t) - x_{j,k}(t)) + c_2 r_{2,j,k}(t) (g_k(t) - x_{j,k}(t))\n$$\n\n**Velocity Clamping:**\nThe problem requires that the magnitude of each velocity component be bounded by $v^{\\max}$, i.e., $|v_{j,k}| \\le v^{\\max}$. We define a clamping operator, $C_{v^{\\max}}(\\cdot)$, which applies this constraint. For a scalar value $u$, the operator is:\n$$\nC_{v^{\\max}}(u) = \\text{sign}(u) \\cdot \\min(|u|, v^{\\max}) = \\max(-v^{\\max}, \\min(u, v^{\\max}))\n$$\nThis operator is applied component-wise to the pre-clamped velocity vector $v'_j(t+1)$ to obtain the final velocity $v_j(t+1)$:\n$$\nv_{j,k}(t+1) = C_{v^{\\max}}(v'_{j,k}(t+1)) \\quad \\forall k \\in \\{1, \\dots, d\\}\n$$\n\n**Position Update and Clamping:**\nThe position of the particle is updated using its newly calculated velocity, following the basic definition of velocity in a discrete-time system:\n$$\nx_j(t+1) = x_j(t) + v_j(t+1)\n$$\nTo enforce that particles remain within the search domain, here specified as $[-5, 5]^d$, a position clamping mechanism is applied after the update. For each component $k$:\n$$\nx_{j,k}(t+1) \\leftarrow \\max(-5, \\min(x_{j,k}(t+1), 5))\n$$\n\n**Best Position Updates:**\nAfter updating a particle's position, its new fitness $f(x_j(t+1))$ is evaluated. The personal and global best positions are updated as follows:\n-   **Personal Best:** If $f(x_j(t+1)) < f(p_j(t))$, then $p_j(t+1) = x_j(t+1)$. Otherwise, $p_j(t+1) = p_j(t)$.\n-   **Global Best:** The global best position $g(t+1)$ is selected from the set of all personal best positions $\\{p_j(t+1) \\mid j=1, \\dots, N\\}$ such that $f(g(t+1)) = \\min_{j} f(p_j(t+1))$.\n\n### 2. Analysis of Velocity Clamping\n\n**Qualitative Analysis:**\nVelocity clamping is a crucial mechanism for controlling the dynamics of the swarm.\n-   **Stability:** Without clamping, a particle's velocity can increase without bound, a phenomenon known as \"swarm explosion.\" This occurs if the parameters $w, c_1, c_2$ create unstable dynamics. An exploding swarm has particles that move so rapidly they \"fly over\" promising regions of the search space, drastically impairing or completely preventing convergence. Clamping imposes a hard limit on the step size a particle can take in one iteration, thereby ensuring the stability of the search process.\n-   **Exploration vs. Exploitation Trade-off:** The value of $v^{\\max}$ directly mediates the balance between global exploration and local exploitation.\n    -   A **small** $v^{\\max}$ forces particles to take small steps. This enhances local search (exploitation) around the current positions but can slow down convergence if the optimum is far away. It also increases the risk of the swarm becoming prematurely trapped in a local minimum, as particles may lack the momentum to escape.\n    -   A **large** $v^{\\max}$ allows particles to traverse the search space quickly, promoting global search (exploration). However, if $v^{\\max}$ is too large, the clamp becomes ineffective, bringing back the risk of instability and overshooting.\n-   **Boundary Case ($v^{\\max} = 0.0$):** This is a degenerate case. The specification requires initializing velocities uniformly in $[-v^{\\max}, v^{\\max}]^d$, which for $v^{\\max}=0.0$ means all initial velocities $v_j(0)$ are $0$. In each subsequent iteration, the velocity update calculates a non-zero $v'_{j,k}(t+1)$ (unless the particle is already at a point where $p_j=g=x_j$), but the clamping operator $C_{0.0}(\\cdot)$ immediately forces it back to $0$. Thus, $v_j(t)=0$ for all $t>0$. Consequently, $x_j(t+1) = x_j(t) + 0 = x_j(t)$. The particles never move from their initial positions. The final global best solution will simply be the best one found in the initial, randomly generated population.\n\n**Quantitative Analysis:**\nThe effect of velocity clamping on the convergence rate for the sphere function $f(x) = \\sum_{k=1}^d x_k^2$ can be analyzed by considering the particle's step size.\n-   **Far from the Optimum:** When a particle $j$ is far from the global minimum at $x^\\star = 0$, the terms $(p_{j,k} - x_{j,k})$ and $(g_k - x_{j,k})$ can be large. This results in a large magnitude for the unclamped velocity $v'_{j,k}$. If $|v'_{j,k}| > v^{\\max}$, the clamp becomes active, setting $|v_{j,k}| = v^{\\max}$. In this regime, the particle's movement towards the optimum is limited not by the swarm dynamics but by the hard limit $v^{\\max}$. The convergence rate is approximately linear, as the particle moves a near-constant distance of at most $\\sqrt{d} \\cdot v^{\\max}$ (in Euclidean distance) per iteration. A larger $v^{\\max}$ (e.g., Case 2 vs. Case 1) will allow for a faster approach toward the basin of attraction of the global optimum.\n-   **Near the Optimum:** As the particle approaches the optimum, its position $x_j$, its personal best $p_j$, and the global best $g$ become close to each other and to $x^\\star=0$. The differences $(p_j - x_j)$ and $(g - x_j)$ become small. Consequently, the magnitude of the calculated velocity $v'_j$ naturally decreases. Eventually, $|v'_{j,k}| \\le v^{\\max}$ for all components, and the clamp no longer activates. In this phase, the convergence is governed by the characteristic dynamics of the linear system described by the coefficients $w, c_1, c_2$. The choice of these parameters determines the final convergence speed and whether the particle will stably converge to the optimum or oscillate around it.\n-   **Cognitive vs. Social Dominance (Case 3 vs. Case 4):** For the convex, unimodal sphere function, a strong social pull ($c_2 > c_1$, Case 4) is generally beneficial. It encourages the entire swarm to rapidly converge on the single best location found so far. A strong cognitive pull ($c_1 > c_2$, Case 3) encourages particles to explore around their own best locations, promoting diversity. While useful for multi-modal problems, this can slow down convergence on a simple unimodal function like the sphere function, as the swarm does not coalesce as quickly. Therefore, we expect Case 4 to yield a better (lower) final fitness value than Case 3.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PSO problem for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (c1, c2, v_max)\n    test_cases = [\n        (2.0, 2.0, 0.0),  # Case 0\n        (2.0, 2.0, 0.5),  # Case 1\n        (2.0, 2.0, 2.0),  # Case 2\n        (3.0, 0.5, 1.0),  # Case 3\n        (0.5, 3.0, 1.0),  # Case 4\n        (1.0, 1.0, 1.0),  # Case 5\n    ]\n\n    # Fixed hyperparameters\n    d = 5          # Dimension\n    N = 25         # Swarm size\n    w = 0.7        # Inertia weight\n    T = 100        # Number of iterations\n    R = 5          # Number of replicates\n    x_min, x_max = -5.0, 5.0 # Search space bounds\n\n    # Objective function (Sphere function)\n    def f(x):\n        # x is an (N, d) array of positions\n        return np.sum(x**2, axis=1)\n\n    results = []\n    # Loop through each test case\n    for i, (c1, c2, v_max) in enumerate(test_cases):\n        replicate_best_vals = []\n        \n        # Run R independent replicates for each test case\n        for r in range(R):\n            # Seed the random number generator for reproducibility\n            seed = 1000 + 100 * i + r\n            rng = np.random.default_rng(seed)\n\n            # --- Initialization ---\n            # Initialize positions uniformly in the search space\n            positions = rng.uniform(x_min, x_max, size=(N, d))\n            \n            # Initialize velocities uniformly in [-v_max, v_max]\n            velocities = rng.uniform(-v_max, v_max, size=(N, d))\n            \n            # Initialize personal best positions and values\n            pbest_positions = np.copy(positions)\n            pbest_values = f(pbest_positions)\n            \n            # Initialize global best position and value\n            min_idx = np.argmin(pbest_values)\n            gbest_position = np.copy(pbest_positions[min_idx])\n            gbest_value = pbest_values[min_idx]\n\n            # --- Iterations ---\n            for _ in range(T):\n                # Generate random coefficients for velocity update\n                r1 = rng.random(size=(N, d))\n                r2 = rng.random(size=(N, d))\n                \n                # Update velocities (with inertia, cognitive, and social components)\n                # The gbest_position (1,d) is broadcasted for the subtraction\n                new_velocities = (w * velocities +\n                                  c1 * r1 * (pbest_positions - positions) +\n                                  c2 * r2 * (gbest_position - positions))\n                \n                # Apply velocity clamping\n                velocities = np.clip(new_velocities, -v_max, v_max)\n                \n                # Update positions\n                positions = positions + velocities\n                \n                # Apply position clamping (containment in search space)\n                positions = np.clip(positions, x_min, x_max)\n                \n                # Evaluate new positions\n                current_values = f(positions)\n                \n                # Update personal bests\n                improvement_mask = current_values < pbest_values\n                pbest_positions[improvement_mask] = positions[improvement_mask]\n                pbest_values[improvement_mask] = current_values[improvement_mask]\n                \n                # Update global best\n                min_pbest_idx = np.argmin(pbest_values)\n                if pbest_values[min_pbest_idx] < gbest_value:\n                    gbest_value = pbest_values[min_pbest_idx]\n                    gbest_position = np.copy(pbest_positions[min_pbest_idx])\n            \n            # Store the final global best value for this replicate\n            replicate_best_vals.append(gbest_value)\n            \n        # Compute the average best-found value across replicates for the current case\n        avg_best_value = np.mean(replicate_best_vals)\n        results.append(avg_best_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3161041"}, {"introduction": "After learning to control a particle's speed, we must also ensure it respects the boundaries of the defined search area. This is a common and critical requirement for solving constrained optimization problems. This hands-on practice challenges you to implement and compare several common boundary handling schemes, such as the 'reflect' and 'absorb' methods [@problem_id:3161093]. By analyzing their effects on feasibility and convergence, you will gain a deeper, more practical understanding of how to effectively apply PSO to real-world problems with domain constraints.", "problem": "Consider the bounded minimization of the shifted sphere objective over a hyper-rectangle. Let $d \\in \\mathbb{N}$, lower and upper bounds $l \\in \\mathbb{R}$ and $u \\in \\mathbb{R}$ with $l < u$, and a shift vector $a \\in \\mathbb{R}^d$. Define the objective function $$f(x) = \\sum_{j=1}^{d} (x_j - a_j)^2,$$ and the feasible set $$\\Omega = [l,u]^d = \\{x \\in \\mathbb{R}^d \\mid \\forall j \\in \\{1,\\dots,d\\}, \\, l \\le x_j \\le u\\}.$$ The unconstrained minimizer is $x^\\star = a$ with $f(x^\\star) = 0$, and under the box constraint the minimizer is the componentwise projection of $a$ onto $\\Omega$, which is $\\tilde{a}_j = \\min(\\max(a_j, l), u)$ for each $j$, yielding $x^\\star_\\Omega = \\tilde{a}$.\n\nYour task is to implement a complete, runnable program that uses Particle Swarm Optimization (PSO) to minimize $f(x)$ over $\\Omega$ and to examine the impact of three boundary handling schemes on feasibility and convergence. Particle Swarm Optimization (PSO) maintains a swarm of $N$ particles with positions $x_i(t) \\in \\mathbb{R}^d$ and velocities $v_i(t) \\in \\mathbb{R}^d$ at iteration $t$. Each particle tracks its personal best position $p_i(t)$ and the swarm tracks the global best position $g(t)$. At each iteration $t$, velocities and positions evolve according to the canonical inertial PSO update, where $$v_i(t+1) = w \\, v_i(t) + c_1 \\, r_1(t) \\odot (p_i(t) - x_i(t)) + c_2 \\, r_2(t) \\odot (g(t) - x_i(t)),$$ $$x_i(t+1) = x_i(t) + v_i(t+1),$$ with inertia weight $w \\in \\mathbb{R}$, cognitive and social coefficients $c_1 \\in \\mathbb{R}$ and $c_2 \\in \\mathbb{R}$, and independent random vectors $r_1(t), r_2(t) \\in [0,1]^d$ drawn componentwise from the uniform distribution. The operator $\\odot$ denotes componentwise multiplication. Velocities are clamped componentwise to $[-v_{\\max}, v_{\\max}]$ for a given $v_{\\max} \\in \\mathbb{R}_{>0}$. Initial positions are drawn uniformly from $\\Omega$ and initial velocities are drawn uniformly from $[-v_{\\max}, v_{\\max}]^d$. Personal and global bests update only when a strictly lower objective value is observed.\n\nBoundary handling is required when an attempted position $x_i^{\\mathrm{prop}}(t+1) = x_i(t) + v_i(t+1)$ lies outside $\\Omega$. Implement the following three schemes, each acting independently per component $j$:\n\n- Reflect: If $x_{ij}^{\\mathrm{prop}} \\notin [l,u]$, use reflective mapping over the interval $[l,u]$ by computing $$z = \\frac{x_{ij}^{\\mathrm{prop}} - l}{u - l}, \\quad k = \\left\\lfloor z \\right\\rfloor, \\quad z_{\\mathrm{frac}} = z - k.$$ Set $$x_{ij}(t+1) = \\begin{cases} l + z_{\\mathrm{frac}}(u - l) & \\text{if } k \\text{ is even}, \\\\ u - z_{\\mathrm{frac}}(u - l) & \\text{if } k \\text{ is odd}, \\end{cases}$$ and flip the velocity component when $k$ is odd, that is $$v_{ij}(t+1) = \\begin{cases} v_{ij}(t+1) & \\text{if } k \\text{ is even}, \\\\ -v_{ij}(t+1) & \\text{if } k \\text{ is odd}. \\end{cases}$$ If $x_{ij}^{\\mathrm{prop}} \\in [l,u]$, keep $x_{ij}(t+1) = x_{ij}^{\\mathrm{prop}}$ and $v_{ij}(t+1)$ unchanged.\n\n- Absorb: If $x_{ij}^{\\mathrm{prop}} < l$, set $x_{ij}(t+1) = l$ and $v_{ij}(t+1) = 0$. If $x_{ij}^{\\mathrm{prop}} > u$, set $x_{ij}(t+1) = u$ and $v_{ij}(t+1) = 0$. If $x_{ij}^{\\mathrm{prop}} \\in [l,u]$, keep $x_{ij}(t+1) = x_{ij}^{\\mathrm{prop}}$ and $v_{ij}(t+1)$ unchanged.\n\n- Random restart: If $x_{ij}^{\\mathrm{prop}} \\notin [l,u]$, set $x_{ij}(t+1)$ to a new value drawn uniformly from $[l,u]$ and set $v_{ij}(t+1) = 0$. If $x_{ij}^{\\mathrm{prop}} \\in [l,u]$, keep $x_{ij}(t+1) = x_{ij}^{\\mathrm{prop}}$ and $v_{ij}(t+1)$ unchanged.\n\nDefine the feasibility metric as the attempted feasibility rate over the run, that is, $$r_{\\mathrm{feasible}} = \\frac{\\text{number of proposals } x_i^{\\mathrm{prop}}(t+1) \\in \\Omega}{\\text{total number of proposals}} = \\frac{\\#\\{(i,t) \\mid x_i(t) + v_i(t+1) \\in \\Omega\\}}{N \\cdot T},$$ where $T \\in \\mathbb{N}$ is the total number of iterations.\n\nDefine the convergence condition via a tolerance $\\varepsilon \\in \\mathbb{R}_{>0}$: the run is said to have converged at iteration $t^\\star$ if $$\\min_{i} f(p_i(t^\\star)) \\le \\varepsilon.$$ Record whether this condition is met and the earliest $t^\\star$ when it is met, with $t^\\star = -1$ if it is never met. Also record the final best objective value $$f^\\star = \\min_{i} f(p_i(T)).$$\n\nImplement the PSO algorithm with boundary handling and metrics as described above. Use an independent and fixed seed for the random number generator in each test case to ensure reproducibility. No physical units are involved. All angles, if any, are not applicable.\n\nTest suite and parameters to implement:\n\n- Case $1$ (happy path, reflect): $d=5$, $l=-5$, $u=5$, $a = [1.5, -2.0, 0.5, -1.0, 2.0]$, $N=30$, $T=200$, $w=0.7$, $c_1=1.5$, $c_2=1.5$, $v_{\\max}=5$, boundary scheme = reflect, seed $=42$, $\\varepsilon = 10^{-6}$.\n\n- Case $2$ (near upper boundary, absorb): $d=5$, $l=-5$, $u=5$, $a = [4.9, 4.8, 4.7, 4.6, 4.5]$, $N=30$, $T=200$, $w=0.7$, $c_1=1.5$, $c_2=1.5$, $v_{\\max}=5$, boundary scheme = absorb, seed $=123$, $\\varepsilon = 10^{-6}$.\n\n- Case $3$ (unattainable zero within domain, random restart): $d=5$, $l=-2$, $u=2$, $a = [10.0, 10.0, 10.0, 10.0, 10.0]$, $N=40$, $T=300$, $w=0.8$, $c_1=1.7$, $c_2=1.7$, $v_{\\max}=4$, boundary scheme = random restart, seed $=7$, $\\varepsilon = 10^{-6}$.\n\n- Case $4$ (low dimension, overshoot, reflect): $d=2$, $l=-1$, $u=1$, $a = [0.99, -0.99]$, $N=10$, $T=150$, $w=0.9$, $c_1=2.05$, $c_2=2.05$, $v_{\\max}=2$, boundary scheme = reflect, seed $=999$, $\\varepsilon = 10^{-6}$.\n\n- Case $5$ (small domain, absorb): $d=3$, $l=0$, $u=1$, $a = [0.9, 0.1, 0.5]$, $N=12$, $T=150$, $w=0.6$, $c_1=1.4$, $c_2=1.4$, $v_{\\max}=1$, boundary scheme = absorb, seed $=2024$, $\\varepsilon = 10^{-6}$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list $[f^\\star, r_{\\mathrm{feasible}}, \\text{converged}, t^\\star]$ for the test cases in the order given above. For example, the output should look like $$[[f^\\star_1, r_{\\mathrm{feasible},1}, \\text{converged}_1, t^\\star_1], [f^\\star_2, r_{\\mathrm{feasible},2}, \\text{converged}_2, t^\\star_2], \\dots].$$ The types must be as follows: $f^\\star$ is a real number (float), $r_{\\mathrm{feasible}}$ is a real number (float) in $[0,1]$, $\\text{converged}$ is a boolean, and $t^\\star$ is an integer with $-1$ indicating no convergence within $T$ iterations.", "solution": "We are minimizing the shifted sphere function $$f(x) = \\sum_{j=1}^{d} (x_j - a_j)^2$$ subject to the box constraint $x \\in \\Omega = [l,u]^d$. The shifted sphere function is strictly convex with a unique unconstrained minimizer at $x^\\star = a$. Under box constraints, the minimizer is the projection of $a$ onto $\\Omega$, so the constrained minimizer is $x^\\star_\\Omega = \\tilde{a}$ where $\\tilde{a}_j = \\min(\\max(a_j, l), u)$ for each $j$, and the minimum objective value is $$f(x^\\star_\\Omega) = \\sum_{j=1}^{d} (\\tilde{a}_j - a_j)^2.$$ If $a \\in \\Omega$, then $x^\\star_\\Omega = a$ and $f(x^\\star_\\Omega) = 0$; if any component of $a$ lies outside the bounds, the minimum within $\\Omega$ is strictly positive.\n\nParticle Swarm Optimization (PSO) is a population-based stochastic optimization method. It maintains a set of $N$ particles, each with position $x_i(t) \\in \\mathbb{R}^d$ and velocity $v_i(t) \\in \\mathbb{R}^d$ at iteration $t$. The update rule blends inertia, attraction to the particle's personal best $p_i(t)$, and attraction to the global best $g(t)$. This design embodies two fundamental principles: exploration via inertia and stochasticity, and exploitation via attraction to known good solutions. The canonical inertial PSO update is given by $$v_i(t+1) = w \\, v_i(t) + c_1 \\, r_1(t) \\odot (p_i(t) - x_i(t)) + c_2 \\, r_2(t) \\odot (g(t) - x_i(t)),$$ $$x_i(t+1) = x_i(t) + v_i(t+1),$$ with independent uniform random vectors $r_1(t), r_2(t) \\in [0,1]^d$. Velocity clamping ensures bounded step sizes: $$v_{ij}(t+1) \\leftarrow \\operatorname{clip}(v_{ij}(t+1), -v_{\\max}, v_{\\max}).$$ Initial positions $x_i(0)$ are drawn uniformly from $\\Omega$ and initial velocities $v_i(0)$ are drawn uniformly from $[-v_{\\max}, v_{\\max}]^d$. After each position update, we evaluate $f(x_i(t+1))$ and update the personal best $p_i(t+1)$ and the global best $g(t+1)$ if improvements occur.\n\nThe presence of constraints requires boundary handling when an attempted position $x_i^{\\mathrm{prop}}(t+1) = x_i(t) + v_i(t+1)$ leaves $\\Omega$. We examine three schemes:\n\n1. Reflect: The reflective mapping enforces a mirror boundary, which preserves exploration while preventing particles from leaving the domain. For a single component $j$, let $$z = \\frac{x_{ij}^{\\mathrm{prop}} - l}{u - l}, \\quad k = \\left\\lfloor z \\right\\rfloor, \\quad z_{\\mathrm{frac}} = z - k.$$ If $k$ is even, we set $$x_{ij}(t+1) = l + z_{\\mathrm{frac}}(u - l);$$ if $k$ is odd, we set $$x_{ij}(t+1) = u - z_{\\mathrm{frac}}(u - l).$$ The parity of $k$ counts the number of interval crossings; when $k$ is odd, the component has bounced an odd number of times, so we flip the velocity sign: $$v_{ij}(t+1) \\leftarrow -v_{ij}(t+1).$$ This mapping correctly handles arbitrary overshoots, including those larger than the interval width $u - l$.\n\n2. Absorb: The absorbing boundary clamps the position to the nearest boundary and resets the corresponding velocity component to zero when a violation occurs: $$x_{ij}(t+1) = \\begin{cases} l & \\text{if } x_{ij}^{\\mathrm{prop}} < l, \\\\ u & \\text{if } x_{ij}^{\\mathrm{prop}} > u, \\\\ x_{ij}^{\\mathrm{prop}} & \\text{otherwise}, \\end{cases} \\quad v_{ij}(t+1) = \\begin{cases} 0 & \\text{if } x_{ij}^{\\mathrm{prop}} \\notin [l,u], \\\\ v_{ij}(t+1) & \\text{otherwise}. \\end{cases}$$ This scheme discourages boundary violations by damping motion upon impact, potentially reducing oscillations but also diminishing exploration near the boundary.\n\n3. Random restart: The random restart boundary replaces an out-of-bounds component with a new random value drawn uniformly from $[l,u]$ and sets its velocity to zero: $$x_{ij}(t+1) \\sim \\mathcal{U}(l,u) \\quad \\text{if } x_{ij}^{\\mathrm{prop}} \\notin [l,u], \\quad v_{ij}(t+1) \\leftarrow 0.$$ This scheme promotes exploration by reintroducing diversity when violations occur, which can help escape stagnation but may slow convergence near precise boundary optima.\n\nTo quantify the effect of each scheme, we measure two metrics:\n\n- Attempted feasibility rate $$r_{\\mathrm{feasible}} = \\frac{\\#\\{(i,t) \\mid x_i(t) + v_i(t+1) \\in \\Omega\\}}{N \\cdot T},$$ which counts proposals before correction and reflects how often particles attempt positions inside $\\Omega$ under the dynamics induced by each boundary rule.\n\n- Convergence behavior: a boolean indicating whether the global best reaches the threshold $\\varepsilon$ at any iteration, and the earliest iteration $t^\\star$ where $$\\min_i f(p_i(t^\\star)) \\le \\varepsilon.$$ We also report the final best objective value $$f^\\star = \\min_i f(p_i(T)).$$\n\nImplementation details:\n\n- Randomness is controlled by a fixed seed per test case using a pseudorandom number generator to ensure reproducible results.\n\n- Personal bests $p_i(t)$ update only on strict improvements of $f(x)$, ensuring monotonic nonincreasing personal best values.\n\n- The global best $g(t)$ tracks the best among personal bests at each iteration.\n\n- Velocities are clamped componentwise to $[-v_{\\max}, v_{\\max}]$ after computation of $v_i(t+1)$ and before position proposals to prevent excessive overshoot.\n\n- Boundary handling schemes are applied componentwise to $x_i^{\\mathrm{prop}}(t+1)$ to yield the final $x_i(t+1)$ and the corrected $v_i(t+1)$, ensuring $x_i(t+1) \\in \\Omega$.\n\nDesign rationale:\n\n- The shifted sphere function’s quadratic structure provides a smooth landscape with a unique minimum, making it ideal for observing PSO dynamics and the interplay between exploration and exploitation.\n\n- Reflect preserves momentum while maintaining feasibility, often improving $r_{\\mathrm{feasible}}$ by reducing extended excursions and can speed convergence when the optimum lies near a boundary.\n\n- Absorb reduces oscillations by nullifying velocity at the boundary, which may decrease boundary crossings, increasing $r_{\\mathrm{feasible}}$, but can slow convergence near boundary optima due to loss of momentum.\n\n- Random restart increases diversity when violations occur, often increasing attempted violations early in the run but potentially aiding convergence in multimodal problems; for the convex shifted sphere, it can slow precise convergence near boundaries yet is effective when the unconstrained minimizer lies outside $\\Omega$, encouraging exploration near the constrained minimizer $x^\\star_\\Omega$.\n\nComputational considerations:\n\n- Each update costs $O(N d)$ operations, and the total runtime is $O(N d T)$ per test case.\n\n- With the given parameters, the program remains computationally lightweight and fully deterministic.\n\nThe program processes the five specified test cases, computes $f^\\star$, $r_{\\mathrm{feasible}}$, the convergence boolean, and $t^\\star$, and outputs a single line with the list of these four-tuple results for each case in order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef sphere_shifted(x, a):\n    # f(x) = sum_j (x_j - a_j)^2\n    diff = x - a\n    return np.dot(diff, diff)\n\ndef reflect_component(x_prop, v_comp, l, u):\n    # Reflective mapping with parity-based velocity flip\n    if l <= x_prop <= u:\n        return x_prop, v_comp\n    width = u - l\n    # Compute z, k = floor(z), z_frac = z - k\n    z = (x_prop - l) / width\n    k = math.floor(z)\n    z_frac = z - k\n    if k % 2 == 0:\n        x_new = l + z_frac * width\n        v_new = v_comp\n    else:\n        x_new = u - z_frac * width\n        v_new = -v_comp\n    # Numerical guard to ensure bounds inclusion\n    if x_new < l:\n        x_new = l\n    elif x_new > u:\n        x_new = u\n    return x_new, v_new\n\ndef absorb_component(x_prop, v_comp, l, u):\n    if x_prop < l:\n        return l, 0.0\n    elif x_prop > u:\n        return u, 0.0\n    else:\n        return x_prop, v_comp\n\ndef restart_component(x_prop, v_comp, l, u, rng):\n    if l <= x_prop <= u:\n        return x_prop, v_comp\n    else:\n        return rng.uniform(l, u), 0.0\n\ndef pso_run(d, l, u, a, N, T, w, c1, c2, vmax, scheme, seed, eps):\n    rng = np.random.default_rng(seed)\n    # Initialize positions uniformly in [l, u]^d\n    X = rng.uniform(l, u, size=(N, d))\n    # Initialize velocities uniformly in [-vmax, vmax]^d\n    V = rng.uniform(-vmax, vmax, size=(N, d))\n    # Personal bests and fitnesses\n    P = X.copy()\n    P_fit = np.array([sphere_shifted(P[i], a) for i in range(N)])\n    # Global best\n    g_idx = int(np.argmin(P_fit))\n    G = P[g_idx].copy()\n    G_fit = P_fit[g_idx]\n    # Metrics\n    total_props = N * T\n    feasible_props = 0\n    converged = False\n    t_star = -1\n\n    for t in range(1, T + 1):\n        for i in range(N):\n            # Random coefficients per particle and dimension\n            r1 = rng.uniform(0.0, 1.0, size=d)\n            r2 = rng.uniform(0.0, 1.0, size=d)\n            # Velocity update\n            V[i] = w * V[i] + c1 * r1 * (P[i] - X[i]) + c2 * r2 * (G - X[i])\n            # Clamp velocity\n            V[i] = np.clip(V[i], -vmax, vmax)\n            # Proposed position\n            X_prop = X[i] + V[i]\n            # Feasibility check before correction (attempted feasibility)\n            inside = (X_prop >= l - 1e-12) & (X_prop <= u + 1e-12)\n            if np.all(inside):\n                feasible_props += 1\n            # Apply boundary handling per component\n            for j in range(d):\n                xpj = X_prop[j]\n                vj = V[i, j]\n                if scheme == 'reflect':\n                    x_new, v_new = reflect_component(xpj, vj, l, u)\n                elif scheme == 'absorb':\n                    x_new, v_new = absorb_component(xpj, vj, l, u)\n                elif scheme == 'restart':\n                    x_new, v_new = restart_component(xpj, vj, l, u, rng)\n                else:\n                    # Default: clamp (should not happen in provided tests)\n                    x_new = min(max(xpj, l), u)\n                    v_new = vj\n                X[i, j] = x_new\n                V[i, j] = v_new\n            # Evaluate and update personal best\n            f_val = sphere_shifted(X[i], a)\n            if f_val < P_fit[i]:\n                P[i] = X[i].copy()\n                P_fit[i] = f_val\n        # Update global best\n        g_idx = int(np.argmin(P_fit))\n        if P_fit[g_idx] < G_fit:\n            G_fit = P_fit[g_idx]\n            G = P[g_idx].copy()\n        # Convergence check\n        if (not converged) and (G_fit <= eps):\n            converged = True\n            t_star = t\n\n    r_feasible = feasible_props / total_props\n    f_star = G_fit\n    return [f_star, r_feasible, converged, t_star]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, reflect\n        {\n            \"d\": 5, \"l\": -5.0, \"u\": 5.0,\n            \"a\": np.array([1.5, -2.0, 0.5, -1.0, 2.0], dtype=float),\n            \"N\": 30, \"T\": 200,\n            \"w\": 0.7, \"c1\": 1.5, \"c2\": 1.5,\n            \"vmax\": 5.0, \"scheme\": \"reflect\",\n            \"seed\": 42, \"eps\": 1e-6\n        },\n        # Case 2: near upper boundary, absorb\n        {\n            \"d\": 5, \"l\": -5.0, \"u\": 5.0,\n            \"a\": np.array([4.9, 4.8, 4.7, 4.6, 4.5], dtype=float),\n            \"N\": 30, \"T\": 200,\n            \"w\": 0.7, \"c1\": 1.5, \"c2\": 1.5,\n            \"vmax\": 5.0, \"scheme\": \"absorb\",\n            \"seed\": 123, \"eps\": 1e-6\n        },\n        # Case 3: unattainable zero within domain, random restart\n        {\n            \"d\": 5, \"l\": -2.0, \"u\": 2.0,\n            \"a\": np.array([10.0, 10.0, 10.0, 10.0, 10.0], dtype=float),\n            \"N\": 40, \"T\": 300,\n            \"w\": 0.8, \"c1\": 1.7, \"c2\": 1.7,\n            \"vmax\": 4.0, \"scheme\": \"restart\",\n            \"seed\": 7, \"eps\": 1e-6\n        },\n        # Case 4: low dimension, overshoot, reflect\n        {\n            \"d\": 2, \"l\": -1.0, \"u\": 1.0,\n            \"a\": np.array([0.99, -0.99], dtype=float),\n            \"N\": 10, \"T\": 150,\n            \"w\": 0.9, \"c1\": 2.05, \"c2\": 2.05,\n            \"vmax\": 2.0, \"scheme\": \"reflect\",\n            \"seed\": 999, \"eps\": 1e-6\n        },\n        # Case 5: small domain, absorb\n        {\n            \"d\": 3, \"l\": 0.0, \"u\": 1.0,\n            \"a\": np.array([0.9, 0.1, 0.5], dtype=float),\n            \"N\": 12, \"T\": 150,\n            \"w\": 0.6, \"c1\": 1.4, \"c2\": 1.4,\n            \"vmax\": 1.0, \"scheme\": \"absorb\",\n            \"seed\": 2024, \"eps\": 1e-6\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        res = pso_run(\n            d=case[\"d\"], l=case[\"l\"], u=case[\"u\"], a=case[\"a\"], N=case[\"N\"], T=case[\"T\"],\n            w=case[\"w\"], c1=case[\"c1\"], c2=case[\"c2\"], vmax=case[\"vmax\"],\n            scheme=case[\"scheme\"], seed=case[\"seed\"], eps=case[\"eps\"]\n        )\n        results.append(res)\n\n    # Final print statement in the exact required format (single line, no spaces).\n    # Convert to string and remove spaces for a compact bracketed list.\n    out = str(results).replace(\" \", \"\")\n    print(out)\n\nsolve()\n```", "id": "3161093"}]}