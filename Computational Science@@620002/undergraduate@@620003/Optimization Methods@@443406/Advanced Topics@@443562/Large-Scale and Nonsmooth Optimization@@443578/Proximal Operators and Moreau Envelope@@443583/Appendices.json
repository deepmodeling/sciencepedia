{"hands_on_practices": [{"introduction": "Proximal operators form the backbone of many modern optimization algorithms, particularly those designed to handle nonsmooth functions common in machine learning. This exercise provides a foundational workout by asking you to derive the proximal operator for the squared hinge loss, a function central to the formulation of Support Vector Machines. By working from first principles of convex analysis, you will see how to handle a piecewise function and apply optimality conditions to find an exact, closed-form solution for the operator [@problem_id:3167994]. This practice will not only solidify your understanding of the core definition but also give you a clear look at how the corresponding Moreau envelope acts as a smoothing tool, with the parameter $\\lambda$ controlling the degree of curvature.", "problem": "Consider the squared hinge loss function defined on the real line by $f(x) = \\max(0, 1 - x)^{2}$. Let $\\lambda > 0$ be a fixed parameter. Use the definitions of the proximal operator and the Moreau envelope as foundational starting points:\n- The proximal operator of a proper lower semicontinuous convex function $f$ is defined by $\\operatorname{prox}_{\\lambda f}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ f(x) + \\frac{1}{2 \\lambda} (x - v)^{2} \\right\\}$.\n- The Moreau envelope of $f$ with parameter $\\lambda$ is defined by $e_{\\lambda} f(v) = \\min_{x \\in \\mathbb{R}} \\left\\{ f(x) + \\frac{1}{2 \\lambda} (x - v)^{2} \\right\\}$.\n\nStarting from these definitions and first principles in convex optimization, derive an explicit, piecewise-analytic expression for the proximal mapping $\\operatorname{prox}_{\\lambda f}(v)$ as a function of the scalar input $v \\in \\mathbb{R}$ and parameter $\\lambda > 0$, and show how the Moreau envelope $e_{\\lambda} f(v)$ simplifies. Then, based on your derivation, critically compare the effect of varying $\\lambda$ in the Moreau envelope to the effect of applying a standard quadratic smoothing to the hinge region, explaining how $\\lambda$ controls the degree of smoothing and curvature without relying on pre-stated formulas.\n\nYour final answer must be the single closed-form expression (as a function of $v$ and $\\lambda$) of the proximal mapping $\\operatorname{prox}_{\\lambda f}(v)$, written as a piecewise expression.", "solution": "The problem statement submitted for consideration is a well-posed exercise in convex optimization. It provides all necessary definitions and asks for the derivation of the proximal operator and Moreau envelope for a specific function, followed by an analytical interpretation.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Function: $f(x) = \\max(0, 1 - x)^{2}$ for $x \\in \\mathbb{R}$.\n- Parameter: $\\lambda > 0$.\n- Definition of Proximal Operator: $\\operatorname{prox}_{\\lambda f}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ f(x) + \\frac{1}{2 \\lambda} (x - v)^{2} \\right\\}$.\n- Definition of Moreau Envelope: $e_{\\lambda} f(v) = \\min_{x \\in \\mathbb{R}} \\left\\{ f(x) + \\frac{1}{2 \\lambda} (x - v)^{2} \\right\\}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with standard, well-established concepts in convex analysis and optimization theory (proximal operators, Moreau envelopes, squared hinge loss). The function $f(x)$ is a proper, lower semicontinuous, and convex function on $\\mathbb{R}$. The objective function to be minimized, $h(x; v, \\lambda) = f(x) + \\frac{1}{2 \\lambda} (x - v)^{2}$, is the sum of a convex function and a strictly convex function (since $\\lambda > 0$), which makes $h(x; v, \\lambda)$ strictly convex. This guarantees that the minimizer exists and is unique, ensuring that the proximal operator is well-posed. The language is objective and the setup is complete and internally consistent.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n**Derivation of the Proximal Operator $\\operatorname{prox}_{\\lambda f}(v)$**\n\nThe proximal operator $\\operatorname{prox}_{\\lambda f}(v)$ is the value of $x$ that minimizes the objective function:\n$$\nh(x; v, \\lambda) = f(x) + \\frac{1}{2 \\lambda} (x - v)^{2}\n$$\nThe function $f(x)$ has a piecewise definition:\n$$\nf(x) = \\begin{cases} (1 - x)^{2} & \\text{if } x < 1 \\\\ 0 & \\text{if } x \\ge 1 \\end{cases}\n$$\nThe function $f(x)$ is differentiable everywhere. Its derivative is:\n$$\nf'(x) = \\begin{cases} \\frac{d}{dx}(1 - x)^{2} = 2(1 - x)(-1) = 2x - 2 & \\text{if } x < 1 \\\\ 0 & \\text{if } x \\ge 1 \\end{cases}\n$$\nThe derivative is continuous at $x=1$, as $f'(1^{-}) = 2(1)-2 = 0$ and $f'(1^{+}) = 0$.\nSince the objective function $h(x; v, \\lambda)$ is convex and differentiable, its unique minimizer $x^*$ is found by setting its derivative with respect to $x$ to zero:\n$$\n\\frac{d}{dx}h(x; v, \\lambda) = f'(x) + \\frac{1}{\\lambda}(x - v) = 0\n$$\nWe analyze this condition based on the piecewise nature of $f'(x)$.\n\nCase 1: The minimizer $x^*$ satisfies $x^* < 1$.\nIn this region, the necessary condition for optimality is:\n$$\n(2x^* - 2) + \\frac{1}{\\lambda}(x^* - v) = 0\n$$\nMultiplying by $\\lambda$ to clear the denominator yields:\n$$\n2\\lambda(x^* - 1) + x^* - v = 0\n$$\n$$\n2\\lambda x^* - 2\\lambda + x^* - v = 0\n$$\n$$\nx^*(2\\lambda + 1) = v + 2\\lambda\n$$\n$$\nx^* = \\frac{v + 2\\lambda}{1 + 2\\lambda}\n$$\nThis solution is valid only if it lies within the assumed region, i.e., $x^* < 1$.\n$$\n\\frac{v + 2\\lambda}{1 + 2\\lambda} < 1\n$$\nSince $\\lambda > 0$, the denominator $1+2\\lambda$ is positive. We can multiply both sides by it without changing the inequality's direction:\n$$\nv + 2\\lambda < 1 + 2\\lambda\n$$\n$$\nv < 1\n$$\nThus, if $v < 1$, the unique global minimizer is $x^* = \\frac{v + 2\\lambda}{1 + 2\\lambda}$.\n\nCase 2: The minimizer $x^*$ satisfies $x^* \\ge 1$.\nIn this region, the derivative of $h(x; v, \\lambda)$ is:\n$$\n\\frac{d}{dx}h(x; v, \\lambda) = 0 + \\frac{1}{\\lambda}(x - v) = \\frac{x - v}{\\lambda}\n$$\nIf we attempt to set this to zero, we get $x = v$. This is a potential minimizer provided it lies in the assumed region, i.e., $v \\ge 1$. If $v \\ge 1$, then $x^* = v$ is a stationary point within the domain $[1, \\infty)$.\nTo confirm this is the global minimum, we must check the behavior of the derivative over the entire real line for the case $v \\ge 1$.\n- For $x \\ge 1$: The derivative is $\\frac{x-v}{\\lambda}$. It is negative for $x < v$, zero at $x=v$, and positive for $x > v$. The function $h(x)$ has its minimum at $x=v$ in this region.\n- For $x < 1$: The derivative is $2(x-1) + \\frac{1}{\\lambda}(x-v)$. Since $x < 1$ and we are in the case $v \\ge 1$, we have $x-1 < 0$ and $x-v < 0$. Therefore, the derivative is a sum of two negative terms, so it is strictly negative for all $x < 1$.\nThis means that for $v \\ge 1$, the function $h(x; v, \\lambda)$ is strictly decreasing on $(-\\infty, 1)$ and has a unique minimum on $[1, \\infty)$ at $x=v$. The global minimum must therefore be $x^* = v$.\n\nCombining these two cases, the proximal operator is given by the piecewise expression:\n$$\n\\operatorname{prox}_{\\lambda f}(v) = \\begin{cases} \\frac{v + 2\\lambda}{1 + 2\\lambda} & \\text{if } v < 1 \\\\ v & \\text{if } v \\ge 1 \\end{cases}\n$$\n\n**The Moreau Envelope $e_{\\lambda} f(v)$ and its Interpretation**\n\nThe Moreau envelope is the value of the objective function at the minimizer $x^* = \\operatorname{prox}_{\\lambda f}(v)$:\n$$\ne_{\\lambda} f(v) = f(x^*) + \\frac{1}{2 \\lambda} (x^* - v)^{2}\n$$\nWe evaluate this for the two cases determined above.\n\nCase 1: $v < 1$.\nThe minimizer is $x^* = \\frac{v + 2\\lambda}{1 + 2\\lambda}$. As shown earlier, if $v<1$, then $x^* < 1$.\nWe compute the two terms for the envelope:\n$$\nf(x^*) = (1 - x^*)^{2} = \\left(1 - \\frac{v + 2\\lambda}{1 + 2\\lambda}\\right)^{2} = \\left(\\frac{1 + 2\\lambda - v - 2\\lambda}{1 + 2\\lambda}\\right)^{2} = \\left(\\frac{1 - v}{1 + 2\\lambda}\\right)^{2} = \\frac{(1 - v)^{2}}{(1 + 2\\lambda)^{2}}\n$$\n$$\n(x^* - v)^{2} = \\left(\\frac{v + 2\\lambda}{1 + 2\\lambda} - v\\right)^{2} = \\left(\\frac{v + 2\\lambda - v(1 + 2\\lambda)}{1 + 2\\lambda}\\right)^{2} = \\left(\\frac{2\\lambda(1 - v)}{1 + 2\\lambda}\\right)^{2} = \\frac{4\\lambda^{2}(1 - v)^{2}}{(1 + 2\\lambda)^{2}}\n$$\nSubstituting these into the expression for $e_{\\lambda} f(v)$:\n$$\ne_{\\lambda} f(v) = \\frac{(1 - v)^{2}}{(1 + 2\\lambda)^{2}} + \\frac{1}{2\\lambda} \\frac{4\\lambda^{2}(1 - v)^{2}}{(1 + 2\\lambda)^{2}} = \\frac{(1 - v)^{2}}{(1 + 2\\lambda)^{2}} + \\frac{2\\lambda(1 - v)^{2}}{(1 + 2\\lambda)^{2}} = \\frac{(1 + 2\\lambda)(1 - v)^{2}}{(1 + 2\\lambda)^{2}} = \\frac{(1 - v)^{2}}{1 + 2\\lambda}\n$$\n\nCase 2: $v \\ge 1$.\nThe minimizer is $x^* = v$. Since $v \\ge 1$, we have $x^* \\ge 1$.\n$$\nf(x^*) = f(v) = 0\n$$\n$$\n\\frac{1}{2\\lambda}(x^* - v)^{2} = \\frac{1}{2\\lambda}(v - v)^{2} = 0\n$$\nTherefore, $e_{\\lambda} f(v) = 0$.\n\nCombining these results, the Moreau envelope is:\n$$\ne_{\\lambda} f(v) = \\begin{cases} \\frac{(1 - v)^{2}}{1 + 2\\lambda} & \\text{if } v < 1 \\\\ 0 & \\text{if } v \\ge 1 \\end{cases} = \\frac{\\max(0, 1-v)^2}{1+2\\lambda} = \\frac{f(v)}{1+2\\lambda}\n$$\nThis remarkably simple result shows that the Moreau envelope of this specific function is a scaled version of the function itself.\n\n**Comparison with Quadratic Smoothing**\n\nThe Moreau envelope is a form of smoothing. We analyze its effect by examining the curvature (second derivative) of $e_{\\lambda} f(v)$ and comparing it to that of the original function $f(v)$.\nThe original function $f(v)$ has a discontinuous second derivative at $v=1$:\n$$\nf''(v) = \\begin{cases} 2 & \\text{if } v < 1 \\\\ 0 & \\text{if } v > 1 \\end{cases}\n$$\nThe Moreau envelope $e_{\\lambda} f(v)$ has the second derivative:\n$$\n(e_{\\lambda}f)''(v) = \\begin{cases} \\frac{2}{1 + 2\\lambda} & \\text{if } v < 1 \\\\ 0 & \\text{if } v > 1 \\end{cases}\n$$\nThe Moreau envelope is also only $C^1$, retaining the discontinuity in the second derivative at $v=1$. A \"standard quadratic smoothing,\" often conceived as a technique to increase the order of differentiability (e.g., via convolution with a smooth, compact kernel), would typically \"round off\" the corner at $v=1$, making the function $C^2$ or smoother. The Moreau envelope does not do this; the location of the non-smoothness is preserved.\n\nInstead, the smoothing effect of the Moreau envelope, as controlled by $\\lambda$, manifests as a change in curvature.\nThe parameter $\\lambda$ directly controls the degree of this smoothing:\n1.  **Curvature Control**: For $v < 1$, the curvature of $e_{\\lambda} f(v)$ is $\\frac{2}{1 + 2\\lambda}$, which is always less than the original curvature of $2$. As $\\lambda \\to 0^+$, the curvature approaches $2$, and $e_{\\lambda} f(v) \\to f(v)$, meaning the smoothing effect vanishes. As $\\lambda \\to \\infty$, the curvature approaches $0$, making the function flatter. This corresponds to maximum smoothing. Thus, $\\lambda$ acts as an inverse-curvature or a direct smoothing parameter: larger $\\lambda$ implies a more heavily smoothed function.\n2.  **Mechanism**: This effect arises from the definition of the envelope as an infimal convolution with a quadratic, $x \\mapsto \\frac{1}{2\\lambda}\\|x\\|^2$. The curvature of this quadratic is $\\frac{1}{\\lambda}$. A large $\\lambda$ corresponds to a very flat quadratic, and convolving with a flatter function produces a flatter result.\n\nIn summary, for the function $f(x) = \\max(0, 1-x)^2$, the Moreau-Yosida smoothing does not create a $C^2$ function by locally altering the neighborhood of $v=1$. Instead, it globally reduces the curvature of the active quadratic part of the function, with the parameter $\\lambda$ precisely controlling this reduction.", "answer": "$$\n\\boxed{\n\\operatorname{prox}_{\\lambda f}(v) = \\begin{cases} \\frac{v + 2\\lambda}{1 + 2\\lambda} & \\text{if } v < 1 \\\\ v & \\text{if } v \\ge 1 \\end{cases}\n}\n$$", "id": "3167994"}, {"introduction": "Beyond its role in algorithms, the Moreau envelope provides a powerful theoretical lens for understanding nonsmooth optimization. It constructs a smooth approximation of a function, whose properties are deeply connected to the original. This practice explores this connection by examining the gradient flow on the Moreau envelope of the simple absolute value function, $f(x)=|x|$. You will derive the famous Huber loss as the envelope and then solve the resulting ordinary differential equation (ODE) that describes the trajectory of gradient descent on this smooth surface [@problem_id:3167965]. Comparing this trajectory to the subgradient flow of the original nonsmooth function reveals precisely how Moreau-Yosida regularization tames nonsmoothness, replacing abrupt stops with smooth, asymptotic convergence.", "problem": "Consider the convex function $f:\\mathbb{R}\\to\\mathbb{R}$ given by $f(x)=|x|$, and fix a smoothing parameter $\\lambda>0$. The Moreau envelope $e_{\\lambda}f:\\mathbb{R}\\to\\mathbb{R}$ is defined by\n$$\ne_{\\lambda}f(x)=\\min_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}\\left(y-x\\right)^{2}\\right\\}.\n$$\nStarting from this definition and the basic properties of convex functions and minimizers, derive an explicit expression for $e_{\\lambda}f(x)$ and its gradient $\\nabla e_{\\lambda}f(x)$.\n\nThen, consider the gradient flow Ordinary Differential Equation (ODE) $\\dot{x}(t)=-\\nabla e_{\\lambda}f\\left(x(t)\\right)$ with initial condition $x(0)=x_{0}\\in\\mathbb{R}$. Solve this ODE explicitly for all $t\\geq 0$ and express your solution as a closed-form function of $t$, $\\lambda$, and $x_{0}$.\n\nFinally, compare the behavior of this solution to the subgradient flow for $f$, namely the differential inclusion $\\dot{x}(t)\\in-\\partial f\\left(x(t)\\right)$ with $x(0)=x_{0}$. Discuss whether the flow reaches $x=0$ in finite or infinite time and the qualitative differences in the trajectories, grounding your reasoning in the derived expressions.\n\nProvide your final explicit formula for $x(t)$ solving $\\dot{x}=-\\nabla e_{\\lambda}f(x)$, expressed in terms of $t$, $\\lambda$, and $x_{0}$. No numerical approximation is required; your answer must be an exact closed-form expression.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective, with all necessary components provided for a rigorous solution within the field of convex optimization.\n\nThe problem asks for three main components: the derivation of the Moreau envelope of $f(x)=|x|$ and its gradient, the solution to the gradient flow ODE based on this envelope, and a comparison with the subgradient flow of the original function $f(x)$.\n\nFirst, we derive the Moreau envelope $e_{\\lambda}f(x)$ for $f(x)=|x|$. The definition is\n$$\ne_{\\lambda}f(x)=\\min_{y\\in\\mathbb{R}}\\left\\{|y|+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}\n$$\nThe minimizer, denoted $y^* = \\text{prox}_{\\lambda f}(x)$, is found by setting the subgradient of the objective function with respect to $y$ to zero. The objective is strictly convex, guaranteeing a unique minimizer. The subgradient condition is $0 \\in \\partial|y^*| + \\frac{1}{\\lambda}(y^*-x)$, which rearranges to $x-y^* \\in \\lambda\\partial|y^*|$.\nThe subgradient of $|y|$ at $y^*$ is $\\{1\\}$ if $y^*>0$, $\\{-1\\}$ if $y^*<0$, and $[-1, 1]$ if $y^*=0$.\nCase 1: $y^*>0$. $x-y^* = \\lambda$, so $y^*=x-\\lambda$. This holds if $x-\\lambda>0$, i.e., $x > \\lambda$.\nCase 2: $y^*<0$. $x-y^* = -\\lambda$, so $y^*=x+\\lambda$. This holds if $x+\\lambda<0$, i.e., $x < -\\lambda$.\nCase 3: $y^*=0$. $x-0 \\in \\lambda[-1, 1]$, so $|x| \\le \\lambda$.\nThe minimizer is the soft-thresholding operator:\n$$\ny^*(x) = \\text{prox}_{\\lambda f}(x) = \\begin{cases} x-\\lambda & \\text{if } x > \\lambda \\\\ 0 & \\text{if } |x| \\leq \\lambda \\\\ x+\\lambda & \\text{if } x < -\\lambda \\end{cases}\n$$\nSubstituting $y^*$ into the definition of $e_{\\lambda}f(x)$:\nIf $x > \\lambda$: $e_{\\lambda}f(x) = |x-\\lambda| + \\frac{1}{2\\lambda}((x-\\lambda)-x)^2 = x-\\lambda + \\frac{\\lambda^2}{2\\lambda} = x - \\frac{\\lambda}{2}$.\nIf $x < -\\lambda$: $e_{\\lambda}f(x) = |x+\\lambda| + \\frac{1}{2\\lambda}((x+\\lambda)-x)^2 = -(x+\\lambda) + \\frac{\\lambda^2}{2\\lambda} = -x - \\frac{\\lambda}{2}$.\nIf $|x| \\le \\lambda$: $e_{\\lambda}f(x) = |0| + \\frac{1}{2\\lambda}(0-x)^2 = \\frac{x^2}{2\\lambda}$.\nThis gives the Moreau envelope, a function known as the Huber loss:\n$$\ne_{\\lambda}f(x) = \\begin{cases} x - \\frac{\\lambda}{2} & \\text{if } x > \\lambda \\\\ \\frac{x^2}{2\\lambda} & \\text{if } |x| \\leq \\lambda \\\\ -x - \\frac{\\lambda}{2} & \\text{if } x < -\\lambda \\end{cases}\n$$\nThe gradient $\\nabla e_{\\lambda}f(x)$ is found by differentiating $e_{\\lambda}f(x)$ or using Moreau's identity $\\nabla e_{\\lambda}f(x) = \\frac{1}{\\lambda}(x - \\text{prox}_{\\lambda f}(x))$:\n$$\n\\nabla e_{\\lambda}f(x) = \\begin{cases} 1 & \\text{if } x > \\lambda \\\\ \\frac{x}{\\lambda} & \\text{if } |x| \\leq \\lambda \\\\ -1 & \\text{if } x < -\\lambda \\end{cases}\n$$\nThe function $e_{\\lambda}f(x)$ is continuously differentiable.\n\nSecond, we solve the gradient flow ODE $\\dot{x}(t) = -\\nabla e_{\\lambda}f(x(t))$ with $x(0)=x_0$.\nCase 1: $|x_0| \\le \\lambda$. The ODE is $\\dot{x} = -x/\\lambda$. This is a separable linear ODE with solution $x(t) = x_0 \\exp(-t/\\lambda)$. As $|x(t)| = |x_0|\\exp(-t/\\lambda) \\le |x_0| \\le \\lambda$, the solution remains in this region for all $t \\ge 0$.\nCase 2: $|x_0| > \\lambda$. Let's assume $x_0 > \\lambda$. Initially, $x(t) > \\lambda$, so the ODE is $\\dot{x}=-1$. The solution is $x(t) = x_0 - t$. This holds until $x(t)$ reaches $\\lambda$, which occurs at time $t_1 = x_0 - \\lambda$. At this time, $x(t_1) = \\lambda$. For $t > t_1$, the dynamics switch to $\\dot{x} = -x/\\lambda$. The solution for $t > t_1$ is $x(t) = C \\exp(-t/\\lambda)$. Using the condition $x(t_1)=\\lambda$: $\\lambda = C \\exp(-t_1/\\lambda) = C \\exp(-(x_0-\\lambda)/\\lambda)$. So, $C = \\lambda \\exp((x_0-\\lambda)/\\lambda)$. The solution for $t > t_1$ is $x(t) = \\lambda \\exp((x_0-\\lambda)/\\lambda) \\exp(-t/\\lambda) = \\lambda \\exp\\left(-\\frac{t-(x_0-\\lambda)}{\\lambda}\\right)$.\nBy symmetry, if $x_0 < -\\lambda$, the solution is $x(t) = x_0+t$ until $t_1=-x_0-\\lambda$ where $x(t_1)=-\\lambda$, followed by $x(t) = -\\lambda \\exp\\left(-\\frac{t-(-x_0-\\lambda)}{\\lambda}\\right)$ for $t>t_1$.\nWe can combine these results into a single piecewise definition for $x(t)$.\n\nThird, we compare this flow with the subgradient flow $\\dot{x}(t) \\in -\\partial f(x(t))$.\nThe subgradient of $f(x)=|x|$ gives the differential inclusion: $\\dot{x} = -1$ for $x>0$, $\\dot{x}=1$ for $x<0$, and $\\dot{x} \\in [-1, 1]$ for $x=0$.\nFor an initial condition $x_0 > 0$, the solution is $x(t) = x_0-t$, which reaches $x=0$ at time $t=x_0$. For $t \\ge x_0$, the trajectory remains at $x=0$ (by choosing $\\dot{x}=0$). The general solution is $x(t) = \\text{sign}(x_0)\\max(0, |x_0|-t)$.\nComparison:\n1. Convergence Time: The subgradient flow reaches the minimum $x=0$ in finite time $T=|x_0|$. The gradient flow of the Moreau envelope converges to $x=0$ asymptotically as $t\\to\\infty$, but never reaches it in finite time due to the exponential decay phase.\n2. Trajectory Smoothness: The subgradient flow solution $x(t)$ is continuous and piecewise linear, but its derivative $\\dot{x}(t)$ is discontinuous at $t=|x_0|$. The trajectory is not $C^1$. The Moreau-smoothed flow solution $x(t)$ is continuously differentiable ($C^1$), joining a linear part and an exponential part seamlessly. However, its second derivative $\\ddot{x}(t)$ is discontinuous at the transition time $t=|x_0|-\\lambda$ (if $|x_0|>\\lambda$), so the trajectory is not $C^2$. The smoothing parameter $\\lambda$ creates a region around the origin where the velocity smoothly decreases to zero, avoiding the abrupt stop of the subgradient flow.\n\nThe final explicit formula for $x(t)$ is given in a piecewise form, conditioned on the initial state $x_0$ relative to $\\lambda$.", "answer": "$$\n\\boxed{\nx(t) = \\begin{cases} x_0 \\exp\\left(-\\frac{t}{\\lambda}\\right) & \\text{if } |x_0| \\le \\lambda \\\\ \\text{sign}(x_0)\\left(|x_0|-t\\right) & \\text{if } |x_0| > \\lambda \\text{ and } 0 \\le t \\le |x_0|-\\lambda \\\\ \\text{sign}(x_0)\\lambda \\exp\\left(-\\frac{t - (|x_0|-\\lambda)}{\\lambda}\\right) & \\text{if } |x_0| > \\lambda \\text{ and } t > |x_0|-\\lambda \\end{cases}\n}\n$$", "id": "3167965"}, {"introduction": "The theoretical elegance of proximal operators translates directly into the design of efficient, practical optimization algorithms. This final hands-on practice challenges you to bridge the gap between theory and computation. You will implement and compare two fundamental iterative methods: the Proximal Point Algorithm (PPA) on a nonsmooth function $f$, and Gradient Descent (GD) on its smooth Moreau envelope $e_{\\lambda}f$. The exercise requires you to first derive the analytical forms of the proximal operator and the envelope's gradient, and then use these to build working solvers in code [@problem_id:3168003]. By observing the behavior of these two closely related algorithms, you will gain tangible insight into their relationship and the role that parameters like step size $\\alpha$ and smoothing $\\lambda$ play in algorithmic performance.", "problem": "Consider the one-dimensional convex function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=|x|+\\dfrac{c}{2}x^{2}$ where $c\\ge 0$ is a parameter. Let $\\lambda>0$ be a smoothing parameter. The Moreau envelope (ME) of $f$ with parameter $\\lambda$ is defined by\n$$e_{\\lambda}f(x)=\\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\dfrac{1}{2\\lambda}(y-x)^{2}\\right\\},$$\nand the proximal operator of $f$ with parameter $\\lambda$ is defined by\n$$\\operatorname{prox}_{\\lambda f}(x)=\\arg\\min_{y\\in\\mathbb{R}}\\left\\{f(y)+\\dfrac{1}{2\\lambda}(y-x)^{2}\\right\\}.$$\n\nTasks:\n1) Starting from the core definitions above, derive a closed-form expression for $\\operatorname{prox}_{\\lambda f}(x)$ for the given $f(x)=|x|+\\dfrac{c}{2}x^{2}$. Next, using only foundational properties of convex optimization and the definitions provided, derive an explicit expression for the gradient of the Moreau envelope, $\\nabla e_{\\lambda}f(x)$, in terms of $x$, $\\lambda$, $c$, and $\\operatorname{prox}_{\\lambda f}(x)$.\n\n2) Implement two iterative schemes:\n- Gradient Descent (GD) on the Moreau envelope: for a constant step size $\\alpha>0$ and an initial point $x_{0}\\in\\mathbb{R}$, define $x_{k+1}=x_{k}-\\alpha\\,\\nabla e_{\\lambda}f(x_{k})$ for $k=0,1,\\dots,N-1$.\n- Proximal Point Algorithm (PPA): for the same $\\lambda$ and initial point $x_{0}$, define $x_{k+1}=\\operatorname{prox}_{\\lambda f}(x_{k})$ for $k=0,1,\\dots,N-1$.\n\nFor each method, run exactly $N$ iterations and report the final iterate.\n\n3) Using your derivations, write a complete, runnable program that:\n- Implements the closed-form $\\operatorname{prox}_{\\lambda f}(x)$ for the specified $f$.\n- Implements $\\nabla e_{\\lambda}f(x)$ via your derived expression.\n- Runs GD on $e_{\\lambda}f$ and PPA on $f$ for the test suite below.\n- For each test case, computes the single comparison number $\\Delta=f(x^{N}_{\\mathrm{GD}})-f(x^{N}_{\\mathrm{PPA}})$, where $x^{N}_{\\mathrm{GD}}$ and $x^{N}_{\\mathrm{PPA}}$ are the final iterates after $N$ iterations of GD and PPA, respectively.\n\nTest suite (each tuple specifies $(c,\\lambda,x_{0},\\alpha,N)$):\n- Case 1 (general convex, moderate smoothing): $(1.0,\\,0.5,\\,3.0,\\,0.4,\\,20)$.\n- Case 2 (near the nonsmooth kink at the origin): $(1.0,\\,0.5,\\,0.2,\\,0.4,\\,10)$.\n- Case 3 (small smoothing parameter): $(1.0,\\,0.05,\\,3.0,\\,0.04,\\,200)$.\n- Case 4 (purely nonsmooth, no quadratic term): $(0.0,\\,0.5,\\,3.0,\\,0.4,\\,20)$.\n- Case 5 (boundary of threshold set): $(1.0,\\,0.5,\\,0.5,\\,0.4,\\,1)$.\n\nFinal output format:\nYour program should produce a single line of output containing the five $\\Delta$ values for the test suite, as a comma-separated list enclosed in square brackets, in the order of the cases above, for example, $[\\,\\Delta_{1},\\Delta_{2},\\Delta_{3},\\Delta_{4},\\Delta_{5}\\,]$. There are no physical units involved; all outputs are real numbers in mathematical units. Angles are not used. Percentages are not used. The outputs are real numbers (floating-point).", "solution": "The problem is evaluated to be valid as it is mathematically sound, self-contained, well-posed, and constitutes a standard exercise in the field of convex optimization. The given definitions, parameters, and tasks are clear and unambiguous.\n\nThe solution proceeds in two parts. First, we derive the closed-form expressions for the proximal operator and the gradient of the Moreau envelope. Second, we describe the iterative algorithms that utilize these expressions.\n\n**Part 1: Analytical Derivations**\n\nLet the function be $f(x) = |x| + \\dfrac{c}{2}x^2$ for $x \\in \\mathbb{R}$, with parameter $c \\ge 0$. The parameter $\\lambda$ is strictly positive, i.e., $\\lambda > 0$.\n\n**1. Derivation of the Proximal Operator $\\operatorname{prox}_{\\lambda f}(x)$**\n\nThe proximal operator of $f$ is defined as the unique minimizer of the following objective function:\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\arg\\min_{y\\in\\mathbb{R}} \\left\\{ g(y) = f(y) + \\dfrac{1}{2\\lambda}(y-x)^2 \\right\\} $$\nSubstituting the expression for $f(y)$, we have:\n$$ g(y) = |y| + \\dfrac{c}{2}y^2 + \\dfrac{1}{2\\lambda}(y-x)^2 $$\nThe function $g(y)$ is strongly convex because it is the sum of a convex function, $|y|$, and a strongly convex quadratic function, $\\frac{c}{2}y^2 + \\frac{1}{2\\lambda}(y-x)^2$ (the quadratic part has a coefficient of $\\frac{1}{2}(c + \\frac{1}{\\lambda}) > 0$ for the $y^2$ term). A strongly convex function has a unique minimizer.\n\nTo find this minimizer, denoted by $y^*$, we use Fermat's rule from convex analysis, which states that the minimum is achieved when $0$ is in the subdifferential of the objective function, i.e., $0 \\in \\partial g(y^*)$. The subdifferential of $g(y)$ is the sum of the subdifferentials of its component functions:\n$$ \\partial g(y) = \\partial|y| + \\nabla\\left(\\dfrac{c}{2}y^2 + \\dfrac{1}{2\\lambda}(y-x)^2\\right) $$\nThe derivative of the smooth part is:\n$$ \\nabla\\left(\\dfrac{c}{2}y^2 + \\dfrac{1}{2\\lambda}(y-x)^2\\right) = cy + \\dfrac{1}{\\lambda}(y-x) = \\left(c + \\dfrac{1}{\\lambda}\\right)y - \\dfrac{x}{\\lambda} $$\nThe subdifferential of the absolute value function is:\n$$ \\partial|y| = \\begin{cases} \\{-1\\}, & \\text{if } y < 0 \\\\ [-1, 1], & \\text{if } y = 0 \\\\ \\{1\\}, & \\text{if } y > 0 \\end{cases} $$\nThe optimality condition $0 \\in \\partial g(y^*)$ becomes:\n$$ 0 \\in \\partial|y^*| + \\left(c + \\dfrac{1}{\\lambda}\\right)y^* - \\dfrac{x}{\\lambda} $$\nThis can be rewritten as:\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)y^* \\in \\partial|y^*| $$\nWe analyze this inclusion based on the sign of $y^*$:\n\nCase I: $y^* > 0$. The subdifferential is $\\partial|y^*| = \\{1\\}$.\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)y^* = 1 \\implies \\left(\\dfrac{1+c\\lambda}{\\lambda}\\right)y^* = \\dfrac{x}{\\lambda} - 1 $$\n$$ y^* = \\dfrac{\\lambda}{1+c\\lambda}\\left(\\dfrac{x-\\lambda}{\\lambda}\\right) = \\dfrac{x-\\lambda}{1+c\\lambda} $$\nThis solution is valid only if $y^* > 0$, which requires $x - \\lambda > 0$, or $x > \\lambda$.\n\nCase II: $y^* < 0$. The subdifferential is $\\partial|y^*| = \\{-1\\}$.\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)y^* = -1 \\implies \\left(\\dfrac{1+c\\lambda}{\\lambda}\\right)y^* = \\dfrac{x}{\\lambda} + 1 $$\n$$ y^* = \\dfrac{\\lambda}{1+c\\lambda}\\left(\\dfrac{x+\\lambda}{\\lambda}\\right) = \\dfrac{x+\\lambda}{1+c\\lambda} $$\nThis solution is valid only if $y^* < 0$, which requires $x + \\lambda < 0$, or $x < -\\lambda$.\n\nCase III: $y^* = 0$. The subdifferential is $\\partial|y^*| = [-1, 1]$.\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)(0) \\in [-1, 1] \\implies \\dfrac{x}{\\lambda} \\in [-1, 1] $$\nThis is equivalent to $-\\lambda \\le x \\le \\lambda$.\n\nCombining these three cases yields the closed-form expression for $y^* = \\operatorname{prox}_{\\lambda f}(x)$:\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\begin{cases} \\dfrac{x+\\lambda}{1+c\\lambda}, & \\text{if } x < -\\lambda \\\\ 0, & \\text{if } -\\lambda \\le x \\le \\lambda \\\\ \\dfrac{x-\\lambda}{1+c\\lambda}, & \\text{if } x > \\lambda \\end{cases} $$\nThis can be written compactly using the soft-thresholding operator, $S_T(x) = \\operatorname{sign}(x)\\max(0, |x|-T)$, as:\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\dfrac{S_{\\lambda}(x)}{1+c\\lambda} $$\n\n**2. Derivation of the Gradient of the Moreau Envelope $\\nabla e_{\\lambda}f(x)$**\n\nThe Moreau envelope is defined as $e_{\\lambda}f(x) = \\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\dfrac{1}{2\\lambda}(y-x)^{2}\\right\\}$.\nA fundamental result in convex analysis, often derived from Danskin's Theorem, states that the Moreau envelope of a proper, lower-semicontinuous, convex function $f$ is continuously differentiable. Its gradient is given by the Moreau identity (or second resolvent identity):\n$$ \\nabla e_{\\lambda}f(x) = \\dfrac{1}{\\lambda}(x - \\operatorname{prox}_{\\lambda f}(x)) $$\nTo demonstrate this using foundational principles as requested, let $\\phi(x,y) = f(y) + \\frac{1}{2\\lambda}(y-x)^2$. The Moreau envelope is $e_{\\lambda}f(x) = \\min_y \\phi(x,y)$. The function $\\phi(x,y)$ is convex in $y$ and continuously differentiable with respect to $x$. Danskin's Theorem on the differentiation of a min-function applies. It states that the gradient of $e_{\\lambda}f(x)$ with respect to $x$ is the gradient of $\\phi(x,y)$ with respect to $x$, evaluated at the point $y = y^*(x) = \\operatorname{prox}_{\\lambda f}(x)$, which is the unique minimizer.\n\nFirst, we compute the gradient of $\\phi(x,y)$ with respect to $x$:\n$$ \\nabla_x \\phi(x,y) = \\nabla_x \\left( f(y) + \\dfrac{1}{2\\lambda}(y-x)^2 \\right) = \\dfrac{1}{2\\lambda} \\cdot 2(y-x) \\cdot (-1) = \\dfrac{1}{\\lambda}(x-y) $$\nEvaluating this at $y = \\operatorname{prox}_{\\lambda f}(x)$ gives the final expression for the gradient of the Moreau envelope:\n$$ \\nabla e_{\\lambda}f(x) = \\dfrac{1}{\\lambda}(x - \\operatorname{prox}_{\\lambda f}(x)) $$\nThis expression connects the gradient of the smoothed function $e_{\\lambda}f$ directly to the proximal operator of the original function $f$.\n\n**Part 2: Iterative Schemes**\n\nThe derived expressions are used to implement two iterative optimization algorithms. Both algorithms aim to find the minimizer of $f(x)$, which is $x^*=0$.\n\n**1. Gradient Descent (GD) on the Moreau Envelope**\n\nThis method applies the standard gradient descent algorithm to the smoothed function $e_{\\lambda}f(x)$. Given a constant step size $\\alpha > 0$ and an initial point $x_0$, the iteration is:\n$$ x_{k+1} = x_k - \\alpha \\nabla e_{\\lambda}f(x_k) $$\nSubstituting our derived expression for the gradient:\n$$ x_{k+1} = x_k - \\dfrac{\\alpha}{\\lambda}(x_k - \\operatorname{prox}_{\\lambda f}(x_k)) $$\n$$ x_{k+1} = \\left(1 - \\dfrac{\\alpha}{\\lambda}\\right)x_k + \\dfrac{\\alpha}{\\lambda}\\operatorname{prox}_{\\lambda f}(x_k) $$\nThis shows that GD on the Moreau envelope is a convex combination of the current iterate $x_k$ and the result of a proximal step, $\\operatorname{prox}_{\\lambda f}(x_k)$. It is often referred to as the forward-backward splitting method or a relaxed proximal point algorithm.\n\n**2. Proximal Point Algorithm (PPA)**\n\nThis method directly uses the proximal operator to generate the next iterate. For the same $\\lambda$ and initial point $x_0$, the iteration is defined as:\n$$ x_{k+1} = \\operatorname{prox}_{\\lambda f}(x_k) $$\nIf the step size $\\alpha$ in the GD scheme were chosen to be equal to $\\lambda$, the GD scheme would become identical to the PPA. In the provided test cases, $\\alpha < \\lambda$, making the GD method an under-relaxed version of the PPA, which is expected to converge more slowly.\n\nThese derivations and algorithmic descriptions form the basis for the required program to compute numerical results for the given test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by implementing and comparing two iterative schemes.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple specifies (c, lambda, x0, alpha, N).\n    test_cases = [\n        (1.0, 0.5, 3.0, 0.4, 20),    # Case 1\n        (1.0, 0.5, 0.2, 0.4, 10),    # Case 2\n        (1.0, 0.05, 3.0, 0.04, 200), # Case 3\n        (0.0, 0.5, 3.0, 0.4, 20),    # Case 4\n        (1.0, 0.5, 0.5, 0.4, 1),     # Case 5\n    ]\n\n    def f(x, c):\n        \"\"\"\n        Computes the value of the function f(x) = |x| + (c/2)*x^2.\n        \"\"\"\n        return np.abs(x) + (c / 2.0) * x**2\n\n    def prox_f(x, c, lam):\n        \"\"\"\n        Computes the proximal operator of f with parameter lambda.\n        prox_{\\lambda f}(x) = S_{\\lambda}(x) / (1 + c*\\lambda)\n        where S is the soft-thresholding operator.\n        \"\"\"\n        # Soft-thresholding operator S_lam(x)\n        soft_threshold = np.sign(x) * np.maximum(0.0, np.abs(x) - lam)\n        return soft_threshold / (1.0 + c * lam)\n\n    def grad_e(x, c, lam):\n        \"\"\"\n        Computes the gradient of the Moreau envelope e_{\\lambda}f(x).\n        \\nabla e_{\\lambda}f(x) = (x - prox_{\\lambda f}(x)) / \\lambda\n        \"\"\"\n        return (x - prox_f(x, c, lam)) / lam\n\n    def run_gd(c, lam, x0, alpha, N):\n        \"\"\"\n        Runs Gradient Descent on the Moreau envelope for N iterations.\n        \"\"\"\n        x_k = x0\n        for _ in range(N):\n            grad = grad_e(x_k, c, lam)\n            x_k = x_k - alpha * grad\n        return x_k\n\n    def run_ppa(c, lam, x0, N):\n        \"\"\"\n        Runs the Proximal Point Algorithm for N iterations.\n        \"\"\"\n        x_k = x0\n        for _ in range(N):\n            x_k = prox_f(x_k, c, lam)\n        return x_k\n\n    results = []\n    for case in test_cases:\n        c, lam, x0, alpha, N = case\n        \n        # Run Gradient Descent on the Moreau envelope\n        x_N_gd = run_gd(c, lam, x0, alpha, int(N))\n        \n        # Run Proximal Point Algorithm\n        x_N_ppa = run_ppa(c, lam, x0, int(N))\n        \n        # Compute the comparison number Delta\n        delta = f(x_N_gd, c) - f(x_N_ppa, c)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3168003"}]}