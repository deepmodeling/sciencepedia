{"hands_on_practices": [{"introduction": "This first practice problem immerses you in the core engine of the Dantzig-Wolfe algorithm: the pricing subproblem. Using a classic multi-commodity network flow scenario, you will step into the role of a logistics planner to see how dual variables, often called \"shadow prices,\" guide the search for profitable new columns or routes. This exercise is designed to build your intuition for the dialogue between the master problem and its subproblems, which is the fundamental mechanism of column generation [@problem_id:3116329].", "problem": "A logistics planner must route two commodities from a single source $s$ to a single sink $t$ through a directed network with node set $N=\\{s,a,b,t\\}$ and arc set $A=\\{(s,a),(s,b),(a,b),(a,t),(b,t)\\}$. Each unit of commodity $k \\in \\{1,2\\}$ delivered from $s$ to $t$ yields revenue $R_k$ and incurs per-arc operating cost $c_{ij}$ on each used arc $(i,j) \\in A$. Arc $(i,j)$ has capacity $U_{ij}$ that couples the commodities: the total flow of all commodities on $(i,j)$ cannot exceed $U_{ij}$. Commodity $k$ has demand upper bound $d_k$: the total routed units of commodity $k$ cannot exceed $d_k$. The planner solves a linear programming master problem in a Dantzig-Wolfe decomposition that uses $s$–$t$ paths as columns; a column corresponds to sending $1$ unit of a specific commodity $k$ along a specific $s$–$t$ path. The master problem is a maximization of total net revenue subject to the arc capacity and commodity demand constraints.\n\nSuppose a restricted master problem has been solved and produced the following current dual multipliers: for each arc $(i,j) \\in A$, the dual on its capacity constraint is $\\pi_{ij} \\ge 0$, and for each commodity $k$, the dual on its demand constraint is $\\mu_k \\ge 0$. The pricing subproblem for each commodity searches for an $s$–$t$ path that maximizes the column’s attractiveness given these multipliers.\n\nData:\n- Revenues: $R_1 = 8$, $R_2 = 5$.\n- Arc operating costs: $c_{s,a}=1$, $c_{s,b}=2$, $c_{a,b}=1$, $c_{a,t}=3$, $c_{b,t}=2$.\n- Current dual multipliers on capacities: $\\pi_{s,a}=0.5$, $\\pi_{s,b}=1.0$, $\\pi_{a,b}=0.2$, $\\pi_{a,t}=1.2$, $\\pi_{b,t}=0.3$.\n- Current dual multipliers on demands: $\\mu_1=1.5$, $\\mu_2=0.5$.\n\nConsider the three simple $s$–$t$ paths available in this network: $p_1: s \\to a \\to t$, $p_2: s \\to b \\to t$, and $p_3: s \\to a \\to b \\to t$.\n\nWhich option correctly states the economic interpretation of the dual variables $\\pi_{ij}$ and also identifies the single column that the pricing subproblem would select as the most promising new column under the given multipliers?\n\nA. The $\\pi_{ij}$ are shadow prices (marginal values) per unit of capacity on arc $(i,j)$. Pricing would select commodity $1$ on path $p_3: s \\to a \\to b \\to t$.\n\nB. The $\\pi_{ij}$ are penalties per unit of demand for each commodity. Pricing would select commodity $1$ on path $p_2: s \\to b \\to t$ because it has the smallest base operating cost.\n\nC. The $\\pi_{ij}$ are subsidies that reduce the effective cost on congested arcs. Pricing would select commodity $2$ on path $p_1: s \\to a \\to t$ because it yields positive profit before considering prices.\n\nD. The $\\pi_{ij}$ are shadow prices (marginal values) per unit of capacity on arc $(i,j)$. Pricing would find no path with positive attractiveness for any commodity, so no column would be selected.", "solution": "We begin from core linear programming and decomposition principles. A Dantzig-Wolfe decomposition expresses a problem with coupling constraints (here, arc capacities) as a master problem over extreme points (here, $s$–$t$ paths) of subproblems (one per commodity), with a pricing step that generates improving columns. In linear programming, for a maximization problem in the form $\\max\\{c^\\top x : Ax \\le b, x \\ge 0\\}$, the dual is $\\min\\{b^\\top y : A^\\top y \\ge c, y \\ge 0\\}$, and the reduced cost of a variable (column) $x_j$ is $r_j = c_j - a_j^\\top y$, where $a_j$ is the $j$-th column of $A$ and $y$ are the current dual multipliers. A column with $r_j > 0$ can improve the master objective; the pricing subproblem seeks the column(s) with maximum positive reduced cost.\n\nApply these facts to the present master problem. A column is indexed by a commodity $k$ and a path $p$, representing $1$ unit of commodity $k$ routed along $p$ from $s$ to $t$. Its primal objective coefficient is the net revenue per unit before pricing: this is the commodity revenue minus the sum of arc operating costs on the path, namely\n$$\nc_{k,p} = R_k - \\sum_{(i,j)\\in p} c_{ij}.\n$$\nIts column in the capacity constraints contributes $1$ unit on each arc $(i,j)$ that is in $p$, and in the demand constraint for commodity $k$ it contributes $1$ unit. Therefore, with dual multipliers $\\pi_{ij} \\ge 0$ for arc capacities and $\\mu_k \\ge 0$ for commodity demands, the reduced cost of column $(k,p)$ is\n$$\nr_{k,p} = c_{k,p} - \\sum_{(i,j)\\in p} \\pi_{ij} - \\mu_k \\;=\\; R_k \\;-\\; \\sum_{(i,j)\\in p} \\big(c_{ij} + \\pi_{ij}\\big) \\;-\\; \\mu_k.\n$$\nThus, for each commodity $k$, the pricing subproblem is to find the $s$–$t$ path $p$ that minimizes the path length with respect to arc weights $c_{ij} + \\pi_{ij}$ and then check whether $r_{k,p} > 0$. Economically, each $\\pi_{ij}$ is the shadow price (marginal value) of one unit of capacity on arc $(i,j)$: it is the rate at which the optimal objective would improve if $U_{ij}$ were relaxed by $1$ unit. Consequently, in pricing, capacity use on an arc is “charged” at price $\\pi_{ij}$ on top of its operating cost $c_{ij}$.\n\nCompute the price-adjusted arc weights and path totals:\n- Arc weights $c_{ij} + \\pi_{ij}$:\n  - $(s,a)$: $1 + 0.5 = 1.5$,\n  - $(s,b)$: $2 + 1.0 = 3.0$,\n  - $(a,b)$: $1 + 0.2 = 1.2$,\n  - $(a,t)$: $3 + 1.2 = 4.2$,\n  - $(b,t)$: $2 + 0.3 = 2.3$.\n- Path totals $\\sum_{(i,j)\\in p} (c_{ij} + \\pi_{ij})$:\n  - $p_1: s \\to a \\to t$: $1.5 + 4.2 = 5.7$,\n  - $p_2: s \\to b \\to t$: $3.0 + 2.3 = 5.3$,\n  - $p_3: s \\to a \\to b \\to t$: $1.5 + 1.2 + 2.3 = 5.0$.\n\nNow evaluate reduced costs $r_{k,p} = R_k - \\mu_k - \\text{(path total)}$:\n\nFor commodity $1$ ($R_1=8$, $\\mu_1=1.5$):\n- $r_{1,p_1} = 8 - 1.5 - 5.7 = 0.8$,\n- $r_{1,p_2} = 8 - 1.5 - 5.3 = 1.2$,\n- $r_{1,p_3} = 8 - 1.5 - 5.0 = 1.5$.\n\nAll three are positive, and the largest is $r_{1,p_3} = 1.5$ for path $p_3: s \\to a \\to b \\to t$.\n\nFor commodity $2$ ($R_2=5$, $\\mu_2=0.5$):\n- $r_{2,p_1} = 5 - 0.5 - 5.7 = -1.2$,\n- $r_{2,p_2} = 5 - 0.5 - 5.3 = -0.8$,\n- $r_{2,p_3} = 5 - 0.5 - 5.0 = -0.5$.\n\nAll are nonpositive for commodity $2$.\n\nTherefore, the pricing subproblem would select the column for commodity $1$ on path $p_3$ as the most attractive improving column (largest positive reduced cost). The economic interpretation of $\\pi_{ij}$ is indeed that of shadow prices for arc capacities, i.e., marginal values per unit of capacity.\n\nOption-by-option analysis:\n\nA. States that $\\pi_{ij}$ are shadow prices per unit capacity on arc $(i,j)$, which matches the dual interpretation as marginal values of capacity. Identifies commodity $1$ on $p_3: s \\to a \\to b \\to t$ as the selected column, which is correct because it has the largest positive reduced cost $1.5$. Verdict: Correct.\n\nB. Claims that $\\pi_{ij}$ are penalties per unit demand; this confuses capacity duals with demand duals. It also selects $p_2$ based on smallest base operating cost, ignoring the capacity prices and demand dual, which is not the pricing criterion. Verdict: Incorrect.\n\nC. Describes $\\pi_{ij}$ as subsidies that reduce effective cost; in fact, they act as prices added to operating costs in the reduced cost. It also selects commodity $2$ on $p_1$, but all reduced costs for commodity $2$ are negative. Verdict: Incorrect.\n\nD. Correctly interprets $\\pi_{ij}$ as shadow prices, but claims no positive reduced cost exists. In fact, commodity $1$ has positive reduced costs, with $p_3$ being the best. Verdict: Incorrect.\n\nHence, only option A is correct.", "answer": "$$\\boxed{A}$$", "id": "3116329"}, {"introduction": "While the pricing subproblem drives the algorithm forward, the overall efficiency of the method hinges on the initial decomposition of the problem. This exercise shifts your focus from executing the algorithm to strategically designing it. You will analyze the structure of a given linear program and determine the optimal way to partition its variables to maximize \"block separability,\" a measure of how cleanly the problem's constraints can be separated [@problem_id:3116333]. Mastering this perspective is key to formulating decompositions that are not just theoretically valid, but computationally practical.", "problem": "Consider the following Linear Programming (LP) problem in standard form, with decision variables $x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6} \\geq 0$:\n$$\n\\min \\; c^{\\top} x \\quad \\text{with} \\quad c = (1, 2, 3, 1, 2, 3),\n$$\nsubject to the equality constraints\n$$\n\\begin{aligned}\nx_{1} + x_{2} + x_{3} &= 6, \\\\\nx_{4} + x_{5} + x_{6} &= 6, \\\\\nx_{1} + x_{2} &= 4, \\\\\nx_{4} + x_{5} &= 4, \\\\\nx_{3} + x_{6} &= 4, \\\\\nx_{2} + x_{4} &= 4.\n\\end{aligned}\n$$\nIn the Dantzig-Wolfe decomposition (DWD), an LP is reformulated by partitioning its variables into blocks, which induces a partition of the constraint rows into subproblem-specific constraints and linking constraints. A constraint row is called “pure” if all its nonzero coefficients involve variables from a single block; otherwise it is “linking.” Denote by $A_{i}$ the subproblem matrix collecting the pure constraints assigned to block $i$.\n\nYou are to partition the variables into exactly two blocks of equal size:\n$$\n\\{x_{1}, x_{2}, x_{3}\\} \\cup \\{x_{4}, x_{5}, x_{6}\\},\n$$\nbut you may assign any three variables to block $1$ and the remaining three to block $2$. Define the block separability index (BSI) $S$ as the number of pure constraints under a given partition; equivalently, $S$ counts how many constraint rows can be assigned to the subproblem matrices $A_{1}$ or $A_{2}$ rather than being linking.\n\nAmong all such two-block partitions, determine the maximal possible value of $S$, denoted $S_{\\max}$. Your final answer must be a single integer giving $S_{\\max}$.", "solution": "The task is to find the maximum possible value of the block separability index, $S$, which is the number of pure constraints, over all partitions of the $6$ variables into two blocks of size $3$. A constraint is pure if all variables with non-zero coefficients in that constraint belong to the same block.\n\nLet the set of all variables be $V = \\{x_1, x_2, x_3, x_4, x_5, x_6\\}$. We are seeking a partition of $V$ into two disjoint blocks, $B_1$ and $B_2$, such that $|B_1| = |B_2| = 3$.\n\nLet us denote the six constraints given in the problem statement as $C_1, \\dots, C_6$. We can associate each constraint with the set of variables it involves:\n\\begin{itemize}\n    \\item $C_1: x_{1} + x_{2} + x_{3} = 6 \\implies V_1 = \\{x_1, x_2, x_3\\}$\n    \\item $C_2: x_{4} + x_{5} + x_{6} = 6 \\implies V_2 = \\{x_4, x_5, x_6\\}$\n    \\item $C_3: x_{1} + x_{2} = 4 \\implies V_3 = \\{x_1, x_2\\}$\n    \\item $C_4: x_{4} + x_{5} = 4 \\implies V_4 = \\{x_4, x_5\\}$\n    \\item $C_5: x_{3} + x_{6} = 4 \\implies V_5 = \\{x_3, x_6\\}$\n    \\item $C_6: x_{2} + x_{4} = 4 \\implies V_6 = \\{x_2, x_4\\}$\n\\end{itemize}\nA constraint $C_k$ is pure with respect to a partition $\\{B_1, B_2\\}$ if its variable set $V_k$ is a subset of either $B_1$ or $B_2$. Otherwise, the constraint is linking.\n\nFirst, let us establish a lower bound for $S_{\\max}$ by analyzing a specific partition. Consider the partition $P_0$ suggested by the structure of the variables in the first two constraints:\n$$B_1 = \\{x_1, x_2, x_3\\} \\quad \\text{and} \\quad B_2 = \\{x_4, x_5, x_6\\}$$\nFor this partition $P_0$, let us determine the number of pure constraints, $S(P_0)$:\n\\begin{itemize}\n    \\item $V_1 = \\{x_1, x_2, x_3\\} = B_1$. Thus, $C_1$ is pure.\n    \\item $V_2 = \\{x_4, x_5, x_6\\} = B_2$. Thus, $C_2$ is pure.\n    \\item $V_3 = \\{x_1, x_2\\} \\subseteq B_1$. Thus, $C_3$ is pure.\n    \\item $V_4 = \\{x_4, x_5\\} \\subseteq B_2$. Thus, $C_4$ is pure.\n    \\item $V_5 = \\{x_3, x_6\\}$. Since $x_3 \\in B_1$ and $x_6 \\in B_2$, $C_5$ is linking.\n    \\item $V_6 = \\{x_2, x_4\\}$. Since $x_2 \\in B_1$ and $x_4 \\in B_2$, $C_6$ is linking.\n\\end{itemize}\nFor this partition, the number of pure constraints is $S(P_0) = 4$. Since $S_{\\max}$ is the maximum value over all possible partitions, we must have $S_{\\max} \\ge 4$.\n\nNext, we must determine if a value of $S > 4$ is achievable. Let us prove by contradiction that $S$ cannot be greater than $4$.\nAssume that a partition exists for which $S \\ge 5$. This implies that at least $5$ of the $6$ constraints must be pure. There are two possibilities for which constraint could be the single linking one (or if all $6$ are pure).\n\nCase 1: Assume the set of pure constraints is $\\{C_1, C_2, C_3, C_4, C_5\\}$.\nFor $C_1$ to be pure, the set of variables $V_1 = \\{x_1, x_2, x_3\\}$ must be entirely contained within one of the blocks. Since the blocks are of size $3$, this requires that one block must be exactly $B_1 = \\{x_1, x_2, x_3\\}$. This choice uniquely determines the other block as its complement, $B_2 = \\{x_4, x_5, x_6\\}$.\nThis is precisely the partition $P_0$ we analyzed earlier. For this partition, we must check if all the assumed constraints are indeed pure.\n- $C_1$ is pure because $V_1 = B_1$.\n- $C_2$ is pure because $V_2 = B_2$.\n- $C_3$ is pure because $V_3 \\subseteq B_1$.\n- $C_4$ is pure because $V_4 \\subseteq B_2$.\n- For $C_5$ to be pure, $V_5=\\{x_3, x_6\\}$ must be a subset of a single block. However, for this partition, $x_3 \\in B_1$ and $x_6 \\in B_2$. Thus, $C_5$ is linking.\nThis contradicts our assumption that $C_5$ is pure for this partition. Therefore, it is impossible for the set $\\{C_1, C_2, C_3, C_4, C_5\\}$ to be simultaneously pure.\n\nCase 2: Assume the set of pure constraints is $\\{C_1, C_2, C_3, C_4, C_6\\}$.\nAs in Case 1, the requirement that $C_1$ is pure for a partition into blocks of size $3$ forces the partition to be $B_1 = \\{x_1, x_2, x_3\\}$ and $B_2 = \\{x_4, x_5, x_6\\}$.\nWe check the purity of $C_6$: $V_6=\\{x_2, x_4\\}$. For this partition, $x_2 \\in B_1$ and $x_4 \\in B_2$, so $C_6$ is linking. This contradicts the assumption that $C_6$ is pure.\n\nAny set of $5$ or $6$ pure constraints must contain either $\\{C_1, C_2, C_3, C_4, C_5\\}$ or $\\{C_1, C_2, C_3, C_4, C_6\\}$ as a subset. More generally, any attempt to make at least $5$ constraints pure requires that either $C_1$ or $C_2$ (or both) must be pure, because the remaining constraints $\\{C_3, C_4, C_5, C_6\\}$ only number $4$.\n\nIf $C_1$ is pure, the partition must be $\\{ \\{x_1, x_2, x_3\\}, \\{x_4, x_5, x_6\\} \\}$.\nIf $C_2$ is pure, the partition must be $\\{ \\{x_4, x_5, x_6\\}, \\{x_1, x_2, x_3\\} \\}$.\nIn either case, the partition is uniquely determined. We have already calculated that for this specific partition, the number of pure constraints is exactly $4$. This contradicts any assumption of achieving $S \\ge 5$.\n\nThe only remaining possibility to achieve $S \\ge 5$ would be for both $C_1$ and $C_2$ to be linking, which would require the five pure constraints to be a subset of $\\{C_2, C_3, C_4, C_5, C_6\\}$ or $\\{C_1, C_3, C_4, C_5, C_6\\}$, but there are only four constraints other than $C_1$ and $C_2$. Thus, if both $C_1$ and $C_2$ are linking, the maximum possible value for $S$ is $4$. A systematic enumeration of all $\\frac{1}{2}\\binom{6}{3}=10$ partitions would confirm this, showing that no partition yields $S>4$.\n\nWe have established a realizable upper bound for $S$. The partition $B_1 = \\{x_1, x_2, x_3\\}$ and $B_2 = \\{x_4, x_5, x_6\\}$ yields $S=4$. We have also proven that no partition can yield $S \\ge 5$. Therefore, the maximal possible value of $S$ is $4$.", "answer": "$$\n\\boxed{4}\n$$", "id": "3116333"}, {"introduction": "Dantzig-Wolfe decomposition is a powerful technique, but it is not a silver bullet. This final practice presents a crucial counterexample to illustrate the potential pitfalls of a naive decomposition strategy. You will explore a scenario where a simple linear program, when decomposed without care, results in a master problem with an exponentially large number of columns, a phenomenon known as the \"curse of dimensionality\" [@problem_id:3116357]. This cautionary tale will solidify your understanding of the critical trade-offs involved and highlight the importance of ensuring that subproblems have a tractable structure.", "problem": "Consider the following Linear Programming (LP) problem, where Linear Programming (LP) denotes optimization of a linear objective over a polyhedral feasible region. Let $n$ be an even positive integer, and define the decision vector $x \\in \\mathbb{R}^{n}$ with components $x_{i}$, $i=1,\\dots,n$. The LP is\n$$\n\\min\\ \\sum_{i=1}^{n} c_{i} x_{i}\n\\quad \\text{subject to} \\quad\n\\sum_{i=1}^{n} x_{i} = \\frac{n}{2}, \\quad 0 \\leq x_{i} \\leq 1 \\ \\text{for all } i=1,\\dots,n,\n$$\nwith strictly increasing costs $c_{i}$ satisfying $c_{1} < c_{2} < \\dots < c_{n}$.\n\n1. Using only fundamental definitions from polyhedral theory and linear optimization (extreme points, convex hull, and linear objectives), justify why the original LP can be solved directly by a simple reasoning based on the structure of its feasible region and the monotonicity of the costs.\n\n2. Now, construct a Dantzig-Wolfe decomposition (DWD) of the above LP by partitioning the variables into $m$ blocks of equal size $k$, so that $n = m k$. For each block $j \\in \\{1,\\dots,m\\}$, define the local feasible set\n$$\nX_{j} = \\{ x^{(j)} \\in \\mathbb{R}^{k} : 0 \\leq x^{(j)}_{p} \\leq 1 \\ \\text{for } p=1,\\dots,k \\},\n$$\nand use a single linking constraint that couples all blocks:\n$$\n\\sum_{j=1}^{m} \\sum_{p=1}^{k} x^{(j)}_{p} = \\frac{n}{2}.\n$$\nIn the Dantzig-Wolfe reformulation, each block $j$ is represented by a convex combination of extreme points of $X_{j}$, and the master problem contains one column per extreme point of each block.\n\nDerive, from first principles, the total number of distinct columns that appear in the master problem under this decomposition, expressed as a function of $m$ and $k$.\n\n3. Finally, evaluate your expression numerically for $m=5$ and $k=8$ (so $n=40$ and $\\frac{n}{2}=20$). No rounding is required. Your final numeric answer must be the total number of columns in the Dantzig-Wolfe master problem under this decomposition.\n\nAdditionally, based on your derivation, briefly identify and explain practical criteria to avoid over-decomposition that causes a large number of columns in the Dantzig-Wolfe master problem. Your explanation should be grounded in the structure of extreme points and linking constraints, but your final answer must be the single number requested above.", "solution": "The user has provided a three-part problem concerning a specific Linear Programming (LP) problem and its Dantzig-Wolfe decomposition.\n\nFirst, a validation of the problem statement is required.\n\n**Step 1: Extract Givens**\n- The LP problem is:\n$$\n\\min\\ \\sum_{i=1}^{n} c_{i} x_{i}\n$$\nsubject to:\n$$\n\\sum_{i=1}^{n} x_{i} = \\frac{n}{2}\n$$\n$$\n0 \\leq x_{i} \\leq 1 \\quad \\text{for all } i=1,\\dots,n\n$$\n- The integer $n$ is an even positive integer.\n- The decision variables are $x_{i} \\in \\mathbb{R}$ for $i=1,\\dots,n$.\n- The cost coefficients $c_{i}$ are strictly increasing, i.e., $c_{1} < c_{2} < \\dots < c_{n}$.\n- For the Dantzig-Wolfe decomposition (DWD), the variables are partitioned into $m$ blocks of equal size $k$, where $n = mk$.\n- For each block $j \\in \\{1,\\dots,m\\}$, the local feasible set is defined as:\n$$\nX_{j} = \\{ x^{(j)} \\in \\mathbb{R}^{k} : 0 \\leq x^{(j)}_{p} \\leq 1 \\ \\text{for } p=1,\\dots,k \\}\n$$\n- A single linking constraint is used:\n$$\n\\sum_{j=1}^{m} \\sum_{p=1}^{k} x^{(j)}_{p} = \\frac{n}{2}\n$$\n- Part 3 requires numerical evaluation for $m=5$ and $k=8$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of linear optimization and decomposition methods.\n- **Scientific/Factual Soundness**: The problem is based on established principles of linear programming and polyhedral theory. The LP structure is valid, and the proposed DWD is a standard technique.\n- **Completeness and Consistency**: The problem is self-contained. All necessary variables ($n, m, k, c_i$), constraints, and definitions are provided. The decomposition structure ($n=mk$) is consistent. The linking constraint is a direct restatement of the original coupling constraint.\n- **Well-Posedness**: Each part of the problem requests a derivation or justification that leads to a unique, meaningful answer. Part 1 is a reasoning task. Part 2 asks for a formula based on the given structure. Part 3 requests a numerical evaluation of that formula. The problem is structured to admit a unique solution.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n**Part 1: Justification for a Direct Solution**\n\nThe fundamental theorem of linear programming states that if an LP has an optimal solution, at least one optimal solution must occur at an extreme point (vertex) of its feasible region. The feasible region of this LP is the polytope $P$ defined by the intersection of the $n$-dimensional hypercube $[0,1]^n$ and the hyperplane $\\sum_{i=1}^{n} x_{i} = \\frac{n}{2}$.\n\nAn extreme point of $P$ cannot have any fractional components. To see this, suppose a feasible solution $x$ has a fractional component, $0 < x_j < 1$. To satisfy the sum constraint $\\sum x_i = \\frac{n}{2}$, there must be at least one other fractional component, say $0 < x_k < 1$. We can then define a perturbation vector $d \\in \\mathbb{R}^n$ with $d_j = 1$, $d_k = -1$, and $d_i = 0$ for all $i \\ne j,k$. For a sufficiently small $\\epsilon > 0$, the points $x + \\epsilon d$ and $x - \\epsilon d$ remain within the hypercube $[0,1]^n$ and on the hyperplane (since $\\sum d_i = 0$). Thus, $x = \\frac{1}{2}(x + \\epsilon d) + \\frac{1}{2}(x - \\epsilon d)$, which shows $x$ is a convex combination of two other feasible points. By definition, $x$ cannot be an extreme point.\n\nTherefore, the extreme points of the feasible region $P$ are binary vectors, $x \\in \\{0, 1\\}^n$. The constraint $\\sum_{i=1}^{n} x_i = \\frac{n}{2}$ further requires that any such extreme point must have exactly $\\frac{n}{2}$ components equal to $1$ and $\\frac{n}{2}$ components equal to $0$.\n\nThe objective is to minimize the linear function $\\sum_{i=1}^{n} c_i x_i$. Given the strictly increasing costs $c_1 < c_2 < \\dots < c_n$, the sum is minimized by assigning the value $1$ to the variables $x_i$ corresponding to the smallest costs, and $0$ to the variables corresponding to the largest costs. To satisfy the constraint that exactly $\\frac{n}{2}$ variables are $1$, we must set $x_i=1$ for the first $\\frac{n}{2}$ variables and $x_i=0$ for the remaining $\\frac{n}{2}$ variables.\n\nThe unique optimal solution is therefore:\n$$\nx_i = 1 \\quad \\text{for } i = 1, \\dots, \\frac{n}{2}\n$$\n$$\nx_i = 0 \\quad \\text{for } i = \\frac{n}{2} + 1, \\dots, n\n$$\nThis direct reasoning, based on the structure of the extreme points and the objective function, solves the LP without requiring an iterative algorithm like simplex.\n\n**Part 2: Derivation of the Number of Columns in the Dantzig-Wolfe Master Problem**\n\nIn the Dantzig-Wolfe method, the master problem is constructed by representing the solution space of each subproblem as the convex hull of its extreme points.\n\nThe subproblem $j$ involves variables $x^{(j)} \\in \\mathbb{R}^k$. The feasible region for subproblem $j$ is the set $X_j = \\{ x^{(j)} \\in \\mathbb{R}^{k} : 0 \\leq x^{(j)}_{p} \\leq 1 \\ \\text{for } p=1,\\dots,k \\}$. This set is a $k$-dimensional hypercube.\n\nAccording to the Minkowski-Weyl theorem, any point $x^{(j)} \\in X_j$ can be expressed as a convex combination of the extreme points of $X_j$. The extreme points of a $k$-dimensional hypercube are its vertices. A $k$-hypercube has $2^k$ vertices. Each vertex is a binary vector in $\\{0, 1\\}^k$. Let us denote the set of these extreme points for block $j$ as $\\{v_{j,q}\\}_{q=1}^{2^k}$.\n\nWe can then write:\n$$\nx^{(j)} = \\sum_{q=1}^{2^k} \\lambda_{j,q} v_{j,q}\n$$\nwhere $\\sum_{q=1}^{2^k} \\lambda_{j,q} = 1$ and $\\lambda_{j,q} \\geq 0$ for all $q$.\n\nThe DWD master problem is formulated using the non-negative variables $\\lambda_{j,q}$. For each block $j \\in \\{1, \\dots, m\\}$ and for each of its $2^k$ extreme points $v_{j,q}$, a variable $\\lambda_{j,q}$ is introduced. Each such variable corresponds to a column in the master problem's constraint matrix.\n\nThe total number of columns in the full master problem is the total number of variables $\\lambda_{j,q}$. This is the number of blocks ($m$) multiplied by the number of extreme points per block.\n\nNumber of blocks = $m$.\nNumber of extreme points for each block's feasible set $X_j$ (a $k$-hypercube) = $2^k$.\n\nThe total number of distinct columns that appear in the master problem is the total number of such $\\lambda_{j,q}$ variables required for the complete representation. This is because each column is uniquely identified by the block index $j$ and the extreme point index $q$. Columns from different blocks ($j_1 \\ne j_2$) are distinct because they contribute to different convexity constraints ($\\sum_q \\lambda_{j_1,q} = 1$ versus $\\sum_q \\lambda_{j_2,q} = 1$). Columns corresponding to different extreme points within the same block are also treated as distinct in the formulation, as they correspond to independent variables $\\lambda_{j,q}$, irrespective of whether their coefficient vectors happen to be identical for a specific set of costs $c_i$.\n\nTherefore, the total number of columns in the master problem is:\n$$\n\\text{Total Columns} = (\\text{Number of blocks}) \\times (\\text{Number of extreme points per block}) = m \\times 2^k\n$$\n\n**Part 3: Numerical Evaluation and Practical Criteria**\n\nGiven the parameters $m=5$ and $k=8$ (which implies $n=mk=40$, consistent with the problem statement that $\\frac{n}{2}=20$ is an integer), we can evaluate the expression derived in Part 2.\n\nThe total number of columns is:\n$$\nN = m \\times 2^k = 5 \\times 2^8\n$$\nWe calculate $2^8$:\n$$\n2^8 = 256\n$$\nNow, we find the total number of columns:\n$$\nN = 5 \\times 256 = 1280\n$$\n\nThe final numerical answer is $1280$.\n\nRegarding the practical criteria to avoid a large number of columns: The expression $m \\times 2^k$ shows that the number of columns is dominated by the term $2^k$, which grows exponentially with the block size $k$. The number of variables in the original problem is fixed, $n=mk$. To minimize the number of columns $m \\times 2^{n/m}$, one must choose the decomposition such that $k$ is small. This corresponds to a decomposition with a large number of blocks, $m$.\n\nThe term \"over-decomposition\" can be ambiguous. If it means creating an excessive number of blocks (large $m$, small $k$), this is precisely the strategy that *minimizes* the number of columns in the master problem. However, this comes at the cost of increasing the number of rows in the master problem (one convexity constraint per block). A large number of master problem columns is not caused by \"over-decomposition\" but rather by \"under-decomposition,\" i.e., using a small number of very large blocks (small $m$, large $k$).\n\nThe core principle is that the number of extreme points of a subproblem's feasible region often grows exponentially with its dimension. In this case, the subproblem regions $X_j$ are $k$-dimensional hypercubes, for which the number of extreme points is exactly $2^k$. Therefore, the primary practical criterion to avoid an intractably large number of columns in the master problem is to define the blocks such that their dimension $k$ is kept small.", "answer": "$$\n\\boxed{1280}\n$$", "id": "3116357"}]}