{"hands_on_practices": [{"introduction": "This first exercise grounds the abstract definition of a subgradient in a concrete, one-dimensional example. By analyzing the function $f(x) = |x| + |x-1|$, you will not only practice applying the subgradient inequality but also discover how the structure of the subdifferential reveals the geometric nature of a function's minimum [@problem_id:3189295]. This problem illuminates the distinction between a unique, sharp minimum and a flat region of optimal solutions, a key concept in understanding optimization landscapes.", "problem": "Let the function be $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=|x|+|x-1|$. Use the definition of a subgradient for a convex function: a real number $g$ is a subgradient of $f$ at $x_{0}$ if and only if $f(y)\\ge f(x_{0})+g\\,(y-x_{0})$ for all $y\\in\\mathbb{R}$. \n\n(a) Prove that $f$ is convex by appealing to fundamental properties of absolute value and the fact that the sum of convex functions is convex.\n\n(b) Determine the subdifferential $\\partial f(0.5)$ by directly applying the subgradient inequality at $x_{0}=0.5$. If the subdifferential at $x_{0}=0.5$ is a singleton set, report its unique element as a single real number.\n\n(c) Based on your result in part (b) and the behavior of $f$ around $x=0.5$, use subdifferentials to discuss the distinction between sharp minima and flat regions. Explicitly classify the nature of $f$ at $x=0.5$ in this sense. \n\nYour final reported value should be the single real number corresponding to the unique subgradient at $x=0.5$ (if it exists). No rounding is required.", "solution": "The problem asks for a three-part analysis of the function $f(x)=|x|+|x-1|$. We must first prove its convexity, then determine its subdifferential at $x_0=0.5$, and finally discuss the nature of the function at that point.\n\n(a) Proof of Convexity\n\nA function $h: \\mathbb{R} \\to \\mathbb{R}$ is defined as convex if for any two points $x_1, x_2$ in its domain and for any scalar $\\lambda \\in [0, 1]$, the following inequality holds:\n$$h(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda h(x_1) + (1-\\lambda) h(x_2)$$\nThe function $f(x)$ is a sum of two functions: $f_1(x) = |x|$ and $f_2(x) = |x-1|$. We will prove that both $f_1(x)$ and $f_2(x)$ are convex.\n\n1.  Convexity of $f_1(x) = |x|$:\n    Using the definition of convexity and the triangle inequality, for any $x_1, x_2 \\in \\mathbb{R}$ and $\\lambda \\in [0, 1]$:\n    $$f_1(\\lambda x_1 + (1-\\lambda) x_2) = |\\lambda x_1 + (1-\\lambda) x_2|$$\n    By the triangle inequality, $|a+b| \\le |a| + |b|$. Thus:\n    $$|\\lambda x_1 + (1-\\lambda) x_2| \\le |\\lambda x_1| + |(1-\\lambda) x_2|$$\n    Since $\\lambda \\ge 0$ and $(1-\\lambda) \\ge 0$, we have $|\\lambda| = \\lambda$ and $|1-\\lambda| = 1-\\lambda$. Therefore:\n    $$|\\lambda x_1| + |(1-\\lambda) x_2| = \\lambda |x_1| + (1-\\lambda) |x_2| = \\lambda f_1(x_1) + (1-\\lambda) f_1(x_2)$$\n    Combining these steps, we get $f_1(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f_1(x_1) + (1-\\lambda) f_1(x_2)$, which proves that $f_1(x) = |x|$ is a convex function.\n\n2.  Convexity of $f_2(x) = |x-1|$:\n    This function is a composition of the convex function $g(u) = |u|$ and the affine function $u(x) = x-1$. The composition of a convex function with an affine function is convex. Alternatively, we can use the definition directly as above:\n    $$f_2(\\lambda x_1 + (1-\\lambda) x_2) = |(\\lambda x_1 + (1-\\lambda) x_2) - 1|$$\n    We can rewrite the expression inside the absolute value as:\n    $$|\\lambda x_1 + (1-\\lambda) x_2 - (\\lambda + (1-\\lambda))| = |\\lambda(x_1 - 1) + (1-\\lambda)(x_2 - 1)|$$\n    Applying the triangle inequality:\n    $$|\\lambda(x_1 - 1) + (1-\\lambda)(x_2 - 1)| \\le |\\lambda(x_1-1)| + |(1-\\lambda)(x_2-1)|$$\n    $$= \\lambda |x_1-1| + (1-\\lambda) |x_2-1| = \\lambda f_2(x_1) + (1-\\lambda) f_2(x_2)$$\n    This proves that $f_2(x) = |x-1|$ is also a convex function.\n\n3.  Sum of Convex Functions:\n    A fundamental property of convex functions is that their sum is also convex. If $f_1$ and $f_2$ are convex, then $f = f_1 + f_2$ is convex.\n    Proof:\n    $$f(\\lambda x_1 + (1-\\lambda) x_2) = f_1(\\lambda x_1 + (1-\\lambda) x_2) + f_2(\\lambda x_1 + (1-\\lambda) x_2)$$\n    By the convexity of $f_1$ and $f_2$:\n    $$\\le [\\lambda f_1(x_1) + (1-\\lambda) f_1(x_2)] + [\\lambda f_2(x_1) + (1-\\lambda) f_2(x_2)]$$\n    Rearranging terms:\n    $$= \\lambda (f_1(x_1) + f_2(x_1)) + (1-\\lambda) (f_1(x_2) + f_2(x_2)) = \\lambda f(x_1) + (1-\\lambda) f(x_2)$$\n    Since $f(x) = |x|+|x-1|$ is the sum of two convex functions, $f(x)$ is itself convex.\n\n(b) Subdifferential at $x_0 = 0.5$\n\nThe subdifferential $\\partial f(x_0)$ is the set of all subgradients $g$ at $x_0$. A real number $g$ is a subgradient of $f$ at $x_0$ if $f(y) \\ge f(x_0) + g(y-x_0)$ for all $y \\in \\mathbb{R}$. We want to find $\\partial f(0.5)$.\n\nFirst, let's evaluate the function at $x_0 = 0.5$:\n$$f(0.5) = |0.5| + |0.5 - 1| = 0.5 + |-0.5| = 0.5 + 0.5 = 1$$\nThe subgradient inequality is:\n$$f(y) \\ge 1 + g(y - 0.5)$$\n$$|y| + |y-1| \\ge 1 + g(y - 0.5)$$\nTo analyze this, let's express $f(x)$ as a piecewise function:\n$$f(x) = \\begin{cases} -2x+1  \\text{if } x  0 \\\\ 1  \\text{if } 0 \\le x \\le 1 \\\\ 2x-1  \\text{if } x  1 \\end{cases}$$\nThe point $x_0 = 0.5$ lies in the open interval $(0, 1)$. Within this interval, the function $f(x)$ is constant, $f(x) = 1$. This means that for any $x \\in (0, 1)$, $f(x)$ is differentiable and its derivative is $f'(x) = 0$.\nFor a convex function that is differentiable at a point $x_0$, the subdifferential is a singleton set containing only the derivative at that point: $\\partial f(x_0) = \\{f'(x_0)\\}$.\nSince $f$ is differentiable at $x_0=0.5$ with $f'(0.5)=0$, we can conclude that the subdifferential is $\\partial f(0.5) = \\{0\\}$.\n\nTo confirm this by direct application of the inequality, we must check if $g=0$ is the unique solution to $|y| + |y-1| \\ge 1 + g(y - 0.5)$ for all $y \\in \\mathbb{R}$.\nFor $g=0$, the inequality becomes $|y| + |y-1| \\ge 1$. This is a form of the triangle inequality: $|y| + |1-y| \\ge |y + (1-y)| = |1| = 1$. This inequality holds for all $y \\in \\mathbb{R}$. So, $g=0$ is a subgradient.\n\nNow, we check if any non-zero $g$ can be a subgradient.\nCase 1: $g  0$. The inequality is $|y| + |y-1| - 1 \\ge g(y-0.5)$. The left-hand side is always non-negative. Let's choose $y  0.5$. For instance, take $y=1$. The inequality becomes $|1|+|1-1|-1 \\ge g(1-0.5)$, which simplifies to $0 \\ge 0.5g$. Since $g0$, this statement is false. Thus, no $g0$ can be a subgradient.\nCase 2: $g  0$. Let's choose $y  0.5$. For instance, take $y=0$. The inequality becomes $|0|+|0-1|-1 \\ge g(0-0.5)$, which simplifies to $0 \\ge -0.5g$. Since $g0$, $-0.5g$ is positive, so the statement $0 \\ge -0.5g$ is false. Thus, no $g0$ can be a subgradient.\n\nThe only value for which the inequality holds for all $y \\in \\mathbb{R}$ is $g=0$. Therefore, the subdifferential is the singleton set $\\partial f(0.5)=\\{0\\}$. The unique element is $0$.\n\n(c) Discussion on Sharp vs. Flat Minima\n\nA point $x_0$ is a global minimizer of a convex function $f$ if and only if $0 \\in \\partial f(x_0)$. Our result from part (b), $\\partial f(0.5)=\\{0\\}$, confirms that $x_0 = 0.5$ is a minimizer of $f(x)$.\n\nThe structure of the subdifferential provides insight into the nature of the minimum.\nA subdifferential that is a singleton set, $\\partial f(x_0) = \\{g_0\\}$, indicates that the function is differentiable at $x_0$, and $g_0 = f'(x_0)$. In our case, $\\partial f(0.5)=\\{0\\}$ means $f'(0.5)=0$.\n\nThis situation can arise in two distinct scenarios for a minimizer:\n1.  **Sharp Minimum of a Strictly Convex Function**: For a strictly convex function like $f(x)=x^2$, the minimum occurs at $x=0$. The function is differentiable there with $f'(0)=0$, so $\\partial f(0)=\\{0\\}$. The minimum is \"sharp\" because it is attained at a unique point, and the function value increases strictly for any deviation from this point.\n2.  **Point within a Flat Region of Minima**: For a convex function that is not strictly convex, the set of minimizers can be an interval. For any point $x_0$ in the interior of this interval, the function is locally constant, a \"flat region\". Consequently, the function is differentiable with a derivative of zero at $x_0$, leading to a subdifferential of $\\{0\\}$.\n\nFor our function $f(x)=|x|+|x-1|$, the minimum value of $1$ is attained for all $x \\in [0, 1]$. This is a flat region of minima. The point $x=0.5$ lies in the interior of this region. The function is not strictly convex on this interval; for instance, for $x_1=0.2$ and $x_2=0.8$, and $\\lambda=0.5$, we have $f(0.5 \\cdot 0.2 + 0.5 \\cdot 0.8) = f(0.5)=1$, and $0.5 f(0.2) + 0.5 f(0.8) = 0.5(1)+0.5(1)=1$. The equality in the convexity definition shows the lack of strict convexity.\n\nTherefore, at $x=0.5$, the singleton subdifferential $\\partial f(0.5)=\\{0\\}$ classifies the point as being part of a **flat region of minima**. This contrasts with a \"sharp\" or \"V-shaped\" minimum, which is often characterized by a non-differentiable point where the subdifferential is a non-trivial interval containing zero. For example, for $h(x)=|x|$, the minimum is at $x=0$, and the subdifferential is $\\partial h(0) = [-1, 1]$. The non-singleton nature of the subdifferential at the points $x=0$ and $x=1$ for our function $f(x)$ (where $\\partial f(0)=[-2,0]$ and $\\partial f(1)=[0,2]$) indicates the \"corners\" at the boundaries of the flat region.", "answer": "$$\\boxed{0}$$", "id": "3189295"}, {"introduction": "Having explored a point where the function is differentiable, we now turn to the more intricate case of a \"kink\" or non-differentiable point. This practice focuses on the function $f(x)=\\max(0, |x|-1)$, which features sharp corners. You will see firsthand how the subdifferential at such a point is no longer a single value but an interval bounded by the left-hand and right-hand derivatives, perfectly capturing the geometry of the V-shape [@problem_id:3189356]. This exercise is crucial for understanding how subgradients generalize the concept of derivatives to non-smooth functions.", "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be defined by $f(x)=\\max\\!\\big(0,|x|-1\\big)$. Using the definition of a subgradient, namely that $g\\in\\mathbb{R}$ is a subgradient of $f$ at $x_{0}$ if and only if $f(y)\\ge f(x_{0})+g\\,(y-x_{0})$ for all $y\\in\\mathbb{R}$, determine the subdifferentials $\\partial f(1)$ and $\\partial f(-1)$. In your reasoning, connect the left and right derivatives at a point of nondifferentiability to the set of subgradients, and explain how the one-sided slopes bound the subdifferential. Finally, report a single numerical quantity: the sum of the diameters (lengths) of $\\partial f(1)$ and $\\partial f(-1)$. Express your final answer as an exact integer.", "solution": "The problem requires the determination of the subdifferentials of the function $f(x)=\\max(0, |x|-1)$ at the points $x=1$ and $x=-1$, and then to compute the sum of the diameters of these subdifferential sets.\n\nFirst, we analyze the function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x) = \\max(0, |x|-1)$. We can express this function in a piecewise form.\nThe term $|x|-1$ is non-negative when $|x| \\ge 1$ (i.e., $x \\ge 1$ or $x \\le -1$) and negative when $|x|  1$ (i.e., $-1  x  1$).\nTherefore, the function $f(x)$ can be written as:\n$$\nf(x) =\n\\begin{cases}\n|x|-1  \\text{if } |x| \\ge 1 \\\\\n0        \\text{if } |x|  1\n\\end{cases}\n$$\nExpanding the absolute value $|x|$ further, we get:\n$$\nf(x) =\n\\begin{cases}\n-x-1  \\text{if } x \\le -1 \\\\\n0       \\text{if } -1  x  1 \\\\\nx-1     \\text{if } x \\ge 1\n\\end{cases}\n$$\nThis function is continuous and convex on $\\mathbb{R}$. The points of non-differentiability are $x=-1$ and $x=1$.\n\nThe problem provides the definition of a subgradient. A scalar $g \\in \\mathbb{R}$ is a subgradient of a function $f$ at a point $x_0$ if for all $y \\in \\mathbb{R}$, the following inequality holds:\n$$f(y) \\ge f(x_0) + g(y-x_0)$$\nThe set of all such subgradients at $x_0$ is called the subdifferential of $f$ at $x_0$, denoted by $\\partial f(x_0)$.\n\nAs requested, we connect this definition to the one-sided derivatives of the function.\nConsider the subgradient inequality $f(y) - f(x_0) \\ge g(y-x_0)$.\nIf we take $y  x_0$, we can divide by the positive quantity $y-x_0$ to get:\n$$\\frac{f(y) - f(x_0)}{y - x_0} \\ge g$$\nTaking the limit as $y$ approaches $x_0$ from the right ($y \\to x_0^+$), we obtain the relationship involving the right derivative, $f'_+(x_0)$:\n$$f'_+(x_0) = \\lim_{y \\to x_0^+} \\frac{f(y) - f(x_0)}{y - x_0} \\ge g$$\nIf we take $y  x_0$, we can divide by the negative quantity $y-x_0$, which reverses the inequality:\n$$\\frac{f(y) - f(x_0)}{y - x_0} \\le g$$\nTaking the limit as $y$ approaches $x_0$ from the left ($y \\to x_0^-$), we obtain the relationship involving the left derivative, $f'_-(x_0)$:\n$$f'_{-}(x_0) = \\lim_{y \\to x_0^-} \\frac{f(y) - f(x_0)}{y - x_0} \\le g$$\nCombining these two results, any subgradient $g$ at $x_0$ must satisfy the condition $f'_{-}(x_0) \\le g \\le f'_+(x_0)$. For a convex function, this condition is also sufficient. Thus, the subdifferential at a point $x_0$ is the closed interval bounded by the left and right derivatives:\n$$\\partial f(x_0) = [f'_{-}(x_0), f'_+(x_0)]$$\n\nNow, we apply this to find the subdifferential $\\partial f(1)$.\nThe point of interest is $x_0 = 1$. From our piecewise definition, $f(1) = 1-1=0$.\nThe left derivative at $x_0=1$ is:\n$$f'_{-}(1) = \\lim_{h \\to 0^-} \\frac{f(1+h) - f(1)}{h}$$\nFor $h0$ and close to $0$, we have $-1  1+h  1$, so $f(1+h)=0$. Thus,\n$$f'_{-}(1) = \\lim_{h \\to 0^-} \\frac{0 - 0}{h} = 0$$\nThe right derivative at $x_0=1$ is:\n$$f'_+(1) = \\lim_{h \\to 0^+} \\frac{f(1+h) - f(1)}{h}$$\nFor $h0$, we have $1+h  1$, so $f(1+h) = (1+h)-1 = h$. Thus,\n$$f'_+(1) = \\lim_{h \\to 0^+} \\frac{h - 0}{h} = \\lim_{h \\to 0^+} 1 = 1$$\nTherefore, the subdifferential of $f$ at $x=1$ is the interval:\n$$\\partial f(1) = [0, 1]$$\n\nNext, we find the subdifferential $\\partial f(-1)$.\nThe point of interest is $x_0 = -1$. From our piecewise definition, we use the case for $x \\le -1$, so $f(-1) = -(-1)-1=0$.\nThe left derivative at $x_0=-1$ is:\n$$f'_{-}(-1) = \\lim_{h \\to 0^-} \\frac{f(-1+h) - f(-1)}{h}$$\nFor $h0$, we have $-1+h  -1$, so $f(-1+h) = -(-1+h)-1 = 1-h-1 = -h$. Thus,\n$$f'_{-}(-1) = \\lim_{h \\to 0^-} \\frac{-h - 0}{h} = \\lim_{h \\to 0^-} -1 = -1$$\nThe right derivative at $x_0=-1$ is:\n$$f'_+(-1) = \\lim_{h \\to 0^+} \\frac{f(-1+h) - f(-1)}{h}$$\nFor $h0$ and close to $0$, we have $-1  -1+h  1$, so $f(-1+h)=0$. Thus,\n$$f'_+(-1) = \\lim_{h \\to 0^+} \\frac{0 - 0}{h} = 0$$\nTherefore, the subdifferential of $f$ at $x=-1$ is the interval:\n$$\\partial f(-1) = [-1, 0]$$\n\nFinally, the problem asks for the sum of the diameters (lengths) of these two subdifferential sets. The diameter of an interval $[a,b]$ is defined as $b-a$.\nThe diameter of $\\partial f(1) = [0, 1]$ is $1 - 0 = 1$.\nThe diameter of $\\partial f(-1) = [-1, 0]$ is $0 - (-1) = 1$.\nThe sum of the diameters is $1 + 1 = 2$.", "answer": "$$\\boxed{2}$$", "id": "3189356"}, {"introduction": "Our final practice elevates the concept from one dimension to the multi-dimensional space $\\mathbb{R}^n$, which is the setting for most real-world optimization problems. By examining the subdifferentials of the fundamental $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ norms at the origin, you will uncover a profound connection between a norm and its dual norm [@problem_id:3189334]. This exercise not only deepens your theoretical understanding but also provides valuable intuition for why the choice of norm matters in applications like machine learning and how the \"size\" of the subdifferential can influence the behavior of optimization algorithms.", "problem": "Consider the functions $f_1(x) = \\lVert x \\rVert_1$, $f_2(x) = \\lVert x \\rVert_2$, and $f_\\infty(x) = \\lVert x \\rVert_\\infty$ on $\\mathbb{R}^n$, where $n \\geq 1$. Recall the definition of the subdifferential of a proper convex function $f$ at a point $x$:\n$$\n\\partial f(x) \\;=\\; \\{\\, g \\in \\mathbb{R}^n \\;:\\; f(y) \\;\\ge\\; f(x) + g^\\top (y - x)\\;\\;\\text{for all}\\;\\; y \\in \\mathbb{R}^n \\,\\} \\, .\n$$\nAlso recall the dual norm of a norm $\\lVert \\cdot \\rVert$ defined by\n$$\n\\lVert g \\rVert_* \\;=\\; \\sup\\{\\, g^\\top y \\;:\\; \\lVert y \\rVert \\le 1 \\,\\} \\, .\n$$\nUsing only these core definitions, reason about the subdifferentials $\\partial f_1(0)$, $\\partial f_2(0)$, and $\\partial f_\\infty(0)$ at $x = 0$ and compare their “sizes” in the sense of set inclusion. Then, consider how these sizes could influence algorithmic design near $x=0$ for nonsmooth optimization, for example the use of the Proximal Gradient Method (PGM) or the Proximal Point Method (PPM).\n\nWhich of the following statements is true?\n\nA. At $x = 0$, $\\partial f_1(0) = \\{ g : \\lVert g \\rVert_\\infty \\le 1 \\}$, $\\partial f_2(0) = \\{ g : \\lVert g \\rVert_2 \\le 1 \\}$, and $\\partial f_\\infty(0) = \\{ g : \\lVert g \\rVert_1 \\le 1 \\}$, which satisfy $\\partial f_\\infty(0) \\subset \\partial f_2(0) \\subset \\partial f_1(0)$. The larger subdifferential at $x=0$ for $f_1$ implies more variability in subgradient choices for plain subgradient methods near $0$, making proximal methods attractive for stability.\n\nB. At $x = 0$, $\\partial f_1(0) = \\{ g : \\lVert g \\rVert_1 \\le 1 \\}$, $\\partial f_2(0) = \\{ g : \\lVert g \\rVert_2 \\le 1 \\}$, and $\\partial f_\\infty(0) = \\{ g : \\lVert g \\rVert_\\infty \\le 1 \\}$, so the three sets are identical; thus, algorithmic behavior near $0$ is essentially the same across these norms.\n\nC. At $x = 0$, $\\partial f_2(0) = \\{ 0 \\}$, because the Euclidean norm is differentiable everywhere; therefore, subgradient methods do not face ambiguity for $f_2$, unlike $f_1$ and $f_\\infty$.\n\nD. The inclusion between subdifferentials at $x = 0$ is $\\partial f_1(0) \\subset \\partial f_2(0) \\subset \\partial f_\\infty(0)$, so the $\\ell_\\infty$ norm has the smallest subdifferential. Therefore, subgradient methods are always fastest for $f_\\infty$ near $0$.", "solution": "First, we determine the subdifferential $\\partial f(0)$ for a general norm $f(x) = \\lVert x \\rVert$. According to the definition of the subdifferential, a vector $g \\in \\mathbb{R}^n$ is in $\\partial f(0)$ if and only if for all $y \\in \\mathbb{R}^n$:\n$$ f(y) \\ge f(0) + g^\\top(y-0) $$\nGiven $f(x) = \\lVert x \\rVert$, we have $f(0) = \\lVert 0 \\rVert = 0$. The inequality becomes:\n$$ \\lVert y \\rVert \\ge g^\\top y \\quad \\text{for all } y \\in \\mathbb{R}^n $$\nThis inequality must hold for all $y$. If we consider vectors $y$ such that $\\lVert y \\rVert \\le 1$, the condition is still $\\lVert y \\rVert \\ge g^\\top y$. Let us take the supremum of $g^\\top y$ over the set $\\{y : \\lVert y \\rVert \\le 1\\}$.\nIf $g \\in \\partial \\lVert \\cdot \\rVert(0)$, then for any $y$ with $\\lVert y \\rVert \\le 1$, we have $g^\\top y \\le \\lVert y \\rVert \\le 1$. This implies that $\\sup \\{g^\\top y : \\lVert y \\rVert \\le 1\\} \\le 1$.\nThe term $\\sup \\{g^\\top y : \\lVert y \\rVert \\le 1\\}$ is precisely the definition of the dual norm, $\\lVert g \\rVert_*$, provided in the problem statement. Thus, the condition for $g$ to be in the subdifferential at $0$ is $\\lVert g \\rVert_* \\le 1$.\nThe subdifferential of a norm at the origin is the unit ball of the dual norm:\n$$ \\partial \\lVert \\cdot \\rVert(0) = \\{g \\in \\mathbb{R}^n : \\lVert g \\rVert_* \\le 1\\} $$\nNow, we apply this general result to the specific norms $f_1, f_2, f_\\infty$. We need to identify their dual norms.\nIt is a standard result in functional analysis that for $p, q \\in [1, \\infty]$ with $1/p + 1/q = 1$, the $\\ell_p$-norm and $\\ell_q$-norm are dual to each other.\n1.  For $f_1(x) = \\lVert x \\rVert_1$ ($p=1$), its dual norm corresponds to $q=\\infty$. Thus, the dual norm of $\\lVert \\cdot \\rVert_1$ is $\\lVert \\cdot \\rVert_\\infty$. The subdifferential is:\n    $$ \\partial f_1(0) = \\{ g \\in \\mathbb{R}^n : \\lVert g \\rVert_\\infty \\le 1 \\} $$\n    This set is the unit hypercube centered at the origin, defined by $\\{ g : \\max_i |g_i| \\le 1 \\}$.\n\n2.  For $f_2(x) = \\lVert x \\rVert_2$ ($p=2$), its dual norm corresponds to $q=2$ (since $1/2 + 1/2 = 1$). The $\\ell_2$-norm is self-dual. The subdifferential is:\n    $$ \\partial f_2(0) = \\{ g \\in \\mathbb{R}^n : \\lVert g \\rVert_2 \\le 1 \\} $$\n    This set is the closed unit Euclidean ball centered at the origin.\n\n3.  For $f_\\infty(x) = \\lVert x \\rVert_\\infty$ ($p=\\infty$), its dual norm corresponds to $q=1$. Thus, the dual norm of $\\lVert \\cdot \\rVert_\\infty$ is $\\lVert \\cdot \\rVert_1$. The subdifferential is:\n    $$ \\partial f_\\infty(0) = \\{ g \\in \\mathbb{R}^n : \\lVert g \\rVert_1 \\le 1 \\} $$\n    This set is the cross-polytope (or generalized octahedron).\n\nNext, we compare these sets using set inclusion. For any vector $g \\in \\mathbb{R}^n$ and $n \\ge 1$, the following inequalities hold:\n$$ \\lVert g \\rVert_\\infty \\le \\lVert g \\rVert_2 \\le \\lVert g \\rVert_1 $$\nLet's verify these inequalities.\n$\\lVert g \\rVert_\\infty \\le \\lVert g \\rVert_2$: Let $|g_k| = \\max_i |g_i| = \\lVert g \\rVert_\\infty$. Then $\\lVert g \\rVert_2^2 = \\sum_{i=1}^n g_i^2 \\ge g_k^2 = \\lVert g \\rVert_\\infty^2$. Taking the square root gives $\\lVert g \\rVert_2 \\ge \\lVert g \\rVert_\\infty. $\n$\\lVert g \\rVert_2 \\le \\lVert g \\rVert_1$: We have $(\\lVert g \\rVert_1)^2 = (\\sum_{i=1}^n |g_i|)^2 = \\sum_{i=1}^n g_i^2 + \\sum_{i \\ne j} |g_i||g_j|$. Since the cross-terms are non-negative, $(\\lVert g \\rVert_1)^2 \\ge \\sum_{i=1}^n g_i^2 = \\lVert g \\rVert_2^2$. Taking the square root gives $\\lVert g \\rVert_1 \\ge \\lVert g \\rVert_2$.\n\nUsing these inequalities, we can establish the set inclusions:\n- If a vector $g$ is in $\\partial f_\\infty(0)$, then $\\lVert g \\rVert_1 \\le 1$. From $\\lVert g \\rVert_2 \\le \\lVert g \\rVert_1$, it follows that $\\lVert g \\rVert_2 \\le 1$, so $g \\in \\partial f_2(0)$. Thus, $\\partial f_\\infty(0) \\subseteq \\partial f_2(0)$.\n- If a vector $g$ is in $\\partial f_2(0)$, then $\\lVert g \\rVert_2 \\le 1$. From $\\lVert g \\rVert_\\infty \\le \\lVert g \\rVert_2$, it follows that $\\lVert g \\rVert_\\infty \\le 1$, so $g \\in \\partial f_1(0)$. Thus, $\\partial f_2(0) \\subseteq \\partial f_1(0)$.\n\nCombining these, we get the chain of inclusions:\n$$ \\partial f_\\infty(0) \\subseteq \\partial f_2(0) \\subseteq \\partial f_1(0) $$\nFor $n1$, these inclusions are strict. For example, if $n=2$, the vector $g=(1, 1/\\sqrt{2})$ is not in $\\partial f_2(0)$ since $\\lVert g \\rVert_2 = \\sqrt{1+1/2}  1$, but it is in $\\partial f_1(0)$ since $\\lVert g \\rVert_\\infty = 1$. The vector $g=(1/\\sqrt{2}, 1/\\sqrt{2})$ is in $\\partial f_2(0)$ since $\\lVert g \\rVert_2 = 1$, but not in $\\partial f_\\infty(0)$ since $\\lVert g \\rVert_1 = 2/\\sqrt{2} = \\sqrt{2}  1$.\n\nFinally, we consider the algorithmic implications. The subgradient method takes steps $x_{k+1} = x_k - \\alpha_k g_k$ where $g_k \\in \\partial f(x_k)$. When $x_k$ is close to $0$, any $g \\in \\partial f(0)$ is a plausible subgradient. The set $\\partial f_1(0)$ is the largest of the three. This means there is a wider range of possible subgradient directions for the $\\ell_1$ norm at the origin. This variability can make the trajectory of a simple subgradient method erratic or slow to converge near the nondifferentiable point. Proximal methods, which replace the subgradient step with the evaluation of a proximal operator, are specifically designed to handle such nonsmoothness in a more stable manner. The Proximal Point Method update, $x_{k+1} = \\text{prox}_{\\alpha_k f}(x_k)$, involves solving a small, regularized optimization problem that effectively \"resolves\" the ambiguity in the subdifferential. This makes the reasoning about the attractiveness of proximal methods for norms with large subdifferentials (like $\\ell_1$) sound.\n\nNow we evaluate the given options.\n\nA. At $x = 0$, $\\partial f_1(0) = \\{ g : \\lVert g \\rVert_\\infty \\le 1 \\}$, $\\partial f_2(0) = \\{ g : \\lVert g \\rVert_2 \\le 1 \\}$, and $\\partial f_\\infty(0) = \\{ g : \\lVert g \\rVert_1 \\le 1 \\}$, which satisfy $\\partial f_\\infty(0) \\subset \\partial f_2(0) \\subset \\partial f_1(0)$. The larger subdifferential at $x=0$ for $f_1$ implies more variability in subgradient choices for plain subgradient methods near $0$, making proximal methods attractive for stability.\n- This statement correctly identifies all three subdifferentials.\n- It correctly states the set inclusion relationship (using strict inclusion, which is true for $n1$).\n- The reasoning about algorithmic implications is correct and aligns with standard understanding in nonsmooth optimization.\n- Verdict: **Correct**.\n\nB. At $x = 0$, $\\partial f_1(0) = \\{ g : \\lVert g \\rVert_1 \\le 1 \\}$, $\\partial f_2(0) = \\{ g : \\lVert g \\rVert_2 \\le 1 \\}$, and $\\partial f_\\infty(0) = \\{ g : \\lVert g \\rVert_\\infty \\le 1 \\}$, so the three sets are identical; thus, algorithmic behavior near $0$ is essentially the same across these norms.\n- The characterizations of $\\partial f_1(0)$ and $\\partial f_\\infty(0)$ are incorrect; they are swapped.\n- The claim that the three sets are identical is false for $n1$.\n- Verdict: **Incorrect**.\n\nC. At $x = 0$, $\\partial f_2(0) = \\{ 0 \\}$, because the Euclidean norm is differentiable everywhere; therefore, subgradient methods do not face ambiguity for $f_2$, unlike $f_1$ and $f_\\infty$.\n- The premise that the Euclidean norm is differentiable everywhere is false. It is not differentiable at $x=0$.\n- The claim that $\\partial f_2(0) = \\{0\\}$ is false. As derived, $\\partial f_2(0)$ is the unit Euclidean ball. A function is differentiable at a point if and only if its subdifferential at that point is a singleton set. Since $\\partial f_2(0)$ is not a singleton, $f_2$ is not differentiable at $0$.\n- Verdict: **Incorrect**.\n\nD. The inclusion between subdifferentials at $x = 0$ is $\\partial f_1(0) \\subset \\partial f_2(0) \\subset \\partial f_\\infty(0)$, so the $\\ell_\\infty$ norm has the smallest subdifferential. Therefore, subgradient methods are always fastest for $f_\\infty$ near $0$.\n- The inclusion relationship is stated in the reverse order of the correct one.\n- The conclusion that the subdifferential of the $\\ell_\\infty$ norm, $\\partial f_\\infty(0)$, is the smallest set is correct, but it is derived from a false premise within the option's text.\n- The final claim that subgradient methods are \"always fastest\" for $f_\\infty$ is an unsubstantiated oversimplification. Convergence speed depends on many factors, and such a strong, general claim is not defensible.\n- Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3189334"}]}