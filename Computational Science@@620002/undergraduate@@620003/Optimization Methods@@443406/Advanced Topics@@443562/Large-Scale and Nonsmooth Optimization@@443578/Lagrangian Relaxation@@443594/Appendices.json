{"hands_on_practices": [{"introduction": "One of the primary strengths of Lagrangian relaxation is its ability to decompose a complex, coupled optimization problem into many smaller, independent subproblems that are easy to solve. This exercise provides a foundational example of this powerful principle. By relaxing the complicating equality constraints $Ax=b$, we will derive the closed-form solution to the resulting subproblem and observe how it exhibits a simple, intuitive \"thresholding\" behavior based on the value of the Lagrange multipliers [@problem_id:3141448].", "problem": "Consider the linear optimization problem with box constraints: minimize the linear objective $\\;c^{\\top}x\\;$ subject to the equality constraint $\\;Ax=b\\;$ and bounds $\\;0 \\le x \\le u\\;$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $c \\in \\mathbb{R}^{n}$, $u \\in \\mathbb{R}^{n}$ with $u_{i}  0$ for all indices $i \\in \\{1,\\dots,n\\}$, and inequalities are understood componentwise. Perform Lagrangian relaxation of the equality constraint $\\;Ax=b\\;$ by introducing the vector of Lagrange multipliers $\\;\\lambda \\in \\mathbb{R}^{m}\\;$, and consider the resulting Lagrangian function $\\;L(x,\\lambda)\\;$ and dual function $\\;g(\\lambda) = \\inf_{0 \\le x \\le u} L(x,\\lambda)\\;$.\n\nStarting only from the definition of the Lagrangian for equality-constrained problems and the basic properties of minimizing a linear function over a closed interval, derive the componentwise minimizer $x(\\lambda)$ that attains the infimum in the dual function. In particular, for each index $i$, determine the form of\n$$\nx_{i}(\\lambda) \\in \\arg\\min_{x_{i} \\in [0,u_{i}]} \\left( c_{i} + a_{i}^{\\top}\\lambda \\right) x_{i},\n$$\nwhere $a_{i} \\in \\mathbb{R}^{m}$ denotes the $i$-th column of the matrix $A$. Show and explain the thresholding behavior with respect to the sign of the scalar $\\;c_{i} + a_{i}^{\\top}\\lambda\\;$.\n\nFor definiteness and to ensure a single-valued answer, when the objective is flat over the interval (that is, when $c_{i} + a_{i}^{\\top}\\lambda = 0$), select the smallest minimizer in the set, namely $x_{i}(\\lambda) = 0$. Provide the final closed-form analytic expression for $x_{i}(\\lambda)$ as a function of $\\lambda$, $c_{i}$, $a_{i}$, and $u_{i}$. The final answer must be a single analytic expression and must not include any units. No numerical rounding is required.", "solution": "The user wants to solve the problem statement.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\n- **Problem Type**: Linear optimization with box constraints.\n- **Objective Function**: Minimize $\\;c^{\\top}x\\;$.\n- **Constraints**:\n  1. Equality constraint: $\\;Ax=b\\;$.\n  2. Box constraints: $\\;0 \\le x \\le u\\;$.\n- **Variable and Constant Definitions**:\n  - $A \\in \\mathbb{R}^{m \\times n}$ is a matrix.\n  - $b \\in \\mathbb{R}^{m}$ is a vector.\n  - $c \\in \\mathbb{R}^{n}$ is a vector.\n  - $u \\in \\mathbb{R}^{n}$ is a vector of upper bounds with $u_{i}  0$ for all indices $i \\in \\{1,\\dots,n\\}$.\n  - $x \\in \\mathbb{R}^{n}$ is the optimization variable.\n  - Inequalities are componentwise.\n- **Methodology**:\n  - Lagrangian relaxation of the equality constraint $\\;Ax=b\\;$.\n  - Lagrange multipliers: $\\;\\lambda \\in \\mathbb{R}^{m}\\;$.\n  - Lagrangian function: $\\;L(x,\\lambda)\\;$.\n  - Dual function: $\\;g(\\lambda) = \\inf_{0 \\le x \\le u} L(x,\\lambda)\\;$.\n- **Task**:\n  - Derive the componentwise minimizer $x_{i}(\\lambda)$ that attains the infimum in the dual function.\n  - Specifically, for each index $i$, determine the form of $x_{i}(\\lambda) \\in \\arg\\min_{x_{i} \\in [0,u_{i}]} \\left( c_{i} + a_{i}^{\\top}\\lambda \\right) x_{i}$.\n  - $a_{i} \\in \\mathbb{R}^{m}$ denotes the $i$-th column of the matrix $A$.\n  - Explain the thresholding behavior with respect to the sign of the scalar $\\;c_{i} + a_{i}^{\\top}\\lambda\\;$.\n- **Tie-Breaking Rule**:\n  - If $c_{i} + a_{i}^{\\top}\\lambda = 0$, select the smallest minimizer, $x_{i}(\\lambda) = 0$.\n- **Required Output**: A single closed-form analytic expression for $x_{i}(\\lambda)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard application of Lagrangian duality theory to a linear program, a core topic in optimization methods. All concepts and formulations are well-established.\n- **Well-Posed**: The problem is clearly stated. It provides all necessary definitions ($\\;A, b, c, u\\;$) and constraints. The objective is unambiguous. The inclusion of a specific tie-breaking rule ensures that the minimizer $x(\\lambda)$ is uniquely defined, making the problem well-posed.\n- **Objective**: The language is formal, mathematical, and free of bias or subjectivity.\n\nThe problem statement is self-contained, consistent, and adheres to the principles of mathematical optimization. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Solution**\n\nThe primal optimization problem is given by:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  c^{\\top}x \\\\\n\\text{subject to} \\quad  Ax = b \\\\\n 0 \\le x \\le u\n\\end{aligned}\n$$\nwhere $0$, $x$, and $u$ are vectors in $\\mathbb{R}^{n}$ and the inequalities are componentwise.\n\nWe perform Lagrangian relaxation on the equality constraint $Ax=b$ by introducing a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^{m}$. The Lagrangian function $L(x, \\lambda)$ is defined as the sum of the original objective function and the dot product of the multipliers with the constraint function:\n$$\nL(x, \\lambda) = c^{\\top}x + \\lambda^{\\top}(Ax - b)\n$$\nThe dual function $g(\\lambda)$ is defined as the infimum of the Lagrangian over the remaining feasible set for $x$, which consists of the box constraints $0 \\le x \\le u$:\n$$\ng(\\lambda) = \\inf_{0 \\le x \\le u} L(x, \\lambda) = \\inf_{0 \\le x \\le u} \\left( c^{\\top}x + \\lambda^{\\top}(Ax - b) \\right)\n$$\nTo find this infimum, we can rearrange the terms in the Lagrangian that depend on $x$:\n$$\nc^{\\top}x + \\lambda^{\\top}(Ax - b) = c^{\\top}x + \\lambda^{\\top}Ax - \\lambda^{\\top}b = (c^{\\top} + \\lambda^{\\top}A)x - \\lambda^{\\top}b\n$$\nThe term $\\lambda^{\\top}b$ is constant with respect to $x$, so it does not affect the minimizer $x(\\lambda)$. The minimization problem becomes:\n$$\nx(\\lambda) = \\arg\\min_{0 \\le x \\le u} \\left( (c^{\\top} + \\lambda^{\\top}A)x \\right)\n$$\nLet's analyze the objective term $(c^{\\top} + \\lambda^{\\top}A)x$. This can be written as a sum over the components of $x$:\n$$\n(c^{\\top} + \\lambda^{\\top}A)x = \\sum_{i=1}^{n} (c_i + (\\lambda^{\\top}A)_i) x_i\n$$\nThe $i$-th component of the row vector $\\lambda^{\\top}A$ is the dot product of $\\lambda^{\\top}$ with the $i$-th column of $A$. The problem defines the $i$-th column of $A$ as $a_i$. Therefore, $(\\lambda^{\\top}A)_i = \\lambda^{\\top}a_i = a_i^{\\top}\\lambda$.\nSubstituting this back, the objective becomes:\n$$\n\\sum_{i=1}^{n} (c_i + a_i^{\\top}\\lambda) x_i\n$$\nThe box constraint $0 \\le x \\le u$ is equivalent to $n$ independent constraints $0 \\le x_i \\le u_i$ for each $i \\in \\{1, \\dots, n\\}$. Because the objective function is a sum of terms each involving only one component $x_i$, and the constraints are also separable by component, the $n$-dimensional minimization problem decomposes into $n$ independent one-dimensional minimization problems:\n$$\n\\min_{0 \\le x \\le u} \\sum_{i=1}^{n} (c_i + a_i^{\\top}\\lambda) x_i = \\sum_{i=1}^{n} \\min_{0 \\le x_i \\le u_i} \\left[ (c_i + a_i^{\\top}\\lambda) x_i \\right]\n$$\nThe minimizer $x(\\lambda) = (x_1(\\lambda), \\dots, x_n(\\lambda))^{\\top}$ is found by solving each one-dimensional problem for $x_i(\\lambda)$ independently. For each index $i \\in \\{1, \\dots, n\\}$, we must find:\n$$\nx_i(\\lambda) \\in \\arg\\min_{x_i \\in [0, u_i]} \\left( c_i + a_i^{\\top}\\lambda \\right) x_i\n$$\nLet the scalar coefficient of $x_i$ be denoted by $s_i = c_i + a_i^{\\top}\\lambda$. The problem for each component is to minimize a linear function $s_i x_i$ over the closed interval $[0, u_i]$. The solution depends entirely on the sign of the coefficient $s_i$. We analyze the three possible cases.\n\n**Case 1: $s_i  0$**\nIf $c_i + a_i^{\\top}\\lambda  0$, the objective $s_i x_i$ is a strictly increasing linear function of $x_i$. To minimize this function over the interval $[0, u_i]$, we must choose the smallest possible value for $x_i$. Therefore, the unique minimizer is $x_i(\\lambda) = 0$.\n\n**Case 2: $s_i  0$**\nIf $c_i + a_i^{\\top}\\lambda  0$, the objective $s_i x_i$ is a strictly decreasing linear function of $x_i$. To minimize this function over the interval $[0, u_i]$, we must choose the largest possible value for $x_i$. Therefore, the unique minimizer is $x_i(\\lambda) = u_i$.\n\n**Case 3: $s_i = 0$**\nIf $c_i + a_i^{\\top}\\lambda = 0$, the objective becomes $0 \\cdot x_i = 0$ for all values of $x_i$. In this case, the objective function is flat, and any value of $x_i$ in the interval $[0, u_i]$ is a minimizer. The problem provides a specific tie-breaking rule to ensure a unique solution: \"select the smallest minimizer in the set, namely $x_{i}(\\lambda) = 0$\".\n\nThis demonstrates the thresholding behavior. The value of $x_i(\\lambda)$ switches from one end of its feasible interval to the other based on the sign of the reduced cost $s_i = c_i + a_i^{\\top}\\lambda$. The threshold is $s_i=0$.\n\nCombining these three cases yields a complete definition for $x_i(\\lambda)$:\n- If $c_i + a_i^{\\top}\\lambda  0$, then $x_i(\\lambda) = 0$.\n- If $c_i + a_i^{\\top}\\lambda = 0$, then $x_i(\\lambda) = 0$.\n- If $c_i + a_i^{\\top}\\lambda  0$, then $x_i(\\lambda) = u_i$.\n\nThe first two conditions can be merged into one. Thus, we arrive at the final closed-form expression for $x_i(\\lambda)$ as a piecewise function:\n$$\nx_i(\\lambda) =\n\\begin{cases}\n0  \\text{if } c_i + a_i^{\\top}\\lambda \\ge 0 \\\\\nu_i  \\text{if } c_i + a_i^{\\top}\\lambda  0\n\\end{cases}\n$$\nThis is the required single analytic expression for the componentwise minimizer.", "answer": "$$\n\\boxed{\nx_{i}(\\lambda) = \\begin{cases} 0  \\text{if } c_{i} + a_{i}^{\\top}\\lambda \\ge 0 \\\\ u_{i}  \\text{if } c_{i} + a_{i}^{\\top}\\lambda  0 \\end{cases}\n}\n$$", "id": "3141448"}, {"introduction": "Lagrangian relaxation is a key technique for finding strong lower bounds for difficult integer programming problems like the Set Cover problem. This hands-on coding practice challenges you to compare the bound obtained from Lagrangian relaxation against the standard Linear Programming (LP) relaxation. You will discover how the strength of the Lagrangian bound is determined by the structural constraints retained in the subproblem, sometimes providing a significant advantage over the LP bound [@problem_id:3141444].", "problem": "Consider the Minimum Cost Set Cover problem defined as follows. Let there be a universe of $m$ elements indexed by $i \\in \\{1,\\dots,m\\}$ and a collection of $n$ sets indexed by $j \\in \\{1,\\dots,n\\}$. Let the binary decision variable $x_j \\in \\{0,1\\}$ indicate whether set $j$ is chosen. Let the coverage matrix $A \\in \\{0,1\\}^{m \\times n}$ have entries $A_{ij} = 1$ if and only if element $i$ is in set $j$, and $A_{ij} = 0$ otherwise. Let the cost vector be $c \\in \\mathbb{R}^n_{\\ge 0}$. The integer program is\n$$\n\\min_{x \\in \\{0,1\\}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1},\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^m$ is the all-ones vector.\n\nA common lower bound is obtained by the Linear Programming relaxation, which replaces $x_j \\in \\{0,1\\}$ with $x_j \\in [0,1]$:\n$$\n\\min_{x \\in [0,1]^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1}.\n$$\nAnother lower bound arises from Lagrangian relaxation, which dualizes one or more complicating constraints using nonnegative Lagrange multipliers while keeping some integrality or structural constraints intact. In general, for multipliers $u \\in \\mathbb{R}^m_{\\ge 0}$ associated with coverage constraints $A x \\ge \\mathbf{1}$, the Lagrangian function for a given structure-preserving subproblem is\n$$\n\\mathcal{L}(u) \\;=\\; \\min_{x \\in \\mathcal{X}} \\left\\{ c^\\top x + u^\\top (\\mathbf{1} - A x) \\right\\}\n\\;=\\; u^\\top \\mathbf{1} + \\min_{x \\in \\mathcal{X}} \\left\\{ \\sum_{j=1}^n \\big(c_j - (A^\\top u)_j\\big) x_j \\right\\},\n$$\nwhere $\\mathcal{X}$ encodes the constraints we keep (for example, integrality, cardinality, or knapsack constraints). The Lagrangian dual maximizes $\\mathcal{L}(u)$ over $u \\ge 0$:\n$$\n\\max_{u \\in \\mathbb{R}^m_{\\ge 0}} \\; \\mathcal{L}(u).\n$$\nWhen $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n \\}$ and all coverage constraints are relaxed, the resulting Lagrangian dual is equivalent to the dual of the Linear Programming relaxation, yielding the same lower bound. When $\\mathcal{X}$ retains additional structure such as a cardinality constraint $\\sum_{j=1}^n x_j \\le B$ or a knapsack constraint $\\sum_{j=1}^n w_j x_j \\le W$ with weights $w \\in \\mathbb{R}^n_{ 0}$ and capacity $W \\in \\mathbb{R}_{ 0}$, the Lagrangian relaxation can provide strictly tighter bounds by exploiting integrality or combinatorial structure in the subproblem.\n\nYour task is to write a program that, for each specified test instance, computes:\n- the Linear Programming relaxation lower bound, and\n- the Lagrangian relaxation lower bound by dualizing coverage constraints with multipliers $u \\ge 0$ and solving the remaining structure-preserving subproblem via a principled method. Use subgradient ascent on $u$ with a diminishing step size to approximately maximize the Lagrangian dual. For the subgradient $g(u)$, use the standard fact that a subgradient of $u^\\top \\mathbf{1} - u^\\top A x^\\star(u)$ at $u$ is $g(u) = \\mathbf{1} - A x^\\star(u)$, where $x^\\star(u)$ is an optimal solution of the current subproblem (with reduced costs $r_j(u) = c_j - (A^\\top u)_j$).\n\nThe subproblem to evaluate $\\mathcal{L}(u)$ differs by instance:\n- For the base Set Cover relaxation (no additional constraints beyond integrality), $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n \\}$, the subproblem minimizes $\\sum_j r_j x_j$, and the optimal $x^\\star(u)$ simply chooses all sets with $r_j(u)  0$; the corresponding Lagrangian dual bound equals the Linear Programming relaxation bound and can be computed exactly by solving the Linear Programming dual: maximize $u^\\top \\mathbf{1}$ subject to $A^\\top u \\le c$, $u \\ge 0$.\n- For the Cardinality-constrained Set Cover, $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n : \\sum_j x_j \\le B \\}$ with given integer $B$, the subproblem chooses up to $B$ sets with the most negative reduced costs $r_j(u)$. The subgradient ascent updates $u$ using $g(u) = \\mathbf{1} - A x^\\star(u)$.\n- For the Knapsack-constrained Set Cover, $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n : \\sum_j w_j x_j \\le W \\}$ with given weights $w_j$ and capacity $W$, the subproblem selects sets with negative reduced costs to minimize $\\sum_j r_j x_j$ subject to the knapsack constraint; equivalently, it is a $0$-$1$ knapsack problem with item profits $p_j(u) = \\max\\{ -r_j(u), 0 \\}$ and weights $w_j$ that maximizes total profit. Use dynamic programming to solve the knapsack subproblem exactly for the given small instances.\n\nFundamental base to use:\n- The definition of Linear Programming relaxation for integer programs, and its duality relating the primal relaxation to a dual problem.\n- The definition of Lagrangian relaxation and subgradient ascent for maximizing a concave dual function, where a subgradient is derived from the relaxed constraints’ residual, i.e., $g(u) = \\mathbf{1} - A x^\\star(u)$.\n- The standard $0$-$1$ knapsack dynamic programming recurrence for small instances.\n\nTest suite:\nProvide computations for the following three instances (all costs are unitless nonnegative reals, and there are no physical units involved):\n- Instance $1$ (Base Set Cover):\n    - $m = 3$, $n = 4$,\n    - Coverage matrix rows indexed by elements $1,2,3$ and columns indexed by sets $1,2,3,4$:\n      $A = \\begin{bmatrix}\n      1  0  1  0 \\\\\n      1  1  0  0 \\\\\n      0  1  0  1\n      \\end{bmatrix}$,\n    - Costs $c = [4, 4, 2, 2]$.\n- Instance $2$ (Cardinality-constrained Set Cover):\n    - Same $A$ and $c$ as Instance $1$,\n    - Cardinality bound $B = 2$.\n- Instance $3$ (Knapsack-constrained Set Cover):\n    - Same $A$ and $c$ as Instance $1$,\n    - Weights $w = [2, 2, 1, 1]$, capacity $W = 3$.\n\nOutput specification:\n- For each instance, compute the Linear Programming relaxation optimal cost and the Lagrangian relaxation lower bound using the prescribed method.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a two-element list $[v_{\\text{LP}}, v_{\\text{LR}}]$ of floats for the corresponding instance in order. For example: $[[v_{\\text{LP},1}, v_{\\text{LR},1}], [v_{\\text{LP},2}, v_{\\text{LR},2}], [v_{\\text{LP},3}, v_{\\text{LR},3}]]$.\n\nDesign for coverage:\n- Instance $1$ tests the equivalence between Linear Programming relaxation and Lagrangian relaxation when relaxing all coverage constraints without additional structure.\n- Instance $2$ tests a cardinality constraint, illustrating a structure where integrality interacts with Lagrangian relaxation; the Lagrangian lower bound is computed via subgradient ascent, and results can differ from the Linear Programming relaxation bound.\n- Instance $3$ tests a knapsack constraint, requiring dynamic programming in the subproblem to properly exploit structure; the Lagrangian relaxation can be tighter due to integrality in the subproblem.\n\nAnswer types:\n- For each instance, the pair $[v_{\\text{LP}}, v_{\\text{LR}}]$ must be floats.", "solution": "The problem requires computing and comparing two types of lower bounds for the Minimum Cost Set Cover problem and its variants: the Linear Programming (LP) relaxation bound, $v_{\\text{LP}}$, and the Lagrangian relaxation bound, $v_{\\text{LR}}$. This will be done for three specific instances designed to illustrate different aspects of these relaxation techniques.\n\nThe core problem is the Set Cover Integer Program (IP):\n$$\n\\min_{x \\in \\{0,1\\}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1}\n$$\nwhere $x_j$ is a binary variable indicating if set $j$ is chosen, $c_j$ is its cost, and the matrix $A$ ensures that each element in the universe is covered.\n\n**1. Linear Programming (LP) Relaxation**\n\nThe LP relaxation is formed by relaxing the integrality constraint $x_j \\in \\{0,1\\}$ to a continuous domain $x_j \\in [0,1]$. For the base Set Cover problem, the LP relaxation is:\n$$\nv_{\\text{LP}} = \\min_{x \\in [0,1]^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1}\n$$\nAny additional constraints in the original IP, such as cardinality or knapsack constraints, are retained in their linear form in the corresponding LP relaxation. The optimal value $v_{\\text{LP}}$ provides a lower bound on the optimal IP value, $v_{\\text{IP}}$, because the feasible region of the LP contains all feasible integer solutions. These LPs are solved using the `scipy.optimize.linprog` function.\n\n**2. Lagrangian Relaxation**\n\nLagrangian relaxation provides an alternative way to find lower bounds. It works by moving \"complicating\" constraints into the objective function with associated penalties, the Lagrange multipliers $u$. In this problem, we relax the coverage constraints $A x \\ge \\mathbf{1}$ (which can be written as $\\mathbf{1} - Ax \\le \\mathbf{0}$). For a vector of non-negative multipliers $u \\in \\mathbb{R}^m_{\\ge 0}$, the Lagrangian function is:\n$$\n\\mathcal{L}(u) = \\min_{x \\in \\mathcal{X}} \\left\\{ c^\\top x + u^\\top (\\mathbf{1} - A x) \\right\\}\n$$\nwhere $\\mathcal{X}$ is the set of feasible solutions defined by the retained constraints (e.g., integrality $x \\in \\{0,1\\}^n$, and possibly cardinality or knapsack constraints). This can be rewritten as:\n$$\n\\mathcal{L}(u) = u^\\top \\mathbf{1} + \\min_{x \\in \\mathcal{X}} \\left\\{ \\sum_{j=1}^n \\left(c_j - (A^\\top u)_j\\right) x_j \\right\\}\n$$\nThe term $r_j(u) = c_j - (A^\\top u)_j$ is the reduced cost of set $j$. The minimization subproblem over $x \\in \\mathcal{X}$ must be solved to evaluate $\\mathcal{L}(u)$. For any $u \\ge 0$, $\\mathcal{L}(u)$ is a lower bound on $v_{\\text{IP}}$. The tightest such bound is found by solving the Lagrangian dual problem:\n$$\nv_{\\text{LR}} = \\max_{u \\ge 0} \\mathcal{L}(u)\n$$\nSince $\\mathcal{L}(u)$ is a concave function, we can use the subgradient ascent method to find the maximum. A subgradient of $\\mathcal{L}(u)$ is given by $g(u) = \\mathbf{1} - A x^\\star(u)$, where $x^\\star(u)$ is an optimal solution to the subproblem for the given $u$. The iterative update for the multipliers is:\n$$\nu_{k+1} = \\max(0, u_k + \\alpha_k g(u_k))\n$$\nwhere $\\alpha_k$ is a step size. A diminishing step size rule, such as $\\alpha_k = \\frac{1}{k+1}$, is chosen for the iterations. The value $v_{\\text{LR}}$ is the maximum value of $\\mathcal{L}(u)$ encountered during the ascent. Theory establishes the relationship $v_{\\text{LP}} \\le v_{\\text{LR}} \\le v_{\\text{IP}}$.\n\nWe now analyze each instance. The common data are the cost vector $c = [4, 4, 2, 2]$ and the coverage matrix $A = \\begin{bmatrix} 1  0  1  0 \\\\ 1  1  0  0 \\\\ 0  1  0  1 \\end{bmatrix}$ for a universe of $m=3$ elements and a collection of $n=4$ sets.\n\n**Instance 1: Base Set Cover**\nThe retained constraints are simply the integrality constraints, $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n \\}$.\n-   **LP Relaxation**: The LP is $\\min_{x \\in [0,1]^4} c^\\top x$ subject to $Ax \\ge \\mathbf{1}$.\n-   **Lagrangian Relaxation**: The subproblem is $\\min_{x \\in \\{0,1\\}^4} \\sum_j r_j(u) x_j$. The optimal solution is to set $x_j^\\star(u) = 1$ if $r_j(u)  0$ and $x_j^\\star(u) = 0$ otherwise. As stated in the problem, the bound obtained from this Lagrangian dual is equal to the LP relaxation bound. We compute both using their respective standard methods to verify this.\n\n**Instance 2: Cardinality-constrained Set Cover**\nAn additional constraint $\\sum_{j=1}^4 x_j \\le B$ with $B=2$ is added.\n-   **LP Relaxation**: The LP is $\\min_{x \\in [0,1]^4} c^\\top x$ subject to $Ax \\ge \\mathbf{1}$ and $\\sum_j x_j \\le 2$.\n-   **Lagrangian Relaxation**: The subproblem's feasible set is $\\mathcal{X} = \\{ x \\in \\{0,1\\}^4 : \\sum_j x_j \\le 2 \\}$. To solve $\\min_{x \\in \\mathcal{X}} \\sum_j r_j(u) x_j$, we compute the reduced costs $r_j(u)$, sort them, and set $x_j^\\star=1$ for the up to $B=2$ sets with the most negative reduced costs. The integrality and cardinality constraints are handled together in the subproblem, which can lead to a tighter bound ($v_{\\text{LR}}  v_{\\text{LP}}$).\n\n**Instance 3: Knapsack-constrained Set Cover**\nAn additional constraint $\\sum_{j=1}^4 w_j x_j \\le W$ with weights $w = [2, 2, 1, 1]$ and capacity $W=3$ is added.\n-   **LP Relaxation**: The LP is $\\min_{x \\in [0,1]^4} c^\\top x$ subject to $Ax \\ge \\mathbf{1}$ and $\\sum_j w_j x_j \\le 3$.\n-   **Lagrangian Relaxation**: The subproblem's feasible set is $\\mathcal{X} = \\{ x \\in \\{0,1\\}^4 : \\sum_j w_j x_j \\le 3 \\}$. The subproblem $\\min_{x \\in \\mathcal{X}} \\sum_j r_j(u) x_j$ is equivalent to the 0-1 knapsack problem, where we seek to maximize total \"profit\" $\\sum_j p_j x_j$ with $p_j(u) = -r_j(u)$. This is solved exactly using a standard dynamic programming algorithm. The integer nature of the knapsack subproblem allows this relaxation to potentially yield a strictly better bound than the LP relaxation.\n\nThe implementation will now proceed to calculate $[v_{\\text{LP}}, v_{\\text{LR}}]$ for each of these three instances.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Computes LP and Lagrangian relaxation bounds for three Set Cover instances.\n    \"\"\"\n    \n    # Common parameters for all instances\n    A = np.array([[1, 0, 1, 0], [1, 1, 0, 0], [0, 1, 0, 1]], dtype=float)\n    c = np.array([4, 4, 2, 2], dtype=float)\n    m, n = A.shape\n    bounds = [(0, 1)] * n\n    N_ITER = 3000\n\n    def solve_subgradient_ascent(subproblem_solver, A_mat, c_vec, m_dim, n_dim):\n        \"\"\"Generic subgradient ascent solver.\"\"\"\n        u = np.zeros(m_dim)\n        best_L = -np.inf\n        for k in range(N_ITER):\n            r = c_vec - A_mat.T @ u\n            x_star = subproblem_solver(r)\n            \n            L_k = u.T @ np.ones(m_dim) + r.T @ x_star\n            if L_k > best_L:\n                best_L = L_k\n            \n            g = np.ones(m_dim) - A_mat @ x_star\n            if np.linalg.norm(g)  1e-9:\n                break\n            \n            alpha = 1.0 / (k + 1.0)\n            u = np.maximum(0, u + alpha * g)\n        return best_L\n\n    results = []\n\n    # --- Instance 1: Base Set Cover ---\n    # LP Relaxation\n    A_ub_1 = -A\n    b_ub_1 = -np.ones(m)\n    res_lp1 = linprog(c, A_ub=A_ub_1, b_ub=b_ub_1, bounds=bounds, method='highs')\n    v_lp1 = res_lp1.fun if res_lp1.success else np.nan\n\n    # Lagrangian Relaxation\n    def subproblem1(r):\n        return (r  0).astype(float)\n    v_lr1 = solve_subgradient_ascent(subproblem1, A, c, m, n)\n    results.append([v_lp1, v_lr1])\n\n    # --- Instance 2: Cardinality-constrained Set Cover ---\n    B = 2\n    # LP Relaxation\n    A_ub_2 = np.vstack([A_ub_1, np.ones(n)])\n    b_ub_2 = np.hstack([b_ub_1, B])\n    res_lp2 = linprog(c, A_ub=A_ub_2, b_ub=b_ub_2, bounds=bounds, method='highs')\n    v_lp2 = res_lp2.fun if res_lp2.success else np.nan\n\n    # Lagrangian Relaxation\n    def subproblem2(r):\n        x = np.zeros(n)\n        sorted_indices = np.argsort(r)\n        count = 0\n        for idx in sorted_indices:\n            if r[idx]  0 and count  B:\n                x[idx] = 1\n                count += 1\n            else:\n                break\n        return x\n    v_lr2 = solve_subgradient_ascent(subproblem2, A, c, m, n)\n    results.append([v_lp2, v_lr2])\n\n    # --- Instance 3: Knapsack-constrained Set Cover ---\n    w = np.array([2, 2, 1, 1], dtype=int)\n    W = 3\n    # LP Relaxation\n    A_ub_3 = np.vstack([A_ub_1, w])\n    b_ub_3 = np.hstack([b_ub_1, W])\n    res_lp3 = linprog(c, A_ub=A_ub_3, b_ub=b_ub_3, bounds=bounds, method='highs')\n    v_lp3 = res_lp3.fun if res_lp3.success else np.nan\n\n    # Lagrangian Relaxation (Knapsack subproblem)\n    def solve_knapsack_dp(profits, weights, capacity):\n        num_items = len(profits)\n        dp = np.zeros((num_items + 1, capacity + 1))\n\n        for i in range(1, num_items + 1):\n            profit = profits[i-1]\n            weight = weights[i-1]\n            for w_cap in range(capacity + 1):\n                dp[i, w_cap] = dp[i-1, w_cap]\n                if w_cap >= weight:\n                    dp[i, w_cap] = max(dp[i, w_cap], dp[i-1, w_cap - weight] + profit)\n\n        x_res = np.zeros(num_items)\n        w_rem = capacity\n        for i in range(num_items, 0, -1):\n            if not np.isclose(dp[i, w_rem], dp[i-1, w_rem]):\n                x_res[i-1] = 1\n                w_rem -= weights[i-1]\n        return x_res\n        \n    def subproblem3(r):\n        profits = -r\n        return solve_knapsack_dp(profits, w, W)\n        \n    v_lr3 = solve_subgradient_ascent(subproblem3, A, c, m, n)\n    results.append([v_lp3, v_lr3])\n    \n    # Format the final output\n    print(str(results))\n\nsolve()\n```", "id": "3141444"}, {"introduction": "The dual function we seek to maximize in Lagrangian relaxation has a special structure: it is always concave, but it is often not smooth. This thought experiment dives into the practical consequences of this non-smoothness, exploring a scenario where the dual function becomes completely flat over an interval. By analyzing this \"plateau,\" you will gain a deeper understanding of why simple subgradient methods can stall and appreciate the motivation for more advanced optimization algorithms [@problem_id:3141528].", "problem": "Consider the following Lagrangian relaxation of a single-constraint, bounded-variable linear optimization problem. Let $x \\in \\mathbb{R}^{3}$ with component-wise bounds $0 \\leq x_i \\leq 1$, cost vector $c = (1,\\,3,\\,0.4)$, weight vector $a = (2,\\,3,\\,1)$ with all components positive, and right-hand side $b = 3$. The primal problem is to minimize the linear cost subject to a single linear inequality:\n$$\n\\min_{x \\in [0,1]^3} \\; c^\\top x \\quad \\text{subject to} \\quad a^\\top x \\geq b.\n$$\nRelax the inequality constraint with a Lagrange multiplier $\\lambda \\geq 0$ and define the dual function $g(\\lambda)$ by the infimum of the Lagrangian over the bounded domain:\n$$\ng(\\lambda) \\;=\\; \\inf_{x \\in [0,1]^3} \\Big( c^\\top x + \\lambda \\big(b - a^\\top x\\big) \\Big), \\quad \\lambda \\in [0,\\infty).\n$$\nTasks:\n- Using first principles (definition of the Lagrangian and the dual function), derive $g(\\lambda)$ explicitly and identify any interval(s) of $\\lambda$ for which $g(\\lambda)$ is constant (flat). In particular, determine whether such an interval exists and, if so, specify it.\n- Based on your derivation, select all correct statements regarding subgradient selection on a flat dual segment and variable-metric methods that can escape plateaus:\n\nA. For any $\\lambda \\in (0.5,\\,1.0)$, the dual function $g(\\lambda)$ is constant and the subgradient set is the singleton $\\{0\\}$; a pure subgradient ascent method with any step-size will not move on this interval.\n\nB. At $\\lambda = 0.5$, any choice from the subgradient set is $0$, so the method cannot move off this breakpoint either.\n\nC. Replacing the Lagrangian by an augmented Lagrangian with a quadratic penalty and then applying a variable-metric method such as limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) on the resulting smooth dual can escape the plateau by creating curvature and nonzero gradients.\n\nD. Using only a diagonal scaling of the subgradient by a positive-definite matrix constructed from past subgradients guarantees escape from a flat region without any smoothing or stabilization.", "solution": "We start from the definitions. The Lagrangian for a single inequality constraint $a^\\top x \\geq b$ with multiplier $\\lambda \\geq 0$ is\n$$\nL(x,\\lambda) \\;=\\; c^\\top x + \\lambda \\big(b - a^\\top x\\big),\n$$\nand the dual function is\n$$\ng(\\lambda) \\;=\\; \\inf_{x \\in [0,1]^3} L(x,\\lambda) \\;=\\; \\lambda b + \\inf_{x \\in [0,1]^3} \\sum_{i=1}^3 \\big(c_i - \\lambda a_i\\big) x_i.\n$$\nBecause the domain $[0,1]^3$ is a Cartesian product of intervals and the objective separates over components, the infimum splits into three independent scalar problems:\n$$\n\\inf_{0 \\leq x_i \\leq 1} \\big(c_i - \\lambda a_i\\big) x_i, \\quad i=1,2,3.\n$$\nFor each $i$, the expression is linear in $x_i$; the minimum over $[0,1]$ is achieved at $x_i=0$ when $c_i - \\lambda a_i \\geq 0$, and at $x_i=1$ when $c_i - \\lambda a_i  0$. Consequently,\n$$\ng(\\lambda) \\;=\\; \\lambda b + \\sum_{i=1}^3 \\min\\{\\,0,\\; c_i - \\lambda a_i \\,\\}.\n$$\nCompute the thresholds $\\lambda_i := \\frac{c_i}{a_i}$ at which $c_i - \\lambda a_i$ changes sign:\n$$\n\\lambda_1 \\;=\\; \\frac{1}{2} \\;=\\; 0.5, \\quad\n\\lambda_2 \\;=\\; \\frac{3}{3} \\;=\\; 1.0, \\quad\n\\lambda_3 \\;=\\; \\frac{0.4}{1} \\;=\\; 0.4.\n$$\nWe analyze $g(\\lambda)$ piecewise between these breakpoints.\n\n1) For $\\lambda \\in [0,\\,0.4)$: all $c_i - \\lambda a_i \\geq 0$, so each $\\min\\{0, c_i - \\lambda a_i\\} = 0$. Hence\n$$\ng(\\lambda) \\;=\\; \\lambda b \\;=\\; 3\\lambda, \\quad \\lambda \\in [0,\\,0.4).\n$$\n\n2) For $\\lambda \\in (0.4,\\,0.5)$: $c_3 - \\lambda a_3  0$ while $c_1 - \\lambda a_1 \\geq 0$, $c_2 - \\lambda a_2 \\geq 0$, thus\n$$\ng(\\lambda) \\;=\\; 3\\lambda + (0.4 - \\lambda) \\;=\\; 0.4 + 2\\lambda, \\quad \\lambda \\in (0.4,\\,0.5).\n$$\n\n3) For $\\lambda \\in (0.5,\\,1.0)$: $c_1 - \\lambda a_1  0$ and $c_3 - \\lambda a_3  0$, $c_2 - \\lambda a_2 \\geq 0$, thus\n$$\ng(\\lambda) \\;=\\; 3\\lambda + (1 - 2\\lambda) + (0.4 - \\lambda) \\;=\\; 1.4 + 0\\cdot \\lambda \\;=\\; 1.4, \\quad \\lambda \\in (0.5,\\,1.0).\n$$\nTherefore $g(\\lambda)$ is flat (constant) on the open interval $(0.5,\\,1.0)$.\n\n4) For $\\lambda \\in (1.0,\\,\\infty)$: all $c_i - \\lambda a_i  0$, hence\n$$\ng(\\lambda) \\;=\\; 3\\lambda + (1 - 2\\lambda) + (3 - 3\\lambda) + (0.4 - \\lambda)\n\\;=\\; 4.4 - 3\\lambda, \\quad \\lambda  1.0.\n$$\n\nThe piecewise derivatives where $g$ is differentiable follow from the structure. In any interval with a fixed set $S(\\lambda)$ of indices $i$ where $c_i - \\lambda a_i  0$, the derivative is\n$$\ng'(\\lambda) \\;=\\; b - \\sum_{i \\in S(\\lambda)} a_i.\n$$\nCompute $g'(\\lambda)$ across intervals:\n- For $\\lambda \\in (0,\\,0.4)$, $S=\\emptyset$, so $g'(\\lambda) = 3$.\n- For $\\lambda \\in (0.4,\\,0.5)$, $S=\\{3\\}$, so $g'(\\lambda) = 3 - 1 = 2$.\n- For $\\lambda \\in (0.5,\\,1.0)$, $S=\\{1,3\\}$, so $g'(\\lambda) = 3 - (2+1) = 0$, confirming the flat segment.\n- For $\\lambda \\in (1.0,\\infty)$, $S=\\{1,2,3\\}$, so $g'(\\lambda) = 3 - (2+3+1) = -3$.\n\nSubgradients of the dual function. For a dual function arising from Lagrangian relaxation of an inequality constraint, a valid subgradient at $\\lambda$ is given by the constraint violation at a minimizer of the Lagrangian subproblem:\n$$\ns(\\lambda) \\in \\partial g(\\lambda) \\quad \\text{with} \\quad s(\\lambda) = b - a^\\top x^*(\\lambda),\n$$\nwhere $x^*(\\lambda) \\in \\arg\\min_{x \\in [0,1]^3} L(x,\\lambda)$. In differentiable regions, the subgradient set is the singleton $\\{g'(\\lambda)\\}$. At breakpoints, multiple minimizers may exist, and the subgradient set becomes the convex hull of the violations associated with these minimizers.\n\nOn the flat segment $(0.5,\\,1.0)$, the minimizer is unique and given by $x_1^*(\\lambda)=1$, $x_2^*(\\lambda)=0$, $x_3^*(\\lambda)=1$. The violation equals\n$$\nb - a^\\top x^*(\\lambda) \\;=\\; 3 - \\big(2\\cdot 1 + 3\\cdot 0 + 1\\cdot 1\\big) \\;=\\; 3 - 3 \\;=\\; 0,\n$$\nso $\\partial g(\\lambda) = \\{0\\}$ and any subgradient ascent step $\\lambda_{k+1} = \\lambda_k + \\alpha_k s(\\lambda_k)$ leaves $\\lambda$ unchanged because $s(\\lambda_k)=0$.\n\nAt the breakpoint $\\lambda = 0.5$, there is a tie for $x_1$ because $c_1 - \\lambda a_1 = 1 - 2\\cdot 0.5 = 0$; $x_1$ can be any value in $[0,1]$. The other components are determined by signs: $x_2=0$ (since $c_2 - 1.5  0$) and $x_3=1$ (since $0.4 - 0.5  0$). Hence the violation can be either\n$$\n3 - (2\\cdot 1 + 3\\cdot 0 + 1\\cdot 1) \\;=\\; 0 \\quad \\text{if } x_1=1,\n$$\nor\n$$\n3 - (2\\cdot 0 + 3\\cdot 0 + 1\\cdot 1) \\;=\\; 2 \\quad \\text{if } x_1=0,\n$$\nand by convexity of the subdifferential, $\\partial g(0.5) = [0,2]$. Thus, at the breakpoint, nonzero subgradients exist.\n\nImplications for variable-metric methods and escaping plateaus. A pure subgradient ascent step scales the current subgradient by any metric or step-size; if the current subgradient is exactly zero, any fixed linear scaling produces a zero step. Therefore, diagonal rescaling of a zero subgradient alone cannot move off the plateau. In contrast, modifying the model (e.g., augmented Lagrangian with a quadratic penalty) yields a different, typically smooth dual function with gradients that vary with $\\lambda$, creating curvature that quasi-Newton variable-metric methods such as limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) can exploit to move and escape flat segments.\n\nOption-by-option analysis:\n- Option A: Correct. We showed $g(\\lambda)$ is constant on $(0.5,\\,1.0)$ and $\\partial g(\\lambda) = \\{0\\}$ there. Any subgradient ascent update $\\lambda_{k+1} = \\lambda_k + \\alpha_k \\cdot 0$ will not move, regardless of $\\alpha_k$.\n- Option B: Incorrect. At $\\lambda = 0.5$, the subgradient set is $\\partial g(0.5) = [0,2]$, so there exist nonzero subgradients; it is not true that any choice is $0$.\n- Option C: Correct. An augmented Lagrangian adds a term like $\\tfrac{\\rho}{2}\\big(b - a^\\top x\\big)^2$, yielding a modified dual function whose gradient equals a penalized violation that varies smoothly with $\\lambda$. This breaks flatness and provides curvature, enabling variable-metric methods such as limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) to make progress off the plateau.\n- Option D: Incorrect. If the current subgradient is exactly zero, multiplying by any positive-definite scaling matrix still yields the zero step. Without smoothing or stabilization, diagonal scaling alone cannot guarantee escape from a flat region.", "answer": "$$\\boxed{AC}$$", "id": "3141528"}]}