{"hands_on_practices": [{"introduction": "To begin, let's ground our understanding with a direct application of the subgradient method. In this exercise [@problem_id:2207185], we explore a hypothetical scenario in robotics where an agent navigates by minimizing the Manhattan distance ($L_1$ norm) to a target, a common non-smooth cost function. By manually calculating the first two steps of the algorithm, you will gain a concrete feel for the iterative update process and see how a diminishing step size guides the search for the minimum.", "problem": "A robotic agent is being trained to navigate to a target coordinate in a 2D plane. The cost function to be minimized is the Manhattan distance (also known as the L1 distance) to the target, defined as $C(\\mathbf{x}) = \\|\\mathbf{x} - \\mathbf{p}\\|_1$, where $\\mathbf{x}=(x,y)$ is the agent's current position and $\\mathbf{p}=(p_x, p_y)$ is the target position. For a vector $\\mathbf{v}=(v_x, v_y)$, the L1 norm is $\\|\\mathbf{v}\\|_1 = |v_x| + |v_y|$.\n\nThe agent uses the subgradient method to update its position. The update rule for the $k$-th iteration is given by $\\mathbf{x}_{k} = \\mathbf{x}_{k-1} - \\alpha_k \\mathbf{g}_{k-1}$, where $\\mathbf{g}_{k-1}$ is any vector from the subdifferential of the cost function $C$ evaluated at position $\\mathbf{x}_{k-1}$.\n\nThe agent starts at an initial position $\\mathbf{x}_0 = (9.5, -2.5)$ and the target is at $\\mathbf{p} = (1.5, 4.5)$. The step size for the $k$-th iteration (for $k=1, 2, \\dots$) is given by the rule $\\alpha_k = \\frac{2}{k}$.\n\nCalculate the agent's position $\\mathbf{x}_2 = (x_2, y_2)$ after two full iterations.", "solution": "We minimize $C(\\mathbf{x})=\\|\\mathbf{x}-\\mathbf{p}\\|_{1}=|x-p_{x}|+|y-p_{y}|$. For $f(t)=|t|$, the subdifferential is $\\partial f(t)=\\{1\\}$ if $t0$, $\\{-1\\}$ if $t0$, and $[-1,1]$ if $t=0$. Therefore, a subgradient of $C$ at $\\mathbf{x}$ is any $\\mathbf{g}=(g_{x},g_{y})$ with $g_{x}\\in\\partial|x-p_{x}|$ and $g_{y}\\in\\partial|y-p_{y}|$.\n\nThe update rule is $\\mathbf{x}_{k}=\\mathbf{x}_{k-1}-\\alpha_{k}\\mathbf{g}_{k-1}$ with $\\alpha_{k}=\\frac{2}{k}$.\n\nIteration $k=1$: With $\\mathbf{x}_{0}=(9.5,-2.5)$ and $\\mathbf{p}=(1.5,4.5)$, we have\n$$\nx_{0}-p_{x}=9.5-1.5=80,\\qquad y_{0}-p_{y}=-2.5-4.5=-70.\n$$\nThus we can choose\n$$\n\\mathbf{g}_{0}=(1,-1).\n$$\nSince $\\alpha_{1}=\\frac{2}{1}=2$, the update is\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{1}\\mathbf{g}_{0}\n=(9.5,-2.5)-2(1,-1)=(9.5-2,\\,-2.5+2)=(7.5,-0.5).\n$$\n\nIteration $k=2$: Now with $\\mathbf{x}_{1}=(7.5,-0.5)$,\n$$\nx_{1}-p_{x}=7.5-1.5=60,\\qquad y_{1}-p_{y}=-0.5-4.5=-50,\n$$\nso we may take again\n$$\n\\mathbf{g}_{1}=(1,-1).\n$$\nWith $\\alpha_{2}=\\frac{2}{2}=1$, the update is\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{2}\\mathbf{g}_{1}\n=(7.5,-0.5)-1(1,-1)=(7.5-1,\\,-0.5+1)=(6.5,0.5).\n$$\n\nTherefore, after two iterations, the agent’s position is $\\mathbf{x}_{2}=(6.5,0.5)$.", "answer": "$$\\boxed{\\begin{pmatrix}6.5  0.5\\end{pmatrix}}$$", "id": "2207185"}, {"introduction": "Having practiced the mechanics of the subgradient update, we now turn to a more subtle and critical aspect of the algorithm's behavior. A common misconception is that the magnitude of the subgradient, $\\|\\mathbf{g}_k\\|$, should approach zero as the iterates $\\mathbf{x}_k$ converge to the solution, similar to gradient descent. This exercise [@problem_id:2207141] demonstrates that this is not the case for subgradient methods and challenges you to quantify this effect, revealing why $\\|\\mathbf{g}_k\\|$ is an unreliable stopping criterion.", "problem": "A guidance system for an autonomous drone is designed to navigate it to a target location, which we define as the origin $(0,0)$ of a 2D Cartesian coordinate system. The energy cost associated with being at a position $\\mathbf{x} = (x_1, x_2)$ is modeled by a nonsmooth, convex function $f(\\mathbf{x})$. Due to the drone's hardware, movement along the $x_1$-axis is more energy-efficient than movement along the $x_2$-axis. This anisotropy is captured by the cost function $f(x_1, x_2) = 1.6 |x_1| + 3.1 |x_2|$.\n\nThe drone uses the subgradient method to iteratively find a path toward the minimum cost location. The position update rule is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k$, where $\\mathbf{x}_k$ is the drone's position at step $k$, $\\alpha_k  0$ is a diminishing step size, and $\\mathbf{g}_k$ is a subgradient of the cost function $f$ at $\\mathbf{x}_k$.\n\nA key aspect of monitoring the convergence of such an algorithm is to track the magnitude of the step direction, given by the subgradient $\\mathbf{g}_k$. A known characteristic of the subgradient method is that the Euclidean norm of the subgradient, $\\|\\mathbf{g}_k\\|_2$, does not necessarily approach zero as the iterates $\\mathbf{x}_k$ approach the optimal solution $\\mathbf{x}^* = (0,0)$.\n\nTo characterize this behavior for the given cost function, determine the greatest lower bound for the Euclidean norm that a subgradient $\\mathbf{g}_k$ can have at any non-optimal point $\\mathbf{x}_k \\neq \\mathbf{x}^*$. This value represents a threshold below which the subgradient norm will never fall, as long as the drone has not reached its final destination.\n\nReport your final answer as a numerical value rounded to two significant figures.", "solution": "We are given the convex, separable, nonsmooth function\n$$\nf(x_{1},x_{2})=1.6\\,|x_{1}|+3.1\\,|x_{2}|.\n$$\nFor the absolute value function, the subdifferential is\n$$\n\\partial|x|=\\begin{cases}\n\\{1\\},  x0,\\\\\n[-1,1],  x=0,\\\\\n\\{-1\\},  x0.\n\\end{cases}\n$$\nTherefore, the subdifferential of $f$ at $\\mathbf{x}=(x_{1},x_{2})$ is\n$$\n\\partial f(\\mathbf{x})=\\left\\{(1.6\\,s_{1},\\,3.1\\,s_{2}) \\;:\\; s_{1}\\in S(x_{1}),\\; s_{2}\\in S(x_{2})\\right\\},\n$$\nwhere $S(x)$ is as defined above.\n\nFor any non-optimal point $\\mathbf{x}\\neq\\mathbf{0}$, the set $\\partial f(\\mathbf{x})$ depends on whether each coordinate is zero or not. Define the minimal attainable subgradient norm at $\\mathbf{x}$ by\n$$\nm(\\mathbf{x})=\\inf_{\\mathbf{g}\\in\\partial f(\\mathbf{x})}\\|\\mathbf{g}\\|_{2}.\n$$\nWe evaluate $m(\\mathbf{x})$ by cases:\n\n1) If $x_{1}\\neq 0$ and $x_{2}\\neq 0$, then $s_{1}\\in\\{\\pm 1\\}$ and $s_{2}\\in\\{\\pm 1\\}$, so any subgradient has the form $(\\pm 1.6,\\pm 3.1)$. Hence\n$$\nm(\\mathbf{x})=\\sqrt{(1.6)^{2}+(3.1)^{2}}.\n$$\n\n2) If $x_{1}\\neq 0$ and $x_{2}=0$, then $s_{1}\\in\\{\\pm 1\\}$ and $s_{2}\\in[-1,1]$, so subgradients are of the form $(\\pm 1.6,\\,3.1\\,s_{2})$. Minimizing the Euclidean norm over $s_{2}\\in[-1,1]$,\n$$\n\\|\\mathbf{g}\\|_{2}^{2}=(1.6)^{2}+(3.1\\,s_{2})^{2}\n$$\nis minimized at $s_{2}=0$, giving\n$$\nm(\\mathbf{x})=1.6.\n$$\n\n3) If $x_{1}=0$ and $x_{2}\\neq 0$, then $s_{1}\\in[-1,1]$ and $s_{2}\\in\\{\\pm 1\\}$, so subgradients are $(1.6\\,s_{1},\\,\\pm 3.1)$. Minimizing over $s_{1}$ yields\n$$\nm(\\mathbf{x})=3.1.\n$$\n\nThus, over all non-optimal points, the smallest attainable subgradient norm is\n$$\n\\inf_{\\mathbf{x}\\neq\\mathbf{0}} m(\\mathbf{x})=\\min\\left\\{1.6,\\,3.1,\\,\\sqrt{(1.6)^{2}+(3.1)^{2}}\\right\\}=1.6.\n$$\nThis lower bound is attained at any point with $x_{1}\\neq 0$ and $x_{2}=0$ by choosing a subgradient with second component equal to zero. Therefore, the greatest lower bound for the Euclidean norm of any subgradient at any non-optimal point is $1.6$. Rounded to two significant figures, it is $1.6$.", "answer": "$$\\boxed{1.6}$$", "id": "2207141"}, {"introduction": "This final practice transitions from pen-and-paper analysis to computational implementation, a crucial step in applying optimization methods to real-world problems. You are tasked with writing a program [@problem_id:3188881] to compare the convergence behavior of several key step-size schedules on a simple but illustrative function. This hands-on coding exercise will provide empirical evidence of the theoretical properties of these rules and highlight their practical strengths and weaknesses.", "problem": "You are asked to implement and compare the convergence behaviors of the subgradient method on the convex function $f(x) = |x|$ under three different step-size schedules. Your comparison must be performed by a complete, runnable program. The foundation should be built from the following core concepts:\n\n1. A function $f:\\mathbb{R}\\to\\mathbb{R}$ is convex if for all $x,y\\in\\mathbb{R}$ and all $\\theta\\in[0,1]$, the inequality $f(\\theta x+(1-\\theta)y)\\leq \\theta f(x)+(1-\\theta)f(y)$ holds. The function $f(x)=|x|$ is convex and Lipschitz continuous with Lipschitz constant $L=1$.\n\n2. A subgradient $g\\in\\partial f(x)$ at a point $x$ satisfies $f(y)\\ge f(x)+g\\cdot(y-x)$ for all $y\\in\\mathbb{R}$. For $f(x)=|x|$, the subdifferential is $\\partial f(x)=\\{\\operatorname{sign}(x)\\}$ for $x\\neq 0$, and $\\partial f(0) = [-1,1]$. Use the deterministic choice $g_k=\\operatorname{sign}(x_k)$ with the convention $\\operatorname{sign}(0)=0$.\n\n3. The subgradient method iterates according to the update $x_{k+1} = x_k - \\alpha_k g_k$, where $\\alpha_k$ is the step size at iteration $k$.\n\nYour program must implement three distinct step-size schedules:\n- Diminishing schedule $\\alpha_k = \\dfrac{c}{\\sqrt{k}}$ for $k\\ge 1$.\n- Harmonic schedule $\\alpha_k = \\dfrac{c}{k}$ for $k\\ge 1$.\n- Polyak’s step-size rule $\\alpha_k = \\dfrac{f(x_k)-f^\\star}{\\lVert g_k\\rVert^2}$ assuming a known lower bound $f^\\star$ on the optimal value. In the scalar case, $\\lVert g_k\\rVert^2 = g_k^2$, which equals $1$ whenever $x_k\\neq 0$.\n\nAdopt the following stopping criterion: declare convergence at the smallest iteration index $k$ such that $|x_k|\\le \\varepsilon$. If the initial point $x_0$ already satisfies $|x_0|\\le \\varepsilon$, the iteration count is $0$. If convergence is not achieved within a specified maximum number of iterations $N_{\\max}$, return the integer $-1$.\n\nScientific consistency requirements:\n- If $x_k=0$, then $g_k=0$ and $f(x_k)=0$; treat this as convergence. For Polyak’s rule, avoid division by zero by checking for convergence before computing the step size.\n- For Polyak’s rule, use the supplied $f^\\star$ as a lower bound. If $f^\\star$ is strictly below the true optimal value for $f(x)=|x|$ (which is $0$), the step-size rule may not converge; your program must robustly report non-convergence by returning $-1$ in such cases when the stopping criterion is not met.\n\nImplement the subgradient method for each schedule and perform the comparison using the test suite below. For each test case, output the number of iterations required to meet the stopping criterion for each schedule, in the fixed order $\\left[\\dfrac{c}{\\sqrt{k}}, \\dfrac{c}{k}, \\text{Polyak}\\right]$.\n\nTest suite (each test case is a tuple $(x_0,c,f^\\star,\\varepsilon,N_{\\max})$):\n- Case $1$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (10,5,0,10^{-2},10^4)$.\n- Case $2$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (0,1,0,10^{-12},10^2)$.\n- Case $3$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (-5,1,0,10^{-2},5\\cdot 10^4)$.\n- Case $4$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (2,1,-1,10^{-3},10^4)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. For example, the expected format is $[\\,[r_{1,1},r_{1,2},r_{1,3}],\\,[r_{2,1},r_{2,2},r_{2,3}],\\,[r_{3,1},r_{3,2},r_{3,3}],\\,[r_{4,1},r_{4,2},r_{4,3}]\\,]$, where each $r_{i,j}$ is an integer as defined above.", "solution": "The user has requested an implementation and comparison of three subgradient method step-size schedules for minimizing the convex function $f(x) = |x|$. The task involves validating the problem statement, providing a reasoned solution if valid, and producing a complete, runnable program as the final answer.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Function to Minimize**: $f(x) = |x|$, a convex and Lipschitz continuous function with Lipschitz constant $L=1$.\n- **Domain**: $f:\\mathbb{R}\\to\\mathbb{R}$.\n- **Subgradient Definition**: A value $g$ such that for a given $x$, $f(y)\\ge f(x)+g\\cdot(y-x)$ for all $y\\in\\mathbb{R}$.\n- **Subdifferential of $f(x)=|x|$**: $\\partial f(x)=\\{\\operatorname{sign}(x)\\}$ for $x\\neq 0$, and $\\partial f(0) = [-1,1]$.\n- **Deterministic Subgradient Choice**: $g_k=\\operatorname{sign}(x_k)$ with the convention $\\operatorname{sign}(0)=0$.\n- **Subgradient Method Update Rule**: $x_{k+1} = x_k - \\alpha_k g_k$, where $\\alpha_k$ is the step size.\n- **Step-Size Schedules**:\n    1.  **Diminishing**: $\\alpha_k = \\dfrac{c}{\\sqrt{k}}$ for $k\\ge 1$.\n    2.  **Harmonic**: $\\alpha_k = \\dfrac{c}{k}$ for $k\\ge 1$.\n    3.  **Polyak**: $\\alpha_k = \\dfrac{f(x_k)-f^\\star}{\\lVert g_k\\rVert^2}$, where $f^\\star$ is a known lower bound on the optimal value, and for the scalar case, $\\lVert g_k\\rVert^2 = g_k^2$.\n- **Stopping Criterion**: Terminate at the smallest iteration index $k$ where $|x_k|\\le \\varepsilon$.\n- **Special Conditions**:\n    - If $|x_0|\\le \\varepsilon$, the iteration count is $0$.\n    - If convergence is not achieved within $N_{\\max}$ iterations, the result is $-1$.\n    - If $x_k=0$, it is treated as convergence. For Polyak's rule, this requires checking for convergence before computing the step size to prevent division by zero.\n- **Test Suite**:\n    - Case $1$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (10,5,0,10^{-2},10^4)$.\n    - Case $2$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (0,1,0,10^{-12},10^2)$.\n    - Case $3$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (-5,1,0,10^{-2},5\\cdot 10^4)$.\n    - Case $4$: $(x_0,c,f^\\star,\\varepsilon,N_{\\max}) = (2,1,-1,10^{-3},10^4)$.\n- **Output Format**: A comma-separated list of lists of integers, e.g., $[\\,[r_{1,1},r_{1,2},r_{1,3}],\\,[r_{2,1},r_{2,2},r_{2,3}],\\dots]$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is an exemplary application of the subgradient method, a fundamental algorithm in convex optimization. The function $f(x)=|x|$ is a standard non-differentiable convex function used for analysis. The definitions of convexity, subgradient, and the subgradient update rule are all standard and correct. The step-size schedules (diminishing, harmonic, and Polyak) are well-established in optimization literature. The discussion of Polyak's rule, including the role of the lower bound $f^\\star$ and the potential for non-convergence if $f^\\star  f_\\text{true}^\\star$, is scientifically accurate.\n-   **Well-Posed**: The problem is well-posed. It specifies the function, the algorithm, all necessary parameters, initial conditions, and a precise stopping criterion. For each test case, a unique sequence of iterates is generated, leading to a determinable number of iterations until convergence or until the maximum limit is reached.\n-   **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n-   **Completeness and Consistency**: The problem provides all necessary information (initial points, parameters, stopping criteria) and does not contain contradictory constraints.\n-   **Realism and Feasibility**: The problem is a numerical experiment. The parameters and conditions are mathematically consistent and computationally feasible.\n\nThe problem statement does not exhibit any flaws such as scientific unsoundness, missing information, or ambiguity.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be developed and implemented as requested.\n\n### Solution Design\n\nThe core of the solution is a function that implements the subgradient method for a given step-size schedule. This function will be executed for each of the three schedules on each of the four test cases.\n\nLet's define a function `run_schedule(schedule_type, x_0, c, f_star, epsilon, n_max)` that encapsulates the iterative process.\nThe inputs are the schedule type (`'diminishing'`, `'harmonic'`, or `'polyak'`), the initial point $x_0$, the step-size constant $c$, the optimal value lower bound $f^\\star$, the tolerance $\\varepsilon$, and the maximum number of iterations $N_{\\max}$.\n\nThe algorithm proceeds as follows:\n1.  Initialize the current point $x$ with the value of $x_0$.\n2.  Check the initial condition: if $|x| \\le \\varepsilon$, the algorithm has already converged. The iteration count is $0$, and the function returns $0$.\n3.  Begin a loop for the iteration count $k$ from $1$ to $N_{\\max}$.\n4.  At the beginning of each iteration $k$, the current value of $x$ corresponds to $x_{k-1}$.\n5.  Determine the subgradient $g = \\operatorname{sign}(x)$. The problem specifies $\\operatorname{sign}(0)=0$. This choice ensures $g \\in \\partial f(x)$. If $x \\neq 0$, then $g$ is either $1$ or $-1$. If $x=0$, then $g=0$. The algorithm should have terminated in the previous step if $x$ became small enough to satisfy the tolerance, so $x \\neq 0$ at the start of an iteration is expected.\n6.  Calculate the step size $\\alpha_k$ based on the `schedule_type`:\n    -   For `diminishing`: $\\alpha_k = c / \\sqrt{k}$.\n    -   For `harmonic`: $\\alpha_k = c / k$.\n    -   For `polyak`: The step size is $\\alpha_k = (f(x_k)-f^\\star) / \\lVert g_k \\rVert^2$. For our problem, $f(x) = |x|$ and $g = \\operatorname{sign}(x)$. For any non-zero $x$, $\\lVert g \\rVert^2 = (\\pm 1)^2 = 1$. Thus, the step size simplifies to $\\alpha_k = |x| - f^\\star$. Division by zero is avoided as this branch is only reached for $x\\neq 0$.\n7.  Update the iterate using the subgradient descent rule: $x_{k} = x_{k-1} - \\alpha_k g_{k-1}$. In the implementation, this is $x \\leftarrow x - \\alpha_k g$.\n8.  Check for convergence: if the new value satisfies $|x| \\le \\varepsilon$, the algorithm has converged. The function returns the current iteration count, $k$.\n9.  If the loop completes without the stopping criterion being met (i.e., $k$ reaches $N_{\\max}$), it indicates non-convergence within the given limit. The function returns $-1$.\n\nThis process will be repeated for all combinations of test cases and schedules. Specifically, for Polyak's rule, Case $4$ uses $f^\\star=-1$, which is strictly less than the true minimum $f_\\text{true}^\\star=0$. This will cause the iterates to oscillate between $1$ and $-1$, never converging to the solution set around $0$. The implementation must correctly handle this by reaching $N_{\\max}$ and returning $-1$. For Cases $1$ and $3$, where $f^\\star=0$, Polyak's method is expected to converge in a single iteration, as the update rule for $x_0 \\neq 0$ becomes $x_1 = x_0 - (|x_0|-0)\\operatorname{sign}(x_0) = x_0 - x_0 = 0$.\n\nThe final output will be constructed by collecting the integer results (number of iterations or $-1$) for each test case into a list of lists and formatting it into the specified string format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares the subgradient method for f(x)=|x| with three\n    different step-size schedules.\n    \"\"\"\n\n    def run_schedule(schedule_type, x0, c, f_star, epsilon, n_max):\n        \"\"\"\n        Runs the subgradient method for a single schedule.\n\n        Args:\n            schedule_type (str): 'diminishing', 'harmonic', or 'polyak'.\n            x0 (float): Initial point.\n            c (float): Constant for step-size schedules.\n            f_star (float): Lower bound on the optimal value for Polyak's rule.\n            epsilon (float): Convergence tolerance.\n            n_max (int): Maximum number of iterations.\n\n        Returns:\n            int: The number of iterations to converge, or -1 if not converged.\n        \"\"\"\n        x = float(x0)\n\n        # Check for convergence at the initial point\n        if abs(x) = epsilon:\n            return 0\n\n        for k in range(1, n_max + 1):\n            # The value of x at the start of iteration k is x_{k-1}\n            f_x = abs(x)\n            \n            # This state is not expected to be reached due to the check after the update.\n            # If x becomes exactly 0, it would have been caught post-update.\n            # This is a safeguard, especially for Polyak's rule.\n            if f_x == 0.0:\n                 return k - 1\n            \n            g = np.sign(x)\n\n            # Calculate step size alpha_k\n            if schedule_type == 'diminishing':\n                alpha = c / np.sqrt(k)\n            elif schedule_type == 'harmonic':\n                alpha = c / k\n            elif schedule_type == 'polyak':\n                # For x != 0, g is +/- 1, so g_norm_sq is 1.\n                g_norm_sq = 1.0\n                alpha = (f_x - f_star) / g_norm_sq\n            else:\n                # This case should not be reached with the current problem setup\n                raise ValueError(\"Unknown schedule type\")\n\n            # Update x to get x_k\n            x = x - alpha * g\n\n            # Check for convergence\n            if abs(x) = epsilon:\n                return k\n\n        # If the loop finishes, convergence was not achieved\n        return -1\n\n    test_cases = [\n        # (x0, c, f_star, epsilon, n_max)\n        (10, 5, 0, 1e-2, 10000),\n        (0, 1, 0, 1e-12, 100),\n        (-5, 1, 0, 1e-2, 50000),\n        (2, 1, -1, 1e-3, 10000),\n    ]\n\n    all_results = []\n    schedules = ['diminishing', 'harmonic', 'polyak']\n\n    for params in test_cases:\n        x0, c, f_star, epsilon, n_max = params\n        case_results = []\n        for schedule in schedules:\n            result = run_schedule(schedule, x0, c, f_star, epsilon, n_max)\n            case_results.append(result)\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    # e.g., [[-1,521,1],[0,0,0],...] - \"[[-1,521,1],[0,0,0],...]\"\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3188881"}]}