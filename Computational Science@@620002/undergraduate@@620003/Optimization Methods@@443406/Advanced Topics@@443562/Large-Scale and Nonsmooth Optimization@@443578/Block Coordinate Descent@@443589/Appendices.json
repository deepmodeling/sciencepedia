{"hands_on_practices": [{"introduction": "To truly appreciate the power of Block Coordinate Descent (BCD), we begin by analyzing its core efficiency. This first practice invites you to look under the hood of the simplest BCD variant, Coordinate Descent (CD), where blocks are of size one. By deriving the computational cost of a single coordinate update for a least-squares problem, you will discover the fundamental reason why these methods are so effective in high-dimensional settings [@problem_id:3103367].", "problem": "Consider minimizing the least-squares objective function $f(x) = \\frac{1}{2}\\|A x - b\\|^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sparse matrix with columns $\\{a_{i}\\}_{i=1}^{n}$ and $b \\in \\mathbb{R}^{m}$. Let $\\text{nnz}(A)$ denote the total number of nonzero entries of $A$ and let $\\text{nnz}_{i}$ denote the number of nonzero entries in column $a_{i}$. Assume the Block Coordinate Descent (BCD) method is applied with blocks of size one (also known as Coordinate Descent (CD)), maintaining the residual $r = A x - b$ at all times. In each iteration, a coordinate $i \\in \\{1,\\dots,n\\}$ is selected uniformly at random, the scalar $\\alpha_{i}$ proportional to $a_{i}^{\\top} r$ is computed, and both $x_{i}$ and $r$ are updated using the column $a_{i}$. Assume the per-iteration arithmetic cost is measured by the number of multiply-add operations, and that interacting with a sparse vector of column structure $a_{i}$ costs proportionally to $\\text{nnz}_{i}$. Also assume that any quantities like $\\|a_{i}\\|^{2}$ are precomputed, so they do not contribute to the per-iteration cost.\n\nStarting from fundamental definitions of $f(x)$ and the arithmetic structure of the operations described, derive the expected per-iteration arithmetic cost of CD in terms of $\\{\\text{nnz}_{i}\\}_{i=1}^{n}$, and compare it to the cost of computing the full gradient $\\nabla f(x) = A^{\\top}(A x - b)$. Express this comparison as the ratio of the expected CD per-iteration cost to the full gradient cost, simplified to a single closed-form expression in terms of $n$ only. No rounding is required, and no physical units apply. Provide the ratio as the final answer.", "solution": "The problem requires the derivation and comparison of computational costs for two procedures related to the minimization of the least-squares objective function $f(x) = \\frac{1}{2}\\|A x - b\\|^{2}$. We begin by analyzing the computational cost of a single iteration of the Coordinate Descent (CD) method and then the cost of a full gradient computation.\n\nFirst, let us analyze the operations within a single iteration of Coordinate Descent. The method involves selecting a coordinate $i \\in \\{1, \\dots, n\\}$ and minimizing $f(x)$ with respect to the variable $x_i$, while holding all other coordinates $x_j$ (for $j \\neq i$) fixed. Let the current iterate be $x$ and the corresponding residual be $r = A x - b$. The update to $x_i$ can be written as $x_{i, \\text{new}} = x_i + \\delta$. The objective function becomes a function of the scalar step $\\delta$:\n$$h(\\delta) = f(x + \\delta e_i) = \\frac{1}{2}\\|A(x + \\delta e_i) - b\\|^2 = \\frac{1}{2}\\|(Ax - b) + \\delta A e_i\\|^2$$\nwhere $e_i$ is the $i$-th standard basis vector. Since $A e_i$ is the $i$-th column of $A$, denoted $a_i$, and $Ax-b=r$, we have:\n$$h(\\delta) = \\frac{1}{2}\\|r + \\delta a_i\\|^2 = \\frac{1}{2}(r^\\top r + 2\\delta a_i^\\top r + \\delta^2 a_i^\\top a_i) = \\frac{1}{2}(\\|r\\|^2 + 2\\delta a_i^\\top r + \\delta^2 \\|a_i\\|^2)$$\nTo find the optimal step $\\delta^*$, we set the derivative of $h(\\delta)$ with respect to $\\delta$ to zero:\n$$\\frac{dh}{d\\delta} = a_i^\\top r + \\delta \\|a_i\\|^2 = 0$$\nThe quantity $a_i^\\top r$ is the $i$-th component of the gradient, $(\\nabla f(x))_i$. The optimal step is thus:\n$$\\delta^* = -\\frac{a_i^\\top r}{\\|a_i\\|^2}$$\nThe CD iteration consists of computing $\\delta^*$ and then updating $x_i$ and the residual $r$. The problem states that the residual $r$ is maintained at all times. The updates are:\n1. $x_i \\leftarrow x_i + \\delta^*$\n2. $r \\leftarrow r + \\delta^* a_i$\n\nWe now evaluate the arithmetic cost of one such iteration for a given coordinate $i$, measured in multiply-add operations.\nThe cost is based on the assumption that operations involving the sparse column $a_i$ cost proportionally to its number of non-zero entries, $\\text{nnz}_i$. The quantities $\\|a_i\\|^2$ are precomputed.\n\n- **Cost of computing $\\delta^*$**: The main computation is the dot product $a_i^\\top r$. The vector $a_i$ is sparse with $\\text{nnz}_i$ non-zero elements, while $r \\in \\mathbb{R}^m$ is treated as dense. This dot product requires $\\text{nnz}_i$ multiplications and $\\text{nnz}_i - 1$ additions. This corresponds to approximately $\\text{nnz}_i$ multiply-add operations. The subsequent division by the precomputed scalar $\\|a_i\\|^2$ is a single operation, which is negligible compared to the dot product for typical values of $\\text{nnz}_i$. Thus, the cost of computing the numerator $a_i^\\top r$ is the dominant part, which we account as $\\text{nnz}_i$ operations.\n\n- **Cost of updating $x_i$ and $r$**: The update $x_i \\leftarrow x_i + \\delta^*$ is a single scalar addition, with negligible cost. The update $r \\leftarrow r + \\delta^* a_i$ is a sparse vector update (a scaled vector addition, or `axpy` operation). Since $a_i$ has $\\text{nnz}_i$ non-zero entries, this operation involves scaling these $\\text{nnz}_i$ entries by $\\delta^*$ and adding them to the corresponding components of $r$. This requires $\\text{nnz}_i$ multiplications and $\\text{nnz}_i$ additions, for a total of $\\text{nnz}_i$ multiply-add operations.\n\nThe total cost for a single CD iteration on coordinate $i$, denoted $C_{\\text{CD}, i}$, is the sum of these costs:\n$$C_{\\text{CD}, i} = (\\text{cost of } a_i^\\top r) + (\\text{cost of } r + \\delta^* a_i) = \\text{nnz}_i + \\text{nnz}_i = 2 \\cdot \\text{nnz}_i$$\n\nThe problem states that the coordinate $i$ is chosen uniformly at random from $\\{1, \\dots, n\\}$. The probability of selecting any specific $i$ is $P(i) = \\frac{1}{n}$. The expected per-iteration cost of CD, $E[C_{\\text{CD}}]$, is the average of $C_{\\text{CD}, i}$ over all possible choices of $i$:\n$$E[C_{\\text{CD}}] = \\sum_{i=1}^{n} P(i) \\cdot C_{\\text{CD}, i} = \\sum_{i=1}^{n} \\frac{1}{n} (2 \\cdot \\text{nnz}_i) = \\frac{2}{n} \\sum_{i=1}^{n} \\text{nnz}_i$$\nThe sum of non-zero entries over all columns, $\\sum_{i=1}^{n} \\text{nnz}_i$, is by definition the total number of non-zero entries in the matrix $A$, denoted $\\text{nnz}(A)$. Therefore,\n$$E[C_{\\text{CD}}] = \\frac{2 \\cdot \\text{nnz}(A)}{n}$$\n\nNext, we determine the cost of computing the full gradient, $\\nabla f(x) = A^{\\top}(A x - b) = A^{\\top}r$. The $i$-th component of the gradient is given by the dot product of the $i$-th row of $A^\\top$ with $r$. The $i$-th row of $A^\\top$ is the $i$-th column of $A$, which is $a_i$. Thus, $(\\nabla f(x))_i = a_i^\\top r$.\nTo compute the full gradient vector, we must compute this dot product for every coordinate $i=1, \\dots, n$. The cost of computing the $i$-th component is $\\text{nnz}_i$. The total cost to compute the full gradient, $C_{\\nabla f}$, is the sum of the costs for all components:\n$$C_{\\nabla f} = \\sum_{i=1}^{n} (\\text{cost of } a_i^\\top r) = \\sum_{i=1}^{n} \\text{nnz}_i = \\text{nnz}(A)$$\nThis is equivalent to performing the sparse matrix-vector product $A^\\top r$, which requires one multiply-add operation for each non-zero entry in $A^\\top$ (or $A$).\n\nFinally, we compute the ratio of the expected per-iteration CD cost to the full gradient cost.\n$$\\text{Ratio} = \\frac{E[C_{\\text{CD}}]}{C_{\\nabla f}} = \\frac{\\frac{2 \\cdot \\text{nnz}(A)}{n}}{\\text{nnz}(A)}$$\nAssuming the matrix $A$ is not the zero matrix, $\\text{nnz}(A) > 0$, and we can simplify the expression by canceling $\\text{nnz}(A)$:\n$$\\text{Ratio} = \\frac{2}{n}$$\nThis result shows that the expected cost of one CD iteration is a factor of $\\frac{n}{2}$ cheaper than computing the full gradient, a key insight into the efficiency of coordinate methods on large-scale problems.", "answer": "$$\\boxed{\\frac{2}{n}}$$", "id": "3103367"}, {"introduction": "Having established the per-iteration efficiency of coordinate-wise methods, we now explore their versatility beyond simple quadratic objectives. This exercise challenges you to apply the BCD framework to the logistic regression model, a cornerstone of modern machine learning. You will derive a principled update rule by employing a one-dimensional Newton step, a technique that demonstrates how BCD can be adapted to tackle more complex, non-linear problems [@problem_id:3103337].", "problem": "Consider a dataset of $m$ labeled examples $\\{(a_{i},y_{i})\\}_{i=1}^{m}$ with $a_{i}\\in\\mathbb{R}^{n}$ and $y_{i}\\in\\{-1,1\\}$. Define the regularized logistic regression objective\n$$\nf(x)\\;=\\;\\sum_{i=1}^{m}\\ln\\!\\big(1+\\exp(-y_{i}\\,a_{i}^{\\top}x)\\big)\\;+\\;\\frac{\\lambda}{2}\\,\\|x\\|^{2},\n$$\nwhere $x\\in\\mathbb{R}^{n}$ and $\\lambda>0$ is a regularization parameter. In a Block Coordinate Descent (BCD) method, a common strategy is to update a single coordinate $x_{j}$ at a time while holding all other coordinates fixed. One principled way to construct such a one-dimensional update is to apply Newtonâ€™s method along coordinate $j$ using a diagonal approximation of the Hessian, which replaces the full Hessian by its diagonal entries.\n\nStarting from fundamental definitions of the gradient and Hessian of $f(x)$, and using only the chain rule and well-tested facts about the logistic function, derive an analytic expression for the coordinate-wise Newton update $x_{j}^{\\text{new}}$ (the updated value of $x_{j}$) in terms of the current $x$, the data $\\{a_{i},y_{i}\\}_{i=1}^{m}$, and $\\lambda$, when the Hessian is approximated by its diagonal. You may define the logistic function as $\\sigma(t)=\\frac{1}{1+\\exp(-t)}$ and use it in your derivation.\n\nYour final answer must be a single closed-form expression for $x_{j}^{\\text{new}}$. No rounding is required.", "solution": "The objective is to derive the update rule for a single coordinate $x_j$ in a Block Coordinate Descent (BCD) framework for the regularized logistic regression objective function:\n$$\nf(x) \\;=\\; \\sum_{i=1}^{m}\\ln\\!\\big(1+\\exp(-y_{i}\\,a_{i}^{\\top}x)\\big)\\;+\\;\\frac{\\lambda}{2}\\,\\|x\\|^{2}\n$$\nThe update rule is specified as a one-dimensional Newton step along the coordinate $x_j$. For a function of a single variable $g(t)$, a Newton step from $t_k$ to $t_{k+1}$ is given by $t_{k+1} = t_k - \\frac{g'(t_k)}{g''(t_k)}$. In our context, we are optimizing $f(x)$ with respect to a single coordinate $x_j$ while keeping all other coordinates $x_k$ (for $k \\neq j$) fixed. The update for $x_j$ is therefore:\n$$\nx_{j}^{\\text{new}} = x_j - \\frac{\\frac{\\partial f}{\\partial x_j}}{\\frac{\\partial^2 f}{\\partial x_j^2}}\n$$\nHere, $\\frac{\\partial f}{\\partial x_j}$ is the $j$-th component of the gradient of $f(x)$, denoted $(\\nabla f(x))_j$. The term $\\frac{\\partial^2 f}{\\partial x_j^2}$ is the $j$-th diagonal element of the Hessian matrix of $f(x)$, denoted $(\\nabla^2 f(x))_{jj}$. This corresponds exactly to the specified method of using the diagonal of the Hessian.\n\nOur derivation proceeds in two steps: first, we compute the partial derivative $(\\nabla f(x))_j$, and second, we compute the second-order partial derivative $(\\nabla^2 f(x))_{jj}$.\n\nThe problem provides the definition of the logistic sigmoid function, $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$. We will use this function and its properties.\n\n**Step 1: Compute the Gradient Component $(\\nabla f(x))_j$**\n\nThe objective function consists of two parts: the loss term $L(x) = \\sum_{i=1}^{m}\\ln(1+\\exp(-y_{i}\\,a_{i}^{\\top}x))$ and the regularization term $R(x) = \\frac{\\lambda}{2}\\,\\|x\\|^{2} = \\frac{\\lambda}{2} \\sum_{k=1}^n x_k^2$. The gradient is the sum of the gradients of these two parts.\n\nThe partial derivative of the regularization term with respect to $x_j$ is:\n$$\n\\frac{\\partial R(x)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left(\\frac{\\lambda}{2} \\sum_{k=1}^n x_k^2\\right) = \\frac{\\lambda}{2} (2x_j) = \\lambda x_j\n$$\n\nFor the loss term, we apply the chain rule. Let $t_i(x) = y_i a_i^\\top x$. Then the $i$-th term in the sum is $\\ln(1+\\exp(-t_i))$.\n$$\n\\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(-t_i)) = \\frac{1}{1+\\exp(-t_i)} \\cdot \\frac{\\partial}{\\partial x_j}(1+\\exp(-t_i))\n$$\n$$\n= \\frac{1}{1+\\exp(-t_i)} \\cdot \\exp(-t_i) \\cdot \\frac{\\partial(-t_i)}{\\partial x_j}\n$$\nThe derivative of the argument $-t_i(x)$ with respect to $x_j$ is:\n$$\n\\frac{\\partial(-t_i)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (-y_i a_i^\\top x) = \\frac{\\partial}{\\partial x_j} \\left(-y_i \\sum_{k=1}^n a_{ik}x_k\\right) = -y_i a_{ij}\n$$\nSubstituting this back, we get:\n$$\n\\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(-t_i)) = \\frac{\\exp(-t_i)}{1+\\exp(-t_i)} \\cdot (-y_i a_{ij}) = -\\frac{\\exp(-y_i a_i^\\top x)}{1+\\exp(-y_i a_i^\\top x)} y_i a_{ij}\n$$\nWe can express the fraction in terms of the sigmoid function $\\sigma(t)$. Notice that $\\sigma(t) - 1 = \\frac{1}{1+\\exp(-t)} - 1 = \\frac{1 - (1+\\exp(-t))}{1+\\exp(-t)} = -\\frac{\\exp(-t)}{1+\\exp(-t)}$.\nThus, the partial derivative of the $i$-th loss term is:\n$$\n\\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(-y_i a_i^\\top x)) = (\\sigma(y_i a_i^\\top x) - 1) y_i a_{ij}\n$$\nSumming over all data points $i=1, \\dots, m$ gives the partial derivative of the total loss $L(x)$:\n$$\n\\frac{\\partial L(x)}{\\partial x_j} = \\sum_{i=1}^m y_i a_{ij} (\\sigma(y_i a_i^\\top x) - 1)\n$$\nCombining the derivatives of the loss and regularization terms, we obtain the full gradient component:\n$$\n(\\nabla f(x))_j = \\frac{\\partial f(x)}{\\partial x_j} = \\lambda x_j + \\sum_{i=1}^{m} y_{i} a_{ij} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)\n$$\n\n**Step 2: Compute the Hessian Diagonal Element $(\\nabla^2 f(x))_{jj}$**\n\nTo find the diagonal element of the Hessian, we differentiate $(\\nabla f(x))_j$ with respect to $x_j$:\n$$\n(\\nabla^2 f(x))_{jj} = \\frac{\\partial^2 f(x)}{\\partial x_j^2} = \\frac{\\partial}{\\partial x_j} \\left(\\lambda x_j + \\sum_{i=1}^{m} y_{i} a_{ij} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)\\right)\n$$\n$$\n= \\lambda + \\sum_{i=1}^{m} y_{i} a_{ij} \\frac{\\partial}{\\partial x_j} \\sigma(y_{i} a_{i}^{\\top} x)\n$$\nWe need the derivative of the sigmoid function, $\\sigma'(t)$. Using the quotient rule or chain rule:\n$$\n\\sigma'(t) = \\frac{d}{dt} \\left( (1+\\exp(-t))^{-1} \\right) = -(1+\\exp(-t))^{-2} \\cdot (-\\exp(-t)) = \\frac{\\exp(-t)}{(1+\\exp(-t))^2}\n$$\nA well-known identity for $\\sigma'(t)$ is $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$:\n$$\n\\sigma(t)(1-\\sigma(t)) = \\frac{1}{1+\\exp(-t)} \\left(1 - \\frac{1}{1+\\exp(-t)}\\right) = \\frac{1}{1+\\exp(-t)} \\frac{\\exp(-t)}{1+\\exp(-t)} = \\frac{\\exp(-t)}{(1+\\exp(-t))^2} = \\sigma'(t)\n$$\nUsing this identity and the chain rule on $\\sigma(y_i a_i^\\top x)$:\n$$\n\\frac{\\partial}{\\partial x_j} \\sigma(y_{i} a_{i}^{\\top} x) = \\sigma'(y_i a_i^\\top x) \\cdot \\frac{\\partial}{\\partial x_j}(y_i a_i^\\top x) = \\sigma'(y_i a_i^\\top x) \\cdot (y_i a_{ij})\n$$\nSubstituting this into the expression for the second derivative:\n$$\n(\\nabla^2 f(x))_{jj} = \\lambda + \\sum_{i=1}^{m} y_{i} a_{ij} \\left( \\sigma'(y_i a_i^\\top x) \\cdot y_i a_{ij} \\right)\n$$\n$$\n= \\lambda + \\sum_{i=1}^{m} y_{i}^2 a_{ij}^2 \\sigma'(y_i a_i^\\top x)\n$$\nSince $y_i \\in \\{-1, 1\\}$, we have $y_i^2 = 1$. Using the identity for $\\sigma'(t)$:\n$$\n(\\nabla^2 f(x))_{jj} = \\lambda + \\sum_{i=1}^{m} a_{ij}^2 \\sigma(y_{i}a_{i}^{\\top}x)(1 - \\sigma(y_{i}a_{i}^{\\top}x))\n$$\n\n**Step 3: Assemble the Newton Update Rule**\n\nNow we substitute the expressions for the gradient component and the Hessian diagonal element into the Newton update formula:\n$$\nx_{j}^{\\text{new}} = x_j - \\frac{(\\nabla f(x))_j}{(\\nabla^2 f(x))_{jj}}\n$$\n$$\nx_{j}^{\\text{new}} = x_j - \\frac{\\lambda x_j + \\sum_{i=1}^{m} a_{ij} y_{i} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)}{\\lambda + \\sum_{i=1}^{m} a_{ij}^2 \\sigma(y_{i}a_{i}^{\\top}x)(1 - \\sigma(y_{i} a_{i}^{\\top} x))}\n$$\nThis is the final closed-form expression for the coordinate-wise Newton update.", "answer": "$$\n\\boxed{x_j - \\frac{\\lambda x_j + \\sum_{i=1}^{m} a_{ij} y_{i} (\\sigma(y_{i} a_{i}^{\\top} x) - 1)}{\\lambda + \\sum_{i=1}^{m} a_{ij}^2 \\sigma(y_{i}a_{i}^{\\top}x)(1 - \\sigma(y_{i} a_{i}^{\\top} x))}}\n$$", "id": "3103337"}, {"introduction": "Theory and derivation provide the foundation, but empirical evidence builds true intuition. In this final practice, you will transition from pen-and-paper analysis to hands-on implementation by comparing the convergence behavior of Coordinate Descent and Gradient Descent. By coding both algorithms and testing them on a carefully designed quadratic problem, you will gain practical insight into how each method performs under ill-conditioning, solidifying your understanding of their respective strengths and weaknesses [@problem_id:3103362].", "problem": "You are given a quadratic objective function in $n$ variables, $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$, where the symmetric matrix $Q \\in \\mathbb{R}^{n \\times n}$ is defined by $Q = \\operatorname{diag}(\\kappa, 1, \\dots, 1) + \\rho \\mathbf{1}\\mathbf{1}^\\top$, with $\\kappa > 0$, $\\rho \\ge 0$, and $\\mathbf{1} \\in \\mathbb{R}^n$ the vector of ones, and where $b = \\mathbf{1}$. Consider two iterative methods: Gradient Descent (GD) and Block Coordinate Descent (BCD). In Block Coordinate Descent (BCD), use cyclic blocks of size $1$ (that is, standard coordinate descent), performing exact minimization along one coordinate at a time. In Gradient Descent (GD), use a fixed step size equal to the inverse of the largest eigenvalue of $Q$.\n\nYour task is to write a complete, runnable program that, for each specified test case, constructs $Q$ and $b$, computes the exact minimizer $x^\\star$ of $f(x)$, and then compares the number of iterations needed by GD and by BCD to reach a prescribed accuracy, as the parameter $\\kappa$ grows. Use the following precise algorithmic specifications:\n\n- Initialization for both methods: start from $x^{(0)} = \\mathbf{0}$.\n- For Gradient Descent (GD): at each iteration $t$, compute the gradient $\\nabla f(x^{(t)}) = Q x^{(t)} - b$ and update $x^{(t+1)} = x^{(t)} - \\alpha \\nabla f(x^{(t)})$, where $\\alpha = 1 / \\lambda_{\\max}(Q)$ and $\\lambda_{\\max}(Q)$ denotes the largest eigenvalue of $Q$.\n- For Block Coordinate Descent (BCD) with block size $1$: perform cyclic coordinate updates over $j = 1, 2, \\dots, n$, and for each coordinate $j$, minimize $f$ with respect to $x_j$ while holding the other coordinates fixed. This exact one-dimensional minimization sets the $j$-th partial derivative to zero, producing the update $x_j \\leftarrow \\left(b_j - \\sum_{k \\ne j} Q_{jk} x_k \\right) / Q_{jj}$. Count one iteration as one full sweep over all $n$ coordinates in increasing order.\n- Stopping criterion for both methods: stop when the relative suboptimality $(f(x) - f(x^\\star)) / (f(x^{(0)}) - f(x^\\star)) \\le \\varepsilon$, with tolerance $\\varepsilon = 10^{-6}$.\n- Maximum iterations: for GD, enforce a cap of $50{,}000$ iterations; for BCD, enforce a cap of $5{,}000$ full sweeps. If the stopping criterion is not met within the cap, return the cap value.\n\nUse the following test suite of parameter triples $(n, \\kappa, \\rho)$:\n\n- Case $1$: $(n, \\kappa, \\rho) = (50, 1, 0)$.\n- Case $2$: $(n, \\kappa, \\rho) = (50, 10, 0.1)$.\n- Case $3$: $(n, \\kappa, \\rho) = (50, 1000, 0.1)$.\n- Case $4$: $(n, \\kappa, \\rho) = (5, 100, 10)$.\n\nYour program must, for each case, compute and return two integers: the iteration count for GD (number of gradient steps) and the iteration count for BCD (number of full sweeps). The final output must aggregate the results of all test cases into a single line, formatted as a comma-separated list enclosed in square brackets with no spaces, where each element is the pair for a case, for example, $[[i_{1,\\mathrm{GD}},i_{1,\\mathrm{BCD}}],[i_{2,\\mathrm{GD}},i_{2,\\mathrm{BCD}}],[i_{3,\\mathrm{GD}},i_{3,\\mathrm{BCD}}],[i_{4,\\mathrm{GD}},i_{4,\\mathrm{BCD}}]]$.\n\nThere are no physical units involved. Angles are not involved. Percentages are not used; the tolerance is a pure decimal. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, exactly like $[[i_{1,\\mathrm{GD}},i_{1,\\mathrm{BCD}}],[i_{2,\\mathrm{GD}},i_{2,\\mathrm{BCD}}],[i_{3,\\mathrm{GD}},i_{3,\\mathrm{BCD}}],[i_{4,\\mathrm{GD}},i_{4,\\mathrm{BCD}}]]$.", "solution": "The problem requires us to compare the convergence of Gradient Descent (GD) and Block Coordinate Descent (BCD) on a specific convex quadratic objective function $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$.\n\n**1. Preliminaries**\nThe function $f(x)$ is a strictly convex quadratic form, as its Hessian, $Q$, is positive definite. The unique global minimizer $x^\\star$ is found by setting the gradient $\\nabla f(x) = Qx - b$ to zero, which gives the linear system $Qx = b$. The solution is $x^\\star = Q^{-1}b$.\n\nFor the stopping criterion, we need the value of the function at the optimum, $f(x^\\star)$, and at the initial point, $f(x^{(0)})$.\nThe initial point is $x^{(0)} = \\mathbf{0}$, so $f(x^{(0)}) = \\tfrac{1}{2} \\mathbf{0}^\\top Q \\mathbf{0} - b^\\top \\mathbf{0} = 0$.\nThe optimal value is $f(x^\\star) = \\tfrac{1}{2} (x^\\star)^\\top Q x^\\star - b^\\top x^\\star$. Since $Qx^\\star = b$, this simplifies to $f(x^\\star) = \\tfrac{1}{2} (x^\\star)^\\top b - b^\\top x^\\star = -\\tfrac{1}{2} b^\\top x^\\star$.\nThe stopping criterion is checked at the beginning of each iteration $t$ for the current iterate $x^{(t)}$:\n$$ \\frac{f(x^{(t)}) - f(x^\\star)}{f(x^{(0)}) - f(x^\\star)} \\le \\varepsilon $$\nwhere $\\varepsilon = 10^{-6}$. The denominator is $f(x^{(0)}) - f(x^\\star) = 0 - (-\\tfrac{1}{2}b^\\top x^\\star) = \\tfrac{1}{2}b^\\top x^\\star$. This value is constant throughout the iterations.\n\n**2. Gradient Descent (GD) Algorithm**\nThe GD algorithm generates a sequence of iterates $x^{(t)}$ using the update rule:\n$$ x^{(t+1)} = x^{(t)} - \\alpha \\nabla f(x^{(t)}) = x^{(t)} - \\alpha (Q x^{(t)} - b) $$\nThe step size $\\alpha$ is fixed to the reciprocal of the largest eigenvalue of $Q$, $\\alpha = 1/\\lambda_{\\max}(Q)$. This choice is standard for quadratic objectives and guarantees convergence. The value of $\\lambda_{\\max}(Q)$ must be computed for each test case. An iteration consists of a single gradient computation and one update step.\n\n**3. Block Coordinate Descent (BCD) Algorithm**\nThe BCD algorithm iteratively minimizes the objective function along one coordinate direction at a time, cycling through the coordinates. For a quadratic function, this minimization can be done exactly. One full iteration (or sweep) consists of updating all $n$ coordinates sequentially for $j=1, 2, \\dots, n$.\nThe update for the $j$-th coordinate, holding all other coordinates $x_k$ ($k \\ne j$) fixed, is derived by solving $\\frac{\\partial f}{\\partial x_j} = 0$.\nThe partial derivative is $(\\nabla f(x))_j = (Qx)_j - b_j = \\sum_{k=1}^n Q_{jk}x_k - b_j = 0$.\nIsolating the new $x_j$ (let's call it $x_j^{\\text{new}}$), we have:\n$$ Q_{jj} x_j^{\\text{new}} + \\sum_{k \\ne j} Q_{jk} x_k = b_j $$\n$$ x_j^{\\text{new}} = \\frac{1}{Q_{jj}} \\left( b_j - \\sum_{k \\ne j} Q_{jk} x_k \\right) $$\nIn the cyclic BCD implementation, when updating $x_j$, the values of $x_k$ for $k < j$ have already been updated in the current sweep, while values for $k > j$ are from the previous sweep. The diagonal entries of $Q$ are $Q_{11} = \\kappa + \\rho$ and $Q_{jj} = 1 + \\rho$ for $j > 1$. Since $\\kappa>0$ and $\\rho\\ge0$, all $Q_{jj}>0$, so the division is well-defined.\n\n**4. Implementation Plan**\nFor each test case $(n, \\kappa, \\rho)$:\n1. Construct the matrix $Q \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^n$.\n2. Compute the exact minimizer $x^\\star$ by solving the linear system $Qx = b$.\n3. Compute the optimal function value $f(x^\\star)$.\n4. Compute the denominator of the stopping criterion, $f_0\\_minus\\_fs = f(x^{(0)}) - f(x^\\star)$.\n5. Run GD:\n   a. Compute $\\lambda_{\\max}(Q)$ and set step size $\\alpha$.\n   b. Initialize $x = \\mathbf{0}$.\n   c. Iterate, counting steps, until the relative suboptimality is $\\le \\varepsilon$ or the iteration cap is reached.\n6. Run BCD:\n   a. Initialize $x = \\mathbf{0}$.\n   b. Iterate, counting full sweeps, until the relative suboptimality is $\\le \\varepsilon$ or the sweep cap is reached. In each sweep, update all $n$ coordinates.\n7. Store the iteration counts for both methods.\nFinally, format all results into the specified string.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for a series of test cases, comparing\n    Gradient Descent (GD) and Block Coordinate Descent (BCD).\n    \"\"\"\n\n    # Parameter triples (n, kappa, rho) for each test case.\n    test_cases = [\n        (50, 1, 0),\n        (50, 10, 0.1),\n        (50, 1000, 0.1),\n        (5, 100, 10)\n    ]\n\n    results = []\n    \n    # Algorithmic constants\n    EPSILON = 1e-6\n    MAX_ITER_GD = 50000\n    MAX_ITER_BCD = 5000\n\n    for n, kappa, rho in test_cases:\n        # 1. Construct Q and b\n        b = np.ones(n)\n        diag_D = np.array([kappa] + [1.0] * (n - 1))\n        D = np.diag(diag_D)\n        ones_vec = np.ones((n, 1))\n        Q = D + rho * (ones_vec @ ones_vec.T)\n\n        # 2. Compute exact minimizer x_star and f_star\n        # The matrix Q is symmetric and positive definite, so linalg.solve is stable.\n        x_star = np.linalg.solve(Q, b)\n        f_star = 0.5 * x_star.T @ Q @ x_star - b.T @ x_star\n\n        # 3. Setup for iterations\n        x0 = np.zeros(n)\n        f0 = 0.0 # f(x0) = 0.5 * 0.T @ Q @ 0 - b.T @ 0 = 0\n        \n        # Denominator for the relative suboptimality stopping criterion\n        f0_minus_f_star = f0 - f_star\n        \n        if f0_minus_f_star < 1e-12: # Already at or very close to optimum\n            results.append([0, 0])\n            continue\n            \n        # --- Gradient Descent (GD) ---\n        iters_gd = MAX_ITER_GD\n        x_gd = x0.copy()\n        \n        # Compute step size alpha = 1 / lambda_max(Q)\n        eigvals = np.linalg.eigvalsh(Q)\n        lambda_max = np.max(eigvals)\n        alpha = 1.0 / lambda_max\n        \n        for i in range(MAX_ITER_GD + 1):\n            # Check stopping criterion at the beginning of the iteration\n            f_gd = 0.5 * x_gd.T @ Q @ x_gd - b.T @ x_gd\n            rel_subopt = (f_gd - f_star) / f0_minus_f_star\n            \n            if rel_subopt <= EPSILON:\n                iters_gd = i\n                break\n            \n            # Perform GD update if not stopping\n            if i < MAX_ITER_GD:\n                grad = Q @ x_gd - b\n                x_gd -= alpha * grad\n\n        # --- Block Coordinate Descent (BCD) ---\n        iters_bcd = MAX_ITER_BCD\n        x_bcd = x0.copy()\n        Q_diag = np.diag(Q)\n\n        for i in range(MAX_ITER_BCD + 1):\n            # Check stopping criterion at the beginning of a full sweep\n            f_bcd = 0.5 * x_bcd.T @ Q @ x_bcd - b.T @ x_bcd\n            rel_subopt = (f_bcd - f_star) / f0_minus_f_star\n\n            if rel_subopt <= EPSILON:\n                iters_bcd = i\n                break\n\n            # Perform BCD update sweep if not stopping\n            if i < MAX_ITER_BCD:\n                for j in range(n):\n                    # Efficiently calculate sum_{k != j} Q_jk * x_k\n                    sum_term = Q[j, :] @ x_bcd - Q_diag[j] * x_bcd[j]\n                    x_bcd[j] = (b[j] - sum_term) / Q_diag[j]\n\n        results.append([iters_gd, iters_bcd])\n\n    # Format the final output string exactly as specified\n    inner_parts = [f\"[{gd},{bcd}]\" for gd, bcd in results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3103362"}]}