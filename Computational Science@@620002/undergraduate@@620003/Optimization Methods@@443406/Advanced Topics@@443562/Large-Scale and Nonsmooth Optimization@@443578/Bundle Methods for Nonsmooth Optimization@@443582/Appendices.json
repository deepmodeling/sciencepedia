{"hands_on_practices": [{"introduction": "The core of any bundle method is the iterative cycle of proposing a candidate step and deciding whether to accept it. This decision is governed by the trade-off between the predicted descent from the model and the actual descent observed in the true objective function. This exercise [@problem_id:3105171] guides you through implementing the fundamental logic of a proximal bundle method, focusing on the \"serious step\" versus \"null step\" mechanism. By coding this yourself, you will gain a hands-on understanding of how the algorithm balances exploration (improving the model via null steps) with exploitation (making progress via serious steps).", "problem": "Consider the convex nonsmooth optimization problem defined by the objective function $f:\\mathbb{R}^n\\to\\mathbb{R}$ given by $f(x)=\\lVert Ax-b\\rVert_\\infty$, where $A\\in\\mathbb{R}^{m\\times n}$ and $b\\in\\mathbb{R}^m$. The infinity norm is defined by $\\lVert y\\rVert_\\infty=\\max_{1\\leq i\\leq m}|y_i|$, and the function $f(x)$ can be written as $f(x)=\\max_{1\\leq i\\leq m}|a_i^\\top x-b_i|$, where $a_i^\\top$ denotes the $i$-th row of $A$. This function is convex and piecewise linear, hence potentially nonsmooth.\n\nYour task is to implement a proximal bundle method to examine the influence of different initializations $x_0$ on the number of null steps before the first serious step. The proximal bundle algorithm should be formulated from fundamental principles of convex analysis and nonsmooth optimization as follows:\n\n1. Begin from the definitions of convexity, subgradients, and cutting-plane models. For a given point $x$, a valid subgradient $g(x)$ of $f$ can be constructed by selecting an index $i^\\star\\in\\operatorname{argmax}_{1\\leq i\\leq m}|a_i^\\top x-b_i|$, and defining\n$$\ng(x)=\\operatorname{sign}(a_{i^\\star}^\\top x-b_{i^\\star})\\,a_{i^\\star},\n$$\nwhere $\\operatorname{sign}(t)=1$ if $t>0$, $\\operatorname{sign}(t)=-1$ if $t<0$, and $\\operatorname{sign}(0)=0$.\n\n2. Maintain a bundle of cutting planes $\\{(y_j,f_j,g_j)\\}_{j=1}^J$, where $y_j\\in\\mathbb{R}^n$ is a previously evaluated point, $f_j=f(y_j)$ is the function value, and $g_j\\in\\mathbb{R}^n$ is a subgradient at $y_j$. Define the cutting-plane model\n$$\nm_J(x)=\\max_{1\\leq j\\leq J}\\left\\{f_j+g_j^\\top(x-y_j)\\right\\}.\n$$\n\n3. At iteration $k$ with current proximal center $x_k$, solve the proximal subproblem\n$$\n\\min_{x\\in\\mathbb{R}^n,\\;z\\in\\mathbb{R}} \\quad z+\\frac{t}{2}\\lVert x-x_k\\rVert_2^2\n\\quad\\text{subject to}\\quad\nz\\geq f_j+g_j^\\top(x-y_j)\\quad\\text{for all }j\\in\\{1,\\dots,J\\},\n$$\nwhere $t>0$ is a fixed proximal parameter, to obtain a candidate $(\\hat{x},\\hat{z})$. Note that at the solution, $\\hat{z}=m_J(\\hat{x})$.\n\n4. Evaluate the actual and predicted decreases. Let $\\Delta_k^{\\text{m}}=f(x_k)-\\hat{z}$ denote the predicted decrease and let $\\Delta_k^{\\text{a}}=f(x_k)-f(\\hat{x})$ denote the actual decrease. Use a standard bundle acceptance test with parameter $\\alpha\\in(0,1)$:\n$$\n\\text{accept as a serious step if}\\quad \\Delta_k^{\\text{a}}\\geq \\alpha\\,\\Delta_k^{\\text{m}}.\n$$\nIf the test is satisfied, record the number of null steps taken before this first serious step and stop. Otherwise, declare a null step, add the new cut $(y_{J+1},f_{J+1},g_{J+1})=(\\hat{x},f(\\hat{x}),g(\\hat{x}))$ to the bundle, keep $x_k$ unchanged, and repeat.\n\n5. To ensure termination in edge cases, impose a maximum number of null steps $K_{\\max}$; if no serious step occurs within $K_{\\max}$ null steps, return $K_{\\max}$.\n\nImplement this procedure for the following fixed problem data:\n- Dimension $n=2$ and $m=2$.\n- Matrix $A=\\begin{bmatrix}2  0 \\\\ 0  3\\end{bmatrix}$.\n- Vector $b=\\begin{bmatrix}2 \\\\ -6\\end{bmatrix}$.\n- Proximal parameter $t=1$.\n- Acceptance parameter $\\alpha=0.3$.\n- Maximum number of null steps $K_{\\max}=50$.\n- Euclidean norm $\\lVert\\cdot\\rVert_2$ used in the proximal term.\n\nUse the following test suite of initial points $x_0\\in\\mathbb{R}^2$:\n- Test $1$: $x_0=\\begin{bmatrix}1 \\\\ -2\\end{bmatrix}$.\n- Test $2$: $x_0=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$.\n- Test $3$: $x_0=\\begin{bmatrix}10 \\\\ 10\\end{bmatrix}$.\n- Test $4$: $x_0=\\begin{bmatrix}-100 \\\\ 50\\end{bmatrix}$.\n- Test $5$: $x_0=\\begin{bmatrix}10^{-8} \\\\ -2 \\cdot 10^{-8}\\end{bmatrix}$.\n\nEach test case requires computing the number of null steps taken by the algorithm before the first serious step occurs, starting from the specified $x_0$. If a serious step occurs immediately (at the first candidate), the number of null steps is $0$. If no serious step occurs within $K_{\\max}$ null steps, report $K_{\\max}$.\n\nFinal output format: Your program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is an integer representing the number of null steps observed for test $i$.", "solution": "The user wants me to implement a proximal bundle method to solve a nonsmooth convex optimization problem and determine the number of null steps before the first serious step is taken for several different initial points.\n\n### 1. Problem Specification\nThe objective function is $f(x) = \\lVert Ax - b \\rVert_\\infty$, where $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^m$.\nThis function can be written as $f(x) = \\max_{1 \\le i \\le m} |a_i^\\top x - b_i|$, where $a_i^\\top$ is the $i$-th row of $A$.\nThe problem provides the following data:\n- $n=2$, $m=2$\n- $A = \\begin{bmatrix} 2  0 \\\\ 0  3 \\end{bmatrix}$, so $a_1 = [2, 0]^\\top$ and $a_2 = [0, 3]^\\top$.\n- $b = \\begin{bmatrix} 2 \\\\ -6 \\end{bmatrix}$, so $b_1 = 2$ and $b_2 = -6$.\nThus, the objective function is $f(x) = \\max \\left( |2x_1 - 2|, |3x_2 + 6| \\right)$.\nThe minimizer of this function is $x^* = [1, -2]^\\top$, where $f(x^*) = 0$.\n\n### 2. The Proximal Bundle Algorithm\nThe algorithm proceeds as follows:\n1.  **Initialization**: Start with an initial point $x_0$. This point serves as the first proximal center, $x_k$ (for $k=0$). A bundle of information $B$ is initialized with the point itself, its function value, and a subgradient: $B = \\{(y_1, f_1, g_1)\\}$, where $y_1=x_k$, $f_1=f(x_k)$, and $g_1 \\in \\partial f(x_k)$. The problem specifies the subgradient calculation:\n    $g(x) = \\operatorname{sign}(a_{i^\\star}^\\top x - b_{i^\\star}) a_{i^\\star}$ for $i^\\star \\in \\operatorname{argmax}_{1 \\le i \\le m} |a_i^\\top x - b_i|$. The sign function is defined as $\\operatorname{sign}(t)=1$ for $t0$, $-1$ for $t0$, and $0$ for $t=0$.\n\n2.  **Subproblem Formulation**: At each iteration, a candidate point $\\hat{x}$ is found by solving a regularized version of the cutting-plane model of the function. The primal subproblem is:\n    $$ \\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}} \\quad z + \\frac{t}{2}\\lVert x - x_k \\rVert_2^2 \\quad \\text{s.t.} \\quad z \\ge f_j + g_j^\\top(x - y_j), \\quad \\forall (y_j, f_j, g_j) \\in B $$\n    This problem is typically solved via its dual form, which is a convex quadratic program (QP). Let the bundle have $J$ elements. The dual QP is:\n    $$ \\min_{\\lambda \\in \\mathbb{R}^J} \\quad \\frac{1}{2t} \\left\\lVert \\sum_{j=1}^J \\lambda_j g_j \\right\\rVert_2^2 + \\sum_{j=1}^J \\lambda_j \\epsilon_j \\quad \\text{s.t.} \\quad \\sum_{j=1}^J \\lambda_j = 1, \\quad \\lambda_j \\ge 0 $$\n    where $\\epsilon_j = f(x_k) - (f_j + g_j^\\top(x_k - y_j))$ is the linearization error of the $j$-th cut at the current center $x_k$.\n\n3.  **Candidate Point and Step Evaluation**: Once the optimal dual variables $\\lambda^*$ are found, the candidate point $\\hat{x}$ is computed as:\n    $$ \\hat{x} = x_k - \\frac{1}{t} \\sum_{j=1}^J \\lambda_j^* g_j $$\n    The quality of this step is evaluated by comparing the actual decrease in function value, $\\Delta^a = f(x_k) - f(\\hat{x})$, with the predicted decrease from the model, $\\Delta^m$. The predicted decrease is given by:\n    $$ \\Delta^m = \\sum_{j=1}^J \\lambda_j^* \\epsilon_j + \\frac{1}{t} \\left\\lVert \\sum_{j=1}^J \\lambda_j^* g_j \\right\\rVert_2^2 $$\n\n4.  **Acceptance Test**: A step from $x_k$ to $\\hat{x}$ is a **serious step** if the actual decrease is a sufficient fraction of the predicted decrease:\n    $$ \\Delta^a \\ge \\alpha \\Delta^m $$\n    where $\\alpha \\in (0, 1)$ is a given parameter.\n    - If it is a serious step, the algorithm has succeeded in finding an improvement. For this problem, we stop and record the number of null steps taken so far.\n    - If the condition is not met, the step is a **null step**. The proximal center $x_k$ is not updated. Instead, the information from the candidate point is used to improve the model by adding a new cutting plane to the bundle: $B \\leftarrow B \\cup \\{(\\hat{x}, f(\\hat{x}), g(\\hat{x}))\\}$. The null step counter is incremented, and the process repeats from the QP subproblem.\n\n5.  **Termination**: The procedure for each test case terminates either upon the first serious step or after a maximum of $K_{\\max}$ null steps have been performed.\n\n### 3. Implementation Plan\nFor each provided initial point $x_0$, the algorithm is executed as described.\n- The core of each iteration involves solving a QP. This will be handled using `scipy.optimize.minimize` with the 'SLSQP' method, which is suitable for constrained quadratic programming.\n- The bundle will be managed as a list of tuples. At each null step, this list grows, and the size of the QP to be solved increases by one.\n- The process starts with a null step count of $0$. If the first candidate point leads to a serious step, the result is $0$. Otherwise, the count is incremented for each null step.\n- The provided parameters are $t=1$, $\\alpha=0.3$, and $K_{\\max}=50$.\n- The implementation will loop through the five test cases, execute the bundle method for each, and store the resulting null step count.\n\n### 4. Code Structure\nThe implementation is encapsulated within a `solve` function.\n- Helper functions `f_obj(x, A, b)` and `g_subgrad(x, A, b)` will compute the objective value and a subgradient, respectively.\n- The main logic for a single test case is separated into a function `solve_bundle_case`. This function initializes the bundle from the starting point $x_0$ and enters a loop that represents the search for a serious step.\n- Inside the loop, it constructs and solves the dual QP for the current bundle, calculates the candidate point $\\hat{x}$, and performs the acceptance test.\n- If the test passes, the loop terminates, and the current null step count is returned. If the test fails, a new cut is added to the bundle, and the loop continues.\n- If the loop completes (i.e., $K_{\\max}$ iterations without a serious step), $K_{\\max}$ is returned.\n- Finally, the `solve` function collects the results from all test cases and prints them in the specified format.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function to run the proximal bundle method for all test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    A = np.array([[2.0, 0.0], [0.0, 3.0]])\n    b = np.array([2.0, -6.0])\n    \n    # --- Algorithm Parameters ---\n    t = 1.0\n    alpha = 0.3\n    K_max = 50\n\n    # --- Test Cases ---\n    test_cases = [\n        np.array([1.0, -2.0]),\n        np.array([0.0, 0.0]),\n        np.array([10.0, 10.0]),\n        np.array([-100.0, 50.0]),\n        np.array([1e-8, -2e-8]),\n    ]\n    \n    # --- Helper Functions ---\n    def f_obj(x, A_mat, b_vec):\n        \"\"\"Computes the objective function f(x) = ||Ax - b||_inf.\"\"\"\n        return np.linalg.norm(A_mat @ x - b_vec, ord=np.inf)\n\n    def g_subgrad(x, A_mat, b_vec):\n        \"\"\"Computes a subgradient g(x) of f at x.\"\"\"\n        y = A_mat @ x - b_vec\n        istar = np.argmax(np.abs(y))\n        sign_val = np.sign(y[istar])\n        # The problem specifies sign(0) = 0. np.sign abides by this.\n        return sign_val * A_mat[istar, :].T\n\n    def run_bundle_for_case(x_start):\n        \"\"\"\n        Executes the proximal bundle method for a single starting point x_start.\n        Returns the number of null steps before the first serious step.\n        \"\"\"\n        x_k = np.array(x_start, dtype=float)\n        f_k = f_obj(x_k, A, b)\n        \n        # The bundle is a list of tuples: (y_j, f_j, g_j)\n        bundle = [(x_k, f_k, g_subgrad(x_k, A, b))]\n\n        for s in range(K_max):\n            J = len(bundle)\n            \n            # --- Form and Solve the Dual QP ---\n            # 1. Collect subgradients into a matrix G and compute linearization errors\n            G_matrix = np.array([item[2] for item in bundle]).T\n            epsilons = np.zeros(J)\n            for j in range(J):\n                y_j, f_j, g_j = bundle[j]\n                epsilons[j] = f_k - (f_j + g_j.T @ (x_k - y_j))\n            \n            # 2. Define the QP objective and its Jacobian\n            H = (1.0 / t) * (G_matrix.T @ G_matrix)\n            \n            def qp_obj(lmbda):\n                return 0.5 * lmbda.T @ H @ lmbda + epsilons.T @ lmbda\n\n            def qp_jac(lmbda):\n                return H @ lmbda + epsilons\n\n            # 3. Define QP constraints: sum(lambda_j) = 1 and lambda_j = 0\n            constraints = [{'type': 'eq', 'fun': lambda lmbda: np.sum(lmbda) - 1.0}]\n            bounds = [(0, None) for _ in range(J)]\n            \n            # 4. Solve the QP\n            lmbda_0 = np.ones(J) / J\n            res = minimize(qp_obj, lmbda_0, jac=qp_jac, bounds=bounds, constraints=constraints, method='SLSQP')\n            lmbda_star = res.x\n            \n            # --- Evaluate Candidate Step ---\n            g_agg = G_matrix @ lmbda_star\n            x_hat = x_k - (1.0 / t) * g_agg\n            \n            # Predicted decrease from the model\n            eps_agg = epsilons.T @ lmbda_star\n            delta_m = eps_agg + (1.0 / t) * np.linalg.norm(g_agg)**2\n            \n            # Actual decrease in objective function\n            f_hat = f_obj(x_hat, A, b)\n            delta_a = f_k - f_hat\n            \n            # --- Acceptance Test ---\n            if delta_a = alpha * delta_m:\n                # Serious step: found an improvement.\n                return s\n            else:\n                # Null step: improve the model by adding the new information.\n                # The proximal center x_k is NOT updated.\n                bundle.append((x_hat, f_hat, g_subgrad(x_hat, A, b)))\n        \n        # If the loop finishes, K_max null steps were taken without a serious step.\n        return K_max\n\n    # --- Main Execution Loop ---\n    results = []\n    for x0 in test_cases:\n        num_null_steps = run_bundle_for_case(x0)\n        results.append(num_null_steps)\n        \n    # --- Format and Print Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3105171"}, {"introduction": "A key innovation of bundle methods over simpler subgradient methods is the concept of an \"aggregate subgradient.\" Instead of using a single, potentially unreliable subgradient, the method synthesizes information from the entire bundle to form a more robust descent direction. This practice [@problem_id:3105153] challenges you to compare the predictive quality of the sophisticated aggregate subgradient, which is the solution to a quadratic program, against a more intuitive averaged subgradient. This comparison will clarify why aggregation is a principled way to find the best direction, leading to more effective steps.", "problem": "You are given a nonsmooth objective defined by the pointwise maximum of linear functions. Let $A \\in \\mathbb{R}^{m \\times n}$ have rows $\\{a_i^\\top\\}_{i=1}^m$, and define the function $f : \\mathbb{R}^n \\to \\mathbb{R}$ by $f(x) = \\max_{1 \\le i \\le m} a_i^\\top x$. The subdifferential $\\partial f(x)$ of $f$ at a point $x$ is the convex hull of the set of active rows $\\{a_i : i \\in I(x)\\}$, where $I(x)$ is the set of indices achieving the maximum at $x$. The aggregate subgradient $s_k$ used in bundle methods is taken to be the minimal Euclidean norm element of $\\partial f(x_k)$, which is the convex combination of active rows at $x_k$ having the smallest norm in $\\mathbb{R}^n$. Formally, if $I(x_k) = \\{i_1,\\dots,i_p\\}$, then $s_k$ is obtained by minimizing the squared Euclidean norm $\\lVert s\\rVert_2^2$ over $s = \\sum_{j=1}^p \\alpha_j a_{i_j}$ with $\\alpha_j \\ge 0$ and $\\sum_{j=1}^p \\alpha_j = 1$. Separately, define an averaged recent-window predictor $\\bar{g}$ as an arithmetic mean of recent subgradients $g_j$ computed at recent iterates $\\{x_j\\}$, where each $g_j$ is any valid subgradient at $x_j$ (for this exercise, take $g_j$ to be one active row $a_i$ at $x_j$, choosing the smallest index in $I(x_j)$ to ensure determinism).\n\nYour task is to implement a program that, for each specified test case, performs the following steps at the current iterate $x_k$:\n1. Compute the aggregate subgradient $s_k$ as the minimal Euclidean norm convex combination of the active rows at $x_k$ (if $I(x_k)$ is a singleton, then $s_k$ is that single vector).\n2. Compute the averaged recent-window subgradient $\\bar{g}$ from the provided recent iterates using the deterministic selection rule described above.\n3. For each predictor $g_{\\text{hat}} \\in \\{s_k, \\bar{g}\\}$, form the steepest-descent direction $d = - g_{\\text{hat}}$, take a small step of size $t$ (given below), and calculate:\n   - The actual decrease $\\Delta f = f(x_k) - f(x_k + t d)$, with $f$ evaluated exactly by the maximum definition.\n   - The linear-model predicted decrease $v_{\\text{hat}} = t \\lVert g_{\\text{hat}}\\rVert_2^2$, which comes from the local linearization of $f$ using $g_{\\text{hat}}$ as the predictor of the subgradient at $x_k$.\n4. Define the alignment error $e(g_{\\text{hat}}) = |\\Delta f - v_{\\text{hat}}|$ and declare that the aggregate subgradient aligns better if $e(s_k)  e(\\bar{g})$; otherwise, the averaged recent-window predictor aligns better or they tie.\n\nYour program must output, for the suite of test cases below, a single line containing a list of boolean values indicating whether the aggregate subgradient $s_k$ aligns better than $\\bar{g}$ for each case.\n\nThere are no physical units involved in this problem. All angles, if any, are implicitly in radians, but this problem does not use angles. All fractions and decimals are already specified as such; do not use percentage signs.\n\nUse the following test suite. In every case, use the step size $t = 0.01$. For each test case, $A$ is given by its rows, $x_k$ is the current iterate, and the recent window is a list of points at which to compute the recent subgradients $g_j$ and average them to form $\\bar{g}$. When selecting a subgradient at a recent point $x_j$, use the smallest index in $I(x_j)$ to choose $a_i$ deterministically.\n\n- Test Case 1 (two active rows at $x_k$):\n  - $A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.7  0.6 \\\\ 1.0  -0.5 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$,\n  - Recent window points: $\\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}, \\begin{bmatrix} 1.2 \\\\ -0.1 \\end{bmatrix} \\right\\}$.\n\n- Test Case 2 (unique active row at $x_k$ with negative entries):\n  - $A = \\begin{bmatrix} -1.0  0.0 \\\\ 0.0  -1.0 \\\\ -0.5  -0.2 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} -1.0 \\\\ -2.0 \\end{bmatrix}$,\n  - Recent window points: $\\left\\{ \\begin{bmatrix} -1.0 \\\\ -2.0 \\end{bmatrix}, \\begin{bmatrix} -0.8 \\\\ -2.2 \\end{bmatrix}, \\begin{bmatrix} -1.2 \\\\ -1.8 \\end{bmatrix}, \\begin{bmatrix} -1.1 \\\\ -2.5 \\end{bmatrix} \\right\\}$.\n\n- Test Case 3 (window size one):\n  - $A = \\begin{bmatrix} 0.0  1.0 \\\\ 1.0  0.1 \\\\ 0.6  0.7 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} 0.2 \\\\ 0.5 \\end{bmatrix}$,\n  - Recent window points: $\\left\\{ \\begin{bmatrix} 0.1 \\\\ 0.4 \\end{bmatrix} \\right\\}$.\n\n- Test Case 4 (three dimensions with ties among duplicates):\n  - $A = \\begin{bmatrix} 1.0  -0.5  0.2 \\\\ 0.8  0.9  -0.3 \\\\ 0.6  0.0  1.2 \\\\ 1.0  -0.5  0.2 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}$,\n  - Recent window points: $\\left\\{ \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}, \\begin{bmatrix} 0.6 \\\\ -0.5 \\\\ 0.8 \\end{bmatrix} \\right\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is a boolean indicating whether $e(s_k)  e(\\bar{g})$ for the corresponding test case. Ensure deterministic selection when ties occur by choosing the smallest index in $I(x)$ at any point where a single subgradient is required.\n\nNote: The aggregate subgradient computation requires solving a convex Quadratic Programming (QP) problem of minimizing a convex quadratic objective over the probability simplex. You must implement a numerically stable and deterministic procedure that produces a valid solution for all given test cases.", "solution": "The problem presented is a valid computational task from the field of nonsmooth optimization. It requires the implementation and comparison of two distinct subgradient predictors for a function defined as the pointwise maximum of affine functions. The problem is scientifically grounded, well-posed, and all terms and data are provided unequivocally. I will proceed with a step-by-step solution.\n\nThe core of the problem is to compare the local predictive power of two subgradient estimates at a point $x_k$ for a function $f(x) = \\max_{1 \\le i \\le m} a_i^\\top x$. The two predictors are the aggregate subgradient $s_k$ and an averaged recent-window subgradient $\\bar{g}$. The comparison is based on the alignment error, which measures the discrepancy between the actual function decrease and the decrease predicted by a linear model based on the subgradient predictor. A smaller alignment error indicates a better predictor.\n\nLet's outline the necessary computational steps for a given test case comprising matrix $A$, current iterate $x_k$, and a set of recent window points. The step size for the descent step is given as $t = 0.01$.\n\nFirst, we define two auxiliary functions: one for evaluating $f(x)$ and another for identifying the set of active subgradients at a point $x$.\nThe function $f(x)$ is computed as $f(x) = \\max_{i} (A x)_i$.\nThe set of active indices at $x$, denoted $I(x)$, is $I(x) = \\{i \\mid a_i^\\top x = f(x)\\}$. Due to floating-point arithmetic, we identify active indices as those for which $f(x) - a_i^\\top x  \\epsilon$ for a small tolerance $\\epsilon$ (e.g., $\\epsilon=10^{-9}$). The corresponding active subgradients are the rows $\\{a_i\\}_{i \\in I(x)}$.\n\n**Step 1: Compute the Aggregate Subgradient $s_k$**\n\nThe aggregate subgradient $s_k$ is the element with the minimum Euclidean norm in the subdifferential $\\partial f(x_k) = \\text{conv}\\{a_i \\mid i \\in I(x_k)\\}$. This is equivalent to projecting the origin onto the convex hull of the active subgradients at $x_k$.\n\nLet the set of active subgradients at $x_k$ be $G_{act} = \\{g_1, g_2, \\dots, g_p\\}$, where each $g_j$ is a row vector $a_i^\\top$ from $A$ for some $i \\in I(x_k)$. We need to find $s_k = \\sum_{j=1}^p \\alpha_j g_j$ that minimizes $\\lVert s_k \\rVert_2^2$ subject to the constraints $\\sum_{j=1}^p \\alpha_j = 1$ and $\\alpha_j \\ge 0$ for all $j$.\n\nThis is a convex Quadratic Programming (QP) problem. Let $G$ be the matrix whose columns are the vectors $g_j \\in G_{act}$. We seek to find the vector $\\alpha = [\\alpha_1, \\dots, \\alpha_p]^\\top$ that solves:\n$$\n\\min_{\\alpha} \\frac{1}{2} \\lVert G \\alpha \\rVert_2^2 = \\frac{1}{2} \\alpha^\\top (G^\\top G) \\alpha\n$$\nsubject to:\n$$\n\\sum_{j=1}^p \\alpha_j = 1, \\quad \\alpha_j \\ge 0, \\quad j=1, \\dots, p\n$$\nThis standard QP problem can be solved using numerical optimization libraries, such as `scipy.optimize.minimize` with the 'SLSQP' method. The objective function is quadratic, and the constraints are linear.\n\nIf $|I(x_k)|=1$, meaning there is only one active subgradient $a_i$, the convex hull is a single point, and thus $s_k = a_i$.\n\n**Step 2: Compute the Averaged Recent-Window Subgradient $\\bar{g}$**\n\nThe predictor $\\bar{g}$ is the arithmetic mean of subgradients computed at a series of recent points provided in the \"recent window\". For each point $x_j$ in the window, we must first determine a single subgradient $g_j \\in \\partial f(x_j)$. The problem specifies a deterministic rule: identify the set of active indices $I(x_j)$ and choose the subgradient $a_i$ corresponding to the smallest index $i$ in $I(x_j)$.\nAfter computing $g_j$ for each $x_j$ in the window, $\\bar{g}$ is calculated as:\n$$\n\\bar{g} = \\frac{1}{|W|} \\sum_{x_j \\in W} g_j\n$$\nwhere $W$ is the set of recent window points.\n\n**Step 3: Calculate Alignment Errors**\n\nFor each predictor $g_{\\text{hat}} \\in \\{s_k, \\bar{g}\\}$, we evaluate its quality. The quality is measured by comparing the actual function decrease to a predicted decrease derived from a linear model of the function.\n\nThe descent direction is taken as $d = -g_{\\text{hat}}$. A step of size $t$ is taken from $x_k$ to a new point $x_{\\text{new}} = x_k + t d$.\n\nThe actual function decrease is:\n$$\n\\Delta f = f(x_k) - f(x_{\\text{new}}) = f(x_k) - f(x_k - t g_{\\text{hat}})\n$$\n\nThe predicted decrease, based on the first-order approximation $f(x_k+d) \\approx f(x_k) + g_{\\text{hat}}^\\top d$, is:\n$$\nv_{\\text{hat}} = -g_{\\text{hat}}^\\top d = -g_{\\text{hat}}^\\top (-t g_{\\text{hat}}) = t \\lVert g_{\\text{hat}} \\rVert_2^2\n$$\n\nThe alignment error for the predictor $g_{\\text{hat}}$ is defined as the absolute difference between the actual and predicted decreases:\n$$\ne(g_{\\text{hat}}) = |\\Delta f - v_{\\text{hat}}|\n$$\nWe compute this error for both $s_k$ and $\\bar{g}$, yielding $e(s_k)$ and $e(\\bar{g})$.\n\n**Step 4: Compare Predictors**\n\nThe final step is to compare the two errors. The aggregate subgradient $s_k$ is considered to align better if its error is strictly smaller than that of the averaged predictor $\\bar{g}$. That is, the result for the test case is `True` if $e(s_k)  e(\\bar{g})$, and `False` otherwise. This comparison is performed for each test case provided. The final output is a list of these boolean results.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    \"\"\"\n    Solves the nonsmooth optimization predictor comparison problem for a suite of test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.0], [0.7, 0.6], [1.0, -0.5]]),\n            \"x_k\": np.array([1.0, 0.0]),\n            \"window_points\": [np.array([1.0, 0.0]), np.array([0.8, 0.2]), np.array([1.2, -0.1])]\n        },\n        {\n            \"A\": np.array([[-1.0, 0.0], [0.0, -1.0], [-0.5, -0.2]]),\n            \"x_k\": np.array([-1.0, -2.0]),\n            \"window_points\": [np.array([-1.0, -2.0]), np.array([-0.8, -2.2]), np.array([-1.2, -1.8]), np.array([-1.1, -2.5])]\n        },\n        {\n            \"A\": np.array([[0.0, 1.0], [1.0, 0.1], [0.6, 0.7]]),\n            \"x_k\": np.array([0.2, 0.5]),\n            \"window_points\": [np.array([0.1, 0.4])]\n        },\n        {\n            \"A\": np.array([[1.0, -0.5, 0.2], [0.8, 0.9, -0.3], [0.6, 0.0, 1.2], [1.0, -0.5, 0.2]]),\n            \"x_k\": np.array([0.2, -1.0, 0.0]),\n            \"window_points\": [np.array([0.2, -1.0, 0.0]), np.array([0.0, -1.0, 0.5]), np.array([0.6, -0.5, 0.8])]\n        }\n    ]\n\n    t = 0.01  # Step size\n    epsilon = 1e-9 # Tolerance for floating point comparisons of active subgradients\n    results = []\n    \n    def f(x_vec, A_mat):\n        \"\"\"Computes the function value f(x) = max(A @ x).\"\"\"\n        return np.max(A_mat @ x_vec)\n\n    def get_active_indices(x_vec, A_mat):\n        \"\"\"Finds the indices of active rows in A at point x.\"\"\"\n        vals = A_mat @ x_vec\n        max_val = np.max(vals)\n        return np.where(max_val - vals  epsilon)[0]\n\n    for case in test_cases:\n        A, x_k, window_points = case[\"A\"], case[\"x_k\"], case[\"window_points\"]\n        \n        # 1. Compute the aggregate subgradient s_k\n        active_indices_k = get_active_indices(x_k, A)\n        \n        # We need to handle duplicate active subgradients carefully.\n        # The QP must be over unique vectors to form a valid basis.\n        unique_active_subgrads, unique_indices = np.unique(A[active_indices_k], axis=0, return_index=True)\n\n        if len(unique_active_subgrads) == 1:\n            s_k = unique_active_subgrads[0]\n        else:\n            # G has unique active subgradients as columns.\n            G = unique_active_subgrads.T\n            p = G.shape[1]\n            Q = G.T @ G\n            \n            def qp_objective(alpha, Q_mat):\n                return 0.5 * alpha.T @ Q_mat @ alpha\n\n            constraints = ({'type': 'eq', 'fun': lambda alpha: np.sum(alpha) - 1})\n            bounds = Bounds([0.] * p, [np.inf] * p)\n            alpha_0 = np.ones(p) / p\n            \n            res = minimize(fun=qp_objective, x0=alpha_0, args=(Q,), method='SLSQP', bounds=bounds, constraints=constraints)\n            optimal_alpha = res.x\n            s_k = G @ optimal_alpha\n\n        # 2. Compute the averaged recent-window subgradient g_bar\n        recent_subgrads = []\n        for x_j in window_points:\n            active_indices_j = get_active_indices(x_j, A)\n            smallest_idx = np.min(active_indices_j)\n            g_j = A[smallest_idx]\n            recent_subgrads.append(g_j)\n        g_bar = np.mean(recent_subgrads, axis=0)\n\n        # 3. Calculate alignment errors\n        f_k = f(x_k, A)\n        \n        # For s_k\n        d_s = -s_k\n        x_new_s = x_k + t * d_s\n        delta_f_s = f_k - f(x_new_s, A)\n        v_hat_s = t * np.linalg.norm(s_k)**2\n        e_s = np.abs(delta_f_s - v_hat_s)\n        \n        # For g_bar\n        d_g = -g_bar\n        x_new_g = x_k + t * d_g\n        delta_f_g = f_k - f(x_new_g, A)\n        v_hat_g = t * np.linalg.norm(g_bar)**2\n        e_g = np.abs(delta_f_g - v_hat_g)\n\n        # 4. Compare and append result\n        results.append(e_s  e_g)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105153"}, {"introduction": "In a real implementation, a bundle of cutting planes can accumulate redundant information, such as multiple identical cuts generated at different points. A robust algorithm must handle this degeneracy gracefully without its stability or solution being affected. This exercise [@problem_id:3105175] demonstrates a remarkable property of the dual formulation of the bundle method's master problem. You will show through direct computation that including duplicate cuts has no effect on the aggregated subgradient or the candidate step, solidifying your understanding of the stability that the dual formulation provides.", "problem": "Consider the nonsmooth convex function defined as the pointwise maximum of a finite set of affine functions (cuts). Let the function be represented by a collection of cuts $\\ell_i(x)$ of the form $\\ell_i(x) = g_i^\\top x + c_i$, where $g_i \\in \\mathbb{R}^n$ and $c_i \\in \\mathbb{R}$, and the cutting-plane model at iteration $k$ be $m_k(x) = \\max_i \\ell_i(x)$. The regularized master problem in bundle methods is to compute the proximal center update $x^\\star$ by solving the convex Quadratic Programming (QP) problem\n$$\n\\min_{x \\in \\mathbb{R}^n, \\, s \\in \\mathbb{R}} \\; s + \\frac{t}{2}\\lVert x - x_k\\rVert_2^2 \\quad \\text{subject to} \\quad s \\ge \\ell_i(x) \\;\\; \\text{for all}\\;\\; i,\n$$\nwhere $t  0$ is the regularization parameter and $x_k \\in \\mathbb{R}^n$ is the current proximal center.\n\nA phenomenon of degeneracy in bundle methods occurs when multiple cuts $\\ell_i$ coincide (are identical) but originate from different points $x_i$ and potentially distinct subgradient evaluations $g_i$, even though they define the same affine function $x \\mapsto g^\\top x + c$. In such situations, aggregation of cuts (via dual multipliers associated with the active constraints of the master problem) should handle redundancy without destabilizing the step $x^\\star$ or the model prediction, and the aggregated cut should remain invariant under duplication of identical cuts.\n\nYour task is to implement a program that, for each scenario described below, solves the regularized master problem using the given cuts, computes the aggregated subgradient $g_{\\text{agg}}$ and the corresponding update $x^\\star$, and then evaluates the effect of duplicating identical cuts on the solution and model value. The aggregated cut can be expressed as $\\ell_{\\text{agg}}(x) = g_{\\text{agg}}^\\top x + c_{\\text{agg}}$, where $c_{\\text{agg}}$ is a convex combination of the offsets $c_i$ with coefficients that arise naturally from the master problem optimality conditions. Your implementation should be based on first principles of convex optimization, starting from the master problem and using only well-established definitions and facts; do not use any shortcut formulas or pre-derived expressions that skip the fundamental derivation path.\n\nUse the following scenarios (the dimension is $n = 2$). In each scenario, construct the specified sets of cuts, solve the master problem twice (once with duplicates and once with the duplicates collapsed to a single representative per identical cut), and compute the requested outputs. All numerical tolerances mentioned below are absolute tolerances.\n\nScenario $\\mathsf{S1}$ (general case):\n- Regularization parameter: $t = 2.0$.\n- Proximal center: $x_k = [1.3, -2.1]$.\n- Identical cut family $\\mathsf{A}$: $g_{\\mathsf{A}} = [2, -1]$, $c_{\\mathsf{A}} = 0.0$, repeated at three distinct points (thus three identical cuts).\n- Additional distinct cut $\\mathsf{B}$: $g_{\\mathsf{B}} = [-1, 2]$, $c_{\\mathsf{B}} = -0.5$, included once.\n- Construct the \"duplicates\" set with three copies of $\\mathsf{A}$ and one copy of $\\mathsf{B}$, and the \"unique\" set with one copy of $\\mathsf{A}$ and one copy of $\\mathsf{B}$.\n- Compute:\n  1. $b_1$: a boolean indicating whether $\\lVert x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\rVert_2 \\le 10^{-9}$.\n  2. $b_2$: a boolean indicating whether $\\lVert g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\rVert_2 \\le 10^{-9}$.\n  3. $d_1$: a float equal to $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\lVert x^\\star_{\\text{dup}} - x_k\\rVert_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\lVert x^\\star_{\\text{uniq}} - x_k\\rVert_2^2 \\right) \\right|$, where $s$ is the optimal epigraph variable in the master problem.\n\nScenario $\\mathsf{S2}$ (edge case: only identical cuts):\n- Regularization parameter: $t = 1.5$.\n- Proximal center: $x_k = [-3.7, 4.2]$.\n- Identical cut family $\\mathsf{A}$ only: $g_{\\mathsf{A}} = [2, -1]$, $c_{\\mathsf{A}} = 0.0$, repeated five times.\n- Construct the \"duplicates\" set with five copies of $\\mathsf{A}$ and the \"unique\" set with one copy of $\\mathsf{A}$.\n- Compute:\n  1. $b_3$: a boolean indicating whether $\\lVert x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\rVert_2 \\le 10^{-9}$.\n  2. $b_4$: a boolean indicating whether $\\lVert g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\rVert_2 \\le 10^{-9}$.\n  3. $d_2$: a float equal to $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\lVert x^\\star_{\\text{dup}} - x_k\\rVert_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\lVert x^\\star_{\\text{uniq}} - x_k\\rVert_2^2 \\right) \\right|$.\n\nScenario $\\mathsf{S3}$ (boundary case: very strong regularization):\n- Regularization parameter: $t = 1000.0$.\n- Proximal center: $x_k = [10.0, -10.0]$.\n- Identical cut family $\\mathsf{A}$: $g_{\\mathsf{A}} = [2, -1]$, $c_{\\mathsf{A}} = 0.0$, repeated four times.\n- Identical cut family $\\mathsf{B}$: $g_{\\mathsf{B}} = [-1, 2]$, $c_{\\mathsf{B}} = -0.5$, repeated four times.\n- Construct the \"duplicates\" set with four copies of $\\mathsf{A}$ and four copies of $\\mathsf{B}$, and the \"unique\" set with one copy of $\\mathsf{A}$ and one copy of $\\mathsf{B}$.\n- Compute:\n  1. $b_5$: a boolean indicating whether $\\lVert x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\rVert_2 \\le 10^{-9}$.\n  2. $b_6$: a boolean indicating whether $\\lVert g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\rVert_2 \\le 10^{-9}$.\n  3. $d_3$: a float equal to $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\lVert x^\\star_{\\text{dup}} - x_k\\rVert_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\lVert x^\\star_{\\text{uniq}} - x_k\\rVert_2^2 \\right) \\right|$.\n\nImplementation requirements:\n- Derive and use a principled algorithm from the master problem and fundamental convex optimization facts. You may work with an equivalent formulation in variables corresponding to the convex combination coefficients of active cuts (dual multipliers), provided your derivation is based on first principles.\n- For each scenario, you must compute $x^\\star$, $s$, and $g_{\\text{agg}}$ for both the duplicates and unique sets.\n- The final outputs must be computed with absolute tolerances of $10^{-9$}$ for the booleans and exact arithmetic as implemented for the floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the nine results aggregated in order as a comma-separated list enclosed in square brackets: $[b_1,b_2,d_1,b_3,b_4,d_2,b_5,b_6,d_3]$.\n- Booleans must be printed as either $\\texttt{True}$ or $\\texttt{False}$, and floats must be printed as standard decimal numbers.\n\nNo physical units or angle units are involved in this problem.", "solution": "The problem requires an analysis of the effect of redundant cuts in the bundle method master problem. This will be accomplished by deriving the dual of the regularized master problem, demonstrating its invariance to duplicate cuts, and numerically verifying this invariance for three specified scenarios.\n\n### Step 1: Problem Validation\n\nThe provided problem statement is a well-posed and scientifically grounded exercise in convex optimization, specifically concerning the theory of bundle methods for nonsmooth functions.\n\n**1. Extraction of Givens:**\nAll necessary parameters for three distinct scenarios ($\\mathsf{S1}$, $\\mathsf{S2}$, $\\mathsf{S3}$) are provided. For each scenario, the givens include:\n- The regularization parameter $t$.\n- The proximal center $x_k$.\n- A set of affine cuts defined by gradients $g_i$ and offsets $c_i$, including families of identical cuts.\n- The definitions for \"duplicates\" and \"unique\" sets of cuts.\n- The precise quantities to be computed: boolean comparisons $b_1, \\dots, b_6$ of solution vectors ($x^\\star$, $g_{\\text{agg}}$) and floating-point differences $d_1, d_2, d_3$ of the master problem's objective value.\n- An absolute tolerance of $10^{-9}$ for vector comparisons.\n\n**2. Validation using Extracted Givens:**\n- **Scientifically Grounded:** The problem is based on the standard formulation of the proximal bundle method, a well-established technique in nonsmooth convex optimization. The master problem is a convex Quadratic Program (QP), and its analysis uses fundamental principles of convex analysis and duality theory. All concepts are standard and factually correct.\n- **Well-Posed:** The master problem is a strictly convex QP (due to the term $\\frac{t}{2}\\lVert x - x_k\\rVert_2^2$ with $t0$), which guarantees a unique solution $(x^\\star, s^\\star)$. The problem is self-contained and provides all data necessary to find this solution.\n- **Objective:** The problem statement is formulated with precise, unambiguous mathematical language. The tasks are objective and verifiable through computation.\n\n**3. Verdict and Action:**\nThe problem is valid. It is scientifically sound, well-posed, and objective. A solution will be developed based on first principles as requested.\n\n### Step 2: Derivation and Solution\n\nThe core of the task is to solve the regularized master problem, which is a convex Quadratic Program (QP):\n$$\n\\min_{x \\in \\mathbb{R}^n, \\, s \\in \\mathbb{R}} \\; s + \\frac{t}{2}\\lVert x - x_k\\rVert_2^2 \\quad \\text{subject to} \\quad g_i^\\top x + c_i - s \\le 0 \\;\\; \\text{for all cuts}\\;\\; i=1, \\dots, m.\n$$\nTo solve this from first principles, we derive and solve its dual problem.\n\n**1. Derivation of the Dual Problem**\nLet $\\lambda_i \\ge 0$ be the Lagrange multiplier associated with the $i$-th constraint. The Lagrangian function is:\n$$\nL(x, s, \\lambda) = s + \\frac{t}{2}\\lVert x - x_k\\rVert_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x + c_i - s)\n$$\nThe dual function $q(\\lambda)$ is the infimum of the Lagrangian over the primal variables $x$ and $s$. We find this infimum by setting the partial derivatives of $L$ with respect to $x$ and $s$ to zero (stationarity conditions).\n\nDerivative with respect to $s$:\n$$\n\\frac{\\partial L}{\\partial s} = 1 - \\sum_{i=1}^m \\lambda_i = 0 \\implies \\sum_{i=1}^m \\lambda_i = 1\n$$\nThis condition reveals that the optimal multipliers $\\lambda_i^\\star$ must form a convex combination.\n\nDerivative with respect to $x$:\n$$\n\\nabla_x L = t(x - x_k) + \\sum_{i=1}^m \\lambda_i g_i = 0\n$$\nThis gives an expression for the optimal primal solution $x^\\star$ in terms of the optimal multipliers $\\lambda^\\star$:\n$$\nx^\\star = x_k - \\frac{1}{t} \\sum_{i=1}^m \\lambda_i g_i\n$$\nSubstituting these conditions back into the Lagrangian to eliminate $x$ and $s$:\n$$\n\\begin{align*}\nq(\\lambda) = s\\left(1 - \\sum_{i=1}^m \\lambda_i\\right) + \\frac{t}{2}\\lVert-\\frac{1}{t}\\sum_{i=1}^m \\lambda_i g_i\\rVert_2^2 + \\sum_{i=1}^m \\lambda_i c_i + \\sum_{i=1}^m \\lambda_i g_i^\\top \\left(x_k - \\frac{1}{t}\\sum_{j=1}^m \\lambda_j g_j\\right) \\\\\n= 0 + \\frac{1}{2t}\\lVert\\sum_{i=1}^m \\lambda_i g_i\\rVert_2^2 + \\sum_{i=1}^m \\lambda_i c_i + \\left(\\sum_{i=1}^m \\lambda_i g_i\\right)^\\top x_k - \\frac{1}{t}\\lVert\\sum_{i=1}^m \\lambda_i g_i\\rVert_2^2 \\\\\n= -\\frac{1}{2t}\\lVert\\sum_{i=1}^m \\lambda_i g_i\\rVert_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x_k + c_i)\n\\end{align*}\n$$\nThe dual problem is to maximize $q(\\lambda)$ subject to the constraints on $\\lambda$:\n$$\n\\max_{\\lambda \\in \\mathbb{R}^m} \\quad -\\frac{1}{2t}\\lVert\\sum_{i=1}^m \\lambda_i g_i\\rVert_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x_k + c_i) \\quad \\text{s.t.} \\quad \\sum_{i=1}^m \\lambda_i = 1, \\;\\; \\lambda_i \\ge 0.\n$$\nThis is equivalent to minimizing $-q(\\lambda)$, which is a convex QP in standard form. Let $G$ be the $n \\times m$ matrix whose columns are the subgradients $g_i$, and let $\\alpha$ be an $m$-vector where $\\alpha_i = g_i^\\top x_k + c_i$. The dual problem becomes:\n$$\n\\min_{\\lambda \\in \\mathbb{R}^m} \\quad \\frac{1}{2t}\\lVert G\\lambda\\rVert_2^2 - \\alpha^\\top \\lambda \\quad \\text{s.t.} \\quad \\mathbf{1}^\\top \\lambda = 1, \\;\\; \\lambda \\ge 0.\n$$\nThis can be written as $\\min_{\\lambda} \\frac{1}{2}\\lambda^\\top H \\lambda + f^\\top \\lambda$ where $H = \\frac{1}{t}G^\\top G$ and $f = -\\alpha$.\n\n**2. Recovery of Primal Solution and Invariance**\nOnce the optimal dual variables $\\lambda^\\star$ are found by solving this QP, we compute the required quantities:\n- **Aggregated Subgradient:** $g_{\\text{agg}} = \\sum_{i=1}^m \\lambda_i^\\star g_i = G\\lambda^\\star$.\n- **Proximal Center Update:** $x^\\star = x_k - \\frac{1}{t} g_{\\text{agg}}$.\n- **Epigraph Variable:** At the optimum, $s$ is forced to the upper envelope of the affine functions, so $s^\\star = \\max_i(g_i^\\top x^\\star + c_i)$.\n\nNow, consider the case of duplicate cuts. Let the set of unique cuts be indexed by $u \\in U$. Let $I_u$ be the set of indices in the original (duplicated) list corresponding to the unique cut $u$. Then for any $i \\in I_u$, we have $g_i = g_u$ and $c_i = c_u$, which implies $\\alpha_i = \\alpha_u$.\nLet $\\hat{\\lambda}_u = \\sum_{i \\in I_u} \\lambda_i$. The terms in the dual objective can be grouped:\n$$\n\\sum_{i=1}^m \\lambda_i g_i = \\sum_{u \\in U} \\sum_{i \\in I_u} \\lambda_i g_u = \\sum_{u \\in U} \\left(\\sum_{i \\in I_u} \\lambda_i\\right) g_u = \\sum_{u \\in U} \\hat{\\lambda}_u g_u\n$$\n$$\n\\sum_{i=1}^m \\lambda_i \\alpha_i = \\sum_{u \\in U} \\sum_{i \\in I_u} \\lambda_i \\alpha_u = \\sum_{u \\in U} \\left(\\sum_{i \\in I_u} \\lambda_i\\right) \\alpha_u = \\sum_{u \\in U} \\hat{\\lambda}_u \\alpha_u\n$$\nThe constraints on $\\lambda$ translate to $\\sum_{u \\in U} \\hat{\\lambda}_u = 1$ and $\\hat{\\lambda}_u \\ge 0$. The dual problem expressed in terms of $\\hat{\\lambda}_u$ is identical to the dual problem for the unique set of cuts.\nThis proves that the optimal value for $\\hat{\\lambda}_u^\\star$ is unique and independent of how many duplicates of cut $u$ are present. Consequently:\n- $g_{\\text{agg}} = \\sum_{u \\in U} \\hat{\\lambda}_u^\\star g_u$ is invariant.\n- $x^\\star = x_k - \\frac{1}{t} g_{\\text{agg}}$ is invariant.\n- $s^\\star = \\max_{u \\in U} (g_u^\\top x^\\star + c_u)$ is invariant.\n- The optimal objective value $s^\\star + \\frac{t}{2}\\lVert x^\\star - x_k\\rVert_2^2$ is invariant.\n\nTherefore, for each scenario, we expect $\\lVert x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\rVert_2$, $\\lVert g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\rVert_2$, and the difference in objective values to be zero, up to numerical precision.\n\n**3. Algorithmic Implementation**\nFor each scenario and for both the \"duplicates\" and \"unique\" cut sets, the following procedure is implemented:\n1. Construct the matrix $G$ and vector $\\alpha$.\n2. Formulate the dual QP objective function and constraints.\n3. Solve the dual QP for $\\lambda^\\star$ using a numerical optimization routine (`scipy.optimize.minimize`).\n4. Compute $g_{\\text{agg}}$, $x^\\star$, and $s^\\star$ from $\\lambda^\\star$.\n5. Calculate the primal objective value.\n6. Compare the results from the \"duplicates\" and \"unique\" runs to compute the required booleans and float differences.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_master_problem(cuts, t, x_k):\n    \"\"\"\n    Solves the regularized bundle method master problem via its dual.\n\n    Args:\n        cuts (list of tuples): A list where each tuple is (g, c)\n                               defining a cut l(x) = g.T @ x + c.\n        t (float): The regularization parameter.\n        x_k (np.ndarray): The current proximal center.\n\n    Returns:\n        tuple: A tuple containing (x_star, g_agg, s_star).\n    \"\"\"\n    m = len(cuts)\n    n = len(x_k)\n\n    if m == 0:\n        # No cuts, trivial solution.\n        g_agg = np.zeros(n)\n        x_star = x_k.copy()\n        # s is unconstrained from below, but in context of min s, it implies\n        # s would go to -inf. The problem context implies a non-empty bundle.\n        # However, the scenarios provided always have at least one cut.\n        # This case is for robustness and won't be hit.\n        s_star = -np.inf \n        return x_star, g_agg, s_star\n\n    G = np.array([cut[0] for cut in cuts]).T  # Shape (n, m)\n    c = np.array([cut[1] for cut in cuts])    # Shape (m,)\n    \n    alpha = G.T @ x_k + c\n\n    # Dual QP objective: min 0.5 * lambda.T @ H @ lambda + f.T @ lambda\n    # H = (1/t) * G.T @ G\n    H = (1.0 / t) * (G.T @ G)\n    # f = -alpha\n    f = -alpha\n\n    def dual_objective(lambda_vec):\n        return 0.5 * lambda_vec.T @ H @ lambda_vec + f.T @ lambda_vec\n\n    # Constraints: sum(lambda) = 1, lambda_i = 0\n    constraints = ({'type': 'eq', 'fun': lambda l: np.sum(l) - 1.0})\n    bounds = tuple((0, None) for _ in range(m))\n\n    # Initial guess for the optimizer\n    lambda0 = np.ones(m) / m\n\n    res = minimize(dual_objective, lambda0, method='SLSQP', bounds=bounds, constraints=constraints)\n    \n    lambda_star = res.x\n\n    # Numerical stability: project lambda_star onto the simplex\n    lambda_star[lambda_star  0] = 0\n    lambda_star /= np.sum(lambda_star)\n\n    # Recover primal solution\n    g_agg = G @ lambda_star\n    x_star = x_k - (1.0 / t) * g_agg\n    s_star = np.max(G.T @ x_star + c)\n    \n    return x_star, g_agg, s_star\n\ndef solve_scenario(t, x_k, cuts_dup, cuts_uniq):\n    \"\"\"\n    Solves a scenario for both duplicate and unique cuts and computes metrics.\n    \"\"\"\n    # Solve for duplicate cuts\n    x_star_dup, g_agg_dup, s_star_dup = solve_master_problem(cuts_dup, t, x_k)\n    obj_val_dup = s_star_dup + (t / 2.0) * np.linalg.norm(x_star_dup - x_k)**2\n\n    # Solve for unique cuts\n    x_star_uniq, g_agg_uniq, s_star_uniq = solve_master_problem(cuts_uniq, t, x_k)\n    obj_val_uniq = s_star_uniq + (t / 2.0) * np.linalg.norm(x_star_uniq - x_k)**2\n    \n    # Compute metrics\n    tol = 1e-9\n    b_x = np.linalg.norm(x_star_dup - x_star_uniq) = tol\n    b_g = np.linalg.norm(g_agg_dup - g_agg_uniq) = tol\n    d_obj = np.abs(obj_val_dup - obj_val_uniq)\n\n    return b_x, b_g, d_obj\n\ndef solve():\n    results = []\n\n    # Scenario S1\n    t1 = 2.0\n    x_k1 = np.array([1.3, -2.1])\n    g_A1, c_A1 = np.array([2.0, -1.0]), 0.0\n    g_B1, c_B1 = np.array([-1.0, 2.0]), -0.5\n    cuts_dup1 = [(g_A1, c_A1)] * 3 + [(g_B1, c_B1)]\n    cuts_uniq1 = [(g_A1, c_A1), (g_B1, c_B1)]\n    b1, b2, d1 = solve_scenario(t1, x_k1, cuts_dup1, cuts_uniq1)\n    results.extend([b1, b2, d1])\n\n    # Scenario S2\n    t2 = 1.5\n    x_k2 = np.array([-3.7, 4.2])\n    g_A2, c_A2 = np.array([2.0, -1.0]), 0.0\n    cuts_dup2 = [(g_A2, c_A2)] * 5\n    cuts_uniq2 = [(g_A2, c_A2)]\n    b3, b4, d2 = solve_scenario(t2, x_k2, cuts_dup2, cuts_uniq2)\n    results.extend([b3, b4, d2])\n    \n    # Scenario S3\n    t3 = 1000.0\n    x_k3 = np.array([10.0, -10.0])\n    g_A3, c_A3 = np.array([2.0, -1.0]), 0.0\n    g_B3, c_B3 = np.array([-1.0, 2.0]), -0.5\n    cuts_dup3 = [(g_A3, c_A3)] * 4 + [(g_B3, c_B3)] * 4\n    cuts_uniq3 = [(g_A3, c_A3), (g_B3, c_B3)]\n    b5, b6, d3 = solve_scenario(t3, x_k3, cuts_dup3, cuts_uniq3)\n    results.extend([b5, b6, d3])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105175"}]}