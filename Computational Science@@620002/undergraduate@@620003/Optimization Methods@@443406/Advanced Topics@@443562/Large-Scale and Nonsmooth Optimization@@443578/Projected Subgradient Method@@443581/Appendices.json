{"hands_on_practices": [{"introduction": "This first exercise grounds the abstract theory of the Projected Subgradient Method in a tangible coding task. We will minimize the common $\\ell_\\infty$-norm objective function subject to a simple box constraint, for which the projection is an intuitive coordinate-wise clipping operation. By measuring metrics like the clipping proportion and the alignment between the intended and actual update steps [@problem_id:3165045], you will develop a quantitative understanding of how projection actively shapes the optimization trajectory.", "problem": "You are asked to design and implement a program that evaluates the use of coordinate-wise clipping as the projection onto a box constraint in the Projected Subgradient Method for a convex optimization problem. The objective is to analyze how frequently the projection step alters the intended update direction when optimizing the function and to examine how such alterations may introduce bias in the effective descent direction. The analysis should be grounded in first principles of convex optimization and should be empirically substantiated by running the method on several test cases of varying box sizes.\n\nStart from the following foundational definitions and facts:\n- A function $f:\\mathbb{R}^n \\to \\mathbb{R}$ is convex if for all $x,y \\in \\mathbb{R}^n$ and for all $\\lambda \\in [0,1]$, one has $f(\\lambda x + (1-\\lambda)y) \\le \\lambda f(x) + (1-\\lambda) f(y)$.\n- The subgradient of a convex function $f$ at a point $x$, denoted $\\partial f(x)$, is the set of all vectors $g \\in \\mathbb{R}^n$ such that $f(y) \\ge f(x) + g^\\top (y-x)$ for all $y \\in \\mathbb{R}^n$. Any element $g \\in \\partial f(x)$ is called a subgradient.\n- The projection of a point $y \\in \\mathbb{R}^n$ onto a nonempty closed convex set $C \\subset \\mathbb{R}^n$, denoted $\\Pi_C(y)$, is the unique point $z \\in C$ minimizing the squared Euclidean distance $\\|z - y\\|_2^2$. For a box constraint $C = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$ with lower bound $\\ell \\in \\mathbb{R}^n$ and upper bound $u \\in \\mathbb{R}^n$ defined coordinate-wise, the projection $\\Pi_C(y)$ is equal to coordinate-wise clipping:\n$$\n\\bigl(\\Pi_C(y)\\bigr)_i = \\min\\bigl\\{\\max\\{y_i, \\ell_i\\}, u_i\\bigr\\} \\quad \\text{for all } i \\in \\{1,\\dots,n\\}.\n$$\n- The Projected Subgradient Method updates iterates via\n$$\nx_{k+1} = \\Pi_C\\bigl(x_k - \\alpha_k g_k\\bigr),\n$$\nwhere $g_k \\in \\partial f(x_k)$ is a subgradient and $\\alpha_k  0$ is a step size.\n\nConsider the convex objective function\n$$\nf(x) = \\|A x - b\\|_\\infty = \\max_{i \\in \\{1,\\dots,m\\}} \\bigl| (A x - b)_i \\bigr|,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$. Your implementation must, from first principles, compute a valid subgradient $g_k \\in \\partial f(x_k)$ at each iteration $k$ and apply the coordinate-wise clipping projection $\\Pi_C$ onto the box $C$.\n\nAlgorithmic requirements:\n1. Initialize $x_0 \\in C$.\n2. At each iteration $k \\in \\{0,1,\\dots,T-1\\}$, compute a subgradient $g_k \\in \\partial f(x_k)$ for the function $f(x) = \\|A x - b\\|_\\infty$ using only the definitions above and elementary properties of subgradients of pointwise maxima and the absolute value function.\n3. Choose the step size sequence $\\alpha_k = \\alpha_0/\\sqrt{k+1}$, where $\\alpha_0  0$ is a constant.\n4. Update $x_{k+1} = \\Pi_C(x_k - \\alpha_k g_k)$ using coordinate-wise clipping onto the box $C$.\n5. Collect the following quantitative metrics across the iterations:\n   - The final objective value $f(x_T)$ after $T$ iterations.\n   - The average per-iteration clipping proportion, defined as the mean over $k$ of the proportion of coordinates for which $(\\Pi_C(x_k - \\alpha_k g_k))_i \\ne (x_k - \\alpha_k g_k)_i$.\n   - The average cosine similarity between the intended step direction $-g_k$ and the actual displacement $d_k = x_{k+1} - x_k$, defined for iterations where both $\\|g_k\\|_2$ and $\\|d_k\\|_2$ are nonzero by\n   $$\n   \\cos_k = \\frac{(-g_k)^\\top d_k}{\\|g_k\\|_2 \\, \\|d_k\\|_2}.\n   $$\n   The average should be taken over iterations with defined $\\cos_k$.\nThese metrics quantify the impact of clipping on the effective update direction and its potential bias.\n\nImplementation constraints:\n- Use $m = 60$ and $n = 25$.\n- Construct $A \\in \\mathbb{R}^{60 \\times 25}$ and $b \\in \\mathbb{R}^{60}$ deterministically using a pseudorandom number generator with a fixed seed $0$, with entries of $A$ drawn independently from a normal distribution with mean $0$ and variance scaled by $1/n$, and entries of $b$ drawn independently from a standard normal distribution with mean $0$ and variance $1$. Explicitly, set $A_{ij} \\sim \\mathcal{N}(0,1)/\\sqrt{n}$ and $b_i \\sim \\mathcal{N}(0,1)$ with the fixed seed $0$.\n- The initial point $x_0$ must be chosen as the midpoint of the box, i.e., $x_0 = (\\ell + u)/2$.\n\nTest suite:\nRun your program on the following four test cases to explore a range of clipping regimes. In each case, use $T = 200$ iterations, and report the metrics listed above as floats.\n- Case $1$: Box $C = [-5,5]^n$ with $\\ell_i = -5$ and $u_i = 5$ for all $i$, and $\\alpha_0 = 1.0$.\n- Case $2$: Box $C = [-0.05,0.05]^n$ with $\\ell_i = -0.05$ and $u_i = 0.05$ for all $i$, and $\\alpha_0 = 1.0$.\n- Case $3$: Box $C$ anisotropic with $\\ell_i = -0.02$ and $u_i = 0.02$ for $i \\in \\{1,\\dots,10\\}$, and $\\ell_i = -0.2$ and $u_i = 0.2$ for $i \\in \\{11,\\dots,25\\}$, and $\\alpha_0 = 1.0$.\n- Case $4$: Box $C = [-0.05,0.05]^n$ with $\\ell_i = -0.05$ and $u_i = 0.05$ for all $i$, and a smaller step scale $\\alpha_0 = 0.1$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to a test case and containing the three floats in the order $\\bigl[f(x_T), \\text{average clipping proportion}, \\text{average cosine similarity}\\bigr]$. Values must be printed in fixed-point decimal format. For example:\n$$\n\\bigl[ [v_{1,1}, v_{1,2}, v_{1,3}], [v_{2,1}, v_{2,2}, v_{2,3}], [v_{3,1}, v_{3,2}, v_{3,3}], [v_{4,1}, v_{4,2}, v_{4,3}] \\bigr].\n$$\nNo physical units are involved. Angles are not directly reported; only the cosine similarity is reported, which is dimensionless. The program must hard-code the test cases above and produce the final output in the exact required format as a single line.", "solution": "The problem requires an analysis of the Projected Subgradient Method for minimizing a convex function subject to box constraints. The analysis focuses on quantifying the effect of the projection step on the algorithm's trajectory. The problem is well-posed, scientifically grounded in the principles of convex optimization, and all parameters are clearly specified. We shall therefore proceed with a complete solution.\n\nThe optimization problem is defined as:\n$$\n\\min_{x \\in C} f(x)\n$$\nwhere the objective function is $f(x) = \\|A x - b\\|_\\infty$ for $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, and the constraint set is a box $C = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$. The vectors $\\ell, u \\in \\mathbb{R}^n$ define the lower and upper bounds for each coordinate of $x$, respectively.\n\n**1. The Projected Subgradient Method**\n\nThe algorithm generates a sequence of iterates $\\{x_k\\}$ via the update rule:\n$$\nx_{k+1} = \\Pi_C(x_k - \\alpha_k g_k)\n$$\nHere, $x_k$ is the current estimate, $g_k$ is a subgradient of $f$ at $x_k$, $\\alpha_k  0$ is the step size, and $\\Pi_C$ is the Euclidean projection onto the set $C$.\n\n**2. Subgradient of the Objective Function**\n\nThe objective function is $f(x) = \\max_{i \\in \\{1,\\dots,m\\}} |(A x - b)_i|$. Let $a_i^\\top$ be the $i$-th row of matrix $A$. The function can be written as a pointwise maximum of $m$ convex functions, $h_i(x) = |a_i^\\top x - b_i|$. \nThe subdifferential of a pointwise maximum function $f(x) = \\max_{i} h_i(x)$ is the convex hull of the union of subdifferentials of the \"active\" functions, i.e., those for which $h_i(x)=f(x)$. Let $I(x) = \\{i \\mid |a_i^\\top x - b_i| = \\|Ax - b\\|_\\infty\\}$ be the set of active indices at $x$. Then,\n$$\n\\partial f(x) = \\text{conv} \\left( \\bigcup_{i \\in I(x)} \\partial h_i(x) \\right)\n$$\nTo find a single subgradient $g_k \\in \\partial f(x_k)$, we can select any active index $j_k \\in I(x_k)$ and choose any subgradient from $\\partial h_{j_k}(x_k)$. A convenient choice for the active index is $j_k = \\text{argmax}_{i \\in \\{1, \\dots, m\\}} |a_i^\\top x_k - b_i|$.\nThe function $h_{j_k}(x) = |a_{j_k}^\\top x - b_{j_k}|$ is a composition of the absolute value function and an affine function. By the chain rule for subdifferentials, a subgradient of $h_{j_k}$ at $x_k$ is given by $s \\cdot \\nabla_x(a_{j_k}^\\top x - b_{j_k}) = s \\cdot a_{j_k}$, where $s \\in \\partial |\\cdot|$ evaluated at $z = a_{j_k}^\\top x_k - b_{j_k}$. The subdifferential of the absolute value function is $\\partial|z| = \\{\\text{sign}(z)\\}$ for $z \\neq 0$ and $\\partial|z| = [-1, 1]$ for $z = 0$.\nA valid and computationally simple choice for the subgradient $g_k$ is thus:\n$$\ng_k = \\text{sign}(a_{j_k}^\\top x_k - b_{j_k}) a_{j_k}\n$$\nwhere $j_k$ is an index for which the maximum in $\\|Ax_k - b\\|_\\infty$ is achieved. We will use the standard definition where $\\text{sign}(0) = 0$, which corresponds to choosing $0$ from the interval $[-1, 1]$, a valid subgradient choice.\n\n**3. Projection onto a Box Constraint**\n\nThe projection of a point $y \\in \\mathbb{R}^n$ onto the box $C = [\\ell, u]$ is separable and can be computed coordinate-wise. For each coordinate $i=1,\\dots,n$, the projection is:\n$$\n(\\Pi_C(y))_i = \\min\\{\\max\\{y_i, \\ell_i\\}, u_i\\}\n$$\nThis operation is also known as clipping.\n\n**4. Algorithmic Implementation and Analysis**\n\nThe full algorithm proceeds as follows for $k=0, 1, \\dots, T-1$:\n- Initialize $x_0 = (\\ell + u)/2$.\n- For each iteration $k$:\n    1. Compute the residual $r_k = A x_k - b$.\n    2. Find the active index $j_k = \\text{argmax}_{i} |r_{k,i}|$.\n    3. Compute the subgradient $g_k = \\text{sign}(r_{k, j_k}) a_{j_k}^\\top$.\n    4. Set the step size $\\alpha_k = \\alpha_0 / \\sqrt{k+1}$.\n    5. Perform the tentative update: $y_k = x_k - \\alpha_k g_k$.\n    6. Project back to the feasible set: $x_{k+1} = \\Pi_C(y_k)$.\n\nTo analyze the impact of the projection, we collect three metrics:\n1.  **Final objective value $f(x_T)$**: Measures the quality of the solution after $T$ iterations.\n2.  **Average clipping proportion**: For each iteration $k$, the proportion of coordinates that are altered by the projection is $p_k = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left((x_{k+1})_i \\neq (y_k)_i\\right)$, where $\\mathbf{1}$ is the indicator function. The average $\\frac{1}{T}\\sum_{k=0}^{T-1} p_k$ quantifies how frequently the projection is active.\n3.  **Average cosine similarity**: The intended update direction is $s_k = -g_k$. The actual displacement is $d_k = x_{k+1} - x_k$. The cosine similarity $\\cos(\\theta_k) = \\frac{s_k^\\top d_k}{\\|s_k\\|_2 \\|d_k\\|_2}$ measures the alignment between these two vectors. A value of $1$ signifies perfect alignment (no effective change in direction by projection), while values less than $1$ indicate that the projection has deflected the step. A negative value would imply the step was taken in a direction opposite to the descent direction, which is possible for non-acute angles. The average is taken over all iterations where both $s_k$ and $d_k$ are non-zero.\n\nThe program will be executed with given parameters $n=25$, $m=60$, $T=200$, and deterministically generated data for $A$ and $b$. Four distinct test cases varying the box $C$ and the initial step size scale $\\alpha_0$ will be evaluated to observe the behavior under different levels of constraint activity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_psm(A, b, ell, u, alpha_0, T):\n    \"\"\"\n    Runs the Projected Subgradient Method for T iterations.\n    \n    Args:\n        A (np.ndarray): The matrix A in the objective function.\n        b (np.ndarray): The vector b in the objective function.\n        ell (np.ndarray): The lower bounds of the box constraint.\n        u (np.ndarray): The upper bounds of the box constraint.\n        alpha_0 (float): The initial step size scale.\n        T (int): The number of iterations.\n        \n    Returns:\n        list: A list containing [final_objective, avg_clipping_prop, avg_cos_sim].\n    \"\"\"\n    m, n = A.shape\n    x = (ell + u) / 2.0\n    \n    clipping_proportions = []\n    cos_similarities = []\n\n    for k in range(T):\n        # 1. Compute a subgradient g_k\n        residual = A @ x - b\n        j_k = np.argmax(np.abs(residual))\n        \n        s_jk = np.sign(residual[j_k])\n        g_k = s_jk * A[j_k, :]\n        \n        # 2. Compute step size alpha_k\n        alpha_k = alpha_0 / np.sqrt(k + 1)\n        \n        # 3. Tentative update y_k\n        y_k = x - alpha_k * g_k\n        \n        # 4. Projection to get x_{k+1}\n        x_next = np.clip(y_k, ell, u)\n        \n        # 5. Collect metrics for analysis\n        # Clipping proportion\n        num_clipped = np.sum(x_next != y_k)\n        clipping_proportions.append(num_clipped / n)\n        \n        # Cosine similarity\n        d_k = x_next - x\n        \n        norm_g = np.linalg.norm(g_k)\n        norm_d = np.linalg.norm(d_k)\n        \n        # Average over iterations where both norms are non-zero.\n        if norm_g  1e-12 and norm_d  1e-12:\n            # Intended step direction is -g_k\n            cos_sim = (-g_k @ d_k) / (norm_g * norm_d)\n            cos_similarities.append(cos_sim)\n            \n        x = x_next\n    \n    # After T iterations, x is x_T\n    final_obj = np.max(np.abs(A @ x - b))\n    \n    avg_clip_prop = np.mean(clipping_proportions) if clipping_proportions else 0.0\n    avg_cos_sim = np.mean(cos_similarities) if cos_similarities else 0.0\n    \n    # The problem asks for floats to be reported.\n    return [float(final_obj), float(avg_clip_prop), float(avg_cos_sim)]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the optimizer, and print results.\n    \"\"\"\n    # Define problem dimensions and seed for deterministic data generation\n    m, n = 60, 25\n    seed = 0\n    T = 200\n\n    # Generate problem data (A, b)\n    rng = np.random.default_rng(seed)\n    A = rng.normal(loc=0.0, scale=1.0, size=(m, n)) / np.sqrt(n)\n    b = rng.normal(loc=0.0, scale=1.0, size=m)\n    \n    # Define the test suite\n    test_cases = [\n        # Case 1: Wide box, standard step size scale\n        {'ell': np.full(n, -5.0), 'u': np.full(n, 5.0), 'alpha_0': 1.0},\n        # Case 2: Narrow box, standard step size scale\n        {'ell': np.full(n, -0.05), 'u': np.full(n, 0.05), 'alpha_0': 1.0},\n        # Case 3: Anisotropic box, standard step size scale\n        {'ell': np.concatenate([np.full(10, -0.02), np.full(n-10, -0.2)]),\n         'u': np.concatenate([np.full(10, 0.02), np.full(n-10, 0.2)]),\n         'alpha_0': 1.0},\n        # Case 4: Narrow box, small step size scale\n        {'ell': np.full(n, -0.05), 'u': np.full(n, 0.05), 'alpha_0': 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_psm(A, b, case['ell'], case['u'], case['alpha_0'], T)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The problem description contains an ambiguity between the textual requirement for\n    # \"fixed-point decimal format\" and the provided generic print statement template.\n    # This implementation adheres strictly to the print statement template provided in\n    # the OUTPUT STRUCTURE section, which relies on the default string representation\n    # of Python lists and floats.\n    print(str(results).replace(\"], [\", \"],[\"))\n    \nsolve()\n```", "id": "3165045"}, {"introduction": "Having established the basic mechanics of projection, we now turn to one of the most critical aspects of the algorithm's performance: the step-size sequence. This practice [@problem_id:3165079] challenges you to implement the projection onto a different, but equally important, convex set—the probability simplex. The core task is to empirically compare the convergence rates of four distinct step-size strategies, from a simple constant step to the theoretically-informed Polyak step-size, providing valuable intuition on how tuning the step-size $\\alpha_k$ can dramatically impact efficiency.", "problem": "Consider the convex optimization problem of minimizing the function $f(x)=\\lVert x\\rVert_{\\infty}$ over the probability simplex $C=\\{x \\in \\mathbb{R}^n \\mid x_i\\ge 0 \\text{ for all } i, \\text{ and } \\sum_{i=1}^n x_i=1\\}$. Begin only from the foundational definitions of convexity, subgradient, norm, and Euclidean projection. Specifically, use the definition of the infinity norm $f(x)=\\max_{i\\in\\{1,\\dots,n\\}} x_i$, the definition of a subgradient for a convex function at a point as any vector $g$ satisfying $f(y)\\ge f(x)+g^{\\top}(y-x)$ for all $y$, and the definition of the Euclidean projection of a point onto a closed convex set as the unique point in the set minimizing Euclidean distance.\n\nTasks:\n1. Derive an explicit minimizer $x^\\star$ of $f(x)$ over $C$, and compute the corresponding optimal value $f^\\star=f(x^\\star)$. Your derivation must start from the above fundamental definitions and must be valid for general $n\\in\\mathbb{N}$.\n2. For the Projected Subgradient Method (PSGM), defined by the iteration\n$$\nx^{k+1}=\\Pi_C\\!\\left(x^k - t_k\\,g^k\\right),\n$$\nwhere $\\Pi_C$ denotes the Euclidean projection onto $C$, $g^k\\in\\partial f(x^k)$ is a subgradient of $f$ at $x^k$, and $\\{t_k\\}_{k\\ge 0}$ is a positive step-size sequence, implement a program that:\n   - Initializes at $x^0=e_1$ (the first standard basis vector in $\\mathbb{R}^n$).\n   - At each iteration chooses a subgradient $g^k$ as the standard basis vector corresponding to the current index of the maximum component of $x^k$ (break ties by choosing the smallest index).\n   - Applies the Euclidean projection onto the simplex $C$.\n   - Stops at the first iteration index $k$ such that $f(x^k)-f^\\star\\le \\varepsilon$, where $\\varepsilon0$ is given per test case.\n   - If no such $k$ is found within a fixed maximum number of iterations $N_{\\max}$, returns $-1$ for that test case.\n\nYou must compare the number of iterations needed to achieve $f(x^k)-f^\\star\\le \\varepsilon$ under four different step-size sequences $\\{t_k\\}$:\n- Constant step-size: $t_k=c$ for a given $c0$.\n- Diminishing square-root step-size: $t_k=\\alpha/\\sqrt{k+1}$ for a given $\\alpha0$.\n- Harmonic step-size: $t_k=\\beta/(k+1)$ for a given $\\beta0$.\n- Polyak step-size (using the known optimal value): $t_k=\\dfrac{f(x^k)-f^\\star}{\\lVert g^k\\rVert_2^2}$.\n\nUse the following test suite, with all steps executed using the above PSGM and the specified parameters:\n- Test case $1$: $n=5$, $\\varepsilon=10^{-3}$, constant $t_k$ with $c=0.1$, $N_{\\max}=20000$.\n- Test case $2$: $n=5$, $\\varepsilon=10^{-3}$, constant $t_k$ with $c=0.5$, $N_{\\max}=20000$.\n- Test case $3$: $n=10$, $\\varepsilon=10^{-4}$, diminishing square-root $t_k$ with $\\alpha=0.3$, $N_{\\max}=20000$.\n- Test case $4$: $n=2$, $\\varepsilon=10^{-6}$, harmonic $t_k$ with $\\beta=0.7$, $N_{\\max}=50000$.\n- Test case $5$: $n=7$, $\\varepsilon=0$, Polyak step-size as defined above, $N_{\\max}=10000$.\n\nFor each test case, the program must output the number of iterations needed to achieve $f(x^k)-f^\\star\\le \\varepsilon$, or $-1$ if this criterion is not met within $N_{\\max}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the integer number of iterations for the corresponding test case. No additional text should be printed.", "solution": "The problem statement is evaluated as valid. It is a well-posed problem in convex optimization, grounded in established mathematical principles. The definitions, conditions, and test cases are complete, consistent, and formalizable.\n\nThe solution proceeds in two parts as requested. First, the analytical derivation of the minimizer and optimal value. Second, a detailed description of the components required for the implementation of the Projected Subgradient Method (PSGM).\n\n### Part 1: Analytical Solution of the Optimization Problem\n\nThe optimization problem is to minimize the function $f(x)=\\lVert x\\rVert_{\\infty}$ subject to the constraint that $x$ lies in the probability simplex in $\\mathbb{R}^n$. This can be written as:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\lVert x \\rVert_\\infty \\quad \\text{subject to} \\quad x \\in C\n$$\nwhere the constraint set is the probability simplex $C = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, x_i \\ge 0 \\text{ for } i=1, \\dots, n\\}$.\n\nThe function to be minimized is $f(x) = \\lVert x\\rVert_{\\infty}$. Since all components $x_i$ of a vector $x \\in C$ are non-negative, the infinity norm simplifies to $f(x) = \\max_{i \\in \\{1, \\dots, n\\}} x_i$.\n\nLet $\\mu = f(x) = \\max_{i} x_i$. By definition of the maximum, for any $x \\in C$, we have $x_i \\le \\mu$ for all $i = 1, \\dots, n$. Summing this inequality over all $i$:\n$$\n\\sum_{i=1}^n x_i \\le \\sum_{i=1}^n \\mu = n\\mu\n$$\nThe simplex constraint requires that $\\sum_{i=1}^n x_i = 1$. Substituting this into the inequality gives:\n$$\n1 \\le n\\mu\n$$\nThis implies that for any feasible point $x \\in C$, the value of the objective function, $\\mu = f(x)$, must satisfy:\n$$\nf(x) \\ge \\frac{1}{n}\n$$\nThis establishes a lower bound of $1/n$ for the optimal value of the problem.\n\nTo show that this lower bound is the minimum value, we must demonstrate that there exists a point $x^\\star \\in C$ for which $f(x^\\star) = 1/n$. Consider the uniform vector $x^\\star$ defined by:\n$$\nx^\\star = \\left(\\frac{1}{n}, \\frac{1}{n}, \\dots, \\frac{1}{n}\\right)^{\\top}\n$$\nWe verify if this point belongs to the simplex $C$:\n1.  For each component $i$, $x^\\star_i = 1/n$. Since $n \\in \\mathbb{N}$, we have $n \\ge 1$, so $x^\\star_i  0$. The non-negativity constraint $x_i \\ge 0$ is satisfied.\n2.  The sum of the components is $\\sum_{i=1}^n x^\\star_i = \\sum_{i=1}^n \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1$. The summation constraint is satisfied.\n\nThus, $x^\\star$ is a point in the feasible set $C$. The value of the objective function at this point is:\n$$\nf^\\star = f(x^\\star) = \\max_{i \\in \\{1, \\dots, n\\}} \\left(\\frac{1}{n}\\right) = \\frac{1}{n}\n$$\nSince we found a feasible point $x^\\star$ that achieves the lower bound $1/n$, this point is a minimizer of $f(x)$ over $C$, and the optimal value is $f^\\star = 1/n$. Furthermore, this minimizer is unique. If another point $y \\in C$ existed with $f(y) = 1/n$, it would mean $\\max_i y_i = 1/n$. If any component $y_j$ were less than $1/n$, then to satisfy $\\sum_i y_i = 1$, at least one other component $y_k$ would have to be greater than $1/n$, contradicting $\\max_i y_i = 1/n$. Therefore, all components of any minimizer must be exactly $1/n$, proving uniqueness.\n\nIn summary, the unique minimizer is $x^\\star = (1/n, \\dots, 1/n)^{\\top}$ and the optimal value is $f^\\star = 1/n$.\n\n### Part 2: Design of the Projected Subgradient Method\n\nThe Projected Subgradient Method (PSGM) is an iterative algorithm for constrained convex optimization. The iteration is given by:\n$$\nx^{k+1} = \\Pi_C(x^k - t_k g^k)\n$$\nwhere $x^k$ is the iterate at step $k$, $g^k$ is a subgradient of $f$ at $x^k$, $t_k  0$ is the step-size, and $\\Pi_C$ is the Euclidean projection onto the set $C$.\n\n**Subgradient of $f(x) = \\lVert x \\rVert_\\infty$**\nFor a general convex function $\\phi$, a vector $g$ is a subgradient at a point $x$ if $\\phi(y) \\ge \\phi(x) + g^{\\top}(y-x)$ for all $y$. The set of all subgradients at $x$ is called the subdifferential, denoted $\\partial \\phi(x)$. For $f(x) = \\lVert x \\rVert_\\infty$, the subdifferential at $x$ is the convex hull of the vectors $e_i \\cdot \\text{sign}(x_i)$ for all indices $i$ corresponding to the maximum absolute value components. Since we operate on the simplex $C$, all $x_i \\ge 0$, so $\\lVert x \\rVert_\\infty = \\max_i x_i$ and $\\text{sign}(x_i)$ is non-negative. The subdifferential simplifies to:\n$$\n\\partial f(x) = \\text{conv}\\{e_i \\mid i \\in I(x)\\}\n$$\nwhere $I(x) = \\{i \\mid x_i = \\max_{j=1,\\dots,n} x_j\\}$ and $e_i$ is the $i$-th standard basis vector. The problem specifies a deterministic rule for choosing one subgradient $g^k \\in \\partial f(x^k)$: let $j$ be the smallest index in $I(x^k)$, and set $g^k = e_j$. This is a valid subgradient as it is one of the vertices of the convex hull defining the subdifferential.\n\n**Euclidean Projection onto the Probability Simplex $(\\Pi_C)$**\nThe projection of a point $z \\in \\mathbb{R}^n$ onto $C$ is the unique solution to the quadratic program:\n$$\n\\Pi_C(z) = \\arg\\min_{x \\in C} \\frac{1}{2} \\lVert x - z \\rVert_2^2\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions for this problem lead to a solution of the form $x_i = \\max(0, z_i - \\theta)$ for all $i$. The parameter $\\theta$ is a Lagrange multiplier associated with the equality constraint $\\sum_i x_i = 1$, and must be chosen to satisfy this constraint. This means $\\theta$ is the root of the equation $\\sum_{i=1}^n \\max(0, z_i - \\theta) = 1$. This can be solved efficiently. First, sort the components of $z$ in descending order: $u_1 \\ge u_2 \\ge \\dots \\ge u_n$. The optimal $\\theta$ is found by identifying the number of components $\\rho$ that will remain positive after projection. The value $\\rho$ is the largest integer $j$ such that $u_j - \\frac{1}{j}(\\sum_{i=1}^j u_i - 1)  0$. Once $\\rho$ is found, $\\theta$ is computed as $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^\\rho u_i - 1)$. The projected vector $x$ is then given by $x_i = \\max(0, z_i - \\theta)$.\n\n**Step-Size Rules**\nThe algorithm compares four step-size sequences $\\{t_k\\}_{k\\ge0}$:\n1.  **Constant:** $t_k = c$, for a given constant $c  0$.\n2.  **Diminishing Square-Root:** $t_k = \\alpha/\\sqrt{k+1}$, for a given $\\alpha  0$.\n3.  **Harmonic:** $t_k = \\beta/(k+1)$, for a given $\\beta  0$.\n4.  **Polyak:** $t_k = \\frac{f(x^k) - f^\\star}{\\lVert g^k \\rVert_2^2}$. Since our choice of subgradient is $g^k=e_j$, its squared Euclidean norm is $\\lVert g^k \\rVert_2^2 = \\lVert e_j \\rVert_2^2 = 1$. The rule simplifies to $t_k = f(x^k) - f^\\star = \\max_i x^k_i - 1/n$.\n\n**Algorithm Summary**\nThe overall algorithm is as follows:\n1.  **Initialization:** For a given test case ($n$, $\\varepsilon$, $N_{\\max}$, step-rule), set $f^\\star = 1/n$. Initialize the iteration counter $k = 0$ and the vector $x^0 = e_1 = (1, 0, \\dots, 0)^{\\top}$.\n2.  **Iteration Loop:** For $k = 0, 1, 2, \\dots, N_{\\max}$:\n    a. Calculate the current objective value $f(x^k) = \\max_i x^k_i$.\n    b. Check the stopping criterion: If $f(x^k) - f^\\star \\le \\varepsilon$, terminate and return the current iteration count $k$.\n    c. If $k=N_{\\max}$, the maximum number of iterations has been reached without convergence; break the loop.\n    d. Select the subgradient $g^k = e_j$, where $j = \\arg\\max_i x^k_i$ (with ties broken by the smallest index).\n    e. Compute the step-size $t_k$ using the specified rule for the test case.\n    f. Perform the update step: $z^k = x^k - t_k g^k$.\n    g. Project back to the simplex to get the next iterate: $x^{k+1} = \\Pi_C(z^k)$.\n3.  **Termination:** If the loop completes without the stopping criterion being met, return $-1$.\n\nThis structured approach will be implemented to solve the given test cases. The test case with $\\varepsilon = 0$ requires the iterates to exactly reach the optimal value, which may not be possible in a finite number of steps with floating-point arithmetic. In such a scenario, the algorithm is expected to run for the maximum number of iterations and return $-1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_simplex(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Projects a vector z onto the probability simplex using an efficient algorithm.\n\n    The probability simplex is the set C = {x | sum(x) = 1, x_i = 0}.\n    The projection is the solution to argmin_{x in C} ||x - z||_2.\n\n    Args:\n        z: A numpy array representing the vector to be projected.\n\n    Returns:\n        A numpy array representing the projected vector.\n    \"\"\"\n    n = len(z)\n    # The algorithm requires sorting the vector z in descending order.\n    u = np.sort(z)[::-1]\n    \n    # Compute the cumulative sum of the sorted vector.\n    cssv = np.cumsum(u)\n    \n    # Find rho, the number of positive components in the projected vector.\n    # This is done by finding the largest j such that u_j - (1/j)(sum_{i=1 to j} u_i - 1)  0.\n    # The condition is vectorized for efficiency.\n    indices = np.arange(1, n + 1)\n    condition = u - (cssv - 1) / indices  0\n    \n    # np.where returns non-zero indices. rho is the 1-based index.\n    rho_idx = np.where(condition)[0][-1]\n    rho = rho_idx + 1\n\n    # Compute the thresholding parameter theta.\n    theta = (cssv[rho_idx] - 1) / rho\n    \n    # The projection is obtained by shifting z by theta and taking the positive part.\n    x = np.maximum(0, z - theta)\n    \n    return x\n\ndef psgm_solver(n: int, epsilon: float, N_max: int, step_rule: str, params: dict) - int:\n    \"\"\"\n    Solves the optimization problem using the Projected Subgradient Method.\n    \n    Minimizes f(x) = ||x||_inf over the probability simplex.\n\n    Args:\n        n: Dimension of the space.\n        epsilon: Tolerance for the stopping criterion f(x^k) - f* = epsilon.\n        N_max: Maximum number of iterations.\n        step_rule: The name of the step-size rule to use.\n        params: A dictionary containing parameters for the step-size rule.\n\n    Returns:\n        The number of iterations k to reach the stopping criterion, or -1 if not met.\n    \"\"\"\n    f_star = 1.0 / n\n    x = np.zeros(n)\n    x[0] = 1.0  # Initialize at x^0 = e_1\n\n    for k in range(N_max + 1):\n        # We are at iteration k, with iterate x (representing x^k).\n        f_xk = np.max(x)\n\n        # Check stopping criterion.\n        if f_xk - f_star = epsilon:\n            return k\n\n        # If k has reached N_max, we've checked x^N_max and failed. Stop.\n        if k == N_max:\n            break\n\n        # --- Prepare for next iteration k+1 ---\n        \n        # 1. Subgradient g^k at x^k.\n        # np.argmax breaks ties by returning the first (smallest) index.\n        j = np.argmax(x)\n        g = np.zeros(n)\n        g[j] = 1.0\n\n        # 2. Step size t_k.\n        tk = 0.0\n        if step_rule == 'constant':\n            tk = params['c']\n        elif step_rule == 'sqrt':\n            tk = params['alpha'] / np.sqrt(k + 1)\n        elif step_rule == 'harmonic':\n            tk = params['beta'] / (k + 1)\n        elif step_rule == 'polyak':\n            # ||g^k||_2^2 is 1 since g^k is a standard basis vector.\n            tk = f_xk - f_star\n        \n        # 3. Update from x^k to x^{k+1}.\n        z = x - tk * g  # Subgradient descent step\n        x = project_simplex(z)  # Projection step\n\n    return -1\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, then prints the results.\n    \"\"\"\n    # (n, epsilon, N_max, step_rule, {params})\n    test_cases = [\n        (5, 1e-3, 20000, 'constant', {'c': 0.1}),\n        (5, 1e-3, 20000, 'constant', {'c': 0.5}),\n        (10, 1e-4, 20000, 'sqrt', {'alpha': 0.3}),\n        (2, 1e-6, 50000, 'harmonic', {'beta': 0.7}),\n        (7, 0.0, 10000, 'polyak', {}),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, epsilon, N_max, rule, params = case\n        result = psgm_solver(n, epsilon, N_max, rule, params)\n        results.append(result)\n\n    # Format and print the final output as a single line.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3165079"}, {"introduction": "A crucial skill for any practitioner is the ability to diagnose and fix an optimization algorithm that isn't converging. A common failure mode for the subgradient method occurs when a constant step-size is chosen too large, leading to oscillations that prevent convergence. This exercise [@problem_id:3164985] allows you to observe this instability firsthand and quantify it with a \"toggle rate\" metric, before implementing two powerful and widely-used remedies: iterate averaging and diminishing step-sizes.", "problem": "Consider the convex, nondifferentiable objective function $f(x) = \\max_{i \\in \\{1,\\dots,m\\}} a_i^\\top x$ over the Euclidean ball constraint set $B_R = \\{x \\in \\mathbb{R}^d \\mid \\|x\\|_2 \\le R\\}$. The projected subgradient method (PSM) applies the update $x_{k+1} = \\Pi_{B_R}\\big(x_k - t_k g_k\\big)$, where $g_k \\in \\partial f(x_k)$ is a subgradient of $f$ at $x_k$, $t_k$ is the step-size, and $\\Pi_{B_R}$ denotes Euclidean projection onto the ball $B_R$. For the function $f(x)$ given above, the subdifferential at any point $x$ is the convex hull of the set of maximizers $\\{a_i : i \\in \\arg\\max_j a_j^\\top x\\}$, and a valid choice is $g_k = a_{i^*}$ where $i^* \\in \\arg\\max_j a_j^\\top x_k$.\n\nStarting from the fundamental definitions of convexity, subgradients, Euclidean projection, and iterative optimization methods, analyze how mis-specifying the step-size as a constant that is too large can lead to oscillations and nonconvergence in the PSM when applied to $f(x) = \\max_i a_i^\\top x$ on $B_R$. Then demonstrate two standard remedies: averaging of iterates and diminishing step-sizes.\n\nYour task is to write a complete, runnable program that:\n- Implements the projected subgradient method for the specific objective $f(x) = \\max_i a_i^\\top x$ under the ball constraint $\\|x\\|_2 \\le R$, with the choice $g_k = a_{i^*}$ for $i^* \\in \\arg\\max_i a_i^\\top x_k$ resolved by taking the smallest index in case of ties.\n- Uses the Euclidean projection $\\Pi_{B_R}(y) = \\frac{R}{\\max\\{R, \\|y\\|_2\\}} y$.\n- Computes three quantitative metrics to diagnose oscillation and convergence behavior over the last window of iterates of size $W$:\n  1. The toggle rate of active subgradient indices, defined as the proportion of iterations in which the index $i^*_k$ changes between successive steps over the entire run, given by $\\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\{i^*_k \\ne i^*_{k-1}\\}$.\n  2. The mean objective over the last window of base iterates, defined as $\\frac{1}{W} \\sum_{k=N-W}^{N-1} f(x_k)$.\n  3. A rescue metric, which is the mean objective over the last window computed on either averaged iterates (for averaging rescue) or on a separate run with diminishing step-size (for step-decay rescue), specified per test case.\n\nYou must apply the above to the following test suite, which explores a \"happy path\" case, boundary conditions, and an edge case:\n\n- Test case $1$ (oscillation in two dimensions with large step-size and averaging rescue):\n  - Dimension $d = 2$.\n  - Vectors $a_1 = (1, 0)$, $a_2 = (0, 1)$ so $m = 2$.\n  - Radius $R = 1$.\n  - Initial point $x_0 = (0, 0)$.\n  - Iterations $N = 200$.\n  - Base step-size $t_k \\equiv t = 2.0$.\n  - Rescue: running average, i.e., $\\bar{x}_k = \\frac{1}{k+1} \\sum_{j=0}^k x_j$, and measure $\\frac{1}{W} \\sum_{k=N-W}^{N-1} f(\\bar{x}_k)$.\n\n- Test case $2$ (smaller constant step-size and averaging rescue):\n  - Dimension $d = 2$.\n  - Vectors $a_1 = (1, 0)$, $a_2 = (0, 1)$ so $m = 2$.\n  - Radius $R = 1$.\n  - Initial point $x_0 = (0, 0)$.\n  - Iterations $N = 200$.\n  - Base step-size $t_k \\equiv t = 0.5$.\n  - Rescue: running average, i.e., $\\bar{x}_k = \\frac{1}{k+1} \\sum_{j=0}^k x_j$, and measure $\\frac{1}{W} \\sum_{k=N-W}^{N-1} f(\\bar{x}_k)$.\n\n- Test case $3$ (three-dimensional edge case with large step-size and averaging rescue):\n  - Dimension $d = 3$.\n  - Vectors $a_1 = (1, 0, 0)$, $a_2 = (0, 1, 0)$, $a_3 = (0, 0, 1)$ so $m = 3$.\n  - Radius $R = 1$.\n  - Initial point $x_0 = (0, 0, 0)$.\n  - Iterations $N = 200$.\n  - Base step-size $t_k \\equiv t = 2.0$.\n  - Rescue: running average, i.e., $\\bar{x}_k = \\frac{1}{k+1} \\sum_{j=0}^k x_j$, and measure $\\frac{1}{W} \\sum_{k=N-W}^{N-1} f(\\bar{x}_k)$.\n\n- Test case $4$ (large constant step-size versus diminishing step-size rescue in two dimensions):\n  - Dimension $d = 2$.\n  - Vectors $a_1 = (1, 0)$, $a_2 = (0, 1)$ so $m = 2$.\n  - Radius $R = 1$.\n  - Initial point $x_0 = (0, 0)$.\n  - Iterations $N = 200$.\n  - Base step-size $t_k \\equiv t = 2.0$.\n  - Rescue: diminishing step-size $t_k = \\frac{c}{\\sqrt{k+1}}$ with $c = 1.0$, and measure $\\frac{1}{W} \\sum_{k=N-W}^{N-1} f(x_k^{\\text{decay}})$ for the separate diminishing-step run.\n\nLet the last-window size be $W = 50$ for all cases.\n\nProgram requirements:\n- Implement the PSM and compute the three metrics for each test case as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  - For test case $1$: toggle rate (base), mean objective over last $W$ base iterates, mean objective over last $W$ rescued iterates (averaged).\n  - For test case $2$: toggle rate (base), mean objective over last $W$ base iterates, mean objective over last $W$ rescued iterates (averaged).\n  - For test case $3$: toggle rate (base), mean objective over last $W$ base iterates, mean objective over last $W$ rescued iterates (averaged).\n  - For test case $4$: toggle rate (base), mean objective over last $W$ base iterates, mean objective over last $W$ rescued iterates (diminishing step-size).\n- Each numeric output must be a float rounded to six decimal places.", "solution": "The problem at hand is to analyze the behavior of the projected subgradient method (PSM) for minimizing a specific type of nonsmooth convex function over a Euclidean ball. The core of the analysis involves understanding the role of the step-size parameter and demonstrating how its improper selection can lead to non-convergent, oscillatory behavior, and how standard techniques—iterate averaging and diminishing step-sizes—can remedy this issue.\n\nWe begin from first principles.\n\n**1. The Projected Subgradient Method**\n\nThe goal is to solve the constrained optimization problem:\n$$\n\\min_{x \\in C} f(x)\n$$\nwhere $f: \\mathbb{R}^d \\to \\mathbb{R}$ is a convex function (not necessarily differentiable) and $C \\subseteq \\mathbb{R}^d$ is a closed convex set.\n\nFor a convex function $f$, a vector $g \\in \\mathbb{R}^d$ is a **subgradient** of $f$ at a point $x$ if it satisfies the inequality:\n$$\nf(y) \\ge f(x) + g^\\top (y - x) \\quad \\forall y \\in \\mathbb{R}^d\n$$\nThe set of all subgradients of $f$ at $x$ is called the **subdifferential**, denoted $\\partial f(x)$. If $f$ is differentiable at $x$, then $\\partial f(x) = \\{\\nabla f(x)\\}$.\n\nThe **subgradient method** is an iterative algorithm that generates a sequence of points $\\{x_k\\}$ via the update rule:\n$$\nx_{k+1} = x_k - t_k g_k\n$$\nwhere $t_k  0$ is the step-size at iteration $k$, and $g_k \\in \\partial f(x_k)$ is any subgradient of $f$ at $x_k$. This update is a generalization of the gradient descent method.\n\nTo handle the constraint $x \\in C$, the update is followed by a projection onto the set $C$. The **Euclidean projection** of a point $y \\in \\mathbb{R}^d$ onto $C$, denoted $\\Pi_C(y)$, is the unique point in $C$ that is closest to $y$:\n$$\n\\Pi_C(y) = \\arg\\min_{z \\in C} \\|z - y\\|_2\n$$\nThe **projected subgradient method (PSM)** combines these two steps:\n$$\nx_{k+1} = \\Pi_C(x_k - t_k g_k)\n$$\n\n**2. Problem Specification**\n\nThe problem provides the following specific components:\n- **Objective function:** $f(x) = \\max_{i \\in \\{1,\\dots,m\\}} a_i^\\top x$. This function is the maximum of a set of linear functions, which is a classic example of a nonsmooth convex function.\n- **Constraint set:** The Euclidean ball of radius $R$, $C = B_R = \\{x \\in \\mathbb{R}^d \\mid \\|x\\|_2 \\le R\\}$. This is a closed and convex set.\n- **Subdifferential and subgradient choice:** For $f(x) = \\max_i a_i^\\top x$, the subdifferential at $x$ is the convex hull of the \"active\" vectors $a_i$, i.e., those for which the maximum is attained:\n$$\n\\partial f(x) = \\text{conv}\\{a_i \\mid i \\in I(x)\\}, \\quad \\text{where } I(x) = \\arg\\max_{j \\in \\{1, \\dots, m\\}} a_j^\\top x\n$$\nThe problem specifies selecting a single subgradient $g_k = a_{i^*}$, where $i^*$ is an index in the active set $I(x_k)$. This is a valid choice as it is an extreme point of the convex hull, and thus an element of $\\partial f(x_k)$.\n- **Projection operator:** The projection onto the ball $B_R$ has a simple closed-form expression:\n$$\n\\Pi_{B_R}(y) = \\begin{cases} y  \\text{if } \\|y\\|_2 \\le R \\\\ R \\frac{y}{\\|y\\|_2}  \\text{if } \\|y\\|_2  R \\end{cases}\n$$\nThis is equivalent to the provided formula $\\Pi_{B_R}(y) = \\frac{R}{\\max\\{R, \\|y\\|_2\\}} y$.\n\n**3. Analysis of Step-Size and Convergence**\n\nThe convergence of the subgradient method critically depends on the choice of step-sizes $\\{t_k\\}$. The fundamental inequality governing the distance to an optimal solution $x^*$ is:\n$$\n\\|x_{k+1} - x^*\\|_2^2 \\le \\|x_k - x^*\\|_2^2 - 2t_k(f(x_k) - f(x^*)) + t_k^2 \\|g_k\\|_2^2\n$$\nThis holds due to the non-expansiveness of the projection operator.\n\n**a. Constant Step-Size:**\nLet $t_k = t$ for all $k$. To guarantee a decrease in distance to the optimal set, we need $2t(f(x_k) - f(x^*))  t^2 \\|g_k\\|_2^2$. If the step-size $t$ is too large, the term $t^2 \\|g_k\\|_2^2$ can dominate, preventing the iterates from approaching $x^*$.\nIn the context of $f(x) = \\max(a_1^\\top x, a_2^\\top x)$, the space is partitioned by the hyperplane $\\{x \\mid a_1^\\top x = a_2^\\top x\\}$. If an iterate $x_k$ is on the side where $a_1^\\top x_k  a_2^\\top x_k$, the subgradient is $g_k=a_1$ and the update moves in the direction $-a_1$. A large step $t$ can cause the next iterate $x_{k+1}$ to \"jump\" across the dividing hyperplane, into the region where $a_2^\\top x_{k+1}  a_1^\\top x_{k+1}$. This makes the next subgradient $g_{k+1}=a_2$, leading to an update in the direction $-a_2$. This back-and-forth movement results in oscillations of both the iterates $x_k$ and the active subgradient index $i^*$. The algorithm fails to converge to a single point and may remain far from the optimal solution. The provided **toggle rate** metric is designed to quantify this oscillatory behavior of the active index.\n\n**b. Remedy 1: Iterate Averaging**\nEven when the base iterates $\\{x_k\\}$ oscillate due to a large constant step-size, the Cesàro mean of the iterates, defined as:\n$$\n\\bar{x}_k = \\frac{1}{k+1} \\sum_{j=0}^k x_j\n$$\noften exhibits much better behavior. The averaging process tends to cancel out the oscillatory components of the updates. For constant step-size, it can be proven that the function value of the averaged iterate, $f(\\bar{x}_k)$, converges to a neighborhood of the optimal value $f(x^*)$. In many practical cases, especially for problems with some structure like the one considered, it converges to $f(x^*)$ itself. This is why the mean objective of the averaged iterates is used as a \"rescue\" metric.\n\n**c. Remedy 2: Diminishing Step-Sizes**\nA standard way to guarantee convergence is to use a step-size sequence $\\{t_k\\}$ that diminishes over time but not too quickly. The typical conditions are:\n$$\n\\sum_{k=0}^{\\infty} t_k = \\infty \\quad \\text{and} \\quad \\sum_{k=0}^{\\infty} t_k^2  \\infty\n$$\nThe first condition ensures the algorithm can traverse any distance, while the second ensures the steps eventually become small enough to prevent overshooting, thus forcing convergence. The problem specifies a rule $t_k = c/\\sqrt{k+1}$. While this sequence does not satisfy $\\sum t_k^2  \\infty$ (since $\\sum 1/(k+1)$ diverges), it is a well-known choice that still ensures convergence of the objective value, i.e., $\\min_{0 \\le j \\le k} f(x_j) \\to f(x^*)$ as $k \\to \\infty$. The rate of convergence is typically slower, but the method is guaranteed to not suffer from the persistent oscillations of a large constant step-size.\n\n**4. Numerical Demonstration**\n\nThe provided test cases are designed to empirically verify this theoretical analysis.\n- **Case 1  3:** Use a large constant step-size $t=2.0$ which is expected to induce strong oscillations. The toggle rate should be high, and the mean objective of the base iterates should be poor (i.e., higher than the optimal value). The averaging rescue metric should show a significant improvement, with its value being close to the true minimum.\n- **Case 2:** Uses a smaller constant step-size $t=0.5$. This should reduce oscillations, leading to a lower toggle rate and better base objective value compared to Case 1.\n- **Case 4:** Directly compares the large constant step-size run with a separate run using a diminishing step-size. This demonstrates that correctly chosen diminishing steps provide a direct path to convergence, achieving a good objective value without needing the post-processing of averaging.\n\nThe program below implements the PSM and calculates the specified metrics for each case, providing quantitative evidence for the behaviors discussed.", "answer": "```python\nimport numpy as np\n\ndef objective_function(x, A):\n    \"\"\"Computes f(x) = max(A @ x).\"\"\"\n    if x.ndim == 1:\n        return np.max(A @ x)\n    # Handle cases where x is a list of vectors for batch computation\n    return np.max(A @ np.array(x).T, axis=0)\n\ndef project(y, R):\n    \"\"\"Projects a point y onto the Euclidean ball of radius R.\"\"\"\n    norm_y = np.linalg.norm(y)\n    if norm_y  R:\n        return R * y / norm_y\n    return y\n\ndef run_psm(A, R, x0, N, step_rule):\n    \"\"\"\n    Runs the Projected Subgradient Method.\n    \n    Args:\n        A (np.ndarray): Matrix of vectors a_i.\n        R (float): Radius of the ball.\n        x0 (np.ndarray): Initial point.\n        N (int): Number of iterations.\n        step_rule (callable): A function k - t_k for step-size.\n        \n    Returns:\n        A tuple of (x_history, f_history, idx_history).\n    \"\"\"\n    x = np.copy(x0)\n    \n    x_history = [np.copy(x)]\n    f_history = [objective_function(x, A)]\n    idx_history = []\n    \n    for k in range(N):\n        # Calculate subgradient\n        vals = A @ x\n        i_star = np.argmax(vals)\n        g = A[i_star]\n        \n        idx_history.append(i_star)\n        \n        # Get step-size\n        t = step_rule(k)\n        \n        # Update and project\n        y = x - t * g\n        x = project(y, R)\n        \n        # Store results\n        x_history.append(np.copy(x))\n        f_history.append(objective_function(x, A))\n        \n    return x_history, f_history, idx_history\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"d\": 2, \"a\": [[1, 0], [0, 1]], \"R\": 1.0, \"x0\": [0, 0],\n            \"N\": 200, \"base_t\": 2.0, \"W\": 50,\n            \"rescue\": {\"type\": \"average\", \"param\": None}\n        },\n        # Case 2\n        {\n            \"d\": 2, \"a\": [[1, 0], [0, 1]], \"R\": 1.0, \"x0\": [0, 0],\n            \"N\": 200, \"base_t\": 0.5, \"W\": 50,\n            \"rescue\": {\"type\": \"average\", \"param\": None}\n        },\n        # Case 3\n        {\n            \"d\": 3, \"a\": [[1, 0, 0], [0, 1, 0], [0, 0, 1]], \"R\": 1.0, \"x0\": [0, 0, 0],\n            \"N\": 200, \"base_t\": 2.0, \"W\": 50,\n            \"rescue\": {\"type\": \"average\", \"param\": None}\n        },\n        # Case 4\n        {\n            \"d\": 2, \"a\": [[1, 0], [0, 1]], \"R\": 1.0, \"x0\": [0, 0],\n            \"N\": 200, \"base_t\": 2.0, \"W\": 50,\n            \"rescue\": {\"type\": \"decay\", \"param\": 1.0} # c=1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = np.array(case[\"a\"])\n        R = case[\"R\"]\n        x0 = np.array(case[\"x0\"])\n        N = case[\"N\"]\n        W = case[\"W\"]\n        base_t = case[\"base_t\"]\n        \n        # 1. Base Run\n        base_step_rule = lambda k: base_t\n        base_x_hist, base_f_hist, base_idx_hist = run_psm(A, R, x0, N, base_step_rule)\n        \n        # Metric 1: Toggle Rate\n        # base_idx_hist has N elements (i*_0 to i*_{N-1})\n        toggles = np.sum(np.array(base_idx_hist[1:]) != np.array(base_idx_hist[:-1]))\n        toggle_rate = toggles / (N - 1)\n        \n        # Metric 2: Mean Objective (Base)\n        # base_f_hist has N+1 elements: f(x_0), ..., f(x_N)\n        # We need sum over f(x_{N-W}), ..., f(x_{N-1}), which corresponds to indices N-W to N-1\n        mean_obj_base = np.mean(base_f_hist[N - W : N])\n        \n        # Metric 3: Mean Objective (Rescue)\n        rescue_type = case[\"rescue\"][\"type\"]\n        if rescue_type == \"average\":\n            f_avg_values = []\n            # Calculate bar_x_k for k in {N-W, ..., N-1}\n            for k in range(N - W, N):\n                # Definition: bar_x_k = (1/(k+1)) * sum_{j=0 to k} x_j\n                # base_x_hist has x_0, ..., x_N. Sum x_0..x_k is sum of first k+1 elements.\n                sum_of_iterates = np.sum(base_x_hist[0 : k + 1], axis=0)\n                bar_x_k = sum_of_iterates / (k + 1)\n                f_avg_values.append(objective_function(bar_x_k, A))\n            mean_obj_rescue = np.mean(f_avg_values)\n\n        elif rescue_type == \"decay\":\n            c = case[\"rescue\"][\"param\"]\n            decay_step_rule = lambda k: c / np.sqrt(k + 1)\n            _, decay_f_hist, _ = run_psm(A, R, x0, N, decay_step_rule)\n            # Same window as base case: f(x_{N-W}), ..., f(x_{N-1})\n            mean_obj_rescue = np.mean(decay_f_hist[N - W : N])\n        \n        results.extend([toggle_rate, mean_obj_base, mean_obj_rescue])\n    \n    # Format the final output string\n    output_str = \",\".join([f\"{r:.6f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3164985"}]}