{"hands_on_practices": [{"introduction": "Online Gradient Descent (OGD) is a robust and universal algorithm for online convex optimization, but its performance is not always optimal. This practice explores how leveraging the specific geometric structure of loss functions, such as second-order information, can lead to significantly better performance. You will compare the standard OGD with a more sophisticated second-order method, the Online Newton Step (ONS), on a sequence of quadratic losses. By working through this problem [@problem_id:3159405], you will quantify the performance gap and understand the theoretical benefits of algorithms that are aware of the problem's geometry, providing a clear motivation for moving beyond first-order methods.", "problem": "Consider an online convex optimization problem with decision set $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$, where $d \\in \\mathbb{N}$ and $B > 0$ are given constants. At each round $t = 1, 2, \\ldots, T$, the learner picks $x_t \\in \\mathcal{X}$, then observes a convex loss function of the form $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$, where the data satisfy the uniform bounds $\\|A_t\\|_{2} \\leq L$ and $\\|b_t\\|_{2} \\leq M$ for known constants $L > 0$ and $M \\geq 0$. The performance is measured by the cumulative regret\n$$\nR_T \\triangleq \\sum_{t=1}^{T} f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^{T} f_t(x).\n$$\nAssume the learner either uses Online Newton Step (ONS) with initialization matrix $\\lambda_0 I_d$ for some $\\lambda_0 > 0$, or Online Gradient Descent (OGD) with the standard choice of constant step size that yields the optimal order of regret for convex Lipschitz losses on a bounded domain. Using only fundamental definitions (convexity, regret) and well-tested facts (norm inequalities, Cauchy–Schwarz inequality, spectral-norm submultiplicativity, matrix-inequality characterization of exp-concavity, and the standard regret guarantees for ONS on exp-concave losses and for OGD on convex Lipschitz losses), do the following:\n- Propose the explicit bound on the residual magnitude $\\|A_t x - b_t\\|_{2}$ over $\\mathcal{X}$, derive a uniform bound on the gradient norm $\\|\\nabla f_t(x)\\|_{2}$ over $\\mathcal{X}$, and identify the exp-concavity parameter $\\eta$ of $f_t$ on $\\mathcal{X}$.\n- Using these derived quantities, specialize the standard ONS regret guarantee to this setting to obtain an explicit upper-bound expression for $R_T$ in terms of $d$, $B$, $L$, $M$, $\\lambda_0$, and $T$ (no hidden constants).\n- Specialize the standard OGD regret guarantee for convex Lipschitz functions on a set of diameter $D$ to this setting, obtaining an explicit upper-bound expression for $R_T$ in terms of $B$, $L$, $M$, and $T$ (no hidden constants).\nProvide your final answer as a single row matrix containing these two expressions in the order: first the ONS regret upper bound and then the OGD regret upper bound. No inequalities or equations should appear in the final answer; write only the two simplified closed-form analytic expressions. No rounding is required.", "solution": "The user wants me to solve the problem by providing the required information in the expected format.\n\n### Problem Validation\n\nFirst, a critical validation of the problem statement is performed.\n\n**Step 1: Extract Givens**\n- Decision set: $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$.\n- Parameters: $d \\in \\mathbb{N}$, $B > 0$, $L > 0$, $M \\geq 0$, $\\lambda_0 > 0$.\n- Time horizon: $t = 1, 2, \\ldots, T$.\n- Loss functions: $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$.\n- Data bounds: $\\|A_t\\|_{2} \\leq L$ and $\\|b_t\\|_{2} \\leq M$.\n- Regret definition: $R_T = \\sum_{t=1}^{T} f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^{T} f_t(x)$.\n- Algorithms to consider: Online Newton Step (ONS) with initialization $\\lambda_0 I_d$ and Online Gradient Descent (OGD) with an optimally chosen constant step size.\n- Permitted assumptions: Use of fundamental definitions and standard, well-tested results from online optimization theory.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a standard exercise in the field of online convex optimization. The setting, involving quadratic loss functions (least squares) on a norm ball, is canonical and extensively studied. All concepts and tools mentioned (regret, ONS, OGD, exp-concavity) are fundamental to the field. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem requests the derivation of explicit upper bounds on regret for two standard algorithms. Given the standard guarantees for these algorithms, the task is well-defined and leads to unique analytical expressions based on the provided parameters.\n- **Objective:** The problem is stated in precise, formal mathematical language, devoid of ambiguity or subjectivity.\n- **Completeness and Consistency:** All necessary parameters ($d$, $B$, $L$, $M$, $\\lambda_0$, $T$) are provided. The problem explicitly allows the use of standard results, making it self-contained for deriving the required bounds. There are no contradictions in the setup.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We may proceed to the solution.\n\n### Derivation of Required Quantities\n\nThe solution requires specializing standard regret bounds for ONS and OGD. To do so, we must first derive key properties of the loss functions $f_t(x)$ over the domain $\\mathcal{X}$.\n\n**1. Uniform Bound on the Gradient Norm**\n\nThe loss function at round $t$ is $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$. Its gradient with respect to $x$ is:\n$$ \\nabla f_t(x) = A_t^T (A_t x - b_t) $$\nWe seek a uniform upper bound on its Euclidean norm, $\\|\\nabla f_t(x)\\|_{2}$, for any $x \\in \\mathcal{X}$ and any $t \\in \\{1, \\ldots, T\\}$. Using the submultiplicativity of the spectral norm ($L_2$ operator norm) and the triangle inequality:\n$$ \\|\\nabla f_t(x)\\|_{2} = \\|A_t^T (A_t x - b_t)\\|_{2} \\leq \\|A_t^T\\|_{2} \\|A_t x - b_t\\|_{2} $$\nSince $\\|A_t^T\\|_{2} = \\|A_t\\|_{2} \\leq L$, this becomes:\n$$ \\|\\nabla f_t(x)\\|_{2} \\leq L \\left( \\|A_t x\\|_{2} + \\|b_t\\|_{2} \\right) $$\nAgain using submultiplicativity and the fact that for $x \\in \\mathcal{X}$, we have $\\|x\\|_{2} \\leq B$:\n$$ \\|\\nabla f_t(x)\\|_{2} \\leq L \\left( \\|A_t\\|_{2} \\|x\\|_{2} + \\|b_t\\|_{2} \\right) \\leq L (L B + M) $$\nThis provides a uniform Lipschitz constant, which we denote as $G$:\n$$ G = L(LB+M) $$\n\n**2. Exp-concavity Parameter**\n\nA differentiable function $f$ is $\\eta$-exp-concave if its Hessian satisfies the matrix inequality $\\nabla^2 f(x) \\succeq \\eta (\\nabla f(x))(\\nabla f(x))^T$. The Hessian of $f_t(x)$ is:\n$$ \\nabla^2 f_t(x) = A_t^T A_t $$\nThe condition for exp-concavity is thus $A_t^T A_t \\succeq \\eta (\\nabla f_t(x))(\\nabla f_t(x))^T$. For any vector $z \\in \\mathbb{R}^d$, this means $z^T (A_t^T A_t) z \\geq \\eta (z^T \\nabla f_t(x))^2$. The left side is $\\|A_t z\\|_2^2$. The right side involves $(z^T \\nabla f_t(x))^2 = ((A_t z)^T(A_t x - b_t))^2$. By the Cauchy-Schwarz inequality, this is bounded by $\\|A_t z\\|_2^2 \\|A_t x - b_t\\|_2^2$.\nTherefore, the condition $A_t^T A_t \\succeq \\eta (\\nabla f_t(x))(\\nabla f_t(x))^T$ is satisfied if $\\|A_t z\\|_2^2 \\geq \\eta \\|A_t z\\|_2^2 \\|A_t x - b_t\\|_2^2$, which simplifies to $1 \\geq \\eta \\|A_t x - b_t\\|_2^2$.\nTo find a uniform parameter $\\eta$ valid for all $t$ and all $x \\in \\mathcal{X}$, we need an upper bound on $\\|A_t x - b_t\\|_2$. As shown in the gradient bound derivation, $\\|A_t x - b_t\\|_{2} \\leq LB + M$.\nThus, the condition holds if $1 \\geq \\eta (LB+M)^2$. We can choose the largest such $\\eta$, which is:\n$$ \\eta = \\frac{1}{(LB+M)^2} $$\n\n### Online Newton Step (ONS) Regret Bound\n\nThe standard regret guarantee for ONS on a sequence of $\\eta$-exp-concave functions with gradients uniformly bounded by $G$, initialized with $\\Lambda_0 = \\lambda_0 I_d$, is:\n$$ R_T \\leq \\frac{d}{2\\eta} \\ln\\left(1 + \\frac{T G^2}{d \\lambda_0}\\right) $$\nSubstituting the derived quantities $G = L(LB+M)$ and $\\eta = \\frac{1}{(LB+M)^2}$:\n$$ R_T \\leq \\frac{d}{2 \\left(\\frac{1}{(LB+M)^2}\\right)} \\ln\\left(1 + \\frac{T (L(LB+M))^2}{d \\lambda_0}\\right) $$\nSimplifying this expression gives the explicit upper bound for ONS:\n$$ R_T \\leq \\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right) $$\n\n### Online Gradient Descent (OGD) Regret Bound\n\nThe standard regret guarantee for OGD with an optimal constant step size, applied to a sequence of convex $G$-Lipschitz functions over a convex set $\\mathcal{X}$ of diameter $D$, is:\n$$ R_T \\leq D G \\sqrt{T} $$\nWe need to find the diameter $D$ of the set $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$. The diameter is defined as $D = \\sup_{x,y \\in \\mathcal{X}} \\|x-y\\|_2$. By the triangle inequality, $\\|x-y\\|_2 \\leq \\|x\\|_2 + \\|y\\|_2 \\leq B + B = 2B$. This maximum is achieved by choosing any $x$ with $\\|x\\|_2=B$ and setting $y=-x$, which yields $\\|x-(-x)\\|_2 = \\|2x\\|_2 = 2\\|x\\|_2 = 2B$. Thus, the diameter is:\n$$ D = 2B $$\nSubstituting $D = 2B$ and the gradient bound $G = L(LB+M)$ into the standard OGD regret guarantee:\n$$ R_T \\leq (2B) (L(LB+M)) \\sqrt{T} $$\nSimplifying this expression gives the explicit upper bound for OGD:\n$$ R_T \\leq 2BL(LB+M)\\sqrt{T} $$\n\n### Final Answer Formulation\n\nThe final answer requires the two derived regret bounds—for ONS and OGD, in that order—presented as a single row matrix.\n- ONS bound: $\\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right)$\n- OGD bound: $2BL(LB+M)\\sqrt{T}$", "answer": "$$\n\\boxed{\\pmatrix{ \\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right) & 2BL(LB+M)\\sqrt{T} }}\n$$", "id": "3159405"}, {"introduction": "After establishing that exploiting problem geometry is crucial, we now examine a powerful family of algorithms designed to do just that: Follow-the-Regularized-Leader (FTRL). This practice focuses on applying FTRL to the probability simplex, a common domain in machine learning, using the canonical negative Shannon entropy as the regularizer. This specific combination gives rise to one of the most celebrated algorithms in online learning. By solving the FTRL optimization problem for a given sequence of losses [@problem_id:3159427], you will derive the famous Multiplicative Weights Update algorithm from first principles, gaining a concrete understanding of how entropic regularization leads to the ubiquitous softmax update rule.", "problem": "Consider an Online Convex Optimization (OCO) problem over the $3$-dimensional probability simplex $\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$. At each round $t\\in\\{1,2,3,4,5\\}$, a learner chooses a decision $x_{t}\\in\\Delta_{3}$, and an adversary reveals a linear loss function $f_{t}(x)=\\ell_{t}\\cdot x$ where $\\ell_{t}\\in\\mathbb{R}^{3}$. The learner uses Follow-The-Regularized-Leader (FTRL) with entropic regularization, specifically the negative Shannon entropy regularizer $R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$, and learning-rate parameter $\\eta>0$. The FTRL decision at round $t+1$ is the solution to the convex program\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}.\n$$\nSuppose an adversary chooses a sequence of losses that align with the corners of the domain in the following way: for a fixed magnitude $M>0$,\n$$\n\\ell_{1}=(M,0,0),\\quad \\ell_{2}=(M,0,0),\\quad \\ell_{3}=(0,M,0),\\quad \\ell_{4}=(0,M,0),\\quad \\ell_{5}=(0,0,M).\n$$\nStarting from no prior losses, use the OCO and FTRL definitions above and principles of convex optimization to derive the explicit analytic expression for the learner’s decision $x_{6}$ produced by the entropic FTRL update after these five rounds. Your final answer must be a single closed-form expression for $x_{6}$ as a point in $\\Delta_{3}$ written in terms of $\\eta$ and $M$. No rounding is required.", "solution": "The user wants to find the learner's decision vector $x_{6}$ after $5$ rounds of an Online Convex Optimization problem. The learner uses the Follow-The-Regularized-Leader (FTRL) algorithm with a negative Shannon entropy regularizer.\n\nThe decision $x_{t+1}$ is determined by solving the following optimization problem:\n$$\nx_{t+1} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}\n$$\nwhere $\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$ is the $3$-dimensional probability simplex, $R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$ is the regularizer, and $\\eta > 0$ is the learning rate.\n\nWe need to compute $x_{6}$, which means we sum the losses from $t=1$ to $t=5$. Let $L_{5}$ be the cumulative loss vector, $L_{5} = \\sum_{s=1}^{5} \\ell_{s}$. The given loss vectors are:\n$$\n\\ell_{1}=(M,0,0) \\\\\n\\ell_{2}=(M,0,0) \\\\\n\\ell_{3}=(0,M,0) \\\\\n\\ell_{4}=(0,M,0) \\\\\n\\ell_{5}=(0,0,M)\n$$\nSumming these vectors component-wise, we get:\n$$\nL_{5} = (M+M, M+M, M) = (2M, 2M, M)\n$$\nThe optimization problem for $x_{6}$ is:\n$$\nx_{6} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta (L_{5} \\cdot x) + R(x)\\right\\}\n$$\nLet's write out the objective function, which we denote as $F(x)$:\n$$\nF(x) = \\eta (2Mx_{1} + 2Mx_{2} + Mx_{3}) + x_{1}\\ln x_{1} + x_{2}\\ln x_{2} + x_{3}\\ln x_{3}\n$$\nThis is a convex optimization problem with constraints $x_{1}+x_{2}+x_{3}=1$ and $x_{i} \\geq 0$ for $i \\in \\{1, 2, 3\\}$. We use the method of Lagrange multipliers to find the solution. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(x, \\nu, \\lambda_{1}, \\lambda_{2}, \\lambda_{3}) = F(x) + \\nu\\left(\\sum_{i=1}^{3}x_{i} - 1\\right) - \\sum_{i=1}^{3}\\lambda_{i}x_{i}\n$$\nwhere $\\nu$ is the Lagrange multiplier for the equality constraint and $\\lambda_i$ are the multipliers for the non-negativity constraints $x_i \\geq 0$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions must be satisfied at the minimum. The stationarity condition requires setting the gradient of the Lagrangian with respect to $x$ to zero. For each component $x_{i}$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{i}} = \\eta (L_{5})_{i} + (\\ln x_{i} + 1) + \\nu - \\lambda_{i} = 0\n$$\nwhere $(L_{5})_{i}$ is the $i$-th component of the vector $L_{5}$.\n\nThe regularizer $R(x)$ is strictly convex and its value approaches $+\\infty$ as any $x_i \\to 0^+$ (since $\\lim_{z\\to 0^+} z\\ln z = 0$ but its derivative $\\ln z + 1 \\to -\\infty$). This acts as a barrier, ensuring that for finite loss values, the optimal solution will be in the interior of the simplex, i.e., $x_{i} > 0$ for all $i$.\nAccording to the KKT complementary slackness condition, $\\lambda_{i}x_{i} = 0$. Since $x_{i} > 0$, we must have $\\lambda_{i} = 0$ for $i=1, 2, 3$.\n\nThe stationarity conditions thus simplify to:\n$$\n\\eta (L_{5})_{i} + \\ln x_{i} + 1 + \\nu = 0\n$$\nSolving for $x_{i}$:\n$$\n\\ln x_{i} = -\\eta (L_{5})_{i} - 1 - \\nu\n$$\n$$\nx_{i} = \\exp(-\\eta (L_{5})_{i} - 1 - \\nu) = \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu)\n$$\nThe term $\\exp(-1-\\nu)$ is a constant for all $i$. We can find its value using the simplex constraint $\\sum_{i=1}^{3}x_{i} = 1$:\n$$\n\\sum_{i=1}^{3} \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu) = 1\n$$\n$$\n\\exp(-1-\\nu) \\left( \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) \\right) = 1\n$$\n$$\n\\exp(-1-\\nu) = \\frac{1}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\nSubstituting this back into the expression for $x_{i}$, we obtain the general solution, which corresponds to a softmax function applied to the negative cumulative losses:\n$$\nx_{i} = \\frac{\\exp(-\\eta (L_{5})_{i})}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\nNow, we substitute the specific components of our cumulative loss vector $L_{5} = (2M, 2M, M)$ to find the components of $x_{6} = (x_{6,1}, x_{6,2}, x_{6,3})$. The denominator is:\n$$\nD = \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) = \\exp(-\\eta (2M)) + \\exp(-\\eta (2M)) + \\exp(-\\eta M) = 2\\exp(-2\\eta M) + \\exp(-\\eta M)\n$$\nThe components of $x_{6}$ are:\n$$\nx_{6,1} = \\frac{\\exp(-\\eta (L_{5})_{1})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-\\eta (L_{5})_{2})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta (L_{5})_{3})}{D} = \\frac{\\exp(-\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\nTo simplify these expressions, we can multiply the numerator and denominator of each fraction by $\\exp(2\\eta M)$:\n$$\nx_{6,1} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n$$\nThus, the learner's decision vector $x_{6}$ is:\n$$\nx_{6} = \\left( \\frac{1}{2 + \\exp(\\eta M)}, \\frac{1}{2 + \\exp(\\eta M)}, \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)} \\right)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2 + \\exp(\\eta M)} & \\frac{1}{2 + \\exp(\\eta M)} & \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n\\end{pmatrix}\n}\n$$", "id": "3159427"}, {"introduction": "Theoretical analyses often focus on algorithms with fixed parameters, but real-world environments are rarely static. This hands-on coding practice introduces the concept of adaptivity, where an algorithm adjusts its behavior in response to observed data. You will implement a variant of Mirror Descent that incorporates an adaptive restart mechanism, using the relationship between successive gradients as a signal to detect a changing environment. By implementing and evaluating this strategy on gradient sequences with different characteristics—from stable trends to adversarial patterns [@problem_id:3159468]—you will gain practical insight into how adaptive heuristics can dramatically improve performance and bridge the gap between static theory and dynamic practical application.", "problem": "You will implement and evaluate an adaptive restart variant of mirror descent in the setting of online convex optimization over a Euclidean ball. The goal is to test whether restarting based on gradient inner products improves regret under gradient sequences exhibiting trend versus pure noise.\n\nThe fundamental base to begin from consists of the following well-tested definitions and facts in online convex optimization. In the online convex optimization protocol over a convex decision set $\\mathcal{K}$, at each round $t \\in \\{1,\\dots,T\\}$: a player chooses a point $x_t \\in \\mathcal{K}$, then a convex loss function $f_t$ is revealed, and the player incurs loss $f_t(x_t)$. The regret with respect to the best fixed comparator in hindsight is\n$$\n\\mathrm{Regret}_T \\triangleq \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x).\n$$\nMirror descent with a differentiable strictly convex mirror map $\\psi$ produces the update\n$$\nx_{t+1} \\in \\arg\\min_{x \\in \\mathcal{K}} \\left\\{ \\eta \\langle g_t, x \\rangle + D_{\\psi}(x, x_t) \\right\\},\n$$\nwhere $g_t \\in \\partial f_t(x_t)$ is a subgradient (for our differentiable linear losses this is the gradient), $\\eta > 0$ is a step-size, and $D_{\\psi}(x,y) \\triangleq \\psi(x) - \\psi(y) - \\langle \\nabla \\psi(y), x - y \\rangle$ is the Bregman divergence. With the Euclidean mirror map $\\psi(x) = \\tfrac{1}{2}\\|x\\|_2^2$, one has $D_{\\psi}(x,y) = \\tfrac{1}{2}\\|x - y\\|_2^2$. Your derivation must start from these definitions and facts without assuming any target formulas.\n\nYou will specialize to linear losses of the form $f_t(x) = \\langle g_t, x \\rangle$ over the Euclidean ball $\\mathcal{K} = \\{ x \\in \\mathbb{R}^d : \\|x\\|_2 \\le R \\}$ with radius $R > 0$. In this case, the best fixed comparator in hindsight is the solution of a linear program over $\\mathcal{K}$, which you should derive explicitly in your solution in terms of the cumulative gradient $G_T = \\sum_{t=1}^T g_t$.\n\nYou will implement two algorithms:\n\n- Baseline mirror descent (Euclidean): starting at $x_1 = 0$, for $t \\in \\{1,\\dots,T\\}$, incur loss $f_t(x_t) = \\langle g_t, x_t \\rangle$, and update\n  $$\n  x_{t+1} = \\Pi_{\\mathcal{K}}(x_t - \\eta g_t),\n  $$\n  where $\\Pi_{\\mathcal{K}}$ is the Euclidean projection onto $\\mathcal{K}$.\n\n- Adaptive-restart mirror descent: define the restart anchor $a_t$ as follows. For $t = 1$, set $a_1 = x_1$. For $t \\ge 2$, compute the cosine condition based on the inner product of successive gradients and a threshold $\\alpha \\in [0,1]$:\n  $$\n  \\langle g_t, g_{t-1} \\rangle \\le - \\alpha \\, \\|g_t\\|_2 \\, \\|g_{t-1}\\|_2.\n  $$\n  If the condition holds, set $a_t = 0$ (restart to the center of $\\mathcal{K}$); otherwise set $a_t = x_t$. Then update\n  $$\n  x_{t+1} = \\Pi_{\\mathcal{K}}(a_t - \\eta g_t).\n  $$\n  The loss at round $t$ is still incurred at $x_t$; the restart decision affects only $x_{t+1}$.\n\nYou must evaluate both algorithms on identical sequences of gradients $g_t$ and report the signed regret difference\n$$\n\\Delta \\mathrm{Regret} \\triangleq \\mathrm{Regret}_T^{\\text{restart}} - \\mathrm{Regret}_T^{\\text{baseline}}.\n$$\nA negative value indicates that restarting improved regret.\n\nYour implementation must adhere to the following scenario and test suite. All vectors and matrices are real-valued, and all operations are Euclidean. There are no physical units involved. Angles, where relevant, are implicitly in radians via the cosine relation. All randomness must be deterministic through fixed seeds. Use $R = 1$ and initialize $x_1 = 0$ in all cases. For each case, construct a gradient sequence $(g_t)_{t=1}^T$ as specified, then run both algorithms with the given parameters.\n\nTest suite. For each case below, you must use exactly the stated parameters. All scalar numbers must be interpreted as real numbers.\n\n- Case $1$ (trend with low noise):\n  - Dimension $d = 5$, rounds $T = 200$.\n  - Base trend $v = 0.3 \\cdot \\mathbf{1}_d$.\n  - Noise standard deviation $\\sigma = 0.02$.\n  - Random seed $11$ for noise.\n  - Step-size $\\eta = 0.2$, restart threshold $\\alpha = 0.0$.\n  - Gradient sequence: $g_t = v + \\sigma \\cdot \\xi_t$, where $\\xi_t \\sim \\mathcal{N}(0, I_d)$ i.i.d., generated with the stated seed.\n\n- Case $2$ (pure noise):\n  - Dimension $d = 10$, rounds $T = 400$.\n  - Base trend $v = 0$.\n  - Noise standard deviation $\\sigma = 0.2$.\n  - Random seed $22$ for noise.\n  - Step-size $\\eta = 0.2$, restart threshold $\\alpha = 0.0$.\n  - Gradient sequence: $g_t = \\sigma \\cdot \\xi_t$, with $\\xi_t$ as above.\n\n- Case $3$ (small trend with higher noise):\n  - Dimension $d = 20$, rounds $T = 400$.\n  - Base trend magnitude $\\|v\\|_2 = 0.05$, with direction drawn by sampling $\\tilde{v} \\sim \\mathcal{N}(0, I_d)$ using random seed $33$ and setting $v = 0.05 \\cdot \\tilde{v} / \\|\\tilde{v}\\|_2$.\n  - Noise standard deviation $\\sigma = 0.3$ with the same random seed $33$ for the noise sequence.\n  - Step-size $\\eta = 0.15$, restart threshold $\\alpha = 0.5$.\n  - Gradient sequence: $g_t = v + \\sigma \\cdot \\xi_t$.\n\n- Case $4$ (boundary case with a single round):\n  - Dimension $d = 3$, rounds $T = 1$.\n  - Base trend $v = 0.5 \\cdot \\mathbf{1}_d$.\n  - Noise standard deviation $\\sigma = 0$.\n  - Step-size $\\eta = 0.5$, restart threshold $\\alpha = 0.0$.\n  - Gradient sequence: $g_1 = v$.\n\n- Case $5$ (adversarial alternating gradients):\n  - Dimension $d = 2$, rounds $T = 50$.\n  - Deterministic gradients alternating on the first coordinate: for odd $t$, $g_t = (1, 0)$; for even $t$, $g_t = (-1, 0)$.\n  - Step-size $\\eta = 0.4$, restart threshold $\\alpha = 0.9$.\n\nFor each case, compute the signed regret difference $\\Delta \\mathrm{Regret}$ defined above. Your program should produce a single line of output containing the results, in order of cases $1$ through $5$, as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$). Each $r_i$ must be a floating-point number. No other output is permitted.", "solution": "The problem is valid. It is scientifically grounded in the established theory of online convex optimization, well-posed with a unique and computable solution for each test case, and objectively formulated with precise mathematical definitions and parameters.\n\n### Step 1: Extract Givens\n\n**Protocol:** Online convex optimization over a convex set $\\mathcal{K}$. At each round $t \\in \\{1,\\dots,T\\}$, a player chooses $x_t \\in \\mathcal{K}$, a convex loss function $f_t$ is revealed, and the player incurs loss $f_t(x_t)$.\n\n**Definitions:**\n- **Regret:** $\\mathrm{Regret}_T \\triangleq \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x)$.\n- **Decision Set:** $\\mathcal{K} = \\{ x \\in \\mathbb{R}^d : \\|x\\|_2 \\le R \\}$, with $R > 0$.\n- **Loss Functions:** $f_t(x) = \\langle g_t, x \\rangle$ for a sequence of gradient vectors $(g_t)_{t=1}^T$.\n- **Target Metric:** $\\Delta \\mathrm{Regret} \\triangleq \\mathrm{Regret}_T^{\\text{restart}} - \\mathrm{Regret}_T^{\\text{baseline}}$.\n\n**Algorithms:**\n- **Initial Condition:** $x_1 = 0$ for both algorithms. Radius is $R=1$.\n- **Baseline Mirror Descent:** $x_{t+1} = \\Pi_{\\mathcal{K}}(x_t - \\eta g_t)$, where $\\Pi_{\\mathcal{K}}$ is the Euclidean projection onto $\\mathcal{K}$.\n- **Adaptive-Restart Mirror Descent:**\n  - Anchor point $a_t$:\n    - For $t=1$, $a_1 = x_1$.\n    - For $t \\ge 2$, if $\\langle g_t, g_{t-1} \\rangle \\le - \\alpha \\, \\|g_t\\|_2 \\, \\|g_{t-1}\\|_2$, then $a_t = 0$; otherwise, $a_t = x_t$.\n  - Update: $x_{t+1} = \\Pi_{\\mathcal{K}}(a_t - \\eta g_t)$.\n- **Loss Incurred:** At round $t$, the loss is $f_t(x_t) = \\langle g_t, x_t \\rangle$ for both algorithms.\n\n**Test Cases:**\n1.  $d = 5, T = 200, v = 0.3 \\cdot \\mathbf{1}_d, \\sigma = 0.02$, seed=$11, \\eta = 0.2, \\alpha = 0.0$.\n2.  $d = 10, T = 400, v = 0, \\sigma = 0.2$, seed=$22, \\eta = 0.2, \\alpha = 0.0$.\n3.  $d = 20, T = 400$, $\\|v\\|_2 = 0.05$ (direction from seed $33$), $\\sigma = 0.3$, seed=$33, \\eta = 0.15, \\alpha = 0.5$.\n4.  $d = 3, T = 1, v = 0.5 \\cdot \\mathbf{1}_d, \\sigma = 0$, $\\eta = 0.5, \\alpha = 0.0$.\n5.  $d = 2, T = 50$, deterministic $g_t$: $(1, 0)$ for $t$ odd, $(-1, 0)$ for $t$ even. $\\eta = 0.4, \\alpha = 0.9$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is consistent and well-defined.\n- **Scientifically Grounded:** The problem is set within the standard framework of online convex optimization, using well-established concepts like mirror descent, regret, and Euclidean projection. All definitions are correct.\n- **Well-Posed:** All parameters for the algorithms and data-generating processes are specified for each test case. The use of random seeds makes the \"random\" aspects deterministic and reproducible. A unique numerical solution exists for each case.\n- **Objective:** The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\n\nThe problem statement is declared **valid**. A solution will be provided.\n\n### Solution Derivation and Algorithmic Design\n\nThe task is to compute the difference in regret, $\\Delta \\mathrm{Regret}$, between a baseline and an adaptive-restart version of mirror descent.\n\n**1. Hindsight Comparator and Regret Simplification**\nThe regret is defined with respect to the best fixed decision in hindsight. For linear losses $f_t(x) = \\langle g_t, x \\rangle$, the cumulative loss for a fixed point $x \\in \\mathcal{K}$ is:\n$$\n\\sum_{t=1}^T f_t(x) = \\sum_{t=1}^T \\langle g_t, x \\rangle = \\left\\langle \\sum_{t=1}^T g_t, x \\right\\rangle = \\langle G_T, x \\rangle,\n$$\nwhere $G_T = \\sum_{t=1}^T g_t$ is the cumulative gradient. The best fixed decision in hindsight is the solution to the optimization problem:\n$$\nx^* = \\arg\\min_{x \\in \\mathcal{K}} \\langle G_T, x \\rangle.\n$$\nThe set $\\mathcal{K}$ is the Euclidean ball $\\{x \\in \\mathbb{R}^d : \\|x\\|_2 \\le R\\}$. By the Cauchy-Schwarz inequality, $\\langle G_T, x \\rangle \\ge -\\|G_T\\|_2 \\|x\\|_2$. Since $\\|x\\|_2 \\le R$, the minimum is achieved when $x$ is antiparallel to $G_T$ and has maximal norm, i.e., $\\|x\\|_2 = R$. Thus, the optimal comparator is:\n$$\nx^* = -R \\frac{G_T}{\\|G_T\\|_2} \\quad (\\text{if } G_T \\neq 0).\n$$\nThe minimum possible cumulative loss is $\\min_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x) = \\langle G_T, x^* \\rangle = -R \\|G_T\\|_2$.\n\nThe regrets for the two algorithms are:\n$$\n\\mathrm{Regret}_T^{\\text{baseline}} = \\sum_{t=1}^T f_t(x_t^{\\text{baseline}}) - (-R \\|G_T\\|_2)\n$$\n$$\n\\mathrm{Regret}_T^{\\text{restart}} = \\sum_{t=1}^T f_t(x_t^{\\text{restart}}) - (-R \\|G_T\\|_2)\n$$\nThe target metric, $\\Delta \\mathrm{Regret}$, simplifies to the difference in total incurred loss, as the hindsight term is common to both and cancels out:\n$$\n\\Delta \\mathrm{Regret} = \\mathrm{Regret}_T^{\\text{restart}} - \\mathrm{Regret}_T^{\\text{baseline}} = \\sum_{t=1}^T f_t(x_t^{\\text{restart}}) - \\sum_{t=1}^T f_t(x_t^{\\text{baseline}}).\n$$\nThis is the quantity we will compute.\n\n**2. Algorithmic Implementation**\nBoth algorithms are variants of projected gradient descent, which is a special case of mirror descent with the Euclidean mirror map $\\psi(x) = \\frac{1}{2}\\|x\\|_2^2$.\n\n**Euclidean Projection:** The projection operator $\\Pi_{\\mathcal{K}}$ maps a point $y \\in \\mathbb{R}^d$ to the closest point in the ball $\\mathcal{K}$. This is given by:\n$$\n\\Pi_{\\mathcal{K}}(y) = \\begin{cases} y & \\text{if } \\|y\\|_2 \\le R \\\\ R \\frac{y}{\\|y\\|_2} & \\text{if } \\|y\\|_2 > R \\end{cases}\n$$\n\n**Algorithm Loop:** For each algorithm, we initialize $x_1 = 0$. Then, for each round $t=1, \\dots, T$, we perform the following steps:\n1.  Incur loss at the current point $x_t$: $\\text{loss}_t = \\langle g_t, x_t \\rangle$.\n2.  Compute the next point $x_{t+1}$ based on the algorithm's specific update rule.\n\n**Baseline Algorithm:** The update rule is a standard projected gradient step:\n$$\nx_{t+1} = \\Pi_{\\mathcal{K}}(x_t - \\eta g_t).\n$$\n\n**Adaptive-Restart Algorithm:** The update depends on an anchor point $a_t$, which is either the current point $x_t$ or the origin $0$.\n- For $t=1$, the anchor is $a_1 = x_1 = 0$.\n- For $t \\ge 2$, the anchor $a_t$ is determined by the cosine of the angle between the current gradient $g_t$ and the previous one $g_{t-1}$. If the gradients are sufficiently anti-aligned, a restart is triggered.\n  - Test condition: $\\langle g_t, g_{t-1} \\rangle \\le - \\alpha \\, \\|g_t\\|_2 \\, \\|g_{t-1}\\|_2$.\n    - If true, restart: $a_t = 0$.\n    - If false, continue: $a_t = x_t$.\nThe update is then performed from this anchor point:\n$$\nx_{t+1} = \\Pi_{\\mathcal{K}}(a_t - \\eta g_t).\n$$\n\n**Implementation Plan:** For each test case, we first generate the full sequence of $T$ gradient vectors according to the specified parameters. Then, we run two separate simulations, one for each algorithm, over this same gradient sequence. In each simulation, we compute the total loss $\\sum_{t=1}^T \\langle g_t, x_t \\rangle$. Finally, we compute their difference to find $\\Delta \\mathrm{Regret}$. This procedure is repeated for all five test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates baseline and adaptive-restart mirror descent\n    for online convex optimization over a Euclidean ball.\n    \"\"\"\n\n    def run_algorithm(gradients, d, R, eta, alpha, mode):\n        \"\"\"\n        Runs a single online optimization algorithm.\n\n        Args:\n            gradients (list of np.ndarray): The sequence of gradient vectors g_t.\n            d (int): Dimension of the space.\n            R (float): Radius of the Euclidean ball.\n            eta (float): Step-size.\n            alpha (float or None): Restart threshold for 'restart' mode.\n            mode (str): Either 'baseline' or 'restart'.\n\n        Returns:\n            float: The total loss incurred by the algorithm.\n        \"\"\"\n        T = len(gradients)\n        x = np.zeros(d)  # This is x_1\n        total_loss = 0.0\n\n        for t in range(T):\n            g = gradients[t]  # This is g_{t+1} in 1-based indexing\n            \n            # Incur loss at the current point x (which is x_{t+1})\n            total_loss += np.dot(g, x)\n            \n            # Determine the point from which to update\n            update_point = x\n            if mode == 'restart':\n                # The restart rule for x_{t+2} (in 1-based) uses g_{t+1} and g_t.\n                # In our 0-indexed loop at step `t`, this corresponds to\n                # checking gradients[t] (current) and gradients[t-1] (previous).\n                # The problem statement says the decision for x_{t+1} is based\n                # on g_t and g_{t-1}. So at step t (which uses g_t), we check g_t and g_{t-1}.\n                if t > 0:\n                    g_current = gradients[t]\n                    g_prev = gradients[t-1]\n                    norm_g_curr = np.linalg.norm(g_current)\n                    norm_g_prev = np.linalg.norm(g_prev)\n                    \n                    if norm_g_curr > 1e-9 and norm_g_prev > 1e-9:\n                        dot_prod = np.dot(g_current, g_prev)\n                        # The problem defines a_t based on g_t and g_{t-1}.\n                        # The update for x_{t+1} uses a_t and g_t.\n                        # So at step `t` (computing x for t+1), we use a_t\n                        # determined by g_t and g_{t-1}.\n                        if dot_prod = -alpha * norm_g_curr * norm_g_prev:\n                            update_point = np.zeros(d) # Restart\n            \n            # Compute x_{t+2} in 1-based, or x for next iter in 0-based\n            y_next = update_point - eta * g\n            norm_y = np.linalg.norm(y_next)\n\n            if norm_y > R:\n                x = R * y_next / norm_y\n            else:\n                x = y_next\n                \n        return total_loss\n\n    test_cases_params = [\n        # Case 1 (trend with low noise)\n        {'d': 5, 'T': 200, 'sigma': 0.02, 'seed': 11, 'eta': 0.2, 'alpha': 0.0, 'case_type': 'trend'},\n        # Case 2 (pure noise)\n        {'d': 10, 'T': 400, 'sigma': 0.2, 'seed': 22, 'eta': 0.2, 'alpha': 0.0, 'case_type': 'noise'},\n        # Case 3 (small trend with higher noise)\n        {'d': 20, 'T': 400, 'v_norm': 0.05, 'sigma': 0.3, 'seed': 33, 'eta': 0.15, 'alpha': 0.5, 'case_type': 'mixed'},\n        # Case 4 (boundary case with a single round)\n        {'d': 3, 'T': 1, 'sigma': 0.0, 'seed': None, 'eta': 0.5, 'alpha': 0.0, 'case_type': 'boundary'},\n        # Case 5 (adversarial alternating gradients)\n        {'d': 2, 'T': 50, 'sigma': 0.0, 'seed': None, 'eta': 0.4, 'alpha': 0.9, 'case_type': 'adversarial'},\n    ]\n\n    R = 1.0\n    results = []\n\n    for params in test_cases_params:\n        d = params['d']\n        T = params['T']\n        eta = params['eta']\n        alpha = params['alpha']\n        sigma = params['sigma']\n        \n        # Generate gradients for the case\n        gradients = []\n        if params['case_type'] == 'trend':\n            rng = np.random.default_rng(params['seed'])\n            v = 0.3 * np.ones(d)\n            for _ in range(T):\n                noise = rng.normal(scale=sigma, size=d)\n                gradients.append(v + noise)\n        elif params['case_type'] == 'noise':\n            rng = np.random.default_rng(params['seed'])\n            for _ in range(T):\n                gradients.append(rng.normal(scale=sigma, size=d))\n        elif params['case_type'] == 'mixed':\n            rng = np.random.default_rng(params['seed'])\n            v_tilde = rng.normal(size=d)\n            v = params['v_norm'] * v_tilde / np.linalg.norm(v_tilde)\n            # Re-seed for the noise sequence to be independent of v direction generation but deterministic\n            rng_noise = np.random.default_rng(params['seed'])\n            for _ in range(T):\n                noise = rng_noise.normal(scale=sigma, size=d)\n                gradients.append(v + noise)\n        elif params['case_type'] == 'boundary':\n            v = 0.5 * np.ones(d)\n            gradients.append(v)\n        elif params['case_type'] == 'adversarial':\n            g_odd = np.zeros(d); g_odd[0] = 1.0\n            g_even = np.zeros(d); g_even[0] = -1.0\n            for t in range(1, T + 1):\n                if t % 2 != 0:\n                    gradients.append(g_odd)\n                else:\n                    gradients.append(g_even)\n\n        # Run both algorithms\n        loss_baseline = run_algorithm(gradients, d, R, eta, alpha, mode='baseline')\n        loss_restart = run_algorithm(gradients, d, R, eta, alpha, mode='restart')\n\n        # Delta Regret is equal to Delta Loss\n        delta_regret = loss_restart - loss_baseline\n        results.append(delta_regret)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3159468"}]}