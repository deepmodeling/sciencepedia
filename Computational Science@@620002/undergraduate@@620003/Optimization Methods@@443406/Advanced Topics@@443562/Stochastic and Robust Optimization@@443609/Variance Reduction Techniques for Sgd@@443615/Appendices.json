{"hands_on_practices": [{"introduction": "One of the most elegant ways to reduce variance is by introducing negative correlation. This exercise explores the method of antithetic variates, where we intentionally pair a sample with its 'opposite' to cancel out noise. By analyzing a simple linear regression model, you will quantify precisely how much variance can be eliminated compared to standard independent sampling, providing a foundational insight into the power of correlated estimators. [@problem_id:3197150]", "problem": "Consider one-dimensional linear regression with squared loss used in Stochastic Gradient Descent (SGD). For a single sample $(x,y)$, define the residual $r$ and the per-sample loss and gradient at parameter $\\theta$ by\n$$\nr \\equiv \\theta x - y,\\quad \\ell(\\theta;x,y) \\equiv \\tfrac{1}{2} r^{2},\\quad g(\\theta;x,y) \\equiv \\frac{\\partial \\ell}{\\partial \\theta} = x r.\n$$\nAssume the data are generated by the model\n$$\nx \\sim \\mathcal{N}(0,\\sigma_{x}^{2}),\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2}),\\quad y = \\theta^{\\ast} x + \\varepsilon,\n$$\nwith $x$ and $\\varepsilon$ independent, and suppose the current iterate is at the true parameter $\\theta = \\theta^{\\ast}$. You estimate the gradient using a mini-batch of size $2$.\n\nTwo mini-batch constructions are considered:\n- Independent and identically distributed (i.i.d.) sampling: draw two independent samples $(x_{1},y_{1})$ and $(x_{2},y_{2})$, and use the average gradient estimator\n$$\n\\widehat{g}_{\\mathrm{iid}} \\equiv \\tfrac{1}{2}\\big(g(\\theta;x_{1},y_{1}) + g(\\theta;x_{2},y_{2})\\big).\n$$\n- Antithetic pairing: for a given draw of $(x,\\varepsilon)$, form two samples that share the same $x$ but have opposite residual signs by taking $y_{1} \\equiv \\theta^{\\ast} x + \\varepsilon$ and $y_{2} \\equiv \\theta^{\\ast} x - \\varepsilon$, and use the average gradient estimator\n$$\n\\widehat{g}_{\\mathrm{anti}} \\equiv \\tfrac{1}{2}\\big(g(\\theta;x,y_{1}) + g(\\theta;x,y_{2})\\big).\n$$\n\nStarting from the above definitions and the stated data model, derive the variances $\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}})$ and $\\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}})$ at $\\theta = \\theta^{\\ast}$, and compute the expected gradient variance reduction defined as\n$$\n\\Delta \\equiv \\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) - \\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}}).\n$$\nExpress your final answer as a closed-form analytic expression in terms of $\\sigma_{x}$ and $\\sigma_{\\varepsilon}$. No rounding is required.", "solution": "The problem asks for the variance of two different mini-batch gradient estimators for one-dimensional linear regression and the difference between these variances. The analysis is to be conducted at the true parameter $\\theta = \\theta^{\\ast}$.\n\nFirst, let us analyze the stochastic gradient for a single sample $(x, y)$ when the parameter is at the true value $\\theta = \\theta^{\\ast}$. The problem states the data generating process is $y = \\theta^{\\ast} x + \\varepsilon$, where $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ are independent.\n\nThe residual at $\\theta = \\theta^{\\ast}$ is given by:\n$$\nr = \\theta^{\\ast} x - y = \\theta^{\\ast} x - (\\theta^{\\ast} x + \\varepsilon) = -\\varepsilon\n$$\nThe per-sample gradient $g(\\theta; x, y) = x r$ evaluated at $\\theta = \\theta^{\\ast}$ is therefore:\n$$\ng(\\theta^{\\ast}; x, y) = x(-\\varepsilon) = -x\\varepsilon\n$$\nLet us denote this stochastic gradient as $g^{\\ast} \\equiv -x\\varepsilon$. We will need its statistical properties. The expectation of $g^{\\ast}$ is:\n$$\n\\mathbb{E}[g^{\\ast}] = \\mathbb{E}[-x\\varepsilon] = -\\mathbb{E}[x]\\mathbb{E}[\\varepsilon]\n$$\nThis factorization is possible because $x$ and $\\varepsilon$ are independent. Given that $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$, their means are $\\mathbb{E}[x] = 0$ and $\\mathbb{E}[\\varepsilon] = 0$. Thus,\n$$\n\\mathbb{E}[g^{\\ast}] = -(0)(0) = 0\n$$\nThe variance of $g^{\\ast}$ is $\\mathrm{Var}(g^{\\ast}) = \\mathbb{E}[(g^{\\ast})^{2}] - (\\mathbb{E}[g^{\\ast}])^{2}$. Since the mean is $0$, this simplifies to:\n$$\n\\mathrm{Var}(g^{\\ast}) = \\mathbb{E}[(g^{\\ast})^{2}] = \\mathbb{E}[(-x\\varepsilon)^{2}] = \\mathbb{E}[x^{2}\\varepsilon^{2}]\n$$\nDue to the independence of $x$ and $\\varepsilon$, $x^{2}$ and $\\varepsilon^{2}$ are also independent. Therefore, we can write:\n$$\n\\mathrm{Var}(g^{\\ast}) = \\mathbb{E}[x^{2}]\\mathbb{E}[\\varepsilon^{2}]\n$$\nFor a random variable $Z$ with mean $\\mu_{Z}$ and variance $\\sigma_{Z}^{2}$, we know $\\mathrm{Var}(Z) = \\mathbb{E}[Z^{2}] - \\mu_{Z}^{2}$, which implies $\\mathbb{E}[Z^{2}] = \\mathrm{Var}(Z) + \\mu_{Z}^{2}$.\nFor $x$, we have $\\mathbb{E}[x] = 0$ and $\\mathrm{Var}(x) = \\sigma_{x}^{2}$, so $\\mathbb{E}[x^{2}] = \\sigma_{x}^{2} + 0^{2} = \\sigma_{x}^{2}$.\nFor $\\varepsilon$, we have $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2}$, so $\\mathbb{E}[\\varepsilon^{2}] = \\sigma_{\\varepsilon}^{2} + 0^{2} = \\sigma_{\\varepsilon}^{2}$.\nSubstituting these into the expression for the variance of $g^{\\ast}$:\n$$\n\\mathrm{Var}(g^{\\ast}) = \\sigma_{x}^{2} \\sigma_{\\varepsilon}^{2}\n$$\n\nNow we can compute the variance for the two mini-batch estimators.\n\n1.  **I.I.D. Sampling Estimator, $\\widehat{g}_{\\mathrm{iid}}$**\n\nThe i.i.d. estimator is defined as $\\widehat{g}_{\\mathrm{iid}} = \\frac{1}{2}(g_{1} + g_{2})$, where $g_{1} = g(\\theta^{\\ast}; x_{1}, y_{1})$ and $g_{2} = g(\\theta^{\\ast}; x_{2}, y_{2})$ are gradients from two independent and identically distributed samples. This means $(x_{1}, \\varepsilon_{1})$ and $(x_{2}, \\varepsilon_{2})$ are independent draws.\nThus, $g_{1} = -x_{1}\\varepsilon_{1}$ and $g_{2} = -x_{2}\\varepsilon_{2}$ are i.i.d. random variables. Each has the same distribution as $g^{\\ast}$.\nThe variance of the average of two independent random variables is:\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) = \\mathrm{Var}\\left(\\frac{1}{2}(g_{1} + g_{2})\\right) = \\left(\\frac{1}{2}\\right)^{2} \\mathrm{Var}(g_{1} + g_{2})\n$$\nSince $g_{1}$ and $g_{2}$ are independent, $\\mathrm{Var}(g_{1} + g_{2}) = \\mathrm{Var}(g_{1}) + \\mathrm{Var}(g_{2})$.\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) = \\frac{1}{4} (\\mathrm{Var}(g_{1}) + \\mathrm{Var}(g_{2}))\n$$\nAs $g_{1}$ and $g_{2}$ are identically distributed, $\\mathrm{Var}(g_{1}) = \\mathrm{Var(g_{2})} = \\mathrm{Var}(g^{\\ast}) = \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}$.\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) = \\frac{1}{4} (\\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2} + \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}) = \\frac{1}{4} (2 \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}) = \\frac{1}{2} \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}\n$$\n\n2.  **Antithetic Pairing Estimator, $\\widehat{g}_{\\mathrm{anti}}$**\n\nThe antithetic estimator is formed using a single draw of $(x, \\varepsilon)$ to construct two samples.\nSample 1: $(x, y_{1})$ with $y_{1} = \\theta^{\\ast} x + \\varepsilon$.\nSample 2: $(x, y_{2})$ with $y_{2} = \\theta^{\\ast} x - \\varepsilon$.\n\nThe gradient for the first sample, which we denote as $g'_{1}$, is:\n$$\ng'_{1} = g(\\theta^{\\ast}; x, y_{1}) = x(\\theta^{\\ast} x - y_{1}) = x(\\theta^{\\ast} x - (\\theta^{\\ast} x + \\varepsilon)) = -x\\varepsilon\n$$\nThe gradient for the second sample, which we denote as $g'_{2}$, is:\n$$\ng'_{2} = g(\\theta^{\\ast}; x, y_{2}) = x(\\theta^{\\ast} x - y_{2}) = x(\\theta^{\\ast} x - (\\theta^{\\ast} x - \\varepsilon)) = x\\varepsilon\n$$\nThe antithetic gradient estimator is the average of these two gradients:\n$$\n\\widehat{g}_{\\mathrm{anti}} = \\frac{1}{2}(g'_{1} + g'_{2}) = \\frac{1}{2}(-x\\varepsilon + x\\varepsilon) = \\frac{1}{2}(0) = 0\n$$\nSince the estimator $\\widehat{g}_{\\mathrm{anti}}$ is identically zero for any realization of the random variables $x$ and $\\varepsilon$, it is a constant. The variance of a constant is zero.\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}}) = \\mathrm{Var}(0) = 0\n$$\n\n3.  **Expected Gradient Variance Reduction, $\\Delta$**\n\nFinally, we compute the variance reduction $\\Delta$ by subtracting the variance of the antithetic estimator from the variance of the i.i.d. estimator.\n$$\n\\Delta = \\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) - \\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}})\n$$\nSubstituting the derived variances:\n$$\n\\Delta = \\frac{1}{2} \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2} - 0 = \\frac{1}{2} \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}\n$$\nThis result quantifies the reduction in gradient variance achieved by using antithetic sampling instead of standard i.i.d. sampling for a mini-batch of size $2$ at the optimal parameter $\\theta^{\\ast}$. In this specific idealized setting, the antithetic estimator completely eliminates the variance.", "answer": "$$\n\\boxed{\\frac{1}{2}\\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}}\n$$", "id": "3197150"}, {"introduction": "Beyond creating artificial sample pairs, we can reduce variance by being smarter about how we draw samples from our dataset. This exercise introduces stratified sampling, a powerful technique that is particularly useful for imbalanced datasets. You will derive the optimal probabilities for sampling from different data strata (e.g., classes) and implement this scheme to see firsthand how it can lead to a more stable and efficient gradient estimator for logistic regression. [@problem_id:3197205]", "problem": "You are given a binary classification dataset with imbalanced labels and asked to analyze the variance of stochastic gradient estimators under different sampling schemes in Stochastic Gradient Descent (SGD). Work strictly from the foundational definitions of empirical risk minimization and unbiased gradient estimation, and avoid using any shortcut formulas not derived from first principles.\n\nConsider the empirical risk of logistic regression for binary labels, defined as follows. Let there be $n$ samples $\\{(x_i,y_i)\\}_{i=1}^n$ with feature vectors $x_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{0,1\\}$. The logistic loss for a single sample is\n$$\n\\ell(w; x_i, y_i) = \\log\\big(1 + e^{\\langle w, x_i \\rangle}\\big) - y_i \\langle w, x_i \\rangle,\n$$\nand the empirical risk is\n$$\nL(w) = \\frac{1}{n}\\sum_{i=1}^n \\ell(w; x_i, y_i).\n$$\nLet the sigmoid function be $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The per-sample gradient is\n$$\ng_i(w) = \\nabla_w \\ell(w; x_i, y_i) = \\big(\\sigma(\\langle w, x_i\\rangle) - y_i\\big)\\, x_i,\n$$\nand the full gradient is\n$$\ng(w) = \\nabla_w L(w) = \\frac{1}{n}\\sum_{i=1}^n g_i(w).\n$$\n\nA single-sample unbiased stochastic gradient estimator is constructed by sampling an index $i$ from $\\{1,\\dots,n\\}$ according to a probability mass function $p_i$ and returning\n$$\n\\widehat{g}(w) = \\frac{1}{n p_i} g_i(w),\n$$\nwhich satisfies $\\mathbb{E}[\\widehat{g}(w)] = g(w)$ by linearity of expectation when $p_i > 0$ for all $i$.\n\nYou must devise a stratified sampling scheme for a binary classification dataset with imbalanced labels as follows:\n- Partition the indices into two strata according to the label: $S_0 = \\{i \\mid y_i = 0\\}$ and $S_1 = \\{i \\mid y_i = 1\\}$, with sizes $n_0 = |S_0|$ and $n_1 = |S_1|$ satisfying $n_0 + n_1 = n$.\n- First sample a stratum $c \\in \\{0,1\\}$ with probability $q_c$.\n- Then, within the chosen stratum $c$, sample uniformly an index $i \\in S_c$.\n- Use the importance-weighted estimator $\\widehat{g}(w)$ with $p_i = q_c / n_c$ and the weight $\\frac{1}{n p_i}$ to preserve unbiasedness.\n\nFrom the definition of variance as the expected squared deviation from the mean,\n$$\n\\mathrm{Var}(\\widehat{g}(w)) = \\mathbb{E}\\big[\\|\\widehat{g}(w) - g(w)\\|_2^2\\big] = \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] - \\|g(w)\\|_2^2,\n$$\nderive from first principles an explicit expression for $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$ under the above stratified scheme. Then, choose the stratum probabilities $q_0$ and $q_1$ to minimize the variance subject to the constraints $q_0 + q_1 = 1$ and $q_0, q_1 > 0$, using only valid mathematical reasoning (e.g., Lagrange multipliers and properties of expectation). Do not introduce any unjustified formulas.\n\nImplement a complete program that:\n- Constructs the following test suite of datasets and parameter vectors.\n- For each test case, computes the uniform-sampling variance (where $p_i = 1/n$ and the estimator is $\\widehat{g}(w) = g_i(w)$) and the variance under your derived stratified scheme.\n- Outputs, for each test case, a single float equal to the ratio of stratified variance to uniform variance.\n- If the uniform variance is zero for a test case, define the ratio to be $1.0$ for that case.\n- Prints a single line containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_1$,$r_2$,$r_3$]\"), with each float rounded to six decimal places.\n\nUse the following test suite. Each test case specifies $(X,y,w)$ explicitly.\n\n- Test case $1$ (moderate imbalance):\n  - $n = 8$, $d = 2$.\n  - Positive-label indices $S_1 = \\{1,2,3\\}$ with $y_1 = 1$, $y_2 = 1$, $y_3 = 1$ and feature vectors\n    $x_1 = [\\,2.0,\\,0.0\\,]$, $x_2 = [\\,1.5,\\,-0.5\\,]$, $x_3 = [\\,1.0,\\,0.5\\,]$.\n  - Negative-label indices $S_0 = \\{4,5,6,7,8\\}$ with $y_4 = 0$, $y_5 = 0$, $y_6 = 0$, $y_7 = 0$, $y_8 = 0$ and feature vectors\n    $x_4 = [\\,-1.0,\\,0.5\\,]$, $x_5 = [\\,-2.0,\\,1.0\\,]$, $x_6 = [\\,-1.5,\\,-0.5\\,]$, $x_7 = [\\,-0.5,\\,0.2\\,]$, $x_8 = [\\,-1.0,\\,-1.0\\,]$.\n  - Parameter vector $w = [\\,0.2,\\,-0.3\\,]$.\n\n- Test case $2$ (extreme imbalance):\n  - $n = 10$, $d = 2$.\n  - Positive-label indices $S_1 = \\{10\\}$ with $y_{10} = 1$ and $x_{10} = [\\,3.0,\\,3.0\\,]$.\n  - Negative-label indices $S_0 = \\{1,2,3,4,5,6,7,8,9\\}$ with $y_i = 0$ for $i \\in S_0$ and feature vectors\n    $x_1 = [\\,-0.1,\\,0.2\\,]$, $x_2 = [\\,-0.2,\\,0.1\\,]$, $x_3 = [\\,-0.3,\\,-0.1\\,]$, $x_4 = [\\,-0.1,\\,-0.2\\,]$, $x_5 = [\\,-0.2,\\,-0.2\\,]$, $x_6 = [\\,-0.05,\\,0.05\\,]$, $x_7 = [\\,-0.15,\\,0.0\\,]$, $x_8 = [\\,-0.1,\\,0.0\\,]$, $x_9 = [\\,-0.25,\\,0.1\\,]$.\n  - Parameter vector $w = [\\,0.1,\\,0.1\\,]$.\n\n- Test case $3$ (balanced and symmetric):\n  - $n = 4$, $d = 2$.\n  - Negative-label indices $S_0 = \\{1,2\\}$ with $y_1 = 0$, $y_2 = 0$ and feature vectors $x_1 = [\\,1.0,\\,0.0\\,]$, $x_2 = [\\,0.0,\\,1.0\\,]$.\n  - Positive-label indices $S_1 = \\{3,4\\}$ with $y_3 = 1$, $y_4 = 1$ and feature vectors $x_3 = [\\,-1.0,\\,0.0\\,]$, $x_4 = [\\,0.0,\\,-1.0\\,]$.\n  - Parameter vector $w = [\\,0.0,\\,0.0\\,]$.\n\n- Test case $4$ (degenerate zero-gradient case):\n  - $n = 6$, $d = 2$.\n  - Feature vectors $x_i = [\\,0.0,\\,0.0\\,]$ for all $i \\in \\{1,2,3,4,5,6\\}$.\n  - Labels $y = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,0\\,]$.\n  - Parameter vector $w = [\\,1.0,\\,-1.0\\,]$.\n\nAngle units and physical units do not apply. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example, \"[0.732145,0.512340,1.000000,1.000000]\".", "solution": "The problem requires the derivation of an optimal stratified sampling scheme for variance reduction in Stochastic Gradient Descent (SGD) and a comparison with uniform sampling for specific test cases. The solution proceeds in two parts: first, a formal derivation from first principles, and second, the implementation of the derived formulas to compute the required variance ratios.\n\n### Part 1: Derivation of the Optimal Stratified Sampling Scheme\n\nThe variance of a single-sample stochastic gradient estimator $\\widehat{g}(w)$ is given by:\n$$\n\\mathrm{Var}(\\widehat{g}(w)) = \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] - \\|g(w)\\|_2^2\n$$\nSince $\\|g(w)\\|_2^2$ is a constant with respect to the sampling process, minimizing the variance is equivalent to minimizing the expected squared L2-norm of the estimator, $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$.\n\n**1. Expression for $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$ under Stratified Sampling**\n\nThe stratified sampling scheme is defined as a two-stage process:\n1.  A stratum $c \\in \\{0, 1\\}$ is selected with probability $q_c$.\n2.  An index $i$ is sampled uniformly from the chosen stratum $S_c$.\n\nThe probability of sampling a specific index $i \\in S_c$ is the product of the probability of choosing stratum $c$ and the probability of choosing $i$ within $S_c$:\n$$\np_i = P(\\text{sample } i) = P(\\text{sample } i \\mid \\text{choose } S_c) P(\\text{choose } S_c) = \\frac{1}{|S_c|} \\cdot q_c = \\frac{q_c}{n_c}\n$$\nThe unbiased stochastic gradient estimator for a sampled index $i$ is:\n$$\n\\widehat{g}(w) = \\frac{1}{n p_i} g_i(w) = \\frac{n_c}{n q_c} g_i(w), \\quad \\text{for } i \\in S_c\n$$\nThe expectation $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$ is taken over this two-stage sampling process. We can compute it using the law of total expectation, conditioning on the chosen stratum $c$:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] = \\sum_{c \\in \\{0,1\\}} P(\\text{stratum } c \\text{ chosen}) \\cdot \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid \\text{stratum } c \\text{ chosen}\\big]\n$$\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] = q_0 \\cdot \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c=0\\big] + q_1 \\cdot \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c=1\\big]\n$$\nGiven that stratum $c$ is chosen, the index $i$ is sampled uniformly from $S_c$. The estimator is $\\widehat{g}(w) = \\frac{n_c}{n q_c} g_i(w)$. The conditional expectation is:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c\\big] = \\mathbb{E}_{i \\sim \\text{Unif}(S_c)}\\left[\\left\\|\\frac{n_c}{n q_c} g_i(w)\\right\\|_2^2\\right] = \\left(\\frac{n_c}{n q_c}\\right)^2 \\mathbb{E}_{i \\sim \\text{Unif}(S_c)}\\left[\\|g_i(w)\\|_2^2\\right]\n$$\nThe expectation over a uniform sample from $S_c$ is simply the average over the elements of $S_c$:\n$$\n\\mathbb{E}_{i \\sim \\text{Unif}(S_c)}\\left[\\|g_i(w)\\|_2^2\\right] = \\frac{1}{n_c} \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c\\big] = \\left(\\frac{n_c}{n q_c}\\right)^2 \\left(\\frac{1}{n_c} \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2\\right) = \\frac{n_c}{n^2 q_c^2} \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2\n$$\nLet us define $V_c = \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2$ as the sum of squared L2-norms of the gradients within stratum $c$. The conditional expectation becomes $\\frac{n_c V_c}{n^2 q_c^2}$.\n\nPlugging this into the law of total expectation formula gives:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] = q_0 \\left(\\frac{n_0 V_0}{n^2 q_0^2}\\right) + q_1 \\left(\\frac{n_1 V_1}{n^2 q_1^2}\\right) = \\frac{1}{n^2} \\left( \\frac{n_0 V_0}{q_0} + \\frac{n_1 V_1}{q_1} \\right)\n$$\nThis is the explicit expression for the expected squared norm of the estimator.\n\n**2. Minimizing the Variance**\n\nWe need to minimize $f(q_0, q_1) = \\frac{n_0 V_0}{q_0} + \\frac{n_1 V_1}{q_1}$ subject to $q_0 + q_1 = 1$ and $q_0, q_1 > 0$.\nSubstitute $q_1 = 1 - q_0$ to get a function of a single variable $q_0 \\in (0,1)$:\n$$\nh(q_0) = \\frac{n_0 V_0}{q_0} + \\frac{n_1 V_1}{1 - q_0}\n$$\nTo find the minimum, we take the derivative with respect to $q_0$ and set it to zero. This is valid if $V_0 > 0$ and $V_1 > 0$, ensuring the critical point is in $(0, 1)$.\n$$\n\\frac{dh}{dq_0} = -\\frac{n_0 V_0}{q_0^2} + \\frac{n_1 V_1}{(1 - q_0)^2} = 0\n$$\n$$\n\\frac{n_1 V_1}{(1 - q_0)^2} = \\frac{n_0 V_0}{q_0^2} \\implies \\frac{q_0^2}{(1 - q_0)^2} = \\frac{n_0 V_0}{n_1 V_1}\n$$\nTaking the square root of both sides (and since $q_0, 1-q_0 > 0$):\n$$\n\\frac{q_0}{1 - q_0} = \\frac{\\sqrt{n_0 V_0}}{\\sqrt{n_1 V_1}}\n$$\nSolving for $q_0$:\n$$\nq_0 \\sqrt{n_1 V_1} = (1 - q_0)\\sqrt{n_0 V_0} = \\sqrt{n_0 V_0} - q_0 \\sqrt{n_0 V_0}\n$$\n$$\nq_0 (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}) = \\sqrt{n_0 V_0}\n$$\nThe optimal stratum probabilities are:\n$$\nq_0^* = \\frac{\\sqrt{n_0 V_0}}{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}} \\quad \\text{and} \\quad q_1^* = \\frac{\\sqrt{n_1 V_1}}{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}}\n$$\n\n**3. Minimal Expected Squared Norm and Variance**\n\nSubstituting these optimal probabilities back into the expression for $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$:\n$$\n\\mathbb{E}_{\\text{strat}}^*[\\|\\widehat{g}(w)\\|_2^2] = \\frac{1}{n^2} \\left( \\frac{n_0 V_0}{q_0^*} + \\frac{n_1 V_1}{q_1^*} \\right)\n$$\nThe first term is $\\frac{n_0 V_0}{q_0^*} = n_0 V_0 \\cdot \\frac{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}}{\\sqrt{n_0 V_0}} = \\sqrt{n_0 V_0} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})$.\nThe second term is $\\frac{n_1 V_1}{q_1^*} = n_1 V_1 \\cdot \\frac{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}}{\\sqrt{n_1 V_1}} = \\sqrt{n_1 V_1} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})$.\nSumming them gives $(\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})^2$. Therefore, the minimal expected squared norm is:\n$$\n\\mathbb{E}_{\\text{strat}}^*[\\|\\widehat{g}(w)\\|_2^2] = \\frac{1}{n^2} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})^2\n$$\nThe minimized variance under optimal stratified sampling is:\n$$\n\\mathrm{Var}_{\\text{strat}}(\\widehat{g}(w)) = \\frac{1}{n^2} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})^2 - \\|g(w)\\|_2^2\n$$\nIf $V_0=V_1=0$, all gradients are zero, this variance is zero.\n\n**4. Comparison with Uniform Sampling**\n\nFor uniform sampling, $p_i = 1/n$ for all $i$. The estimator simplifies to $\\widehat{g}_{\\text{unif}}(w) = g_i(w)$. The expected squared norm is:\n$$\n\\mathbb{E}_{\\text{unif}}[\\|\\widehat{g}(w)\\|_2^2] = \\mathbb{E}_{i \\sim \\text{Unif}}\\left[\\|g_i(w)\\|_2^2\\right] = \\frac{1}{n} \\sum_{i=1}^n \\|g_i(w)\\|_2^2 = \\frac{1}{n} (V_0 + V_1)\n$$\nThe variance under uniform sampling is:\n$$\n\\mathrm{Var}_{\\text{unif}}(\\widehat{g}(w)) = \\frac{1}{n} (V_0 + V_1) - \\|g(w)\\|_2^2\n$$\n\nThe required ratio is $\\mathrm{Var}_{\\text{strat}} / \\mathrm{Var}_{\\text{unif}}$.\n\n### Part 2: Implementation for Test Cases\n\nThe following Python code implements these derived formulas to calculate the variance ratio for each test case. The logic computes the per-sample gradients, the stratum-wise sums of squared norms ($V_0, V_1$), the full gradient norm, and finally the two variances to find their ratio. Special cases, such as zero uniform variance, are handled as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variance reduction problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Test case 1: Moderate imbalance\n        {\n            \"X\": np.array([\n                [2.0, 0.0], [1.5, -0.5], [1.0, 0.5],  # y=1\n                [-1.0, 0.5], [-2.0, 1.0], [-1.5, -0.5], [-0.5, 0.2], [-1.0, -1.0]  # y=0\n            ]),\n            \"y\": np.array([1, 1, 1, 0, 0, 0, 0, 0]),\n            \"w\": np.array([0.2, -0.3])\n        },\n        # Test case 2: Extreme imbalance\n        {\n            \"X\": np.array([\n                [-0.1, 0.2], [-0.2, 0.1], [-0.3, -0.1], [-0.1, -0.2], [-0.2, -0.2],\n                [-0.05, 0.05], [-0.15, 0.0], [-0.1, 0.0], [-0.25, 0.1], # y=0\n                [3.0, 3.0] # y=1\n            ]),\n            \"y\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n            \"w\": np.array([0.1, 0.1])\n        },\n        # Test case 3: Balanced and symmetric\n        {\n            \"X\": np.array([\n                [1.0, 0.0], [0.0, 1.0],  # y=0\n                [-1.0, 0.0], [0.0, -1.0] # y=1\n            ]),\n            \"y\": np.array([0, 0, 1, 1]),\n            \"w\": np.array([0.0, 0.0])\n        },\n        # Test case 4: Degenerate zero-gradient case\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], \n                [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]\n            ]),\n            \"y\": np.array([0, 0, 0, 1, 1, 0]),\n            \"w\": np.array([1.0, -1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, w = case[\"X\"], case[\"y\"], case[\"w\"]\n        \n        n, d = X.shape\n\n        # Sigmoid function\n        def sigmoid(z):\n            return 1.0 / (1.0 + np.exp(-z))\n\n        # 1. Compute per-sample gradients g_i(w)\n        w_dot_x = X @ w\n        sigmas = sigmoid(w_dot_x)\n        # Reshape for broadcasting: (n,) -> (n, 1) to multiply with X (n, d)\n        coeff = (sigmas - y)[:, np.newaxis]\n        per_sample_grads = coeff * X\n\n        # 2. Compute full gradient g(w) and its squared norm\n        full_grad = np.mean(per_sample_grads, axis=0)\n        full_grad_norm_sq = np.dot(full_grad, full_grad)\n\n        # 3. Compute sum of squared L2-norms of gradients per stratum (V_0, V_1)\n        per_sample_grad_norms_sq = np.sum(per_sample_grads**2, axis=1)\n        \n        indices_S0 = np.where(y == 0)[0]\n        indices_S1 = np.where(y == 1)[0]\n        \n        n0 = len(indices_S0)\n        n1 = len(indices_S1)\n\n        V0 = np.sum(per_sample_grad_norms_sq[indices_S0])\n        V1 = np.sum(per_sample_grad_norms_sq[indices_S1])\n\n        # 4. Compute variance for uniform sampling\n        # E[||g_hat||^2] for uniform sampling\n        E_unif_norm_sq = np.sum(per_sample_grad_norms_sq) / n\n        var_unif = E_unif_norm_sq - full_grad_norm_sq\n\n        # Handle the case where uniform variance is zero\n        if np.isclose(var_unif, 0.0):\n            results.append(1.0)\n            continue\n\n        # 5. Compute variance for optimal stratified sampling\n        # E[||g_hat||^2] for stratified sampling\n        # This formula is numerically stable even if V0 or V1 is 0\n        term_0 = np.sqrt(n0 * V0) if n0 > 0 else 0.0\n        term_1 = np.sqrt(n1 * V1) if n1 > 0 else 0.0\n        \n        E_strat_norm_sq = (term_0 + term_1)**2 / n**2\n        var_strat = E_strat_norm_sq - full_grad_norm_sq\n        \n        # 6. Compute and store the ratio\n        ratio = var_strat / var_unif\n        results.append(ratio)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3197205"}, {"introduction": "Modern variance reduction techniques often rely on a powerful idea called control variates, which forms the basis for methods like SVRG. The strategy is to correct a noisy stochastic gradient using a less noisy, but related, reference gradient. This exercise delves into the mechanics of a control variate estimator, showing how a 'snapshot' of the full gradient can drastically reduce variance, especially when the individual loss functions are similar. Your analysis will reveal the mathematical engine that drives the rapid convergence of these advanced optimization algorithms. [@problem_id:3197167]", "problem": "Consider a finite-sum objective in one dimension,\n$$\nf(x) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} f_i(x),\n\\quad\n\\text{where}\n\\quad\nf_i(x) \\;=\\; \\frac{1}{2}\\,(q + \\delta_i)\\,(x - a_i)^2,\n$$\nwith $n \\in \\mathbb{N}$ components that are nearly identical in the following precise sense: the curvatures $q + \\delta_i$ are perturbations of a common $q > 0$, where the random variables $\\delta_i$ satisfy $\\mathbb{E}[\\delta_i] = 0$ and $\\operatorname{Var}(\\delta_i) = \\sigma_{\\delta}^2$, and the shifts $a_i$ have mean $\\bar{a}$ and small dispersion around $\\bar{a}$. Assume $(\\delta_i)$ and $(a_i)$ are independent and identically distributed across $i$, mutually independent, and that $\\sum_{i=1}^{n} \\delta_i (\\,\\bar{a}-a_i\\,) = 0$, which holds in expectation under the independence and zero-mean assumptions.\n\nWe study a variance-reduced Stochastic Gradient Descent (SGD) step that uses a full-gradient snapshot at $x_{\\tilde{}} = \\bar{a}$:\n$$\ng_k \\;=\\; \\nabla f_{i_k}(x_k) \\;-\\; \\nabla f_{i_k}(x_{\\tilde{}}) \\;+\\; \\nabla f(x_{\\tilde{}}),\n$$\nwhere $i_k$ is sampled uniformly from $\\{1,\\dots,n\\}$ at iteration $k$, and the update is\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\eta\\, g_k,\n$$\nwith a constant stepsize $\\eta > 0$ satisfying $0 < \\eta < \\frac{2\\,q}{q^2 + \\sigma_{\\delta}^2}$ so that the second-moment recursion derived below is contractive. Define the snapshot displacement $d_k := x_k - x_{\\tilde{}}$.\n\nTasks:\n- Using only the definitions above and basic properties of variance, show that with $x_{\\tilde{}}=\\bar{a}$ and the stated independence assumptions, the variance of the variance-reduced estimator conditional on $x_k$ is\n$$\n\\operatorname{Var}\\!\\left[g_k \\,\\big|\\, x_k\\right] \\;=\\; \\sigma_{\\delta}^2\\, d_k^2.\n$$\n- Derive the one-step second-moment recursion for $d_k$, and from it, compute the per-iteration variance decay factor, defined as the ratio\n$$\nR \\;:=\\; \\frac{\\mathbb{E}\\left[\\,\\operatorname{Var}\\!\\left(g_{k+1}\\,\\big|\\,x_{k+1}\\right)\\,\\right]}{\\mathbb{E}\\left[\\,\\operatorname{Var}\\!\\left(g_{k}\\,\\big|\\,x_{k}\\right)\\,\\right]},\n$$\nunder the assumption that $i_k$ is independent of $d_k$ for each $k$.\n\nExplain, based on your derivation, why full gradient snapshots are especially effective at variance reduction when heterogeneity is low. Express your final answer as a single closed-form analytic expression for $R$ in terms of $q$, $\\sigma_{\\delta}^2$, and $\\eta$. No numerical rounding is required. The final answer must be a single expression.", "solution": "The solution requires completing two tasks: first, deriving the variance of the stochastic gradient estimator, and second, finding the per-iteration variance decay factor.\n\nFirst, let us define the necessary gradient terms. The gradient of a single component function $f_i(x)$ is:\n$$\n\\nabla f_i(x) = \\frac{d}{dx} \\left( \\frac{1}{2}(q + \\delta_i)(x - a_i)^2 \\right) = (q + \\delta_i)(x - a_i)\n$$\nThe full gradient of the objective function $f(x)$ is the average of the component gradients:\n$$\n\\nabla f(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(x) = \\frac{1}{n} \\sum_{i=1}^{n} (q + \\delta_i)(x - a_i)\n$$\nThe variance-reduced gradient estimator is given by:\n$$\ng_k = \\nabla f_{i_k}(x_k) - \\nabla f_{i_k}(x_{\\tilde{}}) + \\nabla f(x_{\\tilde{}})\n$$\nA crucial step is to evaluate the full gradient at the snapshot point $x_{\\tilde{}} = \\bar{a}$.\n$$\n\\nabla f(x_{\\tilde{}}) = \\nabla f(\\bar{a}) = \\frac{1}{n} \\sum_{i=1}^{n} (q + \\delta_i)(\\bar{a} - a_i) = \\frac{q}{n} \\sum_{i=1}^{n} (\\bar{a} - a_i) + \\frac{1}{n} \\sum_{i=1}^{n} \\delta_i(\\bar{a} - a_i)\n$$\nThe problem statement provides the condition $\\sum_{i=1}^{n} \\delta_i (\\,\\bar{a}-a_i\\,) = 0$, which makes the second term zero. The first term involves $\\bar{a}$, which is the mean of the distribution of the shifts $a_i$. For a finite-sum problem, it is standard to assume that the sample mean equals the true mean, i.e., $\\frac{1}{n} \\sum_{i=1}^{n} a_i = \\bar{a}$. Under this standard assumption, the first term also becomes zero: $\\frac{q}{n} \\sum_{i=1}^{n} (\\bar{a} - a_i) = q (\\bar{a} - \\frac{1}{n}\\sum_{i=1}^{n} a_i) = q(\\bar{a} - \\bar{a}) = 0$.\nThus, the snapshot gradient is zero:\n$$\n\\nabla f(x_{\\tilde{}}) = 0\n$$\nThis significantly simplifies the estimator $g_k$ to:\n$$\ng_k = \\nabla f_{i_k}(x_k) - \\nabla f_{i_k}(x_{\\tilde{}})\n$$\nLet's substitute the gradient expression and the definition $d_k = x_k - x_{\\tilde{}} = x_k - \\bar{a}$:\n$$\ng_k = (q + \\delta_{i_k})(x_k - a_{i_k}) - (q + \\delta_{i_k})(x_{\\tilde{}} - a_{i_k}) = (q + \\delta_{i_k}) \\left( (x_k - a_{i_k}) - (x_{\\tilde{}} - a_{i_k}) \\right)\n$$\n$$\ng_k = (q + \\delta_{i_k}) (x_k - x_{\\tilde{}}) = (q + \\delta_{i_k}) d_k\n$$\n\n**Task 1: Variance of the Gradient Estimator**\n\nWe need to find $\\operatorname{Var}\\!\\left[g_k \\,\\big|\\, x_k\\right]$. The randomness in $g_k$, conditional on $x_k$ (and thus on $d_k$), comes from the choice of index $i_k$. The problem specifies that $(\\delta_i)$ are i.i.d. random variables with $\\mathbb{E}[\\delta_i] = 0$ and $\\operatorname{Var}(\\delta_i) = \\sigma_{\\delta}^2$. When we compute the moments of $g_k$, the expectation is over the random choice of a component from the underlying data-generating distribution.\nLet's compute the conditional expectation of $g_k$:\n$$\n\\mathbb{E}\\left[g_k \\,\\big|\\, x_k\\right] = \\mathbb{E}_{i_k}\\left[ (q + \\delta_{i_k}) d_k \\,\\big|\\, d_k \\right] = d_k \\mathbb{E}[q + \\delta] = d_k (q + \\mathbb{E}[\\delta]) = q d_k\n$$\nThe conditional variance is defined as $\\operatorname{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\nFirst, the second moment of $g_k$:\n$$\n\\mathbb{E}\\left[g_k^2 \\,\\big|\\, x_k\\right] = \\mathbb{E}_{i_k}\\left[ ((q + \\delta_{i_k}) d_k)^2 \\,\\big|\\, d_k \\right] = d_k^2 \\mathbb{E}[(q + \\delta)^2]\n$$\nWe know that $\\operatorname{Var}(\\delta) = \\mathbb{E}[\\delta^2] - (\\mathbb{E}[\\delta])^2$. Since $\\mathbb{E}[\\delta]=0$, we have $\\mathbb{E}[\\delta^2] = \\sigma_\\delta^2$.\nAlso, $\\mathbb{E}[(q + \\delta)^2] = \\mathbb{E}[q^2 + 2q\\delta + \\delta^2] = q^2 + 2q\\mathbb{E}[\\delta] + \\mathbb{E}[\\delta^2] = q^2 + \\sigma_\\delta^2$.\nSo, $\\mathbb{E}\\left[g_k^2 \\,\\big|\\, x_k\\right] = d_k^2 (q^2 + \\sigma_\\delta^2)$.\nNow we can compute the variance:\n$$\n\\operatorname{Var}\\!\\left[g_k \\,\\big|\\, x_k\\right] = \\mathbb{E}\\left[g_k^2 \\,\\big|\\, x_k\\right] - \\left(\\mathbb{E}\\left[g_k \\,\\big|\\, x_k\\right]\\right)^2 = d_k^2(q^2 + \\sigma_\\delta^2) - (q d_k)^2\n$$\n$$\n\\operatorname{Var}\\!\\left[g_k \\,\\big|\\, x_k\\right] = q^2 d_k^2 + \\sigma_\\delta^2 d_k^2 - q^2 d_k^2 = \\sigma_\\delta^2 d_k^2\n$$\nThis completes the first task.\n\n**Task 2: Second-Moment Recursion and Variance Decay Factor**\n\nWe start with the update rule for the displacement $d_k = x_k - \\bar{a}$:\n$$\nd_{k+1} = x_{k+1} - \\bar{a} = (x_k - \\eta g_k) - \\bar{a} = (x_k - \\bar{a}) - \\eta g_k = d_k - \\eta g_k\n$$\nWe want to find the recursion for the expected squared displacement, $\\mathbb{E}[d_k^2]$. Let $\\mathbb{E}_k[\\cdot]$ denote the expectation over the randomness at step $k$ (the choice of $i_k$), conditional on $x_k$.\n$$\n\\mathbb{E}_k[d_{k+1}^2 | x_k] = \\mathbb{E}_k[(d_k - \\eta g_k)^2 | x_k] = d_k^2 - 2\\eta d_k \\mathbb{E}_k[g_k | x_k] + \\eta^2 \\mathbb{E}_k[g_k^2 | x_k]\n$$\nWe use the conditional moments derived earlier: $\\mathbb{E}_k[g_k | x_k] = q d_k$ and $\\mathbb{E}_k[g_k^2 | x_k] = (q^2 + \\sigma_\\delta^2)d_k^2$.\nSubstituting these into the equation:\n$$\n\\mathbb{E}_k[d_{k+1}^2 | x_k] = d_k^2 - 2\\eta d_k (q d_k) + \\eta^2 (q^2 + \\sigma_\\delta^2)d_k^2\n$$\n$$\n\\mathbb{E}_k[d_{k+1}^2 | x_k] = \\left( 1 - 2\\eta q + \\eta^2(q^2 + \\sigma_\\delta^2) \\right) d_k^2\n$$\nTo get the recursion for the unconditional expectation, we take the expectation over all previous steps (i.e., over the distribution of $x_k$):\n$$\n\\mathbb{E}[d_{k+1}^2] = \\mathbb{E}[\\mathbb{E}_k[d_{k+1}^2 | x_k]] = \\mathbb{E}\\left[ \\left( 1 - 2\\eta q + \\eta^2(q^2 + \\sigma_\\delta^2) \\right) d_k^2 \\right]\n$$\n$$\n\\mathbb{E}[d_{k+1}^2] = \\left( 1 - 2\\eta q + \\eta^2(q^2 + \\sigma_\\delta^2) \\right) \\mathbb{E}[d_k^2]\n$$\nThis is the one-step second-moment recursion for $d_k$. Now, we compute the variance decay factor $R$:\n$$\nR = \\frac{\\mathbb{E}\\left[\\,\\operatorname{Var}\\!\\left(g_{k+1}\\,\\big|\\,x_{k+1}\\right)\\,\\right]}{\\mathbb{E}\\left[\\,\\operatorname{Var}\\!\\left(g_{k}\\,\\big|\\,x_{k}\\right)\\,\\right]}\n$$\nUsing our result from Task 1, $\\operatorname{Var}[g_k | x_k] = \\sigma_\\delta^2 d_k^2$, we can write the numerator and the denominator:\n$$\n\\text{Denominator:} \\quad \\mathbb{E}\\left[\\,\\operatorname{Var}\\!\\left(g_{k}\\,\\big|\\,x_{k}\\right)\\,\\right] = \\mathbb{E}[\\sigma_\\delta^2 d_k^2] = \\sigma_\\delta^2 \\mathbb{E}[d_k^2]\n$$\n$$\n\\text{Numerator:} \\quad \\mathbb{E}\\left[\\,\\operatorname{Var}\\!\\left(g_{k+1}\\,\\big|\\,x_{k+1}\\right)\\,\\right] = \\mathbb{E}[\\sigma_\\delta^2 d_{k+1}^2] = \\sigma_\\delta^2 \\mathbb{E}[d_{k+1}^2]\n$$\nThe ratio $R$ is therefore:\n$$\nR = \\frac{\\sigma_\\delta^2 \\mathbb{E}[d_{k+1}^2]}{\\sigma_\\delta^2 \\mathbb{E}[d_k^2]} = \\frac{\\mathbb{E}[d_{k+1}^2]}{\\mathbb{E}[d_k^2]}\n$$\nSubstituting the recursion relationship:\n$$\nR = \\frac{\\left( 1 - 2\\eta q + \\eta^2(q^2 + \\sigma_\\delta^2) \\right) \\mathbb{E}[d_k^2]}{\\mathbb{E}[d_k^2]} = 1 - 2\\eta q + \\eta^2(q^2 + \\sigma_\\delta^2)\n$$\n\n**Explanation on Effectiveness with Low Heterogeneity**\n\nThe effectiveness of the full-gradient snapshot for variance reduction is clearly demonstrated by these results. The variance of the estimator, $\\operatorname{Var}[g_k | x_k] = \\sigma_\\delta^2 d_k^2$, is proportional to two quantities:\n1.  $\\sigma_\\delta^2$: The variance of the curvatures of the component functions, which measures their heterogeneity.\n2.  $d_k^2 = (x_k - \\bar{a})^2$: The squared distance of the current iterate to the mean of the optimal points.\n\nThe variance naturally diminishes to zero as the algorithm converges (i.e., as $x_k \\to \\bar{a}$ and $d_k \\to 0$). This is a key advantage over standard SGD, whose variance typically converges to a positive constant.\n\nWhen heterogeneity is low, $\\sigma_\\delta^2$ is small. This has two profound effects:\n- The variance $\\sigma_\\delta^2 d_k^2$ is small throughout the optimization process, not just near the solution.\n- The per-iteration variance decay factor, $R = 1 - 2\\eta q + \\eta^2(q^2 + \\sigma_\\delta^2)$, becomes smaller. To see this, consider the step-size $\\eta_{opt} = \\frac{q}{q^2 + \\sigma_\\delta^2}$ that minimizes $R$. The corresponding optimal decay factor is $R_{opt} = \\frac{\\sigma_\\delta^2}{q^2 + \\sigma_\\delta^2}$. As $\\sigma_\\delta^2 \\to 0$ (the low heterogeneity limit), $R_{opt} \\to 0$. A decay factor close to zero implies that the variance is reduced extremely rapidly from one iteration to the next. In the ideal case of zero heterogeneity ($\\sigma_\\delta^2=0$), the variance is completely eliminated, and the algorithm converges in one step for this quadratic problem. The snapshot, by centering the update around a \"mean\" behavior, effectively cancels the dominant sources of variance, an effect that is most pronounced when the individual components deviate only slightly from this mean.", "answer": "$$\\boxed{1 - 2\\eta q + \\eta^2(q^2 + \\sigma_{\\delta}^2)}$$", "id": "3197167"}]}