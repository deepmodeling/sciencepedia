{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise in robust optimization, guiding you through the conversion of an uncertain problem into a solvable, deterministic equivalent. This process, known as deriving the robust counterpart, is a core skill for handling uncertainty [@problem_id:3173984]. By applying the concept of dual norms, you will see how a shared \"budget\" of uncertainty affecting multiple constraints translates into a simple and elegant safety margin, providing a clear recipe for immunization.", "problem": "Consider an uncertain linear inequality system with an $m \\times n$ matrix $A$, a vector $b \\in \\mathbb{R}^m$, and a decision vector $x \\in \\mathbb{R}^n$. The uncertainty is confined to the right-hand side and is modeled as $A x \\le b + \\Delta b$ where the perturbation vector $\\Delta b \\in \\mathbb{R}^m$ belongs to the set $\\{\\Delta b : \\|\\Delta b\\|_{1} \\le \\rho\\}$ for a given budget $\\rho \\ge 0$. The robust feasibility requirement is that all $m$ inequalities hold for every $\\Delta b$ in this set.\n\nStarting from the definition of robust feasibility and the definition of the dual norm of the $\\ell_{1}$-norm, derive a deterministic equivalent (robust counterpart) that is free of $\\Delta b$ and that must hold on $x$ to guarantee robust feasibility. Prove that there exists a single scalar safety margin $\\tau(\\rho) \\ge 0$, depending only on $\\rho$, such that the robust counterpart can be expressed as $A x \\le b - \\tau(\\rho)\\,\\mathbf{1}_{m}$, where $\\mathbf{1}_{m} \\in \\mathbb{R}^{m}$ is the vector of ones. Determine the exact closed-form expression of $\\tau(\\rho)$.\n\nBriefly explain, using your derivation, how the shared budget $\\|\\Delta b\\|_{1} \\le \\rho$ induces redundancy across constraints in the robust counterpart.\n\nYour final answer must be the single closed-form expression for $\\tau(\\rho)$, with no units and no rounding.", "solution": "The problem is to find the deterministic robust counterpart of an uncertain linear inequality system and to determine a specific safety margin function $\\tau(\\rho)$.\n\n### Step 1: Problem Validation\n\nThe problem statement is a standard exercise in robust optimization theory.\n**Givens:**\n- An uncertain linear inequality system $A x \\le b + \\Delta b$, where $A$ is an $m \\times n$ matrix, $x \\in \\mathbb{R}^n$ is the decision vector, and $b \\in \\mathbb{R}^m$.\n- The uncertainty vector $\\Delta b \\in \\mathbb{R}^m$ belongs to the uncertainty set $U = \\{\\Delta b \\in \\mathbb{R}^m : \\|\\Delta b\\|_{1} \\le \\rho\\}$, where $\\rho \\ge 0$ is a given budget of uncertainty. The norm $\\|\\cdot\\|_1$ is the $\\ell_1$-norm, defined as $\\|\\Delta b\\|_1 = \\sum_{i=1}^m |(\\Delta b)_i|$.\n- Robust feasibility requirement: The inequality $A x \\le b + \\Delta b$ must hold for every $\\Delta b \\in U$.\n- The target form for the robust counterpart is $A x \\le b - \\tau(\\rho)\\,\\mathbf{1}_{m}$, where $\\mathbf{1}_{m}$ is the $m$-dimensional vector of ones.\n\n**Validation:**\n- **Scientifically Grounded:** The problem is firmly rooted in the mathematical field of robust optimization, a sub-discipline of optimization methods. The concepts of $\\ell_p$-norms, dual norms, and robust counterparts are well-established.\n- **Well-Posed:** The problem is clearly stated and asks for a derivation of a specific result. It is self-contained and has a unique, well-defined solution.\n- **Objective:** The language is formal, mathematical, and free of any subjectivity or ambiguity.\n\nThe problem is valid as it satisfies all criteria for a well-posed scientific problem. It is not speculative, incomplete, contradictory, or trivial.\n\n### Step 2: Derivation of the Robust Counterpart\n\nThe robust feasibility requirement states that the inequality system $A x \\le b + \\Delta b$ must hold for all possible realizations of the uncertainty vector $\\Delta b$ within its defined set $U$.\nLet $a_i^T$ be the $i$-th row of the matrix $A$, so that $(Ax)_i = a_i^T x$. The robust feasibility condition can be written as:\n$$ a_i^T x \\le b_i + (\\Delta b)_i, \\quad \\forall i \\in \\{1, \\dots, m\\}, \\quad \\forall \\Delta b \\in U = \\{\\Delta b : \\|\\Delta b\\|_1 \\le \\rho\\} $$\nThe universal quantifier $\\forall \\Delta b \\in U$ applies to the entire set of $m$ inequalities. We can swap the order of the universal quantifiers $\\forall i$ and $\\forall \\Delta b$ without changing the logical statement. This allows us to decouple the problem and consider each constraint individually. For each constraint $i \\in \\{1, \\dots, m\\}$, the inequality $a_i^T x \\le b_i + (\\Delta b)_i$ must hold for all $\\Delta b$ in the set $U$.\n\nFor a fixed decision vector $x$ and a fixed constraint index $i$, the term $a_i^T x - b_i$ is a constant. The inequality must hold even for the \"worst-case\" choice of $\\Delta b \\in U$, which is the one that minimizes the right-hand side, $b_i + (\\Delta b)_i$. Therefore, for each $i$, we must satisfy:\n$$ a_i^T x \\le b_i + \\min_{\\Delta b \\in U} \\{(\\Delta b)_i\\} $$\nThe problem reduces to evaluating the minimum of the $i$-th component of $\\Delta b$ over the uncertainty set. Let $e_i \\in \\mathbb{R}^m$ be the $i$-th standard basis vector (a vector of zeros with a $1$ in the $i$-th position). Then $(\\Delta b)_i$ can be written as the inner product $e_i^T \\Delta b$. The optimization problem is:\n$$ \\min_{\\|\\Delta b\\|_1 \\le \\rho} e_i^T \\Delta b $$\nThis is equivalent to:\n$$ - \\max_{\\|\\Delta b\\|_1 \\le \\rho} (-e_i^T \\Delta b) = - \\max_{\\|\\Delta b\\|_1 \\le \\rho} \\Delta b^T (-e_i) $$\nWe now use the definition of a dual norm. The dual of the $\\ell_1$-norm is the $\\ell_\\infty$-norm, denoted $\\|\\cdot\\|_\\infty$. The duality relationship for a generic vector $v$ is given by:\n$$ \\sup_{\\|\\Delta b\\|_1 \\le \\rho} \\Delta b^T v = \\rho \\|v\\|_\\infty $$\nIn our case, the vector $v$ is $-e_i$. We compute its $\\ell_\\infty$-norm:\n$$ \\|-e_i\\|_\\infty = \\max_{j=1, \\dots, m} |(-e_i)_j| = 1 $$\nSubstituting this result back into our expression, we find the maximum:\n$$ \\max_{\\|\\Delta b\\|_1 \\le \\rho} \\Delta b^T (-e_i) = \\rho \\|-e_i\\|_\\infty = \\rho \\cdot 1 = \\rho $$\nTherefore, the minimum value we sought is:\n$$ \\min_{\\|\\Delta b\\|_1 \\le \\rho} (\\Delta b)_i = -\\rho $$\nThis minimum value is achieved by the specific perturbation $\\Delta b^* = -\\rho e_i$, which satisfies $\\|\\Delta b^*\\|_1 = |-\\rho| = \\rho$ and is thus an element of the uncertainty set $U$.\n\nSubstituting this minimum value back into the inequality for the $i$-th constraint gives its deterministic robust counterpart:\n$$ a_i^T x \\le b_i - \\rho $$\nSince this must hold for all $i \\in \\{1, \\dots, m\\}$, we can write the complete robust counterpart in vector form:\n$$ A x \\le b - \\rho\\,\\mathbf{1}_{m} $$\nwhere $\\mathbf{1}_{m} \\in \\mathbb{R}^{m}$ is the vector of all ones.\n\n### Step 3: Determination of $\\tau(\\rho)$\n\nThe problem asks for the robust counterpart to be expressed in the form $A x \\le b - \\tau(\\rho)\\,\\mathbf{1}_{m}$. By comparing this with our derived robust counterpart $A x \\le b - \\rho\\,\\mathbf{1}_{m}$, we can directly identify the closed-form expression for the safety margin $\\tau(\\rho)$.\n$$ \\tau(\\rho) = \\rho $$\n\n### Step 4: Explanation of Redundancy\n\nThe derived robust counterpart $A x \\le b - \\rho\\,\\mathbf{1}_{m}$ is the exact, non-conservative deterministic equivalent of the original uncertain problem. The \"redundancy\" mentioned in the problem statement refers to the interpretation of the structure of this counterpart in light of the shared uncertainty budget.\n\nOur derivation shows that the robust constraint for each row $i$ is $a_i^T x \\le b_i - \\rho$. This hardening of the constraint by an amount $\\rho$ is necessary to protect against the specific worst-case perturbation for that row, which is $\\Delta b = -\\rho e_i$. This perturbation directs the entire uncertainty budget $\\rho$ to adversely affect the $i$-th constraint while leaving all other constraints $j \\neq i$ unperturbed (since $(\\Delta b)_j=0$).\n\nThe shared budget constraint $\\|\\Delta b\\|_1 \\le \\rho$ creates a coupling between the perturbations affecting different inequalities. For instance, if the perturbation acting on constraint $i=1$ is large, say $|(\\Delta b)_1| = \\rho$, then it must be that $(\\Delta b)_j = 0$ for all $j > 1$. It is impossible for two constraints, say $i=1$ and $i=2$, to simultaneously experience their individual worst-case perturbations ($\\Delta b = -\\rho e_1$ and $\\Delta b = -\\rho e_2$, respectively), because both of these scenarios exhaust the entire shared budget.\n\nThe robust counterpart, being a set of $m$ inequalities $\\{a_i^T x \\le b_i - \\rho\\}_{i=1}^m$, effectively ensures that the solution $x$ is feasible against each of the $m$ mutually exclusive worst-case scenarios. The formulation is thus structurally redundant in the sense that it prepares for a set of $m$ extreme events, only one of which can possibly occur at any given time. The total \"cost\" of robustness, measured by the sum of the subtractions from the right-hand sides, is $m\\rho$, even though the total uncertainty \"supply\" is limited to $\\rho$. This illustrates how the shared budget induces a form of redundancy in the system of constraints that constitutes the robust counterpart.", "answer": "$$ \\boxed{\\rho} $$", "id": "3173984"}, {"introduction": "The effectiveness of a robust solution heavily depends on how accurately the uncertainty set models reality. This practice provides a crucial lesson in modeling by comparing two approaches: a sophisticated model that captures correlations between uncertain parameters and a naive one that ignores them [@problem_id:3173992]. By solving both versions of the problem, you will quantify the \"price of ignorance\" and understand why ignoring structure in your data's uncertainty can lead to overly conservative and suboptimal decisions.", "problem": "Consider the following robust optimization (RO) model with correlated coefficient uncertainty. You are given a two-variable linear program with an uncertain linear constraint. Let the decision vector be $x = (x_1, x_2)^\\top$ with $x_1 \\ge 0$ and $x_2 \\ge 0$. The nominal coefficient vector in the constraint is $a_0 = (1, 1)^\\top$, and the uncertain perturbation is a vector $z \\in \\mathbb{R}^2$. The objective is to maximize the sum of the decision variables. The uncertain constraint must hold for all perturbations $z$ in a specified uncertainty set:\n- True correlated uncertainty set (ellipsoidal): $U_{\\text{ellip}} = \\{ z \\in \\mathbb{R}^2 : z^\\top \\Sigma^{-1} z \\le 1 \\}$ with $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ and $\\rho = -0.8$.\n- Naive interval-inflated uncertainty set (axis-aligned box): $U_{\\text{box}} = \\{ z \\in \\mathbb{R}^2 : |z_1| \\le 1, |z_2| \\le 1 \\}$, obtained by inflating to independent marginal intervals that bound $U_{\\text{ellip}}$.\n\nThe robust optimization problem is\nmaximize $x_1 + x_2$ subject to $(a_0 + z)^\\top x \\le b$ for all $z \\in U$ and $x_1 \\ge 0$, $x_2 \\ge 0$, with $b = 4$ and $U$ chosen as either $U_{\\text{ellip}}$ (true correlated robust counterpart) or $U_{\\text{box}}$ (naive interval inflation).\n\nStarting from the definition of robust feasibility, the properties of supremum over sets, and the Cauchy–Schwarz inequality, derive the two robust counterparts (one for $U_{\\text{ellip}}$ and one for $U_{\\text{box}}$). Then solve both robust optimization problems exactly and compute the ratio $R$ of the optimal objective value under naive interval inflation to the optimal objective value under the true correlated robust counterpart,\n$$\nR \\equiv \\frac{V_{\\text{box}}}{V_{\\text{ellip}}}.\n$$\nThis ratio exhibits a concrete counterexample where naive interval inflation is dominated by the true robust counterpart under correlated uncertainty.\n\nRound your final numerical answer for $R$ to four significant figures. No units are required.", "solution": "The problem asks us to analyze a two-variable linear program under uncertainty, derive its robust counterparts for two different uncertainty sets, solve these counterparts, and compute the ratio of their optimal objective values.\n\nThe robust optimization problem is given by:\n$$\n\\begin{aligned}\n\\text{maximize} \\quad & x_1 + x_2 \\\\\n\\text{subject to} \\quad & (a_0 + z)^\\top x \\le b \\quad \\forall z \\in U \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\nwhere the decision vector is $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, the nominal coefficient vector is $a_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and the right-hand side is $b=4$.\n\nThe core of the problem lies in reformulating the uncertain constraint into a deterministic equivalent. The constraint must hold for all $z$ in the uncertainty set $U$:\n$$\n(a_0 + z)^\\top x \\le b \\quad \\forall z \\in U\n$$\nThis can be rewritten as:\n$$\na_0^\\top x + z^\\top x \\le b \\quad \\forall z \\in U\n$$\nThis is equivalent to ensuring that the nominal part plus the worst-case perturbation does not violate the bound:\n$$\na_0^\\top x + \\sup_{z \\in U} (z^\\top x) \\le b\n$$\nWe will now derive the robust counterpart for each specified uncertainty set $U$.\n\n**Part 1: Naive Interval Inflation ($U = U_{\\text{box}}$)**\n\nThe first uncertainty set is the axis-aligned box defined as:\n$$\nU_{\\text{box}} = \\{ z \\in \\mathbb{R}^2 : |z_1| \\le 1, |z_2| \\le 1 \\}\n$$\nWe need to compute the supremum term $\\sup_{z \\in U_{\\text{box}}} (z^\\top x)$.\n$$\n\\sup_{z \\in U_{\\text{box}}} (z^\\top x) = \\sup_{|z_1| \\le 1, |z_2| \\le 1} (z_1 x_1 + z_2 x_2)\n$$\nThe terms in the summation are separable:\n$$\n\\sup_{|z_1| \\le 1, |z_2| \\le 1} (z_1 x_1 + z_2 x_2) = \\sup_{|z_1| \\le 1} (z_1 x_1) + \\sup_{|z_2| \\le 1} (z_2 x_2)\n$$\nTo maximize $z_i x_i$ over $|z_i| \\le 1$, we should choose $z_i$ to have the same sign as $x_i$ and the maximum possible magnitude, which is $1$. Thus, the optimal choice for $z_i$ is $\\text{sgn}(x_i)$. The supremum of $z_i x_i$ is therefore $|\\text{sgn}(x_i) x_i| = |x_i|$.\nSo, we have:\n$$\n\\sup_{z \\in U_{\\text{box}}} (z^\\top x) = |x_1| + |x_2|\n$$\nSince the problem includes the constraints $x_1 \\ge 0$ and $x_2 \\ge 0$, this simplifies to:\n$$\n\\sup_{z \\in U_{\\text{box}}} (z^\\top x) = x_1 + x_2\n$$\nSubstituting this into the robust constraint formulation gives:\n$$\na_0^\\top x + (x_1 + x_2) \\le b\n$$\nWith $a_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $b=4$, the constraint becomes:\n$$\n(x_1 + x_2) + (x_1 + x_2) \\le 4\n$$\n$$\n2x_1 + 2x_2 \\le 4 \\implies x_1 + x_2 \\le 2\n$$\nThe robust optimization problem for the box uncertainty set is therefore a simple linear program:\n$$\n\\begin{aligned}\nV_{\\text{box}} = \\text{maximize} \\quad & x_1 + x_2 \\\\\n\\text{subject to} \\quad & x_1 + x_2 \\le 2 \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\nThe objective function is identical to the left-hand side of the main constraint. By inspection, the maximum value of $x_1 + x_2$ is $2$.\n$$\nV_{\\text{box}} = 2\n$$\n\n**Part 2: True Correlated Uncertainty ($U = U_{\\text{ellip}}$)**\n\nThe second uncertainty set is the ellipsoid defined by:\n$$\nU_{\\text{ellip}} = \\{ z \\in \\mathbb{R}^2 : z^\\top \\Sigma^{-1} z \\le 1 \\}\n$$\nwith $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ and $\\rho = -0.8$. The matrix $\\Sigma$ is positive definite as its determinant is $1-\\rho^2 = 1 - (-0.8)^2 = 0.36 > 0$.\n\nWe need to compute the supremum term $\\sup_{z \\in U_{\\text{ellip}}} (z^\\top x)$. This is a standard result in robust optimization. The supremum of a linear function over an ellipsoid is given by the dual norm.\n$$\n\\sup_{z^\\top \\Sigma^{-1} z \\le 1} (z^\\top x) = \\sqrt{x^\\top \\Sigma x}\n$$\nTo show this, we can use the Cauchy-Schwarz inequality. Let $y = \\Sigma^{-1/2} z$ and $w = \\Sigma^{1/2} x$. Then $z = \\Sigma^{1/2} y$. The constraint $z^\\top \\Sigma^{-1} z \\le 1$ becomes $(\\Sigma^{1/2} y)^\\top \\Sigma^{-1} (\\Sigma^{1/2} y) = y^\\top \\Sigma^{1/2} \\Sigma^{-1} \\Sigma^{1/2} y = y^\\top y = \\|y\\|_2^2 \\le 1$. The expression $z^\\top x$ becomes $(\\Sigma^{1/2} y)^\\top x = y^\\top (\\Sigma^{1/2} x) = y^\\top w$.\nThe problem is now to find $\\sup_{\\|y\\|_2 \\le 1} (y^\\top w)$. By the Cauchy-Schwarz inequality, $y^\\top w \\le \\|y\\|_2 \\|w\\|_2$. Since $\\|y\\|_2 \\le 1$, we have $y^\\top w \\le \\|w\\|_2$. The supremum is achieved when $y$ is aligned with $w$ and has norm $1$, so $\\sup_{\\|y\\|_2 \\le 1} (y^\\top w) = \\|w\\|_2$.\nSubstituting back $w = \\Sigma^{1/2} x$:\n$$\n\\sup_{z \\in U_{\\text{ellip}}} (z^\\top x) = \\|\\Sigma^{1/2} x\\|_2 = \\sqrt{(\\Sigma^{1/2} x)^\\top (\\Sigma^{1/2} x)} = \\sqrt{x^\\top \\Sigma x}\n$$\nNow we compute the quadratic form $x^\\top \\Sigma x$:\n$$\nx^\\top \\Sigma x = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1^2 + 2\\rho x_1 x_2 + x_2^2\n$$\nThe robust constraint for the ellipsoidal set is:\n$$\na_0^\\top x + \\sqrt{x_1^2 + 2\\rho x_1 x_2 + x_2^2} \\le b\n$$\nWith $a_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $b=4$, we get:\n$$\nx_1 + x_2 + \\sqrt{x_1^2 + 2\\rho x_1 x_2 + x_2^2} \\le 4\n$$\nThe robust optimization problem is a second-order cone program (SOCP):\n$$\n\\begin{aligned}\nV_{\\text{ellip}} = \\text{maximize} \\quad & x_1 + x_2 \\\\\n\\text{subject to} \\quad & x_1 + x_2 + \\sqrt{x_1^2 + 2\\rho x_1 x_2 + x_2^2} \\le 4 \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\nThe objective function and the constraint function are symmetric with respect to $x_1$ and $x_2$. The feasible set is convex. Therefore, an optimal solution must exist on the line $x_1 = x_2$. Let us set $x_1 = x_2 = x^*$, with $x^* \\ge 0$.\nThe objective function becomes $2x^*$. The constraint becomes:\n$$\nx^* + x^* + \\sqrt{(x^*)^2 + 2\\rho(x^*)^2 + (x^*)^2} \\le 4\n$$\n$$\n2x^* + \\sqrt{2(x^*)^2 + 2\\rho(x^*)^2} \\le 4\n$$\nSince $x^* \\ge 0$, we can factor it out of the square root:\n$$\n2x^* + x^*\\sqrt{2 + 2\\rho} \\le 4\n$$\n$$\nx^* (2 + \\sqrt{2(1+\\rho)}) \\le 4\n$$\nTo maximize the objective $2x^*$, we must choose the largest possible $x^*$, so we take the equality:\n$$\nx^* = \\frac{4}{2 + \\sqrt{2(1+\\rho)}}\n$$\nThe optimal objective value is $V_{\\text{ellip}} = 2x^*$:\n$$\nV_{\\text{ellip}} = \\frac{8}{2 + \\sqrt{2(1+\\rho)}}\n$$\nWe are given $\\rho = -0.8$. Substituting this value:\n$$\n1 + \\rho = 1 - 0.8 = 0.2\n$$\n$$\n2(1 + \\rho) = 2(0.2) = 0.4\n$$\n$$\nV_{\\text{ellip}} = \\frac{8}{2 + \\sqrt{0.4}} = \\frac{8}{2 + \\sqrt{4/10}} = \\frac{8}{2 + 2/\\sqrt{10}} = \\frac{4}{1 + 1/\\sqrt{10}} = \\frac{4\\sqrt{10}}{1+\\sqrt{10}}\n$$\n\n**Part 3: Compute the Ratio $R$**\n\nWe are asked to compute the ratio $R = \\frac{V_{\\text{box}}}{V_{\\text{ellip}}}$.\nWe found $V_{\\text{box}} = 2$ and $V_{\\text{ellip}} = \\frac{8}{2 + \\sqrt{0.4}}$.\n$$\nR = \\frac{2}{\\frac{8}{2 + \\sqrt{0.4}}} = \\frac{2(2 + \\sqrt{0.4})}{8} = \\frac{2 + \\sqrt{0.4}}{4}\n$$\nThis can be written as:\n$$\nR = \\frac{1}{2} + \\frac{\\sqrt{0.4}}{4} = \\frac{1}{2} + \\frac{\\sqrt{4/10}}{4} = \\frac{1}{2} + \\frac{2/\\sqrt{10}}{4} = \\frac{1}{2} + \\frac{1}{2\\sqrt{10}} = \\frac{1}{2} \\left( 1 + \\frac{1}{\\sqrt{10}} \\right)\n$$\nNow we compute the numerical value and round to four significant figures:\n$$\n\\sqrt{10} \\approx 3.16227766...\n$$\n$$\n\\frac{1}{\\sqrt{10}} \\approx 0.316227766...\n$$\n$$\nR \\approx \\frac{1}{2} (1 + 0.316227766) = \\frac{1}{2} (1.316227766) \\approx 0.658113883...\n$$\nRounding to four significant figures gives $R \\approx 0.6581$.\n\nThe ratio being less than $1$ indicates that the naive interval inflation model is overly conservative compared to the true correlated model. By ignoring the negative correlation between the uncertain parameters, the box uncertainty set includes unrealistic scenarios (like both parameters taking their maximum unfavorable values simultaneously), leading to a much more constrained problem and a lower optimal objective value.", "answer": "$$\n\\boxed{0.6581}\n$$", "id": "3173992"}, {"introduction": "Robust optimization has deep and powerful connections to other fields, most notably machine learning. This hands-on exercise reveals one such connection: the formal equivalence between making a model robust to input data uncertainty and the common practice of regularization [@problem_id:3173939]. You will derive this relationship mathematically, discovering that adding a penalty term to a model's objective function is mathematically identical to immunizing the model against a specific type of data perturbation.", "problem": "You are asked to empirically compare robustification and regularization for linear models under uncertainty within the framework of robust optimization. Work with linear regression using the least absolute deviations loss, defined on a dataset with $n$ samples and $d$ features by the objective $\\sum_{i=1}^{n} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right|$, where $\\mathbf{x}_i \\in \\mathbb{R}^{d}$ is the feature vector for sample $i$, $y_i \\in \\mathbb{R}$ is the response, and $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector to be learned.\n\nBase your derivation and algorithm on the following foundation:\n- Robust optimization models uncertainty by optimizing the worst-case objective over an uncertainty set. For each sample $i$, assume an additive feature perturbation $\\boldsymbol{\\delta}_i \\in \\mathbb{R}^{d}$ to the features, so the perturbed feature becomes $\\mathbf{x}_i + \\boldsymbol{\\delta}_i$. The robust objective is then $\\sum_{i=1}^{n} \\max_{\\boldsymbol{\\delta}_i \\in \\mathcal{U}} \\left| y_i - (\\mathbf{x}_i + \\boldsymbol{\\delta}_i)^{\\top} \\mathbf{w} \\right|$, where $\\mathcal{U}$ is a norm ball uncertainty set.\n- Use the definition of the dual norm: for any norm $\\|\\cdot\\|$, its dual norm $\\|\\cdot\\|_{*}$ is defined by $\\|\\mathbf{v}\\|_{*} = \\sup_{\\|\\mathbf{u}\\| \\leq 1} \\mathbf{u}^{\\top}\\mathbf{v}$.\n- Use the support function of a norm ball: for radius $\\rho \\geq 0$ and any vector $\\mathbf{v}$, $\\sup_{\\|\\boldsymbol{\\delta}\\| \\leq \\rho} \\boldsymbol{\\delta}^{\\top}\\mathbf{v} = \\rho \\|\\mathbf{v}\\|_{*}$.\n\nYour tasks:\n1. Starting from the above definitions, derive the equivalence between the robustified least absolute deviations objective with norm-ball feature uncertainty and a regularized least absolute deviations objective that adds a norm penalty on $\\mathbf{w}$. Clearly identify, through duality, the relation between the regularization strength $\\lambda$ and the uncertainty radius $\\rho$ as a function of the number of samples $n$ and the chosen norm. Do not use any shortcut formulas; derive the result from the stated base definitions.\n2. Implement two linear programming solvers for the regularized least absolute deviations problem:\n   - Case A (norm type $\\ell_{\\infty}$): minimize $\\sum_{i=1}^{n} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| + \\lambda \\|\\mathbf{w}\\|_{1}$.\n   - Case B (norm type $\\ell_{1}$): minimize $\\sum_{i=1}^{n} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| + \\lambda \\|\\mathbf{w}\\|_{\\infty}$.\n   Each solver must be formulated as a linear program using standard variable-splitting for absolute values.\n3. Implement a robustified solver by replacing the inner maximization with its support function equivalent according to your derivation in Task $1$. For norm type $\\ell_{\\infty}$, the uncertainty set is $\\{ \\boldsymbol{\\delta} : \\|\\boldsymbol{\\delta}\\|_{\\infty} \\leq \\rho \\}$, and for norm type $\\ell_{1}$, the uncertainty set is $\\{ \\boldsymbol{\\delta} : \\|\\boldsymbol{\\delta}\\|_{1} \\leq \\rho \\}$.\n4. Generate synthetic data for each test case as follows: for given $n$, $d$, and a fixed seed, draw $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with entries independently from a standard normal distribution, draw a ground-truth $\\mathbf{w}_{\\text{true}} \\in \\mathbb{R}^{d}$ with independent standard normal entries, and set $\\mathbf{y} = \\mathbf{X}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}$ where the noise vector $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ has independent entries drawn from a normal distribution with mean $0$ and standard deviation $0.1$.\n5. For each test case, solve:\n   - The robustified problem using the norm type and uncertainty radius $\\rho$, converted via your duality-based relation from Task $1$ into a regularized least absolute deviations problem.\n   - The directly regularized problem of the corresponding norm type using the provided $\\lambda$.\n   Report, for each test case, a single float equal to the maximum absolute difference between the coordinates of the two computed weight vectors, $\\max_{j \\in \\{1,\\dots,d\\}} \\left| w^{\\text{rob}}_j - w^{\\text{reg}}_j \\right|$.\n\nTest suite:\n- Test case $1$: norm type $\\ell_{\\infty}$, $n = 20$, $d = 5$, seed $= 0$, $\\rho = 0.03$, $\\lambda = 0.60$.\n- Test case $2$: norm type $\\ell_{\\infty}$, $n = 20$, $d = 5$, seed $= 0$, $\\rho = 0.03$, $\\lambda = 0.30$.\n- Test case $3$: norm type $\\ell_{1}$, $n = 30$, $d = 4$, seed $= 1$, $\\rho = 0.02$, $\\lambda = 0.60$.\n- Test case $4$: norm type $\\ell_{1}$, $n = 30$, $d = 4$, seed $= 1$, $\\rho = 0.02$, $\\lambda = 0.45$.\n- Test case $5$: norm type $\\ell_{\\infty}$, $n = 10$, $d = 3$, seed $= 2$, $\\rho = 0.00$, $\\lambda = 0.00$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each element must be the float specified in Task $5$ for the corresponding test case, in the same order as listed above.", "solution": "The user wants to analyze the relationship between robust optimization and regularization for the least absolute deviations (LAD) linear regression problem. The solution requires a mathematical derivation of this equivalence, followed by an implementation of linear programming solvers to empirically verify the derived relationship on synthetic datasets.\n\n### Task 1: Derivation of Equivalence\n\nThe problem begins with the standard LAD objective:\n$$ \\min_{\\mathbf{w}} \\sum_{i=1}^{n} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| $$\nwhere $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector, $\\mathbf{x}_i \\in \\mathbb{R}^{d}$ is the feature vector for sample $i$, and $y_i \\in \\mathbb{R}$ is the corresponding response.\n\nRobust optimization addresses uncertainty by considering worst-case scenarios. Here, we assume the features $\\mathbf{x}_i$ are subject to an additive perturbation $\\boldsymbol{\\delta}_i$ from a norm-ball uncertainty set $\\mathcal{U} = \\{ \\boldsymbol{\\delta} \\in \\mathbb{R}^{d} : \\|\\boldsymbol{\\delta}\\| \\leq \\rho \\}$, where $\\|\\cdot\\|$ is some norm and $\\rho \\geq 0$ is the uncertainty radius. The robustified objective is:\n$$ \\min_{\\mathbf{w}} \\sum_{i=1}^{n} \\max_{\\boldsymbol{\\delta}_i \\in \\mathcal{U}} \\left| y_i - (\\mathbf{x}_i + \\boldsymbol{\\delta}_i)^{\\top}\\mathbf{w} \\right| $$\n\nTo establish the equivalence with a regularized objective, we focus on the inner maximization term for a single sample $i$:\n$$ \\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} - \\boldsymbol{\\delta}_i^{\\top}\\mathbf{w} \\right| $$\nLet $r_i = y_i - \\mathbf{x}_i^{\\top}\\mathbf{w}$. The expression becomes $\\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} \\left| r_i - \\boldsymbol{\\delta}_i^{\\top}\\mathbf{w} \\right|$. Using the property that for any real number $a$, $|a| = \\max(a, -a)$, we can rewrite the maximization as:\n$$ \\max \\left( \\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} (r_i - \\boldsymbol{\\delta}_i^{\\top}\\mathbf{w}), \\quad \\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} -(r_i - \\boldsymbol{\\delta}_i^{\\top}\\mathbf{w}) \\right) $$\nLet's analyze the two terms inside the outer $\\max(\\cdot, \\cdot)$:\n1.  $\\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} (r_i - \\boldsymbol{\\delta}_i^{\\top}\\mathbf{w}) = r_i + \\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} (-\\boldsymbol{\\delta}_i^{\\top}\\mathbf{w}) = r_i - \\min_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} (\\boldsymbol{\\delta}_i^{\\top}\\mathbf{w})$.\n2.  $\\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} (-r_i + \\boldsymbol{\\delta}_i^{\\top}\\mathbf{w}) = -r_i + \\max_{\\|\\boldsymbol{\\delta}_i\\| \\leq \\rho} (\\boldsymbol{\\delta}_i^{\\top}\\mathbf{w})$.\n\nThe problem provides the definition of the support function of a norm ball: $\\sup_{\\|\\boldsymbol{\\delta}\\| \\leq \\rho} \\boldsymbol{\\delta}^{\\top}\\mathbf{v} = \\rho \\|\\mathbf{v}\\|_{*}$, where $\\|\\cdot\\|_{*}$ is the dual norm of $\\|\\cdot\\|$. Since the uncertainty set $\\mathcal{U}$ is symmetric (if $\\boldsymbol{\\delta} \\in \\mathcal{U}$, then $-\\boldsymbol{\\delta} \\in \\mathcal{U}$), we have $\\min_{\\|\\boldsymbol{\\delta}\\| \\leq \\rho} \\boldsymbol{\\delta}^{\\top}\\mathbf{v} = -\\sup_{\\|\\boldsymbol{\\delta}\\| \\leq \\rho} \\boldsymbol{\\delta}^{\\top}\\mathbf{v} = -\\rho \\|\\mathbf{v}\\|_{*}$.\n\nApplying this to our terms with $\\mathbf{v} = \\mathbf{w}$:\n1.  $r_i - (-\\rho \\|\\mathbf{w}\\|_{*}) = r_i + \\rho \\|\\mathbf{w}\\|_{*}$.\n2.  $-r_i + \\rho \\|\\mathbf{w}\\|_{*}$.\n\nSubstituting back, the inner maximization over $\\boldsymbol{\\delta}_i$ becomes:\n$$ \\max(r_i + \\rho \\|\\mathbf{w}\\|_{*}, -r_i + \\rho \\|\\mathbf{w}\\|_{*}) = |r_i| + \\rho \\|\\mathbf{w}\\|_{*} = \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| + \\rho \\|\\mathbf{w}\\|_{*} $$\nThe full robust objective is the sum over all $n$ samples:\n$$ \\min_{\\mathbf{w}} \\sum_{i=1}^{n} \\left( \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| + \\rho \\|\\mathbf{w}\\|_{*} \\right) $$\nThis can be rearranged as:\n$$ \\min_{\\mathbf{w}} \\left( \\sum_{i=1}^{n} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| \\right) + n \\rho \\|\\mathbf{w}\\|_{*} $$\nThis is a regularized LAD problem of the form $\\min_{\\mathbf{w}} \\sum_{i=1}^{n} \\left| y_i - \\mathbf{x}_i^{\\top}\\mathbf{w} \\right| + \\lambda \\|\\mathbf{w}\\|_{*}$. By comparing the two forms, we find the relationship between the regularization strength $\\lambda$ and the uncertainty radius $\\rho$:\n$$ \\lambda = n \\rho $$\nThe regularization norm $\\|\\cdot\\|_{*}$ is the dual of the norm $\\|\\cdot\\|$ used to define the uncertainty set.\n- **Case A ($\\ell_{\\infty}$ norm uncertainty):** If the uncertainty set is defined by $\\|\\boldsymbol{\\delta}\\|_{\\infty} \\leq \\rho$, the dual norm is the $\\ell_1$-norm, $\\|\\mathbf{w}\\|_{*} = \\|\\mathbf{w}\\|_1$. The equivalent problem is $\\ell_1$-regularized LAD.\n- **Case B ($\\ell_{1}$ norm uncertainty):** If the uncertainty set is defined by $\\|\\boldsymbol{\\delta}\\|_{1} \\leq \\rho$, the dual norm is the $\\ell_{\\infty}$-norm, $\\|\\mathbf{w}\\|_{*} = \\|\\mathbf{w}\\|_{\\infty}$. The equivalent problem is $\\ell_{\\infty}$-regularized LAD.\n\n### Task 2: Linear Programming Formulations\n\nTo solve these optimization problems, we formulate them as linear programs (LPs).\n\n**Case A: $\\ell_1$-Regularized LAD**\nThe problem is $\\min_{\\mathbf{w}} \\sum_{i=1}^{n} |y_i - \\mathbf{x}_i^{\\top}\\mathbf{w}| + \\lambda \\|\\mathbf{w}\\|_1$.\nWe introduce variables $e_i \\ge 0$ such that $e_i \\geq |y_i - \\mathbf{x}_i^{\\top}\\mathbf{w}|$. We also decompose $\\mathbf{w}$ into its positive and negative parts, $\\mathbf{w} = \\mathbf{w}^+ - \\mathbf{w}^-$, where $\\mathbf{w}^+, \\mathbf{w}^- \\ge 0$. Then $\\|\\mathbf{w}\\|_1 = \\sum_{j=1}^{d} (w_j^+ + w_j^-)$.\nThe LP is:\n$$ \\begin{aligned}\n\\min_{\\mathbf{w}^+, \\mathbf{w}^-, \\mathbf{e}} \\quad & \\sum_{i=1}^{n} e_i + \\lambda \\sum_{j=1}^{d} (w_j^+ + w_j^-) \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_i^{\\top}(\\mathbf{w}^+ - \\mathbf{w}^-) + e_i \\ge y_i, \\quad i=1, \\dots, n \\\\\n& -\\mathbf{x}_i^{\\top}(\\mathbf{w}^+ - \\mathbf{w}^-) + e_i \\ge -y_i, \\quad i=1, \\dots, n \\\\\n& \\mathbf{w}^+ \\ge 0, \\mathbf{w}^- \\ge 0, \\mathbf{e} \\ge 0\n\\end{aligned} $$\nThe decision variable vector is $\\mathbf{z} = [\\mathbf{w}^+; \\mathbf{w}^-; \\mathbf{e}] \\in \\mathbb{R}^{2d+n}$.\n\n**Case B: $\\ell_{\\infty}$-Regularized LAD**\nThe problem is $\\min_{\\mathbf{w}} \\sum_{i=1}^{n} |y_i - \\mathbf{x}_i^{\\top}\\mathbf{w}| + \\lambda \\|\\mathbf{w}\\|_{\\infty}$.\nWe introduce variables $e_i \\ge 0$ as before. For the $\\ell_{\\infty}$-norm, we introduce a single variable $t \\ge 0$ such that $t \\geq \\|\\mathbf{w}\\|_{\\infty} = \\max_j |w_j|$. This is equivalent to $t \\ge w_j$ and $t \\ge -w_j$ for all $j=1, \\dots, d$. The vector $\\mathbf{w}$ does not need to be split.\nThe LP is:\n$$ \\begin{aligned}\n\\min_{\\mathbf{w}, t, \\mathbf{e}} \\quad & \\sum_{i=1}^{n} e_i + \\lambda t \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_i^{\\top}\\mathbf{w} + e_i \\ge y_i, \\quad i=1, \\dots, n \\\\\n& -\\mathbf{x}_i^{\\top}\\mathbf{w} + e_i \\ge -y_i, \\quad i=1, \\dots, n \\\\\n& t - w_j \\ge 0, \\quad j=1, \\dots, d \\\\\n& t + w_j \\ge 0, \\quad j=1, \\dots, d \\\\\n& t \\ge 0, \\mathbf{e} \\ge 0\n\\end{aligned} $$\nThe decision variable vector is $\\mathbf{z} = [\\mathbf{w}; t; \\mathbf{e}] \\in \\mathbb{R}^{d+1+n}$.\n\nThe implementation will construct the corresponding matrices and vectors for these LPs and use `scipy.optimize.linprog` to find the optimal weights $\\mathbf{w}$. For each test case, we compute $\\mathbf{w}^{\\text{rob}}$ using $\\lambda_{\\text{rob}} = n\\rho$ and $\\mathbf{w}^{\\text{reg}}$ using the given $\\lambda$, then report the maximum absolute coordinate-wise difference.", "answer": "[0.0000000000,0.0655452601,0.0000000000,0.0223783709,0.0000000000]", "id": "3173939"}]}