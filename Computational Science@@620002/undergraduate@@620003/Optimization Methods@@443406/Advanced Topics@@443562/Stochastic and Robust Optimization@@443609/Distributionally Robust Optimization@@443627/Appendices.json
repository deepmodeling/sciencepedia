{"hands_on_practices": [{"introduction": "Distributionally Robust Optimization can be viewed as the modern, principled successor to classic robust statistics and probability inequalities. This first exercise builds a direct bridge from the well-known Chebyshev’s inequality to a moment-based distributionally robust chance constraint. By calculating the solution to both, you will directly quantify the improvement gained from using a provably tight bound over a more conservative, general-purpose one [@problem_id:3121629].", "problem": "Consider a scalar uncertain quantity $X$ (for example, a linear cost) with an unknown distribution, but known first two moments: mean $\\mu$ and variance $\\sigma^{2}$, where $\\sigma \\ge 0$. For a given risk level $\\epsilon \\in (0,1)$ and a threshold $\\tau \\in \\mathbb{R}$, a chance constraint requires that $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$. In a distributionally robust setting, one seeks a value of $\\tau$ that guarantees this chance constraint holds for all probability distributions on $\\mathbb{R}$ with mean $\\mu$ and variance $\\sigma^{2}$.\n\nUsing only core definitions of mean, variance, and tail probabilities, together with well-tested inequalities from probability theory, do the following:\n\n(i) Using Chebyshev’s inequality, derive a sufficient condition on $\\tau$ in terms of $\\mu$, $\\sigma$, and $\\epsilon$ that guarantees $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$ for all distributions with mean $\\mu$ and variance $\\sigma^{2}$. Express the resulting bound as $\\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon)$.\n\n(ii) Using the tightest possible one-sided bound that depends only on $\\mu$ and $\\sigma^{2}$ (i.e., the solution of the moment-based distributionally robust chance constraint over all distributions consistent with the given moments), derive the minimal $\\tau$ that guarantees $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$ for all such distributions. Express this as $\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon)$.\n\n(iii) Define the improvement factor\n$$\nR(\\epsilon) \\equiv \\frac{\\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon) - \\mu}{\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon) - \\mu}.\n$$\nCompute $R(\\epsilon)$ in simplest closed form. Your final answer must be the simplified analytic expression for $R(\\epsilon)$ only. Do not provide intermediate values. No rounding is required.", "solution": "We are given a scalar random variable $X$ with unknown distribution but known mean $\\mu$ and variance $\\sigma^{2}$. The chance constraint $\\mathbb{P}(X \\le \\tau) \\ge 1 - \\epsilon$ can be equivalently written as an upper bound on the right tail probability $\\mathbb{P}(X - \\mu \\ge \\tau - \\mu) \\le \\epsilon$. We seek a value of $\\tau$ that holds uniformly over all distributions with the given first two moments.\n\nStep (i): Sufficient condition from Chebyshev’s inequality.\n\nChebyshev’s inequality (a well-tested probability inequality) states that for any random variable with finite mean and variance,\n$$\n\\mathbb{P}\\left(|X - \\mu| \\ge t\\right) \\le \\frac{\\sigma^{2}}{t^{2}} \\quad \\text{for all } t > 0.\n$$\nSince the one-sided event $\\{X - \\mu \\ge t\\}$ is contained in the two-sided event $\\{|X - \\mu| \\ge t\\}$, we have\n$$\n\\mathbb{P}(X - \\mu \\ge t) \\le \\mathbb{P}\\left(|X - \\mu| \\ge t\\right) \\le \\frac{\\sigma^{2}}{t^{2}} \\quad \\text{for all } t > 0.\n$$\nTo ensure $\\mathbb{P}(X - \\mu \\ge t) \\le \\epsilon$, it suffices to require\n$$\n\\frac{\\sigma^{2}}{t^{2}} \\le \\epsilon \\quad \\Longleftrightarrow \\quad t \\ge \\frac{\\sigma}{\\sqrt{\\epsilon}}.\n$$\nSetting $t = \\tau - \\mu$, a sufficient condition for the chance constraint to hold is\n$$\n\\tau - \\mu \\ge \\frac{\\sigma}{\\sqrt{\\epsilon}} \\quad \\Longleftrightarrow \\quad \\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon) = \\mu + \\frac{\\sigma}{\\sqrt{\\epsilon}}.\n$$\n\nStep (ii): Tightest moment-based one-sided bound (distributionally robust chance constraint).\n\nThe tightest possible bound on the one-sided tail probability that depends only on $\\mu$ and $\\sigma^{2}$ is given by the known extremal-moment (one-sided) inequality, often referred to as Cantelli’s inequality (also called the one-sided Chebyshev inequality). For any $t > 0$,\n$$\n\\mathbb{P}(X - \\mu \\ge t) \\le \\frac{\\sigma^{2}}{\\sigma^{2} + t^{2}},\n$$\nand this bound is tight over the family of distributions with mean $\\mu$ and variance $\\sigma^{2}$. Therefore, to guarantee $\\mathbb{P}(X - \\mu \\ge t) \\le \\epsilon$ uniformly over all such distributions, it is necessary and sufficient that\n$$\n\\frac{\\sigma^{2}}{\\sigma^{2} + t^{2}} \\le \\epsilon \\quad \\Longleftrightarrow \\quad \\sigma^{2} \\le \\epsilon(\\sigma^{2} + t^{2})\n\\quad \\Longleftrightarrow \\quad (1 - \\epsilon)\\sigma^{2} \\le \\epsilon t^{2}.\n$$\nSolving for $t$ gives\n$$\nt \\ge \\sigma \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}}.\n$$\nSetting again $t = \\tau - \\mu$, the minimal threshold ensuring the distributionally robust chance constraint is\n$$\n\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon) = \\mu + \\sigma \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}}.\n$$\n\nStep (iii): Compute the improvement factor $R(\\epsilon)$.\n\nBy definition,\n$$\nR(\\epsilon) = \\frac{\\tau_{\\mathrm{Cheb}}(\\mu,\\sigma,\\epsilon) - \\mu}{\\tau_{\\mathrm{DRO}}(\\mu,\\sigma,\\epsilon) - \\mu}\n= \\frac{\\dfrac{\\sigma}{\\sqrt{\\epsilon}}}{\\sigma \\sqrt{\\dfrac{1 - \\epsilon}{\\epsilon}}}.\n$$\nCanceling the common factor $\\sigma$ and simplifying,\n$$\nR(\\epsilon) = \\frac{1/\\sqrt{\\epsilon}}{\\sqrt{(1 - \\epsilon)/\\epsilon}}\n= \\frac{1}{\\sqrt{1 - \\epsilon}}.\n$$\nThis expression is already in simplest closed form for $\\epsilon \\in (0,1)$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{1-\\epsilon}}}$$", "id": "3121629"}, {"introduction": "While theoretical guarantees are important, the practical power of DRO often lies in its ability to produce models that generalize better to new, unseen data. This exercise provides a hands-on coding experience where you will observe the phenomenon of overfitting in a standard Sample Average Approximation (SAA) model due to outliers in the training data. You will then implement a Wasserstein-DRO model and see how it systematically mitigates overfitting, leading to a more robust decision with superior out-of-sample performance [@problem_id:3121634].", "problem": "You are given a toy instance to study the phenomenon of overfitting in Sample Average Approximation (SAA) and the generalization improvement obtained by Wasserstein Distributionally Robust Optimization (DRO). Consider a scalar decision variable $x \\in \\mathbb{R}$ and a scalar uncertain parameter $\\xi \\in \\mathbb{R}$. The loss is defined as $f(x,\\xi) = \\tfrac{1}{2} x^2 - x \\xi$, which is convex in $x$ for all $\\xi$. The true data-generating distribution for $\\xi$ is a mixture distribution: with probability $0.9$ draw $\\xi$ from a Normal distribution $\\mathcal{N}(0,1)$, and with probability $0.1$ set $\\xi$ equal to the constant $10$. The training dataset is constructed to contain $20$ samples: $16$ samples approximately from the $\\mathcal{N}(0,1)$ component and $4$ outliers at $10$. For reproducibility, use the fixed list\n$$\n\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10].\n$$\nAdopt the $1$-Wasserstein distance with ground metric $d(\\xi,\\xi') = |\\xi - \\xi'|$ to define the ambiguity set $\\mathcal{W}_{\\epsilon}$ as the ball of radius $\\epsilon \\ge 0$ centered at the empirical distribution $\\hat{P}_N$ built from the training data. Starting from fundamental definitions of expected risk and ambiguity sets, and without assuming pre-derived robust formulas, derive the SAA estimator $x_{\\text{SAA}}$ and the Wasserstein-DRO estimator $x_{\\text{DRO}}(\\epsilon)$ that minimize, respectively, the empirical expected loss and the worst-case expected loss over $\\mathcal{W}_{\\epsilon}$. Then, evaluate their out-of-sample risks under the true distribution. In this setting, the out-of-sample risk of a decision $x$ is the true expectation $\\mathbb{E}[f(x,\\xi)]$ taken with respect to the mixture distribution described above.\n\nYour program must:\n- Construct $\\Xi_{\\text{train}}$ exactly as provided.\n- Compute the empirical mean $\\bar{\\xi}$ over $\\Xi_{\\text{train}}$.\n- Compute the SAA decision $x_{\\text{SAA}}$ by minimizing $\\tfrac{1}{2} x^2 - x \\bar{\\xi}$.\n- Using the $1$-Wasserstein ball $\\mathcal{W}_{\\epsilon}$ and the ground metric $|\\cdot|$, derive the Wasserstein-DRO decision $x_{\\text{DRO}}(\\epsilon)$ for each given $\\epsilon$ based on principled reasoning, and implement it.\n- Evaluate the out-of-sample risk $\\mathbb{E}\\left[\\tfrac{1}{2} x^2 - x \\xi\\right]$ under the true mixture distribution for each decision $x_{\\text{DRO}}(\\epsilon)$.\n\nUse the following test suite of radius values:\n$$\n[\\;0.0,\\;0.2,\\;1.0,\\;2.0,\\;2.5\\;].\n$$\nThese cover a general \"happy path\" case ($0.2$), the baseline SAA case ($0.0$), a moderate robustification ($1.0$), the boundary threshold case where $\\epsilon$ equals $|\\bar{\\xi}|$ ($2.0$), and an extreme robustification ($2.5$). For each $\\epsilon$ in the list, output the corresponding out-of-sample risk of $x_{\\text{DRO}}(\\epsilon)$ evaluated under the true mixture distribution. The true distribution is specified completely by the mixture weights and component parameters; you must compute the exact mean of the mixture and use it consistently.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\text{result}_1,\\text{result}_2,\\dots]$. All answers must be real numbers (floats) without any physical units. Smaller values indicate better generalization.", "solution": "The problem requires the derivation and comparison of the Sample Average Approximation (SAA) estimator and the Wasserstein Distributionally Robust Optimization (DRO) estimator for a specific quadratic loss function. The performance of these estimators is to be evaluated based on their out-of-sample risk, calculated with respect to a true, known data-generating distribution.\n\nFirst, we define the core components of the problem. The decision variable is a scalar $x \\in \\mathbb{R}$, and the uncertain parameter is a scalar $\\xi \\in \\mathbb{R}$. The loss function is given by $f(x, \\xi) = \\frac{1}{2} x^2 - x \\xi$. The training data consists of $N=20$ samples provided as a fixed list $\\Xi_{\\text{train}}$. The empirical distribution $\\hat{P}_N$ is the discrete uniform distribution over these $N$ samples.\n\nThe SAA estimator, denoted $x_{\\text{SAA}}$, is the solution to the empirical risk minimization problem:\n$$\nx_{\\text{SAA}} = \\arg\\min_{x \\in \\mathbb{R}} \\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)]\n$$\nThe empirical risk is calculated as the average loss over the training samples:\n$$\n\\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)] = \\frac{1}{N} \\sum_{i=1}^{N} f(x, \\xi_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{2} x^2 - x \\xi_i\\right) = \\frac{1}{2} x^2 - x \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\xi_i\\right)\n$$\nLet $\\bar{\\xi} = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_i$ be the empirical mean of the training samples. The SAA objective simplifies to $\\frac{1}{2} x^2 - x \\bar{\\xi}$. This is a strictly convex quadratic function of $x$. To find the minimizer, we set the first derivative with respect to $x$ to zero:\n$$\n\\frac{d}{dx} \\left(\\frac{1}{2} x^2 - x \\bar{\\xi}\\right) = x - \\bar{\\xi} = 0\n$$\nThis yields the SAA estimator:\n$$\nx_{\\text{SAA}} = \\bar{\\xi}\n$$\n\nNext, we address the Wasserstein-DRO estimator, $x_{\\text{DRO}}(\\epsilon)$. It is the solution to the minimax problem:\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)]\n$$\nHere, $\\mathcal{W}_{\\epsilon}$ is the $1$-Wasserstein ball of radius $\\epsilon \\geq 0$ around the empirical distribution $\\hat{P}_N$, defined as $\\mathcal{W}_{\\epsilon} = \\{P \\mid W_1(P, \\hat{P}_N) \\le \\epsilon \\}$, with the ground metric being the absolute difference $d(\\xi, \\xi') = |\\xi - \\xi'|$.\n\nTo solve the minimax problem, we first analyze the inner maximization problem for a fixed $x$:\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)] = \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 + \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi]\n$$\nThe Kantorovich-Rubinstein duality theorem states that the $1$-Wasserstein distance can be expressed as:\n$$\nW_1(P, \\hat{P}_N) = \\sup_{\\phi: \\|\\phi\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_P[\\phi(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[\\phi(\\xi)] \\right)\n$$\nwhere the supremum is over all $1$-Lipschitz functions $\\phi$. From this, for any function $g(\\xi)$, we have the inequality $\\mathbb{E}_P[g(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] \\le W_1(P, \\hat{P}_N) \\cdot L(g)$, where $L(g)$ is the Lipschitz constant of $g$. For any $P \\in \\mathcal{W}_{\\epsilon}$, this implies $\\mathbb{E}_P[g(\\xi)] \\le \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] + \\epsilon L(g)$. This upper bound is known to be tight.\nIn our case, the function of interest is $g(\\xi) = -x\\xi$. Its Lipschitz constant is $L(g) = \\sup_{\\xi_1\\neq\\xi_2}\\frac{|-x\\xi_1 - (-x\\xi_2)|}{|\\xi_1-\\xi_2|} = |-x| = |x|$.\nTherefore, the worst-case expectation is:\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi] = \\mathbb{E}_{\\hat{P}_N}[-x \\xi] + \\epsilon |x| = -x \\bar{\\xi} + \\epsilon |x|\n$$\nSubstituting this back into the DRO objective function, we need to solve:\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\left( \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x| \\right)\n$$\nThe objective function $J(x) = \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x|$ is convex but non-differentiable at $x=0$. We can use subgradient calculus. The minimizer $x^*$ must satisfy $0 \\in \\partial J(x^*)$. The subgradient of $|x|$ is $\\text{sgn}(x)$ for $x \\neq 0$ and $[-1, 1]$ for $x=0$.\nThe subgradient of $J(x)$ is $\\partial J(x) = x - \\bar{\\xi} + \\epsilon \\cdot \\partial |x|$.\nIf $x > 0$, we need $x - \\bar{\\xi} + \\epsilon = 0 \\implies x = \\bar{\\xi} - \\epsilon$. This is valid only if $\\bar{\\xi} - \\epsilon > 0$, i.e., $\\bar{\\xi} > \\epsilon$.\nIf $x < 0$, we need $x - \\bar{\\xi} - \\epsilon = 0 \\implies x = \\bar{\\xi} + \\epsilon$. This is valid only if $\\bar{\\xi} + \\epsilon < 0$, i.e., $\\bar{\\xi} < -\\epsilon$.\nIf $x=0$, we need $0 \\in 0 - \\bar{\\xi} + \\epsilon[-1, 1] = [-\\bar{\\xi}-\\epsilon, -\\bar{\\xi}+\\epsilon]$. This is true if $-\\bar{\\xi}-\\epsilon \\le 0 \\le -\\bar{\\xi}+\\epsilon$, which simplifies to $|\\bar{\\xi}| \\le \\epsilon$.\n\nCombining these conditions, the solution is the soft-thresholding operator applied to $\\bar{\\xi}$ with threshold $\\epsilon$:\n$$\nx_{\\text{DRO}}(\\epsilon) = \\text{sign}(\\bar{\\xi}) \\max(0, |\\bar{\\xi}| - \\epsilon)\n$$\nNote that for $\\epsilon=0$, we have $x_{\\text{DRO}}(0) = \\text{sign}(\\bar{\\xi})\\max(0, |\\bar{\\xi}|) = \\bar{\\xi}$, which correctly recovers the SAA estimator $x_{\\text{SAA}}$.\n\nFinally, we compute the out-of-sample risk $R(x)$ for a given decision $x$. This is the expectation of the loss function under the true data-generating distribution $P_{\\text{true}}$:\n$$\nR(x) = \\mathbb{E}_{P_{\\text{true}}}[f(x, \\xi)] = \\mathbb{E}_{P_{\\text{true}}}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 - x \\mathbb{E}_{P_{\\text{true}}}[\\xi]\n$$\nThe true distribution of $\\xi$ is a mixture: with probability $0.9$, $\\xi \\sim \\mathcal{N}(0,1)$, and with probability $0.1$, $\\xi=10$. The true mean $\\mu_{\\text{true}} = \\mathbb{E}_{P_{\\text{true}}}[\\xi]$ is:\n$$\n\\mu_{\\text{true}} = 0.9 \\cdot \\mathbb{E}[\\mathcal{N}(0,1)] + 0.1 \\cdot 10 = 0.9 \\cdot 0 + 0.1 \\cdot 10 = 1.0\n$$\nThus, the out-of-sample risk is $R(x) = \\frac{1}{2} x^2 - x$.\n\nWe now perform the numerical calculations.\nThe training data is $\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10]$.\nThe sum of the first $16$ symmetric samples is $0$. The sum of the four outliers is $4 \\times 10 = 40$. The total number of samples is $N=20$.\nThe empirical mean is $\\bar{\\xi} = \\frac{0 + 40}{20} = 2.0$.\n\nWe evaluate $x_{\\text{DRO}}(\\epsilon) = \\text{soft}(2.0, \\epsilon)$ and its risk $R(x_{\\text{DRO}}(\\epsilon))$ for each given $\\epsilon$:\n1.  $\\epsilon=0.0$: $x_{\\text{DRO}}(0.0) = \\text{soft}(2.0, 0.0) = 2.0$. Risk $R(2.0) = \\frac{1}{2}(2.0)^2 - 2.0 = 2.0 - 2.0 = 0.0$.\n2.  $\\epsilon=0.2$: $x_{\\text{DRO}}(0.2) = \\text{soft}(2.0, 0.2) = 1.8$. Risk $R(1.8) = \\frac{1}{2}(1.8)^2 - 1.8 = 1.62 - 1.8 = -0.18$.\n3.  $\\epsilon=1.0$: $x_{\\text{DRO}}(1.0) = \\text{soft}(2.0, 1.0) = 1.0$. Risk $R(1.0) = \\frac{1}{2}(1.0)^2 - 1.0 = 0.5 - 1.0 = -0.5$.\n4.  $\\epsilon=2.0$: $x_{\\text{DRO}}(2.0) = \\text{soft}(2.0, 2.0) = 0.0$. Risk $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$.\n5.  $\\epsilon=2.5$: $x_{\\text{DRO}}(2.5) = \\text{soft}(2.0, 2.5) = 0.0$. Risk $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$.\n\nThe SAA solution $x_{\\text{SAA}}=2.0$ overfits to the outliers in the training data, as it is much larger than the true optimal decision $x^* = \\arg\\min_x R(x) = \\arg\\min_x(\\frac{1}{2}x^2-x) = 1.0$. The DRO solution with $\\epsilon=1.0$ corrects the empirical mean perfectly, yielding the optimal out-of-sample performance. Smaller $\\epsilon$ values provide insufficient correction, while larger $\\epsilon$ values over-regularize and shrink the solution too much, degrading performance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SAA and Wasserstein-DRO solutions and their out-of-sample risks.\n    \"\"\"\n    \n    # Define the training dataset as specified in the problem statement.\n    Xi_train = np.array([\n        -1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, \n        -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10.0, 10.0, 10.0, 10.0\n    ])\n\n    # Define the test suite of Wasserstein radii.\n    epsilon_values = [0.0, 0.2, 1.0, 2.0, 2.5]\n\n    # Compute the empirical mean of the training data.\n    # This corresponds to the SAA decision x_SAA.\n    xi_bar = np.mean(Xi_train)\n\n    # The true mean of the data-generating distribution for xi.\n    # P(xi) = 0.9 * N(0, 1) + 0.1 * delta_10\n    # E[xi] = 0.9 * E[N(0,1)] + 0.1 * E[delta_10] = 0.9 * 0 + 0.1 * 10 = 1.0\n    mu_true = 1.0\n\n    def soft_threshold(y, T):\n        \"\"\"\n        The soft-thresholding operator, which gives the Wasserstein-DRO solution.\n        soft(y, T) = sign(y) * max(0, |y| - T)\n        \"\"\"\n        if T  0:\n            raise ValueError(\"Threshold T must be non-negative.\")\n        return np.sign(y) * np.maximum(0, np.abs(y) - T)\n\n    def out_of_sample_risk(x):\n        \"\"\"\n        Computes the out-of-sample risk R(x) = E_true[0.5*x^2 - x*xi].\n        R(x) = 0.5*x^2 - x * E_true[xi]\n        \"\"\"\n        return 0.5 * x**2 - x * mu_true\n\n    results = []\n    for epsilon in epsilon_values:\n        # Compute the Wasserstein-DRO decision x_DRO(epsilon).\n        # For epsilon = 0, this is the SAA decision x_SAA.\n        x_dro = soft_threshold(xi_bar, epsilon)\n        \n        # Evaluate the out-of-sample risk for this decision.\n        risk = out_of_sample_risk(x_dro)\n        results.append(risk)\n\n    # Format the output as a comma-separated list in brackets.\n    # The problem specifies that smaller values indicate better generalization.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3121634"}, {"introduction": "To truly understand the min-max nature of DRO, it is crucial to develop an intuition for the inner 'max' problem, where an adversary chooses the worst possible data distribution from the ambiguity set. This practice offers a geometric and visual perspective on this adversarial process within a Wasserstein ball. You will compute how individual data points are shifted to create the worst-case scenario for a given loss function, making the abstract concept of a worst-case distribution tangible and intuitive [@problem_id:3121643].", "problem": "You are asked to implement a small, self-contained program that computes worst-case sample locations for a distributionally robust optimization problem defined over two-dimensional data under a Wasserstein ball. Consider the following setup. Let there be a finite set of points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$ with an empirical distribution that places mass $1/n$ on each $x_i$. Let the loss be a linear functional $f_w(x) = w^\\top x$ with a given parameter vector $w \\in \\mathbb{R}^2$. Consider an uncertainty set of distributions defined by a Wasserstein ball of order infinity with ground metric given by the Euclidean norm, namely the set of all distributions $Q$ such that $W_\\infty(Q, P_n) \\le \\epsilon$, where $P_n$ is the empirical distribution on $\\{x_i\\}$. In this setting, the worst-case expected loss is defined as the supremum of the expected loss over all distributions in the Wasserstein ball of radius $\\epsilon$, subject to the Wasserstein distance constraint. Using only fundamental definitions and convex analysis principles, derive the optimizer for the worst-case perturbed sample locations $\\{x_i^\\star\\}$ that maximize the average loss when each original sample $x_i$ is allowed to move within an Euclidean ball of radius $\\epsilon$ around $x_i$. Your program must compute these worst-case sample locations for the specified test suite and produce the exact required output format.\n\nFundamental base and definitions you must use:\n- The Wasserstein distance of order infinity $W_\\infty$ between two distributions on $\\mathbb{R}^2$ with Euclidean cost is defined using couplings and the essential supremum of the Euclidean distance; it implies that every mass point can be moved by at most $\\epsilon$.\n- The Euclidean norm is defined by $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$ for $v \\in \\mathbb{R}^2$.\n- For a linear functional $f_w(x) = w^\\top x$, the maximizer over a closed Euclidean ball is attained on the boundary in the direction of the gradient whenever the gradient is nonzero.\n- The Cauchy–Schwarz inequality: for any $a, b \\in \\mathbb{R}^2$, $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$, with equality if $a$ and $b$ are positively colinear.\n\nYour tasks:\n1. From the fundamental base, derive the per-sample maximizer $x_i^\\star$ of $f_w(x)$ under the constraint $\\|x - x_i\\|_2 \\le \\epsilon$ for a given $w$, including the edge case when $\\|w\\|_2 = 0$.\n2. Implement a program that, given a set of points $\\{x_i\\}$, a vector $w$, and a radius $\\epsilon$, computes all worst-case perturbed sample locations $\\{x_i^\\star\\}$.\n3. Apply your implementation to the following test suite. For numerical stability in output, report coordinates rounded to six decimal places.\n\nTest suite (each case is of the form $(X, w, \\epsilon)$, where $X$ is a list of two-dimensional points):\n- Case 1: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.0$.\n- Case 2: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.5$.\n- Case 3: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 1.0$.\n- Case 4: $X = \\{[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]\\}$, $w = [0.0, 0.0]$, $\\epsilon = 0.75$.\n- Case 5: $X = \\{[0.5, -0.5], [1.5, 2.0]\\}$, $w = [-3.0, 4.0]$, $\\epsilon = 0.25$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is a list of the computed worst-case locations for that case. Each location must be a two-element list $[x_1^\\star, x_2^\\star]$ with both entries rounded to six decimals. For example, the format should look like:\n  - For two cases with two points each: $[[[x_{11}^\\star, x_{12}^\\star], [x_{21}^\\star, x_{22}^\\star]], [[y_{11}^\\star, y_{12}^\\star], [y_{21}^\\star, y_{22}^\\star]]]$.\n- There are no physical units involved in this problem.", "solution": "The user wants to solve a worst-case optimization problem under distributional uncertainty.\n\n### **Problem Statement Validation**\n\n**Step 1: Extract Givens**\n- **Data**: A set of $n$ points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$.\n- **Empirical Distribution**: $P_n = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}$, where $\\delta_{x_i}$ is a Dirac mass at point $x_i$.\n- **Loss Function**: A linear functional $f_w(x) = w^\\top x$ for a given vector $w \\in \\mathbb{R}^2$.\n- **Uncertainty Set**: A Wasserstein ball of order infinity, defined as the set of all distributions $Q$ on $\\mathbb{R}^2$ such that the Wasserstein distance $W_\\infty(Q, P_n) \\le \\epsilon$. The ground metric is the Euclidean norm, $\\| \\cdot \\|_2$.\n- **Objective**: Find the worst-case sample locations $\\{x_i^\\star\\}_{i=1}^n$ that achieve the supremum of the expected loss over the uncertainty set. This is mathematically expressed as finding the locations that solve $\\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x]$.\n- **Fundamental Definitions**:\n    - The $W_\\infty$ distance constraint implies that each mass point $x_i$ can be moved to a new location $x'$ within a Euclidean ball of radius $\\epsilon$, i.e., $\\|x' - x_i\\|_2 \\le \\epsilon$.\n    - The Euclidean norm is $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$.\n    - The maximizer of a linear functional $w^\\top x$ over a closed Euclidean ball lies on the boundary in the direction of the gradient $w$, provided $w \\neq 0$.\n    - The Cauchy-Schwarz inequality: $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$ for $a, b \\in \\mathbb{R}^2$.\n- **Tasks**:\n    1. Derive the per-sample maximizer $x_i^\\star$ for $f_w(x)$ under the constraint $\\|x - x_i\\|_2 \\le \\epsilon$, handling the case $\\|w\\|_2 = 0$.\n    2. Implement a program to compute $\\{x_i^\\star\\}$ for given $\\{x_i\\}$, $w$, and $\\epsilon$.\n    3. Run on a given test suite and format the output.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: Yes. The problem is a standard formulation in distributionally robust optimization (DRO), based on established mathematical concepts like Wasserstein distance, convex optimization, and duality theory.\n- **Well-Posed**: Yes. The objective is to maximize a continuous (linear) function over a compact set (a collection of closed balls). By the extreme value theorem, a maximum exists. The derivation will show it is unique when $w \\ne 0$.\n- **Objective**: Yes. The problem is stated using precise, unambiguous mathematical terminology.\n- **Flaw Check**:\n    1.  **Scientific Unsoundness**: None. The formulation is a valid and widely studied problem in DRO.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is formally stated and is central to the topic of distributionally robust optimization.\n    3.  **Incomplete/Contradictory**: None. All necessary data ($X, w, \\epsilon$) for each test case is provided.\n    4.  **Unrealistic/Infeasible**: Not applicable. The problem is purely mathematical; no physical constraints are involved.\n    5.  **Ill-Posed**: None. As argued above, a well-defined solution exists.\n    6.  **Pseudo-Profound/Trivial**: None. The problem requires a correct derivation from first principles and a precise implementation, which constitutes a valid challenge.\n    7.  **Outside Scientific Verifiability**: None. The solution is mathematically derivable and computationally verifiable.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. Proceeding with the solution.\n\n### **Derivation of the Worst-Case Sample Locations**\n\nThe objective is to compute the worst-case expected loss:\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] $$\nA fundamental result in distributionally robust optimization, often derived from Kantorovich duality, states that for a Wasserstein ball of order $\\infty$, the worst-case expectation of a function can be found by taking the supremum over the worst-case realizations of each sample independently. Given that the empirical distribution $P_n$ has mass $1/n$ at each point $x_i$, the problem decouples into $n$ independent subproblems:\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] = \\frac{1}{n} \\sum_{i=1}^n \\sup_{x'} \\{ w^\\top x' \\mid \\|x' - x_i\\|_2 \\le \\epsilon \\} $$\nOur task is to find the optimizer $x_i^\\star$ for each of these $n$ identical maximization problems. Let us analyze a single subproblem for a generic sample $x_i$:\n$$ \\max_{x'} w^\\top x' \\quad \\text{subject to} \\quad \\|x' - x_i\\|_2 \\le \\epsilon $$\nLet the variable transformation be $x' = x_i + \\delta$, where $\\delta \\in \\mathbb{R}^2$. The constraint on $x'$ translates to a constraint on the perturbation $\\delta$:\n$$ \\|(x_i + \\delta) - x_i\\|_2 \\le \\epsilon \\implies \\|\\delta\\|_2 \\le \\epsilon $$\nThe objective function becomes:\n$$ w^\\top x' = w^\\top (x_i + \\delta) = w^\\top x_i + w^\\top \\delta $$\nSince $w^\\top x_i$ is a constant with respect to the optimization variable $\\delta$, maximizing $w^\\top x'$ is equivalent to maximizing $w^\\top \\delta$:\n$$ \\max_{\\delta} w^\\top \\delta \\quad \\text{subject to} \\quad \\|\\delta\\|_2 \\le \\epsilon $$\nWe can determine the optimal perturbation $\\delta^\\star$ using the Cauchy-Schwarz inequality, which states $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$. Applying this, we get:\n$$ w^\\top \\delta \\le \\|w\\|_2 \\|\\delta\\|_2 $$\nGiven the constraint $\\|\\delta\\|_2 \\le \\epsilon$, we can further bound the expression:\n$$ w^\\top \\delta \\le \\|w\\|_2 \\epsilon $$\nEquality (and thus the maximum value) is achieved when $\\delta$ is positively colinear with $w$ and has the maximum possible magnitude, which is $\\epsilon$.\n\nWe now analyze two cases for the vector $w$:\n\n**Case 1: $w \\ne 0$**\nIf $w$ is not the zero vector, then $\\|w\\|_2  0$. The unique optimal perturbation $\\delta^\\star$ that maximizes $w^\\top \\delta$ is the vector of length $\\epsilon$ in the direction of $w$:\n$$ \\delta^\\star = \\epsilon \\frac{w}{\\|w\\|_2} $$\nThe worst-case sample location $x_i^\\star$ is then found by adding this optimal perturbation to the original sample location $x_i$:\n$$ x_i^\\star = x_i + \\delta^\\star = x_i + \\epsilon \\frac{w}{\\|w\\|_2} $$\nThis shows that to maximize a linear loss, each data point should be shifted in the direction of the loss vector's gradient, $w$, to the furthest extent allowed by the uncertainty set, which is a distance of $\\epsilon$. This holds for every sample $i=1, \\dots, n$.\n\n**Case 2: $w = 0$**\nIf $w$ is the zero vector, then $\\|w\\|_2 = 0$. The objective function becomes:\n$$ w^\\top \\delta = 0^\\top \\delta = 0 $$\nIn this case, the objective value is $0$ regardless of the choice of $\\delta$, as long as it satisfies the constraint $\\|\\delta\\|_2 \\le \\epsilon$. Any point in the ball of radius $\\epsilon$ around $x_i$ is an optimizer. A natural and standard choice in such situations is to select the solution with the minimum norm, which corresponds to making the smallest change. This leads to choosing $\\delta^\\star = 0$. Consequently, the worst-case location is the original location itself:\n$$ x_i^\\star = x_i $$\nNote that the formula from Case 1 is undefined if $\\|w\\|_2=0$, so this case must be handled separately.\n\nCombining both cases, the formula for the worst-case location $x_i^\\star$ is:\n$$ x_i^\\star = \\begin{cases} x_i + \\epsilon \\frac{w}{\\|w\\|_2}  \\text{if } \\|w\\|_2  0 \\\\ x_i  \\text{if } \\|w\\|_2 = 0 \\end{cases} $$\nThis formula will be implemented to solve the problem for the given test suite.", "answer": "```python\nimport numpy as np\n\ndef compute_worst_case_locations(X, w, epsilon):\n    \"\"\"\n    Computes the worst-case sample locations for a linear loss.\n\n    Args:\n        X (list of lists): The original sample locations, [[x1, y1], [x2, y2], ...].\n        w (list): The parameter vector [w1, w2] of the linear loss.\n        epsilon (float): The radius of the Wasserstein ball (and per-sample uncertainty sets).\n\n    Returns:\n        list of lists: The worst-case sample locations.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    w_np = np.array(w, dtype=float)\n\n    # Handle the edge case where epsilon is zero. No perturbation is possible.\n    if epsilon == 0.0:\n        return X_np.tolist()\n\n    # Calculate the Euclidean norm of the vector w.\n    norm_w = np.linalg.norm(w_np)\n\n    # If the norm of w is zero (or numerically close to it), the loss is constant.\n    # The optimal perturbation is zero (no change to the points).\n    if norm_w  1e-9:\n        return X_np.tolist()\n    \n    # Calculate the unit vector in the direction of w.\n    u_w = w_np / norm_w\n    \n    # The optimal perturbation is epsilon times the unit vector u_w.\n    # This vector is the same for all points.\n    delta_star = epsilon * u_w\n    \n    # Add the perturbation to all original sample locations.\n    # Numpy's broadcasting handles this efficiently.\n    X_star_np = X_np + delta_star\n    \n    return X_star_np.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.0}),\n        # Case 2\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.5}),\n        # Case 3\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 1.0}),\n        # Case 4\n        ({\"X\": [[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]], \"w\": [0.0, 0.0], \"epsilon\": 0.75}),\n        # Case 5\n        ({\"X\": [[0.5, -0.5], [1.5, 2.0]], \"w\": [-3.0, 4.0], \"epsilon\": 0.25}),\n    ]\n\n    # Store formatted string results for each test case\n    results_str_list = []\n\n    for case in test_cases:\n        worst_case_locs = compute_worst_case_locations(case[\"X\"], case[\"w\"], case[\"epsilon\"])\n        \n        # Format the result for a single case according to the required output format.\n        # Each point is formatted as [x,y] with 6 decimal places.\n        points_str = [f\"[{p[0]:.6f},{p[1]:.6f}]\" for p in worst_case_locs]\n        case_result_str = f\"[{','.join(points_str)}]\"\n        results_str_list.append(case_result_str)\n\n    # Join the results of all cases into a single string.\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    # Final print statement must produce only the specified output format.\n    print(final_output)\n\n# Execute the solver.\nsolve()\n\n```", "id": "3121643"}]}