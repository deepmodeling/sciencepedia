{"hands_on_practices": [{"introduction": "The standard approach to mini-batch optimization involves taking a simple average of gradients from a randomly selected subset of data, treating each sample as equally important. However, what if some samples provide more reliable information than others? This practice explores the concept of variance reduction by assigning optimal, non-uniform weights to different samples within a mini-batch, moving beyond simple averaging to a more statistically powerful approach. By deriving the well-known inverse-variance weighting scheme, you will engage with a fundamental principle for constructing more efficient stochastic estimators [@problem_id:3150573].", "problem": "Consider Stochastic Gradient Descent (SGD) in the mini-batch setting for a single-parameter model. Let a mini-batch be denoted by the index set $B=\\{1,2,3\\}$, and for each $i \\in B$ let $X_i$ be a scalar stochastic gradient estimate that satisfies $\\mathbb{E}[X_i]=\\mu$ (unbiasedness) and $\\operatorname{Var}(X_i)=\\sigma_i^{2}$. Assume the random variables $X_i$ are mutually independent. A weighted mini-batch gradient estimator is formed as $\\widehat{g}=\\sum_{i \\in B} w_i X_i$, where the per-sample weights satisfy the constraint $\\sum_{i \\in B} w_i=1$ to preserve unbiasedness, i.e., $\\mathbb{E}[\\widehat{g}]=\\mu$. \n\nStarting from the definitions of expectation and variance and using only standard properties of independence and linearity, derive the weights that minimize the variance $\\operatorname{Var}(\\widehat{g})$ subject to $\\sum_{i \\in B} w_i=1$. Then, evaluate the optimal weights for a mini-batch with variances $\\sigma_1^2=1$, $\\sigma_2^2=4$, and $\\sigma_3^2=9$. \n\nExpress your final answer as a row vector of the three optimal weights in the order $(w_1, w_2, w_3)$. No rounding is required.", "solution": "The user wants to find the optimal weights for a mini-batch gradient estimator that minimize its variance, subject to the constraint that the estimator remains unbiased.\n\nFirst, let's validate the problem.\n\n### Step 1: Extract Givens\n-   The setting is Stochastic Gradient Descent (SGD) with a mini-batch.\n-   The mini-batch index set is $B=\\{1, 2, 3\\}$.\n-   $X_i$ for $i \\in B$ are scalar stochastic gradient estimates.\n-   The estimates are unbiased: $\\mathbb{E}[X_i]=\\mu$ for all $i \\in B$.\n-   The variance of each estimate is $\\operatorname{Var}(X_i)=\\sigma_i^{2}$.\n-   The random variables $X_i$ are mutually independent.\n-   The weighted mini-batch gradient estimator is $\\widehat{g}=\\sum_{i \\in B} w_i X_i$.\n-   The weights are constrained: $\\sum_{i \\in B} w_i=1$.\n-   A consequence of the constraint is that the estimator is unbiased: $\\mathbb{E}[\\widehat{g}]=\\mu$.\n-   The task is to derive the weights $w_i$ that minimize $\\operatorname{Var}(\\widehat{g})$ subject to the constraint.\n-   Specific variances are given for evaluation: $\\sigma_1^2=1$, $\\sigma_2^2=4$, and $\\sigma_3^2=9$.\n-   The final answer should be a row vector of weights $(w_1, w_2, w_3)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard analysis in the field of optimization methods for machine learning. It is well-posed, as it describes a constrained optimization problem with a quadratic objective function and a linear constraint, which guarantees a unique solution. The problem is stated objectively with precise mathematical definitions and no ambiguity. All necessary information is provided, and there are no contradictions. The problem is a non-trivial but solvable exercise in optimization.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe problem asks to find the weights $(w_1, w_2, w_3)$ that minimize the variance of the weighted mini-batch gradient estimator, $\\widehat{g}$, subject to a constraint that ensures the estimator is unbiased.\n\nThe estimator is defined as a linear combination of the individual stochastic gradient estimates:\n$$\n\\widehat{g} = \\sum_{i \\in B} w_i X_i = w_1 X_1 + w_2 X_2 + w_3 X_3\n$$\nThe constraint on the weights is:\n$$\n\\sum_{i \\in B} w_i = w_1 + w_2 + w_3 = 1\n$$\nFirst, we confirm that this constraint ensures the estimator $\\widehat{g}$ is unbiased. Using the linearity of expectation and the given fact that $\\mathbb{E}[X_i]=\\mu$:\n$$\n\\mathbb{E}[\\widehat{g}] = \\mathbb{E}\\left[\\sum_{i \\in B} w_i X_i\\right] = \\sum_{i \\in B} w_i \\mathbb{E}[X_i] = \\sum_{i \\in B} w_i \\mu = \\mu \\sum_{i \\in B} w_i\n$$\nGiven the constraint $\\sum_{i \\in B} w_i=1$, we have $\\mathbb{E}[\\widehat{g}]=\\mu$. This confirms the estimator remains unbiased.\n\nNext, we derive the expression for the variance of $\\widehat{g}$. The variance of a sum of independent random variables is the sum of their variances. Using the property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ and the mutual independence of the $X_i$:\n$$\n\\operatorname{Var}(\\widehat{g}) = \\operatorname{Var}\\left(\\sum_{i \\in B} w_i X_i\\right) = \\sum_{i \\in B} \\operatorname{Var}(w_i X_i) = \\sum_{i \\in B} w_i^2 \\operatorname{Var}(X_i)\n$$\nSubstituting $\\operatorname{Var}(X_i) = \\sigma_i^2$, we obtain the objective function to be minimized:\n$$\nV(w_1, w_2, w_3) = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_3^2 \\sigma_3^2\n$$\nThis is a constrained optimization problem. We want to minimize $V(w_1, w_2, w_3)$ subject to the constraint $g(w_1, w_2, w_3) = w_1 + w_2 + w_3 - 1 = 0$. We use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(w_1, w_2, w_3, \\lambda) = V(w_1, w_2, w_3) - \\lambda g(w_1, w_2, w_3)\n$$\n$$\n\\mathcal{L}(w_1, w_2, w_3, \\lambda) = (w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_3^2 \\sigma_3^2) - \\lambda(w_1 + w_2 + w_3 - 1)\n$$\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $w_i$ and $\\lambda$ to zero. For $i \\in \\{1, 2, 3\\}$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2 w_i \\sigma_i^2 - \\lambda = 0\n$$\nSolving for $w_i$, we find:\n$$\nw_i = \\frac{\\lambda}{2 \\sigma_i^2}\n$$\nThis shows that the optimal weight for each sample is inversely proportional to its variance. Now, we use the constraint to find the value of the Lagrange multiplier $\\lambda$. We substitute the expression for $w_i$ into the constraint equation $\\sum_{i=1}^3 w_i = 1$:\n$$\n\\sum_{i=1}^3 \\frac{\\lambda}{2 \\sigma_i^2} = 1\n$$\n$$\n\\frac{\\lambda}{2} \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_3^2} \\right) = 1\n$$\nSolving for $\\lambda$:\n$$\n\\lambda = \\frac{2}{\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2}}\n$$\nSubstitute this expression for $\\lambda$ back into the equation for $w_i$:\n$$\nw_i = \\frac{1}{2 \\sigma_i^2} \\left( \\frac{2}{\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2}} \\right) = \\frac{1/\\sigma_i^2}{\\sum_{j=1}^3 1/\\sigma_j^2}\n$$\nThis is the general formula for the optimal weights, known as inverse-variance weighting.\n\nNow, we evaluate a specific case with the given variances: $\\sigma_1^2=1$, $\\sigma_2^2=4$, and $\\sigma_3^2=9$.\nFirst, calculate the sum of the inverse variances:\n$$\n\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2} = \\frac{1}{1} + \\frac{1}{4} + \\frac{1}{9}\n$$\nTo sum these fractions, we find a common denominator, which is $36$:\n$$\n\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2} = \\frac{36}{36} + \\frac{9}{36} + \\frac{4}{36} = \\frac{36 + 9 + 4}{36} = \\frac{49}{36}\n$$\nNow we can compute each optimal weight $w_i$:\nFor $w_1$:\n$$\nw_1 = \\frac{1/\\sigma_1^2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/1}{49/36} = \\frac{1}{49/36} = \\frac{36}{49}\n$$\nFor $w_2$:\n$$\nw_2 = \\frac{1/\\sigma_2^2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/4}{49/36} = \\frac{1}{4} \\times \\frac{36}{49} = \\frac{9}{49}\n$$\nFor $w_3$:\n$$\nw_3 = \\frac{1/\\sigma_3^2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/9}{49/36} = \\frac{1}{9} \\times \\frac{36}{49} = \\frac{4}{49}\n$$\nThe optimal weights are $(w_1, w_2, w_3) = (\\frac{36}{49}, \\frac{9}{49}, \\frac{4}{49})$. As a check, their sum is $\\frac{36}{49} + \\frac{9}{49} + \\frac{4}{49} = \\frac{49}{49} = 1$, which satisfies the constraint.\n\nThe final answer is the row vector of these three optimal weights.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{36}{49} & \\frac{9}{49} & \\frac{4}{49}\n\\end{pmatrix}\n}\n$$", "id": "3150573"}, {"introduction": "While optimizing an estimator for minimum variance is powerful in theory, real-world datasets often contain outliers that can drastically corrupt a mini-batch gradient and destabilize the learning process. This exercise shifts the focus from statistical optimality to algorithmic robustness. You are challenged to think adversarially, identifying the \"worst-case\" mini-batch that maximizes estimation error, and then exploring gradient clipping as a practical defense mechanism. This analysis will provide a concrete understanding of why robustness-enhancing techniques are indispensable components of modern training pipelines [@problem_id:3150590].", "problem": "Consider a dataset of per-sample gradients in one dimension, given by the multiset $\\{-4,-1,0,0,1,5\\}$, and suppose a mini-batch of size $b=3$ is used to form a mini-batch gradient estimator. Let the full-data gradient be the arithmetic mean $\\bar{g}$ of all per-sample gradients, and the mini-batch gradient estimator be the arithmetic mean $\\hat{g}(B)$ of the gradients in the selected mini-batch $B$. In standard mini-batch stochastic methods, $\\hat{g}(B)$ is intended to be a low-variance estimate of $\\bar{g}$ when $B$ is sampled randomly. In adversarial mini-batch construction, an adversary selects the mini-batch $B$ of fixed size $b=3$ to maximize the worst-case squared deviation of the estimator from the true gradient, defined by the mean squared error (MSE) measure $R(B)=(\\hat{g}(B)-\\bar{g})^{2}$.\n\nStarting from the fundamental definitions of the arithmetic mean and basic properties of averages, derive the form of the adversarial selection that maximizes $R(B)$, and compute the exact worst-case value\n$$\nR^{\\star}=\\max_{\\substack{B\\subset\\{-4,-1,0,0,1,5\\}\\\\|B|=3}}(\\hat{g}(B)-\\bar{g})^{2}.\n$$\n\nAs a mitigation strategy, consider scalar gradient clipping of each per-sample gradient to the interval $[-\\tau,\\tau]$ with threshold $\\tau=3$. Define the clipped gradient mapping $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$ and the clipped dataset as $\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}$. Let $\\bar{g}_{\\tau}$ be the mean of the clipped gradients and $\\hat{g}_{\\tau}(B)$ be the mean over a selected mini-batch of clipped gradients. Compute the exact worst-case clipped value\n$$\nR^{\\star}_{\\tau}=\\max_{\\substack{B\\subset\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}\\\\|B|=3}}(\\hat{g}_{\\tau}(B)-\\bar{g}_{\\tau})^{2}.\n$$\n\nReport your final answer as a single row matrix containing $R^{\\star}$ and $R^{\\star}_{\\tau}$, in exact form. No rounding is required.", "solution": "The problem requires the calculation of two quantities representing the worst-case squared deviation of a mini-batch gradient estimator from the true full-data gradient. The first, $R^{\\star}$, is for the original dataset, and the second, $R^{\\star}_{\\tau}$, is for a dataset where each gradient has been subjected to scalar clipping.\n\nFirst, we analyze the unclipped case to determine $R^{\\star}$.\nThe given dataset of per-sample gradients is the multiset $S = \\{-4, -1, 0, 0, 1, 5\\}$. The total number of samples is $N=6$.\nThe full-data gradient, $\\bar{g}$, is the arithmetic mean of these values:\n$$\n\\bar{g} = \\frac{1}{N} \\sum_{g \\in S} g = \\frac{1}{6} (-4 + (-1) + 0 + 0 + 1 + 5) = \\frac{1}{6}\n$$\nThe mini-batch gradient estimator, $\\hat{g}(B)$, for a mini-batch $B$ of size $b=3$ is the mean of the gradients in that mini-batch:\n$$\n\\hat{g}(B) = \\frac{1}{b} \\sum_{g \\in B} g = \\frac{1}{3} \\sum_{g \\in B} g\n$$\nWe are tasked with finding the maximum possible value of the squared deviation $R(B) = (\\hat{g}(B) - \\bar{g})^2$ over all possible mini-batches $B$ of size $3$. This value is denoted by $R^{\\star}$:\n$$\nR^{\\star} = \\max_{B \\subset S, |B|=3} (\\hat{g}(B) - \\bar{g})^2\n$$\nTo maximize this expression, an adversary must select a mini-batch $B$ that maximizes the absolute difference $|\\hat{g}(B) - \\bar{g}|$. This occurs when the mini-batch mean $\\hat{g}(B)$ is either as large as possible or as small as possible. The value of $\\hat{g}(B)$ is directly proportional to the sum of the elements in $B$.\n\nTo find the maximum possible mean, $\\hat{g}_{max}$, we select the $b=3$ largest gradient values from $S$. These are $\\{0, 1, 5\\}$. Let this batch be $B_{max}$. The sum of its elements is $0 + 1 + 5 = 6$. The corresponding mean is:\n$$\n\\hat{g}(B_{max}) = \\frac{1}{3}(6) = 2\n$$\nTo find the minimum possible mean, $\\hat{g}_{min}$, we select the $b=3$ smallest gradient values from $S$. These are $\\{-4, -1, 0\\}$. Let this batch be $B_{min}$. The sum of its elements is $-4 + (-1) + 0 = -5$. The corresponding mean is:\n$$\n\\hat{g}(B_{min}) = \\frac{1}{3}(-5) = -\\frac{5}{3}\n$$\nWe now evaluate the deviation from the true mean $\\bar{g} = \\frac{1}{6}$ for these two extremal cases:\nThe deviation for $\\hat{g}(B_{max})$ is $2 - \\frac{1}{6} = \\frac{12}{6} - \\frac{1}{6} = \\frac{11}{6}$.\nThe deviation for $\\hat{g}(B_{min})$ is $-\\frac{5}{3} - \\frac{1}{6} = -\\frac{10}{6} - \\frac{1}{6} = -\\frac{11}{6}$.\nThe maximum absolute deviation is therefore $\\left|\\frac{11}{6}\\right| = \\frac{11}{6}$.\nThe worst-case squared deviation is the square of this value:\n$$\nR^{\\star} = \\left(\\frac{11}{6}\\right)^2 = \\frac{121}{36}\n$$\n\nNext, we analyze the clipped case to determine $R^{\\star}_{\\tau}$.\nThe gradient clipping is performed with a threshold $\\tau=3$, using the function $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$. We apply this to each gradient in the original dataset $S$:\n$\\phi(-4) = \\max\\{-3, \\min\\{-4, 3\\}\\} = -3$\n$\\phi(-1) = \\max\\{-3, \\min\\{-1, 3\\}\\} = -1$\n$\\phi(0) = 0$\n$\\phi(1) = 1$\n$\\phi(5) = \\max\\{-3, \\min\\{5, 3\\}\\} = 3$\nThe clipped dataset is the multiset $S_{\\tau} = \\{-3, -1, 0, 0, 1, 3\\}$.\nThe mean of the clipped gradients, $\\bar{g}_{\\tau}$, is:\n$$\n\\bar{g}_{\\tau} = \\frac{1}{N} \\sum_{g' \\in S_{\\tau}} g' = \\frac{1}{6}(-3 - 1 + 0 + 0 + 1 + 3) = \\frac{0}{6} = 0\n$$\nThe worst-case clipped squared deviation, $R^{\\star}_{\\tau}$, is the maximum of $(\\hat{g}_{\\tau}(B) - \\bar{g}_{\\tau})^2$ over all mini-batches $B$ of size $3$ from the clipped set $S_{\\tau}$. Since $\\bar{g}_{\\tau} = 0$, this simplifies to maximizing $(\\hat{g}_{\\tau}(B))^2$. This is equivalent to maximizing $|\\hat{g}_{\\tau}(B)|$.\nAs before, we find the mini-batches from $S_{\\tau}$ with the largest and smallest sums.\nFor the maximum sum, we select the $b=3$ largest elements from $S_{\\tau}$: $\\{0, 1, 3\\}$. The sum is $0 + 1 + 3 = 4$. The mean is $\\hat{g}_{\\tau,max} = \\frac{4}{3}$.\nFor the minimum sum, we select the $b=3$ smallest elements from $S_{\\tau}$: $\\{-3, -1, 0\\}$. The sum is $-3 - 1 + 0 = -4$. The mean is $\\hat{g}_{\\tau,min} = -\\frac{4}{3}$.\nThe extremal means are $\\frac{4}{3}$ and $-\\frac{4}{3}$. The maximum absolute value is $\\left|\\frac{4}{3}\\right| = \\frac{4}{3}$.\nThe worst-case clipped squared deviation is the square of this value:\n$$\nR^{\\star}_{\\tau} = \\left(\\frac{4}{3}\\right)^2 = \\frac{16}{9}\n$$\nThe problem requires reporting $R^{\\star}$ and $R^{\\star}_{\\tau}$ as a row matrix.\nWe have $R^{\\star} = \\frac{121}{36}$ and $R^{\\star}_{\\tau} = \\frac{16}{9}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{121}{36} & \\frac{16}{9}\n\\end{pmatrix}\n}\n$$", "id": "3150590"}, {"introduction": "In large-scale machine learning, the desired batch size is often constrained not by theory, but by the physical memory limits of the hardware. A widely used technique to overcome this is gradient accumulation, where gradients from several smaller \"micro-batches\" are aggregated to simulate an update from a single large batch. This problem moves beyond statistical equivalence to investigate the practical, computational consequences of this strategy. By analyzing the accumulation of floating-point rounding errors, you will uncover the subtle yet critical numerical trade-offs involved in implementing optimization algorithms at scale [@problem_id:3150664].", "problem": "You are minimizing an empirical risk with Stochastic Gradient Descent (SGD), estimating the gradient at each iteration by an average over a mini-batch of size $B$. Due to a strict memory limit, you can only process at most $b$ samples at once, with $b \\ll B$. To emulate the $B$-sample gradient, you adopt micro-batching with gradient accumulation: you split the $B$ samples into $m$ micro-batches of size $b$, where $m = B/b$ is an integer, compute each micro-batch average gradient, and then average those $m$ micro-averages.\n\nAssume arithmetic is performed in a floating-point system with rounding to nearest and unit roundoff $u$, following the standard model that each elementary floating-point operation introduces a multiplicative error factor of the form $(1+\\delta)$ with $|\\delta| \\leq u$. Focus on a single fixed component of the gradient vector at a given iteration, and assume all $B$ exact component values being averaged are nonnegative real numbers bounded above by a constant $G$, and that the exact batch mean of this component is strictly positive. Ignore stochastic sampling variance and treat the inputs as exact reals prior to floating-point rounding.\n\nConsider two schemes for computing the batch mean of this component:\n- Scheme A (hypothetical direct $B$-batch): compute the mean by naively summing $B$ values in sequence and then multiplying by $1/B$ once.\n- Scheme M (micro-batching with accumulation): for each of the $m$ micro-batches, naively sum $b$ values in sequence and multiply by $1/b$ once to form a micro-mean; then naively sum the $m$ micro-means in sequence and multiply by $1/m$ once to form the final mean.\n\nAssume naive summation order as written, and no compensated summation or special fused operations are used. Work to first order in $u$ (i.e., neglect terms of order $u^{2}$ and higher).\n\nLet $E_{A}$ denote a first-order worst-case bound on the relative rounding error in the mean computed by Scheme A, and let $E_{M}$ denote the analogous bound for Scheme M. What is the ratio $R = E_{M}/E_{A}$, expressed in closed form in terms of $b$ (and constants), under the stated assumptions? Your final answer must be a single closed-form analytic expression. No rounding is required.", "solution": "The problem statement is critically evaluated according to the specified validation protocol.\n\n### Step 1: Extract Givens\n-   **Method**: Mini-batch Stochastic Gradient Descent (SGD).\n-   **Batch sizes**: Full-batch size is $B$, micro-batch size is $b$. Memory limit is $b$ samples.\n-   **Micro-batching**: $m = B/b$ micro-batches, where $m$ is an integer.\n-   **Floating-point model**: Standard model with unit roundoff $u$. `fl(x op y) = (x op y)(1 + δ)` with $|\\delta| \\leq u$. Analysis is first-order in $u$.\n-   **Data properties**: The gradient components being averaged, let's call them $g_i$, are non-negative ($g_i \\ge 0$), bounded ($g_i \\le G$), and their exact batch mean is strictly positive.\n-   **Scheme A (Direct)**: Compute $\\frac{1}{B} \\sum_{i=1}^{B} g_i$ by first naively summing $B$ values, then performing one division by $B$.\n-   **Scheme M (Micro-batching)**: For each of $m$ micro-batches, compute a micro-mean $\\mu_j = \\frac{1}{b} \\sum_{k=1}^{b} g_{j,k}$ using naive summation then division. Then, compute the final mean by averaging the $m$ micro-means, again using naive summation then division: $\\frac{1}{m} \\sum_{j=1}^{m} \\mu_j$.\n-   **Objective**: Find the ratio $R = E_M / E_A$, where $E_A$ and $E_M$ are the first-order worst-case bounds on the relative rounding error for Scheme A and Scheme M, respectively.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the field of numerical analysis, specifically the study of floating-point arithmetic errors. The context is a realistic application in machine learning optimization. The problem is well-posed, with all necessary data and definitions provided to derive a unique analytical answer. The language is objective and formal. There are no contradictions, factual errors, or unscientific premises. The problem does not violate any of the specified invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation of the Solution\n\nThis problem requires the analysis of rounding errors for two different schemes of computing a mean. We will use the standard first-order error analysis for floating-point arithmetic. A key result we will use is the worst-case relative error bound for the naive summation of $N$ non-negative numbers.\n\nLet $S_N = \\sum_{i=1}^{N} x_i$ be the exact sum of $N$ non-negative numbers, $x_i \\ge 0$. The floating-point sum, computed as $\\hat{S}_N = \\text{fl}(\\sum_{i=1}^{N} x_i)$, can be shown to satisfy the following bound to first order in the unit roundoff $u$:\n$$\n|\\hat{S}_N - S_N| \\le (N-1)u S_N + O(u^2)\n$$\nThis gives a relative error bound for the sum of $|\\frac{\\hat{S}_N - S_N}{S_N}| \\le (N-1)u$.\n\nNow, we analyze the error in computing a mean, $\\mu = \\frac{1}{N}S_N$. The computation is performed as $\\hat{\\mu} = \\text{fl}(\\hat{S}_N / N)$. This involves one final division operation, which introduces an additional relative error.\n$$\n\\hat{\\mu} = \\left(\\frac{\\hat{S}_N}{N}\\right)(1 + \\delta_{div}) \\quad \\text{with } |\\delta_{div}| \\le u\n$$\nLet's express $\\hat{\\mu}$ in terms of the exact mean $\\mu$:\n$$\n\\hat{\\mu} = \\frac{S_N(1 + \\epsilon_{sum})}{N}(1+\\delta_{div}) = \\mu(1+\\epsilon_{sum})(1+\\delta_{div})\n$$\nwhere $\\epsilon_{sum} = (\\hat{S}_N-S_N)/S_N$ is the relative error of the sum, with $|\\epsilon_{sum}| \\le (N-1)u$.\nTo first order in $u$, the total relative error is:\n$$\n\\frac{\\hat{\\mu} - \\mu}{\\mu} \\approx \\epsilon_{sum} + \\delta_{div}\n$$\nThe worst-case bound on the total relative error is found by taking the maximum possible magnitude:\n$$\nE_N = \\max\\left|\\frac{\\hat{\\mu} - \\mu}{\\mu}\\right| = \\max|\\epsilon_{sum}| + \\max|\\delta_{div}| = (N-1)u + u = Nu\n$$\nThis general result provides the worst-case relative error bound for computing the mean of $N$ non-negative numbers using naive summation followed by division.\n\n**Analysis of Scheme A**\n\nScheme A computes the mean of $B$ samples directly. This is a direct application of the general result derived above with $N=B$.\nThe first-order worst-case bound on the relative error for Scheme A is:\n$$\nE_A = Bu\n$$\n\n**Analysis of Scheme M**\n\nScheme M is a two-stage process.\n**Stage 1: Computing micro-means.**\nFor each of the $m$ micro-batches, we compute the mean of $b$ samples. This is again an application of our general result, this time with $N=b$. The relative error for each computed micro-mean, $\\hat{\\mu}_j$, compared to its exact value, $\\mu_j$, is bounded by $bu$.\nLet $\\hat{\\mu}_j = \\mu_j(1 + \\eta_j)$, where $|\\eta_j| \\le bu$. These are the (noisy) inputs for the next stage. Since all original gradient components $g_{j,k}$ are non-negative, the exact micro-means $\\mu_j$ are also non-negative.\n\n**Stage 2: Averaging the micro-means.**\nIn this stage, we compute the final mean $\\hat{\\mu}_M$ by averaging the $m$ computed micro-means $\\hat{\\mu}_j$.\n$$\n\\hat{\\mu}_M = \\text{fl}\\left(\\frac{1}{m} \\sum_{j=1}^{m} \\hat{\\mu}_j \\right)\n$$\nThe total error in $\\hat{\\mu}_M$ comes from two sources:\n1.  **Propagated error**: The error already present in the inputs $\\hat{\\mu}_j$ from Stage 1.\n2.  **Generated error**: The new error introduced by the arithmetic operations (summation and division) in Stage 2.\n\nLet the true final mean be $\\mu = \\frac{1}{m} \\sum_{j=1}^{m} \\mu_j$.\nThe exact mean of the noisy inputs is $M_{noisy} = \\frac{1}{m} \\sum_{j=1}^{m} \\hat{\\mu}_j = \\frac{1}{m} \\sum_{j=1}^{m} \\mu_j(1+\\eta_j)$.\nThe relative propagated error is $\\frac{M_{noisy} - \\mu}{\\mu}$.\n$$\n\\frac{M_{noisy} - \\mu}{\\mu} = \\frac{\\frac{1}{m}\\sum\\mu_j(1+\\eta_j) - \\frac{1}{m}\\sum\\mu_j}{\\frac{1}{m}\\sum\\mu_j} = \\frac{\\sum\\mu_j\\eta_j}{\\sum\\mu_j}\n$$\nThe worst-case bound on this propagated error is:\n$$\n\\left|\\frac{\\sum\\mu_j\\eta_j}{\\sum\\mu_j}\\right| \\le \\frac{\\sum|\\mu_j||\\eta_j|}{\\sum|\\mu_j|} \\le \\frac{\\sum\\mu_j(bu)}{\\sum\\mu_j} = bu\n$$\nThe non-negativity of $\\mu_j$ is used here.\n\nThe generated error comes from computing the mean of $m$ numbers ($\\hat{\\mu}_j$). The inputs $\\hat{\\mu}_j$ are all non-negative. Using our general result for $N=m$, the relative error generated by this computation is bounded by $mu$.\n\nThe total relative error for $\\hat{\\mu}_M$ is, to first order, the sum of the propagated and generated errors. Therefore, the worst-case bound $E_M$ is the sum of their bounds:\n$$\nE_M = (\\text{propagated error bound}) + (\\text{generated error bound}) = bu + mu\n$$\n\n**Calculating the Ratio R**\n\nWe have the error bounds for both schemes:\n-   $E_A = Bu$\n-   $E_M = (b+m)u$\n\nThe ratio $R$ is given by:\n$$\nR = \\frac{E_M}{E_A} = \\frac{(b+m)u}{Bu} = \\frac{b+m}{B}\n$$\nThe problem states that $m = B/b$. Substituting this into the expression for $R$:\n$$\nR = \\frac{b + B/b}{B} = \\frac{b}{B} + \\frac{B/b}{B} = \\frac{b}{B} + \\frac{1}{b}\n$$\nThis is the closed-form expression for the ratio of the worst-case relative error bounds.\nIt can also be written as $\\frac{1}{m} + \\frac{1}{b}$ or $\\frac{b^2 + B}{bB}$. We will use the expression in terms of $b$ and $B$.\nThe phrasing \"in terms of $b$ (and constants)\" is interpreted to mean that problem parameters like $B$ can be included in the final expression as they are constant for a given problem instance.", "answer": "$$\\boxed{\\frac{b}{B} + \\frac{1}{b}}$$", "id": "3150664"}]}