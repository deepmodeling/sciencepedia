{"hands_on_practices": [{"introduction": "To truly understand the heavy-ball method, we must see it in action. This first practice provides a foundational, hands-on experience by pitting the heavy-ball method against standard gradient descent on a classic quadratic objective function. By implementing both algorithms and running them on a problem with a wide spectrum of curvatures, you will directly observe and quantify the core trade-off at the heart of momentum-based optimization: accelerated convergence versus a tendency to oscillate [@problem_id:3135479].", "problem": "You are asked to implement a program that compares the heavy-ball momentum method and classical gradient descent on a separable convex quadratic objective with a geometric progression of eigenvalues. The objective is the function $$f(x)=\\frac{1}{2}\\sum_{i=1}^{d}\\lambda_i x_i^2,$$ where $$\\lambda_i=\\lambda_{\\min}\\,q^{i-1}$$ for an integer dimension $$d\\geq 2$$, a positive base $$\\lambda_{\\min}>0$$, and a geometric ratio $$q>1$$. The comparison must quantify two effects: faster decay on coordinates with large $$\\lambda_i$$ and oscillations on coordinates with small $$\\lambda_i$$.\n\nStarting from fundamental principles of discrete-time steepest descent for quadratic objectives and the heavy-ball momentum concept from classical mechanics analogies, derive the iteration rules suitable for this quadratic, and determine constant step sizes that minimize the worst-case linear convergence rate over the interval $$[\\lambda_{\\min},\\lambda_{\\max}]$$, where $$\\lambda_{\\max}=\\lambda_{\\min}q^{d-1}$$. For gradient descent, determine a single step size $$\\alpha_{\\mathrm{GD}}$$ that minimizes the worst-case contraction factor across all coordinates. For the heavy-ball method, determine $$\\alpha_{\\mathrm{HB}}$$ and $$\\beta_{\\mathrm{HB}}$$ that minimize the worst-case spectral radius of the two-step linear recurrence on this interval. Use these parameters in your simulation, except in one boundary-condition test case where the heavy-ball momentum parameter $$\\beta_{\\mathrm{HB}}$$ is forced to $$0$$ to explicitly test the no-momentum limit.\n\nImplement both methods as follows, with the initial conditions $$x_{0,i}=1$$ for all $$i$$ and $$x_{-1}=x_0$$ for the heavy-ball method. Run each method for $$T$$ iterations. For gradient descent, use the single-step iteration appropriate for this quadratic. For the heavy-ball method, use the two-step iteration appropriate for momentum on this quadratic. Treat each coordinate independently according to its $$\\lambda_i$$.\n\nDefine two quantitative metrics that capture the trade-off:\n- Large-eigenvalue decay ratio: After $$T$$ iterations, compute $$R_{\\mathrm{large}}:=\\frac{\\sum_{i\\in\\mathcal{I}_{\\mathrm{large}}}\\left|x^{\\mathrm{HB}}_{T,i}\\right|}{\\sum_{i\\in\\mathcal{I}_{\\mathrm{large}}}\\left|x^{\\mathrm{GD}}_{T,i}\\right|},$$ where $$\\mathcal{I}_{\\mathrm{large}}$$ is the index set of the top half of coordinates by $$\\lambda_i$$ (i.e., those with the largest $$\\lambda_i$$ values). If the denominator is numerically zero, add a small positive term $$10^{-12}$$ to avoid division by zero. Values of $R_{\\mathrm{large}}  1$ indicate faster decay under heavy-ball momentum on large-eigenvalue coordinates relative to gradient descent.\n- Small-eigenvalue oscillation rate difference: For the bottom half of coordinates by $$\\lambda_i$$, compute the average fraction of iteration steps that flip sign, i.e., for each coordinate $$i$$ in the small half, count the number of $$k\\in\\{0,1,\\dots,T-1\\}$$ such that $$x_{k+1,i}\\cdot x_{k,i}0$$, divide by $$T$$, and average across these coordinates. Do this once for heavy-ball and once for gradient descent, and define $$\\Delta_{\\mathrm{osc,small}}:=\\text{fraction}^{\\mathrm{HB}}-\\text{fraction}^{\\mathrm{GD}}$$. Positive values indicate heavier oscillations under heavy-ball momentum on small-eigenvalue coordinates relative to gradient descent.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with two floating-point numbers per test case in the order [$R_{\\mathrm{large}}$, $\\Delta_{\\mathrm{osc,small}}$], flattened across all test cases. For example, the output should look like [$r_1, \\delta_1, r_2, \\delta_2, \\dots$].\n\nUse the following test suite to ensure coverage of typical and edge behaviors. In all cases, initialize $$x_{0,i}=1$$ for all $$i$$ and set $$x_{-1}=x_0$$ for the heavy-ball method:\n- Case $$1$$ (happy path): $$d=10$$, $$\\lambda_{\\min}=1.0$$, $$q=1.5$$, $$T=50$$, use optimal heavy-ball parameters over $$[\\lambda_{\\min},\\lambda_{\\max}]$$ and the optimal gradient descent step size over the same interval.\n- Case $$2$$ (strongly disparate eigenvalues): $$d=12$$, $$\\lambda_{\\min}=0.1$$, $$q=3.0$$, $$T=80$$, use optimal heavy-ball parameters and optimal gradient descent step size.\n- Case $$3$$ (near-uniform spectrum): $$d=8$$, $$\\lambda_{\\min}=1.0$$, $$q=1.01$$, $$T=60$$, use optimal heavy-ball parameters and optimal gradient descent step size.\n- Case $$4$$ (boundary, no momentum): $$d=10$$, $$\\lambda_{\\min}=1.0$$, $$q=2.0$$, $$T=50$$, set $$\\beta_{\\mathrm{HB}}=0$$ and use the same $$\\alpha$$ for heavy-ball and gradient descent equal to the optimal gradient descent step size over $$[\\lambda_{\\min},\\lambda_{\\max}]$$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [$R_{\\mathrm{large}}^{(1)}, \\Delta_{\\mathrm{osc,small}}^{(1)}, R_{\\mathrm{large}}^{(2)}, \\Delta_{\\mathrm{osc,small}}^{(2)}, R_{\\mathrm{large}}^{(3)}, \\Delta_{\\mathrm{osc,small}}^{(3)}, R_{\\mathrm{large}}^{(4)}, \\Delta_{\\mathrm{osc,small}}^{(4)}$], where the superscript $$^{(j)}$$ indexes the test case $$j\\in\\{1,2,3,4\\}$$. All outputs are unitless real numbers. No percentages should be printed; express all quantities as decimals. The final answer must be executable code that performs these computations exactly as specified and prints only the described single-line output.", "solution": "The user-provided problem statement is critically evaluated and found to be valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization. We will therefore proceed with a full solution.\n\nThe problem asks for a comparison between the classical gradient descent (GD) method and the heavy-ball (HB) momentum method for the minimization of a specific convex quadratic objective function $f(x)$.\n\nThe objective function is given by:\n$$f(x) = \\frac{1}{2}\\sum_{i=1}^{d}\\lambda_i x_i^2 = \\frac{1}{2} x^T \\Lambda x$$\nwhere $x \\in \\mathbb{R}^d$ and $\\Lambda$ is a diagonal matrix with entries $\\Lambda_{ii} = \\lambda_i$. The eigenvalues are structured in a geometric progression:\n$$\\lambda_i = \\lambda_{\\min}\\,q^{i-1} \\quad \\text{for } i=1, \\dots, d$$\nwith given constants $d \\geq 2$, $\\lambda_{\\min}  0$, and $q  1$.\n\nThe Hessian of this function is $\\nabla^2 f(x) = \\Lambda$, which is a constant diagonal matrix. This structural property allows the $d$-dimensional optimization problem to be decoupled into $d$ independent one-dimensional problems, one for each coordinate $x_i$:\n$$\\min_{x_i} f_i(x_i) = \\frac{1}{2}\\lambda_i x_i^2$$\nThe gradient for the $i$-th component is $\\nabla f_i(x_i) = \\lambda_i x_i$. We can analyze the convergence of both algorithms on each coordinate independently.\n\n### Gradient Descent (GD) Analysis\nThe gradient descent update rule is $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$. For the $i$-th coordinate, this becomes:\n$$x_{k+1,i} = x_{k,i} - \\alpha_{\\mathrm{GD}} (\\lambda_i x_{k,i}) = (1 - \\alpha_{\\mathrm{GD}} \\lambda_i) x_{k,i}$$\nThis is a first-order linear recurrence. The magnitude of the state $x_{k,i}$ is multiplied by a factor of $|1 - \\alpha_{\\mathrm{GD}} \\lambda_i|$ at each step. To ensure convergence for all coordinates, this contraction factor must be less than $1$ for all $\\lambda_i$ in the spectrum of the Hessian. The spectrum is the set of eigenvalues $\\{\\lambda_1, \\dots, \\lambda_d\\}$, which lies within the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$, where $\\lambda_{\\max} = \\lambda_{\\min}q^{d-1}$.\n\nThe problem requires finding the step size $\\alpha_{\\mathrm{GD}}$ that minimizes the worst-case (i.e., maximum) contraction factor over this interval:\n$$\\min_{\\alpha} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |1 - \\alpha \\lambda|$$\nThis is a classic result in optimization. The optimal step size is the one for which the function $g(\\lambda) = 1 - \\alpha \\lambda$ has equal magnitude at the endpoints of the interval: $|1 - \\alpha \\lambda_{\\min}| = |-(1 - \\alpha \\lambda_{\\max})|$. This yields:\n$$1 - \\alpha \\lambda_{\\min} = \\alpha \\lambda_{\\max} - 1 \\implies 2 = \\alpha (\\lambda_{\\min} + \\lambda_{\\max})$$\nThus, the optimal constant step size is:\n$$\\alpha_{\\mathrm{GD}}^* = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$$\nThe corresponding worst-case contraction factor is $\\rho_{\\mathrm{GD}}^* = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{\\kappa - 1}{\\kappa + 1}$, where $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ is the condition number of the Hessian.\n\n### Heavy-Ball (HB) Method Analysis\nThe heavy-ball method introduces a momentum term to the update rule:\n$$x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta(x_k - x_{k-1})$$\nFor the $i$-th coordinate, the update becomes a two-step linear recurrence:\n$$x_{k+1,i} = x_{k,i} - \\alpha_{\\mathrm{HB}}(\\lambda_i x_{k,i}) + \\beta_{\\mathrm{HB}}(x_{k,i} - x_{k-1,i})$$\n$$x_{k+1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})x_{k,i} - \\beta_{\\mathrm{HB}}x_{k-1,i}$$\nWe can analyze the stability and convergence of this recurrence by examining its characteristic polynomial:\n$$z^2 - (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})z + \\beta_{\\mathrm{HB}} = 0$$\nThe rate of convergence for coordinate $i$ is determined by the spectral radius $\\rho_{\\mathrm{HB}}(\\lambda_i)$, which is the maximum magnitude of the roots of this polynomial.\n\nThe problem asks for parameters $\\alpha_{\\mathrm{HB}}$ and $\\beta_{\\mathrm{HB}}$ that minimize the worst-case spectral radius over the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$. The solution to this min-max problem is also a standard result, established by Polyak. The optimal parameters are given by:\n$$\\alpha_{\\mathrm{HB}}^* = \\left(\\frac{2}{\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}}}\\right)^2 = \\frac{4}{(\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}})^2}$$\n$$\\beta_{\\mathrm{HB}}^* = \\left(\\frac{\\sqrt{\\lambda_{\\max}} - \\sqrt{\\lambda_{\\min}}}{\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}}}\\right)^2 = \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^2$$\nWith these parameters, the worst-case convergence factor is significantly improved compared to gradient descent: $\\rho_{\\mathrm{HB}}^* = \\sqrt{\\beta_{\\mathrm{HB}}^*} = \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}$.\n\n### Simulation and Metrics\nFor each test case, we perform the following steps:\n1.  Calculate $\\lambda_{\\max} = \\lambda_{\\min}q^{d-1}$ and the full set of eigenvalues $\\lambda_i$ for $i=1, \\dots, d$.\n2.  Calculate the optimal parameters $\\alpha_{\\mathrm{GD}}^*$, $\\alpha_{\\mathrm{HB}}^*$, and $\\beta_{\\mathrm{HB}}^*$ based on the formulas above, except for Case $4$ as specified.\n3.  Initialize the trajectories: $x_{0,i}=1$ for all $i$. For HB, $x_{-1}=x_0$.\n4.  Simulate both algorithms for $T$ iterations. The initial HB step from $x_0$ to $x_1$ uses $x_{-1}=x_0$:\n$x_{1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})x_{0,i} - \\beta_{\\mathrm{HB}}x_{-1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i)x_{0,i}$. This is a simple gradient step. Subsequent steps use the full recurrence.\n5.  After $T$ iterations, we compute the two specified metrics. The eigenvalues are naturally sorted since $q1$, so the \"bottom half\" are coordinates $i=1, \\dots, d/2$ and the \"top half\" are $i=d/2+1, \\dots, d$.\n    -   **Large-eigenvalue decay ratio, $R_{\\mathrm{large}}$**: This metric compares the magnitude of the final state vector components corresponding to large eigenvalues for HB vs. GD. A value less than $1$ indicates faster convergence for HB on these components.\n        $$R_{\\mathrm{large}}=\\frac{\\sum_{i=d/2+1}^{d}\\left|x^{\\mathrm{HB}}_{T,i}\\right|}{\\sum_{i=d/2+1}^{d}\\left|x^{\\mathrm{GD}}_{T,i}\\right|}$$\n        If the denominator is zero, a small value of $10^{-12}$ is added as specified.\n    -   **Small-eigenvalue oscillation rate difference, $\\Delta_{\\mathrm{osc,small}}$**: This metric quantifies the difference in oscillatory behavior on components with small eigenvalues.\n        $$\\Delta_{\\mathrm{osc,small}} = \\left(\\frac{1}{d/2} \\sum_{i=1}^{d/2} \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{I}(x^{\\mathrm{HB}}_{k+1,i} \\cdot x^{\\mathrm{HB}}_{k,i}  0)\\right) - \\left(\\frac{1}{d/2} \\sum_{i=1}^{d/2} \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{I}(x^{\\mathrm{GD}}_{k+1,i} \\cdot x^{\\mathrm{GD}}_{k,i}  0)\\right)$$\n        where $\\mathbb{I}(\\cdot)$ is the indicator function. A positive value implies that the HB method exhibits more sign flips (oscillations) on these components compared to GD.\n\nIn Case $4$, we are given $\\beta_{\\mathrm{HB}}=0$ and $\\alpha_{\\mathrm{HB}}=\\alpha_{\\mathrm{GD}}^*$. The HB update rule simplifies to $x_{k+1,i} = (1 - \\alpha_{\\mathrm{GD}}^*\\lambda_i)x_{k,i}$, which is identical to the GD update. Consequently, we expect the trajectories to be identical, leading to $R_{\\mathrm{large}}=1$ and $\\Delta_{\\mathrm{osc,small}}=0$. This serves as a sanity check for the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating and comparing Gradient Descent (GD)\n    and the Heavy-Ball (HB) method on a specified quadratic objective.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: happy path\n        {'d': 10, 'lambda_min': 1.0, 'q': 1.5, 'T': 50, 'beta_override': None},\n        # Case 2: strongly disparate eigenvalues\n        {'d': 12, 'lambda_min': 0.1, 'q': 3.0, 'T': 80, 'beta_override': None},\n        # Case 3: near-uniform spectrum\n        {'d': 8, 'lambda_min': 1.0, 'q': 1.01, 'T': 60, 'beta_override': None},\n        # Case 4: boundary, no momentum\n        {'d': 10, 'lambda_min': 1.0, 'q': 2.0, 'T': 50, 'beta_override': 0.0},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case['d']\n        lambda_min = case['lambda_min']\n        q = case['q']\n        T = case['T']\n        beta_override = case['beta_override']\n\n        # 1. Define eigenvalues and spectral interval\n        # Using 0-based indexing for arrays: i = 0 to d-1\n        indices = np.arange(d)\n        lambdas = lambda_min * q**indices\n        lambda_max = lambdas[-1]\n\n        # 2. Calculate optimal parameters for GD and HB\n        alpha_gd_opt = 2.0 / (lambda_min + lambda_max)\n        \n        # For Cases 1, 2, 3: use optimal HB parameters\n        if beta_override is None:\n            sqrt_l_min = np.sqrt(lambda_min)\n            sqrt_l_max = np.sqrt(lambda_max)\n            alpha_hb_opt = (2.0 / (sqrt_l_min + sqrt_l_max))**2\n            beta_hb_opt = ((sqrt_l_max - sqrt_l_min) / (sqrt_l_max + sqrt_l_min))**2\n        # For Case 4: use specified parameters\n        else:\n            alpha_hb_opt = alpha_gd_opt\n            beta_hb_opt = beta_override\n\n        # 3. Initialize trajectories\n        x_gd = np.zeros((d, T + 1))\n        x_hb = np.zeros((d, T + 1))\n        \n        x_gd[:, 0] = 1.0\n        x_hb[:, 0] = 1.0\n        \n        #\n        # 4. Run simulations\n        #\n        \n        # Gradient Descent simulation\n        gd_contraction = 1.0 - alpha_gd_opt * lambdas\n        for k in range(T):\n            x_gd[:, k + 1] = gd_contraction * x_gd[:, k]\n            \n        # Heavy-Ball simulation\n        # First step as per x_{-1} = x_0\n        x_hb[:, 1] = (1.0 - alpha_hb_opt * lambdas) * x_hb[:, 0]\n        # Subsequent steps\n        hb_c1 = 1.0 - alpha_hb_opt * lambdas + beta_hb_opt\n        hb_c2 = -beta_hb_opt\n        for k in range(1, T):\n            x_hb[:, k + 1] = hb_c1 * x_hb[:, k] + hb_c2 * x_hb[:, k - 1]\n\n        # 5. Calculate metrics\n        split_idx = d // 2\n\n        # Metric 1: Large-eigenvalue decay ratio (R_large)\n        large_indices = slice(split_idx, d)\n        \n        num = np.sum(np.abs(x_hb[large_indices, T]))\n        den = np.sum(np.abs(x_gd[large_indices, T]))\n        \n        if den == 0.0:\n            den = 1e-12\n        \n        r_large = num / den\n\n        # Metric 2: Small-eigenvalue oscillation rate difference (Delta_osc,small)\n        small_indices = slice(0, split_idx)\n        \n        # Calculate sign flips for trajectory from k=0 to T-1 (T steps)\n        # x[:, 1:T+1] is x_1...x_T\n        # x[:, 0:T] is x_0...x_{T-1}\n        num_coords_small = split_idx\n        \n        sign_flips_hb = np.sum((x_hb[small_indices, 1:T+1] * x_hb[small_indices, 0:T])  0)\n        fraction_hb = sign_flips_hb / (num_coords_small * T)\n        \n        sign_flips_gd = np.sum((x_gd[small_indices, 1:T+1] * x_gd[small_indices, 0:T])  0)\n        fraction_gd = sign_flips_gd / (num_coords_small * T)\n                                 \n        delta_osc_small = fraction_hb - fraction_gd\n\n        results.extend([r_large, delta_osc_small])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3135479"}, {"introduction": "While momentum is a powerful tool for smooth optimization, its behavior can change dramatically when encountering the sharp 'creases' of non-smooth functions. This exercise takes you into this more complex and realistic domain, where you will investigate how momentum can lead to a pathological 'zig-zagging' that hinders convergence on a piecewise linear objective [@problem_id:3135497]. More than just identifying the problem, this practice challenges you to think like an algorithm designer by implementing a creative solution: an anisotropic momentum that intelligently adapts to the local geometry of the function.", "problem": "Consider a convex, piecewise linear objective in two dimensions defined by $$f(\\mathbf{x}) = |x_1| + s\\,|x_2|,$$ where $\\mathbf{x} = (x_1,x_2)$ and $s$ is a positive scalar. Let $\\partial f(\\mathbf{x})$ denote the subdifferential of $f$ at $\\mathbf{x}$. Use the convention $$\\mathrm{sign}(u)=\\begin{cases}1,u0,\\\\0,u=0,\\\\-1,u0,\\end{cases}$$ and, for this $f$, choose the subgradient rule $$\\mathbf{g}(\\mathbf{x})=\\big(\\mathrm{sign}(x_1),\\,s\\,\\mathrm{sign}(x_2)\\big)\\in \\partial f(\\mathbf{x}).$$\n\nStart from a valid physical-mathematical base: Newton’s Second Law of Motion and the definition of a conservative force derived from a potential. A unit-mass particle in a potential energy landscape given by $f(\\mathbf{x})$ with linear viscous damping obeys $$\\ddot{\\mathbf{x}}(t) = -\\nabla f(\\mathbf{x}(t)) - c\\,\\dot{\\mathbf{x}}(t),$$ where $c0$ is a damping coefficient, $\\dot{\\mathbf{x}}(t)$ is velocity, and $\\ddot{\\mathbf{x}}(t)$ is acceleration. Derive a discrete-time optimization iteration from this law for nonsmooth $f$ by replacing $\\nabla f$ with a chosen subgradient $\\mathbf{g}(\\mathbf{x})\\in\\partial f(\\mathbf{x})$ and using consistent first-principles time-discretization. Explain how the resulting method can exhibit “zig-zagging” near nonsmooth points (such as coordinate axes) when the momentum term persists across sign changes of the subgradient.\n\nThen, design a remedy that uses anisotropic momentum: a coordinate-wise momentum coefficient that reacts to subgradient sign agreement. Specifically, let the per-coordinate momentum coefficient at iteration $t$ be $$\\beta_i^{(t)}=\\begin{cases}\\beta_{\\mathrm{same}},\\mathrm{sign}\\big(g_i^{(t)}\\big)=\\mathrm{sign}\\big(g_i^{(t-1)}\\big),\\\\ \\beta_{\\mathrm{flip}},\\text{otherwise,}\\end{cases}$$ for $i\\in\\{1,2\\}$, where $0\\le \\beta_{\\mathrm{flip}}\\beta_{\\mathrm{same}}1$, and $g_i^{(t)}$ is the $i$th component of the chosen subgradient at step $t$. Your program must implement both the isotropic momentum version (single scalar momentum coefficient) and the anisotropic momentum remedy (coordinate-wise momentum coefficients) on the objective $f(\\mathbf{x})$ above, using the subgradient rule given.\n\nDefine the zig-zag count for a trajectory $\\{\\mathbf{x}^{(t)}\\}_{t\\ge 0}$ as follows. For each coordinate $i\\in\\{1,2\\}$, consider the signed step sequence $$\\Delta x_i^{(t)}=x_i^{(t+1)}-x_i^{(t)}.$$ Scan this sequence from $t=0$ to the final iteration, skip any $\\Delta x_i^{(t)}$ equal to $0$, and count one zig-zag whenever the signs of consecutive nonzero steps differ. The total zig-zag count is the sum of the two coordinate-wise counts. Use this definition to measure zig-zagging.\n\nYour program must produce results for the following test suite by simulating the derived discrete-time iteration. In all cases, the initial “previous iterate” must be set equal to the initial iterate to represent zero initial velocity. Use the sign rule above for subgradients.\n\n- Test Case $\\mathbf{A}$ (happy path, shows zig-zagging and the remedy):\n  - Parameters: $s=10$, step size $\\alpha=0.1$, isotropic momentum coefficient $\\beta=0.9$, number of iterations $T=200$, initial point $\\mathbf{x}^{(0)}=(5,-5)$.\n  - Output: two integers, the zig-zag count for isotropic momentum and the zig-zag count for anisotropic momentum with $\\beta_{\\mathrm{same}}=0.9$ and $\\beta_{\\mathrm{flip}}=0.1$.\n\n- Test Case $\\mathbf{B}$ (boundary condition, zero momentum):\n  - Parameters: $s=10$, step size $\\alpha=0.1$, isotropic momentum coefficient $\\beta=0$, number of iterations $T=200$, initial point $\\mathbf{x}^{(0)}=(5,-5)$.\n  - Output: two integers, the zig-zag count for isotropic momentum and the zig-zag count for anisotropic momentum using $\\beta_{\\mathrm{same}}=0$ and $\\beta_{\\mathrm{flip}}=0$.\n\n- Test Case $\\mathbf{C}$ (edge case, start at the minimizer):\n  - Parameters: $s=10$, step size $\\alpha=0.3$, isotropic momentum coefficient $\\beta=0.9$, number of iterations $T=50$, initial point $\\mathbf{x}^{(0)}=(0,0)$.\n  - Output: two floats, the final Euclidean norm $\\|\\mathbf{x}^{(T)}\\|_2$ for isotropic momentum and the final Euclidean norm $\\|\\mathbf{x}^{(T)}\\|_2$ for anisotropic momentum with $\\beta_{\\mathrm{same}}=0.9$ and $\\beta_{\\mathrm{flip}}=0.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[$ \\text{result}_1, \\text{result}_2, \\dots $]\"). The results must be in the order: Test Case $\\mathbf{A}$ isotropic zig-zag count, Test Case $\\mathbf{A}$ anisotropic zig-zag count, Test Case $\\mathbf{B}$ isotropic zig-zag count, Test Case $\\mathbf{B}$ anisotropic zig-zag count, Test Case $\\mathbf{C}$ isotropic final norm, Test Case $\\mathbf{C}$ anisotropic final norm.", "solution": "The problem is valid as it is scientifically grounded in optimization theory and classical mechanics, is well-posed with all necessary parameters and definitions provided, and is expressed in objective, formal language. We will first derive the discrete-time optimization algorithm, then explain the mechanism of zig-zagging, and finally implement the specified algorithms to compute the required results.\n\n### 1. Derivation of the Heavy-Ball Method from Physical Principles\n\nThe problem starts with the second-order ordinary differential equation (ODE) describing the motion of a unit-mass particle in a potential energy landscape $f(\\mathbf{x})$ subject to linear viscous damping:\n$$ \\ddot{\\mathbf{x}}(t) + c\\,\\dot{\\mathbf{x}}(t) + \\nabla f(\\mathbf{x}(t)) = \\mathbf{0} $$\nHere, $\\ddot{\\mathbf{x}}(t)$ is acceleration, $\\dot{\\mathbf{x}}(t)$ is velocity, $c  0$ is the damping coefficient, and $-\\nabla f(\\mathbf{x})$ is the conservative force derived from the potential $f$.\n\nFor the non-smooth objective function $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$, the gradient $\\nabla f$ is not defined everywhere. We replace it with a valid subgradient $\\mathbf{g}(\\mathbf{x}) \\in \\partial f(\\mathbf{x})$, as specified by the rule $\\mathbf{g}(\\mathbf{x}) = (\\mathrm{sign}(x_1),\\,s\\,\\mathrm{sign}(x_2))$. The continuous-time dynamics become:\n$$ \\ddot{\\mathbf{x}}(t) + c\\,\\dot{\\mathbf{x}}(t) + \\mathbf{g}(\\mathbf{x}(t)) = \\mathbf{0} $$\nTo derive a discrete-time iteration, we discretize time with a step size $\\Delta t  0$, letting $\\mathbf{x}^{(k)} \\approx \\mathbf{x}(k \\Delta t)$. We employ finite difference approximations for the derivatives at time $t_k = k \\Delta t$:\n- Acceleration (central difference): $\\ddot{\\mathbf{x}}(t_k) \\approx \\frac{\\mathbf{x}^{(k+1)} - 2\\mathbf{x}^{(k)} + \\mathbf{x}^{(k-1)}}{(\\Delta t)^2}$\n- Velocity (backward difference): $\\dot{\\mathbf{x}}(t_k) \\approx \\frac{\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}}{\\Delta t}$\n\nSubstituting these approximations into the ODE yields:\n$$ \\frac{\\mathbf{x}^{(k+1)} - 2\\mathbf{x}^{(k)} + \\mathbf{x}^{(k-1)}}{(\\Delta t)^2} + c \\left( \\frac{\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}}{\\Delta t} \\right) + \\mathbf{g}(\\mathbf{x}^{(k)}) = \\mathbf{0} $$\nWe rearrange the equation to solve for the next iterate, $\\mathbf{x}^{(k+1)}$:\n$$ \\mathbf{x}^{(k+1)} = 2\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} - c \\Delta t (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) $$\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - c \\Delta t (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) $$\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) + (1 - c \\Delta t)(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\nThis equation has the canonical form of the heavy-ball method:\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{g}^{(k)} + \\beta(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\nwhere $\\mathbf{g}^{(k)} = \\mathbf{g}(\\mathbf{x}^{(k)})$, the step size is identified as $\\alpha = (\\Delta t)^2$, and the momentum coefficient is $\\beta = 1 - c \\Delta t$. The term $\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}$ represents the momentum. The initial condition of zero velocity is modeled by setting the \"previous\" iterate equal to the initial iterate, $\\mathbf{x}^{(-1)} = \\mathbf{x}^{(0)}$, which makes the initial momentum term zero.\n\n### 2. The Zig-Zagging Phenomenon in Non-Smooth Optimization\n\nThe objective function $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$ has non-differentiable \"creases\" along the coordinate axes where the subgradient is discontinuous. The heavy-ball method, with its isotropic momentum coefficient $\\beta$, is prone to oscillatory behavior, or \"zig-zagging,\" when crossing these creases.\n\nConsider an iterate $\\mathbf{x}^{(k)}$ approaching the minimizer at $\\mathbf{0}$. As it crosses an axis, for instance, the $x_2$-axis, the sign of its first coordinate $x_1^{(k)}$ flips. This causes an abrupt sign change in the first component of the subgradient, $g_1^{(k)} = \\mathrm{sign}(x_1^{(k)})$. The gradient-based portion of the update, $-\\alpha \\mathbf{g}^{(k)}$, immediately changes direction to correct the overshoot.\n\nHowever, the momentum term, $\\beta(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)})$, retains inertia from previous steps. This accumulated momentum was built when the subgradient was pointing in the previous direction. After crossing the crease, this momentum opposes the new, corrective subgradient direction. If the momentum coefficient $\\beta$ is large (e.g., $\\beta=0.9$), the momentum term can dominate the subgradient term, causing the next iterate $\\mathbf{x}^{(k+1)}$ to overshoot the axis again, but in the opposite direction. This process repeats, leading to a zig-zagging trajectory across the crease, which slows down convergence.\n\n### 3. Anisotropic Momentum as a Remedy\n\nThe proposed anisotropic momentum scheme is designed to mitigate this zig-zagging. The per-coordinate momentum coefficient $\\beta_i^{(t)}$ is adapted based on the history of the subgradient signs:\n$$ \\beta_i^{(t)}=\\begin{cases}\\beta_{\\mathrm{same}},\\mathrm{sign}\\big(g_i^{(t)}\\big)=\\mathrm{sign}\\big(g_i^{(t-1)}\\big)\\\\ \\beta_{\\mathrm{flip}},\\text{otherwise}\\end{cases} $$\nwhere $0 \\le \\beta_{\\mathrm{flip}}  \\beta_{\\mathrm{same}}  1$.\n\nWhen a coordinate $x_i$ crosses an axis, the corresponding subgradient component $g_i$ flips sign. The rule detects this disagreement, $\\mathrm{sign}(g_i^{(t)}) \\neq \\mathrm{sign}(g_i^{(t-1)})$, and drastically reduces the momentum for that specific coordinate by setting its coefficient to the small value $\\beta_{\\mathrm{flip}}$. This action effectively \"quenches\" the detrimental momentum that causes the overshoot. By damping the oscillation in the problematic coordinate, the algorithm can take a more controlled step guided by the new subgradient. In regions where the subgradient sign is consistent, the coefficient remains high at $\\beta_{\\mathrm{same}}$, preserving the acceleration benefit of momentum. The update rule becomes:\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{g}^{(k)} + \\mathbf{\\beta}^{(k)} \\odot (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\nwhere $\\mathbf{\\beta}^{(k)}$ is a vector of coordinate-wise momentum coefficients and $\\odot$ denotes element-wise multiplication. This targeted damping of momentum only where needed is the key to suppressing zig-zagging and improving convergence on non-smooth objectives.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    def get_subgradient(x, s):\n        \"\"\"Computes the subgradient for f(x) = |x1| + s*|x2|.\"\"\"\n        return np.array([np.sign(x[0]), s * np.sign(x[1])])\n\n    def count_zigzags(trajectory):\n        \"\"\"\n        Counts zig-zags as per the problem definition.\n        The trajectory is a numpy array of shape (T+1, 2).\n        \"\"\"\n        total_zigzag_count = 0\n        num_points = trajectory.shape[0]\n        if num_points  2:\n            return 0\n        \n        #\n        # For each coordinate\n        for i in range(2):\n            steps = trajectory[1:, i] - trajectory[:-1, i]\n            nonzero_steps = steps[steps != 0]\n\n            if len(nonzero_steps)  2:\n                continue\n\n            coord_zigzag_count = 0\n            last_sign = np.sign(nonzero_steps[0])\n            for j in range(1, len(nonzero_steps)):\n                current_sign = np.sign(nonzero_steps[j])\n                if current_sign != last_sign:\n                    coord_zigzag_count += 1\n                last_sign = current_sign\n            total_zigzag_count += coord_zigzag_count\n        \n        return total_zigzag_count\n\n    def run_simulation(s, alpha, T, x0, mode, beta_iso=0.9, beta_same=0.9, beta_flip=0.1):\n        \"\"\"\n        Runs the optimization simulation for either isotropic or anisotropic momentum.\n        \"\"\"\n        x_curr = np.array(x0, dtype=float)\n        x_prev = np.array(x0, dtype=float)\n        \n        trajectory = [x_curr.copy()]\n        \n        # Initialize previous gradient for anisotropic mode check\n        g_prev = get_subgradient(x_curr, s)\n\n        for _ in range(T):\n            g_curr = get_subgradient(x_curr, s)\n            \n            if mode == 'isotropic':\n                beta = beta_iso\n            elif mode == 'anisotropic':\n                signs_g_curr = np.sign(g_curr)\n                signs_g_prev = np.sign(g_prev)\n                # Element-wise check for sign agreement\n                beta = np.where(signs_g_curr == signs_g_prev, beta_same, beta_flip)\n            else:\n                raise ValueError(\"Invalid mode specified.\")\n\n            momentum_term = beta * (x_curr - x_prev)\n            x_next = x_curr - alpha * g_curr + momentum_term\n            \n            trajectory.append(x_next.copy())\n            \n            # Update state for the next iteration\n            x_prev = x_curr\n            x_curr = x_next\n            g_prev = g_curr\n            \n        return np.array(trajectory)\n\n    results = []\n\n    # Test Case A\n    s_A = 10.0\n    alpha_A = 0.1\n    beta_iso_A = 0.9\n    T_A = 200\n    x0_A = (5.0, -5.0)\n    beta_same_A = 0.9\n    beta_flip_A = 0.1\n    \n    traj_A_iso = run_simulation(s_A, alpha_A, T_A, x0_A, 'isotropic', beta_iso=beta_iso_A)\n    zigzag_A_iso = count_zigzags(traj_A_iso)\n    results.append(zigzag_A_iso)\n    \n    traj_A_aniso = run_simulation(s_A, alpha_A, T_A, x0_A, 'anisotropic', beta_same=beta_same_A, beta_flip=beta_flip_A)\n    zigzag_A_aniso = count_zigzags(traj_A_aniso)\n    results.append(zigzag_A_aniso)\n\n    # Test Case B\n    s_B = 10.0\n    alpha_B = 0.1\n    beta_iso_B = 0.0\n    T_B = 200\n    x0_B = (5.0, -5.0)\n    beta_same_B = 0.0\n    beta_flip_B = 0.0\n\n    traj_B_iso = run_simulation(s_B, alpha_B, T_B, x0_B, 'isotropic', beta_iso=beta_iso_B)\n    zigzag_B_iso = count_zigzags(traj_B_iso)\n    results.append(zigzag_B_iso)\n\n    traj_B_aniso = run_simulation(s_B, alpha_B, T_B, x0_B, 'anisotropic', beta_same=beta_same_B, beta_flip=beta_flip_B)\n    zigzag_B_aniso = count_zigzags(traj_B_aniso)\n    results.append(zigzag_B_aniso)\n\n    # Test Case C\n    s_C = 10.0\n    alpha_C = 0.3\n    beta_iso_C = 0.9\n    T_C = 50\n    x0_C = (0.0, 0.0)\n    beta_same_C = 0.9\n    beta_flip_C = 0.1\n\n    traj_C_iso = run_simulation(s_C, alpha_C, T_C, x0_C, 'isotropic', beta_iso=beta_iso_C)\n    norm_C_iso = np.linalg.norm(traj_C_iso[-1])\n    results.append(norm_C_iso)\n\n    traj_C_aniso = run_simulation(s_C, alpha_C, T_C, x0_C, 'anisotropic', beta_same=beta_same_C, beta_flip=beta_flip_C)\n    norm_C_aniso = np.linalg.norm(traj_C_aniso[-1])\n    results.append(norm_C_aniso)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3135497"}, {"introduction": "The impressive acceleration of the heavy-ball method hinges on tuning its parameters, $\\alpha$ and $\\beta$, to the curvature of the landscape. But what happens if our knowledge of that landscape is imperfect? This analytical practice explores the consequences of such a mismatch, where improperly tuned momentum can cause the iterates to 'ring' or spiral around the minimum instead of converging directly [@problem_id:3135498]. By deriving the precise conditions for this oscillatory behavior, you will gain a deeper appreciation for the method's sensitivity and the mathematical connection between its parameters and its dynamic stability.", "problem": "Consider minimizing a one-dimensional quadratic objective function $f(x) = \\tfrac{1}{2} \\lambda_t x^2$ with true curvature $\\lambda_t  0$ using the Heavy-Ball method. The Heavy-Ball iteration, starting from $x_{-1}$ and $x_0$, is defined by the update $x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1})$, where $\\nabla f(x) = \\lambda_t x$. The parameters $\\alpha$ and $\\beta$ are chosen using the classical tuning for a strongly convex quadratic based on estimated spectral bounds $\\mu_e$ and $L_e$, with $0  \\mu_e \\le L_e$. Suppose that the algorithm incorrectly estimates the curvature, so that the actual curvature $\\lambda_t$ differs from the estimated bounds in a way that may cause oscillatory “ringing” around the optimum $x^\\star = 0$.\n\nYour tasks are:\n- Starting from the fundamental definitions given above, derive a condition that determines whether the sequence $\\{x_k\\}$ exhibits oscillatory behavior (complex-conjugate characteristic roots) and, when oscillation occurs, derive a computable expression for the radius of the first ring in terms of $\\mu_e$, $L_e$, $\\lambda_t$, and the initial condition $x_0$. Use the initial condition $x_{-1} = x_0$. Define the “first ring radius” to be the amplitude of the oscillatory envelope at iteration $k = 1$. If the sequence does not oscillate (real characteristic roots), define the ring radius to be $0$. All angles must be treated in radians.\n- Implement a complete, runnable program that computes this ring radius for each test case below, using the classical Heavy-Ball tuning based on $(\\mu_e, L_e)$, and produces a single line of output containing the results as a comma-separated list enclosed in square brackets. Each radius must be expressed as a float rounded to six decimal places.\n\nTest suite (each parameter set is $(\\mu_e, L_e, \\lambda_t, x_0)$):\n1. $(1.0, 9.0, 5.0, 1.0)$\n2. $(1.0, 9.0, 9.0, 1.0)$\n3. $(1.0, 9.0, 20.0, 1.0)$\n4. $(0.25, 9.0, 2.0, 1.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the ring radius for the $i$-th test case, rounded to six decimal places.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Objective function: $f(x) = \\tfrac{1}{2} \\lambda_t x^2$ with true curvature $\\lambda_t  0$.\n- Optimization method: Heavy-Ball method.\n- Iteration update rule: $x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1})$.\n- Gradient: $\\nabla f(x) = \\lambda_t x$.\n- Initial conditions: Starting from $x_{-1}$ and $x_0$, with the specific condition $x_{-1} = x_0$.\n- Parameter tuning: $\\alpha$ and $\\beta$ are chosen using classical tuning based on estimated spectral bounds $\\mu_e$ and $L_e$, where $0  \\mu_e \\le L_e$.\n- Task 1: Derive a condition for oscillatory behavior (complex-conjugate characteristic roots).\n- Task 2: When oscillation occurs, derive a computable expression for the \"first ring radius\" in terms of $\\mu_e$, $L_e$, $\\lambda_t$, and $x_0$.\n- Definition: \"First ring radius\" is the amplitude of the oscillatory envelope at iteration $k = 1$.\n- Condition: If the sequence does not oscillate (real characteristic roots), the ring radius is defined to be $0$.\n- Test suite: Four parameter sets $(\\mu_e, L_e, \\lambda_t, x_0)$ are provided for computation.\n    1. $(1.0, 9.0, 5.0, 1.0)$\n    2. $(1.0, 9.0, 9.0, 1.0)$\n    3. $(1.0, 9.0, 20.0, 1.0)$\n    4. $(0.25, 9.0, 2.0, 1.0)$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem concerns the Heavy-Ball method, a well-established algorithm in the field of numerical optimization. The analysis of its convergence behavior for quadratic functions is a standard and fundamental topic. The physics and mathematics are sound.\n- **Well-Posed**: The problem is well-posed. The objective is clearly defined, the algorithm is specified, and the initial conditions are given. The task is to derive and compute a specific quantity, for which a unique solution exists based on the provided information.\n- **Objective**: The problem is stated using precise mathematical language and definitions, free of any subjectivity or ambiguity. The term \"ring radius\" is explicitly defined for the purpose of the problem.\n- **Completeness and Consistency**: The problem is self-contained. It provides the function, the iteration rule, the parameter tuning method, and the initial conditions needed for the derivation. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Ring Radius\n\nThe Heavy-Ball iteration for the objective function $f(x) = \\frac{1}{2} \\lambda_t x^2$ with gradient $\\nabla f(x) = \\lambda_t x$ is given by:\n$$\nx_{k+1} = x_k - \\alpha (\\lambda_t x_k) + \\beta (x_k - x_{k-1})\n$$\nThis can be rearranged into a second-order linear homogeneous recurrence relation:\n$$\nx_{k+1} - (1 - \\alpha\\lambda_t + \\beta) x_k + \\beta x_{k-1} = 0\n$$\nThe behavior of the sequence $\\{x_k\\}$ is governed by the roots of its characteristic equation:\n$$\nr^2 - (1 - \\alpha\\lambda_t + \\beta) r + \\beta = 0\n$$\nThe roots $r_{1,2}$ are given by the quadratic formula:\n$$\nr_{1,2} = \\frac{(1 - \\alpha\\lambda_t + \\beta) \\pm \\sqrt{(1 - \\alpha\\lambda_t + \\beta)^2 - 4\\beta}}{2}\n$$\nThe sequence $\\{x_k\\}$ exhibits oscillatory behavior if and only if the characteristic roots $r_{1,2}$ are a complex-conjugate pair. This occurs when the discriminant $\\Delta_c$ of the characteristic equation is negative:\n$$\n\\Delta_c = (1 - \\alpha\\lambda_t + \\beta)^2 - 4\\beta  0\n$$\nIf $\\Delta_c \\ge 0$, the roots are real, there is no oscillation, and by the problem's definition, the ring radius is $0$. We now focus on the case where $\\Delta_c  0$.\n\nThe parameters $\\alpha$ and $\\beta$ are determined by the classical tuning rules based on estimated bounds $\\mu_e$ and $L_e$:\n$$\n\\alpha = \\frac{4}{(\\sqrt{L_e} + \\sqrt{\\mu_e})^2}\n$$\n$$\n\\beta = \\left( \\frac{\\sqrt{L_e} - \\sqrt{\\mu_e}}{\\sqrt{L_e} + \\sqrt{\\mu_e}} \\right)^2\n$$\nWhen the roots are complex, the general solution for $x_k$ is of the form $x_k = c_1 r_1^k + c_2 r_2^k$. Since the coefficients of the recurrence are real, the roots must be a complex conjugate pair, $r_2 = \\bar{r_1}$. For $x_k$ to be real, the coefficients must also be conjugates, $c_2 = \\bar{c_1}$. Let $r_1 = R e^{i\\theta}$ and $c_1 = C e^{i\\phi}$. The solution can be written as:\n$$\nx_k = 2 \\text{Re}(c_1 r_1^k) = 2 \\text{Re}(C e^{i\\phi} (R e^{i\\theta})^k) = 2 C R^k \\cos(k\\theta + \\phi)\n$$\nThe term $A_k = 2 C R^k$ is the amplitude of the oscillatory envelope at iteration $k$. The problem defines the \"first ring radius\" as this amplitude at $k=1$, which is $A_1 = 2 C R$. The magnitude of the complex roots is $R = |r_1| = \\sqrt{r_1 r_2}$. From the characteristic equation, the product of the roots is $r_1 r_2 = \\beta$, so $R = \\sqrt{\\beta}$. The radius is therefore $A_1 = 2|c_1| \\sqrt{\\beta}$.\n\nWe find the coefficient $c_1$ using the initial conditions. We are given $x_{-1} = x_0$. We also need $x_1$. From the update rule at $k=0$:\n$$\nx_1 = (1 - \\alpha\\lambda_t + \\beta) x_0 - \\beta x_{-1}\n$$\nSubstituting $x_{-1} = x_0$:\n$$\nx_1 = (1 - \\alpha\\lambda_t + \\beta) x_0 - \\beta x_0 = (1 - \\alpha\\lambda_t) x_0\n$$\nThe coefficients $c_1$ and $c_2$ must satisfy:\n$$\n\\begin{cases}\nc_1 + c_2 = x_0  (k=0) \\\\\nc_1 r_1 + c_2 r_2 = x_1 = (1 - \\alpha\\lambda_t) x_0  (k=1)\n\\end{cases}\n$$\nSolving this system for $c_1$ gives:\n$$\nc_1 = x_0 \\frac{1 - \\alpha\\lambda_t - r_2}{r_1 - r_2}\n$$\nNow we compute its magnitude $|c_1|$. Let $P = 1 - \\alpha\\lambda_t + \\beta$. The roots are $r_{1,2} = \\frac{P \\pm i\\sqrt{-\\Delta_c}}{2}$.\nThe denominator is $|r_1 - r_2| = |i\\sqrt{-\\Delta_c}| = \\sqrt{-\\Delta_c}$.\nThe numerator is $|1 - \\alpha\\lambda_t - r_2| = |(P - \\beta) - \\frac{P - i\\sqrt{-\\Delta_c}}{2}| = |\\frac{P - 2\\beta + i\\sqrt{-\\Delta_c}}{2}|$.\nThe magnitude squared of the numerator is:\n$$\n\\frac{1}{4} \\left( (P - 2\\beta)^2 + (-\\Delta_c) \\right) = \\frac{1}{4} \\left( (P - 2\\beta)^2 + 4\\beta - P^2 \\right) = \\frac{1}{4} (P^2 - 4P\\beta + 4\\beta^2 + 4\\beta - P^2) = \\beta^2 - P\\beta + \\beta = \\beta(1 - P + \\beta)\n$$\nWe have $1 - P + \\beta = 1 - (1 - \\alpha\\lambda_t + \\beta) + \\beta = \\alpha\\lambda_t$.\nSo, the magnitude of the numerator is $\\sqrt{\\beta(\\alpha\\lambda_t)}$.\nTherefore, $|c_1| = |x_0| \\frac{\\sqrt{\\beta \\alpha \\lambda_t}}{\\sqrt{-\\Delta_c}}$.\n\nThe first ring radius $A_1$ is:\n$$\nA_1 = 2 |c_1| \\sqrt{\\beta} = 2 |x_0| \\frac{\\sqrt{\\beta \\alpha \\lambda_t}}{\\sqrt{-\\Delta_c}} \\sqrt{\\beta} = 2 |x_0| \\beta \\frac{\\sqrt{\\alpha \\lambda_t}}{\\sqrt{-\\Delta_c}}\n$$\nSubstituting $-\\Delta_c = 4\\beta - (1 - \\alpha\\lambda_t + \\beta)^2$, the final expression for the ring radius is:\n$$\nA_1 = 2 |x_0| \\beta \\frac{\\sqrt{\\alpha \\lambda_t}}{\\sqrt{4\\beta - (1 - \\alpha\\lambda_t + \\beta)^2}}\n$$\nThis formula is applicable only if the discriminant $\\Delta_c = (1 - \\alpha\\lambda_t + \\beta)^2 - 4\\beta$ is negative. If $\\Delta_c \\ge 0$, the radius is $0$.\n\nThe computational procedure for each test case $(\\mu_e, L_e, \\lambda_t, x_0)$ is:\n1.  Compute $\\alpha$ and $\\beta$ using the classical tuning formulas.\n2.  Compute the discriminant $\\Delta_c = (1 - \\alpha\\lambda_t + \\beta)^2 - 4\\beta$.\n3.  If $\\Delta_c \\ge 0$, the ring radius is $0$.\n4.  If $\\Delta_c  0$, compute the ring radius using the derived formula for $A_1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the first ring radius for the Heavy-Ball method on a quadratic\n    function for a given set of test cases.\n    \"\"\"\n    # Test suite: each tuple is (mu_e, L_e, lambda_t, x_0)\n    test_cases = [\n        (1.0, 9.0, 5.0, 1.0),\n        (1.0, 9.0, 9.0, 1.0),\n        (1.0, 9.0, 20.0, 1.0),\n        (0.25, 9.0, 2.0, 1.0)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        mu_e, L_e, lambda_t, x_0 = case\n\n        # Step 1: Compute Heavy-Ball parameters alpha and beta from estimated bounds\n        sqrt_L_e = np.sqrt(L_e)\n        sqrt_mu_e = np.sqrt(mu_e)\n        \n        # Ensure the denominator is not zero, though problem constraints (0  mu_e = L_e) prevent this.\n        if (sqrt_L_e + sqrt_mu_e)**2 == 0:\n            # This case should not be reached with valid inputs.\n            # Handle defensively.\n            results.append(0.0)\n            continue\n            \n        alpha = 4.0 / (sqrt_L_e + sqrt_mu_e)**2\n        \n        # A more stable way to compute beta\n        # beta = ((sqrt(L_e) - sqrt(mu_e)) / (sqrt(L_e) + sqrt(mu_e)))**2\n        # is equivalent to:\n        condition_number_sqrt = sqrt_L_e / sqrt_mu_e\n        beta = ((condition_number_sqrt - 1) / (condition_number_sqrt + 1))**2\n\n        # Step 2: Compute the discriminant of the characteristic equation\n        # r^2 - (1 - alpha*lambda_t + beta)r + beta = 0\n        poly_coeff_p = 1.0 - alpha * lambda_t + beta\n        discriminant = poly_coeff_p**2 - 4.0 * beta\n\n        # Step 3  4: Determine if oscillation occurs and compute the radius\n        ring_radius = 0.0\n        if discriminant  0:\n            # Oscillation occurs. Compute the radius using the derived formula.\n            # Radius = 2 * |x_0| * beta * sqrt(alpha * lambda_t) / sqrt(-discriminant)\n            \n            numerator = 2.0 * np.abs(x_0) * beta * np.sqrt(alpha * lambda_t)\n            denominator = np.sqrt(-discriminant)\n            \n            if denominator  1e-12: # Avoid division by zero\n                ring_radius = numerator / denominator\n\n        results.append(round(ring_radius, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3135498"}]}