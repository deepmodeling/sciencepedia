{"hands_on_practices": [{"introduction": "To truly understand how primal-dual interior-point methods work, we must first get our hands dirty with the core mechanics of a single iteration. This practice dissects the calculation of the predictor (or affine-scaling) direction, which is the initial step an IPM takes toward the optimal solution. By computing this direction from first principles for a small, concrete linear program, you will gain invaluable insight into how the algorithm systematically reduces the duality gap while navigating the interior of the feasible region [@problem_id:3164522].", "problem": "Consider the Linear Programming (LP) problem minimize $c^{\\top}x$ subject to $Ax=b$ and $x \\ge 0$, with data $A=\\begin{pmatrix}1 & 2\\end{pmatrix}$, $b=3$, and $c=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$. In a primal-dual interior-point method, suppose you are at a strictly feasible primal-dual point $x^{(0)}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, $y^{(0)}=0.25$, and the dual slack $s^{(0)}$ is defined by the dual feasibility condition $A^{\\top}y^{(0)}+s^{(0)}=c$, so that $s^{(0)}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}-\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}\\cdot 0.25$.\n\nStarting from the Karush-Kuhn-Tucker (KKT) conditions and their Newton linearization for the predictor (affine-scaling) step with zero centering parameter, derive the predictor direction $(\\Delta x_{\\mathrm{aff}}, \\Delta y_{\\mathrm{aff}}, \\Delta s_{\\mathrm{aff}})$. Then, along the ray $x(\\alpha)=x^{(0)}+\\alpha\\,\\Delta x_{\\mathrm{aff}}$, $s(\\alpha)=s^{(0)}+\\alpha\\,\\Delta s_{\\mathrm{aff}}$ with step length $\\alpha \\ge 0$, form the duality gap $g(\\alpha)=x(\\alpha)^{\\top}s(\\alpha)$ and determine the exact line-search step $\\alpha^{\\star}$ that minimizes $g(\\alpha)$ subject to maintaining strict positivity $x(\\alpha) \\ge 0$ and $s(\\alpha) \\ge 0$ (componentwise). Also compute the duality gaps at the fixed fraction-to-the-boundary steps $\\alpha=\\frac{1}{2}\\alpha_{\\max}$ and $\\alpha=\\frac{9}{10}\\alpha_{\\max}$, where $\\alpha_{\\max}$ is the largest step preserving nonnegativity in all components of $x(\\alpha)$ and $s(\\alpha)$.\n\nRound your final reported $\\alpha^{\\star}$ to four significant figures. No units are required.", "solution": "The user wants to solve a problem on primal-dual interior-point methods.\n\n### Step 1: Problem Validation\nI will first validate the problem statement.\n\n#### Extracted Givens\n- **LP Problem**: minimize $c^{\\top}x$ subject to $Ax=b$ and $x \\ge 0$.\n- **Data**:\n  - $A = \\begin{pmatrix}1 & 2\\end{pmatrix}$\n  - $b = 3$\n  - $c = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$\n- **Initial Point**:\n  - Primal variables: $x^{(0)} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$\n  - Dual variables: $y^{(0)} = 0.25$\n  - Dual slacks: $s^{(0)}$ is defined by $A^{\\top}y^{(0)} + s^{(0)} = c$.\n- **Task**:\n  1. Derive the predictor (affine-scaling) direction $(\\Delta x_{\\mathrm{aff}}, \\Delta y_{\\mathrm{aff}}, \\Delta s_{\\mathrm{aff}})$ using Newton's method on the Karush-Kuhn-Tucker (KKT) conditions with a centering parameter of zero.\n  2. Define the search ray $x(\\alpha) = x^{(0)} + \\alpha\\Delta x_{\\mathrm{aff}}$ and $s(\\alpha) = s^{(0)} + \\alpha\\Delta s_{\\mathrm{aff}}$.\n  3. Form the duality gap function $g(\\alpha) = x(\\alpha)^{\\top}s(\\alpha)$.\n  4. Find the step length $\\alpha^{\\star}$ that minimizes $g(\\alpha)$ while maintaining strict positivity ($x(\\alpha) > 0$, $s(\\alpha) > 0$).\n  5. Compute the duality gaps at steps $\\alpha = \\frac{1}{2}\\alpha_{\\max}$ and $\\alpha = \\frac{9}{10}\\alpha_{\\max}$, where $\\alpha_{\\max}$ is the maximum feasible step length.\n  6. Report $\\alpha^{\\star}$ rounded to four significant figures.\n\n#### Validation\nThe problem is set within the standard mathematical framework of linear programming and interior-point methods. I will check the self-consistency of the provided data.\n\n- **Primal Feasibility of $x^{(0)}$**:\n  - $A x^{(0)} = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = 1 \\cdot 1 + 2 \\cdot 1 = 3$. This matches the given $b=3$. The constraint $Ax=b$ is satisfied.\n  - $x^{(0)} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} \\ge 0$. This is satisfied. In fact, $x^{(0)} > 0$, so it is a strictly feasible primal point.\n\n- **Dual Feasibility with $y^{(0)}$**:\n  - The dual feasibility constraint is $A^{\\top}y + s = c$. The problem asks to define $s^{(0)}$ from this condition using $y^{(0)}$.\n  - $A^{\\top} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$.\n  - $s^{(0)} = c - A^{\\top}y^{(0)} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}(0.25) = \\begin{pmatrix}2 - 0.25 \\\\ 1 - 0.5\\end{pmatrix} = \\begin{pmatrix}1.75 \\\\ 0.5\\end{pmatrix}$.\n  - We must check if $s^{(0)} \\ge 0$. Indeed, all components are positive, so $(y^{(0)}, s^{(0)})$ constitutes a strictly feasible dual point.\n\nThe initial point $(x^{(0)}, y^{(0)}, s^{(0)})$ is strictly feasible for the primal and dual problems. The problem is well-defined, scientifically sound, and internally consistent.\n\n#### Verdict\nThe problem is valid.\n\n### Step 2: Solution\n\nThe primal-dual interior-point method aims to solve the following system of KKT conditions:\n$$\n\\begin{cases}\nA^{\\top}y + s = c & \\text{(Dual feasibility)} \\\\\nAx = b & \\text{(Primal feasibility)} \\\\\nXSe = 0 & \\text{(Complementarity)}\n\\end{cases}\n$$\nwhere $X = \\text{diag}(x)$, $S = \\text{diag}(s)$, and $e$ is a vector of ones. The method perturbs the complementarity condition to $XSe = \\mu e$ and applies Newton's method to find a step $(\\Delta x, \\Delta y, \\Delta s)$ towards a solution.\n\nThe linearized system is:\n$$\n\\begin{pmatrix}\n0 & A^{\\top} & I \\\\\nA & 0 & 0 \\\\\nS & 0 & X\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta x \\\\\n\\Delta y \\\\\n\\Delta s\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nc - A^{\\top}y - s \\\\\nb - Ax \\\\\n\\sigma\\mu e - XSe\n\\end{pmatrix}\n$$\nAt the feasible point $(x^{(0)}, y^{(0)}, s^{(0)})$, the feasibility residuals are zero: $c - A^{\\top}y^{(0)} - s^{(0)} = 0$ and $b - Ax^{(0)} = 0$. For the predictor (affine-scaling) step, the centering parameter $\\sigma$ is set to $0$. The system for the direction $(\\Delta x_{\\mathrm{aff}}, \\Delta y_{\\mathrm{aff}}, \\Delta s_{\\mathrm{aff}})$ simplifies to:\n$$\n\\begin{align*}\nA^{\\top} \\Delta y_{\\mathrm{aff}} + \\Delta s_{\\mathrm{aff}} &= 0 \\\\\nA \\Delta x_{\\mathrm{aff}} &= 0 \\\\\nS^{(0)} \\Delta x_{\\mathrm{aff}} + X^{(0)} \\Delta s_{\\mathrm{aff}} &= -X^{(0)}S^{(0)}e\n\\end{align*}\n$$\nLet's substitute the given values:\n$A = \\begin{pmatrix}1 & 2\\end{pmatrix}$, $x^{(0)} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, $s^{(0)} = \\begin{pmatrix}1.75 \\\\ 0.5\\end{pmatrix}$.\n$X^{(0)} = \\text{diag}(x^{(0)}) = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}$, $S^{(0)} = \\text{diag}(s^{(0)}) = \\begin{pmatrix}1.75 & 0 \\\\ 0 & 0.5\\end{pmatrix}$.\nNotice $X^{(0)}$ is the identity matrix $I$. Let $\\Delta x_{\\mathrm{aff}} = \\begin{pmatrix}\\Delta x_1 \\\\ \\Delta x_2\\end{pmatrix}$, $\\Delta s_{\\mathrm{aff}} = \\begin{pmatrix}\\Delta s_1 \\\\ \\Delta s_2\\end{pmatrix}$, and $\\Delta y_{\\mathrm{aff}}$ is a scalar.\n\nFrom $A \\Delta x_{\\mathrm{aff}} = 0$:\n$\\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}\\Delta x_1 \\\\ \\Delta x_2\\end{pmatrix} = 0 \\implies \\Delta x_1 + 2\\Delta x_2 = 0$.\n\nFrom $A^{\\top} \\Delta y_{\\mathrm{aff}} + \\Delta s_{\\mathrm{aff}} = 0$:\n$\\begin{pmatrix}1 \\\\ 2\\end{pmatrix} \\Delta y_{\\mathrm{aff}} + \\begin{pmatrix}\\Delta s_1 \\\\ \\Delta s_2\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} \\implies \\Delta s_1 = -\\Delta y_{\\mathrm{aff}}$ and $\\Delta s_2 = -2\\Delta y_{\\mathrm{aff}}$.\n\nFrom $S^{(0)} \\Delta x_{\\mathrm{aff}} + X^{(0)} \\Delta s_{\\mathrm{aff}} = -X^{(0)}S^{(0)}e$:\n$\\begin{pmatrix}1.75 & 0 \\\\ 0 & 0.5\\end{pmatrix} \\begin{pmatrix}\\Delta x_1 \\\\ \\Delta x_2\\end{pmatrix} + \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}\\Delta s_1 \\\\ \\Delta s_2\\end{pmatrix} = -\\begin{pmatrix}1.75 \\cdot 1 \\\\ 0.5 \\cdot 1\\end{pmatrix} = \\begin{pmatrix}-1.75 \\\\ -0.5\\end{pmatrix}$.\nThis gives two equations:\n$1.75 \\Delta x_1 + \\Delta s_1 = -1.75$\n$0.5 \\Delta x_2 + \\Delta s_2 = -0.5$\n\nWe have a system to solve. Substitute expressions for $\\Delta x_1, \\Delta s_1, \\Delta s_2$:\n$1.75 (-2\\Delta x_2) + (-\\Delta y_{\\mathrm{aff}}) = -1.75 \\implies -3.5\\Delta x_2 - \\Delta y_{\\mathrm{aff}} = -1.75$\n$0.5 \\Delta x_2 + (-2\\Delta y_{\\mathrm{aff}}) = -0.5 \\implies 0.5\\Delta x_2 - 2\\Delta y_{\\mathrm{aff}} = -0.5$\n\nFrom the second equation, $0.5\\Delta x_2 = 2\\Delta y_{\\mathrm{aff}} - 0.5 \\implies \\Delta x_2 = 4\\Delta y_{\\mathrm{aff}} - 1$.\nSubstitute into the first equation:\n$-3.5(4\\Delta y_{\\mathrm{aff}} - 1) - \\Delta y_{\\mathrm{aff}} = -1.75$\n$-14\\Delta y_{\\mathrm{aff}} + 3.5 - \\Delta y_{\\mathrm{aff}} = -1.75$\n$-15\\Delta y_{\\mathrm{aff}} = -5.25$\n$\\Delta y_{\\mathrm{aff}} = \\frac{5.25}{15} = \\frac{21/4}{15} = \\frac{21}{60} = \\frac{7}{20} = 0.35$.\n\nNow, we back-substitute to find the other components:\n$\\Delta x_2 = 4(0.35) - 1 = 1.4 - 1 = 0.4 = \\frac{2}{5}$.\n$\\Delta x_1 = -2\\Delta x_2 = -2(0.4) = -0.8 = -\\frac{4}{5}$.\n$\\Delta s_1 = -\\Delta y_{\\mathrm{aff}} = -0.35 = -\\frac{7}{20}$.\n$\\Delta s_2 = -2\\Delta y_{\\mathrm{aff}} = -2(0.35) = -0.7 = -\\frac{7}{10}$.\n\nThe predictor direction is $(\\Delta x_{\\mathrm{aff}}, \\Delta y_{\\mathrm{aff}}, \\Delta s_{\\mathrm{aff}})$ where:\n$\\Delta x_{\\mathrm{aff}} = \\begin{pmatrix}-0.8 \\\\ 0.4\\end{pmatrix}$, $\\Delta y_{\\mathrm{aff}} = 0.35$, $\\Delta s_{\\mathrm{aff}} = \\begin{pmatrix}-0.35 \\\\ -0.7\\end{pmatrix}$.\n\nNext, we form the duality gap function $g(\\alpha) = x(\\alpha)^{\\top}s(\\alpha)$ along the ray defined by this direction, for a step length $\\alpha \\ge 0$:\n$x(\\alpha) = x^{(0)} + \\alpha\\Delta x_{\\mathrm{aff}} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} + \\alpha\\begin{pmatrix}-0.8 \\\\ 0.4\\end{pmatrix} = \\begin{pmatrix}1 - 0.8\\alpha \\\\ 1 + 0.4\\alpha\\end{pmatrix}$.\n$s(\\alpha) = s^{(0)} + \\alpha\\Delta s_{\\mathrm{aff}} = \\begin{pmatrix}1.75 \\\\ 0.5\\end{pmatrix} + \\alpha\\begin{pmatrix}-0.35 \\\\ -0.7\\end{pmatrix} = \\begin{pmatrix}1.75 - 0.35\\alpha \\\\ 0.5 - 0.7\\alpha\\end{pmatrix}$.\n\nThe duality gap is $g(\\alpha) = (x^{(0)} + \\alpha\\Delta x_{\\mathrm{aff}})^{\\top}(s^{(0)} + \\alpha\\Delta s_{\\mathrm{aff}})$.\n$g(\\alpha) = (x^{(0)})^{\\top}s^{(0)} + \\alpha((x^{(0)})^{\\top}\\Delta s_{\\mathrm{aff}} + (\\Delta x_{\\mathrm{aff}})^{\\top}s^{(0)}) + \\alpha^2 (\\Delta x_{\\mathrm{aff}})^{\\top}\\Delta s_{\\mathrm{aff}}$.\nThe initial duality gap is $(x^{(0)})^{\\top}s^{(0)} = 1(1.75) + 1(0.5) = 2.25$.\nThe linear term's coefficient is $\\sum_i(x_i^{(0)}\\Delta s_i + s_i^{(0)}\\Delta x_i)$. From the Newton system, this is $\\sum_i(-x_i^{(0)}s_i^{(0)}) = -(x^{(0)})^{\\top}s^{(0)} = -2.25$.\nThe quadratic term's coefficient is $(\\Delta x_{\\mathrm{aff}})^{\\top}\\Delta s_{\\mathrm{aff}}$. Theoretically, since $A\\Delta x_{\\mathrm{aff}}=0$ and $\\Delta s_{\\mathrm{aff}} = -A^{\\top}\\Delta y_{\\mathrm{aff}}$, this term is $(\\Delta x_{\\mathrm{aff}})^{\\top}(-A^{\\top}\\Delta y_{\\mathrm{aff}}) = -(A\\Delta x_{\\mathrm{aff}})^{\\top}\\Delta y_{\\mathrm{aff}} = 0$.\nLet's verify with our numbers: $(\\Delta x_{\\mathrm{aff}})^{\\top}\\Delta s_{\\mathrm{aff}} = (-0.8)(-0.35) + (0.4)(-0.7) = 0.28 - 0.28 = 0$.\nSo the duality gap function is linear:\n$g(\\alpha) = 2.25 - 2.25\\alpha$.\n\nTo find $\\alpha^{\\star}$, we must maintain strict positivity: $x(\\alpha) > 0$ and $s(\\alpha) > 0$.\n$1 - 0.8\\alpha > 0 \\implies \\alpha < 1/0.8 = 1.25$.\n$1 + 0.4\\alpha > 0$, which is true for all $\\alpha \\ge 0$.\n$1.75 - 0.35\\alpha > 0 \\implies \\alpha < 1.75/0.35 = 5$.\n$0.5 - 0.7\\alpha > 0 \\implies \\alpha < 0.5/0.7 = 5/7$.\nThe largest $\\alpha$ that maintains positivity for all components is $\\alpha_{\\max} = \\min(1.25, 5, 5/7) = 5/7$.\nSo, $\\alpha$ must be in the interval $[0, 5/7)$.\n\nWe want to minimize $g(\\alpha) = 2.25 - 2.25\\alpha$, which is a strictly decreasing function of $\\alpha$. To make $g(\\alpha)$ as small as possible, we must choose $\\alpha$ to be as large as possible. The supremum of the feasible set for $\\alpha$ is $\\alpha_{\\max}=5/7$. While the minimum is not attained on the open interval $[0, 5/7)$, the problem of finding the \"exact line-search step\" is conventionally understood as finding the boundary point where the infimum is achieved. Thus, we take $\\alpha^{\\star} = \\alpha_{\\max}$.\n$\\alpha^{\\star} = 5/7 \\approx 0.7142857...$\nRounding to four significant figures, we get $\\alpha^{\\star} \\approx 0.7143$.\n\nFinally, we compute the duality gaps for the fractional steps.\n$\\alpha_{\\max} = 5/7$.\nThe first step is $\\alpha_1 = \\frac{1}{2}\\alpha_{\\max} = \\frac{1}{2} \\cdot \\frac{5}{7} = \\frac{5}{14}$.\n$g(\\frac{5}{14}) = 2.25(1 - \\frac{5}{14}) = \\frac{9}{4} \\cdot \\frac{9}{14} = \\frac{81}{56}$.\n\nThe second step is $\\alpha_2 = \\frac{9}{10}\\alpha_{\\max} = \\frac{9}{10} \\cdot \\frac{5}{7} = \\frac{45}{70} = \\frac{9}{14}$.\n$g(\\frac{9}{14}) = 2.25(1 - \\frac{9}{14}) = \\frac{9}{4} \\cdot \\frac{5}{14} = \\frac{45}{56}$.\n\nThe problem asks for the value of $\\alpha^{\\star}$ rounded to four significant figures.", "answer": "$$\n\\boxed{0.7143}\n$$", "id": "3164522"}, {"introduction": "An algorithm is only as good as its stopping condition. How do we confidently declare that we have found an optimal solution? This exercise challenges you to think critically about what constitutes optimality by deriving the standard stopping criteria based on primal feasibility, dual feasibility, and the duality gap. Through a carefully constructed hypothetical scenario, you will discover why small feasibility residuals alone are insufficient and why monitoring the duality gap, $x^{\\top}s$, is absolutely essential for preventing premature termination and ensuring a genuinely optimal solution is found [@problem_id:3164561].", "problem": "Consider the Primal-Dual Interior-Point Method (PDIP) applied to Linear Programming (LP), where the primal problem is to minimize $c^{\\top}x$ subject to $Ax=b$ and $x \\geq 0$, and the dual problem is to maximize $b^{\\top}y$ subject to $A^{\\top}y + s = c$ and $s \\geq 0$. Starting from the Karush-Kuhn-Tucker (KKT) conditions for optimality, derive residuals associated with primal feasibility and dual feasibility, define the duality gap in terms of the primal and dual variables, and use these to obtain a stopping test expressed in terms of bounds on the infinity norms of the residuals and the duality gap, with a tolerance parameter $\\epsilon > 0$. Then, construct a concrete LP and a strictly positive primal-dual iterate whose primal and dual residuals are very small but whose duality gap is large, and explain the implications for stopping decisions in PDIP.\n\nSpecifically, consider the LP with $A = \\begin{pmatrix}1 & 1\\end{pmatrix}$, $b = 1$, and $c = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$. Take the iterate $x = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{1}{2}\\end{pmatrix}$, $y = -100$, and $s = \\begin{pmatrix}100 \\\\ 100\\end{pmatrix}$. Compute the duality gap $x^{\\top}s$ for this iterate. Express your final answer as an exact value. No rounding is required.", "solution": "The problem asks for an analysis of the stopping criteria for a Primal-Dual Interior-Point (PDIP) method applied to Linear Programming (LP), culminating in a calculation for a specific example designed to illustrate a key principle.\n\nFirst, we establish the theoretical background. The standard form of a primal LP problem is:\n$$ \\text{minimize} \\quad z_p = c^{\\top}x $$\n$$ \\text{subject to} \\quad Ax = b, \\quad x \\geq 0 $$\nwhere $x \\in \\mathbb{R}^n$, $c \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^m$.\n\nThe corresponding dual LP problem is:\n$$ \\text{maximize} \\quad z_d = b^{\\top}y $$\n$$ \\text{subject to} \\quad A^{\\top}y + s = c, \\quad s \\geq 0 $$\nwhere $y \\in \\mathbb{R}^m$ are the dual variables and $s \\in \\mathbb{R}^n$ are the dual slack variables.\n\nThe Karush-Kuhn-Tucker (KKT) conditions provide the necessary and sufficient conditions for optimality for a primal-dual pair $(x, y, s)$. These conditions are:\n1.  **Primal Feasibility**: $Ax = b$ and $x \\geq 0$.\n2.  **Dual Feasibility**: $A^{\\top}y + s = c$ and $s \\geq 0$.\n3.  **Complementary Slackness**: $x_i s_i = 0$ for all $i=1, \\dots, n$. This can be written in vector form as $XSe = 0$, where $X = \\text{diag}(x)$, $S = \\text{diag}(s)$, and $e$ is a vector of ones.\n\nPDIP methods generate a sequence of iterates $(x^k, y^k, s^k)$ that are strictly positive ($x^k > 0$, $s^k > 0$) but do not necessarily satisfy the other KKT conditions. To monitor progress towards optimality, we define residuals that measure the violation of these conditions.\n\nThe **primal feasibility residual**, $r_p$, measures the violation of the primal equality constraints:\n$$ r_p = b - Ax $$\nThe **dual feasibility residual**, $r_d$, measures the violation of the dual equality constraints:\n$$ r_d = c - A^{\\top}y - s $$\nAn iterate is considered primal-feasible if $r_p=0$ and dual-feasible if $r_d=0$.\n\nThe **duality gap** is the difference between the primal and dual objective function values, $z_p - z_d = c^{\\top}x - b^{\\top}y$. For a primal-feasible $x$ and a dual-feasible $(y, s)$, this gap can be simplified:\n$$ c^{\\top}x - b^{\\top}y = (A^{\\top}y + s)^{\\top}x - (Ax)^{\\top}y = (y^{\\top}A + s^{\\top})x - x^{\\top}A^{\\top}y = y^{\\top}Ax + s^{\\top}x - y^{\\top}Ax = x^{\\top}s $$\nThus, the expression $x^{\\top}s$ is the duality gap, which must be zero at optimality due to the complementary slackness condition.\n\nA robust stopping criterion for a PDIP method must ensure that all KKT conditions are approximately satisfied. This involves checking that the feasibility residuals are small and the duality gap is small. A common stopping test for a given tolerance $\\epsilon > 0$ is to terminate when the relative residuals and the relative duality gap are smaller than $\\epsilon$:\n$$ \\frac{\\|r_p\\|_{\\infty}}{1 + \\|b\\|_{\\infty}} \\leq \\epsilon, \\quad \\frac{\\|r_d\\|_{\\infty}}{1 + \\|c\\|_{\\infty}} \\leq \\epsilon, \\quad \\frac{x^{\\top}s}{n(1 + |c^{\\top}x|)} \\leq \\epsilon $$\nThe problem asks for a stopping test based on absolute bounds, which can be stated as:\n$$ \\|b - Ax\\|_{\\infty} \\leq \\epsilon_p, \\quad \\|c - A^{\\top}y - s\\|_{\\infty} \\leq \\epsilon_d, \\quad |x^{\\top}s| \\leq \\epsilon_g $$\nwhere $\\epsilon_p$, $\\epsilon_d$, and $\\epsilon_g$ are small positive tolerances, possibly derived from a single parameter $\\epsilon$.\n\nNow, we analyze the specific LP provided to illustrate why all three components of the stopping test are necessary. The problem is defined by:\n$$ A = \\begin{pmatrix}1 & 1\\end{pmatrix}, \\quad b = 1, \\quad c = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} $$\nThe iterate provided is:\n$$ x = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{1}{2}\\end{pmatrix}, \\quad y = -100, \\quad s = \\begin{pmatrix}100 \\\\ 100\\end{pmatrix} $$\nWe are given that $x > 0$ and $s > 0$, which is true for this iterate. Let's compute the residuals and the duality gap.\n\n1.  **Primal Feasibility Residual**:\n    $$ r_p = b - Ax = 1 - \\begin{pmatrix}1 & 1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{1}{2}\\end{pmatrix} = 1 - (\\frac{1}{2} + \\frac{1}{2}) = 1 - 1 = 0 $$\n    Since $r_p = 0$, the iterate is perfectly primal-feasible. $\\|r_p\\|_{\\infty} = 0$, which is \"very small\".\n\n2.  **Dual Feasibility Residual**:\n    $$ r_d = c - A^{\\top}y - s = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}(-100) - \\begin{pmatrix}100 \\\\ 100\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}-100 \\\\ -100\\end{pmatrix} - \\begin{pmatrix}100 \\\\ 100\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} $$\n    Since $r_d = 0$, the iterate is perfectly dual-feasible. $\\|r_d\\|_{\\infty} = 0$, which is also \"very small\".\n\n3.  **Duality Gap**: The question asks to compute the duality gap $x^{\\top}s$.\n    $$ x^{\\top}s = \\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}100 \\\\ 100\\end{pmatrix} = \\frac{1}{2} \\cdot 100 + \\frac{1}{2} \\cdot 100 = 50 + 50 = 100 $$\n\n**Implications for Stopping Decisions**:\nThis example demonstrates a crucial point. The given iterate $(x, y, s)$ has zero primal and dual feasibility residuals. If the stopping criterion only checked these residuals (i.e., $\\|r_p\\|_{\\infty} \\leq \\epsilon$ and $\\|r_d\\|_{\\infty} \\leq \\epsilon$), the algorithm might terminate, especially if $\\epsilon$ is any value greater than $0$. However, the duality gap is $x^{\\top}s = 100$, which is not small. The complementary slackness products are $x_1 s_1 = 50$ and $x_2 s_2 = 50$, far from the required value of $0$ for optimality. The optimal objective value for this problem is $0$ (achieved by any feasible $x$, since $c=0$), so the current iterate with objective $c^{\\top}x = 0$ is far from optimal in terms of the duality gap.\n\nThis illustrates that an iterate can be perfectly feasible for both the primal and dual problems but still be far from an optimal solution. The \"distance\" from optimality in this case is captured by the duality gap. Therefore, a reliable stopping criterion for a PDIP method must include a condition on the duality gap (or the individual complementarity products). Relying solely on small feasibility residuals can lead to premature termination with a highly suboptimal solution.\n\nThe final task is to provide the computed value of the duality gap.\n$$ x^{\\top}s = 100 $$\nThis is an exact value.", "answer": "$$\\boxed{100}$$", "id": "3164561"}, {"introduction": "One of the most powerful features of modern interior-point methods is their remarkable robustness and ability to start from nearly anywhere. This capstone practice guides you through the implementation of a full-fledged infeasible-start predictor-corrector IPM, a workhorse algorithm used in many commercial solvers. By testing your implementation against a variety of initial points—some feasible, others wildly infeasible—you will witness firsthand the algorithm's powerful self-correcting nature and its reliable convergence to the optimal solution, illustrating why IPMs are so effective in practice [@problem_id:3164599].", "problem": "Consider the linear program in standard form: minimize the linear objective $c^{\\top} x$ subject to equality constraints $A x = b$ and nonnegativity constraints $x \\ge 0$. The dual variables are $y$ for the equality constraints and the slack variables $s \\ge 0$, with the dual feasibility relation $A^{\\top} y + s = c$. The fundamental base for this problem consists of the Karush-Kuhn-Tucker (KKT) conditions for convex optimization and the barrier-based central path, which together state that at optimality $A x = b$, $A^{\\top} y + s = c$, $x \\ge 0$, $s \\ge 0$, and $x_i s_i = 0$ for all indices $i$.\n\nYour task is to derive and implement a primal-dual Interior-Point Method (IPM) starting from the KKT conditions and the barrier formulation. The method must be able to start from infeasible points with large residuals and converge robustly to an optimal solution. The method must be an infeasible-start primal-dual IPM with a predictor-corrector step and a fraction-to-the-boundary rule to ensure $x$ and $s$ remain strictly positive throughout the iterations. You must use a centering parameter selected adaptively based on the affine-scaling predictor.\n\nYou must solve the following specific instance:\n- Matrix $A \\in \\mathbb{R}^{2 \\times 3}$, vector $b \\in \\mathbb{R}^{2}$, and vector $c \\in \\mathbb{R}^{3}$ are\n$$\nA = \\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & 1 & 0 \\end{bmatrix}, \\quad\nb = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}, \\quad\nc = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.\n$$\n\nStarting from the KKT conditions and barrier central path definitions alone (no shortcut formulas), derive the Newton linearization that yields a symmetric positive definite Schur complement system for the dual step, implement a predictor-corrector primal-dual IPM, and enforce positivity using a fraction-to-the-boundary rule. Use the stopping criterion that terminates when the maximum of the Euclidean norm of the primal residual $A x - b$, the Euclidean norm of the dual residual $A^{\\top} y + s - c$, and the average complementarity $\\mu = \\frac{x^{\\top} s}{n}$ (where $n$ is the dimension of $x$) is less than a tolerance $\\varepsilon$, with $\\varepsilon = 10^{-8}$.\n\nDesign and evaluate the effect of different initializations $(x^{(0)}, y^{(0)}, s^{(0)})$ on convergence. Construct infeasible starts with large residuals to demonstrate robust convergence of the Interior-Point Method (IPM). Use the following test suite of four initializations:\n- Test case $1$ (feasible start): $x^{(0)} = [1, 1, 1]^{\\top}$, $y^{(0)} = [0, 0]^{\\top}$, $s^{(0)} = c$.\n- Test case $2$ (infeasible with large residuals): $x^{(0)} = [10.0, 0.1, 5.0]^{\\top}$, $y^{(0)} = [0, 0]^{\\top}$, $s^{(0)} = 10 \\, c$.\n- Test case $3$ (infeasible with tiny primal variables and inconsistent dual): $x^{(0)} = [10^{-6}, 2 \\cdot 10^{-6}, 3 \\cdot 10^{-6}]^{\\top}$, $y^{(0)} = [100, -200]^{\\top}$, $s^{(0)} = [5, 1, 4]^{\\top}$.\n- Test case $4$ (infeasible with extremely large primal scale and inconsistent slack): $x^{(0)} = [10^{6}, 2 \\cdot 10^{6}, 10^{6}]^{\\top}$, $y^{(0)} = [-1000, 500]^{\\top}$, $s^{(0)} = [0.5, 0.5, 0.5]^{\\top}$.\n\nFor each test case, run the IPM with a maximum of $100$ iterations and the tolerance $\\varepsilon = 10^{-8}$, and record the number of iterations required until convergence according to the stopping criterion. If convergence is not reached within $100$ iterations, record the value $100$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, where each entry is the integer number of iterations for that test case (for example, $[k_1,k_2,k_3,k_4]$).", "solution": "The user has provided a well-posed problem in the field of numerical optimization. The task is to derive and implement a primal-dual interior-point method (IPM) for a specific linear program (LP), and then test its convergence on a suite of four initial conditions. The problem is scientifically grounded, formally specified, and requires a non-trivial implementation based on first principles. The validation criteria are met, and a solution can be produced.\n\n### 1. Problem Formulation and Optimality Conditions\n\nThe standard form of a linear program (LP) is given by:\n$$\n\\begin{align*}\n\\text{minimize} \\quad & c^{\\top} x \\\\\n\\text{subject to} \\quad & A x = b \\\\\n& x \\ge 0\n\\end{align*}\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $c \\in \\mathbb{R}^{n}$, and the optimization variable is $x \\in \\mathbb{R}^{n}$.\n\nThe corresponding dual problem is:\n$$\n\\begin{align*}\n\\text{maximize} \\quad & b^{\\top} y \\\\\n\\text{subject to} \\quad & A^{\\top} y + s = c \\\\\n& s \\ge 0\n\\end{align*}\n$$\nwhere $y \\in \\mathbb{R}^{m}$ and $s \\in \\mathbb{R}^{n}$ are the dual variables.\n\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality link the primal and dual problems. A point $(x, y, s)$ is optimal if and only if it satisfies:\n1.  **Primal Feasibility**: $A x = b$\n2.  **Dual Feasibility**: $A^{\\top} y + s = c$\n3.  **Non-negativity**: $x \\ge 0, s \\ge 0$\n4.  **Complementary Slackness**: $x_i s_i = 0$ for $i = 1, \\dots, n$. This can be written as $XSe = 0$, where $X = \\text{diag}(x)$, $S = \\text{diag}(s)$, and $e$ is a vector of ones.\n\n### 2. The Central Path and Barrier Formulation\n\nInterior-point methods approach the solution by traversing the interior of the feasible region ($x > 0, s > 0$). They replace the non-negativity constraints $x \\ge 0$ with a logarithmic barrier term in the objective function, and the complementary slackness condition $XSe = 0$ with a perturbed version $XSe = \\tau e$ for a barrier parameter $\\tau > 0$.\n\nThe KKT conditions are relaxed to define the **central path**, a set of points $(x(\\tau), y(\\tau), s(\\tau))$ that satisfy:\n$$\n\\begin{align*}\nA x &= b \\\\\nA^{\\top} y + s &= c \\\\\nXSe &= \\tau e\n\\end{align*}\n$$\nAs $\\tau \\to 0$, points on the central path converge to an optimal solution of the original LP. The IPM generates a sequence of iterates that approximately follow this path towards the solution.\n\n### 3. Newton's Method for the KKT System\n\nFor a general (and possibly infeasible) iterate $(x, y, s)$ where $x > 0$ and $s > 0$, we define the residuals for the KKT system:\n-   Primal residual: $r_b = A x - b$\n-   Dual residual: $r_c = A^{\\top} y + s - c$\n\nWe seek a Newton step $(\\Delta x, \\Delta y, \\Delta s)$ that moves the current iterate closer to a point on the central path. This involves linearizing the KKT system:\n$$\n\\begin{align*}\nA(x + \\Delta x) &= b &\\implies A \\Delta x &= -r_b \\\\\nA^{\\top}(y + \\Delta y) + (s + \\Delta s) &= c &\\implies A^{\\top} \\Delta y + \\Delta s &= -r_c \\\\\n(X + \\Delta X)(S + \\Delta S)e &\\approx XSe + S\\Delta x + X\\Delta s = \\sigma \\mu e &\\implies S\\Delta x + X\\Delta s &= \\sigma \\mu e - XSe\n\\end{align*}\n$$\nHere, $\\mu = (x^{\\top}s)/n$ is the current average complementarity gap, and $\\sigma \\in [0, 1]$ is a centering parameter. This gives the following block-structured linear system for the search direction $(\\Delta x, \\Delta y, \\Delta s)$:\n$$\n\\begin{bmatrix}\nA & 0 & 0 \\\\\n0 & A^{\\top} & I \\\\\nS & 0 & X\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \\\\\n\\Delta s\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-r_b \\\\\n-r_c \\\\\n\\sigma \\mu e - XSe\n\\end{bmatrix}\n$$\n\n### 4. Solving the System via Schur Complement\n\nThis system can be solved efficiently using block elimination.\nFrom the second row: $\\Delta s = -r_c - A^{\\top} \\Delta y$.\nSubstitute $\\Delta s$ into the third row:\n$S \\Delta x + X(-r_c - A^{\\top} \\Delta y) = \\sigma \\mu e - XSe$\n$S \\Delta x - XA^{\\top} \\Delta y = \\sigma \\mu e - XSe + Xr_c$\nSince $x, s > 0$, $S$ is invertible. Let $D^2 = S^{-1}X$, a diagonal matrix with positive entries $x_i/s_i$.\n$\\Delta x = S^{-1}(\\sigma \\mu e - XSe + Xr_c + XA^{\\top} \\Delta y) = S^{-1}(\\sigma \\mu e - XSe + Xr_c) + D^2 A^{\\top} \\Delta y$.\n\nFinally, substitute $\\Delta x$ into the first row, $A \\Delta x = -r_b$:\n$A(S^{-1}(\\sigma \\mu e - XSe + Xr_c) + D^2 A^{\\top} \\Delta y) = -r_b$\n$(A D^2 A^{\\top}) \\Delta y = -r_b - A(S^{-1}(\\sigma \\mu e - XSe + Xr_c))$\n$(A D^2 A^{\\top}) \\Delta y = -r_b - A S^{-1}(\\sigma \\mu e - XSe) - A D^2 r_c$\n\nThis is the **Schur complement system** for $\\Delta y$. The matrix $M = A D^2 A^{\\top}$ is an $m \\times m$ symmetric positive definite matrix (assuming $A$ has full row rank), which can be solved efficiently. Once $\\Delta y$ is found, $\\Delta s$ and $\\Delta x$ are recovered by back-substitution:\n1.  Solve $(A D^2 A^{\\top}) \\Delta y = -r_b - A S^{-1}(\\sigma \\mu e - XSe) - A D^2 r_c$ for $\\Delta y$.\n2.  Compute $\\Delta s = -r_c - A^{\\top} \\Delta y$.\n3.  Compute $\\Delta x = S^{-1}(\\sigma \\mu e - XSe - X \\Delta s)$.\n\n### 5. Predictor-Corrector Method\n\nMehrotra's predictor-corrector method improves convergence by using a two-stage approach at each iteration.\n\n**Predictor Step**: First, we compute an \"affine-scaling\" direction by setting $\\sigma=0$. This direction aims directly at the optimal solution without trying to stay centered.\n$$\n\\begin{bmatrix}\nA & 0 & 0 \\\\\n0 & A^\\top & I \\\\\nS & 0 & X\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta x^{aff} \\\\ \\Delta y^{aff} \\\\ \\Delta s^{aff}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-r_b \\\\\n-r_c \\\\\n-X S e\n\\end{bmatrix}\n$$\n\n**Corrector and Centering Step**:\n1.  **Adaptive Centering**: The affine step is used to estimate how much the complementarity gap would be reduced. Based on this, an adaptive centering parameter $\\sigma$ is chosen.\n    -   Calculate a step length $\\alpha_{aff}$ to the boundary along the affine direction.\n    -   Estimate the complementarity gap at this projected point: $\\mu_{aff} = (x + \\alpha_{aff} \\Delta x^{aff})^{\\top} (s + \\alpha_{aff} \\Delta s^{aff}) / n$.\n    -   Set $\\sigma = (\\mu_{aff} / \\mu)^3$. This popular heuristic reduces centering efforts when good progress is made.\n\n2.  **Corrector Term**: The linearization of the complementarity condition $XSe=\\sigma\\mu e$ ignored the second-order term $\\Delta X \\Delta S e$. The corrector step adds this term back, using the affine step as an estimate: $\\Delta X^{aff} \\Delta S^{aff} e$. The right-hand side of the complementarity equation becomes $\\sigma \\mu e - XSe - \\Delta X^{aff} \\Delta S^{aff} e$.\n\n3.  **Final Direction**: A second linear system is solved for the final search direction $(\\Delta x, \\Delta y, \\Delta s)$ using the updated right-hand side.\n$$\n\\begin{bmatrix}\nA & 0 & 0 \\\\\n0 & A^\\top & I \\\\\nS & 0 & X\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \\\\\n\\Delta s\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-r_b \\\\\n-r_c \\\\\n\\sigma \\mu e - X S e - \\Delta X^{aff} \\Delta S^{aff} e\n\\end{bmatrix}\n$$\n\n### 6. Step Length Calculation and Update\n\nTo maintain strict positivity of $x$ and $s$, we cannot take a full Newton step. The fraction-to-the-boundary rule is used to determine separate primal and dual step lengths, $\\alpha_p$ and $\\alpha_d$.\n1.  Compute the maximum step that can be taken before a component of $x$ or $s$ becomes zero:\n    $$\n    \\alpha_{p, \\text{max}} = \\min_{i: \\Delta x_i < 0} \\left( \\frac{-x_i}{\\Delta x_i} \\right) \\quad \\text{and} \\quad \\alpha_{d, \\text{max}} = \\min_{i: \\Delta s_i < 0} \\left( \\frac{-s_i}{\\Delta s_i} \\right)\n    $$\n2.  Choose the actual step lengths as a large fraction $\\eta < 1$ (e.g., $\\eta = 0.995$) of the maximum possible step, capped at $1.0$:\n    $$\n    \\alpha_p = \\min(1.0, \\eta \\alpha_{p, \\text{max}}) \\quad \\text{and} \\quad \\alpha_d = \\min(1.0, \\eta \\alpha_{d, \\text{max}})\n    $$\n3.  Update the variables:\n    $$\n    x^{(k+1)} = x^{(k)} + \\alpha_p \\Delta x \\quad , \\quad y^{(k+1)} = y^{(k)} + \\alpha_d \\Delta y \\quad , \\quad s^{(k+1)} = s^{(k)} + \\alpha_d \\Delta s\n    $$\n\nThe algorithm iterates until the primal residual norm $\\|r_b\\|_2$, the dual residual norm $\\|r_c\\|_2$, and the average complementarity gap $\\mu$ are all below a specified tolerance $\\varepsilon$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_ipm(A, b, c, x0, y0, s0, tol=1e-8, max_iter=100, eta=0.995):\n    \"\"\"\n    Implements a primal-dual predictor-corrector interior-point method\n    for a linear program in standard form: min c^T x, s.t. Ax = b, x >= 0.\n\n    Args:\n        A (np.ndarray): Constraint matrix.\n        b (np.ndarray): Constraint vector.\n        c (np.ndarray): Objective function vector.\n        x0, y0, s0 (np.ndarray): Initial primal, dual, and slack variables.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n        eta (float): Fraction-to-the-boundary parameter.\n\n    Returns:\n        int: Number of iterations required for convergence. Returns max_iter if\n             the method does not converge within the iteration limit.\n    \"\"\"\n    m, n = A.shape\n\n    # Use float64 for better numerical precision.\n    x = np.copy(x0).astype(np.float64)\n    y = np.copy(y0).astype(np.float64)\n    s = np.copy(s0).astype(np.float64)\n\n    # Iteration counter starts at 0 for number of updates.\n    for k in range(max_iter):\n        # --- Evaluate residuals and duality gap ---\n        r_b = A @ x - b\n        r_c = A.T @ y + s - c\n        mu = (x.T @ s) / n\n        \n        # Ensure mu is not negative from floating point issues near boundary\n        if mu  0: mu = 0\n\n        # --- Check for convergence ---\n        norm_rb = np.linalg.norm(r_b)\n        norm_rc = np.linalg.norm(r_c)\n        \n        if max(norm_rb, norm_rc, mu)  tol:\n            return k \n\n        # --- Predictor Step (Affine-Scaling) ---\n        # The RHS of the complementarity equation is -XSe\n        rhs_xs_aff = -x * s\n        \n        # Build and solve the Schur complement system for dy_aff\n        # (A * D^2 * A^T) * dy = -r_b - A * (S^-1 * rhs_xs + D^2 * r_c)\n        # where D^2 = X * S^-1\n        D_squared = x / s\n        M = A @ (D_squared[:, np.newaxis] * A.T)\n        \n        # Using a textbook-equivalent but potentially more stable formulation of the RHS\n        # rhs_y = -(r_b + A @ S^-1 @ (r_xs_aff - X @ r_c))\n        rhs_y_aff = -(r_b + A @ (rhs_xs_aff/s - x * r_c / s))\n\n        try:\n            dy_aff = np.linalg.solve(M, rhs_y_aff)\n        except np.linalg.LinAlgError:\n            return max_iter\n\n        # Back-substitute to find ds_aff and dx_aff\n        ds_aff = -r_c - A.T @ dy_aff\n        dx_aff = (rhs_xs_aff - x * ds_aff) / s\n\n        # --- Calculate adaptive centering parameter sigma ---\n        # Calculate max step alpha to boundary along the affine direction\n        alpha_p_aff = 1.0\n        indices_p = dx_aff  0\n        if np.any(indices_p):\n            alpha_p_aff = min(1.0, np.min(-x[indices_p] / dx_aff[indices_p]))\n\n        alpha_d_aff = 1.0\n        indices_d = ds_aff  0\n        if np.any(indices_d):\n            alpha_d_aff = min(1.0, np.min(-s[indices_d] / ds_aff[indices_d]))\n\n        # Duality gap at the affine-scaled point\n        mu_aff = ((x + alpha_p_aff * dx_aff).T @ (s + alpha_d_aff * ds_aff)) / n\n        if mu_aff  0: mu_aff = 0 # handle floating point issue\n        sigma = (mu_aff / mu)**3 if mu > 1e-12 else 0.0\n\n        # --- Corrector and Centering Step ---\n        # The RHS of complementarity now includes corrector and centering terms\n        rhs_xs_corr_center = sigma * mu - dx_aff * ds_aff\n        \n        # Solve the Schur complement system for the combined direction dy\n        rhs_y = -(r_b + A @ ((rhs_xs_aff + rhs_xs_corr_center) / s - x * r_c / s))\n        \n        try:\n            dy = np.linalg.solve(M, rhs_y)\n        except np.linalg.LinAlgError:\n            return max_iter\n\n        # Back-substitute for the final directions ds and dx\n        ds = -r_c - A.T @ dy\n        dx = (rhs_xs_aff + rhs_xs_corr_center - x * ds) / s\n\n        # --- Step length calculation (fraction-to-the-boundary) ---\n        alpha_p_max = 1.0\n        indices_p = dx  0\n        if np.any(indices_p):\n            alpha_p_max = np.min(-x[indices_p] / dx[indices_p])\n        \n        alpha_d_max = 1.0\n        indices_d = ds  0\n        if np.any(indices_d):\n            alpha_d_max = np.min(-s[indices_d] / ds[indices_d])\n        \n        alpha_p = min(1.0, eta * alpha_p_max)\n        alpha_d = min(1.0, eta * alpha_d_max)\n\n        # --- Update variables ---\n        x += alpha_p * dx\n        y += alpha_d * dy\n        s += alpha_d * ds\n    \n    # If max iterations reached without convergence\n    return max_iter\n\n\ndef solve():\n    # Define the problem instance\n    A = np.array([[1, 1, 1], [2, 1, 0]], dtype=np.float64)\n    b = np.array([3, 3], dtype=np.float64)\n    c = np.array([1, 2, 3], dtype=np.float64)\n    \n    # Define the test cases from the problem statement\n    test_cases = [\n        # Test case 1 (feasible start)\n        (np.array([1.0, 1.0, 1.0]), np.array([0.0, 0.0]), np.copy(c)),\n        # Test case 2 (infeasible with large residuals)\n        (np.array([10.0, 0.1, 5.0]), np.array([0.0, 0.0]), 10.0 * c),\n        # Test case 3 (infeasible with tiny primal variables and inconsistent dual)\n        (np.array([1e-6, 2e-6, 3e-6]), np.array([100.0, -200.0]), np.array([5.0, 1.0, 4.0])),\n        # Test case 4 (infeasible with extremely large primal scale and inconsistent slack)\n        (np.array([1e6, 2e6, 1e6]), np.array([-1000.0, 500.0]), np.array([0.5, 0.5, 0.5]))\n    ]\n\n    results = []\n    for x0, y0, s0 in test_cases:\n        # Main logic to calculate the result for one case\n        iters = solve_ipm(A, b, c, x0, y0, s0, tol=1e-8, max_iter=100)\n        results.append(iters)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3164599"}]}