{"hands_on_practices": [{"introduction": "Interior-point methods rely on barrier functions to transform constrained problems into a sequence of unconstrained ones. The logarithmic barrier is the canonical example of a self-concordant function, forming the bedrock of these algorithms. This first practice is a fundamental exercise that walks you through the mechanics of computing the Newton search direction for a barrier-augmented objective function, a core step in these powerful optimizers [@problem_id:3176672].", "problem": "Consider the barrier-augmented objective in linear optimization defined by $g(x) = c^{\\top} x + \\mu f(x)$, where $x \\in \\mathbb{R}^{n}_{++}$ is strictly positive componentwise, $c \\in \\mathbb{R}^{n}$ is a given cost vector, $\\mu  0$ is a barrier parameter, and $f(x) = -\\sum_{i=1}^{n} \\ln(x_{i})$ is the logarithmic barrier for the nonnegativity constraints. Starting from the foundational definitions of gradient and Hessian and the Newton method for twice continuously differentiable functions, derive the Newton direction $d$ for minimizing $g(x)$ at a current iterate $x$. Then rewrite the resulting linear system for $d$ entirely in terms of slack variables $s_{i} := x_{i}$, $i = 1,\\dots,n$, and solve this system in closed form to obtain $d$ as an explicit expression involving $s$ and $c$.\n\nFinally, apply your derivation to the numerical instance with $n=3$, $c = (2,-1,3)^{\\top}$, $\\mu = 2$, and current iterate $x = (1,2,4)^{\\top}$, and compute the exact Newton direction vector $d$. No rounding is required. Provide your final numerical answer as a single row matrix.", "solution": "The problem asks for the derivation of the Newton direction for minimizing a barrier-augmented objective function in linear optimization, and its subsequent application to a specific numerical instance.\n\nThe objective function to be minimized is given by\n$$g(x) = c^{\\top} x + \\mu f(x)$$\nwhere $x \\in \\mathbb{R}^{n}_{++}$, $c \\in \\mathbb{R}^{n}$, $\\mu  0$, and the logarithmic barrier function is $f(x) = -\\sum_{i=1}^{n} \\ln(x_{i})$. The domain $\\mathbb{R}^{n}_{++}$ consists of all vectors in $\\mathbb{R}^{n}$ with strictly positive components, i.e., $x_{i}  0$ for all $i=1, \\dots, n$.\n\nThe Newton method for minimizing a twice continuously differentiable function $g(x)$ computes a search direction $d$ at a current iterate $x$ by solving the linear system:\n$$\\nabla^2 g(x) d = -\\nabla g(x)$$\nwhere $\\nabla g(x)$ is the gradient of $g(x)$ and $\\nabla^2 g(x)$ is its Hessian matrix.\n\nFirst, we derive the gradient of $g(x)$. The function can be written explicitly in terms of the components of $x$ as:\n$$g(x) = \\sum_{i=1}^{n} c_i x_i - \\mu \\sum_{i=1}^{n} \\ln(x_i)$$\nThe partial derivative with respect to the $j$-th component, $x_j$, is:\n$$\\frac{\\partial g}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n} c_i x_i - \\mu \\sum_{i=1}^{n} \\ln(x_i) \\right) = c_j - \\frac{\\mu}{x_j}$$\nThe gradient is the vector of these partial derivatives:\n$$\\nabla g(x) = \\begin{pmatrix} c_1 - \\mu/x_1 \\\\ c_2 - \\mu/x_2 \\\\ \\vdots \\\\ c_n - \\mu/x_n \\end{pmatrix} = c - \\mu \\begin{pmatrix} 1/x_1 \\\\ 1/x_2 \\\\ \\vdots \\\\ 1/x_n \\end{pmatrix}$$\n\nNext, we derive the Hessian of $g(x)$. The Hessian is the matrix of second-order partial derivatives, with entries $(\\nabla^2 g(x))_{jk} = \\frac{\\partial^2 g}{\\partial x_j \\partial x_k}$.\nFor $j \\neq k$:\n$$\\frac{\\partial^2 g}{\\partial x_j \\partial x_k} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\partial g}{\\partial x_k} \\right) = \\frac{\\partial}{\\partial x_j} \\left( c_k - \\frac{\\mu}{x_k} \\right) = 0$$\nFor $j = k$:\n$$\\frac{\\partial^2 g}{\\partial x_j^2} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\partial g}{\\partial x_j} \\right) = \\frac{\\partial}{\\partial x_j} \\left( c_j - \\frac{\\mu}{x_j} \\right) = - \\mu \\left( -\\frac{1}{x_j^2} \\right) = \\frac{\\mu}{x_j^2}$$\nThus, the Hessian is a diagonal matrix:\n$$\\nabla^2 g(x) = \\begin{pmatrix} \\mu/x_1^2  0  \\cdots  0 \\\\ 0  \\mu/x_2^2  \\cdots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 0  0  \\cdots  \\mu/x_n^2 \\end{pmatrix} = \\mu \\cdot \\text{diag}\\left(\\frac{1}{x_1^2}, \\dots, \\frac{1}{x_n^2}\\right)$$\nWe can represent this compactly as $\\nabla^2 g(x) = \\mu X^{-2}$, where $X = \\text{diag}(x_1, \\dots, x_n)$.\n\nNow, we formulate the Newton system $\\nabla^2 g(x) d = -\\nabla g(x)$:\n$$\\mu \\cdot \\text{diag}\\left(\\frac{1}{x_1^2}, \\dots, \\frac{1}{x_n^2}\\right) d = - \\left( c - \\mu \\begin{pmatrix} 1/x_1 \\\\ \\vdots \\\\ 1/x_n \\end{pmatrix} \\right) = \\mu \\begin{pmatrix} 1/x_1 \\\\ \\vdots \\\\ 1/x_n \\end{pmatrix} - c$$\nThe problem specifies to rewrite this system in terms of slack variables $s_i := x_i$. This is a direct substitution. Let $s$ be the vector with components $s_i$.\n$$\\mu \\cdot \\text{diag}\\left(\\frac{1}{s_1^2}, \\dots, \\frac{1}{s_n^2}\\right) d = \\mu \\begin{pmatrix} 1/s_1 \\\\ \\vdots \\\\ 1/s_n \\end{pmatrix} - c$$\n\nTo solve for the Newton direction $d$, we can solve the system component by component. For each $i \\in \\{1, \\dots, n\\}$, the $i$-th equation is:\n$$\\frac{\\mu}{s_i^2} d_i = \\frac{\\mu}{s_i} - c_i$$\nSolving for $d_i$:\n$$d_i = \\frac{s_i^2}{\\mu} \\left( \\frac{\\mu}{s_i} - c_i \\right) = s_i - \\frac{s_i^2}{\\mu} c_i$$\nThis is the closed-form expression for each component of the Newton direction $d$ in terms of $s_i$, $c_i$, and $\\mu$. In vector form, letting $S = \\text{diag}(s_1, \\dots, s_n)$, this is $d = s - \\frac{1}{\\mu} S^2 c$.\n\nFinally, we apply this result to the given numerical instance:\n$n=3$, $c = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$, $\\mu = 2$, and the current iterate is $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}$.\nFollowing the problem's notation, we have $s = x$, so $s_1 = 1$, $s_2 = 2$, and $s_3 = 4$.\n\nWe compute the components of the Newton direction $d = \\begin{pmatrix} d_1 \\\\ d_2 \\\\ d_3 \\end{pmatrix}$:\nFor $i=1$:\n$$d_1 = s_1 - \\frac{s_1^2}{\\mu} c_1 = 1 - \\frac{1^2}{2} \\cdot 2 = 1 - \\frac{1}{2} \\cdot 2 = 1 - 1 = 0$$\nFor $i=2$:\n$$d_2 = s_2 - \\frac{s_2^2}{\\mu} c_2 = 2 - \\frac{2^2}{2} \\cdot (-1) = 2 - \\frac{4}{2} \\cdot (-1) = 2 - 2 \\cdot (-1) = 2 + 2 = 4$$\nFor $i=3$:\n$$d_3 = s_3 - \\frac{s_3^2}{\\mu} c_3 = 4 - \\frac{4^2}{2} \\cdot 3 = 4 - \\frac{16}{2} \\cdot 3 = 4 - 8 \\cdot 3 = 4 - 24 = -20$$\n\nThe resulting Newton direction vector is $d = \\begin{pmatrix} 0 \\\\ 4 \\\\ -20 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  4  -20\n\\end{pmatrix}\n}\n$$", "id": "3176672"}, {"introduction": "A key advantage of self-concordance is that it makes the performance of Newton's method predictable, guaranteeing a minimum amount of progress at each step. This practice brings theory to life through code by having you derive this guaranteed progress and then empirically verify it. You will witness the reliable performance on a self-concordant function and contrast it with the behavior on a function that lacks this property [@problem_id:3176724].", "problem": "Consider the following comparison of Newton’s method applied to two convex functions, one that is self-concordant and one that is not. Let $x \\in \\mathbb{R}^n$ and define the Newton step direction by $p(x) = -\\nabla^2 f(x)^{-1}\\nabla f(x)$ and the Newton decrement by $\\lambda(x) = \\sqrt{\\nabla f(x)^\\top \\nabla^2 f(x)^{-1}\\nabla f(x)}$. The actual one-step decrease is $\\Delta f = f(x) - f(x^+)$. In the self-concordant analysis, there exists a canonical prediction function, depending only on $\\lambda(x)$, that lower-bounds the one-step decrease when using an appropriate step size. Your task is to derive the appropriate prediction function and then implement a program to empirically compare the actual one-step decrease to this prediction on the two functions below.\n\nFundamental base for derivation:\n- Newton’s method is defined by the direction $p(x)$, the gradient $\\nabla f(x)$, and the Hessian $\\nabla^2 f(x)$.\n- The Newton decrement $\\lambda(x)$ is a scalar defined as above and measures proximity to optimality in second-order methods.\n- For self-concordant functions, the analysis of damped Newton steps provides a lower bound on the function value decrease in terms of a function of $\\lambda(x)$, independent of the particular function instance.\n\nFunctions to compare:\n1. Self-concordant barrier function on the positive orthant:\n   $$f_{\\text{bar}}(x) = -\\sum_{i=1}^n \\log x_i \\quad \\text{with domain } x_i  0.$$\n2. Smooth convex function that is not self-concordant:\n   $$f_{\\text{sqrt}}(x) = \\sum_{i=1}^n \\sqrt{1 + x_i^2}.$$\n\nStep-size policies to be used:\n- Full Newton step: $x^+ = x + p(x)$.\n- Damped Newton step governed by $\\lambda(x)$: $x^+ = x + \\alpha(x)\\,p(x)$ with $\\alpha(x)$ depending only on $\\lambda(x)$.\n\nTasks:\n1. Derive the canonical prediction function of $\\lambda(x)$ that lower-bounds the one-step decrease for self-concordant functions under the corresponding damped Newton step.\n2. Implement a program that, for each test case below, computes:\n   - the Newton direction $p(x)$,\n   - the Newton decrement $\\lambda(x)$,\n   - the step size $\\alpha(x)$ determined by the chosen policy,\n   - the new point $x^+$,\n   - the actual decrease $\\Delta f = f(x) - f(x^+)$,\n   - the predicted lower bound expressed solely via the derived prediction function,\n   - the ratio $r = \\Delta f$ divided by the predicted lower bound.\n3. Use the ratio $r$ to measure predictability. A value $r \\ge 1$ indicates that the actual decrease meets or exceeds the prediction, while $r  1$ indicates a violation of the prediction.\n\nTest suite:\n- Case A: $f_{\\text{bar}}$, $n=2$, $x = [0.8,\\,1.2]$, damped step.\n- Case B: $f_{\\text{bar}}$, $n=3$, $x = [0.2,\\,0.5,\\,2.0]$, damped step.\n- Case C: $f_{\\text{sqrt}}$, $n=2$, $x = [2.0,\\,-1.0]$, damped step.\n- Case D: $f_{\\text{sqrt}}$, $n=2$, $x = [3.0,\\,3.0]$, full step.\n\nOutput specification:\n- Your program should produce a single line of output containing the ratios $r$ for the four cases, in the order A, B, C, D, as a comma-separated list enclosed in square brackets (for example, $[r_A,r_B,r_C,r_D]$).\n- Each $r$ must be output as a floating-point number. No physical units apply.\n\nScientific realism:\n- Ensure all intermediate computations are mathematically consistent and respect the function domains (for $f_{\\text{bar}}$, maintain $x_i  0$ throughout).", "solution": "The problem requires the derivation of a canonical prediction function for the one-step decrease of Newton's method on a self-concordant function, and a subsequent empirical comparison with a non-self-concordant function.\n\n### Part 1: Derivation of the Prediction Function\n\nThe analysis of Newton's method for self-concordant functions provides a guaranteed lower bound on the decrease in the function value, which depends only on the Newton decrement $\\lambda(x)$, not on the specific function or the point $x$.\n\nA function $f$ is defined as self-concordant if it is convex, $C^3$, and its third derivative is bounded by its second derivative. Formally, for all $x$ in the domain of $f$ and for all directions $h \\in \\mathbb{R}^n$, the inequality $|D^3 f(x)[h,h,h]| \\le 2 (D^2 f(x)[h,h])^{3/2}$ holds. This property leads to strong bounds on the function's behavior relative to its local quadratic approximation.\n\nFor a self-concordant function $f$, a key inequality that bounds the function value at a nearby point $y$ using information at $x$ is:\n$$f(y) \\le f(x) + \\nabla f(x)^\\top (y-x) - \\left( \\|y-x\\|_x + \\log(1 - \\|y-x\\|_x) \\right)$$\nwhere $\\|u\\|_x = \\sqrt{u^\\top \\nabla^2 f(x) u}$ is the local norm at $x$. This inequality holds for all $y$ such that $\\|y-x\\|_x  1$.\n\nWe analyze the change in $f$ along the Newton direction, $p(x) = -\\nabla^2 f(x)^{-1}\\nabla f(x)$. Let the new point be $x^+ = x + \\alpha p(x)$, where $\\alpha \\in [0, 1)$ is the step size. The vector from $x$ to $x^+$ is $\\alpha p(x)$. Its length in the local norm is:\n$$ \\|\\alpha p(x)\\|_x = \\alpha \\sqrt{p(x)^\\top \\nabla^2 f(x) p(x)} $$\nBy substituting the definition of $p(x)$, we find:\n$$ p(x)^\\top \\nabla^2 f(x) p(x) = (-\\nabla^2 f(x)^{-1}\\nabla f(x))^\\top \\nabla^2 f(x) (-\\nabla^2 f(x)^{-1}\\nabla f(x)) = \\nabla f(x)^\\top \\nabla^2 f(x)^{-1} \\nabla f(x) = \\lambda(x)^2 $$\nThus, $\\|\\alpha p(x)\\|_x = \\alpha \\lambda(x)$. The self-concordance inequality is applicable if $\\alpha \\lambda(x)  1$.\n\nSubstituting $y-x = \\alpha p(x)$ into the inequality gives:\n$$ f(x+\\alpha p(x)) \\le f(x) + \\alpha \\nabla f(x)^\\top p(x) - \\left( \\alpha\\lambda(x) + \\log(1 - \\alpha\\lambda(x)) \\right) $$\nWe also know that $\\nabla f(x)^\\top p(x) = -\\lambda(x)^2$. Substituting this gives an upper bound on $f(x^+)$:\n$$ f(x+\\alpha p(x)) \\le f(x) - \\alpha\\lambda(x)^2 - \\alpha\\lambda(x) - \\log(1 - \\alpha\\lambda(x)) $$\nThis bound holds for any step size $\\alpha$ such that $\\alpha  1/\\lambda(x)$. The goal of the damped Newton method is to choose $\\alpha$ to achieve the maximum guaranteed decrease, which corresponds to minimizing this upper bound on $f(x^+)$. Let $g(\\alpha)$ be the term that depends on $\\alpha$:\n$$ g(\\alpha) = -\\alpha\\lambda(x)^2 - \\alpha\\lambda(x) - \\log(1 - \\alpha\\lambda(x)) $$\nTo find the optimal $\\alpha$, we differentiate $g(\\alpha)$ with respect to $\\alpha$ and set the derivative to zero:\n$$ g'(\\alpha) = -\\lambda(x)^2 - \\lambda(x) - \\frac{-\\lambda(x)}{1 - \\alpha\\lambda(x)} = -\\lambda(x)^2 - \\lambda(x) + \\frac{\\lambda(x)}{1 - \\alpha\\lambda(x)} = 0 $$\nAssuming $\\lambda(x)  0$, we can divide by $\\lambda(x)$:\n$$ -\\lambda(x) - 1 + \\frac{1}{1 - \\alpha\\lambda(x)} = 0 \\implies \\lambda(x) + 1 = \\frac{1}{1 - \\alpha\\lambda(x)} $$\n$$ 1 - \\alpha\\lambda(x) = \\frac{1}{1+\\lambda(x)} \\implies \\alpha\\lambda(x) = 1 - \\frac{1}{1+\\lambda(x)} = \\frac{\\lambda(x)}{1+\\lambda(x)} $$\nThis yields the optimal step size for the damped Newton method:\n$$ \\alpha(x) = \\frac{1}{1+\\lambda(x)} $$\nThis step size always satisfies the condition $\\alpha(x) \\lambda(x)  1$.\n\nNow, we substitute this optimal step size back into the inequality for $f(x^+)$ to find the guaranteed decrease:\n$$ f(x^+) \\le f(x) - \\left(\\frac{1}{1+\\lambda(x)}\\right)\\lambda(x)^2 - \\left(\\frac{1}{1+\\lambda(x)}\\right)\\lambda(x) - \\log\\left(1 - \\frac{\\lambda(x)}{1+\\lambda(x)}\\right) $$\n$$ f(x^+) \\le f(x) - \\frac{\\lambda(x)^2+\\lambda(x)}{1+\\lambda(x)} - \\log\\left(\\frac{1}{1+\\lambda(x)}\\right) $$\n$$ f(x^+) \\le f(x) - \\lambda(x) + \\log(1+\\lambda(x)) $$\nRearranging this gives the lower bound on the one-step decrease $\\Delta f = f(x) - f(x^+)$:\n$$ \\Delta f \\ge \\lambda(x) - \\log(1+\\lambda(x)) $$\nThis completes the derivation. The canonical prediction function for the lower bound on the decrease is $\\psi(\\lambda) = \\lambda - \\log(1+\\lambda)$, and it is valid when using the damped step size $\\alpha(\\lambda) = 1/(1+\\lambda)$.\n\n### Part 2: Analysis of Test Functions\n\nWe will apply this analysis to the two given functions.\n\n1.  **Self-concordant barrier function: $f_{\\text{bar}}(x) = -\\sum_{i=1}^n \\log x_i$**\n    - Gradient: $\\nabla f_{\\text{bar}}(x)_i = -1/x_i$.\n    - Hessian: $\\nabla^2 f_{\\text{bar}}(x)$ is a diagonal matrix with $(\\nabla^2 f_{\\text{bar}}(x))_{ii} = 1/x_i^2$.\n    - Inverse Hessian: $(\\nabla^2 f_{\\text{bar}}(x)^{-1})_{ii} = x_i^2$.\n    - Newton step: $p(x) = -(\\nabla^2 f_{\\text{bar}})^{-1} \\nabla f_{\\text{bar}} = -\\text{diag}(x_i^2) \\cdot [-1/x_i] = [x_i]$, so $p(x)=x$.\n    - Newton decrement: $\\lambda(x)^2 = \\nabla f_{\\text{bar}}^\\top (\\nabla^2 f_{\\text{bar}})^{-1} \\nabla f_{\\text{bar}} = \\sum_{i=1}^n (-1/x_i)^2 (x_i^2) = \\sum_{i=1}^n 1 = n$.\n    - Therefore, for this function, $\\lambda(x) = \\sqrt{n}$, a constant. For Cases A ($n=2$) and B ($n=3$), $\\lambda$ will be $\\sqrt{2}$ and $\\sqrt{3}$, respectively. The theory predicts that the ratio $r = \\Delta f / (\\lambda - \\log(1+\\lambda))$ will be greater than or equal to $1$.\n\n2.  **Non-self-concordant function: $f_{\\text{sqrt}}(x) = \\sum_{i=1}^n \\sqrt{1 + x_i^2}$**\n    - Gradient: $\\nabla f_{\\text{sqrt}}(x)_i = x_i / \\sqrt{1 + x_i^2}$.\n    - Hessian: $\\nabla^2 f_{\\text{sqrt}}(x)$ is a diagonal matrix with $(\\nabla^2 f_{\\text{sqrt}}(x))_{ii} = (1 + x_i^2)^{-3/2}$.\n    - Inverse Hessian: $(\\nabla^2 f_{\\text{sqrt}}(x)^{-1})_{ii} = (1 + x_i^2)^{3/2}$.\n    - Newton step: $p(x)_i = -(1+x_i^2)^{3/2} \\cdot (x_i/\\sqrt{1+x_i^2}) = -x_i(1+x_i^2)$.\n    - Newton decrement: $\\lambda(x)^2 = \\sum_{i=1}^n (x_i/\\sqrt{1+x_i^2})^2 (1+x_i^2)^{3/2} = \\sum_{i=1}^n x_i^2\\sqrt{1+x_i^2}$.\n    - Since this function is not self-concordant, the derived prediction is not guaranteed to hold. We expect to find cases where the ratio $r  1$. Case C uses the damped step, which may or may not satisfy the bound. Case D uses a full Newton step ($\\alpha=1$), which is known to be potentially unstable for non-self-concordant functions far from the optimum.\n\n### Part 3: Computational Procedure\n\nFor each test case, the following quantities are computed in sequence:\n1.  The vector $x$ and the choice of function ($f_{\\text{bar}}$ or $f_{\\text{sqrt}}$) and step policy are taken from the test suite.\n2.  The gradient $\\nabla f(x)$ and Hessian $\\nabla^2 f(x)$ are computed based on the analytical formulas.\n3.  The Newton step is calculated by solving the system $\\nabla^2 f(x) p(x) = -\\nabla f(x)$. For the given functions, this is simplified due to the diagonal Hessian: $p(x) = -(\\nabla^2 f(x))^{-1} \\nabla f(x)$.\n4.  The Newton decrement is calculated as $\\lambda(x) = \\sqrt{-\\nabla f(x)^\\top p(x)}$.\n5.  The step size $\\alpha(x)$ is determined: $\\alpha(x) = 1/(1+\\lambda(x))$ for a 'damped' step, and $\\alpha(x)=1$ for a 'full' step.\n6.  The next iterate is computed: $x^+ = x + \\alpha(x) p(x)$.\n7.  The actual decrease in function value is computed: $\\Delta f = f(x) - f(x^+)$.\n8.  The predicted lower bound is computed using the derived formula: $\\Delta f_{\\text{pred}} = \\lambda(x) - \\log(1+\\lambda(x))$.\n9.  The ratio $r = \\Delta f / \\Delta f_{\\text{pred}}$ is calculated to evaluate the prediction's validity.\n\nThe implementation will carry out these steps for all four specified test cases.", "answer": "```python\nimport numpy as np\n\ndef f_bar(x):\n    \"\"\"\n    Computes value, gradient, and Hessian for f(x) = -sum(log(x_i)).\n    \"\"\"\n    if np.any(x = 0):\n        # The point must be in the domain of the function.\n        raise ValueError(\"Input x must be in the positive orthant.\")\n    val = -np.sum(np.log(x))\n    grad = -1.0 / x\n    hess = np.diag(1.0 / (x**2))\n    return val, grad, hess\n\ndef f_bar_val(x):\n    \"\"\"\n    Computes only the value of f_bar.\n    \"\"\"\n    if np.any(x = 0):\n        return np.inf\n    return -np.sum(np.log(x))\n\ndef f_sqrt(x):\n    \"\"\"\n    Computes value, gradient, and Hessian for f(x) = sum(sqrt(1 + x_i^2)).\n    \"\"\"\n    term = np.sqrt(1 + x**2)\n    val = np.sum(term)\n    grad = x / term\n    hess = np.diag(1.0 / (term**3))\n    return val, grad, hess\n\ndef f_sqrt_val(x):\n    \"\"\"\n    Computes only the value of f_sqrt.\n    \"\"\"\n    term = np.sqrt(1 + x**2)\n    return np.sum(term)\n\ndef solve():\n    \"\"\"\n    Main function to run the Newton's method comparison for the given test cases.\n    \"\"\"\n    test_cases = [\n        {'func_handle': f_bar, 'val_handle': f_bar_val, 'x': np.array([0.8, 1.2]), 'policy': 'damped'},\n        {'func_handle': f_bar, 'val_handle': f_bar_val, 'x': np.array([0.2, 0.5, 2.0]), 'policy': 'damped'},\n        {'func_handle': f_sqrt, 'val_handle': f_sqrt_val, 'x': np.array([2.0, -1.0]), 'policy': 'damped'},\n        {'func_handle': f_sqrt, 'val_handle': f_sqrt_val, 'x': np.array([3.0, 3.0]), 'policy': 'full'}\n    ]\n\n    ratios = []\n\n    for case in test_cases:\n        func = case['func_handle']\n        val_func = case['val_handle']\n        x = case['x']\n        policy = case['policy']\n\n        # 1. Get function value, gradient, and Hessian at x.\n        f_x, grad_x, hess_x = func(x)\n        \n        # 2. Compute Newton step p and decrement lambda.\n        # p = -H^{-1}g. `solve` is more stable than `inv` but for diagonal H, direct inversion is fine.\n        p_x = -np.linalg.solve(hess_x, grad_x)\n        \n        # lambda^2 = -g^T p\n        lambda_sq = -grad_x.T @ p_x\n        # Prevent numerical issues with small negative numbers due to floating point arithmetic.\n        lambda_x = np.sqrt(max(0, lambda_sq))\n\n        # 3. Determine step size alpha.\n        if policy == 'damped':\n            alpha = 1.0 / (1.0 + lambda_x)\n        else:  # 'full'\n            alpha = 1.0\n\n        # 4. Compute new point x_plus.\n        x_plus = x + alpha * p_x\n        \n        # 5. Compute actual decrease delta_f.\n        f_x_plus = val_func(x_plus)\n        delta_f_actual = f_x - f_x_plus\n\n        # 6. Compute predicted lower bound on decrease.\n        # This prediction is derived for self-concordant functions with a damped step.\n        if lambda_x  1e-9: # Avoid log(1) issues, though lambda isn't zero for these cases.\n            delta_f_pred = lambda_x - np.log(1.0 + lambda_x)\n        else:\n            # For lambda - 0, lambda - log(1+lambda) approx lambda - (lambda - lambda^2/2) = lambda^2/2\n            delta_f_pred = 0.5 * lambda_x**2\n\n        # 7. Compute the ratio r.\n        # A ratio r = 1 means the prediction holds.\n        if abs(delta_f_pred)  1e-12:\n            # If predicted decrease is zero, the ratio is conceptually infinite if there's any actual decrease.\n            # This case does not occur in the problem set.\n            ratio = np.inf if delta_f_actual  1e-12 else 1.0\n        else:\n            ratio = delta_f_actual / delta_f_pred\n        \n        ratios.append(ratio)\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```", "id": "3176724"}, {"introduction": "We now combine our insights to build a practical, high-performance optimization algorithm based on a two-phase approach: a cautious \"damped\" phase for when we are far from the optimum, and a fast \"pure\" Newton phase for when we are close. In this capstone exercise, you will implement this strategy, using the Newton decrement $\\lambda(x)$ as a reliable signal to switch gears and enter the region of rapid, quadratic convergence [@problem_id:3176751].", "problem": "You are asked to implement and validate a switching rule between damped Newton steps and full Newton steps for minimizing self-concordant functions in unconstrained form. The implementation must be a complete, runnable program.\n\nThe fundamental base for this problem consists of the following definitions and well-tested facts:\n\n- A function $f:\\mathbb{R}^n \\to \\mathbb{R}$ is self-concordant (Self-Concordant (SC)) if it is three times continuously differentiable, convex on an open domain, and satisfies the self-concordance inequalities that control the third derivative of $f$ in terms of the local quadratic form induced by the Hessian; canonical examples include logarithmic barriers such as $-\\log(x)$ and $-\\log(1-x^2)$ where the domain is the interior of the corresponding feasible set.\n- Given a twice continuously differentiable convex function $f$, Newton's method at a point $x$ uses the Newton step $s(x)$ defined implicitly by the linear system $ \\nabla^2 f(x)\\, s(x) = -\\nabla f(x)$, and the Newton decrement $ \\lambda(x) = \\sqrt{\\nabla f(x)^{\\mathsf{T}} \\left(\\nabla^2 f(x)\\right)^{-1} \\nabla f(x)}$, which equals $\\sqrt{-\\nabla f(x)^{\\mathsf{T}} s(x)}$ when $s(x)$ solves the Newton system.\n\nThe switching rule to be implemented is:\n- While $ \\lambda(x)  0.5$, take the damped Newton step with step size $ t(x) = \\dfrac{1}{1 + \\lambda(x)}$.\n- As soon as $ \\lambda(x) \\le 0.5$, switch to taking full Newton steps with step size $ t(x) = 1$ thereafter.\n\nYour program must:\n1) Implement the above rule safely on the domain of $f$ by using feasibility-preserving backtracking if the proposed step $x^{+} = x + t s(x)$ leaves the domain or fails to reduce $f(x)$ strictly. The backtracking must only shrink the step size to restore feasibility and decrease, but the diagnostic must still record whether the full step was accepted at the switch without any backtracking.\n2) Monitor the following diagnostics once the condition $ \\lambda(x) \\le 0.5$ is first met (this event is called \"the switch\"):\n   - $D_1$: whether the first full Newton step at the switch was accepted without backtracking and strictly decreased $f$.\n   - $D_2$: whether the objective values were strictly decreasing at every iteration after the switch (including the first post-switch step).\n   - $D_3$: whether the observed reduction of the Newton decrement is in the quadratic regime for at least the first two post-switch iterations that occur before termination, measured by checking that $ \\lambda_{k+1} \\le 3.0 \\,\\lambda_k^2$ for those iterations.\n\nDeclare a test suite consisting of the following six cases. Each case is a pair $(f, x^{(0)})$ with the specified function and starting point. All numbers must be implemented as specified.\n\n- Case $1$: $f_1(x) = -\\log(1 - x^2)$ on the domain $\\{x \\in \\mathbb{R}: |x|  1\\}$ with $x^{(0)} = 0.8$.\n- Case $2$: $f_1(x) = -\\log(1 - x^2)$ on the domain $\\{x \\in \\mathbb{R}: |x|  1\\}$ with $x^{(0)} = 0.1$.\n- Case $3$: $f_2(x) = -\\sum_{i=1}^3 \\log(x_i) + \\dfrac{1}{2}\\|x - c\\|_2^2$ on the domain $\\{x \\in \\mathbb{R}^3: x_i  0 \\text{ for } i=1,2,3\\}$ with $c = (1.0, 2.0, 3.0)$ and $x^{(0)} = (0.2, 0.2, 0.2)$.\n- Case $4$: $f_2(x) = -\\sum_{i=1}^3 \\log(x_i) + \\dfrac{1}{2}\\|x - c\\|_2^2$ on the domain $\\{x \\in \\mathbb{R}^3: x_i  0 \\text{ for } i=1,2,3\\}$ with $c = (1.0, 2.0, 3.0)$ and $x^{(0)} = (5.0, 5.0, 5.0)$.\n- Case $5$: $f_3(x) = -\\sum_{j=1}^3 \\log\\!\\left(b_j - a_j^{\\mathsf{T}} x\\right) + \\dfrac{1}{2}\\|x\\|_2^2$ on the domain $\\{x \\in \\mathbb{R}^2: b - A x \\in \\mathbb{R}^3_{++}\\}$ with\n  $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ -1  -1 \\end{bmatrix}$, $b = (2.0, 2.0, 1.0)$ and $x^{(0)} = (0.0, 0.0)$.\n- Case $6$: $f_3(x) = -\\sum_{j=1}^3 \\log\\!\\left(b_j - a_j^{\\mathsf{T}} x\\right) + \\dfrac{1}{2}\\|x\\|_2^2$ on the domain $\\{x \\in \\mathbb{R}^2: b - A x \\in \\mathbb{R}^3_{++}\\}$ with\n  $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ -1  -1 \\end{bmatrix}$, $b = (2.0, 2.0, 1.0)$ and $x^{(0)} = (1.5, 1.5)$.\n\nAlgorithmic requirements:\n- Terminate when $ \\lambda(x) \\le 10^{-8}$ or when the iteration count reaches $50$.\n- In backtracking, if the proposed $x^{+} = x + t s(x)$ is infeasible or $f(x^{+}) \\ge f(x)$, reduce $t$ by multiplying by $0.5$ repeatedly until feasibility and strict decrease are achieved, or until $t  10^{-12}$, in which case the iteration may terminate as failed for that case.\n\nYour program must output, for each test case, a boolean indicating whether all diagnostics $D_1, D_2, D_3$ are simultaneously satisfied for that case. The final output format must be a single line containing a list of the six boolean results in order, as a comma-separated list enclosed in square brackets, for example, $[{\\tt True},{\\tt False},\\dots]$.", "solution": "The user has provided a problem from the field of convex optimization, specifically concerning the implementation and empirical validation of a standard algorithm: Newton's method for minimizing self-concordant functions. The problem specifies a precise switching rule between a damped phase and a pure Newton phase, along with backtracking line search for safety, termination criteria, and a set of diagnostics to validate theoretical properties of the algorithm.\n\n### Problem Validation\n\nFirst, the problem statement is critically evaluated according to the specified criteria.\n\n**1. Extraction of Givens:**\n- **Objective:** Implement and validate a switching rule for Newton's method on unconstrained self-concordant functions.\n- **Newton Step:** $s(x)$ is defined by the solution to the linear system $\\nabla^2 f(x) s(x) = -\\nabla f(x)$.\n- **Newton Decrement:** $\\lambda(x) = \\sqrt{-\\nabla f(x)^{\\mathsf{T}} s(x)}$.\n- **Switching Rule:** Use step size $t(x) = \\frac{1}{1 + \\lambda(x)}$ while $\\lambda(x)  0.5$. Switch to $t(x) = 1$ permanently once $\\lambda(x) \\le 0.5$.\n- **Backtracking:** If a step $x^{+} = x + t s(x)$ is infeasible or fails to decrease the function value ($f(x^{+}) \\ge f(x)$), the step size $t$ is to be repeatedly halved until both conditions are met. If $t$ falls below $10^{-12}$, the process fails.\n- **Termination Criteria:** Stop when $\\lambda(x) \\le 10^{-8}$ or after $50$ iterations.\n- **Diagnostics ($D_1, D_2, D_3$):** After the switch to full steps ($t=1$), monitor:\n    - $D_1$: The first full Newton step is accepted without any backtracking.\n    - $D_2$: The objective function values are strictly decreasing in all iterations after the switch.\n    - $D_3$: The Newton decrement exhibits quadratic convergence for at least the first two post-switch iterations, verified by checking if $\\lambda_{k+1} \\le 3.0 \\lambda_k^2$.\n- **Test Suite:** Six specific cases are defined, each with a function $f(x)$, its domain, and an initial point $x^{(0)}$.\n- **Output:** A boolean list indicating if all diagnostics ($D_1, D_2, D_3$) were met for each of the six cases.\n\n**2. Validation using Extracted Givens:**\n- **Scientific Grounding:** The problem is firmly rooted in the established theory of interior-point methods for convex optimization. Self-concordant functions, the Newton decrement, and the two-phase (damped and quadratic) convergence of Newton's method are cornerstone concepts in this field. The specific functions chosen are canonical examples of self-concordant barriers. The diagnostics are designed to test well-known theoretical properties. The problem is scientifically sound.\n- **Well-Posedness:** The algorithmic rules, termination criteria, and diagnostics are specified with mathematical precision. The inputs are clearly defined. This structure ensures that a unique and meaningful outcome can be determined for each test case.\n- **Objectivity:** The problem is described using formal mathematical language, free from any subjective or ambiguous statements.\n- **Completeness and Consistency:** All necessary information (functions, derivatives, constants, initial points, algorithmic logic) is provided. The initial points are verified to be within the domains of their respective functions. The backtracking rule is a standard safety feature that complements, rather than contradicts, the main step-size selection rule. The setup is self-contained and consistent.\n\n**3. Verdict and Action:**\nThe problem is **valid**. It is a well-defined standard exercise in numerical optimization that tests both the understanding of the underlying theory and the ability to implement the corresponding algorithm correctly. The solution process may proceed.\n\n### Solution Design\n\nThe core of the solution is a Python program that implements the specified Newton's method. For each of the six test cases, this program will:\n\n$1$. **Define the Objective and its Derivatives:** For each function $f(x)$, its gradient vector $\\nabla f(x)$ and Hessian matrix $\\nabla^2 f(x)$ must be correctly implemented.\n    - For $f_1(x) = -\\log(1 - x^2)$: $f_1'(x) = \\frac{2x}{1-x^2}$ and $f_1''(x) = \\frac{2(1+x^2)}{(1-x^2)^2}$.\n    - For $f_2(x) = -\\sum_{i=1}^3 \\log(x_i) + \\frac{1}{2}\\|x - c\\|_2^2$: $\\nabla f_2(x) = -x^{\\circ(-1)} + x - c$ and $\\nabla^2 f_2(x) = \\text{diag}(x_i^{-2}) + I$, where $x^{\\circ(-1)}$ denotes element-wise inversion and $I$ is the identity matrix.\n    - For $f_3(x) = -\\sum_{j=1}^3 \\log(b_j - a_j^{\\mathsf{T}} x) + \\frac{1}{2}\\|x\\|_2^2$: $\\nabla f_3(x) = \\sum_{j=1}^3 \\frac{a_j}{b_j - a_j^{\\mathsf{T}} x} + x$ and $\\nabla^2 f_3(x) = \\sum_{j=1}^3 \\frac{a_j a_j^{\\mathsf{T}}}{(b_j - a_j^{\\mathsf{T}} x)^2} + I$.\n\n$2$. **Implement the Main Algorithm Loop:**\nA single function, `run_newton_method`, encapsulates the iterative process.\n- **Initialization:** Set the current point $x$ to the initial point $x^{(0)}$, and initialize trackers for algorithm status and diagnostic results.\n- **Per-Iteration Steps:**\n    a. Compute the gradient $\\nabla f(x)$ and Hessian $\\nabla^2 f(x)$.\n    b. Solve the Newton system $\\nabla^2 f(x) s = -\\nabla f(x)$ for the step $s$ using `numpy.linalg.solve`.\n    c. Compute the Newton decrement $\\lambda(x) = \\sqrt{-\\nabla f(x)^{\\mathsf{T}} s}$.\n    d. Check for termination ($\\lambda(x) \\le 10^{-8}$) or maximum iterations.\n    e. Apply the switching rule to determine the initial step size $t_{initial}$: $1$ if in the pure Newton phase ($\\lambda \\le 0.5$ has been reached), or $1/(1+\\lambda)$ if in the damped phase.\n    f. Perform the backtracking line search: starting with $t = t_{initial}$, check if $x+ts$ is feasible and strictly decreases $f$. If not, set a `backtracked` flag to true and shrink $t$ by a factor of $0.5$. Repeat until a valid step is found or $t$ becomes too small.\n    g. Update the current point: $x \\leftarrow x + ts$.\n    h. Collect data for diagnostics: record if backtracking occurred on the first full step ($D_1$), check for strict function decrease after the switch ($D_2$), and store the history of $\\lambda$ values for the final evaluation of $D_3$.\n\n$3$. **Evaluate Diagnostics and Final Result:**\n- After the loop terminates for a given case, the collected data is used to evaluate the diagnostics.\n- The switch to the pure Newton phase must have occurred.\n- $D_1$ is satisfied if the first full step after the switch was taken without any backtracking.\n- $D_2$ is satisfied if the function value strictly decreased at every step post-switch (this is ensured by the line search, so this diagnostic should pass if the algorithm succeeds).\n- $D_3$ is satisfied if at least two post-switch iterations occurred and the quadratic convergence inequality, $\\lambda_{k+1} \\le 3.0 \\lambda_k^2$, holds for both of them. This requires having computed at least three $\\lambda$ values after the switch point.\n- A final boolean result for the case is true only if all three diagnostics are satisfied.\n\nThe results from all six test cases are aggregated into a single list and printed in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_newton_method(f, grad_f, hess_f, is_feasible, x0):\n    \"\"\"\n    Implements Newton's method with a switching rule for self-concordant functions.\n    The method tracks diagnostics D1, D2, and D3 as specified.\n    \"\"\"\n    MAX_ITER = 50\n    TOL_LAMBDA = 1e-8\n    TOL_T = 1e-12\n    \n    x = np.array(x0, dtype=float)\n    if x.ndim == 0:\n        x = x.reshape(1)\n\n    lambda_history = []\n    switched = False\n    switch_k = -1\n    \n    # Diagnostic flags\n    d1_ok = False # D1: First full step accepted without backtracking.\n    d2_ok = True  # D2: Objective strictly decreasing post-switch.\n    \n    final_k = -1\n\n    for k in range(MAX_ITER):\n        final_k = k\n        if not is_feasible(x):\n            return {'success': False, 'reason': 'infeasible_point'}\n        \n        f_val = f(x)\n        grad = grad_f(x)\n        hess = hess_f(x)\n        \n        try:\n            hess_mat = np.atleast_2d(hess)\n            s = np.linalg.solve(hess_mat, -grad)\n        except np.linalg.LinAlgError:\n            return {'success': False, 'reason': 'singular_hessian'}\n\n        lambda_sq = -np.dot(grad, s)\n        lambda_val = np.sqrt(max(0, lambda_sq))\n        lambda_history.append(lambda_val)\n\n        if lambda_val = TOL_LAMBDA:\n            break\n\n        # Determine step size based on switching rule\n        is_full_step_phase = switched\n        if not is_full_step_phase and lambda_val = 0.5:\n            is_full_step_phase = True\n            if not switched:\n                switched = True\n                switch_k = k\n        \n        t_initial = 1.0 if is_full_step_phase else 1.0 / (1.0 + lambda_val)\n        \n        # Backtracking line search\n        t = t_initial\n        backtracked = False\n        f_next = np.inf\n\n        while t = TOL_T:\n            x_next = x + t * s\n            step_good = False\n            if is_feasible(x_next):\n                f_next_try = f(x_next)\n                if f_next_try  f_val:\n                    f_next = f_next_try\n                    step_good = True\n            \n            if step_good:\n                break\n            \n            t *= 0.5\n            backtracked = True\n            \n        if t  TOL_T:\n            return {'success': False, 'reason': 'backtracking_failed'}\n\n        # Update x\n        x = x_next\n        \n        # Update diagnostics\n        if switched:\n            if k == switch_k:\n                # D1 check: first post-switch step must be full (t_initial=1) and not backtracked\n                if t_initial == 1.0 and not backtracked:\n                    d1_ok = True\n            \n            if k = switch_k:\n                # D2 check: f must decrease at every post-switch step\n                if f_next = f_val: # This is a safety check; line search should prevent this.\n                    d2_ok = False\n    \n    return {\n        'success': True,\n        'switched': switched,\n        'switch_k': switch_k,\n        'final_k': final_k,\n        'd1_ok': d1_ok,\n        'd2_ok': d2_ok,\n        'lambda_history': lambda_history\n    }\n\ndef solve():\n    #\n    # Define test cases from the problem statement.\n    #\n\n    # Case 1  2: f(x) = -log(1 - x^2)\n    is_feasible1 = lambda x: np.abs(x[0])  1.0\n    f1 = lambda x: -np.log(1 - x[0]**2) if is_feasible1(x) else np.inf\n    grad1 = lambda x: np.array([2 * x[0] / (1 - x[0]**2)])\n    hess1 = lambda x: (2 * (1 + x[0]**2)) / (1 - x[0]**2)**2\n\n    # Case 3  4: f(x) = -sum(log(x_i)) + 0.5 * ||x - c||^2\n    c = np.array([1.0, 2.0, 3.0])\n    is_feasible2 = lambda x: np.all(x  1e-12)\n    def f2(x):\n        if not is_feasible2(x): return np.inf\n        return -np.sum(np.log(x)) + 0.5 * np.sum((x - c)**2)\n    grad2 = lambda x: -1.0/x + (x - c)\n    hess2 = lambda x: np.diag(1.0/x**2 + 1.0)\n    \n    # Case 5  6: f(x) = -sum(log(b_j - a_j^T x)) + 0.5 * ||x||^2\n    A = np.array([[1.0, 0.0], [0.0, 1.0], [-1.0, -1.0]])\n    b = np.array([2.0, 2.0, 1.0])\n    is_feasible3 = lambda x: np.all(b - A @ x  1e-12)\n    def f3(x):\n        if not is_feasible3(x): return np.inf\n        return -np.sum(np.log(b - A @ x)) + 0.5 * (x @ x)\n    def grad3(x):\n        return A.T @ (1.0 / (b - A @ x)) + x\n    def hess3(x):\n        d_vec = 1.0 / (b - A @ x)**2\n        return A.T @ np.diag(d_vec) @ A + np.eye(2)\n\n    test_cases = [\n        {'f': f1, 'grad_f': grad1, 'hess_f': hess1, 'is_feasible': is_feasible1, 'x0': [0.8]},\n        {'f': f1, 'grad_f': grad1, 'hess_f': hess1, 'is_feasible': is_feasible1, 'x0': [0.1]},\n        {'f': f2, 'grad_f': grad2, 'hess_f': hess2, 'is_feasible': is_feasible2, 'x0': [0.2, 0.2, 0.2]},\n        {'f': f2, 'grad_f': grad2, 'hess_f': hess2, 'is_feasible': is_feasible2, 'x0': [5.0, 5.0, 5.0]},\n        {'f': f3, 'grad_f': grad3, 'hess_f': hess3, 'is_feasible': is_feasible3, 'x0': [0.0, 0.0]},\n        {'f': f3, 'grad_f': grad3, 'hess_f': hess3, 'is_feasible': is_feasible3, 'x0': [1.5, 1.5]},\n    ]\n    \n    results = []\n    \n    for case_params in test_cases:\n        run_output = run_newton_method(\n            case_params['f'], \n            case_params['grad_f'], \n            case_params['hess_f'], \n            case_params['is_feasible'], \n            case_params['x0']\n        )\n        \n        if not run_output['success'] or not run_output['switched']:\n            results.append(False)\n            continue\n            \n        d1_res = run_output['d1_ok']\n        d2_res = run_output['d2_ok']\n        \n        # D3: Quadratic convergence of lambda for first two post-switch iterations\n        d3_res = False\n        switch_k = run_output['switch_k']\n        lambda_hist = run_output['lambda_history']\n        \n        # To check D3, we need lambda values at switch_k, switch_k+1, and switch_k+2.\n        # This requires the loop to run at least up to k = switch_k + 2.\n        # So len(lambda_hist) must be = switch_k + 3.\n        if len(lambda_hist) = switch_k + 3:\n            check1 = lambda_hist[switch_k + 1] = 3.0 * lambda_hist[switch_k]**2\n            check2 = lambda_hist[switch_k + 2] = 3.0 * lambda_hist[switch_k + 1]**2\n            if check1 and check2:\n                d3_res = True\n        \n        # All diagnostics must be satisfied\n        results.append(d1_res and d2_res and d3_res)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3176751"}]}