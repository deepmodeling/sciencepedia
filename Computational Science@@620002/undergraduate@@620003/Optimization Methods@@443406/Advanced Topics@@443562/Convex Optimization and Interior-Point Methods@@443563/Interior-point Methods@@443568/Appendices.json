{"hands_on_practices": [{"introduction": "The heart of any interior-point method is the iterative process of solving a barrier-augmented optimization problem using Newton's method. To truly understand how IPMs navigate the feasible region, it is essential to grasp the mechanics of a single Newton step. This practice exercise [@problem_id:3139232] provides a foundational, hands-on calculation for a box-constrained quadratic program, guiding you through the process of computing the gradient, Hessian, and the resulting search direction. By working through this example, you will see precisely how the logarithmic barrier function penalizes proximity to the boundaries and influences the path towards the optimum.", "problem": "Consider the box-constrained Quadratic Program (QP), defined as minimizing the quadratic objective subject to simple bounds, $$\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} x^{\\top} Q x + q^{\\top} x \\quad \\text{subject to} \\quad \\ell \\leq x \\leq u,$$ where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $q \\in \\mathbb{R}^{n}$, and the bounds satisfy $\\ell_{i} < u_{i}$ for all $i \\in \\{1,\\dots,n\\}$. Interior-point methods convert inequality constraints into a barrier-augmented unconstrained problem using a strictly convex barrier function that diverges on the boundary of the feasible region.\n\n(a) Starting from the core definition of a logarithmic barrier that penalizes proximity to each bound, derive a barrier formulation for the box constraints that uses separate logarithmic terms for the lower and upper bounds. Then write the barrier-augmented objective $F_{t}(x)$ that combines the original objective with the barrier, using a positive scalar parameter $t > 0$.\n\n(b) For the three-dimensional case ($n = 3$) with \n$$Q = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}, \\quad q = \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\ell = \\begin{pmatrix} 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad u = \\begin{pmatrix} 3 \\\\ 2 \\\\ \\frac{5}{2} \\end{pmatrix},$$ \ntake the barrier parameter $t = 1$ and the strictly feasible point \n$$x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.$$\nCompute, by first principles, the gradient $\\nabla F_{t}(x)$ and Hessian $\\nabla^{2} F_{t}(x)$ of the barrier-augmented objective at the given $x$. Using exact arithmetic, perform one Newton step $p$ defined by solving the linear system \n$$\\nabla^{2} F_{t}(x) \\, p = - \\nabla F_{t}(x),$$ \nand then report the Newton decrement squared, defined by \n$$\\lambda(x)^{2} = \\nabla F_{t}(x)^{\\top} \\left(\\nabla^{2} F_{t}(x)\\right)^{-1} \\nabla F_{t}(x).$$\nGive the final answer for $\\lambda(x)^{2}$ as a single exact rational number. Do not round.", "solution": "**Part (a): Barrier Formulation**\n\nThe box constraint $\\ell \\leq x \\leq u$ can be written as $n$ pairs of inequality constraints:\n$$ x_i - \\ell_i \\geq 0 \\quad \\text{and} \\quad u_i - x_i \\geq 0 \\quad \\text{for } i = 1, \\dots, n $$\nThe logarithmic barrier function for a set of inequalities $g_j(x) \\geq 0$ is $\\phi(x) = -\\sum_j \\ln(g_j(x))$. Applying this to our constraints, the barrier function for the box constraints is:\n$$ \\phi(x) = -\\sum_{i=1}^{n} \\ln(x_i - \\ell_i) - \\sum_{i=1}^{n} \\ln(u_i - x_i) $$\nThis function is strictly convex and smooth in the interior of the feasible region, and it approaches $+\\infty$ as $x$ approaches the boundary.\n\nInterior-point methods solve a sequence of unconstrained problems where the objective is an augmentation of the original objective with the barrier function. A common formulation for the barrier-augmented objective is:\n$$ F_t(x) = f(x) + \\frac{1}{t} \\phi(x) $$\nwhere $f(x) = \\frac{1}{2} x^{\\top} Q x + q^{\\top} x$ is the original objective function and $t > 0$ is the barrier parameter. As $t \\to \\infty$, the solutions to $\\min F_t(x)$ trace the central path towards the solution of the original constrained problem.\n\nSubstituting the expressions for $f(x)$ and $\\phi(x)$, we get:\n$$ F_t(x) = \\frac{1}{2} x^{\\top} Q x + q^{\\top} x - \\frac{1}{t} \\sum_{i=1}^{n} \\left[ \\ln(x_i - \\ell_i) + \\ln(u_i - x_i) \\right] $$\n\n**Part (b): Computation for the 3D Case**\n\nFor this part, we are given $n=3$, $t=1$, and specific values for $Q, q, \\ell, u,$ and $x$. With $t=1$, the objective function becomes:\n$$ F_1(x) = \\frac{1}{2} x^{\\top} Q x + q^{\\top} x - \\sum_{i=1}^{3} \\left[ \\ln(x_i - \\ell_i) + \\ln(u_i - x_i) \\right] $$\n\n**1. Gradient and Hessian Computation**\n\nThe gradient of $F_1(x)$ is:\n$$ \\nabla F_1(x) = Q x + q - \\begin{pmatrix} \\frac{1}{x_1 - \\ell_1} \\\\ \\frac{1}{x_2 - \\ell_2} \\\\ \\frac{1}{x_3 - \\ell_3} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{u_1 - x_1} \\\\ \\frac{1}{u_2 - x_2} \\\\ \\frac{1}{u_3 - x_3} \\end{pmatrix} $$\nThe Hessian of $F_1(x)$ is:\n$$ \\nabla^2 F_1(x) = Q + \\text{diag}\\left(\\frac{1}{(x_i - \\ell_i)^2}\\right) + \\text{diag}\\left(\\frac{1}{(u_i - x_i)^2}\\right) $$\nWe evaluate these at the given point $x = (1, 0, 1)^{\\top}$. First, we compute the vectors $x-\\ell$ and $u-x$:\n$$ x - \\ell = \\begin{pmatrix} 1 - 0 \\\\ 0 - (-1) \\\\ 1 - 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/2 \\end{pmatrix} $$\n$$ u - x = \\begin{pmatrix} 3 - 1 \\\\ 2 - 0 \\\\ 5/2 - 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3/2 \\end{pmatrix} $$\n\nLet's compute the gradient, which we denote by $g$:\n$$ Qx = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\n$$ Qx + q = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nThe barrier gradient terms are:\n$$ \\begin{pmatrix} 1/1 \\\\ 1/1 \\\\ 1/(1/2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\quad \\text{and} \\quad \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/(3/2) \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 2/3 \\end{pmatrix} $$\nSo, the gradient is:\n$$ g = \\nabla F_1(x) = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 1 + 1/2 \\\\ 1 + 1/2 \\\\ 1 + 2/3 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 5/3 \\end{pmatrix} $$\n\nNext, we compute the Hessian, which we denote by $H$:\nThe diagonal matrices from the barrier term are:\n$$ \\text{diag}\\left(\\frac{1}{1^2}, \\frac{1}{1^2}, \\frac{1}{(1/2)^2}\\right) = \\text{diag}(1, 1, 4) $$\n$$ \\text{diag}\\left(\\frac{1}{2^2}, \\frac{1}{2^2}, \\frac{1}{(3/2)^2}\\right) = \\text{diag}(1/4, 1/4, 4/9) $$\nThe Hessian is the sum of $Q$ and these two diagonal matrices:\n$$ H = \\nabla^2 F_1(x) = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 1/4 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 4/9 \\end{pmatrix} $$\n$$ H = \\begin{pmatrix} 4+1+1/4 & 1 & 0 \\\\ 1 & 3+1+1/4 & 1 \\\\ 0 & 1 & 2+4+4/9 \\end{pmatrix} = \\begin{pmatrix} 21/4 & 1 & 0 \\\\ 1 & 17/4 & 1 \\\\ 0 & 1 & 58/9 \\end{pmatrix} $$\n\n**2. Newton Step and Newton Decrement Squared**\n\nThe Newton decrement squared is $\\lambda(x)^2 = g^{\\top}H^{-1}g$. To compute this with exact rational arithmetic, we can use the formula $\\lambda(x)^2 = g^{\\top} (\\text{adj}(H)) g / \\det(H)$, where $\\text{adj}(H)$ is the adjugate of $H$.\n\nFirst, we compute the determinant of $H$:\n$$ \\det(H) = \\frac{21}{4}\\left(\\frac{17}{4} \\cdot \\frac{58}{9} - 1\\right) - 1\\left(1 \\cdot \\frac{58}{9}\\right) = \\frac{21}{4}\\left(\\frac{986}{36} - \\frac{36}{36}\\right) - \\frac{58}{9} $$\n$$ \\det(H) = \\frac{21}{4}\\left(\\frac{950}{36}\\right) - \\frac{58}{9} = \\frac{3325}{24} - \\frac{58}{9} = \\frac{3 \\cdot 3325 - 8 \\cdot 58}{72} = \\frac{9975 - 464}{72} = \\frac{9511}{72} $$\n\nNext, we compute the product $g^{\\top} \\text{adj}(H) g$. Let $y = \\text{adj}(H) g$. Then the product is $g^{\\top}y$.\n$$ \\text{adj}(H) = \\begin{pmatrix} \\frac{17}{4}\\frac{58}{9}-1 & -\\frac{58}{9} & 1 \\\\ -\\frac{58}{9} & \\frac{21}{4}\\frac{58}{9} & -\\frac{21}{4} \\\\ 1 & -\\frac{21}{4} & \\frac{21}{4}\\frac{17}{4}-1 \\end{pmatrix} = \\begin{pmatrix} 475/18 & -58/9 & 1 \\\\ -58/9 & 203/6 & -21/4 \\\\ 1 & -21/4 & 341/16 \\end{pmatrix} $$\n$$ y_1 = (\\frac{475}{18})(\\frac{3}{2}) + (-\\frac{58}{9})(\\frac{3}{2}) + (1)(\\frac{5}{3}) = \\frac{475}{12} - \\frac{58}{6} + \\frac{5}{3} = \\frac{475 - 116 + 20}{12} = \\frac{379}{12} $$\n$$ y_2 = (-\\frac{58}{9})(\\frac{3}{2}) + (\\frac{203}{6})(\\frac{3}{2}) + (-\\frac{21}{4})(\\frac{5}{3}) = -\\frac{58}{6} + \\frac{203}{4} - \\frac{35}{4} = -\\frac{29}{3} + \\frac{168}{4} = -\\frac{29}{3} + 42 = \\frac{97}{3} $$\n$$ y_3 = (1)(\\frac{3}{2}) + (-\\frac{21}{4})(\\frac{3}{2}) + (\\frac{341}{16})(\\frac{5}{3}) = \\frac{3}{2} - \\frac{63}{8} + \\frac{1705}{48} = \\frac{72 - 378 + 1705}{48} = \\frac{1399}{48} $$\nNow we compute $g^{\\top}y$:\n$$ g^{\\top}y = (\\frac{3}{2})(\\frac{379}{12}) + (\\frac{3}{2})(\\frac{97}{3}) + (\\frac{5}{3})(\\frac{1399}{48}) = \\frac{1137}{24} + \\frac{97}{2} + \\frac{6995}{144} $$\nThe least common denominator is $144$.\n$$ g^{\\top}y = \\frac{6 \\cdot 1137}{144} + \\frac{72 \\cdot 97}{144} + \\frac{6995}{144} = \\frac{6822 + 6984 + 6995}{144} = \\frac{20801}{144} $$\nFinally, we compute the Newton decrement squared:\n$$ \\lambda(x)^2 = \\frac{g^{\\top} \\text{adj}(H) g}{\\det(H)} = \\frac{20801/144}{9511/72} = \\frac{20801}{144} \\cdot \\frac{72}{9511} = \\frac{20801}{2 \\cdot 9511} = \\frac{20801}{19022} $$", "answer": "$$\\boxed{\\frac{20801}{19022}}$$", "id": "3139232"}, {"introduction": "Interior-point methods demonstrate remarkable versatility, extending elegantly from problems with vector variables to those involving matrix variables, such as Semidefinite Programs (SDPs). This practice [@problem_id:3139151] explores this generalization by focusing on the logarithmic determinant (log-det) barrier, the key to handling the positive semidefinite constraint $X \\succeq 0$. You will tackle a small-scale SDP, calculating the Newton direction for a $2 \\times 2$ matrix variable by hand. This exercise is designed to demystify the concepts of matrix-valued gradients and Hessians, providing a clear window into the inner workings of IPMs in the more advanced setting of semidefinite optimization.", "problem": "Consider the Semidefinite Programming (SDP) problem of minimizing the linear objective over the positive semidefinite cone with linear equality constraints. Specifically, consider the $2 \\times 2$ symmetric variable matrix $X$ and the problem\n$$\\min\\ \\mathrm{tr}(C X)\\quad \\text{subject to}\\quad \\mathrm{tr}(A_1 X) = b_1,\\ \\mathrm{tr}(A_2 X) = b_2,\\ X \\succeq 0,$$\nwhere $C = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}$, $A_1 = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}$, $A_2 = \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix}$, $b_1 = 2$, and $b_2 = 1$. Using an interior-point method with the logarithmic determinant barrier $-\\mu \\ln \\det(X)$, form the barrier-augmented objective. Then, at the feasible interior point\n$$X^{(k)} = \\begin{pmatrix}2 & 0.2 \\\\ 0.2 & 1\\end{pmatrix},$$\nwith barrier parameter $\\mu = 1$, derive from first principles the gradient and Hessian of the barrier term with respect to $X$ using the Frobenius inner product, set up the equality-constrained Newton system, and compute the Newton direction $\\Delta X$ that preserves the linearized equality constraints at $X^{(k)}$.\n\nReport the Newton direction as the ordered triple $(\\Delta x_{11}, \\Delta x_{12}, \\Delta x_{22})$, where $X = \\begin{pmatrix}x_{11} & x_{12} \\\\ x_{12} & x_{22}\\end{pmatrix}$. Your final answer must be a single row matrix containing these three entries. No rounding is required.", "solution": "The problem is stated as:\n$$ \\min_{X} \\ \\mathrm{tr}(C X) $$\n$$ \\text{subject to} \\quad \\mathrm{tr}(A_1 X) = b_1, \\ \\mathrm{tr}(A_2 X) = b_2, \\ X \\succeq 0 $$\nwhere $X$ is a $2 \\times 2$ symmetric matrix, $X = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{12} & x_{22} \\end{pmatrix}$.\n\nThe given data are:\n$C = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}$, $A_1 = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}$, $A_2 = \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix}$, $b_1 = 2$, $b_2 = 1$.\nThe current iterate is $X^{(k)} = \\begin{pmatrix}2 & 0.2 \\\\ 0.2 & 1\\end{pmatrix}$ and the barrier parameter is $\\mu=1$.\n\nFirst, we verify that $X^{(k)}$ is a strictly feasible point.\n1.  Symmetry: $X^{(k)}$ is symmetric.\n2.  Equality constraints:\n    $\\mathrm{tr}(A_1 X^{(k)}) = \\mathrm{tr}\\left(\\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} \\begin{pmatrix}2 & 0.2 \\\\ 0.2 & 1\\end{pmatrix}\\right) = 2 = b_1$.\n    $\\mathrm{tr}(A_2 X^{(k)}) = \\mathrm{tr}\\left(\\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}2 & 0.2 \\\\ 0.2 & 1\\end{pmatrix}\\right) = 1 = b_2$.\n    The constraints are satisfied.\n3.  Strict positivity: $X^{(k)} \\succ 0$.\n    The leading principal minors of $X^{(k)}$ are $2 > 0$ and $\\det(X^{(k)}) = 2(1) - (0.2)^2 = 2 - 0.04 = 1.96 > 0$.\n    Thus, $X^{(k)}$ is a strictly feasible point (an interior point).\n\nThe interior-point method uses a barrier function to handle the $X \\succeq 0$ constraint. The barrier-augmented objective function is:\n$$ f(X) = \\mathrm{tr}(C X) - \\mu \\ln \\det(X) $$\nThe Newton step $\\Delta X$ is found by solving a quadratically approximated problem subject to linearized constraints. The constraints on the Newton step $\\Delta X$ are derived from the linearization of the original equality constraints:\n$$ \\mathrm{tr}(A_i \\Delta X) = 0 \\quad \\text{for } i=1, 2 $$\nGiven the forms of $A_1$ and $A_2$, these constraints become $\\Delta x_{11} = 0$ and $\\Delta x_{22} = 0$. Thus, $\\Delta X$ has the form:\n$$ \\Delta X = \\begin{pmatrix} 0 & \\Delta x_{12} \\\\ \\Delta x_{12} & 0 \\end{pmatrix} $$\n\nThe Newton step is found by solving the KKT system for the subproblem. The relevant equation is:\n$$ \\nabla f(X^{(k)}) + \\nabla^2 f(X^{(k)})[\\Delta X] + y_1 A_1 + y_2 A_2 = 0 $$\nThe gradient of $f(X)$ is $\\nabla f(X) = C - \\mu X^{-1}$. The Hessian of $f(X)$ acting on $\\Delta X$ is $\\nabla^2 f(X)[\\Delta X] = \\mu X^{-1} \\Delta X X^{-1}$.\nSubstituting these and $\\mu=1$ into the Newton system equation gives:\n$$ C - (X^{(k)})^{-1} + (X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1} + y_1 A_1 + y_2 A_2 = 0 $$\nThis is a matrix equation. Since $A_1$ and $A_2$ are diagonal, their $(1,2)$ components are zero. The Lagrange multiplier terms $y_1 A_1 + y_2 A_2$ therefore have no influence on the $(1,2)$ entry of the equation. Taking the $(1,2)$ component of the matrix equation gives:\n$$ C_{12} - (X^{(k)-1})_{12} + ((X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1})_{12} = 0 $$\nWe have $C_{12} = 0$, so this simplifies to:\n$$ ((X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1})_{12} = (X^{(k)-1})_{12} $$\nLet's compute the inverse of $X^{(k)}$. We use fractional representation $0.2 = 1/5$.\n$X^{(k)} = \\begin{pmatrix}2 & 1/5 \\\\ 1/5 & 1\\end{pmatrix}$.\n$\\det(X^{(k)}) = 2(1) - (1/5)^2 = 2 - 1/25 = 49/25$.\n$X^{(k)-1} = \\frac{1}{49/25} \\begin{pmatrix} 1 & -1/5 \\\\ -1/5 & 2 \\end{pmatrix} = \\frac{25}{49} \\begin{pmatrix} 1 & -1/5 \\\\ -1/5 & 2 \\end{pmatrix} = \\begin{pmatrix} 25/49 & -5/49 \\\\ -5/49 & 50/49 \\end{pmatrix}$.\nSo, $(X^{(k)-1})_{12} = -5/49$.\n\nLet $V = X^{(k)-1}$. Let $\\delta = \\Delta x_{12}$. We have $\\Delta X = \\begin{pmatrix} 0 & \\delta \\\\ \\delta & 0 \\end{pmatrix}$.\nWe need to compute the $(1,2)$ component of $V \\Delta X V$.\n$V \\Delta X = \\begin{pmatrix} V_{11} & V_{12} \\\\ V_{12} & V_{22} \\end{pmatrix} \\begin{pmatrix} 0 & \\delta \\\\ \\delta & 0 \\end{pmatrix} = \\begin{pmatrix} \\delta V_{12} & \\delta V_{11} \\\\ \\delta V_{22} & \\delta V_{12} \\end{pmatrix}$.\nThe $(1,2)$ component of $(V \\Delta X) V$ is $(V\\Delta X)_{1,1}V_{1,2} + (V\\Delta X)_{1,2}V_{2,2} = (\\delta V_{12})V_{12} + (\\delta V_{11})V_{22} = \\delta (V_{12}^2 + V_{11}V_{22})$.\n\nSubstituting this into our equation: $\\delta (V_{12}^2 + V_{11}V_{22}) = V_{12}$.\nWe have the values for the components of $V$: $V_{11} = 25/49$, $V_{12} = -5/49$, $V_{22} = 50/49$.\n$$ \\delta \\left( \\left(-\\frac{5}{49}\\right)^2 + \\left(\\frac{25}{49}\\right)\\left(\\frac{50}{49}\\right) \\right) = -\\frac{5}{49} $$\n$$ \\delta \\left( \\frac{25}{49^2} + \\frac{1250}{49^2} \\right) = -\\frac{5}{49} $$\n$$ \\delta \\left( \\frac{1275}{2401} \\right) = -\\frac{5}{49} $$\n$$ \\delta = -\\frac{5}{49} \\cdot \\frac{2401}{1275} $$\nSince $2401 = 49^2$, this simplifies to:\n$$ \\delta = -5 \\cdot \\frac{49}{1275} $$\nFactoring the denominator, $1275 = 5 \\times 255 = 5 \\times 5 \\times 51 = 25 \\times 51$.\n$$ \\delta = -5 \\cdot \\frac{49}{25 \\cdot 51} = -\\frac{49}{5 \\cdot 51} = -\\frac{49}{255} $$\n\nSo, $\\Delta x_{12} = -49/255$.\nThe full Newton direction is specified by the triple $(\\Delta x_{11}, \\Delta x_{12}, \\Delta x_{22})$.\nWe have found $\\Delta x_{11}=0$, $\\Delta x_{12}=-49/255$, and $\\Delta x_{22}=0$.\nThe Newton direction is $(0, -49/255, 0)$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 0 & -\\frac{49}{255} & 0 \\end{pmatrix} } $$", "id": "3139151"}, {"introduction": "Moving from single-step calculations to a complete, practical implementation is a critical step in mastering any optimization algorithm. This hands-on coding practice [@problem_id:2402731] challenges you to build a primal-dual interior-point solver for a common portfolio optimization problem. A key feature of this exercise is the requirement to solve the internal linear systems without forming explicit matrix inverses, a crucial technique for large-scale problems. By implementing the solver using a matrix-free approach with the Conjugate Gradient method, you will gain invaluable insight into the design of efficient, scalable, and numerically robust optimization software.", "problem": "You are asked to write a complete, runnable program that computes optimal long-only portfolio weights for a set of quadratic programs defined by mean-variance objectives with a single full-investment constraint. The program must solve each quadratic program by only using products of the Hessian matrix with vectors and must not compute or use any explicit matrix inverse of the Hessian.\n\nThe optimization problem for a given test case is: minimize the objective\n$$\n\\frac{1}{2}\\,\\boldsymbol{x}^{\\top} Q\\,\\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{x}\n$$\nsubject to the constraints\n$$\n\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1,\\quad \\boldsymbol{x} \\succeq \\boldsymbol{0},\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{n}$ is the expected return vector, $\\boldsymbol{1}$ is the all-ones vector in $\\mathbb{R}^{n}$, and $\\boldsymbol{0}$ is the zero vector in $\\mathbb{R}^{n}$. The decision variable is $\\boldsymbol{x} \\in \\mathbb{R}^{n}$. All computations are to be done in purely numerical terms; no physical units apply.\n\nYour program must produce, for each test case, the optimal weight vector $\\boldsymbol{x}^{\\star}$ as a list of real numbers, each rounded to six decimal places. The constraint $\\boldsymbol{x} \\succeq \\boldsymbol{0}$ enforces long-only positions. The full-investment constraint is $\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1$.\n\nThe test suite consists of the following three cases. In each case, $A = \\boldsymbol{1}^{\\top}$ and $b = 1$ define the single equality constraint. All numeric entries below are exact.\n\n- Test case $1$ ($n=4$):\n  - Hessian\n    $$\n    Q_1 =\n    \\begin{bmatrix}\n    0.10 & 0.02 & 0.04 & 0.00 \\\\\n    0.02 & 0.08 & 0.01 & 0.00 \\\\\n    0.04 & 0.01 & 0.09 & 0.00 \\\\\n    0.00 & 0.00 & 0.00 & 0.05\n    \\end{bmatrix}\n    $$\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_1 = \\begin{bmatrix} 0.12 \\\\ 0.10 \\\\ 0.07 \\\\ 0.03 \\end{bmatrix}\n    $$\n\n- Test case $2$ ($n=3$):\n  - Hessian\n    $$\n    Q_2 =\n    \\begin{bmatrix}\n    0.20 & 0.15 & 0.15 \\\\\n    0.15 & 0.30 & 0.25 \\\\\n    0.15 & 0.25 & 0.50\n    \\end{bmatrix}\n    $$\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_2 = \\begin{bmatrix} 0.18 \\\\ 0.02 \\\\ 0.01 \\end{bmatrix}\n    $$\n\n- Test case $3$ ($n=3$):\n  - Hessian\n    $$\n    Q_3 = 0.02 \\cdot\n    \\begin{bmatrix}\n    1.0 & 0.999 & 0.9995 \\\\\n    0.999 & 1.0 & 0.9992 \\\\\n    0.9995 & 0.9992 & 1.0\n    \\end{bmatrix}\n    +\n    10^{-6}\\, I_3\n    $$\n    where $I_3$ is the $3 \\times 3$ identity matrix.\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_3 = \\begin{bmatrix} 0.050 \\\\ 0.051 \\\\ 0.049 \\end{bmatrix}\n    $$\n\nYour program must:\n- Enforce the constraints exactly within numerical tolerance.\n- Use only matrix-vector products involving $Q$; do not form or use any explicit inverse of $Q$ or of any derived matrix.\n- Return the optimizer $\\boldsymbol{x}^{\\star}$ for each test case, rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the rounded weights for the corresponding test case, for example,\n$$\n[ [x_{1,1},x_{1,2},\\dots], [x_{2,1},x_{2,2},\\dots], [x_{3,1},x_{3,2},\\dots] ].\n$$\nNo spaces are required, and each entry must be printed with exactly six digits after the decimal point.\n\nDesign for coverage:\n- The first case is a standard instance with moderate correlations.\n- The second case encourages boundary solutions where some optimal weights may be numerically zero.\n- The third case is nearly collinear and tests numerical stability.\n\nThe expected answer for each test case is the list of optimal weights as real numbers. Each list must consist of exactly $n$ floating-point numbers.", "solution": "The problem presented is a convex quadratic program (QP) concerning optimal portfolio allocation under a mean-variance framework. The objective is to minimize a function representing a trade-off between portfolio risk (variance) and return, subject to a full-investment constraint and a long-only (non-negativity) constraint on the portfolio weights $\\boldsymbol{x}$.\n\nThe optimization problem is formally stated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & f(\\boldsymbol{x}) = \\frac{1}{2}\\,\\boldsymbol{x}^{\\top} Q\\,\\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{x} \\\\\n\\text{subject to} \\quad & \\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1 \\\\\n& \\boldsymbol{x} \\succeq \\boldsymbol{0}\n\\end{aligned}\n$$\nwhere $Q$ is a symmetric positive definite covariance matrix, $\\boldsymbol{\\mu}$ is the vector of expected returns, and $\\boldsymbol{x}$ is the vector of portfolio weights. The condition that $Q$ is positive definite ensures that the objective function $f(\\boldsymbol{x})$ is strictly convex. The feasible region, defined by the linear equality and non-negativity constraints, is a simplex, which is a closed and convex set. The minimization of a strictly convex function over a non-empty, closed, convex set guarantees the existence of a unique optimal solution $\\boldsymbol{x}^{\\star}$.\n\nThe optimality of a solution is fully characterized by the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(\\boldsymbol{x}, \\lambda, \\boldsymbol{s}) = \\frac{1}{2}\\boldsymbol{x}^{\\top} Q \\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top} \\boldsymbol{x} - \\lambda(\\boldsymbol{1}^{\\top}\\boldsymbol{x} - 1) - \\boldsymbol{s}^{\\top}\\boldsymbol{x}\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier for the equality constraint and $\\boldsymbol{s} \\in \\mathbb{R}^n$ is the vector of Lagrange multipliers for the non-negativity constraints. The KKT conditions for optimality are:\n1.  **Stationarity:** $\\nabla_{\\boldsymbol{x}} \\mathcal{L} = Q\\boldsymbol{x} - \\boldsymbol{\\mu} - \\lambda\\boldsymbol{1} - \\boldsymbol{s} = \\boldsymbol{0}$\n2.  **Primal Feasibility:** $\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1$, $\\boldsymbol{x} \\succeq \\boldsymbol{0}$\n3.  **Dual Feasibility:** $\\boldsymbol{s} \\succeq \\boldsymbol{0}$\n4.  **Complementary Slackness:** $x_i s_i = 0$ for all $i \\in \\{1, \\dots, n\\}$\n\nTo solve this system of equations and inequalities, we employ a primal-dual interior-point method (IPM). This class of algorithms is highly effective for QPs and is well-suited to the problem's constraints, particularly the restriction against forming or using explicit matrix inverses. IPMs iteratively solve a sequence of perturbed KKT systems, approaching the optimal solution from the interior of the feasible region.\n\nThe complementary slackness conditions $x_i s_i = 0$ are replaced by a perturbed version, $x_i s_i = \\tau$, where $\\tau > 0$ is a barrier parameter that is driven to zero as the iterations proceed. The core of the IPM is to apply Newton's method to solve this perturbed KKT system for the search directions $(\\Delta\\boldsymbol{x}, \\Delta\\lambda, \\Delta\\boldsymbol{s})$. This leads to a linear system at each iteration.\n\nLet the current iterate be $(\\boldsymbol{x}, \\lambda, \\boldsymbol{s})$. The primal and dual residuals are:\n$$\n\\begin{aligned}\n\\boldsymbol{r}_p &= \\boldsymbol{1}^{\\top}\\boldsymbol{x} - 1 \\\\\n\\boldsymbol{r}_d &= Q\\boldsymbol{x} - \\boldsymbol{\\mu} - \\lambda\\boldsymbol{1} - \\boldsymbol{s}\n\\end{aligned}\n$$\nThe Newton system for the search directions $(\\Delta\\boldsymbol{x}, \\Delta\\lambda, \\Delta\\boldsymbol{s})$ is:\n$$\n\\begin{bmatrix}\nQ & \\boldsymbol{1} & I \\\\\n\\boldsymbol{1}^{\\top} & 0 & \\boldsymbol{0}^{\\top} \\\\\nS & \\boldsymbol{0} & X\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta\\boldsymbol{x} \\\\\n-\\Delta\\lambda \\\\\n-\\Delta\\boldsymbol{s}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{r}_d \\\\\nr_p \\\\\nX\\boldsymbol{s} - \\sigma \\mu_{gap} \\boldsymbol{1}\n\\end{bmatrix}\n$$\nwhere $X = \\text{diag}(\\boldsymbol{x})$, $S = \\text{diag}(\\boldsymbol{s})$, $\\mu_{gap} = (\\boldsymbol{x}^{\\top}\\boldsymbol{s})/n$ is the duality gap, and $\\sigma \\in [0, 1]$ is a centering parameter.\n\nTo adhere to the constraint of not forming or inverting matrices, we do not solve this system directly. Instead, we eliminate $\\Delta\\boldsymbol{s}$ to obtain a smaller, equivalent system known as the augmented system or normal equations:\n$$\n\\begin{bmatrix}\nQ + X^{-1}S & \\boldsymbol{1} \\\\\n\\boldsymbol{1}^{\\top} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta\\boldsymbol{x} \\\\\n-\\Delta\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{b}_1 \\\\\nb_2\n\\end{bmatrix}\n$$\nHere, the matrix $H_{bar} = Q + X^{-1}S$ is symmetric and positive definite, as $Q$ is positive definite and $X^{-1}S$ is a diagonal matrix with positive entries (since $\\boldsymbol{x} > \\boldsymbol{0}$ and $\\boldsymbol{s} > \\boldsymbol{0}$ in the interior).\n\nTo solve this system for $(\\Delta\\boldsymbol{x}, \\Delta\\lambda)$ without inverting $H_{bar}$, we use the Schur complement method. We solve for $\\Delta\\boldsymbol{x}$ in terms of $\\Delta\\lambda$ from the first block equation: $\\Delta\\boldsymbol{x} = H_{bar}^{-1}(\\boldsymbol{b}_1 - \\Delta\\lambda\\boldsymbol{1})$. Substituting this into the second block equation gives a scalar equation for $\\Delta\\lambda$:\n$$\n\\Delta\\lambda = \\frac{\\boldsymbol{1}^{\\top}H_{bar}^{-1}\\boldsymbol{b}_1 - b_2}{\\boldsymbol{1}^{\\top}H_{bar}^{-1}\\boldsymbol{1}}\n$$\nComputing $\\Delta\\lambda$ requires the vectors $H_{bar}^{-1}\\boldsymbol{b}_1$ and $H_{bar}^{-1}\\boldsymbol{1}$. These are obtained by solving the linear systems $H_{bar}\\boldsymbol{y}_1 = \\boldsymbol{b}_1$ and $H_{bar}\\boldsymbol{y}_2 = \\boldsymbol{1}$. Crucially, we use the Conjugate Gradient (CG) method to solve these systems. CG is an iterative algorithm that finds the solution using only matrix-vector products of the form $H_{bar}\\boldsymbol{v} = Q\\boldsymbol{v} + (X^{-1}S)\\boldsymbol{v}$. This operation relies only on matrix-vector products with the original Hessian $Q$, thereby satisfying all problem constraints.\n\nOnce $\\Delta\\lambda$ and $\\Delta\\boldsymbol{x}$ are found, $\\Delta\\boldsymbol{s}$ is recovered. A line search is then performed to determine a step size $\\alpha \\in (0, 1]$ that maintains the positivity of $\\boldsymbol{x}$ and $\\boldsymbol{s}$. The iterates are updated:\n$$\n\\boldsymbol{x} \\leftarrow \\boldsymbol{x} + \\alpha \\Delta\\boldsymbol{x}, \\quad \\lambda \\leftarrow \\lambda + \\alpha \\Delta\\lambda, \\quad \\boldsymbol{s} \\leftarrow \\boldsymbol{s} + \\alpha \\Delta\\boldsymbol{s}\n$$\nThis process is repeated until the primal residuals, dual residuals, and the duality gap are all below a specified tolerance, at which point the algorithm has converged to the optimal solution $\\boldsymbol{x}^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(matvec_func, b, x0=None, tol=1e-10, max_iter=1000):\n    \"\"\"\n    Solves the symmetric positive-definite system Ax = b using the Conjugate Gradient method.\n    'matvec_func' is a function that computes the product A@v.\n    \"\"\"\n    n = len(b)\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    r = b - matvec_func(x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old) < tol:\n        return x\n\n    for _ in range(max_iter):\n        Ap = matvec_func(p)\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n\n        if np.sqrt(rs_new) < tol:\n            break\n\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return x\n\ndef solve_portfolio_optimization(Q, mu, max_iter=50, tol_outer=1e-9, tol_inner=1e-12, eta=0.995, sigma=0.1):\n    \"\"\"\n    Solves the quadratic program using a primal-dual interior-point method.\n    The method iteratively solves the KKT conditions using Newton's method,\n    with an inner loop using Conjugate Gradient to solve linear systems\n    without explicit matrix inversion.\n    \"\"\"\n    n = len(mu)\n    \n    # Initialization\n    x = np.ones(n) / n\n    lam = 0.0\n    s = np.ones(n)\n\n    ones_n = np.ones(n)\n\n    for _ in range(max_iter):\n        # Calculate residuals and duality gap\n        res_p = np.sum(x) - 1.0\n        res_d = Q @ x - mu - lam * ones_n - s\n        gap = np.dot(s, x) / n\n\n        # Convergence check\n        norm_res_d = np.linalg.norm(res_d)\n        if norm_res_d < tol_outer and abs(res_p) < tol_outer and gap < tol_outer:\n            break\n\n        # Define matrix-vector product for the CG solver's system matrix H_bar = Q + X^-1 * S\n        s_over_x = s / x\n        def h_bar_mv(v):\n            return Q @ v + s_over_x * v\n\n        # Set up the right-hand sides for the reduced Newton system\n        rhs_dx_eq = -res_d - s + (sigma * gap) / x\n        rhs_lam_eq = -res_p\n\n        # Solve the linear systems using Conjugate Gradient\n        # System 1: H_bar * y1 = rhs_dx_eq\n        # System 2: H_bar * y2 = 1\n        y1 = conjugate_gradient(h_bar_mv, rhs_dx_eq, tol=tol_inner, max_iter=2*n)\n        y2 = conjugate_gradient(h_bar_mv, ones_n, tol=tol_inner, max_iter=2*n)\n\n        # Compute search directions for lambda, x, and s\n        dlam = (rhs_lam_eq - np.sum(y1)) / np.sum(y2)\n        dx = y1 + dlam * y2\n        ds = (-s * x - s * dx + sigma * gap) / x\n        \n        # Line search for step sizes to maintain positivity\n        alpha_p = 1.0\n        if np.any(dx < 0):\n            alpha_p = min(1.0, np.min(-x[dx < 0] / dx[dx < 0]))\n            \n        alpha_s = 1.0\n        if np.any(ds < 0):\n            alpha_s = min(1.0, np.min(-s[ds < 0] / ds[ds < 0]))\n        \n        alpha = min(alpha_p, alpha_s) * eta\n\n        # Update iterates\n        x += alpha * dx\n        s += alpha * ds\n        lam += alpha * dlam\n\n    # Post-processing for clean output\n    x[x < tol_outer] = 0.0\n    x /= np.sum(x)\n    \n    return x\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    \"\"\"\n    # Test case 1\n    Q1 = np.array([\n        [0.10, 0.02, 0.04, 0.00],\n        [0.02, 0.08, 0.01, 0.00],\n        [0.04, 0.01, 0.09, 0.00],\n        [0.00, 0.00, 0.00, 0.05]\n    ])\n    mu1 = np.array([0.12, 0.10, 0.07, 0.03])\n\n    # Test case 2\n    Q2 = np.array([\n        [0.20, 0.15, 0.15],\n        [0.15, 0.30, 0.25],\n        [0.15, 0.25, 0.50]\n    ])\n    mu2 = np.array([0.18, 0.02, 0.01])\n\n    # Test case 3\n    Q3_base = 0.02 * np.array([\n        [1.0,    0.999,  0.9995],\n        [0.999,  1.0,    0.9992],\n        [0.9995, 0.9992, 1.0]\n    ])\n    Q3 = Q3_base + 1e-6 * np.identity(3)\n    mu3 = np.array([0.050, 0.051, 0.049])\n\n    test_cases = [\n        (Q1, mu1),\n        (Q2, mu2),\n        (Q3, mu3),\n    ]\n\n    results = []\n    for Q, mu in test_cases:\n        x_star = solve_portfolio_optimization(Q, mu)\n        results.append(x_star)\n\n    # Format the final output string exactly as required\n    def format_list(arr):\n        return '[' + ','.join([f'{x:.6f}' for x in arr]) + ']'\n        \n    formatted_strings = [format_list(res) for res in results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```", "id": "2402731"}]}