{"hands_on_practices": [{"introduction": "A cornerstone of understanding interior-point methods is the relationship between the barrier parameter $\\mu$ and the duality gap. This first exercise provides a crucial bridge between theory and practice. You will begin by deriving one of the most elegant results of the central path for linear programs: the duality gap is precisely $n\\mu$. Then, you will implement a Newton-based solver to numerically compute points on the central path and verify this theoretical identity, observing firsthand how controlling $\\mu$ allows us to systematically reduce the duality gap. [@problem_id:3107281]", "problem": "Consider a standard linear program in equality form with nonnegativity constraints. The primal problem is to minimize $c^{\\mathsf{T}} x$ subject to $A x = b$ and $x \\ge 0$, and the dual problem is to maximize $b^{\\mathsf{T}} y$ subject to $A^{\\mathsf{T}} y + s = c$ and $s \\ge 0$. The central path for a barrier parameter $\\mu \\in \\mathbb{R}_{>0}$ is defined as the set of triplets $(x(\\mu), y(\\mu), s(\\mu))$ satisfying the perturbed Karush–Kuhn–Tucker (KKT) conditions\n$$\nA x = b,\\quad A^{\\mathsf{T}} y + s = c,\\quad X S \\mathbf{e} = \\mu \\mathbf{e},\\quad x > 0,\\ s > 0,\n$$\nwhere $X = \\mathrm{diag}(x)$, $S = \\mathrm{diag}(s)$, and $\\mathbf{e}$ is the vector of all ones of dimension $n$.\n\nTasks:\n1. Starting only from the definitions of the primal and dual problems, the stationarity and complementary slackness conditions, and the central path definition above, derive a symbolic identity that expresses the duality gap $g(\\mu) \\equiv c^{\\mathsf{T}} x(\\mu) - b^{\\mathsf{T}} y(\\mu)$ purely in terms of the barrier parameter $\\mu$ and the number of primal variables $n$. Do not assume any result beyond these definitions.\n2. Design a damped Newton method that, given $(A,b,c)$ and a positive $\\mu$, numerically computes a strictly positive solution $(x(\\mu), y(\\mu), s(\\mu))$ to the central path equations by solving the nonlinear system\n$$\nF(x,y,s;\\mu) \\equiv \n\\begin{bmatrix}\nA x - b \\\\\nA^{\\mathsf{T}} y + s - c \\\\\nX S \\mathbf{e} - \\mu \\mathbf{e}\n\\end{bmatrix}\n= \\mathbf{0},\n$$\nwith a line search that maintains $x > 0$ and $s > 0$. Use the Newton step obtained by linearizing $F$ at the current iterate and a fraction-to-the-boundary step length to enforce positivity.\n3. For each test case below, use the same set of barrier parameters $\\mu \\in \\{ 1.0,\\ 0.1,\\ 10^{-3},\\ 10^{-6} \\}$. For each $\\mu$ and each test case, compute a numerical central path solution $(x(\\mu),y(\\mu),s(\\mu))$ using your Newton method. Then evaluate the absolute deviation\n$$\n\\Delta(\\mu) \\equiv \\left|\\, c^{\\mathsf{T}} x(\\mu) - b^{\\mathsf{T}} y(\\mu) \\;-\\; f(n,\\mu)\\,\\right|,\n$$\nwhere $f(n,\\mu)$ is the function of $n$ and $\\mu$ you derived in Task 1. Your program must output, for each test case, the list of $\\Delta(\\mu)$ values corresponding to the four specified $\\mu$ values, in the exact order given.\n\nTest suite:\n- Test case 1:\n  - $m = 1$, $n = 2$,\n  - $A = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 2 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 2 & 1.5 \\end{bmatrix}^{\\mathsf{T}}$.\n- Test case 2:\n  - $m = 2$, $n = 3$,\n  - $A = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1 & 3 & 2 \\end{bmatrix}^{\\mathsf{T}}$.\n- Test case 3:\n  - $m = 3$, $n = 5$,\n  - $A = \\begin{bmatrix}\n      1 & 0 & 1 & 0 & 1 \\\\\n      0 & 1 & 1 & 2 & 0 \\\\\n      2 & 0 & 0 & 1 & 1\n    \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 3 \\\\ 4 \\\\ 4 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1 & 2 & 3 & 4 & 1 \\end{bmatrix}^{\\mathsf{T}}$.\n- Test case 4:\n  - $m = 2$, $n = 4$,\n  - $A = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 1 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 0.5 & 1.5 & 2.0 & 3.0 \\end{bmatrix}^{\\mathsf{T}}$.\n\nImplementation details and requirements:\n- Use the Newton system obtained by differentiating $F(x,y,s;\\mu)$ to compute $(\\Delta x, \\Delta y, \\Delta s)$ at each iteration. Use a fraction-to-the-boundary step length with factor $\\tau = 0.99$ to maintain strict positivity for $x$ and $s$. Employ a simple backtracking line search if needed to reduce the residual norm $\\|F\\|_2$.\n- Use the initial strictly feasible point $x^{(0)} = \\mathbf{1}_n$, $y^{(0)} = \\mathbf{0}_m$, $s^{(0)} = c$, which satisfies $A x^{(0)} = b$ and $A^{\\mathsf{T}} y^{(0)} + s^{(0)} = c$ with $x^{(0)} > 0$, $s^{(0)} > 0$ for the given test cases.\n- Numerical tolerances must be chosen to reliably resolve $\\Delta(\\mu)$ down to at least $10^{-9}$ in absolute value when $\\mu = 10^{-6}$.\n- Your program must produce a single line of output containing the results as a comma-separated list of four sublists, one per test case, each sublist listing the four $\\Delta(\\mu)$ values in the order $\\mu \\in \\{ 1.0,\\ 0.1,\\ 10^{-3},\\ 10^{-6} \\}$. For example, the output format must be\n  $$\n  \\big[\\,[\\Delta_1(\\mu_1),\\Delta_1(\\mu_2),\\Delta_1(\\mu_3),\\Delta_1(\\mu_4)],\\ [\\Delta_2(\\mu_1),\\ldots],\\ [\\Delta_3(\\mu_1),\\ldots],\\ [\\Delta_4(\\mu_1),\\ldots]\\,\\big]\n  $$\n  with no extra text.", "solution": "The problem is evaluated as valid, as it is scientifically grounded in the established theory of interior-point methods for linear programming, is well-posed with all necessary information provided, and is formulated objectively.\n\n### Task 1: Symbolic Identity for the Duality Gap\n\nThe first task is to derive an identity for the duality gap, $g(\\mu)$, on the central path, expressed in terms of the barrier parameter $\\mu$ and the number of primal variables $n$. The duality gap is defined as the difference between the primal and dual objective function values:\n$$\ng(\\mu) \\equiv c^{\\mathsf{T}} x(\\mu) - b^{\\mathsf{T}} y(\\mu)\n$$\nwhere $(x(\\mu), y(\\mu), s(\\mu))$ is a point on the central path. The central path is defined by the Karush-Kuhn-Tucker (KKT) conditions for the logarithmic barrier problem. The relevant conditions provided in the problem statement are:\n1. Primal feasibility: $A x = b$\n2. Dual feasibility and stationarity: $A^{\\mathsf{T}} y + s = c$\n3. Perturbed complementary slackness: $X S \\mathbf{e} = \\mu \\mathbf{e}$, where $X = \\mathrm{diag}(x)$ and $S = \\mathrm{diag}(s)$\n\nWe begin with the definition of the duality gap. We can substitute the expression for $c$ from the dual feasibility condition into the gap equation:\n$$\ng(\\mu) = (A^{\\mathsf{T}} y + s)^{\\mathsf{T}} x - b^{\\mathsf{T}} y\n$$\nUsing the properties of the transpose, $(U+V)^{\\mathsf{T}} = U^{\\mathsf{T}} + V^{\\mathsf{T}}$ and $(UV)^{\\mathsf{T}} = V^{\\mathsf{T}} U^{\\mathsf{T}}$, we expand the first term:\n$$\ng(\\mu) = (y^{\\mathsf{T}} A + s^{\\mathsf{T}}) x - b^{\\mathsf{T}} y = y^{\\mathsf{T}} (Ax) + s^{\\mathsf{T}} x - b^{\\mathsf{T}} y\n$$\nNext, we use the primal feasibility condition, $A x = b$, to substitute for the term $(Ax)$:\n$$\ng(\\mu) = y^{\\mathsf{T}} b + s^{\\mathsf{T}} x - b^{\\mathsf{T}} y\n$$\nThe terms $y^{\\mathsf{T}} b$ and $b^{\\mathsf{T}} y$ are both scalars representing the same inner product, so they are equal. Thus, they cancel each other out:\n$$\ng(\\mu) = s^{\\mathsf{T}} x\n$$\nThis shows that the duality gap is equal to the inner product of the primal variables $x$ and the dual slack variables $s$.\n\nNow, we use the perturbed complementary slackness condition, $X S \\mathbf{e} = \\mu \\mathbf{e}$. This is a vector equation, which can be written component-wise as:\n$$\nx_i s_i = \\mu \\quad \\text{for } i = 1, 2, \\ldots, n\n$$\nThe inner product $s^{\\mathsf{T}} x$ is the sum of these component-wise products:\n$$\ns^{\\mathsf{T}} x = \\sum_{i=1}^{n} s_i x_i\n$$\nSubstituting $x_i s_i = \\mu$ for each term in the summation, we get:\n$$\ns^{\\mathsf{T}} x = \\sum_{i=1}^{n} \\mu = n \\mu\n$$\nTherefore, the duality gap on the central path is directly proportional to the barrier parameter $\\mu$ and the number of primal variables $n$.\n$$\ng(\\mu) = c^{\\mathsf{T}} x(\\mu) - b^{\\mathsf{T}} y(\\mu) = n \\mu\n$$\nThe function $f(n, \\mu)$ requested in the problem is thus $f(n, \\mu) = n \\mu$.\n\n### Task 2: Damped Newton Method Design\n\nThe second task is to design a damped Newton method to find a point $(x(\\mu), y(\\mu), s(\\mu))$ on the central path for a given $\\mu > 0$. This involves solving the nonlinear system of equations $F(x,y,s;\\mu) = \\mathbf{0}$, where:\n$$\nF(x,y,s;\\mu) = \n\\begin{bmatrix}\nA x - b \\\\\nA^{\\mathsf{T}} y + s - c \\\\\nX S \\mathbf{e} - \\mu \\mathbf{e}\n\\end{bmatrix}\n= \\mathbf{0}\n$$\nNewton's method iteratively finds a search direction $(\\Delta x, \\Delta y, \\Delta s)$ by solving the linearized system $J(\\Delta x, \\Delta y, \\Delta s)^{\\mathsf{T}} = -F$, where $J$ is the Jacobian of $F$. The Jacobian of $F$ with respect to $(x, y, s)$ is:\n$$\nJ(x,y,s) = \n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x}(Ax-b) & \\frac{\\partial}{\\partial y}(Ax-b) & \\frac{\\partial}{\\partial s}(Ax-b) \\\\\n\\frac{\\partial}{\\partial x}(A^{\\mathsf{T}}y+s-c) & \\frac{\\partial}{\\partial y}(A^{\\mathsf{T}}y+s-c) & \\frac{\\partial}{\\partial s}(A^{\\mathsf{T}}y+s-c) \\\\\n\\frac{\\partial}{\\partial x}(XSe-\\mu e) & \\frac{\\partial}{\\partial y}(XSe-\\mu e) & \\frac{\\partial}{\\partial s}(XSe-\\mu e)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nA & \\mathbf{0} & \\mathbf{0} \\\\\n\\mathbf{0} & A^{\\mathsf{T}} & I \\\\\nS & \\mathbf{0} & X\n\\end{bmatrix}\n$$\nThe Newton system is defined by $J \\begin{pmatrix} \\Delta x \\\\ \\Delta y \\\\ \\Delta s \\end{pmatrix} = -\\begin{pmatrix} A x - b \\\\ A^{\\mathsf{T}} y + s - c \\\\ X S \\mathbf{e} - \\mu \\mathbf{e} \\end{pmatrix}$. Let us define the residuals for primal feasibility ($r_p$), dual feasibility ($r_d$), and centrality ($r_c$):\n$$\nr_p = b - Ax \\\\\nr_d = c - A^{\\mathsf{T}} y - s \\\\\nr_c = \\mu \\mathbf{e} - X S \\mathbf{e} = \\mu \\mathbf{e} - \\mathrm{diag}(x) \\mathrm{diag}(s) \\mathbf{e}\n$$\nThe Newton system becomes:\n1. $A \\Delta x = r_p$\n2. $A^{\\mathsf{T}} \\Delta y + \\Delta s = r_d$\n3. $S \\Delta x + X \\Delta s = r_c$\n\nTo solve this system efficiently, we use block elimination. From (2), we express $\\Delta s$ in terms of $\\Delta y$: $\\Delta s = r_d - A^{\\mathsf{T}} \\Delta y$. Substitute this into (3):\n$$\nS \\Delta x + X(r_d - A^{\\mathsf{T}} \\Delta y) = r_c \\implies S \\Delta x - X A^{\\mathsf{T}} \\Delta y = r_c - X r_d\n$$\nSince $S$ is diagonal and positive (as $s>0$), we can solve for $\\Delta x$:\n$$\n\\Delta x = S^{-1}(X A^{\\mathsf{T}} \\Delta y + r_c - X r_d) = X S^{-1} A^{\\mathsf{T}} \\Delta y + S^{-1}(r_c - X r_d)\n$$\nNow substitute this expression for $\\Delta x$ into (1):\n$$\nA \\left( X S^{-1} A^{\\mathsf{T}} \\Delta y + S^{-1}(r_c - X r_d) \\right) = r_p\n$$\nRearranging gives the normal equations system for $\\Delta y$:\n$$\n(A X S^{-1} A^{\\mathsf{T}}) \\Delta y = r_p - A S^{-1}(r_c - X r_d)\n$$\nThis is a linear system of size $m \\times m$ for $\\Delta y$, where $m$ is the number of rows of $A$. The matrix $M = A X S^{-1} A^{\\mathsf{T}}$ is symmetric and positive definite (assuming $A$ has full row rank), and thus invertible.\n\nThe algorithm for one Newton iteration is as follows:\n1. Given the current iterate $(x, y, s)$, calculate the residuals $r_p$, $r_d$, and $r_c$.\n2. Form the normal equations matrix $M = A \\mathrm{diag}(x) \\mathrm{diag}(s^{-1}) A^{\\mathsf{T}}$ and the right-hand side vector $v = r_p - A \\mathrm{diag}(s^{-1})(r_c - \\mathrm{diag}(x)r_d)$.\n3. Solve the $m \\times m$ system $M \\Delta y = v$ for $\\Delta y$.\n4. Back-substitute to find $\\Delta s$: $\\Delta s = r_d - A^{\\mathsf{T}} \\Delta y$.\n5. Back-substitute to find $\\Delta x$: $\\Delta x = S^{-1}(r_c - X \\Delta s) = \\mathrm{diag}(s^{-1})(r_c - \\mathrm{diag}(x)\\Delta s)$.\n\nTo maintain strict positivity for $x$ and $s$, a line search is performed. A step length $\\alpha \\in (0, 1]$ is chosen. The update is then $(x, y, s) \\leftarrow (x, y, s) + \\alpha (\\Delta x, \\Delta y, \\Delta s)$.\nThe step length $\\alpha$ is determined by the fraction-to-the-boundary rule:\n$$\n\\alpha_x^{\\max} = \\min \\left( \\{1\\} \\cup \\{ -x_i/\\Delta x_i \\mid \\Delta x_i < 0 \\} \\right) \\\\\n\\alpha_s^{\\max} = \\min \\left( \\{1\\} \\cup \\{ -s_i/\\Delta s_i \\mid \\Delta s_i < 0 \\} \\right) \\\\\n\\alpha = \\tau \\min(\\alpha_x^{\\max}, \\alpha_s^{\\max})\n$$\nwhere $\\tau \\in (0, 1)$ is a safety factor, given as $\\tau=0.99$. If backtracking is needed, $\\alpha$ can be reduced further (e.g., by a factor of $0.5$) until the residual norm $\\|F\\|_2$ decreases.\n\n### Task 3: Numerical Implementation and Evaluation\n\nThe final task is to implement the Newton method and apply it to the given test cases. For each test case and each value of $\\mu \\in \\{ 1.0, 0.1, 10^{-3}, 10^{-6} \\}$, the algorithm computes the central path point $(x(\\mu), y(\\mu), s(\\mu))$. The initial point for the Newton iterations is specified as $x^{(0)} = \\mathbf{1}_n$, $y^{(0)} = \\mathbf{0}_m$, and $s^{(0)} = c$. For all test cases, this initial point is strictly feasible, satisfying $A x^{(0)} = b$, $A^{\\mathsf{T}} y^{(0)} + s^{(0)} = c$, $x^{(0)} > 0$, and $s^{(0)} > 0$. The iterations continue until the Euclidean norm of the residual vector, $\\|F(x,y,s;\\mu)\\|_2$, falls below a prescribed tolerance (e.g., $10^{-12}$).\n\nAfter convergence, the absolute deviation $\\Delta(\\mu)$ is calculated:\n$$\n\\Delta(\\mu) = \\left| c^{\\mathsf{T}} x(\\mu) - b^{\\mathsf{T}} y(\\mu) - n \\mu \\right|\n$$\nGiven that $c^{\\mathsf{T}}x-b^{\\mathsf{T}}y = s^{\\mathsf{T}}x$ for feasible points, and the algorithm drives $XSe$ towards $\\mu e$, we can also write this as:\n$$\n\\Delta(\\mu) = \\left| s(\\mu)^{\\mathsf{T}} x(\\mu) - n \\mu \\right|\n$$\nThis value measures how closely the numerically computed duality gap matches the theoretical value on the central path. The final output will be a list containing the computed $\\Delta(\\mu)$ values for each test case.", "answer": "```python\nimport numpy as np\n\ndef newton_solver(A, b, c, mu, x0, y0, s0):\n    \"\"\"\n    Solves the central path equations using a damped Newton method.\n    F(x,y,s;mu) = [Ax - b; A^T y + s - c; X S e - mu e] = 0\n    \"\"\"\n    x, y, s = x0.copy(), y0.copy(), s0.copy()\n    m, n = A.shape\n    e = np.ones(n)\n    \n    # Algorithmic parameters\n    max_iter = 50\n    tolerance = 1e-12  # Tolerance for the residual norm\n    tau = 0.99  # Fraction-to-the-boundary factor\n\n    for i in range(max_iter):\n        # Calculate residuals\n        r_p = b - (A @ x)\n        r_d = c - (A.T @ y) - s\n        r_c = mu * e - x * s\n\n        # Check for convergence\n        residuals = np.concatenate((r_p, r_d, r_c))\n        res_norm = np.linalg.norm(residuals)\n        if res_norm < tolerance:\n            break\n\n        # Solve the Newton system using normal equations\n        # Efficiently compute M = A @ diag(x/s) @ A.T\n        x_over_s = x / s\n        M = (A * x_over_s) @ A.T\n\n        # Efficiently compute rhs = r_p - A @ ((r_c - x * r_d) / s)\n        rhs_vec = (r_c - x * r_d) / s\n        rhs = r_p - A @ rhs_vec\n\n        # Solve for dy\n        dy = np.linalg.solve(M, rhs)\n        \n        # Back-substitute for ds\n        ds = r_d - A.T @ dy\n        \n        # Back-substitute for dx\n        dx = (r_c - x * ds) / s\n\n        # Line search (fraction-to-the-boundary rule)\n        alpha_x_vals = [-xi / dxi for xi, dxi in zip(x, dx) if dxi < 0]\n        alpha_x = min([1.0] + alpha_x_vals)\n\n        alpha_s_vals = [-si / dsi for si, dsi in zip(s, ds) if dsi < 0]\n        alpha_s = min([1.0] + alpha_s_vals)\n\n        alpha = tau * min(alpha_x, alpha_s)\n        \n        # Simple backtracking to ensure residual reduction (optional but good practice)\n        current_res_norm_sq = res_norm**2\n        for _ in range(10): # Max 10 backtracking steps\n            x_new, y_new, s_new = x + alpha * dx, y + alpha * dy, s + alpha * ds\n            r_p_new = b - (A @ x_new)\n            r_d_new = c - (A.T @ y_new) - s_new\n            r_c_new = mu * e - x_new * s_new\n            new_res_norm_sq = np.linalg.norm(r_p_new)**2 + np.linalg.norm(r_d_new)**2 + np.linalg.norm(r_c_new)**2\n            if new_res_norm_sq < current_res_norm_sq:\n                break\n            alpha *= 0.5 # Reduce step size\n        \n        # Update variables\n        x, y, s = x_new, y_new, s_new\n\n    return x, y, s\n\ndef solve_for_case(A, b, c):\n    \"\"\"\n    Computes the absolute deviation Delta(mu) for a given problem instance\n    and a set of mu values.\n    \"\"\"\n    m, n = A.shape\n    mu_values = [1.0, 0.1, 1e-3, 1e-6]\n    delta_values = []\n\n    for mu in mu_values:\n        # Per the problem, use the same initial feasible point for each mu.\n        x0 = np.ones(n)\n        y0 = np.zeros(m)\n        s0 = c.copy()\n\n        # Compute the point on the central path\n        x_mu, y_mu, s_mu = newton_solver(A, b, c, mu, x0, y0, s0)\n\n        # Calculate the numerical duality gap\n        # Although c.T@x - b.T@y is the primary definition, s.T@x is numerically\n        # more direct for checking the centrality condition.\n        # The KKT conditions imply they are equal at the solution.\n        numerical_gap = s_mu.T @ x_mu\n\n        # Theoretical gap on the central path is n*mu\n        theoretical_gap = n * mu\n\n        # Calculate the absolute deviation\n        delta = abs(numerical_gap - theoretical_gap)\n        delta_values.append(delta)\n        \n    return delta_values\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"A\": np.array([[1.0, 1.0]]),\n            \"b\": np.array([2.0]),\n            \"c\": np.array([2.0, 1.5])\n        },\n        # Test case 2\n        {\n            \"A\": np.array([[1.0, 2.0, 0.0], [0.0, 1.0, 1.0]]),\n            \"b\": np.array([3.0, 2.0]),\n            \"c\": np.array([1.0, 3.0, 2.0])\n        },\n        # Test case 3\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 1.0, 0.0, 1.0],\n                [0.0, 1.0, 1.0, 2.0, 0.0],\n                [2.0, 0.0, 0.0, 1.0, 1.0]\n            ]),\n            \"b\": np.array([3.0, 4.0, 4.0]),\n            \"c\": np.array([1.0, 2.0, 3.0, 4.0, 1.0])\n        },\n        # Test case 4\n        {\n            \"A\": np.array([[1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 1.0]]),\n            \"b\": np.array([2.0, 3.0]),\n            \"c\": np.array([0.5, 1.5, 2.0, 3.0])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_for_case(case[\"A\"], case[\"b\"], case[\"c\"])\n        all_results.append(result)\n\n    # Format the final output string to be exactly as required (no spaces)\n    output_str = repr(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3107281"}, {"introduction": "While the central path offers a theoretically sound route to an optimal solution, numerical practice reveals that the journey is not always smooth. This exercise explores the practical challenges of path-following by constructing a scenario where the feasible region is a narrow wedge formed by nearly parallel constraints. By implementing a barrier method for this problem, you will investigate how this geometric \"squeezing\" leads to an ill-conditioned Hessian matrix, affecting the stability and efficiency of the Newton steps. This practice provides valuable insight into the numerical robustness of interior-point algorithms. [@problem_id:3107306]", "problem": "You will construct and analyze a family of two-dimensional linear programs whose active constraint normals are nearly parallel, and then implement a path-following interior-point method based on a logarithmic barrier to empirically reveal how the central path navigates a narrow feasible wedge and how matrix conditioning affects Newton steps.\n\nThe fundamental base you should use is the convexity of the logarithmic barrier, the Karush–Kuhn–Tucker (KKT) optimality conditions, and Newton’s method for solving smooth unconstrained minimization of a barrier-augmented objective. From these, derive the central path equations and the Newton step for the barrier subproblem.\n\nConsider the following construction. For a given angle parameter $\\theta \\in (0,\\pi/2)$ (angle unit: radians), define the two inequality constraints with nearly parallel outward normals\n$$\na_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\\quad a_2 = \\begin{bmatrix}\\cos\\theta \\\\ \\sin\\theta\\end{bmatrix},\\quad b_1 = 1,\\quad b_2 = 1,\n$$\ntogether with the symmetric box constraints\n$$\n- B \\le x_1 \\le B,\\quad - B \\le x_2 \\le B,\\quad \\text{with } B = 2.\n$$\nLet the objective be to minimize $c^\\top x$ with\n$$\nc = - (a_1 + a_2).\n$$\nThis choice induces an optimal solution near the intersection where both $a_1^\\top x = b_1$ and $a_2^\\top x = b_2$ are active. For a set of inequality constraints indexed by $i \\in \\{1,\\dots,m\\}$, write them uniformly as\n$$\ng_i(x) = b_i - a_i^\\top x > 0,\\quad \\text{so that } x \\text{ must satisfy } g_i(x) > 0 \\text{ for all } i.\n$$\nThe log-barrier path-following subproblem for a parameter $t > 0$ is\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f_t(x) = t\\, c^\\top x - \\sum_{i=1}^m \\log\\big(g_i(x)\\big),\n$$\nwhose central path is characterized by the unique minimizer $x^\\star(t)$ of $f_t$, as $t \\to +\\infty$.\n\nTasks you must complete:\n\n1) Starting from the definitions of $f_t(x)$ and $g_i(x)$, derive the gradient $\\nabla f_t(x)$ and the Hessian $\\nabla^2 f_t(x)$ in terms of $\\{a_i,b_i\\}$ and $t$.\n\n2) Implement a path-following (barrier) method that constructs an increasing sequence $t_k$ and, for each $t_k$, finds the Newton point $x^\\star(t_k)$ by applying damped Newton steps to $f_{t_k}$ starting from the previous central point as a warm start. Use the following concrete and testable choices:\n- Initialize at $t_0 = 1$ and $x_0 = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$.\n- Use a multiplicative schedule $t_{k+1} = \\mu \\, t_k$ with $\\mu = 10$.\n- Let the number of inequality constraints be $m = 6$ (two angular constraints and four box constraints).\n- Terminate the outer loop when the surrogate duality gap $m / t_k \\le \\varepsilon_{\\text{gap}}$ with $\\varepsilon_{\\text{gap}} = 10^{-4}$.\n- For the inner Newton loop at a fixed $t_k$, compute the Newton step $\\Delta x$ by solving $\\nabla^2 f_{t_k}(x)\\, \\Delta x = - \\nabla f_{t_k}(x)$, and use a backtracking line search that enforces:\n  (i) strict feasibility $g_i(x + \\alpha \\Delta x) > 0$ via a fraction-to-the-boundary step with fraction $\\eta = 0.99$,\n  (ii) an Armijo decrease condition with parameter $\\beta = 10^{-4}$ and backtracking contraction factor $\\gamma = 0.5$ starting at $\\alpha = 1$.\n- Stop the inner Newton loop when the squared Newton decrement $\\lambda(x)^2 = \\nabla f_t(x)^\\top \\left(\\nabla^2 f_t(x)\\right)^{-1} \\nabla f_t(x)$ satisfies $\\lambda(x)^2 / 2 \\le 10^{-8}$.\n\n3) Quantify conditioning. At each Newton iteration, compute the condition number (in the spectral $2$-norm) of the Hessian $\\nabla^2 f_t(x)$. Aggregate the maximum value observed over the entire run.\n\n4) Quantify step acceptance. Track the minimum step size $\\alpha$ accepted by the backtracking line search across the entire run.\n\n5) Quantify computational effort. Count the total number of Newton steps taken across all outer iterations until termination.\n\nProvide a didactic dataset in the form of a test suite by evaluating the algorithm for the following angles (in radians), which progress from a well-conditioned to a nearly parallel regime:\n- $\\theta = 0.5$,\n- $\\theta = 0.1$,\n- $\\theta = 0.01$,\n- $\\theta = 0.001$.\n\nFor each test case $\\theta$, your program must return the triplet\n$$\n[\\text{total\\_newton\\_steps},\\; \\text{max\\_condition\\_number},\\; \\text{min\\_accepted\\_step}],\n$$\nwhere $\\text{total\\_newton\\_steps}$ is an integer, and the other two quantities are real numbers. No physical units apply; angles are in radians as specified.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to the test case in the same order as above and is itself a list in the form $[\\text{total\\_newton\\_steps},\\text{max\\_condition\\_number},\\text{min\\_accepted\\_step}]$. For example, the shape must be\n$$\n\\big[ [\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot] \\big].\n$$", "solution": "The user wants to implement and analyze a path-following interior-point method for a family of two-dimensional linear programs (LPs). The defining characteristic of these LPs is the presence of two constraint normals that become nearly parallel as a parameter $\\theta$ approaches zero, which is designed to highlight the numerical challenges associated with ill-conditioning.\n\nThe solution will be presented in the following steps:\n1.  Derivation of the gradient and Hessian of the logarithmic barrier objective function.\n2.  Detailed description of the path-following algorithm, including the outer loop for updating the barrier parameter $t$ and the inner Newton's method loop for centering.\n3.  Explanation of the line search procedure, reconciling the multiple conditions specified.\n4.  Justification of the data to be collected: total Newton steps, maximum Hessian condition number, and minimum accepted step size.\n\n### 1. Mathematical Formulation and Derivatives\n\nThe problem is to minimize a linear objective $c^\\top x$ subject to a set of linear inequality constraints. We are given $m=6$ constraints written in the form $a_i^\\top x < b_i$ for $i=1,\\dots,m$. In the barrier method, these are handled by defining functions $g_i(x) = b_i - a_i^\\top x > 0$.\n\nThe path-following method solves a sequence of unconstrained optimization problems defined by the objective function $f_t(x)$:\n$$ f_t(x) = t c^\\top x - \\sum_{i=1}^m \\log\\big(g_i(x)\\big) = t c^\\top x - \\sum_{i=1}^m \\log(b_i - a_i^\\top x) $$\nwhere $t > 0$ is the barrier parameter. The set of minimizers $x^\\star(t)$ for $t > 0$ forms the central path.\n\n**Task 1: Gradient and Hessian Derivation**\n\nTo apply Newton's method to minimize $f_t(x)$, we must compute its gradient $\\nabla f_t(x)$ and Hessian $\\nabla^2 f_t(x)$.\n\n**Gradient $\\nabla f_t(x)$:**\nThe gradient is computed by differentiating $f_t(x)$ with respect to the vector $x$. Using the chain rule on the logarithm term, $\\nabla_x \\log(u(x)) = \\frac{1}{u(x)}\\nabla_x u(x)$.\nHere, $u(x) = g_i(x) = b_i - a_i^\\top x$, so $\\nabla_x g_i(x) = -a_i$.\nThe gradient of the linear term $\\nabla_x(t c^\\top x)$ is simply $t c$.\n$$ \\nabla f_t(x) = \\nabla \\left( t c^\\top x - \\sum_{i=1}^m \\log(b_i - a_i^\\top x) \\right) $$\n$$ \\nabla f_t(x) = t c - \\sum_{i=1}^m \\frac{1}{b_i - a_i^\\top x} (-a_i) $$\n$$ \\nabla f_t(x) = t c + \\sum_{i=1}^m \\frac{a_i}{b_i - a_i^\\top x} $$\n\n**Hessian $\\nabla^2 f_t(x)$:**\nThe Hessian is the matrix of second partial derivatives, obtained by differentiating the gradient. The linear term $t c$ has a zero Hessian. For the summation term, we differentiate $\\frac{a_i}{b_i - a_i^\\top x}$ with respect to $x$.\n$$ \\nabla^2 f_t(x) = \\nabla \\left( t c + \\sum_{i=1}^m \\frac{a_i}{b_i - a_i^\\top x} \\right)^\\top $$\nUsing the rule $\\nabla_x \\left( \\frac{v(x)}{u(x)} \\right) = \\frac{u \\nabla v - v \\nabla u^\\top}{u^2}$ is complex. A simpler approach is to see each term as $a_i (b_i - a_i^\\top x)^{-1}$ and apply the chain rule again.\n$$ \\nabla_x \\left( (b_i - a_i^\\top x)^{-1} \\right) = -(b_i - a_i^\\top x)^{-2} (-a_i^\\top) = \\frac{a_i^\\top}{(b_i - a_i^\\top x)^2} $$\nThe derivative of the vector-valued function is then:\n$$ \\nabla_x \\left( \\frac{a_i}{b_i - a_i^\\top x} \\right) = a_i \\otimes \\nabla_x \\left( \\frac{1}{b_i - a_i^\\top x} \\right) = a_i \\frac{a_i^\\top}{(b_i - a_i^\\top x)^2} = \\frac{a_i a_i^\\top}{(b_i - a_i^\\top x)^2} $$\nThe symbol $\\otimes$ denotes the outer product. Summing over all constraints yields the Hessian:\n$$ \\nabla^2 f_t(x) = \\sum_{i=1}^m \\frac{a_i a_i^\\top}{(b_i - a_i^\\top x)^2} $$\nThe Hessian is a sum of positive semi-definite rank-one matrices. Since the set of vectors $\\{a_i\\}$ spans $\\mathbb{R}^2$, the Hessian is positive definite for any $x$ in the strict interior of the feasible set, which ensures that $f_t(x)$ is strictly convex and has a unique minimizer.\n\n### 2. Algorithmic Implementation\n\nThe algorithm consists of two nested loops.\n\n**Outer Loop: Path-Following**\nThis loop iteratively increases the barrier parameter $t$ to drive the solution towards the boundary of the feasible region, where the optimum of the original LP lies.\n1.  **Initialization**: Start with $t_0=1$ and a strictly feasible point $x_0 = [0, 0]^\\top$.\n2.  **Iteration**: At each step $k$, solve the unconstrained subproblem $\\min_x f_{t_k}(x)$ using Newton's method, starting from the previous solution $x^\\star(t_{k-1})$ as a warm start. The result is $x^\\star(t_k)$.\n3.  **Update**: Increase the barrier parameter using the multiplicative rule $t_{k+1} = \\mu t_k$, with $\\mu = 10$.\n4.  **Termination**: The loop terminates when the surrogate duality gap, $m/t_k$, falls below a specified tolerance $\\varepsilon_{\\text{gap}} = 10^{-4}$. The surrogate gap $m/t$ is an upper bound on the true duality gap and is a standard termination criterion for barrier methods.\n\n**Inner Loop: Newton's Method for Centering**\nFor a fixed $t=t_k$, we find the minimizer of $f_t(x)$ using damped Newton's method.\n1.  **Initialization**: Start at $x = x^\\star(t_{k-1})$ (or $x_0$ for the first outer iteration).\n2.  **Newton Step Calculation**: Compute the Newton step $\\Delta x$ by solving the linear system:\n    $$ \\nabla^2 f_t(x) \\Delta x = - \\nabla f_t(x) $$\n    where the gradient and Hessian are evaluated at the current iterate $x$.\n3.  **Newton Decrement**: Calculate the squared Newton decrement, $\\lambda(x)^2$:\n    $$ \\lambda(x)^2 = \\nabla f_t(x)^\\top (\\nabla^2 f_t(x))^{-1} \\nabla f_t(x) = - \\nabla f_t(x)^\\top \\Delta x $$\n4.  **Termination**: The inner loop terminates when $\\lambda(x)^2 / 2 \\le 10^{-8}$. The quantity $\\lambda(x)^2/2$ is an estimate of the suboptimality $f_t(x) - f_t(x^\\star(t))$.\n5.  **Line Search and Update**: Find an appropriate step size $\\alpha$ using backtracking line search. Update the solution: $x \\leftarrow x + \\alpha \\Delta x$.\n\n### 3. Backtracking Line Search\n\nThe step size $\\alpha$ is crucial for ensuring convergence. The problem specifies a backtracking line search with multiple conditions, which can be interpreted as a robust procedure:\n1.  **Feasibility Capping**: First, determine the maximum possible step size $\\alpha_{\\text{max}}$ that maintains strict feasibility, i.e., $g_i(x + \\alpha \\Delta x) > 0$. This is given by $\\alpha_{\\text{max}} = \\min \\{ (b_i - a_i^\\top x) / (a_i^\\top \\Delta x) \\}$ over all $i$ for which $a_i^\\top \\Delta x > 0$. An initial step size is proposed as $\\alpha = \\min(1.0, \\eta \\alpha_{\\text{max}})$ with $\\eta=0.99$. This prevents steps that land exactly on the boundary, which would make the log-barrier function undefined. This interpretation reconciles the \"starting at $\\alpha=1$\" general instruction with the specific \"fraction-to-the-boundary\" rule.\n2.  **Armijo Condition**: Starting with this capped $\\alpha$, the line search then contracts the step size until the Armijo-Goldstein sufficient decrease condition is met:\n    $$ f_t(x + \\alpha \\Delta x) \\le f_t(x) + \\beta \\alpha \\nabla f_t(x)^\\top \\Delta x $$\n    with $\\beta=10^{-4}$. If the condition fails, the step size is reduced by a factor $\\gamma=0.5$: $\\alpha \\leftarrow \\gamma \\alpha$.\n\nThis two-phase approach (capping for feasibility, then backtracking for sufficient decrease) ensures that every step is both strictly feasible and makes sufficient progress in minimizing the objective.\n\n### 4. Analysis and Quantifiers\n\nThe problem asks to quantify the algorithm's performance, especially as the constraints become nearly parallel ($\\theta \\to 0$).\n-   **Total Newton Steps**: This measures the total computational effort required to solve the LP to the desired precision. As $\\theta$ decreases, the central path becomes sharper, and the warm starts become less effective, potentially increasing the number of Newton steps per outer iteration.\n-   **Maximum Hessian Condition Number**: The condition number $\\kappa(\\nabla^2 f_t(x))$ measures the sensitivity of the Newton system solution to perturbations. As $\\theta \\to 0$, $a_1$ and $a_2$ become nearly collinear. When the iterate $x$ is near the corner where these two constraints are active, the corresponding terms in the Hessian sum, $\\frac{a_1 a_1^\\top}{g_1(x)^2}$ and $\\frac{a_2 a_2^\\top}{g_2(x)^2}$, create a matrix with one very large eigenvalue (direction $a_1+a_2$) and one much smaller eigenvalue (direction $a_1-a_2$). This leads to an explosion in the condition number, indicating severe ill-conditioning.\n-   **Minimum Accepted Step Size**: This tracks the smallest step size $\\alpha$ accepted by the line search. Severe ill-conditioning or sharp curvature in the central path may force the algorithm to take very small steps to satisfy the feasibility and/or Armijo conditions, indicating difficulty in making progress.\n\nBy tracking these three metrics for decreasing values of $\\theta$, we can empirically observe the degradation in numerical performance predicted by the theory of interior-point methods.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the path-following algorithm for the specified test cases.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    B = 2.0  # Box constraint size\n    M = 6    # Number of inequality constraints\n\n    # --- Path-Following Algorithm Parameters ---\n    T_0 = 1.0          # Initial barrier parameter\n    MU = 10.0          # Barrier parameter update factor\n    EPS_GAP = 1e-4     # Surrogate duality gap tolerance for termination\n\n    # --- Newton's Method (Inner Loop) Parameters ---\n    NEWTON_TOL_LAMBDA_SQ = 2 * 1e-8  # Termination tolerance for lambda^2\n    ETA = 0.99                     # Fraction-to-the-boundary for line search\n    BETA = 1e-4                    # Armijo condition parameter\n    GAMMA = 0.5                    # Backtracking line search contraction factor\n\n    def setup_problem(theta):\n        \"\"\"Constructs the LP matrices A, b, and cost vector c for a given theta.\"\"\"\n        a1 = np.array([1.0, 0.0])\n        b1 = 1.0\n        a2 = np.array([np.cos(theta), np.sin(theta)])\n        b2 = 1.0\n        \n        c = -(a1 + a2)\n        \n        A = np.array([\n            a1,                      # Constraint: x1 < 1\n            a2,                      # Constraint: cos(theta)*x1 + sin(theta)*x2 < 1\n            [1.0, 0.0],              # Box: x1 < 2\n            [-1.0, 0.0],             # Box: -x1 < 2\n            [0.0, 1.0],              # Box: x2 < 2\n            [0.0, -1.0]              # Box: -x2 < 2\n        ])\n        \n        b = np.array([b1, b2, B, B, B, B])\n        \n        return c, A, b\n\n    def objective_f_t(t, c, A, b, x):\n        \"\"\"Computes the log-barrier objective function f_t(x).\"\"\"\n        g = b - A @ x\n        if np.any(g <= 1e-12):  # Safety check for strict feasibility\n            return np.inf\n        return t * (c @ x) - np.sum(np.log(g))\n\n    def solve_for_theta(theta):\n        \"\"\"\n        Runs the full path-following method for a single theta value.\n        \n        Returns:\n            A list containing [total_newton_steps, max_condition_number, min_accepted_step].\n        \"\"\"\n        c, A, b = setup_problem(theta)\n        \n        # Initialization\n        t = T_0\n        x = np.array([0.0, 0.0])\n\n        total_newton_steps = 0\n        max_condition_number = 0.0\n        min_accepted_step = 1.0\n\n        # --- Outer Loop: Path-Following ---\n        while M / t > EPS_GAP:\n            \n            # --- Inner Loop: Newton's Method for Centering ---\n            while True:\n                # Calculate g(x), gradient, and Hessian\n                g = b - A @ x\n                inv_g = 1.0 / g\n                \n                grad = t * c + A.T @ inv_g\n                \n                weights = inv_g**2\n                H = A.T @ (A * weights[:, np.newaxis])\n                \n                # Track condition number\n                cond_H = np.linalg.cond(H)\n                max_condition_number = max(max_condition_number, cond_H)\n\n                # Solve Newton system: H * delta_x = -grad\n                try:\n                    delta_x = np.linalg.solve(H, -grad)\n                except np.linalg.LinAlgError:\n                    # Fallback for singular Hessian, though unlikely in theory\n                    delta_x = np.linalg.pinv(H, rcond=1e-15) @ -grad\n\n                # Calculate squared Newton decrement\n                lambda_sq = -grad.T @ delta_x\n                \n                # Check for inner loop convergence\n                if lambda_sq <= NEWTON_TOL_LAMBDA_SQ:\n                    break\n                \n                # --- Backtracking Line Search ---\n                # 1. Cap step size by fraction-to-the-boundary rule\n                alpha_max_boundary = np.inf\n                v = A @ delta_x\n                # Indices where we are moving towards a boundary (a_i^T * delta_x > 0)\n                mask = v > 1e-12\n                if np.any(mask):\n                    alpha_max_boundary = np.min(g[mask] / v[mask])\n                \n                alpha = min(1.0, ETA * alpha_max_boundary)\n\n                # 2. Enforce Armijo sufficient decrease condition\n                f_current = objective_f_t(t, c, A, b, x)\n                armijo_check_term = BETA * grad.T @ delta_x\n\n                while True:\n                    x_next = x + alpha * delta_x\n                    f_next = objective_f_t(t, c, A, b, x_next)\n                    if f_next <= f_current + alpha * armijo_check_term:\n                        break  # Armijo condition satisfied\n                    \n                    alpha *= GAMMA\n                    if alpha < 1e-16: # Safety break for extremely small alpha\n                        break\n                \n                # Update x and tracking variables\n                x = x_next\n                min_accepted_step = min(min_accepted_step, alpha)\n                total_newton_steps += 1\n            \n            # --- Update t for the next outer iteration ---\n            t *= MU\n\n        return [total_newton_steps, max_condition_number, min_accepted_step]\n\n    # --- Main Execution ---\n    test_cases = [0.5, 0.1, 0.01, 0.001]\n    \n    # Store results from each test case\n    results = []\n    for theta in test_cases:\n        res = solve_for_theta(theta)\n        results.append(res)\n    \n    # Format the output string exactly as required\n    formatted_results = []\n    for res in results:\n        # Format: [integer, float, float]\n        formatted_sublist = f\"[{res[0]},{res[1]},{res[2]}]\"\n        formatted_results.append(formatted_sublist)\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\n# Run the main solver\nsolve()\n```", "id": "3107306"}, {"introduction": "Interior-point methods offer a fundamentally different approach to solving linear programs compared to vertex-hopping algorithms like the simplex method. This exercise provides a clear, visualizable demonstration of this difference. You will explore an optimization problem over an $\\ell_{\\infty}$ ball—a simple hypercube—where the optimal solution is known to lie at a vertex. By implementing a path-following method, you will trace the central path and observe how it elegantly curves through the interior of the cube, approaching the vertex solution only as the barrier parameter $\\mu$ tends to zero, thereby remaining strictly within the interior for any finite $\\mu$. [@problem_id:3107367]", "problem": "Consider the linear optimization problem of minimizing a linear objective over the $\\ell_{\\infty}$ ball. Let $n \\in \\mathbb{N}$ be the dimension, $r \\in \\mathbb{R}_{>0}$ be the ball radius, and $c \\in \\mathbb{R}^{n}$ be the objective coefficient vector. The problem is to minimize $c^{\\top} x$ subject to $\\lVert x \\rVert_{\\infty} \\le r$, which is equivalent to the system of $2n$ linear inequalities $-r \\le x_i \\le r$ for $i \\in \\{1,\\dots,n\\}$. The exact solution to this linear program lies at a vertex of the hypercube $\\{-r,r\\}^{n}$ unless some component of $c$ is zero.\n\nThis task is to implement a central path computation using a logarithmic barrier and a path-following method, then examine numerically how the central path avoids vertices while the exact solution lies at a vertex. Use the following fundamental base:\n- The $\\ell_{\\infty}$ ball $\\{x \\in \\mathbb{R}^{n} : \\lVert x \\rVert_{\\infty} \\le r\\}$ is the intersection of the inequalities $x_i \\le r$ and $-x_i \\le r$ for all $i \\in \\{1,\\dots,n\\}$.\n- The logarithmic barrier for an inequality $a^{\\top} x \\le b$ is $-\\log(b - a^{\\top} x)$, which tends to $+\\infty$ as the slack $b - a^{\\top} x$ approaches $0$ from the positive side.\n- The central path is defined by the unique minimizers $x(t)$ of the strictly convex barrier-augmented objective for $t \\in \\mathbb{R}_{>0}$, starting from an interior point and remaining in the interior for all finite $t$.\n\nStarting from these principles, do not use any shortcut formulas. Derive what you need from scratch. In particular:\n1. Reformulate the barrier problem for the $\\ell_{\\infty}$ ball constraints. Show how the barrier augments the objective $c^{\\top} x$ and why the resulting function is strictly convex and finite only for $\\lVert x \\rVert_{\\infty} < r$.\n2. Derive the stationarity condition for the central path points $x(t)$ and implement a damped Newton method to solve the resulting nonlinear system for given $t$. Ensure the method maintains feasibility, i.e., $\\lVert x \\rVert_{\\infty} < r$, at every iteration by choosing appropriate step sizes.\n3. Compute the exact solution $x^{\\star}$ of the original linear program using only basic reasoning about linear objectives over boxes. Handle the case when some $c_i = 0$ in a scientifically sound way.\n4. For each test case, after computing the central path point $x(t)$, report:\n   - The Euclidean distance from $x(t)$ to the nearest vertex of the hypercube $\\{-r,r\\}^{n}$, as a real number.\n   - The objective gap $c^{\\top} x(t) - c^{\\top} x^{\\star}$, as a real number.\n   - A boolean that is true if $\\max_i |x_i(t)| < r$ and false otherwise, indicating whether the central path point strictly avoids the boundary (therefore vertices) for the chosen finite $t$.\n\nYour program must implement the damped Newton method and produce outputs for the following test suite of parameter values:\n- Test case $1$: $n = 2$, $r = 0.2$, $c = (1,2)$, $t = 5$.\n- Test case $2$: $n = 2$, $r = 0.2$, $c = (1,2)$, $t = 100$.\n- Test case $3$: $n = 3$, $r = 0.1$, $c = (1,-0.5,3)$, $t = 20$.\n- Test case $4$: $n = 3$, $r = 0.1$, $c = (1,0,1)$, $t = 20$.\n- Test case $5$: $n = 2$, $r = 0.05$, $c = (-1,-1)$, $t = 50$.\n\nThere are no physical quantities in this problem, so no physical units are required. Angles do not appear, so no angle unit is required. Percentages are not used.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[d, g, b]$ where $d$ is the Euclidean distance to the nearest vertex, $g$ is the objective gap, and $b$ is the interior boolean. For example, the output should be of the form $[[d_1,g_1,b_1],[d_2,g_2,b_2],\\dots]$.", "solution": "The problem requires the implementation of a central path, path-following method for a linear program over the $\\ell_{\\infty}$ ball. We must derive the necessary components from first principles and use a damped Newton method for the computation.\n\n### 1. Problem Reformulation and Barrier Function\n\nThe linear optimization problem is to minimize the objective function $f_0(x) = c^{\\top}x$ for a vector $x \\in \\mathbb{R}^n$, subject to the constraint $\\lVert x \\rVert_{\\infty} \\le r$. Here, $c \\in \\mathbb{R}^n$ is the objective coefficient vector, and $r \\in \\mathbb{R}_{>0}$ is the radius of the ball.\n\nThe constraint $\\lVert x \\rVert_{\\infty} \\le r$ is equivalent to the set of $2n$ linear inequalities:\n$x_i \\le r, \\quad \\text{for } i \\in \\{1,\\dots,n\\}$\n$-x_i \\le r, \\quad \\text{for } i \\in \\{1,\\dots,n\\}$\n\nThe logarithmic barrier function, $\\Phi(x)$, for this set of inequalities is the sum of the individual barriers:\n$$ \\Phi(x) = \\sum_{i=1}^{n} \\left( -\\log(r - x_i) - \\log(r - (-x_i)) \\right) = - \\sum_{i=1}^{n} \\log(r^2 - x_i^2) $$\nThe domain of $\\Phi(x)$ is the set of points where the arguments of the logarithms are positive, which is $r^2 - x_i^2 > 0$ for all $i$. This condition is equivalent to $|x_i| < r$ for all $i$, or $\\lVert x \\rVert_{\\infty} < r$. This is the strict interior of the feasible set. As any $x_i$ approaches either $r$ or $-r$, the term $\\log(r^2 - x_i^2)$ approaches $-\\infty$, so $\\Phi(x)$ approaches $+\\infty$.\n\nThe central path is defined by the minimizers $x(t)$ of the barrier-augmented objective function for a parameter $t > 0$:\n$$ F(x, t) = c^{\\top}x + \\frac{1}{t}\\Phi(x) = c^{\\top}x - \\frac{1}{t}\\sum_{i=1}^{n} \\log(r^2 - x_i^2) $$\nThis function is defined and finite only on the strict interior of the feasible set, $\\{x \\in \\mathbb{R}^n : \\lVert x \\rVert_{\\infty} < r\\}$.\n\nTo show that $F(x, t)$ is strictly convex, we analyze its Hessian matrix. The objective $c^{\\top}x$ is linear, hence convex. We examine the convexity of the barrier term. Let $\\phi(x) = -\\frac{1}{t} \\sum_{i=1}^n \\log(r^2 - x_i^2)$. The second partial derivatives are:\n$$ \\frac{\\partial^2 \\phi}{\\partial x_j \\partial x_k} = 0 \\quad \\text{for } j \\neq k $$\n$$ \\frac{\\partial^2 \\phi}{\\partial x_j^2} = -\\frac{1}{t} \\frac{\\partial^2}{\\partial x_j^2} \\log(r^2 - x_j^2) = -\\frac{1}{t} \\frac{\\partial}{\\partial x_j} \\left( \\frac{-2x_j}{r^2 - x_j^2} \\right) = \\frac{2}{t} \\frac{(r^2 - x_j^2) - x_j(-2x_j)}{(r^2-x_j^2)^2} = \\frac{2}{t} \\frac{r^2 + x_j^2}{(r^2 - x_j^2)^2} $$\nSince $t>0$, $r>0$, and $|x_j| < r$, the term $\\frac{\\partial^2 \\phi}{\\partial x_j^2}$ is strictly positive. The Hessian of $\\phi(x)$ is therefore a diagonal matrix with strictly positive diagonal entries, meaning it is positive definite. Thus, $\\phi(x)$ is a strictly convex function. The sum of a convex function ($c^{\\top}x$) and a strictly convex function ($\\phi(x)$) is strictly convex. A strictly convex function on a convex set has a unique minimizer.\n\n### 2. Central Path Stationarity and Newton's Method\n\nThe points $x(t)$ on the central path are the unique minimizers of $F(x, t)$. To find them, we set the gradient of $F(x,t)$ with respect to $x$ to zero (the stationarity condition):\n$$ \\nabla_x F(x, t) = \\nabla_x \\left( c^{\\top}x - \\frac{1}{t}\\sum_{i=1}^{n} \\log(r^2 - x_i^2) \\right) = 0 $$\nThe gradient vector $\\nabla_x F(x, t)$ has components:\n$$ (\\nabla_x F(x, t))_j = c_j - \\frac{1}{t} \\frac{-2x_j}{r^2 - x_j^2} = c_j + \\frac{2x_j}{t(r^2 - x_j^2)} $$\nWe must solve the nonlinear system of equations $\\nabla_x F(x, t) = 0$ for $x$. A damped Newton method is an iterative procedure for this purpose. Starting from a point $x_k$, the next iterate is $x_{k+1} = x_k + \\alpha_k \\Delta x_k$, where $\\Delta x_k$ is the Newton step and $\\alpha_k$ is a step size.\n\nThe Newton step $\\Delta x_k$ is found by solving the linear system:\n$$ \\nabla^2 F(x_k, t) \\Delta x_k = - \\nabla F(x_k, t) $$\nwhere $\\nabla^2 F(x_k, t)$ is the Hessian of $F(x,t)$ at $x_k$. As derived earlier, the Hessian is a diagonal matrix with entries:\n$$ (\\nabla^2 F(x, t))_{jj} = \\frac{2}{t} \\frac{r^2 + x_j^2}{(r^2 - x_j^2)^2} $$\nSince the Hessian is diagonal, the system decouples, and the Newton step components are easily computed:\n$$ (\\Delta x_k)_j = - \\frac{(\\nabla F(x_k, t))_j}{(\\nabla^2 F(x_k, t))_{jj}} $$\nThe step size $\\alpha_k \\in (0, 1]$ is chosen via a backtracking line search to ensure two conditions:\n1.  **Feasibility**: The next iterate $x_{k+1}$ must remain strictly feasible, i.e., $\\lVert x_k + \\alpha_k \\Delta x_k \\rVert_{\\infty} < r$.\n2.  **Sufficient Decrease**: The step must provide a sufficient decrease in the objective function value, typically enforced by the Armijo condition: $F(x_k + \\alpha_k \\Delta x_k, t) \\le F(x_k, t) + \\gamma \\alpha_k \\nabla F(x_k, t)^{\\top} \\Delta x_k$, for a small constant $\\gamma \\in (0, 0.5)$.\n\nThe iterative process starts with a strictly feasible point, such as $x_0 = 0$, and continues until the norm of the gradient $\\lVert \\nabla F(x_k, t) \\rVert_2$ falls below a certain tolerance, indicating that a stationary point has been found.\n\n### 3. Exact Solution of the Linear Program\n\nThe original problem is to minimize $\\sum_{i=1}^n c_i x_i$ subject to $-r \\le x_i \\le r$ for all $i$. The objective and constraints are separable. The contribution of each component $c_i x_i$ to the sum can be minimized independently.\nTo minimize $c_i x_i$ over the interval $[-r, r]$:\n- If $c_i > 0$, the minimum value is achieved when $x_i$ is most negative, so $x_i^{\\star} = -r$.\n- If $c_i < 0$, the minimum value is achieved when $x_i$ is most positive, so $x_i^{\\star} = r$.\n- If $c_i = 0$, the term $c_i x_i$ is always $0$, so any $x_i \\in [-r, r]$ is optimal for this component. The set of optimal solutions is a face of the hypercube.\n\nThe optimal objective value $p^{\\star} = c^{\\top}x^{\\star}$ is unique, regardless of the non-uniqueness of $x^\\star$ when some $c_i=0$. It can be calculated as:\n$$ p^{\\star} = \\sum_{i: c_i > 0} c_i(-r) + \\sum_{i: c_i < 0} c_i(r) = -r \\sum_{i: c_i > 0} c_i + r \\sum_{i: c_i < 0} c_i = -r \\left( \\sum_{i: c_i > 0} |c_i| + \\sum_{i: c_i < 0} |c_i| \\right) = -r \\sum_{i=1}^n |c_i| $$\n\n### 4. Required Computations\n\nFor each test case, after computing the central path point $x(t)$, we determine the following quantities:\n\n- **Euclidean distance to the nearest vertex ($d$)**: The vertices of the feasible hypercube are the points in $\\{-r, r\\}^n$. For a given point $x(t)$, the vertex $v^{\\star}$ nearest in Euclidean distance is found by setting each component $v^{\\star}_i$ to the closer of $-r$ or $r$. That is, $v^{\\star}_i = r \\cdot \\text{sign}(x_i(t))$ (with $\\text{sign}(0)$ being $1$ by convention). The distance is $d = \\lVert x(t) - v^{\\star} \\rVert_2$.\n\n- **Objective gap ($g$)**: This is the difference between the objective value at the central path point and the true optimal value: $g = c^{\\top}x(t) - p^{\\star}$.\n\n- **Interior boolean ($b$)**: This boolean is `true` if the computed point $x(t)$ is strictly inside the feasible region, i.e., $\\max_i |x_i(t)| < r$, and `false` otherwise. For a finite $t$ and a correct implementation, this should always be `true`.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the specified optimization problems using a damped Newton method\n    for the central path and computes the required metrics.\n    \"\"\"\n    \n    test_cases = [\n        {'n': 2, 'r': 0.2, 'c': np.array([1, 2]), 't': 5},\n        {'n': 2, 'r': 0.2, 'c': np.array([1, 2]), 't': 100},\n        {'n': 3, 'r': 0.1, 'c': np.array([1, -0.5, 3]), 't': 20},\n        {'n': 3, 'r': 0.1, 'c': np.array([1, 0, 1]), 't': 20},\n        {'n': 2, 'r': 0.05, 'c': np.array([-1, -1]), 't': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, r, c, t = case['n'], case['r'], case['c'], case['t']\n\n        # --- Damped Newton Method to find x(t) ---\n        \n        def F(x, t_p, c_p, r_p):\n            # Barrier augmented objective function\n            if np.max(np.abs(x)) >= r_p:\n                return np.inf\n            return c_p.T @ x - (1/t_p) * np.sum(np.log(r_p**2 - x**2))\n\n        def grad_F(x, t_p, c_p, r_p):\n            # Gradient of F\n            return c_p + (2 / t_p) * (x / (r_p**2 - x**2))\n\n        def Hess_F_diag(x, t_p, c_p, r_p):\n            # Diagonal of the Hessian of F\n            return (2 / t_p) * (r_p**2 + x**2) / (r_p**2 - x**2)**2\n\n        # Newton method parameters\n        x_t = np.zeros(n)\n        max_iter = 100\n        tolerance = 1e-9\n        \n        # Backtracking line search parameters\n        alpha = 1.0\n        beta = 0.5  # backtracking factor\n        gamma = 0.01 # Armijo condition parameter\n\n        for _ in range(max_iter):\n            grad = grad_F(x_t, t, c, r)\n            \n            # Check for convergence\n            if np.linalg.norm(grad) < tolerance:\n                break\n            \n            hess_diag = Hess_F_diag(x_t, t, c, r)\n            delta_x = -grad / hess_diag\n            \n            # Backtracking line search\n            alpha = 1.0\n            m = gamma * grad.T @ delta_x # Precompute for Armijo condition\n            f_x = F(x_t, t, c, r)\n            \n            while True:\n                x_new = x_t + alpha * delta_x\n                # Check feasibility first, as F is undefined outside\n                if np.max(np.abs(x_new)) >= r:\n                    alpha *= beta\n                    continue\n                # Check Armijo condition\n                if F(x_new, t, c, r) > f_x + alpha * m:\n                    alpha *= beta\n                else:\n                    break\n            \n            x_t += alpha * delta_x\n\n        # --- Exact Solution Calculation ---\n        p_star = -r * np.sum(np.abs(c))\n\n        # --- Compute required outputs ---\n        \n        # 1. Distance d to the nearest vertex\n        # The nearest vertex v_star has components r*sign(x_i)\n        v_star = np.where(x_t >= 0, r, -r)\n        d = np.linalg.norm(x_t - v_star)\n        \n        # 2. Objective gap g\n        g = c.T @ x_t - p_star\n        \n        # 3. Interior boolean b\n        b = np.max(np.abs(x_t)) < r\n\n        results.append([d, g, b])\n\n    # --- Format final output string ---\n    formatted_results = []\n    for res in results:\n        # Format boolean as lowercase 'true'/'false'\n        formatted_res = f\"[{res[0]},{res[1]},{str(res[2]).lower()}]\"\n        formatted_results.append(formatted_res)\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3107367"}]}