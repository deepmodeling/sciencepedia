{"hands_on_practices": [{"introduction": "The first step to mastering any algorithm is understanding its core mechanics. This exercise guides you through a single, complete iteration of the primal Affine Scaling method by hand. By calculating the Lagrange multiplier, search direction, and step size for a small-scale linear program, you will gain a concrete understanding of how each component contributes to the algorithm's progress towards an optimal solution [@problem_id:3095992].", "problem": "Consider the following Linear Program (LP): minimize the linear objective $c^{\\top} x$ subject to the affine equality constraints $A x = b$ and the nonnegativity constraints $x \\ge 0$. The data are\n$$\nA = \\begin{pmatrix} 1  1  1 \\end{pmatrix}, \\quad b = 3, \\quad c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix},\n$$\nand the strictly feasible starting point is\n$$\nx^{(0)} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix}.\n$$\nPerform one full iteration of the primal Affine Scaling (AS) method, beginning from first principles: interpret the search direction as the steepest descent direction for the linearized objective $c^{\\top} d$ restricted to the affine feasible set $A d = 0$ under the local norm induced by $X^{-1}$, where $X = \\mathrm{diag}(x^{(0)})$. Using this interpretation, determine the Lagrange multiplier $y$ that enforces first-order feasibility, then form the corresponding search direction $p$, and finally choose a step size $\\alpha$ using the fraction-to-the-boundary rule with parameter $\\beta = \\tfrac{9}{10}$. Compute the updated point $x^{(1)} = x^{(0)} + \\alpha p$. Provide the exact values for the components of $x^{(1)}$; do not round. Express your final answer as a single row vector.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. The provided starting point $x^{(0)}$ is verified to be strictly feasible:\n$A x^{(0)} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{2} + \\frac{3}{2} + 1 = 3 = b$.\nAll components of $x^{(0)}$ are strictly positive. Thus, the problem is valid and we proceed with the solution.\n\nThe primal affine scaling method generates a search direction $p$ at a current strictly feasible point $x$ by considering a scaled version of the problem. As described, we seek the steepest descent direction for the objective $c^{\\top}d$ within the feasible subspace $Ad=0$, measured by the local norm $\\|d\\|_{X^{-1}} = \\sqrt{d^{\\top}X^{-2}d}$, where $X = \\mathrm{diag}(x)$. This is formulated as the optimization problem:\n$$\n\\begin{array}{ll}\n\\text{minimize}  c^{\\top} d \\\\\n\\text{subject to}  Ad = 0 \\\\\n                  d^{\\top}X^{-2}d = \\text{constant}\n\\end{array}\n$$\nThe Lagrangian for this problem is $L(d, y) = c^{\\top}d - y^{\\top}(Ad)$, where we consider only the equality constraint on the direction. The optimality condition with respect to $d$ in the scaled norm implies that the gradient of the objective with respect to the scaled coordinates must be orthogonal to the scaled feasible subspace. The search direction $p$ is given by the projection of the negative scaled gradient onto the null space of the scaled constraint matrix.\n\nLet the scaled variables be $\\tilde{d} = X^{-1} d$ and the scaled data be $\\tilde{A} = AX$ and $\\tilde{c} = Xc$. The search direction in the scaled space, $\\tilde{p}$, is the projection of $-\\tilde{c}$ onto the null space of $\\tilde{A}$:\n$$\n\\tilde{p} = -\\left(I - \\tilde{A}^{\\top}(\\tilde{A}\\tilde{A}^{\\top})^{-1}\\tilde{A}\\right)\\tilde{c}\n$$\nThe search direction in the original space is then $p = X\\tilde{p}$.\n\nAlternatively, and more directly, the first-order optimality conditions for the original LP imply the existence of a dual variable estimate $y$ and a reduced cost vector $s = c - A^{\\top}y$. The affine scaling search direction $p$ is given by $p = -X^2 s$. The dual estimate $y$ (the Lagrange multiplier) is chosen to ensure that the search direction lies in the null space of $A$, i.e., $Ap=0$.\n$A(-X^2(c-A^{\\top}y)) = 0 \\implies -AX^2c + AX^2A^{\\top}y = 0 \\implies y = (AX^2A^{\\top})^{-1}AX^2c$. This matches the problem's request to find the Lagrange multiplier $y$.\n\nWe are given the starting point $x^{(0)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix}$.\nThe scaling matrix $X$ and its square are:\n$$\nX = \\mathrm{diag}(x^{(0)}) = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{3}{2}  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad X^2 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nGiven data: $A = \\begin{pmatrix} 1  1  1 \\end{pmatrix}$ and $c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we compute the components needed for the Lagrange multiplier $y$:\n$$\nAX^2 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix}\n$$\n$$\nAX^2A^{\\top} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{4} + \\frac{9}{4} + 1 = \\frac{10}{4} + 1 = \\frac{5}{2} + 1 = \\frac{7}{2}\n$$\n$$\nAX^2c = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\frac{1}{4} + \\frac{18}{4} + 0 = \\frac{19}{4}\n$$\nNow, we compute the Lagrange multiplier $y$:\n$$\ny = (AX^2A^{\\top})^{-1}AX^2c = \\left(\\frac{7}{2}\\right)^{-1} \\left(\\frac{19}{4}\\right) = \\frac{2}{7} \\cdot \\frac{19}{4} = \\frac{19}{14}\n$$\nNext, we determine the reduced cost vector $s = c - A^{\\top}y$:\n$$\ns = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\left(\\frac{19}{14}\\right) = \\begin{pmatrix} 1 - \\frac{19}{14} \\\\ 2 - \\frac{19}{14} \\\\ 0 - \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{14-19}{14} \\\\ \\frac{28-19}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix}\n$$\nThe search direction $p$ is $p = -X^2 s$:\n$$\np = - \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = - \\begin{pmatrix} -\\frac{5}{56} \\\\ \\frac{81}{56} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix}\n$$\nTo determine the step size $\\alpha$, we use the fraction-to-the-boundary rule, $\\alpha = \\beta \\alpha_{\\max}$. First, we find the maximum step $\\alpha_{\\max}$ that preserves nonnegativity, $x^{(0)} + \\alpha p \\ge 0$. This is limited by components where $p_i  0$.\n$$\n\\alpha_{\\max} = \\min_{i: p_i  0} \\left\\{ \\frac{x_i^{(0)}}{-p_i} \\right\\}\n$$\nIn our case, only $p_2 = -\\frac{81}{56}$ is negative.\n$$\n\\alpha_{\\max} = \\frac{x_2^{(0)}}{-p_2} = \\frac{\\frac{3}{2}}{-(-\\frac{81}{56})} = \\frac{3}{2} \\cdot \\frac{56}{81} = \\frac{3 \\cdot 28}{81} = \\frac{28}{27}\n$$\nThe step size parameter is $\\beta = \\frac{9}{10}$. So the step size is:\n$$\n\\alpha = \\beta \\alpha_{\\max} = \\frac{9}{10} \\cdot \\frac{28}{27} = \\frac{1}{10} \\cdot \\frac{28}{3} = \\frac{28}{30} = \\frac{14}{15}\n$$\nFinally, we compute the new iterate $x^{(1)} = x^{(0)} + \\alpha p$:\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\frac{14}{15} \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{14 \\cdot 5}{15 \\cdot 56} \\\\ \\frac{14 \\cdot (-81)}{15 \\cdot 56} \\\\ \\frac{14 \\cdot 76}{15 \\cdot 56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{15 \\cdot 4} \\\\ \\frac{-81}{15 \\cdot 4} \\\\ \\frac{76}{15 \\cdot 4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{60} \\\\ -\\frac{81}{60} \\\\ \\frac{76}{60} \\end{pmatrix}\n$$\n$$\nx_1^{(1)} = \\frac{1}{2} + \\frac{5}{60} = \\frac{30}{60} + \\frac{5}{60} = \\frac{35}{60} = \\frac{7}{12}\n$$\n$$\nx_2^{(1)} = \\frac{3}{2} - \\frac{81}{60} = \\frac{90}{60} - \\frac{81}{60} = \\frac{9}{60} = \\frac{3}{20}\n$$\n$$\nx_3^{(1)} = 1 + \\frac{76}{60} = \\frac{60}{60} + \\frac{76}{60} = \\frac{136}{60} = \\frac{34}{15}\n$$\nThe updated point is $x^{(1)} = \\begin{pmatrix} \\frac{7}{12} \\\\ \\frac{3}{20} \\\\ \\frac{34}{15} \\end{pmatrix}$.\nAs a check, we confirm $A x^{(1)} = b$:\n$$\n\\frac{7}{12} + \\frac{3}{20} + \\frac{34}{15} = \\frac{35}{60} + \\frac{9}{60} + \\frac{136}{60} = \\frac{35+9+136}{60} = \\frac{180}{60} = 3\n$$\nThe constraint is satisfied.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{12}  \\frac{3}{20}  \\frac{34}{15} \\end{pmatrix}}\n$$", "id": "3095992"}, {"introduction": "Beyond the mechanics, it's crucial to build intuition for the algorithm's trajectory and behavior. This problem explores how the affine scaling direction is shaped by both the cost function and the current iterate's distance from the boundary [@problem_id:3095980]. By analyzing the relative \"shrink rates\" of different variables, you will see precisely how the algorithm prioritizes which variables to reduce, guiding the iterates toward the optimal face of the feasible region.", "problem": "Consider the linear program (LP) in standard form\nminimize $c^{\\top} x$ subject to $A x = b$, $x \\ge 0$,\nwith\n$A = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, and $c = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$.\nSuppose we apply the affine scaling algorithm starting from the strictly feasible point $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$, which satisfies $A x^{(0)} = b$ and $x^{(0)}  0$. The affine scaling step is defined as the steepest descent of $c^{\\top} x$ within the affine hull $\\{x : A x = b\\}$ measured in the local metric induced by the current positive diagonal scaling $D = \\operatorname{diag}(x)$, and then taking a positivity-preserving step along that direction.\n\nSelect all statements that are correct.\n\nA. The unique optimal solution is $x^{\\star} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, which lies on the face defined by $x_2 = 0$ and $x_4 = 0$.\n\nB. At $x^{(0)}$, the affine scaling search direction $d$ decreases $x_2$ twice as fast as $x_4$ in both absolute and relative terms; specifically, $d_2 = -0.25$ and $d_4 = -0.125$, so $|d_2| = 2 |d_4|$ and $(-d_2/x_2)/(-d_4/x_4) = 2$.\n\nC. At $x^{(0)}$, the affine scaling direction is proportional to the negative cost vector, i.e., $d = -\\lambda c$ for some $\\lambda  0$.\n\nD. For the affine scaling direction $d$ at a strictly feasible $x$, the componentwise relative shrink rate per unit step satisfies $-d_i/x_i = x_i p_i$, where $p = c - A^{\\top} y$ is the reduced cost vector induced by the scaling and $y$ enforces feasibility in the affine hull. Consequently, at $x^{(0)}$ where $x_2^{(0)} = x_4^{(0)}$ and $p_2  p_4  0$, coordinate $x_2$ shrinks faster than $x_4$ as the method approaches the optimal face.", "solution": "The user wants me to analyze the first step of the affine scaling algorithm for a given linear program (LP) and starting point, and evaluate several statements about it.\n\n### Step 1: Extract Givens\nThe linear program is given in standard form:\nminimize $c^{\\top} x$\nsubject to $A x = b$, $x \\ge 0$.\n\nThe specific data are:\n-   $A = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix}$\n-   $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $c = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$\n-   The starting point is $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n\nThe problem describes the affine scaling step as the steepest descent of $c^{\\top} x$ in the affine hull $\\{x : A x = b\\}$ measured in a local metric, followed by a positivity-preserving step. The local metric is induced by the scaling $D = \\operatorname{diag}(x)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Groundedness**: The problem describes a well-known interior-point method, the affine scaling algorithm, applied to a standard-form linear program. The concepts are mathematically and algorithmically sound.\n2.  **Well-Posedness**: The LP is well-defined. Let's check the feasibility of the starting point $x^{(0)}$.\n    -   $A x^{(0)} = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 0.5+0.5 \\\\ 0.5+0.5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = b$. The equality constraint is satisfied.\n    -   $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}  0$. The non-negativity constraint is strictly satisfied.\n    Thus, $x^{(0)}$ is a strictly feasible point.\n3.  **Objectivity**: The problem statement uses precise mathematical language and provides objective data.\n\nThe problem is self-contained, consistent, and well-posed. It is based on established principles of optimization theory. Therefore, the problem is **valid**.\n\n### Derivation of the Affine Scaling Direction\n\nThe affine scaling algorithm search direction $d$ at a strictly feasible point $x$ is given by $d = -D^2 p$, where $D = \\operatorname{diag}(x)$ is the scaling matrix and $p = c - A^{\\top} y$ is the reduced cost vector. The dual variable estimate $y$ is computed to ensure that the direction lies in the null space of the scaled constraint matrix, i.e., $(AD)d = 0$. This leads to the formula:\n$$y = (A D^2 A^{\\top})^{-1} A D^2 c$$\nLet's compute these quantities for the given problem at $x^{(0)}$.\n\n1.  **Scaling Matrix**:\n    $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$, so $D = \\operatorname{diag}(0.5, 0.5, 0.5, 0.5) = 0.5 I$.\n    $D^2 = \\operatorname{diag}(0.25, 0.25, 0.25, 0.25) = 0.25 I$.\n\n2.  **Dual Variable Estimate $y$**:\n    Since $D^2 = 0.25 I$, the formula for $y$ simplifies:\n    $y = (A (0.25 I) A^{\\top})^{-1} A (0.25 I) c = (0.25 A A^{\\top})^{-1} (0.25 A c) = (A A^{\\top})^{-1} A c$.\n    $A A^{\\top} = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1^2+1^2  0 \\\\ 0  1^2+1^2 \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}$.\n    $(A A^{\\top})^{-1} = \\begin{bmatrix} 1/2  0 \\\\ 0  1/2 \\end{bmatrix} = 0.5 I$.\n    $A c = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0+2 \\\\ 3+4 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix}$.\n    $y = (A A^{\\top})^{-1} (A c) = \\begin{bmatrix} 0.5  0 \\\\ 0  0.5 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3.5 \\end{bmatrix}$.\n\n3.  **Reduced Cost Vector $p$**:\n    $p = c - A^{\\top} y$.\n    $A^{\\top} y = \\begin{bmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 3.5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 3.5 \\\\ 3.5 \\end{bmatrix}$.\n    $p = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 3.5 \\\\ 3.5 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix}$.\n\n4.  **Search Direction $d$**:\n    $d = -D^2 p = -0.25 I \\begin{bmatrix} -1 \\\\ 1 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix} = -0.25 \\begin{bmatrix} -1 \\\\ 1 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix}$.\n    We can verify that $d$ lies in the null space of $A$:\n    $A d = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix} = \\begin{bmatrix} 0.25 - 0.25 \\\\ 0.125 - 0.125 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\n### Option-by-Option Analysis\n\n**A. The unique optimal solution is $x^{\\star} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, which lies on the face defined by $x_2 = 0$ and $x_4 = 0$.**\n\nTo find the optimal solution, we analyze the LP. The objective is to minimize $c^{\\top} x = 0x_1 + 2x_2 + 3x_3 + 4x_4$.\nThe constraints are:\n1.  $x_1 + x_2 = 1$\n2.  $x_3 + x_4 = 1$\n3.  $x_1, x_2, x_3, x_4 \\ge 0$\n\nFrom the constraints, $x_1 \\ge 0 \\implies 1-x_2 \\ge 0 \\implies x_2 \\le 1$. Similarly, $x_3 \\ge 0 \\implies 1-x_4 \\ge 0 \\implies x_4 \\le 1$.\nWe can express the objective function in terms of $x_2$ and $x_4$:\n$c^{\\top} x = 2x_2 + 3x_3 + 4x_4 = 2x_2 + 3(1-x_4) + 4x_4 = 3 + 2x_2 + x_4$.\nWe need to minimize $f(x_2, x_4) = 3 + 2x_2 + x_4$ subject to $0 \\le x_2 \\le 1$ and $0 \\le x_4 \\le 1$.\nSince the coefficients of $x_2$ and $x_4$ are positive ($+2$ and $+1$), the minimum value is achieved when $x_2$ and $x_4$ are at their minimum possible values, which is $0$.\nSo, the optimal values are $x_2 = 0$ and $x_4 = 0$.\nThis choice uniquely determines the other variables:\n$x_1 = 1 - x_2 = 1 - 0 = 1$.\n$x_3 = 1 - x_4 = 1 - 0 = 1$.\nThe unique optimal solution is $x^{\\star} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\nThis solution satisfies $x_2 = 0$ and $x_4 = 0$, so it lies on the face defined by these equations.\n**Verdict: Correct.**\n\n**B. At $x^{(0)}$, the affine scaling search direction $d$ decreases $x_2$ twice as fast as $x_4$ in both absolute and relative terms; specifically, $d_2 = -0.25$ and $d_4 = -0.125$, so $|d_2| = 2 |d_4|$ and $(-d_2/x_2)/(-d_4/x_4) = 2$.**\n\nFrom our calculation of the search direction, $d = \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix}$.\nSo, $d_2 = -0.25$ and $d_4 = -0.125$. The specific values in the statement are correct.\nLet's check the absolute rates:\n$|d_2| = |-0.25| = 0.25$.\n$|d_4| = |-0.125| = 0.125$.\nThe ratio is $|d_2|/|d_4| = 0.25/0.125 = 2$, which means $|d_2| = 2 |d_4|$. The absolute rate of decrease of $x_2$ is twice that of $x_4$.\nNow let's check the relative rates at $x^{(0)}$, where $x_2^{(0)}=0.5$ and $x_4^{(0)}=0.5$. The relative rate of change for a component $x_i$ is $d_i/x_i$. The relative shrink rate is $-d_i/x_i$.\nRelative shrink rate for $x_2$: $-d_2/x_2^{(0)} = -(-0.25)/0.5 = 0.5$.\nRelative shrink rate for $x_4$: $-d_4/x_4^{(0)} = -(-0.125)/0.5 = 0.25$.\nThe ratio of these relative shrink rates is $(0.5) / (0.25) = 2$. This matches the expression $(-d_2/x_2)/(-d_4/x_4) = 2$.\nThe statement is fully supported by the calculations.\n**Verdict: Correct.**\n\n**C. At $x^{(0)}$, the affine scaling direction is proportional to the negative cost vector, i.e., $d = -\\lambda c$ for some $\\lambda  0$.**\n\nWe have $d = \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix}$ and $c = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$.\nIf $d = -\\lambda c$, then for some $\\lambda  0$ we would have:\n$\\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix} = -\\lambda \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2\\lambda \\\\ -3\\lambda \\\\ -4\\lambda \\end{bmatrix}$.\nComparing the first components gives $0.25 = 0$, which is a contradiction.\nMore fundamentally, the affine scaling direction $d$ must lie in the null space of $A$, meaning $Ad = 0$. Let's check if $-c$ lies in the null space of $A$.\n$A(-c) = -Ac = -\\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix} \\neq 0$.\nSince $-c$ is not in the null space of $A$, the search direction $d$ cannot be proportional to $-c$.\n**Verdict: Incorrect.**\n\n**D. For the affine scaling direction $d$ at a strictly feasible $x$, the componentwise relative shrink rate per unit step satisfies $-d_i/x_i = x_i p_i$, where $p = c - A^{\\top} y$ is the reduced cost vector induced by the scaling and $y$ enforces feasibility in the affine hull. Consequently, at $x^{(0)}$ where $x_2^{(0)} = x_4^{(0)}$ and $p_2  p_4  0$, coordinate $x_2$ shrinks faster than $x_4$ as the method approaches the optimal face.**\n\nLet's first verify the formula. The search direction is $d = -D^2 p$. Component-wise, this is $d_i = -(D^2)_{ii} p_i$. Since $D = \\operatorname{diag}(x_1, \\dots, x_n)$, $D^2 = \\operatorname{diag}(x_1^2, \\dots, x_n^2)$. So, $d_i = -x_i^2 p_i$.\nThe componentwise relative shrink rate is $-d_i/x_i$.\n$-d_i/x_i = -(-x_i^2 p_i) / x_i = x_i p_i$.\nThe formula is correct. The description of $p$ and $y$ is also accurate.\nNow, let's evaluate the consequence at $x^{(0)}$.\nAt $x^{(0)}$, we have $x_2^{(0)} = 0.5$ and $x_4^{(0)} = 0.5$. The condition $x_2^{(0)} = x_4^{(0)}$ holds.\nFrom our calculation of the reduced cost vector $p$, we have $p_2 = 1$ and $p_4 = 0.5$. The condition $p_2  p_4  0$ also holds.\nThe conclusion is that \"coordinate $x_2$ shrinks faster than $x_4$\". Let's compare the shrink rates.\nRelative shrink rate for $x_2$: $-d_2/x_2^{(0)} = x_2^{(0)} p_2 = (0.5)(1) = 0.5$.\nRelative shrink rate for $x_4$: $-d_4/x_4^{(0)} = x_4^{(0)} p_4 = (0.5)(0.5) = 0.25$.\nSince $0.5  0.25$, the relative shrink rate of $x_2$ is greater than that of $x_4$.\nWe can also compare the absolute shrink rates, which are the magnitudes of the components of $d$.\n$|d_2| = |-x_2^2 p_2| = (0.5)^2(1) = 0.25$.\n$|d_4| = |-x_4^2 p_4| = (0.5)^2(0.5) = 0.125$.\nSince $0.25  0.125$, $x_2$ also shrinks faster in absolute terms. The conclusion is correct.\nThe statement correctly provides the general formula for the relative shrink rate and then correctly applies it to the specific initial point to draw a valid conclusion about the behavior of the algorithm.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABD}$$", "id": "3095980"}, {"introduction": "True mastery often comes from implementation, and this practice moves from manual calculation to computational experiment. You will implement the Affine Scaling Algorithm from first principles and use it to investigate how the choice of a starting point—the geometrically special analytic center versus a random interior point—affects convergence [@problem_id:3096001]. This coding exercise solidifies your understanding of the entire algorithmic workflow and provides insight into the practical performance considerations of interior-point methods.", "problem": "Consider the problem class of Linear Programming (LP): minimize a linear objective under equality constraints with strict positivity, stated as: find $x \\in \\mathbb{R}^n$ that solves\n$$\n\\text{minimize } c^\\top x \\quad \\text{subject to } A x = b,\\ \\ x  0,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $c \\in \\mathbb{R}^n$ are given data, and $x  0$ denotes strict componentwise positivity. The Affine Scaling Algorithm (ASA) is an interior method that iteratively updates a strictly positive feasible point to decrease the objective while maintaining equality feasibility, using a descent direction derived from first principles and a step size rule that preserves positivity.\n\nYour task is to design and implement a numerical experiment that compares convergence behavior of the Affine Scaling Algorithm (ASA) when initialized at two different interior points for the same LP: the analytic center versus a random interior point. Use the following foundational bases in your design:\n\n- The LP feasibility set is the intersection of an affine subspace $\\{x \\mid A x = b\\}$ with the open positive orthant $\\{x \\mid x  0\\}$.\n- The analytic center of a polytope with strictly positive variables (under equality constraints) is the maximizer of the barrier function $\\sum_{i=1}^n \\log(x_i)$ over the feasible set. For the special case $A = \\mathbf{1}^\\top$ (a single equality $\\sum_{i=1}^n x_i = 1$), symmetry implies the analytic center is the uniform point with all coordinates equal, i.e., $x_i = 1/n$ for all $i$.\n- A random interior point for the simplex constraint $\\sum_{i=1}^n x_i = 1$ with $x  0$ can be sampled from a Dirichlet distribution with strictly positive parameters.\n\nImplement the standard primal Affine Scaling Algorithm (ASA) from first principles, enforcing the following requirements:\n\n- At each iteration, form a strictly interior, equality-feasible update $x \\leftarrow x + \\alpha d$ with a direction $d$ that respects equality feasibility and a step size $\\alpha \\in (0,1)$ scaled to ensure the updated iterate remains strictly positive.\n- Use a stopping criterion based on a scientifically sound measure of stationarity and progress that is derivable from the LP optimality conditions and the ASA construction. Your criterion should terminate when the iterates are sufficiently close to stationarity or when no significant progress can be made within positivity constraints.\n\nExperimental design:\n\n- For each test case, run ASA twice on the same LP data: once initialized at the analytic center (uniform point) and once at a random interior point (Dirichlet sample with all parameters equal to $1$ and a specified random seed). Record the number of iterations taken to terminate and the final objective value $c^\\top x$ at termination for both initializations.\n\nTest suite:\n\n- Common constraint for all cases: $m = 1$, $A = \\mathbf{1}^\\top \\in \\mathbb{R}^{1 \\times n}$, $b = [1]$, so the feasible set is the standard open simplex $\\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1,\\ x_i  0\\}$.\n- Use the following cases to exercise different behaviors:\n  - Case $1$ (happy path): $n = 4$, $c = [1, 2, 3, 4]$, random seed $42$, step fraction $\\theta = 0.9$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $10000$.\n  - Case $2$ (boundary condition with flat objective): $n = 4$, $c = [1, 1, 1, 1]$, random seed $7$, step fraction $\\theta = 0.9$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $10000$.\n  - Case $3$ (larger dimension and highly nonuniform costs): $n = 8$, $c = [4, 1, 3, 2, 5, 0.5, 7, 0.1]$, random seed $123$, step fraction $\\theta = 0.9$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $10000$.\n\nInitializations per case:\n\n- Analytic center: $x_{\\text{ac}} = \\left[\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right]$.\n- Random interior: $x_{\\text{rand}}$ drawn from a Dirichlet distribution with all parameters equal to $1$ using the specified random seed.\n\nOutput specification:\n\n- For each test case, produce the tuple of results $[\\text{iter}_{\\text{ac}}, \\text{iter}_{\\text{rand}}, \\text{obj}_{\\text{ac}}, \\text{obj}_{\\text{rand}}]$, where $\\text{iter}_{\\text{ac}}$ and $\\text{iter}_{\\text{rand}}$ are integers giving the iteration counts until termination, and $\\text{obj}_{\\text{ac}}$ and $\\text{obj}_{\\text{rand}}$ are floats giving the final objective values $c^\\top x$ at termination.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, with one four-element list per test case, in the order of the test suite above. For example, the format must be of the form\n$$\n\\left[[\\text{iter}_1^{\\text{ac}}, \\text{iter}_1^{\\text{rand}}, \\text{obj}_1^{\\text{ac}}, \\text{obj}_1^{\\text{rand}}],\\ [\\text{iter}_2^{\\text{ac}}, \\text{iter}_2^{\\text{rand}}, \\text{obj}_2^{\\text{ac}}, \\text{obj}_2^{\\text{rand}}],\\ [\\text{iter}_3^{\\text{ac}}, \\text{iter}_3^{\\text{rand}}, \\text{obj}_3^{\\text{ac}}, \\text{obj}_3^{\\text{rand}}]\\right].\n$$\n\nNo physical units are involved, and angles are not applicable. All numerical outputs must be plain floats or integers as specified.", "solution": "The problem requires the implementation and comparison of the Primal Affine Scaling Algorithm (ASA) for a Linear Program (LP) under two different initializations. The validation procedure confirms that the problem is well-posed, scientifically sound, and contains all necessary information for a unique, verifiable solution.\n\nThe LP is formulated as:\n$$\n\\text{minimize } c^\\top x \\quad \\text{subject to } A x = b,\\ \\ x  0\n$$\nwhere $x, c \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^m$. The condition $x  0$ signifies that each component $x_i$ must be strictly positive, making this an interior-point method. The algorithm generates a sequence of strictly feasible points $\\{x_k\\}$ that converge to the optimal solution.\n\nThe core of the Affine Scaling Algorithm is the computation of a descent direction at each strictly feasible iterate $x_k$. Let $X_k = \\text{diag}(x_k)$ be the diagonal matrix whose diagonal entries are the components of $x_k$. The algorithm employs a scaling transformation $y = X_k^{-1}x$, which maps the current iterate $x_k$ to the vector of all ones, $e = [1, \\dots, 1]^\\top$. In this scaled space, the feasibility constraints become $A X_k y = b$ and $y  0$.\n\nThe goal is to find an update $x_{k+1} = x_k + d_k$ that remains feasible and decreases the objective function. In the scaled space, this corresponds to finding a direction $d_y$ such that $y_{k+1} = e + d_y$. For the new point to satisfy the equality constraints, we must have $A X_k (e + d_y) = b$. Since $A X_k e = A x_k = b$, this implies that the search direction in the scaled space must lie in the null space of the scaled constraint matrix, i.e., $A X_k d_y = 0$.\n\nTo find the best descent direction, we seek to minimize the change in the objective function, which in the scaled space is $(c^\\top X_k) d_y$. To ensure the problem is well-defined, we constrain the search direction to a unit sphere, $\\|d_y\\|_2 = 1$. The subproblem at each iteration is thus:\n$$\n\\text{minimize } (X_k c)^\\top d_y \\quad \\text{subject to } A X_k d_y = 0, \\quad \\|d_y\\|_2 = 1\n$$\nThe solution to this subproblem is the negative projection of the scaled cost vector, $c_s = X_k c$, onto the null space of $M = A X_k$. The projection matrix onto this null space is $P = I - M^\\top(M M^\\top)^{-1}M$. The optimal scaled direction is thus $d_y \\propto -P c_s$.\n\nThis leads to the primal affine scaling search direction in the original unscaled space. Let $p_k$ be the estimate of the dual variables, given by:\n$$\np_k = (A X_k^2 A^\\top)^{-1} A X_k^2 c\n$$\nThe dual slack, or reduced cost vector, is $s_k = c - A^\\top p_k$. The search direction $d_k$ is then computed as:\n$$\nd_k = -X_k^2 s_k = -X_k^2(c - A^\\top p_k)\n$$\nThis direction is guaranteed to be a feasible descent direction, as $A d_k = 0$ and $c^\\top d_k = -s_k^\\top X_k^2 s_k = -\\|X_k s_k\\|_2^2 \\le 0$.\n\nFor the specific test cases provided, the constraints are simplified to the standard simplex: $m = 1$, $A = \\mathbf{1}^\\top$, and $b=[1]$. The calculations become:\n- The matrix $A X_k^2 A^\\top$ becomes a scalar: $\\mathbf{1}^\\top X_k^2 \\mathbf{1} = \\sum_{i=1}^n x_{k,i}^2$.\n- The dual variable estimate $p_k$ is also a scalar:\n$$\np_k = \\frac{\\mathbf{1}^\\top X_k^2 c}{\\mathbf{1}^\\top X_k^2 \\mathbf{1}} = \\frac{\\sum_{i=1}^n x_{k,i}^2 c_i}{\\sum_{i=1}^n x_{k,i}^2}\n$$\n- The dual slack is $s_k = c - \\mathbf{1} p_k$.\n- The search direction components are $d_{k,i} = -x_{k,i}^2(c_i - p_k)$.\n\nOnce the direction $d_k$ is found, the next iterate is $x_{k+1} = x_k + \\alpha d_k$. The step size $\\alpha$ must be chosen to ensure strict feasibility, $x_{k+1}  0$. This requires $x_{k,i} + \\alpha d_{k,i}  0$ for all $i$. If $d_{k,i}  0$, we must have $\\alpha  -x_{k,i}/d_{k,i}$. The maximum possible step size is therefore:\n$$\n\\alpha_{\\max} = \\min_{i: d_{k,i}  0} \\left( \\frac{-x_{k,i}}{d_{k,i}} \\right)\n$$\nTo avoid stepping exactly to the boundary, a fraction $\\theta \\in (0,1)$ is used, yielding the final step size $\\alpha = \\theta \\alpha_{\\max}$. For this problem, $\\theta = 0.9$.\n\nThe algorithm must terminate when a suitable stopping criterion is met. A robust criterion, derivable from the Karush-Kuhn-Tucker (KKT) optimality conditions, is based on the stationarity of the iterates. The objective value ceases to change when $c^\\top d_k = -\\|X_k s_k\\|_2^2 = 0$. Since $x_k  0$, this implies $s_k=0$. The search direction $d_k = -X_k^2 s_k$ would also be zero. Therefore, the algorithm can be terminated when the norm of the search direction becomes smaller than a given tolerance $\\varepsilon$:\n$$\n\\|d_k\\|_2  \\varepsilon\n$$\nThis condition signifies that no significant feasible descent can be made, indicating that the current point is near optimal.\n\nThe experiment will execute this algorithm for each test case, starting from two initial points:\n1.  The analytic center of the simplex, $x_{\\text{ac}} = [\\frac{1}{n}, \\dots, \\frac{1}{n}]^\\top$.\n2.  A random interior point, $x_{\\text{rand}}$, sampled from a Dirichlet distribution with parameters $\\alpha_i=1$ for all $i=1, \\dots, n$, using a specified random seed for reproducibility.\n\nThe number of iterations and the final objective value $c^\\top x$ are recorded for both runs in each case, allowing for a comparison of convergence behavior based on the starting point.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef affine_scaling(\n    A: np.ndarray,\n    b: np.ndarray,\n    c: np.ndarray,\n    x0: np.ndarray,\n    theta: float,\n    epsilon: float,\n    max_iter: int,\n):\n    \"\"\"\n    Implements the Primal Affine Scaling Algorithm for Linear Programming.\n\n    Args:\n        A (np.ndarray): Constraint matrix (m x n).\n        b (np.ndarray): Constraint vector (m x 1).\n        c (np.ndarray): Cost vector (n x 1).\n        x0 (np.ndarray): Initial strictly feasible point (n x 1).\n        theta (float): Step size fraction (0  theta  1).\n        epsilon (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: (iteration_count, final_objective_value)\n    \"\"\"\n    x = x0.copy()\n    n = len(c)\n\n    for k in range(max_iter):\n        # 1. Check for positivity\n        if np.any(x = 0):\n            # This should not happen with proper step size control\n            raise ValueError(\"Iterate x is not strictly positive.\")\n\n        # 2. Compute dual variables, slack, and search direction\n        x_sq = x * x\n        \n        # For the general case:\n        # Xk_sq = np.diag(x_sq)\n        # M = A @ Xk_sq @ A.T\n        # p = np.linalg.solve(M, A @ Xk_sq @ c)\n        # s = c - A.T @ p\n        \n        # Specialized for A = 1^T (scalar calculations)\n        p_num = np.dot(x_sq, c)\n        p_den = np.sum(x_sq)\n\n        # Handle case where denominator is zero (can only happen if x=0)\n        if p_den  1e-20:\n             p = 0.0\n        else:\n             p = p_num / p_den\n        \n        s = c - p\n        d = -x_sq * s\n        \n        # 3. Check stopping criterion\n        norm_d = np.linalg.norm(d)\n        if norm_d  epsilon:\n            return k, np.dot(c, x)\n\n        # 4. Compute step size\n        d_neg_indices = np.where(d  0)[0]\n\n        if len(d_neg_indices) == 0:\n            # This implies d >= 0. Since Ad = 0, we have sum(d_i) = 0.\n            # If all d_i >= 0, this means d must be the zero vector.\n            # This case is handled by the stopping criterion norm_d  epsilon.\n            # If we reach here, it's a numerical precision issue.\n            return k, np.dot(c, x)\n\n        alpha_max = np.min(-x[d_neg_indices] / d[d_neg_indices])\n        alpha = theta * alpha_max\n\n        # 5. Update x\n        x = x + alpha * d\n    \n    # Max iterations reached\n    return max_iter, np.dot(c, x)\n\n\ndef solve():\n    \"\"\"\n    Runs the experimental comparison of the Affine Scaling Algorithm\n    for the specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=4, c=[1, 2, 3, 4], seed=42\n        {\n            \"n\": 4,\n            \"c\": np.array([1.0, 2.0, 3.0, 4.0]),\n            \"seed\": 42,\n            \"theta\": 0.9,\n            \"epsilon\": 1e-10,\n            \"max_iter\": 10000,\n        },\n        # Case 2: n=4, c=[1, 1, 1, 1], seed=7\n        {\n            \"n\": 4,\n            \"c\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"seed\": 7,\n            \"theta\": 0.9,\n            \"epsilon\": 1e-10,\n            \"max_iter\": 10000,\n        },\n        # Case 3: n=8, c=[4, 1, 3, 2, 5, 0.5, 7, 0.1], seed=123\n        {\n            \"n\": 8,\n            \"c\": np.array([4.0, 1.0, 3.0, 2.0, 5.0, 0.5, 7.0, 0.1]),\n            \"seed\": 123,\n            \"theta\": 0.9,\n            \"epsilon\": 1e-10,\n            \"max_iter\": 10000,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        c = case[\"c\"]\n        seed = case[\"seed\"]\n        theta = case[\"theta\"]\n        epsilon = case[\"epsilon\"]\n        max_iter = case[\"max_iter\"]\n\n        # Common constraints for all cases\n        A = np.ones((1, n))\n        b = np.array([1.0])\n\n        # Initialization 1: Analytic Center\n        x_ac_init = np.full(n, 1.0 / n)\n        \n        iter_ac, obj_ac = affine_scaling(A, b, c, x_ac_init, theta, epsilon, max_iter)\n\n        # Initialization 2: Random Interior Point from Dirichlet distribution\n        rng = np.random.default_rng(seed)\n        # Using numpy.random.Generator.dirichlet, available in numpy >= 1.17\n        x_rand_init = rng.dirichlet(np.ones(n))\n\n        iter_rand, obj_rand = affine_scaling(A, b, c, x_rand_init, theta, epsilon, max_iter)\n\n        results.append([iter_ac, iter_rand, obj_ac, obj_rand])\n\n    # Final print statement must match the exact specified format.\n    # The str() of a list of lists in Python is very close.\n    # We remove spaces to be certain.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3096001"}]}