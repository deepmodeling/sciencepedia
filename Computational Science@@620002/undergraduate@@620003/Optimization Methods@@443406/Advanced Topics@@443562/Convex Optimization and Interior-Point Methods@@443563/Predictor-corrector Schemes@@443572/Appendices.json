{"hands_on_practices": [{"introduction": "The predictor-corrector concept can be elegantly introduced through the lens of penalty methods for constrained optimization. In this exercise [@problem_id:3163753], we will consider a simple scenario where the \"predictor\" is the unconstrained minimizer of a function, which naively ignores the constraints. The \"corrector\" then adjusts this solution by minimizing a new objective that includes a penalty term for constraint violation, effectively pulling the solution back towards the feasible region. By deriving the magnitude of this adjustment as a function of the penalty parameter $\\rho$, you will gain a concrete understanding of how the penalty strength controls the intensity of the correction.", "problem": "Consider the one-dimensional constrained optimization problem with objective function $f(x) = \\frac{1}{2}(x - 1)^{2}$ and nonlinear inequality constraint $g(x) = x - \\frac{1}{2} \\leq 0$. A predictor-corrector scheme is employed as follows: the predictor step is obtained by solving the unconstrained problem (ignoring any nonbinding constraints), and the corrector step is obtained by minimizing the penalized objective $F_{\\rho}(x) = f(x) + \\rho \\,\\max\\!\\big(0, g(x)\\big)^{2}$, where $\\rho > 0$ is the penalty parameter.\n\nStarting from the fundamental definition of unconstrained minimization (first-order stationarity of the objective) and the definition of the penalty function (with the max operator defining a piecewise-smooth correction), derive the corrected minimizer $x^{c}(\\rho)$ produced by the penalized objective. Define the correction intensity $I(\\rho)$ as the absolute displacement from the predictor to the corrector, that is $I(\\rho) = \\big|x^{p} - x^{c}(\\rho)\\big|$, where $x^{p}$ denotes the predictor. Provide the closed-form analytic expression of $I(\\rho)$ as a function of $\\rho$.\n\nYour final answer must be the single analytic expression for $I(\\rho)$ and must not include any units or additional commentary.", "solution": "The problem requires the derivation of the correction intensity $I(\\rho)$ for a specific predictor-corrector scheme applied to a one-dimensional constrained optimization problem.\n\nThe objective function is $f(x) = \\frac{1}{2}(x - 1)^{2}$ and the inequality constraint is $g(x) = x - \\frac{1}{2} \\leq 0$.\n\nFirst, we determine the predictor solution, $x^{p}$. The problem states that the predictor step is obtained by solving the unconstrained problem. This means we must find the value of $x$ that minimizes $f(x)$ without regard for the constraint $g(x) \\leq 0$. The minimum of the convex function $f(x)$ occurs where its derivative is zero:\n$$f'(x) = x - 1 = 0 \\implies x = 1$$\nThus, the predictor solution is $x^{p} = 1$. This solution violates the constraint, since $g(1) = 1 - 1/2 = 1/2 \\not\\leq 0$.\n\nNext, we determine the corrector solution, $x^{c}(\\rho)$, by minimizing the penalized objective function:\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} + \\rho \\,\\max\\!\\big(0, x - \\frac{1}{2}\\big)^{2}$$\nSince the predictor violates the constraint, the corrector must lie in the region where the penalty is active, i.e., $x > 1/2$. In this region, the function is:\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} + \\rho \\left(x - \\frac{1}{2}\\right)^{2} \\quad \\text{for } x > \\frac{1}{2}$$\nTo find the minimum, we compute its first derivative and set it to zero:\n$$F'_{\\rho}(x) = (x - 1) + 2\\rho \\left(x - \\frac{1}{2}\\right) = (1 + 2\\rho)x - (1 + \\rho)$$\nSetting the derivative to zero gives the stationary point:\n$$(1 + 2\\rho)x = 1 + \\rho \\implies x = \\frac{1 + \\rho}{1 + 2\\rho}$$\nFor $\\rho > 0$, we can verify that this point is indeed in the region $x > 1/2$, as $2(1+\\rho) > 1(1+2\\rho) \\implies 2 > 1$. The second derivative $F''_{\\rho}(x) = 1 + 2\\rho > 0$, confirming this is a minimum. Thus, the corrector solution is:\n$$x^{c}(\\rho) = \\frac{1 + \\rho}{1 + 2\\rho}$$\n\nFinally, we compute the correction intensity $I(\\rho)$ as the absolute displacement from the predictor to the corrector:\n$$I(\\rho) = |x^{p} - x^{c}(\\rho)| = \\left| 1 - \\frac{1 + \\rho}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{(1 + 2\\rho) - (1 + \\rho)}{1 + 2\\rho} \\right| = \\left| \\frac{\\rho}{1 + 2\\rho} \\right|$$\nSince $\\rho > 0$, both the numerator and denominator are positive, so the absolute value is redundant.\n$$I(\\rho) = \\frac{\\rho}{1 + 2\\rho}$$\nThis is the closed-form analytic expression for the correction intensity.", "answer": "$$\n\\boxed{\\frac{\\rho}{1 + 2\\rho}}\n$$", "id": "3163753"}, {"introduction": "Beyond handling static constraints, the predictor-corrector framework provides a powerful perspective for analyzing the dynamics of iterative algorithms. This practice [@problem_id:3163746] reframes the popular momentum optimization method, where the \"predictor\" step updates the velocity and the \"corrector\" step applies this change to update the position. By analyzing this scheme on a simple convex quadratic function, $f(x)=\\frac{1}{2}\\lambda x^{2}$, you will determine the precise conditions on the step size $\\alpha$ that prevent overshooting the minimizer, a crucial aspect of ensuring stable and efficient convergence.", "problem": "Consider the momentum-based predictor-corrector scheme for minimizing a smooth function $f(x)$, defined by the predictor step $v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$ and the corrector step $x_{k+1}=x_{k}+v_{k+1}$, where $\\alpha>0$ is the step size and $\\beta \\geq 0$ is the momentum coefficient. Assume a strictly convex quadratic objective $f(x)=\\frac{1}{2}\\lambda x^{2}$ with curvature parameter $\\lambda>0$, so that the unique minimizer is at $x^{\\star}=0$ and the gradient is $\\nabla f(x)=\\lambda x$. Start from $x_{0}>0$ and $v_{0}=0$. Define an overshoot at a correction step to mean that the iterate crosses the minimizer, i.e., $x_{k+1}<0$ when $x_{k}>0$. Using only the fundamental properties of quadratic objectives and the stated update rules, derive the largest step size $\\alpha^{\\star}(\\beta,\\lambda)$ such that neither the first correction $x_{1}$ nor the second correction $x_{2}$ overshoots (that is, $x_{1}\\geq 0$ and $x_{2}\\geq 0$). Express your final answer as a closed-form analytic expression in terms of $\\beta$ and $\\lambda$. No rounding is required.", "solution": "The update rules for the predictor-corrector scheme are given by $v_{k+1} = \\beta v_{k} - \\alpha \\nabla f(x_{k})$ and $x_{k+1} = x_{k} + v_{k+1}$. For the objective function $f(x)=\\frac{1}{2}\\lambda x^{2}$, the gradient is $\\nabla f(x) = \\lambda x$. Substituting this gives the coupled linear recurrence:\n$v_{k+1} = \\beta v_{k} - \\alpha \\lambda x_{k}$. We will calculate the first two iterates, $x_1$ and $x_2$, starting from the initial conditions $x_0 > 0$ and $v_0 = 0$.\n\n**First Correction Step ($k=0$):**\nWe compute $v_1$ and then $x_1$:\n$$v_1 = \\beta v_0 - \\alpha \\lambda x_0 = \\beta(0) - \\alpha \\lambda x_0 = -\\alpha \\lambda x_0$$\n$$x_1 = x_0 + v_1 = x_0 - \\alpha \\lambda x_0 = (1 - \\alpha \\lambda) x_0$$\nThe first non-overshoot condition is $x_1 \\geq 0$. Since $x_0 > 0$, this requires $1 - \\alpha \\lambda \\geq 0$, which yields the first constraint on the step size:\n$$\\alpha \\leq \\frac{1}{\\lambda}$$\n\n**Second Correction Step ($k=1$):**\nNext, we compute $v_2$ and then $x_2$, using the values of $x_1$ and $v_1$:\n$$v_2 = \\beta v_1 - \\alpha \\lambda x_1 = \\beta(-\\alpha \\lambda x_0) - \\alpha \\lambda ((1 - \\alpha \\lambda) x_0)$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda + (\\alpha \\lambda)^2] x_0 = [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\nNow, we compute $x_2$:\n$$x_2 = x_1 + v_2 = (1 - \\alpha \\lambda) x_0 + [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n$$x_2 = [(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1] x_0$$\nThe second non-overshoot condition is $x_2 \\geq 0$. Since $x_0 > 0$, this requires the polynomial in $\\alpha\\lambda$ to be non-negative:\n$$(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1 \\geq 0$$\nLet $u = \\alpha \\lambda$. The inequality becomes $u^2 - (2+\\beta) u + 1 \\geq 0$. This is an upward-opening parabola in $u$, and the inequality holds when $u$ is outside its roots. The roots of $u^2 - (2+\\beta) u + 1 = 0$ are:\n$$u = \\frac{2+\\beta \\pm \\sqrt{(2+\\beta)^2 - 4}}{2} = \\frac{2+\\beta \\pm \\sqrt{\\beta^2 + 4\\beta}}{2}$$\nLet the two roots be $u_1 \\leq u_2$. The inequality holds if $u \\leq u_1$ or $u \\geq u_2$.\n\n**Combining the Constraints:**\nWe need the largest $\\alpha > 0$ that satisfies both non-overshoot conditions. In terms of $u = \\alpha \\lambda$:\n1.  From $x_1 \\geq 0$: $u \\leq 1$.\n2.  From $x_2 \\geq 0$: $u \\leq u_1$ or $u \\geq u_2$.\nFor $\\beta \\geq 0$, one can verify that $u_1 \\leq 1 \\leq u_2$. Therefore, the intersection of these conditions is $u \\leq u_1$. To find the largest permissible step size $\\alpha^{\\star}$, we take the largest possible value for $u$, which is $u_1$:\n$$\\alpha^{\\star} \\lambda = u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\nSolving for $\\alpha^{\\star}$ gives the final expression:\n$$\\alpha^{\\star} = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2\\lambda} = \\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}$$\nThis is the largest step size that guarantees neither the first nor the second correction step overshoots the minimizer.", "answer": "$$\n\\boxed{\\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}}\n$$", "id": "3163746"}, {"introduction": "For problems with more complex constraints, such as optimizing over the probability simplex $\\Delta$, the design of the corrector step becomes critical. This hands-on problem [@problem_id:3163727] challenges you to implement and compare two distinct correction strategies following a standard gradient prediction step. You will contrast the familiar Euclidean projection with an entropic correction based on the Kullback-Leibler divergence, an idea central to the mirror descent algorithm. This exercise demonstrates that the choice of geometry for the correction step is a key design decision that can significantly influence an algorithm's behavior and performance.", "problem": "Consider the minimization of a smooth convex function over the probability simplex. Let the simplex be defined as $\\Delta = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, \\; x_i \\ge 0\\}$. Let the objective be $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$, where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive semidefinite and $c \\in \\mathbb{R}^n$. The gradient is $\\nabla f(x) = Q x + c$. Starting from the fundamental definitions of the projected gradient method and mirror descent with an entropic proximity function, construct a predictor-corrector scheme for this constrained minimization problem in two variants:\n\n- Variant A (Euclidean projection): Predict with a gradient step and correct by projecting onto the affine and inequality constraints that define $\\Delta$.\n- Variant B (Entropic correction): Predict with a gradient step and correct using an entropic proximity (the Kullback-Leibler divergence) to remain on $\\Delta$.\n\nYour tasks are:\n- Formulate the predictor step based on the unconstrained gradient descent principle.\n- Derive the Euclidean projection corrector onto $\\Delta$ from first principles using necessary optimality conditions.\n- Derive the entropic correction update from first principles by minimizing a linearized objective regularized by the Kullback-Leibler divergence.\n- Implement both variants as iterative schemes with a fixed step size $\\alpha$ and a fixed number of iterations $T$.\n- For each test case, run both variants from the same initial point in $\\Delta$ for $T$ steps and compare their final objective values.\n\nThe final program must:\n- Implement both schemes faithfully according to the derivations.\n- For each test case, compute a boolean output indicating whether Variant B achieves a strictly lower final objective value than Variant A.\n\nUse the following test suite, where each case specifies $(n, Q, c, \\alpha, T, x^{(0)})$:\n1. Happy path: $n=3$, $Q=\\begin{bmatrix}2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2\\end{bmatrix}$, $c=\\begin{bmatrix}0.5 \\\\ -0.2 \\\\ 0.1\\end{bmatrix}$, $\\alpha=0.2$, $T=200$, $x^{(0)}=\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3}\\end{bmatrix}$.\n2. Boundary start: $n=2$, $Q=\\begin{bmatrix}1 & 0.2 \\\\ 0.2 & 1\\end{bmatrix}$, $c=\\begin{bmatrix}0.0 \\\\ 0.4\\end{bmatrix}$, $\\alpha=0.5$, $T=100$, $x^{(0)}=\\begin{bmatrix}1.0 \\\\ 0.0\\end{bmatrix}$.\n3. Large step size: $n=4$, $Q=\\operatorname{diag}(1.5, 2.0, 0.7, 0.3)$, $c=\\begin{bmatrix}-0.3 \\\\ 0.1 \\\\ 0.0 \\\\ 0.2\\end{bmatrix}$, $\\alpha=1.0$, $T=60$, $x^{(0)}=\\begin{bmatrix}0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25\\end{bmatrix}$.\n4. Flat quadratic (linear objective): $n=3$, $Q=\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $c=\\begin{bmatrix}0.1 \\\\ -0.05 \\\\ 0.0\\end{bmatrix}$, $\\alpha=0.3$, $T=100$, $x^{(0)}=\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3}\\end{bmatrix}$.\n5. Higher dimension: $n=5$, $Q=\\operatorname{diag}(0.5, 1.0, 1.5, 0.8, 1.2)$, $c=\\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.05 \\\\ 0.0 \\\\ -0.2\\end{bmatrix}$, $\\alpha=0.15$, $T=150$, $x^{(0)}=\\begin{bmatrix}0.5 \\\\ 0.2 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1\\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4,result_5]$), where each $result_i$ is a boolean for test case $i$ indicating whether Variant B is strictly better than Variant A (i.e., whether its final objective value is strictly lower).", "solution": "The problem asks for the formulation, implementation, and comparison of two predictor-corrector schemes for minimizing a smooth convex quadratic function $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$ over the probability simplex $\\Delta = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, \\; x_i \\ge 0\\}$.\n\nThe general iterative structure is:\n1.  **Predictor Step**: Take a step from the current iterate $x^{(k)}$ based on the negative gradient, yielding a candidate point $y^{(k+1)} = x^{(k)} - \\alpha \\nabla f(x^{(k)})$.\n2.  **Corrector Step**: Adjust $y^{(k+1)}$ to produce the next iterate $x^{(k+1)}$ that satisfies the simplex constraints.\n\n**Variant A: Corrector via Euclidean Projection**\nThis variant, the Projected Gradient Descent method, corrects the predicted point $y^{(k+1)}$ by finding its closest point in $\\Delta$ with respect to the Euclidean distance. This correction is the solution to the quadratic subproblem:\n$$\nx^{(k+1)} = \\operatorname{proj}_{\\Delta}(y^{(k+1)}) = \\arg\\min_{z \\in \\Delta} \\frac{1}{2} \\|z - y^{(k+1)}\\|_2^2\n$$\nThe solution to this projection problem can be found efficiently. It involves solving for a single Lagrange multiplier $\\nu$ from the scalar equation $\\sum_{i=1}^n \\max(0, y_i^{(k+1)} - \\nu) = 1$, which can be done via a sorting-based algorithm. Once $\\nu$ is found, the projection is computed as $x_i^{(k+1)} = \\max(0, y_i^{(k+1)} - \\nu)$.\n\n**Variant B: Corrector via Entropic Regularization**\nThis variant, an instance of the Mirror Descent algorithm, combines the predictor and corrector into a single update. It finds the next iterate $x^{(k+1)}$ by minimizing a linear approximation of the function at $x^{(k)}$, regularized by the Kullback-Leibler (KL) divergence, $D_{KL}(z \\| x^{(k)}) = \\sum_{i=1}^n z_i \\log\\left(\\frac{z_i}{x_i^{(k)}}\\right)$. The update rule is:\n$$\nx^{(k+1)} = \\arg\\min_{z \\in \\Delta} \\left\\{ \\langle \\nabla f(x^{(k)}), z \\rangle + \\frac{1}{\\alpha} D_{KL}(z \\| x^{(k)}) \\right\\}\n$$\nThis minimization problem has a closed-form solution, known as the Exponentiated Gradient update:\n$$\nx_i^{(k+1)} = \\frac{x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))}{\\sum_{j=1}^n x_j^{(k)} \\exp(-\\alpha \\nabla_j f(x^{(k)}))}\n$$\nThis update is multiplicative and inherently keeps the iterates within the probability simplex.\n\n**Implementation Summary**\n-   **Variant A (Projected Gradient Descent)**: At each step, it takes a standard Euclidean gradient step and then projects the result back onto the simplex.\n-   **Variant B (Mirror Descent/Exponentiated Gradient)**: It uses a non-Euclidean geometry induced by the entropy function. The update is multiplicative and computationally simpler than Euclidean projection.\n\nThe implementation will execute both algorithms for a fixed number of iterations $T$ and compare the final objective function values.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path\n        {'n': 3, 'Q': np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]]), \n         'c': np.array([0.5, -0.2, 0.1]), 'alpha': 0.2, 'T': 200, \n         'x0': np.array([1/3, 1/3, 1/3])},\n        # Case 2: Boundary start\n        {'n': 2, 'Q': np.array([[1, 0.2], [0.2, 1]]), \n         'c': np.array([0.0, 0.4]), 'alpha': 0.5, 'T': 100, \n         'x0': np.array([1.0, 0.0])},\n        # Case 3: Large step size\n        {'n': 4, 'Q': np.diag([1.5, 2.0, 0.7, 0.3]), \n         'c': np.array([-0.3, 0.1, 0.0, 0.2]), 'alpha': 1.0, 'T': 60, \n         'x0': np.array([0.25, 0.25, 0.25, 0.25])},\n        # Case 4: Flat quadratic (linear objective)\n        {'n': 3, 'Q': np.zeros((3, 3)), \n         'c': np.array([0.1, -0.05, 0.0]), 'alpha': 0.3, 'T': 100, \n         'x0': np.array([1/3, 1/3, 1/3])},\n        # Case 5: Higher dimension\n        {'n': 5, 'Q': np.diag([0.5, 1.0, 1.5, 0.8, 1.2]), \n         'c': np.array([0.2, -0.1, 0.05, 0.0, -0.2]), 'alpha': 0.15, 'T': 150, \n         'x0': np.array([0.5, 0.2, 0.1, 0.1, 0.1])}\n    ]\n\n    results = []\n    for case in test_cases:\n        Q, c, alpha, T, x0 = case['Q'], case['c'], case['alpha'], case['T'], case['x0']\n\n        # Run Variant A\n        x_final_A = run_variant_A(Q, c, alpha, T, x0)\n        \n        # Run Variant B\n        x_final_B = run_variant_B(Q, c, alpha, T, x0)\n\n        # Calculate final objective values\n        f_A = objective_function(x_final_A, Q, c)\n        f_B = objective_function(x_final_B, Q, c)\n        \n        # Compare and store the boolean result\n        results.append(f_B  f_A)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef objective_function(x, Q, c):\n    \"\"\"Computes the objective function f(x) = 0.5 * x.T @ Q @ x + c.T @ x.\"\"\"\n    return 0.5 * x.T @ Q @ x + c.T @ x\n\ndef gradient(x, Q, c):\n    \"\"\"Computes the gradient of the objective function.\"\"\"\n    return Q @ x + c\n\ndef project_onto_simplex(y):\n    \"\"\"Projects a vector y onto the probability simplex.\"\"\"\n    n = len(y)\n    y_sorted = np.sort(y)[::-1]\n    y_cumsum = np.cumsum(y_sorted)\n    \n    # Find rho: the number of positive components in the projection\n    candidates = (y_cumsum - 1) / np.arange(1, n + 1)\n    rho_idx = np.where(y_sorted > candidates)[0]\n    \n    if len(rho_idx) == 0:\n        rho = n\n    else:    \n        rho = rho_idx[-1] + 1\n    \n    theta = (y_cumsum[rho - 1] - 1) / rho\n    \n    x_proj = np.maximum(0, y - theta)\n    return x_proj\n\ndef run_variant_A(Q, c, alpha, T, x0):\n    \"\"\"Implements Variant A: Predictor-corrector with Euclidean projection.\"\"\"\n    x = np.copy(x0)\n    for _ in range(T):\n        grad = gradient(x, Q, c)\n        y = x - alpha * grad\n        x = project_onto_simplex(y)\n    return x\n\ndef run_variant_B(Q, c, alpha, T, x0):\n    \"\"\"Implements Variant B: Predictor-corrector with entropic correction.\"\"\"\n    x = np.copy(x0)\n    for _ in range(T):\n        grad = gradient(x, Q, c)\n        # Using a guard for exp to prevent overflow/underflow on extreme grad values.\n        # Clip the argument of exp to a reasonable range.\n        exp_arg = -alpha * grad\n        exp_arg = np.clip(exp_arg, -700, 700)\n        x_unnormalized = x * np.exp(exp_arg)\n        # Normalize to stay on the simplex\n        x = x_unnormalized / np.sum(x_unnormalized)\n    return x\n\nsolve()\n```", "id": "3163727"}]}