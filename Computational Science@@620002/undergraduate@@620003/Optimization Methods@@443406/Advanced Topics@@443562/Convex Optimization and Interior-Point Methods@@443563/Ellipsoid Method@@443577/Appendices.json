{"hands_on_practices": [{"introduction": "The power of the ellipsoid method hinges on a \"separation oracle,\" a routine that, given a point outside the feasible set, finds a hyperplane separating the two. This first practice grounds this abstract concept in a concrete application of calculus, showing how the gradient of a convex function naturally provides the separating hyperplane needed to make a cut [@problem_id:3125337]. Understanding this principle is the first step to mastering the algorithm's mechanics.", "problem": "Consider applying the ellipsoid method to a feasibility problem in $\\mathbb{R}^{2}$, where the feasible region is the convex set defined by the quadratic constraint $x^{\\top} Q x \\leq 1$. Let the symmetric, positive-definite matrix be\n$$\nQ = \\begin{pmatrix}\n2  1 \\\\\n1  3\n\\end{pmatrix},\n$$\nand suppose the current query point is $c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, which is found to violate the constraint.\n\nStarting from the fundamental definition of convex differentiable functions and the first-order characterization of convexity (supporting hyperplanes via the gradient and the subgradient inequality), derive a valid separating halfspace for the feasible set at $c$. Specifically:\n- Treat $f(x) = x^{\\top} Q x - 1$ as the convex function defining the constraint boundary.\n- Use first principles to obtain the cut of the form $g^{\\top} x \\leq \\beta$ that separates the feasible region from $c$, where $g$ is the cut normal derived from the gradient at $c$.\n- Compute the explicit values of $g$ and $\\beta$ for the given $Q$ and $c$.\n\nReport the cut normal components and the scalar bound $\\beta$ together as a single row vector $\\begin{pmatrix} g_{1}  g_{2}  \\beta \\end{pmatrix}$. No rounding is required.", "solution": "We begin with the constraint $x^{\\top} Q x \\leq 1$ and define the function\n$$\nf(x) = x^{\\top} Q x - 1.\n$$\nBecause $Q$ is symmetric and positive-definite, the quadratic form $x^{\\top} Q x$ is strictly convex. Therefore, $f(x)$ is a convex differentiable function, and its gradient exists everywhere. The fundamental first-order characterization of convex differentiable functions states that for any $x, y \\in \\mathbb{R}^{n}$,\n$$\nf(y) \\geq f(x) + \\nabla f(x)^{\\top} (y - x).\n$$\nFor any feasible point $y$ (i.e., any $y$ such that $f(y) \\leq 0$), substituting $x = c$ yields\n$$\n0 \\geq f(y) \\geq f(c) + \\nabla f(c)^{\\top} (y - c).\n$$\nRearranging gives the separating inequality\n$$\n\\nabla f(c)^{\\top} y \\leq \\nabla f(c)^{\\top} c - f(c).\n$$\nThus a valid separating halfspace for the feasible set is\n$$\ng^{\\top} x \\leq \\beta \\quad \\text{with} \\quad g = \\nabla f(c), \\quad \\beta = g^{\\top} c - f(c).\n$$\n\nWe now compute $\\nabla f(x)$. Since $f(x) = x^{\\top} Q x - 1$ with $Q$ symmetric,\n$$\n\\nabla f(x) = (Q + Q^{\\top}) x = 2 Q x.\n$$\nEvaluating at $c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n$$\nQ c = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} 2 \\cdot 1 + 1 \\cdot 1 \\\\ 1 \\cdot 1 + 3 \\cdot 1 \\end{pmatrix}\n= \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}.\n$$\nTherefore,\n$$\ng = \\nabla f(c) = 2 Q c = 2 \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 8 \\end{pmatrix}.\n$$\n\nNext, compute $f(c)$:\n$$\nf(c) = c^{\\top} Q c - 1 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - 1.\n$$\nFirst, compute $Q c = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, then\n$$\nc^{\\top} Q c = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = 1 \\cdot 3 + 1 \\cdot 4 = 7,\n$$\nso\n$$\nf(c) = 7 - 1 = 6.\n$$\n\nFinally, compute $\\beta$:\n$$\n\\beta = g^{\\top} c - f(c) = \\begin{pmatrix} 6  8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - 6 = (6 \\cdot 1 + 8 \\cdot 1) - 6 = 14 - 6 = 8.\n$$\n\nTherefore, the separating halfspace is\n$$\n\\begin{pmatrix} 6  8 \\end{pmatrix}^{\\top} x \\leq 8,\n$$\nand the requested row vector of cut parameters is\n$$\n\\begin{pmatrix} 6  8  8 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 6  8  8 \\end{pmatrix}}$$", "id": "3125337"}, {"introduction": "Once a separating hyperplane is found, the ellipsoid method executes its main step: computing a new, smaller ellipsoid that contains the \"correct\" half of the old one. This exercise provides a hands-on walkthrough of the complete update formulas for the ellipsoid's center and shape matrix over two full iterations [@problem_id:3125375]. By performing these calculations manually, you will gain a tangible understanding of how the ellipsoid geometrically shrinks and repositions itself to find the feasible region.", "problem": "Consider the following feasibility instance of a linear program (LP) in $\\mathbb{R}^{3}$:\nFind $x \\in \\mathbb{R}^{3}$ such that\n- $x_{1} \\ge 0$, $x_{2} \\ge 0$, $x_{3} \\ge 0$,\n- $x_{1} + x_{2} + x_{3} \\le 1$.\n\nLet the initial ellipsoid be\n$$\n\\mathcal{E}_{0} = \\{\\, x \\in \\mathbb{R}^{3} : (x - c_{0})^{\\mathsf{T}} P_{0}^{-1} (x - c_{0}) \\le 1 \\,\\},\n$$\nwith center $c_{0} = (1, 1, 1)^{\\mathsf{T}}$ and shape matrix $P_{0} = 4 I_{3}$, where $I_{3}$ is the $3 \\times 3$ identity matrix. You will run two iterations of the classical central-cut ellipsoid method using the violated inequality as the separating hyperplane at each iteration. At each iteration, when a violated inequality $a^{\\mathsf{T}} x \\le b$ is found at the current center $c$, use the central cut $a^{\\mathsf{T}} x \\le a^{\\mathsf{T}} c$ to update the ellipsoid.\n\nTasks:\n1. At iteration $1$, identify a violated constraint at $c_{0}$, form the corresponding central cut, and compute the updated center $c_{1}$ and shape matrix $P_{1}$ of the next ellipsoid $\\mathcal{E}_{1}$. Explicitly show your intermediate computations.\n2. At iteration $2$, repeat the same process: using $c_{1}$ and $P_{1}$, identify a violated constraint at $c_{1}$, form the central cut, and compute the updated center $c_{2}$ and shape matrix $P_{2}$ of $\\mathcal{E}_{2}$.\n3. After each iteration, justify that the true feasible region is still contained in the updated ellipsoid by using only the definition of the central cut and set-containment reasoning.\n4. Finally, compute the exact value of the ratio $\\det(P_{2}) / \\det(P_{0})$. Provide your final answer as a single exact expression. Do not round.", "solution": "The problem is set in dimension $n=3$. The feasible region $S$ is defined by the linear inequalities:\n$$S = \\{ x \\in \\mathbb{R}^{3} \\mid x_1 \\ge 0, x_2 \\ge 0, x_3 \\ge 0, x_1+x_2+x_3 \\le 1 \\}$$\nThese can be written in the matrix form $a_i^{\\mathsf{T}}x \\le b_i$:\n1. $-x_1 \\le 0$\n2. $-x_2 \\le 0$\n3. $-x_3 \\le 0$\n4. $x_1+x_2+x_3 \\le 1$\n\nThe initial ellipsoid is $\\mathcal{E}_0 = \\{ x \\in \\mathbb{R}^{3} : (x - c_0)^{\\mathsf{T}} P_0^{-1} (x - c_0) \\le 1 \\}$, with center $c_0 = (1, 1, 1)^{\\mathsf{T}}$ and shape matrix $P_0 = 4I_3 = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix}$.\n\nThe update formulas for the central-cut ellipsoid method for dimension $n=3$ are:\nThe next center $c_{k+1}$ is given by:\n$$c_{k+1} = c_k - \\frac{1}{n+1} \\frac{P_k g_k}{\\sqrt{g_k^{\\mathsf{T}} P_k g_k}} = c_k - \\frac{1}{4} \\frac{P_k g_k}{\\sqrt{g_k^{\\mathsf{T}} P_k g_k}}$$\nThe next shape matrix $P_{k+1}$ is given by:\n$$P_{k+1} = \\frac{n^2}{n^2-1} \\left( P_k - \\frac{2}{n+1} \\frac{(P_k g_k)(P_k g_k)^{\\mathsf{T}}}{g_k^{\\mathsf{T}} P_k g_k} \\right) = \\frac{9}{8} \\left( P_k - \\frac{1}{2} \\frac{(P_k g_k)(P_k g_k)^{\\mathsf{T}}}{g_k^{\\mathsf{T}} P_k g_k} \\right)$$\nwhere $g_k$ is the normal vector of the violated constraint $a^{\\mathsf{T}}x \\le b$, so $g_k=a$.\n\n**Task 1: Iteration 1**\n\nFirst, we check which constraint is violated by the center $c_0 = (1, 1, 1)^{\\mathsf{T}}$.\n1. $-x_1 \\le 0 \\implies -1 \\le 0$. (Satisfied)\n2. $-x_2 \\le 0 \\implies -1 \\le 0$. (Satisfied)\n3. $-x_3 \\le 0 \\implies -1 \\le 0$. (Satisfied)\n4. $x_1+x_2+x_3 \\le 1 \\implies 1+1+1 = 3 \\le 1$. (Violated)\n\nThe violated constraint is $x_1+x_2+x_3 \\le 1$. The normal vector is $g_0 = (1, 1, 1)^{\\mathsf{T}}$.\nNow we compute the terms needed for the update.\n$$P_0 g_0 = (4I_3) g_0 = 4g_0 = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix}$$\n$$g_0^{\\mathsf{T}} P_0 g_0 = g_0^{\\mathsf{T}} (4g_0) = 4 (g_0^{\\mathsf{T}} g_0) = 4(1^2+1^2+1^2) = 12$$\n$$\\sqrt{g_0^{\\mathsf{T}} P_0 g_0} = \\sqrt{12} = 2\\sqrt{3}$$\n\nNow, we compute the updated center $c_1$:\n$$c_1 = c_0 - \\frac{1}{4} \\frac{P_0 g_0}{\\sqrt{g_0^{\\mathsf{T}} P_0 g_0}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{4} \\frac{\\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix}}{2\\sqrt{3}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{2\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\left(1 - \\frac{\\sqrt{3}}{6}\\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nSo, $c_1 = \\left( \\frac{6-\\sqrt{3}}{6}, \\frac{6-\\sqrt{3}}{6}, \\frac{6-\\sqrt{3}}{6} \\right)^{\\mathsf{T}}$.\n\nNext, we compute the updated shape matrix $P_1$:\n$$(P_0 g_0)(P_0 g_0)^{\\mathsf{T}} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix} \\begin{pmatrix} 4  4  4 \\end{pmatrix} = 16 \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}$$\n$$P_1 = \\frac{9}{8} \\left( 4I_3 - \\frac{1}{2} \\frac{16 \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}}{12} \\right) = \\frac{9}{8} \\left( 4I_3 - \\frac{2}{3} \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} \\right)$$\n$$P_1 = \\frac{9}{8} \\begin{pmatrix} 4-\\frac{2}{3}  -\\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  4-\\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  -\\frac{2}{3}  4-\\frac{2}{3} \\end{pmatrix} = \\frac{9}{8} \\begin{pmatrix} \\frac{10}{3}  -\\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{10}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  -\\frac{2}{3}  \\frac{10}{3} \\end{pmatrix} = \\frac{9}{8} \\frac{2}{3} \\begin{pmatrix} 5  -1  -1 \\\\ -1  5  -1 \\\\ -1  -1  5 \\end{pmatrix}$$\n$$P_1 = \\frac{3}{4} \\begin{pmatrix} 5  -1  -1 \\\\ -1  5  -1 \\\\ -1  -1  5 \\end{pmatrix}$$\n\n**Task 2: Iteration 2**\n\nWe check which constraint is violated by $c_1 = (\\alpha, \\alpha, \\alpha)^{\\mathsf{T}}$, where $\\alpha = 1 - \\frac{\\sqrt{3}}{6}$.\nSince $\\sqrt{3}  6$, we have $\\alpha  0$. The first three constraints $-x_i \\le 0$ are satisfied as $-\\alpha  0$.\nFor the fourth constraint:\n$$x_1+x_2+x_3 = 3\\alpha = 3\\left(1 - \\frac{\\sqrt{3}}{6}\\right) = 3 - \\frac{\\sqrt{3}}{2}$$\nWe check if $3 - \\frac{\\sqrt{3}}{2} \\le 1$. This is equivalent to $2 \\le \\frac{\\sqrt{3}}{2}$, or $4 \\le \\sqrt{3}$, which is false since $16 \\le 3$ is false. Thus, the constraint $x_1+x_2+x_3 \\le 1$ is violated.\nThe normal vector is $g_1 = (1, 1, 1)^{\\mathsf{T}}$.\nWe compute the terms for the update:\n$$P_1 g_1 = \\frac{3}{4} \\begin{pmatrix} 5  -1  -1 \\\\ -1  5  -1 \\\\ -1  -1  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{3}{4} \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} = \\frac{9}{4} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n$$g_1^{\\mathsf{T}} P_1 g_1 = g_1^{\\mathsf{T}} (P_1 g_1) = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\frac{9}{4} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{9}{4}(1+1+1) = \\frac{27}{4}$$\n$$\\sqrt{g_1^{\\mathsf{T}} P_1 g_1} = \\sqrt{\\frac{27}{4}} = \\frac{3\\sqrt{3}}{2}$$\n\nNow, we compute the updated center $c_2$:\n$$c_2 = c_1 - \\frac{1}{4} \\frac{P_1 g_1}{\\sqrt{g_1^{\\mathsf{T}} P_1 g_1}} = \\left(1 - \\frac{\\sqrt{3}}{6}\\right)\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{4} \\frac{\\frac{9}{4}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}}{\\frac{3\\sqrt{3}}{2}}$$\n$$ \\frac{\\frac{9}{4}}{\\frac{3\\sqrt{3}}{2}} = \\frac{9}{4} \\cdot \\frac{2}{3\\sqrt{3}} = \\frac{3}{2\\sqrt{3}} = \\frac{\\sqrt{3}}{2} $$\n$$c_2 = \\left(1 - \\frac{\\sqrt{3}}{6}\\right)\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{4} \\left(\\frac{\\sqrt{3}}{2}\\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\left(1 - \\frac{\\sqrt{3}}{6} - \\frac{\\sqrt{3}}{8}\\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n$$c_2 = \\left(1 - \\left(\\frac{4\\sqrt{3}+3\\sqrt{3}}{24}\\right)\\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\left(1 - \\frac{7\\sqrt{3}}{24}\\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n\nFinally, we compute the updated shape matrix $P_2$:\n$$(P_1 g_1)(P_1 g_1)^{\\mathsf{T}} = \\left(\\frac{9}{4}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\right) \\left(\\frac{9}{4}\\begin{pmatrix} 1  1  1 \\end{pmatrix}\\right) = \\frac{81}{16} \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}$$\n$$P_2 = \\frac{9}{8} \\left( P_1 - \\frac{1}{2} \\frac{\\frac{81}{16} \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}}{\\frac{27}{4}} \\right) = \\frac{9}{8} \\left( P_1 - \\frac{1}{2} \\left(\\frac{3}{4}\\right) \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} \\right)$$\n$$P_2 = \\frac{9}{8} \\left( \\frac{3}{4} \\begin{pmatrix} 5  -1  -1 \\\\ -1  5  -1 \\\\ -1  -1  5 \\end{pmatrix} - \\frac{3}{8} \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} \\right)$$\n$$P_2 = \\frac{9}{8} \\frac{3}{8} \\left( 2 \\begin{pmatrix} 5  -1  -1 \\\\ -1  5  -1 \\\\ -1  -1  5 \\end{pmatrix} - \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} \\right) = \\frac{27}{64} \\begin{pmatrix} 9  -3  -3 \\\\ -3  9  -3 \\\\ -3  -3  9 \\end{pmatrix}$$\n$$P_2 = \\frac{81}{64} \\begin{pmatrix} 3  -1  -1 \\\\ -1  3  -1 \\\\ -1  -1  3 \\end{pmatrix}$$\n\n**Task 3: Justification of Feasible Region Containment**\n\nLet $S$ be the feasible region and $\\mathcal{E}_k$ be the ellipsoid at iteration $k$. By assumption, $S \\subseteq \\mathcal{E}_0$. The ellipsoid method maintains the invariant $S \\subseteq \\mathcal{E}_k$ for all $k \\ge 0$.\nAt iteration $k$, we have a center $c_k$ and an ellipsoid $\\mathcal{E}_k$ such that $S \\subseteq \\mathcal{E}_k$. If $c_k \\notin S$, a violated constraint $a^{\\mathsf{T}}x \\le b$ is found, meaning $a^{\\mathsf{T}}c_k  b$.\nFor any point $x \\in S$, it must satisfy this constraint, so $a^{\\mathsf{T}}x \\le b$.\nCombining these, we get $a^{\\mathsf{T}}x \\le b  a^{\\mathsf{T}}c_k$ for all $x \\in S$. This implies that the entire feasible region $S$ lies in the half-space $\\{x \\in \\mathbb{R}^n : a^{\\mathsf{T}}x \\le a^{\\mathsf{T}}c_k \\}$.\nThe next ellipsoid $\\mathcal{E}_{k+1}$ is constructed to contain the half-ellipsoid $\\mathcal{E}_k' = \\mathcal{E}_k \\cap \\{x \\in \\mathbb{R}^n : a^{\\mathsf{T}}x \\le a^{\\mathsf{T}}c_k \\}$.\nSince $S \\subseteq \\mathcal{E}_k$ and $S$ is also in the cutting half-space, we have $S \\subseteq \\mathcal{E}_k'$.\nBy construction of the method, $\\mathcal{E}_k' \\subseteq \\mathcal{E}_{k+1}$.\nTherefore, by transitivity of set containment, $S \\subseteq \\mathcal{E}_{k+1}$.\nThis logic holds for both iterations ($k=0$ and $k=1$), ensuring that the feasible region remains within the updated ellipsoids $\\mathcal{E}_1$ and $\\mathcal{E}_2$.\n\n**Task 4: Ratio of Determinants**\n\nThe ratio of the determinants of consecutive shape matrices for a central cut in dimension $n$ is given by the formula:\n$$\\frac{\\det(P_{k+1})}{\\det(P_k)} = \\left(\\frac{n}{n+1}\\right)^2 \\left(\\frac{n^2}{n^2-1}\\right)^{n-1}$$\nFor $n=3$, this ratio is:\n$$\\frac{\\det(P_{k+1})}{\\det(P_k)} = \\left(\\frac{3}{4}\\right)^2 \\left(\\frac{3^2}{3^2-1}\\right)^{3-1} = \\frac{9}{16} \\left(\\frac{9}{8}\\right)^2 = \\frac{9}{16} \\frac{81}{64} = \\frac{729}{1024}$$\nThis ratio is constant for each iteration since both steps use a central cut. We need to find the ratio $\\frac{\\det(P_2)}{\\det(P_0)}$.\n$$\\frac{\\det(P_2)}{\\det(P_0)} = \\frac{\\det(P_2)}{\\det(P_1)} \\cdot \\frac{\\det(P_1)}{\\det(P_0)}$$\nSince both iteration ratios are $\\frac{729}{1024}$, we have:\n$$\\frac{\\det(P_2)}{\\det(P_0)} = \\left( \\frac{729}{1024} \\right)^2 = \\frac{729^2}{1024^2} = \\frac{531441}{1048576}$$\nThis can also be expressed using prime factorizations: $729 = 3^6$ and $1024 = 2^{10}$.\n$$\\frac{\\det(P_2)}{\\det(P_0)} = \\left( \\frac{3^6}{2^{10}} \\right)^2 = \\frac{3^{12}}{2^{20}}$$\nWe can also verify this directly from our computed matrices.\n$\\det(P_0) = \\det(4I_3) = 4^3=64$.\n$\\det(P_2) = \\det\\left(\\frac{81}{64} \\begin{pmatrix} 3  -1  -1 \\\\ -1  3  -1 \\\\ -1  -1  3 \\end{pmatrix}\\right) = \\left(\\frac{81}{64}\\right)^3 \\det\\begin{pmatrix} 3  -1  -1 \\\\ -1  3  -1 \\\\ -1  -1  3 \\end{pmatrix}$.\nThe determinant of the matrix is $3(9-1) - (-1)(-3-1) + (-1)(1+3) = 24 - 4 - 4 = 16$.\n$\\det(P_2) = \\left(\\frac{81}{64}\\right)^3 \\cdot 16 = \\frac{(3^4)^3}{(2^6)^3} \\cdot 2^4 = \\frac{3^{12}}{2^{18}} \\cdot 2^4 = \\frac{3^{12}}{2^{14}}$.\nThe ratio is $\\frac{\\det(P_2)}{\\det(P_0)} = \\frac{3^{12}/2^{14}}{64} = \\frac{3^{12}/2^{14}}{2^6} = \\frac{3^{12}}{2^{20}}$.\nThe exact expression is $\\frac{531441}{1048576}$.", "answer": "$$\\boxed{\\frac{531441}{1048576}}$$", "id": "3125375"}, {"introduction": "While any valid separating hyperplane guarantees convergence, the *choice* of cut can dramatically affect practical performance. This advanced practice explores the strategic aspect of the ellipsoid method by comparing cuts aligned with an ellipsoid's longest and shortest axes [@problem_id:3125298]. By analyzing the impact on the ellipsoid's condition number, you will uncover a crucial insight: cuts that make the ellipsoid more spherical generally lead to faster convergence.", "problem": "Consider the central-cut iteration of the ellipsoid method on an ellipsoid $E(P,c) = \\{x \\in \\mathbb{R}^n : (x-c)^\\top P^{-1} (x-c) \\le 1\\}$, where $P \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $c \\in \\mathbb{R}^n$. A separating hyperplane passes through the center $c$ with normal $g \\in \\mathbb{R}^n$, i.e., the cut is $\\{x : g^\\top (x-c) \\le 0\\}$. Assume $n \\ge 2$. Let the spectral decomposition of $P$ be $P = Q \\Lambda Q^\\top$, with $\\Lambda = \\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_n)$, ordered as $\\lambda_1 \\ge \\cdots \\ge \\lambda_n  0$, and let $v_i$ denote the eigenvector corresponding to $\\lambda_i$. The method replaces $E(P,c)$ by a minimal-volume ellipsoid $E(P^+,c^+)$ that contains the half-ellipsoid after the cut. You are to compare two anisotropic choices for $g$: aligning $g$ with $v_{\\min} = v_n$ (the smallest-eigenvalue direction) versus aligning $g$ with $v_{\\max} = v_1$ (the largest-eigenvalue direction). Using core facts of the ellipsoid method for central cuts and basic linear algebra, determine which statement best characterizes the effect of these choices on the per-iteration change in volume, on the largest semi-axis length, on the condition number $\\kappa(P) = \\lambda_{\\max}(P)/\\lambda_{\\min}(P)$, and on the practical convergence trend.\n\nChoose exactly one option.\n\nA. Aligning $g$ with $v_{\\min}$ yields the greatest decrease in the largest semi-axis and the fastest improvement in the condition number; aligning with $v_{\\max}$ has the opposite effect.\n\nB. The per-iteration volume reduction factor for central cuts is independent of the direction of $g$. Aligning $g$ with $v_{\\max}$ produces the greatest decrease in the largest semi-axis and improves the condition number by a factor of $(n-1)/(n+1)$, while aligning $g$ with $v_{\\min}$ increases the largest semi-axis and worsens the condition number by a factor of $(n+1)/(n-1)$, typically slowing practical convergence.\n\nC. Aligning $g$ with either $v_{\\min}$ or $v_{\\max}$ produces identical updates to the eigenvalues of $P$ up to relabeling, so both the condition number and the largest semi-axis change are the same in either case; only the center $c$ update differs.\n\nD. Aligning $g$ with $v_{\\min}$ yields both the maximal volume reduction and the maximal reduction in the largest semi-axis for all $n$, making it the universally optimal choice for convergence.", "solution": "The problem asks for a comparison of two choices for the normal vector $g$ of a central cut in the ellipsoid method. The ellipsoid is given by $E(P,c) = \\{x \\in \\mathbb{R}^n : (x-c)^\\top P^{-1} (x-c) \\le 1\\}$, where $P$ is symmetric positive definite with eigenvalues $\\lambda_1 \\ge \\cdots \\ge \\lambda_n  0$ and corresponding eigenvectors $v_1, \\ldots, v_n$. The two choices for $g$ are alignment with $v_{\\max} = v_1$ (eigenvector corresponding to the largest eigenvalue $\\lambda_1$) and alignment with $v_{\\min} = v_n$ (eigenvector corresponding to the smallest eigenvalue $\\lambda_n$). The semi-axes of the ellipsoid are aligned with the eigenvectors $v_i$ and have lengths $\\sqrt{\\lambda_i}$.\n\nFirst, we state the update formulas for the matrix $P$ after a central cut defined by the hyperplane $g^\\top (x-c) \\le 0$. The new ellipsoid $E(P^+, c^+)$ has its matrix $P^+$ given by:\n$$\nP^+ = \\frac{n^2}{n^2-1} \\left( P - \\frac{2}{n+1} \\frac{(Pg)(Pg)^\\top}{g^\\top P g} \\right)\n$$\nwhere $n \\ge 2$ is the dimension.\n\nLet's analyze the per-iteration change in volume. The volume of the ellipsoid $E(P,c)$ is proportional to $\\sqrt{\\det(P)}$. The ratio of the new volume to the old volume is:\n$$\n\\frac{\\mathrm{vol}(E^+)}{\\mathrm{vol}(E)} = \\sqrt{\\frac{\\det(P^+)}{\\det(P)}}\n$$\nUsing the matrix determinant lemma, $\\det(A - uv^\\top) = (1 - v^\\top A^{-1}u)\\det(A)$, we can find $\\det(P^+)$.\nLet $A=P$, $u = \\frac{\\sqrt{2}}{\\sqrt{n+1}} \\frac{Pg}{\\sqrt{g^\\top P g}}$ and $v=u$.\n$$\n\\det\\left(P - \\frac{2}{n+1} \\frac{(Pg)(Pg)^\\top}{g^\\top P g}\\right) = \\det(P) \\left(1 - \\frac{2}{n+1} \\frac{g^\\top P P^{-1} Pg}{g^\\top P g}\\right) = \\det(P) \\left(1 - \\frac{2}{n+1}\\right) = \\det(P) \\frac{n-1}{n+1}\n$$\nTherefore,\n$$\n\\det(P^+) = \\left(\\frac{n^2}{n^2-1}\\right)^n \\det\\left(P - \\frac{2}{n+1} \\frac{(Pg)(Pg)^\\top}{g^\\top P g}\\right) = \\left(\\frac{n^2}{n^2-1}\\right)^n \\left(\\frac{n-1}{n+1}\\right) \\det(P)\n$$\nThe volume ratio is:\n$$\n\\frac{\\mathrm{vol}(E^+)}{\\mathrm{vol}(E)} = \\sqrt{\\left(\\frac{n^2}{n^2-1}\\right)^n \\left(\\frac{n-1}{n+1}\\right)} = \\frac{n}{n+1} \\left(\\frac{n^2}{n^2-1}\\right)^{(n-1)/2}\n$$\nThis expression is a constant that depends only on the dimension $n$, not on the direction of the cut $g$. This is a fundamental property of the central-cut ellipsoid method.\n\nNow, we analyze the effect of the two specific choices of $g$ on the eigenvalues of $P$, which determine the semi-axis lengths and the condition number $\\kappa(P) = \\lambda_1/\\lambda_n$. Without loss of generality, let's work in the basis of eigenvectors, so $P = \\Lambda = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_n)$, and the eigenvectors $v_i$ are the standard basis vectors $e_i$.\n\n**Case 1: Aligning $g$ with $v_{\\min} = v_n$**\nWe set $g = v_n = e_n$.\n$g^\\top P g = e_n^\\top \\Lambda e_n = \\lambda_n$.\n$Pg = \\Lambda e_n = \\lambda_n e_n$.\nThe update formula for $P$ becomes:\n$$\nP^+ = \\frac{n^2}{n^2-1} \\left( \\Lambda - \\frac{2}{n+1} \\frac{(\\lambda_n e_n)(\\lambda_n e_n)^\\top}{\\lambda_n} \\right) = \\frac{n^2}{n^2-1} \\left( \\Lambda - \\frac{2\\lambda_n}{n+1} e_n e_n^\\top \\right)\n$$\nSince $\\Lambda$ and $e_n e_n^\\top$ are diagonal matrices, $P^+$ is also diagonal. Its diagonal entries are the new eigenvalues $\\lambda_i^+$.\nFor $i \\in \\{1, \\ldots, n-1\\}$, the corresponding diagonal entry is not affected by the rank-one term:\n$\\lambda_i^+ = \\frac{n^2}{n^2-1} \\lambda_i$.\nFor $i=n$:\n$\\lambda_n^+ = \\frac{n^2}{n^2-1} \\left( \\lambda_n - \\frac{2\\lambda_n}{n+1} \\right) = \\frac{n^2}{n^2-1} \\lambda_n \\left( \\frac{n-1}{n+1} \\right) = \\frac{n^2}{(n+1)^2} \\lambda_n$.\n\nLet's analyze the effects:\n- **Largest semi-axis**: The original length is $\\sqrt{\\lambda_1}$. The new length is $\\sqrt{\\lambda_1^+} = \\sqrt{\\frac{n^2}{n^2-1} \\lambda_1}$. Since $n \\ge 2$, $\\frac{n^2}{n^2-1}  1$. Thus, the largest semi-axis length *increases*.\n- **Condition number**: The new condition number is $\\kappa(P^+) = \\lambda_{\\max}^+ / \\lambda_{\\min}^+$. The new largest eigenvalue is $\\lambda_1^+ = \\frac{n^2}{n^2-1} \\lambda_1$. The new smallest eigenvalue is $\\lambda_n^+ = \\frac{n^2}{(n+1)^2} \\lambda_n$.\n$$\n\\kappa(P^+) = \\frac{\\lambda_1^+}{\\lambda_n^+} = \\frac{\\frac{n^2}{n^2-1} \\lambda_1}{\\frac{n^2}{(n+1)^2} \\lambda_n} = \\frac{(n+1)^2}{n^2-1} \\frac{\\lambda_1}{\\lambda_n} = \\frac{(n+1)^2}{(n-1)(n+1)} \\kappa(P) = \\frac{n+1}{n-1} \\kappa(P)\n$$\nSince $\\frac{n+1}{n-1}  1$, the condition number *worsens* (increases).\n\n**Case 2: Aligning $g$ with $v_{\\max} = v_1$**\nWe set $g = v_1 = e_1$.\n$g^\\top P g = e_1^\\top \\Lambda e_1 = \\lambda_1$.\n$Pg = \\Lambda e_1 = \\lambda_1 e_1$.\nThe update becomes:\n$$\nP^+ = \\frac{n^2}{n^2-1} \\left( \\Lambda - \\frac{2}{n+1} \\frac{(\\lambda_1 e_1)(\\lambda_1 e_1)^\\top}{\\lambda_1} \\right) = \\frac{n^2}{n^2-1} \\left( \\Lambda - \\frac{2\\lambda_1}{n+1} e_1 e_1^\\top \\right)\n$$\nThis $P^+$ is also diagonal. The new eigenvalues are:\nFor $i=1$:\n$\\lambda_1^+ = \\frac{n^2}{n^2-1} \\left( \\lambda_1 - \\frac{2\\lambda_1}{n+1} \\right) = \\frac{n^2}{(n+1)^2} \\lambda_1$.\nFor $i \\in \\{2, \\ldots, n\\}$:\n$\\lambda_i^+ = \\frac{n^2}{n^2-1} \\lambda_i$.\n\nLet's analyze the effects:\n- **Largest semi-axis**: The semi-axis in the direction of the cut, which was the longest, shrinks significantly: $\\sqrt{\\lambda_1} \\to \\sqrt{\\lambda_1^+} = \\frac{n}{n+1}\\sqrt{\\lambda_1}$. All other semi-axes are stretched by a factor of $\\sqrt{\\frac{n^2}{n^2-1}}$. The new largest semi-axis has length $\\sqrt{\\lambda_{\\max}(P^+)} = \\sqrt{\\max(\\lambda_1^+, \\lambda_2^+)} = \\sqrt{\\max\\left(\\frac{n^2}{(n+1)^2}\\lambda_1, \\frac{n^2}{n^2-1}\\lambda_2\\right)}$. Compared to Case 1 where the largest semi-axis always increases, this choice provides a mechanism to reduce the largest semi-axis, and thus offers the \"greatest decrease\" in a comparative sense. The length of the first semi-axis itself is reduced.\n- **Condition number**: In the typical case of a badly conditioned (eccentric) ellipsoid where $\\lambda_1 \\gg \\lambda_2 \\ge \\cdots \\ge \\lambda_n$, the largest eigenvalue remains $\\lambda_1^+$ and the smallest remains $\\lambda_n^+$.\n$$\n\\kappa(P^+) = \\frac{\\lambda_1^+}{\\lambda_n^+} = \\frac{\\frac{n^2}{(n+1)^2} \\lambda_1}{\\frac{n^2}{n^2-1} \\lambda_n} = \\frac{n^2-1}{(n+1)^2} \\frac{\\lambda_1}{\\lambda_n} = \\frac{(n-1)(n+1)}{(n+1)^2} \\kappa(P) = \\frac{n-1}{n+1} \\kappa(P)\n$$\nSince $\\frac{n-1}{n+1}  1$, the condition number *improves* (decreases).\n\n**Practical Convergence**: Making the ellipsoid more spherical (improving the condition number) generally leads to better practical convergence, as it reduces anisotropy. Worsening the condition number makes the ellipsoid more \"cigar-shaped\", which can slow down convergence as the search region becomes highly distorted. Therefore, aligning the cut with $v_{\\max}$ is the superior strategy for practical performance.\n\nNow, we evaluate each option.\n\nA. Aligning $g$ with $v_{\\min}$ yields the greatest decrease in the largest semi-axis and the fastest improvement in the condition number; aligning with $v_{\\max}$ has the opposite effect.\nThis is factually incorrect. Aligning with $v_{\\min}$ *increases* the largest semi-axis and *worsens* the condition number.\nVerdict: **Incorrect**.\n\nB. The per-iteration volume reduction factor for central cuts is independent of the direction of $g$. Aligning $g$ with $v_{\\max}$ produces the greatest decrease in the largest semi-axis and improves the condition number by a factor of $(n-1)/(n+1)$, while aligning $g$ with $v_{\\min}$ increases the largest semi-axis and worsens the condition number by a factor of $(n+1)/(n-1)$, typically slowing practical convergence.\nThis statement accurately summarizes our findings.\n- The volume reduction is indeed independent of $g$.\n- Aligning with $v_{\\max}$ provides the best (and only) strategy of the two to reduce the longest axis, and is thus described as yielding the \"greatest decrease\".\n- It improves the condition number by the specified factor in the typical case.\n- Aligning with $v_{\\min}$ does increase the largest semi-axis and worsens the condition number by the specified factor.\n- The conclusion about practical convergence is sound.\nVerdict: **Correct**.\n\nC. Aligning $g$ with either $v_{\\min}$ or $v_{\\max}$ produces identical updates to the eigenvalues of $P$ up to relabeling, so both the condition number and the largest semi-axis change are the same in either case; only the center $c$ update differs.\nThis is false. The set of new eigenvalues in Case 1 is $\\{\\frac{n^2}{n^2-1}\\lambda_1, \\ldots, \\frac{n^2}{n^2-1}\\lambda_{n-1}, \\frac{n^2}{(n+1)^2}\\lambda_n\\}$. In Case 2, it is $\\{\\frac{n^2}{(n+1)^2}\\lambda_1, \\frac{n^2}{n^2-1}\\lambda_2, \\ldots, \\frac{n^2}{n^2-1}\\lambda_n\\}$. These two sets of numbers are not the same (unless the ellipsoid is a sphere, i.e., $\\lambda_1=\\lambda_n$). As a result, the changes to the condition number and largest semi-axis are different.\nVerdict: **Incorrect**.\n\nD. Aligning $g$ with $v_{\\min}$ yields both the maximal volume reduction and the maximal reduction in the largest semi-axis for all $n$, making it the universally optimal choice for convergence.\nThis is false on multiple counts. The volume reduction is independent of $g$. Aligning with $v_{\\min}$ *increases*, not reduces, the largest semi-axis. It is a poor choice for convergence.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "3125298"}]}