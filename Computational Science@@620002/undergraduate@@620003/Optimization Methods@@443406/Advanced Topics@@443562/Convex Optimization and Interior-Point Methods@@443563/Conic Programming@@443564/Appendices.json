{"hands_on_practices": [{"introduction": "This problem grounds conic programming in the practical domain of data analysis. You will learn how to model shape constraints, such as monotonicity and convexity, using simple linear inequalities on difference variables. This practice is essential for translating qualitative properties into the precise language of optimization, forming the basis for both Linear Programs (LP) and Second-Order Cone Programs (SOCP) [@problem_id:3111059].", "problem": "Consider $n$ equally spaced design points $x_1, x_2, \\dots, x_n$ with observed responses $y_1, y_2, \\dots, y_n$. A fitted sequence $f_1, f_2, \\dots, f_n$ is called monotone nondecreasing if its first forward differences $d_i := f_{i+1} - f_i$ satisfy $d_i \\ge 0$ for all $i$, and it is called discretely convex if its second forward differences $\\Delta^2 f_i := f_{i+2} - 2 f_{i+1} + f_i$ satisfy $\\Delta^2 f_i \\ge 0$ for all $i$ where defined. These are discrete analogues of monotonicity and convexity grounded in difference operators for uniformly spaced inputs.\n\nYou will construct shape-constrained regression models and analyze a smoothness trade-off using well-established conic optimization primitives. Specifically:\n\n- A Linear Program (LP) can model absolute-value loss and an absolute-value smoothness penalty using auxiliary variables, and monotonicity/convexity can be enforced through linear inequalities on differences.\n\n- A Second-Order Cone Program (SOCP) can model Euclidean norm loss and an Euclidean norm smoothness penalty using second-order cone constraints, with the same monotonicity/convexity enforced via linear inequalities on differences.\n\nFormulate both models over differences, then explicitly solve a small LP instance.\n\n1. Using only the definitions of monotonicity and discrete convexity stated above and the standard decomposition of an absolute value via nonnegative auxiliary variables, express the following shape-constrained regression as a Linear Program over difference variables:\n   Minimize the objective \n   $$\\sum_{i=1}^{n} |f_i - y_i| + \\mu \\sum_{i=1}^{n-2} |\\Delta^2 f_i|$$\n   subject to monotone nondecreasing and discretely convex constraints on the sequence $f_1, \\dots, f_n$. Your formulation must be in terms of first differences $d_i := f_{i+1} - f_i$ and nonnegative auxiliary variables that linearize the absolute values.\n\n2. Using the definition of the Euclidean norm and the standard second-order cone representation of norm inequalities, express the analogous Second-Order Cone Program (SOCP) that minimizes\n   $$\\|f - y\\|_2 + \\mu \\left\\| \\Delta^2 f \\right\\|_2$$\n   over sequences $f_1, \\dots, f_n$ that are monotone nondecreasing and discretely convex, where $\\Delta^2 f$ collects the second forward differences. Your SOCP must introduce scalar variables to bound these norms and must enforce the monotonicity and convexity via linear inequalities on differences.\n\n3. Consider the LP you formulated in part 1 with $n = 3$, uniformly spaced $x_1 = 0$, $x_2 = 1$, $x_3 = 2$, and observed responses $y_1 = 0$, $y_2 = 2$, $y_3 = 1$. Solve this LP exactly for an arbitrary smoothing weight $\\mu \\ge 0$ and report the optimal fitted value at $x_3$, namely $f_3$. If rounding is needed, state your exact result; do not round.\n\nYour final answer must be the single value of $f_3$ for the specified instance, written as an exact number.", "solution": "### Part 1: Linear Programming (LP) Formulation\n\nThe goal is to formulate the given optimization problem as a Linear Program. The variables of the optimization will be the first value of the sequence, $f_1$, and the first-order differences, $d_i = f_{i+1} - f_i$ for $i=1, \\dots, n-1$.\n\nFrom these variables, we can express the fitted values $f_i$ and the second-order differences $\\Delta^2 f_i$:\n-   $f_1$ is a primary variable.\n-   $f_i = f_1 + \\sum_{k=1}^{i-1} d_k$ for $i=2, \\dots, n$.\n-   $\\Delta^2 f_i = (f_{i+2} - f_{i+1}) - (f_{i+1} - f_i) = d_{i+1} - d_i$ for $i=1, \\dots, n-2$.\n\nThe objective function contains absolute values, which must be linearized. We introduce nonnegative auxiliary variables:\n-   $u_i$ to represent $|f_i - y_i|$ for $i=1, \\dots, n$. A term $|z|$ is replaced by a variable $w$ in the objective, subject to constraints $w \\ge z$ and $w \\ge -z$.\n-   $v_i$ to represent $|\\Delta^2 f_i|$ for $i=1, \\dots, n-2$.\n\nThe shape constraints are expressed in terms of the difference variables:\n-   Monotonicity: $d_i \\ge 0$ for $i=1, \\dots, n-1$.\n-   Convexity: $\\Delta^2 f_i = d_{i+1} - d_i \\ge 0$ for $i=1, \\dots, n-2$.\n\nThe complete LP formulation is as follows:\n\n**Variables:**\n-   $f_1 \\in \\mathbb{R}$\n-   $d_i \\in \\mathbb{R}$ for $i=1, \\dots, n-1$\n-   $u_i \\in \\mathbb{R}$ for $i=1, \\dots, n$\n-   $v_i \\in \\mathbb{R}$ for $i=1, \\dots, n-2$\n\n**Objective:**\nMinimize $ \\sum_{i=1}^{n} u_i + \\mu \\sum_{i=1}^{n-2} v_i $\n\n**Constraints:**\n1.  **Loss Linearization:**\n    -   $u_1 \\ge f_1 - y_1$\n    -   $u_1 \\ge -(f_1 - y_1)$\n    -   For $i = 2, \\dots, n$:\n        -   $u_i \\ge \\left(f_1 + \\sum_{k=1}^{i-1} d_k\\right) - y_i$\n        -   $u_i \\ge -\\left(f_1 + \\sum_{k=1}^{i-1} d_k\\right) + y_i$\n2.  **Penalty Linearization:** For $i=1, \\dots, n-2$:\n    -   $v_i \\ge d_{i+1} - d_i$\n    -   $v_i \\ge -(d_{i+1} - d_i)$\n3.  **Monotonicity:** For $i=1, \\dots, n-1$:\n    -   $d_i \\ge 0$\n4.  **Convexity:** For $i=1, \\dots, n-2$:\n    -   $d_{i+1} - d_i \\ge 0$\n5.  **Nonnegativity of auxiliary variables:**\n    -   $u_i \\ge 0$ for $i=1, \\dots, n$\n    -   $v_i \\ge 0$ for $i=1, \\dots, n-2$\n    (Note: These are implied by the linearization constraints but are standard to state for an LP).\n\n### Part 2: Second-Order Cone Programming (SOCP) Formulation\n\nThe objective $\\|f-y\\|_2 + \\mu \\|\\Delta^2 f\\|_2$ involves Euclidean norms. We introduce two scalar auxiliary variables, $t_1$ and $t_2$, to bound these norms. The problem is equivalent to minimizing $t_1 + \\mu t_2$ subject to $\\|f-y\\|_2 \\le t_1$ and $\\|\\Delta^2 f\\|_2 \\le t_2$. These inequalities are second-order cone constraints.\n\nUsing the same primary variables $f_1$ and $d_i$ as in Part 1.\n\n**Variables:**\n-   $f_1 \\in \\mathbb{R}$\n-   $d_i \\in \\mathbb{R}$ for $i=1, \\dots, n-1$\n-   $t_1, t_2 \\in \\mathbb{R}$\n\n**Objective:**\nMinimize $ t_1 + \\mu t_2 $\n\n**Constraints:**\n1.  **Loss Norm Constraint (SOC):**\n    -   Let the vector $z_f = f-y$. Its components are:\n        -   $z_{f,1} = f_1 - y_1$\n        -   $z_{f,i} = \\left(f_1 + \\sum_{k=1}^{i-1} d_k\\right) - y_i$ for $i=2, \\dots, n$.\n    -   The constraint is $\\|z_f\\|_2 \\le t_1$, which is the standard second-order cone constraint on the vector $(t_1, z_f^T)$.\n\n2.  **Penalty Norm Constraint (SOC):**\n    -   Let the vector $z_d = \\Delta^2 f$. Its components are:\n        -   $z_{d,i} = d_{i+1} - d_i$ for $i=1, \\dots, n-2$.\n    -   The constraint is $\\|z_d\\|_2 \\le t_2$, which is the standard second-order cone constraint on the vector $(t_2, z_d^T)$.\n\n3.  **Monotonicity (Linear):** For $i=1, \\dots, n-1$:\n    -   $d_i \\ge 0$\n\n4.  **Convexity (Linear):** For $i=1, \\dots, n-2$:\n    -   $d_{i+1} - d_i \\ge 0$\n\n### Part 3: Solving the LP Instance\n\nWe are given $n=3$, $y_1=0$, $y_2=2$, $y_3=1$, and $\\mu \\ge 0$. The task is to find the optimal $f_3$.\n\nFor $n=3$, the decision variables are $f_1, d_1, d_2$.\n-   $f_1 = f_1$\n-   $f_2 = f_1 + d_1$\n-   $f_3 = f_1 + d_1 + d_2$\n\nThe constraints are:\n-   Monotonicity: $d_1 \\ge 0$, $d_2 \\ge 0$.\n-   Convexity: There is one constraint for $i=1$: $\\Delta^2 f_1 = d_2 - d_1 \\ge 0$.\nCombining these gives $d_2 \\ge d_1 \\ge 0$.\n\nThe convexity constraint $d_2 - d_1 \\ge 0$ implies that the second difference is always non-negative. Therefore, $|\\Delta^2 f_1| = |d_2 - d_1| = d_2 - d_1$. The objective function from Part 1 simplifies. We do not need the $v_i$ variables. The problem becomes:\nMinimize $|f_1 - y_1| + |f_2 - y_2| + |f_3 - y_3| + \\mu (d_2 - d_1)$.\nSubstituting the expressions for $f_i$ and the given $y_i$:\nMinimize $J(f_1, d_1, d_2) = |f_1 - 0| + |f_1 + d_1 - 2| + |f_1 + d_1 + d_2 - 1| + \\mu(d_2 - d_1)$\nSubject to $d_2 \\ge d_1 \\ge 0$.\n\nThis is a convex optimization problem. For fixed $d_1, d_2$, the objective is a sum of absolute values in $f_1$:\n$\\min_{f_1} \\left( |f_1 - 0| + |f_1 - (2 - d_1)| + |f_1 - (1 - d_1 - d_2)| \\right)$\nThe solution to $\\min_x \\sum_k |x-p_k|$ is the median of the points $\\{p_k\\}$. Here, the points are $p_A=1 - d_1 - d_2$, $p_B=0$, and $p_C=2-d_1$.\nThe optimal $f_1$ is $\\text{median}\\{1-d_1-d_2, 0, 2-d_1\\}$.\nSince $d_2 \\ge 0$, we have $p_C - p_A = (2-d_1)-(1-d_1-d_2) = 1+d_2 \\ge 1$, so $p_C > p_A$. We analyze three cases based on the ordering of these points relative to $0$.\n\nCase 1: $1 - d_1 - d_2 \\ge 0 \\implies d_1+d_2 \\le 1$.\nThe order is $p_C > p_A \\ge p_B=0$. The median is $p_A$. Optimal $f_1 = 1-d_1-d_2$.\nThe sum of absolute values becomes $(p_C-p_B) = (2-d_1)-0 = 2-d_1$.\nThe objective becomes $J = 2-d_1 + \\mu(d_2-d_1) = 2 - d_1(1+\\mu) + \\mu d_2$.\nWe minimize this linear function over the region defined by $d_2 \\ge d_1 \\ge 0$ and $d_1+d_2 \\le 1$. This is a triangle with vertices $(0,0), (0,1), (1/2,1/2)$. The minimum of a linear function over a polytope occurs at a vertex.\n-   At $(0,0)$: $J=2$.\n-   At $(0,1)$: $J=2+\\mu$.\n-   At $(1/2,1/2)$: $J=2-(1/2)(1+\\mu)+\\mu/2 = 2-1/2 = 3/2$.\nThe minimum in this region is $3/2$, occurring at $(d_1, d_2) = (1/2, 1/2)$.\n\nCase 2: $2 - d_1 \\le 0 \\implies d_1 \\ge 2$.\nThe order is $p_B=0 \\ge p_C > p_A$. The median is $p_C$. Optimal $f_1 = 2-d_1$.\nThe sum of absolute values becomes $(p_B-p_A) = 0-(1-d_1-d_2) = d_1+d_2-1$.\nThe objective is $J = d_1+d_2-1 + \\mu(d_2-d_1) = d_1(1-\\mu) + d_2(1+\\mu)-1$.\nWe minimize over the region $d_2 \\ge d_1 \\ge 2$. For any $\\mu \\ge 0$, the minimum occurs at the corner point $(2,2)$.\nAt $(2,2)$, $J = 2(1-\\mu) + 2(1+\\mu)-1 = 2-2\\mu+2+2\\mu-1=3$.\n\nCase 3: $1 - d_1 - d_2 < 0 < 2-d_1$. This corresponds to $d_1+d_2 > 1$ and $d_1 < 2$.\nThe order is $p_C > p_B=0 > p_A$. The median is $p_B=0$. Optimal $f_1=0$.\nThe sum of absolute values is $(p_C-p_A)=(2-d_1)-(1-d_1-d_2) = 1+d_2$.\nThe objective is $J = 1+d_2 + \\mu(d_2-d_1) = 1 + d_2(1+\\mu) - \\mu d_1$.\nWe minimize over the region $d_2 \\ge d_1 \\ge 0$, $d_1+d_2>1$, $d_1<2$. The gradient of $J$ with respect to $(d_1, d_2)$ is $(-\\mu, 1+\\mu)$. For $\\mu>0$, this vector points toward increasing $d_1$ and decreasing $d_2$. This directs us to the boundary $d_2=d_1$.\nOn this line, $J=1+d_1(1+\\mu)-\\mu d_1 = 1+d_1$. The region is $d_1 \\in (1/2, 2)$, so the infimum is approached as $d_1 \\to 1/2$, giving $J \\to 1+1/2=3/2$.\nIf $\\mu=0$, $J=1+d_2$. To minimize, we need smallest $d_2$, which is approached at the point $(1/2,1/2)$, giving $J \\to 1+1/2=3/2$.\n\nComparing the minimum values from all cases ($3/2$, $3$, and $3/2$), the global minimum of the objective function is $3/2$. This minimum is achieved at the point $(d_1, d_2)=(1/2, 1/2)$, which is valid for any $\\mu \\ge 0$.\n\nAt this optimal point, we determine the value of $f_1$. The point $(d_1,d_2)=(1/2,1/2)$ is on the boundary between Case 1 and Case 3 ($d_1+d_2=1$).\nUsing the rule from Case 1: $f_1 = 1 - d_1 - d_2 = 1 - 1/2 - 1/2 = 0$.\nUsing the rule from Case 3: $f_1=0$.\nBoth are consistent. The optimal value is $f_1=0$.\n\nWe can now find the optimal fitted sequence:\n-   $f_1 = 0$\n-   $f_2 = f_1 + d_1 = 0 + 1/2 = 1/2$\n-   $f_3 = f_1 + d_1 + d_2 = 0 + 1/2 + 1/2 = 1$\n\nThe problem asks for the optimal value of $f_3$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3111059"}, {"introduction": "This exercise demonstrates the power of conic reformulation by tackling the trust-region subproblem, a cornerstone of nonlinear optimization. You will convert a quadratically constrained quadratic program into a standard Second-Order Cone Program (SOCP), providing a direct link between classical methods and modern conic optimization. By comparing the SOCP formulation to the traditional eigenvalue-based solution, you will gain a deeper appreciation for the equivalence and underlying structure of these problems [@problem_id:3111118].", "problem": "Consider the quadratic trust-region subproblem\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\quad \\text{subject to} \\quad \\|x\\|_{2} \\leq \\delta,\n$$\nwhere $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite, $c \\in \\mathbb{R}^{2}$, and $\\delta > 0$. Let\n$$\nQ = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad c = \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix}, \\quad \\delta = \\sqrt{5}.\n$$\nYour tasks are:\n\n1) Starting only from core definitions of convexity, the definition of a second-order cone, and epigraph transformations, recast the problem as an equivalent Second-Order Cone Program (SOCP). Introduce an epigraph variable to linearize the quadratic objective, and express all constraints using the Euclidean second-order cone and, where appropriate, the rotated second-order cone. Present the SOCP in a canonical conic form with linear objective and conic constraints only.\n\n2) Independently derive the optimizer using the eigenvalue-based closed-form approach that follows from the Karush–Kuhn–Tucker (KKT) conditions for the trust-region problem. Use the stationarity condition to relate the optimizer and the Lagrange multiplier associated with the trust-region constraint, determine whether the trust-region is active, and solve for the multiplier. Then determine the corresponding optimizer.\n\n3) Compare the canonical conic formulation from part 1) with the eigenvalue-based optimizer from part 2) by verifying they are consistent. Finally, compute the optimal objective value\n$$\nf^{\\star} \\triangleq \\min_{\\|x\\|_{2} \\leq \\delta} \\left\\{ \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\right\\}.\n$$\nReport $f^{\\star}$ as a single real number. No rounding is required.", "solution": "### Part 1: Recasting as a Second-Order Cone Program (SOCP)\n\nThe original problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\quad \\text{s.t.} \\quad \\|x\\|_{2} \\leq \\delta\n$$\nWe introduce an epigraph variable $t \\in \\mathbb{R}$ to represent the value of the objective function. The problem is equivalent to:\n$$\n\\min_{x \\in \\mathbb{R}^{2}, t \\in \\mathbb{R}} \\;\\; t \\quad \\text{s.t.} \\quad \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\leq t \\quad \\text{and} \\quad \\|x\\|_{2} \\leq \\delta\n$$\nThe objective is now linear. We must express the constraints in conic form.\n\nThe trust-region constraint $\\|x\\|_{2} \\leq \\delta$ is a standard second-order cone constraint. The second-order cone of dimension $n+1$ is defined as $\\mathcal{K}_s^{n+1} = \\{ (y_0, y_1, \\dots, y_n) \\in \\mathbb{R}^{n+1} \\mid y_0 \\ge \\|(y_1, \\dots, y_n)\\|_2 \\}$. For $x \\in \\mathbb{R}^2$, this constraint can be written as:\n$$ \\begin{pmatrix} \\delta \\\\ x \\end{pmatrix} \\in \\mathcal{K}_s^3 $$\nwhere $x=(x_1, x_2)^{\\top}$ and $\\delta = \\sqrt{5}$.\n\nThe quadratic constraint is $\\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\leq t$. Since $Q$ is symmetric positive definite, we can find a matrix $A$ such that $Q = A^{\\top}A$. Here, $Q$ is diagonal, so we can choose $A = Q^{1/2}$.\n$$\nA = Q^{1/2} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe constraint becomes $\\frac{1}{2}\\|Ax\\|_2^2 + c^{\\top}x \\leq t$, which can be rearranged to:\n$$\n\\|Ax\\|_2^2 \\leq 2(t - c^{\\top}x)\n$$\nThis inequality has the structure of a rotated second-order cone constraint. A rotated second-order cone of dimension $k+2$ is defined as $\\mathcal{K}_r^{k+2} = \\{ (u, v, y_1, \\dots, y_k) \\in \\mathbb{R}^{k+2} \\mid 2uv \\ge \\|y\\|_2^2, u \\ge 0, v \\ge 0 \\}$.\nTo put our inequality into this form, we can let $y = Ax$, $u=1$, and introduce a new variable $v = t - c^{\\top}x$. The inequality becomes $\\|y\\|_2^2 \\leq 2v$, with $u=1 \\ge 0$. The condition $v \\ge 0$ is implicitly enforced by the cone definition.\nThis gives the conic constraint:\n$$\n\\begin{pmatrix} 1 \\\\ v \\\\ Ax \\end{pmatrix} \\in \\mathcal{K}_r^4\n$$\nWith the substitution $v = t - c^{\\top}x$, we have $t = v + c^{\\top}x$. We can substitute this into the objective function. The problem becomes:\n$$\n\\min_{x \\in \\mathbb{R}^2, v \\in \\mathbb{R}} \\;\\; c^{\\top}x + v\n$$\nThe decision variables for the SOCP are $x_1, x_2,$ and $v$. The final SOCP formulation in canonical form is:\n$$\n\\min_{x_1, x_2, v} \\;\\; -3x_1 - 4x_2 + v\n$$\nsubject to the two conic constraints:\n$$\n\\begin{pmatrix} \\sqrt{5} \\\\ x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathcal{K}_s^3\n$$\n$$\n\\begin{pmatrix} 1 \\\\ v \\\\ \\sqrt{2}x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathcal{K}_r^4\n$$\n\n### Part 2: Eigenvalue-based Solution via KKT Conditions\n\nThe Lagrangian for the trust-region subproblem is:\n$$ L(x, \\lambda) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x + \\frac{\\lambda}{2} (x^{\\top}x - \\delta^2) $$\nThe Karush-Kuhn-Tucker (KKT) conditions for an optimal solution $x^\\star$ and Lagrange multiplier $\\lambda^\\star$ are:\n1.  **Stationarity:** $\\nabla_x L(x^\\star, \\lambda^\\star) = Qx^\\star + c + \\lambda^\\star x^\\star = (Q + \\lambda^\\star I)x^\\star + c = 0$.\n2.  **Primal Feasibility:** $\\|x^\\star\\|_2^2 \\leq \\delta^2$.\n3.  **Dual Feasibility:** $\\lambda^\\star \\ge 0$.\n4.  **Complementary Slackness:** $\\lambda^\\star (\\|x^\\star\\|_2^2 - \\delta^2) = 0$.\n\nFirst, consider the case where the solution is in the interior of the trust region, i.e., $\\|x^\\star\\|_2 < \\delta$. By complementary slackness, this implies $\\lambda^\\star = 0$. The stationarity condition simplifies to $Qx^\\star = -c$. The unconstrained minimizer is $x_{unc} = -Q^{-1}c$.\nGiven $Q=\\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$, its inverse is $Q^{-1} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n$$ x_{unc} = -\\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ 4 \\end{pmatrix} $$\nWe check if this solution is feasible:\n$$ \\|x_{unc}\\|_2^2 = (3/2)^2 + 4^2 = \\frac{9}{4} + 16 = \\frac{73}{4} = 18.25 $$\nThe squared radius is $\\delta^2 = (\\sqrt{5})^2 = 5$. Since $18.25 > 5$, the unconstrained solution is not feasible. This means the optimal solution must lie on the boundary of the trust region.\n\nThus, the constraint is active: $\\|x^\\star\\|_2^2 = \\delta^2$. By complementary slackness, this requires $\\lambda^\\star > 0$ (since if $\\lambda^\\star=0$, we get the infeasible $x_{unc}$).\nFrom the stationarity condition, we have $(Q + \\lambda I)x = -c$. The eigenvalues of $Q$ are $2$ and $1$. Since $\\lambda > 0$, the matrix $Q+\\lambda I$ is positive definite and thus invertible.\n$$ x(\\lambda) = -(Q+\\lambda I)^{-1}c $$\n$$ Q+\\lambda I = \\begin{pmatrix} 2+\\lambda & 0 \\\\ 0 & 1+\\lambda \\end{pmatrix} \\implies (Q+\\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{2+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix} $$\n$$ x(\\lambda) = -\\begin{pmatrix} \\frac{1}{2+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2+\\lambda} \\\\ \\frac{4}{1+\\lambda} \\end{pmatrix} $$\nWe enforce the active constraint condition $\\|x(\\lambda)\\|_2^2 = \\delta^2$:\n$$ \\left(\\frac{3}{2+\\lambda}\\right)^2 + \\left(\\frac{4}{1+\\lambda}\\right)^2 = 5 $$\n$$ \\frac{9}{(2+\\lambda)^2} + \\frac{16}{(1+\\lambda)^2} = 5 $$\nBy inspection, we can test integer values for $\\lambda$. For $\\lambda=1$:\n$$ \\frac{9}{(2+1)^2} + \\frac{16}{(1+1)^2} = \\frac{9}{3^2} + \\frac{16}{2^2} = \\frac{9}{9} + \\frac{16}{4} = 1 + 4 = 5 $$\nSo, $\\lambda^\\star = 1$ is the unique positive solution to this secular equation.\nThe optimal solution $x^\\star$ is found by substituting $\\lambda^\\star=1$ back into the expression for $x(\\lambda)$:\n$$ x^\\star = \\begin{pmatrix} \\frac{3}{2+1} \\\\ \\frac{4}{1+1} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} $$\n\n### Part 3: Comparison and Optimal Value Calculation\n\nWe verify that the optimizer $x^\\star = (1, 2)^{\\top}$ from Part 2 is consistent with the SOCP formulation from Part 1.\nFor $x^\\star = (1, 2)^{\\top}$, the first SOCP constraint is:\n$$ \\begin{pmatrix} \\sqrt{5} \\\\ 1 \\\\ 2 \\end{pmatrix} \\in \\mathcal{K}_s^3 \\iff \\sqrt{5} \\ge \\sqrt{1^2+2^2} = \\sqrt{5} $$\nThis constraint is satisfied (and active).\nThe second SOCP constraint involves the variable $v$:\n$$ \\begin{pmatrix} 1 \\\\ v \\\\ \\sqrt{2}(1) \\\\ 2 \\end{pmatrix} \\in \\mathcal{K}_r^4 \\iff 2(1)v \\ge (\\sqrt{2})^2 + 2^2 \\implies 2v \\ge 2+4=6 \\implies v \\ge 3 $$\nThe SOCP seeks to minimize the objective function $c^{\\top}x + v = -3x_1 - 4x_2 + v$. For the fixed optimal $x^\\star=(1,2)^{\\top}$, this becomes:\n$$ \\min_{v \\ge 3} \\;\\; -3(1) - 4(2) + v = \\min_{v \\ge 3} \\;\\; -11 + v $$\nThe minimum is achieved at the smallest possible value of $v$, which is $v=3$.\nThe optimal value of the SOCP objective is $-11+3 = -8$.\n\nFinally, we compute the optimal objective value $f^\\star$ of the original problem using the optimizer $x^\\star = (1, 2)^{\\top}$:\n$$ f^\\star = f(x^\\star) = \\frac{1}{2} x^{\\star\\top} Q x^\\star + c^{\\top} x^\\star $$\n$$ c^{\\top}x^\\star = \\begin{pmatrix} -3 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = -3 - 8 = -11 $$\n$$ x^{\\star\\top} Q x^\\star = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = 1(2) + 2(2) = 6 $$\n$$ f^\\star = \\frac{1}{2}(6) + (-11) = 3 - 11 = -8 $$\nThe optimal value $f^\\star = -8$ matches the optimal value of the SOCP objective. The two approaches yield consistent results. The optimal objective value is $-8$.", "answer": "$$\n\\boxed{-8}\n$$", "id": "3111118"}, {"introduction": "This practice explores the advanced application of conic duality in the context of robust optimization, where decisions must be resilient to uncertainty. You will analyze a max-min problem, which is typically difficult to solve, and learn how to use strong duality to transform it into a tractable single-level conic program. This powerful technique of swapping the minimizer and maximizer is a key theoretical tool with significant practical impact in fields like engineering and finance [@problem_id:3111113].", "problem": "Consider the following robust optimization problem in the setting of conic programming. Let the uncertainty set for a scenario vector be ellipsoidal, given by $\\{\\bar{\\theta} + P z : \\|z\\|_{2} \\leq 1\\}$, where $\\bar{\\theta} \\in \\mathbb{R}^{2}$ and $P \\in \\mathbb{R}^{2 \\times 2}$. A decision-maker chooses $x \\in \\mathbb{R}^{2}$ subject to the trust-region constraint $\\|x\\|_{2} \\leq 1$. The robust objective is the worst-case linear payoff against the uncertainty:\n$$\n\\max_{\\|x\\|_{2} \\leq 1} \\ \\min_{\\|z\\|_{2} \\leq 1} \\ x^{\\top}(\\bar{\\theta} + P z).\n$$\nYou are given $\\bar{\\theta} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $P = 2 I_{2}$, where $I_{2}$ denotes the $2 \\times 2$ identity matrix.\n\nUsing only the fundamental definitions of:\n- the Second-Order Cone (SOC), defined as $\\mathcal{Q}^{n+1} = \\{(t, w) \\in \\mathbb{R} \\times \\mathbb{R}^{n} : \\|w\\|_{2} \\leq t\\}$,\n- the conic primal-dual pair in standard form, where a primal is written as $\\min_{z} \\ c^{\\top}z$ subject to $A z + b \\in \\mathcal{K}$ with $\\mathcal{K}$ a closed convex cone, and its dual is $\\max_{y} \\ -b^{\\top} y$ subject to $A^{\\top} y + c = 0$, $y \\in \\mathcal{K}^{*}$,\n- Slater’s condition (strict feasibility) implying strong duality for conic programs,\n\nperform the following tasks:\n1. For a fixed $x$, formulate the inner minimization over $z$ as a conic program over the Second-Order Cone, derive its conic dual, and use strong duality to express the inner minimization value as a function of $x$ only.\n2. By swapping the $\\min$ and $\\max$ via strong duality, rewrite the original max-min robust objective as a single-level maximization involving $x$ and any dual variables necessary.\n3. Verify that Slater’s condition holds for both the inner primal and dual problems under the given data, thereby justifying the use of strong duality.\n4. Solve the resulting problem to determine the exact optimal robust value of the objective. Express your final answer as a single exact real number or a closed-form analytic expression. No rounding is required.", "solution": "The problem is a robust optimization problem given by\n$$\n\\max_{\\|x\\|_{2} \\leq 1} \\ \\min_{\\|z\\|_{2} \\leq 1} \\ x^{\\top}(\\bar{\\theta} + P z)\n$$\nwith parameters $\\bar{\\theta} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $P = 2 I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix. We will address the tasks sequentially.\n\n**1. Inner Minimization as a Conic Program**\n\nFor a fixed decision vector $x \\in \\mathbb{R}^{2}$, the inner problem is\n$$\n\\min_{\\|z\\|_{2} \\leq 1} x^{\\top}(\\bar{\\theta} + P z) = x^{\\top}\\bar{\\theta} + \\min_{\\|z\\|_{2} \\leq 1} x^{\\top} P z\n$$\nLet's focus on the minimization part: $\\min_{\\|z\\|_{2} \\leq 1} (P^{\\top}x)^{\\top}z$. This is an optimization problem over the variable $z \\in \\mathbb{R}^2$. The constraint $\\|z\\|_{2} \\leq 1$ can be expressed using the Second-Order Cone (SOC) $\\mathcal{Q}^3 = \\{(t, w) \\in \\mathbb{R} \\times \\mathbb{R}^{2} : \\|w\\|_{2} \\leq t\\}$ by setting $t=1$ and $w=z$, so $(1, z) \\in \\mathcal{Q}^3$.\n\nWe formulate this as a conic program in the specified primal form: $\\min_{\\tilde{z}} c^{\\top}\\tilde{z}$ subject to $A \\tilde{z} + b \\in \\mathcal{K}$.\nLet the optimization variable be $\\tilde{z} = z \\in \\mathbb{R}^2$.\nThe objective function is $(P^{\\top}x)^{\\top}z$, so we set the cost vector $c = P^{\\top}x$.\nThe constraint $(1, z) \\in \\mathcal{Q}^3$ must be written as $A z + b \\in \\mathcal{Q}^3$. We can define $A$ and $b$ as follows:\n$$\nA = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^3\n$$\nWith these choices, $A z + b = \\begin{pmatrix} 0 \\\\ z_1 \\\\ z_2 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ z \\end{pmatrix}$.\nThe cone is $\\mathcal{K} = \\mathcal{Q}^3$. The primal conic program is:\n$$\n\\text{(Primal)} \\quad \\min_{z \\in \\mathbb{R}^2} \\ (P^{\\top}x)^{\\top}z \\quad \\text{s.t.} \\quad Az + b \\in \\mathcal{Q}^3\n$$\nThe dual problem is given by $\\max_{y} -b^{\\top} y$ subject to $A^{\\top} y + c = 0$ and $y \\in \\mathcal{K}^{*}$. The second-order cone is self-dual, so $\\mathcal{K}^{*} = \\mathcal{Q}^3$. The dual variable is $y = (y_0, y_w) \\in \\mathbb{R} \\times \\mathbb{R}^2$.\nThe dual constraints are:\n1. $y \\in \\mathcal{Q}^3 \\implies \\|y_w\\|_2 \\leq y_0$.\n2. $A^{\\top}y + c = 0$. Let's compute $A^{\\top}y$:\n$$\nA^{\\top}y = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} y_0 \\\\ y_{w1} \\\\ y_{w2} \\end{pmatrix} = \\begin{pmatrix} y_{w1} \\\\ y_{w2} \\end{pmatrix} = y_w\n$$\nSo the constraint becomes $y_w + P^{\\top}x = 0$, which implies $y_w = -P^{\\top}x$.\n\nThe dual objective is $-b^{\\top}y = -\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} y_0 \\\\ y_w \\end{pmatrix} = -y_0$.\nThe dual problem is thus:\n$$\n\\text{(Dual)} \\quad \\max_{y_0, y_w} \\ -y_0 \\quad \\text{s.t.} \\quad y_w = -P^{\\top}x, \\quad \\|y_w\\|_2 \\leq y_0\n$$\nTo maximize $-y_0$, we must minimize $y_0$. The constraint $\\|y_w\\|_2 \\leq y_0$ implies that the minimum possible value for $y_0$ is $\\|y_w\\|_2$. Substituting $y_w = -P^{\\top}x$, the optimal $y_0$ is $y_0^* = \\|-P^{\\top}x\\|_2 = \\|P^{\\top}x\\|_2$.\nThe optimal value of the dual problem is $-y_0^* = -\\|P^{\\top}x\\|_2$.\n\nAssuming strong duality (which we will justify in Task 3), the optimal value of the primal equals the optimal value of the dual. The value of the inner minimization is:\n$$\n\\min_{\\|z\\|_{2} \\leq 1} x^{\\top}(\\bar{\\theta} + P z) = x^{\\top}\\bar{\\theta} - \\|P^{\\top}x\\|_2\n$$\n\n**2. Rewriting the Max-Min Problem**\n\nUsing the strong duality result from Task 1, we can replace the inner minimization problem with its dual maximization problem.\nThe original problem $\\max_{\\|x\\|_{2} \\leq 1} \\left( \\min_{\\|z\\|_{2} \\leq 1} x^{\\top}(\\bar{\\theta} + P z) \\right)$ becomes:\n$$\n\\max_{\\|x\\|_{2} \\leq 1} \\left( x^{\\top}\\bar{\\theta} + \\max_{y_0, y_w: \\|y_w\\|_2 \\leq y_0, y_w = -P^{\\top}x} (-y_0) \\right)\n$$\nThis can be rewritten as a single-level maximization problem over variables $x$, $y_0$, and $y_w$:\n$$\n\\begin{aligned}\n\\max_{x, y_0, y_w} \\quad & x^{\\top}\\bar{\\theta} - y_0 \\\\\n\\text{s.t.} \\quad & \\|x\\|_{2} \\leq 1 \\\\\n& \\|y_w\\|_{2} \\leq y_0 \\\\\n& P^{\\top}x + y_w = 0\n\\end{aligned}\n$$\nThis is a single-level optimization problem, which is also a Second-Order Cone Program (SOCP).\n\n**3. Verification of Slater's Condition**\n\nSlater's condition for conic programs guarantees strong duality. We need to check it for both the inner primal and dual problems.\n\nFor the primal problem: $\\min_{z} (P^{\\top}x)^{\\top}z$ s.t. $Az + b \\in \\mathcal{Q}^3$.\nSlater's condition requires the existence of a feasible point $z_0$ such that $Az_0+b$ is in the interior of the cone $\\mathcal{K}=\\mathcal{Q}^3$. The interior of $\\mathcal{Q}^3$ is $\\text{int}(\\mathcal{Q}^3) = \\{(t,w) \\in \\mathbb{R} \\times \\mathbb{R}^2 : \\|w\\|_2 < t\\}$.\nWe need to find $z_0 \\in \\mathbb{R}^2$ such that $(1, z_0) \\in \\text{int}(\\mathcal{Q}^3)$, which means $\\|z_0\\|_2 < 1$. We can simply choose $z_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Then $\\|z_0\\|_2 = 0 < 1$. Thus, a strictly feasible point exists, and Slater's condition holds for the primal problem for any fixed $x$.\n\nFor the dual problem: $\\max_{y} -b^{\\top}y$ s.t. $A^{\\top}y+c=0, y \\in \\mathcal{Q}^3$.\nSlater's condition requires the existence of a feasible point $y^*$ that lies in the interior of the cone, i.e., $y^* \\in \\text{int}(\\mathcal{Q}^3)$ and satisfies the equality constraint $A^{\\top}y^*+c=0$.\nWe need to find $y^*=(y_0^*, y_w^*) \\in \\mathbb{R} \\times \\mathbb{R}^2$ such that $\\|y_w^*\\|_2 < y_0^*$ and $y_w^* + P^{\\top}x = 0$.\nLet's choose $y_w^* = -P^{\\top}x$. To satisfy the strict inequality, we need to find a $y_0^*$ such that $\\| -P^{\\top}x \\|_2 < y_0^*$, which is $\\|P^{\\top}x\\|_2 < y_0^*$.\nWe can always find such a $y_0^*$, for example, by choosing $y_0^* = \\|P^{\\top}x\\|_2 + 1$.\nThe point $y^* = (\\|P^{\\top}x\\|_2 + 1, -P^{\\top}x)$ is strictly feasible for the dual problem. Thus, Slater's condition holds for the dual as well.\n\nSince Slater's condition holds for both the primal and dual problems, strong duality is justified.\n\n**4. Solving the Resulting Problem**\n\nFrom Task 1 and 2, the problem reduces to solving:\n$$\nV = \\max_{\\|x\\|_{2} \\leq 1} \\left( x^{\\top}\\bar{\\theta} - \\|P^{\\top}x\\|_2 \\right)\n$$\nWe are given $\\bar{\\theta} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $P = 2 I_{2}$. Therefore, $P^{\\top} = (2 I_{2})^{\\top} = 2 I_{2}$.\nThis gives $\\|P^{\\top}x\\|_2 = \\|2 I_{2} x\\|_2 = \\|2x\\|_2 = 2\\|x\\|_2$.\nThe objective function becomes $f(x) = x^{\\top}\\bar{\\theta} - 2\\|x\\|_2$. The optimization problem is:\n$$\nV = \\max_{\\|x\\|_{2} \\leq 1} f(x) = \\max_{\\|x\\|_{2} \\leq 1} \\left( 3x_1 + x_2 - 2\\sqrt{x_1^2 + x_2^2} \\right)\n$$\nThe domain is the closed unit disk in $\\mathbb{R}^2$, which is a compact set. The function $f(x)$ is continuous on this domain, so a maximum exists. We check for the maximum in the interior and on the boundary of the disk.\n\nThe function is not differentiable at $x=0$. At this point, $f(0) = 0$.\n\nFor the interior of the disk ($0 < \\|x\\|_2 < 1$), we seek critical points by setting the gradient of $f(x)$ to zero.\n$$\n\\nabla f(x) = \\bar{\\theta} - 2 \\frac{x}{\\|x\\|_2}\n$$\nSetting $\\nabla f(x) = 0$ gives $x = \\frac{\\|x\\|_2}{2}\\bar{\\theta}$. Taking the Euclidean norm of both sides:\n$$\n\\|x\\|_2 = \\left\\| \\frac{\\|x\\|_2}{2}\\bar{\\theta} \\right\\| = \\frac{\\|x\\|_2}{2}\\|\\bar{\\theta}\\|_2\n$$\nSince we are in the interior, $\\|x\\|_2 > 0$, so we can divide by it to get $1 = \\frac{1}{2}\\|\\bar{\\theta}\\|_2$, which implies $\\|\\bar{\\theta}\\|_2 = 2$.\nLet's calculate $\\|\\bar{\\theta}\\|_2$:\n$$\n\\|\\bar{\\theta}\\|_2 = \\sqrt{3^2 + 1^2} = \\sqrt{9+1} = \\sqrt{10}\n$$\nSince $\\|\\bar{\\theta}\\|_2 = \\sqrt{10} \\neq 2$, there are no critical points in the interior of the disk.\n\nThe maximum must therefore lie on the boundary, where $\\|x\\|_2 = 1$. The problem on the boundary is:\n$$\n\\max_{\\|x\\|_{2} = 1} \\left( x^{\\top}\\bar{\\theta} - 2(1) \\right) = \\left( \\max_{\\|x\\|_{2} = 1} x^{\\top}\\bar{\\theta} \\right) - 2\n$$\nBy the Cauchy-Schwarz inequality, $x^{\\top}\\bar{\\theta} \\leq \\|x\\|_2 \\|\\bar{\\theta}\\|_2$. The maximum value of $x^{\\top}\\bar{\\theta}$ for $\\|x\\|_2=1$ is $\\|\\bar{\\theta}\\|_2$, achieved when $x$ is aligned with $\\bar{\\theta}$, i.e., $x = \\frac{\\bar{\\theta}}{\\|\\bar{\\theta}\\|_2}$.\nThe maximum value of $f(x)$ on the boundary is $\\|\\bar{\\theta}\\|_2 - 2 = \\sqrt{10} - 2$.\n\nComparing the values, we have $f(0) = 0$ and the boundary maximum $\\sqrt{10}-2$. Since $\\sqrt{10} > \\sqrt{4} = 2$, the value $\\sqrt{10}-2$ is positive.\nThus, the global maximum value is $V = \\sqrt{10}-2$.\nThis is the optimal robust value of the objective function.", "answer": "$$\\boxed{\\sqrt{10}-2}$$", "id": "3111113"}]}