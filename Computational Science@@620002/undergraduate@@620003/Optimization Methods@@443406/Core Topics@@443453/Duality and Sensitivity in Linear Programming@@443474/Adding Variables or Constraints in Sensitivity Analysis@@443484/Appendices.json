{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it is crucial to build intuition. This first practice addresses the most fundamental question of sensitivity analysis: when does adding a new constraint actually change the optimal outcome? By exploring a simple two-dimensional linear program, you will determine the precise threshold at which a new constraint becomes \"active\" and forces a change in the optimal solution. This exercise [@problem_id:3095369] provides a clear geometric understanding of how constraints shape the feasible region and impact optimality.", "problem": "Consider the Linear Programming (LP) problem of minimizing a linear cost over a polyhedral feasible set. Let the decision vector be $x \\in \\mathbb{R}^{2}$ and consider the following instance:\nMinimize $c^{\\top}x$ with $$c = \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix}$$ subject to the constraints\n$x_{1} + x_{2} \\leq 6$, $2 x_{1} + x_{2} \\leq 10$, $x_{1} \\geq 0$, and $x_{2} \\geq 0$.\nYou are asked to analyze the sensitivity of the optimal solution to the addition of a new upper bound constraint on the second variable, $x_{2} \\leq U$, where $U \\in \\mathbb{R}$ is a parameter. Starting only from fundamental properties of linear programs (such as the polyhedral nature of the feasible region and the fact that optimal solutions can be found at extreme points), determine the threshold value $U^{\\star}$ such that:\n- for all $U \\geq U^{\\star}$, the added constraint $x_{2} \\leq U$ is nonbinding at the optimal solution of the original problem, and\n- for all $U  U^{\\star}$, the added constraint excludes the original optimal solution and thereby changes the optimal basis.\nProvide the value of $U^{\\star}$ as a single real number. No rounding is required.", "solution": "The problem asks for a threshold value $U^{\\star}$ for a new constraint $x_{2} \\leq U$ added to an existing Linear Programming (LP) problem. The threshold is defined by the point at which the new constraint begins to alter the original optimal solution. To find this threshold, we must first solve the original LP problem.\n\nThe original LP problem is stated as:\nMinimize $z = c^{\\top}x = -x_{1} - 3x_{2}$\nSubject to:\n$x_{1} + x_{2} \\leq 6$\n$2x_{1} + x_{2} \\leq 10$\n$x_{1} \\geq 0$\n$x_{2} \\geq 0$\n\nHere, the decision vector is $$x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$$. The feasible region is a convex polyhedron in the first quadrant of the $x_{1}$-$x_{2}$ plane. According to the fundamental theorem of linear programming, if an optimal solution exists, it must occur at one of the extreme points (vertices) of this feasible region. The vertices are the points of intersection of the boundary lines of the constraints.\n\nLet's identify these vertices:\n1.  The intersection of $x_{1} = 0$ and $x_{2} = 0$. This gives the vertex $V_{1} = (0, 0)$.\n2.  The intersection of $x_{1} = 0$ and $x_{1} + x_{2} = 6$. Substituting $x_{1} = 0$ gives $x_{2} = 6$. We must check if this point satisfies the other constraints: $2(0) + 6 = 6 \\leq 10$. It does. So, we have the vertex $V_{2} = (0, 6)$.\n3.  The intersection of $x_{2} = 0$ and $2x_{1} + x_{2} = 10$. Substituting $x_{2} = 0$ gives $2x_{1} = 10$, so $x_{1} = 5$. We must check if this point satisfies the other constraints: $5 + 0 = 5 \\leq 6$. It does. So, we have the vertex $V_{3} = (5, 0)$.\n4.  The intersection of $x_{1} + x_{2} = 6$ and $2x_{1} + x_{2} = 10$. We can solve this system of two linear equations. Subtracting the first equation from the second gives:\n    $(2x_{1} + x_{2}) - (x_{1} + x_{2}) = 10 - 6$\n    $x_{1} = 4$\n    Substituting $x_{1} = 4$ into the first equation: $4 + x_{2} = 6$, which gives $x_{2} = 2$.\n    The other constraints $x_{1} \\geq 0$ and $x_{2} \\geq 0$ are satisfied. So, we have the vertex $V_{4} = (4, 2)$.\n\nThe feasible region is a polygon defined by the vertices $V_{1}$, $V_{3}$, $V_{4}$, and $V_{2}$. Now, we evaluate the objective function $z = -x_{1} - 3x_{2}$ at each of these vertices to find the minimum value.\n\n-   At $V_{1}(0, 0)$: $z = -1(0) - 3(0) = 0$.\n-   At $V_{2}(0, 6)$: $z = -1(0) - 3(6) = -18$.\n-   At $V_{3}(5, 0)$: $z = -1(5) - 3(0) = -5$.\n-   At $V_{4}(4, 2)$: $z = -1(4) - 3(2) = -4 - 6 = -10$.\n\nThe minimum value of the objective function is $-18$, which occurs at the vertex $V_{2}$. Therefore, the optimal solution to the original problem is $$x^{\\star}_{\\text{orig}} = \\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix}$$.\n\nNow, we introduce the new constraint $x_{2} \\leq U$. The new feasible region is the intersection of the original feasible region with the half-plane defined by this new constraint.\n\nThe problem asks for the threshold $U^{\\star}$ such that for $U  U^{\\star}$, the new constraint \"excludes the original optimal solution\". The original optimal solution is the point $(0, 6)$. For this point to be included in the new feasible region, its coordinates must satisfy the new constraint, i.e., $6 \\leq U$.\nConversely, the original optimal solution $(0, 6)$ is excluded from the new feasible region if its coordinates violate the new constraint, i.e., $6 > U$.\n\n-   If $U \\geq 6$, the original optimal solution $x^{\\star}_{\\text{orig}} = (0, 6)$ is still part of the new feasible region. Since we are minimizing the same objective function over a new feasible region that is a subset of the original one, and the original minimizer is still available, it must remain the optimal solution for the new problem. The optimal value remains $-18$.\n\n-   If $U  6$, the original optimal solution $x^{\\star}_{\\text{orig}} = (0, 6)$ is no longer feasible. The new feasible region is a \"truncated\" version of the original one, and a new optimal solution must be found on the boundary of this new region. The addition of the constraint changes the problem's solution.\n\nThe threshold value $U^{\\star}$ represents the critical value of the parameter $U$ at which this change in behavior occurs. This transition happens precisely when $U$ is equal to the $x_{2}$-coordinate of the original optimal solution.\n\nThus, the threshold value is $U^{\\star} = 6$.\n\nLet's verify this against the problem's conditions:\n1.  \"for all $U \\geq U^{\\star}$ (i.e., $U \\geq 6$), the added constraint $x_{2} \\leq U$ is nonbinding at the optimal solution of the original problem\". With $U^{\\star}=6$, if $U > 6$, the original optimum $(0,6)$ satisfies $x_2  U$, so the constraint is nonbinding. If $U = 6$, it satisfies $x_2=U$, so the constraint is binding. The language \"nonbinding\" might be interpreted as \"does not change the location of the optimum\". In this sense, for all $U \\ge 6$, the optimum remains at $(0,6)$.\n\n2.  \"for all $U  U^{\\star}$ (i.e., $U  6$), the added constraint excludes the original optimal solution and thereby changes the optimal basis.\" As shown, if $U  6$, the point $(0,6)$ is no longer feasible because $6 \\not\\leq U$. This perfectly matches our analysis.\n\nThe value that separates these two regimes is the $x_{2}$-coordinate of the original optimal solution.\n$U^{\\star} = (x^{\\star}_{\\text{orig}})_{2} = 6$.", "answer": "$$\\boxed{6}$$", "id": "3095369"}, {"introduction": "When a new constraint invalidates our current optimal solution, we need an efficient way to find the new optimum without re-solving the entire problem from scratch. This practice [@problem_id:3095270] demonstrates the power of the dual simplex method, an elegant algorithm for re-optimizing a system that has become primal-infeasible but remains dual-feasible. You will perform a single pivot to restore feasibility and, in doing so, discover the deep connection between this process and the economic insight provided by shadow prices.", "problem": "Consider the linear program in inequality form with slack variables:\n- Maximize the objective $z = 3 x_1 + 2 x_2$.\n- Subject to the constraints\n  - $x_1 + x_2 \\leq 4$,\n  - $x_1 \\leq 2$,\n  - $x_2 \\leq 3$,\n  - $x_1 \\geq 0$, $x_2 \\geq 0$.\nIntroduce slack variables $s_1$, $s_2$, and $s_3$ to write the constraints as $x_1 + x_2 + s_1 = 4$, $x_1 + s_2 = 2$, $x_2 + s_3 = 3$, with $s_1, s_2, s_3 \\geq 0$.\n\nYou are told that the current optimal basis is known and consists of the basic variables $\\{x_1, x_2, s_3\\}$ with basic feasible solution $(x_1, x_2, s_1, s_2, s_3) = (2, 2, 0, 0, 1)$ and objective value $z^{\\star} = 10$.\n\nNow an additional linear constraint is proposed that cuts off the current optimal vertex:\n- New constraint $\\tilde{c}$: $x_1 + x_2 \\leq 3.9$,\nwith an added slack variable $s_4 \\geq 0$ so that $x_1 + x_2 + s_4 = 3.9$.\n\nStarting only from the core definitions of linear programming feasibility and optimality, and the primalâ€“dual relationship together with complementary slackness (no pre-given shortcuts), do the following:\n- Justify that the new constraint $\\tilde{c}$ renders the current basic feasible solution infeasible.\n- Design a minimal simplex pivot path from the known optimal basis to a new optimal basic feasible solution that restores feasibility and optimality after adding $\\tilde{c}$.\n- Analyze the local sensitivity of the optimal objective to decreasing the right-hand side of constraints of the form $x_1 + x_2 \\leq b$ in a neighborhood of $b = 4$, and use this to verify the change in the optimal objective value caused by adding $\\tilde{c}$.\n\nWhat is the new optimal objective value after adding the constraint $\\tilde{c}$? Round your final numerical answer to four significant figures.", "solution": "The user has provided a linear programming (LP) problem and has asked for an analysis of the effect of adding a new constraint. The tasks include justifying the infeasibility of the current optimal solution under the new constraint, finding the new optimal solution using a simplex pivot, and verifying the result using sensitivity analysis derived from the primal-dual relationship.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Objective Function (Maximize):** $z = 3 x_1 + 2 x_2$\n- **Original Constraints:**\n  - $x_1 + x_2 \\leq 4$\n  - $x_1 \\leq 2$\n  - $x_2 \\leq 3$\n  - $x_1 \\geq 0$, $x_2 \\geq 0$\n- **Standard Form with Slack Variables ($s_1, s_2, s_3 \\geq 0$):**\n  - $x_1 + x_2 + s_1 = 4$\n  - $x_1 + s_2 = 2$\n  - $x_2 + s_3 = 3$\n- **Current Optimal Solution Information:**\n  - **Basic Variables:** $\\{x_1, x_2, s_3\\}$\n  - **Basic Feasible Solution (BFS):** $(x_1, x_2, s_1, s_2, s_3) = (2, 2, 0, 0, 1)$\n  - **Optimal Objective Value:** $z^{\\star} = 10$\n- **New Proposed Constraint ($\\tilde{c}$):**\n  - $x_1 + x_2 \\leq 3.9$\n  - **Standard Form with Slack Variable ($s_4 \\geq 0$):** $x_1 + x_2 + s_4 = 3.9$\n\n**1.2. Validate Using Extracted Givens**\nFirst, we verify the provided optimal solution for the original problem.\n- **Feasibility:**\n  - $x_1 + x_2 = 2 + 2 = 4 \\leq 4$ (Constraint 1 holds, $s_1=0$)\n  - $x_1 = 2 \\leq 2$ (Constraint 2 holds, $s_2=0$)\n  - $x_2 = 2 \\leq 3$ (Constraint 3 holds, $s_3=1$)\n  - $x_1=2 \\geq 0$, $x_2=2 \\geq 0$ (Non-negativity holds)\nThe solution is feasible.\n- **Objective Value:**\n  - $z = 3x_1 + 2x_2 = 3(2) + 2(2) = 6 + 4 = 10$. This matches the given $z^\\star$.\n- **Basis:**\n  - The basic variables are $\\{x_1, x_2, s_3\\}$ with values $\\{2, 2, 1\\}$, all non-negative. The non-basic variables are $\\{s_1, s_2\\}$ with values $\\{0, 0\\}$. This is a valid basic feasible solution.\n\nThe problem statement is self-contained, scientifically grounded in the theory of linear programming, and internally consistent.\n\n**1.3. Verdict**\nThe problem is valid.\n\n### Step 2: Solution Derivation\n\n**2.1. Infeasibility of the Current Solution with the New Constraint**\nThe current optimal solution is $(x_1, x_2) = (2, 2)$. The new constraint is $\\tilde{c}: x_1 + x_2 \\leq 3.9$.\nSubstituting the values of $x_1$ and $x_2$ into the new constraint:\n$$2 + 2 = 4$$\nThe value $4$ is not less than or equal to $3.9$. Thus, the current optimal solution $(x_1, x_2) = (2, 2)$ violates the new constraint and is no longer feasible for the modified problem.\n\n**2.2. Finding the New Optimum via the Dual Simplex Method**\nWe begin by constructing the final simplex tableau for the original problem. The system of equations is:\n$z - 3x_1 - 2x_2 = 0$\n$x_1 + x_2 + s_1 = 4$\n$x_1 + s_2 = 2$\n$x_2 + s_3 = 3$\n\nThe optimal basis is $B=\\{x_1, x_2, s_3\\}$ and the non-basic variables are $N=\\{s_1, s_2\\}$. We need to express the basic variables and the objective function in terms of the non-basic variables. From the second and first constraints (with $s_1=s_2=0$):\n$x_1 = 2 - s_2$\n$x_2 = 4 - x_1 - s_1 = 4 - (2 - s_2) - s_1 = 2 - s_1 + s_2$\nAnd from the third constraint:\n$s_3 = 3 - x_2 = 3 - (2 - s_1 + s_2) = 1 + s_1 - s_2$\n\nSubstitute $x_1$ and $x_2$ into the objective function:\n$z = 3(2-s_2) + 2(2-s_1+s_2) = 6 - 3s_2 + 4 - 2s_1 + 2s_2 = 10 - 2s_1 - s_2$\nThis can be rewritten as $z + 2s_1 + s_2 = 10$.\n\nThe final simplex tableau for the original problem represents this dictionary:\n$$\n\\begin{array}{c|c|ccccc|c}\n\\text{Basis}  z  x_1  x_2  s_1  s_2  s_3  \\text{RHS} \\\\\n\\hline\nz  1  0  0  2  1  0  10 \\\\\n\\hline\nx_1  0  1  0  0  1  0  2 \\\\\nx_2  0  0  1  1  -1  0  2 \\\\\ns_3  0  0  0  -1  1  1  1\n\\end{array}\n$$\nThe z-row coefficients for non-basic variables are non-negative ($2$ and $1$), so this is indeed an optimal tableau for a maximization problem.\n\nNow, we add the new constraint $x_1 + x_2 + s_4 = 3.9$. We must express this constraint in terms of the non-basic variables $s_1, s_2$:\n$(2 - s_2) + (2 - s_1 + s_2) + s_4 = 3.9$\n$4 - s_1 + s_4 = 3.9$\n$s_4 = -0.1 + s_1$\nThis gives the new constraint row: $-s_1 + s_4 = -0.1$. We add this to the tableau, with $s_4$ as the new basic variable for this row.\n\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis}  z  x_1  x_2  s_1  s_2  s_3  s_4  \\text{RHS} \\\\\n\\hline\nz  1  0  0  2  1  0  0  10 \\\\\n\\hline\nx_1  0  1  0  0  1  0  0  2 \\\\\nx_2  0  0  1  1  -1  0  0  2 \\\\\ns_3  0  0  0  -1  1  1  0  1 \\\\\ns_4  0  0  0  -1  0  0  1  -0.1\n\\end{array}\n$$\nThe z-row is still optimal (all coefficients $\\ge 0$). However, the solution is not primal feasible because $s_4 = -0.1$, which violates the non-negativity constraint. This is the correct setup for the dual simplex method.\n\n**Dual Simplex Pivot:**\n1.  **Select Pivot Row:** Choose the row with the most negative value in the RHS column. This is the $s_4$ row. The variable $s_4$ will leave the basis.\n2.  **Select Pivot Column:** Find the minimum ratio $|\\frac{z_j-c_j}{a_{ij}}|$ for all $j$ where the pivot row entry $a_{ij}$ is negative. The pivot row ($i=s_4$) has only one negative entry: $a_{s_4, s_1} = -1$.\n    The ratio is $|\\frac{2}{-1}|=2$. Thus, the variable $s_1$ will enter the basis. The pivot element is $-1$.\n3.  **Perform Pivot:**\n    - Divide the pivot row (the $s_4$ row) by the pivot element ($-1$) to make the pivot entry $1$. The new row becomes the $s_1$ row in the next tableau:\n    Row $s_1' = (0, 0, 0, 1, 0, 0, -1 | 0.1)$.\n    - Use this new row to make all other entries in the pivot column ($s_1$ column) zero.\n        - New z-row: $\\text{Row } z' = \\text{Row } z - 2 \\cdot \\text{Row } s_1' = (1, 0, 0, 0, 1, 0, 2 | 9.8)$.\n        - New $x_1$-row: The entry is already $0$, so no change. Row $x_1'=$ Row $x_1$.\n        - New $x_2$-row: $\\text{Row } x_2' = \\text{Row } x_2 - 1 \\cdot \\text{Row } s_1' = (0, 0, 1, 0, -1, 0, 1 | 1.9)$.\n        - New $s_3$-row: $\\text{Row } s_3' = \\text{Row } s_3 - (-1) \\cdot \\text{Row } s_1' = \\text{Row } s_3 + \\text{Row } s_1' = (0, 0, 0, 0, 1, 1, -1 | 1.1)$.\n\nThe new tableau is:\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis}  z  x_1  x_2  s_1  s_2  s_3  s_4  \\text{RHS} \\\\\n\\hline\nz  1  0  0  0  1  0  2  9.8 \\\\\n\\hline\nx_1  0  1  0  0  1  0  0  2 \\\\\nx_2  0  0  1  0  -1  0  1  1.9 \\\\\ns_3  0  0  0  0  1  1  -1  1.1 \\\\\ns_1  0  0  0  1  0  0  -1  0.1\n\\end{array}\n$$\nThe RHS column is now all non-negative, so the solution is primal feasible. The z-row coefficients are all non-negative, so it remains dual feasible (optimal).\nThe new optimal solution is: a BFS with basic variables $\\{x_1, x_2, s_3, s_1\\}$ and non-basic variables $\\{s_2, s_4\\}$.\n- $x_1 = 2$\n- $x_2 = 1.9$\n- $s_3 = 1.1$\n- $s_1 = 0.1$\n- $s_2=0$, $s_4=0$\nThe new optimal objective value is $z^{**} = 9.8$.\n\n**2.3. Verification using Sensitivity Analysis and Primal-Dual Theory**\nThe original primal LP (P) and its dual (D) are:\n(P) Maximize $z = 3x_1 + 2x_2$ s.t.\n$x_1 + x_2 \\leq 4$\n$x_1 \\leq 2$\n$x_2 \\leq 3$\n$x_1, x_2 \\geq 0$\n\n(D) Minimize $w = 4y_1 + 2y_2 + 3y_3$ s.t.\n$y_1 + y_2 \\geq 3$\n$y_1 + y_3 \\geq 2$\n$y_1, y_2, y_3 \\geq 0$\n\nThe original optimal primal solution is $(x_1, x_2) = (2, 2)$ with $z^\\star=10$.\nBy complementary slackness:\n- $x_1=2 > 0 \\implies y_1 + y_2 = 3$.\n- $x_2=2 > 0 \\implies y_1 + y_3 = 2$.\n- The constraint $x_2 \\leq 3$ is not binding ($2  3$, so slack $s_3=1 > 0$) $\\implies y_3 = 0$.\n- The constraints $x_1+x_2 \\leq 4$ and $x_1 \\leq 2$ are binding (slacks $s_1=0, s_2=0$), so their dual variables $y_1, y_2$ can be non-zero.\n\nSolving for the dual variables:\n$y_3=0 \\implies y_1+0=2 \\implies y_1=2$.\nThen $2+y_2=3 \\implies y_2=1$.\nThe optimal dual solution is $(y_1, y_2, y_3) = (2, 1, 0)$. This solution is dual feasible.\nThe dual variable $y_1=2$ is the shadow price for the first constraint, $x_1 + x_2 \\leq 4$. It represents the rate of change of the optimal objective value with respect to a change in the right-hand side (RHS) of this constraint, $\\frac{\\partial z^\\star}{\\partial b_1} = y_1=2$.\n\nThe introduction of the new constraint $x_1 + x_2 \\leq 3.9$ makes the original constraint $x_1 + x_2 \\leq 4$ redundant. This is equivalent to changing the RHS of the first constraint from $b_1=4$ to $b_1'=3.9$. The change is $\\Delta b_1 = 3.9 - 4 = -0.1$.\nThis change is small, so we can predict the change in the objective value as:\n$$ \\Delta z^\\star \\approx y_1 \\Delta b_1 = 2 \\times (-0.1) = -0.2 $$\nThe predicted new optimal value is $z^\\star_{new} = z^\\star + \\Delta z^\\star = 10 - 0.2 = 9.8$.\n\nThis prediction is exact as long as the change $\\Delta b_1$ is within the range where the optimal basis $\\{x_1, x_2, s_3\\}$ does not change. This range is determined by keeping the basic solution $x_B' = A_B^{-1}b'$ non-negative, where $b' = [4+\\Delta b_1, 2, 3]^T$. We found the dictionary expressions to be $x_1=2$, $x_2=2+\\Delta b_1$, $s_3=1-\\Delta b_1$. Feasibility requires $2 \\ge 0$, $2+\\Delta b_1 \\ge 0 \\implies \\Delta b_1 \\ge -2$, and $1-\\Delta b_1 \\ge 0 \\implies \\Delta b_1 \\le 1$. The change $\\Delta b_1 = -0.1$ is within the interval $[-2, 1]$, so the basis does not change and the shadow price calculation is exact.\n\nThe new optimal value of $9.8$ calculated by the dual simplex method is confirmed by the sensitivity analysis.\n\nThe final question asks for the new optimal objective value after adding the constraint $\\tilde{c}$, rounded to four significant figures. The calculated value is exactly $9.8$. To express this with four significant figures, we write $9.800$.", "answer": "$$\\boxed{9.800}$$", "id": "3095270"}, {"introduction": "Sensitivity analysis also equips us to evaluate new opportunities, which in linear programming corresponds to adding new variables. This exercise [@problem_id:3095291] shifts our focus from constraints to decision variables, tackling the question: at what point does a potential new activity become profitable enough to include in our optimal plan? By calculating the reduced cost as a function of a parameter, you will identify the critical value that makes a new variable \"attractive\" to enter the basis, providing a quantitative tool for strategic decision-making.", "problem": "Consider a Linear Programming (LP) problem in primal standard form: maximize $c^{\\top} x$ subject to $A x = b$, $x \\ge 0$. The current optimal basis uses the $2 \\times 2$ basis matrix $B$ and basic cost vector $c_B$ given by\n$$\nB = \\begin{pmatrix} 2  1 \\\\ 1  1 \\end{pmatrix}, \\quad c_B = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}.\n$$\nA new decision variable $x_{+}$ is proposed to be added to the model. Its constraint column is scaled by a nonnegative parameter $\\alpha$, namely $A_{+}(\\alpha) = \\alpha\\, a$ with\n$$\na = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\n$$\nand its objective coefficient is $c_{+} = 5$. Using the definition of the simplex multipliers $y^{\\top} = c_B^{\\top} B^{-1}$ and the reduced cost $r_{+}(\\alpha) = c_{+} - y^{\\top} A_{+}(\\alpha)$ for a maximization problem, determine the critical value $\\alpha^{\\star}$ at which the reduced cost $r_{+}(\\alpha)$ crosses zero, thereby making the new variable $x_{+}$ marginally attractive to enter the basis.\n\nProvide your final answer as a single real number in simplest exact form. No rounding is required.", "solution": "We begin from the primal standard form of Linear Programming (LP): maximize $c^{\\top} x$ subject to $A x = b$, $x \\ge 0$. For a given basis $B$ with basic costs $c_B$, the simplex multipliers (also called dual prices) are defined by\n$$\ny^{\\top} = c_B^{\\top} B^{-1}.\n$$\nFor any nonbasic column $A_j$, the reduced cost in a maximization problem is defined by\n$$\nr_j = c_j - y^{\\top} A_j.\n$$\nA nonbasic variable is attractive to enter the basis when $r_j > 0$; the threshold at which it becomes marginally attractive is where $r_j = 0$.\n\nHere, the new variable $x_{+}$ has column $A_{+}(\\alpha) = \\alpha\\, a$ with $a = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and objective coefficient $c_{+} = 5$. Therefore its reduced cost as a function of $\\alpha$ is\n$$\nr_{+}(\\alpha) = c_{+} - y^{\\top} A_{+}(\\alpha) = c_{+} - \\alpha\\, y^{\\top} a.\n$$\n\nWe compute $y^{\\top}$ from the given $B$ and $c_B$. First find $B^{-1}$. With\n$$\nB = \\begin{pmatrix} 2  1 \\\\ 1  1 \\end{pmatrix},\n$$\nthe determinant is $\\det(B) = 2 \\cdot 1 - 1 \\cdot 1 = 1$, hence\n$$\nB^{-1} = \\begin{pmatrix} 1  -1 \\\\ -1  2 \\end{pmatrix}.\n$$\nThus,\n$$\ny^{\\top} = c_B^{\\top} B^{-1} = \\begin{pmatrix} 4  1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} 4 \\cdot 1 + 1 \\cdot (-1)  4 \\cdot (-1) + 1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 3  -2 \\end{pmatrix}.\n$$\n\nNext compute $y^{\\top} a$:\n$$\ny^{\\top} a = \\begin{pmatrix} 3  -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 3 \\cdot 2 + (-2) \\cdot 1 = 6 - 2 = 4.\n$$\n\nTherefore,\n$$\nr_{+}(\\alpha) = c_{+} - \\alpha\\, (y^{\\top} a) = 5 - 4 \\alpha.\n$$\nThe critical value $\\alpha^{\\star}$ solves $r_{+}(\\alpha^{\\star}) = 0$:\n$$\n5 - 4 \\alpha^{\\star} = 0 \\quad \\Longrightarrow \\quad \\alpha^{\\star} = \\frac{5}{4}.\n$$\n\nSince $y^{\\top} a = 4 > 0$, for the maximization problem the new variable is attractive when $r_{+}(\\alpha) > 0$, i.e., when $\\alpha  \\alpha^{\\star}$. The requested critical value where the reduced cost crosses zero is $\\alpha^{\\star} = \\frac{5}{4}$.", "answer": "$$\\boxed{\\frac{5}{4}}$$", "id": "3095291"}]}