{"hands_on_practices": [{"introduction": "To build a strong foundation, we first explore the core mechanics of trust-region radius updates in an idealized setting. This exercise simplifies the problem by using a convex quadratic function, where the quadratic model is a perfect representation of the objective (i.e., $\\rho_k = 1$). This allows you to analytically derive the exact evolution of the trust-region radius $\\Delta_k$ and the gradient norm $\\|g_k\\|$, providing deep insight into how the algorithm transitions from boundary-constrained steps to the final, optimal interior step. [@problem_id:3193997]", "problem": "Consider a trust-region method applied to minimizing a strictly convex quadratic objective function $f(x) = \\frac{1}{2} x^{\\top} Q x + b^{\\top} x$, where $Q \\in \\mathbb{R}^{2 \\times 2}$ is known and symmetric positive definite. Suppose $Q = 2 I$, where $I$ is the identity matrix, and the initial point $x_0$ is such that the initial gradient is $g_0 = \\nabla f(x_0) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, so $\\|g_0\\| = 1$. The trust-region subproblem at iteration $k$ is to minimize the quadratic model $m_k(s) = f(x_k) + g_k^{\\top} s + \\frac{1}{2} s^{\\top} Q s$ subject to $\\|s\\| \\leq \\Delta_k$. Assume the idealized case in which the ratio of actual to predicted reduction satisfies $\\rho_k = 1$ for all $k$ and that at every iteration the exact global minimizer $s_k$ of the trust-region subproblem is computed.\n\nUse the following radius update rule:\n- If the computed step satisfies $\\|s_k\\| = \\Delta_k$ (i.e., the step lies on the boundary of the trust region), then set $\\Delta_{k+1} = \\tau \\Delta_k$ with $\\tau = 1.5$.\n- If the computed step satisfies $\\|s_k\\|  \\Delta_k$ (i.e., the step lies strictly inside the trust region), then leave the radius unchanged: $\\Delta_{k+1} = \\Delta_k$.\n\nLet the initial trust-region radius be $\\Delta_0 = 0.1$. Under these assumptions and update rule:\n1. Derive from first principles the exact solution $s_k$ of the trust-region subproblem for the quadratic model with $Q = 2 I$, distinguishing the boundary and interior cases, and show how the gradient $\\|g_k\\|$ evolves.\n2. Using the derived relations, compute the exact sequence $\\Delta_0, \\Delta_1, \\Delta_2, \\ldots$ up to and including the first iteration index $K$ for which the step is interior (i.e., $\\|s_K\\|  \\Delta_K$), which leads to convergence in the next iterate.\n3. Briefly explain the tradeoff between aggressive trust-region radius growth (larger $\\tau$) and numerical stability in the context of trust-region methods for quadratics.\n\nProvide your final answer as the row vector containing $(\\Delta_0, \\Delta_1, \\ldots, \\Delta_K, K)$. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the theory of numerical optimization, well-posed, and presented objectively. It asks for the analysis of a trust-region method on a strictly convex quadratic function, which is a standard and illustrative case in optimization literature. All required parameters and rules are provided.\n\nThe problem will be solved in three parts as requested.\n\nPart 1: Derivation of the subproblem solution and gradient evolution.\n\nThe trust-region subproblem at iteration $k$ is to find a step $s_k$ that solves:\n$$ \\min_{s} \\quad m_k(s) = f(x_k) + g_k^{\\top} s + \\frac{1}{2} s^{\\top} Q s $$\n$$ \\text{subject to} \\quad \\|s\\| \\leq \\Delta_k $$\nwhere $g_k = \\nabla f(x_k)$ and $Q = 2I$. The model is $m_k(s) = f(x_k) + g_k^{\\top} s + s^{\\top}s$.\n\nThis is a convex optimization problem, so the Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality. The Lagrangian is:\n$$ \\mathcal{L}(s, \\lambda) = m_k(s) + \\frac{\\lambda}{2} (s^{\\top}s - \\Delta_k^2) = f(x_k) + g_k^{\\top} s + s^{\\top}s + \\frac{\\lambda}{2} (s^{\\top}s - \\Delta_k^2) $$\nThe KKT conditions are:\n1. Stationarity: $\\nabla_s \\mathcal{L}(s, \\lambda) = g_k + 2s + \\lambda s = g_k + (2+\\lambda)s = 0$\n2. Primal feasibility: $\\|s\\|^2 \\leq \\Delta_k^2$\n3. Dual feasibility: $\\lambda \\geq 0$\n4. Complementary slackness: $\\lambda (\\|s\\|^2 - \\Delta_k^2) = 0$\n\nFrom the stationarity condition, we have $(2+\\lambda)s = -g_k$. Since $Q=2I$ is positive definite, $2+\\lambda > 0$ for any $\\lambda \\ge 0$. Thus, we can write the solution $s$ as:\n$$ s_k(\\lambda) = -\\frac{1}{2+\\lambda} g_k $$\n\nWe analyze two cases based on the complementary slackness condition.\n\nCase A: Interior Solution ($\\|s_k\\|  \\Delta_k$).\nThe constraint is inactive, which implies $\\lambda=0$. The step is the unconstrained minimizer of the model $m_k(s)$:\n$$ s_k = -\\frac{1}{2} g_k $$\nThis case occurs if the norm of the unconstrained step is within the trust-region radius:\n$$ \\|s_k\\| = \\|-\\frac{1}{2} g_k\\| = \\frac{1}{2} \\|g_k\\|  \\Delta_k $$\nThis is equivalent to the condition $\\|g_k\\|  2\\Delta_k$.\n\nCase B: Boundary Solution ($\\|s_k\\| = \\Delta_k$).\nThe constraint is active, which implies $\\lambda \\ge 0$. We must solve for $\\lambda$ such that $\\|s_k(\\lambda)\\| = \\Delta_k$.\n$$ \\|s_k(\\lambda)\\| = \\left\\|-\\frac{1}{2+\\lambda} g_k\\right\\| = \\frac{1}{2+\\lambda} \\|g_k\\| = \\Delta_k $$\nSolving for $2+\\lambda$ gives $2+\\lambda = \\frac{\\|g_k\\|}{\\Delta_k}$.\nSubstituting this back into the expression for $s_k$:\n$$ s_k = -\\frac{1}{(\\|g_k\\|/\\Delta_k)} g_k = -\\Delta_k \\frac{g_k}{\\|g_k\\|} $$\nThis is a step in the steepest descent direction with a length equal to the trust-region radius $\\Delta_k$. This case occurs when the unconstrained step would lie outside or on the boundary of the trust region, i.e., $\\frac{1}{2}\\|g_k\\| \\ge \\Delta_k$, or $\\|g_k\\| \\ge 2\\Delta_k$.\n\nNext, we derive the gradient evolution. The objective function is $f(x) = \\frac{1}{2}x^\\top Q x + b^\\top x$, so its gradient is $\\nabla f(x) = Qx+b$.\nThe new iterate is $x_{k+1} = x_k + s_k$. The new gradient is:\n$$ g_{k+1} = \\nabla f(x_{k+1}) = Q(x_k + s_k) + b = (Qx_k+b) + Qs_k = g_k + Qs_k $$\nWith $Q=2I$, this becomes $g_{k+1} = g_k + 2s_k$.\n\nLet's examine the gradient update in each case:\n- For an interior solution ($s_k = -\\frac{1}{2} g_k$):\n  $$ g_{k+1} = g_k + 2(-\\frac{1}{2} g_k) = g_k - g_k = 0 $$\n  The algorithm finds the exact minimum in this step.\n- For a boundary solution ($s_k = -\\Delta_k \\frac{g_k}{\\|g_k\\|}$):\n  $$ g_{k+1} = g_k + 2\\left(-\\Delta_k \\frac{g_k}{\\|g_k\\|}\\right) = \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) g_k $$\n  The new gradient $g_{k+1}$ is collinear with $g_k$. The norm of the new gradient is:\n  $$ \\|g_{k+1}\\| = \\left\\| \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) g_k \\right\\| = \\left|1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right| \\|g_k\\| $$\n  Since this case occurs when $\\|g_k\\| \\ge 2\\Delta_k$, the term $1 - \\frac{2\\Delta_k}{\\|g_k\\|}$ is non-negative.\n  $$ \\|g_{k+1}\\| = \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) \\|g_k\\| = \\|g_k\\| - 2\\Delta_k $$\n\nPart 2: Computation of the sequence $\\Delta_0, \\Delta_1, \\ldots, \\Delta_K$.\n\nWe are given $\\|g_0\\| = 1$ and $\\Delta_0 = 0.1$. The radius update is $\\Delta_{k+1} = 1.5 \\Delta_k$ for a boundary step and $\\Delta_{k+1} = \\Delta_k$ for an interior step. We seek the first index $K$ where the step becomes interior.\n\nIteration $k=0$:\n- We have $\\|g_0\\| = 1$ and $\\Delta_0 = 0.1$.\n- We check the condition: $\\|g_0\\| = 1$ versus $2\\Delta_0 = 2(0.1) = 0.2$.\n- Since $1 \\ge 0.2$, we are in the boundary case. The step is $\\|s_0\\|=\\Delta_0$.\n- The new gradient norm is $\\|g_1\\| = \\|g_0\\| - 2\\Delta_0 = 1 - 0.2 = 0.8$.\n- The new radius is $\\Delta_1 = 1.5 \\Delta_0 = 1.5 \\times 0.1 = 0.15$.\n\nIteration $k=1$:\n- We have $\\|g_1\\| = 0.8$ and $\\Delta_1 = 0.15$.\n- We check the condition: $\\|g_1\\| = 0.8$ versus $2\\Delta_1 = 2(0.15) = 0.3$.\n- Since $0.8 \\ge 0.3$, we are in the boundary case. The step is $\\|s_1\\|=\\Delta_1$.\n- The new gradient norm is $\\|g_2\\| = \\|g_1\\| - 2\\Delta_1 = 0.8 - 0.3 = 0.5$.\n- The new radius is $\\Delta_2 = 1.5 \\Delta_1 = 1.5 \\times 0.15 = 0.225$.\n\nIteration $k=2$:\n- We have $\\|g_2\\| = 0.5$ and $\\Delta_2 = 0.225$.\n- We check the condition: $\\|g_2\\| = 0.5$ versus $2\\Delta_2 = 2(0.225) = 0.45$.\n- Since $0.5 \\ge 0.45$, we are in the boundary case. The step is $\\|s_2\\|=\\Delta_2$.\n- The new gradient norm is $\\|g_3\\| = \\|g_2\\| - 2\\Delta_2 = 0.5 - 0.45 = 0.05$.\n- The new radius is $\\Delta_3 = 1.5 \\Delta_2 = 1.5 \\times 0.225 = 0.3375$.\n\nIteration $k=3$:\n- We have $\\|g_3\\| = 0.05$ and $\\Delta_3 = 0.3375$.\n- We check the condition: $\\|g_3\\| = 0.05$ versus $2\\Delta_3 = 2(0.3375) = 0.675$.\n- Since $0.05  0.675$, we are in the interior case. This is the first interior step.\n- Thus, the index is $K=3$.\n- The step taken is $s_3 = -\\frac{1}{2}g_3$, with $\\|s_3\\| = \\frac{1}{2}\\|g_3\\| = \\frac{0.05}{2} = 0.025$. This is indeed less than $\\Delta_3=0.3375$. According to the rule, the radius would not change, so $\\Delta_4 = \\Delta_3$.\n- The algorithm terminates at the next step, since $g_4=0$.\n\nThe sequence of radii up to $\\Delta_K$ is:\n$\\Delta_0 = 0.1$\n$\\Delta_1 = 0.15$\n$\\Delta_2 = 0.225$\n$\\Delta_3 = 0.3375$\nThe index is $K=3$.\nThe required row vector is $(\\Delta_0, \\Delta_1, \\Delta_2, \\Delta_3, K) = (0.1, 0.15, 0.225, 0.3375, 3)$.\n\nPart 3: Tradeoff explanation.\n\nThe trust-region radius update factor, $\\tau$, governs how quickly the trust-region size adapts. The choice of $\\tau$ represents a fundamental tradeoff between convergence speed and robustness, particularly for general nonlinear functions.\n\nAn aggressive growth strategy (a large $\\tau$, e.g., $\\tau > 2$) allows the trust-region radius $\\Delta_k$ to increase rapidly when the model is a good predictor of the objective function (i.e., when the ratio of actual to predicted reduction $\\rho_k$ is high). For a quadratic function, as in this problem, the model is a perfect representation of the function. Therefore, the only impediment to taking the optimal Newton step ($s = -H^{-1}g$) is the trust-region constraint. A large $\\tau$ allows the trust region to expand quickly to a size where it can contain the Newton step, leading to convergence in fewer iterations. In this idealized setting, a more aggressive $\\tau$ is strictly better for performance.\n\nHowever, in the more general case of non-quadratic objective functions, the quadratic model $m_k(s)$ is only a local approximation. An overly aggressive radius expansion can lead to a situation where the trust region becomes too large, and the model is no longer a faithful representation of the objective function at the boundary of the region. This can lead to a computed step $s_k$ that is poor, resulting in a small or even negative value of $\\rho_k$. Consequentially, the step is rejected, and the trust region must be shrunk. This can cause the algorithm to oscillate between aggressive expansions and sharp contractions of the radius, which is inefficient and numerically unstable.\n\nA conservative growth strategy (a smaller $\\tau$, e.g., $\\tau \\in (1, 2]$) promotes numerical stability and robustness. By expanding the trust region more cautiously, it is more likely that the model remains a good approximation within the trust region. This leads to a higher probability of accepting steps and making steady progress, which is particularly important when dealing with highly nonlinear functions or when far from a local minimum. The price for this stability is potentially slower convergence, as it may take more iterations for the trust region to become large enough to permit longer, more productive steps.\n\nIn summary, the tradeoff is between achieving rapid convergence on well-behaved (nearly quadratic) functions and ensuring stable, reliable progress on more difficult, highly nonlinear problems.", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.1  0.15  0.225  0.3375  3 \\end{pmatrix} } $$", "id": "3193997"}, {"introduction": "Moving from theory to practice, this exercise tackles the famous non-convex Rosenbrock function, which features a narrow, curved valley that challenges many optimization algorithms. You will implement a full-featured trust-region method using the practical truncated Conjugate Gradient (CG) method to solve the subproblems. By comparing different parameter settings for the radius update rule, you will diagnose and understand critical failure modes like oscillation and premature radius collapse, learning how to tune the algorithm for robust performance on difficult, realistic landscapes. [@problem_id:3193957]", "problem": "You are asked to implement and study a trust-region method with explicit trust-region radius updates on a nonconvex function that exhibits narrow, curved valleys. The goal is to diagnose when naive update rules for the trust-region radius cause oscillation or premature collapse of the trust-region radius, and to demonstrate parameter choices that maintain progress and drive the trust-region radius to zero only near stationary points.\n\nConsider the two-variable Rosenbrock function with a narrow curved valley,\n$$\nf(x,y) \\;=\\; 100\\,(y - x^2)^2 + (1 - x)^2,\n$$\ndefined for all $(x,y)\\in \\mathbb{R}^2$. Let $x_k = (x_k^{(1)}, x_k^{(2)})^\\top \\in \\mathbb{R}^2$, and define the gradient $g_k = \\nabla f(x_k)$ and the Hessian $B_k = \\nabla^2 f(x_k)$. The trust-region model at iteration $k$ is the second-order Taylor approximation\n$$\nm_k(s) \\;=\\; f(x_k) + g_k^\\top s + \\tfrac{1}{2} s^\\top B_k s,\n$$\nwith a trust-region constraint $\\|s\\|_2 \\le \\Delta_k$, where $\\Delta_k  0$ is the trust-region radius and $\\|\\cdot\\|_2$ denotes the Euclidean norm. At each iteration you seek an approximate solution to the trust-region subproblem that minimizes $m_k(s)$ subject to $\\|s\\|_2 \\le \\Delta_k$.\n\nUse the following foundational definitions and facts as the base of your derivation and implementation:\n- The gradient and Hessian of a twice continuously differentiable function are defined as the first and second derivatives that characterize the first and second-order Taylor models.\n- The trust-region ratio is defined by\n$$\n\\rho_k \\;=\\; \\frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} \\;=\\; \\frac{\\text{actual reduction}}{\\text{predicted reduction}}.\n$$\n- A basic, well-tested acceptance rule is to accept a step $s_k$ if $\\rho_k \\ge \\eta$, where $\\eta \\in (0,1)$.\n- A standard trust-region radius update uses parameters $\\gamma_{\\text{dec}} \\in (0,1)$ and $\\gamma_{\\text{inc}}  1$, and optionally a second threshold $\\eta_{\\text{inc}} \\in (\\eta,1)$ to decide when to expand.\n\nYour tasks:\n1) Implement a trust-region method using the truncated conjugate-gradient method (Steihaug-type) to approximately solve the subproblem. This method builds search directions using local curvature information and either stops on negative curvature or when it hits the trust-region boundary, without needing to explicitly invert $B_k$.\n2) Implement the step acceptance and trust-region radius update rules using parameters $\\eta$, $\\eta_{\\text{inc}}$, $\\gamma_{\\text{dec}}$, $\\gamma_{\\text{inc}}$, with the following logic:\n   - If $\\rho_k  \\eta$, reject the step and set $\\Delta_{k+1} = \\max(\\gamma_{\\text{dec}} \\Delta_k, \\Delta_{\\min})$.\n   - If $\\rho_k \\ge \\eta$, accept the step. If in addition $\\rho_k  \\eta_{\\text{inc}}$ and $\\|s_k\\|_2$ is on the boundary (i.e., $\\|s_k\\|_2$ equals $\\Delta_k$ up to numerical tolerance), set $\\Delta_{k+1} = \\min(\\gamma_{\\text{inc}} \\Delta_k, \\Delta_{\\max})$. Otherwise, keep $\\Delta_{k+1} = \\Delta_k$.\n   - Use fixed bounds $\\Delta_{\\min}$ and $\\Delta_{\\max}$ with $0  \\Delta_{\\min} \\ll 1 \\ll \\Delta_{\\max}$.\n3) Diagnose oscillation and premature collapse of $\\Delta_k$ by tracking whether $\\Delta_k$ becomes very small for many consecutive iterations while the gradient norm is still large. Specifically, define:\n   - A small-radius threshold $\\Delta_{\\text{small}}$,\n   - A “far-from-stationary” threshold on the gradient $\\tau_g$,\n   - A consecutive-iteration window length $M$,\n   and declare “premature collapse away from stationarity” if there exists an index window of length $M$ (ignoring an initial warmup of $W$ iterations) where $\\Delta_k  \\Delta_{\\text{small}}$ and $\\|g_k\\|_2  \\tau_g$ hold simultaneously at every iteration in the window.\n4) For numerical assessment, use the initial point $x_0 = (-1.2, 1.0)^\\top$, maximum iteration cap $K_{\\max}$, and the following stopping and assessment thresholds:\n   - Stationarity tolerance $\\varepsilon_g$ for declaring convergence via $\\|g_k\\|_2 \\le \\varepsilon_g$,\n   - Target objective threshold $f_\\text{target}$ to assert visible decrease in function value,\n   - Parameters $\\Delta_{\\text{small}}$, $\\tau_g$, $M$, $W$ as above.\n\nTest suite:\nProvide the following five test cases, each a tuple of the parameters $(\\eta, \\eta_{\\text{inc}}, \\gamma_{\\text{dec}}, \\gamma_{\\text{inc}}, \\Delta_0, \\Delta_{\\max}, K_{\\max})$:\n- Case A (naive, aggressive shrink, stringent acceptance): $(0.95, 0.98, 0.1, 4.0, 1.0, 100.0, 1000)$.\n- Case B (tuned, conservative acceptance, moderate updates): $(0.1, 0.75, 0.5, 2.0, 1.0, 100.0, 1000)$.\n- Case C (tuned, large initial radius): $(0.1, 0.75, 0.5, 2.0, 10.0, 100.0, 1000)$.\n- Case D (tuned, very small initial radius): $(0.1, 0.75, 0.5, 2.0, 10^{-4}, 100.0, 1000)$.\n- Case E (naive, slightly less aggressive but still stringent): $(0.9, 0.9, 0.2, 3.0, 1.0, 100.0, 1000)$.\n\nFixed assessment thresholds to use in all cases:\n- $\\Delta_{\\min} = 10^{-12}$,\n- $\\Delta_{\\text{small}} = 10^{-8}$,\n- $\\tau_g = 10^{-2}$,\n- $M = 15$,\n- $W = 20$,\n- $\\varepsilon_g = 10^{-5}$,\n- $f_\\text{target} = 10^{-4}$.\n\nYour program should, for each test case, run the trust-region method and return a list of three integers encoding booleans as $1$ for true and $0$ for false:\n- $b_1$: whether significant progress was made, defined as $f(x_{\\text{final}}) \\le f_\\text{target}$,\n- $b_2$: whether there was premature collapse away from stationarity (as defined in item $3$),\n- $b_3$: whether convergence to a stationary point was achieved, defined as $\\|g_{\\text{final}}\\|_2 \\le \\varepsilon_g$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for all five cases as a comma-separated list enclosed in square brackets, where each element is itself a list $[b_1,b_2,b_3]$. For example: $[[1,0,1],[\\dots],\\dots]$. No additional text should be printed. Since no physical units, angles, or percentages are involved, no unit conversion is required. Implement everything in a self-contained manner with no external input files or user interaction.", "solution": "The problem requires the implementation and analysis of a trust-region optimization algorithm applied to the nonconvex Rosenbrock function. The analysis focuses on the behavior of the trust-region radius, specifically diagnosing conditions that lead to its premature collapse. The solution will be developed by first defining the objective function and its derivatives, then detailing the components of the trust-region method including the subproblem solver and radius update logic, and finally explaining the evaluation criteria.\n\nThe objective function is the two-variable Rosenbrock function, defined as:\n$$\nf(x, y) = 100(y - x^2)^2 + (1 - x)^2\n$$\nLet the vector of variables be $x = (x^{(1)}, x^{(2)})^\\top$. The function is then $f(x^{(1)}, x^{(2)}) = 100(x^{(2)} - (x^{(1)})^2)^2 + (1 - x^{(1)})^2$. This function is a standard benchmark for optimization algorithms due to its narrow, curved valley leading to the global minimum at $(1, 1)^\\top$, where $f(1, 1) = 0$.\n\nFor a second-order trust-region method, we require the gradient vector $g(x) = \\nabla f(x)$ and the Hessian matrix $B(x) = \\nabla^2 f(x)$.\nThe partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x^{(1)}} = 200(y - x^2)(-2x) - 2(1 - x) = -400x(y - x^2) - 2(1 - x)\n$$\n$$\n\\frac{\\partial f}{\\partial x^{(2)}} = 200(y - x^2)\n$$\nThus, the gradient is:\n$$\ng(x, y) = \\begin{pmatrix} -400x(y - x^2) - 2(1 - x) \\\\ 200(y - x^2) \\end{pmatrix}\n$$\nThe second-order partial derivatives are:\n$$\n\\frac{\\partial^2 f}{(\\partial x^{(1)})^2} = -400(y - x^2) - 400x(-2x) + 2 = -400y + 1200x^2 + 2\n$$\n$$\n\\frac{\\partial^2 f}{(\\partial x^{(2)})^2} = 200\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x^{(2)} \\partial x^{(1)}} = \\frac{\\partial^2 f}{\\partial x^{(1)} \\partial x^{(2)}} = -400x\n$$\nThis gives the Hessian matrix:\n$$\nB(x, y) = \\begin{pmatrix} 1200x^2 - 400y + 2  -400x \\\\ -400x  200 \\end{pmatrix}\n$$\nThe trust-region method iteratively seeks to minimize the objective function. At each iteration $k$, starting from a point $x_k$, we construct a quadratic model $m_k(s)$ of the function $f$ around $x_k$:\n$$\nm_k(s) = f(x_k) + g_k^\\top s + \\frac{1}{2} s^\\top B_k s\n$$\nwhere $g_k = g(x_k)$ and $B_k = B(x_k)$. We then find a step $s_k$ that approximately solves the trust-region subproblem:\n$$\n\\min_{s \\in \\mathbb{R}^2} m_k(s) \\quad \\text{subject to} \\quad \\|s\\|_2 \\le \\Delta_k\n$$\nHere, $\\Delta_k  0$ is the trust-region radius, which defines a region around $x_k$ where we \"trust\" the model to be a good approximation of $f$.\n\nTo solve this subproblem, we use the truncated conjugate-gradient (CG) method, often called the Steihaug-Toint algorithm. This iterative method is well-suited because it can handle an indefinite Hessian $B_k$ and naturally incorporates the trust-region bound. The method applies CG to solve the linear system $B_k s = -g_k$, but with two crucial modifications:\n1.  **Negative Curvature:** If a direction $d_j$ is encountered such that $d_j^\\top B_k d_j \\le 0$, the quadratic model $m_k(s)$ is not convex along this direction. The CG process is terminated, and the step $s_k$ is taken by moving along $d_j$ from the current CG iterate until it hits the trust-region boundary $\\|s\\|_2 = \\Delta_k$.\n2.  **Boundary Intersection:** If a CG step would result in an iterate $s_{j+1}$ outside the trust region (i.e., $\\|s_{j+1}\\|_2  \\Delta_k$), the step is truncated to land exactly on the boundary. The process is terminated.\n\nIn both boundary-hitting cases, we find a scalar $\\tau  0$ by solving the quadratic equation $\\|s_j + \\tau d_j\\|_2^2 = \\Delta_k^2$ for $\\tau$ and take the positive root. The final step is then $s_k = s_j + \\tau d_j$.\n\nOnce an approximate solution $s_k$ to the subproblem is found, we evaluate its quality by comparing the actual reduction in the objective function to the reduction predicted by the model. The ratio $\\rho_k$ is defined as:\n$$\n\\rho_k = \\frac{\\text{actual reduction}}{\\text{predicted reduction}} = \\frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} = \\frac{f(x_k) - f(x_k + s_k)}{-g_k^\\top s_k - \\frac{1}{2} s_k^\\top B_k s_k}\n$$\nThe predicted reduction should be positive for a valid step. If it is not, the model is poor, and we treat this case by setting $\\rho_k$ to a value that ensures step rejection (e.g., $\\rho_k=0$). Based on $\\rho_k$, we accept or reject the step and update the trust-region radius:\n- If $\\rho_k  \\eta$: The model is a poor fit. The step is rejected ($x_{k+1} = x_k$), and the trust region is shrunk: $\\Delta_{k+1} = \\max(\\gamma_{\\text{dec}} \\Delta_k, \\Delta_{\\min})$.\n- If $\\rho_k \\ge \\eta$: The model is adequate. The step is accepted: $x_{k+1} = x_k + s_k$. The radius is then adjusted. If the model agreement is very good ($\\rho_k  \\eta_{\\text{inc}}$) and the step was constrained by the boundary ($\\|s_k\\|_2$ is close to $\\Delta_k$), we expand the trust region: $\\Delta_{k+1} = \\min(\\gamma_{\\text{inc}} \\Delta_k, \\Delta_{\\max})$. Otherwise, the radius remains unchanged: $\\Delta_{k+1} = \\Delta_k$.\n\nThe overall algorithm proceeds by iterating these steps until the norm of the gradient $\\|g_k\\|_2$ falls below a tolerance $\\varepsilon_g$, or a maximum number of iterations $K_{\\max}$ is reached.\n\nThe analysis involves diagnosing \"premature collapse away from stationarity.\" This phenomenon occurs when the trust-region radius $\\Delta_k$ becomes very small, hindering progress, while the iterate is still far from a stationary point (i.e., $\\|g_k\\|_2$ is large). This is detected if, for $M$ consecutive iterations after an initial warmup period of $W$ iterations, the conditions $\\Delta_k  \\Delta_{\\text{small}}$ and $\\|g_k\\|_2  \\tau_g$ both hold.\n\nFinally, for each test case, we evaluate the performance based on three criteria:\n1.  $b_1$: Whether significant progress was made ($f(x_{\\text{final}}) \\le f_\\text{target}$).\n2.  $b_2$: Whether premature collapse, as defined above, occurred.\n3.  $b_3$: Whether convergence to a stationary point was achieved ($\\|g_{\\text{final}}\\|_2 \\le \\varepsilon_g$).\n\nThe choice of parameters $(\\eta, \\eta_{\\text{inc}}, \\gamma_{\\text{dec}}, \\gamma_{\\text{inc}})$ critically affects performance. Stringent acceptance criteria (high $\\eta$) combined with aggressive radius shrinking (low $\\gamma_{\\text{dec}}$) can easily lead to premature collapse, as the algorithm may fail to navigate the narrow valley of the Rosenbrock function. Conversely, more lenient and conservative parameters typically lead to robust convergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    \"\"\"Computes the Rosenbrock function value.\"\"\"\n    return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\ndef rosenbrock_grad(x):\n    \"\"\"Computes the gradient of the Rosenbrock function.\"\"\"\n    grad = np.zeros(2)\n    grad[0] = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n    grad[1] = 200.0 * (x[1] - x[0]**2)\n    return grad\n\ndef rosenbrock_hessian(x):\n    \"\"\"Computes the Hessian of the Rosenbrock function.\"\"\"\n    hess = np.zeros((2, 2))\n    hess[0, 0] = 1200.0 * x[0]**2 - 400.0 * x[1] + 2.0\n    hess[0, 1] = -400.0 * x[0]\n    hess[1, 0] = -400.0 * x[0]\n    hess[1, 1] = 200.0\n    return hess\n\ndef truncated_cg(g, B, delta):\n    \"\"\"\n    Solves the trust-region subproblem using the truncated conjugate-gradient\n    (Steihaug-Toint) method.\n    \"\"\"\n    s = np.zeros_like(g)\n    r = g.copy()\n    d = -r.copy()\n    hit_boundary = False\n\n    if np.linalg.norm(r) == 0:\n        return s, hit_boundary\n\n    max_cg_iter = len(g)\n    for j in range(max_cg_iter):\n        dBd = d.T @ B @ d\n\n        if dBd = 0:\n            # Negative curvature detected. Find tau to hit the boundary.\n            a = d.T @ d\n            b = 2 * (s.T @ d)\n            c = s.T @ s - delta**2\n            # We want the positive root of a*tau^2 + b*tau + c = 0\n            tau = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s += tau * d\n            hit_boundary = True\n            break\n        \n        alpha = (r.T @ r) / dBd\n        s_new = s + alpha * d\n\n        if np.linalg.norm(s_new) >= delta:\n            # Step hits or exceeds boundary. Find tau to be exactly on boundary.\n            a = d.T @ d\n            b = 2 * (s.T @ d)\n            c = s.T @ s - delta**2\n            tau = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s += tau * d\n            hit_boundary = True\n            break\n\n        s = s_new\n        r_new = r + alpha * (B @ d)\n        \n        # CG convergence check\n        if np.linalg.norm(r_new)  1e-6 * np.linalg.norm(g):\n            break\n\n        beta = (r_new.T @ r_new) / (r.T @ r)\n        r = r_new\n        d = -r + beta * d\n        \n    return s, hit_boundary\n\ndef run_trust_region(params, fixed_params):\n    \"\"\"Runs the trust-region algorithm for a given set of parameters.\"\"\"\n    eta, eta_inc, gamma_dec, gamma_inc, delta_0, delta_max, k_max = params\n    delta_min = fixed_params['delta_min']\n    delta_small = fixed_params['delta_small']\n    tau_g = fixed_params['tau_g']\n    M = fixed_params['M']\n    W = fixed_params['W']\n    eps_g = fixed_params['eps_g']\n    f_target = fixed_params['f_target']\n\n    x = np.array([-1.2, 1.0])\n    delta = delta_0\n\n    delta_history = []\n    gnorm_history = []\n\n    for k in range(k_max):\n        f_k = rosenbrock(x)\n        g_k = rosenbrock_grad(x)\n        B_k = rosenbrock_hessian(x)\n        \n        g_norm = np.linalg.norm(g_k)\n        delta_history.append(delta)\n        gnorm_history.append(g_norm)\n\n        if g_norm = eps_g:\n            break\n\n        s_k, hit_boundary = truncated_cg(g_k, B_k, delta)\n\n        pred_reduction = -(g_k.T @ s_k + 0.5 * s_k.T @ B_k @ s_k)\n        \n        x_new = x + s_k\n        actual_reduction = f_k - rosenbrock(x_new)\n\n        if pred_reduction = 0:\n            rho_k = -1.0 # Will force rejection\n        else:\n            rho_k = actual_reduction / pred_reduction\n\n        if rho_k  eta:\n            # Reject step, shrink radius\n            delta = max(gamma_dec * delta, delta_min)\n        else:\n            # Accept step\n            x = x_new\n            # Update radius\n            if rho_k > eta_inc and hit_boundary:\n                delta = min(gamma_inc * delta, delta_max)\n            # else delta remains the same\n\n    x_final = x\n    f_final = rosenbrock(x_final)\n    g_final_norm = np.linalg.norm(rosenbrock_grad(x_final))\n\n    # Assessment\n    b1 = 1 if f_final = f_target else 0\n    b3 = 1 if g_final_norm = eps_g else 0\n\n    premature_collapse = False\n    collapse_counter = 0\n    if len(delta_history) > W + M:\n        for i in range(W, len(delta_history)):\n            if delta_history[i]  delta_small and gnorm_history[i] > tau_g:\n                collapse_counter += 1\n            else:\n                collapse_counter = 0\n            \n            if collapse_counter >= M:\n                premature_collapse = True\n                break\n    b2 = 1 if premature_collapse else 0\n    \n    return [b1, b2, b3]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (eta, eta_inc, gamma_dec, gamma_inc, delta_0, delta_max, k_max)\n        (0.95, 0.98, 0.1, 4.0, 1.0, 100.0, 1000), # Case A\n        (0.1, 0.75, 0.5, 2.0, 1.0, 100.0, 1000),  # Case B\n        (0.1, 0.75, 0.5, 2.0, 10.0, 100.0, 1000), # Case C\n        (0.1, 0.75, 0.5, 2.0, 1e-4, 100.0, 1000), # Case D\n        (0.9, 0.9, 0.2, 3.0, 1.0, 100.0, 1000),   # Case E\n    ]\n\n    fixed_params = {\n        'delta_min': 1e-12,\n        'delta_small': 1e-8,\n        'tau_g': 1e-2,\n        'M': 15,\n        'W': 20,\n        'eps_g': 1e-5,\n        'f_target': 1e-4,\n    }\n\n    results = []\n    for case in test_cases:\n        result = run_trust_region(case, fixed_params)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3193957"}, {"introduction": "Standard algorithms can fail in pathological landscapes, and a true expert must know how to adapt. This advanced practice presents an objective function with a nearly flat ridge, a scenario where typical trust-region update rules can stall, failing to expand the radius even when the model is accurate. Your task is to analyze this failure and design a more intelligent, \"stagnation-aware\" expansion rule that empowers the algorithm to escape such regions, thereby moving from simply using an algorithm to actively improving its design for greater robustness. [@problem_id:3193978]", "problem": "You are asked to design and implement a program that examines trust-region radius update rules on a twice continuously differentiable objective with a nearly flat ridge and to propose a principled modification that increases the trust-region radius even when the model decrease is small, provided the model is highly reliable. The core focus is on how, why, and when to update the trust-region radius to ensure convergence out of flat regions where steps remain tiny even when the agreement between the model and the true objective is good.\n\nThe fundamental base to use is the standard trust-region method for unconstrained optimization. At an iterate $x_k \\in \\mathbb{R}^n$, the quadratic model is\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top B_k p,\n$$\nwhere $g_k = \\nabla f(x_k)$ and $B_k = \\nabla^2 f(x_k)$ is the Hessian matrix. The trust-region subproblem seeks\n$$\n\\min_{p \\in \\mathbb{R}^n} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta_k,\n$$\nwith $\\Delta_k  0$ the current trust-region radius. A candidate step $p_k$ is judged by the ratio\n$$\n\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)},\n$$\nwhich compares the actual reduction to the predicted reduction. The standard update rule shrinks $\\Delta_k$ if $\\rho_k$ is small, and grows $\\Delta_k$ if $\\rho_k$ is large and the step is on or near the boundary.\n\nConstruct a smooth objective with a nearly flat ridge:\n$$\nf(x_1, x_2) = \\sqrt{\\varepsilon + x_1^2} + \\tfrac{\\kappa}{2} x_2^2,\n$$\nwith fixed positive scalars $\\varepsilon  0$ and $\\kappa  0$. Its gradient and Hessian are\n$$\ng(x) = \\begin{bmatrix} \\dfrac{x_1}{\\sqrt{\\varepsilon + x_1^2}} \\\\ \\kappa x_2 \\end{bmatrix}, \\qquad\nB(x) = \\begin{bmatrix} \\dfrac{\\varepsilon}{(\\varepsilon + x_1^2)^{3/2}}  0 \\\\ 0  \\kappa \\end{bmatrix}.\n$$\nNote that near the ridge at $x_1 \\approx 0$ the curvature in the $x_1$ direction is large while the function value changes very slowly along $x_1$, and the component $g_1$ can be tiny; this can yield steps $p_k$ with $\\rho_k \\approx 1$ but $\\|p_k\\|$ very small, so standard rules may not increase $\\Delta_k$.\n\nYour tasks:\n- Implement an exact solver for the trust-region subproblem in dimension $n=2$ using the Lagrange multiplier optimality condition: find $\\lambda \\ge 0$ such that \n$$\n(B(x_k) + \\lambda I) p_k = -g(x_k), \\quad \\text{with} \\quad \\|p_k\\| \\le \\Delta_k,\n$$\nand if $\\|p_k\\|  \\Delta_k$ for $\\lambda=0$, increase $\\lambda$ until $\\|p_k\\| = \\Delta_k$ (use a robust bisection method on $\\lambda$).\n- Compute the actual reduction $f(x_k) - f(x_k + p_k)$ and the predicted reduction $m_k(0) - m_k(p_k) = -g_k^\\top p_k - \\tfrac{1}{2} p_k^\\top B_k p_k$, and then compute $\\rho_k$.\n- Implement the standard trust-region radius update decision rule:\n  - If $\\rho_k  0.25$, mark the decision as \"shrink\" ($-1$).\n  - Else if $\\rho_k  0.75$ and $\\|p_k\\| \\ge 0.8 \\Delta_k$, mark as \"increase\" ($+1$).\n  - Else mark as \"no change\" ($0$).\n- Propose and implement a stagnation-aware expansion rule to escape flat regions even when model decrease is small:\n  - If $\\rho_k \\ge \\eta_{\\text{high}}$ and $\\|p_k\\| \\le \\chi \\Delta_k$ and $m_k(0) - m_k(p_k) \\le c_{\\text{pred}} \\|g_k\\| \\Delta_k$ and $\\|g_k\\|  \\tau_g$, mark as \"increase\" ($+1$).\n  - Else if $\\rho_k  0.25$, mark as \"shrink\" ($-1$).\n  - Else if $\\rho_k  0.75$ and $\\|p_k\\| \\ge 0.8 \\Delta_k$, mark as \"increase\" ($+1$).\n  - Else mark as \"no change\" ($0$).\nHere $\\eta_{\\text{high}}$, $\\chi$, $c_{\\text{pred}}$, and $\\tau_g$ are fixed thresholds that you must choose to be reasonable for this scale; use $\\eta_{\\text{high}} = 0.9$, $\\chi = 0.2$, $c_{\\text{pred}} = 0.2$, and $\\tau_g = 10^{-8}$.\n\nTest suite specification:\nUse $\\varepsilon = 10^{-6}$ and $\\kappa = 20$. For each case below, the program must compute $p_k$, $\\rho_k$, and return the pair of decisions $[d_{\\text{std}}, d_{\\text{prop}}]$ with $d_{\\text{std}}, d_{\\text{prop}} \\in \\{-1, 0, +1\\}$:\n1. $x_k = (10^{-8}, 10^{-3})$, $\\Delta_k = 10^{-2}$: nearly flat ridge with tiny interior step and $\\rho_k \\approx 1$; the stagnation-aware rule should increase while the standard rule should not.\n2. $x_k = (10^{-1}, 0)$, $\\Delta_k = 5 \\cdot 10^{-2}$: good model agreement near boundary; both rules should increase.\n3. $x_k = (2, 0)$, $\\Delta_k = 10$: poor model agreement with a very large step candidate; both rules should shrink.\n4. $x_k = (0, 0)$, $\\Delta_k = 10^{-1}$: zero gradient edge case; both rules should return no change.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the test suite as a comma-separated Python-style list of lists with the decisions for each case in order, for example, $[[d_1^{\\text{std}}, d_1^{\\text{prop}}],[d_2^{\\text{std}}, d_2^{\\text{prop}}],\\dots]$. There are no physical units or angle units involved in this problem. All numeric answers in the output are integers in $\\{-1,0,1\\}$, and the aggregated output is a list of these integer pairs.", "solution": "The user has presented a valid problem in numerical optimization. The task is to analyze and compare two different trust-region radius update strategies for a specific objective function known to cause slow convergence. The problem is scientifically grounded, well-posed, and contains all necessary information for a unique, verifiable solution.\n\nThe problem centers on the unconstrained optimization of a twice continuously differentiable objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ using a trust-region method. The specific function is designed to have a nearly flat ridge, which poses a challenge for standard algorithms. The function is given by:\n$$\nf(x_1, x_2) = \\sqrt{\\varepsilon + x_1^2} + \\frac{\\kappa}{2} x_2^2\n$$\nwhere $\\varepsilon  0$ and $\\kappa  0$ are fixed positive scalars. For our tests, $\\varepsilon = 10^{-6}$ and $\\kappa = 20$. The gradient $g(x) = \\nabla f(x)$ and Hessian $B(x) = \\nabla^2 f(x)$ are:\n$$\ng(x) = \\begin{bmatrix} \\frac{x_1}{\\sqrt{\\varepsilon + x_1^2}} \\\\ \\kappa x_2 \\end{bmatrix}, \\qquad\nB(x) = \\begin{bmatrix} \\frac{\\varepsilon}{(\\varepsilon + x_1^2)^{3/2}}  0 \\\\ 0  \\kappa \\end{bmatrix}\n$$\nAt each iteration $k$, given a point $x_k$, we form a quadratic model of the objective function:\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\frac{1}{2} p^\\top B_k p\n$$\nwhere $g_k = g(x_k)$ and $B_k = B(x_k)$. The core of the trust-region method is to find a step $p_k$ that approximately solves the trust-region subproblem:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\|_2 \\le \\Delta_k\n$$\nwhere $\\Delta_k  0$ is the trust-region radius.\n\nTo solve this subproblem, we use the Karush-Kuhn-Tucker (KKT) conditions. A solution $p_k$ must satisfy:\n$$\n(B_k + \\lambda I) p_k = -g_k\n$$\nfor some Lagrange multiplier $\\lambda \\ge 0$. Here, $I$ is the $2 \\times 2$ identity matrix. The conditions also require that $\\lambda(\\Delta_k - \\|p_k\\|_2) = 0$. Since $\\varepsilon  0$ and $\\kappa  0$, the Hessian $B_k$ is a diagonal matrix with positive entries, meaning it is positive definite. This simplifies the subproblem solution significantly. The algorithm to find $p_k$ is as follows:\n$1$. Compute the full Newton step (unconstrained minimizer of $m_k(p)$) by setting $\\lambda=0$: $p_k(0) = -B_k^{-1} g_k$.\n$2$. If $\\|p_k(0)\\|_2 \\le \\Delta_k$, the Newton step lies within the trust region and is the optimal solution. Thus, $p_k = p_k(0)$. This is the interior solution.\n$3$. If $\\|p_k(0)\\|_2  \\Delta_k$, the solution must lie on the boundary of the trust region, i.e., $\\|p_k\\|_2 = \\Delta_k$. We must find a $\\lambda  0$ that satisfies this condition. The step as a function of $\\lambda$ is $p_k(\\lambda) = -(B_k + \\lambda I)^{-1} g_k$. We must solve the scalar nonlinear secular equation $\\|p_k(\\lambda)\\|_2 - \\Delta_k = 0$ for $\\lambda$. Since $B_k$ is diagonal with entries $b_{11}$ and $b_{22}$, the equation becomes:\n$$\n\\sqrt{\\left(\\frac{-g_1}{b_{11} + \\lambda}\\right)^2 + \\left(\\frac{-g_2}{b_{22} + \\lambda}\\right)^2} = \\Delta_k\n$$\nThis equation is solved for $\\lambda  0$ using a numerical root-finding method, specifically bisection, which is robust as the function $\\|p_k(\\lambda)\\|_2$ is monotonically decreasing for $\\lambda  0$.\n\nOnce the step $p_k$ is computed, its quality is assessed by the ratio $\\rho_k$ of the actual reduction in the objective function to the reduction predicted by the quadratic model:\n$$\n\\rho_k = \\frac{\\text{Ared}}{\\text{Pred}} = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}\n$$\nThe predicted reduction is given by $m_k(0) - m_k(p_k) = -g_k^\\top p_k - \\frac{1}{2} p_k^\\top B_k p_k$. If $Pred = 0$, which occurs if and only if $g_k = 0$ (since $B_k$ is positive definite), then $p_k=0$ and $Ared=0$. In this scenario, $\\rho_k$ is undefined, and the trust-region radius should remain unchanged.\n\nThe core of the investigation lies in the rules for updating $\\Delta_k$ for the next iteration, $\\Delta_{k+1}$.\nThe standard update rule is based on $\\rho_k$ and the step length relative to the radius:\n- If $\\rho_k  0.25$: The model is a poor predictor. Shrink the trust region. Decision is \"shrink\" ($-1$).\n- Else if $\\rho_k  0.75$ and $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$: The model is a good predictor and the step was constrained by the boundary. Increase the trust region. Decision is \"increase\" ($+1$).\n- Else: The step is acceptable but provides no strong reason to change the radius. Decision is \"no change\" ($0$).\n\nThe proposed stagnation-aware expansion rule is designed to handle cases where the model is accurate ($\\rho_k$ is high), but the step $p_k$ and the predicted reduction are very small. This occurs in flat regions. The rule prioritizes expansion under these conditions to escape the flat region more quickly. The logic is sequential:\n$1$. If $\\rho_k \\ge \\eta_{\\text{high}}$ ($0.9$), $\\|p_k\\|_2 \\le \\chi \\Delta_k$ ($0.2$), $Pred \\le c_{\\text{pred}} \\|g_k\\|_2 \\Delta_k$ ($0.2$), and $\\|g_k\\|_2  \\tau_g$ ($10^{-8}$): These conditions identify a small but highly accurate step in a non-stationary region where the predicted decay from the linear term is small. Increase the trust region. Decision is \"increase\" ($+1$).\n$2$. Else if $\\rho_k  0.25$: Shrink the trust region. Decision is \"shrink\" ($-1$).\n$3$. Else if $\\rho_k  0.75$ and $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$: Standard increase condition. Decision is \"increase\" ($+1$).\n$4$. Else: Decision is \"no change\" ($0$).\n\nApplying this framework to the test cases yields the following analysis:\n\nCase $1$: $x_k = (10^{-8}, 10^{-3})$, $\\Delta_k = 10^{-2}$.\nAt this point, $x_1$ is very close to $0$. The gradient component $g_1$ is tiny ($\\approx 10^{-5}$), while the Hessian component $b_{11}$ is large ($\\approx 1000$). The resulting Newton step is an interior solution ($p_k \\approx (-10^{-8}, -10^{-3})$) with a very small norm $\\|p_k\\|_2 \\approx 10^{-3}$. The model is highly accurate, leading to $\\rho_k \\approx 1$.\n- Standard rule: $\\rho_k  0.75$, but $\\|p_k\\|_2 \\approx 10^{-3}$ is much smaller than $0.8 \\Delta_k = 8 \\cdot 10^{-3}$. Thus, the decision is \"no change\" ($0$).\n- Proposed rule: The stagnation condition is triggered. $\\rho_k \\approx 1 \\ge 0.9$, $\\|p_k\\|_2 \\approx 10^{-3} \\le 0.2 \\Delta_k = 2 \\cdot 10^{-3}$, the predicted reduction is small satisfying the $c_{pred}$ criterion, and $\\|g_k\\|_2 \\approx 2 \\cdot 10^{-2}  10^{-8}$. All conditions are met, leading to an \"increase\" ($+1$) decision. This demonstrates the proposed rule's advantage.\n\nCase $2$: $x_k = (10^{-1}, 0)$, $\\Delta_k = 5 \\cdot 10^{-2}$.\nThe Newton step is very large, so the solution $p_k$ lies on the trust-region boundary, with $\\|p_k\\|_2 = \\Delta_k$. The model agreement is excellent, with $\\rho_k \\approx 1$.\n- Both rules: The condition $\\rho_k  0.75$ and $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$ is satisfied. The proposed rule's stagnation condition is not met because $\\|p_k\\|_2 = \\Delta_k$ violates $\\|p_k\\|_2 \\le \\chi \\Delta_k$. Both rules correctly decide to \"increase\" ($+1$).\n\nCase $3$: $x_k = (2, 0)$, $\\Delta_k = 10$.\nFar from the origin, the function is nearly linear in $x_1$, so the second-order model is a poor approximation over a large distance. The step $p_k$ is on the boundary with $\\|p_k\\|_2 = 10$. The actual function value increases after the step, yielding a negative actual reduction $Ared  0$, and thus $\\rho_k  0$.\n- Both rules: Since $\\rho_k  0.25$, both rules correctly decide to \"shrink\" ($-1$).\n\nCase $4$: $x_k = (0, 0)$, $\\Delta_k = 10^{-1}$.\nThis is a stationary point, as $g_k = (0,0)$. The optimal step is $p_k = 0$. Consequently, $Ared=0$ and $Pred=0$. In this case, $\\rho_k$ is undefined. No change in the trust-region radius is warranted.\n- Both rules: The logic handles this by defaulting to \"no change\" ($0$). The specific condition $\\|g_k\\|_2  \\tau_g$ in the proposed rule explicitly prevents expansion at a stationary point.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\n# --- Problem Definition ---\n\n# Fixed parameters for the objective function\nEPSILON = 1e-6\nKAPPA = 20.0\n\n# Thresholds for the proposed stagnation-aware rule\nETA_HIGH = 0.9\nCHI = 0.2\nC_PRED = 0.2\nTAU_G = 1e-8\n\ndef f_obj(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Objective function f(x_1, x_2).\"\"\"\n    return np.sqrt(eps + x[0]**2) + (kap / 2.0) * x[1]**2\n\ndef g_grad(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Gradient of the objective function.\"\"\"\n    g1 = x[0] / np.sqrt(eps + x[0]**2)\n    g2 = kap * x[1]\n    return np.array([g1, g2])\n\ndef B_hess(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Hessian of the objective function.\"\"\"\n    b11 = eps / (eps + x[0]**2)**1.5\n    b22 = kap\n    # The Hessian is diagonal\n    return np.diag([b11, b22])\n\ndef solve_tr_subproblem(gk, Bk, delta_k):\n    \"\"\"\n    Solves the 2D trust-region subproblem min m(p) s.t. ||p|| = delta_k\n    using the KKT conditions and bisection for the boundary case.\n    \"\"\"\n    if np.linalg.norm(gk) == 0:\n        return np.zeros(2)\n\n    # Since B_k is positive definite, we can compute the Newton step.\n    try:\n        p_newton = -np.linalg.solve(Bk, gk)\n    except np.linalg.LinAlgError:\n        # This case is not expected here as Bk is positive definite\n        p_newton = -np.linalg.pinv(Bk) @ gk\n    \n    if np.linalg.norm(p_newton) = delta_k:\n        # Interior solution\n        return p_newton\n\n    # Boundary solution: find lambda > 0 such that ||p(lambda)|| = delta_k\n    b_diag = np.diag(Bk)\n    \n    def secular_eq(lam):\n        # Using the diagonal structure of Bk for p(lambda)\n        p_lam = -gk / (b_diag + lam)\n        return np.linalg.norm(p_lam) - delta_k\n    \n    # Establish a safe search bracket [lambda_low, lambda_high] for bisection.\n    lambda_low = 0.0\n    # An upper bound can be derived from ||g||/delta_k. Start there and increase if needed.\n    lambda_high = np.linalg.norm(gk) / delta_k\n    while secular_eq(lambda_high) > 0:\n        lambda_high *= 2.0\n\n    # Use a robust root-finding method (bisection) to find lambda\n    sol = root_scalar(secular_eq, bracket=[lambda_low, lambda_high], method='bisect')\n    lam_star = sol.root\n    \n    p_star = -gk / (b_diag + lam_star)\n    return p_star\n\ndef compute_decisions(xk, pk, delta_k):\n    \"\"\"\n    Computes rho_k and returns the decisions for both standard and proposed rules.\n    \"\"\"\n    gk = g_grad(xk)\n    Bk = B_hess(xk)\n\n    actual_reduction = f_obj(xk) - f_obj(xk + pk)\n    predicted_reduction = -gk.T @ pk - 0.5 * pk.T @ Bk @ pk\n\n    if abs(predicted_reduction)  1e-15:\n        # If predicted reduction is zero (e.g. g_k=0), p_k=0, no change.\n        return [0, 0]\n\n    rho_k = actual_reduction / predicted_reduction\n    norm_pk = np.linalg.norm(pk)\n    norm_gk = np.linalg.norm(gk)\n\n    # Standard rule\n    d_std = 0\n    if rho_k  0.25:\n        d_std = -1\n    elif rho_k > 0.75 and norm_pk >= 0.8 * delta_k:\n        d_std = 1\n\n    # Proposed stagnation-aware rule\n    d_prop = 0\n    # The conditions must be checked in the specified order (if-elif-else)\n    stagnation_increase = (\n        rho_k >= ETA_HIGH and\n        norm_pk = CHI * delta_k and\n        predicted_reduction = C_PRED * norm_gk * delta_k and\n        norm_gk > TAU_G\n    )\n    \n    if stagnation_increase:\n        d_prop = 1\n    elif rho_k  0.25:\n        d_prop = -1\n    elif rho_k > 0.75 and norm_pk >= 0.8 * delta_k:\n        d_prop = 1\n    \n    return [d_std, d_prop]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute decisions.\n    \"\"\"\n    # Test cases: (x_k_tuple, delta_k)\n    test_cases = [\n        ((1e-8, 1e-3), 1e-2),\n        ((1e-1, 0.0), 5e-2),\n        ((2.0, 0.0), 10.0),\n        ((0.0, 0.0), 1e-1),\n    ]\n\n    results = []\n    for case in test_cases:\n        x_k_tuple, delta_k = case\n        x_k = np.array(x_k_tuple)\n        \n        # Calculate g_k and B_k at the current point\n        g_k = g_grad(x_k)\n        B_k = B_hess(x_k)\n        \n        # Solve the trust-region subproblem to get the step p_k\n        p_k = solve_tr_subproblem(g_k, B_k, delta_k)\n\n        # Compute the decisions based on the two different update rules\n        decisions = compute_decisions(x_k, p_k, delta_k)\n        results.append(decisions)\n\n    # Format the final output string exactly as specified\n    result_strs = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(result_strs)}]\"\n    print(final_output)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3193978"}]}