{"hands_on_practices": [{"introduction": "The first step in any least-squares optimization is to define a measure of error between your model's predictions and the actual data. This error is captured in the residual vector, where each component represents the discrepancy for a single data point. This practice [@problem_id:2217008] provides a concrete, geometric scenario—fitting a circle to data points—to help you build a strong intuition for calculating this fundamental quantity.", "problem": "A common task in optimization is fitting a geometric model to a set of data points. Consider the problem of fitting a circle to a set of points in a 2D plane. The circle is defined by a parameter vector $\\mathbf{p} = [x_c, y_c, R]^T$, where $(x_c, y_c)$ is the center of the circle and $R$ is its radius.\n\nFor a set of $n$ data points $(x_i, y_i)$, the goal of a nonlinear least squares fit is to find the parameter vector $\\mathbf{p}$ that minimizes the sum of squared residuals. The residual for the $i$-th data point, $r_i(\\mathbf{p})$, is defined as the difference between the point's distance to the proposed center $(x_c, y_c)$ and the proposed radius $R$.\n\nYou are given three data points representing measurements in a Cartesian coordinate system where all coordinates are in meters:\n$P_1 = (1.0, 7.0)$\n$P_2 = (6.0, 2.0)$\n$P_3 = (9.0, 8.0)$\n\nAn iterative optimization algorithm, such as the Levenberg-Marquardt algorithm, begins with an initial guess for the parameters. The initial guess for the circle's parameters is given as $\\mathbf{p}_0 = [x_{c,0}, y_{c,0}, R_0]^T = [5.0, 5.0, 4.0]^T$.\n\nCalculate the residual vector $\\mathbf{r}(\\mathbf{p}_0) = [r_1(\\mathbf{p}_0), r_2(\\mathbf{p}_0), r_3(\\mathbf{p}_0)]^T$ for this initial guess. Express each component of the resulting vector in meters, rounded to four significant figures. Present your final answer as a single row matrix.", "solution": "For a circle with parameters $\\mathbf{p} = [x_{c}, y_{c}, R]^{T}$ and a data point $(x_{i}, y_{i})$, the residual is defined as the difference between the Euclidean distance from the point to the center and the radius:\n$$\nr_{i}(\\mathbf{p}) = \\sqrt{(x_{i} - x_{c})^{2} + (y_{i} - y_{c})^{2}} - R.\n$$\nWith the initial guess $\\mathbf{p}_{0} = [5.0, 5.0, 4.0]^{T}$, compute each residual.\n\nFor $P_{1} = (1.0, 7.0)$:\n$$\nd_{1} = \\sqrt{(1.0 - 5.0)^{2} + (7.0 - 5.0)^{2}} = \\sqrt{(-4.0)^{2} + (2.0)^{2}} = \\sqrt{16 + 4} = \\sqrt{20},\n$$\n$$\nr_{1}(\\mathbf{p}_{0}) = \\sqrt{20} - 4.0 \\approx 0.4721 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{2} = (6.0, 2.0)$:\n$$\nd_{2} = \\sqrt{(6.0 - 5.0)^{2} + (2.0 - 5.0)^{2}} = \\sqrt{(1.0)^{2} + (-3.0)^{2}} = \\sqrt{1 + 9} = \\sqrt{10},\n$$\n$$\nr_{2}(\\mathbf{p}_{0}) = \\sqrt{10} - 4.0 \\approx -0.8377 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{3} = (9.0, 8.0)$:\n$$\nd_{3} = \\sqrt{(9.0 - 5.0)^{2} + (8.0 - 5.0)^{2}} = \\sqrt{(4.0)^{2} + (3.0)^{2}} = \\sqrt{16 + 9} = \\sqrt{25} = 5.0,\n$$\n$$\nr_{3}(\\mathbf{p}_{0}) = 5.0 - 4.0 = 1.000 \\text{ (to four significant figures)}.\n$$\n\nThus the residual vector, as a row matrix, is:\n$$\n\\begin{pmatrix}\n0.4721 & -0.8377 & 1.000\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.4721 & -0.8377 & 1.000 \\end{pmatrix}}$$", "id": "2217008"}, {"introduction": "Once we know the error, we need to determine how to adjust our model's parameters to reduce it. The Jacobian matrix is the key to this, as it tells us how sensitive the residuals are to small changes in each parameter. This exercise [@problem_id:2217052] moves from a geometric model to a common rational function model, giving you essential practice in deriving and calculating the Jacobian, which forms the core of the Levenberg-Marquardt update step.", "problem": "In the field of nonlinear optimization, algorithms like the Levenberg-Marquardt algorithm are used to fit a model function to a set of data points by minimizing the sum of the squares of the residuals. A key component in this process is the Jacobian matrix of the model function.\n\nConsider a model used to describe the concentration of a substance over time, given by the two-parameter rational function:\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\nwhere $t$ is the independent variable (e.g., time), and $\\mathbf{p} = [a, b]^T$ is the vector of parameters to be determined.\n\nSuppose we have collected the following three data points $(t_i, y_i)$:\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\nThe Jacobian matrix $\\mathbf{J}$ for this fitting problem is an $m \\times n$ matrix, where $m$ is the number of data points and $n$ is the number of parameters. Its elements are defined by $J_{ij} = \\frac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$.\n\nCalculate the numerical value of the Jacobian matrix $\\mathbf{J}$ evaluated at the initial parameter guess $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$. All given numerical values are dimensionless.\n\nExpress your final answer as a $3 \\times 2$ matrix, with each element rounded to three significant figures.", "solution": "We are given the model function $f(t; a, b) = \\dfrac{a}{1 + bt}$ and the Jacobian matrix defined by $J_{ij} = \\dfrac{\\partial f(t_{i}; \\mathbf{p})}{\\partial p_{j}}$ for the parameter vector $\\mathbf{p} = [a, b]^{T}$. Thus, each row of $\\mathbf{J}$ corresponds to a data point $t_{i}$, and the two columns correspond to the derivatives with respect to $a$ and $b$.\n\nFirst, compute the partial derivatives symbolically. Write $f(t; a, b) = a(1 + bt)^{-1}$. Then, using the power rule and chain rule:\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\nTherefore, for each data point $t_{i}$, the Jacobian row is\n$$\n\\left[\\frac{1}{1 + b t_{i}},\\ -\\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\nEvaluate at the initial guess $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ and the given $t$ values.\n\nFor $t_{1} = 1$: $1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = \\frac{3}{2}$, $\\frac{\\partial f}{\\partial a} = \\frac{2}{3}$, $\\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 1}{(3/2)^{2}} = -\\frac{12}{9} = -\\frac{4}{3}$.\n\nFor $t_{2} = 2$: $1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2$, $\\frac{\\partial f}{\\partial a} = \\frac{1}{2}$, $\\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 2}{2^{2}} = -\\frac{6}{4} = -\\frac{3}{2}$.\n\nFor $t_{3} = 4$: $1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3$, $\\frac{\\partial f}{\\partial a} = \\frac{1}{3}$, $\\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 4}{3^{2}} = -\\frac{12}{9} = -\\frac{4}{3}$.\n\nAssemble the Jacobian and round each element to three significant figures:\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n0.667 & -1.33 \\\\\n0.500 & -1.50 \\\\\n0.333 & -1.33\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.667 & -1.33 \\\\ 0.500 & -1.50 \\\\ 0.333 & -1.33\\end{pmatrix}}$$", "id": "2217052"}, {"introduction": "With the residual vector $r$ and the Jacobian matrix $J$, we can construct different optimization algorithms. However, the way these components are used leads to vastly different performance characteristics, especially when starting far from the optimal solution. This final practice [@problem_id:3247386] challenges you to think qualitatively about the convergence paths of Gradient Descent, Gauss-Newton, and Levenberg-Marquardt, synthesizing your knowledge to understand why the LM algorithm's adaptive nature makes it so robust and effective.", "problem": "Consider the Beale test function, which can be written as a nonlinear least-squares objective with residuals defined by $r_1(x,y) = 1.5 - x + x y$, $r_2(x,y) = 2.25 - x + x y^2$, and $r_3(x,y) = 2.625 - x + x y^3$. Define the objective $F(x,y) = r_1(x,y)^2 + r_2(x,y)^2 + r_3(x,y)^2$. It is known that the global minimizer satisfies $F(3, 0.5) = 0$. Let the starting point be $\\mathbf{x}_0 = (x_0, y_0) = (-2, -2)$, which is a deliberately poor initial guess. For each of the following standard methods applied to minimizing $F(x,y)$, assume safeguards that ensure monotonic decrease of $F$ at each iteration: Gradient Descent (GD) with backtracking step-size selection, Gauss-Newton (GN) with a backtracking line search on the nonlinear least-squares step, and Levenberg-Marquardt (LM) with an adaptive damping parameter interpreted as a trust-region radius.\n\nBased only on the structure of the nonlinear least-squares problem $F(x,y)$, the properties of the gradient and curvature induced by the residuals $r_i(x,y)$, and the governing definitions of the methods (steepest descent direction for GD, normal-equations step for GN, and a trust-region blend of GN and GD for LM), choose the option that best characterizes the typical qualitative convergence paths (the sequences $\\{\\mathbf{x}_k\\}$ in $\\mathbb{R}^2$) of the three methods starting from $\\mathbf{x}_0 = (-2, -2)$ when minimizing $F(x,y)$.\n\nA. Starting from $\\mathbf{x}_0 = (-2, -2)$, Gradient Descent takes short steps along the negative gradient, zig-zagging while it attempts to follow the curved narrow valley toward $(3, 0.5)$ and converges slowly; Gauss-Newton, using the $J(\\mathbf{x})^\\top J(\\mathbf{x})$ curvature model far from the solution, tends to propose overly aggressive or misaligned steps that can leave the valley unless curtailed by the line search; Levenberg-Marquardt begins with heavily damped steps that behave like a safe Gradient Descent in the trust region, then progressively reduces damping and transitions to Gauss-Newton-like steps as it enters the valley, producing a path that is initially conservative and later more direct toward $(3, 0.5)$.\n\nB. Gauss-Newton always converges fastest from any initial guess because its curvature model is the exact Hessian for sum-of-squares problems, so its path is essentially a straight line to $(3, 0.5)$; Levenberg-Marquardt and Gradient Descent both oscillate and are necessarily slower.\n\nC. Gradient Descent converges in the fewest iterations because $F(x,y)$ is convex, while Levenberg-Marquardt stalls unless initialized near $(3, 0.5)$; Gauss-Newton may still converge but cannot accelerate beyond Gradient Descent on $F(x,y)$.\n\nD. With backtracking or trust-region safeguards in place, all three methods produce essentially identical paths from $\\mathbf{x}_0$ because line searches make the iterates invariant to the choice of direction, which removes distinctions among Gauss-Newton, Levenberg-Marquardt, and Gradient Descent on $F(x,y)$.", "solution": "The user has provided a problem statement regarding the qualitative convergence behavior of three standard optimization algorithms—Gradient Descent (GD), Gauss-Newton (GN), and Levenberg-Marquardt (LM)—on a specific nonlinear least-squares problem.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the givens and validate the problem statement.\n\n**Givens:**\n1.  **Residuals:** The problem is defined by three residual functions of two variables, $\\mathbf{x} = (x, y)$:\n    -   $r_1(x,y) = 1.5 - x + x y$\n    -   $r_2(x,y) = 2.25 - x + x y^2$\n    -   $r_3(x,y) = 2.625 - x + x y^3$\n2.  **Objective Function:** The objective function to be minimized is the sum of the squares of the residuals:\n    $$F(x,y) = r_1(x,y)^2 + r_2(x,y)^2 + r_3(x,y)^2$$\n3.  **Global Minimizer:** The global minimum is at $\\mathbf{x}^* = (3, 0.5)$, where the objective function value is $F(3, 0.5) = 0$. This indicates it is a **zero-residual problem**, meaning $r_i(3, 0.5) = 0$ for all $i \\in \\{1, 2, 3\\}$.\n4.  **Starting Point:** The initial iterate is $\\mathbf{x}_0 = (x_0, y_0) = (-2, -2)$.\n5.  **Algorithms:**\n    -   **Gradient Descent (GD):** Uses the steepest descent direction with a backtracking line search.\n    -   **Gauss-Newton (GN):** Uses the step derived from the normal equations, with a backtracking line search.\n    -   **Levenberg-Marquardt (LM):** Uses an adaptive damping parameter, interpreted as a trust-region method.\n6.  **Assumption:** All methods employ safeguards that ensure a monotonic decrease in the objective function $F$ at each iteration.\n7.  **Question:** The task is to characterize the qualitative convergence paths of these three methods from the starting point $\\mathbf{x}_0$ based on their fundamental definitions and the structure of the problem.\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem uses a standard test function (Beale's function) from the field of numerical optimization. The algorithms (GD, GN, LM) and their properties are fundamental concepts in numerical analysis and scientific computing. The problem is firmly grounded in established mathematical principles.\n2.  **Well-Posed:** The problem is well-posed. It asks for a qualitative comparison of algorithm behaviors, which is a standard method of analysis in numerical optimization. The provided information (function, starting point, algorithms, and known solution) is sufficient to make a reasoned, qualitative judgment based on the theoretical properties of the methods.\n3.  **Objective:** The problem is stated using precise mathematical definitions and asks for a characterization based on these objective properties, not on subjective opinion.\n\n**Verdict:** The problem statement is valid. It is a well-posed, scientifically sound question in numerical optimization. I will proceed with the solution.\n\n### Step 2: Derivation and Analysis\n\nThe objective function is of the form $F(\\mathbf{x}) = \\frac{1}{2} \\|\\mathbf{r}(\\mathbf{x})\\|_2^2$, although the factor of $\\frac{1}{2}$ is omitted in the problem description. This omission scales the gradient and Hessian but does not change the qualitative behavior of the step directions. Let $\\mathbf{r}(\\mathbf{x}) = [r_1, r_2, r_3]^\\top$.\n\nThe gradient of $F$ is $\\nabla F(\\mathbf{x}) = J(\\mathbf{x})^\\top \\mathbf{r}(\\mathbf{x})$, where $J(\\mathbf{x})$ is the Jacobian of $\\mathbf{r}(\\mathbf{x})$.\nThe Hessian of $F$ is $\\nabla^2 F(\\mathbf{x}) = J(\\mathbf{x})^\\top J(\\mathbf{x}) + \\sum_{i=1}^3 r_i(\\mathbf{x}) \\nabla^2 r_i(\\mathbf{x})$.\n\nLet's analyze the properties of the problem at the starting point $\\mathbf{x}_0 = (-2, -2)$:\n-   $r_1(-2, -2) = 1.5 - (-2) + (-2)(-2) = 1.5 + 2 + 4 = 7.5$\n-   $r_2(-2, -2) = 2.25 - (-2) + (-2)(-2)^2 = 2.25 + 2 - 8 = -3.75$\n-   $r_3(-2, -2) = 2.625 - (-2) + (-2)(-2)^3 = 2.625 + 2 + 16 = 20.625$\nThe residuals are large, confirming that $\\mathbf{x}_0$ is far from the solution. The Beale function is known to have a very narrow, curved valley leading to its minimum.\n\nNow, we analyze the behavior of each method from this starting point.\n\n**Gradient Descent (GD)**\n-   **Step Direction:** $\\mathbf{p}_k = -\\nabla F(\\mathbf{x}_k)$. This is the direction of steepest descent.\n-   **Behavior:** In a narrow, curved valley, the steepest descent direction typically points nearly perpendicular to the valley's floor, towards the opposite wall. A line search will find the minimum along this direction, which is a short step across the valley. The next iteration's gradient will point back across the valley. This leads to a characteristic \"zig-zag\" pattern, making very slow progress along the valley towards the minimum. GD is a first-order method and is well-known to suffer from slow convergence in such ill-conditioned problems.\n\n**Gauss-Newton (GN)**\n-   **Step Direction:** The step $\\mathbf{p}_k$ is the solution to the normal equations $(J(\\mathbf{x}_k)^\\top J(\\mathbf{x}_k)) \\mathbf{p}_k = -J(\\mathbf{x}_k)^\\top \\mathbf{r}(\\mathbf{x}_k)$. This is derived by approximating the true Hessian with $\\nabla^2 F(\\mathbf{x}) \\approx J(\\mathbf{x})^\\top J(\\mathbf{x})$.\n-   **Behavior:** This approximation is valid when the term $\\sum_i r_i(\\mathbf{x}) \\nabla^2 r_i(\\mathbf{x})$ is small. This occurs near a zero-residual solution (since $r_i \\to 0$) or for nearly linear problems (since $\\nabla^2 r_i \\approx 0$). At the start point $\\mathbf{x}_0 = (-2, -2)$, the residuals are large, so the approximation is poor. The GN method, using this flawed model of the local curvature, is likely to compute an \"ambitious\" step that is very large and points in a direction that does not lead to a good reduction in $F$. Without a safeguard, the method could easily diverge. The specified backtracking line search acts as a crucial safeguard by repeatedly reducing the step size until a decrease in $F$ is achieved. However, the search is still performed along a poorly chosen direction, so while divergence is avoided, the path can be erratic, and it may take many backtracking steps to find an acceptable (and likely very small) step.\n\n**Levenberg-Marquardt (LM)**\n-   **Step Direction:** The step $\\mathbf{p}_k$ is the solution to the damped system $(J(\\mathbf{x}_k)^\\top J(\\mathbf{x}_k) + \\lambda_k I) \\mathbf{p}_k = -J(\\mathbf{x}_k)^\\top \\mathbf{r}(\\mathbf{x}_k)$. The damping parameter $\\lambda_k \\ge 0$ is adapted at each iteration.\n-   **Behavior:** LM is designed to overcome the instability of GN.\n    -   When far from the solution (like at $\\mathbf{x}_0$), a well-implemented LM algorithm will find that the pure GN step (with $\\lambda_k=0$) is poor. It will increase $\\lambda_k$. For large $\\lambda_k$, the term $\\lambda_k I$ dominates the matrix, and the step becomes $\\mathbf{p}_k \\approx -\\frac{1}{\\lambda_k} J(\\mathbf{x}_k)^\\top \\mathbf{r}(\\mathbf{x}_k)$, which is a short step in the steepest descent direction. This makes the method behave like a cautious, stable GD method initially.\n    -   As the iterates approach the solution and enter the valley, the residuals decrease, and the GN model becomes more accurate. The algorithm can then reduce $\\lambda_k$, allowing the method to take steps that are increasingly similar to GN steps.\n    -   Near the zero-residual solution $\\mathbf{x}^* = (3, 0.5)$, the method will behave almost identically to GN, exhibiting rapid (quadratic) convergence.\n-   This adaptive nature provides both robustness far from the solution and speed near it. The path is initially conservative like GD, then becomes more direct and efficient like GN as it nears the solution.\n\n### Step 3: Evaluation of Options\n\n-   **A. Starting from $\\mathbf{x}_0 = (-2, -2)$, Gradient Descent takes short steps along the negative gradient, zig-zagging while it attempts to follow the curved narrow valley toward $(3, 0.5)$ and converges slowly; Gauss-Newton, using the $J(\\mathbf{x})^\\top J(\\mathbf{x})$ curvature model far from the solution, tends to propose overly aggressive or misaligned steps that can leave the valley unless curtailed by the line search; Levenberg-Marquardt begins with heavily damped steps that behave like a safe Gradient Descent in the trust region, then progressively reduces damping and transitions to Gauss-Newton-like steps as it enters the valley, producing a path that is initially conservative and later more direct toward $(3, 0.5)$.**\n    - This option accurately describes the well-known theoretical and practical behaviors of all three algorithms under the given conditions. The description of GD's zig-zagging, GN's unreliability far from the solution, and LM's adaptive nature are all correct.\n    - **Verdict: Correct.**\n\n-   **B. Gauss-Newton always converges fastest from any initial guess because its curvature model is the exact Hessian for sum-of-squares problems, so its path is essentially a straight line to $(3, 0.5)$; Levenberg-Marquardt and Gradient Descent both oscillate and are necessarily slower.**\n    - This statement contains a fundamental error. The Gauss-Newton curvature model, $J(\\mathbf{x})^\\top J(\\mathbf{x})$, is an *approximation* of the true Hessian, $\\nabla^2 F(\\mathbf{x}) = J(\\mathbf{x})^\\top J(\\mathbf{x}) + \\sum_i r_i(\\mathbf{x}) \\nabla^2 r_i(\\mathbf{x})$. It is not exact unless the second term is zero, which is not true in general. Because it is an approximation that is poor when residuals are large, GN is not guaranteed to converge from any initial guess, let alone fastest.\n    - **Verdict: Incorrect.**\n\n-   **C. Gradient Descent converges in the fewest iterations because $F(x,y)$ is convex, while Levenberg-Marquardt stalls unless initialized near $(3, 0.5)$; Gauss-Newton may still converge but cannot accelerate beyond Gradient Descent on $F(x,y)$.**\n    - This statement is incorrect on multiple counts. First-order methods like GD typically require many more iterations than quasi-second-order methods like LM or GN (when GN works). Second, the claim that LM stalls unless initialized near the solution is the opposite of its primary design feature; LM is specifically designed for robustness far from the solution. Third, the convexity of $F(x,y)$ is not guaranteed, and even if it were convex, GD is not the fastest algorithm in terms of iteration count.\n    - **Verdict: Incorrect.**\n\n-   **D. With backtracking or trust-region safeguards in place, all three methods produce essentially identical paths from $\\mathbf{x}_0$ because line searches make the iterates invariant to the choice of direction, which removes distinctions among Gauss-Newton, Levenberg-Marquardt, and Gradient Descent on $F(x,y)$.**\n    - This reflects a misunderstanding of optimization algorithms. The core difference between these methods *is* the choice of search direction. A safeguard like a line search only determines the step *length* along that chosen direction; it does not change the direction itself. A trust region can modify the direction (as in LM, blending GD and GN), but it does so in a way that is fundamentally tied to the algorithm's definition. The paths are not identical; in fact, they are qualitatively very different, as described in option A.\n    - **Verdict: Incorrect.**\n\nBased on this analysis, option A provides the only correct and complete qualitative characterization.", "answer": "$$\\boxed{A}$$", "id": "3247386"}]}