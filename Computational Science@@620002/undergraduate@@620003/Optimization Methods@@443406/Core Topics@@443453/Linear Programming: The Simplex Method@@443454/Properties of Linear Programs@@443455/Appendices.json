{"hands_on_practices": [{"introduction": "Before searching for an optimal solution to a linear program, we must first answer a more fundamental question: does a feasible solution even exist? This practice problem [@problem_id:3165483] delves into the concept of infeasibility, where constraints are mutually contradictory. You will learn to identify a minimal set of conflicting constraints, known as an Irreducible Infeasible Subsystem (IIS), and use the powerful ideas from duality, like Farkas' Lemma, to formally certify that no solution can satisfy them.", "problem": "Consider the following feasibility Linear Program (LP) in variables $x \\in \\mathbb{R}^{2}$:\n- Constraints:\n  1) $x_{1} \\geq 1$\n  2) $x_{2} \\geq 1$\n  3) $x_{1} + x_{2} \\leq 1$\n  4) $x_{1} \\leq 2$\n  5) $x_{2} \\leq 2$\n\nYour tasks are:\n- Using only core definitions of feasibility for linear inequalities and the Alternative Theorem for linear inequalities (Farkas' Lemma), determine a subset of constraints that forms an Irreducible Infeasible Subsystem (IIS), where Irreducible Infeasible Subsystem (IIS) means a set of constraints that is infeasible but becomes feasible if any single constraint from the set is removed.\n- Identify a nonnegative vector of multipliers (a dual ray) that certifies infeasibility of that IIS by combining the inequalities into a contradiction of the form $0 \\leq \\text{negative}$, after placing all constraints into the unified form $A x \\leq b$.\n- Suppose you are allowed to minimally relax only the right-hand sides of the three IIS constraints to restore feasibility. Specifically, replace $x_{1} \\geq 1$, $x_{2} \\geq 1$, and $x_{1} + x_{2} \\leq 1$ by $x_{1} \\geq 1 - \\rho_{1}$, $x_{2} \\geq 1 - \\rho_{2}$, and $x_{1} + x_{2} \\leq 1 + \\rho_{3}$ with $\\rho_{1}, \\rho_{2}, \\rho_{3} \\geq 0$. Formulate the minimal-relaxation problem that finds the smallest total relaxation magnitude $s^{\\star} = \\rho_{1} + \\rho_{2} + \\rho_{3}$ such that the relaxed system is feasible, and then compute the exact value of $s^{\\star}$.\n\nReport only the value of $s^{\\star}$ as your final answer. No rounding is required.", "solution": "The problem asks for a three-part analysis of a given system of linear inequalities. First, we must identify an Irreducible Infeasible Subsystem (IIS). Second, we must provide a dual certificate of this infeasibility using Farkas' Lemma. Third, we must formulate and solve a problem to find the minimum total relaxation of the right-hand sides of the IIS constraints to restore feasibility.\n\nThe given system of linear inequalities in variables $x = (x_1, x_2) \\in \\mathbb{R}^{2}$ is:\n1) $x_{1} \\geq 1$\n2) $x_{2} \\geq 1$\n3) $x_{1} + x_{2} \\leq 1$\n4) $x_{1} \\leq 2$\n5) $x_{2} \\leq 2$\n\nPart 1: Identification of the Irreducible Infeasible Subsystem (IIS)\n\nAn IIS is a subset of constraints that is infeasible, but from which the removal of any single constraint renders the subset feasible.\nBy inspection, constraints ($1$), ($2$), and ($3$) appear to be in conflict. Let's analyze this subset.\nFrom constraint ($1$), we have $x_{1} \\geq 1$.\nFrom constraint ($2$), we have $x_{2} \\geq 1$.\nSumming these two inequalities yields $x_{1} + x_{2} \\geq 1 + 1 = 2$.\nHowever, constraint ($3$) states that $x_{1} + x_{2} \\leq 1$.\nThe combination of these three constraints leads to the contradiction $2 \\leq x_{1} + x_{2} \\leq 1$, which is impossible. Therefore, the subsystem consisting of constraints ($1$), ($2$), and ($3$) is infeasible.\n\nTo confirm this is an *irreducible* subsystem, we must show that removing any one of these three constraints makes the remaining system feasible.\n- Remove constraint ($1$): The system is $x_{2} \\geq 1$ and $x_{1} + x_{2} \\leq 1$. This implies $x_1 \\leq 1 - x_2$. Since $x_2 \\geq 1$, we have $1-x_2 \\leq 0$, so $x_1 \\leq 0$. A feasible point satisfying $x_2 \\geq 1$ and $x_1 \\leq 0$ is, for instance, $(x_{1}, x_{2}) = (0, 1)$. This point satisfies $x_2=1 \\geq 1$ and $x_1+x_2 = 0+1=1 \\leq 1$. The system is feasible.\n- Remove constraint ($2$): The system is $x_{1} \\geq 1$ and $x_{1} + x_{2} \\leq 1$. Symmetrically, this implies $x_2 \\leq 1 - x_1$. Since $x_1 \\geq 1$, we have $1-x_1 \\leq 0$, so $x_2 \\leq 0$. A feasible point is $(x_{1}, x_{2}) = (1, 0)$. This system is feasible.\n- Remove constraint ($3$): The system is $x_{1} \\geq 1$ and $x_{2} \\geq 1$. A feasible point is $(x_{1}, x_{2}) = (1, 1)$. This system is feasible.\n\nSince the subsystem $\\{1, 2, 3\\}$ is infeasible and removing any single constraint restores feasibility, it is an Irreducible Infeasible Subsystem (IIS).\n\nPart 2: Farkas' Lemma Certificate of Infeasibility\n\nFarkas' Lemma, in a form for linear inequalities, states that a system $Ax \\leq b$ is infeasible if and only if there exists a vector of multipliers $y \\geq 0$ such that $y^{T}A = 0$ and $y^{T}b < 0$.\nFirst, we write the IIS constraints in the standard form $Ax \\leq b$:\n1) $x_{1} \\geq 1 \\implies -x_{1} \\leq -1$\n2) $x_{2} \\geq 1 \\implies -x_{2} \\leq -1$\n3) $x_{1} + x_{2} \\leq 1$\n\nThis gives the matrix representation:\n$$\nA = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix}, \\quad x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}, \\quad b = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nWe seek a vector $y = (y_1, y_2, y_3)^T$ with $y_1, y_2, y_3 \\geq 0$ satisfying the conditions of the lemma.\nThe condition $y^{T}A = 0$ translates to:\n$$\n\\begin{pmatrix} y_{1} & y_{2} & y_{3} \\end{pmatrix} \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} -y_{1} + y_{3} & -y_{2} + y_{3} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}\n$$\nThis gives the system of equations:\n$-y_{1} + y_{3} = 0 \\implies y_{1} = y_{3}$\n$-y_{2} + y_{3} = 0 \\implies y_{2} = y_{3}$\nThus, we must have $y_{1} = y_{2} = y_{3}$. Since $y \\geq 0$ and we need a non-trivial certificate, let's choose the simplest positive multipliers, $y_{1} = y_{2} = y_{3} = 1$. So, a valid vector of multipliers (a generator for the dual ray) is $y = (1, 1, 1)^T$.\n\nNow we check the second condition, $y^{T}b < 0$:\n$$\ny^{T}b = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = (1)(-1) + (1)(-1) + (1)(1) = -1 - 1 + 1 = -1\n$$\nSince $-1 < 0$, the condition is satisfied. The vector $y = (1, 1, 1)^T$ is a valid set of Farkas multipliers that certifies the infeasibility of the IIS. Multiplying the inequalities in $Ax \\leq b$ by the corresponding components of $y$ and summing them gives the manifest contradiction $0 \\leq -1$.\n\nPart 3: Minimal Relaxation Problem\n\nWe are asked to relax the RHS of the IIS constraints to restore feasibility. The relaxed system is:\n1) $x_{1} \\geq 1 - \\rho_{1}$\n2) $x_{2} \\geq 1 - \\rho_{2}$\n3) $x_{1} + x_{2} \\leq 1 + \\rho_{3}$\nwhere $\\rho_{1}, \\rho_{2}, \\rho_{3} \\geq 0$. We wish to minimize the total relaxation $s = \\rho_{1} + \\rho_{2} + \\rho_{3}$.\n\nIn the standard form $Ax \\leq b'$, the relaxed right-hand-side vector $b'$ is:\n$$\nb' = \\begin{pmatrix} -1 + \\rho_{1} \\\\ -1 + \\rho_{2} \\\\ 1 + \\rho_{3} \\end{pmatrix}\n$$\nFor the relaxed system $Ax \\leq b'$ to be feasible, the Alternative Theorem (Farkas' Lemma) requires that for every $y \\geq 0$ with $y^T A = 0$, we must have $y^T b' \\geq 0$.\nFrom Part 2, the vectors $y$ satisfying these conditions are of the form $y = (k, k, k)^T$ for any $k \\geq 0$. To satisfy the feasibility condition for all such $y$, we only need to check it for the generator of the ray, $y = (1, 1, 1)^T$.\nThe condition is $y^T b' \\geq 0$:\n$$\n\\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} -1 + \\rho_{1} \\\\ -1 + \\rho_{2} \\\\ 1 + \\rho_{3} \\end{pmatrix} \\geq 0\n$$\n$$\n(-1 + \\rho_{1}) + (-1 + \\rho_{2}) + (1 + \\rho_{3}) \\geq 0\n$$\n$$\n-1 + \\rho_{1} + \\rho_{2} + \\rho_{3} \\geq 0\n$$\nThis simplifies to $\\rho_{1} + \\rho_{2} + \\rho_{3} \\geq 1$.\nThe minimal relaxation problem is thus to find the minimum of $s = \\rho_{1} + \\rho_{2} + \\rho_{3}$ subject to the constraints $\\rho_{1} + \\rho_{2} + \\rho_{3} \\geq 1$ and $\\rho_{1}, \\rho_{2}, \\rho_{3} \\geq 0$.\nThe objective function is $s$ itself, and the primary constraint is $s \\geq 1$. Therefore, the minimum possible value for $s$ is $1$.\nThe minimal total relaxation magnitude is $s^{\\star} = 1$. This can be achieved, for example, by setting $\\rho_3 = 1$ and $\\rho_1 = \\rho_2 = 0$, which makes the system $x_1 \\geq 1, x_2 \\geq 1, x_1+x_2 \\leq 2$ feasible at the point $(1,1)$.", "answer": "$$\\boxed{1}$$", "id": "3165483"}, {"introduction": "Once we confirm a linear program is feasible, the next step is to find an optimal solution. This exercise [@problem_id:3165494] provides a highly geometric perspective on optimality using the familiar shape of a cube as the feasible set. By exploring which objective functions make each corner of the cube optimal, you will develop a deep intuition for the relationship between the cost vector and the geometry of the feasible region, leading to the formal definition of a normal cone.", "problem": "Consider the Linear Program (LP) in dimension $3$ given by maximizing the linear functional $c^{\\top} x$ over a cube:\nmaximize $c^{\\top} x$ subject to $0 \\leq x_i \\leq 1$ for $i \\in \\{1,2,3\\}$, where $c \\in \\mathbb{R}^3$ is a given cost vector and $x \\in \\mathbb{R}^3$ is the decision variable. Let $P \\subset \\mathbb{R}^3$ denote the feasible set, which is the cube $[0,1]^3$. Use only core definitions from linear programming and convex analysis, including the definition of a polyhedron, a vertex, and the normal cone $N_P(v)$ at a point $v \\in P$ defined by\n$$\nN_P(v) \\;=\\; \\{\\, c \\in \\mathbb{R}^3 \\;:\\; c^{\\top}(x - v) \\leq 0 \\text{ for all } x \\in P \\,\\}.\n$$\nPerform the following steps.\n\n(a) Express the cube constraints in matrix form $A x \\leq b$, specifying $A \\in \\mathbb{R}^{6 \\times 3}$ and $b \\in \\mathbb{R}^6$ explicitly.\n\n(b) For a vertex $v \\in \\{0,1\\}^3$, use the definition of the normal cone to characterize the set of cost vectors $c \\in \\mathbb{R}^3$ for which $v$ is an optimal solution of the LP. State the condition on the signs of the components of $c$ in terms of the components of $v$.\n\n(c) Describe the normal cones $N_P(v)$ geometrically for all $8$ vertices $v \\in \\{0,1\\}^3$ by identifying which orthant of $\\mathbb{R}^3$ each $N_P(v)$ equals.\n\n(d) Assume now that the cost vector $c$ is random with a continuous, spherically symmetric distribution in $\\mathbb{R}^3$, meaning its probability density function can be written as $f(c) = g(\\|c\\|)$ for some integrable function $g : [0,\\infty) \\to \\mathbb{R}_+$, and that $c$ has no atoms (so $\\mathbb{P}(c_i = 0) = 0$ for all $i$). What is the exact probability that the optimal solution of the LP is $x^{\\star} = (1,1,1)$? Express your final answer as a reduced fraction. Do not round.", "solution": "(a) Express the cube constraints in matrix form $A x \\leq b$.\n\nThe feasible set $P$ is the cube $[0,1]^3$, defined by the constraints $0 \\leq x_i \\leq 1$ for each component $i \\in \\{1, 2, 3\\}$ of the vector $x = (x_1, x_2, x_3)^{\\top}$. These constraints can be separated into two groups of three inequalities each.\n\nThe upper bounds are:\n$x_1 \\leq 1$\n$x_2 \\leq 1$\n$x_3 \\leq 1$\n\nThe lower bounds are $x_i \\ge 0$, which are equivalent to $-x_i \\leq 0$:\n$-x_1 \\leq 0$\n$-x_2 \\leq 0$\n$-x_3 \\leq 0$\n\nTo express these $6$ inequalities in the matrix form $A x \\leq b$, where $A \\in \\mathbb{R}^{6 \\times 3}$, $x \\in \\mathbb{R}^3$, and $b \\in \\mathbb{R}^6$, we construct the matrix $A$ and vector $b$ row by row.\n\nThe first three rows correspond to the upper bounds:\n$$\n\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} x \\leq 1 \\\\\n\\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} x \\leq 1 \\\\\n\\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} x \\leq 1\n$$\nThis block can be represented by the $3 \\times 3$ identity matrix $I_3$.\n\nThe next three rows correspond to the lower bounds:\n$$\n\\begin{pmatrix} -1 & 0 & 0 \\end{pmatrix} x \\leq 0 \\\\\n\\begin{pmatrix} 0 & -1 & 0 \\end{pmatrix} x \\leq 0 \\\\\n\\begin{pmatrix} 0 & 0 & -1 \\end{pmatrix} x \\leq 0\n$$\nThis block can be represented by the negative of the identity matrix, $-I_3$.\n\nCombining these, the matrix $A \\in \\mathbb{R}^{6 \\times 3}$ is formed by stacking $I_3$ on top of $-I_3$, and the vector $b \\in \\mathbb{R}^6$ is formed by stacking a vector of ones on top of a vector of zeros.\n\nExplicitly, the matrix $A$ is:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n$$\nAnd the vector $b$ is:\n$$\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\n\n(b) For a vertex $v \\in \\{0,1\\}^3$, characterize the set of cost vectors $c$ for which $v$ is an optimal solution.\n\nA feasible point $v \\in P$ is an optimal solution to the problem of maximizing $c^{\\top}x$ over $P$ if and only if $c^{\\top}v \\ge c^{\\top}x$ for all $x \\in P$. This is equivalent to the condition $c^{\\top}(x - v) \\leq 0$ for all $x \\in P$. The set of cost vectors $c$ that satisfy this condition is precisely the normal cone to $P$ at $v$, denoted $N_P(v)$. The problem asks us to characterize this set.\n\nLet $v = (v_1, v_2, v_3)^{\\top}$ be a vertex of the cube, so $v_i \\in \\{0, 1\\}$ for $i \\in \\{1, 2, 3\\}$. We need to find the conditions on $c = (c_1, c_2, c_3)^{\\top}$ such that $\\sum_{i=1}^3 c_i (x_i - v_i) \\leq 0$ for all $x \\in [0,1]^3$.\n\nTo find necessary conditions, we can select specific points $x \\in P$. Consider a point $x$ which differs from $v$ only in the $i$-th component. Let $x_j = v_j$ for $j \\neq i$, and let $x_i$ be free to vary in $[0,1]$. For the inequality to hold, we must have $c_i(x_i - v_i) \\leq 0$ for all $x_i \\in [0,1]$.\n\nWe analyze this for the two possible values of $v_i$:\nCase 1: $v_i = 0$. The condition is $c_i(x_i - 0) \\leq 0$, i.e., $c_i x_i \\leq 0$, for all $x_i \\in [0,1]$. Since we can choose $x_i=1$, we must have $c_i \\cdot 1 \\le 0$, which implies $c_i \\leq 0$. If $c_i \\leq 0$, then for any non-negative $x_i$, the product $c_i x_i$ is non-positive, so the condition is satisfied.\n\nCase 2: $v_i = 1$. The condition is $c_i(x_i - 1) \\leq 0$ for all $x_i \\in [0,1]$. Since we can choose $x_i=0$, we must have $c_i(0-1) \\le 0$, which implies $-c_i \\leq 0$, or $c_i \\geq 0$. If $c_i \\geq 0$, then for any $x_i \\in [0,1]$, the term $(x_i - 1)$ is non-positive, so the product $c_i(x_i-1)$ is non-positive. The condition is satisfied.\n\nThese conditions on each component $c_i$ are also sufficient. If the sign of each $c_i$ is determined as above, then for any $x \\in [0,1]^3$, each term $c_i(x_i - v_i)$ in the sum $\\sum_{i=1}^3 c_i(x_i - v_i)$ is non-positive. The sum of non-positive terms is itself non-positive.\n\nTherefore, a vertex $v \\in \\{0,1\\}^3$ is an optimal solution for a cost vector $c$ if and only if for each $i \\in \\{1,2,3\\}$:\n- $c_i \\geq 0$ if $v_i = 1$\n- $c_i \\leq 0$ if $v_i = 0$\n\nThis can be stated compactly as $c_i(2v_i - 1) \\geq 0$ for each $i \\in \\{1,2,3\\}$.\n\n(c) Describe the normal cones $N_P(v)$ geometrically for all $8$ vertices.\n\nThe normal cone $N_P(v)$ is the set of vectors $c$ satisfying the conditions derived in part (b). These conditions constrain the sign of each component of $c$, which geometrically defines one of the $2^3 = 8$ orthants of $\\mathbb{R}^3$. The orthants are the regions of space defined by the signs of the coordinates.\n\nThe correspondence between the $8$ vertices $v$ and the orthants that constitute their normal cones $N_P(v)$ is as follows:\n- For $v = (0,0,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\leq 0, c_3 \\leq 0 \\}$ (the non-positive orthant).\n- For $v = (1,0,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\leq 0, c_3 \\leq 0 \\}$.\n- For $v = (0,1,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\geq 0, c_3 \\leq 0 \\}$.\n- For $v = (0,0,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\leq 0, c_3 \\geq 0 \\}$.\n- For $v = (1,1,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\geq 0, c_3 \\leq 0 \\}$.\n- For $v = (1,0,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\leq 0, c_3 \\geq 0 \\}$.\n- For $v = (0,1,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\geq 0, c_3 \\geq 0 \\}$.\n- For $v = (1,1,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\geq 0, c_3 \\geq 0 \\}$ (the non-negative orthant).\n\nThese $8$ normal cones are the 8 closed orthants of $\\mathbb{R}^3$. They partition the entire space, i.e., $\\bigcup_{v \\in \\{0,1\\}^3} N_P(v) = \\mathbb{R}^3$. Their interiors are disjoint.\n\n(d) Calculate the probability that the optimal solution is $x^{\\star} = (1,1,1)$.\n\nThe optimal solution is $x^{\\star} = (1,1,1)$ if and only if the cost vector $c$ belongs to the normal cone $N_P((1,1,1))$. From part (c), this is the non-negative orthant:\n$$\nN_P((1,1,1)) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\geq 0, c_3 \\geq 0 \\}\n$$\nWe need to find the probability $\\mathbb{P}(c \\in N_P((1,1,1)))$, which is $\\mathbb{P}(c_1 \\geq 0, c_2 \\geq 0, c_3 \\geq 0)$.\n\nThe cost vector $c$ is drawn from a continuous, spherically symmetric distribution. A distribution is spherically symmetric if its probability density function $f(c)$ depends only on the norm of $c$, i.e., $f(c) = g(\\|c\\|)$. This property implies that the probability measure is invariant under rotations and reflections.\n\nThe three coordinate planes ($c_1=0$, $c_2=0$, $c_3=0$) divide $\\mathbb{R}^3$ into 8 disjoint (except for the boundaries) orthants. The problem specifies that the distribution is continuous and $\\mathbb{P}(c_i=0)=0$. This means the probability of $c$ lying on any of these boundary planes is zero, so we can ignore these boundaries in our probability calculation.\n\nThe spherical symmetry of the distribution means that the probability density is the same at any two points that are equidistant from the origin. All 8 orthants are geometrically congruent; any orthant can be mapped to any other by a sequence of reflections across the coordinate planes. For example, reflecting across the $c_1=0$ plane maps the non-negative orthant to the orthant where $c_1 \\leq 0, c_2 \\geq 0, c_3 \\geq 0$. Since reflections preserve the norm $\\|c\\|$, the probability density function is invariant under these transformations.\n\nConsequently, the probability of the random vector $c$ falling into any of the 8 orthants is the same. Let this common probability be $p$.\n$$\np = \\mathbb{P}(c \\in O_k) \\quad \\text{for each orthant } O_k.\n$$\nThe union of the 8 orthants is the entire space $\\mathbb{R}^3$. Therefore, the sum of their probabilities must be 1:\n$$\n\\sum_{k=1}^8 \\mathbb{P}(c \\in O_k) = 1\n$$\n$$\n8 p = 1\n$$\nThis yields $p = \\frac{1}{8}$.\n\nThe event that the optimal solution is $x^{\\star}=(1,1,1)$ corresponds to the event that $c$ is in the non-negative orthant. The probability of this event is $p$.\nTherefore, the exact probability that the optimal solution is $x^{\\star} = (1,1,1)$ is $\\frac{1}{8}$.", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "3165494"}, {"introduction": "An optimal solution provides a snapshot for a specific problem setup, but in practice, conditions change. This hands-on problem [@problem_id:3165476] guides you through sensitivity analysis, a crucial aspect of linear programming that explores how the optimal solution responds to changes in the problem data. You will use the elegant framework of duality theory to determine the \"shadow price\" of a constraint—the exact rate at which the optimal value changes as a resource limit is relaxed—revealing deep insights into the value of constraints.", "problem": "Consider the following Linear Program (LP), defined in terms of a matrix of coefficients, a right-hand-side vector, and a cost vector. Let the primal LP be\n$$\n\\begin{aligned}\n\\text{maximize} \\quad & c^{\\top} x \\\\\n\\text{subject to} \\quad & A x \\le b, \\\\\n& x \\ge 0,\n\\end{aligned}\n$$\nwhere\n$$\nA = \\begin{pmatrix}\n1 & 1 \\\\\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n4 \\\\\n6 \\\\\n9\n\\end{pmatrix}, \\quad\nc = \\begin{pmatrix}\n3 \\\\\n2\n\\end{pmatrix},\n$$\nand all inequalities are componentwise. For this LP, define the optimal value function $v(b)$ to be the maximum objective value attained at optimality for the right-hand-side vector $b$.\n\nNow consider relaxing a single constraint by increasing its right-hand side. Specifically, for the index $i = 2$, replace the constraint $a_{2}^{\\top} x \\le b_{2}$ by $a_{2}^{\\top} x \\le b_{2} + \\delta$, where $\\delta \\ge 0$ and $a_{2}^{\\top}$ denotes the second row of $A$. Let $b(\\delta) = b + \\delta e_{2}$, where $e_{2}$ is the second standard basis vector in $\\mathbb{R}^{3}$, and let $v(b(\\delta))$ denote the optimal value of the LP with the perturbed right-hand side.\n\nUsing only the core definitions of Linear Programming duality, strong duality, complementary slackness, and the Lagrangian, determine the right-hand derivative at $\\delta = 0$ of the optimal value with respect to this relaxation, that is, compute\n$$\n\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta}.\n$$\nExpress your final answer as a single real number. No rounding is required.", "solution": "This problem asks for the right-hand derivative of the optimal value function of a given linear program (LP) with respect to a perturbation in the right-hand side of the second constraint. This value is known as the shadow price of the constraint. The solution will proceed by first solving the original primal and dual LPs, and then using duality theory to analyze the effect of the perturbation.\n\n### 1. The Primal and Dual LPs\n\nThe primal LP (P) is given by:\n$$\n\\begin{array}{ll}\n\\text{maximize}   & 3x_1 + 2x_2 \\\\\n\\text{subject to} & x_1 + x_2 \\le 4 \\\\\n                  & 2x_1 + x_2 \\le 6 \\\\\n                  & x_1 + 3x_2 \\le 9 \\\\\n                  & x_1, x_2 \\ge 0\n\\end{array}\n$$\nThe dual LP (D) is formulated as:\n$$\n\\begin{array}{ll}\n\\text{minimize}   & 4y_1 + 6y_2 + 9y_3 \\\\\n\\text{subject to} & y_1 + 2y_2 + y_3 \\ge 3 \\\\\n                  & y_1 + y_2 + 3y_3 \\ge 2 \\\\\n                  & y_1, y_2, y_3 \\ge 0\n\\end{array}\n$$\n\n### 2. Solving the Primal LP\n\nThe feasible region of the primal LP is a convex polygon in the first quadrant of the $x_1$-$x_2$ plane. The maximum value of the linear objective function must occur at one of the vertices of this region. We identify the vertices by finding the intersection points of the constraint boundaries.\nThe vertices of the feasible region are:\n- $(0, 0)$: Intersection of $x_1=0$ and $x_2=0$. Objective value: $3(0) + 2(0) = 0$.\n- $(3, 0)$: Intersection of $2x_1 + x_2=6$ and $x_2=0$. Objective value: $3(3) + 2(0) = 9$.\n- $(0, 3)$: Intersection of $x_1 + 3x_2=9$ and $x_1=0$. Objective value: $3(0) + 2(3) = 6$.\n- $(2, 2)$: Intersection of $x_1 + x_2=4$ and $2x_1 + x_2=6$. Objective value: $3(2) + 2(2) = 10$.\n- $(1.5, 2.5)$: Intersection of $x_1 + x_2=4$ and $x_1 + 3x_2=9$. Objective value: $3(1.5) + 2(2.5) = 4.5 + 5 = 9.5$.\n\nComparing the objective values, the maximum is $10$, which occurs at the vertex $(2, 2)$. Thus, the optimal primal solution is $x^* = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$, and the optimal value of the unperturbed LP is $v(b) = c^{\\top}x^* = 10$.\n\n### 3. Finding the Optimal Dual Solution\n\nWe use the complementary slackness conditions to find the optimal dual solution $y^*$. These conditions state that for optimal solutions $x^*$ and $y^*$:\n- $y_i^* (b_i - a_i^{\\top}x^*) = 0$ for each primal constraint $i=1, 2, 3$.\n- $x_j^* ((A^{\\top}y^*)_j - c_j) = 0$ for each primal variable $j=1, 2$.\n\nAt the optimal primal solution $x^* = (2, 2)^{\\top}$:\n- Constraint $1$: $x_1^* + x_2^* = 2 + 2 = 4$. The constraint is binding (active).\n- Constraint $2$: $2x_1^* + x_2^* = 2(2) + 2 = 6$. The constraint is binding.\n- Constraint $3$: $x_1^* + 3x_2^* = 2 + 3(2) = 8 < 9$. The constraint is slack (inactive).\n\nFrom the first set of complementary slackness conditions, since the third primal constraint is slack, the corresponding dual variable must be zero: $y_3^*=0$.\n\nThe primal variables are both non-zero, $x_1^* = 2 > 0$ and $x_2^* = 2 > 0$. From the second set of conditions, this implies that the corresponding dual constraints must be binding:\n- Dual Constraint $1$: $y_1^* + 2y_2^* + y_3^* = 3$.\n- Dual Constraint $2$: $y_1^* + y_2^* + 3y_3^* = 2$.\n\nSubstituting $y_3^*=0$ into this system gives:\n1. $y_1^* + 2y_2^* = 3$\n2. $y_1^* + y_2^* = 2$\n\nSubtracting the second equation from the first yields $y_2^* = 1$. Substituting $y_2^*=1$ into the second equation gives $y_1^* + 1 = 2$, so $y_1^* = 1$.\nThe optimal dual solution is $y^* = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\nTo verify, we check the dual objective value: $b^{\\top}y^* = 4(1) + 6(1) + 9(0) = 10$. This matches the primal optimal value $v(b)$, as expected from the Strong Duality Theorem.\n\n### 4. Analyzing the Perturbed Problem\n\nWe are asked to compute the limit $\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta}$.\nThe optimal value of the perturbed LP, $v(b(\\delta))$, is given by the optimal value of its dual:\n$$\nv(b(\\delta)) = \\min_{y \\in Y} (b(\\delta))^{\\top} y\n$$\nwhere $Y = \\{y \\in \\mathbb{R}^3 \\mid A^{\\top}y \\ge c, y \\ge 0\\}$ is the dual feasible set. The set $Y$ is independent of $b$ and $\\delta$.\nSubstituting $b(\\delta) = b + \\delta e_2$:\n$$\nv(b(\\delta)) = \\min_{y \\in Y} (b + \\delta e_2)^{\\top} y = \\min_{y \\in Y} (b^{\\top}y + \\delta y_2)\n$$\nLet $Y_{opt}(b) \\subseteq Y$ be the set of optimal solutions to the unperturbed dual problem. By definition, for any $y \\in Y_{opt}(b)$, we have $b^{\\top}y = v(b)$.\nFor any $y' \\notin Y_{opt}(b)$, we have $b^{\\top}y' > v(b)$.\nFor a sufficiently small $\\delta > 0$, the term $\\delta y_2$ is a small perturbation to the objective function. An optimal solution to the perturbed problem must be sought from the set $Y_{opt}(b)$, because for any $y' \\notin Y_{opt}(b)$, the gap $b^{\\top}y' - v(b) > 0$ will dominate the $\\delta$-term.\nThus, for small $\\delta > 0$:\n$$\nv(b(\\delta)) = \\min_{y \\in Y_{opt}(b)} (b^{\\top}y + \\delta y_2) = \\min_{y \\in Y_{opt}(b)} (v(b) + \\delta y_2)\n$$\n$$\nv(b(\\delta)) = v(b) + \\delta \\left( \\min_{y \\in Y_{opt}(b)} y_2 \\right)\n$$\nRearranging and taking the limit:\n$$\n\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta} = \\min_{y \\in Y_{opt}(b)} y_2\n$$\nIn our case, the system of equations for the dual variables yielded a unique solution, $y^* = (1, 1, 0)^{\\top}$. Therefore, the set of optimal dual solutions is a singleton: $Y_{opt}(b) = \\{y^*\\}$.\nThe expression for the derivative simplifies to:\n$$\n\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta} = y_2^*\n$$\nFrom our calculation in part (3), we have $y_2^*=1$.\nThe required right-hand derivative is $1$. This value represents the shadow price of the second constraint, indicating the rate at which the optimal objective value increases as the right-hand side of this constraint is relaxed.", "answer": "$$\n\\boxed{1}\n$$", "id": "3165476"}]}