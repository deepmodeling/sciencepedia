{"hands_on_practices": [{"introduction": "This first exercise provides a foundational link between geometric intuition and the formal algebraic conditions of optimality. By visualizing the problem as finding the closest point in a feasible region to a target point, we can understand why an inequality constraint becomes active. This practice will guide you through justifying the active constraint from first principles and then using the Karush-Kuhn-Tucker (KKT) conditions to find the associated Lagrange multiplier [@problem_id:3094283].", "problem": "Consider the constrained optimization problem in the $2$-dimensional Euclidean space: minimize the strictly convex quadratic objective $f(x_1,x_2)=(x_1-1)^2+(x_2-1)^2$ subject to the affine inequality constraint $x_1+x_2\\ge 3$. Starting from the foundational facts that the unconstrained minimizer of a differentiable convex function is characterized by a zero gradient, and that the unique minimizer of the squared Euclidean distance to a point over a closed convex set is its Euclidean projection, justify from first principles why the sum constraint becomes active at the constrained minimizer. Then, using the stationarity and complementary slackness conditions from the Karush–Kuhn–Tucker (KKT) optimality framework for a single inequality constraint $g(x_1,x_2)\\le 0$, compute the value of the optimal Lagrange multiplier associated with the active constraint. Your final answer must be the single numerical value of this multiplier. No rounding is required.", "solution": "### Solution Derivation\n\nThe problem is to solve the following constrained optimization problem:\n$$ \\text{minimize} \\quad f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 $$\n$$ \\text{subject to} \\quad x_1 + x_2 \\ge 3 $$\n\nThe solution is presented in two parts as requested.\n\n**Part 1: Justification for the Active Constraint**\n\nThe objective function $f(x_1, x_2)$ can be interpreted as the squared Euclidean distance between a variable point $x = (x_1, x_2)$ and a fixed point $p = (1, 1)$. Minimizing $f(x)$ is therefore equivalent to finding the point $x$ in the feasible region that is closest to $p$.\n\nFirst, let us find the unconstrained minimizer of $f(x_1, x_2)$. According to the provided principle, the minimizer of a differentiable convex function is located where the gradient is zero. The gradient of $f(x)$ is:\n$$ \\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2(x_2 - 1) \\end{pmatrix} $$\nSetting the gradient to the zero vector gives:\n$$ 2(x_1 - 1) = 0 \\implies x_1 = 1 $$\n$$ 2(x_2 - 1) = 0 \\implies x_2 = 1 $$\nThe unconstrained minimizer is the point $x_{unc} = (1, 1)$, which is the point $p$ itself.\n\nNext, we check if this unconstrained minimizer lies within the feasible region defined by the constraint $x_1 + x_2 \\ge 3$. Substituting the coordinates of $x_{unc}$ into the inequality:\n$$ 1 + 1 = 2 $$\nSince $2  3$, the condition $x_1 + x_2 \\ge 3$ is not satisfied. The unconstrained minimizer $x_{unc}$ is not a feasible point.\n\nThe feasible set is $S = \\{ (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 3 \\}$. This set is a closed half-plane, which is a closed convex set. The problem is to find the point in $S$ that minimizes the squared Euclidean distance to the point $p = (1, 1)$. Based on the second provided principle, this solution is the Euclidean projection of $p$ onto the set $S$.\n\nSince the point $p = (1, 1)$ lies outside the closed convex set $S$, its projection onto $S$ must lie on the boundary of $S$. The boundary of the feasible set $S$ is the line defined by the equation $x_1 + x_2 = 3$.\nTherefore, the constrained minimizer, let us call it $x^* = (x_1^*, x_2^*)$, must satisfy the equality:\n$$ x_1^* + x_2^* = 3 $$\nWhen an inequality constraint holds with equality at a specific point, it is said to be an **active constraint** at that point. Thus, we have justified from first principles that the constraint $x_1 + x_2 \\ge 3$ must be active at the solution.\n\n**Part 2: Computation of the Lagrange Multiplier using KKT Conditions**\n\nTo apply the KKT framework, we first write the problem in the standard form:\n$$ \\text{minimize} \\quad f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 $$\n$$ \\text{subject to} \\quad g(x_1, x_2) = 3 - x_1 - x_2 \\le 0 $$\nThe Lagrangian function $L(x_1, x_2, \\lambda)$ is defined as:\n$$ L(x_1, x_2, \\lambda) = f(x_1, x_2) + \\lambda g(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 + \\lambda(3 - x_1 - x_2) $$\nwhere $\\lambda$ is the Lagrange multiplier associated with the constraint $g(x_1, x_2) \\le 0$.\n\nThe Karush–Kuhn–Tucker (KKT) conditions for an optimal solution $(x_1^*, x_2^*)$ with multiplier $\\lambda^*$ are:\n1.  **Stationarity:** $\\nabla_x L(x_1^*, x_2^*, \\lambda^*) = 0$\n2.  **Primal Feasibility:** $g(x_1^*, x_2^*) \\le 0$\n3.  **Dual Feasibility:** $\\lambda^* \\ge 0$\n4.  **Complementary Slackness:** $\\lambda^* g(x_1^*, x_2^*) = 0$\n\nLet's apply these conditions. The stationarity condition requires the partial derivatives of the Lagrangian with respect to $x_1$ and $x_2$ to be zero:\n$$ \\frac{\\partial L}{\\partial x_1} = 2(x_1 - 1) - \\lambda = 0 \\implies 2x_1 - 2 = \\lambda $$\n$$ \\frac{\\partial L}{\\partial x_2} = 2(x_2 - 1) - \\lambda = 0 \\implies 2x_2 - 2 = \\lambda $$\nFrom these two equations, we deduce that $2x_1 - 2 = 2x_2 - 2$, which simplifies to $x_1 = x_2$.\n\nThe complementary slackness condition $\\lambda (3 - x_1 - x_2) = 0$ implies that either $\\lambda = 0$ or $3 - x_1 - x_2 = 0$.\nIf $\\lambda = 0$, the stationarity conditions would give $x_1=1$ and $x_2=1$. As established in Part 1, this point $(1, 1)$ is not feasible because it violates the primal feasibility condition $3 - x_1 - x_2 \\le 0$ (since $3-1-1 = 1  0$).\nTherefore, we must have $\\lambda \\ne 0$. Due to dual feasibility ($\\lambda \\ge 0$), it must be that $\\lambda  0$. From complementary slackness, this forces $g(x_1, x_2) = 0$, which is $3 - x_1 - x_2 = 0$, or $x_1 + x_2 = 3$. This confirms our earlier finding that the constraint is active.\n\nWe now have a system of two linear equations for $x_1$ and $x_2$:\n1. $x_1 = x_2$\n2. $x_1 + x_2 = 3$\n\nSubstituting the first equation into the second gives $x_1 + x_1 = 3$, which leads to $2x_1 = 3$, so $x_1 = 1.5$. Since $x_1 = x_2$, we also have $x_2 = 1.5$. The optimal point is $x^* = (1.5, 1.5)$.\n\nFinally, we compute the value of the optimal Lagrange multiplier $\\lambda^*$ using the stationarity condition:\n$$ \\lambda^* = 2x_1^* - 2 = 2(1.5) - 2 = 3 - 2 = 1 $$\nThis value satisfies dual feasibility, as $\\lambda^* = 1 \\ge 0$. All KKT conditions are satisfied. The value of the optimal Lagrange multiplier is $1$.", "answer": "$$\\boxed{1}$$", "id": "3094283"}, {"introduction": "Building on the concept of a single active constraint, this practice explores a scenario with multiple constraints defining the feasible region. The challenge is to understand how the optimal solution changes depending on the location of the unconstrained minimum relative to these constraints. You will derive a complete map of the solution, expressed as a piecewise function, which reveals how different sets of constraints become active in different regions of the parameter space [@problem_id:3094227].", "problem": "Consider the convex optimization problem of minimizing the quadratic objective $f(x,y)=(x-1)^{2}+(y+3)^{2}$ subject to the linear inequality constraints $y\\ge x$ and $x\\ge 0$. The unconstrained minimizer of $f$ is the point where its gradient vanishes. Treat the constrained minimizer as the Euclidean projection of the unconstrained minimizer onto the feasible set, and derive the solution using the Karush-Kuhn-Tucker (KKT) conditions. Starting from the core definitions of convexity, Euclidean projection onto a closed convex set, and the KKT optimality system for convex quadratic problems with linear inequality constraints, determine the exact conditions under which the line constraint $y=x$ is active at the solution. Generalize the objective to $f_{a,b}(x,y)=(x-a)^{2}+(y-b)^{2}$ with unconstrained minimizer $(a,b) \\in \\mathbb{R}^{2}$ and derive a closed-form expression for the constrained minimizer $(x^{\\star},y^{\\star})$ as a function of $(a,b)$, expressed as a single analytic piecewise formula that encompasses all cases of active and inactive constraints. Your final answer should be the explicit piecewise expression for $(x^{\\star},y^{\\star})$ in terms of $(a,b)$. No rounding is required and no units are involved. Additionally, explain qualitatively how $(x^{\\star},y^{\\star})$ changes as $(a,b)$ crosses the boundary of the feasible region and identify precisely when $y=x$ is active.", "solution": "### Solution Derivation\n\nThe problem is to find the point $(x^\\star, y^\\star)$ in the feasible set $S = \\{ (x,y) \\in \\mathbb{R}^2 \\mid y \\ge x, x \\ge 0 \\}$ that minimizes the squared Euclidean distance to a given point $(a,b)$. This is equivalent to finding the Euclidean projection of $(a,b)$ onto $S$.\n\nWe formulate this as a constrained optimization problem:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(x,y) = (x-a)^2 + (y-b)^2 \\\\\n\\text{subject to} \\quad  g_1(x,y) = x - y \\le 0 \\\\\n g_2(x,y) = -x \\le 0\n\\end{aligned}\n$$\n\nThe Karush-Kuhn-Tucker (KKT) conditions for a solution $(x^\\star, y^\\star)$ with Lagrange multipliers $\\mu_1, \\mu_2$ are:\n\n1.  **Stationarity**: $\\nabla f(x^\\star, y^\\star) + \\mu_1 \\nabla g_1(x^\\star, y^\\star) + \\mu_2 \\nabla g_2(x^\\star, y^\\star) = 0$\n    $$2(x^\\star - a) + \\mu_1(1) + \\mu_2(-1) = 0$$\n    $$2(y^\\star - b) + \\mu_1(-1) + \\mu_2(0) = 0$$\n2.  **Primal Feasibility**:\n    $$x^\\star - y^\\star \\le 0$$\n    $$-x^\\star \\le 0$$\n3.  **Dual Feasibility**:\n    $$\\mu_1 \\ge 0$$\n    $$\\mu_2 \\ge 0$$\n4.  **Complementary Slackness**:\n    $$\\mu_1 (x^\\star - y^\\star) = 0$$\n    $$\\mu_2 (-x^\\star) = 0$$\n\nFrom the stationarity equations, we can express the multipliers in terms of the solution variables:\nFrom the second equation: $\\mu_1 = 2(y^\\star - b)$.\nFrom the first equation: $\\mu_2 = 2(x^\\star - a) + \\mu_1 = 2(x^\\star - a) + 2(y^\\star - b) = 2(x^\\star + y^\\star - a - b)$.\n\nThe KKT system can be solved by considering four cases based on the complementary slackness conditions.\n\n**Case 1: No constraints are active ($\\mu_1 = 0, \\mu_2 = 0$).**\nThe stationarity equations give $x^\\star = a$ and $y^\\star = b$.\nFor this to be a valid solution, it must be primal feasible: $a-b \\le 0 \\implies b \\ge a$ and $-a \\le 0 \\implies a \\ge 0$.\nSo, if $(a,b)$ is in the feasible region $S$, it is its own projection.\nRegion: $\\{ (a,b) \\mid a \\ge 0 \\text{ and } b \\ge a \\}$. Solution: $(x^\\star, y^\\star) = (a,b)$.\n\n**Case 2: $g_1$ is active ($x^\\star=y^\\star$), $g_2$ is inactive ($x^\\star  0 \\implies \\mu_2=0$).**\nWith $x^\\star=y^\\star$ and $\\mu_2=0$, stationarity becomes:\n$2(x^\\star - a) + \\mu_1 = 0$\n$2(x^\\star - b) - \\mu_1 = 0$\nAdding these gives $4x^\\star - 2a - 2b = 0$, so $x^\\star = \\frac{a+b}{2}$. Thus, $(x^\\star, y^\\star) = (\\frac{a+b}{2}, \\frac{a+b}{2})$.\nConditions for validity:\n- Primal feasibility: $x^\\star  0 \\implies a+b  0$.\n- Dual feasibility: $\\mu_1 = 2(x^\\star - b) = a+b-2b = a-b \\ge 0 \\implies a \\ge b$.\nCombining these with non-strict inequalities for the boundary cases, we define the region.\nRegion: $\\{ (a,b) \\mid a \\ge b \\text{ and } a+b \\ge 0 \\}$. Solution: $(x^\\star, y^\\star) = (\\frac{a+b}{2}, \\frac{a+b}{2})$. This corresponds to projection onto the boundary line $y=x$.\n\n**Case 3: $g_2$ is active ($x^\\star=0$), $g_1$ is inactive ($y^\\star  x^\\star \\implies \\mu_1=0$).**\nWith $x^\\star=0$ and $\\mu_1=0$, stationarity becomes:\n$2(-a) - \\mu_2 = 0 \\implies \\mu_2 = -2a$.\n$2(y^\\star - b) = 0 \\implies y^\\star=b$.\nSolution is $(x^\\star, y^\\star) = (0,b)$.\nConditions for validity:\n- Primal feasibility: $y^\\star  x^\\star \\implies b  0$.\n- Dual feasibility: $\\mu_2 = -2a \\ge 0 \\implies a \\le 0$.\nRegion: $\\{ (a,b) \\mid a \\le 0 \\text{ and } b \\ge 0 \\}$. Solution: $(x^\\star, y^\\star) = (0,b)$. This corresponds to projection onto the boundary line $x=0$.\n\n**Case 4: Both $g_1$ and $g_2$ are active ($x^\\star=y^\\star$ and $x^\\star=0$).**\nThis immediately gives the solution $(x^\\star, y^\\star) = (0,0)$.\nConditions for validity come from dual feasibility ($\\mu_1 \\ge 0, \\mu_2 \\ge 0$):\n$\\mu_1 = 2(y^\\star - b) = -2b \\ge 0 \\implies b \\le 0$.\n$\\mu_2 = 2(x^\\star + y^\\star - a - b) = -2(a+b) \\ge 0 \\implies a+b \\le 0$.\nRegion: $\\{ (a,b) \\mid b \\le 0 \\text{ and } a+b \\le 0 \\}$. Solution: $(x^\\star, y^\\star) = (0,0)$. This corresponds to projection onto the vertex $(0,0)$.\n\nThese four closed regions cover the entire $\\mathbb{R}^2$ plane for $(a,b)$. On their boundaries, the solution formulas are consistent, ensuring the projection is a continuous function of $(a,b)$.\n\n### Analysis of Active Constraints and Qualitative Behavior\n\n**When is the constraint $y=x$ active?**\nThe constraint $y=x$ is active at the solution $(x^\\star, y^\\star)$ when $x^\\star = y^\\star$. Analyzing our four cases:\n1.  Solution $(a,b)$: Active if $a=b$. This occurs on the boundary of the feasible set, $\\{ (a,b) \\mid a \\ge 0, b=a \\}$.\n2.  Solution $(\\frac{a+b}{2}, \\frac{a+b}{2})$: Always active by definition. This holds for the region $\\{ (a,b) \\mid a \\ge b, a+b \\ge 0 \\}$.\n3.  Solution $(0,b)$: Active if $b=0$. This occurs for $\\{ (a,b) \\mid a \\le 0, b=0 \\}$.\n4.  Solution $(0,0)$: Always active by definition. This holds for the region $\\{ (a,b) \\mid b \\le 0, a+b \\le 0 \\}$.\n\nCombining these, the constraint $y=x$ is active if and only if $(a,b)$ belongs to the union of the regions from Case 2 and Case 4:\n$\\{ (a,b) \\mid (a \\ge b \\text{ and } a+b \\ge 0) \\lor (b \\le 0 \\text{ and } a+b \\le 0) \\}$.\n\nFor the specific initial problem with $f(x,y)=(x-1)^2+(y+3)^2$, we have $(a,b)=(1,-3)$. This point falls into Case 4 since $b=-3 \\le 0$ and $a+b = 1-3=-2 \\le 0$. The solution is $(x^\\star, y^\\star)=(0,0)$. At this solution, $x^\\star=y^\\star=0$, so the constraint $y=x$ is active.\n\n**Qualitative Behavior of the Solution**\nThe solution $(x^\\star, y^\\star)$ is the projection of the unconstrained minimizer $(a,b)$ onto the feasible set $S$.\n- When $(a,b)$ is inside the feasible set $S$ (Region 1), the solution is simply $(a,b)$.\n- As $(a,b)$ crosses a boundary of $S$, the solution $(x^\\star, y^\\star)$ \"sticks\" to that boundary.\n- If $(a,b)$ crosses the boundary $b=a$ into Region 2, the solution becomes its projection onto the line $y=x$, which is $(\\frac{a+b}{2}, \\frac{a+b}{2})$. The solution point slides along the line $y=x$.\n- If $(a,b)$ crosses the boundary $a=0$ into Region 3, the solution becomes its projection onto the $y$-axis, which is $(0,b)$. The solution point slides along the line $x=0$.\n- If $(a,b)$ is in Region 4, which is geometrically \"behind\" the vertex of the feasible cone, the projection is always the vertex itself. Thus, for any $(a,b)$ in this region, the solution is fixed at $(0,0)$.\nThe function mapping $(a,b)$ to $(x^\\star, y^\\star)$ is continuous across all of $\\mathbb{R}^2$, but its derivatives are discontinuous across the boundaries of the four regions.\n\nThe closed-form piecewise expression for the constrained minimizer $(x^\\star, y^\\star)$ is the final answer.", "answer": "$$\n\\boxed{\n(x^\\star, y^\\star) = \n\\begin{cases} \n(a, b)  \\text{if } a \\ge 0 \\text{ and } b \\ge a \\\\\n\\left( \\frac{a+b}{2}, \\frac{a+b}{2} \\right)  \\text{if } a \\ge b \\text{ and } a+b \\ge 0 \\\\\n(0, b)  \\text{if } a \\le 0 \\text{ and } b \\ge 0 \\\\\n(0, 0)  \\text{if } b \\le 0 \\text{ and } a+b \\le 0\n\\end{cases}\n}\n$$", "id": "3094227"}, {"introduction": "In real-world applications, problem data is often subject to change or uncertainty. This final exercise demonstrates the dynamic nature of active constraints by introducing a parameter that perturbs one of the constraint boundaries. Your task is to pinpoint the exact \"tipping point\" at which the identity of the active constraint at the optimizer changes, providing insight into the sensitivity of the solution to the problem's formulation [@problem_id:3094305].", "problem": "Consider the strictly convex quadratic program in two variables $x = (x_1,x_2)$:\n$$\\min_{x \\in \\mathbb{R}^2} f(x) \\quad \\text{subject to} \\quad g_1(x) \\le 0, \\; g_2(x) \\le 0,$$\nwhere\n$$f(x) = \\frac{1}{2}\\|x - x_0\\|_2^2, \\quad x_0 = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix},$$\nand the constraints are the two affine half-spaces\n$$g_1(x) = x_1 + x_2 - 2 \\le 0, \\qquad g_2(x) = x_1 + x_2 - \\left(\\frac{41}{20} + t\\right) \\le 0,$$\nwith the scalar parameter $t \\in \\mathbb{R}$. The geometric configuration is such that both constraints share the same normal direction but have slightly different offsets, so at $t$ near zero one constraint is active at the optimizer and the other is nearly active. Determine the exact value of the parameter $t^{\\star}$ at which an infinitesimal change flips which constraint binds at the unique optimizer; that is, find the value of $t$ for which the identity of the active constraint at the minimizer changes. Express your answer as an exact reduced fraction.", "solution": "The problem stated is a strictly convex quadratic program. We are asked to find the value of a parameter $t$, denoted $t^{\\star}$, at which the identity of the active constraint at the unique optimizer changes.\n\nThe objective function to be minimized is\n$$f(x) = \\frac{1}{2}\\|x - x_0\\|_2^2 = \\frac{1}{2}\\left((x_1 - 2)^2 + (x_2 - 1)^2\\right)$$\nwhere $x = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix}$ and $x_0 = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$. Minimizing $f(x)$ is equivalent to finding the point $x$ in the feasible region that is closest to the point $x_0$. This is a projection problem.\n\nThe feasible region, $S$, is defined by the intersection of two linear inequality constraints:\n$$g_1(x) = x_1 + x_2 - 2 \\le 0$$\n$$g_2(x) = x_1 + x_2 - \\left(\\frac{41}{20} + t\\right) \\le 0$$\nBoth constraints define half-spaces with the same normal vector $\\begin{pmatrix}1  1\\end{pmatrix}^T$. The feasible region is the intersection of these two half-spaces. Let $c_1 = 2$ and $c_2(t) = \\frac{41}{20} + t$. The constraints can be written as $x_1 + x_2 \\le c_1$ and $x_1 + x_2 \\le c_2(t)$. The feasible region $S$ is therefore given by the single, more restrictive constraint:\n$$S = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\le \\min(c_1, c_2(t)) \\}$$\n\nThe unconstrained minimizer of $f(x)$ is $x_0 = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$. We evaluate the constraints at this point to determine if it is feasible.\nFor the first constraint: $g_1(x_0) = 2 + 1 - 2 = 1$. Since $1  0$, the constraint $g_1(x) \\le 0$ is violated. This implies that the unconstrained minimizer $x_0$ is not in the feasible region $S$ for any value of $t$, because $S$ is a subset of the half-space defined by $g_1(x) \\le 0$.\nSince the objective function is strictly convex and the feasible set is closed and convex, a unique minimizer $x^{\\star}$ exists. Because $x_0$ is outside $S$, the minimizer $x^{\\star}$ must lie on the boundary of $S$.\n\nThe boundary of the feasible region $S$ is the line $x_1 + x_2 = \\min(c_1, c_2(t))$. An inequality constraint $g_i(x) \\le 0$ is said to be active (or binding) at the optimizer $x^{\\star}$ if $g_i(x^{\\star}) = 0$. The problem asks for the value of $t$ at which the identity of the active constraint flips. This occurs precisely when the relative ordering of $c_1$ and $c_2(t)$ changes, as this redefines which constraint forms the boundary of the feasible region.\n\nThe transition point $t^{\\star}$ is the value of $t$ for which the two constraint boundaries coincide:\n$$c_1 = c_2(t^{\\star})$$\n$$2 = \\frac{41}{20} + t^{\\star}$$\nSolving for $t^{\\star}$, we find:\n$$t^{\\star} = 2 - \\frac{41}{20} = \\frac{40}{20} - \\frac{41}{20} = -\\frac{1}{20}$$\n\nTo verify this, we analyze the active constraints for $t$ on either side of $t^{\\star} = -1/20$.\n\nCase 1: $t  t^{\\star}$, i.e., $t  -1/20$.\nIn this case, $\\frac{41}{20} + t  \\frac{41}{20} - \\frac{1}{20} = \\frac{40}{20} = 2$. So, $c_2(t)  c_1$.\nThe effective constraint is $x_1 + x_2 \\le \\min(2, c_2(t)) = 2$.\nThe optimizer $x^{\\star}$ is the projection of $x_0$ onto the line $x_1 + x_2 = 2$.\nAt the optimizer, we have $x_1^{\\star} + x_2^{\\star} - 2 = 0$, which means $g_1(x^{\\star}) = 0$. Thus, constraint $g_1$ is active.\nFor the second constraint, $g_2(x^{\\star}) = x_1^{\\star} + x_2^{\\star} - c_2(t) = 2 - (\\frac{41}{20} + t) = -\\frac{1}{20} - t$.\nSince $t  -1/20$, it follows that $-t  1/20$, and so $-\\frac{1}{20} - t  0$.\nTherefore, $g_2(x^{\\star})  0$, and constraint $g_2$ is inactive.\n\nCase 2: $t  t^{\\star}$, i.e., $t  -1/20$.\nIn this case, $\\frac{41}{20} + t  2$. So, $c_2(t)  c_1$.\nThe effective constraint is $x_1 + x_2 \\le \\min(2, c_2(t)) = \\frac{41}{20} + t$.\nThe optimizer $x^{\\star}$ is the projection of $x_0$ onto the line $x_1 + x_2 = \\frac{41}{20} + t$.\nAt the optimizer, we have $x_1^{\\star} + x_2^{\\star} - (\\frac{41}{20} + t) = 0$, which means $g_2(x^{\\star}) = 0$. Thus, constraint $g_2$ is active.\nFor the first constraint, $g_1(x^{\\star}) = x_1^{\\star} + x_2^{\\star} - 2 = (\\frac{41}{20} + t) - 2 = \\frac{1}{20} + t$.\nSince $t  -1/20$, it follows that $\\frac{1}{20} + t  0$.\nTherefore, $g_1(x^{\\star})  0$, and constraint $g_1$ is inactive.\n\nAs $t$ increases through the value $t^{\\star} = -1/20$, the identity of the single active constraint switches from $g_2$ to $g_1$. At the precise value $t=t^\\star$, both constraint functions evaluate to zero at the optimizer, meaning both are active because they are identical. The value $t^{\\star}$ marks the transition point where an infinitesimal change in $t$ alters which constraint is uniquely active.\n\nThe exact value of the parameter $t^{\\star}$ is $-\\frac{1}{20}$. This is an exact reduced fraction as requested.", "answer": "$$\\boxed{-\\frac{1}{20}}$$", "id": "3094305"}]}