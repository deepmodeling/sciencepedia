{"hands_on_practices": [{"introduction": "Mastering equality constrained optimization begins with the foundational technique of Lagrange multipliers. This practice provides a concrete application from continuum mechanics, where we seek the maximum and minimum stress on a material. By framing this physical problem as finding the extrema of a function on a unit circle, you will apply the core principles of Lagrange multipliers in a clear and tangible context [@problem_id:2183834].", "problem": "In the field of continuum mechanics, the stress state at a point within a material is characterized by a stress tensor. For a two-dimensional (plane stress) problem, the stress tensor components at a particular point on a thin plate are given as $\\sigma_{xx} = 80.0$ Megapascals (MPa), $\\sigma_{yy} = 20.0$ MPa, and $\\sigma_{xy} = -30.0$ MPa.\n\nThe normal stress, $\\sigma_n$, on a plane passing through this point depends on the orientation of the plane, which can be defined by a unit normal vector $\\mathbf{n}$ with components $(n_x, n_y)$. The value of the normal stress is given by the formula:\n$$ \\sigma_n = \\sigma_{xx} n_x^2 + \\sigma_{yy} n_y^2 + 2 \\sigma_{xy} n_x n_y $$\nSince $\\mathbf{n}$ is a unit vector, its components must satisfy the condition $n_x^2 + n_y^2 = 1$.\n\nThe extreme values of $\\sigma_n$ over all possible orientations are known as the principal stresses. Determine the maximum principal stress, $\\sigma_\\text{max}$, and the minimum principal stress, $\\sigma_\\text{min}$.\n\nExpress your answers in MPa, rounded to three significant figures. Present the two values, $\\sigma_\\text{max}$ and $\\sigma_\\text{min}$, in this order, as a row matrix.", "solution": "The extrema of the normal stress on planes through a point are the principal stresses, which are the eigenvalues of the in-plane stress tensor. For a unit normal $\\mathbf{n}$, the normal stress is $\\sigma_{n}=\\mathbf{n}^{T}\\boldsymbol{\\sigma}\\mathbf{n}$ with the constraint $\\mathbf{n}^{T}\\mathbf{n}=1$. Introducing a Lagrange multiplier $\\lambda$ for this constraint, define $L(\\mathbf{n},\\lambda)=\\mathbf{n}^{T}\\boldsymbol{\\sigma}\\mathbf{n}-\\lambda(\\mathbf{n}^{T}\\mathbf{n}-1)$. Stationarity with respect to $\\mathbf{n}$ gives\n$$\n\\frac{\\partial L}{\\partial \\mathbf{n}}=2\\boldsymbol{\\sigma}\\mathbf{n}-2\\lambda \\mathbf{n}=\\mathbf{0}\\quad\\Rightarrow\\quad (\\boldsymbol{\\sigma}-\\lambda \\mathbf{I})\\mathbf{n}=\\mathbf{0},\n$$\nso nontrivial solutions require\n$$\n\\det(\\boldsymbol{\\sigma}-\\lambda \\mathbf{I})=0.\n$$\nFor the given plane stress tensor with components $\\sigma_{xx}$, $\\sigma_{yy}$, and $\\sigma_{xy}$,\n$$\n\\det\\begin{pmatrix}\\sigma_{xx}-\\lambda & \\sigma_{xy}\\\\ \\sigma_{xy} & \\sigma_{yy}-\\lambda\\end{pmatrix}=(\\sigma_{xx}-\\lambda)(\\sigma_{yy}-\\lambda)-\\sigma_{xy}^{2}=0,\n$$\nwhich expands to\n$$\n\\lambda^{2}-(\\sigma_{xx}+\\sigma_{yy})\\lambda+(\\sigma_{xx}\\sigma_{yy}-\\sigma_{xy}^{2})=0.\n$$\nThe roots (principal stresses) are\n$$\n\\lambda=\\frac{\\sigma_{xx}+\\sigma_{yy}}{2}\\pm \\sqrt{\\left(\\frac{\\sigma_{xx}-\\sigma_{yy}}{2}\\right)^{2}+\\sigma_{xy}^{2}}.\n$$\nSubstituting $\\sigma_{xx}=80.0$, $\\sigma_{yy}=20.0$, and $\\sigma_{xy}=-30.0$ (in MPa),\n$$\n\\frac{\\sigma_{xx}+\\sigma_{yy}}{2}=50.0,\\qquad \\left(\\frac{\\sigma_{xx}-\\sigma_{yy}}{2}\\right)^{2}=30^{2}=900,\\qquad \\sigma_{xy}^{2}=900,\n$$\nso\n$$\n\\sqrt{\\left(\\frac{\\sigma_{xx}-\\sigma_{yy}}{2}\\right)^{2}+\\sigma_{xy}^{2}}=\\sqrt{1800}=30\\sqrt{2}.\n$$\nTherefore,\n$$\n\\sigma_{\\max}=50+30\\sqrt{2},\\qquad \\sigma_{\\min}=50-30\\sqrt{2}.\n$$\nNumerically (in MPa) to three significant figures,\n$$\n\\sigma_{\\max}\\approx 92.4,\\qquad \\sigma_{\\min}\\approx 7.57.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 92.4 & 7.57 \\end{pmatrix}}$$", "id": "2183834"}, {"introduction": "First-order conditions identify stationary points, but how do we classify them as minima, maxima, or saddles? This exercise delves into the crucial role of second-order conditions in a fascinating and non-obvious scenario. You will explore how a point that is a saddle in the full ambient space can actually be a strict local minimum when confined to the constraint manifold, reinforcing the importance of analyzing the Hessian of the Lagrangian on the tangent space [@problem_id:3126154].", "problem": "Consider the equality-constrained optimization problem with objective function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ given by $f(x,y) = x^2 - y^2$ and a single equality constraint $g(x,y) = y = 0$. Let $(x^\\star,y^\\star) = (0,0)$ be the point of interest. Use the foundational definition of a constrained stationary point via the Lagrangian and the tangent space characterization of feasible directions. The Lagrangian is defined as $\\mathcal{L}(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)$, and the tangent space at a feasible point is the null space of the Jacobian of the constraint. The Karush-Kuhn-Tucker (KKT) conditions specify the first-order necessary conditions for constrained optimality, and second-order conditions assess the definiteness of the quadratic form of the Hessian of the Lagrangian on the tangent space.\n\nWhich of the following statements about $(0,0)$ are correct?\n\nA. $(0,0)$ is a constrained stationary point, with a Lagrange multiplier $\\lambda = 0$ satisfying the first-order necessary conditions.\n\nB. In the full ambient space $\\mathbb{R}^2$, $(0,0)$ is a strict saddle point of $f$, since there exist directions along which $f$ increases and directions along which $f$ decreases.\n\nC. Restricted to the constraint manifold $\\{(x,y)\\in\\mathbb{R}^2 : y = 0\\}$, $(0,0)$ is a strict local minimum of $f$.\n\nD. The reduced Hessian of the Lagrangian at $(0,0)$, formed by restricting the Hessian to the tangent space directions of the constraint, is positive definite.\n\nE. The second-order necessary conditions for constrained local optimality fail at $(0,0)$, so $(0,0)$ cannot be a constrained local minimum.\n\nSelect all options that are correct.", "solution": "The problem statement is a valid exercise in equality-constrained optimization. We are given the objective function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ as $f(x,y) = x^2 - y^2$ and the equality constraint $g(x,y) = y=0$. We are asked to analyze the nature of the point $(x^\\star,y^\\star) = (0,0)$.\n\nFirst, we verify that the point $(x^\\star,y^\\star) = (0,0)$ is a feasible point. Substituting into the constraint equation, we have $g(0,0) = 0$, which satisfies the condition.\n\nThe Lagrangian function for this problem is defined as $\\mathcal{L}(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)$.\nSubstituting the given functions, we get:\n$$ \\mathcal{L}(x,y,\\lambda) = (x^2 - y^2) + \\lambda y $$\n\nWe will now evaluate each statement.\n\n**A. $(0,0)$ is a constrained stationary point, with a Lagrange multiplier $\\lambda = 0$ satisfying the first-order necessary conditions.**\n\nA point $(x^\\star, y^\\star)$ is a constrained stationary point if there exists a Lagrange multiplier $\\lambda^\\star$ such that the gradient of the Lagrangian with respect to the variables $(x,y)$ is zero at that point. The first-order necessary conditions (KKT conditions for equality constraints) are:\n$$ \\nabla_{x,y} \\mathcal{L}(x^\\star, y^\\star, \\lambda^\\star) = 0 $$\n$$ g(x^\\star, y^\\star) = 0 $$\nWe have already confirmed the second condition (feasibility). Now we compute the gradient of the Lagrangian:\n$$ \\nabla_{x,y} \\mathcal{L}(x,y,\\lambda) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ -2y + \\lambda \\end{pmatrix} $$\nWe evaluate this gradient at the point $(x^\\star, y^\\star) = (0,0)$:\n$$ \\nabla_{x,y} \\mathcal{L}(0,0,\\lambda) = \\begin{pmatrix} 2(0) \\\\ -2(0) + \\lambda \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\lambda \\end{pmatrix} $$\nFor this gradient to be the zero vector, we must have $\\lambda = 0$.\nSince we found a value $\\lambda^\\star = 0$ that satisfies the first-order necessary condition $\\nabla_{x,y} \\mathcal{L}(0,0,0) = (0,0)^T$, the point $(0,0)$ is indeed a constrained stationary point.\nThus, statement A is **Correct**.\n\n**B. In the full ambient space $\\mathbb{R}^2$, $(0,0)$ is a strict saddle point of $f$, since there exist directions along which $f$ increases and directions along which $f$ decreases.**\n\nWe analyze the unconstrained function $f(x,y) = x^2 - y^2$ in the neighborhood of $(0,0)$. First, we find the critical points by setting the gradient of $f$ to zero:\n$$ \\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix} $$\nSetting $\\nabla f(x,y) = (0,0)^T$ gives $x=0$ and $y=0$. So, $(0,0)$ is a critical point of the unconstrained function $f$.\nNext, we classify this critical point using the Hessian matrix of $f$:\n$$ H_f(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} $$\nThe Hessian is constant. Its eigenvalues are $\\lambda_1 = 2$ and $\\lambda_2 = -2$. Since the eigenvalues have opposite signs, the Hessian is indefinite, which is the condition for a saddle point. The point is a strict (or non-degenerate) saddle point because none of the eigenvalues are zero.\nAs the statement correctly notes, in the direction of the first eigenvector, $d_1 = (1,0)^T$, the function behaves like $f(t,0) = t^2$, which increases. In the direction of the second eigenvector, $d_2 = (0,1)^T$, the function behaves like $f(0,t) = -t^2$, which decreases.\nThus, statement B is **Correct**.\n\n**C. Restricted to the constraint manifold $\\{(x,y)\\in\\mathbb{R}^2 : y = 0\\}$, $(0,0)$ is a strict local minimum of $f$.**\n\nThe constraint manifold is the set of all points where $y=0$. This is the $x$-axis in the $\\mathbb{R}^2$ plane.\nTo analyze the function $f$ restricted to this manifold, we substitute $y=0$ into the expression for $f(x,y)$:\n$$ f_{constrained}(x) = f(x,0) = x^2 - (0)^2 = x^2 $$\nWe are interested in the behavior of this one-dimensional function at the point $(0,0)$, which corresponds to $x=0$. The function $h(x) = x^2$ has a derivative $h'(x) = 2x$, which is zero at $x=0$. The second derivative is $h''(x) = 2$, which is positive. This indicates a strict local minimum at $x=0$. More directly, for any $x \\neq 0$ in any neighborhood of $0$, $h(x) = x^2 > 0 = h(0)$.\nTherefore, the point $(0,0)$ is a strict local minimum of $f$ when restricted to the constraint manifold.\nThus, statement C is **Correct**.\n\n**D. The reduced Hessian of the Lagrangian at $(0,0)$, formed by restricting the Hessian to the tangent space directions of the constraint, is positive definite.**\n\nThis statement concerns the second-order sufficient conditions for optimality. The Hessian of the Lagrangian with respect to $(x,y)$ is:\n$$ H_{\\mathcal{L}}(x,y,\\lambda) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} $$\nAt the stationary point $(x^\\star,y^\\star,\\lambda^\\star) = (0,0,0)$, the Hessian is $H_{\\mathcal{L}}(0,0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$.\nThe tangent space at a feasible point is the null space of the Jacobian of the active constraints. The constraint is $g(x,y)=y=0$. The Jacobian of $g$ is:\n$$ J_g(x,y) = \\nabla g(x,y)^T = \\begin{pmatrix} 0 & 1 \\end{pmatrix} $$\nThe tangent space $T$ at $(0,0)$ is the set of all vectors $z = (z_1, z_2)^T \\in \\mathbb{R}^2$ such that $J_g(0,0) z = 0$:\n$$ \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = 0 \\implies z_2 = 0 $$\nSo, the tangent space is the set of vectors of the form $(z_1, 0)^T$, which is the $x$-axis. A basis for this space is the vector $Z = (1,0)^T$.\nTo determine the definiteness of the Hessian on this tangent space, we evaluate the quadratic form $z^T H_{\\mathcal{L}} z$ for any non-zero vector $z \\in T$. Let $z = (z_1, 0)^T$ with $z_1 \\neq 0$.\n$$ z^T H_{\\mathcal{L}} z = \\begin{pmatrix} z_1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2z_1 & 0 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ 0 \\end{pmatrix} = 2z_1^2 $$\nFor any $z_1 \\neq 0$, $2z_1^2 > 0$. This means the Hessian of the Lagrangian is positive definite when restricted to the tangent space. The reduced Hessian is the matrix $Z^T H_{\\mathcal{L}} Z = (1,0) \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = [2]$, which is a positive definite $1 \\times 1$ matrix.\nThus, statement D is **Correct**.\n\n**E. The second-order necessary conditions for constrained local optimality fail at $(0,0)$, so $(0,0)$ cannot be a constrained local minimum.**\n\nThe second-order necessary condition (SONC) for a point $(x^\\star, y^\\star)$ to be a constrained local minimum is that the Hessian of the Lagrangian, $H_{\\mathcal{L}}(x^\\star, y^\\star, \\lambda^\\star)$, must be positive semi-definite on the tangent space $T(x^\\star, y^\\star)$. This means $z^T H_{\\mathcal{L}} z \\ge 0$ for all $z \\in T(x^\\star, y^\\star)$.\nFrom our analysis for option D, we found that for any non-zero $z \\in T(0,0)$, $z^T H_{\\mathcal{L}}(0,0,0) z = 2z_1^2 > 0$. This is a strictly positive value, which certainly satisfies the condition of being non-negative ($\\ge 0$). In fact, the stronger second-order sufficient conditions for a strict local minimum are satisfied.\nSince the SONC are satisfied, the statement that they fail is false.\nThus, statement E is **Incorrect**.\n\nIn summary, statements A, B, C, and D are correct, while E is incorrect.", "answer": "$$\\boxed{ABCD}$$", "id": "3126154"}, {"introduction": "While analytical methods are powerful, most real-world optimization problems require numerical algorithms. This practice challenges you to bridge theory and implementation by coding a method that disentangles the search for a solution into two fundamental components: a \"normal\" step to satisfy the constraints and a \"tangential\" step to optimize the objective. This exercise provides direct experience with the logic that underpins many modern optimization solvers [@problem_id:3126067].", "problem": "Consider the equality constrained optimization problem of minimizing an objective function $f(x)$ subject to equality constraints $c(x)=0$. Let $x\\in\\mathbb{R}^n$ and $c(x)\\in\\mathbb{R}^m$ with $m\\leq n$. Denote the Jacobian of the constraints by $A(x)=\\nabla c(x)\\in\\mathbb{R}^{m\\times n}$ and the gradient of the objective by $g(x)=\\nabla f(x)\\in\\mathbb{R}^n$. A standard approach to handle infeasibility in iterative methods is to compute a feasibility restoration step $p$ by solving the subproblem $\\min_{p}\\|p\\|$ subject to the linearized constraint $A(x)p=-c(x)$, and then to couple it with a tangential step $t$ that stays in the null space of $A(x)$ to decrease the objective while preserving feasibility to first order. Your task is to implement this approach from first principles and quantitatively analyze its effect on convergence.\n\nStarting from fundamental definitions, implement an iterative method that alternates between:\n- A feasibility restoration step $p_k$ computed at the current iterate $x_k$ by solving $\\min_{p}\\|p\\|$ subject to $A(x_k)p=-c(x_k)$, and updating $x_{k+\\frac{1}{2}}=x_k+p_k$.\n- A tangential step $t_k$ computed at $x_{k+\\frac{1}{2}}$ that satisfies $A(x_{k+\\frac{1}{2}})t_k=0$ and locally decreases $f$, and updating $x_{k+1}=x_{k+\\frac{1}{2}}+t_k$.\n\nUse backtracking with a sufficient decrease condition to choose the tangential step size. Angles for any trigonometric functions must be interpreted in radians. Stop the normal-only method when the constraint violation norm $\\|c(x)\\|$ is less than a feasibility tolerance. For the coupled method, run for a fixed number of iterations and record the iteration at which feasibility is first achieved and the final objective value. All floating-point outputs must be rounded to $6$ decimal places.\n\nFundamental base to be used in your derivation and implementation:\n- The Karush–Kuhn–Tucker (KKT) conditions for equality constrained optimization state that at a local minimizer $(x^\\star,\\lambda^\\star)$, stationarity requires $g(x^\\star)+A(x^\\star)^\\top\\lambda^\\star=0$ and feasibility requires $c(x^\\star)=0$.\n- Linearization of constraints $c(x+p)\\approx c(x)+A(x)p$ near $x$.\n- Orthogonal projections in Euclidean spaces, and the concept of the null space of a matrix.\n\nImplement two regimes for each test case:\n- Normal-only: apply only the feasibility restoration steps $p_k$ until $\\|c(x)\\|\\leq\\varepsilon_{\\text{feas}}$ or a maximum of $N_{\\max}$ iterations is reached. Report the number of iterations needed to achieve feasibility and the objective value at termination.\n- Coupled: in each iteration, compute a feasibility restoration step followed by a tangential step with backtracking to decrease $f$ while maintaining tangency. Run for $N_{\\text{run}}$ iterations, record the first iteration at which $\\|c(x)\\|\\leq\\varepsilon_{\\text{feas}}$, and report the final objective value.\n\nUse the following concrete objective and test suite. The objective is the same in all test cases:\n- Objective function: $f(x)=\\left(x_1-1\\right)^2+\\left(x_2+2\\right)^2+\\frac{1}{2}x_3^2$.\n- Objective gradient: $g(x)=\\left[2\\left(x_1-1\\right),\\,2\\left(x_2+2\\right),\\,x_3\\right]^\\top$.\n\nTest suite with three cases:\n- Case $1$ (happy path, linear constraints with full row rank):\n  - Constraints: $c(x)=\\begin{bmatrix}x_1+x_2+x_3-1\\\\ x_1-x_2\\end{bmatrix}$.\n  - Jacobian: $A(x)=\\begin{bmatrix}1&1&1\\\\ 1&-1&0\\end{bmatrix}$.\n  - Initial point: $x_0=\\begin{bmatrix}3\\\\ -4\\\\ 2\\end{bmatrix}$.\n- Case $2$ (nonlinear constraints with curvature, angles in radians):\n  - Constraints: $c(x)=\\begin{bmatrix}x_1^2+x_2-1\\\\ x_1+\\sin(x_3)-\\frac{1}{2}\\end{bmatrix}$.\n  - Jacobian: $A(x)=\\begin{bmatrix}2x_1&1&0\\\\ 1&0&\\cos(x_3)\\end{bmatrix}$.\n  - Initial point: $x_0=\\begin{bmatrix}2\\\\ -1\\\\ 1\\end{bmatrix}$.\n- Case $3$ (redundant linear constraints, rank deficiency):\n  - Constraints: $c(x)=\\begin{bmatrix}x_1+x_2+x_3-1\\\\ 2x_1+2x_2+2x_3-2\\end{bmatrix}$.\n  - Jacobian: $A(x)=\\begin{bmatrix}1&1&1\\\\ 2&2&2\\end{bmatrix}$.\n  - Initial point: $x_0=\\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix}$.\n\nAlgorithmic parameters:\n- Feasibility tolerance: $\\varepsilon_{\\text{feas}}=10^{-8}$.\n- Maximum iterations for the normal-only regime: $N_{\\max}=50$.\n- Number of coupled iterations: $N_{\\text{run}}=30$.\n- Initial tangential step size: $\\alpha_0=1$.\n- Backtracking reduction factor: $\\tau=0.5$.\n- Sufficient decrease parameter: $\\sigma=10^{-4}$.\n\nYour program must, for each test case, compute and return the list containing:\n- The integer number of iterations required by the normal-only regime to achieve feasibility (or $N_{\\max}$ if not achieved).\n- The integer number of iterations in the coupled regime at which feasibility is first achieved (or $N_{\\text{run}}$ if not achieved).\n- The floating-point objective value at termination of the normal-only regime, rounded to $6$ decimal places.\n- The floating-point objective value at termination of the coupled regime after $N_{\\text{run}}$ iterations, rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three per-test-case lists enclosed in square brackets, for example, $\\left[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]\\right]$, where each $a_i$ and $b_i$ are integers and each $c_i$ and $d_i$ are floats rounded to $6$ decimal places.", "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed problem in the field of numerical optimization.\n\n### **1. Problem Validation**\n\n#### **Step 1: Extract Givens**\n\n- **Objective Function**: $f(x) = \\left(x_1-1\\right)^2+\\left(x_2+2\\right)^2+\\frac{1}{2}x_3^2$, with $x \\in \\mathbb{R}^3$.\n- **Objective Gradient**: $g(x) = \\nabla f(x) = \\left[2\\left(x_1-1\\right),\\,2\\left(x_2+2\\right),\\,x_3\\right]^\\top$.\n- **Equality Constraints**: $c(x) = 0$, with $c(x) \\in \\mathbb{R}^m$.\n- **Constraint Jacobian**: $A(x) = \\nabla c(x) \\in \\mathbb{R}^{m \\times n}$.\n- **Iterative Method**: A two-step process at each iteration $k$:\n  1.  **Feasibility Restoration (Normal) Step**: $p_k$ solves $\\min_{p}\\|p\\|$ subject to $A(x_k)p=-c(x_k)$. The update is $x_{k+\\frac{1}{2}}=x_k+p_k$.\n  2.  **Tangential Step**: $t_k$ satisfies $A(x_{k+\\frac{1}{2}})t_k=0$ and decreases $f$. The update is $x_{k+1}=x_{k+\\frac{1}{2}}+t_k$.\n- **Test Cases**:\n  - **Case 1**: $c(x)=\\begin{bmatrix}x_1+x_2+x_3-1\\\\ x_1-x_2\\end{bmatrix}$, $A(x)=\\begin{bmatrix}1&1&1\\\\ 1&-1&0\\end{bmatrix}$, $x_0=\\begin{bmatrix}3, -4, 2\\end{bmatrix}^\\top$.\n  - **Case 2**: $c(x)=\\begin{bmatrix}x_1^2+x_2-1\\\\ x_1+\\sin(x_3)-\\frac{1}{2}\\end{bmatrix}$, $A(x)=\\begin{bmatrix}2x_1&1&0\\\\ 1&0&\\cos(x_3)\\end{bmatrix}$, $x_0=\\begin{bmatrix}2, -1, 1\\end{bmatrix}^\\top$.\n  - **Case 3**: $c(x)=\\begin{bmatrix}x_1+x_2+x_3-1\\\\ 2x_1+2x_2+2x_3-2\\end{bmatrix}$, $A(x)=\\begin{bmatrix}1&1&1\\\\ 2&2&2\\end{bmatrix}$, $x_0=\\begin{bmatrix}0, 0, 0\\end{bmatrix}^\\top$.\n- **Algorithmic Parameters**:\n  - Feasibility tolerance: $\\varepsilon_{\\text{feas}}=10^{-8}$.\n  - Max iterations (normal-only): $N_{\\max}=50$.\n  - Number of iterations (coupled): $N_{\\text{run}}=30$.\n  - Initial step size (tangential): $\\alpha_0=1$.\n  - Backtracking factor: $\\tau=0.5$.\n  - Sufficient decrease parameter: $\\sigma=10^{-4}$.\n\n#### **Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded in the principles of numerical optimization, specifically methods for equality-constrained nonlinear programming. The proposed two-phase iterative scheme (restoration followed by optimization) is a standard technique related to sequential quadratic programming (SQP) and feasible direction methods. The problem is well-posed, providing all necessary functions, initial conditions, and parameters for a numerical implementation. The language is objective and mathematically precise. There are no contradictions, ambiguities, or invalid scientific premises.\n\n#### **Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### **2. Principle-Based Derivation**\n\nThe iterative method consists of two distinct steps: a normal step to restore feasibility and a tangential step to optimize the objective function while maintaining feasibility to first order. We will derive each step from fundamental principles.\n\n#### **Feasibility Restoration Step (Normal Step)**\n\nAt an iterate $x_k$ where $c(x_k) \\neq 0$, we seek a correction step $p_k$ that moves towards satisfying the constraints. The constraints are linearized around $x_k$: $c(x_k+p) \\approx c(x_k) + A(x_k)p$. To achieve feasibility ($c(x_k+p) = 0$), we must satisfy the linear system $A(x_k)p = -c(x_k)$.\n\nThis system is generally underdetermined ($m \\le n$), admitting infinite solutions for $p$. A standard choice is the solution with the minimum Euclidean norm, which avoids excessively large steps. This leads to the optimization subproblem:\n$$\n\\min_{p} \\frac{1}{2} \\|p\\|^2_2 \\quad \\text{subject to} \\quad A(x_k)p = -c(x_k)\n$$\nThe factor of $\\frac{1}{2}$ and the square on the norm are for convenience and do not change the minimizer. We form the Lagrangian:\n$$\n\\mathcal{L}(p, \\nu) = \\frac{1}{2}p^\\top p + \\nu^\\top(A(x_k)p + c(x_k))\n$$\nwhere $\\nu \\in \\mathbb{R}^m$ is the vector of Lagrange multipliers. The first-order KKT conditions for optimality are:\n$$\n\\nabla_p \\mathcal{L} = p + A(x_k)^\\top \\nu = 0 \\implies p = -A(x_k)^\\top \\nu\n$$\n$$\n\\nabla_\\nu \\mathcal{L} = A(x_k)p + c(x_k) = 0\n$$\nSubstituting the expression for $p$ into the second equation yields:\n$$\nA(x_k)(-A(x_k)^\\top \\nu) + c(x_k) = 0 \\implies (A(x_k)A(x_k)^\\top)\\nu = c(x_k)\n$$\nIf the Jacobian $A(x_k)$ has full row rank, the matrix $A(x_k)A(x_k)^\\top$ is invertible, and we can solve for $\\nu$. However, to handle rank-deficient cases (like Case 3), we use the Moore-Penrose pseudo-inverse, denoted by $A(x_k)^+$. The minimum-norm solution to $A(x_k)p = -c(x_k)$ is universally given by:\n$$\np_k = -A(x_k)^+ c(x_k)\n$$\nThe feasibility restoration update is then $x_{k+\\frac{1}{2}} = x_k + p_k$.\n\n#### **Tangential Step**\n\nAfter the normal step, the new point $x_{k+\\frac{1}{2}}$ is closer to the feasible set. The tangential step $t_k$ aims to decrease the objective function $f$ without immediately moving away from the feasible set. This is achieved by moving in a direction that is tangent to the constraints at $x_{k+\\frac{1}{2}}$. A direction $t$ is tangent if it lies in the null space of the constraint Jacobian, i.e., $A(x_{k+\\frac{1}{2}})t = 0$.\n\nTo find a suitable tangential direction, we can project the negative gradient of the objective function, $-g(x_{k+\\frac{1}{2}})$, onto the null space of $A(x_{k+\\frac{1}{2}})$. This projected gradient is the direction of steepest descent within the tangent subspace.\n\nThe projection matrix onto the null space of a matrix $A$ is given by $P_N = I - A^+A$. The tangential search direction $t_{\\text{dir}}$ is therefore:\n$$\nt_{\\text{dir}} = -P_N g(x_{k+\\frac{1}{2}}) = -(I - A(x_{k+\\frac{1}{2}})^+A(x_{k+\\frac{1}{2}}))g(x_{k+\\frac{1}{2}})\n$$\nThis direction is a descent direction for $f$, since $g^\\top t_{\\text{dir}} = -g^\\top P_N^\\top P_N g = -\\|P_N g\\|^2 = -\\|t_{\\text{dir}}\\|^2 \\le 0$. The descent is strict unless the projected gradient is zero, which is a KKT optimality condition.\n\nThe full tangential step is $t_k = \\alpha_k t_{\\text{dir}}$, where the step size $\\alpha_k > 0$ is determined by a backtracking line search. Starting with an initial guess $\\alpha = \\alpha_0$, we iteratively reduce $\\alpha$ by a factor $\\tau$ until the Armijo-Goldstein sufficient decrease condition is met:\n$$\nf(x_{k+\\frac{1}{2}} + \\alpha t_{\\text{dir}}) \\le f(x_{k+\\frac{1}{2}}) + \\sigma \\alpha g(x_{k+\\frac{1}{2}})^\\top t_{\\text{dir}}\n$$\nwhere $\\sigma \\in (0,1)$ is a small constant. The final update for the iteration is $x_{k+1} = x_{k+\\frac{1}{2}} + t_k$.\n\n### **3. Algorithmic Regimes**\n\n1.  **Normal-only Regime**: Successive updates are computed using only the feasibility restoration step, $x_{k+1} = x_k + p_k$. This process is repeated until the constraint violation $\\|c(x_k)\\|$ is below the tolerance $\\varepsilon_{\\text{feas}}$ or a maximum of $N_{\\max}$ iterations is reached.\n\n2.  **Coupled Regime**: Each iteration consists of a normal step followed by a tangential step with backtracking. This is performed for a fixed number of iterations, $N_{\\text{run}}$. We record the iteration at which feasibility is first achieved and the final objective value.", "answer": "```python\nimport numpy as np\n\n# Global problem parameters from the problem statement.\nE_FEAS = 1e-8\nN_MAX = 50\nN_RUN = 30\nALPHA0 = 1.0\nTAU = 0.5\nSIGMA = 1e-4\n\n# Objective function and its gradient.\ndef f(x):\n    \"\"\"Objective function f(x).\"\"\"\n    return (x[0] - 1)**2 + (x[1] + 2)**2 + 0.5 * x[2]**2\n\ndef g(x):\n    \"\"\"Gradient of the objective function, g(x).\"\"\"\n    return np.array([2 * (x[0] - 1), 2 * (x[1] + 2), x[2]])\n\n# Case 1: Linear constraints.\ndef c1(x):\n    \"\"\"Constraint function for Case 1.\"\"\"\n    return np.array([x[0] + x[1] + x[2] - 1, x[0] - x[1]])\n\ndef A1(x):\n    \"\"\"Jacobian of the constraints for Case 1.\"\"\"\n    return np.array([[1, 1, 1], [1, -1, 0]])\n\n# Case 2: Nonlinear constraints.\ndef c2(x):\n    \"\"\"Constraint function for Case 2.\"\"\"\n    return np.array([x[0]**2 + x[1] - 1, x[0] + np.sin(x[2]) - 0.5])\n\ndef A2(x):\n    \"\"\"Jacobian of the constraints for Case 2.\"\"\"\n    return np.array([[2 * x[0], 1, 0], [1, 0, np.cos(x[2])]])\n\n# Case 3: Redundant linear constraints.\ndef c3(x):\n    \"\"\"Constraint function for Case 3.\"\"\"\n    return np.array([x[0] + x[1] + x[2] - 1, 2 * x[0] + 2 * x[1] + 2 * x[2] - 2])\n\ndef A3(x):\n    \"\"\"Jacobian of the constraints for Case 3.\"\"\"\n    return np.array([[1, 1, 1], [2, 2, 2]])\n\ndef run_optimization(c_func, A_func, x0):\n    \"\"\"\n    Executes both the normal-only and coupled optimization regimes for a given test case.\n    \"\"\"\n    # --- Normal-only regime ---\n    x_n = np.copy(x0)\n    iter_normal = N_MAX\n    # Loop to check feasibility at the start of iteration k (0-indexed).\n    # k represents the number of iterations performed.\n    for k in range(N_MAX + 1):\n        if np.linalg.norm(c_func(x_n)) <= E_FEAS:\n            iter_normal = k\n            break\n        if k == N_MAX:\n            break\n        # Feasibility restoration step\n        ck = c_func(x_n)\n        Ak = A_func(x_n)\n        # Use Moore-Penrose pseudo-inverse for robustness (handles rank-deficiency).\n        p = -np.linalg.pinv(Ak) @ ck\n        x_n += p\n    obj_normal = f(x_n)\n\n    # --- Coupled regime ---\n    x_c = np.copy(x0)\n    iter_feas_coupled = N_RUN\n    \n    # Check if the initial point is already feasible.\n    if np.linalg.norm(c_func(x_c)) <= E_FEAS:\n        iter_feas_coupled = 0\n\n    for k in range(N_RUN):\n        # Normal (feasibility restoration) step\n        ck = c_func(x_c)\n        Ak = A_func(x_c)\n        p = -np.linalg.pinv(Ak) @ ck\n        x_half = x_c + p\n\n        # Tangential step\n        g_half = g(x_half)\n        A_half = A_func(x_half)\n        # Projector onto the null space of A_half\n        P_N = np.eye(x_c.shape[0]) - np.linalg.pinv(A_half) @ A_half\n        t_dir = -P_N @ g_half\n\n        t = np.zeros_like(x_c)\n        if np.linalg.norm(t_dir) > 1e-12:\n            alpha = ALPHA0\n            f_half = f(x_half)\n            g_dot_t = g_half @ t_dir\n            \n            # Backtracking line search with Armijo condition\n            max_backtrack_iter = 30 # Prevents excessive backtracking\n            for _ in range(max_backtrack_iter):\n                if f(x_half + alpha * t_dir) <= f_half + SIGMA * alpha * g_dot_t:\n                    break\n                alpha *= TAU\n            else: # If loop finishes, backtracking failed.\n                 alpha = 0.0\n            \n            t = alpha * t_dir\n        \n        x_c = x_half + t\n\n        # Check for feasibility if not yet achieved\n        if iter_feas_coupled == N_RUN and np.linalg.norm(c_func(x_c)) <= E_FEAS:\n            iter_feas_coupled = k + 1\n    \n    obj_coupled = f(x_c)\n\n    return [iter_normal, iter_feas_coupled, round(obj_normal, 6), round(obj_coupled, 6)]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, then prints the formatted results.\n    \"\"\"\n    test_cases = [\n        {\"c_func\": c1, \"A_func\": A1, \"x0\": np.array([3.0, -4.0, 2.0])},\n        {\"c_func\": c2, \"A_func\": A2, \"x0\": np.array([2.0, -1.0, 1.0])},\n        {\"c_func\": c3, \"A_func\": A3, \"x0\": np.array([0.0, 0.0, 0.0])}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_optimization(case[\"c_func\"], case[\"A_func\"], case[\"x0\"])\n        results.append(result)\n    \n    # Format the output as a single string as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3126067"}]}