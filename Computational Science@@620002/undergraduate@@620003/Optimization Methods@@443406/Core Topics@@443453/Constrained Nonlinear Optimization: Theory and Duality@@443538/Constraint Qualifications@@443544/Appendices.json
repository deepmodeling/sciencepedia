{"hands_on_practices": [{"introduction": "The Linear Independence Constraint Qualification (LICQ) is a foundational condition that ensures the gradients of active constraints are well-behaved at a potential solution. This exercise provides direct practice in applying the definition of LICQ by checking for linear dependence among a given set of constraint gradients. Mastering this skill is the first step toward diagnosing the regularity of a constrained optimization problem and ensuring the validity of its optimality conditions. [@problem_id:3112190]", "problem": "Consider a constrained optimization problem in $\\mathbb{R}^{3}$ with three inequality constraints that are active at a candidate solution point $x^{\\star} \\in \\mathbb{R}^{3}$. The outward normal vectors (equivalently, the gradients of the active constraint functions at $x^{\\star}$) are given by\n$$\nn_{1} = (1,\\,1,\\,0),\\quad n_{2} = (2,\\,2,\\,0),\\quad n_{3} = (0,\\,0,\\,1).\n$$\nLet $A \\in \\mathbb{R}^{3 \\times 3}$ denote the active constraint Jacobian matrix whose rows are $n_{1}$, $n_{2}$, and $n_{3}$. Using the definition of the Linear Independence Constraint Qualification (LICQ), assess the linear independence of the active constraint gradients by determining the rank $r$ of $A$. Report only the value of $r$ as your final numerical answer.", "solution": "The problem asks to determine the rank of the active constraint Jacobian matrix, $A$, for a constrained optimization problem. The rank of this matrix is used to assess whether the Linear Independence Constraint Qualification (LICQ) is satisfied at the candidate solution point $x^{\\star}$.\n\nThe problem is defined in $\\mathbb{R}^{3}$. At the point $x^{\\star}$, there are three active inequality constraints. The gradients of these active constraint functions, which correspond to the outward normal vectors, are given as:\n$$n_{1} = (1,\\,1,\\,0)$$\n$$n_{2} = (2,\\,2,\\,0)$$\n$$n_{3} = (0,\\,0,\\,1)$$\n\nThe active constraint Jacobian matrix, $A$, is formed by taking these gradient vectors as its rows. Therefore, the matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is:\n$$\nA = \\begin{pmatrix}\n1  1  0 \\\\\n2  2  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\n\nThe Linear Independence Constraint Qualification (LICQ) holds at a point if and only if the set of gradients of the active constraints at that point is linearly independent. The number of active constraints is $3$. Thus, LICQ holds if the set $\\{n_{1}, n_{2}, n_{3}\\}$ is linearly independent. This is equivalent to the condition that the rank of the matrix $A$ be equal to the number of active constraints, which is $3$. The rank of a matrix, denoted as $r = \\text{rank}(A)$, is the dimension of the vector space spanned by its rows (or columns), which is equal to the maximum number of linearly independent rows (or columns).\n\nWe can determine the rank of matrix $A$ by several methods. A standard procedure is to reduce the matrix to its row echelon form using Gaussian elimination. The number of non-zero rows in the resulting echelon form matrix is the rank.\n\nLet the rows of $A$ be $R_{1}$, $R_{2}$, and $R_{3}$.\n$$\nA = \\begin{pmatrix}\nR_{1} \\\\\nR_{2} \\\\\nR_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  1  0 \\\\\n2  2  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nWe observe that the second row, $R_{2}$, is a scalar multiple of the first row, $R_{1}$, specifically $R_{2} = 2R_{1}$. This indicates that the rows are linearly dependent. Let's formalize this with a row operation. We replace the second row $R_{2}$ with $R_{2} - 2R_{1}$:\n$$\n\\begin{pmatrix}\n1  1  0 \\\\\n2 - 2(1)  2 - 2(1)  0 - 2(0) \\\\\n0  0  1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  1  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nTo obtain a standard row echelon form, we can swap the second and third rows:\n$$\n\\begin{pmatrix}\n1  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThis matrix is in row echelon form. It contains two non-zero rows. Therefore, the rank of the matrix $A$ is $2$.\n$$r = \\text{rank}(A) = 2$$\n\nAlternatively, we can assess linear independence directly. The set of vectors $\\{n_{1}, n_{2}, n_{3}\\}$ is linearly dependent if there exist scalars $c_{1}, c_{2}, c_{3}$, not all zero, such that $c_{1}n_{1} + c_{2}n_{2} + c_{3}n_{3} = \\mathbf{0}$.\nFrom inspection, $n_{2} = 2n_{1}$. This relationship can be written as $2n_{1} - n_{2} + 0n_{3} = \\mathbf{0}$. This is a non-trivial linear combination of the vectors that equals the zero vector (with coefficients $c_{1}=2$, $c_{2}=-1$, $c_{3}=0$), which proves that the set of vectors is linearly dependent.\nSince the set of three vectors is linearly dependent, its rank must be less than $3$. The subset of vectors $\\{n_{1}, n_{3}\\} = \\{(1,1,0), (0,0,1)\\}$ is clearly linearly independent, as one cannot be written as a scalar multiple of the other. Thus, the dimension of the space spanned by $\\{n_{1}, n_{2}, n_{3}\\}$ is $2$.\n\nSince the rank $r=2$ is less than the number of active constraints ($3$), the active constraint gradients are linearly dependent, and the LICQ condition is not satisfied at the point $x^{\\star}$. The problem asks for the rank $r$ of the matrix $A$.\n\nBased on the row reduction, the rank is $r=2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "3112190"}, {"introduction": "Understanding when a constraint qualification fails is as important as knowing its definition, as this failure has direct theoretical and practical consequences. This problem explores a classic case of LICQ failure caused by redundant equality constraints. By working through this example, you will uncover one of the most important outcomes of such a failure: the non-uniqueness of the Lagrange multipliers associated with the solution. [@problem_id:3112197]", "problem": "Consider the equality-constrained optimization problem with decision variable $x \\in \\mathbb{R}^2$, objective $f(x) = x_1^2 + x_2^2$, and two equality constraints $h_1(x) = x_1 + x_2 = 0$ and $h_2(x) = 2 x_1 + 2 x_2 = 0$. The feasible set is $\\mathcal{F} = \\{ x \\in \\mathbb{R}^2 : h_1(x) = 0, h_2(x) = 0 \\}$. Use only core definitions of feasibility, constraint gradients, Jacobian rank, and the Karush–Kuhn–Tucker (KKT) conditions to analyze the degeneracy caused by duplicated equality constraints and the failure of the Linear Independence Constraint Qualification (LICQ). Compute the following quantities:\n- The rank $r$ of the constraint Jacobian $J(x)$ at an arbitrary $x \\in \\mathcal{F}$, where $J(x)$ has rows $\\nabla h_1(x)^\\top$ and $\\nabla h_2(x)^\\top$.\n- The dimension $d_{\\mathrm{null}}$ of the null space of $J(x)$.\n- Let $x^\\star$ be the global minimizer of $f$ over $\\mathcal{F}$. At $x^\\star$, determine the dimension $d_{\\lambda}$ of the set of Lagrange multipliers $\\lambda \\in \\mathbb{R}^2$ satisfying the KKT stationarity condition.\n\nReport your final result as a single row matrix $\\left( r \\quad d_{\\mathrm{null}} \\quad d_{\\lambda} \\right)$ using exact values. No rounding is required, and no units apply. You may refer to the Linear Independence Constraint Qualification (LICQ) by its full name on first use.", "solution": "The problem requires the analysis of an equality-constrained optimization problem characterized by linearly dependent constraints. We must compute three quantities related to the constraint Jacobian and the set of Lagrange multipliers.\n\nThe problem is defined as:\nMinimize $f(x) = x_1^2 + x_2^2$ for $x = (x_1, x_2) \\in \\mathbb{R}^2$.\nSubject to the equality constraints:\n$h_1(x) = x_1 + x_2 = 0$\n$h_2(x) = 2x_1 + 2x_2 = 0$\n\nThe feasible set is $\\mathcal{F} = \\{ x \\in \\mathbb{R}^2 : h_1(x) = 0, h_2(x) = 0 \\}$. The second constraint, $h_2(x) = 2(x_1 + x_2) = 2h_1(x)$, is redundant. Thus, the feasible set is determined solely by the first constraint:\n$\\mathcal{F} = \\{ x \\in \\mathbb{R}^2 : x_1 + x_2 = 0 \\}$. This set represents a line in $\\mathbb{R}^2$ passing through the origin.\n\nFirst, we compute the rank $r$ of the constraint Jacobian $J(x)$.\nThe gradients of the constraint functions are:\n$$ \\nabla h_1(x) = \\begin{pmatrix} \\frac{\\partial h_1}{\\partial x_1} \\\\ \\frac{\\partial h_1}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ \\nabla h_2(x) = \\begin{pmatrix} \\frac{\\partial h_2}{\\partial x_1} \\\\ \\frac{\\partial h_2}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} $$\nThe constraint Jacobian $J(x)$ is defined with rows $\\nabla h_1(x)^\\top$ and $\\nabla h_2(x)^\\top$:\n$$ J(x) = \\begin{pmatrix} \\nabla h_1(x)^\\top \\\\ \\nabla h_2(x)^\\top \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 2  2 \\end{pmatrix} $$\nThe Jacobian matrix is constant for all $x \\in \\mathbb{R}^2$. To find its rank, we observe that the second row is $2$ times the first row. The rows are linearly dependent. Since the matrix is non-zero, its rank must be $1$.\n$$ r = \\mathrm{rank}(J(x)) = 1 $$\nThis holds for any point $x$, including any $x \\in \\mathcal{F}$. The problem states that the gradients of the active constraints must be linearly independent for the Linear Independence Constraint Qualification (LICQ) to hold. Here, we have two equality constraints, which are always active. Their gradients, $\\nabla h_1(x)$ and $\\nabla h_2(x)$, are linearly dependent for all $x$. Thus, LICQ fails everywhere.\n\nSecond, we compute the dimension $d_{\\mathrm{null}}$ of the null space of $J(x)$. The rank-nullity theorem states that for a matrix $A$ with $n$ columns, $\\mathrm{rank}(A) + \\mathrm{dim}(\\mathrm{null}(A)) = n$. For the Jacobian $J(x)$, which is a $2 \\times 2$ matrix, the number of columns is $n=2$.\nUsing the rank $r=1$ we just found:\n$$ r + d_{\\mathrm{null}} = n \\implies 1 + d_{\\mathrm{null}} = 2 $$\n$$ d_{\\mathrm{null}} = 1 $$\n\nThird, we compute the dimension $d_{\\lambda}$ of the set of Lagrange multipliers $\\lambda = (\\lambda_1, \\lambda_2)$ at the global minimizer $x^\\star$.\nFirst, we find the optimizer $x^\\star$. The objective function $f(x) = x_1^2 + x_2^2 = \\|x\\|_2^2$ is the squared Euclidean distance from the origin. The feasible set $\\mathcal{F}$ is the line $x_1 + x_2 = 0$, which passes through the origin. The point on this line closest to the origin is the origin itself. Therefore, the global minimizer is:\n$$ x^\\star = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThe Karush-Kuhn-Tucker (KKT) conditions for an equality-constrained problem require that at a local minimum $x^\\star$, there exist Lagrange multipliers $\\lambda$ such that the gradient of the Lagrangian is zero. The Lagrangian function is:\n$$ \\mathcal{L}(x, \\lambda) = f(x) + \\lambda_1 h_1(x) + \\lambda_2 h_2(x) $$\nThe stationarity condition is $\\nabla_x \\mathcal{L}(x^\\star, \\lambda) = 0$, which can be written as:\n$$ \\nabla f(x^\\star) + \\sum_{i=1}^{2} \\lambda_i \\nabla h_i(x^\\star) = 0 $$\nWe compute the gradient of the objective function:\n$$ \\nabla f(x) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix} $$\nAt the minimizer $x^\\star = (0, 0)$, the gradient is $\\nabla f(x^\\star) = (0, 0)^\\top$. The stationarity condition becomes:\n$$ \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis yields a system of two identical linear equations for $\\lambda_1$ and $\\lambda_2$:\n$$ \\lambda_1 + 2\\lambda_2 = 0 $$\nThe set of Lagrange multipliers satisfying this condition is given by $\\lambda_1 = -2\\lambda_2$. This set can be parameterized by a single variable, for instance $c \\in \\mathbb{R}$, such that $\\lambda_2 = c$ and $\\lambda_1 = -2c$. The set of multipliers is:\n$$ \\{ \\lambda \\in \\mathbb{R}^2 : \\lambda = c \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}, c \\in \\mathbb{R} \\} $$\nThis set is a one-dimensional subspace of $\\mathbb{R}^2$ spanned by the vector $(-2, 1)^\\top$. Therefore, the dimension of the set of Lagrange multipliers is $1$.\n$$ d_{\\lambda} = 1 $$\nThis non-uniqueness of the Lagrange multiplier is a direct consequence of the failure of LICQ. In general, the dimension of the KKT multiplier set is $m - \\mathrm{rank}(J(x^\\star))$, where $m$ is the number of constraints, provided the set is non-empty. Here, $m=2$ and $\\mathrm{rank}(J(x^\\star))=1$, so the dimension is $2 - 1 = 1$.\n\nIn summary, the computed quantities are:\n- $r = 1$\n- $d_{\\mathrm{null}} = 1$\n- $d_{\\lambda} = 1$\n\nThe final result is reported as a row matrix $(r \\quad d_{\\mathrm{null}} \\quad d_{\\lambda})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  1\n\\end{pmatrix}\n}\n$$", "id": "3112197"}, {"introduction": "When LICQ does not hold, we can sometimes turn to weaker, more general conditions, such as the Mangasarian-Fromovitz Constraint Qualification (MFCQ). This exercise delves into the geometric heart of MFCQ, which concerns the existence of a single direction that moves into the feasible region from a point on the boundary. You will analyze a scenario with conflicting gradients where no such direction exists, providing a sharp and intuitive understanding of MFCQ's requirements and its limits. [@problem_id:3146844]", "problem": "Consider the inequality constraints $g_1(x)=x_1+x_2^2\\le 0$ and $g_2(x)=-x_1+x_2^2\\le 0$ in $\\mathbb{R}^2$, and the point $x^*=(0,0)$. Use the fundamental definition of the Mangasarian-Fromovitz constraint qualification (MFCQ), which for a set of inequality constraints requires the existence of a direction $d\\in\\mathbb{R}^n$ such that the directional derivatives of all constraints that are active at $x^*$ are strictly negative. Rely on the core definitions of gradient, directional derivative, and active constraints, with the Euclidean norm $\\|\\cdot\\|_2$.\n\nDefine the strict-descent margin over unit directions\n$$\n\\alpha^*=\\sup_{\\|d\\|_2=1}\\;\\min_{i\\in\\{1,2\\}}\\left(-\\nabla g_i(x^*)\\cdot d\\right),\n$$\nwhich quantifies the best achievable simultaneous strict decrease rate of all active inequality constraints at $x^*$. Compute the exact value of $\\alpha^*$, and interpret conceptually whether the gradients' requirements along the $x_1$-axis conflict. Your final reported answer must be the exact value of $\\alpha^*$ with no rounding and no units.", "solution": "The problem asks for the computation of the strict-descent margin, defined as $\\alpha^*=\\sup_{\\|d\\|_2=1}\\;\\min_{i\\in\\{1,2\\}}\\left(-\\nabla g_i(x^*)\\cdot d\\right)$, for the inequality constraints $g_1(x)=x_1+x_2^2\\le 0$ and $g_2(x)=-x_1+x_2^2\\le 0$ at the point $x^*=(0,0) \\in \\mathbb{R}^2$. We must also provide a conceptual interpretation of the result.\n\nFirst, we identify the set of active constraints at $x^*=(0,0)$. A constraint $g_i(x) \\le 0$ is active at $x^*$ if $g_i(x^*) = 0$.\nFor the first constraint, $g_1(x) = x_1+x_2^2$:\n$$g_1(x^*) = g_1(0,0) = 0 + 0^2 = 0$$\nSince $g_1(x^*) = 0$, the constraint $g_1$ is active at $x^*$.\n\nFor the second constraint, $g_2(x) = -x_1+x_2^2$:\n$$g_2(x^*) = g_2(0,0) = -0 + 0^2 = 0$$\nSince $g_2(x^*) = 0$, the constraint $g_2$ is also active at $x^*$.\nThe set of indices of active constraints at $x^*$ is therefore $I(x^*) = \\{1, 2\\}$.\n\nNext, we compute the gradients of these active constraints at the point $x^*=(0,0)$.\nFor $g_1(x)$, the gradient is:\n$$\\nabla g_1(x) = \\begin{pmatrix} \\frac{\\partial g_1}{\\partial x_1} \\\\ \\frac{\\partial g_1}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2x_2 \\end{pmatrix}$$\nEvaluating at $x^*=(0,0)$:\n$$\\nabla g_1(x^*) = \\begin{pmatrix} 1 \\\\ 2(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nFor $g_2(x)$, the gradient is:\n$$\\nabla g_2(x) = \\begin{pmatrix} \\frac{\\partial g_2}{\\partial x_1} \\\\ \\frac{\\partial g_2}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2x_2 \\end{pmatrix}$$\nEvaluating at $x^*=(0,0)$:\n$$\\nabla g_2(x^*) = \\begin{pmatrix} -1 \\\\ 2(0) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$$\n\nNow we can formulate the expression for $\\alpha^*$. The directional derivative of a function $g_i$ at $x^*$ in the direction $d$ is given by $\\nabla g_i(x^*) \\cdot d$. The Mangasarian-Fromovitz constraint qualification (MFCQ) holds if there exists a direction $d$ such that this directional derivative is strictly negative for all active constraints. The quantity $\\alpha^*$ measures the optimal value of this simultaneous strict decrease, maximized over all unit vectors $d$.\n\nThe expression for $\\alpha^*$ is:\n$$\\alpha^*=\\sup_{\\|d\\|_2=1}\\;\\min_{i\\in I(x^*)}\\left(-\\nabla g_i(x^*)\\cdot d\\right)$$\nSince $I(x^*) = \\{1,2\\}$, we have:\n$$\\alpha^*=\\sup_{\\|d\\|_2=1}\\;\\min\\left(-\\nabla g_1(x^*)\\cdot d, -\\nabla g_2(x^*)\\cdot d\\right)$$\nLet $d = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}$ be a unit vector, so its components satisfy the condition $d_1^2 + d_2^2 = 1$. We compute the dot products:\n$$-\\nabla g_1(x^*)\\cdot d = -\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = -d_1$$\n$$-\\nabla g_2(x^*)\\cdot d = -\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = d_1$$\nSubstituting these into the expression for $\\alpha^*$:\n$$\\alpha^* = \\sup_{d_1^2+d_2^2=1} \\min(-d_1, d_1)$$\nLet's analyze the function inside the supremum, $f(d_1) = \\min(-d_1, d_1)$.\nIf $d_1 \\ge 0$, then $-d_1 \\le d_1$, so $\\min(-d_1, d_1) = -d_1$.\nIf $d_1  0$, then $-d_1  d_1$, so $\\min(-d_1, d_1) = d_1$.\nThis function is equivalent to $-|d_1|$.\nSo, the problem simplifies to:\n$$\\alpha^* = \\sup_{d_1^2+d_2^2=1} (-|d_1|)$$\nThe objective function depends only on $d_1$. The constraint $d_1^2+d_2^2=1$ implies that the possible values for $d_1$ are in the interval $[-1, 1]$. We want to find the maximum value of $-|d_1|$ for $d_1 \\in [-1, 1]$.\nThe function $-|d_1|$ is always non-positive. Its maximum value is attained when $|d_1|$ is minimized. Over the interval $[-1, 1]$, the minimum value of $|d_1|$ is $0$, which occurs at $d_1=0$.\nThe maximum value of $-|d_1|$ is therefore $-|0|=0$.\n\nThis maximum is achieved when $d_1=0$. From the unit norm constraint $d_1^2+d_2^2=1$, we have $0^2+d_2^2=1$, which implies $d_2 = \\pm 1$. Thus, the supremum is achieved for the directions $d = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ and $d = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$.\nThe value of the strict-descent margin is $\\alpha^* = 0$.\n\nConceptually, the MFCQ requires the existence of a single direction $d$ that is a descent direction for all active constraints simultaneously. This means $\\nabla g_i(x^*) \\cdot d  0$ for all $i \\in I(x^*)$. In our case, this translates to the two conditions:\n1. $\\nabla g_1(x^*) \\cdot d = d_1  0$\n2. $\\nabla g_2(x^*) \\cdot d = -d_1  0 \\implies d_1  0$\nThese two conditions, $d_1  0$ and $d_1  0$, are mutually exclusive. It is impossible to find a direction $d$ that satisfies both. Therefore, the MFCQ does not hold at $x^*$.\nThe conflict arises because the active gradients, $\\nabla g_1(x^*) = (1,0)^T$ and $\\nabla g_2(x^*) = (-1,0)^T$, are antiparallel vectors. They lie along the $x_1$-axis and point in opposite directions. Geometrically, a descent direction must form an obtuse angle with every active gradient vector. It is impossible for any vector $d$ to form an obtuse angle with both $(1,0)^T$ and $(-1,0)^T$.\nThe value $\\alpha^*$ quantifies this situation. A positive $\\alpha^*$ would indicate that MFCQ holds. Our result $\\alpha^*=0$ signifies that the best we can do is find a direction where the directional derivatives are non-positive, but not all strictly negative. This \"best\" direction is orthogonal to the conflicting gradients (i.e., along the $x_2$-axis), making both directional derivatives equal to $0$, and thus $-\\nabla g_i(x^*) \\cdot d = 0$ for both $i=1,2$. The minimum is $0$, which is the supremum over all possible directions.", "answer": "$$\\boxed{0}$$", "id": "3146844"}]}