{"hands_on_practices": [{"introduction": "Our exploration of first-order necessary conditions begins with the most fundamental case: unconstrained optimization. Before we tackle complex constraints, it is crucial to understand how the derivative provides a powerful tool for finding optima. This practice [@problem_id:2173061] provides a classic example from statistics, asking you to derive the familiar arithmetic mean as the value that minimizes the sum of squared errors, purely by applying the condition that the derivative must be zero at a minimum.", "problem": "An experimental physicist conducts an experiment $n$ times to measure a fundamental physical constant. The set of measurements obtained is $\\{y_1, y_2, \\ldots, y_n\\}$. To determine a single best estimate, $c$, for the constant based on these measurements, the physicist decides to use the principle of least squares. This principle states that the best estimate is the value of $c$ that minimizes the sum of the squared deviations between the estimate and each measurement.\n\nThe sum of squared deviations is given by the function:\n$$ S(c) = \\sum_{i=1}^n (y_i - c)^2 $$\n\nUsing the first-order necessary condition for a minimum, determine the value of $c$ that minimizes $S(c)$. Express your answer as a symbolic expression in terms of the measurements $y_i$ and the total number of measurements $n$.", "solution": "We seek $c$ that minimizes $S(c) = \\sum_{i=1}^{n} (y_{i} - c)^{2}$. Using the first-order necessary condition, differentiate $S(c)$ with respect to $c$:\n$$\n\\frac{dS}{dc} = \\sum_{i=1}^{n} \\frac{d}{dc}(y_{i} - c)^{2} = \\sum_{i=1}^{n} 2(y_{i} - c)(-1) = 2 \\sum_{i=1}^{n} (c - y_{i}) = 2\\left(nc - \\sum_{i=1}^{n} y_{i}\\right).\n$$\nSet the derivative to zero:\n$$\n2\\left(nc - \\sum_{i=1}^{n} y_{i}\\right) = 0 \\quad \\Longrightarrow \\quad nc - \\sum_{i=1}^{n} y_{i} = 0 \\quad \\Longrightarrow \\quad c = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}.\n$$\nFor completeness, the second derivative is\n$$\n\\frac{d^{2}S}{dc^{2}} = \\sum_{i=1}^{n} 2 = 2n > 0,\n$$\nwhich confirms that this critical point is a minimum. Therefore, the minimizing $c$ is the arithmetic mean of the measurements.", "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n} y_{i}}$$", "id": "2173061"}, {"introduction": "As we move from unconstrained to constrained optimization, the Karush-Kuhn-Tucker (KKT) conditions become our primary tool. However, their power depends on their correct application, and a common stumbling block is the sign convention for Lagrange multipliers associated with inequality constraints. This exercise [@problem_id:3129929] is designed to diagnose and correct this frequent error, providing a firm conceptual foundation for why multipliers for standard inequality constraints ($g_i(x) \\le 0$) must be non-negative and how to handle different constraint forms.", "problem": "Consider the constrained optimization problem: minimize $f(x)=x$ subject to $x\\ge 0$. A student writes the Lagrangian as $L(x,\\lambda)=f(x)+\\lambda\\,h(x)$ with $h(x)=x$ and enforces $\\lambda\\ge 0$, then applies stationarity at $x^\\star=0$ to obtain $1+\\lambda^\\star=0$, hence $\\lambda^\\star=-1$, and claims that the Karush–Kuhn–Tucker (KKT) conditions can hold with a negative multiplier. Your tasks are to diagnose the mistake, correct the sign conventions, and justify from first principles why the nonnegativity $\\lambda_i^\\star\\ge 0$ is necessary for inequality constraints in the standard form.\n\nWhich of the following statements are correct?\n\nA. The student’s negative multiplier is acceptable because KKT imposes no sign restriction on multipliers for inequality constraints.\n\nB. If one keeps the inequality as $h(x)\\ge 0$, the Lagrangian must be $L(x,\\mu)=f(x)-\\mu\\,h(x)$ with $\\mu\\ge 0$ (or equivalently $L(x,\\lambda)=f(x)+\\lambda\\,h(x)$ with $\\lambda\\le 0$). Under a consistent convention, the multiplier at $x^\\star=0$ is nonnegative in the first formulation.\n\nC. Writing the constraint in the standard form $g(x)\\le 0$ as $g(x)=-x\\le 0$ restores the standard KKT requirement $\\lambda\\ge 0$ and yields $\\lambda^\\star=1$ at $x^\\star=0$.\n\nD. The requirement $\\lambda_i^\\star\\ge 0$ for inequalities $g_i(x)\\le 0$ is necessary because, at a local minimizer, any feasible direction $d$ satisfies $\\nabla g_i(x^\\star)^\\top d\\le 0$ for active constraints, and ensuring $\\nabla f(x^\\star)$ lies in the conic hull of $\\{\\nabla g_i(x^\\star)\\}$ requires nonnegative coefficients; otherwise a feasible descent direction would exist.\n\nE. For equality constraints, the associated multipliers must also be nonnegative; otherwise the KKT conditions fail.\n\nSelect all that apply.", "solution": "The user's problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Optimization problem: minimize $f(x)=x$ subject to $x\\ge 0$.\n- Student's procedure:\n    - Lagrangian: $L(x,\\lambda)=f(x)+\\lambda\\,h(x)$\n    - Constraint function: $h(x)=x$ (from the constraint $x\\ge 0$)\n    - Assumed multiplier sign: $\\lambda\\ge 0$\n    - Stationarity is applied at the candidate solution $x^\\star=0$.\n    - The stationarity equation is $1+\\lambda^\\star=0$.\n    - The resulting multiplier is $\\lambda^\\star=-1$.\n    - The student's claim is that KKT conditions can hold with a negative multiplier.\n- The tasks are to diagnose the mistake, correct the sign conventions, and provide a first-principles justification for the multiplier's nonnegativity in the standard form.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in the theory of constrained optimization, specifically concerning the Karush-Kuhn-Tucker (KKT) conditions. All components are mathematically well-defined and are central to the field of optimization methods.\n- **Well-Posed:** The optimization problem itself has a clear and unique solution ($x^\\star=0$). The question asks for an analysis of a common conceptual error in applying the KKT framework, which is a well-defined pedagogical task.\n- **Objective:** The problem is stated objectively, presenting a flawed line of reasoning and asking for its correction and a theoretical justification. It is free of subjective or ambiguous language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It presents a clear, self-contained, and mathematically sound scenario for analysis. I will now proceed with the solution.\n\n***\n\nThe core of the issue lies in the inconsistent application of standard conventions for the Karush-Kuhn-Tucker (KKT) conditions. The standard form for a minimization problem is typically expressed as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & f(x) \\\\\n\\text{subject to} \\quad & g_i(x) \\le 0, \\quad i=1, \\dots, m \\\\\n& h_j(x) = 0, \\quad j=1, \\dots, p\n\\end{aligned}\n$$\nThe Lagrangian for this problem is defined as $L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^p \\mu_j h_j(x)$. The first-order necessary KKT conditions for a point $x^\\star$ to be a local minimizer (under certain regularity conditions) are:\n1.  **Stationarity:** $\\nabla_x L(x^\\star, \\lambda^\\star, \\mu^\\star) = 0$\n2.  **Primal Feasibility:** $g_i(x^\\star) \\le 0$ for all $i$, and $h_j(x^\\star) = 0$ for all $j$.\n3.  **Dual Feasibility:** $\\lambda_i^\\star \\ge 0$ for all $i$. (Note: multipliers $\\mu_j$ for equality constraints are not restricted in sign).\n4.  **Complementary Slackness:** $\\lambda_i^\\star g_i(x^\\star) = 0$ for all $i$.\n\nThe student's error is a failure to adhere to these conventions. The student used a constraint of the form $h(x) = x \\ge 0$, which is not in the standard $g_i(x) \\le 0$ format, yet they assumed the multiplier sign convention $\\lambda \\ge 0$ that belongs to the standard format. This inconsistency leads to a contradiction.\n\nNow, we evaluate each statement.\n\n**A. The student’s negative multiplier is acceptable because KKT imposes no sign restriction on multipliers for inequality constraints.**\nThis statement is fundamentally incorrect. The KKT conditions absolutely impose a sign restriction on the multipliers associated with inequality constraints. This condition, known as dual feasibility, is crucial. For the standard formulation with constraints $g_i(x) \\le 0$, the multipliers must be non-negative, $\\lambda_i \\ge 0$. The student's result of $\\lambda^\\star=-1$ is a symptom of an incorrectly formulated Lagrangian system, not evidence that multipliers can be arbitrarily signed.\n**Verdict: Incorrect.**\n\n**B. If one keeps the inequality as $h(x)\\ge 0$, the Lagrangian must be $L(x,\\mu)=f(x)-\\mu\\,h(x)$ with $\\mu\\ge 0$ (or equivalently $L(x,\\lambda)=f(x)+\\lambda\\,h(x)$ with $\\lambda\\le 0$). Under a consistent convention, the multiplier at $x^\\star=0$ is nonnegative in the first formulation.**\nThis statement describes the correct ways to handle a constraint of the form $h(x) \\ge 0$. Let's analyze both conventions for the given problem where $f(x)=x$ and $h(x)=x \\ge 0$.\n-   **Convention 1:** $L(x, \\mu) = f(x) - \\mu h(x) = x - \\mu x$, with the requirement $\\mu \\ge 0$.\n    The stationarity condition is $\\frac{\\partial L}{\\partial x} = 1 - \\mu = 0$, which yields $\\mu^\\star = 1$. This result is consistent with the requirement $\\mu^\\star \\ge 0$.\n-   **Convention 2 (equivalent):** $L(x, \\lambda) = f(x) + \\lambda h(x) = x + \\lambda x$, with the requirement $\\lambda \\le 0$.\n    The stationarity condition is $\\frac{\\partial L}{\\partial x} = 1 + \\lambda = 0$, which yields $\\lambda^\\star = -1$. This result is consistent with the requirement $\\lambda^\\star \\le 0$. This is what the student calculated, but they failed to recognize that this form requires a non-positive multiplier.\nThe statement correctly identifies these two consistent formulations and accurately states that in the first formulation ($L=f-\\mu h$), the resulting multiplier is $\\mu^\\star=1$, which is non-negative.\n**Verdict: Correct.**\n\n**C. Writing the constraint in the standard form $g(x)\\le 0$ as $g(x)=-x\\le 0$ restores the standard KKT requirement $\\lambda\\ge 0$ and yields $\\lambda^\\star=1$ at $x^\\star=0$.**\nThis statement describes the most common and robust method for resolving the ambiguity: converting the problem to the standard form.\n- The constraint $x \\ge 0$ is equivalent to $g(x) = -x \\le 0$.\n- The problem is to minimize $f(x)=x$ subject to $g(x)=-x \\le 0$.\n- The standard Lagrangian is $L(x, \\lambda) = f(x) + \\lambda g(x) = x + \\lambda(-x) = x(1-\\lambda)$.\n- The standard dual feasibility condition is $\\lambda \\ge 0$.\n- The stationarity condition is $\\frac{\\partial L}{\\partial x} = 1 - \\lambda = 0$, which gives $\\lambda^\\star = 1$.\n- The candidate solution is $x^\\star = 0$. Let's check all KKT conditions with $\\lambda^\\star=1$:\n    1. Stationarity: $1 - \\lambda^\\star = 1 - 1 = 0$. (Holds)\n    2. Primal Feasibility: $g(x^\\star) = -0 \\le 0$. (Holds)\n    3. Dual Feasibility: $\\lambda^\\star = 1 \\ge 0$. (Holds)\n    4. Complementary Slackness: $\\lambda^\\star g(x^\\star) = 1 \\cdot (-0) = 0$. (Holds)\nAll conditions hold. The statement correctly asserts that this conversion leads to the standard requirement $\\lambda \\ge 0$ and correctly calculates $\\lambda^\\star = 1$.\n**Verdict: Correct.**\n\n**D. The requirement $\\lambda_i^\\star\\ge 0$ for inequalities $g_i(x)\\le 0$ is necessary because, at a local minimizer, any feasible direction $d$ satisfies $\\nabla g_i(x^\\star)^\\top d\\le 0$ for active constraints, and ensuring $\\nabla f(x^\\star)$ lies in the conic hull of $\\{\\nabla g_i(x^\\star)\\}$ requires nonnegative coefficients; otherwise a feasible descent direction would exist.**\nThis statement provides the fundamental geometric justification for the non-negativity of multipliers.\nLet $x^\\star$ be a local minimum.\n1.  A direction $d$ is a *feasible direction* from $x^\\star$ only if it does not immediately lead out of the feasible set. For any active constraint $g_i(x^\\star)=0$, this means we must have $g_i(x^\\star + \\alpha d) \\le 0$ for small $\\alpha > 0$. By a first-order Taylor expansion, $g_i(x^\\star) + \\alpha \\nabla g_i(x^\\star)^\\top d \\le 0$, which simplifies to $\\nabla g_i(x^\\star)^\\top d \\le 0$.\n2.  A direction $d$ is a *descent direction* if moving along it decreases the objective function, i.e., $\\nabla f(x^\\star)^\\top d  0$.\n3.  At a local minimum $x^\\star$, there can be no direction $d$ which is simultaneously feasible and a descent direction.\nAccording to Farkas' Lemma (a theorem of the alternative), this condition is equivalent to stating that the negative gradient of the objective, $-\\nabla f(x^\\star)$, must be in the conic hull of the active constraint gradients, $\\{\\nabla g_i(x^\\star)\\}_{i \\in I(x^\\star)}$. That is, $-\\nabla f(x^\\star) = \\sum_{i \\in I(x^\\star)} \\lambda_i^\\star \\nabla g_i(x^\\star)$ for some coefficients $\\lambda_i^\\star \\ge 0$. Rearranging gives the stationarity condition $\\nabla f(x^\\star) + \\sum_{i \\in I(x^\\star)} \\lambda_i^\\star \\nabla g_i(x^\\star) = 0$, where the multipliers $\\lambda_i^\\star$ are non-negative.\nThe statement in option D, while slightly imprecise in saying `∇f(x*)` lies in the conic hull (it's `-∇f(x*)` for the standard Lagrangian), captures the essence of this \"separating hyperplane\" argument perfectly: the sign of the multipliers is what prevents a feasible descent direction from existing. The core logic is sound and provides the correct first-principles justification.\n**Verdict: Correct.**\n\n**E. For equality constraints, the associated multipliers must also be nonnegative; otherwise the KKT conditions fail.**\nThis statement is incorrect. The multipliers associated with equality constraints, $h_j(x)=0$, are unrestricted in sign ($\\mu_j \\in \\mathbb{R}$). An equality constraint $h(x)=0$ can be modeled as two simultaneous inequality constraints: $g_1(x) = h(x) \\le 0$ and $g_2(x) = -h(x) \\le 0$. The corresponding term in the Lagrangian would be $\\lambda_1 g_1(x) + \\lambda_2 g_2(x) = \\lambda_1 h(x) + \\lambda_2(-h(x)) = (\\lambda_1 - \\lambda_2)h(x)$, with $\\lambda_1, \\lambda_2 \\ge 0$. Defining $\\mu = \\lambda_1 - \\lambda_2$, we can see that $\\mu$ can take any real value. Thus, the multiplier for an equality constraint is sign-unrestricted.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BCD}$$", "id": "3129929"}, {"introduction": "Having mastered the foundational principles of both unconstrained optimization and the KKT sign conventions, we can now assemble the full toolkit to solve a comprehensive constrained problem. This practice [@problem_id:3129884] challenges you to systematically apply the complete set of KKT conditions—primal and dual feasibility, complementary slackness, and stationarity—to a problem involving both equality and inequality constraints. By working through this example, you will develop a robust methodology for identifying candidate optimal points in complex, realistic scenarios.", "problem": "Let the decision variable be $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$. Consider the constrained optimization problem of minimizing the objective\n$$\nf(\\mathbf{x}) = (x_1 - 3)^2 + (x_2 + 1)^2 + 2(x_1 - x_2)\n$$\nsubject to the equality constraint\n$$\nh(\\mathbf{x}) = x_1 + x_2 - 2 = 0\n$$\nand the inequality constraints\n$$\ng_1(\\mathbf{x}) = -x_1 \\le 0, \\quad g_2(\\mathbf{x}) = -x_2 \\le 0, \\quad g_3(\\mathbf{x}) = x_1 - 2 \\le 0.\n$$\nPerform the following tasks:\n- Using the definition of a constrained local minimum and first-order necessary conditions from optimization methods, systematically write the Karush-Kuhn-Tucker (KKT) conditions (primal feasibility, dual feasibility, complementary slackness, and stationarity) for this problem, introducing a Lagrange multiplier $\\lambda$ for $h(\\mathbf{x})$ and nonnegative multipliers $\\mu_1$, $\\mu_2$, $\\mu_3$ for $g_1(\\mathbf{x}), g_2(\\mathbf{x}), g_3(\\mathbf{x})$.\n- Determine the active set of constraints at a KKT point that solves the problem, and deduce which inequality multipliers must be strictly positive, which must be zero, and which may be zero even if the corresponding constraint is active.\n- Compute the value of the Lagrange multiplier associated with the equality constraint at the KKT point.\n\nExpress the final answer as a single real number. No rounding is required.", "solution": "The constrained optimization problem is to minimize the objective function\n$$ f(\\mathbf{x}) = (x_1 - 3)^2 + (x_2 + 1)^2 + 2(x_1 - x_2) $$\nwhere $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$, subject to the equality constraint\n$$ h(\\mathbf{x}) = x_1 + x_2 - 2 = 0 $$\nand the inequality constraints\n$$ g_1(\\mathbf{x}) = -x_1 \\le 0 $$\n$$ g_2(\\mathbf{x}) = -x_2 \\le 0 $$\n$$ g_3(\\mathbf{x}) = x_1 - 2 \\le 0 $$\n\nThe objective function is a strictly convex quadratic function (its Hessian is positive definite), and the feasible region defined by the linear constraints is a convex set. Therefore, any point that satisfies the Karush-Kuhn-Tucker (KKT) conditions is a unique global minimum.\n\nFirst, as requested, we systematically write the KKT conditions for this problem. The Lagrangian function $L(\\mathbf{x}, \\lambda, \\boldsymbol{\\mu})$ is defined with multipliers $\\lambda$ for $h(\\mathbf{x})$ and $\\mu_1, \\mu_2, \\mu_3$ for $g_1(\\mathbf{x}), g_2(\\mathbf{x}), g_3(\\mathbf{x})$:\n$$ L(x_1, x_2, \\lambda, \\mu_1, \\mu_2, \\mu_3) = f(\\mathbf{x}) + \\lambda h(\\mathbf{x}) + \\sum_{i=1}^3 \\mu_i g_i(\\mathbf{x}) $$\n$$ L(x_1, x_2, \\lambda, \\mu_1, \\mu_2, \\mu_3) = \\left[(x_1 - 3)^2 + (x_2 + 1)^2 + 2(x_1 - x_2)\\right] + \\lambda(x_1 + x_2 - 2) + \\mu_1(-x_1) + \\mu_2(-x_2) + \\mu_3(x_1 - 2) $$\nThe KKT conditions for a point $\\mathbf{x}^*$ to be a constrained local minimum are:\n\n1.  **Primal Feasibility**: The point must satisfy all constraints.\n    $$ x_1^* + x_2^* - 2 = 0 $$\n    $$ -x_1^* \\le 0 \\implies x_1^* \\ge 0 $$\n    $$ -x_2^* \\le 0 \\implies x_2^* \\ge 0 $$\n    $$ x_1^* - 2 \\le 0 \\implies x_1^* \\le 2 $$\n\n2.  **Dual Feasibility**: The multipliers for the inequality constraints must be non-negative.\n    $$ \\mu_1 \\ge 0, \\quad \\mu_2 \\ge 0, \\quad \\mu_3 \\ge 0 $$\n\n3.  **Complementary Slackness**: The product of each inequality multiplier and its corresponding constraint function evaluation must be zero.\n    $$ \\mu_1(-x_1^*) = 0 $$\n    $$ \\mu_2(-x_2^*) = 0 $$\n    $$ \\mu_3(x_1^* - 2) = 0 $$\n\n4.  **Stationarity**: The gradient of the Lagrangian with respect to the decision variables $\\mathbf{x}$ must be zero at the point $\\mathbf{x}^*$. The gradient of the objective function is:\n    $$ \\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2(x_1 - 3)(1) + 2 \\\\ 2(x_2 + 1)(1) - 2 \\end{pmatrix} = \\begin{pmatrix} 2x_1 - 4 \\\\ 2x_2 \\end{pmatrix} $$\n    The gradients of the constraint functions are:\n    $$ \\nabla h(\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad \\nabla g_1(\\mathbf{x}) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}, \\quad \\nabla g_2(\\mathbf{x}) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad \\nabla g_3(\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $$\n    The stationarity condition $\\nabla_{\\mathbf{x}} L(\\mathbf{x}^*, \\lambda, \\boldsymbol{\\mu}) = \\mathbf{0}$ yields the following two equations:\n    $$ 2x_1^* - 4 + \\lambda - \\mu_1 + \\mu_3 = 0 $$\n    $$ 2x_2^* + \\lambda - \\mu_2 = 0 $$\n\nTo solve this system, we first determine the optimal point $\\mathbf{x}^*$. We can substitute the equality constraint $x_2 = 2 - x_1$ into the objective function to obtain a one-dimensional problem in $x_1$. The feasible range for $x_1$ is determined by $x_1 \\ge 0$ and $x_2 = 2 - x_1 \\ge 0$, which implies $x_1 \\in [0, 2]$. The constraint $x_1 \\le 2$ is redundant.\nThe new objective function is $F(x_1) = f(x_1, 2-x_1)$:\n$$ F(x_1) = (x_1 - 3)^2 + ((2-x_1) + 1)^2 + 2(x_1 - (2-x_1)) $$\n$$ F(x_1) = (x_1 - 3)^2 + (3 - x_1)^2 + 2(2x_1 - 2) $$\n$$ F(x_1) = 2(x_1 - 3)^2 + 4(x_1 - 1) = 2(x_1^2 - 6x_1 + 9) + 4x_1 - 4 = 2x_1^2 - 8x_1 + 14 $$\nTo find the minimum of $F(x_1)$ over $[0, 2]$, we compute its derivative:\n$$ F'(x_1) = 4x_1 - 8 $$\nSetting $F'(x_1) = 0$ gives $x_1 = 2$. This point is the vertex of the upward-opening parabola $F(x_1)$, and it lies at the boundary of the feasible interval $[0, 2]$. Therefore, the minimum value of $F(x_1)$ on this interval occurs at $x_1^*=2$.\nThe corresponding value for $x_2$ is $x_2^* = 2 - x_1^* = 2 - 2 = 0$.\nThe KKT point and thus the optimal solution is $\\mathbf{x}^* = (2, 0)$.\n\nNext, we determine the active set and the multipliers. At $\\mathbf{x}^* = (2, 0)$:\n- $h(\\mathbf{x}^*) = 2 + 0 - 2 = 0$ (Active).\n- $g_1(\\mathbf{x}^*) = -x_1^* = -2  0$ (Inactive).\n- $g_2(\\mathbf{x}^*) = -x_2^* = -0 = 0$ (Active).\n- $g_3(\\mathbf{x}^*) = x_1^* - 2 = 2 - 2 = 0$ (Active).\nThe active set of constraints is $\\{h, g_2, g_3\\}$.\n\nFrom the complementary slackness conditions:\n- Since $g_1$ is inactive, its multiplier must be zero: $\\mu_1 = 0$.\n- For $g_2$ and $g_3$, which are active, the condition is satisfied, and the multipliers $\\mu_2$ and $\\mu_3$ are not necessarily zero. From dual feasibility, $\\mu_2 \\ge 0$ and $\\mu_3 \\ge 0$.\n\nNow we substitute $\\mathbf{x}^* = (2, 0)$ and $\\mu_1 = 0$ into the stationarity equations:\n1.  $2(2) - 4 + \\lambda - 0 + \\mu_3 = 0 \\implies \\lambda + \\mu_3 = 0$.\n2.  $2(0) + \\lambda - \\mu_2 = 0 \\implies \\lambda - \\mu_2 = 0$.\n\nFrom the second equation, $\\lambda = \\mu_2$. Since $\\mu_2 \\ge 0$ (dual feasibility), we must have $\\lambda \\ge 0$.\nFrom the first equation, $\\mu_3 = -\\lambda$. Since $\\mu_3 \\ge 0$ (dual feasibility), we must have $-\\lambda \\ge 0$, which implies $\\lambda \\le 0$.\n\nTo satisfy both $\\lambda \\ge 0$ and $\\lambda \\le 0$, the only possibility is $\\lambda = 0$.\nConsequently, this gives $\\mu_2 = \\lambda = 0$ and $\\mu_3 = -\\lambda = 0$.\nThe full set of multipliers is $(\\lambda, \\mu_1, \\mu_2, \\mu_3) = (0, 0, 0, 0)$.\nThis is a degenerate case where active inequality constraints have zero multipliers. This occurs because the unconstrained minimum of the objective function at $(2, 0)$ happens to coincide with a vertex of the feasible region.\n\nAnswering the specific questions:\n- The KKT conditions have been written out above.\n- The active set at the KKT point $\\mathbf{x}^*=(2,0)$ consists of $\\{h, g_2, g_3\\}$. The multiplier $\\mu_1$ must be zero as $g_1$ is inactive. The multipliers $\\mu_2$ and $\\mu_3$ correspond to active constraints, but they are also zero in this solution; this demonstrates that an active constraint can have a zero multiplier. No inequality multiplier is strictly positive.\n- The value of the Lagrange multiplier associated with the equality constraint at the KKT point is $\\lambda = 0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3129884"}]}