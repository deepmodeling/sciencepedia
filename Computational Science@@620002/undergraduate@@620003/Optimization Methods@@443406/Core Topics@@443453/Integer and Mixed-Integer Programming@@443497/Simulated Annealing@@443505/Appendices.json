{"hands_on_practices": [{"introduction": "This first practice problem isolates the core mechanic of simulated annealing: the Metropolis acceptance criterion. By calculating the temperature $T$ required to achieve a specific acceptance probability for an unfavorable move, you will gain a hands-on understanding of how this crucial parameter governs the algorithm's exploratory behavior. This exercise [@problem_id:2202507] provides a concrete foundation for grasping the balance between exploitation and exploration.", "problem": "A data center is optimizing the allocation of computational tasks across its server farm using a simulated annealing algorithm. The objective is to minimize the total operational cost. In this context, the \"energy\" of a particular allocation configuration is defined by its projected operational cost, measured in abstract \"cost units.\"\n\nThe algorithm is currently evaluating a configuration, which we will call the current state, with a cost of $C_{current} = 85.5$ cost units. A new configuration, the neighboring state, is proposed by moving a specific task. This change results in a higher projected cost of $C_{neighbor} = 88.0$ cost units.\n\nThe simulated annealing algorithm sometimes accepts \"uphill\" moves (i.e., moves to states with higher cost) to escape local minima. The probability of accepting such a move is governed by a Boltzmann-like distribution dependent on a control parameter called \"temperature,\" $T$. In this model, the temperature $T$ has the same units as the cost.\n\nFor the algorithm to effectively explore the solution space at this stage, the system designers have determined that this specific uphill move should be accepted with a probability of exactly $0.15$. What is the required value of the temperature parameter $T$ to achieve this acceptance probability?\n\nRound your final answer to three significant figures.", "solution": "In simulated annealing, an uphill move with cost increase $\\Delta C$ is accepted with probability given by the Boltzmann-like factor:\n$$\nP_{\\text{accept}} = \\exp\\!\\left(-\\frac{\\Delta C}{T}\\right).\n$$\nHere, the current state has cost $C_{\\text{current}} = 85.5$ and the neighbor has cost $C_{\\text{neighbor}} = 88.0$, so the cost increase is\n$$\n\\Delta C = C_{\\text{neighbor}} - C_{\\text{current}} = 88.0 - 85.5 = 2.5.\n$$\nWe require this move to be accepted with probability $0.15$, so\n$$\n\\exp\\!\\left(-\\frac{2.5}{T}\\right) = 0.15.\n$$\nTaking the natural logarithm of both sides and solving for $T$ gives\n$$\n-\\frac{2.5}{T} = \\ln(0.15) \\quad \\Rightarrow \\quad T = -\\frac{2.5}{\\ln(0.15)} = \\frac{2.5}{\\ln\\!\\left(\\frac{1}{0.15}\\right)} = \\frac{2.5}{\\ln\\!\\left(\\frac{20}{3}\\right)}.\n$$\nEvaluating numerically and rounding to three significant figures,\n$$\nT \\approx 1.32.\n$$", "answer": "$$\\boxed{1.32}$$", "id": "2202507"}, {"introduction": "Building upon the acceptance criterion, this exercise simulates a complete step within the simulated annealing process. You will analyze a system with a discrete set of states and a defined neighborhood structure to calculate the total probability of transitioning away from a local, but not global, energy minimum. This practice [@problem_id:2202527] illustrates the interplay between proposing a random move and deciding whether to accept it, which is fundamental to escaping local optima.", "problem": "A system can exist in one of six possible configurations, labeled {A, B, C, D, E, F}. A physicist is using a Simulated Annealing (SA) algorithm to find the configuration with the minimum energy. The energy, $E$, for each configuration is given as:\n$E(A) = 10$, $E(B) = 2$, $E(C) = 8$, $E(D) = 0$, $E(E) = 12$, $E(F) = 15$.\n\nThe SA algorithm works in steps. At each step, starting from a current configuration, it proposes a move to a new configuration. The set of possible new configurations, called the neighborhood, is very restricted. The allowed transitions are only between adjacent configurations in the sequence F-E-D-C-B-A. Specifically:\n- From A, one can only move to B.\n- From B, one can move to A or C.\n- From C, one can move to B or D.\n- From D, one can move to C or E.\n- From E, one can move to D or F.\n- From F, one can only move to E.\n\nAt a particular point, the algorithm has found the configuration B, and the annealing temperature is fixed for this step at $T=3$ (in units where the Boltzmann constant $k_B=1$). In the next step of the algorithm, a neighboring configuration is chosen uniformly at random from the set of allowed transitions. This proposed move is then accepted or rejected based on the Metropolis criterion:\n- If the change in energy, $\\Delta E = E_{new} - E_{current}$, is less than or equal to zero, the move is always accepted.\n- If $\\Delta E > 0$, the move is accepted with a probability of $P_{accept} = \\exp(-\\Delta E / T)$.\nIf a proposed move is rejected, the system remains in its current configuration.\n\nGiven that the system is currently in configuration B, calculate the total probability that the algorithm will transition to a different configuration in this single step. Round your final answer to three significant figures.", "solution": "We are at configuration B with energy $E(B)=2$. From B, the allowed neighboring configurations are A and C, chosen uniformly, so each is proposed with probability $1/2$.\n\nFor a proposed move to A:\n- The energy change is $\\Delta E_{B\\to A} = E(A) - E(B) = 10 - 2 = 8 > 0$.\n- With $T=3$ and $k_{B}=1$, the Metropolis acceptance probability is $P_{\\text{acc}}(B\\to A) = \\exp\\!\\left(-\\frac{\\Delta E}{T}\\right) = \\exp\\!\\left(-\\frac{8}{3}\\right)$.\n\nFor a proposed move to C:\n- The energy change is $\\Delta E_{B\\to C} = E(C) - E(B) = 8 - 2 = 6 > 0$.\n- The acceptance probability is $P_{\\text{acc}}(B\\to C) = \\exp\\!\\left(-\\frac{6}{3}\\right) = \\exp(-2)$.\n\nThe total probability to transition away from B in this step equals the sum over neighbors of the proposal probability times the acceptance probability:\n$$\nP_{\\text{move}}=\\frac{1}{2}\\exp\\!\\left(-\\frac{8}{3}\\right)+\\frac{1}{2}\\exp(-2).\n$$\nEvaluating numerically,\n$$\n\\exp(-2)\\approx 0.135335283,\\quad \\exp\\!\\left(-\\frac{8}{3}\\right)\\approx 0.06948345,\n$$\nso\n$$\nP_{\\text{move}}\\approx \\frac{1}{2}\\left(0.06948345+0.135335283\\right)\\approx 0.102409\\;\\;\\to\\;\\;0.102\\text{ (three significant figures)}.\n$$", "answer": "$$\\boxed{0.102}$$", "id": "2202527"}, {"introduction": "This capstone exercise challenges you to move from theory to implementation by building a complete simulated annealing solver for the bin-packing problem. You will define a complex energy function, design move proposal strategies, and manage a cooling schedule to find high-quality solutions. This comprehensive practice [@problem_id:3182664] synthesizes all the core concepts of SA, providing invaluable experience in applying the algorithm to a non-trivial optimization task from first principles.", "problem": "You will implement a complete simulated annealing solver for a discrete bin-packing energy minimization problem, starting from first principles. You will define the energy function so that it penalizes bin overflow and imbalance, propose local moves that explore the configuration space, justify a statistically principled acceptance rule, and study how temperature scheduling affects the reduction of conflicts. The problem is purely mathematical and discrete; there are no physical units and no angle measurements.\n\nGiven a set of $n$ items with nonnegative sizes $\\{s_i\\}_{i=1}^n$, a fixed number of bins $B$, and a bin capacity $C$, a configuration is a function $x:\\{1,\\dots,n\\}\\to\\{1,\\dots,B\\}$ assigning each item to a bin. For bin $b\\in\\{1,\\dots,B\\}$, let the load be $L_b(x)=\\sum_{i: x(i)=b} s_i$, and define the utilization $u_b(x)=L_b(x)/C$. Define the energy \n$$\nE(x)=\\alpha\\sum_{b=1}^B \\max\\{0,\\,u_b(x)-1\\}+\\beta\\left(\\frac{1}{B}\\sum_{b=1}^B \\left(u_b(x)-\\bar{u}(x)\\right)^2\\right),\n$$\nwhere $\\bar{u}(x)=\\frac{1}{B}\\sum_{b=1}^B u_b(x)$ is the average utilization, $\\alpha>0$ controls the strength of overflow penalties, and $\\beta>0$ controls the strength of an imbalance penalty via the variance of the utilizations. The goals are to minimize $E(x)$ and to understand the role of temperature in simulated annealing.\n\nYou must implement a simulated annealing algorithm from first principles as follows, beginning from the base that Markov chains with a Boltzmann stationary law are obtained using symmetry of proposals and detailed balance:\n\n- State space and energy: The states are all assignments $x$; the energy is $E(x)$ as defined above.\n- Move proposals: Use a proposal distribution constructed from two symmetric move types that preserve the state space connectivity:\n  1. Single-item reassignment: pick a uniformly random item index $i\\in\\{1,\\dots,n\\}$ and a uniformly random destination bin $b'\\in\\{1,\\dots,B\\}$ with $b'\\neq x(i)$, and reassign $i$ to $b'$.\n  2. Two-item swap: pick two distinct items $i\\neq j$ uniformly at random; if $x(i)\\neq x(j)$, swap their bin assignments.\n- Acceptance rule: Use a Metropolis-type acceptance rule derived from the Boltzmann distribution associated with energy $E(x)$ at temperature $T>0$. The acceptance rule must be justified from the principle of detailed balance in your written solution, and your program must implement the explicit acceptance probability you derive. Your implementation must also include a correct limiting behavior as $T\\to 0^+$.\n- Temperature schedule: Use a geometric (multiplicative) cooling schedule $T_k = T_0 r^k$ with $T_0\\ge 0$ and $r\\in(0,1)$, and analyze its effect qualitatively on the ability to reduce conflicts (overflows and imbalance). Your written solution must justify why this schedule corresponds to an exponential cooling behavior starting from a simple decay principle.\n\nImplementation details that must be followed:\n- Initialize from a deterministic greedy load-balancing assignment: sort items in non-increasing order by size and place each item into the bin with the smallest current load (break ties by the lowest-index bin). This ensures the same starting point across test cases.\n- Use the two move types with equal probability at each iteration, sampling uniformly at random among the eligible choices.\n- For acceptance decisions, derive and implement a correct and numerically stable acceptance probability. If $T=0$ at any iteration, the algorithm must reduce to pure descent, accepting only non-worsening moves.\n- Keep track of the best energy encountered $E^*$ and return it at the end of the run for each test case.\n- Use a pseudo-random number generator with a fixed seed provided for each test case, to ensure reproducibility.\n\nTest suite and required program output:\nProvide results for the following four test cases. For each case, run simulated annealing for a fixed number of iterations $K$. The algorithm parameters must be exactly as specified. For all cases, use the same energy parameters $\\alpha$ and $\\beta$ unless otherwise specified.\n\nCommon energy parameters: $\\alpha=20.0$, $\\beta=1.0$.\n\n- Case A (general exploration with moderate cooling):\n  - Items $\\{s_i\\}$: $[7,5,6,2,3,4,8,1,9,2,6,5]$\n  - Number of bins $B$: $4$\n  - Capacity $C$: $10$\n  - Initial temperature $T_0$: $5.0$\n  - Cooling factor $r$: $0.997$\n  - Iterations $K$: $4000$\n  - Seed: $11$\n- Case B (boundary behavior at zero temperature, greedy descent):\n  - Items $\\{s_i\\}$: $[7,5,6,2,3,4,8,1,9,2,6,5]$\n  - Number of bins $B$: $4$\n  - Capacity $C$: $10$\n  - Initial temperature $T_0$: $0.0$\n  - Cooling factor $r$: $0.500$\n  - Iterations $K$: $4000$\n  - Seed: $11$\n- Case C (very slow cooling from high temperature):\n  - Items $\\{s_i\\}$: $[7,5,6,2,3,4,8,1,9,2,6,5]$\n  - Number of bins $B$: $4$\n  - Capacity $C$: $10$\n  - Initial temperature $T_0$: $20.0$\n  - Cooling factor $r$: $0.999$\n  - Iterations $K$: $4000$\n  - Seed: $11$\n- Case D (overflow-dominated instance, moderate cooling):\n  - Items $\\{s_i\\}$: $[9,9,8,7,6,6,5,5,4,4]$\n  - Number of bins $B$: $4$\n  - Capacity $C$: $10$\n  - Initial temperature $T_0$: $10.0$\n  - Cooling factor $r$: $0.998$\n  - Iterations $K$: $5000$\n  - Seed: $99$\n\nFor each case, your program must output the best energy value $E^*$ found, rounded to exactly $4$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[E^*_A,E^*_B,E^*_C,E^*_D]$. For example, the output format must look like \"[0.1234,0.2345,0.3456,0.4567]\".", "solution": "We begin from the base of statistical mechanics and Markov chain Monte Carlo. A fundamental and well-tested principle is that a system at thermal equilibrium at absolute temperature $T>0$ occupies a microstate $x$ with probability proportional to the Boltzmann weight $\\exp(-E(x)/T)$, where $E(x)$ is an energy function. To construct a Markov chain over states that has this stationary distribution, a standard approach is the Metropolis-Hastings method with proposals that obey detailed balance.\n\nLet $q(x\\to y)$ be a proposal distribution over states $y$ given the current state $x$. If we ensure that $q$ is symmetric, meaning $q(x\\to y)=q(y\\to x)$ for all pairs $(x,y)$ with nonzero probability, then detailed balance with respect to the Boltzmann distribution reduces to a condition on the acceptance probability $A(x\\to y)$:\n$$\n\\pi_T(x) q(x\\to y) A(x\\to y) = \\pi_T(y) q(y\\to x) A(y\\to x),\n$$\nwhere $\\pi_T(x)\\propto \\exp(-E(x)/T)$ is the target distribution. Under symmetry of $q$, we obtain\n$$\n\\frac{A(x\\to y)}{A(y\\to x)} = \\frac{\\pi_T(y)}{\\pi_T(x)} = \\exp\\left(-\\frac{E(y)-E(x)}{T}\\right) = \\exp\\left(-\\frac{\\Delta E}{T}\\right).\n$$\nA well-tested choice that satisfies this ratio and yields $A\\in[0,1]$ is the Metropolis acceptance function\n$$\nA(x\\to y) = \\min\\left\\{1,\\; \\exp\\left(-\\frac{\\Delta E}{T}\\right)\\right\\},\n$$\nwith $\\Delta E=E(y)-E(x)$. This has two limiting properties consistent with optimization: if $\\Delta E\\le 0$, then $A=1$ and all improving moves are accepted; if $\\Delta E>0$ and $T\\to 0^+$, then $A\\to 0$ and uphill moves are rejected, recovering greedy descent. At strictly positive $T$, the chain occasionally accepts uphill moves, facilitating escape from local minima.\n\nWe next define the discrete optimization problem. For $B$ bins of capacity $C$ and item sizes $\\{s_i\\}_{i=1}^n$, a configuration $x$ assigns each item to a bin. The load on bin $b$ is $L_b(x)=\\sum_{i: x(i)=b} s_i$, and the utilization is $u_b(x)=L_b(x)/C$. The energy function is a sum of two terms:\n- An overflow penalty $\\alpha\\sum_{b=1}^B \\max\\{0,\\,u_b(x)-1\\}$, which is zero if every bin is within capacity and increases linearly with overflow otherwise. The scalar $\\alpha>0$ scales how strongly overflow is penalized.\n- A variance penalty $\\beta\\cdot \\frac{1}{B}\\sum_{b=1}^B \\left(u_b(x)-\\bar{u}(x)\\right)^2$, where $\\bar{u}(x)=\\frac{1}{B}\\sum_{b=1}^B u_b(x)$ is the mean utilization. This term encourages balanced bins and is scaled by $\\beta>0$.\n\nBoth terms are dimensionless since utilizations $u_b$ are already normalized by $C$.\n\nMove proposals must be symmetric to use the Metropolis acceptance without explicit Hastings corrections. We choose two move types with equal probability:\n- Single-item reassignment: pick an item index $i$ uniformly from $\\{1,\\dots,n\\}$ and a destination bin $b'$ uniformly from $\\{1,\\dots,B\\}\\setminus\\{x(i)\\}$, and reassign $i$ to $b'$. The reverse move has the same probability, establishing symmetry.\n- Two-item swap: pick distinct items $i\\neq j$ uniformly at random. If $x(i)\\neq x(j)$, swap their bins. The reverse operation is the same swap, again symmetric.\n\nFor the temperature schedule, we employ geometric cooling $T_k=T_0 r^k$ with $r\\in(0,1)$. This schedule arises by discretizing an exponential decay governed by the simple linear differential equation $\\frac{dT}{dk}=-\\lambda T$ with solution $T(k)=T_0 e^{-\\lambda k}$. Using a discrete multiplicative decrement with $r=e^{-\\lambda}$ yields $T_k=T_0 r^k$, a standard, well-tested schedule in practice. At high temperature, the acceptance probability $A(x\\to y)$ is close to $1$ even for modestly positive $\\Delta E$, yielding broad exploration; as $T$ decreases, the factor $\\exp(-\\Delta E/T)$ shrinks for positive $\\Delta E$, increasingly favoring improving moves and consolidating into minima. Thus, a high $T_0$ with slow cooling (large $r$ close to $1$) promotes global exploration and the reduction of conflicts by enabling rearrangements that may temporarily worsen variance to relieve overflow, whereas $T_0=0$ recovers pure descent, which can quickly stagnate.\n\nAlgorithm design:\n- Initialization uses a deterministic greedy load-balancing heuristic: sort items in non-increasing order and place each into the bin with the current smallest load, breaking ties by bin index. This yields a reproducible starting $x_0$ independent of random seeds.\n- Iteratively propose a move using one of the two symmetric types, compute $\\Delta E$, and accept with the Metropolis probability. If $T=0$, accept only when $\\Delta E\\le 0$ to avoid division by zero and to implement the correct limiting behavior.\n- Maintain the current state and the best state (lowest energy) seen. Update the temperature by $T\\leftarrow rT$ each iteration.\n\nTest suite design and coverage:\n- Case A uses moderate initial temperature and cooling factor for a typical scenario.\n- Case B sets $T_0=0$ to test the boundary case where simulated annealing reduces to greedy descent with no uphill acceptance; this probes the effect of temperature on escaping local minima.\n- Case C uses a very high $T_0$ and very slow cooling $r$ close to $1$, testing broad exploration and its impact on final energy within the same budget of iterations.\n- Case D uses an instance where total item size exceeds total capacity, forcing overflow. This tests the algorithm’s trade-off between distributing overflow and balancing variance under moderate cooling.\n\nFor each case, the output is the best energy $E^*$ encountered, rounded to $4$ decimal places, printed as a single list in the order $[E^*_A,E^*_B,E^*_C,E^*_D]$. Because the random seed is fixed for each case, results are reproducible.\n\nThe program implements all the above specifications precisely and produces the required single-line output.", "answer": "```python\nimport numpy as np\n\ndef greedy_initial_assignment(sizes, B):\n    # Deterministic greedy: sort items by size desc, place into bin with min load\n    n = len(sizes)\n    indices = sorted(range(n), key=lambda i: (-sizes[i], i))\n    loads = [0.0] * B\n    assignment = [0] * n\n    for i in indices:\n        b = min(range(B), key=lambda k: (loads[k], k))\n        assignment[i] = b\n        loads[b] += sizes[i]\n    return assignment, np.array(loads, dtype=float)\n\ndef energy_from_loads(loads, C, alpha, beta):\n    B = len(loads)\n    u = loads / C\n    overflow = np.maximum(0.0, u - 1.0).sum()\n    mean_u = u.mean() if B > 0 else 0.0\n    var_u = ((u - mean_u) ** 2).mean() if B > 0 else 0.0\n    return alpha * overflow + beta * var_u\n\ndef propose_move(rng, assignment, loads, sizes, B):\n    n = len(assignment)\n    move_type = rng.integers(0, 2)  # 0: single-item move, 1: two-item swap\n    new_assignment = assignment.copy()\n    new_loads = loads.copy()\n    if move_type == 0:\n        # Single-item reassignment\n        i = int(rng.integers(0, n))\n        current_bin = new_assignment[i]\n        # Choose a different bin\n        if B > 1:\n            dest_bin = int(rng.integers(0, B - 1))\n            if dest_bin >= current_bin:\n                dest_bin += 1\n        else:\n            dest_bin = current_bin\n        if dest_bin != current_bin:\n            size_i = sizes[i]\n            new_assignment[i] = dest_bin\n            new_loads[current_bin] -= size_i\n            new_loads[dest_bin] += size_i\n        # else, identity move; return unchanged (will lead to deltaE = 0)\n    else:\n        # Two-item swap\n        if n >= 2:\n            i = int(rng.integers(0, n))\n            j = int(rng.integers(0, n - 1))\n            if j >= i:\n                j += 1\n            bi, bj = new_assignment[i], new_assignment[j]\n            if bi != bj:\n                # swap assignments\n                new_assignment[i], new_assignment[j] = bj, bi\n                si, sj = sizes[i], sizes[j]\n                # update loads\n                new_loads[bi] -= si\n                new_loads[bj] += si\n                new_loads[bj] -= sj\n                new_loads[bi] += sj\n            # else, identity (no change)\n    return new_assignment, new_loads\n\ndef simulated_annealing(sizes, B, C, alpha, beta, T0, r, iterations, seed):\n    rng = np.random.default_rng(seed)\n    assignment, loads = greedy_initial_assignment(sizes, B)\n    current_E = energy_from_loads(loads, C, alpha, beta)\n    best_E = current_E\n    best_assignment = assignment.copy()\n    best_loads = loads.copy()\n    T = float(T0)\n\n    for k in range(iterations):\n        cand_assignment, cand_loads = propose_move(rng, assignment, loads, sizes, B)\n        cand_E = energy_from_loads(cand_loads, C, alpha, beta)\n        deltaE = cand_E - current_E\n\n        accept = False\n        if deltaE = 0.0:\n            accept = True\n        else:\n            if T = 1e-12:\n                accept = False\n            else:\n                # Metropolis acceptance\n                # Clip exponent to avoid overflow in extreme cases\n                exponent = -deltaE / T\n                # numpy exp is stable for moderate exponents; large negative exponents underflow to 0 naturally\n                p = np.exp(exponent) if exponent > -745 else 0.0\n                u = rng.random()\n                accept = (u  p)\n\n        if accept:\n            assignment = cand_assignment\n            loads = cand_loads\n            current_E = cand_E\n            if current_E  best_E:\n                best_E = current_E\n                best_assignment = assignment.copy()\n                best_loads = loads.copy()\n\n        # geometric cooling\n        T *= r\n\n    return best_E, best_assignment, best_loads\n\ndef solve():\n    # Common energy parameters\n    alpha = 20.0\n    beta = 1.0\n\n    test_cases = [\n        # Case A\n        {\n            \"sizes\": [7, 5, 6, 2, 3, 4, 8, 1, 9, 2, 6, 5],\n            \"B\": 4,\n            \"C\": 10,\n            \"T0\": 5.0,\n            \"r\": 0.997,\n            \"iterations\": 4000,\n            \"seed\": 11\n        },\n        # Case B\n        {\n            \"sizes\": [7, 5, 6, 2, 3, 4, 8, 1, 9, 2, 6, 5],\n            \"B\": 4,\n            \"C\": 10,\n            \"T0\": 0.0,\n            \"r\": 0.500,\n            \"iterations\": 4000,\n            \"seed\": 11\n        },\n        # Case C\n        {\n            \"sizes\": [7, 5, 6, 2, 3, 4, 8, 1, 9, 2, 6, 5],\n            \"B\": 4,\n            \"C\": 10,\n            \"T0\": 20.0,\n            \"r\": 0.999,\n            \"iterations\": 4000,\n            \"seed\": 11\n        },\n        # Case D\n        {\n            \"sizes\": [9, 9, 8, 7, 6, 6, 5, 5, 4, 4],\n            \"B\": 4,\n            \"C\": 10,\n            \"T0\": 10.0,\n            \"r\": 0.998,\n            \"iterations\": 5000,\n            \"seed\": 99\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        sizes = case[\"sizes\"]\n        B = case[\"B\"]\n        C = case[\"C\"]\n        T0 = case[\"T0\"]\n        r = case[\"r\"]\n        iterations = case[\"iterations\"]\n        seed = case[\"seed\"]\n\n        best_E, _, _ = simulated_annealing(\n            np.array(sizes, dtype=float), B, C, alpha, beta, T0, r, iterations, seed\n        )\n        results.append(best_E)\n\n    # Format results to 4 decimal places\n    formatted = \"[\" + \",\".join(f\"{x:.4f}\" for x in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3182664"}]}