{"hands_on_practices": [{"introduction": "To build a strong foundation, it's essential to trace the step-by-step behavior of core search algorithms. This first exercise allows you to do just that by comparing two fundamental strategies: Depth-First Search (DFS) and Best-First Search. By simulating their progress on small, concrete search trees and using a formal \"regret\" metric, you will gain a tangible understanding of how their different philosophies for exploring nodes can lead to vastly different performance outcomes [@problem_id:3157396].", "problem": "Consider a deterministic tree search setting in which a search policy selects one frontier node to expand at each step. A node expansion reveals its children and their associated evaluation values, and incurs a unit cost of $1$. Leaves carry an objective value $\\mathrm{obj}(\\ell)$ that is the true quality of the solution at leaf $\\ell$. The optimal leaf is the leaf with minimal objective value, denoted by $v^{\\star} = \\min_{\\ell} \\mathrm{obj}(\\ell)$. The search terminates at the first step when the optimal leaf is expanded. For any policy $\\pi$ and tree $T$, define the expansion count $E_{\\pi}(T)$ as the total number of node expansions performed until the optimal leaf is expanded. Define the oracle policy as the ideal policy that knows the identity of the optimal leaf in advance and expands only the nodes along the unique path from the root to that leaf, so $E_{\\mathrm{oracle}}(T)$ equals the number of nodes on that path. The regret of policy $\\pi$ on tree $T$ is\n$$\nR_{\\pi}(T) = E_{\\pi}(T) - E_{\\mathrm{oracle}}(T).\n$$\nWe will compare two node selection policies: depth-first selection and best-first selection. Depth-first selection uses a Last-In First-Out (LIFO) stack discipline with a fixed child generation order at every internal node. Best-first selection chooses the frontier node with the smallest heuristic $h(n)$; ties are broken by earlier generation time (first generated, first selected). For leaves $\\ell$, set $h(\\ell) = \\mathrm{obj}(\\ell)$.\n\nTwo benchmark trees are given.\n\nTree $T_1$: The root $r_1$ has two children generated in the order $a$ then $b$. Heuristic values are $h(a) = 5$ and $h(b) = 6$. Node $a$ has two leaf children $a_1$ and $a_2$ with $\\mathrm{obj}(a_1) = 5$ and $\\mathrm{obj}(a_2) = 9$; thus $h(a_1) = 5$ and $h(a_2) = 9$. Node $b$ has two leaf children $b_1$ and $b_2$ with $\\mathrm{obj}(b_1) = 4$ and $\\mathrm{obj}(b_2) = 10$; thus $h(b_1) = 4$ and $h(b_2) = 10$. The optimal leaf in $T_1$ is $b_1$ with objective $4$.\n\nTree $T_2$: The root $r_2$ has two children generated in the order $d$ then $c$. Heuristic values are $h(d) = 2.0$ and $h(c) = 1.6$. Node $d$ has a single leaf child $d_1$ with $\\mathrm{obj}(d_1) = 2.0$, so $h(d_1) = 2.0$. Node $c$ has one internal child $c_A$ with $h(c_A) = 1.5$ and one leaf child $c_B$ with $\\mathrm{obj}(c_B) = 5$, so $h(c_B) = 5$. Node $c_A$ has two leaf children $c_{A1}$ and $c_{A2}$ with $\\mathrm{obj}(c_{A1}) = 3.5$ and $\\mathrm{obj}(c_{A2}) = 4.1$, so $h(c_{A1}) = 3.5$ and $h(c_{A2}) = 4.1$. The optimal leaf in $T_2$ is $d_1$ with objective $2.0$.\n\nUsing only the above definitions and data, compute the ratio\n$$\n\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)},\n$$\nwhere $\\mathrm{DFS}$ denotes the depth-first selection policy and $\\mathrm{BF}$ denotes the best-first selection policy. Express your final answer as an exact fraction. No rounding is required. No units are required.", "solution": "The objective is to compute the ratio $\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)}$. The regret of a policy $\\pi$ on a tree $T$ is defined as $R_{\\pi}(T) = E_{\\pi}(T) - E_{\\mathrm{oracle}}(T)$, where $E_{\\pi}(T)$ is the total number of node expansions.\n\nA \"node expansion\" is an action that incurs a cost of $1$. The search terminates when the optimal leaf is expanded. The problem states that the oracle policy's expansion count, $E_{\\mathrm{oracle}}(T)$, is equal to the number of nodes on the path from the root to the optimal leaf. This implies that leaves are \"expanded\" in the sense that they are selected from the frontier, which incurs a cost of $1$ and ends the search if the leaf is optimal. We will apply this interpretation consistently.\n\nFirst, we determine the expansion count for the oracle policy, $E_{\\mathrm{oracle}}(T)$, for each tree.\nFor tree $T_1$, the optimal leaf is $b_1$. The unique path from the root $r_1$ to $b_1$ is $r_1 \\to b \\to b_1$. This path contains $3$ nodes. Therefore, $E_{\\mathrm{oracle}}(T_1) = 3$. The expanded nodes are $r_1$, $b$, and $b_1$.\nFor tree $T_2$, the optimal leaf is $d_1$. The unique path from the root $r_2$ to $d_1$ is $r_2 \\to d \\to d_1$. This path contains $3$ nodes. Therefore, $E_{\\mathrm{oracle}}(T_2) = 3$. The expanded nodes are $r_2$, $d$, and $d_1$.\n\nNext, we calculate the expansion count $E_{\\pi}(T)$ for the Depth-First Search ($\\mathrm{DFS}$) and Best-First Search ($\\mathrm{BF}$) policies on each tree.\n\n**Analysis of Tree $T_1$**\nThe optimal leaf is $b_1$ with $\\mathrm{obj}(b_1) = 4$.\n\n**1. DFS on $T_1$:**\n$\\mathrm{DFS}$ uses a LIFO stack. The child generation order at $r_1$ is $a$ then $b$. We assume a fixed, consistent order for other nodes' children (e.g., $a_1$ then $a_2$ for $a$, and $b_1$ then $b_2$ for $b$).\n- Step 1: Expand $r_1$. Expansions: {$r_1$}. Cost: $1$. Children $a, b$ are generated. Stack (top first): [$a, b$].\n- Step 2: Expand $a$. Expansions: {$r_1, a$}. Cost: $2$. Children $a_1, a_2$ are generated. Stack: [$a_1, a_2, b$].\n- Step 3: Expand $a_1$. Expansions: {$r_1, a, a_1$}. Cost: $3$. This is a leaf, but it is not optimal ($\\mathrm{obj}(a_1)=5 \\neq 4$). Stack: [$a_2, b$].\n- Step 4: Expand $a_2$. Expansions: {$r_1, a, a_1, a_2$}. Cost: $4$. This is a leaf, but it is not optimal ($\\mathrm{obj}(a_2)=9 \\neq 4$). Stack: [$b$].\n- Step 5: Expand $b$. Expansions: {$r_1, a, a_1, a_2, b$}. Cost: $5$. Children $b_1, b_2$ are generated. Stack: [$b_1, b_2$].\n- Step 6: Expand $b_1$. Expansions: {$r_1, a, a_1, a_2, b, b_1$}. Cost: $6$. This is the optimal leaf. The search terminates.\nThe total number of expansions is $E_{\\mathrm{DFS}}(T_1) = 6$.\nThe regret is $R_{\\mathrm{DFS}}(T_1) = E_{\\mathrm{DFS}}(T_1) - E_{\\mathrm{oracle}}(T_1) = 6 - 3 = 3$.\n\n**2. BF on $T_1$:**\n$\\mathrm{BF}$ uses a priority queue ordered by the heuristic $h(n)$. Ties are broken by earlier generation time.\n- Step 1: Expand $r_1$. Expansions: {$r_1$}. Cost: $1$. Frontier (priority queue): {($a, h=5$), ($b, h=6$)}. Node $a$ has higher priority.\n- Step 2: Expand $a$ (since $h(a)=5 < h(b)=6$). Expansions: {$r_1, a$}. Cost: $2$. Children $a_1, a_2$ are added to the frontier. Frontier: {($a_1, h=5$), ($b, h=6$), ($a_2, h=9$)}. Node $a_1$ has the lowest $h$-value.\n- Step 3: Expand $a_1$. Expansions: {$r_1, a, a_1$}. Cost: $3$. This leaf is not optimal. Frontier: {($b, h=6$), ($a_2, h=9$)}. Node $b$ has higher priority.\n- Step 4: Expand $b$. Expansions: {$r_1, a, a_1, b$}. Cost: $4$. Children $b_1, b_2$ are added. Frontier: {($b_1, h=4$), ($a_2, h=9$), ($b_2, h=10$)}. Node $b_1$ now has the lowest $h$-value.\n- Step 5: Expand $b_1$. Expansions: {$r_1, a, a_1, b, b_1$}. Cost: $5$. This is the optimal leaf. The search terminates.\nThe total number of expansions is $E_{\\mathrm{BF}}(T_1) = 5$.\nThe regret is $R_{\\mathrm{BF}}(T_1) = E_{\\mathrm{BF}}(T_1) - E_{\\mathrm{oracle}}(T_1) = 5 - 3 = 2$.\n\n**Analysis of Tree $T_2$**\nThe optimal leaf is $d_1$ with $\\mathrm{obj}(d_1) = 2.0$.\n\n**1. DFS on $T_2$:**\nThe child generation order at $r_2$ is $d$ then $c$.\n- Step 1: Expand $r_2$. Expansions: {$r_2$}. Cost: $1$. Children $d, c$ are generated. Stack: [$d, c$].\n- Step 2: Expand $d$. Expansions: {$r_2, d$}. Cost: $2$. Child $d_1$ is generated. Stack: [$d_1, c$].\n- Step 3: Expand $d_1$. Expansions: {$r_2, d, d_1$}. Cost: $3$. This is the optimal leaf. The search terminates.\nThe total number of expansions is $E_{\\mathrm{DFS}}(T_2) = 3$.\nThe regret is $R_{\\mathrm{DFS}}(T_2) = E_{\\mathrm{DFS}}(T_2) - E_{\\mathrm{oracle}}(T_2) = 3 - 3 = 0$.\n\n**2. BF on $T_2$:**\n- Step 1: Expand $r_2$. Expansions: {$r_2$}. Cost: $1$. Frontier: {($c, h=1.6$), ($d, h=2.0$)}. Node $c$ has higher priority.\n- Step 2: Expand $c$ (since $h(c)=1.6 < h(d)=2.0$). Expansions: {$r_2, c$}. Cost: $2$. Children $c_A, c_B$ are added. Frontier: {($c_A, h=1.5$), ($d, h=2.0$), ($c_B, h=5.0$)}. Node $c_A$ has the lowest $h$-value.\n- Step 3: Expand $c_A$. Expansions: {$r_2, c, c_A$}. Cost: $3$. Children $c_{A1}, c_{A2}$ are added. Frontier: {($d, h=2.0$), ($c_{A1}, h=3.5$), ($c_{A2}, h=4.1$), ($c_B, h=5.0$)}. Node $d$ now has the lowest $h$-value.\n- Step 4: Expand $d$. Expansions: {$r_2, c, c_A, d$}. Cost: $4$. Child $d_1$ is added. Frontier: {($d_1, h=2.0$), ($c_{A1}, h=3.5$), ($c_{A2}, h=4.1$), ($c_B, h=5.0$)}. Although $d$ and $d_1$ have the same heuristic value ($h=2.0$), $d$ was a frontier node from the expansion of the root, while $d_1$ is newly added. After $d$ is expanded and removed, $d_1$ is the new frontier node with the lowest $h$-value.\n- Step 5: Expand $d_1$. Expansions: {$r_2, c, c_A, d, d_1$}. Cost: $5$. This is the optimal leaf. The search terminates.\nThe total number of expansions is $E_{\\mathrm{BF}}(T_2) = 5$.\nThe regret is $R_{\\mathrm{BF}}(T_2) = E_{\\mathrm{BF}}(T_2) - E_{\\mathrm{oracle}}(T_2) = 5 - 3 = 2$.\n\n**Final Calculation**\nWe have the following regret values:\n$R_{\\mathrm{DFS}}(T_1) = 3$\n$R_{\\mathrm{DFS}}(T_2) = 0$\n$R_{\\mathrm{BF}}(T_1) = 2$\n$R_{\\mathrm{BF}}(T_2) = 2$\n\nNow we compute the ratio $\\rho$:\n$$\n\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)} = \\frac{3 + 0}{2 + 2} = \\frac{3}{4}\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3157396"}, {"introduction": "While Best-First Search intuitively seems superior due to its use of heuristic guidance, it's crucial to understand its potential weaknesses. This practice moves beyond specific examples to an analytical exploration of a worst-case scenario where the heuristic is completely uninformative. By deriving a closed-form expression for the performance gap, you will see how a \"smarter\" strategy can perform exponentially worse than a simple one, highlighting the critical dependence of greedy algorithms on the quality of their guiding information [@problem_id:3157470].", "problem": "Consider a minimization problem over binary decision variables with $n \\in \\mathbb{N}$, where each complete assignment $x \\in \\{0,1\\}^{n}$ has objective value given by the function $f(x)$ defined as follows: $f(x) = 0$ if $x = \\mathbf{0}$ and $f(x) = M$ otherwise, with $M > 0$. A Branch and Bound (B&B) algorithm is used with the standard pruning rule for minimization: a node $u$ is pruned if its lower bound $L(u)$ satisfies $L(u) \\geq z^{*}$, where $z^{*}$ is the incumbent (best known) objective value. The branching scheme assigns variables in index order $i = 1,2,\\dots,n$, with the left child fixing $x_{i} = 0$ and the right child fixing $x_{i} = 1$. The lower bound function is the trivial bound $L(u) = 0$ for every node $u$, which is valid because the objective values are nonnegative.\n\nTwo node selection strategies are considered:\n- Depth-First Search (DFS): always select the deepest node available, breaking ties by choosing the left child before the right child.\n- Best-First Search (BestFS): always select the node with the smallest lower bound $L(u)$, breaking ties by preferring shallower depth first and then lexicographic order on the partial assignments.\n\nAssume that the search tree is a full binary tree with root at depth $0$ and leaves at depth $n$. Define a node expansion as selecting a node for processing (either branching it into its children if it is not a leaf, or evaluating $f(x)$ if it is a leaf). In this constructed instance, the unique optimal leaf $x = \\mathbf{0}$ lies at the leftmost leaf along the path of always choosing the left child.\n\nDerive, from the core definitions of Branch and Bound and the specified node selection rules, the exact number of node expansions performed by DFS and by BestFS until termination. Then, compute the ratio of expansions, defined as the number of expansions under BestFS divided by the number of expansions under DFS, as a closed-form analytic expression in $n$. Express your final answer as a single closed-form expression in terms of $n$.", "solution": "The goal is to determine the number of node expansions for two different node selection strategies, Depth-First Search (DFS) and Best-First Search (BestFS), and then compute the ratio of these numbers. An expansion is defined as selecting a node for processing (branching for an internal node, or evaluating the objective function for a leaf node). A node is pruned if its lower bound $L(u)$ is greater than or equal to the incumbent value $z^*$. A pruned node is not expanded.\n\nThe B&B algorithm is initialized with the root node in the list of open nodes and an incumbent value of $z^* = \\infty$. The lower bound for any node $u$ in the search tree is given as $L(u) = 0$.\n\n**1. Analysis of Depth-First Search (DFS)**\n\nThe DFS strategy selects the deepest available node, breaking ties by choosing the left child (corresponding to fixing a variable to $0$) before the right child.\n\n1.  The search begins at the root node (depth $0$). It is expanded, creating two children at depth $1$: the left child for $x_1=0$ and the right child for $x_1=1$. This is $1$ expansion.\n2.  According to the DFS rule, the deeper node is selected. Since both children are at the same depth, the tie-breaker rule applies, and the left child ($x_1=0$) is selected for expansion. This is the $2$-nd expansion.\n3.  This process continues. The algorithm always proceeds down the leftmost path of the tree because at each step, it generates a new, deeper level, and the left child at that new level is immediately selected. This path corresponds to setting $x_i=0$ for $i=1, 2, \\dots, n$.\n4.  The algorithm expands the $n$ internal nodes along this leftmost path: the root (depth $0$), the node for $(x_1=0)$ (depth $1$), ..., and finally the node for $(x_1=0, \\dots, x_{n-1}=0)$ (depth $n-1$). This constitutes $n$ expansions.\n5.  After expanding the node at depth $n-1$, the next node to be selected is its left child, which is the leaf node at depth $n$ corresponding to the complete assignment $x = \\mathbf{0} = (0, 0, \\dots, 0)$. This leaf is expanded. This is the $(n+1)$-th expansion.\n6.  Upon expanding this leaf, the objective function is evaluated: $f(\\mathbf{0}) = 0$. Since this value is less than the current incumbent $z^* = \\infty$, the incumbent is updated to $z^* = 0$.\n7.  At this point, the pruning rule is to prune any node $u$ if $L(u) \\geq z^*=0$. Since the lower bound for every node is given as $L(u) = 0$, the condition $0 \\geq 0$ is always met.\n8.  Therefore, any subsequent node selected from the open list will be immediately pruned. According to the problem's definition, pruned nodes are not expanded. No further expansions occur.\n\nThe total number of node expansions for DFS, denoted $N_{DFS}$, is the sum of the $n$ internal nodes on the leftmost path and the one leftmost leaf.\n$$N_{DFS} = n + 1$$\n\n**2. Analysis of Best-First Search (BestFS)**\n\nThe BestFS strategy selects the node with the smallest lower bound $L(u)$.\n\n1.  In this problem, every node $u$ in the tree has the same lower bound, $L(u) = 0$. Therefore, the tie-breaking rules are always invoked.\n2.  The first tie-breaking rule is to prefer shallower nodes. This forces the algorithm to behave as a Breadth-First Search (BFS), expanding all nodes at a given depth before moving to the next deeper level.\n3.  The algorithm starts by expanding the root node at depth $0$. ($1$ expansion).\n4.  It then expands all $2^1 = 2$ nodes at depth $1$.\n5.  It proceeds to expand all $2^2 = 4$ nodes at depth $2$, and so on.\n6.  This continues until all internal nodes of the tree have been expanded. The internal nodes are those at depths $k = 0, 1, \\dots, n-1$. The total number of such nodes is the sum of a geometric series:\n    $$ \\sum_{k=0}^{n-1} 2^k = \\frac{2^n - 1}{2-1} = 2^n - 1 $$\n    Up to this point, no leaf has been evaluated, so the incumbent remains $z^* = \\infty$.\n7.  After all $2^{n-1}$ nodes at depth $n-1$ are expanded, the open list contains all $2^n$ leaf nodes at depth $n$.\n8.  The BestFS rule is now applied to this list of leaves. All have $L(u)=0$ and all are at the same depth $n$. The final tie-breaker is \"lexicographic order on the partial assignments\".\n9.  The lexicographically first assignment is $x = \\mathbf{0} = (0, 0, \\dots, 0)$. The corresponding leaf node is selected for expansion. This is one additional expansion.\n10. Upon expanding this leaf, the objective value $f(\\mathbf{0}) = 0$ is found, and the incumbent is updated to $z^* = 0$.\n11. As in the DFS case, the pruning condition $L(u) \\geq z^*$ now becomes $0 \\geq 0$, which is always true. All other $2^n - 1$ leaf nodes remaining in the open list will be pruned upon selection and will not be expanded.\n\nThe total number of expansions for BestFS, denoted $N_{BestFS}$, is the sum of all internal node expansions and the single expansion of the optimal leaf.\n$$N_{BestFS} = (2^n - 1) + 1 = 2^n$$\n\n**3. Ratio of Expansions**\n\nThe problem asks for the ratio of the number of expansions under BestFS to the number of expansions under DFS.\n$$ \\text{Ratio} = \\frac{N_{BestFS}}{N_{DFS}} $$\nSubstituting the derived expressions for $N_{BestFS}$ and $N_{DFS}$:\n$$ \\text{Ratio} = \\frac{2^n}{n+1} $$\nThis is the closed-form analytic expression for the ratio in terms of $n$.", "answer": "$$\n\\boxed{\\frac{2^n}{n+1}}\n$$", "id": "3157470"}, {"introduction": "Real-world optimization solvers rarely use pure search strategies; instead, they employ sophisticated hybrid rules to balance competing goals. This final practice provides a window into this design complexity by introducing a penalized Best-First rule that encourages diversification—a key concept for avoiding getting trapped in locally promising but globally suboptimal regions of the search space. By applying this nuanced rule, you will see how practical algorithms are engineered to overcome the limitations of the basic strategies we've explored [@problem_id:3157443].", "problem": "A solver applies branch-and-bound to a Mixed-Integer Linear Programming (MILP) problem and uses a best-first node selection rule. In best-first selection, the next node to expand is the one with the largest Linear Programming (LP) relaxation bound among all open nodes. To encourage diversification, the solver modifies the selection by introducing a penalty when the node’s anticipated next branching variable matches any of the variables used in the most recently expanded nodes.\n\nStart from the following core definitions: in branch-and-bound for MILP, each open node has an LP relaxation objective bound, and best-first selection chooses the node with the largest bound. A diversification mechanism can be incorporated by subtracting a fixed penalty from the selection score if the node’s anticipated next branching variable belongs to a recent history of branching variables used at previously expanded nodes.\n\nAt a certain stage of the search on a small MILP with binary variables $x_1, x_2, x_3$, the frontier consists of $4$ open nodes indexed by $i \\in \\{1,2,3,4\\}$. For each node $i$, denote its LP relaxation upper bound by $U_i$ and its anticipated next branching variable (for example, the most fractional variable at that node) by $v_i \\in \\{x_1,x_2,x_3\\}$. The solver maintains a history set $H$ of the last $L=2$ branching variables used in the two most recently expanded nodes; here $H=\\{x_1,x_2\\}$. The penalized best-first score used for node selection is\n$$\nS_i \\;=\\; \\begin{cases}\nU_i - \\lambda, & \\text{if } v_i \\in H,\\\\\nU_i, & \\text{if } v_i \\notin H,\n\\end{cases}\n$$\nwith penalty parameter $\\lambda>0$.\n\nFor the current frontier, the data are:\n- $U_1 = 10.2$, $v_1 = x_1$.\n- $U_2 = 10.2$, $v_2 = x_2$.\n- $U_3 = 10.1$, $v_3 = x_1$.\n- $U_4 = 10.0$, $v_4 = x_3$.\n\nLet $\\lambda = 0.25$ and $H=\\{x_1,x_2\\}$. The solver selects the node with the largest $S_i$; in the event of a tie, it selects the node with the smallest index.\n\nCompute the index $i^\\star \\in \\{1,2,3,4\\}$ of the node that will be selected next. Give your answer as a single integer. No rounding is required.", "solution": "The problem requires us to determine which node will be selected next in a branch-and-bound search for a Mixed-Integer Linear Programming (MILP) problem. The selection is based on a penalized best-first rule.\n\nThe core of the problem lies in computing a score, $S_i$, for each of the $i \\in \\{1, 2, 3, 4\\}$ open nodes on the search frontier. The node with the highest score is selected. The problem specifies a tie-breaking rule: if multiple nodes share the highest score, the one with the smallest index is chosen.\n\nThe score $S_i$ for node $i$ is defined by the following piecewise function:\n$$\nS_i \\;=\\; \\begin{cases}\nU_i - \\lambda, & \\text{if } v_i \\in H,\\\\\nU_i, & \\text{if } v_i \\notin H,\n\\end{cases}\n$$\nwhere $U_i$ is the Linear Programming (LP) relaxation upper bound of node $i$, $v_i$ is its anticipated next branching variable, $H$ is the history set of recently used branching variables, and $\\lambda$ is a positive penalty parameter.\n\nThe given data are:\n- The set of open nodes is indexed by $i \\in \\{1, 2, 3, 4\\}$.\n- The LP relaxation upper bounds are $U_1 = 10.2$, $U_2 = 10.2$, $U_3 = 10.1$, and $U_4 = 10.0$.\n- The anticipated next branching variables are $v_1 = x_1$, $v_2 = x_2$, $v_3 = x_1$, and $v_4 = x_3$.\n- The history set of the last $L=2$ branching variables is $H = \\{x_1, x_2\\}$.\n- The penalty parameter is $\\lambda = 0.25$.\n\nWe will now compute the score $S_i$ for each of the four nodes.\n\nFor node $i=1$:\nThe anticipated branching variable is $v_1 = x_1$. We check if $v_1$ is in the history set $H = \\{x_1, x_2\\}$. Since $x_1 \\in H$, the score is penalized.\n$$S_1 = U_1 - \\lambda = 10.2 - 0.25 = 9.95$$\n\nFor node $i=2$:\nThe anticipated branching variable is $v_2 = x_2$. We check if $v_2$ is in the history set $H = \\{x_1, x_2\\}$. Since $x_2 \\in H$, the score is penalized.\n$$S_2 = U_2 - \\lambda = 10.2 - 0.25 = 9.95$$\n\nFor node $i=3$:\nThe anticipated branching variable is $v_3 = x_1$. We check if $v_3$ is in the history set $H = \\{x_1, x_2\\}$. Since $x_1 \\in H$, the score is penalized.\n$$S_3 = U_3 - \\lambda = 10.1 - 0.25 = 9.85$$\n\nFor node $i=4$:\nThe anticipated branching variable is $v_4 = x_3$. We check if $v_4$ is in the history set $H = \\{x_1, x_2\\}$. Since $x_3 \\notin H$, the score is not penalized.\n$$S_4 = U_4 = 10.0$$\n\nNow, we collect the computed scores for all nodes:\n- $S_1 = 9.95$\n- $S_2 = 9.95$\n- $S_3 = 9.85$\n- $S_4 = 10.0$\n\nThe selection rule is to choose the node with the largest score $S_i$. We compare the scores:\n$$\\max(S_1, S_2, S_3, S_4) = \\max(9.95, 9.95, 9.85, 10.0) = 10.0$$\nThe maximum score is $10.0$, which corresponds exclusively to node $4$. Since there is no tie for the maximum score, the tie-breaking rule is not needed.\n\nTherefore, the solver will select the node with index $i^\\star = 4$.", "answer": "$$\n\\boxed{4}\n$$", "id": "3157443"}]}