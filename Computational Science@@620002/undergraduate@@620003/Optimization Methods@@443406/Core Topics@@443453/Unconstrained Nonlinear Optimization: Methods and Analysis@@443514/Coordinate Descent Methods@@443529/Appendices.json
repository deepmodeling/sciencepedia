{"hands_on_practices": [{"introduction": "To begin, we will apply the core mechanism of coordinate descent to a straightforward problem. This first exercise walks you through a single, complete cycle of the algorithm on a smooth, convex quadratic function. Mastering this fundamental procedure of minimizing the function one coordinate at a time is the essential first step to understanding how coordinate descent operates in more complex scenarios. [@problem_id:2164480]", "problem": "Consider the function of two variables $f(x,y) = 2x^2 + y^2 + xy - 6x - 5y$. An optimization routine is employed to find the minimum of this function using the coordinate descent algorithm. The process starts from the initial point $(x_0, y_0) = (0, 0)$.\n\nYou are asked to perform exactly one full cycle of coordinate descent. The update order within the cycle is as follows: first, minimize the function with respect to the $x$-coordinate, and then, using the newly found $x$-coordinate, minimize the function with respect to the $y$-coordinate.\n\nDetermine the coordinates $(x_1, y_1)$ of the point after this single cycle. Your answer should consist of the two coordinate values, expressed as exact fractions.", "solution": "We perform one full cycle of coordinate descent on the quadratic function $f(x,y) = 2x^{2} + y^{2} + xy - 6x - 5y$ starting from $(x_{0}, y_{0}) = (0, 0)$, updating first $x$ and then $y$. For a fixed coordinate, minimizing a differentiable function with respect to that coordinate is done by setting the corresponding partial derivative to zero; since $f$ is quadratic and strictly convex in each coordinate (as verified by positive second partial derivatives), this yields the unique minimizer in that coordinate.\n\nFirst, minimize with respect to $x$ holding $y = y_{0} = 0$ fixed. Compute the partial derivative with respect to $x$:\n$$\n\\frac{\\partial f}{\\partial x} = 4x + y - 6.\n$$\nSet it to zero at $y=0$:\n$$\n4x + 0 - 6 = 0 \\quad \\Longrightarrow \\quad 4x = 6 \\quad \\Longrightarrow \\quad x = \\frac{3}{2}.\n$$\nSince $\\frac{\\partial^{2} f}{\\partial x^{2}} = 4 > 0$, this is the unique minimizer in $x$ for fixed $y=0$. Thus, after the $x$-update we have $x_{1} = \\frac{3}{2}$ and $y$ remains $0$.\n\nNext, minimize with respect to $y$ holding $x = x_{1} = \\frac{3}{2}$ fixed. Compute the partial derivative with respect to $y$:\n$$\n\\frac{\\partial f}{\\partial y} = 2y + x - 5.\n$$\nSet it to zero at $x=\\frac{3}{2}$:\n$$\n2y + \\frac{3}{2} - 5 = 0 \\quad \\Longrightarrow \\quad 2y - \\frac{7}{2} = 0 \\quad \\Longrightarrow \\quad 2y = \\frac{7}{2} \\quad \\Longrightarrow \\quad y = \\frac{7}{4}.\n$$\nSince $\\frac{\\partial^{2} f}{\\partial y^{2}} = 2 > 0$, this is the unique minimizer in $y$ for fixed $x=\\frac{3}{2}$. Therefore, after one full cycle, the updated point is $\\left(x_{1}, y_{1}\\right) = \\left(\\frac{3}{2}, \\frac{7}{4}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{2}  \\frac{7}{4}\\end{pmatrix}}$$", "id": "2164480"}, {"introduction": "Having practiced the basic mechanics, we now investigate a more nuanced aspect of the algorithm: the influence of the update order. While coordinate descent will converge to a minimum for a wide class of functions, the specific path it takes is not unique. This exercise directly compares the outcomes of two different update sequences on the same objective function, revealing how the order of optimization can lead to different intermediate points in the iterative process. [@problem_id:2164438]", "problem": "Consider the optimization of a two-variable objective function given by\n$$\nf(x, y) = (x + 2y - c_1)^2 + (3x - y - c_2)^2\n$$\nwhere $c_1$ and $c_2$ are real-valued constants.\n\nAn optimization procedure is initiated from the starting point $(x_0, y_0) = (0, 0)$ using the coordinate descent method. In each step of this method, a single coordinate is updated to the value that minimizes the objective function, while the other coordinate is held fixed at its current value. A \"full cycle\" of the algorithm consists of updating each coordinate exactly once.\n\nWe wish to compare the results after one full cycle for two different update orders:\n1.  Let $P_{xy} = (x_1, y_1)$ be the point obtained after one full cycle with the update order $(x, \\text{then } y)$. This means we first update $x$ (call it $x_1$) holding $y$ at $y_0$, and then update $y$ (call it $y_1$) holding $x$ at its new value $x_1$.\n2.  Let $P_{yx} = (x'_1, y'_1)$ be the point obtained after one full cycle with the update order $(y, \\text{then } x)$. This means we first update $y$ (call it $y'_1$) holding $x$ at $x_0$, and then update $x$ (call it $x'_1$) holding $y$ at its new value $y'_1$.\n\nDetermine the four coordinates $x_1, y_1, x'_1$, and $y'_1$ as analytic expressions in terms of the constants $c_1$ and $c_2$. Present your answer as a single $1 \\times 4$ row matrix containing the coordinates in the order $(x_1, y_1, x'_1, y'_1)$.", "solution": "We minimize $f(x,y)=(x+2y-c_{1})^{2}+(3x-y-c_{2})^{2}$ coordinate-wise. For fixed $y$, the minimizer in $x$ is obtained by setting $\\frac{\\partial f}{\\partial x}=0$:\n$$\n\\frac{\\partial f}{\\partial x}=2(x+2y-c_{1})+6(3x-y-c_{2})=20x-2y-2c_{1}-6c_{2}.\n$$\nSet to zero and solve for $x$ to get\n$$\n20x-2y-2c_{1}-6c_{2}=0 \\;\\;\\Rightarrow\\;\\; x^{*}(y)=\\frac{y+c_{1}+3c_{2}}{10}.\n$$\nFor fixed $x$, the minimizer in $y$ is obtained by setting $\\frac{\\partial f}{\\partial y}=0$:\n$$\n\\frac{\\partial f}{\\partial y}=4(x+2y-c_{1})-2(3x-y-c_{2})=-2x+10y-4c_{1}+2c_{2}.\n$$\nSet to zero and solve for $y$ to get\n$$\n-2x+10y-4c_{1}+2c_{2}=0 \\;\\;\\Rightarrow\\;\\; y^{*}(x)=\\frac{x+2c_{1}-c_{2}}{5}.\n$$\n\nOrder $(x,\\text{ then }y)$ from $(x_{0},y_{0})=(0,0)$:\n- Update $x$ with $y=y_{0}=0$: \n$$\nx_{1}=x^{*}(0)=\\frac{0+c_{1}+3c_{2}}{10}=\\frac{c_{1}+3c_{2}}{10}.\n$$\n- Update $y$ with $x=x_{1}$:\n$$\ny_{1}=y^{*}(x_{1})=\\frac{x_{1}+2c_{1}-c_{2}}{5}=\\frac{\\frac{c_{1}+3c_{2}}{10}+2c_{1}-c_{2}}{5}=\\frac{21c_{1}-7c_{2}}{50}.\n$$\n\nOrder $(y,\\text{ then }x)$ from $(x_{0},y_{0})=(0,0)$:\n- Update $y$ with $x=x_{0}=0$:\n$$\ny'_{1}=y^{*}(0)=\\frac{0+2c_{1}-c_{2}}{5}=\\frac{2c_{1}-c_{2}}{5}.\n$$\n- Update $x$ with $y=y'_{1}$:\n$$\nx'_{1}=x^{*}(y'_{1})=\\frac{y'_{1}+c_{1}+3c_{2}}{10}=\\frac{\\frac{2c_{1}-c_{2}}{5}+c_{1}+3c_{2}}{10}=\\frac{7c_{1}+14c_{2}}{50}.\n$$\n\nTherefore, the requested row matrix in the order $(x_{1},y_{1},x'_{1},y'_{1})$ is\n$$\n\\begin{pmatrix}\n\\frac{c_{1}+3c_{2}}{10}  \\frac{21c_{1}-7c_{2}}{50}  \\frac{7c_{1}+14c_{2}}{50}  \\frac{2c_{1}-c_{2}}{5}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{c_{1}+3c_{2}}{10}  \\frac{21c_{1}-7c_{2}}{50}  \\frac{7c_{1}+14c_{2}}{50}  \\frac{2c_{1}-c_{2}}{5}\\end{pmatrix}}$$", "id": "2164438"}, {"introduction": "One of the significant advantages of coordinate descent is its effectiveness on functions that are not differentiable everywhere. This final practice demonstrates the algorithm's robustness by applying it to a non-smooth objective function involving an absolute value, which creates a \"sharp edge\" in the optimization landscape. By analyzing the algorithm's path from different starting points, you will develop a stronger geometric intuition for how coordinate descent navigates towards a minimum even in the absence of a well-defined gradient. [@problem_id:2164429]", "problem": "Consider a function of two variables $f: \\mathbb{R}^2 \\to \\mathbb{R}$ given by $f(x, y) = |x - y|$. We will analyze the behavior of the coordinate descent algorithm on this function. The algorithm starts at an initial point $(x_0, y_0)$ and generates a sequence of points that iteratively minimize the function.\n\nThe coordinate descent algorithm proceeds as follows: for each iteration $k=0, 1, 2, \\dots$, starting from a point $(x_k, y_k)$, it generates the next point $(x_{k+1}, y_{k+1})$ in two steps:\n1.  First, update the $x$-coordinate by solving the one-dimensional minimization problem:\n    $x_{new} = \\arg\\min_{z \\in \\mathbb{R}} f(z, y_k)$.\n2.  Then, update the $y$-coordinate using the new $x$-coordinate:\n    $y_{new} = \\arg\\min_{w \\in \\mathbb{R}} f(x_{new}, w)$.\nThe point for the next iteration is then $(x_{k+1}, y_{k+1}) = (x_{new}, y_{new})$.\n\nLet the sequence of points generated by every single coordinate update be $P_0, P_{0.5}, P_1, P_{1.5}, \\dots$, where $P_0=(x_0, y_0)$, $P_{0.5}=(x_1, y_0)$, $P_1=(x_1, y_1)$, etc. The total path length is the sum of the Euclidean lengths of the segments connecting consecutive points in this sequence until the algorithm converges (i.e., the points no longer change).\n\nConsider two scenarios based on the starting point:\n- **Scenario A**: The starting point is $(x_0, y_0) = (c, c)$ for some constant $c > 0$. Let the total path length be $L_A$.\n- **Scenario B**: The starting point is $(x_0, y_0) = (c, -c)$ for the same constant $c > 0$. Let the total path length be $L_B$.\n\nCalculate the value of the ratio $\\frac{L_B}{c}$.", "solution": "We analyze coordinate descent for $f(x,y)=|x-y|$. For a fixed $y$, define $g(z)=|z-y|$. The function $g$ is minimized when $z=y$, since $|z-y|\\geq 0$ with equality if and only if $z=y$. Thus,\n$$\nx_{\\text{new}}=\\arg\\min_{z\\in\\mathbb{R}}|z-y_{k}|=y_{k}.\n$$\nSimilarly, for a fixed $x$, define $h(w)=|x-w|$, minimized uniquely at $w=x$, so\n$$\ny_{\\text{new}}=\\arg\\min_{w\\in\\mathbb{R}}|x_{\\text{new}}-w|=x_{\\text{new}}.\n$$\nTherefore, each full iteration maps $(x_{k},y_{k})$ to $(x_{k+1},y_{k+1})=(y_{k},y_{k})$.\n\nScenario A: Start at $(x_{0},y_{0})=(c,c)$ with $c0$. The first $x$-update gives $x_{\\text{new}}=y_{0}=c$, so $P_{0.5}=(c,c)=P_{0}$. The $y$-update gives $y_{\\text{new}}=x_{\\text{new}}=c$, so $P_{1}=(c,c)=P_{0.5}$. No movement occurs at any step, hence the total path length is $L_{A}=0$.\n\nScenario B: Start at $(x_{0},y_{0})=(c,-c)$ with $c0$. The first $x$-update gives $x_{\\text{new}}=y_{0}=-c$, so\n$$\nP_{0.5}=(-c,-c).\n$$\nThe Euclidean length of the segment from $P_{0}$ to $P_{0.5}$ is\n$$\n\\sqrt{\\left((-c)-c\\right)^{2}+\\left((-c)-(-c)\\right)^{2}}=\\sqrt{(-2c)^{2}}=2c.\n$$\nThe $y$-update then gives $y_{\\text{new}}=x_{\\text{new}}=-c$, so $P_{1}=(-c,-c)=P_{0.5}$ and contributes zero additional length. Since $x=y$ at $(-c,-c)$, all subsequent updates keep the point unchanged. Therefore, the total path length is $L_{B}=2c$.\n\nHence,\n$$\n\\frac{L_{B}}{c}=\\frac{2c}{c}=2.\n$$", "answer": "$$\\boxed{2}$$", "id": "2164429"}]}