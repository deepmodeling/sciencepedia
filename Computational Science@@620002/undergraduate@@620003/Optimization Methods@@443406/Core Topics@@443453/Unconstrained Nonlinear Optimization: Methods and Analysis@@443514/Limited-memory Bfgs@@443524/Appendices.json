{"hands_on_practices": [{"introduction": "The efficiency of L-BFGS comes from its ability to approximate the inverse Hessian using only a few recent steps. This practice focuses on the fundamental data the algorithm stores: the displacement vectors $\\mathbf{s}_k$ and the corresponding gradient difference vectors $\\mathbf{y}_k$. By calculating these vectors yourself, you will gain a concrete understanding of how L-BFGS captures curvature information from the optimizer's trajectory. [@problem_id:2184596]", "problem": "You are analyzing the behavior of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm, a popular quasi-Newton method for unconstrained optimization. The algorithm builds an approximation of the inverse Hessian matrix by storing the $m$ most recent pairs of vectors $(\\mathbf{s}_k, \\mathbf{y}_k)$. Here, $\\mathbf{x}_k$ is the iterate at step $k$, $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ is the displacement vector, and $\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)$ is the change in the gradient vector for some objective function $f(\\mathbf{x})$.\n\nConsider the optimization of the two-dimensional convex quadratic function $f(\\mathbf{x}) = f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. An optimization routine has produced the following sequence of three iterates (position vectors):\n$$\n\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad \\mathbf{x}_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix}\n$$\nCalculate the two pairs of history vectors, $(\\mathbf{s}_0, \\mathbf{y}_0)$ and $(\\mathbf{s}_1, \\mathbf{y}_1)$, that an L-BFGS algorithm would store based on this sequence of iterates.\n\nExpress your answer as a single $2 \\times 4$ matrix where the columns represent the vectors $\\mathbf{s}_0, \\mathbf{y}_0, \\mathbf{s}_1$, and $\\mathbf{y}_1$ in that specific order. Use fractions for any non-integer values.", "solution": "The goal is to compute the displacement vectors $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and the gradient difference vectors $\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)$ for $k=0$ and $k=1$.\n\nFirst, we need to find the gradient of the objective function $f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 - 2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 3 \\cdot 2(x_2 + 1) = 6(x_2 + 1)\n$$\nSo, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} 2(x_1 - 2) \\\\ 6(x_2 + 1) \\end{pmatrix}\n$$\n\nNext, we evaluate the gradient at each of the given iterates $\\mathbf{x}_0$, $\\mathbf{x}_1$, and $\\mathbf{x}_2$. Let's denote these gradients as $\\mathbf{g}_0$, $\\mathbf{g}_1$, and $\\mathbf{g}_2$.\n\nFor $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$\n\\mathbf{g}_0 = \\nabla f(0, 0) = \\begin{pmatrix} 2(0 - 2) \\\\ 6(0 + 1) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}\n$$\n\nFor $\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$:\n$$\n\\mathbf{g}_1 = \\nabla f(1, -2) = \\begin{pmatrix} 2(1 - 2) \\\\ 6(-2 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(-1) \\\\ 6(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix}\n$$\n\nFor $\\mathbf{x}_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\end{pmatrix}$:\n$$\n\\mathbf{g}_2 = \\nabla f(2, -1.5) = \\begin{pmatrix} 2(2 - 2) \\\\ 6(-1.5 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(0) \\\\ 6(-0.5) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}\n$$\n\nNow we can compute the displacement vectors $\\mathbf{s}_0$ and $\\mathbf{s}_1$.\n\nFor $k=0$:\n$$\n\\mathbf{s}_0 = \\mathbf{x}_1 - \\mathbf{x}_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\n\\mathbf{s}_1 = \\mathbf{x}_2 - \\mathbf{x}_1 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1.5 - (-2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n\nNext, we compute the gradient difference vectors $\\mathbf{y}_0$ and $\\mathbf{y}_1$.\n\nFor $k=0$:\n$$\n\\mathbf{y}_0 = \\mathbf{g}_1 - \\mathbf{g}_0 = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} - \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} -2 - (-4) \\\\ -6 - 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\n\\mathbf{y}_1 = \\mathbf{g}_2 - \\mathbf{g}_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 0 - (-2) \\\\ -3 - (-6) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n\nFinally, we assemble the results into a $2 \\times 4$ matrix where the columns are $\\mathbf{s}_0, \\mathbf{y}_0, \\mathbf{s}_1, \\mathbf{y}_1$.\n$$\n\\mathbf{s}_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad \\mathbf{y}_0 = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}, \\quad \\mathbf{s}_1 = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad \\mathbf{y}_1 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nThe resulting matrix is:\n$$\n\\begin{pmatrix} 1 & 2 & 1 & 2 \\\\ -2 & -12 & \\frac{1}{2} & 3 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2 & 1 & 2 \\\\ -2 & -12 & \\frac{1}{2} & 3 \\end{pmatrix}}\n$$", "id": "2184596"}, {"introduction": "At the heart of the L-BFGS algorithm is the elegant two-loop recursion, a procedure that efficiently computes the search direction without ever forming an explicit Hessian matrix. This exercise guides you step-by-step through this crucial computation, revealing how the stored history vectors $(\\mathbf{s}_i, \\mathbf{y}_i)$ are used to implicitly apply the inverse Hessian approximation. Mastering this process is key to understanding what makes L-BFGS both powerful and memory-efficient. [@problem_id:2184578]", "problem": "The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm is a popular quasi-Newton method for unconstrained optimization. In each iteration $k$, the algorithm computes a search direction $\\mathbf{p}_k$ by applying an approximation of the inverse Hessian matrix to the negative of the current gradient, $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$. This approximation is constructed implicitly using a limited history of the $m$ most recent steps.\n\nThe history is stored as pairs of vectors $(\\mathbf{s}_i, \\mathbf{y}_i)$ for $i=k-m, \\dots, k-1$, where $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ is the change in position and $\\mathbf{y}_i = \\mathbf{g}_{i+1} - \\mathbf{g}_i$ is the change in the gradient. The search direction $\\mathbf{p}_k$ is then found by a procedure known as the L-BFGS two-loop recursion.\n\nConsider an L-BFGS update at step $k$ with a memory size of $m=2$. The relevant data available from previous steps are:\n- Current gradient: $\\mathbf{g}_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- History from step $k-1$: $\\mathbf{s}_{k-1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\mathbf{y}_{k-1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- History from step $k-2$: $\\mathbf{s}_{k-2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $\\mathbf{y}_{k-2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n\nYour task is to compute the search direction vector $\\mathbf{p}_k$ for this iteration. Express your answer as a $2 \\times 1$ column vector with exact rational components.", "solution": "The L-BFGS search direction $\\mathbf{p}_k$ is computed by approximating the product $-H_k \\mathbf{g}_k$, where $H_k$ is the inverse Hessian approximation. This is achieved efficiently using the two-loop recursion algorithm. We are given $m=2$, the gradient $\\mathbf{g}_k$, and the history vectors $(\\mathbf{s}_{k-1}, \\mathbf{y}_{k-1})$ and $(\\mathbf{s}_{k-2}, \\mathbf{y}_{k-2})$.\n\nThe algorithm is as follows:\n\n1.  Initialize a vector $\\mathbf{q}$ with the current gradient:\n    $\\mathbf{q} = \\mathbf{g}_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\n2.  **First Loop (backward pass):** This loop iterates from $i = k-1$ down to $i = k-m$. In our case, $i$ goes from $k-1$ to $k-2$.\n    We first pre-calculate the scalars $\\rho_i = \\frac{1}{\\mathbf{y}_i^T \\mathbf{s}_i}$.\n    For $i = k-1$:\n    $\\mathbf{y}_{k-1}^T \\mathbf{s}_{k-1} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (1)(0) = 1$.\n    So, $\\rho_{k-1} = \\frac{1}{1} = 1$.\n\n    For $i = k-2$:\n    $\\mathbf{y}_{k-2}^T \\mathbf{s}_{k-2} = \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (-1)(0) + (2)(1) = 2$.\n    So, $\\rho_{k-2} = \\frac{1}{2}$.\n\n    Now, we perform the loop updates. We will also store the computed $\\alpha_i$ values, as they are needed in the second loop.\n    -   **For $i = k-1$**:\n        $\\alpha_{k-1} = \\rho_{k-1} \\mathbf{s}_{k-1}^T \\mathbf{q} = (1) \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = (1)((1)(1) + (0)(-2)) = 1$.\n        $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_{k-1} \\mathbf{y}_{k-1} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - (1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ -2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n\n    -   **For $i = k-2$**:\n        $\\alpha_{k-2} = \\rho_{k-2} \\mathbf{s}_{k-2}^T \\mathbf{q} = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((0)(0) + (1)(-3)) = -\\frac{3}{2}$.\n        $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_{k-2} \\mathbf{y}_{k-2} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\left(-\\frac{3}{2}\\right) \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} -\\frac{3}{2} \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 - \\frac{3}{2} \\\\ -3 + 3 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix}$.\n\n3.  **Initial Hessian Scaling:** The initial inverse Hessian approximation $H_k^0$ is a diagonal matrix $\\gamma_k I$, where $\\gamma_k = \\frac{\\mathbf{s}_{k-1}^T \\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^T \\mathbf{y}_{k-1}}$. We initialize our result vector $\\mathbf{r}$ by multiplying this scaled identity matrix with the current $\\mathbf{q}$.\n    $\\mathbf{s}_{k-1}^T \\mathbf{y}_{k-1} = (1)(1) + (0)(1) = 1$.\n    $\\mathbf{y}_{k-1}^T \\mathbf{y}_{k-1} = (1)^2 + (1)^2 = 2$.\n    $\\gamma_k = \\frac{1}{2}$.\n    $\\mathbf{r} = \\gamma_k \\mathbf{q} = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix}$.\n\n4.  **Second Loop (forward pass):** This loop iterates from $i = k-m$ up to $i = k-1$. In our case, $i$ goes from $k-2$ to $k-1$.\n    -   **For $i = k-2$**:\n        $\\beta = \\rho_{k-2} \\mathbf{y}_{k-2}^T \\mathbf{r} = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((-1)(-\\frac{3}{4}) + (2)(0)) = \\frac{3}{8}$.\n        $\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_{k-2} (\\alpha_{k-2} - \\beta) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{3}{2} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{12}{8} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n    -   **For $i = k-1$**:\n        $\\beta = \\rho_{k-1} \\mathbf{y}_{k-1}^T \\mathbf{r} = (1) \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = (1)(-\\frac{6}{8}) + (1)(-\\frac{15}{8}) = -\\frac{21}{8}$.\n        $\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_{k-1} (\\alpha_{k-1} - \\beta) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(1 - \\left(-\\frac{21}{8}\\right)\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(\\frac{8}{8} + \\frac{21}{8}\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} \\frac{29}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n5.  The final result of the two-loop recursion is the vector $\\mathbf{r} = H_k \\mathbf{g}_k$. The search direction is $\\mathbf{p}_k = -\\mathbf{r}$.\n    $\\mathbf{p}_k = - \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}}$$", "id": "2184578"}, {"introduction": "Moving from manual calculations to practical implementation, this exercise challenges you to build a core component of the L-BFGS optimizer, including the two-loop recursion and a line search. You will investigate a subtle but critical detail: the initial scaling of the Hessian approximation, $\\gamma_k$, and its impact on the step size. This coding practice bridges the gap between theory and real-world performance, demonstrating how implementation choices can significantly influence an algorithm's behavior. [@problem_id:3142851]", "problem": "Consider the limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method for unconstrained smooth optimization of a twice continuously differentiable objective function $f(\\mathbf{x})$. The goal is to determine how the choice of initial inverse-Hessian scaling $H_0 = \\gamma_k I$ at iteration $k$ affects the magnitude of the step taken by the algorithm. Two scaling choices are commonly used:\n- $\\gamma_k = \\dfrac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\top}\\mathbf{y}_{k-1}}$,\n- $\\gamma_k = \\dfrac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{s}_{k-1}^{\\top}\\mathbf{s}_{k-1}}$,\nwhere $\\mathbf{s}_{k-1} = \\mathbf{x}_k - \\mathbf{x}_{k-1}$ and $\\mathbf{y}_{k-1} = \\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1})$, with the curvature condition $\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1} > 0$ assumed.\n\nBase your reasoning solely on the following underlying principles:\n- The gradient $\\nabla f(\\mathbf{x})$ points in the direction of steepest ascent, so a descent method uses a direction $\\mathbf{p}_k$ such that $\\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k < 0$.\n- The quasi-Newton secant condition enforces $\\mathbf{B}_k \\mathbf{s}_{k-1} = \\mathbf{y}_{k-1}$ for the approximate Hessian $\\mathbf{B}_k$ and its inverse $\\mathbf{H}_k = \\mathbf{B}_k^{-1}$, preserving symmetry and positive definiteness.\n- Limited-memory storage uses only the most recent $m$ pairs $\\left(\\mathbf{s}_i, \\mathbf{y}_i\\right)$ to construct $\\mathbf{H}_k$ implicitly via a two-loop recursion compatible with the secant condition.\n\nWrite a complete program that:\n- Constructs the L-BFGS search direction $\\mathbf{p}_k = -\\mathbf{H}_k \\nabla f(\\mathbf{x}_k)$ using the standard limited-memory two-loop recursion with an initial inverse-Hessian $H_0 = \\gamma_k I$ at iteration $k$.\n- Uses a backtracking line search to find a step length $\\alpha_k$ that satisfies the Armijo condition (sufficient decrease): $f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$, with parameters $c_1 = 10^{-4}$, initial step length $\\alpha_0 = 1.0$, reduction factor $\\tau = 0.5$, and a maximum of $50$ backtracking iterations.\n- Computes, for each test case, the Euclidean norm of the accepted step $\\alpha_k \\mathbf{p}_k$ for both scaling choices of $\\gamma_k$, and reports the ratio\n$$r = \\dfrac{\\left\\|\\alpha_k^{(y)} \\mathbf{p}_k^{(y)}\\right\\|_2}{\\left\\|\\alpha_k^{(s)} \\mathbf{p}_k^{(s)}\\right\\|_2},$$\nwhere superscripts $(y)$ and $(s)$ denote the choices $\\gamma_k = \\dfrac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\top}\\mathbf{y}_{k-1}}$ and $\\gamma_k = \\dfrac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{s}_{k-1}^{\\top}\\mathbf{s}_{k-1}}$, respectively.\n\nImplement the following test suite. For each case, use the specified function $f(\\mathbf{x})$, its gradient $\\nabla f(\\mathbf{x})$, the current point $\\mathbf{x}_k$, and the stored pairs. Construct $\\mathbf{s}_{i}$ and $\\mathbf{y}_{i}$ from the given sequence of points and gradients as $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ and $\\mathbf{y}_i = \\nabla f(\\mathbf{x}_{i+1}) - \\nabla f(\\mathbf{x}_i)$. The memory size is the number of provided pairs for that case.\n\n- Test case $1$ (quadratic, memory size $0$):\n  - Dimension $n = 3$.\n  - $f(\\mathbf{x}) = \\dfrac{1}{2}\\mathbf{x}^{\\top}A\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$ with\n    $$A = \\begin{bmatrix}100 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 10\\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}.$$\n  - $\\mathbf{x}_{k-1} = \\begin{bmatrix}0.9 \\\\ 0.9 \\\\ 0.9\\end{bmatrix}$, $\\mathbf{x}_k = \\begin{bmatrix}1.0 \\\\ 1.0 \\\\ 1.0\\end{bmatrix}$.\n  - Use $\\mathbf{s}_{k-1}$ and $\\mathbf{y}_{k-1}$ computed from these points to define $\\gamma_k$, but do not store any pairs in memory (so the two-loop recursion uses only $H_0 = \\gamma_k I$).\n\n- Test case $2$ (Rosenbrock, memory size $2$):\n  - Dimension $n = 2$.\n  - $f(\\mathbf{x}) = 100(\\mathbf{x}_2 - \\mathbf{x}_1^2)^2 + (1 - \\mathbf{x}_1)^2$.\n  - $\\nabla f(\\mathbf{x}) = \\begin{bmatrix}-400(\\mathbf{x}_2 - \\mathbf{x}_1^2)\\mathbf{x}_1 - 2(1 - \\mathbf{x}_1) \\\\ 200(\\mathbf{x}_2 - \\mathbf{x}_1^2)\\end{bmatrix}$.\n  - Points: $\\mathbf{x}_{k-2} = \\begin{bmatrix}-1.5 \\\\ 1.5\\end{bmatrix}$, $\\mathbf{x}_{k-1} = \\begin{bmatrix}-1.3 \\\\ 1.2\\end{bmatrix}$, $\\mathbf{x}_k = \\begin{bmatrix}-1.2 \\\\ 1.0\\end{bmatrix}$.\n  - Store the two most recent pairs $\\left(\\mathbf{s}_{k-2}, \\mathbf{y}_{k-2}\\right)$ and $\\left(\\mathbf{s}_{k-1}, \\mathbf{y}_{k-1}\\right)$ for the two-loop recursion. Use $\\left(\\mathbf{s}_{k-1}, \\mathbf{y}_{k-1}\\right)$ to define $\\gamma_k$.\n\n- Test case $3$ (quartic valley, memory size $0$):\n  - Dimension $n = 2$.\n  - $f(\\mathbf{x}) = \\dfrac{1}{2}\\left(\\mathbf{x}_1^2 + 1000\\,\\mathbf{x}_2^2\\right) + 0.01\\left(\\mathbf{x}_1^4 + \\mathbf{x}_2^4\\right)$.\n  - $\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\\mathbf{x}_1 + 0.04\\,\\mathbf{x}_1^3 \\\\ 1000\\,\\mathbf{x}_2 + 0.04\\,\\mathbf{x}_2^3\\end{bmatrix}$.\n  - $\\mathbf{x}_{k-1} = \\begin{bmatrix}1.9 \\\\ 0.19\\end{bmatrix}$, $\\mathbf{x}_k = \\begin{bmatrix}2.0 \\\\ 0.2\\end{bmatrix}$.\n  - Use $\\mathbf{s}_{k-1}$ and $\\mathbf{y}_{k-1}$ computed from these points to define $\\gamma_k$, but do not store any pairs in memory.\n\nFor each test case, compute the ratio $r$ defined above. Your program should produce a single line of output containing the three ratios as a comma-separated list enclosed in square brackets (for example, $\\left[ r_1, r_2, r_3 \\right]$). All outputs must be real numbers (floats). No physical units are involved, and angles are not used.", "solution": "The problem requires an analysis of the effect of two different initial inverse-Hessian scaling strategies within the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm. Specifically, we are to compute the search direction and step length for a single iteration of L-BFGS starting from a given state $(\\mathbf{x}_k, \\nabla f(\\mathbf{x}_k))$ and a memory of past steps. The analysis will compare the magnitudes of the resulting full steps, $\\alpha_k \\mathbf{p}_k$, for each scaling choice.\n\nThe L-BFGS method is a quasi-Newton method for unconstrained optimization, designed to solve problems of the form $\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$, where $f$ is a continuously differentiable function. Quasi-Newton methods iteratively find the minimum by generating a sequence of points $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a search direction and $\\alpha_k > 0$ is a step length. The search direction is defined by $\\mathbf{p}_k = -\\mathbf{H}_k \\nabla f(\\mathbf{x}_k)$, where $\\mathbf{H}_k$ is an approximation to the inverse of the Hessian matrix, $\\left(\\nabla^2 f(\\mathbf{x}_k)\\right)^{-1}$.\n\nThe core of L-BFGS lies in its method for constructing $\\mathbf{H}_k$. Instead of forming and storing the dense $n \\times n$ matrix, it stores a limited history of the $m$ most recent displacement vectors $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ and gradient difference vectors $\\mathbf{y}_i = \\nabla f(\\mathbf{x}_{i+1}) - \\nabla f(\\mathbf{x}_i)$. These pairs $(\\mathbf{s}_i, \\mathbf{y}_i)$ implicitly define the Hessian approximation through the secant condition $\\mathbf{B}_{i+1} \\mathbf{s}_i = \\mathbf{y}_i$ (or for the inverse, $\\mathbf{H}_{i+1} \\mathbf{y}_i = \\mathbf{s}_i$). The curvature condition $\\mathbf{s}_i^{\\top}\\mathbf{y}_i > 0$ must hold for the Hessian approximation to remain positive definite, ensuring that $\\mathbf{p}_k$ is a descent direction.\n\nThe matrix-vector product $\\mathbf{H}_k \\nabla f(\\mathbf{x}_k)$ is computed efficiently using a two-loop recursion. This procedure starts with an initial diagonal approximation to the inverse Hessian, $\\mathbf{H}_0 = \\gamma_k \\mathbf{I}$, and recursively applies updates based on the $m$ stored pairs.\n\nLet $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ be the gradient at the current iterate $\\mathbf{x}_k$. The memory at iteration $k$ consists of the set of pairs $\\{ (\\mathbf{s}_i, \\mathbf{y}_i) \\}_{i=k-m}^{k-1}$. The standard two-loop algorithm to compute $\\mathbf{r} = \\mathbf{H}_k \\mathbf{g}_k$ is as follows:\n\n$1$. Initialize $\\mathbf{q} \\leftarrow \\mathbf{g}_k$.\n$2$. **First Loop (backward pass):** Iterate from $i = k-1$ down to $k-m$. For each pair $(\\mathbf{s}_i, \\mathbf{y}_i)$:\n   - Compute $\\rho_i = \\dfrac{1}{\\mathbf{y}_i^{\\top}\\mathbf{s}_i}$.\n   - Compute the coefficient $\\beta_i = \\rho_i \\mathbf{s}_i^{\\top}\\mathbf{q}$.\n   - Update the vector: $\\mathbf{q} \\leftarrow \\mathbf{q} - \\beta_i \\mathbf{y}_i$.\n$3$. **Initial Hessian Scaling:** Apply the initial inverse Hessian approximation:\n   - $\\mathbf{r} \\leftarrow \\mathbf{H}_0 \\mathbf{q} = \\gamma_k \\mathbf{I} \\mathbf{q} = \\gamma_k \\mathbf{q}$.\n$4$. **Second Loop (forward pass):** Iterate from $i = k-m$ up to $k-1$. For each pair $(\\mathbf{s}_i, \\mathbf{y}_i)$:\n   - Recompute $\\rho_i = \\dfrac{1}{\\mathbf{y}_i^{\\top}\\mathbf{s}_i}$.\n   - Compute the coefficient $\\delta = \\rho_i \\mathbf{y}_i^{\\top}\\mathbf{r}$.\n   - Update the vector: $\\mathbf{r} \\leftarrow \\mathbf{r} + (\\beta_i - \\delta)\\mathbf{s}_i$.\n\nThe final search direction is $\\mathbf{p}_k = -\\mathbf{r}$. If the memory size $m=0$, the loops are skipped, and the direction is simply the scaled steepest descent direction, $\\mathbf{p}_k = -\\gamma_k \\mathbf{g}_k$.\n\nThe choice of the scaling factor $\\gamma_k$ is critical as it sets the scale of the initial Hessian approximation. The problem asks us to investigate two common choices, both based on the most recent pair $(\\mathbf{s}_{k-1}, \\mathbf{y}_{k-1})$:\n- **Scaling choice (y):** $\\gamma_k^{(y)} = \\dfrac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\top}\\mathbf{y}_{k-1}}$. This choice scales $\\mathbf{H}_0$ using information from the gradient differences. It makes the eigenvalues of $\\mathbf{B}_0 \\approx (\\mathbf{H}_0)^{-1}$ approximate the eigenvalues of the averaged Hessian along $\\mathbf{y}_{k-1}$.\n- **Scaling choice (s):** $\\gamma_k^{(s)} = \\dfrac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{s}_{k-1}^{\\top}\\mathbf{s}_{k-1}}$. This value is the Rayleigh quotient of the average Hessian $\\bar{\\mathbf{B}} = \\int_0^1 \\nabla^2 f(\\mathbf{x}_{k-1} + \\tau \\mathbf{s}_{k-1}) d\\tau$ with respect to $\\mathbf{s}_{k-1}$. A more common scaling strategy for the inverse Hessian approximation is to use the reciprocal of this value. This problem investigates the behavior of using the Rayleigh quotient directly.\n\nOnce the search direction $\\mathbf{p}_k$ is determined, a line search procedure is used to find a suitable step length $\\alpha_k > 0$. The problem specifies a backtracking line search to satisfy the Armijo condition of sufficient decrease:\n$$f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$$\nThe procedure starts with an initial step length $\\alpha_0 = 1.0$ and reduces it by a factor $\\tau = 0.5$ until the condition is met, using the parameter $c_1 = 10^{-4}$.\n\nThe final objective is to compute the ratio of the Euclidean norms of the accepted steps for the two scaling choices:\n$$r = \\dfrac{\\left\\|\\alpha_k^{(y)} \\mathbf{p}_k^{(y)}\\right\\|_2}{\\left\\|\\alpha_k^{(s)} \\mathbf{p}_k^{(s)}\\right\\|_2}$$\nwhere the superscripts $(y)$ and $(s)$ correspond to the scaling choices for $\\gamma_k$. This ratio quantifies the relative magnitude of the optimization step produced by each scaling strategy under the specified conditions. We will now implement this procedure for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the L-BFGS step computation for all test cases.\n    \"\"\"\n\n    def lbfgs_two_loop(gk, memory, gamma):\n        \"\"\"\n        Computes the L-BFGS search direction p_k = -H_k * g_k using the\n        two-loop recursion.\n\n        Args:\n            gk (np.ndarray): The gradient at the current point, grad f(x_k).\n            memory (list): A list of (s_i, y_i) tuples, from oldest to newest.\n            gamma (float): The initial Hessian scaling factor.\n\n        Returns:\n            np.ndarray: The search direction p_k.\n        \"\"\"\n        m = len(memory)\n        q = gk.copy()\n        \n        if m == 0:\n            return -gamma * q\n\n        betas = np.zeros(m)\n        rhos = np.zeros(m)\n\n        # First loop (backward pass)\n        for i in range(m - 1, -1, -1):\n            s_i, y_i = memory[i]\n            # The curvature condition s_i^T * y_i > 0 is assumed to hold\n            rhos[i] = 1.0 / np.dot(y_i, s_i)\n            betas[i] = rhos[i] * np.dot(s_i, q)\n            q -= betas[i] * y_i\n\n        # Initial Hessian approximation\n        r = gamma * q\n\n        # Second loop (forward pass)\n        for i in range(m):\n            s_i, y_i = memory[i]\n            delta = rhos[i] * np.dot(y_i, r)\n            r += (betas[i] - delta) * s_i\n            \n        return -r\n\n    def backtracking_linesearch(f, grad_f, xk, pk, c1, tau, alpha0):\n        \"\"\"\n        Performs a backtracking line search to find a step length alpha\n        that satisfies the Armijo condition.\n\n        Args:\n            f (callable): The objective function.\n            grad_f (callable): The gradient of the objective function.\n            xk (np.ndarray): The current point.\n            pk (np.ndarray): The search direction.\n            c1 (float): Armijo condition parameter.\n            tau (float): Reduction factor for alpha.\n            alpha0 (float): Initial step length.\n\n        Returns:\n            float: The accepted step length.\n        \"\"\"\n        alpha = alpha0\n        f_xk = f(xk)\n        grad_f_xk_dot_pk = np.dot(grad_f(xk), pk)\n\n        for _ in range(50): # Max 50 iterations\n            if f(xk + alpha * pk) <= f_xk + c1 * alpha * grad_f_xk_dot_pk:\n                return alpha\n            alpha *= tau\n        return alpha\n\n    def compute_step_norm(gamma_k, f, grad_f, xk, memory, line_search_params):\n        \"\"\"\n        Computes the norm of the accepted L-BFGS step for a given gamma.\n        \"\"\"\n        gk = grad_f(xk)\n        pk = lbfgs_two_loop(gk, memory, gamma_k)\n        \n        # Check if pk is a descent direction\n        if np.dot(gk, pk) >= 0:\n            # This should not happen if curvature condition holds and H_k is PD.\n            # As a fallback, use steepest descent.\n            pk = -gk\n            \n        alpha_k = backtracking_linesearch(f, grad_f, xk, pk, **line_search_params)\n        step = alpha_k * pk\n        return np.linalg.norm(step)\n\n    # Line search parameters\n    line_search_params = {\n        'c1': 1e-4,\n        'tau': 0.5,\n        'alpha0': 1.0\n    }\n\n    # --- Test Case 1 ---\n    def f1(x):\n        A = np.diag([100.0, 1.0, 10.0])\n        return 0.5 * x.T @ A @ x\n    def grad_f1(x):\n        A = np.diag([100.0, 1.0, 10.0])\n        return A @ x\n    \n    x_k_minus_1_tc1 = np.array([0.9, 0.9, 0.9])\n    x_k_tc1 = np.array([1.0, 1.0, 1.0])\n    s_km1_tc1 = x_k_tc1 - x_k_minus_1_tc1\n    y_km1_tc1 = grad_f1(x_k_tc1) - grad_f1(x_k_minus_1_tc1)\n    \n    sTy = np.dot(s_km1_tc1, y_km1_tc1)\n    gamma_y_tc1 = sTy / np.dot(y_km1_tc1, y_km1_tc1)\n    gamma_s_tc1 = sTy / np.dot(s_km1_tc1, s_km1_tc1)\n    \n    memory_tc1 = []\n    norm_y_tc1 = compute_step_norm(gamma_y_tc1, f1, grad_f1, x_k_tc1, memory_tc1, line_search_params)\n    norm_s_tc1 = compute_step_norm(gamma_s_tc1, f1, grad_f1, x_k_tc1, memory_tc1, line_search_params)\n    ratio1 = norm_y_tc1 / norm_s_tc1 if norm_s_tc1 != 0 else 0.0\n\n    # --- Test Case 2 ---\n    def f2(x):\n        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n    def grad_f2(x):\n        return np.array([\n            -400 * (x[1] - x[0]**2) * x[0] - 2 * (1 - x[0]),\n            200 * (x[1] - x[0]**2)\n        ])\n    \n    x_points_tc2 = [\n        np.array([-1.5, 1.5]),\n        np.array([-1.3, 1.2]),\n        np.array([-1.2, 1.0])\n    ]\n    x_k_tc2 = x_points_tc2[-1]\n    memory_tc2 = []\n    for i in range(len(x_points_tc2) - 2, -1, -1):\n        s_i = x_points_tc2[i+1] - x_points_tc2[i]\n        y_i = grad_f2(x_points_tc2[i+1]) - grad_f2(x_points_tc2[i])\n        memory_tc2.insert(0, (s_i, y_i)) # Oldest to newest\n    \n    s_km1_tc2, y_km1_tc2 = memory_tc2[-1]\n    \n    sTy = np.dot(s_km1_tc2, y_km1_tc2)\n    gamma_y_tc2 = sTy / np.dot(y_km1_tc2, y_km1_tc2)\n    gamma_s_tc2 = sTy / np.dot(s_km1_tc2, s_km1_tc2)\n\n    norm_y_tc2 = compute_step_norm(gamma_y_tc2, f2, grad_f2, x_k_tc2, memory_tc2, line_search_params)\n    norm_s_tc2 = compute_step_norm(gamma_s_tc2, f2, grad_f2, x_k_tc2, memory_tc2, line_search_params)\n    ratio2 = norm_y_tc2 / norm_s_tc2 if norm_s_tc2 != 0 else 0.0\n\n    # --- Test Case 3 ---\n    def f3(x):\n        return 0.5 * (x[0]**2 + 1000 * x[1]**2) + 0.01 * (x[0]**4 + x[1]**4)\n    def grad_f3(x):\n        return np.array([\n            x[0] + 0.04 * x[0]**3,\n            1000 * x[1] + 0.04 * x[1]**3\n        ])\n    \n    x_k_minus_1_tc3 = np.array([1.9, 0.19])\n    x_k_tc3 = np.array([2.0, 0.2])\n    s_km1_tc3 = x_k_tc3 - x_k_minus_1_tc3\n    y_km1_tc3 = grad_f3(x_k_tc3) - grad_f3(x_k_minus_1_tc3)\n    \n    sTy = np.dot(s_km1_tc3, y_km1_tc3)\n    gamma_y_tc3 = sTy / np.dot(y_km1_tc3, y_km1_tc3)\n    gamma_s_tc3 = sTy / np.dot(s_km1_tc3, s_km1_tc3)\n    \n    memory_tc3 = []\n    norm_y_tc3 = compute_step_norm(gamma_y_tc3, f3, grad_f3, x_k_tc3, memory_tc3, line_search_params)\n    norm_s_tc3 = compute_step_norm(gamma_s_tc3, f3, grad_f3, x_k_tc3, memory_tc3, line_search_params)\n    ratio3 = norm_y_tc3 / norm_s_tc3 if norm_s_tc3 != 0 else 0.0\n    \n    results = [ratio1, ratio2, ratio3]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3142851"}]}