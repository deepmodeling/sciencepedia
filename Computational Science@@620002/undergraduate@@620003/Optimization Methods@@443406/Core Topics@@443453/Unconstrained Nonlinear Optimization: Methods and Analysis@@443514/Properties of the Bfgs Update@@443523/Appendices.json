{"hands_on_practices": [{"introduction": "To truly understand the BFGS method, we must start with its fundamental mechanics. This first exercise [@problem_id:3166921] guides you through a direct application of the BFGS update formula to build the next Hessian approximation, $B_{k+1}$. More importantly, it demonstrates how the foundational secant condition provides an elegant shortcut for calculations involving the inverse Hessian, $H_{k+1}$, highlighting the deep connection between these two matrices.", "problem": "Consider the unconstrained minimization of a twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by the quadratic model $f(x)=\\frac{1}{2}x^{\\top}A x$, where $A\\in\\mathbb{R}^{2\\times 2}$ is symmetric positive definite with\n$$\nA=\\begin{pmatrix}4 & 1 \\\\ 1 & 3\\end{pmatrix}.\n$$\nLet $x_{k}\\in\\mathbb{R}^{2}$ be a current iterate and $x_{k+1}=x_{k}+s_{k}$ with a given step $s_{k}\\in\\mathbb{R}^{2}$ equal to\n$$\ns_{k}=\\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}.\n$$\nDefine the gradient change $y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})$. For this quadratic $f$, recall that $\\nabla f(x)=A x$, so $y_{k}=A s_{k}$. Let $B_{k}\\in\\mathbb{R}^{2\\times 2}$ be a symmetric positive definite approximation to the Hessian at iteration $k$, and set $B_{k}=I$, where $I$ is the identity matrix. Using the defining principles of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update—symmetry, the secant condition $B_{k+1}s_{k}=y_{k}$, and minimal deviation from $B_{k}$ in the metric induced by $B_{k}^{-1}$ to preserve positive definiteness—construct $B_{k+1}$ directly from $B_{k}$, $s_{k}$, and $y_{k}$. Then, establish the connection to the inverse approximation $H_{k+1}=B_{k+1}^{-1}$, and use this connection to evaluate the scalar\n$$\ns_{k}^{\\top}H_{k+1}y_{k}.\n$$\nExpress your final answer as a single real number. No rounding is required.", "solution": "The problem asks us to consider a quadratic minimization problem and perform one step of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update for the Hessian approximation. We are then required to evaluate a specific scalar quantity.\n\nThe problem provides the following data and definitions:\nThe objective function is a quadratic form $f(x)=\\frac{1}{2}x^{\\top}A x$, where $x \\in \\mathbb{R}^{2}$.\nThe Hessian matrix $A$ is given as $A=\\begin{pmatrix}4 & 1 \\\\ 1 & 3\\end{pmatrix}$. The matrix is symmetric and positive definite as its determinant is $4 \\times 3 - 1 \\times 1 = 11 > 0$ and its trace is $4+3=7>0$.\nThe step from iterate $x_k$ to $x_{k+1}$ is $s_k = x_{k+1} - x_k = \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}$.\nThe gradient change is $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. For the given quadratic function, the gradient is $\\nabla f(x) = A x$. Thus, $y_k = A x_{k+1} - A x_k = A(x_{k+1}-x_k) = A s_k$.\nThe initial Hessian approximation is the identity matrix, $B_k = I = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}$.\n\nFirst, we calculate the gradient change vector $y_k$:\n$$\ny_k = A s_k = \\begin{pmatrix}4 & 1 \\\\ 1 & 3\\end{pmatrix} \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}4(-2) + 1(1) \\\\ 1(-2) + 3(1)\\end{pmatrix} = \\begin{pmatrix}-7 \\\\ 1\\end{pmatrix}.\n$$\nThe BFGS update requires that the condition $s_k^{\\top}y_k > 0$ is met to ensure that the updated matrix $B_{k+1}$ remains positive definite (given $B_k$ is positive definite). We verify this condition:\n$$\ns_k^{\\top}y_k = \\begin{pmatrix}-2 & 1\\end{pmatrix} \\begin{pmatrix}-7 \\\\ 1\\end{pmatrix} = (-2)(-7) + (1)(1) = 14 + 1 = 15.\n$$\nSince $15 > 0$, the condition is satisfied.\n\nThe problem asks to construct the next Hessian approximation, $B_{k+1}$. The BFGS update formula is given by:\n$$\nB_{k+1} = B_k + \\frac{y_k y_k^{\\top}}{y_k^{\\top} s_k} - \\frac{B_k s_k s_k^{\\top} B_k}{s_k^{\\top} B_k s_k}.\n$$\nWe compute the necessary components for this formula. We have $B_k=I$.\nThe vector $B_k s_k$ is:\n$$\nB_k s_k = I s_k = s_k = \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}.\n$$\nThe scalar $s_k^{\\top} B_k s_k$ is:\n$$\ns_k^{\\top} B_k s_k = s_k^{\\top} I s_k = s_k^{\\top} s_k = (-2)^2 + 1^2 = 5.\n$$\nThe term $y_k^{\\top}s_k$ was already computed and is equal to $15$.\n\nNow we construct the two rank-one update matrices.\nThe first update term is:\n$$\n\\frac{y_k y_k^{\\top}}{y_k^{\\top} s_k} = \\frac{1}{15} \\begin{pmatrix}-7 \\\\ 1\\end{pmatrix} \\begin{pmatrix}-7 & 1\\end{pmatrix} = \\frac{1}{15} \\begin{pmatrix}49 & -7 \\\\ -7 & 1\\end{pmatrix}.\n$$\nThe second update term is:\n$$\n\\frac{B_k s_k s_k^{\\top} B_k}{s_k^{\\top} B_k s_k} = \\frac{s_k s_k^{\\top}}{s_k^{\\top} s_k} = \\frac{1}{5} \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix} \\begin{pmatrix}-2 & 1\\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix}4 & -2 \\\\ -2 & 1\\end{pmatrix}.\n$$\nSubstituting these into the BFGS formula for $B_{k+1}$:\n$$\nB_{k+1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} + \\frac{1}{15} \\begin{pmatrix}49 & -7 \\\\ -7 & 1\\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix}4 & -2 \\\\ -2 & 1\\end{pmatrix}\n$$\nTo combine these matrices, we use a common denominator of $15$:\n$$\nB_{k+1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} + \\begin{pmatrix}49/15 & -7/15 \\\\ -7/15 & 1/15\\end{pmatrix} - \\begin{pmatrix}12/15 & -6/15 \\\\ -6/15 & 3/15\\end{pmatrix}\n$$\n$$\nB_{k+1} = \\begin{pmatrix}1 + 49/15 - 12/15 & 0 - 7/15 + 6/15 \\\\ 0 - 7/15 + 6/15 & 1 + 1/15 - 3/15\\end{pmatrix} = \\begin{pmatrix}(15+49-12)/15 & -1/15 \\\\ -1/15 & (15+1-3)/15\\end{pmatrix}\n$$\n$$\nB_{k+1} = \\begin{pmatrix}52/15 & -1/15 \\\\ -1/15 & 13/15\\end{pmatrix} = \\frac{1}{15}\\begin{pmatrix}52 & -1 \\\\ -1 & 13\\end{pmatrix}.\n$$\nHaving constructed $B_{k+1}$, we are next asked to establish the connection to its inverse, $H_{k+1} = B_{k+1}^{-1}$, and use this to evaluate $s_k^{\\top}H_{k+1}y_k$.\n\nA fundamental defining property of the BFGS update is that the new Hessian approximation $B_{k+1}$ must satisfy the secant condition:\n$$\nB_{k+1}s_k = y_k.\n$$\nThis condition ensures that the new quadratic model's gradient matches the observed gradient change along the direction of the step $s_k$.\n\nLet $H_{k+1}$ be the inverse of $B_{k+1}$, i.e., $H_{k+1} = B_{k+1}^{-1}$. Since $B_{k+1}$ is symmetric and positive definite, its inverse exists and is also symmetric and positive definite. We can pre-multiply the secant condition by $H_{k+1}$:\n$$\nH_{k+1}(B_{k+1}s_k) = H_{k+1}y_k.\n$$\nUsing the associativity of matrix multiplication and the definition of the inverse matrix, $H_{k+1}B_{k+1} = I$:\n$$\n(H_{k+1}B_{k+1})s_k = H_{k+1}y_k\n$$\n$$\nI s_k = H_{k+1}y_k\n$$\n$$\ns_k = H_{k+1}y_k.\n$$\nThis relationship, known as the inverse secant condition, is the \"connection\" asked for in the problem. It is a direct consequence of the way the BFGS update is constructed.\n\nWe can now use this result to evaluate the required scalar quantity $s_k^{\\top}H_{k+1}y_k$. We substitute the expression $s_k$ for $H_{k+1}y_k$:\n$$\ns_k^{\\top}H_{k+1}y_k = s_k^{\\top}(H_{k+1}y_k) = s_k^{\\top}s_k.\n$$\nThe problem is reduced to calculating the squared Euclidean norm of the vector $s_k$.\n$$\ns_k^{\\top}s_k = \\begin{pmatrix}-2 & 1\\end{pmatrix} \\begin{pmatrix}-2 \\\\ 1\\end{pmatrix} = (-2)^2 + (1)^2 = 4 + 1 = 5.\n$$\nTherefore, the value of the scalar expression is $5$. The explicit calculation of $B_{k+1}$ was an intermediate step to demonstrate understanding of the full BFGS procedure, but the final answer is obtained more elegantly by exploiting the fundamental properties of the update.", "answer": "$$\n\\boxed{5}\n$$", "id": "3166921"}, {"introduction": "The elegance of the BFGS update relies on a crucial assumption: that the function exhibits positive curvature along the search direction. This practice problem [@problem_id:3167001] explores what happens when this condition is violated, a common occurrence in non-convex optimization. By working through a scenario at a saddle point, you will see how the standard update can fail and learn a practical damping strategy to restore the positive-definite property of the Hessian approximation, ensuring the algorithm remains stable and effective.", "problem": "Consider the quadratic objective function defined by $f(x) = \\frac{1}{2} x^{\\top} A x$ on $\\mathbb{R}^{2}$, where $A = \\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix}$. Let the current iterate be $x_{k} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and the next iterate be $x_{k+1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, so that $s_{k} = x_{k+1} - x_{k}$. Let the current inverse Hessian approximation be $H_{k} = I$, and define $B_{k} = H_{k}^{-1}$. \n\nTasks:\n1. Verify that the point $x = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ is a saddle point of $f$, and compute $s_{k}$ and $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$. Show that $y_{k}^{\\top} s_{k} \\le 0$.\n2. Using the undamped inverse update from the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method, compute $H_{k+1}$ from $H_{k}$, $s_{k}$, and $y_{k}$, and demonstrate that $H_{k+1}$ is not positive definite.\n3. To restore positive definiteness, consider a damping strategy that replaces $y_{k}$ by a damped vector $\\bar{y}_{k} = \\theta y_{k} + (1 - \\theta) B_{k} s_{k}$ with a parameter $\\theta \\in (0,1]$. Impose the curvature condition $s_{k}^{\\top} \\bar{y}_{k} = \\delta \\, s_{k}^{\\top} B_{k} s_{k}$ with $\\delta = \\frac{1}{5}$. Determine the largest $\\theta$ in $(0,1]$ satisfying this equality, and then compute the corresponding damped update $H_{k+1}$ to verify positive definiteness.\n\nAnswer specification: Report the value of $\\theta$ as an exact rational number in simplest terms. No rounding is required.", "solution": "The problem is well-posed and grounded in the principles of numerical optimization, specifically concerning the properties of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update for quasi-Newton methods. We will proceed with a full solution.\n\nThe objective function is $f(x) = \\frac{1}{2} x^{\\top} A x$ for $x \\in \\mathbb{R}^{2}$, with $A = \\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix}$. This can be written as $f(x_1, x_2) = \\frac{1}{2}(x_1^2 - x_2^2)$.\n\n### Task 1: Saddle Point and Curvature Condition\n\nFirst, we compute the gradient $\\nabla f(x)$ and the Hessian $\\nabla^2 f(x)$ of the objective function.\nThe gradient is given by $\\nabla f(x) = A x = \\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix} \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} = \\begin{pmatrix}x_1 \\\\ -x_2\\end{pmatrix}$.\nThe Hessian is the constant matrix $\\nabla^2 f(x) = A = \\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix}$.\n\nTo verify that $x = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ is a saddle point, we check the first- and second-order conditions.\n1.  The gradient at $x = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ is $\\nabla f(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, confirming that it is a critical point.\n2.  The Hessian matrix $A$ has eigenvalues $\\lambda_1 = 1$ and $\\lambda_2 = -1$. Since the eigenvalues have opposite signs, the Hessian is indefinite. A critical point with an indefinite Hessian is a saddle point.\n\nNext, we compute the vectors $s_k$ and $y_k$.\nThe iterates are given as $x_{k} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and $x_{k+1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\nThe step vector $s_k$ is defined as $s_{k} = x_{k+1} - x_{k}$:\n$$s_k = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$$\nThe vector $y_k$ is defined as the change in gradients, $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$.\n$\\nabla f(x_{k}) = \\nabla f(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n$\\nabla f(x_{k+1}) = \\nabla f(\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}) = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}$.\nTherefore,\n$$y_k = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}$$\nNow we check the sign of the curvature $y_k^\\top s_k$:\n$$y_k^\\top s_k = \\begin{pmatrix}0 & -1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = (0)(0) + (-1)(1) = -1$$\nSince $y_k^\\top s_k = -1 \\le 0$, the standard curvature condition for the BFGS method, which requires $y_k^\\top s_k > 0$ to guarantee positive definiteness of the updated Hessian approximation, is not satisfied.\n\n### Task 2: Undamped BFGS Update\n\nThe inverse BFGS update formula is given by:\n$$H_{k+1} = \\left(I - \\frac{s_k y_k^{\\top}}{y_k^{\\top} s_k}\\right) H_k \\left(I - \\frac{y_k s_k^{\\top}}{y_k^{\\top} s_k}\\right) + \\frac{s_k s_k^{\\top}}{y_k^{\\top} s_k}$$\nWe are given the initial inverse Hessian approximation $H_k = I = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}$ and we have already computed $y_k^\\top s_k = -1$.\nLet $\\rho_k = 1/(y_k^\\top s_k) = -1$.\nWe compute the components for the update:\n$s_k y_k^\\top = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0 & -1\\end{pmatrix} = \\begin{pmatrix}0 & 0 \\\\ 0 & -1\\end{pmatrix}$.\n$y_k s_k^\\top = \\begin{pmatrix}0 \\\\ -1\\end{pmatrix}\\begin{pmatrix}0 & 1\\end{pmatrix} = \\begin{pmatrix}0 & 0 \\\\ 0 & -1\\end{pmatrix}$.\n$s_k s_k^\\top = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0 & 1\\end{pmatrix} = \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix}$.\nThe terms in the main formula are:\n$I - \\rho_k s_k y_k^\\top = I - (-1) s_k y_k^\\top = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} + \\begin{pmatrix}0 & 0 \\\\ 0 & -1\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}$.\n$I - \\rho_k y_k s_k^\\top = I - (-1) y_k s_k^\\top = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} + \\begin{pmatrix}0 & 0 \\\\ 0 & -1\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}$.\nSubstituting these into the update formula:\n$$H_{k+1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} + (-1) \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix}$$\n$$H_{k+1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} - \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} - \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix}$$\nTo check if $H_{k+1}$ is positive definite, we use Sylvester's criterion. The leading principal minors must all be positive.\nThe first minor is $H_{11} = 1 > 0$.\nThe second minor is $\\det(H_{k+1}) = (1)(-1) - (0)(0) = -1$.\nSince the determinant is negative, $H_{k+1}$ is not positive definite.\n\n### Task 3: Damped BFGS Update\n\nTo restore positive definiteness, we use a damping strategy. The vector $y_k$ is replaced by $\\bar{y}_k = \\theta y_k + (1-\\theta) B_k s_k$, where $\\theta \\in (0, 1]$. We are given $H_k = I$, so its inverse, the direct Hessian approximation, is $B_k = H_k^{-1} = I$.\nThe damped vector becomes $\\bar{y}_k = \\theta y_k + (1-\\theta) s_k$.\n\nThe parameter $\\theta$ is determined by imposing the modified curvature condition $s_k^\\top \\bar{y}_k = \\delta s_k^\\top B_k s_k$ with $\\delta = \\frac{1}{5}$.\nSubstituting the expression for $\\bar{y}_k$ and $B_k=I$:\n$$s_k^\\top (\\theta y_k + (1-\\theta) s_k) = \\frac{1}{5} s_k^\\top s_k$$\nExpanding the left side gives:\n$$\\theta (s_k^\\top y_k) + (1-\\theta) (s_k^\\top s_k) = \\frac{1}{5} (s_k^\\top s_k)$$\nWe have the previously computed values $s_k^\\top y_k = -1$ and $s_k^\\top s_k = \\begin{pmatrix}0 & 1\\end{pmatrix}\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = 1$.\nSubstituting these values into the equation for $\\theta$:\n$$\\theta(-1) + (1-\\theta)(1) = \\frac{1}{5}(1)$$\n$$-\\theta + 1 - \\theta = \\frac{1}{5}$$\n$$1 - 2\\theta = \\frac{1}{5}$$\n$$2\\theta = 1 - \\frac{1}{5} = \\frac{4}{5}$$\n$$\\theta = \\frac{2}{5}$$\nThis value lies in the specified interval $(0, 1]$, so it is the required value.\n\nWe now compute the damped update for $H_{k+1}$ using this $\\theta$.\nFirst, we find the damped vector $\\bar{y}_k$:\n$$\\bar{y}_k = \\frac{2}{5} y_k + \\left(1-\\frac{2}{5}\\right) s_k = \\frac{2}{5} \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\frac{3}{5} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -2/5\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 3/5\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1/5\\end{pmatrix}$$\nThe new curvature term is $s_k^\\top \\bar{y}_k = \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1/5\\end{pmatrix} = \\frac{1}{5}$. This is positive as required.\n\nWe apply the BFGS update formula with $\\bar{y}_k$:\n$$H_{k+1} = \\left(I - \\frac{s_k \\bar{y}_k^{\\top}}{\\bar{y}_k^{\\top} s_k}\\right) H_k \\left(I - \\frac{\\bar{y}_k s_k^{\\top}}{\\bar{y}_k^{\\top} s_k}\\right) + \\frac{s_k s_k^{\\top}}{\\bar{y}_k^{\\top} s_k}$$\nLet $\\rho_k = 1/(\\bar{y}_k^\\top s_k) = 1/(1/5) = 5$.\nWe compute the components:\n$s_k \\bar{y}_k^\\top = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}0 & 1/5\\end{pmatrix} = \\begin{pmatrix}0 & 0 \\\\ 0 & 1/5\\end{pmatrix}$.\n$\\bar{y}_k s_k^\\top = \\begin{pmatrix}0 \\\\ 1/5\\end{pmatrix}\\begin{pmatrix}0 & 1\\end{pmatrix} = \\begin{pmatrix}0 & 0 \\\\ 0 & 1/5\\end{pmatrix}$.\n$I - \\rho_k s_k \\bar{y}_k^\\top = I - 5 \\begin{pmatrix}0 & 0 \\\\ 0 & 1/5\\end{pmatrix} = I - \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}$.\nSubstituting into the formula for $H_{k+1}$:\n$$H_{k+1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} + 5 \\begin{pmatrix}0 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} + \\begin{pmatrix}0 & 0 \\\\ 0 & 5\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 5\\end{pmatrix}$$\nTo verify that this matrix is positive definite, we check its eigenvalues, which are its diagonal entries $1$ and $5$. Since both are positive, the matrix is positive definite. Alternatively, the leading principal minors are $1>0$ and $\\det(H_{k+1}) = 5 > 0$, confirming positive definiteness.\n\nThe value of $\\theta$ determined is $\\frac{2}{5}$.", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "3167001"}, {"introduction": "Perhaps the most remarkable feature of the BFGS algorithm is its self-correcting nature. Even with a poor initial guess for the function's curvature, the updates progressively gather information and improve the Hessian approximation. This hands-on coding exercise [@problem_id:3166990] allows you to witness this property directly by implementing BFGS with an exact line search on a quadratic function and measuring how the inverse Hessian approximation converges towards the true inverse, regardless of its starting point.", "problem": "Consider a smooth unconstrained optimization problem with a convex quadratic objective function defined by $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $b \\in \\mathbb{R}^{n}$, and $x \\in \\mathbb{R}^{n}$. The gradient is $\\nabla f(x) = A x - b$. Quasi-Newton methods construct iterates $x_{k+1} = x_k + \\alpha_k p_k$ with $p_k = - H_k \\nabla f(x_k)$, where $H_k$ is an approximation to the inverse Hessian. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is the quasi-Newton scheme that enforces the secant condition $H_{k+1} y_k = s_k$ with $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$, while updating $H_k$ to $H_{k+1}$ in a symmetric, positive definite-preserving manner via a rank-two change that minimizes the alteration subject to the secant condition.\n\nYou will implement a BFGS algorithm for convex quadratics that uses exact line search along the chosen direction $p_k$. The exact line search step size must be the minimizer of $\\phi(\\alpha) = f(x_k + \\alpha p_k)$ along the ray; for a convex quadratic $f$, this minimizer is uniquely defined. Use the following fundamental bases and definitions:\n- The objective $f(x)$ and gradient $\\nabla f(x)$ as specified above.\n- The definitions of $s_k$, $y_k$, and the secant condition.\n- Exact line search along a direction $p_k$ for a convex quadratic.\n\nYour task is to:\n1. Starting from the stated fundamental bases, derive the BFGS inverse-Hessian update that enforces the secant condition $H_{k+1} y_k = s_k$, preserves symmetry and positive definiteness when $y_k^{\\top} s_k > 0$, and arises from a least-change principle. Explain why exact line search on a convex quadratic ensures $y_k^{\\top} s_k > 0$.\n2. Using your derived update, implement a program that, for each test case below, performs BFGS iterations with exact line search starting from a specified initial vector $x_0$ and initial inverse-Hessian approximation $H_0$. For each test case, run exactly $K$ iterations unless an iteration would be undefined due to $\\| \\nabla f(x_k) \\|_2$ being sufficiently small or a denominator in the formulas becoming numerically negligible; in such cases, stop early. After finishing, compute the spectral norm decay metric $\\| H_K A - I \\|_2$, where $I$ is the $n \\times n$ identity matrix and $\\| \\cdot \\|_2$ is the matrix operator two-norm (largest singular value). This quantity numerically reflects the self-correcting nature: even with a poor $H_0$, repeated BFGS updates on a convex quadratic drive $H_k$ toward $A^{-1}$.\n3. Produce the final program output as a single line containing a comma-separated list enclosed in square brackets of the final values of $\\| H_K A - I \\|_2$ for the provided test suite, in the order given below. These outputs are dimensionless real numbers.\n\nTest suite:\n- Case $1$ (happy path with poorly scaled $H_0$):\n  - $n = 3$\n  - $$A_1 = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n  - $$x_{0,1} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,1} = 0.1 \\, I_3.$$\n  - Iterations $K_1 = 3$.\n- Case $2$ (boundary case with $H_0 = A^{-1}$):\n  - $n = 3$\n  - $$A_2 = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}.$$\n  - $$x_{0,2} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,2} = A_2^{-1}.$$\n  - Iterations $K_2 = 3$.\n- Case $3$ (ill-conditioned quadratic):\n  - $n = 3$\n  - $$A_3 = \\begin{bmatrix} 10^{-3} & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 10^{3} \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n  - $$x_{0,3} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,3} = I_3.$$\n  - Iterations $K_3 = 3$.\n- Case $4$ (poorly oriented but symmetric positive definite $H_0$):\n  - $n = 3$\n  - $$A_4 = \\begin{bmatrix} 6 & 2 & 1 \\\\ 2 & 5 & 0 \\\\ 1 & 0 & 3 \\end{bmatrix}, \\quad b_4 = \\begin{bmatrix} 0.5 \\\\ -1 \\\\ 2 \\end{bmatrix}.$$\n  - $$x_{0,4} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad H_{0,4} = \\begin{bmatrix} 3 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 0.5 \\end{bmatrix}.$$\n  - Iterations $K_4 = 3$.\n\nImplementation details:\n- Use exact line search along $p_k$ for a convex quadratic $f$ to determine $\\alpha_k$, which is the unique minimizer of $f(x_k + \\alpha p_k)$ along the direction $p_k$.\n- Stop early if $\\| \\nabla f(x_k) \\|_2 \\leq 10^{-12}$, if $p_k^{\\top} A p_k \\leq 10^{-14}$, or if $y_k^{\\top} s_k \\leq 10^{-14}$ would make the update ill-defined.\n- After the iterations for each case, compute the single float $\\| H_K A - I \\|_2$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[r_1, r_2, r_3, r_4]$, where $r_i$ corresponds to Case $i$.\n\nNo physical units or angle units are involved in this problem. All outputs must be real numbers without units.", "solution": "### 1. Derivation and Theoretical Background\n\nThe Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is a quasi-Newton algorithm for unconstrained optimization. It iteratively builds an approximation $H_k$ to the inverse of the Hessian matrix. The update from $H_k$ to $H_{k+1}$ is designed to satisfy several key properties.\n\n#### 1.1 The BFGS Inverse Hessian Update\n\nThe BFGS update is derived to satisfy the secant condition, preserve symmetry, and maintain positive definiteness. For a move from $x_k$ to $x_{k+1}$, we define the displacement $s_k$ and the change in gradient $y_k$:\n$$ s_k = x_{k+1} - x_k $$\n$$ y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k) $$\nThe secant condition requires the new inverse Hessian approximation $H_{k+1}$ to map $y_k$ to $s_k$:\n$$ H_{k+1} y_k = s_k $$\nFor a quadratic function $f(x) = \\frac{1}{2}x^{\\top} A x - b^{\\top} x$, the gradient is $\\nabla f(x) = Ax-b$. Thus, $y_k = (Ax_{k+1}-b) - (Ax_k-b) = A(x_{k+1}-x_k) = As_k$. The secant condition then becomes $H_{k+1} A s_k = s_k$, which means that $H_{k+1}$ acts as the inverse of $A$ in the direction of $s_k$.\n\nThe BFGS update for the inverse Hessian $H_k$ is the unique symmetric update that satisfies the secant condition and minimizes a weighted Frobenius norm of the change, $\\|H_k^{-1/2}(H_{k+1}-H_k)H_k^{-1/2}\\|_F$. The resulting update formula is a rank-two correction to $H_k$:\n$$ H_{k+1} = \\left(I - \\frac{s_k y_k^{\\top}}{y_k^{\\top} s_k}\\right) H_k \\left(I - \\frac{y_k s_k^{\\top}}{y_k^{\\top} s_k}\\right) + \\frac{s_k s_k^{\\top}}{y_k^{\\top} s_k} $$\nwhere $I$ is the identity matrix.\n\nLet's verify its properties:\n1.  **Secant Condition**: We must show that $H_{k+1} y_k = s_k$. Let $\\rho_k = 1 / (y_k^{\\top} s_k)$.\n    $$ H_{k+1} y_k = \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) y_k + \\rho_k s_k s_k^{\\top} y_k $$\n    The term $\\left(I - \\rho_k y_k s_k^{\\top}\\right) y_k$ simplifies to $y_k - \\rho_k y_k (s_k^{\\top} y_k) = y_k - \\frac{1}{y_k^{\\top} s_k} y_k (y_k^{\\top} s_k) = y_k - y_k = 0$.\n    Therefore, the first part of the expression vanishes:\n    $$ H_{k+1} y_k = 0 + \\rho_k s_k (s_k^{\\top} y_k) = \\frac{1}{y_k^{\\top} s_k} s_k (y_k^{\\top} s_k) = s_k $$\n    The secant condition is satisfied.\n\n2.  **Symmetry**: If $H_k$ is symmetric ($H_k = H_k^{\\top}$), then $H_{k+1}$ must also be symmetric. The term $\\frac{s_k s_k^{\\top}}{y_k^{\\top} s_k}$ is an outer product scaled by a scalar, so it is symmetric. The first term is of the form $M H_k M^{\\top}$ with $M = (I - \\rho_k s_k y_k^{\\top})$, which does not immediately appear symmetric. However, the form is $M H_k (M')^T$ where $M' = (I - \\rho_k s_k y_k^T)$ and not $(I - \\rho_k y_k s_k^T)$. Let's re-examine the formula: $(I - \\rho_k y_k s_k^{\\top})^{\\top} = I - \\rho_k (s_k y_k^{\\top})^{\\top} = I - \\rho_k y_k s_k^{\\top}$. So let's check the transpose of the first term:\n    $$ \\left[ \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) \\right]^{\\top} = \\left(I - \\rho_k y_k s_k^{\\top}\\right)^{\\top} H_k^{\\top} \\left(I - \\rho_k s_k y_k^{\\top}\\right)^{\\top} $$\n    $$ = \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) $$\n    The first term is symmetric, and thus $H_{k+1}$ is symmetric.\n\n3.  **Positive Definiteness**: If $H_k$ is positive definite and the curvature condition $y_k^{\\top} s_k > 0$ holds, then $H_{k+1}$ is also positive definite. For any non-zero vector $z \\in \\mathbb{R}^n$, we have:\n    $$ z^{\\top} H_{k+1} z = z^{\\top}\\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right)z + \\rho_k z^{\\top}s_k s_k^{\\top}z $$\n    Let $v = (I - \\rho_k y_k s_k^{\\top})z$. The first term becomes $v^{\\top} H_k v$. Since $H_k$ is positive definite, $v^{\\top} H_k v \\ge 0$. The second term is $\\rho_k (z^{\\top}s_k)^2 = \\frac{(z^{\\top}s_k)^2}{y_k^{\\top}s_k}$. Since $y_k^{\\top}s_k > 0$, this term is also non-negative. The sum is zero only if both terms are zero, which implies $z$ is a multiple of $y_k$ and also orthogonal to $s_k$, which can only happen if $z=0$. Thus $H_{k+1}$ is positive definite.\n\n#### 1.2 The Curvature Condition and Exact Line Search\n\nThe preservation of positive definiteness hinges on the curvature condition $y_k^{\\top} s_k > 0$. For a convex quadratic objective $f(x)$ with a symmetric positive definite (SPD) Hessian $A$, this condition is guaranteed by an exact line search.\nThe step is $s_k = \\alpha_k p_k$, where $p_k = -H_k \\nabla f(x_k)$. Since $H_k$ is SPD, $p_k$ is a descent direction if $\\nabla f(x_k) \\ne 0$:\n$$ \\nabla f(x_k)^{\\top} p_k = - \\nabla f(x_k)^{\\top} H_k \\nabla f(x_k) < 0 $$\nAs shown before, for a quadratic, $y_k = A s_k$. Therefore:\n$$ y_k^{\\top} s_k = (A s_k)^{\\top} s_k = s_k^{\\top} A s_k $$\nSince $A$ is SPD, $s_k^{\\top} A s_k > 0$ for any non-zero $s_k$. The vector $s_k = \\alpha_k p_k$ is non-zero because $p_k \\ne 0$ (if not at optimum) and the step length $\\alpha_k$ from an exact line search will be positive for a descent direction.\n\n#### 1.3 Exact Line Search for a Quadratic Function\n\nThe exact line search step size $\\alpha_k$ minimizes $\\phi(\\alpha) = f(x_k + \\alpha p_k)$.\n$$ \\phi(\\alpha) = \\frac{1}{2}(x_k + \\alpha p_k)^{\\top}A(x_k + \\alpha p_k) - b^{\\top}(x_k + \\alpha p_k) $$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$ \\frac{d\\phi}{d\\alpha} = \\nabla f(x_k + \\alpha p_k)^{\\top} p_k = 0 $$\nThe gradient at the new point is $\\nabla f(x_k + \\alpha p_k) = A(x_k + \\alpha p_k) - b = (Ax_k - b) + \\alpha A p_k = \\nabla f(x_k) + \\alpha A p_k$.\nSubstituting this into the derivative equation:\n$$ (\\nabla f(x_k) + \\alpha A p_k)^{\\top} p_k = 0 $$\n$$ \\nabla f(x_k)^{\\top} p_k + \\alpha p_k^{\\top} A p_k = 0 $$\nSolving for $\\alpha$ gives the optimal step size $\\alpha_k$:\n$$ \\alpha_k = - \\frac{\\nabla f(x_k)^{\\top} p_k}{p_k^{\\top} A p_k} $$\nSince $p_k$ is a descent direction, the numerator is positive. Since $A$ is SPD, the denominator is positive. Thus, $\\alpha_k > 0$.\n\n### 2. Algorithm Summary and Implementation\n\nFor each test case, the algorithm proceeds as follows:\n1.  Initialize $x_0$ and $H_0$.\n2.  For $k = 0, 1, \\dots, K-1$:\n    a.  Compute the gradient $g_k = \\nabla f(x_k) = A x_k - b$.\n    b.  Check for convergence: if $\\| g_k \\|_2$ is below a tolerance ($10^{-12}$), terminate the loop.\n    c.  Compute the search direction $p_k = -H_k g_k$.\n    d.  Calculate the exact line search step size $\\alpha_k = - (g_k^{\\top} p_k) / (p_k^{\\top} A p_k)$. Check for a near-zero denominator ($10^{-14}$) and terminate if found.\n    e.  Update the position: $x_{k+1} = x_k + \\alpha_k p_k$.\n    f.  Calculate $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n    g.  Check the curvature condition: if $y_k^{\\top} s_k$ is below a tolerance ($10^{-14}$), terminate the loop.\n    h.  Perform the BFGS update to find $H_{k+1}$ using the formula derived above.\n    i.  Set $x_k \\leftarrow x_{k+1}$ and $H_k \\leftarrow H_{k+1}$.\n3.  After the loop finishes, compute the final metric $\\|H_K A - I\\|_2$, where $H_K$ is the final inverse Hessian approximation.\n\nThis procedure is implemented in the following Python code for each test case provided in the problem statement.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_bfgs(A, b, x0, H0, K_max):\n    \"\"\"\n    Performs BFGS iterations for a convex quadratic with exact line search.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the quadratic objective.\n        b (np.ndarray): The linear term vector of the quadratic objective.\n        x0 (np.ndarray): The initial point.\n        H0 (np.ndarray): The initial inverse Hessian approximation.\n        K_max (int): The maximum number of iterations.\n\n    Returns:\n        float: The spectral norm of (H_final * A - I).\n    \"\"\"\n    n = A.shape[0]\n    x_k = x0.copy()\n    H_k = H0.copy()\n    identity = np.identity(n)\n\n    for k in range(K_max):\n        # Step a: Compute gradient\n        grad_k = A @ x_k - b\n\n        # Step b: Check for convergence\n        grad_norm = np.linalg.norm(grad_k)\n        if grad_norm <= 1e-12:\n            break\n\n        # Step c: Compute search direction\n        p_k = -H_k @ grad_k\n        \n        # Step d: Compute exact line search step size\n        denom_alpha = p_k.T @ A @ p_k\n        if denom_alpha <= 1e-14:\n            break\n            \n        alpha_k = -(grad_k.T @ p_k) / denom_alpha\n\n        # Step e: Update position\n        x_k_plus_1 = x_k + alpha_k * p_k\n\n        # Step f: Define s_k and y_k\n        s_k = x_k_plus_1 - x_k\n        grad_k_plus_1 = A @ x_k_plus_1 - b\n        y_k = grad_k_plus_1 - grad_k\n        \n        # Step g: Check curvature condition\n        y_k_T_s_k = y_k.T @ s_k\n        if y_k_T_s_k <= 1e-14:\n            break\n            \n        # Step h: Perform the BFGS update\n        rho_k = 1.0 / y_k_T_s_k\n        \n        term1 = identity - rho_k * np.outer(s_k, y_k)\n        term2 = identity - rho_k * np.outer(y_k, s_k)\n        \n        H_k_plus_1 = term1 @ H_k @ term2 + rho_k * np.outer(s_k, s_k)\n\n        # Step i: Update for next iteration\n        x_k = x_k_plus_1\n        H_k = H_k_plus_1\n\n    # After loop, compute the final metric\n    metric = np.linalg.norm(H_k @ A - identity, ord=2)\n    return metric\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the results.\n    \"\"\"\n    # Case 1\n    A1 = np.array([[4, 1, 0], [1, 3, 0], [0, 0, 2]], dtype=float)\n    b1 = np.array([1, 2, 3], dtype=float)\n    x0_1 = np.array([0, 0, 0], dtype=float)\n    H0_1 = 0.1 * np.identity(3)\n    K1 = 3\n\n    # Case 2\n    A2 = np.array([[2, 0, 0], [0, 5, 0], [0, 0, 1]], dtype=float)\n    b2 = np.array([1, -1, 2], dtype=float)\n    x0_2 = np.array([0, 0, 0], dtype=float)\n    H0_2 = np.linalg.inv(A2)\n    K2 = 3\n\n    # Case 3\n    A3 = np.array([[1e-3, 0, 0], [0, 1, 0], [0, 0, 1e3]], dtype=float)\n    b3 = np.array([1, 1, 1], dtype=float)\n    x0_3 = np.array([0, 0, 0], dtype=float)\n    H0_3 = np.identity(3)\n    K3 = 3\n\n    # Case 4\n    A4 = np.array([[6, 2, 1], [2, 5, 0], [1, 0, 3]], dtype=float)\n    b4 = np.array([0.5, -1, 2], dtype=float)\n    x0_4 = np.array([0, 0, 0], dtype=float)\n    H0_4 = np.array([[3, 1, 0], [1, 1, 0], [0, 0, 0.5]], dtype=float)\n    K4 = 3\n    \n    test_cases = [\n        (A1, b1, x0_1, H0_1, K1),\n        (A2, b2, x0_2, H0_2, K2),\n        (A3, b3, x0_3, H0_3, K3),\n        (A4, b4, x0_4, H0_4, K4)\n    ]\n\n    results = []\n    for A, b, x0, H0, K_max in test_cases:\n        result = run_bfgs(A, b, x0, H0, K_max)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # print(f\"[{','.join(map(str, results))}]\") # This line is for local execution.\n```", "answer": "[2.220446049250313e-16,2.7755575615628914e-16,0.0,4.440892098500626e-16]", "id": "3166990"}]}