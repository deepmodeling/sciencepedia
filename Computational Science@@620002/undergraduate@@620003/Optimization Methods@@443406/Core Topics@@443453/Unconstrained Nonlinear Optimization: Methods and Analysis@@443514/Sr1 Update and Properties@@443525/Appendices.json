{"hands_on_practices": [{"introduction": "To truly grasp the Symmetric Rank-1 (SR1) update, we begin with its simplest form. This exercise guides you through deriving the SR1 formula in a one-dimensional setting, revealing its elegant connection to the classic secant method. By applying it to a quadratic function, you will demonstrate a core property of quasi-Newton methods: the ability to learn the exact curvature in a finite number of steps [@problem_id:3184208].", "problem": "Consider a one-dimensional unconstrained smooth minimization problem for a twice continuously differentiable objective function $f:\\mathbb{R}\\to\\mathbb{R}$ with gradient $g(x)=\\nabla f(x)$. In a Quasi-Newton (QN) method, a scalar curvature approximation $B_k$ to the Hessian $\\nabla^2 f(x_k)$ is maintained, and the step $s_k$ is defined by the linear model $B_k s_k = -g(x_k)$ with the new iterate $x_{k+1}=x_k+s_k$. The Symmetric Rank-One (SR1) update constructs $B_{k+1}$ to satisfy the QN secant condition $B_{k+1}s_k = y_k$, where $y_k := g(x_{k+1}) - g(x_k)$, using a rank-one correction to $B_k$ and requiring the update to be well-defined in the one-dimensional case whenever $y_k - B_k s_k \\neq 0$.\n\n- Task 1 (analytical): Starting only from the QN secant condition $B_{k+1}s_k = y_k$ and the fact that SR1 makes a rank-one change to $B_k$ in one dimension, derive the expression for $B_{k+1}$ in terms of $y_k$ and $s_k$ when $y_k - B_k s_k \\neq 0$. Then, using the structure of a quadratic function $f(x) = \\frac{1}{2} a x^2 + b x + c$ with constant curvature $a > 0$, explain why the one-dimensional SR1 update attains the exact Newton curvature after a single step in this setting, and show how this implies that the subsequent QN step coincides with the Newton step for this quadratic.\n\n- Task 2 (piecewise linear-quadratic illustration): Consider the piecewise quadratic objective\n$$\nf(x) = \n\\begin{cases}\n\\frac{1}{2}\\cdot 3\\, x^2 - 1\\cdot x, & x \\leq 0, \\\\\n\\frac{1}{2}\\cdot 1\\, x^2 - 1\\cdot x, & x > 0,\n\\end{cases}\n$$\nwhich is continuous at $x=0$. Let $x_0 = -\\frac{1}{4}$ and $B_0 = 2$. Perform one QN step to get $x_1 = x_0 + s_0$ from $B_0 s_0 = -g(x_0)$, then compute $y_0 = g(x_1) - g(x_0)$ and use your result from Task 1 to obtain $B_1$. Finally, take one more QN step from $x_1$ using $B_1$ to obtain $x_2 = x_1 + s_1$ with $B_1 s_1 = -g(x_1)$. Provide the value of $x_2$ rounded to five significant figures. No physical units are involved; report the number as a pure real value.", "solution": "The user has provided a two-part problem concerning the Symmetric Rank-One (SR1) update for a one-dimensional Quasi-Newton (QN) optimization method.\n\nFirst, the problem's validity is assessed.\n- **Step 1: Extract Givens**\n  - Problem context: One-dimensional unconstrained smooth minimization of a twice continuously differentiable objective function $f:\\mathbb{R}\\to\\mathbb{R}$.\n  - Gradient: $g(x) = \\nabla f(x)$.\n  - QN step: $x_{k+1} = x_k + s_k$, where $B_k s_k = -g(x_k)$. $B_k$ is a scalar approximation to the Hessian $\\nabla^2 f(x_k)$.\n  - Secant condition: $B_{k+1}s_k = y_k$, where $y_k = g(x_{k+1}) - g(x_k)$.\n  - SR1 property: $B_{k+1}$ is formed by a symmetric rank-one correction to $B_k$.\n  - 1D well-definedness condition: The update is well-defined whenever $y_k - B_k s_k \\neq 0$.\n  - **Task 1**: Derive the 1D SR1 update for $B_{k+1}$. Analyze its behavior for a quadratic function $f(x) = \\frac{1}{2} a x^2 + b x + c$ ($a > 0$), showing it computes the exact curvature in one step and that the subsequent QN step is a Newton step.\n  - **Task 2**: A numerical illustration with $f(x) = \\begin{cases} \\frac{1}{2}\\cdot 3\\, x^2 - 1\\cdot x, & x \\leq 0, \\\\ \\frac{1}{2}\\cdot 1\\, x^2 - 1\\cdot x, & x > 0. \\end{cases}$\n  - Initial conditions for Task 2: $x_0 = -\\frac{1}{4}$ and $B_0 = 2$.\n  - Required output for Task 2: Value of the second iterate $x_2$, rounded to five significant figures.\n\n- **Step 2: Validate Using Extracted Givens**\n  - The problem is scientifically grounded in the field of numerical optimization.\n  - The problem is well-posed, with all necessary information provided for both tasks.\n  - There is a minor inconsistency: the introduction specifies a twice continuously differentiable function, while Task 2 uses a function that is only once continuously differentiable (the second derivative is discontinuous at $x=0$). However, the mechanics of the QN steps and the SR1 update only require the existence of the gradient, which is continuous for the given piecewise function. Therefore, all calculations are well-defined and can be performed as requested. This inconsistency does not invalidate the problem but rather serves to test the application of the algorithm in a non-ideal but tractable scenario.\n  \n- **Step 3: Verdict and Action**\n  - The problem is deemed valid. A full solution will be provided.\n\n### Task 1: Analytical Derivation and Properties\n\nThe SR1 update requires that the new Hessian approximation, $B_{k+1}$, is obtained by a symmetric rank-one correction to the previous approximation, $B_k$. In a one-dimensional setting, $B_k$ and $B_{k+1}$ are scalars. A rank-one correction matrix is also a scalar. Let us denote this correction by $c_k$. Thus, the update has the form:\n$$B_{k+1} = B_k + c_k$$\nThe symmetry requirement is trivially satisfied as scalars are symmetric.\n\nThis new approximation must satisfy the secant condition:\n$$B_{k+1} s_k = y_k$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = g(x_{k+1}) - g(x_k)$.\n\nSubstituting the update form into the secant condition gives:\n$$(B_k + c_k) s_k = y_k$$\n$$B_k s_k + c_k s_k = y_k$$\nWe can solve for the correction term $c_k$. Assuming we are not at a stationary point, $g(x_k) \\neq 0$, which implies $s_k \\neq 0$ (unless $B_k=0$, a degenerate case). We can therefore divide by $s_k$:\n$$c_k = \\frac{y_k - B_k s_k}{s_k}$$\nThe problem states that the update is well-defined whenever $y_k - B_k s_k \\neq 0$. This condition ensures that the correction $c_k$ is non-zero, meaning the approximation is meaningfully updated.\n\nSubstituting the expression for $c_k$ back into the update formula for $B_{k+1}$:\n$$B_{k+1} = B_k + \\frac{y_k - B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - \\frac{B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - B_k$$\nThis simplifies to:\n$$B_{k+1} = \\frac{y_k}{s_k}$$\nThis expression is the one-dimensional SR1 update formula. It is equivalent to the secant method's approximation of the second derivative.\n\nNow, consider the quadratic objective function $f(x) = \\frac{1}{2} a x^2 + b x + c$, with constant curvature $a > 0$.\nThe gradient is $g(x) = a x + b$.\nThe exact Hessian is $\\nabla^2 f(x) = a$ for all $x$.\n\nLet's analyze one step of the QN method starting from an arbitrary iterate $x_k$ and an arbitrary initial Hessian approximation $B_k$. The step is $x_{k+1} = x_k + s_k$. The gradient difference is:\n$$y_k = g(x_{k+1}) - g(x_k) = (a x_{k+1} + b) - (a x_k + b) = a (x_{k+1} - x_k) = a s_k$$\nNow, we compute the updated Hessian approximation $B_{k+1}$ using the derived SR1 formula:\n$$B_{k+1} = \\frac{y_k}{s_k} = \\frac{a s_k}{s_k} = a$$\nThis demonstrates that after a single step on a quadratic function, the 1D SR1 update yields the exact Hessian (curvature), $a$, regardless of the initial approximation $B_k$.\n\nThe subsequent QN step, $s_{k+1}$, is determined by the equation $B_{k+1} s_{k+1} = -g(x_{k+1})$. Since we have shown that $B_{k+1} = a$, this equation becomes:\n$$a s_{k+1} = -g(x_{k+1})$$\nA Newton step, $p_{k+1}$, is defined by the equation $\\nabla^2 f(x_{k+1}) p_{k+1} = -g(x_{k+1})$. For the quadratic function, $\\nabla^2 f(x_{k+1}) = a$. The Newton step equation is:\n$$a p_{k+1} = -g(x_{k+1})$$\nComparing the two equations, we see that $s_{k+1} = p_{k+1}$. Therefore, the QN step following the SR1 update is identical to the Newton step. For a quadratic function, the Newton step from any point $x$ converges to the minimizer in a single iteration.\n\n### Task 2: Piecewise Linear-Quadratic Illustration\n\nWe are given the objective function:\n$$f(x) = \n\\begin{cases}\n\\frac{3}{2} x^2 - x, & x \\leq 0, \\\\\n\\frac{1}{2} x^2 - x, & x > 0,\n\\end{cases}\n$$\nThe gradient is:\n$$g(x) = f'(x) = \n\\begin{cases}\n3x - 1, & x \\leq 0, \\\\\nx - 1, & x > 0.\n\\end{cases}\n$$\nThe initial conditions are $x_0 = -\\frac{1}{4}$ and $B_0 = 2$.\n\n**Step 1: Compute $x_1$**\nFirst, we compute the gradient at $x_0$. Since $x_0 = -0.25 \\leq 0$, we use the first case for $g(x)$:\n$$g(x_0) = g(-0.25) = 3(-0.25) - 1 = -0.75 - 1 = -1.75$$\nThe first QN step $s_0$ is found from $B_0 s_0 = -g(x_0)$:\n$$2 s_0 = -(-1.75) = 1.75$$\n$$s_0 = \\frac{1.75}{2} = 0.875$$\nThe next iterate $x_1$ is:\n$$x_1 = x_0 + s_0 = -0.25 + 0.875 = 0.625$$\n\n**Step 2: Compute $B_1$**\nTo compute the updated Hessian approximation $B_1$, we use the formula $B_1 = y_0/s_0$. We first need to compute $y_0 = g(x_1) - g(x_0)$.\nWe have $x_1 = 0.625 > 0$, so we use the second case for $g(x)$:\n$$g(x_1) = g(0.625) = 0.625 - 1 = -0.375$$\nNow we compute $y_0$:\n$$y_0 = g(x_1) - g(x_0) = -0.375 - (-1.75) = 1.375$$\nWe can now find $B_1$:\n$$B_1 = \\frac{y_0}{s_0} = \\frac{1.375}{0.875} = \\frac{11/8}{7/8} = \\frac{11}{7}$$\n\n**Step 3: Compute $x_2$**\nThe second QN step $s_1$ is found from $B_1 s_1 = -g(x_1)$:\n$$\\frac{11}{7} s_1 = -(-0.375) = 0.375 = \\frac{3}{8}$$\nSolving for $s_1$:\n$$s_1 = \\frac{3}{8} \\cdot \\frac{7}{11} = \\frac{21}{88}$$\nThe final iterate $x_2$ is:\n$$x_2 = x_1 + s_1 = 0.625 + \\frac{21}{88} = \\frac{5}{8} + \\frac{21}{88} = \\frac{5 \\cdot 11}{88} + \\frac{21}{88} = \\frac{55+21}{88} = \\frac{76}{88} = \\frac{19}{22}$$\nTo provide the numerical answer, we compute the decimal value of $x_2$:\n$$x_2 = \\frac{19}{22} \\approx 0.86363636...$$\nRounding to five significant figures gives:\n$$x_2 \\approx 0.86364$$", "answer": "$$\n\\boxed{0.86364}\n$$", "id": "3184208"}, {"introduction": "The SR1 method's most significant advantage over its popular counterpart, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method, is its ability to effectively handle non-convexity. This practice delves into the behavior of SR1 near a saddle point, a common feature in complex optimization landscapes. Through a carefully designed experiment, you will see firsthand how SR1's capacity to generate indefinite Hessian approximations allows it to identify and exploit escape directions from saddle points, a task where standard BFGS often falters [@problem_id:3184299].", "problem": "Consider the quadratic objective function defined for $x \\in \\mathbb{R}^2$ by\n$$\nf(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2,\n$$\nwhere $\\alpha \\in \\mathbb{R}$ is a parameter. Let $H \\in \\mathbb{R}^{2 \\times 2}$ denote the symmetric Hessian matrix of $f$, and let $g(x) = \\nabla f(x)$ denote the gradient. The point $x = 0$ is a saddle point. In the context of quasi-Newton methods, two standard Hessian approximation updates are Symmetric Rank-One (SR1) and Broyden-Fletcher-Goldfarb-Shanno (BFGS). The SR1 update allows indefinite approximations, while the BFGS update is commonly used with a curvature condition that enforces positive definiteness.\n\nStarting from the following fundamental base:\n- For a twice continuously differentiable quadratic objective of the form $f(x) = \\tfrac{1}{2} x^\\top H x$ with $H$ symmetric, the gradient is $g(x) = H x$ and the Hessian is the constant matrix $H$.\n- A saddle point of a twice differentiable function is characterized locally by an indefinite Hessian, meaning $H$ has both positive and negative eigenvalues.\n- Quasi-Newton updates construct a sequence of symmetric matrices $\\{B_k\\}$ that aim to approximate the true Hessian $H$ by enforcing the secant condition $B_{k+1} s_k \\approx y_k$, where $s_k = x_{k+1} - x_k$ and $y_k = g(x_{k+1}) - g(x_k)$.\n\nTasks:\n1. Construct $H$ and $g(x)$ for the given $f(x)$ starting from the fundamental base above, and determine the eigenvalues and eigenvectors of $H$. Use these to identify the negative-curvature eigendirection $v_{-}$ associated with the smallest (most negative) eigenvalue $\\lambda_{-}$ of $H$.\n2. Explain, using first principles and the secant condition, why enforcing positive definiteness in the approximate Hessian may suppress detection of negative curvature near a saddle, and why allowing indefiniteness (as SR1 does) can encode the negative curvature direction into the approximation.\n3. Implement a single-update comparison experiment as follows:\n   - Initialize the approximate Hessian with $B_0 = I$, the identity matrix.\n   - For a given $\\alpha$ and a given starting point $x^{(0)} \\in \\mathbb{R}^2$, form a small step $s = h \\, v_{-}$ with $h = 0.1$, where $v_{-}$ is the unit eigenvector of $H$ corresponding to $\\lambda_{-}$. Set $x^{(1)} = x^{(0)} + s$ and compute $y = g(x^{(1)}) - g(x^{(0)})$. For a quadratic $f$, this simplifies to $y = H s$.\n   - Apply the SR1 update to obtain $B_{\\mathrm{SR1}}$ from $B_0$ and the pair $(s,y)$.\n   - Apply the BFGS update to obtain $B_{\\mathrm{BFGS}}$ from $B_0$ and the pair $(s,y)$, but enforce the standard curvature condition by skipping the update entirely when $s^\\top y \\le 0$ so that $B_{\\mathrm{BFGS}} = B_0$ in that case.\n   - For each $B$ in $\\{B_{\\mathrm{SR1}}, B_{\\mathrm{BFGS}}\\}$, compute its eigen-decomposition and extract its smallest eigenvalue and corresponding unit eigenvector. Determine whether $B$ captures negative curvature (i.e., has a negative smallest eigenvalue) and whether its smallest-eigenvector aligns with the true negative-curvature eigendirection $v_{-}$, quantified by the absolute cosine $|\\langle v_{-}, v_B \\rangle|$ between the unit vectors. Use a threshold of $0.9$ for declaring a strong alignment.\n4. Using the above procedure, evaluate the following test suite of parameter settings:\n   - Test case $1$: $\\alpha = 0$, $x^{(0)} = (0.2, -0.05)$.\n   - Test case $2$: $\\alpha = 1$, $x^{(0)} = (-0.1, 0.15)$.\n   - Test case $3$: $\\alpha = 3$, $x^{(0)} = (0.05, 0.06)$.\n   The scalar step size in the construction of $s$ is $h = 0.1$ in all cases.\n5. For each test case, produce a boolean result that is true if and only if $B_{\\mathrm{SR1}}$ both (i) has a negative smallest eigenvalue and (ii) its smallest-eigenvector has absolute cosine with $v_{-}$ at least $0.9$, while simultaneously $B_{\\mathrm{BFGS}}$ fails to have any negative eigenvalue. This boolean is intended to quantify the advantage of SR1’s allowance of indefiniteness for encoding the correct escape direction near the saddle.\n   \nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three test cases in order, for example: $[{\\tt result1},{\\tt result2},{\\tt result3}]$, where each ${\\tt resulti}$ is either ${\\tt True}$ or ${\\tt False}$.\n- No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical optimization and multivariable calculus, is well-posed with a clear objective and sufficient data, and uses precise, objective language. We may therefore proceed with a solution.\n\nThe core of the problem is to compare the behavior of the Symmetric Rank-One (SR1) and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton updates in the vicinity of a saddle point. The analysis hinges on how each method's Hessian approximation, $B$, evolves when a step is taken along a direction of negative curvature.\n\n### Task 1: System Characterization (Gradient, Hessian, and Eigendecomposition)\n\nThe given objective function is $f(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2$ for $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$.\n\nFirst, we compute the gradient vector $g(x) = \\nabla f(x)$:\n$$\ng(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 + \\alpha x_2 \\\\ -2x_2 + \\alpha x_1 \\end{pmatrix}\n$$\nThis can be written in matrix form as $g(x) = Hx$, where the matrix $H$ is:\n$$\nH = \\begin{pmatrix} 2 & \\alpha \\\\ \\alpha & -2 \\end{pmatrix}\n$$\nThe Hessian of $f(x)$ is the matrix of second partial derivatives, $\\nabla^2 f(x)$, which is indeed the constant, symmetric matrix $H$. The problem states that for a quadratic function of the form $f(x) = \\frac{1}{2} x^\\top H x$, the gradient is $g(x) = Hx$. Our function satisfies this structure.\n\nNext, we find the eigenvalues of $H$ by solving the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 2-\\lambda & \\alpha \\\\ \\alpha & -2-\\lambda \\end{pmatrix} = (2-\\lambda)(-2-\\lambda) - \\alpha^2 = -(4-\\lambda^2) - \\alpha^2 = \\lambda^2 - 4 - \\alpha^2 = 0\n$$\nThis gives $\\lambda^2 = 4 + \\alpha^2$, so the eigenvalues are $\\lambda = \\pm\\sqrt{4 + \\alpha^2}$.\nAs $\\sqrt{4+\\alpha^2} > 0$ for any real $\\alpha$, $H$ has one positive eigenvalue, $\\lambda_+ = \\sqrt{4+\\alpha^2}$, and one negative eigenvalue, $\\lambda_- = -\\sqrt{4+\\alpha^2}$. This confirms that the Hessian is indefinite and the origin $x=0$ (where $g(0)=0$) is a saddle point.\n\nThe negative-curvature eigendirection $v_{-}$ is the eigenvector corresponding to the smallest (most negative) eigenvalue $\\lambda_{-}$. We solve the system $(H - \\lambda_{-}I)v_{-} = 0$:\n$$\n\\begin{pmatrix} 2 - \\lambda_{-} & \\alpha \\\\ \\alpha & -2 - \\lambda_{-} \\end{pmatrix} v_{-} = \\begin{pmatrix} 2 + \\sqrt{4+\\alpha^2} & \\alpha \\\\ \\alpha & -2 + \\sqrt{4+\\alpha^2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get $(2 + \\sqrt{4+\\alpha^2})v_1 + \\alpha v_2 = 0$. An unnormalized eigenvector is thus proportional to $\\begin{pmatrix} \\alpha \\\\ -(2+\\sqrt{4+\\alpha^2}) \\end{pmatrix}$. The vector $v_{-}$ required by the problem is the unit-norm version of this vector.\n\n### Task 2: Theoretical Underpinnings of SR1 and BFGS Updates\n\nThe secant condition, $B_{k+1}s_k = y_k$, is central to quasi-Newton methods. Here, $s_k = x_{k+1} - x_k$ and $y_k = g_{k+1} - g_k$. For a quadratic function with Hessian $H$, we have $y_k = H s_k$.\n\n**BFGS Update and Positive Definiteness:**\nThe standard BFGS update formula is:\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k} + \\frac{y_k y_k^\\top}{y_k^\\top s_k}\n$$\nA key property of this update is that if $B_k$ is positive definite and the curvature condition $y_k^\\top s_k > 0$ holds, then $B_{k+1}$ is also positive definite. The curvature condition $y_k^\\top s_k = s_k^\\top H s_k$ checks if the function's curvature along the direction $s_k$ is positive. Near a saddle point, it is possible to choose a step $s_k$ such that $s_k^\\top H s_k \\le 0$. This corresponds to moving along a direction of non-positive curvature. As specified, optimization algorithms using BFGS often enforce positive definiteness by skipping the update when $y_k^\\top s_k \\le 0$. If we start with $B_0 = I$ (which is positive definite) and only perform updates when $y_k^\\top s_k > 0$, the entire sequence of Hessian approximations $\\{B_k\\}$ remains positive definite. A positive definite matrix can only have positive eigenvalues. Consequently, such a strategy prevents the Hessian approximation from ever capturing the negative curvature (i.e., negative eigenvalues) of the true Hessian $H$ near a saddle point.\n\n**SR1 Update and Indefiniteness:**\nThe SR1 update formula is:\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\nUnlike BFGS, the denominator $(y_k - B_k s_k)^\\top s_k$ in the SR1 formula can be positive, negative, or zero. If the denominator is negative, the rank-one term subtracted from $B_k$ can introduce negative eigenvalues, allowing $B_{k+1}$ to become indefinite. This is crucial for modeling saddle points.\n\nConsider the specific step proposed in the problem: $s = h v_{-}$, where $v_{-}$ is the negative-curvature eigendirection. For this step, $y = H s = H(h v_{-}) = h (H v_{-}) = h (\\lambda_{-} v_{-}) = \\lambda_{-} s$.\nLet's analyze the SR1 update starting from $B_0 = I$:\nThe numerator term is $(y - B_0 s)(y - B_0 s)^\\top = (\\lambda_{-} s - s)(\\lambda_{-} s - s)^\\top = (\\lambda_{-}-1)^2 s s^\\top$.\nThe denominator is $(y - B_0 s)^\\top s = (\\lambda_{-} s - s)^\\top s = (\\lambda_{-} - 1)s^\\top s = (\\lambda_{-} - 1)\\|s\\|^2$.\nSince $\\lambda_{-} = -\\sqrt{4+\\alpha^2} \\le -2$, the denominator is strictly negative.\nThe update is:\n$$\nB_{\\mathrm{SR1}} = I + \\frac{(\\lambda_{-}-1)^2 s s^\\top}{(\\lambda_{-} - 1)\\|s\\|^2} = I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top\n$$\nLet's test the action of $B_{\\mathrm{SR1}}$ on the step vector $s$:\n$$\nB_{\\mathrm{SR1}} s = \\left( I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top \\right) s = s + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s (s^\\top s) = s + (\\lambda_{-} - 1)s = \\lambda_{-} s\n$$\nThis shows that $s$ (and thus $v_{-}$) is an eigenvector of the updated matrix $B_{\\mathrm{SR1}}$ with the eigenvalue $\\lambda_{-}$. Since $\\lambda_{-}$ is negative, the SR1 update has successfully incorporated the negative curvature of the true Hessian $H$ into its approximation. Any vector $w$ orthogonal to $s$ is an eigenvector with eigenvalue $1$, since $s^\\top w=0$ implies $B_{\\mathrm{SR1}}w = (I + \\dots)w = Iw = w$. Thus, $B_{\\mathrm{SR1}}$ has eigenvalues $\\lambda_{-}$ and $1$, correctly capturing the negative curvature.\n\n### Tasks 3-5: Implementation and Evaluation\n\nThe logic described above is implemented for the three specified test cases. For each case, we construct $H$, find its negative eigendirection $v_{-}$, and compute the step $s$ and gradient difference $y$. We then apply the SR1 and BFGS update rules.\n\nFor the BFGS update, the crucial quantity is the curvature $s^\\top y$. In our setup, $s^\\top y = s^\\top (\\lambda_{-} s) = \\lambda_{-} \\|s\\|^2$. Since $\\lambda_{-} < 0$ and $\\|s\\|^2 > 0$, the curvature is always negative. Therefore, the BFGS update is skipped in all test cases, and $B_{\\mathrm{BFGS}}$ remains the identity matrix $I$, which has no negative eigenvalues.\n\nFor the SR1 update, as shown analytically, the resulting matrix $B_{\\mathrm{SR1}}$ will have $v_{-}$ as an eigenvector with eigenvalue $\\lambda_{-}$. This means $B_{\\mathrm{SR1}}$ will have a negative eigenvalue, and its corresponding eigenvector will be perfectly aligned with the true negative-curvature direction $v_{-}$.\n\nConsequently, for all test cases, the condition for the boolean result will be met:\n1. $B_{\\mathrm{SR1}}$ has a negative smallest eigenvalue ($\\lambda_{-}$).\n2. The corresponding eigenvector $v_{B,SR1}$ is $v_{-}$, so the absolute cosine $|\\langle v_{-}, v_{B,SR1} \\rangle|$ is $1$, which is $\\ge 0.9$.\n3. $B_{\\mathrm{BFGS}}$ remains $I$ and has no negative eigenvalues.\n\nThe final boolean result will be true for all three test cases. The provided Python code executes this procedure numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a single-step comparison of SR1 and BFGS updates near a saddle point\n    for a given quadratic function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x0_tuple)\n        (0.0, (0.2, -0.05)),\n        (1.0, (-0.1, 0.15)),\n        (3.0, (0.05, 0.06)),\n    ]\n\n    results = []\n    \n    # Parameters for the experiment\n    h = 0.1\n    alignment_threshold = 0.9\n\n    for alpha, x0_tuple in test_cases:\n        x0 = np.array(x0_tuple) # x0 is not used in the calculations as per problem logic\n        \n        # 1. Construct H and find its negative-curvature eigendirection\n        H = np.array([\n            [2.0, alpha],\n            [alpha, -2.0]\n        ])\n        \n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues, eigenvectors = np.linalg.eigh(H)\n        lambda_minus = eigenvalues[0]\n        v_minus = eigenvectors[:, 0]\n\n        # 3. Form the step s and gradient difference y\n        B0 = np.eye(2)\n        s = h * v_minus\n        \n        # For a quadratic function f(x) = 1/2 * x^T*H*x, y = g(x1)-g(x0) = Hx1-Hx0 = H(x1-x0) = Hs\n        y = H @ s\n        \n        # --- SR1 Update ---\n        y_minus_B0s = y - B0 @ s\n        den_sr1 = (y_minus_B0s).T @ s\n        # As shown in the analysis, den_sr1 is non-zero for this problem setup.\n        B_sr1 = B0 + np.outer(y_minus_B0s, y_minus_B0s) / den_sr1\n\n        # --- BFGS Update (with curvature check) ---\n        sTy = s.T @ y\n        if sTy > 0:\n            # This branch is not taken in this specific problem, as s^T*y will be negative.\n            B0s = B0 @ s\n            sTB0s = s.T @ B0s\n            B_bfgs = B0 - np.outer(B0s, B0s) / sTB0s + np.outer(y, y) / sTy\n        else:\n            # Curvature condition s^T*y <= 0 is met, so we skip the BFGS update.\n            B_bfgs = B0\n\n            \n        # 4. Analyze the resulting Hessian approximations\n        \n        # SR1 Analysis\n        eigvals_sr1, eigvecs_sr1 = np.linalg.eigh(B_sr1)\n        sr1_min_eigval = eigvals_sr1[0]\n        v_B_sr1 = eigvecs_sr1[:, 0]\n        \n        sr1_has_neg_curv = sr1_min_eigval < 0\n        # The dot product of two unit vectors is the cosine of the angle between them.\n        sr1_is_aligned = abs(v_minus.T @ v_B_sr1) >= alignment_threshold\n        sr1_condition_met = sr1_has_neg_curv and sr1_is_aligned\n\n        # BFGS Analysis\n        eigvals_bfgs, _ = np.linalg.eigh(B_bfgs)\n        bfgs_min_eigval = eigvals_bfgs[0]\n        bfgs_has_neg_curv = bfgs_min_eigval < 0\n\n        # 5. Determine the final boolean result for the test case\n        # The result is True if SR1 succeeds and BFGS fails to capture negative curvature.\n        final_result = sr1_condition_met and not bfgs_has_neg_curv\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3184299"}, {"introduction": "In practical optimization, no single method is universally superior; the best approach often combines the strengths of multiple techniques. This final practice challenges you to build a sophisticated hybrid algorithm that leverages the robustness of BFGS in convex regions and the agility of SR1 when non-convexity is detected. By implementing this switching strategy, you will synthesize your understanding of both methods to create a powerful and practical optimization tool [@problem_id:3184218].", "problem": "Consider the unconstrained minimization of a continuously differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$. Let $x_k\\in\\mathbb{R}^n$ denote the iterate at step $k$, $g_k=\\nabla f(x_k)$ the gradient, and $H_k\\in\\mathbb{R}^{n\\times n}$ an approximation to the inverse Hessian matrix. A quasi-Newton method computes a search direction $p_k=-H_k g_k$ and updates $H_k$ using information from the step $s_k=x_{k+1}-x_k$ and the gradient change $y_k=g_{k+1}-g_k$.\n\nProblem: Implement a multi-stage quasi-Newton algorithm that begins with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update and, upon detecting nonconvexity through negative curvature, permanently switches to the Symmetric Rank-One (SR1) update. The algorithm must adhere to the following specification derived from core definitions and well-tested optimization facts:\n\n- Start with $H_0=I$.\n- At iteration $k$, compute $p_k=-H_k g_k$. If $g_k^\\top p_k\\ge 0$, replace $p_k$ by the steepest descent direction $p_k=-g_k$ to enforce a descent step.\n- Use a backtracking Armijo line search to find a step length $\\alpha_k$: initialize with $\\alpha_k=1$, and reduce $\\alpha_k$ by a constant factor until the Armijo condition $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$ is satisfied. Use $c_1=10^{-4}$ and a shrink factor of $1/2$. All trigonometric function arguments are angles in radians.\n- Define $s_k=x_{k+1}-x_k$ and $y_k=g_{k+1}-g_k$ after the line search is accepted.\n- Negative curvature detection and stage switching rule: while in BFGS stage, if the curvature condition $s_k^\\top y_k>\\theta$ holds, continue with BFGS; otherwise, detect negative curvature and permanently switch to SR1 updates from the next update onward. Use the threshold $\\theta=10^{-10}$.\n- In SR1 stage, apply SR1 updates whenever the update is well-defined; if the SR1 denominator is too small in magnitude, skip the update.\n- Terminate when $\\lVert g_k\\rVert_2\\le\\varepsilon$ with $\\varepsilon=10^{-6}$ or when the iteration count reaches the maximum of $200$.\n\nYou must test the implementation on the following set of objective functions, which includes convex and nonconvex cases:\n\n$1.$ Convex quadratic: $f(x)=\\tfrac{1}{2}x^\\top A x + b^\\top x$ with $A=\\begin{bmatrix}4 & 1\\\\ 1 & 3\\end{bmatrix}$ and $b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$, starting point $x_0=\\begin{bmatrix}-2\\\\ 2\\end{bmatrix}$.\n\n$2.$ Rosenbrock function: $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$, starting point $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$.\n\n$3.$ Saddle-quartic (nonconvex but bounded below): $f(x_1,x_2)=x_1^2-x_2^2+0.1(x_1^4+x_2^4)$, starting point $x_0=\\begin{bmatrix}0\\\\ 0.2\\end{bmatrix}$.\n\n$4.$ Quadratic plus trigonometric perturbation (mixed curvature): $f(x)=\\tfrac{1}{2}x^\\top A x+0.2\\sin(3x_1)+0.2\\cos(2x_2)$ with $A=\\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}$, starting point $x_0=\\begin{bmatrix}1.5\\\\ -1\\end{bmatrix}$.\n\nFor each test case, your program must return three values: the final objective value $f(x_{\\text{final}})$ rounded to six decimal places, the number of iterations (an integer), and a boolean flag indicating whether the algorithm switched to SR1 at least once. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself formatted as a list, for example $[ [f_1,i_1,\\text{flag}_1],[f_2,i_2,\\text{flag}_2],\\dots ]$. The boolean flag must be printed in lowercase as either `true` or `false`. No other output is permitted.\n\nTest Suite Summary:\n- Case $1$: Convex quadratic with $A=\\begin{bmatrix}4 & 1\\\\ 1 & 3\\end{bmatrix}$, $b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$, $x_0=\\begin{bmatrix}-2\\\\ 2\\end{bmatrix}$.\n- Case $2$: Rosenbrock with $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$.\n- Case $3$: Saddle-quartic with $x_0=\\begin{bmatrix}0\\\\ 0.2\\end{bmatrix}$.\n- Case $4$: Quadratic plus trigonometric with $A=\\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}$, $x_0=\\begin{bmatrix}1.5\\\\ -1\\end{bmatrix}$.\n\nAll functions are purely mathematical; no physical units are involved. Angles for trigonometric functions are in radians. The final output format must strictly follow the single-line specification above.", "solution": "The user has provided a well-defined problem in numerical optimization. The task is to implement a hybrid quasi-Newton algorithm that starts with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update for the inverse Hessian approximation and permanently switches to the Symmetric Rank-One (SR1) update upon detection of non-positive curvature. This response will detail the validation of the problem statement and present a full solution and implementation.\n\n**Problem Validation**\n\nThe problem is subjected to rigorous validation against the specified criteria.\n\n-   **Step 1: Extracted Givens**\n    -   **Algorithm Type**: Unconstrained minimization using a multi-stage quasi-Newton method.\n    -   **Objective Function**: $f:\\mathbb{R}^n\\to\\mathbb{R}$, continuously differentiable.\n    -   **Initial State**: Iterate $x_0$, inverse Hessian approximation $H_0=I$ (identity matrix).\n    -   **Iterative Process**:\n        -   Gradient: $g_k = \\nabla f(x_k)$.\n        -   Search Direction: $p_k = -H_k g_k$.\n        -   Descent Safeguard: If $g_k^\\top p_k \\ge 0$, set $p_k = -g_k$.\n        -   Line Search: Backtracking Armijo with condition $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$.\n        -   Line Search Parameters: Initial $\\alpha_k=1$, $c_1=10^{-4}$, shrink factor $1/2$.\n        -   Updates: $s_k=x_{k+1}-x_k = \\alpha_k p_k$, $y_k=g_{k+1}-g_k = \\nabla f(x_{k+1})-\\nabla f(x_k)$.\n    -   **Hybrid Strategy**:\n        -   Starts in BFGS mode.\n        -   Curvature Condition: $s_k^\\top y_k > \\theta$ with $\\theta=10^{-10}$.\n        -   Switching Rule: If $s_k^\\top y_k \\le \\theta$, permanently switch to SR1 mode.\n    -   **Quasi-Newton Updates**:\n        -   BFGS (if in BFGS mode and curvature condition holds).\n        -   SR1 (if in SR1 mode), with a skip condition if the denominator is \"too small in magnitude\".\n    -   **Termination**: $\\lVert g_k\\rVert_2 \\le \\varepsilon$ with $\\varepsilon=10^{-6}$, or maximum iterations $k_{\\text{max}} = 200$.\n    -   **Test Suite**: Four specified objective functions with their respective starting points.\n    -   **Output Requirement**: For each test case, return $[f(x_{\\text{final}}), \\text{iterations}, \\text{SR1 used flag}]$. The final objective value is rounded to six decimal places.\n\n-   **Step 2: Validation Using Extracted Givens**\n    -   **Scientific Grounding**: The problem is well-grounded in the theory of numerical optimization. BFGS and SR1 are canonical quasi-Newton methods. The condition $s_k^\\top y_k > 0$ is essential for BFGS to maintain positive definiteness. Using its violation as a trigger to switch to the more general SR1 method (which does not require positive definiteness) is a standard and sound heuristic for handling non-convexity. The Armijo line search and descent direction safeguards are also standard components. The test functions are classical benchmarks in the field.\n    -   **Well-Posedness**: The problem is well-posed. The algorithm is clearly specified, and for the given smooth functions, it is expected to generate a sequence of iterates that converges to a stationary point. The termination criteria ensure the algorithm halts. The only minor ambiguity (\"denominator is too small in magnitude\" for SR1) is a standard implementation detail that can be resolved with a reasonable threshold (e.g., $|(s_k - H_k y_k)^\\top y_k| < 10^{-8}$), making the problem fully formalizable.\n    -   **Objectivity**: The problem is stated in precise, objective mathematical language.\n    -   **Conclusion**: No flaws are identified. The problem is valid.\n\n-   **Step 3: Verdict and Action**\n    The problem is valid. A complete solution will be provided.\n\n**Solution Methodology**\n\nThe specified hybrid quasi-Newton algorithm is implemented to solve the unconstrained optimization problem $\\min_{x \\in \\mathbb{R}^n} f(x)$. The method generates a sequence of iterates $\\{x_k\\}$ via the update rule $x_{k+1} = x_k + s_k$, where $s_k = \\alpha_k p_k$ is the step. Here, $p_k$ is the search direction and $\\alpha_k$ is the step length.\n\n1.  **Initialization**: The algorithm starts at a given point $x_0$. The iteration counter is set to $k=0$. The initial approximation to the inverse Hessian matrix is the $n \\times n$ identity matrix, $H_0 = I$. A flag tracks the algorithm's mode, initialized to `BFGS`.\n\n2.  **Main Iteration Loop**: At each iteration $k = 0, 1, 2, \\dots$:\n    a. **Termination Check**: Compute the gradient $g_k = \\nabla f(x_k)$. If its Euclidean norm satisfies $\\lVert g_k \\rVert_2 \\le \\varepsilon = 10^{-6}$, the algorithm terminates, as a stationary point has been found with sufficient accuracy. The loop also terminates if $k$ reaches the maximum of $200$.\n\n    b. **Search Direction**: The search direction is computed using the current inverse Hessian approximation: $p_k = -H_k g_k$. To guarantee that $p_k$ is a descent direction (i.e., $g_k^\\top p_k < 0$), a check is performed. If $g_k^\\top p_k \\ge 0$, which can occur if $H_k$ is not positive definite, the direction is reset to the steepest descent direction, $p_k = -g_k$.\n\n    c. **Line Search**: The step length $\\alpha_k$ is determined by a backtracking line search to satisfy the Armijo condition:\n    $$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top p_k$$\n    The search starts with a trial step length $\\alpha_k=1$. If the condition is not met, $\\alpha_k$ is repeatedly multiplied by a shrink factor of $1/2$ until the condition is satisfied. The parameter $c_1$ is set to $10^{-4}$.\n\n    d. **Update Iterate**: The new iterate is computed as $x_{k+1} = x_k + \\alpha_k p_k$. The step vector is $s_k = x_{k+1} - x_k$, and the gradient difference vector is $y_k = g_{k+1} - g_k$, where $g_{k+1} = \\nabla f(x_{k+1})$.\n\n    e. **Inverse Hessian Update**: The core of the hybrid strategy lies in how $H_k$ is updated to $H_{k+1}$.\n        -   **BFGS Mode**: If the algorithm is in the BFGS mode, the curvature is checked. If $s_k^\\top y_k > \\theta = 10^{-10}$, the curvature is sufficiently positive, and the BFGS update is applied:\n            $$H_{k+1} = \\left(I - \\rho_k s_k y_k^\\top\\right) H_k \\left(I - \\rho_k y_k s_k^\\top\\right) + \\rho_k s_k s_k^\\top, \\quad \\text{where } \\rho_k = \\frac{1}{s_k^\\top y_k}$$\n            If the curvature condition is not met ($s_k^\\top y_k \\le \\theta$), it indicates non-positive curvature. The algorithm permanently switches to SR1 mode. This switch is flagged.\n        -   **SR1 Mode**: Once in SR1 mode (either by switching in the current step or from a previous one), the SR1 update is attempted. The update formula is:\n            $$H_{k+1} = H_k + \\frac{(s_k - H_k y_k)(s_k - H_k y_k)^\\top}{(s_k - H_k y_k)^\\top y_k}$$\n            This update is only performed if the denominator is not close to zero. A practical check is implemented: if $|(s_k - H_k y_k)^\\top y_k| \\le 10^{-8}$, the update is skipped, and we set $H_{k+1} = H_k$.\n\n    f. **Advance**: Increment the iteration counter, $k \\leftarrow k+1$, and proceed to the next iteration.\n\nThis procedure is applied to each of the four test functions provided in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_quasi_newton(func, grad, x0, A=None, b=None):\n    \"\"\"\n    Implements a hybrid BFGS/SR1 quasi-Newton optimization algorithm.\n\n    Args:\n        func (callable): The objective function f(x, A, b).\n        grad (callable): The gradient of the objective function grad(x, A, b).\n        x0 (np.ndarray): The starting point.\n        A (np.ndarray, optional): Matrix parameter for the function.\n        b (np.ndarray, optional): Vector parameter for the function.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: The final objective function value, rounded to 6 decimal places.\n            - int: The total number of iterations.\n            - bool: True if the algorithm switched to SR1, False otherwise.\n    \"\"\"\n    # Parameters from problem statement\n    c1 = 1e-4\n    shrink_factor = 0.5\n    grad_tol = 1e-6\n    max_iter = 200\n    theta = 1e-10\n    sr1_denom_tol = 1e-8\n\n    n = len(x0)\n    x = np.copy(x0).astype(float)\n    H = np.eye(n)\n    \n    k = 0\n    switched_to_sr1 = False\n    mode = 'bfgs'\n\n    while k < max_iter:\n        g = grad(x, A, b)\n\n        # Termination condition on gradient norm\n        if np.linalg.norm(g) <= grad_tol:\n            break\n\n        # Compute search direction\n        p = -H @ g\n\n        # Ensure search direction is a descent direction\n        if g.T @ p >= 0:\n            p = -g\n        \n        # Backtracking Armijo line search\n        alpha = 1.0\n        fk = func(x, A, b)\n        g_dot_p = g.T @ p\n        \n        # The line search loop\n        while func(x + alpha * p, A, b) > fk + c1 * alpha * g_dot_p:\n            alpha *= shrink_factor\n        \n        # Update iterate and compute step and gradient difference\n        s = alpha * p\n        x_next = x + s\n        g_next = grad(x_next, A, b)\n        y = g_next - g\n\n        s_dot_y = s.T @ y\n\n        # Mode-dependent update of the inverse Hessian approximation\n        if mode == 'bfgs':\n            if s_dot_y > theta:\n                # Apply BFGS update\n                rho = 1.0 / s_dot_y\n                I = np.eye(n)\n                term1 = I - rho * np.outer(s, y)\n                H = term1 @ H @ term1.T + rho * np.outer(s, s)\n            else:\n                # Switch to SR1 and fall through to apply SR1 in this iteration\n                mode = 'sr1'\n                switched_to_sr1 = True\n\n        if mode == 'sr1':\n            v = s - H @ y\n            denom = v.T @ y\n            if abs(denom) > sr1_denom_tol:\n                # Apply SR1 update\n                H = H + np.outer(v, v) / denom\n            # Else: skip the update, H_{k+1} = H_k\n        \n        # Prepare for the next iteration\n        x = x_next\n        k += 1\n\n    final_f = func(x, A, b)\n    return round(final_f, 6), k, switched_to_sr1\n\n# --- Test Case Definitions ---\n\n# 1. Convex quadratic\ndef func1(x, A, b):\n    return 0.5 * x.T @ A @ x + b.T @ x\ndef grad1(x, A, b):\n    return A @ x + b\n\n# 2. Rosenbrock function\ndef func2(x, A, b): # A, b are placeholders\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\ndef grad2(x, A, b): # A, b are placeholders\n    return np.array([\n        -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2),\n        200 * (x[1] - x[0]**2)\n    ])\n\n# 3. Saddle-quartic function\ndef func3(x, A, b): # A, b are placeholders\n    return x[0]**2 - x[1]**2 + 0.1 * (x[0]**4 + x[1]**4)\ndef grad3(x, A, b): # A, b are placeholders\n    return np.array([\n        2 * x[0] + 0.4 * x[0]**3,\n        -2 * x[1] + 0.4 * x[1]**3\n    ])\n\n# 4. Quadratic plus trigonometric perturbation\ndef func4(x, A, b): # b is a placeholder\n    return 0.5 * x.T @ A @ x + 0.2 * np.sin(3 * x[0]) + 0.2 * np.cos(2 * x[1])\ndef grad4(x, A, b): # b is a placeholder\n    return A @ x + np.array([\n        0.6 * np.cos(3 * x[0]),\n        -0.4 * np.sin(2 * x[1])\n    ])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"func\": func1, \"grad\": grad1, \"x0\": np.array([-2.0, 2.0]), \n            \"A\": np.array([[4.0, 1.0], [1.0, 3.0]]), \"b\": np.array([-1.0, 2.0])\n        },\n        {\n            \"func\": func2, \"grad\": grad2, \"x0\": np.array([-1.2, 1.0]),\n            \"A\": None, \"b\": None\n        },\n        {\n            \"func\": func3, \"grad\": grad3, \"x0\": np.array([0.0, 0.2]),\n            \"A\": None, \"b\": None\n        },\n        {\n            \"func\": func4, \"grad\": grad4, \"x0\": np.array([1.5, -1.0]),\n            \"A\": np.array([[2.0, 0.0], [0.0, 1.0]]), \"b\": None\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        f_val, iters, flag = hybrid_quasi_newton(\n            case[\"func\"], case[\"grad\"], case[\"x0\"], A=case[\"A\"], b=case[\"b\"]\n        )\n        # Format boolean as required ('true' or 'false')\n        flag_str = str(flag).lower()\n        results.append(f\"[{f_val:.6f},{iters},{flag_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3184218"}]}