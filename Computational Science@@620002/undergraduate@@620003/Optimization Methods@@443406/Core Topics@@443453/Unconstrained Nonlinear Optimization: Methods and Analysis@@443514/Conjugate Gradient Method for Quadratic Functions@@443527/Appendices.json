{"hands_on_practices": [{"introduction": "To truly grasp the Conjugate Gradient method, there is no substitute for working through the calculations by hand. This first exercise guides you through the core mechanics of the algorithm on a small, well-behaved system. By manually computing the residuals, search directions, and step sizes, you will build a concrete understanding of how the method iteratively constructs an optimal solution and why it is guaranteed to converge in at most $n$ steps for an $n$-dimensional problem [@problem_id:3111640].", "problem": "Consider the unconstrained quadratic minimization problem with objective function $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^\\top A \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$, where $A \\in \\mathbb{R}^{3 \\times 3}$ is symmetric positive definite (SPD) with distinct eigenvalues. Let\n$$\nA = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 5\n\\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\quad\n\\mathbf{x}_{0} = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nNote that $A$ is SPD and has distinct eigenvalues $2$, $3$, and $5$. Apply the Conjugate Gradient (CG) method to minimize $f(\\mathbf{x})$, starting at $\\mathbf{x}_{0}$, by constructing search directions that are pairwise $A$-conjugate and by performing at each step an exact line search along the current search direction. Work from first principles as follows:\n- Use the definition of $f(\\mathbf{x})$ and the Euclidean inner product to derive the step size at each iteration by minimizing $f(\\mathbf{x}_{k} + \\alpha \\mathbf{p}_{k})$ with respect to $\\alpha$.\n- Impose $A$-conjugacy of successive directions to determine the recurrence for the new search direction, and justify the orthogonality relations that arise between residuals and search directions.\n- Carry out exactly $3$ Conjugate Gradient iterations $(k = 0, 1, 2)$ by hand, computing $\\mathbf{x}_{1}$, $\\mathbf{x}_{2}$, and $\\mathbf{x}_{3}$, and verify termination by showing the residual at $\\mathbf{x}_{3}$ is the zero vector.\n\nWhat is the exact value of $f(\\mathbf{x}_{3})$? Provide your answer as a single simplified fraction. No rounding is required.", "solution": "The problem is to minimize the quadratic function $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top A \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$ using the Conjugate Gradient (CG) method. The gradient of this function is $\\nabla f(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b}$. The minimum is achieved when $\\nabla f(\\mathbf{x}) = \\mathbf{0}$, which corresponds to solving the linear system $A \\mathbf{x} = \\mathbf{b}$. The residual at an iterate $\\mathbf{x}_k$ is defined as $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k = -\\nabla f(\\mathbf{x}_k)$. The CG method generates a sequence of iterates $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is the search direction and $\\alpha_k$ is the step size.\n\nFirst, we derive the step size $\\alpha_k$ by performing an exact line search. We minimize $\\phi(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ with respect to $\\alpha$:\n$$\n\\phi(\\alpha) = \\frac{1}{2} (\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^\\top A (\\mathbf{x}_k + \\alpha \\mathbf{p}_k) - \\mathbf{b}^\\top (\\mathbf{x}_k + \\alpha \\mathbf{p}_k)\n$$\nExpanding this expression and using the symmetry of $A$ ($A=A^\\top$), we get:\n$$\n\\phi(\\alpha) = \\frac{1}{2} \\mathbf{x}_k^\\top A \\mathbf{x}_k - \\mathbf{b}^\\top \\mathbf{x}_k + \\alpha (\\mathbf{p}_k^\\top A \\mathbf{x}_k - \\mathbf{p}_k^\\top \\mathbf{b}) + \\frac{1}{2} \\alpha^2 \\mathbf{p}_k^\\top A \\mathbf{p}_k\n$$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$\n\\frac{d\\phi}{d\\alpha} = \\mathbf{p}_k^\\top A \\mathbf{x}_k - \\mathbf{p}_k^\\top \\mathbf{b} + \\alpha \\mathbf{p}_k^\\top A \\mathbf{p}_k = \\mathbf{p}_k^\\top (A \\mathbf{x}_k - \\mathbf{b}) + \\alpha \\mathbf{p}_k^\\top A \\mathbf{p}_k = -\\mathbf{p}_k^\\top \\mathbf{r}_k + \\alpha \\mathbf{p}_k^\\top A \\mathbf{p}_k = 0\n$$\nSolving for $\\alpha$, we get the step size:\n$$\n\\alpha_k = \\frac{\\mathbf{p}_k^\\top \\mathbf{r}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k}\n$$\n\nNext, we determine the recurrence for the search direction $\\mathbf{p}_k$. The initial search direction is the steepest descent direction, $\\mathbf{p}_0 = \\mathbf{r}_0$. For subsequent steps, the new search direction $\\mathbf{p}_{k+1}$ is constructed to be a linear combination of the new residual $\\mathbf{r}_{k+1}$ and the previous search direction $\\mathbf{p}_k$:\n$$\n\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k\n$$\nThe coefficient $\\beta_k$ is chosen to enforce $A$-conjugacy between successive search directions, i.e., $\\mathbf{p}_{k+1}^\\top A \\mathbf{p}_k = 0$.\n$$\n(\\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k)^\\top A \\mathbf{p}_k = 0 \\implies \\mathbf{r}_{k+1}^\\top A \\mathbf{p}_k + \\beta_k \\mathbf{p}_k^\\top A \\mathbf{p}_k = 0\n$$\nThis gives the Hestenes-Stiefel formula for $\\beta_k$:\n$$\n\\beta_k = -\\frac{\\mathbf{r}_{k+1}^\\top A \\mathbf{p}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k}\n$$\nThe residual is updated as $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k$, from which we get $A \\mathbf{p}_k = \\frac{1}{\\alpha_k} (\\mathbf{r}_k - \\mathbf{r}_{k+1})$. Substituting this into the numerator for $\\beta_k$:\n$$\n\\mathbf{r}_{k+1}^\\top A \\mathbf{p}_k = \\frac{1}{\\alpha_k} \\mathbf{r}_{k+1}^\\top (\\mathbf{r}_k - \\mathbf{r}_{k+1}) = -\\frac{1}{\\alpha_k} \\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}\n$$\nHere we have used the fact that successive residuals are orthogonal, $\\mathbf{r}_{k+1}^\\top \\mathbf{r}_k = 0$. Also, the search direction update $\\mathbf{p}_k = \\mathbf{r}_k + \\beta_{k-1} \\mathbf{p}_{k-1}$ and the orthogonality of residuals $\\mathbf{r}_k^\\top \\mathbf{r}_j=0$ for $j < k$ implies $\\mathbf{p}_k^\\top \\mathbf{r}_k = (\\mathbf{r}_k + \\beta_{k-1} \\mathbf{p}_{k-1})^\\top \\mathbf{r}_k = \\mathbf{r}_k^\\top \\mathbf{r}_k$. This simplifies the step size formula to $\\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{r}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k}$.\nSubstituting this into the expression for $\\beta_k$:\n$$\n\\beta_k = -\\frac{- (\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}) / \\alpha_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k} = \\frac{\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}}{\\alpha_k (\\mathbf{p}_k^\\top A \\mathbf{p}_k)} = \\frac{\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}}{(\\frac{\\mathbf{r}_k^\\top \\mathbf{r}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k}) (\\mathbf{p}_k^\\top A \\mathbf{p}_k)} = \\frac{\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\top \\mathbf{r}_k}\n$$\nThis is the Fletcher-Reeves formula for $\\beta_k$, which we will use for the calculations.\n\nWe are given $A = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}$, $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$, and $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Iteration $k=0$:**\n1.  Initial residual: $\\mathbf{r}_0 = \\mathbf{b} - A \\mathbf{x}_0 = \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n2.  Initial search direction: $\\mathbf{p}_0 = \\mathbf{r}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n3.  Compute $\\mathbf{r}_0^\\top \\mathbf{r}_0 = 1^2 + 1^2 + 1^2 = 3$.\n4.  Compute $A \\mathbf{p}_0 = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}$.\n5.  Compute $\\mathbf{p}_0^\\top A \\mathbf{p}_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = 2+3+5 = 10$.\n6.  Step size: $\\alpha_0 = \\frac{\\mathbf{r}_0^\\top \\mathbf{r}_0}{\\mathbf{p}_0^\\top A \\mathbf{p}_0} = \\frac{3}{10}$.\n7.  Update solution: $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0 \\mathbf{p}_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{3}{10} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix}$.\n8.  Update residual: $\\mathbf{r}_1 = \\mathbf{r}_0 - \\alpha_0 A \\mathbf{p}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{3}{10} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/10 \\\\ 1 - 9/10 \\\\ 1 - 15/10 \\end{pmatrix} = \\begin{pmatrix} 4/10 \\\\ 1/10 \\\\ -5/10 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix}$.\n\n**Iteration $k=1$:**\n1.  Compute $\\mathbf{r}_1^\\top \\mathbf{r}_1 = (\\frac{1}{10})^2 (4^2 + 1^2 + (-5)^2) = \\frac{1}{100}(16+1+25) = \\frac{42}{100} = \\frac{21}{50}$.\n2.  Compute $\\beta_0 = \\frac{\\mathbf{r}_1^\\top \\mathbf{r}_1}{\\mathbf{r}_0^\\top \\mathbf{r}_0} = \\frac{21/50}{3} = \\frac{7}{50}$.\n3.  Update search direction: $\\mathbf{p}_1 = \\mathbf{r}_1 + \\beta_0 \\mathbf{p}_0 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} + \\frac{7}{50} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 20 \\\\ 5 \\\\ -25 \\end{pmatrix} + \\frac{1}{50} \\begin{pmatrix} 7 \\\\ 7 \\\\ 7 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix}$.\n4.  Compute $A \\mathbf{p}_1 = \\frac{1}{50} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix}$.\n5.  Compute $\\mathbf{p}_1^\\top A \\mathbf{p}_1 = \\frac{1}{50} \\begin{pmatrix} 27 & 12 & -18 \\end{pmatrix} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{2500}(27 \\cdot 54 + 12 \\cdot 36 - 18 \\cdot (-90)) = \\frac{1458+432+1620}{2500} = \\frac{3510}{2500} = \\frac{351}{250}$.\n6.  Step size: $\\alpha_1 = \\frac{\\mathbf{r}_1^\\top \\mathbf{r}_1}{\\mathbf{p}_1^\\top A \\mathbf{p}_1} = \\frac{21/50}{351/250} = \\frac{21}{50} \\cdot \\frac{250}{351} = \\frac{21 \\cdot 5}{351} = \\frac{105}{351} = \\frac{35}{117}$.\n7.  Update solution: $\\mathbf{x}_2 = \\mathbf{x}_1 + \\alpha_1 \\mathbf{p}_1 = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix} + \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{10}\\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{7}{1170} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{117}{1170} \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{1}{1170} \\begin{pmatrix} 189 \\\\ 84 \\\\ -126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 351+189 \\\\ 351+84 \\\\ 351-126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 540 \\\\ 435 \\\\ 225 \\end{pmatrix} = \\frac{15}{1170} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix}$.\n8.  Update residual: $\\mathbf{r}_2 = \\mathbf{r}_1 - \\alpha_1 A \\mathbf{p}_1 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{7}{65} \\begin{pmatrix} 3 \\\\ 2 \\\\ -5 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 52-42 \\\\ 13-28 \\\\ -65+70 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$.\n\n**Iteration $k=2$:**\n1.  Compute $\\mathbf{r}_2^\\top \\mathbf{r}_2 = (\\frac{1}{26})^2 (2^2 + (-3)^2 + 1^2) = \\frac{1}{676}(4+9+1) = \\frac{14}{676} = \\frac{7}{338}$.\n2.  Compute $\\beta_1 = \\frac{\\mathbf{r}_2^\\top \\mathbf{r}_2}{\\mathbf{r}_1^\\top \\mathbf{r}_1} = \\frac{7/338}{21/50} = \\frac{7}{338} \\cdot \\frac{50}{21} = \\frac{1}{338} \\frac{50}{3} = \\frac{25}{507}$.\n3.  Update search direction: $\\mathbf{p}_2 = \\mathbf{r}_2 + \\beta_1 \\mathbf{p}_1 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{25}{507} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{39}{1014} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 78+27 \\\\ -117+12 \\\\ 39-18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 105 \\\\ -105 \\\\ 21 \\end{pmatrix} = \\frac{21}{1014} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix}$.\n4.  Compute $A \\mathbf{p}_2 = \\frac{7}{338} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$.\n5.  Compute $\\mathbf{p}_2^\\top A \\mathbf{p}_2 = \\frac{7}{338} \\begin{pmatrix} 5 & -5 & 1 \\end{pmatrix} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{245}{338^2}(10+15+1) = \\frac{245 \\cdot 26}{338^2} = \\frac{245}{338 \\cdot 13} = \\frac{245}{4394}$.\n6.  Step size: $\\alpha_2 = \\frac{\\mathbf{r}_2^\\top \\mathbf{r}_2}{\\mathbf{p}_2^\\top A \\mathbf{p}_2} = \\frac{7/338}{245/4394} = \\frac{7}{338} \\frac{4394}{245} = \\frac{7 \\cdot 13}{245} = \\frac{91}{245} = \\frac{13}{35}$.\n7.  Update solution: $\\mathbf{x}_3 = \\mathbf{x}_2 + \\alpha_2 \\mathbf{p}_2 = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{13}{35} \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{5 \\cdot 26} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{130} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{5}{390} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{3}{390} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 180+15 \\\\ 145-15 \\\\ 75+3 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 195 \\\\ 130 \\\\ 78 \\end{pmatrix} = \\begin{pmatrix} 195/390 \\\\ 130/390 \\\\ 78/390 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$.\n8.  Update residual: $\\mathbf{r}_3 = \\mathbf{r}_2 - \\alpha_2 A \\mathbf{p}_2 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{35} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe residual $\\mathbf{r}_3$ is the zero vector, which verifies that the method has converged to the exact solution in $3$ iterations, as expected for a $3 \\times 3$ system.\nThe final solution is $\\mathbf{x}_3 = \\mathbf{x}^* = A^{-1} \\mathbf{b} = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$.\n\nWe are asked to compute $f(\\mathbf{x}_3)$.\n$$\nf(\\mathbf{x}_3) = \\frac{1}{2} \\mathbf{x}_3^\\top A \\mathbf{x}_3 - \\mathbf{b}^\\top \\mathbf{x}_3\n$$\nSince $\\mathbf{r}_3 = \\mathbf{b} - A\\mathbf{x}_3 = \\mathbf{0}$, we have $A\\mathbf{x}_3 = \\mathbf{b}$. Substituting this into the expression for $f(\\mathbf{x}_3)$:\n$$\nf(\\mathbf{x}_3) = \\frac{1}{2} \\mathbf{x}_3^\\top \\mathbf{b} - \\mathbf{b}^\\top \\mathbf{x}_3 = -\\frac{1}{2} \\mathbf{b}^\\top \\mathbf{x}_3\n$$\nNow we compute the value:\n$$\n\\mathbf{b}^\\top \\mathbf{x}_3 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{5} = \\frac{15}{30} + \\frac{10}{30} + \\frac{6}{30} = \\frac{31}{30}\n$$\nTherefore, the minimum value of the function is:\n$$\nf(\\mathbf{x}_3) = -\\frac{1}{2} \\left( \\frac{31}{30} \\right) = -\\frac{31}{60}\n$$", "answer": "$$\\boxed{-\\frac{31}{60}}$$", "id": "3111640"}, {"introduction": "Moving from manual calculation to software implementation brings new practical challenges. A crucial question for any iterative method is when to stop. This practice explores two key stopping criteria: one based on the computable residual ($r_k = b - A x_k$) and another based on the true error ($e_k = x_k - x^\\star$), which is typically unknown in practice. By implementing the CG algorithm and observing these metrics on systems with different conditioning, you will gain insight into their relationship and learn why choosing an appropriate stopping rule is a critical aspect of numerical optimization [@problem_id:3111692].", "problem": "Consider the unconstrained minimization of a strictly convex quadratic function $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^\\top A \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$ where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $\\mathbf{b} \\in \\mathbb{R}^n$. The unique minimizer $\\mathbf{x}^\\star$ satisfies the first-order optimality condition $A \\mathbf{x}^\\star = \\mathbf{b}$. The Conjugate Gradient (CG) method iteratively produces approximations $\\mathbf{x}_k$ that minimize $f(\\mathbf{x})$ over growing Krylov subspaces. Two widely used stopping criteria for the CG iterates are based on the following quantities:\n\n- The residual $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k$, and the relative residual norm $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{b}\\|_2$.\n- The error $\\mathbf{e}_k = \\mathbf{x}_k - \\mathbf{x}^\\star$, and the energy norm $\\|\\mathbf{e}_k\\|_A = \\sqrt{\\mathbf{e}_k^\\top A \\mathbf{e}_k}$.\n\nYour task is to:\n1. Implement the Conjugate Gradient (CG) method for symmetric positive definite $A$ and compute the iterate sequence $\\{\\mathbf{x}_k\\}$ starting from $\\mathbf{x}_0 = \\mathbf{0}$.\n2. For each iterate $\\mathbf{x}_k$, compute the residual $\\mathbf{r}_k$ and the error $\\mathbf{e}_k$ using the exact solution $\\mathbf{x}^\\star = A^{-1} \\mathbf{b}$. Use these to evaluate $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{b}\\|_2$ and $\\|\\mathbf{e}_k\\|_A$ at each iteration $k$.\n3. For given tolerances $\\tau_{\\mathrm{res}}$ and $\\tau_{\\mathrm{en}}$, define the stopping iterations\n   $$k_{\\mathrm{res}} = \\min\\{k \\ge 0 : \\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{b}\\|_2 \\le \\tau_{\\mathrm{res}}\\}, \\quad\n   k_{\\mathrm{en}} = \\min\\{k \\ge 0 : \\|\\mathbf{e}_k\\|_A \\le \\tau_{\\mathrm{en}}\\}.$$\n   If a criterion is not met within the prescribed iteration limit $k \\in \\{0,1,\\dots,n\\}$, set the corresponding stopping iteration to $n$.\n4. Report, for each test case, the pair $(k_{\\mathrm{res}}, k_{\\mathrm{en}})$ and whether they differ. If $\\|\\mathbf{b}\\|_2 = 0$, define the relative residual at $k=0$ to be $0$ and treat the problem as solved with $\\mathbf{x}^\\star = \\mathbf{0}$.\n\nUse the following test suite with explicit matrices, vectors, and tolerances. All matrices are diagonal and all entries are real numbers:\n\n- Case $1$ (happy path, well-conditioned):\n  - $A = \\mathrm{diag}(1, 2, 3, 4, 5)$,\n  - $\\mathbf{b} = [1, 0.5, -1, 2, 0.25]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-8}$, $\\tau_{\\mathrm{en}} = 10^{-8}$.\n\n- Case $2$ (significant eigenvalue spread, ill-conditioned):\n  - $A = \\mathrm{diag}(10^{-6}, 10^{-4}, 10^{-2}, 1, 10)$,\n  - $\\mathbf{b} = [1, 1, 1, 1, 1]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-8}$, $\\tau_{\\mathrm{en}} = 10^{-8}$.\n\n- Case $3$ (boundary condition, zero right-hand side):\n  - $A = \\mathrm{diag}(2, 3, 4)$,\n  - $\\mathbf{b} = [0, 0, 0]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-8}$, $\\tau_{\\mathrm{en}} = 10^{-8}$.\n\n- Case $4$ (different practical thresholds to provoke divergence of criteria):\n  - $A = \\mathrm{diag}(10^{-4}, 1, 100)$,\n  - $\\mathbf{b} = [1, 1, 1]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-10}$, $\\tau_{\\mathrm{en}} = 10^{-4}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a bracketed list of the form $[k_{\\mathrm{res}},k_{\\mathrm{en}},\\mathrm{diff}]$ where $\\mathrm{diff}$ is a boolean indicating whether $k_{\\mathrm{res}} \\ne k_{\\mathrm{en}}$. There must be no spaces anywhere in the output. For example, if there were two cases the output format would be $[[k_{\\mathrm{res}}^{(1)},k_{\\mathrm{en}}^{(1)},\\mathrm{diff}^{(1)}],[k_{\\mathrm{res}}^{(2)},k_{\\mathrm{en}}^{(2)},\\mathrm{diff}^{(2)}]]$.", "solution": "The user has provided a problem that requires the implementation and analysis of the Conjugate Gradient (CG) method for minimizing a strictly convex quadratic function. The core of the task is to compare two common stopping criteria: one based on the relative residual norm and the other on the error's energy norm.\n\n### Problem Formulation\n\nThe problem is to find the minimizer $\\mathbf{x}^\\star$ of the unconstrained quadratic objective function:\n$$ f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top A \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x} $$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix and $\\mathbf{b} \\in \\mathbb{R}^n$. The unique minimizer $\\mathbf{x}^\\star$ is the solution to the linear system of equations derived from the first-order optimality condition $\\nabla f(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b} = \\mathbf{0}$:\n$$ A \\mathbf{x}^\\star = \\mathbf{b} $$\n\nThe Conjugate Gradient method is an iterative algorithm designed to solve such systems efficiently. It generates a sequence of approximations $\\{\\mathbf{x}_k\\}$ that provably converges to $\\mathbf{x}^\\star$ in at most $n$ iterations in exact arithmetic. The problem specifies an initial guess of $\\mathbf{x}_0 = \\mathbf{0}$.\n\n### The Conjugate Gradient Algorithm\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization ($k=0$):**\n    *   Set the initial guess: $\\mathbf{x}_0 = \\mathbf{0}$.\n    *   Compute the initial residual: $\\mathbf{r}_0 = \\mathbf{b} - A \\mathbf{x}_0 = \\mathbf{b}$.\n    *   Set the initial search direction: $\\mathbf{p}_0 = \\mathbf{r}_0$.\n\n2.  **Iterative Refinement:** For $k = 0, 1, 2, \\dots, n-1$:\n    *   Compute the matrix-vector product $A \\mathbf{p}_k$.\n    *   Calculate the optimal step size $\\alpha_k$ along the direction $\\mathbf{p}_k$:\n        $$ \\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{r}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k} $$\n    *   Update the solution iterate:\n        $$ \\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k $$\n    *   Update the residual using a computationally stable formula:\n        $$ \\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k $$\n    *   Compute the factor $\\beta_k$ to construct the next A-orthogonal search direction:\n        $$ \\beta_k = \\frac{\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\top \\mathbf{r}_k} $$\n    *   Update the search direction:\n        $$ \\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k $$\n    \nAt each iteration $k$, the iterate $\\mathbf{x}_k$ minimizes the quadratic function $f(\\mathbf{x})$ over the affine subspace $\\mathbf{x}_0 + \\mathcal{K}_k(A, \\mathbf{r}_0)$, where $\\mathcal{K}_k(A, \\mathbf{r}_0) = \\mathrm{span}\\{\\mathbf{r}_0, A\\mathbf{r}_0, \\dots, A^{k-1}\\mathbf{r}_0\\}$ is the $k$-th Krylov subspace.\n\n### Monitored Quantities and Stopping Criteria\n\nThe performance of the algorithm is monitored using two key metrics at each iteration $k \\in \\{0, 1, \\dots, n\\}$:\n\n1.  **Relative Residual Norm:** The residual $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k$ measures how well the current iterate $\\mathbf{x}_k$ satisfies the target equation. The relative residual norm is defined as $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{b}\\|_2$, where $\\| \\cdot \\|_2$ denotes the Euclidean norm. For the special case where $\\mathbf{b}=\\mathbf{0}$, the problem specifies that this ratio is $0$ at $k=0$.\n\n2.  **Energy Norm of the Error:** The error is $\\mathbf{e}_k = \\mathbf{x}_k - \\mathbf{x}^\\star$. The A-norm, or energy norm, of the error is defined as $\\|\\mathbf{e}_k\\|_A = \\sqrt{\\mathbf{e}_k^\\top A \\mathbf{e}_k}$. The CG method is fundamentally designed to minimize this quantity at each step; that is, $\\mathbf{x}_k$ is the solution to $\\min_{\\mathbf{x} \\in \\mathbf{x}_0 + \\mathcal{K}_k(A, \\mathbf{r}_0)} \\|\\mathbf{x}-\\mathbf{x}^\\star\\|_A$.\n\nBased on these quantities and given tolerances $\\tau_{\\mathrm{res}}$ and $\\tau_{\\mathrm{en}}$, the stopping iterations are defined as the first iteration $k$ where each respective criterion is met:\n$$ k_{\\mathrm{res}} = \\min\\{k \\ge 0 : \\frac{\\|\\mathbf{r}_k\\|_2}{\\|\\mathbf{b}\\|_2} \\le \\tau_{\\mathrm{res}}\\} $$\n$$ k_{\\mathrm{en}} = \\min\\{k \\ge 0 : \\|\\mathbf{e}_k\\|_A \\le \\tau_{\\mathrm{en}}\\} $$\nIf a criterion is not satisfied for any $k \\in \\{0, 1, \\dots, n\\}$, the corresponding stopping iteration is set to $n$.\n\n### Implementation Details\n\nFor each test case, the implementation proceeds as follows:\n- The exact solution $\\mathbf{x}^\\star$ is computed first to enable calculation of the error $\\mathbf{e}_k$. Since all matrices $A$ are diagonal, $\\mathbf{x}^\\star_i = \\mathbf{b}_i / A_{ii}$.\n- The stopping iterations $k_{\\mathrm{res}}$ and $k_{\\mathrm{en}}$ are initialized to $n$.\n- The conditions are checked at $k=0$ with the initial values $\\mathbf{x}_0=\\mathbf{0}$, $\\mathbf{r}_0=\\mathbf{b}$, and $\\mathbf{e}_0=-\\mathbf{x}^\\star$. If a criterion is met, the corresponding stopping iteration is set to $0$.\n- The CG algorithm is then executed for $k = 1, \\dots, n$ iterations. At each step, the two criteria are checked. If a criterion is met for the first time at iteration $k$, the corresponding stopping iteration is updated to $k$.\n- The loop can terminate early if both stopping iterations have been found.\n- The special case of $\\mathbf{b}=\\mathbf{0}$ is handled as defined: $\\mathbf{x}^\\star = \\mathbf{0}$, leading to $\\mathbf{r}_0=\\mathbf{0}$ and $\\mathbf{e}_0=\\mathbf{0}$. Therefore, both criteria are met at $k=0$, yielding $(k_{\\mathrm{res}}, k_{\\mathrm{en}}) = (0, 0)$.\n- The product of a diagonal matrix $A$ with a vector $\\mathbf{p}$ is computed efficiently via element-wise multiplication of the diagonal entries of $A$ and the elements of $\\mathbf{p}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_cg_case(A_diag: np.ndarray, b: np.ndarray, tau_res: float, tau_en: float):\n    \"\"\"\n    Runs the Conjugate Gradient method for a single test case.\n\n    Args:\n        A_diag: The diagonal entries of the matrix A.\n        b: The right-hand side vector b.\n        tau_res: The tolerance for the relative residual norm.\n        tau_en: The tolerance for the energy norm of the error.\n\n    Returns:\n        A tuple (k_res, k_en) with the stopping iterations.\n    \"\"\"\n    n = len(b)\n    A = np.diag(A_diag)\n    b_norm = np.linalg.norm(b, 2)\n\n    # Special case from problem description: b = 0\n    if b_norm == 0:\n        # x_star = 0. x_0 = 0.\n        # r_0 = b - A*x_0 = 0. rel_res is defined as 0.\n        # e_0 = x_0 - x_star = 0. en_norm is 0.\n        # Both criteria are met at k=0.\n        return 0, 0\n\n    x_star = b / A_diag\n    \n    # Initialize stopping iterations to n as per problem spec\n    k_res = n\n    k_en = n\n\n    # --- Check criteria at iteration k=0 ---\n    x = np.zeros(n)\n    r = b - A @ x  # At k=0, x_0 = 0, so r_0 = b\n\n    rel_res = np.linalg.norm(r, 2) / b_norm\n    e = x - x_star  # e_0 = -x_star\n    en_norm = np.sqrt(e.T @ A @ e)\n\n    if rel_res = tau_res:\n        k_res = 0\n    if en_norm = tau_en:\n        k_en = 0\n    \n    if k_res != n and k_en != n:\n        return k_res, k_en\n        \n    # --- CG Iterations from k=1 to n ---\n    p = r.copy()\n    rs_old = r.T @ r\n\n    for k in range(1, n + 1):\n        # Efficient matrix-vector product for diagonal A\n        Ap = A_diag * p\n        \n        alpha = rs_old / (p.T @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Calculate stopping criteria quantities for iterate k\n        rel_res = np.linalg.norm(r, 2) / b_norm\n        e = x - x_star\n        en_norm = np.sqrt(e.T @ A @ e)\n        \n        if k_res == n and rel_res = tau_res:\n            k_res = k\n        if k_en == n and en_norm = tau_en:\n            k_en = k\n\n        # Early exit if both stopping iterations have been found\n        if k_res != n and k_en != n:\n            break\n            \n        # Check for numerical convergence to prevent division by small rs_old\n        rs_new = r.T @ r\n        if np.sqrt(rs_new)  1e-15:\n            # If residual is numerically zero, solution is exact.\n            # Both criteria will be met if tolerances are not zero.\n            if k_res == n: k_res = k\n            if k_en == n: k_en = k\n            break\n\n        # Update for next CG step\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return k_res, k_en\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the CG analysis for each, printing the formatted result.\n    \"\"\"\n    test_cases = [\n        # Case 1: Well-conditioned\n        (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.array([1.0, 0.5, -1.0, 2.0, 0.25]), 1e-8, 1e-8),\n        # Case 2: Ill-conditioned\n        (np.array([1e-6, 1e-4, 1e-2, 1.0, 10.0]), np.array([1.0, 1.0, 1.0, 1.0, 1.0]), 1e-8, 1e-8),\n        # Case 3: Zero right-hand side\n        (np.array([2.0, 3.0, 4.0]), np.array([0.0, 0.0, 0.0]), 1e-8, 1e-8),\n        # Case 4: Different tolerances\n        (np.array([1e-4, 1.0, 100.0]), np.array([1.0, 1.0, 1.0]), 1e-10, 1e-4),\n    ]\n\n    results_for_print = []\n    for case in test_cases:\n        A_diag, b, tau_res, tau_en = case\n        k_res, k_en = run_cg_case(A_diag, b, tau_res, tau_en)\n        diff = k_res != k_en\n        # Format string manually to avoid spaces, per problem spec\n        results_for_print.append(f\"[{k_res},{k_en},{str(diff).lower()}]\")\n\n    # Final print statement in the exact required format with no spaces.\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n```", "id": "3111692"}, {"introduction": "The remarkable efficiency of the Conjugate Gradient method is not accidental; it stems from a deep connection to the spectral properties of the matrix $A$. This exercise provides a window into the convergence behavior of CG by analyzing the error in the basis of $A$'s eigenvectors. By tracking how the algorithm systematically reduces error components associated with different eigenvalues, you will discover the principle that CG tends to \"annihilate\" errors corresponding to extreme eigenvalues first. This provides a powerful intuition for why CG is so effective, especially for problems with widely separated eigenvalues [@problem_id:3111694].", "problem": "Consider the unconstrained minimization of the strictly convex quadratic function $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) and $\\mathbf{b} \\in \\mathbb{R}^{n}$. The Conjugate Gradient (CG) method solves this problem by iteratively updating $\\mathbf{x}_{k}$ using search directions that are $A$-conjugate, without forming $A^{-1}$. Starting from first principles, one may characterize CG iterates $\\mathbf{x}_{k}$ through the error $\\mathbf{e}_{k} = \\mathbf{x}_{k} - \\mathbf{x}^{\\star}$, where $\\mathbf{x}^{\\star}$ is the unique minimizer satisfying $A \\mathbf{x}^{\\star} = \\mathbf{b}$. When $A$ admits an eigendecomposition $A = U \\Lambda U^{\\top}$ with orthonormal eigenvectors $U = [\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}]$ and eigenvalues $\\Lambda = \\operatorname{diag}(\\lambda_{1}, \\ldots, \\lambda_{n})$, the error components along the eigenbasis are given by the projections $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} \\mathbf{e}_{k}$. It is empirically and theoretically observed that CG iterates progressively suppress error components associated with extreme eigenvalues of $A$ (very large and very small) in characteristic patterns.\n\nYour task is to write a complete, runnable program that:\n- Implements the Conjugate Gradient (CG) method for SPD matrices $A$ to minimize $f(\\mathbf{x})$ in $\\mathbb{R}^{3}$.\n- For given test cases, constructs $A$ with known eigenvectors and eigenvalues, sets $\\mathbf{x}^{\\star}$ and $\\mathbf{x}_{0}$, runs CG for exactly $n$ iterations (with $n = 3$), and records the error projections $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} (\\mathbf{x}_{k} - \\mathbf{x}^{\\star})$ for each eigenvector $\\mathbf{u}_{i}$ at each iteration $k \\in \\{0,1,2,3\\}$.\n- Defines, for each eigencomponent $i$, the \"annihilation iteration\" $t_{i}$ as the smallest $k \\in \\{0,1,2,3\\}$ such that $\\left|\\alpha_{i}^{(k)}\\right| \\leq \\tau \\left|\\alpha_{i}^{(0)}\\right|$, where $\\tau \\in (0,1)$ is a given threshold fraction.\n- Determines whether CG exhibits the following ordering of annihilation times: the component aligned with the largest eigenvalue is annihilated earlier than the component aligned with the middle eigenvalue, which in turn is annihilated earlier than the component aligned with the smallest eigenvalue. Formally, with eigenvalues ordered as $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$, verify whether $t_{\\max}  t_{\\text{mid}}  t_{\\min}$ holds.\n\nUse the following test suite, which covers distinct behaviors:\n- Test case $1$ (happy path with strong spectral separation): $A = \\operatorname{diag}([1, 10, 100])$, $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, threshold $\\tau = 0.05$.\n- Test case $2$ (same spectrum with a nontrivial eigenbasis): Let $U = R_{z}(\\theta) R_{x}(\\phi)$, where $R_{z}(\\theta) = \\begin{bmatrix}\\cos \\theta  -\\sin \\theta  0 \\\\ \\sin \\theta  \\cos \\theta  0 \\\\ 0  0  1 \\end{bmatrix}$ and $R_{x}(\\phi) = \\begin{bmatrix} 1  0  0 \\\\ 0  \\cos \\phi  -\\sin \\phi \\\\ 0  \\sin \\phi  \\cos \\phi \\end{bmatrix}$, with angles specified in radians. Take $\\theta = \\pi/4$, $\\phi = \\pi/3$, set $\\Lambda = \\operatorname{diag}([1, 10, 100])$, and $A = U \\Lambda U^{\\top}$. Use $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, and threshold $\\tau = 0.05$.\n- Test case $3$ (near-equal eigenvalues): $A = \\operatorname{diag}([10, 11, 12])$, $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, threshold $\\tau = 0.05$.\n- Test case $4$ (ill-conditioned spectrum): $A = \\operatorname{diag}([10^{-3}, 1, 10^{3}])$, $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, threshold $\\tau = 0.05$.\n\nFor each test case:\n1. Compute the eigendecomposition $A = U \\Lambda U^{\\top}$ and sort eigenvalues so that $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$, with corresponding eigenvectors $\\mathbf{u}_{\\min}$, $\\mathbf{u}_{\\text{mid}}$, and $\\mathbf{u}_{\\max}$.\n2. Run Conjugate Gradient for exactly $n = 3$ iterations starting from $\\mathbf{x}_{0}$, where $\\mathbf{b} = A \\mathbf{x}^{\\star}$, and record $\\mathbf{x}_{k}$ for $k = 0, 1, 2, 3$.\n3. Compute $t_{\\min}$, $t_{\\text{mid}}$, and $t_{\\max}$ as defined above with threshold fraction $\\tau$.\n4. Output a boolean indicating whether $t_{\\max}  t_{\\text{mid}}  t_{\\min}$ is satisfied.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_{1}, r_{2}, r_{3}, r_{4}]$, where each $r_{i}$ is a boolean for test case $i$. Only radians are permitted for angles. No physical units are involved. The final outputs must be booleans. The single printed line is the only output your program should produce.", "solution": "The user provided a valid problem.\nThe problem requires an investigation into the convergence behavior of the Conjugate Gradient (CG) method for minimizing a strictly convex quadratic function $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), and the dimension is specified as $n=3$. The unique minimizer $\\mathbf{x}^{\\star}$ of $f(\\mathbf{x})$ is the solution to the linear system $A \\mathbf{x}^{\\star} = \\mathbf{b}$. The core of the problem is to verify a hypothesis about the rate at which CG reduces error components aligned with the eigenvectors of $A$. Specifically, we test whether the error component corresponding to the largest eigenvalue is suppressed faster than the one for the middle eigenvalue, which is in turn suppressed faster than the one for the smallest eigenvalue.\n\n**Conjugate Gradient (CG) Algorithm**\nThe CG method is an iterative algorithm that, starting from an initial guess $\\mathbf{x}_{0}$, generates a sequence of approximations $\\{\\mathbf{x}_{k}\\}$ that converge to $\\mathbf{x}^{\\star}$. The update rules for a given iteration $k$ are as follows:\n\nInitialize:\n$\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$\n$\\mathbf{p}_{0} = \\mathbf{r}_{0}$\n\nFor $k = 0, 1, 2, \\ldots$:\nStep size: $\\alpha_{k} = \\frac{\\mathbf{r}_{k}^{\\top} \\mathbf{r}_{k}}{\\mathbf{p}_{k}^{\\top} A \\mathbf{p}_{k}}$\nUpdate solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_{k} \\mathbf{p}_{k}$\nUpdate residual: $\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - \\alpha_{k} A \\mathbf{p}_{k}$\nUpdate direction: $\\beta_{k} = \\frac{\\mathbf{r}_{k+1}^{\\top} \\mathbf{r}_{k+1}}{\\mathbf{r}_{k}^{\\top} \\mathbf{r}_{k}}$, then $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_{k} \\mathbf{p}_{k}$\n\nThe search directions $\\{\\mathbf{p}_k\\}$ are constructed to be $A$-conjugate, meaning $\\mathbf{p}_i^\\top A \\mathbf{p}_j = 0$ for $i \\neq j$.\n\n**Error Analysis in the Eigenbasis**\nThe error at iteration $k$ is the vector $\\mathbf{e}_{k} = \\mathbf{x}_{k} - \\mathbf{x}^{\\star}$. Since $A$ is SPD, it has a complete set of orthonormal eigenvectors $\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}$ with corresponding positive eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. We can express the error $\\mathbf{e}_{k}$ in this eigenbasis. The component of the error along each eigenvector $\\mathbf{u}_{i}$ is given by the projection $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} \\mathbf{e}_{k}$. The problem investigates the behavior of the magnitudes $|\\alpha_{i}^{(k)}|$ as $k$ increases.\n\n**Verification Procedure**\nThe solution is implemented as a program that follows these steps for each test case:\n1.  **System Setup**: The matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is constructed as specified. For a given minimizer $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, the vector $\\mathbf{b}$ is computed as $\\mathbf{b} = A \\mathbf{x}^{\\star}$.\n2.  **Eigendecomposition**: The program computes the eigendecomposition of $A$ to find its eigenvalues and eigenvectors. The eigenvalues are sorted in ascending order, $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$, and their corresponding eigenvectors are identified as $\\mathbf{u}_{\\min}$, $\\mathbf{u}_{\\text{mid}}$, and $\\mathbf{u}_{\\max}$.\n3.  **CG Iteration**: The CG algorithm is executed for exactly $n=3$ iterations, starting from the initial guess $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$. This generates the sequence of iterates $\\mathbf{x}_{0}, \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}$.\n4.  **Annihilation Time Calculation**: For each eigen-direction $i \\in \\{\\min, \\text{mid}, \\max\\}$, the \"annihilation iteration\" $t_{i}$ is determined. This is defined as the smallest iteration index $k \\in \\{0, 1, 2, 3\\}$ for which the relative error projection magnitude drops below a given threshold $\\tau = 0.05$. Formally, $t_{i} = \\min \\left\\{ k \\in \\{0,1,2,3\\} \\, \\Big| \\, \\left|\\alpha_{i}^{(k)}\\right| \\leq \\tau \\left|\\alpha_{i}^{(0)}\\right| \\right\\}$. In exact arithmetic, CG converges in at most $n$ steps, so $\\mathbf{e}_{3} = \\mathbf{0}$, guaranteeing that $\\alpha_{i}^{(3)} = 0$. This ensures that the condition for annihilation is always met by $k=3$ at the latest, so $t_i$ is always well-defined within the set $\\{0, 1, 2, 3\\}$.\n5.  **Hypothesis Testing**: The program evaluates the boolean condition $t_{\\max}  t_{\\text{mid}}  t_{\\min}$. This relationship formalizes the hypothesis that error components associated with larger eigenvalues are suppressed more rapidly. The program outputs `True` if the condition is satisfied and `False` otherwise. This process is repeated for all four test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0, n_iter):\n    \"\"\"\n    Implements the Conjugate Gradient method for solving Ax=b.\n    Runs for exactly n_iter iterations and returns all intermediate solutions.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    \n    # Check for the trivial case where the initial guess is already the solution.\n    if np.linalg.norm(r)  1e-15:\n        return [x0.copy()] * (n_iter + 1)\n        \n    rs_old = r.T @ r\n    iterates = [x0.copy()]\n    \n    for _ in range(n_iter):\n        Ap = A @ p\n        \n        # Denominator of alpha can be zero if p is in the null space of A.\n        # For an SPD matrix A, this only happens if p=0.\n        # p=0 implies r=0, so the algorithm would have converged.\n        denom = p.T @ Ap\n        if np.isclose(denom, 0):\n            # This indicates convergence has been reached.\n            # Continue filling iterates list with the current solution.\n            for _ in range(n_iter - len(iterates) + 1):\n                iterates.append(x.copy())\n            return iterates\n\n        alpha = rs_old / denom\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r.T @ r\n        \n        # In exact arithmetic, p cannot be zero unless r is, so rs_old won't be zero.\n        # Floating point arithmetic can lead to issues, though unlikely in this controlled problem.\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n        \n        iterates.append(x.copy())\n        \n    return iterates\n\ndef analyze_case(A, x_star, x0, tau):\n    \"\"\"\n    Analyzes a single test case according to the problem description.\n    \"\"\"\n    n = A.shape[0]\n    b = A @ x_star\n    \n    # Step 1: Compute eigendecomposition.\n    # np.linalg.eigh returns eigenvalues in ascending order, which is required.\n    lambdas, U = np.linalg.eigh(A)\n    \n    # Eigenvectors corresponding to lambda_min, lambda_mid, lambda_max.\n    eigenvectors = {'min': U[:, 0], 'mid': U[:, 1], 'max': U[:, 2]}\n\n    # Step 2: Run Conjugate Gradient for n=3 iterations.\n    # The list will contain x_0, x_1, x_2, x_3.\n    x_k_list = conjugate_gradient(A, b, x0, n)\n    \n    # Step 3: Compute error projections and annihilation times.\n    annihilation_times = {}\n    \n    # Compute initial error and its projections.\n    e0 = x_k_list[0] - x_star\n    alpha_i_0 = {\n        'min': eigenvectors['min'].T @ e0,\n        'mid': eigenvectors['mid'].T @ e0,\n        'max': eigenvectors['max'].T @ e0\n    }\n    \n    for i_key in ['min', 'mid', 'max']:\n        initial_proj_mag = abs(alpha_i_0[i_key])\n        \n        # Handle case where initial projection is already zero.\n        if np.isclose(initial_proj_mag, 0):\n            annihilation_times[i_key] = 0\n            continue\n\n        # Find the smallest k where the annihilation condition is met.\n        found = False\n        for k in range(n + 1):  # k from 0 to 3\n            ek = x_k_list[k] - x_star\n            alpha_i_k = eigenvectors[i_key].T @ ek\n            \n            if abs(alpha_i_k) = tau * initial_proj_mag:\n                annihilation_times[i_key] = k\n                found = True\n                break\n        \n        # This fallback is for safety but should not be reached in exact arithmetic,\n        # as CG converges in n steps, making e_n = 0.\n        if not found:\n            annihilation_times[i_key] = n + 1 \n\n    t_min = annihilation_times['min']\n    t_mid = annihilation_times['mid']\n    t_max = annihilation_times['max']\n\n    # Step 4: Output boolean indicating if t_max  t_mid  t_min is satisfied.\n    return t_max  t_mid  t_min\n\ndef solve():\n    # Define common parameters for all test cases.\n    x_star = np.array([1.0, 1.0, 1.0])\n    x0 = np.array([0.0, 0.0, 0.0])\n    tau = 0.05\n\n    # Test Case 1: Happy path with strong spectral separation\n    A1 = np.diag([1.0, 10.0, 100.0])\n    \n    # Test Case 2: Same spectrum with a nontrivial eigenbasis\n    theta = np.pi / 4.0\n    phi = np.pi / 3.0\n    cos_t, sin_t = np.cos(theta), np.sin(theta)\n    cos_p, sin_p = np.cos(phi), np.sin(phi)\n    \n    Rz = np.array([[cos_t, -sin_t, 0.0], \n                   [sin_t, cos_t, 0.0], \n                   [0.0, 0.0, 1.0]])\n    Rx = np.array([[1.0, 0.0, 0.0], \n                   [0.0, cos_p, -sin_p], \n                   [0.0, sin_p, cos_p]])\n                   \n    U = Rz @ Rx\n    Lambda = np.diag([1.0, 10.0, 100.0])\n    A2 = U @ Lambda @ U.T\n\n    # Test Case 3: Near-equal eigenvalues\n    A3 = np.diag([10.0, 11.0, 12.0])\n\n    # Test Case 4: Ill-conditioned spectrum\n    A4 = np.diag([1e-3, 1.0, 1e3])\n    \n    test_cases_matrices = [A1, A2, A3, A4]\n    \n    results = []\n    for A in test_cases_matrices:\n        result = analyze_case(A, x_star, x0, tau)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3111694"}]}