{"hands_on_practices": [{"introduction": "The core idea of quasi-Newton methods is their ability to build an approximation of a function's curvature using only gradient information. The secant equation, $B_{k+1}s_k = y_k$, is the central mechanism for this learning process. This first exercise provides a concrete, step-by-step calculation to demystify this concept, showing how an initially simple Hessian approximation is updated with information about the true curvature after just one step [@problem_id:3170225].", "problem": "Consider unconstrained smooth minimization of a strictly convex quadratic objective in $\\mathbb{R}^{2}$ given by $f(x) = \\frac{1}{2} x^{\\top} Q x$, where $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite. The gradient is $\\nabla f(x) = Q x$ and the Hessian is the constant matrix $Q$. A quasi-Newton method builds successive symmetric positive definite approximations $B_{k}$ to the Hessian by enforcing the secant equation $B_{k+1} s_{k} = y_{k}$, where $s_{k} = x_{k+1} - x_{k}$ and $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) update is obtained by choosing $B_{k+1}$ that satisfies the secant equation and makes the least possible change to $B_{k}$ under a positive-definite metric while preserving symmetry and positive definiteness.\n\nWork with the specific data $Q = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$, initial point $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, and initial Hessian approximation $B_{0} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$. Take one quasi-Newton step using the search direction $p_{0} = - B_{0}^{-1} \\nabla f(x_{0})$ and the step length $\\alpha_{0} = \\frac{1}{2}$ to form $x_{1} = x_{0} + \\alpha_{0} p_{0}$. Define $s_{0} = x_{1} - x_{0}$ and $y_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0})$. Then perform one symmetric rank-two BFGS update to obtain $B_{1}$.\n\nCompute the $(1,2)$ (upper-right off-diagonal) entry of $B_{1}$ and express your final numeric value exactly as a simplified fraction. Briefly interpret, in terms of the secant equation and curvature coupling, why an off-diagonal entry can emerge from an initially diagonal $B_{0}$ even after a single update. The final reported quantity must be the single number equal to the $(1,2)$ entry of $B_{1}$.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard application of the BFGS quasi-Newton method for a convex quadratic function. All necessary data are provided and are internally consistent.\n\nThe objective function is $f(x) = \\frac{1}{2} x^{\\top} Q x$ with $Q = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$. The gradient is $\\nabla f(x) = Qx$.\nThe initial conditions are the point $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the Hessian approximation $B_{0} = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\n\nFirst, we compute the gradient at the initial point $x_{0}$:\n$$\n\\nabla f(x_{0}) = Q x_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n\nNext, we compute the search direction $p_{0}$ using the initial Hessian approximation $B_{0}$:\n$$\np_{0} = -B_{0}^{-1} \\nabla f(x_{0}) = -I^{-1} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}\n$$\n\nUsing the given step length $\\alpha_{0} = \\frac{1}{2}$, we find the next iterate $x_{1}$ and the step vector $s_{0}$:\n$$\ns_{0} = \\alpha_{0} p_{0} = \\frac{1}{2} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n$$\nx_{1} = x_{0} + s_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n\nNow, we compute the gradient at the new point $x_{1}$ to find the change in gradient $y_{0}$:\n$$\n\\nabla f(x_{1}) = Q x_{1} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix}\n$$\n$$\ny_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0}) = \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{2} \\\\ -\\frac{5}{2} \\end{pmatrix}\n$$\nFor a quadratic function, we have the property $y_{k} = Q s_{k}$. We can verify this:\n$$\nQ s_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -2 - \\frac{1}{2} \\\\ -1 - \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{2} \\\\ -\\frac{5}{2} \\end{pmatrix} = y_{0}\n$$\nThe calculation is consistent.\n\nThe BFGS update formula for $B_{1}$ is:\n$$\nB_{1} = B_{0} - \\frac{B_{0} s_{0} s_{0}^{\\top} B_{0}}{s_{0}^{\\top} B_{0} s_{0}} + \\frac{y_{0} y_{0}^{\\top}}{y_{0}^{\\top} s_{0}}\n$$\nSince $B_{0} = I$, the formula simplifies to:\n$$\nB_{1} = I - \\frac{s_{0} s_{0}^{\\top}}{s_{0}^{\\top} s_{0}} + \\frac{y_{0} y_{0}^{\\top}}{y_{0}^{\\top} s_{0}}\n$$\n\nWe need to compute the scalar denominators and the outer product matrices.\nThe first denominator is:\n$$\ns_{0}^{\\top} s_{0} = (-1)^{2} + \\left(-\\frac{1}{2}\\right)^{2} = 1 + \\frac{1}{4} = \\frac{5}{4}\n$$\nThe second denominator is:\n$$\ny_{0}^{\\top} s_{0} = \\begin{pmatrix} -\\frac{5}{2}  -\\frac{5}{2} \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\left(-\\frac{5}{2}\\right)(-1) + \\left(-\\frac{5}{2}\\right)\\left(-\\frac{1}{2}\\right) = \\frac{5}{2} + \\frac{5}{4} = \\frac{15}{4}\n$$\nThe value $y_{0}^{\\top} s_{0}$ must be positive to ensure $B_{1}$ can be positive definite, which holds here.\n\nThe problem asks for the $(1,2)$ entry of $B_{1}$, which we denote as $(B_{1})_{1,2}$. We can compute this entry directly without forming the entire matrix $B_1$:\n$$\n(B_{1})_{1,2} = (I)_{1,2} - \\frac{(s_{0} s_{0}^{\\top})_{1,2}}{s_{0}^{\\top} s_{0}} + \\frac{(y_{0} y_{0}^{\\top})_{1,2}}{y_{0}^{\\top} s_{0}}\n$$\nThe $(1,2)$ entry of the identity matrix $I$ is $0$.\nThe outer product matrices' entries are:\n$$\n(s_{0} s_{0}^{\\top})_{1,2} = (s_{0})_{1} (s_{0})_{2} = (-1)\\left(-\\frac{1}{2}\\right) = \\frac{1}{2}\n$$\n$$\n(y_{0} y_{0}^{\\top})_{1,2} = (y_{0})_{1} (y_{0})_{2} = \\left(-\\frac{5}{2}\\right)\\left(-\\frac{5}{2}\\right) = \\frac{25}{4}\n$$\nSubstituting these values into the expression for $(B_{1})_{1,2}$:\n$$\n(B_{1})_{1,2} = 0 - \\frac{\\frac{1}{2}}{\\frac{5}{4}} + \\frac{\\frac{25}{4}}{\\frac{15}{4}}\n$$\n$$\n(B_{1})_{1,2} = -\\left(\\frac{1}{2} \\cdot \\frac{4}{5}\\right) + \\frac{25}{15} = -\\frac{4}{10} + \\frac{5}{3} = -\\frac{2}{5} + \\frac{5}{3}\n$$\n$$\n(B_{1})_{1,2} = \\frac{-2 \\cdot 3 + 5 \\cdot 5}{15} = \\frac{-6 + 25}{15} = \\frac{19}{15}\n$$\n\nBrief interpretation:\nAn off-diagonal entry emerges in $B_{1}$ because the update process incorporates information about the true curvature of the objective function, which is encoded in the non-diagonal Hessian matrix $Q$. The initial approximation $B_{0}=I$ assumes no coupling between the variables. The BFGS update corrects this. The core of the update is enforcing the secant equation, $B_{1}s_0 = y_0$. Here, $s_0 = (-1, -1/2)^\\top$ and $y_0 = (-5/2, -5/2)^\\top$. Since $y_0$ is not a scalar multiple of $s_0$, the matrix $B_1$ required to perform this mapping cannot be a multiple of the identity matrix; it must contain off-diagonal terms to represent the rotation and shearing effect of the true Hessian. The term $\\frac{y_0 y_0^\\top}{y_0^\\top s_0}$ in the BFGS formula explicitly introduces this coupling. The vector $y_0=Q s_0$ captures the effect of the true Hessian $Q$ on the step $s_0$. The outer product $y_0 y_0^\\top$ creates a rank-one matrix whose structure reflects this interaction. Because both components of $y_0$ are non-zero, the off-diagonal entries of $y_0 y_0^\\top$ are non-zero, thus injecting the observed curvature coupling into the new Hessian approximation $B_1$.", "answer": "$$\\boxed{\\frac{19}{15}}$$", "id": "3170225"}, {"introduction": "While the popular BFGS update guarantees a positive-definite Hessian approximation, this can be a limitation when optimizing non-convex functions with regions of negative curvature. This practice contrasts the behavior of BFGS with the Symmetric Rank-One (SR1) update, which can adapt to and model negative curvature. By working through this example, you will see how the SR1 method can generate more accurate Hessian models near saddle points, highlighting important trade-offs between different quasi-Newton strategies [@problem_id:3170247].", "problem": "Consider the unconstrained minimization of the quadratic model function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x)=\\tfrac{1}{2}x^{\\top}Qx$, where $Q=\\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix}$. You observe two consecutive iterates $x_{k}$ and $x_{k+1}$ with displacement $s_{k}=x_{k+1}-x_{k}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$. Let $g(x)=\\nabla f(x)=Qx$, so the gradient displacement is $y_{k}=g(x_{k+1})-g(x_{k})$. Take the initial Hessian approximation $B_{k}=I$.\n\nUse only the fundamental definitions that quasi-Newton methods impose through the secant equation and symmetry, and the standard construction principles of the symmetric rank-one (SR1) and Broyden–Fletcher–Goldfarb–Shanno (BFGS) Hessian updates, to do the following:\n\n1) Compute $y_{k}$ from $Q$ and $s_{k}$.\n\n2) Using the SR1 Hessian update from $B_{k}$ with the curvature pair $(s_{k},y_{k})$, construct $B_{k+1}^{\\text{SR1}}$. Using the BFGS Hessian update from $B_{k}$ with the same $(s_{k},y_{k})$, construct $B_{k+1}^{\\text{BFGS}}$. Explain, using the vector $(y_{k}-B_{k}s_{k})$, why the SR1 update is able to incorporate negative curvature aligned with the saddle direction of $Q$ while the BFGS update, under the condition $s_{k}^{\\top}y_{k}0$, preserves positive definiteness.\n\n3) Let $x_{k+1}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$, so that $g_{k+1}=g(x_{k+1})$. Form one Newton-like step from each updated Hessian, $p_{k+1}^{\\text{SR1}}=-\\left(B_{k+1}^{\\text{SR1}}\\right)^{-1}g_{k+1}$ and $p_{k+1}^{\\text{BFGS}}=-\\left(B_{k+1}^{\\text{BFGS}}\\right)^{-1}g_{k+1}$.\n\n4) The negative curvature eigendirection of $Q$ is the unit vector $v_{-}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. Compute the squared cosines $\\cos^{2}(\\theta_{\\text{SR1}})$ and $\\cos^{2}(\\theta_{\\text{BFGS}})$, where $\\theta_{\\text{method}}$ is the angle between $p_{k+1}^{\\text{method}}$ and $v_{-}$. Then compute the ratio\n$$\nR=\\frac{\\cos^{2}(\\theta_{\\text{SR1}})}{\\cos^{2}(\\theta_{\\text{BFGS}})}.\n$$\n\nExpress the final result for $R$ as a simplified exact fraction. No rounding is required.", "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, and objective. All data and conditions are self-contained and consistent. We may proceed with the solution.\n\nThe problem asks for a sequence of calculations related to the SR1 and BFGS quasi-Newton updates for a quadratic function with an indefinite Hessian. We will address each of the four parts of the problem in order.\n\nThe function is $f(x)=\\tfrac{1}{2}x^{\\top}Qx$ with $Q=\\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix}$.\nThe gradient is $g(x)=\\nabla f(x)=Qx$.\nThe initial Hessian approximation is $B_{k}=I=\\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix}$.\nThe displacement vector is $s_{k}=x_{k+1}-x_{k}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$.\n\n**Part 1: Compute $y_{k}$**\nThe gradient displacement $y_{k}$ is defined as $y_{k}=g(x_{k+1})-g(x_{k})$. Since $g(x)=Qx$ is a linear function of $x$, we can write:\n$$y_{k} = Qx_{k+1} - Qx_{k} = Q(x_{k+1}-x_{k}) = Qs_{k}$$\nSubstituting the given matrices for $Q$ and $s_{k}$:\n$$y_{k} = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix}(1)(1) + (0)(1)\\\\ (0)(1) + (-0.2)(1)\\end{pmatrix} = \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix}$$\n\n**Part 2: Construct $B_{k+1}^{\\text{SR1}}$ and $B_{k+1}^{\\text{BFGS}}$ and analyze**\nFirst, we compute the necessary intermediate quantities.\nThe curvature condition term is $s_{k}^{\\top}y_{k}$:\n$$s_{k}^{\\top}y_{k} = \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix} = (1)(1) + (1)(-0.2) = 1 - 0.2 = 0.8$$\nSince $s_{k}^{\\top}y_{k}  0$, the standard BFGS update is well-defined and will preserve the positive definiteness of $B_{k}$.\n\nFor the SR1 update, we need the vector $y_{k}-B_{k}s_{k}$:\n$$B_{k}s_{k} = I s_{k} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix}1\\\\ 1\\end{pmatrix}$$\n$$y_{k}-B_{k}s_{k} = \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix} - \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix}0\\\\ -1.2\\end{pmatrix}$$\nThe denominator for the SR1 update is $(y_{k}-B_{k}s_{k})^{\\top}s_{k}$:\n$$(y_{k}-B_{k}s_{k})^{\\top}s_{k} = \\begin{pmatrix}0  -1.2\\end{pmatrix} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = (0)(1) + (-1.2)(1) = -1.2$$\nSince this denominator is non-zero, the SR1 update is well-defined.\n\nNow, we construct the updated Hessian approximations.\nThe SR1 update formula is:\n$$B_{k+1}^{\\text{SR1}} = B_{k} + \\frac{(y_{k}-B_{k}s_{k})(y_{k}-B_{k}s_{k})^{\\top}}{(y_{k}-B_{k}s_{k})^{\\top}s_{k}}$$\n$$B_{k+1}^{\\text{SR1}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} + \\frac{1}{-1.2} \\begin{pmatrix}0\\\\ -1.2\\end{pmatrix} \\begin{pmatrix}0  -1.2\\end{pmatrix} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\frac{1}{1.2} \\begin{pmatrix}0  0\\\\ 0  1.44\\end{pmatrix}$$\n$$B_{k+1}^{\\text{SR1}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\begin{pmatrix}0  0\\\\ 0  1.2\\end{pmatrix} = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix}$$\nRemarkably, $B_{k+1}^{\\text{SR1}} = Q$. The SR1 update has recovered the exact Hessian of the quadratic function in one step.\n\nThe BFGS update formula is:\n$$B_{k+1}^{\\text{BFGS}} = B_{k} - \\frac{B_{k}s_{k}s_{k}^{\\top}B_{k}}{s_{k}^{\\top}B_{k}s_{k}} + \\frac{y_{k}y_{k}^{\\top}}{y_{k}^{\\top}s_{k}}$$\nWe need the terms $s_{k}^{\\top}B_{k}s_{k} = s_{k}^{\\top}Is_{k} = s_{k}^{\\top}s_{k} = 1^{2}+1^{2}=2$, and $B_{k}s_{k} = s_{k}$.\n$$s_{k}s_{k}^{\\top} = \\begin{pmatrix}1\\\\ 1\\end{pmatrix}\\begin{pmatrix}1  1\\end{pmatrix} = \\begin{pmatrix}1  1\\\\ 1  1\\end{pmatrix}$$\n$$y_{k}y_{k}^{\\top} = \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix}\\begin{pmatrix}1  -0.2\\end{pmatrix} = \\begin{pmatrix}1  -0.2\\\\ -0.2  0.04\\end{pmatrix}$$\nSubstituting into the BFGS formula:\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\frac{1}{2}\\begin{pmatrix}1  1\\\\ 1  1\\end{pmatrix} + \\frac{1}{0.8}\\begin{pmatrix}1  -0.2\\\\ -0.2  0.04\\end{pmatrix}$$\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\begin{pmatrix}0.5  0.5\\\\ 0.5  0.5\\end{pmatrix} + \\begin{pmatrix}1.25  -0.25\\\\ -0.25  0.05\\end{pmatrix}$$\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1-0.5+1.25  0-0.5-0.25 \\\\ 0-0.5-0.25  1-0.5+0.05\\end{pmatrix} = \\begin{pmatrix}1.75  -0.75\\\\ -0.75  0.55\\end{pmatrix}$$\n\nAnalysis: The vector $y_{k}-B_{k}s_{k}$ represents the discrepancy between the observed gradient change $y_{k}$ and the change predicted by the current Hessian model $B_{k}$.\nThe SR1 update formula directly incorporates this discrepancy vector in a rank-one correction: $B_{k+1} = B_{k} + \\frac{v v^{\\top}}{v^{\\top}s_{k}}$, where $v = y_{k}-B_{k}s_{k}$. In this problem, the denominator $v^{\\top}s_{k} = -1.2$ is negative. This means the rank-one matrix added to $B_{k}$ is negative semidefinite. This allows the SR1 update to decrease the eigenvalues of the Hessian approximation, and in this specific case, it introduces a negative eigenvalue, correctly identifying the negative curvature of $Q$.\nIn contrast, the BFGS update is constructed to preserve positive definiteness when the curvature condition $y_{k}^{\\top}s_{k}0$ holds. Although derived from the same secant equation $B_{k+1}s_{k}=y_{k}$, its structure as a sum of $B_{k}$ (with a rank-one matrix subtracted) and another rank-one matrix $\\frac{y_{k}y_{k}^{\\top}}{y_{k}^{\\top}s_{k}}$ guarantees that if $B_{k}$ is positive definite, $B_{k+1}^{\\text{BFGS}}$ remains so. It cannot introduce negative curvature. It effectively \"averages\" the curvature information contained in the pair $(s_{k}, y_{k})$ in a way that produces a safe, positive definite model, even if the true Hessian is indefinite.\n\n**Part 3: Newton-like steps**\nWe are given $x_{k+1}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. The gradient at this point is:\n$$g_{k+1} = g(x_{k+1}) = Qx_{k+1} = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix} \\begin{pmatrix}0\\\\ 1\\end{pmatrix} = \\begin{pmatrix}0\\\\ -0.2\\end{pmatrix}$$\nThe search directions are $p_{k+1} = -B_{k+1}^{-1}g_{k+1}$.\n\nFor the SR1 update:\n$$B_{k+1}^{\\text{SR1}} = Q = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix} \\implies (B_{k+1}^{\\text{SR1}})^{-1} = \\begin{pmatrix}1  0\\\\ 0  -5\\end{pmatrix}$$\n$$p_{k+1}^{\\text{SR1}} = -\\begin{pmatrix}1  0\\\\ 0  -5\\end{pmatrix} \\begin{pmatrix}0\\\\ -0.2\\end{pmatrix} = -\\begin{pmatrix}0\\\\ 1\\end{pmatrix} = \\begin{pmatrix}0\\\\ -1\\end{pmatrix}$$\n\nFor the BFGS update:\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1.75  -0.75\\\\ -0.75  0.55\\end{pmatrix} = \\begin{pmatrix}\\frac{7}{4}  -\\frac{3}{4}\\\\ -\\frac{3}{4}  \\frac{11}{20}\\end{pmatrix}$$\nThe determinant is $\\det(B_{k+1}^{\\text{BFGS}}) = (\\frac{7}{4})(\\frac{11}{20}) - (-\\frac{3}{4})^{2} = \\frac{77}{80} - \\frac{9}{16} = \\frac{77-45}{80} = \\frac{32}{80} = \\frac{2}{5}$.\nThe inverse is:\n$$(B_{k+1}^{\\text{BFGS}})^{-1} = \\frac{1}{2/5} \\begin{pmatrix}\\frac{11}{20}  \\frac{3}{4}\\\\ \\frac{3}{4}  \\frac{7}{4}\\end{pmatrix} = \\frac{5}{2} \\begin{pmatrix}\\frac{11}{20}  \\frac{15}{20}\\\\ \\frac{15}{20}  \\frac{35}{20}\\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix}11  15\\\\ 15  35\\end{pmatrix}$$\nThe search direction is:\n$$p_{k+1}^{\\text{BFGS}} = -\\frac{1}{8} \\begin{pmatrix}11  15\\\\ 15  35\\end{pmatrix} \\begin{pmatrix}0\\\\ -0.2\\end{pmatrix} = -\\frac{1}{8} \\begin{pmatrix}11  15\\\\ 15  35\\end{pmatrix} \\begin{pmatrix}0\\\\ -\\frac{1}{5}\\end{pmatrix}$$\n$$p_{k+1}^{\\text{BFGS}} = -\\frac{1}{8} \\begin{pmatrix}-3\\\\ -7\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{8}\\\\ \\frac{7}{8}\\end{pmatrix}$$\n\n**Part 4: Compute the ratio R**\nThe negative curvature eigendirection of $Q$ is $v_{-}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. We have $\\|v_{-}\\| = 1$. The squared cosine of the angle $\\theta$ between a vector $p$ and $v_{-}$ is given by $\\cos^{2}(\\theta) = \\frac{(p^{\\top}v_{-})^{2}}{\\|p\\|^{2}\\|v_{-}\\|^{2}} = \\frac{(p^{\\top}v_{-})^{2}}{\\|p\\|^{2}}$.\n\nFor the SR1 step:\n$$p_{k+1}^{\\text{SR1}} = \\begin{pmatrix}0\\\\ -1\\end{pmatrix}$$\n$$p_{k+1}^{\\text{SR1}\\top}v_{-} = \\begin{pmatrix}0  -1\\end{pmatrix}\\begin{pmatrix}0\\\\ 1\\end{pmatrix} = -1$$\n$$\\|p_{k+1}^{\\text{SR1}}\\|^{2} = 0^{2} + (-1)^{2} = 1$$\n$$\\cos^{2}(\\theta_{\\text{SR1}}) = \\frac{(-1)^{2}}{1} = 1$$\n\nFor the BFGS step:\n$$p_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}\\frac{3}{8}\\\\ \\frac{7}{8}\\end{pmatrix}$$\n$$p_{k+1}^{\\text{BFGS}\\top}v_{-} = \\begin{pmatrix}\\frac{3}{8}  \\frac{7}{8}\\end{pmatrix}\\begin{pmatrix}0\\\\ 1\\end{pmatrix} = \\frac{7}{8}$$\n$$\\|p_{k+1}^{\\text{BFGS}}\\|^{2} = \\left(\\frac{3}{8}\\right)^{2} + \\left(\\frac{7}{8}\\right)^{2} = \\frac{9}{64} + \\frac{49}{64} = \\frac{58}{64} = \\frac{29}{32}$$\n$$\\cos^{2}(\\theta_{\\text{BFGS}}) = \\frac{(\\frac{7}{8})^{2}}{\\frac{29}{32}} = \\frac{\\frac{49}{64}}{\\frac{29}{32}} = \\frac{49}{64} \\cdot \\frac{32}{29} = \\frac{49}{2 \\cdot 29} = \\frac{49}{58}$$\n\nFinally, we compute the ratio $R$:\n$$R = \\frac{\\cos^{2}(\\theta_{\\text{SR1}})}{\\cos^{2}(\\theta_{\\text{BFGS}})} = \\frac{1}{\\frac{49}{58}} = \\frac{58}{49}$$", "answer": "$$\\boxed{\\frac{58}{49}}$$", "id": "3170247"}, {"introduction": "Moving from theory to practice, a key challenge is ensuring optimization algorithms are robust, especially on non-convex landscapes where the curvature condition $s_k^\\top y_k \\gt 0$ may be weak or violated. A common and effective heuristic is to \"restart\" the BFGS method by resetting the inverse Hessian approximation $H_k$ to the identity matrix when its quality is poor. In this hands-on coding exercise, you will implement a BFGS solver with a restart strategy to quantify its impact on performance, bridging the gap between the theoretical update formula and the design of practical, robust optimization software [@problem_id:3170188].", "problem": "Consider unconstrained smooth minimization of a twice continuously differentiable objective in $\\mathbb{R}^n$ using quasi-Newton methods. You are to investigate the role of restarts in the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method applied to a nonconvex objective. Start from the fundamental base that for unconstrained minimization, the steepest descent direction is the negative gradient, and Newton's method uses the Hessian to generate search directions. Quasi-Newton methods aim to approximate the inverse Hessian iteratively using only gradient evaluations and enforce the secant equation that captures local curvature sampled along iterates. The secant equation states that the inverse Hessian approximation $H_{k+1}$ should satisfy $H_{k+1} y_k = s_k$, where $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. Positive definiteness of $H_k$ is crucial to generate descent directions $p_k = -H_k \\nabla f(x_k)$.\n\nYour task is to:\n- Derive, from the secant equation and a minimal-change principle in a suitable norm, a symmetric positive definite inverse update that preserves the secant equation and show the condition under which the update maintains positive definiteness.\n- Implement a BFGS-type quasi-Newton method that uses a backtracking line search satisfying the Armijo sufficient decrease condition. Angles inside trigonometric functions must be treated in radians.\n- Incorporate a restart policy: when the curvature measure $s_k^\\top y_k$ is too small relative to $\\|s_k\\| \\, \\|y_k\\|$, reset the inverse approximation to the identity. Compare runs with and without such restarts on a specific nonconvex objective and quantify robustness improvements.\n\nUse the following nonconvex objective in $\\mathbb{R}^2$:\n$$\nf(x_1,x_2) = \\tfrac{1}{2}(x_1^2 - x_2^2) + 0.1(x_1^4 + x_2^4) + 2\\cos(x_1) + 2\\cos(x_2),\n$$\nwhere all angles in $\\cos(\\cdot)$ are in radians. The gradient is the vector of partial derivatives with respect to $x_1$ and $x_2$.\n\nImplement the algorithm that:\n- Initializes $H_0$ as the identity matrix.\n- At iteration $k$, forms the search direction $p_k = -H_k \\nabla f(x_k)$.\n- Uses a backtracking line search to find a step size $\\alpha_k$ that satisfies the Armijo condition $f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$ with $c_1 \\in (0,1)$.\n- Updates $x_{k+1} = x_k + \\alpha_k p_k$, $s_k = x_{k+1} - x_k$, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n- Applies the inverse Hessian update that enforces the secant equation and preserves symmetry and positive definiteness when the curvature condition $s_k^\\top y_k  0$ holds.\n- Incorporates the restart rule that, when enabled, resets $H_{k+1} = I$ if $s_k^\\top y_k \\le \\tau \\|s_k\\| \\, \\|y_k\\|$ for a given threshold $\\tau \\ge 0$; otherwise performs the inverse update if $s_k^\\top y_k  0$. If the search direction is not a descent direction (i.e., $\\nabla f(x_k)^\\top p_k \\ge 0$), enforce a safeguard that ensures descent.\n\nDefine convergence as $\\|\\nabla f(x_k)\\| \\le \\varepsilon$ for a given tolerance $\\varepsilon  0$ within a maximum number of iterations.\n\nTest Suite:\n- Use the following common settings for all runs: tolerance $\\varepsilon = 10^{-5}$, maximum iterations $300$, backtracking parameter $c_1 = 10^{-4}$, step shrinking factor $0.5$, and a maximum of $50$ backtracking reductions per iteration.\n- Apply the algorithm to the set of initial points $x_0 \\in \\mathbb{R}^2$:\n$$\n[-3,-3],\\ [2,2],\\ [4,-4],\\ [0.1,-0.1],\\ [-2.5,2.25],\\ [5,5],\\ [-5,5].\n$$\n- Evaluate the following five parameter cases:\n    1. No restarts: restart disabled (ignore $\\tau$).\n    2. Restarts enabled with $\\tau = 10^{-8}$.\n    3. Restarts enabled with $\\tau = 10^{-4}$.\n    4. Restarts enabled with $\\tau = 10^{-2}$.\n    5. Restarts enabled with $\\tau = 5 \\times 10^{-1}$.\n\nFor each parameter case, for the entire set of initial points, compute:\n- The integer count of successful runs (number of initial points for which convergence is achieved).\n- The mean number of iterations over only the successful runs, expressed as a real number (use the arithmetic mean; if there are zero successes, report $0.0$).\n- The mean number of backtracking reductions per successful run, expressed as a real number (if there are zero successes, report $0.0$).\n\nFinal Output Format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets. Concatenate the three numbers per parameter case in the order listed above, yielding a list of length $15$:\n$$\n[\\text{succ}_1,\\ \\text{mean\\_iters}_1,\\ \\text{mean\\_backs}_1,\\ \\ldots,\\ \\text{succ}_5,\\ \\text{mean\\_iters}_5,\\ \\text{mean\\_backs}_5].\n$$", "solution": "The problem requires the derivation of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) inverse Hessian update, an analysis of its positive definiteness, and the implementation of a BFGS-based quasi-Newton optimizer. This optimizer will include a backtracking line search and a restart policy to handle nonconvexity. The performance of the algorithm with and without restarts will be evaluated on a given nonconvex objective function.\n\n### Part 1: Derivation and Analysis of the BFGS Update\n\nA quasi-Newton method constructs a sequence of matrices $\\{H_k\\}$ that approximate the inverse Hessian of the objective function, $f(x)$. The update from $H_k$ to $H_{k+1}$ must satisfy the secant equation, which captures curvature information.\n\n**The Secant Equation**\nThe secant equation requires the new inverse Hessian approximation, $H_{k+1}$, to map the change in gradient, $y_k$, to the change in position, $s_k$:\n$$H_{k+1} y_k = s_k$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n\n**Derivation from a Minimal-Change Principle**\nThe BFGS update is derived by finding an update to the Hessian approximation, $B_k = H_k^{-1}$, that is symmetric, satisfies its version of the secant equation ($B_{k+1}s_k = y_k$), and is closest to $B_k$ in a suitable norm. The \"minimal change\" principle for BFGS is typically formulated as solving the following optimization problem for the correction $E = B_{k+1} - B_k$:\n$$\n\\min_{E} \\|E\\| \\quad \\text{subject to} \\quad E=E^\\top \\text{ and } (B_k+E)s_k = y_k\n$$\nThe choice of a specific weighted Frobenius norm leads to the BFGS update for the Hessian approximation $B_{k+1}$:\n$$B_{k+1} = B_k + \\frac{y_k y_k^\\top}{y_k^\\top s_k} - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k}$$\nThe corresponding update for the inverse Hessian approximation, $H_{k+1} = B_{k+1}^{-1}$, can be found by applying the Sherman-Morrison-Woodbury formula twice. This procedure yields the widely used BFGS inverse update formula:\n$$H_{k+1} = \\left(I - \\frac{s_k y_k^\\top}{y_k^\\top s_k}\\right) H_k \\left(I - \\frac{y_k s_k^\\top}{y_k^\\top s_k}\\right) + \\frac{s_k s_k^\\top}{y_k^\\top s_k}$$\nwhere $I$ is the identity matrix. This is one of the most effective quasi-Newton update formulas.\n\n**Condition for Preserving Positive Definiteness**\nA crucial property of the BFGS update is its ability to preserve the positive definiteness of the Hessian approximations, which ensures that the search direction $p_k = -H_k \\nabla f(x_k)$ is a descent direction. Let us assume $H_k$ is symmetric positive definite. We want to find the condition under which $H_{k+1}$ is also positive definite.\n\nFor any non-zero vector $z \\in \\mathbb{R}^n$, we examine the quadratic form $z^\\top H_{k+1} z$:\n$$z^\\top H_{k+1} z = z^\\top \\left(I - \\frac{s_k y_k^\\top}{y_k^\\top s_k}\\right) H_k \\left(I - \\frac{y_k s_k^\\top}{y_k^\\top s_k}\\right) z + z^\\top \\left(\\frac{s_k s_k^\\top}{y_k^\\top s_k}\\right) z$$\nLet $V = I - \\frac{s_k y_k^\\top}{y_k^\\top s_k}$. The expression becomes:\n$$z^\\top H_{k+1} z = z^\\top V H_k V^\\top z + \\frac{(z^\\top s_k)^2}{y_k^\\top s_k} = (V^\\top z)^\\top H_k (V^\\top z) + \\frac{(z^\\top s_k)^2}{y_k^\\top s_k}$$\nSince $H_k$ is positive definite, the term $(V^\\top z)^\\top H_k (V^\\top z)$ is non-negative. It is strictly positive unless $V^\\top z = 0$.\nFor $z^\\top H_{k+1} z  0$ for all $z \\neq 0$, the second term, $\\frac{(z^\\top s_k)^2}{y_k^\\top s_k}$, plays a critical role. Its denominator, $y_k^\\top s_k$, must be positive. If $y_k^\\top s_k \\le 0$, we cannot guarantee positivity.\nIf we assume the **curvature condition** $s_k^\\top y_k  0$ holds, then the second term $\\frac{(z^\\top s_k)^2}{y_k^\\top s_k}$ is always non-negative. The sum is zero only if both terms are zero, which implies $(V^\\top z)^\\top H_k (V^\\top z) = 0$ and $(z^\\top s_k)^2 = 0$. This occurs only if $z=0$.\nThus, if $H_k$ is positive definite, $H_{k+1}$ remains positive definite if and only if the curvature condition $s_k^\\top y_k  0$ is satisfied. This condition implies that the average curvature of $f$ along the segment $s_k$ is positive.\n\n### Part 2: Algorithm Implementation\n\nThe algorithm is a BFGS quasi-Newton method with a backtracking line search and a restart policy for handling nonconvexity.\n\n**Objective Function and Gradient**\nThe objective function is $f: \\mathbb{R}^2 \\to \\mathbb{R}$:\n$$ f(x_1,x_2) = \\tfrac{1}{2}(x_1^2 - x_2^2) + 0.1(x_1^4 + x_2^4) + 2\\cos(x_1) + 2\\cos(x_2) $$\nIts gradient is $\\nabla f(x_1, x_2) = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}]^\\top$:\n$$ \\nabla f(x_1, x_2) = \\begin{bmatrix} x_1 + 0.4x_1^3 - 2\\sin(x_1) \\\\ -x_2 + 0.4x_2^3 - 2\\sin(x_2) \\end{bmatrix} $$\n\n**Algorithm Steps**\n1.  **Initialization**: Given starting point $x_0$, tolerance $\\varepsilon = 10^{-5}$, max iterations $N_{max}=300$. Initialize inverse Hessian approximation $H_0 = I$ (identity matrix). Set iteration counter $k=0$.\n\n2.  **Convergence Check**: At each iteration $k$, compute the gradient $\\nabla f(x_k)$. If $\\|\\nabla f(x_k)\\| \\le \\varepsilon$, the algorithm has converged.\n\n3.  **Search Direction**: Compute the search direction $p_k = -H_k \\nabla f(x_k)$.\n\n4.  **Descent Direction Safeguard**: Check if $p_k$ is a descent direction by evaluating $\\nabla f(x_k)^\\top p_k$. If $H_k$ is positive definite, this will be negative. If due to numerical issues or a non-positive definite $H_k$ we have $\\nabla f(x_k)^\\top p_k \\ge 0$, the direction is not a descent direction. The safeguard resets the search direction to the steepest descent direction, $p_k = -\\nabla f(x_k)$, and resets $H_k = I$ to ensure the next iteration starts fresh.\n\n5.  **Line Search**: Find a step size $\\alpha_k  0$ using backtracking. Start with $\\alpha_k=1.0$. While the **Armijo condition** $f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$ is not satisfied (with $c_1=10^{-4}$), reduce $\\alpha_k$ by a factor of $0.5$. This is repeated up to a maximum of $50$ times. If a suitable $\\alpha_k$ is not found, the run is considered to have failed.\n\n6.  **Update Position**: Update the iterate: $x_{k+1} = x_k + \\alpha_k p_k$.\n\n7.  **Hessian Approximation Update with Restart Policy**:\n    *   Compute $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n    *   Compute the curvature measure $s_k^\\top y_k$ and the product of norms $\\|s_k\\| \\|y_k\\|$.\n    *   **Restart Rule**: If restarts are enabled and the condition $s_k^\\top y_k \\le \\tau \\|s_k\\| \\|y_k\\|$ is met for a given threshold $\\tau$, the inverse Hessian approximation is reset to the identity matrix: $H_{k+1} = I$. This happens when the curvature is negative, zero, or not sufficiently positive (i.e., the angle between $s_k$ and $y_k$ is large).\n    *   **BFGS Update**: If the restart condition is not met, the standard update is performed, but only if the curvature condition $s_k^\\top y_k  0$ holds. This prevents the loss of positive definiteness.\n    *   **Skip Update**: If neither the restart rule is triggered nor the positive curvature condition is met (e.g., in the no-restart case with $s_k^\\top y_k \\le 0$), the update is skipped: $H_{k+1} = H_k$.\n    *   To avoid numerical issues, updates are also skipped if $\\|s_k\\|$ or $\\|y_k\\|$ are close to zero.\n\n8.  **Iteration**: Increment $k$ and return to step 2. If $k$ reaches $N_{max}$, the run is considered to have failed.\n\n### Part 3: Performance Quantification\n\nThe robustness and efficiency of the algorithm under different restart policies are quantified by running it from a set of 7 initial points for each of the 5 parameter cases (one without restarts, four with different $\\tau$ values). For each case, we compute:\n1.  **Success Count**: The number of initial points from which the algorithm converges to a solution satisfying $\\|\\nabla f(x_k)\\| \\le 10^{-5}$.\n2.  **Mean Iterations**: The arithmetic mean of the number of iterations for all successful runs. If no runs are successful, this is $0.0$.\n3.  **Mean Backtracking Reductions**: The arithmetic mean of the total number of backtracking step size reductions for all successful runs. This indicates the cost of the line search. If no runs are successful, this is $0.0$.\n\nComparing these statistics across the parameter cases reveals the impact of the restart strategy on the algorithm's ability to solve problems on a nonconvex landscape. A higher success count indicates greater robustness. Lower mean iterations and backtracking reductions for a similar success count indicate higher efficiency.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the BFGS experiments and print the results.\n    \"\"\"\n    \n    # ------------------ Problem Definition ------------------\n    \n    def f(x):\n        \"\"\"Nonconvex objective function.\"\"\"\n        x1, x2 = x[0], x[1]\n        return (0.5 * (x1**2 - x2**2) + \n                0.1 * (x1**4 + x2**4) + \n                2.0 * np.cos(x1) + 2.0 * np.cos(x2))\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        x1, x2 = x[0], x[1]\n        df_dx1 = x1 + 0.4 * x1**3 - 2.0 * np.sin(x1)\n        df_dx2 = -x2 + 0.4 * x2**3 - 2.0 * np.sin(x2)\n        return np.array([df_dx1, df_dx2])\n\n    # ------------------ BFGS Solver Implementation ------------------\n\n    def bfgs_solver(x0, restart_enabled, tau, epsilon, max_iter, c1, shrink, max_bt):\n        \"\"\"\n        BFGS solver with optional restarts.\n        Returns: (success, iterations, total_backtracking_reductions)\n        \"\"\"\n        x = np.array(x0, dtype=float)\n        n = len(x)\n        H = np.identity(n)\n        \n        total_bt_reductions = 0\n        \n        for k in range(max_iter):\n            g = grad_f(x)\n            \n            if np.linalg.norm(g) = epsilon:\n                return True, k, total_bt_reductions\n\n            p = -H @ g\n            grad_dot_p = g @ p\n            \n            # Safeguard against non-descent directions\n            if grad_dot_p >= 0:\n                H = np.identity(n)\n                p = -g\n                grad_dot_p = g @ p\n\n            # Backtracking line search (Armijo condition)\n            alpha = 1.0\n            fx = f(x)\n            num_bt = 0\n            while num_bt  max_bt:\n                x_new = x + alpha * p\n                if f(x_new) = fx + c1 * alpha * grad_dot_p:\n                    break\n                alpha *= shrink\n                num_bt += 1\n            \n            if num_bt == max_bt:\n                # Line search failed to find a step\n                return False, k + 1, total_bt_reductions\n\n            total_bt_reductions += num_bt\n            \n            # Update variables\n            s = alpha * p\n            x_new = x + s\n            g_new = grad_f(x_new)\n            y = g_new - g\n            \n            # Update H. Note: x is updated at the end of the loop\n            norm_s = np.linalg.norm(s)\n            norm_y = np.linalg.norm(y)\n\n            # Avoid numerical instability with very small steps/gradient changes\n            if norm_s > 1e-12 and norm_y > 1e-12:\n                sTy = s @ y\n                perform_restart = restart_enabled and (sTy = tau * norm_s * norm_y)\n\n                if perform_restart:\n                    H = np.identity(n)\n                elif sTy > 0:\n                    # BFGS inverse Hessian update formula\n                    # H_{k+1} = (I - rho*s*y.T) H (I - rho*y*s.T) + rho*s*s.T\n                    rho = 1.0 / sTy\n                    Hy = H @ y\n                    # Using a numerically stable expansion\n                    H += rho * (1.0 + rho * (y @ Hy)) * np.outer(s, s) - \\\n                         rho * (np.outer(Hy, s) + np.outer(s, Hy))\n                # else: curvature condition not met, skip update (H remains H)\n            \n            x = x_new\n\n        return False, max_iter, total_bt_reductions\n\n    # ------------------ Experiment Setup ------------------\n\n    # Common settings\n    epsilon = 1e-5\n    max_iter = 300\n    c1 = 1e-4\n    shrink_factor = 0.5\n    max_bt_reductions = 50\n\n    # Initial points\n    initial_points = [\n        [-3.0, -3.0], [2.0, 2.0], [4.0, -4.0], [0.1, -0.1],\n        [-2.5, 2.25], [5.0, 5.0], [-5.0, 5.0]\n    ]\n\n    # Parameter cases\n    parameter_cases = [\n        {'restart_enabled': False, 'tau': 0.0},       # Case 1: No restarts\n        {'restart_enabled': True, 'tau': 1e-8},       # Case 2\n        {'restart_enabled': True, 'tau': 1e-4},       # Case 3\n        {'restart_enabled': True, 'tau': 1e-2},       # Case 4\n        {'restart_enabled': True, 'tau': 5e-1}        # Case 5\n    ]\n    \n    final_results = []\n\n    # ------------------ Run Experiments and Collect Data ------------------\n    \n    for case in parameter_cases:\n        successful_runs = 0\n        successful_iters = []\n        successful_backs = []\n        \n        for x0 in initial_points:\n            success, iters, backs = bfgs_solver(\n                x0,\n                case['restart_enabled'],\n                case['tau'],\n                epsilon,\n                max_iter,\n                c1,\n                shrink_factor,\n                max_bt_reductions\n            )\n            \n            if success:\n                successful_runs += 1\n                successful_iters.append(iters)\n                successful_backs.append(backs)\n        \n        # Calculate statistics for the case\n        mean_iters = np.mean(successful_iters) if successful_iters else 0.0\n        mean_backs = np.mean(successful_backs) if successful_backs else 0.0\n        \n        final_results.extend([successful_runs, mean_iters, mean_backs])\n\n    # ------------------ Format and Print Output ------------------\n    \n    # Format the results into the required string format\n    output_str = \",\".join(map(str, final_results))\n    print(f\"[{output_str}]\")\n\n\nsolve()\n\n```", "id": "3170188"}]}