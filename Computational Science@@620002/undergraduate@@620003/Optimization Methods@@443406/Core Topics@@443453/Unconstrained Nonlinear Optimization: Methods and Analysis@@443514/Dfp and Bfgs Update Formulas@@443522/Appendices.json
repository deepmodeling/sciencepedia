{"hands_on_practices": [{"introduction": "The best way to understand the DFP and BFGS updates is to compute them by hand. This exercise guides you through a direct calculation for a simple two-dimensional case, starting from the identity matrix $H_0 = I$. By working through the arithmetic, you will gain a concrete feel for how the vectors $s_k$ and $y_k$ are used to build the rank-two corrections and see the structural differences between the two formulas firsthand [@problem_id:3119496].", "problem": "Consider a twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ and a quasi-Newton iteration that maintains a symmetric positive definite approximation $H_{k}$ to the inverse Hessian of $f$. The next approximation $H_{k+1}$ is required to satisfy the quasi-Newton secant condition $H_{k+1}y_{k}=s_{k}$ and be obtained by a rank-two symmetric correction to $H_{k}$ following the least-change principle in an appropriate inner-product space. Two such constructions are classical: the Davidon–Fletcher–Powell (DFP) update and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update, both defined for inverse-Hessian approximations and guaranteed to preserve positive definiteness when $s_{k}^{\\top}y_{k}>0$.\n\nStarting from the fundamental quasi-Newton secant condition and symmetry, derive the two inverse-Hessian updates that correspond to the DFP and BFGS constructions. Then, in the concrete case $n=2$ with $H_{0}=I$, $s=(1,0)^{\\top}$, and $y=(1,\\epsilon)^{\\top}$ for a scalar $\\epsilon$ with $s^{\\top}y=1>0$, compute by hand the resulting updated matrices $H_{1}^{\\mathrm{DFP}}$ and $H_{1}^{\\mathrm{BFGS}}$.\n\nDefine the scalar function\n$$\n\\Delta(\\epsilon) \\equiv \\ln\\!\\big(\\det(H_{1}^{\\mathrm{DFP}})\\big) - \\ln\\!\\big(\\det(H_{1}^{\\mathrm{BFGS}})\\big).\n$$\nEvaluate the limit\n$$\n\\lim_{\\epsilon\\to 0} \\frac{\\Delta(\\epsilon)}{\\epsilon^{2}}.\n$$\nProvide your final answer as a single real number. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The context is the field of numerical optimization, specifically quasi-Newton methods. I will proceed with the solution.\n\nThe problem requires the derivation and application of the Davidon–Fletcher–Powell (DFP) and Broyden–Fletcher–Goldfarb–Shanno (BFGS) update formulas for the inverse Hessian approximation, $H_k$. These updates are central to quasi-Newton methods for unconstrained optimization. They are derived from the principle of least change, which seeks a new approximation $H_{k+1}$ that is closest to the current one $H_k$ (in a specific norm) while satisfying the secant condition $H_{k+1} y_k = s_k$. Here, $s_k = x_{k+1} - x_k$ is the step in the variables, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient.\n\nThe DFP update for the inverse Hessian approximation $H_k$ is given by:\n$$\nH_{k+1}^{\\mathrm{DFP}} = H_k + \\frac{s_k s_k^\\top}{s_k^\\top y_k} - \\frac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k}\n$$\nThis update arises from minimizing $\\|H_{k+1} - H_k\\|$ in a weighted Frobenius norm related to the geometry induced by the Hessian.\n\nThe BFGS update for the inverse Hessian approximation $H_k$ is given by:\n$$\nH_{k+1}^{\\mathrm{BFGS}} = H_k + \\left(1 + \\frac{y_k^\\top H_k y_k}{s_k^\\top y_k}\\right) \\frac{s_k s_k^\\top}{s_k^\\top y_k} - \\frac{H_k y_k s_k^\\top + s_k y_k^\\top H_k}{s_k^\\top y_k}\n$$\nThe BFGS update is dual to the DFP update; it is derived by minimizing the change in the Hessian approximation $B_k = H_k^{-1}$ and then inverting the result using the Sherman-Morrison-Woodbury formula. Both updates generate symmetric matrices and preserve positive definiteness if $s_k^\\top y_k > 0$.\n\nWe are given the specific case for $k=0$ with $n=2$:\n- Initial inverse Hessian approximation: $H_0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n- Step vector: $s \\equiv s_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- Gradient difference vector: $y \\equiv y_0 = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}$\n\nFirst, we compute the necessary scalar products:\n- $s^\\top y = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = 1(1) + 0(\\epsilon) = 1$. The condition $s^\\top y > 0$ is satisfied.\n- $H_0 y = I y = y = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}$.\n- $y^\\top H_0 y = y^\\top (I y) = y^\\top y = \\begin{pmatrix} 1 & \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = 1^2 + \\epsilon^2 = 1 + \\epsilon^2$.\n\nNext, we compute the necessary outer products:\n- $s s^\\top = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n- $H_0 y y^\\top H_0 = (H_0 y)(H_0 y)^\\top = y y^\\top = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 & \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & \\epsilon^2 \\end{pmatrix}$.\n\nNow we compute $H_1^{\\mathrm{DFP}}$ by substituting these into the DFP formula:\n$$\nH_1^{\\mathrm{DFP}} = H_0 + \\frac{s s^\\top}{s^\\top y} - \\frac{H_0 y y^\\top H_0}{y^\\top H_0 y}\n$$\n$$\nH_1^{\\mathrm{DFP}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\frac{1}{1} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\frac{1}{1 + \\epsilon^2} \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & \\epsilon^2 \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{DFP}} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{1 + \\epsilon^2} & \\frac{\\epsilon}{1 + \\epsilon^2} \\\\ \\frac{\\epsilon}{1 + \\epsilon^2} & \\frac{\\epsilon^2}{1 + \\epsilon^2} \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{1}{1 + \\epsilon^2} & -\\frac{\\epsilon}{1 + \\epsilon^2} \\\\ -\\frac{\\epsilon}{1 + \\epsilon^2} & 1 - \\frac{\\epsilon^2}{1 + \\epsilon^2} \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{DFP}} = \\begin{pmatrix} \\frac{2(1 + \\epsilon^2) - 1}{1 + \\epsilon^2} & -\\frac{\\epsilon}{1 + \\epsilon^2} \\\\ -\\frac{\\epsilon}{1 + \\epsilon^2} & \\frac{1 + \\epsilon^2 - \\epsilon^2}{1 + \\epsilon^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + 2\\epsilon^2}{1 + \\epsilon^2} & -\\frac{\\epsilon}{1 + \\epsilon^2} \\\\ -\\frac{\\epsilon}{1 + \\epsilon^2} & \\frac{1}{1 + \\epsilon^2} \\end{pmatrix}\n$$\n\nNext, we compute $H_1^{\\mathrm{BFGS}}$. We need the additional cross terms:\n- $H_0 y s^\\top = y s^\\top = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ \\epsilon & 0 \\end{pmatrix}$.\n- $s y^\\top H_0 = s y^\\top = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 0 \\end{pmatrix}$.\nSubstituting into the BFGS formula:\n$$\nH_1^{\\mathrm{BFGS}} = H_0 + \\left(1 + \\frac{y^\\top H_0 y}{s^\\top y}\\right) \\frac{s s^\\top}{s^\\top y} - \\frac{H_0 y s^\\top + s y^\\top H_0}{s^\\top y}\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\left(1 + \\frac{1 + \\epsilon^2}{1}\\right) \\frac{1}{1} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\frac{1}{1} \\left( \\begin{pmatrix} 1 & 0 \\\\ \\epsilon & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 0 \\end{pmatrix} \\right)\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + (2 + \\epsilon^2) \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 2 & \\epsilon \\\\ \\epsilon & 0 \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 2 + \\epsilon^2 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 2 & \\epsilon \\\\ \\epsilon & 0 \\end{pmatrix} = \\begin{pmatrix} 1 + 2 + \\epsilon^2 - 2 & 0 + 0 - \\epsilon \\\\ 0 + 0 - \\epsilon & 1 + 0 - 0 \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 + \\epsilon^2 & -\\epsilon \\\\ -\\epsilon & 1 \\end{pmatrix}\n$$\n\nNow we compute the determinants of these matrices.\nFor $H_1^{\\mathrm{DFP}}$:\n$$\n\\det(H_1^{\\mathrm{DFP}}) = \\left(\\frac{1 + 2\\epsilon^2}{1 + \\epsilon^2}\\right) \\left(\\frac{1}{1 + \\epsilon^2}\\right) - \\left(-\\frac{\\epsilon}{1 + \\epsilon^2}\\right) \\left(-\\frac{\\epsilon}{1 + \\epsilon^2}\\right)\n$$\n$$\n\\det(H_1^{\\mathrm{DFP}}) = \\frac{1 + 2\\epsilon^2}{(1 + \\epsilon^2)^2} - \\frac{\\epsilon^2}{(1 + \\epsilon^2)^2} = \\frac{1 + 2\\epsilon^2 - \\epsilon^2}{(1 + \\epsilon^2)^2} = \\frac{1 + \\epsilon^2}{(1 + \\epsilon^2)^2} = \\frac{1}{1 + \\epsilon^2}\n$$\nFor $H_1^{\\mathrm{BFGS}}$:\n$$\n\\det(H_1^{\\mathrm{BFGS}}) = (1 + \\epsilon^2)(1) - (-\\epsilon)(-\\epsilon) = 1 + \\epsilon^2 - \\epsilon^2 = 1\n$$\nThese results are consistent with the general determinant update formulas for DFP and BFGS:\n$\\det(H_{k+1}^{\\mathrm{DFP}}) = \\det(H_k) \\frac{s_k^\\top y_k}{y_k^\\top H_k y_k}$ and $\\det(H_{k+1}^{\\mathrm{BFGS}}) = \\det(H_k) \\frac{s_k^\\top H_k^{-1} s_k}{s_k^\\top y_k}$.\nIn our case, $\\det(H_0)=1$, $s^\\top y = 1$, $y^\\top H_0 y = 1+\\epsilon^2$, $s^\\top H_0^{-1} s = s^\\top I s = s^\\top s = 1$.\n$\\det(H_1^{\\mathrm{DFP}}) = 1 \\cdot \\frac{1}{1+\\epsilon^2} = \\frac{1}{1+\\epsilon^2}$.\n$\\det(H_1^{\\mathrm{BFGS}}) = 1 \\cdot \\frac{1}{1} = 1$.\n\nWe are asked to evaluate the limit of $\\frac{\\Delta(\\epsilon)}{\\epsilon^2}$ as $\\epsilon \\to 0$, where\n$$\n\\Delta(\\epsilon) = \\ln\\big(\\det(H_1^{\\mathrm{DFP}})\\big) - \\ln\\big(\\det(H_1^{\\mathrm{BFGS}})\\big)\n$$\nSubstituting our results for the determinants:\n$$\n\\Delta(\\epsilon) = \\ln\\left(\\frac{1}{1 + \\epsilon^2}\\right) - \\ln(1) = -\\ln(1 + \\epsilon^2) - 0 = -\\ln(1 + \\epsilon^2)\n$$\nThe limit to be evaluated is:\n$$\n\\lim_{\\epsilon\\to 0} \\frac{\\Delta(\\epsilon)}{\\epsilon^2} = \\lim_{\\epsilon\\to 0} \\frac{-\\ln(1 + \\epsilon^2)}{\\epsilon^2}\n$$\nThis is an indeterminate form $\\frac{0}{0}$. We can resolve this using the Taylor series expansion for $\\ln(1+x)$ around $x=0$, which is $\\ln(1+x) = x - \\frac{x^2}{2} + O(x^3)$.\nLetting $x = \\epsilon^2$, we have:\n$$\n\\ln(1 + \\epsilon^2) = \\epsilon^2 - \\frac{(\\epsilon^2)^2}{2} + O((\\epsilon^2)^3) = \\epsilon^2 - \\frac{\\epsilon^4}{2} + O(\\epsilon^6)\n$$\nSubstituting this into the limit expression:\n$$\n\\lim_{\\epsilon\\to 0} \\frac{-(\\epsilon^2 - \\frac{\\epsilon^4}{2} + O(\\epsilon^6))}{\\epsilon^2} = \\lim_{\\epsilon\\to 0} \\left(-1 + \\frac{\\epsilon^2}{2} - O(\\epsilon^4)\\right)\n$$\nAs $\\epsilon \\to 0$, all terms except the constant term vanish.\n$$\n\\lim_{\\epsilon\\to 0} \\frac{-\\ln(1 + \\epsilon^2)}{\\epsilon^2} = -1\n$$\nAlternatively, applying L'Hôpital's rule with respect to the variable $u = \\epsilon^2$:\n$$\n\\lim_{u\\to 0^+} \\frac{-\\ln(1 + u)}{u} = \\lim_{u\\to 0^+} \\frac{-\\frac{d}{du}\\ln(1 + u)}{\\frac{d}{du}u} = \\lim_{u\\to 0^+} \\frac{-1/(1+u)}{1} = \\frac{-1}{1+0} = -1\n$$\nBoth methods yield the same result.", "answer": "$$\n\\boxed{-1}\n$$", "id": "3119496"}, {"introduction": "While the DFP and BFGS formulas appear similar, their performance in practice can differ dramatically due to numerical stability. This problem constructs a scenario where the DFP update is prone to generating an ill-conditioned inverse Hessian approximation. By analyzing the critical denominator term $y_k^\\top H_k y_k$ in the DFP update, you will diagnose a specific weakness that the BFGS formulation successfully avoids, illustrating why BFGS is generally preferred [@problem_id:3119434].", "problem": "Consider unconstrained minimization of the strictly convex quadratic function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ with \n$$\nQ \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix},\n$$\nand suppose we apply a quasi-Newton method with the initial inverse-Hessian approximation \n$$\nH_{0} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-8} \\end{pmatrix}.\n$$\nDefine the starting point $x_{0}$ implicitly by requiring that the first quasi-Newton search direction $p_{0}$, given by $p_{0} = - H_{0} \\nabla f(x_{0})$, equals the vector \n$$\nv \\;=\\; \\begin{pmatrix} -\\tfrac{1}{3} \\\\ \\tfrac{2}{3} \\end{pmatrix}.\n$$\nTake a unit step length, so that $x_{1} = x_{0} + p_{0}$, and define the displacement and gradient change as $s_{0} = x_{1}-x_{0} = p_{0}$ and $y_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0})$. For this strictly convex quadratic, use the fundamental facts that $\\nabla f(x) = Q x$ and $y_{0} = Q s_{0}$.\n\nYour task is to analyze why the Davidon-Fletcher-Powell (DFP) update can become ill-conditioned in this setting while the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update remains well-scaled, by focusing on the curvature quantities $y_{0}^{\\top} H_{0} y_{0}$ (which appears in DFP) and $s_{0}^{\\top} y_{0}$ (which appears in BFGS). In particular, compute the exact value of the DFP-critical denominator $y_{0}^{\\top} H_{0} y_{0}$ for the construction above.\n\nGive your final answer as a single real number. No rounding is required.", "solution": "We start from the core definitions for a quadratic model. For $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ with symmetric positive definite $Q$, the gradient is $\\nabla f(x) = Q x$. For a quasi-Newton step, the search direction is $p_{0} = - H_{0} \\nabla f(x_{0})$. The step and gradient displacement are $s_{0} = x_{1} - x_{0}$ and $y_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0})$. For a quadratic, the gradient is affine, so \n$$\ny_{0} \\;=\\; \\nabla f(x_{0} + s_{0}) - \\nabla f(x_{0}) \\;=\\; Q s_{0}.\n$$\n\nBy construction, we require $p_{0} = v$ where \n$$\nv \\;=\\; \\begin{pmatrix} -\\tfrac{1}{3} \\\\ \\tfrac{2}{3} \\end{pmatrix}.\n$$\nWe also take a unit step length, so $s_{0} = p_{0} = v$. Therefore,\n$$\ny_{0} \\;=\\; Q s_{0} \\;=\\; Q v \\;=\\; \n\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n\\begin{pmatrix} -\\tfrac{1}{3} \\\\ \\tfrac{2}{3} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 2\\cdot(-\\tfrac{1}{3}) + 1\\cdot(\\tfrac{2}{3}) \\\\ 1\\cdot(-\\tfrac{1}{3}) + 2\\cdot(\\tfrac{2}{3}) \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nThus, $y_{0}$ is exactly the second coordinate unit vector. The DFP update uses the curvature quantity $y_{0}^{\\top} H_{0} y_{0}$ in the denominator of its second term. We compute\n$$\ny_{0}^{\\top} H_{0} y_{0}\n\\;=\\;\n\\begin{pmatrix} 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-8} \\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;=\\;\n10^{-8}.\n$$\n\nThis shows why the Davidon-Fletcher-Powell (DFP) update can become numerically ill-conditioned: the denominator $y_{0}^{\\top} H_{0} y_{0}$ is extremely small, so the rank-one correction term that involves division by $y_{0}^{\\top} H_{0} y_{0}$ will be excessively large in magnitude, which destabilizes $H_{1}$ and can stall progress. In contrast, for the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update, the relevant curvature quantity is $s_{0}^{\\top} y_{0}$, and here \n$$\ns_{0}^{\\top} y_{0} \\;=\\; v^{\\top} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\;=\\; \\tfrac{2}{3},\n$$\nwhich is well-scaled. Hence, BFGS remains well-behaved while DFP is prone to a numerically dangerous amplification, providing the desired contrast.\n\nThe requested exact value of $y_{0}^{\\top} H_{0} y_{0}$ is therefore $10^{-8}$.", "answer": "$$\\boxed{1 \\times 10^{-8}}$$", "id": "3119434"}, {"introduction": "Theory is essential, but the ultimate test of an optimization method is its performance in a realistic context. This exercise simulates a single step of an optimization algorithm on a quadratic function, including an exact line search to find the optimal step size $\\alpha_k$. By computing both the DFP and BFGS updates and comparing them to the true inverse Hessian, you will gain a practical understanding of why the BFGS method is often a more effective and reliable choice in computational practice [@problem_id:3285119].", "problem": "Consider unconstrained minimization of a smooth function $f:\\mathbb{R}^n \\to \\mathbb{R}$ using quasi-Newton methods that build a sequence of symmetric positive definite approximations $H_k \\approx \\nabla^2 f(x_k)^{-1}$ to the inverse Hessian. The quasi-Newton updates are characterized by the secant condition and a minimal-change principle:\n- The secant condition requires $H_{k+1} y_k = s_k$, where $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n- The minimal-change principle selects $H_{k+1}$ closest to $H_k$ (in a suitable matrix norm) among symmetric matrices satisfying the secant condition.\n\nTwo classical choices are the Davidon-Fletcher-Powell (DFP) and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) inverse updates, each arising from a different minimal-change metric but both enforcing the secant condition and symmetry.\n\nPerform the following one-step numerical experiment on a strictly convex quadratic and compare the two updates.\n\nData for the experiment:\n- Let $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$ with $Q = \\begin{bmatrix} 10 & 2 \\\\ 2 & 1 \\end{bmatrix}$ (symmetric positive definite), $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, and initial point $x_0 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n- Let $H_0 = I$ (the $2\\times 2$ identity), and take the search direction $p_0 = - H_0 \\nabla f(x_0)$.\n- Use the exact line search $\\alpha_0 = \\arg\\min_{\\alpha \\in \\mathbb{R}} f(x_0 + \\alpha p_0)$ to define $x_1 = x_0 + \\alpha_0 p_0$, then compute $s_0 = x_1 - x_0$ and $y_0 = \\nabla f(x_1) - \\nabla f(x_0)$.\n- Using the standard inverse-Hessian DFP and BFGS update constructions (each defined by enforcing symmetry and the secant condition while minimizing a suitable matrix norm of $H_{k+1} - H_k$), build $H_1^{\\mathrm{DFP}}$ and $H_1^{\\mathrm{BFGS}}$ from $H_0$, $s_0$, and $y_0$.\n\nUsing these $H_1$ matrices, answer which of the following statements are correct for this experiment:\n\nA. For this quadratic and setup, both $H_1^{\\mathrm{DFP}}$ and $H_1^{\\mathrm{BFGS}}$ are symmetric positive definite, and $H_1^{\\mathrm{BFGS}}$ is closer to the true $Q^{-1}$ in Frobenius norm.\n\nB. For this quadratic and setup, $H_1^{\\mathrm{DFP}}$ is indefinite, whereas $H_1^{\\mathrm{BFGS}}$ is positive definite.\n\nC. $H_1^{\\mathrm{BFGS}}$ does not satisfy the secant condition $H_1 y_0 = s_0$, whereas $H_1^{\\mathrm{DFP}}$ does.\n\nD. Both updates satisfy the secant condition and symmetry; in this experiment, $\\lVert H_1^{\\mathrm{BFGS}} - Q^{-1}\\rVert_F < \\lVert H_1^{\\mathrm{DFP}} - Q^{-1}\\rVert_F$.\n\nE. With exact line search on a strictly convex quadratic, the DFP and BFGS inverse updates coincide, so $H_1^{\\mathrm{DFP}} = H_1^{\\mathrm{BFGS}}$.\n\nSelect all that apply.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\nThe problem statement provides the following information for an unconstrained minimization problem of a function $f:\\mathbb{R}^n \\to \\mathbb{R}$ using quasi-Newton methods.\n\n- **Function to minimize**: $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$.\n- **Matrix $Q$**: $Q = \\begin{bmatrix} 10 & 2 \\\\ 2 & 1 \\end{bmatrix}$. The problem statement asserts it is symmetric positive definite.\n- **Vector $b$**: $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, so $f(x) = \\tfrac{1}{2} x^\\top Q x$.\n- **Initial point**: $x_0 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n- **Initial inverse Hessian approximation**: $H_0 = I$, the $2\\times 2$ identity matrix.\n- **Search direction rule**: $p_0 = - H_0 \\nabla f(x_0)$.\n- **Line search rule**: Exact line search, $\\alpha_0 = \\arg\\min_{\\alpha \\in \\mathbb{R}} f(x_0 + \\alpha p_0)$.\n- **Iterate update**: $x_1 = x_0 + \\alpha_0 p_0$.\n- **Update vectors**: $s_0 = x_1 - x_0$ and $y_0 = \\nabla f(x_1) - \\nabla f(x_0)$.\n- **Quasi-Newton updates**:\n    - Davidon-Fletcher-Powell (DFP) inverse update: $H_1^{\\mathrm{DFP}}$.\n    - Broyden-Fletcher-Goldfarb-Shanno (BFGS) inverse update: $H_1^{\\mathrm{BFGS}}$.\n- **Core properties**: The updates enforce the secant condition $H_{k+1} y_k = s_k$ and symmetry.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It is a standard numerical experiment in the field of numerical optimization, comparing two of the most famous quasi-Newton update formulas (DFP and BFGS) on a strictly convex quadratic function. The matrix $Q$ given is indeed symmetric. Its determinant is $\\det(Q) = (10)(1) - (2)(2) = 6 > 0$ and its trace is $10+1=11 > 0$, confirming it is positive definite. The function $f(x)$ is therefore strictly convex with a unique minimizer. All procedures and definitions are standard in the field.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly relevant to unconstrained optimization.\n3.  **Incomplete or Contradictory Setup**: The problem is well-defined and self-contained. All data required to perform the one-step experiment are provided. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The setup is a textbook example and is completely feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. A unique solution exists for the calculations requested. The terminology used is standard and unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial; it requires detailed calculation and understanding of the properties of quasi-Newton methods. It is a substantive test of knowledge.\n7.  **Outside Scientific Verifiability**: All claims and calculations are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will proceed.\n\n## Derivation and Solution\n\nThe core of the problem is to compute the DFP and BFGS updated inverse Hessian approximations, $H_1^{\\mathrm{DFP}}$ and $H_1^{\\mathrm{BFGS}}$, after one step of optimization, and compare them.\n\n**Step 1: Initial Calculations**\nThe function is $f(x) = \\frac{1}{2}x^\\top Q x$. The gradient is $\\nabla f(x) = Qx$.\n- **Initial gradient**:\n$$ \\nabla f(x_0) = Q x_0 = \\begin{bmatrix} 10 & 2 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 10 - 2 \\\\ 2 - 1 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 1 \\end{bmatrix} $$\n- **Initial search direction**:\n$$ p_0 = -H_0 \\nabla f(x_0) = -I \\nabla f(x_0) = -\\begin{bmatrix} 8 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix} $$\n\n**Step 2: Exact Line Search**\nThe step length $\\alpha_0$ is found by minimizing $f(x_0 + \\alpha p_0)$. For a quadratic function, the exact step length is given by $\\alpha_k = - \\frac{\\nabla f(x_k)^\\top p_k}{p_k^\\top Q p_k}$.\n- **Numerator**: $\\nabla f(x_0)^\\top p_0 = \\begin{bmatrix} 8 & 1 \\end{bmatrix} \\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix} = -64 - 1 = -65$.\n- **Denominator**:\n  $Q p_0 = \\begin{bmatrix} 10 & 2 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -80 - 2 \\\\ -16 - 1 \\end{bmatrix} = \\begin{bmatrix} -82 \\\\ -17 \\end{bmatrix}$.\n  $p_0^\\top Q p_0 = \\begin{bmatrix} -8 & -1 \\end{bmatrix} \\begin{bmatrix} -82 \\\\ -17 \\end{bmatrix} = (-8)(-82) + (-1)(-17) = 656 + 17 = 673$.\n- **Step length**:\n$$ \\alpha_0 = - \\frac{-65}{673} = \\frac{65}{673} $$\n\n**Step 3: Compute Update Vectors $s_0$ and $y_0$**\n- **Step vector $s_0$**:\n$$ s_0 = x_1 - x_0 = \\alpha_0 p_0 = \\frac{65}{673} \\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix} $$\n- **Gradient difference vector $y_0$**: For a quadratic function, $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k) = Q(x_{k+1} - x_k) = Q s_k$.\n$$ y_0 = Q s_0 = \\frac{65}{673} Q \\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix} = \\frac{65}{673} \\begin{bmatrix} -82 \\\\ -17 \\end{bmatrix} $$\nWe must verify the curvature condition $s_0^\\top y_0 > 0$.\n$$ s_0^\\top y_0 = s_0^\\top (Q s_0) = \\alpha_0^2 p_0^\\top Q p_0 = \\left(\\frac{65}{673}\\right)^2 (673) = \\frac{65^2}{673} = \\frac{4225}{673} > 0 $$\nThe condition holds, which is expected as $Q$ is positive definite.\n\n**Step 4: Update Formulas**\nThe DFP update for the inverse Hessian is:\n$$ H_{k+1}^{\\mathrm{DFP}} = H_k + \\frac{s_k s_k^\\top}{s_k^\\top y_k} - \\frac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} $$\nThe BFGS update for the inverse Hessian is:\n$$ H_{k+1}^{\\mathrm{BFGS}} = H_k + \\frac{(s_k^\\top y_k + y_k^\\top H_k y_k) s_k s_k^\\top}{(s_k^\\top y_k)^2} - \\frac{H_k y_k s_k^\\top + s_k y_k^\\top H_k}{s_k^\\top y_k} $$\nFor our problem, $k=0$ and $H_0=I$.\n\n### Option-by-Option Analysis\n\n**Option C: $H_1^{\\mathrm{BFGS}}$ does not satisfy the secant condition $H_1 y_0 = s_0$, whereas $H_1^{\\mathrm{DFP}}$ does.**\nBoth DFP and BFGS methods are constructed to satisfy the secant condition $H_{k+1} y_k = s_k$. This can be shown by direct substitution. For any generic update from the Broyden class (which includes DFP and BFGS), this condition is central to the derivation. The problem statement itself says the updates are characterized by the secant condition. Therefore, this statement is false.\n**Verdict: Incorrect.**\n\n**Option E: With exact line search on a strictly convex quadratic, the DFP and BFGS inverse updates coincide, so $H_1^{\\mathrm{DFP}} = H_1^{\\mathrm{BFGS}}$.**\nThe DFP and BFGS updates are identical if and only if $H_k y_k$ is a scalar multiple of $s_k$. With $H_0 = I$, this condition becomes $y_0$ being a scalar multiple of $s_0$.\n$s_0 = \\frac{65}{673} \\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix}$ and $y_0 = \\frac{65}{673} \\begin{bmatrix} -82 \\\\ -17 \\end{bmatrix}$.\nThe vectors $\\begin{bmatrix} -8 \\\\ -1 \\end{bmatrix}$ and $\\begin{bmatrix} -82 \\\\ -17 \\end{bmatrix}$ are not parallel, since $-82/(-8) = 10.25$ while $-17/(-1) = 17$.\nThus, $H_1^{\\mathrm{DFP}} \\neq H_1^{\\mathrm{BFGS}}$.\n**Verdict: Incorrect.**\n\n**Option B: For this quadratic and setup, $H_1^{\\mathrm{DFP}}$ is indefinite, whereas $H_1^{\\mathrm{BFGS}}$ is positive definite.**\nA fundamental property of both DFP and BFGS updates is that they preserve positive definiteness of the inverse Hessian approximation $H_k$, provided that $H_k$ is symmetric positive definite and the curvature condition $s_k^\\top y_k > 0$ holds. In our experiment, $H_0 = I$ is symmetric positive definite, and we have shown $s_0^\\top y_0 > 0$. Therefore, both $H_1^{\\mathrm{DFP}}$ and $H_1^{\\mathrm{BFGS}}$ must be symmetric positive definite. The claim that $H_1^{\\mathrm{DFP}}$ is indefinite is false.\n**Verdict: Incorrect.**\n\n**Option A and D Analysis**\nBoth options A and D make claims about properties of the updates and then compare their respective distances to the true inverse Hessian, $Q^{-1}$.\n- **A**: \"both $H_1^{\\mathrm{DFP}}$ and $H_1^{\\mathrm{BFGS}}$ are symmetric positive definite, and $H_1^{\\mathrm{BFGS}}$ is closer to the true $Q^{-1}$ in Frobenius norm.\"\n- **D**: \"Both updates satisfy the secant condition and symmetry; in this experiment, $\\lVert H_1^{\\mathrm{BFGS}} - Q^{-1}\\rVert_F < \\lVert H_1^{\\mathrm{DFP}} - Q^{-1}\\rVert_F$.\"\n\nThe preliminary clauses in both statements are correct:\n- From the analysis of B, both $H_1$ matrices are symmetric and positive definite. (Premise of A is correct).\n- From the analysis of C and by construction, both updates satisfy the secant condition and produce symmetric matrices. (Premise of D is correct).\n\nThe core of both A and D is the inequality $\\lVert H_1^{\\mathrm{BFGS}} - Q^{-1}\\rVert_F < \\lVert H_1^{\\mathrm{DFP}} - Q^{-1}\\rVert_F$. We must compute the matrices and the norms to verify this.\n\n**Step 5: Compute Matrices and Norms**\n- **True Inverse Hessian $Q^{-1}$**:\n$Q = \\begin{bmatrix} 10 & 2 \\\\ 2 & 1 \\end{bmatrix}$, $\\det(Q) = 6$.\n$Q^{-1} = \\frac{1}{6} \\begin{bmatrix} 1 & -2 \\\\ -2 & 10 \\end{bmatrix} \\approx \\begin{bmatrix} 0.1667 & -0.3333 \\\\ -0.3333 & 1.6667 \\end{bmatrix}$.\n\n- **DFP Update $H_1^{\\mathrm{DFP}}$**:\nWe need the terms for the DFP formula with $H_0=I$:\n$y_0^\\top y_0 = \\left(\\frac{65}{673}\\right)^2 ((-82)^2 + (-17)^2) = \\left(\\frac{65}{673}\\right)^2 (6724+289) = \\left(\\frac{65}{673}\\right)^2 (7013)$.\n$$ H_1^{\\mathrm{DFP}} = I + \\frac{s_0 s_0^\\top}{s_0^\\top y_0} - \\frac{y_0 y_0^\\top}{y_0^\\top y_0} = I + \\frac{\\alpha_0^2 p_0 p_0^\\top}{\\alpha_0^2 p_0^\\top Q p_0} - \\frac{\\alpha_0^2 (Qp_0)(Qp_0)^\\top}{\\alpha_0^2 (Qp_0)^\\top(Qp_0)} $$\n$$ H_1^{\\mathrm{DFP}} = I + \\frac{p_0 p_0^\\top}{p_0^\\top Q p_0} - \\frac{(Qp_0)(Qp_0)^\\top}{(Qp_0)^\\top(Qp_0)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\frac{1}{673} \\begin{bmatrix} 64 & 8 \\\\ 8 & 1 \\end{bmatrix} - \\frac{1}{7013} \\begin{bmatrix} 6724 & 1394 \\\\ 1394 & 289 \\end{bmatrix} $$\nNumerically:\n$ H_1^{\\mathrm{DFP}} \\approx \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 0.0951 & 0.0119 \\\\ 0.0119 & 0.0015 \\end{bmatrix} - \\begin{bmatrix} 0.9588 & 0.1988 \\\\ 0.1988 & 0.0412 \\end{bmatrix} = \\begin{bmatrix} 0.1363 & -0.1869 \\\\ -0.1869 & 0.9603 \\end{bmatrix} $\n\n- **BFGS Update $H_1^{\\mathrm{BFGS}}$**:\nUsing the formula with $H_0=I$:\n$H_1^{\\mathrm{BFGS}} = I + \\frac{(s_0^\\top y_0 + y_0^\\top y_0) s_0 s_0^\\top}{(s_0^\\top y_0)^2} - \\frac{y_0 s_0^\\top + s_0 y_0^\\top}{s_0^\\top y_0}$.\nLet's compute the coefficients.\n$s_0^\\top y_0 = \\frac{4225}{673}$. $y_0^\\top y_0 = \\frac{4225 \\cdot 7013}{673^2}$.\nTerm 1 multiplier: $1 + \\frac{y_0^\\top y_0}{s_0^\\top y_0} = 1 + \\frac{7013}{673} = \\frac{7686}{673}$.\nTerm 1: $\\frac{7686}{673} \\frac{s_0s_0^\\top}{s_0^\\top y_0} = \\frac{7686}{673} \\frac{1}{673} \\begin{bmatrix} 64 & 8 \\\\ 8 & 1 \\end{bmatrix} = \\frac{7686}{452929} \\begin{bmatrix} 64 & 8 \\\\ 8 & 1 \\end{bmatrix}$.\nTerm 2: $\\frac{1}{s_0^\\top y_0}(y_0s_0^\\top+s_0y_0^\\top) = \\frac{673}{4225}\\frac{4225}{673^2} \\left[ \\begin{bmatrix}-82\\\\-17\\end{bmatrix}\\begin{bmatrix}-8&-1\\end{bmatrix} + \\begin{bmatrix}-8\\\\-1\\end{bmatrix}\\begin{bmatrix}-82&-17\\end{bmatrix} \\right] = \\frac{1}{673} \\left[ \\begin{bmatrix}656&82\\\\136&17\\end{bmatrix} + \\begin{bmatrix}656&136\\\\82&17\\end{bmatrix} \\right] = \\frac{1}{673} \\begin{bmatrix}1312&218\\\\218&34\\end{bmatrix}$.\n$H_1^{\\mathrm{BFGS}} = I + \\frac{7686}{452929} \\begin{bmatrix} 64 & 8 \\\\ 8 & 1 \\end{bmatrix} - \\frac{1}{673} \\begin{bmatrix} 1312 & 218 \\\\ 218 & 34 \\end{bmatrix}$.\nNumerically:\n$ H_1^{\\mathrm{BFGS}} \\approx \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 1.0861 & 0.1357 \\\\ 0.1357 & 0.0170 \\end{bmatrix} - \\begin{bmatrix} 1.9495 & 0.3240 \\\\ 0.3240 & 0.0505 \\end{bmatrix} = \\begin{bmatrix} 0.1366 & -0.1883 \\\\ -0.1883 & 0.9665 \\end{bmatrix} $\n(A slight difference in calculation from scratchpad, but effect should be same. Let's trust the algebraic forms more).\nThe previous calculation was $H_1^{\\mathrm{BFGS}} = \\frac{1}{452929} \\begin{bmatrix} 61737 & -85126 \\\\ -85126 & 437733 \\end{bmatrix} \\approx \\begin{bmatrix} 0.136306 & -0.18794 \\\\ -0.18794 & 0.966449 \\end{bmatrix}$. Let's use this more precise one.\n\n- **Norm Comparison**: We compare $\\lVert H_k - Q^{-1} \\rVert_F^2$.\n$E_1^{\\mathrm{DFP}} = H_1^{\\mathrm{DFP}} - Q^{-1} \\approx \\begin{bmatrix} 0.1363 & -0.1869 \\\\ -0.1869 & 0.9603 \\end{bmatrix} - \\begin{bmatrix} 0.1667 & -0.3333 \\\\ -0.3333 & 1.6667 \\end{bmatrix} = \\begin{bmatrix} -0.0304 & 0.1464 \\\\ 0.1464 & -0.7064 \\end{bmatrix}$.\n$ \\lVert E_1^{\\mathrm{DFP}} \\rVert_F^2 \\approx (-0.0304)^2 + 2 \\cdot (0.1464)^2 + (-0.7064)^2 \\approx 0.00092 + 0.04287 + 0.49900 \\approx 0.5428 $.\n\n$E_1^{\\mathrm{BFGS}} = H_1^{\\mathrm{BFGS}} - Q^{-1} \\approx \\begin{bmatrix} 0.1363 & -0.1879 \\\\ -0.1879 & 0.9664 \\end{bmatrix} - \\begin{bmatrix} 0.1667 & -0.3333 \\\\ -0.3333 & 1.6667 \\end{bmatrix} = \\begin{bmatrix} -0.0304 & 0.1454 \\\\ 0.1454 & -0.7003 \\end{bmatrix}$.\n$ \\lVert E_1^{\\mathrm{BFGS}} \\rVert_F^2 \\approx (-0.0304)^2 + 2 \\cdot (0.1454)^2 + (-0.7003)^2 \\approx 0.00092 + 0.04228 + 0.49042 \\approx 0.5336 $.\n\nSince $0.5336 < 0.5428$, we have $\\lVert H_1^{\\mathrm{BFGS}} - Q^{-1}\\rVert_F < \\lVert H_1^{\\mathrm{DFP}} - Q^{-1}\\rVert_F$. The inequality holds.\n\n**Conclusion for A and D**:\n- **Statement A**: It states that both matrices are symmetric positive definite (which is true) and that $H_1^{\\mathrm{BFGS}}$ is closer to $Q^{-1}$ in Frobenius norm (which is true). Therefore, statement A is correct.\n- **Statement D**: It states that both updates satisfy the secant condition and symmetry (which is true by definition) and gives the explicit inequality for the Frobenius norms (which we have verified to be true). Therefore, statement D is correct.\n\nBoth statements A and D are factually correct descriptions of the outcome of this specific numerical experiment.\n\n**Final Verdicts**:\n- Option A: **Correct**.\n- Option B: **Incorrect**.\n- Option C: **Incorrect**.\n- Option D: **Correct**.\n- Option E: **Incorrect**.\n\nThe problem asks to select all that apply.", "answer": "$$\\boxed{AD}$$", "id": "3285119"}]}