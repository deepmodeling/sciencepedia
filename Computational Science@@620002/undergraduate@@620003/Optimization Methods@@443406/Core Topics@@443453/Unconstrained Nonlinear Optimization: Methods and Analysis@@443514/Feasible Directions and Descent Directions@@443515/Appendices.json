{"hands_on_practices": [{"introduction": "To begin, we translate the abstract concept of a feasible descent direction into a concrete calculation. This first practice [@problem_id:3128723] focuses on a problem with a single, nonlinear equality constraint, which defines a smooth curve or \"manifold\" in space. By computing the tangent space at a specific point, you will learn how to identify directions that stay on this manifold (to first order) while simultaneously decreasing the objective function, a foundational skill for tackling equality-constrained optimization.", "problem": "Consider the equality-constrained optimization problem with objective function $f(x) = x_{1}$ and constraint $h(x) = x_{1}^{2} + x_{2}^{2} - x_{1} = 0$. The feasible set is the curve defined by $h(x) = 0$, which is nonconvex. Let $x^{(0)} = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$, which satisfies the constraint.\n\nUsing the differentiability of $h$ and the first-order feasibility condition for equality constraints (i.e., directions tangent to the constraint manifold must satisfy the linearized constraint), compute the tangent space at $x^{(0)}$ and then identify one feasible descent direction for $f$ at $x^{(0)}$ that strictly decreases $f$ to first order.\n\nReturn your final answer as a row matrix containing four entries $\\left(t_{1}, t_{2}, d_{1}, d_{2}\\right)$, where $(t_{1}, t_{2})$ are the components of a basis tangent vector at $x^{(0)}$ and $(d_{1}, d_{2})$ are the components of a feasible descent direction for $f$ at $x^{(0)}$. Provide exact values with no rounding.", "solution": "The user wants me to solve the problem.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n#### Step 1.1: Extract Givens\n\n*   Objective function: $f(x) = x_{1}$\n*   Equality constraint: $h(x) = x_{1}^{2} + x_{2}^{2} - x_{1} = 0$\n*   Point of evaluation: $x^{(0)} = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$\n*   Condition on the point: $x^{(0)}$ satisfies the constraint $h(x^{(0)}) = 0$. Let's verify this: $h\\left(\\frac{1}{2}, \\frac{1}{2}\\right) = \\left(\\frac{1}{2}\\right)^{2} + \\left(\\frac{1}{2}\\right)^{2} - \\frac{1}{2} = \\frac{1}{4} + \\frac{1}{4} - \\frac{1}{2} = \\frac{1}{2} - \\frac{1}{2} = 0$. The condition holds.\n*   Task 1: Compute the tangent space at $x^{(0)}$ using the first-order feasibility condition for equality constraints: $\\nabla h(x^{(0)})^T d = 0$.\n*   Task 2: Identify one feasible descent direction for $f$ at $x^{(0)}$. This is a direction $d$ that is in the tangent space and satisfies $\\nabla f(x^{(0)})^T d  0$.\n*   Output format: A row matrix $(t_1, t_2, d_1, d_2)$, where $(t_1, t_2)$ is a basis tangent vector and $(d_1, d_2)$ is a feasible descent direction.\n\n#### Step 1.2: Validate Using Extracted Givens\n\n*   **Scientific or Factual Unsoundness**: The problem is a standard exercise in constrained optimization theory. The concepts of objective functions, constraints, tangent spaces, and descent directions are mathematically sound. No violation.\n*   **Non-Formalizable or Irrelevant**: The problem is entirely formalizable within the context of multivariable calculus and optimization methods. It is directly related to the topic of feasible and descent directions. No violation.\n*   **Incomplete or Contradictory Setup**: All necessary components (functions $f$ and $h$, and point $x^{(0)}$) are provided and are consistent. The point $x^{(0)}$ is confirmed to be in the feasible set. No violation.\n*   **Unrealistic or Infeasible**: The problem is purely mathematical and does not involve physical quantities, so this criterion is not applicable.\n*   **Ill-Posed or Poorly Structured**: The problem is well-posed. The tangent space at a regular point is well-defined. The existence of a descent direction depends on the gradients, which can be computed. The problem asks for \"one\" feasible direction, which correctly implies potential non-uniqueness. The same applies to the basis vector for the tangent space. No violation.\n*   **Pseudo-Profound, Trivial, or Tautological**: The problem requires a standard application of definitions and calculations from optimization theory. It is a non-trivial but straightforward problem. No violation.\n*   **Outside Scientific Verifiability**: The problem is mathematical and its solution is verifiable through logical deduction and calculation. No violation.\n\n#### Step 1.3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe problem asks for two main components: a basis vector for the tangent space at a given point on a constraint manifold, and a feasible descent direction at that point.\n\nThe objective function is $f(x) = x_1$, where $x = (x_1, x_2)^T$.\nThe equality constraint is $h(x) = x_1^2 + x_2^2 - x_1 = 0$.\nThe point of interest is $x^{(0)} = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$.\n\n**Part 1: Compute the tangent space at $x^{(0)}$**\n\nThe tangent space at $x^{(0)}$ is the set of all directions $d \\in \\mathbb{R}^2$ that are orthogonal to the gradient of the constraint function $h(x)$ at $x^{(0)}$. This is defined by the equation $\\nabla h(x^{(0)})^T d = 0$.\n\nFirst, we must compute the gradient of $h(x)$:\n$$\n\\nabla h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1} \\\\ \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 - 1 \\\\ 2x_2 \\end{pmatrix}\n$$\nNext, we evaluate this gradient at the point $x^{(0)} = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$:\n$$\n\\nabla h(x^{(0)}) = \\begin{pmatrix} 2\\left(\\frac{1}{2}\\right) - 1 \\\\ 2\\left(\\frac{1}{2}\\right) \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe tangent space is defined by the set of all vectors $d=(d_1, d_2)^T$ that satisfy:\n$$\n\\nabla h(x^{(0)})^T d = 0\n$$\n$$\n\\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = 0\n$$\nThis matrix multiplication yields the equation:\n$$\n0 \\cdot d_1 + 1 \\cdot d_2 = 0 \\implies d_2 = 0\n$$\nSo, any vector $d$ in the tangent space must be of the form $d = (d_1, 0)$ for any $d_1 \\in \\mathbb{R}$. The tangent space is a $1$-dimensional subspace of $\\mathbb{R}^2$. A basis for this space is any non-zero vector of this form. A simple choice for a basis vector is to set $d_1=1$.\nLet the basis tangent vector be $t = (t_1, t_2) = (1, 0)$.\n\n**Part 2: Identify a feasible descent direction at $x^{(0)}$**\n\nA direction $d$ is a *feasible descent direction* if it meets two conditions:\n1. It is a feasible direction, meaning it lies in the tangent space. From Part 1, this implies $d$ must be of the form $(d_1, 0)$.\n2. It is a descent direction for the objective function $f$, meaning the directional derivative of $f$ along $d$ is negative. This is expressed as $\\nabla f(x^{(0)})^T d  0$.\n\nFirst, we compute the gradient of the objective function $f(x) = x_1$:\n$$\n\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe gradient of $f$ is constant for all $x$, so $\\nabla f(x^{(0)}) = (1, 0)^T$.\n\nNow, we apply the descent condition using a general vector $d = (d_1, 0)$ from the tangent space:\n$$\n\\nabla f(x^{(0)})^T d  0\n$$\n$$\n\\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\end{pmatrix}  0\n$$\nThis gives the inequality:\n$$\n1 \\cdot d_1 + 0 \\cdot 0  0 \\implies d_1  0\n$$\nTherefore, any vector $d = (d_1, 0)$ with $d_1  0$ is a feasible descent direction at $x^{(0)}$. The problem asks for one such direction. A simple choice is to set $d_1 = -1$.\nA valid feasible descent direction is $d = (d_1, d_2) = (-1, 0)$.\n\n**Conclusion**\n\nA basis vector for the tangent space at $x^{(0)}$ is $t = (t_1, t_2) = (1, 0)$.\nA feasible descent direction for $f$ at $x^{(0)}$ is $d = (d_1, d_2) = (-1, 0)$.\n\nThe final answer should be provided as a row matrix $(t_1, t_2, d_1, d_2)$.\nSubstituting the values we found: $(1, 0, -1, 0)$.\n\nGeometrically, the constraint $h(x) = x_1^2 + x_2^2 - x_1 = 0$ can be rewritten as $(x_1 - 1/2)^2 + x_2^2 = (1/2)^2$, which describes a circle of radius $1/2$ centered at $(1/2, 0)$. The point $x^{(0)} = (1/2, 1/2)$ is the point at the top of this circle. The tangent line at this point is horizontal, so the tangent vectors are of the form $(k, 0)$, which confirms our calculation. The objective is $f(x)=x_1$, so a descent direction must decrease $x_1$. From the point $(1/2, 1/2)$, moving along the horizontal tangent line to decrease $x_1$ means moving to the left. This corresponds to a direction vector $(d_1, 0)$ with $d_1  0$, which also confirms our result.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0  -1  0 \\end{pmatrix}}\n$$", "id": "3128723"}, {"introduction": "Real-world problems often involve multiple inequality constraints, creating feasible regions with \"corners\" or vertices where the path forward is not obvious. This exercise [@problem_id:3128694] explores such a scenario, demonstrating how to characterize all valid directions of movement using the cone of feasible directions. You will see what happens when the most intuitive path of descent—the negative gradient—is not feasible, and learn how to find the steepest feasible descent direction, which represents the best possible local move.", "problem": "Consider the constrained optimization problem in $\\mathbb{R}^{2}$: minimize $f(x)$ subject to $g_{i}(x) \\le 0$ for $i \\in \\{1,2,3\\}$, where\n- $f(x) = x_{1} + x_{2}$,\n- $g_{1}(x) = x_{1} + x_{2} - 2 \\le 0$,\n- $g_{2}(x) = -x_{1} \\le 0$,\n- $g_{3}(x) = -x_{2} \\le 0$.\n\nLet the boundary point be $x^{\\ast} = (0,2)$. Starting from the core definitions in constrained optimization:\n- A direction $d$ is a feasible direction at $x^{\\ast}$ if, for each constraint active at $x^{\\ast}$, the first-order change satisfies $\\nabla g_{i}(x^{\\ast})^{\\mathsf{T}} d \\le 0$.\n- A direction $d$ is a descent direction for $f$ at $x^{\\ast}$ if $\\nabla f(x^{\\ast})^{\\mathsf{T}} d  0$.\n\nTasks:\n- Identify the set of active constraints at $x^{\\ast}$ and use their gradients to construct the cone of feasible directions $\\mathcal{C}(x^{\\ast})$ at $x^{\\ast}$.\n- Test whether $-\\nabla f(x^{\\ast})$ lies in $\\mathcal{C}(x^{\\ast})$.\n- If $-\\nabla f(x^{\\ast}) \\notin \\mathcal{C}(x^{\\ast})$, determine the steepest feasible descent direction by solving the following small quadratic program (QP): minimize $\\nabla f(x^{\\ast})^{\\mathsf{T}} d$ subject to $d \\in \\mathcal{C}(x^{\\ast})$ and $\\|d\\|_{2} \\le 1$.\n\nReport the unique angle $\\theta$ (in radians) of the optimal unit direction $d^{\\ast}$ measured from the positive $x_{1}$-axis, taking $\\theta \\in [-\\pi, \\pi)$. Express your final answer as an exact real number in radians. No rounding is required. No units should appear in the final boxed answer.", "solution": "The user wants to solve a constrained optimization problem.\n\nHere's my plan:\n1.  **Validate the problem statement.**\n    *   Extract all the given information verbatim.\n    *   Check for scientific soundness, self-consistency, and clarity. The problem is a standard optimization problem, so it should be valid.\n2.  **Solve the problem if it's valid.**\n    *   **Step 1: Identify Active Constraints.**\n        *   I have the point $x^{\\ast} = (0, 2)$.\n        *   I need to check which of the constraints $g_i(x) \\le 0$ are satisfied with equality, i.e., $g_i(x^{\\ast}) = 0$.\n        *   $g_1(x) = x_1 + x_2 - 2$. At $x^{\\ast}=(0,2)$, $g_1(x^{\\ast}) = 0 + 2 - 2 = 0$. So, $g_1$ is active.\n        *   $g_2(x) = -x_1$. At $x^{\\ast}=(0,2)$, $g_2(x^{\\ast}) = -0 = 0$. So, $g_2$ is active.\n        *   $g_3(x) = -x_2$. At $x^{\\ast}=(0,2)$, $g_3(x^{\\ast}) = -2  0$. So, $g_3$ is not active.\n        *   The set of active constraints at $x^{\\ast}$ is $I(x^{\\ast}) = \\{1, 2\\}$.\n\n    *   **Step 2: Construct the Cone of Feasible Directions.**\n        *   The problem defines a feasible direction $d$ at $x^{\\ast}$ as one where $\\nabla g_i(x^{\\ast})^{\\mathsf{T}} d \\le 0$ for all active constraints $i \\in I(x^{\\ast})$.\n        *   I need the gradients of the active constraints.\n        *   $f(x) = x_1 + x_2 \\implies \\nabla f(x) = (1, 1)^{\\mathsf{T}}$. So, $\\nabla f(x^{\\ast}) = (1, 1)^{\\mathsf{T}}$.\n        *   $g_1(x) = x_1 + x_2 - 2 \\implies \\nabla g_1(x) = (1, 1)^{\\mathsf{T}}$. So, $\\nabla g_1(x^{\\ast}) = (1, 1)^{\\mathsf{T}}$.\n        *   $g_2(x) = -x_1 \\implies \\nabla g_2(x) = (-1, 0)^{\\mathsf{T}}$. So, $\\nabla g_2(x^{\\ast}) = (-1, 0)^{\\mathsf{T}}$.\n        *   Let $d = (d_1, d_2)^{\\mathsf{T}}$. The conditions for $d$ to be in the cone of feasible directions $\\mathcal{C}(x^{\\ast})$ are:\n            *   $\\nabla g_1(x^{\\ast})^{\\mathsf{T}} d \\le 0 \\implies (1, 1) \\cdot (d_1, d_2) \\le 0 \\implies d_1 + d_2 \\le 0$.\n            *   $\\nabla g_2(x^{\\ast})^{\\mathsf{T}} d \\le 0 \\implies (-1, 0) \\cdot (d_1, d_2) \\le 0 \\implies -d_1 \\le 0 \\implies d_1 \\ge 0$.\n        *   So, $\\mathcal{C}(x^{\\ast}) = \\{ d \\in \\mathbb{R}^2 \\mid d_1 + d_2 \\le 0 \\text{ and } d_1 \\ge 0 \\}$.\n\n    *   **Step 3: Test if $-\\nabla f(x^{\\ast})$ is in $\\mathcal{C}(x^{\\ast})$.**\n        *   We have $\\nabla f(x^{\\ast}) = (1, 1)^{\\mathsf{T}}$.\n        *   Let $d_{SD} = -\\nabla f(x^{\\ast}) = (-1, -1)^{\\mathsf{T}}$.\n        *   Check if $d_{SD}$ satisfies the conditions for $\\mathcal{C}(x^{\\ast})$.\n            *   Condition 1: $d_1 \\ge 0$. Here, $d_1 = -1$. This is not satisfied. So, $-1 \\ge 0$ is false.\n        *   Therefore, $-\\nabla f(x^{\\ast}) \\notin \\mathcal{C}(x^{\\ast})$.\n\n    *   **Step 4: Find the Steepest Feasible Descent Direction.**\n        *   The problem asks to solve the QP:\n            *   minimize $\\nabla f(x^{\\ast})^{\\mathsf{T}} d$\n            *   subject to $d \\in \\mathcal{C}(x^{\\ast})$ and $\\|d\\|_2 \\le 1$.\n        *   Substituting the known values:\n            *   minimize $(1, 1)^{\\mathsf{T}} (d_1, d_2)^{\\mathsf{T}} = d_1 + d_2$\n            *   subject to:\n                1.  $d_1 + d_2 \\le 0$\n                2.  $d_1 \\ge 0$\n                3.  $d_1^2 + d_2^2 \\le 1$\n        *   We want to make the sum $d_1 + d_2$ as small (i.e., as negative) as possible.\n        *   From condition (1), $d_1 + d_2$ must be non-positive.\n        *   From condition (2), $d_1$ must be non-negative.\n        *   From condition (1), $d_2 \\le -d_1$. Since $d_1 \\ge 0$, this means $d_2$ must be negative or zero.\n        *   The feasible set for the direction is the intersection of the cone $\\mathcal{C}(x^{\\ast})$ and the unit disk centered at the origin.\n        *   The cone $\\mathcal{C}(x^{\\ast})$ is the region where $d_1 \\ge 0$ and $d_2 \\le -d_1$. This is the wedge in the fourth quadrant between the negative $d_2$-axis and the line $d_1 + d_2 = 0$. The boundary rays are along $(0, -1)$ and $(1, -1)$.\n        *   The vector $(1, -1)$ normalized is $\\frac{1}{\\sqrt{2}}(1, -1)$. Let's call this $v_1$. The angle is $-\\pi/4$.\n        *   The vector $(0, -1)$ is a unit vector. Let's call this $v_2$. The angle is $-\\pi/2$.\n        *   The feasible set for the unit direction is the arc of the unit circle between angles $-\\pi/2$ and $-\\pi/4$.\n        *   We need to find the point $(d_1, d_2)$ on the unit circle within this arc that minimizes the function $F(d_1, d_2) = d_1 + d_2$.\n        *   Let $d_1 = \\cos\\phi$ and $d_2 = \\sin\\phi$. The constraint is $d_1^2 + d_2^2 = 1$.\n        *   The feasible range for the angle $\\phi$ is $[-\\pi/2, -\\pi/4]$.\n        *   We need to minimize $F(\\phi) = \\cos\\phi + \\sin\\phi$.\n        *   Let's check the derivative: $F'(\\phi) = -\\sin\\phi + \\cos\\phi$.\n        *   We set $F'(\\phi) = 0 \\implies \\cos\\phi = \\sin\\phi \\implies \\tan\\phi = 1$. The solutions are $\\phi = \\pi/4 + k\\pi$.\n        *   The solution $\\phi = \\pi/4$ is outside our feasible interval $[-\\pi/2, -\\pi/4]$. The next closest one is $\\phi = -3\\pi/4$, which is also outside.\n        *   Since the derivative is non-zero in the interior of the interval, the minimum must occur at one of the endpoints.\n        *   Let's evaluate $F(\\phi)$ at the endpoints $\\phi = -\\pi/2$ and $\\phi = -\\pi/4$.\n        *   At $\\phi = -\\pi/2$: $d = (0, -1)$. $F = 0 + (-1) = -1$.\n        *   At $\\phi = -\\pi/4$: $d = (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$. $F = \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = 0$.\n        *   The minimum value is $-1$, which occurs at $\\phi = -\\pi/2$.\n        *   So the optimal direction is $d^{\\ast} = (0,-1)$.\n        *   Let's double check this. The problem is to find the direction $d$ that minimizes $\\nabla f^{\\mathsf{T}} d$. This is equivalent to finding the direction $d$ in the feasible set that forms the largest obtuse angle with $\\nabla f$.\n        *   $\\nabla f = (1,1)$. The angle of $\\nabla f$ is $\\pi/4$.\n        *   The feasible directions are in the sector between $-\\pi/2$ and $-\\pi/4$.\n        *   We want to minimize $\\nabla f^{\\mathsf{T}} d = \\|\\nabla f\\| \\|d\\| \\cos(\\alpha)$, where $\\alpha$ is the angle between $\\nabla f$ and $d$.\n        *   With $\\|d\\|=1$, we want to minimize $\\sqrt{2} \\cos(\\alpha)$. This is equivalent to maximizing the angle $\\alpha$.\n        *   The angle of $\\nabla f$ is $\\pi/4$. The angles of the feasible directions $d$ are in $[-\\pi/2, -\\pi/4]$.\n        *   Let $\\theta_d$ be the angle of $d$. The angle between them is $\\alpha = |\\theta_d - \\pi/4|$.\n        *   If $\\theta_d = -\\pi/4$, $\\alpha = |-\\pi/4 - \\pi/4| = |-\\pi/2| = \\pi/2$. $\\cos(\\pi/2)=0$.\n        *   If $\\theta_d = -\\pi/2$, $\\alpha = |-\\pi/2 - \\pi/4| = |-3\\pi/4| = 3\\pi/4$. $\\cos(3\\pi/4) = -1/\\sqrt{2}$.\n        *   The value of the objective function is $\\sqrt{2} \\cos(\\alpha)$.\n        *   At $\\theta_d = -\\pi/4$, objective is $\\sqrt{2}\\cos(\\pi/2) = 0$.\n        *   At $\\theta_d = -\\pi/2$, objective is $\\sqrt{2}\\cos(3\\pi/4) = \\sqrt{2}(-1/\\sqrt{2}) = -1$.\n        *   This confirms that the minimum is achieved when the angle of direction $d$ is $-\\pi/2$.\n        *   The question asks for the unique angle $\\theta$ of the optimal unit direction $d^{\\ast}$.\n        *   The optimal direction is $d^{\\ast} = (0, -1)$. The angle of this vector, measured from the positive $x_1$-axis, is indeed $-\\pi/2$. The range for $\\theta$ is given as $[-\\pi, \\pi)$, and $-\\pi/2$ is in this range.\n\n    *   **Alternative interpretation**: The problem of finding the steepest descent direction is often defined as finding the projection of $-\\nabla f$ onto the cone of feasible directions.\n        *   Let's solve min $\\|d - (-\\nabla f)\\|^2$ subject to $d \\in \\mathcal{C}(x^{\\ast})$.\n        *   minimize $\\|(d_1, d_2) - (-1, -1)\\|^2 = \\|(d_1+1, d_2+1)\\|^2 = (d_1+1)^2 + (d_2+1)^2$.\n        *   Subject to $d_1 \\ge 0$ and $d_1+d_2 \\le 0$.\n        *   Let's find the unconstrained minimum: $d_1 = -1, d_2 = -1$. This point is $( -1, -1)$, which is $-\\nabla f$.\n        *   This point violates $d_1 \\ge 0$. So the minimum must be on the boundary of the feasible region.\n        *   The feasible region is the cone $d_1 \\ge 0, d_2 \\le -d_1$.\n        *   The boundaries are $d_1=0, d_2 \\le 0$ (negative $d_2$ axis) and $d_1+d_2=0, d_1 \\ge 0$ (line in 4th quadrant).\n        *   Let's project $( -1, -1)$ onto the line $d_1=0$. The projection is $(0, -1)$. This point is in the feasible region ($d_1=0 \\ge 0$, $d_1+d_2=0-1=-1 \\le 0$). The distance squared is $(0+1)^2 + (-1+1)^2 = 1$.\n        *   Let's project $( -1, -1)$ onto the line $d_1+d_2=0$. The projection $p$ of a point $q$ onto a plane $n^{\\mathsf{T}}x-c=0$ is $p = q - \\frac{n^{\\mathsf{T}}q - c}{n^{\\mathsf{T}}n}n$. Here, $q=(-1,-1)$, $n=(1,1)$, $c=0$.\n        *   $p = (-1,-1) - \\frac{(1,1)^{\\mathsf{T}}(-1,-1) - 0}{(1,1)^{\\mathsf{T}}(1,1)}(1,1) = (-1,-1) - \\frac{-2}{2}(1,1) = (-1,-1) - (-1)(1,1) = (-1,-1) + (1,1) = (0,0)$.\n        *   The point $(0,0)$ is in the feasible region. The distance squared is $(0+1)^2+(0+1)^2 = 2$.\n        *   The closest point in the cone to $( -1, -1)$ is $(0, -1)$.\n        *   So the projection of $-\\nabla f$ onto $\\mathcal{C}(x^{\\ast})$ is the vector $(0, -1)$.\n        *   The steepest descent direction $d^{\\ast}$ is the normalized version of this projection. Let's call the projection $p^{\\ast}$.\n        *   $p^{\\ast} = (0,-1)$. It is already a unit vector. So $d^{\\ast} = (0, -1)$.\n        *   This confirms my previous result.\n\n    *   Let's re-read the problem statement carefully to ensure I followed the definition given.\n        *   \"...determine the steepest feasible descent direction by solving the following small quadratic program (QP): minimize $\\nabla f(x^{\\ast})^{\\mathsf{T}} d$ subject to $d \\in \\mathcal{C}(x^{\\ast})$ and $\\|d\\|_{2} \\le 1$.\"\n        *   My formulation:\n            *   minimize $d_1 + d_2$\n            *   subject to:\n                1.  $d_1 + d_2 \\le 0$\n                2.  $d_1 \\ge 0$\n                3.  $d_1^2 + d_2^2 \\le 1$\n        *   This formulation is correct according to the problem. The analysis that the minimum must be at an extreme point of the feasible set is also sound. The feasible set is the intersection of a cone and the unit ball. The objective function is linear. The minimum of a linear function over a compact convex set is achieved on the boundary of the set.\n        *   The feasible set $\\mathcal{F} = \\{ d \\in \\mathbb{R}^2 \\mid d_1+d_2 \\le 0,\\ d_1\\ge 0,\\ d_1^2+d_2^2 \\le 1 \\}$.\n        *   This is a segment of the unit disk. The boundary consists of three parts:\n            1.  The line segment from $(0,0)$ to $(0,-1)$. On this segment, $d_1=0$, $-1 \\le d_2 \\le 0$. The objective is $d_1+d_2 = d_2$. The minimum is at $d_2=-1$, so $d=(0, -1)$. Value is $-1$.\n            2.  The line segment from $(0,0)$ to $(1/\\sqrt{2}, -1/\\sqrt{2})$. On this segment, $d_1+d_2=0$. The objective is $d_1+d_2=0$.\n            3.  The circular arc from $(0,-1)$ to $(1/\\sqrt{2}, -1/\\sqrt{2})$. On this arc, $d_1^2+d_2^2=1$, $d = (\\cos\\phi, \\sin\\phi)$ for $\\phi \\in [-\\pi/2, -\\pi/4]$. The objective is $\\cos\\phi+\\sin\\phi$. As analyzed before, the derivative is $\\cos\\phi-\\sin\\phi$, which is positive on $[-\\pi/2, -\\pi/4)$. For example, at $\\phi = -\\pi/3$, $\\cos(-\\pi/3) - \\sin(-\\pi/3) = 1/2 - (-\\sqrt{3}/2) = (1+\\sqrt{3})/2  0$. So the function is strictly increasing on the interval. The minimum is at the left endpoint, $\\phi=-\\pi/2$. The minimum value is $\\cos(-\\pi/2) + \\sin(-\\pi/2) = 0 - 1 = -1$. The maximum is at the right endpoint, $\\phi=-\\pi/4$. The maximum value is $\\cos(-\\pi/4)+\\sin(-\\pi/4) = 1/\\sqrt{2} - 1/\\sqrt{2} = 0$.\n        *   Comparing the minimums on all parts of the boundary:\n            *   On segment 1: min value -1 at $d=(0,-1)$.\n            *   On segment 2: min value 0 everywhere on the segment.\n            *   On segment 3 (arc): min value -1 at $d=(0,-1)$.\n        *   The global minimum over the feasible set $\\mathcal{F}$ is $-1$, and it is uniquely achieved at $d^{\\ast} = (0,-1)$.\n        *   The question asks for the unique angle $\\theta$ of this direction.\n        *   The vector $d^{\\ast} = (0, -1)$ corresponds to an angle of $-\\pi/2$ radians from the positive $x_1$-axis.\n\n3.  **Final verification and formatting.**\n    *   The problem is well-posed and has a unique solution.\n    *   The steps are logical and based on the definitions provided.\n    *   The calculations are straightforward.\n    *   The final answer is an exact real number, as requested.\n    *   The angle is in the specified range $[-\\pi, \\pi)$.\n    *   I must write the entire solution using LaTeX for all mathematical entities.", "answer": "$$\n\\boxed{-\\frac{\\pi}{2}}\n$$", "id": "3128694"}, {"introduction": "Feasible and descent directions are not just theoretical constructs; they are the engine that drives iterative optimization algorithms. This final practice [@problem_id:3128719] illustrates this by examining the projected gradient method, a common algorithm for constrained problems. You will first uncover a scenario where a naive implementation fails by cycling endlessly, and then see how applying a systematic line search along a proper feasible descent direction guarantees that the algorithm makes steady progress toward the optimal solution.", "problem": "Consider the constrained optimization problem of minimizing the smooth function $$f(x) = \\tfrac{1}{2}\\,(x_{1} - x_{2})^{2}$$ over the closed convex polyhedron $$\\mathcal{C} = \\{x \\in \\mathbb{R}^{2} \\,:\\, x_{1} \\geq 0,\\; x_{2} \\geq 0,\\; x_{1} + x_{2} \\leq 1\\}.$$ The gradient is $$\\nabla f(x) = Qx,$$ where $$Q = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}.$$ Define the naive projected gradient iteration with fixed step size $$\\alpha = 1$$ by $$x^{+} = \\Pi_{\\mathcal{C}}\\big(x - \\alpha \\nabla f(x)\\big),$$ where $$\\Pi_{\\mathcal{C}}(\\cdot)$$ denotes the Euclidean projection onto $$\\mathcal{C}$$. Start from the vertex $$x^{(0)} = (1,0)^{\\top}.$$ Using only the fundamental definitions of feasible directions, the tangent cone, and the Armijo backtracking condition, do the following:\n\n1. Establish that the naive projected gradient iteration cycles between the vertices $$x^{(0)} = (1,0)^{\\top}$$ and $$x^{(1)} = (0,1)^{\\top}$$ without decreasing $$f$$.\n\n2. Impose an Armijo backtracking rule with feasibility along a feasible descent direction at $$x^{(0)}$$. Use the sufficient decrease parameter $$\\sigma = \\tfrac{1}{4}$$, backtracking factor $$\\beta = \\tfrac{1}{2}$$, and initial trial step size $$\\alpha_{0} = 1$$. Choose the feasible descent direction $$d^{(0)}$$ as the negative gradient projected onto the tangent cone of $$\\mathcal{C}$$ at $$x^{(0)}$$ (ensuring first-order feasibility). Determine the largest backtracked step size in the sequence $$\\{\\alpha_{0},\\, \\beta \\alpha_{0},\\, \\beta^{2} \\alpha_{0},\\, \\ldots\\}$$ that satisfies the Armijo inequality at $$x^{(0)}$$, and then compute the guaranteed magnitude of decrease $$\\Delta$$ in the objective value prescribed by the Armijo condition for that accepted step, namely $$\\Delta = \\sigma\\,\\alpha\\,\\|\\nabla f(x^{(0)})\\|^{2}.$$\n\nYour final answer must be the single real number equal to $$\\Delta$$. No rounding is required.", "solution": "The user has presented a constrained optimization problem and requested an analysis of two different algorithmic approaches.\n\n### Problem Validation\nThe problem is first validated against the required criteria.\n\n**Step 1: Extract Givens**\n- Objective function: $f(x) = \\frac{1}{2}(x_{1} - x_{2})^{2}$\n- Constraint set: $\\mathcal{C} = \\{x \\in \\mathbb{R}^{2} \\,:\\, x_{1} \\geq 0,\\; x_{2} \\geq 0,\\; x_{1} + x_{2} \\leq 1\\}$\n- Gradient: $\\nabla f(x) = Qx$, where $Q = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$\n- Naive projected gradient iteration: $x^{+} = \\Pi_{\\mathcal{C}}(x - \\alpha \\nabla f(x))$ with fixed step size $\\alpha = 1$\n- Initial point: $x^{(0)} = (1,0)^{\\top}$\n- Armijo backtracking parameters: $\\sigma = \\frac{1}{4}$, $\\beta = \\frac{1}{2}$, initial trial step size $\\alpha_{0} = 1$\n- Feasible descent direction: $d^{(0)} = \\Pi_{T_{\\mathcal{C}}(x^{(0)})}(-\\nabla f(x^{(0)}))$\n- Quantity to compute: $\\Delta = \\sigma\\,\\alpha\\,\\|\\nabla f(x^{(0)})\\|^{2}$ for the accepted step size $\\alpha$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of numerical optimization, specifically concerning projected gradient methods for constrained convex optimization. All terms are standard and well-defined. The data is consistent and complete. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe solution is presented in two parts as requested.\n\n**Part 1: Analysis of the Naive Projected Gradient Iteration**\n\nWe start at the point $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe value of the objective function at this point is $f(x^{(0)}) = \\frac{1}{2}(1 - 0)^2 = \\frac{1}{2}$.\nThe gradient of $f$ at $x^{(0)}$ is $\\nabla f(x^{(0)}) = \\begin{pmatrix} 1 - 0 \\\\ 0 - 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe naive projected gradient iteration with step size $\\alpha = 1$ is given by $x^{+} = \\Pi_{\\mathcal{C}}(x - \\nabla f(x))$.\nFor the first iteration, we compute the unconstrained step:\n$$y^{(0)} = x^{(0)} - \\alpha \\nabla f(x^{(0)}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.$$\nThe next iterate $x^{(1)}$ is the projection of $y^{(0)}$ onto the feasible set $\\mathcal{C}$.\nThe point $y^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ satisfies the constraints of $\\mathcal{C}$: $x_{1} = 0 \\geq 0$, $x_{2} = 1 \\geq 0$, and $x_{1} + x_{2} = 0+1=1 \\leq 1$.\nSince $y^{(0)}$ is in $\\mathcal{C}$, its projection onto $\\mathcal{C}$ is the point itself.\n$$x^{(1)} = \\Pi_{\\mathcal{C}}(y^{(0)}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.$$\nThe value of the objective function at the new iterate is $f(x^{(1)}) = \\frac{1}{2}(0 - 1)^2 = \\frac{1}{2}$.\nWe observe that $f(x^{(1)}) = f(x^{(0)})$, so no decrease in the objective function has occurred.\n\nNow we perform the second iteration, starting from $x^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe gradient at $x^{(1)}$ is $\\nabla f(x^{(1)}) = \\begin{pmatrix} 0 - 1 \\\\ 1 - 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nThe unconstrained step is:\n$$y^{(1)} = x^{(1)} - \\alpha \\nabla f(x^{(1)}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.$$\nThe next iterate $x^{(2)}$ is the projection of $y^{(1)}$ onto $\\mathcal{C}$.\nThe point $y^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ is the starting point $x^{(0)}$, which is also in $\\mathcal{C}$.\n$$x^{(2)} = \\Pi_{\\mathcal{C}}(y^{(1)}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = x^{(0)}.$$\nThe iteration returns to the starting point. Thus, the sequence of iterates is $x^{(0)}, x^{(1)}, x^{(0)}, x^{(1)}, \\ldots$, cycling between the vertices $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ without any decrease in the objective function value, which remains constant at $\\frac{1}{2}$. This establishes the claim in the first part of the problem.\n\n**Part 2: Armijo Backtracking with a Feasible Descent Direction**\n\nWe start again at $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nFirst, we determine the tangent cone $T_{\\mathcal{C}}(x^{(0)})$. The feasible set $\\mathcal{C}$ is defined by the inequalities $g_1(x) = -x_1 \\leq 0$, $g_2(x) = -x_2 \\leq 0$, and $g_3(x) = x_1+x_2-1 \\leq 0$. At $x^{(0)}$, the active constraints are $g_2(x^{(0)}) = 0$ and $g_3(x^{(0)}) = 0$.\nThe gradients of the active constraints are $\\nabla g_2(x) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$ and $\\nabla g_3(x) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nFor a convex polyhedron, the tangent cone is the set of directions $d = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}$ such that $\\nabla g_i(x^{(0)})^\\top d \\leq 0$ for all active constraints $i$.\n$$ \\nabla g_2(x^{(0)})^\\top d = \\begin{pmatrix} 0  -1 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = -d_2 \\leq 0 \\implies d_2 \\geq 0.$$\n$$ \\nabla g_3(x^{(0)})^\\top d = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = d_1 + d_2 \\leq 0.$$\nSo, the tangent cone is $T_{\\mathcal{C}}(x^{(0)}) = \\{d \\in \\mathbb{R}^2 \\,|\\, d_2 \\geq 0, d_1+d_2 \\leq 0\\}$.\n\nThe search direction $d^{(0)}$ is the projection of the negative gradient onto this cone. The negative gradient is $-\\nabla f(x^{(0)}) = -\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nLet's check if this vector lies in the tangent cone. For $d_1 = -1$ and $d_2 = 1$:\n$d_2 = 1 \\geq 0$ (satisfied).\n$d_1+d_2 = -1+1=0 \\leq 0$ (satisfied).\nSince $-\\nabla f(x^{(0)})$ is in the closed convex cone $T_{\\mathcal{C}}(x^{(0)})$, its projection onto the cone is the vector itself.\n$$d^{(0)} = \\Pi_{T_{\\mathcal{C}}(x^{(0)})}(-\\nabla f(x^{(0)})) = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}.$$\nThis is a descent direction, as $\\nabla f(x^{(0)})^\\top d^{(0)} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = -1 - 1 = -2  0$.\n\nNext, we perform the Armijo backtracking line search to find an acceptable step size $\\alpha$ from the sequence $\\{\\alpha_0, \\beta \\alpha_0, \\beta^2 \\alpha_0, \\ldots\\} = \\{1, \\frac{1}{2}, \\frac{1}{4}, \\ldots\\}$.\nThe Armijo condition is: $f(x^{(0)} + \\alpha d^{(0)}) \\leq f(x^{(0)}) + \\sigma \\alpha \\nabla f(x^{(0)})^\\top d^{(0)}$.\nWe have $f(x^{(0)}) = \\frac{1}{2}$, $\\sigma=\\frac{1}{4}$, and $\\nabla f(x^{(0)})^\\top d^{(0)} = -2$. The condition becomes:\n$$f\\left(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\\right) \\leq \\frac{1}{2} + \\frac{1}{4} \\alpha (-2) = \\frac{1}{2} - \\frac{\\alpha}{2}.$$\nThe new point is $x(\\alpha) = \\begin{pmatrix} 1-\\alpha \\\\ \\alpha \\end{pmatrix}$. The function value is $f(x(\\alpha)) = \\frac{1}{2}((1-\\alpha) - \\alpha)^2 = \\frac{1}{2}(1-2\\alpha)^2$.\nThe inequality is:\n$$\\frac{1}{2}(1-2\\alpha)^2 \\leq \\frac{1}{2} - \\frac{\\alpha}{2}.$$\n$$(1-2\\alpha)^2 \\leq 1 - \\alpha.$$\n$$1 - 4\\alpha + 4\\alpha^2 \\leq 1 - \\alpha.$$\n$$4\\alpha^2 - 3\\alpha \\leq 0.$$\n$$\\alpha(4\\alpha - 3) \\leq 0.$$\nSince we are seeking $\\alpha  0$, this simplifies to $4\\alpha - 3 \\leq 0$, or $\\alpha \\leq \\frac{3}{4}$.\n\nNow we test the trial step sizes:\n- For $\\alpha_0 = 1$: $1 \\not\\leq \\frac{3}{4}$. The condition is not satisfied.\n- For $\\alpha_1 = \\beta \\alpha_0 = \\frac{1}{2}$: $\\frac{1}{2} \\leq \\frac{3}{4}$. The condition is satisfied.\nWe also check that the step is feasible. For a step $\\alpha$, the point $x(\\alpha) = (1-\\alpha, \\alpha)$ must be in $\\mathcal{C}$, which requires $1-\\alpha \\ge 0$, $\\alpha \\ge 0$, and $(1-\\alpha)+\\alpha \\le 1$. These simplify to $0 \\le \\alpha \\le 1$. All trial steps are in this range.\nThe largest step size in the sequence satisfying the Armijo condition is $\\alpha = \\frac{1}{2}$.\n\nFinally, we compute the guaranteed magnitude of decrease $\\Delta$ for this accepted step, using the given formula $\\Delta = \\sigma\\,\\alpha\\,\\|\\nabla f(x^{(0)})\\|^{2}$.\nWe have $\\sigma = \\frac{1}{4}$ and the accepted step size is $\\alpha = \\frac{1}{2}$.\nThe squared norm of the gradient is $\\|\\nabla f(x^{(0)})\\|^2 = \\left\\|\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\right\\|^2 = 1^2 + (-1)^2 = 2$.\nSubstituting these values:\n$$\\Delta = \\left(\\frac{1}{4}\\right) \\left(\\frac{1}{2}\\right) (2) = \\frac{2}{8} = \\frac{1}{4}.$$\nThe guaranteed decrease is $\\frac{1}{4}$.\nNote that the actual decrease is $f(x^{(0)}) - f(x^{(0)} + \\frac{1}{2} d^{(0)}) = \\frac{1}{2} - f(\\frac{1}{2}, \\frac{1}{2}) = \\frac{1}{2} - 0 = \\frac{1}{2}$, which is greater than or equal to the guaranteed decrease of $\\frac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "3128719"}]}