{"hands_on_practices": [{"introduction": "The effectiveness of an optimization algorithm often depends on how well it navigates the geometry of the objective function. The Barzilai-Borwein (BB) method is celebrated for its ability to accelerate convergence on challenging, ill-conditioned problems where simpler methods like steepest descent falter. This first exercise provides a foundational look at the BB step size calculation for a simple quadratic function. By calculating a BB step and contrasting its behavior with that of a classic steepest descent step, you will develop a crucial geometric intuition for why \"overstepping\" the local minimum can lead to faster progress in the long run [@problem_id:2162618].", "problem": "Consider the problem of minimizing the quadratic objective function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T A \\mathbf{x}$, where $\\mathbf{x} = (x_1, x_2)^T \\in \\mathbb{R}^2$. This function represents a simplified cost function in a system identification task, where the goal is to find the optimal parameter vector $\\mathbf{x}$ that minimizes the error. The matrix $A$ is given by:\n$$A = \\begin{pmatrix} 100 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\nThe high condition number of this matrix indicates that the two parameters have vastly different sensitivities, making optimization challenging for standard methods.\n\nWe will use a gradient-based iterative method to find the minimum, starting from the initial point $\\mathbf{x}_0 = (1, 10)^T$. The general update rule is $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$.\n\nFor the first iteration (from $k=0$ to $k=1$), a single step of the steepest descent method is performed using an exact line search. The step size $\\alpha_0$ is chosen to be the one that exactly minimizes $f(\\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0))$.\n\nFor all subsequent iterations ($k \\geq 1$), we switch to the Barzilai-Borwein (BB) method. One of the common step-size strategies for the Barzilai-Borwein method is defined as:\n$$\\alpha_k = \\frac{s_{k-1}^T s_{k-1}}{s_{k-1}^T y_{k-1}}$$\nwhere $s_{k-1} = \\mathbf{x}_k - \\mathbf{x}_{k-1}$ is the displacement vector from the previous step, and $y_{k-1} = \\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1})$ is the change in the gradient.\n\nYou are tasked with two objectives:\n1.  Calculate the value of the first Barzilai-Borwein step size, $\\alpha_1$. Round your numerical answer to four significant figures.\n2.  The BB method is known to often outperform the steepest descent method on ill-conditioned quadratics, despite its non-monotonic behavior (i.e., the function value $f(\\mathbf{x}_k)$ may not decrease at every step). Select the statement below that best provides the geometric intuition for this accelerated convergence.\n\n(A) The BB method guarantees a monotonic decrease in the magnitude of the gradient at each step, forcing it to zero more rapidly.\n\n(B) By using a step size from the previous iteration's information, the method ensures that successive gradient vectors are perfectly collinear, eliminating zig-zagging behavior.\n\n(C) The BB step size is typically larger than what an exact line search would yield for the current direction. This \"overstepping\" breaks the strict orthogonality of consecutive gradients seen in steepest descent, producing a new search direction that is better aligned with the long-axis of the cost function's elliptical level sets.\n\n(D) The method computes a step size that is always the average of the reciprocals of the largest and smallest eigenvalues of the Hessian matrix $A$, providing a balanced move in all eigendirections.\n\n(E) The BB step size is always smaller than the steepest descent step size, which makes the algorithm more stable and prevents the divergence that can occur with large step sizes in ill-conditioned problems.\n\nProvide your answers for Part 1 and Part 2 as a two-element row matrix, with the numerical answer for $\\alpha_1$ as the first element and the capital letter corresponding to the correct choice for Part 2 as the second element.", "solution": "We are minimizing the quadratic $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}A\\mathbf{x}$ with $A=\\mathrm{diag}(100,1)$. For a symmetric matrix $A$, the gradient is $\\nabla f(\\mathbf{x})=A\\mathbf{x}$.\n\nStarting from $\\mathbf{x}_{0}=\\begin{pmatrix}1\\\\10\\end{pmatrix}$, the steepest descent direction at $k=0$ is $-\\nabla f(\\mathbf{x}_{0})$. Compute the gradient:\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{0}=\\begin{pmatrix}100\\\\10\\end{pmatrix}.\n$$\nWith exact line search along $-\\mathbf{g}_{0}$, the optimal step size is the standard quadratic formula\n$$\n\\alpha_{0}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}}.\n$$\nCompute the required inner products:\n$$\n\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=100^{2}+10^{2}=10100,\\qquad\nA\\mathbf{g}_{0}=\\begin{pmatrix}10000\\\\10\\end{pmatrix},\\qquad\n\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}=100\\cdot 10000+10\\cdot 10=1000100.\n$$\nHence\n$$\n\\alpha_{0}=\\frac{10100}{1000100}=\\frac{101}{10001}.\n$$\nUpdate to $\\mathbf{x}_{1}$:\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}\n=\\begin{pmatrix}1\\\\10\\end{pmatrix}-\\frac{101}{10001}\\begin{pmatrix}100\\\\10\\end{pmatrix}\n=\\begin{pmatrix}1-\\frac{10100}{10001}\\\\ 10-\\frac{1010}{10001}\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{99}{10001}\\\\ \\frac{99000}{10001}\\end{pmatrix}.\n$$\n\nFor the first Barzilai-Borwein step ($k=1$), define\n$$\n\\mathbf{s}_{0}=\\mathbf{x}_{1}-\\mathbf{x}_{0}=-\\alpha_{0}\\mathbf{g}_{0},\\qquad\n\\mathbf{y}_{0}=\\nabla f(\\mathbf{x}_{1})-\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{1}-A\\mathbf{x}_{0}=A(\\mathbf{x}_{1}-\\mathbf{x}_{0})=A\\mathbf{s}_{0}.\n$$\nThe BB step size prescribed is\n$$\n\\alpha_{1}=\\frac{\\mathbf{s}_{0}^{T}\\mathbf{s}_{0}}{\\mathbf{s}_{0}^{T}\\mathbf{y}_{0}}\n=\\frac{(-\\alpha_{0}\\mathbf{g}_{0})^{T}(-\\alpha_{0}\\mathbf{g}_{0})}{(-\\alpha_{0}\\mathbf{g}_{0})^{T}(A(-\\alpha_{0}\\mathbf{g}_{0}))}\n=\\frac{\\alpha_{0}^{2}\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\alpha_{0}^{2}\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}}\n=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}}\n=\\alpha_{0}.\n$$\nTherefore,\n$$\n\\alpha_{1}=\\frac{101}{10001}\\approx 0.0100999\\ldots,\n$$\nwhich rounded to four significant figures is $0.01010$.\n\nFor the geometric intuition, the steepest descent method with exact line search produces gradients that are orthogonal at consecutive iterates on quadratics, which induces a zig-zag along the narrow valley for ill-conditioned problems. The Barzilai-Borwein method selects a step size using secant information that is typically larger than the exact line-search step for the current steepest-descent direction, thereby breaking the strict orthogonality of consecutive gradients and yielding a search direction better aligned with the long axis of the elliptical level sets, accelerating convergence on ill-conditioned quadratics. This corresponds to option (C).", "answer": "$$\\boxed{\\begin{pmatrix} 0.01010 & C \\end{pmatrix}}$$", "id": "2162618"}, {"introduction": "Many real-world optimization problems involve constraints, such as physical limits on parameters. The projected gradient method is a standard approach for handling such problems, but its interaction with step-size strategies like Barzilai-Borwein requires careful consideration. This practice explores the subtle but important consequences of applying the BB method in a constrained setting [@problem_id:3100604]. When an iterate is projected onto a feasible set, the optimization path becomes \"kinked,\" which challenges the assumptions of the secant method underlying the BB step. You will investigate two different ways to define the gradient difference vector $y_k$ and analyze how well each one maintains consistency with the underlying quadratic model.", "problem": "Consider the unconstrained quadratic objective and its box-constrained variant. Let the objective be $$f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x,$$ where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $c \\in \\mathbb{R}^n$. The gradient is $$\\nabla f(x) = Q x + c.$$ Let the feasible set be the axis-aligned box $$[l,u] = \\{ x \\in \\mathbb{R}^n \\mid l_i \\le x_i \\le u_i \\text{ for all } i \\},$$ and let the projection onto the box be the element-wise clipping operator $$\\Pi_{[l,u]}(z)_i = \\min(\\max(z_i,l_i),u_i).$$\n\nYou will analyze the effect of Barzilai-Borwein step sizes near the constraint boundaries in a projected gradient method, where the projection can cause kinked paths. The projected gradient iteration is $$x_{k+1} = \\Pi_{[l,u]}(x_k - \\alpha_k \\nabla f(x_k)).$$ The Barzilai-Borwein approach selects $\\alpha_k$ using information from the most recent step $s_k$ and gradient change $y_k$ based on the secant relation. In unconstrained smooth problems, $s_k$ and $y_k$ arise naturally from consecutive iterates and gradients. Near boundaries in constrained problems, the projection modifies $x_{k+1}$, which can kink the path and complicate how $y_k$ should be computed.\n\nImplement and compare two variants for defining the gradient change $y_k$ in the projected setting:\n- Variant A (post-projection): define $s_k = x_k - x_{k-1}$ and $y_k = \\nabla f(x_k) - \\nabla f(x_{k-1})$ using the projected iterates.\n- Variant B (pre-projection): let $\\hat{x}_k = x_{k-1} - \\alpha_{k-1} \\nabla f(x_{k-1})$ be the tentative, unprojected step, define $s_k = x_k - x_{k-1}$ using the projected iterate $x_k = \\Pi_{[l,u]}(\\hat{x}_k)$, and set $y_k = \\nabla f(\\hat{x}_k) - \\nabla f(x_{k-1})$.\n\nFor each variant, compute $\\alpha_k$ using the Barzilai-Borwein principle derived from the secant relation between $s_k$ and $y_k$, and use it in the next iteration. To avoid numerical instabilities, if the computation is ill-conditioned (for example, a near-zero denominator), fall back to a safe positive step size and clamp the step size within a reasonable positive range. Use a fixed iteration limit of $60$ iterations.\n\nTo evaluate the consistency of each variant with the underlying quadratic model, compute at the last iteration the normalized secant residual $$r = \\frac{\\lVert Q s - y \\rVert_2}{\\lVert y \\rVert_2 + \\varepsilon},$$ where $\\varepsilon$ is a small positive scalar used only to avoid division by zero. A smaller $r$ indicates better consistency with the quadratic secant relation.\n\nUse the following test suite, where all problems are in dimension $n = 2$ with the same box $[l,u] = [0,1]^2$. In all cases, set the maximum iterations to $60$ and use $\\varepsilon = 10^{-12}$ when computing the residual.\n- Test case $1$ (interior optimum, mild conditioning): $$Q = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad c = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}, \\quad l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad u = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.$$\n- Test case $2$ (boundary optimum, both coordinates saturate at the upper bound): $$Q = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad c = \\begin{bmatrix} -6 \\\\ -6 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}, \\quad l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad u = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.$$\n- Test case $3$ (corner start, kinked path likely): $$Q = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad c = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\quad l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad u = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.$$\n\nFor each test case, run both Variant A and Variant B independently. Report, for each test case, the final objective values and the final normalized secant residuals as four real numbers in the order $[f_{\\text{A}}, f_{\\text{B}}, r_{\\text{A}}, r_{\\text{B}}]$, where $f_{\\text{A}}$ and $r_{\\text{A}}$ are from Variant A, and $f_{\\text{B}}$ and $r_{\\text{B}}$ are from Variant B.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of four floating-point numbers in the order $[f_{\\text{A}}, f_{\\text{B}}, r_{\\text{A}}, r_{\\text{B}}]$. For example, a valid output line has the form $$[[\\text{case}_1],[\\text{case}_2],[\\text{case}_3]],$$ where $\\text{case}_i$ is the four-number list for test case $i$.", "solution": "The user has provided a well-defined problem in the field of numerical optimization, asking for a comparison of two variants of the Barzilai-Borwein (BB) projected gradient method for box-constrained quadratic programming. The problem is scientifically grounded, formally specified, and requires a numerical implementation to evaluate the performance and theoretical consistency of two different strategies for defining the gradient difference vector $y_k$ near constraint boundaries.\n\nFirst, we establish the mathematical framework. The objective function is a strictly convex quadratic form:\n$$f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix and $c \\in \\mathbb{R}^n$. The gradient of this function is affine:\n$$\\nabla f(x) = Q x + c$$\nThe optimization is performed over a convex, closed set defined by an axis-aligned box:\n$$\\mathcal{C} = [l, u] = \\{ x \\in \\mathbb{R}^n \\mid l_i \\le x_i \\le u_i \\text{ for all } i=1, \\dots, n \\}$$\nThe projected gradient method generates a sequence of iterates $\\{x_k\\}$ via the update rule:\n$$x_{k+1} = \\Pi_{[l,u]}(x_k - \\alpha_k \\nabla f(x_k))$$\nHere, $\\alpha_k > 0$ is the step size at iteration $k$, and $\\Pi_{[l,u]}(\\cdot)$ is the Euclidean projection onto the feasible set $\\mathcal{C}$, which for a box is a simple element-wise clipping operation:\n$$\\Pi_{[l,u]}(z)_i = \\min(\\max(z_i, l_i), u_i)$$\n\nThe Barzilai-Borwein method provides a strategy for choosing the step size $\\alpha_k$. It aims to approximate the Hessian matrix $Q$ with a simple diagonal matrix $\\lambda I$ by enforcing a secant condition. Given two consecutive iterates $x_{k-1}$ and $x_k$, we define the step vector $s_k = x_k - x_{k-1}$ and the gradient difference vector $y_k = \\nabla f(x_k) - \\nabla f(x_{k-1})$. The secant equation relates these quantities: $y_k \\approx B_k s_k$, where $B_k$ is an approximation to the Hessian. For our quadratic problem, this relation is exact: $y_k = Q s_k$.\n\nThe BB method sets $B_k = \\alpha_k^{-1} I$ and determines $\\alpha_k$ by minimizing a norm of the secant equation residual. We will use the so-called \"long\" BB step (or BB1):\n$$\\alpha_k = \\arg\\min_{\\alpha} \\left\\lVert \\alpha^{-1} s_k - y_k \\right\\rVert_2^2 = \\frac{s_k^\\top s_k}{s_k^\\top y_k}$$\nThis step size $\\alpha_k$, computed using information from iteration $k-1$ to $k$, is used to find the next iterate $x_{k+1}$.\n\nIn the context of projected gradient methods, the projection can introduce \"kinks\" in the path of the iterates, especially near the boundary of the feasible set. This raises the question of how to best define $y_k$ to retain meaningful second-order information. The problem specifies two variants:\n- **Variant A (post-projection)**: The gradient difference $y_k$ is computed using the final projected iterates:\n$$s_k = x_k - x_{k-1}$$\n$$y_k^{(\\text{A})} = \\nabla f(x_k) - \\nabla f(x_{k-1})$$\n- **Variant B (pre-projection)**: The gradient difference uses the gradient at the tentative, unprojected iterate. Let $\\hat{x}_k = x_{k-1} - \\alpha_{k-1} \\nabla f(x_{k-1})$ be the iterate before projection, so $x_k = \\Pi_{[l,u]}(\\hat{x}_k)$. Then:\n$$s_k = x_k - x_{k-1}$$\n$$y_k^{(\\text{B})} = \\nabla f(\\hat{x}_k) - \\nabla f(x_{k-1})$$\n\nThe implementation will proceed for a fixed number of iterations, $N_{iter} = 60$. An initial step size $\\alpha_0 = 0.1$ is chosen. For subsequent iterations $k \\ge 1$, $\\alpha_k$ is computed using the BB1 formula. To ensure numerical stability, we introduce two safeguards:\n$1$. The step size is clamped to the interval $[10^{-5}, 10^5]$ to prevent extreme values.\n$2$. If the denominator $|s_k^\\top y_k|$ is close to zero (less than $10^{-12}$), we fall back to using the previous step size, i.e., $\\alpha_k = \\alpha_{k-1}$.\n\nTo evaluate the consistency of each variant with the underlying quadratic model, we compute the normalized secant residual at the final iteration, $k=N_{iter}=60$:\n$$r = \\frac{\\lVert Q s_k - y_k \\rVert_2}{\\lVert y_k \\rVert_2 + \\varepsilon}$$\nwhere $s_k = x_k - x_{k-1}$, $y_k$ is from either Variant A or B, and $\\varepsilon=10^{-12}$ is a small regularization constant.\nFor Variant A, due to the linearity of the gradient of the quadratic function, we have:\n$$y_k^{(\\text{A})} = \\nabla f(x_k) - \\nabla f(x_{k-1}) = (Q x_k + c) - (Q x_{k-1} + c) = Q(x_k - x_{k-1}) = Q s_k$$\nTherefore, the numerator $\\lVert Q s_k - y_k^{(\\text{A})} \\rVert_2$ should theoretically be zero, and the residual $r_{\\text{A}}$ should be close to machine precision.\nFor Variant B, the gradient difference is:\n$$y_k^{(\\text{B})} = \\nabla f(\\hat{x}_k) - \\nabla f(x_{k-1}) = Q(\\hat{x}_k - x_{k-1})$$\nThe numerator of the residual becomes:\n$$\\lVert Q s_k - y_k^{(\\text{B})} \\rVert_2 = \\lVert Q(x_k - x_{k-1}) - Q(\\hat{x}_k - x_{k-1}) \\rVert_2 = \\lVert Q(x_k - \\hat{x}_k) \\rVert_2$$\nThis term will be non-zero whenever the projection is active (i.e., $x_k \\neq \\hat{x}_k$). Thus, we expect $r_{\\text{B}} > 0$ in cases where the algorithm path interacts with the constraint boundaries.\n\nThe algorithm is implemented for each test case and for each variant. The final objective value $f(x_{60})$ and the final secant residual $r$ (using $s_{60}$ and $y_{60}$) are recorded.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_projected_bb(Q, c, l, u, x0, variant, max_iter=60, epsilon=1e-12):\n    \"\"\"\n    Implements the Projected Barzilai-Borwein method for a box-constrained QP.\n\n    Args:\n        Q (np.ndarray): The quadratic matrix.\n        c (np.ndarray): The linear term vector.\n        l (np.ndarray): The lower bounds of the box.\n        u (np.ndarray): The upper bounds of the box.\n        x0 (np.ndarray): The initial point.\n        variant (str): 'A' or 'B', for post-projection or pre-projection y_k.\n        max_iter (int): The number of iterations.\n        epsilon (float): Small constant for residual calculation.\n\n    Returns:\n        tuple: A tuple containing the final objective value (float) and the\n               final normalized secant residual (float).\n    \"\"\"\n\n    # History of iterates and step sizes\n    x_hist = [np.copy(x0)]  # x_hist[k] is x_k\n    alpha_hist = []      # alpha_hist[k] is alpha_k\n\n    # k=0: First step with a fixed initial step size alpha_0\n    alpha_0 = 0.1\n    grad_0 = Q @ x0 + c\n    x_1 = np.clip(x0 - alpha_0 * grad_0, l, u)\n    x_hist.append(x_1)\n    alpha_hist.append(alpha_0)\n\n    # Main loop to generate x_2 to x_{max_iter}\n    # In iteration k (1-based), we compute alpha_k and x_{k+1}\n    for k in range(1, max_iter):\n        x_k = x_hist[k]\n        x_k_minus_1 = x_hist[k-1]\n        alpha_k_minus_1 = alpha_hist[k-1]\n        \n        s_k = x_k - x_k_minus_1\n        grad_k_minus_1 = Q @ x_k_minus_1 + c\n\n        if variant == 'A':\n            grad_k = Q @ x_k + c\n            y_k = grad_k - grad_k_minus_1\n        elif variant == 'B':\n            # Tentative point that was projected to get x_k\n            x_hat_k = x_k_minus_1 - alpha_k_minus_1 * grad_k_minus_1\n            grad_hat_k = Q @ x_hat_k + c\n            y_k = grad_hat_k - grad_k_minus_1\n        else:\n            raise ValueError(\"Variant must be 'A' or 'B'\")\n        \n        # Compute alpha_k using the BB1 formula\n        s_dot_y = s_k.dot(y_k)\n        if abs(s_dot_y) < 1e-14: # Small tolerance for denominator\n            alpha_k = alpha_k_minus_1  # Fallback to previous step size\n        else:\n            alpha_k = s_k.dot(s_k) / s_dot_y\n        \n        # Safeguard the step size\n        alpha_k = np.clip(alpha_k, 1e-5, 1e5)\n        \n        # Compute x_{k+1}\n        grad_k = Q @ x_k + c # Re-use or re-calculate as needed\n        x_k_plus_1 = np.clip(x_k - alpha_k * grad_k, l, u)\n        \n        # Store for the next iteration\n        x_hist.append(x_k_plus_1)\n        alpha_hist.append(alpha_k)\n\n    # After the loop, x_hist contains x_0, ..., x_{max_iter}\n    x_final = x_hist[max_iter]\n    \n    # Calculate final objective value\n    final_f = 0.5 * x_final.T @ Q @ x_final + c.T @ x_final\n\n    # Calculate final normalized secant residual using s_{max_iter}, y_{max_iter}\n    k = max_iter\n    s_final = x_hist[k] - x_hist[k - 1]\n    grad_k_minus_1 = Q @ x_hist[k - 1] + c\n    alpha_k_minus_1 = alpha_hist[k - 1]\n\n    if variant == 'A':\n        grad_k = Q @ x_hist[k] + c\n        y_final = grad_k - grad_k_minus_1\n    elif variant == 'B':\n        x_hat_k = x_hist[k-1] - alpha_k_minus_1 * grad_k_minus_1\n        grad_hat_k = Q @ x_hat_k + c\n        y_final = grad_hat_k - grad_k_minus_1\n    \n    residual_numerator = np.linalg.norm(Q @ s_final - y_final)\n    residual_denominator = np.linalg.norm(y_final) + epsilon\n    final_r = residual_numerator / residual_denominator\n    \n    return final_f, final_r\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the B-B variants, and prints results.\n    \"\"\"\n    test_cases = [\n        {\n            \"Q\": np.array([[4.0, 1.0], [1.0, 3.0]]),\n            \"c\": np.array([-2.0, -1.0]),\n            \"x0\": np.array([0.8, 0.2]),\n        },\n        {\n            \"Q\": np.array([[4.0, 1.0], [1.0, 3.0]]),\n            \"c\": np.array([-6.0, -6.0]),\n            \"x0\": np.array([0.1, 0.1]),\n        },\n        {\n            \"Q\": np.array([[4.0, 1.0], [1.0, 3.0]]),\n            \"c\": np.array([-2.0, -1.0]),\n            \"x0\": np.array([1.0, 0.0]),\n        },\n    ]\n    \n    l = np.array([0.0, 0.0])\n    u = np.array([1.0, 1.0])\n    max_iter = 60\n    epsilon = 1e-12\n\n    results = []\n    for case in test_cases:\n        Q, c, x0 = case[\"Q\"], case[\"c\"], case[\"x0\"]\n        \n        f_A, r_A = run_projected_bb(Q, c, l, u, x0, 'A', max_iter, epsilon)\n        f_B, r_B = run_projected_bb(Q, c, l, u, x0, 'B', max_iter, epsilon)\n        \n        results.append([f_A, f_B, r_A, r_B])\n\n    # Convert list of lists to the required string format\n    result_str = \",\".join(map(str, results))\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```", "id": "3100604"}, {"introduction": "Moving from theoretical formulas to robust, practical code requires anticipating and handling potential numerical instabilities. The Barzilai-Borwein method, particularly the second variant (BB2), contains a potential pitfall that can lead to catastrophic failure if not properly managed. In this final exercise, you will analyze a scenario where the BB2 step size formula can produce an extremely large and unstable step due to a near-zero denominator [@problem_id:3100631]. By examining this failure mode, you will learn to appreciate the importance of building safeguards into optimization algorithms and evaluate different strategies for ensuring stability, a critical skill for any practitioner in numerical computing.", "problem": "Consider the gradient method applied to a smooth quadratic objective in $3$ dimensions, defined by $f(x) = \\tfrac{1}{2} x^{\\top} H x$ with a symmetric positive definite Hessian $H \\in \\mathbb{R}^{3 \\times 3}$. Let $H = \\operatorname{diag}(10, 10, 10^{-8})$, and suppose at iteration $k-1$ the point is $x_{k-1} = (1, 1, 1)^{\\top}$ and the next iterate is taken along the third coordinate direction: $x_{k} = x_{k-1} + \\delta e_{3} = (1, 1, 1 + \\delta)^{\\top}$ for some $\\delta \\in \\mathbb{R}$, where $e_{3}$ is the third standard basis vector. Define the standard gradient-difference and step-difference quantities used in gradient-based methods: $s_{k-1} = x_{k} - x_{k-1}$ and $y_{k-1} = \\nabla f(x_{k}) - \\nabla f(x_{k-1})$. Assume a Barzilai–Borwein (BB) gradient method that periodically computes the second Barzilai–Borwein step size (BB2), which is built from the secant idea using $s_{k-1}$ and $y_{k-1}$, and recall that gradients may be nonzero even if gradient differences are small.\n\nPart $\\mathrm{I}$ (construction): Using the fundamental relation for quadratic objectives, $\\nabla f(x) = H x$, show that for $\\delta$ of order $1$ the quantity $y_{k-1}^{\\top} y_{k-1}$ can be arbitrarily close to machine precision while the individual gradients $\\nabla f(x_{k-1})$ and $\\nabla f(x_{k})$ are nonzero and of moderately large magnitude. Explain the role of anisotropic curvature (eigenvalues of $H$) in producing this effect.\n\nPart $\\mathrm{II}$ (safeguards): The BB2 step size depends critically on $y_{k-1}^{\\top} y_{k-1}$. In the scenario above, selecting BB2 without safeguards may lead to numerical instability. Which of the following strategies is the most appropriate to ensure stable iteration while preserving the secant-motivated scaling and providing a principled fallback to the first Barzilai–Borwein step size (BB1)?\n\nA. Impose bounds on the BB2 step size by clipping it into an interval $[\\alpha_{\\min}, \\alpha_{\\max}]$ with $\\alpha_{\\min} > 0$ and $\\alpha_{\\max} < \\infty$; additionally, if $y_{k-1}^{\\top} y_{k-1} \\le \\varepsilon$ for a small threshold $\\varepsilon > 0$, bypass BB2 and compute the BB1 step size instead, using it only if $s_{k-1}^{\\top} y_{k-1} > 0$; otherwise, reuse the previous step size or a safe default.\n\nB. Replace $y_{k-1}^{\\top} y_{k-1}$ by $y_{k-1}^{\\top} y_{k-1} + \\varepsilon$ with a small $\\varepsilon > 0$ in the BB2 formula and proceed without any bounding or fallback, since the regularization alone prevents division by zero.\n\nC. Normalize the gradient difference before forming BB2 by setting $\\tilde{y}_{k-1} = y_{k-1} / \\|y_{k-1}\\|_{2}$ and then use $\\tilde{y}_{k-1}$ in place of $y_{k-1}$ to avoid small denominators; this ensures the denominator is exactly $1$.\n\nD. Rely on a monotone backtracking line search after computing BB2, which will reject any unstable step lengths; no bounding or fallback is needed because the line search alone enforces stability.\n\nSelect the single best option.", "solution": "This problem is scientifically sound and illustrates a critical failure mode of the BB2 step size in ill-conditioned problems. We analyze it in two parts.\n\n**Part I: Construction and Analysis**\n\nThe objective function is $f(x) = \\tfrac{1}{2} x^{\\top} H x$ with $H = \\operatorname{diag}(10, 10, 10^{-8})$. The gradient is $\\nabla f(x) = Hx$.\nGiven $x_{k-1} = (1, 1, 1)^{\\top}$ and $x_{k} = (1, 1, 1 + \\delta)^{\\top}$, we first calculate the gradients:\n$$ \\nabla f(x_{k-1}) = H x_{k-1} = \\begin{pmatrix} 10 \\\\ 10 \\\\ 10^{-8} \\end{pmatrix} $$\n$$ \\nabla f(x_{k}) = H x_{k} = \\begin{pmatrix} 10 \\\\ 10 \\\\ 10^{-8}(1+\\delta) \\end{pmatrix} $$\nFor $\\delta \\sim O(1)$, the norms of these gradients are dominated by the first two components, $\\|\\nabla f\\| \\approx \\sqrt{10^2+10^2} \\approx 14.14$, which is a moderately large magnitude.\n\nNext, we compute the step difference $s_{k-1}$ and the gradient difference $y_{k-1}$:\n$$ s_{k-1} = x_{k} - x_{k-1} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\delta \\end{pmatrix} $$\n$$ y_{k-1} = \\nabla f(x_{k}) - \\nabla f(x_{k-1}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\delta \\cdot 10^{-8} \\end{pmatrix} $$\nThe critical quantity for the BB2 step size is the denominator $y_{k-1}^{\\top} y_{k-1}$:\n$$ y_{k-1}^{\\top} y_{k-1} = \\|y_{k-1}\\|_2^2 = (\\delta \\cdot 10^{-8})^2 = \\delta^2 \\cdot 10^{-16} $$\nFor $\\delta \\sim O(1)$, this value is on the order of $10^{-16}$, which is close to machine precision.\n\n**Explanation:** The Hessian's eigenvalues ($10, 10, 10^{-8}$) show highly anisotropic curvature. The step $s_{k-1}$ is taken purely in the direction of minimum curvature ($e_3$). The resulting gradient change $y_{k-1}=Hs_{k-1}$ is scaled by the tiny eigenvalue $\\lambda_3=10^{-8}$, making its magnitude extremely small. However, the overall gradient remains large due to components in the high-curvature directions. This creates the described scenario.\n\n**Part II: Safeguards for the BB2 Step Size**\n\nThe second Barzilai-Borwein (BB2) step size is $\\alpha_k^{\\text{BB2}} = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$. Let's compute it for this case:\n$$ \\alpha_k^{\\text{BB2}} = \\frac{(\\delta e_3)^{\\top} (\\delta \\cdot 10^{-8} e_3)}{(\\delta \\cdot 10^{-8})^2} = \\frac{\\delta^2 \\cdot 10^{-8}}{\\delta^2 \\cdot 10^{-16}} = 10^8 $$\nThis enormous step size would cause the algorithm to diverge. We need a robust safeguard.\n\n- **Option A** proposes a comprehensive, multi-layered defense: (1) Check for a near-zero denominator. (2) If so, fall back to the BB1 step. (3) Always check the curvature condition ($s_{k-1}^{\\top} y_{k-1} > 0$). (4) Use a final catch-all by clipping the resulting step size into a safe interval $[\\alpha_{\\min}, \\alpha_{\\max}]$. This is the standard procedure for a robust BB implementation. It preserves the secant motivation where possible but prioritizes stability.\n- **Option B** (regularization) is an ad-hoc fix that modifies the BB formula and is not a general solution.\n- **Option C** (normalization) destroys the essential scaling information from the secant condition, violating a core principle of the method.\n- **Option D** (monotone line search) is incompatible with the non-monotone nature of the BB method and is a computationally expensive, reactive fix for a bad step-size calculation.\n\nTherefore, Option A represents the most appropriate and principled strategy.", "answer": "$$\\boxed{A}$$", "id": "3100631"}]}