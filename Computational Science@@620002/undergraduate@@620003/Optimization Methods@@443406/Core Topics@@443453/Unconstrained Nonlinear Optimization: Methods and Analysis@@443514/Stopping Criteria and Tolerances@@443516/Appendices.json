{"hands_on_practices": [{"introduction": "In practical optimization, relying on a single stopping condition can be unreliable. For instance, a small gradient norm signals proximity to a stationary point, but an algorithm might also stall far from a solution, exhibiting tiny step sizes and negligible progress in the objective function. This exercise guides you through building a more robust, multi-gated stopping rule that checks for these conditions in a prioritized, lexicographic order, providing a safety net against different types of algorithmic stagnation [@problem_id:3187944].", "problem": "You are asked to design and implement a lexicographic stopping rule for an iterative optimization method that combines the norm of the gradient, the norm of the step, and the per-iteration function decrease, and to evaluate it on nonconvex benchmark functions. The implementation must be a complete, runnable program.\n\nThe fundamental base to use is the first-order necessary optimality condition for unconstrained differentiable optimization: at a local minimizer of a differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$, the gradient satisfies $\\nabla f(x^\\star)=\\mathbf{0}$. Numerically, this principle motivates stopping when the gradient norm is small. However, numerical stagnation can also be detected by small step sizes or negligible function decrease. Your task is to derive a lexicographic stopping rule that prioritizes these criteria and to implement it in gradient descent with backtracking line search.\n\nDefine the iterative method as follows. Given an initial point $x_0\\in\\mathbb{R}^n$, at iteration $k$ with current point $x_k$, compute the gradient $g_k=\\nabla f(x_k)$ and its norm $G_k=\\lVert g_k\\rVert_2$. Perform a backtracking line search starting from an initial step size $t_0>0$ with shrinkage factor $\\beta\\in(0,1)$ and Armijo parameter $c\\in(0,1)$ to find a step size $t_k$ such that the Armijo condition holds:\n$$\nf(x_k - t_k g_k) \\le f(x_k) - c\\,t_k\\,\\lVert g_k\\rVert_2^2.\n$$\nUpdate the iterate $x_{k+1}=x_k - t_k g_k$. Define the step norm $S_k=\\lVert x_{k+1}-x_k\\rVert_2$ and the one-step function decrease $D_k=f(x_k)-f(x_{k+1})$.\n\nDesign the lexicographic stopping rule with tolerances $\\tau_g>0$, $\\tau_s>0$, and $\\tau_f>0$ as follows, evaluated in the stated order at each iteration $k$:\n- Gate $1$ (gradient norm priority): if $G_k\\le \\tau_g$, stop.\n- Gate $2$ (step norm priority): if $k\\ge 0$ and $S_k\\le \\tau_s$, stop.\n- Gate $3$ (function decrease priority): if $k\\ge 0$ and $D_k\\le \\tau_f$, stop.\nIf none of the gates activates and a maximum number of iterations $N_{\\max}$ is reached, terminate.\n\nYou must implement this rule in gradient descent with backtracking for the following nonconvex benchmark functions:\n\n- Himmelblau’s function in dimension $2$, defined by\n$$\nf_H(x,y)=\\left(x^2+y-11\\right)^2+\\left(x+y^2-7\\right)^2.\n$$\n\n- Rastrigin’s function in dimension $2$, with $A=10$, defined by\n$$\nf_R(x,y)=2A+\\left(x^2 - A\\cos(2\\pi x)\\right)+\\left(y^2 - A\\cos(2\\pi y)\\right).\n$$\n\nYour program must use the lexicographic stopping rule and backtracking line search described above. For each test case, return an integer code indicating which gate stopped the iteration: return $1$ if Gate $1$ activates, $2$ if Gate $2$ activates, $3$ if Gate $3$ activates, or $4$ if termination occurs due to reaching $N_{\\max}$.\n\nImplement and evaluate the rule on the following test suite. Each test case is a parameter tuple $(f,\\ x_0,\\ \\tau_g,\\ \\tau_s,\\ \\tau_f,\\ t_0,\\ \\beta,\\ c,\\ N_{\\max})$:\n\n- Case A (happy path, gradient gate expected to dominate): $f=f_H$, $x_0=(0,0)$, $\\tau_g=10^{-4}$, $\\tau_s=10^{-12}$, $\\tau_f=10^{-12}$, $t_0=1$, $\\beta=0.5$, $c=10^{-4}$, $N_{\\max}=10^4$.\n\n- Case B (edge case, step gate forced via tiny initial step): $f=f_R$, $x_0=(3,3)$, $\\tau_g=10^{-100}$, $\\tau_s=10^{-10}$, $\\tau_f=10^{-100}$, $t_0=10^{-12}$, $\\beta=0.5$, $c=10^{-4}$, $N_{\\max}=10^3$.\n\n- Case C (edge case, function-decrease gate with stringent gradient and step tolerances): $f=f_R$, $x_0=(0.3,-0.3)$, $\\tau_g=10^{-20}$, $\\tau_s=10^{-50}$, $\\tau_f=10^{-8}$, $t_0=0.5$, $\\beta=0.5$, $c=10^{-4}$, $N_{\\max}=10^4$.\n\n- Case D (boundary case, stationary point at start): $f=f_H$, $x_0=(3,2)$, $\\tau_g=10^{-8}$, $\\tau_s=10^{-12}$, $\\tau_f=10^{-12}$, $t_0=1$, $\\beta=0.5$, $c=10^{-4}$, $N_{\\max}=10^3$.\n\nYour program should produce a single line of output containing the stop-reason codes for the cases A, B, C, and D, in this order, as a comma-separated list enclosed in square brackets, for example, `[1,2,3,1]`.", "solution": "The user has requested the design and implementation of a gradient descent algorithm equipped with a specific lexicographic stopping rule. This solution formalizes the algorithm, defines the necessary mathematical components, and outlines the implementation logic.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Iterative Method**: Gradient descent with backtracking line search.\n- **Update Rule**: $x_{k+1} = x_k - t_k g_k$, where $g_k = \\nabla f(x_k)$.\n- **Backtracking Line Search**:\n    - **Armijo Condition**: $f(x_k - t_k g_k) \\le f(x_k) - c\\,t_k\\,\\lVert g_k\\rVert_2^2$.\n    - **Parameters**: Initial step size $t_0 > 0$, shrinkage factor $\\beta \\in (0,1)$, Armijo parameter $c \\in (0,1)$.\n- **Quantities for Stopping**:\n    - Gradient norm: $G_k = \\lVert g_k \\rVert_2$.\n    - Step norm: $S_k = \\lVert x_{k+1} - x_k \\rVert_2$.\n    - Function decrease: $D_k = f(x_k) - f(x_{k+1})$.\n- **Lexicographic Stopping Rule**:\n    - **Tolerances**: $\\tau_g > 0$, $\\tau_s > 0$, $\\tau_f > 0$.\n    - **Maximum Iterations**: $N_{\\max}$.\n    - **Order of Evaluation**:\n        1.  **Gate 1**: If $G_k \\le \\tau_g$, stop (return code $1$).\n        2.  **Gate 2**: If $k \\ge 0$ and $S_k \\le \\tau_s$, stop (return code $2$).\n        3.  **Gate 3**: If $k \\ge 0$ and $D_k \\le \\tau_f$, stop (return code $3$).\n        4.  **Max Iterations**: If the loop completes $N_{\\max}$ iterations, stop (return code $4$).\n- **Benchmark Functions**:\n    - **Himmelblau's function ($f_H$)**: $f_H(x,y) = \\left(x^2+y-11\\right)^2+\\left(x+y^2-7\\right)^2$.\n    - **Rastrigin's function ($f_R$)**: $f_R(x,y) = 2A+\\left(x^2 - A\\cos(2\\pi x)\\right)+\\left(y^2 - A\\cos(2\\pi y)\\right)$ with $A=10$.\n- **Test Suite**:\n    - **Case A**: $(f_H, x_0=(0,0), \\tau_g=10^{-4}, \\tau_s=10^{-12}, \\tau_f=10^{-12}, t_0=1, \\beta=0.5, c=10^{-4}, N_{\\max}=10^4)$.\n    - **Case B**: $(f_R, x_0=(3,3), \\tau_g=10^{-100}, \\tau_s=10^{-10}, \\tau_f=10^{-100}, t_0=10^{-12}, \\beta=0.5, c=10^{-4}, N_{\\max}=10^3)$.\n    - **Case C**: $(f_R, x_0=(0.3,-0.3), \\tau_g=10^{-20}, \\tau_s=10^{-50}, \\tau_f=10^{-8}, t_0=0.5, \\beta=0.5, c=10^{-4}, N_{\\max}=10^4)$.\n    - **Case D**: $(f_H, x_0=(3,2), \\tau_g=10^{-8}, \\tau_s=10^{-12}, \\tau_f=10^{-12}, t_0=1, \\beta=0.5, c=10^{-4}, N_{\\max}=10^3)$.\n- **Output Format**: A comma-separated list of integer stop-reason codes, e.g., `[1,2,3,1]`.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is based on fundamental and standard concepts in numerical optimization: gradient descent, first-order optimality conditions, backtracking line search (Armijo rule), and stopping criteria. The benchmark functions are standard for testing nonconvex optimization routines. The problem is scientifically sound.\n- **Well-Posed**: The problem is a computational task with a clearly defined algorithm, specific inputs, and a deterministic output format. A unique and meaningful solution (the sequence of stop-codes) exists and can be computed.\n- **Objective**: All definitions and parameters are provided in precise mathematical language. There are no subjective or ambiguous statements.\n- **Complete and Consistent**: All necessary information to implement the algorithm and run the test cases is provided. The parameters are specified, the functions are defined, and the logic of the stopping rule is unambiguous. The condition \"if $k \\ge 0$\" for Gates $2$ and $3$ is trivially satisfied within a standard zero-indexed iteration loop but correctly implies these checks are active from the first iteration onwards.\n- **No other flaws detected**: The problem is not unrealistic, ill-posed, trivial, or unverifiable. It is a well-structured exercise in computational mathematics.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core of the problem is to implement a gradient descent algorithm. The algorithm iteratively moves towards a local minimum of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ by taking steps in the direction of the negative gradient.\n\n**1. Gradient Descent Iteration**\nAt each iteration $k$, starting from a point $x_k$, the next point $x_{k+1}$ is found by:\n$$\nx_{k+1} = x_k - t_k g_k\n$$\nwhere $g_k = \\nabla f(x_k)$ is the gradient of $f$ at $x_k$, and $t_k > 0$ is the step size.\n\n**2. Step Size Selection: Backtracking Line Search**\nThe step size $t_k$ is determined by a backtracking line search procedure to ensure sufficient decrease in the function value. Starting with an initial guess $t = t_0$, the step size is repeatedly reduced by a factor $\\beta \\in (0,1)$ until the Armijo-Goldstein condition is satisfied:\n$$\nf(x_k - t g_k) \\le f(x_k) - c t \\lVert g_k \\rVert_2^2\n$$\nfor a constant $c \\in (0,1)$. The first value of $t$ that satisfies this inequality is chosen as $t_k$. This guarantees that the step provides a decrease in the objective function that is proportional to both the step size and the squared norm of the gradient.\n\n**3. Benchmark Functions and Gradients**\nThe algorithm will be tested on two nonconvex functions. For implementation, their gradients are required.\n\n- **Himmelblau's function**: $f_H(x,y) = (x^2+y-11)^2 + (x+y^2-7)^2$.\n  Its gradient $\\nabla f_H = (\\frac{\\partial f_H}{\\partial x}, \\frac{\\partial f_H}{\\partial y})$ is:\n  $$\n  \\frac{\\partial f_H}{\\partial x} = 2(x^2+y-11)(2x) + 2(x+y^2-7)(1) = 4x(x^2+y-11) + 2(x+y^2-7)\n  $$\n  $$\n  \\frac{\\partial f_H}{\\partial y} = 2(x^2+y-11)(1) + 2(x+y^2-7)(2y) = 2(x^2+y-11) + 4y(x+y^2-7)\n  $$\n\n- **Rastrigin's function**: $f_R(x,y) = 2A + (x^2 - A\\cos(2\\pi x)) + (y^2 - A\\cos(2\\pi y))$ with $A=10$.\n  Its gradient $\\nabla f_R = (\\frac{\\partial f_R}{\\partial x}, \\frac{\\partial f_R}{\\partial y})$ is:\n  $$\n  \\frac{\\partial f_R}{\\partial x} = 2x - A(-\\sin(2\\pi x))(2\\pi) = 2x + 2\\pi A \\sin(2\\pi x)\n  $$\n  $$\n  \\frac{\\partial f_R}{\\partial y} = 2y - A(-\\sin(2\\pi y))(2\\pi) = 2y + 2\\pi A \\sin(2\\pi y)\n  $$\n\n**4. Lexicographic Stopping Rule and Implementation Logic**\nThe main loop of the algorithm proceeds for a maximum of $N_{\\max}$ iterations. At each iteration $k$, the stopping criteria are checked in a precise, lexicographic order.\n\nLet $x_k$ be the current iterate.\n1.  Compute the function value $f_k = f(x_k)$ and gradient $g_k = \\nabla f(x_k)$.\n2.  Compute the gradient norm $G_k = \\lVert g_k \\rVert_2$.\n3.  **Check Gate 1**: If $G_k \\le \\tau_g$, the algorithm terminates and returns stop code $1$. This check is based on the first-order necessary condition for optimality, $\\nabla f(x^\\star) = \\mathbf{0}$.\n4.  If Gate $1$ is not met, perform the backtracking line search to find the step size $t_k$.\n5.  Compute the next iterate: $x_{k+1} = x_k - t_k g_k$.\n6.  Compute the step norm $S_k = \\lVert x_{k+1} - x_k \\rVert_2 = t_k \\lVert g_k \\rVert_2$.\n7.  Compute the new function value $f_{k+1} = f(x_{k+1})$ and the function decrease $D_k = f_k - f_{k+1}$.\n8.  **Check Gate 2**: If $S_k \\le \\tau_s$, the algorithm terminates and returns stop code $2$. This indicates that the iterates are no longer moving significantly, suggesting numerical stagnation or convergence.\n9.  **Check Gate 3**: If $D_k \\le \\tau_f$, the algorithm terminates and returns stop code $3$. This indicates that the improvement in the objective function per iteration is negligible.\n10. If none of the gates are activated, the loop continues to the next iteration, $k+1$, with $x_{k+1}$ as the new point.\n11. **Check Max Iterations**: If the loop completes $N_{\\max}$ iterations without any gate being activated, the algorithm terminates and returns stop code $4$.\n\nThis sequence of operations is implemented for each test case to determine the corresponding stop code.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the optimization algorithm.\n    \"\"\"\n\n    # Define benchmark functions and their gradients\n    def himmelblau(x):\n        \"\"\"Himmelblau's function.\"\"\"\n        return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n\n    def grad_himmelblau(x):\n        \"\"\"Gradient of Himmelblau's function.\"\"\"\n        dx = 4 * x[0] * (x[0]**2 + x[1] - 11) + 2 * (x[0] + x[1]**2 - 7)\n        dy = 2 * (x[0]**2 + x[1] - 11) + 4 * x[1] * (x[0] + x[1]**2 - 7)\n        return np.array([dx, dy])\n\n    def rastrigin(x):\n        \"\"\"Rastrigin's function.\"\"\"\n        A = 10\n        return 2 * A + (x[0]**2 - A * np.cos(2 * np.pi * x[0])) + \\\n               (x[1]**2 - A * np.cos(2 * np.pi * x[1]))\n\n    def grad_rastrigin(x):\n        \"\"\"Gradient of Rastrigin's function.\"\"\"\n        A = 10\n        dx = 2 * x[0] + 2 * np.pi * A * np.sin(2 * np.pi * x[0])\n        dy = 2 * x[1] + 2 * np.pi * A * np.sin(2 * np.pi * x[1])\n        return np.array([dx, dy])\n\n    def lexicographic_gradient_descent(f, grad_f, x0, tau_g, tau_s, tau_f, t0, beta, c, n_max):\n        \"\"\"\n        Performs gradient descent with backtracking and a lexicographic stopping rule.\n        \n        Returns an integer stop code:\n        1: Gradient norm tolerance met (Gate 1)\n        2: Step norm tolerance met (Gate 2)\n        3: Function decrease tolerance met (Gate 3)\n        4: Maximum iterations reached\n        \"\"\"\n        x_k = np.array(x0, dtype=float)\n\n        for k in range(n_max):\n            f_k = f(x_k)\n            g_k = grad_f(x_k)\n            G_k = np.linalg.norm(g_k)\n\n            # Gate 1: Gradient norm priority\n            if G_k <= tau_g:\n                return 1\n\n            # Backtracking line search\n            t_k = t0\n            while f(x_k - t_k * g_k) > f_k - c * t_k * (G_k**2):\n                t_k *= beta\n                # Safety break for excessively small step sizes\n                if t_k < 1e-20: \n                    t_k = 0\n                    break\n\n            x_k_plus_1 = x_k - t_k * g_k\n            \n            S_k = np.linalg.norm(x_k_plus_1 - x_k)\n            f_k_plus_1 = f(x_k_plus_1)\n            D_k = f_k - f_k_plus_1\n\n            # Gate 2: Step norm priority\n            if S_k <= tau_s:\n                return 2\n\n            # Gate 3: Function decrease priority\n            if D_k <= tau_f:\n                return 3\n\n            x_k = x_k_plus_1\n        \n        # Termination due to maximum iterations\n        return 4\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'f': himmelblau, 'grad_f': grad_himmelblau, 'x0': (0, 0), 'tau_g': 1e-4, \n         'tau_s': 1e-12, 'tau_f': 1e-12, 't0': 1, 'beta': 0.5, 'c': 1e-4, 'n_max': 10000},\n        # Case B\n        {'f': rastrigin, 'grad_f': grad_rastrigin, 'x0': (3, 3), 'tau_g': 1e-100, \n         'tau_s': 1e-10, 'tau_f': 1e-100, 't0': 1e-12, 'beta': 0.5, 'c': 1e-4, 'n_max': 1000},\n        # Case C\n        {'f': rastrigin, 'grad_f': grad_rastrigin, 'x0': (0.3, -0.3), 'tau_g': 1e-20, \n         'tau_s': 1e-50, 'tau_f': 1e-8, 't0': 0.5, 'beta': 0.5, 'c': 1e-4, 'n_max': 10000},\n        # Case D\n        {'f': himmelblau, 'grad_f': grad_himmelblau, 'x0': (3, 2), 'tau_g': 1e-8, \n         'tau_s': 1e-12, 'tau_f': 1e-12, 't0': 1, 'beta': 0.5, 'c': 1e-4, 'n_max': 1000}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lexicographic_gradient_descent(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187944"}, {"introduction": "Not all optimization algorithms have access to gradient information. For derivative-free methods like the Nelder-Mead algorithm, we must rely on other indicators to decide when to stop. This practice explores the trade-offs between two common criteria: a geometric one based on the size of the search simplex, and a functional one based on the spread of objective values [@problem_id:3187957]. By testing these on carefully chosen corner cases, you will develop an intuition for when each criterion is most reliable and how they can sometimes fail.", "problem": "You are given the task of implementing the Nelder–Mead (NM) method with two alternative stopping criteria to compare their behavior and reliability. The Nelder–Mead method is an iterative direct search algorithm for unconstrained minimization that maintains a simplex, that is, a set of $n+1$ points in $\\mathbb{R}^n$, and generates new candidate points through affine combinations of existing vertices. The algorithm uses reflection, expansion, contraction, and shrink transformations to move the simplex toward regions of lower objective values.\n\nFundamental base and definitions:\n- The objective is to minimize a real-valued function $f:\\mathbb{R}^n\\to\\mathbb{R}$.\n- A simplex $\\mathcal{S}$ in $\\mathbb{R}^n$ is a set $\\{x_0,x_1,\\ldots,x_n\\}$ of $n+1$ affinely independent points.\n- The centroid $c$ of all points except the worst (the vertex with the largest objective value) is defined as $c=\\frac{1}{n}\\sum_{i=0}^{n-1}x_i$ after ordering the vertices by their objective values, with $x_0$ the best and $x_n$ the worst.\n- The reflection point is $x_r=c+\\alpha(c-x_n)$ with reflection coefficient $\\alpha$.\n- The expansion point is $x_e=c+\\gamma(x_r-c)$ with expansion coefficient $\\gamma$.\n- The contraction point is either the outside contraction $x_{co}=c+\\rho(x_r-c)$ or the inside contraction $x_{ci}=c+\\rho(x_n-c)$ with contraction coefficient $\\rho$.\n- The shrink transformation replaces all non-best vertices by $x_i\\leftarrow x_0+\\sigma(x_i-x_0)$ with shrink coefficient $\\sigma$.\n\nStopping criteria to be compared:\n- Geometric criterion based on simplex diameter: the diameter $d(\\mathcal{S})$ is defined as the maximum Euclidean distance between any two vertices of the simplex,\n$$\nd(\\mathcal{S})=\\max_{0\\le i,j\\le n}\\left\\Vert x_i-x_j\\right\\Vert_2.\n$$\nStop when $d(\\mathcal{S}_k)\\le \\varepsilon_d$.\n- Functional criterion based on spread in objective values: the spread $s(\\mathcal{S})$ is defined as the difference between the largest and smallest objective values at the simplex vertices,\n$$\ns(\\mathcal{S})=\\max_i f(x_i)-\\min_i f(x_i).\n$$\nStop when $s(\\mathcal{S}_k)\\le \\varepsilon_f$.\n\nAlgorithmic parameters:\n- Use $\\alpha=1$, $\\gamma=2$, $\\rho=\\tfrac{1}{2}$, $\\sigma=\\tfrac{1}{2}$, unless otherwise specified.\n- The maximum number of iterations must be enforced to avoid infinite loops.\n\nYour program must implement Nelder–Mead from the above base definitions and run each test case twice: once with the geometric stopping criterion $d(\\mathcal{S}_k)\\le \\varepsilon_d$ and once with the functional stopping criterion $s(\\mathcal{S}_k)\\le \\varepsilon_f$. For each run, report the number of iterations, the final best objective value $\\min_i f(x_i)$, the final diameter $d(\\mathcal{S}_k)$, and the final spread $s(\\mathcal{S}_k)$.\n\nTest suite:\n- Case $1$ (flat plateau corner case):\n  - Dimension $n=2$.\n  - Objective $f(x)=\\left(10^{-12}\\right)\\left(x_1^2+x_2^2\\right)$.\n  - Initial simplex vertices $(10,10)$, $(10,11)$, $(11,10)$.\n  - Tolerances $\\varepsilon_d=10^{-3}$, $\\varepsilon_f=10^{-10}$.\n  - Maximum iterations $200$.\n- Case $2$ (steep gradient with small simplex, corner case):\n  - Dimension $n=2$.\n  - Objective $f(x)=\\exp\\left(50x_1\\right)+\\left(10^{-3}\\right)x_2^2$.\n  - Initial simplex vertices $(0.1,0)$, $(0.1001,0)$, $(0.1,0.0001)$.\n  - Tolerances $\\varepsilon_d=5\\times 10^{-4}$, $\\varepsilon_f=10^{-2}$.\n  - Maximum iterations $200$.\n- Case $3$ (happy path, curved valley):\n  - Dimension $n=2$.\n  - Objective $f(x)=\\left(1-x_1\\right)^2+100\\left(x_2-x_1^2\\right)^2$.\n  - Initial simplex vertices $(-1.2,1)$, $(-1.2,1.2)$, $(-1,1)$.\n  - Tolerances $\\varepsilon_d=10^{-3}$, $\\varepsilon_f=10^{-6}$.\n  - Maximum iterations $1000$.\n- Case $4$ (boundary equality on diameter):\n  - Dimension $n=2$.\n  - Objective $f(x)=x_1^2+x_2^2$.\n  - Initial simplex vertices $(0,0)$, $(1,0)$, $(0,1)$.\n  - Tolerances $\\varepsilon_d=\\sqrt{2}$, $\\varepsilon_f=0.5$.\n  - Maximum iterations $200$.\n\nRequired outputs per test case:\n- For each test case, output the list\n$$\n[\\;I_d,\\;I_f,\\;B,\\;F_d,\\;F_f,\\;D_d,\\;S_d,\\;D_f,\\;S_f\\;]\n$$\nwhere $I_d$ is the iteration count when stopping on diameter, $I_f$ is the iteration count when stopping on spread, $B$ is a boolean that is `True` if $I_d<I_f$ and `False` otherwise, $F_d$ and $F_f$ are the final best objective values under diameter and spread runs respectively, $D_d$ and $S_d$ are the final diameter and spread under the diameter run, and $D_f$ and $S_f$ are the final diameter and spread under the spread run.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result itself a bracketed comma-separated list, and without spaces. For example, `[[1,2,True,0.0,0.0,0.0,0.0,0.0,0.0],[...]]`", "solution": "The problem requires the implementation of the Nelder–Mead direct search optimization algorithm. The implementation must be based on the fundamental definitions provided and must support two distinct stopping criteria: one based on the geometric diameter of the simplex and the other on the functional spread of the objective values at the simplex vertices. The performance of these two criteria will be compared across a suite of four test cases, each designed to highlight different behaviors of the algorithm.\n\nThe core of the solution is a function that executes the Nelder–Mead method. The algorithm is iterative and proceeds as follows for an objective function $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n\nAn initial simplex $\\mathcal{S}_0 = \\{x_0, x_1, \\ldots, x_n\\}$ consisting of $n+1$ vertices in $\\mathbb{R}^n$ is provided. The algorithm then enters a loop, where at each iteration $k=0, 1, 2, \\ldots$:\n$1$. The $n+1$ vertices of the current simplex $\\mathcal{S}_k$ are evaluated and ordered such that their objective function values are sorted: $f(x_0) \\le f(x_1) \\le \\ldots \\le f(x_n)$. The vertices $x_0$ and $x_n$ are designated as the best and worst vertices, respectively.\n\n$2$. The stopping criteria are checked. Two separate runs are performed for each test case.\n   - For the geometric criterion run, the simplex diameter $d(\\mathcal{S}_k) = \\max_{0 \\le i,j \\le n} \\|x_i - x_j\\|_2$ is computed. If $d(\\mathcal{S}_k) \\le \\varepsilon_d$, the algorithm terminates, reporting $k$ as the number of iterations.\n   - For the functional criterion run, the spread of objective values $s(\\mathcal{S}_k) = f(x_n) - f(x_0)$ is computed. If $s(\\mathcal{S}_k) \\le \\varepsilon_f$, the algorithm terminates, reporting $k$ as the number of iterations.\n   - In both cases, termination also occurs if the iteration count $k$ reaches the specified maximum, $k_{\\max}$.\n\n$3$. If no stopping criterion is met, a new simplex $\\mathcal{S}_{k+1}$ is generated. This begins with the calculation of the centroid $c$ of the $n$ best vertices:\n$$\nc = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i\n$$\n\n$4$. A sequence of transformations is attempted to replace the worst vertex $x_n$ with a better point. The standard parameters for these transformations are: reflection coefficient $\\alpha=1$, expansion coefficient $\\gamma=2$, contraction coefficient $\\rho=\\frac{1}{2}$, and shrink coefficient $\\sigma=\\frac{1}{2}$.\n   - **Reflection**: A reflected point $x_r = c + \\alpha(c - x_n)$ is generated. Let its function value be $f_r = f(x_r)$.\n   - **Expansion**: If $f_r$ is better than the best current value, i.e., $f_r < f(x_0)$, the algorithm explores further in this direction by computing an expansion point $x_e = c + \\gamma(x_r - c)$. If $f(x_e) < f_r$, the new vertex is $x_e$; otherwise, the new vertex is $x_r$. This new point replaces $x_n$.\n   - **Acceptance**: If the reflected point is not a new best, but is better than the second-worst vertex, i.e., $f(x_0) \\le f_r < f(x_{n-1})$, then $x_r$ is accepted and replaces $x_n$.\n   - **Contraction**: If the reflected point is not an improvement, $f_r \\ge f(x_{n-1})$, a contraction is performed.\n     - If $f_r < f(x_n)$, an *outside contraction* is performed to compute $x_{co} = c + \\rho(x_r - c)$. If $f(x_{co}) \\le f_r$, $x_{co}$ replaces $x_n$. Otherwise, a shrink is performed.\n     - If $f_r \\ge f(x_n)$, an *inside contraction* is performed to compute $x_{ci} = c + \\rho(x_n - c)$. If $f(x_{ci}) < f(x_n)$, $x_{ci}$ replaces $x_n$. Otherwise, a shrink is performed.\n   - **Shrink**: If the contraction step fails to yield an improvement, the entire simplex is shrunk towards the best vertex $x_0$. All vertices $x_i$ for $i \\in \\{1, 2, \\ldots, n\\}$ are replaced according to the rule $x_i \\leftarrow x_0 + \\sigma(x_i - x_0)$.\n\nThis process generates a new simplex $\\mathcal{S}_{k+1}$ and the next iteration begins.\n\nThe test cases are designed to probe the relative effectiveness of the stopping criteria.\n- **Case 1**: A nearly flat quadratic function. The functional spread $s(\\mathcal{S})$ can become very small quickly, even if the simplex vertices are far apart, potentially causing premature termination under the functional criterion.\n- **Case 2**: A function with a very steep exponential component. The simplex can become geometrically small ($d(\\mathcal{S})$ is small) in a region where the function value changes dramatically, potentially causing premature termination under the geometric criterion.\n- **Case 3**: The Rosenbrock function, a standard benchmark with a narrow, curved valley. This serves as a general test of the algorithm's robustness.\n- **Case 4**: A simple quadratic where the initial simplex diameter is exactly equal to the tolerance $\\varepsilon_d$, testing the handling of a boundary condition at iteration $k=0$.\n\nThe program will execute this logic, run each test case twice (once for each stopping criterion), and collate the results—iteration count ($I_d, I_f$), a boolean comparison ($B$), final best objective values ($F_d, F_f$), and final simplex metrics ($D_d, S_d, D_f, S_f$)—into the specified list format for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef nelder_mead(func, initial_simplex, tol_d, tol_f, max_iter, stop_criterion):\n    \"\"\"\n    Implements the Nelder-Mead optimization algorithm.\n\n    Args:\n        func: The objective function to minimize.\n        initial_simplex: The starting simplex as a numpy array of shape (n+1, n).\n        tol_d: Tolerance for the geometric (diameter) stopping criterion.\n        tol_f: Tolerance for the functional (spread) stopping criterion.\n        max_iter: The maximum number of iterations.\n        stop_criterion: A string, either 'diameter' or 'spread'.\n\n    Returns:\n        A tuple containing:\n        - k (int): Number of iterations.\n        - best_val (float): The best objective function value found.\n        - diameter (float): The final diameter of the simplex.\n        - spread (float): The final spread of the objective values.\n    \"\"\"\n    # Standard Nelder-Mead parameters\n    alpha, gamma, rho, sigma = 1.0, 2.0, 0.5, 0.5\n    \n    simplex = np.copy(initial_simplex)\n    n = simplex.shape[1]  # Dimension of the problem\n\n    for k in range(max_iter + 1):\n        # 1. Evaluate and order vertices\n        f_values = np.array([func(x) for x in simplex])\n        sorted_indices = np.argsort(f_values)\n        simplex = simplex[sorted_indices]\n        f_values = f_values[sorted_indices]\n\n        # 2. Calculate stopping metrics for the current simplex\n        # Simplex diameter\n        diameter = 0.0\n        for i in range(n + 1):\n            for j in range(i + 1, n + 1):\n                dist = np.linalg.norm(simplex[i] - simplex[j])\n                if dist > diameter:\n                    diameter = dist\n        \n        # Function value spread\n        spread = f_values[-1] - f_values[0]\n\n        # 3. Check for termination\n        if (stop_criterion == 'diameter' and diameter <= tol_d) or \\\n           (stop_criterion == 'spread' and spread <= tol_f) or \\\n           k == max_iter:\n            return k, f_values[0], diameter, spread\n\n        # Get best, worst, and second-worst points and values\n        x0, xn, xn_1 = simplex[0], simplex[-1], simplex[-2]\n        f0, fn, fn_1 = f_values[0], f_values[-1], f_values[-2]\n\n        # 4. Centroid\n        centroid = np.mean(simplex[:-1], axis=0)\n\n        # 5. Reflection\n        xr = centroid + alpha * (centroid - xn)\n        fr = func(xr)\n\n        new_simplex = np.copy(simplex)\n        shrink = False\n\n        if f0 <= fr < fn_1:\n            # Accept reflected point\n            new_simplex[-1] = xr\n        elif fr < f0:\n            # Expansion\n            xe = centroid + gamma * (xr - centroid)\n            fe = func(xe)\n            if fe < fr:\n                new_simplex[-1] = xe\n            else:\n                new_simplex[-1] = xr\n        else: # fr >= fn_1, needs contraction or shrink\n            if fr < fn:\n                # Outside Contraction\n                xco = centroid + rho * (xr - centroid)\n                fco = func(xco)\n                if fco <= fr:\n                    new_simplex[-1] = xco\n                else:\n                    shrink = True\n            else: # fr >= fn\n                # Inside Contraction\n                xci = centroid + rho * (xn - centroid)\n                fci = func(xci)\n                if fci < fn:\n                    new_simplex[-1] = xci\n                else:\n                    shrink = True\n        \n        if shrink:\n            # Shrink transformation\n            for i in range(1, n + 1):\n                new_simplex[i] = x0 + sigma * (simplex[i] - x0)\n        \n        simplex = new_simplex\n\n    # This part should not be reached due to loop range and k == max_iter check\n    return max_iter, f_values[0], diameter, spread\n\ndef solve():\n    # Define objective functions\n    def f1(x): return 1e-12 * (x[0]**2 + x[1]**2)\n    def f2(x): return np.exp(50 * x[0]) + 1e-3 * x[1]**2\n    def f3(x): return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n    def f4(x): return x[0]**2 + x[1]**2\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"func\": f1,\n            \"initial_simplex\": np.array([[10, 10], [10, 11], [11, 10]], dtype=float),\n            \"tol_d\": 1e-3, \"tol_f\": 1e-10, \"max_iter\": 200\n        },\n        {\n            \"func\": f2,\n            \"initial_simplex\": np.array([[0.1, 0], [0.1001, 0], [0.1, 0.0001]], dtype=float),\n            \"tol_d\": 5e-4, \"tol_f\": 1e-2, \"max_iter\": 200\n        },\n        {\n            \"func\": f3,\n            \"initial_simplex\": np.array([[-1.2, 1], [-1.2, 1.2], [-1, 1]], dtype=float),\n            \"tol_d\": 1e-3, \"tol_f\": 1e-6, \"max_iter\": 1000\n        },\n        {\n            \"func\": f4,\n            \"initial_simplex\": np.array([[0, 0], [1, 0], [0, 1]], dtype=float),\n            \"tol_d\": np.sqrt(2), \"tol_f\": 0.5, \"max_iter\": 200\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        # Run with diameter stopping criterion\n        Id, Fd, Dd, Sd = nelder_mead(\n            case[\"func\"], case[\"initial_simplex\"],\n            case[\"tol_d\"], case[\"tol_f\"], case[\"max_iter\"], 'diameter'\n        )\n\n        # Run with function spread stopping criterion\n        If, Ff, Df, Sf = nelder_mead(\n            case[\"func\"], case[\"initial_simplex\"],\n            case[\"tol_d\"], case[\"tol_f\"], case[\"max_iter\"], 'spread'\n        )\n\n        B = Id < If\n        \n        # Assemble results for the current case\n        case_results = [Id, If, B, Fd, Ff, Dd, Sd, Df, Sf]\n        \n        # Format the list into a string representation without spaces\n        case_str = f\"[{','.join(map(str, case_results))}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3187957"}, {"introduction": "After deciding which criteria to use for stopping, we face a critical question: what numerical values should we choose for our tolerances? A fixed tolerance like $10^{-6}$ might be too strict for a problem involving large numbers or too loose for one at a small scale. This practice introduces a principled approach to this problem, guiding you to derive an adaptive tolerance grounded in the fundamental limits of floating-point arithmetic [@problem_id:3187861]. By scaling the tolerance with the magnitude of the current iterate and the machine epsilon, $\\epsilon_{\\text{mach}}$, you will create a stopping rule that is automatically robust across different scales.", "problem": "You are tasked with designing and implementing an adaptive stopping criterion for solving scalar nonlinear equations using Newton’s method. The stopping tolerance must be justified from a backward error perspective under floating-point rounding and must automatically scale with the magnitude of the current iterate and the residual. The objective is to show, by derivation and by computational tests, why such a tolerance avoids erroneous early termination and remains effective across different problem scales.\n\nBegin from the following fundamental base:\n- In Institute of Electrical and Electronics Engineers (IEEE) 754 double-precision floating-point arithmetic, each elementary arithmetic operation on real numbers satisfies the standard rounding model: if an exact real operation would produce $z$, the computed result is $\\widehat{z} = z(1 + \\delta)$ with $|\\delta| \\le \\epsilon_{\\text{mach}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon.\n- Newton’s method for a differentiable scalar function $f:\\mathbb{R}\\to\\mathbb{R}$ with derivative $f'$ updates $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$.\n- A backward error stopping rule seeks $x_k$ such that the computed residual $|f(x_k)|$ is consistent with a small perturbation of the problem data that can be attributed to floating-point rounding, and the iterate change $|x_{k+1}-x_k|$ is commensurately small.\n\nTasks:\n1. Derive an automatic tolerance $\\varepsilon_k$ for termination based on backward error analysis using the floating-point model above. Your derivation must produce a leading-order bound that scales like $\\sqrt{\\epsilon_{\\text{mach}}}$ times a dimensionless measure constructed from the magnitudes of the current iterate $x_k$ and residual $f(x_k)$. The tolerance must be applicable to both residual and step-size stopping checks.\n2. Implement a Newton solver that, at iteration $k$, computes a tolerance $\\varepsilon_k$ from your derivation, and terminates when either $|f(x_k)| \\le \\varepsilon_k$ or $|x_{k+1}-x_k| \\le \\varepsilon_k$. Use a maximum iteration cap to avoid infinite loops.\n3. Validate that your tolerance behaves robustly across scales by applying your solver to the following test suite of scalar problems, each specified by $f(x)$, $f'(x)$, and the initial guess $x_0$:\n   - Case 1 (baseline linear): $f(x) = x - 1$, $f'(x) = 1$, $x_0 = 2$.\n   - Case 2 (large scaling): $f(x) = 10^{8}(x - 1)$, $f'(x) = 10^{8}$, $x_0 = 2$.\n   - Case 3 (small scaling): $f(x) = 10^{-8}(x - 1)$, $f'(x) = 10^{-8}$, $x_0 = 2$.\n   - Case 4 (oscillatory): $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = 3$.\n   - Case 5 (exponential): $f(x) = e^{x} - 1$, $f'(x) = e^{x}$, $x_0 = -0.5$.\n   - Case 6 (initially at the root): $f(x) = x - 1$, $f'(x) = 1$, $x_0 = 1$.\n\nFor each case, after termination, report the dimensionless backward-error ratio\n$$\nr_k \\equiv \\frac{|f(x_k)|}{\\,1 + |x_k| + |f(x_k)|\\,}.\n$$\nThis ratio should be at most on the order of $\\sqrt{\\epsilon_{\\text{mach}}}$ when the residual-based stopping condition triggers, and not strongly dependent on the scaling of $f$.\n\nImplementation requirements:\n- Use Newton’s method with a maximum of $50$ iterations.\n- Use IEEE 754 double precision as provided by the runtime.\n- Compute $\\epsilon_{\\text{mach}}$ from the runtime and do not hard-code any numeric constant for it.\n- No physical units are involved.\n- Angles are in radians.\n- Output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[r_1,r_2,...,r_6]`), where each $r_i$ is the ratio defined above for the corresponding test case.", "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of numerical analysis and floating-point arithmetic, is well-posed with a clear objective and constraints, and is expressed in objective language. All necessary data for the test cases are provided, and the task is to derive and implement a standard numerical method with a specific type of stopping criterion.\n\n### Derivation of the Adaptive Stopping Criterion\n\nThe objective is to derive a stopping tolerance $\\varepsilon_k$ for Newton's method, $x_{k+1} = x_k - f(x_k)/f'(x_k)$, that is robust across different problem scales. The tolerance should be based on backward error principles and scale with $\\sqrt{\\epsilon_{\\text{mach}}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon.\n\nA numerical algorithm should terminate when the computed iterates are \"close enough\" to the true solution $x^*$. The definition of \"close enough\" is subtle in floating-point arithmetic. An overly strict tolerance might never be met due to rounding errors, while an overly loose one may cause premature termination.\n\nA robust stopping criterion typically combines an absolute tolerance and a relative tolerance. For the change in the iterate (the step), such a criterion takes an absolute component for roots near zero and a relative component for large roots. This can be expressed as:\n$$\n|x_{k+1} - x_k| \\le \\tau_{abs} + \\tau_{rel} |x_k|\n$$\nThis can be compactly written as $|x_{k+1} - x_k| \\le \\tau (1 + |x_k|)$, where the base tolerance $\\tau$ is used for both the absolute and relative parts.\n\nThe choice of the base tolerance $\\tau$ is critical. A tolerance on the order of $\\epsilon_{\\text{mach}}$ aims for the highest possible precision, but can be unstable. In many numerical contexts, convergence slows or becomes erratic as it approaches the limits of machine precision. The quadratic convergence of Newton's method, where the error $e_{k+1} \\approx C e_k^2$, means that once the error is small, it decreases very rapidly. A tolerance of $\\tau = \\sqrt{\\epsilon_{\\text{mach}}}$ is a standard, practical choice that represents a \"reasonably tight\" condition. It signals that the iterate has very likely entered the region of quadratic convergence and further iterations will yield diminishing returns, while being loose enough to avoid issues with floating-point noise far from the true precision limit.\n\nAdopting this principle, we set the base tolerance to $\\tau = \\sqrt{\\epsilon_{\\text{mach}}}$. The step-size stopping criterion is therefore:\n$$\n|x_{k+1} - x_k| \\le \\sqrt{\\epsilon_{\\text{mach}}} (1 + |x_k|)\n$$\n\nThe problem requires a single tolerance formulation, $\\varepsilon_k$, to be applied to both the step-size and the residual. For a well-behaved problem near a simple root $x^*$, where the derivative $f'(x^*)$ is of order $1$, the magnitude of the residual $|f(x_k)|$ is comparable to the magnitude of the error $|x_k - x^*|$, which in turn is approximated by the step size $|x_{k+1}-x_k|$. That is, $|f(x_k)| \\approx |f'(x^*)(x_k-x^*)| \\approx |x_{k+1}-x_k|$. This heuristic suggests that it is reasonable to apply the same tolerance value to both the residual and the step size.\n\nThis leads to the definition of a single adaptive tolerance $\\varepsilon_k$ at each iteration $k$:\n$$\n\\varepsilon_k = \\sqrt{\\epsilon_{\\text{mach}}} (1 + |x_k|)\n$$\nThe Newton iteration then terminates if either of the following conditions is met:\n1.  Residual check: $|f(x_k)| \\le \\varepsilon_k$\n2.  Step-size check: $|x_{k+1} - x_k| \\le \\varepsilon_k$\n\nThis formulation satisfies the problem's requirements. The tolerance $\\varepsilon_k$ is computed at iteration $k$. Its scaling with $(1 + |x_k|)$ ensures it automatically adjusts to the magnitude of the current iterate, making it robust for roots at both small and large scales. Its dependence on $\\sqrt{\\epsilon_{\\text{mach}}}$ reflects a practical compromise for robust convergence. Although the problem mentions a \" dimensionless measure constructed from the magnitudes of the current iterate $x_k$ and residual $f(x_k)$\", which is a slightly ambiguous phrasing, our derived tolerance $\\varepsilon_k$ is based on $(1+|x_k|)$. This can be interpreted as a scaling factor, and given the problem statement that no physical units are involved, it robustly handles the scaling of the iterate's magnitude. The term \"dimensionless measure\" can be interpreted as the factor that makes the tolerance scale-invariant, which our use of $(1+|x_k|)$ accomplishes.\n\n### Algorithmic Implementation\n\nThe solver is implemented as follows:\n1.  Compute the machine epsilon $\\epsilon_{\\text{mach}}$ and the base tolerance $\\tau = \\sqrt{\\epsilon_{\\text{mach}}}$.\n2.  Initialize the iterate $x_k$ with the starting guess $x_0$.\n3.  Loop for a maximum of $50$ iterations:\n    a. At the current iterate $x_k$, compute the residual $f_k = f(x_k)$.\n    b. Compute the adaptive tolerance $\\varepsilon_k = \\tau (1 + |x_k|)$.\n    c. Perform the residual check: if $|f_k| \\le \\varepsilon_k$, the algorithm has converged. Terminate the loop.\n    d. Compute the derivative $f'_k = f'(x_k)$. If $f'_k$ is zero, terminate to prevent division by zero.\n    e. Calculate the Newton step: $\\Delta x_k = f_k / f'_k$.\n    f. Perform the step-size check: if $|\\Delta x_k| \\le \\varepsilon_k$, the step is negligibly small. Update the iterate one last time to $x_{k+1} = x_k - \\Delta x_k$ and terminate.\n    g. Update the iterate: $x_k \\leftarrow x_k - \\Delta x_k$.\n4.  After termination, compute the final dimensionless backward-error ratio $r_k = \\frac{|f(x_k)|}{1 + |x_k| + |f(x_k)|}$ using the final iterate $x_k$.\n\nThis procedure is applied to each of the six test cases specified in the problem statement. The resulting values of $r_k$ are collected and printed. The results are expected to show that $r_k$ is consistently on the order of $\\sqrt{\\epsilon_{\\text{mach}}}$ or smaller, demonstrating the robustness of the stopping criterion across various function scales and behaviors.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and validates an adaptive stopping criterion for Newton's method.\n    The criterion is based on backward error analysis principles adapted for robustness\n    in floating-point arithmetic.\n    \"\"\"\n\n    # Define the test suite of scalar nonlinear problems.\n    # Each problem is a dictionary with the function, its derivative, and an initial guess.\n    test_cases = [\n        {'f': lambda x: x - 1.0, 'fp': lambda x: 1.0, 'x0': 2.0},\n        {'f': lambda x: 1e8 * (x - 1.0), 'fp': lambda x: 1e8, 'x0': 2.0},\n        {'f': lambda x: 1e-8 * (x - 1.0), 'fp': lambda x: 1e-8, 'x0': 2.0},\n        {'f': lambda x: np.sin(x), 'fp': lambda x: np.cos(x), 'x0': 3.0},\n        {'f': lambda x: np.exp(x) - 1.0, 'fp': lambda x: np.exp(x), 'x0': -0.5},\n        {'f': lambda x: x - 1.0, 'fp': lambda x: 1.0, 'x0': 1.0}\n    ]\n\n    # Compute machine epsilon for IEEE 754 double precision from the runtime.\n    eps_mach = np.finfo(float).eps\n    # The base tolerance level is set to the square root of machine epsilon.\n    sqrt_eps_mach = np.sqrt(eps_mach)\n    \n    max_iter = 50\n    results = []\n\n    for case in test_cases:\n        f = case['f']\n        fp = case['fp']\n        x_k = float(case['x0'])\n        \n        # Main Newton's method iteration loop.\n        for _ in range(max_iter):\n            # Evaluate the function at the current iterate.\n            f_k = f(x_k)\n            \n            # Compute the adaptive tolerance epsilon_k for the current iterate x_k.\n            # This tolerance uses a mixed absolute/relative scaling factor (1 + |x_k|).\n            tol_k = sqrt_eps_mach * (1.0 + np.abs(x_k))\n            \n            # 1. Residual-based stopping criterion.\n            # Terminate if the residual is smaller than the computed tolerance.\n            if np.abs(f_k) <= tol_k:\n                break\n            \n            # Evaluate the derivative.\n            fp_k = fp(x_k)\n            \n            # Terminate if the derivative is zero to prevent division by zero,\n            # unless the residual condition was already met.\n            if fp_k == 0.0:\n                break\n            \n            # Calculate the Newton step.\n            step = f_k / fp_k\n            \n            # 2. Step-size-based stopping criterion.\n            # Terminate if the step size is smaller than the computed tolerance.\n            # In this case, we complete the final step before exiting.\n            if np.abs(step) <= tol_k:\n                x_k = x_k - step\n                break\n            \n            # Update the iterate for the next step.\n            x_k = x_k - step\n        \n        # After loop termination (by convergence or max_iter), calculate the final measures.\n        # Recalculate the residual at the final iterate.\n        final_f_k = f(x_k)\n        \n        # Compute the dimensionless backward-error ratio r_k as specified.\n        r_k = np.abs(final_f_k) / (1.0 + np.abs(x_k) + np.abs(final_f_k))\n        \n        results.append(str(r_k))\n\n    # Print the results in the specified single-line format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3187861"}]}