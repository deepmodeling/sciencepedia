{"hands_on_practices": [{"introduction": "To truly master the Nonlinear Conjugate Gradient (NCG) method, we must move from theory to implementation. This first practice challenges you to build a complete NCG solver from the ground up, applying it to a suite of classic benchmark problems that test its performance in various scenarios, such as navigating the narrow valley of the Rosenbrock function. By successfully implementing the core loop involving gradient calculations, search direction updates, and a proper line search, you will build a foundational tool and gain practical insight into the mechanics of first-order optimization [@problem_id:2418452].", "problem": "You are given several differentiable objective functions on Euclidean spaces, along with initial points. Your task is to write a complete program that, for each case, computes an approximate minimizer using only function values and exact first derivatives. The gradient that your program uses must be exact within machine precision for the specified objectives; do not use any finite-difference approximations. Do not use any information beyond first derivatives. Use a termination criterion based on the Euclidean norm of the gradient.\n\nMathematical setup:\n\n- Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be a continuously differentiable function, and let $\\nabla f(\\mathbf{x})$ denote its gradient. Starting from a given initial point $\\mathbf{x}_0 \\in \\mathbb{R}^n$, compute a sequence $\\{\\mathbf{x}_k\\}$ that attempts to minimize $f$ using only evaluations of $f$ and $\\nabla f$.\n- Terminate when $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ or when the number of iterations reaches a specified maximum. Use the Euclidean norm for $\\|\\cdot\\|_2$.\n- The algorithm must not use any second-order information (no Hessians or Hessian-vector products) and must not use finite-difference derivative approximations. The program must compute exact gradients of the specified functions within machine precision.\n\nTest suite:\n\nFor each case below, you are given the function $f(\\mathbf{x})$, the dimension $n$, and the initial point $\\mathbf{x}_0$.\n\n- Case A (nonconvex, narrow valley, two variables):\n  - Dimension: $n=2$.\n  - Objective:\n    $$f(\\mathbf{x}) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2.$$\n  - Initial point: $\\mathbf{x}_0 = (-1.2,\\; 1.0)$.\n\n- Case B (ill-conditioned separable quadratic, five variables):\n  - Dimension: $n=5$.\n  - Objective:\n    $$f(\\mathbf{x}) = \\tfrac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2,\\quad \\lambda_i = 10^{\\,i-1}.$$\n  - Initial point: $\\mathbf{x}_0 = (1,\\; -1,\\; 1,\\; -1,\\; 1)$.\n\n- Case C (smooth, coupled, nonconvex, three variables):\n  - Dimension: $n=3$.\n  - Objective:\n    $$f(\\mathbf{x}) = 0.1\\,(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3.$$\n  - Initial point: $\\mathbf{x}_0 = (0.5,\\; -0.5,\\; 0.0)$.\n\n- Case D (already at the minimizer, four variables):\n  - Dimension: $n=4$.\n  - Objective:\n    $$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2.$$\n  - Initial point: $\\mathbf{x}_0 = (1,\\; 1,\\; 1,\\; 1).$\n\nRequirements:\n\n- Stopping tolerance: use $\\varepsilon = 10^{-6}$ on the gradient norm.\n- Maximum number of iterations: use $N_{\\max} = 10000$ per case.\n- The gradient used by your program must be exact within machine precision for the specified objectives.\n- The program must not read any input and must not write any output other than the final line described below.\n\nOutput specification:\n\n- For each case, report the final objective value $f(\\mathbf{x}_\\star)$ at termination, rounded to exactly six digits after the decimal point.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the four rounded objective values for Case A, Case B, Case C, and Case D in that order. For example, a valid output format is:\n  - \"[0.000123,0.000000,1.234567,0.000000]\"\n\nNo physical units are involved. Angles, where used in trigonometric functions, are in radians by mathematical convention. The outputs must be real numbers and must follow the exact format described above.", "solution": "The problem posed is a standard numerical optimization task, requiring the minimization of several well-defined, differentiable functions. The method is constrained to be of first-order, meaning it may only use function values, $f(\\mathbf{x})$, and gradient values, $\\nabla f(\\mathbf{x})$. Second-order information, such as the Hessian matrix, is forbidden. Given these constraints, a highly suitable and efficient algorithm is the Nonlinear Conjugate Gradient (CG) method. While the simpler method of steepest descent also satisfies the constraints, its convergence rate is notoriously poor for problems with high curvature or ill-conditioning, such as the Rosenbrock function (Case A) and the given quadratic function with disparate eigenvalues (Case B). The CG method accelerates convergence by constructing search directions that are a conjugate-like extension of the gradients, effectively incorporating information from previous steps.\n\nThe iterative procedure for the Nonlinear CG algorithm, starting from an initial point $\\mathbf{x}_0$, is defined for $k=0, 1, 2, \\dots$ as follows:\n1. Compute the gradient at the current iterate: $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$.\n2. Check for convergence: if the Euclidean norm of the gradient $\\|\\mathbf{g}_k\\|_2$ is below a specified tolerance $\\varepsilon$, the algorithm terminates.\n3. Compute the search direction $\\mathbf{p}_k$. For the first iteration ($k=0$), the direction is the steepest descent direction, $\\mathbf{p}_0 = -\\mathbf{g}_0$. For subsequent iterations ($k > 0$), the direction is a linear combination of the current negative gradient and the previous search direction:\n   $$\n   \\mathbf{p}_k = -\\mathbf{g}_k + \\beta_k \\mathbf{p}_{k-1}\n   $$\n   The scalar $\\beta_k$ determines the specific variant of the CG method.\n4. Perform a line search to determine an appropriate step size $\\alpha_k > 0$ along the direction $\\mathbf{p}_k$.\n5. Update the iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n\nFor this implementation, the Polak–Ribière–Polyak (PRP) formula is selected for $\\beta_k$ due to its generally superior empirical performance compared to other formulas like Fletcher-Reeves. The PRP formula is:\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\\mathbf{g}_k^T (\\mathbf{g}_k - \\mathbf{g}_{k-1})}{\\mathbf{g}_{k-1}^T \\mathbf{g}_{k-1}}\n$$\nTo improve the robustness and guarantee global convergence properties, this is augmented into the PRP+ method, where $\\beta_k = \\max(0, \\beta_k^{\\text{PRP}})$. This modification prevents the algorithm from taking poor steps if $\\beta_k^{\\text{PRP}}$ becomes negative, which can occur far from a local minimum. Additionally, as a safeguard, the search direction $\\mathbf{p}_k$ is forcibly reset to the steepest descent direction $-\\mathbf{g}_k$ if it ceases to be a descent direction (i.e., if $\\mathbf{p}_k^T \\mathbf{g}_k \\ge 0$).\n\nThe step size $\\alpha_k$ is found using a line search that satisfies the strong Wolfe conditions:\n1. Sufficient Decrease (Armijo) Condition: $f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\mathbf{g}_k^T \\mathbf{p}_k$\n2. Curvature Condition: $|\\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)^T \\mathbf{p}_k| \\le c_2 |\\mathbf{g}_k^T \\mathbf{p}_k|$\nThe constants are chosen as standard values $c_1 = 10^{-4}$ and $c_2 = 0.9$. These conditions ensure that each step achieves a meaningful reduction in the objective function value. The `scipy.optimize.line_search` function is used to implement this step.\n\nThe analytical gradients are required to be exact. The gradients for the four test cases are derived as follows:\n\nCase A: Rosenbrock function, $f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$ for $\\mathbf{x} \\in \\mathbb{R}^2$.\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\nCase B: Ill-conditioned separable quadratic, $f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2$ with $\\lambda_i = 10^{i-1}$ for $\\mathbf{x} \\in \\mathbb{R}^5$.\nThe gradient component for each $x_j$ is $\\frac{\\partial f}{\\partial x_j} = \\lambda_j x_j$.\n$$\n\\nabla f(\\mathbf{x})_j = 10^{j-1} x_j, \\quad \\text{for } j=1, \\dots, 5\n$$\n\nCase C: Smooth, coupled, nonconvex function, $f(\\mathbf{x}) = 0.1(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3$ for $\\mathbf{x} \\in \\mathbb{R}^3$.\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 0.2x_1 + \\cos(x_1)\\cos(x_2) \\\\ 0.2x_2 - \\sin(x_1)\\sin(x_2) \\\\ 0.2x_3 + e^{x_3} - 1 \\end{pmatrix}\n$$\n\nCase D: Simple quadratic, $f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2$ for $\\mathbf{x} \\in \\mathbb{R}^4$.\nThe gradient component for each $x_j$ is $\\frac{\\partial f}{\\partial x_j} = 2(x_j-1)$.\n$$\n\\nabla f(\\mathbf{x})_j = 2(x_j - 1), \\quad \\text{for } j=1, \\dots, 4\n$$\nFor this case, the initial point $\\mathbf{x}_0 = (1, 1, 1, 1)$ is the function's unique global minimum. Therefore, $\\nabla f(\\mathbf{x}_0) = \\mathbf{0}$, and the algorithm terminates immediately at iteration $k=0$ with an objective value of $f(\\mathbf{x}_0)=0$.\n\nThe implementation combines these elements into a single program. A general solver function encapsulates the CG logic, and it is called for each test case with the respective objective function, gradient, and initial point. The program terminates when $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ or after $10000$ iterations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef conjugate_gradient_solver(f, grad_f, x0, tol=1e-6, max_iter=10000):\n    \"\"\"\n    Minimizes a function using the Nonlinear Conjugate Gradient method (Polak-Ribière-Polyak+).\n    \"\"\"\n    x_k = np.array(x0, dtype=float)\n    f_k = f(x_k)\n    g_k = grad_f(x_k)\n    grad_norm = np.linalg.norm(g_k)\n\n    if grad_norm <= tol:\n        return f(x_k)\n\n    p_k = -g_k\n    \n    k = 0\n    while k < max_iter and grad_norm > tol:\n        # Perform line search to find alpha_k satisfying strong Wolfe conditions.\n        # c1=1e-4 and c2=0.9 are standard for CG.\n        try:\n            line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n            alpha_k = line_search_result[0]\n        except Exception:\n            # line_search can sometimes raise errors for extreme values\n            alpha_k = None\n\n        # If line search fails, restart with steepest descent.\n        if alpha_k is None:\n            p_k = -g_k\n            try:\n                line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n                alpha_k = line_search_result[0]\n            except Exception:\n                alpha_k = None\n            \n            if alpha_k is None:\n                # If it still fails, terminate. Could be due to precision limits.\n                break\n\n        x_k_plus_1 = x_k + alpha_k * p_k\n        g_k_plus_1 = grad_f(x_k_plus_1)\n\n        # Polak-Ribière-Polyak+ update for beta\n        g_k_dot_g_k = np.dot(g_k, g_k)\n        if g_k_dot_g_k == 0:\n            beta_k_plus_1 = 0.0\n        else:\n            beta_numerator = np.dot(g_k_plus_1, g_k_plus_1 - g_k)\n            beta_k_plus_1 = max(0, beta_numerator / g_k_dot_g_k)\n        \n        # New search direction\n        p_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * p_k\n\n        # Check for descent direction. If not, reset to steepest descent.\n        if np.dot(p_k_plus_1, g_k_plus_1) >= 0:\n            p_k_plus_1 = -g_k_plus_1\n\n        # Update variables for the next iteration\n        x_k = x_k_plus_1\n        g_k = g_k_plus_1\n        p_k = p_k_plus_1\n        f_k = f(x_k) # Can be taken from line_search output, but re-evaluating is simple.\n        \n        grad_norm = np.linalg.norm(g_k)\n        k += 1\n\n    return f(x_k)\n\ndef solve():\n    # Final print statement in the exact required format.\n    \n    # Case A: Rosenbrock function\n    def f_A(x):\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f_A(x):\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    # Case B: Ill-conditioned separable quadratic\n    def f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return 0.5 * np.sum(lambdas * x**2)\n\n    def grad_f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return lambdas * x\n\n    # Case C: Smooth, coupled, nonconvex\n    def f_C(x):\n        term1 = 0.1 * np.sum(x**2)\n        term2 = np.sin(x[0]) * np.cos(x[1])\n        term3 = np.exp(x[2]) - x[2]\n        return term1 + term2 + term3\n\n    def grad_f_C(x):\n        df_dx1 = 0.2 * x[0] + np.cos(x[0]) * np.cos(x[1])\n        df_dx2 = 0.2 * x[1] - np.sin(x[0]) * np.sin(x[1])\n        df_dx3 = 0.2 * x[2] + np.exp(x[2]) - 1.0\n        return np.array([df_dx1, df_dx2, df_dx3])\n    \n    # Case D: Simple quadratic\n    def f_D(x):\n        return np.sum((x - 1.0)**2)\n\n    def grad_f_D(x):\n        return 2.0 * (x - 1.0)\n    \n    test_cases = [\n        {'f': f_A, 'grad_f': grad_f_A, 'x0': [-1.2, 1.0]},\n        {'f': f_B, 'grad_f': grad_f_B, 'x0': [1.0, -1.0, 1.0, -1.0, 1.0]},\n        {'f': f_C, 'grad_f': grad_f_C, 'x0': [0.5, -0.5, 0.0]},\n        {'f': f_D, 'grad_f': grad_f_D, 'x0': [1.0, 1.0, 1.0, 1.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_f_val = conjugate_gradient_solver(\n            f=case['f'],\n            grad_f=case['grad_f'],\n            x0=case['x0'],\n            tol=1e-6,\n            max_iter=10000\n        )\n        results.append(final_f_val)\n\n    # Format output as specified\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2418452"}, {"introduction": "A robust algorithm is often defined by its safeguards, and understanding why they exist is as important as knowing how to implement them. This exercise provides a vivid demonstration of this principle by pitting a correctly implemented NCG solver against an intentionally \"faulty\" version that omits the crucial Armijo sufficient decrease condition. By observing where the naive, fixed-step approach fails, you will develop a deeper appreciation for why line search conditions are essential for guaranteeing convergence and making optimization algorithms reliable in practice [@problem_id:2418455].", "problem": "You are asked to implement and compare two variants of the nonlinear conjugate gradient method for unconstrained minimization of a continuously differentiable function. The first variant is a proper nonlinear conjugate gradient algorithm that enforces the first Wolfe condition (Armijo sufficient decrease) via backtracking line search. The second variant is intentionally faulty: it does not enforce the Armijo condition and instead uses a fixed unit step length at every iteration. Your task is to demonstrate, through carefully chosen test functions, that omitting the Armijo condition can lead to nonconvergence or divergence, even when a proper method converges.\n\nStart from the following foundational base:\n- For an objective function $f:\\mathbb{R}^n \\to \\mathbb{R}$ that is continuously differentiable, a descent method generates iterates $x_{k+1} = x_k + \\alpha_k d_k$ with search direction $d_k$ satisfying $g_k^\\top d_k < 0$, where $g_k = \\nabla f(x_k)$ and $\\alpha_k > 0$ is a step length.\n- The first Wolfe (Armijo) sufficient decrease condition requires that, for constants $c_1 \\in (0,1)$, the step length satisfies\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k.\n$$\n- In a nonlinear conjugate gradient method, the search directions are constructed by\n$$\nd_0 = -g_0,\\quad d_k = -g_k + \\beta_k d_{k-1}\\ \\text{for}\\ k \\ge 1,\n$$\nwith a classical choice of $\\beta_k$ such as Polak–Ribiere–Plus, and with a safeguard to reset $d_k=-g_k$ if $g_k^\\top d_k \\ge 0$ to maintain descent.\n- For smooth convex quadratic objectives of the form $f(x) = \\tfrac{1}{2} x^\\top Q x$ with a symmetric positive-definite matrix $Q$, gradient descent with a fixed step $\\alpha$ yields the linear iteration $x_{k+1} = (I - \\alpha Q) x_k$. Convergence to the minimizer $x^\\star=0$ occurs if and only if the spectral radius satisfies $\\rho(I - \\alpha Q) < 1$, equivalently $0 < \\alpha < 2/\\lambda_{\\max}(Q)$.\n\nYour program must implement:\n- A proper nonlinear conjugate gradient solver that uses a backtracking line search enforcing the Armijo condition with user-chosen constants $c_1 \\in (0,1)$ and backtracking ratio $\\tau \\in (0,1)$.\n- A faulty nonlinear conjugate gradient solver that uses $\\alpha_k \\equiv 1$ for all $k$ (no sufficient decrease check).\n\nDesign details you must adhere to:\n- Use the Polak–Ribiere–Plus choice for the conjugacy parameter $\\beta_k$ and a reset safeguard if the computed direction is not a descent direction.\n- Terminate when $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ for a given tolerance $\\varepsilon > 0$, or when a fixed iteration budget is exhausted.\n- Declare nonconvergence if the iteration budget is exhausted without meeting the gradient tolerance. Declare divergence if the objective value becomes not-a-number, exceeds a large threshold, or the iterate norm exceeds a large threshold.\n\nConstruct a test suite that demonstrates different behaviors:\n- Test A (divergence witness): a convex quadratic $f(x) = \\tfrac{1}{2} x^\\top Q x$ with $Q = \\mathrm{diag}(10.0, 0.1)$ and initial point $x_0 = [1.0, 1.0]$. By the spectral-radius criterion, a fixed step $\\alpha = 1$ violates $0 < \\alpha < 2/\\lambda_{\\max}(Q)$ because $\\lambda_{\\max}(Q) = 10.0$, and therefore the faulty method is expected to diverge, while the Armijo-enforced method should converge.\n- Test B (happy path): a well-conditioned convex quadratic with $Q = \\mathrm{diag}(0.5, 0.25)$ and $x_0 = [2.0, -3.0]$. Here $\\lambda_{\\max}(Q) = 0.5$, so a fixed step $\\alpha = 1$ satisfies $0 < \\alpha < 2/\\lambda_{\\max}(Q)$ and both methods should converge.\n- Test C (boundary condition): the same convex quadratic $Q = \\mathrm{diag}(1.0, 1.0)$ with $x_0 = [0.0, 0.0]$, which is already a minimizer. Both methods should detect convergence immediately.\n- Test D (nonconvex stress test): a scaled Rosenbrock function $f(x_1,x_2) = 10\\,(x_2 - x_1^2)^2 + (1 - x_1)^2$ with $x_0 = [-1.2, 1.0]$. The faulty method’s unit steps can cause blow-up on this curved valley, while the proper method with Armijo backtracking should converge to the minimizer near $[1,1]$.\n\nNumerical parameters to use in your program:\n- Gradient norm tolerance $\\varepsilon = 10^{-6}$, maximum iterations $N_{\\max} = 5000$, Armijo constant $c_1 = 10^{-4}$, backtracking ratio $\\tau = 0.5$.\n- Divergence thresholds: declare divergence if $\\|x_k\\|_2 > 10^{12}$ or $f(x_k) > 10^{50}$ or $f(x_k)$ is not-a-number.\n\nYour program must:\n- Implement both solvers, run them on all four tests, and determine, for each test, an integer code based on the observed behavior:\n    - Output $1$ if the proper method converges and the faulty method does not converge (either nonconvergent or divergent).\n    - Output $0$ if both converge.\n    - Output $-1$ if neither converges.\n    - Output $2$ if the faulty method converges and the proper method does not.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, the output format must look like a single Python-style list literal such as [r1,r2,r3,r4], where each entry is one of the specified integer codes.\n\nAngle units are not involved. There are no physical units in this problem. All quantities are dimensionless. The output must strictly follow the specified single-line format.", "solution": "The problem posed is the implementation and comparative analysis of two variants of the nonlinear conjugate gradient (NCG) method for unconstrained optimization. One variant is correctly implemented, adhering to the fundamental principles of line search methods by enforcing the Armijo sufficient decrease condition. The second variant is intentionally flawed, employing a fixed unit step length, thereby omitting the crucial safeguard of a line search. The objective is to demonstrate computationally that the omission of the Armijo condition can lead to failure, specifically non-convergence or divergence, on problems where a correctly implemented algorithm succeeds. The problem is well-posed, scientifically sound, and provides a clear basis for algorithmic implementation and verification.\n\nA general unconstrained minimization problem is considered, seeking to find a local minimizer of a continuously differentiable objective function $f: \\mathbb{R}^n \\to \\mathbb{R}$. The NCG method is an iterative algorithm that generates a sequence of points $\\{x_k\\}_{k \\ge 0}$ using the update rule:\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\nHere, $x_k \\in \\mathbb{R}^n$ is the current iterate, $d_k \\in \\mathbb{R}^n$ is the search direction, and $\\alpha_k > 0$ is the step length. The gradient of the objective function at $x_k$ is denoted by $g_k = \\nabla f(x_k)$.\n\nThe search direction $d_k$ is constructed to be a descent direction, meaning $g_k^\\top d_k < 0$. The NCG directions are defined recursively. The initial direction is the steepest descent direction, $d_0 = -g_0$. For subsequent iterations $k \\ge 1$, the direction is a linear combination of the current negative gradient and the previous direction:\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\nThe scalar $\\beta_k$ is the conjugacy parameter. The problem specifies the Polak–Ribière–Plus variant, which is known for its strong numerical performance. It is defined as:\n$$\n\\beta_k = \\max \\left\\{ 0, \\frac{g_k^\\top(g_k - g_{k-1})}{\\|g_{k-1}\\|_2^2} \\right\\}\n$$\nThis choice incorporates a non-negativity constraint, which helps in ensuring global convergence under certain conditions. A vital safeguard is the reset condition: if the computed direction $d_k$ fails to be a descent direction (i.e., if $g_k^\\top d_k \\ge 0$), the method is reset by setting the search direction to that of steepest descent, $d_k = -g_k$.\n\nThe core of this investigation lies in the determination of the step length $\\alpha_k$.\n\nThe **proper NCG method** employs a backtracking line search to find a step length $\\alpha_k$ that satisfies the Armijo sufficient decrease condition. For a given constant $c_1 \\in (0, 1)$, this condition is:\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n$$\nThis inequality ensures that the reduction in the objective function is at least a fraction of the decrease predicted by the linear approximation of $f$ at $x_k$. The backtracking procedure starts with an initial trial step, typically $\\alpha = 1$, and iteratively reduces it by a factor $\\tau \\in (0, 1)$ (e.g., $\\alpha \\leftarrow \\tau \\alpha$) until the condition is met. The parameters specified are $c_1 = 10^{-4}$ and $\\tau = 0.5$.\n\nThe **faulty NCG method** bypasses this critical check and naively sets $\\alpha_k = 1$ for all iterations $k \\ge 0$. While this may be acceptable for certain well-behaved functions or if the initial iterate is close to the solution, it is generally an unreliable strategy that can lead to failure.\n\nTermination of the algorithm is dictated by the norm of the gradient. The iteration is considered converged if $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ for a tolerance $\\varepsilon = 10^{-6}$. The process is aborted if the number of iterations exceeds a budget $N_{\\max} = 5000$, which is classified as non-convergence. Divergence is declared if the iterate norm $\\|x_k\\|_2$ exceeds $10^{12}$, the function value $f(x_k)$ exceeds $10^{50}$, or if $f(x_k)$ becomes not-a-number (NaN).\n\nThe analysis is performed on a suite of four test cases designed to expose the differing behaviors of the two methods.\n\n**Test A**: A convex quadratic function $f(x) = \\frac{1}{2} x^\\top Q x$ with a poorly conditioned Hessian matrix $Q = \\mathrm{diag}(10.0, 0.1)$. For a quadratic function, the NCG method with a fixed step $\\alpha$ is equivalent to the linear iterative system $x_{k+1} = (I - \\alpha Q) x_k$. This system converges if and only if the spectral radius of the iteration matrix, $\\rho(I - \\alpha Q)$, is less than $1$. With $\\alpha=1$, the eigenvalues of $I-Q$ are $1-10.0 = -9.0$ and $1-0.1 = 0.9$. The spectral radius is $\\rho(I - Q) = \\max\\{|-9.0|, |0.9|\\} = 9.0$, which is greater than $1$. The faulty method is therefore guaranteed to diverge. The proper method, with its adaptive step length from the Armijo condition, is expected to converge.\n\n**Test B**: A well-conditioned convex quadratic function with $Q = \\mathrm{diag}(0.5, 0.25)$. Here, for the faulty method with $\\alpha=1$, the eigenvalues of $I-Q$ are $1-0.5 = 0.5$ and $1-0.25=0.75$. The spectral radius is $\\rho(I - Q) = 0.75 < 1$, satisfying the convergence condition. Thus, both the faulty and proper methods are expected to converge.\n\n**Test C**: A convex quadratic with $x_0 = [0.0, 0.0]$, which is the global minimum. The initial gradient is $\\nabla f(x_0) = 0$. Both algorithms must check the termination condition before the first iteration and declare convergence immediately.\n\n**Test D**: The nonconvex Rosenbrock function, $f(x_1,x_2) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$, from the starting point $x_0 = [-1.2, 1.0]$. This function is a classic benchmark characterized by a narrow, curved valley. The fixed unit step of the faulty method is likely to cause the iterates to \"jump\" over the valley, leading to an increase in the function value and erratic behavior, likely causing divergence or non-convergence. In contrast, the backtracking line search of the proper method will systematically reduce the step length to ensure sufficient decrease, allowing the iterates to follow the valley toward the minimum at $[1,1]$.\n\nThe outcome for each test is an integer code: $1$ if the proper method converges and the faulty one does not; $0$ if both converge; $-1$ if neither converges; and $2$ if the faulty method converges but the proper one does not. This systematic comparison provides clear evidence for the indispensable role of the line search mechanism in ensuring the robustness of descent-based optimization algorithms.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares a proper and a faulty nonlinear conjugate gradient method.\n    \"\"\"\n\n    # --- Numerical Parameters ---\n    EPSILON = 1e-6\n    MAX_ITER = 5000\n    C1 = 1e-4\n    TAU = 0.5\n    DIV_NORM_THRESHOLD = 1e12\n    DIV_F_THRESHOLD = 1e50\n\n    def nonlinear_cg(f, grad_f, x0, use_armijo):\n        \"\"\"\n        Nonlinear Conjugate Gradient (NCG) solver.\n\n        Args:\n            f: Objective function.\n            grad_f: Gradient of the objective function.\n            x0: Initial point.\n            use_armijo: Boolean flag to use Armijo line search.\n\n        Returns:\n            A string indicating the outcome: \"converged\", \"nonconverged\", \"diverged\".\n        \"\"\"\n        x_k = np.copy(x0).astype(np.float64)\n        g_k = grad_f(x_k)\n        \n        # Initial check for convergence at x0\n        norm_g_k = np.linalg.norm(g_k)\n        if norm_g_k <= EPSILON:\n            return \"converged\"\n\n        d_k = -g_k\n        k = 0\n\n        while k < MAX_ITER:\n            # Line Search\n            if use_armijo:\n                alpha_k = 1.0\n                descent_condition_val = C1 * np.dot(g_k, d_k)\n                # The dot product g_k.T @ d_k should be < 0 due to safeguard\n                try:\n                    f_k = f(x_k)\n                    while f(x_k + alpha_k * d_k) > f_k + alpha_k * descent_condition_val:\n                        alpha_k *= TAU\n                        if alpha_k < 1e-15: # Prevent infinite loop if step size becomes too small\n                           return \"nonconverged\"\n                except (OverflowError, ValueError):\n                    return \"diverged\" # f() evaluation might fail\n            else:\n                alpha_k = 1.0\n\n            # Update position\n            x_k_plus_1 = x_k + alpha_k * d_k\n\n            # Check for divergence\n            try:\n                f_next = f(x_k_plus_1)\n                if np.linalg.norm(x_k_plus_1) > DIV_NORM_THRESHOLD or f_next > DIV_F_THRESHOLD or np.isnan(f_next):\n                    return \"diverged\"\n            except (OverflowError, ValueError):\n                return \"diverged\"\n\n            g_k_plus_1 = grad_f(x_k_plus_1)\n            norm_g_k_plus_1 = np.linalg.norm(g_k_plus_1)\n\n            # Check for convergence\n            if norm_g_k_plus_1 <= EPSILON:\n                return \"converged\"\n\n            # Polak-Ribiere-Plus (PR+) for beta\n            norm_g_k_sq = norm_g_k**2\n            if norm_g_k_sq > 1e-14: # Safeguard against division by zero\n                beta_k_plus_1 = max(0, np.dot(g_k_plus_1, g_k_plus_1 - g_k) / norm_g_k_sq)\n            else:\n                beta_k_plus_1 = 0.0\n\n            # Update search direction\n            d_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * d_k\n\n            # Restart if not a descent direction\n            if np.dot(g_k_plus_1, d_k_plus_1) >= 0:\n                d_k_plus_1 = -g_k_plus_1\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            g_k = g_k_plus_1\n            norm_g_k = norm_g_k_plus_1\n            d_k = d_k_plus_1\n            k += 1\n\n        return \"nonconverged\"\n    \n    # --- Test Case Definitions ---\n\n    # Test A: Divergence Witness\n    Q_A = np.diag([10.0, 0.1])\n    def f_A(x): return 0.5 * x.T @ Q_A @ x\n    def grad_f_A(x): return Q_A @ x\n    x0_A = np.array([1.0, 1.0])\n\n    # Test B: Happy Path\n    Q_B = np.diag([0.5, 0.25])\n    def f_B(x): return 0.5 * x.T @ Q_B @ x\n    def grad_f_B(x): return Q_B @ x\n    x0_B = np.array([2.0, -3.0])\n\n    # Test C: Boundary Condition\n    Q_C = np.diag([1.0, 1.0])\n    def f_C(x): return 0.5 * x.T @ Q_C @ x\n    def grad_f_C(x): return Q_C @ x\n    x0_C = np.array([0.0, 0.0])\n\n    # Test D: Nonconvex Stress Test (Scaled Rosenbrock)\n    def f_D(x): return 10.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n    def grad_f_D(x):\n        df_dx1 = -40.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 20.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n    x0_D = np.array([-1.2, 1.0])\n\n    test_cases = [\n        (f_A, grad_f_A, x0_A),\n        (f_B, grad_f_B, x0_B),\n        (f_C, grad_f_C, x0_C),\n        (f_D, grad_f_D, x0_D),\n    ]\n\n    results = []\n    for f, grad_f, x0 in test_cases:\n        proper_status = nonlinear_cg(f, grad_f, x0, use_armijo=True)\n        faulty_status = nonlinear_cg(f, grad_f, x0, use_armijo=False)\n\n        proper_converged = (proper_status == \"converged\")\n        faulty_converged = (faulty_status == \"converged\")\n\n        if proper_converged and not faulty_converged:\n            results.append(1)\n        elif proper_converged and faulty_converged:\n            results.append(0)\n        elif not proper_converged and not faulty_converged:\n            results.append(-1)\n        elif not proper_converged and faulty_converged:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2418455"}, {"introduction": "The path to a minimum is not always straightforward, nor is the rate of descent always predictable. This practice delves into a subtle but important characteristic of NCG methods: the potentially non-monotonic behavior of the gradient norm, $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2$, during the optimization process. While one might intuitively expect the gradient's magnitude to shrink at every step, this exercise reveals that temporary increases are possible and are part of the sophisticated strategy NCG employs to navigate complex function landscapes more efficiently than simpler methods [@problem_id:2418485].", "problem": "You must write a complete and runnable program that, for a fixed set of objective functions and starting points, determines whether the Euclidean norm of the gradient, denoted by $\\lVert \\nabla f(x_k) \\rVert_2$, is non-monotonic during an optimization process based on Nonlinear Conjugate Gradient (NCG). The task is formalized as follows.\n\nLet $f : \\mathbb{R}^n \\to \\mathbb{R}$ be continuously differentiable. Consider an iterative scheme that generates a sequence $\\{x_k\\}_{k \\ge 0}$ with search directions $\\{d_k\\}_{k \\ge 0}$, step lengths $\\{\\alpha_k\\}_{k \\ge 0}$, gradients $g_k \\equiv \\nabla f(x_k)$, and gradient norms $\\lVert g_k \\rVert_2$. The initial search direction is $d_0 = -g_0$. For $k \\ge 0$, define the update\n$$\nx_{k+1} = x_k + \\alpha_k d_k, \\quad d_{k+1} = -g_{k+1} + \\beta_k d_k,\n$$\nwith $\\beta_k$ chosen according to one of the classical NCG formulas:\n- Fletcher–Reeves (FR): $\\beta_k^{\\mathrm{FR}} = \\dfrac{\\langle g_{k+1}, g_{k+1} \\rangle}{\\langle g_k, g_k \\rangle}$,\n- Polak–Ribière–Polyak nonnegative modification (PR+): $\\beta_k^{\\mathrm{PR}+} = \\max\\!\\left\\{0,\\; \\dfrac{\\langle g_{k+1}, g_{k+1} - g_k \\rangle}{\\langle g_k, g_k \\rangle}\\right\\}$,\n- Hestenes–Stiefel nonnegative modification (HS+): $\\beta_k^{\\mathrm{HS}+} = \\max\\!\\left\\{0,\\; \\dfrac{\\langle g_{k+1}, g_{k+1} - g_k \\rangle}{\\langle d_k, g_{k+1} - g_k \\rangle}\\right\\}$,\nwhere $\\langle \\cdot,\\cdot \\rangle$ denotes the standard inner product in $\\mathbb{R}^n$.\n\nThe step length $\\alpha_k$ must satisfy the strong Wolfe conditions along the line $\\phi_k(\\alpha) = f(x_k + \\alpha d_k)$ with constants $c_1 \\in (0,1)$ and $c_2 \\in (c_1,1)$:\n$$\n\\phi_k(\\alpha_k) \\le \\phi_k(0) + c_1 \\alpha_k \\phi_k'(0), \n\\quad\n\\lvert \\phi_k'(\\alpha_k) \\rvert \\le c_2 \\lvert \\phi_k'(0) \\rvert,\n$$\nwhere $\\phi_k'(\\alpha) = \\nabla f(x_k + \\alpha d_k)^\\top d_k$. If a computed search direction $d_k$ fails to be a descent direction, that is, if $\\langle d_k, g_k \\rangle \\ge 0$, then enforce a restart by setting $d_k \\leftarrow -g_k$.\n\nYour program must implement this method and, for each test case listed below, compute the finite sequence $\\{\\lVert g_k \\rVert_2\\}_{k=0}^{K}$ produced by the algorithm until either $\\lVert g_k \\rVert_2 \\le \\varepsilon$ or $k$ reaches a given iteration limit. Then, for each case, output a boolean indicating whether the sequence of gradient norms is non-monotonic, meaning that there exists an index $k$ with $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$.\n\nUse the following fixed constants for all test cases: $c_1 = 10^{-4}$, $c_2 = 0.9$, initial step length $\\alpha_0 = 1$, gradient tolerance $\\varepsilon = 10^{-8}$, and maximum number of iterations $K_{\\max} = 500$. All inner products and norms are Euclidean, and all numbers are real.\n\nYou must use the following test suite of objective functions, dimensions, starting points, and $\\beta_k$-variants:\n\n- Test Case 1 (nonconvex, two-dimensional):\n  - Objective: the two-dimensional Rosenbrock function\n    $$\n    f_1(x) = (1 - x_1)^2 + 100\\,(x_2 - x_1^2)^2,\n    $$\n    with $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$.\n  - Starting point: $x_0 = (-1.2,\\, 1.0)^\\top$.\n  - Variant: $\\beta_k = \\beta_k^{\\mathrm{PR}+}$.\n\n- Test Case 2 (strictly convex quadratic, three-dimensional):\n  - Objective: the spherical quadratic\n    $$\n    f_2(x) = \\tfrac{1}{2}\\,\\lVert x \\rVert_2^2,\n    $$\n    with $x \\in \\mathbb{R}^3$.\n  - Starting point: $x_0 = (3,\\, -4,\\, 1)^\\top$.\n  - Variant: $\\beta_k = \\beta_k^{\\mathrm{FR}}$.\n\n- Test Case 3 (nonconvex, five-dimensional):\n  - Objective: the $5$-dimensional Rosenbrock function\n    $$\n    f_3(x) = \\sum_{i=1}^{4} \\left[(1 - x_i)^2 + 100\\,\\left(x_{i+1} - x_i^2\\right)^2\\right],\n    $$\n    with $x = (x_1,\\dots,x_5)^\\top \\in \\mathbb{R}^5$.\n  - Starting point: $x_0 = (-1.2,\\, 1.0,\\, -1.2,\\, 1.0,\\, -1.2)^\\top$.\n  - Variant: $\\beta_k = \\beta_k^{\\mathrm{HS}+}$.\n\nFor each test case, you must return a boolean value defined as:\n- Return true if there exists at least one index $k$ such that $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$.\n- Return false otherwise.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). The results must appear in the order of the test cases above. There are no physical units involved in this problem. Angles are not used. Percentages are not used. The output elements are booleans.", "solution": "The problem presented is a well-defined computational task in the field of numerical optimization. It requires the implementation of the Nonlinear Conjugate Gradient (NCG) method to determine if the sequence of gradient norms exhibits non-monotonic behavior for a specified set of objective functions and starting conditions. The problem is scientifically grounded, formally specified, and all necessary parameters are provided. It is therefore deemed valid.\n\nThe core of the problem is to solve an unconstrained optimization problem of the form\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x)\n$$\nwhere $f$ is a continuously differentiable function. The NCG method generates a sequence of iterates $\\{x_k\\}$ starting from an initial point $x_0$. The update rule is given by\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\nwhere $d_k$ is the search direction and $\\alpha_k > 0$ is the step length.\n\nThe initial search direction is the direction of steepest descent, $d_0 = -g_0$, where $g_k \\equiv \\nabla f(x_k)$ is the gradient of the objective function at $x_k$. For subsequent iterations ($k \\ge 1$), the search direction is computed as a linear combination of the current negative gradient and the previous search direction:\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\nThe coefficient $\\beta_k$ is crucial and defines the specific variant of the NCG method. The problem specifies three classical formulas for $\\beta_k$:\n\n$1$. Fletcher–Reeves (FR):\n$$\n\\beta_k^{\\mathrm{FR}} = \\frac{\\langle g_k, g_k \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product.\n\n$2$. Polak–Ribière–Polyak with a non-negativity constraint (PR+):\n$$\n\\beta_k^{\\mathrm{PR}+} = \\max\\left\\{0, \\frac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\\right\\}\n$$\nThis modification prevents large values of $\\beta_k$ when the method is far from a solution, which can improve performance.\n\n$3$. Hestenes–Stiefel with a non-negativity constraint (HS+):\n$$\n\\beta_k^{\\mathrm{HS}+} = \\max\\left\\{0, \\frac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle d_{k-1}, g_k - g_{k-1} \\rangle}\\right\\}\n$$\nThis variant is often considered effective but requires careful implementation, as the denominator could theoretically become zero. However, when combined with a line search satisfying the strong Wolfe conditions, the denominator is guaranteed to be positive.\n\nThe step length $\\alpha_k$ is determined by a line search procedure to ensure sufficient decrease in the function value and convergence of the algorithm. The problem mandates the use of the strong Wolfe conditions:\n$1$. Armijo (sufficient decrease) condition: $f(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k \\langle g_k, d_k \\rangle$.\n$2$. Strong curvature condition: $|\\langle \\nabla f(x_k + \\alpha_k d_k), d_k \\rangle| \\le c_2 |\\langle g_k, d_k \\rangle|$.\nThe constants are specified as $c_1 = 10^{-4}$ and $c_2 = 0.9$, which are standard choices. These conditions ensure that the step length is both effective and not excessively small. The `scipy.optimize.line_search` function is utilized for this purpose, as it provides a robust implementation for finding such an $\\alpha_k$.\n\nA crucial aspect of modern NCG methods is the restart strategy. If the computed search direction $d_k$ is not a descent direction, i.e., if $\\langle g_k, d_k \\rangle \\ge 0$, the method is reset by discarding the previous direction information and setting $d_k = -g_k$. This ensures that each step contributes to minimizing the objective function.\n\nThe overall algorithm proceeds as follows:\n$1$. Initialize $k=0$, $x_0$, and compute $g_0 = \\nabla f(x_0)$. Set $d_0 = -g_0$. Maintain a record of gradient norms.\n$2$. For $k = 0, 1, 2, \\dots, K_{\\max}-1$:\n    a. Check for convergence: if $\\lVert g_k \\rVert_2 \\le \\varepsilon = 10^{-8}$, terminate.\n    b. Ensure $d_k$ is a descent direction. If $\\langle g_k, d_k \\rangle \\ge 0$, set $d_k = -g_k$.\n    c. Perform a line search to find $\\alpha_k > 0$ satisfying the strong Wolfe conditions with constants $c_1=10^{-4}$ and $c_2=0.9$, using an initial guess of $\\alpha=1$.\n    d. Update the position: $x_{k+1} = x_k + \\alpha_k d_k$.\n    e. Compute the new gradient $g_{k+1} = \\nabla f(x_{k+1})$.\n    f. Check for non-monotonicity: Compare $\\lVert g_{k+1} \\rVert_2$ with $\\lVert g_k \\rVert_2$. If $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$, a non-monotonic event has occurred.\n    g. Compute $\\beta_{k+1}$ using the formula specified for the test case (FR, PR+, or HS+).\n    h. Update the search direction: $d_{k+1} = -g_{k+1} + \\beta_{k+1} d_k$.\n$3$. If the loop completes without convergence, it terminates at $k = K_{\\max} = 500$.\n\nThis procedure is applied to three test cases:\n- Case 1: The $2$-dimensional Rosenbrock function, a classic non-convex benchmark, with the PR+ method. The gradient is $\\nabla f_1(x) = [400x_1^3 - 400x_1x_2 + 2x_1 - 2, 200(x_2 - x_1^2)]^\\top$.\n- Case 2: A simple $3$-dimensional strictly convex quadratic function, with the FR method. The gradient is $\\nabla f_2(x) = x$.\n- Case 3: The $5$-dimensional Rosenbrock function, extending the challenge of Case 1, with the HS+ method. The gradient expression is more complex, with distinct formulas for its first, last, and intermediate components.\n\nFor each case, a boolean flag is tracked to determine if any step $k$ results in $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$. The final output is a list of these boolean values. The implementation will be encapsulated in a Python script using `numpy` for linear algebra and `scipy.optimize.line_search` for the line search, adhering strictly to the problem's specifications.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef solve():\n    \"\"\"\n    Solves the main problem by running NCG for three test cases.\n    \"\"\"\n\n    # --- Global Constants ---\n    C1 = 1e-4\n    C2 = 0.9\n    EPSILON = 1e-8\n    K_MAX = 500\n\n    # --- Objective Functions and Gradients ---\n\n    # Test Case 1 & 3: Rosenbrock function and its gradient\n    def rosenbrock_f(x):\n        return np.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1.0 - x[:-1])**2.0)\n\n    def rosenbrock_grad(x):\n        n = len(x)\n        grad = np.zeros(n)\n        # Gradient for x_i, 1 < i < n (0-indexed: 0 < i < n-1)\n        if n > 2:\n            grad[1:-1] = (200 * (x[1:-1] - x[:-2]**2)\n                          - 400 * (x[2:] - x[1:-1]**2) * x[1:-1]\n                          - 2 * (1 - x[1:-1]))\n        # Gradient for x_1 (0-indexed: x_0)\n        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n        # Gradient for x_n (0-indexed: x_{n-1})\n        grad[-1] = 200 * (x[-1] - x[-2]**2)\n        return grad\n\n    # Test Case 2: Spherical quadratic function and its gradient\n    def quadratic_f(x):\n        return 0.5 * np.dot(x, x)\n\n    def quadratic_grad(x):\n        return x\n\n    # --- NCG Algorithm Implementation ---\n\n    def run_ncg(f, grad_f, x0, beta_method):\n        \"\"\"\n        Runs the Nonlinear Conjugate Gradient algorithm for a given function.\n\n        Args:\n            f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The starting point.\n            beta_method: The formula for beta ('FR', 'PR+', 'HS+').\n\n        Returns:\n            A boolean indicating if the gradient norm was non-monotonic.\n        \"\"\"\n        k = 0\n        x = np.copy(x0)\n        \n        g = grad_f(x)\n        grad_norm = np.linalg.norm(g)\n        f_val = f(x)\n        \n        d = -g\n        \n        is_non_monotonic = False\n\n        while k < K_MAX and grad_norm > EPSILON:\n            # Restart if not a descent direction\n            if np.dot(g, d) >= 0:\n                d = -g\n\n            # Perform line search satisfying strong Wolfe conditions\n            # old_old_fval=None ensures the initial step guess is 1.0\n            alpha, _, _, f_val_new, _, g_new = line_search(\n                f=f, myfprime=grad_f, xk=x, pk=d, gfk=g, old_fval=f_val,\n                c1=C1, c2=C2, old_old_fval=None\n            )\n            \n            if alpha is None:\n                # Line search failed, terminate optimization\n                break\n\n            # Update position\n            x_new = x + alpha * d\n            \n            # Check for non-monotonicity in gradient norm\n            grad_norm_new = np.linalg.norm(g_new)\n            if grad_norm_new > grad_norm:\n                is_non_monotonic = True\n\n            # Calculate beta for the next iteration\n            y = g_new - g\n            g_dot_g = np.dot(g, g)\n            \n            if g_dot_g == 0:\n                beta = 0.0 # Should not happen due to termination condition\n            elif beta_method == 'FR':\n                beta = np.dot(g_new, g_new) / g_dot_g\n            elif beta_method == 'PR+':\n                beta = max(0.0, np.dot(g_new, y) / g_dot_g)\n            elif beta_method == 'HS+':\n                denom = np.dot(d, y)\n                if denom == 0:\n                    beta = 0.0 # Restart if denominator is zero\n                else:\n                    beta = max(0.0, np.dot(g_new, y) / denom)\n            else:\n                raise ValueError(\"Unknown beta method\")\n\n            # Update search direction\n            d_new = -g_new + beta * d\n            \n            # Prepare for next iteration\n            k += 1\n            x, g, f_val, d = x_new, g_new, f_val_new, d_new\n            grad_norm = grad_norm_new\n            \n        return is_non_monotonic\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            'f': rosenbrock_f,\n            'grad_f': rosenbrock_grad,\n            'x0': np.array([-1.2, 1.0]),\n            'beta_method': 'PR+'\n        },\n        {\n            'f': quadratic_f,\n            'grad_f': quadratic_grad,\n            'x0': np.array([3.0, -4.0, 1.0]),\n            'beta_method': 'FR'\n        },\n        {\n            'f': rosenbrock_f,\n            'grad_f': rosenbrock_grad,\n            'x0': np.array([-1.2, 1.0, -1.2, 1.0, -1.2]),\n            'beta_method': 'HS+'\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_ncg(case['f'], case['grad_f'], case['x0'], case['beta_method'])\n        results.append(result)\n\n    # --- Final Output ---\n    # Convert booleans to lowercase strings as per implied format\n    output_str = ','.join(map(lambda b: str(b).lower(), results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2418485"}]}