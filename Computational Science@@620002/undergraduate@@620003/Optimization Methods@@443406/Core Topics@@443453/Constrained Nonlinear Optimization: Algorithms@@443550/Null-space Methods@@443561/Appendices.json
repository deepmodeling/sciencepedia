{"hands_on_practices": [{"introduction": "To master a new method, there is no substitute for walking through a concrete example from start to finish. This first exercise provides a complete, step-by-step guide to applying the null-space method to a small-scale quadratic program. By explicitly computing the null-space basis, finding a feasible point, and solving the resulting unconstrained problem, you will see how a constrained problem in $\\mathbb{R}^2$ is transformed into a simple one-dimensional problem. Comparing this result with the solution from the Karush-Kuhn-Tucker (KKT) conditions will confirm your answer and deepen your understanding of how these different perspectives lead to the same optimal point [@problem_id:3158284].", "problem": "Consider the equality-constrained quadratic optimization problem\n$$\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} H x + g^{\\top} x \\quad \\text{subject to} \\quad A x = b,$$\nwhere the data are\n$$H = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad b = 2.$$\nThis problem is strictly convex because the Hessian matrix $H$ is symmetric positive definite. Using the null-space method grounded in linear algebra and the optimality conditions for equality-constrained optimization, carry out the following:\n\n1. Compute the full $QR$ factorization of the transpose $A^{\\top}$, i.e., find an orthogonal matrix $Q \\in \\mathbb{R}^{2 \\times 2}$ and an upper-triangular (in the rectangular sense) matrix $R \\in \\mathbb{R}^{2 \\times 1}$ such that $A^{\\top} = Q R$. From $Q$, identify a matrix $Z \\in \\mathbb{R}^{2 \\times 1}$ whose column forms an orthonormal basis for the null space of $A$.\n\n2. Find any feasible point $x_{0} \\in \\mathbb{R}^{2}$ satisfying $A x_{0} = b$.\n\n3. Using the null-space parameterization $x = x_{0} + Z p$ with $p \\in \\mathbb{R}$, derive the reduced one-dimensional quadratic in $p$ obtained by substituting into the objective. Solve for the minimizer $p^{\\star}$ and compute the corresponding primal minimizer $x^{\\star} = x_{0} + Z p^{\\star}$.\n\n4. Verify that $x^{\\star}$ satisfies feasibility $A x^{\\star} = b$ and that the reduced optimality condition in the null space holds.\n\n5. Independently, solve the equality-constrained problem via the Karush–Kuhn–Tucker (KKT) conditions, which for equality constraints assert stationarity and feasibility. Form and solve the linear KKT system to obtain the primal solution. Compare it to the null-space solution.\n\nReport your final answer as the optimal primal vector $x^{\\star}$ in row-matrix form. No rounding is required; provide exact values.", "solution": "The problem is a well-posed, equality-constrained quadratic program. It is scientifically sound, self-contained, and all data required for its solution are provided. The Hessian matrix $H$ is symmetric and positive definite, which guarantees that the objective function is strictly convex and that a unique minimizer exists. The constraint matrix $A$ has full row rank, ensuring the feasibility of the constraints. Therefore, the problem is valid, and we may proceed with the solution by following the specified steps.\n\nThe objective is to minimize $f(x) = \\frac{1}{2} x^{\\top} H x + g^{\\top} x$ subject to $A x = b$, with the given data:\n$$H = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad b = 2$$\n\n**1. QR Factorization and Null-Space Basis**\n\nWe compute the full $QR$ factorization of $A^{\\top}$. The matrix $A^{\\top}$ is a column vector in $\\mathbb{R}^{2}$:\n$$A^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nWe need to find an orthogonal matrix $Q \\in \\mathbb{R}^{2 \\times 2}$ and an upper-triangular matrix $R \\in \\mathbb{R}^{2 \\times 1}$ such that $A^{\\top} = QR$. We can use the Gram-Schmidt process. Let the columns of $Q$ be $q_1$ and $q_2$.\n\nThe first column $q_1$ is a normalized version of $A^{\\top}$:\n$$\\|A^{\\top}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\n$$q_1 = \\frac{A^{\\top}}{\\|A^{\\top}\\|} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$$\nThe second column $q_2$ must be a unit vector orthogonal to $q_1$. We can choose:\n$$q_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$$\nWe can verify that $q_1^{\\top} q_2 = 0$ and $\\|q_2\\|=1$.\nThe orthogonal matrix $Q$ is formed by these columns:\n$$Q = \\begin{pmatrix} q_1 & q_2 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix}$$\nThe matrix $R$ is given by $R = Q^{\\top} A^{\\top}$:\n$$R = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix}$$\nThis $R$ is upper-triangular.\n\nThe null space of $A$, denoted $\\mathcal{N}(A)$, consists of all vectors $z$ such that $Az=0$. For $A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$, this means $z_1+z_2=0$. The columns of $Q$ can be partitioned as $Q = \\begin{pmatrix} Y & Z \\end{pmatrix}$, where the columns of $Y$ form a basis for the range of $A^{\\top}$ and the columns of $Z$ form a basis for the null space of $A$. Here, $Y$ corresponds to $q_1$ and $Z$ to $q_2$.\n$$Z = q_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$$\nWe verify that $AZ = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} = 1/\\sqrt{2} - 1/\\sqrt{2} = 0$. So, $Z$ indeed forms an orthonormal basis for $\\mathcal{N}(A)$.\n\n**2. Finding a Feasible Point**\n\nWe need to find any particular solution $x_0$ to the linear system $Ax=b$:\n$$\\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 2 \\quad \\implies \\quad x_1 + x_2 = 2$$\nA simple choice is $x_1=1$, $x_2=1$. Thus, a feasible point is:\n$$x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n**3. Null-Space Parameterization and Solution**\n\nAny feasible point $x$ can be expressed as the sum of a particular feasible point $x_0$ and a vector from the null space of $A$. Since $Z$ is a basis for $\\mathcal{N}(A)$, any such vector can be written as $Zp$ for some $p \\in \\mathbb{R}$.\n$$x = x_0 + Zp = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} p$$\nWe substitute this into the objective function $f(x)$ to obtain a reduced quadratic function $q(p)$:\n$$f(x_0+Zp) = \\frac{1}{2}(x_0+Zp)^{\\top}H(x_0+Zp) + g^{\\top}(x_0+Zp)$$\n$$q(p) = \\frac{1}{2} p^{\\top}(Z^{\\top}HZ)p + (x_0^{\\top}HZ + g^{\\top}Z)p + (\\frac{1}{2}x_0^{\\top}Hx_0 + g^{\\top}x_0)$$\nSince $p$ is a scalar, we can write this as:\n$$q(p) = \\frac{1}{2} (Z^{\\top}HZ) p^2 + (Z^{\\top}(Hx_0+g)) p + \\text{const.}$$\nWe compute the coefficients. The reduced Hessian is:\n$$H Z = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 2/\\sqrt{2} \\\\ -4/\\sqrt{2} \\end{pmatrix}$$\n$$Z^{\\top}HZ = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 2/\\sqrt{2} \\\\ -4/\\sqrt{2} \\end{pmatrix} = \\frac{2}{2} - \\frac{-4}{2} = 1+2 = 3$$\nThe reduced gradient term's coefficient is:\n$$Hx_0 + g = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$$\n$$Z^{\\top}(Hx_0+g) = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = \\frac{3}{\\sqrt{2}} - \\frac{3}{\\sqrt{2}} = 0$$\nThe reduced objective function is:\n$$q(p) = \\frac{3}{2} p^2 + \\text{const.}$$\nTo find the minimizer $p^{\\star}$, we set the derivative with respect to $p$ to zero:\n$$\\frac{d q}{d p} = 3p = 0 \\quad \\implies \\quad p^{\\star} = 0$$\nThe optimal primal solution $x^{\\star}$ is then:\n$$x^{\\star} = x_0 + Z p^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} (0) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n**4. Verification**\n\nFirst, we check feasibility of $x^{\\star}$:\n$$A x^{\\star} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1+1=2 = b$$\nThe constraint is satisfied.\nSecond, we check the reduced optimality condition. The gradient of the reduced problem, $\\nabla_p q(p) = (Z^{\\top}HZ)p + Z^{\\top}(Hx_0+g)$, must be zero at $p^{\\star}$. This is equivalent to checking that the gradient of the original objective function at $x^{\\star}$, $\\nabla f(x^{\\star}) = Hx^{\\star}+g$, is orthogonal to the null space, i.e., $Z^{\\top}(Hx^{\\star}+g)=0$.\n$$Hx^{\\star}+g = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$$\n$$Z^{\\top}(Hx^{\\star}+g) = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = 0$$\nThe reduced optimality condition holds.\n\n**5. Solution via KKT Conditions**\n\nThe Karush-Kuhn-Tucker (KKT) conditions for this equality-constrained problem are stationarity and primal feasibility. Let $\\lambda \\in \\mathbb{R}$ be the Lagrange multiplier for the single constraint.\n1. Stationarity: $\\nabla f(x) + A^{\\top}\\lambda = 0 \\implies Hx + g + A^{\\top}\\lambda = 0$\n2. Primal Feasibility: $Ax=b$\n\nThis forms a system of linear equations:\n$$\\begin{pmatrix} H & A^{\\top} \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -g \\\\ b \\end{pmatrix}$$\nSubstituting the given matrices and vectors:\n$$\\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 4 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix}$$\nThis represents the system of equations:\n(i) $2x_1 + \\lambda = -1$\n(ii) $4x_2 + \\lambda = 1$\n(iii) $x_1 + x_2 = 2$\n\nFrom (iii), $x_2 = 2-x_1$. From (i), $\\lambda = -1 - 2x_1$. Substituting these into (ii):\n$$4(2-x_1) + (-1-2x_1) = 1$$\n$$8 - 4x_1 - 1 - 2x_1 = 1$$\n$$7 - 6x_1 = 1$$\n$$6x_1 = 6 \\implies x_1 = 1$$\nThen, $x_2 = 2 - 1 = 1$.\nThe optimal primal solution is $x^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThis result is identical to the one obtained via the null-space method, confirming the correctness of our solution. The corresponding optimal Lagrange multiplier is $\\lambda = -1 - 2(1) = -3$.\n\nThe final answer is the optimal primal vector $x^{\\star}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 1 \\end{pmatrix}}$$", "id": "3158284"}, {"introduction": "This practice problem shifts our focus to the fundamental linear algebra that makes the null-space method possible. Here, we consider a problem with homogeneous constraints ($Ax=0$), meaning the feasible set is a subspace, not just an affine space. This simplification allows us to concentrate on the core task of constructing a basis for the null space, $\\mathcal{N}(A)$, by explicitly finding the orthogonal complement of the range of $A^\\top$. This exercise reinforces the essential connection between optimization theory and linear algebra, demonstrating how geometric insights about vector spaces can be used to systematically simplify and solve constrained problems [@problem_id:3158300].", "problem": "Consider the equality-constrained optimization problem in $\\mathbb{R}^{3}$ defined by the quadratic objective and homogeneous linear constraints\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\;\\; f(x) \\equiv \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\quad \\text{subject to} \\quad A x = 0,\n$$\nwhere\n$$\nQ = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}, \\qquad c = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\qquad A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}.\n$$\nUse null-space methods grounded in core linear algebra definitions. Specifically:\n- Starting from the definitions of the range space $\\mathcal{R}(A^{\\top})$ and its orthogonal complement, construct by hand an orthonormal basis for $\\mathcal{R}(A^{\\top})^{\\perp}$, and assemble the null-space basis matrix $Z$ whose columns form that orthonormal basis.\n- Reparameterize the feasible set by $x = Z y$ with $y \\in \\mathbb{R}$, derive the reduced unconstrained problem in $y$ from first principles, and solve it.\n- Report the minimal value of $f(x)$ attained under the constraint $A x = 0$.\n\nExpress the final answer as a single real number. No rounding is required.", "solution": "The problem is validated as a well-posed, scientifically grounded, and formally stated equality-constrained quadratic programming problem. All necessary data are provided, and the requested null-space method is a standard and appropriate technique for its solution. We may proceed.\n\nThe problem is to find the minimum of the objective function\n$$\nf(x) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x\n$$\nsubject to the linear constraint $A x = 0$, where $x \\in \\mathbb{R}^{3}$, and the given matrices are\n$$\nQ = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}, \\qquad c = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\qquad A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}.\n$$\nThe Hessian matrix $Q$ is diagonal with positive entries $2$, $3$, and $4$, which means it is positive definite. A quadratic function with a positive definite Hessian is strictly convex. The feasible set, defined by $Ax=0$, is a non-empty, closed, and convex set (a subspace of $\\mathbb{R}^{3}$). The problem of minimizing a strictly convex function over such a set has a unique solution.\n\nWe will employ the null-space method as specified. The feasible set consists of all vectors $x$ satisfying $A x = 0$, which is the definition of the null space of $A$, denoted $\\mathcal{N}(A)$. According to the fundamental theorem of linear algebra, the null space of a matrix is the orthogonal complement of the range of its transpose: $\\mathcal{N}(A) = \\mathcal{R}(A^{\\top})^{\\perp}$.\n\nThe first step is to construct an orthonormal basis for $\\mathcal{N}(A)$. The transpose of $A$ is\n$$\nA^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nThe columns of $A^{\\top}$, which are $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$, are linearly independent and thus form a basis for the range space $\\mathcal{R}(A^{\\top})$. A vector $z = \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix}$ belongs to $\\mathcal{N}(A) = \\mathcal{R}(A^{\\top})^{\\perp}$ if and only if it is orthogonal to both $v_1$ and $v_2$. This gives the system of equations:\n$$\nv_1^{\\top} z = 0 \\implies z_1 + z_2 = 0\n$$\n$$\nv_2^{\\top} z = 0 \\implies z_2 + z_3 = 0\n$$\nFrom the first equation, we have $z_1 = -z_2$. From the second equation, we have $z_3 = -z_2$. We can choose a value for $z_2$ to find a basis vector. Let $z_2 = -1$. Then $z_1 = 1$ and $z_3 = 1$. A basis vector for the null space $\\mathcal{N}(A)$ is therefore\n$$\nz_{base} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}.\n$$\nThe problem requires an orthonormal basis. The dimension of $\\mathcal{N}(A)$ is $1$, so we simply need to normalize this vector. The Euclidean norm is\n$$\n\\|z_{base}\\| = \\sqrt{1^2 + (-1)^2 + 1^2} = \\sqrt{3}.\n$$\nAn orthonormal basis for $\\mathcal{N}(A)$ is formed by the single vector $\\frac{1}{\\sqrt{3}} z_{base}$. The null-space basis matrix $Z$ is the $3 \\times 1$ matrix whose column is this basis vector:\n$$\nZ = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}.\n$$\nAny feasible vector $x$ can be expressed as a linear combination of the basis vectors of the null space. In this case, $x = Z y$ for some scalar $y \\in \\mathbb{R}$. We substitute this reparameterization into the objective function to obtain a reduced unconstrained problem in the variable $y$:\n$$\nf(x) = f(Zy) = \\frac{1}{2} (Zy)^{\\top} Q (Zy) + c^{\\top} (Zy)\n$$\n$$\ng(y) \\equiv f(Zy) = \\frac{1}{2} y^{\\top} (Z^{\\top} Q Z) y + (c^{\\top} Z) y.\n$$\nSince $y$ is a scalar, this simplifies to\n$$\ng(y) = \\frac{1}{2} (Z^{\\top} Q Z) y^2 + (c^{\\top} Z) y.\n$$\nWe now compute the coefficients of this quadratic function of $y$. The reduced Hessian is the $1 \\times 1$ matrix (a scalar) $Z^{\\top} Q Z$:\n$$\nZ^{\\top} Q Z = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 & -1 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} \\right)\n$$\n$$\nZ^{\\top} Q Z = \\frac{1}{3} \\begin{pmatrix} 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -3 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\n$$\nZ^{\\top} Q Z = \\frac{1}{3} (2 \\cdot 1 + (-3) \\cdot (-1) + 4 \\cdot 1) = \\frac{1}{3}(2+3+4) = \\frac{9}{3} = 3.\n$$\nThe reduced gradient term is the scalar $c^{\\top} Z$:\n$$\nc^{\\top} Z = \\begin{pmatrix} 1 & -2 & 3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{3}}(1 \\cdot 1 + (-2) \\cdot (-1) + 3 \\cdot 1) = \\frac{1}{\\sqrt{3}}(1+2+3) = \\frac{6}{\\sqrt{3}} = 2\\sqrt{3}.\n$$\nThe reduced unconstrained problem is to minimize the function $g(y)$:\n$$\ng(y) = \\frac{1}{2}(3) y^2 + (2\\sqrt{3}) y = \\frac{3}{2} y^2 + 2\\sqrt{3} y.\n$$\nTo find the minimum, we compute the derivative of $g(y)$ with respect to $y$ and set it to $0$:\n$$\ng'(y) = \\frac{d}{dy} \\left(\\frac{3}{2} y^2 + 2\\sqrt{3} y\\right) = 3y + 2\\sqrt{3}.\n$$\nSetting $g'(y) = 0$ gives the optimal value $y^*$:\n$$\n3y^* + 2\\sqrt{3} = 0 \\implies 3y^* = -2\\sqrt{3} \\implies y^* = -\\frac{2\\sqrt{3}}{3}.\n$$\nThe second derivative is $g''(y) = 3 > 0$, confirming that $y^*$ corresponds to a minimum.\n\nThe minimal value of the original objective function $f(x)$ is the value of the reduced function $g(y)$ at $y^*$:\n$$\nf_{min} = g(y^*) = \\frac{3}{2} (y^*)^2 + 2\\sqrt{3} y^*.\n$$\nSubstituting the value of $y^*$:\n$$\nf_{min} = \\frac{3}{2} \\left(-\\frac{2\\sqrt{3}}{3}\\right)^2 + 2\\sqrt{3} \\left(-\\frac{2\\sqrt{3}}{3}\\right)\n$$\n$$\nf_{min} = \\frac{3}{2} \\left(\\frac{4 \\cdot 3}{9}\\right) - \\frac{4 \\cdot (\\sqrt{3})^2}{3} = \\frac{3}{2} \\left(\\frac{12}{9}\\right) - \\frac{12}{3}\n$$\n$$\nf_{min} = \\frac{3}{2} \\left(\\frac{4}{3}\\right) - 4 = \\frac{12}{6} - 4 = 2-4 = -2.\n$$\nThe minimal value of $f(x)$ subject to the constraint $Ax=0$ is $-2$.", "answer": "$$\n\\boxed{-2}\n$$", "id": "3158300"}, {"introduction": "While systematic procedures like QR factorization can always generate a null-space basis, true mastery involves developing an intuition for a problem's underlying structure. This exercise presents a problem where the constraints have a special, decoupled form. This structure invites you to construct a basis for the null space not through a generic algorithm, but through direct observation and insight. By representing the degrees of freedom as simple differences between variables, you can build a natural and intuitive basis, transforming the problem in a way that is both efficient and conceptually elegant [@problem_id:3158219].", "problem": "Consider the equality-constrained quadratic optimization problem in $\\mathbb{R}^{4}$:\nminimize $f(x) = \\tfrac{1}{2}\\,x^{\\top} H x$ subject to $A x = b$, where $H \\in \\mathbb{R}^{4 \\times 4}$ is diagonal, $A \\in \\mathbb{R}^{2 \\times 4}$ enforces sums of disjoint coordinate pairs, and $b \\in \\mathbb{R}^{2}$ sets the pairwise sums to specified values. Specifically, let\n$$\nH = \\operatorname{diag}(2,4,1,3), \\quad\nA = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\[4pt] 0 & 0 & 1 & 1 \\end{pmatrix}, \\quad\nb = \\begin{pmatrix} 2 \\\\[4pt] 2 \\end{pmatrix}.\n$$\nUsing only the definitions of the Lagrangian for equality-constrained problems and the null space of a matrix as the starting point, proceed as follows:\n\n- Construct a feasible particular solution $x_{p}$ satisfying $A x_{p} = b$.\n- Derive explicitly a full-column-rank matrix $Z \\in \\mathbb{R}^{4 \\times 2}$ whose columns form a basis of the null space of $A$, choosing basis vectors as coordinate differences that naturally encode the constraints on sums.\n- Express all feasible $x$ in the form $x = x_{p} + Z z$ with $z \\in \\mathbb{R}^{2}$, form the reduced quadratic in $z$, and solve analytically for the unique minimizer $z^{\\star}$.\n- Recover the corresponding optimizer $x^{\\star}$ and evaluate the optimal objective value $f(x^{\\star})$.\n\nReport only the exact value of $f(x^{\\star})$ as a reduced fraction. No rounding is required and no units are involved.", "solution": "We begin from first principles for equality-constrained quadratic optimization. The objective is the quadratic function $f(x) = \\tfrac{1}{2}\\,x^{\\top} H x$ with a symmetric positive definite matrix $H$. The constraints are linear equalities $A x = b$. The method of Lagrange multipliers introduces the Lagrangian\n$$\n\\mathcal{L}(x,\\lambda) = \\tfrac{1}{2}\\,x^{\\top} H x + \\lambda^{\\top}(A x - b),\n$$\nand the Karush–Kuhn–Tucker (KKT) first-order optimality conditions are\n$$\nH x + A^{\\top} \\lambda = 0, \\quad A x = b.\n$$\nAn equivalent approach, called the null-space method, parameterizes all feasible points via a particular feasible point and a basis of the null space of $A$. We follow this route.\n\nStep 1: Construct a feasible particular solution $x_{p}$.\nThe constraints are $x_{1} + x_{2} = 2$ and $x_{3} + x_{4} = 2$. A convenient choice is to split each sum evenly:\n$$\nx_{p} = \\begin{pmatrix} 1 \\\\[4pt] 1 \\\\[4pt] 1 \\\\[4pt] 1 \\end{pmatrix},\n$$\nwhich satisfies $A x_{p} = b$.\n\nStep 2: Derive a null-space basis $Z$ as differences of coordinates.\nThe null space $\\mathcal{N}(A)$ consists of vectors $v \\in \\mathbb{R}^{4}$ with $A v = 0$, i.e., vectors satisfying $v_{1} + v_{2} = 0$ and $v_{3} + v_{4} = 0$. Natural basis vectors are coordinate differences within each pair:\n$$\nz_{1} = \\begin{pmatrix} 1 \\\\[4pt] -1 \\\\[4pt] 0 \\\\[4pt] 0 \\end{pmatrix}, \\quad\nz_{2} = \\begin{pmatrix} 0 \\\\[4pt] 0 \\\\[4pt] 1 \\\\[4pt] -1 \\end{pmatrix}.\n$$\nCollect these as columns of\n$$\nZ = \\begin{pmatrix}\n1 & 0 \\\\\n-1 & 0 \\\\\n0 & 1 \\\\\n0 & -1\n\\end{pmatrix}.\n$$\nOne verifies $A Z = 0$, so $Z$ has columns that span $\\mathcal{N}(A)$.\n\nStep 3: Parameterize feasibility and form the reduced problem.\nEvery feasible $x$ can be written as $x = x_{p} + Z z$ with $z \\in \\mathbb{R}^{2}$. Substituting into $f$ yields the reduced quadratic in $z$:\n$$\n\\phi(z) \\equiv f(x_{p} + Z z) = \\tfrac{1}{2}\\,(x_{p} + Z z)^{\\top} H (x_{p} + Z z).\n$$\nExpanding and using symmetry of $H$,\n$$\n\\phi(z) = \\tfrac{1}{2}\\,z^{\\top}(Z^{\\top} H Z) z + (Z^{\\top} H x_{p})^{\\top} z + \\tfrac{1}{2}\\,x_{p}^{\\top} H x_{p}.\n$$\nThe reduced Hessian is $Z^{\\top} H Z \\in \\mathbb{R}^{2 \\times 2}$ and, because $H$ is positive definite and $Z$ has full column rank, $Z^{\\top} H Z$ is positive definite. The unique minimizer $z^{\\star}$ satisfies the first-order condition\n$$\n\\nabla \\phi(z) = (Z^{\\top} H Z)\\,z + Z^{\\top} H x_{p} = 0,\n$$\nso\n$$\nz^{\\star} = -\\,(Z^{\\top} H Z)^{-1} Z^{\\top} H x_{p}.\n$$\n\nStep 4: Compute $Z^{\\top} H Z$ and $Z^{\\top} H x_{p}$, solve for $z^{\\star}$, recover $x^{\\star}$.\nFirst compute $H Z$ by acting $H = \\operatorname{diag}(2,4,1,3)$ on the columns of $Z$:\n$$\nH z_{1} = \\begin{pmatrix} 2 \\\\[4pt] -4 \\\\[4pt] 0 \\\\[4pt] 0 \\end{pmatrix}, \\quad\nH z_{2} = \\begin{pmatrix} 0 \\\\[4pt] 0 \\\\[4pt] 1 \\\\[4pt] -3 \\end{pmatrix}.\n$$\nTherefore,\n$$\nZ^{\\top} H Z\n= \\begin{pmatrix}\nz_{1}^{\\top} H z_{1} & z_{1}^{\\top} H z_{2} \\\\\nz_{2}^{\\top} H z_{1} & z_{2}^{\\top} H z_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 0 \\\\\n0 & 4\n\\end{pmatrix}.\n$$\nNext compute $H x_{p}$:\n$$\nH x_{p} = \\operatorname{diag}(2,4,1,3)\\begin{pmatrix} 1 \\\\[4pt] 1 \\\\[4pt] 1 \\\\[4pt] 1 \\end{pmatrix}\n= \\begin{pmatrix} 2 \\\\[4pt] 4 \\\\[4pt] 1 \\\\[4pt] 3 \\end{pmatrix}.\n$$\nThen\n$$\nZ^{\\top} H x_{p}\n= \\begin{pmatrix}\nz_{1}^{\\top} H x_{p} \\\\\nz_{2}^{\\top} H x_{p}\n\\end{pmatrix}\n= \\begin{pmatrix}\n(1)(2) + (-1)(4) + (0)(1) + (0)(3) \\\\\n(0)(2) + (0)(4) + (1)(1) + (-1)(3)\n\\end{pmatrix}\n= \\begin{pmatrix} -2 \\\\[4pt] -2 \\end{pmatrix}.\n$$\nThus,\n$$\nz^{\\star} = - \\begin{pmatrix} 6 & 0 \\\\ 0 & 4 \\end{pmatrix}^{-1} \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n= - \\begin{pmatrix} \\tfrac{1}{6} & 0 \\\\ 0 & \\tfrac{1}{4} \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{1}{3} \\\\[4pt] \\tfrac{1}{2} \\end{pmatrix}.\n$$\nRecover $x^{\\star} = x_{p} + Z z^{\\star}$:\n$$\nx^{\\star} = \\begin{pmatrix} 1 \\\\[4pt] 1 \\\\[4pt] 1 \\\\[4pt] 1 \\end{pmatrix}\n+ \\begin{pmatrix}\n1 & 0 \\\\\n-1 & 0 \\\\\n0 & 1 \\\\\n0 & -1\n\\end{pmatrix}\n\\begin{pmatrix} \\tfrac{1}{3} \\\\[4pt] \\tfrac{1}{2} \\end{pmatrix}\n= \\begin{pmatrix}\n1 + \\tfrac{1}{3} \\\\\n1 - \\tfrac{1}{3} \\\\\n1 + \\tfrac{1}{2} \\\\\n1 - \\tfrac{1}{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\tfrac{4}{3} \\\\[4pt] \\tfrac{2}{3} \\\\[4pt] \\tfrac{3}{2} \\\\[4pt] \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n\nStep 5: Evaluate the optimal objective value $f(x^{\\star})$.\nCompute $x^{\\star \\top} H x^{\\star}$ using $H = \\operatorname{diag}(2,4,1,3)$:\n$$\nx^{\\star \\top} H x^{\\star}\n= 2\\left(\\tfrac{4}{3}\\right)^{2} + 4\\left(\\tfrac{2}{3}\\right)^{2} + 1\\left(\\tfrac{3}{2}\\right)^{2} + 3\\left(\\tfrac{1}{2}\\right)^{2}\n= \\tfrac{32}{9} + \\tfrac{16}{9} + \\tfrac{9}{4} + \\tfrac{3}{4}.\n$$\nCombine terms:\n$$\n\\tfrac{32}{9} + \\tfrac{16}{9} = \\tfrac{48}{9} = \\tfrac{16}{3}, \\quad\n\\tfrac{9}{4} + \\tfrac{3}{4} = \\tfrac{12}{4} = 3,\n$$\nso\n$$\nx^{\\star \\top} H x^{\\star} = \\tfrac{16}{3} + 3 = \\tfrac{25}{3}.\n$$\nTherefore,\n$$\nf(x^{\\star}) = \\tfrac{1}{2}\\,x^{\\star \\top} H x^{\\star} = \\tfrac{1}{2} \\cdot \\tfrac{25}{3} = \\tfrac{25}{6}.\n$$\nBecause $Z^{\\top} H Z$ is positive definite, this minimizer is unique and the computed value is the unique optimal objective value.", "answer": "$$\\boxed{\\tfrac{25}{6}}$$", "id": "3158219"}]}