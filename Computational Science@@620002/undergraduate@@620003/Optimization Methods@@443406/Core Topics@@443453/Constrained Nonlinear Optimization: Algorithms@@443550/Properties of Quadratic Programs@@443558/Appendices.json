{"hands_on_practices": [{"introduction": "Finding a point that satisfies the Karush-Kuhn-Tucker (KKT) conditions is a crucial first step, but it doesn't guarantee a minimum. This exercise provides hands-on practice with the second-order sufficient conditions, which allow us to confirm if a KKT point is a strict local minimizer. You will calculate the reduced Hessian and analyze its properties, a fundamental skill for characterizing solutions in constrained optimization. [@problem_id:3166499]", "problem": "Consider the following equality-constrained quadratic program (QP): minimize the quadratic objective\n$$f(x) \\;=\\; \\tfrac{1}{2}\\,x^{\\top} H x \\;+\\; q^{\\top} x,$$\nwhere $x \\in \\mathbb{R}^{3}$, the Hessian matrix is\n$$H \\;=\\; \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix},$$\nand the linear term is\n$$q \\;=\\; \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix}.$$\nThe constraints are linear equalities given by\n$$A x \\;=\\; b, \\quad \\text{with} \\quad A \\;=\\; \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad b \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.$$\n\nStarting from the first principles of constrained optimization, use the Karush–Kuhn–Tucker (KKT) conditions to identify the KKT point $(x^{\\star},\\lambda^{\\star})$ and the tangent space of feasible directions at $x^{\\star}$. Construct any matrix $Z \\in \\mathbb{R}^{3 \\times 1}$ whose single column is an orthonormal basis vector for the null space of the active constraints (that is, for the null space of $A$). Using the Lagrangian Hessian (which for a QP equals $H$), form the reduced Hessian\n$$R \\;=\\; Z^{\\top} H Z.$$\n\nThen, use the second-order sufficient conditions to assess whether $x^{\\star}$ is a strict local minimizer by examining the definiteness of $R$. Finally, compute the unique eigenvalue of the reduced Hessian $R$ associated with this problem. Provide your final answer as the exact value of this eigenvalue. No rounding is required.", "solution": "The problem presented is a valid equality-constrained quadratic program (QP). We can proceed by applying the Karush-Kuhn-Tucker (KKT) conditions to find the optimal solution.\n\nThe objective function to minimize is $f(x) = \\frac{1}{2}x^{\\top}Hx + q^{\\top}x$, subject to the linear equality constraints $Ax = b$. The given matrices and vectors are:\n$$H = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}, \\quad q = \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nwhere $x \\in \\mathbb{R}^{3}$.\n\nThe Lagrangian function for this constrained optimization problem is:\n$$L(x, \\lambda) = f(x) + \\lambda^{\\top}(Ax - b) = \\frac{1}{2}x^{\\top}Hx + q^{\\top}x + \\lambda^{\\top}(Ax - b)$$\nwhere $\\lambda \\in \\mathbb{R}^{2}$ is the vector of Lagrange multipliers.\n\nThe first-order necessary conditions for optimality, the KKT conditions, are found by setting the gradient of the Lagrangian with respect to $x$ to zero, and ensuring the constraints are satisfied.\n$1$. Stationarity: $\\nabla_{x} L(x, \\lambda) = Hx + q + A^{\\top}\\lambda = 0$\n$2$. Primal Feasibility: $Ax = b$\n\nThese two conditions form a system of linear equations in $x$ and $\\lambda$. We can write this as a single block matrix equation:\n$$ \\begin{pmatrix} H & A^{\\top} \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -q \\\\ b \\end{pmatrix} $$\nSubstituting the given matrices and vectors, where $A^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ and $-q = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$:\n$$ \\begin{pmatrix} 4 & 1 & 0 & 1 & 0 \\\\ 1 & 3 & 0 & 1 & 0 \\\\ 0 & 0 & 2 & 0 & 1 \\\\ 1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\nFrom the last two rows (primal feasibility constraints):\n- $x_1 + x_2 = 1$\n- $x_3 = 0$\n\nFrom the third row of the system:\n$2x_3 + \\lambda_2 = 0$. Substituting $x_3 = 0$, we get $2(0) + \\lambda_2 = 0$, which implies $\\lambda_2 = 0$.\n\nNow we use the first two rows and the constraint $x_1 + x_2 = 1$:\n$1$. $4x_1 + x_2 + \\lambda_1 = 1$\n$2$. $x_1 + 3x_2 + \\lambda_1 = -2$\n\nSubstitute $x_2 = 1 - x_1$ into the first equation:\n$4x_1 + (1 - x_1) + \\lambda_1 = 1 \\implies 3x_1 + 1 + \\lambda_1 = 1 \\implies 3x_1 + \\lambda_1 = 0$, so $\\lambda_1 = -3x_1$.\n\nSubstitute $x_2 = 1 - x_1$ and $\\lambda_1 = -3x_1$ into the second equation:\n$x_1 + 3(1 - x_1) + (-3x_1) = -2 \\implies x_1 + 3 - 3x_1 - 3x_1 = -2 \\implies -5x_1 + 3 = -2 \\implies -5x_1 = -5 \\implies x_1 = 1$.\n\nNow we find the remaining variables:\n$x_2 = 1 - x_1 = 1 - 1 = 0$.\n$\\lambda_1 = -3x_1 = -3(1) = -3$.\n\nThe KKT point $(x^{\\star}, \\lambda^{\\star})$ is therefore:\n$$ x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\lambda^{\\star} = \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} $$\n\nNext, we determine the tangent space of feasible directions at $x^{\\star}$. For linear equality constraints, this is the null space of the matrix $A$, denoted $\\text{Null}(A)$. We seek vectors $z \\in \\mathbb{R}^{3}$ such that $Az = 0$.\n$$ \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the system of equations:\n$z_1 + z_2 = 0 \\implies z_1 = -z_2$\n$z_3 = 0$\nAny vector in the null space can be written as $z = \\begin{pmatrix} -t \\\\ t \\\\ 0 \\end{pmatrix} = t \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ for some scalar $t \\in \\mathbb{R}$.\nThe null space is one-dimensional, and a basis vector is $v = \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\nThe problem requires constructing a matrix $Z \\in \\mathbb{R}^{3 \\times 1}$ whose column is an orthonormal basis for $\\text{Null}(A)$. We normalize the basis vector $v$:\n$\\|v\\| = \\sqrt{(-1)^2 + 1^2 + 0^2} = \\sqrt{2}$.\nThe orthonormal basis vector is $\\frac{v}{\\|v\\|} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nSo, the matrix $Z$ is:\n$$ Z = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n\nNow, we form the reduced Hessian $R = Z^{\\top} H Z$. This is a $1 \\times 1$ matrix.\nFirst, we compute the product $HZ$:\n$$ HZ = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 4(-1) + 1(1) \\\\ 1(-1) + 3(1) \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -3 \\\\ 2 \\\\ 0 \\end{pmatrix} $$\nNext, we compute $Z^{\\top} (HZ)$:\n$$ R = Z^{\\top}HZ = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 & 1 & 0 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -3 \\\\ 2 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{2} ((-1)(-3) + (1)(2) + (0)(0)) = \\frac{1}{2}(3+2) = \\frac{5}{2} $$\nThe reduced Hessian is the $1 \\times 1$ matrix $R = [\\frac{5}{2}]$.\n\nThe second-order sufficient condition for a strict local minimizer states that the reduced Hessian must be positive definite. Since the only entry of $R$ is $\\frac{5}{2} > 0$, $R$ is positive definite. This confirms that $x^{\\star}$ is a strict local minimizer.\n\nFinally, we are asked to find the unique eigenvalue of the reduced Hessian $R$. For a $1 \\times 1$ matrix $[a]$, the eigenvalue is simply the a itself.\nThus, the eigenvalue of $R = [\\frac{5}{2}]$ is $\\frac{5}{2}$.", "answer": "$$\\boxed{\\frac{5}{2}}$$", "id": "3166499"}, {"introduction": "Not all optimization problems have a solution; some are infeasible because their constraints are contradictory. This practice explores how the concept of duality provides a powerful tool to rigorously prove infeasibility. By constructing the dual of an infeasible Quadratic Program, you will derive a Farkas-type certificate, offering a deep insight into the geometric relationship between the primal and dual problems. [@problem_id:3166488]", "problem": "Consider the following convex Quadratic Program (QP), where the objective is strictly convex and the linear inequality constraints are potentially conflicting:\nMinimize over $x \\in \\mathbb{R}^{2}$ the function $f(x) = \\tfrac{1}{2}\\|x\\|_{2}^{2}$ subject to the two linear inequalities\n$x_{1} + x_{2} \\leq 0$ and $-x_{1} - x_{2} \\leq -1$.\nLet $A \\in \\mathbb{R}^{2 \\times 2}$ and $b \\in \\mathbb{R}^{2}$ denote the matrix and vector collecting the constraints in the standard form $A x \\leq b$, with the first row corresponding to $x_{1} + x_{2} \\leq 0$ and the second row corresponding to $-x_{1} - $x_{2} \\leq -1$.\n\nTasks:\n1) Starting from the fundamental definition of the Lagrangian and the Lagrangian dual function, derive the dual problem for this QP. Use the weak duality principle to argue the (in)feasibility of the primal, and identify a feasible ray of dual variables along which the dual objective grows without bound. From this, extract a Farkas-type certificate of infeasibility, that is, a nonzero vector $y \\in \\mathbb{R}^{2}$ satisfying $y \\geq 0$, $A^{\\top} y = 0$, and $b^{\\top} y < 0$.\n\n2) To remove the scaling ambiguity of the certificate, impose the normalization $b^{\\top} y = -1$ and minimize $\\|y\\|_{2}$ over all such certificates. Compute the unique resulting vector $y$ explicitly.\n\n3) Interpret the certificate geometrically in terms of the two half-spaces defined by the constraints and the conic combination of their outward normals.\n\nAnswer specification: Provide only the vector from Task $2$ as your final answer, written as a single row vector. No rounding is required.", "solution": "The problem asks for the analysis of an infeasible convex Quadratic Program (QP). We must first validate the problem statement, then proceed with the tasks if it is valid. The problem is mathematically well-defined and internally consistent; the apparent conflict in the constraints is intentional and central to the problem, which focuses on generating a certificate of infeasibility. The problem is therefore valid.\n\nThe QP is given by:\nMinimize $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2} = \\frac{1}{2}x^{\\top}x$ over $x \\in \\mathbb{R}^{2}$, subject to the constraints:\n$1. \\quad x_{1} + x_{2} \\leq 0$\n$2. \\quad -x_{1} - x_{2} \\leq -1$\n\nThese constraints can be written in the standard form $Ax \\leq b$. The first row of $A$ and $b$ corresponds to the first constraint, and the second row to the second constraint.\nThe matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and vector $b \\in \\mathbb{R}^{2}$ are:\n$$ A = \\begin{pmatrix} 1 & 1 \\\\ -1 & -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} $$\nThe second constraint, $-x_{1} - x_{2} \\leq -1$, is equivalent to $x_{1} + x_{2} \\geq 1$. The feasible set requires $x_1+x_2$ to be simultaneously less than or equal to $0$ and greater than or equal to $1$, which is impossible. Thus, the primal problem is infeasible.\n\n**Task 1: Dual Problem and Certificate of Infeasibility**\n\nWe begin by constructing the Lagrangian for the QP. Let $y = (y_1, y_2)^{\\top} \\in \\mathbb{R}^2$ be the vector of Lagrange multipliers. The Lagrangian is:\n$$ L(x, y) = f(x) + y^{\\top}(Ax - b) = \\frac{1}{2}x^{\\top}x + y^{\\top}Ax - y^{\\top}b $$\nThe domain of the Lagrangian is $x \\in \\mathbb{R}^2$ and $y \\in \\mathbb{R}^2$ with the implicit constraint that for the dual problem, we require $y \\geq 0$ (component-wise, i.e., $y_1 \\ge 0, y_2 \\ge 0$).\n\nThe Lagrangian dual function $g(y)$ is the infimum of $L(x, y)$ over $x$:\n$$ g(y) = \\inf_{x \\in \\mathbb{R}^2} L(x, y) $$\nSince $L(x, y)$ is a strictly convex quadratic function of $x$, its infimum is attained where its gradient with respect to $x$ is zero:\n$$ \\nabla_x L(x, y) = x + A^{\\top}y = 0 $$\nThis gives the unique minimizer $x^{*}(y) = -A^{\\top}y$. Substituting this back into the Lagrangian gives the dual function:\n$$ g(y) = \\frac{1}{2}(-A^{\\top}y)^{\\top}(-A^{\\top}y) + y^{\\top}A(-A^{\\top}y) - y^{\\top}b $$\n$$ g(y) = \\frac{1}{2}y^{\\top}A A^{\\top}y - y^{\\top}A A^{\\top}y - y^{\\top}b $$\n$$ g(y) = -\\frac{1}{2}y^{\\top}A A^{\\top}y - b^{\\top}y $$\nThe dual problem is to maximize $g(y)$ subject to the non-negativity constraint on the multipliers:\n$$ \\max_{y \\geq 0} \\left( -\\frac{1}{2}y^{\\top}A A^{\\top}y - b^{\\top}y \\right) $$\nLet's compute the matrix $A A^{\\top}$:\n$$ A^{\\top} = \\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix} $$\n$$ A A^{\\top} = \\begin{pmatrix} 1 & 1 \\\\ -1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix} $$\nAccording to the weak duality principle, the optimal value of the dual problem provides a lower bound on the optimal value of the primal problem. As the primal problem is infeasible, its optimal value is formally $+\\infty$. By the strong duality theorem for QPs, if the primal is infeasible, the dual problem is either infeasible or unbounded. Since $y=0$ is a feasible point for the dual, the dual must be unbounded, meaning its optimal value is $+\\infty$.\n\nAn unbounded dual objective suggests there exists a ray $\\{y_0 + s d \\mid s \\to \\infty\\}$ with $d \\ge 0$ along which $g(y)$ increases indefinitely. For the dual objective to be unbounded, there must be a direction $d \\neq 0$ such that $d \\geq 0$, $A A^{\\top}d$ is related to $d$ in a special way, and $-b^{\\top}d > 0$. More precisely, if we substitute $y=sd$ for $s>0$, $g(sd) = -s^2 (\\frac{1}{2} d^\\top AA^\\top d) - s(b^\\top d)$. For this to go to $+\\infty$ as $s \\to \\infty$, the quadratic term must be non-positive, and the linear term must be positive. This requires $d^\\top A A^\\top d=0$ and $b^\\top d < 0$.\nThe condition $d^\\top A A^\\top d = \\|A^\\top d\\|_2^2 = 0$ implies $A^\\top d = 0$.\nSo, we seek a direction $d$ which is a Farkas-type certificate of infeasibility, satisfying:\n$1. \\quad d \\ge 0$\n$2. \\quad A^{\\top}d = 0$\n$3. \\quad b^{\\top}d < 0$\n\nLet's find such a vector, which we call $y$ as in the problem statement. From $A^{\\top}y = 0$:\n$$ \\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} y_1 - y_2 \\\\ y_1 - y_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis implies $y_1 = y_2$. So, any such vector $y$ must be of the form $y = (s, s)^{\\top}$ for some scalar $s$.\nThe condition $y \\geq 0$ requires $s \\ge 0$. Since we are looking for a nonzero vector, we need $s > 0$.\nThe condition $b^{\\top}y < 0$ becomes:\n$$ b^{\\top}y = \\begin{pmatrix} 0 & -1 \\end{pmatrix} \\begin{pmatrix} s \\\\ s \\end{pmatrix} = -s < 0 $$\nThis is true for any $s > 0$.\nThus, any vector $y = (s, s)^{\\top}$ with $s > 0$ is a Farkas-type certificate of infeasibility. The ray of dual variables is $y = (s,s)^{\\top}$ for $s > 0$. Along this ray, $A^\\top y = 0$, so $AA^\\top y = 0$, and the dual objective is $g(y) = -b^\\top y = s$, which grows to $\\infty$ as $s \\to \\infty$.\n\n**Task 2: Normalized Certificate**\n\nWe are asked to find a unique certificate by imposing the normalization $b^{\\top}y = -1$ and then minimizing $\\|y\\|_2$.\nThe set of certificates is described by $y \\ge 0$, $A^\\top y=0$. As found, this is the set $\\{ (s,s)^\\top \\mid s > 0 \\}$.\nWe impose the normalization condition on a vector $y = (s, s)^{\\top}$:\n$$ b^{\\top}y = -s = -1 \\implies s = 1 $$\nThis normalization selects a unique vector from the ray of certificates: $y = (1, 1)^{\\top}$.\nLet's verify this vector:\n- $y = (1, 1)^{\\top} \\ge 0$: True.\n- $A^{\\top}y = \\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$: True.\n- $b^{\\top}y = \\begin{pmatrix} 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = -1$: True, and this is $<0$.\n\nThe problem then asks to minimize $\\|y\\|_2$ over all certificates satisfying the normalization. Since the set of vectors $y$ fulfilling ($y \\ge 0, A^\\top y = 0, b^\\top y = -1$) is a singleton set containing only the vector $(1, 1)^{\\top}$, the minimization is trivial. The unique vector that results is $y = (1, 1)^{\\top}$.\n\n**Task 3: Geometric Interpretation**\n\nThe two linear inequalities define two closed half-spaces in $\\mathbb{R}^2$:\n$H_1: \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\le 0 \\}$\n$H_2: \\{ x \\in \\mathbb{R}^2 \\mid -x_1 - x_2 \\le -1 \\} \\equiv \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 1 \\}$\nThe boundary of $H_1$ is the line $x_1+x_2 = 0$, and the boundary of $H_2$ is the line $x_1+x_2 = 1$. These are parallel lines. The feasible set of the QP is the intersection $H_1 \\cap H_2$, which is empty.\n\nA Farkas certificate $y=(y_1, y_2)^\\top$ provides weights for a linear combination of the inequalities that demonstrates this emptiness. The conditions for the certificate are $y \\ge 0$, $A^\\top y = \\sum_{i=1}^2 y_i a_i = 0$, and $b^\\top y = \\sum_{i=1}^2 y_i b_i < 0$. Here, $a_1=(1,1)^\\top$ and $a_2=(-1,-1)^\\top$ are the normal vectors to the constraint boundaries.\nThe certificate $y=(1,1)^\\top$ satisfies these conditions:\n- $y_1=1 \\ge 0, y_2=1 \\ge 0$.\n- $A^\\top y = 1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This shows that a non-negative combination of the normal vectors (which point away from their respective half-spaces) sums to zero. Geometrically, the normals are equal and opposite.\n- $b^\\top y = 1 \\cdot (0) + 1 \\cdot (-1) = -1 < 0$.\n\nIf we assume a feasible point $x$ exists, then $a_1^\\top x \\le b_1$ and $a_2^\\top x \\le b_2$. Multiplying by $y_1=1$ and $y_2=1$ respectively and summing gives: $y_1(a_1^\\top x) + y_2(a_2^\\top x) \\le y_1 b_1 + y_2 b_2$. This simplifies to $(y_1 a_1 + y_2 a_2)^\\top x \\le b^\\top y$. Using the certificate properties, this becomes $(0)^\\top x \\le -1$, or $0 \\le -1$. This is a contradiction, thereby proving the infeasibility of the primal problem. The certificate $y = (1, 1)^{\\top}$ corresponds to simply adding the two original inequalities, which directly yields the contradiction $0 \\le -1$.\n\nThe final answer is the explicit vector from Task 2.\nThe calculated vector is $y = (1, 1)^{\\top}$. The required output format is a row vector.", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 & 1 \\end{pmatrix} } $$", "id": "3166488"}, {"introduction": "In many practical applications, the data defining an optimization problem is not fixed but can vary. This exercise delves into sensitivity analysis, exploring how the optimal solution of a quadratic program responds to changes in the constraint vector. By deriving the optimizer as an explicit function of the problem data, you will learn to calculate the solution's sensitivity, a key aspect in understanding the robustness of an optimal decision. [@problem_id:3179267]", "problem": "Consider the equality-constrained quadratic program with decision variable $x \\in \\mathbb{R}^{3}$:\n$$\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} x^{\\top} Q x \\quad \\text{subject to} \\quad A x = b,$$\nwhere $Q \\in \\mathbb{R}^{3 \\times 3}$ is symmetric positive definite, $A \\in \\mathbb{R}^{2 \\times 3}$ has full row rank, and $b \\in \\mathbb{R}^{2}$. Let\n$$Q = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}.$$\nDefine the perturbation path $b(\\varepsilon) = b + \\varepsilon d$ with direction $d = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ and scalar parameter $\\varepsilon \\in \\mathbb{R}$.\n\nStarting from first principles—specifically, the properties of strictly convex quadratic functions and the Karush–Kuhn–Tucker (KKT) optimality conditions for equality-constrained convex programs—derive the optimizer $x^{\\star}(b)$ as a function of the right-hand side $b$. Use this derivation to determine the linear sensitivity of $x^{\\star}(b(\\varepsilon))$ with respect to $\\varepsilon$ at $\\varepsilon = 0$.\n\nCompute the directional derivative $\\frac{d}{d\\varepsilon} \\, x^{\\star}_{1}(b(\\varepsilon)) \\big|_{\\varepsilon=0}$, where $x^{\\star}_{1}$ denotes the first component of $x^{\\star}$. Express your final answer exactly (no rounding).", "solution": "The user wants to find the sensitivity of the optimal solution to an equality-constrained quadratic program with respect to a perturbation in the right-hand side of the constraints.\n\n### Step 1: Extract Givens\nThe problem is an equality-constrained quadratic program (QP):\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} x^{\\top} Q x \\quad \\text{subject to} \\quad A x = b $$\nThe given data are:\n- Decision variable: $x \\in \\mathbb{R}^{3}$.\n- Objective function matrix: $Q = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$. It is specified that $Q$ is symmetric positive definite.\n- Constraint matrix: $A = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}$. It is specified that $A$ has full row rank.\n- Constraint right-hand side vector: $b = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} \\in \\mathbb{R}^{2}$.\n- Perturbation path: $b(\\varepsilon) = b + \\varepsilon d$, with direction $d = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ and scalar parameter $\\varepsilon \\in \\mathbb{R}$.\n- Quantity to compute: The directional derivative $\\frac{d}{d\\varepsilon} \\, x^{\\star}_{1}(b(\\varepsilon)) \\big|_{\\varepsilon=0}$, where $x^{\\star}_{1}$ is the first component of the optimizer $x^{\\star}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard equality-constrained quadratic program. The objective function $f(x) = \\frac{1}{2} x^{\\top} Q x$ is strictly convex because $Q$ is symmetric positive definite. The constraints are affine. This is a convex optimization problem. The use of Karush–Kuhn–Tucker (KKT) conditions is the fundamental method for solving such problems. The concept of sensitivity analysis with respect to problem data is a standard topic in optimization theory. The problem is scientifically sound.\n2.  **Well-Posed**: Since the objective function is strictly convex and the feasible set defined by $Ax=b$ is non-empty and convex (it's an affine subspace), a unique optimal solution $x^{\\star}$ exists for any given $b$. The matrix $A$ has full row rank, which ensures the linear independence of the constraints. The problem of finding the sensitivity of the unique solution with respect to a perturbation in $b$ is well-defined.\n3.  **Objective**: The problem is stated in precise mathematical terms, with all variables, matrices, and objectives clearly defined. There is no subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, and objective. I will proceed with the solution.\n\n### Solution Derivation\n\nThe problem is to solve\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} x^{\\top} Q x \\quad \\text{subject to} \\quad A x = b $$\nThis is a convex optimization problem with linear equality constraints. The Karush–Kuhn–Tucker (KKT) conditions are necessary and sufficient for optimality. We begin by forming the Lagrangian, with $\\nu \\in \\mathbb{R}^2$ as the vector of Lagrange multipliers for the constraints $Ax=b$:\n$$ L(x, \\nu) = \\frac{1}{2} x^{\\top} Q x - \\nu^{\\top}(Ax - b) $$\nThe first-order KKT conditions are obtained by setting the gradient of the Lagrangian with respect to $x$ to zero (stationarity) and by enforcing the original constraints (primal feasibility):\n$$ \\nabla_x L(x^{\\star}, \\nu^{\\star}) = Qx^{\\star} - A^{\\top}\\nu^{\\star} = 0 $$\n$$ \\nabla_\\nu L(x^{\\star}, \\nu^{\\star}) = -(Ax^{\\star} - b) = 0 \\implies Ax^{\\star} = b $$\nThis forms a system of linear equations for the optimal primal-dual pair $(x^{\\star}, \\nu^{\\star})$:\n$$ \\begin{pmatrix} Q & -A^{\\top} \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} x^{\\star} \\\\ \\nu^{\\star} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix} $$\nFrom the first equation, $Qx^{\\star} = A^{\\top}\\nu^{\\star}$. Since $Q$ is symmetric positive definite, it is invertible. Thus, we can express $x^{\\star}$ in terms of $\\nu^{\\star}$:\n$$ x^{\\star} = Q^{-1}A^{\\top}\\nu^{\\star} $$\nSubstituting this into the second equation, $Ax^{\\star}=b$:\n$$ A(Q^{-1}A^{\\top}\\nu^{\\star}) = b $$\nThe matrix $S = AQ^{-1}A^{\\top}$ is the Schur complement of $Q$ in the KKT matrix. Since $A$ has full row rank and $Q^{-1}$ is positive definite, $S$ is symmetric and positive definite, and therefore invertible. We can solve for $\\nu^{\\star}$:\n$$ \\nu^{\\star}(b) = (AQ^{-1}A^{\\top})^{-1}b $$\nSubstituting this back into the expression for $x^{\\star}$, we obtain the optimizer $x^{\\star}$ as an explicit function of the right-hand side $b$:\n$$ x^{\\star}(b) = Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}b $$\nThe expression for $x^{\\star}(b)$ is linear in $b$. Now, we introduce the perturbed right-hand side $b(\\varepsilon) = b + \\varepsilon d$. The optimizer as a function of $\\varepsilon$ is:\n$$ x^{\\star}(b(\\varepsilon)) = Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}(b + \\varepsilon d) $$\n$$ x^{\\star}(b(\\varepsilon)) = Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}b + \\varepsilon \\left( Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}d \\right) $$\nTo find the linear sensitivity, we take the derivative with respect to $\\varepsilon$:\n$$ \\frac{d}{d\\varepsilon} x^{\\star}(b(\\varepsilon)) = Q^{-1}A^{\\top}(AQ^{-1}A^{\\top})^{-1}d $$\nThis derivative is constant with respect to $\\varepsilon$, so its value at $\\varepsilon=0$ is the same. Let's denote this derivative vector as $\\dot{x}^{\\star}$. We now compute this vector using the given numerical values.\n\nFirst, we compute the required matrices:\n$Q = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\implies Q^{-1} = \\begin{pmatrix} 1/2 & 0 & 0 \\\\ 0 & 1/3 & 0 \\\\ 0 & 0 & 1/5 \\end{pmatrix}$\n$A = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\implies A^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 0 & 1 \\end{pmatrix}$\n$d = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n\nNext, we compute the matrix $S = AQ^{-1}A^{\\top}$:\n$$ AQ^{-1}A^{\\top} = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 & 0 & 0 \\\\ 0 & 1/3 & 0 \\\\ 0 & 0 & 1/5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} 1/2 & 2/3 & 0 \\\\ 0 & 1/3 & 1/5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} \\frac{1}{2}(1) + \\frac{2}{3}(2) & \\frac{2}{3}(1) \\\\ \\frac{1}{3}(2) & \\frac{1}{3}(1) + \\frac{1}{5}(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} + \\frac{4}{3} & \\frac{2}{3} \\\\ \\frac{2}{3} & \\frac{1}{3} + \\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3+8}{6} & \\frac{2}{3} \\\\ \\frac{2}{3} & \\frac{5+3}{15} \\end{pmatrix} = \\begin{pmatrix} 11/6 & 2/3 \\\\ 2/3 & 8/15 \\end{pmatrix} $$\nNow, we compute the inverse $S^{-1} = (AQ^{-1}A^{\\top})^{-1}$. The determinant of $S$ is:\n$$ \\det(S) = \\left(\\frac{11}{6}\\right)\\left(\\frac{8}{15}\\right) - \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right) = \\frac{88}{90} - \\frac{4}{9} = \\frac{44}{45} - \\frac{20}{45} = \\frac{24}{45} = \\frac{8}{15} $$\nThe inverse is:\n$$ S^{-1} = \\frac{1}{8/15} \\begin{pmatrix} 8/15 & -2/3 \\\\ -2/3 & 11/6 \\end{pmatrix} = \\frac{15}{8} \\begin{pmatrix} 8/15 & -2/3 \\\\ -2/3 & 11/6 \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{15 \\cdot 2}{8 \\cdot 3} \\\\ -\\frac{15 \\cdot 2}{8 \\cdot 3} & \\frac{15 \\cdot 11}{8 \\cdot 6} \\end{pmatrix} = \\begin{pmatrix} 1 & -5/4 \\\\ -5/4 & 55/16 \\end{pmatrix} $$\nNow, we can compute the sensitivity of the Lagrange multipliers, $\\dot{\\nu}^{\\star} = S^{-1}d$:\n$$ \\dot{\\nu}^{\\star} = \\begin{pmatrix} 1 & -5/4 \\\\ -5/4 & 55/16 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(1) + (-\\frac{5}{4})(-2) \\\\ (-\\frac{5}{4})(1) + (\\frac{55}{16})(-2) \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{5}{2} \\\\ -\\frac{5}{4} - \\frac{55}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{10}{8} - \\frac{55}{8} \\end{pmatrix} = \\begin{pmatrix} 7/2 \\\\ -65/8 \\end{pmatrix} $$\nFinally, we compute the sensitivity of the primal variable, $\\dot{x}^{\\star} = Q^{-1}A^{\\top}\\dot{\\nu}^{\\star}$:\n$$ \\dot{x}^{\\star} = \\begin{pmatrix} 1/2 & 0 & 0 \\\\ 0 & 1/3 & 0 \\\\ 0 & 0 & 1/5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 7/2 \\\\ -65/8 \\end{pmatrix} = \\begin{pmatrix} 1/2 & 0 \\\\ 2/3 & 1/3 \\\\ 0 & 1/5 \\end{pmatrix} \\begin{pmatrix} 7/2 \\\\ -65/8 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} (\\frac{1}{2})(\\frac{7}{2}) \\\\ (\\frac{2}{3})(\\frac{7}{2}) + (\\frac{1}{3})(-\\frac{65}{8}) \\\\ (\\frac{1}{5})(-\\frac{65}{8}) \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{4} \\\\ \\frac{7}{3} - \\frac{65}{24} \\\\ -\\frac{13}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{4} \\\\ \\frac{56}{24} - \\frac{65}{24} \\\\ -\\frac{13}{8} \\end{pmatrix} = \\begin{pmatrix} 7/4 \\\\ -9/24 \\\\ -13/8 \\end{pmatrix} = \\begin{pmatrix} 7/4 \\\\ -3/8 \\\\ -13/8 \\end{pmatrix} $$\nThe vector of sensitivities is $\\frac{d}{d\\varepsilon} x^{\\star}(b(\\varepsilon)) \\big|_{\\varepsilon=0} = \\begin{pmatrix} 7/4 \\\\ -3/8 \\\\ -13/8 \\end{pmatrix}$.\nThe problem asks for the first component of this vector, which is $\\frac{d}{d\\varepsilon} \\, x^{\\star}_{1}(b(\\varepsilon)) \\big|_{\\varepsilon=0}$.\n$$ \\frac{d}{d\\varepsilon} \\, x^{\\star}_{1}(b(\\varepsilon)) \\big|_{\\varepsilon=0} = \\frac{7}{4} $$", "answer": "$$\\boxed{\\frac{7}{4}}$$", "id": "3179267"}]}