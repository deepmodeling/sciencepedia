{"hands_on_practices": [{"introduction": "To truly grasp the Augmented Lagrangian method, let's start with the basics. This first exercise [@problem_id:2208360] guides you through a single, complete iteration for a simple one-dimensional problem. By manually calculating the updated variable $x_1$ and the new multiplier estimate $\\lambda_1$, you will gain a concrete understanding of the core two-step process: solving the primal subproblem and then updating the dual variable.", "problem": "Consider the constrained optimization problem of minimizing the objective function $f(x) = x^2$ subject to the equality constraint $h(x) = x - 3 = 0$.\n\nThe augmented Lagrangian method is an iterative algorithm for solving such problems. The augmented Lagrangian function is defined as:\n$$L_A(x, \\lambda; \\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2} [h(x)]^2$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\rho > 0$ is the penalty parameter.\n\nA single iteration of the method, starting from an estimate $\\lambda_k$, consists of two main steps:\n1.  Find the next iterate $x_{k+1}$ by solving the unconstrained minimization problem:\n    $$x_{k+1} = \\arg\\min_{x} L_A(x, \\lambda_k; \\rho)$$\n2.  Update the Lagrange multiplier using the formula:\n    $$\\lambda_{k+1} = \\lambda_k + \\rho h(x_{k+1})$$\n\nPerform one full iteration of the augmented Lagrangian method starting with an initial multiplier estimate $\\lambda_0 = 1$ and using a penalty parameter $\\rho = 2$. Determine the resulting values for the new iterate $x_1$ and the updated multiplier $\\lambda_1$.\n\nExpress your answer as a row matrix $\\begin{pmatrix} x_1 & \\lambda_1 \\end{pmatrix}$ using exact fractions.", "solution": "We are given $f(x) = x^{2}$ and $h(x) = x - 3$, with augmented Lagrangian\n$$L_{A}(x,\\lambda;\\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^{2}.$$\nWith $\\lambda_{0} = 1$ and $\\rho = 2$, this becomes\n$$L_{A}(x,1;2) = x^{2} + 1\\cdot(x-3) + \\frac{2}{2}(x-3)^{2} = x^{2} + x - 3 + (x-3)^{2}.$$\nTo obtain $x_{1}$, solve the unconstrained minimization:\n$$x_{1} = \\arg\\min_{x} L_{A}(x,1;2).$$\nDifferentiate and set the derivative to zero:\n$$\\frac{d}{dx}L_{A}(x,1;2) = 2x + 1 + 2(x-3) = 4x - 5,$$\n$$4x - 5 = 0 \\implies x_{1} = \\frac{5}{4}.$$\nThe second derivative is\n$$\\frac{d^{2}}{dx^{2}}L_{A}(x,1;2) = 4 > 0,$$\nso $x_{1} = \\frac{5}{4}$ is the unique minimizer.\n\nUpdate the multiplier using $\\lambda_{1} = \\lambda_{0} + \\rho h(x_{1})$:\n$$h(x_{1}) = \\frac{5}{4} - 3 = -\\frac{7}{4},$$\n$$\\lambda_{1} = 1 + 2\\left(-\\frac{7}{4}\\right) = 1 - \\frac{7}{2} = -\\frac{5}{2}.$$\n\nThus, the resulting values are $x_{1} = \\frac{5}{4}$ and $\\lambda_{1} = -\\frac{5}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{4} & -\\frac{5}{2} \\end{pmatrix}}$$", "id": "2208360"}, {"introduction": "The heart of each Augmented Lagrangian iteration lies in solving an unconstrained minimization subproblem. This practice [@problem_id:2208379] isolates that crucial first step for a two-dimensional problem, allowing you to focus on the mechanics of finding the minimizer $x^{(1)}$. You will see firsthand how the penalty parameter $\\rho$ directly influences the solution of this subproblem, a key element in steering the overall algorithm toward feasibility.", "problem": "Consider the optimization problem of minimizing the function $f(x_1, x_2) = x_1^2 + x_2^2$ subject to the linear equality constraint $x_1 + x_2 - 2 = 0$.\n\nThis problem can be addressed using the Augmented Lagrangian Method. For a general optimization problem with objective function $f(x)$ and an equality constraint $h(x) = 0$, the augmented Lagrangian is defined as:\n$$L_\\rho(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^2$$\nwhere $\\lambda$ is the estimate of the Lagrange multiplier and $\\rho > 0$ is a positive penalty parameter.\n\nThe method involves a sequence of unconstrained minimization subproblems. Starting with an initial multiplier estimate $\\lambda_0$, one finds the vector $x^{(1)}$ that minimizes $L_\\rho(x, \\lambda_0)$.\n\nYour task is to solve this first subproblem. Given an initial Lagrange multiplier estimate $\\lambda_0 = 0$, find the vector $x^{(1)} = (x_1^{(1)}, x_2^{(1)})$ that minimizes the corresponding augmented Lagrangian. Express your answer as a row vector in terms of the penalty parameter $\\rho$.", "solution": "We are given $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ and the equality constraint $h(x) = x_{1} + x_{2} - 2 = 0$. The augmented Lagrangian with parameter $\\rho > 0$ is\n$$\nL_{\\rho}(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}\\left[h(x)\\right]^{2}.\n$$\nWith the initial multiplier estimate $\\lambda_{0} = 0$, the first subproblem is the unconstrained minimization of\n$$\nL_{\\rho}(x, 0) = x_{1}^{2} + x_{2}^{2} + \\frac{\\rho}{2}\\left(x_{1} + x_{2} - 2\\right)^{2}.\n$$\nTo find its minimizer, set the gradient to zero. Let $s = x_{1} + x_{2} - 2$. Then\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{1}} = 2x_{1} + \\rho s = 2x_{1} + \\rho(x_{1} + x_{2} - 2) = 0,\n$$\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{2}} = 2x_{2} + \\rho s = 2x_{2} + \\rho(x_{1} + x_{2} - 2) = 0.\n$$\nThese yield the linear system\n$$\n(2+\\rho)x_{1} + \\rho x_{2} = 2\\rho, \\qquad \\rho x_{1} + (2+\\rho)x_{2} = 2\\rho.\n$$\nBy symmetry, the solution satisfies $x_{1} = x_{2} = t$. Substituting gives\n$$\n2t + \\rho(2t - 2) = 0 \\;\\;\\Longrightarrow\\;\\; (2 + 2\\rho)t = 2\\rho \\;\\;\\Longrightarrow\\;\\; t = \\frac{\\rho}{1+\\rho}.\n$$\nHence $x_{1}^{(1)} = x_{2}^{(1)} = \\frac{\\rho}{1+\\rho}$. The Hessian of $L_{\\rho}(x,0)$ is $2I + \\rho\\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix}$, which is positive definite for $\\rho > 0$, so this stationary point is the unique global minimizer.\n\nTherefore,\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{\\rho}{1+\\rho} & \\frac{\\rho}{1+\\rho} \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{1+\\rho} & \\frac{\\rho}{1+\\rho}\\end{pmatrix}}$$", "id": "2208379"}, {"introduction": "Moving from manual calculation to robust implementation, a critical aspect of the Method of Multipliers is managing the penalty parameter $\\rho$. A poorly chosen $\\rho$ can lead to slow convergence or numerical instability. This advanced practice [@problem_id:3099716] challenges you to implement and compare two different penalty update strategies, demonstrating how an adaptive rule that balances primal and dual progress can lead to a more efficient and reliable solver.", "problem": "Consider the linear optimization problem with equality constraints: minimize the linear objective $f(x)=c^{\\top}x$ subject to $Ax=b$, where $A\\in\\mathbb{R}^{m\\times n}$ has full column rank, $b\\in\\mathbb{R}^{m}$, and $c\\in\\mathbb{R}^{n}$. The Method of Multipliers (also known as the augmented Lagrangian method) iteratively enforces feasibility by combining the classical Lagrangian with a quadratic penalty on the constraint violation. The algorithm uses a penalty parameter $\\rho_k>0$ and a Lagrange multiplier vector $\\lambda^k\\in\\mathbb{R}^{m}$ at iteration $k$.\n\nFundamental base and definitions to use:\n- The classical Lagrangian for equality constraints is $L(x,\\lambda)=f(x)+\\lambda^{\\top}(Ax-b)$.\n- The augmented Lagrangian adds a quadratic penalty on the constraint violation, leading to a strongly convex inner minimization in $x$ when $A$ has full column rank.\n- The primal residual is defined as the constraint violation $r^{k+1}=Ax^{k+1}-b$, and the dual change is measured by $s^{k+1}=\\lambda^{k+1}-\\lambda^{k}$.\n\nYou must design and implement two penalty update strategies for the penalty parameter $\\rho_k$:\n1. A multiplicative update rule $\\rho_{k+1}=\\tau\\rho_k$ with a fixed $\\tau>0$.\n2. An adaptive rule based on the norms $\\|r^{k+1}\\|_2=\\|Ax^{k+1}-b\\|_2$ and $\\|s^{k+1}\\|_2=\\|\\lambda^{k+1}-\\lambda^{k}\\|_2$, which increases or decreases $\\rho_k$ depending on the relative magnitudes of these norms.\n\nYour program must:\n- Implement the Method of Multipliers for the specified problem, performing at each iteration: minimize the augmented Lagrangian with respect to $x$ given the current $\\lambda^k$ and $\\rho_k$, update the multiplier $\\lambda^{k+1}$, compute the residuals, and update $\\rho_{k+1}$ according to the chosen rule.\n- Use a stopping criterion that terminates when both $\\|Ax^{k}-b\\|_2\\leq \\varepsilon_{\\text{primal}}$ and $\\|\\lambda^{k}-\\lambda^{k-1}\\|_2\\leq \\varepsilon_{\\text{dual}}$, or when a maximum number of iterations is reached.\n- For numerical stability, if needed, you may bound $\\rho_k$ within a reasonable interval to avoid extreme values.\n\nTest suite:\nUse the following fixed problem data:\n- $A=\\begin{bmatrix}1 & 1 \\\\ 1 & -1\\end{bmatrix}$,\n- $b=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$,\n- $c=\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$.\n\nUse tolerance values $\\varepsilon_{\\text{primal}}=10^{-8}$ and $\\varepsilon_{\\text{dual}}=10^{-8}$, and a maximum iteration count of $1000$.\n\nEvaluate the following five test cases, each defined by its penalty update configuration and initial penalty:\n- Case 1 (multiplicative): $\\rho_0=0.05$, $\\tau=2.0$.\n- Case 2 (multiplicative, no change): $\\rho_0=1.0$, $\\tau=1.0$.\n- Case 3 (adaptive): $\\rho_0=0.05$, increase factor $\\tau_{\\text{inc}}=2.0$, decrease factor $\\tau_{\\text{dec}}=0.5$, threshold $\\gamma=10.0$, and clip $\\rho_k$ to $[10^{-6},10^{6}]$.\n- Case 4 (adaptive, more aggressive thresholds): $\\rho_0=0.001$, $\\tau_{\\text{inc}}=1.5$, $\\tau_{\\text{dec}}=0.7$, $\\gamma=2.0$, and clip $\\rho_k$ to $[10^{-6},10^{6}]$.\n- Case 5 (multiplicative, large initial penalty, no change): $\\rho_0=100.0$, $\\tau=1.0$.\n\nFor each test case, your program should return a list of three values: the number of iterations taken to meet the stopping criterion (an integer), the final penalty value $\\rho_{\\text{final}}$ (a float), and the final primal residual norm $\\|Ax^{\\text{final}}-b\\|_2$ (a float).\n\nFinal output format:\nYour program should produce a single line of output containing the results for all five test cases as a comma-separated list of lists enclosed in square brackets, for example, \"[[iters1,rho1,res1],[iters2,rho2,res2],...]\" where each inner list corresponds to one test case in the order specified above. No physical units are involved, and angles are not present.", "solution": "The problem is valid as it is scientifically grounded in established principles of convex optimization, well-posed with a unique solution, and formally specified with all necessary data and constraints.\n\nThe problem asks for the implementation of the Method of Multipliers, also known as the Augmented Lagrangian Method, to solve a linear program with equality constraints. The core of the method is to iteratively solve an unconstrained minimization problem and then update a set of Lagrange multipliers. The objective is to minimize a linear function $f(x)=c^{\\top}x$ subject to the linear equality constraints $Ax=b$.\n\nThe augmented Lagrangian function, $L_{\\rho}(x, \\lambda)$, combines the standard Lagrangian with a quadratic penalty term for the constraint violation. For this problem, it is defined as:\n$$L_{\\rho}(x, \\lambda) = f(x) + \\lambda^{\\top}(Ax - b) + \\frac{\\rho}{2}\\|Ax - b\\|_2^2$$\nwhere $\\lambda$ is the vector of Lagrange multipliers and $\\rho > 0$ is the penalty parameter.\n\nThe Method of Multipliers proceeds in iterations, indexed by $k$. At each iteration $k$, given the current multiplier estimate $\\lambda^k$ and penalty parameter $\\rho_k$, the method involves two main steps:\n\n**1. Primal Variable Update ($x$-minimization):**\nThe first step is to find the vector $x^{k+1}$ that minimizes the augmented Lagrangian $L_{\\rho_k}(x, \\lambda^k)$ with respect to $x$:\n$$x^{k+1} = \\arg\\min_x L_{\\rho_k}(x, \\lambda^k) = \\arg\\min_x \\left( c^{\\top}x + (\\lambda^k)^{\\top}(Ax - b) + \\frac{\\rho_k}{2}\\|Ax - b\\|_2^2 \\right)$$\nSince $L_{\\rho_k}(x, \\lambda^k)$ is a strictly convex function of $x$ (as $A$ has full column rank, $A^{\\top}A$ is positive definite, making the Hessian of the quadratic term positive definite), we can find the minimizer by setting the gradient with respect to $x$ to zero:\n$$\\nabla_x L_{\\rho_k}(x, \\lambda^k) = c + A^{\\top}\\lambda^k + \\rho_k A^{\\top}(Ax - b) = 0$$\nRearranging the terms, we obtain a linear system of equations for $x^{k+1}$:\n$$(\\rho_k A^{\\top}A) x^{k+1} = \\rho_k A^{\\top}b - c - A^{\\top}\\lambda^k$$\nThis system can be solved for $x^{k+1}$ at each iteration. Given the problem data, $A = \\begin{bmatrix}1 & 1 \\\\ 1 & -1\\end{bmatrix}$, we find that $A^{\\top}A = \\begin{bmatrix}2 & 0 \\\\ 0 & 2\\end{bmatrix} = 2I$. This simplifies the linear system significantly, but the general form is used in the implementation for correctness.\n\n**2. Dual Variable Update ($\\lambda$-update):**\nAfter computing $x^{k+1}$, the Lagrange multiplier vector is updated using the standard first-order rule:\n$$\\lambda^{k+1} = \\lambda^k + \\rho_k (Ax^{k+1} - b)$$\nThis update rule can be interpreted as a dual ascent step on the dual function associated with the augmented Lagrangian.\n\n**3. Penalty Parameter Update ($\\rho$-update):**\nThe performance of the method is sensitive to the choice of the penalty parameter $\\rho_k$. The problem requires implementing two update strategies for $\\rho_{k+1}$ based on the outcomes of iteration $k+1$. The primal residual is $r^{k+1} = Ax^{k+1} - b$ and the dual change is $s^{k+1} = \\lambda^{k+1} - \\lambda^k$.\n\n- **Multiplicative Rule:** This is a simple, pre-determined update schedule where the penalty parameter is scaled by a fixed factor $\\tau > 0$ at each step:\n$$\\rho_{k+1} = \\tau\\rho_k$$\nIf $\\tau > 1$, $\\rho_k$ grows, which tends to enforce primal feasibility more strongly. If $\\tau = 1$, the penalty is constant.\n\n- **Adaptive Rule:** This strategy adjusts $\\rho_k$ based on the relative progress of primal feasibility and dual variable change. The goal is to balance the reduction of the primal residual norm, $\\|r^{k+1}\\|_2$, and the dual change norm, $\\|s^{k+1}\\|_2$. The rule is as follows, using parameters $\\tau_{\\text{inc}}$, $\\tau_{\\text{dec}}$, and $\\gamma$:\n  - If $\\|r^{k+1}\\|_2 > \\gamma \\|s^{k+1}\\|_2$, the primal residual is considered too large, so the penalty is increased: $\\rho_{k+1} = \\tau_{\\text{inc}}\\rho_k$.\n  - If $\\|s^{k+1}\\|_2 > \\gamma \\|r^{k+1}\\|_2$, the dual variable is changing significantly, suggesting the penalty might be too high, so it is decreased: $\\rho_{k+1} = \\tau_{\\text{dec}}\\rho_k$.\n  - Otherwise, the penalty parameter remains unchanged: $\\rho_{k+1} = \\rho_k$.\nTo prevent $\\rho_k$ from becoming excessively large or small, it is clipped to a predefined range $[\\rho_{\\min}, \\rho_{\\max}]$.\n\n**Algorithm and Termination:**\nThe overall algorithm is as follows:\n1. Initialize $k=0$, $\\lambda^0 = \\mathbf{0}$, and $\\rho_0$.\n2. For $k=0, 1, 2, \\dots$ up to a maximum number of iterations:\n   a. Solve $(\\rho_k A^{\\top}A) x^{k+1} = \\rho_k A^{\\top}b - c - A^{\\top}\\lambda^k$ for $x^{k+1}$.\n   b. Update $\\lambda^{k+1} = \\lambda^k + \\rho_k (Ax^{k+1} - b)$.\n   c. Calculate the primal residual norm $\\|r^{k+1}\\|_2 = \\|Ax^{k+1}-b\\|_2$ and the dual change norm $\\|s^{k+1}\\|_2 = \\|\\lambda^{k+1}-\\lambda^k\\|_2$.\n   d. Check for convergence: if $\\|r^{k+1}\\|_2 \\leq \\varepsilon_{\\text{primal}}$ and $\\|s^{k+1}\\|_2 \\leq \\varepsilon_{\\text{dual}}$, terminate.\n   e. Update $\\rho_{k+1}$ using either the multiplicative or adaptive rule.\nThe implementation will execute this algorithm for each of the five test cases defined in the problem, returning the number of iterations, the final penalty parameter value, and the final primal residual norm.", "answer": "```python\nimport numpy as np\n\ndef run_solver(A, b, c, eps_primal, eps_dual, max_iter, rule, rho0, tau=None, tau_inc=None, tau_dec=None, gamma=None, rho_min=None, rho_max=None):\n    \"\"\"\n    Implements the Method of Multipliers for a linear program.\n\n    Args:\n        A (np.ndarray): Constraint matrix.\n        b (np.ndarray): Constraint vector.\n        c (np.ndarray): Objective function vector.\n        eps_primal (float): Tolerance for the primal residual norm.\n        eps_dual (float): Tolerance for the dual change norm.\n        max_iter (int): Maximum number of iterations.\n        rule (str): Penalty update rule ('multiplicative' or 'adaptive').\n        rho0 (float): Initial penalty parameter.\n        tau (float, optional): Factor for multiplicative update.\n        tau_inc (float, optional): Increase factor for adaptive update.\n        tau_dec (float, optional): Decrease factor for adaptive update.\n        gamma (float, optional): Threshold for adaptive update.\n        rho_min (float, optional): Minimum value for rho in adaptive update.\n        rho_max (float, optional): Maximum value for rho in adaptive update.\n\n    Returns:\n        list: [number of iterations, final rho, final primal residual norm].\n    \"\"\"\n    m, n = A.shape\n    lam = np.zeros(m)\n    rho = rho0\n    \n    # Pre-compute constant matrices/vectors\n    AtA = A.T @ A\n    Atb = A.T @ b\n\n    x = np.zeros(n)\n    rho_used_in_iter = rho0\n\n    for k in range(1, max_iter + 1):\n        rho_used_in_iter = rho\n        lam_prev = np.copy(lam)\n\n        # Step 1: x-minimization\n        H = rho * AtA\n        q = rho * Atb - c - A.T @ lam_prev\n        x = np.linalg.solve(H, q)\n\n        # Step 2: lambda-update and residual calculation\n        primal_res_vec = A @ x - b\n        lam = lam_prev + rho * primal_res_vec\n        \n        primal_res_norm = np.linalg.norm(primal_res_vec)\n        dual_res_norm = np.linalg.norm(lam - lam_prev)\n\n        # Step 3: Check for convergence\n        if primal_res_norm = eps_primal and dual_res_norm = eps_dual:\n            return [k, rho_used_in_iter, primal_res_norm]\n\n        # Step 4: rho-update for the next iteration\n        if rule == 'multiplicative':\n            rho *= tau\n        elif rule == 'adaptive':\n            if primal_res_norm > gamma * dual_res_norm:\n                rho *= tau_inc\n            elif dual_res_norm > gamma * primal_res_norm:\n                rho *= tau_dec\n            \n            # Clip rho to the specified bounds\n            rho = np.clip(rho, rho_min, rho_max)\n\n    # Reached max iterations, return the values from the last iteration\n    final_primal_res_norm = np.linalg.norm(A @ x - b)\n    return [max_iter, rho_used_in_iter, final_primal_res_norm]\n\ndef solve():\n    \"\"\"\n    Sets up the problem and runs the specified test cases.\n    \"\"\"\n    # Fixed problem data\n    A = np.array([[1.0, 1.0], [1.0, -1.0]])\n    b = np.array([1.0, 0.0])\n    c = np.array([1.0, 2.0])\n    \n    # Algorithm parameters\n    eps_primal = 1e-8\n    eps_dual = 1e-8\n    max_iter = 1000\n\n    # Test suite configurations\n    test_cases = [\n        {\"rule\": \"multiplicative\", \"rho0\": 0.05, \"tau\": 2.0},\n        {\"rule\": \"multiplicative\", \"rho0\": 1.0, \"tau\": 1.0},\n        {\"rule\": \"adaptive\", \"rho0\": 0.05, \"tau_inc\": 2.0, \"tau_dec\": 0.5, \"gamma\": 10.0, \"rho_min\": 1e-6, \"rho_max\": 1e6},\n        {\"rule\": \"adaptive\", \"rho0\": 0.001, \"tau_inc\": 1.5, \"tau_dec\": 0.7, \"gamma\": 2.0, \"rho_min\": 1e-6, \"rho_max\": 1e6},\n        {\"rule\": \"multiplicative\", \"rho0\": 100.0, \"tau\": 1.0},\n    ]\n\n    results = []\n    for case_params in test_cases:\n        res = run_solver(A, b, c, eps_primal, eps_dual, max_iter, **case_params)\n        results.append(res)\n        \n    # Format the output as specified\n    formatted_results = [f\"[{item[0]},{item[1]},{item[2]}]\" for item in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3099716"}]}