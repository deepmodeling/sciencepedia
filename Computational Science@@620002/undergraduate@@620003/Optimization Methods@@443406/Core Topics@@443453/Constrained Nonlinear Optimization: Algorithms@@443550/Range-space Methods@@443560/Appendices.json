{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of range-space methods, we will first master the fundamental mechanics. This initial practice focuses on deriving the reduced system for the Lagrange multipliers directly from the first-order optimality (KKT) conditions of an equality-constrained quadratic program. By working through a concrete numerical example, you will solidify your understanding of how the Hessian matrix $H$ and the constraint matrix $A$ combine to form the range-space matrix $S = AH^{-1}A^{\\top}$, whose properties govern the solution for the multipliers [@problem_id:3171154].", "problem": "Consider the equality-constrained quadratic program with objective function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x}$ and equality constraints $A\\mathbf{x} = \\mathbf{b}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $A \\in \\mathbb{R}^{m \\times n}$ has full row rank. In the range-space method, one eliminates the primal variable to obtain a reduced system for the Lagrange multipliers whose coefficient matrix is denoted by $S$.\n\nConstruct a toy instance where $n = 5$ and $m = 2$ with a diagonally scaled Hessian $H$ and a constraints matrix $A$ that forms sums of components, specifically:\n- Let $H = \\operatorname{diag}(1,\\,0.1,\\,10,\\,2,\\,0.5)$.\n- Let $A \\in \\mathbb{R}^{2 \\times 5}$ have its first row equal to $[1,\\,1,\\,1,\\,0,\\,0]$ and its second row equal to $[0,\\,0,\\,1,\\,1,\\,1]$.\n\nStarting from first principles for equality-constrained quadratic optimization, derive the reduced multiplier system and compute the determinant of the matrix $S$ for this instance. Your final answer must be a single real-valued number. Provide the exact value; do not round.", "solution": "The problem is a standard equality-constrained quadratic program (QP), which is deemed valid as it is scientifically grounded, well-posed, and all necessary information to solve for the determinant of the matrix $S$ is provided.\n\nThe optimization problem is formulated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} \\\\\n\\text{subject to} \\quad & A\\mathbf{x} = \\mathbf{b}\n\\end{aligned}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^n$, $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, and $\\mathbf{b} \\in \\mathbb{R}^m$.\n\nThe Lagrangian function for this problem is:\n$$ \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} + \\boldsymbol{\\lambda}^{\\top}(A\\mathbf{x} - \\mathbf{b}) $$\nwhere $\\boldsymbol{\\lambda} \\in \\mathbb{R}^m$ is the vector of Lagrange multipliers.\n\nThe first-order necessary conditions for optimality (Karush-Kuhn-Tucker or KKT conditions) are obtained by setting the gradients of the Lagrangian with respect to $\\mathbf{x}$ and $\\boldsymbol{\\lambda}$ to zero:\n$$ \\nabla_{\\mathbf{x}} \\mathcal{L} = H\\mathbf{x} + A^{\\top}\\boldsymbol{\\lambda} = \\mathbf{0} $$\n$$ \\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L} = A\\mathbf{x} - \\mathbf{b} = \\mathbf{0} $$\nThis is a system of linear equations often written in matrix form as:\n$$ \\begin{pmatrix} H & A^{\\top} \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x} \\\\ \\boldsymbol{\\lambda} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{b} \\end{pmatrix} $$\n\nThe range-space method eliminates the primal variable $\\mathbf{x}$ to solve for the dual variable $\\boldsymbol{\\lambda}$ first. From the first KKT equation, $H\\mathbf{x} = -A^{\\top}\\boldsymbol{\\lambda}$. Since $H$ is given as symmetric positive definite, it is invertible. We can express $\\mathbf{x}$ as:\n$$ \\mathbf{x} = -H^{-1}A^{\\top}\\boldsymbol{\\lambda} $$\nSubstituting this expression for $\\mathbf{x}$ into the second KKT equation (the feasibility constraint) yields:\n$$ A(-H^{-1}A^{\\top}\\boldsymbol{\\lambda}) - \\mathbf{b} = \\mathbf{0} $$\n$$ -(AH^{-1}A^{\\top})\\boldsymbol{\\lambda} = \\mathbf{b} $$\nThis can be rewritten as a reduced system for the Lagrange multipliers:\n$$ (AH^{-1}A^{\\top})\\boldsymbol{\\lambda} = -\\mathbf{b} $$\nThe coefficient matrix of this reduced system is denoted by $S$. Therefore,\n$$ S = AH^{-1}A^{\\top} $$\nThe problem requires us to compute the determinant of this matrix, $\\det(S)$, for the given instance.\n\nThe given matrices are:\n- $H = \\operatorname{diag}(1,\\,0.1,\\,10,\\,2,\\,0.5)$. We can write the decimal values as fractions for exact computation: $H = \\operatorname{diag}(1, \\frac{1}{10}, 10, 2, \\frac{1}{2})$.\n- $A \\in \\mathbb{R}^{2 \\times 5}$ given by $A = \\begin{pmatrix} 1 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 1 \\end{pmatrix}$.\n\nFirst, we compute the inverse of $H$. Since $H$ is a diagonal matrix, its inverse $H^{-1}$ is a diagonal matrix whose diagonal elements are the reciprocals of the corresponding elements of $H$:\n$$ H^{-1} = \\operatorname{diag}(1^{-1}, (\\frac{1}{10})^{-1}, 10^{-1}, 2^{-1}, (\\frac{1}{2})^{-1}) = \\operatorname{diag}(1, 10, \\frac{1}{10}, \\frac{1}{2}, 2) $$\nNumerically, this is $H^{-1} = \\operatorname{diag}(1, 10, 0.1, 0.5, 2)$.\n\nNext, we compute the transpose of $A$:\n$$ A^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\n\nNow, we can compute the matrix $S = AH^{-1}A^{\\top}$. Let us compute the product $H^{-1}A^{\\top}$ first:\n$$ H^{-1}A^{\\top} = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 10 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{10} & 0 & 0 \\\\ 0 & 0 & 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 & 1 \\cdot 0 \\\\ 10 \\cdot 1 & 10 \\cdot 0 \\\\ \\frac{1}{10} \\cdot 1 & \\frac{1}{10} \\cdot 1 \\\\ \\frac{1}{2} \\cdot 0 & \\frac{1}{2} \\cdot 1 \\\\ 2 \\cdot 0 & 2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 10 & 0 \\\\ \\frac{1}{10} & \\frac{1}{10} \\\\ 0 & \\frac{1}{2} \\\\ 0 & 2 \\end{pmatrix} $$\n\nFinally, we pre-multiply this result by $A$ to obtain $S$:\n$$ S = A(H^{-1}A^{\\top}) = \\begin{pmatrix} 1 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 10 & 0 \\\\ \\frac{1}{10} & \\frac{1}{10} \\\\ 0 & \\frac{1}{2} \\\\ 0 & 2 \\end{pmatrix} $$\nThe resulting matrix $S$ is a $2 \\times 2$ matrix. Its elements are:\n$$ S_{11} = (1)(1) + (1)(10) + (1)(\\frac{1}{10}) + (0)(0) + (0)(0) = 1 + 10 + \\frac{1}{10} = 11 + \\frac{1}{10} = \\frac{111}{10} $$\n$$ S_{12} = (1)(0) + (1)(0) + (1)(\\frac{1}{10}) + (0)(\\frac{1}{2}) + (0)(2) = \\frac{1}{10} $$\n$$ S_{21} = (0)(1) + (0)(10) + (1)(\\frac{1}{10}) + (1)(0) + (1)(0) = \\frac{1}{10} $$\n$$ S_{22} = (0)(0) + (0)(0) + (1)(\\frac{1}{10}) + (1)(\\frac{1}{2}) + (1)(2) = \\frac{1}{10} + \\frac{5}{10} + \\frac{20}{10} = \\frac{26}{10} $$\nSo the matrix $S$ is:\n$$ S = \\begin{pmatrix} \\frac{111}{10} & \\frac{1}{10} \\\\ \\frac{1}{10} & \\frac{26}{10} \\end{pmatrix} $$\nThe determinant of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $ad - bc$. Thus, the determinant of $S$ is:\n$$ \\det(S) = \\left(\\frac{111}{10}\\right)\\left(\\frac{26}{10}\\right) - \\left(\\frac{1}{10}\\right)\\left(\\frac{1}{10}\\right) $$\n$$ \\det(S) = \\frac{111 \\times 26}{100} - \\frac{1}{100} $$\nLet's compute the product $111 \\times 26$:\n$$ 111 \\times 26 = 111 \\times (20 + 6) = 2220 + 666 = 2886 $$\nSubstituting this back into the determinant expression:\n$$ \\det(S) = \\frac{2886}{100} - \\frac{1}{100} = \\frac{2885}{100} = 28.85 $$\nThe determinant is an exact real-valued number.", "answer": "$$\\boxed{28.85}$$", "id": "3171154"}, {"introduction": "Having learned how to formulate the reduced system, we now investigate one of its most important numerical properties: conditioning. A common concern is whether an ill-conditioned Hessian $H$ will necessarily lead to an ill-conditioned reduced system. This practice explores this question through a carefully constructed thought experiment, which demonstrates that this is not always the case. You will see how the alignment of the constraint manifold with the eigenspaces of the Hessian can lead to a perfectly well-conditioned reduced system, even when the original problem's curvature is highly anisotropic [@problem_id:3171127].", "problem": "Consider the equality-constrained quadratic program minimize $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} H \\mathbf{x}$ subject to $A \\mathbf{x} = \\mathbf{b}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) and $A \\in \\mathbb{R}^{m \\times n}$ has full row rank. The range-space method reduces the Karush–Kuhn–Tucker (KKT) system to a linear system in the Lagrange multipliers. Starting only from the KKT optimality conditions and the definition of the two-norm condition number, carry out this reduction to obtain the reduced system and its system matrix. Then evaluate the conditioning of this reduced system for the following specific construction, which is designed so that $H$ is ill-conditioned but the reduced system matrix is well-conditioned due to alignment of the rows of $A$ with the dominant eigenvectors of $H$.\n\nConstruction: Let $n = 3$, $m = 2$, and choose\n$$\nH = \\begin{pmatrix}\n10^{6} & 0 & 0 \\\\\n0 & 10^{6} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}, \\quad\n\\mathbf{b} \\in \\mathbb{R}^{2} \\text{ arbitrary.}\n$$\nYou may assume $H$ is SPD and that the two-norm condition number of an SPD matrix $M$ is $\\kappa_{2}(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$, where $\\lambda_{\\max}(M)$ and $\\lambda_{\\min}(M)$ are the largest and smallest eigenvalues of $M$. No other specialized formulas may be used without derivation.\n\nTasks:\n1. Starting from the KKT conditions, eliminate the primal variable to obtain the reduced linear system in the multipliers and identify its system matrix.\n2. Using the construction above, compute the two-norm condition number of the reduced system matrix and explain, using eigenvector alignment, why it is well-conditioned even though $H$ is ill-conditioned.\n\nWhat is the value of the two-norm condition number of the reduced system matrix for this construction? Provide an exact value. Do not round. Express your final answer as a real number with no units.", "solution": "The problem asks for the derivation of the reduced system matrix for an equality-constrained quadratic program and the calculation of its condition number for a specific construction.\n\n### Step 1: Derivation of the Reduced System\n\nThe problem is an equality-constrained quadratic program (EQP) of the form:\n$$\n\\text{minimize } f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} H \\mathbf{x} \\quad \\text{subject to } A\\mathbf{x} = \\mathbf{b}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^{n}$, $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $A \\in \\mathbb{R}^{m \\times n}$ has full row rank ($m \\le n$), and $\\mathbf{b} \\in \\mathbb{R}^{m}$.\n\nThe Lagrangian function for this problem is:\n$$\nL(\\mathbf{x}, \\boldsymbol{\\lambda}) = f(\\mathbf{x}) + \\boldsymbol{\\lambda}^{\\top}(A\\mathbf{x} - \\mathbf{b}) = \\frac{1}{2} \\mathbf{x}^{\\top} H \\mathbf{x} + \\boldsymbol{\\lambda}^{\\top}A\\mathbf{x} - \\boldsymbol{\\lambda}^{\\top}\\mathbf{b}\n$$\nwhere $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{m}$ is the vector of Lagrange multipliers.\n\nThe first-order Karush–Kuhn–Tucker (KKT) optimality conditions are found by setting the gradients of the Lagrangian with respect to $\\mathbf{x}$ and $\\boldsymbol{\\lambda}$ to zero:\n1.  $\\nabla_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}) = H\\mathbf{x} + A^{\\top}\\boldsymbol{\\lambda} = 0$ (Stationarity)\n2.  $\\nabla_{\\boldsymbol{\\lambda}} L(\\mathbf{x}, \\boldsymbol{\\lambda}) = A\\mathbf{x} - \\mathbf{b} = 0$ (Primal Feasibility)\n\nThese two conditions form a block linear system, known as the KKT system:\n$$\n\\begin{pmatrix}\nH & A^{\\top} \\\\\nA & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{x} \\\\\n\\boldsymbol{\\lambda}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{0} \\\\\n\\mathbf{b}\n\\end{pmatrix}\n$$\nThe range-space method involves eliminating the primal variable $\\mathbf{x}$ to obtain a smaller system solely in terms of the dual variable $\\boldsymbol{\\lambda}$.\n\nFrom the stationarity condition, $H\\mathbf{x} + A^{\\top}\\boldsymbol{\\lambda} = 0$, we can express $\\mathbf{x}$ in terms of $\\boldsymbol{\\lambda}$. Since $H$ is given as SPD, it is invertible. Therefore:\n$$\nH\\mathbf{x} = -A^{\\top}\\boldsymbol{\\lambda}\n$$\n$$\n\\mathbf{x} = -H^{-1}A^{\\top}\\boldsymbol{\\lambda}\n$$\nNow, substitute this expression for $\\mathbf{x}$ into the primal feasibility condition, $A\\mathbf{x} = \\mathbf{b}$:\n$$\nA(-H^{-1}A^{\\top}\\boldsymbol{\\lambda}) = \\mathbf{b}\n$$\n$$\n-(AH^{-1}A^{\\top})\\boldsymbol{\\lambda} = \\mathbf{b}\n$$\n$$\n(AH^{-1}A^{\\top})\\boldsymbol{\\lambda} = -\\mathbf{b}\n$$\nThis is the reduced linear system in the Lagrange multipliers $\\boldsymbol{\\lambda}$. The system matrix, which we denote as $M_R$, is:\n$$\nM_R = AH^{-1}A^{\\top}\n$$\nThis matrix is an $m \\times m$ matrix. It is also SPD, since $H^{-1}$ is SPD (as $H$ is SPD) and $A$ has full row rank. This completes the first task.\n\n### Step 2: Analysis of the Specific Construction\n\nWe are given the following specific matrices for $n=3$ and $m=2$:\n$$\nH = \\begin{pmatrix}\n10^{6} & 0 & 0 \\\\\n0 & 10^{6} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n$$\nFirst, we compute the inverse of $H$. Since $H$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal entries:\n$$\nH^{-1} = \\begin{pmatrix}\n10^{-6} & 0 & 0 \\\\\n0 & 10^{-6} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nNext, we compute the reduced system matrix $M_R = AH^{-1}A^{\\top}$. The transpose of $A$ is:\n$$\nA^{\\top} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\n$$\nNow we perform the matrix multiplication:\n$$\nM_R = AH^{-1}A^{\\top} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n10^{-6} & 0 & 0 \\\\\n0 & 10^{-6} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\n$$\nFirst, compute the product $AH^{-1}$:\n$$\nAH^{-1} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n10^{-6} & 0 & 0 \\\\\n0 & 10^{-6} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n10^{-6} & 0 & 0 \\\\\n0 & 10^{-6} & 0\n\\end{pmatrix}\n$$\nNow, multiply by $A^{\\top}$:\n$$\nM_R = (AH^{-1})A^{\\top} = \\begin{pmatrix}\n10^{-6} & 0 & 0 \\\\\n0 & 10^{-6} & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n10^{-6} & 0 \\\\\n0 & 10^{-6}\n\\end{pmatrix}\n$$\nSo, the reduced system matrix is $M_R = 10^{-6} I_{2}$, where $I_2$ is the $2 \\times 2$ identity matrix.\n\nTo compute the two-norm condition number $\\kappa_2(M_R)$, we use the formula for an SPD matrix: $\\kappa_2(M_R) = \\lambda_{\\max}(M_R) / \\lambda_{\\min}(M_R)$. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $M_R$ are both $10^{-6}$.\n$$\n\\lambda_{\\max}(M_R) = 10^{-6}\n$$\n$$\n\\lambda_{\\min}(M_R) = 10^{-6}\n$$\nTherefore, the condition number is:\n$$\n\\kappa_2(M_R) = \\frac{10^{-6}}{10^{-6}} = 1\n$$\n\n### Explanation of Conditioning\n\nThe conditioning of $H$ itself is poor. The eigenvalues of $H$ are its diagonal entries: $\\lambda_1 = 10^6$, $\\lambda_2 = 10^6$, and $\\lambda_3 = 1$. The condition number of $H$ is:\n$$\n\\kappa_2(H) = \\frac{\\lambda_{\\max}(H)}{\\lambda_{\\min}(H)} = \\frac{10^6}{1} = 10^6\n$$\nA condition number of $10^6$ indicates that $H$ is ill-conditioned. However, the reduced system matrix $M_R$ is perfectly conditioned with $\\kappa_2(M_R) = 1$.\n\nThis favorable outcome is due to the special alignment between the constraint matrix $A$ and the eigenvectors of the Hessian matrix $H$. The eigenvectors of the diagonal matrix $H$ are the standard basis vectors:\n- $v_1 = (1, 0, 0)^{\\top}$ corresponding to $\\lambda = 10^6$.\n- $v_2 = (0, 1, 0)^{\\top}$ corresponding to $\\lambda = 10^6$.\n- $v_3 = (0, 0, 1)^{\\top}$ corresponding to $\\lambda = 1$.\n\nThe rows of the constraint matrix $A$ are $a_1^{\\top} = (1, 0, 0)$ and $a_2^{\\top} = (0, 1, 0)$. The range of $A^{\\top}$, denoted $\\operatorname{range}(A^{\\top})$, is spanned by the columns of $A^{\\top}$, which are precisely the eigenvectors $v_1$ and $v_2$. This subspace, $\\text{span}\\{v_1, v_2\\}$, is the eigenspace of $H$ corresponding to the large eigenvalue $\\lambda=10^6$.\n\nThe matrix $M_R = AH^{-1}A^{\\top}$ effectively measures the action of $H^{-1}$ on the subspace defined by the constraints, which is $\\operatorname{range}(A^{\\top})$. For any vector $\\mathbf{y} \\in \\mathbb{R}^2$, the vector $A^{\\top}\\mathbf{y}$ lies in $\\operatorname{range}(A^{\\top})$. Since this subspace is an eigenspace of $H$ (and thus of $H^{-1}$), the action of $H^{-1}$ on any vector $\\mathbf{z} \\in \\operatorname{range}(A^{\\top})$ is simple scalar multiplication by the corresponding eigenvalue of $H^{-1}$, which is $1/10^6 = 10^{-6}$.\n$$\nH^{-1}(A^{\\top}\\mathbf{y}) = 10^{-6}(A^{\\top}\\mathbf{y})\n$$\nSubstituting this into the expression for $M_R$:\n$$\nM_R \\mathbf{y} = A (H^{-1} A^{\\top} \\mathbf{y}) = A(10^{-6} A^{\\top} \\mathbf{y}) = 10^{-6} (AA^{\\top})\\mathbf{y}\n$$\nWe compute $AA^{\\top}$:\n$$\nAA^{\\top} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} = I_2\n$$\nThus, $M_R \\mathbf{y} = 10^{-6} I_2 \\mathbf{y}$, which implies $M_R = 10^{-6} I_2$.\n\nThe ill-conditioning of $H$ arises from the vast difference in how it scales vectors in the eigenspace $\\text{span}\\{v_1, v_2\\}$ versus the eigenspace $\\text{span}\\{v_3\\}$. However, the structure of $A$ ensures that the reduced system matrix $M_R$ only \"sees\" the uniform scaling behavior of $H^{-1}$ within the single eigenspace spanned by the rows of $A$. The other dynamic mode of $H^{-1}$ (associated with eigenvalue $1$ and eigenvector $v_3$) is related to the null space of $A$, and its effect is completely filtered out in the product $AH^{-1}A^{\\top}$. As a result, $M_R$ is a scalar multiple of the identity matrix, which is perfectly conditioned.\n\nThe value of the two-norm condition number of the reduced system matrix for this construction is exactly $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3171127"}, {"introduction": "The true power of range-space methods often becomes apparent in the context of iterative algorithms, such as active-set methods, where constraints are added or removed sequentially. Recomputing the inverse of the range-space matrix $S$ from scratch at each iteration would be computationally prohibitive. This advanced practice demonstrates how to efficiently update the system by applying block matrix inversion formulas, a technique related to the Sherman-Morrison-Woodbury formula. This exercise highlights the practical utility and computational elegance of the range-space approach in a dynamic setting [@problem_id:3171093].", "problem": "In equality-constrained quadratic programming, range-space methods reduce a Karush–Kuhn–Tucker (KKT) system to solving linear systems with the symmetric positive definite matrix $S = A H^{-1} A^{\\top}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, and the constraints are $A \\mathbf{x} = \\mathbf{b}$. Consider augmenting the active set by appending one new equality constraint $a^{\\top} \\mathbf{x} = b_{+}$, forming the augmented matrix $A_{+} \\in \\mathbb{R}^{(m+1) \\times n}$ by stacking $a^{\\top}$ below $A$. Starting from the core definitions of the range-space matrix and standard block matrix identities, derive the expression for the inverse of the augmented range-space matrix $S_{+} = A_{+} H^{-1} A_{+}^{\\top}$ in terms of $S^{-1}$, $A$, $H$, and $a$, and identify the scalar that appears in the lower-right corner of $S_{+}^{-1}$.\n\nThen, for the specific data\n$$\nH = \\operatorname{diag}(2,\\,3,\\,6), \\quad\nA = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 2\n\\end{pmatrix}, \\quad\na = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix},\n$$\ncompute the exact value (no rounding) of the lower-right entry of $S_{+}^{-1}$. Provide your final answer as a single real number with no units.", "solution": "The analysis of the problem will be conducted in two parts. First, a general symbolic derivation for the inverse of the augmented range-space matrix, $S_{+}^{-1}$, will be performed. Second, the specific numerical data provided will be used to compute the value of the lower-right entry of $S_{+}^{-1}$.\n\n### Part 1: Symbolic Derivation of $S_{+}^{-1}$\n\nThe problem defines the initial range-space matrix as $S = A H^{-1} A^{\\top}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $A \\in \\mathbb{R}^{m \\times n}$ has full row rank. These conditions ensure that $S \\in \\mathbb{R}^{m \\times m}$ is symmetric and positive definite, and therefore invertible.\n\nA new equality constraint is introduced, leading to an augmented matrix $A_{+}$ formed by appending the new constraint row $a^{\\top}$ to $A$:\n$$\nA_{+} = \\begin{pmatrix} A \\\\ a^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{(m+1) \\times n}\n$$\nThe corresponding augmented range-space matrix, $S_{+}$, is defined as:\n$$\nS_{+} = A_{+} H^{-1} A_{+}^{\\top}\n$$\nWe can express $S_{+}$ in a $2 \\times 2$ block matrix form by substituting the block structure of $A_{+}$:\n$$\nS_{+} = \\begin{pmatrix} A \\\\ a^{\\top} \\end{pmatrix} H^{-1} \\begin{pmatrix} A^{\\top} & a \\end{pmatrix}\n$$\nPerforming the block matrix multiplication, we get:\n$$\nS_{+} = \\begin{pmatrix} A H^{-1} A^{\\top} & A H^{-1} a \\\\ a^{\\top} H^{-1} A^{\\top} & a^{\\top} H^{-1} a \\end{pmatrix}\n$$\nWe can identify the blocks in this matrix. The top-left block is the original matrix $S = A H^{-1} A^{\\top}$. Let us define the following quantities:\n- The vector $u = A H^{-1} a \\in \\mathbb{R}^{m \\times 1}$.\n- The scalar $\\delta = a^{\\top} H^{-1} a \\in \\mathbb{R}$.\n\nSince $H$ is symmetric, $H^{-1}$ is also symmetric. Therefore, the lower-left block is $(a^{\\top} H^{-1} A^{\\top}) = (A (H^{-1})^{\\top} a)^{\\top} = (A H^{-1} a)^{\\top} = u^{\\top}$.\nThus, $S_{+}$ can be written concisely as:\n$$\nS_{+} = \\begin{pmatrix} S & u \\\\ u^{\\top} & \\delta \\end{pmatrix}\n$$\nTo find the inverse $S_{+}^{-1}$, we use the formula for the inverse of a $2 \\times 2$ block matrix. The formula relies on the Schur complement of either the top-left block $S$ or the bottom-right block $\\delta$. Let $\\gamma$ be the Schur complement of $S$, which is a scalar in this case:\n$$\n\\gamma = \\delta - u^{\\top} S^{-1} u\n$$\nFor $S_{+}^{-1}$ to exist, $S_{+}$ must be non-singular. This requires the new constraint $a^{\\top}$ to be linearly independent of the rows of $A$, which ensures that $\\gamma \\neq 0$. The inverse $S_{+}^{-1}$ is then given by the block matrix inversion formula (also known as the partitioned inverse formula):\n$$\nS_{+}^{-1} = \\begin{pmatrix} S^{-1} + S^{-1} u \\gamma^{-1} u^{\\top} S^{-1} & -S^{-1} u \\gamma^{-1} \\\\ -\\gamma^{-1} u^{\\top} S^{-1} & \\gamma^{-1} \\end{pmatrix}\n$$\nThis is the general expression for the inverse of the augmented range-space matrix. The scalar that appears in the lower-right corner of $S_{+}^{-1}$ is $\\gamma^{-1}$.\n$$\n\\text{Lower-right entry of } S_{+}^{-1} = \\gamma^{-1} = (\\delta - u^{\\top} S^{-1} u)^{-1} = \\left( (a^{\\top} H^{-1} a) - (a^{\\top} H^{-1} A^{\\top}) (A H^{-1} A^{\\top})^{-1} (A H^{-1} a) \\right)^{-1}\n$$\n\n### Part 2: Numerical Calculation\n\nWe are given the following specific data:\n$$\nH = \\operatorname{diag}(2,\\,3,\\,6) = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 6 \\end{pmatrix}, \\quad\nA = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}, \\quad\na = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nTo find the lower-right entry of $S_{+}^{-1}$, we need to compute $\\gamma^{-1}$. We proceed step-by-step.\n\n1.  **Compute $H^{-1}$**: Since $H$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal entries.\n    $$\n    H^{-1} = \\operatorname{diag}\\left(\\frac{1}{2},\\,\\frac{1}{3},\\,\\frac{1}{6}\\right) = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{6} \\end{pmatrix}\n    $$\n\n2.  **Compute $\\delta = a^{\\top} H^{-1} a$**:\n    First, compute $H^{-1}a$:\n    $$\n    H^{-1}a = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix}\n    $$\n    Then, compute the dot product $a^{\\top}(H^{-1}a)$:\n    $$\n    \\delta = \\begin{pmatrix} 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix} = (1)\\left(\\frac{1}{2}\\right) + (-1)\\left(-\\frac{1}{3}\\right) + (1)\\left(\\frac{1}{6}\\right) = \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3+2+1}{6} = \\frac{6}{6} = 1\n    $$\n\n3.  **Compute $u = A H^{-1} a$**:\n    Using the previously calculated $H^{-1}a$:\n    $$\n    u = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} (1)(\\frac{1}{2}) + (1)(-\\frac{1}{3}) + (0)(\\frac{1}{6}) \\\\ (0)(\\frac{1}{2}) + (1)(-\\frac{1}{3}) + (2)(\\frac{1}{6}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} - \\frac{1}{3} \\\\ -\\frac{1}{3} + \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{6} \\\\ 0 \\end{pmatrix}\n    $$\n    So, $u^{\\top} = \\begin{pmatrix} \\frac{1}{6} & 0 \\end{pmatrix}$.\n\n4.  **Compute $S^{-1} = (A H^{-1} A^{\\top})^{-1}$**:\n    First, compute $S = A H^{-1} A^{\\top}$.\n    $$\n    A H^{-1} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{3} & \\frac{2}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}\n    $$\n    $$\n    S = (A H^{-1}) A^{\\top} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}(1) + \\frac{1}{3}(1) & \\frac{1}{2}(0) + \\frac{1}{3}(1) \\\\ \\frac{1}{3}(1) & \\frac{1}{3}(1) + \\frac{1}{3}(2) \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} & \\frac{1}{3} \\\\ \\frac{1}{3} & 1 \\end{pmatrix}\n    $$\n    Now, compute the inverse $S^{-1}$. The determinant of $S$ is:\n    $$\n    \\det(S) = \\left(\\frac{5}{6}\\right)(1) - \\left(\\frac{1}{3}\\right)\\left(\\frac{1}{3}\\right) = \\frac{5}{6} - \\frac{1}{9} = \\frac{15 - 2}{18} = \\frac{13}{18}\n    $$\n    The inverse is:\n    $$\n    S^{-1} = \\frac{1}{\\det(S)} \\begin{pmatrix} 1 & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{5}{6} \\end{pmatrix} = \\frac{18}{13} \\begin{pmatrix} 1 & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{5}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{13} & -\\frac{6}{13} \\\\ -\\frac{6}{13} & \\frac{15}{13} \\end{pmatrix}\n    $$\n\n5.  **Compute $u^{\\top} S^{-1} u$**:\n    $$\n    u^{\\top} S^{-1} u = \\begin{pmatrix} \\frac{1}{6} & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{18}{13} & -\\frac{6}{13} \\\\ -\\frac{6}{13} & \\frac{15}{13} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6} \\\\ 0 \\end{pmatrix}\n    $$\n    Let's first compute $S^{-1}u$:\n    $$\n    S^{-1}u = \\begin{pmatrix} \\frac{18}{13} & -\\frac{6}{13} \\\\ -\\frac{6}{13} & \\frac{15}{13} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{13 \\cdot 6} \\\\ -\\frac{6}{13 \\cdot 6} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{13} \\\\ -\\frac{1}{13} \\end{pmatrix}\n    $$\n    Now multiply by $u^{\\top}$:\n    $$\n    u^{\\top} (S^{-1} u) = \\begin{pmatrix} \\frac{1}{6} & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{13} \\\\ -\\frac{1}{13} \\end{pmatrix} = \\left(\\frac{1}{6}\\right)\\left(\\frac{3}{13}\\right) = \\frac{3}{78} = \\frac{1}{26}\n    $$\n\n6.  **Compute $\\gamma = \\delta - u^{\\top} S^{-1} u$**:\n    $$\n    \\gamma = 1 - \\frac{1}{26} = \\frac{25}{26}\n    $$\n\n7.  **Compute the final answer, $\\gamma^{-1}$**:\n    The lower-right entry of $S_{+}^{-1}$ is $\\gamma^{-1}$.\n    $$\n    \\gamma^{-1} = \\left(\\frac{25}{26}\\right)^{-1} = \\frac{26}{25}\n    $$", "answer": "$$\\boxed{1.04}$$", "id": "3171093"}]}