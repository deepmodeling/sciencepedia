{"hands_on_practices": [{"introduction": "The power of a Prediction by Partial Matching (PPM) model lies in its ability to learn statistical patterns from data. The very first step in this process is to build a hierarchy of frequency tables, where each table tracks which symbols follow a context of a certain length. This exercise [@problem_id:1647197] provides hands-on practice in constructing these fundamental tables, allowing you to see exactly how the model organizes information after processing a sequence.", "problem": "In the field of data compression, statistical models are used to predict upcoming symbols in a data stream. One such class of models is built using the Prediction by Partial Matching (PPM) algorithm. A PPM model maintains frequency counts of symbols that follow specific preceding sequences, known as contexts.\n\nFor a given context order `k`, the model builds a frequency table. A `k`-order context is a sequence of `k` symbols. The table for order `k` records, for each distinct `k`-order context `C` found in a training sequence, which symbols immediately followed `C` and how many times each one appeared. The order `k=0` model is a special case with an empty context, and its table simply records the total frequency of each symbol in the entire sequence.\n\nConsider a PPM model that is being trained on the text sequence `S = \"AGADADAGA\"` from left to right. Which of the following options correctly represents the complete set of context frequency tables for orders `k=2`, `k=1`, and `k=0` after the model has processed the entire sequence?\n\nThe tables are represented using the notation: `k=N: {Context1: {Symbol1: Count1, ...}, Context2: {...}, ...}`.\n\nA.\n`k=2`: {`AG`: {`A`: 1}, `GA`: {`D`: 1}, `AD`: {`A`: 1}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 1, `D`: 1}, `G`: {`A`: 1}, `D`: {`A`: 1}}\n`k=0`: {`A`: 5, `G`: 2, `D`: 2}\n\nB.\n`k=2`: {`AG`: {`A`: 2}, `GA`: {`D`: 1}, `AD`: {`A`: 2}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 2, `D`: 2}, `G`: {`A`: 2}, `D`: {`A`: 2}}\n`k=0`: {`A`: 4, `G`: 3, `D`: 2}\n\nC.\n`k=2`: {`AG`: {`A`: 2}, `GA`: {`D`: 1}, `AD`: {`A`: 2}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 2, `D`: 2}, `G`: {`A`: 2}, `D`: {`A`: 2}}\n`k=0`: {`A`: 5, `G`: 2, `D`: 2}\n\nD.\n`k=2`: {`GA`: {`A`: 2}, `AG`: {`A`: 2}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 2, `D`: 2}, `G`: {`A`: 2}, `D`: {`A`: 1, `G`: 1}}\n`k=0`: {`A`: 5, `G`: 2, `D`: 2}", "solution": "We are given the sequence S = \"AGADADAGA\" of length $n=9$, processed left to right. For a PPM model of order $k$, each occurrence is derived from positions $i=k+1$ to $n$, where the context is the length-$k$ substring $S_{i-k}\\dots S_{i-1}$ and the symbol counted is $S_{i}$. The order $k=0$ model simply records total symbol frequencies in the entire sequence.\n\nCompute the $k=2$ table using $i=3$ to $9$:\n- $i=3$: context \"AG\", next \"A\" gives \"AG\": {\"A\": 1}.\n- $i=4$: context \"GA\", next \"D\" gives \"GA\": {\"D\": 1}.\n- $i=5$: context \"AD\", next \"A\" gives \"AD\": {\"A\": 1}.\n- $i=6$: context \"DA\", next \"D\" gives \"DA\": {\"D\": 1}.\n- $i=7$: context \"AD\", next \"A\" updates \"AD\": {\"A\": 2}.\n- $i=8$: context \"DA\", next \"G\" updates \"DA\": {\"D\": 1, \"G\": 1}.\n- $i=9$: context \"AG\", next \"A\" updates \"AG\": {\"A\": 2}.\nThus $k=2$: {\"AG\": {\"A\": 2}, \"GA\": {\"D\": 1}, \"AD\": {\"A\": 2}, \"DA\": {\"D\": 1, \"G\": 1}}.\n\nCompute the $k=1$ table using $i=2$ to $9$:\n- $i=2$: context \"A\", next \"G\" gives \"A\": {\"G\": 1}.\n- $i=3$: context \"G\", next \"A\" gives \"G\": {\"A\": 1}.\n- $i=4$: context \"A\", next \"D\" updates \"A\": {\"G\": 1, \"D\": 1}.\n- $i=5$: context \"D\", next \"A\" gives \"D\": {\"A\": 1}.\n- $i=6$: context \"A\", next \"D\" updates \"A\": {\"G\": 1, \"D\": 2}.\n- $i=7$: context \"D\", next \"A\" updates \"D\": {\"A\": 2}.\n- $i=8$: context \"A\", next \"G\" updates \"A\": {\"G\": 2, \"D\": 2}.\n- $i=9$: context \"G\", next \"A\" updates \"G\": {\"A\": 2}.\nThus $k=1$: {\"A\": {\"G\": 2, \"D\": 2}, \"G\": {\"A\": 2}, \"D\": {\"A\": 2}}.\n\nCompute the $k=0$ table by total frequencies in S:\n- \"A\" occurs at positions $1,3,5,7,9$: total $5$.\n- \"G\" occurs at positions $2,8$: total $2$.\n- \"D\" occurs at positions $4,6$: total $2$.\nThus $k=0$: {\"A\": 5, \"G\": 2, \"D\": 2}.\n\nComparing with the options, the set of tables matches exactly option C.", "answer": "$$\\boxed{C}$$", "id": "1647197"}, {"introduction": "With the frequency tables established, the next step is to use them for prediction. A crucial feature of PPM is its 'escape' mechanism, which allows the model to gracefully handle symbols that are new or rare in a particular context by falling back to a shorter context. This practice [@problem_id:1647202] walks you through calculating the probability of a symbol using a simplified PPM model, giving you a clear look at how context-specific information and escape probabilities are combined to make a final prediction.", "problem": "Prediction by Partial Matching (PPM) is a statistical data compression algorithm that predicts the next symbol in a sequence based on the preceding symbols, known as the \"context\". This problem explores a specific variant of this algorithm.\n\nConsider a \"Simplified PPM\" model defined by the following rules, which is used to predict the symbol following a given sequence.\n\n**Algorithm Definition: Simplified PPM**\n\n1.  **Initial Context**: To predict the probability of a symbol following a sequence, the model starts by considering the context of the maximum possible order. For a given maximum order $k_{\\text{max}}$, the initial context is the last $k_{\\text{max}}$ symbols of the sequence.\n\n2.  **Model Statistics (for an order-$k$ context, $c_k$)**:\n    *   The model's statistics are derived from a training sequence.\n    *   Let $N(c_k)$ be the total number of times the context $c_k$ appears in the training sequence, where each appearance is immediately followed by at least one symbol.\n    *   Let $N(s|c_k)$ be the number of times a specific symbol $s$ is observed immediately following the context $c_k$.\n    *   Let $U(c_k)$ be the set of unique symbols that have been observed to follow $c_k$.\n\n3.  **Probability Calculation at Order $k$**:\n    *   **Case 1 (Symbol Seen)**: If the symbol $s$ to be predicted is in the set $U(c_k)$, its probability is calculated directly:\n        $$P_k(s | c_k) = \\frac{N(s|c_k)}{N(c_k) + |U(c_k)|}$$\n    *   **Case 2 (Symbol Not Seen - Escape)**: If the symbol $s$ is *not* in the set $U(c_k)$, the model \"escapes\" to a lower-order model. The probability is the product of an escape probability and the probability from the next lower-order model:\n        $$P_k(s | c_k) = P_{\\text{esc},k} \\times P_{k-1}(s | c_{k-1})$$\n        where the escape probability is $P_{\\text{esc},k} = \\frac{|U(c_k)|}{N(c_k) + |U(c_k)|}$ and $c_{k-1}$ is the suffix of context $c_k$ of length $k-1$.\n\n4.  **Base Models**:\n    *   **Order-0 Model**: The context is the empty string, $c_0 = \\text{\"\"}$. For this model, $N(c_0)$ is the total length of the training sequence, and $U(c_0)$ is the set of all unique symbols in the entire sequence.\n    *   **Order-(-1) Model**: This is a uniform prior. If an escape occurs from the Order-0 model (which happens when predicting a symbol that never appears in the training sequence), this model assigns a probability of $1/M$ to any symbol, where $M$ is the size of the alphabet.\n\n**Problem:**\n\nA Simplified PPM model is built using the training sequence `AABABAA`. The alphabet of possible symbols consists of all unique symbols present in this sequence. The maximum model order is set to $k_{\\text{max}}=2$.\n\nUsing this specific model, what is the probability that the symbol immediately following the sequence `AABABAA` is 'A'? Your answer should be a fraction in its simplest form.", "solution": "The training sequence is AABABAA, so the alphabet is $\\{A,B\\}$. The maximum order is $k_{\\text{max}}=2$, so the initial context for predicting the next symbol after AABABAA is the last two symbols, $c_{2}=\\text{\"AA\"}$.\n\nFor an order-$k$ context $c_{k}$, the model uses:\n$$P_{k}(s\\mid c_{k})=\\begin{cases}\n\\frac{N(s\\mid c_{k})}{N(c_{k})+|U(c_{k})|}, & \\text{if } s\\in U(c_{k}),\\\\[6pt]\nP_{\\text{esc},k}\\,P_{k-1}(s\\mid c_{k-1}), & \\text{if } s\\notin U(c_{k}),\n\\end{cases}$$\nwith escape probability\n$$P_{\\text{esc},k}=\\frac{|U(c_{k})|}{N(c_{k})+|U(c_{k})|}.$$\n\nOrder-2 statistics from the training sequence are computed by counting contexts of length $2$ that are immediately followed by a symbol. The occurrences are:\n- $\\text{\"AA\"}\\to\\text{\"B\"}$ (from positions $1$–$3$),\n- $\\text{\"AB\"}\\to\\text{\"A\"}$ (from positions $2$–$4$ and $4$–$6$),\n- $\\text{\"BA\"}\\to\\text{\"B\"}$ (from positions $3$–$5$) and $\\text{\"BA\"}\\to\\text{\"A\"}$ (from positions $5$–$7$).\nThe terminal $\\text{\"AA\"}$ at positions $6$–$7$ has no following symbol and is not counted.\n\nThus, for $c_{2}=\\text{\"AA\"}$,\n$$N(\\text{\"AA\"})=1,\\quad U(\\text{\"AA\"})=\\{\\text{\"B\"}\\},\\quad N(A\\mid \\text{\"AA\"})=0.$$\nSince $A\\notin U(\\text{\"AA\"})$, we escape:\n$$P_{2}(A\\mid \\text{\"AA\"})=P_{\\text{esc},2}\\,P_{1}(A\\mid \\text{\"A\"}),\\quad P_{\\text{esc},2}=\\frac{|U(\\text{\"AA\"})|}{N(\\text{\"AA\"})+|U(\\text{\"AA\"})|}=\\frac{1}{1+1}=\\frac{1}{2}.$$\n\nNow compute the order-1 probability for $c_{1}=\\text{\"A\"}$ by counting contexts of length $1$ that are immediately followed by a symbol. The occurrences are:\n- $\\text{\"A\"}\\to\\text{\"A\"}$ (positions $1$–$2$ and $6$–$7$),\n- $\\text{\"A\"}\\to\\text{\"B\"}$ (positions $2$–$3$ and $4$–$5$).\nHence,\n$$N(\\text{\"A\"})=4,\\quad U(\\text{\"A\"})=\\{\\text{\"A\"},\\text{\"B\"}\\},\\quad N(A\\mid \\text{\"A\"})=2,$$\nso\n$$P_{1}(A\\mid \\text{\"A\"})=\\frac{N(A\\mid \\text{\"A\"})}{N(\\text{\"A\"})+|U(\\text{\"A\"})|}=\\frac{2}{4+2}=\\frac{1}{3}.$$\n\nCombine the factors:\n$$P_{2}(A\\mid \\text{\"AA\"})=\\frac{1}{2}\\cdot \\frac{1}{3}=\\frac{1}{6}.$$\nNo further escape is needed because $A\\in U(\\text{\"A\"})$, so lower orders are not used.\n\nTherefore, the probability that the symbol following AABABAA is $A$ under this Simplified PPM model is $\\frac{1}{6}$.", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "1647202"}, {"introduction": "To implement PPM efficiently, the hierarchical contexts are often stored in a specialized tree-like data structure known as a trie. Different variants of PPM, such as PPM-C, define specific rules for how this trie is updated as new data is processed. This problem [@problem_id:1647187] challenges you to visualize this dynamic update process, focusing on how a single new character can alter symbol counts and create new pathways throughout the model's context trie.", "problem": "An information theory engineer is implementing a data compression algorithm based on Prediction by Partial Matching (PPM). The specific variant being used is PPM-C with a maximum context order of $k=2$. The model builds a context trie to store symbol frequencies.\n\nThe PPM-C update rule is as follows: when a new symbol is observed, its count is incremented in the tables for all contexts from order $k$ down to order -1 (the root context).\n- Each node in the trie represents a context and stores a frequency table of symbols that have followed it.\n- A context of order $m$ is the sequence of $m$ characters immediately preceding the current symbol. The root represents the order -1 context, which tracks the frequencies of all symbols regardless of context.\n- In addition to symbol counts, each non-root node maintains an \"escape count,\" which is the number of *unique* symbol types that have been observed to follow that context. If a symbol is seen for the first time in a given context, a new entry is created for it in that node's frequency table with a count of 1, and the node's escape count is incremented by 1.\n\nThe model has already processed the 9-character sequence `MISSISSIP`. Now, it is about to process the next character, which is `S`, making the full sequence `MISSISSIPS`.\n\nLet `C(symbol|context)` denote the frequency count of a `symbol` observed immediately after a given `context`. The root context is denoted by `root`. Let `Escape(context)` denote the escape count for a given context node.\n\nAfter the model processes the final `S`, which of the following statements correctly describes the state of the counts in the nodes for contexts `IP`, `P`, and the `root`?\n\nA. In the node for context `IP`, `C(S|IP) = 1` and `Escape(IP) = 1`. In the node for context `P`, `C(S|P) = 1` and `Escape(P) = 1`. At the root, `C(S|root) = 5`.\nB. In the node for context `IP`, `C(S|IP) = 1` and `Escape(IP) = 0`. In the node for context `P`, `C(S|P) = 1` and `Escape(P) = 0`. At the root, `C(S|root) = 5`.\nC. The model only updates the highest-order context and the root. Therefore, in the node for context `IP`, `C(S|IP) = 1`, while the node for context `P` remains unchanged. At the root, `C(S|root) = 5`.\nD. Since `S` has been seen before, no new branches are created. The count `C(S|root)` is updated to 5, but the nodes for contexts `IP` and `P` remain unchanged as they did not previously have an `S` branch.\nE. At the root, `C(S|root)` is updated to 5, and the root's escape count is also updated to 5. In the node for context `P`, `C(S|P) = 1`. In the node for context `IP`, `C(S|IP) = 1`.", "solution": "We denote the processed sequence before the final update as the length-9 string with symbols $(s_{1},\\dots,s_{9})=\\text{M,I,S,S,I,S,S,I,P}$ and the next symbol $s_{10}=\\text{S}$. The maximum context order is $k=2$, so when observing $s_{10}$ the model updates counts for contexts of orders $2,1,0,-1$ as per the stated rule: for each context, increment the count of the observed symbol; if the symbol is new to that context, create its entry with count $1$ and increment the escape count for that node by $1$ (non-root only).\n\nIdentify the contexts immediately preceding $s_{10}$:\n- Order $2$ context is the pair $(s_{8},s_{9})=(\\text{I},\\text{P})$, i.e., $\\text{IP}$.\n- Order $1$ context is $s_{9}=\\text{P}$.\n- The root is order $-1$.\n\nConsider the node for context $\\text{IP}$. Before $t=10$, there was no time $t\\leq 9$ with $(s_{t-2},s_{t-1})=(\\text{I},\\text{P})$ followed by a recorded symbol, because the only occurrence of the bigram $\\text{IP}$ in positions $1$ to $9$ is $(s_{8},s_{9})$ at the end, with no following symbol yet observed. Therefore, $s_{10}=\\text{S}$ is the first symbol observed after context $\\text{IP}$. By the update rule, a new entry is created and incremented, so\n$$\nC(S|\\text{IP})=1,\\quad \\mathrm{Escape}(\\text{IP})=1.\n$$\n\nConsider the node for context $\\text{P}$. The only $\\text{P}$ in positions $1$ to $9$ is $s_{9}=\\text{P}$. No symbol had yet been observed to follow $\\text{P}$ before $t=10$. Hence $s_{10}=\\text{S}$ is the first symbol observed after context $\\text{P}$, so\n$$\nC(S|\\text{P})=1,\\quad \\mathrm{Escape}(\\text{P})=1.\n$$\n\nAt the root (order $-1$), it tracks symbol frequencies regardless of context. In the prefix $\\text{MISSISSIP}$, $\\text{S}$ appears at positions $3,4,6,7$, so initially $C(S|\\text{root})=4$. After observing $s_{10}=\\text{S}$, the root count increments by $1$:\n$$\nC(S|\\text{root})=5.\n$$\n\nThese outcomes match exactly the description in option A.", "answer": "$$\\boxed{A}$$", "id": "1647187"}]}