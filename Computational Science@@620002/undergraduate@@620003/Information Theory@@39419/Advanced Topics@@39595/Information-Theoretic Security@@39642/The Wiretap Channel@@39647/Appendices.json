{"hands_on_practices": [{"introduction": "This first practice problem introduces the foundational concept of the wiretap channel in its most straightforward form. We will explore a hypothetical scenario where the legitimate receiver, Bob, has a perfect, noiseless channel, while the eavesdropper, Eve, must contend with a noisy one [@problem_id:1664575]. This exercise is designed to build your intuition on secrecy capacity, $C_S$, by showing how it represents the information advantage you have over the eavesdropper, calculated as the difference between the main channel capacity and the wiretap channel capacity.", "problem": "In the context of physical layer security, we consider a simple wiretap channel model. Alice wants to send a binary message to a legitimate receiver, Bob, over a communication channel. An eavesdropper, Eve, is also listening in on a separate channel.\n\nThe communication process is modeled as follows:\n- Alice's transmitted signal is a binary random variable $X$ from the alphabet $\\{0, 1\\}$.\n- The channel from Alice to Bob is perfect, meaning Bob receives a signal $Y$ that is identical to Alice's transmission, i.e., $Y = X$.\n- The channel from Alice to Eve is a Binary Symmetric Channel (BSC). When Alice transmits a bit, there is a fixed probability $p_E$ that the bit is flipped before it reaches Eve. Eve receives a signal $Z$ from the alphabet $\\{0, 1\\}$.\n\nThe goal is to determine the secrecy capacity, $C_S$, of this system, which represents the maximum rate at which Alice can transmit information to Bob such that Eve learns absolutely nothing about the message.\n\nThe binary entropy function is defined as $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ for $p \\in [0, 1]$.\n\nDetermine the secrecy capacity $C_S$ as a function of the crossover probability $p_E$. Express your answer in terms of $p_E$ and the binary entropy function $H_2(\\cdot)$. The capacity is measured in bits per channel use.", "solution": "The secrecy capacity $C_S$ of a wiretap channel is defined as the maximum achievable secrecy rate over all possible input distributions. The secrecy rate for a given input distribution $p(X)$ is the difference between the mutual information of the main channel (Alice-to-Bob) and the wiretapper's channel (Alice-to-Eve).\n$$C_S = \\max_{p(X)} [I(X;Y) - I(X;Z)]$$\nHere, $X$ is the input from Alice, $Y$ is the output to Bob, and $Z$ is the output to Eve. We will calculate the two mutual information terms separately and then find the maximum of their difference.\n\nFirst, we analyze the main channel (Alice to Bob).\nThe channel is noiseless, so $Y=X$. The mutual information is given by $I(X;Y) = H(Y) - H(Y|X)$.\nSince $Y$ is completely determined by $X$, the conditional entropy $H(Y|X)=0$.\nThis simplifies the mutual information to $I(X;Y) = H(Y)$. Since $Y=X$, we have $H(Y)=H(X)$.\nSo, for the main channel, $I(X;Y) = H(X)$.\nThe capacity of the main channel, $C_B$, is the maximum of this mutual information over all input distributions: $C_B = \\max_{p(X)} H(X)$. For a binary random variable $X$, the entropy $H(X)$ is maximized when the distribution is uniform, i.e., $P(X=0)=P(X=1)=1/2$. In this case, $H(X) = H_2(1/2) = 1$ bit. So, $C_B = 1$ bit per channel use.\n\nSecond, we analyze the wiretapper's channel (Alice to Eve).\nThis channel is a Binary Symmetric Channel (BSC) with crossover probability $p_E$. The mutual information is $I(X;Z) = H(Z) - H(Z|X)$.\nFor a BSC, the conditional entropy $H(Z|X)$ is independent of the input distribution and is equal to the binary entropy of the crossover probability: $H(Z|X) = H_2(p_E)$.\nThus, $I(X;Z) = H(Z) - H_2(p_E)$.\nThe capacity of Eve's channel, $C_E$, is the maximum of this mutual information: $C_E = \\max_{p(X)} [H(Z) - H_2(p_E)]$.\nTo maximize this expression, we need to maximize $H(Z)$, the entropy of the output. The output entropy $H(Z)$ is maximized when the output distribution $p(Z)$ is uniform.\nLet's see if a uniform input $P(X=0)=P(X=1)=1/2$ leads to a uniform output. The probability of Eve receiving a 0 is:\n$P(Z=0) = P(Z=0|X=0)P(X=0) + P(Z=0|X=1)P(X=1)$\n$P(Z=0) = (1-p_E)(\\frac{1}{2}) + (p_E)(\\frac{1}{2}) = \\frac{1-p_E+p_E}{2} = \\frac{1}{2}$.\nSince $P(Z=0) + P(Z=1) = 1$, we must have $P(Z=1)=1/2$ as well. So, a uniform input on a BSC results in a uniform output. This maximizes $H(Z)$ to be $H_2(1/2)=1$.\nTherefore, the capacity of Eve's channel is $C_E = 1 - H_2(p_E)$, and this is achieved with a uniform input distribution.\n\nFinally, we calculate the secrecy capacity $C_S$.\nThis wiretap channel is a \"degraded\" channel, because the main channel is a special case of the wiretapper's channel (a BSC with crossover probability 0), making it less noisy. More formally, the Markov chain $X \\to Y \\to Z$ holds. Since $Y=X$, this condition is met by defining the transition from $Y$ to $Z$ as a BSC with probability $p_E$.\n\nFor degraded wiretap channels where the capacity of the main channel $C_B$ and the capacity of the wiretapper's channel $C_E$ are both achieved by the same input distribution, the secrecy capacity is simply the difference of the individual capacities: $C_S = C_B - C_E$.\nIn our case, both $C_B$ and $C_E$ are achieved with a uniform input distribution ($P(X=0)=P(X=1)=1/2$).\nTherefore, we can use this simplified formula.\nSubstituting the values we found:\n$C_S = C_B - C_E = 1 - (1 - H_2(p_E))$\n$C_S = H_2(p_E)$\n\nSo, the secrecy capacity of this wiretap channel is given by the binary entropy function of Eve's crossover probability.", "answer": "$$\\boxed{H_2(p_E)}$$", "id": "1664575"}, {"introduction": "Having established the basics, we now turn to a different, but equally illustrative, extreme. In this problem, the eavesdropper's channel is so poor that it is completely uselessâ€”Eve learns nothing about the transmitted message, meaning her mutual information with the source is $I(X;Z) = 0$ [@problem_id:1664534]. By analyzing this scenario, you will see how the secrecy capacity, $C_S$, simplifies to just the capacity of the main channel to Bob. This demonstrates a key principle: perfect secrecy is limited only by our ability to communicate reliably with the intended recipient when the eavesdropper is effectively deaf.", "problem": "Alice is sending a secret message to Bob using a binary communication scheme, where the input alphabet is $\\mathcal{X} = \\{0, 1\\}$. The communication channel between Alice and Bob is a Binary Symmetric Channel (BSC), characterized by a crossover probability $p_B$, where $0  p_B \\le 1/2$. Simultaneously, an eavesdropper, Eve, is monitoring the transmission. Eve's wiretap channel is defective in a peculiar way: for any bit transmitted by Alice, Eve's receiver deterministically outputs a '0'. Let the input from Alice be the random variable $X$, the output at Bob's receiver be $Y$, and the output at Eve's receiver be $Z$.\n\nCalculate the secrecy capacity, $C_s$, for this system. Express your answer as a closed-form analytic expression in terms of the crossover probability $p_B$. All logarithms in your final expression should be base-2.", "solution": "The secrecy capacity, $C_s$, of a wiretap channel is defined as the maximum of the difference between the mutual information of the main channel (Alice-Bob) and the wiretap channel (Alice-Eve) over all possible input distributions $p(x)$.\n$$C_s = \\max_{p(x)} [I(X;Y) - I(X;Z)]$$\n\nFirst, we analyze the wiretap channel from Alice to Eve. The channel is described as deterministic: for any input $X \\in \\{0, 1\\}$, the output is always $Z=0$. This means the conditional probabilities are $P(Z=0|X=0) = 1$ and $P(Z=0|X=1) = 1$.\n\nWe calculate the mutual information $I(X;Z) = H(Z) - H(Z|X)$.\n\nTo calculate $H(Z|X)$, we use the definition:\n$$H(Z|X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\sum_{z \\in \\mathcal{Z}} P(Z=z|X=x) \\log_2(P(Z=z|X=x))$$\nFor $X=0$, $P(Z=0|X=0)=1$. The entropy contribution is $-1 \\log_2(1) = 0$.\nFor $X=1$, $P(Z=0|X=1)=1$. The entropy contribution is $-1 \\log_2(1) = 0$.\nThus, $H(Z|X=x) = 0$ for all $x$, which means $H(Z|X) = 0$.\n\nNext, we calculate the entropy of the output, $H(Z)$. Since the output $Z$ is always 0, regardless of the input distribution $p(x)$, the probability distribution of $Z$ is $P(Z=0) = 1$ and $P(Z=z \\neq 0) = 0$. The entropy of a deterministic variable is zero.\n$$H(Z) = -P(Z=0) \\log_2(P(Z=0)) = -1 \\log_2(1) = 0$$\nTherefore, the mutual information for the wiretap channel is:\n$$I(X;Z) = H(Z) - H(Z|X) = 0 - 0 = 0$$\nThis makes intuitive sense: since Eve always observes a '0', she learns nothing about Alice's input bit $X$.\n\nNow, we can substitute this result back into the formula for secrecy capacity:\n$$C_s = \\max_{p(x)} [I(X;Y) - 0] = \\max_{p(x)} I(X;Y)$$\nThis expression is simply the capacity of the main channel from Alice to Bob, which is a BSC with crossover probability $p_B$. Let's call this capacity $C_B$.\n$$C_s = C_B = \\max_{p(x)} I(X;Y)$$\nThe mutual information for the main channel is $I(X;Y) = H(Y) - H(Y|X)$.\nFor a BSC, the conditional entropy $H(Y|X)$ is constant regardless of the input distribution and is equal to the binary entropy of the crossover probability:\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) H(Y|X=x) = H_2(p_B) = -p_B \\log_2(p_B) - (1-p_B) \\log_2(1-p_B)$$\nSo, to maximize $I(X;Y) = H(Y) - H_2(p_B)$, we need to maximize $H(Y)$. The entropy of the binary output $Y$ is maximized when its distribution is uniform, i.e., $p(Y=0)=p(Y=1)=1/2$. This is achieved by choosing a uniform input distribution, $p(X=0)=p(X=1)=1/2$.\nWith a uniform input, the output probabilities are:\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-p_B)\\frac{1}{2} + p_B\\frac{1}{2} = \\frac{1}{2}$$\n$$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = p_B\\frac{1}{2} + (1-p_B)\\frac{1}{2} = \\frac{1}{2}$$\nWith a uniform output distribution, the output entropy is maximal:\n$$H(Y)_{\\max} = H_2(1/2) = -\\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) = 1$$\nTherefore, the capacity of the BSC is:\n$$C_B = H(Y)_{\\max} - H_2(p_B) = 1 - H_2(p_B)$$\nSubstituting the full expression for the binary entropy function:\n$$C_s = C_B = 1 - [-p_B \\log_2(p_B) - (1-p_B) \\log_2(1-p_B)]$$\n$$C_s = 1 + p_B \\log_2(p_B) + (1-p_B) \\log_2(1-p_B)$$\nThis is the final expression for the secrecy capacity.", "answer": "$$\\boxed{1 + p_B \\log_{2}(p_B) + (1-p_B) \\log_{2}(1-p_B)}$$", "id": "1664534"}, {"introduction": "To synthesize your understanding, this final practice problem challenges you to act as a system engineer evaluating three different communication designs [@problem_id:1664565]. You will calculate and compare the secrecy rates for scenarios involving different noise levels for Bob and Eve. This exercise not only reinforces the previous concepts but also reveals a critical, non-obvious insight: if the channel impairments affect both the legitimate user and the eavesdropper identically, such that they receive the exact same signal, the secrecy rate plummets to zero. This demonstrates that security relies fundamentally on maintaining an information advantage.", "problem": "An engineer is tasked with evaluating the security of three different designs for a binary communication system intended to transmit a secret message. In all scenarios, the input from the transmitter (Alice) is a random bit $X$ following a uniform distribution, such that $P(X=0)=P(X=1)=1/2$. The legitimate receiver is Bob, and a potential eavesdropper is Eve. All noise processes are modeled as sequences of independent and identically distributed Bernoulli random variables, which are also independent of the input $X$. The performance metric is the secrecy rate, defined as $R_s = I(X;Y) - I(X;Z)$, where $Y$ and $Z$ are the bits received by Bob and Eve, respectively, and $I(\\cdot;\\cdot)$ denotes the mutual information. All logarithms are base 2.\n\n*   **Scenario A:** Bob has a perfect, noiseless channel, so he receives $Y=X$. Eve's channel is a Binary Symmetric Channel (BSC) with a crossover probability of $q_E = 1/4$. Her received bit is $Z = X \\oplus N_E$, where $N_E$ is a Bernoulli random variable with $P(N_E=1) = q_E$.\n\n*   **Scenario B:** Bob's channel is a BSC with crossover probability $q_B = 1/8$, such that $Y = X \\oplus N_B$ where $N_B \\sim \\text{Bernoulli}(q_B)$. Eve's channel is also a BSC, but with a crossover probability of $q_E = 1/4$, such that $Z = X \\oplus N_E$ where $N_E \\sim \\text{Bernoulli}(q_E)$. The noise variables $N_B$ and $N_E$ are independent.\n\n*   **Scenario C:** A hardware implementation flaw causes both Bob's and Eve's receivers to be affected by the exact same noise source $N$, which is a Bernoulli random variable with $P(N=1) = 1/8$. Their received bits are $Y = X \\oplus N$ and $Z = X \\oplus N$.\n\nCalculate the secrecy rate in bits per channel use for each of the three scenarios. Provide your answer as a set of three numerical values corresponding to $(R_{s,A}, R_{s,B}, R_{s,C})$. Round each value to three significant figures.", "solution": "The secrecy rate is defined as $R_{s}=I(X;Y)-I(X;Z)$ with logarithms base $2$. For a Binary Symmetric Channel (BSC) with crossover probability $q$ and a uniform binary input $X$, the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nWith $X$ uniform and a BSC, $Y$ is uniform, so $H(Y)=1$, and $H(Y|X)=H_{2}(q)$, where the binary entropy function is\n$$\nH_{2}(q)=-q \\log_{2}(q)-(1-q)\\log_{2}(1-q).\n$$\nHence, for a BSC, $I(X;Y)=1-H_{2}(q)$.\n\nScenario A: Bob has a perfect channel ($Y=X$), so $I(X;Y)=H(X)=1$. Eve has a BSC with $q_{E}=\\frac{1}{4}$, so $I(X;Z)=1-H_{2}\\!\\left(\\frac{1}{4}\\right)$. Therefore,\n$$\nR_{s,A}=1-\\bigl(1-H_{2}\\!\\left(\\tfrac{1}{4}\\right)\\bigr)=H_{2}\\!\\left(\\tfrac{1}{4}\\right).\n$$\nCompute $H_{2}\\!\\left(\\tfrac{1}{4}\\right)$:\n$$\nH_{2}\\!\\left(\\tfrac{1}{4}\\right)=-\\tfrac{1}{4}\\log_{2}\\!\\left(\\tfrac{1}{4}\\right)-\\tfrac{3}{4}\\log_{2}\\!\\left(\\tfrac{3}{4}\\right)\n=\\tfrac{1}{2}-\\tfrac{3}{4}\\log_{2}\\!\\left(\\tfrac{3}{4}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\tfrac{3}{4}\\right)=\\log_{2}(3)-2\\approx-0.415037$, we get\n$$\nH_{2}\\!\\left(\\tfrac{1}{4}\\right)\\approx 0.5-0.75\\times(-0.415037)\\approx 0.811278,\n$$\nso $R_{s,A}\\approx 0.811278$, which rounds to $0.811$.\n\nScenario B: Bob has a BSC with $q_{B}=\\frac{1}{8}$ and Eve has a BSC with $q_{E}=\\frac{1}{4}$. Thus\n$$\nI(X;Y)=1-H_{2}\\!\\left(\\tfrac{1}{8}\\right),\\quad I(X;Z)=1-H_{2}\\!\\left(\\tfrac{1}{4}\\right),\n$$\nand\n$$\nR_{s,B}=H_{2}\\!\\left(\\tfrac{1}{4}\\right)-H_{2}\\!\\left(\\tfrac{1}{8}\\right).\n$$\nCompute $H_{2}\\!\\left(\\tfrac{1}{8}\\right)$:\n$$\nH_{2}\\!\\left(\\tfrac{1}{8}\\right)=-\\tfrac{1}{8}\\log_{2}\\!\\left(\\tfrac{1}{8}\\right)-\\tfrac{7}{8}\\log_{2}\\!\\left(\\tfrac{7}{8}\\right)\n=\\tfrac{3}{8}-\\tfrac{7}{8}\\log_{2}\\!\\left(\\tfrac{7}{8}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\tfrac{7}{8}\\right)=\\log_{2}(7)-3\\approx-0.192645$, we get\n$$\nH_{2}\\!\\left(\\tfrac{1}{8}\\right)\\approx 0.375-0.875\\times(-0.192645)\\approx 0.543564.\n$$\nHence,\n$$\nR_{s,B}\\approx 0.811278-0.543564\\approx 0.267714,\n$$\nwhich rounds to $0.268$.\n\nScenario C: Both receivers observe the same corrupted bit with the same noise $N$:\n$$\nY=X\\oplus N,\\quad Z=X\\oplus N,\n$$\nso $Z=Y$ deterministically. Therefore $I(X;Z)=I(X;Y)$, which implies\n$$\nR_{s,C}=I(X;Y)-I(X;Z)=0.\n$$\nNumerically, with $q=\\frac{1}{8}$, $I(X;Y)=1-H_{2}\\!\\left(\\tfrac{1}{8}\\right)\\approx 0.456436$ and $I(X;Z)$ is identical, so the difference is $0$. Rounding each value to three significant figures yields\n$$\n(R_{s,A},R_{s,B},R_{s,C})\\approx (0.811,\\,0.268,\\,0.000).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.811  0.268  0.000 \\end{pmatrix}}$$", "id": "1664565"}]}