{"hands_on_practices": [{"introduction": "Let's begin by exploring one of the most fundamental properties of the One-Time Pad. This exercise guides you through a calculation that demonstrates a key principle of perfect secrecy: the ciphertext's statistical distribution is independent of the original message. By determining the probability of a specific ciphertext outcome, you will gain a hands-on intuition for why the OTP is considered theoretically unbreakable. [@problem_id:1644135]", "id": "1644135", "problem": "A basic secure communication system uses an encryption scheme known as the One-Time Pad (OTP). In this system, a binary message $M$ of length $n$ bits is encrypted using a binary key $K$, also of length $n$ bits. The resulting ciphertext $C$ is produced by taking the bitwise Exclusive OR (XOR) of the message and the key, which can be written as $C = M \\oplus K$.\n\nAssume that the message $M$ is chosen uniformly at random from the set of all possible $n$-bit strings, and independently, the key $K$ is also chosen uniformly at random from the set of all possible $n$-bit strings.\n\nCalculate the probability that the resulting ciphertext $C$ is the specific $n$-bit string consisting of all zeros. Express your answer as a function of $n$.\n\n", "solution": "Let $M \\in \\{0,1\\}^{n}$ and $K \\in \\{0,1\\}^{n}$ be independent and uniformly distributed, and let $C = M \\oplus K$ be the ciphertext. We seek $\\Pr(C = 0^{n})$, where $0^{n}$ denotes the $n$-bit all-zero string.\n\nFor bits $a,b \\in \\{0,1\\}$, the property of XOR is $a \\oplus b = 0$ if and only if $a = b$. Applying this bitwise to $n$-bit strings, we have that $C = 0^{n}$ if and only if $M = K$.\n\nThus,\n$$\n\\Pr(C = 0^{n}) = \\Pr(M = K) = \\sum_{m \\in \\{0,1\\}^{n}} \\Pr(M = m, K = m).\n$$\nBy independence of $M$ and $K$,\n$$\n\\Pr(M = m, K = m) = \\Pr(M = m)\\Pr(K = m).\n$$\nSince both are uniform on $\\{0,1\\}^{n}$, for every $m$,\n$$\n\\Pr(M = m) = \\frac{1}{2^{n}}, \\quad \\Pr(K = m) = \\frac{1}{2^{n}}.\n$$\nTherefore,\n$$\n\\Pr(C = 0^{n}) = \\sum_{m \\in \\{0,1\\}^{n}} \\frac{1}{2^{n}} \\cdot \\frac{1}{2^{n}} = 2^{n} \\cdot \\frac{1}{2^{n}} \\cdot \\frac{1}{2^{n}} = \\frac{1}{2^{n}}.\n$$\nEquivalently, conditioning on any fixed $M$, there is exactly one key $K$ (namely $K = M$) among $2^{n}$ equally likely keys that yields $C = 0^{n}$, giving the same probability $\\frac{1}{2^{n}}$.", "answer": "$$\\boxed{\\frac{1}{2^{n}}}$$"}, {"introduction": "Building on the ideal properties of the OTP, we now investigate a realistic threat scenario involving a partial information leak. In this practice, you will step into the shoes of an analyst who has gained knowledge of a portion of the plaintext corresponding to an intercepted ciphertext. Using the concept of conditional entropy, you will precisely quantify the remaining uncertainty about the secret key, illustrating how security degrades as information is revealed. [@problem_id:1644090]", "id": "1644090", "problem": "A secure military communication system uses a One-Time Pad (OTP) for encrypting messages. In this system, messages, keys, and ciphertexts are treated as sequences of bits.\n\nThe message $M$ is a sequence of $L$ bits, $M = (M_1, M_2, \\dots, M_L)$. The key $K$, also a sequence of $L$ bits, $K = (K_1, K_2, \\dots, K_L)$, is generated by a true random number generator where each key bit $K_i$ is chosen independently and uniformly at random from $\\{0, 1\\}$. Furthermore, the key $K$ is statistically independent of the message $M$. The ciphertext $C = (C_1, C_2, \\dots, C_L)$ is produced by applying the bitwise exclusive-OR (XOR) operation: $C_i = M_i \\oplus K_i$ for each bit $i$ from $1$ to $L$.\n\nAn intelligence agency intercepts a specific ciphertext $C$. Sometime later, a partial security breach reveals the first $b$ bits of the original plaintext message, where $0 < b < L$. Let this known portion of the message be denoted as $M_{1..b} = (M_1, M_2, \\dots, M_b)$. The agency has no information about the remaining $L-b$ bits of the plaintext.\n\nThe agency wants to quantify its remaining uncertainty about the entire key $K$ used to generate the intercepted ciphertext. Calculate the conditional entropy of the key $K$ given the full ciphertext $C$ and the known portion of the plaintext $M_{1..b}$. Express your answer as a symbolic expression in terms of $L$ and $b$. The unit of entropy in this problem is bits, which implies the use of the base-2 logarithm in all entropy calculations.\n\n", "solution": "Let $M=(M_{1},\\dots,M_{L})$, $K=(K_{1},\\dots,K_{L})$, and $C=(C_{1},\\dots,C_{L})$ with $C_{i}=M_{i}\\oplus K_{i}$. Partition all vectors into the first $b$ components and the remaining $L-b$ components:\n$$\nK=\\bigl(K_{1..b},\\,K_{b+1..L}\\bigr),\\quad M=\\bigl(M_{1..b},\\,M_{b+1..L}\\bigr),\\quad C=\\bigl(C_{1..b},\\,C_{b+1..L}\\bigr).\n$$\nWe are to compute $H\\bigl(K\\,\\big|\\,C,M_{1..b}\\bigr)$ in bits.\n\nBy the chain rule for entropy,\n$$\nH\\bigl(K\\,\\big|\\,C,M_{1..b}\\bigr)\n=H\\bigl(K_{1..b},K_{b+1..L}\\,\\big|\\,C_{1..b},C_{b+1..L},M_{1..b}\\bigr)\n$$\n$$\n=H\\bigl(K_{1..b}\\,\\big|\\,C_{1..b},C_{b+1..L},M_{1..b}\\bigr)+H\\bigl(K_{b+1..L}\\,\\big|\\,K_{1..b},C_{1..b},C_{b+1..L},M_{1..b}\\bigr).\n$$\n\nFor the first term, since $C_{i}=M_{i}\\oplus K_{i}$, knowing $C_{i}$ and $M_{i}$ determines $K_{i}$ for each $i\\leq b$, hence\n$$\nH\\bigl(K_{1..b}\\,\\big|\\,C_{1..b},C_{b+1..L},M_{1..b}\\bigr)=0.\n$$\n\nFor the second term, use independence across positions and between $K$ and $M$. The variables $\\bigl(K_{b+1..L},C_{b+1..L}\\bigr)$ are independent of $\\bigl(K_{1..b},C_{1..b},M_{1..b}\\bigr)$ given the one-time pad model with independent bits across positions. Therefore,\n$$\nH\\bigl(K_{b+1..L}\\,\\big|\\,K_{1..b},C_{1..b},C_{b+1..L},M_{1..b}\\bigr)=H\\bigl(K_{b+1..L}\\,\\big|\\,C_{b+1..L}\\bigr).\n$$\n\nWe now evaluate $H\\bigl(K_{b+1..L}\\,\\big|\\,C_{b+1..L}\\bigr)$. Given the statement that the agency has no information about $M_{b+1..L}$, we model each unknown plaintext bit $M_{i}$ for $i>b$ as independent and uniformly distributed on $\\{0,1\\}$, independent of $K$. For any $i>b$ and any $c,k\\in\\{0,1\\}$,\n$$\n\\Pr\\bigl(K_{i}=k,C_{i}=c\\bigr)=\\sum_{m\\in\\{0,1\\}}\\Pr\\bigl(K_{i}=k,M_{i}=m\\bigr)\\,\\mathbf{1}\\{c=m\\oplus k\\}\n=\\Pr(K_{i}=k)\\Pr\\bigl(M_{i}=c\\oplus k\\bigr)=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\nHence $K_{i}$ and $C_{i}$ are independent and $K_{i}$ remains uniform given $C_{i}$:\n$$\n\\Pr\\bigl(K_{i}=k\\,\\big|\\,C_{i}=c\\bigr)=\\frac{\\Pr(K_{i}=k,C_{i}=c)}{\\Pr(C_{i}=c)}=\\frac{1/4}{1/2}=\\frac{1}{2},\n$$\nso\n$$\nH\\bigl(K_{i}\\,\\big|\\,C_{i}\\bigr)=H(K_{i})=1\\quad\\text{bit}.\n$$\nIndependence across positions implies\n$$\nH\\bigl(K_{b+1..L}\\,\\big|\\,C_{b+1..L}\\bigr)=\\sum_{i=b+1}^{L}H\\bigl(K_{i}\\,\\big|\\,C_{i}\\bigr)=\\sum_{i=b+1}^{L}1=L-b.\n$$\n\nCombining both parts,\n$$\nH\\bigl(K\\,\\big|\\,C,M_{1..b}\\bigr)=0+(L-b)=L-b\\quad\\text{bits}.\n$$", "answer": "$$\\boxed{L-b}$$"}, {"introduction": "Our final practice tackles a critical aspect of implementing any cryptographic system: the integrity of the key generation process. This exercise explores the consequences of using a key space that is smaller than ideally required, a potential implementation flaw. You will use the concept of mutual information to calculate the maximum amount of information about the message that leaks to an adversary, providing a concrete measure of the security lost due to a weakened key space. [@problem_id:1644119]", "id": "1644119", "problem": "A cryptography startup is designing a system based on the one-time pad principle. In their system, a secret message $M$, represented as an $n$-bit binary string, is encrypted by performing a bitwise XOR operation with an $n$-bit key $K$. The resulting $n$-bit ciphertext is $C = M \\oplus K$.\n\nDue to a flaw in the key generation process, the key $K$ is not chosen uniformly at random from the entire set of $2^n$ possible keys. Instead, it is chosen uniformly at random from a specific, publicly known subset of keys, denoted by $\\mathcal{S}$. The size of this subset is known to be $|\\mathcal{S}| = 2^{n-k}$, where $n$ and $k$ are positive integers with $k < n$.\n\nAn adversary, Eve, intercepts a single ciphertext $C$. Eve knows the encryption algorithm ($C = M \\oplus K$) and the exact composition of the restricted key set $\\mathcal{S}$. The amount of information that Eve can learn about the message $M$ from observing the ciphertext $C$ is quantified by the mutual information, $I(M; C)$. This value may depend on the statistical distribution of the original message $M$.\n\nAssuming the message $M$ and the key $K$ are chosen independently, determine the maximum possible value of the mutual information $I(M; C)$ in bits. This maximum is to be taken over all possible probability distributions for the message $M$. Your answer should be an expression in terms of $n$ and/or $k$.\n\n", "solution": "Let $H(\\cdot)$ denote Shannon entropy in bits. The mutual information satisfies\n$$\nI(M;C)=H(C)-H(C|M).\n$$\nSince $C=M\\oplus K$ and, for any fixed $m$, the map $k\\mapsto c=m\\oplus k$ is a bijection on $\\{0,1\\}^{n}$, we have\n$$\nH(C|M=m)=H(K) \\quad \\text{for all } m,\n$$\nand because $M$ and $K$ are independent, this yields\n$$\nH(C|M)=H(K).\n$$\nGiven that $K$ is uniform on $\\mathcal{S}$ with $|\\mathcal{S}|=2^{n-k}$, we obtain\n$$\nH(K)=\\log_{2}|\\mathcal{S}|=\\log_{2}\\left(2^{n-k}\\right)=n-k.\n$$\nTherefore\n$$\nI(M;C)=H(C)-(n-k).\n$$\nSince $C$ takes values in $\\{0,1\\}^{n}$, we have the bound $H(C)\\leq n$, which implies\n$$\nI(M;C)\\leq n-(n-k)=k.\n$$\nTo show this upper bound is achievable, choose $M$ to be uniform over $\\{0,1\\}^{n}$. Then for any $c$,\n$$\nP_{C}(c)=\\sum_{m}P_{M}(m)P_{K}(c\\oplus m)=\\frac{1}{2^{n}}\\sum_{m}P_{K}(c\\oplus m)=\\frac{1}{2^{n}}\\sum_{k}P_{K}(k)=\\frac{1}{2^{n}},\n$$\nso $C$ is uniform and $H(C)=n$. Substituting gives\n$$\nI(M;C)=n-(n-k)=k.\n$$\nHence, the maximum over all message distributions is $k$ bits.", "answer": "$$\\boxed{k}$$"}]}