{"hands_on_practices": [{"introduction": "To build our understanding of secret key agreement, we begin with the most fundamental requirement: the key material must be completely unpredictable to an eavesdropper. This initial practice explores a scenario where information is structurally hidden from Eve, analogous to the perfect secrecy afforded by a one-time pad. By working through this exercise [@problem_id:1656964], you will see how mutual information quantifies secrecy and discover a situation where an eavesdropper's partial knowledge gives her zero information about the final secret bit.", "problem": "In a communication scenario designed for secret key generation, Alice produces a random bit $X$ for transmission to Bob. Alice's source bit is generated by taking the exclusive-or (XOR, denoted by $\\oplus$) of two independent and secret bits, $S_1$ and $S_2$. Each of $S_1$ and $S_2$ is the result of a fair coin flip, independently taking a value from the set $\\{0, 1\\}$ with a probability of $1/2$. Therefore, Alice's source bit is $X = S_1 \\oplus S_2$.\n\nBob receives a version of $X$, denoted by $Y$, over a noisy communication link. This link is modeled as a Binary Symmetric Channel (BSC), which is a channel where each transmitted bit is flipped (i.e., $0 \\to 1$ or $1 \\to 0$) with a fixed crossover probability $\\epsilon$, where $0 < \\epsilon < 1/2$.\n\nMeanwhile, an eavesdropper, Eve, is able to perfectly intercept one of Alice's secret bits. Eve's observation is denoted by $Z$, where $Z = S_1$.\n\nThe maximum rate at which Alice and Bob can generate a shared secret key that is information-theoretically secure from Eve is given by $K = I(X; Y) - I(X; Z)$, where $I(\\cdot; \\cdot)$ denotes the mutual information and all logarithms are base 2.\n\nCalculate the secret key rate $K$ in bits per channel use. Express your answer as a closed-form analytic expression in terms of the crossover probability $\\epsilon$. You may use the binary entropy function $H_b(\\epsilon)$ in your answer.", "solution": "Let $S_{1}$ and $S_{2}$ be independent $\\operatorname{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$ random variables, and let $X=S_{1}\\oplus S_{2}$. Since $X=0$ if and only if $S_{1}=S_{2}$ and $X=1$ if and only if $S_{1}\\neq S_{2}$, and the four pairs $(0,0),(0,1),(1,0),(1,1)$ are equally likely, we have\n$$\n\\Pr(X=0)=\\Pr(S_{1}=S_{2})=\\frac{1}{2}, \\quad \\Pr(X=1)=\\frac{1}{2},\n$$\nso $H(X)=1$.\n\nBob observes $Y$ through a BSC with crossover probability $\\epsilon$, so\n$$\n\\Pr(Y\\neq X)=\\epsilon, \\quad \\Pr(Y=X)=1-\\epsilon.\n$$\nFor a BSC, $H(Y|X)=H_{b}(\\epsilon)$ by definition of conditional entropy and the channel law. The output $Y$ is also uniform because the channel is symmetric and $X$ is uniform:\n$$\n\\Pr(Y=1)=\\Pr(Y=1|X=1)\\Pr(X=1)+\\Pr(Y=1|X=0)\\Pr(X=0)=(1-\\epsilon)\\frac{1}{2}+\\epsilon\\frac{1}{2}=\\frac{1}{2},\n$$\nhence $H(Y)=1$. Therefore,\n$$\nI(X;Y)=H(Y)-H(Y|X)=1-H_{b}(\\epsilon).\n$$\n\nEve observes $Z=S_{1}$. We compute $I(X;Z)=H(X)-H(X|Z)$. Given $Z=z$, we have $X=z\\oplus S_{2}$ with $S_{2}\\sim \\operatorname{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$ independent of $Z$, so $X|Z=z$ is uniform:\n$$\n\\Pr(X=0|Z=z)=\\Pr(S_{2}=z)=\\frac{1}{2}, \\quad \\Pr(X=1|Z=z)=\\frac{1}{2},\n$$\nwhich implies $H(X|Z=z)=1$ for both $z\\in\\{0,1\\}$, and hence $H(X|Z)=1$. Consequently,\n$$\nI(X;Z)=H(X)-H(X|Z)=1-1=0.\n$$\n\nThe secret key rate is\n$$\nK=I(X;Y)-I(X;Z)=\\left(1-H_{b}(\\epsilon)\\right)-0=1-H_{b}(\\epsilon).\n$$", "answer": "$$\\boxed{1-H_{b}(\\epsilon)}$$", "id": "1656964"}, {"introduction": "While perfect information hiding is the ideal, most practical scenarios involve some degree of information leakage. This next problem [@problem_id:1657000] models such a case, directly illustrating the core principle of secret key capacity: the achievable secret rate is precisely the information advantage that Alice and Bob hold over Eve. Your task will be to quantify the information shared between Alice and Bob, $I(X;Y)$, and then subtract the portion of that information leaked to Eve, $I(X;Z)$, to find the net rate of secure bits they can generate.", "problem": "In a communication scenario designed for secret key generation, Alice transmits information to Bob, while an eavesdropper, Eve, listens in. The protocol proceeds as follows for each \"channel use\":\n\n1.  Alice generates a pair of independent and uniformly random bits, $X_1$ and $X_2$. This means for each bit, the probability of it being 0 is equal to the probability of it being 1, which is $1/2$. The pair $(X_1, X_2)$ constitutes the total information Alice sends in one channel use.\n\n2.  Bob receives a corresponding pair of bits, $(Y_1, Y_2)$. Bob's channel is partially noisy. The first bit is received perfectly, such that $Y_1 = X_1$. The second bit is subject to noise, modeled by a Binary Symmetric Channel (BSC) with a crossover probability $p$, where $0 < p < 1/2$. This implies that the probability of Bob receiving the wrong second bit is $P(Y_2 \\neq X_2) = p$.\n\n3.  Eve is unable to tap into Bob's channel directly. Instead, her equipment perfectly intercepts the parity of the bits Alice sent. Eve observes the variable $Z = X_1 \\oplus X_2$, where $\\oplus$ denotes the addition modulo 2 (XOR operation). This observation is noiseless.\n\nAlice and Bob can use a public, authenticated channel for post-processing (reconciliation and privacy amplification) to agree on a shared secret key. The maximum achievable rate of this secret key is the difference between the information Bob receives about Alice's transmission and the information Eve receives.\n\nCalculate the maximum achievable secret key rate in bits per channel use. Express your answer as a function of the crossover probability $p$. You may use the binary entropy function $H_b(p)$ in your answer. All logarithms are base 2.", "solution": "Let $X \\triangleq (X_{1},X_{2})$, $Y \\triangleq (Y_{1},Y_{2})$, and $Z \\triangleq X_{1} \\oplus X_{2}$. The problem states that the maximum achievable secret key rate is the difference\n$$\nR = I(X;Y) - I(X;Z).\n$$\nWe compute each term.\n\nFirst, compute $I(X;Y)$. Since $Y_{1}=X_{1}$ (noiseless) and $Y_{2}=X_{2}\\oplus N$ with $N \\sim \\mathrm{Bern}(p)$ independent of $(X_{1},X_{2})$, we have\n$$\nH(Y\\mid X)=H(Y_{1},Y_{2}\\mid X_{1},X_{2})=H(Y_{1}\\mid X_{1},X_{2})+H(Y_{2}\\mid X_{1},X_{2})=0+H(N)=H_{b}(p).\n$$\nBecause $X_{1}$ and $X_{2}$ are independent and uniform, $Y_{1}$ is uniform, and $Y_{2}$ is also uniform (a BSC output with uniform input). Moreover, $Y_{1}$ depends only on $X_{1}$, and $Y_{2}$ depends only on $(X_{2},N)$, which are independent of $X_{1}$; hence $Y_{1}$ and $Y_{2}$ are independent. Therefore,\n$$\nH(Y)=H(Y_{1})+H(Y_{2})=1+1=2,\n$$\nand thus\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)=2-H_{b}(p).\n$$\n\nNext, compute $I(X;Z)$. Since $Z$ is a deterministic function of $X$, we have $H(Z\\mid X)=0$ and hence\n$$\nI(X;Z)=H(Z).\n$$\nWith $X_{1}$ and $X_{2}$ independent and uniform, the parity $Z=X_{1}\\oplus X_{2}$ is uniform, so $H(Z)=1$. Hence\n$$\nI(X;Z)=1.\n$$\n\nCombining the two,\n$$\nR=I(X;Y)-I(X;Z)=(2-H_{b}(p)) - 1 = 1 - H_{b}(p).\n$$\nThis is nonnegative for $0<p<\\frac{1}{2}$ since $H_{b}(p)\\in(0,1)$, and it matches the intuition that as $p \\to 0$ the rate approaches $1$, and as $p \\to \\frac{1}{2}$ the rate approaches $0$.", "answer": "$$\\boxed{1 - H_{b}(p)}$$", "id": "1657000"}, {"introduction": "Finally, we scale our analysis to long sequences of data, which reflects how real-world communication systems operate. This practice introduces a crucial asymptotic concept: the *rate* of information leakage [@problem_id:1656961]. You will analyze a hypothetical scenario where, for any finite sequence, Eve gains some information; however, her knowledge grows much more slowly than the information shared between Alice and Bob. This exercise demonstrates how a secure key can be distilled by ensuring the rate of Eve's information gain approaches zero, leaving Alice and Bob with a decisive information advantage in the long run.", "problem": "In a communication scenario involving three parties—Alice, Bob, and an eavesdropper Eve—a scheme for generating a shared secret key is being analyzed. Alice generates a sequence of $n$ bits, denoted by $X^n = (X_1, X_2, \\dots, X_n)$. Each bit $X_i$ is an independent and identically distributed random variable, chosen uniformly at random, such that $P(X_i=1) = P(X_i=0) = 1/2$.\n\nAlice transmits this sequence to Bob over a noisy channel. This channel is modeled as a Binary Symmetric Channel (BSC), which means each bit is flipped independently with a constant probability $p$. The sequence received by Bob is denoted by $Y^n$.\n\nMeanwhile, Eve attempts to gain information about Alice's sequence. However, due to her limited access, she cannot observe the sequence $X^n$ directly. Instead, Eve is only able to determine the total number of ones in Alice's original sequence. Let this value be $W = \\sum_{i=1}^{n} X_i$.\n\nThe maximum rate (in bits per channel use) at which Alice and Bob can generate a secret key that remains perfectly secure from Eve is given by the secret key capacity. For this scenario, you are asked to determine this rate in the asymptotic limit of very long sequences (i.e., as $n \\to \\infty$).\n\nProvide a closed-form analytic expression for this secret key rate as a function of the crossover probability $p$. You may use the binary entropy function $H_b(p)$ in your answer.", "solution": "Let $\\{X_{i}\\}_{i=1}^{n}$ be IID with $P(X_{i}=1)=P(X_{i}=0)=\\frac{1}{2}$, and let the channel to Bob be a BSC with crossover probability $p$. Then $Y_{i}=X_{i}\\oplus N_{i}$ with $\\{N_{i}\\}$ IID Bernoulli$(p)$ and independent of $\\{X_{i}\\}$. Eve observes only $W=\\sum_{i=1}^{n}X_{i}$.\n\nIn the source model with unlimited public discussion overheard by Eve, an upper bound on the secret key capacity per symbol is\n$$\n\\limsup_{n\\to\\infty}\\frac{1}{n}I(X^{n};Y^{n}\\mid E_{n}),\n$$\nwhere $E_{n}$ denotes Eve’s total observation for blocklength $n$. Here $E_{n}=W$. Using the chain rule identity $I(A;B\\mid C)=I(A;B)-I(C;B)+I(C;B\\mid A)$ with $A=X^{n}$, $B=Y^{n}$, $C=W$ and the fact that $W$ is a deterministic function of $X^{n}$, hence $I(W;Y^{n}\\mid X^{n})=0$, we obtain\n$$\nI(X^{n};Y^{n}\\mid W)=I(X^{n};Y^{n})-I(W;Y^{n}).\n$$\nTherefore,\n$$\n\\frac{1}{n}I(X^{n};Y^{n}\\mid W)=\\frac{1}{n}I(X^{n};Y^{n})-\\frac{1}{n}I(W;Y^{n}).\n$$\nSince $(X_{i},Y_{i})$ are IID pairs generated by a memoryless BSC, $I(X^{n};Y^{n})=n\\,I(X;Y)$. Moreover, $I(W;Y^{n})\\leq H(W)\\leq \\log_{2}(n+1)$, which implies $\\lim_{n\\to\\infty}\\frac{1}{n}I(W;Y^{n})=0$. Hence\n$$\n\\lim_{n\\to\\infty}\\frac{1}{n}I(X^{n};Y^{n}\\mid W)=I(X;Y).\n$$\nThis yields the converse bound\n$$\nR_{\\text{SK}}\\leq I(X;Y).\n$$\n\nTo evaluate $I(X;Y)$, note that with $X\\sim\\text{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$ and a BSC$(p)$, we have $H(Y\\mid X)=H_{b}(p)$ and $Y\\sim\\text{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$ so $H(Y)=1$. Therefore\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)=1-H_{b}(p).\n$$\n\nAchievability follows by the standard information reconciliation plus privacy amplification scheme. Specifically, Alice sends approximately $nH(X\\mid Y)=nH_{b}(p)$ bits of public syndrome information enabling Bob to reconstruct $X^{n}$ with vanishing error. Eve’s total side information consists of the public discussion plus $W$. The residual extractable secret randomness from $X^{n}$ that is uniform and independent of Eve is approximately $H(X^{n}\\mid \\text{public},W)$, which is lower bounded by $H(X^{n}\\mid W)-nH(X\\mid Y)-o(n)$. Since $H(X^{n}\\mid W)\\geq H(X^{n})-H(W)=n-\\mathcal{O}(\\log_{2}n)$, the achievable key length is $n- nH_{b}(p)-\\mathcal{O}(\\log_{2}n)$, yielding the per-symbol rate\n$$\n\\lim_{n\\to\\infty}\\frac{1}{n}\\left(n-nH_{b}(p)-\\mathcal{O}(\\log_{2}n)\\right)=1-H_{b}(p).\n$$\n\nCombining the converse and achievability, the secret key capacity in bits per channel use is\n$$\n1-H_{b}(p).\n$$", "answer": "$$\\boxed{1 - H_{b}(p)}$$", "id": "1656961"}]}