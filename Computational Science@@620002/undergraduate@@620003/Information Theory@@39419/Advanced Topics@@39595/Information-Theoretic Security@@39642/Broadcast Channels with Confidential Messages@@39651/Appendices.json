{"hands_on_practices": [{"introduction": "Quantifying the security of a communication system begins with a fundamental question: how much more information does the intended recipient receive compared to an eavesdropper? This exercise provides a concrete scenario to practice this core calculation. By computing the mutual information for both the legitimate and eavesdropper's channels, you will determine the 'information advantage', a direct measure of the potential for secure communication in this broadcast setup. [@problem_id:1606121]", "problem": "A technology firm uses a digital communication system to broadcast one of three possible strategic directives to its team. The set of directives is $\\mathcal{X} = \\{\\text{PROJECT ALPHA}, \\text{PROJECT BETA}, \\text{PROJECT GAMMA}\\}$. These are encoded as symbols $x \\in \\{0, 1, 2\\}$, respectively. Based on historical data, each directive is chosen with equal probability. The transmission is sent over a broadcast channel, reaching both an intended employee (receiver 1) and a corporate competitor who is eavesdropping (receiver 2).\n\nLet $X$ be the random variable for the chosen directive, with outputs $Y_1$ for the employee and $Y_2$ for the competitor. Both output alphabets are $\\mathcal{Y}_1 = \\mathcal{Y}_2 = \\{0, 1, 2\\}$. The communication channels are characterized by the following transition probability matrices, where $W_{ij} = P(Y=j | X=i)$:\n\nThe channel to the employee (receiver 1) is described by matrix $W_1$:\n$$\nW_1 = \\begin{pmatrix} 0.8 & 0.1 & 0.1 \\\\ 0.1 & 0.8 & 0.1 \\\\ 0.1 & 0.1 & 0.8 \\end{pmatrix}\n$$\n\nThe channel to the competitor (receiver 2) is described by matrix $W_2$:\n$$\nW_2 = \\begin{pmatrix} 0.50 & 0.25 & 0.25 \\\\ 0.25 & 0.50 & 0.25 \\\\ 0.40 & 0.30 & 0.30 \\end{pmatrix}\n$$\n\nCalculate the firm's \"information advantage,\" which is defined as the difference between the mutual information of the employee's channel and the competitor's channel, $I(X;Y_1) - I(X;Y_2)$. All information-theoretic quantities are to be calculated using the base-2 logarithm.\n\nExpress your answer in bits, rounded to four significant figures.", "solution": "Let $P(X=i)=\\frac{1}{3}$ for $i\\in\\{0,1,2\\}$. For any discrete memoryless channel, $I(X;Y)=H(Y)-H(Y|X)$ with base-2 logarithms.\n\nEmployee’s channel $W_{1}$:\nThe output distribution is\n$$\nP_{Y_{1}}(j)=\\sum_{i=0}^{2}P(X=i)W_{1,ij}=\\frac{1}{3}\\sum_{i=0}^{2}W_{1,ij}=\\frac{1}{3},\\quad j\\in\\{0,1,2\\},\n$$\nso $H(Y_{1})=\\log_{2}(3)$. The conditional entropy per row is\n$$\nH\\big((0.8,0.1,0.1)\\big)=-\\big(0.8\\log_{2}0.8+0.1\\log_{2}0.1+0.1\\log_{2}0.1\\big)\n=-\\big(0.8\\log_{2}0.8+0.2\\log_{2}0.1\\big).\n$$\nUsing $\\log_{2}(0.8)=-0.321928094887362$ and $\\log_{2}(0.1)=-3.321928094887362$,\n$$\nH(Y_{1}|X)=0.921928094887362,\n$$\nand hence\n$$\nI(X;Y_{1})=\\log_{2}(3)-0.921928094887362=0.663034405833794\\ \\text{bits}.\n$$\n\nCompetitor’s channel $W_{2}$:\nThe output distribution is\n$$\nP_{Y_{2}}=\\left(\\frac{1.15}{3},\\frac{1.05}{3},\\frac{0.80}{3}\\right)=\\left(\\frac{23}{60},\\frac{7}{20},\\frac{4}{15}\\right).\n$$\nThus\n$$\nH(Y_{2})=-\\sum_{j}P_{Y_{2}}(j)\\log_{2}P_{Y_{2}}(j)\n= -\\left(\\frac{23}{60}\\log_{2}\\frac{23}{60}+\\frac{7}{20}\\log_{2}\\frac{7}{20}+\\frac{4}{15}\\log_{2}\\frac{4}{15}\\right)\n$$\n$$\n=0.530275978494744+0.530100610490415+0.508504158828938=1.568880747814097\\ \\text{bits}.\n$$\nThe conditional entropy is the average of the row entropies:\n$$\nH(Y_{2}|X)=\\frac{1}{3}\\left[H(0.5,0.25,0.25)+H(0.5,0.25,0.25)+H(0.4,0.3,0.3)\\right]\n=\\frac{2}{3}\\cdot 1.5+\\frac{1}{3}\\cdot H(0.4,0.3,0.3).\n$$\nWith $\\log_{2}(0.4)=-1.321928094887362$ and $\\log_{2}(0.3)=-1.736965594166206$,\n$$\nH(0.4,0.3,0.3)=-\\left(0.4\\log_{2}0.4+0.3\\log_{2}0.3+0.3\\log_{2}0.3\\right)=1.570950594454668,\n$$\nso\n$$\nH(Y_{2}|X)=\\frac{2}{3}\\cdot 1.5+\\frac{1}{3}\\cdot 1.570950594454668=1.523650198151556,\n$$\nand\n$$\nI(X;Y_{2})=1.568880747814097-1.523650198151556=0.045230549662541\\ \\text{bits}.\n$$\n\nTherefore, the information advantage is\n$$\nI(X;Y_{1})-I(X;Y_{2})=0.663034405833794-0.045230549662541=0.617803856171253\\ \\text{bits},\n$$\nwhich rounded to four significant figures is $0.6178$ bits.", "answer": "$$\\boxed{0.6178}$$", "id": "1606121"}, {"introduction": "While calculating information advantage for a specific setup is crucial, it's often more powerful to understand the general principles that govern secure communication. This practice problem moves beyond a single case to explore the fundamental condition required for a positive secrecy capacity, where $C_s \\gt 0$. You will investigate how the relative quality of the main channel and the eavesdropper's channel dictates whether secret communication is theoretically possible at a non-zero rate. [@problem_id:1606176]", "problem": "An intelligence agency is transmitting a secret message, encoded as a binary random variable $X \\in \\{0, 1\\}$, to a friendly operative (Bob). Unfortunately, an adversary (Eve) is also intercepting the transmission. This scenario is modeled as a binary broadcast channel.\n\nThe communication link to Bob is modeled as a binary Z-channel. In this channel, if the input bit is '1', it is always received correctly as '1'. However, if the input bit is '0', it is correctly received as '0' with probability $1-p_1$ and is incorrectly flipped to a '1' with probability $p_1$.\n\nThe communication link to Eve is modeled as a second, independent binary Z-channel. Similarly, an input bit '1' is always received by Eve as '1'. An input bit '0' is received as '0' with probability $1-p_2$ and is flipped to '1' with probability $p_2$.\n\nThe parameters $p_1$ and $p_2$ are channel error probabilities, and they lie in the range $0 \\le p_1 < 1$ and $0 \\le p_2 < 1$.\n\nThe secrecy capacity, $C_s$, of this broadcast channel determines if secure communication is possible. It is defined as the maximum possible rate of secret information transmission and is given by the formula $C_s = \\max_{P(X)} [I(X; Y_1) - I(X; Y_2)]$, where $Y_1$ is the output received by Bob, $Y_2$ is the output received by Eve, $P(X)$ is the probability distribution of the input message $X$, and $I(\\cdot;\\cdot)$ denotes the mutual information. A positive secrecy capacity ($C_s > 0$) is the necessary and sufficient condition for secure communication to be possible at a non-zero rate.\n\nWhich of the following conditions on the error probabilities $p_1$ and $p_2$ guarantees that secure communication is possible (i.e., $C_s > 0$)?\n\nA. $p_1 > p_2$\n\nB. $p_1 < p_2$\n\nC. $p_1 = p_2$\n\nD. $p_1 + p_2 < 1$\n\nE. $p_1 + p_2 > 1$\n\nF. $|p_1 - p_2| > 0.5$", "solution": "The secrecy capacity is defined as $C_{s}=\\max_{P(X)}\\left[I(X;Y_{1})-I(X;Y_{2})\\right]$, where $Y_{1}$ is Bob’s Z-channel output with parameter $p_{1}$ and $Y_{2}$ is Eve’s Z-channel output with parameter $p_{2}$. Denote $q=P(X=0)$ and $1-q=P(X=1)$. For a binary Z-channel with parameter $p\\in[0,1)$, the transition law is: if $X=1$ then $Y=1$ deterministically; if $X=0$ then $Y=0$ with probability $1-p$ and $Y=1$ with probability $p$.\n\nFor fixed $p$ and $q$, the output distribution is $P(Y=1)=1-q(1-p)$ and $P(Y=0)=q(1-p)$. The conditional entropy is $H(Y|X)=q\\,H_{b}(p)$ because $H(Y|X=0)=H_{b}(p)$ and $H(Y|X=1)=0$. The output entropy is $H(Y)=H_{b}\\!\\left(q(1-p)\\right)$, where we define the binary entropy function in bits by $H_{b}(t)=-t\\log_2 t-(1-t)\\log_2(1-t)$ for $t\\in[0,1]$. Hence the mutual information for a Z-channel with parameter $p$ under input distribution $q$ is\n$$\nI_{q}(p)\\equiv I(X;Y)=H_{b}\\!\\left(q(1-p)\\right)-q\\,H_{b}(p).\n$$\nTherefore,\n$$\nI(X;Y_{1})-I(X;Y_{2})=I_{q}(p_{1})-I_{q}(p_{2})=H_{b}\\!\\left(q(1-p_{1})\\right)-H_{b}\\!\\left(q(1-p_{2})\\right)-q\\left[H_{b}(p_{1})-H_{b}(p_{2})\\right].\n$$\n\nTo determine when $C_{s}>0$, we analyze the monotonicity of $I_{q}(p)$ with respect to $p$. Differentiating $I_{q}(p)$ with respect to $p$ yields\n$$\n\\frac{\\partial I_{q}}{\\partial p}(p)=\\frac{\\partial}{\\partial p}\\left[H_{b}\\!\\left(q(1-p)\\right)-q\\,H_{b}(p)\\right]\n=H_{b}'\\!\\left(q(1-p)\\right)\\cdot(-q)-q\\,H_{b}'(p),\n$$\nand since $H_{b}'(t)=\\log_2\\!\\left(\\frac{1-t}{t}\\right)$, we get\n$$\n\\frac{\\partial I_{q}}{\\partial p}(p)=-q\\log_2\\!\\left(\\frac{1-q(1-p)}{q(1-p)}\\right)-q\\log_2\\!\\left(\\frac{1-p}{p}\\right)\n=-q\\log_2\\!\\left(\\frac{1-q(1-p)}{q\\,p}\\right).\n$$\nFor $q\\in(0,1)$ and $p\\in(0,1)$, observe that $1-q(1-p)=(1-q)+q p>q p$, so the ratio $\\frac{1-q(1-p)}{q\\,p}>1$ and thus $\\log_2\\!\\left(\\frac{1-q(1-p)}{q\\,p}\\right)>0$. Hence\n$$\n\\frac{\\partial I_{q}}{\\partial p}(p)<0 \\quad \\text{for all } q\\in(0,1),\\;p\\in(0,1).\n$$\nTherefore, for any fixed $q\\in(0,1)$, $I_{q}(p)$ is strictly decreasing in $p$. It follows that:\n- If $p_{1}<p_{2}$, then for any $q\\in(0,1)$, $I_{q}(p_{1})>I_{q}(p_{2})$, so there exists such a $q$ with $I_{q}(p_{1})-I_{q}(p_{2})>0$, and consequently $C_{s}=\\max_{q\\in[0,1]}[I_{q}(p_{1})-I_{q}(p_{2})]>0$.\n- If $p_{1}=p_{2}$, then $I_{q}(p_{1})-I_{q}(p_{2})=0$ for all $q$, so $C_{s}=0$.\n- If $p_{1}>p_{2}$, then for any $q\\in(0,1)$, $I_{q}(p_{1})<I_{q}(p_{2})$, hence $I_{q}(p_{1})-I_{q}(p_{2})<0$, and the maximum over $q$ is attained at a trivial input (e.g., $q=0$ or $q=1$) with value $0$, so $C_{s}\\le 0$ and cannot be positive.\n\nThus the necessary and sufficient condition for $C_{s}>0$ is $p_{1}<p_{2}$. Among the listed options, this corresponds exactly to option B. All other listed conditions are neither necessary nor sufficient for $C_{s}>0$ in general.", "answer": "$$\\boxed{B}$$", "id": "1606176"}, {"introduction": "Secrecy is not always about exploiting random noise in a channel; it can also be achieved through clever deterministic design. This problem challenges you to analyze a broadcast channel where the outputs are deterministic functions of the input, revealing different pieces of information to the recipient and the eavesdropper. By finding the optimal input strategy, you will discover how to create perfect secrecy by structuring the signal set itself, a key concept in information-theoretic security. [@problem_id:1606180]", "problem": "A transmitter sends a symbol $X$ chosen from an input alphabet $\\mathcal{X} = \\{0, 1, 2, 3\\}$ over a deterministic, discrete, memoryless broadcast channel. The channel has two outputs, $Y_1$ and $Y_2$. The output $Y_1$ is received by a legitimate user, Bob, while the output $Y_2$ is intercepted by an eavesdropper, Eve.\n\nThe channel is defined by the following deterministic functions:\n- Bob's received symbol is $Y_1 = X \\pmod 2$.\n- Eve's received symbol is $Y_2 = X \\pmod 3$.\n\nThe goal is to send a message to Bob with the highest possible rate, while keeping it perfectly secret from Eve. This maximum achievable secret rate is known as the secrecy capacity, $C_s$.\n\nDetermine the secrecy capacity of this channel. Provide your final answer as an exact analytic expression in units of bits per channel use.", "solution": "We model the system as a discrete memoryless wiretap channel with input $X \\in \\{0,1,2,3\\}$ and deterministic outputs $Y_{1} = X \\bmod 2$ (to Bob) and $Y_{2} = X \\bmod 3$ (to Eve). The secrecy capacity for a general wiretap channel is\n$$\nC_{s} = \\max_{P_{U,X}} \\left[I(U;Y_{1}) - I(U;Y_{2})\\right], \\quad \\text{with } U \\to X \\to (Y_{1},Y_{2}).\n$$\nSince Bob’s channel is deterministic, we can upper bound any such rate by\n$$\nI(U;Y_{1}) - I(U;Y_{2}) = I(U;Y_{1}\\mid Y_{2}) - I(U;Y_{2}\\mid Y_{1}) \\leq I(U;Y_{1}\\mid Y_{2}) \\leq I(X;Y_{1}\\mid Y_{2}),\n$$\nwhere the last inequality follows from data processing given $U \\to X \\to (Y_{1},Y_{2})$. Because $Y_{1}$ is a deterministic function of $X$, we have\n$$\nI(X;Y_{1}\\mid Y_{2}) = H(Y_{1}\\mid Y_{2}).\n$$\nThus\n$$\nC_{s} \\leq \\max_{P_{X}} H(Y_{1}\\mid Y_{2}).\n$$\nThis upper bound is achievable by choosing $U = Y_{1}$ (which satisfies $U \\to X \\to (Y_{1},Y_{2})$ since $Y_{1}$ is a function of $X$), yielding\n$$\nI(U;Y_{1}) - I(U;Y_{2}) = H(Y_{1}) - I(Y_{1};Y_{2}) = H(Y_{1}\\mid Y_{2}).\n$$\nTherefore,\n$$\nC_{s} = \\max_{P_{X}} H(Y_{1}\\mid Y_{2}).\n$$\n\nWe now compute and maximize $H(Y_{1}\\mid Y_{2})$ over $P_{X}$. The deterministic mapping is\n$$\nX=0 \\mapsto (Y_{1},Y_{2})=(0,0),\\quad X=1 \\mapsto (1,1),\\quad X=2 \\mapsto (0,2),\\quad X=3 \\mapsto (1,0).\n$$\nThese four pairs are distinct, so $(Y_{1},Y_{2})$ is in one-to-one correspondence with $X$, implying\n$$\nH(Y_{1},Y_{2}) = H(X) \\quad \\text{and} \\quad H(Y_{1}\\mid Y_{2}) = H(Y_{1},Y_{2}) - H(Y_{2}) = H(X) - H(Y_{2}).\n$$\nLet $p_{i} = P(X=i)$ for $i \\in \\{0,1,2,3\\}$, and define $q_{0} = P(Y_{2}=0) = p_{0}+p_{3}$, $q_{1} = P(Y_{2}=1) = p_{1}$, $q_{2} = P(Y_{2}=2) = p_{2}$. Since $Y_{2}$ deterministically partitions the inputs as $\\{0,3\\}$, $\\{1\\}$, and $\\{2\\}$, we have\n$$\nH(Y_{1}\\mid Y_{2}) = H(X\\mid Y_{2}) = q_{0}\\, h_{2}\\!\\left(\\frac{p_{0}}{p_{0}+p_{3}}\\right) + q_{1} \\cdot 0 + q_{2} \\cdot 0 = (p_{0}+p_{3})\\, h_{2}\\!\\left(\\frac{p_{0}}{p_{0}+p_{3}}\\right),\n$$\nwhere $h_{2}(t) = -t \\log_{2} t - (1-t)\\log_{2}(1-t)$ is the binary entropy function. Let $s = p_{0}+p_{3} \\in [0,1]$ and $t = \\frac{p_{0}}{p_{0}+p_{3}} \\in [0,1]$ (for $s>0$). Then\n$$\nH(Y_{1}\\mid Y_{2}) = s\\, h_{2}(t).\n$$\nFor fixed $s$, $h_{2}(t)$ is maximized at $t=\\frac{1}{2}$ with value $1$, so the expression becomes $s$. Maximizing over $s \\in [0,1]$ gives the maximum $s=1$, achieved by choosing $p_{1}=p_{2}=0$ and $p_{0}=p_{3}=\\frac{1}{2}$. Therefore,\n$$\n\\max_{P_{X}} H(Y_{1}\\mid Y_{2}) = 1 \\text{ bit}.\n$$\nThis is also intuitive since $Y_{1} \\in \\{0,1\\}$, so $H(Y_{1}) \\leq 1$, and equality is achieved by the choice $P(X=0)=P(X=3)=\\frac{1}{2}$, for which $Y_{1}$ is a fair bit while $Y_{2}$ is constant ($Y_{2}=0$), yielding perfect secrecy.\n\nHence the secrecy capacity is $1$ bit per channel use.", "answer": "$$\\boxed{1}$$", "id": "1606180"}]}