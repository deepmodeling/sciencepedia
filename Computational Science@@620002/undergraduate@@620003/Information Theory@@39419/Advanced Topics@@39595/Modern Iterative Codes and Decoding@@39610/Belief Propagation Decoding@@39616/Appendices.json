{"hands_on_practices": [{"introduction": "The journey of decoding a received signal begins with quantifying the evidence provided by the noisy channel. This first practice focuses on calculating the initial Log-Likelihood Ratio ($LLR$), a fundamental metric that represents our initial belief about a transmitted bit based on the observation. By working through this foundational step, you will see how the physical properties of a communication channel are translated into the mathematical language of belief propagation.[@problem_id:1603925]", "problem": "A simple error correction scheme is the (3,1) repetition code, where a single data bit is encoded into a three-bit codeword by repeating the data bit three times. To transmit data bit '0', the codeword '000' is sent, and to transmit data bit '1', the codeword '111' is sent.\n\nThese codeword bits are transmitted sequentially over a noisy Binary Symmetric Channel (BSC). A BSC is a channel model where each transmitted bit has a probability $p$ of being flipped to the opposite value (a '0' becomes a '1' or a '1' becomes a '0'), and a probability $1-p$ of being transmitted correctly. For this particular communication system, the crossover probability is given as $p = 0.1$.\n\nSuppose a codeword is being transmitted, and the first of the three received bits is observed to be '1'.\n\nModern decoding algorithms, such as belief propagation, compute an initial reliability value for each received bit. This value, representing the channel evidence, is defined as the natural logarithm of the ratio of two conditional probabilities: the probability of receiving a '1' for that bit position, given the original data bit was '0', divided by the probability of receiving a '1' for that bit position, given the original data bit was '1'.\n\nCalculate this reliability value based on the first received bit. Round your final answer to four significant figures.", "solution": "Over a Binary Symmetric Channel with crossover probability $p$, the conditional probabilities for a single transmitted bit are:\n$$P(\\text{receive }1 \\mid \\text{transmit }0) = p, \\quad P(\\text{receive }1 \\mid \\text{transmit }1) = 1-p.$$\nIn the $(3,1)$ repetition code, each transmitted bit equals the original data bit, so the channel evidence (log-likelihood ratio) for a received $1$ is defined as\n$$L = \\ln\\!\\left(\\frac{P(\\text{receive }1 \\mid \\text{data }0)}{P(\\text{receive }1 \\mid \\text{data }1)}\\right) = \\ln\\!\\left(\\frac{p}{1-p}\\right).$$\nWith $p=0.1$, this becomes\n$$L = \\ln\\!\\left(\\frac{0.1}{0.9}\\right) = \\ln\\!\\left(\\frac{1}{9}\\right) = -\\ln(9) \\approx -2.197224576.$$\nRounding to four significant figures gives $-2.197$.", "answer": "$$\\boxed{-2.197}$$", "id": "1603925"}, {"introduction": "Once initial beliefs are established, the Belief Propagation algorithm begins its iterative process of refining these beliefs by passing messages through the code's factor graph. This exercise zooms in on the crucial check-node-to-variable-node update, where information from multiple bits is synthesized to provide new evidence about a single bit. Mastering this core mechanic, which involves the use of the hyperbolic tangent function $ \\tanh $, is key to understanding how the decoder converges towards a correct solution.[@problem_id:1603932]", "problem": "In the field of digital communications, belief propagation is a powerful message-passing algorithm used for decoding error-correcting codes. Consider a simple linear block code defined by a factor graph with three variable nodes, representing the codeword bits ($c_1, c_2, c_3$), and two check nodes, $f_A$ and $f_B$. The check node $f_A$ is connected to $c_1$ and $c_2$ and enforces the parity check constraint $c_1 \\oplus c_2 = 0$ (where $\\oplus$ denotes addition modulo 2). The check node $f_B$ is connected to $c_2$ and $c_3$ and enforces $c_2 \\oplus c_3 = 0$. This configuration is equivalent to a [3,1] repetition code.\n\nA codeword from this code is transmitted over a noisy channel. At the receiver, the initial beliefs about the transmitted bits are captured by the channel Log-Likelihood Ratios (LLRs). The LLR for a bit $c$ is defined as $L(c) = \\ln \\frac{P(c=0 | \\text{observation})}{P(c=1 | \\text{observation})}$. The measured channel LLRs for the three bits are:\n$L_{ch}(c_1) = +0.5$\n$L_{ch}(c_2) = -1.2$\n$L_{ch}(c_3) = -0.8$\n\nYour task is to calculate the very first message sent from the check node $f_A$ to the variable node $c_1$ during the first iteration of the LLR-based belief propagation algorithm. A message in this algorithm is also an LLR value.\n\nExpress your answer as a single real number, rounded to three significant figures.", "solution": "We work in the LLR domain. At the first iteration, variable-to-check messages are initialized by the channel LLRs, so for any variable node $c_i$ and adjacent check $f$,\n$$\nL_{c_i \\to f}^{(1)} = L_{ch}(c_i).\n$$\nFor a parity-check node $f$ enforcing $c_1 \\oplus c_2 = 0$, the LLR message from $f$ to $c_1$ is given by the standard check-node update rule\n$$\nL_{f \\to c_1}^{(1)} = 2 \\arctanh\\!\\left(\\prod_{v \\in \\mathcal{N}(f)\\setminus c_1} \\tanh\\!\\left(\\frac{L_{v \\to f}^{(1)}}{2}\\right)\\right).\n$$\nFor $f_A$ with neighbors $\\{c_1, c_2\\}$, this reduces to\n$$\nL_{f_A \\to c_1}^{(1)} = 2 \\arctanh\\!\\left(\\tanh\\!\\left(\\frac{L_{c_2 \\to f_A}^{(1)}}{2}\\right)\\right).\n$$\nUsing the identity $\\arctanh(\\tanh(x)) = x$ for all real $x$, we obtain\n$$\nL_{f_A \\to c_1}^{(1)} = L_{c_2 \\to f_A}^{(1)}.\n$$\nAt the first iteration, $L_{c_2 \\to f_A}^{(1)} = L_{ch}(c_2) = -1.2$. Therefore,\n$$\nL_{f_A \\to c_1}^{(1)} = -1.2.\n$$\nRounded to three significant figures, the required message is $-1.20$.", "answer": "$$\\boxed{-1.20}$$", "id": "1603932"}, {"introduction": "An ideal decoder always finds the correct codeword, but in practice, iterative decoders like Belief Propagation can fail, especially on challenging channels. This final practice explores what to do in such a scenario, demonstrating how the final posterior $LLR$s serve as a powerful diagnostic tool. You will learn to interpret the $LLR$ magnitudes, $|L|$, as confidence levels, allowing you to identify the most likely error locations in a failed decoding attempt.[@problem_id:1603921]", "problem": "A linear block code is defined by the following parity-check matrix $H$ over the binary field $\\mathbb{F}_2$:\n$$\nH = \\begin{pmatrix}\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nA codeword from this code is transmitted over a noisy channel. A belief propagation (BP) decoder is used to recover the transmitted codeword. After a set number of iterations, the decoder outputs a vector of final posterior Log-Likelihood Ratios (LLRs) for the six coded bits ($c_1, c_2, c_3, c_4, c_5, c_6$). The LLR for a bit $c_i$ is defined as $L(c_i) = \\ln(P(c_i=0|\\text{observation})/P(c_i=1|\\text{observation}))$.\n\nThe final posterior LLR vector is:\n$$\n\\mathbf{L}_{\\text{post}} = [4.1, -0.3, 0.7, -2.5, -0.5, 1.8]\n$$\nA hard decision is made on each bit, where a positive LLR maps to a bit value of 0 and a negative LLR maps to a bit value of 1. The resulting hard-decision vector is not a valid codeword of the code defined by $H$.\n\nYour task is to identify the bit positions that are most likely to be in error in this hard-decision vector. Provide a list of the three most probable error locations, identified by their bit indices (from 1 to 6). The list should be ordered from the most likely error position to the third most likely.", "solution": "We are given the parity-check matrix $H$ and the final posterior LLRs $\\mathbf{L}_{\\text{post}} = [4.1, -0.3, 0.7, -2.5, -0.5, 1.8]$. A hard decision maps $L(c_{i})>0$ to $\\hat{c}_{i}=0$ and $L(c_{i})<0$ to $\\hat{c}_{i}=1$. Applying this to each component yields\n$$\n\\hat{\\mathbf{c}} = [0,\\,1,\\,0,\\,1,\\,1,\\,0].\n$$\nTo verify that $\\hat{\\mathbf{c}}$ is not a valid codeword, compute the syndrome $\\mathbf{s}=H\\hat{\\mathbf{c}}^{T}$ over $\\mathbb{F}_{2}$. Using the rows of $H$:\n- First check: $c_{1}+c_{3}+c_{4}=0+0+1=1$.\n- Second check: $c_{2}+c_{3}+c_{5}=1+0+1=0$.\n- Third check: $c_{1}+c_{2}+c_{6}=0+1+0=1$.\nThus\n$$\n\\mathbf{s}=\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}\\neq\\mathbf{0},\n$$\nso $\\hat{\\mathbf{c}}$ is not a valid codeword.\n\nThe posterior LLR for bit $i$ is $L_{i}=\\ln\\!\\big(P(c_{i}=0|\\text{obs})/P(c_{i}=1|\\text{obs})\\big)$. Given the hard decision $\\hat{c}_{i}$, the posterior probability that this hard decision is wrong equals\n$$\np_{e,i}=\\begin{cases}\n\\displaystyle \\frac{1}{1+\\exp(L_{i})}, & L_{i}>0,\\\n$$1ex]\n\\displaystyle \\frac{1}{1+\\exp(-L_{i})}, & L_{i}<0,\n\\end{cases}\n$$\nwhich can be written compactly as\n$$\np_{e,i}=\\frac{1}{1+\\exp(|L_{i}|)}.\n$$\nTherefore, $p_{e,i}$ is a strictly decreasing function of $|L_{i}|$. The three most probable error locations are thus the three indices with the smallest $|L_{i}|$.\n\nCompute the magnitudes:\n$$\n|L_{1}|=4.1,\\quad |L_{2}|=0.3,\\quad |L_{3}|=0.7,\\quad |L_{4}|=2.5,\\quad |L_{5}|=0.5,\\quad |L_{6}|=1.8.\n$$\nOrdering these from smallest to largest gives indices $2$ $(0.3)$, $5$ $(0.5)$, and $3$ $(0.7)$ as the three smallest magnitudes. Hence, the three most likely error positions, from most to third most likely, are bits $2$, $5$, and $3$.", "answer": "$$\\boxed{\\begin{pmatrix}2 & 5 & 3\\end{pmatrix}}$$", "id": "1603921"}]}