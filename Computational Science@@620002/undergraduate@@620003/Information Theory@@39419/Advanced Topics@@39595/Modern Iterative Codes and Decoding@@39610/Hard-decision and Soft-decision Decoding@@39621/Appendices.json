{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with a classic and highly illustrative scenario. This exercise [@problem_id:1629075] directly contrasts hard-decision and soft-decision decoding using a simple repetition code. By working through a carefully chosen example, you will see firsthand how discarding reliability information can lead to a decoding error, while retaining it allows for the correct decision, vividly demonstrating the fundamental advantage of soft-decision techniques.", "problem": "In a simple digital communication system, a single information bit $b \\in \\{0, 1\\}$ is encoded for reliability using a three-bit repetition code, denoted as $R_3$. If the information bit is $b=0$, the transmitted codeword is $(0,0,0)$. If the information bit is $b=1$, the transmitted codeword is $(1,1,1)$. For transmission, the bit $0$ is mapped to a signal level of $-1$ arbitrary unit, and the bit $1$ is mapped to a signal level of $+1$ arbitrary unit. The codeword is then sent through a noisy channel.\n\nAt the receiver, a sequence of three real-valued numbers, $y = (y_1, y_2, y_3)$, is observed. These values are proportional to the Log-Likelihood Ratios (LLRs) for each received bit. A positive value $y_i$ indicates that the $i$-th bit is more likely to be a $1$, while a negative value indicates it is more likely to be a $0$. The magnitude $|y_i|$ represents the confidence in this assessment.\n\nSuppose the received sequence is $y = (+0.8, +0.9, -2.1)$.\n\nWe will compare two decoding strategies:\n\n1.  **Hard-Decision Decoding**: This decoder first makes an individual, or \"hard,\" decision for each received value $y_i$. A received value $y_i > 0$ is decoded as the bit $\\hat{b}_i = 1$, and a value $y_i < 0$ is decoded as $\\hat{b}_i = 0$. The final decoded bit, $b_{HD}$, is then determined by a majority vote on the sequence of hard-decoded bits $(\\hat{b}_1, \\hat{b}_2, \\hat{b}_3)$.\n\n2.  **Soft-Decision Decoding**: This decoder uses the \"soft\" information, i.e., the actual real values, directly. The decision metric, $L$, is the sum of the received values: $L = y_1 + y_2 + y_3$. The final decoded bit, $b_{SD}$, is determined by the sign of $L$. If $L > 0$, the decoder decides $b_{SD} = 1$. If $L < 0$, it decides $b_{SD} = 0$.\n\nDetermine the decoded bits $b_{HD}$ and $b_{SD}$ for the given received sequence. Present your answer as an ordered pair $(b_{HD}, b_{SD})$.", "solution": "The problem asks us to decode a received sequence $y = (+0.8, +0.9, -2.1)$ using two different methods: hard-decision decoding and soft-decision decoding. The original information bit $b$ was encoded using a three-bit repetition code. We need to find the decoded bits $b_{HD}$ and $b_{SD}$ and present them as a pair.\n\nFirst, let's perform hard-decision decoding. This process involves two steps: first, quantizing each received value to a binary bit (0 or 1), and second, performing a majority vote on the resulting sequence of bits.\n\nThe rule for quantization is:\n- If $y_i > 0$, the hard decision for that bit is $\\hat{b}_i = 1$.\n- If $y_i < 0$, the hard decision for that bit is $\\hat{b}_i = 0$.\n\nApplying this rule to the received sequence $y = (+0.8, +0.9, -2.1)$:\n- For $y_1 = +0.8$: Since $0.8 > 0$, the hard decision is $\\hat{b}_1 = 1$.\n- For $y_2 = +0.9$: Since $0.9 > 0$, the hard decision is $\\hat{b}_2 = 1$.\n- For $y_3 = -2.1$: Since $-2.1 < 0$, the hard decision is $\\hat{b}_3 = 0$.\n\nThis gives us the hard-decision bit sequence $(\\hat{b}_1, \\hat{b}_2, \\hat{b}_3) = (1, 1, 0)$.\n\nThe next step is to apply a majority vote to this sequence. The sequence contains two 1s and one 0. The bit that appears most frequently is 1. Therefore, the majority-logic decoder's output is 1.\nSo, the hard-decision decoded bit is $b_{HD} = 1$.\n\nNext, let's perform soft-decision decoding. This method uses the raw received values without prior quantization. The decision is based on the sign of the sum of the received values, which act as a decision metric $L$.\n\nThe decision metric $L$ is calculated as:\n$$L = y_1 + y_2 + y_3$$\n\nSubstituting the given values:\n$$L = (+0.8) + (+0.9) + (-2.1)$$\n$$L = 1.7 - 2.1$$\n$$L = -0.4$$\n\nThe rule for soft-decision decoding is:\n- If $L > 0$, decide in favor of bit 1.\n- If $L < 0$, decide in favor of bit 0.\n\nIn our case, $L = -0.4$, which is less than 0. According to the rule, the decoder decides in favor of bit 0.\nSo, the soft-decision decoded bit is $b_{SD} = 0$.\n\nThis result highlights a key difference between the two methods. The hard-decision decoder sees two votes for \"1\" and one for \"0\", and decides \"1\". The soft-decision decoder, however, weighs the evidence. It sees two weak votes for \"1\" (from $+0.8$ and $+0.9$) and one very strong vote for \"0\" (from $-2.1$). The single strong evidence for \"0\" outweighs the two weak pieces of evidence for \"1\", causing the final decision to be \"0\".\n\nFinally, we are asked to present the answer as the ordered pair $(b_{HD}, b_{SD})$.\nFrom our calculations, we have $b_{HD} = 1$ and $b_{SD} = 0$.\n\nThus, the final answer is the pair $(1, 0)$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 0 \\end{pmatrix}}$$", "id": "1629075"}, {"introduction": "While soft-decision decoding is powerful, it is crucial to understand the theoretical foundations of its simpler counterpart. This practice [@problem_id:1629077] challenges you to go beyond a naive threshold and derive the *optimal* hard-decision rule. You will explore how to minimize the expected cost of errors when they are not equally consequential and the source bits are not equally likely, providing deep insight into Bayesian decision theory and optimal receiver design.", "problem": "A digital communication system transmits a binary signal representing a bit $X$, where $X$ can be either 0 or 1. The a priori probability of transmitting a 0 is $P(X=0) = p_0$, and the probability of transmitting a 1 is $P(X=1) = 1-p_0$. To transmit the bit, it is first mapped to a voltage level: bit 0 is mapped to $-A$ and bit 1 is mapped to $+A$, where $A$ is a positive constant.\n\nThe signal is transmitted over an Additive White Gaussian Noise (AWGN) channel. The received signal at the detector is a continuous random variable $Y = s + N$, where $s \\in \\{-A, +A\\}$ is the transmitted voltage level, and $N$ is a Gaussian noise component with a mean of zero and a variance of $\\sigma^2$.\n\nThe receiver must decide whether a 0 or a 1 was transmitted based on the observed value $y$ of the received signal $Y$. It employs a simple threshold detector: if $y > \\gamma$, the system decides $\\hat{X}=1$; otherwise, it decides $\\hat{X}=0$.\n\nThe consequences of a decision error are not symmetric. The cost of mistaking a transmitted 0 for a 1 (a $0 \\to 1$ error) is $C_1$, and the cost of mistaking a transmitted 1 for a 0 (a $1 \\to 0$ error) is $C_0$. Both $C_0$ and $C_1$ are positive real constants. A correct decision incurs zero cost.\n\nDetermine the optimal decision threshold $\\gamma$ that minimizes the total expected cost. Your final answer should be an analytic expression in terms of the given parameters: $p_0, A, \\sigma, C_0,$ and $C_1$.", "solution": "Define hypotheses: $H_{0}$ corresponds to transmitting $X=0$ with signal $s=-A$ and prior $P(H_{0})=p_{0}$; $H_{1}$ corresponds to transmitting $X=1$ with signal $s=+A$ and prior $P(H_{1})=1-p_{0}$. The observation model is $Y=s+N$ with $N\\sim\\mathcal{N}(0,\\sigma^{2})$, so the conditional densities are\n$$\np(y\\mid H_{1})=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(y-A)^{2}}{2\\sigma^{2}}\\right),\\qquad\np(y\\mid H_{0})=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(y+A)^{2}}{2\\sigma^{2}}\\right).\n$$\nThe decision costs are: deciding $H_{1}$ when $H_{0}$ is true costs $C_{10}=C_{1}$ (a $0\\to 1$ error), and deciding $H_{0}$ when $H_{1}$ is true costs $C_{01}=C_{0}$ (a $1\\to 0$ error). Correct decisions have zero cost.\n\nFor an observation $y$, the conditional risks are\n$$\nR_{1}(y)=C_{10}P(H_{0}\\mid y)=C_{1}P(H_{0}\\mid y),\\qquad R_{0}(y)=C_{01}P(H_{1}\\mid y)=C_{0}P(H_{1}\\mid y).\n$$\nThe Bayes rule (minimum expected cost) decides $H_{1}$ if $R_{1}(y)<R_{0}(y)$, i.e.,\n$$\nC_{1}P(H_{0}\\mid y)<C_{0}P(H_{1}\\mid y).\n$$\nUsing Bayes’ rule, $P(H_{i}\\mid y)=\\frac{p(y\\mid H_{i})P(H_{i})}{p(y)}$, this is equivalent to the likelihood-ratio test\n$$\n\\frac{p(y\\mid H_{1})}{p(y\\mid H_{0})}>\\frac{C_{1}P(H_{0})}{C_{0}P(H_{1})}=\\frac{C_{1}\\,p_{0}}{C_{0}\\,(1-p_{0})}.\n$$\nCompute the likelihood ratio using the Gaussian densities:\n$$\n\\Lambda(y)\\triangleq\\frac{p(y\\mid H_{1})}{p(y\\mid H_{0})}\n=\\exp\\!\\left(-\\frac{(y-A)^{2}}{2\\sigma^{2}}+\\frac{(y+A)^{2}}{2\\sigma^{2}}\\right)\n=\\exp\\!\\left(\\frac{(y+A)^{2}-(y-A)^{2}}{2\\sigma^{2}}\\right)\n=\\exp\\!\\left(\\frac{4Ay}{2\\sigma^{2}}\\right)=\\exp\\!\\left(\\frac{2Ay}{\\sigma^{2}}\\right).\n$$\nThus the decision rule is\n$$\n\\exp\\!\\left(\\frac{2Ay}{\\sigma^{2}}\\right)>\\frac{C_{1}\\,p_{0}}{C_{0}\\,(1-p_{0})}.\n$$\nSince $\\exp(\\cdot)$ is strictly increasing, this is equivalent to\n$$\n\\frac{2Ay}{\\sigma^{2}}>\\ln\\!\\left(\\frac{C_{1}\\,p_{0}}{C_{0}\\,(1-p_{0})}\\right),\n$$\nwhich, for $A>0$, yields the optimal threshold detector “decide $H_{1}$ if $y>\\gamma$” with\n$$\n\\gamma=\\frac{\\sigma^{2}}{2A}\\,\\ln\\!\\left(\\frac{C_{1}\\,p_{0}}{C_{0}\\,(1-p_{0})}\\right).\n$$\nThis $\\gamma$ minimizes the total expected cost under the given priors and asymmetric costs.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2A}\\,\\ln\\!\\left(\\frac{C_{1}\\,p_{0}}{C_{0}\\,(1-p_{0})}\\right)}$$", "id": "1629077"}, {"introduction": "Having appreciated the value of soft information, we now turn to its formal quantification. This exercise [@problem_id:1629082] focuses on calculating the Log-Likelihood Ratio (LLR), the standard metric for soft information in modern communication systems. By working with a 4-QAM modulation scheme, you will see how to derive this crucial value from the received signal, connecting the physical layer to the inputs required by powerful soft-decision channel decoders.", "problem": "A digital communication system sends data over an Additive White Gaussian Noise (AWGN) channel using 4-ary Quadrature Amplitude Modulation (4-QAM). The average energy per transmitted symbol is normalized to $E_s=1$.\n\nThe four constellation points are defined by a Gray-coded mapping of a pair of bits, $(b_1, b_2)$. The first bit, $b_1$, determines the in-phase (real) component of the symbol, and the second bit, $b_2$, determines the quadrature (imaginary) component. The specific mapping from bits to symbol components is as follows:\n- $b_1=0 \\implies$ In-phase component is $-A$\n- $b_1=1 \\implies$ In-phase component is $+A$\n- $b_2=0 \\implies$ Quadrature component is $+A$\n- $b_2=1 \\implies$ Quadrature component is $-A$\n\nHere, the positive real constant $A$ is chosen such that the average symbol energy equals $E_s=1$. The four possible symbols are assumed to be transmitted with equal probability.\n\nThe received complex value is modeled as $y = s+n$, where $s$ is the transmitted constellation point and $n=n_I+jn_Q$ is the complex Gaussian noise. The real and imaginary parts of the noise, $n_I$ and $n_Q$, are independent random variables, each with a mean of zero and a variance of $\\sigma^2$.\n\nGiven a received value of $y = 0.25 - 0.6j$ and a noise variance per real dimension of $\\sigma^2 = 0.5$, calculate the Log-Likelihood Ratio (LLR) for the first bit, $b_1$. The LLR for a bit $b$ is defined as $L(b) = \\ln\\left(\\frac{P(b=1|y)}{P(b=0|y)}\\right)$ and serves as a soft-decision metric.\n\nRound your final answer to three significant figures.", "solution": "The problem asks for the Log-Likelihood Ratio (LLR) of the first bit, $b_1$, given a received complex value $y$. The LLR is defined as $L(b_1) = \\ln\\left(\\frac{P(b_1=1|y)}{P(b_1=0|y)}\\right)$.\n\nFirst, let's determine the value of the constant $A$. The four constellation points are $s \\in \\{ A+jA, A-jA, -A+jA, -A-jA \\}$. The squared magnitude (energy) of any of these points is $|s|^2 = (\\pm A)^2 + (\\pm A)^2 = 2A^2$. Since all four symbols are equiprobable, the average symbol energy is $E_s = \\frac{1}{4} \\sum_{i=1}^4 |s_i|^2 = \\frac{1}{4}(4 \\times 2A^2) = 2A^2$. Given that $E_s=1$, we have $2A^2 = 1$, which gives $A = 1/\\sqrt{2}$.\n\nUsing Bayes' theorem, the posterior probability of a bit value is $P(b_1=i|y) = \\frac{p(y|b_1=i)P(b_1=i)}{p(y)}$. Since the four symbols are equiprobable, the bits $b_1=0$ and $b_1=1$ are also equiprobable, so $P(b_1=0) = P(b_1=1) = 1/2$. The LLR expression simplifies to:\n$$L(b_1) = \\ln\\left(\\frac{p(y|b_1=1)P(b_1=1)/p(y)}{p(y|b_1=0)P(b_1=0)/p(y)}\\right) = \\ln\\left(\\frac{p(y|b_1=1)}{p(y|b_1=0)}\\right)$$\n\nThe probability density $p(y|b_1=i)$ is the marginal probability, which can be found by summing over all possible symbols corresponding to $b_1=i$. The event $b_1=0$ corresponds to transmitting a symbol with an in-phase component of $-A$. The two such symbols are $s_{00} = -A+jA$ and $s_{01} = -A-jA$. The event $b_1=1$ corresponds to an in-phase component of $+A$, with symbols $s_{10} = A+jA$ and $s_{11} = A-jA$.\nSince the two possibilities for $b_2$ are equally likely for a given $b_1$, we can write:\n$$p(y|b_1=0) = \\frac{1}{2} p(y|s=s_{00}) + \\frac{1}{2} p(y|s=s_{01})$$\n$$p(y|b_1=1) = \\frac{1}{2} p(y|s=s_{10}) + \\frac{1}{2} p(y|s=s_{11})$$\nThe conditional PDF for receiving $y$ given a transmitted symbol $s$ over an AWGN channel is $p(y|s) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{|y-s|^2}{2\\sigma^2}\\right)$.\n\nSubstituting these into the LLR expression:\n$$L(b_1) = \\ln\\left(\\frac{\\frac{1}{2\\pi\\sigma^2}\\left[\\exp\\left(-\\frac{|y-s_{10}|^2}{2\\sigma^2}\\right) + \\exp\\left(-\\frac{|y-s_{11}|^2}{2\\sigma^2}\\right)\\right]}{\\frac{1}{2\\pi\\sigma^2}\\left[\\exp\\left(-\\frac{|y-s_{00}|^2}{2\\sigma^2}\\right) + \\exp\\left(-\\frac{|y-s_{01}|^2}{2\\sigma^2}\\right)\\right]}\\right)$$\nThe constant factors cancel. Let $y = y_I + j y_Q$.\nThe numerator's argument becomes:\n$$ \\exp\\left(-\\frac{(y_I-A)^2}{2\\sigma^2}\\right) \\left[ \\exp\\left(-\\frac{(y_Q-A)^2}{2\\sigma^2}\\right) + \\exp\\left(-\\frac{(y_Q+A)^2}{2\\sigma^2}\\right) \\right] $$\nThe denominator's argument similarly becomes:\n$$ \\exp\\left(-\\frac{(y_I+A)^2}{2\\sigma^2}\\right) \\left[ \\exp\\left(-\\frac{(y_Q-A)^2}{2\\sigma^2}\\right) + \\exp\\left(-\\frac{(y_Q+A)^2}{2\\sigma^2}\\right) \\right] $$\nThe terms in the square brackets are identical and cancel when we take the ratio inside the logarithm.\n$$ L(b_1) = \\ln\\left(\\frac{\\exp\\left(-\\frac{(y_I-A)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(y_I+A)^2}{2\\sigma^2}\\right)}\\right) = \\frac{-(y_I-A)^2}{2\\sigma^2} - \\frac{-(y_I+A)^2}{2\\sigma^2} $$\n$$ L(b_1) = \\frac{1}{2\\sigma^2} \\left[ (y_I+A)^2 - (y_I-A)^2 \\right] = \\frac{1}{2\\sigma^2} [ (y_I^2 + 2Ay_I + A^2) - (y_I^2 - 2Ay_I + A^2) ] $$\n$$ L(b_1) = \\frac{4Ay_I}{2\\sigma^2} = \\frac{2Ay_I}{\\sigma^2} $$\nThis is a general result for the LLR of the in-phase bit in a square QAM constellation.\n\nNow we substitute the given numerical values:\n- $A = 1/\\sqrt{2}$\n- $y = 0.25 - 0.6j \\implies y_I = 0.25$\n- $\\sigma^2 = 0.5$\n\n$$ L(b_1) = \\frac{2 \\times (1/\\sqrt{2}) \\times 0.25}{0.5} = \\frac{\\sqrt{2} \\times 0.25}{0.5} = 0.5\\sqrt{2} $$\nNumerically, this is:\n$$ L(b_1) \\approx 0.5 \\times 1.41421356... = 0.70710678... $$\nRounding the result to three significant figures, we get $0.707$.", "answer": "$$\\boxed{0.707}$$", "id": "1629082"}]}