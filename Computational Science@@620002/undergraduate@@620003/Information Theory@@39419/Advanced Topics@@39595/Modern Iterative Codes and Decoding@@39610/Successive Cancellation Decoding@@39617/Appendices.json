{"hands_on_practices": [{"introduction": "The fundamental principle behind polar codes is channel polarization, the process of transforming identical communication channels into a set of synthetic channels with polarized reliabilities—some nearly perfect, others nearly useless. This exercise [@problem_id:1661168] provides a foundational look at this phenomenon using the analytically tractable Binary Erasure Channel (BEC). By calculating the capacities of the two synthetic channels created from a single polarization step, you will gain a quantitative understanding of how one channel improves while the other degrades, which is the core mechanism that makes polar codes work.", "problem": "Channel polarization is a foundational technique in modern coding theory, used to construct channels that are either nearly perfect or nearly useless from multiple instances of a given, non-extreme channel. This allows for the design of highly efficient error-correcting codes, such as polar codes.\n\nConsider a memoryless Binary Erasure Channel (BEC), denoted by $W$. This channel has an input alphabet $\\mathcal{X} = \\{0, 1\\}$ and an output alphabet $\\mathcal{Y} = \\{0, 1, e\\}$, where 'e' represents an erasure. For any input bit $x \\in \\{0, 1\\}$, the channel correctly transmits the bit (i.e., the output is $y=x$) with probability $1-\\epsilon$, and it erases the bit (i.e., the output is $y=e$) with probability $\\epsilon$. The capacity of this channel is $I(W) = 1-\\epsilon$.\n\nWe apply a single step of the Arıkan polar transform to two independent uses of this channel $W$. Two information bits, $u_1$ and $u_2$, are drawn from $\\{0, 1\\}$. They are encoded into two channel inputs, $x_1$ and $x_2$, using the transformation $x_1 = u_1 \\oplus u_2$ and $x_2 = u_2$, where $\\oplus$ denotes addition modulo 2. These are then transmitted over the two channel instances, yielding outputs $y_1$ and $y_2$.\n\nThis process creates two new synthetic channels, $W^-$ and $W^+$, which are decoded successively.\n1. The channel $W^-$ is used to transmit $u_1$. Its input is $u_1$ and its effective output is the pair $(y_1, y_2)$. The decoding of $u_1$ is performed assuming no knowledge of the value of $u_2$.\n2. The channel $W^+$ is used to transmit $u_2$. Its input is $u_2$ and its effective output is the pair $(y_1, y_2)$. Crucially, the decoder for $u_2$ is assumed to have perfect knowledge of $u_1$, as if it has already been successfully decoded in the first step.\n\nAssuming the information bits $u_1$ and $u_2$ are chosen independently and uniformly at random, determine the Shannon capacities of the synthetic channels, $I(W^-)$ and $I(W^+)$. Your final answer should be a pair of analytic expressions in terms of the erasure probability $\\epsilon$.", "solution": "Let $W$ be a binary erasure channel with input $X \\in \\{0,1\\}$, output $Y \\in \\{0,1,e\\}$, and erasure probability $\\epsilon$, so $W(y|x)$ equals $1-\\epsilon$ if $y=x$, equals $\\epsilon$ if $y=e$, and equals $0$ otherwise. Two independent uses of $W$ are applied to inputs $X_{1}$ and $X_{2}$, producing outputs $Y_{1}$ and $Y_{2}$, with independence given by the memoryless property:\n$$\nP_{Y_{1},Y_{2}|X_{1},X_{2}}(y_{1},y_{2}|x_{1},x_{2})=W(y_{1}|x_{1})\\,W(y_{2}|x_{2}).\n$$\nThe Arıkan transform maps independent uniform bits $U_{1},U_{2}$ to $X_{1}=U_{1}\\oplus U_{2}$ and $X_{2}=U_{2}$.\n\nBy definition of the synthetic channels,\n$$\nI(W^{-})=I(U_{1};Y_{1},Y_{2}),\\qquad I(W^{+})=I(U_{2};Y_{1},Y_{2},U_{1}),\n$$\nwith $U_{1},U_{2}$ independent and uniformly distributed. For the BEC, it is convenient to characterize these synthetic channels via their effective erasure events and then use that a BEC with erasure probability $\\delta$ has capacity $1-\\delta$ under uniform input.\n\nFirst, consider $W^{-}$, which conveys $U_{1}$ through the effective output $(Y_{1},Y_{2})$ without knowledge of $U_{2}$ at the decoder. The decoder can determine $U_{1}$ if and only if both $X_{1}$ and $X_{2}$ are known from the outputs, because\n$$\nU_{1}=X_{1}\\oplus X_{2}.\n$$\nThe symbol $X_{i}$ is known exactly if and only if $Y_{i}\\in\\{0,1\\}$ (i.e., not erased). Therefore, $U_{1}$ is recoverable if and only if the event $\\{Y_{1}\\neq e\\}\\cap\\{Y_{2}\\neq e\\}$ occurs. Since each use of the channel erases independently with probability $\\epsilon$,\n$$\nP(Y_{1}\\neq e,\\,Y_{2}\\neq e)=(1-\\epsilon)^{2}.\n$$\nHence the effective erasure probability of $W^{-}$ is\n$$\n\\epsilon^{-}=1-(1-\\epsilon)^{2}=2\\epsilon-\\epsilon^{2},\n$$\nso its capacity is\n$$\nI(W^{-})=1-\\epsilon^{-}=1-(2\\epsilon-\\epsilon^{2})=(1-\\epsilon)^{2}.\n$$\n\nNext, consider $W^{+}$, which conveys $U_{2}$ through $(Y_{1},Y_{2})$ with $U_{1}$ known at the decoder. There are two disjoint ways to recover $U_{2}$:\n1) If $Y_{2}\\neq e$, then $X_{2}=Y_{2}$ is known and thus $U_{2}=X_{2}$ is known.\n2) If $Y_{2}=e$ but $Y_{1}\\neq e$, then $X_{1}=Y_{1}$ is known and, using the side information $U_{1}$, one computes $U_{2}=U_{1}\\oplus X_{1}$.\nThe only failure (erasure) occurs when both outputs are erased, i.e., $Y_{1}=e$ and $Y_{2}=e$. By independence,\n$$\nP(Y_{1}=e,\\,Y_{2}=e)=\\epsilon^{2}.\n$$\nTherefore the effective erasure probability of $W^{+}$ is\n$$\n\\epsilon^{+}=\\epsilon^{2},\n$$\nand its capacity is\n$$\nI(W^{+})=1-\\epsilon^{+}=1-\\epsilon^{2}.\n$$\n\nAs a consistency check, the chain rule and memoryless structure give\n$$\nI(W^{-})+I(W^{+})=I(U_{1};Y_{1},Y_{2})+I(U_{2};Y_{1},Y_{2}|U_{1})=I(U_{1},U_{2};Y_{1},Y_{2}),\n$$\nwhile\n$$\nI(U_{1},U_{2};Y_{1},Y_{2})=I(X_{1},X_{2};Y_{1},Y_{2})=I(X_{1};Y_{1})+I(X_{2};Y_{2})=2(1-\\epsilon),\n$$\nand indeed\n$$\n(1-\\epsilon)^{2}+(1-\\epsilon^{2})=2(1-\\epsilon).\n$$\nThus the capacities are\n$$\nI(W^{-})=(1-\\epsilon)^{2},\\qquad I(W^{+})=1-\\epsilon^{2}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}(1-\\epsilon)^{2} & 1-\\epsilon^{2}\\end{pmatrix}}$$", "id": "1661168"}, {"introduction": "Moving from the abstract principle of polarization to a concrete application, this practice problem [@problem_id:1646943] examines the smallest possible polar code of length $N=2$ over a Binary Symmetric Channel (BSC). You will trace the entire process from encoding with a frozen bit to the sequential decoding steps at the receiver. Calculating the exact probability of a decoding error will solidify your understanding of how an SC decoder makes its maximum-likelihood decisions based on received information.", "problem": "A simple rate-1/2 polar code of length $N=2$ is designed to transmit a single information bit over a Binary Symmetric Channel (BSC). The BSC is a memoryless channel that flips each transmitted bit with a crossover probability $p$. The encoding for this polar code is defined by the transformation $\\mathbf{x} = \\mathbf{u} G_2$, where $\\mathbf{u} = (u_1, u_2)$ are the source bits, $\\mathbf{x} = (x_1, x_2)$ are the channel input bits, and the generator matrix is\n$$G_2 = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$$.\nAll additions are performed modulo 2 (i.e., the XOR operation).\n\nFor this coding scheme, the bit-channel synthesized for $u_1$ is deemed less reliable and is \"frozen\" by fixing its value to $u_1 = 0$. The information bit is assigned to $u_2$, which corresponds to a more reliable synthesized channel.\n\nSuppose we want to transmit the information bit $u_2 = 1$. The resulting codeword is sent over a BSC with a crossover probability of $p=0.1$. At the receiver, a successive cancellation decoder operates. It first sets the estimate $\\hat{u}_1$ to the known frozen value, 0. Then, it uses the channel outputs $(y_1, y_2)$ and the value of $\\hat{u}_1$ to produce an estimate $\\hat{u}_2$ by applying a maximum-likelihood decision rule.\n\nCalculate the exact probability that the decoder makes an error in estimating the information bit, i.e., find $P(\\hat{u}_2 \\neq u_2)$. Express your final answer as a numerical value rounded to four significant figures.", "solution": "The problem asks for the probability of error in decoding the information bit $u_2$ for a length-2 polar code. We will solve this by following the encoding and decoding steps precisely.\n\nFirst, we determine the transmitted codeword. The source bits are given by the frozen bit $u_1 = 0$ and the information bit $u_2 = 1$, so $\\mathbf{u} = (0, 1)$. The encoding is given by $\\mathbf{x} = \\mathbf{u} G_2$:\n$$ \\mathbf{x} = (x_1, x_2) = (u_1, u_2) \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} = (u_1 \\oplus u_2, u_2) $$\nwhere $\\oplus$ denotes addition modulo 2. Substituting the values of $u_1$ and $u_2$:\n$$ x_1 = 0 \\oplus 1 = 1 $$\n$$ x_2 = 1 $$\nSo, the transmitted codeword is $\\mathbf{x} = (1, 1)$. This codeword is sent over a BSC with crossover probability $p=0.1$. The received vector is $\\mathbf{y} = (y_1, y_2)$.\n\nNext, we analyze the successive cancellation (SC) decoder. The decoder first estimates $u_1$. Since $u_1$ is a frozen bit with a known value of 0, the decoder sets its estimate $\\hat{u}_1 = 0$. There is no uncertainty or chance of error in this step.\n\nThen, the decoder estimates $u_2$ using the received vector $\\mathbf{y} = (y_1, y_2)$ and its knowledge that $u_1=0$ (or more formally, its estimate $\\hat{u}_1=0$). The decoder makes a maximum-likelihood decision. It compares the probability of receiving $\\mathbf{y}$ if $u_2=0$ versus if $u_2=1$, given that $u_1=0$.\nThe decoder decides $\\hat{u}_2=0$ if $P(y_1, y_2|u_1=0, u_2=0) \\ge P(y_1, y_2|u_1=0, u_2=1)$.\nThe decoder decides $\\hat{u}_2=1$ if $P(y_1, y_2|u_1=0, u_2=0) < P(y_1, y_2|u_1=0, u_2=1)$.\n\nSince the channel is memoryless, the joint probability is the product of individual probabilities:\n$$ P(y_1, y_2|u_1, u_2) = P(y_1|x_1) P(y_2|x_2) = P(y_1|u_1 \\oplus u_2) P(y_2|u_2) $$\nLet's evaluate the two probabilities for the decoder's comparison, given $u_1=0$:\nIf $u_2=0$: $x_1=0\\oplus0=0, x_2=0$. The probability is $P(y_1|x_1=0)P(y_2|x_2=0)$.\nIf $u_2=1$: $x_1=0\\oplus1=1, x_2=1$. The probability is $P(y_1|x_1=1)P(y_2|x_2=1)$.\n\nThe decision rule can be expressed using a likelihood ratio (LR). Let $L(y) = \\frac{P(y|x=0)}{P(y|x=1)}$. The decoder decides $\\hat{u}_2=0$ if the LR is greater than or equal to 1:\n$$ \\frac{P(y_1, y_2|u_1=0, u_2=0)}{P(y_1, y_2|u_1=0, u_2=1)} = \\frac{P(y_1|x_1=0)P(y_2|x_2=0)}{P(y_1|x_1=1)P(y_2|x_2=1)} = L(y_1) L(y_2) \\ge 1 $$\nAn error occurs if the decoder decides $\\hat{u}_2=0$ when we actually transmitted $u_2=1$. This happens when $L(y_1) L(y_2) \\ge 1$.\n\nFor a BSC with crossover probability $p$, we have $P(y=b|x=a) = 1-p$ if $a=b$ and $p$ if $a \\neq b$. The likelihoods are:\n$L(y=0) = \\frac{P(y=0|x=0)}{P(y=0|x=1)} = \\frac{1-p}{p}$\n$L(y=1) = \\frac{P(y=1|x=0)}{P(y=1|x=1)} = \\frac{p}{1-p}$\nGiven $p=0.1$, we have $1-p=0.9$. So, $L(0) = \\frac{0.9}{0.1} = 9$ and $L(1) = \\frac{0.1}{0.9} = \\frac{1}{9}$.\n\nNow we check the condition $L(y_1)L(y_2) \\ge 1$ for the four possible received vectors $\\mathbf{y}=(y_1, y_2)$:\n1. $\\mathbf{y}=(0,0)$: $L(0)L(0) = 9 \\times 9 = 81 \\ge 1$. This leads to an error ($\\hat{u}_2=0$).\n2. $\\mathbf{y}=(0,1)$: $L(0)L(1) = 9 \\times \\frac{1}{9} = 1 \\ge 1$. This leads to an error ($\\hat{u}_2=0$).\n3. $\\mathbf{y}=(1,0)$: $L(1)L(0) = \\frac{1}{9} \\times 9 = 1 \\ge 1$. This leads to an error ($\\hat{u}_2=0$).\n4. $\\mathbf{y}=(1,1)$: $L(1)L(1) = \\frac{1}{9} \\times \\frac{1}{9} = \\frac{1}{81} < 1$. This leads to a correct decision ($\\hat{u}_2=1$).\n\nAn error occurs if the received vector is $(0,0)$, $(0,1)$, or $(1,0)$. The total probability of error is the sum of the probabilities of these three events, given that the transmitted codeword was $\\mathbf{x}=(1,1)$.\nThe BSC acts on each bit independently.\n$$P(\\text{error}) = P(\\mathbf{y}=(0,0)|\\mathbf{x}=(1,1)) + P(\\mathbf{y}=(0,1)|\\mathbf{x}=(1,1)) + P(\\mathbf{y}=(1,0)|\\mathbf{x}=(1,1))$$\n\n1. $P(\\mathbf{y}=(0,0)|\\mathbf{x}=(1,1)) = P(y_1=0|x_1=1) P(y_2=0|x_2=1) = p \\cdot p = p^2$. (Both bits flipped).\n2. $P(\\mathbf{y}=(0,1)|\\mathbf{x}=(1,1)) = P(y_1=0|x_1=1) P(y_2=1|x_2=1) = p \\cdot (1-p)$. (First bit flipped).\n3. $P(\\mathbf{y}=(1,0)|\\mathbf{x}=(1,1)) = P(y_1=1|x_1=1) P(y_2=0|x_2=1) = (1-p) \\cdot p$. (Second bit flipped).\n\nThe total probability of error is the sum:\n$$P(\\text{error}) = p^2 + p(1-p) + (1-p)p = p^2 + p - p^2 + p - p^2 = 2p - p^2$$\n\nNow, we substitute the given value $p=0.1$:\n$$P(\\text{error}) = 2(0.1) - (0.1)^2 = 0.2 - 0.01 = 0.19$$\n\nThe problem asks for the answer rounded to four significant figures.\n$0.1900$.", "answer": "$$\\boxed{0.1900}$$", "id": "1646943"}, {"introduction": "The \"successive\" nature of SC decoding creates a structured dependency where the estimation of each bit relies on the decisions made for the preceding bits. This exercise [@problem_id:1661150] extends our analysis to a slightly larger polar code of length $N=4$ to explore this critical dependency structure. By identifying which previously estimated bits are required to decode the final information bit, you will gain insight into the flow of information and the recursive nature of the SC decoding algorithm as the code length grows.", "problem": "Consider a polar code of block length $N=4$. The vector of information bits is $u = (u_1, u_2, u_3, u_4)$ and the vector of encoded bits (the codeword) is $x = (x_1, x_2, x_3, x_4)$. The encoding process is defined by the following set of equations, where $\\oplus$ denotes addition modulo 2:\n$$x_1 = u_1 \\oplus u_2 \\oplus u_3 \\oplus u_4$$\n$$x_2 = u_3 \\oplus u_4$$\n$$x_3 = u_2 \\oplus u_4$$\n$$x_4 = u_4$$\n\nThis code is decoded using a Successive Cancellation (SC) decoder. The SC decoder receives a noisy version of the codeword $x$ and estimates the information bits sequentially in the order $\\hat{u}_1, \\hat{u}_2, \\hat{u}_3, \\hat{u}_4$. The calculation of the log-likelihood ratio for any bit $u_i$ is dependent on the received channel values and the hard decisions made for all preceding information bits, $(\\hat{u}_1, \\ldots, \\hat{u}_{i-1})$.\n\nThe dependencies in the decoding process are governed by the code's factor graph structure. For the final decoding step, which is to determine the estimate $\\hat{u}_4$, which of the previously estimated bits $(\\hat{u}_1, \\hat{u}_2, \\hat{u}_3)$ are fundamentally required as inputs for the calculation of the log-likelihood ratio of $u_4$?\n\nA. Only $\\hat{u}_3$\n\nB. Only $\\hat{u}_1$ and $\\hat{u}_3$\n\nC. Only $\\hat{u}_2$ and $\\hat{u}_3$\n\nD. All of $\\hat{u}_1$, $\\hat{u}_2$, and $\\hat{u}_3$\n\nE. None of the previous bits are required", "solution": "The encoding is linear over modulo-2 addition: for $u=(u_{1},u_{2},u_{3},u_{4})$, the codeword $x=(x_{1},x_{2},x_{3},x_{4})$ is\n$$\nx_{1}=u_{1}\\oplus u_{2}\\oplus u_{3}\\oplus u_{4},\\quad\nx_{2}=u_{3}\\oplus u_{4},\\quad\nx_{3}=u_{2}\\oplus u_{4},\\quad\nx_{4}=u_{4}.\n$$\nUnder SC decoding, the log-likelihood ratio (LLR) for $u_{4}$ is computed conditioned on the previously decided bits $(\\hat{u}_{1},\\hat{u}_{2},\\hat{u}_{3})$:\n$$\nL(u_{4})=\\ln\\frac{P(y\\mid u_{4}=0,\\hat{u}_{1},\\hat{u}_{2},\\hat{u}_{3})}{P(y\\mid u_{4}=1,\\hat{u}_{1},\\hat{u}_{2},\\hat{u}_{3})}.\n$$\nBecause the channel is memoryless,\n$$\nP(y\\mid u_{4},\\hat{u}_{1},\\hat{u}_{2},\\hat{u}_{3})=\\prod_{i=1}^{4}W\\!\\left(y_{i}\\mid x_{i}(u_{4},\\hat{u}_{1},\\hat{u}_{2},\\hat{u}_{3})\\right),\n$$\nwhere $W(y_{i}\\mid x_{i})$ is the single-use channel law, and $x_{i}$ are the encoded bits as functions of $u_{4}$ and the fixed $(\\hat{u}_{1},\\hat{u}_{2},\\hat{u}_{3})$:\n$$\nx_{1}=\\hat{u}_{1}\\oplus\\hat{u}_{2}\\oplus\\hat{u}_{3}\\oplus u_{4},\\quad\nx_{2}=\\hat{u}_{3}\\oplus u_{4},\\quad\nx_{3}=\\hat{u}_{2}\\oplus u_{4},\\quad\nx_{4}=u_{4}.\n$$\nDefine the channel LLRs $L_{i}\\triangleq \\ln\\frac{W(y_{i}\\mid 0)}{W(y_{i}\\mid 1)}$. For any binary-input channel and any $c\\in\\{0,1\\}$,\n$$\n\\ln\\frac{W(y\\mid c)}{W(y\\mid \\bar{c})}=(-1)^{c}L,\\quad L=\\ln\\frac{W(y\\mid 0)}{W(y\\mid 1)}.\n$$\nSince flipping $u_{4}$ flips all $x_{i}$, and the offsets $c_{i}$ that determine signs are\n$$\nc_{1}=\\hat{u}_{1}\\oplus\\hat{u}_{2}\\oplus\\hat{u}_{3},\\quad c_{2}=\\hat{u}_{3},\\quad c_{3}=\\hat{u}_{2},\\quad c_{4}=0,\n$$\nthe LLR for $u_{4}$ is\n$$\nL(u_{4})=(-1)^{\\hat{u}_{1}\\oplus\\hat{u}_{2}\\oplus\\hat{u}_{3}}L_{1}+(-1)^{\\hat{u}_{3}}L_{2}+(-1)^{\\hat{u}_{2}}L_{3}+L_{4}.\n$$\nEquivalently, in the SC message-passing form using the $f$ and $g$ functions with\n$$\nf(L_{a},L_{b})=2\\,\\arctanh\\!\\big(\\tanh(L_{a}/2)\\tanh(L_{b}/2)\\big),\\quad g(L_{a},L_{b},s)=(-1)^{s}L_{a}+L_{b},\n$$\nthe messages entering the right subtree are\n$$\nL_{c3}=g(L_{1},L_{2},\\hat{u}_{1}\\oplus\\hat{u}_{2}),\\quad L_{c4}=g(L_{3},L_{4},\\hat{u}_{2}),\n$$\nthen\n$$\nL(u_{4})=g(L_{c3},L_{c4},\\hat{u}_{3})=(-1)^{\\hat{u}_{3}}L_{c3}+L_{c4}.\n$$\nBoth expressions show that $L(u_{4})$ fundamentally requires $\\hat{u}_{1}$, $\\hat{u}_{2}$, and $\\hat{u}_{3}$ (through the combinations $\\hat{u}_{1}\\oplus\\hat{u}_{2}$, $\\hat{u}_{2}$, and $\\hat{u}_{3}$). Therefore, all previously estimated bits are required for the LLR calculation of $u_{4}$.", "answer": "$$\\boxed{D}$$", "id": "1661150"}]}