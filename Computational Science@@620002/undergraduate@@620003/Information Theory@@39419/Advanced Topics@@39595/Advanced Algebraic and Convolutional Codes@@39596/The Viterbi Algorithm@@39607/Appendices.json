{"hands_on_practices": [{"introduction": "The Viterbi algorithm is fundamentally a dynamic programming approach that builds a solution step-by-step. This first practice exercise is designed to give you a concrete feel for the core mechanics of the algorithm's \"forward pass.\" By manually calculating the path probability metric, $\\delta_t(j)$, and the backpointer, $\\psi_t(j)$, for each state in a trellis, you will build a foundational understanding of how the algorithm efficiently prunes less likely paths at each time step [@problem_id:1664342].", "problem": "A system monitoring atmospheric particles is modeled as a Hidden Markov Model (HMM). The system can be in one of three hidden states, $S_1, S_2, S_3$, corresponding to low, medium, and high pollution levels, respectively. At each time step, the system transitions to a new state (or stays in the same state) and emits an observation, which can be either \"Clear\" (C) or \"Hazy\" (H).\n\nThe parameters of the HMM are as follows:\n\n1.  **Initial State Probabilities** ($\\pi$): The probability distribution of the state at time $t=1$.\n    $$ \\pi = P(q_1=S_i) = \\begin{pmatrix} 0.7 & 0.2 & 0.1 \\end{pmatrix} $$\n    where $\\pi_i$ is the probability of starting in state $S_i$.\n\n2.  **State Transition Probabilities** ($A$): The probability of transitioning from state $S_i$ to state $S_j$.\n    $$ A = \\{a_{ij}\\} = P(q_t=S_j | q_{t-1}=S_i) = \\begin{pmatrix} 0.6 & 0.3 & 0.1 \\\\ 0.2 & 0.5 & 0.3 \\\\ 0.1 & 0.2 & 0.7 \\end{pmatrix} $$\n\n3.  **Observation Emission Probabilities** ($B$): The probability of observing a particular emission given the current state.\n    $$ b_j(O_k) = P(O_k | q_t=S_j) $$\n    The values are given by:\n    -   In state $S_1$: $P(\\text{C} | S_1) = 0.8$, $P(\\text{H} | S_1) = 0.2$\n    -   In state $S_2$: $P(\\text{C} | S_2) = 0.5$, $P(\\text{H} | S_2) = 0.5$\n    -   In state $S_3$: $P(\\text{C} | S_3) = 0.1$, $P(\\text{H} | S_3) = 0.9$\n\nA sequence of three observations is recorded: $O = (\\text{C}, \\text{H}, \\text{H})$.\n\nTo find the most likely sequence of hidden states that produced these observations, one computes the Viterbi path variables. Let $\\delta_t(j)$ be the maximum probability of any single path ending in state $S_j$ at time $t$, given the first $t$ observations. Let $\\psi_t(j)$ be the state at time $t-1$ that is part of this most likely path.\n\nYour task is to calculate the values for $\\delta_3(j)$ and $\\psi_3(j)$ for all states $j \\in \\{1, 2, 3\\}$. The states are indexed by 1, 2, and 3. The backpointers $\\psi_t(j)$ should also be represented by these state indices.\n\nPresent your answer as a single row matrix with six entries in the format $(\\delta_3(1), \\delta_3(2), \\delta_3(3), \\psi_3(1), \\psi_3(2), \\psi_3(3))$. Round the values of $\\delta_3(j)$ to four significant figures.", "solution": "We use the Viterbi recursion. For observation sequence $O=(\\text{C},\\text{H},\\text{H})$, define $\\delta_{t}(j)=\\max_{q_{1:t-1}}P(q_{1:t-1},q_{t}=S_{j},O_{1:t})$ and $\\psi_{t}(j)=\\arg\\max_{i}\\{\\delta_{t-1}(i)a_{ij}\\}$. The recursions are:\n$$\n\\delta_{1}(j)=\\pi_{j}b_{j}(O_{1}),\\quad \\psi_{1}(j)\\text{ undefined},\n$$\n$$\n\\delta_{t}(j)=\\left[\\max_{i}\\delta_{t-1}(i)a_{ij}\\right]b_{j}(O_{t}),\\quad \\psi_{t}(j)=\\arg\\max_{i}\\{\\delta_{t-1}(i)a_{ij}\\}.\n$$\n\nInitialization at $t=1$ with $O_{1}=\\text{C}$:\n$$\n\\delta_{1}(1)=0.7\\cdot 0.8=0.56,\\quad \\delta_{1}(2)=0.2\\cdot 0.5=0.1,\\quad \\delta_{1}(3)=0.1\\cdot 0.1=0.01.\n$$\n\nInduction to $t=2$ with $O_{2}=\\text{H}$:\n- For $j=1$:\n$$\n\\max_{i}\\{\\delta_{1}(i)a_{i1}\\}=\\max\\{0.56\\cdot 0.6,\\,0.1\\cdot 0.2,\\,0.01\\cdot 0.1\\}=0.336,\n$$\n$$\n\\delta_{2}(1)=0.336\\cdot 0.2=0.0672,\\quad \\psi_{2}(1)=1.\n$$\n- For $j=2$:\n$$\n\\max_{i}\\{\\delta_{1}(i)a_{i2}\\}=\\max\\{0.56\\cdot 0.3,\\,0.1\\cdot 0.5,\\,0.01\\cdot 0.2\\}=0.168,\n$$\n$$\n\\delta_{2}(2)=0.168\\cdot 0.5=0.084,\\quad \\psi_{2}(2)=1.\n$$\n- For $j=3$:\n$$\n\\max_{i}\\{\\delta_{1}(i)a_{i3}\\}=\\max\\{0.56\\cdot 0.1,\\,0.1\\cdot 0.3,\\,0.01\\cdot 0.7\\}=0.056,\n$$\n$$\n\\delta_{2}(3)=0.056\\cdot 0.9=0.0504,\\quad \\psi_{2}(3)=1.\n$$\n\nInduction to $t=3$ with $O_{3}=\\text{H}$:\n- For $j=1$:\n$$\n\\max_{i}\\{\\delta_{2}(i)a_{i1}\\}=\\max\\{0.0672\\cdot 0.6,\\,0.084\\cdot 0.2,\\,0.0504\\cdot 0.1\\}=0.04032,\n$$\n$$\n\\delta_{3}(1)=0.04032\\cdot 0.2=0.008064,\\quad \\psi_{3}(1)=1.\n$$\n- For $j=2$:\n$$\n\\max_{i}\\{\\delta_{2}(i)a_{i2}\\}=\\max\\{0.0672\\cdot 0.3,\\,0.084\\cdot 0.5,\\,0.0504\\cdot 0.2\\}=0.042,\n$$\n$$\n\\delta_{3}(2)=0.042\\cdot 0.5=0.021,\\quad \\psi_{3}(2)=2.\n$$\n- For $j=3$:\n$$\n\\max_{i}\\{\\delta_{2}(i)a_{i3}\\}=\\max\\{0.0672\\cdot 0.1,\\,0.084\\cdot 0.3,\\,0.0504\\cdot 0.7\\}=0.03528,\n$$\n$$\n\\delta_{3}(3)=0.03528\\cdot 0.9=0.031752,\\quad \\psi_{3}(3)=3.\n$$\n\nRounding $\\delta_{3}(j)$ to four significant figures gives:\n$$\n\\delta_{3}(1)=0.008064,\\quad \\delta_{3}(2)=0.02100,\\quad \\delta_{3}(3)=0.03175.\n$$\n\nTherefore, the requested row matrix is $(\\delta_{3}(1),\\delta_{3}(2),\\delta_{3}(3),\\psi_{3}(1),\\psi_{3}(2),\\psi_{3}(3))$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.008064 & 0.02100 & 0.03175 & 1 & 2 & 3 \\end{pmatrix}}$$", "id": "1664342"}, {"introduction": "Once the forward pass of the Viterbi algorithm has populated the trellis with path probabilities and backpointers, the final step is to find the single most likely sequence. This is accomplished through a simple and elegant \"traceback\" procedure. This exercise isolates that crucial final step, allowing you to practice reconstructing the optimal path by following the chain of backpointers from the most probable final state back to the beginning [@problem_id:863125].", "problem": "In the context of a Hidden Markov Model (HMM), the Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, called the Viterbi path, that results in a sequence of observed events.\n\nConsider an HMM with $K=4$ hidden states, indexed by the set $\\{1, 2, 3, 4\\}$, and an observation sequence of length $T=6$. The forward pass of the Viterbi algorithm has already been completed. This pass computes, among other things, a backpointer matrix, which we will denote here with elements $\\psi_t(j)$ for time steps $t \\in \\{2, ..., T\\}$ and states $j \\in \\{1, ..., K\\}$. The value of $\\psi_t(j)$ is the index of the state at time $t-1$ that is on the most likely path to state $j$ at time $t$.\n\nThe computed backpointer values are given in the table below:\n\n| Current State ($j$) | $\\psi_2(j)$ | $\\psi_3(j)$ | $\\psi_4(j)$ | $\\psi_5(j)$ | $\\psi_6(j)$ |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1 | 1 | 3 | 4 | 1 | 2 |\n| 2 | 1 | 1 | 2 | 3 | 4 |\n| 3 | 2 | 2 | 1 | 3 | 3 |\n| 4 | 3 | 2 | 3 | 4 | 4 |\n\nFurthermore, the algorithm determined that the most probable hidden state at the final time step, $T=6$, is state 3. Let $Q^* = (q^*_1, q^*_2, q^*_3, q^*_4, q^*_5, q^*_6)$ be the most likely sequence of hidden states. We are given $q^*_6 = 3$.\n\nUsing the provided backpointer table and the final state, perform the traceback step of the Viterbi algorithm to determine the index of the most probable hidden state at the initial time step, $t=1$. Derive the value of $q^*_1$.", "solution": "We use the traceback recursion for the Viterbi path:\n$$\nq^*_t = \\psi_{t+1}\\bigl(q^*_{t+1}\\bigr).\n$$\nGiven $q^*_6=3$, compute backwards:\n$$\nq^*_5 = \\psi_6(3) = 3,\n$$\n$$\nq^*_4 = \\psi_5(3) = 3,\n$$\n$$\nq^*_3 = \\psi_4(3) = 1,\n$$\n$$\nq^*_2 = \\psi_3(1) = 3,\n$$\n$$\nq^*_1 = \\psi_2(3) = 2.\n$$\nThus,\n$$\nq^*_1 = 2.\n$$", "answer": "$$\\boxed{2}$$", "id": "863125"}, {"introduction": "A true test of understanding a tool is knowing how to adapt it for novel purposes. While the Viterbi algorithm is designed to find the *most* probable path, what if we need to find the *least* probable one, perhaps to detect an anomaly? This advanced exercise challenges you to think creatively about the algorithm's structure, showing how a simple transformation of the probabilities allows you to use the standard Viterbi machinery to solve this inverse problem [@problem_id:1664317].", "problem": "In the field of network security, a Hidden Markov Model (HMM) is often used to model the \"normal\" behavior of a system, such as a server's sequence of CPU load states. Anomaly detection can then be framed as identifying sequences of operations that are extremely unlikely under this model of normal behavior. Your task is to develop a method for finding the *least probable* sequence of hidden states that could have generated a given observation sequence.\n\nConsider an HMM defined by a set of hidden states $S = \\{S_1, S_2\\}$, a set of observation symbols $O = \\{O_1, O_2, O_3\\}$, and the following parameters:\n\n1.  **Initial State Probabilities ($\\pi$):** The probability of the system starting in a particular state.\n    $P(x_1 = S_1) = 0.6$\n    $P(x_1 = S_2) = 0.4$\n\n2.  **State Transition Probability Matrix ($A$):** The probability of transitioning from state $S_i$ to state $S_j$, denoted $P(x_t = S_j | x_{t-1} = S_i)$.\n    $$\n    A = \\begin{pmatrix} 0.7 & 0.3 \\\\ 0.5 & 0.5 \\end{pmatrix}\n    $$\n    where $A_{ij}$ is the probability of transitioning from state $S_i$ to $S_j$.\n\n3.  **Observation Emission Probability Matrix ($B$):** The probability of observing symbol $O_k$ given the system is in state $S_j$, denoted $P(y_t = O_k | x_t = S_j)$.\n    $$\n    B = \\begin{pmatrix} 0.2 & 0.5 & 0.3 \\\\ 0.6 & 0.1 & 0.3 \\end{pmatrix}\n    $$\n    where $B_{jk}$ is the probability of observing $O_k$ from state $S_j$.\n\nGiven the observation sequence $Y = (O_1, O_3, O_2)$, find the hidden state sequence $X = (x_1, x_2, x_3)$ that minimizes the joint probability $P(X, Y)$. You must use a systematic method based on dynamic programming; brute-force enumeration of all possible paths is not permitted. Express your answer as a sequence of state indices, where $S_1$ corresponds to index 1 and $S_2$ corresponds to index 2.", "solution": "The problem asks for the hidden state sequence $X = (x_1, x_2, x_3)$ that minimizes the joint probability $P(X, Y)$ for a given observation sequence $Y = (y_1, y_2, y_3)$. The standard Viterbi algorithm is designed to find the sequence $X$ that *maximizes* this probability. We can, however, adapt the problem to use the standard Viterbi algorithm by performing a transformation.\n\nThe joint probability of a state sequence $X=(x_1, \\dots, x_T)$ and an observation sequence $Y=(y_1, \\dots, y_T)$ is given by:\n$$P(X, Y) = \\pi_{x_1} B_{x_1}(y_1) \\prod_{t=2}^{T} A_{x_{t-1}, x_t} B_{x_t}(y_t)$$\nwhere $\\pi_{x_1}$ is the initial probability of state $x_1$, $A_{ij}$ is the transition probability from state $i$ to $j$, and $B_j(y_t)$ is the emission probability of observation $y_t$ from state $j$.\n\nWe want to find $\\arg\\min_{X} P(X, Y)$. Since all probabilities are positive, minimizing $P(X, Y)$ is equivalent to maximizing its reciprocal, $1/P(X, Y)$.\n$$ \\frac{1}{P(X, Y)} = \\frac{1}{\\pi_{x_1} B_{x_1}(y_1) \\prod_{t=2}^{T} A_{x_{t-1}, x_t} B_{x_t}(y_t)} = \\left(\\frac{1}{\\pi_{x_1}}\\right) \\left(\\frac{1}{B_{x_1}(y_1)}\\right) \\prod_{t=2}^{T} \\left(\\frac{1}{A_{x_{t-1}, x_t}}\\right) \\left(\\frac{1}{B_{x_t}(y_t)}\\right) $$\nThis expression has the same multiplicative structure as the original probability. We can therefore define a new set of \"pseudo-probabilities\" (or costs) which are the reciprocals of the original probabilities:\n-   Initial costs: $\\pi'_i = 1/\\pi_i$\n-   Transition costs: $A'_{ij} = 1/A_{ij}$\n-   Emission costs: $B'_{j}(k) = 1/B_{j}(k)$\n\nNow, the problem is transformed into finding the state sequence $X$ that *maximizes* the product of these costs. This is a problem that can be solved directly by the Viterbi algorithm using these new cost values.\n\nLet's define the parameters for our \"inverse\" problem:\n-   States: $S_1, S_2$\n-   Observation Sequence: $Y = (O_1, O_3, O_2)$\n-   Initial costs: $\\pi' = [1/0.6, 1/0.4] \\approx [1.667, 2.5]$\n-   Transition costs: $A' = \\begin{pmatrix} 1/0.7 & 1/0.3 \\\\ 1/0.5 & 1/0.5 \\end{pmatrix} \\approx \\begin{pmatrix} 1.429 & 3.333 \\\\ 2.0 & 2.0 \\end{pmatrix}$\n-   Emission costs $B'$ for the given sequence $Y=(O_1, O_3, O_2)$:\n    -   $t=1 (O_1)$: $B'_{S_1}(O_1) = 1/0.2 = 5$, $B'_{S_2}(O_1) = 1/0.6 \\approx 1.667$\n    -   $t=2 (O_3)$: $B'_{S_1}(O_3) = 1/0.3 \\approx 3.333$, $B'_{S_2}(O_3) = 1/0.3 \\approx 3.333$\n    -   $t=3 (O_2)$: $B'_{S_1}(O_2) = 1/0.5 = 2$, $B'_{S_2}(O_2) = 1/0.1 = 10$\n\nWe now apply the Viterbi algorithm. Let $\\delta_t(i)$ be the maximum cost of any path ending in state $S_i$ at time $t$, and let $\\psi_t(i)$ be the back-pointer to the previous state on that path.\n\n**Step 1: Initialization (t=1)**\nThe observation is $y_1 = O_1$.\n$\\delta_1(S_1) = \\pi'_{S_1} \\times B'_{S_1}(O_1) = \\frac{1}{0.6} \\times \\frac{1}{0.2} = \\frac{1}{0.12} \\approx 8.333$\n$\\delta_1(S_2) = \\pi'_{S_2} \\times B'_{S_2}(O_1) = \\frac{1}{0.4} \\times \\frac{1}{0.6} = \\frac{1}{0.24} \\approx 4.167$\n$\\psi_1(S_1) = \\text{null}$, $\\psi_1(S_2) = \\text{null}$\n\n**Step 2: Recursion (t=2)**\nThe observation is $y_2 = O_3$.\nFor state $S_1$:\n$\\delta_2(S_1) = \\max \\left( \\delta_1(S_1) \\times A'_{S_1,S_1}, \\delta_1(S_2) \\times A'_{S_2,S_1} \\right) \\times B'_{S_1}(O_3)$\n$\\delta_2(S_1) = \\max \\left( \\frac{1}{0.12} \\times \\frac{1}{0.7}, \\frac{1}{0.24} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.3} = \\max \\left( \\frac{1}{0.084}, \\frac{1}{0.12} \\right) \\times \\frac{1}{0.3}$\n$\\delta_2(S_1) = \\max(11.905, 8.333) \\times 3.333 = 11.905 \\times 3.333 = \\frac{1}{0.084 \\times 0.3} = \\frac{1}{0.0252} \\approx 39.683$\nThe maximum came from state $S_1$, so $\\psi_2(S_1) = S_1$.\n\nFor state $S_2$:\n$\\delta_2(S_2) = \\max \\left( \\delta_1(S_1) \\times A'_{S_1,S_2}, \\delta_1(S_2) \\times A'_{S_2,S_2} \\right) \\times B'_{S_2}(O_3)$\n$\\delta_2(S_2) = \\max \\left( \\frac{1}{0.12} \\times \\frac{1}{0.3}, \\frac{1}{0.24} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.3} = \\max \\left( \\frac{1}{0.036}, \\frac{1}{0.12} \\right) \\times \\frac{1}{0.3}$\n$\\delta_2(S_2) = \\max(27.778, 8.333) \\times 3.333 = 27.778 \\times 3.333 = \\frac{1}{0.036 \\times 0.3} = \\frac{1}{0.0108} \\approx 92.593$\nThe maximum came from state $S_1$, so $\\psi_2(S_2) = S_1$.\n\n**Step 3: Recursion (t=3)**\nThe observation is $y_3 = O_2$.\nFor state $S_1$:\n$\\delta_3(S_1) = \\max \\left( \\delta_2(S_1) \\times A'_{S_1,S_1}, \\delta_2(S_2) \\times A'_{S_2,S_1} \\right) \\times B'_{S_1}(O_2)$\n$\\delta_3(S_1) = \\max \\left( \\frac{1}{0.0252} \\times \\frac{1}{0.7}, \\frac{1}{0.0108} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.5} = \\max \\left( \\frac{1}{0.01764}, \\frac{1}{0.0054} \\right) \\times 2$\n$\\delta_3(S_1) = \\max(56.69, 185.185) \\times 2 = 185.185 \\times 2 = \\frac{1}{0.0054 \\times 0.5} = \\frac{1}{0.0027} \\approx 370.37$\nThe maximum came from state $S_2$, so $\\psi_3(S_1) = S_2$.\n\nFor state $S_2$:\n$\\delta_3(S_2) = \\max \\left( \\delta_2(S_1) \\times A'_{S_1,S_2}, \\delta_2(S_2) \\times A'_{S_2,S_2} \\right) \\times B'_{S_2}(O_2)$\n$\\delta_3(S_2) = \\max \\left( \\frac{1}{0.0252} \\times \\frac{1}{0.3}, \\frac{1}{0.0108} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.1} = \\max \\left( \\frac{1}{0.00756}, \\frac{1}{0.0054} \\right) \\times 10$\n$\\delta_3(S_2) = \\max(132.275, 185.185) \\times 10 = 185.185 \\times 10 = \\frac{1}{0.0054 \\times 0.1} = \\frac{1}{0.00054} \\approx 1851.85$\nThe maximum came from state $S_2$, so $\\psi_3(S_2) = S_2$.\n\n**Step 4: Termination and Backtracking**\nThe highest final cost is $\\delta_3(S_2) \\approx 1851.85$.\nThus, the final state of the least likely path is $x_3 = S_2$.\n\nWe now backtrack to find the complete path:\n-   $x_3 = S_2$\n-   $x_2 = \\psi_3(x_3) = \\psi_3(S_2) = S_2$\n-   $x_1 = \\psi_2(x_2) = \\psi_2(S_2) = S_1$\n\nThe least likely state sequence is $(S_1, S_2, S_2)$.\nExpressed in terms of state indices (where $S_1=1, S_2=2$), the sequence is $(1, 2, 2)$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 2 & 2 \\end{pmatrix}}$$", "id": "1664317"}]}