{"hands_on_practices": [{"introduction": "Before we dive into encoding strategies, it's crucial to establish a baseline. This practice explores the fundamental limit of reconstruction when the encoder sends no information at all (a zero-rate code), relying solely on the correlated side information available at the decoder. By calculating the minimum achievable distortion, you will connect the Wyner-Ziv framework to the principles of Bayesian estimation and understand the starting point of the rate-distortion curve [@problem_id:1668798].", "problem": "A binary data source produces a random variable $X \\in \\{0, 1\\}$ with a known probability distribution $P(X=1) = p_X$. This source is to be encoded for transmission, but the decoder will have access to correlated side information, $Y$. The side information is generated by a noisy version of the source, specifically by passing $X$ through a Binary Symmetric Channel (BSC) with a crossover probability of $\\epsilon$. The decoder receives no information from the encoder (a zero-rate coding scenario) and must produce an estimate $\\hat{X}$ of the source symbol $X$ based solely on the observed side information $Y$.\n\nThe performance of the estimator is measured by the average probability of error, which corresponds to the average Hamming distortion $E[d(X, \\hat{X})]$, where $d(x, \\hat{x})=1$ if $x \\ne \\hat{x}$ and $d(x, \\hat{x})=0$ if $x = \\hat{x}$. An optimal decoder will implement an estimation rule $\\hat{x}(y)$ that minimizes this average error probability. This minimum achievable error is denoted as $D_{\\min}$.\n\nGiven the source probability $p_X = 0.800$ and the channel's crossover probability $\\epsilon = 0.300$, calculate the value of $D_{\\min}$. Express your answer as a decimal rounded to three significant figures.", "solution": "The problem asks for the minimum average error probability, $D_{\\min}$, when estimating a binary source $X$ using only the correlated side information $Y$. This scenario corresponds to Bayesian estimation, where the goal is to find an estimator $\\hat{x}(y)$ that minimizes the average distortion. For Hamming distortion, the average distortion is the probability of error, $D = P(X \\ne \\hat{X})$.\n\nAn optimal estimator, which minimizes the probability of error, is the Maximum A Posteriori (MAP) estimator. This estimator chooses the symbol $x$ that has the highest posterior probability given the observation $y$:\n$$ \\hat{x}(y) = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nThe total minimum error probability, $D_{\\min}$, is the sum of the error probabilities for each possible observation $y$, weighted by the probability of that observation occurring:\n$$ D_{\\min} = P(Y=0)P(\\text{error}|Y=0) + P(Y=1)P(\\text{error}|Y=1) $$\nThe probability of making an error, given an observation $y$, is the posterior probability of the symbol that was *not* chosen by the MAP rule. This is the smaller of the two posterior probabilities.\n$$ P(\\text{error}|Y=y) = \\min_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nSubstituting this into the expression for $D_{\\min}$:\n$$ D_{\\min} = P(Y=0) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=0) \\right) + P(Y=1) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=1) \\right) $$\nUsing the identity $P(A,B) = P(A)P(B|A)$, we can simplify the expression by bringing the marginal probability $P(Y=y)$ inside the minimum operator:\n$$ D_{\\min} = \\min_{x \\in \\{0,1\\}} P(X=x, Y=0) + \\min_{x \\in \\{0,1\\}} P(X=x, Y=1) $$\nThis elegant formula allows us to compute $D_{\\min}$ by calculating the four joint probabilities $P(X=x, Y=y)$.\n\nThe problem provides the following parameters:\n- Source probability: $P(X=1) = p_X = 0.800$. This implies $P(X=0) = 1 - p_X = 0.200$.\n- Channel crossover probability: $\\epsilon = 0.300$. This is the probability that a bit is flipped, i.e., $P(Y \\neq X|X)$. The probability of correct transmission is $1-\\epsilon = 0.700$.\n\nNow, we compute the four joint probabilities using the formula $P(X=x, Y=y) = P(Y=y|X=x)P(X=x)$:\n1.  $P(X=0, Y=0) = P(Y=0|X=0)P(X=0) = (1-\\epsilon)(1-p_X) = (0.700)(0.200) = 0.140$.\n2.  $P(X=1, Y=0) = P(Y=0|X=1)P(X=1) = \\epsilon \\cdot p_X = (0.300)(0.800) = 0.240$.\n3.  $P(X=0, Y=1) = P(Y=1|X=0)P(X=0) = \\epsilon(1-p_X) = (0.300)(0.200) = 0.060$.\n4.  $P(X=1, Y=1) = P(Y=1|X=1)P(X=1) = (1-\\epsilon)p_X = (0.700)(0.800) = 0.560$.\n\nWe can substitute these values into our expression for $D_{\\min}$:\n$$ D_{\\min} = \\min(P(X=0, Y=0), P(X=1, Y=0)) + \\min(P(X=0, Y=1), P(X=1, Y=1)) $$\n$$ D_{\\min} = \\min(0.140, 0.240) + \\min(0.060, 0.560) $$\nTaking the minimum from each pair:\n$$ D_{\\min} = 0.140 + 0.060 = 0.200 $$\n\nTo gain more insight, let's examine the explicit MAP decision rule:\n- If $Y=0$ is observed: The decoder compares $P(X=0, Y=0)=0.140$ with $P(X=1, Y=0)=0.240$. Since $0.240 > 0.140$, the decoder concludes that it is more probable that $X=1$ was sent. Thus, $\\hat{x}(0) = 1$.\n- If $Y=1$ is observed: The decoder compares $P(X=0, Y=1)=0.060$ with $P(X=1, Y=1)=0.560$. Since $0.560 > 0.060$, the decoder concludes $\\hat{x}(1) = 1$.\n\nIn both cases, the optimal decision is to guess $\\hat{X}=1$, regardless of the observed side information $Y$. This happens because the source is highly biased towards $X=1$ (a strong prior), and the channel is sufficiently noisy that the evidence from observing $Y=0$ is not strong enough to overcome this prior belief. The optimal strategy is thus to always guess the most likely source symbol.\nThe error probability of this strategy is the probability that the guess is wrong, which is $P(X \\ne 1) = P(X=0) = 1 - p_X = 1 - 0.800 = 0.200$. This confirms the result from our calculation.\n\nThe final answer, rounded to three significant figures, is 0.200.", "answer": "$$\\boxed{0.200}$$", "id": "1668798"}, {"introduction": "Having established the zero-rate limit, we now examine the other extreme: perfect, lossless reconstruction. This exercise requires you to calculate the minimum rate needed to perfectly recover the source data when side information is present at the decoder, a scenario governed by the Slepian-Wolf theorem. This problem solidifies the central role of conditional entropy, $H(X|Y)$, as the theoretical rate for achieving zero distortion in distributed source coding [@problem_id:1668812].", "problem": "In a remote environmental monitoring system, a primary sensor measures the state of a system, denoted by the random variable $X$, which can take one of three values: $X \\in \\{0, 1, 2\\}$. These states are observed to occur with equal probability. A secondary, less reliable sensor provides correlated side information, denoted by the random variable $Y$, which also takes values in $\\{0, 1, 2\\}$. The observations from the secondary sensor are available at a central data hub.\n\nThe correlation between the two sensors is characterized by the following conditional probabilities: when the primary sensor measures state $x$, the probability that the secondary sensor also measures $x$ is $P(Y=x | X=x) = 0.8$. When the primary sensor measures state $x$, the probability that the secondary sensor measures any other specific state $y \\neq x$ is the same for all other states, i.e., $P(Y=y | X=x) = 0.1$ for $y \\neq x$.\n\nThe primary sensor must encode its sequence of measurements and transmit them to the central hub. The secondary sensor's data is not available to the primary sensor's encoder. According to the principles of distributed source coding (specifically, the Slepian-Wolf theorem, which is a special case of Wyner-Ziv theory for lossless compression), what is the theoretical minimum average data rate required for the primary sensor's transmission to allow the central hub to perfectly reconstruct the sequence of states $X$ without any loss?\n\nExpress your answer in bits per measurement, rounded to four significant figures.", "solution": "We are asked for the theoretical minimum average data rate for lossless reconstruction of $X$ at the decoder when correlated side information $Y$ is available only at the decoder. By the Slepian-Wolf theorem, the minimum achievable rate for the encoder observing only $X$ is the conditional entropy\n$$\nR_{\\min}=H(X|Y).\n$$\n\nGiven $X \\in \\{0,1,2\\}$ with $P(X=x)=\\frac{1}{3}$ for all $x$, and a symmetric channel $P(Y=y|X=x)$ specified by\n$$\nP(Y=x|X=x)=0.8,\\quad P(Y=y|X=x)=0.1\\ \\text{for }y\\neq x,\n$$\nwe first compute the marginal of $Y$:\n$$\nP(Y=y)=\\sum_{x}P(Y=y|X=x)P(X=x)=\\frac{1}{3}\\left(0.8+0.1+0.1\\right)=\\frac{1}{3},\n$$\nso $Y$ is also uniform.\n\nUsing Bayesâ€™ rule and the symmetry, for any $y$ we have\n$$\nP(X=y|Y=y)=\\frac{P(Y=y|X=y)P(X=y)}{P(Y=y)}=\\frac{0.8\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.8,\n$$\nand for $x\\neq y$,\n$$\nP(X=x|Y=y)=\\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}=\\frac{0.1\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.1.\n$$\nTherefore, the posterior distribution $P(X|Y=y)$ is the same for every $y$ and equals $\\{0.8,0.1,0.1\\}$. Hence\n$$\nH(X|Y)=\\sum_{y}P(Y=y)\\,H\\big(P(X|Y=y)\\big)=H\\big(0.8,0.1,0.1\\big).\n$$\nEvaluating the entropy in bits,\n$$\nH(X|Y)=-\\left[0.8\\log_{2}(0.8)+0.1\\log_{2}(0.1)+0.1\\log_{2}(0.1)\\right]\n=-0.8\\log_{2}(0.8)-0.2\\log_{2}(0.1).\n$$\nUsing $\\log_{2}(0.8)\\approx -0.3219280949$ and $\\log_{2}(0.1)\\approx -3.3219280949$, we get\n$$\nH(X|Y)\\approx 0.8\\times 0.3219280949+0.2\\times 3.3219280949\\approx 0.9219280949\\ \\text{bits/measurement}.\n$$\nRounding to four significant figures gives $0.9219$ bits per measurement.\n\nTherefore, the theoretical minimum average data rate required is $H(X|Y)\\approx 0.9219$ bits per measurement.", "answer": "$$\\boxed{0.9219}$$", "id": "1668812"}, {"introduction": "After exploring the key endpoints of the rate-distortion function, we can now probe its deeper structural properties. This conceptual problem invites you to leverage symmetry in the source statistics and distortion measure to uncover an elegant and fundamental relationship between encoding a source $X$ given $Y$ versus encoding $Y$ given $X$. This practice will sharpen your theoretical intuition and highlight the beautiful symmetry inherent in the Wyner-Ziv theorem [@problem_id:1668833].", "problem": "Consider a system with a pair of correlated, discrete random variables $(X, Y)$, which are drawn from the same finite alphabet $\\mathcal{A}$ according to a joint probability mass function (pmf) $p(x, y)$. The source is known to be symmetric, meaning that its joint pmf satisfies $p(x, y) = p(y, x)$ for all possible outcomes $x, y \\in \\mathcal{A}$.\n\nWe are interested in the problem of lossy source coding with side information, which is described by the Wyner-Ziv rate-distortion theory. Let $R_{X|Y}(D)$ denote the Wyner-Ziv rate-distortion function. This function gives the minimum rate (in bits per symbol) required to encode the source variable $X$ such that a decoder, which has perfect access to the side information $Y$, can generate a reconstruction $\\hat{X}$ of $X$. This reconstruction must satisfy an average distortion constraint $E[d(X, \\hat{X})] \\le D$, where $D$ is a non-negative distortion level. The function $d(u, v)$ is a given distortion measure for $u, v \\in \\mathcal{A}$.\n\nSimilarly, let $R_{Y|X}(D)$ be the corresponding minimum rate required to encode $Y$, assuming the decoder has perfect access to $X$ as side information, to achieve the same maximum average distortion $D$ for the reconstruction of $Y$.\n\nAssume that the distortion measure $d(u,v)$ is symmetric, i.e., $d(u,v) = d(v,u)$ for all $u,v \\in \\mathcal{A}$. Based on the given information, what is the fundamental relationship between the two rate-distortion functions $R_{X|Y}(D)$ and $R_{Y|X}(D)$ for any allowed distortion $D \\ge 0$?\n\nA. $R_{X|Y}(D) < R_{Y|X}(D)$\n\nB. $R_{X|Y}(D) > R_{Y|X}(D)$\n\nC. $R_{X|Y}(D) = R_{Y|X}(D)$\n\nD. The relationship depends on the specific joint distribution $p(x,y)$ and the distortion measure $d(u,v)$, and cannot be determined in general.", "solution": "The problem asks for the relationship between the Wyner-Ziv rate-distortion functions $R_{X|Y}(D)$ and $R_{Y|X}(D)$ for a source with a symmetric joint probability mass function $p(x, y) = p(y, x)$. We will demonstrate that the two rates are equal by leveraging this symmetry.\n\nThe Wyner-Ziv rate-distortion function $R_{X|Y}(D)$ is defined as the minimum of the conditional mutual information $I(X;U|Y)$ over all auxiliary random variables $U$ that form a Markov chain $U-X-Y$, subject to a distortion constraint. Formally,\n$$R_{X|Y}(D) = \\min_{p(u|x), g_x} I(X;U|Y)$$\nwhere the minimization is over all conditional distributions $p(u|x)$ and reconstruction functions $g_x: \\mathcal{U} \\times \\mathcal{Y} \\to \\hat{\\mathcal{X}}$ such that $E[d(X, g_x(U,Y))] \\le D$. Note that the alphabets for $X$ and $Y$ are the same, $\\mathcal{A}$, and we can assume the reconstruction alphabets $\\hat{\\mathcal{X}}$ and $\\hat{\\mathcal{Y}}$ are also $\\mathcal{A}$. The alphabets for the auxiliary variables $U$ and $V$ can be chosen appropriately.\n\nLet $(U^*, g_x^*)$ be an optimal choice of auxiliary random variable and reconstruction function that achieves the minimum for $R_{X|Y}(D)$. The conditional distribution for $U^*$ is $p_{U^*|X}(u|x)$, and it forms the Markov chain $U^*-X-Y$. The rate is $R_{X|Y}(D) = I(X;U^*|Y)$, and the distortion is $E[d(X, g_x^*(U^*,Y))] \\le D$.\n\nNow, we will construct a candidate solution for the second problem, $R_{Y|X}(D)$, based on this optimal solution. Let's define a new auxiliary random variable $V$ and a reconstruction function $g_y$. We exploit the symmetry of the problem setup.\n1. Define the conditional distribution of $V$ given $Y$ to be the same as that of $U^*$ given $X$: $p_{V|Y}(v|y) := p_{U^*|X}(v|y)$. This ensures that $V-Y-X$ forms a Markov chain as required.\n2. Define the reconstruction function $g_y$ by swapping the roles of the side information: $g_y(v,x) := g_x^*(v,x)$.\n\nFirst, let's verify if this candidate solution $(V, g_y)$ satisfies the distortion constraint for the $Y|X$ problem. The expected distortion is:\n$$E[d(Y, g_y(V,X))] = \\sum_{x,y,v} p(x,y,v) d(y, g_y(v,x))$$\nThe joint pmf is $p(x,y,v) = p(v|y)p(x,y) = p_{U^*|X}(v|y)p(x,y)$. Substituting this and the definition of $g_y$:\n$$E[d(Y, g_y(V,X))] = \\sum_{x,y,v} p_{U^*|X}(v|y)p(x,y) d(y, g_x^*(v,x))$$\nNow, we perform a change of summation variables. Let's swap the dummy variables $x \\leftrightarrow y'$, and $y \\leftrightarrow x'$.\n$$E[d(Y, g_y(V,X))] = \\sum_{x',y',v} p_{U^*|X}(v|x')p(y',x') d(x', g_x^*(v,y'))$$\nUsing the source symmetry $p(y',x') = p(x',y')$, this becomes:\n$$E[d(Y, g_y(V,X))] = \\sum_{x',y',v} p_{U^*|X}(v|x')p(x',y') d(x', g_x^*(v,y'))$$\nThis expression is exactly the expected distortion for the first problem, $E[d(X, g_x^*(U^*,Y))] = \\sum_{x,y,u} p(u|x)p(x,y) d(x, g_x^*(u,y))$, just with differently named summation variables.\nTherefore, $E[d(Y, g_y(V,X))] = E[d(X, g_x^*(U^*,Y))] \\le D$. Our candidate solution is valid as it satisfies the distortion constraint. Note that the symmetry of the distortion measure $d$ was not required for this step.\n\nNext, we calculate the rate $I(Y;V|X)$ for our candidate solution. By definition of the minimum, we have $R_{Y|X}(D) \\le I(Y;V|X)$.\n$$I(Y;V|X) = H(Y|X) - H(Y|X,V)$$\n$$I(X;U^*|Y) = H(X|Y) - H(X|Y,U^*)$$\nDue to the symmetry $p(x,y)=p(y,x)$, the marginal distributions are identical ($p_X(a)=p_Y(a)$ for all $a$), which implies $H(X)=H(Y)$. This also leads to $H(X|Y) = H(Y|X)$.\n\nNow we compare the conditional entropy terms $H(Y|X,V)$ and $H(X|Y,U^*)$.\n$$H(Y|X,V) = - \\sum_{x,y,v} p(x,y,v) \\log p(y|x,v)$$\nwith $p(x,y,v) = p_{V|Y}(v|y)p(x,y) = p_{U^*|X}(v|y)p(x,y)$.\nLet's swap the dummy variables $x \\leftrightarrow y'$ and $y \\leftrightarrow x'$.\n$$H(Y|X,V) = - \\sum_{y',x',v} p_{U^*|X}(v|x')p(y',x') \\log p(x'|y',v)$$\nUsing $p(y',x')=p(x',y')$, this is:\n$$H(Y|X,V) = - \\sum_{x',y',v} p_{U^*|X}(v|x')p(x',y') \\log p(x'|y',v)$$\nThis expression is precisely $H(X|Y,U^*)$ with variables renamed. Thus, $H(Y|X,V) = H(X|Y,U^*)$.\n\nSince $H(X|Y) = H(Y|X)$ and $H(X|Y,U^*) = H(Y|X,V)$, we have:\n$$I(Y;V|X) = I(X;U^*|Y)$$\nOur constructed candidate solution for the $Y|X$ problem achieves a rate of $I(Y;V|X)$, which is equal to $R_{X|Y}(D)$. Since $R_{Y|X}(D)$ is the minimum possible rate, it must be that:\n$$R_{Y|X}(D) \\le I(Y;V|X) = R_{X|Y}(D)$$\n\nThe entire argument is symmetric. We could have started with an optimal solution $(V^{**}, g_y^{**})$ for $R_{Y|X}(D)$ and constructed a valid candidate solution for the $R_{X|Y}(D)$ problem that achieves the same rate. This would show that $R_{X|Y}(D) \\le R_{Y|X}(D)$.\n\nCombining the two inequalities, $R_{Y|X}(D) \\le R_{X|Y}(D)$ and $R_{X|Y}(D) \\le R_{Y|X}(D)$, we are forced to conclude that they must be equal.\n$$R_{X|Y}(D) = R_{Y|X}(D)$$\nTherefore, the relationship between the two rates is one of equality, independent of the specific symmetric distribution or distortion level.", "answer": "$$\\boxed{C}$$", "id": "1668833"}]}