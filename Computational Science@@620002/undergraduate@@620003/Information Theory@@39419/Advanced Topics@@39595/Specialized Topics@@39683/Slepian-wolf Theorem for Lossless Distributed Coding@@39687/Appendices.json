{"hands_on_practices": [{"introduction": "The Slepian-Wolf theorem provides the theoretical limits for compressing correlated data from separate sources. This first practice problem will guide you through the fundamental calculation of the achievable rate region, defined by the inequalities $R_X \\ge H(X|Y)$, $R_Y \\ge H(Y|X)$, and $R_X + R_Y \\ge H(X,Y)$, for a simple, concrete scenario involving two sensors. By working through it, you'll gain hands-on experience in applying the entropy-based formulas that define the boundaries of efficient distributed data compression [@problem_id:1658816].", "problem": "Two environmental sensors, $S_X$ and $S_Y$, are deployed to monitor a system that can be in one of three distinct states. Each state is equally likely to occur. The sensors produce binary outputs, denoted by the random variables $X$ and $Y$, respectively. The joint behavior of the sensors is characterized by their outputs for each system state:\n- In State 1, the sensors output $(X,Y) = (0,0)$.\n- In State 2, the sensors output $(X,Y) = (0,1)$.\n- In State 3, the sensors output $(X,Y) = (1,1)$.\n\nThe outputs $X$ and $Y$ are encoded separately at rates $R_X$ and $R_Y$ (in bits per symbol), respectively. The two compressed bitstreams are then transmitted to a central processing unit. This unit must be able to losslessly reconstruct the original pair $(X,Y)$ from the two streams.\n\nAccording to the Slepian-Wolf theorem for distributed source coding, the set of all achievable rate pairs $(R_X, R_Y)$ that allow for this perfect reconstruction forms a region in the rate plane. Which of the following options correctly describes this achievable rate region? Note that all logarithms are base 2.\n\nA. The region defined by the inequalities $R_X \\ge \\log_2(3) - \\frac{2}{3}$ and $R_Y \\ge \\log_2(3) - \\frac{2}{3}$.\n\nB. The region defined by the inequalities $R_X \\ge \\frac{2}{3}$, $R_Y \\ge \\frac{2}{3}$, and $R_X + R_Y \\ge \\log_2(3)$.\n\nC. The region defined by the inequalities $R_X \\ge 0$, $R_Y \\ge 0$, and $R_X + R_Y \\ge \\log_2(3)$.\n\nD. The region defined by the inequalities $R_X \\ge \\frac{2}{3}$, $R_Y \\ge \\frac{2}{3}$, and $R_X + R_Y \\ge 2\\left(\\log_2(3) - \\frac{2}{3}\\right)$.\n\nE. The region defined by the inequalities $R_X \\ge \\log_2(3)$, $R_Y \\ge \\log_2(3)$, and $R_X + R_Y \\ge \\log_2(3)$.", "solution": "Let the system state be a random variable taking three equally likely values, each mapped deterministically to $(X,Y)$ as follows: $(0,0)$, $(0,1)$, $(1,1)$, each with probability $\\frac{1}{3}$, and $(1,0)$ with probability $0$. The Slepian-Wolf region for lossless distributed compression of $(X,Y)$ is given by the inequalities\n$$\nR_{X} \\geq H(X|Y), \\quad R_{Y} \\geq H(Y|X), \\quad R_{X}+R_{Y} \\geq H(X,Y).\n$$\n\nFirst compute $H(X,Y)$. Since there are three outcomes with probability $\\frac{1}{3}$,\n$$\nH(X,Y) = - \\sum_{(x,y)} p(x,y) \\log_{2} p(x,y) = -3 \\cdot \\frac{1}{3} \\log_{2} \\left(\\frac{1}{3}\\right) = \\log_{2} 3.\n$$\n\nNext compute the marginals. We have $P(X=0)=\\frac{2}{3}$ and $P(X=1)=\\frac{1}{3}$, so\n$$\nH(X) = -\\frac{2}{3}\\log_{2}\\left(\\frac{2}{3}\\right) - \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right)\n= -\\frac{2}{3}\\left(\\log_{2}2 - \\log_{2}3\\right) + \\frac{1}{3}\\log_{2}3\n= \\log_{2}3 - \\frac{2}{3}.\n$$\nSimilarly, $P(Y=0)=\\frac{1}{3}$ and $P(Y=1)=\\frac{2}{3}$, yielding\n$$\nH(Y) = -\\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) - \\frac{2}{3}\\log_{2}\\left(\\frac{2}{3}\\right)\n= \\log_{2}3 - \\frac{2}{3}.\n$$\n\nNow compute the conditional entropies using $H(X|Y)=H(X,Y)-H(Y)$ and $H(Y|X)=H(X,Y)-H(X)$:\n$$\nH(X|Y) = \\log_{2}3 - \\left(\\log_{2}3 - \\frac{2}{3}\\right) = \\frac{2}{3},\n$$\n$$\nH(Y|X) = \\log_{2}3 - \\left(\\log_{2}3 - \\frac{2}{3}\\right) = \\frac{2}{3}.\n$$\n\nTherefore, the Slepian-Wolf achievable region is\n$$\nR_{X} \\geq \\frac{2}{3}, \\quad R_{Y} \\geq \\frac{2}{3}, \\quad R_{X} + R_{Y} \\geq \\log_{2} 3,\n$$\nwhich corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1658816"}, {"introduction": "Let's explore a special but highly instructive case: one data source is a deterministic function of another. This scenario, where the output of one source is perfectly determined by the other ($Y=X^2$), reveals how perfect correlation impacts compression limits by making one of the conditional entropies, $H(Y|X)$, equal to zero [@problem_id:1658801]. This practice will not only test your understanding of the Slepian-Wolf bounds but also challenge you to find an optimal operating point on the boundary of the achievable region.", "problem": "Two correlated information sources, denoted by the random variables $X$ and $Y$, are being monitored. The source $X$ can produce one of four possible values from the set $\\mathcal{X} = \\{-3, -1, 1, 3\\}$, with each value occurring with equal probability. The source $Y$ is perfectly correlated with $X$ through the deterministic relationship $Y = X^2$. A data compression system is designed to encode the outputs of these two sources separately, at rates $R_X$ and $R_Y$ respectively, for transmission to a central station where they will be jointly decoded. The goal is to ensure that the original sequences from both sources can be reconstructed without any loss of information.\n\nAccording to the Slepian-Wolf theorem, there is an achievable region of rate pairs $(R_X, R_Y)$ that allows for such lossless reconstruction. If the encoder for source $Y$ is designed to operate at a fixed rate of $R_Y = 0.5$ bits per symbol, what is the absolute minimum rate $R_X$ that must be used for encoding source $X$? Express your answer in bits per symbol, rounded to two significant figures. All logarithms are to be taken in base 2.", "solution": "Let $X$ be uniform over $\\{-3,-1,1,3\\}$, so $P(X=x)=\\frac{1}{4}$ for each of the four values. The mapping $Y=X^{2}$ yields $Y\\in\\{1,9\\}$ with $P(Y=1)=P(X\\in\\{-1,1\\})=\\frac{1}{2}$ and $P(Y=9)=P(X\\in\\{-3,3\\})=\\frac{1}{2}$.\n\nCompute the relevant entropies (all logarithms are base $2$):\n$$\nH(X)=-\\sum_{x}P(x)\\log_{2}P(x)=-4\\cdot \\frac{1}{4}\\log_{2}\\frac{1}{4}=2,\n$$\n$$\nH(Y)=-\\sum_{y}P(y)\\log_{2}P(y)=-2\\cdot \\frac{1}{2}\\log_{2}\\frac{1}{2}=1.\n$$\nSince $Y$ is a deterministic function of $X$, $H(Y|X)=0$. Given $Y$, $X$ is equally likely to be one of two values (the two signs), hence\n$$\nH(X|Y)=1.\n$$\nThe joint entropy is\n$$\nH(X,Y)=H(X)+H(Y|X)=2+0=2.\n$$\n\nThe Slepian-Wolf achievable region for lossless reconstruction requires\n$$\nR_{X}\\geq H(X|Y),\\quad R_{Y}\\geq H(Y|X),\\quad R_{X}+R_{Y}\\geq H(X,Y).\n$$\nSubstituting the values and the fixed $R_{Y}=0.5$ gives\n$$\nR_{X}\\geq 1,\\quad R_{X}\\geq H(X,Y)-R_{Y}=2-0.5=1.5.\n$$\nTherefore the absolute minimum rate for $X$ is\n$$\nR_{X,\\min}=\\max\\{H(X|Y),\\,H(X,Y)-R_{Y}\\}=\\max\\{1,\\,1.5\\}=1.5,\n$$\nwhich in bits per symbol, rounded to two significant figures, is $1.5$.", "answer": "$$\\boxed{1.5}$$", "id": "1658801"}, {"introduction": "A common misconception is to equate 'uncorrelated' with 'independent'. This exercise delves into this crucial distinction by presenting two sources that are uncorrelated but still statistically dependent [@problem_id:1658814]. You will discover that the benefits of distributed source coding extend beyond linearly correlated data, highlighting that the true measure of dependency for compression is shared information, as captured by conditional entropy, not simply the covariance.", "problem": "Consider two discrete random sources, $X$ and $Y$, which generate sequences of symbols. These sources are known to be correlated. An experiment establishes their joint probability mass function, $p(x,y)$, for pairs of symbols $(X,Y)$ as follows:\n$p(-1, 1) = \\frac{1}{3}$\n$p(0, 0) = \\frac{1}{3}$\n$p(1, 1) = \\frac{1}{3}$\nand $p(x,y) = 0$ for all other pairs of $(x,y)$.\n\nThe output of the sources consists of long sequences $(X_1, X_2, \\dots, X_n)$ and $(Y_1, Y_2, \\dots, Y_n)$, where each pair $(X_i, Y_i)$ is an independent and identically distributed (i.i.d.) draw from the joint distribution $p(x,y)$ described above. It is a known fact for this distribution that the random variables $X$ and $Y$ are uncorrelated, but they are not statistically independent.\n\nTwo separate encoders are used to compress the sequences from source $X$ and source $Y$ at rates $R_X$ and $R_Y$ respectively. A single joint decoder receives both compressed streams and must be able to losslessly reconstruct both original sequences.\n\nSuppose one desires to encode the sequence from source $Y$ at a rate $R_Y$ exactly equal to its standalone Shannon entropy, $H(Y)$. To guarantee that both sequences can be losslessly jointly decoded, what is the absolute minimum achievable code rate $R_X$ for the sequence from source $X$?\n\nExpress your answer as a single closed-form analytic expression in bits per symbol. Logarithms are to be taken in base 2.", "solution": "We use the Slepianâ€“Wolf lossless distributed source coding region for two correlated discrete memoryless sources $(X,Y)$. The achievable rate region is characterized by\n$$\nR_{X} \\geq H(X|Y), \\quad R_{Y} \\geq H(Y|X), \\quad R_{X} + R_{Y} \\geq H(X,Y),\n$$\nwith all entropies in bits (logarithms base 2).\n\nHere $R_{Y}$ is fixed to $H(Y)$. The absolute minimum $R_{X}$ that still satisfies the region is\n$$\nR_{X,\\min}=\\max\\!\\big\\{H(X|Y),\\, H(X,Y)-R_{Y}\\big\\}=\\max\\!\\big\\{H(X|Y),\\, H(X,Y)-H(Y)\\big\\}.\n$$\nSince $H(X,Y)=H(Y)+H(X|Y)$, this reduces to\n$$\nR_{X,\\min}=H(X|Y).\n$$\n\nWe now compute $H(X|Y)$ from the given joint pmf:\n$$\np(-1,1)=\\frac{1}{3},\\quad p(0,0)=\\frac{1}{3},\\quad p(1,1)=\\frac{1}{3},\\quad \\text{else }0.\n$$\nFirst compute the marginal of $Y$:\n$$\nP(Y=1)=\\frac{1}{3}+\\frac{1}{3}=\\frac{2}{3}, \\quad P(Y=0)=\\frac{1}{3}.\n$$\nNext, the conditional distributions of $X$ given $Y$:\n- If $Y=0$, then the only possible pair is $(0,0)$, so $P(X=0\\mid Y=0)=1$ and $H(X\\mid Y=0)=0$.\n- If $Y=1$, then the possible pairs are $(-1,1)$ and $(1,1)$ with equal probability after conditioning, so $P(X=-1\\mid Y=1)=\\frac{1}{2}$ and $P(X=1\\mid Y=1)=\\frac{1}{2}$, hence $H(X\\mid Y=1)=1$.\n\nTherefore,\n$$\nH(X\\mid Y)=P(Y=0)\\,H(X\\mid Y=0)+P(Y=1)\\,H(X\\mid Y=1)\n=\\frac{1}{3}\\cdot 0+\\frac{2}{3}\\cdot 1=\\frac{2}{3}.\n$$\nSince $R_{Y}=H(Y)$ by stipulation, the minimal feasible $R_{X}$ that guarantees lossless joint decoding is $H(X\\mid Y)=\\frac{2}{3}$ bits per symbol.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1658814"}]}