{"hands_on_practices": [{"introduction": "Understanding how entropy varies with the underlying probability distribution is fundamental to grasping the concept of uncertainty. This first exercise provides a hands-on way to explore this relationship by focusing on the collision entropy, $H_2(X)$. By solving for the probabilities that maximize entropy in a simple three-symbol source, you will build intuition for what makes a distribution more or less predictable. [@problem_id:1655420]", "problem": "In information theory, the Rényi entropy of order $\\alpha$ is a generalization of the standard Shannon entropy. For a discrete random variable $X$ that can take $n$ possible values with probabilities $\\{p_1, p_2, \\dots, p_n\\}$, the Rényi entropy is defined as:\n$$H_\\alpha(X) = \\frac{1}{1-\\alpha} \\ln \\left( \\sum_{i=1}^n p_i^\\alpha \\right)$$\nfor any real number $\\alpha \\ge 0$ and $\\alpha \\ne 1$. A particularly useful case is the collision entropy, which corresponds to $\\alpha=2$.\n\nConsider a discrete information source that emits one of three symbols, $\\{s_1, s_2, s_3\\}$. The probabilities of these symbols being emitted are $P(X=s_1) = p$, $P(X=s_2) = p$, and $P(X=s_3) = 1-2p$, where $p$ is a real parameter. To ensure that these probabilities are valid (i.e., non-negative and sum to one), the parameter $p$ is constrained to the interval $[0, 1/2]$.\n\nFind the specific value of $p$ within this interval that maximizes the collision entropy, $H_2(X)$, of the information source.", "solution": "For order two, the Rényi (collision) entropy is obtained from the definition\n$$H_{2}(X)=\\frac{1}{1-2}\\ln\\!\\left(\\sum_{i=1}^{n}p_{i}^{2}\\right)=-\\ln\\!\\left(\\sum_{i=1}^{n}p_{i}^{2}\\right).$$\nWith probabilities $\\{p,p,1-2p\\}$, the sum of squares is\n$$S(p)=p^{2}+p^{2}+(1-2p)^{2}=2p^{2}+1-4p+4p^{2}=6p^{2}-4p+1.$$\nSince $S(p)>0$ on $[0,\\,\\frac{1}{2}]$, maximizing $H_{2}(X)=-\\ln(S(p))$ is equivalent to minimizing $S(p)$ on $[0,\\,\\frac{1}{2}]$ because $-\\ln(\\cdot)$ is strictly decreasing on $(0,\\infty)$. The function $S(p)$ is a convex quadratic with\n$$S'(p)=12p-4,\\qquad S''(p)=12>0.$$\nSetting the first derivative to zero gives the unique critical point\n$$12p-4=0\\;\\;\\Longrightarrow\\;\\;p=\\frac{1}{3},$$\nwhich lies in $[0,\\,\\frac{1}{2}]$. By convexity, this point is the global minimizer of $S(p)$ on the interval, hence it maximizes $H_{2}(X)$.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1655420"}, {"introduction": "The power of information-theoretic concepts like Rényi entropy lies in their wide applicability across different scientific disciplines. This practice problem demonstrates this by asking you to calculate the entropy for a random variable following a Zeta distribution, which has deep connections to number theory and statistical physics. This exercise will sharpen your skills in applying the fundamental definition of Rényi entropy to complex and important probability distributions found in other fields. [@problem_id:132115]", "problem": "The Rényi entropy is a family of information measures that generalizes the standard Shannon entropy. For a discrete random variable $X$ with probability mass function (PMF) $p_k = P(X=k)$, the Rényi entropy of order $\\alpha$ is defined as\n$$\nH_\\alpha(X) = \\frac{1}{1-\\alpha} \\ln \\left( \\sum_{k} p_k^\\alpha \\right)\n$$\nwhere $\\alpha \\ge 0$ is a real number, $\\alpha \\neq 1$, and the sum is over the support of the random variable. The logarithm is the natural logarithm.\n\nConsider a discrete random variable $K$ that follows a Zeta distribution with parameter $s$. The random variable $K$ takes values in the set of positive integers $\\{1, 2, 3, \\dots\\}$. Its PMF is given by\n$$\nP(K=k) = \\frac{k^{-s}}{\\zeta(s)}\n$$\nfor $k \\in \\{1, 2, 3, \\dots\\}$. Here, $s>1$ is a real parameter, and $\\zeta(s) = \\sum_{n=1}^\\infty n^{-s}$ is the Riemann zeta function.\n\nCalculate the Rényi entropy of order $\\alpha$, $H_\\alpha(K)$, for this random variable. Assume the parameters $\\alpha$ and $s$ are such that the condition $s\\alpha > 1$ is satisfied. Express your answer in terms of $s$, $\\alpha$, and the Riemann zeta function $\\zeta(\\cdot)$.", "solution": "The Rényi entropy of order $\\alpha$ for a discrete random variable $K$ with probability mass function $p_k$ is defined as:\n$$\nH_\\alpha(K) = \\frac{1}{1-\\alpha} \\ln \\left( \\sum_{k} p_k^\\alpha \\right)\n$$\nGiven that $K$ follows a Zeta distribution with parameter $s > 1$, the PMF is:\n$$\nP(K=k) = p_k = \\frac{k^{-s}}{\\zeta(s)}, \\quad k = 1, 2, 3, \\dots\n$$\nwhere $\\zeta(s) = \\sum_{n=1}^\\infty n^{-s}$ is the Riemann zeta function. The condition $s\\alpha > 1$ ensures convergence.\n\nFirst, compute $p_k^\\alpha$:\n$$\np_k^\\alpha = \\left( \\frac{k^{-s}}{\\zeta(s)} \\right)^\\alpha = k^{-s\\alpha} \\cdot \\zeta(s)^{-\\alpha}\n$$\nSubstitute into the sum:\n$$\n\\sum_{k=1}^\\infty p_k^\\alpha = \\sum_{k=1}^\\infty k^{-s\\alpha} \\cdot \\zeta(s)^{-\\alpha} = \\zeta(s)^{-\\alpha} \\sum_{k=1}^\\infty k^{-s\\alpha}\n$$\nThe sum $\\sum_{k=1}^\\infty k^{-s\\alpha}$ is the Riemann zeta function evaluated at $s\\alpha$:\n$$\n\\sum_{k=1}^\\infty k^{-s\\alpha} = \\zeta(s\\alpha)\n$$\nThus:\n$$\n\\sum_{k=1}^\\infty p_k^\\alpha = \\zeta(s)^{-\\alpha} \\zeta(s\\alpha)\n$$\nNow, take the natural logarithm:\n$$\n\\ln \\left( \\sum_{k=1}^\\infty p_k^\\alpha \\right) = \\ln \\left( \\zeta(s)^{-\\alpha} \\zeta(s\\alpha) \\right) = \\ln \\left( \\zeta(s\\alpha) \\right) - \\ln \\left( \\zeta(s)^{\\alpha} \\right) = \\ln \\zeta(s\\alpha) - \\alpha \\ln \\zeta(s)\n$$\nSubstitute into the Rényi entropy formula:\n$$\nH_\\alpha(K) = \\frac{1}{1-\\alpha} \\left( \\ln \\zeta(s\\alpha) - \\alpha \\ln \\zeta(s) \\right)\n$$\nThe expression simplifies to:\n$$\nH_\\alpha(K) = \\frac{ \\ln \\zeta(s\\alpha) - \\alpha \\ln \\zeta(s) }{1-\\alpha}\n$$\nThis is the Rényi entropy for the Zeta distribution, expressed in terms of $s$, $\\alpha$, and the Riemann zeta function $\\zeta(\\cdot)$.", "answer": "$$ \\boxed{ \\dfrac{ \\ln \\zeta(s\\alpha) - \\alpha \\ln \\zeta(s) }{1-\\alpha} } $$", "id": "132115"}, {"introduction": "This advanced problem moves beyond simple calculation and into the realm of optimization, a common task in fields like signal processing and quantum computing. You are challenged to discover the specific probability distribution that minimizes the collision entropy while satisfying a fixed constraint on its mean value. This exercise illustrates a powerful technique in constrained optimization and reveals a surprising, elegant principle about the nature of extremal distributions. [@problem_id:1655423]", "problem": "A quantum information processor is designed based on a system with $N=11$ distinct states, labeled by an integer index $k \\in \\{0, 1, \\dots, 10\\}$. The probability of the system being in state $k$ is given by $p_k$. A key performance metric for certain algorithms is the state's \"purity\", which is related to the collision entropy. The collision entropy, a special case of Rényi entropy, is defined as $H_2(X) = -\\ln \\left( \\sum_{k=0}^{10} p_k^2 \\right)$.\n\nThe system is prepared in a state such that the mean value of the state index, $\\mathbb{E}[X] = \\sum_{k=0}^{10} k \\, p_k$, is precisely constrained to the non-integer value $\\mu = 3.8$.\n\nYour task is to determine the complete probability distribution $\\{p_k\\}_{k=0}^{10}$ that minimizes the collision entropy $H_2(X)$ under this constraint. Identify all states $k$ that have a non-zero probability, and provide their corresponding probability values. Let these states be indexed by $k_1 < k_2 < \\dots < k_M$. Provide the numerical values for $k_1, p_{k_1}, k_2, p_{k_2}, \\dots, k_M, p_{k_M}$ as a sequence. Round all probability values to three significant figures.", "solution": "The problem asks us to find the probability distribution $\\{p_k\\}_{k=0}^{10}$ that minimizes the collision entropy $H_2(X) = -\\ln \\left( \\sum_{k=0}^{10} p_k^2 \\right)$ subject to certain constraints.\n\nStep 1: Formulate the optimization problem.\nMinimizing $H_2(X)$ is equivalent to maximizing its argument in the logarithm, since $-\\ln(x)$ is a monotonically decreasing function. Let $S = \\sum_{k=0}^{10} p_k^2$. The problem is thus to maximize $S$ subject to the following constraints:\n1. Normalization: $\\sum_{k=0}^{10} p_k = 1$\n2. Mean value: $\\sum_{k=0}^{10} k \\, p_k = \\mu = 3.8$\n3. Non-negativity: $p_k \\ge 0$ for all $k \\in \\{0, 1, \\dots, 10\\}$\n\nStep 2: Analyze the nature of the solution.\nThe set of all valid probability distributions satisfying these constraints forms a convex set (specifically, a convex polytope) in the 11-dimensional space of probabilities $(p_0, p_1, \\dots, p_{10})$. The objective function $S(\\mathbf{p}) = \\sum_{k=0}^{10} p_k^2$ is a strictly convex function. The Hessian matrix of $S$ is $H_{ij} = \\frac{\\partial^2 S}{\\partial p_i \\partial p_j} = 2\\delta_{ij}$, which is a diagonal matrix with all positive entries (2), making it positive definite.\n\nA fundamental theorem of convex optimization states that the maximum of a convex function over a compact convex set must be achieved at one of the extreme points (vertices) of the set. We must therefore characterize the vertices of the feasible set.\n\nA distribution $\\mathbf{p}$ is a vertex if it cannot be written as a convex combination of two other distinct distributions in the set. Let's assume a distribution has three or more non-zero probabilities, say at indices $k_1 < k_2 < k_3$. We can construct a vector $\\mathbf{v}$ with non-zero components only at these indices such that $\\mathbf{v}$ is orthogonal to the constraint vectors. That is, $\\sum v_k = 0$ and $\\sum k v_k = 0$. For instance, let $v_{k_1} = k_2 - k_3$, $v_{k_2} = k_3 - k_1$, and $v_{k_3} = k_1 - k_2$. The sum of components is zero. The sum $\\sum k v_k = k_1(k_2-k_3) + k_2(k_3-k_1) + k_3(k_1-k_2) = 0$.\nFor a small enough $\\epsilon > 0$, the distributions $\\mathbf{p}' = \\mathbf{p} + \\epsilon \\mathbf{v}$ and $\\mathbf{p}'' = \\mathbf{p} - \\epsilon \\mathbf{v}$ are also in the feasible set. Then $\\mathbf{p} = \\frac{1}{2}\\mathbf{p}' + \\frac{1}{2}\\mathbf{p}''$, which contradicts the assumption that $\\mathbf{p}$ is a vertex. Therefore, any vertex of the feasible set can have at most two non-zero probabilities.\n\nStep 3: Solve for the two-point distribution.\nLet the two states with non-zero probability be $i$ and $j$, where $i < j$. The constraints become:\n1. $p_i + p_j = 1$\n2. $i \\cdot p_i + j \\cdot p_j = \\mu$\n\nFrom (1), we have $p_j = 1 - p_i$. Substituting into (2):\n$i \\cdot p_i + j(1 - p_i) = \\mu$\n$p_i(i - j) = \\mu - j$\n$p_i = \\frac{\\mu - j}{i - j} = \\frac{j - \\mu}{j - i}$\n\nAnd for $p_j$:\n$p_j = 1 - p_i = 1 - \\frac{j - \\mu}{j - i} = \\frac{j - i - (j - \\mu)}{j - i} = \\frac{\\mu - i}{j - i}$\n\nFor these to be valid probabilities, we need $p_i \\ge 0$ and $p_j \\ge 0$. This implies $j - \\mu \\ge 0$ and $\\mu - i \\ge 0$, so we must have $i \\le \\mu \\le j$.\n\nStep 4: Find the optimal pair $(i, j)$.\nThe sum of squares is $S(i,j) = p_i^2 + p_j^2 = \\left(\\frac{j - \\mu}{j - i}\\right)^2 + \\left(\\frac{\\mu - i}{j - i}\\right)^2$. We need to find the pair of integers $(i, j)$ with $0 \\le i < j \\le 10$ and $i \\le 3.8 \\le j$ that maximizes this quantity.\nLet's analyze the behavior of $S(i, j)$ by taking a partial derivative with respect to $j$ (treating it as continuous for a moment) for a fixed $i$:\n$\\frac{\\partial S}{\\partial j} = \\frac{2(\\mu - i)}{(j - i)^3} (j + i - 2\\mu)$.\nSince $j > i$ and $\\mu > i$, the sign is determined by the term $(j + i - 2\\mu)$. The function increases with $j$ if $j > 2\\mu - i$ and decreases if $j < 2\\mu - i$. This implies that for a fixed $i$, the maximum value of $S$ must occur at the boundaries of the allowed range for $j$, i.e., $j = \\lceil \\mu \\rceil$ or $j = 10$.\nSimilarly, by analyzing $\\frac{\\partial S}{\\partial i}$, one can show that for a fixed $j$, the maximum must occur at $i = \\lfloor \\mu \\rfloor$ or $i = 0$.\nThus, the optimal pair $(i, j)$ must be one of the four candidate pairs formed by combining these boundary values:\n$(\\lfloor \\mu \\rfloor, \\lceil \\mu \\rceil)$, $(\\lfloor \\mu \\rfloor, 10)$, $(0, \\lceil \\mu \\rceil)$, $(0, 10)$.\n\nStep 5: Test the candidate pairs for $\\mu = 3.8$.\nWe have $\\lfloor \\mu \\rfloor = 3$ and $\\lceil \\mu \\rceil = 4$. The candidate pairs are $(3,4)$, $(3,10)$, $(0,4)$, and $(0,10)$. All satisfy the condition $i \\le 3.8 \\le j$.\n\nCase 1: $(i, j) = (3, 4)$\n$p_3 = \\frac{4 - 3.8}{4 - 3} = 0.2$\n$p_4 = \\frac{3.8 - 3}{4 - 3} = 0.8$\n$S = (0.2)^2 + (0.8)^2 = 0.04 + 0.64 = 0.68$\n\nCase 2: $(i, j) = (0, 4)$\n$p_0 = \\frac{4 - 3.8}{4 - 0} = \\frac{0.2}{4} = 0.05$\n$p_4 = \\frac{3.8 - 0}{4 - 0} = \\frac{3.8}{4} = 0.95$\n$S = (0.05)^2 + (0.95)^2 = 0.0025 + 0.9025 = 0.905$\n\nCase 3: $(i, j) = (3, 10)$\n$p_3 = \\frac{10 - 3.8}{10 - 3} = \\frac{6.2}{7} \\approx 0.88571$\n$p_{10} = \\frac{3.8 - 3}{10 - 3} = \\frac{0.8}{7} \\approx 0.11429$\n$S \\approx (0.88571)^2 + (0.11429)^2 \\approx 0.7845 + 0.0131 = 0.7976$\n\nCase 4: $(i, j) = (0, 10)$\n$p_0 = \\frac{10 - 3.8}{10 - 0} = 0.62$\n$p_{10} = \\frac{3.8 - 0}{10 - 0} = 0.38$\n$S = (0.62)^2 + (0.38)^2 = 0.3844 + 0.1444 = 0.5288$\n\nComparing the values of $S$, the maximum value is $S=0.905$, which occurs for the pair $(i, j) = (0, 4)$.\n\nStep 6: State the final distribution.\nThe distribution that minimizes the collision entropy is non-zero only for $k=0$ and $k=4$. The probabilities are:\n$p_0 = 0.05$\n$p_4 = 0.95$\nAll other probabilities $p_k$ for $k \\notin \\{0, 4\\}$ are zero.\nRounding to three significant figures, we get $p_0 = 0.0500$ and $p_4 = 0.950$.\nThe indices in increasing order are $k_1=0$ and $k_2=4$. The sequence of values is $k_1, p_{k_1}, k_2, p_{k_2}$.\nThis gives $(0, 0.0500)$ and $(4, 0.950)$.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & 0.0500 & 4 & 0.950 \\end{pmatrix}}$$", "id": "1655423"}]}