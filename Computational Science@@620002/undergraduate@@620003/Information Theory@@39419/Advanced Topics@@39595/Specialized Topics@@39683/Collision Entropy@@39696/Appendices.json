{"hands_on_practices": [{"introduction": "To truly master the concept of collision entropy, we must move from theoretical definitions to practical application. This section provides a series of hands-on exercises designed to build your skills progressively. We will start with a direct numerical calculation, then advance to deriving a general formula for a fundamental probability distribution, and finally, tackle a conceptual proof that illuminates a key property of information. These practices will not only reinforce your computational abilities but also deepen your intuitive understanding of how collision entropy quantifies uncertainty. Note that collision entropy can be expressed in different units depending on the base of the logarithm used; 'nats' for the natural logarithm ($\\ln$) and 'bits' for the base-2 logarithm ($\\log_2$). While the principles are identical, pay close attention to the definition provided in each problem.\n\nOur first exercise grounds the abstract definition of collision entropy in a concrete scenario from biophysics. By calculating the collision entropy for a simplified model of an ion channel, you will practice the fundamental computational steps: determining the collision probability from a given set of state probabilities and then converting it into an entropy value. This straightforward calculation is the bedrock upon which more complex information-theoretic analyses are built. [@problem_id:1611469]", "problem": "A biophysics research group develops a simplified statistical model for the behavior of a single ion channel in a cell membrane. At any given moment, the channel is assumed to be in one of three distinct conformational states: 'Open' (allowing ions to pass), 'Closed' (blocking ion flow), or 'Inactivated' (a different type of non-conducting state). Based on extensive experimental data, the probability of finding the channel in the 'Open' state is $0.6$, in the 'Closed' state is $0.3$, and in the 'Inactivated' state is $0.1$.\n\nFor a discrete random variable $X$ with outcomes $\\{x_1, x_2, \\dots, x_n\\}$ and a probability mass function $P(X=x_i) = p_i$, a quantity known as the collision probability, $P_{\\text{coll}}(X)$, is defined as the probability that two independent samples drawn from this distribution are identical. Mathematically, it is given by $P_{\\text{coll}}(X) = \\sum_{i=1}^{n} p_i^2$. The Rényi entropy of order 2, often denoted $H_2(X)$, is then defined as $H_2(X) = -\\ln(P_{\\text{coll}}(X))$, where $\\ln$ is the natural logarithm.\n\nCalculate the Rényi entropy of order 2, $H_2(X)$, for the described ion channel model. Provide your final answer as a numerical value rounded to three significant figures.", "solution": "We are given a discrete random variable with three outcomes corresponding to the ion channel states, with probabilities $p_{\\text{Open}}=0.6$, $p_{\\text{Closed}}=0.3$, and $p_{\\text{Inactivated}}=0.1$. The collision probability is defined as the probability that two independent samples match, which is the sum of squared probabilities:\n$$\nP_{\\text{coll}}(X)=\\sum_{i} p_{i}^{2}=(0.6)^{2}+(0.3)^{2}+(0.1)^{2}.\n$$\nCompute each term:\n$$\n(0.6)^{2}=0.36,\\quad (0.3)^{2}=0.09,\\quad (0.1)^{2}=0.01,\n$$\nhence\n$$\nP_{\\text{coll}}(X)=0.36+0.09+0.01=0.46.\n$$\nThe Rényi entropy of order $2$ is defined as\n$$\nH_{2}(X)=-\\ln\\!\\big(P_{\\text{coll}}(X)\\big)=-\\ln(0.46).\n$$\nEvaluating the natural logarithm,\n$$\n-\\ln(0.46)\\approx 0.776528789.\n$$\nRounded to three significant figures, this yields\n$$\nH_{2}(X)\\approx 0.777.\n$$", "answer": "$$\\boxed{0.777}$$", "id": "1611469"}, {"introduction": "Moving beyond specific numerical values, this practice challenges you to generalize your understanding. You will derive a formula for the collision entropy of a Bernoulli trial, which models any process with two possible outcomes, as a function of its success probability $p$. This exercise, framed in the context of a quantum qubit measurement, is crucial for understanding how entropy changes as the underlying probability distribution shifts from deterministic to maximally uncertain. [@problem_id:1611496]", "problem": "In the field of quantum computing, a single quantum bit (qubit) can exist in a superposition of states. When measured in a particular basis, its state collapses to one of two definite outcomes, which we can label as 0 and 1. Consider such a measurement process, where the outcome is described by a discrete random variable $X$ that takes values in the set $\\{0, 1\\}$. The system is prepared such that the probability of observing the outcome 1 is given by a parameter $p$, where $0  p  1$.\n\nIn information theory, the collision entropy (also known as the Rényi entropy of order 2) is a measure of the uncertainty associated with a random variable. For a discrete random variable $Y$ with a probability mass function $p_Y(y)$ over a set of outcomes $\\mathcal{Y}$, the collision entropy is defined as:\n$$H_2(Y) = -\\log_2 \\left( \\sum_{y \\in \\mathcal{Y}} [p_Y(y)]^2 \\right)$$\nYour task is to calculate the collision entropy, $H_2(X)$, for the qubit measurement outcome $X$. Express your final answer as a function of the parameter $p$.", "solution": "The problem asks for the collision entropy $H_2(X)$ of a random variable $X$ that models a qubit measurement outcome. The collision entropy is defined as $H_2(X) = -\\log_2 \\left( \\sum_{x} [P(X=x)]^2 \\right)$.\n\nFirst, we need to identify the probability mass function (PMF) of the random variable $X$. The problem states that $X$ can take on two values: 0 and 1. The probability of the outcome being 1 is given as $p$.\n$$P(X=1) = p$$\nSince the sum of all probabilities must equal 1, the probability of the outcome being 0 is:\n$$P(X=0) = 1 - P(X=1) = 1 - p$$\nSo, the PMF for the random variable $X$ is fully defined for the two possible outcomes $x \\in \\{0, 1\\}$.\n\nNext, we calculate the sum of the squares of these probabilities, which is the term inside the logarithm in the definition of collision entropy.\n$$\\sum_{x \\in \\{0,1\\}} [P(X=x)]^2 = [P(X=0)]^2 + [P(X=1)]^2$$\nSubstituting the probabilities we found:\n$$\\sum_{x} [P(X=x)]^2 = (1-p)^2 + p^2$$\n\nNow, we expand and simplify this expression:\n$$(1-p)^2 + p^2 = (1 - 2p + p^2) + p^2$$\n$$= 2p^2 - 2p + 1$$\n\nFinally, we substitute this simplified sum back into the formula for the collision entropy $H_2(X)$:\n$$H_2(X) = -\\log_2 \\left( \\sum_{x} [P(X=x)]^2 \\right) = -\\log_2(2p^2 - 2p + 1)$$\nThis is the collision entropy for the qubit measurement outcome $X$, expressed as a function of the parameter $p$.", "answer": "$$\\boxed{-\\log_{2}(2p^{2} - 2p + 1)}$$", "id": "1611496"}, {"introduction": "This final practice transitions from computation to a conceptual proof, testing your grasp of the core ideas behind joint entropy. You will explore a system where one random variable is a deterministic function of another, a common scenario in signal processing and data analysis. By proving the relationship between the joint collision entropy $H_2(X,Y)$ and the individual entropy $H_2(X)$, you will uncover a profound principle: deterministic operations do not add new uncertainty to a system. [@problem_id:1611486]", "problem": "Consider two discrete random variables, $X$ and $Y$, with finite alphabets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively. The random variable $X$ has a probability mass function $P(X=x)$ for $x \\in \\mathcal{X}$. The random variable $Y$ is a deterministic function of $X$, meaning that for each $x \\in \\mathcal{X}$, there is a unique $y \\in \\mathcal{Y}$ such that if $X=x$, then $Y=y$. We can write this relationship as $Y=f(X)$.\n\nThe collision entropy (or Rényi entropy of order 2) for a discrete random variable $Z$ with probability mass function $P(Z=z)$ is defined as:\n$$H_2(Z) = -\\ln \\left( \\sum_{z} [P(Z=z)]^2 \\right)$$\nSimilarly, the joint collision entropy of $X$ and $Y$ is defined using their joint probability mass function $P(X=x, Y=y)$:\n$$H_2(X,Y) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} [P(X=x, Y=y)]^2 \\right)$$\n\nGiven this setup, express the joint collision entropy $H_2(X,Y)$ in terms of the collision entropy of $X$, denoted as $H_2(X)$. Your final answer should be a simple expression involving only $H_2(X)$.", "solution": "We are given discrete random variables $X$ and $Y$ with finite alphabets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively, and $Y$ is a deterministic function of $X$, written as $Y=f(X)$. By the definition of a deterministic mapping, for each $x \\in \\mathcal{X}$, we have $P(Y=f(x) \\mid X=x)=1$ and $P(Y=y \\mid X=x)=0$ for $y \\neq f(x)$.\nUsing the definition of joint probabilities via conditioning, this implies\n$$\nP(X=x, Y=y) = P(X=x) P(Y=y \\mid X=x) = \n\\begin{cases}\nP(X=x),  \\text{if } y=f(x),\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nThe joint collision entropy is defined as\n$$\nH_{2}(X,Y) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\left[P(X=x, Y=y)\\right]^{2} \\right).\n$$\nSubstituting the structure of $P(X=x, Y=y)$ from the deterministic relation,\n$$\n\\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\left[P(X=x, Y=y)\\right]^{2}\n= \\sum_{x \\in \\mathcal{X}} \\left[P(X=x, Y=f(x))\\right]^{2} + \\sum_{x \\in \\mathcal{X}} \\sum_{y \\neq f(x)} 0\n= \\sum_{x \\in \\mathcal{X}} \\left[P(X=x)\\right]^{2}.\n$$\nTherefore,\n$$\nH_{2}(X,Y) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\left[P(X=x)\\right]^{2} \\right).\n$$\nBut the right-hand side is exactly the collision entropy of $X$, by definition:\n$$\nH_{2}(X) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\left[P(X=x)\\right]^{2} \\right).\n$$\nHence,\n$$\nH_{2}(X,Y) = H_{2}(X).\n$$", "answer": "$$\\boxed{H_{2}(X)}$$", "id": "1611486"}]}