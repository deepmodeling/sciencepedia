{"hands_on_practices": [{"introduction": "To fully grasp the Information Bottleneck (IB) principle, it is helpful to first examine its boundaries. This practice problem [@problem_id:1631245] explores the two most extreme compression scenarios: transmitting a perfect copy of the original data, and transmitting a signal that is completely independent of it. By calculating the mutual information pairs $(I(X; T), I(T; Y))$ for these two cases, you will establish the upper and lower bounds for the compression-relevance trade-off, providing essential context for any optimization task.", "problem": "In the design of a remote medical monitoring system, a sensor measures a discrete physiological state $X$, which can take one of three values: `low`, `medium`, or `high`. This state is known to be correlated with a patient's future health outcome $Y$, which can be either `healthy` or `sick`. The joint probability distribution $p(X, Y)$ describing this relationship is given by the table below:\n\n| $p(X,Y)$ | $Y = \\text{healthy}$ | $Y = \\text{sick}$ |\n| :--- | :---: | :---: |\n| $X = \\text{low}$ | $1/4$ | $0$ |\n| $X = \\text{medium}$ | $1/4$ | $1/4$ |\n| $X = \\text{high}$ | $0$ | $1/4$ |\n\nTo conserve transmission bandwidth, the sensor's reading $X$ is mapped to a compressed representation $T$ before being sent to a central server. The effectiveness of this compression is evaluated using the principles of the Information Bottleneck (IB) method. This method considers two key quantities: the mutual information $I(X; T)$, which quantifies the \"bottleneck\" size (a smaller value means more compression), and the mutual information $I(T; Y)$, which measures how much information the compressed signal retains about the relevant outcome $Y$ (a larger value is better).\n\nYou are asked to analyze two extreme compression schemes:\n\n1.  **Scheme 1 (No Compression):** The transmitted signal, denoted $T_1$, is a perfect and uncompressed copy of the sensor's measurement.\n2.  **Scheme 2 (Maximum Useless Compression):** The transmitted signal, denoted $T_2$, is generated in such a way that it is statistically independent of the sensor's measurement $X$.\n\nFor each of these two schemes, determine the values of the pair $(I(X; T), I(T; Y))$. Use base 2 for all logarithm calculations, which means that all information-theoretic quantities should be in units of bits.\n\nProvide your answer as a 2x2 matrix of numerical values. The first row should contain the pair of values for Scheme 1, and the second row should contain the pair for Scheme 2. In both rows, the first column must be the value of $I(X;T)$ and the second column must be the value of $I(T;Y)$.", "solution": "We are given the joint distribution of $(X,Y)$. First compute the marginals:\n$$\np_{X}(\\text{low})=\\frac{1}{4},\\quad p_{X}(\\text{medium})=\\frac{1}{2},\\quad p_{X}(\\text{high})=\\frac{1}{4},\n$$\n$$\np_{Y}(\\text{healthy})=\\frac{1}{2},\\quad p_{Y}(\\text{sick})=\\frac{1}{2}.\n$$\nUsing base-2 logarithms, the entropy of $X$ is\n$$\nH(X)=-\\sum_{x}p_{X}(x)\\log_{2}p_{X}(x)=-\\left[\\frac{1}{4}\\log_{2}\\frac{1}{4}+\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right]=\\frac{3}{2}.\n$$\nThe entropy of $Y$ is\n$$\nH(Y)=-\\sum_{y}p_{Y}(y)\\log_{2}p_{Y}(y)=-\\left[\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{2}\\log_{2}\\frac{1}{2}\\right]=1.\n$$\nThe conditional distribution of $Y$ given $X$ yields\n$$\nH(Y|X=\\text{low})=0,\\quad H(Y|X=\\text{medium})=1,\\quad H(Y|X=\\text{high})=0,\n$$\nso\n$$\nH(Y|X)=\\sum_{x}p_{X}(x)H(Y|X=x)=\\frac{1}{2}.\n$$\nTherefore the mutual information between $X$ and $Y$ is\n$$\nI(X;Y)=H(Y)-H(Y|X)=1-\\frac{1}{2}=\\frac{1}{2}.\n$$\n\nScheme 1 (No compression): $T_{1}=X$, so by definition\n$$\nI(X;T_{1})=I(X;X)=H(X)=\\frac{3}{2},\n$$\nand by data processing with identity mapping $T_{1}=X$,\n$$\nI(T_{1};Y)=I(X;Y)=\\frac{1}{2}.\n$$\n\nScheme 2 (Maximum useless compression): $T_{2}$ is independent of $X$, i.e., $p(t|x)=p(t)$ for all $x$. Then\n$$\nI(X;T_{2})=0.\n$$\nMoreover,\n$$\np(t,y)=\\sum_{x}p(t|x)p(x,y)=\\sum_{x}p(t)p(x,y)=p(t)\\sum_{x}p(x,y)=p(t)p(y),\n$$\nso $T_{2}$ is also independent of $Y$ and\n$$\nI(T_{2};Y)=0.\n$$\n\nThus, the requested pairs $(I(X;T),I(T;Y))$ are\n- Scheme 1: $\\left(\\frac{3}{2},\\frac{1}{2}\\right)$,\n- Scheme 2: $(0,0)$,\nassembled into the required $2\\times 2$ matrix.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}}$$", "id": "1631245"}, {"introduction": "Real-world compression is not always deterministic. This exercise introduces a stochastic encoder, where the compressed representation $T$ is a probabilistic function of the input $X$. This setup [@problem_id:1631250] models a specific case where information flows through a Markov chain, $X \\to Y \\to T$, allowing you to practice calculating the key quantities $I(X;T)$ and $I(T;Y)$ and deepen your understanding of how information is preserved and lost in multi-stage processing.", "problem": "In a simplified model of a feature extraction system, we analyze input data consisting of a pair of independent and unbiased binary signals, $X = (X_1, X_2)$, where $P(X_1=0) = P(X_1=1) = 0.5$ and $P(X_2=0) = P(X_2=1) = 0.5$. A relevant feature of this data is its parity, defined as $Y = X_1 \\oplus X_2$, where $\\oplus$ denotes the exclusive OR (XOR) operation.\n\nTo reduce data dimensionality, the input $X$ is compressed into a ternary representation $T$, which can take one of three states from the set $\\mathcal{T} = \\{t_1, t_2, t_3\\}$. The compression is achieved via a stochastic mapping defined by a conditional probability distribution $p(t|x)$. The mapping rule is as follows:\n- If the input pair $X$ has a parity of 0 (i.e., $Y=0$), it is deterministically mapped to state $t_1$.\n- If the input pair $X$ has a parity of 1 (i.e., $Y=1$), it is randomly mapped to either state $t_2$ with probability $0.5$ or to state $t_3$ with probability $0.5$.\n\nThe Information Bottleneck (IB) method is used to evaluate this compression scheme. The quality of the compression is assessed by two mutual information quantities: $I(X;T)$, which measures how much information the compressed representation $T$ retains about the original data $X$, and $I(T;Y)$, which measures how much information $T$ preserves about the relevant feature $Y$.\n\nCalculate the values of $I(X;T)$ and $I(T;Y)$ for this system. Use base-2 logarithms for all information-theoretic calculations. Express your answer as an ordered pair of numerical values $(I(X;T), I(T;Y))$, with each value given in bits and rounded to four significant figures.", "solution": "We model $X=(X_{1},X_{2})$ with $X_{1},X_{2}$ independent and unbiased, so $p(x)=\\frac{1}{4}$ for each of the four pairs. The relevant variable is $Y=X_{1}\\oplus X_{2}$, so $P(Y=0)=P(Y=1)=\\frac{1}{2}$. The compression $X\\mapsto T$ depends only on $Y$: if $Y=0$, then $T=t_{1}$ deterministically; if $Y=1$, then $T=t_{2}$ or $T=t_{3}$ with probabilities $\\frac{1}{2}$ each. Thus, $X\\to Y\\to T$ is a Markov chain and $T$ is conditionally independent of $X$ given $Y$.\n\nFirst compute the marginal distribution of $T$:\n$$\nP(T=t_{1})=P(Y=0)=\\frac{1}{2},\\quad P(T=t_{2})=P(T=t_{3})=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\nHence the entropy of $T$ is\n$$\nH(T)=-\\sum_{t\\in\\{t_{1},t_{2},t_{3}\\}}P(t)\\log_{2}P(t)=-\\left(\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}+\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}+\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\right)=\\tfrac{3}{2}\\ \\text{bits}.\n$$\n\nFor $I(X;T)$, use $I(X;T)=H(T)-H(T|X)$ with\n$$\nH(T|X)=\\sum_{x}P(x)H(T|X=x).\n$$\nFor $x$ with parity $Y=0$, $T$ is deterministic ($t_{1}$), so $H(T|X=x)=0$. For $x$ with parity $Y=1$, $T$ is $t_{2}$ or $t_{3}$ with probabilities $\\frac{1}{2}$ each, so $H(T|X=x)=1$. Since half the $x$ have parity $0$ and half have parity $1$,\n$$\nH(T|X)=\\tfrac{1}{2}\\cdot 0+\\tfrac{1}{2}\\cdot 1=\\tfrac{1}{2}\\ \\text{bits},\n$$\nand therefore\n$$\nI(X;T)=H(T)-H(T|X)=\\tfrac{3}{2}-\\tfrac{1}{2}=1\\ \\text{bit}.\n$$\n\nFor $I(T;Y)$, one way is to use $I(T;Y)=H(Y)-H(Y|T)$. From the mapping, $T=t_{1}$ implies $Y=0$, and $T\\in\\{t_{2},t_{3}\\}$ implies $Y=1$, so $Y$ is a deterministic function of $T$ and $H(Y|T)=0$. Since $H(Y)=1$ bit (because $Y$ is unbiased),\n$$\nI(T;Y)=H(Y)-H(Y|T)=1-0=1\\ \\text{bit}.\n$$\nEquivalently, via $I(T;Y)=H(T)-H(T|Y)$ with $H(T|Y)=\\tfrac{1}{2}\\cdot 0+\\tfrac{1}{2}\\cdot 1=\\tfrac{1}{2}$, we obtain $I(T;Y)=\\tfrac{3}{2}-\\tfrac{1}{2}=1$.\n\nRounding to four significant figures, both values are $1.000$ bits. The ordered pair $(I(X;T),I(T;Y))$ is therefore $(1.000,1.000)$.", "answer": "$$\\boxed{\\begin{pmatrix}1.000 & 1.000\\end{pmatrix}}$$", "id": "1631250"}, {"introduction": "The central challenge of the Information Bottleneck method is not just to analyze a given compression scheme, but to find the optimal one. This problem [@problem_id:1631238] puts you in the designer's seat, tasking you with finding the best one-bit encoding for a given dataset and a specific trade-off parameter $\\beta$. You will apply the IB objective function to evaluate different partitions of the input data, directly engaging with the core task of optimizing the balance between information relevance and compression.", "problem": "In the field of machine learning, it is often desirable to create a compressed, low-dimensional representation of a high-dimensional input variable, while retaining as much relevant information as possible about a target variable. This problem explores a simplified version of this trade-off.\n\nConsider a system where an input signal $X$ is drawn uniformly at random from the set of four symbols $\\{a, b, c, d\\}$. A related binary output signal $Y$ can take values in $\\{0, 1\\}$. The relationship between $X$ and $Y$ is described by the following conditional probabilities:\n- $P(Y=1|X=a) = 0.1$\n- $P(Y=1|X=b) = 0.2$\n- $P(Y=1|X=c) = 0.8$\n- $P(Y=1|X=d) = 0.9$\n\nWe want to design a one-bit deterministic encoder for $X$. This means we partition the set of four input symbols $\\{a, b, c, d\\}$ into two non-empty groups. All symbols in the first group are mapped to a compressed representation $T=0$, and all symbols in the second group are mapped to $T=1$.\n\nThe quality of a given encoding (i.e., a partition) is measured by a score $S$, defined as:\n$$S = I(T; Y) - \\beta I(X; T)$$\nHere, $I(A; B)$ denotes the mutual information between random variables $A$ and $B$, all logarithms are base-2, and information is measured in bits. The parameter $\\beta$ balances the trade-off between how much information the representation $T$ has about the target $Y$ (relevance, $I(T;Y)$), and how much information $T$ retains about the original signal $X$ (complexity, $I(X;T)$).\n\nFor a trade-off parameter of $\\beta = 0.2$, determine the maximum possible score $S$ over all possible one-bit deterministic encodings. Round your final answer to three significant figures.", "solution": "Let $X$ be uniform on $\\{a,b,c,d\\}$ and $Y \\in \\{0,1\\}$ with $P(Y=1|X=a)=0.1$, $P(Y=1|X=b)=0.2$, $P(Y=1|X=c)=0.8$, $P(Y=1|X=d)=0.9$. A one-bit deterministic encoder $T=f(X)$ partitions $\\{a,b,c,d\\}$ into two non-empty groups. For such a deterministic $T$, we have $I(X;T)=H(T)$, since $H(T|X)=0$. The relevance is $I(T;Y)=H(Y)-H(Y|T)$. With $X$ uniform, the marginal of $Y$ is\n$$\nP(Y=1)=\\frac{0.1+0.2+0.8+0.9}{4}=\\frac{2}{4}=0.5 \\;\\Rightarrow\\; H(Y)=1.\n$$\nThus, for any partition with group sizes $n_{0}$ and $n_{1}$ (where $n_{0}+n_{1}=4$), letting $q_{t}=P(Y=1|T=t)$ be the average of the corresponding $P(Y=1|X=x)$ over the symbols in group $t$, we have\n$$\nI(T;Y)=1-\\sum_{t\\in\\{0,1\\}} \\frac{n_{t}}{4}\\,h_{2}(q_{t}),\\qquad I(X;T)=H(T)=h_{2}\\!\\left(\\frac{n_{0}}{4}\\right),\n$$\nso the score is\n$$\nS=1-\\sum_{t} \\frac{n_{t}}{4}\\,h_{2}(q_{t})-\\beta\\,h_{2}\\!\\left(\\frac{n_{0}}{4}\\right),\\qquad \\beta=0.2,\n$$\nwhere $h_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$.\n\nEnumerate all distinct partitions up to swapping the labels $T=0,1$.\n\n1) Two-two splits:\n- $\\{a,b\\}\\mid\\{c,d\\}$: $q_{\\text{low}}=\\frac{0.1+0.2}{2}=0.15$, $q_{\\text{high}}=\\frac{0.8+0.9}{2}=0.85$, hence $h_{2}(0.15)=h_{2}(0.85)$. Then\n$$\nH(Y|T)=\\frac{1}{2}h_{2}(0.15)+\\frac{1}{2}h_{2}(0.85)=h_{2}(0.15),\\quad H(T)=h_{2}\\!\\left(\\frac{1}{2}\\right)=1,\n$$\nso\n$$\nS=1-h_{2}(0.15)-0.2\\times 1=0.8-h_{2}(0.15).\n$$\nCompute $h_{2}(0.15)=-0.15\\log_{2}(0.15)-0.85\\log_{2}(0.85)\\approx 0.609840304716,$ hence\n$$\nS\\approx 0.8-0.609840304716=0.190159695284.\n$$\n- $\\{a,c\\}\\mid\\{b,d\\}$: $q=\\frac{0.1+0.8}{2}=0.45$ and $\\frac{0.2+0.9}{2}=0.55$, so $H(Y|T)=h_{2}(0.45)$ and $H(T)=1$. Then\n$$\nS=1-h_{2}(0.45)-0.2,\\quad h_{2}(0.45)\\approx 0.992454708 \\;\\Rightarrow\\; S\\approx -0.192454708.\n$$\n- $\\{a,d\\}\\mid\\{b,c\\}$: both groups have $q=0.5$, so $I(T;Y)=0$ and $S=-0.2$.\n\nThe best two-two split is $\\{a,b\\}\\mid\\{c,d\\}$ with $S\\approx 0.190159695284$.\n\n2) One-three splits (here $H(T)=h_{2}(1/4)=0.811278124459$, so the penalty term is $0.2\\times 0.811278124459\\approx 0.162255624892$):\n- Singleton $\\{a\\}$: $h_{2}(0.1)\\approx 0.468995593$, other group mean $q=\\frac{0.2+0.8+0.9}{3}=\\frac{19}{30}\\approx 0.633333333$, $h_{2}(q)\\approx 0.947161163$. Then\n$$\nH(Y|T)=\\frac{1}{4}h_{2}(0.1)+\\frac{3}{4}h_{2}\\!\\left(\\tfrac{19}{30}\\right)\\approx 0.827619771,\n$$\nso $I(T;Y)\\approx 0.172380229$ and\n$$\nS\\approx 0.172380229-0.162255625\\approx 0.010124604.\n$$\n- Singleton $\\{b\\}$: $h_{2}(0.2)=0.721928095$, other group $q=\\frac{0.1+0.8+0.9}{3}=0.6$, $h_{2}(0.6)\\approx 0.970950594$. Then $I(T;Y)\\approx 0.091305031$ and $S\\approx -0.070950594$.\n- Singleton $\\{c\\}$ is symmetric to $\\{b\\}$: $S\\approx -0.070950594$.\n- Singleton $\\{d\\}$ is symmetric to $\\{a\\}$: $S\\approx 0.010124604$.\n\nThus the maximal score over all one-bit deterministic encodings is achieved by the partition $\\{a,b\\}\\mid\\{c,d\\}$, yielding\n$$\nS_{\\max}=0.8-h_{2}(0.15)\\approx 0.190159695284.\n$$\nRounded to three significant figures, this is $0.190$.", "answer": "$$\\boxed{0.190}$$", "id": "1631238"}]}