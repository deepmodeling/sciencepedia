{"hands_on_practices": [{"introduction": "This first exercise demonstrates the core principle of Gelfand-Pinsker coding in its most intuitive form: interference pre-cancellation. We will explore a hypothetical channel where an additive interference, known to the transmitter, corrupts the signal. This practice [@problem_id:1626074] will guide you through designing an encoding strategy that effectively 'pre-subtracts' the known interference, allowing for surprisingly perfect communication.", "problem": "Consider a communication channel designed for a specialized digital system that operates using ternary logic. The channel input, denoted by $X$, must be selected from the alphabet $\\mathcal{X} = \\{0, 1, 2\\}$. The channel is affected by an additive state variable $S$, which also takes values from the alphabet $\\mathcal{S} = \\{0, 1, 2\\}$. The state $S$ is a random variable, uniformly distributed over its alphabet, and it is known to the transmitter before it chooses the input $X$. The channel output, $Y$, is determined by the relationship $Y = (X + S) \\pmod 3$, where the operation is addition modulo 3. The output alphabet is thus $\\mathcal{Y} = \\{0, 1, 2\\}$.\n\nDetermine the capacity of this channel. Express your answer as a closed-form analytic expression in units of bits per channel use.", "solution": "We are given a discrete memoryless channel with input alphabet $\\mathcal{X}=\\{0,1,2\\}$, state alphabet $\\mathcal{S}=\\{0,1,2\\}$, and output alphabet $\\mathcal{Y}=\\{0,1,2\\}$. The channel law is $Y=(X+S)\\pmod 3$, where $S$ is uniformly distributed on $\\mathcal{S}$ and is known to the transmitter before choosing $X$ for each channel use.\n\nFor a channel with state known non-causally at the transmitter, The Gelfand-Pinsker theorem states that the capacity is\n$$\nC=\\max_{p(u),\\,x(u,s)} I(U;Y),\n$$\nwhere $U$ is an auxiliary random variable independent of $S$ and $X=x(U,S)$ is a deterministic function.\n\nFirst, an upper bound follows from the finite output alphabet:\n$$\nI(U;Y)\\leq H(Y)\\leq \\log_{2}|\\mathcal{Y}|=\\log_{2} 3,\n$$\nso $C\\leq \\log_{2} 3$.\n\nTo achieve this bound, choose $U$ taking values in $\\{0,1,2\\}$, independent of $S$, with distribution $p(u)$ to be optimized, and define the encoding function\n$$\nx(u,s)=(u-s)\\pmod 3.\n$$\nThen the channel output satisfies\n$$\nY=(X+S)\\pmod 3=\\big((u-s)\\pmod 3+s\\big)\\pmod 3=u,\n$$\nsince addition and subtraction are over the group $\\mathbb{Z}_{3}$. Hence $Y$ is a deterministic function of $U$ with $Y=U$, which implies\n$$\nI(U;Y)=H(Y)-H(Y|U)=H(U)-0=H(U).\n$$\nMaximizing $H(U)$ over distributions on a ternary alphabet yields $H(U)\\leq \\log_{2} 3$, with equality for the uniform distribution on $\\{0,1,2\\}$. Therefore, with $U$ uniform,\n$$\nI(U;Y)=\\log_{2} 3.\n$$\nCombining the achievability with the upper bound, the capacity is\n$$\nC=\\log_{2} 3\\ \\text{bits per channel use}.\n$$", "answer": "$$\\boxed{\\log_{2} 3}$$", "id": "1626074"}, {"introduction": "Building on the idea of pre-coding, this problem explores a channel with multiplicative interference—think of it as a random sign flip. The key task here [@problem_id:1626041] is not just to find the capacity with state information, but to quantify the exact benefit by comparing it to a scenario where the transmitter is uninformed. This exercise highlights how knowing the channel state at the transmitter can turn a noisy channel into a perfect one, and it allows us to measure this improvement in bits per channel use.", "problem": "Consider a communication channel described by the equation $Y = S \\cdot X$, where the input $X$ is chosen by the transmitter from the set $\\{-1, 1\\}$, and the output $Y$ is observed by the receiver. The channel is subject to a multiplicative state $S$, also taking values from the set $\\{-1, 1\\}$. The state $S$ is a random variable that remains constant for each use of the channel and is drawn from a distribution given by $P(S=1)=p$ and $P(S=-1)=1-p$, with $0 < p < 1$.\n\nWe are interested in the capacity of this channel under two different conditions of state knowledge.\n1.  **Uninformed Case:** The realization of the state $S$ is unknown to both the transmitter and the receiver. Let the capacity in this scenario be $C_0$.\n2.  **Transmitter-Informed Case:** The realization of the state $S$ is known non-causally to the transmitter before it chooses the input $X$, but it remains unknown to the receiver. Let the capacity in this scenario be $C_T$.\n\nCalculate the channel capacity gain, $\\Delta C = C_T - C_0$, which represents the benefit of providing state information to the transmitter.\n\nExpress your answer as a function of $p$. Use $\\log_2$ for any logarithms, as capacity is measured in bits per channel use.", "solution": "We analyze the two scenarios by computing the mutual information optimized over the encoder strategy.\n\nUninformed case. The channel is $Y = S X$ with $X \\in \\{-1,1\\}$ and $S \\in \\{-1,1\\}$ independent of $X$, with $P(S=1)=p$ and $P(S=-1)=1-p$. For any input distribution $P_{X}$, the mutual information is\n$$\nI(X;Y) = H(Y) - H(Y|X).\n$$\nGiven $X=x$, $Y=Sx$ is a one-to-one function of $S$, hence $H(Y|X=x)=H(S)$ for all $x$, and thus\n$$\nH(Y|X)=H(S) = -p \\log_{2} p - (1-p)\\log_{2}(1-p).\n$$\nSince $Y \\in \\{-1,1\\}$, we have $H(Y) \\leq 1$, with equality when $Y$ is equiprobable. Choosing $P(X=1)=P(X=-1)=\\frac{1}{2}$ yields $P(Y=1)=p \\cdot \\frac{1}{2} + (1-p)\\cdot \\frac{1}{2}=\\frac{1}{2}$ and hence $H(Y)=1$. Therefore,\n$$\nC_{0}=\\max_{P_{X}} I(X;Y) = 1 - \\big[-p \\log_{2} p - (1-p)\\log_{2}(1-p)\\big].\n$$\n\nTransmitter-informed case (non-causal state). With non-causal state knowledge at the transmitter, the capacity is given by the Gelfand-Pinsker formula\n$$\nC_{T}=\\max_{P_{U},\\,x(u,s)} I(U;Y),\n$$\nwhere $U$ is an auxiliary independent of $S$, and $X=x(U,S)$. Choose $U \\in \\{-1,1\\}$ equiprobable and let $x(u,s)=u s$. Then\n$$\nY = S \\cdot X = S \\cdot (U S) = U,\n$$\ndeterministically. Hence $I(U;Y)=H(U)=1$. Since the output alphabet is binary, $I(U;Y) \\leq 1$, so this is optimal and\n$$\nC_{T}=1.\n$$\n\nCapacity gain. Combining the two,\n$$\n\\Delta C = C_{T} - C_{0} = 1 - \\left[1 - \\big(-p \\log_{2} p - (1-p)\\log_{2}(1-p)\\big)\\right]\n= -p \\log_{2} p - (1-p)\\log_{2}(1-p).\n$$\nThis equals the binary entropy $H_{2}(p)$ (with base-$2$ logarithms).", "answer": "$$\\boxed{-p \\log_{2} p - (1-p)\\log_{2}(1-p)}$$", "id": "1626041"}, {"introduction": "Not all interference can be neatly canceled. This final practice [@problem_id:1626065] presents a more nuanced scenario where a 'bad' channel state renders communication impossible, effectively 'jamming' the output. Instead of inverting the state's effect, the optimal strategy involves adapting the transmission to the state by communicating only when the channel is 'good'. This problem illustrates the flexibility of Gelfand-Pinsker coding, showing how it enables intelligent adaptation to dynamic channel conditions.", "problem": "Consider a digital communication channel where the transmission quality is affected by an external environmental state, $S$. The state $S$ is a binary random variable, taking values in $\\{0, 1\\}$. The state is 'good' ($S=0$) with probability $1-q$ and 'bad' ($S=1$) with probability $q$, where $0 < q < 1$. The state for each channel use is independent and identically distributed.\n\nThe channel input $X$ and output $Y$ are also binary, $X, Y \\in \\{0, 1\\}$. The channel's behavior is described by the conditional probabilities $p(y|x,s)$ as follows:\n- If the state is good ($S=0$), the channel is noiseless: the output is identical to the input, $Y=X$.\n- If the state is bad ($S=1$), the channel output is always 0, regardless of the input: $p(Y=0|X=0, S=1) = 1$ and $p(Y=0|X=1, S=1) = 1$.\n\nCrucially, the transmitter has non-causal knowledge of the state $S$; that is, it knows the value of $S$ before choosing the input $X$ to transmit.\n\nDetermine the capacity of this channel in bits per channel use. Express your answer as a function of $q$.", "solution": "Let $S \\in \\{0,1\\}$ be i.i.d. with $\\Pr[S=1]=q$ and $\\Pr[S=0]=1-q$. The channel is state-dependent with conditional law $p(y|x,s)$ defined by: if $s=0$ then $y=x$ (noiseless), and if $s=1$ then $y=0$ deterministically. The transmitter knows the state non-causally; the receiver does not.\n\nBy the Gelfand–Pinsker theorem for channels with state known non-causally at the encoder, the capacity is\n$$\nC=\\max_{p(u|s),\\,x(u,s)}\\bigl[I(U;Y)-I(U;S)\\bigr].\n$$\nWe construct an achievable scheme by choosing an auxiliary $U \\in \\{0,1\\}$ and an encoding function $x=f(u,s)$ so that $Y=U$ deterministically under the channel law. This is feasible if and only if $U$ satisfies $\\Pr[U=1|S=1]=0$ (because when $s=1$ the channel output is forced to $y=0$), and when $s=0$ we set $x=u$ so that $y=x=u$. Under this choice, $Y=U$ deterministically, hence\n$$\nI(U;Y)=H(Y), \\quad I(U;S)=I(Y;S)=H(Y)-H(Y|S),\n$$\nwhich gives the achievable value\n$$\nI(U;Y)-I(U;S)=H(Y|S).\n$$\nThe conditional distribution $p(y|s)$ is constrained by the channel and encoder choice: when $s=1$, $y=0$ almost surely, so $H(Y|S=1)=0$; when $s=0$, we may choose any $p=\\Pr[Y=1|S=0] \\in [0,1]$, giving $H(Y|S=0)=H_{2}(p)$, where $H_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$. Therefore,\n$$\nH(Y|S)=(1-q)H_{2}(p)+q\\cdot 0=(1-q)H_{2}(p),\n$$\nwhich is maximized at $p=\\frac{1}{2}$ with $H_{2}\\left(\\frac{1}{2}\\right)=1$. Hence the achievable rate is\n$$\nC \\ge (1-q)\\,.\n$$\n\nFor the converse, provide the state $S$ also to the receiver, which cannot decrease capacity. The resulting channel conditioned on $S$ has capacity $1$ bit when $S=0$ (noiseless) and capacity $0$ when $S=1$ (output independent of input). Averaging over $S$ yields an upper bound\n$$\nC \\le \\max_{p(x|s)} I(X;Y|S)=(1-q)\\cdot 1 + q\\cdot 0=(1-q).\n$$\nCombining the achievability and converse gives the capacity\n$$\nC(q)=1-q \\quad \\text{bits per channel use.}\n$$", "answer": "$$\\boxed{1-q}$$", "id": "1626065"}]}