{"hands_on_practices": [{"introduction": "A key feature that distinguishes relative entropy from other information-theoretic quantities like Shannon entropy is its behavior under a change of variables. This exercise explores the fundamental property of the invariance of Kullback-Leibler divergence under affine transformations. Understanding this invariance is crucial as it demonstrates that relative entropy measures an intrinsic relationship between two probability distributions, independent of the coordinate system used to describe the random variable [@problem_id:1655206].", "problem": "Let $X$ be a continuous random variable with two possible probability density functions (PDFs), denoted by $p_X(x)$ and $q_X(x)$. The Kullback-Leibler (KL) divergence, also known as relative entropy, is a measure of how one probability distribution diverges from a second, expected probability distribution. For the continuous variable $X$, it is defined as:\n$$\nD_{KL}(p_X || q_X) = \\int_{-\\infty}^{\\infty} p_X(x) \\ln\\left(\\frac{p_X(x)}{q_X(x)}\\right) dx\n$$\nAssume that this integral is well-defined and converges.\n\nNow, consider a new random variable $Y$ that is created by applying an affine transformation to $X$. The relationship is given by $Y = aX + b$, where $a$ is a non-zero real constant and $b$ is a real constant. Let $p_Y(y)$ and $q_Y(y)$ be the corresponding PDFs for the random variable $Y$ derived from $p_X(x)$ and $q_X(x)$, respectively.\n\nWhat is the relationship between the KL divergence for $Y$, denoted as $D_{KL}(p_Y || q_Y)$, and the KL divergence for $X$, denoted as $D_{KL}(p_X || q_X)$?\n\nA. $D_{KL}(p_Y || q_Y) = D_{KL}(p_X || q_X) - \\ln(|a|)$\n\nB. $D_{KL}(p_Y || q_Y) = a D_{KL}(p_X || q_X) + b$\n\nC. $D_{KL}(p_Y || q_Y) = |a| D_{KL}(p_X || q_X)$\n\nD. $D_{KL}(p_Y || q_Y) = D_{KL}(p_X || q_X) + \\ln(|a|)$\n\nE. $D_{KL}(p_Y || q_Y) = D_{KL}(p_X || q_X)$", "solution": "We start with the affine transformation $Y=aX+b$ with $a \\neq 0$. The change-of-variables formula for densities gives\n$$\np_{Y}(y)=p_{X}\\!\\left(\\frac{y-b}{a}\\right)\\left|\\frac{d}{dy}\\left(\\frac{y-b}{a}\\right)\\right|=\\frac{1}{|a|}\\,p_{X}\\!\\left(\\frac{y-b}{a}\\right),\n$$\nand similarly\n$$\nq_{Y}(y)=\\frac{1}{|a|}\\,q_{X}\\!\\left(\\frac{y-b}{a}\\right).\n$$\nHence the ratio simplifies to\n$$\n\\frac{p_{Y}(y)}{q_{Y}(y)}=\\frac{p_{X}\\!\\left(\\frac{y-b}{a}\\right)}{q_{X}\\!\\left(\\frac{y-b}{a}\\right)}.\n$$\nBy definition,\n$$\nD_{KL}(p_{Y}\\,||\\,q_{Y})=\\int_{-\\infty}^{\\infty} p_{Y}(y)\\,\\ln\\!\\left(\\frac{p_{Y}(y)}{q_{Y}(y)}\\right)\\,dy.\n$$\nSubstitute $x=\\frac{y-b}{a}$ so that $y=ax+b$ and, over the entire real line, $dy=|a|\\,dx$. Using the expressions above,\n$$\nD_{KL}(p_{Y}\\,||\\,q_{Y})=\\int_{-\\infty}^{\\infty}\\left(\\frac{1}{|a|}p_{X}(x)\\right)\\ln\\!\\left(\\frac{p_{X}(x)}{q_{X}(x)}\\right)\\,|a|\\,dx\n=\\int_{-\\infty}^{\\infty} p_{X}(x)\\,\\ln\\!\\left(\\frac{p_{X}(x)}{q_{X}(x)}\\right)\\,dx.\n$$\nTherefore,\n$$\nD_{KL}(p_{Y}\\,||\\,q_{Y})=D_{KL}(p_{X}\\,||\\,q_{X}).\n$$\nThis invariance reflects the cancellation of the Jacobian factors when both distributions are pushed forward by the same invertible transformation.", "answer": "$$\\boxed{E}$$", "id": "1655206"}, {"introduction": "While relative entropy provides a powerful measure of dissimilarity, its asymmetry ($D_{KL}(p||q) \\neq D_{KL}(q||p)$) means it is not a true distance metric. This practice addresses this by introducing the symmetric Jeffreys divergence while calculating it for the ubiquitous Gaussian distribution. You will discover how the divergence between two normal distributions with different means relates directly to their statistical separation, providing a concrete link between information theory and statistical distance [@problem_id:1655241].", "problem": "In information theory, one way to measure the \"distance\" or dissimilarity between two probability distributions is through divergence measures. Consider two continuous probability density functions, $p(x)$ and $q(x)$, defined for a random variable $x$ over the real line. The Kullback-Leibler (KL) divergence, or relative entropy, from $q$ to $p$ is defined as:\n$$D_{KL}(p||q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\nThe KL divergence is not symmetric, meaning $D_{KL}(p||q) \\neq D_{KL}(q||p)$ in general. To create a symmetric measure, the Jeffreys divergence, $J(p, q)$, is defined as the sum of the two KL divergences:\n$$J(p, q) = D_{KL}(p||q) + D_{KL}(q||p)$$\nLet $p(x)$ be the probability density function for a univariate normal distribution with mean $\\mu_A$ and variance $\\sigma^2$. Let $q(x)$ be the probability density function for another univariate normal distribution with a different mean $\\mu_B$ but the same variance $\\sigma^2$.\n\nDetermine the Jeffreys divergence $J(p, q)$ for these two normal distributions. Express your answer as a symbolic expression in terms of $\\mu_A$, $\\mu_B$, and $\\sigma$.", "solution": "We begin with the definition of the Kullback-Leibler divergence:\n$$\nD_{KL}(p\\|q)=\\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx=\\mathbb{E}_{p}\\!\\left[\\ln p(X)-\\ln q(X)\\right].\n$$\nLet $p(x)$ and $q(x)$ be the densities of univariate normal distributions with the same variance $\\sigma^{2}$ and means $\\mu_{A}$ and $\\mu_{B}$, respectively:\n$$\np(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma^{2}}\\right),\\quad\nq(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma^{2}}\\right).\n$$\nTaking logarithms,\n$$\n\\ln p(x)=-\\ln(\\sqrt{2\\pi}\\,\\sigma)-\\frac{(x-\\mu_{A})^{2}}{2\\sigma^{2}},\\quad\n\\ln q(x)=-\\ln(\\sqrt{2\\pi}\\,\\sigma)-\\frac{(x-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\nTherefore,\n$$\n\\ln p(x)-\\ln q(x)=-\\frac{(x-\\mu_{A})^{2}-(x-\\mu_{B})^{2}}{2\\sigma^{2}}=\\frac{(x-\\mu_{B})^{2}-(x-\\mu_{A})^{2}}{2\\sigma^{2}}.\n$$\nTaking expectation under $p$,\n$$\nD_{KL}(p\\|q)=\\mathbb{E}_{p}\\!\\left[\\frac{(X-\\mu_{B})^{2}-(X-\\mu_{A})^{2}}{2\\sigma^{2}}\\right]\n=\\frac{1}{2\\sigma^{2}}\\left(\\mathbb{E}_{p}[(X-\\mu_{B})^{2}]-\\mathbb{E}_{p}[(X-\\mu_{A})^{2}]\\right).\n$$\nUsing $\\mathbb{E}_{p}[X]=\\mu_{A}$ and $\\operatorname{Var}_{p}(X)=\\sigma^{2}$, and the identity $\\mathbb{E}[(X-a)^{2}]=\\operatorname{Var}(X)+(\\mathbb{E}[X]-a)^{2}$, we obtain\n$$\n\\mathbb{E}_{p}[(X-\\mu_{A})^{2}]=\\sigma^{2},\\quad\n\\mathbb{E}_{p}[(X-\\mu_{B})^{2}]=\\sigma^{2}+(\\mu_{A}-\\mu_{B})^{2}.\n$$\nHence,\n$$\nD_{KL}(p\\|q)=\\frac{1}{2\\sigma^{2}}\\left(\\sigma^{2}+(\\mu_{A}-\\mu_{B})^{2}-\\sigma^{2}\\right)\n=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\nBy symmetry of the roles of $p$ and $q$ in the above derivation (interchanging $\\mu_{A}$ and $\\mu_{B}$ leaves the expression unchanged), we also have\n$$\nD_{KL}(q\\|p)=\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma^{2}}=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\nTherefore, the Jeffreys divergence, defined as $J(p,q)=D_{KL}(p\\|q)+D_{KL}(q\\|p)$, is\n$$\nJ(p,q)=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}+\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{\\sigma^{2}}.\n$$", "answer": "$$\\boxed{\\frac{(\\mu_{A}-\\mu_{B})^{2}}{\\sigma^{2}}}$$", "id": "1655241"}, {"introduction": "In practical applications, we often approximate a complex or unknown true data distribution with a simpler, more tractable model. Relative entropy provides a principled way to quantify the \"information loss\" or error incurred by such an approximation. This problem challenges you to calculate the KL divergence when a \"true\" standard normal distribution is approximated by a standard Laplace distribution, giving you hands-on experience in evaluating model misspecification [@problem_id:1655225].", "problem": "In information theory, the Kullback-Leibler (KL) divergence, also known as relative entropy, measures how one probability distribution diverges from a second, reference probability distribution. For two continuous probability distributions $P$ and $Q$ with corresponding probability density functions (PDFs) $p(x)$ and $q(x)$ defined over the real numbers, the KL divergence of $Q$ from $P$ is defined as:\n$$D_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\nConsider a random variable whose true distribution is a standard normal distribution, which we will denote as $P$. Its PDF is given by:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\nSuppose we approximate this distribution using a standard Laplace distribution, which we will denote as $Q$. Its PDF is given by:\n$$q(x) = \\frac{1}{2} \\exp(-|x|)$$\nCalculate the KL divergence $D_{\\text{KL}}(P \\| Q)$, which quantifies the information lost when using $Q$ to approximate $P$. Express your answer as a single closed-form analytic expression.", "solution": "We start from the definition of the KL divergence for continuous distributions:\n$$\nD_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx\n= \\mathbb{E}_{P}[\\ln p(X)] - \\mathbb{E}_{P}[\\ln q(X)].\n$$\nFor the standard normal $P$ with $p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$, compute\n$$\n\\ln p(x) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{x^{2}}{2},\n$$\nso\n$$\n\\mathbb{E}_{P}[\\ln p(X)] = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\mathbb{E}_{P}[X^{2}] = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2},\n$$\nsince $\\mathbb{E}_{P}[X^{2}] = 1$ for a standard normal.\n\nFor the standard Laplace $Q$ with $q(x) = \\frac{1}{2}\\exp(-|x|)$, we have\n$$\n\\ln q(x) = -\\ln 2 - |x|,\n$$\nthus\n$$\n\\mathbb{E}_{P}[\\ln q(X)] = -\\ln 2 - \\mathbb{E}_{P}[|X|].\n$$\nTo evaluate $\\mathbb{E}_{P}[|X|]$ with $X \\sim \\mathcal{N}(0,1)$, use symmetry:\n$$\n\\mathbb{E}_{P}[|X|] = 2\\int_{0}^{\\infty} x\\,\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)\\,dx.\n$$\nLet $u = \\frac{x^{2}}{2}$, so $du = x\\,dx$. Then\n$$\n\\mathbb{E}_{P}[|X|] = \\frac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} \\exp(-u)\\,du = \\frac{2}{\\sqrt{2\\pi}}\\cdot 1 = \\sqrt{\\frac{2}{\\pi}}.\n$$\nTherefore,\n$$\nD_{\\text{KL}}(P \\| Q) = \\left(-\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\right) - \\left(-\\ln 2 - \\sqrt{\\frac{2}{\\pi}}\\right)\n= \\sqrt{\\frac{2}{\\pi}} + \\ln 2 - \\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}.\n$$\nSimplifying the logarithms,\n$$\n\\ln 2 - \\frac{1}{2}\\ln(2\\pi) = \\ln 2^{1/2} - \\frac{1}{2}\\ln \\pi = \\frac{1}{2}\\ln 2 - \\frac{1}{2}\\ln \\pi = \\frac{1}{2}\\ln\\!\\left(\\frac{2}{\\pi}\\right) = -\\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right),\n$$\nwhich yields the closed-form expression\n$$\nD_{\\text{KL}}(P \\| Q) = \\sqrt{\\frac{2}{\\pi}} - \\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right) - \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{2}{\\pi}}-\\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right)-\\frac{1}{2}}$$", "id": "1655225"}]}