{"hands_on_practices": [{"introduction": "A cornerstone result in information theory states that for discrete memoryless channels (DMCs), the presence of a feedback path from receiver to transmitter does not increase the channel capacity. This practice provides a direct application of this principle by asking you to compute the capacity of a Z-channel [@problem_id:1669160]. By working through this, you will reinforce the core skill of finding capacity by maximizing the mutual information, confirming that the fundamental limit remains unchanged even when feedback is available.", "problem": "A digital communication system is modeled by a binary asymmetric channel known as a Z-channel. The input alphabet is $\\mathcal{X} = \\{0, 1\\}$ and the output alphabet is $\\mathcal{Y} = \\{0, 1\\}$. The channel's behavior is defined by the following conditional probabilities:\n- A transmitted '0' is always received correctly as a '0'.\n- A transmitted '1' is received correctly as a '1' with probability $1-p$, but is erroneously flipped to a '0' with probability $p$, where $0  p  1$.\n\nAn enhancement is proposed for this system: after each symbol is received, a perfect, instantaneous, and noiseless feedback path from the receiver to the transmitter is established. This feedback informs the transmitter of the exact symbol ($0$ or $1$) that was just received at the output. The transmitter is free to use this information to adapt its transmission strategy.\n\nYour task is to determine the fundamental capacity of this Z-channel with the described feedback mechanism. The capacity is the maximum rate, in bits per channel use, at which information can be transmitted with arbitrarily low error probability.\n\nExpress your answer as a closed-form analytic expression in terms of the crossover probability $p$. For all logarithms in your final answer, use base 2.", "solution": "Let the channel input be $X \\in \\{0,1\\}$ with $P(X=1)=q$ and $P(X=0)=1-q$, and the output be $Y \\in \\{0,1\\}$. The Z-channel transition probabilities are $P(Y=0 \\mid X=0)=1$, $P(Y=1 \\mid X=0)=0$, $P(Y=1 \\mid X=1)=1-p$, and $P(Y=0 \\mid X=1)=p$, with $0p1$.\n\nFirst, we address the effect of feedback. For a discrete memoryless channel, the feedback capacity is\n$$\nC_{\\text{FB}}=\\sup \\lim_{n \\to \\infty} \\frac{1}{n} I(X^{n} \\to Y^{n}),\n$$\nwhere $I(X^{n} \\to Y^{n})=\\sum_{i=1}^{n} I(X_{i};Y_{i} \\mid Y^{i-1})$ is the directed information. By memorylessness and the fact that $Y_{i}$ depends on $X_{i}$ only through the one-step transition law, we have for each $i$\n$$\nI(X_{i};Y_{i} \\mid Y^{i-1}) \\leq \\max_{P_{X}} I(X;Y),\n$$\nbecause conditioning on $Y^{i-1}$ induces some input distribution $P_{X_{i}\\mid Y^{i-1}}$, and $I(X_{i};Y_{i}\\mid Y^{i-1}=y^{i-1}) \\leq \\max_{P_{X}} I(X;Y)$ for every $y^{i-1}$. Therefore,\n$$\nI(X^{n} \\to Y^{n}) \\leq n \\max_{P_{X}} I(X;Y),\n$$\nwhich gives $C_{\\text{FB}} \\leq C$, where $C=\\max_{P_{X}} I(X;Y)$ is the usual (no-feedback) capacity. Achievability of $C$ with feedback is immediate by using the same i.i.d. capacity-achieving input distribution and ignoring the feedback, so $C_{\\text{FB}}=C$. Hence, it suffices to compute $C=\\max_{q \\in [0,1]} I(X;Y)$.\n\nFor the Z-channel, compute\n$$\nP(Y=1)=P(Y=1 \\mid X=1)P(X=1)+P(Y=1 \\mid X=0)P(X=0)=(1-p)q,\n$$\nand hence $P(Y=0)=1-(1-p)q$. The output entropy is\n$$\nH(Y)=H_{b}((1-p)q),\n$$\nwhere $H_{b}(u)=-u \\log_{2} u-(1-u)\\log_{2}(1-u)$ is the binary entropy function (with base-2 logarithms). The conditional entropy is\n$$\nH(Y \\mid X)=P(X=0) H(Y \\mid X=0)+P(X=1) H(Y \\mid X=1)=0 \\cdot (1-q)+q \\, H_{b}(p)=q \\, H_{b}(p).\n$$\nTherefore,\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=H_{b}((1-p)q)-q \\, H_{b}(p).\n$$\n\nWe maximize $f(q)=H_{b}((1-p)q)-q \\, H_{b}(p)$ over $q \\in [0,1]$. Since $H_{b}$ is concave and the composition with an affine map preserves concavity, $f(q)$ is concave and has a unique maximizer in $[0,1]$. Differentiate using $H_{b}'(u)=\\log_{2}\\!\\left(\\frac{1-u}{u}\\right)$:\n$$\nf'(q)=(1-p)\\, H_{b}'((1-p)q)-H_{b}(p)=(1-p)\\log_{2}\\!\\left(\\frac{1-(1-p)q}{(1-p)q}\\right)-H_{b}(p).\n$$\nSet $f'(q)=0$ to obtain\n$$\n\\log_{2}\\!\\left(\\frac{1-(1-p)q}{(1-p)q}\\right)=\\frac{H_{b}(p)}{1-p}.\n$$\nDefine\n$$\nA \\triangleq 2^{\\frac{H_{b}(p)}{1-p}}.\n$$\nThen\n$$\n\\frac{1-(1-p)q}{(1-p)q}=A \\quad \\Longrightarrow \\quad (1-p)q=\\frac{1}{1+A}.\n$$\nLet $r \\triangleq (1-p)q=\\frac{1}{1+A}$. The maximizing $q^{\\ast}$ is $q^{\\ast}=\\frac{r}{1-p}=\\frac{1}{(1-p)(1+A)}$. The corresponding maximum mutual information is\n$$\nC=f(q^{\\ast})=H_{b}(r)-\\frac{r}{1-p} H_{b}(p).\n$$\nUse $r=\\frac{1}{1+A}$ and the identity $H_{b}(r)=\\log_{2}(1+A)-\\frac{A}{1+A}\\log_{2} A$ (obtained by substituting $r=\\frac{1}{1+A}$ and $1-r=\\frac{A}{1+A}$) to get\n$$\nC=\\log_{2}(1+A)-\\frac{A}{1+A}\\log_{2} A-\\frac{1}{1+A}\\log_{2} A=\\log_{2}(1+A)-\\log_{2} A=\\log_{2}\\!\\left(1+\\frac{1}{A}\\right).\n$$\nNow express $A$ in terms of $p$ alone. Since\n$$\n\\frac{H_{b}(p)}{1-p}=\\frac{-p \\log_{2} p-(1-p)\\log_{2}(1-p)}{1-p}=-\\frac{p}{1-p}\\log_{2} p-\\log_{2}(1-p),\n$$\nwe have\n$$\nA=2^{\\frac{H_{b}(p)}{1-p}}=2^{-\\left(\\frac{p}{1-p}\\log_{2} p+\\log_{2}(1-p)\\right)}=\\frac{1}{(1-p)\\, p^{\\frac{p}{1-p}}}.\n$$\nTherefore\n$$\n\\frac{1}{A}=(1-p)\\, p^{\\frac{p}{1-p}},\n$$\nand the capacity is\n$$\nC=\\log_{2}\\!\\left(1+(1-p)\\, p^{\\frac{p}{1-p}}\\right).\n$$\nBecause feedback does not increase the capacity of a discrete memoryless channel, this is also the capacity with the described perfect, instantaneous, noiseless feedback.", "answer": "$$\\boxed{\\log_{2}\\!\\left(1+(1-p)\\,p^{\\frac{p}{1-p}}\\right)}$$", "id": "1669160"}, {"introduction": "Having established that feedback does not increase the capacity limit, we now explore the profound consequences of this limit. This problem investigates a scenario where one attempts to transmit information from a source at a rate (its entropy) that exceeds the channel's capacity [@problem_id:1624717]. This exercise will guide you through the application of the data processing inequality and Fano's inequality to derive a quantitative lower bound on the achievable error probability, providing a concrete demonstration of why the capacity ceiling is unbreakable for reliable communication.", "problem": "An information theorist is analyzing the fundamental limits of a communication system designed to transmit data from a source modeled as an independent and identically distributed (i.i.d.) Bernoulli process with parameter $p$. The transmission occurs over a Binary Symmetric Channel (BSC), an acronym for a channel that flips each transmitted bit with a crossover probability of $\\epsilon$. The system is operated at a rate of one source symbol per channel use.\n\nThe theorist considers a powerful coding scheme that performs joint source-channel coding over very long blocks of data. This scheme also has access to an ideal (zero-delay, error-free) feedback channel, allowing the transmitter's encoding strategy for a given symbol to depend on all previously received channel outputs. The primary performance metric is the average bit error probability, $p_b$, defined as the long-term average probability that a decoded bit differs from its original source bit.\n\nAssume a scenario where the source generates information at a rate fundamentally faster than the channel can reliably handle; specifically, the entropy of the source is greater than the capacity of the channel.\n\nLet $H(x) = -x \\log_2(x) - (1-x) \\log_2(1-x)$ be the binary entropy function for $x \\in (0,1)$. Let $H^{-1}(y)$ be the inverse of this function defined for a domain of $y \\in [0, 1]$ that maps to a range of $x \\in [0, 1/2]$.\n\nWhich of the following expressions represents the tightest possible lower bound on the average bit error probability $p_b$ that *any* such communication system can achieve under these conditions?\n\nA. $p_b \\ge \\epsilon$\n\nB. Communication with an arbitrarily small $p_b > 0$ is possible with a sufficiently complex coding scheme.\n\nC. $p_b \\ge H^{-1}(H(p))$\n\nD. $p_b \\ge H^{-1}(H(p) + H(\\epsilon) - 1)$\n\nE. $p_b \\ge H^{-1}(1 - H(\\epsilon))$", "solution": "The problem asks for a lower bound on the average bit error probability $p_b$ when transmitting a Bernoulli($p$) source over a BSC($\\epsilon$) at a rate of one source symbol per channel use, under the condition that the source entropy $H(p)$ is greater than the channel capacity $C$.\n\nStep 1: Relate source entropy, information transmission, and channel capacity.\nLet $X^n = (X_1, ..., X_n)$ be a sequence of $n$ source symbols, and $\\hat{X}^n = (\\hat{X}_1, ..., \\hat{X}_n)$ be the corresponding sequence of decoded symbols. Since the source is i.i.d. Bernoulli($p$), the entropy of the source sequence is $H(X^n) = nH(p)$.\n\nThe relationship between the entropy of the source, the mutual information between the source and its estimate, and the conditional entropy (equivocation) is given by:\n$$H(X^n) = I(X^n; \\hat{X}^n) + H(X^n | \\hat{X}^n)$$\nwhere $I(X^n; \\hat{X}^n)$ is the mutual information.\n\nThe data processing inequality states that for any processing of the channel output $Y^n$ to get $\\hat{X}^n$, the mutual information cannot increase. The encoding and channel transmission form a Markov chain $X^n \\to U^n \\to Y^n \\to \\hat{X}^n$, where $U^n$ is the channel input sequence. Thus,\n$$I(X^n; \\hat{X}^n) \\le I(U^n; Y^n)$$\nEven with feedback, the channel input $U_i$ at time $i$ can depend on $X^n$ and previous channel outputs $Y^{i-1}$. However, a fundamental result of information theory is that for a discrete memoryless channel (DMC), feedback does not increase the channel capacity. The mutual information across $n$ uses of a DMC is bounded by $n$ times the capacity $C$:\n$$I(U^n; Y^n) \\le nC$$\nThe capacity of a BSC with crossover probability $\\epsilon$ is given by $C = 1 - H(\\epsilon)$.\n\nCombining these inequalities, we have:\n$$I(X^n; \\hat{X}^n) \\le nC$$\nSubstituting this back into the entropy equation:\n$$n H(p) = I(X^n; \\hat{X}^n) + H(X^n | \\hat{X}^n) \\le nC + H(X^n | \\hat{X}^n)$$\nRearranging this gives a lower bound on the equivocation:\n$$H(X^n | \\hat{X}^n) \\ge n H(p) - nC = n(H(p) - C)$$\n\nStep 2: Relate the equivocation to the bit error probability using Fano's Inequality.\nWe need to find an upper bound on $H(X^n | \\hat{X}^n)$ in terms of the average bit error probability $p_b$.\nUsing the chain rule for entropy and the fact that conditioning reduces entropy:\n$$H(X^n | \\hat{X}^n) = H(X_1, ..., X_n | \\hat{X}_1, ..., \\hat{X}_n) = \\sum_{i=1}^{n} H(X_i | X_1, ..., X_{i-1}, \\hat{X}^n) \\le \\sum_{i=1}^{n} H(X_i | \\hat{X}_i)$$\nFor a single bit, the conditional entropy $H(X_i | \\hat{X}_i)$ is related to the probability of error for that bit, $p_{e,i} = P(X_i \\ne \\hat{X}_i)$, by Fano's inequality for a single variable:\n$$H(X_i | \\hat{X}_i) \\le H(p_{e,i})$$\nTherefore, we have:\n$$H(X^n | \\hat{X}^n) \\le \\sum_{i=1}^{n} H(p_{e,i})$$\nThe binary entropy function $H(x)$ is concave. By Jensen's inequality:\n$$\\frac{1}{n} \\sum_{i=1}^{n} H(p_{e,i}) \\le H\\left(\\frac{1}{n} \\sum_{i=1}^{n} p_{e,i}\\right)$$\nThe term inside the entropy function on the right is the definition of the average bit error probability, $p_b$. So,\n$$\\sum_{i=1}^{n} H(p_{e,i}) \\le n H(p_b)$$\nThis gives the upper bound on equivocation:\n$$H(X^n | \\hat{X}^n) \\le n H(p_b)$$\n\nStep 3: Combine bounds and solve for $p_b$.\nWe have two bounds on $H(X^n | \\hat{X}^n)$:\n$$n(H(p) - C) \\le H(X^n | \\hat{X}^n) \\le n H(p_b)$$\nThis implies:\n$$n(H(p) - C) \\le n H(p_b)$$\n$$H(p) - C \\le H(p_b)$$\nNow substitute the capacity of the BSC, $C = 1 - H(\\epsilon)$:\n$$H(p) - (1 - H(\\epsilon)) \\le H(p_b)$$\n$$H(p) + H(\\epsilon) - 1 \\le H(p_b)$$\nThe problem assumes we are in a regime where $H(p)  C$, which means $H(p) + H(\\epsilon) - 1  0$. By convention, the bit error probability $p_b$ is assumed to be less than or equal to $1/2$ (if it were greater, one could flip all bits to get a better error rate). The binary entropy function $H(x)$ is monotonically increasing on the interval $[0, 1/2]$. Therefore, we can apply its inverse, $H^{-1}(y)$, to both sides of the inequality without changing the direction of the inequality:\n$$H^{-1}(H(p) + H(\\epsilon) - 1) \\le H^{-1}(H(p_b))$$\nGiven that $H^{-1}$ is the inverse of $H$ on the range $[0, 1/2]$, this simplifies to:\n$$p_b \\ge H^{-1}(H(p) + H(\\epsilon) - 1)$$\nThis provides the tightest possible lower bound on $p_b$ based on these information-theoretic arguments.\n\nStep 4: Evaluate the options.\nThe derived expression matches option D. Let's analyze the other options:\nA. $p_b \\ge \\epsilon$: This is a plausible distractor. A naive transmission without coding would yield $p_b=\\epsilon$. Coding can sometimes reduce the error rate below $\\epsilon$, but this bound is not universally tight. Our derived bound can be smaller or larger than $\\epsilon$ depending on parameters.\nB. Arbitrarily small $p_b$ is possible: This is a statement of the channel coding theorem, which holds only if the information rate is less than capacity. Here the information rate is $H(p)$ (since we send 1 source bit/channel use), and the problem states $H(p)C$. Thus, this statement is false.\nC. $p_b \\ge H^{-1}(H(p)) = p$: This corresponds to the source statistics, not the channel limitations. It represents an incorrect application of the principles.\nE. $p_b \\ge H^{-1}(1 - H(\\epsilon)) = H^{-1}(C)$: This bound ignores the rate of information generation from the source, $H(p)$, and only considers the channel capacity. It would be the result of an incomplete analysis.\n\nTherefore, the only correct and tightest bound among the choices is D.", "answer": "$$\\boxed{D}$$", "id": "1624717"}, {"introduction": "If feedback does not increase capacity, what is its practical value? This final practice addresses that question by examining its role in enabling efficient variable-length codes, where transmission continues until a desired level of certainty is reached [@problem_id:1624739]. You will calculate the maximum achievable rate for such a scheme under a specific reliability constraint, demonstrating how feedback improves practical performance while still being fundamentally governed by the channel capacity $C$.", "problem": "A communication system is designed to transmit a single message bit, $W$, which can be either 0 or 1 with equal probability ($P(W=0) = P(W=1) = 0.5$). The transmission occurs over a Binary Symmetric Channel (BSC) with a crossover probability of $p=0.1$. The capacity of this channel is given by $C = 1 - H_b(p)$, where $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ is the binary entropy function.\n\nTo enhance reliability, the system employs a variable-length coding scheme that utilizes a perfect, instantaneous, and noiseless feedback channel. At each time step $i=1, 2, 3, \\dots$, the transmitter sends a symbol $X_i \\in \\{0, 1\\}$. The choice of $X_i$ may depend on the message $W$ and all previously received symbols $(Y_1, \\dots, Y_{i-1})$, which are known to the transmitter via the feedback link. The transmission continues until a stopping time $N$, which is determined by the receiver.\n\nThe rate of this variable-length code is defined as $R = \\frac{\\log_2(2)}{\\mathbb{E}[N]} = \\frac{1}{\\mathbb{E}[N]}$, where $\\mathbb{E}[N]$ is the expected number of channel uses (i.e., the expected stopping time). For any such feedback scheme on a Discrete Memoryless Channel (DMC), the mutual information between the message and the full sequence of received symbols is bounded by $I(W; Y^N) \\le C \\cdot \\mathbb{E}[N]$.\n\nYour task is to determine the theoretical upper bound on the achievable rate $R$ if the decoding process must achieve an average probability of error no greater than $P_e = 0.01$. Express your answer in bits per channel use, rounded to four significant figures.", "solution": "The message $W$ is uniformly distributed over $\\{0,1\\}$, so $H(W)=1$. For any variable-length code with feedback over a DMC, the mutual information is bounded as $I(W;Y^{N}) \\le C \\,\\mathbb{E}[N]$, where $C=1-H_{b}(p)$ and $H_{b}(x)=-x\\log_{2}(x)-(1-x)\\log_{2}(1-x)$. With average error probability $P_{e}$, Fano’s inequality gives $H(W|Y^{N}) \\le H_{b}(P_{e})$ because $|{\\mathcal W}|=2$, hence\n$$\nI(W;Y^{N})=H(W)-H(W|Y^{N}) \\ge 1 - H_{b}(P_{e}).\n$$\nCombining,\n$$\n1 - H_{b}(P_{e}) \\le I(W;Y^{N}) \\le C\\,\\mathbb{E}[N],\n$$\nso\n$$\n\\mathbb{E}[N] \\ge \\frac{1 - H_{b}(P_{e})}{C}, \\quad R=\\frac{1}{\\mathbb{E}[N]} \\le \\frac{C}{1 - H_{b}(P_{e})}.\n$$\nFor the BSC with $p=0.1$, the capacity is\n$$\nC=1-H_{b}(0.1)=1+\\;0.1\\log_{2}(0.1)+0.9\\log_{2}(0.9)\\approx 0.5310044064.\n$$\nFor $P_{e}=0.01$,\n$$\nH_{b}(0.01)=-0.01\\log_{2}(0.01)-0.99\\log_{2}(0.99)\\approx 0.08079313590,\n$$\nso $1-H_{b}(0.01)\\approx 0.91920686410$. Therefore,\n$$\nR \\le \\frac{0.5310044064}{0.91920686410} \\approx 0.5776767,\n$$\nwhich, rounded to four significant figures, is $0.5777$ bits per channel use.", "answer": "$$\\boxed{0.5777}$$", "id": "1624739"}]}