{"hands_on_practices": [{"introduction": "The Entropy Power Inequality (EPI) becomes an equality if and only if the random variables involved are Gaussian. This exercise provides a concrete verification of this fundamental condition. By calculating the entropy power for two independent Gaussian noise sources and their sum [@problem_id:1620997], you will see firsthand how the entropy power of the sum, $N(X+Y)$, equals the sum of the individual entropy powers, $N(X) + N(Y)$, a property that mirrors the additivity of variance for independent Gaussians.", "problem": "A crucial component in a deep space communication receiver is designed to operate at extremely low temperatures. Its performance is limited by two primary sources of internal thermal noise. These noise sources can be accurately modeled as independent, zero-mean Gaussian random variables representing noise voltages. Let these random variables be $X$ and $Y$.\n\nFrom laboratory measurements, the variance of the first noise source is $\\sigma_X^2 = 3.5 \\text{ V}^2$, and the variance of the second noise source is $\\sigma_Y^2 = 5.2 \\text{ V}^2$. The total noise voltage affecting the signal is the sum of these two, $Z = X+Y$.\n\nIn information theory, the \"informativeness\" of a continuous random signal can be quantified by its entropy power. The entropy power, $N(V)$, of a random variable $V$ with differential entropy $h(V)$ is defined by the expression:\n$$N(V) = \\frac{1}{2\\pi e} \\exp(2h(V))$$\nwhere $e$ is the base of the natural logarithm and $\\pi$ is the mathematical constant.\n\nCalculate the entropy power of the total noise voltage, $N(Z)$. Express your answer in units of $\\text{V}^2$.", "solution": "Let $X$ and $Y$ be independent, zero-mean Gaussian random variables with variances $\\sigma_{X}^{2}=3.5$ and $\\sigma_{Y}^{2}=5.2$ (in units of $\\text{V}^{2}$). The sum $Z=X+Y$ is Gaussian by the closure of the Gaussian family under addition, and independence implies variance additivity:\n$$\n\\sigma_{Z}^{2}=\\operatorname{Var}(Z)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)=\\sigma_{X}^{2}+\\sigma_{Y}^{2}.\n$$\n\nThe differential entropy of a univariate Gaussian random variable with variance $\\sigma^{2}$ is\n$$\nh(Z)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{Z}^{2}\\big).\n$$\nBy the definition of entropy power,\n$$\nN(Z)=\\frac{1}{2\\pi e}\\exp\\!\\big(2h(Z)\\big)=\\frac{1}{2\\pi e}\\exp\\!\\left(2\\cdot\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{Z}^{2}\\big)\\right)=\\frac{1}{2\\pi e}\\exp\\!\\left(\\ln\\!\\big(2\\pi e\\,\\sigma_{Z}^{2}\\big)\\right)=\\sigma_{Z}^{2}.\n$$\nTherefore, for Gaussian $Z$, the entropy power equals its variance. Substituting the given variances,\n$$\n\\sigma_{Z}^{2}=3.5+5.2=8.7,\n$$\nso $N(Z)=8.7$ in units of $\\text{V}^{2}$.", "answer": "$$\\boxed{8.7}$$", "id": "1620997"}, {"introduction": "While Gaussian variables represent the case of equality in the EPI, most other distributions result in a strict inequality. This practice [@problem_id:1620980] delves into the non-Gaussian world by considering the sum of two independent uniform random variables. Your task is to calculate the \"entropy power gap,\" the amount by which $N(X+Y)$ exceeds $N(X) + N(Y)$, thus actively demonstrating the \"inequality\" part of the EPI for a tangible case.", "problem": "In information theory, the concept of entropy provides a measure of uncertainty for a random variable. For a continuous random variable $A$ with a probability density function (PDF) $f_A(a)$, the differential entropy is defined as $h(A) = -\\int_{-\\infty}^{\\infty} f_A(a) \\ln(f_A(a)) da$.\n\nFrom this, one can define the entropy power of the random variable, denoted $N(A)$, which represents the variance of a Gaussian random variable having the same differential entropy as $A$. The formula for entropy power is $N(A) = \\frac{1}{2\\pi e} \\exp(2h(A))$.\n\nA fundamental result connecting the entropies of sums of independent random variables is the Entropy Power Inequality (EPI). For two independent random variables $X$ and $Y$, the EPI states that the entropy power of their sum is at least the sum of their individual entropy powers: $N(X+Y) \\ge N(X) + N(Y)$.\n\nThe equality holds if and only if $X$ and $Y$ are Gaussian random variables. For non-Gaussian variables, there is a strict inequality, leading to an \"entropy power gap\". This gap, $\\Delta N$, is the amount by which the entropy power of the sum exceeds the sum of the individual entropy powers: $\\Delta N = N(X+Y) - (N(X) + N(Y))$.\n\nConsider two independent and identically distributed (i.i.d.) random variables, $X$ and $Y$, where each follows a uniform distribution on the interval $[0, 1]$. Let their sum be $Z=X+Y$.\n\nCalculate the exact value of the entropy power gap, $\\Delta N$, for this specific case. Your final answer should be a closed-form analytic expression in terms of mathematical constants like $\\pi$ and $e$.", "solution": "We are given i.i.d. random variables $X$ and $Y$, each uniform on $[0,1]$, and we define $Z=X+Y$. The entropy power of a continuous random variable $A$ with differential entropy $h(A)$ is\n$$\nN(A)=\\frac{1}{2\\pi e}\\exp\\!\\left(2h(A)\\right).\n$$\nThe entropy power gap is\n$$\n\\Delta N = N(X+Y) - \\left(N(X)+N(Y)\\right).\n$$\n\nFirst, compute the differential entropy of $X$ and $Y$. Since $X\\sim\\mathrm{Unif}[0,1]$, its PDF is $f_{X}(x)=1$ for $x\\in[0,1]$ and $0$ otherwise. Therefore,\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} f_{X}(x)\\ln f_{X}(x)\\,dx = -\\int_{0}^{1} 1\\cdot \\ln(1)\\,dx = 0.\n$$\nBy identical distribution, $h(Y)=0$. Hence,\n$$\nN(X) = N(Y) = \\frac{1}{2\\pi e}\\exp\\!\\left(2\\cdot 0\\right)=\\frac{1}{2\\pi e}.\n$$\n\nNext, compute $h(Z)$. The PDF of $Z=X+Y$ is the triangular density obtained by convolution:\n$$\nf_{Z}(z) = \\begin{cases}\nz, & 0\\le z \\le 1,\\\\\n2-z, & 1< z \\le 2,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThus,\n$$\nh(Z) = -\\int_{-\\infty}^{\\infty} f_{Z}(z)\\ln f_{Z}(z)\\,dz\n= -\\left(\\int_{0}^{1} z\\ln z\\,dz + \\int_{1}^{2} (2-z)\\ln(2-z)\\,dz\\right).\n$$\nIn the second integral, substitute $u=2-z$, so $du=-dz$ and the limits map as $z=1\\mapsto u=1$, $z=2\\mapsto u=0$. Then\n$$\n\\int_{1}^{2} (2-z)\\ln(2-z)\\,dz = \\int_{1}^{0} u\\ln u\\,(-du) = \\int_{0}^{1} u\\ln u\\,du.\n$$\nTherefore,\n$$\nh(Z) = -2\\int_{0}^{1} t\\ln t\\,dt.\n$$\nUsing the standard integral $\\int_{0}^{1} t^{\\alpha}\\ln t\\,dt = -\\frac{1}{(\\alpha+1)^{2}}$ for $\\alpha>-1$, with $\\alpha=1$, we get\n$$\n\\int_{0}^{1} t\\ln t\\,dt = -\\frac{1}{4},\n$$\nso\n$$\nh(Z) = -2\\left(-\\frac{1}{4}\\right)=\\frac{1}{2}.\n$$\nThen the entropy power of $Z$ is\n$$\nN(Z) = \\frac{1}{2\\pi e}\\exp\\!\\left(2h(Z)\\right) = \\frac{1}{2\\pi e}\\exp(1) = \\frac{1}{2\\pi}.\n$$\n\nFinally, the entropy power gap is\n$$\n\\Delta N = N(Z) - \\left(N(X)+N(Y)\\right) = \\frac{1}{2\\pi} - 2\\cdot\\frac{1}{2\\pi e}\n= \\frac{1}{2\\pi} - \\frac{1}{\\pi e}\n= \\frac{e-2}{2\\pi e}.\n$$\nThis is strictly positive since $e>2$, consistent with the strict EPI inequality for non-Gaussian $X$ and $Y$.", "answer": "$$\\boxed{\\frac{e-2}{2\\pi e}}$$", "id": "1620980"}, {"introduction": "A critical requirement for the Entropy Power Inequality is the statistical independence of the random variables. This exercise challenges you to explore what happens when this assumption is violated. By constructing a specific pair of *dependent* Gaussian variables [@problem_id:1621044], you will discover that the inequality no longer holds, providing a powerful illustration of why the independence condition is not just a technicality but is essential to the theorem's validity.", "problem": "In information theory, the Entropy Power Inequality (EPI) provides a fundamental limit on the entropy of the sum of random variables. This problem explores a scenario where the standard conditions for the EPI are not met.\n\nConsider two independent Gaussian random variables, $A$ and $B$. The variable $A$ is drawn from a normal distribution with mean 0 and variance $\\sigma_A^2 = 1$, which we denote as $A \\sim \\mathcal{N}(0, 1)$. The variable $B$ is drawn from a normal distribution with mean 0 and variance $\\sigma_B^2 = 3$, denoted as $B \\sim \\mathcal{N}(0, 3)$.\n\nFrom these underlying variables, we construct two new random variables, $X$ and $Y$, as linear combinations:\n$$X = A + B$$\n$$Y = A - B$$\nNote that $X$ and $Y$ are not independent.\n\nThe entropy power of a continuous random variable $Z$ with differential entropy $h(Z)$ is defined as $N(Z) = \\frac{1}{2\\pi e}\\exp(2h(Z))$. For any Gaussian random variable $W$ with distribution $\\mathcal{N}(\\mu, \\sigma^2)$, its differential entropy is given by the formula $h(W) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)$.\n\nYour task is to calculate the specific value of the ratio $\\frac{N(X+Y)}{N(X) + N(Y)}$ for the variables $X$ and $Y$ defined above. Express your answer as an exact fraction in simplest form.", "solution": "We are given independent Gaussian random variables $A \\sim \\mathcal{N}(0,1)$ and $B \\sim \\mathcal{N}(0,3)$, and define $X=A+B$ and $Y=A-B$. Since $X$ and $Y$ are linear combinations of jointly Gaussian variables, each is Gaussian. Compute their second-order moments:\n$$\n\\operatorname{Var}(X)=\\operatorname{Var}(A)+\\operatorname{Var}(B)=1+3=4,\n$$\n$$\n\\operatorname{Var}(Y)=\\operatorname{Var}(A)+\\operatorname{Var}(B)=4,\n$$\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(A+B,A-B)=\\operatorname{Var}(A)-\\operatorname{Var}(B)=1-3=-2,\n$$\nso $X$ and $Y$ are not independent.\n\nNext, observe that\n$$\nX+Y=(A+B)+(A-B)=2A,\n$$\nhence $X+Y$ is Gaussian with\n$$\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(2A)=4\\operatorname{Var}(A)=4.\n$$\nEquivalently, using joint moments,\n$$\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)+2\\,\\operatorname{Cov}(X,Y)=4+4+2(-2)=4,\n$$\nconfirming the same result.\n\nFor any Gaussian $W \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, its differential entropy is $h(W)=\\frac{1}{2}\\ln(2\\pi e \\sigma^{2})$, and the entropy power is\n$$\nN(W)=\\frac{1}{2\\pi e}\\exp\\bigl(2h(W)\\bigr)=\\frac{1}{2\\pi e}\\exp\\!\\bigl(\\ln(2\\pi e \\sigma^{2})\\bigr)=\\sigma^{2}.\n$$\nTherefore, for these Gaussian variables,\n$$\nN(X)=\\operatorname{Var}(X)=4,\\quad N(Y)=\\operatorname{Var}(Y)=4,\\quad N(X+Y)=\\operatorname{Var}(X+Y)=4.\n$$\nThe requested ratio is\n$$\n\\frac{N(X+Y)}{N(X)+N(Y)}=\\frac{4}{4+4}=\\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1621044"}]}