{"hands_on_practices": [{"introduction": "Marton's coding scheme carefully balances the information sent to each user, and a key part of its performance analysis is understanding the statistical dependence between the auxiliary variables. Before exploring complex correlations, this first exercise grounds us in the fundamentals by calculating the mutual information $I(X_1; X_2)$ between two completely independent information streams [@problem_id:1639343]. This calculation establishes a crucial baseline for the more general case and clarifies the meaning of the correction term, $I(U_1; U_2)$, in the sum-rate bound.", "problem": "In the study of information theory, foundational models for communication systems, such as broadcast channels, often involve analyzing the statistical relationship between different information streams. Consider two such streams represented by a pair of discrete random variables $(X_1, X_2)$. Let both $X_1$ and $X_2$ be independent and identically distributed (i.i.d.) Bernoulli random variables, each with a probability of success $p = 1/2$. This means that for each variable, the outcomes $\\{0, 1\\}$ are equally likely.\n\nCalculate the mutual information $I(X_1; X_2)$ between these two variables. Provide the answer as a single real number in units of bits.", "solution": "We are given two discrete random variables $X_{1}$ and $X_{2}$ that are independent and identically distributed Bernoulli with parameter $p=\\frac{1}{2}$. Independence implies the joint pmf factorizes as $p_{X_{1},X_{2}}(x_{1},x_{2})=p_{X_{1}}(x_{1})p_{X_{2}}(x_{2})$ for all $x_{1},x_{2}\\in\\{0,1\\}$.\n\nThe mutual information between $X_{1}$ and $X_{2}$ (in bits) is defined by\n$$\nI(X_{1};X_{2})=\\sum_{x_{1}}\\sum_{x_{2}} p_{X_{1},X_{2}}(x_{1},x_{2}) \\log_{2}\\!\\left(\\frac{p_{X_{1},X_{2}}(x_{1},x_{2})}{p_{X_{1}}(x_{1})p_{X_{2}}(x_{2})}\\right).\n$$\nUsing independence, for every $(x_{1},x_{2})$ with nonzero probability,\n$$\n\\frac{p_{X_{1},X_{2}}(x_{1},x_{2})}{p_{X_{1}}(x_{1})p_{X_{2}}(x_{2})}=1 \\quad \\Rightarrow \\quad \\log_{2}(1)=0,\n$$\nso every summand is zero and therefore\n$$\nI(X_{1};X_{2})=0.\n$$\nEquivalently, using the entropy identity $I(X_{1};X_{2})=H(X_{1})+H(X_{2})-H(X_{1},X_{2})$ and the fact that independence gives $H(X_{1},X_{2})=H(X_{1})+H(X_{2})$, we again obtain $I(X_{1};X_{2})=0$.", "answer": "$$\\boxed{0}$$", "id": "1639343"}, {"introduction": "With a solid grasp of mutual information, we can now apply it to determine achievable communication rates for a broadcast channel. This practice problem models a common scenario where information sent to two users can be subject to erasures, analogous to packet loss in a data network [@problem_id:1639326]. By calculating the maximum symmetric rate for this channel under a simple independent input strategy, you will practice using Marton's bounds to evaluate the performance limits of a communication system.", "problem": "Consider a two-user Discrete Memoryless Broadcast Channel (DM-BC) where a transmitter sends independent information to two separate receivers. The transmitter uses two independent binary inputs, $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{0, 1\\}$, to encode the message for receiver 1 and receiver 2, respectively. The inputs are drawn from independent and uniform distributions, such that $P(X_1=i) = 0.5$ and $P(X_2=j) = 0.5$ for all $i, j \\in \\{0, 1\\}$.\n\nThe channel is characterized by its outputs $Y_1$ and $Y_2$ which belong to the alphabet $\\{0, 1, \\text{e}\\}$, where 'e' denotes an erasure. The channel's behavior is governed by a random state variable $S$ which is independent of the inputs. The state $S$ can take one of three values $\\{s_1, s_2, s_3\\}$ with the following probabilities: $P(S=s_1) = 0.3$, $P(S=s_2) = 0.5$, and $P(S=s_3) = 0.2$. The channel outputs are determined by the inputs and the state as follows:\n- If $S=s_1$, then $Y_1 = X_1$ and $Y_2 = \\text{e}$.\n- If $S=s_2$, then $Y_1 = \\text{e}$ and $Y_2 = X_2$.\n- If $S=s_3$, then $Y_1 = X_1$ and $Y_2 = X_2$.\n\nAn achievable rate region for this channel can be found using a simplified version of Marton's coding framework, where the auxiliary random variables are set to be the channel inputs themselves, i.e., $U_1 = X_1$ and $U_2 = X_2$. Given that the inputs $X_1$ and $X_2$ are independent, this leads to an achievable rate region defined by the inequalities:\n$R_1 \\le I(X_1; Y_1)$\n$R_2 \\le I(X_2; Y_2)$\n\nDetermine the maximum achievable symmetric rate $R = R_1 = R_2$ for this channel. Express your answer in bits per channel use, rounded to four significant figures.", "solution": "The problem asks for the maximum achievable symmetric rate $R = R_1 = R_2$ for a given broadcast channel. The problem states that the achievable rates must satisfy the conditions $R_1 \\le I(X_1; Y_1)$ and $R_2 \\le I(X_2; Y_2)$. For a symmetric rate $R$, both conditions must hold, so we must have:\n$R \\le I(X_1; Y_1)$\n$R \\le I(X_2; Y_2)$\n\nThe maximum such $R$ is therefore the minimum of these two mutual information values: $R = \\min(I(X_1; Y_1), I(X_2; Y_2))$.\nOur task is to compute these two mutual information terms. The inputs $X_1$ and $X_2$ are independent and uniformly distributed Bernoulli random variables ($P(X_k=0)=P(X_k=1)=0.5$).\n\n**Step 1: Calculate $I(X_1; Y_1)$**\n\nThe mutual information is defined as $I(X_1; Y_1) = H(Y_1) - H(Y_1|X_1)$. Let's compute each term.\n\nFirst, we find the conditional entropy $H(Y_1|X_1)$. This is the entropy of the output $Y_1$ when the input $X_1$ is known. Let's find the conditional probability distribution $p(y_1|x_1)$.\n- The event $Y_1 = X_1$ occurs if the channel state is $S=s_1$ or $S=s_3$. The probability of this is $P(S=s_1) + P(S=s_3) = 0.3 + 0.2 = 0.5$.\n- The event $Y_1 = \\text{e}$ (erasure) occurs if the channel state is $S=s_2$. The probability of this is $P(S=s_2) = 0.5$.\n- The event $Y_1 = 1-X_1$ (an error) never occurs.\nSo, for a given $x_1$, the output $Y_1$ is either $x_1$ with probability $0.5$ or 'e' with probability $0.5$.\nThe conditional entropy $H(Y_1|X_1=x_1)$ is the entropy of this distribution:\n$$H(Y_1|X_1=x_1) = -(0.5 \\log_2(0.5) + 0.5 \\log_2(0.5)) = - \\log_2(0.5) = 1 \\text{ bit}$$\nSince this value is the same for $X_1=0$ and $X_1=1$, the average conditional entropy is $H(Y_1|X_1) = 1$ bit.\n\nAlternatively, this channel from $X_1$ to $Y_1$ is a Binary Erasure Channel (BEC) with erasure probability $p_e = P(S=s_2) = 0.5$. The information $I(X_1; Y_1)$ for a uniform input into a BEC is $1-p_e$. Let's verify this using the full formula.\n\nNext, we find the entropy of the output $H(Y_1)$. We need the marginal probability distribution $p(y_1)$.\n- $P(Y_1=\\text{e}) = P(S=s_2) = 0.5$.\n- $P(Y_1=0) = P(Y_1=0|X_1=0)P(X_1=0) + P(Y_1=0|X_1=1)P(X_1=1)$. Since $P(Y_1=0|X_1=1)=0$, this is $P(Y_1=0) = P(Y_1=X_1|X_1=0)P(X_1=0) = (0.5)(0.5) = 0.25$.\n- $P(Y_1=1) = P(Y_1=1|X_1=1)P(X_1=1) + P(Y_1=1|X_1=0)P(X_1=0)$. Since $P(Y_1=1|X_1=0)=0$, this is $P(Y_1=1) = P(Y_1=X_1|X_1=1)P(X_1=1) = (0.5)(0.5) = 0.25$.\nThe distribution of $Y_1$ is $(P(Y_1=0), P(Y_1=1), P(Y_1=\\text{e})) = (0.25, 0.25, 0.5)$.\nThe entropy is:\n$$H(Y_1) = -(0.25 \\log_2(0.25) + 0.25 \\log_2(0.25) + 0.5 \\log_2(0.5))$$\n$$H(Y_1) = -(2 \\times 0.25 \\times (-2) + 0.5 \\times (-1)) = -( -1 + (-0.5) ) = 1.5 \\text{ bits}$$\nNow, we can compute the mutual information:\n$$I(X_1; Y_1) = H(Y_1) - H(Y_1|X_1) = 1.5 - 1 = 0.5 \\text{ bits}$$\nAs noted before, for a BEC with erasure probability $p_e=0.5$ and uniform input, the capacity is $1-p_e = 1-0.5=0.5$ bits.\n\n**Step 2: Calculate $I(X_2; Y_2)$**\n\nThe calculation is symmetric. The channel from $X_2$ to $Y_2$ is also a BEC.\n- The event $Y_2 = X_2$ occurs if the channel state is $S=s_2$ or $S=s_3$. The probability is $P(S=s_2) + P(S=s_3) = 0.5 + 0.2 = 0.7$.\n- The event $Y_2 = \\text{e}$ (erasure) occurs if the channel state is $S=s_1$. The probability is $P(S=s_1) = 0.3$.\nThis corresponds to a BEC with erasure probability $p'_e = 0.3$.\nWith a uniform input $X_2$, the mutual information is:\n$$I(X_2; Y_2) = 1 - p'_e = 1 - 0.3 = 0.7 \\text{ bits}$$\n\n**Step 3: Determine the maximum symmetric rate**\n\nThe maximum achievable symmetric rate is the minimum of the two individual rates:\n$$R = \\min(I(X_1; Y_1), I(X_2; Y_2)) = \\min(0.5, 0.7) = 0.5 \\text{ bits per channel use}$$\n\nThe problem requires the answer to be rounded to four significant figures.\n$$R = 0.5000 \\text{ bits per channel use}$$", "answer": "$$\\boxed{0.5000}$$", "id": "1639326"}, {"introduction": "Building on the previous exercises, this problem challenges you to apply the same principles to a more abstract broadcast channel with a non-binary input alphabet. Here, the channel input is determined by a specific logical mapping from two underlying auxiliary variables, a common technique in advanced code design [@problem_id:1639344]. This practice requires a careful, step-by-step application of the definitions of entropy and mutual information to uncover the channel's capacity limits for each user.", "problem": "Consider a discrete memoryless Broadcast Channel (BC) with a ternary input alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ and two binary output alphabets $\\mathcal{Y}_1 = \\mathcal{Y}_2 = \\{0, 1\\}$. The channel is characterized by the conditional probability distribution $p(y_1, y_2 | x)$, defined as follows for given success probabilities $p, q \\in (0,1)$:\n- If input $x=0$, the output is $(y_1, y_2) = (0, 0)$ with probability 1.\n- If input $x=1$, the output is $(y_1, y_2) = (1, 0)$ with probability $p$, and $(y_1, y_2) = (0, 0)$ with probability $1-p$.\n- If input $x=2$, the output is $(y_1, y_2) = (0, 1)$ with probability $q$, and $(y_1, y_2) = (0, 0)$ with probability $1-q$.\n- All other conditional probabilities $p(y_1, y_2 | x)$ are zero.\n\nTo communicate over this channel, a coding scheme based on Marton's inner bound is used. The input distribution is constructed using two independent auxiliary random variables, $U_1$ and $U_2$. Both $U_1$ and $U_2$ are uniformly distributed over the alphabet $\\{A, B\\}$. The channel input $X$ is then determined as a deterministic function of the pair $(U_1, U_2)$:\n- $X=1$ if $(U_1, U_2) = (B, A)$.\n- $X=2$ if $(U_1, U_2) = (A, B)$.\n- $X=0$ otherwise (i.e., if $(U_1, U_2) = (A, A)$ or $(U_1, U_2) = (B, B)$).\n\nFor the specified channel and this input distribution, Marton's theory defines an achievable rate region for the rate pair $(R_1, R_2)$. Your task is to determine the maximum achievable rate for user 1, denoted $R_{1,\\max}$, and the maximum achievable rate for user 2, denoted $R_{2,\\max}$. These correspond to the rate on one axis when the rate on the other is zero.\n\nExpress your answers for $R_{1,\\max}$ and $R_{2,\\max}$ in terms of the parameters $p$ and $q$. Your expressions may use the binary entropy function, defined as $H_b(z) = -z \\log_2(z) - (1-z)\\log_2(1-z)$ for $z \\in (0,1)$, with $H_b(0) = H_b(1)=0$. All logarithms are base 2. Present your final answer as a pair $(R_{1, \\max}, R_{2, \\max})$.", "solution": "By Marton’s inner bound with independent auxiliaries and a deterministic encoder $X=f(U_{1},U_{2})$, the achievable region contains all rate pairs satisfying\n$$\nR_{1} \\leq I(U_{1};Y_{1}), \\quad R_{2} \\leq I(U_{2};Y_{2}), \\quad R_{1}+R_{2} \\leq I(U_{1};Y_{1})+I(U_{2};Y_{2})-I(U_{1};U_{2}).\n$$\nSince $U_{1}$ and $U_{2}$ are independent, $I(U_{1};U_{2})=0$, so the sum constraint is redundant. Therefore, the axis intercepts are\n$$\nR_{1,\\max}=I(U_{1};Y_{1}), \\qquad R_{2,\\max}=I(U_{2};Y_{2}).\n$$\n\nWe now compute $I(U_{1};Y_{1})$ for the given mapping and channel. The mapping is\n- $(U_{1},U_{2})=(A,A)\\ \\Rightarrow\\ X=0$,\n- $(U_{1},U_{2})=(B,B)\\ \\Rightarrow\\ X=0$,\n- $(U_{1},U_{2})=(B,A)\\ \\Rightarrow\\ X=1$,\n- $(U_{1},U_{2})=(A,B)\\ \\Rightarrow\\ X=2$,\nwith each pair having probability $\\frac{1}{4}$. The channel yields $Y_{1}=1$ only when $X=1$ and that occurs with success probability $p$; otherwise $Y_{1}=0$.\n\nCondition on $U_{1}$:\n- If $U_{1}=A$, then $(U_{1},U_{2})$ is either $(A,A)$ or $(A,B)$, which map to $X=0$ or $X=2$, both giving $Y_{1}=0$. Hence\n$$\nP(Y_{1}=1 \\mid U_{1}=A)=0,\\quad P(Y_{1}=0 \\mid U_{1}=A)=1,\\quad H(Y_{1}\\mid U_{1}=A)=H_{b}(0)=0.\n$$\n- If $U_{1}=B$, then $(U_{1},U_{2})$ is either $(B,A)$ or $(B,B)$, which map to $X=1$ or $X=0$. Given $U_{1}=B$, $U_{2}=A$ with probability $\\frac{1}{2}$, yielding $X=1$ and $Y_{1}=1$ with probability $p$; otherwise $Y_{1}=0$. Hence\n$$\nP(Y_{1}=1 \\mid U_{1}=B)=\\frac{1}{2}p,\\quad P(Y_{1}=0 \\mid U_{1}=B)=1-\\frac{1}{2}p,\\quad H(Y_{1}\\mid U_{1}=B)=H_{b}\\!\\left(\\frac{p}{2}\\right).\n$$\nAveraging over $U_{1}$ (which is uniform),\n$$\nH(Y_{1}\\mid U_{1})=\\frac{1}{2}H_{b}(0)+\\frac{1}{2}H_{b}\\!\\left(\\frac{p}{2}\\right)=\\frac{1}{2}H_{b}\\!\\left(\\frac{p}{2}\\right).\n$$\nThe marginal of $Y_{1}$ is\n$$\nP(Y_{1}=1)=P(X=1)\\,p=\\frac{1}{4}p,\\quad P(Y_{1}=0)=1-\\frac{p}{4},\n$$\nso\n$$\nH(Y_{1})=H_{b}\\!\\left(\\frac{p}{4}\\right).\n$$\nTherefore,\n$$\nR_{1,\\max}=I(U_{1};Y_{1})=H(Y_{1})-H(Y_{1}\\mid U_{1})=H_{b}\\!\\left(\\frac{p}{4}\\right)-\\frac{1}{2}H_{b}\\!\\left(\\frac{p}{2}\\right).\n$$\n\nBy symmetry, $Y_{2}=1$ only when $X=2$ and that occurs with success probability $q$. Repeating the same steps for $U_{2}$ and $Y_{2}$ gives\n- $P(Y_{2}=1 \\mid U_{2}=A)=0$, so $H(Y_{2}\\mid U_{2}=A)=0$,\n- $P(Y_{2}=1 \\mid U_{2}=B)=\\frac{1}{2}q$, so $H(Y_{2}\\mid U_{2}=B)=H_{b}\\!\\left(\\frac{q}{2}\\right)$,\n- $H(Y_{2}\\mid U_{2})=\\frac{1}{2}H_{b}\\!\\left(\\frac{q}{2}\\right)$,\n- $P(Y_{2}=1)=\\frac{1}{4}q$, hence $H(Y_{2})=H_{b}\\!\\left(\\frac{q}{4}\\right)$.\n\nThus,\n$$\nR_{2,\\max}=I(U_{2};Y_{2})=H_{b}\\!\\left(\\frac{q}{4}\\right)-\\frac{1}{2}H_{b}\\!\\left(\\frac{q}{2}\\right).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} H_{b}\\!\\left(\\frac{p}{4}\\right)-\\frac{1}{2}H_{b}\\!\\left(\\frac{p}{2}\\right) & H_{b}\\!\\left(\\frac{q}{4}\\right)-\\frac{1}{2}H_{b}\\!\\left(\\frac{q}{2}\\right) \\end{pmatrix}}$$", "id": "1639344"}]}