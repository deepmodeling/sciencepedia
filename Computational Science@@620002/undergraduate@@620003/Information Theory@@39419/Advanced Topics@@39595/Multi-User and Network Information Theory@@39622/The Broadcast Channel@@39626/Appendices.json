{"hands_on_practices": [{"introduction": "In many practical communication scenarios, a signal travels to multiple receivers such that one receiver's observation is a further-degraded version of another's. A simple model for this is a cascade of processing stages where noise is added at each step. This exercise [@problem_id:1662955] formalizes this physical intuition, showing how such a setup naturally leads to the statistical structure of a Markov chain, which is the defining property of a physically degraded broadcast channel.", "problem": "In a simplified model of a two-stage signal processing cascade, an initial discrete signal, represented by the random variable $X$, is transmitted. The signal passes through the first processing stage, which adds noise. The output of this stage is the random variable $Y_1$, defined by the relation $Y_1 = X + Z_1$. This intermediate signal $Y_1$ is then fed into a second processing stage, which introduces additional noise. The final output signal is the random variable $Y_2$, defined by the relation $Y_2 = Y_1 + Z_2$. The initial signal $X$ and the noise terms $Z_1$ and $Z_2$ are all mutually independent random variables.\n\nWhich of the following statements accurately describes the statistical relationship among the random variables $X$, $Y_1$, and $Y_2$?\n\nA. $X \\to Y_2 \\to Y_1$ forms a Markov chain.\n\nB. $X, Y_1$, and $Y_2$ are mutually independent.\n\nC. The variables are related by the Markov chain $Y_1 \\to X \\to Y_2$.\n\nD. $X$ and $Y_2$ are independent.\n\nE. $X \\to Y_1 \\to Y_2$ forms a Markov chain.", "solution": "Let $X$, $Z_{1}$, and $Z_{2}$ be mutually independent random variables, and define $Y_{1} = X + Z_{1}$ and $Y_{2} = Y_{1} + Z_{2} = X + Z_{1} + Z_{2}$. We analyze the conditional distributions implied by these relations to determine the correct statistical structure.\n\nBy definition, a Markov chain $U \\to V \\to W$ holds if and only if $W$ is conditionally independent of $U$ given $V$, i.e., $p_{W|U,V}(w|u,v) = p_{W|V}(w|v)$ for all values.\n\nTo test $X \\to Y_{1} \\to Y_{2}$, compute the conditional distribution:\n$$\np_{Y_{2}|Y_{1},X}(y_{2}|y_{1},x) = p_{Z_{2}}(y_{2} - y_{1}),\n$$\nsince $Y_{2} = y_{1} + Z_{2}$ and $Z_{2}$ is independent of $(X, Z_{1})$ and hence independent of $Y_{1}$. Therefore,\n$$\np_{Y_{2}|Y_{1},X}(y_{2}|y_{1},x) = p_{Z_{2}}(y_{2} - y_{1}) = p_{Y_{2}|Y_{1}}(y_{2}|y_{1}),\n$$\nwhich verifies the Markov property $X \\to Y_{1} \\to Y_{2}$. Thus option E is true.\n\nWe now show that the other options are false.\n\nFor A: $X \\to Y_{2} \\to Y_{1}$ would require $p_{Y_{1}|X,Y_{2}}(y_{1}|x,y_{2}) = p_{Y_{1}|Y_{2}}(y_{1}|y_{2})$. Using the independence structure and Bayes’ rule,\n$$\np_{Y_{1}|X,Y_{2}}(y_{1}|x,y_{2}) \\propto p_{Z_{1}}(y_{1} - x)\\,p_{Z_{2}}(y_{2} - y_{1}),\n$$\nwhereas\n$$\np_{Y_{1}|Y_{2}}(y_{1}|y_{2}) \\propto \\sum_{x'} p_{X}(x')\\,p_{Z_{1}}(y_{1} - x')\\,p_{Z_{2}}(y_{2} - y_{1})\n$$\nfor discrete $X$ (or the analogous integral for continuous $X$). These cannot be equal for all $x$ unless $X$ is degenerate, so A is false in general.\n\nFor C: $Y_{1} \\to X \\to Y_{2}$ would require $p_{Y_{2}|X,Y_{1}}(y_{2}|x,y_{1}) = p_{Y_{2}|X}(y_{2}|x)$. However,\n$$\np_{Y_{2}|X,Y_{1}}(y_{2}|x,y_{1}) = p_{Z_{2}}(y_{2} - y_{1}),\n$$\nwhile\n$$\np_{Y_{2}|X}(y_{2}|x) = p_{Z_{1}+Z_{2}}(y_{2} - x),\n$$\nwhich are generally different (a single-noise shift versus a convolution), so C is false unless $Z_{1}$ is almost surely zero, which is not assumed.\n\nFor D: $X$ and $Y_{2}$ are independent would require $p_{Y_{2}|X}(y_{2}|x) = p_{Y_{2}}(y_{2})$ for all $x$. But\n$$\np_{Y_{2}|X}(y_{2}|x) = p_{Z_{1}+Z_{2}}(y_{2} - x),\n$$\nwhich depends on $x$ unless $X$ is degenerate. Hence D is false in general.\n\nFor B: Mutual independence fails already because $Y_{1} = X + Z_{1}$ implies\n$$\np_{Y_{1}|X}(y_{1}|x) = p_{Z_{1}}(y_{1} - x),\n$$\nwhich depends on $x$, so $X$ and $Y_{1}$ are not independent. Therefore B is false.\n\nThe only correct statement is $X \\to Y_{1} \\to Y_{2}$.", "answer": "$$\\boxed{E}$$", "id": "1662955"}, {"introduction": "While the concept of a degraded channel is intuitive, a rigorous test is needed to classify any given broadcast channel. This practice [@problem_id:1617320] challenges you to apply the formal definition of a degraded channel by examining the conditional probabilities derived from a channel's transition matrix. This hands-on analysis will not only reinforce the underlying mathematical conditions but also reveal the existence of non-degraded channels, which require more sophisticated coding strategies.", "problem": "In information theory, a broadcast channel (BC) models a scenario where one sender, with input alphabet $\\mathcal{X}$, transmits a signal to multiple receivers, with respective output alphabets $\\mathcal{Y}_1, \\mathcal{Y}_2, \\dots$. A discrete memoryless BC is characterized by the conditional probability distribution $p(y_1, y_2, \\dots | x)$.\n\nConsider a binary-input, binary-output BC with input alphabet $\\mathcal{X} = \\{0, 1\\}$ and two output alphabets $\\mathcal{Y}_1 = \\{0, 1\\}$ and $\\mathcal{Y}_2 = \\{0, 1\\}$. The channel is defined by the transition probability $p(y_1, y_2|x)$.\n\nA BC is defined as **degraded** if one receiver's output is a statistically noisier version of the other's. Formally, this occurs if one of the following two Markov chain conditions is met:\n1.  $X \\to Y_1 \\to Y_2$: This implies that for any choice of $(x, y_1, y_2)$, the conditional probability $p(y_2 | x, y_1)$ is independent of $x$ whenever $p(x, y_1) > 0$.\n2.  $X \\to Y_2 \\to Y_1$: This implies that for any choice of $(x, y_1, y_2)$, the conditional probability $p(y_1 | x, y_2)$ is independent of $x$ whenever $p(x, y_2) > 0$.\n\nA channel is **non-degraded** if it is not degraded, meaning neither of the two Markov chain conditions above is satisfied.\n\nFour different broadcast channels (A, B, C, D) are defined below by their transition probability matrices. For each channel, two matrices are given: $M_0$ for input $x=0$ and $M_1$ for input $x=1$. The entries of these matrices are defined as $(M_x)_{i,j} = p(y_1=i, y_2=j|x)$ for $i,j \\in \\{0, 1\\}$.\n\nWhich of the following channels is a non-degraded broadcast channel?\n\nA.\n$$M_0 = \\begin{pmatrix} 1/2 & 1/4 \\\\ 1/12 & 1/6 \\end{pmatrix}, \\quad M_1 = \\begin{pmatrix} 1/6 & 1/12 \\\\ 1/4 & 1/2 \\end{pmatrix}$$\n\nB.\n$$M_0 = \\begin{pmatrix} 1/2 & 1/12 \\\\ 1/4 & 1/6 \\end{pmatrix}, \\quad M_1 = \\begin{pmatrix} 1/6 & 1/4 \\\\ 1/12 & 1/2 \\end{pmatrix}$$\n\nC.\n$$M_0 = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/2 \\end{pmatrix}, \\quad M_1 = \\begin{pmatrix} 0 & 1/2 \\\\ 1/2 & 0 \\end{pmatrix}$$\n\nD.\n$$M_0 = \\begin{pmatrix} 2/3 & 1/3 \\\\ 0 & 0 \\end{pmatrix}, \\quad M_1 = \\begin{pmatrix} 0 & 0 \\\\ 1/3 & 2/3 \\end{pmatrix}$$", "solution": "A discrete memoryless broadcast channel is degraded if either $X \\to Y_{1} \\to Y_{2}$ holds, i.e., $p(y_{2}\\mid x,y_{1})$ is independent of $x$ whenever $p(x,y_{1})>0$, or $X \\to Y_{2} \\to Y_{1}$ holds, i.e., $p(y_{1}\\mid x,y_{2})$ is independent of $x$ whenever $p(x,y_{2})>0$. We test each channel by computing the relevant conditional distributions from the given joint probabilities $p(y_{1},y_{2}\\mid x)$.\n\nChannel A:\nFor $x=0$, row sums are $p(y_{1}=0\\mid 0)=\\frac{1}{2}+\\frac{1}{4}=\\frac{3}{4}$ and $p(y_{1}=1\\mid 0)=\\frac{1}{12}+\\frac{1}{6}=\\frac{1}{4}$. Thus\n$$\np(y_{2}\\mid x=0,y_{1}=0)=\\left(\\frac{\\frac{1}{2}}{\\frac{3}{4}},\\frac{\\frac{1}{4}}{\\frac{3}{4}}\\right)=\\left(\\frac{2}{3},\\frac{1}{3}\\right),\\quad\np(y_{2}\\mid x=0,y_{1}=1)=\\left(\\frac{\\frac{1}{12}}{\\frac{1}{4}},\\frac{\\frac{1}{6}}{\\frac{1}{4}}\\right)=\\left(\\frac{1}{3},\\frac{2}{3}\\right).\n$$\nFor $x=1$, row sums are $p(y_{1}=0\\mid 1)=\\frac{1}{6}+\\frac{1}{12}=\\frac{1}{4}$ and $p(y_{1}=1\\mid 1)=\\frac{1}{4}+\\frac{1}{2}=\\frac{3}{4}$. Thus\n$$\np(y_{2}\\mid x=1,y_{1}=0)=\\left(\\frac{\\frac{1}{6}}{\\frac{1}{4}},\\frac{\\frac{1}{12}}{\\frac{1}{4}}\\right)=\\left(\\frac{2}{3},\\frac{1}{3}\\right),\\quad\np(y_{2}\\mid x=1,y_{1}=1)=\\left(\\frac{\\frac{1}{4}}{\\frac{3}{4}},\\frac{\\frac{1}{2}}{\\frac{3}{4}}\\right)=\\left(\\frac{1}{3},\\frac{2}{3}\\right).\n$$\nThese are independent of $x$ for both $y_{1}\\in\\{0,1\\}$ with positive probability, so $X\\to Y_{1}\\to Y_{2}$ holds. Hence A is degraded.\n\nChannel B:\nFor $x=0$, row sums are $p(y_{1}=0\\mid 0)=\\frac{1}{2}+\\frac{1}{12}=\\frac{7}{12}$ and $p(y_{1}=1\\mid 0)=\\frac{1}{4}+\\frac{1}{6}=\\frac{5}{12}$. Then\n$$\np(y_{2}\\mid x=0,y_{1}=0)=\\left(\\frac{\\frac{1}{2}}{\\frac{7}{12}},\\frac{\\frac{1}{12}}{\\frac{7}{12}}\\right)=\\left(\\frac{6}{7},\\frac{1}{7}\\right),\\quad\np(y_{2}\\mid x=0,y_{1}=1)=\\left(\\frac{\\frac{1}{4}}{\\frac{5}{12}},\\frac{\\frac{1}{6}}{\\frac{5}{12}}\\right)=\\left(\\frac{3}{5},\\frac{2}{5}\\right).\n$$\nFor $x=1$, row sums are $p(y_{1}=0\\mid 1)=\\frac{1}{6}+\\frac{1}{4}=\\frac{5}{12}$ and $p(y_{1}=1\\mid 1)=\\frac{1}{12}+\\frac{1}{2}=\\frac{7}{12}$. Then\n$$\np(y_{2}\\mid x=1,y_{1}=0)=\\left(\\frac{\\frac{1}{6}}{\\frac{5}{12}},\\frac{\\frac{1}{4}}{\\frac{5}{12}}\\right)=\\left(\\frac{2}{5},\\frac{3}{5}\\right),\\quad\np(y_{2}\\mid x=1,y_{1}=1)=\\left(\\frac{\\frac{1}{12}}{\\frac{7}{12}},\\frac{\\frac{1}{2}}{\\frac{7}{12}}\\right)=\\left(\\frac{1}{7},\\frac{6}{7}\\right).\n$$\nThus $p(y_{2}\\mid x,y_{1})$ depends on $x$, so $X\\to Y_{1}\\to Y_{2}$ fails. Check $X\\to Y_{2}\\to Y_{1}$ via columns. For $x=0$, column sums are $p(y_{2}=0\\mid 0)=\\frac{1}{2}+\\frac{1}{4}=\\frac{3}{4}$ and $p(y_{2}=1\\mid 0)=\\frac{1}{12}+\\frac{1}{6}=\\frac{1}{4}$, giving\n$$\np(y_{1}\\mid x=0,y_{2}=0)=\\left(\\frac{\\frac{1}{2}}{\\frac{3}{4}},\\frac{\\frac{1}{4}}{\\frac{3}{4}}\\right)=\\left(\\frac{2}{3},\\frac{1}{3}\\right),\\quad\np(y_{1}\\mid x=0,y_{2}=1)=\\left(\\frac{\\frac{1}{12}}{\\frac{1}{4}},\\frac{\\frac{1}{6}}{\\frac{1}{4}}\\right)=\\left(\\frac{1}{3},\\frac{2}{3}\\right).\n$$\nFor $x=1$, column sums are $p(y_{2}=0\\mid 1)=\\frac{1}{6}+\\frac{1}{12}=\\frac{1}{4}$ and $p(y_{2}=1\\mid 1)=\\frac{1}{4}+\\frac{1}{2}=\\frac{3}{4}$, giving\n$$\np(y_{1}\\mid x=1,y_{2}=0)=\\left(\\frac{\\frac{1}{6}}{\\frac{1}{4}},\\frac{\\frac{1}{12}}{\\frac{1}{4}}\\right)=\\left(\\frac{2}{3},\\frac{1}{3}\\right),\\quad\np(y_{1}\\mid x=1,y_{2}=1)=\\left(\\frac{\\frac{1}{4}}{\\frac{3}{4}},\\frac{\\frac{1}{2}}{\\frac{3}{4}}\\right)=\\left(\\frac{1}{3},\\frac{2}{3}\\right).\n$$\nThese are independent of $x$, so $X\\to Y_{2}\\to Y_{1}$ holds. Hence B is degraded.\n\nChannel C:\nFor $x=0$, rows yield $p(y_{2}\\mid x=0,y_{1}=0)=(1,0)$ and $p(y_{2}\\mid x=0,y_{1}=1)=(0,1)$. For $x=1$, rows yield $p(y_{2}\\mid x=1,y_{1}=0)=(0,1)$ and $p(y_{2}\\mid x=1,y_{1}=1)=(1,0)$. For each fixed $y_{1}$, these depend on $x$, so $X\\to Y_{1}\\to Y_{2}$ fails. Using columns, for $x=0$ we get $p(y_{1}\\mid x=0,y_{2}=0)=(1,0)$ and $p(y_{1}\\mid x=0,y_{2}=1)=(0,1)$, while for $x=1$ we get $p(y_{1}\\mid x=1,y_{2}=0)=(0,1)$ and $p(y_{1}\\mid x=1,y_{2}=1)=(1,0)$. For each fixed $y_{2}$, these depend on $x$, so $X\\to Y_{2}\\to Y_{1}$ fails. Hence C is non-degraded.\n\nChannel D:\nHere $p(y_{1}\\mid x=0)=(1,0)$ and $p(y_{1}\\mid x=1)=(0,1)$, so for each $y_{1}$ only one $x$ yields positive probability. Define $q(y_{2}\\mid y_{1}=0)=\\left(\\frac{2}{3},\\frac{1}{3}\\right)$ and $q(y_{2}\\mid y_{1}=1)=\\left(\\frac{1}{3},\\frac{2}{3}\\right)$. Then $p(y_{1},y_{2}\\mid x)=p(y_{1}\\mid x)q(y_{2}\\mid y_{1})$ matches the given matrices, so $X\\to Y_{1}\\to Y_{2}$ holds. Hence D is degraded.\n\nOnly channel C is non-degraded.", "answer": "$$\\boxed{C}$$", "id": "1617320"}, {"introduction": "The classification of a broadcast channel as degraded is not just a theoretical exercise; it has profound implications for its capacity. For degraded channels, the capacity region is well understood and can be achieved using a technique called superposition coding. This problem [@problem_id:1662908] provides a concrete application where one user has a perfect channel and the other a noisy one, creating a classic degraded scenario. You will calculate a specific achievable rate pair on the boundary of the capacity region, illustrating the fundamental trade-off between the information rates sent to the two users.", "problem": "Consider a two-user discrete memoryless Broadcast Channel (BC) where a single transmitter sends information to two separate receivers. The channel input, denoted by $X$, is a binary symbol from the alphabet $\\{0, 1\\}$.\n\nThe channel to the first user is perfectly noiseless. Its output $Y_1$ is identical to the input, i.e., $Y_1 = X$.\n\nThe channel to the second user is a Binary Symmetric Channel (BSC). Its output $Y_2$ is a flipped version of the input $X$ with a crossover probability $p$, where $0 < p < 1/2$. The conditional probability is given by $P(Y_2 \\ne X | X) = p$.\n\nLet $(R_1, R_2)$ be a pair of achievable communication rates in bits per channel use for user 1 and user 2, respectively. If the rate for user 1 is fixed at $R_1 = H_b(p)$, where $H_b(z) = -z \\log_2(z) - (1-z)\\log_2(1-z)$ is the binary entropy function, what is the maximum possible achievable rate $R_2$ for user 2?\n\nProvide your answer as a closed-form analytic expression in terms of the crossover probability $p$ and the binary entropy function $H_b(\\cdot)$.", "solution": "Since User 1 has a perfect channel ($Y_1 = X$) while User 2 has a noisy one, this is a physically degraded broadcast channel where User 1 is the stronger user and User 2 is the weaker user (formally, the Markov chain $X \\to Y_1 \\to Y_2$ holds). The capacity region for a physically degraded broadcast channel is the union over all $p(u)p(x|u)$ of rate pairs satisfying\n$$\nR_{1} \\leq I(X;Y_{1}\\mid U), \\qquad R_{2} \\leq I(U;Y_{2}).\n$$\nSince $Y_{1}=X$ deterministically,\n$$\nI(X;Y_{1}\\mid U) = H(X\\mid U).\n$$\nThus any achievable pair must satisfy\n$$\nR_{1} \\leq H(X\\mid U), \\qquad R_{2} \\leq I(U;Y_{2}).\n$$\nWe are given $R_{1}=H_{b}(p)$. To maximize $R_{2}$ under the constraint $H(X\\mid U) \\geq H_{b}(p)$, it is optimal to use exactly $H(X\\mid U)=H_{b}(p)$; adding extra randomness in $X$ given $U$ would only further degrade $Y_{2}$ and decrease $I(U;Y_{2})$.\n\nBy symmetry of the BSC and standard extremal arguments for degraded broadcast channels, the optimal choice for fixed $H(X\\mid U)$ is to take $U$ binary and uniform, and let $X$ be obtained from $U$ through a binary symmetric “test channel”: $X=U \\oplus S$ with $S \\sim \\mathrm{Bern}(r)$ independent of $U$, so that\n$H(X\\mid U)=H(S)=H_{b}(r)$.\nEnforcing $H(X\\mid U)=H_{b}(p)$ gives $H_{b}(r)=H_{b}(p)$, and with $0<p<\\frac{1}{2}$ we may choose $r=p$.\n\nThe effective channel from $U$ to $Y_{2}$ is then the cascade of two independent BSCs: $U \\to X$ with crossover $r=p$ and $X \\to Y_{2}$ with crossover $p$. The cascade is a BSC with crossover\n$p \\star r = r(1-p) + (1-r)p = p(1-p) + (1-p)p = 2p(1-p)$.\nWith $U$ uniform, this yields\n$I(U;Y_{2}) = 1 - H_{b}(p \\star r) = 1 - H_{b}\\bigl(2p(1-p)\\bigr)$.\nThis value is achievable by the above superposition construction, and by the degraded BC capacity characterization and channel symmetry it is also the maximum $R_{2}$ compatible with $R_{1}=H_{b}(p)$.\n\nTherefore, the maximum achievable $R_{2}$ is\n$$\nR_{2}^{\\max} = 1 - H_{b}\\bigl(2p(1-p)\\bigr).\n$$", "answer": "$$\\boxed{1 - H_{b}\\!\\left(2p(1-p)\\right)}$$", "id": "1662908"}]}