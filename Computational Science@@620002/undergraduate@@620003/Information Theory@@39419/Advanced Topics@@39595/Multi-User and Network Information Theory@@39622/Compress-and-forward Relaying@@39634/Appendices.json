{"hands_on_practices": [{"introduction": "The core idea of compress-and-forward relaying involves a relay transmitting a simplified version of what it hears. This first exercise explores the most basic form of this strategy: coarse quantization. By modeling a relay that reduces its observation to a single bit, you will use the fundamental tool of mutual information, $I(X; B)$, to precisely quantify how much information this compressed signal provides to the destination. This practice establishes a quantitative foundation for evaluating the effectiveness of any relaying scheme.", "problem": "In a wireless communication system, a source node transmits a single bit $X$ to a destination node. The bit $X$ is chosen from the set $\\{-1, +1\\}$ with equal probability. An intermediate relay node assists the communication. The relay receives a noisy version of the source's signal, which it quantizes into a single bit $B$ using a simple sign function: $B=+1$ if the received signal is positive, and $B=-1$ if it is negative. This bit $B$ is then transmitted to the destination over an error-free link.\n\nDue to noise in the link between the source and the relay, the quantization process is not perfect. The probability that the relay's bit $B$ is the same as the original source bit $X$ is $0.9$. That is, $P(B=X) = 0.9$.\n\nAssuming this model, calculate the mutual information $I(X; B)$ between the source bit $X$ and the relay's bit $B$. Express your final answer in bits, rounded to three significant figures.", "solution": "Let $X \\in \\{-1,+1\\}$ with $P(X=+1)=P(X=-1)=\\frac{1}{2}$. The relay produces $B \\in \\{-1,+1\\}$ such that $P(B=X)=0.9$ and $P(B \\neq X)=0.1$. This defines a binary symmetric channel (BSC) with crossover probability $q=0.1$.\n\nThe mutual information between $X$ and $B$ is\n$$\nI(X;B)=H(B)-H(B|X).\n$$\nFirst, compute $H(B)$. The output $B$ is symmetric because the input is uniform and the channel is symmetric:\n$$\nP(B=+1)=P(B=+1|X=+1)P(X=+1)+P(B=+1|X=-1)P(X=-1)=0.9 \\cdot \\frac{1}{2}+0.1 \\cdot \\frac{1}{2}=\\frac{1}{2},\n$$\nand similarly $P(B=-1)=\\frac{1}{2}$. Hence\n$$\nH(B)=-\\sum_{b \\in \\{-1,+1\\}} P(B=b)\\log_{2} P(B=b)=-2 \\cdot \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right)=1.\n$$\nNext, compute the conditional entropy. For a BSC with crossover probability $q=0.1$,\n$$\nH(B|X)=-q \\log_{2} q-(1-q)\\log_{2}(1-q)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nTherefore,\n$$\nI(X;B)=1-\\left[-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\right].\n$$\nEvaluating the logarithms,\n$$\n-0.1\\log_{2}(0.1)\\approx 0.3321928095,\\quad -0.9\\log_{2}(0.9)\\approx 0.1368027841,\n$$\nso\n$$\nH(B|X)\\approx 0.4689955936,\\quad I(X;B)\\approx 1-0.4689955936=0.5310044064.\n$$\nRounding to three significant figures yields $I(X;B)\\approx 0.531$ bits.", "answer": "$$\\boxed{0.531}$$", "id": "1611908"}, {"introduction": "A relay can do more than just compress the raw signal it receives; it can perform intelligent processing to extract more relevant information first. This practice explores a powerful soft-decision forwarding strategy, where the relay computes a Log-Likelihood Ratio (LLR) before quantizing it. You will learn how the destination optimally combines this sophisticated piece of information from the relay with its own direct observation, providing insight into how Bayesian inference is applied to enhance communication reliability.", "problem": "Consider a wireless communication system comprising a source (S), a relay (R), and a destination (D). The source transmits a symbol $X$, chosen with equal probability from the set $\\{-1, +1\\}$. This transmission is received by both the relay and the destination.\n\nThe communication links are modeled as Additive White Gaussian Noise (AWGN) channels. The signal received at the relay is $Y_R = X + N_R$, and the signal received at the destination is $Y_D = X + N_D$. The noise terms $N_R$ and $N_D$ are statistically independent Gaussian random variables with zero mean and variances $\\sigma_R^2$ and $\\sigma_D^2$, respectively.\n\nThe relay implements a \"soft-decision forward\" strategy. It first computes the Log-Likelihood Ratio (LLR) of the source symbol $X$ based on its observation $Y_R$. It then performs a one-bit quantization of this LLR to generate a message $M \\in \\{-1, +1\\}$. The message is set to $M=+1$ if the relay's computed LLR is positive, and $M=-1$ if it is negative. This message $M$ is then transmitted to the destination over an error-free, dedicated channel.\n\nThe destination's goal is to make an optimal decision about the original symbol $X$ by combining its direct observation $Y_D$ with the message $M$ received from the relay. The optimal combination under the Maximum A Posteriori (MAP) criterion is based on the total LLR, defined as $L_{total} = \\ln \\frac{P(Y_D=y_D, M | X=+1)}{P(Y_D=y_D, M | X=-1)}$, where $\\ln$ denotes the natural logarithm.\n\nSuppose the noise variances are $\\sigma_R^2 = 2.0$ and $\\sigma_D^2 = 3.0$. The destination observes the value $y_D = -0.75$ and receives the message $M=+1$ from the relay.\n\nCalculate the numerical value of the total LLR, $L_{total}$, at the destination.\n\nFor your calculation, you may need the complementary cumulative distribution function (often called the Q-function) of the standard normal distribution, $Q(x) = P(Z > x)$ for $Z \\sim \\mathcal{N}(0,1)$. Round your final answer to four significant figures.", "solution": "We work with equiprobable $X \\in \\{-1,+1\\}$ and independent AWGN observations $Y_{R}=X+N_{R}$ and $Y_{D}=X+N_{D}$ with $N_{R} \\sim \\mathcal{N}(0,\\sigma_{R}^{2})$, $N_{D} \\sim \\mathcal{N}(0,\\sigma_{D}^{2})$. The relay computes the LLR of $X$ from $Y_{R}$:\n$$\nL_{R}(y_{R})=\\ln \\frac{p(y_{R}\\,|\\,X=+1)}{p(y_{R}\\,|\\,X=-1)}\n=\\frac{-(y_{R}-1)^{2}+(y_{R}+1)^{2}}{2\\sigma_{R}^{2}}=\\frac{2y_{R}}{\\sigma_{R}^{2}}.\n$$\nThus $\\operatorname{sign}(L_{R})=\\operatorname{sign}(y_{R})$, and the relayâ€™s one-bit message is $M=+1$ if and only if $Y_{R}>0$.\n\nAt the destination, by conditional independence of $Y_{D}$ and $M$ given $X$,\n$$\nL_{\\text{total}}=\\ln \\frac{p(Y_{D}=y_{D}\\,|\\,X=+1)}{p(Y_{D}=y_{D}\\,|\\,X=-1)}+\\ln \\frac{\\Pr(M\\,|\\,X=+1)}{\\Pr(M\\,|\\,X=-1)}.\n$$\nThe first term is the standard AWGN LLR:\n$$\n\\ln \\frac{p(Y_{D}=y_{D}\\,|\\,X=+1)}{p(Y_{D}=y_{D}\\,|\\,X=-1)}\n=\\frac{-(y_{D}-1)^{2}+(y_{D}+1)^{2}}{2\\sigma_{D}^{2}}=\\frac{2y_{D}}{\\sigma_{D}^{2}}.\n$$\nFor the second term with the observed $M=+1$,\n$$\n\\Pr(M=+1\\,|\\,X=+1)=\\Pr(Y_{R}>0\\,|\\,X=+1)=\\Pr\\!\\left(Z> \\frac{0-1}{\\sigma_{R}}\\right)=\\Phi\\!\\left(\\frac{1}{\\sigma_{R}}\\right)=1-Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right),\n$$\n$$\n\\Pr(M=+1\\,|\\,X=-1)=\\Pr(Y_{R}>0\\,|\\,X=-1)=\\Pr\\!\\left(Z> \\frac{0-(-1)}{\\sigma_{R}}\\right)=Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right),\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$, $\\Phi$ is its CDF, and $Q(x)=\\Pr(Z>x)$. Therefore,\n$$\nL_{\\text{total}}=\\frac{2y_{D}}{\\sigma_{D}^{2}}+\\ln \\left(\\frac{1-Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right)}{Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right)}\\right).\n$$\n\nNow substitute the given values $\\sigma_{R}^{2}=2$, $\\sigma_{D}^{2}=3$, $y_{D}=-0.75$, and $M=+1$:\n$$\n\\frac{2y_{D}}{\\sigma_{D}^{2}}=\\frac{2(-0.75)}{3}=-0.5,\n\\qquad\na=\\frac{1}{\\sigma_{R}}=\\frac{1}{\\sqrt{2}}.\n$$\nUsing $Q(a)=Q\\!\\left(\\frac{1}{\\sqrt{2}}\\right)\\approx 0.239758$ (so $1-Q(a)\\approx 0.760242$),\n$$\n\\ln \\left(\\frac{1-Q(a)}{Q(a)}\\right)=\\ln \\left(\\frac{0.760242}{0.239758}\\right)\\approx \\ln(3.17083)\\approx 1.15395.\n$$\nHence,\n$$\nL_{\\text{total}}\\approx -0.5+1.15395=0.65395,\n$$\nwhich to four significant figures is $0.6540$.", "answer": "$$\\boxed{0.6540}$$", "id": "1611890"}, {"introduction": "To fully understand the limits of a relayed system, we must analyze the flow of information from end to end, accounting for impairments at every step. This exercise models a complete source-relay-destination link using continuous Gaussian signals, where noise is introduced not only over the wireless channels but also within the relay's own processing hardware. By deriving the mutual information $I(X; Y_D)$, you will gain a deeper understanding of how power constraints and accumulating noise sources interact to define the ultimate capacity of a practical relay channel.", "problem": "Consider a simple linear relay network consisting of a source node (S), a relay node (R), and a destination node (D). There is no direct communication link between the source and the destination. The communication process occurs in two hops: S-to-R and R-to-D.\n\nThe source transmits a signal $X$ which is modeled as a Gaussian random variable with zero mean and variance $P_S$, representing the average transmit power of the source.\n\nThe S-to-R link is impaired by noise. The signal received by the relay is given by $Y_R = X + N_R$, where $N_R$ is additive white Gaussian noise with zero mean and variance $\\sigma_R^2$.\n\nAt the relay, the received signal $Y_R$ undergoes a noisy processing step before retransmission. This imperfect processing is modeled as the addition of an independent noise term, $N_Q$, to the signal. The signal prepared for transmission is thus $Z = Y_R + N_Q$. The processing noise $N_Q$ is a zero-mean Gaussian random variable with variance $\\sigma_Q^2$.\n\nThe relay must adhere to an average transmit power constraint of $P_R$. To meet this constraint, it scales the signal $Z$ by a constant factor $k$ to produce its transmission signal $X_R = kZ$, such that the average power of $X_R$ is exactly $P_R$.\n\nFinally, the destination receives the signal from the R-to-D link, which is also corrupted by noise. The received signal at the destination is $Y_D = X_R + N_D$, where $N_D$ is additive white Gaussian noise with zero mean and variance $\\sigma_D^2$.\n\nAssume that the source signal $X$ and all noise variables $N_R$, $N_Q$, and $N_D$ are mutually independent.\n\nDerive a closed-form analytic expression for the mutual information $I(X; Y_D)$ between the source signal and the signal received at the destination. Your final expression should be in terms of the parameters $P_S, P_R, \\sigma_R^2, \\sigma_Q^2,$ and $\\sigma_D^2$.", "solution": "Let $X \\sim \\mathcal{N}(0,P_{S})$, $N_{R} \\sim \\mathcal{N}(0,\\sigma_{R}^{2})$, $N_{Q} \\sim \\mathcal{N}(0,\\sigma_{Q}^{2})$, and $N_{D} \\sim \\mathcal{N}(0,\\sigma_{D}^{2})$, all mutually independent. The relay receives $Y_{R} = X + N_{R}$ and forms $Z = Y_{R} + N_{Q} = X + N_{R} + N_{Q}$, which is zero-mean Gaussian with\n$$\n\\operatorname{Var}(Z) = P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2}.\n$$\nThe relay transmits $X_{R} = kZ$ under the power constraint $E[X_{R}^{2}] = P_{R}$, which gives\n$$\nk^{2} E[Z^{2}] = P_{R} \\quad \\Rightarrow \\quad k^{2} = \\frac{P_{R}}{P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2}}.\n$$\nThe destination observes\n$$\nY_{D} = X_{R} + N_{D} = k(X + N_{R} + N_{Q}) + N_{D} = kX + W,\n$$\nwhere the effective noise $W = kN_{R} + kN_{Q} + N_{D}$ is zero-mean Gaussian, independent of $X$, with\n$$\n\\operatorname{Var}(W) = k^{2}(\\sigma_{R}^{2} + \\sigma_{Q}^{2}) + \\sigma_{D}^{2}.\n$$\nThus $Y_{D}|X$ is Gaussian with variance $\\operatorname{Var}(W)$ and $Y_{D}$ is Gaussian with variance $k^{2}P_{S} + \\operatorname{Var}(W)$. Using $h(\\mathcal{N}(0,v)) = \\frac{1}{2}\\ln(2\\pi \\exp(1)v)$, the mutual information is\n$$\nI(X;Y_{D}) = h(Y_{D}) - h(Y_{D}|X) = \\frac{1}{2}\\ln\\!\\left(\\frac{k^{2}P_{S} + \\operatorname{Var}(W)}{\\operatorname{Var}(W)}\\right) = \\frac{1}{2}\\ln\\!\\left(1 + \\frac{k^{2}P_{S}}{k^{2}(\\sigma_{R}^{2} + \\sigma_{Q}^{2}) + \\sigma_{D}^{2}}\\right).\n$$\nSubstituting $k^{2} = \\frac{P_{R}}{P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2}}$ yields\n$$\nI(X;Y_{D}) = \\frac{1}{2}\\ln\\!\\left(1 + \\frac{\\frac{P_{R}}{P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2}} P_{S}}{\\frac{P_{R}}{P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2}}(\\sigma_{R}^{2} + \\sigma_{Q}^{2}) + \\sigma_{D}^{2}}\\right)\n= \\frac{1}{2}\\ln\\!\\left(1 + \\frac{P_{R}P_{S}}{P_{R}(\\sigma_{R}^{2} + \\sigma_{Q}^{2}) + \\sigma_{D}^{2}(P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2})}\\right).\n$$\nThis is the desired closed-form expression in terms of $P_{S}, P_{R}, \\sigma_{R}^{2}, \\sigma_{Q}^{2}, \\sigma_{D}^{2}$.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\!\\left(1 + \\frac{P_{R}P_{S}}{P_{R}\\left(\\sigma_{R}^{2} + \\sigma_{Q}^{2}\\right) + \\sigma_{D}^{2}\\left(P_{S} + \\sigma_{R}^{2} + \\sigma_{Q}^{2}\\right)}\\right)}$$", "id": "1611892"}]}