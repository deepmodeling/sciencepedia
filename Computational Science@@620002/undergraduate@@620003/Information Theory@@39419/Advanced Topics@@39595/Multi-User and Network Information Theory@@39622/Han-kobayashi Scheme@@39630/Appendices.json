{"hands_on_practices": [{"introduction": "The power of the Han-Kobayashi scheme lies in its sophisticated message-splitting strategy. To truly grasp this, we must understand the roles of the auxiliary random variables that define the common and private message components. This first exercise [@problem_id:1628827] invites you to explore this structure through a thought experiment: what happens when a user forgoes sending a private message? By analyzing this special case, you will clarify the distinct function of each part of the transmitted signal.", "problem": "Consider a two-user discrete memoryless interference channel (DM-IC) defined by the transition probability $p(y_1, y_2|x_1, x_2)$, where $X_1 \\in \\mathcal{X}_1$ and $X_2 \\in \\mathcal{X}_2$ are the channel inputs, and $Y_1 \\in \\mathcal{Y}_1$ and $Y_2 \\in \\mathcal{Y}_2$ are the channel outputs. The Han-Kobayashi (HK) scheme provides an achievable rate region for this channel by splitting each user's message into a common and a private part. The random coding argument for this scheme relies on three auxiliary random variables, $U_0, U_1, U_2$, with a joint probability distribution $p(u_0, u_1, u_2)$. The channel inputs $X_1$ and $X_2$ are then generated according to $p(x_1|u_0, u_1)$ and $p(x_2|u_0, u_2)$, respectively. This structure implies that $U_1$ is associated with User 1's private message, $U_2$ with User 2's private message, and $U_0$ with a message component common to both users.\n\nThe general HK achievable rate region is the convex hull of the union of all rate pairs $(R_1, R_2)$ satisfying the following inequalities for some choice of the joint distribution $p(u_0, u_1, u_2)p(x_1|u_0, u_1)p(x_2|u_0, u_2)$:\n\n1. $R_1 \\le I(X_1; Y_1 | U_0, U_2)$\n2. $R_2 \\le I(X_2; Y_2 | U_0, U_1)$\n3. $R_1 + R_2 \\le I(X_1, X_2; Y_1|U_0) + I(X_2; Y_2 | X_1, U_0, U_1)$\n4. $R_1 + R_2 \\le I(X_1, X_2; Y_2|U_0) + I(X_1; Y_1 | X_2, U_0, U_2)$\n\nNow, consider a specialized coding strategy where User 1 does not send a private message. This is formally equivalent to setting the auxiliary random variable $U_1$ to be a constant. Which of the following statements provides the most accurate and complete description of the consequences of this choice?\n\nA. The achievable rate region remains unchanged because the choice of the distribution $p(u_0, u_1, u_2)$ is arbitrary and can compensate for $U_1$ being a constant.\n\nB. The scheme reduces to treating interference as noise for both users, resulting in the rate bounds $R_1 \\le I(X_1; Y_1)$ and $R_2 \\le I(X_2; Y_2)$.\n\nC. This choice forces the rate for user 1 to be zero ($R_1 = 0$), as no private message can be sent. The only achievable rates are for user 2.\n\nD. The scheme becomes one where User 1 transmits a common-only message, while User 2 transmits a message with both common and private parts. The rate bound for User 2 simplifies to $R_2 \\le I(X_2; Y_2 | U_0)$.\n\nE. The joint distribution of the inputs simplifies to $p(x_1, x_2) = p(x_1)p(x_2)$, meaning the inputs become independent and the problem reduces to two parallel point-to-point channels.", "solution": "The problem asks to analyze the effect of setting the auxiliary random variable $U_1$ to a constant within the Han-Kobayashi (HK) coding scheme for the two-user interference channel. Let's analyze the implications of this constraint and then evaluate each of the given options.\n\n**Analysis of the Constraint**\n\nThe auxiliary random variable $U_1$ is used in the random coding argument to generate the codebook for the private part of User 1's message. Specifically, for each private message $w_{1p}$, a codeword $u_1^n(w_{1p})$ is drawn from an i.i.d. distribution governed by $p(u_1)$. The channel input $X_1$ is then generated based on both this private component and a common component $U_0$, according to $p(x_1|u_0, u_1)$.\n\nIf we set $U_1$ to be a constant, say $U_1 = c$, this has two main consequences:\n\n1.  **Conceptual Consequence:** Since $U_1$ is constant, it cannot carry any information. The codebook for User 1's private message, which is indexed by different values of $U_1$, effectively collapses. There is only one \"message\" that can be sent through this private channel part, meaning the rate of the private message for User 1 must be zero. However, User 1 can still send information through the common message component, which is associated with the random variable $U_0$. Therefore, User 1's total rate $R_1$ is not necessarily zero, but consists only of the common message rate. The coding scheme becomes one where User 1 sends a common-only message, while User 2, which still uses both $U_0$ and $U_2$, can send a message with both common and private components.\n\n2.  **Probabilistic Consequence:** Setting $U_1$ to a constant means its probability mass function is a Dirac delta. Consequently, $H(U_1)=0$ and $U_1$ is independent of all other random variables. The joint probability distribution that defines the coding scheme, $p(u_0, u_1, u_2)p(x_1|u_0, u_1)p(x_2|u_0, u_2)$, simplifies. The term $p(x_1|u_0, u_1)$ becomes $p(x_1|u_0)$. The overall distribution is now chosen from the family $p(u_0, u_2)p(x_1|u_0)p(x_2|u_0, u_2)$.\n\nWe use these consequences to evaluate the given options.\n\n**Evaluation of Options**\n\n*   **A. The achievable rate region remains unchanged...**\n    This is incorrect. The general HK region is an optimization over all possible joint distributions $p(u_0, u_1, u_2)p(x_1|u_0, u_1)p(x_2|u_0, u_2)$. By setting $U_1$ to a constant, we are restricting this optimization to a smaller set of distributions. This results in an achievable rate region that is a subset of, and generally smaller than, the full HK region.\n\n*   **B. The scheme reduces to treating interference as noise for both users...**\n    This is incorrect. The \"treating interference as noise\" (TIN) scheme typically corresponds to a specific choice of auxiliary variables, e.g., $U_0 = \\emptyset, U_1 = X_1, U_2 = X_2$, leading to rate bounds like $R_1 \\le I(X_1; Y_1|X_2)$ and $R_2 \\le I(X_2; Y_2|X_1)$. The simplification in the problem is different. Furthermore, the bounds $R_1 \\le I(X_1; Y_1)$ and $R_2 \\le I(X_2; Y_2)$ correspond to ignoring the interference entirely, which is generally not achievable.\n\n*   **C. This choice forces the rate for user 1 to be zero ($R_1 = 0$)...**\n    This is incorrect. As discussed, only the private part of User 1's message has a zero rate. User 1 can still transmit information via the common auxiliary variable $U_0$. The first inequality in the HK bounds, $R_1 \\le I(X_1; Y_1 | U_0, U_2)$, can still be positive, allowing for a non-zero rate $R_1$ composed solely of a common message.\n\n*   **D. The scheme becomes one where User 1 transmits a common-only message...**\n    This is the correct statement. Let's break it down:\n    -   \"The scheme becomes one where User 1 transmits a common-only message, while User 2 transmits a message with both common and private parts.\" This is the correct conceptual interpretation, as explained in our initial analysis. User 1 loses its private message capability (linked to $U_1$), while User 2 retains both common ($U_0$) and private ($U_2$) capabilities.\n    -   \"The rate bound for User 2 simplifies to $R_2 \\le I(X_2; Y_2 | U_0)$.\" We check this by simplifying the second HK inequality: $R_2 \\le I(X_2; Y_2 | U_0, U_1)$. Since $U_1$ is a constant, it is independent of all other variables. A fundamental property of mutual information is that conditioning on a constant does not change its value: $I(A; B | C, K) = I(A; B | C)$ if $K$ is a constant. Thus, $I(X_2; Y_2 | U_0, U_1) = I(X_2; Y_2 | U_0)$. This mathematical simplification is correct.\n    Both parts of the statement are accurate, providing a complete description of the situation.\n\n*   **E. The joint distribution of the inputs simplifies to $p(x_1, x_2) = p(x_1)p(x_2)$...**\n    This is incorrect. The inputs $X_1$ and $X_2$ are not generally independent. After setting $U_1$ to a constant, the joint distribution of all variables is based on $p(u_0, u_2)p(x_1|u_0)p(x_2|u_0, u_2)$. To find the joint distribution of the inputs, we must marginalize over $U_0$ and $U_2$:\n    $$p(x_1, x_2) = \\sum_{u_0, u_2} p(u_0, u_2) p(x_1|u_0) p(x_2|u_0, u_2)$$\n    Since both $X_1$ and $X_2$ depend on the common random variable $U_0$, they are coupled and not independent, unless $U_0$ itself is a constant (which would remove the common message capability entirely).\n\nBased on this step-by-step evaluation, option D is the only one that provides an accurate and complete description.", "answer": "$$\\boxed{D}$$", "id": "1628827"}, {"introduction": "Message splitting is not just a theoretical curiosity; it is a practical tool for optimizing communication in the presence of interference. This next practice [@problem_id:1628794] grounds the abstract Han-Kobayashi framework in the context of the ubiquitous Gaussian Interference Channel. You will tackle a core engineering problem: how to optimally allocate power between the common and private parts of a signal to maximize the total data rate. This demonstrates the tangible trade-offs involved in managing interference.", "problem": "Consider a symmetric Gaussian Interference Channel (GIC) where two transmitter-receiver pairs communicate simultaneously. The received signals at the two receivers are given by the models:\n$$Y_1 = X_1 + \\alpha X_2 + Z_1$$\n$$Y_2 = \\alpha X_1 + X_2 + Z_2$$\nHere, $X_1$ and $X_2$ are the signals transmitted by user 1 and user 2, respectively, each with an average power constraint $E[X_i^2] \\le P$. The parameter $\\alpha$ represents the symmetric interference coefficient. The noises $Z_1$ and $Z_2$ are independent Gaussian random variables with zero mean and unit variance, i.e., $\\mathcal{N}(0,1)$.\n\nTo manage interference, a Han-Kobayashi coding scheme is employed. Each user $i$ splits their message into a common part and a private part. The transmitted signal is a superposition $X_i = X_{ic} + X_{ip}$, where $X_{ic}$ is the codeword for the common message and $X_{ip}$ is the codeword for the private message. The total power $P$ is distributed between these two parts using a power allocation factor $\\beta \\in [0,1]$, such that the power of the common part is $E[X_{ic}^2] = \\beta P$ and the power of the private part is $E[X_{ip}^2] = (1-\\beta) P$.\n\nWithin this scheme, an achievable sum-rate for a symmetric rate point ($R_1 = R_2$) is given by:\n$$R_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_2\\left(1 + \\frac{\\beta P(1+\\alpha^2)}{(1-\\beta)P(1+\\alpha^2) + 1}\\right) + \\log_2\\left(1 + \\frac{(1-\\beta)P}{\\alpha^2 (1-\\beta)P + 1}\\right)$$\nThe first term corresponds to the sum-rate of the common messages which must be decoded by both receivers, and the second term corresponds to the sum of the two private rates, where each receiver decodes its own private message.\n\nYour task is to find the optimal power allocation factor $\\beta$ that maximizes this achievable sum-rate $R_{\\text{sum}}(\\beta)$.\n\nAssume the channel operates in a weak interference regime where $0 < \\alpha < 1$. Furthermore, assume the power $P$ and interference $\\alpha$ are such that the optimal value of $\\beta$ lies strictly between 0 and 1. Your final answer should be a closed-form analytic expression for $\\beta$ in terms of $P$ and $\\alpha$.", "solution": "We are given the achievable sum-rate as a function of the power-split parameter $\\beta \\in [0,1]$:\n$$\nR_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_{2}\\left(1 + \\frac{\\beta P(1+\\alpha^{2})}{(1-\\beta)P(1+\\alpha^{2}) + 1}\\right) + \\log_{2}\\left(1 + \\frac{(1-\\beta)P}{\\alpha^{2} (1-\\beta)P + 1}\\right).\n$$\nMaximizing $R_{\\text{sum}}(\\beta)$ over $\\beta$ is equivalent to maximizing the same expression with $\\log_{2}$ replaced by $\\ln$, because $\\log_{2}(x) = \\ln(x)/\\ln 2$ and $\\ln 2$ is a positive constant. \n\nIntroduce the shorthand $Q \\triangleq (1-\\beta)P$ so that $Q \\in (0,P)$ under the stated assumption that the optimum lies strictly between $0$ and $1$. Then observe that\n$$\n1 + \\frac{\\beta P(1+\\alpha^{2})}{(1-\\beta)P(1+\\alpha^{2}) + 1}\n= \\frac{(1-\\beta)P(1+\\alpha^{2}) + 1 + \\beta P(1+\\alpha^{2})}{(1-\\beta)P(1+\\alpha^{2}) + 1}\n= \\frac{P(1+\\alpha^{2}) + 1}{Q(1+\\alpha^{2}) + 1},\n$$\nand\n$$\n1 + \\frac{(1-\\beta)P}{\\alpha^{2} (1-\\beta)P + 1}\n= \\frac{\\alpha^{2} Q + 1 + Q}{\\alpha^{2} Q + 1}\n= \\frac{(1+\\alpha^{2})Q + 1}{\\alpha^{2} Q + 1}.\n$$\nTherefore,\n$$\nR_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_{2}\\!\\big(P(1+\\alpha^{2}) + 1\\big) - \\frac{1}{2}\\log_{2}\\!\\big((1+\\alpha^{2})Q + 1\\big) + \\log_{2}\\!\\big((1+\\alpha^{2})Q + 1\\big) - \\log_{2}\\!\\big(\\alpha^{2} Q + 1\\big),\n$$\nwhich simplifies to\n$$\nR_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_{2}\\!\\big(P(1+\\alpha^{2}) + 1\\big) + \\frac{1}{2}\\log_{2}\\!\\big((1+\\alpha^{2})Q + 1\\big) - \\log_{2}\\!\\big(\\alpha^{2} Q + 1\\big).\n$$\nDiscarding the constant term independent of $Q$, the maximization reduces to maximizing\n$$\ng(Q) \\triangleq \\frac{1}{2}\\ln\\!\\big((1+\\alpha^{2})Q + 1\\big) - \\ln\\!\\big(\\alpha^{2} Q + 1\\big).\n$$\nCompute the derivative:\n$$\ng'(Q) = \\frac{1}{2}\\cdot\\frac{1+\\alpha^{2}}{(1+\\alpha^{2})Q + 1} - \\frac{\\alpha^{2}}{\\alpha^{2} Q + 1}.\n$$\nSetting $g'(Q)=0$ yields\n$$\n\\frac{1}{2}\\cdot\\frac{1+\\alpha^{2}}{(1+\\alpha^{2})Q + 1} = \\frac{\\alpha^{2}}{\\alpha^{2} Q + 1}.\n$$\nCross-multiplying and simplifying,\n$$\n\\frac{1}{2}(1+\\alpha^{2})(\\alpha^{2}Q + 1) = \\alpha^{2}\\big((1+\\alpha^{2})Q + 1\\big),\n$$\n$$\n\\frac{1}{2}\\alpha^{2}(1+\\alpha^{2})Q + \\frac{1}{2}(1+\\alpha^{2}) = \\alpha^{2}(1+\\alpha^{2})Q + \\alpha^{2},\n$$\n$$\n\\frac{1}{2}(1+\\alpha^2) - \\alpha^2 = \\alpha^2(1+\\alpha^2)Q - \\frac{1}{2}\\alpha^2(1+\\alpha^2)Q\n$$\n$$\n\\frac{1}{2}(1-\\alpha^2) = \\frac{1}{2}\\alpha^2(1+\\alpha^2)Q\n$$\nwhich gives\n$$\n\\alpha^{2}(1+\\alpha^{2})Q = 1 - \\alpha^{2}.\n$$\nHence the critical point is\n$$\nQ^{\\star} = \\frac{1 - \\alpha^{2}}{\\alpha^{2}(1+\\alpha^{2})}.\n$$\nSince $0<\\alpha<1$, we have $1-\\alpha^{2}>0$, and $g'(0) = \\frac{1}{2}(1-\\alpha^{2})>0$ while $g'(Q)\\to -\\frac{1}{2Q}<0$ as $Q\\to\\infty$, so this critical point is the unique maximizer in the feasible interval provided it lies in $(0,P)$, which is ensured by the stated assumption.\n\nTransforming back to $\\beta$ using $Q=(1-\\beta)P$,\n$$\n1 - \\beta^{\\star} = \\frac{Q^{\\star}}{P} = \\frac{1 - \\alpha^{2}}{P\\,\\alpha^{2}(1+\\alpha^{2})},\n$$\nso the optimal power split is\n$$\n\\beta^{\\star} = 1 - \\frac{1 - \\alpha^{2}}{P\\,\\alpha^{2}(1+\\alpha^{2})}.\n$$\nUnder the stated assumption that the optimum lies strictly between $0$ and $1$, this interior solution applies; otherwise, the boundary values $\\beta=0$ or $\\beta=1$ would need to be considered.", "answer": "$$\\boxed{1 - \\frac{1 - \\alpha^{2}}{P\\,\\alpha^{2}(1+\\alpha^{2})}}$$", "id": "1628794"}, {"introduction": "An achievable rate region, like that from Han-Kobayashi, defines the boundary of what is possible, and different points on this boundary can correspond to different communication strategies. In this final exercise [@problem_id:53414], you will get your hands dirty by calculating achievable sum-rates for a specific, non-trivial interference channel. By comparing rates from two distinct decoding approaches, you will see how the choice of whether to decode interference or treat it as noise directly translates into quantifiable performance differences.", "problem": "Consider a two-user binary interference channel where the inputs are $X_1, X_2 \\in \\{0, 1\\}$ and the outputs are $Y_1, Y_2 \\in \\{0, 1\\}$. The channel is defined by the following additive noise model:\n$$Y_1 = X_1 \\oplus N_1$$\n$$Y_2 = X_2 \\oplus N_2$$\nwhere $\\oplus$ denotes addition modulo 2. The binary noise variables $N_1$ and $N_2$ are not independent of the inputs. Specifically, the noise on one link depends on the signal transmitted on the other link. This dependence is characterized by the following conditional probabilities:\n$$P(N_1=1 | X_2=x_2) = p(1-x_2)$$\n$$P(N_2=1 | X_1=x_1) = q(1-x_1)$$\nwhere $p, q \\in (0, 1)$ are constant channel parameters. This channel model is an \"Asymmetric Quieting Interference Channel\" (AQIC), where transmitting a '1' on the interfering link \"quiets\" the noise on the other link, making it a perfect channel.\n\nTwo different achievable sum-rates can be obtained from the Han-Kobayashi achievable region by considering two distinct decoding strategies at the receivers. These sum-rates are functions of the input probability distribution $p(x_1, x_2)$:\n1.  $S_A = I(X_1; Y_1 | X_2) + I(X_2; Y_2)$: This corresponds to a strategy where receiver 1 first decodes the interference from sender 2 and cancels it, while receiver 2 treats the interference from sender 1 as noise.\n2.  $S_B = I(X_1; Y_1) + I(X_2; Y_2 | X_1)$: This corresponds to the symmetric strategy where receiver 2 decodes and cancels interference, while receiver 1 treats interference as noise.\n\nTo compare these two strategies for a simple and common signaling scheme, assume the inputs are independent and uniformly distributed, i.e., $X_1 \\sim \\text{Bernoulli}(1/2)$ and $X_2 \\sim \\text{Bernoulli}(1/2)$.\n\nDerive the value of the difference $\\Delta S = S_A - S_B$ for this specific input distribution. Express your answer in terms of the channel parameters $p$ and $q$. Use the binary entropy function $h(x) = -x \\log_2 x - (1-x) \\log_2(1-x)$.", "solution": "We are to compute $\\Delta S = S_A - S_B$ for the given input distribution $X_1 \\sim \\text{Bernoulli}(1/2)$ and $X_2 \\sim \\text{Bernoulli}(1/2)$, independent. The sum-rates are:\n\n$$\nS_A = I(X_1; Y_1 | X_2) + I(X_2; Y_2), \\quad S_B = I(X_1; Y_1) + I(X_2; Y_2 | X_1).\n$$\n\n\nFirst, compute $S_A$:\n- $I(X_1; Y_1 | X_2)$: Given $X_2$, the channel from $X_1$ to $Y_1$ is:\n  - If $X_2 = 0$, $Y_1 = X_1 \\oplus N_1$ with $P(N_1=1) = p$, so a BSC($p$), and $I(X_1; Y_1 | X_2=0) = 1 - h(p)$.\n  - If $X_2 = 1$, $N_1 = 0$ almost surely, so $Y_1 = X_1$, and $I(X_1; Y_1 | X_2=1) = 1$.\n  Thus,\n  \n$$\n  I(X_1; Y_1 | X_2) = P(X_2=0)(1 - h(p)) + P(X_2=1) \\cdot 1 = \\frac{1}{2}(1 - h(p)) + \\frac{1}{2} \\cdot 1 = 1 - \\frac{1}{2}h(p).\n  $$\n\n- $I(X_2; Y_2)$: Since $N_2$ depends only on $X_1$ (independent of $X_2$), $P(N_2=1) = E[q(1-X_1)] = q(1 - 1/2) = q/2$, so the channel from $X_2$ to $Y_2$ is BSC($q/2$). With $X_2$ uniform, $Y_2$ is uniform, so:\n  \n$$\n  I(X_2; Y_2) = H(Y_2) - H(Y_2|X_2) = 1 - h(q/2).\n  $$\n\n- Therefore,\n  \n$$\n  S_A = \\left(1 - \\frac{1}{2}h(p)\\right) + \\left(1 - h(q/2)\\right) = 2 - \\frac{1}{2}h(p) - h(q/2).\n  $$\n\n\nNext, compute $S_B$:\n- $I(X_2; Y_2 | X_1)$: Given $X_1$, the channel from $X_2$ to $Y_2$ is:\n  - If $X_1 = 0$, $Y_2 = X_2 \\oplus N_2$ with $P(N_2=1) = q$, so a BSC($q$), and $I(X_2; Y_2 | X_1=0) = 1 - h(q)$.\n  - If $X_1 = 1$, $N_2 = 0$ almost surely, so $Y_2 = X_2$, and $I(X_2; Y_2 | X_1=1) = 1$.\n  Thus,\n  \n$$\n  I(X_2; Y_2 | X_1) = P(X_1=0)(1 - h(q)) + P(X_1=1) \\cdot 1 = \\frac{1}{2}(1 - h(q)) + \\frac{1}{2} \\cdot 1 = 1 - \\frac{1}{2}h(q).\n  $$\n\n- $I(X_1; Y_1)$: Since $N_1$ depends only on $X_2$ (independent of $X_1$), $P(N_1=1) = E[p(1-X_2)] = p(1 - 1/2) = p/2$, so the channel from $X_1$ to $Y_1$ is BSC($p/2$). With $X_1$ uniform, $Y_1$ is uniform, so:\n  \n$$\n  I(X_1; Y_1) = H(Y_1) - H(Y_1|X_1) = 1 - h(p/2).\n  $$\n\n- Therefore,\n  \n$$\n  S_B = \\left(1 - h(p/2)\\right) + \\left(1 - \\frac{1}{2}h(q)\\right) = 2 - h(p/2) - \\frac{1}{2}h(q).\n  $$\n\n\nNow, compute $\\Delta S$:\n\n$$\n\\Delta S = S_A - S_B = \\left(2 - \\frac{1}{2}h(p) - h(q/2)\\right) - \\left(2 - h(p/2) - \\frac{1}{2}h(q)\\right) = h(p/2) - \\frac{1}{2}h(p) + \\frac{1}{2}h(q) - h(q/2).\n$$\n\n\nThis is the expression for $\\Delta S$ in terms of $p$ and $q$, using the binary entropy function $h$.", "answer": "$$\\boxed{ h\\left(\\frac{p}{2}\\right) - \\frac{1}{2} h(p) + \\frac{1}{2} h(q) - h\\left(\\frac{q}{2}\\right) }$$", "id": "53414"}]}