{"hands_on_practices": [{"introduction": "We begin with a foundational model in network information theory: the binary adder channel. This exercise [@problem_id:1663770] challenges you to determine the maximum total information throughput (the sum-rate capacity) for two users sharing this simple, deterministic channel. By working through this problem, you will apply the core principle that for deterministic multiple-access channels, maximizing the sum-rate is equivalent to maximizing the entropy of the channel's output, $H(Y)$.", "problem": "Two independent environmental sensors, Sensor 1 and Sensor 2, are deployed to monitor a certain condition. Each sensor produces a binary output: $X_1 \\in \\{0, 1\\}$ for Sensor 1 and $X_2 \\in \\{0, 1\\}$ for Sensor 2. An output of '1' signifies the detection of a specific event, while '0' signifies its absence. The sensors transmit their signals simultaneously over a shared, noiseless communication channel to a central processing hub. The physical nature of the channel is such that it acts as a binary adder; the signal $Y$ received by the hub is the arithmetic sum of the individual sensor signals, i.e., $Y = X_1 + X_2$.\n\nThe events detected by the sensors are statistically independent. The probability that Sensor 1 detects an event is given by $P(X_1=1) = p_1$, and the corresponding probability for Sensor 2 is $P(X_2=1) = p_2$. The pair of probabilities $(p_1, p_2)$ defines the input distribution for this system, which can be configured by calibrating the sensors.\n\nThe goal is to configure the sensor sensitivities (i.e., choose the values of $p_1$ and $p_2$) to maximize the total information throughput from the sensors to the hub. This corresponds to finding the sum-rate capacity of this Multiple-Access Channel (MAC).\n\nWhich of the following options correctly identifies the maximum achievable sum-rate ($R_1 + R_2$) in bits per channel use and the corresponding input probability distribution $(p_1, p_2)$ that achieves this maximum?\n\nA. Maximum sum-rate of $1.0$ bit per channel use, achieved with input probabilities $(p_1, p_2) = (0.5, 0.5)$.\n\nB. Maximum sum-rate of $\\log_2(3)$ bits per channel use, achieved with input probabilities $(p_1, p_2) = (0.5, 0.5)$.\n\nC. Maximum sum-rate of $1.5$ bits per channel use, achieved with input probabilities $(p_1, p_2) = (0.5, 0.5)$.\n\nD. Maximum sum-rate of $1.0$ bit per channel use, achieved with input probabilities $(p_1, p_2) = (1.0, 0.5)$.\n\nE. Maximum sum-rate of $2.0$ bits per channel use, achieved with input probabilities $(p_1, p_2) = (0.5, 0.5)$.", "solution": "We model the binary-adder multiple-access channel with independent Bernoulli inputs $X_{1} \\sim \\mathrm{Bern}(p_{1})$ and $X_{2} \\sim \\mathrm{Bern}(p_{2})$. The channel is deterministic: $Y = X_{1} + X_{2}$ with output alphabet $\\{0,1,2\\}$.\n\nFor a memoryless deterministic MAC, the capacity region satisfies\n$$\nR_{1} \\leq H(X_{1} \\mid X_{2}), \\quad R_{2} \\leq H(X_{2} \\mid X_{1}), \\quad R_{1} + R_{2} \\leq H(Y),\n$$\nwith entropies in bits (base-$2$). Since $X_{1}$ and $X_{2}$ are independent, $H(X_{1} \\mid X_{2}) = H(X_{1})$ and $H(X_{2} \\mid X_{1}) = H(X_{2})$. Therefore, the maximum achievable sum-rate equals\n$$\n\\max_{p_{1},p_{2}} \\min\\big(H(X_{1}) + H(X_{2}),\\, H(Y)\\big).\n$$\nBecause $H(Y) \\leq \\log_{2}(3)$ and $H(X_{1}) + H(X_{2}) \\leq 2$, we can choose $p_{1} = p_{2} = \\frac{1}{2}$ to ensure $H(X_{1}) + H(X_{2}) = 2 \\geq H(Y)$, so the binding constraint for the sum-rate is $H(Y)$. Hence the sum-capacity is\n$$\n\\max_{p_{1},p_{2}} H(Y).\n$$\n\nWe compute the distribution of $Y$ as a function of $(p_{1}, p_{2})$:\n$$\nP(Y=0) = (1-p_{1})(1-p_{2}), \\quad P(Y=1) = p_{1} + p_{2} - 2 p_{1} p_{2}, \\quad P(Y=2) = p_{1} p_{2}.\n$$\nThus\n$$\nH(Y) = -\\sum_{y \\in \\{0,1,2\\}} P(Y=y)\\, \\log_{2} P(Y=y).\n$$\n\nFirst, the absolute upper bound $H(Y) \\leq \\log_{2}(3)$ would require a uniform output distribution $P(Y=0) = P(Y=1) = P(Y=2) = \\frac{1}{3}$. Setting $P(Y=2) = p_{1} p_{2} = \\frac{1}{3}$ and $P(Y=0) = (1-p_{1})(1-p_{2}) = \\frac{1}{3}$ implies with $s := p_{1} + p_{2}$ and $t := p_{1} p_{2}$ that $t = \\frac{1}{3}$ and\n$$\n1 - s + t = \\frac{1}{3} \\quad \\Rightarrow \\quad s - t = \\frac{2}{3} \\quad \\Rightarrow \\quad s = 1.\n$$\nWith $s=1$ and $t=\\frac{1}{3}$, the quadratic $z^{2} - s z + t = 0$ becomes $z^{2} - z + \\frac{1}{3} = 0$, whose discriminant is $1 - \\frac{4}{3} = -\\frac{1}{3} < 0$, so there are no real $(p_{1}, p_{2})$ achieving uniform $Y$. Hence $H(Y) < \\log_{2}(3)$ for all feasible $(p_{1}, p_{2})$.\n\nNext, to maximize $H(Y)$ over feasible $(p_{1}, p_{2})$, observe that for fixed $P(Y=1)$, the sum $P(Y=0) + P(Y=2)$ is fixed, and by concavity of entropy, $H(Y)$ is maximized when $P(Y=0) = P(Y=2)$. This equality requires\n$$\n(1-p_{1})(1-p_{2}) = p_{1} p_{2} \\quad \\Leftrightarrow \\quad 1 - p_{1} - p_{2} = 0 \\quad \\Leftrightarrow \\quad p_{1} + p_{2} = 1.\n$$\nThus it suffices to optimize along $p_{1} + p_{2} = 1$. Define $t := p_{1} p_{2}$. Under $p_{1} + p_{2} = 1$, the distribution becomes\n$$\nP(Y=0) = t, \\quad P(Y=1) = 1 - 2 t, \\quad P(Y=2) = t,\n$$\nwith feasibility $0 \\leq t \\leq \\frac{1}{4}$ (the maximum attained at $p_{1} = p_{2} = \\frac{1}{2}$). The entropy as a function of $t$ is\n$$\nH(t) = -2 t \\log_{2} t - (1 - 2 t) \\log_{2}(1 - 2 t).\n$$\nDifferentiate:\n$$\nH'(t) = 2 \\log_{2}\\!\\left(\\frac{1 - 2 t}{t}\\right).\n$$\nSetting $H'(t) = 0$ gives $1 - 2 t = t$, i.e., $t = \\frac{1}{3}$, which is outside the feasible interval $[0, \\frac{1}{4}]$. Since\n$$\nH''(t) = \\frac{2}{\\ln 2}\\!\\left(-\\frac{2}{1 - 2 t} - \\frac{1}{t}\\right) < 0 \\quad \\text{for } t \\in (0, \\tfrac{1}{2}),\n$$\n$H(t)$ is strictly concave on the feasible interval, so the maximum over $[0, \\frac{1}{4}]$ occurs at the endpoint $t = \\frac{1}{4}$. This corresponds to $p_{1} = p_{2} = \\frac{1}{2}$ and yields\n$$\nH(Y) = -2 \\cdot \\frac{1}{4} \\log_{2}\\!\\left(\\frac{1}{4}\\right) - \\left(1 - 2 \\cdot \\frac{1}{4}\\right) \\log_{2}\\!\\left(1 - 2 \\cdot \\frac{1}{4}\\right)\n= -\\frac{1}{2} \\log_{2}\\!\\left(\\frac{1}{4}\\right) - \\frac{1}{2} \\log_{2}\\!\\left(\\frac{1}{2}\\right)\n= \\frac{3}{2}.\n$$\n\nTherefore, the maximum achievable sum-rate is $\\frac{3}{2}$ bits per channel use, achieved by $(p_{1}, p_{2}) = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$. Among the provided options, this corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1663770"}, {"introduction": "How does the physical structure of a channel affect its ability to carry information? This practice [@problem_id:1663791] invites you to explore this question by comparing the sum-rate capacity of the binary adder channel with that of a binary OR channel. This comparative analysis is a powerful way to build intuition, revealing how the number of distinct output levels directly influences the maximum achievable data rate.", "problem": "Consider two different noiseless two-user Multiple-Access Channels (MACs). In both channels, two independent users, User 1 and User 2, transmit binary symbols $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{0, 1\\}$, respectively. The users can choose their input probabilities $P(X_1=1)$ and $P(X_2=1)$ to maximize the total information rate.\n\nIn Channel A, the output $Y_A$ is the arithmetic sum of the inputs:\n$$Y_A = X_1 + X_2$$\nThe output alphabet for this channel is $\\mathcal{Y}_A = \\{0, 1, 2\\}$.\n\nIn Channel B, the output $Y_B$ is the logical OR of the inputs:\n$$Y_B = X_1 \\lor X_2$$\nThe output alphabet for this channel is $\\mathcal{Y}_B = \\{0, 1\\}$.\n\nLet $C_{\\text{sum, A}}$ be the sum-rate capacity of Channel A, and let $C_{\\text{sum, B}}$ be the sum-rate capacity of Channel B. Both capacities are measured in bits per channel use. Which of the following statements correctly describes the relationship between these two capacities?\n\nA. $C_{\\text{sum, A}} > C_{\\text{sum, B}}$\n\nB. $C_{\\text{sum, A}} < C_{\\text{sum, B}}$\n\nC. $C_{\\text{sum, A}} = C_{\\text{sum, B}}$\n\nD. The relationship cannot be determined from the information given.", "solution": "For a deterministic multiple-access channel with independent users and a given product input distribution, the sum-rate is\n$$I(X_{1},X_{2};Y) = H(Y) - H(Y \\mid X_{1},X_{2}) = H(Y),$$\nsince $Y$ is a deterministic function of $(X_{1},X_{2})$ and thus $H(Y \\mid X_{1},X_{2})=0$. Therefore, the sum-rate capacity equals\n$$C_{\\text{sum}} = \\max_{P_{X_{1}} P_{X_{2}}} H(Y).$$\n\nChannel A: $Y_{A} = X_{1} + X_{2}$ with $\\mathcal{Y}_{A}=\\{0,1,2\\}$. Let $p_{1} = P(X_{1}=1)$ and $p_{2} = P(X_{2}=1)$. Then\n$$P(Y_{A}=0) = (1-p_{1})(1-p_{2}), \\quad P(Y_{A}=1) = p_{1} + p_{2} - 2 p_{1} p_{2}, \\quad P(Y_{A}=2) = p_{1} p_{2}.$$\nThus,\n$$H(Y_{A}) = -\\sum_{y \\in \\{0,1,2\\}} P(Y_{A}=y) \\log_{2} P(Y_{A}=y).$$\nTo locate stationary points, compute the partial derivatives. Define $P_{0}=(1-p_{1})(1-p_{2})$, $P_{1}=p_{1}+p_{2}-2p_{1}p_{2}$, $P_{2}=p_{1}p_{2}$. Using that for $f(P)=-P \\log_{2} P$ and an affine dependence $P(\\theta)$, $\\frac{\\partial}{\\partial \\theta}\\sum f(P)= -\\sum \\frac{\\partial P}{\\partial \\theta} \\log_{2} P$ (since $\\sum P=1$), we get\n$$\\frac{\\partial H}{\\partial p_{1}} = -\\left[-(1-p_{2}) \\log_{2} P_{0} + (1-2p_{2}) \\log_{2} P_{1} + p_{2} \\log_{2} P_{2}\\right],$$\n$$\\frac{\\partial H}{\\partial p_{2}} = -\\left[-(1-p_{1}) \\log_{2} P_{0} + (1-2p_{1}) \\log_{2} P_{1} + p_{1} \\log_{2} P_{2}\\right].$$\nSubtracting these yields the stationarity condition\n$$(p_{1}-p_{2})\\left[-2 \\log_{2} P_{1} + \\log_{2} P_{2} + \\log_{2} P_{0}\\right]=0.$$\nHence any interior stationary point must satisfy either $p_{1}=p_{2}$ or $P_{1}^{2} = P_{0} P_{2}$. Note $P_{0}P_{2}=(1-p_{1})(1-p_{2})p_{1}p_{2} = p_{1}(1-p_{1})p_{2}(1-p_{2}) = P_{01}P_{10}$, where $P_{01}=(1-p_{1})p_{2}$ and $P_{10}=p_{1}(1-p_{2})$, while $P_{1}=P_{01}+P_{10}$. By the inequality $(u+v)^{2} \\geq 4uv$, $P_{1}^{2} \\geq 4 P_{01}P_{10}=4P_{0}P_{2}$, so $P_{1}^{2} = P_{0} P_{2}$ can only hold at degenerate boundary points where $P_{01}P_{10}=0$. Therefore the only nondegenerate stationary points satisfy $p_{1}=p_{2}$.\n\nSet $p_{1}=p_{2}=p$. Then $Y_{A}$ has distribution\n$$P(Y_{A}=0)=(1-p)^{2}, \\quad P(Y_{A}=1)=2p(1-p), \\quad P(Y_{A}=2)=p^{2},$$\nso\n$$H(Y_{A}) = - (1-p)^{2} \\log_{2}(1-p)^{2} - 2p(1-p) \\log_{2}\\!\\big(2p(1-p)\\big) - p^{2} \\log_{2} p^{2}.$$\nBy symmetry $H(Y_{A})$ is maximized at $p=\\frac{1}{2}$, yielding\n$$P(Y_{A}) = \\left(\\frac{1}{4}, \\frac{1}{2}, \\frac{1}{4}\\right), \\quad H(Y_{A}) = -\\frac{1}{4}\\log_{2}\\frac{1}{4} - \\frac{1}{2}\\log_{2}\\frac{1}{2} - \\frac{1}{4}\\log_{2}\\frac{1}{4} = \\frac{3}{2} \\text{ bits}.$$\nAt the boundaries $p \\in \\{0,1\\}$ the entropy is zero, so $p=\\frac{1}{2}$ is the global maximizer. Therefore\n$$C_{\\text{sum, A}} = \\max_{p_{1},p_{2}} H(Y_{A}) = \\frac{3}{2} \\text{ bits per use}.$$\n\nChannel B: $Y_{B} = X_{1} \\lor X_{2}$ with $\\mathcal{Y}_{B}=\\{0,1\\}$. Here\n$$P(Y_{B}=0) = (1-p_{1})(1-p_{2}), \\quad P(Y_{B}=1) = 1 - (1-p_{1})(1-p_{2}).$$\nThus\n$$H(Y_{B}) = h\\!\\left(1 - (1-p_{1})(1-p_{2})\\right),$$\nwhere $h(u) = -u \\log_{2} u - (1-u)\\log_{2}(1-u)$ is the binary entropy function. Since $H(Y_{B}) \\leq 1$ for a binary output and equality is achieved when $P(Y_{B}=0)=P(Y_{B}=1)=\\frac{1}{2}$, choose, for example, $p_{1}=0$ and $p_{2}=\\frac{1}{2}$ to make $(1-p_{1})(1-p_{2})=\\frac{1}{2}$. Therefore\n$$C_{\\text{sum, B}} = \\max_{p_{1},p_{2}} H(Y_{B}) = 1 \\text{ bit per use}.$$\n\nComparing,\n$$C_{\\text{sum, A}} = \\frac{3}{2} \\quad \\text{and} \\quad C_{\\text{sum, B}} = 1,$$\nso\n$$C_{\\text{sum, A}} > C_{\\text{sum, B}}.$$", "answer": "$$\\boxed{A}$$", "id": "1663791"}, {"introduction": "Moving from discrete models to the continuous domain, we now consider the ubiquitous Gaussian Multiple-Access Channel, a cornerstone model for wireless communication. This problem [@problem_id:1663805] addresses a critical question of resource management: how should a fixed total power budget be allocated among users to maximize the overall data throughput? The solution reveals a fundamental and elegant principle of power allocation for sum-rate maximization in this noisy environment.", "problem": "Two autonomous environmental sensors, Sensor 1 and Sensor 2, are deployed in a remote location to monitor air and soil quality, respectively. They simultaneously transmit their data to a central receiver. This communication system can be modeled as a two-user Gaussian Multiple-Access Channel (MAC).\n\nThe signal received at the central hub is given by the linear sum of the two transmitted signals plus noise:\n$$Y = X_1 + X_2 + Z$$\nHere, $X_1$ and $X_2$ are the signals from Sensor 1 and Sensor 2, which are treated as independent random variables. $Z$ is Additive White Gaussian Noise (AWGN) with a mean of zero and a variance (power) of $N$. The powers of the transmitted signals are $P_1 = E[X_1^2]$ and $P_2 = E[X_2^2]$.\n\nThe sensors share a common power source, imposing a total power constraint $P$. This means any power allocation $(P_1, P_2)$ must satisfy $P_1 + P_2 \\le P$. The goal of the system design is to maximize the sum-rate capacity, $C_{sum} = R_1 + R_2$, which represents the maximum total data rate that can be reliably transmitted from both sensors to the receiver.\n\nWhich of the following power allocation strategies, $(P_1, P_2)$, will achieve the maximum possible sum-rate capacity for this channel?\n\nA. Allocate all available power to Sensor 1 ($P_1 = P, P_2 = 0$).\n\nB. Allocate all available power to Sensor 2 ($P_1 = 0, P_2 = P$).\n\nC. Allocate the power equally between the two sensors ($P_1 = P/2, P_2 = P/2$).\n\nD. Any allocation $(P_1, P_2)$ that uses the total available power, i.e., for which $P_1 + P_2 = P$, will achieve the same maximum sum-rate.\n\nE. The allocation must be asymmetric but non-zero for both, for example, $P_1 = P/3, P_2 = 2P/3$.", "solution": "We model the uplink as a two-user Gaussian multiple-access channel with $Y=X_{1}+X_{2}+Z$, where $Z$ is zero-mean Gaussian with variance $N$ and $X_{1}$, $X_{2}$ are independent with powers $P_{1}$ and $P_{2}$. For any fixed power allocation $(P_{1},P_{2})$, the sum-rate is upper bounded by the mutual information\n$$\nR_{1}+R_{2} \\le I(X_{1},X_{2};Y).\n$$\nUsing the chain rule of entropy and the additive model, we compute\n$$\nI(X_{1},X_{2};Y)=h(Y)-h(Y|X_{1},X_{2})=h(X_{1}+X_{2}+Z)-h(Z),\n$$\nsince $Y|X_{1},X_{2}=Z$. For given second moments $E[X_{1}^{2}]=P_{1}$, $E[X_{2}^{2}]=P_{2}$ and independent $X_{1},X_{2}$, the variance of $Y$ is\n$$\n\\operatorname{Var}(Y)=P_{1}+P_{2}+N.\n$$\nAmong all distributions with a fixed variance, the Gaussian maximizes differential entropy. Thus the maximum of $I(X_{1},X_{2};Y)$ over input distributions with given powers is achieved when $X_{1}$ and $X_{2}$ are Gaussian, yielding\n$$\nI(X_{1},X_{2};Y)=\\frac{1}{2}\\log_2\\!\\left(2\\pi e\\,(P_{1}+P_{2}+N)\\right)-\\frac{1}{2}\\log_2\\!\\left(2\\pi e\\,N\\right)=\\frac{1}{2}\\log_2\\!\\left(1+\\frac{P_{1}+P_{2}}{N}\\right).\n$$\nTherefore, the sum-rate bound and its maximum depend only on the total transmit power $P_{1}+P_{2}$, not on how it is split between the two users. Under the total power constraint $P_{1}+P_{2}\\le P$, the sum-rate is maximized by using the full power, i.e., any allocation satisfying $P_{1}+P_{2}=P$. Hence all such allocations achieve the same maximum sum-rate\n$$\nC_{\\text{sum}}^{\\star}=\\frac{1}{2}\\log_2\\!\\left(1+\\frac{P}{N}\\right),\n$$\nand no particular asymmetric or single-user allocation is required; allocating all power to one sensor or splitting it equally are all special cases of using the full power and thus achieve the same sum-rate maximum.", "answer": "$$\\boxed{D}$$", "id": "1663805"}]}