{"hands_on_practices": [{"introduction": "We begin our exploration of the Multiple-Access Channel (MAC) with a foundational case to build our intuition. This exercise [@problem_id:1608075] models a scenario where the channel only transmits one user's signal while completely ignoring the other. By applying the general formulas for the capacity region, you will confirm that the mathematical framework produces the correct, intuitive result, establishing a solid baseline before we tackle more complex interactions.", "problem": "A two-user Multiple Access Channel (MAC) is defined by two binary inputs, $X_1 \\in \\{0, 1\\}$ from User 1 and $X_2 \\in \\{0, 1\\}$ from User 2, and a single output $Y$. The inputs $X_1$ and $X_2$ are transmitted independently. The channel is deterministic and is characterized by the relationship $Y = X_1$, meaning the output is always identical to the input from User 1 and completely unaffected by the input from User 2. The communication rates for User 1 and User 2 are denoted by $R_1$ and $R_2$ respectively, with units of bits per channel use.\n\nWhich of the following options correctly describes the capacity region $\\mathcal{C}$, which is the set of all achievable rate pairs $(R_1, R_2)$ for this channel?\n\nA. A square region with corners at (0,0), (1,0), (1,1), and (0,1).\n\nB. A triangular region with corners at (0,0), (1,0), and (0,1).\n\nC. A line segment on the $R_1$-axis, connecting the points (0,0) and (1,0).\n\nD. A line segment on the $R_2$-axis, connecting the points (0,0) and (0,1).\n\nE. A single point at the origin (0,0).", "solution": "We use the standard capacity region of a two-user memoryless MAC. For any product input distribution $p(x_{1})p(x_{2})$, the achievable rate pairs $(R_{1},R_{2})$ satisfy\n$$\n\\begin{aligned}\nR_{1} &\\leq I(X_{1};Y\\mid X_{2}),\\\\\nR_{2} &\\leq I(X_{2};Y\\mid X_{1}),\\\\\nR_{1}+R_{2} &\\leq I(X_{1},X_{2};Y),\n\\end{aligned}\n$$\nand the capacity region $\\mathcal{C}$ is the convex closure (e.g., via time-sharing) of the union of these regions over all such input distributions.\n\nGiven the deterministic channel $Y=X_{1}$, we compute each mutual information term.\n\nFirst, for $I(X_{2};Y\\mid X_{1})$:\n$$\n\\begin{aligned}\nI(X_{2};Y\\mid X_{1}) &= H(Y\\mid X_{1}) - H(Y\\mid X_{1},X_{2})\\\\\n&= H(X_{1}\\mid X_{1}) - H(X_{1}\\mid X_{1},X_{2})\\\\\n&= 0 - 0\\\\\n&= 0.\n\\end{aligned}\n$$\nThus $R_{2} \\leq 0$, which forces $R_{2}=0$.\n\nNext, for $I(X_{1};Y\\mid X_{2})$:\n$$\n\\begin{aligned}\nI(X_{1};Y\\mid X_{2}) &= H(Y\\mid X_{2}) - H(Y\\mid X_{1},X_{2})\\\\\n&= H(X_{1}\\mid X_{2}) - 0\\\\\n&= H(X_{1}),\n\\end{aligned}\n$$\nwhere we used that $Y=X_{1}$ and $X_{1}$ and $X_{2}$ are independent, so $H(X_{1}\\mid X_{2})=H(X_{1})$. Therefore $R_{1} \\leq H(X_{1})$.\n\nFor the sum-rate:\n$$\n\\begin{aligned}\nI(X_{1},X_{2};Y) &= H(Y) - H(Y\\mid X_{1},X_{2})\\\\\n&= H(X_{1}) - 0\\\\\n&= H(X_{1}),\n\\end{aligned}\n$$\nso $R_{1}+R_{2} \\leq H(X_{1})$, which is redundant given $R_{2}=0$ and $R_{1} \\leq H(X_{1})$.\n\nFor binary $X_{1}$ with logs in base $2$, $H(X_{1}) \\leq 1$, with equality when $X_{1}$ is Bernoulli with parameter $\\frac{1}{2}$. Taking the union over all product input distributions $p(x_{1})p(x_{2})$ yields $0 \\leq R_{1} \\leq 1$ and $R_{2}=0$. Convexification via time-sharing does not introduce any $R_{2}>0$ points, so the capacity region is exactly the line segment on the $R_{1}$-axis from $(0,0)$ to $(1,0)$.\n\nHence the correct option is the line segment on the $R_{1}$-axis, i.e., option C.", "answer": "$$\\boxed{C}$$", "id": "1608075"}, {"introduction": "Moving from a non-interactive channel, we now examine a simple, deterministic MAC where user signals combine at the receiver [@problem_id:1608091]. In this hypothetical setup, the receiver observes the arithmetic sum of two binary inputs, creating interference between the users. This practice will guide you in calculating the maximum symmetric rate, illustrating how the sum-rate constraint naturally arises from input ambiguity even in a noiseless channel.", "problem": "Two independent, minimalistic sensors are deployed to monitor a specific environmental condition. Sensor 1 transmits a binary value $X_1$, and Sensor 2 transmits a binary value $X_2$, where $X_1, X_2 \\in \\{0, 1\\}$. These two sensors transmit their data simultaneously to a central processing unit over a shared, noiseless communication channel.\n\nThe communication hardware is designed such that the signal received by the processing unit, $Y$, is the arithmetic sum of the two transmitted bits: $Y = X_1 + X_2$. Consequently, the alphabet for the received signal is $Y \\in \\{0, 1, 2\\}$.\n\nThe sensors must be configured to operate at the same transmission rate, i.e., $R_1 = R_2 = R$. Determine the maximum possible value for this symmetric rate $R$. Express your final answer as a single real number representing the rate in units of bits per channel use.", "solution": "We model the uplink as a discrete memoryless multiple-access channel with two independent binary inputs $X_{1},X_{2}\\in\\{0,1\\}$ and deterministic output $Y=X_{1}+X_{2}\\in\\{0,1,2\\}$. For such a MAC, the capacity region is given by the union over all product input distributions $P_{X_{1}}P_{X_{2}}$ of all rate pairs $(R_{1},R_{2})$ satisfying\n$$\nR_{1}\\leq I(X_{1};Y\\mid X_{2}),\\quad R_{2}\\leq I(X_{2};Y\\mid X_{1}),\\quad R_{1}+R_{2}\\leq I(X_{1},X_{2};Y),\n$$\nwhere all entropies and mutual informations are measured in bits, i.e., with $\\log_{2}$.\n\nBecause the channel is deterministic, $H(Y\\mid X_{1},X_{2})=0$, so\n$$\nI(X_{1},X_{2};Y)=H(Y).\n$$\nMoreover,\n$$\nI(X_{1};Y\\mid X_{2})=H(Y\\mid X_{2})-H(Y\\mid X_{1},X_{2})=H(Y\\mid X_{2}),\n$$\nand similarly $I(X_{2};Y\\mid X_{1})=H(Y\\mid X_{1})$. For the adder $Y=X_{1}+X_{2}$, conditioning on $X_{2}=0$ yields $Y=X_{1}$, and conditioning on $X_{2}=1$ yields $Y=1+X_{1}$, which is a bijection of $X_{1}$. Hence $H(Y\\mid X_{2})=H(X_{1})$ and, by symmetry, $H(Y\\mid X_{1})=H(X_{2})$. Therefore, for any independent inputs,\n$$\nR_{1}\\leq H(X_{1}),\\quad R_{2}\\leq H(X_{2}),\\quad R_{1}+R_{2}\\leq H(Y).\n$$\n\nLet $X_{1}\\sim\\mathrm{Bern}(p_{1})$ and $X_{2}\\sim\\mathrm{Bern}(p_{2})$ be independent. Then\n$$\nH(X_{1})=h_{2}(p_{1}),\\quad H(X_{2})=h_{2}(p_{2}),\n$$\nwhere $h_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p)$ is the binary entropy function, and the distribution of $Y$ is\n$$\n\\Pr(Y=0)=(1-p_{1})(1-p_{2}),\\quad \\Pr(Y=1)=p_{1}+p_{2}-2p_{1}p_{2},\\quad \\Pr(Y=2)=p_{1}p_{2},\n$$\nso that\n$$\nH(Y)=H\\big((1-p_{1})(1-p_{2}),\\, p_{1}+p_{2}-2p_{1}p_{2},\\, p_{1}p_{2}\\big).\n$$\n\nFor symmetric operation $R_{1}=R_{2}=R$, the constraints reduce to\n$$\nR\\leq h_{2}(p_{1}),\\quad R\\leq h_{2}(p_{2}),\\quad 2R\\leq H(Y).\n$$\nMaximizing the symmetric rate requires maximizing the minimum of these three bounds over $p_{1},p_{2}$. By symmetry, it is optimal to set $p_{1}=p_{2}=p$. Then $H(X_{1})=H(X_{2})=h_{2}(p)$ and\n$$\n\\Pr(Y=0)=(1-p)^{2},\\quad \\Pr(Y=1)=2p(1-p),\\quad \\Pr(Y=2)=p^{2}.\n$$\nThe entropy $H(Y)$ is maximized at $p=\\frac{1}{2}$ (by symmetry and concavity of entropy), yielding\n$$\nH(X_{1})=H(X_{2})=h_{2}\\!\\left(\\tfrac{1}{2}\\right)=1,\\quad H(Y)=H\\!\\left(\\tfrac{1}{4},\\tfrac{1}{2},\\tfrac{1}{4}\\right)=\\tfrac{3}{2}.\n$$\nThus, at $p=\\frac{1}{2}$, the symmetric constraints are\n$$\nR\\leq 1,\\quad 2R\\leq \\tfrac{3}{2}\\ \\Rightarrow\\ R\\leq \\tfrac{3}{4}.\n$$\nThe sum-rate bound is the active constraint, so the maximum symmetric rate is $R^*=\\frac{3}{4}$ bits per channel use.\nThis rate is achievable with independent equiprobable inputs, and is optimal since any $(R,R)$ must satisfy $2R\\leq H(Y)\\leq \\frac{3}{2}$ for this channel.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1608091"}, {"introduction": "Our final practice explores a more complex scenario that tests the understanding of information and redundancy [@problem_id:1608093]. Building upon the binary sum channel, we imagine the receiver obtains two separate outputs: the noiseless sum and a noisy version of the XOR sum of the inputs. This problem challenges you to determine if this additional observation expands the capacity region, leading to a powerful insight about what constitutes useful versus redundant information in communication systems.", "problem": "Consider a two-user binary-input Multiple Access Channel (MAC). User 1 and User 2 select inputs $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{0, 1\\}$ respectively, which are assumed to be independent. The receiver observes a pair of outputs $Y = (Y_1, Y_2)$ for each use of the channel.\n\nThe first output, $Y_1$, is the noiseless arithmetic sum of the inputs:\n$$Y_1 = X_1 + X_2$$\nThe second output, $Y_2$, is obtained by passing the modulo-2 sum (XOR) of the inputs through a Binary Symmetric Channel (BSC). Specifically:\n$$Y_2 = (X_1 \\oplus X_2) \\oplus Z$$\nwhere $\\oplus$ denotes addition modulo 2, and $Z$ is a Bernoulli random variable with $P(Z=1) = p$ and $P(Z=0) = 1-p$, for a fixed crossover probability $0 < p < 1/2$. The noise variable $Z$ is independent of the inputs $X_1$ and $X_2$.\n\nThe capacity region $\\mathcal{C}$ is the set of all achievable rate pairs $(R_1, R_2)$, where rates are measured in bits per channel use. Determine the area of the capacity region $\\mathcal{C}$. Express your answer as a single real number.", "solution": "For a two-user discrete memoryless MAC with independent inputs $X_{1}$ and $X_{2}$, the capacity region (with time-sharing) is the union over input distributions $P_{X_{1}}P_{X_{2}}$ of the pentagons defined by\n$$\nR_{1} \\leq I(X_{1};Y \\mid X_{2}), \\quad\nR_{2} \\leq I(X_{2};Y \\mid X_{1}), \\quad\nR_{1}+R_{2} \\leq I(X_{1},X_{2};Y),\n$$\nwhere $Y=(Y_{1},Y_{2})$ is the channel output.\n\nLet $X_{1} \\sim \\mathrm{Bern}(\\alpha)$ and $X_{2} \\sim \\mathrm{Bern}(\\beta)$, independent. All entropies and mutual informations are measured in bits, i.e., with base-$2$ logarithms. Write the binary entropy as $h_{2}(u) = -u \\log_{2} u - (1-u)\\log_{2}(1-u)$.\n\nFirst compute $I(X_{1};Y \\mid X_{2})$. Given $X_{2}$, $Y_{1}=X_{1}+X_{2}$ deterministically reveals $X_{1}$, hence $H(X_{1} \\mid Y,X_{2})=0$. Since $X_{1}$ is independent of $X_{2}$, $H(X_{1} \\mid X_{2})=H(X_{1})=h_{2}(\\alpha)$. Therefore\n$$\nI(X_{1};Y \\mid X_{2}) = H(X_{1} \\mid X_{2}) - H(X_{1} \\mid Y,X_{2}) = h_{2}(\\alpha).\n$$\nBy symmetry,\n$$\nI(X_{2};Y \\mid X_{1}) = h_{2}(\\beta).\n$$\n\nNext compute $I(X_{1},X_{2};Y)$. Since $Y=(Y_{1},Y_{2})$ with $Y_{1}=X_{1}+X_{2}$ deterministic and $Y_{2}=(X_{1}\\oplus X_{2}) \\oplus Z$ where $Z \\sim \\mathrm{Bern}(p)$ is independent of $(X_{1},X_{2})$, we have\n$$\nH(Y \\mid X_{1},X_{2}) = H(Y_{1},Y_{2} \\mid X_{1},X_{2}) = H(Y_{2} \\mid X_{1},X_{2}) = h_{2}(p).\n$$\nMoreover, $S = X_{1} \\oplus X_{2}$ is a deterministic function of $Y_{1}$ (indeed, $S=1$ iff $Y_{1}=1$ and $S=0$ otherwise), so\n$$\nH(Y) = H(Y_{1},Y_{2}) = H(Y_{1}) + H(Y_{2} \\mid Y_{1}) = H(Y_{1}) + h_{2}(p),\n$$\nwhich yields\n$$\nI(X_{1},X_{2};Y) = H(Y) - H(Y \\mid X_{1},X_{2}) = H(Y_{1}).\n$$\nThus the three defining quantities for the pentagon corresponding to $(\\alpha,\\beta)$ are\n$$\na := I(X_{1};Y \\mid X_{2}) = h_{2}(\\alpha), \\quad\nb := I(X_{2};Y \\mid X_{1}) = h_{2}(\\beta), \\quad\nc := I(X_{1},X_{2};Y) = H(Y_{1}),\n$$\nwith $Y_{1} \\in \\{0,1,2\\}$ distributed as\n$$\n\\begin{aligned}\nP(Y_{1}=0) &= (1-\\alpha)(1-\\beta), \\\\\nP(Y_{1}=1) &= \\alpha(1-\\beta) + (1-\\alpha)\\beta, \\\\\nP(Y_{1}=2) &= \\alpha \\beta,\n\\end{aligned}\n$$\nso that $H(Y_{1})$ is the ternary entropy of these masses.\n\nFor a fixed $(a,b,c)$ satisfying $c \\leq a+b$, the corresponding capacity pentagon has area\n$$\n\\mathsf{Area}(a,b,c) = a b - \\frac{1}{2}\\bigl(a + b - c\\bigr)^{2},\n$$\nsince it is the area of the rectangle $[0,a] \\times [0,b]$ minus the right triangle cut off by $R_{1}+R_{2} \\leq c$ with leg lengths $a+b-c$.\n\nWe now maximize these bounds over input distributions. Clearly $a \\leq 1$ with equality at $\\alpha=\\tfrac{1}{2}$, and $b \\leq 1$ with equality at $\\beta=\\tfrac{1}{2}$. The entropy $H(Y_{1})$ is maximized at $\\alpha=\\beta=\\tfrac{1}{2}$, where\n$$\nP(Y_{1}=0)=\\tfrac{1}{4}, \\quad P(Y_{1}=1)=\\tfrac{1}{2}, \\quad P(Y_{1}=2)=\\tfrac{1}{4},\n$$\nso\n$$\nH(Y_{1}) = -\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4} - \\tfrac{1}{2}\\log_{2}\\tfrac{1}{2} - \\tfrac{1}{4}\\log_{2}\\tfrac{1}{4} = \\tfrac{3}{2}.\n$$\nTherefore, at $\\alpha=\\beta=\\tfrac{1}{2}$ we have simultaneously\n$$\na^{\\star}=1, \\quad b^{\\star}=1, \\quad c^{\\star}=\\tfrac{3}{2}.\n$$\nFor any other $(\\alpha,\\beta)$, $a \\leq a^{\\star}$, $b \\leq b^{\\star}$, and $c \\leq c^{\\star}$. Hence every pentagon for any input distribution is contained in the pentagon with $(a^{\\star},b^{\\star},c^{\\star})$, and allowing time-sharing cannot enlarge beyond this maximal pentagon. Consequently, the capacity region equals the pentagon with $a=1$, $b=1$, $c=\\tfrac{3}{2}$, independent of $p$.\n\nIts area is\n$$\n\\mathsf{Area}(\\mathcal{C}) = 1 \\cdot 1 - \\frac{1}{2}\\bigl(1 + 1 - \\tfrac{3}{2}\\bigr)^{2}\n= 1 - \\frac{1}{2}\\bigl(\\tfrac{1}{2}\\bigr)^{2}\n= 1 - \\frac{1}{8}\n= \\frac{7}{8}.\n$$\nThus, the area of the capacity region is $\\tfrac{7}{8}$.", "answer": "$$\\boxed{\\frac{7}{8}}$$", "id": "1608093"}]}