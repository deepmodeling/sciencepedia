{"hands_on_practices": [{"introduction": "The Multiple-Access Channel (MAC) models a scenario where multiple transmitters communicate with a single receiver, a fundamental setup in wireless systems. This exercise [@problem_id:1642856] provides a foundational understanding of the MAC capacity region by examining an extreme case. By analyzing a channel where one user cannot modulate their input signal, you will gain a sharp intuition for why a user's achievable rate is fundamentally linked to their ability to convey information.", "problem": "Consider a two-user discrete memoryless Multiple-Access Channel (MAC). Let the inputs from the two users be denoted by the random variables $X_1$ and $X_2$, and the output of the channel be denoted by $Y$. The channel is a noiseless binary adder channel, defined by the relationship $Y = X_1 \\oplus X_2$, where $\\oplus$ represents addition modulo 2 (the logical XOR operation).\n\nThe input alphabets for the two users are asymmetric. User 1 has an input alphabet $\\mathcal{X}_1 = \\{0, 1\\}$, while User 2 is restricted to a singleton input alphabet $\\mathcal{X}_2 = \\{0\\}$.\n\nThe capacity region $\\mathcal{C}$ of this MAC is the set of all achievable rate pairs $(R_1, R_2)$, where rates are measured in bits per channel use. Which of the following options correctly describes the capacity region $\\mathcal{C}$?\n\nA. The set of points $(R_1, R_2)$ such that $0 \\le R_1 \\le 1$ and $R_2 = 0$.\n\nB. The set of points $(R_1, R_2)$ such that $R_1 = 0$ and $0 \\le R_2 \\le 1$.\n\nC. The set of points $(R_1, R_2)$ such that $0 \\le R_1 \\le 1$ and $0 \\le R_2 \\le 1$.\n\nD. The set of points $(R_1, R_2)$ such that $R_1 \\ge 0, R_2 \\ge 0,$ and $R_1 + R_2 \\le 1$.\n\nE. The set of points $(R_1, R_2)$ such that $R_1 = 1$ and $R_2 = 1$.", "solution": "The problem asks for the capacity region of a specific two-user Multiple-Access Channel (MAC). The capacity region $\\mathcal{C}$ for a two-user MAC is generally defined as the convex hull of the union of all rate pairs $(R_1, R_2)$ satisfying the following inequalities for some product input distribution $p(x_1)p(x_2)$:\n$$R_1 \\ge 0$$\n$$R_2 \\ge 0$$\n$$R_1 \\le I(X_1; Y | X_2)$$\n$$R_2 \\le I(X_2; Y | X_1)$$\n$$R_1 + R_2 \\le I(X_1, X_2; Y)$$\nAll mutual information quantities are measured in bits, implying the use of logarithm base 2.\n\nLet's analyze the given channel properties:\nChannel law: $Y = X_1 \\oplus X_2$\nInput alphabet for User 1: $\\mathcal{X}_1 = \\{0, 1\\}$\nInput alphabet for User 2: $\\mathcal{X}_2 = \\{0\\}$\n\nA crucial aspect of this problem is the input alphabet for User 2. Since $\\mathcal{X}_2$ contains only the symbol 0, the input from User 2 is not a random variable in the usual sense. It is a constant: $X_2 = 0$ with probability 1. This significantly simplifies the channel behavior.\n\nSubstituting $X_2 = 0$ into the channel equation, we get:\n$$Y = X_1 \\oplus 0 = X_1$$\nThe channel output $Y$ is identical to the input from User 1. The channel effectively behaves as a noiseless point-to-point channel for User 1, and User 2 has no ability to modulate the signal.\n\nNow, we evaluate the mutual information terms under this condition.\n\n**Step 1: Evaluate the bound on $R_2$.**\nThe rate for User 2 is bounded by $R_2 \\le I(X_2; Y | X_1)$.\nBy definition, mutual information is $I(X_2; Y | X_1) = H(Y | X_1) - H(Y | X_1, X_2)$.\nSince the channel is noiseless and $Y = X_1$, the output $Y$ is fully determined by the input $X_1$. Therefore, the conditional entropy $H(Y | X_1) = 0$.\nThis immediately implies that $I(X_2; Y | X_1) = 0$.\nSince rates must be non-negative ($R_2 \\ge 0$) and we have the constraint $R_2 \\le 0$, it must be that $R_2 = 0$.\nThis makes intuitive sense: a user that can only transmit a single, constant symbol cannot convey any information, so their achievable data rate is zero.\n\n**Step 2: Evaluate the bound on $R_1$.**\nThe rate for User 1 is bounded by $R_1 \\le I(X_1; Y | X_2)$.\nBy definition, $I(X_1; Y | X_2) = H(Y | X_2) - H(Y | X_1, X_2)$.\nSince $Y = X_1$, the term $H(Y | X_1, X_2) = H(X_1 | X_1, X_2) = 0$.\nThe term $H(Y|X_2)$ is the entropy of $Y$ given that $X_2=0$. But since $X_2$ is always 0, conditioning on it provides no new information about $Y$ that we didn't already have. So, $H(Y | X_2) = H(Y)$.\nAnd because $Y=X_1$, we have $H(Y) = H(X_1)$.\nTherefore, the bound on $R_1$ is $R_1 \\le H(X_1)$.\nTo find the capacity region, we must consider all possible input distributions for User 1, i.e., all $p(x_1)$. The maximum value of $H(X_1)$ for a binary random variable $X_1 \\in \\{0, 1\\}$ occurs when the distribution is uniform, i.e., $P(X_1=0) = P(X_1=1) = 1/2$.\nFor this distribution, the entropy is:\n$$H(X_1) = - \\sum_{x_1 \\in \\mathcal{X}_1} p(x_1) \\log_2 p(x_1) = - \\left( \\frac{1}{2}\\log_2\\frac{1}{2} + \\frac{1}{2}\\log_2\\frac{1}{2} \\right) = - \\left( \\frac{1}{2}(-1) + \\frac{1}{2}(-1) \\right) = 1 \\text{ bit}$$\nThus, the maximum value for the bound is 1, leading to the constraint $R_1 \\le 1$.\n\n**Step 3: Evaluate the sum-rate bound.**\nThe sum rate is bounded by $R_1 + R_2 \\le I(X_1, X_2; Y)$.\nBy definition, $I(X_1, X_2; Y) = H(Y) - H(Y | X_1, X_2)$.\nWe already established that $H(Y | X_1, X_2) = 0$ (since $Y=X_1$).\nSo, $I(X_1, X_2; Y) = H(Y)$. Again, since $Y=X_1$, this becomes $H(X_1)$.\nThe sum-rate constraint is $R_1 + R_2 \\le H(X_1)$.\nThe maximum value of the right-hand side is 1 bit, as found in Step 2. So, $R_1 + R_2 \\le 1$.\n\n**Step 4: Combine the constraints to define the capacity region.**\nWe have the following set of inequalities for the rate pair $(R_1, R_2)$:\n1. $R_1 \\ge 0$\n2. $R_2 = 0$ (from $R_2 \\ge 0$ and $R_2 \\le 0$)\n3. $R_1 \\le 1$\n4. $R_1 + R_2 \\le 1$\n\nSubstituting $R_2 = 0$ from constraint (2) into constraint (4) gives $R_1 + 0 \\le 1$, which is $R_1 \\le 1$. This is identical to constraint (3).\nTherefore, the complete set of constraints defining the capacity region $\\mathcal{C}$ is:\n$$0 \\le R_1 \\le 1 \\quad \\text{and} \\quad R_2 = 0$$\nThis describes a line segment on the $R_1$-axis, from the origin $(0,0)$ to the point $(1,0)$.\n\nComparing this result with the given choices:\nA. The set of points $(R_1, R_2)$ such that $0 \\le R_1 \\le 1$ and $R_2 = 0$. - This matches our derived region.\nB. The set of points $(R_1, R_2)$ such that $R_1 = 0$ and $0 \\le R_2 \\le 1$. - Incorrect.\nC. The set of points $(R_1, R_2)$ such that $0 \\le R_1 \\le 1$ and $0 \\le R_2 \\le 1$. - Incorrect.\nD. The set of points $(R_1, R_2)$ such that $R_1 \\ge 0, R_2 \\ge 0,$ and $R_1 + R_2 \\le 1$. - This is a triangle, which is incorrect. Our region is a line segment, which is a subset of this triangle.\nE. The set of points $(R_1, R_2)$ such that $R_1 = 1$ and $R_2 = 1$. - Incorrect.\n\nThe correct description of the capacity region is given in option A.", "answer": "$$\\boxed{A}$$", "id": "1642856"}, {"introduction": "In contrast to the MAC, the Broadcast Channel (BC) models a single transmitter sending information to multiple receivers. This problem [@problem_id:1642881] explores the capacity for sending a common message, which must be reliably decoded by all receivers simultaneously. By determining the maximum rate for identical Binary Erasure Channels (BECs), you will see how the overall system performance is dictated by the constraints of each individual receiver link, a \"weakest link\" principle common in network design.", "problem": "A deep-space satellite is tasked with broadcasting a critical software patch to two identical, independent ground stations, designated Station A and Station B. The transmission occurs as a sequence of binary digits (bits). Due to unpredictable atmospheric interference, each bit transmitted from the satellite to a ground station is subject to potential loss.\n\nThe communication channel between the satellite and each ground station can be modeled as follows:\n1.  With a probability of $1-\\epsilon$, a transmitted bit is received correctly by the station.\n2.  With a probability of $\\epsilon$, the bit is erased during transmission. In this case, the station's receiver detects that a bit was sent but has no information about its value (0 or 1).\n\nThis process occurs independently for each bit and for each station. The channel described is sometimes referred to as a Binary Erasure Channel (BEC). The primary mission objective is to ensure that the entire software patch can be reconstructed with an arbitrarily low probability of error at *both* Station A and Station B, using the same transmission from the satellite.\n\nDetermine the maximum possible rate of transmission (in bits per channel use) for this common message. Your answer should be an expression in terms of the erasure probability $\\epsilon$.", "solution": "Let $M$ be the common message, uniformly distributed over $\\{1, \\dots, 2^{nR}\\}$, and let $X^n(M)$ be the codeword of length $n$ sent by the satellite. Each station $i \\in \\{A,B\\}$ observes $Y_i^n$ through an independent memoryless binary erasure channel (BEC) with erasure probability $\\epsilon$, used $n$ times. The reliability requirement is that the probability of error at both stations tends to zero, which implies the individual error probabilities at each station tend to zero as $n \\to \\infty$.\n\nConverse (outer bound on the rate):\nFor each receiver $i \\in \\{A,B\\}$, by Fano's inequality,\n$$\nnR \\le I(M;Y_i^n) + n\\delta_n,\n$$\nwhere $\\delta_n \\to 0$ as $n \\to \\infty$. By the data-processing inequality $M \\to X^n \\to Y_i^n$,\n$$\nI(M;Y_i^n) \\le I(X^n;Y_i^n).\n$$\nUsing the chain rule and memorylessness,\n$$\nI(X^n;Y_i^n) \\le \\sum_{t=1}^{n} I(X_t;Y_{i,t}).\n$$\nCombining and dividing by $n$,\n$$\nR \\le \\frac{1}{n} \\sum_{t=1}^{n} I(X_t;Y_{i,t}) + \\delta_n.\n$$\nTaking the minimum over $i \\in \\{A,B\\}$,\n$$\nR \\le \\min_{i \\in \\{A,B\\}} \\frac{1}{n} \\sum_{t=1}^{n} I(X_t;Y_{i,t}) + \\delta_n.\n$$\nUsing $\\min_{i} \\frac{1}{n} \\sum_{t} a_{i,t} \\ge \\frac{1}{n} \\sum_{t} \\min_{i} a_{i,t}$,\n$$\nR \\le \\frac{1}{n} \\sum_{t=1}^{n} \\min_{i \\in \\{A,B\\}} I(X_t;Y_{i,t}) + \\delta_n.\n$$\nEach term satisfies\n$$\n\\min_{i \\in \\{A,B\\}} I(X_t;Y_{i,t}) \\le \\max_{p(x)} \\min_{i \\in \\{A,B\\}} I(X;Y_i),\n$$\nso\n$$\nR \\le \\max_{p(x)} \\min_{i \\in \\{A,B\\}} I(X;Y_i) + \\delta_n.\n$$\nLetting $n \\to \\infty$ yields the common-message capacity\n$$\nC_{\\text{common}} = \\max_{p(x)} \\min\\{I(X;Y_A), I(X;Y_B)\\}.\n$$\n\nSince both channels are identical BECs with erasure probability $\\epsilon$, $I(X;Y_A) = I(X;Y_B) = I(X;Y)$, so\n$$\nC_{\\text{common}} = \\max_{p(x)} I(X;Y).\n$$\n\nSingle-receiver mutual information for the BEC:\nLet $X \\in \\{0,1\\}$ with distribution $p(x)$ and $Y \\in \\{0,1,?\\}$ be the BEC output with erasure probability $\\epsilon$:\n$$\n\\Pr(Y=? \\mid X) = \\epsilon, \\quad \\Pr(Y=X \\mid X) = 1-\\epsilon.\n$$\nThen\n$$\nI(X;Y) = H(X) - H(X \\mid Y).\n$$\nCompute\n$$\nH(X \\mid Y) = \\Pr(Y=?) H(X \\mid Y=?) + \\Pr(Y \\neq ?) H(X \\mid Y \\neq ?).\n$$\nGiven $Y \\neq ?$, we observe $Y=X$, so $H(X \\mid Y \\neq ?) = 0$. Given $Y=?$, $X$ is unrevealed, so $H(X \\mid Y=?) = H(X)$. Hence\n$$\nH(X \\mid Y) = \\epsilon H(X) + (1-\\epsilon)\\cdot 0 = \\epsilon H(X),\n$$\nand thus\n$$\nI(X;Y) = H(X) - \\epsilon H(X) = (1-\\epsilon) H(X).\n$$\nMaximizing over $p(x)$ gives $H(X) \\le 1$ with equality at the uniform input, so\n$$\n\\max_{p(x)} I(X;Y) = 1 - \\epsilon.\n$$\n\nTherefore,\n$$\nC_{\\text{common}} = 1 - \\epsilon.\n$$\n\nAchievability:\nChoose the uniform input distribution and a sequence of codes achieving the BEC capacity $1-\\epsilon$ for a single receiver. Each station then decodes with probability of error tending to zero. By the union bound, the probability that at least one station errs also tends to zero. Hence the rate $1-\\epsilon$ is achievable for the common message, matching the converse.\n\nThus, the maximum possible transmission rate to ensure reliable reconstruction at both stations is $1 - \\epsilon$ bits per channel use.", "answer": "$$\\boxed{1-\\epsilon}$$", "id": "1642881"}, {"introduction": "Moving beyond single-hop models, this problem explores information flow in a multi-hop tree network where the goal is not just communication, but distributed computation. It introduces the powerful network cut-set bound, which provides a fundamental limit on the information that can flow through any network. To solve this problem [@problem_id:1642885], you must identify the network's bottleneck and then construct a coding scheme that achieves this fundamental limit, providing direct practice in proving the capacity of a network.", "problem": "A large-scale environmental monitoring system is architected as a perfect binary tree. At each of the $N=2^d$ leaves of the tree, for an integer depth $d > 2$, there is a sensor $S_k$ for $k \\in \\{1, 2, \\dots, N\\}$. In each observation cycle, sensor $S_k$ measures a binary environmental state $X_k \\in \\{0, 1\\}$. The states $\\{X_k\\}_{k=1}^N$ are modeled as independent and identically distributed random variables, with $P(X_k=1) = P(X_k=0) = 1/2$.\n\nThe sensors are connected upwards through a hierarchy of nodes, culminating in a single data fusion center at the root of the tree. Communication between any child node and its parent node occurs over a dedicated noiseless digital link. All links in the network have the same uniform capacity of $C$ bits per observation cycle. Intermediate nodes in the tree can perform arbitrary computations on the messages they receive from their children before sending a new message to their own parent.\n\nThe fusion center must be able to losslessly recover two distinct pieces of information at the end of each cycle:\n1. The state of the first sensor, $X_1$.\n2. The global parity of all sensor states, defined as $S_{\\text{global}} = \\bigoplus_{k=1}^N X_k = (X_1 + X_2 + \\dots + X_N) \\pmod 2$.\n\nDetermine the minimum integer value of the link capacity $C$ (in bits per observation cycle) that is required for the fusion center to accomplish this task.", "solution": "We model the required outputs as the function $F = (X_1, S_{\\text{global}})$, where $S_{\\text{global}} = \\bigoplus_{k=1}^N X_k$. For any edge $e$ of the tree, removing it partitions the leaves into disjoint sets $A$ and $A^c$. Since the only way information from $A$ can reach the root is across $e$, an information cut-set bound implies that the rate across $e$ must satisfy\n$$\nC \\ge H(F \\mid X_{A^c}).\n$$\nWe evaluate $H(F \\mid X_{A^c})$ by cases.\n\nIf $1 \\notin A$, then $X_1$ is determined by $X_{A^c}$, and\n$$\nH(F \\mid X_{A^c}) = H(S_{\\text{global}} \\mid X_{A^c}) = H\\Big(\\bigoplus_{k \\in A} X_k \\mid X_{A^c}\\Big) = 1,\n$$\nbecause the parity of any nonempty subset of independent fair bits is a single fair bit independent of $X_{A^c}$.\n\nIf $1 \\in A$, write $Y = \\bigoplus_{k \\in A \\setminus \\{1\\}} X_k$. Given $X_{A^c}$, the pair $(X_1, S_{\\text{global}})$ is equivalent to $(X_1, X_1 \\oplus Y)$, so\n$$\nH(F \\mid X_{A^c}) = H(X_1, X_1 \\oplus Y).\n$$\nIf $|A| = 1$, then $Y = 0$ and $(X_1, X_1 \\oplus Y) = (X_1, X_1)$, hence $H(F \\mid X_{A^c}) = H(X_1) = 1$. If $|A| \\ge 2$, then $Y$ is a fair bit independent of $X_1$, so the mapping $(X_1, Y) \\mapsto (X_1, X_1 \\oplus Y)$ is bijective and\n$$\nH(F \\mid X_{A^c}) = H(X_1, Y) = 2.\n$$\nIn the perfect binary tree with depth $d > 2$, there exists at least one edge for which $A$ contains $1$ and $|A| \\ge 2$ (for example, the edge immediately above the parent of leaf $1$ and all higher edges on that path). Therefore, the cut-set lower bound gives $C \\ge 2$.\n\nTo show achievability with $C = 2$, define for each subtree $T$ the two-bit summary\n$$\nB_1(T) = \\bigoplus_{k \\in T} X_k, \\quad B_2(T) = \\begin{cases}\nX_1, & \\text{if } 1 \\in T,\\\\\n0, & \\text{if } 1 \\notin T.\n\\end{cases}\n$$\nAt a leaf $k$, send $(B_1, B_2) = (X_k, 0)$ if $k \\neq 1$ and $(B_1, B_2) = (X_1, X_1)$ if $k = 1$. Each internal node with children subtrees $L$ and $R$ computes\n$$\nB_1(T) = B_1(L) \\oplus B_1(R), \\quad B_2(T) = B_2(L) \\oplus B_2(R),\n$$\nand forwards the two-bit vector $(B_1(T), B_2(T))$ to its parent. At the root, $(B_1, B_2) = \\big(\\bigoplus_{k=1}^N X_k, X_1\\big) = (S_{\\text{global}}, X_1)$, exactly the required outputs. Every edge carries exactly two bits, so $C = 2$ suffices.\n\nCombining the converse and achievability, the minimum integer capacity is $C = 2$.", "answer": "$$\\boxed{2}$$", "id": "1642885"}]}