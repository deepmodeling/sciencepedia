{"hands_on_practices": [{"introduction": "The source-channel separation theorem provides a powerful blueprint for communication system design, with the first step being to quantify the information a source produces. This practice task [@problem_id:1659351] guides you through calculating the total information rate, $R$, generated by multiple independent sensors—a common scenario in modern IoT and monitoring systems. By mastering this, you will understand how to determine the fundamental lower bound on channel capacity, $C$, required for reliable data transmission, as dictated by the principle $C \\ge R$.", "problem": "A research team is designing a simple wireless sensor network to monitor environmental conditions in a remote habitat. The network consists of two independent types of sensors, whose data must be multiplexed and transmitted over a single digital communication channel.\n\n- **Sensor A (Temperature):** This sensor reports the ambient temperature state as either \"Nominal\" or \"Elevated\". Extensive prior measurements indicate that the \"Nominal\" state occurs with a probability of $p_A = \\frac{7}{8}$. This sensor generates one report every 2.0 seconds.\n\n- **Sensor B (Vibration):** This sensor detects ground vibrations and classifies them into one of three states: \"Quiescent\", \"Minor\", or \"Significant\". The probabilities of these states are $p_Q = \\frac{1}{2}$, $p_M = \\frac{1}{4}$, and $p_S = \\frac{1}{4}$, respectively. This sensor generates one report every 0.5 seconds.\n\nTo ensure the data from both sensors can be reconstructed at the receiver with an arbitrarily low probability of error, the capacity of the communication channel must be sufficient to handle the total information rate generated by the sensors.\n\nCalculate the minimum required channel capacity, in bits per second, needed to reliably transmit the data from both sensors. Round your final answer to three significant figures.", "solution": "The minimum channel capacity, $C_{min}$, required for reliable transmission is equal to the total information rate, $R_{total}$, of the source. Since the two sensors operate independently, their total information rate is the sum of their individual information rates. We will calculate the information rate for each sensor separately and then add them. The information rate $R$ of a source is the product of its symbol rate $R_s$ and its entropy per symbol $H(X)$.\n\nFirst, let's analyze Sensor A. Let $X_A$ be the random variable representing the output of Sensor A. The possible outcomes are \"Nominal\" and \"Elevated\".\nThe probability of a \"Nominal\" reading is given as $P(X_A = \\text{Nominal}) = \\frac{7}{8}$.\nThe probability of an \"Elevated\" reading is therefore $P(X_A = \\text{Elevated}) = 1 - \\frac{7}{8} = \\frac{1}{8}$.\n\nThe entropy of Sensor A, $H(X_A)$, is calculated using the formula for the entropy of a discrete random variable, $H(X) = -\\sum_{i} p_i \\log_2(p_i)$.\n$$H(X_A) = - \\left( \\frac{7}{8} \\log_2\\left(\\frac{7}{8}\\right) + \\frac{1}{8} \\log_2\\left(\\frac{1}{8}\\right) \\right)$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$ and $\\log_2(1) = 0$, $\\log_2(8) = 3$:\n$$H(X_A) = - \\left( \\frac{7}{8} (\\log_2(7) - \\log_2(8)) + \\frac{1}{8} (\\log_2(1) - \\log_2(8)) \\right)$$\n$$H(X_A) = - \\left( \\frac{7}{8} (\\log_2(7) - 3) + \\frac{1}{8} (0 - 3) \\right)$$\n$$H(X_A) = - \\frac{7}{8} \\log_2(7) + \\frac{21}{8} + \\frac{3}{8} = \\frac{24}{8} - \\frac{7}{8} \\log_2(7) = 3 - \\frac{7}{8} \\log_2(7) \\text{ bits/symbol}$$\nThe symbol rate of Sensor A is $R_{s,A} = \\frac{1 \\text{ report}}{2.0 \\text{ s}} = 0.5 \\text{ symbols/s}$.\nThe information rate of Sensor A is $R_A = R_{s,A} \\times H(X_A)$:\n$$R_A = 0.5 \\left( 3 - \\frac{7}{8} \\log_2(7) \\right) \\text{ bits/s}$$\n\nNext, let's analyze Sensor B. Let $X_B$ be the random variable representing the output of Sensor B. The outcomes are \"Quiescent\", \"Minor\", and \"Significant\" with probabilities $p_Q = 1/2$, $p_M = 1/4$, and $p_S = 1/4$.\nThe entropy of Sensor B, $H(X_B)$, is:\n$$H(X_B) = - \\left( \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) \\right)$$\nSince $\\log_2(1/2) = -1$ and $\\log_2(1/4) = -2$:\n$$H(X_B) = - \\left( \\frac{1}{2} (-1) + \\frac{1}{4} (-2) + \\frac{1}{4} (-2) \\right) = - \\left( -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right) = \\frac{3}{2} = 1.5 \\text{ bits/symbol}$$\nThe symbol rate of Sensor B is $R_{s,B} = \\frac{1 \\text{ report}}{0.5 \\text{ s}} = 2.0 \\text{ symbols/s}$.\nThe information rate of Sensor B is $R_B = R_{s,B} \\times H(X_B)$:\n$$R_B = 2.0 \\times 1.5 = 3.0 \\text{ bits/s}$$\n\nThe total information rate $R_{total}$ is the sum of $R_A$ and $R_B$:\n$$R_{total} = R_A + R_B = 0.5 \\left( 3 - \\frac{7}{8} \\log_2(7) \\right) + 3.0$$\nTo find the numerical value, we use $\\log_2(7) = \\frac{\\ln(7)}{\\ln(2)} \\approx \\frac{1.94591}{0.69315} \\approx 2.80735$.\n$$R_A \\approx 0.5 \\left( 3 - \\frac{7}{8} \\times 2.80735 \\right) \\approx 0.5 (3 - 2.45643) = 0.5(0.54357) \\approx 0.271785 \\text{ bits/s}$$\n$$R_{total} \\approx 0.271785 + 3.0 = 3.271785 \\text{ bits/s}$$\nThe minimum required channel capacity is $C_{min} = R_{total}$.\nRounding the result to three significant figures, we get:\n$$C_{min} \\approx 3.27 \\text{ bits/s}$$", "answer": "$$\\boxed{3.27}$$", "id": "1659351"}, {"introduction": "A key insight from information theory is the distinction between a source's fundamental information content (its entropy, $H(S)$) and the actual data rate produced by a specific encoder. This exercise [@problem_id:1659327] explores a scenario where a simple, fixed-length code is used for practical reasons, rather than an optimal compression scheme. This will help you understand that for reliable communication, the channel capacity must be sufficient to handle the rate of the *encoded* data, which may be greater than the source's entropy.", "problem": "A minimalist weather sensor is designed to report atmospheric conditions at a remote location. It can only report one of three states: 'Sunny' (S), 'Cloudy' (C), or 'Rainy' (R). Based on historical data for the region, the probabilities of these states occurring are $P(S) = 0.5$, $P(C) = 0.25$, and $P(R) = 0.25$.\n\nTo simplify the electronic design, the engineers have implemented a simple, fixed-length binary code to represent these states before transmission. Specifically, every state reported by the sensor is encoded into a 2-bit codeword. This encoded binary stream is then transmitted over a noisy wireless channel.\n\nAccording to Shannon's channel coding theorem, for data to be transmitted reliably (with an arbitrarily low probability of error), the rate of the data entering the channel must be less than the channel's capacity.\n\nWhat is the absolute minimum channel capacity required to ensure reliable transmission of the data generated by this specific weather sensor's encoder? Express your answer in units of bits per sensor reading.", "solution": "Let the weather state be a random variable $X \\in \\{S,C,R\\}$ with probabilities $P(S)=0.5$, $P(C)=0.25$, and $P(R)=0.25$. Its entropy, which is the theoretical lower bound for average lossless source coding, is\n$$\nH(X)=-\\sum_{x}P(x)\\log_{2}P(x)\n= -\\left[0.5\\log_{2}(0.5)+0.25\\log_{2}(0.25)+0.25\\log_{2}(0.25)\\right]\n= 1.5\\ \\text{bits per reading}.\n$$\nHowever, the implemented encoder uses a fixed-length code with exactly 2 bits per state, so the data rate entering the channel is\n$$\nR_{\\text{in}} = 2\\ \\text{bits per sensor reading}.\n$$\nBy Shannon’s channel coding theorem, reliable transmission with arbitrarily low error requires the data rate to be strictly less than the channel capacity, i.e., $R_{\\text{in}}<C$. Therefore, the absolute minimum capacity threshold that must be met is\n$$\nC_{\\min} = R_{\\text{in}} = 2\\ \\text{bits per sensor reading},\n$$\nnoting that in practice one requires $C>2$, but the minimum threshold value is $2$.", "answer": "$$\\boxed{2}$$", "id": "1659327"}, {"introduction": "Shannon's theorems are powerful, but they are also precise statements about limits and possibilities. This thought experiment [@problem_id:1659343] probes the strict conditions of the source-channel separation theorem, specifically what happens at the theoretical boundary where the information rate equals channel capacity, $R = C$. Grappling with this scenario is crucial for understanding why reliable communication requires a \"safety margin\" where the rate is strictly less than capacity ($R \\lt C$) and what \"arbitrarily low error\" truly means in theory versus practice.", "problem": "Two engineers, Anja and Ben, are on a team designing a next-generation wireless communication system. Their task is to transmit data from a discrete memoryless source, denoted by $S$, over a noisy discrete memoryless channel. After careful analysis, they determine the entropy of the source to be $H(S)$ and the capacity of the channel to be $C$.\n\nDuring a design meeting, Anja proposes a specific system configuration. She claims, \"According to the source-channel separation theorem, if we ensure that our channel's capacity is perfectly matched to our source's entropy, so that $C = H(S)$, then we can design a practical system using finite-blocklength codes that achieves error-free communication.\"\n\nBen is skeptical of this claim and believes there is a flaw in Anja's reasoning. Based on a rigorous understanding of information theory, which of the following statements most accurately evaluates Anja's claim?\n\nA. Anja is correct. The condition $C = H(S)$ represents the ideal matching of source complexity to channel capability, which is the exact requirement for error-free transmission as guaranteed by the source-channel separation theorem.\n\nB. Anja is incorrect. The source-channel separation theorem implies that reliable communication is possible only if the source entropy is strictly less than the channel capacity, i.e., $H(S) < C$. At the boundary condition $C = H(S)$, the probability of error cannot be made arbitrarily close to zero, even for codes with infinitely long blocklengths.\n\nC. Anja is partially correct. Error-free communication is theoretically possible for $C = H(S)$, but only in the limit as the code's blocklength approaches infinity. Any practical system with finite-blocklength codes will necessarily have errors.\n\nD. Anja is incorrect. The source-channel separation theorem does not apply here because it is a purely theoretical concept that assumes ideal components, whereas any practical system with finite-blocklength codes is non-ideal.\n\nE. Anja is incorrect. The source-channel separation theorem states that source coding and channel coding are independent problems, but it makes no guarantee about achieving error-free communication, only about achieving a minimal data rate.", "solution": "We model a discrete memoryless source with entropy $H(S)$ and a discrete memoryless channel with capacity $C$. The fundamental theorems needed are:\n\n1) Source coding theorem (lossless, almost-sure reconstruction): For any rate $R_{s}$ and blocklength $n$, there exist source codes with block error probability $P_{e}^{(n)} \\to 0$ as $n \\to \\infty$ if $R_{s} > H(S)$. Conversely, if $R_{s} < H(S)$, then $P_{e}^{(n)}$ is bounded away from zero for all $n$. The infimum achievable rate equals $H(S)$, and the standard achievability statement uses strict inequality: for every $\\delta > 0$, rates $R_{s} = H(S) + \\delta$ are achievable with $P_{e}^{(n)} \\to 0$.\n\n2) Channel coding theorem (reliable transmission over a DMC): For any rate $R_{c} < C$, there exist channel codes with $P_{e}^{(n)} \\to 0$ as $n \\to \\infty$. The strong converse for DMCs states that for $R_{c} > C$, $P_{e}^{(n)} \\to 1$, and at the boundary $R_{c} = C$, $P_{e}^{(n)}$ cannot be driven to zero in general (it stays bounded away from zero).\n\n3) Source–channel separation theorem (lossless): Reliable end-to-end communication of a DMS over a DMC with vanishing error is possible if and only if $H(S) < C$. Achievability uses an intermediate rate $R$ satisfying $H(S) < R < C$: first compress the source at rate $R$ with vanishing source coding error, then transmit at rate $R$ over the channel with vanishing channel decoding error, yielding vanishing overall error as $n \\to \\infty$. If $H(S) > C$, no scheme can achieve vanishing error. At the boundary $H(S) = C$, there is no slack to pick $R$ with $H(S) < R < C$, and the strong converse at the channel side precludes vanishing error at rate exactly $C$.\n\nImplications for the claim $C = H(S)$:\n\n- Separation requires a strict inequality $H(S) < C$ to allow a rate $R$ in the open interval $(H(S), C)$. When $C = H(S)$, no such $R$ exists, so the separation-based achievability argument does not go through.\n\n- Independently of separation, channel coding at $R_{c} = C$ does not yield vanishing error probability for DMCs (strong converse). Thus, even with infinite blocklength, error probability cannot be made arbitrarily small at $R_{c} = C$, let alone with finite blocklength.\n\n- For finite blocklength $n$, finite-blocklength bounds further quantify the necessary backoff from capacity by a positive term that typically scales like $O(n^{-1/2})$ to achieve a given small error probability, reinforcing that operation at exactly $C$ cannot yield arbitrarily small error.\n\nTherefore, Anja’s assertion that setting $C = H(S)$ allows a practical finite-blocklength, error-free system contradicts both the strict-inequality requirement of separation and the strong converse for channel coding. Among the options, the statement that correctly reflects these facts is that reliable communication is possible only if $H(S) < C$, and at the boundary $C = H(S)$ the error cannot be made arbitrarily small even as blocklength grows.\n\nOption B precisely states this.", "answer": "$$\\boxed{B}$$", "id": "1659343"}]}