{"hands_on_practices": [{"introduction": "To truly grasp the concepts of information theory, we must move from abstract definitions to concrete calculations. This first exercise provides foundational practice in computing Fisher information, a key measure of the precision with which we can estimate a parameter. By working with the exponential distribution—a model commonly used to describe phenomena like radioactive decay or waiting times—you will build essential skills for quantifying the information contained within your data. [@problem_id:1653754]", "problem": "In particle physics, the time until a specific type of unstable particle decays can be modeled as a random variable. A common model for this decay time is the exponential distribution. The Probability Density Function (PDF) for a single decay time $X$ is given by\n$$ f(x; \\lambda) = \\lambda \\exp(-\\lambda x) $$\nfor $x \\ge 0$, where $\\lambda > 0$ is the decay rate parameter. A larger $\\lambda$ implies the particle decays more quickly on average.\n\nAn experiment is conducted to estimate the decay rate $\\lambda$ of a newly discovered particle. In this experiment, a physicist observes a set of $n$ independent and identically distributed (i.i.d.) decay times, denoted as $X_1, X_2, \\ldots, X_n$.\n\nThe Fisher information provides a way to measure the amount of information that an observable random variable carries about an unknown parameter of a distribution that models it. For a set of $n$ i.i.d. observations, the Fisher information $I_n(\\lambda)$ quantifies the precision with which the parameter $\\lambda$ can be estimated from the data.\n\nCalculate the Fisher information, $I_n(\\lambda)$, for the set of $n$ decay time measurements with respect to the decay rate parameter $\\lambda$. Express your answer in terms of $n$ and $\\lambda$.", "solution": "We model each decay time $X_{i}$ as i.i.d. with density $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\ge 0$, where $\\lambda>0$. For observations $x_{1},\\ldots,x_{n}$, the likelihood is\n$$\nL(\\lambda;x_{1:n})=\\prod_{i=1}^{n}\\lambda \\exp(-\\lambda x_{i})=\\lambda^{n}\\exp\\!\\Big(-\\lambda\\sum_{i=1}^{n}x_{i}\\Big).\n$$\nThe log-likelihood is\n$$\n\\ell(\\lambda)=\\ln L(\\lambda;x_{1:n})=\\sum_{i=1}^{n}\\big(\\ln \\lambda-\\lambda x_{i}\\big)=n\\ln \\lambda-\\lambda\\sum_{i=1}^{n}x_{i}.\n$$\nThe score function (first derivative) is\n$$\n\\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda}=\\frac{n}{\\lambda}-\\sum_{i=1}^{n}x_{i},\n$$\nand the second derivative is\n$$\n\\frac{\\partial^{2} \\ell(\\lambda)}{\\partial \\lambda^{2}}=-\\frac{n}{\\lambda^{2}}.\n$$\nBy the definition of Fisher information for i.i.d. samples under standard regularity conditions,\n$$\nI_{n}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell(\\lambda)}{\\partial \\lambda^{2}}\\right].\n$$\nSince $\\partial^{2}\\ell/\\partial \\lambda^{2}=-n/\\lambda^{2}$ does not depend on the data, its expectation equals itself, yielding\n$$\nI_{n}(\\lambda)=-\\Big(-\\frac{n}{\\lambda^{2}}\\Big)=\\frac{n}{\\lambda^{2}}.\n$$\nEquivalently, the Fisher information for one observation is $I_{1}(\\lambda)=\\frac{1}{\\lambda^{2}}$, and additivity over $n$ i.i.d. observations gives $I_{n}(\\lambda)=n I_{1}(\\lambda)=\\frac{n}{\\lambda^{2}}$.", "answer": "$$\\boxed{\\frac{n}{\\lambda^{2}}}$$", "id": "1653754"}, {"introduction": "Having practiced the calculation of Fisher information, we now explore its direct relationship with entropy in a simple discrete system. This problem examines a Bernoulli process, the building block of digital information, to investigate the connection between uncertainty (entropy) and estimability (Fisher information). By finding the parameter value that maximizes a system's entropy, you will discover a fundamental and intuitive trade-off: the state of maximum uncertainty is precisely where an observation provides the least information about the underlying parameter. [@problem_id:1653764]", "problem": "Consider a simple binary system, such as a memory bit or a digital signal, which can be in one of two states: 'on' (represented by 1) or 'off' (represented by 0). Let the random variable $X$ describe the state of the system, following a Bernoulli distribution with parameter $p$. That is, the probability of the system being 'on' is $P(X=1) = p$, and the probability of it being 'off' is $P(X=0) = 1-p$, for $p \\in (0, 1)$.\n\nThe uncertainty of the system's state is measured by the Shannon entropy, given by the function $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$, where $\\ln$ denotes the natural logarithm.\n\nThe Fisher information, which quantifies how much information an observation of $X$ provides about the parameter $p$, is defined for a Bernoulli distribution as $I(p) = \\frac{1}{p(1-p)}$.\n\nYour task is to analyze the point of maximum uncertainty. First, find the value of $p$ that maximizes the entropy $H(p)$. Second, evaluate the Fisher information $I(p)$ at this specific value of $p$.\n\nProvide two numbers as your final answer: the value of $p$ that maximizes the entropy, and the corresponding value of the Fisher information. Express both values as exact fractions or integers.", "solution": "We are given the Bernoulli entropy $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$ for $p \\in (0,1)$ and the Fisher information $I(p) = \\frac{1}{p(1-p)}$. To find the value of $p$ that maximizes $H(p)$, we differentiate with respect to $p$ and set the derivative to zero.\n\nCompute the first derivative using standard differentiation rules:\n$$\n\\frac{d}{dp}\\big[-p \\ln p\\big] = -(\\ln p + 1), \\quad \\frac{d}{dp}\\big[-(1-p)\\ln(1-p)\\big] = \\ln(1-p) + 1.\n$$\nTherefore,\n$$\nH'(p) = -(\\ln p + 1) + (\\ln(1-p) + 1) = \\ln(1-p) - \\ln p = \\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\nSet $H'(p) = 0$ to find critical points:\n$$\n\\ln\\!\\left(\\frac{1-p}{p}\\right) = 0 \\quad \\Longrightarrow \\quad \\frac{1-p}{p} = 1 \\quad \\Longrightarrow \\quad 1 - p = p \\quad \\Longrightarrow \\quad p = \\frac{1}{2}.\n$$\nTo verify that this critical point is a maximum, compute the second derivative:\n$$\nH''(p) = \\frac{d}{dp}\\big[\\ln(1-p) - \\ln p\\big] = -\\frac{1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right).\n$$\nFor $p \\in (0,1)$, both terms inside the parentheses are positive, so $H''(p) < 0$, confirming that $p = \\frac{1}{2}$ is a point of maximum entropy.\n\nNext, evaluate the Fisher information at this $p$:\n$$\nI\\!\\left(\\frac{1}{2}\\right) = \\frac{1}{\\left(\\frac{1}{2}\\right)\\left(1 - \\frac{1}{2}\\right)} = \\frac{1}{\\frac{1}{4}} = 4.\n$$\nThus, the value of $p$ that maximizes the entropy is $\\frac{1}{2}$, and the corresponding Fisher information is $4$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & 4 \\end{pmatrix}}$$", "id": "1653764"}, {"introduction": "In our final practice, we turn to the cornerstone of statistics and signal processing: the Gaussian distribution. This problem uncovers a remarkably elegant and profound connection between Fisher information and differential entropy, one that is not immediately obvious. You will calculate a specific product involving the Fisher information $I(\\mu)$ and the entropy power $\\exp(2h(X))$, revealing a universal constant that is independent of the distribution's variance $\\sigma^2$. This surprising result, an instance of the de Bruijn identity, illustrates a deep structural property linking estimation precision and statistical uncertainty. [@problem_id:1653753]", "problem": "A measurement of a physical quantity is modeled by a random variable $X$ that follows a Gaussian (normal) distribution. The probability density function (PDF) for this distribution is given by:\n$$p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nHere, $\\mu$ is the mean of the distribution, representing the true value of the physical quantity, and $\\sigma^2$ is the variance, representing the measurement uncertainty. The variance $\\sigma^2$ is a known positive constant, while the mean $\\mu$ is the parameter we wish to gain information about.\n\nFor such a distribution, two important quantities from information theory are the Fisher information and the differential entropy.\n\nThe Fisher information, $I(\\theta)$, quantifies the amount of information that an observable random variable $X$ carries about an unknown parameter $\\theta$ of a distribution that models $X$. For a parameter $\\theta$, it is defined as the expectation of the squared score:\n$$I(\\theta) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\ln p(x; \\theta)\\right)^2\\right]$$\nwhere the expectation $\\mathbb{E}[\\cdot]$ is taken with respect to the distribution $p(x; \\theta)$.\n\nThe differential entropy, $h(X)$, is a measure of the average uncertainty of a continuous random variable. It is defined as:\n$$h(X) = -\\int_{-\\infty}^{\\infty} p(x) \\ln p(x) \\, dx$$\nwhich can also be expressed using the expectation operator as $h(X) = -\\mathbb{E}[\\ln p(X)]$.\n\nYour task is to calculate the value of the product $I(\\mu) \\cdot \\exp(2h(X))$, where $I(\\mu)$ is the Fisher information of the Gaussian distribution with respect to its mean $\\mu$, and $h(X)$ is its differential entropy. The final result should be a symbolic expression in terms of fundamental mathematical constants.", "solution": "We start with the Gaussian density\n$$\np(x;\\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\n$$\nwhere $\\sigma^{2}$ is known and $\\mu$ is the parameter of interest.\n\nFisher information for $\\mu$:\nThe log-density is\n$$\n\\ln p(x;\\mu,\\sigma^{2})=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}.\n$$\nThe score with respect to $\\mu$ is\n$$\n\\frac{\\partial}{\\partial \\mu}\\ln p(x;\\mu,\\sigma^{2})=\\frac{x-\\mu}{\\sigma^{2}}.\n$$\nTherefore, using $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$,\n$$\nI(\\mu)=\\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\mu}\\ln p(X;\\mu,\\sigma^{2})\\right)^{2}\\right]\n=\\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma^{2}}\\right)^{2}\\right]\n=\\frac{1}{\\sigma^{4}}\\mathbb{E}[(X-\\mu)^{2}]=\\frac{1}{\\sigma^{2}}.\n$$\n\nDifferential entropy:\nBy definition,\n$$\nh(X)=-\\mathbb{E}[\\ln p(X;\\mu,\\sigma^{2})]\n=\\mathbb{E}\\!\\left[\\frac{1}{2}\\ln(2\\pi\\sigma^{2})+\\frac{(X-\\mu)^{2}}{2\\sigma^{2}}\\right]\n=\\frac{1}{2}\\ln(2\\pi\\sigma^{2})+\\frac{1}{2},\n$$\nwhere we used $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$. Equivalently,\n$$\nh(X)=\\frac{1}{2}\\ln\\!\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big).\n$$\nHence,\n$$\n\\exp\\big(2h(X)\\big)=\\exp\\!\\left(\\ln\\!\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big)\\right)=2\\pi\\,\\exp(1)\\,\\sigma^{2}.\n$$\n\nProduct:\n$$\nI(\\mu)\\cdot \\exp\\big(2h(X)\\big)=\\frac{1}{\\sigma^{2}}\\cdot\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big)=2\\pi\\,\\exp(1).\n$$\nThis final expression depends only on fundamental constants.", "answer": "$$\\boxed{2 \\pi \\exp(1)}$$", "id": "1653753"}]}