{"hands_on_practices": [{"introduction": "Calculating Fisher information begins with mastering the fundamental definition. This first exercise provides a classic application, asking you to quantify the information about the rate parameter $\\beta$ in a Gamma distribution, a common model for waiting times in physical processes. By working through this problem [@problem_id:1624973], you will solidify the core computational workflow: defining the log-likelihood, calculating its second derivative, and finding its expected value.", "problem": "In statistical physics, the time intervals between quantum events, such as the decay of radioactive particles, are often modeled using probability distributions. Consider a scenario where a physicist is modeling the waiting time, $T$, until a specific number of decay events, $\\alpha$, have occurred. This waiting time is described by a random variable $T$ following a Gamma distribution.\n\nThe probability density function (PDF) for the Gamma distribution is given by:\n$$f(t; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} t^{\\alpha-1} \\exp(-\\beta t)$$\nfor $t > 0$, and $f(t; \\alpha, \\beta) = 0$ otherwise. Here, $\\Gamma(\\alpha)$ is the Gamma function.\n\nIn this model, the shape parameter $\\alpha$ is a known positive integer representing the number of events being waited for. The rate parameter $\\beta > 0$ is an unknown physical constant related to the average decay rate of the source, which the physicist wishes to estimate. The Fisher information, $I(\\beta)$, quantifies the amount of information that a single observation of the waiting time $T$ carries about the unknown parameter $\\beta$.\n\nCalculate the Fisher information $I(\\beta)$ for the rate parameter $\\beta$, based on a single observation from this Gamma distribution. Express your answer as a symbolic expression in terms of $\\alpha$ and $\\beta$.", "solution": "We have a single observation $T$ from the Gamma distribution with known shape $\\alpha>0$ and unknown rate $\\beta>0$, with density\n$$\nf(t;\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\,t^{\\alpha-1}\\exp(-\\beta t),\\quad t>0.\n$$\nThe log-likelihood for a single observation is\n$$\n\\ell(\\beta; t)=\\ln f(t;\\alpha,\\beta)=\\alpha\\ln\\beta-\\ln\\Gamma(\\alpha)+(\\alpha-1)\\ln t-\\beta t,\n$$\nwhere $\\Gamma(\\alpha)$ and $t$ are constants with respect to $\\beta$ because $\\alpha$ is known and $t$ is observed.\n\nThe score function is the first derivative with respect to $\\beta$:\n$$\n\\frac{\\partial \\ell}{\\partial \\beta}=\\frac{\\alpha}{\\beta}-t.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2}\\ell}{\\partial \\beta^{2}}=-\\frac{\\alpha}{\\beta^{2}}.\n$$\nBy the definition of Fisher information for a single observation (under standard regularity conditions, which hold here since the support does not depend on $\\beta$ and the derivatives exist),\n$$\nI(\\beta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial \\beta^{2}}\\right]=-\\mathbb{E}\\!\\left[-\\frac{\\alpha}{\\beta^{2}}\\right]=\\frac{\\alpha}{\\beta^{2}}.\n$$\nThus, the Fisher information in one observation about $\\beta$ is $\\alpha/\\beta^{2}$.", "answer": "$$\\boxed{\\frac{\\alpha}{\\beta^{2}}}$$", "id": "1624973"}, {"introduction": "Real-world measurements are often incomplete or processed before we analyze them. This practice explores such a scenario, where a particle counter only reports the parity (even or odd) of the count, not the exact number. Your task is to determine the Fisher information about the underlying Poisson rate $\\lambda$ from this single binary output [@problem_id:1624981]. This problem demonstrates how to quantify the information that survives data processing and reveals the connection between the original data's distribution and the information in its summary.", "problem": "A specialized sensor system is designed to detect particles emitted from a source. The number of particles, $K$, detected within a fixed time interval is a random variable that follows a Poisson distribution with a mean rate $\\lambda$. The probability mass function for this distribution is given by:\n$$P(K=k | \\lambda) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}, \\quad \\text{for } k \\in \\{0, 1, 2, \\ldots\\}$$\nDue to hardware limitations, the sensor does not report the exact count $K$. Instead, it only outputs a single binary value, $Y$, which indicates the parity of the count. If the number of detected particles $K$ is even (0, 2, 4, ...), the output is $Y=0$. If $K$ is odd (1, 3, 5, ...), the output is $Y=1$.\n\nAssuming you have a single measurement of the binary output $Y$, determine the Fisher information, $I(\\lambda)$, for the parameter $\\lambda$. Express your answer as a closed-form analytic expression in terms of $\\lambda$.", "solution": "Let $K \\sim \\text{Poisson}(\\lambda)$ and $Y=\\mathbf{1}\\{K \\text{ is odd}\\}$. We first compute $P(Y=0 \\mid \\lambda)$ and $P(Y=1 \\mid \\lambda)$. Define\n$$\nS \\equiv \\mathbb{E}[(-1)^{K}] = \\sum_{k=0}^{\\infty} (-1)^{k} \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!} = \\exp(-\\lambda) \\sum_{k=0}^{\\infty} \\frac{(-\\lambda)^{k}}{k!} = \\exp(-\\lambda)\\exp(-\\lambda) = \\exp(-2\\lambda).\n$$\nSince $(-1)^{K}=1$ when $K$ is even and $(-1)^{K}=-1$ when $K$ is odd, we have\n$$\nS = P(K \\text{ even}) - P(K \\text{ odd}) = P(Y=0) - P(Y=1),\n$$\nand also $P(Y=0) + P(Y=1) = 1$. Solving gives\n$$\nP(Y=0 \\mid \\lambda) = \\frac{1 + \\exp(-2\\lambda)}{2}, \\quad P(Y=1 \\mid \\lambda) = \\frac{1 - \\exp(-2\\lambda)}{2}.\n$$\nThus $Y \\sim \\text{Bernoulli}(p(\\lambda))$ with\n$$\np(\\lambda) = \\frac{1 - \\exp(-2\\lambda)}{2}.\n$$\nFor a single Bernoulli observation with success probability $p(\\lambda)$, the log-likelihood for $y \\in \\{0,1\\}$ is\n$$\n\\ell(\\lambda; y) = y \\ln p(\\lambda) + (1 - y) \\ln(1 - p(\\lambda)).\n$$\nDifferentiating,\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda} = y \\frac{p'(\\lambda)}{p(\\lambda)} - (1 - y) \\frac{p'(\\lambda)}{1 - p(\\lambda)} = p'(\\lambda) \\left( \\frac{y}{p(\\lambda)} - \\frac{1 - y}{1 - p(\\lambda)} \\right).\n$$\nThe Fisher information is\n$$\nI(\\lambda) = \\mathbb{E}\\!\\left[ \\left( \\frac{\\partial \\ell}{\\partial \\lambda} \\right)^{2} \\right]\n= \\left(p'(\\lambda)\\right)^{2} \\, \\mathbb{E}\\!\\left[ \\left( \\frac{y}{p(\\lambda)} - \\frac{1 - y}{1 - p(\\lambda)} \\right)^{2} \\right].\n$$\nSince $P(Y=1)=p(\\lambda)$ and $P(Y=0)=1-p(\\lambda)$, the inner expectation evaluates to\n$$\np(\\lambda) \\left( \\frac{1}{p(\\lambda)} \\right)^{2} + \\left(1 - p(\\lambda)\\right) \\left( \\frac{1}{1 - p(\\lambda)} \\right)^{2}\n= \\frac{1}{p(\\lambda)} + \\frac{1}{1 - p(\\lambda)} = \\frac{1}{p(\\lambda)\\left(1 - p(\\lambda)\\right)}.\n$$\nHence the Fisher information simplifies to\n$$\nI(\\lambda) = \\frac{\\left(p'(\\lambda)\\right)^{2}}{p(\\lambda)\\left(1 - p(\\lambda)\\right)}.\n$$\nWith $p(\\lambda) = \\frac{1 - \\exp(-2\\lambda)}{2}$, we have\n$$\np'(\\lambda) = \\exp(-2\\lambda),\n$$\nand\n$$\np(\\lambda)\\left(1 - p(\\lambda)\\right) = \\frac{1 - \\exp(-2\\lambda)}{2} \\cdot \\frac{1 + \\exp(-2\\lambda)}{2} = \\frac{1 - \\exp(-4\\lambda)}{4}.\n$$\nTherefore,\n$$\nI(\\lambda) = \\frac{\\exp(-4\\lambda)}{\\frac{1 - \\exp(-4\\lambda)}{4}} = \\frac{4 \\exp(-4\\lambda)}{1 - \\exp(-4\\lambda)} = \\frac{4}{\\exp(4\\lambda) - 1}.\n$$\nThis is the Fisher information for a single binary parity observation $Y$.", "answer": "$$\\boxed{\\frac{4}{\\exp(4\\lambda)-1}}$$", "id": "1624981"}, {"introduction": "Most statistical models involve more than one unknown parameter. This final exercise generalizes our analysis to the multi-parameter case by introducing the Fisher Information Matrix. Revisiting the Gamma distribution, you will now assume both the shape parameter $\\alpha$ and the scale parameter $\\theta$ are unknown [@problem_id:1914846]. Calculating the full $2 \\times 2$ information matrix is a crucial step toward understanding the fundamental limits on the precision of multi-parameter estimation and the interplay between estimators.", "problem": "In statistical modeling, the Gamma distribution is a versatile two-parameter family of continuous probability distributions. Consider a single random variable $X$ drawn from a Gamma distribution with an unknown shape parameter $\\alpha > 0$ and an unknown scale parameter $\\theta > 0$. The probability density function (PDF) is given by:\n$$f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha}} x^{\\alpha-1} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x > 0$$\nwhere $\\Gamma(\\alpha)$ is the standard Gamma function.\n\nThe efficiency of estimators for the parameters $(\\alpha, \\theta)$ is related to the Fisher information matrix. The element in the $i$-th row and $j$-th column of the Fisher information matrix $I(\\boldsymbol{\\eta})$ for a parameter vector $\\boldsymbol{\\eta} = (\\eta_1, \\eta_2, \\dots, \\eta_k)$ is defined as $I_{ij}(\\boldsymbol{\\eta}) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\eta_i \\partial \\eta_j}\\right]$, where $\\ell$ is the log-likelihood function for a single observation.\n\nFor this problem, you will need the definition of the trigamma function, which is the second derivative of the log-gamma function: $\\psi'(\\alpha) = \\frac{d^2}{d\\alpha^2}\\ln\\Gamma(\\alpha)$.\n\nYour task is to determine the complete $2 \\times 2$ Fisher information matrix, $I(\\alpha, \\theta)$, for the parameter vector $(\\alpha, \\theta)$ based on a single observation $X$ from this Gamma distribution. Express your answer as a matrix whose entries are functions of $\\alpha$, $\\theta$, and potentially the trigamma function $\\psi'(\\alpha)$.", "solution": "The problem asks for the Fisher information matrix for the parameters $(\\alpha, \\theta)$ of a Gamma distribution based on a single observation $X$. The parameter vector is $\\boldsymbol{\\eta} = (\\alpha, \\theta)$. The Fisher information matrix $I(\\alpha, \\theta)$ is a $2 \\times 2$ matrix with elements given by:\n$$I_{ij} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\eta_i \\partial \\eta_j}\\right]$$\nwhere $\\ell = \\ell(\\alpha, \\theta | x)$ is the log-likelihood function.\n\nFirst, we write down the log-likelihood function for a single observation $x$ from the Gamma distribution.\nThe PDF is $f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha}} x^{\\alpha-1} \\exp\\left(-\\frac{x}{\\theta}\\right)$.\nTaking the natural logarithm, we get the log-likelihood:\n$$\\ell(\\alpha, \\theta | x) = \\ln\\left( \\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha}} x^{\\alpha-1} \\exp\\left(-\\frac{x}{\\theta}\\right) \\right)$$\n$$\\ell(\\alpha, \\theta | x) = -\\ln(\\Gamma(\\alpha)) - \\alpha \\ln(\\theta) + (\\alpha-1)\\ln(x) - \\frac{x}{\\theta}$$\n\nNext, we compute the first-order partial derivatives of $\\ell$ with respect to $\\alpha$ and $\\theta$.\nWe use the definition of the digamma function, $\\psi(\\alpha) = \\frac{d}{d\\alpha}\\ln\\Gamma(\\alpha) = \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)}$.\n$$\\frac{\\partial \\ell}{\\partial \\alpha} = -\\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} - \\ln(\\theta) + \\ln(x) = -\\psi(\\alpha) - \\ln(\\theta) + \\ln(x)$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{\\alpha}{\\theta} + \\frac{x}{\\theta^2}$$\n\nNow, we compute the second-order partial derivatives.\nThe derivative of the digamma function is the trigamma function, $\\psi'(\\alpha) = \\frac{d}{d\\alpha}\\psi(\\alpha)$.\n$$\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\psi'(\\alpha)$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{\\alpha}{\\theta^2} - \\frac{2x}{\\theta^3}$$\nAnd the mixed partial derivative:\n$$\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\theta} = \\frac{\\partial}{\\partial \\alpha} \\left( -\\frac{\\alpha}{\\theta} + \\frac{x}{\\theta^2} \\right) = -\\frac{1}{\\theta}$$\nBy Clairaut's theorem, $\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\alpha} = \\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\theta} = -\\frac{1}{\\theta}$.\n\nThe next step is to find the expectation of the negative of these second derivatives. The expectation is taken with respect to the distribution of $X$, which is Gamma($\\alpha, \\theta$). A key property of the Gamma($\\alpha, \\theta$) distribution is that its expected value is $E[X] = \\alpha\\theta$.\n\nLet's compute the elements of the Fisher information matrix $I(\\alpha, \\theta) = \\begin{pmatrix} I_{11} & I_{12} \\\\ I_{21} & I_{22} \\end{pmatrix}$.\n\nFor $I_{11}$ (related to $\\alpha, \\alpha$):\n$$I_{11} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\alpha^2}\\right] = -E[-\\psi'(\\alpha)]$$\nSince $\\psi'(\\alpha)$ is a function of $\\alpha$ only and does not depend on $x$, its expectation is just the function itself.\n$$I_{11} = \\psi'(\\alpha)$$\n\nFor $I_{22}$ (related to $\\theta, \\theta$):\n$$I_{22} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = -E\\left[\\frac{\\alpha}{\\theta^2} - \\frac{2X}{\\theta^3}\\right]$$\nUsing the linearity of expectation:\n$$I_{22} = -\\left(E\\left[\\frac{\\alpha}{\\theta^2}\\right] - E\\left[\\frac{2X}{\\theta^3}\\right]\\right) = -\\left(\\frac{\\alpha}{\\theta^2} - \\frac{2E[X]}{\\theta^3}\\right)$$\nSubstitute $E[X] = \\alpha\\theta$:\n$$I_{22} = -\\left(\\frac{\\alpha}{\\theta^2} - \\frac{2(\\alpha\\theta)}{\\theta^3}\\right) = -\\left(\\frac{\\alpha}{\\theta^2} - \\frac{2\\alpha}{\\theta^2}\\right) = -\\left(-\\frac{\\alpha}{\\theta^2}\\right) = \\frac{\\alpha}{\\theta^2}$$\n\nFor the off-diagonal elements $I_{12}$ and $I_{21}$ (related to $\\alpha, \\theta$):\n$$I_{12} = I_{21} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\theta}\\right] = -E\\left[-\\frac{1}{\\theta}\\right]$$\nThe term $-\\frac{1}{\\theta}$ is a constant with respect to $x$, so its expectation is the constant itself.\n$$I_{12} = I_{21} = \\frac{1}{\\theta}$$\n\nFinally, we assemble the Fisher information matrix:\n$$I(\\alpha, \\theta) = \\begin{pmatrix} I_{11} & I_{12} \\\\ I_{21} & I_{22} \\end{pmatrix} = \\begin{pmatrix} \\psi'(\\alpha) & \\frac{1}{\\theta} \\\\ \\frac{1}{\\theta} & \\frac{\\alpha}{\\theta^2} \\end{pmatrix}$$", "answer": "$$\\boxed{\\begin{pmatrix} \\psi'(\\alpha) & \\frac{1}{\\theta} \\\\ \\frac{1}{\\theta} & \\frac{\\alpha}{\\theta^2} \\end{pmatrix}}$$", "id": "1914846"}]}