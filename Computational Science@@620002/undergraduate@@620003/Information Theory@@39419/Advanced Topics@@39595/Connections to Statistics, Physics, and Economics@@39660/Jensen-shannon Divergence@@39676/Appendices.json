{"hands_on_practices": [{"introduction": "Let's begin by putting the Jensen-Shannon Divergence into practice with a direct calculation. This exercise models a common scenario in machine learning where we need to compare the predictive outputs of two different models. By working through this foundational problem [@problem_id:1634129], you will solidify your understanding of the JSD formula and gain hands-on experience in quantifying the dissimilarity between two simple probability distributions.", "problem": "In the field of machine learning, it is often necessary to quantify the similarity between two probability distributions. One such measure is the Jensen-Shannon Divergence (JSD).\n\nConsider two competing image classification models, Model A and Model B. Both models are tasked with classifying an image into one of three possible categories: 'Cat', 'Dog', or 'Bird'. For a specific input image, Model A outputs a probability distribution $P$ over the categories, and Model B outputs a probability distribution $Q$. The distributions are given as:\n$$P = [p_1, p_2, p_3] = [0.8, 0.1, 0.1]$$\n$$Q = [q_1, q_2, q_3] = [0.1, 0.8, 0.1]$$\nwhere the elements correspond to the probabilities of 'Cat', 'Dog', and 'Bird', respectively.\n\nThe Jensen-Shannon Divergence between $P$ and $Q$ is defined using the Kullback-Leibler Divergence (KLD). For two discrete probability distributions $X = [x_i]$ and $Y = [y_i]$, the KLD is given by:\n$$D_{KL}(X || Y) = \\sum_{i} x_i \\log_{2}\\left(\\frac{x_i}{y_i}\\right)$$\nThe JSD is then defined as a symmetrized version of the KLD:\n$$JSD(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)$$\nwhere $M$ is the mixture distribution defined as $M = \\frac{1}{2}(P+Q)$. The logarithm is taken base 2, so the resulting divergence is measured in bits.\n\nCalculate the Jensen-Shannon Divergence, $JSD(P || Q)$, for the given distributions. Express your answer in bits, rounded to four significant figures.", "solution": "We are given $P=[0.8,0.1,0.1]$ and $Q=[0.1,0.8,0.1]$. The Jensen-Shannon Divergence is defined by\n$$\nJSD(P||Q)=\\frac{1}{2}D_{KL}(P||M)+\\frac{1}{2}D_{KL}(Q||M),\n$$\nwhere the mixture distribution is $M=\\frac{1}{2}(P+Q)$. Compute $M$ elementwise:\n$$\nM=\\frac{1}{2}([0.8+0.1,\\,0.1+0.8,\\,0.1+0.1])=[0.45,\\,0.45,\\,0.1].\n$$\nUsing the definition of Kullback-Leibler divergence with base-2 logarithm,\n$$\nD_{KL}(X||Y)=\\sum_{i}x_{i}\\log_{2}\\!\\left(\\frac{x_{i}}{y_{i}}\\right),\n$$\nwe compute $D_{KL}(P||M)$:\n$$\nD_{KL}(P||M)=0.8\\log_{2}\\!\\left(\\frac{0.8}{0.45}\\right)+0.1\\log_{2}\\!\\left(\\frac{0.1}{0.45}\\right)+0.1\\log_{2}\\!\\left(\\frac{0.1}{0.1}\\right).\n$$\nNote $\\frac{0.8}{0.45}=\\frac{16}{9}$, $\\frac{0.1}{0.45}=\\frac{2}{9}$, and $\\frac{0.1}{0.1}=1$, hence\n$$\nD_{KL}(P||M)=0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right)+0.1\\log_{2}(1).\n$$\nSince $\\log_{2}(1)=0$, this simplifies to\n$$\nD_{KL}(P||M)=0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right).\n$$\nBy symmetry, exchanging the first two coordinates, we get\n$$\nD_{KL}(Q||M)=0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right)+0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right).\n$$\nTherefore,\n$$\nJSD(P||Q)=\\frac{1}{2}\\left[0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right)\\right]+\\frac{1}{2}\\left[0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right)\\right],\n$$\nwhich equals\n$$\nJSD(P||Q)=0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right).\n$$\nSimplify each logarithm using $\\log_{2}\\!\\left(\\frac{16}{9}\\right)=\\log_{2}(16)-\\log_{2}(9)=4-2\\log_{2}(3)$ and $\\log_{2}\\!\\left(\\frac{2}{9}\\right)=\\log_{2}(2)-\\log_{2}(9)=1-2\\log_{2}(3)$. Then\n$$\nJSD(P||Q)=0.8\\left(4-2\\log_{2}(3)\\right)+0.1\\left(1-2\\log_{2}(3)\\right)=3.3-1.8\\log_{2}(3).\n$$\nFor a numerical value in bits, use $\\log_{2}(3)\\approx 1.5849625007$:\n$$\nJSD(P||Q)\\approx 3.3-1.8\\times 1.5849625007\\approx 3.3-2.8529325013\\approx 0.4470674987.\n$$\nRounding to four significant figures gives $0.4471$ bits.", "answer": "$$\\boxed{0.4471}$$", "id": "1634129"}, {"introduction": "Building on our foundational calculation, we now apply the Jensen-Shannon Divergence to a more structured scenario. This problem asks you to compare two binomial distributions, which are often used to model processes with binary outcomes, such as success/failure or defect/no-defect. This practice [@problem_id:1634170] is valuable because it requires you to first define the probability distributions from a parametric model before calculating their divergence, a common task in applied statistics and data analysis.", "problem": "Consider two distinct processes for manufacturing a component, Process A and Process B. Both processes can result in a component having 0, 1, or 2 defects, indexed by $k \\in \\{0, 1, 2\\}$.\n\nThe probability of a component from Process A having $k$ defects is given by the probability mass function $P(k)$, defined as:\n$$\nP(k) = \\binom{2}{k} (0.5)^{k} (1 - 0.5)^{2-k}\n$$\nThe probability of a component from Process B having $k$ defects is given by the probability mass function $Q(k)$, defined as:\n$$\nQ(k) = \\binom{2}{k} (0.25)^{k} (1 - 0.25)^{2-k}\n$$\nTo quantify the dissimilarity between these two manufacturing processes, we can compute a symmetric divergence score, $S$. This score is calculated in two steps. First, we define a mixture probability distribution, $M(k)$, as the average of the two distributions:\n$$\nM(k) = \\frac{P(k) + Q(k)}{2}\n$$\nSecond, we calculate the score $S$ using the following formula:\n$$\nS = \\frac{1}{2} \\sum_{k=0}^{2} \\left[ P(k) \\ln\\left(\\frac{P(k)}{M(k)}\\right) + Q(k) \\ln\\left(\\frac{Q(k)}{M(k)}\\right) \\right]\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nCalculate the numerical value of the dissimilarity score $S$. Round your final answer to four significant figures.", "solution": "We are given two binomial probability mass functions over $k \\in \\{0,1,2\\}$: $P(k)$ with parameters $n=2$, $p=\\frac{1}{2}$ and $Q(k)$ with parameters $n=2$, $p=\\frac{1}{4}$. Using the binomial formula,\n$$\nP(k) = \\binom{2}{k}\\left(\\frac{1}{2}\\right)^{k}\\left(1-\\frac{1}{2}\\right)^{2-k}, \n\\quad\nQ(k) = \\binom{2}{k}\\left(\\frac{1}{4}\\right)^{k}\\left(1-\\frac{1}{4}\\right)^{2-k}.\n$$\nEvaluating these for each $k$ yields\n$$\nP(0)=\\frac{1}{4},\\quad P(1)=\\frac{1}{2},\\quad P(2)=\\frac{1}{4},\n$$\n$$\nQ(0)=\\left(\\frac{3}{4}\\right)^{2}=\\frac{9}{16},\\quad Q(1)=2\\cdot\\frac{1}{4}\\cdot\\frac{3}{4}=\\frac{3}{8},\\quad Q(2)=\\left(\\frac{1}{4}\\right)^{2}=\\frac{1}{16}.\n$$\nThe mixture distribution $M(k)=\\frac{P(k)+Q(k)}{2}$ is then\n$$\nM(0)=\\frac{\\frac{1}{4}+\\frac{9}{16}}{2}=\\frac{13}{32},\\quad\nM(1)=\\frac{\\frac{1}{2}+\\frac{3}{8}}{2}=\\frac{7}{16},\\quad\nM(2)=\\frac{\\frac{1}{4}+\\frac{1}{16}}{2}=\\frac{5}{32}.\n$$\nThe dissimilarity score is\n$$\nS=\\frac{1}{2}\\sum_{k=0}^{2}\\left[P(k)\\ln\\!\\left(\\frac{P(k)}{M(k)}\\right)+Q(k)\\ln\\!\\left(\\frac{Q(k)}{M(k)}\\right)\\right].\n$$\nCompute the ratios for each $k$:\n$$\n\\frac{P(0)}{M(0)}=\\frac{\\frac{1}{4}}{\\frac{13}{32}}=\\frac{8}{13},\\quad\n\\frac{Q(0)}{M(0)}=\\frac{\\frac{9}{16}}{\\frac{13}{32}}=\\frac{18}{13},\n$$\n$$\n\\frac{P(1)}{M(1)}=\\frac{\\frac{1}{2}}{\\frac{7}{16}}=\\frac{8}{7},\\quad\n\\frac{Q(1)}{M(1)}=\\frac{\\frac{3}{8}}{\\frac{7}{16}}=\\frac{6}{7},\n$$\n$$\n\\frac{P(2)}{M(2)}=\\frac{\\frac{1}{4}}{\\frac{5}{32}}=\\frac{8}{5},\\quad\n\\frac{Q(2)}{M(2)}=\\frac{\\frac{1}{16}}{\\frac{5}{32}}=\\frac{2}{5}.\n$$\nThus the sum inside $S$ decomposes as\n$$\n\\sum_{k=0}^{2}\\left[P(k)\\ln\\!\\left(\\frac{P(k)}{M(k)}\\right)+Q(k)\\ln\\!\\left(\\frac{Q(k)}{M(k)}\\right)\\right]\n= C_{0}+C_{1}+C_{2},\n$$\nwhere\n$$\nC_{0}=\\frac{1}{4}\\ln\\!\\left(\\frac{8}{13}\\right)+\\frac{9}{16}\\ln\\!\\left(\\frac{18}{13}\\right),\\quad\nC_{1}=\\frac{1}{2}\\ln\\!\\left(\\frac{8}{7}\\right)+\\frac{3}{8}\\ln\\!\\left(\\frac{6}{7}\\right),\\quad\nC_{2}=\\frac{1}{4}\\ln\\!\\left(\\frac{8}{5}\\right)+\\frac{1}{16}\\ln\\!\\left(\\frac{2}{5}\\right).\n$$\nUsing the logarithm property $\\ln\\!\\left(\\frac{a}{b}\\right)=\\ln a-\\ln b$ and then evaluating numerically,\n$$\n\\ln\\!\\left(\\frac{8}{13}\\right)\\approx -0.485507816,\\quad\n\\ln\\!\\left(\\frac{18}{13}\\right)\\approx 0.325422400,\\quad\n\\ln\\!\\left(\\frac{8}{7}\\right)\\approx 0.133531393,\\quad\n\\ln\\!\\left(\\frac{6}{7}\\right)\\approx -0.154150680,\n$$\n$$\n\\ln\\!\\left(\\frac{8}{5}\\right)\\approx 0.470003629,\\quad\n\\ln\\!\\left(\\frac{2}{5}\\right)\\approx -0.916290732.\n$$\nHence\n$$\nC_{0}\\approx \\frac{1}{4}(-0.485507816)+\\frac{9}{16}(0.325422400)\\approx 0.061673146,\n$$\n$$\nC_{1}\\approx \\frac{1}{2}(0.133531393)+\\frac{3}{8}(-0.154150680)\\approx 0.008959191,\n$$\n$$\nC_{2}\\approx \\frac{1}{4}(0.470003629)+\\frac{1}{16}(-0.916290732)\\approx 0.060232737.\n$$\nSumming gives\n$$\n\\sum_{k=0}^{2}\\left[P(k)\\ln\\!\\left(\\frac{P(k)}{M(k)}\\right)+Q(k)\\ln\\!\\left(\\frac{Q(k)}{M(k)}\\right)\\right]\\approx 0.130865074,\n$$\nso\n$$\nS=\\frac{1}{2}\\times 0.130865074\\approx 0.065432537.\n$$\nRounded to four significant figures, the dissimilarity score is $0.06543$.", "answer": "$$\\boxed{0.06543}$$", "id": "1634170"}, {"introduction": "After practicing numerical computations, let's now transition to a more analytical exercise to deepen our conceptual understanding. This problem [@problem_id:1634132] challenges you to derive a general formula for the JSD between two fundamental types of distributions: one of complete uncertainty (uniform) and one of complete certainty (deterministic). By working with symbolic variables rather than fixed numbers, you will gain insight into how the JSD scales and behaves in these important limiting cases.", "problem": "Consider a system that transmits messages using a discrete alphabet $\\mathcal{X}$ containing $N$ distinct symbols, where $N$ is an integer such that $N \\ge 2$. We are interested in comparing two different probability distributions, $P$ and $Q$, over this alphabet.\n\nThe first distribution, $P$, is a uniform distribution, where every symbol has an equal probability of occurrence.\n\nThe second distribution, $Q$, is a deterministic distribution, where one specific symbol is transmitted with certainty (probability 1) and all other $N-1$ symbols have a probability of 0.\n\nThe dissimilarity between these two distributions can be quantified using the Jensen-Shannon Divergence (JSD). The JSD is a symmetrized and smoothed version of the Kullback-Leibler (KL) divergence. For any two discrete probability distributions $A$ and $B$ over the same alphabet $\\mathcal{X} = \\{x_1, x_2, \\dots, x_N\\}$, the KL divergence is defined as:\n$$D_{KL}(A || B) = \\sum_{i=1}^{N} A(x_i) \\ln \\left(\\frac{A(x_i)}{B(x_i)}\\right)$$\nHere, $\\ln$ denotes the natural logarithm, and by convention, the term in the sum is taken to be zero if $A(x_i) = 0$.\n\nThe JSD between $P$ and $Q$ is then given by:\n$$\\text{JSD}(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)$$\nwhere $M$ is the mixture distribution defined as $M = \\frac{1}{2}(P+Q)$.\n\nDerive a closed-form analytic expression for the Jensen-Shannon Divergence, $\\text{JSD}(P || Q)$, in terms of the alphabet size $N$.", "solution": "Let the alphabet be $\\mathcal{X}=\\{x_{1},\\dots,x_{N}\\}$ with $N\\ge 2$. The uniform distribution $P$ assigns $P(x_{i})=\\frac{1}{N}$ for all $i$, and the deterministic distribution $Q$ assigns $Q(x_{1})=1$ and $Q(x_{i})=0$ for $i\\ge 2$.\n\nThe mixture distribution is $M=\\frac{1}{2}(P+Q)$, so\n$$\nM(x_{1})=\\frac{1}{2}\\left(\\frac{1}{N}+1\\right)=\\frac{N+1}{2N},\\qquad\nM(x_{i})=\\frac{1}{2}\\left(\\frac{1}{N}+0\\right)=\\frac{1}{2N}\\quad\\text{for }i=2,\\dots,N.\n$$\n\nCompute $D_{KL}(P\\|\\!M)$ using $D_{KL}(A\\|\\!B)=\\sum_{i}A(x_{i})\\ln\\!\\left(\\frac{A(x_{i})}{B(x_{i})}\\right)$:\n$$\nD_{KL}(P\\|\\!M)=\\sum_{i=1}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{M(x_{i})}\\right)\n=\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{\\frac{N+1}{2N}}\\right)+\\sum_{i=2}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{\\frac{1}{2N}}\\right).\n$$\nThis gives\n$$\nD_{KL}(P\\|\\!M)=\\frac{1}{N}\\ln\\!\\left(\\frac{2}{N+1}\\right)+(N-1)\\cdot\\frac{1}{N}\\ln 2\n=\\left(\\frac{1}{N}+\\frac{N-1}{N}\\right)\\ln 2-\\frac{1}{N}\\ln(N+1)\n=\\ln 2-\\frac{1}{N}\\ln(N+1).\n$$\n\nCompute $D_{KL}(Q\\|\\!M)$; only $x_{1}$ contributes since $Q(x_{1})=1$:\n$$\nD_{KL}(Q\\|\\!M)=\\ln\\!\\left(\\frac{1}{M(x_{1})}\\right)=-\\ln\\!\\left(\\frac{N+1}{2N}\\right)=\\ln\\!\\left(\\frac{2N}{N+1}\\right)=\\ln 2+\\ln N-\\ln(N+1).\n$$\n\nThe Jensen-Shannon divergence is\n$$\n\\text{JSD}(P\\|\\!Q)=\\frac{1}{2}\\left[D_{KL}(P\\|\\!M)+D_{KL}(Q\\|\\!M)\\right]\n=\\frac{1}{2}\\left[\\ln 2-\\frac{1}{N}\\ln(N+1)+\\ln 2+\\ln N-\\ln(N+1)\\right].\n$$\nTherefore,\n$$\n\\text{JSD}(P\\|\\!Q)=\\ln 2+\\frac{1}{2}\\ln N-\\frac{1}{2}\\left(1+\\frac{1}{N}\\right)\\ln(N+1)\n=\\ln 2+\\frac{1}{2}\\ln N-\\frac{N+1}{2N}\\ln(N+1).\n$$\nThis is a closed-form expression in terms of $N$.", "answer": "$$\\boxed{\\ln 2+\\frac{1}{2}\\ln N-\\frac{N+1}{2N}\\ln(N+1)}$$", "id": "1634132"}]}