{"hands_on_practices": [{"introduction": "The principle of minimum information provides a powerful framework for updating our knowledge in light of new evidence. This exercise demonstrates how to find the most objective probability distribution that satisfies new constraints, such as an observed average value, by minimizing the Kullback-Leibler (KL) divergence from an initial belief. Mastering this technique [@problem_id:1631993] is essential for understanding the foundations of statistical modeling and maximum entropy methods used across science and engineering.", "problem": "An analyst is studying a special three-sided die with faces labeled 1, 2, and 3. The analyst's initial belief, based on symmetry, is that the die is fair, meaning each outcome has a probability of 1/3. This initial belief is represented by a uniform probability distribution $u(x)$ over the set of outcomes $X = \\{1, 2, 3\\}$.\n\nAfter observing a large number of rolls, the analyst finds that the sample mean of the outcomes is exactly 2.5. This new information leads the analyst to update their belief to a new probability distribution, $p(x)$. According to the principle of minimum information, the new distribution $p(x)$ should be the one that is closest to the original distribution $u(x)$ while being consistent with the new data.\n\nThe \"distance\" between the two distributions is measured by the Kullback-Leibler (KL) divergence, defined as $D_{KL}(p\\|u) = \\sum_{x \\in X} p(x) \\ln\\left(\\frac{p(x)}{u(x)}\\right)$, where $\\ln$ denotes the natural logarithm.\n\nFind the probability of rolling a 3, i.e., $p(3)$, under this updated distribution. Express your answer as an exact analytic expression.", "solution": "We must find the distribution $p(x)$ on $X=\\{1,2,3\\}$ that minimizes the Kullback-Leibler divergence to the uniform $u(x)$ subject to the constraints $\\sum_{x}p(x)=1$ and $\\sum_{x}x\\,p(x)=\\frac{5}{2}$. This is a constrained optimization problem:\n$$\n\\min_{p}\\; D_{KL}(p\\|u)=\\sum_{x=1}^{3}p(x)\\ln\\!\\left(\\frac{p(x)}{u(x)}\\right)\n$$\nsubject to\n$$\n\\sum_{x=1}^{3}p(x)=1,\\qquad \\sum_{x=1}^{3}x\\,p(x)=\\frac{5}{2}.\n$$\nIntroduce Lagrange multipliers $\\lambda$ and $\\mu$ and consider\n$$\n\\mathcal{L}=\\sum_{x=1}^{3}p(x)\\ln\\!\\left(\\frac{p(x)}{u(x)}\\right)+\\lambda\\!\\left(\\sum_{x=1}^{3}p(x)-1\\right)+\\mu\\!\\left(\\sum_{x=1}^{3}x\\,p(x)-\\frac{5}{2}\\right).\n$$\nStationarity with respect to $p(x)$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p(x)}=\\ln\\!\\left(\\frac{p(x)}{u(x)}\\right)+1+\\lambda+\\mu x=0,\n$$\nso\n$$\n\\ln p(x)=\\ln u(x)-1-\\lambda-\\mu x \\quad\\Rightarrow\\quad p(x)=u(x)\\,A\\,\\exp(\\eta x),\n$$\nwhere $A=\\exp(-1-\\lambda)$ and $\\eta=-\\mu$. Normalization fixes $A$:\n$$\np(x)=\\frac{u(x)\\exp(\\eta x)}{\\sum_{k=1}^{3}u(k)\\exp(\\eta k)}.\n$$\nSince $u(x)=\\frac{1}{3}$ is uniform, the factor $u$ cancels and\n$$\np(x)=\\frac{\\exp(\\eta x)}{\\sum_{k=1}^{3}\\exp(\\eta k)}.\n$$\nLet $r=\\exp(\\eta)>0$. Then\n$$\np(x)=\\frac{r^{x}}{r+r^{2}+r^{3}},\\qquad \\sum_{x=1}^{3}x\\,p(x)=\\frac{r+2r^{2}+3r^{3}}{r+r^{2}+r^{3}}=\\frac{5}{2}.\n$$\nDividing numerator and denominator by $r$ yields\n$$\n\\frac{1+2r+3r^{2}}{1+r+r^{2}}=\\frac{5}{2}.\n$$\nCross-multiplying gives\n$$\n2(1+2r+3r^{2})=5(1+r+r^{2}) \\;\\Rightarrow\\; 2+4r+6r^{2}=5+5r+5r^{2} \\;\\Rightarrow\\; r^{2}-r-3=0.\n$$\nThus\n$$\nr=\\frac{1+\\sqrt{13}}{2}.\n$$\nNow\n$$\np(3)=\\frac{r^{3}}{r+r^{2}+r^{3}}=\\frac{r^{2}}{1+r+r^{2}}.\n$$\nUsing $r^{2}=r+3$ (from $r^{2}-r-3=0$), we obtain\n$$\np(3)=\\frac{r+3}{4+2r}=\\frac{1}{2}\\cdot\\frac{r+3}{2+r}.\n$$\nSubstituting $r=\\frac{1+\\sqrt{13}}{2}$,\n$$\nr+3=\\frac{7+\\sqrt{13}}{2},\\quad 2+r=\\frac{5+\\sqrt{13}}{2},\n$$\nso\n$$\np(3)=\\frac{1}{2}\\cdot\\frac{7+\\sqrt{13}}{5+\\sqrt{13}}=\\frac{1}{2}\\cdot\\frac{(7+\\sqrt{13})(5-\\sqrt{13})}{25-13}=\\frac{22-2\\sqrt{13}}{24}=\\frac{11-\\sqrt{13}}{12}.\n$$\nTherefore, the updated probability of rolling a 3 is $\\frac{11-\\sqrt{13}}{12}$.", "answer": "$$\\boxed{\\frac{11-\\sqrt{13}}{12}}$$", "id": "1631993"}, {"introduction": "After updating our models with data, we naturally ask: how much information did that data actually provide? This is precisely the question that Fisher Information answers. This practice explores a foundational calculation of Fisher Information for a sequence of Bernoulli trials, a model that underpins countless real-world scenarios from clinical trials to digital communication [@problem_id:1632005]. By working through this problem, you will gain a concrete understanding of how we can quantify the \"value\" of data in estimating an unknown parameter.", "problem": "In a digital communication system, a sequence of $n$ identical bits is transmitted through a noisy channel to establish a baseline for its reliability. Each transmitted bit is a '1'. Due to noise, each bit reception is an independent random event. The probability of correctly receiving a '1' is denoted by $p$, where $0 < p < 1$. Correspondingly, the probability of an error (receiving a '0') is $1-p$. The sequence of $n$ received bits can thus be modeled as a set of $n$ independent Bernoulli trials.\n\nThe Fisher Information is a fundamental concept in information theory and statistics that quantifies the amount of information a random variable carries about an unknown parameter upon which its probability depends. For this communication system, the parameter of interest is the success probability $p$.\n\nDetermine the Fisher Information, $I(p)$, for the parameter $p$ based on the sequence of $n$ received bits. Express your answer as a single closed-form analytic expression in terms of $n$ and $p$.", "solution": "The problem asks for the Fisher Information $I(p)$ for the success probability parameter $p$ of $n$ independent Bernoulli trials.\n\nLet the sequence of received bits be represented by the random variables $X_1, X_2, \\ldots, X_n$. Each $X_i$ is a Bernoulli random variable where $X_i=1$ if the bit is received correctly (success) and $X_i=0$ if it is received incorrectly (failure). The probability mass function (PMF) for a single trial $X_i$ is given by:\n$$P(X_i=x | p) = p^x (1-p)^{1-x} \\quad \\text{for } x \\in \\{0, 1\\}$$\n\nSince the trials are independent, the joint probability mass function, or the likelihood function $L(p)$ for a specific observed sequence $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$, is the product of the individual PMFs:\n$$L(p | \\mathbf{x}) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i}$$\nThis can be simplified by combining the exponents:\n$$L(p | \\mathbf{x}) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i}$$\nLet $k = \\sum_{i=1}^{n} x_i$ be the total number of successes (correctly received bits) in the sequence of $n$ trials. The likelihood function becomes:\n$$L(p | k) = p^k (1-p)^{n-k}$$\n\nThe Fisher Information is typically calculated using the log-likelihood function, denoted by $\\ell(p)$. The log-likelihood is the natural logarithm of the likelihood function:\n$$\\ell(p | k) = \\ln(L(p | k)) = \\ln(p^k (1-p)^{n-k}) = k \\ln(p) + (n-k) \\ln(1-p)$$\n\nOne definition of the Fisher Information $I(p)$ is the negative of the expected value of the second derivative of the log-likelihood function with respect to the parameter $p$:\n$$I(p) = -E\\left[\\frac{\\partial^2 \\ell(p|k)}{\\partial p^2}\\right]$$\n\nFirst, we find the first derivative of the log-likelihood with respect to $p$. This is known as the score function.\n$$\\frac{\\partial \\ell}{\\partial p} = \\frac{\\partial}{\\partial p} [k \\ln(p) + (n-k) \\ln(1-p)] = \\frac{k}{p} - \\frac{n-k}{1-p}$$\n\nNext, we find the second derivative of the log-likelihood with respect to $p$:\n$$\\frac{\\partial^2 \\ell}{\\partial p^2} = \\frac{\\partial}{\\partial p} \\left[\\frac{k}{p} - \\frac{n-k}{1-p}\\right] = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}(-1) = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}$$\n\nNow, we compute the expected value of this second derivative. The random variable here is $k$, the number of successes. Since $k$ is the sum of $n$ independent and identically distributed Bernoulli trials with success probability $p$, $k$ follows a binomial distribution, $k \\sim \\text{Bin}(n, p)$. The expected value of $k$ is $E[k] = np$.\n\nWe substitute this into the expression for $I(p)$:\n$$I(p) = -E\\left[-\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}\\right] = E\\left[\\frac{k}{p^2} + \\frac{n-k}{(1-p)^2}\\right]$$\nBy linearity of expectation:\n$$I(p) = \\frac{E[k]}{p^2} + \\frac{E[n-k]}{(1-p)^2}$$\nWe know $E[k] = np$. Also, $E[n-k] = E[n] - E[k] = n - np = n(1-p)$.\nSubstituting these expected values:\n$$I(p) = \\frac{np}{p^2} + \\frac{n(1-p)}{(1-p)^2}$$\nSimplifying the terms gives:\n$$I(p) = \\frac{n}{p} + \\frac{n}{1-p}$$\nCombining the terms over a common denominator:\n$$I(p) = n \\left(\\frac{1-p+p}{p(1-p)}\\right) = \\frac{n}{p(1-p)}$$\n\nThus, the Fisher Information for the parameter $p$ from $n$ Bernoulli trials is $\\frac{n}{p(1-p)}$. This result represents the total information from all $n$ trials, which is simply $n$ times the Fisher Information for a single trial, $I_1(p) = \\frac{1}{p(1-p)}$.", "answer": "$$\\boxed{\\frac{n}{p(1-p)}}$$", "id": "1632005"}, {"introduction": "Bridging the gap from theoretical formulas to practical data analysis requires confronting the limitations of finite samples. While the Shannon entropy formula is exact for a known distribution, we must often estimate it using observed frequencies, a method known as the \"plug-in\" estimator. This exercise [@problem_id:1632000] guides you through a derivation of the leading-order term of the systematic bias in this estimator, revealing a fundamental challenge in computational statistics and underscoring the importance of understanding estimator properties.", "problem": "Consider a discrete random variable $X$ that can take on one of $K$ distinct outcomes, $\\{x_1, x_2, \\dots, x_K\\}$. The corresponding true probabilities are given by the set $\\{p_1, p_2, \\dots, p_K\\}$, where each $p_i > 0$ and their sum is $\\sum_{i=1}^K p_i = 1$. The true Shannon entropy of this distribution, which measures its uncertainty, is given by the formula $H = -\\sum_{i=1}^K p_i \\ln(p_i)$.\n\nIn a practical scenario, the true probabilities $p_i$ are unknown and must be estimated from data. To do this, we collect a set of $N$ independent and identically distributed samples from the distribution of $X$. Let $n_i$ denote the number of times the outcome $x_i$ is observed in this sample, such that $\\sum_{i=1}^K n_i = N$. The empirical probability of outcome $x_i$ is then estimated as $\\hat{p}_i = n_i / N$.\n\nThe \"plug-in\" estimator for the entropy, denoted $\\hat{H}$, is computed by substituting these empirical probabilities into the entropy formula:\n$$\n\\hat{H} = -\\sum_{i=1}^K \\hat{p}_i \\ln(\\hat{p}_i)\n$$\nBy convention, the term $\\hat{p}_i \\ln(\\hat{p}_i)$ is taken to be zero if $\\hat{p}_i=0$.\n\nThis empirical estimator $\\hat{H}$ is known to be biased, meaning its expected value, $\\mathbb{E}[\\hat{H}]$, does not equal the true entropy $H$. For a large number of samples $N$, this bias, defined as $B(\\hat{H}) = \\mathbb{E}[\\hat{H}] - H$, can be expressed as a series expansion in powers of $1/N$.\n\nDetermine the leading-order term of this bias. Your answer should be a single closed-form analytic expression in terms of the total number of samples $N$ and the number of distinct outcomes $K$.", "solution": "Let $g(u)=-u\\ln(u)$. Then $H=\\sum_{i=1}^{K}g(p_{i})$ and $\\hat{H}=\\sum_{i=1}^{K}g(\\hat{p}_{i})$. For each $i$, perform a second-order Taylor expansion of $g(\\hat{p}_{i})$ about $p_{i}$:\n$$\ng(\\hat{p}_{i})=g(p_{i})+g'(p_{i})(\\hat{p}_{i}-p_{i})+\\frac{1}{2}g''(p_{i})(\\hat{p}_{i}-p_{i})^{2}+R_{i},\n$$\nwhere $R_{i}$ collects higher-order terms. The derivatives are\n$$\ng'(u)=-\\ln(u)-1,\\qquad g''(u)=-\\frac{1}{u}.\n$$\nTaking expectations and using $\\mathbb{E}[\\hat{p}_{i}]=p_{i}$ gives $\\mathbb{E}[\\hat{p}_{i}-p_{i}]=0$, so the linear term vanishes in expectation. Because $g$ is separable across $i$, there are no mixed second derivatives, hence cross-covariances do not contribute at second order. Therefore,\n$$\n\\mathbb{E}[g(\\hat{p}_{i})]-g(p_{i})=\\frac{1}{2}g''(p_{i})\\operatorname{Var}(\\hat{p}_{i})+O\\!\\left(\\frac{1}{N^{2}}\\right).\n$$\nFor a multinomial sample of size $N$,\n$$\n\\operatorname{Var}(\\hat{p}_{i})=\\frac{p_{i}(1-p_{i})}{N}.\n$$\nThus,\n$$\n\\mathbb{E}[g(\\hat{p}_{i})]-g(p_{i})=\\frac{1}{2}\\left(-\\frac{1}{p_{i}}\\right)\\frac{p_{i}(1-p_{i})}{N}+O\\!\\left(\\frac{1}{N^{2}}\\right)=-\\frac{1-p_{i}}{2N}+O\\!\\left(\\frac{1}{N^{2}}\\right).\n$$\nSumming over $i=1,\\dots,K$ yields\n$$\n\\mathbb{E}[\\hat{H}]-H=\\sum_{i=1}^{K}\\left(-\\frac{1-p_{i}}{2N}\\right)+O\\!\\left(\\frac{1}{N^{2}}\\right)=-\\frac{1}{2N}\\sum_{i=1}^{K}(1-p_{i})+O\\!\\left(\\frac{1}{N^{2}}\\right).\n$$\nUsing $\\sum_{i=1}^{K}p_{i}=1$ gives $\\sum_{i=1}^{K}(1-p_{i})=K-1$, hence the leading-order bias term is\n$$\n\\mathbb{E}[\\hat{H}]-H=-\\frac{K-1}{2N}+O\\!\\left(\\frac{1}{N^{2}}\\right).\n$$\nTherefore, the leading-order term of the bias is $-\\frac{K-1}{2N}$.", "answer": "$$\\boxed{-\\frac{K-1}{2N}}$$", "id": "1632000"}]}