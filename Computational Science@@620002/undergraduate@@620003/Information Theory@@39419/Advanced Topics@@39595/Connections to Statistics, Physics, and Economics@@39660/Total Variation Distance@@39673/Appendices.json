{"hands_on_practices": [{"introduction": "To begin, let's ground our understanding of total variation distance with a tangible calculation. This first practice problem [@problem_id:1664855] asks you to compare two binomial distributions, a common task in quality control and statistical testing. By working through this example, you will gain hands-on experience in applying the total variation distance formula and see how it quantifies the maximum difference in probability for any event, offering a direct measure of distinguishability.", "problem": "Two competing manufacturers, \"CircuitSys\" and \"Logicore,\" produce a type of specialized logic chip. Each chip contains 3 identical processing cores. The quality of a chip is determined by the number of non-defective cores it contains.\n\nFor a chip from CircuitSys, each of the 3 cores has an independent probability of being non-defective equal to $p_C = \\frac{1}{2}$.\nFor a chip from Logicore, each of the 3 cores has an independent probability of being non-defective equal to $p_L = \\frac{1}{4}$.\n\nLet the random variable $X_C$ be the number of non-defective cores in a CircuitSys chip, and $X_L$ be the number of non-defective cores in a Logicore chip. A quality control agency wants to design a binary test to distinguish between chips from the two manufacturers. The test involves specifying a set of outcomes $A \\subseteq \\{0, 1, 2, 3\\}$, where an outcome is the number of non-defective cores. To achieve the best possible separation between the two manufacturers, the agency decides to choose the set $A$ that maximizes the absolute difference in the probabilities, $|P(X_C \\in A) - P(X_L \\in A)|$.\n\nCalculate this maximum possible difference. Express your answer as an exact fraction in its simplest form.", "solution": "Let $X_{C} \\sim \\mathrm{Binomial}(3,\\frac{1}{2})$ and $X_{L} \\sim \\mathrm{Binomial}(3,\\frac{1}{4})$. Their probability mass functions are\n$$\nP_{C}(k)=\\binom{3}{k}\\left(\\frac{1}{2}\\right)^{k}\\left(1-\\frac{1}{2}\\right)^{3-k}=\\binom{3}{k}\\left(\\frac{1}{2}\\right)^{3},\\quad k\\in\\{0,1,2,3\\},\n$$\n$$\nP_{L}(k)=\\binom{3}{k}\\left(\\frac{1}{4}\\right)^{k}\\left(\\frac{3}{4}\\right)^{3-k},\\quad k\\in\\{0,1,2,3\\}.\n$$\nEvaluating each,\n$$\nP_{C}(0)=\\frac{1}{8},\\quad P_{C}(1)=\\frac{3}{8},\\quad P_{C}(2)=\\frac{3}{8},\\quad P_{C}(3)=\\frac{1}{8},\n$$\n$$\nP_{L}(0)=\\left(\\frac{3}{4}\\right)^{3}=\\frac{27}{64},\\quad P_{L}(1)=3\\cdot\\frac{1}{4}\\cdot\\left(\\frac{3}{4}\\right)^{2}=\\frac{27}{64},\\quad P_{L}(2)=3\\cdot\\left(\\frac{1}{4}\\right)^{2}\\cdot\\frac{3}{4}=\\frac{9}{64},\\quad P_{L}(3)=\\left(\\frac{1}{4}\\right)^{3}=\\frac{1}{64}.\n$$\nLet $d(k)=P_{C}(k)-P_{L}(k)$. Then\n$$\nd(0)=\\frac{1}{8}-\\frac{27}{64}=-\\frac{19}{64},\\quad d(1)=\\frac{3}{8}-\\frac{27}{64}=-\\frac{3}{64},\\quad d(2)=\\frac{3}{8}-\\frac{9}{64}=\\frac{15}{64},\\quad d(3)=\\frac{1}{8}-\\frac{1}{64}=\\frac{7}{64}.\n$$\nFor any $A\\subseteq\\{0,1,2,3\\}$,\n$$\nP(X_{C}\\in A)-P(X_{L}\\in A)=\\sum_{k\\in A}d(k).\n$$\nTo maximize the absolute difference, choose $A$ to include exactly those $k$ with $d(k)0$ (this is the standard characterization of the set achieving the total variation distance). Hence take $A=\\{2,3\\}$, giving\n$$\n\\max_{A}\\left|P(X_{C}\\in A)-P(X_{L}\\in A)\\right|=\\sum_{k:d(k)0}d(k)=\\frac{15}{64}+\\frac{7}{64}=\\frac{22}{64}=\\frac{11}{32}.\n$$\nEquivalently, this equals $\\frac{1}{2}\\sum_{k=0}^{3}|P_{C}(k)-P_{L}(k)|=\\frac{1}{2}\\cdot\\frac{44}{64}=\\frac{11}{32}$.", "answer": "$$\\boxed{\\frac{11}{32}}$$", "id": "1664855"}, {"introduction": "Having mastered the basic calculation, we now explore a fundamental property of total variation distance: its behavior under data processing. This exercise [@problem_id:1664823] demonstrates that applying a function to a random variable cannot increase the distinguishability between its potential distributions. This principle, a form of a Data Processing Inequality, is a cornerstone of information theory, highlighting that information can be lost but never created through processing.", "problem": "In information theory, one way to measure the \"distinguishability\" of two probability distributions is the Total Variation (TV) distance. For two probability distributions $P$ and $Q$ defined on the same discrete sample space $\\mathcal{A}$, the TV distance is given by the formula:\n$$d_{TV}(P, Q) = \\frac{1}{2} \\sum_{a \\in \\mathcal{A}} |P(a) - Q(a)|$$\n\nConsider two random variables, $X_1$ and $X_2$, which both take values from the set $\\mathcal{X} = \\{0, 1, 2, 3\\}$. Their respective probability mass functions, denoted $P_1(x)$ and $P_2(x)$, are given in the table below.\n\n| $x$ | $P_1(x)$ | $P_2(x)$ |\n|:---:|:--------:|:--------:|\n| 0   | 0.4      | 0.2      |\n| 1   | 0.3      | 0.1      |\n| 2   | 0.1      | 0.4      |\n| 3   | 0.2      | 0.3      |\n\nNow, let's define a new random variable $Y$ as a function of $X$ by the transformation $Y = X \\pmod 2$. The sample space for $Y$ is $\\mathcal{Y} = \\{0, 1\\}$. Let $Q_1(y)$ and $Q_2(y)$ be the probability mass functions for the random variables $Y_1 = X_1 \\pmod 2$ and $Y_2 = X_2 \\pmod 2$, respectively.\n\nCalculate the ratio $R = \\frac{d_{TV}(Q_1, Q_2)}{d_{TV}(P_1, P_2)}$. Round your final answer to three significant figures.", "solution": "We are given two pmfs $P_{1}$ and $P_{2}$ on $\\mathcal{X}=\\{0,1,2,3\\}$ and the total variation distance\n$$\nd_{TV}(P,Q)=\\frac{1}{2}\\sum_{a\\in\\mathcal{A}}|P(a)-Q(a)|.\n$$\nFirst compute $d_{TV}(P_{1},P_{2})$:\n$$\n\\sum_{x\\in\\{0,1,2,3\\}}|P_{1}(x)-P_{2}(x)|=|0.4-0.2|+|0.3-0.1|+|0.1-0.4|+|0.2-0.3|.\n$$\nEvaluating each term,\n$$\n0.2+0.2+0.3+0.1=0.8,\\quad\\text{so}\\quad d_{TV}(P_{1},P_{2})=\\frac{1}{2}\\cdot 0.8=0.4.\n$$\n\nDefine $Y=X\\bmod 2$ with $\\mathcal{Y}=\\{0,1\\}$. For $Y_{1}=X_{1}\\bmod 2$ and $Y_{2}=X_{2}\\bmod 2$, their pmfs are obtained by summing over preimages of $0$ (even) and $1$ (odd):\n$$\nQ_{1}(0)=P_{1}(0)+P_{1}(2)=0.4+0.1=0.5,\\quad Q_{1}(1)=P_{1}(1)+P_{1}(3)=0.3+0.2=0.5,\n$$\n$$\nQ_{2}(0)=P_{2}(0)+P_{2}(2)=0.2+0.4=0.6,\\quad Q_{2}(1)=P_{2}(1)+P_{2}(3)=0.1+0.3=0.4.\n$$\nNow compute $d_{TV}(Q_{1},Q_{2})$:\n$$\n\\sum_{y\\in\\{0,1\\}}|Q_{1}(y)-Q_{2}(y)|=|0.5-0.6|+|0.5-0.4|=0.1+0.1=0.2,\n$$\nhence\n$$\nd_{TV}(Q_{1},Q_{2})=\\frac{1}{2}\\cdot 0.2=0.1.\n$$\n\nThe ratio is\n$$\nR=\\frac{d_{TV}(Q_{1},Q_{2})}{d_{TV}(P_{1},P_{2})}=\\frac{0.1}{0.4}=0.25.\n$$\nRounded to three significant figures, $R=0.250$.", "answer": "$$\\boxed{0.250}$$", "id": "1664823"}, {"introduction": "Our final practice problem presents a thought-provoking scenario to deepen your conceptual understanding of what total variation distance truly measures. This problem [@problem_id:1664827] challenges you to compare two distributions that share the exact same average value but are, in fact, maximally different. This exercise brilliantly illustrates that summary statistics like the mean can be deceptive, and the total variation distance provides a more robust measure of how fundamentally separable two probabilistic models are.", "problem": "Let $P$ and $Q$ be two distinct probability distributions for a random variable $X$ defined over the sample space $\\Omega = \\{1, 2, 3, 4\\}$.\n\nThe distributions are constructed based on the following rules:\n1.  The probability mass function $P(x)$ is non-zero only for outcomes in the subset $\\{1, 4\\}$, and the probabilities for these two outcomes are equal.\n2.  The probability mass function $Q(x)$ is non-zero only for outcomes in the subset $\\{2, 3\\}$, and the probabilities for these two outcomes are also equal.\n3.  The two distributions result in the same expected value for the random variable $X$, i.e., $\\mathbb{E}_P[X] = \\mathbb{E}_Q[X]$.\n\nYour task is to calculate the total variation distance between $P$ and $Q$. The total variation distance is defined as $d_{TV}(P, Q) = \\frac{1}{2} \\sum_{x \\in \\Omega} |P(x) - Q(x)|$.\n\nExpress your answer as a single real number.", "solution": "The problem asks for the total variation distance between two probability distributions $P$ and $Q$ on the sample space $\\Omega = \\{1, 2, 3, 4\\}$. We must first determine the explicit forms of the probability mass functions $P(x)$ and $Q(x)$ using the given rules.\n\n**Step 1: Determine the probability distribution P.**\nAccording to rule 1, the distribution $P$ has non-zero probabilities only for $x=1$ and $x=4$. Let $P(1) = p_1$ and $P(4) = p_4$. We are told these probabilities are equal, so $p_1 = p_4$. The probabilities for other outcomes are zero: $P(2) = 0$ and $P(3) = 0$.\nFor any valid probability distribution, the sum of all probabilities over the sample space must equal 1.\n$$ \\sum_{x \\in \\Omega} P(x) = P(1) + P(2) + P(3) + P(4) = 1 $$\nSubstituting the known values and the equality condition:\n$$ p_1 + 0 + 0 + p_1 = 1 $$\n$$ 2p_1 = 1 $$\n$$ p_1 = \\frac{1}{2} $$\nTherefore, $p_1 = P(1) = 1/2$ and $p_4 = P(4) = 1/2$. The full distribution $P$ is:\n$P = (P(1), P(2), P(3), P(4)) = (\\frac{1}{2}, 0, 0, \\frac{1}{2})$.\n\n**Step 2: Determine the probability distribution Q.**\nAccording to rule 2, the distribution $Q$ has non-zero probabilities only for $x=2$ and $x=3$. Let $Q(2) = q_2$ and $Q(3) = q_3$. We are told these probabilities are equal, so $q_2 = q_3$. The probabilities for other outcomes are zero: $Q(1) = 0$ and $Q(4) = 0$.\nThe sum of probabilities must be 1:\n$$ \\sum_{x \\in \\Omega} Q(x) = Q(1) + Q(2) + Q(3) + Q(4) = 1 $$\nSubstituting the known values and the equality condition:\n$$ 0 + q_2 + q_2 + 0 = 1 $$\n$$ 2q_2 = 1 $$\n$$ q_2 = \\frac{1}{2} $$\nTherefore, $q_2 = Q(2) = 1/2$ and $q_3 = Q(3) = 1/2$. The full distribution $Q$ is:\n$Q = (Q(1), Q(2), Q(3), Q(4)) = (0, \\frac{1}{2}, \\frac{1}{2}, 0)$.\n\n**Step 3: Verify the expectation constraint.**\nRule 3 states that the expectations must be equal, $\\mathbb{E}_P[X] = \\mathbb{E}_Q[X]$. Let's verify this as a consistency check.\nThe expectation for $P$ is:\n$$ \\mathbb{E}_P[X] = \\sum_{x \\in \\Omega} x P(x) = 1 \\cdot P(1) + 2 \\cdot P(2) + 3 \\cdot P(3) + 4 \\cdot P(4) $$\n$$ \\mathbb{E}_P[X] = 1 \\cdot \\left(\\frac{1}{2}\\right) + 2 \\cdot (0) + 3 \\cdot (0) + 4 \\cdot \\left(\\frac{1}{2}\\right) = \\frac{1}{2} + 2 = \\frac{5}{2} $$\nThe expectation for $Q$ is:\n$$ \\mathbb{E}_Q[X] = \\sum_{x \\in \\Omega} x Q(x) = 1 \\cdot Q(1) + 2 \\cdot Q(2) + 3 \\cdot Q(3) + 4 \\cdot Q(4) $$\n$$ \\mathbb{E}_Q[X] = 1 \\cdot (0) + 2 \\cdot \\left(\\frac{1}{2}\\right) + 3 \\cdot \\left(\\frac{1}{2}\\right) + 4 \\cdot (0) = 1 + \\frac{3}{2} = \\frac{5}{2} $$\nSince $\\mathbb{E}_P[X] = \\mathbb{E}_Q[X] = 5/2$, the distributions we found are consistent with all the rules given in the problem statement.\n\n**Step 4: Calculate the total variation distance.**\nThe total variation distance is defined as $d_{TV}(P, Q) = \\frac{1}{2} \\sum_{x \\in \\Omega} |P(x) - Q(x)|$. We compute the sum of the absolute differences of the probabilities term by term.\n$$ \\sum_{x \\in \\Omega} |P(x) - Q(x)| = |P(1) - Q(1)| + |P(2) - Q(2)| + |P(3) - Q(3)| + |P(4) - Q(4)| $$\n$$ = \\left|\\frac{1}{2} - 0\\right| + \\left|0 - \\frac{1}{2}\\right| + \\left|0 - \\frac{1}{2}\\right| + \\left|\\frac{1}{2} - 0\\right| $$\n$$ = \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} = 2 $$\nNow, we multiply this sum by $1/2$ to get the total variation distance:\n$$ d_{TV}(P, Q) = \\frac{1}{2} \\times 2 = 1 $$\nThe total variation distance between the two distributions is 1.", "answer": "$$\\boxed{1}$$", "id": "1664827"}]}