{"hands_on_practices": [{"introduction": "We begin our hands-on practice with a scenario that is both intuitive and deeply illustrative of large deviation principles. This exercise explores the probability of a 'zero-count' event—a situation where a specific symbol from a source alphabet fails to appear in a long sequence [@problem_id:1655867]. The beauty of this problem lies in its dual nature: the answer can be found using basic probability theory, yet it also serves as a perfect validation for the more powerful machinery of Sanov's theorem, showcasing how the rate function $C = -\\ln(1-p_K)$ elegantly emerges from both perspectives.", "problem": "Consider a source that generates a sequence of symbols $X_1, X_2, \\ldots, X_n$ which are independent and identically distributed (i.i.d.). The symbols are drawn from a finite alphabet $\\mathcal{X} = \\{x_1, x_2, \\ldots, x_K\\}$ where $K \\ge 2$. The probability of generating symbol $x_k$ is given by $p_k = \\text{Pr}(X_i = x_k)$ for any $i$. We are given that $p_k > 0$ for all $k \\in \\{1, 2, \\ldots, K\\}$.\n\nFor a very long sequence of length $n$, the empirical frequency of the symbol $x_k$ is the fraction of times it appears in the sequence. The probability that the empirical frequency of the specific symbol $x_K$ is exactly zero is a rare event. For large $n$, this probability can be well-approximated by the form:\n$$ \\text{Pr}(\\text{empirical frequency of } x_K \\text{ is } 0) \\approx \\exp(-nC) $$\nwhere $C$ is a positive constant that depends on the source probabilities.\n\nDetermine the constant $C$ as an analytic expression in terms of the probability $p_K$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. over $\\mathcal{X}$ with $\\text{Pr}(X_{i}=x_{K})=p_{K}$ and $0<p_{K}<1$. The event that the empirical frequency of $x_{K}$ is zero is exactly the event that $x_{K}$ never appears in the $n$ independent draws. By independence,\n$$\n\\text{Pr}(\\text{no }x_{K}\\text{ in }n\\text{ draws})=(1-p_{K})^{n}.\n$$\nRewrite this probability in exponential form using the logarithm:\n$$\n(1-p_{K})^{n}=\\exp\\!\\big(n\\ln(1-p_{K})\\big)=\\exp\\!\\big(-n\\big[-\\ln(1-p_{K})\\big]\\big).\n$$\nComparing with the asymptotic form $\\exp(-nC)$, we identify\n$$\nC=-\\ln(1-p_{K}).\n$$\nSince $0<p_{K}<1$, we have $1-p_{K}\\in(0,1)$, hence $-\\ln(1-p_{K})>0$, so $C$ is positive as required.\n\nFor completeness, the same constant arises from large deviations (Sanov’s theorem): minimizing the relative entropy $D(q\\|p)$ over distributions $q$ with $q_{K}=0$ gives the optimizer $q_{i}=p_{i}/(1-p_{K})$ for $i<K$, and\n$$\nD(q\\|p)=\\sum_{i<K}q_{i}\\ln\\!\\left(\\frac{q_{i}}{p_{i}}\\right)=\\sum_{i<K}q_{i}\\ln\\!\\left(\\frac{1}{1-p_{K}}\\right)=\\ln\\!\\left(\\frac{1}{1-p_{K}}\\right)=-\\ln(1-p_{K}),\n$$\nmatching the constant above.", "answer": "$$\\boxed{-\\ln(1-p_{K})}$$", "id": "1655867"}, {"introduction": "Having established a foundational understanding, we now tackle a more general application of Sanov's theorem where the rare event is defined by a structural property of the observed frequencies. Here, we analyze the likelihood of observing an empirical distribution that adheres to a specific linear constraint [@problem_id:1655912]. This problem introduces the concept of an information projection (I-projection), requiring you to find the most likely 'unlikely' distribution by minimizing the Kullback-Leibler divergence $D_{KL}(P || Q)$ over the constrained set, thereby practicing the central computational technique of large deviation theory.", "problem": "A certain random process generates a sequence of symbols from the alphabet $\\mathcal{X} = \\{A, B, C\\}$. The symbols are generated independently and identically, following the probability distribution $Q$, where $Q(A) = 1/2$, $Q(B) = 1/4$, and $Q(C) = 1/4$.\n\nWe observe a long sequence of $n$ symbols and compute their empirical frequency distribution, $P = (p_A, p_B, p_C)$, where $p_x$ is the fraction of times symbol $x$ appeared in the sequence. We are interested in the probability of observing an empirical distribution that belongs to a specific set $\\mathcal{E}$. This set $\\mathcal{E}$ contains all possible probability distributions on $\\mathcal{X}$ for which the probability of 'A' is the arithmetic mean of the probabilities of 'B' and 'C'.\n\nAccording to Sanov's theorem on large deviations, for a large number of observations $n$, this probability behaves as $P(P \\in \\mathcal{E}) \\approx \\exp(-n I)$, where $I$ is the rate constant. This constant is the minimum possible value of the Kullback-Leibler (KL) divergence from the source distribution $Q$ to any distribution $P$ within the set $\\mathcal{E}$:\n$$I = \\inf_{P \\in \\mathcal{E}} D_{KL}(P || Q)$$\nThe KL divergence is given by the formula $D_{KL}(P || Q) = \\sum_{x \\in \\{A, B, C\\}} p_x \\ln\\left(\\frac{p_x}{q_x}\\right)$.\n\nDetermine the value of the rate constant $I$. Provide your answer as a single closed-form analytic expression, in units of nats.", "solution": "We are to compute the Sanov rate\n$$I=\\inf_{P\\in\\mathcal{E}}D_{KL}(P\\|Q),\\quad D_{KL}(P\\|Q)=\\sum_{x\\in\\{A,B,C\\}}p_{x}\\ln\\left(\\frac{p_{x}}{q_{x}}\\right),$$\nwhere $Q(A)=\\frac{1}{2}$, $Q(B)=\\frac{1}{4}$, $Q(C)=\\frac{1}{4}$, and $\\mathcal{E}=\\{P:\\,p_{A}=(p_{B}+p_{C})/2\\}$.\n\nLet $P=(a,b,c)$ with $a=p_{A}$, $b=p_{B}$, $c=p_{C}$. The constraints are\n$$a=\\frac{b+c}{2},\\qquad a+b+c=1,\\qquad a,b,c\\geq 0.$$\nSubstituting $a=\\frac{b+c}{2}$ into the normalization gives\n$$\\frac{b+c}{2}+b+c=1\\;\\;\\Longrightarrow\\;\\;\\frac{3}{2}(b+c)=1\\;\\;\\Longrightarrow\\;\\;b+c=\\frac{2}{3},\\quad a=\\frac{1}{3}.$$\nThus $\\mathcal{E}$ reduces to the line segment $\\{(a,b,c):a=\\frac{1}{3},\\,b+c=\\frac{2}{3},\\,b,c\\geq 0\\}$.\n\nWe minimize $D_{KL}(P\\|Q)$ over this segment. Since $q_{B}=q_{C}$ and the constraint is symmetric in $b$ and $c$, the minimizer satisfies $b=c$ by symmetry and strict convexity. To formalize, minimize\n$$f(b,c)=a\\ln\\left(\\frac{a}{q_{A}}\\right)+b\\ln\\left(\\frac{b}{q_{B}}\\right)+c\\ln\\left(\\frac{c}{q_{C}}\\right),$$\nsubject to $b+c=\\frac{2}{3}$ with $a=\\frac{1}{3}$. Using a Lagrange multiplier $\\lambda$ for $b+c=\\frac{2}{3}$, the stationarity conditions are\n$$\\frac{\\partial}{\\partial b}\\left[b\\ln\\left(\\frac{b}{q_{B}}\\right)\\right]=\\ln\\left(\\frac{b}{q_{B}}\\right)+1=\\lambda,\\qquad \\frac{\\partial}{\\partial c}\\left[c\\ln\\left(\\frac{c}{q_{C}}\\right)\\right]=\\ln\\left(\\frac{c}{q_{C}}\\right)+1=\\lambda.$$\nHence $\\ln\\left(\\frac{b}{q_{B}}\\right)=\\ln\\left(\\frac{c}{q_{C}}\\right)$, so $\\frac{b}{q_{B}}=\\frac{c}{q_{C}}$. Since $q_{B}=q_{C}=\\frac{1}{4}$, this yields $b=c$. Combined with $b+c=\\frac{2}{3}$, we get $b=c=\\frac{1}{3}$, and thus $a=\\frac{1}{3}$. This point is feasible (strictly positive) and, by strict convexity of $D_{KL}$, is the unique minimizer.\n\nTherefore the I-projection is $P^{\\star}=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$, and the rate constant is\n$$I=D_{KL}(P^{\\star}\\|Q)=\\frac{1}{3}\\ln\\left(\\frac{\\frac{1}{3}}{\\frac{1}{2}}\\right)+\\frac{1}{3}\\ln\\left(\\frac{\\frac{1}{3}}{\\frac{1}{4}}\\right)+\\frac{1}{3}\\ln\\left(\\frac{\\frac{1}{3}}{\\frac{1}{4}}\\right).$$\nCompute each term: $\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}$ and $\\frac{\\frac{1}{3}}{\\frac{1}{4}}=\\frac{4}{3}$. Hence\n$$I=\\frac{1}{3}\\ln\\left(\\frac{2}{3}\\right)+\\frac{2}{3}\\ln\\left(\\frac{4}{3}\\right)=\\frac{1}{3}\\ln\\left(\\frac{2}{3}\\cdot\\left(\\frac{4}{3}\\right)^{2}\\right)=\\frac{1}{3}\\ln\\left(\\frac{32}{27}\\right).$$\nEquivalently, $I=\\frac{5}{3}\\ln 2-\\ln 3$. Either is a single closed-form analytic expression in nats.", "answer": "$$\\boxed{\\frac{1}{3}\\ln\\left(\\frac{32}{27}\\right)}$$", "id": "1655912"}, {"introduction": "Our final practice problem introduces a powerful extension of Sanov's theorem: the contraction principle. Instead of defining the rare event directly on the empirical frequencies, we consider an event defined by a function of them—in this case, the empirical entropy $H(\\hat{p})$ [@problem_id:1655900]. This exercise will guide you through the essential process of translating a condition on a derived quantity back to a set of constraints on the underlying probability distribution. Solving this problem demonstrates mastery in handling more complex large deviation scenarios, a key skill for advanced applications in statistical physics and machine learning.", "problem": "Consider a source that generates a sequence of binary digits (bits) in an independent and identically distributed (i.i.d.) manner. The probability of generating a '1' is $p = 1/4$, and the probability of generating a '0' is $1-p = 3/4$. For a long sequence of $N$ bits, let $\\hat{p}$ denote the empirical frequency of '1's (i.e., the number of '1's in the sequence divided by $N$). The empirical entropy of the sequence is given by $H(\\hat{p}) = -\\hat{p} \\log_2(\\hat{p}) - (1-\\hat{p})\\log_2(1-\\hat{p})$.\n\nWe are interested in the probability of a specific large deviation event: the event that the empirical entropy $H(\\hat{p})$ of the sequence is greater than or equal to the entropy of a different binary source that produces '1's with a probability of $1/3$.\n\nAccording to the theory of large deviations, for very large $N$, the probability of this event, $P_N$, can be approximated as $P_N \\approx \\exp(-N \\cdot I)$, where $I$ is a positive constant known as the rate function or large deviation exponent.\n\nDetermine the exact value of this exponent $I$. Your answer should be a closed-form analytic expression involving natural logarithms ($\\ln$).", "solution": "The source is i.i.d. Bernoulli with true parameter $p=1/4$. Let $q$ denote a generic empirical frequency. The large deviations principle for empirical distributions (Sanov's theorem) states that, for a set $A$ of empirical measures,\n$$\n\\Pr(\\hat{p} \\in A) \\approx \\exp\\!\\left(-N \\inf_{q \\in A} D(q\\|p)\\right),\n$$\nwhere for Bernoulli laws\n$$\nD(q\\|p) = q \\ln\\!\\left(\\frac{q}{p}\\right) + (1-q)\\ln\\!\\left(\\frac{1-q}{1-p}\\right).\n$$\n\nThe event of interest is $H(\\hat{p}) \\geq H(1/3)$ where $H$ is defined with base-$2$ logarithms. Since changing the logarithm base multiplies entropy by the positive constant $1/\\ln 2$, the inequality $H(\\hat{p}) \\geq H(1/3)$ is equivalent to $H_{e}(\\hat{p}) \\geq H_{e}(1/3)$, where $H_{e}(q) = -q \\ln q - (1-q)\\ln(1-q)$. Because $H_{e}(q)$ is symmetric around $q=1/2$ and strictly increasing on $[0,1/2]$, the constraint $H_{e}(q) \\geq H_{e}(1/3)$ is equivalent to $q \\in [1/3,\\,2/3]$.\n\nHence, by Sanov's theorem and the contraction principle, the large deviation exponent is\n$$\nI = \\inf_{q \\in [1/3,\\,2/3]} D(q\\|1/4).\n$$\nThe function $D(q\\|1/4)$ is convex in $q$ with a unique global minimum at $q=1/4$, which lies outside $[1/3,\\,2/3]$. Therefore, the infimum over the interval is attained at the boundary point closest to $1/4$, namely $q^{\\star}=1/3$. Thus\n$$\nI = D\\!\\left(\\frac{1}{3}\\,\\Big\\|\\,\\frac{1}{4}\\right)\n= \\frac{1}{3}\\ln\\!\\left(\\frac{1/3}{1/4}\\right) + \\frac{2}{3}\\ln\\!\\left(\\frac{1-1/3}{1-1/4}\\right)\n= \\frac{1}{3}\\ln\\!\\left(\\frac{4}{3}\\right) + \\frac{2}{3}\\ln\\!\\left(\\frac{8}{9}\\right).\n$$\nCombining terms,\n$$\nI = \\frac{1}{3}\\big(2\\ln 2 - \\ln 3\\big) + \\frac{2}{3}\\big(3\\ln 2 - 2\\ln 3\\big)\n= \\frac{8}{3}\\ln 2 - \\frac{5}{3}\\ln 3\n= \\frac{1}{3}\\ln\\!\\left(\\frac{256}{243}\\right).\n$$\nThis is the exact large deviation exponent.", "answer": "$$\\boxed{\\frac{1}{3}\\ln\\!\\left(\\frac{256}{243}\\right)}$$", "id": "1655900"}]}