{"hands_on_practices": [{"introduction": "To fully leverage the power of exponential families, the first essential skill is to recognize and represent them. Many common probability distributions, such as the Gaussian, Poisson, and Bernoulli, can be re-expressed in a standard canonical form. This exercise [@problem_id:1623491] provides practical training in this \"translation\" process, asking you to take the familiar geometric distribution and algebraically manipulate it into the exponential family structure, identifying its fundamental components: the base measure $h(x)$, the sufficient statistic $T(x)$, and the log-partition function $A(\\eta)$. Mastering this skill is crucial for applying the unified theoretical tools of exponential families to specific, real-world models.", "problem": "In designing a reliable data transmission protocol, an engineer models the process of sending a data packet. Each transmission attempt is an independent Bernoulli trial with a probability of success $p$, where $0 < p < 1$. Let the random variable $X$ denote the number of failed attempts before the first successful transmission occurs. The probability mass function for $X$ is given by the geometric distribution:\n$$P(X=x) = (1-p)^x p, \\quad \\text{for } x \\in \\{0, 1, 2, \\dots\\}$$\nAny probability distribution that can be written in the form\n$$f(x|\\theta) = h(x) \\exp(\\eta(\\theta) T(x) - A(\\eta(\\theta)))$$\nis a member of the exponential family. For the canonical form, we use the natural parameter $\\eta = \\eta(\\theta)$ and write the distribution as:\n$$f(x|\\eta) = h(x) \\exp(\\eta T(x) - A(\\eta))$$\nwhere $h(x)$ is the base measure, $T(x)$ is the sufficient statistic, $\\eta$ is the natural parameter, and $A(\\eta)$ is the log-partition function (or cumulant-generating function).\n\nYour task is to represent the given geometric distribution in this canonical exponential family form. Which of the following correctly identifies the triplet of components $(h(x), T(x), A(\\eta))$, where $\\eta = \\ln(1-p)$?\n\nA. $(h(x)=1, \\quad T(x)=x, \\quad A(\\eta)=-\\ln(1-e^\\eta))$\n\nB. $(h(x)=1, \\quad T(x)=x, \\quad A(\\eta)=\\eta - \\ln(1-e^\\eta))$\n\nC. $(h(x)=p, \\quad T(x)=x, \\quad A(\\eta)=0)$\n\nD. $(h(x)=1, \\quad T(x)=x, \\quad A(\\eta)=\\ln(1-e^\\eta))$\n\nE. $(h(x)=1, \\quad T(x)=1, \\quad A(\\eta)=-\\ln(1-e^\\eta))$", "solution": "We start with the geometric pmf for the number of failures before the first success:\n$$\nP(X=x) = (1-p)^{x} p,\\quad x \\in \\{0,1,2,\\dots\\}.\n$$\nLet $q = 1-p$, so $0<q<1$. The problem specifies the natural parameter $\\eta = \\ln(1-p)$, hence $\\eta = \\ln q$ and $q = \\exp(\\eta)$ with $\\eta < 0$.\n\nExpress the pmf in terms of $\\eta$:\n$$\nP(X=x) = (1-q) q^{x} = \\bigl(1-\\exp(\\eta)\\bigr)\\exp(\\eta x).\n$$\nTo write this in canonical exponential family form,\n$$\nf(x \\mid \\eta) = h(x)\\exp\\bigl(\\eta T(x) - A(\\eta)\\bigr),\n$$\nwe can choose $h(x)=1$ and $T(x)=x$. Then normalization determines $A(\\eta)$. Enforce\n$$\n\\sum_{x=0}^{\\infty} f(x \\mid \\eta) = \\sum_{x=0}^{\\infty} \\exp\\bigl(\\eta x - A(\\eta)\\bigr) = \\exp\\bigl(-A(\\eta)\\bigr)\\sum_{x=0}^{\\infty} \\exp(\\eta x) = 1.\n$$\nFor $\\eta<0$, the geometric series sums to\n$$\n\\sum_{x=0}^{\\infty} \\exp(\\eta x) = \\frac{1}{1-\\exp(\\eta)}.\n$$\nThus,\n$$\n\\exp\\bigl(-A(\\eta)\\bigr)\\cdot \\frac{1}{1-\\exp(\\eta)} = 1\n\\quad\\Longrightarrow\\quad\n\\exp\\bigl(-A(\\eta)\\bigr) = 1 - \\exp(\\eta)\n\\quad\\Longrightarrow\\quad\n-A(\\eta) = \\ln\\bigl(1-\\exp(\\eta)\\bigr),\n$$\nso\n$$\nA(\\eta) = -\\ln\\bigl(1-\\exp(\\eta)\\bigr).\n$$\nTherefore, the canonical form is obtained with\n$$\nh(x)=1,\\quad T(x)=x,\\quad A(\\eta)=-\\ln\\bigl(1-\\exp(\\eta)\\bigr),\n$$\nwhich corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1623491"}, {"introduction": "Beyond just re-expressing known distributions, the exponential family framework has deep connections to fundamental principles of inference. The principle of maximum entropy (MaxEnt) provides a powerful, unbiased method for assigning probabilities based on limited data. This principle states that given certain constraints (like a known average value), the most objective probability distribution is the one that maximizes Shannon entropy. This exercise [@problem_id:1623482] guides you through a classic application of MaxEnt, demonstrating how it naturally gives rise to a distribution from the exponential family (specifically, a Boltzmann distribution) to model the energy levels of a physical system.", "problem": "A physical system is known to possess three discrete energy levels, $E_1=\\epsilon_0, E_2=2\\epsilon_0, E_3=3\\epsilon_0$, where $\\epsilon_0$ is a positive energy constant. Through a series of measurements on a large ensemble of identical systems prepared under the same conditions, the average energy is determined to be $\\langle E \\rangle = \\frac{7}{3}\\epsilon_0$. Based on the principle of maximum entropy, the system's state distribution $\\{p_1, p_2, p_3\\}$ is assumed to be the one with the highest possible entropy that is consistent with this observed average energy. Calculate the probability $p_2$ of finding the system in the second energy level, $E_2$. Express your answer as a single closed-form analytic expression.", "solution": "The problem asks for the probability distribution $\\{p_1, p_2, p_3\\}$ that maximizes the Shannon entropy, subject to certain constraints. The Shannon entropy is given by $H(p_1, p_2, p_3) = - \\sum_{i=1}^{3} p_i \\ln(p_i)$.\n\nThe constraints on the probabilities are:\n1. Normalization: The sum of all probabilities must be 1.\n$$g_1(p_1, p_2, p_3) = \\sum_{i=1}^{3} p_i - 1 = p_1 + p_2 + p_3 - 1 = 0$$\n2. Average energy: The expectation value of the energy must match the observed value.\n$$g_2(p_1, p_2, p_3) = \\sum_{i=1}^{3} p_i E_i - \\langle E \\rangle = p_1 E_1 + p_2 E_2 + p_3 E_3 - \\langle E \\rangle = 0$$\n\nWe use the method of Lagrange multipliers to solve this constrained optimization problem. The Lagrangian $\\mathcal{L}$ is constructed as:\n$$\\mathcal{L}(p_1, p_2, p_3, \\lambda_1, \\lambda_2) = H - \\lambda_1 g_1 - \\lambda_2 g_2$$\n$$\\mathcal{L} = -\\sum_{i=1}^{3} p_i \\ln(p_i) - \\lambda_1 (p_1+p_2+p_3-1) - \\lambda_2 (p_1 E_1 + p_2 E_2 + p_3 E_3 - \\langle E \\rangle)$$\nTo find the maximum, we take the partial derivative of $\\mathcal{L}$ with respect to each $p_i$ and set it to zero.\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} = -(\\ln(p_i) + 1) - \\lambda_1 - \\lambda_2 E_i = 0$$\nSolving for $p_i$:\n$$\\ln(p_i) = -1 - \\lambda_1 - \\lambda_2 E_i$$\n$$p_i = \\exp(-1 - \\lambda_1 - \\lambda_2 E_i) = \\exp(-1-\\lambda_1) \\exp(-\\lambda_2 E_i)$$\nLet's define a new constant $\\beta = \\lambda_2$. The term $\\exp(-1-\\lambda_1)$ is a constant that can be determined by the normalization constraint. Let's call it $1/Z$. The probability distribution then takes the form of a Boltzmann distribution, which is a member of the exponential family:\n$$p_i = \\frac{1}{Z} \\exp(-\\beta E_i)$$\nThe normalization constant $Z$, also known as the partition function, is found by summing the probabilities:\n$$Z = \\sum_{i=1}^{3} \\exp(-\\beta E_i)$$\n\nNow, we use the given values: $E_1=\\epsilon_0, E_2=2\\epsilon_0, E_3=3\\epsilon_0$, and $\\langle E \\rangle = \\frac{7}{3}\\epsilon_0$. The average energy constraint is:\n$$\\langle E \\rangle = \\sum_{i=1}^{3} p_i E_i = \\frac{1}{Z} \\sum_{i=1}^{3} E_i \\exp(-\\beta E_i) = \\frac{7}{3}\\epsilon_0$$\nSubstituting the expressions for $E_i$:\n$$\\frac{\\epsilon_0 \\exp(-\\beta \\epsilon_0) + 2\\epsilon_0 \\exp(-2\\beta \\epsilon_0) + 3\\epsilon_0 \\exp(-3\\beta \\epsilon_0)}{\\exp(-\\beta \\epsilon_0) + \\exp(-2\\beta \\epsilon_0) + \\exp(-3\\beta \\epsilon_0)} = \\frac{7}{3}\\epsilon_0$$\nThe constant $\\epsilon_0$ cancels from both sides. Let's introduce a variable $x = \\exp(-\\beta \\epsilon_0)$. The equation becomes:\n$$\\frac{x + 2x^2 + 3x^3}{x + x^2 + x^3} = \\frac{7}{3}$$\nSince $x$ cannot be zero (as probabilities would be undefined), we can factor out $x$ from the numerator and denominator:\n$$\\frac{1 + 2x + 3x^2}{1 + x + x^2} = \\frac{7}{3}$$\nNow we solve for $x$:\n$$3(1 + 2x + 3x^2) = 7(1 + x + x^2)$$\n$$3 + 6x + 9x^2 = 7 + 7x + 7x^2$$\n$$2x^2 - x - 4 = 0$$\nThis is a quadratic equation for $x$. Using the quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$x = \\frac{1 \\pm \\sqrt{(-1)^2 - 4(2)(-4)}}{2(2)} = \\frac{1 \\pm \\sqrt{1 + 32}}{4} = \\frac{1 \\pm \\sqrt{33}}{4}$$\nSince $x = \\exp(-\\beta \\epsilon_0)$ and $\\epsilon_0>0$ is a physical energy, $\\beta$ must be real, which means $x$ must be a positive real number. Therefore, we take the positive root:\n$$x = \\frac{1 + \\sqrt{33}}{4}$$\nThe problem asks for the probability $p_2$. We express $p_2$ in terms of $x$:\n$$p_2 = \\frac{\\exp(-2\\beta\\epsilon_0)}{Z} = \\frac{\\exp(-2\\beta\\epsilon_0)}{\\exp(-\\beta\\epsilon_0) + \\exp(-2\\beta\\epsilon_0) + \\exp(-3\\beta\\epsilon_0)}$$\n$$p_2 = \\frac{x^2}{x + x^2 + x^3} = \\frac{x}{1 + x + x^2}$$\nFrom the quadratic equation $2x^2 - x - 4 = 0$, we have $x^2 = \\frac{x+4}{2}$. Let's substitute this into the denominator of the expression for $p_2$:\n$$1 + x + x^2 = 1 + x + \\frac{x+4}{2} = \\frac{2 + 2x + x + 4}{2} = \\frac{3x+6}{2}$$\nNow substitute this back into the expression for $p_2$:\n$$p_2 = \\frac{x}{\\frac{3x+6}{2}} = \\frac{2x}{3x+6} = \\frac{2x}{3(x+2)}$$\nFinally, we substitute the value of $x = \\frac{1+\\sqrt{33}}{4}$:\n$$p_2 = \\frac{2(\\frac{1+\\sqrt{33}}{4})}{3(\\frac{1+\\sqrt{33}}{4} + 2)} = \\frac{\\frac{1+\\sqrt{33}}{2}}{3(\\frac{1+\\sqrt{33}+8}{4})} = \\frac{1+\\sqrt{33}}{2} \\cdot \\frac{4}{3(9+\\sqrt{33})} = \\frac{2(1+\\sqrt{33})}{3(9+\\sqrt{33})}$$\nTo simplify, we rationalize the denominator by multiplying the numerator and denominator by the conjugate of $(9+\\sqrt{33})$, which is $(9-\\sqrt{33})$:\n$$p_2 = \\frac{2(1+\\sqrt{33})(9-\\sqrt{33})}{3(9+\\sqrt{33})(9-\\sqrt{33})} = \\frac{2(9 - \\sqrt{33} + 9\\sqrt{33} - 33)}{3(9^2 - (\\sqrt{33})^2)} = \\frac{2(8\\sqrt{33} - 24)}{3(81 - 33)}$$\n$$p_2 = \\frac{2 \\cdot 8(\\sqrt{33} - 3)}{3(48)} = \\frac{16(\\sqrt{33} - 3)}{144}$$\nSimplifying the fraction $\\frac{16}{144} = \\frac{1}{9}$:\n$$p_2 = \\frac{\\sqrt{33} - 3}{9}$$", "answer": "$$\\boxed{\\frac{\\sqrt{33}-3}{9}}$$", "id": "1623482"}, {"introduction": "The elegance of the exponential family representation lies not just in its unifying structure, but in the powerful analytical shortcuts it offers. Once a distribution is expressed in canonical form, its properties, such as moments, can often be calculated with remarkable ease. This problem [@problem_id:1623484] explores this utility by asking you to calculate an expected value, $E[X]$, not through the traditional sum $\\sum x \\cdot p(x)$, but by using a fundamental property of the log-partition function $A(\\eta)$. You will see how the expected value of a sufficient statistic can be found simply by taking the derivative of $A(\\eta)$, showcasing the profound computational advantages of this mathematical framework.", "problem": "Consider a discrete random variable $X$ whose domain is the set of integers $\\{0, 1, 2\\}$. The probability mass function (PMF) of $X$ is described by a two-parameter exponential family of the form:\n$$p(x | \\eta_1, \\eta_2) = \\exp(\\eta_1 T_1(x) + \\eta_2 T_2(x) - A(\\eta_1, \\eta_2))$$\nwhere $\\eta_1$ and $\\eta_2$ are the natural parameters, and $A(\\eta_1, \\eta_2)$ is the log-partition function that normalizes the distribution. The base measure is uniform, i.e., $h(x) = 1$ for all $x$ in the domain.\n\nThe sufficient statistics for this family are defined as:\n$T_1(x) = x$\n$T_2(x) = |x-1|$\n\nFor a specific instance of this distribution, the natural parameters are given as $\\eta_1 = 1.0$ and $\\eta_2 = -1.0$. Calculate the expected value of the random variable, $E[X]$.\n\nRound your final answer to four significant figures.", "solution": "The exponential family is given by\n$$p(x \\mid \\eta_{1},\\eta_{2})=\\exp\\!\\left(\\eta_{1}T_{1}(x)+\\eta_{2}T_{2}(x)-A(\\eta_{1},\\eta_{2})\\right),$$\nwith base measure $h(x)=1$, sufficient statistics $T_{1}(x)=x$, $T_{2}(x)=|x-1|$, and log-partition function\n$$A(\\eta_{1},\\eta_{2})=\\ln\\!\\left(\\sum_{x\\in\\{0,1,2\\}}\\exp\\!\\left(\\eta_{1}x+\\eta_{2}|x-1|\\right)\\right).$$\nA standard property of exponential families is\n$$E[T_{1}(X)]=\\frac{\\partial A}{\\partial \\eta_{1}},$$\nso $E[X]=\\frac{\\partial A}{\\partial \\eta_{1}}$.\n\nFor $\\eta_{1}=1$ and $\\eta_{2}=-1$, compute the unnormalized weights\n$$w(x)=\\exp\\!\\left(\\eta_{1}x+\\eta_{2}|x-1|\\right).$$\nFor $x=0,1,2$:\n$$w(0)=\\exp(-1),\\quad w(1)=\\exp(1),\\quad w(2)=\\exp(1).$$\nThus the partition function is\n$$Z=\\sum_{x}w(x)=\\exp(-1)+2\\exp(1),\\quad A=\\ln Z.$$\nThen\n$$E[X]=\\frac{\\partial A}{\\partial \\eta_{1}}=\\frac{1}{Z}\\sum_{x}x\\,w(x)=\\frac{0\\cdot \\exp(-1)+1\\cdot \\exp(1)+2\\cdot \\exp(1)}{\\exp(-1)+2\\exp(1)}=\\frac{3\\exp(1)}{\\exp(-1)+2\\exp(1)}=\\frac{3\\exp(2)}{1+2\\exp(2)}.$$\nNumerically,\n$$E[X]\\approx \\frac{3\\exp(2)}{1+2\\exp(2)}\\approx 1.404931605,$$\nwhich rounded to four significant figures is $1.405$.", "answer": "$$\\boxed{1.405}$$", "id": "1623484"}]}