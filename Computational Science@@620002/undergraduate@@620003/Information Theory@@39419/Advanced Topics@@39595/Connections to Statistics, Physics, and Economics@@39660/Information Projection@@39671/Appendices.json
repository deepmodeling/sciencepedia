{"hands_on_practices": [{"introduction": "Information projection provides a powerful and principled way to update our knowledge in light of new evidence. This first exercise [@problem_id:1631738] presents a foundational scenario where we start with a prior probability model and must adjust it to satisfy a new, precise constraint. By finding the new distribution that is minimally different from our prior—as measured by the Kullback-Leibler divergence $D_{KL}(P || Q)$—we ensure our update is as conservative as possible. This core practice is fundamental in fields ranging from statistical physics to machine learning.", "problem": "An engineer is modeling a loaded six-sided die. The probability distribution for the outcomes $\\{1, 2, 3, 4, 5, 6\\}$, denoted by $Q$, is found to be non-uniform. The probabilities are given as follows: $Q(1) = 1/12$, $Q(2) = 1/6$, $Q(3) = 1/12$, $Q(4) = 1/3$, $Q(5) = 1/12$, and $Q(6) = 1/4$.\n\nThe engineer needs to find a new theoretical probability distribution, $P$, for the same die that is as \"close\" as possible to $Q$, while satisfying a new design constraint. The constraint is that the total probability of rolling an even number (an outcome in the set $\\{2, 4, 6\\}$) must be exactly $1/3$.\n\n\"Closeness\" between the two distributions $P$ and $Q$ is measured by the Kullback-Leibler (KL) divergence, defined as $D_{KL}(P || Q) = \\sum_{i=1}^6 P(i) \\ln \\left(\\frac{P(i)}{Q(i)}\\right)$, where $\\ln$ denotes the natural logarithm. Your task is to find the distribution $P$ that minimizes this divergence subject to the given constraint.\n\nDetermine the six probabilities of the new distribution, $P(1)$ through $P(6)$. Express your answer as a row matrix containing the six probabilities $[P(1), P(2), P(3), P(4), P(5), P(6)]$, with each probability given as an exact fraction.", "solution": "The problem is to find a probability distribution $P = (p_1, p_2, p_3, p_4, p_5, p_6)$ that minimizes the Kullback-Leibler (KL) divergence from a given distribution $Q = (q_1, q_2, q_3, q_4, q_5, q_6)$, subject to certain constraints.\n\nThe objective function to minimize is the KL divergence:\n$$ D_{KL}(P || Q) = \\sum_{i=1}^{6} p_i \\ln\\left(\\frac{p_i}{q_i}\\right) $$\nThe given distribution is $Q = (1/12, 2/12, 1/12, 4/12, 1/12, 3/12) = (1/12, 1/6, 1/12, 1/3, 1/12, 1/4)$.\n\nThere are two constraints on the distribution $P$:\n1.  The probabilities must sum to one: $\\sum_{i=1}^{6} p_i = 1$.\n2.  The sum of probabilities for even outcomes must be $1/3$: $p_2 + p_4 + p_6 = 1/3$.\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. We define the Lagrangian $\\mathcal{L}$:\n$$ \\mathcal{L}(p_1, \\dots, p_6, \\lambda_0, \\lambda_1) = \\sum_{i=1}^{6} p_i \\ln\\left(\\frac{p_i}{q_i}\\right) - \\lambda_0 \\left(\\sum_{i=1}^{6} p_i - 1\\right) - \\lambda_1 \\left(p_2 + p_4 + p_6 - \\frac{1}{3}\\right) $$\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $p_k$ to zero.\n$$ \\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left(p_k \\ln p_k - p_k \\ln q_k\\right) - \\lambda_0 - \\lambda_1 \\delta_{k \\in E} = 0 $$\nwhere $E = \\{2, 4, 6\\}$ is the set of even outcomes and $\\delta_{k \\in E}$ is the Kronecker delta, which is 1 if $k \\in E$ and 0 otherwise.\n\nThe derivative is:\n$$ (\\ln p_k + 1) - \\ln q_k - \\lambda_0 - \\lambda_1 \\delta_{k \\in E} = 0 $$\n$$ \\ln\\left(\\frac{p_k}{q_k}\\right) = \\lambda_0 - 1 + \\lambda_1 \\delta_{k \\in E} $$\nSolving for $p_k$:\n$$ p_k = q_k \\exp(\\lambda_0 - 1 + \\lambda_1 \\delta_{k \\in E}) $$\nWe can rewrite this in a more convenient form. Let $C_0 = \\exp(\\lambda_0 - 1)$ and $C_1 = \\exp(\\lambda_1)$. The form of the solution is then:\n- For odd outcomes ($k \\in \\{1, 3, 5\\}$): $p_k = q_k C_0 \\exp(0) = C_0 q_k$.\n- For even outcomes ($k \\in \\{2, 4, 6\\}$): $p_k = q_k C_0 \\exp(\\lambda_1) = C_0 C_1 q_k$.\n\nNow we use the constraints to find the constants $C_0$ and $C_1$.\nFirst, let's calculate the sum of probabilities for odd and even outcomes in the original distribution $Q$.\nSum for odd outcomes: $Q_{odd} = q_1 + q_3 + q_5 = \\frac{1}{12} + \\frac{1}{12} + \\frac{1}{12} = \\frac{3}{12} = \\frac{1}{4}$.\nSum for even outcomes: $Q_{even} = q_2 + q_4 + q_6 = \\frac{2}{12} + \\frac{4}{12} + \\frac{3}{12} = \\frac{9}{12} = \\frac{3}{4}$.\n\nNow we apply the constraints to $P$:\nConstraint 2: $p_2 + p_4 + p_6 = 1/3$.\n$$ \\sum_{k \\in E} p_k = \\sum_{k \\in E} C_0 C_1 q_k = C_0 C_1 \\sum_{k \\in E} q_k = C_0 C_1 Q_{even} = \\frac{1}{3} $$\nSubstituting the value of $Q_{even}$:\n$$ C_0 C_1 \\left(\\frac{3}{4}\\right) = \\frac{1}{3} \\implies C_0 C_1 = \\frac{4}{9} $$\n\nConstraint 1: $\\sum_{k=1}^{6} p_k = 1$.\n$$ \\sum_{k=1}^{6} p_k = \\sum_{k \\in \\{1,3,5\\}} p_k + \\sum_{k \\in \\{2,4,6\\}} p_k = 1 $$\nThe second sum is given by constraint 2 as $1/3$. The first sum is:\n$$ \\sum_{k \\in \\{1,3,5\\}} p_k = \\sum_{k \\in \\{1,3,5\\}} C_0 q_k = C_0 \\sum_{k \\in \\{1,3,5\\}} q_k = C_0 Q_{odd} $$\nSo the first constraint becomes:\n$$ C_0 Q_{odd} + \\frac{1}{3} = 1 $$\nSubstituting the value of $Q_{odd}$:\n$$ C_0 \\left(\\frac{1}{4}\\right) + \\frac{1}{3} = 1 \\implies \\frac{C_0}{4} = 1 - \\frac{1}{3} = \\frac{2}{3} \\implies C_0 = \\frac{8}{3} $$\nNow we can find $C_1$ using $C_0 C_1 = 4/9$:\n$$ \\left(\\frac{8}{3}\\right) C_1 = \\frac{4}{9} \\implies C_1 = \\frac{4}{9} \\times \\frac{3}{8} = \\frac{12}{72} = \\frac{1}{6} $$\n\nWith $C_0 = 8/3$ and $C_0 C_1 = 4/9$, we can find each probability $p_k$:\nFor odd $k \\in \\{1, 3, 5\\}$: $p_k = C_0 q_k = \\frac{8}{3} q_k$.\n$p_1 = \\frac{8}{3} \\times \\frac{1}{12} = \\frac{8}{36} = \\frac{2}{9}$.\n$p_3 = \\frac{8}{3} \\times \\frac{1}{12} = \\frac{8}{36} = \\frac{2}{9}$.\n$p_5 = \\frac{8}{3} \\times \\frac{1}{12} = \\frac{8}{36} = \\frac{2}{9}$.\n\nFor even $k \\in \\{2, 4, 6\\}$: $p_k = (C_0 C_1) q_k = \\frac{4}{9} q_k$.\n$p_2 = \\frac{4}{9} \\times q_2 = \\frac{4}{9} \\times \\frac{1}{6} = \\frac{4}{54} = \\frac{2}{27}$.\n$p_4 = \\frac{4}{9} \\times q_4 = \\frac{4}{9} \\times \\frac{1}{3} = \\frac{4}{27}$.\n$p_6 = \\frac{4}{9} \\times q_6 = \\frac{4}{9} \\times \\frac{1}{4} = \\frac{1}{9}$.\n\nLet's check our final distribution $P$:\n$p_1+p_2+p_3+p_4+p_5+p_6 = \\frac{2}{9} + \\frac{2}{27} + \\frac{2}{9} + \\frac{4}{27} + \\frac{2}{9} + \\frac{1}{9}$\n$= \\frac{6}{27} + \\frac{2}{27} + \\frac{6}{27} + \\frac{4}{27} + \\frac{6}{27} + \\frac{3}{27} = \\frac{6+2+6+4+6+3}{27} = \\frac{27}{27} = 1$. The sum is 1.\n$p_2+p_4+p_6 = \\frac{2}{27} + \\frac{4}{27} + \\frac{1}{9} = \\frac{2}{27} + \\frac{4}{27} + \\frac{3}{27} = \\frac{9}{27} = \\frac{1}{3}$. The constraint is satisfied.\n\nThe final distribution is $P = (2/9, 2/27, 2/9, 4/27, 2/9, 1/9)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{9} & \\frac{2}{27} & \\frac{2}{9} & \\frac{4}{27} & \\frac{2}{9} & \\frac{1}{9} \\end{pmatrix}}$$", "id": "1631738"}, {"introduction": "The power of information projection lies in its versatility to handle various forms of constraints. Building upon the previous exercise, this problem [@problem_id:1631708] demonstrates how to update a distribution when the new information pertains to a collective property—in this case, the cumulative probability over a subset of outcomes. This practice reinforces the core Lagrange multiplier technique while showing its applicability to more complex constraints often encountered in data analysis and systems modeling.", "problem": "A data acquisition system monitors a process that can occupy one of ten discrete energy states, labeled by the integers in the set $S = \\{1, 2, \\ldots, 10\\}$. The principle of maximum entropy initially leads us to model the system with a uniform probability distribution, $Q$, where $Q(x) = 1/10$ for each state $x \\in S$. An updated measurement provides a new piece of information: the cumulative probability of the system being in a state $x \\le 3$ is exactly 0.5. We need to find a new probability distribution, $P$, that incorporates this constraint while being as close as possible to our original uniform model $Q$.\n\nIn information theory, this \"closest\" distribution is the one that minimizes the Kullback-Leibler (KL) divergence (also known as relative entropy) from $Q$ to $P$. The KL divergence is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in S} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\nFind the probability distribution $P$ that minimizes this divergence subject to the given constraint. From this distribution, determine the probability $P(X=1)$. Round your final answer to four significant figures.", "solution": "We must minimize the Kullback-Leibler divergence\n$$D_{KL}(P||Q)=\\sum_{x\\in S} P(x)\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)$$\nsubject to the constraints $\\sum_{x\\in S}P(x)=1$ and $\\sum_{x\\leq 3}P(x)=0.5$, with $Q(x)=\\frac{1}{10}$ for all $x\\in S=\\{1,2,\\ldots,10\\}$.\n\nIntroduce Lagrange multipliers $\\alpha$ and $\\beta$ for the normalization and the cumulative constraint, respectively, and form the Lagrangian\n$$\\mathcal{L}=\\sum_{x\\in S} P(x)\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)+\\alpha\\left(\\sum_{x\\in S}P(x)-1\\right)+\\beta\\left(\\sum_{x\\leq 3}P(x)-0.5\\right).$$\nTaking the derivative with respect to $P(x)$ and setting it to zero yields two cases:\n- For $x\\leq 3$,\n$$\\frac{\\partial \\mathcal{L}}{\\partial P(x)}=\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)+1+\\alpha+\\beta=0 \\quad\\Rightarrow\\quad P(x)=Q(x)\\exp(-1-\\alpha-\\beta).$$\n- For $x>3$,\n$$\\frac{\\partial \\mathcal{L}}{\\partial P(x)}=\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)+1+\\alpha=0 \\quad\\Rightarrow\\quad P(x)=Q(x)\\exp(-1-\\alpha).$$\n\nThus $P$ is constant within each group. Let $a=\\exp(-1-\\alpha-\\beta)$ and $b=\\exp(-1-\\alpha)$. Then\n$$P(x)=\\frac{1}{10}a \\text{ for } x\\leq 3,\\qquad P(x)=\\frac{1}{10}b \\text{ for } x>3.$$\nImpose the constraints. From $\\sum_{x\\leq 3}P(x)=0.5$,\n$$3\\cdot \\frac{1}{10}a=0.5 \\quad\\Rightarrow\\quad a=\\frac{5}{3}.$$\nFrom normalization $\\sum_{x\\in S}P(x)=1$,\n$$3\\cdot \\frac{1}{10}a+7\\cdot \\frac{1}{10}b=1 \\quad\\Rightarrow\\quad 3a+7b=10 \\quad\\Rightarrow\\quad 3\\cdot \\frac{5}{3}+7b=10 \\quad\\Rightarrow\\quad b=\\frac{5}{7}.$$\nTherefore,\n$$P(x)=\\frac{1}{6} \\text{ for } x\\in\\{1,2,3\\},\\qquad P(x)=\\frac{1}{14} \\text{ for } x\\in\\{4,5,6,7,8,9,10\\}.$$\nHence\n$$P(X=1)=\\frac{1}{6}.$$\nRounded to four significant figures, this is $0.1667$.", "answer": "$$\\boxed{0.1667}$$", "id": "1631708"}, {"introduction": "What it means for two distributions to be \"close\" can be defined in more than one way. While our previous examples minimized $D_{KL}(P || Q)$ (an I-projection), this exercise [@problem_id:1631718] explores the alternative of minimizing the reverse divergence, $D_{KL}(Q || P)$. This procedure, known as finding the M-projection, finds the best approximation within a constrained family of distributions (here, symmetric ones) to a given distribution $Q$. Tackling this problem reveals the subtle yet important differences between these two measures of divergence and leads to a surprisingly elegant and intuitive result.", "problem": "An experimental physicist is studying a particular decay process. The experiment measures an integer-valued quantity, $k$, for each decay event. The physical bounds on this quantity are $0 \\le k \\le N$ for a known integer $N$. After recording a large number of events, the physicist has constructed an empirical probability distribution $Q(k)$ for the outcomes $k \\in \\{0, 1, \\dots, N\\}$.\n\nBased on a fundamental theory, the true underlying probability distribution, let's call it $P(k)$, is expected to be symmetric. A probability distribution $P$ on the set $\\{0, 1, \\dots, N\\}$ is defined as symmetric if it satisfies the condition $P(k) = P(N-k)$ for all integers $k$ from $0$ to $N$.\n\nThe physicist wants to find the symmetric distribution $P$ that serves as the best approximation to her empirical data $Q$. In information theory, a standard measure for the dissimilarity between a true distribution $P$ and an empirical distribution $Q$ is the Kullback-Leibler (KL) divergence, defined as:\n$$ D_{KL}(Q || P) = \\sum_{k=0}^{N} Q(k) \\ln\\left(\\frac{Q(k)}{P(k)}\\right) $$\nwhere $\\ln$ is the natural logarithm. It is assumed that we only sum over $k$ for which $Q(k) > 0$.\n\nYour task is to find the specific symmetric probability distribution $P$ that minimizes this KL divergence $D_{KL}(Q || P)$. Your final answer should be a closed-form analytic expression for $P(k)$ in terms of the probabilities from the empirical distribution $Q$ and the parameter $N$.", "solution": "We are given an empirical distribution $Q$ on $\\{0,1,\\dots,N\\}$ and seek a symmetric distribution $P$ (i.e., $P(k)=P(N-k)$ for all $k$) that minimizes the Kullback-Leibler divergence\n$$\nD_{KL}(Q\\|P)=\\sum_{k=0}^{N} Q(k)\\ln\\left(\\frac{Q(k)}{P(k)}\\right),\n$$\nwith the convention that the sum is effectively over $k$ with $Q(k)>0$. Since $Q$ is fixed, minimizing $D_{KL}(Q\\|P)$ is equivalent to maximizing\n$$\n\\sum_{k=0}^{N} Q(k)\\ln P(k)\n$$\nsubject to the constraints that $P(k)\\ge 0$, $\\sum_{k=0}^{N} P(k)=1$, and $P(k)=P(N-k)$ for all $k$.\n\nExploit the symmetry by parameterizing $P$ over symmetry orbits. For $0\\le k<\\frac{N}{2}$, define $s_{k}$ by $P(k)=P(N-k)=s_{k}$. If $N$ is even, also define $t=P\\!\\left(\\frac{N}{2}\\right)$. The normalization constraint becomes:\n- If $N$ is odd: $2\\sum_{k=0}^{(N-1)/2} s_{k}=1$.\n- If $N$ is even: $2\\sum_{k=0}^{N/2-1} s_{k}+t=1$.\n\nWrite the objective in terms of these variables. For a pair $(k,N-k)$ with $k<\\frac{N}{2}$, the contribution is\n$$\nQ(k)\\ln s_{k}+Q(N-k)\\ln s_{k}=\\bigl(Q(k)+Q(N-k)\\bigr)\\ln s_{k}.\n$$\nIf $N$ is even, the midpoint contributes $Q(N/2)\\ln t$. Define weights $w_{k}=Q(k)+Q(N-k)$ for $0\\le k<\\frac{N}{2}$, and if $N$ is even, $w_{\\mathrm{mid}}=Q(N/2)$.\n\nFor $N$ odd, the Lagrangian is\n$$\n\\mathcal{L}=\\sum_{k=0}^{(N-1)/2} w_{k}\\ln s_{k}+\\lambda\\left(2\\sum_{k=0}^{(N-1)/2} s_{k}-1\\right).\n$$\nSetting derivatives to zero gives, for each $k$,\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}}=\\frac{w_{k}}{s_{k}}+2\\lambda=0\n\\quad\\Rightarrow\\quad\ns_{k}=-\\frac{w_{k}}{2\\lambda}.\n$$\nImposing normalization,\n$$\n2\\sum_{k=0}^{(N-1)/2} s_{k}=-\\frac{2}{2\\lambda}\\sum_{k=0}^{(N-1)/2} w_{k}=-\\frac{1}{\\lambda}\\sum_{k=0}^{(N-1)/2} w_{k}=1.\n$$\nSince $\\sum_{k=0}^{(N-1)/2} w_{k}=\\sum_{k=0}^{N} Q(k)=1$, we obtain $\\lambda=-1$, hence\n$$\ns_{k}=\\frac{w_{k}}{2}=\\frac{Q(k)+Q(N-k)}{2}.\n$$\n\nFor $N$ even, the Lagrangian is\n$$\n\\mathcal{L}=\\sum_{k=0}^{N/2-1} w_{k}\\ln s_{k}+w_{\\mathrm{mid}}\\ln t+\\lambda\\left(2\\sum_{k=0}^{N/2-1} s_{k}+t-1\\right).\n$$\nStationarity yields\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}}=\\frac{w_{k}}{s_{k}}+2\\lambda=0\n\\quad\\Rightarrow\\quad\ns_{k}=-\\frac{w_{k}}{2\\lambda},\n\\qquad\n\\frac{\\partial \\mathcal{L}}{\\partial t}=\\frac{w_{\\mathrm{mid}}}{t}+\\lambda=0\n\\quad\\Rightarrow\\quad\nt=-\\frac{w_{\\mathrm{mid}}}{\\lambda}.\n$$\nNormalization gives\n$$\n2\\sum_{k=0}^{N/2-1} s_{k}+t=-\\frac{1}{\\lambda}\\left(\\sum_{k=0}^{N/2-1} w_{k}+w_{\\mathrm{mid}}\\right)=-\\frac{1}{\\lambda}\\sum_{k=0}^{N} Q(k)=-\\frac{1}{\\lambda}=1,\n$$\nhence again $\\lambda=-1$, so\n$$\ns_{k}=\\frac{w_{k}}{2}=\\frac{Q(k)+Q(N-k)}{2},\\qquad t=w_{\\mathrm{mid}}=Q(N/2).\n$$\n\nCombining both cases, we obtain a single closed-form expression valid for all $N$ and all $k$:\n$$\nP(k)=\\frac{Q(k)+Q(N-k)}{2},\\quad k=0,1,\\dots,N.\n$$\nThis distribution is symmetric by construction, normalized since\n$$\n\\sum_{k=0}^{N} P(k)=\\frac{1}{2}\\sum_{k=0}^{N}\\bigl(Q(k)+Q(N-k)\\bigr)=\\frac{1}{2}\\bigl(1+1\\bigr)=1,\n$$\nand it minimizes $D_{KL}(Q\\|P)$ under the symmetry constraint. If $Q(k)=Q(N-k)=0$ for some pair, then $P(k)=P(N-k)=0$ follows, which is consistent with maximizing the objective while satisfying normalization.", "answer": "$$\\boxed{\\frac{Q(k)+Q(N-k)}{2}}$$", "id": "1631718"}]}