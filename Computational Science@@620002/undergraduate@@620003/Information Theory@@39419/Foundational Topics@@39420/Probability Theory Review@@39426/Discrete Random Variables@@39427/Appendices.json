{"hands_on_practices": [{"introduction": "A fundamental concept when working with random variables is understanding how they behave when transformed by a function. This practice explores this by modeling a simple hash function, a common tool in computer science. By analyzing the output of the transformation $Z = X \\pmod 3$ where $X$ is the outcome of a fair die roll, you'll gain hands-on experience in deriving the probability mass function (PMF) of a new variable from a known one [@problem_id:1618711]. This exercise is a perfect first step into the practical manipulation of discrete random variables.", "problem": "In a simplified model of a computer's hash table, integer keys are mapped to a small set of storage bins. The hash function used is $h(k) = k \\pmod 3$. The keys are generated from a random process, modeled as the outcome of a roll of a fair 8-sided die, whose faces are labeled with the integers $\\{1, 2, 3, 4, 5, 6, 7, 8\\}$.\n\nLet the random variable $X$ represent the outcome of a single roll of this die. The random variable $Z$ represents the bin number assigned by the hash function, so $Z = X \\pmod 3$.\n\nDetermine the Probability Mass Function (PMF) of the random variable $Z$. Specifically, calculate the probabilities $p_0, p_1, p_2$, where $p_i = P(Z=i)$ for $i \\in \\{0, 1, 2\\}$. Your final answer should be a row matrix containing the values of $p_0, p_1, p_2$ in that order, expressed as exact fractions.", "solution": "The die is fair, so $X$ is uniform on $\\{1,2,3,4,5,6,7,8\\}$ with $P(X=x)=\\frac{1}{8}$ for each $x$. The hash bin variable is $Z=X \\bmod 3$, taking values in $\\{0,1,2\\}$. For each $i \\in \\{0,1,2\\}$, the probability is the sum over the preimage of $i$ under the modulo map:\n$$\np_{i}=P(Z=i)=\\sum_{x \\in S_{i}} P(X=x), \\quad S_{i}=\\{x \\in \\{1,2,\\ldots,8\\}: x \\equiv i \\pmod{3}\\}.\n$$\nCompute each preimage:\n- For $i=0$: $S_{0}=\\{3,6\\}$, so $|S_{0}|=2$ and $p_{0}=2 \\cdot \\frac{1}{8}=\\frac{1}{4}$.\n- For $i=1$: $S_{1}=\\{1,4,7\\}$, so $|S_{1}|=3$ and $p_{1}=3 \\cdot \\frac{1}{8}=\\frac{3}{8}$.\n- For $i=2$: $S_{2}=\\{2,5,8\\}$, so $|S_{2}|=3$ and $p_{2}=3 \\cdot \\frac{1}{8}=\\frac{3}{8}$.\nThus the PMF of $Z$ in the order $(p_{0},p_{1},p_{2})$ is as follows.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{4} & \\frac{3}{8} & \\frac{3}{8}\\end{pmatrix}}$$", "id": "1618711"}, {"introduction": "In information theory, we often analyze the efficiency of data compression schemes by calculating the average length of codewords, or the expected value $E[L]$. However, to understand the consistency of a code's performance, we must also consider its variability. This exercise [@problem_id:1618709] guides you through calculating the variance of the codeword length, $\\text{Var}(L)$, offering a more complete statistical description and deeper insight into the behavior of a variable-length prefix code.", "problem": "A simplified model for a remote environmental sensor describes its data output as a source emitting symbols from the alphabet $\\mathcal{S} = \\{s_1, s_2, s_3, s_4\\}$. The long-term observed probabilities for emitting each symbol are given by the Probability Mass Function (PMF) $p(s)$:\n$p(s_1) = \\frac{1}{2}$, $p(s_2) = \\frac{1}{4}$, $p(s_3) = \\frac{1}{8}$, and $p(s_4) = \\frac{1}{8}$.\n\nTo compress data for transmission, the sensor employs a binary prefix code, where each symbol $s_i$ is mapped to a unique binary codeword $c_i$. The specific codebook is as follows:\n- $c_1$ (for $s_1$): `0`\n- $c_2$ (for $s_2$): `10`\n- $c_3$ (for $s_3$): `110`\n- $c_4$ (for $s_4$): `111`\n\nLet $L$ be the discrete random variable representing the length of the codeword generated by the sensor for a particular symbol emission. Calculate the variance of $L$, denoted as $\\text{Var}(L)$. Express your final answer as a fraction in simplest form.", "solution": "Let $L$ be the codeword length random variable. Using the given codebook, the lengths are $1$ for $s_{1}$, $2$ for $s_{2}$, and $3$ for both $s_{3}$ and $s_{4}$. Therefore, the distribution of $L$ is:\n$$\n\\Pr(L=1)=p(s_{1})=\\frac{1}{2},\\quad \\Pr(L=2)=p(s_{2})=\\frac{1}{4},\\quad \\Pr(L=3)=p(s_{3})+p(s_{4})=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\nCompute the mean:\n$$\n\\mathbb{E}[L]=1\\cdot\\frac{1}{2}+2\\cdot\\frac{1}{4}+3\\cdot\\frac{1}{4}=\\frac{1}{2}+\\frac{1}{2}+\\frac{3}{4}=\\frac{7}{4}.\n$$\nCompute the second moment:\n$$\n\\mathbb{E}[L^2]=1^{2}\\cdot\\frac{1}{2}+2^{2}\\cdot\\frac{1}{4}+3^{2}\\cdot\\frac{1}{4}=\\frac{1}{2}+1+\\frac{9}{4}=\\frac{15}{4}.\n$$\nUsing $\\operatorname{Var}(L)=\\mathbb{E}[L^2]-(\\mathbb{E}[L])^2$, we obtain:\n$$\n\\operatorname{Var}(L)=\\frac{15}{4}-\\left(\\frac{7}{4}\\right)^{2}=\\frac{15}{4}-\\frac{49}{16}=\\frac{60-49}{16}=\\frac{11}{16}.\n$$", "answer": "$$\\boxed{\\frac{11}{16}}$$", "id": "1618709"}, {"introduction": "Often in science and engineering, we can measure the average behavior of a system but don't know its underlying probabilities. This problem presents such a scenario, where you must act as a system analyst to reverse-engineer a probability distribution from operational data. By cleverly combining constraints from both the expected cost of an operation and the theoretical properties of an optimal prefix code, you can uniquely determine the hidden probabilities governing a hypothetical network router [@problem_id:1618686]. This demonstrates the power of information-theoretic principles in solving practical inference problems.", "problem": "A speculative design for a next-generation network router directs incoming data packets to one of three outbound ports, labeled 'a', 'b', and 'c'. The routing decision is probabilistic, governed by a fixed probability distribution. Let $X$ be the discrete random variable representing the chosen port, with an alphabet $\\mathcal{X} = \\{a, b, c\\}$ and corresponding probabilities $P(X=a)$, $P(X=b)$, and $P(X=c)$.\n\nTo analyze the router's energy efficiency, a numerical cost is assigned to the use of each port: the cost for using port 'a' is 1 unit, for port 'b' is 2 units, and for port 'c' is 3 units. A long-term performance analysis of a prototype reveals that the average cost per routed packet is exactly $1.5$ units.\n\nFurthermore, to monitor the router's behavior, the sequence of port choices is encoded for transmission over a low-bandwidth channel. An optimal prefix-free binary code (such as a Huffman code) is constructed for the source $X$. The measured average code length of this optimal code is found to be exactly $1.5$ bits per symbol.\n\nBased on this information, determine the numerical value of the probability that the router chooses port 'b', $P(X=b)$.", "solution": "Let $p_{a} = P(X=a)$, $p_{b} = P(X=b)$, and $p_{c} = P(X=c)$. The normalization and average-cost constraints are\n$$\np_{a} + p_{b} + p_{c} = 1,\n$$\n$$\n1\\cdot p_{a} + 2\\cdot p_{b} + 3\\cdot p_{c} = 1.5.\n$$\nEliminate $p_{a}$ using $p_{a} = 1 - p_{b} - p_{c}$ in the cost equation:\n$$\n1\\cdot(1 - p_{b} - p_{c}) + 2 p_{b} + 3 p_{c} = 1.5,\n$$\n$$\n1 - p_{b} - p_{c} + 2 p_{b} + 3 p_{c} = 1.5,\n$$\n$$\n1 + p_{b} + 2 p_{c} = 1.5,\n$$\n$$\np_{b} + 2 p_{c} = 0.5.\n$$\nHence\n$$\np_{b} = 0.5 - 2 p_{c}, \\quad p_{a} = 1 - p_{b} - p_{c} = 0.5 + p_{c}.\n$$\nFeasibility requires $p_{c} \\ge 0$, $p_{b} \\ge 0 \\Rightarrow p_{c} \\le \\frac{1}{4}$, and $p_{a} \\ge 0$ (automatic).\n\nNow use the coding constraint. For a binary optimal prefix-free code (e.g., Huffman) with three symbols, the optimal codeword lengths are typically $\\{1,2,2\\}$, with the shortest length $1$ assigned to the most probable symbol. The optimal average code length is therefore\n$$\nL^{\\ast} = 1\\cdot p_{\\max} + 2\\cdot (1 - p_{\\max}) = 2 - p_{\\max},\n$$\nwhere $p_{\\max} = \\max\\{p_{a}, p_{b}, p_{c}\\}$. The measured optimal average length is $1.5$, so\n$$\n2 - p_{\\max} = 1.5 \\;\\Rightarrow\\; p_{\\max} = 0.5.\n$$\nFrom the cost-derived relations, $p_{a} = 0.5 + p_{c} \\ge 0.5$, with strict inequality if $p_{c} > 0$. Since $p_{\\max}$ must equal $0.5$, it follows that $p_{c} = 0$, and then\n$$\np_{a} = 0.5, \\quad p_{b} = 0.5.\n$$\nThis distribution satisfies both constraints: the average cost is $1\\cdot 0.5 + 2\\cdot 0.5 + 3\\cdot 0 = 1.5$. For a 3-symbol alphabet $\\{a, b, c\\}$ with probabilities $\\{0.5, 0.5, 0\\}$, an optimal prefix code (e.g., from a Huffman construction) will have lengths $\\{1, 2, 2\\}$, which gives an average length of $0.5\\cdot 1 + 0.5\\cdot 2 + 0\\cdot 2 = 1.5$. Therefore,\n$$\nP(X=b) = \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1618686"}]}