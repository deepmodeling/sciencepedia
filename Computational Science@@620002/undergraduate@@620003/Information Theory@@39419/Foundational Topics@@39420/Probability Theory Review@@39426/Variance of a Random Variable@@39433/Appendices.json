{"hands_on_practices": [{"introduction": "In many data analysis and signal processing tasks, raw data is rarely used as is; it is often rescaled and shifted to be more interpretable or to fit a specific range. This first exercise explores how such linear transformations affect the spread of the data, a crucial concept measured by variance. By working through this problem [@problem_id:1409797], you will practice applying the fundamental properties of variance, specifically how it changes with scaling and shifting, in a practical scenario of calibrating model scores.", "problem": "A data scientist is analyzing the output of a machine learning model designed to predict customer engagement. The model generates a raw, uncalibrated numerical score for each customer profile, represented by a random variable $X$. After analyzing a large sample of scores, the data scientist determines that the expected value (mean) of these scores is $\\mathbb{E}[X] = 2$, and the expected value of the square of the scores is $\\mathbb{E}[X^2] = 13$.\n\nTo make these scores more interpretable and align them with a business-defined scale, a linear transformation is applied to create a new score, $Y$. The transformation is given by the equation $Y = \\frac{1}{2}X + 8$.\n\nCalculate the variance of the transformed scores, $\\operatorname{Var}(Y)$. Your answer should be a numerical value.", "solution": "The goal is to compute the variance of the transformed random variable $Y$, which is defined as $Y = \\frac{1}{2}X + 8$. We are given the first two moments of the original random variable $X$: $\\mathbb{E}[X] = 2$ and $\\mathbb{E}[X^2] = 13$.\n\nFirst, we need to calculate the variance of the original random variable, $X$. The variance of a random variable is defined by the formula:\n$$ \\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 $$\nSubstituting the given values into this formula:\n$$ \\operatorname{Var}(X) = 13 - (2)^2 = 13 - 4 = 9 $$\n\nNext, we use the properties of variance for a linear transformation. For a random variable $X$ and constants $a$ and $b$, the variance of the transformed variable $Y = aX + b$ is given by:\n$$ \\operatorname{Var}(Y) = \\operatorname{Var}(aX + b) = a^2 \\operatorname{Var}(X) $$\nNote that the additive constant $b$ does not affect the variance, as variance is a measure of spread, and adding a constant shifts the distribution without changing its spread.\n\nIn our problem, the transformation is $Y = \\frac{1}{2}X + 8$. By comparing this to the general form $Y = aX + b$, we can identify the constants as $a = \\frac{1}{2}$ and $b = 8$.\n\nNow, we can apply the variance property using our value for $a$ and the previously calculated $\\operatorname{Var}(X)$:\n$$ \\operatorname{Var}(Y) = \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(X) $$\n$$ \\operatorname{Var}(Y) = \\frac{1}{4} \\times 9 $$\n$$ \\operatorname{Var}(Y) = \\frac{9}{4} $$\n\nFinally, we express the result as a numerical value in decimal form:\n$$ \\operatorname{Var}(Y) = 2.25 $$", "answer": "$$\\boxed{2.25}$$", "id": "1409797"}, {"introduction": "When dealing with signals and noise, a key question is how to combine information to improve quality. This thought experiment compares two common approaches: amplifying a single measurement versus adding two independent measurements. This practice [@problem_id:1409813] will challenge your intuition and deepen your understanding of how variance behaves with scaling versus summation, revealing a critical distinction for designing robust systems in fields like communications and signal processing.", "problem": "In a signal processing application, the noise associated with a measurement is modeled as a random variable $X$. This noise has an expected value $\\mathbb{E}[X]=0$ and a known, non-zero variance $\\operatorname{Var}(X) = \\sigma^2$. Two different techniques are proposed to process the signal, which in turn affect the resulting noise.\n\n- **Technique 1:** A single measurement is performed, and the resulting value is digitally multiplied by 2. The new noise variable is $Y_1 = 2X$.\n- **Technique 2:** Two independent measurements are performed. The noise from these measurements are random variables $X_A$ and $X_B$, which are independent and identically distributed, both following the same distribution as $X$. The results are then added together. The new noise variable is $Y_2 = X_A + X_B$.\n\nWhich of the following statements correctly compares the variance of the resulting noise from the two techniques?\n\nA. The variance of $Y_1$ is greater than the variance of $Y_2$.\n\nB. The variance of $Y_2$ is greater than the variance of $Y_1$.\n\nC. The variances of $Y_1$ and $Y_2$ are equal.\n\nD. The comparison depends on the specific probability distribution of $X$ (e.g., Normal, Uniform).\n\nE. The comparison cannot be made without the numerical value of $\\sigma^2$.", "solution": "We are given a random variable $X$ with $\\mathbb{E}[X]=0$ and $\\operatorname{Var}(X)=\\sigma^{2}$, where $\\sigma^{2}>0$. Two techniques create new noise variables $Y_{1}$ and $Y_{2}$, and we compare their variances.\n\nFirst, use the scaling property of variance: for any constant $a$ and any random variable with finite variance,\n$$\n\\operatorname{Var}(aX)=\\mathbb{E}\\big[(aX-\\mathbb{E}[aX])^{2}\\big]=\\mathbb{E}\\big[(a(X-\\mathbb{E}[X]))^{2}\\big]=a^{2}\\mathbb{E}\\big[(X-\\mathbb{E}[X])^{2}\\big]=a^{2}\\operatorname{Var}(X).\n$$\nApplying this to Technique 1 with $Y_{1}=2X$,\n$$\n\\operatorname{Var}(Y_{1})=\\operatorname{Var}(2X)=4\\operatorname{Var}(X)=4\\sigma^{2}.\n$$\n\nNext, consider Technique 2 with $Y_{2}=X_{A}+X_{B}$, where $X_{A}$ and $X_{B}$ are independent and identically distributed as $X$. Using the variance of a sum,\n$$\n\\operatorname{Var}(Y_{2})=\\operatorname{Var}(X_{A}+X_{B})=\\operatorname{Var}(X_{A})+\\operatorname{Var}(X_{B})+2\\operatorname{Cov}(X_{A},X_{B}).\n$$\nIndependence implies $\\operatorname{Cov}(X_{A},X_{B})=0$, and since both have variance $\\sigma^{2}$,\n$$\n\\operatorname{Var}(Y_{2})=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2}.\n$$\n\nComparing the two,\n$$\n\\operatorname{Var}(Y_{1})=4\\sigma^{2} \\quad \\text{and} \\quad \\operatorname{Var}(Y_{2})=2\\sigma^{2},\n$$\nand because $\\sigma^{2}>0$, it follows that $4\\sigma^{2}>2\\sigma^{2}$. Therefore, the variance of $Y_{1}$ is greater than the variance of $Y_{2}$, which corresponds to option A. This comparison does not depend on the specific distribution of $X$, only on the variance and independence, and does not require the numerical value of $\\sigma^{2}$.", "answer": "$$\\boxed{A}$$", "id": "1409813"}, {"introduction": "The concept of a memoryless source is a cornerstone of information theory, modeling everything from simple data streams to sequences of communication symbols. This exercise [@problem_id:1667144] asks you to analyze a key statistic of such a source: the variability in the number of bits needed to observe a specific outcome. You will connect the abstract idea of variance to the geometric distribution, a fundamental probability model for analyzing 'waiting times' in random processes.", "problem": "Consider a memoryless binary source that generates a sequence of bits. The probability of generating a '1' is $p$, and the probability of generating a '0' is $1-p$, with $0 < p < 1$. The generation of each bit is an independent event. Let the random variable $N$ represent the number of bits that must be observed from the source until the first '1' appears. For example, if the sequence is '001...', then $N=3$. Determine the variance of $N$, denoted as $\\operatorname{Var}(N)$, as a function of $p$.", "solution": "The source emits independent Bernoulli trials with success probability $p$ for symbol $1$. Let $N$ be the trial index of the first success. Then $N$ follows the geometric distribution on $\\{1,2,3,\\ldots\\}$ with probability mass function\n$$\n\\Pr(N=n)= (1-p)^{n-1}p,\\quad n=1,2,\\ldots\n$$\nLet $r=1-p$ so that $0<r<1$. We will compute $\\mathbb{E}[N]$ and $\\mathbb{E}[N^{2}]$ using series identities derived from the geometric series. Start with\n$$\n\\sum_{n=0}^{\\infty} r^{n}=\\frac{1}{1-r}.\n$$\nDifferentiate both sides with respect to $r$ to obtain\n$$\n\\sum_{n=1}^{\\infty} n r^{\\,n-1}=\\frac{1}{(1-r)^{2}}.\n$$\nDifferentiate once more:\n$$\n\\sum_{n=2}^{\\infty} n(n-1) r^{\\,n-2}=\\frac{2}{(1-r)^{3}}.\n$$\nMultiplying the last equation by $r$ and adding the first derivative identity gives\n$$\n\\sum_{n=2}^{\\infty} n(n-1) r^{\\,n-1}=\\frac{2r}{(1-r)^{3}},\\quad \\sum_{n=1}^{\\infty} n r^{\\,n-1}=\\frac{1}{(1-r)^{2}}.\n$$\nNote that $n^{2}=n(n-1)+n$, hence\n$$\n\\sum_{n=1}^{\\infty} n^{2} r^{\\,n-1}=\\sum_{n=1}^{\\infty} n(n-1) r^{\\,n-1}+\\sum_{n=1}^{\\infty} n r^{\\,n-1}=\\frac{2r}{(1-r)^{3}}+\\frac{1}{(1-r)^{2}}=\\frac{1+r}{(1-r)^{3}}.\n$$\n\nNow compute the moments of $N$:\n$$\n\\mathbb{E}[N]=\\sum_{n=1}^{\\infty} n\\,(1-p)^{n-1}p=p\\sum_{n=1}^{\\infty} n r^{\\,n-1}=p\\cdot \\frac{1}{(1-r)^{2}}=\\frac{p}{p^{2}}=\\frac{1}{p}.\n$$\nSimilarly,\n$$\n\\mathbb{E}[N^{2}]=\\sum_{n=1}^{\\infty} n^{2}\\,(1-p)^{n-1}p=p\\sum_{n=1}^{\\infty} n^{2} r^{\\,n-1}=p\\cdot \\frac{1+r}{(1-r)^{3}}=p\\cdot \\frac{1+(1-p)}{p^{3}}=\\frac{2-p}{p^{2}}.\n$$\nTherefore, the variance is\n$$\n\\operatorname{Var}(N)=\\mathbb{E}[N^{2}]-\\bigl(\\mathbb{E}[N]\\bigr)^{2}=\\frac{2-p}{p^{2}}-\\left(\\frac{1}{p}\\right)^{2}=\\frac{1-p}{p^{2}}.\n$$", "answer": "$$\\boxed{\\frac{1-p}{p^{2}}}$$", "id": "1667144"}]}