{"hands_on_practices": [{"introduction": "Understanding the shape of a probability distribution is as important as knowing its average or spread. Skewness, the third standardized moment, provides a crucial measure of asymmetry. This first exercise allows you to apply the definition of skewness to the most fundamental building block of digital information: a single bit whose transmission is modeled by a Bernoulli random variable. By calculating it for a specific scenario [@problem_id:1629524], you will gain a concrete understanding of what this higher-order moment represents.", "problem": "In the analysis of digital communication systems, a simple model for the transmission of a single data bit over a noisy channel is the Bernoulli trial. Let the random variable $X$ represent the outcome of the transmission, where $X=1$ indicates a successful transmission and $X=0$ indicates a transmission error. The behavior of this random variable is characterized by a single parameter, $p$, which is the probability of a successful transmission, i.e., $P(X=1) = p$ and $P(X=0) = 1-p$.\n\nThe skewness of a distribution is a measure of its asymmetry. For a random variable $X$ with mean $\\mu = E[X]$ and standard deviation $\\sigma = \\sqrt{E[(X-\\mu)^2]}$, the skewness, denoted by $\\gamma_1$, is defined as the third standardized moment:\n$$ \\gamma_1 = \\frac{E[(X-\\mu)^3]}{\\sigma^3} $$\n\nConsider a specific communication channel where the probability of successful bit transmission is found to be $p = 1/4$. Calculate the skewness, $\\gamma_1$, of the random variable $X$ that models this transmission.\n\nReport your answer as a real number rounded to four significant figures.", "solution": "For a Bernoulli random variable $X \\in \\{0,1\\}$ with $P(X=1)=p$ and $P(X=0)=1-p$, the mean and variance are\n$$\n\\mu = E[X] = p, \\qquad \\sigma^{2} = \\operatorname{Var}(X) = p(1-p), \\qquad \\sigma = \\sqrt{p(1-p)}.\n$$\nThe third central moment is\n$$\nE[(X-\\mu)^{3}] = (1-p)^{3}\\cdot p + (-p)^{3}\\cdot (1-p) = p(1-p)^{3} - p^{3}(1-p).\n$$\nFactor $p(1-p)$ to obtain\n$$\nE[(X-\\mu)^{3}] = p(1-p)\\big[(1-p)^{2} - p^{2}\\big] = p(1-p)(1 - 2p).\n$$\nTherefore, the skewness is\n$$\n\\gamma_{1} = \\frac{E[(X-\\mu)^{3}]}{\\sigma^{3}} = \\frac{p(1-p)(1-2p)}{\\big(p(1-p)\\big)^{3/2}} = \\frac{1 - 2p}{\\sqrt{p(1-p)}}.\n$$\nFor $p = \\frac{1}{4}$,\n$$\n\\gamma_{1} = \\frac{1 - 2\\cdot \\frac{1}{4}}{\\sqrt{\\frac{1}{4}\\left(1 - \\frac{1}{4}\\right)}} = \\frac{\\frac{1}{2}}{\\sqrt{\\frac{3}{16}}} = \\frac{\\frac{1}{2}}{\\frac{\\sqrt{3}}{4}} = \\frac{2}{\\sqrt{3}}.\n$$\nAs a decimal rounded to four significant figures,\n$$\n\\gamma_{1} \\approx 1.155.\n$$", "answer": "$$\\boxed{1.155}$$", "id": "1629524"}, {"introduction": "Building on the concept of skewness, we now explore a more dynamic scenario common in network communications: the number of attempts needed for a successful transmission. This 'time-to-success' is modeled by a Geometric distribution, which is inherently asymmetric. This practice [@problem_id:1629560] challenges you to derive a general expression for skewness, revealing how the system's underlying success probability $p$ shapes the distribution of waiting times.", "problem": "In a wireless communication system, data packets are transmitted over a channel where each transmission attempt is an independent Bernoulli trial. The probability of a successful transmission for any given attempt is $p$, with $0 < p < 1$. Let the random variable $X$ denote the number of attempts required to achieve the first successful packet delivery. This variable represents the \"time-to-success\" in the system.\n\nCalculate the skewness of the probability distribution of $X$. Express your answer as a function of $p$.", "solution": "Let $X$ denote the number of attempts until the first success with success probability $p$, so $X$ is geometric on $\\{1,2,\\dots\\}$ with $\\mathbb{P}(X=k)=p(1-p)^{k-1}$. Define $Y=X-1$, which has support $\\{0,1,2,\\dots\\}$ with $\\mathbb{P}(Y=k)=p(1-p)^{k}$. Skewness is the standardized third central moment $\\gamma_{1}=\\mu_{3}/\\sigma^{3}$, which is invariant under translation. Therefore, the skewness of $X$ equals that of $Y$.\n\nCompute moments for $Y$ via its probability generating function $G_{Y}(t)=\\sum_{k=0}^{\\infty} p(1-p)^{k} t^{k}=\\dfrac{p}{1-(1-p)t}$. The factorial moments satisfy $E[(Y)_{m}]=G_{Y}^{(m)}(1)$. Differentiate:\n$$\nG_{Y}'(t)=\\frac{p(1-p)}{(1-(1-p)t)^{2}},\\quad\nG_{Y}''(t)=\\frac{2p(1-p)^{2}}{(1-(1-p)t)^{3}},\\quad\nG_{Y}'''(t)=\\frac{6p(1-p)^{3}}{(1-(1-p)t)^{4}}.\n$$\nEvaluating at $t=1$ using $1-(1-p)=p$ gives\n$$\nE[(Y)_{1}]=\\frac{1-p}{p},\\quad E[(Y)_{2}]=\\frac{2(1-p)^{2}}{p^{2}},\\quad E[(Y)_{3}]=\\frac{6(1-p)^{3}}{p^{3}}.\n$$\nRelate raw moments to factorial moments via $Y^{2}=(Y)_{2}+(Y)_{1}$ and $Y^{3}=(Y)_{3}+3(Y)_{2}+(Y)_{1}$:\n$$\nE[Y]=\\frac{1-p}{p},\\quad E[Y^{2}]=\\frac{2(1-p)^{2}}{p^{2}}+\\frac{1-p}{p},\\quad E[Y^{3}]=\\frac{6(1-p)^{3}}{p^{3}}+\\frac{6(1-p)^{2}}{p^{2}}+\\frac{1-p}{p}.\n$$\nThe variance is\n$$\n\\sigma^{2}=\\operatorname{Var}(Y)=E[Y^{2}]-(E[Y])^{2}=\\frac{1-p}{p^{2}},\n$$\nso $\\sigma=\\sqrt{1-p}/p$ and $\\sigma^{3}=(1-p)^{3/2}/p^{3}$. The third central moment is\n$$\n\\mu_{3}=E[(Y-E[Y])^{3}]=E[Y^{3}]-3E[Y]E[Y^{2}]+3(E[Y])^{2}E[Y]-(E[Y])^{3}\n=\\frac{(1-p)(2-p)}{p^{3}}.\n$$\nHence the skewness is\n$$\n\\gamma_{1}=\\frac{\\mu_{3}}{\\sigma^{3}}=\\frac{\\dfrac{(1-p)(2-p)}{p^{3}}}{\\dfrac{(1-p)^{3/2}}{p^{3}}}=\\frac{2-p}{\\sqrt{1-p}}.\n$$\nSince skewness is invariant under translation and $X=Y+1$, the skewness of $X$ equals that of $Y$. Therefore, the skewness of $X$ is $(2-p)/\\sqrt{1-p}$.", "answer": "$$\\boxed{\\frac{2-p}{\\sqrt{1-p}}}$$", "id": "1629560"}, {"introduction": "Higher-order moments are not just for characterizing distribution shape; they are essential tools for analyzing transformations of random variables. This exercise transitions from discrete to continuous distributions, focusing on ubiquitous Gaussian noise. You will use the second and fourth moments of a standard normal signal, $Z$, to determine the variance of its corresponding power, represented as $Y=Z^2$. This common task in signal processing [@problem_id:1629549] demonstrates the practical power of moments in linking the statistics of related physical quantities.", "problem": "In a simplified model of a communication system, the amplitude of a random noise signal at any given instant is described by a standard normal random variable, $Z$. A standard normal random variable has a probability density function $p(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$ for $-\\infty < z < \\infty$. The instantaneous power associated with this noise is often related to the square of its amplitude. We define a new random variable, $Y = Z^2$, which represents the normalized instantaneous power of this noise. All quantities are dimensionless.\n\nCalculate the variance of the normalized instantaneous power, $Y$.", "solution": "The problem asks for the variance of the random variable $Y$, defined as $Y = Z^2$, where $Z$ is a standard normal random variable.\n\nThe variance of a random variable $Y$ is given by the formula:\n$$ \\text{Var}(Y) = E[Y^2] - (E[Y])^2 $$\nwhere $E[\\cdot]$ denotes the expectation operator.\n\nFirst, we express the variance of $Y$ in terms of the random variable $Z$. Substituting $Y = Z^2$:\n$$ \\text{Var}(Y) = E[(Z^2)^2] - (E[Z^2])^2 = E[Z^4] - (E[Z^2])^2 $$\nThis shows that we need to compute the second and fourth moments of the standard normal random variable $Z$.\n\nThe $n$-th moment of a continuous random variable $Z$ with probability density function (PDF) $p(z)$ is defined as:\n$$ E[Z^n] = \\int_{-\\infty}^{\\infty} z^n p(z) \\,dz $$\nFor the standard normal distribution, the PDF is $p(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$.\n\nLet's calculate the second moment, $E[Z^2]$.\n$$ E[Z^2] = \\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\nFor a standard normal distribution, the mean $E[Z]$ is 0 and the variance $\\text{Var}(Z)$ is 1. The variance can also be expressed as $\\text{Var}(Z) = E[Z^2] - (E[Z])^2$.\nTherefore, $1 = E[Z^2] - 0^2$, which directly gives $E[Z^2] = 1$.\nSo, the expectation of $Y$ is $E[Y] = E[Z^2] = 1$.\n\nNext, we calculate the fourth moment, $E[Z^4]$.\n$$ E[Z^4] = \\int_{-\\infty}^{\\infty} z^4 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\nWe can solve this integral using integration by parts, where $\\int u \\,dv = uv - \\int v \\,du$.\nLet's choose $u = z^3$ and $dv = z \\exp\\left(-\\frac{z^2}{2}\\right) dz$.\nThen $du = 3z^2 dz$ and $v = \\int z \\exp\\left(-\\frac{z^2}{2}\\right) dz = -\\exp\\left(-\\frac{z^2}{2}\\right)$.\n\nThe integral for $E[Z^4]$ becomes:\n$$ E[Z^4] = \\frac{1}{\\sqrt{2\\pi}} \\left[ \\left( z^3 \\right) \\left( -\\exp\\left(-\\frac{z^2}{2}\\right) \\right) \\right]_{-\\infty}^{\\infty} - \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\left( -\\exp\\left(-\\frac{z^2}{2}\\right) \\right) (3z^2) \\,dz $$\nThe first term (the boundary term) is evaluated at $\\pm \\infty$. As $z \\to \\pm\\infty$, the exponential term $\\exp(-z^2/2)$ goes to zero much faster than any polynomial term $z^3$ goes to infinity. Thus, the boundary term is 0.\n$$ \\left[ -z^3 \\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{-\\infty}^{\\infty} = 0 - 0 = 0 $$\nSo, we are left with the second term:\n$$ E[Z^4] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} 3z^2 \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\n$$ E[Z^4] = 3 \\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\nThe integral in this expression is exactly the definition of the second moment, $E[Z^2]$.\n$$ E[Z^4] = 3 \\cdot E[Z^2] $$\nSince we already found that $E[Z^2] = 1$, we have:\n$$ E[Z^4] = 3 \\cdot 1 = 3 $$\n\nNow we have both moments needed to calculate the variance of $Y$.\n$$ \\text{Var}(Y) = E[Z^4] - (E[Z^2])^2 $$\nSubstituting the values we found:\n$$ \\text{Var}(Y) = 3 - (1)^2 = 3 - 1 = 2 $$\n\nThus, the variance of the normalized instantaneous power $Y$ is 2.", "answer": "$$\\boxed{2}$$", "id": "1629549"}]}