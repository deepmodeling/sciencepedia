{"hands_on_practices": [{"introduction": "Let's begin with a foundational practice that combines probability theory with the concept of differential entropy. This exercise explores a scenario common in system design, where reliability is enhanced by taking the maximum value from two independent sensors. Your task is to first determine the probability distribution of this new random variable, $Z = \\max(X, Y)$, and then apply the definition of differential entropy to quantify its uncertainty [@problem_id:1613617].", "problem": "In a simple model of a fault-tolerant electronic system, two independent sensors provide readings, represented by the continuous random variables $X$ and $Y$. Due to normalization and calibration, both a single sensor's reading is equally likely to be any value in the interval $[0, 1]$. Thus, both $X$ and $Y$ are described by a uniform probability distribution on $[0, 1]$.\n\nTo improve reliability against \"fail-low\" errors where a sensor might erroneously report a value of zero, the system's final output, $Z$, is determined by taking the maximum of the two sensor readings, i.e., $Z = \\max(X, Y)$.\n\nThe uncertainty associated with a continuous random variable $V$ having a probability density function (PDF) $p_V(v)$ is measured by its differential entropy, $h(V)$, calculated in units of \"nats\". The formula for differential entropy is given by:\n$$h(V) = -\\int_{-\\infty}^{\\infty} p_V(v) \\ln(p_V(v)) dv$$\nCalculate the differential entropy, $h(Z)$, of the system's output. Present your answer as a single closed-form analytic expression.", "solution": "Let $X$ and $Y$ be independent and identically distributed as $\\operatorname{Uniform}(0,1)$. Define $Z=\\max(X,Y)$. For $z\\in[0,1]$, the cumulative distribution function of $Z$ is\n$$\nF_{Z}(z)=\\mathbb{P}(Z\\leq z)=\\mathbb{P}(X\\leq z,\\,Y\\leq z)=\\mathbb{P}(X\\leq z)\\mathbb{P}(Y\\leq z)=z^{2},\n$$\nusing independence. For $z<0$, $F_{Z}(z)=0$, and for $z>1$, $F_{Z}(z)=1$. Differentiating on $(0,1)$ gives the probability density function\n$$\nf_{Z}(z)=\\frac{d}{dz}F_{Z}(z)=2z,\\quad 0<z<1.\n$$\nThe differential entropy of $Z$ is\n$$\nh(Z)=-\\int_{0}^{1} f_{Z}(z)\\ln\\big(f_{Z}(z)\\big)\\,dz=-\\int_{0}^{1}2z\\ln(2z)\\,dz.\n$$\nSplit the logarithm:\n$$\n-\\int_{0}^{1}2z\\ln(2z)\\,dz=-(\\ln 2)\\int_{0}^{1}2z\\,dz-\\int_{0}^{1}2z\\ln z\\,dz.\n$$\nEvaluate the first integral:\n$$\n\\int_{0}^{1}2z\\,dz=\\left[z^{2}\\right]_{0}^{1}=1,\n$$\nso the first term equals $-\\ln 2$. For the second integral, integrate by parts with $u=\\ln z$ and $dv=2z\\,dz$, so $du=\\frac{1}{z}\\,dz$ and $v=z^{2}$:\n$$\n\\int_{0}^{1}2z\\ln z\\,dz=\\left[z^{2}\\ln z\\right]_{0}^{1}-\\int_{0}^{1}z\\,dz=0-\\left[\\frac{z^{2}}{2}\\right]_{0}^{1}=-\\frac{1}{2}.\n$$\nTherefore,\n$$\nh(Z)=-\\ln 2-\\left(-\\frac{1}{2}\\right)=\\frac{1}{2}-\\ln 2.\n$$", "answer": "$$\\boxed{\\frac{1}{2}-\\ln 2}$$", "id": "1613617"}, {"introduction": "Building on the previous exercise, we now consider another fundamental operation: the summation of random variables. This situation models the combination of independent signals or noise sources, a ubiquitous phenomenon in physics and engineering. You will derive the distribution for $Z = X+Y$ using convolution—a key technique in probability theory—and then proceed to calculate its differential entropy, reinforcing your skills with a classic distribution shape [@problem_id:1617737].", "problem": "Consider two independent and identically distributed continuous random variables, $X$ and $Y$. Both variables follow a uniform distribution on the interval $[0, L]$, where $L=e$, the base of the natural logarithm. A new random variable, $Z$, is formed by their sum, $Z = X+Y$.\n\nCalculate the differential entropy, $h(Z)$, of the random variable $Z$. Provide the exact numerical value for your answer.", "solution": "Let $X$ and $Y$ be independent and identically distributed with $X,Y \\sim \\mathrm{Unif}(0,L)$. The sum $Z=X+Y$ has the triangular density given by the convolution of two uniforms:\n$$\nf_{Z}(z)=\\begin{cases}\n\\frac{z}{L^{2}}, & 0 \\leq z \\leq L,\\\\\n\\frac{2L - z}{L^{2}}, & L \\leq z \\leq 2L,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe differential entropy is\n$$\nh(Z)=-\\int_{0}^{L}\\frac{z}{L^{2}}\\ln\\!\\left(\\frac{z}{L^{2}}\\right)\\,dz-\\int_{L}^{2L}\\frac{2L-z}{L^{2}}\\ln\\!\\left(\\frac{2L-z}{L^{2}}\\right)\\,dz.\n$$\nBy the substitution $u=2L-z$ in the second integral, both integrals are equal. Defining\n$$\nI=\\int_{0}^{L}\\frac{z}{L^{2}}\\ln\\!\\left(\\frac{z}{L^{2}}\\right)\\,dz,\n$$\nwe have $h(Z)=-2I$. Compute $I$ via $z=Lt$, $dz=L\\,dt$:\n$$\nI=\\int_{0}^{1}t\\left[\\ln t-\\ln L\\right]\\,dt=\\int_{0}^{1}t\\ln t\\,dt-(\\ln L)\\int_{0}^{1}t\\,dt.\n$$\nUsing $\\int_{0}^{1}t\\,dt=\\frac{1}{2}$ and $\\int_{0}^{1}t\\ln t\\,dt=-\\frac{1}{4}$, we obtain\n$$\nI=-\\frac{1}{4}-\\frac{1}{2}\\ln L,\n$$\nhence\n$$\nh(Z)=-2I=\\frac{1}{2}+\\ln L.\n$$\nWith $L=\\exp(1)$, we have $\\ln L=\\ln(\\exp(1))=1$, so\n$$\nh(Z)=\\frac{1}{2}+1=\\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1617737"}, {"introduction": "Our final practice presents a more challenging and realistic scenario involving the multiplicative interaction of random variables, such as signal attenuation through multiple layers. Calculating the entropy of $Z = XY$ requires a more intricate derivation for the probability density function and introduces an integral connected to fundamental mathematical constants [@problem_id:1613654]. This problem demonstrates how information-theoretic calculations can intersect with deeper mathematical concepts, reflecting the richness of the field.", "problem": "In a simplified model of a wireless communication channel, the amplitude of a transmitted signal is attenuated by two independent, non-identical atmospheric layers. The attenuation factor for the first layer is represented by a random variable $X$, which is uniformly distributed on the interval $[0, 1]$. The attenuation factor for the second layer is represented by a random variable $Y$, also uniformly distributed on $[0, 1]$. The two attenuation processes are statistically independent. The total attenuation factor for the signal is the product of the individual factors, $Z = XY$.\n\nTo characterize the uncertainty associated with the total signal attenuation, we are interested in its differential entropy, $h(Z)$. The differential entropy of a continuous random variable $W$ with a probability density function (PDF) $f_W(w)$ over its support is defined as:\n$$h(W) = -\\int_{-\\infty}^{\\infty} f_W(w) \\ln(f_W(w)) dw$$\nCalculate the differential entropy $h(Z)$ of the total attenuation factor $Z$. Your final answer should be a concise analytic expression, potentially involving fundamental mathematical constants. Define the Euler-Mascheroni constant as $\\gamma$.", "solution": "Let $X$ and $Y$ be independent, each uniformly distributed on $[0,1]$, and define $Z=XY$. To find $h(Z)$ we first determine the probability density function $f_{Z}(z)$.\n\nFor $0<z<1$, the cumulative distribution function is\n$$\nF_{Z}(z)=\\mathbb{P}(XY\\le z)=\\int_{0}^{1}\\mathbb{P}\\!\\left(Y\\le \\frac{z}{x}\\right)dx.\n$$\nSplit the integral at $x=z$:\n- For $0<x\\le z$, $\\frac{z}{x}\\ge 1$ so $\\mathbb{P}(Y\\le z/x)=1$.\n- For $z<x\\le 1$, $0<\\frac{z}{x}<1$ so $\\mathbb{P}(Y\\le z/x)=\\frac{z}{x}$.\n\nThus,\n$$\nF_{Z}(z)=\\int_{0}^{z}1\\,dx+\\int_{z}^{1}\\frac{z}{x}\\,dx\n=z+z\\left(\\ln(1)-\\ln z\\right)=z-z\\ln z,\n$$\nwith $F_{Z}(z)=0$ for $z\\le 0$ and $F_{Z}(z)=1$ for $z\\ge 1$. Differentiating on $(0,1)$ gives the density\n$$\nf_{Z}(z)=\\frac{d}{dz}\\bigl(z-z\\ln z\\bigr)=-\\ln z,\\qquad 0<z<1,\n$$\nand $f_{Z}(z)=0$ otherwise.\n\nThe differential entropy is\n$$\nh(Z)=-\\int_{0}^{1}f_{Z}(z)\\ln f_{Z}(z)\\,dz\n=-\\int_{0}^{1}\\bigl(-\\ln z\\bigr)\\,\\ln\\bigl(-\\ln z\\bigr)\\,dz.\n$$\nEvaluate the integral via the substitution $t=-\\ln z$, so $z=\\exp(-t)$ and $dz=-\\exp(-t)\\,dt$. As $z$ runs from $0$ to $1$, $t$ runs from $\\infty$ to $0$. Then\n$$\n\\int_{0}^{1}\\bigl(-\\ln z\\bigr)\\,\\ln\\bigl(-\\ln z\\bigr)\\,dz\n=\\int_{\\infty}^{0}t\\ln t\\bigl(-\\exp(-t)\\bigr)\\,dt\n=\\int_{0}^{\\infty}t\\ln t\\,\\exp(-t)\\,dt.\n$$\nRecognize this as a Gamma-function derivative. For $a>0$,\n$$\n\\Gamma(a)=\\int_{0}^{\\infty}t^{a-1}\\exp(-t)\\,dt,\\qquad\n\\Gamma'(a)=\\int_{0}^{\\infty}t^{a-1}\\ln t\\,\\exp(-t)\\,dt=\\Gamma(a)\\psi(a),\n$$\nwhere $\\psi$ is the digamma function. With $a=2$,\n$$\n\\int_{0}^{\\infty}t\\ln t\\,\\exp(-t)\\,dt=\\Gamma'(2)=\\Gamma(2)\\psi(2)=1\\cdot\\psi(2).\n$$\nUsing $\\psi(2)=1-\\gamma$, where $\\gamma$ is the Euler–Mascheroni constant, we have\n$$\n\\int_{0}^{\\infty}t\\ln t\\,\\exp(-t)\\,dt=1-\\gamma.\n$$\nTherefore,\n$$\nh(Z)=-\\bigl(1-\\gamma\\bigr)=\\gamma-1.\n$$\nSince the logarithm used is the natural logarithm, the entropy is expressed in nats.", "answer": "$$\\boxed{\\gamma-1}$$", "id": "1613654"}]}