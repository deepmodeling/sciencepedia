{"hands_on_practices": [{"introduction": "Bayes' rule provides a formal mechanism for updating our beliefs in light of new evidence. This first exercise [@problem_id:1603705] offers a foundational application within the context of digital communication. We will explore a scenario involving a Binary Erasure Channel, where a transmitted bit is either received correctly or lost as an 'erasure', and use Bayes' rule to calculate the probability of the original input given this ambiguous outcome.", "problem": "A digital memory device stores data as a sequence of binary bits. Let the source bit, denoted by the random variable $X$, be either $0$ or $1$. From extensive analysis of the data patterns, it is known that the prior probability of a bit being $0$ is $P(X=0) = \\alpha$.\n\nWhen a bit is read from the device, one of three outcomes can occur for the received symbol, denoted by $Y$: the bit is read correctly as $0$, correctly as $1$, or the read operation fails, resulting in an 'erasure' symbol, which we denote as '?'. The device is designed such that it never flips a bit; a stored $0$ is never read as a $1$, and a stored $1$ is never read as a $0$.\n\nThe reliability of the read operation, however, depends on the stored value. The probability of an erasure occurring when the stored bit is $0$ is $P(Y='?'|X=0) = p_0$. The probability of an erasure occurring when the stored bit is $1$ is $P(Y='?'|X=1) = p_1$.\n\nSuppose a single bit is read from the device and the outcome is an erasure, '?'. Determine the posterior probability that the bit originally stored in the device was a $0$. Provide your answer as a single closed-form analytic expression in terms of $\\alpha$, $p_0$, and $p_1$.", "solution": "We are asked to find the posterior probability that the transmitted bit was a $0$ given that the received symbol was an erasure. This can be written as $P(X=0 | Y='?')$.\n\nTo solve this, we apply Bayes' rule, which states:\n$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n\nIn the context of our problem, let event $A$ be $X=0$ (the stored bit is 0) and event $B$ be $Y='?'$ (an erasure is observed). Substituting these into Bayes' rule, we get:\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n\nThe problem provides the following values:\n- $P(X=0) = \\alpha$\n- $P(Y='?' | X=0) = p_0$\n\nThe only term we need to calculate is the total probability of observing an erasure, $P(Y='?')$. We can find this using the law of total probability, summing over all possible inputs for $X$:\n$$ P(Y='?') = P(Y='?' | X=0) \\cdot P(X=0) + P(Y='?' | X=1) \\cdot P(X=1) $$\n\nWe are given $P(X=0) = \\alpha$, which implies that the probability of the stored bit being $1$ is $P(X=1) = 1 - P(X=0) = 1 - \\alpha$. We are also given the conditional probability $P(Y='?' | X=1) = p_1$.\n\nNow, we can substitute all the known probabilities into the expression for $P(Y='?')$:\n$$ P(Y='?') = (p_0) \\cdot (\\alpha) + (p_1) \\cdot (1 - \\alpha) $$\n$$ P(Y='?') = \\alpha p_0 + p_1(1-\\alpha) $$\n\nWith the expression for $P(Y='?')$, we can now complete the calculation for our target posterior probability using Bayes' rule:\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n$$ P(X=0 | Y='?') = \\frac{p_0 \\cdot \\alpha}{\\alpha p_0 + p_1(1-\\alpha)} $$\n\nThis is the final expression for the posterior probability that the stored bit was a $0$ given that an erasure was observed.", "answer": "$$\\boxed{\\frac{\\alpha p_{0}}{\\alpha p_{0} + p_{1}(1-\\alpha)}}$$", "id": "1603705"}, {"introduction": "The power of Bayesian inference lies in its flexibility to handle various forms of evidence, even indirect ones. This problem [@problem_id:1603706] moves beyond simple channel errors to a more abstract scenario involving source coding. By only knowing the length of an optimally compressed codeword, you will use Bayes' rule to infer which of two possible sources was more likely to have generated the message, demonstrating how Bayesian reasoning can serve as a powerful tool for model selection.", "problem": "In a communication system, messages are generated from one of two possible discrete memoryless sources, $S_A$ or $S_B$. The system first randomly selects a source, choosing $S_A$ with a probability of $0.4$ and $S_B$ with a probability of $0.6$. Once a source is selected, it generates a single symbol from its alphabet. Each symbol is then encoded into a binary string using an optimal prefix code designed specifically for that source.\n\nThe alphabets and the probability distributions for the symbols are as follows:\n\n- Source $S_A$ has the alphabet $\\{A_1, A_2, A_3\\}$ with probabilities $P(A_1)=0.5$, $P(A_2)=0.25$, and $P(A_3)=0.25$.\n- Source $S_B$ has the alphabet $\\{B_1, B_2, B_3, B_4\\}$ with probabilities $P(B_1)=0.6$, $P(B_2)=0.2$, $P(B_3)=0.1$, and $P(B_4)=0.1$.\n\nAn observer intercepts an encoded binary string but can only determine its length. The observed codeword has a length of exactly 2 bits. Given this information, calculate the probability that the source selected by the system was $S_A$. Round your final answer to three significant figures.", "solution": "We are asked for the posterior probability that the selected source was $S_{A}$ given that the observed codeword length is $2$ bits. Let $L$ denote the codeword length, and use Bayes’ theorem:\n$$\nP(S_{A}\\mid L=2)=\\frac{P(L=2\\mid S_{A})P(S_{A})}{P(L=2\\mid S_{A})P(S_{A})+P(L=2\\mid S_{B})P(S_{B})}.\n$$\n\nBecause each source uses its own optimal prefix code (Huffman code) for its symbol distribution, we determine the codeword lengths for each source.\n\nFor source $S_{A}$ with probabilities $0.5,0.25,0.25$, the Huffman procedure combines the two symbols with probabilities $0.25$ and $0.25$ to make $0.5$, then combines $0.5$ with $0.5$. This yields lengths $1$ for the $0.5$ symbol and $2$ for the two $0.25$ symbols. Therefore,\n$$\nP(L=2\\mid S_{A})=0.25+0.25=0.5.\n$$\n\nFor source $S_{B}$ with probabilities $0.6,0.2,0.1,0.1$, the Huffman procedure first combines $0.1$ and $0.1$ to get $0.2$, yielding multiset $\\{0.6,0.2,0.2\\}$. Then it combines the two $0.2$’s to get $0.4$, and finally combines $0.4$ with $0.6$. This yields lengths $1$ for the $0.6$ symbol, $2$ for the original $0.2$ symbol, and $3$ for the two $0.1$ symbols. Therefore,\n$$\nP(L=2\\mid S_{B})=0.2.\n$$\n\nThe prior source selection probabilities are $P(S_{A})=0.4$ and $P(S_{B})=0.6$. Hence\n$$\nP(L=2)=P(L=2\\mid S_{A})P(S_{A})+P(L=2\\mid S_{B})P(S_{B})\n=0.5\\cdot 0.4+0.2\\cdot 0.6=0.2+0.12=0.32.\n$$\nTherefore,\n$$\nP(S_{A}\\mid L=2)=\\frac{0.5\\cdot 0.4}{0.32}=\\frac{0.2}{0.32}=0.625.\n$$\nRounded to three significant figures, this is $0.625$.", "answer": "$$\\boxed{0.625}$$", "id": "1603706"}, {"introduction": "Our final practice [@problem_id:1603712] represents a significant conceptual step, moving from inferring discrete hidden states to learning the underlying parameters of a model itself. Here, the probability $p$ of a binary source is treated as an unknown quantity that we can describe with a probability distribution. This exercise illustrates the core of Bayesian parameter estimation: using observed data to update our belief about a continuous parameter, a fundamental technique that forms the basis for much of modern machine learning and statistical inference.", "problem": "An engineer is tasked with characterizing a novel binary memory source. The source generates a sequence of bits (0s and 1s). The generation of each bit is assumed to be an independent event. The probability that any given bit is a '1' is an unknown constant, $p$. The probability that it is a '0' is therefore $1-p$.\n\nDue to the manufacturing process, the parameter $p$ is not fixed across all such sources but can be modeled as a random variable. Based on design specifications, the engineer's initial belief about the distribution of $p$ is described by a Beta probability density function with parameters $\\alpha=2$ and $\\beta=2$. The probability density function is given by:\n$$f(p) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)}$$\nfor $0 < p < 1$, where $B(\\alpha, \\beta)$ is the Beta function which serves as the normalization constant.\n\nTo refine this model, the engineer observes a test sequence of $100$ bits generated by a particular source. This sequence is found to contain exactly $70$ ones and $30$ zeros.\n\nGiven this new data, calculate the updated expected value of the probability parameter $p$. Express your answer as a decimal rounded to three significant figures.", "solution": "We model the bit sequence as independent Bernoulli trials with success probability $p$. The prior for $p$ is $\\operatorname{Beta}(\\alpha,\\beta)$ with $\\alpha=2$ and $\\beta=2$, having density\n$$\nf(p)=\\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}, \\quad 0<p<1.\n$$\nFrom the data, there are $n=100$ trials with $S=70$ ones (successes) and $F=30$ zeros (failures), so the likelihood given $p$ is, up to a multiplicative constant,\n$$\nL(p\\mid \\text{data}) \\propto p^{S}(1-p)^{F}.\n$$\nBy Bayes' theorem and conjugacy of the Beta prior with the Bernoulli/Binomial likelihood, the posterior is proportional to\n$$\nf(p\\mid \\text{data}) \\propto f(p)\\,L(p\\mid \\text{data}) \\propto p^{(\\alpha-1)+S}(1-p)^{(\\beta-1)+F},\n$$\nwhich is recognized as a Beta density with updated parameters\n$$\n\\alpha_{\\text{post}}=\\alpha+S=2+70=72, \\quad \\beta_{\\text{post}}=\\beta+F=2+30=32.\n$$\nTherefore, $p\\mid \\text{data} \\sim \\operatorname{Beta}(72,32)$. The posterior expected value is the mean of a Beta distribution:\n$$\n\\mathbb{E}[p\\mid \\text{data}] = \\frac{\\alpha_{\\text{post}}}{\\alpha_{\\post}+\\beta_{\\post}} = \\frac{72}{72+32} = \\frac{72}{104}=\\frac{9}{13}.\n$$\nAs a decimal rounded to three significant figures,\n$$\n\\frac{9}{13}\\approx 0.692307\\ldots \\rightarrow 0.692.\n$$", "answer": "$$\\boxed{0.692}$$", "id": "1603712"}]}