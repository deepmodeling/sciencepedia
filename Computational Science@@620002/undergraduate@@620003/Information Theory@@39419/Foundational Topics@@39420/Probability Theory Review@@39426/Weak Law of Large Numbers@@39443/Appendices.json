{"hands_on_practices": [{"introduction": "We will begin with a classic engineering problem: how many measurements are enough? This practice [@problem_id:1407167] grounds the Weak Law of Large Numbers in a practical context, showing how it guarantees that we can achieve a desired estimation precision by collecting sufficient data. You will apply Chebyshev's inequality, the core mathematical tool behind the WLLN, to determine the minimum number of sensors needed to meet a strict quality standard.", "problem": "A team of environmental scientists is deploying a network of low-cost sensors to monitor the concentration of a specific dissolved chemical in a large, stable reservoir. The true concentration of the chemical, denoted by $\\mu$, is unknown but constant throughout the reservoir.\n\nEach sensor, when deployed, provides a single measurement. Let $X_i$ be the measurement from the $i$-th sensor. These measurements are assumed to be independent and identically distributed random variables. While the sensors are unbiased, meaning the expected value of any measurement is the true concentration ($E[X_i] = \\mu$), they are subject to random error. The quality of these sensors is characterized by the standard deviation of their measurements, which is known to be $\\sigma = 0.5$ parts per million (ppm).\n\nThe team plans to calculate the average of all the sensor readings, $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$, where $n$ is the number of sensors deployed. To ensure the reliability of their findings, the team requires that the absolute difference between their average measurement and the true concentration is no more than $0.02$ ppm. Furthermore, this condition must be met with a probability of at least $0.99$.\n\nWhat is the minimum number of sensors, $n$, the team must deploy to satisfy this requirement?", "solution": "Let $\\{X_{i}\\}_{i=1}^{n}$ be i.i.d. with $E[X_{i}] = \\mu$ and $\\operatorname{Var}(X_{i}) = \\sigma^{2}$, where $\\sigma = 0.5$. The sample mean is $\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$, with\n$$\nE[\\bar{X}_{n}] = \\mu, \\qquad \\operatorname{Var}(\\bar{X}_{n}) = \\frac{\\sigma^{2}}{n}.\n$$\nBy Chebyshev's inequality, for any $\\varepsilon > 0$,\n$$\n\\mathbb{P}\\!\\left(|\\bar{X}_{n} - \\mu| \\geq \\varepsilon\\right) \\leq \\frac{\\operatorname{Var}(\\bar{X}_{n})}{\\varepsilon^{2}} = \\frac{\\sigma^{2}}{n \\varepsilon^{2}}.\n$$\nEquivalently,\n$$\n\\mathbb{P}\\!\\left(|\\bar{X}_{n} - \\mu| \\leq \\varepsilon\\right) \\geq 1 - \\frac{\\sigma^{2}}{n \\varepsilon^{2}}.\n$$\nWe require $\\mathbb{P}(|\\bar{X}_{n} - \\mu| \\leq 0.02) \\geq 0.99$, so set $\\varepsilon = 0.02$ and impose\n$$\n1 - \\frac{\\sigma^{2}}{n (0.02)^{2}} \\geq 0.99 \\quad \\Longleftrightarrow \\quad \\frac{\\sigma^{2}}{n (0.02)^{2}} \\leq 0.01.\n$$\nSolving for $n$ gives\n$$\nn \\geq \\frac{\\sigma^{2}}{0.01 \\,(0.02)^{2}}.\n$$\nSubstituting $\\sigma = 0.5$,\n$$\nn \\geq \\frac{(0.5)^{2}}{0.01 \\times (0.02)^{2}} = \\frac{0.25}{0.01 \\times 0.0004} = \\frac{0.25}{0.000004} = 62500.\n$$\nSince $n$ must be an integer, the minimum number of sensors is $n = 62500$.", "answer": "$$\\boxed{62500}$$", "id": "1407167"}, {"introduction": "The power of the WLLN extends beyond averaging simple observations. This exercise [@problem_id:1967327] explores the convergence of the sample second moment, a key statistic in physics and engineering for characterizing quantities like average energy. You will learn to apply the WLLN to a new sequence of random variables derived from the original data, demonstrating the law's broad utility in analyzing more complex statistical properties.", "problem": "Consider a sequence of random variables $X_1, X_2, \\dots, X_n$ that are independent and identically distributed (i.i.d.). Each random variable $X_i$ in this sequence has a known finite mean $E[X_i] = \\mu$ and a known finite, positive variance $\\text{Var}(X_i) = \\sigma^2$.\n\nWe define the sample second moment about the origin for this sequence as:\n$$M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i^2$$\nThis quantity is of interest in various fields, for instance, in physics where it might relate to the average energy of a system of particles.\n\nDetermine the value to which $M_n$ converges in probability as the sample size $n$ approaches infinity. Express your answer as a closed-form analytic expression in terms of $\\mu$ and $\\sigma$.", "solution": "The problem asks for the value to which the sample second moment, $M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i^2$, converges in probability. This is a direct application of the Weak Law of Large Numbers (WLLN).\n\nThe WLLN states that for a sequence of independent and identically distributed (i.i.d.) random variables $Y_1, Y_2, \\dots$ with a finite expected value $E[Y_i] = \\mu_Y$, their sample mean $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ converges in probability to $\\mu_Y$. We can write this as $\\bar{Y}_n \\xrightarrow{p} \\mu_Y$ as $n \\to \\infty$.\n\nTo apply the WLLN to our problem, let's define a new sequence of random variables $Y_i = X_i^2$. The quantity $M_n$ can then be rewritten as the sample mean of this new sequence:\n$$M_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}_n$$\n\nNow, we must check if the conditions for the WLLN are met for the sequence $Y_i$.\n1.  **I.I.D. Condition:** The problem states that the random variables $X_1, X_2, \\dots$ are i.i.d. Since each $Y_i$ is a function of the corresponding $X_i$ (specifically $Y_i = X_i^2$), and the function is the same for all $i$, the sequence of random variables $Y_1, Y_2, \\dots$ is also independent and identically distributed.\n\n2.  **Finite Mean Condition:** The WLLN requires that the expected value of $Y_i$, denoted $E[Y_i]$, is finite. Let's calculate this expectation.\n$$E[Y_i] = E[X_i^2]$$\nWe can relate $E[X_i^2]$ to the given mean and variance of $X_i$. The definition of variance is:\n$$\\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2$$\nWe are given that $\\text{Var}(X_i) = \\sigma^2$ and $E[X_i] = \\mu$. Substituting these values into the variance formula gives:\n$$\\sigma^2 = E[X_i^2] - \\mu^2$$\nSolving for $E[X_i^2]$, we get:\n$$E[X_i^2] = \\mu^2 + \\sigma^2$$\nSince $\\mu$ and $\\sigma^2$ are given as finite, the expectation $E[Y_i] = \\mu^2 + \\sigma^2$ is also finite. Let's denote this common mean of the $Y_i$ sequence as $\\mu_Y = \\mu^2 + \\sigma^2$.\n\nSince both conditions for the WLLN are satisfied for the sequence $Y_i = X_i^2$, we can conclude that their sample mean, $M_n$, converges in probability to their true mean, $\\mu_Y$.\n$$M_n \\xrightarrow{p} E[Y_i] = \\mu^2 + \\sigma^2$$\n\nThus, the value to which $M_n$ converges in probability is $\\mu^2 + \\sigma^2$.", "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2}}$$", "id": "1967327"}, {"introduction": "A crucial assumption of the WLLN is the independence of observations, but what happens when this condition is violated? This final practice [@problem_id:1668567] investigates a sensor network where all readings are influenced by a common, fluctuating environmental factor, introducing a systemic correlation. This problem will challenge your intuition and reveal the fundamental difference between random error, which can be averaged away, and systemic error, which cannot.", "problem": "A large-scale environmental sensing network is deployed to monitor a certain physical quantity, $T$. The value of $T$ is not a fixed constant but fluctuates over time due to complex environmental dynamics. For the purpose of analysis, $T$ is modeled as a random variable with a well-defined but unknown mean $E[T] = \\mu$ and a finite, non-zero variance $\\text{Var}(T) = \\sigma_T^2$.\n\nThe network consists of $n$ identical sensors. The reading of the $i$-th sensor, denoted by $X_i$, is a sum of the true quantity $T$ and an independent internal noise term $\\epsilon_i$. Thus, $X_i = T + \\epsilon_i$. The noise terms $\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n$ are modeled as independent and identically distributed (i.i.d.) random variables, each with an expected value of $E[\\epsilon_i] = 0$ and a finite variance $\\text{Var}(\\epsilon_i) = \\sigma_\\epsilon^2$. Furthermore, the noise terms are independent of the physical quantity $T$.\n\nTo estimate the mean quantity $\\mu$, an engineer computes the sample average of all sensor readings:\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i $$\nWhile it is often assumed that such an average converges to the true mean $\\mu$ for large $n$, the common influence of $T$ on all sensors introduces a systemic correlation. Your task is to determine the limit of the sample average $\\bar{X}_n$ as the number of sensors $n$ approaches infinity. Find the value $L$ to which $\\bar{X}_n$ converges in probability, meaning that for any arbitrary small positive number $\\delta$, the probability $P(|\\bar{X}_n - L| \\geq \\delta)$ approaches zero as $n \\to \\infty$.\n\nExpress your answer for $L$ as an analytic expression in terms of the variables defined.", "solution": "We start from the given measurement model for each sensor,\n$$\nX_{i} = T + \\epsilon_{i},\n$$\nwhere $T$ is a random variable with $E[T] = \\mu$ and $\\text{Var}(T) = \\sigma_{T}^{2}$, and the $\\epsilon_{i}$ are i.i.d. with $E[\\epsilon_{i}] = 0$ and $\\text{Var}(\\epsilon_{i}) = \\sigma_{\\epsilon}^{2}$, and are independent of $T$.\n\nThe sample average over $n$ sensors is\n$$\n\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}.\n$$\nSubstituting the model into the average gives\n$$\n\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(T + \\epsilon_{i}\\right) = T + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_{i}.\n$$\nDefine the noise average\n$$\n\\bar{\\epsilon}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_{i}.\n$$\nThen\n$$\n\\bar{X}_{n} = T + \\bar{\\epsilon}_{n}.\n$$\n\nBy the Strong Law of Large Numbers (or the Weak Law, which suffices for convergence in probability), since the $\\epsilon_{i}$ are i.i.d. with finite mean $E[\\epsilon_{i}] = 0$, we have\n$$\n\\bar{\\epsilon}_{n} \\xrightarrow{p} 0 \\quad \\text{as } n \\to \\infty.\n$$\nTherefore, for any $\\delta > 0$,\n$$\nP\\left(\\left|\\bar{X}_{n} - T\\right| \\geq \\delta\\right) \n= P\\left(\\left|T + \\bar{\\epsilon}_{n} - T\\right| \\geq \\delta\\right) \n= P\\left(\\left|\\bar{\\epsilon}_{n}\\right| \\geq \\delta\\right) \\to 0 \\quad \\text{as } n \\to \\infty.\n$$\nThis directly proves that\n$$\n\\bar{X}_{n} \\xrightarrow{p} T.\n$$\nEquivalently, the limit in probability $L$ is the random variable $T$ (not the constant $\\mu$), because the common term $T$ does not average out across sensors whereas the independent noises do. For completeness, note that $E[\\bar{X}_{n}] = \\mu$ for all $n$, and $\\text{Var}(\\bar{X}_{n}) = \\sigma_{T}^{2} + \\sigma_{\\epsilon}^{2}/n \\to \\sigma_{T}^{2}$, consistent with convergence in probability to $T$ rather than to a constant.", "answer": "$$\\boxed{T}$$", "id": "1668567"}]}