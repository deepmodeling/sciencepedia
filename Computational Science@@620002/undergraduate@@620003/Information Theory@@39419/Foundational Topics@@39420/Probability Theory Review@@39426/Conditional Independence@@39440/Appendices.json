{"hands_on_practices": [{"introduction": "This first practice explores a scenario fundamental to communication and inference: a single source influencing multiple, separate observations. By modeling a signal $Z$ transmitted through two independent noisy channels to produce outputs $X$ and $Y$, we investigate how a common cause induces correlation. This exercise will help you quantify the mutual information between the two outputs, which are only dependent because they share a common origin, illustrating a key structure known as a \"common cause.\"", "problem": "Consider a simple communication system where a source's state $Z$ can be either 'LOW' (represented by the binary value 0) or 'HIGH' (represented by 1). The source selects a state with equal probability, i.e., $p(Z=0) = p(Z=1) = 0.5$. The state $Z$ is transmitted simultaneously to two physically separate and independent receivers, A and B.\n\nThe transmission channels to both receivers are noisy. The observation at receiver A is denoted by the binary variable $X$, and at receiver B by $Y$. For each receiver, there is a constant probability of a bit-flip error, $\\alpha = 1/4$. That is, the probability that a receiver records a state different from the one transmitted is $p(X \\neq Z) = \\alpha$ and $p(Y \\neq Z) = \\alpha$. The errors in the two channels are independent of each other, conditioned on the source state $Z$.\n\nCalculate the mutual information $I(X;Y)$ between the observations of the two receivers. Express your final answer as a closed-form analytic expression in bits.", "solution": "Let $Z \\in \\{0,1\\}$ with $p(Z=0)=p(Z=1)=\\frac{1}{2}$. The observations $X$ and $Y$ are produced by independent binary symmetric channels (BSC) with crossover probability $\\alpha=\\frac{1}{4}$, so $p(X \\neq Z \\mid Z)=\\alpha$ and $p(Y \\neq Z \\mid Z)=\\alpha$, and $X \\perp Y \\mid Z$.\n\nFirst, compute the marginals of $X$ and $Y$. For $X$,\n$$\np(X=1) = \\sum_{z \\in \\{0,1\\}} p(Z=z)\\,p(X=1 \\mid Z=z) = \\frac{1}{2}\\big[(1-\\alpha)+\\alpha\\big] = \\frac{1}{2},\n$$\nand similarly $p(X=0)=\\frac{1}{2}$, so $H(X)=1$ bit; likewise $H(Y)=1$ bit.\n\nNext, compute the joint distribution of $(X,Y)$. Using conditional independence given $Z$,\n$$\np(X=Y) = p(X=Y=Z) + p(X=Y=1-Z) = (1-\\alpha)^{2} + \\alpha^{2},\n$$\n$$\np(X \\neq Y) = 2\\alpha(1-\\alpha).\n$$\nDefine $r \\equiv p(X \\neq Y) = 2\\alpha(1-\\alpha)$ and $q \\equiv p(X=Y) = 1-r = (1-\\alpha)^{2} + \\alpha^{2}$. By symmetry, the four joint probabilities are\n$$\np(0,0) = p(1,1) = \\frac{q}{2}, \\quad p(0,1) = p(1,0) = \\frac{r}{2}.\n$$\n\nThe joint entropy is\n$$\nH(X,Y) = -2 \\cdot \\frac{q}{2} \\log_{2}\\!\\left(\\frac{q}{2}\\right) - 2 \\cdot \\frac{r}{2} \\log_{2}\\!\\left(\\frac{r}{2}\\right)\n= - q \\log_{2}\\!\\left(\\frac{q}{2}\\right) - r \\log_{2}\\!\\left(\\frac{r}{2}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\frac{q}{2}\\right) = \\log_{2} q - 1$ and $\\log_{2}\\!\\left(\\frac{r}{2}\\right) = \\log_{2} r - 1$, and $q+r=1$, this simplifies to\n$$\nH(X,Y) = 1 - \\big[q \\log_{2} q + r \\log_{2} r\\big].\n$$\nTherefore, the mutual information is\n$$\nI(X;Y) = H(X) + H(Y) - H(X,Y) = 2 - \\Big[1 - \\big(q \\log_{2} q + r \\log_{2} r\\big)\\Big]\n= 1 + q \\log_{2} q + r \\log_{2} r.\n$$\n\nSubstitute $\\alpha=\\frac{1}{4}$. Then\n$$\nr = 2\\alpha(1-\\alpha) = 2 \\cdot \\frac{1}{4} \\cdot \\frac{3}{4} = \\frac{3}{8}, \n\\quad\nq = 1 - r = \\frac{5}{8}.\n$$\nHence\n$$\nI(X;Y) = 1 + \\frac{5}{8} \\log_{2}\\!\\left(\\frac{5}{8}\\right) + \\frac{3}{8} \\log_{2}\\!\\left(\\frac{3}{8}\\right),\n$$\nwhich is an exact closed-form expression in bits. Equivalently, $I(X;Y) = 1 - h_{2}\\!\\left(\\frac{3}{8}\\right)$, where $h_{2}$ denotes the binary entropy function in bits.", "answer": "$$\\boxed{1+\\frac{5}{8}\\log_{2}\\left(\\frac{5}{8}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{8}\\right)}$$", "id": "1612658"}, {"introduction": "In contrast to the previous problem, this exercise demonstrates one of the most surprising effects in probability: how conditioning on a common effect can create dependence between independent causes. We consider two independent data streams, $X$ and $Y$, combined to produce a third variable $Z$. This \"collider\" or \"v-structure\" is crucial for understanding paradoxes in statistics and for building valid causal models, as it shows that observing a shared outcome can link otherwise unrelated events.", "problem": "In many modern data systems, information is processed in parallel streams. Consider a simplified model of such a system with two independent binary data streams, represented by random variables $X$ and $Y$. Both streams are designed to be statistically unbiased, meaning the probability of a bit being 0 or 1 is equal. Thus, $P(X=0) = P(X=1) = 1/2$, and $P(Y=0) = P(Y=1) = 1/2$. A monitoring process does not observe $X$ and $Y$ directly but instead observes a third binary variable $Z$, which is generated by taking the exclusive OR (XOR) of the corresponding bits from the two streams: $Z = X \\oplus Y$.\n\nAn engineer is analyzing the information flow in this system. A key question is how much information the two initial streams share, conditioned on the observed output. Specifically, calculate the conditional mutual information $I(X;Y \\mid Z)$ between the two streams $X$ and $Y$, given the observation of the combined stream $Z$.\n\nExpress your answer in bits.", "solution": "We are given two independent unbiased binary random variables $X$ and $Y$ with $P(X=0)=P(X=1)=\\frac{1}{2}$ and $P(Y=0)=P(Y=1)=\\frac{1}{2}$, and $Z=X\\oplus Y$. We are to compute the conditional mutual information $I(X;Y\\mid Z)$ in bits.\n\nBy definition,\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z).\n$$\nFirst compute $H(X\\mid Z)$. We have\n$$\nP(Z=0)=P(X=0,Y=0)+P(X=1,Y=1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\n$$\n$$\nP(Z=1)=P(X=0,Y=1)+P(X=1,Y=0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nFor $z=0$,\n$$\nP(X=0\\mid Z=0)=\\frac{P(X=0,Z=0)}{P(Z=0)}=\\frac{P(X=0,Y=0)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\nand similarly $P(X=1\\mid Z=0)=\\frac{1}{2}$. Thus $H(X\\mid Z=0)=1$ bit.\n\nFor $z=1$,\n$$\nP(X=0\\mid Z=1)=\\frac{P(X=0,Z=1)}{P(Z=1)}=\\frac{P(X=0,Y=1)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\nand similarly $P(X=1\\mid Z=1)=\\frac{1}{2}$. Thus $H(X\\mid Z=1)=1$ bit.\n\nTherefore,\n$$\nH(X\\mid Z)=\\sum_{z\\in\\{0,1\\}}P(Z=z)\\,H(X\\mid Z=z)=\\frac{1}{2}\\cdot 1+\\frac{1}{2}\\cdot 1=1.\n$$\n\nNext compute $H(X\\mid Y,Z)$. Since $Z=X\\oplus Y$, the value of $X$ is determined by $Y$ and $Z$ via $X=Y\\oplus Z$. Hence\n$$\nH(X\\mid Y,Z)=0.\n$$\n\nPutting these together,\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z)=1-0=1.\n$$\nThus the conditional mutual information is $1$ bit.", "answer": "$$\\boxed{1}$$", "id": "1612630"}, {"introduction": "Moving beyond simple cause-and-effect diagrams, this final problem challenges you to apply the definition of conditional independence in a more abstract setting. You'll investigate whether two independent events, $X$ and $Y$, remain independent after being conditioned on a more complex, systemic propertyâ€”the parity of their sum with a third independent event $Z$. This practice deepens your understanding by revealing that conditional independence can be a subtle property, sometimes holding only under very specific conditions, and requires a rigorous application of its mathematical foundations.", "problem": "Consider three random variables $X, Y, and Z$ that represent the outcomes of three independent and identically distributed Bernoulli trials. The probability of success (i.e., the variable taking the value 1) for each trial is $p$, where $P(\\text{trial}=1) = p$, and the probability of failure (value 0) is $P(\\text{trial}=0) = 1-p$. The parameter $p$ is restricted to the open interval $(0, 1)$.\n\nLet $E$ be the event that the sum of the outcomes is an even number, i.e., $E$ is the event where $X+Y+Z$ is an even integer.\n\nUnder what conditions are the random variables $X$ and $Y$ conditionally independent given the event $E$?\n\nA. $X$ and $Y$ are always conditionally independent given $E$, for any value of $p \\in (0,1)$.\n\nB. $X$ and $Y$ are never conditionally independent given $E$, for any value of $p \\in (0,1)$.\n\nC. $X$ and $Y$ are conditionally independent given $E$ if and only if $p = 1/2$.\n\nD. $X$ and $Y$ are conditionally independent given $E$ if and only if $p$ is a rational number.\n\nE. The conditional independence of $X$ and $Y$ given $E$ cannot be determined without knowing the specific outcome of $Z$.", "solution": "Let $X,Y,Z$ be independent $\\operatorname{Bernoulli}(p)$ with $p \\in (0,1)$. Define $E=\\{X+Y+Z \\text{ is even}\\}$. The event $E$ occurs when either exactly $0$ or exactly $2$ of the variables are $1$, so\n$$\nP(E)=P(X=0,Y=0,Z=0)+P(\\text{exactly two of }X,Y,Z\\text{ are }1)\n=(1-p)^{3}+3p^{2}(1-p).\n$$\nEquivalently,\n$$\nP(E)=(1-p)\\big((1-p)^{2}+3p^{2}\\big).\n$$\n\nCompute the joint probabilities conditioned on $E$:\n- If $X=1$ and $Y=1$, then $Z$ must be $0$ for $E$ to occur, so\n$$\nP(X=1,Y=1,E)=P(X=1,Y=1,Z=0)=p^{2}(1-p).\n$$\n- If $X=1$ and $Y=0$, then $Z$ must be $1$, so\n$$\nP(X=1,Y=0,E)=P(X=1,Y=0,Z=1)=p(1-p)p=p^{2}(1-p).\n$$\n- By symmetry,\n$$\nP(X=0,Y=1,E)=p^{2}(1-p), \\quad P(X=0,Y=0,E)=(1-p)^{3}.\n$$\nTherefore,\n$$\nP(X=1,Y=1 \\mid E)=\\frac{p^{2}(1-p)}{P(E)}, \\quad\nP(X=1,Y=0 \\mid E)=\\frac{p^{2}(1-p)}{P(E)},\n$$\n$$\nP(X=0,Y=1 \\mid E)=\\frac{p^{2}(1-p)}{P(E)}, \\quad\nP(X=0,Y=0 \\mid E)=\\frac{(1-p)^{3}}{P(E)}.\n$$\n\nThe marginals under $E$ are\n$$\nP(X=1 \\mid E)=P(X=1,Y=1 \\mid E)+P(X=1,Y=0 \\mid E)=\\frac{2p^{2}(1-p)}{P(E)},\n$$\nand by symmetry $P(Y=1 \\mid E)=\\frac{2p^{2}(1-p)}{P(E)}$.\n\nFor conditional independence, we require for example\n$$\nP(X=1,Y=1 \\mid E)=P(X=1 \\mid E)\\,P(Y=1 \\mid E).\n$$\nSubstituting the expressions above gives\n$$\n\\frac{p^{2}(1-p)}{P(E)}=\\left(\\frac{2p^{2}(1-p)}{P(E)}\\right)^{2}.\n$$\nMultiplying both sides by $P(E)^{2}$ and noting $p \\in (0,1)$ so we can divide by $p^{2}(1-p)>0$, we obtain\n$$\nP(E)=4p^{2}(1-p).\n$$\nUsing $P(E)=(1-p)\\big((1-p)^{2}+3p^{2}\\big)$ and dividing by $(1-p)>0$ yields\n$$\n(1-p)^{2}+3p^{2}=4p^{2} \\quad \\Longrightarrow \\quad 1-2p+4p^{2}=4p^{2} \\quad \\Longrightarrow \\quad 1-2p=0,\n$$\nso\n$$\np=\\frac{1}{2}.\n$$\n\nSufficiency holds: when $p=\\frac{1}{2}$, the four joint conditional probabilities above are equal, each being $\\frac{1}{4}$, and the marginals are $\\frac{1}{2}$, so $P(X=a,Y=b \\mid E)=P(X=a \\mid E)P(Y=b \\mid E)$ for all $a,b \\in \\{0,1\\}$.\n\nTherefore, $X$ and $Y$ are conditionally independent given $E$ if and only if $p=\\frac{1}{2}$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1612639"}]}