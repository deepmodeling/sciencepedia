{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) provides a powerful link between theoretical probability and empirical observation. This first exercise demonstrates the law in its most direct form, showing how the long-term average of repeated, independent measurements converges to a predictable constant. By working through this problem, you will practice the fundamental skill of calculating the expected value from a probability density function and see how it dictates the eventual outcome of a random process [@problem_id:1460774].", "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2 & \\text{for } 0 \\le x \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.", "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2}, & 0\\le x\\le 1,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]<\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1460774"}, {"introduction": "In many scientific and engineering applications, we analyze not the raw data, but a function of it. This practice explores how the SLLN applies to such transformed random variables, specifically by investigating the sample average of $Y_i = \\sqrt{X_i}$. The key insight here is that the law still holds for the transformed sequence, provided it has a finite mean—a condition you will learn to verify. This exercise deepens your understanding of the SLLN's requirements and its versatility in data analysis [@problem_id:1957052].", "problem": "A research team is studying the energy emissions from a newly discovered type of microscopic organism. The energy emitted by a single organism during a short observation period is a random variable. The team conducts a series of independent experiments, with each experiment measuring the energy emission, $X_i$, from a new, randomly selected organism. These measurements, $X_1, X_2, X_3, \\dots$, can be modeled as a sequence of independent and identically distributed (i.i.d.) positive random variables. It is known from theoretical models that the expected energy emission is finite, i.e., $E[X_i] = \\mu$ for some finite constant $\\mu > 0$.\n\nFor a data processing task, an analyst is not interested in the energy itself, but in a related quantity, $Y_i = \\sqrt{X_i}$. The analyst computes the sample average of these transformed quantities over $n$ experiments:\n$$S_n = \\frac{1}{n}\\sum_{i=1}^n Y_i = \\frac{1}{n}\\sum_{i=1}^n \\sqrt{X_i}$$\nAs the number of experiments $n$ grows infinitely large, what value does the sample average $S_n$ converge to almost surely? Express your answer in terms of the random variable $X_1$.", "solution": "The problem asks for the almost sure limit of the sample average $S_n = \\frac{1}{n}\\sum_{i=1}^n \\sqrt{X_i}$ as $n \\to \\infty$.\n\nLet's define a new sequence of random variables $Y_i = \\sqrt{X_i}$ for $i=1, 2, 3, \\dots$. Since the random variables $X_1, X_2, \\dots$ are independent and identically distributed (i.i.d.), the random variables $Y_1 = \\sqrt{X_1}, Y_2 = \\sqrt{X_2}, \\dots$ are also independent and identically distributed. This is because any function of i.i.d. random variables results in a new sequence of i.i.d. random variables.\n\nThe sample average can be rewritten in terms of the $Y_i$ variables:\n$$S_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$$\nThis is the sample mean of the i.i.d. sequence $Y_i$.\n\nWe can apply the Strong Law of Large Numbers (SLLN) to the sequence $\\{Y_i\\}$. The SLLN states that if $Y_1, Y_2, \\dots$ is a sequence of i.i.d. random variables with a finite expected value, i.e., $E[|Y_1|] < \\infty$, then the sample mean converges almost surely to the true mean:\n$$\\frac{1}{n}\\sum_{i=1}^n Y_i \\xrightarrow{\\text{a.s.}} E[Y_1] \\quad \\text{as } n \\to \\infty$$\n\nTo use the SLLN, we must verify that $E[|Y_1|]$ is finite.\nWe have $Y_1 = \\sqrt{X_1}$. The problem states that the $X_i$ are positive random variables, so $X_1 > 0$. This implies that $\\sqrt{X_1}$ is a real and positive value, so $|Y_1| = |\\sqrt{X_1}| = \\sqrt{X_1}$. We need to check if $E[\\sqrt{X_1}]$ is finite.\n\nWe can use Jensen's inequality to relate $E[\\sqrt{X_1}]$ to the known finite mean $E[X_1] = \\mu$. Jensen's inequality for a concave function $g(x)$ and a random variable $X$ states that $E[g(X)] \\le g(E[X])$.\n\nThe function $g(x) = \\sqrt{x}$ is a concave function for $x > 0$. This can be shown by checking its second derivative:\n$g'(x) = \\frac{1}{2}x^{-1/2}$\n$g''(x) = -\\frac{1}{4}x^{-3/2}$\nSince $x > 0$, we have $g''(x) < 0$, which confirms that $g(x) = \\sqrt{x}$ is strictly concave.\n\nApplying Jensen's inequality with $g(x) = \\sqrt{x}$ and the random variable $X_1$, we get:\n$$E[\\sqrt{X_1}] \\le \\sqrt{E[X_1]}$$\nThe problem states that $E[X_1] = \\mu$, and $\\mu$ is a finite positive constant. Therefore,\n$$E[\\sqrt{X_1}] \\le \\sqrt{\\mu}$$\nSince $\\mu$ is finite, $\\sqrt{\\mu}$ is also finite. This shows that $E[\\sqrt{X_1}]$ is finite.\nBecause $Y_1 = \\sqrt{X_1}$ is a positive random variable, we have $E[|Y_1|] = E[Y_1] = E[\\sqrt{X_1}]$, which is finite.\n\nThe condition for the SLLN is satisfied for the sequence $\\{Y_i\\}$. Therefore, we can conclude that the sample average $S_n$ converges almost surely to the expectation of $Y_1$:\n$$\\lim_{n \\to \\infty} S_n = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^n Y_i \\xrightarrow{\\text{a.s.}} E[Y_1]$$\nSubstituting back $Y_1 = \\sqrt{X_1}$, we find the limit:\n$$\\lim_{n \\to \\infty} S_n \\xrightarrow{\\text{a.s.}} E[\\sqrt{X_1}]$$\nThe question asks for the value to which $S_n$ converges almost surely. This value is $E[\\sqrt{X_1}]$.", "answer": "$$\\boxed{E[\\sqrt{X_1}]}$$", "id": "1957052"}, {"introduction": "The classic SLLN is built on the assumption of independent and identically distributed (i.i.d.) random variables, but what happens when data points are not independent? This problem challenges you to analyze a sequence where each term is a product of consecutive variables, creating a local dependency. You will learn a clever technique to bypass this complication by decomposing the process, demonstrating how the core principles of the SLLN can be applied even in more complex, non-i.i.d. scenarios [@problem_id:1460782].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on a common probability space. These variables are characterized by a mean of $E[X_n] = 0$ and a second moment of $E[X_n^2] = 1$ for all $n \\ge 1$.\n\nWe form a new sequence of sample averages defined as $A_n = \\frac{1}{n} \\sum_{i=1}^{n-1} X_i X_{i+1}$ for $n \\ge 2$.\n\nDetermine the value to which the sequence $A_n$ converges almost surely as $n$ approaches infinity.", "solution": "Define $Y_{i} = X_{i}X_{i+1}$ for $i \\ge 1$. Then $A_{n} = \\frac{1}{n} \\sum_{i=1}^{n-1} Y_{i}$ for $n \\ge 2$. By independence of the $X_{i}$ and identical distribution, each $Y_{i}$ has the same distribution, with\n$$\nE[Y_{i}] = E[X_{i}X_{i+1}] = E[X_{i}]\\,E[X_{i+1}] = 0,\n$$\nand\n$$\nE[Y_{i}^{2}] = E[X_{i}^{2}X_{i+1}^{2}] = E[X_{i}^{2}]\\,E[X_{i+1}^{2}] = 1.\n$$\nHence $E[|Y_{i}|] \\le \\sqrt{E[Y_{i}^{2}]} = 1$ by Cauchy–Schwarz, so $Y_{i}$ are integrable.\n\nNext, observe the dependence structure: the family $\\{Y_{i}\\}_{i \\ge 1}$ is $1$-dependent, and more specifically the subsequences $\\{Y_{2j-1}\\}_{j \\ge 1}$ and $\\{Y_{2j}\\}_{j \\ge 1}$ consist of independent random variables because they depend on disjoint sets of the independent $X_{i}$. Moreover, each subsequence is identically distributed with mean $0$ and finite second moment. By the Kolmogorov strong law of large numbers applied separately to each subsequence,\n$$\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j-1} \\to 0 \\quad \\text{a.s.}, \n\\qquad\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j} \\to 0 \\quad \\text{a.s.}\n$$\n\nFor each $n \\ge 2$, let $N_{o}(n)$ be the number of odd indices in $\\{1,\\dots,n-1\\}$ and $N_{e}(n)$ the number of even indices in $\\{1,\\dots,n-1\\}$. Then $N_{o}(n) + N_{e}(n) = n-1$, with $N_{o}(n) = \\lceil (n-1)/2 \\rceil$ and $N_{e}(n) = \\lfloor (n-1)/2 \\rfloor$, so $N_{o}(n)/n \\to \\frac{1}{2}$ and $N_{e}(n)/n \\to \\frac{1}{2}$ deterministically as $n \\to \\infty$. Decompose\n$$\nA_{n} \n= \\frac{1}{n} \\sum_{i=1}^{n-1} Y_{i}\n= \\frac{N_{o}(n)}{n} \\left( \\frac{1}{N_{o}(n)} \\sum_{j=1}^{N_{o}(n)} Y_{2j-1} \\right)\n+ \\frac{N_{e}(n)}{n} \\left( \\frac{1}{N_{e}(n)} \\sum_{j=1}^{N_{e}(n)} Y_{2j} \\right).\n$$\nAs $n \\to \\infty$, we have $N_{o}(n) \\to \\infty$ and $N_{e}(n) \\to \\infty$, so by the almost sure limits of the subsequence averages and the deterministic limits of the weights, it follows that $A_{n} \\to 0$ almost surely.\n\nTherefore, the sequence $A_{n}$ converges almost surely to $0$.", "answer": "$$\\boxed{0}$$", "id": "1460782"}]}