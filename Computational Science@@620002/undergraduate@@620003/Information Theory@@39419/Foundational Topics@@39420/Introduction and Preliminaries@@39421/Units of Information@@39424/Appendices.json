{"hands_on_practices": [{"introduction": "Before we begin calculating with information, it's crucial to understand its fundamental properties. The Shannon information, or self-information, $I(p) = -\\log_b(p)$, is not just a mathematical formula; it represents a physical concept. This exercise [@problem_id:1666609] challenges you to think critically about the relationship between probability and information, specifically why observing an event can only lead to a gain in information (or zero gain), never a loss.", "problem": "A novice researcher, working on a preliminary model for a complex biological system, makes a calculation error. Their model erroneously predicts the probability of a specific protein folding into a functional state as $p_{\\text{fold}} = 1.6$. Undeterred by this anomalous value, the researcher attempts to calculate the Shannon information, also known as self-information, associated with this event. They use the standard formula $I(p) = -\\log_2(p)$, where the information is measured in bits.\n\nWhich of the following statements best describes the fundamental conceptual flaw in this calculation, beyond the initial error of obtaining a probability greater than one?\n\nA. The calculation is impossible because the logarithm of a number greater than 1 is undefined.\n\nB. The resulting information content would be positive, which is physically meaningless as information must be a non-positive quantity.\n\nC. The calculation would yield a negative value for information, which is conceptually invalid because observing an event, regardless of its probability, cannot increase uncertainty.\n\nD. The formula $I(p) = -\\log_2(p)$ is only valid for probabilities less than 0.5; for probabilities greater than 0.5, the correct formula is $I(p) = -\\log_2(1-p)$.\n\nE. The information content would be a complex number, which has no physical interpretation in standard information theory.", "solution": "Start from the standard definition of self-information (Shannon information) for a single event with probability $p$:\n$$\nI(p) = -\\log_{2}(p).\n$$\nThis expression is defined and interpreted for valid probabilities $p$ satisfying $0 < p \\leq 1$. Over this domain, $\\log_{2}(p) \\leq 0$, hence\n$$\nI(p) = -\\log_{2}(p) \\geq 0,\n$$\nwith $I(1) = 0$ and $\\lim_{p \\to 0^{+}} I(p) = +\\infty$. Thus, self-information (surprisal) is nonnegative.\n\nIf one erroneously substitutes an invalid probability $p = 1.6$ (note $1.6 > 1$), then\n$$\n\\log_{2}(1.6) > 0 \\quad \\Rightarrow \\quad I(1.6) = -\\log_{2}(1.6)  0.\n$$\nA negative value of $I$ is conceptually invalid in Shannon’s framework, because self-information quantifies the “surprise” or information gained upon observing an event and is inherently nonnegative. Negative self-information would suggest that observing the event increases uncertainty (i.e., yields a negative information gain), which contradicts the basic interpretation of self-information.\n\nEvaluate the options:\n- A is false because $\\log_{2}(p)$ is defined for $p > 0$, including $p > 1$.\n- B is false because for $p = 1.6$, $I(p)  0$, not positive; moreover, information is nonnegative for valid $p$.\n- C correctly identifies that the computation would yield a negative information value, which is conceptually invalid since observing an event cannot increase uncertainty.\n- D is false; $I(p) = -\\log_{2}(p)$ is valid for all $p$ in $(0,1]$; there is no switch to $-\\log_{2}(1-p)$ for $p > 0.5$.\n- E is false because $\\log_{2}(1.6)$ is real, not complex.\n\nTherefore, the best description of the conceptual flaw is that the calculation produces a negative self-information value, which is not meaningful in standard information theory.", "answer": "$$\\boxed{C}$$", "id": "1666609"}, {"introduction": "Different scientific and engineering fields have adopted different logarithmic bases to quantify information, leading to units like bits (base 2), nats (base $e$), and hartleys (base 10). Being able to translate between these \"languages\" is a fundamental skill for any information scientist. This next practice problem [@problem_id:1666613] provides a direct application of the change-of-base formula to convert a measurement from a decimal-based system (hartleys) to the standard binary system (bits).", "problem": "In the field of information theory, the amount of information, $I$, gained from observing an event with probability $p$ is defined as $I = -\\log_{b}(p)$. The choice of the base of the logarithm, $b$, determines the unit of information. The standard unit is the 'bit', which corresponds to a base of $b=2$.\n\nAn interplanetary probe is analyzing signals from a newly discovered celestial phenomenon. The probe's internal systems, designed with a decimal-based architecture, measure information content using the 'hartley' unit, which corresponds to a base of $b=10$. After a period of observation, the probe calculates that the mutual information between two correlated signal patterns is $2.5$ hartleys. For this measurement to be compared with standard scientific data on Earth, it must be converted into bits.\n\nCalculate the value of this mutual information in bits. Express your final answer in bits, rounded to three significant figures.", "solution": "The information content with base $b$ is $I_{b}(p) = -\\log_{b}(p)$. For the same event probability $p$, the values in bases $10$ and $2$ are related by the change-of-base formula:\n$$\n\\log_{2}(p) = \\frac{\\log_{10}(p)}{\\log_{10}(2)} \\quad \\Rightarrow \\quad -\\log_{2}(p) = -\\frac{\\log_{10}(p)}{\\log_{10}(2)}.\n$$\nHence, if $I_{10} = -\\log_{10}(p)$ is the information in hartleys and $I_{2} = -\\log_{2}(p)$ is the information in bits, then\n$$\nI_{2} = \\frac{I_{10}}{\\log_{10}(2)} = I_{10}\\,\\log_{2}(10) = I_{10}\\,\\frac{\\ln 10}{\\ln 2}.\n$$\nGiven $I_{10} = 2.5$, we compute\n$$\nI_{2} = 2.5\\,\\frac{\\ln 10}{\\ln 2}.\n$$\nUsing numerical values to obtain the requested three-significant-figure approximation,\n$$\n\\ln 10 \\approx 2.302585093, \\quad \\ln 2 \\approx 0.693147180 \\quad \\Rightarrow \\quad \\frac{\\ln 10}{\\ln 2} \\approx 3.321928095,\n$$\nso\n$$\nI_{2} \\approx 2.5 \\times 3.321928095 \\approx 8.304820237 \\approx 8.30 \\text{ (to three significant figures)}.\n$$", "answer": "$$\\boxed{8.30}$$", "id": "1666613"}, {"introduction": "In many practical scenarios, such as analyzing data from a satellite with multiple instruments, we must synthesize information from different, independent sources. This task becomes more complex when each source reports its data using a different unit of information. This problem [@problem_id:1666590] simulates just such a scenario, requiring you to combine the principle of entropy additivity for independent events with unit conversion skills across bits, nats, and hartleys to arrive at a single, unified measurement.", "problem": "A deep-space probe is equipped with two independent scientific instruments for analyzing the interstellar medium. The first instrument, a plasma wave detector, takes a measurement whose information content, or Shannon entropy, is quantified by its primary processor as $H_P = 2.5$ nats. The unit 'nat' signifies that the entropy calculation uses the natural logarithm (base $e$). The second instrument, a magnetometer, records the local magnetic field. The entropy of its measurement is calculated by a legacy subsystem to be $H_M = 4.0$ bits. The unit 'bit' signifies a calculation using the base-2 logarithm.\n\nA mission scientist needs to combine these data points for a unified report. The reporting standard for this mission requires all entropy values to be expressed in hartleys, a unit based on the base-10 logarithm. Assuming the measurements from the plasma wave detector and the magnetometer are statistically independent, what is the total joint entropy of a combined observation from both instruments?\n\nExpress your final answer in hartleys, rounded to four significant figures.", "solution": "We are given two entropy values expressed in different logarithmic bases: $H_{P}=2.5$ nats (natural logarithm) and $H_{M}=4.0$ bits (base-2 logarithm). The joint entropy of independent measurements adds, so in any common base,\n$$\nH_{\\text{joint}}=H_{P}+H_{M},\n$$\nprovided both terms are expressed in the same units. The reporting unit is hartleys (base-10 logarithm), so we convert each entropy to hartleys and then add.\n\nFor any base $b$, $\\log_{b}(x)=\\frac{\\ln x}{\\ln b}$. Therefore:\n- Converting nats to hartleys: if $H^{(\\mathrm{nat})}$ is in nats, then\n$$\nH^{(\\mathrm{hart})}=\\frac{H^{(\\mathrm{nat})}}{\\ln 10}.\n$$\nThus,\n$$\nH_{P}^{(\\mathrm{hart})}=\\frac{2.5}{\\ln 10}.\n$$\n- Converting bits to hartleys: if $H^{(\\mathrm{bit})}$ is in bits, then\n$$\nH^{(\\mathrm{hart})}=H^{(\\mathrm{bit})}\\log_{10}(2)=H^{(\\mathrm{bit})}\\frac{\\ln 2}{\\ln 10}.\n$$\nThus,\n$$\nH_{M}^{(\\mathrm{hart})}=4.0\\,\\frac{\\ln 2}{\\ln 10}.\n$$\n\nBy independence, the joint entropy in hartleys is\n$$\nH_{\\text{joint}}^{(\\mathrm{hart})}=\\frac{2.5}{\\ln 10}+4.0\\,\\frac{\\ln 2}{\\ln 10}.\n$$\nNumerically, using $\\ln 10\\approx 2.302585093$ and $\\ln 2\\approx 0.6931471806$,\n$$\n\\frac{2.5}{\\ln 10}\\approx 1.085736205,\\quad 4.0\\,\\frac{\\ln 2}{\\ln 10}=4.0\\,\\log_{10}(2)\\approx 1.204119983,\n$$\nso\n$$\nH_{\\text{joint}}^{(\\mathrm{hart})}\\approx 2.289856187.\n$$\nRounded to four significant figures, this is $2.290$ hartleys.", "answer": "$$\\boxed{2.290}$$", "id": "1666590"}]}