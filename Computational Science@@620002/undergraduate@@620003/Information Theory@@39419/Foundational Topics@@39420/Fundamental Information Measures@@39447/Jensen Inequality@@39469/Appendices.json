{"hands_on_practices": [{"introduction": "Theory comes alive through practice. Let's begin by verifying Jensen's inequality with a direct calculation. This exercise explores the relationship between the average of a function's output and the function of an average input, a common theme in physics and engineering. By calculating and comparing the true average power, which depends on $E[X^4]$, with a hypothetical power based on the average signal strength, $(E[X])^4$, we gain a concrete intuition for why $E[g(X)] \\ge g(E[X])$ holds for the convex function $g(x)=x^4$ [@problem_id:1926117].", "problem": "A certain electronic component in a communication system experiences a fluctuating input signal strength, which we model as a random variable $X$. The signal strength $X$ is uniformly distributed over the interval $[0, 1]$ in normalized units. The instantaneous power consumption of the component, denoted by $P$, is found to be directly proportional to the fourth power of the signal strength, such that $P = \\alpha X^4$, where $\\alpha$ is a positive constant representing the component's characteristics.\n\nAn engineer is interested in comparing two different measures of power consumption. The first is the true average power consumption over time, which is given by the expected value of $P$, denoted as $E[P]$. The second is a simplified, hypothetical power consumption, $P_{\\text{avg}}$, which would occur if the component were subjected to a constant signal strength equal to the average signal strength, $E[X]$. This hypothetical power is thus given by $P_{\\text{avg}} = \\alpha (E[X])^4$.\n\nCalculate the specific numerical value of the ratio $\\frac{E[P]}{P_{\\text{avg}}}$. Provide your answer as a decimal value rounded to two significant figures.", "solution": "We model $X$ as uniformly distributed on $[0,1]$, so its probability density function is $f_{X}(x)=1$ for $x\\in[0,1]$ and $0$ otherwise. The instantaneous power is $P=\\alpha X^{4}$ with $\\alpha0$.\n\nUsing linearity of expectation,\n$$\nE[P]=E[\\alpha X^{4}]=\\alpha E[X^{4}].\n$$\nFor $X\\sim\\text{Uniform}(0,1)$,\n$$\nE[X^{4}]=\\int_{0}^{1}x^{4}f_{X}(x)\\,dx=\\int_{0}^{1}x^{4}\\,dx=\\left.\\frac{x^{5}}{5}\\right|_{0}^{1}=\\frac{1}{5},\n$$\nso\n$$\nE[P]=\\frac{\\alpha}{5}.\n$$\nThe average signal strength is\n$$\nE[X]=\\int_{0}^{1}x\\,dx=\\left.\\frac{x^{2}}{2}\\right|_{0}^{1}=\\frac{1}{2}.\n$$\nThus the hypothetical power is\n$$\nP_{\\text{avg}}=\\alpha\\left(E[X]\\right)^{4}=\\alpha\\left(\\frac{1}{2}\\right)^{4}=\\frac{\\alpha}{16}.\n$$\nTherefore, the ratio is\n$$\n\\frac{E[P]}{P_{\\text{avg}}}=\\frac{\\alpha/5}{\\alpha/16}=\\frac{16}{5}=3.2.\n$$\nRounded to two significant figures, the value remains $3.2$.", "answer": "$$\\boxed{3.2}$$", "id": "1926117"}, {"introduction": "Jensen's inequality is not just an abstract mathematical curiosity; it has profound implications in statistics. This practice problem demonstrates its power by explaining a subtle but crucial concept: the bias of statistical estimators [@problem_id:1926161]. We will investigate why the sample standard deviation $S$ systematically underestimates the true population standard deviation $\\sigma$. This is a direct consequence of applying Jensen's inequality to the concave function $g(x) = \\sqrt{x}$, for which $E[\\sqrt{Y}] \\le \\sqrt{E[Y]}$.", "problem": "Consider a small, finite population consisting of just two numbers: $\\{0, L\\}$, where $L$ is a positive constant. From this population, we draw a random sample of size $n=2$ with replacement. Let the two observations in the sample be denoted by $X_1$ and $X_2$.\n\nThe sample mean is given by $\\bar{X} = \\frac{X_1 + X_2}{2}$. The unbiased sample variance is calculated using the formula $S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$. The sample standard deviation is defined as $S = \\sqrt{S^2}$.\n\nThe population from which the samples are drawn has a true standard deviation, denoted by $\\sigma$.\n\nYour task is to calculate the exact value of the ratio of the expected value of the sample standard deviation, $E[S]$, to the true population standard deviation, $\\sigma$. Express your answer as a single closed-form analytic expression.", "solution": "The population is the two-point set $\\{0, L\\}$ sampled uniformly with replacement, so $P(X=0)=P(X=L)=\\frac{1}{2}$. The population mean is\n$$\n\\mu=\\frac{0+L}{2}=\\frac{L}{2},\n$$\nand the population variance is\n$$\n\\sigma^{2}=E[(X-\\mu)^{2}]=\\frac{1}{2}\\left(\\left(0-\\frac{L}{2}\\right)^{2}+\\left(L-\\frac{L}{2}\\right)^{2}\\right)=\\frac{L^{2}}{4},\n$$\nso the population standard deviation is\n$$\n\\sigma=\\frac{L}{2}.\n$$\nFor a sample of size $n=2$, the unbiased sample variance is\n$$\nS^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}=\\sum_{i=1}^{2}(X_{i}-\\bar{X})^{2},\n$$\nwith $\\bar{X}=\\frac{X_{1}+X_{2}}{2}$. Using $X_{1}-\\bar{X}=\\frac{X_{1}-X_{2}}{2}$ and $X_{2}-\\bar{X}=-\\frac{X_{1}-X_{2}}{2}$, we obtain\n$$\nS^{2}=\\left(\\frac{X_{1}-X_{2}}{2}\\right)^{2}+\\left(-\\frac{X_{1}-X_{2}}{2}\\right)^{2}=\\frac{(X_{1}-X_{2})^{2}}{2},\n$$\nso\n$$\nS=\\sqrt{S^{2}}=\\frac{|X_{1}-X_{2}|}{\\sqrt{2}}.\n$$\nEnumerating the four equally likely sample pairs $(0,0)$, $(0,L)$, $(L,0)$, $(L,L)$, we have $S=0$ when the two draws are equal and $S=\\frac{L}{\\sqrt{2}}$ when they differ. Hence\n$$\nE[S]=P(X_{1}\\neq X_{2})\\cdot\\frac{L}{\\sqrt{2}}=\\left(2\\cdot\\frac{1}{2}\\cdot\\frac{1}{2}\\right)\\frac{L}{\\sqrt{2}}=\\frac{L}{2\\sqrt{2}}.\n$$\nTherefore, the desired ratio is\n$$\n\\frac{E[S]}{\\sigma}=\\frac{\\frac{L}{2\\sqrt{2}}}{\\frac{L}{2}}=\\frac{1}{\\sqrt{2}}.\n$$\nThis expression is independent of $L$ and is the exact closed-form value.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$", "id": "1926161"}, {"introduction": "Now, let's apply Jensen's inequality to a cornerstone concept in information theory. The Data Processing Inequality, which states that no amount of local data processing can increase information content, is a direct consequence of this powerful mathematical tool [@problem_id:1633912]. In this exercise, you will calculate the Kullback-Leibler (KL) divergence—a measure of difference between probability distributions—before and after a data-coarsening step. This provides a quantitative confirmation that information, in this context, can be lost but never created through processing.", "problem": "In information theory, data processing can alter the statistical properties of a signal. Consider a discrete random variable $X$ that can take one of four possible values from the alphabet $\\mathcal{X} = \\{x_1, x_2, x_3, x_4\\}$. The behavior of this variable is described by a probability distribution $P_X$. We wish to compare $P_X$ to a reference distribution $Q_X$ over the same alphabet.\n\nThe given distributions are:\n$P_X = (P_X(x_1), P_X(x_2), P_X(x_3), P_X(x_4)) = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{8}\\right)$\n$Q_X = (Q_X(x_1), Q_X(x_2), Q_X(x_3), Q_X(x_4)) = \\left(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}\\right)$\n\nThe random variable $X$ undergoes a processing step, which is described by a function $g: \\mathcal{X} \\to \\mathcal{Y}$ that maps the original outcomes to a new, smaller alphabet $\\mathcal{Y} = \\{y_A, y_B\\}$. This creates a new \"coarse-grained\" random variable $Y = g(X)$. The mapping is defined as follows:\n$g(x_1) = y_A$\n$g(x_2) = y_A$\n$g(x_3) = y_B$\n$g(x_4) = y_B$\n\nThis processing induces new probability distributions $P_Y$ and $Q_Y$ on the alphabet $\\mathcal{Y}$. The Kullback-Leibler (KL) divergence, or relative entropy, between two discrete probability distributions $P$ and $Q$ over the same alphabet $\\mathcal{Z}$ is defined as $D_{KL}(P || Q) = \\sum_{z \\in \\mathcal{Z}} P(z) \\log_2 \\frac{P(z)}{Q(z)}$, and its value is measured in bits.\n\nCalculate the exact value of the quantity $D_{KL}(P_X || Q_X) - D_{KL}(P_Y || Q_Y)$. Present your final answer as a single closed-form analytic expression in units of bits.", "solution": "We use the definition of KL divergence for discrete distributions over a common alphabet $\\mathcal{Z}$:\n$$\nD_{KL}(P\\Vert Q)=\\sum_{z\\in\\mathcal{Z}} P(z)\\,\\log_{2}\\!\\left(\\frac{P(z)}{Q(z)}\\right).\n$$\nFirst compute $D_{KL}(P_{X}\\Vert Q_{X})$ for $P_{X}=\\left(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{8}\\right)$ and $Q_{X}=\\left(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}\\right)$:\n$$\n\\begin{aligned}\nD_{KL}(P_{X}\\Vert Q_{X})\n=\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\frac{1}{2}}{\\frac{1}{4}}\\right)\n+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{\\frac{1}{4}}{\\frac{1}{4}}\\right)\n+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{\\frac{1}{8}}{\\frac{1}{4}}\\right)\n+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{\\frac{1}{8}}{\\frac{1}{4}}\\right)\\\\\n=\\frac{1}{2}\\log_{2}(2)+\\frac{1}{4}\\log_{2}(1)+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{2}\\right)+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{2}\\right)\\\\\n=\\frac{1}{2}\\cdot 1+ \\frac{1}{4}\\cdot 0+\\frac{1}{8}\\cdot(-1)+\\frac{1}{8}\\cdot(-1)=\\frac{1}{4}.\n\\end{aligned}\n$$\nNext compute the induced distributions on $\\mathcal{Y}=\\{y_{A},y_{B}\\}$ under $g$ with $g(x_{1})=y_{A}$, $g(x_{2})=y_{A}$, $g(x_{3})=y_{B}$, $g(x_{4})=y_{B}$:\n$$\nP_{Y}(y_{A})=P_{X}(x_{1})+P_{X}(x_{2})=\\frac{3}{4},\\quad P_{Y}(y_{B})=\\frac{1}{4},\n$$\n$$\nQ_{Y}(y_{A})=Q_{X}(x_{1})+Q_{X}(x_{2})=\\frac{1}{2},\\quad Q_{Y}(y_{B})=\\frac{1}{2}.\n$$\nThus,\n$$\n\\begin{aligned}\nD_{KL}(P_{Y}\\Vert Q_{Y})\n=\\frac{3}{4}\\log_{2}\\!\\left(\\frac{\\frac{3}{4}}{\\frac{1}{2}}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{\\frac{1}{4}}{\\frac{1}{2}}\\right)\\\\\n=\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{2}\\right)\n=\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right)-\\frac{1}{4}.\n\\end{aligned}\n$$\nTherefore, the desired difference is\n$$\nD_{KL}(P_{X}\\Vert Q_{X})-D_{KL}(P_{Y}\\Vert Q_{Y})\n=\\frac{1}{4}-\\left(\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right)-\\frac{1}{4}\\right)\n=\\frac{1}{2}-\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right).\n$$\nThis is an exact closed-form expression in bits, consistent with the data processing inequality which guarantees nonnegativity of this difference.", "answer": "$$\\boxed{\\frac{1}{2}-\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right)}$$", "id": "1633912"}]}