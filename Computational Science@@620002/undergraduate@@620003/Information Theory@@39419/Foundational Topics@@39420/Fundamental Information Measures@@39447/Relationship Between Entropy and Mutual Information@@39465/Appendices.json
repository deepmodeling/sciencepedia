{"hands_on_practices": [{"introduction": "We begin our hands-on exploration by examining one of the most direct relationships between mutual information and entropy. A core definition of mutual information is $I(X;Y) = H(Y) - H(Y|X)$, which states that the information shared between $X$ and $Y$ is the total uncertainty of $Y$ minus the uncertainty that remains about $Y$ even when $X$ is known. This practice will guide you through a common scenario where the output $Y$ is a deterministic function of the input $X$, allowing you to see how this fundamental equation simplifies and reveals the true nature of their shared information. [@problem_id:1653479]", "problem": "A digital voltmeter is used to measure a fluctuating voltage signal. The measurement process is modeled by a discrete random variable $X$, which can take values from the set of integers $\\mathcal{X} = \\{-K, -K+1, \\dots, -1, 0, 1, \\dots, K-1, K\\}$, where $K$ is a positive integer. The probability of measuring a specific voltage $x \\in \\mathcal{X}$ is given by the probability mass function $p(x) = P(X=x)$.\n\nThe data from the voltmeter is sent to a logging system that, due to a design flaw, only records the squared value of the voltage. Let this recorded value be represented by a new random variable $Y$. The relationship between the original measurement and the recorded value is therefore $Y = X^2$.\n\nThe mutual information between $X$ and $Y$, denoted as $I(X;Y)$, quantifies the reduction in uncertainty about one variable gained by observing the other. Based on the relationship between $X$ and $Y$, derive a simplified expression for $I(X;Y)$. Your final answer should express the mutual information solely in terms of the Shannon entropy of either $H(X)$ or $H(Y)$.", "solution": "The mutual information $I(X;Y)$ between two discrete random variables $X$ and $Y$ can be defined in several equivalent ways. A convenient definition for this problem is in terms of entropy and conditional entropy:\n$$I(X;Y) = H(Y) - H(Y|X)$$\nwhere $H(Y)$ is the Shannon entropy of the random variable $Y$, and $H(Y|X)$ is the conditional entropy of $Y$ given $X$.\n\nLet's analyze the conditional entropy term, $H(Y|X)$. The definition of conditional entropy is:\n$$H(Y|X) = -\\sum_{x \\in \\mathcal{X}} P(X=x) \\sum_{y \\in \\mathcal{Y}} P(Y=y|X=x) \\log_2 P(Y=y|X=x)$$\nHere, $\\mathcal{X}$ is the set of possible values for $X$, and $\\mathcal{Y}$ is the set of possible values for $Y$.\n\nThe core of the problem lies in the relationship between $X$ and $Y$, which is given as $Y = X^2$. This means that $Y$ is a deterministic function of $X$. If we know the value of the random variable $X$, say $X=x_0$, then the value of $Y$ is completely determined and is equal to $y_0 = x_0^2$. There is no uncertainty remaining about the value of $Y$ once $X$ is known.\n\nThis deterministic relationship has a direct consequence for the conditional probability $P(Y=y|X=x)$. For any given value $x \\in \\mathcal{X}$:\n- The probability that $Y=x^2$ is 1, so $P(Y=x^2|X=x)=1$.\n- The probability that $Y=y$ for any $y \\neq x^2$ is 0, so $P(Y=y|X=x)=0$ for $y \\neq x^2$.\n\nNow, let's evaluate the inner sum in the definition of $H(Y|X)$ for a fixed value of $x$:\n$$\\sum_{y \\in \\mathcal{Y}} P(Y=y|X=x) \\log_2 P(Y=y|X=x)$$\nThis sum has only one non-zero term, which occurs when $y=x^2$. For this term, $P(Y=x^2|X=x) = 1$. The value of this term is $1 \\cdot \\log_2(1)$.\nSince $\\log_2(1) = 0$, the inner sum evaluates to 0 for any given $x$.\n\nAs a result, the entire expression for the conditional entropy $H(Y|X)$ becomes:\n$$H(Y|X) = -\\sum_{x \\in \\mathcal{X}} P(X=x) \\cdot (0) = 0$$\nThe conditional entropy of $Y$ given $X$ is zero, which is the mathematical statement corresponding to the fact that knowing $X$ removes all uncertainty about $Y$.\n\nFinally, we substitute this result back into the formula for mutual information:\n$$I(X;Y) = H(Y) - H(Y|X)$$\n$$I(X;Y) = H(Y) - 0$$\n$$I(X;Y) = H(Y)$$\n\nThus, the mutual information between the original measurement $X$ and the recorded value $Y=X^2$ is equal to the entropy of the recorded value $Y$. This fits the requirement that the answer be expressed solely in terms of $H(X)$ or $H(Y)$.", "answer": "$$\\boxed{H(Y)}$$", "id": "1653479"}, {"introduction": "Having established a case where mutual information equals the output entropy, we now tackle a common misconception. It is tempting to assume that a channel with a higher output entropy $H(Y)$ is transmitting more information, but this is not always true. This exercise challenges you to compare two different communication channels to understand the critical distinction between the total uncertainty of an output and the information it actually conveys about the input. By analyzing a perfect channel against one that introduces erasures, you will prove that a higher output entropy does not necessarily mean a higher mutual information $I(X;Y)$, a key insight for evaluating any communication system. [@problem_id:1653502]", "problem": "An information theorist is analyzing two different communication systems designed to transmit a signal from a binary source. The source, represented by the random variable $X$, produces symbols from the alphabet $\\{0, 1\\}$ with equal probability, i.e., $P(X=0) = P(X=1) = 1/2$. All logarithms in this problem are base 2.\n\nThe two communication systems are modeled as information channels:\n\n1.  **Channel 1 (Erasure Channel):** This channel takes an input $x \\in \\{0, 1\\}$ and produces an output symbol $Y$ from the alphabet $\\{0, 1, e\\}$, where '$e$' represents an erasure. The channel operates as follows: with a probability $p = 3/4$, the channel erases the input, and the output is $Y=e$. With probability $1-p$, the transmission is successful, and the output is $Y=x$.\n\n2.  **Channel 2 (Ideal Channel):** This channel is a perfect, noiseless transmitter. It takes an input $x \\in \\{0, 1\\}$ and produces an output $Z=x$ with no possibility of error.\n\nYour task is to compare the entropic properties of these two channels. Calculate the output entropy ratio, defined as $R_H = \\frac{H(Y)}{H(Z)}$, and the mutual information ratio, defined as $R_I = \\frac{I(X;Y)}{I(X;Z)}$.\n\nProvide your final answer as a pair of closed-form analytic expressions for $R_H$ and $R_I$, in that order.", "solution": "The source $X$ is uniform on $\\{0,1\\}$, so its entropy with base-2 logarithms is\n$$\nH(X) = - \\sum_{x \\in \\{0,1\\}} \\frac{1}{2} \\log_{2}\\left(\\frac{1}{2}\\right) = 1.\n$$\nChannel 2 is ideal with $Z=X$. Therefore,\n$$\nH(Z) = H(X) = 1,\\quad H(Z \\mid X) = 0,\\quad I(X;Z) = H(Z) - H(Z \\mid X) = 1.\n$$\n\nFor Channel 1 (erasure with probability $p=\\frac{3}{4}$), the output $Y$ takes values in $\\{0,1,e\\}$. With $X$ uniform,\n$$\nP(Y=e) = p = \\frac{3}{4},\\quad P(Y=0) = P(X=0)(1-p) = \\frac{1}{2}\\cdot\\frac{1}{4} = \\frac{1}{8},\\quad P(Y=1) = \\frac{1}{8}.\n$$\nHence the output entropy is\n$$\nH(Y) = -\\left[\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right) + \\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right) + \\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right)\\right].\n$$\nUsing $\\log_{2}\\left(\\frac{3}{4}\\right) = \\log_{2}(3) - 2$ and $\\log_{2}\\left(\\frac{1}{8}\\right) = -3$, this simplifies to\n$$\nH(Y) = -\\frac{3}{4}\\left(\\log_{2}(3) - 2\\right) - \\frac{1}{4}(-3) = -\\frac{3}{4}\\log_{2}(3) + \\frac{9}{4}.\n$$\n\nThe mutual information for the erasure channel satisfies\n$$\nI(X;Y) = H(X) - H(X \\mid Y).\n$$\nConditioning on $Y$, if $Y=e$ then $X$ retains its prior entropy $H(X)$, while if $Y \\in \\{0,1\\}$ then $X$ is determined and the conditional entropy is $0$. Thus\n$$\nH(X \\mid Y) = P(Y=e) H(X \\mid Y=e) + P(Y\\in\\{0,1\\}) H(X \\mid Y \\in \\{0,1\\}) = p\\,H(X) + (1-p)\\cdot 0 = p\\,H(X),\n$$\nso\n$$\nI(X;Y) = H(X) - p\\,H(X) = (1-p)H(X) = \\frac{1}{4}.\n$$\n\nTherefore, the requested ratios are\n$$\nR_{H} = \\frac{H(Y)}{H(Z)} = \\frac{-\\frac{3}{4}\\log_{2}(3) + \\frac{9}{4}}{1} = -\\frac{3}{4}\\log_{2}(3) + \\frac{9}{4},\n$$\nand\n$$\nR_{I} = \\frac{I(X;Y)}{I(X;Z)} = \\frac{\\frac{1}{4}}{1} = \\frac{1}{4}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{3}{4}\\log_{2}(3)+\\frac{9}{4} & \\frac{1}{4}\\end{pmatrix}}$$", "id": "1653502"}, {"introduction": "Our final practice expands our scope from two variables to three, introducing the powerful concept of conditional mutual information, $I(X;Y|Z)$. This measures the information shared between $X$ and $Y$ when a third variable, $Z$, is already known. This thought experiment explores a fascinating scenario where two variables $X$ and $Y$ are initially independent, but become dependent when conditioned on their sum $Z = (X+Y) \\pmod m$. Working through this problem will demonstrate the sometimes counter-intuitive principle that conditioning on a related variable can reveal information that was not apparent before, a concept with deep implications in cryptography and network coding. [@problem_id:1653476]", "problem": "Let $X$ and $Y$ be two independent discrete random variables, each uniformly distributed over the set of integers $\\{0, 1, 2, \\dots, m-1\\}$, where $m$ is an integer greater than 1. A third random variable $Z$ is defined as their sum modulo $m$, such that $Z = (X+Y) \\pmod m$. Calculate the conditional mutual information $I(X;Y|Z)$ and express your answer in terms of the Shannon entropy $H(X)$.", "solution": "Let $X$ and $Y$ be independent and each uniform on $\\{0,1,\\dots,m-1\\}$, and let $Z=(X+Y)\\bmod m$. By the definition of conditional mutual information,\n$$\nI(X;Y|Z)=H(X|Z)-H(X|Y,Z).\n$$\nFirst, observe that given $Z$ and $Y$, $X$ is determined uniquely by modular arithmetic:\n$$\nX \\equiv Z - Y \\pmod m,\n$$\nso $H(X|Y,Z)=0$.\n\nNext, compute $H(X|Z)$. For any $x,z\\in\\{0,1,\\dots,m-1\\}$,\n$$\n\\Pr\\{X=x,Z=z\\}=\\Pr\\{X=x,\\,Y\\equiv z-x \\pmod m\\}=\\Pr\\{X=x\\}\\Pr\\{Y=z-x\\}=\\frac{1}{m}\\cdot\\frac{1}{m}=\\frac{1}{m^{2}}.\n$$\nSumming over $x$,\n$$\n\\Pr\\{Z=z\\}=\\sum_{x=0}^{m-1}\\Pr\\{X=x,Z=z\\}=m\\cdot\\frac{1}{m^{2}}=\\frac{1}{m}.\n$$\nThus, for all $x,z$,\n$$\n\\Pr\\{X=x\\,|\\,Z=z\\}=\\frac{\\Pr\\{X=x,Z=z\\}}{\\Pr\\{Z=z\\}}=\\frac{1/m^{2}}{1/m}=\\frac{1}{m},\n$$\nso $X$ is uniform given $Z=z$. Therefore $H(X|Z=z)=H(X)$ for every $z$, and hence\n$$\nH(X|Z)=H(X).\n$$\nCombining these results,\n$$\nI(X;Y|Z)=H(X|Z)-H(X|Y,Z)=H(X)-0=H(X).\n$$", "answer": "$$\\boxed{H(X)}$$", "id": "1653476"}]}