{"hands_on_practices": [{"introduction": "The chain rule for entropy is a powerful tool for dissecting the total uncertainty of a complex system into more manageable parts. This practice [@problem_id:1608627] provides a concrete, step-by-step application of the rule, $H(X,Y) = H(X) + H(Y|X)$, to a simple scenario involving two dependent events. By calculating the entropy of a student's quiz answers, you will gain hands-on experience in how the uncertainty of an initial event and the conditional uncertainty of a subsequent event combine to give the total joint entropy.", "problem": "A student is taking a two-question true/false quiz. Let the random variables $A_1$ and $A_2$ represent the student's answer to the first question ($Q_1$) and the second question ($Q_2$), respectively. The possible outcomes for each answer are 'True' or 'False'. The student's answering behavior has been modeled by a probabilistic process.\n\nThe probability that the student answers 'True' for the first question is $P(A_1 = \\text{'True'}) = 0.8$.\n\nThe student's answer to the second question is conditionally dependent on their answer to the first.\n- If the student answered 'True' for $Q_1$, the probability they will answer 'True' for $Q_2$ is $P(A_2 = \\text{'True'} | A_1 = \\text{'True'}) = 0.5$.\n- If the student answered 'False' for $Q_1$, the probability they will answer 'True' for $Q_2$ is $P(A_2 = \\text{'True'} | A_1 = \\text{'False'}) = 0.25$.\n\nCalculate the total entropy of the joint probability distribution of the student's answers, $H(A_1, A_2)$. Express your final answer in bits, rounded to three significant figures. Use the base-2 logarithm for all entropy calculations.", "solution": "We apply the chain rule for entropy with base-2 logarithms:\n$$\nH(A_{1},A_{2})=H(A_{1})+H(A_{2}\\mid A_{1}).\n$$\nFirst compute the marginal entropy of $A_{1}$. With $P(A_{1}=\\text{T})=0.8$ and $P(A_{1}=\\text{F})=0.2$,\n$$\nH(A_{1})=-\\big[0.8\\,\\log_{2}(0.8)+0.2\\,\\log_{2}(0.2)\\big].\n$$\nUsing $\\log_{2}(0.8)\\approx -0.3219280949$ and $\\log_{2}(0.2)\\approx -2.3219280949$,\n$$\nH(A_{1})\\approx -\\big[0.8(-0.3219280949)+0.2(-2.3219280949)\\big]\\approx 0.7219280949.\n$$\n\nNext compute the conditional entropy $H(A_{2}\\mid A_{1})$:\n$$\nH(A_{2}\\mid A_{1})=P(A_{1}=\\text{T})\\,H(A_{2}\\mid A_{1}=\\text{T})+P(A_{1}=\\text{F})\\,H(A_{2}\\mid A_{1}=\\text{F}).\n$$\nGiven $P(A_{2}=\\text{T}\\mid A_{1}=\\text{T})=0.5$, we have\n$$\nH(A_{2}\\mid A_{1}=\\text{T})=-\\big[0.5\\,\\log_{2}(0.5)+0.5\\,\\log_{2}(0.5)\\big]=1.\n$$\nGiven $P(A_{2}=\\text{T}\\mid A_{1}=\\text{F})=0.25$, we have\n$$\nH(A_{2}\\mid A_{1}=\\text{F})=-\\big[0.25\\,\\log_{2}(0.25)+0.75\\,\\log_{2}(0.75)\\big].\n$$\nUsing $\\log_{2}(0.25)=-2$ and $\\log_{2}(0.75)\\approx -0.4150374993$,\n$$\nH(A_{2}\\mid A_{1}=\\text{F})\\approx -\\big[0.25(-2)+0.75(-0.4150374993)\\big]\\approx 0.8112781245.\n$$\nTherefore,\n$$\nH(A_{2}\\mid A_{1})\\approx 0.8\\times 1+0.2\\times 0.8112781245\\approx 0.9622556249.\n$$\n\nFinally, the joint entropy is\n$$\nH(A_{1},A_{2})\\approx 0.7219280949+0.9622556249\\approx 1.6841837198.\n$$\nRounded to three significant figures (in bits), this is $1.68$.", "answer": "$$\\boxed{1.68}$$", "id": "1608627"}, {"introduction": "After exploring dependent events, we can deepen our understanding by examining an important limiting case: perfect correlation. This exercise [@problem_id:1991843] uses the chain rule to formally demonstrate that no new information is gained by measuring a quantity that is already known. You will prove the intuitive result that the joint entropy of a variable with itself, $H(X,X)$, reduces to just the entropy of the variable, $H(X)$, because the conditional entropy of a variable given itself, $H(X|X)$, is zero.", "problem": "Consider a system composed of two distinguishable subunits, A and B. Each subunit can exist in one of $N$ possible discrete energy levels, indexed by $i \\in \\{1, 2, \\dots, N\\}$. Initially, the subunits are prepared independently. They are then brought into contact and allowed to interact through a special process that perfectly correlates their states. After this interaction, the two subunits are always found to be in the same energy level; that is, if a measurement of subunit A's energy level yields index $i$, a simultaneous measurement of subunit B's energy level is guaranteed to also yield index $i$.\n\nThe interaction may redistribute the probabilities of occupying the energy levels. After the interaction, the probability of finding subunit A in the $i$-th energy level is given by $p_i$, where $\\sum_{i=1}^{N} p_i = 1$. Let $X_A$ and $X_B$ be the random variables representing the energy level indices of subunits A and B, respectively, after the interaction.\n\nUsing the statistical mechanics definition of entropy which involves the Boltzmann constant $k_B$ and the natural logarithm, what is the joint Shannon entropy $S(X_A, X_B)$ of the combined system after the interaction? Express your answer as a symbolic expression in terms of $k_B$ and the probabilities $p_i$.", "solution": "We are given two distinguishable subunits A and B whose post-interaction joint distribution of energy-level indices is perfectly correlated such that $X_A = X_B$ with probability one. The marginal distribution of $X_A$ (and equivalently $X_B$) is specified by $p_i$ with $\\sum_{i=1}^{N} p_i = 1$. Therefore, the joint probability mass function is\n$$\nP(X_A=i, X_B=j) =\n\\begin{cases}\np_i, & i=j, \\\\\n0, & i \\neq j.\n\\end{cases}\n$$\nUsing the statistical mechanics form of Shannon entropy, the joint entropy is defined as\n$$\nS(X_A, X_B) = -k_{B} \\sum_{i=1}^{N} \\sum_{j=1}^{N} P(X_A=i, X_B=j)\\,\\ln\\big(P(X_A=i, X_B=j)\\big).\n$$\nSubstituting the joint probabilities and noting that terms with $P=0$ contribute zero by continuity of $x \\ln x$ at $x=0$, the double sum reduces to the diagonal terms $i=j$:\n$$\nS(X_A, X_B) = -k_{B} \\sum_{i=1}^{N} p_i \\ln p_i.\n$$\nThus, the joint entropy equals the entropy of the marginal distribution $\\{p_i\\}$, consistent with the fact that $X_B$ is a deterministic function of $X_A$ after the interaction.", "answer": "$$\\boxed{-k_{B}\\sum_{i=1}^{N} p_{i}\\ln p_{i}}$$", "id": "1991843"}, {"introduction": "The true power of the chain rule becomes apparent when we extend it from two variables to a sequence of many. This advanced practice [@problem_id:1608589] challenges you to apply the generalized chain rule, $H(X_1, \\dots, X_n) = \\sum_{i=1}^{n} H(X_i | X_1, \\dots, X_{i-1})$, to the process of generating a random permutation. By calculating and summing the entropy at each sequential step, you will discover an elegant connection between a dynamic process and the static uncertainty of its final outcome.", "problem": "A cryptographic system relies on generating random permutations for its key schedule. A permutation $\\pi = (\\pi_1, \\pi_2, \\dots, \\pi_n)$ of the set of integers $\\{1, 2, \\dots, n\\}$ is generated through a sequential selection process. First, an element $\\pi_1$ is chosen uniformly at random from the entire set of $n$ integers. Next, an element $\\pi_2$ is chosen uniformly at random from the $n-1$ remaining integers. This process continues, such that at step $k$, the element $\\pi_k$ is chosen uniformly at random from the $n-k+1$ integers not yet selected. This concludes when $\\pi_n$ is chosen, which is the last remaining integer.\n\nLet the sequence of random variables representing this process be $(\\pi_1, \\pi_2, \\dots, \\pi_n)$. Calculate the total joint entropy $H(\\pi_1, \\pi_2, \\dots, \\pi_n)$ of this sequence.\n\nExpress your answer as a single, closed-form analytic expression in terms of $n$. Use the base-2 logarithm in your final expression.", "solution": "The problem asks for the joint entropy of a sequence of random variables $(\\pi_1, \\pi_2, \\dots, \\pi_n)$ that are generated sequentially. The joint entropy can be calculated using the chain rule for entropy, which states:\n$$H(X_1, X_2, \\dots, X_n) = \\sum_{k=1}^{n} H(X_k | X_1, \\dots, X_{k-1})$$\nBy convention, $H(X_1 | X_0)$ is simply $H(X_1)$.\n\nApplying this rule to our problem, the joint entropy is:\n$$H(\\pi_1, \\pi_2, \\dots, \\pi_n) = H(\\pi_1) + H(\\pi_2 | \\pi_1) + H(\\pi_3 | \\pi_1, \\pi_2) + \\dots + H(\\pi_n | \\pi_1, \\dots, \\pi_{n-1})$$\n\nWe need to calculate each conditional entropy term in this sum.\n\nStep 1: Calculate $H(\\pi_1)$.\nThe first element, $\\pi_1$, is chosen uniformly at random from a set of $n$ elements. The entropy of a discrete random variable that is uniform over a set of size $M$ is $\\log_2(M)$. In this case, $M=n$.\n$$H(\\pi_1) = \\log_2(n)$$\n\nStep 2: Calculate $H(\\pi_2 | \\pi_1)$.\nGiven that $\\pi_1$ has been chosen, the second element, $\\pi_2$, is chosen uniformly at random from the remaining $n-1$ elements. The conditional probability distribution of $\\pi_2$ given any specific value of $\\pi_1$ is uniform over a set of size $n-1$. Therefore, the conditional entropy is:\n$$H(\\pi_2 | \\pi_1) = \\log_2(n-1)$$\n\nStep 3: Generalize for the $k$-th term, $H(\\pi_k | \\pi_1, \\dots, \\pi_{k-1})$.\nAt the $k$-th step, the first $k-1$ elements $(\\pi_1, \\dots, \\pi_{k-1})$ have already been chosen. The next element, $\\pi_k$, is chosen uniformly from the set of elements that have not yet been selected. The size of this set is $n - (k-1) = n - k + 1$.\nThe conditional distribution of $\\pi_k$ is uniform over a set of size $n-k+1$. The conditional entropy is therefore:\n$$H(\\pi_k | \\pi_1, \\dots, \\pi_{k-1}) = \\log_2(n-k+1)$$\n\nStep 4: Sum all the conditional entropy terms.\nWe can now write the total joint entropy as the sum of these terms from $k=1$ to $n$:\n$$H(\\pi_1, \\dots, \\pi_n) = \\sum_{k=1}^{n} H(\\pi_k | \\pi_1, \\dots, \\pi_{k-1}) = \\sum_{k=1}^{n} \\log_2(n-k+1)$$\nLet's expand this sum to see the pattern:\n$$H(\\pi_1, \\dots, \\pi_n) = \\log_2(n-1+1) + \\log_2(n-2+1) + \\dots + \\log_2(n-n+1)$$\n$$H(\\pi_1, \\dots, \\pi_n) = \\log_2(n) + \\log_2(n-1) + \\dots + \\log_2(1)$$\nThe last term, for $k=n$, is $H(\\pi_n | \\pi_1, \\dots, \\pi_{n-1}) = \\log_2(n-n+1) = \\log_2(1) = 0$. This makes sense, as the last element is determined once the first $n-1$ are known.\n\nStep 5: Combine the logarithmic terms.\nUsing the property of logarithms that $\\log(a) + \\log(b) = \\log(ab)$, we can combine the sum of logarithms into the logarithm of a product:\n$$H(\\pi_1, \\dots, \\pi_n) = \\log_2(n \\cdot (n-1) \\cdot (n-2) \\cdot \\dots \\cdot 2 \\cdot 1)$$\nThe product $n \\cdot (n-1) \\cdot \\dots \\cdot 1$ is the definition of the factorial of $n$, denoted as $n!$.\nTherefore, the final expression for the joint entropy is:\n$$H(\\pi_1, \\dots, \\pi_n) = \\log_2(n!)$$\nThis result is consistent with the fact that the sequential generation process produces each of the $n!$ possible permutations with equal probability of $\\frac{1}{n!}$, and the entropy of a uniform distribution over $n!$ outcomes is $\\log_2(n!)$.", "answer": "$$\\boxed{\\log_{2}(n!)}$$", "id": "1608589"}]}