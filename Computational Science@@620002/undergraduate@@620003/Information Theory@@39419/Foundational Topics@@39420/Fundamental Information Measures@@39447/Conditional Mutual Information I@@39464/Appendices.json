{"hands_on_practices": [{"introduction": "Let's begin with a familiar scenario from probability theory to build our foundational skills in information theory. By analyzing the properties of a single, fair die roll, this exercise [@problem_id:1612830] provides a concrete, step-by-step guide to calculating conditional mutual information from first principles. It beautifully illustrates how partitioning the sample space by conditioning on a variable $Z$ can fundamentally alter the informational relationship between two other variables, $X$ and $Y$.", "problem": "An electronic game generates a random integer outcome, $\\omega$, from the set $\\{1, 2, 3, 4, 5, 6\\}$. The process simulates a single roll of a fair six-sided die, so each outcome $\\omega$ has a probability of $1/6$. Based on the outcome, three binary random variables, $X$, $Y$, and $Z$, are determined as follows:\n\n-   $X=1$ if the outcome $\\omega$ is an odd number; otherwise, $X=0$.\n-   $Y=1$ if the outcome $\\omega$ is a prime number; otherwise, $Y=0$. For the purpose of this problem, the prime numbers in the set of outcomes are $\\{2, 3, 5\\}$.\n-   $Z=1$ if the outcome $\\omega$ is strictly greater than 4; otherwise, $Z=0$.\n\nCalculate the conditional mutual information $I(X; Y | Z)$. Express your answer in bits as a closed-form analytic expression. Note that information measured in bits corresponds to the use of base-2 logarithms in entropy calculations.", "solution": "Let $\\Omega=\\{1,2,3,4,5,6\\}$ with $P(\\omega)=\\frac{1}{6}$ for each outcome. Define\n- $X=1$ for $\\{1,3,5\\}$ and $X=0$ for $\\{2,4,6\\}$,\n- $Y=1$ for $\\{2,3,5\\}$ and $Y=0$ for $\\{1,4,6\\}$,\n- $Z=1$ for $\\{5,6\\}$ and $Z=0$ for $\\{1,2,3,4\\}$.\n\nWe seek $I(X;Y|Z)$, which can be written as\n$$\nI(X;Y|Z)=\\sum_{z}P(z)\\,I(X;Y\\,|\\,Z=z)\n=\\sum_{z}P(z)\\sum_{x,y}P(x,y\\,|\\,z)\\,\\log_{2}\\!\\left(\\frac{P(x,y\\,|\\,z)}{P(x\\,|\\,z)P(y\\,|\\,z)}\\right).\n$$\n\nFirst, $P(Z=1)=\\frac{2}{6}=\\frac{1}{3}$ and $P(Z=0)=\\frac{4}{6}=\\frac{2}{3}$.\n\nCase $Z=1$: The outcomes are $\\{5,6\\}$, each with conditional probability $\\frac{1}{2}$. For $\\omega=5$ we have $(X,Y)=(1,1)$; for $\\omega=6$ we have $(X,Y)=(0,0)$. Hence $P(X=1\\,|\\,Z=1)=P(Y=1\\,|\\,Z=1)=\\frac{1}{2}$ and $P(X=Y\\,|\\,Z=1)=1$. Therefore $Y$ is a deterministic function of $X$ (and vice versa) given $Z=1$, so\n$$\nI(X;Y\\,|\\,Z=1)=H(X\\,|\\,Z=1)-H(X\\,|\\,Y,Z=1)=H(X\\,|\\,Z=1).\n$$\nSince $P(X=1\\,|\\,Z=1)=\\frac{1}{2}$,\n$$\nH(X\\,|\\,Z=1)=-\\sum_{x\\in\\{0,1\\}}P(x\\,|\\,Z=1)\\log_{2}P(x\\,|\\,Z=1)=1,\n$$\nso $I(X;Y\\,|\\,Z=1)=1$.\n\nCase $Z=0$: The outcomes are $\\{1,2,3,4\\}$, each with conditional probability $\\frac{1}{4}$. The pairs $(X,Y)$ realized are $(1,0)$ from $\\omega=1$, $(0,1)$ from $\\omega=2$, $(1,1)$ from $\\omega=3$, and $(0,0)$ from $\\omega=4$, each with probability $\\frac{1}{4}$. Thus\n$$\nP(X=1\\,|\\,Z=0)=\\frac{1}{2},\\quad P(Y=1\\,|\\,Z=0)=\\frac{1}{2},\\quad P(X=x,Y=y\\,|\\,Z=0)=\\frac{1}{4}=P(X=x\\,|\\,Z=0)P(Y=y\\,|\\,Z=0),\n$$\nso $X$ and $Y$ are independent given $Z=0$, and hence\n$$\nI(X;Y\\,|\\,Z=0)=0.\n$$\n\nTherefore,\n$$\nI(X;Y\\,|\\,Z)=P(Z=1)\\,I(X;Y\\,|\\,Z=1)+P(Z=0)\\,I(X;Y\\,|\\,Z=0)=\\frac{1}{3}\\cdot 1+\\frac{2}{3}\\cdot 0=\\frac{1}{3}.\n$$\n\nFor completeness, this agrees with the identity $I(X;Y|Z)=H(X|Z)-H(X|Y,Z)$. Indeed, $H(X|Z=1)=1$ and $H(X|Z=0)=1$, so $H(X|Z)=1$. Also $H(X|Y,Z=1)=0$ and $H(X|Y,Z=0)=1$, so $H(X|Y,Z)=\\frac{1}{3}\\cdot 0+\\frac{2}{3}\\cdot 1=\\frac{2}{3}$. Thus $I(X;Y|Z)=1-\\frac{2}{3}=\\frac{1}{3}$ bits.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1612830"}, {"introduction": "This next practice explores one of the most fascinating and sometimes counter-intuitive aspects of probabilistic reasoning and information. We start with two completely independent events—fair coin flips—and discover that conditioning on their sum makes them statistically dependent [@problem_id:1612850]. This phenomenon, often called \"explaining away,\" is a critical concept for understanding how observing related data can create apparent linkages between variables that were originally independent.", "problem": "Consider a simple digital communication system involving two independent binary sources, Source 1 and Source 2. Each source generates a single bit, represented by the random variables $X_1$ and $X_2$, respectively. For each source, the bit is a '0' with probability 0.5 and a '1' with probability 0.5.\n\nA monitoring station receives these two bits and computes their sum, defined by the random variable $Z = X_1 + X_2$. The value of this sum $Z$ is then observed. We are interested in quantifying how much information the bit from Source 1 gives about the bit from Source 2, once their sum $Z$ is known. This is a measure of the remaining statistical dependence between the sources after observing their sum.\n\nCalculate the conditional mutual information $I(X_1; X_2 | Z)$. All calculations involving logarithms must use base 2. Express your final answer in bits as a decimal value.", "solution": "Let $X_{1}$ and $X_{2}$ be independent $\\text{Bernoulli}\\left(\\frac{1}{2}\\right)$ random variables. Define $Z=X_{1}+X_{2}$, which takes values in $\\{0,1,2\\}$ with\n$$\nP(Z=0)=\\frac{1}{4},\\quad P(Z=1)=\\frac{1}{2},\\quad P(Z=2)=\\frac{1}{4}.\n$$\nThe conditional mutual information is\n$$\nI(X_{1};X_{2}|Z)=\\sum_{z}P(z)\\,I(X_{1};X_{2}|Z=z),\n$$\nwith all entropies and logarithms in base $2$ (bits).\n\nCompute $I(X_{1};X_{2}|Z=z)$ for each $z$ using $I(X_{1};X_{2}|Z=z)=H(X_{1}|Z=z)-H(X_{1}|X_{2},Z=z)$.\n\n1) For $z=0$: only $(X_{1},X_{2})=(0,0)$ is possible, so $H(X_{1}|Z=0)=0$ and $H(X_{1}|X_{2},Z=0)=0$. Hence $I(X_{1};X_{2}|Z=0)=0$.\n\n2) For $z=2$: only $(X_{1},X_{2})=(1,1)$ is possible, so $H(X_{1}|Z=2)=0$ and $H(X_{1}|X_{2},Z=2)=0$. Hence $I(X_{1};X_{2}|Z=2)=0$.\n\n3) For $z=1$: the pairs $(0,1)$ and $(1,0)$ occur with equal probability $\\frac{1}{2}$. Thus $X_{1}|Z=1$ is $\\text{Bernoulli}\\left(\\frac{1}{2}\\right)$, giving\n$$\nH(X_{1}|Z=1)=-\\left(\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}+\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\right)=1.\n$$\nGiven $Z=1$ and $X_{2}$, $X_{1}$ is determined by $X_{1}=1-X_{2}$, so $H(X_{1}|X_{2},Z=1)=0$. Hence $I(X_{1};X_{2}|Z=1)=1$.\n\nAverage over $Z$:\n$$\nI(X_{1};X_{2}|Z)=\\frac{1}{4}\\cdot 0+\\frac{1}{2}\\cdot 1+\\frac{1}{4}\\cdot 0=\\frac{1}{2}.\n$$\nTherefore, the conditional mutual information is $0.5$ bits.", "answer": "$$\\boxed{0.5}$$", "id": "1612850"}, {"introduction": "Now we will apply our understanding to a slightly more complex scenario modeled after a digital communication system [@problem_id:1612825]. This problem moves beyond uniform probabilities and introduces a functional mapping between a message and its encoded bits, requiring a careful and systematic application of the definitions of conditional entropy and mutual information. It serves as an excellent exercise to solidify your computational skills and your ability to handle more realistic, non-uniform probability distributions.", "problem": "Consider a digital communication system where a source generates a message $X$ selected from the set $\\{0, 1, 2, 3\\}$. The probabilities of these messages are given by $P(X=0) = \\frac{1}{8}$, $P(X=1) = \\frac{1}{4}$, $P(X=2) = \\frac{3}{8}$, and $P(X=3) = \\frac{1}{4}$.\n\nThe message $X$ is encoded into a two-bit binary number. We define two random variables derived from this binary representation. Let $Y$ be the value of the most significant bit (MSB) of $X$, and let $Z$ be the value of the least significant bit (LSB) of $X$. For example, if the message is $X=2$, its standard two-bit binary representation is $10$, which means $Y=1$ (the MSB) and $Z=0$ (the LSB).\n\nThe conditional mutual information $I(X; Y | Z)$ measures the reduction in uncertainty about the original message $X$ provided by knowing the MSB ($Y$), given that the LSB ($Z$) is already known.\n\nCalculate the conditional mutual information $I(X;Y|Z)$ in bits. Express your final answer as a single closed-form analytic expression using the base-2 logarithm.", "solution": "Let $X \\in \\{0,1,2,3\\}$ with $P(X=0)=\\frac{1}{8}$, $P(X=1)=\\frac{1}{4}$, $P(X=2)=\\frac{3}{8}$, $P(X=3)=\\frac{1}{4}$. Define $Y$ as the MSB and $Z$ as the LSB of the two-bit binary representation of $X$, so $(Y,Z)$ is a deterministic function of $X$:\n$$\nX=0 \\to (Y,Z)=(0,0),\\quad X=1 \\to (Y,Z)=(0,1),\\quad X=2 \\to (Y,Z)=(1,0),\\quad X=3 \\to (Y,Z)=(1,1).\n$$\nCompute the joint distribution of $(Y,Z)$:\n$$\nP(Y=0,Z=0)=\\frac{1}{8},\\quad P(Y=0,Z=1)=\\frac{1}{4},\\quad P(Y=1,Z=0)=\\frac{3}{8},\\quad P(Y=1,Z=1)=\\frac{1}{4}.\n$$\nThus the marginal of $Z$ is\n$$\nP(Z=0)=\\frac{1}{8}+\\frac{3}{8}=\\frac{1}{2},\\qquad P(Z=1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nThe conditional distributions of $Y$ given $Z$ are\n$$\nP(Y=0|Z=0)=\\frac{\\frac{1}{8}}{\\frac{1}{2}}=\\frac{1}{4},\\quad P(Y=1|Z=0)=\\frac{\\frac{3}{8}}{\\frac{1}{2}}=\\frac{3}{4},\n$$\n$$\nP(Y=0|Z=1)=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\\quad P(Y=1|Z=1)=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2}.\n$$\nBy definition of conditional mutual information and using that $Y$ is a deterministic function of $X$,\n$$\nI(X;Y|Z)=H(Y|Z)-H(Y|X,Z)=H(Y|Z).\n$$\nCompute $H(Y|Z)$ by averaging the conditional entropies:\n$$\nH(Y|Z)=\\sum_{z}P(Z=z)H(Y|Z=z)=\\frac{1}{2}H(Y|Z=0)+\\frac{1}{2}H(Y|Z=1),\n$$\nwhere\n$$\nH(Y|Z=0)=-\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right),\n$$\n$$\nH(Y|Z=1)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\nSimplify $H(Y|Z=0)$:\n$$\n-\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)=\\frac{1}{2},\\qquad -\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right)=-\\frac{3}{4}\\left(\\log_{2}3-2\\right)= -\\frac{3}{4}\\log_{2}3+\\frac{3}{2},\n$$\nso\n$$\nH(Y|Z=0)=\\frac{1}{2}-\\frac{3}{4}\\log_{2}3+\\frac{3}{2}=2-\\frac{3}{4}\\log_{2}3.\n$$\nTherefore,\n$$\nI(X;Y|Z)=H(Y|Z)=\\frac{1}{2}\\left(2-\\frac{3}{4}\\log_{2}3\\right)+\\frac{1}{2}=\\frac{3}{2}-\\frac{3}{8}\\log_{2}3.\n$$\nThis is in bits, expressed using the base-2 logarithm as required.", "answer": "$$\\boxed{\\frac{3}{2}-\\frac{3}{8}\\log_{2}(3)}$$", "id": "1612825"}]}