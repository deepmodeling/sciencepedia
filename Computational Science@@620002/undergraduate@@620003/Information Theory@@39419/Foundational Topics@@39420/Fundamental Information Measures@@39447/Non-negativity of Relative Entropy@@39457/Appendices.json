{"hands_on_practices": [{"introduction": "The best way to understand relative entropy is to calculate it. This first exercise provides a concrete scenario to practice computing the Kullback-Leibler (KL) divergence, $D_{KL}(P||Q)$. By quantifying the information gap between an assumed model (a fair coin) and an observed reality (a biased coin), you will gain a hands-on feel for how KL divergence measures the 'penalty' for using an incorrect assumption. [@problem_id:1643625]", "problem": "An engineer is tasked with analyzing a new semiconductor manufacturing process. Each chip produced by this process can be classified into one of two states: 'operational' or 'defective'. In the absence of initial data, a simplified model is proposed which assumes that any given chip has an equal probability of being operational or defective.\n\nAfter a pilot production run, a detailed analysis reveals that the true probability of a chip being 'operational' is $p$. To quantify the discrepancy between the simplified model and the observed reality, a \"model divergence\" metric, $D$, is calculated. This metric is defined by the formula:\n\n$$D = \\sum_{s \\in S} P(s) \\ln\\left(\\frac{P(s)}{Q(s)}\\right)$$\n\nwhere $S = \\{\\text{operational, defective}\\}$ is the set of all possible states for a chip, $P(s)$ is the true probability distribution of the states determined from the pilot run, and $Q(s)$ is the probability distribution assumed by the simplified model.\n\nCalculate the value of the model divergence metric $D$ given that the true probability of a chip being operational is $p = 0.90$. Report your answer as a real number rounded to three significant figures.", "solution": "We are given a binary state space $S=\\{\\text{operational},\\text{defective}\\}$ with true distribution $P$ and model distribution $Q$. The model assumes $Q(\\text{operational})=Q(\\text{defective})=\\frac{1}{2}$. The true distribution from the pilot run is $P(\\text{operational})=p$ and $P(\\text{defective})=1-p$, with $p=0.90$.\n\nThe model divergence is the Kullbackâ€“Leibler divergence\n$$\nD=\\sum_{s\\in S}P(s)\\ln\\!\\left(\\frac{P(s)}{Q(s)}\\right).\n$$\nSubstituting the two states and their probabilities,\n$$\nD=p\\ln\\!\\left(\\frac{p}{\\frac{1}{2}}\\right)+(1-p)\\ln\\!\\left(\\frac{1-p}{\\frac{1}{2}}\\right)\n=p\\ln(2p)+(1-p)\\ln\\!\\bigl(2(1-p)\\bigr).\n$$\nFor $p=0.90$, we get\n$$\nD=0.90\\,\\ln(1.8)+0.10\\,\\ln(0.2).\n$$\nUsing $\\ln(1.8)\\approx 0.5877866649$ and $\\ln(0.2)=-\\ln(5)\\approx -1.6094379124$, compute\n$$\n0.90\\,\\ln(1.8)\\approx 0.90\\times 0.5877866649\\approx 0.5290079984,\n$$\n$$\n0.10\\,\\ln(0.2)\\approx 0.10\\times(-1.6094379124)\\approx -0.1609437912.\n$$\nTherefore,\n$$\nD\\approx 0.5290079984-0.1609437912=0.3680642072.\n$$\nRounding to three significant figures gives $D\\approx 0.368$.", "answer": "$$\\boxed{0.368}$$", "id": "1643625"}, {"introduction": "While KL divergence measures the 'dissimilarity' between two distributions, it's crucial to understand that it is not a true distance metric. This exercise highlights the fundamental property of asymmetry by having you calculate both $D_{KL}(P||Q)$ and $D_{KL}(Q||P)$ and see that they are not equal. Understanding this asymmetry is key to correctly interpreting and applying the concept. [@problem_id:1643606]", "problem": "In information theory, the Kullback-Leibler (KL) divergence, or relative entropy, is a measure of how one probability distribution differs from a second, reference probability distribution.\n\nConsider a discrete random variable that can take one of three possible outcomes, which we label as $\\{O_1, O_2, O_3\\}$. Two different probabilistic models, model $P$ and model $Q$, have been proposed to describe the likelihood of these outcomes.\n\nModel $P$ is described by the probability distribution $P(O_1) = \\frac{1}{2}$, $P(O_2) = \\frac{1}{4}$, and $P(O_3) = \\frac{1}{4}$.\nModel $Q$ assumes that all three outcomes are equally likely; that is, it is a uniform distribution over the set of outcomes.\n\nYour task is to calculate the KL divergence of $P$ from $Q$, denoted $D_{KL}(P||Q)$, and the KL divergence of $Q$ from $P$, denoted $D_{KL}(Q||P)$. Provide your result as an ordered pair of values $(D_{KL}(P||Q), D_{KL}(Q||P))$.\n\nExpress your answers as exact analytic expressions in nats, corresponding to the use of the natural logarithm ($\\ln$).", "solution": "We have three outcomes $\\{O_{1},O_{2},O_{3}\\}$ with model $P$ given by $P(O_{1})=\\frac{1}{2}$, $P(O_{2})=\\frac{1}{4}$, $P(O_{3})=\\frac{1}{4}$, and model $Q$ uniform so that $Q(O_{i})=\\frac{1}{3}$ for $i\\in\\{1,2,3\\}$.\n\nBy definition, the Kullback-Leibler divergence of $P$ from $Q$ is\n$$\nD_{KL}(P\\|Q)=\\sum_{i=1}^{3}P(O_{i})\\ln\\!\\left(\\frac{P(O_{i})}{Q(O_{i})}\\right).\n$$\nSubstituting the probabilities,\n$$\nD_{KL}(P\\|Q)=\\frac{1}{2}\\ln\\!\\left(\\frac{\\frac{1}{2}}{\\frac{1}{3}}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{\\frac{1}{4}}{\\frac{1}{3}}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{\\frac{1}{4}}{\\frac{1}{3}}\\right)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{3}{2}\\right)+\\frac{1}{2}\\ln\\!\\left(\\frac{3}{4}\\right)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{9}{8}\\right).\n$$\n\nSimilarly, the Kullback-Leibler divergence of $Q$ from $P$ is\n$$\nD_{KL}(Q\\|P)=\\sum_{i=1}^{3}Q(O_{i})\\ln\\!\\left(\\frac{Q(O_{i})}{P(O_{i})}\\right),\n$$\nwhich gives\n$$\nD_{KL}(Q\\|P)=\\frac{1}{3}\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{1}{2}}\\right)+\\frac{1}{3}\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{1}{4}}\\right)+\\frac{1}{3}\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{1}{4}}\\right)\n=\\frac{1}{3}\\ln\\!\\left(\\frac{2}{3}\\right)+\\frac{2}{3}\\ln\\!\\left(\\frac{4}{3}\\right)\n=\\frac{1}{3}\\ln\\!\\left(\\frac{32}{27}\\right).\n$$\n\nTherefore, the ordered pair $(D_{KL}(P\\|Q), D_{KL}(Q\\|P))$ is $\\left(\\frac{1}{2}\\ln\\left(\\frac{9}{8}\\right), \\frac{1}{3}\\ln\\left(\\frac{32}{27}\\right)\\right)$ in nats.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}\\ln\\left(\\frac{9}{8}\\right) & \\frac{1}{3}\\ln\\left(\\frac{32}{27}\\right)\\end{pmatrix}}$$", "id": "1643606"}, {"introduction": "The non-negativity of relative entropy is not just a mathematical curiosity; it's a powerful principle used for optimization in science and engineering. This problem connects theory to practice by showing how minimizing the average KL divergence can determine the best parameters for a model. You will see that the optimal model is the one that perfectly matches the true underlying process, a direct consequence of $D_{KL}(P||Q)$ being zero only when $P=Q$. [@problem_id:1643617]", "problem": "An engineer is designing a simplified model for a digital communication channel. The input to the channel is a binary random variable $X \\in \\{0, 1\\}$, with a known prior probability distribution $p(X=1) = p$ and $p(X=0) = 1-p$, where $0 < p < 1$.\n\nThe true physical behavior of the channel is described by a conditional probability density function $p(y|x)$, where $y$ is the continuous output signal. For a given input $x$, the output $y$ follows a Gaussian distribution with a mean that depends on the input bit and a constant variance $\\sigma^2$. Specifically, the true channel is given by:\n$$p(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu_x)^2}{2\\sigma^2}\\right)$$\nwhere the means are $\\mu_1 = c$ and $\\mu_0 = -c$ for some known positive constant $c$.\n\nThe engineer's simplified model, $q(y|x)$, is also assumed to be Gaussian with the same variance $\\sigma^2$, but with model parameters for the means, $\\nu_1$ and $\\nu_0$.\n$$q(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\nu_x)^2}{2\\sigma^2}\\right)$$\nTo find the best-fitting model, the engineer decides to minimize the average information loss between the true channel and the model. This loss is quantified by the expected Kullback-Leibler (KL) divergence between $p(y|x)$ and $q(y|x)$, where the expectation is taken over the distribution of the input $X$. The objective function to minimize is:\n$$L(\\nu_0, \\nu_1) = \\mathbb{E}_{p(x)}[D_{KL}(p(y|x) || q(y|x))]$$\nDetermine the expressions for the optimal model parameters $\\nu_0$ and $\\nu_1$ that minimize this average divergence. The final answer should be a set of expressions for $\\nu_0$ and $\\nu_1$ in terms of $c$, $p$, and $\\sigma$.", "solution": "We are given a binary input $X \\in \\{0,1\\}$ with $p(X=1)=p$ and $p(X=0)=1-p$, and a true conditional density $p(y|x)$ that is Gaussian with variance $\\sigma^{2}$ and mean $\\mu_{x}$. The model $q(y|x)$ is also Gaussian with the same variance $\\sigma^{2}$ and mean $\\nu_{x}$. The average loss to be minimized is the expected Kullback-Leibler divergence\n$$\nL(\\nu_{0},\\nu_{1})=\\mathbb{E}_{p(x)}\\left[D_{KL}\\left(p(y|x)\\,\\|\\,q(y|x)\\right)\\right].\n$$\nFor fixed $x$, the KL divergence between two univariate Gaussians with the same variance $\\sigma^{2}$ and means $\\mu_{x}$ and $\\nu_{x}$ is computed directly from the definition:\n$$\nD_{KL}\\left(p(y|x)\\,\\|\\,q(y|x)\\right)=\\mathbb{E}_{p(y|x)}\\left[\\ln\\frac{p(y|x)}{q(y|x)}\\right].\n$$\nUsing\n$$\n\\ln p(y|x)=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(y-\\mu_{x})^{2}}{2\\sigma^{2}},\\quad\n\\ln q(y|x)=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(y-\\nu_{x})^{2}}{2\\sigma^{2}},\n$$\ntheir difference is\n$$\n\\ln p(y|x)-\\ln q(y|x)=\\frac{(y-\\nu_{x})^{2}-(y-\\mu_{x})^{2}}{2\\sigma^{2}}.\n$$\nTaking the expectation with respect to $p(y|x)$, where $y\\sim\\mathcal{N}(\\mu_{x},\\sigma^{2})$, we use\n$$\n\\mathbb{E}\\left[(y-\\mu_{x})^{2}\\right]=\\sigma^{2},\\quad\n\\mathbb{E}\\left[(y-\\nu_{x})^{2}\\right]=\\sigma^{2}+(\\mu_{x}-\\nu_{x})^{2},\n$$\nto obtain\n$$\nD_{KL}\\left(p(y|x)\\,\\|\\,q(y|x)\\right)=\\frac{(\\mu_{x}-\\nu_{x})^{2}}{2\\sigma^{2}}.\n$$\nTherefore, the objective becomes\n$$\nL(\\nu_{0},\\nu_{1})=\\mathbb{E}_{p(x)}\\left[\\frac{(\\mu_{x}-\\nu_{x})^{2}}{2\\sigma^{2}}\\right]\n=\\frac{p}{2\\sigma^{2}}(\\mu_{1}-\\nu_{1})^{2}+\\frac{1-p}{2\\sigma^{2}}(\\mu_{0}-\\nu_{0})^{2}.\n$$\nThis function is separable and strictly convex in $(\\nu_{0},\\nu_{1})$ because $0<p<1$ and $\\sigma^{2}>0$. Taking partial derivatives and setting to zero,\n$$\n\\frac{\\partial L}{\\partial \\nu_{1}}=\\frac{p}{\\sigma^{2}}(\\nu_{1}-\\mu_{1})=0 \\;\\Rightarrow\\; \\nu_{1}^{\\star}=\\mu_{1},\n$$\n$$\n\\frac{\\partial L}{\\partial \\nu_{0}}=\\frac{1-p}{\\sigma^{2}}(\\nu_{0}-\\mu_{0})=0 \\;\\Rightarrow\\; \\nu_{0}^{\\star}=\\mu_{0}.\n$$\nWith the true means given as $\\mu_1 = c$ and $\\mu_0 = -c$, the optimal parameters are found by substituting these into our general result:\n$$\n\\nu_{1}^{\\star}=c,\\qquad \\nu_{0}^{\\star}=-c,\n$$\nindependent of $p$ and $\\sigma$.", "answer": "$$\\boxed{\\begin{pmatrix}-c & c\\end{pmatrix}}$$", "id": "1643617"}]}