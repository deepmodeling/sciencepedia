{"hands_on_practices": [{"introduction": "A core task in applying information theory is to first precisely define the set of all possible outcomes. This exercise [@problem_id:1629232] hones this skill by requiring you to determine the size of a sample space defined by specific constraints. By calculating the Hartley entropy for a hypothetical security protocol, you will practice the foundational step of translating real-world rules into a quantifiable measure of uncertainty.", "problem": "A researcher is designing a basic security protocol for a network of environmental sensors. The protocol, named 'Protocol Gamma,' generates a temporary access key for each data transmission. The key is a single integer selected uniformly at random from a specific set of allowed values. According to the design specifications for Protocol Gamma, the integer key must be chosen from the range of 1 to 20, inclusive. Furthermore, a hardware constraint requires that the chosen integer must be a multiple of 4.\n\nAs part of the system analysis, you are asked to quantify the information content associated with the selection of a single key. Calculate the Hartley entropy (also known as max-entropy) for the process of generating a single key under Protocol Gamma. The Hartley entropy is a measure of uncertainty for a set of possible outcomes, and for this problem, it should be determined using a base-2 logarithm to be expressed in units of bits. Provide the final answer as a closed-form analytic expression.", "solution": "The problem asks for the Hartley entropy of a set of possible security keys. The Hartley entropy, $H_0$, for a set of $N$ equally likely outcomes is given by the formula:\n$$H_0 = \\log_{b}(N)$$\nThe problem specifies that the entropy should be expressed in bits, which implies the use of a base-2 logarithm ($b=2$). Therefore, the formula we will use is:\n$$H_0 = \\log_{2}(N)$$\nwhere $N$ is the total number of possible keys.\n\nOur first step is to determine the set of all possible keys and find its size, $N$. The problem states that a key is an integer that satisfies two conditions:\n1. It must be in the range [1, 20], inclusive.\n2. It must be a multiple of 4.\n\nLet's list all integers in the range from 1 to 20 that are multiples of 4. A number $k$ is a multiple of 4 if $k = 4m$ for some integer $m$.\nWe need to find integers $k$ such that $1 \\le k \\le 20$ and $k=4m$.\n\n- For $m=1$, $k = 4 \\times 1 = 4$. This is in the range.\n- For $m=2$, $k = 4 \\times 2 = 8$. This is in the range.\n- For $m=3$, $k = 4 \\times 3 = 12$. This is in the range.\n- For $m=4$, $k = 4 \\times 4 = 16$. This is in the range.\n- For $m=5$, $k = 4 \\times 5 = 20$. This is in the range.\n- For $m=6$, $k = 4 \\times 6 = 24$. This is outside the range $[1, 20]$.\n\nThus, the set of all possible keys, $S$, is:\n$$S = \\{4, 8, 12, 16, 20\\}$$\n\nThe number of possible outcomes, $N$, is the number of elements in the set $S$. By counting the elements, we find:\n$$N = |S| = 5$$\n\nNow we can calculate the Hartley entropy by substituting $N=5$ into the formula:\n$$H_0 = \\log_{2}(5)$$\n\nThe problem asks for a closed-form analytic expression. The expression $\\log_{2}(5)$ is a complete and exact analytic representation of the Hartley entropy in bits.", "answer": "$$\\boxed{\\log_{2}(5)}$$", "id": "1629232"}, {"introduction": "Hartley entropy provides a direct way to measure how adding constraints reduces the uncertainty within a system. In this practice [@problem_id:1629265], you will explore this concept by calculating the change in entropy on a chessboard when the available choices are restricted. This exercise demonstrates the tangible connection between imposing rules and the reduction of information required to describe an outcome.", "problem": "In the development of a chess-playing Artificial Intelligence (AI), the process of selecting a square on the board is analyzed using information theory. The Hartley entropy, which quantifies the information required to specify an outcome from a set of $N$ equally likely possibilities, is used for this analysis.\n\nConsider a standard $8 \\times 8$ chessboard.\nFirst, analyze the initial version of the AI, which can select any single square on the board with equal probability.\nNext, analyze a refined version where the AI's choice is restricted. In this version, the AI can only select a square that is not on the perimeter of the board (i.e., it cannot select a square in the first or last row, nor in the first or last column).\n\nCalculate the reduction in the Hartley entropy, in bits, that results from this refinement. Express your answer as a single closed-form analytic expression.", "solution": "Hartley entropy quantifies the information required to specify an outcome from a set of $N$ equally likely possibilities. When measured in bits, the Hartley entropy is given by\n$$\nH = \\log_{2}(N).\n$$\nFor a standard $8 \\times 8$ chessboard, the number of squares is\n$$\nN_{1} = 8 \\times 8 = 64,\n$$\nso the initial AI has entropy\n$$\nH_{1} = \\log_{2}(64).\n$$\nIn the refined version, the AI cannot choose squares on the perimeter. The interior squares form a $(8-2) \\times (8-2)$ grid, so\n$$\nN_{2} = 6 \\times 6 = 36,\n$$\nand the refined entropy is\n$$\nH_{2} = \\log_{2}(36).\n$$\nThe reduction in Hartley entropy is the difference:\n$$\n\\Delta H = H_{1} - H_{2} = \\log_{2}(64) - \\log_{2}(36) = \\log_{2}\\!\\left(\\frac{64}{36}\\right) = \\log_{2}\\!\\left(\\frac{16}{9}\\right).\n$$\nThis is a single closed-form analytic expression in bits.", "answer": "$$\\boxed{\\log_{2}\\!\\left(\\frac{16}{9}\\right)}$$", "id": "1629265"}, {"introduction": "The principles of combinatorics are deeply intertwined with information theory, as the method of counting outcomes directly determines a system's entropy. This problem [@problem_id:1629258] challenges you to compare the information content of two different code-generation schemes based on sampling with and without replacement. By analyzing this difference, you will gain a deeper appreciation for how underlying probabilistic structures influence entropy.", "problem": "A team of cryptographers is evaluating two algorithms for generating 3-character-long access codes. The character pool for these codes consists of 8 distinct, non-alphanumeric symbols.\n\nThe first algorithm, \"Method R,\" generates an ordered sequence of 3 characters where symbols can be repeated. This is analogous to sampling with replacement.\n\nThe second algorithm, \"Method U,\" generates an ordered sequence of 3 characters where each symbol in the sequence must be unique. This is analogous to sampling without replacement.\n\nThe uncertainty associated with a set of possible outcomes is measured by its Hartley entropy, defined as $H_0 = \\log_2(N)$, where $N$ is the total number of distinct outcomes. The unit for this entropy is bits.\n\nCalculate the absolute difference in Hartley entropy, in bits, between the set of all possible codes generated by Method R and the set of all possible codes generated by Method U. Present your answer as a single, simplified, closed-form analytic expression.", "solution": "Let $H_R$ be the Hartley entropy for the set of codes generated by Method R, and $H_U$ be the Hartley entropy for the set of codes generated by Method U. We are asked to find the absolute difference, $|\\Delta H| = |H_R - H_U|$.\n\nThe Hartley entropy is given by the formula $H_0 = \\log_2(N)$, where $N$ is the number of possible outcomes.\n\n**Step 1: Calculate the number of outcomes for Method R.**\nMethod R generates an ordered 3-character code from a pool of 8 symbols with replacement. For each of the 3 positions in the code, there are 8 possible choices.\nThe total number of distinct codes, $N_R$, is the product of the number of choices for each position:\n$$N_R = 8 \\times 8 \\times 8 = 8^3 = 512$$\n\n**Step 2: Calculate the Hartley entropy for Method R.**\nUsing the formula for Hartley entropy:\n$$H_R = \\log_2(N_R) = \\log_2(512) = \\log_2(2^9)$$\nUsing the logarithm property $\\log_a(b^c) = c \\log_a(b)$:\n$$H_R = 9 \\log_2(2) = 9 \\times 1 = 9 \\text{ bits}$$\n\n**Step 3: Calculate the number of outcomes for Method U.**\nMethod U generates an ordered 3-character code from a pool of 8 symbols without replacement. This is a permutation problem. The number of ways to choose an ordered sequence of $k=3$ items from a set of $n=8$ items without replacement is given by the permutation formula $P(n, k) = \\frac{n!}{(n-k)!}$.\n$$N_U = P(8, 3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6$$\n$$N_U = 336$$\n\n**Step 4: Calculate the Hartley entropy for Method U.**\nUsing the formula for Hartley entropy:\n$$H_U = \\log_2(N_U) = \\log_2(336)$$\n\n**Step 5: Calculate the absolute difference in entropy.**\nThe absolute difference is $|\\Delta H| = |H_R - H_U|$. Since $N_R = 512 > N_U = 336$, it follows that $H_R > H_U$. Therefore, the absolute difference is simply $H_R - H_U$.\n$$\\Delta H = H_R - H_U = 9 - \\log_2(336)$$\nTo express this as a single term, we can use the property of logarithms that $\\log_a(b) - \\log_a(c) = \\log_a\\left(\\frac{b}{c}\\right)$. We first write $9$ as a logarithm with base 2: $9 = 9 \\log_2(2) = \\log_2(2^9) = \\log_2(512)$.\n$$\\Delta H = \\log_2(512) - \\log_2(336) = \\log_2\\left(\\frac{512}{336}\\right)$$\nNow, we simplify the fraction:\n$$\\frac{512}{336} = \\frac{256}{168} = \\frac{128}{84} = \\frac{64}{42} = \\frac{32}{21}$$\nTherefore, the absolute difference in entropy is:\n$$\\Delta H = \\log_2\\left(\\frac{32}{21}\\right)$$\nThis is the final, simplified, closed-form analytic expression.", "answer": "$$\\boxed{\\log_{2}\\left(\\frac{32}{21}\\right)}$$", "id": "1629258"}]}