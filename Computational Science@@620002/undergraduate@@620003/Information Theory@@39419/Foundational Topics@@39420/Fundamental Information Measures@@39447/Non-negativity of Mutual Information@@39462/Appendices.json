{"hands_on_practices": [{"introduction": "The most direct way to understand mutual information is to calculate it. This first exercise provides foundational practice in computing mutual information directly from a given joint probability distribution. By working through the data from a hypothetical sensor system [@problem_id:1643384], you will apply the core formula for $I(X;Y)$ and numerically verify its non-negativity, a cornerstone property of mutual information.", "problem": "Consider a simplified model of a monitoring system for an industrial chemical reactor. Two binary sensors are used: one for temperature and one for pressure. Let $X$ be the random variable representing the state of the temperature sensor, and $Y$ be the random variable representing the state of the pressure sensor. Both sensors output a value of $0$ for a \"normal\" reading and $1$ for an \"alert\" reading. Over a long period of observation, the system's behavior is characterized by the following joint probability mass function, $p(x,y) = P(X=x, Y=y)$:\n\n$p(0,0) = 0.10$\n$p(0,1) = 0.20$\n$p(1,0) = 0.40$\n$p(1,1) = 0.30$\n\nCalculate the mutual information, $I(X;Y)$, between the two sensor readings. All calculations should use the base-2 logarithm. Express your answer in bits, rounded to three significant figures.", "solution": "We are asked to compute the mutual information between two binary random variables $X$ and $Y$ with joint pmf $p(x,y)$. By definition, using base-2 logarithms, the mutual information is\n$$\nI(X;Y)=\\sum_{x\\in\\{0,1\\}}\\sum_{y\\in\\{0,1\\}} p(x,y)\\,\\log_{2}\\!\\left(\\frac{p(x,y)}{p_{X}(x)\\,p_{Y}(y)}\\right).\n$$\nFirst compute the marginal distributions:\n$$\np_{X}(0)=p(0,0)+p(0,1)=0.10+0.20=0.30,\\quad p_{X}(1)=p(1,0)+p(1,1)=0.40+0.30=0.70,\n$$\n$$\np_{Y}(0)=p(0,0)+p(1,0)=0.10+0.40=0.50,\\quad p_{Y}(1)=p(0,1)+p(1,1)=0.20+0.30=0.50.\n$$\nNow evaluate each term in the sum:\n- For $(x,y)=(0,0)$:\n$$\n\\frac{p(0,0)}{p_{X}(0)p_{Y}(0)}=\\frac{0.10}{0.30\\times 0.50}=\\frac{2}{3},\\quad \\text{contribution}=0.10\\,\\log_{2}\\!\\left(\\frac{2}{3}\\right)\\approx -0.0584963.\n$$\n- For $(x,y)=(0,1)$:\n$$\n\\frac{p(0,1)}{p_{X}(0)p_{Y}(1)}=\\frac{0.20}{0.30\\times 0.50}=\\frac{4}{3},\\quad \\text{contribution}=0.20\\,\\log_{2}\\!\\left(\\frac{4}{3}\\right)\\approx 0.0830075.\n$$\n- For $(x,y)=(1,0)$:\n$$\n\\frac{p(1,0)}{p_{X}(1)p_{Y}(0)}=\\frac{0.40}{0.70\\times 0.50}=\\frac{8}{7},\\quad \\text{contribution}=0.40\\,\\log_{2}\\!\\left(\\frac{8}{7}\\right)\\approx 0.0770580.\n$$\n- For $(x,y)=(1,1)$:\n$$\n\\frac{p(1,1)}{p_{X}(1)p_{Y}(1)}=\\frac{0.30}{0.70\\times 0.50}=\\frac{6}{7},\\quad \\text{contribution}=0.30\\,\\log_{2}\\!\\left(\\frac{6}{7}\\right)\\approx -0.0667177.\n$$\nSumming the four contributions gives\n$$\nI(X;Y)\\approx -0.0584963+0.0830075+0.0770580-0.0667177=0.0348516\\ \\text{bits}.\n$$\nRounding to three significant figures yields $0.0349$ bits.", "answer": "$$\\boxed{0.0349}$$", "id": "1643384"}, {"introduction": "While the total average mutual information $I(X;Y)$ is always non-negative, the situation is more nuanced for individual outcomes. This practice introduces the concept of *pointwise mutual information*, $i(x;y)$, which captures the information that a single outcome $Y=y$ provides about a single outcome $X=x$. By exploring a system where some outcomes are less likely to occur together than by chance [@problem_id:1643401], you will see how $i(x;y)$ can be negative and how these contributions combine to yield a non-negative total, deepening your insight into the nature of statistical dependency.", "problem": "Consider a simple noisy communication channel with input $X$ and output $Y$. The set of possible inputs (the input alphabet) is $\\mathcal{X} = \\{A_1, A_2\\}$, and the set of possible outputs (the output alphabet) is $\\mathcal{Y} = \\{B_1, B_2\\}$. The behavior of the channel is completely characterized by the joint probability distribution $p(x, y)$ of the input-output pairs, which is given as follows:\n- $p(X=A_1, Y=B_1) = \\frac{3}{8}$\n- $p(X=A_1, Y=B_2) = \\frac{1}{8}$\n- $p(X=A_2, Y=B_1) = \\frac{1}{8}$\n- $p(X=A_2, Y=B_2) = \\frac{3}{8}$\n\nThe pointwise mutual information, $i(x;y)$, quantifies the information that observing outcome $y$ provides about outcome $x$. It can be negative for specific outcomes $(x,y)$, indicating that their joint occurrence is less likely than if they were independent. This happens even when the total average mutual information, $I(X;Y)$, remains non-negative.\n\nFor the system defined above, which of the following options correctly identifies an outcome pair $(x,y)$ with negative pointwise mutual information and gives the correct value for the total mutual information $I(X;Y)$? Express the mutual information in bits, as a symbolic expression involving base-2 logarithms.\n\nA. Pair: $(A_1, B_1)$; $I(X;Y) = \\frac{3}{4}\\log_{2}(3) - 1$\n\nB. Pair: $(A_2, B_1)$; $I(X;Y) = 2 - \\frac{3}{4}\\log_{2}(3)$\n\nC. Pair: $(A_1, B_1)$; $I(X;Y) = 0$\n\nD. Pair: $(A_2, B_1)$; $I(X;Y) = \\frac{3}{4}\\log_{2}(3) - 1$\n\nE. Pair: $(A_1, B_2)$; $I(X;Y) = 1$", "solution": "We are given the joint pmf:\n$$\np(A_{1},B_{1})=\\frac{3}{8},\\quad p(A_{1},B_{2})=\\frac{1}{8},\\quad p(A_{2},B_{1})=\\frac{1}{8},\\quad p(A_{2},B_{2})=\\frac{3}{8}.\n$$\nFirst compute marginals:\n$$\np_{X}(A_{1})=\\frac{3}{8}+\\frac{1}{8}=\\frac{1}{2},\\quad p_{X}(A_{2})=\\frac{1}{2},\\quad\np_{Y}(B_{1})=\\frac{3}{8}+\\frac{1}{8}=\\frac{1}{2},\\quad p_{Y}(B_{2})=\\frac{1}{2}.\n$$\nFor any pair, $p_{X}(x)p_{Y}(y)=\\frac{1}{4}$. The pointwise mutual information is\n$$\ni(x;y)=\\log_{2}\\!\\left(\\frac{p(x,y)}{p_{X}(x)p_{Y}(y)}\\right).\n$$\nEvaluate for each pair:\n- For $(A_{1},B_{1})$ and $(A_{2},B_{2})$:\n$$\ni=\\log_{2}\\!\\left(\\frac{3/8}{1/4}\\right)=\\log_{2}\\!\\left(\\frac{3}{2}\\right)>0.\n$$\n- For $(A_{1},B_{2})$ and $(A_{2},B_{1})$:\n$$\ni=\\log_{2}\\!\\left(\\frac{1/8}{1/4}\\right)=\\log_{2}\\!\\left(\\frac{1}{2}\\right)=-1<0.\n$$\nThus any of $(A_{1},B_{2})$ or $(A_{2},B_{1})$ has negative pointwise mutual information.\n\nNow compute the total mutual information:\n$$\nI(X;Y)=\\sum_{x,y}p(x,y)\\log_{2}\\!\\left(\\frac{p(x,y)}{p_{X}(x)p_{Y}(y)}\\right)\n=2\\cdot\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{2}\\right)+2\\cdot\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{2}\\right).\n$$\nThis simplifies to\n$$\nI(X;Y)=\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right)-\\frac{1}{4}\n=\\frac{3}{4}\\bigl(\\log_{2}(3)-1\\bigr)-\\frac{1}{4}\n=\\frac{3}{4}\\log_{2}(3)-1.\n$$\nTherefore, a correct option is the one that lists a negative pair such as $(A_{2},B_{1})$ and gives $I(X;Y)=\\frac{3}{4}\\log_{2}(3)-1$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1643401"}, {"introduction": "Mutual information can also be understood as the reduction in uncertainty about one variable gained from observing another. This exercise shifts our perspective to the equivalent and powerful formula $I(X;Y) = H(X) - H(X|Y)$. Using the intuitive example of coin flips [@problem_id:1643375], you will calculate the information that the sum of two flips provides about the outcome of one of the individual flips, solidifying your understanding of mutual information as a measure of shared information content.", "problem": "Consider a simple system where two distinct events are modeled as independent, fair coin flips. Let the random variable $X_1$ represent the outcome of the first flip and $X_2$ represent the outcome of the second flip. For both variables, an outcome of tails is assigned the numerical value 0, and an outcome of heads is assigned the value 1.\n\nA new random variable, $Y$, is defined as the sum of the outcomes of these two flips, i.e., $Y = X_1 + X_2$. We are interested in the amount of information that observing the sum $Y$ provides about the outcome of the first flip, $X_1$. This quantity is formally known as the mutual information between $X_1$ and $Y$, denoted as $I(X_1; Y)$.\n\nCalculate the value of the mutual information $I(X_1; Y)$. In all calculations, use the logarithm base 2, such that the final answer is expressed in units of bits. Report your numerical answer rounded to three significant figures.", "solution": "Let $X_{1}$ and $X_{2}$ be independent fair coin flips with values in $\\{0,1\\}$, and let $Y=X_{1}+X_{2}\\in\\{0,1,2\\}$. Using base-2 logarithms, the mutual information is\n$$\nI(X_{1};Y)=H(X_{1})-H(X_{1}\\mid Y).\n$$\nSince $X_{1}$ is a fair coin, its entropy is\n$$\nH(X_{1})=1.\n$$\nFirst compute the distribution of $Y$:\n$$\n\\mathbb{P}(Y=0)=\\mathbb{P}(0,0)=\\tfrac{1}{4},\\quad \\mathbb{P}(Y=1)=\\mathbb{P}(0,1)+\\mathbb{P}(1,0)=\\tfrac{1}{2},\\quad \\mathbb{P}(Y=2)=\\mathbb{P}(1,1)=\\tfrac{1}{4}.\n$$\nNext compute $H(X_{1}\\mid Y=y)$ for each $y$:\n- If $y=0$, then $X_{1}=0$ with probability $1$, so $H(X_{1}\\mid Y=0)=0$.\n- If $y=2$, then $X_{1}=1$ with probability $1$, so $H(X_{1}\\mid Y=2)=0$.\n- If $y=1$, then $\\mathbb{P}(X_{1}=1\\mid Y=1)=\\tfrac{1}{2}$ and $\\mathbb{P}(X_{1}=0\\mid Y=1)=\\tfrac{1}{2}$, so\n$$\nH(X_{1}\\mid Y=1)=-\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}-\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}=1.\n$$\nTherefore,\n$$\nH(X_{1}\\mid Y)=\\sum_{y}\\mathbb{P}(Y=y)\\,H(X_{1}\\mid Y=y)=\\tfrac{1}{4}\\cdot 0+\\tfrac{1}{2}\\cdot 1+\\tfrac{1}{4}\\cdot 0=\\tfrac{1}{2}.\n$$\nSubstituting back gives\n$$\nI(X_{1};Y)=H(X_{1})-H(X_{1}\\mid Y)=1-\\tfrac{1}{2}=\\tfrac{1}{2}.\n$$\nExpressed numerically in bits and rounded to three significant figures, this is $0.500$.", "answer": "$$\\boxed{0.500}$$", "id": "1643375"}]}