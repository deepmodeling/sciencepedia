{"hands_on_practices": [{"introduction": "We begin our hands-on exploration by grounding the abstract concept of entropy in a tangible application. This first practice asks you to calculate the Shannon entropy for a binary source, which represents the fundamental theoretical limit for lossless data compression. Mastering this calculation is the first step toward understanding how information theory quantifies uncertainty and sets benchmarks for a wide range of engineering designs [@problem_id:1604155].", "problem": "A digital communication system transmits data as a stream of binary symbols, either '0' or '1'. Due to the nature of the information being encoded, the symbols are not equally likely. A statistical analysis of a very long transmission reveals that the probability of a '1' appearing, which we denote as $p$, is $p = \\frac{1}{8}$. An engineer is tasked with designing an optimal lossless compression scheme for this data source. The theoretical limit for such compression is given by the average information content per symbol. Calculate this fundamental lower bound on the average number of bits required to represent each symbol from this source.\n\nExpress your answer in bits per symbol, rounded to three significant figures.", "solution": "The theoretical lower bound on the average number of bits per source symbol for optimal lossless compression is the Shannon entropy of the binary source. For a binary symbol $X$ with $P(X=1)=p$ and $P(X=0)=1-p$, the entropy in bits per symbol is\n$$\nH(X)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nGiven $p=\\frac{1}{8}$ and $1-p=\\frac{7}{8}$, we compute\n$$\nH(X)=-\\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right)-\\frac{7}{8}\\log_{2}\\left(\\frac{7}{8}\\right).\n$$\nUse $\\log_{2}\\left(\\frac{1}{8}\\right)=\\log_{2}\\left(2^{-3}\\right)=-3$ and $\\log_{2}\\left(\\frac{7}{8}\\right)=\\log_{2}(7)-3$ to simplify:\n$$\nH(X)=-\\frac{1}{8}(-3)-\\frac{7}{8}\\left(\\log_{2}(7)-3\\right)=\\frac{3}{8}-\\frac{7}{8}\\log_{2}(7)+\\frac{21}{8}.\n$$\nCombine constants:\n$$\nH(X)=3-\\frac{7}{8}\\log_{2}(7).\n$$\nFor a numerical value, use the change-of-base formula $\\log_{2}(7)=\\frac{\\ln(7)}{\\ln(2)}$:\n$$\n\\log_{2}(7)\\approx 2.807354922,\\quad \\Rightarrow\\quad H(X)\\approx 3-\\frac{7}{8}\\times 2.807354922\\approx 0.543564443.\n$$\nRounding to three significant figures gives $0.544$ bits per symbol.", "answer": "$$\\boxed{0.544}$$", "id": "1604155"}, {"introduction": "Moving beyond direct calculation to conceptual insight, this next exercise challenges you to reason about the nature of uncertainty itself. Instead of plugging values into a formula, you will use the key properties of the binary entropy function, $H(p)$, to compare the entropy of different sources. This practice is designed to build a strong intuitive understanding of how entropy varies with probability, highlighting its symmetry and the fact that maximum uncertainty occurs when outcomes are equally likely [@problem_id:1604183].", "problem": "An information theorist is analyzing the fundamental limits of data compression for four different, independent binary sources. Each source produces a stream of '0's and '1's. The behavior of each source is characterized by the probability of it emitting a '1'. The Shannon entropy of a source determines the theoretical minimum average number of bits required to represent each symbol from that source.\n\nFor a binary source where the probability of emitting a '1' is $p$ and the probability of emitting a '0' is $1-p$, the entropy $H(p)$ in bits per symbol is given by the binary entropy function:\n$$H(p) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p)$$\nA lower entropy implies the source is more structured and predictable, allowing for better compression.\n\nThe probabilities of emitting a '1' for the four sources are as follows:\n- Source A: $p_A = 0.02$\n- Source B: $p_B = 0.48$\n- Source C: $p_C = 0.52$\n- Source D: $p_D = 0.99$\n\nLet $S_A, S_B, S_C,$ and $S_D$ be the Shannon entropies for sources A, B, C, and D, respectively. Your task is to rank these entropies. Which of the following options correctly orders the entropies from smallest to largest?\n\nA. $S_D < S_A < S_B < S_C$\n\nB. $S_A < S_D < S_B < S_C$\n\nC. $S_D < S_A < S_B = S_C$\n\nD. $S_A = S_D < S_B = S_C$\n\nE. $S_A < S_D < S_C < S_B$", "solution": "We are given the binary entropy function in bits per symbol for a Bernoulli source:\n$$\nH(p) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p).\n$$\nStep 1: Symmetry. Observe that\n$$\nH(1-p) = -(1-p)\\log_{2}(1-p) - p \\log_{2}(p) = H(p),\n$$\nso $H$ is symmetric about $p=\\frac{1}{2}$.\n\nStep 2: Monotonicity. Rewrite using natural logarithms:\n$$\nH(p) = -\\frac{p \\ln p + (1-p)\\ln(1-p)}{\\ln 2}.\n$$\nDifferentiate with respect to $p$:\n$$\nH'(p) = -\\frac{\\ln p - \\ln(1-p)}{\\ln 2} = -\\frac{\\ln\\!\\left(\\frac{p}{1-p}\\right)}{\\ln 2}.\n$$\nHence $H'(p)>0$ when $0<p<\\frac{1}{2}$ and $H'(p)<0$ when $\\frac{1}{2}<p<1$. Therefore, $H$ increases on $(0,\\frac{1}{2}]$, decreases on $[\\frac{1}{2},1)$, and attains its maximum at $p=\\frac{1}{2}$.\n\nStep 3: Apply to the given probabilities.\n- For $p_{B}=0.48$ and $p_{C}=0.52$, by symmetry $H(0.48)=H(0.52)$, so $S_{B}=S_{C}$, and both are near the maximum.\n- For $p_{D}=0.99$, by symmetry $H(0.99)=H(0.01)$.\n- Compare $S_{A}=H(0.02)$ and $S_{D}=H(0.99)=H(0.01)$. Since $H$ is strictly increasing on $(0,\\frac{1}{2})$, we have $H(0.01)<H(0.02)$, hence $S_{D}<S_{A}$.\n\nCombining, we obtain the ordering\n$$\nS_{D} < S_{A} < S_{B} = S_{C}.\n$$\nThis corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1604183"}, {"introduction": "Our final practice tackles a common and practical challenge in signal analysis: the inverse problem. While it is straightforward to calculate entropy from a known probability, how can we determine the underlying source probability $p$ if we only know its measured entropy $c$? This exercise guides you through deriving the Newton-Raphson iterative method to solve the equation $H(p) = c$, demonstrating a powerful technique for inverting functions that lack simple analytical solutions [@problem_id:1604135].", "problem": "In the field of information theory, the binary entropy function is a fundamental measure of the uncertainty associated with a binary random variable. Consider a discrete memoryless source that generates binary symbols, where the probability of generating a '1' is $p$ and a '0' is $1-p$. The entropy of this source, measured in nats, is given by the function:\n$$H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$$\nfor $p \\in (0, 1)$.\n\nIn a practical scenario, we might measure the entropy of a source to be a constant value $c$ (where $0 < c \\le \\ln(2)$) and wish to determine the underlying source probability $p$. Since the equation $H(p) = c$ cannot be solved for $p$ analytically using elementary functions, numerical methods are required.\n\nYour task is to derive the iterative update rule for finding $p$ by applying the Newton-Raphson method. Let $p_n$ be the estimate of $p$ at the $n$-th iteration. Find the expression for the next estimate, $p_{n+1}$, as a function of $p_n$ and the target entropy $c$.", "solution": "We aim to solve $H(p)=c$ for $p \\in (0,1)$, where $H(p)=-p \\ln(p)-(1-p)\\ln(1-p)$ and $0<c \\le \\ln(2)$. Define the root-finding function\n$$\nf(p)=H(p)-c=-p \\ln(p)-(1-p)\\ln(1-p)-c.\n$$\nTo apply the Newton-Raphson method, we need $f'(p)$. Differentiate $H(p)$ with respect to $p$:\n- For the first term, using the product rule and derivative of $\\ln(p)$, we have\n$$\n\\frac{d}{dp}\\bigl[-p \\ln(p)\\bigr]=-(\\ln(p)+1).\n$$\n- For the second term, let $g(p)=(1-p)\\ln(1-p)$. Then\n$$\ng'(p)=-\\ln(1-p)-1,\n$$\nso\n$$\n\\frac{d}{dp}\\bigl[-(1-p)\\ln(1-p)\\bigr]=-\\bigl(-\\ln(1-p)-1\\bigr)=\\ln(1-p)+1.\n$$\nTherefore,\n$$\nH'(p)=-(\\ln(p)+1)+\\bigl(\\ln(1-p)+1\\bigr)=\\ln(1-p)-\\ln(p)=\\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\nHence,\n$$\nf'(p)=H'(p)=\\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\nThe Newton-Raphson update for solving $f(p)=0$ is\n$$\np_{n+1}=p_{n}-\\frac{f(p_{n})}{f'(p_{n})}\n= p_{n}-\\frac{-p_{n}\\ln(p_{n})-(1-p_{n})\\ln(1-p_{n})-c}{\\ln\\!\\left(\\frac{1-p_{n}}{p_{n}}\\right)}.\n$$\nEquivalently, this can be written as\n$$\np_{n+1}=p_{n}+\\frac{c+p_{n}\\ln(p_{n})+(1-p_{n})\\ln(1-p_{n})}{\\ln\\!\\left(\\frac{1-p_{n}}{p_{n}}\\right)}.\n$$\nBoth forms express the next iterate $p_{n+1}$ purely in terms of $p_{n}$ and $c$.", "answer": "$$\\boxed{p_{n+1}=p_{n}-\\frac{-p_{n}\\ln(p_{n})-(1-p_{n})\\ln(1-p_{n})-c}{\\ln\\!\\left(\\frac{1-p_{n}}{p_{n}}\\right)}}$$", "id": "1604135"}]}