{"hands_on_practices": [{"introduction": "The most fundamental way to understand mutual information is to compute it directly from a joint probability distribution. This exercise uses a hypothetical scenario from a sociological study to provide a concrete, data-driven application of the formula $I(X;Y) = \\sum_{x,y} p(x,y) \\log_{2}\\frac{p(x,y)}{p(x)p(y)}$. By working through this problem [@problem_id:1662184], you will practice computing marginal distributions from a joint table and then applying the definition to quantify the statistical dependence between two variables.", "problem": "A sociological study investigates the statistical relationship between an individual's highest level of education and their annual income bracket within a specific population. Let $E$ be a discrete random variable representing the education level, with outcomes $\\{e_1, e_2, e_3\\}$ corresponding to \"High School or less\", \"Bachelor's Degree\", and \"Graduate Degree\", respectively. Let $I$ be a discrete random variable representing the income bracket, with outcomes $\\{i_1, i_2, i_3\\}$ corresponding to \"Low\", \"Medium\", and \"High\", respectively.\n\nBased on extensive survey data, the joint probability mass function $P(I=i, E=e)$ for these two variables is found to be as follows:\n\n$P(I=i_1, E=e_1) = 0.20$\n$P(I=i_1, E=e_2) = 0.10$\n$P(I=i_1, E=e_3) = 0.02$\n\n$P(I=i_2, E=e_1) = 0.15$\n$P(I=i_2, E=e_2) = 0.20$\n$P(I=i_2, E=e_3) = 0.10$\n\n$P(I=i_3, E=e_1) = 0.05$\n$P(I=i_3, E=e_2) = 0.10$\n$P(I=i_3, E=e_3) = 0.08$\n\nCalculate the value of the mutual information $I(I; E)$ between the two variables. All logarithms should be taken in base 2. Express your answer in bits, rounded to four significant figures.", "solution": "We are given the joint pmf values $P(I=i_{k},E=e_{j})$ for $k,j\\in\\{1,2,3\\}$. First, compute the marginals by summing the joint probabilities over the other variable:\n$$\nP(I=i_{1})=0.20+0.10+0.02=0.32,\\quad P(I=i_{2})=0.15+0.20+0.10=0.45,\\quad P(I=i_{3})=0.05+0.10+0.08=0.23,\n$$\n$$\nP(E=e_{1})=0.20+0.15+0.05=0.40,\\quad P(E=e_{2})=0.10+0.20+0.10=0.40,\\quad P(E=e_{3})=0.02+0.10+0.08=0.20.\n$$\nBy definition, the mutual information in bits is\n$$\nI(I;E)=\\sum_{k=1}^{3}\\sum_{j=1}^{3} P(I=i_{k},E=e_{j})\\,\\log_{2}\\!\\left(\\frac{P(I=i_{k},E=e_{j})}{P(I=i_{k})\\,P(E=e_{j})}\\right).\n$$\nCompute each contribution $c_{kj}=P(i_{k},e_{j})\\log_{2}\\!\\big(P(i_{k},e_{j})/[P(i_{k})P(e_{j})]\\big)$:\n\nFor $i_{1}$:\n- $(i_{1},e_{1})$: $p=0.20$, denominator $0.32\\cdot 0.40=0.128$, ratio $0.20/0.128=25/16$, so\n$$\nc_{11}=0.20\\log_{2}\\!\\left(\\frac{25}{16}\\right)=0.20\\cdot 0.6438561897747247\\approx 0.1287712379549449.\n$$\n- $(i_{1},e_{2})$: $p=0.10$, denominator $0.128$, ratio $0.10/0.128=25/32$, so\n$$\nc_{12}=0.10\\log_{2}\\!\\left(\\frac{25}{32}\\right)=0.10\\cdot(-0.3561438102252753)\\approx -0.0356143810225275.\n$$\n- $(i_{1},e_{3})$: $p=0.02$, denominator $0.32\\cdot 0.20=0.064$, ratio $0.02/0.064=5/16$, so\n$$\nc_{13}=0.02\\log_{2}\\!\\left(\\frac{5}{16}\\right)=0.02\\cdot(-1.6780719051126376)\\approx -0.0335614381022528.\n$$\nRow sum for $i_{1}$:\n$$\nc_{11}+c_{12}+c_{13}\\approx 0.0595954188301647.\n$$\n\nFor $i_{2}$:\n- $(i_{2},e_{1})$: $p=0.15$, denominator $0.45\\cdot 0.40=0.18$, ratio $0.15/0.18=5/6$, so\n$$\nc_{21}=0.15\\log_{2}\\!\\left(\\frac{5}{6}\\right)=0.15\\cdot(-0.2630344058337938)\\approx -0.0394551608750691.\n$$\n- $(i_{2},e_{2})$: $p=0.20$, denominator $0.18$, ratio $0.20/0.18=10/9$, so\n$$\nc_{22}=0.20\\log_{2}\\!\\left(\\frac{10}{9}\\right)=0.20\\cdot 0.1520030934450500\\approx 0.0304006186890100.\n$$\n- $(i_{2},e_{3})$: $p=0.10$, denominator $0.45\\cdot 0.20=0.09$, ratio $0.10/0.09=10/9$, so\n$$\nc_{23}=0.10\\log_{2}\\!\\left(\\frac{10}{9}\\right)=0.10\\cdot 0.1520030934450500\\approx 0.0152003093445050.\n$$\nRow sum for $i_{2}$:\n$$\nc_{21}+c_{22}+c_{23}\\approx 0.0061457671584459.\n$$\n\nFor $i_{3}$:\n- $(i_{3},e_{1})$: $p=0.05$, denominator $0.23\\cdot 0.40=0.092$, ratio $0.05/0.092=25/46$, so\n$$\nc_{31}=0.05\\log_{2}\\!\\left(\\frac{25}{46}\\right)\\approx 0.05\\cdot(-0.8797057662822882)\\approx -0.0439852883141144.\n$$\n- $(i_{3},e_{2})$: $p=0.10$, denominator $0.092$, ratio $0.10/0.092=25/23$, so\n$$\nc_{32}=0.10\\log_{2}\\!\\left(\\frac{25}{23}\\right)\\approx 0.10\\cdot 0.1202942337177110\\approx 0.0120294233717711.\n$$\n- $(i_{3},e_{3})$: $p=0.08$, denominator $0.23\\cdot 0.20=0.046$, ratio $0.08/0.046=40/23$, so\n$$\nc_{33}=0.08\\log_{2}\\!\\left(\\frac{40}{23}\\right)\\approx 0.08\\cdot 0.7983661388303490\\approx 0.0638692911064279.\n$$\nRow sum for $i_{3}$:\n$$\nc_{31}+c_{32}+c_{33}\\approx 0.0319134261640846.\n$$\n\nFinally, sum all contributions to obtain the mutual information in bits:\n$$\nI(I;E)\\approx 0.0595954188301647+0.0061457671584459+0.0319134261640846=0.0976546121526952.\n$$\nRounded to four significant figures, this is $0.09765$ bits.", "answer": "$$\\boxed{0.09765}$$", "id": "1662184"}, {"introduction": "What happens to mutual information when one variable is a deterministic function of another? This practice explores this important special case, where knowing the input variable $X$ leaves no uncertainty about the output variable $Y$. As you will see, this causes the conditional entropy $H(Y|X)$ to become zero, simplifying the mutual information to $I(X;Y) = H(Y)$ [@problem_id:1662186]. This result powerfully illustrates that the information shared between the two is precisely the information content, or entropy, of the output variable.", "problem": "Consider a perfectly fair 8-sided die with faces numbered sequentially from 1 to 8. Let the discrete random variable $X$ represent the outcome of a single roll of this die. A second binary random variable, $Y$, is determined by the outcome of $X$. If the value of $X$ is less than or equal to 3 (i.e., $X \\in \\{1, 2, 3\\}$), then $Y=1$. Otherwise, if $X > 3$, then $Y=0$.\n\nCalculate the mutual information, $I(X;Y)$, between these two random variables.\n\nProvide your answer as an exact analytic expression in units of bits. For all calculations involving logarithms, you must use base 2.", "solution": "Let $X$ be uniform on $\\{1,2,3,4,5,6,7,8\\}$, so $P(X=x)=\\frac{1}{8}$ for each $x$. Define $Y$ by $Y=1$ if $X\\in\\{1,2,3\\}$ and $Y=0$ otherwise, i.e., $Y$ is a deterministic function of $X$.\n\nBy definition, the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nSince $Y$ is a deterministic function of $X$, the conditional distribution $P(Y|X=x)$ is degenerate for every $x$, hence $H(Y|X=x)=0$ for all $x$, and thus $H(Y|X)=0$. Therefore,\n$$\nI(X;Y)=H(Y).\n$$\n\nWe compute the distribution of $Y$:\n$$\nP(Y=1)=P(X\\in\\{1,2,3\\})=\\frac{3}{8},\\qquad P(Y=0)=\\frac{5}{8}.\n$$\nThe entropy of $Y$ in bits is\n$$\nH(Y)=-\\sum_{y\\in\\{0,1\\}} P(Y=y)\\log_{2}P(Y=y)\n= -\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right) - \\frac{5}{8}\\log_{2}\\!\\left(\\frac{5}{8}\\right).\n$$\nHence,\n$$\nI(X;Y) = -\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right) - \\frac{5}{8}\\log_{2}\\!\\left(\\frac{5}{8}\\right)\\ \\text{bits}.\n$$", "answer": "$$\\boxed{-\\frac{3}{8}\\log_{2}\\left(\\frac{3}{8}\\right)-\\frac{5}{8}\\log_{2}\\left(\\frac{5}{8}\\right)}$$", "id": "1662186"}, {"introduction": "The symmetry of mutual information, $I(X;Y) = I(Y;X)$, is a cornerstone of the concept, yet it might not be immediately intuitive. This exercise provides a direct, hands-on verification of this property. You will calculate the mutual information between a variable $X$ and a function of it, $Y=f(X)$, first as the reduction in uncertainty of $X$ given $Y$ ($H(X)-H(X|Y)$), and then as the reduction in uncertainty of $Y$ given $X$ ($H(Y)-H(Y|X)$). By demonstrating that both paths lead to the same result [@problem_id:1662217], we gain a much deeper appreciation for this elegant symmetry.", "problem": "Consider a discrete random variable $X$ that is uniformly distributed over the set of integers $\\mathcal{X} = \\{-3, -2, -1, 0, 1, 2, 3\\}$. A second random variable $Y$ is generated from $X$ according to the deterministic function $Y = f(X)$, where $f(X) = X \\pmod 3$. The modulo operation is defined as $a \\pmod n = a - n \\lfloor a/n \\rfloor$, where $\\lfloor \\cdot \\rfloor$ is the floor function. Calculate the mutual information $I(X;Y)$ between the two variables. Express your answer as a single symbolic expression in terms of logarithms with base 2. All entropies should be calculated using logarithms to base 2 (in units of bits).", "solution": "The mutual information $I(X;Y)$ quantifies the reduction in uncertainty of one random variable due to the knowledge of another. It is symmetric, meaning $I(X;Y) = I(Y;X)$. We can calculate it using either of two equivalent formulas:\n1. $I(X;Y) = H(X) - H(X|Y)$\n2. $I(X;Y) = H(Y) - H(Y|X)$\n\nWe will calculate the result using the first formula and then verify it using the second formula to explicitly demonstrate the symmetry for this case.\n\nFirst, let's analyze the properties of the random variables $X$ and $Y$.\nThe random variable $X$ is uniformly distributed over the set $\\mathcal{X} = \\{-3, -2, -1, 0, 1, 2, 3\\}$. The size of this set is $|\\mathcal{X}|=7$. The probability of any specific outcome is $P(X=x) = \\frac{1}{7}$ for all $x \\in \\mathcal{X}$.\n\nThe random variable $Y$ is defined by $Y = X \\pmod 3$. Let's find the possible values of $Y$ (the set $\\mathcal{Y}$) and their probabilities.\n- $X = -3 \\implies Y = -3 \\pmod 3 = 0$\n- $X = -2 \\implies Y = -2 \\pmod 3 = 1$\n- $X = -1 \\implies Y = -1 \\pmod 3 = 2$\n- $X = 0 \\implies Y = 0 \\pmod 3 = 0$\n- $X = 1 \\implies Y = 1 \\pmod 3 = 1$\n- $X = 2 \\implies Y = 2 \\pmod 3 = 2$\n- $X = 3 \\implies Y = 3 \\pmod 3 = 0$\n\nThe sample space for $Y$ is $\\mathcal{Y} = \\{0, 1, 2\\}$.\nThe probabilities for the values of $Y$ are:\n- $P(Y=0) = P(X=-3) + P(X=0) + P(X=3) = \\frac{1}{7} + \\frac{1}{7} + \\frac{1}{7} = \\frac{3}{7}$.\n- $P(Y=1) = P(X=-2) + P(X=1) = \\frac{1}{7} + \\frac{1}{7} = \\frac{2}{7}$.\n- $P(Y=2) = P(X=-1) + P(X=2) = \\frac{1}{7} + \\frac{1}{7} = \\frac{2}{7}$.\n\n**Calculation using $I(X;Y) = H(X) - H(X|Y)$**\n\nFirst, we calculate the entropy of $X$, denoted $H(X)$. Since $X$ follows a uniform distribution over 7 possible outcomes:\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} P(X=x) \\log_{2}(P(X=x)) = -7 \\times \\left(\\frac{1}{7} \\log_{2}\\left(\\frac{1}{7}\\right)\\right) = -\\log_{2}\\left(\\frac{1}{7}\\right) = \\log_{2}(7)$$\n\nNext, we calculate the conditional entropy $H(X|Y)$, which is the expected value of the entropy of $X$ conditioned on the values of $Y$:\n$$H(X|Y) = \\sum_{y \\in \\mathcal{Y}} P(Y=y) H(X|Y=y)$$\nWe need to find the conditional entropies for each value of $y$.\n- For $Y=0$: The possible values of $X$ are $\\{-3, 0, 3\\}$. Given $Y=0$, $X$ is uniformly distributed over these 3 values, so $P(X=x|Y=0) = 1/3$ for $x \\in \\{-3, 0, 3\\}$.\n$$H(X|Y=0) = - \\sum_{x \\in \\{-3,0,3\\}} \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) = -3 \\times \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) = \\log_{2}(3)$$\n- For $Y=1$: The possible values of $X$ are $\\{-2, 1\\}$. Given $Y=1$, $X$ is uniformly distributed over these 2 values, so $P(X=x|Y=1) = 1/2$ for $x \\in \\{-2, 1\\}$.\n$$H(X|Y=1) = - \\sum_{x \\in \\{-2,1\\}} \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) = -2 \\times \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) = \\log_{2}(2) = 1$$\n- For $Y=2$: The possible values of $X$ are $\\{-1, 2\\}$. Given $Y=2$, $X$ is uniformly distributed over these 2 values, so $P(X=x|Y=2) = 1/2$ for $x \\in \\{-1, 2\\}$.\n$$H(X|Y=2) = - \\sum_{x \\in \\{-1,2\\}} \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) = -2 \\times \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) = \\log_{2}(2) = 1$$\n\nNow, we compute the total conditional entropy $H(X|Y)$:\n$$H(X|Y) = P(Y=0)H(X|Y=0) + P(Y=1)H(X|Y=1) + P(Y=2)H(X|Y=2)$$\n$$H(X|Y) = \\left(\\frac{3}{7}\\right) \\log_{2}(3) + \\left(\\frac{2}{7}\\right) (1) + \\left(\\frac{2}{7}\\right) (1) = \\frac{3}{7}\\log_{2}(3) + \\frac{4}{7}$$\n\nFinally, we calculate the mutual information:\n$$I(X;Y) = H(X) - H(X|Y) = \\log_{2}(7) - \\left( \\frac{3}{7}\\log_{2}(3) + \\frac{4}{7} \\right) = \\log_{2}(7) - \\frac{4}{7} - \\frac{3}{7}\\log_{2}(3)$$\n\n**Verification using $I(Y;X) = H(Y) - H(Y|X)$**\n\nFirst, we calculate the entropy of $Y$, denoted $H(Y)$:\n$$H(Y) = -\\sum_{y \\in \\mathcal{Y}} P(Y=y) \\log_{2}(P(Y=y))$$\n$$H(Y) = -\\left[ \\frac{3}{7}\\log_{2}\\left(\\frac{3}{7}\\right) + \\frac{2}{7}\\log_{2}\\left(\\frac{2}{7}\\right) + \\frac{2}{7}\\log_{2}\\left(\\frac{2}{7}\\right) \\right]$$\n$$H(Y) = -\\frac{3}{7}(\\log_{2}(3) - \\log_{2}(7)) - \\frac{4}{7}(\\log_{2}(2) - \\log_{2}(7))$$\n$$H(Y) = -\\frac{3}{7}\\log_{2}(3) + \\frac{3}{7}\\log_{2}(7) - \\frac{4}{7}(1) + \\frac{4}{7}\\log_{2}(7)$$\n$$H(Y) = \\frac{7}{7}\\log_{2}(7) - \\frac{4}{7} - \\frac{3}{7}\\log_{2}(3) = \\log_{2}(7) - \\frac{4}{7} - \\frac{3}{7}\\log_{2}(3)$$\n\nNext, we calculate the conditional entropy $H(Y|X)$. Since $Y$ is a deterministic function of $X$, knowing the value of $X$ completely determines the value of $Y$. This means there is no uncertainty left in $Y$ once $X$ is known. For any given $X=x$, the conditional probability distribution $P(Y|X=x)$ has a probability of 1 for one value of $y$ and 0 for all others. The entropy of such a distribution is 0.\n$$H(Y|X=x) = 0 \\quad \\text{for all } x \\in \\mathcal{X}$$\nTherefore, the total conditional entropy is also zero:\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} P(X=x) H(Y|X=x) = \\sum_{x \\in \\mathcal{X}} P(X=x) \\cdot 0 = 0$$\n\nNow, we calculate the mutual information using this second formula:\n$$I(Y;X) = H(Y) - H(Y|X) = H(Y) - 0 = H(Y)$$\n$$I(Y;X) = \\log_{2}(7) - \\frac{4}{7} - \\frac{3}{7}\\log_{2}(3)$$\n\nBoth computations yield the same result, confirming that $I(X;Y) = I(Y;X)$. The mutual information is the reduction in uncertainty about $X$ after learning $Y$, which is $H(X) - H(X|Y)$, or equivalently, the reduction in uncertainty about $Y$ after learning $X$, which is $H(Y) - H(Y|X)$.", "answer": "$$\\boxed{\\log_{2}(7) - \\frac{4}{7} - \\frac{3}{7}\\log_{2}(3)}$$", "id": "1662217"}]}