{"hands_on_practices": [{"introduction": "The chain rule for relative entropy decomposes the divergence of a joint distribution, $D(p(x,y)||q(x,y))$, into more manageable parts. Before applying the full rule, it's essential to master its components, starting with the marginal divergence [@problem_id:1609351]. This exercise provides hands-on practice in calculating the Kullback-Leibler ($KL$) divergence for the marginal distributions derived from two joint models, a crucial first step in understanding the information contribution of each variable.", "problem": "Consider two binary random variables, $X_1$ and $X_2$, which can each take a value from the set $\\{0, 1\\}$. An engineer is evaluating two different probabilistic models, denoted by $p$ and $q$, for the joint behavior of these variables.\n\nThe first model, a specific data-driven model, is described by the joint probability mass function $p(x_1, x_2)$ with the following values:\n- $p(X_1=0, X_2=0) = \\frac{1}{2}$\n- $p(X_1=0, X_2=1) = \\frac{1}{4}$\n- $p(X_1=1, X_2=0) = \\frac{1}{8}$\n- $p(X_1=1, X_2=1) = \\frac{1}{8}$\n\nThe second model, a simpler baseline model, assumes the outcomes are uniformly distributed. It is described by the joint probability mass function $q(x_1, x_2)$:\n- $q(X_1=0, X_2=0) = \\frac{1}{4}$\n- $q(X_1=0, X_2=1) = \\frac{1}{4}$\n- $q(X_1=1, X_2=0) = \\frac{1}{4}$\n- $q(X_1=1, X_2=1) = \\frac{1}{4}$\n\nYour task is to quantify the information gain, or \"surprise,\" of using model $p$ instead of model $q$ when considering each variable marginally. Calculate the Kullback-Leibler divergence (also known as relative entropy) between the marginal distributions of $X_1$, denoted as $D(p(x_1) || q(x_1))$, and the Kullback-Leibler divergence between the marginal distributions of $X_2$, denoted as $D(p(x_2) || q(x_2))$.\n\nProvide your final answers as a pair of two exact analytic expressions, using the base-2 logarithm. The first expression should be for $D(p(x_1) || q(x_1))$ and the second for $D(p(x_2) || q(x_2))$.", "solution": "We are given two binary random variables with joint distributions under models $p$ and $q$. The Kullback-Leibler divergence between two discrete distributions $r$ and $s$ on the same support, using base-2 logarithm, is defined as\n$$\nD(r||s)=\\sum_{x} r(x)\\log_{2}\\left(\\frac{r(x)}{s(x)}\\right).\n$$\nWe need $D(p(x_{1})||q(x_{1}))$ and $D(p(x_{2})||q(x_{2}))$, so we first compute the marginals.\n\nFrom the given $p(x_{1},x_{2})$,\n$$\np(x_{1}=0)=p(0,0)+p(0,1)=\\frac{1}{2}+\\frac{1}{4}=\\frac{3}{4},\\quad\np(x_{1}=1)=p(1,0)+p(1,1)=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4},\n$$\n$$\np(x_{2}=0)=p(0,0)+p(1,0)=\\frac{1}{2}+\\frac{1}{8}=\\frac{5}{8},\\quad\np(x_{2}=1)=p(0,1)+p(1,1)=\\frac{1}{4}+\\frac{1}{8}=\\frac{3}{8}.\n$$\nFrom the uniform $q(x_{1},x_{2})$, the marginals are\n$$\nq(x_{1}=0)=q(0,0)+q(0,1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\\quad\nq(x_{1}=1)=\\frac{1}{2},\n$$\n$$\nq(x_{2}=0)=q(0,0)+q(1,0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\\quad\nq(x_{2}=1)=\\frac{1}{2}.\n$$\nNow compute the divergences. For $X_{1}$,\n$$\nD(p(x_{1})||q(x_{1}))=\\sum_{x_{1}\\in\\{0,1\\}} p(x_{1})\\log_{2}\\left(\\frac{p(x_{1})}{q(x_{1})}\\right)\n=\\frac{3}{4}\\log_{2}\\left(\\frac{\\frac{3}{4}}{\\frac{1}{2}}\\right)+\\frac{1}{4}\\log_{2}\\left(\\frac{\\frac{1}{4}}{\\frac{1}{2}}\\right)\n=\\frac{3}{4}\\log_{2}\\left(\\frac{3}{2}\\right)+\\frac{1}{4}\\log_{2}\\left(\\frac{1}{2}\\right).\n$$\nSince $\\log_{2}\\left(\\frac{1}{2}\\right)=-1$, this can also be written as\n$$\nD(p(x_{1})||q(x_{1}))=\\frac{3}{4}\\log_{2}\\left(\\frac{3}{2}\\right)-\\frac{1}{4}.\n$$\nFor $X_{2}$,\n$$\nD(p(x_{2})||q(x_{2}))=\\sum_{x_{2}\\in\\{0,1\\}} p(x_{2})\\log_{2}\\left(\\frac{p(x_{2})}{q(x_{2})}\\right)\n=\\frac{5}{8}\\log_{2}\\left(\\frac{\\frac{5}{8}}{\\frac{1}{2}}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{\\frac{3}{8}}{\\frac{1}{2}}\\right)\n=\\frac{5}{8}\\log_{2}\\left(\\frac{5}{4}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{4}\\right).\n$$\nThese are exact analytic expressions using base-2 logarithms, as required.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{4}\\log_{2}\\left(\\frac{3}{2}\\right)-\\frac{1}{4} & \\frac{5}{8}\\log_{2}\\left(\\frac{5}{4}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{4}\\right)\\end{pmatrix}}$$", "id": "1609351"}, {"introduction": "Having practiced with a component of the chain rule, we now explore its full power in simplifying complex problems. This practice presents a scenario where the conditional probabilities of two models are identical, meaning $p(y|x) = q(y|x)$ [@problem_id:1609354]. By applying the chain rule, $D(p(x,y)||q(x,y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x)|p(x))$, you will see how the total divergence elegantly collapses, providing clear insight into how the rule partitions information.", "problem": "Consider two distinct joint probability distributions, $p(x,y)$ and $q(x,y)$, over a pair of binary random variables $X$ and $Y$, where the sample space for both variables is $\\{0, 1\\}$. The distributions are constructed based on the following marginal and conditional probabilities:\n\n1.  The marginal probability distribution for $X$ under $p$ is given by $p(X=0) = \\frac{1}{2}$ and $p(X=1) = \\frac{1}{2}$.\n2.  The marginal probability distribution for $X$ under $q$ is given by $q(X=0) = \\frac{1}{3}$ and $q(X=1) = \\frac{2}{3}$.\n3.  The conditional probability distributions of $Y$ given $X$ are the same for both $p$ and $q$, and are specified as:\n    *   $p(Y=0|X=0) = q(Y=0|X=0) = \\frac{1}{4}$\n    *   $p(Y=0|X=1) = q(Y=0|X=1) = \\frac{3}{4}$\n\nThe remaining conditional probabilities, $p(Y=1|X=x)$ and $q(Y=1|X=x)$, are determined by the axiom that probabilities must sum to one.\n\nCalculate the Kullback-Leibler (KL) divergence, also known as relative entropy, $D(p(x,y) || q(x,y))$. Express your answer as a closed-form analytic expression using base-2 logarithms.", "solution": "The problem asks for the calculation of the Kullback-Leibler (KL) divergence $D(p(x,y) || q(x,y))$ between two joint probability distributions $p(x,y)$ and $q(x,y)$.\n\nThe chain rule for KL divergence states that the divergence of a joint distribution can be decomposed into the sum of the divergence of a marginal distribution and a conditional divergence:\n$$D(p(x,y) || q(x,y)) = D(p(x) || q(x)) + D(p(y|x) || q(y|x) | p(x))$$\n\nLet's analyze the second term, the conditional relative entropy, which is defined as:\n$$D(p(y|x) || q(y|x) | p(x)) = \\sum_{x \\in \\{0,1\\}} p(x) D(p(y|X=x) || q(y|X=x))$$\nThe inner term $D(p(y|X=x) || q(y|X=x))$ is the KL divergence between the conditional distributions of $Y$ for a specific value of $X$. We know that for any two probability distributions $P$ and $Q$, $D(P||Q) \\ge 0$, with equality holding if and only if $P=Q$.\n\nLet's examine the conditional distributions given in the problem.\nFor $X=0$:\nWe are given $p(Y=0|X=0) = \\frac{1}{4}$ and $q(Y=0|X=0) = \\frac{1}{4}$.\nSince the conditional probabilities for a given $x$ must sum to 1:\n$p(Y=1|X=0) = 1 - p(Y=0|X=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$.\n$q(Y=1|X=0) = 1 - q(Y=0|X=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$.\nThus, for all $y \\in \\{0,1\\}$, we have $p(y|X=0) = q(y|X=0)$.\n\nFor $X=1$:\nWe are given $p(Y=0|X=1) = \\frac{3}{4}$ and $q(Y=0|X=1) = \\frac{3}{4}$.\nSimilarly:\n$p(Y=1|X=1) = 1 - p(Y=0|X=1) = 1 - \\frac{3}{4} = \\frac{1}{4}$.\n$q(Y=1|X=1) = 1 - q(Y=0|X=1) = 1 - \\frac{3}{4} = \\frac{1}{4}$.\nThus, for all $y \\in \\{0,1\\}$, we have $p(y|X=1) = q(y|X=1)$.\n\nSince the conditional distributions $p(y|X=x)$ and $q(y|X=x)$ are identical for both $x=0$ and $x=1$, the KL divergence between them is zero for each case:\n$D(p(y|X=0) || q(y|X=0)) = 0$.\n$D(p(y|X=1) || q(y|X=1)) = 0$.\n\nNow we can compute the overall conditional relative entropy:\n$$D(p(y|x) || q(y|x) | p(x)) = p(X=0) D(p(y|X=0) || q(y|X=0)) + p(X=1) D(p(y|X=1) || q(y|X=1))$$\n$$D(p(y|x) || q(y|x) | p(x)) = p(X=0) \\cdot 0 + p(X=1) \\cdot 0 = 0$$\n\nSubstituting this result back into the chain rule equation simplifies the problem significantly:\n$$D(p(x,y) || q(x,y)) = D(p(x) || q(x)) + 0 = D(p(x) || q(x))$$\nThe problem is now reduced to calculating the KL divergence between the marginal distributions $p(x)$ and $q(x)$.\n\nThe marginal distributions are given as:\n$p(x)$: $p(X=0) = \\frac{1}{2}$, $p(X=1) = \\frac{1}{2}$.\n$q(x)$: $q(X=0) = \\frac{1}{3}$, $q(X=1) = \\frac{2}{3}$.\n\nThe formula for $D(p(x) || q(x))$ is:\n$$D(p(x) || q(x)) = \\sum_{x \\in \\{0,1\\}} p(x) \\log_2 \\frac{p(x)}{q(x)}$$\n$$D(p(x) || q(x)) = p(X=0) \\log_2 \\left(\\frac{p(X=0)}{q(X=0)}\\right) + p(X=1) \\log_2 \\left(\\frac{p(X=1)}{q(X=1)}\\right)$$\nSubstituting the values:\n$$D(p(x) || q(x)) = \\frac{1}{2} \\log_2 \\left(\\frac{1/2}{1/3}\\right) + \\frac{1}{2} \\log_2 \\left(\\frac{1/2}{2/3}\\right)$$\n$$D(p(x) || q(x)) = \\frac{1}{2} \\log_2 \\left(\\frac{3}{2}\\right) + \\frac{1}{2} \\log_2 \\left(\\frac{3}{4}\\right)$$\nUsing the logarithm property $\\log_2(a) + \\log_2(b) = \\log_2(ab)$:\n$$D(p(x) || q(x)) = \\frac{1}{2} \\log_2 \\left(\\frac{3}{2} \\cdot \\frac{3}{4}\\right) = \\frac{1}{2} \\log_2 \\left(\\frac{9}{8}\\right)$$\nUsing the property $\\log_2(a/b) = \\log_2(a) - \\log_2(b)$:\n$$D(p(x) || q(x)) = \\frac{1}{2} (\\log_2(9) - \\log_2(8))$$\nSince $\\log_2(9) = \\log_2(3^2) = 2\\log_2(3)$ and $\\log_2(8) = \\log_2(2^3) = 3$:\n$$D(p(x) || q(x)) = \\frac{1}{2} (2\\log_2(3) - 3)$$\n$$D(p(x) || q(x)) = \\log_2(3) - \\frac{3}{2}$$\nTherefore, the KL divergence $D(p(x,y) || q(x,y))$ is $\\log_2(3) - \\frac{3}{2}$.", "answer": "$$\\boxed{\\log_{2}(3) - \\frac{3}{2}}$$", "id": "1609354"}, {"introduction": "This final practice challenges you to apply the principles of decomposition, central to the chain rule, in a more abstract setting. You will analyze the divergence between two mixture models, $p(x,y) = \\alpha p_1(x,y) + (1-\\alpha) p_2(x,y)$ and its approximation [@problem_id:1609364]. The solution requires a similar style of thinking: partitioning the problem space based on conditional properties to reveal a surprisingly simple and fundamental result.", "problem": "In a data synthesis process, a system generates pairs of random variables $(X, Y)$ from one of two underlying mechanisms, described by probability distributions $p_1(x,y)$ and $p_2(x,y)$. At each step, the system chooses mechanism 1 with a true probability of $\\alpha$ and mechanism 2 with a true probability of $1-\\alpha$, where $0 < \\alpha < 1$. The resulting true distribution of the generated data is a mixture model given by $p(x,y) = \\alpha p_1(x,y) + (1-\\alpha) p_2(x,y)$.\n\nThe following two properties of the underlying mechanisms are known:\n1.  The marginal probability distributions for the variable $Y$ are identical for both mechanisms, that is, $p_1(y) = p_2(y)$ for all possible outcomes $y$.\n2.  For any given outcome $y$, the set of possible outcomes for $X$ under mechanism 1 is completely distinct from the set of possible outcomes for $X$ under mechanism 2. In other words, their supports are disjoint, so if $p_1(x|y) > 0$, then $p_2(x|y) = 0$, and vice versa.\n\nAn engineer creates an approximate model, $q(x,y)$, to describe the system. This model correctly assumes the mixture form and the same component distributions $p_1$ and $p_2$, but it is based on an incorrect estimate of the mixing probability, using a value $\\beta$ instead of $\\alpha$, where $0 < \\beta < 1$. The engineer's model is thus $q(x,y) = \\beta p_1(x,y) + (1-\\beta) p_2(x,y)$.\n\nCalculate the information loss incurred by using the approximate model $q(x,y)$ instead of the true model $p(x,y)$. This quantity is defined as the expected value of the logarithmic ratio of the probabilities, $\\mathbb{E}_{p}\\left[\\ln \\frac{p(x,y)}{q(x,y)}\\right]$, also known as the Kullback-Leibler (KL) divergence $D_{KL}(p || q)$. Express your final answer as a symbolic expression in terms of $\\alpha$ and $\\beta$.", "solution": "We are asked to compute the Kullback-Leibler divergence\n$$\nD_{KL}(p\\|q)=\\mathbb{E}_{p}\\left[\\ln\\frac{p(X,Y)}{q(X,Y)}\\right]=\\iint p(x,y)\\,\\ln\\!\\left(\\frac{p(x,y)}{q(x,y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y,\n$$\nfor $p(x,y)=\\alpha p_{1}(x,y)+(1-\\alpha)p_{2}(x,y)$ and $q(x,y)=\\beta p_{1}(x,y)+(1-\\beta)p_{2}(x,y)$, under the assumptions:\n1) $p_{1}(y)=p_{2}(y)$ for all $y$,\n2) for each $y$, the supports of $p_{1}(x\\mid y)$ and $p_{2}(x\\mid y)$ are disjoint; that is, if $p_{1}(x\\mid y)>0$ then $p_{2}(x\\mid y)=0$, and vice versa.\n\nDefine the sets\n$$\nA=\\{(x,y):p_{1}(x\\mid y)>0\\},\\qquad B=\\{(x,y):p_{2}(x\\mid y)>0\\}.\n$$\nBy assumption 2, $A$ and $B$ are disjoint and cover the support of $p$ up to sets of $p$-measure zero. On $A$, we have $p_{2}(x\\mid y)=0$, hence $p_{2}(x,y)=p_{2}(x\\mid y)p_{2}(y)=0$, which implies\n$$\np(x,y)=\\alpha p_{1}(x,y),\\qquad q(x,y)=\\beta p_{1}(x,y),\\qquad (x,y)\\in A.\n$$\nTherefore, for $(x,y)\\in A$,\n$$\n\\ln\\!\\left(\\frac{p(x,y)}{q(x,y)}\\right)=\\ln\\!\\left(\\frac{\\alpha p_{1}(x,y)}{\\beta p_{1}(x,y)}\\right)=\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right).\n$$\nSimilarly, on $B$ we have $p_{1}(x\\mid y)=0$, hence $p_{1}(x,y)=0$, and thus\n$$\np(x,y)=(1-\\alpha)p_{2}(x,y),\\qquad q(x,y)=(1-\\beta)p_{2}(x,y),\\qquad (x,y)\\in B,\n$$\nwhich yields\n$$\n\\ln\\!\\left(\\frac{p(x,y)}{q(x,y)}\\right)=\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right),\\qquad (x,y)\\in B.\n$$\n\nUsing these piecewise constants inside the expectation, we partition the integral and pull out the constants:\n$$\nD_{KL}(p\\|q)=\\iint_{A} p(x,y)\\,\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y+\\iint_{B} p(x,y)\\,\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\nThis simplifies to\n$$\nD_{KL}(p\\|q)=\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right)\\iint_{A} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y+\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right)\\iint_{B} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\nWe evaluate the probabilities $\\iint_{A} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y$ and $\\iint_{B} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y$. On $A$, $p(x,y)=\\alpha p_{1}(x,y)$ and $p_{1}$ has all its mass on $A$ due to the disjoint-support assumption, so\n$$\n\\iint_{A} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=\\alpha\\iint_{A} p_{1}(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=\\alpha\\cdot 1=\\alpha.\n$$\nAnalogously, on $B$, $p(x,y)=(1-\\alpha)p_{2}(x,y)$ and $p_{2}$ has all its mass on $B$, so\n$$\n\\iint_{B} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=(1-\\alpha)\\iint_{B} p_{2}(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=(1-\\alpha)\\cdot 1=(1-\\alpha).\n$$\nSubstituting these back gives\n$$\nD_{KL}(p\\|q)=\\alpha\\,\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right)+(1-\\alpha)\\,\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right).\n$$\nThis is the desired symbolic expression in terms of $\\alpha$ and $\\beta$.", "answer": "$$\\boxed{\\alpha\\ln\\left(\\frac{\\alpha}{\\beta}\\right)+\\left(1-\\alpha\\right)\\ln\\left(\\frac{1-\\alpha}{1-\\beta}\\right)}$$", "id": "1609364"}]}