{"hands_on_practices": [{"introduction": "A core tenet of information theory, encapsulated by the data processing inequality, is that you cannot create information out of thin air. This exercise provides a concrete demonstration of this principle by starting with two independent variables, where the mutual information is zero by definition. You will verify that no amount of deterministic processing on one variable can create a new variable that shares information with the other, confirming that $I(X; Z) = 0$ if $I(X; Y) = 0$ for the Markov chain $X \\rightarrow Y \\rightarrow Z$. [@problem_id:1613376]", "problem": "In a quality control system for a semiconductor manufacturing line, the state of a specific etching process is monitored. This state is represented by a discrete random variable $X$, where $X=0$ indicates a successful process and $X=1$ indicates a failure. Historical data shows that the probability of a failure is $P(X=1) = \\frac{1}{5}$.\n\nIndependently, a sensor monitors the ambient pressure in the cleanroom, which is known to be unrelated to the etching process outcome. The pressure is quantized into one of four discrete levels, represented by the random variable $Y \\in \\{1, 2, 3, 4\\}$. The probability distribution for these pressure levels is given by $P(Y=1) = \\frac{1}{2}$, $P(Y=2) = \\frac{1}{4}$, $P(Y=3) = \\frac{1}{8}$, and $P(Y=4) = \\frac{1}{8}$.\n\nA real-time analytics engine receives the raw pressure level $Y$ and computes a summary statistic $Z$ using a deterministic function: $Z = (Y^2 - 1) \\pmod{3}$. This summary statistic is logged for later analysis.\n\nAn engineer wants to determine if there is any information about the process state $X$ contained within the summary statistic $Z$. Calculate the mutual information $I(X; Z)$ between the process state and the summary statistic. Express your answer in bits.", "solution": "The mutual information between two discrete random variables $X$ and $Z$ is defined as:\n$$I(X; Z) = \\sum_{x \\in \\mathcal{X}} \\sum_{z \\in \\mathcal{Z}} p(x, z) \\log_2 \\left( \\frac{p(x, z)}{p(x) p(z)} \\right)$$\nwhere $p(x, z)$ is the joint probability mass function of $(X, Z)$, and $p(x)$ and $p(z)$ are the marginal probability mass functions of $X$ and $Z$, respectively. The calculation is to be done in bits, so we use the logarithm to base 2.\n\nWe are given the probability distribution for the process state $X$:\n$P(X=1) = \\frac{1}{5}$\n$P(X=0) = 1 - P(X=1) = 1 - \\frac{1}{5} = \\frac{4}{5}$.\n\nWe are also given the probability distribution for the pressure level $Y$:\n$P(Y=1) = \\frac{1}{2}$\n$P(Y=2) = \\frac{1}{4}$\n$P(Y=3) = \\frac{1}{8}$\n$P(Y=4) = \\frac{1}{8}$\n\nThe summary statistic $Z$ is a deterministic function of $Y$, given by $Z = g(Y) = (Y^2 - 1) \\pmod{3}$. First, we must determine the possible values of $Z$ and its probability distribution, $p(z)$.\n- For $Y=1$: $Z = (1^2 - 1) \\pmod{3} = 0 \\pmod{3} = 0$.\n- For $Y=2$: $Z = (2^2 - 1) \\pmod{3} = 3 \\pmod{3} = 0$.\n- For $Y=3$: $Z = (3^2 - 1) \\pmod{3} = 8 \\pmod{3} = 2$.\n- For $Y=4$: $Z = (4^2 - 1) \\pmod{3} = 15 \\pmod{3} = 0$.\nThe set of possible values for $Z$ is $\\mathcal{Z} = \\{0, 2\\}$.\n\nNow we can calculate the marginal probability distribution $p(z)$:\n$P(Z=0) = P(Y=1) + P(Y=2) + P(Y=4) = \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} = \\frac{4}{8} + \\frac{2}{8} + \\frac{1}{8} = \\frac{7}{8}$.\n$P(Z=2) = P(Y=3) = \\frac{1}{8}$.\nAs a check, $P(Z=0) + P(Z=2) = \\frac{7}{8} + \\frac{1}{8} = 1$.\n\nNext, we need to find the joint probability distribution $p(x, z)$. For any pair of values $(x, z)$:\n$p(x, z) = P(X=x, Z=z) = P(X=x, g(Y)=z)$.\nWe can write this as a sum over the values of $y$ that map to $z$:\n$P(X=x, g(Y)=z) = \\sum_{y: g(y)=z} P(X=x, Y=y)$.\n\nThe problem states that the process state $X$ and the pressure level $Y$ are independent. Therefore, their joint probability is the product of their marginal probabilities: $P(X=x, Y=y) = P(X=x) P(Y=y)$.\nSubstituting this into the expression for $p(x, z)$:\n$p(x, z) = \\sum_{y: g(y)=z} \\left( P(X=x) P(Y=y) \\right)$.\nSince $P(X=x)$ does not depend on $y$, we can factor it out of the sum:\n$p(x, z) = P(X=x) \\left( \\sum_{y: g(y)=z} P(Y=y) \\right)$.\nThe sum in the parenthesis is precisely the definition of $P(g(Y)=z)$, which is $P(Z=z)$.\nTherefore, we have shown that $p(x, z) = P(X=x) P(Z=z) = p(x) p(z)$.\n\nThis result means that the random variables $X$ and $Z$ are independent.\n\nNow we can calculate the mutual information $I(X; Z)$. Substituting $p(x, z) = p(x) p(z)$ into the definition of mutual information:\n$$I(X; Z) = \\sum_{x \\in \\mathcal{X}} \\sum_{z \\in \\mathcal{Z}} p(x) p(z) \\log_2 \\left( \\frac{p(x) p(z)}{p(x) p(z)} \\right)$$\n$$I(X; Z) = \\sum_{x \\in \\mathcal{X}} \\sum_{z \\in \\mathcal{Z}} p(x) p(z) \\log_2(1)$$\nSince $\\log_2(1) = 0$, the entire expression becomes:\n$$I(X; Z) = \\sum_{x \\in \\mathcal{X}} \\sum_{z \\in \\mathcal{Z}} p(x) p(z) \\cdot 0 = 0$$\n\nAlternatively, one could recognize that since $Z$ is a deterministic function of $Y$, the variables form a Markov chain $X \\to Y \\to Z$. The Data Processing Inequality states that for any such Markov chain, $I(X; Z) \\le I(X; Y)$.\nWe are given that $X$ and $Y$ are independent. The mutual information between two independent variables is zero, so $I(X; Y) = 0$.\nCombining these facts, we have $I(X; Z) \\le 0$. Since mutual information is always non-negative ($I(X;Z) \\ge 0$), the only possible value is $I(X; Z) = 0$.\n\nBoth methods lead to the same result. The mutual information between the process state $X$ and the summary statistic $Z$ is 0 bits.", "answer": "$$\\boxed{0}$$", "id": "1613376"}, {"introduction": "While processing cannot create information, it can certainly destroy it. This practice problem moves from the baseline case of zero information to a scenario where information is actively lost. By constructing a specific two-stage communication system, you will calculate how an initial signal containing information ($I(X;Y) \\gt 0$) can be processed in such a way that the final signal contains no information about the original source whatsoever ($I(X;Z) = 0$), providing a striking example of the \"inequality\" in action. [@problem_id:1613411]", "problem": "Consider a simplified model of a two-stage digital communication system. A source produces a binary signal $X$, which can be either 0 or 1. The prior probability distribution for the source is symmetric, i.e., $P(X=0) = P(X=1) = 1/2$.\n\nThe signal $X$ is first transmitted through a noisy channel to an intermediate node, producing a signal $Y \\in \\{0, 1\\}$. This channel is a Binary Symmetric Channel with a crossover probability of $\\epsilon = 1/3$. This means the probability of a bit flip is $P(Y \\neq x | X = x) = 1/3$.\n\nThe signal $Y$ is then processed by the intermediate node and re-transmitted to a final destination, producing a signal $Z \\in \\{+, -\\}$. This second stage is also a noisy channel governed by the following conditional probabilities:\n- $P(Z = + | Y = 0) = 1/4$\n- $P(Z = + | Y = 1) = 1/4$\n\nThe entire process forms a Markov chain $X \\rightarrow Y \\rightarrow Z$.\n\nCalculate the mutual information between the original signal and the intermediate signal, $I(X;Y)$, and the mutual information between the original signal and the final signal, $I(X;Z)$. Express your answers for the mutual information in bits. Your final answer should be a row matrix containing the values for $I(X;Y)$ and $I(X;Z)$, in that order.", "solution": "We are given a Markov chain $X \\rightarrow Y \\rightarrow Z$ with $X \\in \\{0,1\\}$, $P(X=0)=P(X=1)=\\frac{1}{2}$, a first-stage Binary Symmetric Channel (BSC) with crossover probability $\\epsilon=\\frac{1}{3}$, and a second stage where $P(Z=+ \\mid Y=0)=P(Z=+ \\mid Y=1)=\\frac{1}{4}$.\n\nFirst, compute $I(X;Y)$. By definition, $I(X;Y)=H(Y)-H(Y \\mid X)$.\n\nFor the BSC with $\\epsilon=\\frac{1}{3}$ and symmetric input, the conditional distribution of $Y$ given $X=x$ is $P(Y=x \\mid X=x)=\\frac{2}{3}$ and $P(Y \\neq x \\mid X=x)=\\frac{1}{3}$. The marginal of $Y$ is\n$$\nP(Y=0)=P(Y=0 \\mid X=0)P(X=0)+P(Y=0 \\mid X=1)P(X=1)=\\frac{2}{3}\\cdot \\frac{1}{2}+\\frac{1}{3}\\cdot \\frac{1}{2}=\\frac{1}{2},\n$$\nand similarly $P(Y=1)=\\frac{1}{2}$. Hence\n$$\nH(Y)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\nThe conditional entropy is\n$$\nH(Y \\mid X)=\\sum_{x \\in \\{0,1\\}}P(X=x)\\,H(Y \\mid X=x)=H\\!\\left(\\tfrac{2}{3},\\tfrac{1}{3}\\right)\n= -\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)-\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right).\n$$\nTherefore\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right).\n$$\n\nNext, compute $I(X;Z)$. We are given $P(Z=+ \\mid Y=0)=P(Z=+ \\mid Y=1)=\\frac{1}{4}$, so $P(Z=- \\mid Y=0)=P(Z=- \\mid Y=1)=\\frac{3}{4}$. Thus the distribution of $Z$ does not depend on $Y$, implying $Z$ is independent of $Y$. Because $X \\rightarrow Y \\rightarrow Z$ is a Markov chain, $Z$ is also independent of $X$, hence\n$$\nI(X;Z)=0.\n$$\nEquivalently, one can verify $P(Z=+)=\\sum_{y}P(Z=+ \\mid Y=y)P(Y=y)=\\frac{1}{4}$ and $H(Z \\mid X)=H(Z)$, which again gives $I(X;Z)=H(Z)-H(Z \\mid X)=0$.\n\nThe mutual informations in bits are thus $I(X;Y)=1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)$ and $I(X;Z)=0$.", "answer": "$$\\boxed{\\begin{pmatrix}1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)  0\\end{pmatrix}}$$", "id": "1613411"}, {"introduction": "Having seen that information can be lost during processing, it is natural to ask: when is it preserved? This problem explores the conditions for equality in the data processing inequality, $I(X;Z) = I(X;Y)$. By analyzing a common digital workflowâ€”lossy compression followed by lossless compressionâ€”you will prove that if a processing step is perfectly reversible, no information about the original source is lost in that step. [@problem_id:1613402]", "problem": "Consider a data processing pipeline involving a large digital photograph. The original, uncompressed raw image data is represented by a discrete random variable $X$.\n\nFirst, this raw image is converted to a more common format using a *lossy* compression algorithm. This process permanently discards some of the original image information to reduce file size. The data of this new, lossily compressed image is represented by a random variable $Y$. Because this step is lossy, the original data $X$ cannot be perfectly recovered from $Y$.\n\nSecond, the file corresponding to $Y$ is further compressed using a *lossless* data compression algorithm, such as the Lempel-Ziv algorithm often used in ZIP archives. This produces a final file represented by a random variable $Z$. This second step is perfectly reversible, meaning the data $Y$ can be exactly reconstructed from the data $Z$ without any loss of information.\n\nLet $I(A; B)$ denote the mutual information between two random variables $A$ and $B$. Given this two-step process, what is the most specific relationship that is guaranteed to hold between the mutual information $I(X; Y)$ and the mutual information $I(X; Z)$?\n\nA. $I(X; Y) > I(X; Z)$\n\nB. $I(X; Y)  I(X; Z)$\n\nC. $I(X; Y) = I(X; Z)$\n\nD. $I(X; Y) \\ge I(X; Z)$\n\nE. The relationship cannot be determined from the information given.", "solution": "Let $X$ denote the raw image data, $Y$ the lossy-compressed image, and $Z$ the losslessly compressed version of $Y$. The processing chain implies the Markov chain $X \\to Y \\to Z$, because $Z$ depends on $X$ only through $Y$. Since the second step is lossless and perfectly reversible, there exists a bijection $f$ such that $Z=f(Y)$ and $Y=f^{-1}(Z)$. Thus $Y$ is a deterministic function of $Z$ and $Z$ is a deterministic function of $Y$.\n\nUsing the data processing inequality for the Markov chain $X \\to Y \\to Z$ with $Z=f(Y)$, we have\n$$\nI(X;Z) \\le I(X;Y).\n$$\nBecause $Y=f^{-1}(Z)$, we also have the reverse Markov chain $X \\to Z \\to Y$, which gives\n$$\nI(X;Y) \\le I(X;Z).\n$$\nCombining these two inequalities yields\n$$\nI(X;Y) = I(X;Z).\n$$\n\nEquivalently, using the definition $I(X;Y)=H(X)-H(X|Y)$ and $I(X;Z)=H(X)-H(X|Z)$, and noting that $Z$ and $Y$ are bijectively related, conditioning on $Z$ is equivalent to conditioning on $Y$. Formally, since $Z=f(Y)$ and $Y=f^{-1}(Z)$,\n$$\nH(X|Z) = H\\bigl(X \\mid f^{-1}(Z)\\bigr) = H(X|Y),\n$$\nwhich directly gives\n$$\nI(X;Z) = H(X) - H(X|Z) = H(X) - H(X|Y) = I(X;Y).\n$$\n\nTherefore, the most specific guaranteed relationship is equality.", "answer": "$$\\boxed{C}$$", "id": "1613402"}]}