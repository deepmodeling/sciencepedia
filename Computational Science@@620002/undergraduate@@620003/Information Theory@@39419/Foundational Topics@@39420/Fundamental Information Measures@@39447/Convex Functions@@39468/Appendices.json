{"hands_on_practices": [{"introduction": "A cornerstone of analyzing functions in calculus is the second derivative test, which provides a clear criterion for convexity. This first practice applies this fundamental tool to a function constructed from the binary entropy, $H_b(p)$, a measure central to information theory. By investigating the convexity of $g(p) = \\exp(-H_b(p))$, you will gain hands-on experience in applying calculus to reveal the underlying structure of information-theoretic functions [@problem_id:1614169].", "problem": "In information theory, the uncertainty of a binary random variable with a probability of success $p$ is quantified by the binary entropy function, $H_b(p)$, defined for $p \\in (0, 1)$ as:\n$$H_b(p) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p)$$\nwhere the logarithm is base 2.\n\nConsider a function $g(p)$ that is related to the \"certainty\" or \"purity\" of this binary source, defined as:\n$$g(p) = \\exp(-H_b(p))$$\nInvestigate the properties of this function. Which of the following statements correctly describes the function $g(p)$ on its domain $p \\in (0, 1)$?\n\nA. Strictly convex on the interval $(0, 1)$.\n\nB. Strictly concave on the interval $(0, 1)$.\n\nC. Both convex and concave on the interval $(0, 1)$.\n\nD. Neither strictly convex nor strictly concave on the interval $(0, 1)$.", "solution": "We first rewrite the binary entropy in terms of natural logarithms. Using $\\log_{2}(x) = \\frac{\\ln x}{\\ln 2}$, the binary entropy is\n$$\nH_{b}(p) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p) = -\\frac{p \\ln p + (1-p) \\ln(1-p)}{\\ln 2}.\n$$\nHence\n$$\ng(p) = \\exp\\!\\big(-H_{b}(p)\\big) = \\exp\\!\\left(\\frac{p \\ln p + (1-p) \\ln(1-p)}{\\ln 2}\\right).\n$$\nDefine $c = \\frac{1}{\\ln 2} > 0$ and $f(p) = p \\ln p + (1-p) \\ln(1-p)$. Then $g(p) = \\exp\\!\\big(c f(p)\\big)$.\n\nWe compute derivatives of $f$. Differentiating,\n$$\nf'(p) = \\frac{d}{dp}\\big(p \\ln p\\big) + \\frac{d}{dp}\\big((1-p)\\ln(1-p)\\big) = (\\ln p + 1) + \\big(-\\ln(1-p) - 1\\big) = \\ln\\!\\left(\\frac{p}{1-p}\\right).\n$$\nDifferentiating again,\n$$\nf''(p) = \\frac{d}{dp}\\left(\\ln p - \\ln(1-p)\\right) = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1}{p(1-p)}.\n$$\nFor $p \\in (0,1)$, we have $p(1-p) > 0$, hence $f''(p) > 0$ on $(0,1)$, so $f$ is strictly convex on $(0,1)$. Since $c > 0$, the function $c f$ is also strictly convex on $(0,1)$.\n\nNow compute derivatives of $g$. Using the chain rule,\n$$\ng'(p) = \\frac{d}{dp}\\exp\\!\\big(c f(p)\\big) = c f'(p) \\exp\\!\\big(c f(p)\\big) = c f'(p) g(p).\n$$\nDifferentiating once more,\n$$\ng''(p) = c f''(p) g(p) + c f'(p) g'(p) = c f''(p) g(p) + c f'(p)\\big(c f'(p) g(p)\\big) = g(p)\\big(c f''(p) + c^{2} (f'(p))^{2}\\big).\n$$\nFor $p \\in (0,1)$, we have $g(p) > 0$, $c > 0$, and $f''(p) = \\frac{1}{p(1-p)} > 0$, while $(f'(p))^{2} \\ge 0$. Therefore,\n$$\ng''(p) = g(p)\\big(c f''(p) + c^{2} (f'(p))^{2}\\big) > 0 \\quad \\text{for all } p \\in (0,1).\n$$\nHence $g$ is strictly convex on $(0,1)$. Consequently, among the given options, the correct description is that $g(p)$ is strictly convex on $(0,1)$.", "answer": "$$\\boxed{A}$$", "id": "1614169"}, {"introduction": "While calculus is a powerful tool, we often build complex systems from simpler components with known properties. This exercise explores how the property of concavity behaves under the minimum operation, a common scenario in system design where performance is limited by the weakest link [@problem_id:1614199]. By analyzing the effective capacity of a communication system, you will see that the minimum of concave functions is itself concave, illustrating a fundamental principle of convex optimization that has direct practical implications.", "problem": "A communications engineer is analyzing a new adaptive transmission protocol. The protocol can operate in one of two modes, Mode 1 or Mode 2, and the theoretical maximum data rate (channel capacity) for each mode depends on a singular, adjustable power allocation parameter, $x$, where $x \\ge 0$.\n\nThe capacity for Mode 1, denoted $C_1(x)$, and for Mode 2, denoted $C_2(x)$, are given by the following functions:\n- $C_1(x) = R_1 \\ln(1 + k_1 x)$\n- $C_2(x) = R_2 \\ln(1 + k_2 x)$\n\nHere, $R_1, R_2, k_1,$ and $k_2$ are positive real constants that characterize the physical properties of each mode.\n\nA crucial feature of the protocol is a fail-safe mechanism. For a given power allocation $x$, the system's guaranteed effective capacity, $C_{eff}(x)$, is conservatively defined as the minimum of the capacities achievable in the two modes.\n\nBased on this design, which of the following statements correctly describes the behavior of the marginal capacity gain (the rate of change of capacity with respect to the power allocation $x$) of the effective capacity $C_{eff}(x)$ as $x$ increases from 0?\n\nA. The marginal capacity gain is always strictly increasing.\n\nB. The marginal capacity gain is always constant.\n\nC. The marginal capacity gain is non-increasing.\n\nD. The marginal capacity gain can both increase and decrease, depending on the value of $x$.\n\nE. The behavior of the marginal capacity gain depends on the specific values of the constants $R_1, R_2, k_1,$ and $k_2$.", "solution": "We are given two mode capacities\n$$\nC_{1}(x) = R_{1}\\ln(1 + k_{1} x), \\quad C_{2}(x) = R_{2}\\ln(1 + k_{2} x),\n$$\nwith $R_{1},R_{2},k_{1},k_{2} > 0$, and the effective capacity\n$$\nC_{\\mathrm{eff}}(x) = \\min\\{C_{1}(x),C_{2}(x)\\}, \\quad x \\ge 0.\n$$\nFirst, compute the first and second derivatives of each mode:\n$$\nC_{i}'(x) = \\frac{R_{i}k_{i}}{1 + k_{i}x} > 0, \\quad C_{i}''(x) = -\\frac{R_{i}k_{i}^{2}}{(1 + k_{i}x)^{2}} < 0, \\quad i \\in \\{1,2\\}.\n$$\nThus, each $C_{i}(x)$ is increasing and concave on $[0,\\infty)$, and its marginal gain $C_{i}'(x)$ is strictly decreasing in $x$.\n\nNext, analyze $C_{\\mathrm{eff}}(x) = \\min\\{C_{1}(x),C_{2}(x)\\}$. Since $C_{1}$ and $C_{2}$ are concave, their negatives $-C_{1}$ and $-C_{2}$ are convex. The pointwise maximum of convex functions is convex, so $\\max\\{-C_{1},-C_{2}\\}$ is convex. Therefore,\n$$\nC_{\\mathrm{eff}}(x) \\;=\\; \\min\\{C_{1}(x),C_{2}(x)\\} \\;=\\; -\\,\\max\\{-C_{1}(x),-C_{2}(x)\\}\n$$\nis concave on $[0,\\infty)$ as the negative of a convex function.\n\nA concave function has a non-increasing marginal gain: where differentiable, its derivative is non-increasing; at nondifferentiable points (e.g., a switching point where $C_{1}(x) = C_{2}(x)$), the left and right derivatives exist and satisfy $C_{\\mathrm{eff}}'^{-}(x) \\ge C_{\\mathrm{eff}}'^{+}(x)$. Consequently, the marginal capacity gain of $C_{\\mathrm{eff}}(x)$ cannot increase with $x$.\n\nEquivalently, piecewise,\n$$\nC_{\\mathrm{eff}}'(x)=\n\\begin{cases}\nC_{1}'(x) = \\dfrac{R_{1}k_{1}}{1+k_{1}x}, & \\text{if } C_{1}(x) \\le C_{2}(x) \\\\[1em]\nC_{2}'(x) = \\dfrac{R_{2}k_{2}}{1+k_{2}x}, & \\text{if } C_{2}(x) \\le C_{1}(x)\n\\end{cases}\n$$\nso within each region it is strictly decreasing, and at any switch it does not jump upward. Hence the marginal capacity gain of $C_{\\mathrm{eff}}(x)$ is non-increasing as $x$ increases from $0$.\n\nTherefore, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1614199"}, {"introduction": "Many concepts in information theory, such as distance and divergence, are not functions of a single scalar but of entire probability distributions. This practice elevates our understanding of convexity to functions defined on vector spaces [@problem_id:1614164]. You will demonstrate that the Total Variation distance, a key metric for comparing two probability distributions, is a jointly convex function of the distributions, revealing a beautiful connection between the geometric properties of norms and the analytical properties of statistical distances.", "problem": "In information theory and statistics, various metrics are used to quantify the \"distance\" between two probability distributions. One such fundamental metric is the Total Variation distance.\n\nLet $P = (p_1, p_2, \\dots, p_n)$ and $Q = (q_1, q_2, \\dots, q_n)$ be two probability distributions over a finite set of $n$ outcomes. This means $p_i \\ge 0$, $q_i \\ge 0$ for all $i \\in \\{1, \\dots, n\\}$, and $\\sum_{i=1}^n p_i = \\sum_{i=1}^n q_i = 1$. The set of all such distributions on $n$ outcomes forms a convex set.\n\nThe Total Variation (TV) distance between $P$ and $Q$ is defined as:\n$$D_{TV}(P, Q) = \\frac{1}{2} \\sum_{i=1}^n |p_i - q_i|$$\n\nA function $f(P, Q)$ is said to be jointly convex in the pair $(P, Q)$ if for any two pairs of distributions $(P_1, Q_1)$ and $(P_2, Q_2)$, and for any scalar $\\lambda \\in [0, 1]$, the following inequality holds:\n$$f(\\lambda P_1 + (1-\\lambda) P_2, \\lambda Q_1 + (1-\\lambda) Q_2) \\le \\lambda f(P_1, Q_1) + (1-\\lambda) f(P_2, Q_2)$$\nA function is jointly concave if the inequality is reversed ($ \\ge $). A function is convex in its first argument $P$ if, for any fixed distribution $Q$, the function $g(P) = f(P, Q)$ is convex.\n\nAnalyze the properties of the Total Variation distance $D_{TV}(P, Q)$. Which of the following statements is correct?\n\nA. $D_{TV}(P, Q)$ is jointly concave in the pair of distributions $(P, Q)$.\n\nB. $D_{TV}(P, Q)$ is jointly convex in the pair of distributions $(P, Q)$.\n\nC. $D_{TV}(P, Q)$ is convex in its first argument $P$ for any fixed $Q$, but it is not jointly convex in the pair $(P, Q)$.\n\nD. $D_{TV}(P, Q)$ is neither jointly convex nor jointly concave because the function involves the absolute value, which is not everywhere differentiable.\n\nE. The convexity property of $D_{TV}(P, Q)$ is dependent on the size $n$ of the underlying alphabet; it is convex only for $n \\le 2$.", "solution": "We rewrite the Total Variation distance as a norm of a linear transformation:\n$$\nD_{TV}(P,Q)=\\frac{1}{2}\\sum_{i=1}^{n}|p_{i}-q_{i}|=\\frac{1}{2}\\|P-Q\\|_{1}.\n$$\nFix any two pairs of distributions $(P_{1},Q_{1})$ and $(P_{2},Q_{2})$, and any $\\lambda\\in[0,1]$. Define\n$$\nP_{\\lambda}=\\lambda P_{1}+(1-\\lambda)P_{2},\\qquad Q_{\\lambda}=\\lambda Q_{1}+(1-\\lambda)Q_{2}.\n$$\nThen\n$$\nP_{\\lambda}-Q_{\\lambda}=\\lambda(P_{1}-Q_{1})+(1-\\lambda)(P_{2}-Q_{2}).\n$$\nUsing the convexity of the $\\ell_{1}$ norm (equivalently, the triangle inequality together with positive homogeneity), we have for any vectors $a,b$ and any $\\lambda\\in[0,1]$,\n$$\n\\|\\lambda a+(1-\\lambda)b\\|_{1}\\leq \\lambda\\|a\\|_{1}+(1-\\lambda)\\|b\\|_{1}.\n$$\nApplying this with $a=P_{1}-Q_{1}$ and $b=P_{2}-Q_{2}$ yields\n$$\n\\|P_{\\lambda}-Q_{\\lambda}\\|_{1}=\\|\\lambda(P_{1}-Q_{1})+(1-\\lambda)(P_{2}-Q_{2})\\|_{1}\\leq \\lambda\\|P_{1}-Q_{1}\\|_{1}+(1-\\lambda)\\|P_{2}-Q_{2}\\|_{1}.\n$$\nMultiplying both sides by $\\frac{1}{2}$ gives\n$$\nD_{TV}(P_{\\lambda},Q_{\\lambda})\\leq \\lambda D_{TV}(P_{1},Q_{1})+(1-\\lambda)D_{TV}(P_{2},Q_{2}),\n$$\nwhich proves that $D_{TV}$ is jointly convex in $(P,Q)$. As an immediate corollary, for any fixed $Q$, the mapping $P\\mapsto D_{TV}(P,Q)=\\frac{1}{2}\\|P-Q\\|_{1}$ is convex in $P$.\n\nWe now argue it is not jointly concave. Suppose, toward contradiction, that joint concavity holds. Consider the function $g(u)=\\frac{1}{2}\\|u\\|_{1}$ on the set of attainable differences $u=P-Q$. For any $u$ in this set with $u\\neq 0$, joint concavity would imply, for $\\lambda\\in(0,1)$,\n$$\ng(\\lambda u+(1-\\lambda)(-u))\\geq \\lambda g(u)+(1-\\lambda)g(-u)=\\lambda g(u)+(1-\\lambda)g(u)=g(u).\n$$\nBut the left-hand side equals $g((2\\lambda-1)u)=\\frac{1}{2}|2\\lambda-1|\\|u\\|_{1}<\\frac{1}{2}\\|u\\|_{1}=g(u)$ for $\\lambda\\in(0,1)\\setminus\\{\\tfrac{1}{2}\\}$, a contradiction. Hence $D_{TV}$ is not jointly concave.\n\nTherefore:\n- Statement B is true: $D_{TV}$ is jointly convex in $(P,Q)$.\n- Statement C is incomplete/false as it denies joint convexity.\n- Statement A is false (it is not jointly concave).\n- Statement D is false because nondifferentiability does not preclude convexity (absolute value is a standard convex, nondifferentiable function).\n- Statement E is false because the convexity argument holds for any $n$; it depends only on norm properties and linearity, not on dimension.\n\nThus the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1614164"}]}