{"hands_on_practices": [{"introduction": "To begin, let's build our intuition with a direct application of the log sum inequality. This first exercise [@problem_id:1637885] strips the concept down to its core, asking you to apply the inequality to simple two-component vectors. By working through this foundational example, you will become familiar with the structure of the inequality and how to use it to establish a lower bound for a given expression.", "problem": "In digital signal processing, a common task is to compare feature vectors extracted from signals. Consider two feature vectors, $\\mathbf{a} = \\langle a_0, a_1 \\rangle$ and $\\mathbf{b} = \\langle b_0, b_1 \\rangle$, where the components are positive real numbers representing metrics associated with two distinct states, '0' and '1', of a binary signal.\n\nA fundamental tool for comparing such quantities is the log sum inequality. It states that for any two sets of positive real numbers, $\\{x_1, x_2, \\dots, x_n\\}$ and $\\{y_1, y_2, \\dots, y_n\\}$, the following relationship holds:\n$$\n\\sum_{i=1}^{n} x_i \\ln \\left( \\frac{x_i}{y_i} \\right) \\ge \\left( \\sum_{i=1}^{n} x_i \\right) \\ln \\left( \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} y_i} \\right)\n$$\nHere, $\\ln$ denotes the natural logarithm.\n\nThe expression $\\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right)$ can be interpreted as a generalized divergence measure between the feature vectors $\\mathbf{a}$ and $\\mathbf{b}$. According to the log sum inequality, this divergence measure has a guaranteed lower bound that can be expressed in terms of the components of $\\mathbf{a}$ and $\\mathbf{b}$.\n\nWhat is this lower bound? Provide your answer as a single closed-form analytic expression in terms of $a_0, a_1, b_0,$ and $b_1$.", "solution": "The objective is to find the lower bound for the expression $\\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right)$ by applying the provided log sum inequality.\n\nThe log sum inequality is given as:\n$$\n\\sum_{i=1}^{n} x_i \\ln \\left( \\frac{x_i}{y_i} \\right) \\ge \\left( \\sum_{i=1}^{n} x_i \\right) \\ln \\left( \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} y_i} \\right)\n$$\nThis inequality states that the term on the left-hand side (LHS) is always greater than or equal to the term on the right-hand side (RHS). Therefore, the RHS of the inequality represents the lower bound for the LHS.\n\nIn our specific problem, we are dealing with two-component vectors, so the number of terms in the sum is $n=2$. The indices for summation are given as $i \\in \\{0, 1\\}$. We can match the terms in our problem to the general form of the inequality.\n\nLet the set $\\{x_1, x_2, \\dots, x_n\\}$ correspond to the components of vector $\\mathbf{a}$. Since our indices are 0 and 1, we can set:\n$x_0 = a_0$\n$x_1 = a_1$\n\nSimilarly, let the set $\\{y_1, y_2, \\dots, y_n\\}$ correspond to the components of vector $\\mathbf{b}$:\n$y_0 = b_0$\n$y_1 = b_1$\n\nThe divergence expression given in the problem is $\\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right)$. This expression is precisely the LHS of the log sum inequality for our specific sets of numbers.\n$$\n\\text{LHS} = \\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right) = a_0 \\ln \\left(\\frac{a_0}{b_0}\\right) + a_1 \\ln \\left(\\frac{a_1}{b_1}\\right)\n$$\n\nTo find the lower bound for this expression, we need to evaluate the RHS of the log sum inequality using our specific values. The general form of the RHS is:\n$$\n\\text{RHS} = \\left( \\sum_{i=0}^{1} x_i \\right) \\ln \\left( \\frac{\\sum_{i=0}^{1} x_i}{\\sum_{i=0}^{1} y_i} \\right)\n$$\n\nFirst, we calculate the sums:\nThe sum of the $x_i$ terms is $\\sum_{i=0}^{1} x_i = x_0 + x_1 = a_0 + a_1$.\nThe sum of the $y_i$ terms is $\\sum_{i=0}^{1} y_i = y_0 + y_1 = b_0 + b_1$.\n\nNow, we substitute these sums into the expression for the RHS:\n$$\n\\text{RHS} = (a_0 + a_1) \\ln \\left( \\frac{a_0 + a_1}{b_0 + b_1} \\right)\n$$\n\nAccording to the log sum inequality, $\\text{LHS} \\ge \\text{RHS}$. Thus, the expression $(a_0 + a_1) \\ln \\left( \\frac{a_0 + a_1}{b_0 + b_1} \\right)$ is the lower bound for the given divergence measure. This is the desired closed-form analytic expression.", "answer": "$$\\boxed{(a_0 + a_1) \\ln\\left(\\frac{a_0 + a_1}{b_0 + b_1}\\right)}$$", "id": "1637885"}, {"introduction": "Beyond simple mechanics, the log sum inequality is a cornerstone for proving profound results in information theory. This practice problem [@problem_id:1637863] guides you in using the inequality to demonstrate a fundamental property of the Kullback-Leibler (KL) divergenceâ€”that it is always non-negative. This result, known as Gibbs' inequality, is essential for understanding how we measure the 'distance' between probability distributions.", "problem": "Consider a system with three mutually exclusive outcomes, labeled 1, 2, and 3. Let $P = (p_1, p_2, p_3)$ be an arbitrary probability distribution for these outcomes, where $p_i \\ge 0$ for all $i$ and $\\sum_{i=1}^{3} p_i = 1$. Let $U = (u_1, u_2, u_3)$ be the uniform probability distribution over the same outcomes, meaning $u_1 = u_2 = u_3 = 1/3$.\n\nThe Kullback-Leibler (KL) divergence, a measure of how one probability distribution is different from a second, reference probability distribution, is defined for $P$ and $U$ as:\n$$ D_{KL}(P || U) = \\sum_{i=1}^{3} p_i \\log_2\\left(\\frac{p_i}{u_i}\\right) $$\nBy convention, if $p_i = 0$, the corresponding term $0 \\log_2(0/u_i)$ is taken to be 0.\n\nYou are given the log sum inequality, which states that for any two sets of non-negative numbers $\\{a_1, a_2, \\dots, a_n\\}$ and $\\{b_1, b_2, \\dots, b_n\\}$:\n$$ \\sum_{i=1}^n a_i \\log_2 \\left( \\frac{a_i}{b_i} \\right) \\ge \\left( \\sum_{i=1}^n a_i \\right) \\log_2 \\left( \\frac{\\sum_{i=1}^n a_i}{\\sum_{i=1}^n b_i} \\right) $$\nEquality holds if and only if $\\frac{a_i}{b_i} = c$ for some constant $c$ for all $i$ where $a_i > 0$.\n\nUsing this inequality, determine the minimum possible value of $D_{KL}(P || U)$ over all possible probability distributions $P$.", "solution": "We apply the log sum inequality to the KL divergence. Set $n=3$, $a_{i}=p_{i}$, and $b_{i}=u_{i}$ for $i \\in \\{1,2,3\\}$. Then $a_{i} \\ge 0$, $b_{i} \\ge 0$, and\n$$\n\\sum_{i=1}^{3} a_{i} = \\sum_{i=1}^{3} p_{i} = 1, \\qquad \\sum_{i=1}^{3} b_{i} = \\sum_{i=1}^{3} u_{i} = 1.\n$$\nThe log sum inequality with base $2$ gives\n$$\n\\sum_{i=1}^{3} p_{i} \\log_2\\left(\\frac{p_{i}}{u_{i}}\\right) \\ge \\left(\\sum_{i=1}^{3} p_{i}\\right) \\log_2\\left(\\frac{\\sum_{i=1}^{3} p_{i}}{\\sum_{i=1}^{3} u_{i}}\\right) = 1 \\cdot \\log_2(1) = 0.\n$$\nBy the definition of KL divergence,\n$$\nD_{KL}(P || U) = \\sum_{i=1}^{3} p_{i} \\log_2\\left(\\frac{p_{i}}{u_{i}}\\right) \\ge 0.\n$$\nTo see this lower bound is attainable, evaluate at $P=U$:\n$$\nD_{KL}(U || U) = \\sum_{i=1}^{3} u_{i} \\log_2\\left(\\frac{u_{i}}{u_{i}}\\right) = \\sum_{i=1}^{3} u_{i} \\log_2(1) = 0.\n$$\nTherefore, the minimum possible value of $D_{KL}(P || U)$ over all admissible $P$ is $0$, achieved at $P=U$.", "answer": "$$\\boxed{0}$$", "id": "1637863"}, {"introduction": "A complete understanding of any inequality includes knowing the specific conditions under which it becomes an equality. This final exercise [@problem_id:1637900] shifts our focus to this very aspect of the log sum inequality. By exploring the case where the two sequences are proportional, you will translate this theoretical condition into a concrete algebraic problem, reinforcing the deep relationship between the elements being compared.", "problem": "In information theory, the log sum inequality relates two sequences of positive numbers, $a=(a_1, \\dots, a_n)$ and $b=(b_1, \\dots, b_n)$. A key aspect of this inequality is its condition for equality: the inequality becomes an exact equation if and only if the two sequences are proportional. This means the ratio $\\frac{a_i}{b_i}$ must be a constant for all corresponding pairs of elements $(a_i, b_i)$.\n\nConsider the specific two-element sequences $a=(2, 8)$ and $b=(x, y)$, where $x$ and $y$ are positive real numbers. It is given that these two sequences satisfy the equality condition of the log sum inequality. Furthermore, the product of the terms in the second sequence is constrained such that $xy=32$.\n\nYour task is to find the value of the sum $x+y$.", "solution": "The problem provides two conditions that the positive real numbers $x$ and $y$ must satisfy. We will use these two conditions to form a system of equations and solve for $x$ and $y$.\n\nFirst, we use the condition for equality in the log sum inequality. The problem states that this occurs when the sequences $a=(a_1, a_2)$ and $b=(b_1, b_2)$ are proportional. This means the ratio of corresponding elements is constant:\n$$ \\frac{a_1}{b_1} = \\frac{a_2}{b_2} $$\nWe are given the sequences $a=(2, 8)$ and $b=(x, y)$. Substituting these values into the proportionality condition gives:\n$$ \\frac{2}{x} = \\frac{8}{y} $$\nWe can rearrange this equation to find a relationship between $x$ and $y$. Cross-multiplying yields:\n$$ 2y = 8x $$\nDividing both sides by 2, we get a simpler linear relationship:\n$$ y = 4x $$\n\nSecond, we use the additional constraint given in the problem statement:\n$$ xy = 32 $$\n\nNow we have a system of two equations with two unknowns:\n1. $y = 4x$\n2. $xy = 32$\n\nWe can solve this system by substituting the expression for $y$ from the first equation into the second equation:\n$$ x(4x) = 32 $$\nThis simplifies to:\n$$ 4x^2 = 32 $$\nDivide both sides by 4:\n$$ x^2 = 8 $$\nSince the problem states that $x$ is a positive real number, we take the positive square root of 8:\n$$ x = \\sqrt{8} = \\sqrt{4 \\times 2} = 2\\sqrt{2} $$\n\nNow that we have the value for $x$, we can find the value for $y$ using the relationship $y=4x$:\n$$ y = 4(2\\sqrt{2}) = 8\\sqrt{2} $$\n\nThe problem asks for the value of the sum $x+y$. We add the values we found for $x$ and $y$:\n$$ x+y = 2\\sqrt{2} + 8\\sqrt{2} $$\nCombining the terms, we get:\n$$ x+y = (2+8)\\sqrt{2} = 10\\sqrt{2} $$\nThus, the value of the sum $x+y$ is $10\\sqrt{2}$.", "answer": "$$\\boxed{10\\sqrt{2}}$$", "id": "1637900"}]}