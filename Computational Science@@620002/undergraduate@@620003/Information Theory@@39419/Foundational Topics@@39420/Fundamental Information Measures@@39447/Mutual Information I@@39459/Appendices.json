{"hands_on_practices": [{"introduction": "Let's begin by exploring the most direct type of relationship between two variables. This problem examines a scenario where one variable, $Y$, is completely determined by another, $X$. By working through this exercise [@problem_id:1642332], you will see how mutual information $I(X;Y)$ elegantly simplifies to the entropy of the determined variable, $H(Y)$, providing a clear and foundational understanding of information transfer.", "problem": "An integer $X$ is selected uniformly at random from the set $\\{1, 2, \\dots, 100\\}$. A second random variable $Y$ is defined to be the last digit of the integer $X$. For example, if the selected integer is $X=57$, then the value of $Y$ is 7. If the selected integer is $X=90$, the value of $Y$ is 0.\n\nDetermine the mutual information $I(X;Y)$ between these two random variables. Express your answer as a single closed-form analytic expression in units of bits, which implies the use of the logarithm to the base 2.", "solution": "Let $X$ be uniformly distributed on $\\{1,2,\\dots,100\\}$, so $P(X=x)=\\frac{1}{100}$ for each $x$. Define $Y$ as the last digit of $X$. Then $Y$ is a deterministic function of $X$, which implies the conditional entropy satisfies $H(Y|X)=0$. By the definition of mutual information,\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nTo compute $H(Y)$, determine the distribution of $Y$. For each $y\\in\\{0,1,\\dots,9\\}$, the integers in $\\{1,\\dots,100\\}$ with last digit $y$ are exactly $10$ values, hence\n$$\nP(Y=y)=\\frac{10}{100}=\\frac{1}{10},\\quad y\\in\\{0,1,\\dots,9\\}.\n$$\nThus $Y$ is uniform on $10$ outcomes, and its entropy in bits is\n$$\nH(Y)=-\\sum_{y=0}^{9}\\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)=\\log_{2}(10).\n$$\nTherefore,\n$$\nI(X;Y)=\\log_{2}(10).\n$$\nAs a cross-check, using $I(X;Y)=H(X)-H(X|Y)$, we have $H(X)=\\log_{2}(100)$ and, given any $Y=y$, the $10$ possible $X$ values are equally likely, so $H(X|Y=y)=\\log_{2}(10)$ for all $y$, yielding $H(X|Y)=\\log_{2}(10)$ and hence $I(X;Y)=\\log_{2}(100)-\\log_{2}(10)=\\log_{2}(10)$.", "answer": "$$\\boxed{\\log_{2}(10)}$$", "id": "1642332"}, {"introduction": "Building on the basics, we now move to a more common and nuanced situation where variables are probabilistically linked. This classic exercise [@problem_id:1642355] uses a simple real-world scenario to demonstrate how knowing the outcome of one event changes our certainty about another. This practice will sharpen your skills in calculating joint and conditional probabilities, which are essential for computing mutual information.", "problem": "Consider a family with two children. Assume the gender of each child is independent and the probability of having a boy (B) or a girl (G) is equal for each birth. Let $X$ be a binary random variable such that $X=1$ if the first child is a girl and $X=0$ otherwise. Let $Y$ be another binary random variable such that $Y=1$ if both children are girls and $Y=0$ otherwise.\n\nCalculate the mutual information $I(X; Y)$ between these two random variables. Express your answer in bits as a single closed-form analytic expression.", "solution": "The sample space of ordered pairs of children is $\\{BB, BG, GB, GG\\}$ with each outcome having probability $\\frac{1}{4}$ by independence and equal likelihood of genders.\n\nBy definition, $X=1$ if the first child is a girl, else $X=0$. Thus $P(X=1)=P(GB)+P(GG)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}$ and $P(X=0)=\\frac{1}{2}$. Also, $Y=1$ if both are girls, which happens only for $GG$, so $P(Y=1)=\\frac{1}{4}$ and $P(Y=0)=\\frac{3}{4}$.\n\nThe joint distribution of $(X,Y)$ is:\n- $P(X=1,Y=1)=P(GG)=\\frac{1}{4}$,\n- $P(X=1,Y=0)=P(GB)=\\frac{1}{4}$,\n- $P(X=0,Y=1)=0$ (impossible, since $Y=1$ implies the first child is a girl),\n- $P(X=0,Y=0)=P(BB)+P(BG)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}$.\n\nUsing the definition of entropy $H(Y)=-\\sum_{y}P(Y=y)\\log_{2}P(Y=y)$, we have\n$$\nH(Y)=-\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right).\n$$\nThe conditional entropy is $H(Y\\mid X)=\\sum_{x}P(X=x)H(Y\\mid X=x)$. Given $X=0$, $Y=0$ with probability $1$, so $H(Y\\mid X=0)=0$. Given $X=1$, the second child is equally likely to be $G$ or $B$, so $P(Y=1\\mid X=1)=\\frac{1}{2}$ and $P(Y=0\\mid X=1)=\\frac{1}{2}$, hence $H(Y\\mid X=1)=1$. Therefore,\n$$\nH(Y\\mid X)=P(X=0)\\cdot 0+P(X=1)\\cdot 1=\\frac{1}{2}.\n$$\nBy the definition of mutual information, $I(X;Y)=H(Y)-H(Y\\mid X)$, so\n$$\nI(X;Y)=-\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right)-\\frac{1}{2}.\n$$\nUsing $\\log_{2}\\left(\\frac{1}{4}\\right)=-2$, the first term simplifies to $\\frac{1}{2}$, which cancels the final $-\\frac{1}{2}$ term. This yields\n$$\nI(X;Y)=-\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right)=\\frac{3}{4}\\log_{2}\\left(\\frac{4}{3}\\right).\n$$\nThis is the mutual information in bits.", "answer": "$$\\boxed{-\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right)}$$", "id": "1642355"}, {"introduction": "Our final practice explores a more subtle but powerful concept: how two variables can share information through a common underlying cause, even without a direct link. This principle is fundamental in fields from signal processing to genetics, where correlations arise from shared components or ancestry. This problem [@problem_id:1642335] will challenge you to dissect the structure of random variables to uncover and quantify these indirect statistical dependencies.", "problem": "Consider a simple signal processing model where new signals are generated from sums of input bits. Let $X_1$, $X_2$, and $X_3$ be three mutually independent random variables, each representing a bit that is equally likely to be 0 or 1. That is, for each $i \\in \\{1, 2, 3\\}$, $P(X_i=0) = P(X_i=1) = \\frac{1}{2}$.\n\nTwo new random variables, $Y$ and $Z$, are formed by taking sums of these bits as follows:\n$$Y = X_1 + X_2$$\n$$Z = X_2 + X_3$$\n\nCalculate the mutual information $I(Y; Z)$. Express your answer in bits, using the base-2 logarithm for all calculations. The final answer should be a single real number, which can be provided as a fraction or a decimal.", "solution": "We are given independent bits $X_{1},X_{2},X_{3}$ with $P(X_{i}=0)=P(X_{i}=1)=\\frac{1}{2}$ for each $i$. Define $Y=X_{1}+X_{2}$ and $Z=X_{2}+X_{3}$. The mutual information in bits is\n$$\nI(Y;Z)=H(Y)+H(Z)-H(Y,Z),\n$$\nwhere all entropies use $\\log_{2}$.\n\nFirst compute the marginals. Since $Y=X_{1}+X_{2}$ is the sum of two independent Bernoulli random variables with parameter $\\frac{1}{2}$,\n$$\nP(Y=0)=\\frac{1}{4},\\quad P(Y=1)=\\frac{1}{2},\\quad P(Y=2)=\\frac{1}{4}.\n$$\nThus\n$$\nH(Y)=-\\sum_{y\\in\\{0,1,2\\}}P(Y=y)\\log_{2}P(Y=y)\n= -\\left(\\frac{1}{4}\\log_{2}\\frac{1}{4}+\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)\n=\\frac{3}{2}\\ \\text{bits}.\n$$\nBy symmetry, $H(Z)=\\frac{3}{2}$ bits.\n\nNext compute the joint distribution of $(Y,Z)$. Condition on $X_{2}$. Given $X_{2}=0$, we have $Y=X_{1}$ and $Z=X_{3}$, which are independent and each Bernoulli with parameter $\\frac{1}{2}$. Therefore,\n$$\nP\\big((Y,Z)=(a,b)\\mid X_{2}=0\\big)=\\frac{1}{4}\\quad\\text{for }(a,b)\\in\\{0,1\\}\\times\\{0,1\\}.\n$$\nGiven $X_{2}=1$, we have $Y=1+X_{1}$ and $Z=1+X_{3}$, again independent with each supported on $\\{1,2\\}$ equally likely. Therefore,\n$$\nP\\big((Y,Z)=(a,b)\\mid X_{2}=1\\big)=\\frac{1}{4}\\quad\\text{for }(a,b)\\in\\{1,2\\}\\times\\{1,2\\}.\n$$\nUsing the law of total probability with $P(X_{2}=0)=P(X_{2}=1)=\\frac{1}{2}$, the nonzero joint probabilities are:\n$$\n\\begin{aligned}\n&P(Y,Z)\\text{ at }(0,0),(0,1),(1,0),(1,2),(2,1),(2,2)\\text{ equals }\\frac{1}{8}\\ \\text{each},\\\\\n&P(Y,Z)\\text{ at }(1,1)\\text{ equals }\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n\\end{aligned}\n$$\nHence\n$$\nH(Y,Z)=-\\sum_{y,z}P(y,z)\\log_{2}P(y,z)\n=-\\left(6\\cdot\\frac{1}{8}\\log_{2}\\frac{1}{8}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)\n=\\frac{11}{4}\\ \\text{bits}.\n$$\nFinally,\n$$\nI(Y;Z)=H(Y)+H(Z)-H(Y,Z)=\\frac{3}{2}+\\frac{3}{2}-\\frac{11}{4}=\\frac{1}{4}\\ \\text{bits}.\n$$", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1642335"}]}