{"hands_on_practices": [{"introduction": "This first practice serves as a foundational exercise to solidify your understanding of the core definition of mutual information as a relative entropy. Using a hypothetical yet intuitive weather forecasting model, you will directly apply the formula $I(X;Y) = D_{KL}(p(x,y) || p(x)p(y))$. This involves the essential steps of computing marginal distributions from a joint distribution and then summing the terms to find the mutual information, giving you a tangible grasp of how KL divergence quantifies the information shared between two variables [@problem_id:1654581].", "problem": "Consider a simplified model for a weather forecasting system. Let the random variable $X$ represent the daily forecast, which can take one of three states: $x_1$ ('Forecast: Sunny'), $x_2$ ('Forecast: Cloudy'), or $x_3$ ('Forecast: Rainy'). Let the random variable $Y$ represent the actual weather outcome for that day, which can take one of two states: $y_1$ ('Actual: Sunny') or $y_2$ ('Actual: Not Sunny').\n\nAfter observing the system for a long time, the joint probability distribution $p(x, y)$ for the forecast and the actual outcome has been determined as follows:\n- $p(X=x_1, Y=y_1) = 5/16$\n- $p(X=x_1, Y=y_2) = 1/16$\n- $p(X=x_2, Y=y_1) = 1/8$\n- $p(X=x_2, Y=y_2) = 3/16$\n- $p(X=x_3, Y=y_1) = 1/16$\n- $p(X=x_3, Y=y_2) = 1/4$\n\nCalculate the mutual information $I(X; Y)$ between the forecast $X$ and the actual weather $Y$. Express your answer in units of bits, rounded to four significant figures. All logarithms should be interpreted as base 2.", "solution": "We are given the joint distribution of $(X,Y)$ over $\\{x_{1},x_{2},x_{3}\\}\\times\\{y_{1},y_{2}\\}$:\n$$\np(x_{1},y_{1})=\\frac{5}{16},\\quad p(x_{1},y_{2})=\\frac{1}{16},\\quad\np(x_{2},y_{1})=\\frac{1}{8},\\quad p(x_{2},y_{2})=\\frac{3}{16},\\quad\np(x_{3},y_{1})=\\frac{1}{16},\\quad p(x_{3},y_{2})=\\frac{1}{4}.\n$$\nFirst compute the marginals by summing the joint probabilities over the other variable:\n$$\np(x_{1})=\\frac{5}{16}+\\frac{1}{16}=\\frac{3}{8},\\quad\np(x_{2})=\\frac{1}{8}+\\frac{3}{16}=\\frac{5}{16},\\quad\np(x_{3})=\\frac{1}{16}+\\frac{1}{4}=\\frac{5}{16},\n$$\n$$\np(y_{1})=\\frac{5}{16}+\\frac{1}{8}+\\frac{1}{16}=\\frac{8}{16}=\\frac{1}{2},\\quad\np(y_{2})=\\frac{1}{16}+\\frac{3}{16}+\\frac{1}{4}=\\frac{8}{16}=\\frac{1}{2}.\n$$\nThe mutual information is\n$$\nI(X;Y)=\\sum_{x}\\sum_{y}p(x,y)\\,\\log_{2}\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right).\n$$\nSince $p(y_{1})=p(y_{2})=\\frac{1}{2}$, the ratio simplifies to $\\frac{p(x,y)}{p(x)\\,p(y)}=\\frac{2\\,p(x,y)}{p(x)}$. Compute each term:\n$$\n\\begin{aligned}\n(x_{1},y_{1}):&\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{5}{16}}{\\frac{3}{8}}=\\frac{5}{3},&&\\text{contribution } \\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right),\\\\\n(x_{1},y_{2}):&\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{3}{8}}=\\frac{1}{3},&&\\text{contribution } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right),\\\\\n(x_{2},y_{1}):&\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{1}{8}}{\\frac{5}{16}}=\\frac{4}{5},&&\\text{contribution } \\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right),\\\\\n(x_{2},y_{2}):&\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{3}{16}}{\\frac{5}{16}}=\\frac{6}{5},&&\\text{contribution } \\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right),\\\\\n(x_{3},y_{1}):&\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{5}{16}}=\\frac{2}{5},&&\\text{contribution } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right),\\\\\n(x_{3},y_{2}):&\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{4}}{\\frac{5}{16}}=\\frac{8}{5},&&\\text{contribution } \\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right).\n\\end{aligned}\n$$\nThus\n$$\n\\begin{aligned}\nI(X;Y)\n&=\\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right)+\\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right)\\\\\n&=\\frac{5}{16}(\\log_{2}5-\\log_{2}3)+\\frac{1}{16}(-\\log_{2}3)+\\frac{1}{8}(2-\\log_{2}5)+\\frac{3}{16}(\\log_{2}3+1-\\log_{2}5)+\\frac{1}{16}(1-\\log_{2}5)+\\frac{1}{4}(3-\\log_{2}5)\\\\\n&=\\left(\\frac{1}{4}+\\frac{3}{16}+\\frac{1}{16}+\\frac{3}{4}\\right)+\\left(-\\frac{5}{16}-\\frac{1}{16}+\\frac{3}{16}\\right)\\log_{2}3+\\left(\\frac{5}{16}-\\frac{1}{8}-\\frac{3}{16}-\\frac{1}{16}-\\frac{1}{4}\\right)\\log_{2}5\\\\\n&=\\frac{5}{4}-\\frac{3}{16}\\log_{2}3-\\frac{5}{16}\\log_{2}5.\n\\end{aligned}\n$$\nEvaluating numerically with base-2 logarithms and rounding to four significant figures gives\n$$\nI(X;Y)\\approx 0.2272\\ \\text{bits}.\n$$", "answer": "$$\\boxed{0.2272}$$", "id": "1654581"}, {"introduction": "Building upon the basic calculation, this exercise introduces a common scenario in real-world systems: impossibility. By working with a joint distribution where one outcome has zero probability, you will see how the definition of mutual information via KL divergence handles this situation through the convention that $0 \\log(0) = 0$. This practice reinforces your computational skills and deepens your understanding of the robustness of the formula in a practical communication channel context [@problem_id:1654642].", "problem": "Consider a simplified model for a digital communication system involving a transmitted binary signal $X$ and a received binary signal $Y$. Both $X$ and $Y$ can take values from the set $\\{0, 1\\}$. Due to a specific type of systematic failure in the channel, some combinations of transmitted and received signals are more or less likely than others, and one combination is impossible. The joint probability mass function, $p(x,y) = P(X=x, Y=y)$, that describes the system's behavior is given by:\n\n$p(0,0) = 0$\n$p(0,1) = \\frac{1}{4}$\n$p(1,0) = \\frac{1}{2}$\n$p(1,1) = \\frac{1}{4}$\n\nCalculate the mutual information $I(X;Y)$ between the transmitted and received signals. Provide your answer as a closed-form analytic expression in units of bits.", "solution": "The mutual information $I(X;Y)$ quantifies the reduction in uncertainty about the random variable $X$ due to the knowledge of the random variable $Y$. It can be expressed as the Kullback-Leibler (KL) divergence, or relative entropy, between the joint probability distribution $p(x,y)$ and the product of the marginal probability distributions $p(x)p(y)$. The formula is:\n$$I(X;Y) = D_{KL}(p(x,y) || p(x)p(y)) = \\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x,y) \\log_{2}\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\nThe logarithm is base 2 because the required unit for the answer is bits.\n\nFirst, we must calculate the marginal probability distributions for $X$ and $Y$.\n\nThe marginal distribution of $X$, denoted $p(x)$, is found by summing the joint probabilities over all possible values of $Y$:\nFor $x=0$:\n$p(X=0) = p(0,0) + p(0,1) = 0 + \\frac{1}{4} = \\frac{1}{4}$\nFor $x=1$:\n$p(X=1) = p(1,0) + p(1,1) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}$\n\nThe marginal distribution of $Y$, denoted $p(y)$, is found by summing the joint probabilities over all possible values of $X$:\nFor $y=0$:\n$p(Y=0) = p(0,0) + p(1,0) = 0 + \\frac{1}{2} = \\frac{1}{2}$\nFor $y=1$:\n$p(Y=1) = p(0,1) + p(1,1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\n\nNow we can compute the terms in the sum for $I(X;Y)$. The sum is over all four combinations of $(x,y)$.\n\nTerm for $(x,y) = (0,0)$:\nSince $p(0,0) = 0$, the entire term is $0 \\times \\log_{2}(\\dots)$. By convention, $0 \\log 0 = 0$, so this term contributes 0 to the sum.\n\nTerm for $(x,y) = (0,1)$:\nThe product of marginals is $p(X=0)p(Y=1) = (\\frac{1}{4})(\\frac{1}{2}) = \\frac{1}{8}$.\nThe term is:\n$$p(0,1) \\log_{2}\\left(\\frac{p(0,1)}{p(X=0)p(Y=1)}\\right) = \\frac{1}{4} \\log_{2}\\left(\\frac{1/4}{1/8}\\right) = \\frac{1}{4} \\log_{2}(2) = \\frac{1}{4} \\times 1 = \\frac{1}{4}$$\n\nTerm for $(x,y) = (1,0)$:\nThe product of marginals is $p(X=1)p(Y=0) = (\\frac{3}{4})(\\frac{1}{2}) = \\frac{3}{8}$.\nThe term is:\n$$p(1,0) \\log_{2}\\left(\\frac{p(1,0)}{p(X=1)p(Y=0)}\\right) = \\frac{1}{2} \\log_{2}\\left(\\frac{1/2}{3/8}\\right) = \\frac{1}{2} \\log_{2}\\left(\\frac{4}{3}\\right)$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$:\n$$\\frac{1}{2} (\\log_{2}(4) - \\log_{2}(3)) = \\frac{1}{2} (2 - \\log_{2}(3)) = 1 - \\frac{1}{2}\\log_{2}(3)$$\n\nTerm for $(x,y) = (1,1)$:\nThe product of marginals is $p(X=1)p(Y=1) = (\\frac{3}{4})(\\frac{1}{2}) = \\frac{3}{8}$.\nThe term is:\n$$p(1,1) \\log_{2}\\left(\\frac{p(1,1)}{p(X=1)p(Y=1)}\\right) = \\frac{1}{4} \\log_{2}\\left(\\frac{1/4}{3/8}\\right) = \\frac{1}{4} \\log_{2}\\left(\\frac{2}{3}\\right)$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$:\n$$\\frac{1}{4} (\\log_{2}(2) - \\log_{2}(3)) = \\frac{1}{4} (1 - \\log_{2}(3)) = \\frac{1}{4} - \\frac{1}{4}\\log_{2}(3)$$\n\nFinally, we sum all the terms to find the mutual information:\n$$I(X;Y) = 0 + \\frac{1}{4} + \\left(1 - \\frac{1}{2}\\log_{2}(3)\\right) + \\left(\\frac{1}{4} - \\frac{1}{4}\\log_{2}(3)\\right)$$\n$$I(X;Y) = \\left(\\frac{1}{4} + 1 + \\frac{1}{4}\\right) - \\left(\\frac{1}{2}\\log_{2}(3) + \\frac{1}{4}\\log_{2}(3)\\right)$$\n$$I(X;Y) = \\frac{3}{2} - \\frac{3}{4}\\log_{2}(3)$$\nThis is the final closed-form analytic expression for the mutual information in bits.", "answer": "$$\\boxed{\\frac{3}{2} - \\frac{3}{4}\\log_{2}(3)}$$", "id": "1654642"}, {"introduction": "This final practice elevates our perspective from numerical computation to conceptual proof, demonstrating the true power of the relative entropy framework. You will use the KL divergence definition of conditional mutual information to derive a cornerstone of information theory for a Markov chain. By showing that $I(X;Z|Y)=0$ if $X \\to Y \\to Z$, this exercise reveals how the mathematical definition establishes fundamental principles about information flow, such as the Data Processing Inequality [@problem_id:1654632].", "problem": "Consider a sequence of three discrete random variables $X, Y, Z$ that form a Markov chain, denoted as $X \\to Y \\to Z$. This relationship implies that given the present state $Y$, the future state $Z$ is conditionally independent of the past state $X$. Mathematically, this condition is defined as $p(z | x, y) = p(z | y)$ for all values $x, y, z$ for which the joint probability $p(x, y) > 0$.\n\nIn information theory, the conditional mutual information $I(X; Z | Y)$ quantifies the amount of information that $X$ and $Z$ share, given that $Y$ is known. It can be defined in terms of the Kullback-Leibler (KL) divergence between the true conditional joint distribution $p(x, z | y)$ and the product of the conditional marginal distributions $p(x|y)p(z|y)$. The formula is given by:\n\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x, z | y)}{p(x|y)p(z|y)} \\right)$$\n\nwhere the sums are over all possible values of the random variables.\n\nGiven these definitions, calculate the value of the conditional mutual information $I(X; Z | Y)$ for any set of variables forming a Markov chain $X \\to Y \\to Z$. Express your answer as a single number in units of bits.", "solution": "The problem asks for the calculation of the conditional mutual information $I(X; Z | Y)$ for a set of random variables $X, Y, Z$ that form a Markov chain $X \\to Y \\to Z$. We are given the definition of $I(X; Z | Y)$ in terms of the Kullback-Leibler divergence:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x, z | y)}{p(x|y)p(z|y)} \\right)$$\nOur goal is to simplify the term inside the logarithm using the properties of the Markov chain.\n\nThe key to solving this is to show that for a Markov chain, the numerator inside the logarithm, $p(x, z | y)$, is equal to the denominator, $p(x|y)p(z|y)$. Let's prove this factorization.\n\nBy the definition of conditional probability, the term $p(x, z | y)$ can be written as:\n$$p(x, z | y) = \\frac{p(x, y, z)}{p(y)}$$\nNow, we apply the chain rule of probability to the joint distribution $p(x, y, z)$:\n$$p(x, y, z) = p(z | x, y) p(x, y)$$\nSubstituting this back into the expression for $p(x, z | y)$:\n$$p(x, z | y) = \\frac{p(z | x, y) p(x, y)}{p(y)}$$\nWe are given that $X, Y, Z$ form a Markov chain $X \\to Y \\to Z$. The defining property of this chain is that $p(z | x, y) = p(z | y)$. We can substitute this property into our equation:\n$$p(x, z | y) = \\frac{p(z | y) p(x, y)}{p(y)}$$\nNext, we can rearrange the terms on the right-hand side:\n$$p(x, z | y) = p(z | y) \\left( \\frac{p(x, y)}{p(y)} \\right)$$\nWe recognize the term in the parentheses as the definition of the conditional probability $p(x|y)$:\n$$p(x|y) = \\frac{p(x, y)}{p(y)}$$\nSubstituting this back, we get the desired factorization:\n$$p(x, z | y) = p(z | y) p(x | y)$$\nThis result shows that for a Markov chain, the variables $X$ and $Z$ are conditionally independent given $Y$.\n\nNow we can substitute this result back into the original formula for conditional mutual information:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x|y)p(z|y)}{p(x|y)p(z|y)} \\right)$$\nThe fraction inside the logarithm simplifies to 1:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2}(1)$$\nSince $\\log_{2}(1) = 0$, the expression becomes:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\cdot 0$$\nThis simplifies to:\n$$I(X; Z | Y) = 0$$\nThus, the conditional mutual information between $X$ and $Z$ given $Y$ is 0 for any Markov chain $X \\to Y \\to Z$. This is an important result known as the Data Processing Inequality, of which this is a specific case. The value is 0 bits.", "answer": "$$\\boxed{0}$$", "id": "1654632"}]}