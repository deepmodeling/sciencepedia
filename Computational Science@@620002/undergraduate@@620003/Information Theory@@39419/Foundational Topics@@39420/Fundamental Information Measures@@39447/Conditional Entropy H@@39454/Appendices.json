{"hands_on_practices": [{"introduction": "Let's begin with a foundational exercise to build our intuition for conditional entropy. We'll use the familiar scenario of a fair die roll to quantify the remaining uncertainty about the die's outcome once we are told whether it is even or odd. By working through this practice, you will see how partial information reduces uncertainty and how to apply the definition of conditional entropy, $H(X|Y)$, in a clear and direct manner [@problem_id:1612385].", "problem": "Consider a standard, fair six-sided die. Let the random variable $X$ represent the outcome of a single roll of this die, so its set of possible values is $\\{1, 2, 3, 4, 5, 6\\}$, with each outcome being equally likely. A second random variable, $Y$, is determined by the parity of $X$. Specifically, $Y$ takes the value 0 if the outcome $X$ is an even number, and $Y$ takes the value 1 if the outcome $X$ is an odd number.\n\nAn experiment is performed where the die is rolled, but you are only told the value of $Y$ (i.e., you are only told whether the outcome was even or odd). We are interested in the amount of uncertainty that remains about the specific outcome $X$ after the parity information $Y$ has been revealed.\n\nCalculate this remaining average uncertainty about the outcome $X$. Express your answer in units of bits, which implies the use of a base-2 logarithm for all entropy calculations. Present your answer as a single, closed-form analytic expression.", "solution": "Let $X$ be uniformly distributed on $\\{1,2,3,4,5,6\\}$, so $P(X=k)=\\frac{1}{6}$ for each $k$. Let $Y$ be the parity of $X$, with $Y=0$ if $X\\in\\{2,4,6\\}$ and $Y=1$ if $X\\in\\{1,3,5\\}$. Since each of the three even and three odd outcomes are equally likely, we have\n$$\nP(Y=0)=\\sum_{x\\in\\{2,4,6\\}}P(X=x)=3\\cdot\\frac{1}{6}=\\frac{1}{2},\\quad P(Y=1)=\\frac{1}{2}.\n$$\nBecause $Y$ is a deterministic function of $X$, for $y\\in\\{0,1\\}$ and $x$ consistent with $y$,\n$$\nP(X=x\\mid Y=y)=\\frac{P(X=x, Y=y)}{P(Y=y)}=\\frac{P(X=x)}{P(Y=y)}=\\frac{\\frac{1}{6}}{\\frac{1}{2}}=\\frac{1}{3},\n$$\nand $P(X=x\\mid Y=y)=0$ otherwise. Thus, conditional on $Y=y$, $X$ is uniform over a set of three outcomes.\n\nThe conditional entropy given $Y=y$ is\n$$\nH(X\\mid Y=y)=-\\sum_{x}P(X=x\\mid Y=y)\\log_{2}\\bigl(P(X=x\\mid Y=y)\\bigr)\n=-3\\cdot\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)=\\log_{2}(3).\n$$\nThe average remaining uncertainty is the conditional entropy\n$$\nH(X\\mid Y)=\\sum_{y\\in\\{0,1\\}}P(Y=y)\\,H(X\\mid Y=y)\n=\\frac{1}{2}\\log_{2}(3)+\\frac{1}{2}\\log_{2}(3)=\\log_{2}(3).\n$$\nTherefore, the remaining average uncertainty about $X$ after observing $Y$ is $\\log_{2}(3)$ bits.", "answer": "$$\\boxed{\\log_{2}(3)}$$", "id": "1612385"}, {"introduction": "This next practice explores a more nuanced aspect of conditional entropy. Here, we consider two independent coin flips and are only told their sum. Unlike the previous problem, the amount of remaining uncertainty about one flip's outcome depends on what the sum is. This exercise is designed to clarify how conditional entropy, $H(X|S)$, represents the *average* uncertainty, calculated by weighing the uncertainty for each possible piece of information we might receive [@problem_id:1612398].", "problem": "Consider two independent binary sources, Source 1 and Source 2. Source 1 produces a random variable $X$, and Source 2 produces a random variable $Y$. Both $X$ and $Y$ can take values in the set $\\{0, 1\\}$, with $P(X=1) = P(X=0) = \\frac{1}{2}$ and $P(Y=1) = P(Y=0) = \\frac{1}{2}$.\n\nAn observer does not have access to the individual outputs $X$ and $Y$. Instead, the observer only knows the value of their sum, $S = X+Y$.\n\nCalculate the average uncertainty about the value of $X$ that remains after the value of $S$ is revealed. Express your final answer as a single real number in units of bits.", "solution": "We are asked to compute the average uncertainty about $X$ after observing $S=X+Y$, which is the conditional entropy $H(X|S)$ measured in bits. Throughout, we use base-2 logarithms, denoted $\\log_{2}$.\n\nGiven $X,Y \\in \\{0,1\\}$, independent with $P(X=1)=P(X=0)=\\frac{1}{2}$ and $P(Y=1)=P(Y=0)=\\frac{1}{2}$, the sum $S=X+Y$ takes values in $\\{0,1,2\\}$ with\n$$\nP(S=0)=P(X=0,Y=0)=\\frac{1}{4},\\quad P(S=2)=P(X=1,Y=1)=\\frac{1}{4},\\quad P(S=1)=\\frac{1}{2}.\n$$\nWe compute $H(X|S)$ via\n$$\nH(X|S)=\\sum_{s\\in\\{0,1,2\\}} P(S=s)\\,H(X|S=s).\n$$\nFor $s=0$, the event $S=0$ implies $(X,Y)=(0,0)$, hence $P(X=0|S=0)=1$ and\n$$\nH(X|S=0)=-\\left(1\\cdot \\log_{2}1\\right)=0.\n$$\nFor $s=2$, the event $S=2$ implies $(X,Y)=(1,1)$, hence $P(X=1|S=2)=1$ and\n$$\nH(X|S=2)=0.\n$$\nFor $s=1$, the event $S=1$ corresponds to $(X,Y)=(1,0)$ or $(0,1)$. Using independence,\n$$\nP(X=1,S=1)=P(X=1,Y=0)=\\frac{1}{4},\\quad P(S=1)=\\frac{1}{2},\n$$\nso\n$$\nP(X=1|S=1)=\\frac{P(X=1,S=1)}{P(S=1)}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\\quad P(X=0|S=1)=\\frac{1}{2}.\n$$\nThus\n$$\nH(X|S=1)=-\\left(\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{2}\\log_{2}\\frac{1}{2}\\right)=1.\n$$\nTherefore,\n$$\nH(X|S)=P(S=0)\\cdot 0+P(S=1)\\cdot 1+P(S=2)\\cdot 0=\\frac{1}{2}\\ \\text{bits}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1612398"}, {"introduction": "We conclude with a famous puzzle that beautifully illustrates the power of information theory. The Monty Hall problem is a classic brain-teaser about probability, but we can analyze it through the lens of conditional entropy. This practice challenges you to calculate the contestant's remaining uncertainty about the car's location *after* the host reveals a goat, connecting the abstract concept of conditional entropy to the practical process of updating beliefs based on new, strategically revealed information [@problem_id:1612388].", "problem": "Consider a variation of a classic game show problem designed to explore the flow of information. There are three identical, closed doors: Door 1, Door 2, and Door 3. Behind one door is a valuable car, and behind the other two are goats. The placement of the car is random, with each door having an equal probability of hiding the car.\n\nYou, the contestant, make an initial choice and select Door 1.\n\nThe host, who knows the location of the car, then opens one of the two remaining doors (Door 2 or Door 3) to reveal a goat. The host's strategy is as follows: if the car is behind your chosen door (Door 1), the host opens one of the other two doors (Door 2 or Door 3) with equal probability. If the car is not behind your chosen door, the host is constrained to open the only other door that hides a goat.\n\nLet $C$ be the random variable representing the door number hiding the car, with possible outcomes $\\{1, 2, 3\\}$. Let $H$ be the random variable representing the door number opened by the host, with possible outcomes $\\{2, 3\\}$.\n\nCalculate the conditional entropy $H(C|H)$, which quantifies the average remaining uncertainty about the car's location after you have observed the host's action. Express your answer as a single analytic expression in bits. Note that information theory calculations use base-2 logarithms.", "solution": "Let $C \\in \\{1,2,3\\}$ be the car’s door with $P(C=c)=\\frac{1}{3}$ for each $c$, and let $H \\in \\{2,3\\}$ be the host’s opened door. The host policy implies the conditional probabilities:\n$$\nP(H=2 \\mid C=1)=\\frac{1}{2}, \\quad P(H=3 \\mid C=1)=\\frac{1}{2}, \\quad P(H=2 \\mid C=2)=0, \\quad P(H=3 \\mid C=2)=1,\n$$\n$$\nP(H=2 \\mid C=3)=1, \\quad P(H=3 \\mid C=3)=0.\n$$\nBy the law of total probability,\n$$\nP(H=2)=\\sum_{c=1}^{3} P(H=2 \\mid C=c)P(C=c)=\\frac{1}{2}\\cdot \\frac{1}{3}+0\\cdot \\frac{1}{3}+1\\cdot \\frac{1}{3}=\\frac{1}{2},\n$$\nand similarly $P(H=3)=\\frac{1}{2}$.\n\nUsing Bayes’ rule, for $H=2$,\n$$\nP(C=1 \\mid H=2)=\\frac{P(H=2 \\mid C=1)P(C=1)}{P(H=2)}=\\frac{\\frac{1}{2}\\cdot \\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}, \\quad\nP(C=3 \\mid H=2)=\\frac{1\\cdot \\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}, \\quad\nP(C=2 \\mid H=2)=0.\n$$\nBy symmetry, for $H=3$,\n$$\nP(C=1 \\mid H=3)=\\frac{1}{3}, \\quad P(C=2 \\mid H=3)=\\frac{2}{3}, \\quad P(C=3 \\mid H=3)=0.\n$$\n\nThe conditional entropy is\n$$\nH(C \\mid H)=\\sum_{h \\in \\{2,3\\}} P(H=h)\\left(-\\sum_{c \\in \\{1,2,3\\}} P(C=c \\mid H=h)\\log_{2} P(C=c \\mid H=h)\\right),\n$$\nwith the convention $0\\log_{2} 0=0$ by continuity. For each $h$, the posterior over $C$ is $\\left(\\frac{1}{3},\\frac{2}{3},0\\right)$ up to permutation, hence\n$$\nH(C \\mid H)=\\frac{1}{2}\\left(-\\frac{1}{3}\\log_{2}\\frac{1}{3}-\\frac{2}{3}\\log_{2}\\frac{2}{3}\\right)+\\frac{1}{2}\\left(-\\frac{1}{3}\\log_{2}\\frac{1}{3}-\\frac{2}{3}\\log_{2}\\frac{2}{3}\\right)\n=-\\frac{1}{3}\\log_{2}\\frac{1}{3}-\\frac{2}{3}\\log_{2}\\frac{2}{3}.\n$$\nSimplifying using $\\log_{2}\\left(\\frac{1}{3}\\right)=-\\log_{2} 3$ and $\\log_{2}\\left(\\frac{2}{3}\\right)=1-\\log_{2} 3$,\n$$\n-\\frac{1}{3}\\left(-\\log_{2} 3\\right)-\\frac{2}{3}\\left(1-\\log_{2} 3\\right)=\\frac{1}{3}\\log_2 3 - \\frac{2}{3} + \\frac{2}{3}\\log_2 3 = \\log_{2} 3-\\frac{2}{3}.\n$$\nTherefore, the conditional entropy in bits is $\\log_{2}(3)-\\frac{2}{3}$.", "answer": "$$\\boxed{\\log_{2}(3)-\\frac{2}{3}}$$", "id": "1612388"}]}