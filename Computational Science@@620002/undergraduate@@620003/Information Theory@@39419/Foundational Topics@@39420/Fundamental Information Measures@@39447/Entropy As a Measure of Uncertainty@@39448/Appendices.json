{"hands_on_practices": [{"introduction": "In many introductory examples, we calculate entropy from a given set of probabilities. This first exercise takes a step back to ask where those probabilities might come from, bridging the gap between statistical physics and information theory. You will see how the probability distribution of a data source, and thus its entropy, can be derived from its underlying physical properties, in this case, a set of energy levels [@problem_id:1620545]. This practice grounds the abstract concept of entropy in a tangible physical model.", "problem": "A simplified model for a data source, such as a sensor monitoring a physical system, generates symbols from a set of four distinct types, labeled $\\{S_0, S_1, S_2, S_3\\}$. The probability $P(S_i)$ of generating symbol $S_i$ is governed by a statistical mechanical principle, where each symbol type corresponds to a discrete energy level $E_i$. The probability distribution is given by:\n$$ P(S_i) = \\frac{1}{Z} \\exp(-\\alpha E_i) $$\nwhere $\\alpha$ is a positive constant related to the system's properties and $Z$ is the partition function, defined as $Z = \\sum_{j=0}^{3} \\exp(-\\alpha E_j)$.\nThe energy levels for the four symbol types have been determined to be:\n- $E_0 = 0$\n- $E_1 = \\frac{\\ln(2)}{\\alpha}$\n- $E_2 = \\frac{2\\ln(2)}{\\alpha}$\n- $E_3 = \\frac{2\\ln(2)}{\\alpha}$\n\nCalculate the Shannon entropy $H$ of this data source. The entropy measures the average uncertainty or information content per symbol. Express your answer in units of bits. For the entropy calculation, the base of the logarithm must be 2.", "solution": "We are given a Boltzmann-type distribution over four symbols with energies $E_{i}$ and probabilities $P(S_{i}) = Z^{-1}\\exp(-\\alpha E_{i})$, where the partition function is $Z = \\sum_{j=0}^{3} \\exp(-\\alpha E_{j})$. With $E_{0} = 0$, $E_{1} = \\frac{\\ln(2)}{\\alpha}$, $E_{2} = \\frac{2\\ln(2)}{\\alpha}$, and $E_{3} = \\frac{2\\ln(2)}{\\alpha}$, compute the unnormalized weights $w_{i} = \\exp(-\\alpha E_{i})$:\n$$\nw_{0} = \\exp(0) = 1,\\quad\nw_{1} = \\exp\\!\\left(-\\alpha\\cdot\\frac{\\ln(2)}{\\alpha}\\right) = \\exp(-\\ln 2) = 2^{-1},\n$$\n$$\nw_{2} = \\exp\\!\\left(-\\alpha\\cdot\\frac{2\\ln(2)}{\\alpha}\\right) = \\exp(-2\\ln 2) = 2^{-2},\\quad\nw_{3} = 2^{-2}.\n$$\nThe partition function is\n$$\nZ = w_{0} + w_{1} + w_{2} + w_{3} = 1 + 2^{-1} + 2^{-2} + 2^{-2} = 1 + 2^{-1} + 2\\cdot 2^{-2} = 1 + 2^{-1} + 2^{-1} = 2.\n$$\nHence the probabilities are\n$$\nP(S_{0}) = \\frac{1}{2},\\quad P(S_{1}) = \\frac{2^{-1}}{2} = \\frac{1}{4},\\quad P(S_{2}) = \\frac{2^{-2}}{2} = \\frac{1}{8},\\quad P(S_{3}) = \\frac{1}{8}.\n$$\nThe Shannon entropy in bits is\n$$\nH = -\\sum_{i=0}^{3} P(S_{i})\\,\\log_{2} P(S_{i}) = -\\left[\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) + \\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right) + \\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right) + \\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)\\right].\n$$\nUsing $\\log_{2}(1/2) = -1$, $\\log_{2}(1/4) = -2$, and $\\log_{2}(1/8) = -3$, we obtain\n$$\nH = \\frac{1}{2}\\cdot 1 + \\frac{1}{4}\\cdot 2 + 2\\cdot\\frac{1}{8}\\cdot 3 = \\frac{1}{2} + \\frac{1}{2} + \\frac{3}{4} = \\frac{7}{4}.\n$$\nThus the entropy is $\\frac{7}{4}$ bits.", "answer": "$$\\boxed{\\frac{7}{4}}$$", "id": "1620545"}, {"introduction": "Entropy isn't just a number to compute; its value tells a story about a system's level of uncertainty. This practice [@problem_id:1620519] is designed to build your intuition for what a change in entropy really means. By exploring a common scenario where a system is simplified—losing the ability to distinguish between certain outcomes—you will see firsthand how a loss of information corresponds to a decrease in entropy, making the system more predictable.", "problem": "A simplified diagnostic system for a wireless communication network monitors transmission packets and classifies them into one of four mutually exclusive states: {S1, S2, S3, S4}. Initially, under normal operating conditions, all four states are observed to occur with equal probability.\n\nA system upgrade is performed to simplify the monitoring process. After the upgrade, the diagnostic tool no longer distinguishes between states S3 and S4. Instead, any occurrence of either S3 or S4 is reported as a single, combined state, which we will call S-prime (S'). The probabilities of states S1 and S2 remain unchanged, and the probability of the new state S' is the sum of the original probabilities of S3 and S4.\n\nThe uncertainty of the system's output is quantified by the Shannon entropy, $H$, defined as $H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2(p(x_i))$, where $p(x_i)$ is the probability of the $i$-th outcome.\n\nCalculate the entropy of the probability distribution for the states reported by the upgraded system. Express your answer in bits, rounded to four significant figures.", "solution": "Let the set of initial states be $X = \\{S1, S2, S3, S4\\}$. The problem states that these four states occur with equal probability. Since the sum of probabilities must be 1, the probability of each state is:\n$P(S1) = P(S2) = P(S3) = P(S4) = \\frac{1}{4}$.\n\nAfter the system upgrade, the states S3 and S4 are merged into a new state S'. The new set of observable states is $X' = \\{S1, S2, S'\\}$. We need to determine the probability distribution for this new set of states.\n\nThe probabilities of S1 and S2 are unchanged:\n$P_{new}(S1) = P(S1) = \\frac{1}{4}$\n$P_{new}(S2) = P(S2) = \\frac{1}{4}$\n\nThe probability of the new state S' is the sum of the probabilities of the original states that were merged to form it:\n$P_{new}(S') = P(S3) + P(S4) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\n\nWe can check that the new probabilities sum to 1:\n$P_{new}(S1) + P_{new}(S2) + P_{new}(S') = \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{2} = \\frac{2}{4} + \\frac{1}{2} = \\frac{1}{2} + \\frac{1}{2} = 1$.\nSo, the new probability distribution is $p_{new} = \\{\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}\\}$.\n\nNow, we calculate the entropy of this new distribution using the Shannon entropy formula, $H = -\\sum_{i} p_i \\log_2(p_i)$.\nThe sum will have three terms, one for each state in $X'$.\n\n$$H_{new} = - \\left[ P_{new}(S1) \\log_2(P_{new}(S1)) + P_{new}(S2) \\log_2(P_{new}(S2)) + P_{new}(S') \\log_2(P_{new}(S')) \\right]$$\n\nSubstitute the probability values into the formula:\n$$H_{new} = - \\left[ \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) \\right]$$\n\nTo simplify, we evaluate the logarithm terms:\n$\\log_2\\left(\\frac{1}{4}\\right) = \\log_2(2^{-2}) = -2$\n$\\log_2\\left(\\frac{1}{2}\\right) = \\log_2(2^{-1}) = -1$\n\nNow, substitute these values back into the entropy equation:\n$$H_{new} = - \\left[ \\frac{1}{4}(-2) + \\frac{1}{4}(-2) + \\frac{1}{2}(-1) \\right]$$\n$$H_{new} = - \\left[ -\\frac{2}{4} - \\frac{2}{4} - \\frac{1}{2} \\right]$$\n$$H_{new} = - \\left[ -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right]$$\n$$H_{new} = - \\left[ -\\frac{3}{2} \\right]$$\n$$H_{new} = \\frac{3}{2} = 1.5$$\n\nThe problem asks for the answer in bits, rounded to four significant figures.\nThe calculated value is exactly 1.5. To express this with four significant figures, we write it as 1.500.", "answer": "$$\\boxed{1.500}$$", "id": "1620519"}, {"introduction": "In the real world, we rarely possess either absolute ignorance or complete knowledge about a system; we typically operate with partial information. This final exercise [@problem_id:1620533] places you in a practical scenario from cryptography where an information leak reveals some properties of a secret key without exposing the key itself. Your task is to quantify the *remaining* uncertainty, providing a powerful introduction to the concept of conditional entropy and how it measures what we still don't know.", "problem": "A fault-tolerant memory system is designed to store an 8-bit cryptographic key, denoted by the bit vector $K = (K_1, K_2, \\dots, K_8)$. Initially, the key is chosen uniformly at random from all possible $2^8$ bit strings. The memory system has a built-in error-checking mechanism that, due to a design flaw, inadvertently leaks some information to a potential attacker. This attacker does not learn any of the key bits directly, but is able to determine the parity of the first four bits and the parity of the last four bits. Specifically, the attacker learns the values of two quantities: $P_1 = K_1 \\oplus K_2 \\oplus K_3 \\oplus K_4$ and $P_2 = K_5 \\oplus K_6 \\oplus K_7 \\oplus K_8$, where $\\oplus$ denotes the exclusive OR (XOR) operation.\n\nGiven this information leakage, calculate the average remaining entropy of the secret key $K$. Express your answer in bits. The entropy should be calculated using the base-2 logarithm.", "solution": "Let $K=(K_{1},\\dots,K_{8})$ be uniformly distributed over $\\{0,1\\}^{8}$, and let $P_{1}=K_{1}\\oplus K_{2}\\oplus K_{3}\\oplus K_{4}$ and $P_{2}=K_{5}\\oplus K_{6}\\oplus K_{7}\\oplus K_{8}$. The attacker learns $(P_{1},P_{2})$, so the average remaining uncertainty about $K$ is the conditional entropy $H(K\\,|\\,P_{1},P_{2})$ with base-$2$ logarithms.\n\nUse the identity for deterministic leakage: since $(P_{1},P_{2})$ is a deterministic function of $K$, we have $H(P_{1},P_{2}\\,|\\,K)=0$, hence the mutual information satisfies $I(K;(P_{1},P_{2}))=H(P_{1},P_{2})$. Therefore,\n$$\nH(K\\,|\\,P_{1},P_{2}) \\;=\\; H(K)-I(K;(P_{1},P_{2})) \\;=\\; H(K)-H(P_{1},P_{2}).\n$$\nBecause $K$ is uniform over $2^{8}$ values, \n$$\nH(K)=\\log_{2}(2^{8})=8.\n$$\nNext, $P_{1}$ depends only on $(K_{1},\\dots,K_{4})$ and $P_{2}$ only on $(K_{5},\\dots,K_{8})$. Under the uniform distribution, these halves are independent and each parity is $\\mathrm{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$. Thus\n$$\nH(P_{1})=-\\sum_{p\\in\\{0,1\\}}\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1,\\qquad H(P_{2})=1,\n$$\nand independence gives\n$$\nH(P_{1},P_{2})=H(P_{1})+H(P_{2})=2.\n$$\nTherefore,\n$$\nH(K\\,|\\,P_{1},P_{2})=8-2=6.\n$$\n\nAs a counting cross-check: fixing $(P_{1},P_{2})$ imposes one linear constraint on each group of four bits, leaving $4-1=3$ degrees of freedom per group, hence $2^{3}\\cdot 2^{3}=2^{6}$ possible keys are consistent with any given $(P_{1},P_{2})$. A uniform posterior over $2^{6}$ possibilities has entropy $\\log_{2}(2^{6})=6$, agreeing with the result above.", "answer": "$$\\boxed{6}$$", "id": "1620533"}]}