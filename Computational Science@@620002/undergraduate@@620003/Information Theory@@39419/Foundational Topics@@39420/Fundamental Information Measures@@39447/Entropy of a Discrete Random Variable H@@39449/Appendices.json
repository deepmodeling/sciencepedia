{"hands_on_practices": [{"introduction": "The first step in mastering any new concept is to apply it directly. This exercise provides a straightforward scenario—a simplified model of stock price movement—allowing you to practice the fundamental calculation of Shannon entropy. By quantifying the uncertainty in this model, you'll gain a concrete understanding of how the entropy formula, $H(X) = -\\sum_{i} P(x_i) \\log_2(P(x_i))$, works in a practical context. [@problem_id:1620711]", "problem": "A financial analyst is developing a simple probabilistic model to describe the daily price movement of a particular volatile stock. The model considers three possible outcomes for the stock's closing price compared to its opening price: 'RISE', 'FALL', or 'STATIC'. Let $X$ be a discrete random variable representing this daily outcome. Based on an analysis of historical data, the analyst assigns the following probabilities to these outcomes:\n- The probability of a 'RISE' is $P(X = \\text{'RISE'}) = 0.4$.\n- The probability of a 'FALL' is $P(X = \\text{'FALL'}) = 0.4$.\n- The probability of the price remaining 'STATIC' is $P(X = \\text{'STATIC'}) = 0.2$.\n\nCalculate the Shannon entropy, $H(X)$, which quantifies the average uncertainty or \"information content\" of this daily outcome model. Express your answer in bits, rounded to three significant figures.", "solution": "Shannon entropy in bits for a discrete random variable with outcomes having probabilities $\\{p_{i}\\}$ is defined as\n$$H(X)=-\\sum_{i}p_{i}\\log_{2}(p_{i}).$$\nWith $p_{\\text{RISE}}=0.4$, $p_{\\text{FALL}}=0.4$, and $p_{\\text{STATIC}}=0.2$, we substitute to obtain\n$$H(X)=-\\left[0.4\\log_{2}(0.4)+0.4\\log_{2}(0.4)+0.2\\log_{2}(0.2)\\right] = -\\left[0.8\\log_{2}(0.4)+0.2\\log_{2}(0.2)\\right].$$\nUsing $\\log_{2}(0.4)=\\log_{2}\\!\\left(\\frac{2}{5}\\right)=1-\\log_{2}(5)\\approx -1.32192809489$ and $\\log_{2}(0.2)=\\log_{2}\\!\\left(\\frac{1}{5}\\right)=-\\log_{2}(5)\\approx -2.32192809489$, we get\n$$H(X)\\approx -\\left[0.8(-1.32192809489)+0.2(-2.32192809489)\\right]=1.52192809489.$$\nRounding to three significant figures gives $H(X)\\approx 1.52$ bits.", "answer": "$$\\boxed{1.52}$$", "id": "1620711"}, {"introduction": "Simply calculating an entropy value is only half the story; we also need to interpret what that value means. This problem introduces the crucial concept of maximum entropy, which serves as a benchmark for the highest possible uncertainty for a given number of outcomes. By calculating the \"entropy deficit,\" you will learn to assess how much information or predictability is gained when a system's outcomes are not uniformly distributed. [@problem_id:1620765]", "problem": "A simple weather prediction model for a specific location has four possible outputs: 'Sunny', 'Cloudy', 'Rainy', or 'Windy'. Over a long period of observation, it is determined that the model predicts 'Sunny' with probability $1/2$, 'Cloudy' with probability $1/4$, and both 'Rainy' and 'Windy' with a probability of $1/8$ each.\n\nThe entropy of a probability distribution is a measure of its average uncertainty. An ideal model with the same number of possible outcomes but maximum uncertainty would have a uniform probability distribution. Calculate the \"entropy deficit,\" which is the difference between this maximum possible entropy and the actual entropy of the given weather model's output distribution.\n\nUse the base-2 logarithm for all entropy calculations. Express your final answer in bits as a decimal number.", "solution": "Let the four outcomes be Sunny, Cloudy, Rainy, Windy with probabilities $p_{S}=\\frac{1}{2}$, $p_{C}=\\frac{1}{4}$, $p_{R}=\\frac{1}{8}$, and $p_{W}=\\frac{1}{8}$. The entropy (in bits) of a discrete distribution $p$ is\n$$\nH(p)=-\\sum_{i} p_{i}\\log_{2}(p_{i}).\n$$\nCompute the individual logarithms:\n$$\n\\log_{2}\\!\\left(\\frac{1}{2}\\right)=-1,\\quad \\log_{2}\\!\\left(\\frac{1}{4}\\right)=-2,\\quad \\log_{2}\\!\\left(\\frac{1}{8}\\right)=-3.\n$$\nTherefore,\n$$\nH(p)=-\\left[\\frac{1}{2}(-1)+\\frac{1}{4}(-2)+\\frac{1}{8}(-3)+\\frac{1}{8}(-3)\\right]\n=-\\left[-\\frac{1}{2}-\\frac{1}{2}-\\frac{3}{8}-\\frac{3}{8}\\right]\n=\\frac{7}{4}.\n$$\nThe maximum possible entropy with four equally likely outcomes is\n$$\nH_{\\max}=\\log_{2}(4)=2.\n$$\nThe entropy deficit is the difference\n$$\nH_{\\max}-H(p)=2-\\frac{7}{4}=\\frac{1}{4}=0.25.\n$$", "answer": "$$\\boxed{0.25}$$", "id": "1620765"}, {"introduction": "We often want to design systems to be either as predictable or as unpredictable as possible. This advanced exercise puts you in the role of a nanoscale engineer aiming to maximize a device's informational capacity by maximizing its state entropy. Using calculus, you will determine the optimal operating conditions that lead to the most uncertainty, thereby deriving the fundamental principle that entropy is maximized for a uniform probability distribution. [@problem_id:1620732]", "problem": "A research team is developing a nanoscale switching device that can exist in three distinct conformational states: $S_1$, $S_2$, and $S_3$. The state of the device is probabilistic. Due to a built-in molecular symmetry, the probabilities of the device being in states $S_1$ and $S_2$ are identical. We denote this common probability by $p$, so $P(S_1) = P(S_2) = p$. Consequently, the probability of being in state $S_3$ is $P(S_3) = 1 - 2p$. The parameter $p$ can be tuned by an external control voltage and is constrained to the physical range $p \\in [0, 1/2]$. The team aims to operate the device under conditions that maximize its Shannon entropy, which quantifies the uncertainty or information content of the device's state. The Shannon entropy $H$ for a discrete random variable with outcomes $x_i$ and corresponding probabilities $P(x_i)$ is given by the formula $H = -\\sum_{i} P(x_i) \\log_2(P(x_i))$.\n\nDetermine the value of $p$ that maximizes the entropy $H$ of the device's state distribution. Provide the answer as a closed-form analytic expression.", "solution": "The device has three states with probabilities $P(S_{1})=p$, $P(S_{2})=p$, and $P(S_{3})=1-2p$, where $p\\in[0,\\frac{1}{2}]$. The Shannon entropy as a function of $p$ is\n$$\nH(p)=-\\left[2p\\,\\log_{2}(p)+(1-2p)\\,\\log_{2}(1-2p)\\right].\n$$\nTo maximize $H(p)$ on the interval, differentiate with respect to $p$. Use $\\log_{2}x=\\frac{\\ln x}{\\ln 2}$ and the product rule. First,\n$$\n\\frac{d}{dp}\\left(2p\\,\\log_{2}p\\right)=2\\left(\\log_{2}p+\\frac{1}{\\ln 2}\\right),\n$$\nand with $a(p)=1-2p$, $b(p)=\\log_{2}(1-2p)$,\n$$\n\\frac{d}{dp}\\left[a(p)b(p)\\right]=a'(p)b(p)+a(p)b'(p)=-2\\log_{2}(1-2p)-\\frac{2}{\\ln 2}.\n$$\nTherefore,\n$$\nH'(p)=-\\left[2\\left(\\log_{2}p+\\frac{1}{\\ln 2}\\right)+\\left(-2\\log_{2}(1-2p)-\\frac{2}{\\ln 2}\\right)\\right]\n=-2\\left[\\log_{2}p-\\log_{2}(1-2p)\\right]\n=-2\\log_{2}\\!\\left(\\frac{p}{1-2p}\\right).\n$$\nSet $H'(p)=0$ to find critical points:\n$$\n\\log_{2}\\!\\left(\\frac{p}{1-2p}\\right)=0\\quad\\Longrightarrow\\quad \\frac{p}{1-2p}=1\\quad\\Longrightarrow\\quad p=1-2p\\quad\\Longrightarrow\\quad p=\\frac{1}{3}.\n$$\nTo verify this gives a maximum, compute the second derivative. Using $u(p)=\\frac{p}{1-2p}$,\n$$\nH''(p)=-2\\,\\frac{d}{dp}\\left[\\log_{2}u(p)\\right]\n=-\\frac{2}{\\ln 2}\\cdot\\frac{u'(p)}{u(p)}\n=-\\frac{2}{\\ln 2}\\cdot\\frac{1}{p(1-2p)}<0\\quad\\text{for }p\\in(0,\\tfrac{1}{2}),\n$$\nso $H$ is strictly concave on the domain and the critical point is the unique global maximizer. Since $\\frac{1}{3}\\in[0,\\frac{1}{2}]$, the entropy is maximized at $p=\\frac{1}{3}$.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1620732"}]}