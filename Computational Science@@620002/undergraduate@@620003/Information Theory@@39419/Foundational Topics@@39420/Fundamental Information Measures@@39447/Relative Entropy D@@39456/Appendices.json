{"hands_on_practices": [{"introduction": "The first step in mastering relative entropy is to get comfortable with the fundamental computation. This exercise provides a foundational workout by asking you to calculate the Kullback-Leibler (KL) divergence between two binomial distributions, which are essential models for sequences of independent trials. By working through this problem ([@problem_id:1654970]), you will see how the divergence between distributions of sequences relates back to the divergence for a single event, building a core skill for comparing statistical models.", "problem": "In the field of statistical modeling, it is often necessary to quantify the difference between two probability distributions. One common measure is the Kullback-Leibler (KL) divergence, also known as relative entropy. For two discrete probability distributions $P(x)$ and $Q(x)$ defined over the same sample space $\\mathcal{X}$, the KL divergence of $Q$ from $P$ is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\nwhere $\\ln$ denotes the natural logarithm.\n\nConsider two independent manufacturing processes creating a specific type of semiconductor.\n- Process 1 is modeled by a random variable $K_1$ that follows a binomial distribution with parameters $(n, p_1)$. This represents the number of non-defective semiconductors in a batch of size $n$, where $p_1$ is the probability of any single semiconductor being non-defective.\n- Process 2 is modeled by a random variable $K_2$ that also follows a binomial distribution, but with parameters $(n, p_2)$. Here, $p_2$ is the probability of a single semiconductor being non-defective.\n\nThe probability mass function (PMF) for a binomial distribution $B(n, p)$ is given by $P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$ for $k \\in \\{0, 1, \\dots, n\\}$.\n\nLet $P_1$ denote the binomial distribution for Process 1 and $P_2$ denote the distribution for Process 2. Assuming $p_1, p_2 \\in (0,1)$, find a closed-form analytic expression for the Kullback-Leibler divergence $D_{KL}(P_1 || P_2)$ in terms of $n$, $p_1$, and $p_2$.", "solution": "We denote the PMFs of $K_{1} \\sim \\mathrm{Bin}(n,p_{1})$ and $K_{2} \\sim \\mathrm{Bin}(n,p_{2})$ by\n$$\nP_{1}(k)=\\binom{n}{k}p_{1}^{k}(1-p_{1})^{n-k}, \\quad\nP_{2}(k)=\\binom{n}{k}p_{2}^{k}(1-p_{2})^{n-k},\n$$\nfor $k \\in \\{0,1,\\dots,n\\}$. By definition, the Kullback-Leibler divergence of $P_{2}$ from $P_{1}$ is\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})=\\sum_{k=0}^{n}P_{1}(k)\\,\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right).\n$$\nCompute the likelihood ratio inside the logarithm:\n$$\n\\frac{P_{1}(k)}{P_{2}(k)}=\\frac{\\binom{n}{k}p_{1}^{k}(1-p_{1})^{n-k}}{\\binom{n}{k}p_{2}^{k}(1-p_{2})^{n-k}}\n=\\left(\\frac{p_{1}}{p_{2}}\\right)^{k}\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)^{n-k}.\n$$\nTaking the logarithm yields\n$$\n\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right)\n= k\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right) + (n-k)\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right).\n$$\nSubstitute back into the definition of $D_{KL}$:\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n=\\sum_{k=0}^{n}P_{1}(k)\\left[ k\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right) + (n-k)\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\right].\n$$\nSince the logarithmic factors do not depend on $k$, we can factor them out and recognize the sums as expectations under $K \\sim \\mathrm{Bin}(n,p_{1})$:\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n=\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)\\sum_{k=0}^{n}k\\,P_{1}(k)\n+\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\sum_{k=0}^{n}(n-k)\\,P_{1}(k).\n$$\nUsing $\\sum_{k=0}^{n}k\\,P_{1}(k)=\\mathbb{E}_{P_{1}}[K]=n p_{1}$ and $\\sum_{k=0}^{n}(n-k)\\,P_{1}(k)=n-\\mathbb{E}_{P_{1}}[K]=n(1-p_{1})$, we obtain\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n= n p_{1}\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)\n+ n(1-p_{1})\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right).\n$$\nEquivalently, this is $n$ times the KL divergence between $\\mathrm{Bernoulli}(p_{1})$ and $\\mathrm{Bernoulli}(p_{2})$, and it is finite for $p_{1},p_{2}\\in(0,1)$.", "answer": "$$\\boxed{n\\left[p_{1}\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)+(1-p_{1})\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\right]}$$", "id": "1654970"}, {"introduction": "While the definition of KL divergence is straightforward, its properties for multi-variable systems can be subtle and even counter-intuitive. This thought experiment ([@problem_id:1655003]) challenges the simple assumption that the divergence of a joint distribution is the sum of its marginals. By analyzing a carefully constructed scenario, you will apply the chain rule for relative entropy, a powerful identity that clarifies how dependencies between variables impact the overall divergence.", "problem": "Consider two binary random variables, $X$ and $Y$, each taking values in the set $\\{0, 1\\}$. We define two distinct joint probability distributions over these variables, denoted by $p(x,y)$ and $q(x,y)$.\n\nThese distributions are constructed from the following components:\n\nFirst, the marginal distributions for the random variable $X$ are given as:\n- For the first model, the distribution is $p(x)$, with $p(X=0) = \\frac{1}{2}$.\n- For the second model, the distribution is $q(x)$, with $q(X=0) = \\frac{1}{4}$.\n\nSecond, the conditional probability distribution of $Y$ given $X$ is identical for both models. Let's denote this common conditional distribution by $f(y|x)$, so that $p(y|x) = f(y|x)$ and $q(y|x) = f(y|x)$. The specific conditional probabilities are:\n- $f(Y=0 | X=0) = \\frac{3}{4}$\n- $f(Y=0 | X=1) = \\frac{1}{4}$\n\nThe full joint distributions are then formed by the product rule of probability: $p(x,y) = p(x)f(y|x)$ and $q(x,y) = q(x)f(y|x)$.\n\nThe Kullback-Leibler (KL) divergence, also known as relative entropy, between two discrete probability distributions $u(z)$ and $v(z)$ defined over the same alphabet is given by $D(u||v) = \\sum_z u(z) \\ln\\frac{u(z)}{v(z)}$, where the sum is over all possible outcomes $z$ and $\\ln$ denotes the natural logarithm.\n\nYour task is to compute the value of the quantity $\\Delta$, defined as:\n$$ \\Delta = D(p(x,y)||q(x,y)) - D(p(x)||q(x)) - D(p(y)||q(y)) $$\nExpress your answer as a single closed-form analytic expression.", "solution": "The problem asks for the computation of the quantity $\\Delta = D(p(x,y)||q(x,y)) - D(p(x)||q(x)) - D(p(y)||q(y))$.\n\nThe most direct way to evaluate this is to use the chain rule for Kullback-Leibler (KL) divergence, which states:\n$$ D(p(x,y)||q(x,y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x)) $$\nHere, $D(p(y|x)||q(y|x))$ is the conditional relative entropy, defined as $\\sum_{x} p(x) D(p(y|X=x)||q(y|X=x))$.\n\nSubstituting the chain rule into the expression for $\\Delta$:\n$$ \\Delta = \\left( D(p(x)||q(x)) + D(p(y|x)||q(y|x)) \\right) - D(p(x)||q(x)) - D(p(y)||q(y)) $$\nThe term $D(p(x)||q(x))$ cancels out, simplifying the expression to:\n$$ \\Delta = D(p(y|x)||q(y|x)) - D(p(y)||q(y)) $$\n\nNow, we evaluate each of the two remaining terms.\n\nFirst, let's compute $D(p(y|x)||q(y|x))$. The definition is:\n$$ D(p(y|x)||q(y|x)) = \\sum_{x \\in \\{0,1\\}} p(x) \\sum_{y \\in \\{0,1\\}} p(y|x) \\ln\\frac{p(y|x)}{q(y|x)} $$\nAccording to the problem statement, the conditional distributions are identical, i.e., $p(y|x) = f(y|x)$ and $q(y|x) = f(y|x)$. Therefore, the ratio inside the logarithm is $\\frac{p(y|x)}{q(y|x)} = 1$ for all $x, y$ (where the distributions are defined). The natural logarithm of 1 is 0.\nThus, every term in the summation is zero, which means:\n$$ D(p(y|x)||q(y|x)) = 0 $$\nThe expression for $\\Delta$ simplifies further to:\n$$ \\Delta = - D(p(y)||q(y)) $$\n\nNext, we need to compute $D(p(y)||q(y))$. To do this, we must first find the marginal distributions $p(y)$ and $q(y)$ using the law of total probability.\n\nThe marginal distribution $p(y)$ is:\n$$ p(y) = \\sum_{x \\in \\{0,1\\}} p(x) p(y|x) = \\sum_{x \\in \\{0,1\\}} p(x) f(y|x) $$\nWe are given $p(X=0) = 1/2$, so $p(X=1) = 1-1/2 = 1/2$. We are also given $f(Y=0|X=0) = 3/4$ and $f(Y=0|X=1) = 1/4$. This implies $f(Y=1|X=0) = 1-3/4 = 1/4$ and $f(Y=1|X=1) = 1-1/4 = 3/4$.\nFor $y=0$:\n$p(Y=0) = p(X=0)f(Y=0|X=0) + p(X=1)f(Y=0|X=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{3}{4}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{4}\\right) = \\frac{3}{8} + \\frac{1}{8} = \\frac{4}{8} = \\frac{1}{2}$.\nSince $Y$ is a binary variable, $p(Y=1) = 1 - p(Y=0) = 1 - 1/2 = 1/2$.\n\nThe marginal distribution $q(y)$ is:\n$$ q(y) = \\sum_{x \\in \\{0,1\\}} q(x) q(y|x) = \\sum_{x \\in \\{0,1\\}} q(x) f(y|x) $$\nWe are given $q(X=0) = 1/4$, so $q(X=1) = 1-1/4 = 3/4$.\nFor $y=0$:\n$q(Y=0) = q(X=0)f(Y=0|X=0) + q(X=1)f(Y=0|X=1) = \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{4}\\right) + \\left(\\frac{3}{4}\\right)\\left(\\frac{1}{4}\\right) = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$.\nThis implies $q(Y=1) = 1 - q(Y=0) = 1 - 3/8 = 5/8$.\n\nNow we can compute $D(p(y)||q(y))$:\n$$ D(p(y)||q(y)) = \\sum_{y \\in \\{0,1\\}} p(y) \\ln\\frac{p(y)}{q(y)} $$\n$$ D(p(y)||q(y)) = p(Y=0) \\ln\\frac{p(Y=0)}{q(Y=0)} + p(Y=1) \\ln\\frac{p(Y=1)}{q(Y=1)} $$\n$$ D(p(y)||q(y)) = \\left(\\frac{1}{2}\\right) \\ln\\frac{1/2}{3/8} + \\left(\\frac{1}{2}\\right) \\ln\\frac{1/2}{5/8} $$\n$$ D(p(y)||q(y)) = \\frac{1}{2} \\ln\\left(\\frac{8}{6}\\right) + \\frac{1}{2} \\ln\\left(\\frac{8}{10}\\right) = \\frac{1}{2} \\ln\\left(\\frac{4}{3}\\right) + \\frac{1}{2} \\ln\\left(\\frac{4}{5}\\right) $$\nUsing the property $\\ln(a) + \\ln(b) = \\ln(ab)$:\n$$ D(p(y)||q(y)) = \\frac{1}{2} \\ln\\left(\\frac{4}{3} \\cdot \\frac{4}{5}\\right) = \\frac{1}{2} \\ln\\left(\\frac{16}{15}\\right) $$\n\nFinally, we find $\\Delta$:\n$$ \\Delta = - D(p(y)||q(y)) = - \\frac{1}{2} \\ln\\left(\\frac{16}{15}\\right) $$\nUsing the property $-\\ln(a) = \\ln(1/a)$:\n$$ \\Delta = \\frac{1}{2} \\ln\\left(\\frac{15}{16}\\right) $$\nThis negative result demonstrates that the quantity $D(p(x,y)||q(x,y))$ is not always greater than or equal to the sum of the KL divergences of its marginals, $D(p(x)||q(x)) + D(p(y)||q(y))$.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\left(\\frac{15}{16}\\right)}$$", "id": "1655003"}, {"introduction": "Relative entropy is more than just a metric; it's a cornerstone of modern statistical modeling and optimization. This practice problem ([@problem_id:1655009]) showcases one of its most powerful applications: finding a new probability distribution that incorporates new information while staying as close as possible to a prior belief. This method, known as the principle of minimum discrimination information, is a fundamental technique for updating models in fields ranging from machine learning to statistical physics.", "problem": "A cloud computing platform allocates its resources among four distinct classes of computational tasks, labeled 1, 2, 3, and 4. A historical analysis has established a baseline resource allocation strategy represented by a prior probability distribution $Q = (q_1, q_2, q_3, q_4)$, where $q_i$ is the fraction of resources allocated to task class $i$. The prior distribution is given by $Q = (0.1, 0.2, 0.3, 0.4)$.\n\nEach task class has an associated average power consumption, given by a function $f(i)$. The values are $f(1) = 1.0$, $f(2) = 2.0$, $f(3) = 4.0$, and $f(4) = 5.0$, in normalized power units.\n\nTo meet new energy efficiency targets, the platform needs to adopt a new allocation strategy $P = (p_1, p_2, p_3, p_4)$ that adjusts the average power consumption to a new target value of $C=3.9$ power units. To ensure a smooth transition and minimize disruption, the new strategy $P$ must be as close as possible to the prior strategy $Q$. The \"closeness\" is measured by the Kullback-Leibler (KL) divergence, also known as relative entropy, defined as $D_{KL}(P||Q) = \\sum_{i=1}^4 p_i \\ln\\left(\\frac{p_i}{q_i}\\right)$.\n\nYour task is to find the new allocation strategy $P$ that minimizes $D_{KL}(P||Q)$ subject to the constraints that it is a valid probability distribution and that the expected power consumption is equal to the target value $C$.\n\nCalculate the probability $p_4$ for the fourth task class under this new optimal allocation strategy. Round your final answer to three significant figures.", "solution": "We minimize the relative entropy $D_{KL}(P||Q) = \\sum_{i=1}^{4} p_{i} \\ln\\left(\\frac{p_{i}}{q_{i}}\\right)$ subject to $\\sum_{i=1}^{4} p_{i} = 1$ and $\\sum_{i=1}^{4} p_{i} f(i) = C$ with $C=3.9$. Introduce Lagrange multipliers $\\alpha$ and $\\beta$ and form the Lagrangian\n$$\n\\mathcal{L} = \\sum_{i=1}^{4} p_{i} \\ln\\left(\\frac{p_{i}}{q_{i}}\\right) + \\alpha \\left(\\sum_{i=1}^{4} p_{i} - 1\\right) + \\beta \\left(\\sum_{i=1}^{4} p_{i} f(i) - C\\right).\n$$\nStationarity with respect to $p_{i}$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{i}} = \\ln\\left(\\frac{p_{i}}{q_{i}}\\right) + 1 + \\alpha + \\beta f(i) = 0,\n$$\nso\n$$\np_{i} = q_{i} \\exp\\left(-1 - \\alpha - \\beta f(i)\\right).\n$$\nLet $\\eta = -\\beta$ and define the normalizer\n$$\nZ(\\eta) = \\sum_{j=1}^{4} q_{j} \\exp\\left(\\eta f(j)\\right).\n$$\nThen\n$$\np_{i} = \\frac{q_{i} \\exp\\left(\\eta f(i)\\right)}{Z(\\eta)}, \\quad Z(\\eta) = \\sum_{j=1}^{4} q_{j} \\exp\\left(\\eta f(j)\\right).\n$$\nThe expectation constraint becomes\n$$\n\\sum_{i=1}^{4} p_{i} f(i) = \\frac{1}{Z(\\eta)} \\sum_{i=1}^{4} q_{i} f(i) \\exp\\left(\\eta f(i)\\right) = C.\n$$\nWith $Q=(0.1,0.2,0.3,0.4)$ and $f(1)=1$, $f(2)=2$, $f(3)=4$, $f(4)=5$, we have\n$$\nZ(\\eta) = 0.1 \\exp(\\eta) + 0.2 \\exp(2\\eta) + 0.3 \\exp(4\\eta) + 0.4 \\exp(5\\eta),\n$$\n$$\n\\sum_{i=1}^{4} q_{i} f(i) \\exp\\left(\\eta f(i)\\right) = 0.1 \\cdot 1 \\cdot \\exp(\\eta) + 0.2 \\cdot 2 \\cdot \\exp(2\\eta) + 0.3 \\cdot 4 \\cdot \\exp(4\\eta) + 0.4 \\cdot 5 \\cdot \\exp(5\\eta).\n$$\nSet this ratio equal to $C=3.9$:\n$$\n\\frac{0.1 \\exp(\\eta) + 0.4 \\exp(2\\eta) + 1.2 \\exp(4\\eta) + 2.0 \\exp(5\\eta)}{0.1 \\exp(\\eta) + 0.2 \\exp(2\\eta) + 0.3 \\exp(4\\eta) + 0.4 \\exp(5\\eta)} = 3.9.\n$$\nLet $t = \\exp(\\eta) > 0$. Then the equation becomes\n$$\n\\frac{0.1 t + 0.4 t^{2} + 1.2 t^{4} + 2.0 t^{5}}{0.1 t + 0.2 t^{2} + 0.3 t^{4} + 0.4 t^{5}} = 3.9.\n$$\nCross-multiplying and simplifying yields\n$$\n0.44 t^5 + 0.03 t^4 - 0.38 t^2 - 0.29 t = 0,\n$$\nequivalently, for $t>0$,\n$$\n0.44\\, t^{4} + 0.03\\, t^{3} - 0.38\\, t - 0.29 = 0.\n$$\nSolving numerically for $t>1$ (since the target $C=3.9$ exceeds the prior mean $3.7$) gives $t \\approx 1.11132$.\n\nWith this $t$, compute the normalizer and $p_{4}$. First compute powers:\n$$\nt \\approx 1.11132,\\quad t^{2} \\approx 1.2350321424,\\quad t^{4} \\approx 1.5253043928,\\quad t^{5} \\approx 1.6951012778.\n$$\nThen\n$$\nZ = 0.1 t + 0.2 t^{2} + 0.3 t^{4} + 0.4 t^{5} \\approx 0.111132 + 0.24700642848 + 0.45759131784 + 0.67804051112 \\approx 1.49377025741.\n$$\nTherefore\n$$\np_{4} = \\frac{0.4 t^{5}}{Z} \\approx \\frac{0.67804051112}{1.49377025741} \\approx 0.453912,\n$$\nwhich, rounded to three significant figures, is $0.454$.", "answer": "$$\\boxed{0.454}$$", "id": "1655009"}]}