{"hands_on_practices": [{"introduction": "The Data Processing Inequality is a cornerstone of information theory, stating that processing data cannot create new information. This first exercise [@problem_id:1613405] grounds this abstract principle in a concrete scenario involving a digital signal passing through two cascaded noisy channels. By analyzing this system, you'll formalize the intuition that each step of processing can only degrade the signal's quality and reduce the information it carries about the original source.", "problem": "A binary signal $X$ is drawn from a distribution where $P(X=0) = \\alpha$ and $P(X=1) = 1-\\alpha$, with $0 < \\alpha < 1$. The signal is sent through a cascade of two independent noisy channels. The first channel is a Binary Symmetric Channel (BSC), which flips the transmitted bit with a crossover probability of $p_1$, producing an intermediate signal $Y$. This signal $Y$ is then immediately fed as input into a second, independent BSC with a crossover probability of $p_2$, producing the final signal $Z$. Assume that the channels are neither perfect nor completely random, so their crossover probabilities are in the range $0 < p_1 < 1$ and $0 < p_2 < 1$.\n\nLet $I(X; Y)$ denote the mutual information between the source $X$ and the intermediate signal $Y$, and let $I(X; Z)$ denote the mutual information between the source $X$ and the final signal $Z$. All mutual information values are calculated in units of bits. Which of the following relationships is universally true for the described system under the given conditions?\n\nA. $I(X; Z) > I(X; Y)$\n\nB. $I(X; Z) = I(X; Y)$\n\nC. $I(X; Z) < I(X; Y)$\n\nD. $I(X; Z) \\le I(X; Y)$\n\nE. $I(X; Z) \\ge I(X; Y)$", "solution": "The problem describes a system where a signal $X$ is processed to create $Y$, which is then further processed to create $Z$. The key insight is to recognize the relationship between the three random variables $X$, $Y$, and $Z$.\n\nThe output of the first channel, $Y$, depends only on its input, $X$. The output of the second channel, $Z$, depends only on its input, $Y$. This means that if we know the intermediate signal $Y$, the final signal $Z$ is conditionally independent of the original signal $X$. This structure is known as a Markov chain, which is denoted as $X \\to Y \\to Z$. The joint probability distribution for such a chain factorizes as $p(x, y, z) = p(x) p(y|x) p(z|y)$.\n\nWe can analyze the relationship between $I(X;Y)$ and $I(X;Z)$ using the chain rule for mutual information. The mutual information between $X$ and the pair $(Y, Z)$ can be expanded in two ways:\n\n1.  $I(X; Y, Z) = I(X; Y) + I(X; Z|Y)$\n2.  $I(X; Y, Z) = I(X; Z) + I(X; Y|Z)$\n\nFrom the definition of the Markov chain $X \\to Y \\to Z$, we know that $X$ and $Z$ are conditionally independent given $Y$. This property implies that the conditional mutual information $I(X; Z|Y)$ is zero.\n$$I(X; Z|Y) = 0$$\n\nSubstituting this result into the first expansion gives:\n$$I(X; Y, Z) = I(X; Y) + 0 = I(X; Y)$$\n\nNow we can equate the two expressions for $I(X; Y, Z)$:\n$$I(X; Y) = I(X; Z) + I(X; Y|Z)$$\n\nA fundamental property of mutual information is that it is always non-negative. This applies to conditional mutual information as well. Therefore:\n$$I(X; Y|Z) \\ge 0$$\n\nThis term, $I(X; Y|Z)$, represents the remaining uncertainty about $Y$ that is resolved by knowing $X$, even after $Z$ is already known. Intuitively, it is the information \"lost\" in the second channel.\n\nCombining our equation with the non-negativity property, we get:\n$$I(X; Y) \\ge I(X; Z)$$\nor equivalently,\n$$I(X; Z) \\le I(X; Y)$$\n\nThis result is a famous theorem in information theory called the Data Processing Inequality. It states that post-processing of data (in this case, passing $Y$ through the second BSC to get $Z$) cannot increase the mutual information with respect to the original source $X$.\n\nLet's analyze the given options based on this inequality:\n- Option A ($I(X; Z) > I(X; Y)$) and Option E ($I(X; Z) \\ge I(X; Y)$) are incorrect as they contradict the Data Processing Inequality (unless equality holds).\n- Option B ($I(X; Z) = I(X; Y)$) is not always true. Equality holds if and only if $I(X; Y|Z) = 0$. This would mean that $Y$ can be perfectly determined from $Z$. For a BSC with crossover probability $p_2 \\in (0, 1)$, the mapping from $Y$ to $Z$ is not one-to-one, so in general $Y$ cannot be recovered from $Z$ without uncertainty. For example, if $p_1=0.1, p_2=0.1, \\alpha=0.5$, a direct calculation would show a strict inequality.\n- Option C ($I(X; Z) < I(X; Y)$) is also not always true. While it holds for most non-trivial cases, it is possible for equality to occur. For example, if the first channel is such that $I(X; Y) = 0$ (e.g., if $p_1=0.5$ and $\\alpha=0.5$), then since $Z$ is a further processed version of $Y$, we must also have $I(X; Z) = 0$. In this case, $I(X; Z) = I(X; Y) = 0$.\n- Option D ($I(X; Z) \\le I(X; Y)$) is the only relationship that is universally true for any choice of $\\alpha$, $p_1$, and $p_2$ within the specified ranges, as it correctly captures the Data Processing Inequality, including the possibility of equality in specific edge cases.", "answer": "$$\\boxed{D}$$", "id": "1613405"}, {"introduction": "Moving from discrete bits to continuous signals, this practice explores information loss in an analog system with additive Gaussian noise [@problem_id:1613384]. You will explicitly calculate and compare the mutual information before and after a second noisy stage. This quantitative approach provides a powerful confirmation of the Data Processing Inequality, showing how the addition of more noise irretrievably diminishes the information content.", "problem": "Consider a simplified model for a signal cascade in a communication system. An initial continuous signal, modeled by a random variable $X$, is a zero-mean Gaussian random variable with variance $\\sigma_X^2$. This signal passes through a first noisy channel, which adds independent, zero-mean Gaussian noise $N_1$ with variance $\\sigma_{N_1}^2$. The resulting signal is $Y = X + N_1$. This signal $Y$ is then fed into a second channel, which adds further independent, zero-mean Gaussian noise $N_2$ with variance $\\sigma_{N_2}^2$. The final output signal is $Z = Y + N_2$. The random variables $X$, $N_1$, and $N_2$ are all mutually independent. Assume that all variances, $\\sigma_X^2$, $\\sigma_{N_1}^2$, and $\\sigma_{N_2}^2$, are strictly positive.\n\nAll information-theoretic quantities are to be measured in nats, which means all logarithms are natural logarithms ($\\ln$). Let $I(X;Y)$ be the mutual information between the original signal $X$ and the signal after the first stage, $Y$. Let $I(X;Z)$ be the mutual information between the original signal $X$ and the final signal, $Z$.\n\nWhich of the following statements correctly describes the relationship between $I(X;Y)$ and $I(X;Z)$?\n\nA. $I(X;Y) < I(X;Z)$\n\nB. $I(X;Y) > I(X;Z)$\n\nC. $I(X;Y) = I(X;Z)$\n\nD. The relationship cannot be determined without knowing the specific numerical values of the variances.\n\nE. $I(X;Y)$ and $I(X;Z)$ are both infinite.", "solution": "We are given $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, $N_{1} \\sim \\mathcal{N}(0,\\sigma_{N_{1}}^{2})$, $N_{2} \\sim \\mathcal{N}(0,\\sigma_{N_{2}}^{2})$, all mutually independent and with strictly positive variances. The channels are $Y = X + N_{1}$ and $Z = Y + N_{2} = X + (N_{1}+N_{2})$. Since $N_{1}$ and $N_{2}$ are independent Gaussians, $N_{1}+N_{2} \\sim \\mathcal{N}(0,\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2})$ and is independent of $X$.\n\nWe compute mutual informations using $I(U;V) = h(V) - h(V|U)$ and the differential entropy of a zero-mean Gaussian $W \\sim \\mathcal{N}(0,\\sigma^{2})$, namely $h(W) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)$.\n\nFirst stage:\n- $Y = X + N_{1}$ is Gaussian with variance $\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2}$, so\n$$\nh(Y) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2})\\big).\n$$\n- Given $X$, $Y|X = X + N_{1}$ has the same distribution as $N_{1}$, so\n$$\nh(Y|X) = h(N_{1}) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,\\sigma_{N_{1}}^{2}\\big).\n$$\nThus\n$$\nI(X;Y) = h(Y) - h(Y|X) = \\frac{1}{2} \\ln\\!\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2}}{\\sigma_{N_{1}}^{2}}\\right) = \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right).\n$$\n\nSecond stage:\n- $Z = X + (N_{1}+N_{2})$ is Gaussian with variance $\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}$, so\n$$\nh(Z) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2})\\big).\n$$\n- Given $X$, $Z|X$ has the same distribution as $N_{1}+N_{2}$, so\n$$\nh(Z|X) = h(N_{1}+N_{2}) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2})\\big).\n$$\nThus\n$$\nI(X;Z) = h(Z) - h(Z|X) = \\frac{1}{2} \\ln\\!\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right) = \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right).\n$$\n\nComparison:\nSince $\\sigma_{N_{2}}^{2} > 0$, we have $\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2} > \\sigma_{N_{1}}^{2}$, which implies\n$$\n\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}} < \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}.\n$$\nBecause $\\ln$ is strictly increasing,\n$$\n\\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right) < \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right),\n$$\nso $I(X;Z) < I(X;Y)$.\n\nThis also aligns with the data processing inequality for the Markov chain $X \\to Y \\to Z$, which guarantees $I(X;Z) \\leq I(X;Y)$; strict inequality holds here because additional independent Gaussian noise is added in the second stage and all variances are strictly positive.\n\nTherefore, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1613384"}, {"introduction": "The inequality $I(X;Z) \\le I(X;Y)$ tells us information can't increase, but can it be completely eliminated? This problem [@problem_id:1613411] challenges you to analyze a cleverly constructed channel where the second processing stage is designed to be independent of its input. This thought experiment demonstrates a powerful edge case of the Data Processing Inequality, showing how to design a system that intentionally erases all transmitted information, resulting in zero mutual information at the final output.", "problem": "Consider a simplified model of a two-stage digital communication system. A source produces a binary signal $X$, which can be either 0 or 1. The prior probability distribution for the source is symmetric, i.e., $P(X=0) = P(X=1) = 1/2$.\n\nThe signal $X$ is first transmitted through a noisy channel to an intermediate node, producing a signal $Y \\in \\{0, 1\\}$. This channel is a Binary Symmetric Channel with a crossover probability of $\\epsilon = 1/3$. This means the probability of a bit flip is $P(Y \\neq x | X = x) = 1/3$.\n\nThe signal $Y$ is then processed by the intermediate node and re-transmitted to a final destination, producing a signal $Z \\in \\{+, -\\}$. This second stage is also a noisy channel governed by the following conditional probabilities:\n- $P(Z = + | Y = 0) = 1/4$\n- $P(Z = + | Y = 1) = 1/4$\n\nThe entire process forms a Markov chain $X \\rightarrow Y \\rightarrow Z$.\n\nCalculate the mutual information between the original signal and the intermediate signal, $I(X;Y)$, and the mutual information between the original signal and the final signal, $I(X;Z)$. Express your answers for the mutual information in bits. Your final answer should be a row matrix containing the values for $I(X;Y)$ and $I(X;Z)$, in that order.", "solution": "We are given a Markov chain $X \\rightarrow Y \\rightarrow Z$ with $X \\in \\{0,1\\}$, $P(X=0)=P(X=1)=\\frac{1}{2}$, a first-stage Binary Symmetric Channel (BSC) with crossover probability $\\epsilon=\\frac{1}{3}$, and a second stage where $P(Z=+ \\mid Y=0)=P(Z=+ \\mid Y=1)=\\frac{1}{4}$.\n\nFirst, compute $I(X;Y)$. By definition, $I(X;Y)=H(Y)-H(Y \\mid X)$.\n\nFor the BSC with $\\epsilon=\\frac{1}{3}$ and symmetric input, the conditional distribution of $Y$ given $X=x$ is $P(Y=x \\mid X=x)=\\frac{2}{3}$ and $P(Y \\neq x \\mid X=x)=\\frac{1}{3}$. The marginal of $Y$ is\n$$\nP(Y=0)=P(Y=0 \\mid X=0)P(X=0)+P(Y=0 \\mid X=1)P(X=1)=\\frac{2}{3}\\cdot \\frac{1}{2}+\\frac{1}{3}\\cdot \\frac{1}{2}=\\frac{1}{2},\n$$\nand similarly $P(Y=1)=\\frac{1}{2}$. Hence\n$$\nH(Y)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\nThe conditional entropy is\n$$\nH(Y \\mid X)=\\sum_{x \\in \\{0,1\\}}P(X=x)\\,H(Y \\mid X=x)=H\\!\\left(\\tfrac{2}{3},\\tfrac{1}{3}\\right)\n= -\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)-\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right).\n$$\nTherefore\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right).\n$$\n\nNext, compute $I(X;Z)$. We are given $P(Z=+ \\mid Y=0)=P(Z=+ \\mid Y=1)=\\frac{1}{4}$, so $P(Z=- \\mid Y=0)=P(Z=- \\mid Y=1)=\\frac{3}{4}$. Thus the distribution of $Z$ does not depend on $Y$, implying $Z$ is independent of $Y$. Because $X \\rightarrow Y \\rightarrow Z$ is a Markov chain, $Z$ is also independent of $X$, hence\n$$\nI(X;Z)=0.\n$$\nEquivalently, one can verify $P(Z=+)=\\sum_{y}P(Z=+ \\mid Y=y)P(Y=y)=\\frac{1}{4}$ and $H(Z \\mid X)=H(Z)$, which again gives $I(X;Z)=H(Z)-H(Z \\mid X)=0$.\n\nThe mutual informations in bits are thus $I(X;Y)=1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)$ and $I(X;Z)=0$.", "answer": "$$\\boxed{\\begin{pmatrix}1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right) & 0\\end{pmatrix}}$$", "id": "1613411"}]}