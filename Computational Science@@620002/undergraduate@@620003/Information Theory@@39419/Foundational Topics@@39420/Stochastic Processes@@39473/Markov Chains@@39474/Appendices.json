{"hands_on_practices": [{"introduction": "The core of a Markov chain lies in its step-by-step evolution. Before we can analyze long-term behaviors, we must master the fundamentals of predicting the chain's state in the immediate future. This exercise provides hands-on practice in calculating the probability of a specific sequence of events by combining the initial state probabilities with the transition rules, demonstrating how the chain's future path depends only on its present state. [@problem_id:1639051]", "problem": "A simplified biophysical model describes the transcriptional activity of a particular gene. The gene's promoter can exist in one of two states: 'ON' (actively transcribing messenger RNA) or 'OFF' (inactive). The state of the gene at a discrete time step $n$ is represented by the random variable $X_n$. This process is modeled as a time-homogeneous Markov chain.\n\nAt the beginning of the observation, at time $n=0$, the probability that the gene is in the 'ON' state is given as $P(X_0 = \\text{'ON'}) = \\frac{1}{3}$.\n\nThe state transitions over a single time step are governed by the following probabilities:\n- The probability that a gene in the 'ON' state at time $n$ remains in the 'ON' state at time $n+1$ is $0.8$.\n- The probability that a gene in the 'OFF' state at time $n$ transitions to the 'ON' state at time $n+1$ is $0.4$.\n\nCalculate the joint probability that the gene is in the 'ON' state at time step $n=1$ and in the 'OFF' state at time step $n=2$. In other words, find the value of $P(X_1 = \\text{'ON'}, X_2 = \\text{'OFF'})$. Express your final answer as a decimal rounded to three significant figures.", "solution": "The problem asks for the joint probability $P(X_1 = \\text{'ON'}, X_2 = \\text{'OFF'})$. We can solve this by first defining the states and the transition probabilities, then using the properties of Markov chains and the chain rule of probability.\n\nLet's denote the two states as $S_{on} = \\text{'ON'}$ and $S_{off} = \\text{'OFF'}$.\n\nFirst, we establish the complete set of initial probabilities and transition probabilities from the information given.\n\nThe initial probabilities at time $n=0$ are:\n$P(X_0 = S_{on}) = \\frac{1}{3}$\nSince there are only two states, the probability of being in the 'OFF' state initially is:\n$P(X_0 = S_{off}) = 1 - P(X_0 = S_{on}) = 1 - \\frac{1}{3} = \\frac{2}{3}$\n\nThe transition probabilities are given as:\n$P(X_{n+1} = S_{on} | X_n = S_{on}) = 0.8$\n$P(X_{n+1} = S_{on} | X_n = S_{off}) = 0.4$\n\nFrom these, we can deduce the other two transition probabilities, as the probabilities of transitioning from a given state must sum to 1:\n$P(X_{n+1} = S_{off} | X_n = S_{on}) = 1 - P(X_{n+1} = S_{on} | X_n = S_{on}) = 1 - 0.8 = 0.2$\n$P(X_{n+1} = S_{off} | X_n = S_{off}) = 1 - P(X_{n+1} = S_{on} | X_n = S_{off}) = 1 - 0.4 = 0.6$\n\nNow, we can calculate the target joint probability $P(X_1 = S_{on}, X_2 = S_{off})$. Using the chain rule for probability, we can write:\n$$P(X_1 = S_{on}, X_2 = S_{off}) = P(X_2 = S_{off} | X_1 = S_{on}) \\times P(X_1 = S_{on})$$\n\nThe first term, $P(X_2 = S_{off} | X_1 = S_{on})$, is the probability of transitioning from 'ON' to 'OFF'. Due to the time-homogeneous property of the Markov chain, this is simply the one-step transition probability we found earlier:\n$$P(X_2 = S_{off} | X_1 = S_{on}) = 0.2$$\n\nThe second term, $P(X_1 = S_{on})$, is the total probability of the gene being in the 'ON' state at time $n=1$. We can calculate this using the law of total probability, summing over the possible states at $n=0$:\n$$P(X_1 = S_{on}) = P(X_1 = S_{on} | X_0 = S_{on})P(X_0 = S_{on}) + P(X_1 = S_{on} | X_0 = S_{off})P(X_0 = S_{off})$$\nSubstituting the known values:\n$$P(X_1 = S_{on}) = (0.8) \\left(\\frac{1}{3}\\right) + (0.4) \\left(\\frac{2}{3}\\right)$$\n$$P(X_1 = S_{on}) = \\frac{0.8}{3} + \\frac{0.8}{3} = \\frac{1.6}{3}$$\n\nNow we can substitute both calculated components back into our original equation for the joint probability:\n$$P(X_1 = S_{on}, X_2 = S_{off}) = (0.2) \\times \\left(\\frac{1.6}{3}\\right)$$\n$$P(X_1 = S_{on}, X_2 = S_{off}) = \\frac{0.32}{3}$$\n$$P(X_1 = S_{on}, X_2 = S_{off}) = 0.10666...$$\n\nThe problem requires the answer to be rounded to three significant figures. The first significant figure is 1, the second is 0, and the third is 6. The next digit is 6, which is 5 or greater, so we round the third significant figure up.\n$$0.10666... \\approx 0.107$$", "answer": "$$\\boxed{0.107}$$", "id": "1639051"}, {"introduction": "After a Markov chain runs for a long time, it often settles into a state of equilibrium where the probability of being in any particular state becomes constant. This equilibrium, known as the stationary distribution, reveals the system's long-term tendencies. This practice challenges you to calculate this fundamental property for an intuitive system, providing a concrete understanding of how to determine where the system will spend its time on average. [@problem_id:1639083]", "problem": "A new model of a robotic vacuum cleaner is being tested in a small apartment that consists of three rooms: a Living Room, a Kitchen, and a Bedroom. The robot's movement between these rooms is probabilistic and can be described by a discrete-time Markov chain. The state of the system is the room the robot currently occupies. After each cleaning cycle in a room, the robot decides its next location based on a fixed set of probabilities.\n\nThe movement rules are as follows:\n- When the robot is in the Living Room, it has a probability of $\\frac{1}{2}$ of remaining in the Living Room for the next cycle and a probability of $\\frac{1}{2}$ of moving to the Kitchen. It cannot move directly from the Living Room to the Bedroom.\n- When the robot is in the Kitchen, it always moves to a different room. It moves to the Living Room with a probability of $\\frac{1}{3}$ and to the Bedroom with a probability of $\\frac{2}{3}$.\n- When the robot is in the Bedroom, it has a probability of $\\frac{1}{4}$ of remaining in the Bedroom for the next cycle and a probability of $\\frac{3}{4}$ of moving to the Kitchen. It cannot move directly from the Bedroom to the Living Room.\n\nAfter the robot has been operating for a very long time, the probability of finding it in any given room approaches a steady state. This is known as the stationary distribution of the Markov chain.\n\nCalculate this stationary distribution. Present your answer as a row matrix of the form $(\\pi_L, \\pi_K, \\pi_B)$, where $\\pi_L$, $\\pi_K$, and $\\pi_B$ represent the long-term probabilities of finding the robot in the Living Room, Kitchen, and Bedroom, respectively. Express these probabilities as exact fractions.", "solution": "Model the robot’s movement as a discrete-time Markov chain with states ordered as Living Room (L), Kitchen (K), Bedroom (B). The transition matrix $P$ with rows summing to $1$ is\n$$\nP=\\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n\\frac{1}{3} & 0 & \\frac{2}{3} \\\\\n0 & \\frac{3}{4} & \\frac{1}{4}\n\\end{pmatrix}.\n$$\nLet the stationary distribution be $\\pi=(\\pi_{L},\\pi_{K},\\pi_{B})$, a row vector satisfying $\\pi=\\pi P$ and $\\pi_{L}+\\pi_{K}+\\pi_{B}=1$. Writing component equations from $\\pi=\\pi P$ gives\n$$\n\\pi_{L}=\\frac{1}{2}\\pi_{L}+\\frac{1}{3}\\pi_{K},\\quad\n\\pi_{K}=\\frac{1}{2}\\pi_{L}+\\frac{3}{4}\\pi_{B},\\quad\n\\pi_{B}=\\frac{2}{3}\\pi_{K}+\\frac{1}{4}\\pi_{B}.\n$$\nFrom the first equation,\n$$\n\\pi_{L}-\\frac{1}{2}\\pi_{L}=\\frac{1}{3}\\pi_{K}\\;\\Rightarrow\\;\\frac{1}{2}\\pi_{L}=\\frac{1}{3}\\pi_{K}\\;\\Rightarrow\\;\\pi_{K}=\\frac{3}{2}\\pi_{L}.\n$$\nFrom the third equation,\n$$\n\\pi_{B}-\\frac{1}{4}\\pi_{B}=\\frac{2}{3}\\pi_{K}\\;\\Rightarrow\\;\\frac{3}{4}\\pi_{B}=\\frac{2}{3}\\pi_{K}\\;\\Rightarrow\\;\\pi_{B}=\\frac{8}{9}\\pi_{K}.\n$$\nExpressing all in terms of $\\pi_{L}$ using $\\pi_{K}=\\frac{3}{2}\\pi_{L}$ yields\n$$\n\\pi_{B}=\\frac{8}{9}\\cdot\\frac{3}{2}\\pi_{L}=\\frac{4}{3}\\pi_{L}.\n$$\nApply normalization:\n$$\n\\pi_{L}+\\pi_{K}+\\pi_{B}=\\pi_{L}+\\frac{3}{2}\\pi_{L}+\\frac{4}{3}\\pi_{L}=\\left(1+\\frac{3}{2}+\\frac{4}{3}\\right)\\pi_{L}=\\frac{23}{6}\\pi_{L}=1,\n$$\nso\n$$\n\\pi_{L}=\\frac{6}{23},\\quad \\pi_{K}=\\frac{3}{2}\\cdot\\frac{6}{23}=\\frac{9}{23},\\quad \\pi_{B}=\\frac{4}{3}\\cdot\\frac{6}{23}=\\frac{8}{23}.\n$$\nTherefore, the stationary distribution is $(\\frac{6}{23},\\frac{9}{23},\\frac{8}{23})$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{23} & \\frac{9}{23} & \\frac{8}{23} \\end{pmatrix}}$$", "id": "1639083"}, {"introduction": "Markov chains are more than just an abstract mathematical tool; they are a powerful framework for modeling real-world phenomena, including information sources. This problem illustrates a key application in data compression, where understanding the memory of a source—the dependency of a symbol on the previous one—allows for more efficient encoding. By comparing a generic compression strategy with one that leverages the Markovian structure, you will quantify the tangible benefits of this advanced modeling approach. [@problem_id:1639043]", "problem": "A discrete-time, time-homogeneous Markov source produces a sequence of symbols from the alphabet $\\mathcal{S} = \\{A, B, C\\}$. The state of the source at any time corresponds to the symbol it has just emitted. The state transitions are governed by the following right stochastic matrix $P$, where the entry $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$. The states are ordered as $(A, B, C)$.\n\n$$\nP = \\begin{pmatrix}\n0.8 & 0.1 & 0.1 \\\\\n0.2 & 0.6 & 0.2 \\\\\n0.1 & 0.2 & 0.7\n\\end{pmatrix}\n$$\n\nTwo different binary encoding strategies are being evaluated for compressing the output of this source.\n- **Strategy 1:** A single, static Huffman code is designed based on the unconditional probabilities of the symbols, as determined by the source's stationary distribution.\n- **Strategy 2:** A set of three distinct Huffman codes is used. The choice of codebook for the current symbol depends on the previously emitted symbol (the previous state).\n\nCalculate the compression efficiency gain achieved by using Strategy 2 over Strategy 1. This gain is defined as the reduction in the long-run average codeword length per symbol.\n\nExpress your final answer in units of bits/symbol, rounded to three significant figures.", "solution": "Let the stationary symbol probabilities be $s_{A}, s_{B}, s_{C}$ with $s_{A}+s_{B}+s_{C}=1$ and $(s_{A},s_{B},s_{C})P=(s_{A},s_{B},s_{C})$. From\n$$\n\\begin{aligned}\ns_{A}&=0.8 s_{A}+0.2 s_{B}+0.1 s_{C},\\\\\ns_{B}&=0.1 s_{A}+0.6 s_{B}+0.2 s_{C},\n\\end{aligned}\n$$\nwe obtain $2 s_{A}-2 s_{B}-s_{C}=0$ and $4 s_{B}-s_{A}-2 s_{C}=0$. Solving with $s_{A}+s_{B}+s_{C}=1$ gives\n$$\ns_{A}=\\frac{8}{19},\\quad s_{B}=\\frac{5}{19},\\quad s_{C}=\\frac{6}{19}.\n$$\n\nStrategy 1 uses a single Huffman code for $(s_{A},s_{B},s_{C})=(\\frac{8}{19},\\frac{5}{19},\\frac{6}{19})$. For three symbols, the optimal prefix lengths are $(1,2,2)$ with the largest probability assigned length $1$. The expected length is\n$$\nL_{1}=\\frac{8}{19}\\cdot 1+\\frac{5}{19}\\cdot 2+\\frac{6}{19}\\cdot 2=\\frac{30}{19}.\n$$\n\nStrategy 2 uses context-dependent Huffman codes based on the conditional distributions given the previous symbol, i.e., the rows of $P$.\n- Given $A$: $(0.8,0.1,0.1)=(\\frac{4}{5},\\frac{1}{10},\\frac{1}{10})$ yields lengths $(1,2,2)$ and\n$$\nL_{A}=\\frac{4}{5}\\cdot 1+\\frac{1}{10}\\cdot 2+\\frac{1}{10}\\cdot 2=\\frac{6}{5}.\n$$\n- Given $B$: $(0.2,0.6,0.2)=(\\frac{1}{5},\\frac{3}{5},\\frac{1}{5})$ yields lengths $(1,2,2)$ and\n$$\nL_{B}=\\frac{3}{5}\\cdot 1+\\frac{1}{5}\\cdot 2+\\frac{1}{5}\\cdot 2=\\frac{7}{5}.\n$$\n- Given $C$: $(0.1,0.2,0.7)=(\\frac{1}{10},\\frac{1}{5},\\frac{7}{10})$ yields lengths $(1,2,2)$ and\n$$\nL_{C}=\\frac{7}{10}\\cdot 1+\\frac{1}{5}\\cdot 2+\\frac{1}{10}\\cdot 2=\\frac{13}{10}.\n$$\nThe long-run average code length under Strategy 2 is the stationary average over contexts:\n$$\nL_{2}=s_{A}L_{A}+s_{B}L_{B}+s_{C}L_{C}\n=\\frac{8}{19}\\cdot\\frac{6}{5}+\\frac{5}{19}\\cdot\\frac{7}{5}+\\frac{6}{19}\\cdot\\frac{13}{10}\n=\\frac{48}{95}+\\frac{35}{95}+\\frac{39}{95}=\\frac{122}{95}.\n$$\n\nThe compression efficiency gain is the reduction $L_{1}-L_{2}$:\n$$\nL_{1}-L_{2}=\\frac{30}{19}-\\frac{122}{95}=\\frac{150-122}{95}=\\frac{28}{95}\\approx 0.2947368421.\n$$\nRounded to three significant figures, the gain is $0.295$ bits/symbol.", "answer": "$$\\boxed{0.295}$$", "id": "1639043"}]}