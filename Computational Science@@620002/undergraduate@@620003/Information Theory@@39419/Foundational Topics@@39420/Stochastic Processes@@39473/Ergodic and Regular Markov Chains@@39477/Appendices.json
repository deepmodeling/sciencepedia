{"hands_on_practices": [{"introduction": "The cornerstone of analyzing a Markov chain's long-term behavior is finding its stationary distribution. This distribution describes the probability of finding the system in each state after it has run for a very long time, reaching a stable equilibrium. This first practice problem [@problem_id:1621829] will guide you through the fundamental algebraic steps to calculate this distribution for a simple two-state system, a core skill for understanding more complex models.", "problem": "A simplified model for the power management of a computer's Central Processing Unit (CPU) describes its state at discrete time intervals. The CPU can be in one of two states: State 1 (Active) or State 2 (Low-Power). The transition between these states is modeled as a Markov chain.\nThe probability of the CPU remaining in the Active state from one time step to the next is $p_{11} = 0.75$.\nThe probability of the CPU remaining in the Low-Power state from one time step to the next is $p_{22} = 0.5$.\nThe system must transition to the other state if it does not remain in its current state.\nAssuming this system runs for a very long time, it will approach a steady state where the probability of being in either state becomes constant. This steady-state probability distribution is known as the stationary distribution.\nCalculate the stationary distribution vector $\\pi = (\\pi_1, \\pi_2)$, where $\\pi_1$ is the long-term probability of finding the CPU in the Active state, and $\\pi_2$ is the long-term probability of finding it in the Low-Power state.\nExpress the components of the stationary distribution vector as exact fractions.", "solution": "Let the two states be State 1 (Active) and State 2 (Low-Power). The problem provides the probabilities of staying in a given state. We can use this information to construct the one-step transition probability matrix $P$. The matrix $P$ is a $2 \\times 2$ matrix where the entry $P_{ij}$ is the probability of transitioning from state $i$ to state $j$.\n\nThe probability of remaining in the Active state (State 1) is given as $p_{11} = 0.75$. Since the sum of probabilities in any row of a transition matrix must be 1, the probability of transitioning from the Active state to the Low-Power state (State 2) is $p_{12} = 1 - p_{11} = 1 - 0.75 = 0.25$.\n\nThe probability of remaining in the Low-Power state (State 2) is given as $p_{22} = 0.5$. Therefore, the probability of transitioning from the Low-Power state to the Active state (State 1) is $p_{21} = 1 - p_{22} = 1 - 0.5 = 0.5$.\n\nWith these probabilities, the transition matrix $P$ is:\n$$\nP = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{21} & p_{22} \\end{pmatrix} = \\begin{pmatrix} 0.75 & 0.25 \\\\ 0.5 & 0.5 \\end{pmatrix}\n$$\n\nThe stationary distribution is a probability vector $\\pi = (\\pi_1, \\pi_2)$ that satisfies two conditions:\n1. $\\pi P = \\pi$\n2. $\\pi_1 + \\pi_2 = 1$ (The sum of the probabilities must be 1)\n\nLet's use the first condition, $\\pi P = \\pi$:\n$$\n(\\pi_1, \\pi_2) \\begin{pmatrix} 0.75 & 0.25 \\\\ 0.5 & 0.5 \\end{pmatrix} = (\\pi_1, \\pi_2)\n$$\n\nThis matrix equation gives us a system of two linear equations:\n(i) $0.75 \\pi_1 + 0.5 \\pi_2 = \\pi_1$\n(ii) $0.25 \\pi_1 + 0.5 \\pi_2 = \\pi_2$\n\nLet's simplify the first equation (i):\n$$\n0.5 \\pi_2 = \\pi_1 - 0.75 \\pi_1\n$$\n$$\n0.5 \\pi_2 = 0.25 \\pi_1\n$$\nMultiplying both sides by 4 to remove the decimals, we get:\n$$\n2 \\pi_2 = \\pi_1\n$$\n\nThe second equation (ii) is redundant, as it will yield the same relationship. Let's verify:\n$$\n0.25 \\pi_1 = \\pi_2 - 0.5 \\pi_2\n$$\n$$\n0.25 \\pi_1 = 0.5 \\pi_2\n$$\nMultiplying by 4 gives:\n$$\n\\pi_1 = 2 \\pi_2\n$$\nAs expected, both equations are equivalent.\n\nNow we use the second condition, the normalization equation:\n$$\n\\pi_1 + \\pi_2 = 1\n$$\nSubstitute the relation $\\pi_1 = 2 \\pi_2$ into the normalization equation:\n$$\n(2 \\pi_2) + \\pi_2 = 1\n$$\n$$\n3 \\pi_2 = 1\n$$\n$$\n\\pi_2 = \\frac{1}{3}\n$$\n\nNow we can find $\\pi_1$ using the relationship $\\pi_1 = 2 \\pi_2$:\n$$\n\\pi_1 = 2 \\left( \\frac{1}{3} \\right) = \\frac{2}{3}\n$$\n\nThus, the stationary distribution vector is $\\pi = (\\pi_1, \\pi_2) = (\\frac{2}{3}, \\frac{1}{3})$. This means that, in the long run, the CPU will be in the Active state approximately $66.7\\%$ of the time and in the Low-Power state approximately $33.3\\%$ of the time. The problem asks for the components as exact fractions.\nThe stationary distribution is $(\\frac{2}{3}, \\frac{1}{3})$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\end{pmatrix}}$$", "id": "1621829"}, {"introduction": "While calculating a stationary distribution is a key skill, it's equally important to understand the conditions under which a unique, stable equilibrium exists. This depends on the structural properties of the Markov chain. This exercise [@problem_id:1621848] challenges you to classify a chain based on its properties of irreducibility and periodicity, which together determine if a chain is ergodic and thus predictable in the long run.", "problem": "A simplified model of a network data router's buffer is described by a discrete-time Markov chain with three states: $S_1$ (Empty), $S_2$ (Half-full), and $S_3$ (Full). The router's state is observed at the end of each time step. The transitions between states are governed by the following rules:\n\n1.  If the buffer is Empty ($S_1$) at the start of a time step, an incoming packet will certainly arrive, causing the buffer to become Half-full ($S_2$) by the end of the time step.\n2.  If the buffer is Half-full ($S_2$), there is a 50% chance that a new packet arrives, making it Full ($S_3$), and a 50% chance that a packet is processed and sent, making it Empty ($S_1$). These are the only two possibilities.\n3.  If the buffer is Full ($S_3$), the router prioritizes sending a packet. It will certainly process and send one packet, causing the buffer to become Half-full ($S_2$) by the end of the time step.\n\nBased on this model, how would you classify the resulting Markov chain?\n\nA. The chain is regular.\n\nB. The chain is ergodic but not regular.\n\nC. The chain is irreducible but not ergodic.\n\nD. The chain is not irreducible.\n\nE. The chain has at least one absorbing state.", "solution": "Let the states be ordered as $(S_{1},S_{2},S_{3})$. From the rules:\n- From $S_{1}$, the next state is $S_{2}$ with probability $1$.\n- From $S_{2}$, the next state is $S_{1}$ with probability $\\frac{1}{2}$ and $S_{3}$ with probability $\\frac{1}{2}$.\n- From $S_{3}$, the next state is $S_{2}$ with probability $1$.\n\nThus the one-step transition matrix is\n$$\nP=\\begin{pmatrix}\n0 & 1 & 0\\\\\n\\frac{1}{2} & 0 & \\frac{1}{2}\\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\n\nIrreducibility: A finite-state Markov chain is irreducible if every state communicates with every other state. From $S_{1}$ we reach $S_{2}$ in one step and $S_{3}$ in two steps via $S_{2}$. From $S_{3}$ we reach $S_{2}$ in one step and $S_{1}$ in two steps via $S_{2}$. From $S_{2}$ we reach $S_{1}$ or $S_{3}$ in one step. Hence every pair of states communicates, so the chain is irreducible.\n\nPeriodicity: The period of a state $i$ is $d(i)=\\gcd\\{n\\ge 1: (P^{n})_{ii}>0\\}$. Consider $S_{2}$. We have $(P^{1})_{22}=0$, $(P^{2})_{22}>0$ since $S_{2}\\to S_{1}\\to S_{2}$ occurs with probability $\\frac{1}{2}\\cdot 1>0$, and $(P^{3})_{22}=0$ because any path of length $3$ from $S_{2}$ must leave $S_{2}$ at step $1$, return at step $2$, and then leave at step $3$, so it cannot end at $S_{2}$. More generally, returns to $S_{2}$ occur only at even times, so $\\{n\\ge 1:(P^{n})_{22}>0\\}=\\{2,4,6,\\dots\\}$ and $d(S_{2})=2$. In an irreducible chain, all states share the same period, so every state has period $2$. Therefore the chain is periodic (not aperiodic).\n\nConsequences for the options:\n- There is no absorbing state because $(P)_{ii}\\neq 1$ for all $i$. Hence option E is false.\n- The chain is irreducible (so option D is false), but it is periodic with period $2$, so it is not ergodic in the standard sense (irreducible and aperiodic). Thus option B is false.\n- A regular chain (primitive) requires some power $P^{n}$ to have all entries strictly positive; this fails for a periodic bipartite chain like this one, so option A is false.\n- Therefore the correct classification is that the chain is irreducible but not ergodic.\n\nHence the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1621848"}, {"introduction": "The concepts of stationary distributions and ergodicity are not just theoretical; they have profound applications in information theory. A primary application is in quantifying the information generated by a source modeled as a Markov chain. In this practice problem [@problem_id:1621886], you will apply your knowledge of stationary distributions to calculate the entropy rate of a Markov source, a fundamental measure of its unpredictability and information content.", "problem": "Consider a simplified model for a volatile digital memory bit, which can be in one of two states: State 0 or State 1. The system is modeled as a discrete-time Markov chain. At each time step, the bit has a probability $p$ of flipping to the opposite state and a probability $1-p$ of remaining in its current state. This behavior is symmetric for both states. Let the probability of flipping be $p = 0.15$.\n\nCalculate the entropy rate of this Markov source. Express your answer in bits per symbol, rounded to four significant figures.", "solution": "Let the two states of the Markov chain be $S_0$ and $S_1$. The problem describes a symmetric transition model. The transition probabilities are given by:\n- $P(S_1|S_0) = p$ (flipping from 0 to 1)\n- $P(S_0|S_0) = 1-p$ (staying at 0)\n- $P(S_0|S_1) = p$ (flipping from 1 to 0)\n- $P(S_1|S_1) = 1-p$ (staying at 1)\n\nThe transition probability matrix $P$ is:\n$$\nP = \\begin{pmatrix}\n1-p & p \\\\\np & 1-p\n\\end{pmatrix}\n$$\n\nThe entropy rate $H(\\mathcal{S})$ of a stationary Markov chain is given by the formula:\n$$\nH(\\mathcal{S}) = \\sum_{i} \\pi_i H(S'|S=S_i)\n$$\nwhere $\\pi_i$ is the stationary probability of being in state $S_i$, and $H(S'|S=S_i)$ is the conditional entropy of the next state, given that the current state is $S_i$.\n\nFirst, we need to find the stationary distribution $\\pi = (\\pi_0, \\pi_1)$, which satisfies the equation $\\pi P = \\pi$. This leads to the system of linear equations:\n$$\n\\pi_0 (1-p) + \\pi_1 p = \\pi_0\n$$\n$$\n\\pi_0 p + \\pi_1 (1-p) = \\pi_1\n$$\nalong with the normalization condition $\\pi_0 + \\pi_1 = 1$.\n\nFrom the first equation:\n$$\n\\pi_0 - \\pi_0 p + \\pi_1 p = \\pi_0 \\implies \\pi_1 p = \\pi_0 p\n$$\nFor $p \\neq 0$, this implies $\\pi_1 = \\pi_0$.\nSubstituting this into the normalization condition:\n$$\n\\pi_0 + \\pi_0 = 1 \\implies 2\\pi_0 = 1 \\implies \\pi_0 = \\frac{1}{2}\n$$\nTherefore, the stationary distribution is $\\pi = (\\frac{1}{2}, \\frac{1}{2})$. This result is expected due to the symmetry of the chain.\n\nNext, we calculate the conditional entropies. The entropy is calculated using the formula $H = -\\sum_k p_k \\log_2(p_k)$.\n\nFor state $S_0$, the next state can be $S_0$ with probability $1-p$ or $S_1$ with probability $p$. The conditional entropy is:\n$$\nH(S'|S=S_0) = -[(1-p)\\log_2(1-p) + p\\log_2(p)]\n$$\n\nFor state $S_1$, the next state can be $S_1$ with probability $1-p$ or $S_0$ with probability $p$. The conditional entropy is:\n$$\nH(S'|S=S_1) = -[p\\log_2(p) + (1-p)\\log_2(1-p)]\n$$\nWe see that $H(S'|S=S_0) = H(S'|S=S_1)$. This value is the binary entropy function, often denoted as $H_b(p)$.\n\nNow we can calculate the entropy rate:\n$$\nH(\\mathcal{S}) = \\pi_0 H(S'|S=S_0) + \\pi_1 H(S'|S=S_1)\n$$\n$$\nH(\\mathcal{S}) = \\frac{1}{2} H_b(p) + \\frac{1}{2} H_b(p) = H_b(p)\n$$\nSo, the entropy rate of this source is simply the binary entropy of the flip probability $p$.\n$$\nH(\\mathcal{S}) = -[p\\log_2(p) + (1-p)\\log_2(1-p)]\n$$\nWe are given $p = 0.15$, so $1-p = 0.85$. Substituting these values:\n$$\nH(\\mathcal{S}) = -[0.15 \\log_2(0.15) + 0.85 \\log_2(0.85)]\n$$\nTo compute the numerical value, we can use the change of base formula for logarithms, $\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$:\n$$\nH(\\mathcal{S}) = - \\frac{1}{\\ln(2)} [0.15 \\ln(0.15) + 0.85 \\ln(0.85)]\n$$\nUsing a calculator:\n$$\n\\ln(0.15) \\approx -1.8971206\n$$\n$$\n\\ln(0.85) \\approx -0.1625189\n$$\n$$\n\\ln(2) \\approx 0.6931472\n$$\n$$\nH(\\mathcal{S}) = - \\frac{1}{0.6931472} [0.15(-1.8971206) + 0.85(-0.1625189)]\n$$\n$$\nH(\\mathcal{S}) = - \\frac{1}{0.6931472} [-0.28456809 - 0.138141065]\n$$\n$$\nH(\\mathcal{S}) = - \\frac{-0.422709155}{0.6931472} \\approx 0.6098403\n$$\nThe problem asks for the answer rounded to four significant figures.\n$$\nH(\\mathcal{S}) \\approx 0.6098\n$$\nThe unit is bits per symbol.", "answer": "$$\\boxed{0.6098}$$", "id": "1621886"}]}