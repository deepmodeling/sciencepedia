{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we'll start with a foundational model: a simple two-state Markov process. This exercise models a hypothetical weather system, providing a tangible context for calculating the entropy rate of a source with memory [@problem_id:1621329]. By working through this problem, you will practice the core computational steps, from determining the stationary distribution to applying the formula for conditional entropy, building a solid base for analyzing more complex systems.", "problem": "A simplified climatological model for a remote island describes the weather on any given day as being either \"Sunny\" or \"Rainy\". The model is a stationary first-order Markov process. The probability that a Sunny day is followed by a Rainy day is $p$. Similarly, the probability that a Rainy day is followed by a Sunny day is also $p$. The value of $p$ is a constant such that $0 < p < 1$.\n\nAssuming this model has reached its steady state, determine the entropy rate of the daily weather sequence. Express your answer as a symbolic expression in terms of $p$. For all calculations, use logarithms in base 2. The final answer should be in units of bits per day.", "solution": "Let the two states of the Markov process be $S$ for a Sunny day and $R$ for a Rainy day. The problem statement provides the transition probabilities between these states.\n\nThe probability of transitioning from Sunny to Rainy is given as $P(R|S) = p$.\nSince the weather on the next day can only be Sunny or Rainy, the sum of probabilities of transitioning from a Sunny day must be 1. Therefore, the probability of a Sunny day being followed by another Sunny day is:\n$P(S|S) = 1 - P(R|S) = 1 - p$.\n\nSimilarly, the probability of transitioning from Rainy to Sunny is given as $P(S|R) = p$.\nThe probability of a Rainy day being followed by another Rainy day is:\n$P(R|R) = 1 - P(S|R) = 1 - p$.\n\nWe can represent these transition probabilities in a transition matrix $P$, where the rows correspond to the current state and columns to the next state (in the order S, R):\n$$\nP = \\begin{pmatrix} P(S|S) & P(R|S) \\\\ P(S|R) & P(R|R) \\end{pmatrix} = \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix}\n$$\n\nThe problem asks for the entropy rate of the process in its steady state. For a stationary Markov chain, the entropy rate $H(\\mathcal{X})$ is given by the conditional entropy of the next state given the current state:\n$$\nH(\\mathcal{X}) = H(X_{n+1} | X_n) = \\sum_{i \\in \\{S, R\\}} \\pi_i H(X_{n+1} | X_n=i)\n$$\nwhere $\\pi = (\\pi_S, \\pi_R)$ is the stationary distribution of the Markov chain.\n\nFirst, we need to find the stationary distribution $\\pi$, which satisfies the equation $\\pi P = \\pi$. This, along with the normalization condition $\\pi_S + \\pi_R = 1$, allows us to solve for $\\pi_S$ and $\\pi_R$.\nThe equation $\\pi P = \\pi$ gives:\n$$\n(\\pi_S, \\pi_R) \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix} = (\\pi_S, \\pi_R)\n$$\nThis results in the system of linear equations:\n1. $\\pi_S(1-p) + \\pi_R p = \\pi_S$\n2. $\\pi_S p + \\pi_R (1-p) = \\pi_R$\n\nFrom equation 1:\n$-\\pi_S p + \\pi_R p = 0 \\implies p(\\pi_R - \\pi_S) = 0$.\nSince $0 < p < 1$, we must have $\\pi_R = \\pi_S$.\n\nUsing the normalization condition $\\pi_S + \\pi_R = 1$:\n$\\pi_S + \\pi_S = 1 \\implies 2\\pi_S = 1 \\implies \\pi_S = \\frac{1}{2}$.\nTherefore, $\\pi_R = \\frac{1}{2}$. The stationary distribution is $\\pi = (\\frac{1}{2}, \\frac{1}{2})$. This is expected for a symmetric transition matrix.\n\nNext, we calculate the conditional entropies for each state. The problem specifies using base-2 logarithms.\nThe conditional entropy given that the current state is Sunny is:\n$$\nH(X_{n+1} | X_n=S) = -\\sum_{j \\in \\{S,R\\}} P(j|S) \\log_2 P(j|S)\n$$\n$$\nH(X_{n+1} | X_n=S) = -[P(S|S)\\log_2 P(S|S) + P(R|S)\\log_2 P(R|S)]\n$$\n$$\nH(X_{n+1} | X_n=S) = -[(1-p)\\log_2(1-p) + p\\log_2(p)]\n$$\n\nThe conditional entropy given that the current state is Rainy is:\n$$\nH(X_{n+1} | X_n=R) = -\\sum_{j \\in \\{S,R\\}} P(j|R) \\log_2 P(j|R)\n$$\n$$\nH(X_{n+1} | X_n=R) = -[P(S|R)\\log_2 P(S|R) + P(R|R)\\log_2 P(R|R)]\n$$\n$$\nH(X_{n+1} | X_n=R) = -[p\\log_2(p) + (1-p)\\log_2(1-p)]\n$$\nBoth conditional entropies are equal to the binary entropy function $H_b(p)$.\n\nFinally, we calculate the entropy rate using the stationary distribution:\n$$\nH(\\mathcal{X}) = \\pi_S H(X_{n+1} | X_n=S) + \\pi_R H(X_{n+1} | X_n=R)\n$$\n$$\nH(\\mathcal{X}) = \\frac{1}{2} [-p\\log_2(p) - (1-p)\\log_2(1-p)] + \\frac{1}{2} [-p\\log_2(p) - (1-p)\\log_2(1-p)]\n$$\n$$\nH(\\mathcal{X}) = (\\frac{1}{2} + \\frac{1}{2}) [-p\\log_2(p) - (1-p)\\log_2(1-p)]\n$$\n$$\nH(\\mathcal{X}) = -p\\log_2(p) - (1-p)\\log_2(1-p)\n$$\nThis function is often denoted as the binary entropy function $H_b(p)$. The units are bits per symbol, or in this context, bits per day.", "answer": "$$\\boxed{-p\\log_{2}(p) - (1-p)\\log_{2}(1-p)}$$", "id": "1621329"}, {"introduction": "What happens to the entropy rate when a process becomes completely predictable? This practice problem explores that crucial question by examining a stationary system that, once in a state, never leaves [@problem_id:1621350]. This scenario, which corresponds to a transition matrix being the identity matrix, reveals a fundamental principle: the entropy rate quantifies the average new information generated per step, which for a deterministic process is zero.", "problem": "A data logging system monitors two independent subsystems, System A and System B, at discrete time intervals, generating a sequence of state pairs.\n\nSystem A is a high-stability memory device with $M$ distinct states. Once the device is set to a particular state, it remains in that state indefinitely. The evolution of its state over time is modeled as a stationary Markov process.\n\nSystem B is a sensor monitoring a fluctuating binary phenomenon. It has two states, labeled 0 and 1. At each time step, there is a probability $p$ that the system transitions to the opposite state, and a probability $1-p$ that it remains in the same state. This process is also modeled as a stationary Markov source.\n\nThe data logger records the pair of states from both systems at each time step. Determine the entropy rate of this composite information source. Express your answer as a symbolic expression in terms of the parameter $p$, in units of bits per time step. For calculations, the base of the logarithm must be 2.", "solution": "Let the composite information source be denoted by $\\mathcal{C}$, which produces a sequence of pairs $(S_A, S_B)$, where $S_A$ is the state of System A and $S_B$ is the state of System B. Let the individual sources for System A and System B be $\\mathcal{A}$ and $\\mathcal{B}$, respectively. Since the two subsystems are independent, the entropy rate of the composite source is the sum of the entropy rates of the individual sources:\n$$H(\\mathcal{C}) = H(\\mathcal{A}) + H(\\mathcal{B})$$\n\nWe will now calculate the entropy rate for each source separately.\n\nFirst, consider System A. It has $M$ states and once it enters a state, it never leaves. This can be described by an $M \\times M$ Markov transition matrix $P^{(A)}$ where the diagonal elements are 1 and all off-diagonal elements are 0. This is the identity matrix, $P^{(A)} = I_M$.\n\nThe entropy rate of a stationary Markov source $\\mathcal{S}$ is given by the formula:\n$$H(\\mathcal{S}) = \\sum_{i} \\pi_i H_i$$\nwhere $\\pi_i$ is the stationary probability of being in state $i$, and $H_i$ is the entropy of the next state distribution given that the current state is $i$. The quantity $H_i$ is calculated as:\n$$H_i = -\\sum_{j} P_{ij} \\log_{2}(P_{ij})$$\nwhere $P_{ij}$ is the transition probability from state $i$ to state $j$.\n\nFor System A, the transition probabilities are $P^{(A)}_{ij} = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. For any given state $i$, the conditional entropy $H_i^{(A)}$ is:\n$$H_i^{(A)} = -\\sum_{j=1}^{M} P^{(A)}_{ij} \\log_{2}(P^{(A)}_{ij}) = - P^{(A)}_{ii} \\log_{2}(P^{(A)}_{ii}) - \\sum_{j \\neq i} P^{(A)}_{ij} \\log_{2}(P^{(A)}_{ij})$$\nSubstituting the values $P^{(A)}_{ii} = 1$ and $P^{(A)}_{ij} = 0$ for $j \\neq i$:\n$$H_i^{(A)} = - 1 \\cdot \\log_{2}(1) - \\sum_{j \\neq i} 0 \\cdot \\log_{2}(0)$$\nUsing the convention that $x \\log x \\to 0$ as $x \\to 0$, the expression simplifies to:\n$$H_i^{(A)} = -1 \\cdot 0 - 0 = 0$$\nSince this is true for all states $i=1, \\dots, M$, the entropy rate of System A is:\n$$H(\\mathcal{A}) = \\sum_{i=1}^{M} \\pi_i^{(A)} \\cdot 0 = 0$$\nThis result makes intuitive sense: after the initial state is known, the system's evolution is completely deterministic (it never changes), so no new information is generated at each time step.\n\nNext, consider System B. This is a binary Markov source with states {0, 1} and the transition matrix:\n$$P^{(B)} = \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix}$$\nSince the source is stationary, its state distribution $\\boldsymbol{\\pi}^{(B)} = (\\pi_0, \\pi_1)$ must satisfy the stationary condition $\\boldsymbol{\\pi}^{(B)} P^{(B)} = \\boldsymbol{\\pi}^{(B)}$.\n$$(\\pi_0, \\pi_1) \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix} = (\\pi_0, \\pi_1)$$\nThis gives the system of equations:\n1. $\\pi_0(1-p) + \\pi_1 p = \\pi_0 \\implies -\\pi_0 p + \\pi_1 p = 0 \\implies \\pi_0 = \\pi_1$ (for $p \\neq 0$).\n2. $\\pi_0 p + \\pi_1 (1-p) = \\pi_1 \\implies \\pi_0 p - \\pi_1 p = 0 \\implies \\pi_0 = \\pi_1$ (for $p \\neq 0$).\nWe also have the normalization condition $\\pi_0 + \\pi_1 = 1$. Combining these, we find the stationary distribution:\n$$\\pi_0 = \\pi_1 = \\frac{1}{2}$$\nNote that if $p=0$, $P^{(B)}$ is the identity matrix and any distribution is stationary. If $p=1$, the system deterministically flips state, and $\\boldsymbol{\\pi}^{(B)}=(1/2, 1/2)$ is the unique stationary distribution. The following calculation holds for all valid $p \\in [0, 1]$.\n\nNow we calculate the conditional entropies $H_0^{(B)}$ and $H_1^{(B)}$ for System B.\n$$H_0^{(B)} = - \\left( P^{(B)}_{00} \\log_{2}(P^{(B)}_{00}) + P^{(B)}_{01} \\log_{2}(P^{(B)}_{01}) \\right) = - \\left( (1-p) \\log_{2}(1-p) + p \\log_{2}(p) \\right)$$\n$$H_1^{(B)} = - \\left( P^{(B)}_{10} \\log_{2}(P^{(B)}_{10}) + P^{(B)}_{11} \\log_{2}(P^{(B)}_{11}) \\right) = - \\left( p \\log_{2}(p) + (1-p) \\log_{2}(1-p) \\right)$$\nBoth are equal to the binary entropy function, $H_b(p) = -p\\log_{2}(p) - (1-p)\\log_{2}(1-p)$.\n\nThe entropy rate of System B is:\n$$H(\\mathcal{B}) = \\pi_0 H_0^{(B)} + \\pi_1 H_1^{(B)} = \\frac{1}{2} H_b(p) + \\frac{1}{2} H_b(p) = H_b(p)$$\n$$H(\\mathcal{B}) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p)$$\n\nFinally, the total entropy rate of the composite source $\\mathcal{C}$ is the sum of the individual rates:\n$$H(\\mathcal{C}) = H(\\mathcal{A}) + H(\\mathcal{B}) = 0 + \\left( -p \\log_{2}(p) - (1-p) \\log_{2}(1-p) \\right)$$\n$$H(\\mathcal{C}) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p)$$", "answer": "$$\\boxed{-p \\log_{2}(p) - (1-p) \\log_{2}(1-p)}$$", "id": "1621350"}, {"introduction": "Our final practice problem investigates the other extreme: a Markov process designed to have no memory, where the next state is statistically independent of the current one [@problem_id:1621359]. This exercise is a powerful conceptual check, demonstrating how the general formula for a Markov source's entropy rate elegantly simplifies. You will see firsthand that when memory is removed, the process behaves as an Independent and Identically Distributed (IID) source, and its entropy rate becomes the familiar entropy of a single outcome.", "problem": "Consider a simplified model for a random text generator that produces a sequence of characters from the alphabet $\\{X, Y, Z\\}$. The generator is designed as a first-order stationary Markov source. However, due to a particular design choice, the probability of generating the next character is actually independent of the current character in the sequence.\n\nSpecifically, the probability of generating an 'X' is $p_X$, the probability of generating a 'Y' is $p_Y$, and the probability of generating a 'Z' is $p_Z$, regardless of the preceding character. The values $p_X$, $p_Y$, and $p_Z$ are positive constants that sum to one: $p_X + p_Y + p_Z = 1$.\n\nAssuming this process has started long ago and has reached a steady state, determine the entropy rate of this source. Express your answer as an analytic expression in terms of $p_X$, $p_Y$, and $p_Z$, using the base-2 logarithm.", "solution": "The problem asks for the entropy rate of a stationary first-order Markov source. Let the set of states be $S = \\{X, Y, Z\\}$.\n\nFirst, we need to define the transition probability matrix $P$ for this Markov source. The entry $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$. Based on the problem description, the probability of the next character is independent of the current character. This means that each row of the transition matrix is identical.\n\nThe rows of the matrix represent the current state (X, Y, or Z), and the columns represent the next state (X, Y, or Z). The transition matrix $P$ is therefore:\n$$\nP = \\begin{pmatrix}\nP(X_{n+1}=X | X_n=X) & P(X_{n+1}=Y | X_n=X) & P(X_{n+1}=Z | X_n=X) \\\\\nP(X_{n+1}=X | X_n=Y) & P(X_{n+1}=Y | X_n=Y) & P(X_{n+1}=Z | X_n=Y) \\\\\nP(X_{n+1}=X | X_n=Z) & P(X_{n+1}=Y | X_n=Z) & P(X_{n+1}=Z | X_n=Z)\n\\end{pmatrix}\n$$\nGiven the description, these probabilities are $p_X, p_Y, p_Z$ respectively, regardless of the current state. So, the matrix becomes:\n$$\nP = \\begin{pmatrix}\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z\n\\end{pmatrix}\n$$\n\nThe entropy rate $H(\\mathcal{X})$ of a stationary Markov source is given by the formula:\n$$\nH(\\mathcal{X}) = \\sum_{i \\in S} \\pi_i H(P_{i,\\cdot})\n$$\nwhere $\\boldsymbol{\\pi} = (\\pi_X, \\pi_Y, \\pi_Z)$ is the stationary distribution of the Markov chain, and $H(P_{i,\\cdot})$ is the entropy of the probability distribution given by the $i$-th row of the transition matrix $P$.\n\nFirst, we find the stationary distribution $\\boldsymbol{\\pi}$ by solving the equation $\\boldsymbol{\\pi} P = \\boldsymbol{\\pi}$, subject to the constraint $\\pi_X + \\pi_Y + \\pi_Z = 1$.\nThe equation $\\boldsymbol{\\pi} P = \\boldsymbol{\\pi}$ expands to:\n$$\n(\\pi_X, \\pi_Y, \\pi_Z)\n\\begin{pmatrix}\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z\n\\end{pmatrix}\n= (\\pi_X, \\pi_Y, \\pi_Z)\n$$\nThis gives us a system of linear equations:\n1.  $\\pi_X p_X + \\pi_Y p_X + \\pi_Z p_X = \\pi_X$\n2.  $\\pi_X p_Y + \\pi_Y p_Y + \\pi_Z p_Y = \\pi_Y$\n3.  $\\pi_X p_Z + \\pi_Y p_Z + \\pi_Z p_Z = \\pi_Z$\n\nLet's analyze the first equation:\n$(\\pi_X + \\pi_Y + \\pi_Z) p_X = \\pi_X$\nSince $\\pi_X + \\pi_Y + \\pi_Z = 1$, this simplifies to $p_X = \\pi_X$.\nSimilarly, from the second and third equations, we get $p_Y = \\pi_Y$ and $p_Z = \\pi_Z$.\nThus, the stationary distribution is $\\boldsymbol{\\pi} = (p_X, p_Y, p_Z)$. This is consistent with the constraint $\\sum \\pi_i = p_X + p_Y + p_Z = 1$.\n\nNext, we calculate the entropy of each row of the transition matrix. Let $H_i = H(P_{i,\\cdot}) = -\\sum_{j \\in S} P_{ij} \\log_2(P_{ij})$.\nSince all rows of $P$ are identical, the entropy of each row is the same. Let's calculate the entropy for the first row (state X):\n$$\nH_X = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z))\n$$\nSimilarly, for the second and third rows:\n$$\nH_Y = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z)) = H_X\n$$\n$$\nH_Z = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z)) = H_X\n$$\n\nFinally, we calculate the entropy rate $H(\\mathcal{X})$:\n$$\nH(\\mathcal{X}) = \\pi_X H_X + \\pi_Y H_Y + \\pi_Z H_Z\n$$\nSubstituting the values we found for $\\boldsymbol{\\pi}$ and the row entropies:\n$$\nH(\\mathcal{X}) = p_X H_X + p_Y H_X + p_Z H_X = (p_X + p_Y + p_Z) H_X\n$$\nSince $p_X + p_Y + p_Z = 1$, we have:\n$$\nH(\\mathcal{X}) = 1 \\cdot H_X = H_X\n$$\nSo, the entropy rate of the source is simply the entropy of the common row vector.\n$$\nH(\\mathcal{X}) = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z))\n$$\nThis result is expected, as a Markov source whose next state is independent of the current state is equivalent to a memoryless or Independent and Identically Distributed (IID) source. The entropy rate of an IID source is simply the entropy of a single symbol's probability distribution.", "answer": "$$\\boxed{-(p_X \\log_{2}(p_X) + p_Y \\log_{2}(p_Y) + p_Z \\log_{2}(p_Z))}$$", "id": "1621359"}]}