{"hands_on_practices": [{"introduction": "This first exercise provides a complete walkthrough of generating a canonical Huffman code from the ground up. Starting with the probabilities of different symbols, you will first determine the optimal codelengths using the standard Huffman algorithm. This problem [@problem_id:1607377] is fundamental because it connects the statistical properties of the source data directly to the final, structured, and efficient codebook used for compression.", "problem": "An automated astronomical observatory classifies transient celestial events into four categories based on their observed properties. After a long observation period, the frequencies of these events have been established:\n- Type A: 50%\n- Type B: 25%\n- Type C: 12.5%\n- Type D: 12.5%\n\nTo compress the data stream of these classifications for efficient storage, a canonical Huffman code is to be constructed. The process for ordering symbols to assign codewords requires that for any symbols with the same probability (and therefore the same codelength), the assignment follows alphabetical order (e.g., 'C' is assigned a codeword before 'D').\n\nWhich of the following sets of codewords represents the correct canonical Huffman code for these event types?\n\nA. Type A: 0, Type B: 10, Type C: 110, Type D: 111\n\nB. Type A: 1, Type B: 01, Type C: 001, Type D: 000\n\nC. Type A: 0, Type B: 11, Type C: 100, Type D: 101\n\nD. Type A: 0, Type B: 10, Type C: 111, Type D: 110\n\nE. Type A: 0, Type B: 100, Type C: 101, Type D: 111", "solution": "Let the event probabilities be $p(A)=\\frac{1}{2}$, $p(B)=\\frac{1}{4}$, $p(C)=\\frac{1}{8}$, and $p(D)=\\frac{1}{8}$. The Huffman procedure combines the two least probable symbols at each step:\n\n1. Combine $C$ and $D$: $\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}$, yielding a node of probability $\\frac{1}{4}$.\n2. Now the multiset is $\\left\\{\\frac{1}{2},\\frac{1}{4},\\frac{1}{4}\\right\\}$. Combine the two $\\frac{1}{4}$ nodes to get $\\frac{1}{2}$.\n3. Combine the two $\\frac{1}{2}$ nodes to complete the tree.\n\nFrom this, the codeword lengths are determined by depths in the tree: $l(A)=1$, $l(B)=2$, $l(C)=3$, and $l(D)=3$.\n\nTo form the canonical Huffman code, order symbols by increasing length and, for equal lengths, alphabetically: $(A,1)$, $(B,2)$, $(C,3)$, $(D,3)$. Assign canonical codes by starting with code value $0$ at the shortest length and incrementing in binary, left-shifting when moving to longer lengths:\n\n- Assign to $A$ the length-$1$ code $0$.\n- Increment the code value to $1$ and left-shift by $1$ to move to length $2$: $1 \\mapsto 10_{2}$, assign $B \\to 10$.\n- Increment to $11_{2}$ and left-shift by $1$ to move to length $3$: $11_{2} \\mapsto 110_{2}$, assign $C \\to 110$.\n- Increment to $111_{2}$ at the same length, assign $D \\to 111$.\n\nThus the canonical code is $A:0$, $B:10$, $C:110$, $D:111$, which matches option A. Options C, D, and E have either incorrect lengths or violate the alphabetical tie-breaking within the same length; option B assigns the shortest code $1$ instead of $0$, which is not canonical.", "answer": "$$\\boxed{A}$$", "id": "1607377"}, {"introduction": "In many practical scenarios, you might encounter a prefix code that is functional but not 'canonical,' making the decoder more complex than necessary. This exercise [@problem_id:1607358] challenges you to perform a crucial standardization task: converting a given non-canonical codebook into its equivalent canonical form. The key insight you'll practice here is that only the lengths of the original codewords matter, not their specific binary patterns.", "problem": "In the design of a data compression scheme for a Lightweight Telemetry Protocol (LTP) used in Internet of Things (IoT) devices, a set of five common status messages, denoted by the symbols {A, B, C, D, E}, are encoded using a prefix code. The engineering team's current prototype uses the following codebook:\n- A: 000\n- B: 001\n- C: 010\n- D: 10\n- E: 11\n\nWhile this code is a valid prefix code, it does not conform to the protocol's requirement for a *canonical Huffman code*, which simplifies decoder implementation. To create a canonical code, a standard procedure must be followed:\n1.  The symbols are first sorted by their codeword length in ascending order.\n2.  Any symbols having the same codeword length are then sorted alphabetically.\n3.  The first symbol in the sorted list is assigned an all-zero codeword of the appropriate length.\n4.  Each subsequent symbol's codeword is generated by taking the previous symbol's codeword, adding 1 to it (as if it were a binary integer), and then appending zero-bits to the right (left-shifting) until the codeword has the required length.\n\nYour task is to convert the given non-canonical codebook into its equivalent canonical Huffman codebook based on the lengths from the prototype and the specified procedure. Which of the following represents the correct canonical codebook?\n\nA. A: 100, B: 101, C: 110, D: 00, E: 01\n\nB. A: 000, B: 001, C: 010, D: 10, E: 11\n\nC. A: 110, B: 101, C: 100, D: 01, E: 00\n\nD. A: 010, B: 011, C: 100, D: 00, E: 01\n\nE. A: 100, B: 110, C: 111, D: 00, E: 01", "solution": "We first extract the codeword lengths from the given prototype, preserving only lengths as required for constructing a canonical Huffman code:\n- $A$: length $3$\n- $B$: length $3$\n- $C$: length $3$\n- $D$: length $2$\n- $E$: length $2$\n\nFollowing the canonical construction procedure:\n1. Sort symbols by increasing codeword length; break ties alphabetically. This yields the order: $D$ (2), $E$ (2), $A$ (3), $B$ (3), $C$ (3).\n\n2. Assign codewords according to the rule: the first symbol gets an all-zero codeword of its length; each subsequent codeword is obtained by adding $1$ (as a binary integer) to the previous codeword and then appending zeros to the right until the required length is reached.\n\n- For $D$ (length $2$): assign the all-zero codeword of length $2$, which is $00$.\n- For $E$ (length $2$): take the previous codeword $00$, add $1$ to get $01$; the length is already $2$, so assign $01$.\n- For $A$ (length $3$): take the previous codeword $01$, add $1$ to get $10$, then append zeros until length $3$ is reached, giving $100$.\n- For $B$ (length $3$): take the previous codeword $100$, add $1$ to get $101$; length is already $3$, so assign $101$.\n- For $C$ (length $3$): take the previous codeword $101$, add $1$ to get $110$; length is already $3$, so assign $110$.\n\nThus, the canonical codebook is:\n- $A: 100$\n- $B: 101$\n- $C: 110$\n- $D: 00$\n- $E: 01$\n\nComparing with the options, this matches option A.", "answer": "$$\\boxed{A}$$", "id": "1607358"}, {"introduction": "After mastering the mechanics of creating canonical codes, it's important to understand why they are so efficient for representing compression schemes. This problem [@problem_id:1607368] is a conceptual check that shifts focus from calculation to principle, asking you to identify the minimum information needed to reconstruct a codebook. Successfully answering this question demonstrates a deep understanding of what makes canonical Huffman codes a compact and powerful tool in data compression.", "problem": "In the field of data compression, a canonical Huffman code is a special type of prefix code that can be described in a very compact way, saving space when transmitting the codebook itself. The construction of a canonical codebook relies on a standard procedure: first, symbols are sorted primarily by their assigned codelengths (from shortest to longest) and secondarily by their value (e.g., alphabetically or by ASCII value). The first symbol in this sorted list is assigned a codeword of all zeros, with the appropriate length. Each subsequent codeword is derived by taking the previous codeword, adding one to its integer value, and then appending zero-bits as needed to lengthen it for the next symbol in the sorted list.\n\nA software developer is designing a system where a server compresses data and sends it to a client. To allow the client to decompress the data, the server must also send the information needed to reconstruct the canonical Huffman codebook. The developer claims that to save bandwidth, the server only needs to transmit two integers to the client: the minimum codelength ($L_{min}$) and the maximum codelength ($L_{max}$) present in the code.\n\nAnother engineer argues this is insufficient for the client to uniquely reconstruct the codebook. What single piece of information, in addition to $L_{min}$ and $L_{max}$, is fundamentally necessary for the reconstruction?\n\nA. The total number of symbols in the source alphabet.\n\nB. The list of how many codewords exist for each codelength from $L_{min}$ to $L_{max}$.\n\nC. The original probability of occurrence for each symbol in the source data.\n\nD. The specific binary string representing the codeword for the most probable symbol.\n\nE. The average codelength of the entire code.", "solution": "Canonical Huffman codes are determined entirely by the multiset of codeword lengths, not by the exact probabilities. Let the alphabet be known and totally ordered (e.g., by ASCII). Let $L_{min}$ and $L_{max}$ be the smallest and largest codeword lengths, and let $n_{\\ell}$ denote the number of codewords of length $\\ell$ for each integer $\\ell$ with $L_{min} \\le \\ell \\le L_{max}$. The multiset of lengths is equivalently represented by the list $\\{n_{\\ell}\\}_{\\ell=L_{min}}^{L_{max}}$.\n\nThe fundamental property used is that canonical assignment depends only on the lengths and the fixed symbol order: when symbols are sorted by nondecreasing length and then by symbol value, their codewords are assigned by taking the previous codeword as a binary integer, adding $1$, and, when moving to a longer length, appending zero-bits as needed.\n\nFormally, let the occupied lengths be $L_{1} < L_{2} < \\cdots < L_{K}$ where $n_{L_{k}} > 0$. Define the first code value (as an integer) at the shortest length by\n$$\nv_{L_{1}} = 0,\n$$\nand assign the $n_{L_{1}}$ codewords of length $L_{1}$ as the integers\n$$\nv_{L_{1}},\\ v_{L_{1}}+1,\\ \\ldots,\\ v_{L_{1}} + n_{L_{1}} - 1\n$$\nwritten in binary with exactly $L_{1}$ bits. Inductively, for each subsequent occupied length $L_{k} > L_{k-1}$, the first code value is\n$$\nv_{L_{k}} = \\left(v_{L_{k-1}} + n_{L_{k-1}}\\right) 2^{L_{k} - L_{k-1}},\n$$\nand the $n_{L_{k}}$ codewords of length $L_{k}$ are\n$$\nv_{L_{k}},\\ v_{L_{k}}+1,\\ \\ldots,\\ v_{L_{k}} + n_{L_{k}} - 1\n$$\nwritten in binary with exactly $L_{k}$ bits. This recurrence precisely encodes the rule “add one to the previous codeword’s integer value and append zeros when increasing length.” Hence, given the counts $\\{n_{\\ell}\\}$ and the known symbol order, the canonical codebook is uniquely determined.\n\nBy contrast, knowing only $L_{min}$ and $L_{max}$ is insufficient: there are many different sequences $\\{n_{\\ell}\\}$ consistent with those bounds. The only necessary and sufficient aggregate constraint on the counts for a complete prefix code is the Kraft equality\n$$\n\\sum_{\\ell=L_{min}}^{L_{max}} n_{\\ell} 2^{-\\ell} = 1,\n$$\nbut this has infinitely many solutions for $\\{n_{\\ell}\\}$ with the same $L_{min}$ and $L_{max}$. For example, choosing $n_{L_{min}} = 1$ forces $n_{L_{max}} = 2^{L_{max}} - 2^{L_{max} - L_{min}}$ to satisfy the Kraft equality when only two lengths are used, yet many other distributions across intermediate lengths also satisfy the equality. Therefore, $L_{min}$ and $L_{max}$ do not uniquely determine the codebook.\n\nAmong the options, the single additional piece of information that is fundamentally necessary is the list of how many codewords exist for each codelength from $L_{min}$ to $L_{max}$, namely $\\{n_{\\ell}\\}_{\\ell=L_{min}}^{L_{max}}$. With that list and the known symbol order, the canonical codebook is uniquely reconstructible. This corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1607368"}]}