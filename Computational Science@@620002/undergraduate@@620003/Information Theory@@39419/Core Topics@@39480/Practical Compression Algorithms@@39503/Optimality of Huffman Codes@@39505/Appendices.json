{"hands_on_practices": [{"introduction": "The Huffman coding algorithm provides an elegant, optimal solution for data compression by building a prefix-free code from the ground up. Its power lies in a simple, iterative greedy strategy: at each stage, the two least probable symbols (or symbol groups) are merged into a new, combined symbol. This foundational exercise [@problem_id:1644372] allows you to practice this essential first step, which is the building block for constructing the entire optimal coding tree.", "problem": "A remote weather station transmits data packets representing the current atmospheric condition. The data source has an alphabet of five distinct symbols, each with a known probability of occurrence: 'Sunny' ($p_S = 0.40$), 'Cloudy' ($p_C = 0.25$), 'Rainy' ($p_R = 0.15$), 'Windy' ($p_W = 0.12$), and 'Foggy' ($p_F = 0.08$).\n\nTo optimize the data transmission by minimizing the average code length, a Huffman coding scheme is to be constructed. The Huffman algorithm is a greedy procedure that iteratively builds a binary tree by merging the two least probable symbols at each step.\n\nWhat is the set of probabilities corresponding to the reduced source alphabet after the very first merge step of the Huffman algorithm is performed?\n\nA. $\\{0.40, 0.25, 0.20, 0.15\\}$\n\nB. $\\{0.65, 0.15, 0.12, 0.08\\}$\n\nC. $\\{0.48, 0.25, 0.15, 0.12\\}$\n\nD. $\\{0.40, 0.25, 0.15, 0.10\\}$\n\nE. $\\{0.40, 0.25, 0.15, 0.12, 0.08\\}$", "solution": "Given the source with probabilities $p_{S}=0.40$, $p_{C}=0.25$, $p_{R}=0.15$, $p_{W}=0.12$, and $p_{F}=0.08$, the Huffman algorithm merges at each step the two least probable symbols. The two least probabilities are $0.08$ and $0.12$, corresponding to $p_{F}$ and $p_{W}$.\n\nBy the Huffman merge rule, the new composite symbol has probability equal to the sum of the merged probabilities:\n$$\np_{WF}=p_{W}+p_{F}=0.12+0.08=0.20.\n$$\nThe reduced source alphabet after this first merge contains the remaining original probabilities and the new composite probability:\n$$\n\\{p_{S},p_{C},p_{R},p_{WF}\\}=\\{0.40,0.25,0.15,0.20\\}.\n$$\nThis set matches option A (order within a set is irrelevant). Therefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1644372"}, {"introduction": "Once you understand the basic merge operation, the next step is to apply it recursively to construct a complete Huffman tree and determine the resulting code's efficiency. This practice [@problem_id:1623278] requires you to perform the full Huffman algorithm for a given probability distribution. By calculating the minimum possible average code length, $L$, you will solidify your understanding of how the algorithm as a whole achieves optimal compression for a prefix code.", "problem": "A deep-space probe is designed to monitor and classify five distinct types of cosmic ray events, labeled A, B, C, D, and E. The probe operates autonomously and, due to power limitations for its transmitter, must encode the classification of each detected event into a sequence of binary digits (bits) using the most efficient scheme possible. From extensive prior observations, the long-term probabilities for the detection of each event type within a standard observation window have been determined. Events A and B each occur with a probability of $1/3$. Events C, D, and E each occur with a probability of $1/9$.\n\nAssuming the goal is to minimize the average number of bits transmitted per event, what is this minimum possible average length? Express your answer as a single fraction.", "solution": "We model the problem as constructing a binary prefix code to minimize the expected codeword length. Let the symbol probabilities be $p_{A}=p_{B}=\\frac{1}{3}$ and $p_{C}=p_{D}=p_{E}=\\frac{1}{9}$. For optimality among binary prefix codes, we apply Huffman coding.\n\nStep-by-step Huffman merges using the smallest probabilities:\n1. Merge two $\\frac{1}{9}$ symbols to form a node of weight $\\frac{2}{9}$. This increases the codeword lengths of those two symbols by $1$.\n2. Merge $\\frac{2}{9}$ with the remaining $\\frac{1}{9}$ to form a node of weight $\\frac{3}{9}=\\frac{1}{3}$. This increases the codeword lengths of the three $\\frac{1}{9}$ symbols by $1$ (cumulative so far: the first two have increased by $2$, the last by $1$).\n3. Now the multiset of weights is $\\left\\{\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right\\}$. Merge any two $\\frac{1}{3}$ to get a node of weight $\\frac{2}{3}$. This increases the codeword lengths of those two symbols by $1$.\n4. Merge $\\frac{2}{3}$ with the remaining $\\frac{1}{3}$ to form the root. This increases the codeword lengths of all three symbols under these nodes by $1$.\n\nTracing depths (codeword lengths):\n- Symbols $A$ and $B$ (both with probability $\\frac{1}{3}$) are merged together in step 3 and then merged with the other $\\frac{1}{3}$ node in step 4, so $l_{A}=l_{B}=2$.\n- The symbol among $\\{C,D,E\\}$ that did not participate in the first merge (call it $E$) is merged in steps 2 and 4, so $l_{E}=2$.\n- The two symbols that were merged first (call them $C$ and $D$) are merged in steps 1, 2, and 4, so $l_{C}=l_{D}=3$.\n\nThus the expected codeword length is\n$$\nL=\\frac{1}{3}\\cdot l_{A}+\\frac{1}{3}\\cdot l_{B}+\\frac{1}{9}\\cdot l_{C}+\\frac{1}{9}\\cdot l_{D}+\\frac{1}{9}\\cdot l_{E}\n= \\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 2.\n$$\nCompute:\n$$\nL=\\frac{2}{3}+\\frac{2}{3}+\\frac{3}{9}+\\frac{3}{9}+\\frac{2}{9}=\\frac{4}{3}+\\frac{8}{9}=\\frac{12}{9}+\\frac{8}{9}=\\frac{20}{9}.\n$$\nBy the optimality of Huffman coding, this is the minimum possible average number of bits per event. As a consistency check, the source entropy in bits is\n$$\nH=-2\\cdot \\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)-3\\cdot \\frac{1}{9}\\log_{2}\\!\\left(\\frac{1}{9}\\right)=\\frac{4}{3}\\log_{2}(3),\n$$\nand indeed $H \\leq \\frac{20}{9}$, consistent with the coding bound.", "answer": "$$\\boxed{\\frac{20}{9}}$$", "id": "1623278"}, {"introduction": "The proof of Huffman's optimality rests on the critical insight that the two least probable symbols should be assigned the longest codewords and be siblings in the coding tree. This exercise [@problem_id:1644338] puts that principle to the test by exploring a hypothetical scenario where an engineer's faulty algorithm deviates from this crucial first step. By comparing the average length of the resulting suboptimal code with the true Huffman code, you'll gain a concrete understanding of why the specific greedy choice made by the Huffman algorithm is essential for achieving optimality.", "problem": "An engineer is designing a data compression scheme for a communication system that transmits data from a simple four-symbol source. The source alphabet is $\\mathcal{A} = \\{A, B, C, D\\}$, and the symbols are generated independently with the following probabilities: $P(A) = 0.4$, $P(B) = 0.3$, $P(C) = 0.16$, and $P(D) = 0.14$.\n\nThe engineer intends to use the optimal Huffman coding algorithm to create a binary prefix code, which we will call Code H. However, during the implementation, a bug is introduced. This bug alters the first step of the Huffman algorithm. Instead of combining the two symbols with the lowest probabilities, the faulty algorithm (which generates a code we'll call Code M) combines the symbol with the *second-lowest probability* and the symbol with the *third-lowest probability*. After this incorrect first step, the rest of the faulty algorithm proceeds correctly by always combining the two nodes with the lowest current probabilities until a single root node is formed.\n\nYour task is to quantify the inefficiency introduced by this bug. Calculate the difference between the average codeword length of the mistaken code, $L_M$, and the average codeword length of the correct Huffman code, $L_H$.\n\nExpress the value of $L_M - L_H$ in bits/symbol, rounded to three significant figures.", "solution": "We are given a memoryless source with alphabet $\\mathcal{A}=\\{A,B,C,D\\}$ and probabilities $P(A)=0.4$, $P(B)=0.3$, $P(C)=0.16$, $P(D)=0.14$. The average codeword length for a code with lengths $\\{l(A),l(B),l(C),l(D)\\}$ is\n$$\nL=\\sum_{x\\in\\mathcal{A}} P(x)\\,l(x).\n$$\n\nCorrect Huffman code (Code H):\n- Start by combining the two smallest probabilities: $0.14$ and $0.16$ combine to form a node of weight $0.30$.\n- Now the multiset of weights is $\\{0.30,0.30,0.40\\}$. Combine the two smallest: the two $0.30$ nodes combine to weight $0.60$.\n- Finally combine $0.40$ and $0.60$ to form the root.\n\nThe resulting codeword lengths are:\n- $l(A)=1$ (it joins only at the last merge),\n- $l(B)=2$ (it is merged at the second step and then at the root),\n- $l(C)=3$, $l(D)=3$ (they are merged first, then at the second step, then at the root).\n\nThus,\n$$\nL_{H}=0.4\\cdot 1+0.3\\cdot 2+0.16\\cdot 3+0.14\\cdot 3=0.4+0.6+0.48+0.42=1.9.\n$$\n\nFaulty code (Code M):\n- The bug combines the second-lowest and third-lowest probabilities first: $0.16$ and $0.30$ combine to form $0.46$.\n- Now the multiset is $\\{0.14,0.40,0.46\\}$. Proceeding correctly, combine the two smallest: $0.14$ and $0.40$ to form $0.54$.\n- Finally combine $0.46$ and $0.54$ to form the root.\n\nThe resulting codeword lengths are:\n- $l(B)=2$ and $l(C)=2$ (merged in the first step and at the root),\n- $l(D)=2$ and $l(A)=2$ (merged in the second step and at the root).\n\nThus,\n$$\nL_{M}=0.4\\cdot 2+0.3\\cdot 2+0.16\\cdot 2+0.14\\cdot 2=2.0.\n$$\n\nTherefore, the inefficiency introduced is\n$$\nL_{M}-L_{H}=2.0-1.9=0.1,\n$$\nwhich, rounded to three significant figures, is $0.100$ bits per symbol.", "answer": "$$\\boxed{0.100}$$", "id": "1644338"}]}