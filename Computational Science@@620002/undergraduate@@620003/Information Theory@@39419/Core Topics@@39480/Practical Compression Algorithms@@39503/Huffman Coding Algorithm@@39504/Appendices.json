{"hands_on_practices": [{"introduction": "The fundamental goal of Huffman coding is to create an optimal prefix-free code that minimizes the average number of bits required to represent symbols from a source. This first exercise provides a classic, hands-on application of the Huffman algorithm, starting from a sample data stream. By first calculating the empirical probabilities of symbols and then systematically constructing the Huffman tree, you will determine the theoretical minimum average codeword length for the given source. This practice solidifies the core mechanics of the algorithm and is an essential first step in mastering lossless data compression. [@problem_id:1630312]", "problem": "A deep-space probe uses a simplified communication protocol for transmitting its scientific data. The protocol is based on a source alphabet consisting of five distinct symbols: $\\mathcal{S} = \\{A, B, C, D, E\\}$. To design an efficient data compression scheme, a mission analyst studies a representative sample of telemetry data containing a sequence of 12 symbols:\n$$A, B, A, C, A, B, D, A, E, C, B, D$$\nAssuming this sample is statistically representative of the source's output, determine the theoretical minimum average number of binary digits (bits) required to encode one symbol using an optimal symbol-by-symbol, variable-length binary encoding scheme.\n\nExpress your answer as an exact fraction.", "solution": "First, we compute the empirical probabilities of the symbols from the 12-symbol sample:\n- Counts: $N_A=4$, $N_B=3$, $N_C=2$, $N_D=2$, $N_E=1$.\n- Probabilities: $p_A=\\frac{4}{12}$, $p_B=\\frac{3}{12}$, $p_C=\\frac{2}{12}$, $p_D=\\frac{2}{12}$, $p_E=\\frac{1}{12}$.\n\nTo find the minimum average code length, we apply the Huffman algorithm. We will build the tree by repeatedly merging the two nodes with the lowest probabilities.\n1.  The initial probabilities are $\\{p_E=\\frac{1}{12}, p_C=\\frac{2}{12}, p_D=\\frac{2}{12}, p_B=\\frac{3}{12}, p_A=\\frac{4}{12}\\}$.\n2.  **Step 1:** Merge the two smallest, $p_E$ and $p_C$. Create a new node with probability $p_{EC} = \\frac{1}{12} + \\frac{2}{12} = \\frac{3}{12}$. The list of active nodes becomes $\\{p_D=\\frac{2}{12}, p_B=\\frac{3}{12}, p_{EC}=\\frac{3}{12}, p_A=\\frac{4}{12}\\}$.\n3.  **Step 2:** Merge the two smallest, $p_D$ and $p_B$. Create a new node with probability $p_{DB} = \\frac{2}{12} + \\frac{3}{12} = \\frac{5}{12}$. The list becomes $\\{p_{EC}=\\frac{3}{12}, p_A=\\frac{4}{12}, p_{DB}=\\frac{5}{12}\\}$.\n4.  **Step 3:** Merge the two smallest, $p_{EC}$ and $p_A$. Create a new node with probability $p_{ECA} = \\frac{3}{12} + \\frac{4}{12} = \\frac{7}{12}$. The list becomes $\\{p_{DB}=\\frac{5}{12}, p_{ECA}=\\frac{7}{12}\\}$.\n5.  **Step 4:** Merge the final two nodes to form the root.\n\nBy tracing the paths from the root to each leaf in the resulting tree, we find the codeword lengths ($l_s$):\n- $l_A = 2$\n- $l_B = 2$\n- $l_D = 2$\n- $l_C = 3$\n- $l_E = 3$\n\nNote that symbols C and D have the same probability ($\\frac{2}{12}$), but their codeword lengths are different (3 and 2, respectively). This is a valid outcome of the Huffman algorithm that depends on the tie-breaking choices made during construction. The resulting code is still optimal.\n\nThe minimum average length $\\bar{L}$ is calculated as the sum of each symbol's probability multiplied by its codeword length:\n$$\n\\bar{L}\n= \\sum_{s \\in \\{A,B,C,D,E\\}} p_{s}\\,l_{s}\n= \\frac{4}{12}\\cdot 2 + \\frac{3}{12}\\cdot 2 + \\frac{2}{12}\\cdot 3 + \\frac{2}{12}\\cdot 2 + \\frac{1}{12}\\cdot 3\n= \\frac{8+6+6+4+3}{12}\n= \\frac{27}{12}\n= \\frac{9}{4}.\n$$\nTherefore, the theoretical minimum average number of bits per symbol is $\\frac{9}{4}$.", "answer": "$$\\boxed{\\frac{9}{4}}$$", "id": "1630312"}, {"introduction": "While mechanically executing the Huffman algorithm is a key skill, developing an intuition for its behavior in special cases leads to a deeper understanding. This problem investigates a scenario where one symbol's probability of occurrence is greater than $0.5$, making it more likely than all other symbols combined. By analyzing the greedy, bottom-up merging process of the algorithm in this context, you can deduce a crucial property of the resulting code without performing the full construction. This exercise demonstrates how understanding the algorithm's core principles can lead to powerful predictive insights. [@problem_id:1630300]", "problem": "A deep-space probe is analyzing the atmospheric composition of a newly discovered exoplanet. The probe transmits data as a stream of symbols, where each symbol represents a specific molecule detected. To conserve power, the probe uses an optimal prefix-free binary encoding scheme, specifically the Huffman coding algorithm, based on the measured relative frequencies of the molecules.\n\nThe onboard spectrometer has determined the following probabilities for the five most common molecules in the atmosphere:\n- Water vapor ($\\text{H}_2\\text{O}$): $0.53$\n- Dinitrogen ($\\text{N}_2$): $0.21$\n- Methane ($\\text{CH}_4$): $0.11$\n- Carbon Dioxide ($\\text{CO}_2$): $0.09$\n- Argon ($\\text{Ar}$): $0.06$\n\nThe sum of these probabilities is $1.00$. All other molecules occur with negligible probability. Based on this probability distribution, determine the length of the binary codeword assigned to the symbol for Water vapor ($\\text{H}_2\\text{O}$).", "solution": "We use the Huffman coding algorithm: at each step, combine the two least probable symbols into a new node whose probability is their sum; repeat until one node remains. In the resulting tree, the codeword length of a symbol equals its depth (the number of edges from the root to its leaf).\n\nList the given probabilities in ascending order: $0.06, 0.09, 0.11, 0.21, 0.53$.\n\nCombine the two smallest at each step:\n$$0.06+0.09=0.15,$$\nso the multiset becomes $0.11, 0.15, 0.21, 0.53$.\n\nNext,\n$$0.11+0.15=0.26,$$\nso the multiset becomes $0.21, 0.26, 0.53$.\n\nNext,\n$$0.21+0.26=0.47,$$\nso the multiset becomes $0.47, 0.53$.\n\nFinally,\n$$0.47+0.53=1.00,$$\nyielding the root.\n\nThe symbol with probability $0.53$ is only merged at the final step, so it attaches directly to the root and has depth $1$. Therefore, the length of its binary codeword is $1$. As a consistency check, the full set of lengths produced by these merges is $1, 2, 3, 4, 4$, which satisfies the Kraft equality:\n$$2^{-1}+2^{-2}+2^{-3}+2^{-4}+2^{-4}=1.$$\nThus, the optimal prefix-free code assigns the most probable symbol a codeword of length $1$.", "answer": "$$\\boxed{1}$$", "id": "1630300"}, {"introduction": "A common point of confusion is whether the Huffman algorithm produces a single, unique code for a given source. This is not always the case; when ties in probability occur during the merging process, different choices can lead to different, yet equally optimal, Huffman codes. This exercise explores this non-uniqueness by having you construct two valid Huffman codes for the same source and introduces the concept of codeword length variance to compare them. This advanced practice highlights that while all Huffman codes for a source share the same minimal average length, other statistical properties, like the spread of codeword lengths, can differ based on the choices made during construction. [@problem_id:1630317]", "problem": "A discrete memoryless source emits symbols from the alphabet $\\mathcal{S} = \\{S_1, S_2, S_3, S_4, S_5, S_6\\}$. The probabilities of these symbols are given by the set $P = \\{0.3, 0.2, 0.2, 0.1, 0.1, 0.1\\}$, where $p(S_1)=0.3$, $p(S_2)=0.2$, $p(S_3)=0.2$, and $p(S_4)=p(S_5)=p(S_6)=0.1$.\n\nThe Huffman coding algorithm is used to generate an optimal prefix-free binary code for this source. The execution of the algorithm involves iteratively merging the two nodes (symbols or groups of symbols) with the lowest probabilities. When ties in probabilities occur, different choices can be made, potentially leading to different valid Huffman trees. While all resulting codes are optimal and share the same minimum average codeword length, the distribution of individual codeword lengths can differ.\n\nYour task is to determine the maximum possible variance of the codeword length, $L$, for an optimal code generated for this source. The variance is defined as $\\text{Var}(L) = \\sum_{i=1}^{6} p(S_i) (l_i - \\bar{L})^2 = E[L^2] - (E[L])^2$, where $l_i$ is the length of the codeword for symbol $S_i$, and $\\bar{L} = E[L]$ is the average codeword length.\n\nExpress your answer as a single decimal number, rounded to three significant figures.", "solution": "Let the source probabilities be $p_{1}=0.3$, $p_{2}=0.2$, $p_{3}=0.2$, $p_{4}=0.1$, $p_{5}=0.1$, $p_{6}=0.1$. The Huffman merge sequence by weights is fixed:\n- Merge two $0.1$ to $0.2$.\n- Merge $0.1$ with $0.2$ to $0.3$.\n- Merge $0.2$ with $0.2$ to $0.4$.\n- Merge $0.3$ with $0.3$ to $0.6$.\n- Merge $0.6$ with $0.4$ to $1$.\n\nTies allow two structurally different optimal assignments of codeword lengths.\n\nCase I (do not merge the initial $0.1+0.1$ pair with the remaining $0.1$ at the next step): At step 2, merge the remaining $0.1$ with a singleton $0.2$. The resulting lengths are\n- $l_{1}=2$ (for $p=0.3$),\n- one $p=0.2$ symbol gets $l=2$ and one gets $l=3$,\n- all three $p=0.1$ symbols get $l=3$.\nThus the length distribution is $L=2$ with total probability $0.5$ and $L=3$ with total probability $0.5$. Hence\n$$\nE[L]=2\\cdot 0.5+3\\cdot 0.5=2.5,\\quad\nE[L^2]=4\\cdot 0.5+9\\cdot 0.5=6.5,\n$$\nso\n$$\n\\text{Var}(L)=E[L^2]-(E[L])^{2}=6.5-(2.5)^{2}=0.25.\n$$\n\nCase II (merge the initial $0.1+0.1$ pair with the remaining $0.1$ at the next step): At step 2, merge the remaining $0.1$ with the $0.2$ node formed by two $0.1$â€™s. The resulting lengths are\n- $l_{1}=2$ (for $p=0.3$),\n- both $p=0.2$ symbols get $l=2$,\n- one $p=0.1$ symbol gets $l=3$,\n- the other two $p=0.1$ symbols get $l=4$.\nThus the length distribution is $L=2$ with probability $0.7$, $L=3$ with probability $0.1$, and $L=4$ with probability $0.2$. Hence\n$$\nE[L]=2\\cdot 0.7+3\\cdot 0.1+4\\cdot 0.2=2.5,\n$$\n$$\nE[L^2]=4\\cdot 0.7+9\\cdot 0.1+16\\cdot 0.2=6.9,\n$$\nso\n$$\n\\text{Var}(L)=E[L^2]-(E[L])^{2}=6.9-6.25=0.65.\n$$\n\nBoth cases are optimal with the same minimal average length $E[L]=2.5$. The variance is maximized in Case II. Therefore, the maximum possible variance is $0.65$, which to three significant figures is $0.650$.", "answer": "$$\\boxed{0.650}$$", "id": "1630317"}]}