{"hands_on_practices": [{"introduction": "The first step in mastering the concept of redundancy is to apply its fundamental definition. This exercise provides a clear, practical scenario involving a fixed-length code for a source with a non-uniform probability distribution. By calculating the difference between the average codeword length, $\\bar{L}$, and the source entropy, $H$, you will directly quantify the inefficiency inherent in using a simple code for a source with predictable patterns [@problem_id:1652803].", "problem": "A sensor in a smart city's traffic management system is designed to monitor a single traffic light. At any given moment, the light can be in one of three states: Red, Yellow, or Green. Based on extensive historical traffic data, the probability of the light being Red is $p_R = 0.5$, the probability of it being Yellow is $p_Y = 0.1$, and the probability of it being Green is $p_G = 0.4$. To transmit the state of the light, the system employs a fixed-length binary encoding scheme. Since there are three distinct states, each state is assigned a unique 2-bit codeword.\n\nCalculate the redundancy of this encoding scheme. Express your answer in bits per symbol, rounded to three significant figures.", "solution": "Redundancy in a fixed-length source code is the difference between the average codeword length and the source entropy (in bits per symbol). The entropy of a discrete source with probabilities $\\{p_{i}\\}$ is\n$$\nH = -\\sum_{i} p_{i} \\log_{2}(p_{i}).\n$$\nWith $p_{R} = 0.5$, $p_{Y} = 0.1$, and $p_{G} = 0.4$, the entropy is\n$$\nH = -\\left[0.5 \\log_{2}(0.5) + 0.1 \\log_{2}(0.1) + 0.4 \\log_{2}(0.4)\\right].\n$$\nUsing $\\log_{2}(0.5) = -1$, and evaluating the others numerically,\n$$\nH = -\\left[0.5(-1) + 0.1 \\log_{2}(0.1) + 0.4 \\log_{2}(0.4)\\right]\n= 0.5 - 0.1 \\log_{2}(0.1) - 0.4 \\log_{2}(0.4).\n$$\nWith $\\log_{2}(0.1) \\approx -3.321928094$ and $\\log_{2}(0.4) \\approx -1.321928095$,\n$$\nH \\approx 0.5 + 0.332192809 + 0.528771238 = 1.360964047 \\text{ bits/symbol}.\n$$\nA fixed-length 2-bit code has average length\n$$\n\\bar{L} = 2 \\text{ bits/symbol}.\n$$\nTherefore, the redundancy is\n$$\nR = \\bar{L} - H \\approx 2 - 1.360964047 = 0.639035953 \\text{ bits/symbol}.\n$$\nRounded to three significant figures, the redundancy is $0.639$ bits per symbol.", "answer": "$$\\boxed{0.639}$$", "id": "1652803"}, {"introduction": "Redundancy often arises from a mismatch between a code's design and the actual statistics of the information source. This problem [@problem_id:1652808] illustrates a scenario where a system designed for a broad range of possibilities now operates under much more constrained conditions. Working through this example will deepen your understanding of how a code that is optimal for one probability distribution can become highly redundant if the source behavior changes.", "problem": "A digital monitoring system is designed to transmit the state of a remote sensor. The sensor can be in one of 8 distinct states, which we can label as $s_1, s_2, \\dots, s_8$. The communication protocol uses a fixed-length binary code to represent these states. This code was designed under the initial assumption that all 8 states occur with equal probability.\n\nAfter deployment, a persistent partial failure in the sensor's circuitry alters its behavior. It is observed that the sensor now only ever reports state $s_1$ with a probability of $p_1 = 3/4$, or state $s_2$ with a probability of $p_2 = 1/4$. The other six states ($s_3$ through $s_8$) are never reported.\n\nGiven that the original fixed-length code is still in use, calculate the redundancy of this coding scheme with respect to the new, faulty probability distribution of the sensor.\n\nExpress your answer as a closed-form analytic expression in units of bits per symbol. Your expression may use logarithms, which should be specified in base 2.", "solution": "A fixed-length binary code for an alphabet of size 8 requires exactly $\\lceil \\log_{2}(8) \\rceil = 3$ bits per symbol. Therefore, the average codeword length remains\n$$\n\\bar{L} = 3 \\text{ bits/symbol}.\n$$\nWith the faulty sensor, the source distribution is $p_{1} = \\frac{3}{4}$, $p_{2} = \\frac{1}{4}$, and $p_{3}=\\cdots=p_{8}=0$. The Shannon entropy (in bits per symbol) is\n$$\nH = -\\sum_{i=1}^{8} p_{i} \\log_{2}(p_{i}) = -\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right) - \\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right),\n$$\nsince the zero-probability terms contribute $0$. Simplifying,\n$$\nH = -\\frac{3}{4}\\bigl(\\log_{2} 3 - \\log_{2} 4\\bigr) - \\frac{1}{4}\\bigl(\\log_{2} 1 - \\log_{2} 4\\bigr)\n= -\\frac{3}{4}\\log_{2} 3 + \\frac{3}{2} + \\frac{1}{2}\n= 2 - \\frac{3}{4}\\log_{2} 3.\n$$\nRedundancy is defined as the excess average code length over the source entropy:\n$$\nR = \\bar{L} - H = 3 - \\left(2 - \\frac{3}{4}\\log_{2} 3\\right) = 1 + \\frac{3}{4}\\log_{2} 3 \\text{ bits/symbol}.\n$$", "answer": "$$\\boxed{1+\\frac{3}{4}\\log_{2} 3}$$", "id": "1652808"}, {"introduction": "After learning to calculate redundancy, we can ask a more profound question: what conditions make a code most efficient? This problem [@problem_id:1652849] challenges you to think about this from an optimization perspective. By determining the source probability that minimizes redundancy for a simple binary code, you will uncover the fundamental relationship between redundancy and source entropy, revealing that inefficiency is lowest when uncertainty is highest.", "problem": "Consider a simple digital sensor that monitors an environmental condition. The sensor has two possible output states, which we will denote as '0' and '1'. This stream of outputs can be modeled as a Discrete Memoryless Source (DMS). For this source, the probability of emitting a '0' is $p$, and the probability of emitting a '1' is $1-p$, where $p$ is a value strictly between 0 and 1.\nThe output from this sensor is directly transmitted using a simple binary code where the source symbol '0' is encoded as the codeword '0' and the source symbol '1' is encoded as the codeword '1'.\n\nThe redundancy of a code for a source is a measure of its inefficiency. It is defined as the difference between the average codeword length per source symbol and the entropy of the source. For this problem, the entropy should be calculated in bits (using base-2 logarithm).\n\nDetermine the exact value of the probability $p$ that minimizes the redundancy of this encoding scheme.", "solution": "A binary discrete memoryless source with symbol probabilities $P(0)=p$ and $P(1)=1-p$ is encoded with a fixed-length binary code where each symbol maps to one bit. The average codeword length per source symbol is therefore\n$$\n\\bar{L}=1.\n$$\nThe source entropy in bits is\n$$\nH(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nThe redundancy is defined as\n$$\nR(p)=\\bar{L}-H(p)=1+p\\log_{2}(p)+(1-p)\\log_{2}(1-p).\n$$\nMinimizing $R(p)$ is equivalent to maximizing $H(p)$. Differentiate $H(p)$ with respect to $p$:\n$$\n\\frac{dH}{dp}=-\\left[\\log_{2}(p)+\\frac{1}{\\ln 2}\\right]+\\left[\\log_{2}(1-p)+\\frac{1}{\\ln 2}\\right]\n=\\log_{2}(1-p)-\\log_{2}(p)=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right).\n$$\nSet the derivative to zero to find critical points:\n$$\n\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=0 \\;\\;\\Longrightarrow\\;\\; \\frac{1-p}{p}=1 \\;\\;\\Longrightarrow\\;\\; p=\\frac{1}{2}.\n$$\nCheck the second derivative to confirm a maximum of $H(p)$ (and thus a minimum of $R(p)$):\n$$\n\\frac{d^{2}H}{dp^{2}}=-\\frac{1}{(1-p)\\ln 2}-\\frac{1}{p\\ln 2}<0 \\quad \\text{for } p\\in(0,1),\n$$\nso $p=\\frac{1}{2}$ maximizes $H(p)$ and therefore minimizes $R(p)$. Hence, the redundancy is minimized at $p=\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1652849"}]}