{"hands_on_practices": [{"introduction": "While Huffman's algorithm guarantees a code with the minimum possible average length, the construction process itself is not always unique. This exercise delves into a scenario with probability ties, which allows for multiple, equally optimal codes to be constructed. By building two different Huffman trees for the same source, you will explore how structural differences can impact the variance of the codeword lengths, revealing that \"optimal\" does not always mean \"identical\" in every statistical measure [@problem_id:1659058].", "problem": "Consider a discrete memoryless source that generates symbols from the alphabet $\\mathcal{S} = \\{\\mathcal{A}, \\mathcal{B}, \\mathcal{C}, \\mathcal{D}\\}$. The probabilities of these symbols are given by $P(\\mathcal{A}) = 0.4$, $P(\\mathcal{B}) = 0.2$, $P(\\mathcal{C}) = 0.2$, and $P(\\mathcal{D}) = 0.2$.\n\nThe Huffman coding algorithm can be used to generate an optimal binary prefix code for this source. However, due to the specific probabilities, tie-breaking choices during the construction process can lead to the creation of at least two different optimal codes, which are distinguished by having different sets of codeword lengths.\n\nLet $L$ be a random variable representing the length of the codeword for a symbol drawn from the source. The value of $L$ for a symbol $s_i \\in \\mathcal{S}$ is its codeword length, $l_i$, and it occurs with probability $p_i = P(s_i)$. The variance of the codeword length is defined as $\\sigma^2 = E[L^2] - (E[L])^2$.\n\nYour task is to construct two different optimal Huffman codes, which we will call Code 1 and Code 2, such that the multiset of codeword lengths for Code 1, $\\{l_{1,\\mathcal{A}}, l_{1,\\mathcal{B}}, l_{1,\\mathcal{C}}, l_{1,\\mathcal{D}}\\}$, is different from the multiset of codeword lengths for Code 2, $\\{l_{2,\\mathcal{A}}, l_{2,\\mathcal{B}}, l_{2,\\mathcal{C}}, l_{2,\\mathcal{D}}\\}$.\n\nCalculate the variance of the codeword length for each of these two codes, denoted as $\\sigma_1^2$ and $\\sigma_2^2$. Then, compute the absolute difference between these two variances, $|\\sigma_1^2 - \\sigma_2^2|$.\n\nRound your final answer to two significant figures.", "solution": "The source has probabilities $p_{\\mathcal{A}}=0.4$ and $p_{\\mathcal{B}}=p_{\\mathcal{C}}=p_{\\mathcal{D}}=0.2$. When applying the Huffman algorithm, a tie-breaking choice must be made that leads to two different tree structures.\n\n**Construct Code 1**: After an initial merge of two 0.2-probability symbols, the probability pool becomes {0.4, 0.4, 0.2}. A tie-breaking choice is made to merge the 0.2-probability symbol with the original 0.4-probability symbol ($\\mathcal{A}$). The final merge combines the two remaining nodes. This construction results in a balanced tree and a code with length multiset $\\{2,2,2,2\\}$. For the random length $L$, we have $P(L=2)=1$. Therefore,\n$$\nE[L]=2,\\qquad E[L^{2}]=4,\\qquad \\sigma_{1}^{2}=E[L^{2}]-(E[L])^{2}=4-4=0.\n$$\n\n**Construct Code 2**: This code results from the other tie-breaking choice. After the initial merge, from the pool {0.4, 0.4, 0.2}, we merge the 0.2-probability symbol with the *newly created* 0.4-probability node. The final merge combines this new 0.6-probability node with the remaining 0.4-probability symbol ($\\mathcal{A}$). This results in an unbalanced tree with length multiset $\\{1,2,3,3\\}$. The induced distribution of $L$ is $P(L=1)=0.4$, $P(L=2)=0.2$, $P(L=3)=0.4$. Compute\n$$\nE[L]=0.4\\cdot 1+0.2\\cdot 2+0.4\\cdot 3=2,\n$$\n$$\nE[L^{2}]=0.4\\cdot 1^{2}+0.2\\cdot 2^{2}+0.4\\cdot 3^{2}=0.4+0.8+3.6=4.8,\n$$\nso\n$$\n\\sigma_{2}^{2}=E[L^{2}]-(E[L])^{2}=4.8-4=0.8.\n$$\n\nThe absolute difference between the variances is\n$$\n|\\sigma_{1}^{2}-\\sigma_{2}^{2}|=|0-0.8|=0.8,\n$$\nwhich rounded to two significant figures is $0.80$.", "answer": "$$\\boxed{0.80}$$", "id": "1659058"}, {"introduction": "Moving beyond assigning discrete codes to individual symbols, arithmetic coding represents an entire sequence of symbols with a single fraction within the interval $[0, 1)$. This powerful approach often achieves compression rates closer to the theoretical limit by avoiding the overhead of assigning an integer number of bits to each symbol. This practice provides a hands-on walkthrough of the core mechanism, asking you to recursively narrow down an interval to find the precise floating-point range that represents a given message [@problem_id:1659115].", "problem": "A simple stochastic source generates symbols from the alphabet {A, B, C}. The probabilities of these symbols are given as follows:\n$P(\\text{A}) = 0.25$\n$P(\\text{B}) = 0.60$\n$P(\\text{C}) = 0.15$\n\nA specific data compression scheme maps any sequence of these symbols to a unique floating-point sub-interval within the initial interval $[0, 1)$. The mapping process is defined recursively:\n1. The initial interval is $[0, 1)$. This interval is partitioned into sub-intervals corresponding to the symbols A, B, and C, arranged in that order. The length of each sub-interval is equal to the probability of the corresponding symbol.\n2. To encode a sequence, the initial interval is narrowed down to the sub-interval of the first symbol in the sequence.\n3. This new, smaller interval is then itself partitioned in the same proportions as the original interval $[0, 1)$ was partitioned. The process then narrows down to the sub-interval corresponding to the second symbol in the sequence.\n4. This recursive procedure is repeated for all symbols in the sequence.\n\nDetermine the final interval $[L, U)$ that represents the sequence \"BCA\". Present your answer as a row matrix containing the lower bound $L$ and the upper bound $U$, in that order, with both values rounded to five significant figures.", "solution": "We apply the recursive interval refinement as in arithmetic coding. Let the current interval be $[L, U)$ with width $R=U-L$. With the symbol order A, B, C and probabilities $p_{A}=0.25$, $p_{B}=0.60$, $p_{C}=0.15$, the partitions within any $[L,U)$ are:\n- A: $[L,\\, L + 0.25R)$\n- B: $[L + 0.25R,\\, L + 0.85R)$\n- C: $[L + 0.85R,\\, U)$\n\nStart with $[L_{0}, U_{0}) = [0, 1)$, so $R_{0}=1$.\n\nFirst symbol B: select $[L_{1}, U_{1}) = [L_{0} + 0.25R_{0},\\, L_{0} + 0.85R_{0}) = [0.25,\\, 0.85)$. Then $R_{1} = U_{1} - L_{1} = 0.60$.\n\nSecond symbol C: within $[0.25, 0.85)$ choose $[L_{2}, U_{2}) = [L_{1} + 0.85R_{1},\\, U_{1}) = [0.25 + 0.85 \\cdot 0.60,\\, 0.85) = [0.76,\\, 0.85)$. Then $R_{2} = U_{2} - L_{2} = 0.09$.\n\nThird symbol A: within $[0.76, 0.85)$ choose $[L_{3}, U_{3}) = [L_{2},\\, L_{2} + 0.25R_{2}) = [0.76,\\, 0.76 + 0.25 \\cdot 0.09] = [0.76,\\, 0.7825)$.\n\nThus the final interval is $[L, U) = [0.76, 0.7825)$. Rounding each bound to five significant figures gives $L = 0.76000$ and $U = 0.78250$.", "answer": "$$\\boxed{\\begin{pmatrix}0.76000  0.78250\\end{pmatrix}}$$", "id": "1659115"}, {"introduction": "A fundamental principle of data compression is that no lossless algorithm can guarantee a reduction in size for all possible inputs. This exercise illuminates this concept using the intuitive method of Run-Length Encoding (RLE). You will analyze a hypothetical scenario to determine the precise conditions under which RLE provides a net benefit, reinforcing the crucial idea that the effectiveness of any compression algorithm is fundamentally dependent on the statistical properties of the data itself [@problem_id:1659074].", "problem": "A digital communication system for a deep-space probe employs a simple lossless compression algorithm to reduce the size of binary telemetry data before transmission. The algorithm is a form of Run-Length Encoding (RLE), where a sequence of identical, consecutive bits is called a \"run\". Each run is encoded into a fixed-length packet consisting of two parts: an $n$-bit unsigned integer representing the length of the run, followed by a single bit indicating the value of the bit in the run (0 or 1). For this specific system, the run counter size is $n=5$.\n\nA particular stream of telemetry data is known to have a total length of $L = 3600$ bits. Analysis of this data type reveals that it is always composed entirely of two types of runs: \"short runs\" of length $l_s = 3$ bits, and \"long runs\" of length $l_l = 25$ bits. Let $N_s$ and $N_l$ be the number of short runs and long runs in the stream, respectively. Note that both $N_s$ and $N_l$ must be non-negative integers.\n\nFor the data to be considered \"compressible,\" the total length of the RLE-encoded data must be strictly less than the original length of $L=3600$ bits. Determine the maximum possible number of short runs, $N_s$, that can be present in such a 3600-bit stream while still allowing it to be compressible. Provide your answer as an integer.", "solution": "Each run is encoded by an $n$-bit count plus one bit for the run value, so the encoded length per run is $n+1$ bits. With $n=5$, each run uses $5+1=6$ bits. The maximum representable run length is $2^{n}-1=31$, which covers both $3$ and $25$, so the encoding is valid for all runs in the stream.\n\nLet $R$ be the total number of runs, so $R=N_{s}+N_{l}$. The encoded length is $6R$ bits. For compressibility we require\n$$\n6R3600 \\quad \\Longrightarrow \\quad R600.\n$$\nSince $R$ is an integer, this is equivalent to $R\\leq 599$.\n\nThe total length constraint is\n$$\n3N_{s}+25N_{l}=3600.\n$$\nFor integer solutions, reduce this modulo $3$. Since $25\\equiv 1 \\pmod{3}$ and $3600\\equiv 0 \\pmod{3}$, we have\n$$\n3N_{s}+25N_{l}\\equiv 0+N_{l}\\equiv 0 \\pmod{3} \\quad \\Longrightarrow \\quad N_{l}\\equiv 0 \\pmod{3}.\n$$\nHence write $N_{l}=3k$ with $k\\in \\mathbb{Z}_{\\geq 0}$. Substituting gives\n$$\n3N_{s}+75k=3600 \\quad \\Longrightarrow \\quad N_{s}=1200-25k.\n$$\nThus\n$$\nR=N_{s}+N_{l}=(1200-25k)+3k=1200-22k.\n$$\nThe compressibility condition $R600$ implies\n$$\n1200-22k600 \\quad \\Longrightarrow \\quad -22k-600 \\quad \\Longrightarrow \\quad 22k600.\n$$\nWith integer $k$, this yields $k\\geq 28$ (since $22\\cdot 27=594$ and $22\\cdot 28=616$). Because $N_{s}=1200-25k$ decreases as $k$ increases, the maximum $N_{s}$ under the compressibility constraint occurs at the smallest feasible $k$, namely $k=28$. Therefore\n$$\nN_{s}=1200-25\\cdot 28=1200-700=500.\n$$\nA quick check: for $k=28$, $N_{l}=84$, $R=1200-22\\cdot 28=584$, and $6R=35043600$, so the stream is compressible.\n\nHence, the maximum possible number of short runs is $500$.", "answer": "$$\\boxed{500}$$", "id": "1659074"}]}