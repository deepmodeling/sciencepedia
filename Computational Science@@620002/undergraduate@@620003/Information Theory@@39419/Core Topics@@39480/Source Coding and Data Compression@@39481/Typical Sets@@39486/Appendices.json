{"hands_on_practices": [{"introduction": "This first practice exercise serves as a direct application of the definition of a weakly typical set. You will test whether a specific sequence belongs to the typical set by calculating its per-symbol negative log-probability and comparing it to the source entropy, $H(X)$. This hands-on calculation [@problem_id:1666266] solidifies the core concept of the Asymptotic Equipartition Property (AEP): that typical sequences are those whose probabilities are close to $2^{-nH(X)}$.", "problem": "A memoryless binary source generates symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The probability of emitting a '1' is $p=0.2$. The entropy of this source is denoted by $H(X)$. An engineer observes the specific 10-symbol sequence $x^{10} = 1000010000$.\n\nThe set of weakly $\\epsilon$-typical sequences of length $n$, as defined by the Asymptotic Equipartition Property (AEP), is the set $A_{\\epsilon}^{(n)}$ of all sequences $y^n$ whose probability $P(y^n)$ satisfies the condition:\n$$ \\left| -\\frac{1}{n} \\log_2 P(y^n) - H(X) \\right| \\leq \\epsilon $$\nFor this memoryless binary source, the probability of a sequence $y^n$ with $k$ ones and $n-k$ zeros is given by $P(y^n) = p^k (1-p)^{n-k}$.\n\nFor comparison, one can also calculate the empirical entropy of the sequence, $H(y^n)$, which is the entropy of a distribution matching the symbol frequencies within $y^n$.\n\nGiven $\\epsilon = 0.1$, which of the following statements is true for the observed sequence $x^{10}$?\n\nA. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is greater than $H(X)$, and the sequence is in $A_{0.1}^{(10)}$.\n\nB. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is less than $H(X)$, and the sequence is in $A_{0.1}^{(10)}$.\n\nC. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is equal to $H(X)$, and the sequence is in $A_{0.1}^{(10)}$.\n\nD. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is greater than $H(X)$, and the sequence is not in $A_{0.1}^{(10)}$.\n\nE. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is less than $H(X)$, and the sequence is not in $A_{0.1}^{(10)}$.", "solution": "The source is i.i.d. Bernoulli with parameter $p=0.2$. Its entropy is\n$$\nH(X)=-p\\log_{2}p-(1-p)\\log_{2}(1-p).\n$$\nFor a length-$n$ sequence $y^{n}$ with $k$ ones, the probability is $P(y^{n})=p^{k}(1-p)^{n-k}$, so\n$$\n-\\frac{1}{n}\\log_{2}P(y^{n})=-\\frac{k}{n}\\log_{2}p-\\left(1-\\frac{k}{n}\\right)\\log_{2}(1-p).\n$$\nFor the observed sequence $x^{10}=1000010000$, the number of ones is $k=2$, hence $\\frac{k}{n}=\\frac{2}{10}=0.2=p$. Substituting $\\frac{k}{n}=p$ into the expression gives\n$$\n-\\frac{1}{10}\\log_{2}P(x^{10})=-p\\log_{2}p-(1-p)\\log_{2}(1-p)=H(X).\n$$\nTherefore,\n$$\n\\left|-\\frac{1}{10}\\log_{2}P(x^{10})-H(X)\\right|=0\\leq \\epsilon \\quad \\text{for} \\quad \\epsilon=0.1,\n$$\nso the sequence is in $A_{0.1}^{(10)}$, and the value equals $H(X)$. This corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1666266"}, {"introduction": "Building on the definition of typicality, this problem explores one of the most powerful consequences of the AEP: the relationship between source entropy and the number of typical sequences. Here, you will reverse the usual line of reasoning to estimate a source's entropy, $H(X)$, from the known size of its typical set, $|A_{\\epsilon}^{(n)}|$. This exercise [@problem_id:1666262] demonstrates the fundamental principle behind source coding and data compressionâ€”that almost all the probability is concentrated in a \"small\" set of approximately $2^{nH(X)}$ sequences.", "problem": "A team of computer scientists is developing an optimal compression algorithm for a data stream generated by a specific type of sensor. This sensor's output can be modeled as a stationary binary memoryless source, which we denote as $X$. The source produces long sequences of bits (0s and 1s). The team analyzes sequences of length $n = 100$ bits. Using statistical analysis, they determine that the number of \"typical\" sequences, which occur with high probability and are the primary target for their compression scheme, is approximately $1.2 \\times 10^{15}$. For the purposes of this model, the size of this typical set, $|A_{\\epsilon}^{(n)}|$, is directly related to the source entropy.\n\nGiven the necessary values $\\ln(1.2 \\times 10^{15}) \\approx 34.72$ and $\\ln(2) \\approx 0.693$, estimate the entropy $H(X)$ of the binary source. Calculate your answer in bits per symbol and round it to three significant figures.", "solution": "We model the stationary binary memoryless source by the Asymptotic Equipartition Property, which implies that the typical set size scales as\n$$\n|A_{\\epsilon}^{(n)}| \\approx 2^{n H(X)}.\n$$\nTaking natural logarithms on both sides gives\n$$\n\\ln |A_{\\epsilon}^{(n)}| \\approx n H(X) \\ln 2.\n$$\nSolving for $H(X)$,\n$$\nH(X) \\approx \\frac{\\ln |A_{\\epsilon}^{(n)}|}{n \\ln 2}.\n$$\nSubstituting the given values $n=100$, $\\ln(1.2 \\times 10^{15}) \\approx 34.72$, and $\\ln(2) \\approx 0.693$,\n$$\nH(X) \\approx \\frac{34.72}{100 \\times 0.693} = \\frac{34.72}{69.3} \\approx 0.501.\n$$\nThus, the entropy is approximately $0.501$ bits per symbol (to three significant figures).", "answer": "$$\\boxed{0.501}$$", "id": "1666262"}, {"introduction": "A common misconception is that the single most probable sequence must also be a \"typical\" one. This exercise is designed to challenge that intuition and highlight the crucial difference between maximum probability and typicality. By analyzing a highly biased source [@problem_id:1666210], you will discover that the most likely individual outcome can be statistically unrepresentative of the source's average behavior (as measured by entropy) and, therefore, may not be a member of the typical set. This reveals a deeper insight into what typicality truly represents.", "problem": "Consider a discrete memoryless source that produces a sequence of binary outcomes, which we will call \"Heads\" (H) and \"Tails\" (T). This source is analogous to flipping a biased coin. The probability of producing a \"Heads\" is $P(X=\\text{H}) = 0.9$, and the probability of producing a \"Tails\" is $P(X=\\text{T}) = 0.1$.\n\nA sequence $x^n = (x_1, x_2, \\dots, x_n)$ of length $n$ is said to be a member of the typical set $A_{\\epsilon}^{(n)}$ if its empirical entropy is close to the true entropy of the source. Specifically, the condition for membership is:\n$$ \\left| -\\frac{1}{n} \\log_2 P(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $H(X)$ is the Shannon entropy of the source $X$, $P(x^n)$ is the probability of the specific sequence $x^n$, and all logarithms are base-2.\n\nFor a sequence of length $n=10$ generated by this source and a given parameter $\\epsilon = 0.1$, determine which of the following statements is correct.\n\nA. The most probable sequence is HHHHHHHHHH, and it belongs to the typical set $A_{0.1}^{(10)}$.\n\nB. The most probable sequence is HHHHHHHHHH, and it does not belong to the typical set $A_{0.1}^{(10)}$.\n\nC. The most probable sequence is one with exactly nine Heads and one Tail (e.g., HHHHHHHHHT), and it belongs to the typical set $A_{0.1}^{(10)}$.\n\nD. The most probable sequence is one with exactly nine Heads and one Tail (e.g., HHHHHHHHHT), and it does not belong to the typical set $A_{0.1}^{(10)}$.\n\nE. The most probable sequence is one with exactly eight Heads and two Tails, and it belongs to the typical set $A_{0.1}^{(10)}$.", "solution": "Let the source be Bernoulli with $p \\triangleq P(X=\\text{H})=0.9$ and $q \\triangleq P(X=\\text{T})=1-p=0.1$. For any length-$10$ sequence with $k$ Heads, its probability is $P = p^{k} q^{10-k}$. The ratio between probabilities of sequences with $k+1$ and $k$ Heads is\n$$\n\\frac{p^{k+1} q^{9-k}}{p^{k} q^{10-k}}=\\frac{p}{q}.\n$$\nSince $\\frac{p}{q}=\\frac{0.9}{0.1}=9>1$, the probability increases with $k$, so the most probable single sequence is the one with $k=10$, namely HHHHHHHHHH.\n\nNow check whether HHHHHHHHHH is in $A_{0.1}^{(10)}$. For this sequence,\n$$\n-\\frac{1}{10}\\log_{2} P(\\text{HHHHHHHHHH})=-\\frac{1}{10}\\log_{2}\\left(p^{10}\\right)=-\\log_{2} p.\n$$\nThe source entropy is\n$$\nH(X)=-p\\log_{2} p - q\\log_{2} q.\n$$\nThus the typicality deviation is\n$$\n\\left|-\\log_{2} p - H(X)\\right|=\\left|-\\log_{2} p + p\\log_{2} p + q\\log_{2} q\\right|\n=\\left|(p-1)\\log_{2} p + q\\log_{2} q\\right|\n$$\n$$\n=\\left|-(1-p)\\log_{2} p + (1-p)\\log_{2} q\\right|\n=(1-p)\\left|\\log_{2}\\!\\left(\\frac{q}{p}\\right)\\right|.\n$$\nWith $p=0.9$ and $q=0.1$, this becomes\n$$\n(1-p)\\left|\\log_{2}\\!\\left(\\frac{q}{p}\\right)\\right|=0.1\\left|\\log_{2}\\!\\left(\\frac{1}{9}\\right)\\right|=0.1\\log_{2}(9).\n$$\nSince $9>2$, we have $\\log_{2}(9)>\\log_{2}(2)=1$, hence $0.1\\log_{2}(9)>0.1=\\epsilon$. Therefore HHHHHHHHHH does not satisfy the typicality criterion for $\\epsilon=0.1$.\n\nCombining the two conclusions: the most probable sequence is HHHHHHHHHH, and it does not belong to $A_{0.1}^{(10)}$. Hence, statement B is correct.", "answer": "$$\\boxed{B}$$", "id": "1666210"}]}