{"hands_on_practices": [{"introduction": "The Asymptotic Equipartition Property (AEP) fundamentally relies on the concept of a \"typical set.\" This first exercise provides a concrete, hands-on opportunity to understand this core idea by directly applying its mathematical definition. By working with a small, manageable sequence length, you will determine exactly which sequences from a binary source belong to the typical set, transforming the abstract conditions of the AEP into a tangible sorting task. [@problem_id:1603197]", "problem": "A simple digital biosensor is designed to monitor a specific biological process. The sensor outputs a stream of binary digits ('0' or '1'). The process being monitored is such that the sensor's output can be modeled as a memoryless source where each digit is generated independently. The probability of the sensor outputting a '1' is $p(1) = 0.2$, and the probability of outputting a '0' is $p(0) = 0.8$.\n\nThe entropy of this source is $H(X) \\approx 0.722$ bits. According to the Asymptotic Equipartition Property (AEP), we can define a \"typical set\" of sequences. A sequence $x^n = (x_1, x_2, \\dots, x_n)$ of length $n$ is considered to be in the typical set $A_\\epsilon^{(n)}$ if its sample entropy is close to the true entropy $H(X)$. The condition is given by:\n$$ \\left| -\\frac{1}{n}\\log_2 p(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $p(x^n)$ is the probability of the specific sequence $x^n$.\n\nConsider all possible output sequences of length $n=4$. For a tolerance of $\\epsilon = 0.15$, determine the total number of distinct sequences that belong to the typical set $A_{0.15}^{(4)}$.\n\nFor your calculations, you may use the following base-2 logarithm values: $\\log_2(0.2) \\approx -2.322$ and $\\log_2(0.8) \\approx -0.322$.", "solution": "The source is memoryless and binary with $p(1)=0.2$ and $p(0)=0.8$. For a length-$4$ sequence $x^{4}$, let $k$ be the number of ones in the sequence. Then the probability of any particular sequence with $k$ ones is\n$$\np(x^{4})=p(1)^{k}p(0)^{4-k}=0.2^{k}0.8^{4-k}.\n$$\nThe sample entropy per symbol is\n$$\n-\\frac{1}{4}\\log_{2}p(x^{4})=-\\frac{1}{4}\\left[k\\log_{2}(0.2)+(4-k)\\log_{2}(0.8)\\right].\n$$\nUsing $\\log_{2}(0.2)\\approx -2.322$ and $\\log_{2}(0.8)\\approx -0.322$, this becomes\n$$\n-\\frac{1}{4}\\log_{2}p(x^{4})=-\\frac{1}{4}\\left[k(-2.322)+(4-k)(-0.322)\\right]\n=\\frac{1}{4}\\left(2.322k+0.322\\cdot 4-0.322k\\right)\n=\\frac{1}{4}\\left(2.0k+1.288\\right)\n=0.5k+0.322.\n$$\nThe typical set condition with $H(X)\\approx 0.722$ and $\\epsilon=0.15$ is\n$$\n\\left|-\\frac{1}{4}\\log_{2}p(x^{4})-H(X)\\right|\\le 0.15\n\\;\\;\\Longleftrightarrow\\;\\;\n\\left|0.5k+0.322-0.722\\right|\\le 0.15\n\\;\\;\\Longleftrightarrow\\;\\;\n\\left|0.5k-0.4\\right|\\le 0.15.\n$$\nThis inequality yields\n$$\n-0.15\\le 0.5k-0.4\\le 0.15\n\\;\\;\\Longleftrightarrow\\;\\;\n0.25\\le 0.5k\\le 0.55\n\\;\\;\\Longleftrightarrow\\;\\;\n0.5\\le k\\le 1.1.\n$$\nSince $k$ must be an integer in $\\{0,1,2,3,4\\}$, the only admissible value is $k=1$. The number of distinct sequences of length $4$ with exactly one $1$ is\n$$\n\\binom{4}{1}=4.\n$$\nTherefore, the total number of sequences in $A_{0.15}^{(4)}$ is $4$.", "answer": "$$\\boxed{4}$$", "id": "1603197"}, {"introduction": "Now that we have a method for identifying typical sequences, let's explore why they are so central to information theory. This practice powerfully demonstrates the \"equipartition\" aspect of the AEP, where probability is almost equally divided among members of the typical set. By calculating and comparing the probabilities of a typical versus a non-typical sequence, you will numerically verify that almost all the probability mass is concentrated in this small subset of all possible outcomes. [@problem_id:1603164]", "problem": "A simplified model for a telemetry stream from a deep space probe treats the stream as a sequence of symbols generated by a discrete memoryless source. The source has an alphabet of three symbols, $\\mathcal{X} = \\{A, B, C\\}$, with the following probabilities of emission:\n- $P(A) = \\frac{1}{2}$\n- $P(B) = \\frac{1}{4}$\n- $P(C) = \\frac{1}{4}$\n\nConsider two specific sequences of length $n=12$ that could be received from this source:\n- Sequence 1: $S_1 = \\text{AAAAAABBBCCC}$\n- Sequence 2: $S_2 = \\text{AAAAAAAAAAAA}$\n\nCalculate the ratio of the probability of receiving sequence $S_1$ to the probability of receiving sequence $S_2$. That is, compute the value of $\\frac{P(S_1)}{P(S_2)}$.\n\nGive your answer as a single real number, rounded to three significant figures.", "solution": "For a discrete memoryless source, the probability of a specific length-$n$ sequence factorizes as the product of the symbol probabilities at each position. For a sequence with counts $n_{A}$, $n_{B}$, $n_{C}$, its probability is\n$$\nP(S)=P(A)^{n_{A}}P(B)^{n_{B}}P(C)^{n_{C}}.\n$$\nSequence $S_{1}=\\text{AAAAAABBBCCC}$ has counts $n_{A}=6$, $n_{B}=3$, $n_{C}=3$. Thus\n$$\nP(S_{1})=\\left(\\frac{1}{2}\\right)^{6}\\left(\\frac{1}{4}\\right)^{3}\\left(\\frac{1}{4}\\right)^{3}=\\left(\\frac{1}{2}\\right)^{6}\\left(\\frac{1}{4}\\right)^{6}=\\left(\\frac{1}{2}\\right)^{6}\\left(\\left(\\frac{1}{2}\\right)^2\\right)^{6}=\\left(\\frac{1}{2}\\right)^{6}\\left(\\frac{1}{2}\\right)^{12}=\\left(\\frac{1}{2}\\right)^{18}.\n$$\nSequence $S_{2}=\\text{AAAAAAAAAAAA}$ has counts $n_{A}=12$, $n_{B}=0$, $n_{C}=0$. Thus\n$$\nP(S_{2})=\\left(\\frac{1}{2}\\right)^{12}.\n$$\nThe ratio is\n$$\n\\frac{P(S_{1})}{P(S_{2})}=\\frac{\\left(\\frac{1}{2}\\right)^{18}}{\\left(\\frac{1}{2}\\right)^{12}}=\\left(\\frac{1}{2}\\right)^{6}=\\frac{1}{64}=0.015625.\n$$\nRounding to three significant figures gives $0.0156$.", "answer": "$$\\boxed{0.0156}$$", "id": "1603164"}, {"introduction": "A common pitfall is to confuse a sequence that appears patterned or structured with one that is statistically \"typical\". This final practice directly addresses that misconception by analyzing a highly regular sequence that is, in fact, non-typical for the given source. By quantifying its deviation from the source's true entropy $H(X)$, you will reinforce the understanding that typicality is a statistical property rooted in symbol frequencies, not a subjective measure of randomness or complexity. [@problem_id:1603162]", "problem": "A memoryless binary source generates a sequence of bits, where each bit is an independent and identically distributed (i.i.d.) random variable $X$. The probability of emitting a '1' is $p_1 = 0.75$, and the probability of emitting a '0' is $p_0 = 0.25$. The entropy of this source in bits is given by $H(X) = -p_0 \\log_2(p_0) - p_1 \\log_2(p_1)$.\n\nThe Asymptotic Equipartition Property (AEP) provides a method to classify long sequences. For a large sequence length $n$ and a small tolerance $\\epsilon > 0$, a specific sequence $x^n = (x_1, x_2, \\dots, x_n)$ is considered a member of the \"typical set\" $A_\\epsilon^{(n)}$ if the following condition is met:\n$$ \\left| -\\frac{1}{n}\\log_2 p(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $p(x^n)$ is the probability of observing the specific sequence $x^n$. The value $-\\frac{1}{n}\\log_2 p(x^n)$ represents the information content per symbol for that particular sequence. Sequences for which this value deviates significantly from the source entropy $H(X)$ are considered non-typical.\n\nConsider a highly structured sequence $S$ of length $n=1000$ that consists of alternating 0s and 1s, starting with a 0: $S = (0, 1, 0, 1, \\dots, 0, 1)$. To quantify how non-typical this sequence is, we can calculate the minimum value of $\\epsilon$ for which $S$ is included in the typical set $A_\\epsilon^{(n)}$. This minimum value is precisely the absolute difference given in the AEP definition.\n\nCalculate this minimum required $\\epsilon$ for the sequence $S$. Express your answer in bits, rounded to three significant figures.", "solution": "The problem asks for the minimum value of $\\epsilon$ that satisfies the condition for the sequence $S$ to be in the typical set $A_\\epsilon^{(1000)}$. This minimum value is given by the expression:\n$$ \\epsilon_{\\text{min}} = \\left| -\\frac{1}{n}\\log_2 p(S) - H(X) \\right| $$\nWe need to calculate the two terms inside the absolute value: the source entropy $H(X)$ and the per-symbol information content of the sequence $S$, which is $-\\frac{1}{n}\\log_2 p(S)$.\n\nFirst, let's calculate the entropy of the source, $H(X)$. The probabilities are given as $p_1 = P(X=1) = 0.75 = \\frac{3}{4}$ and $p_0 = P(X=0) = 0.25 = \\frac{1}{4}$.\nThe entropy $H(X)$ is:\n$$ H(X) = -p_0 \\log_2(p_0) - p_1 \\log_2(p_1) $$\n$$ H(X) = -\\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) - \\frac{3}{4} \\log_2\\left(\\frac{3}{4}\\right) $$\nUsing the properties of logarithms, $\\log(a/b) = \\log(a) - \\log(b)$ and $\\log(1/a) = -\\log(a)$:\n$$ H(X) = -\\frac{1}{4} (-\\log_2(4)) - \\frac{3}{4} (\\log_2(3) - \\log_2(4)) $$\nSince $\\log_2(4) = \\log_2(2^2) = 2$:\n$$ H(X) = -\\frac{1}{4}(-2) - \\frac{3}{4}(\\log_2(3) - 2) $$\n$$ H(X) = \\frac{1}{2} - \\frac{3}{4}\\log_2(3) + \\frac{3}{4}(2) $$\n$$ H(X) = \\frac{1}{2} + \\frac{3}{2} - \\frac{3}{4}\\log_2(3) = 2 - \\frac{3}{4}\\log_2(3) $$\n\nNext, let's analyze the sequence $S = (0, 1, 0, 1, \\dots, 0, 1)$ of length $n=1000$. Since the sequence is alternating and starts with 0, there will be an equal number of 0s and 1s.\nThe number of 0s is $n_0 = \\frac{n}{2} = \\frac{1000}{2} = 500$.\nThe number of 1s is $n_1 = \\frac{n}{2} = \\frac{1000}{2} = 500$.\n\nThe probability of this specific sequence $S$, $p(S)$, is the product of the probabilities of each independent bit.\n$$ p(S) = p_0^{n_0} \\cdot p_1^{n_1} = \\left(\\frac{1}{4}\\right)^{500} \\cdot \\left(\\frac{3}{4}\\right)^{500} $$\n\nNow, we calculate the per-symbol information content, $-\\frac{1}{n}\\log_2 p(S)$.\n$$ -\\frac{1}{n}\\log_2 p(S) = -\\frac{1}{1000} \\log_2\\left( \\left(\\frac{1}{4}\\right)^{500} \\cdot \\left(\\frac{3}{4}\\right)^{500} \\right) $$\n$$ = -\\frac{1}{1000} \\left[ 500 \\log_2\\left(\\frac{1}{4}\\right) + 500 \\log_2\\left(\\frac{3}{4}\\right) \\right] $$\n$$ = -\\frac{500}{1000} \\left[ \\log_2\\left(\\frac{1}{4}\\right) + \\log_2\\left(\\frac{3}{4}\\right) \\right] $$\n$$ = -\\frac{1}{2} [-\\log_2(4) + (\\log_2(3) - \\log_2(4))] $$\n$$ = -\\frac{1}{2} [-2 + \\log_2(3) - 2] = -\\frac{1}{2} [\\log_2(3) - 4] $$\n$$ = 2 - \\frac{1}{2}\\log_2(3) $$\n\nFinally, we compute the absolute difference $\\epsilon_{\\text{min}}$:\n$$ \\epsilon_{\\text{min}} = \\left| \\left(2 - \\frac{1}{2}\\log_2(3)\\right) - \\left(2 - \\frac{3}{4}\\log_2(3)\\right) \\right| $$\n$$ \\epsilon_{\\text{min}} = \\left| 2 - \\frac{1}{2}\\log_2(3) - 2 + \\frac{3}{4}\\log_2(3) \\right| $$\n$$ \\epsilon_{\\text{min}} = \\left| \\left(\\frac{3}{4} - \\frac{1}{2}\\right)\\log_2(3) \\right| = \\left| \\left(\\frac{3}{4} - \\frac{2}{4}\\right)\\log_2(3) \\right| $$\n$$ \\epsilon_{\\text{min}} = \\left| \\frac{1}{4}\\log_2(3) \\right| $$\nSince $\\log_2(3) > 0$, the absolute value is redundant.\n$$ \\epsilon_{\\text{min}} = \\frac{1}{4}\\log_2(3) $$\n\nTo get the numerical answer, we use the value $\\log_2(3) \\approx 1.5849625$.\n$$ \\epsilon_{\\text{min}} \\approx \\frac{1}{4} \\times 1.5849625 = 0.396240625 $$\nThe problem asks for the answer in bits, rounded to three significant figures.\n$$ \\epsilon_{\\text{min}} \\approx 0.396 \\text{ bits} $$", "answer": "$$\\boxed{0.396}$$", "id": "1603162"}]}