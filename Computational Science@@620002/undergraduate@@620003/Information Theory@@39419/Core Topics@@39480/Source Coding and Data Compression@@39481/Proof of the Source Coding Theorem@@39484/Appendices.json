{"hands_on_practices": [{"introduction": "The proof of the Source Coding Theorem relies on the Asymptotic Equipartition Property (AEP), which begins with the idea of a 'typical' sequence. A sequence is considered typical if its statistical properties closely match the properties of the source that generated it. This first exercise [@problem_id:1648667] provides a concrete, hands-on opportunity to apply the formal definition of a typical set by calculating the sample entropy of a specific sequence and checking if it falls within a given tolerance of the true source entropy, $H(X)$.", "problem": "In the study of data compression, a fundamental concept is the \"typical set,\" which contains sequences that are \"statistically representative\" of a random source. Consider a memoryless binary source that generates a sequence of bits ('0' or '1'). The probability of emitting a '0' is $P(0) = \\frac{1}{4}$, and the probability of emitting a '1' is $P(1) = \\frac{3}{4}$.\n\nThe Shannon entropy of this source, denoted $H(X)$, quantifies the average uncertainty per symbol and is given by the formula:\n$$H(X) = -P(0) \\log_2(P(0)) - P(1) \\log_2(P(1))$$\n\nFor a specific output sequence $\\mathbf{x}$ of length $n$, we can define its sample entropy, $H(\\mathbf{x})$, as:\n$$H(\\mathbf{x}) = -\\frac{1}{n} \\log_2(P(\\mathbf{x}))$$\nwhere $P(\\mathbf{x})$ is the probability of observing that particular sequence.\n\nA sequence $\\mathbf{x}$ is said to belong to the typical set $A_\\epsilon^{(n)}$ for a given tolerance $\\epsilon > 0$ if its sample entropy is close to the true source entropy, satisfying the condition:\n$$|H(\\mathbf{x}) - H(X)| \\le \\epsilon$$\n\nSuppose the source emits the following sequence of length $n=8$:\n$$\\mathbf{x} = 01101011$$\nUsing a tolerance of $\\epsilon = 0.2$, calculate the sample entropy $H(\\mathbf{x})$ for this sequence and determine if it belongs to the typical set $A_{0.2}^{(8)}$. Which of the following statements is correct?\n\nA. The sample entropy is $2 - \\frac{3}{4}\\log_2(3)$, and the sequence belongs to the typical set.\n\nB. The sample entropy is $2 - \\frac{5}{8}\\log_2(3)$, and the sequence belongs to the typical set.\n\nC. The sample entropy is $2 - \\frac{5}{8}\\log_2(3)$, and the sequence does not belong to the typical set.\n\nD. The sample entropy is $\\frac{1}{8}\\log_2(3)$, and the sequence does not belong to the typical set.\n\nE. The sample entropy is $2 - \\frac{3}{4}\\log_2(3)$, and the sequence does not belong to the typical set.", "solution": "The source is memoryless with $P(0)=\\frac{1}{4}$ and $P(1)=\\frac{3}{4}$. For the sequence $\\mathbf{x}=01101011$ of length $n=8$, the number of ones is $k=5$ and the number of zeros is $n-k=3$. By the memoryless property,\n$$\nP(\\mathbf{x})=\\left(\\frac{3}{4}\\right)^{5}\\left(\\frac{1}{4}\\right)^{3}.\n$$\nThe sample entropy is\n$$\nH(\\mathbf{x})=-\\frac{1}{8}\\log_{2}P(\\mathbf{x})=-\\frac{1}{8}\\left[5\\log_{2}\\left(\\frac{3}{4}\\right)+3\\log_{2}\\left(\\frac{1}{4}\\right)\\right].\n$$\nUsing $\\log_{2}\\left(\\frac{3}{4}\\right)=\\log_{2}3-2$ and $\\log_{2}\\left(\\frac{1}{4}\\right)=-2$, we obtain\n$$\nH(\\mathbf{x})=-\\frac{1}{8}\\left[5(\\log_{2}3-2)+3(-2)\\right]\n=-\\frac{1}{8}\\left(5\\log_{2}3-16\\right)\n=2-\\frac{5}{8}\\log_{2}3.\n$$\nThe source entropy is\n$$\nH(X)=-\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right)\n=2-\\frac{3}{4}\\log_{2}3.\n$$\nTherefore,\n$$\n|H(\\mathbf{x})-H(X)|=\\left|2-\\frac{5}{8}\\log_{2}3-\\left(2-\\frac{3}{4}\\log_{2}3\\right)\\right|\n=\\left(\\frac{3}{4}-\\frac{5}{8}\\right)\\log_{2}3=\\frac{1}{8}\\log_{2}3.\n$$\nWe compare this to $\\epsilon=0.2=\\frac{1}{5}$. The inequality\n$$\n\\frac{1}{8}\\log_{2}3\\le\\frac{1}{5}\n$$\nis equivalent to $\\log_{2}3\\le\\frac{8}{5}$, i.e., $3\\le 2^{8/5}$. Raising both sides to the fifth power gives $3^{5}\\le 2^{8}$, and since $3^{5}=243256=2^{8}$, we have $\\frac{1}{8}\\log_{2}3\\frac{1}{5}$. Hence the sequence belongs to the typical set $A_{0.2}^{(8)}$.\n\nThus, the sample entropy is $2-\\frac{5}{8}\\log_{2}3$, and the sequence is typical. The correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1648667"}, {"introduction": "After understanding what makes a single sequence typical, the next step is to consider the entire set of such sequences, known as the typical set $A_{\\epsilon}^{(n)}$. While the AEP tells us that for large $n$, a randomly generated sequence is almost certain to be in this set, it also makes a key claim: the size of this set is much smaller than the total number of possible sequences. This exercise [@problem_id:1648656] allows you to calculate the upper bound on the size of the typical set, providing a quantitative sense of the 'small' but 'highly probable' nature of this set, which is the cornerstone of typical set-based compression.", "problem": "In information theory, the Asymptotic Equipartition Property (AEP) provides a powerful way to characterize the behavior of long sequences of random variables. For a discrete memoryless source with entropy $H(X)$, the set of \"typical\" sequences of length $n$, denoted $A_{\\epsilon}^{(n)}$, contains sequences whose empirical entropy is close to the true entropy. The AEP establishes that for any $\\epsilon > 0$, the number of sequences in this set, $|A_{\\epsilon}^{(n)}|$, has a maximum possible value given by the upper bound $2^{n(H(X) + \\epsilon)}$, where the entropy $H(X)$ is measured in bits.\n\nConsider a source modeled by the outcome of a single roll of a fair, six-sided die. We observe a sequence of $n=100$ independent and identically distributed outcomes from this source. For a chosen deviation parameter of $\\epsilon = 0.1$, calculate the maximum possible number of sequences in the typical set $A_{\\epsilon}^{(n)}$.\n\nFor your calculations, use the following constants:\n$\\ln(2) = 0.6931$\n$\\ln(6) = 1.7918$\n$\\log_{10}(2) = 0.3010$\n\nExpress your final answer in scientific notation, rounded to three significant figures.", "solution": "We use the Asymptotic Equipartition Property (AEP) upper bound for the cardinality of the typical set of an i.i.d. discrete memoryless source:\n$$\n|A_{\\epsilon}^{(n)}| \\leq 2^{n \\left(H(X)+\\epsilon\\right)}.\n$$\nFor a fair six-sided die, the entropy in bits is\n$$\nH(X) = -\\sum_{i=1}^{6} \\frac{1}{6} \\log_{2}\\left(\\frac{1}{6}\\right) = \\log_{2}(6).\n$$\nWith $n=100$ and $\\epsilon=0.1$, the maximal size is therefore\n$$\n|A_{\\epsilon}^{(n)}|_{\\max} = 2^{100\\left(\\log_{2}(6)+0.1\\right)}.\n$$\nTo express this in scientific notation, convert to base 10 using $2^{y} = 10^{y \\log_{10}(2)}$:\n$$\n|A_{\\epsilon}^{(n)}|_{\\max} = 10^{100\\left(\\log_{2}(6)+0.1\\right)\\log_{10}(2)}.\n$$\nCompute $\\log_{2}(6)$ using the provided natural logarithms:\n$$\n\\log_{2}(6) = \\frac{\\ln(6)}{\\ln(2)} = \\frac{1.7918}{0.6931} \\approx 2.5852.\n$$\nThen\n$$\n100\\left(\\log_{2}(6)+0.1\\right) = 100\\left(2.5852+0.1\\right) = 268.52,\n$$\nand the base-10 exponent becomes\n$$\nb = 268.52 \\cdot \\log_{10}(2) = 268.52 \\cdot 0.3010 = 80.82452.\n$$\nThus\n$$\n|A_{\\epsilon}^{(n)}|_{\\max} = 10^{80.82452} = 10^{0.82452} \\times 10^{80}.\n$$\nNow evaluate the mantissa $10^{0.82452}$. Using $10^{x} = 2^{x/\\log_{10}(2)}$,\n$$\n10^{0.82452} = 2^{\\frac{0.82452}{0.3010}} \\approx 2^{2.73927} = 4 \\cdot 2^{0.73927} = 4 \\cdot \\exp\\!\\left(0.73927 \\ln(2)\\right).\n$$\nWith $\\ln(2)=0.6931$,\n$$\n0.73927 \\ln(2) \\approx 0.73927 \\cdot 0.6931 \\approx 0.51239,\n$$\nand a series approximation gives\n$$\n\\exp(0.51239) \\approx 1 + 0.51239 + \\frac{0.51239^{2}}{2} + \\frac{0.51239^{3}}{6} + \\frac{0.51239^{4}}{24} + \\frac{0.51239^{5}}{120} \\approx 1.6692.\n$$\nTherefore,\n$$\n10^{0.82452} \\approx 4 \\times 1.6692 \\approx 6.6768 \\approx 6.68 \\quad \\text{(to three significant figures)}.\n$$\nHence,\n$$\n|A_{\\epsilon}^{(n)}|_{\\max} \\approx 6.68 \\times 10^{80}.\n$$", "answer": "$$\\boxed{6.68 \\times 10^{80}}$$", "id": "1648656"}, {"introduction": "Having established that most source outputs belong to a relatively small typical set, we can now design an efficient compression scheme. The strategy is to assign short codewords to the probable typical sequences and longer ones to the rare non-typical sequences. This final practice problem [@problem_id:1648687] brings all the concepts together by guiding you through the derivation of the expected codeword length for such a block-coding scheme, mathematically demonstrating how this approach allows us to achieve an average compression rate that approaches the fundamental limit set by the source entropy, $H(X)$.", "problem": "A data compression system is being designed for a Discrete Memoryless Source (DMS). The source produces symbols from a finite alphabet $\\mathcal{X}$ of size $|\\mathcal{X}|$, and has a known entropy of $H(X)$ bits per symbol. The system employs a block coding strategy, encoding sequences of $n$ symbols at a time, where $n$ is a large integer.\n\nThe encoding algorithm partitions all possible source sequences $x^n$ of length $n$ into two groups: a 'typical set' $A_{\\epsilon}^{(n)}$ and a 'non-typical set', based on a small positive constant $\\epsilon$. The length of the binary codeword assigned to a sequence $x^n$ is denoted by $L(x^n)$.\n\nThe codeword lengths are assigned as follows:\n- For any sequence in the typical set ($x^n \\in A_{\\epsilon}^{(n)}$), the assigned codeword has a length of $L_{typ} = \\lceil n(H(X) + \\epsilon) \\rceil$ bits.\n- For any sequence in the non-typical set ($x^n \\notin A_{\\epsilon}^{(n)}$), the assigned codeword has a constant length of $L_{non\\_typ} = B$ bits, where $B$ is a fixed integer larger than $L_{typ}$.\n\nLet $P_{nt}$ denote the total probability of the non-typical set, i.e., $P_{nt} = P(X^n \\notin A_{\\epsilon}^{(n)})$. For the algebraic simplification, assume $n$ is large enough that the ceiling function can be approximated by its argument, i.e., $\\lceil z \\rceil \\approx z$.\n\nDerive an analytic expression for the expected codeword length per source symbol, $\\bar{L} = \\frac{1}{n} E[L(X^n)]$. Express your answer in terms of $H(X)$, $\\epsilon$, $P_{nt}$, $n$, and $B$.", "solution": "The expected codeword length for a sequence of length $n$, denoted $E[L(X^n)]$, is calculated by summing the length of each possible sequence's codeword, $L(x^n)$, weighted by its probability of occurrence, $p(x^n)$. The sum is taken over all possible sequences $x^n$ in the source's alphabet space $\\mathcal{X}^n$.\n$$ E[L(X^n)] = \\sum_{x^n \\in \\mathcal{X}^n} p(x^n) L(x^n) $$\nWe can split this summation into two parts: one over the typical set $A_{\\epsilon}^{(n)}$ and one over the non-typical set, which is the complement of $A_{\\epsilon}^{(n)}$.\n$$ E[L(X^n)] = \\sum_{x^n \\in A_{\\epsilon}^{(n)}} p(x^n) L(x^n) + \\sum_{x^n \\notin A_{\\epsilon}^{(n)}} p(x^n) L(x^n) $$\nAccording to the problem description, the codeword length is constant within each of these sets. For all typical sequences, $L(x^n) = L_{typ} = \\lceil n(H(X) + \\epsilon) \\rceil$. For all non-typical sequences, $L(x^n) = L_{non\\_typ} = B$. We can therefore factor these constant lengths out of the summations.\n$$ E[L(X^n)] = L_{typ} \\sum_{x^n \\in A_{\\epsilon}^{(n)}} p(x^n) + L_{non\\_typ} \\sum_{x^n \\notin A_{\\epsilon}^{(n)}} p(x^n) $$\nThe sums of probabilities are, by definition, the total probability of a sequence being in the typical set and the non-typical set, respectively.\n$$ \\sum_{x^n \\in A_{\\epsilon}^{(n)}} p(x^n) = P(X^n \\in A_{\\epsilon}^{(n)}) $$\n$$ \\sum_{x^n \\notin A_{\\epsilon}^{(n)}} p(x^n) = P(X^n \\notin A_{\\epsilon}^{(n)}) = P_{nt} $$\nSince the two sets are complementary, the probability of being in the typical set is $P(X^n \\in A_{\\epsilon}^{(n)}) = 1 - P(X^n \\notin A_{\\epsilon}^{(n)}) = 1 - P_{nt}$.\nSubstituting these probabilities and the codeword length definitions back into the expression for the expected length, we get:\n$$ E[L(X^n)] = \\lceil n(H(X) + \\epsilon) \\rceil (1 - P_{nt}) + B \\cdot P_{nt} $$\nThe problem states to use the approximation $\\lceil z \\rceil \\approx z$ for large $z$. Applying this to the first term, we have:\n$$ E[L(X^n)] \\approx n(H(X) + \\epsilon)(1 - P_{nt}) + B \\cdot P_{nt} $$\nThe question asks for the expected codeword length per source symbol, $\\bar{L}$, which is obtained by dividing the total expected length by the block size $n$.\n$$ \\bar{L} = \\frac{E[L(X^n)]}{n} \\approx \\frac{n(H(X) + \\epsilon)(1 - P_{nt}) + B \\cdot P_{nt}}{n} $$\nWe can separate the terms to simplify the expression:\n$$ \\bar{L} \\approx \\frac{n(H(X) + \\epsilon)(1 - P_{nt})}{n} + \\frac{B \\cdot P_{nt}}{n} $$\n$$ \\bar{L} \\approx (H(X) + \\epsilon)(1 - P_{nt}) + \\frac{B}{n} P_{nt} $$\nThis is the final expression for the expected codeword length per source symbol under the given approximations.", "answer": "$$\\boxed{(H(X) + \\epsilon)(1 - P_{nt}) + \\frac{B}{n} P_{nt}}$$", "id": "1648687"}]}