{"hands_on_practices": [{"introduction": "The connection between entropy and data compression is rooted in a powerful idea: for long sequences, only a small fraction of all possible outcomes are actually likely to occur. This 'typical set' has a size that can be estimated with surprising accuracy using the source entropy, approximately $2^{nH(X)}$. This foundational exercise [@problem_id:1650580] invites you to derive the expression for the binary entropy function $H(p)$ by starting with a combinatorial counting problem and applying Stirling's approximation, revealing how entropy naturally emerges as the key to quantifying the size of the set of typical sequences.", "problem": "Consider a communication system that transmits data from a binary source. The source independently generates '0's and '1's, with the probability of generating a '1' being $p$, and '0' being $1-p$.\n\nFor a large block length $n$, the overwhelming majority of sequences that are likely to be observed (i.e., the typical sequences) contain a number of '1's, denoted by $k$, that is very close to the statistical expectation, $np$.\n\nLet's model the set of typical sequences as the collection of all binary sequences of length $n$ that contain exactly $k = np$ ones. The number of such sequences is given by the binomial coefficient $\\binom{n}{k}$. A fundamental result from information theory, related to the Asymptotic Equipartition Property (AEP), states that for large $n$, the size of this set can be approximated by an expression of the form:\n$$ \\binom{n}{np} \\approx 2^{n \\cdot H_{\\text{comb}}(p)} $$\nwhere $H_{\\text{comb}}(p)$ can be interpreted as a combinatorially-derived entropy for the source.\n\nYour task is to derive the analytical expression for $H_{\\text{comb}}(p)$. To do this, use Stirling's approximation for the factorial, which for large $m$ is given by $\\ln(m!) \\approx m \\ln m - m$. You should assume that $n$, $np$, and $n(1-p)$ are all sufficiently large for this approximation to be accurate. The final expression for $H_{\\text{comb}}(p)$ should be given in terms of $p$ and base-2 logarithms.", "solution": "We want an explicit expression for $H_{\\text{comb}}(p)$ in the approximation\n$$\n\\binom{n}{np} \\approx 2^{n \\cdot H_{\\text{comb}}(p)}.\n$$\nAssume $n$, $np$, and $n(1-p)$ are large and, for convenience, that $np$ is an integer. Start by taking natural logarithms and applying Stirlingâ€™s approximation $\\ln(m!) \\approx m \\ln m - m$:\n$$\n\\ln \\binom{n}{np} = \\ln(n!) - \\ln((np)!) - \\ln((n(1-p))!) \\approx \\left(n \\ln n - n\\right) - \\left(np \\ln(np) - np\\right) - \\left(n(1-p)\\ln(n(1-p)) - n(1-p)\\right).\n$$\nExpand the logarithms $\\ln(np) = \\ln n + \\ln p$ and $\\ln(n(1-p)) = \\ln n + \\ln(1-p)$:\n$$\n\\ln \\binom{n}{np} \\approx n \\ln n - n - \\left[np(\\ln n + \\ln p) - np\\right] - \\left[n(1-p)(\\ln n + \\ln(1-p)) - n(1-p)\\right].\n$$\nDistribute and collect terms:\n$$\n\\ln \\binom{n}{np} \\approx n \\ln n - n - np \\ln n - np \\ln p + np - n(1-p) \\ln n - n(1-p) \\ln(1-p) + n(1-p).\n$$\nGroup the $\\ln n$ terms and the constant terms:\n- The $\\ln n$ terms cancel:\n$$\nn \\ln n - np \\ln n - n(1-p) \\ln n = n \\ln n - n \\ln n = 0.\n$$\n- The constants cancel:\n$$\n-n + np + n(1-p) = -n + n = 0.\n$$\nThe remaining terms are\n$$\n\\ln \\binom{n}{np} \\approx -np \\ln p - n(1-p) \\ln(1-p) = n\\left[-p \\ln p - (1-p)\\ln(1-p)\\right].\n$$\nExponentiating and converting from natural logarithms to base-$2$ using $\\exp(x) = 2^{x / \\ln 2}$ yields\n$$\n\\binom{n}{np} \\approx \\exp\\!\\left(n\\left[-p \\ln p - (1-p)\\ln(1-p)\\right]\\right)\n= 2^{\\,n\\left[-p \\log_{2} p - (1-p)\\log_{2}(1-p)\\right]}.\n$$\nComparing with $\\binom{n}{np} \\approx 2^{n \\cdot H_{\\text{comb}}(p)}$, we identify\n$$\nH_{\\text{comb}}(p) = -p \\log_{2} p - (1-p)\\log_{2}(1-p).\n$$\nThis expression extends continuously to $p \\in \\{0,1\\}$ by the convention $0 \\log_{2} 0 = 0$.", "answer": "$$\\boxed{-p \\log_{2} p - (1-p)\\log_{2}(1-p)}$$", "id": "1650580"}, {"introduction": "A common point of confusion when first learning about typical sets is the assumption that the most probable sequence must be a member of this set. This exercise [@problem_id:1650620] is designed to challenge that intuition and clarify the true meaning of typicality. By analyzing a simple biased source, you will determine whether the single most likely outcome satisfies the conditions for being typical, providing a crucial insight into why typical sequences are defined by their statistical resemblance to the source, not just their individual likelihood.", "problem": "Consider a binary Discrete Memoryless Source (DMS) that generates sequences of symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The probability of emitting symbol '0' is $P(0) = 0.9$, and the probability of emitting symbol '1' is $P(1) = 0.1$. The entropy of this source is denoted by $H(X)$.\n\nFor a given $\\epsilon  0$ and a sequence length $n$, the typical set, $A_{\\epsilon}^{(n)}$, is defined as the set of all sequences $x^n = (x_1, x_2, \\dots, x_n)$ that satisfy the following condition:\n$$ \\left| -\\frac{1}{n} \\log_{2} P(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $P(x^n)$ is the probability of the sequence $x^n$.\n\nLet's set the parameter $\\epsilon = 0.1$. Based on the source properties and definitions provided, which of the following statements is correct regarding the single most probable sequence of length $n$ generated by this source?\n\nA. The most probable sequence of any length $n$ is a member of the typical set $A_{0.1}^{(n)}$.\n\nB. The most probable sequence of any length $n$ is not a member of the typical set $A_{0.1}^{(n)}$.\n\nC. Whether the most probable sequence is a member of $A_{0.1}^{(n)}$ depends on the specific value of the sequence length $n$.\n\nD. The concept of a typical set and its defining condition cannot be applied to the single most probable sequence.\n\nE. The most probable sequence is only a member of the typical set $A_{0.1}^{(n)}$ if the source is unbiased, i.e., if $P(0)=P(1)=0.5$.", "solution": "A binary DMS produces i.i.d. symbols, so for any length-$n$ sequence $x^{n}=(x_{1},\\dots,x_{n})$ the probability factors as\n$$\nP(x^{n})=\\prod_{i=1}^{n}P(x_{i}).\n$$\nWith $P(0)P(1)$, the single most probable sequence is the one that uses the most probable symbol in every position, namely $0^{n}=(0,0,\\dots,0)$. Its probability is\n$$\nP(0^{n})=P(0)^{n}.\n$$\nHence its per-symbol self-information is\n$$\n-\\frac{1}{n}\\log_{2}P(0^{n})=-\\frac{1}{n}\\log_{2}\\big(P(0)^{n}\\big)=-\\log_{2}P(0).\n$$\nThe source entropy is\n$$\nH(X)=-P(0)\\log_{2}P(0)-P(1)\\log_{2}P(1).\n$$\nThe deviation that defines typicality for $0^{n}$ is therefore\n$$\n\\left|-\\frac{1}{n}\\log_{2}P(0^{n})-H(X)\\right|=\\left|-\\log_{2}P(0)-H(X)\\right|.\n$$\nSubstituting $H(X)$ and simplifying,\n$$\n\\begin{aligned}\nH(X)-\\big(-\\log_{2}P(0)\\big)\n=\\big[-P(0)\\log_{2}P(0)-P(1)\\log_{2}P(1)\\big]+\\log_{2}P(0)\\\\\n=(1-P(0))\\log_{2}P(0)-P(1)\\log_{2}P(1)\\\\\n=P(1)\\big[\\log_{2}P(0)-\\log_{2}P(1)\\big]\\\\\n=P(1)\\log_{2}\\!\\left(\\frac{P(0)}{P(1)}\\right).\n\\end{aligned}\n$$\nSince $P(0)=0.9$ and $P(1)=0.1$,\n$$\n\\left|-\\log_{2}P(0)-H(X)\\right|=H(X)+\\log_{2}P(0)=0.1\\,\\log_{2}(9).\n$$\nTo compare with $\\epsilon=0.1$, note that $98$ implies $\\log_{2}(9)\\log_{2}(8)=3$, hence\n$$\n0.1\\,\\log_{2}(9)0.1\\times 3=0.30.1.\n$$\nTherefore,\n$$\n\\left|-\\frac{1}{n}\\log_{2}P(0^{n})-H(X)\\right|0.1=\\epsilon,\n$$\nso $0^{n}$ is not in $A_{0.1}^{(n)}$. This deviation is independent of $n$, hence the conclusion holds for any sequence length $n$.\n\nThus, the single most probable sequence of any length $n$ is not a member of the typical set $A_{0.1}^{(n)}$ for this source, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1650620"}, {"introduction": "To truly understand a concept, it is often helpful to test it at its extremes. This exercise [@problem_id:1650606] applies the principles of typical sets to the simplest possible case: a deterministic source with zero uncertainty and zero entropy. You will identify the exact composition and size of the typical set in this scenario and compare it to the upper bound predicted by the Asymptotic Equipartition Property (AEP), solidifying your understanding of how entropy governs the structure of typical sequences even when no randomness is present.", "problem": "Consider a highly predictable information source, modeled as a discrete memoryless source with an alphabet $\\mathcal{X} = \\{a_1, a_2, \\dots, a_M\\}$, where $M \\geq 2$. The source is deterministic, with the probability of emitting symbol $a_1$ being $P(a_1) = 1$, and the probabilities of all other symbols being zero, i.e., $P(a_i) = 0$ for $i \\in \\{2, 3, \\dots, M\\}$. The symbols are generated as a sequence of independent and identically distributed (IID) random variables.\n\nAccording to the Asymptotic Equipartition Property (AEP), for any given $\\epsilon  0$, the set of \"typical\" sequences of length $n$, denoted $A_{\\epsilon}^{(n)}$, can be defined. A sequence $x^n = (x_1, \\dots, x_n)$ belongs to $A_{\\epsilon}^{(n)}$ if its sample entropy is close to the source entropy $H(X)$:\n$$ \\left| -\\frac{1}{n} \\log_2 P(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $P(x^n)$ is the probability of the sequence $x^n$.\n\nThe AEP also provides a well-known upper bound on the size of this typical set: $|A_{\\epsilon}^{(n)}| \\le 2^{n(H(X) + \\epsilon)}$.\n\nFor this deterministic source, find an analytic expression for the ratio $R$ of the true size of the typical set, $|A_{\\epsilon}^{(n)}|$, to its AEP upper bound. Express your answer in terms of $n$ and $\\epsilon$.", "solution": "The problem asks for the ratio $R = \\frac{|A_{\\epsilon}^{(n)}|}{2^{n(H(X) + \\epsilon)}}$. To find this ratio, we need to determine three quantities: the entropy of the source $H(X)$, the true size of the typical set $|A_{\\epsilon}^{(n)}|$, and then substitute these into the given expressions.\n\nFirst, let's calculate the Shannon entropy $H(X)$ of the source. The entropy for a discrete source with alphabet $\\mathcal{X}$ and probability distribution $P(x)$ is given by the formula:\n$$ H(X) = -\\sum_{x \\in \\mathcal{X}} P(x) \\log_2 P(x) $$\nFor the given deterministic source, the probabilities are $P(a_1) = 1$ and $P(a_i) = 0$ for $i \\geq 2$. Substituting these into the entropy formula:\n$$ H(X) = -\\left( P(a_1) \\log_2 P(a_1) + \\sum_{i=2}^{M} P(a_i) \\log_2 P(a_i) \\right) $$\n$$ H(X) = -\\left( 1 \\cdot \\log_2(1) + \\sum_{i=2}^{M} 0 \\cdot \\log_2(0) \\right) $$\nIn information theory, the expression $0 \\log_2 0$ is defined to be 0 (as $\\lim_{p\\to 0^+} p \\log p = 0$). Also, $\\log_2(1) = 0$. Therefore, the entropy is:\n$$ H(X) = -(1 \\cdot 0 + 0) = 0 $$\nThe entropy of a deterministic source is zero, which reflects the absence of uncertainty.\n\nNext, we must find the true size of the typical set, $|A_{\\epsilon}^{(n)}|$. A sequence $x^n$ is in the typical set $A_{\\epsilon}^{(n)}$ if it satisfies the condition:\n$$ \\left| -\\frac{1}{n} \\log_2 P(x^n) - H(X) \\right| \\le \\epsilon $$\nSubstituting $H(X) = 0$, the condition simplifies to:\n$$ \\left| -\\frac{1}{n} \\log_2 P(x^n) \\right| \\le \\epsilon $$\nSince the source is IID, the probability of a sequence $x^n = (x_1, \\dots, x_n)$ is $P(x^n) = \\prod_{k=1}^{n} P(x_k)$.\nLet's consider two cases for the composition of a sequence $x^n$:\nCase 1: The sequence $x^n$ contains at least one symbol other than $a_1$.\nIf there is at least one $x_k = a_i$ with $i \\geq 2$, then $P(x_k) = 0$. Consequently, the probability of the entire sequence is $P(x^n) = 0$. For such a sequence, $\\log_2 P(x^n) = \\log_2(0) = -\\infty$. The sample entropy $-\\frac{1}{n} \\log_2 P(x^n)$ becomes infinite. The condition $|\\infty| \\le \\epsilon$ is false for any finite, positive $\\epsilon$. Thus, any sequence containing a symbol other than $a_1$ is not in the typical set.\n\nCase 2: The sequence $x^n$ consists only of the symbol $a_1$.\nThere is only one such sequence: $x_0^n = (a_1, a_1, \\dots, a_1)$. The probability of this sequence is:\n$$ P(x_0^n) = \\prod_{k=1}^{n} P(a_1) = 1^n = 1 $$\nLet's check if this sequence is in the typical set by substituting its probability into the condition:\n$$ \\left| -\\frac{1}{n} \\log_2(1) - 0 \\right| = \\left| -\\frac{1}{n} \\cdot 0 \\right| = 0 $$\nThe condition becomes $0 \\le \\epsilon$. Since $\\epsilon$ is given as a positive constant, this inequality is always true.\nTherefore, the only sequence in the typical set is $x_0^n$. This means the size of the typical set is exactly 1.\n$$ |A_{\\epsilon}^{(n)}| = 1 $$\n\nNow we can calculate the ratio $R$. The AEP upper bound is given as $2^{n(H(X) + \\epsilon)}$. Substituting $H(X) = 0$, the bound is:\n$$ \\text{Upper Bound} = 2^{n(0 + \\epsilon)} = 2^{n\\epsilon} $$\nFinally, the ratio $R$ is the true size divided by this upper bound:\n$$ R = \\frac{|A_{\\epsilon}^{(n)}|}{\\text{Upper Bound}} = \\frac{1}{2^{n\\epsilon}} $$\nThis can be rewritten using a negative exponent:\n$$ R = 2^{-n\\epsilon} $$", "answer": "$$\\boxed{2^{-n\\epsilon}}$$", "id": "1650606"}]}