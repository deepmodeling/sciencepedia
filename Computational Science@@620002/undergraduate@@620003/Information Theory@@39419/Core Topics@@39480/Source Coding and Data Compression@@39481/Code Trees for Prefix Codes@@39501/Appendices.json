{"hands_on_practices": [{"introduction": "A primary goal of data compression is to represent information using the fewest bits possible on average. The first step in evaluating any encoding scheme is to determine its performance for a given data source. This exercise provides a foundational practice in calculating the average codeword length, a crucial metric that quantifies the efficiency of a prefix code based on the probabilities of the source symbols. Mastering this calculation is essential for comparing different codes and understanding the principles of optimal data compression. [@problem_id:1610986]", "problem": "A research team is developing a simple compression scheme for a new type of low-power weather sensor. The sensor can report one of four possible states, which we represent with the abstract symbols $s_1, s_2, s_3, \\text{and } s_4$. To transmit the data efficiently, the team proposes a binary encoding scheme that maps these symbols to the following codewords:\n- $s_1 \\rightarrow 01$\n- $s_2 \\rightarrow 1$\n- $s_3 \\rightarrow 000$\n- $s_4 \\rightarrow 001$\n\nFor a stream of such codewords to be unambiguously and instantaneously decodable, the code must be a prefix code. A code is defined as a prefix code if no codeword in the set is a prefix of any other codeword.\n\nBased on historical weather patterns, the probabilities of occurrence for each symbol are estimated as follows:\n- $P(s_1) = 0.20$\n- $P(s_2) = 0.50$\n- $P(s_3) = 0.15$\n- $P(s_4) = 0.15$\n\nCalculate the average codeword length for this encoding scheme. The average codeword length is the expected value of the lengths of the codewords, weighted by their probabilities. Provide your answer as a single decimal number.", "solution": "A prefix code requires that no codeword is a prefix of any other. The given set `{01, 1, 000, 001}` is prefix-free because none of these strings begins with another entire codeword from the set.\n\nLet the codeword lengths be $l(s_1)=2$, $l(s_2)=1$, $l(s_3)=3$, and $l(s_4)=3$, corresponding to `01`, `1`, `000`, and `001` respectively.\n\nThe average codeword length $L$ is the expected value of the codeword lengths:\n$$\nL=\\sum_{i=1}^{4}P(s_{i})\\,l(s_{i})=P(s_{1})l(s_{1})+P(s_{2})l(s_{2})+P(s_{3})l(s_{3})+P(s_{4})l(s_{4}).\n$$\nSubstituting the given probabilities and lengths:\n$$\nL=0.20\\cdot 2+0.50\\cdot 1+0.15\\cdot 3+0.15\\cdot 3=0.4+0.5+0.45+0.45=1.8.\n$$\nTherefore, the average codeword length is $1.8$.", "answer": "$$\\boxed{1.8}$$", "id": "1610986"}, {"introduction": "Understanding why an algorithm is optimal often involves exploring what happens when its core logic is altered. This thought experiment challenges you to analyze a flawed version of the Huffman coding procedure, where the most probable symbols are merged first instead of the least probable ones. By calculating the outcome of this incorrect \"greedy\" approach, you will gain a much deeper appreciation for the elegant logic behind the real Huffman algorithm and why its counter-intuitive choice leads to the shortest possible average codeword length. [@problem_id:1610961]", "problem": "In a course on information theory, a student named Alex is tasked with generating a prefix code for a discrete memoryless source. The source has an alphabet of five symbols, $\\mathcal{A} = \\{S_1, S_2, S_3, S_4, S_5\\}$, with corresponding probabilities of occurrence given by the set $\\mathcal{P} = \\{0.40, 0.25, 0.20, 0.10, 0.05\\}$.\n\nAlex correctly remembers that the Huffman algorithm constructs a binary tree by iteratively merging nodes, but misremembers the core principle. Instead of merging the two nodes with the *lowest* probabilities, Alex's procedure merges the two nodes with the *highest* probabilities at each step.\n\nThe procedure is as follows:\n1. Start with a list of nodes, one for each symbol, weighted by its probability.\n2. Identify the two nodes in the list with the highest probabilities.\n3. Merge these two nodes into a new parent node. The probability of this new node is the sum of the probabilities of the two child nodes.\n4. When assigning bits for the code, the branch leading to the child node with the higher probability is assigned a '0', and the branch leading to the child node with the lower probability is assigned a '1'.\n5. Replace the two child nodes with the new parent node in the list.\n6. Repeat steps 2-5 until only one node, the root of the tree, remains.\n\nCalculate the expected codeword length of the prefix code generated by Alex's incorrect procedure. Express your answer in bits per symbol, rounded to three significant figures.", "solution": "The source symbols have probabilities $p(S_1)=0.40$, $p(S_2)=0.25$, $p(S_3)=0.20$, $p(S_4)=0.10$, and $p(S_5)=0.05$. The incorrect procedure merges at each step the two nodes with the highest probabilities. The expected codeword length is $L=\\sum_{i}p(S_{i})\\,\\ell(S_{i})$, where $\\ell(S_{i})$ is the depth (codeword length) of symbol $S_{i}$ in the resulting binary tree. Each time a symbol’s node is merged into a new parent, its depth increases by $1$.\n\nI simulate the merges and track depth increments:\n\nFirst merge: merge $0.40$ and $0.25$ to form $0.65$. Symbols $S_{1}$ and $S_{2}$ each gain depth $+1$.\n\nSecond merge: merge $0.65$ and $0.20$ to form $0.85$. Symbols $S_{1}$, $S_{2}$, and $S_{3}$ each gain depth $+1$.\n\nThird merge: merge $0.85$ and $0.10$ to form $0.95$. Symbols $S_{1}$, $S_{2}$, $S_{3}$, and $S_{4}$ each gain depth $+1$.\n\nFourth merge: merge $0.95$ and $0.05$ to form $1.00$. All symbols gain depth $+1$.\n\nTherefore, the final codeword lengths are $\\ell(S_{1})=4$, $\\ell(S_{2})=4$, $\\ell(S_{3})=3$, $\\ell(S_{4})=2$, and $\\ell(S_{5})=1$. The expected codeword length is\n$$\nL=0.40\\cdot 4+0.25\\cdot 4+0.20\\cdot 3+0.10\\cdot 2+0.05\\cdot 1.\n$$\nCompute term by term:\n$$\n0.40\\cdot 4=1.60,\\quad 0.25\\cdot 4=1.00,\\quad 0.20\\cdot 3=0.60,\\quad 0.10\\cdot 2=0.20,\\quad 0.05\\cdot 1=0.05,\n$$\nso\n$$\nL=1.60+1.00+0.60+0.20+0.05=3.45.\n$$\nEquivalently, for any binary prefix tree built by successive merges, $L$ equals the sum of the combined probabilities at each merge. The merges produce combined probabilities $0.65$, $0.85$, $0.95$, and $1.00$, whose sum is\n$$\n0.65+0.85+0.95+1.00=3.45,\n$$\nconfirming the result. Rounded to three significant figures, the expected codeword length is $3.45$ bits per symbol.", "answer": "$$\\boxed{3.45}$$", "id": "1610961"}, {"introduction": "Optimal codes are tailored to specific statistical properties of a data source, but what happens when those properties change over time? This practice problem moves from static theory to a dynamic, real-world scenario where a once-optimal Huffman code becomes inefficient because the source's symbol probabilities have shifted. By calculating the \"inefficiency penalty\"—the difference between the code's current performance and a newly optimized one—you will quantify the practical cost of using a non-adaptive code and understand the motivation behind adaptive compression systems. [@problem_id:1610971]", "problem": "A data compression system uses a Huffman code to losslessly compress data from a discrete memoryless source. The source emits symbols from the set $S = \\{s_1, s_2, s_3, s_4, s_5\\}$. Initially, the system is configured for a stationary probability distribution $P_0$, where the probabilities of the symbols are given by:\n$p_0(s_1) = 0.45$\n$p_0(s_2) = 0.25$\n$p_0(s_3) = 0.15$\n$p_0(s_4) = 0.10$\n$p_0(s_5) = 0.05$\n\nBased on this distribution $P_0$, an optimal Huffman code is generated, fixing a specific set of codeword lengths $\\{l_0(s_1), l_0(s_2), l_0(s_3), l_0(s_4), l_0(s_5)\\}$ for the symbols.\n\nAfter some time, the underlying process generating the data changes, and the source statistics are updated to a new stationary distribution, $P_1$. The new probabilities are:\n$p_1(s_1) = 0.05$\n$p_1(s_2) = 0.25$\n$p_1(s_3) = 0.15$\n$p_1(s_4) = 0.10$\n$p_1(s_5) = 0.45$\n\nThe system, however, does not update its coding scheme and continues to use the original codeword lengths, $l_0$, that were optimal for $P_0$. This results in a suboptimal average codeword length.\n\nLet $L_{sub}$ be the average codeword length when using the original code (with lengths $l_0$) on the source with the new probabilities $P_1$.\nLet $L_{opt}$ be the new optimal average codeword length that would be achieved by generating a new Huffman code specifically for the distribution $P_1$.\n\nThe \"inefficiency penalty\" of not updating the code is the difference $\\Delta L = L_{sub} - L_{opt}$. Calculate the value of this penalty. Round your final answer to four significant figures.", "solution": "The problem asks for the inefficiency penalty $\\Delta L = L_{sub} - L_{opt}$, which arises from using an old Huffman code on a data source with changed statistics. We need to calculate the average codeword length for the suboptimal case ($L_{sub}$) and the new optimal case ($L_{opt}$), and then find their difference.\n\n**Step 1: Determine the codeword lengths for the initial Huffman code ($l_0$).**\nThe initial code is built using the probability distribution $P_0$: $\\{p_0(s_1)=0.45, p_0(s_2)=0.25, p_0(s_3)=0.15, p_0(s_4)=0.10, p_0(s_5)=0.05\\}$.\nThe Huffman algorithm proceeds by iteratively merging the two nodes with the lowest probabilities.\n\n1.  Initial list of nodes (symbol, probability):\n    ($s_5$, 0.05), ($s_4$, 0.10), ($s_3$, 0.15), ($s_2$, 0.25), ($s_1$, 0.45).\n\n2.  Merge the two lowest probability nodes, $s_5$ and $s_4$. This creates a new internal node, let's call it $n_1$, with probability $0.05 + 0.10 = 0.15$.\n    The list becomes: ($s_3$, 0.15), ($n_1$, 0.15), ($s_2$, 0.25), ($s_1$, 0.45).\n\n3.  Merge the two new lowest probability nodes, $s_3$ and $n_1$. This creates a new internal node, $n_2$, with probability $0.15 + 0.15 = 0.30$.\n    The list becomes: ($s_2$, 0.25), ($n_2$, 0.30), ($s_1$, 0.45).\n\n4.  Merge the two lowest probability nodes, $s_2$ and $n_2$. This creates a new internal node, $n_3$, with probability $0.25 + 0.30 = 0.55$.\n    The list becomes: ($s_1$, 0.45), ($n_3$, 0.55).\n\n5.  Merge the final two nodes, $s_1$ and $n_3$, to form the root of the tree with probability $1.0$.\n\nNow, we can determine the depth (codeword length) of each symbol in this tree, $T_0$.\n-   $s_1$ is a direct child of the root. Its depth is 1. So, $l_0(s_1) = 1$.\n-   $n_3$ is a direct child of the root.\n-   $s_2$ is a child of $n_3$. Its depth is 2. So, $l_0(s_2) = 2$.\n-   $n_2$ is a child of $n_3$.\n-   $s_3$ is a child of $n_2$. Its depth is 3. So, $l_0(s_3) = 3$.\n-   $n_1$ is a child of $n_2$.\n-   $s_4$ is a child of $n_1$. Its depth is 4. So, $l_0(s_4) = 4$.\n-   $s_5$ is a child of $n_1$. Its depth is 4. So, $l_0(s_5) = 4$.\n\nThe initial codeword lengths are: $\\{l_0(s_1), l_0(s_2), l_0(s_3), l_0(s_4), l_0(s_5)\\} = \\{1, 2, 3, 4, 4\\}$.\n\n**Step 2: Calculate the suboptimal average length $L_{sub}$.**\n$L_{sub}$ is the average length using the old lengths $l_0$ with the new probabilities $P_1$.\n$P_1$: $\\{p_1(s_1)=0.05, p_1(s_2)=0.25, p_1(s_3)=0.15, p_1(s_4)=0.10, p_1(s_5)=0.45\\}$.\n$$L_{sub} = \\sum_{i=1}^{5} p_1(s_i) l_0(s_i)$$\n$$L_{sub} = p_1(s_1)l_0(s_1) + p_1(s_2)l_0(s_2) + p_1(s_3)l_0(s_3) + p_1(s_4)l_0(s_4) + p_1(s_5)l_0(s_5)$$\n$$L_{sub} = (0.05)(1) + (0.25)(2) + (0.15)(3) + (0.10)(4) + (0.45)(4)$$\n$$L_{sub} = 0.05 + 0.50 + 0.45 + 0.40 + 1.80 = 3.20$$\n\n**Step 3: Determine the codeword lengths for the new optimal Huffman code ($l_1$).**\nThe new code is built using the probability distribution $P_1$. The set of probability values in $P_1$ is $\\{0.05, 0.10, 0.15, 0.25, 0.45\\}$. This is the *exact same set* of probability values as in $P_0$, just assigned to different symbols. Since the structure of the Huffman tree depends only on the set of probability values, the new optimal tree $T_1$ will have the same structure as $T_0$. Consequently, the set of optimal codeword lengths will also be the same: $\\{1, 2, 3, 4, 4\\}$.\n\nThe key difference is which symbol gets which length. In an optimal Huffman code, the highest probability symbol gets the shortest codeword length.\n-   $p_1(s_5) = 0.45$ (highest), so $l_1(s_5) = 1$.\n-   $p_1(s_2) = 0.25$ (second highest), so $l_1(s_2) = 2$.\n-   $p_1(s_3) = 0.15$ (third highest), so $l_1(s_3) = 3$.\n-   $p_1(s_4) = 0.10$ (fourth highest), so $l_1(s_4) = 4$.\n-   $p_1(s_1) = 0.05$ (lowest), so $l_1(s_1) = 4$.\n\nThe new optimal codeword lengths are: $\\{l_1(s_1), l_1(s_2), l_1(s_3), l_1(s_4), l_1(s_5)\\} = \\{4, 2, 3, 4, 1\\}$.\n\n**Step 4: Calculate the new optimal average length $L_{opt}$.**\n$L_{opt}$ is the average length using the new lengths $l_1$ with the new probabilities $P_1$.\n$$L_{opt} = \\sum_{i=1}^{5} p_1(s_i) l_1(s_i)$$\n$$L_{opt} = p_1(s_1)l_1(s_1) + p_1(s_2)l_1(s_2) + p_1(s_3)l_1(s_3) + p_1(s_4)l_1(s_4) + p_1(s_5)l_1(s_5)$$\n$$L_{opt} = (0.05)(4) + (0.25)(2) + (0.15)(3) + (0.10)(4) + (0.45)(1)$$\n$$L_{opt} = 0.20 + 0.50 + 0.45 + 0.40 + 0.45 = 2.00$$\n\n**Step 5: Calculate the inefficiency penalty $\\Delta L$.**\n$$\\Delta L = L_{sub} - L_{opt}$$\n$$\\Delta L = 3.20 - 2.00 = 1.20$$\nRounding the result to four significant figures gives $1.200$.", "answer": "$$\\boxed{1.200}$$", "id": "1610971"}]}