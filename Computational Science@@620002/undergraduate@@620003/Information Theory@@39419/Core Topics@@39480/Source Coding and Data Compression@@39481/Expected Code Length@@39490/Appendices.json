{"hands_on_practices": [{"introduction": "The cornerstone of creating efficient, prefix-free codes is the Huffman algorithm, a greedy procedure that guarantees an optimal solution for minimizing the average number of bits per symbol. This first exercise provides a direct application of this fundamental procedure, challenging you to construct an optimal code for a given set of source probabilities [@problem_id:1623278]. Mastering this process is the first step toward understanding how to minimize the average data rate required to transmit information.", "problem": "A deep-space probe is designed to monitor and classify five distinct types of cosmic ray events, labeled A, B, C, D, and E. The probe operates autonomously and, due to power limitations for its transmitter, must encode the classification of each detected event into a sequence of binary digits (bits) using the most efficient scheme possible. From extensive prior observations, the long-term probabilities for the detection of each event type within a standard observation window have been determined. Events A and B each occur with a probability of $1/3$. Events C, D, and E each occur with a probability of $1/9$.\n\nAssuming the goal is to minimize the average number of bits transmitted per event, what is this minimum possible average length? Express your answer as a single fraction.", "solution": "To find the minimum expected length, we apply the Huffman coding algorithm. The source probabilities are $p_A=p_B=1/3$ and $p_C=p_D=p_E=1/9$.\n\nThe Huffman procedure involves iteratively merging the two lowest-probability nodes:\n1.  **Step 1:** Merge two $1/9$ symbols (e.g., C and D) to form a node with probability $2/9$. The list of weights becomes $\\{1/3, 1/3, 1/9, 2/9\\}$.\n2.  **Step 2:** Merge the two new smallest probabilities, the remaining $1/9$ symbol (E) and the $2/9$ node, to create a composite node, $N_{CDE}$, with probability $1/3$.\n3.  **Step 3:** The list of weights is now $\\{1/3, 1/3, 1/3\\}$, corresponding to symbols A, B, and the composite node $N_{CDE}$. There is a tie. The Huffman algorithm allows any two to be merged. We merge the two original symbols, A and B, into a node $N_{AB}$ of weight $2/3$. The list becomes $\\{1/3, 2/3\\}$.\n4.  **Step 4:** Merge the final two nodes, $N_{CDE}$ and $N_{AB}$, to form the root of the tree.\n\nFrom this tree structure, we determine the codeword length (depth) for each symbol:\n- The root's children are $N_{CDE}$ (weight 1/3) and $N_{AB}$ (weight 2/3), each at depth 1.\n- The children of $N_{AB}$ are A and B. Their depth is $1 (\\text{from root to } N_{AB}) + 1 = 2$. Thus, $l_A = 2$ and $l_B = 2$.\n- The sub-tree for $N_{CDE}$ was formed by merging E with the node for C and D. Thus, relative to $N_{CDE}$, E has a depth of 1, and C and D have a depth of 2.\n- Since $N_{CDE}$ is at depth 1 from the main root, the total depths are $l_E = 1+1=2$, and $l_C=l_D=1+2=3$.\n\nThe set of codeword lengths is $\\{2, 2, 2, 3, 3\\}$. The two symbols with probability $1/3$ (A, B) are assigned length 2. For the three symbols with probability $1/9$ (C, D, E), one is assigned length 2 and two are assigned length 3. The expected length $L$ is:\n$$\nL = \\left( \\frac{1}{3} \\cdot 2 + \\frac{1}{3} \\cdot 2 \\right) + \\left( \\frac{1}{9} \\cdot 2 + \\frac{1}{9} \\cdot 3 + \\frac{1}{9} \\cdot 3 \\right)\n$$\n$$\nL = \\frac{4}{3} + \\frac{2+3+3}{9} = \\frac{4}{3} + \\frac{8}{9} = \\frac{12}{9} + \\frac{8}{9} = \\frac{20}{9}.\n$$\nBy the optimality of Huffman coding, this is the minimum possible average number of bits per event. As a consistency check, the source entropy in bits is\n$$\nH=-2\\cdot \\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)-3\\cdot \\frac{1}{9}\\log_{2}\\!\\left(\\frac{1}{9}\\right)=\\frac{4}{3}\\log_{2}(3),\n$$\nand indeed $H \\leq \\frac{20}{9}$, consistent with the coding bound.", "answer": "$$\\boxed{\\frac{20}{9}}$$", "id": "1623278"}, {"introduction": "While practical codes have integer-length codewords, it is theoretically insightful to consider an ideal scenario where lengths can be real numbers, precisely matching the information content of each symbol. This thought experiment [@problem_id:1623255] explores the crucial concept of cross-entropy: the performance penalty incurred when a code designed for an assumed probability distribution is used on a source with a different, true distribution. This problem illuminates the cost of inaccurate statistical modeling and provides a bridge between practical coding and foundational information theory.", "problem": "A communications engineer is tasked with designing a source code for a memoryless binary source whose alphabet is $\\mathcal{A} = \\{0, 1\\}$. The engineer designs an optimal binary prefix code based on the assumption that the source is a Bernoulli process with parameter $p_0$, where the probability of generating a '1' is $P_0(1) = p_0$ and the probability of generating a '0' is $P_0(0) = 1-p_0$. For such an ideal optimal code, the length of the codeword assigned to a symbol $s$ with an assumed probability $p(s)$ is given by $l(s) = -\\log_2(p(s))$.\n\nAfter the code is constructed, it is deployed to encode a different source. This new source is also a memoryless Bernoulli process, but its true parameter is $p_1$. Thus, the actual probability of generating a '1' is $P_1(1) = p_1$ and the probability of generating a '0' is $P_1(0) = 1-p_1$. The parameters $p_0$ and $p_1$ are both in the range $(0, 1)$.\n\nDerive an expression for the expected codeword length, $L$, when the code designed for the $p_0$ source is used to encode the $p_1$ source. Your answer should be an analytic expression in terms of $p_0$ and $p_1$.", "solution": "We are given a memoryless binary source with alphabet $\\mathcal{A}=\\{0,1\\}$ and an ideal code designed for an assumed Bernoulli parameter $p_{0}$, so that for a symbol $s\\in\\{0,1\\}$ with assumed probability $P_{0}(s)$, the codeword length is\n$$\nl(s)=-\\log_{2}\\big(P_{0}(s)\\big).\n$$\nFor the binary symbols, this gives\n$$\nl(1)=-\\log_{2}(p_{0}),\\qquad l(0)=-\\log_{2}(1-p_{0}).\n$$\nThe code is used on a different Bernoulli source with true parameter $p_{1}$, so that $P_{1}(1)=p_{1}$ and $P_{1}(0)=1-p_{1}$. The expected codeword length under the true source is the expectation of the codeword length with respect to the true distribution:\n$$\nL=\\sum_{s\\in\\{0,1\\}}P_{1}(s)\\,l(s).\n$$\nSubstituting the expressions for $l(s)$, we obtain\n$$\nL=P_{1}(1)\\,l(1)+P_{1}(0)\\,l(0)=p_{1}\\big(-\\log_{2}(p_{0})\\big)+(1-p_{1})\\big(-\\log_{2}(1-p_{0})\\big).\n$$\nTherefore,\n$$\nL=-p_{1}\\log_{2}(p_{0})-(1-p_{1})\\log_{2}(1-p_{0}).\n$$\nThis expression is the cross-entropy between the true distribution $P_1$ and the assumed distribution $P_0$. The conditions $p_{0},p_{1}\\in(0,1)$ ensure all logarithms are well-defined.", "answer": "$$\\boxed{-p_{1}\\log_{2}(p_{0})-(1-p_{1})\\log_{2}(1-p_{0})}$$", "id": "1623255"}, {"introduction": "Real-world applications can impose constraints that go beyond simply minimizing the expected length. This advanced problem [@problem_id:1623266] introduces an alphabetic constraint, where the lexicographical order of binary codewords must correspond to the intrinsic order of the source symbols. You will find that the standard Huffman algorithm's greedy approach is no longer optimal, requiring a more general and powerful method like dynamic programming to solve this constrained optimization problem.", "problem": "A data acquisition system monitors a physical process that can be in one of five discrete states, denoted by the ordered alphabet $S = \\{s_1, s_2, s_3, s_4, s_5\\}$, where the index implies a natural ordering ($s_1 < s_2 < s_3 < s_4 < s_5$). After a long observation period, the stationary probability distribution for these states has been determined as follows:\n$p(s_1) = 0.05$\n$p(s_2) = 0.30$\n$p(s_3) = 0.05$\n$p(s_4) = 0.40$\n$p(s_5) = 0.20$\n\nFor efficient storage, the stream of state symbols must be encoded into a binary prefix code. A critical requirement for the system's data-retrieval algorithm is that the lexicographical order of the binary codewords must correspond to the intrinsic order of the states. That is, if $C(s_i)$ is the codeword for state $s_i$, the condition $C(s_i) < C(s_j)$ must hold for all $i < j$, where the comparison is lexicographical.\n\nDetermine the expected codeword length for an optimal binary prefix code that satisfies this alphabetic constraint. Express your answer as a decimal rounded to three significant figures.", "solution": "We seek an optimal binary alphabetic (ordered) prefix code for ordered states $s_{1}<s_{2}<s_{3}<s_{4}<s_{5}$ with probabilities $w_{1}=0.05$, $w_{2}=0.30$, $w_{3}=0.05$, $w_{4}=0.40$, $w_{5}=0.20$. For an alphabetic code, the leaves must appear in order in a full binary tree. The expected codeword length is the weighted external path length $L=\\sum_{i=1}^{5}w_{i}d_{i}$, where $d_{i}$ is the depth of leaf $i$.\n\nWe use dynamic programming to solve this. For any interval of symbols $[i,j]$, let $W(i,j)=\\sum_{k=i}^{j}w_{k}$ be the sum of probabilities, and let $F(i,j)$ be the minimal weighted path length for a sub-tree built on symbols $i$ through $j$. The recurrence relation is\n$$\nF(i,i)=0,\\quad F(i,j)=\\min_{r\\in\\{i,\\dots,j-1\\}}\\big(F(i,r)+F(r+1,j)\\big)+W(i,j).\n$$\nThe final answer is $F(1,5)$.\nFirst, compute necessary sums $W(i,j)$: $W(1,2)=0.35$, $W(2,3)=0.35$, $W(3,4)=0.45$, $W(4,5)=0.60$, $W(1,3)=0.40$, $W(2,4)=0.75$, $W(3,5)=0.65$, $W(1,4)=0.80$, $W(2,5)=0.95$, $W(1,5)=1.00$.\n\nLength 2 intervals:\n$$\nF(1,2)=0.35,\\quad F(2,3)=0.35,\\quad F(3,4)=0.45,\\quad F(4,5)=0.60.\n$$\n\nLength 3 intervals:\n$$\nF(1,3)=W(1,3)+\\min\\{F(1,1)+F(2,3), F(1,2)+F(3,3)\\}=0.40+0.35=0.75\n$$\n$$\nF(2,4)=W(2,4)+\\min\\{F(2,2)+F(3,4), F(2,3)+F(4,4)\\}=0.75+0.35=1.10\n$$\n$$\nF(3,5)=W(3,5)+\\min\\{F(3,3)+F(4,5), F(3,4)+F(5,5)\\}=0.65+0.45=1.10\n$$\n\nLength 4 intervals:\n$$\nF(1,4)=W(1,4)+\\min\\{F(1,1)+F(2,4), F(1,2)+F(3,4), F(1,3)+F(4,4)\\}=0.80+\\min\\{1.10, 0.35+0.45, 0.75\\}=0.80+0.75=1.55\n$$\n$$\nF(2,5)=W(2,5)+\\min\\{F(2,2)+F(3,5), F(2,3)+F(4,5), F(2,4)+F(5,5)\\}=0.95+\\min\\{1.10, 0.35+0.60, 1.10\\}=0.95+0.95=1.90\n$$\n\nLength 5 interval:\n$$\nF(1,5)=W(1,5)+\\min\\{F(1,1)+F(2,5), F(1,2)+F(3,5), F(1,3)+F(4,5), F(1,4)+F(5,5)\\}\n$$\n$$\nF(1,5)=1.00+\\min\\{1.90, 0.35+1.10, 0.75+0.60, 1.55\\}=1.00+1.35=2.35.\n$$\n\nThe minimal expected codeword length for an optimal alphabetic binary prefix code is $2.35$. This result corresponds to a tree where the initial split is between $\\{s_1,s_2,s_3\\}$ and $\\{s_4,s_5\\}$. One possible set of codeword lengths is $\\{2, 3, 3, 2, 2\\}$, which gives an expected length of $0.05\\cdot 2+0.30\\cdot 3+0.05\\cdot 3+0.40\\cdot 2+0.20\\cdot 2=0.10+0.90+0.15+0.80+0.40=2.35$.", "answer": "$$\\boxed{2.35}$$", "id": "1623266"}]}