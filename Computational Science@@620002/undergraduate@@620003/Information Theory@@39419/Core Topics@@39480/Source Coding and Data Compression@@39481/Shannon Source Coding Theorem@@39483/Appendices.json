{"hands_on_practices": [{"introduction": "The journey into data compression begins with a fundamental question: what is the absolute minimum number of bits required to represent information from a source? Shannon's Source Coding Theorem provides a beautiful answer through the concept of entropy, which measures the average uncertainty or \"surprise\" of a source's outcomes. This first practice [@problem_id:1657634] will guide you through calculating the entropy for a simple information source, giving you a tangible value for its theoretical compression limit.", "problem": "An interstellar probe, the \"Argo,\" is designed to explore a distant star system. Its communication system transmits data packets back to Earth. Each packet is classified into one of three categories based on its scientific importance and system status: \"Nominal\" (representing routine environmental readings), \"Alert\" (indicating an unexpected but non-critical scientific find), or \"Error\" (signaling a malfunction in one of the probe's subsystems).\n\nBased on long-term operational statistics from similar missions, the probabilities of these packet types are as follows:\n- The probability of a \"Nominal\" packet is $P_N = 0.80$.\n- The probability of an \"Alert\" packet is $P_A = 0.15$.\n- The probability of an \"Error\" packet is $P_E = 0.05$.\n\nBefore transmission, these packets are encoded into a binary stream. According to the principles of information theory, there is a fundamental limit on the efficiency of any possible lossless compression scheme for this data source. Determine the theoretical minimum average number of bits required to represent one such data packet. Express your answer in bits per packet, rounded to four significant figures.", "solution": "The theoretical minimum average number of bits per packet for a discrete memoryless source is given by the Shannon entropy in bits,\n$$\nH = -\\sum_{i} p_{i}\\log_{2}(p_{i}).\n$$\nFor the three packet types with probabilities $P_{N} = 0.80$, $P_{A} = 0.15$, and $P_{E} = 0.05$, the entropy is\n$$\nH = -\\left(0.80\\log_{2}(0.80) + 0.15\\log_{2}(0.15) + 0.05\\log_{2}(0.05)\\right).\n$$\nEvaluating each term numerically,\n$$\n\\log_{2}(0.80) \\approx -0.3219280949,\\quad \\log_{2}(0.15) \\approx -2.736965594,\\quad \\log_{2}(0.05) \\approx -4.3219280949,\n$$\nso\n$$\n-0.80\\log_{2}(0.80) \\approx 0.2575424759,\\quad -0.15\\log_{2}(0.15) \\approx 0.4105448391,\\quad -0.05\\log_{2}(0.05) \\approx 0.2160964047.\n$$\nSumming gives\n$$\nH \\approx 0.2575424759 + 0.4105448391 + 0.2160964047 = 0.8841837198 \\text{ bits per packet}.\n$$\nRounding to four significant figures yields $0.8842$ bits per packet.", "answer": "$$\\boxed{0.8842}$$", "id": "1657634"}, {"introduction": "Having established how to calculate the entropy for a single symbol, we can now scale this concept up to understand its real-world impact on large datasets. The Source Coding Theorem is not just a theoretical curiosity; it sets a hard limit on how small you can losslessly compress a large file. This exercise [@problem_id:1657609] translates the abstract notion of entropy into a concrete prediction about the minimum storage size required for a massive data file.", "problem": "A deep-space probe is analyzing the composition of a distant star's photosphere by sending a stream of data packets to Earth. Each packet represents a quantum state observed. The set of all possible quantum states is modeled as a discrete memoryless source $X$. After long-term observation, mission scientists have determined that the Shannon entropy of this source is $H(X) = 2.5$ bits per symbol. For a particular observational period, the probe generates a data file containing an uncompressed sequence of exactly $1.0 \\times 10^7$ symbols from this source.\n\nTo minimize transmission time, the data must be compressed losslessly before being sent. According to Shannon's noiseless coding theorem, there is a theoretical minimum for the average number of bits needed to represent each symbol from the source. Calculate the theoretical minimum storage size required for this entire data file after applying an ideal lossless compression algorithm.\n\nExpress your answer in Mebibits (Mibit), where 1 Mebibit = $2^{20}$ bits. Round your final answer to four significant figures.", "solution": "Shannonâ€™s noiseless coding theorem states that the minimal achievable average code length per symbol for a discrete memoryless source equals its Shannon entropy. Therefore, for $N$ symbols, the theoretical minimum total number of bits required is\n$$\nB_{\\min}=N\\,H(X).\n$$\nWith $N=1.0\\times 10^{7}$ symbols and $H(X)=2.5$ bits per symbol,\n$$\nB_{\\min}=(1.0\\times 10^{7})(2.5)=2.5\\times 10^{7}\\ \\text{bits}.\n$$\nExpressing this in Mebibits using $1\\ \\text{Mibit}=2^{20}$ bits gives\n$$\nS_{\\text{Mibit}}=\\frac{2.5\\times 10^{7}}{2^{20}}\\ \\text{Mibit} \\approx 23.84\\ \\text{Mibit},\n$$\nwhere the numerical value is rounded to four significant figures as requested.", "answer": "$$\\boxed{23.84}$$", "id": "1657609"}, {"introduction": "In the real world, our compression algorithms aim to approach the theoretical limit defined by entropy, but rarely reach it perfectly due to practical constraints. To measure how close we get, we use the concept of \"coding efficiency,\" which provides a clear performance metric for any implementation. This final practice [@problem_id:1657617] asks you to calculate this efficiency, helping you understand how to benchmark a given coding scheme against the ultimate goal set by Shannon's theorem.", "problem": "A data science team is developing a compression algorithm for a stream of symbols generated by a discrete memoryless source. Through extensive statistical analysis, they have determined that the entropy of this source, denoted as $H(S)$, is $3.15$ bits per symbol. This value represents the fundamental lower bound on the average number of bits required to encode each symbol from this source.\n\nThe team's implementation of a new prefix-free coding scheme results in an average codeword length, $L$, of $3.24$ bits per symbol. The coding efficiency is a measure of how close an actual code's performance is to the theoretical optimum, and it is defined as the ratio of the theoretical minimum average length to the actual average length.\n\nCalculate the coding efficiency of the team's algorithm. Express your answer as a decimal rounded to four significant figures.", "solution": "The coding efficiency for a prefix-free code relative to a discrete memoryless source with entropy $H(S)$ and achieved average codeword length $L$ is defined as the ratio of the theoretical minimum average length to the actual average length:\n$$\\eta = \\frac{H(S)}{L}.$$\nWith $H(S) = 3.15$ bits per symbol and $L = 3.24$ bits per symbol, the efficiency is\n$$\\eta = \\frac{3.15}{3.24}.$$\nTo compute this exactly, clear decimals and simplify:\n$$\\eta = \\frac{315}{324} = \\frac{35}{36}.$$\nIts decimal expansion is\n$$\\eta = 0.972222\\ldots.$$\nRounding to four significant figures gives\n$$\\eta = 0.9722.$$", "answer": "$$\\boxed{0.9722}$$", "id": "1657617"}]}