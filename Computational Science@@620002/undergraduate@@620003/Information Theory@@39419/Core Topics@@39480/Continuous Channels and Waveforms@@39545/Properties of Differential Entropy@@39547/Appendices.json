{"hands_on_practices": [{"introduction": "This first practice problem provides a foundational exercise in computing differential entropy. We will move beyond the common Gaussian distribution and explore the Laplace distribution, which is crucial for modeling signals with heavier tails. By working through this calculation step-by-step, you will solidify your understanding of the definition of differential entropy and practice the integration techniques required to apply it. [@problem_id:1649134]", "problem": "In the field of signal processing and robust statistics, the Laplace distribution is often used to model phenomena where data exhibits heavier tails than the normal distribution, meaning outliers are more probable. Consider a noise signal in a communication channel, represented by a random variable $X$. This noise follows a Laplace distribution with a mean location parameter $\\mu$ and a positive scale parameter $b$. The probability density function (PDF) of $X$ is given by:\n$$ f(x; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) $$\nfor all real numbers $x$.\n\nYour task is to calculate the differential entropy, $h(X)$, of this random variable. Differential entropy is a measure of the average uncertainty of a continuous random variable.\n\nExpress your final answer as a symbolic expression in terms of the scale parameter $b$ and the mathematical constant $e$.", "solution": "The differential entropy $h(X)$ of a continuous random variable $X$ with probability density function (PDF) $f(x)$ is defined as:\n$$ h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\,dx $$\nFirst, let's find the expression for $\\ln(f(x))$ for the given Laplace PDF.\n$$ f(x) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) $$\nTaking the natural logarithm of both sides, we get:\n$$ \\ln(f(x)) = \\ln\\left(\\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)\\right) $$\nUsing the properties of logarithms, $\\ln(AB) = \\ln(A) + \\ln(B)$ and $\\ln(\\exp(C)) = C$, we have:\n$$ \\ln(f(x)) = \\ln\\left(\\frac{1}{2b}\\right) - \\frac{|x-\\mu|}{b} = -\\ln(2b) - \\frac{|x-\\mu|}{b} $$\nNow, we substitute this expression into the integral for differential entropy:\n$$ h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\left(-\\ln(2b) - \\frac{|x-\\mu|}{b}\\right) \\,dx $$\nWe can split this into two separate integrals:\n$$ h(X) = -\\int_{-\\infty}^{\\infty} f(x)(-\\ln(2b)) \\,dx - \\int_{-\\infty}^{\\infty} f(x)\\left(-\\frac{|x-\\mu|}{b}\\right) \\,dx $$\n$$ h(X) = \\ln(2b) \\int_{-\\infty}^{\\infty} f(x) \\,dx + \\frac{1}{b} \\int_{-\\infty}^{\\infty} |x-\\mu| f(x) \\,dx $$\nLet's evaluate each term.\n\nFor the first term, the integral $\\int_{-\\infty}^{\\infty} f(x) \\,dx$ is the integral of a probability density function over its entire domain. By definition, this must equal 1.\n$$ \\int_{-\\infty}^{\\infty} f(x) \\,dx = 1 $$\nSo, the first term simplifies to $\\ln(2b)$.\n\nThe second term involves the integral $\\int_{-\\infty}^{\\infty} |x-\\mu| f(x) \\,dx$. This is the definition of the expectation of the random variable $|X-\\mu|$, which is the mean absolute deviation from the mean.\n$$ \\int_{-\\infty}^{\\infty} |x-\\mu| f(x) \\,dx = E[|X-\\mu|] $$\nLet's compute this expectation:\n$$ E[|X-\\mu|] = \\int_{-\\infty}^{\\infty} |x-\\mu| \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) \\,dx $$\nTo simplify this integral, we make a substitution $u = x - \\mu$, which means $du = dx$. The limits of integration remain from $-\\infty$ to $\\infty$.\n$$ E[|X-\\mu|] = \\int_{-\\infty}^{\\infty} |u| \\frac{1}{2b} \\exp\\left(-\\frac{|u|}{b}\\right) \\,du $$\nThe integrand, $|u| \\exp(-|u|/b)$, is an even function of $u$. Therefore, we can change the integration interval from $(-\\infty, \\infty)$ to $(0, \\infty)$ and multiply by 2:\n$$ E[|X-\\mu|] = 2 \\int_{0}^{\\infty} u \\frac{1}{2b} \\exp\\left(-\\frac{u}{b}\\right) \\,du = \\frac{1}{b} \\int_{0}^{\\infty} u \\exp\\left(-\\frac{u}{b}\\right) \\,du $$\nThis integral can be solved using integration by parts, $\\int U dV = UV - \\int V dU$.\nLet $U = u$ and $dV = \\exp(-u/b) \\,du$.\nThen $dU = du$ and $V = \\int \\exp(-u/b) \\,du = -b \\exp(-u/b)$.\n$$ \\int_{0}^{\\infty} u \\exp\\left(-\\frac{u}{b}\\right) \\,du = \\left[u \\left(-b \\exp\\left(-\\frac{u}{b}\\right)\\right)\\right]_{0}^{\\infty} - \\int_{0}^{\\infty} \\left(-b \\exp\\left(-\\frac{u}{b}\\right)\\right) \\,du $$\nThe first term evaluates to 0 at both limits:\n- As $u \\to \\infty$, the term $\\lim_{u\\to\\infty} -bu \\exp(-u/b) = 0$ (exponential decay dominates polynomial growth).\n- At $u=0$, the term is $-b(0)\\exp(0) = 0$.\nSo, we are left with the second term:\n$$ b \\int_{0}^{\\infty} \\exp\\left(-\\frac{u}{b}\\right) \\,du = b \\left[-b \\exp\\left(-\\frac{u}{b}\\right)\\right]_{0}^{\\infty} $$\n$$ = b \\left( \\lim_{u\\to\\infty} -b \\exp\\left(-\\frac{u}{b}\\right) - \\left(-b \\exp(0)\\right) \\right) = b(0 - (-b)) = b^2 $$\nTherefore, $E[|X-\\mu|] = \\frac{1}{b} (b^2) = b$. Note that the result is independent of $\\mu$, which is expected as differential entropy is invariant to shifts in location.\n\nNow, we substitute this result back into our expression for $h(X)$:\n$$ h(X) = \\ln(2b) + \\frac{1}{b} E[|X-\\mu|] = \\ln(2b) + \\frac{1}{b}(b) = \\ln(2b) + 1 $$\nFinally, to express the answer in terms of $b$ and $e$, we use the property that $1 = \\ln(e)$:\n$$ h(X) = \\ln(2b) + \\ln(e) = \\ln(2be) $$", "answer": "$$\\boxed{\\ln(2be)}$$", "id": "1649134"}, {"introduction": "Building on direct calculation, this exercise connects differential entropy to the practical field of signal processing and estimation theory. We will explore the relationship between the uncertainty in a signal $X$ given an observation $Y$ (quantified by conditional entropy $h(X|Y)$) and the uncertainty in the error of a simple estimator. This problem reveals a fundamental insight: conditioning on information reduces entropy, and any function of that information, $g(Y)$, cannot provide a better estimate of $X$ than one that leverages the full conditional distribution. [@problem_id:1649100]", "problem": "In a signal processing context, a zero-mean random signal $X$ with variance $\\sigma_X^2$ is transmitted over a noisy channel. The received signal is $Y$, also with a mean of zero and variance $\\sigma_Y^2$. The transmitted and received signals are jointly Gaussian, with a correlation coefficient $\\rho$ such that $|\\rho|1$.\n\nAn engineer designs a simple linear estimator to recover the original signal, given by $\\hat{X} = g(Y) = aY$, where $a$ is a fixed, real-valued gain factor. The quality of this estimation process can be analyzed from an information-theoretic perspective.\n\nTwo important quantities are the conditional differential entropy $h(X|Y)$, which quantifies the irreducible uncertainty about $X$ after observing $Y$, and the differential entropy of the estimation error, $h(X - \\hat{X})$, which quantifies the uncertainty in the error itself.\n\nCalculate the difference $\\Delta h = h(X - aY) - h(X|Y)$. Your answer should be a closed-form analytic expression in terms of $\\sigma_X, \\sigma_Y, \\rho,$ and $a$.\n\nFor your reference, the differential entropy of a one-dimensional normal random variable $W$ with variance $\\sigma_W^2$ is $h(W) = \\frac{1}{2}\\ln(2\\pi e \\sigma_W^2)$.", "solution": "Let $X$ and $Y$ be jointly Gaussian with zero means, variances $\\sigma_{X}^{2}$ and $\\sigma_{Y}^{2}$, and correlation coefficient $\\rho$, so $\\operatorname{Cov}(X,Y)=\\rho\\sigma_{X}\\sigma_{Y}$.\n\nFirst, the estimation error $X-aY$ is Gaussian with variance\n$$\n\\operatorname{Var}(X-aY)=\\operatorname{Var}(X)+a^{2}\\operatorname{Var}(Y)-2a\\,\\operatorname{Cov}(X,Y)\n=\\sigma_{X}^{2}+a^{2}\\sigma_{Y}^{2}-2a\\rho\\sigma_{X}\\sigma_{Y}.\n$$\nUsing $h(W)=\\frac{1}{2}\\ln(2\\pi e\\,\\sigma_{W}^{2})$ for a one-dimensional Gaussian $W$, we get\n$$\nh(X-aY)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,[\\sigma_{X}^{2}+a^{2}\\sigma_{Y}^{2}-2a\\rho\\sigma_{X}\\sigma_{Y}]\\big).\n$$\n\nNext, for jointly Gaussian $(X,Y)$, the conditional distribution $X|Y$ is Gaussian with variance\n$$\n\\operatorname{Var}(X|Y)=\\sigma_{X}^{2}-\\frac{\\operatorname{Cov}(X,Y)^{2}}{\\sigma_{Y}^{2}}\n=\\sigma_{X}^{2}-\\frac{\\rho^{2}\\sigma_{X}^{2}\\sigma_{Y}^{2}}{\\sigma_{Y}^{2}}\n=\\sigma_{X}^{2}(1-\\rho^{2}),\n$$\nso\n$$\nh(X|Y)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{X}^{2}(1-\\rho^{2})\\big).\n$$\n\nTherefore, the difference is\n$$\n\\Delta h\n=h(X-aY)-h(X|Y)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{\\sigma_{X}^{2}+a^{2}\\sigma_{Y}^{2}-2a\\rho\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2}(1-\\rho^{2})}\\right),\n$$\nwhich is well-defined since $|\\rho|1$ implies $1-\\rho^{2}0$, and $\\operatorname{Var}(X-aY)0$ for all real $a$ under the stated assumptions.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\!\\left(\\frac{\\sigma_{X}^{2}+a^{2}\\sigma_{Y}^{2}-2a\\rho\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2}(1-\\rho^{2})}\\right)}$$", "id": "1649100"}, {"introduction": "Our final practice problem moves from specific calculations to a powerful, general principle about entropy. We investigate how the entropy of a mixture of two probability distributions behaves as a function of the mixing proportion, $\\alpha$. This exercise demonstrates the fundamental concavity of differential entropy, a property with deep implications for optimization, information geometry, and the understanding of how combining information sources affects overall uncertainty. [@problem_id:1649115]", "problem": "A researcher is developing a new type of Random Number Generator (RNG). The device can operate in one of two modes, Mode 1 and Mode 2, each producing random numbers according to distinct, continuous probability density functions (PDFs), $p_1(x)$ and $p_2(x)$ respectively. The domains of these PDFs are subsets of the real line. To create a more complex output distribution, the researcher implements a mixing protocol. A control parameter $\\alpha \\in [0, 1]$ is set, and the final output random variable $X$ is drawn from the mixture density $f_X(x; \\alpha) = (1 - \\alpha) p_1(x) + \\alpha p_2(x)$.\n\nThe unpredictability of the output is quantified by its differential entropy, $h(X; \\alpha) = -\\int_{-\\infty}^{\\infty} f_X(x; \\alpha) \\ln(f_X(x; \\alpha)) dx$. The researcher wants to understand the fundamental properties of this entropy as a function of the mixing parameter $\\alpha$. Assume that $p_1(x)$ and $p_2(x)$ are not identical (i.e., the set of $x$ where $p_1(x) \\neq p_2(x)$ has non-zero measure) and that the individual differential entropies $h_1 = -\\int p_1(x)\\ln p_1(x)dx$ and $h_2 = -\\int p_2(x)\\ln p_2(x)dx$ are finite.\n\nConsider the following statements about $h(X; \\alpha)$. Which one is the only statement that is guaranteed to be true for *any* valid choice of distinct $p_1(x)$ and $p_2(x)$?\n\nA. The function $h(X; \\alpha)$ is always maximized at one of the endpoints, i.e., at $\\alpha=0$ or $\\alpha=1$.\n\nB. The function $h(X; \\alpha)$ is a strictly concave function of $\\alpha$ on the interval $(0, 1)$.\n\nC. There always exists a value $\\alpha^* \\in (0, 1)$ such that $h(X; \\alpha^*) > \\max(h_1, h_2)$.\n\nD. If the individual entropies are equal, i.e., $h_1 = h_2$, then $h(X; \\alpha)$ is maximized at $\\alpha=0.5$.\n\nE. The entropy of the mixture is bounded above by the linear interpolation of the individual entropies, i.e., $h(X; \\alpha) \\le (1-\\alpha) h_1 + \\alpha h_2$.", "solution": "Let the mixture density be $f_{\\alpha}(x) = (1-\\alpha)p_{1}(x) + \\alpha p_{2}(x)$ for $\\alpha \\in [0,1]$. Its differential entropy is\n$$\nh(\\alpha) \\equiv h(X;\\alpha) = -\\int f_{\\alpha}(x)\\,\\ln f_{\\alpha}(x)\\,dx.\n$$\nFirst, verify finiteness of $h(\\alpha)$ for $\\alpha \\in (0,1)$. Introduce a binary variable $Z$ taking values in $\\{1,2\\}$ with $\\mathbb{P}(Z=2)=\\alpha$ and $\\mathbb{P}(Z=1)=1-\\alpha$, and $X|Z=z \\sim p_{z}$. Then the joint entropy satisfies the chain rules\n$$\nh(X,Z) = H(Z) + h(X|Z) = h(X) + H(Z|X),\n$$\nwhere $H(\\cdot)$ denotes discrete entropy with natural logarithms. Hence\n$$\nh(X) = H(Z) + h(X|Z) - H(Z|X) = H(\\alpha) + (1-\\alpha)h_{1} + \\alpha h_{2} - H(Z|X),\n$$\nwith $H(\\alpha) = -[(1-\\alpha)\\ln(1-\\alpha) + \\alpha \\ln \\alpha]$ and $H(Z|X) \\ge 0$. Therefore\n$$\n(1-\\alpha)h_{1} + \\alpha h_{2} \\le h(\\alpha) \\le (1-\\alpha)h_{1} + \\alpha h_{2} + H(\\alpha),\n$$\nso $h(\\alpha)$ is finite on $(0,1)$ because $h_{1},h_{2}$ are finite and $H(\\alpha)$ is finite.\n\nNext, establish strict concavity of $h(\\alpha)$ in $\\alpha$. Consider the functional\n$$\n\\Phi(f) = -\\int f(x)\\ln f(x)\\,dx,\n$$\ndefined on the convex set of densities. The integrand $\\varphi(u) = -u\\ln u$ for $u>0$ satisfies\n$$\n\\varphi''(u) = -\\frac{1}{u}  0,\n$$\nso $\\varphi$ is strictly concave on $(0,\\infty)$, and hence $\\Phi$ is strictly concave: for densities $f \\neq g$ almost everywhere and any $t \\in (0,1)$,\n$$\n\\Phi((1-t)f + t g) > (1-t)\\Phi(f) + t\\Phi(g).\n$$\nFix $0 \\le \\alpha_{1}  \\alpha_{2} \\le 1$ and $t \\in (0,1)$. The affine map $\\alpha \\mapsto f_{\\alpha}$ gives\n$$\nf_{(1-t)\\alpha_{1} + t\\alpha_{2}} = (1-t)f_{\\alpha_{1}} + t f_{\\alpha_{2}}.\n$$\nSince $p_{1} \\neq p_{2}$ on a set of positive measure and $\\alpha_{1} \\neq \\alpha_{2}$, we have $f_{\\alpha_{1}} \\neq f_{\\alpha_{2}}$ on a set of positive measure because\n$f_{\\alpha_{1}}(x) - f_{\\alpha_{2}}(x) = (\\alpha_{1}-\\alpha_{2})\\bigl(p_{2}(x) - p_{1}(x)\\bigr)$.\nTherefore, by strict concavity of $\\Phi$,\n$$\nh\\bigl((1-t)\\alpha_{1} + t\\alpha_{2}\\bigr) = \\Phi\\bigl(f_{(1-t)\\alpha_{1} + t\\alpha_{2}}\\bigr) > (1-t)\\Phi(f_{\\alpha_{1}}) + t\\Phi(f_{\\alpha_{2}}) = (1-t)h(\\alpha_{1}) + t h(\\alpha_{2}).\n$$\nHence $h(\\alpha)$ is a strictly concave function of $\\alpha$ on $(0,1)$. This establishes statement B.\n\nWe now examine the other statements:\n\nA. Since $h(\\alpha)$ is strictly concave, its maximum on $[0,1]$ need not occur at the endpoints; it can occur at an interior point. For example, if $p_{1}$ and $p_{2}$ have disjoint supports, then $h(\\alpha) = (1-\\alpha)h_{1} + \\alpha h_{2} + H(\\alpha)$, whose derivative vanishes at an interior point, contradicting the claim that the maximum is always at $\\alpha=0$ or $\\alpha=1$. Thus A is not guaranteed.\n\nC. It is not guaranteed that $h(\\alpha)$ exceeds $\\max(h_{1},h_{2})$. For instance, take $p_{1}$ and $p_{2}$ as Gaussians with the same mean but distinct variances $\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}$. Then for any $\\alpha \\in (0,1)$, the mixture has variance $\\mathrm{Var}(X) = (1-\\alpha)\\sigma_{1}^{2} + \\alpha \\sigma_{2}^{2}$, and the maximum-entropy principle yields\n$$\nh(\\alpha) \\le \\frac{1}{2}\\ln\\bigl(2\\pi \\exp(1)\\,\\mathrm{Var}(X)\\bigr)  \\frac{1}{2}\\ln\\bigl(2\\pi \\exp(1)\\,\\max\\{\\sigma_{1}^{2},\\sigma_{2}^{2}\\}\\bigr) = \\max\\{h_{1},h_{2}\\},\n$$\nso $h(\\alpha)$ can be strictly below $\\max(h_{1},h_{2})$ for all $\\alpha \\in (0,1)$. Thus C is not guaranteed.\n\nD. Even if $h_{1} = h_{2}$, there is no symmetry forcing the maximizer to be at $\\alpha = 0.5$; $h(\\alpha)$ need not be symmetric about $\\alpha=0.5$ because $f_{\\alpha}$ and $f_{1-\\alpha}$ generally differ. A strictly concave function with equal endpoint values can have its unique maximum at an interior point not equal to $0.5$. Thus D is not guaranteed.\n\nE. This inequality is reversed: by concavity of $h$ in the density,\n$$\nh(\\alpha) = h\\bigl((1-\\alpha)p_{1} + \\alpha p_{2}\\bigr) \\ge (1-\\alpha)h_{1} + \\alpha h_{2},\n$$\nnot $\\le$. Therefore E is false in general.\n\nThe only statement that is guaranteed to be true for any distinct $p_{1}$ and $p_{2}$ with finite entropies is B.", "answer": "$$\\boxed{B}$$", "id": "1649115"}]}