{"hands_on_practices": [{"introduction": "This practice explores a fundamental concept from communication theory: measuring the information content of a signal corrupted by noise. By calculating the joint differential entropy of the original signal $X$ and the received signal $S = X+N$, we can quantify the total uncertainty in the system. This exercise is a perfect opportunity to apply the chain rule for differential entropy and leverage its key properties, such as invariance to translation, to arrive at an elegant solution [@problem_id:1634666].", "problem": "Consider a simple communication system model. A continuous random variable $X$, representing a signal, is uniformly distributed over the interval $[0, a]$, where $a$ is a positive constant. This signal is transmitted through a channel that introduces additive noise. The noise is represented by a random variable $N$, which follows a Gaussian distribution with a mean of zero and a variance of $\\sigma^2$. The signal $X$ and the noise $N$ are statistically independent. The received signal is given by the sum $S = X + N$.\n\nDetermine the joint differential entropy, denoted as $h(X, S)$, of the signal $X$ and the received signal $S$. Express your answer as a closed-form analytic expression in terms of the parameters $a$ and $\\sigma$. You may use standard mathematical constants like $\\pi$ and $e$ in your expression.", "solution": "Let $X \\sim \\mathrm{Unif}[0,a]$ and $N \\sim \\mathcal{N}(0,\\sigma^{2})$, independent, and define $S=X+N$. We seek $h(X,S)$.\n\nBy the chain rule for differential entropy,\n$$\nh(X,S)=h(X)+h(S|X).\n$$\nGiven $X=x$, we have $S|X=x=x+N$. Differential entropy is invariant under translations, so $h(S|X=x)=h(N)$ for every $x$, and hence\n$$\nh(S|X)=h(N).\n$$\n\nCompute each term:\n1) For the uniform distribution on $[0,a]$ with density $f_{X}(x)=\\frac{1}{a}$ on $[0,a]$,\n$$\nh(X)=-\\int_{0}^{a}\\frac{1}{a}\\ln\\!\\left(\\frac{1}{a}\\right)\\,\\mathrm{d}x=\\ln a.\n$$\n2) For a Gaussian with variance $\\sigma^{2}$,\n$$\nh(N)=\\frac{1}{2}\\ln\\!\\left(2\\pi e \\sigma^{2}\\right).\n$$\n\nTherefore,\n$$\nh(X,S)=\\ln a+\\frac{1}{2}\\ln\\!\\left(2\\pi e \\sigma^{2}\\right).\n$$\n\nEquivalently, using a linear change of variables, $(X,N)\\mapsto (X,S)$ has Jacobian determinant $1$, so $h(X,S)=h(X,N)=h(X)+h(N)$, leading to the same result.", "answer": "$$\\boxed{\\ln a+\\frac{1}{2}\\ln\\!\\left(2\\pi e \\sigma^{2}\\right)}$$", "id": "1634666"}, {"introduction": "Order statistics, such as the minimum and maximum of a set of random variables, appear frequently in fields ranging from reliability engineering to financial modeling. This exercise challenges you to first derive the joint probability distribution of the minimum and maximum of two independent uniform random variables. You will then use this distribution to calculate their joint differential entropy, providing a concrete example of how information theory quantifies the uncertainty in correlated variables derived from independent sources [@problem_id:1634692].", "problem": "Consider two independent random variables, $X$ and $Y$, both following a continuous uniform distribution on the interval $[0, 1]$. Let us define two new random variables, $U$ and $V$, as the minimum and maximum of $X$ and $Y$, respectively. That is, $U = \\min(X, Y)$ and $V = \\max(X, Y)$.\n\nThe joint differential entropy of two continuous random variables $U$ and $V$ with a joint probability density function (PDF) $f_{U,V}(u,v)$ is defined as:\n$$h(U, V) = -\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{U,V}(u,v) \\ln(f_{U,V}(u,v)) \\,du\\,dv$$\nwhere the integration is performed over the support of the distribution.\n\nCalculate the value of the joint differential entropy $h(U, V)$. Express your answer as a symbolic expression.", "solution": "Our goal is to compute the joint differential entropy $h(U, V)$ for $U = \\min(X, Y)$ and $V = \\max(X, Y)$, where $X$ and $Y$ are independent random variables uniformly distributed on $[0,1]$.\n\nFirst, we must find the joint probability density function (PDF) of the random variables $U$ and $V$, denoted as $f_{U,V}(u,v)$. The random variables $X$ and $Y$ are independent and uniformly distributed on $[0,1]$, so their joint PDF is $f_{X,Y}(x,y) = 1$ for $(x,y) \\in [0,1] \\times [0,1]$ and $0$ otherwise.\n\nThe transformation from $(X, Y)$ to $(U, V)$ maps a point in the $xy$-plane to a point in the $uv$-plane. Since $X$ and $Y$ are in $[0,1]$, their minimum $U$ and maximum $V$ must also be in $[0,1]$. Furthermore, by definition, the minimum cannot be greater than the maximum, so we must have $U \\le V$. Therefore, the support of the joint distribution of $(U,V)$ is the triangular region $\\mathcal{R} = \\{(u,v) | 0 \\le u \\le v \\le 1\\}$.\n\nNow, let's find the PDF $f_{U,V}(u,v)$ on this support region using the method of transformation of random variables. The transformation $(x,y) \\to (u,v)$ is not one-to-one. For any pair $(u, v)$ in the support region with $u < v$, there are two points in the $(X,Y)$ space that map to it: $(X,Y)=(u,v)$ and $(X,Y)=(v,u)$.\n\nThe general formula for the PDF of transformed variables for a many-to-one mapping is given by a sum over all pre-images:\n$$f_{U,V}(u,v) = \\sum_{i} f_{X,Y}(x_i, y_i) |\\det(J_i)|$$\nwhere $(x_i, y_i)$ are the pre-images of $(u,v)$ and $J_i$ is the Jacobian of the $i$-th inverse transformation.\n\nThe two inverse transformations are:\n1. $T_1^{-1}: (u,v) \\to (x,y) = (u,v)$. This corresponds to the case $X=U, Y=V$, which happens when $X \\le Y$. The Jacobian of this inverse transformation is $J_1 = \\det \\begin{pmatrix} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{pmatrix} = \\det \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = 1$.\n2. $T_2^{-1}: (u,v) \\to (x,y) = (v,u)$. This corresponds to the case $Y=U, X=V$, which happens when $Y < X$. The Jacobian of this inverse transformation is $J_2 = \\det \\begin{pmatrix} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{pmatrix} = \\det \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = -1$.\n\nFor any point $(u,v)$ in the support region $\\mathcal{R}$, both pre-images $(u,v)$ and $(v,u)$ are in the unit square $[0,1] \\times [0,1]$. Thus, $f_{X,Y}(u,v) = 1$ and $f_{X,Y}(v,u) = 1$.\nApplying the formula, we get:\n$$f_{U,V}(u,v) = f_{X,Y}(u,v) |\\det(J_1)| + f_{X,Y}(v,u) |\\det(J_2)| = 1 \\cdot |1| + 1 \\cdot |-1| = 1 + 1 = 2$$\nSo, the joint PDF is $f_{U,V}(u,v) = 2$ for $(u,v) \\in \\mathcal{R}$ and $0$ otherwise.\n\nLet's verify this is a valid PDF by integrating over the support $\\mathcal{R}$:\n$$ \\int_{\\mathcal{R}} f_{U,V}(u,v) \\,du\\,dv = \\int_{0}^{1} \\int_{0}^{v} 2 \\,du\\,dv = \\int_{0}^{1} [2u]_{u=0}^{u=v} \\,dv = \\int_{0}^{1} 2v \\,dv = [v^2]_{v=0}^{v=1} = 1 $$\nThe PDF is valid.\n\nNow we can calculate the joint differential entropy using the provided formula:\n$$h(U, V) = -\\int_{\\mathcal{R}} f_{U,V}(u,v) \\ln(f_{U,V}(u,v)) \\,du\\,dv$$\nSince $f_{U,V}(u,v) = 2$ on the support region $\\mathcal{R}$, the term $\\ln(f_{U,V}(u,v))$ is simply $\\ln(2)$, which is a constant.\n$$h(U, V) = -\\int_{\\mathcal{R}} 2 \\ln(2) \\,du\\,dv$$\nWe can pull the constant term $-2\\ln(2)$ out of the integral:\n$$h(U, V) = -2\\ln(2) \\int_{\\mathcal{R}} 1 \\,du\\,dv$$\nThe integral $\\int_{\\mathcal{R}} 1 \\,du\\,dv$ is simply the area of the support region $\\mathcal{R}$. The region $\\mathcal{R} = \\{(u,v) | 0 \\le u \\le v \\le 1\\}$ is a triangle in the $uv$-plane with vertices at $(0,0)$, $(0,1)$, and $(1,1)$. The area of this triangle is $\\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}$.\nTherefore,\n$$h(U, V) = -2\\ln(2) \\times \\frac{1}{2} = -\\ln(2)$$\nAlternatively, since the joint distribution of $(U,V)$ is uniform over the region $\\mathcal{R}$, its differential entropy is given by the logarithm of the volume (in this case, area) of the support region. The area of $\\mathcal{R}$ is $\\frac{1}{2}$. Thus, the entropy is $h(U,V) = \\ln(\\text{Area}(\\mathcal{R})) = \\ln(\\frac{1}{2}) = -\\ln(2)$.", "answer": "$$\\boxed{-\\ln(2)}$$", "id": "1634692"}, {"introduction": "How much uncertainty about one coordinate of a randomly chosen point remains after we learn the other? This problem explores this question in a simple geometric setting: a point chosen uniformly from a unit disk. You will calculate the conditional differential entropy $h(Y|X)$, which requires first determining the distribution of one variable given the other, and then averaging the resulting entropies. This practice provides a hands-on application of the definition of conditional entropy and hones your skills in working with multi-dimensional probability distributions [@problem_id:1634659].", "problem": "A two-dimensional random vector $(X, Y)$ is formed by selecting a point uniformly at random from the region defined by the inequality $x^2 + y^2 \\le 1$. The differential entropy of a continuous random variable $Z$ with probability density function $f_Z(z)$ is defined as $h(Z) = -\\int f_Z(z) \\ln(f_Z(z)) dz$, where the integral is over the support of $Z$. The conditional differential entropy $h(Y|X)$ is the expected value of the entropy of $Y$ conditioned on $X$.\n\nCalculate the conditional differential entropy $h(Y|X)$. The base of the logarithm for the entropy calculation is Euler's number $e$ (i.e., the units are nats). Express your answer as a single real number.", "solution": "Let $(X,Y)$ be uniform on the unit disk $\\{(x,y): x^{2}+y^{2} \\leq 1\\}$, so the joint pdf is $f_{X,Y}(x,y)=\\frac{1}{\\pi}$ on the disk and $0$ otherwise. The marginal density of $X$ is\n$$\nf_{X}(x)=\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\frac{1}{\\pi}\\,dy=\\frac{2}{\\pi}\\sqrt{1-x^{2}},\\quad |x|\\leq 1.\n$$\nConditioned on $X=x$, $Y$ is uniform on the interval $[-\\sqrt{1-x^{2}},\\sqrt{1-x^{2}}]$, so\n$$\nf_{Y|X}(y|x)=\\frac{1}{2\\sqrt{1-x^{2}}},\\quad |y|\\leq \\sqrt{1-x^{2}}.\n$$\nThe conditional differential entropy for a uniform distribution on an interval of length $L$ is $\\ln(L)$; equivalently,\n$$\nh(Y|X=x)=-\\int f_{Y|X}(y|x)\\ln f_{Y|X}(y|x)\\,dy=\\ln\\bigl(2\\sqrt{1-x^{2}}\\bigr).\n$$\nHence\n$$\nh(Y|X)=\\mathbb{E}[\\ln(2\\sqrt{1-X^{2}})]=\\int_{-1}^{1}f_{X}(x)\\ln(2\\sqrt{1-x^{2}})\\,dx\n=\\ln 2+\\frac{1}{2}\\int_{-1}^{1}\\frac{2}{\\pi}\\sqrt{1-x^{2}}\\,\\ln(1-x^{2})\\,dx.\n$$\nLet\n$$\nI=\\int_{-1}^{1}\\frac{2}{\\pi}\\sqrt{1-x^{2}}\\,\\ln(1-x^{2})\\,dx.\n$$\nWith the substitution $x=\\cos\\theta$ for $\\theta\\in[0,\\pi]$, we have $dx=-\\sin\\theta\\,d\\theta$, $\\sqrt{1-x^{2}}=\\sin\\theta$, and $\\ln(1-x^{2})=\\ln(\\sin^{2}\\theta)=2\\ln(\\sin\\theta)$. Thus\n$$\nI=\\frac{2}{\\pi}\\int_{-1}^{1}\\sqrt{1-x^{2}}\\,\\ln(1-x^{2})\\,dx\n=\\frac{4}{\\pi}\\int_{0}^{\\pi}\\sin^{2}\\theta\\,\\ln(\\sin\\theta)\\,d\\theta.\n$$\nDefine $K(a)=\\int_{0}^{\\pi}\\sin^{a}\\theta\\,d\\theta=\\sqrt{\\pi}\\,\\frac{\\Gamma\\!\\left(\\frac{a+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{a}{2}+1\\right)}$. Then\n$$\n\\frac{d}{da}K(a)=\\int_{0}^{\\pi}\\sin^{a}\\theta\\,\\ln(\\sin\\theta)\\,d\\theta,\n$$\nso at $a=2$,\n$$\nJ\\equiv\\int_{0}^{\\pi}\\sin^{2}\\theta\\,\\ln(\\sin\\theta)\\,d\\theta=K'(2)=K(2)\\cdot\\frac{1}{2}\\Bigl[\\psi\\!\\left(\\frac{3}{2}\\right)-\\psi(2)\\Bigr].\n$$\nUsing $K(2)=\\frac{\\pi}{2}$ and the digamma identities $\\psi(1)=-\\gamma$, $\\psi\\!\\left(\\frac{1}{2}\\right)=-\\gamma-2\\ln 2$, and $\\psi(z+1)=\\psi(z)+\\frac{1}{z}$, we get\n$$\n\\psi\\!\\left(\\frac{3}{2}\\right)=\\psi\\!\\left(\\frac{1}{2}\\right)+2=-\\gamma-2\\ln 2+2,\\quad \\psi(2)=\\psi(1)+1=-\\gamma+1,\n$$\nhence\n$$\n\\psi\\!\\left(\\frac{3}{2}\\right)-\\psi(2)=-2\\ln 2+1.\n$$\nTherefore\n$$\nJ=\\frac{\\pi}{4}\\bigl(1-2\\ln 2\\bigr),\\qquad I=\\frac{4}{\\pi}J=1-2\\ln 2.\n$$\nFinally,\n$$\nh(Y|X)=\\ln 2+\\frac{1}{2}I=\\ln 2+\\frac{1}{2}\\bigl(1-2\\ln 2\\bigr)=\\frac{1}{2}.\n$$\nThus the conditional differential entropy is $\\frac{1}{2}$ nats.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1634659"}]}