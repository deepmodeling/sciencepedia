{"hands_on_practices": [{"introduction": "This first exercise provides a foundational calculation for mutual information in a continuous setting [@problem_id:1642064]. By modeling a simple physical system of additive noise sources with Gaussian distributions, you will practice applying the core formulas for differential entropy. This problem demonstrates the elegant properties of jointly Gaussian variables and offers a clear, intuitive result for how information is shared when signals are combined.", "problem": "In a simplified model of a passive electronic circuit, two identical resistors at the same temperature produce independent thermal noise voltages, represented by the random variables $V_1$ and $V_2$. Both $V_1$ and $V_2$ are modeled as independent and identically distributed (i.i.d.) Gaussian random variables, each with a mean $\\mu$ and a variance $\\sigma^2$. An instrument measures the total voltage across these two sources in series, giving a reading $V_{\\text{total}} = V_1 + V_2$.\n\nCalculate the mutual information between the noise voltage of the first resistor, $V_1$, and the total measured voltage, $V_{\\text{total}}$. Express your answer in nats as a closed-form analytic expression.", "solution": "Let $V_{1}$ and $V_{2}$ be i.i.d. Gaussian random variables with mean $\\mu$ and variance $\\sigma^{2}$, and let $V_{\\text{total}}=V_{1}+V_{2}$. The mutual information between $V_{1}$ and $V_{\\text{total}}$ is\n$$\nI(V_{1};V_{\\text{total}})=h(V_{\\text{total}})-h(V_{\\text{total}}\\mid V_{1}),\n$$\nwhere $h(\\cdot)$ denotes differential entropy.\n\nFirst, compute the variance of $V_{\\text{total}}$. By independence,\n$$\n\\operatorname{Var}(V_{\\text{total}})=\\operatorname{Var}(V_{1}+V_{2})=\\operatorname{Var}(V_{1})+\\operatorname{Var}(V_{2})=2\\sigma^{2}.\n$$\nSince $V_{\\text{total}}$ is Gaussian with variance $2\\sigma^{2}$, its entropy is\n$$\nh(V_{\\text{total}})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\operatorname{Var}(V_{\\text{total}})\\big)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\cdot 2\\sigma^{2}\\big).\n$$\n\nNext, determine $h(V_{\\text{total}}\\mid V_{1})$. Given $V_{1}=v_{1}$, we have\n$$\nV_{\\text{total}}\\mid V_{1}=v_{1}=v_{1}+V_{2},\n$$\nwhich is Gaussian with mean $v_{1}+\\mu$ and variance $\\operatorname{Var}(V_{2})=\\sigma^{2}$. The entropy of a Gaussian depends only on its variance, hence\n$$\nh(V_{\\text{total}}\\mid V_{1})=h(V_{2})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big).\n$$\n\nTherefore, the mutual information is\n$$\nI(V_{1};V_{\\text{total}})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\cdot 2\\sigma^{2}\\big)-\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)=\\frac{1}{2}\\ln(2).\n$$\nThis result is in nats and is independent of the mean $\\mu$.", "answer": "$$\\boxed{\\frac{1}{2}\\ln(2)}$$", "id": "1642064"}, {"introduction": "While Gaussian models are powerful, many real-world signals are better described by other distributions. This practice problem [@problem_id:1642099] challenges you to calculate mutual information for a channel with uniformly distributed signals and noise. You will need to derive the output distribution through convolution and then compute its differential entropy by direct integration, providing essential skills for analyzing a broader class of communication systems.", "problem": "A communication channel is modeled as follows. The input signal is a continuous random variable $X$ that is uniformly distributed over the interval $[-a, a]$, where $a > 0$. During transmission, the signal is corrupted by additive noise, represented by another continuous random variable $Z$, which is independent of $X$ and is uniformly distributed over the interval $[-b, b]$, where $b > 0$. The received signal is thus $Y = X + Z$.\n\nAssuming that the parameters satisfy the condition $a > b$, determine the mutual information $I(X;Y)$ between the input signal $X$ and the received signal $Y$. Express your answer as a symbolic expression in terms of $a$ and $b$.", "solution": "We aim to compute the mutual information between $X$ and $Y=X+Z$ when $X \\sim \\text{Unif}([-a,a])$, $Z \\sim \\text{Unif}([-b,b])$, independent, with $a>b>0$. Using the definition of mutual information for continuous variables in an additive noise channel, we have\n$$\nI(X;Y) = h(Y) - h(Y|X) = h(Y) - h(Z),\n$$\nsince $Y|X=x$ has the same distribution as $Z$.\n\nFirst, we determine the probability density function of $Y$. The densities of $X$ and $Z$ are\n$$\nf_{X}(x) = \\frac{1}{2a} \\mathbf{1}_{\\{|x|\\le a\\}}, \\quad f_{Z}(z) = \\frac{1}{2b} \\mathbf{1}_{\\{|z|\\le b\\}}.\n$$\nThe density of $Y=X+Z$ is the convolution\n$$\nf_{Y}(y) = \\int_{-\\infty}^{\\infty} f_{X}(y-z) f_{Z}(z) \\, dz = \\frac{1}{4ab} \\cdot \\text{length}\\left( [y-a,y+a] \\cap [-b,b] \\right).\n$$\nFor $a>b$, this yields a trapezoidal density:\n- For $|y| \\le a-b$, the intersection length is $2b$, hence $f_{Y}(y) = \\frac{1}{2a}$.\n- For $a-b \\le |y| \\le a+b$, the intersection length is $a+b-|y|$, hence $f_{Y}(y) = \\frac{a+b-|y|}{4ab}$.\n- For $|y| > a+b$, $f_{Y}(y)=0$.\n\nNext, compute the differential entropy of $Y$:\n$$\nh(Y) = -\\int_{-\\infty}^{\\infty} f_{Y}(y) \\ln f_{Y}(y) \\, dy.\n$$\nBy symmetry,\n$$\nh(Y) = -2 \\left[ \\int_{0}^{a-b} \\frac{1}{2a} \\ln\\!\\left(\\frac{1}{2a}\\right) dy + \\int_{a-b}^{a+b} \\frac{a+b-y}{4ab} \\ln\\!\\left( \\frac{a+b-y}{4ab} \\right) dy \\right].\n$$\nThe first integral evaluates to\n$$\n-2 \\int_{0}^{a-b} \\frac{1}{2a} \\ln\\!\\left(\\frac{1}{2a}\\right) dy = - \\frac{a-b}{a} \\ln\\!\\left(\\frac{1}{2a}\\right) = \\left(\\frac{a-b}{a}\\right) \\ln(2a).\n$$\nFor the second integral, substitute $t=a+b-y$, so $y$ runs from $a-b$ to $a+b$ as $t$ runs from $2b$ to $0$. Then\n$$\n-2 \\int_{a-b}^{a+b} \\frac{a+b-y}{4ab} \\ln\\!\\left( \\frac{a+b-y}{4ab} \\right) dy\n= -2 \\int_{0}^{2b} \\frac{t}{4ab} \\ln\\!\\left( \\frac{t}{4ab} \\right) dt.\n$$\nLet\n$$\nJ = \\int_{0}^{2b} t \\ln\\!\\left( \\frac{t}{4ab} \\right) dt = \\int_{0}^{2b} t \\ln t \\, dt - \\ln(4ab) \\int_{0}^{2b} t \\, dt.\n$$\nUsing $\\int t \\ln t \\, dt = \\frac{t^{2}}{2} \\left( \\ln t - \\frac{1}{2} \\right)$ and $\\int t \\, dt = \\frac{t^{2}}{2}$, we get\n$$\nJ = \\left. \\frac{t^{2}}{2} \\left( \\ln t - \\frac{1}{2} \\right) \\right|_{0}^{2b} - \\ln(4ab) \\left. \\fract^{2}}{2} \\right|_{0}^{2b}\n= 2b^{2} \\left( \\ln(2b) - \\frac{1}{2} - \\ln(4ab) \\right)\n= -2 b^{2} \\ln(2a) - b^{2}.\n$$\nHence\n$$\n-2 \\int_{a-b}^{a+b} \\frac{a+b-y}{4ab} \\ln\\!\\left( \\frac{a+b-y}{4ab} \\right) dy\n= -\\frac{1}{2ab} J\n= \\frac{b}{a} \\ln(2a) + \\frac{b}{2a}.\n$$\nCombining both parts,\n$$\nh(Y) = \\left(\\frac{a-b}{a}\\right) \\ln(2a) + \\left( \\frac{b}{a} \\ln(2a) + \\frac{b}{2a} \\right)\n= \\ln(2a) + \\frac{b}{2a}.\n$$\n\nThe noise $Z$ is uniform on $[-b,b]$, so its differential entropy is\n$$\nh(Z) = \\ln(2b).\n$$\nTherefore the mutual information is\n$$\nI(X;Y) = h(Y) - h(Z) = \\left[ \\ln(2a) + \\frac{b}{2a} \\right] - \\ln(2b) = \\ln\\!\\left( \\frac{a}{b} \\right) + \\frac{b}{2a}.\n$$\nThis expression is in nats and is valid for $a>b>0$.", "answer": "$$\\boxed{\\ln\\left(\\frac{a}{b}\\right)+\\frac{b}{2a}}$$", "id": "1642099"}, {"introduction": "Our exploration advances to the crucial concept of conditional mutual information, which quantifies how information between two variables changes when a third is known [@problem_id:1642045]. This problem presents a fascinating scenario where two initially independent variables become dependent upon observing their sum. Tackling this requires leveraging the properties of multivariate Gaussian distributions and provides deep insight into the complex interplay of information in interconnected systems.", "problem": "Let $X_1$, $X_2$, and $Z$ be three independent and identically distributed (i.i.d.) random variables, each following a standard normal distribution, i.e., $X_1, X_2, Z \\sim \\mathcal{N}(0, 1)$. A fourth random variable $Y$ is constructed from their sum, defined as $Y = X_1 + X_2 + Z$.\n\nYour task is to compute the conditional mutual information between $X_1$ and $X_2$ given $Y$, which is denoted by $I(X_1; X_2 | Y)$. The result should be expressed as an analytic expression involving the natural logarithm (`\\ln`).", "solution": "Let $X_{1}, X_{2}, Z$ be independent with $X_{1}, X_{2}, Z \\sim \\mathcal{N}(0,1)$ and define $Y = X_{1} + X_{2} + Z$. The vector $(X_{1}, X_{2}, Y)$ is jointly Gaussian. We compute the covariance matrix entries:\n$$\n\\operatorname{Var}(X_{1}) = 1,\\quad \\operatorname{Var}(X_{2}) = 1,\\quad \\operatorname{Cov}(X_{1},X_{2}) = 0,\n$$\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(X_{1}) + \\operatorname{Var}(X_{2}) + \\operatorname{Var}(Z) = 3,\n$$\n$$\n\\operatorname{Cov}(X_{1},Y) = \\operatorname{Cov}(X_{1}, X_{1} + X_{2} + Z) = 1,\\quad \\operatorname{Cov}(X_{2},Y) = 1.\n$$\nThus,\n$$\n\\Sigma_{(X_{1},X_{2},Y)} =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 3\n\\end{pmatrix}.\n$$\nFor jointly Gaussian variables, the conditional covariance of $(X_{1},X_{2})$ given $Y$ is given by the Schur complement:\n$$\n\\Sigma_{(X_{1},X_{2})|Y} = \\Sigma_{(X_{1},X_{2})} - \\Sigma_{(X_{1},X_{2}),Y}\\,\\Sigma_{Y}^{-1}\\,\\Sigma_{Y,(X_{1},X_{2})}.\n$$\nHere $\\Sigma_{(X_{1},X_{2})} = I_{2}$, $\\Sigma_{(X_{1},X_{2}),Y} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, and $\\Sigma_{Y} = 3$, so\n$$\n\\Sigma_{(X_{1},X_{2})|Y} = I_{2} - \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}\\frac{1}{3}\\begin{pmatrix}1 & 1\\end{pmatrix}\n= \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X_{1}\\mid Y) = \\frac{2}{3},\\quad \\operatorname{Var}(X_{2}\\mid Y) = \\frac{2}{3},\\quad \\det\\big(\\Sigma_{(X_{1},X_{2})|Y}\\big) = \\frac{1}{3}.\n$$\nFor jointly Gaussian scalars, the conditional mutual information satisfies\n$$\nI(X_{1};X_{2}\\mid Y) = \\frac{1}{2}\\ln\\!\\left(\\frac{\\operatorname{Var}(X_{1}\\mid Y)\\,\\operatorname{Var}(X_{2}\\mid Y)}{\\det\\big(\\Sigma_{(X_{1},X_{2})|Y}\\big)}\\right).\n$$\nSubstituting the computed quantities yields\n$$\nI(X_{1};X_{2}\\mid Y) = \\frac{1}{2}\\ln\\!\\left(\\frac{\\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right)}{\\frac{1}{3}}\\right)\n= \\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right).\n$$\nEquivalently, using the conditional correlation $\\rho_{12\\mid Y} = -\\frac{1}{2}$, one can verify $I(X_{1};X_{2}\\mid Y) = -\\frac{1}{2}\\ln(1-\\rho_{12\\mid Y}^{2}) = \\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right)$.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right)}$$", "id": "1642045"}]}