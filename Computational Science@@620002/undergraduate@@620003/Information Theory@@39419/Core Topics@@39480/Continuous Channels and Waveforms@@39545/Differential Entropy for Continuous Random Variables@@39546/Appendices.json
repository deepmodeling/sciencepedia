{"hands_on_practices": [{"introduction": "The first step in mastering differential entropy is to apply its definition directly to a probability distribution. This exercise [@problem_id:1617715] provides a hands-on opportunity to calculate the entropy of a random variable following a symmetric triangular distribution. Through this practice, you will reinforce the fundamental integral definition of differential entropy and learn how to use properties like symmetry to make complex calculations more manageable.", "problem": "Consider a continuous random variable $X$ whose Probability Density Function (PDF) is a symmetric triangular distribution. The distribution is centered at $x=0$ and is non-zero only over the interval $[-w, w]$, where $w$ is a positive constant. The PDF is given by:\n$$p(x) = \\begin{cases} \\frac{1}{w} \\left(1 - \\frac{|x|}{w}\\right) & \\text{for } x \\in [-w, w] \\\\ 0 & \\text{otherwise} \\end{cases}$$\nFor a system where this parameter is $w = 5$, calculate the differential entropy $h(X)$ of the random variable. Express your answer as a single closed-form analytic expression.", "solution": "The differential entropy of a continuous random variable with PDF $p(x)$ is defined as\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} p(x)\\,\\ln p(x)\\,dx.\n$$\nFor the symmetric triangular PDF\n$$\np(x) = \\frac{1}{w}\\left(1 - \\frac{|x|}{w}\\right), \\quad x \\in [-w,w],\n$$\nand $p(x)=0$ otherwise, we use symmetry to write\n$$\nh(X) = -2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\ln\\!\\left[\\frac{1}{w}\\left(1 - \\frac{x}{w}\\right)\\right] dx.\n$$\nSplit the logarithm and integrate term by term:\n$$\nh(X) = -2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\left[\\ln\\!\\left(\\frac{1}{w}\\right) + \\ln\\!\\left(1 - \\frac{x}{w}\\right)\\right] dx\n$$\n$$\n= -2 \\ln\\!\\left(\\frac{1}{w}\\right) \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) dx \\;-\\; 2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\ln\\!\\left(1 - \\frac{x}{w}\\right) dx.\n$$\nEvaluate the first integral:\n$$\n\\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) dx = \\frac{1}{w}\\left[\\int_{0}^{w} 1\\,dx - \\frac{1}{w}\\int_{0}^{w} x\\,dx\\right] = \\frac{1}{w}\\left[w - \\frac{w^{2}}{2w}\\right] = \\frac{1}{2}.\n$$\nThus the first term equals\n$$\n-2 \\ln\\!\\left(\\frac{1}{w}\\right) \\cdot \\frac{1}{2} = -\\ln\\!\\left(\\frac{1}{w}\\right) = \\ln w.\n$$\nFor the second term, use the substitution $t = 1 - \\frac{x}{w}$, so $dx = -w\\,dt$, and as $x$ goes from $0$ to $w$, $t$ goes from $1$ to $0$. Then\n$$\n-2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\ln\\!\\left(1 - \\frac{x}{w}\\right) dx\n= -2 \\int_{1}^{0} \\frac{1}{w}\\,t \\ln t\\,(-w\\,dt)\n= -2 \\int_{0}^{1} t \\ln t\\,dt.\n$$\nUse the standard integral $\\int_{0}^{1} t \\ln t\\,dt = -\\frac{1}{4}$ to obtain\n$$\n-2 \\int_{0}^{1} t \\ln t\\,dt = -2 \\left(-\\frac{1}{4}\\right) = \\frac{1}{2}.\n$$\nCombining both parts gives\n$$\nh(X) = \\ln w + \\frac{1}{2}.\n$$\nFor $w = 5$, the differential entropy is\n$$\nh(X) = \\ln 5 + \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\ln 5 + \\frac{1}{2}}$$", "id": "1617715"}, {"introduction": "Building on the basics, we now explore how the entropy of a random variable is affected by a functional transformation. In many systems, a variable of interest is not observed directly but is a function of another variable. This practice [@problem_id:1617710] challenges you to determine the differential entropy of a new variable $Y$ that is the square of a uniformly distributed variable $X$. This will sharpen your skills in applying the change-of-variables formula to find a new PDF, a critical step before calculating the entropy of the transformed system.", "problem": "Let $X$ be a continuous random variable that is uniformly distributed over the interval $[-1, 1]$. Its Probability Density Function (PDF), $f_X(x)$, is given by:\n$$ f_X(x) = \\begin{cases} \\frac{1}{2} & \\text{for } -1 \\le x \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases} $$\nA new random variable $Y$ is created through the transformation $Y = X^2$.\n\nThe differential entropy $h(Y)$ of a continuous random variable $Y$ with a PDF $f_Y(y)$ is defined as:\n$$ h(Y) = -\\int_{S} f_Y(y) \\ln(f_Y(y)) \\, dy $$\nwhere $S$ is the support of the random variable $Y$ (the set of all possible values of $Y$), and $\\ln$ denotes the natural logarithm.\n\nCalculate the differential entropy $h(Y)$. Express your answer as a single, closed-form analytic expression.", "solution": "We start from the given $X \\sim \\text{Uniform}([-1,1])$ with\n$$\nf_{X}(x)=\\begin{cases}\n\\frac{1}{2}, & -1 \\le x \\le 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nDefine $Y=X^{2}$. The support of $Y$ is $S=[0,1]$.\n\nTo find $f_{Y}(y)$, use the change-of-variables formula for transformations with multiple preimages: if $Y=g(X)$ with differentiable $g$ and, for a given $y$, the preimages are $\\{x_{i}(y)\\}$ satisfying $g(x_{i})=y$, then\n$$\nf_{Y}(y)=\\sum_{i}\\frac{f_{X}(x_{i}(y))}{|g'(x_{i}(y))|}.\n$$\nHere $g(x)=x^{2}$, so $g'(x)=2x$. For $y \\in (0,1]$, the roots are $x_{1}(y)=\\sqrt{y}$ and $x_{2}(y)=-\\sqrt{y}$, both within $[-1,1]$. Therefore,\n$$\nf_{Y}(y)=\\frac{f_{X}(\\sqrt{y})}{|2\\sqrt{y}|}+\\frac{f_{X}(-\\sqrt{y})}{|2(-\\sqrt{y})|}\n=\\frac{\\frac{1}{2}}{2\\sqrt{y}}+\\frac{\\frac{1}{2}}{2\\sqrt{y}}\n=\\frac{1}{2\\sqrt{y}}, \\quad 0<y\\le 1,\n$$\nand $f_{Y}(y)=0$ otherwise. This density is properly normalized since\n$$\n\\int_{0}^{1}\\frac{1}{2\\sqrt{y}}\\,dy=\\left[\\sqrt{y}\\right]_{0}^{1}=1.\n$$\n\nThe differential entropy is\n$$\nh(Y)=-\\int_{0}^{1} f_{Y}(y)\\,\\ln\\bigl(f_{Y}(y)\\bigr)\\,dy\n=-\\int_{0}^{1}\\frac{1}{2\\sqrt{y}}\\ln\\!\\left(\\frac{1}{2\\sqrt{y}}\\right)dy.\n$$\nUse the substitution $y=t^{2}$ with $t\\in[0,1]$, so $dy=2t\\,dt$ and $\\frac{1}{2\\sqrt{y}}\\,dy=dt$. Then\n$$\nh(Y)=-\\int_{0}^{1}\\ln\\!\\left(\\frac{1}{2t}\\right)dt\n=\\int_{0}^{1}\\ln(2t)\\,dt.\n$$\nAn antiderivative is $t\\ln(2t)-t$, since $\\frac{d}{dt}\\bigl(t\\ln(2t)-t\\bigr)=\\ln(2t)$. Evaluating,\n$$\nh(Y)=\\left[t\\ln(2t)-t\\right]_{0}^{1}\n=\\left(1\\cdot\\ln 2-1\\right)-\\lim_{t\\to 0^{+}}\\bigl(t\\ln(2t)-t\\bigr).\n$$\nBecause $\\lim_{t\\to 0^{+}}t\\ln(2t)=0$ and $\\lim_{t\\to 0^{+}}t=0$, the lower-limit contribution is $0$. Hence\n$$\nh(Y)=\\ln 2 - 1.\n$$", "answer": "$$\\boxed{\\ln 2 - 1}$$", "id": "1617710"}, {"introduction": "Information is often about the relationship between variables. A key concept is conditional entropy, which measures the uncertainty of one variable given knowledge of another. This exercise [@problem_id:1617717] provides a concrete scenario with a joint distribution defined over a simple geometric region. By calculating the conditional differential entropy $h(Y|X)$, you will practice the full workflow of moving from a joint PDF to marginal and conditional distributions, gaining insight into how information about one variable reduces uncertainty about another.", "problem": "Consider a physical system where the position of a particle is described by two continuous random variables, $X$ and $Y$. The joint probability density function (PDF), denoted by $f(x,y)$, is uniform over a two-dimensional region and zero elsewhere. This region is a right-angled triangle in the $xy$-plane, defined by the vertices at coordinates $(0,0)$, $(1,0)$, and $(1,1)$.\n\nYour task is to quantify the uncertainty in the particle's $Y$ position, given that the $X$ position is known. Specifically, calculate the conditional differential entropy $h(Y|X)$.\n\nPresent your final answer as an analytic expression.", "solution": "The triangular support is $R=\\{(x,y): 0 \\leq x \\leq 1,\\; 0 \\leq y \\leq x\\}$, whose area is $\\frac{1}{2}$. Since the joint PDF is uniform on $R$, its value is the constant\n$$\nf_{X,Y}(x,y)=\\frac{1}{\\text{Area}(R)}=2,\\quad (x,y)\\in R,\n$$\nand $f_{X,Y}(x,y)=0$ otherwise.\n\nThe marginal density of $X$ is\n$$\nf_{X}(x)=\\int_{0}^{x} 2\\,dy=2x,\\quad 0 \\leq x \\leq 1.\n$$\nHence the conditional density of $Y$ given $X=x$ is\n$$\nf_{Y|X}(y|x)=\\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\\frac{2}{2x}=\\frac{1}{x},\\quad 0 \\leq y \\leq x,\n$$\nso $Y|X=x$ is uniform on $[0,x]$.\n\nFor a uniform density on $[0,x]$, the conditional differential entropy is\n$$\nh(Y|X=x)=-\\int_{0}^{x}\\frac{1}{x}\\ln\\!\\left(\\frac{1}{x}\\right)dy=\\ln x.\n$$\nTherefore,\n$$\nh(Y|X)=\\mathbb{E}[\\ln X]=\\int_{0}^{1} 2x\\,\\ln x\\,dx.\n$$\nCompute the integral by parts:\n$$\n\\int x\\ln x\\,dx=\\frac{x^{2}}{2}\\ln x-\\int \\frac{x^{2}}{2}\\cdot\\frac{1}{x}\\,dx=\\frac{x^{2}}{2}\\ln x-\\frac{x^{2}}{4}+C,\n$$\nso\n$$\n\\int_{0}^{1} 2x\\,\\ln x\\,dx=2\\left[\\frac{x^{2}}{2}\\ln x-\\frac{x^{2}}{4}\\right]_{0}^{1}=2\\left(0-\\frac{1}{4}-0\\right)=-\\frac{1}{2}.\n$$\nHence,\n$$\nh(Y|X)=-\\frac{1}{2}.\n$$", "answer": "$$\\boxed{-\\frac{1}{2}}$$", "id": "1617717"}]}