{"hands_on_practices": [{"introduction": "We begin by tackling a common point of confusion: unlike its discrete counterpart, differential entropy can be negative. This exercise challenges you to find the condition on a Gaussian's variance that makes its entropy negative, prompting a deeper look into what this signifies about a distribution's concentration. Understanding this property is crucial for correctly interpreting differential entropy as a relative measure of uncertainty. [@problem_id:1617961]", "problem": "In information theory, the differential entropy of a continuous random variable measures its average uncertainty. Unlike the Shannon entropy of a discrete random variable, which is always non-negative, differential entropy can take on negative values.\n\nConsider a random variable $X$ that follows a one-dimensional Gaussian (or normal) distribution with a mean of zero and a variance of $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(0, \\sigma^2)$. The differential entropy for this variable, expressed in nats, is given by the formula:\n$$h(X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)$$\nwhere $\\ln$ is the natural logarithm and $e$ is Euler's number, the base of the natural logarithm.\n\nFrom the following list of possible values for the variance $\\sigma^2$, select the one that results in a negative differential entropy, $h(X) < 0$.\n\nA. $\\sigma^2 = 1$\n\nB. $\\sigma^2 = \\frac{1}{\\pi}$\n\nC. $\\sigma^2 = \\frac{1}{2e}$\n\nD. $\\sigma^2 = \\frac{1}{2\\pi e}$\n\nE. $\\sigma^2 = \\frac{1}{4\\pi e}$", "solution": "We are given that for $X \\sim \\mathcal{N}(0,\\sigma^{2})$, the differential entropy in nats is\n$$\nh(X) = \\frac{1}{2}\\ln\\!\\big(2\\pi e \\sigma^{2}\\big).\n$$\nWe require $h(X) < 0$. Since $\\ln$ is strictly increasing, the inequality\n$$\n\\frac{1}{2}\\ln\\!\\big(2\\pi e \\sigma^{2}\\big) < 0\n$$\nis equivalent to\n$$\n\\ln\\!\\big(2\\pi e \\sigma^{2}\\big) < 0 \\iff 2\\pi e \\sigma^{2} < 1 \\iff \\sigma^{2} < \\frac{1}{2\\pi e}.\n$$\nWe now compare each option to the threshold $\\frac{1}{2\\pi e}$:\n- A: $\\sigma^{2} = 1 > \\frac{1}{2\\pi e}$, so $h(X) > 0$.\n- B: $\\sigma^{2} = \\frac{1}{\\pi} > \\frac{1}{2\\pi e}$ because $\\frac{1}{\\pi} > \\frac{1}{2\\pi e} \\iff 1 > \\frac{1}{2e}$, which holds.\n- C: $\\sigma^{2} = \\frac{1}{2e} > \\frac{1}{2\\pi e}$ since $\\pi > 1$.\n- D: $\\sigma^{2} = \\frac{1}{2\\pi e}$ gives $h(X) = \\frac{1}{2}\\ln(1) = 0$, not negative.\n- E: $\\sigma^{2} = \\frac{1}{4\\pi e} < \\frac{1}{2\\pi e}$, so $h(X) < 0$.\n\nThus the only choice yielding negative differential entropy is E.", "answer": "$$\\boxed{E}$$", "id": "1617961"}, {"introduction": "A cornerstone of experimental science and signal processing is averaging multiple measurements to reduce noise. This practice explores the information-theoretic consequence of this common procedure. You will derive how the differential entropy changes when you compute the sample mean of several independent Gaussian measurements, revealing a quantitative link between statistical precision and informational uncertainty. [@problem_id:1618009]", "problem": "In digital signal processing and data analysis, a common technique to improve measurement precision is to average multiple independent readings. This process tends to reduce the effect of random noise.\n\nConsider a scenario where a scientist collects $n$ measurements, which are modeled as independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n$. Each measurement $X_i$ is known to follow a standard normal distribution, which is a Gaussian distribution with a mean of 0 and a variance of 1.\n\nThe scientist computes an aggregated measurement, $\\bar{X}_n$, by taking the sample mean of these $n$ values:\n$$\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\nThe uncertainty or \"surprise\" associated with this new random variable $\\bar{X}_n$ can be quantified by its differential entropy. For a general Gaussian random variable $Y$ with variance $\\sigma^2$, its differential entropy $h(Y)$ is given by the formula:\n$$\nh(Y) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nDetermine the differential entropy of the sample mean, $h(\\bar{X}_n)$, as a function of the number of measurements, $n$. Your answer should be a symbolic expression.", "solution": "We are given independent identically distributed $X_{1},\\dots,X_{n}$ with $X_{i}\\sim \\mathcal{N}(0,1)$ and the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. A linear combination of independent Gaussian random variables is Gaussian, so $\\bar{X}_{n}$ is Gaussian. Its mean is\n$$\n\\mathbb{E}[\\bar{X}_{n}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}]=\\frac{1}{n}\\cdot n\\cdot 0=0.\n$$\nUsing independence and the variance scaling property $\\operatorname{Var}(cZ)=c^{2}\\operatorname{Var}(Z)$, its variance is\n$$\n\\operatorname{Var}(\\bar{X}_{n})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}(X_{i})=\\frac{1}{n^{2}}\\cdot n\\cdot 1=\\frac{1}{n}.\n$$\nFor a Gaussian random variable $Y$ with variance $\\sigma^{2}$, the differential entropy is\n$$\nh(Y)=\\frac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right).\n$$\nSubstituting $\\sigma^{2}=\\frac{1}{n}$ for $Y=\\bar{X}_{n}$ gives\n$$\nh(\\bar{X}_{n})=\\frac{1}{2}\\ln\\!\\left(2\\pi e\\,\\frac{1}{n}\\right)=\\frac{1}{2}\\ln\\!\\left(\\frac{2\\pi e}{n}\\right).\n$$\nThis expresses the differential entropy of the sample mean as a function of $n$.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\!\\left(\\frac{2\\pi e}{n}\\right)}$$", "id": "1618009"}, {"introduction": "We conclude with an application from communication engineering that demonstrates the practical power of entropy maximization. This problem casts you as a system designer allocating a fixed power budget across two channels to maximize the total information they can carry. This optimization task highlights why the Gaussian distribution is so central to information theory, as it maximizes entropy for a given power constraint. [@problem_id:1617989]", "problem": "Consider a dual-channel communication system designed to transmit two independent signals. These signals are modeled as zero-mean Gaussian random variables, $X_1$ and $X_2$, with powers corresponding to their variances, $\\sigma_1^2$ and $\\sigma_2^2$, respectively.\n\nThe system operates under a resource constraint. The total weighted power consumption must not exceed a total budget $P$. This relationship is expressed as $a_1 \\sigma_1^2 + a_2 \\sigma_2^2 = P$, where $a_1 > 0$ and $a_2 > 0$ are given positive constants representing the resource cost per unit of power for each channel.\n\nTo maximize the total information throughput, one must maximize the joint differential entropy of the two signals, $h(X_1, X_2)$. For reference, the differential entropy of a single Gaussian random variable $X$ with variance $\\sigma^2$ is given by the formula $h(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)$.\n\nDetermine the values of the individual variances, $\\sigma_1^2$ and $\\sigma_2^2$, that maximize the joint differential entropy subject to the power budget constraint. Present your answers for $\\sigma_1^2$ and then $\\sigma_2^2$.", "solution": "We are given two independent, zero-mean Gaussian random variables $X_{1}$ and $X_{2}$ with variances $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$. For independent variables, the joint differential entropy equals the sum of individual entropies:\n$$\nh(X_{1},X_{2})=h(X_{1})+h(X_{2}).\n$$\nFor a Gaussian random variable with variance $\\sigma^{2}$, $h(X)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)$. Therefore,\n$$\nh(X_{1},X_{2})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{1}^{2}\\big)+\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{2}^{2}\\big)\n=\\ln(2\\pi e)+\\frac{1}{2}\\ln\\!\\big(\\sigma_{1}^{2}\\sigma_{2}^{2}\\big).\n$$\nMaximizing $h(X_{1},X_{2})$ over $\\sigma_{1}^{2},\\sigma_{2}^{2}>0$ subject to the resource constraint $a_{1}\\sigma_{1}^{2}+a_{2}\\sigma_{2}^{2}=P$ is equivalent to maximizing $\\ln(\\sigma_{1}^{2})+\\ln(\\sigma_{2}^{2})$ under the same constraint, since constants do not affect the optimizer. This is a concave maximization with a linear equality constraint; we use Lagrange multipliers.\n\nDefine\n$$\n\\mathcal{L}=\\frac{1}{2}\\ln(\\sigma_{1}^{2})+\\frac{1}{2}\\ln(\\sigma_{2}^{2})-\\lambda\\big(a_{1}\\sigma_{1}^{2}+a_{2}\\sigma_{2}^{2}-P\\big).\n$$\nStationarity conditions are\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{1}^{2}}=\\frac{1}{2}\\frac{1}{\\sigma_{1}^{2}}-\\lambda a_{1}=0\n\\quad\\Rightarrow\\quad \\sigma_{1}^{2}=\\frac{1}{2\\lambda a_{1}},\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{2}^{2}}=\\frac{1}{2}\\frac{1}{\\sigma_{2}^{2}}-\\lambda a_{2}=0\n\\quad\\Rightarrow\\quad \\sigma_{2}^{2}=\\frac{1}{2\\lambda a_{2}}.\n$$\nImposing the constraint $a_{1}\\sigma_{1}^{2}+a_{2}\\sigma_{2}^{2}=P$ gives\n$$\na_{1}\\left(\\frac{1}{2\\lambda a_{1}}\\right)+a_{2}\\left(\\frac{1}{2\\lambda a_{2}}\\right)=P\n\\;\\Rightarrow\\;\n\\frac{1}{2\\lambda}+\\frac{1}{2\\lambda}=P\n\\;\\Rightarrow\\;\n\\frac{1}{\\lambda}=P\n\\;\\Rightarrow\\;\n\\lambda=\\frac{1}{P}.\n$$\nSubstituting back yields\n$$\n\\sigma_{1}^{2}=\\frac{P}{2a_{1}},\\qquad \\sigma_{2}^{2}=\\frac{P}{2a_{2}}.\n$$\nBecause the objective is strictly concave in $(\\sigma_{1}^{2},\\sigma_{2}^{2})$ and the feasible set is affine with $a_{1},a_{2}>0$, this stationary point is the unique global maximizer, and it uses the full budget.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{P}{2 a_{1}} & \\frac{P}{2 a_{2}}\\end{pmatrix}}$$", "id": "1617989"}]}