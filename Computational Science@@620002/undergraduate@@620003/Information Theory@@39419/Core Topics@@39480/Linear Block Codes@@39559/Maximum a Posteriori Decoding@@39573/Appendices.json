{"hands_on_practices": [{"introduction": "This first exercise lays the groundwork for understanding Maximum a Posteriori (MAP) decoding. You will tackle a fundamental scenario: deciding on a single transmitted bit from a source with known, non-uniform prior probabilities over a simple noisy channel [@problem_id:1639810]. This practice is crucial for seeing how prior knowledge influences the decision, moving beyond just the raw evidence provided by the received signal.", "problem": "A remote environmental monitoring system uses a binary sensor to transmit its findings. The sensor's state is represented by a binary random variable $X$, where $X=0$ indicates a \"Normal\" condition and $X=1$ indicates an \"Alert\" condition. Long-term data shows that the \"Normal\" state is more common, with prior probabilities given by $P(X=0) = 0.7$ and $P(X=1) = 0.3$.\n\nThe sensor's state is transmitted to a base station over a noisy communication link that behaves as a Binary Symmetric Channel (BSC). A BSC is characterized by a single parameter, the crossover probability $p$, which is the probability that a transmitted bit is incorrectly received (i.e., a '0' is received as a '1' or a '1' is received as a '0'). For this particular channel, the crossover probability is $p = 0.1$.\n\nAt the base station, a decoder receives a symbol $Y \\in \\{0, 1\\}$ and must make an estimate, $\\hat{X}$, of the originally transmitted symbol $X$. To achieve the lowest possible rate of incorrect decisions, a Maximum a Posteriori (MAP) decision rule is employed.\n\nYour task is to determine the specific MAP decision rule and the resulting average probability of error, $P_e = P(\\hat{X} \\neq X)$. The decision rule is fully described by the pair of outputs $(\\hat{X}(Y=0), \\hat{X}(Y=1))$, which specifies the estimate for each possible received symbol.\n\nWhich of the following options correctly states the MAP decision rule and the corresponding average probability of error?\n\nA. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 1)$, and $P_e = 0.10$.\n\nB. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 0)$, and $P_e = 0.30$.\n\nC. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 1), and $P_e = 0.07$.\n\nD. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 0)$, and $P_e = 0.10$.\n\nE. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (1, 0)$, and $P_e = 0.90$.", "solution": "We are given a binary source with priors $P(X=0)=0.7$ and $P(X=1)=0.3$, and a Binary Symmetric Channel (BSC) with crossover probability $p=0.1$. The received symbol is $Y \\in \\{0,1\\}$. The MAP rule selects, for each received $y$, the hypothesis $x \\in \\{0,1\\}$ that maximizes the posterior $P(X=x \\mid Y=y)$, which is equivalent to maximizing the product $P(Y=y \\mid X=x)P(X=x)$.\n\nFor a BSC, the likelihoods are\n$$\nP(Y=y \\mid X=x)=\\begin{cases}\n1-p, & y=x,\\\\\np, & y \\neq x.\n\\end{cases}\n$$\n\nDecision for $Y=0$: compare $(1-p)P(X=0)$ to $p P(X=1)$.\n$$\n(1-p)P(X=0) \\overset{?}{\\ge} p P(X=1).\n$$\nSubstituting the given values,\n$$\n(1-0.1)\\cdot 0.7 = 0.63 \\quad \\text{and} \\quad 0.1 \\cdot 0.3 = 0.03,\n$$\nso $(1-p)P(X=0) > p P(X=1)$ and the MAP decision is $\\hat{X}(Y=0)=0$.\n\nDecision for $Y=1$: compare $p P(X=0)$ to $(1-p)P(X=1)$.\n$$\np P(X=0) \\overset{?}{\\ge} (1-p) P(X=1).\n$$\nSubstituting,\n$$\n0.1 \\cdot 0.7 = 0.07 \\quad \\text{and} \\quad (1-0.1)\\cdot 0.3 = 0.27,\n$$\nso $p P(X=0) < (1-p)P(X=1)$ and the MAP decision is $\\hat{X}(Y=1)=1$.\n\nTherefore, the MAP rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1))=(0,1)$.\n\nNext, compute the average probability of error under this rule:\n$$\nP_{e} = P(X=0) P(\\hat{X} \\neq X \\mid X=0) + P(X=1) P(\\hat{X} \\neq X \\mid X=1).\n$$\nGiven the rule, an error occurs if and only if the channel flips the bit:\n$$\nP(\\hat{X} \\neq X \\mid X=0) = P(Y=1 \\mid X=0)=p, \\quad P(\\hat{X} \\neq X \\mid X=1) = P(Y=0 \\mid X=1)=p.\n$$\nHence,\n$$\nP_{e} = P(X=0)\\,p + P(X=1)\\,p = p\\big(P(X=0)+P(X=1)\\big) = p = 0.1.\n$$\nEquivalently, using $P_{e} = 1 - \\sum_{y} \\max_{x} P(X=x, Y=y)$,\n$$\n\\sum_{y} \\max_{x} P(X=x, Y=y) = (1-p)P(X=0) + (1-p)P(X=1) = (1-p),\n$$\nso $P_{e} = 1 - (1-p) = p = 0.1$, confirming the result.\n\nThus the correct option is the rule $(0,1)$ with $P_{e}=0.10$.", "answer": "$$\\boxed{A}$$", "id": "1639810"}, {"introduction": "Building on the foundational concepts, this problem introduces a more complex and realistic scenario. We will explore a situation with a non-binary source, where different messages are mapped to codewords and sent over independent but non-identical channels [@problem_id:1639840]. This exercise demonstrates how to apply the MAP rule when the received data doesn't perfectly match any possible transmission, forcing a careful weighing of likelihoods and priors to determine the most plausible original message.", "problem": "A specialized data storage system handles three types of data packets, labeled A, B, and C. The prior probabilities of encountering each packet type are given by $P(X=A) = \\frac{1}{2}$, $P(X=B) = \\frac{1}{3}$, and $P(X=C) = \\frac{1}{6}$, where $X$ is the random variable representing the packet type. Before storage, each packet is encoded into a 2-bit codeword as follows:\n- Packet A is encoded as `00`.\n- Packet B is encoded as `01`.\n- Packet C is encoded as `11`.\n\nThe two bits of the codeword are stored in two separate, independent memory modules. The first bit is stored in Module 1, and the second bit in Module 2. Both modules are subject to bit-flip errors and can be modeled as communication channels. Module 1 behaves as a Binary Symmetric Channel (BSC) with a crossover probability (the probability of a bit flip) of $p_1 = 0.1$. Module 2 is less reliable and is modeled as a BSC with a crossover probability of $p_2 = 0.2$.\n\nSuppose that upon retrieval, the 2-bit sequence read from the memory modules is $Y = 10$. Given this observation, which of the original packet types is the most likely to have been stored? Your task is to determine the Maximum A Posteriori (MAP) estimate for the original packet $X$.\n\nWhich of the following represents the MAP estimate for the original packet?\n\nA. A\n\nB. B\n\nC. C\n\nD. B and C are equally likely and more probable than A.\n\nE. It is impossible to determine from the given information.", "solution": "We want the Maximum A Posteriori (MAP) estimate, defined by\n$$\n\\hat{x}_{\\text{MAP}}=\\arg\\max_{x\\in\\{A,B,C\\}} P(X=x\\mid Y=10),\n$$\nwhich is equivalent to maximizing the posterior proportional to prior times likelihood:\n$$\nP(X=x\\mid Y=10)\\propto P(Y=10\\mid X=x)P(X=x).\n$$\nThe encoding is deterministic: $A\\mapsto 00$, $B\\mapsto 01$, $C\\mapsto 11$. The two memory modules are independent Binary Symmetric Channels with crossover probabilities $p_{1}$ for the first bit and $p_{2}$ for the second bit. For a BSC, the probability of no flip is $1-p$ and of a flip is $p$. Because the modules act independently on the two bits, the likelihood factors as\n$$\nP(Y=10\\mid X=x)=P(Y_{1}=1\\mid \\text{bit}_{1}(x))\\cdot P(Y_{2}=0\\mid \\text{bit}_{2}(x)).\n$$\nCompute each likelihood:\n- For $X=A$ (codeword $00$): the first bit flips ($0\\to 1$) with probability $p_{1}$ and the second bit does not flip ($0\\to 0$) with probability $1-p_{2}$, so\n$$\nP(Y=10\\mid X=A)=p_{1}(1-p_{2}).\n$$\n- For $X=B$ (codeword $01$): the first bit flips ($0\\to 1$) with probability $p_{1}$ and the second bit flips ($1\\to 0$) with probability $p_{2}$, so\n$$\nP(Y=10\\mid X=B)=p_{1}p_{2}.\n$$\n- For $X=C$ (codeword $11$): the first bit does not flip ($1\\to 1$) with probability $1-p_{1}$ and the second bit flips ($1\\to 0$) with probability $p_{2}$, so\n$$\nP(Y=10\\mid X=C)=(1-p_{1})p_{2}.\n$$\nMultiply by priors to form unnormalized posteriors:\n$$\nS_{A}=P(X=A)P(Y=10\\mid X=A)=\\frac{1}{2}\\,p_{1}(1-p_{2}),\n$$\n$$\nS_{B}=P(X=B)P(Y=10\\mid X=B)=\\frac{1}{3}\\,p_{1}p_{2},\n$$\n$$\nS_{C}=P(X=C)P(Y=10\\mid X=C)=\\frac{1}{6}\\,(1-p_{1})p_{2}.\n$$\nSubstitute $p_{1}=\\frac{1}{10}$ and $p_{2}=\\frac{1}{5}$:\n$$\nS_{A}=\\frac{1}{2}\\cdot \\frac{1}{10}\\cdot \\left(1-\\frac{1}{5}\\right)=\\frac{1}{2}\\cdot \\frac{1}{10}\\cdot \\frac{4}{5}=\\frac{1}{25},\n$$\n$$\nS_{B}=\\frac{1}{3}\\cdot \\frac{1}{10}\\cdot \\frac{1}{5}=\\frac{1}{150},\n$$\n$$\nS_{C}=\\frac{1}{6}\\cdot \\left(1-\\frac{1}{10}\\right)\\cdot \\frac{1}{5}=\\frac{1}{6}\\cdot \\frac{9}{10}\\cdot \\frac{1}{5}=\\frac{3}{100}.\n$$\nCompare:\n$$\n\\frac{1}{25}>\\frac{3}{100}>\\frac{1}{150}.\n$$\nTherefore the MAP estimate is $X=A$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1639840"}, {"introduction": "This final practice advances from making isolated decisions to the powerful domain of sequential estimation. We consider a source where the value of each transmitted bit depends on the one before it, a structure known as a Markov chain [@problem_id:1639803]. Your task is to dynamically update the probability of the true state as a sequence of noisy observations arrives, illustrating a core technique essential for applications like speech recognition and bioinformatics.", "problem": "Consider a simplified model for a data storage system where binary bits, represented by a sequence of random variables $\\{X_k\\}_{k=1,2,3,\\dots}$ with $X_k \\in \\{0, 1\\}$, are stored sequentially. Due to physical interactions between adjacent storage cells, the system behaves as a first-order binary Markov source. The transition probabilities are given by $P(X_k = 1 | X_{k-1} = 0) = 0.2$ and $P(X_k = 0 | X_{k-1} = 1) = 0.3$ for $k \\ge 2$. For the very first bit, the probabilities are equal, i.e., $P(X_1=0) = P(X_1=1) = 0.5$.\n\nWhen the data is read, the process is corrupted by noise, which is modeled as a memoryless Binary Symmetric Channel (BSC). The crossover probability of the BSC is $\\epsilon = 0.1$, meaning the probability of a bit being flipped during the read-out process is $0.1$. Let $\\{Y_k\\}_{k=1,2,3,\\dots}$ be the sequence of bits read from the storage system.\n\nAn operator reads the first three bits from the device and obtains the sequence $Y_1=1, Y_2=0, Y_3=1$. To perform an optimal bit-by-bit Maximum a Posteriori (MAP) decision, the operator needs to compute the posterior probability of each transmitted bit given the observed sequence up to that point.\n\nCalculate the posterior probability $P(X_3=1 | Y_1=1, Y_2=0, Y_3=1)$. Round your final answer to four significant figures.", "solution": "We model the hidden bits as a first-order binary Markov chain with transition probabilities\n$$\nP(X_{k}=1\\mid X_{k-1}=0)=0.2,\\quad P(X_{k}=0\\mid X_{k-1}=1)=0.3,\n$$\nso\n$$\nP(X_{k}=0\\mid X_{k-1}=0)=0.8,\\quad P(X_{k}=1\\mid X_{k-1}=1)=0.7,\n$$\nand initial distribution\n$$\nP(X_{1}=0)=P(X_{1}=1)=\\frac{1}{2}.\n$$\nThe observations come through a memoryless Binary Symmetric Channel with crossover probability $\\epsilon=0.1$, so for each $k$,\n$$\nP(Y_{k}=y\\mid X_{k}=x)=\\begin{cases}\n0.9,& y=x,\\\\\n0.1,& y\\neq x.\n\\end{cases}\n$$\nDefine the forward variables $\\alpha_{k}(x)=P(X_{k}=x,Y_{1}=y_{1},\\ldots,Y_{k}=y_{k})$. Then\n$$\n\\alpha_{1}(x_{1})=P(Y_{1}=y_{1}\\mid X_{1}=x_{1})P(X_{1}=x_{1}),\n$$\nand for $k\\geq 2$,\n$$\n\\alpha_{k}(x_{k})=P(Y_{k}=y_{k}\\mid X_{k}=x_{k})\\sum_{x_{k-1}\\in\\{0,1\\}}P(X_{k}=x_{k}\\mid X_{k-1}=x_{k-1})\\,\\alpha_{k-1}(x_{k-1}).\n$$\n\nGiven $y_{1}=1$, we have\n$$\n\\alpha_{1}(1)=0.9\\cdot\\frac{1}{2}=\\frac{9}{20},\\qquad \\alpha_{1}(0)=0.1\\cdot\\frac{1}{2}=\\frac{1}{20}.\n$$\n\nGiven $y_{2}=0$, compute the prediction to time $2$:\n$$\n\\sum_{x_{1}}P(X_{2}=0\\mid X_{1}=x_{1})\\alpha_{1}(x_{1})=0.8\\cdot\\frac{1}{20}+0.3\\cdot\\frac{9}{20}=\\frac{7}{40},\n$$\n$$\n\\sum_{x_{1}}P(X_{2}=1\\mid X_{1}=x_{1})\\alpha_{1}(x_{1})=0.2\\cdot\\frac{1}{20}+0.7\\cdot\\frac{9}{20}=\\frac{13}{40}.\n$$\nThen\n$$\n\\alpha_{2}(0)=0.9\\cdot\\frac{7}{40}=\\frac{63}{400},\\qquad \\alpha_{2}(1)=0.1\\cdot\\frac{13}{40}=\\frac{13}{400}.\n$$\n\nGiven $y_{3}=1$, compute the prediction to time $3$:\n$$\n\\sum_{x_{2}}P(X_{3}=0\\mid X_{2}=x_{2})\\alpha_{2}(x_{2})=0.8\\cdot\\frac{63}{400}+0.3\\cdot\\frac{13}{400}=\\frac{543}{4000},\n$$\n$$\n\\sum_{x_{2}}P(X_{3}=1\\mid X_{2}=x_{2})\\alpha_{2}(x_{2})=0.2\\cdot\\frac{63}{400}+0.7\\cdot\\frac{13}{400}=\\frac{217}{4000}.\n$$\nThus\n$$\n\\alpha_{3}(1)=0.9\\cdot\\frac{217}{4000}=\\frac{1953}{40000},\\qquad \\alpha_{3}(0)=0.1\\cdot\\frac{543}{4000}=\\frac{543}{40000}.\n$$\n\nThe desired posterior is\n$$\nP(X_{3}=1\\mid Y_{1}=1,Y_{2}=0,Y_{3}=1)=\\frac{\\alpha_{3}(1)}{\\alpha_{3}(0)+\\alpha_{3}(1)}=\\frac{1953/40000}{(1953+543)/40000}=\\frac{1953}{2496}=\\frac{651}{832}.\n$$\nNumerically,\n$$\n\\frac{651}{832}\\approx 0.782451923\\ldots,\n$$\nwhich rounded to four significant figures is $0.7825$.", "answer": "$$\\boxed{0.7825}$$", "id": "1639803"}]}