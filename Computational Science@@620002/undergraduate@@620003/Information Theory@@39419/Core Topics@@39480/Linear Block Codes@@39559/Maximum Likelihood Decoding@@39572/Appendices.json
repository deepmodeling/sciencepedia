{"hands_on_practices": [{"introduction": "Maximum Likelihood (ML) decoding is founded on a simple, powerful principle: given what we've observed, what was the most likely cause? This first exercise strips the concept down to its essence. By working with a simple Binary Asymmetric Channel, you will directly apply the definition of ML decoding—choosing the input $X$ that maximizes the conditional probability $P(Y|X)$—to make a decision based on a single received bit [@problem_id:1640427].", "problem": "A deep-space probe communicates with a ground station on Earth by transmitting binary data. The communication channel is noisy and is modeled as a Binary Asymmetric Channel (BAC). Let the transmitted bit be $X \\in \\{0, 1\\}$ and the received bit be $Y \\in \\{0, 1\\}$.\n\nDue to various forms of interference, the probability of a bit being flipped during transmission is not the same for both binary symbols. The channel is characterized by the following conditional probabilities:\n- The probability of receiving a '1' given that a '0' was sent is $P(Y=1|X=0) = 0.1$.\n- The probability of receiving a '0' given that a '1' was sent is $P(Y=0|X=1) = 0.2$.\n\nAn engineer at the ground station observes a single received bit, which is $Y=1$. To make the best guess about the original data, the engineer uses the Maximum Likelihood (ML) decoding criterion. What is the ML estimate for the transmitted bit $X$?\n\nA. 0\nB. 1\nC. Both 0 and 1 are equally likely under the ML criterion.\nD. Insufficient information is provided to determine the ML estimate.", "solution": "We use the Maximum Likelihood (ML) decoding rule, which selects the symbol $\\hat{x}$ that maximizes the conditional likelihood given the observation:\n$$\n\\hat{x}_{\\text{ML}}(y)=\\arg\\max_{x\\in\\{0,1\\}} P(Y=y\\mid X=x).\n$$\nGiven $Y=1$, we compare $P(Y=1\\mid X=0)$ and $P(Y=1\\mid X=1)$. From the channel description,\n$$\nP(Y=1\\mid X=0)=0.1,\n$$\nand\n$$\nP(Y=1\\mid X=1)=1-P(Y=0\\mid X=1)=1-0.2=0.8.\n$$\nSince $0.80.1$, the ML decision is\n$$\n\\hat{X}_{\\text{ML}}=1.\n$$\nThus, the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1640427"}, {"introduction": "While the principle of maximizing probability is universal, its application often takes on a more geometric form in practice. This problem transitions from discrete channels to the ubiquitous Additive White Gaussian Noise (AWGN) channel, a model for many real-world communication systems. You will discover that for Gaussian noise, maximizing the likelihood is equivalent to minimizing the squared Euclidean distance between the received signal and the possible transmitted signals, a cornerstone of modern digital receiver design [@problem_id:1640492].", "problem": "A simple digital communication system transmits one of two binary digits, '0' or '1', in each time interval. The bits are assumed to be transmitted with equal probability. These bits are represented by voltage levels using a modulation scheme known as Binary Phase Shift Keying (BPSK). Specifically, bit '0' is transmitted as the voltage level $s_0 = +\\sqrt{E_b}$ and bit '1' is transmitted as the voltage level $s_1 = -\\sqrt{E_b}$, where $E_b$ is a positive constant representing the energy per bit.\n\nThe signal is sent over a channel that is modeled as an Additive White Gaussian Noise (AWGN) channel. This means the voltage measured at the receiver, $r$, is the sum of the transmitted voltage and a random noise component, $n$. The noise $n$ is a random variable that follows a Gaussian (normal) distribution with a mean of zero and a variance of $\\sigma^2$.\n\nSuppose that in a particular transmission, the voltage at the receiver is measured to be $r = -0.1\\sqrt{E_b}$. Based on this observation, determine which binary digit was most probably transmitted. Your answer should be the single digit '0' or '1'.", "solution": "Let the two hypotheses be:\n- $H_{0}$: bit $0$ sent with signal $s_{0} = +\\sqrt{E_{b}}$,\n- $H_{1}$: bit $1$ sent with signal $s_{1} = -\\sqrt{E_{b}}$.\n\nThe received sample is $r = s + n$ with $n \\sim \\mathcal{N}(0,\\sigma^{2})$. Under AWGN, the conditional density is\n$$\np(r \\mid s_{i}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp\\!\\left(-\\frac{(r - s_{i})^{2}}{2 \\sigma^{2}}\\right).\n$$\nWith equal priors, the MAP rule reduces to the ML rule: choose the hypothesis that maximizes $p(r \\mid s_{i})$, equivalently minimizes $(r - s_{i})^{2}$. Define the log-likelihood ratio\n$$\n\\ln \\Lambda(r) = \\ln p(r \\mid s_{0}) - \\ln p(r \\mid s_{1}) = -\\frac{(r - s_{0})^{2} - (r - s_{1})^{2}}{2 \\sigma^{2}}.\n$$\nDecide $H_{0}$ if $\\ln \\Lambda(r) \\ge 0$, which is equivalent to\n$$\n(r - s_{0})^{2} \\le (r - s_{1})^{2}.\n$$\nCompute the difference:\n$$\n(r - s_{0})^{2} - (r - s_{1})^{2} = -2 r (s_{0} - s_{1}) + (s_{0}^{2} - s_{1}^{2}) = (s_{0} - s_{1})(-2 r + s_{0} + s_{1}).\n$$\nFor BPSK, $s_{0} = +\\sqrt{E_{b}}$ and $s_{1} = -\\sqrt{E_{b}}$, hence $s_{0} - s_{1} = 2 \\sqrt{E_{b}}$ and $s_{0} + s_{1} = 0$. Therefore,\n$$\n(r - s_{0})^{2} - (r - s_{1})^{2} = -4 \\sqrt{E_{b}}\\, r.\n$$\nThus, decide $H_{0}$ if $-4 \\sqrt{E_{b}}\\, r \\le 0$, which is equivalent to $r \\ge 0$, and decide $H_{1}$ if $r  0$. The observation is $r = -0.1 \\sqrt{E_{b}}  0$, so the most probable transmitted bit is $1$.", "answer": "$$\\boxed{1}$$", "id": "1640492"}, {"introduction": "Real-world systems transmit data in blocks, not just single bits, using error-correcting codes to enhance reliability. This practice extends ML decoding to codewords from a linear block code transmitted over a Binary Symmetric Channel (BSC). You will see how the ML rule simplifies to a minimum Hamming distance decision and, importantly, confront a critical aspect of decoding: the possibility of a decoding failure when ambiguity arises [@problem_id:1640448].", "problem": "In a digital communication system, a linear block code $\\mathcal{C}$ is used for error detection and correction. The code is a $(5,3)$ code, meaning it maps 3-bit message vectors $\\mathbf{u}$ to 5-bit codewords $\\mathbf{c}$. The code is generated by the following generator matrix $G$ over the binary field (i.e., all additions are modulo-2):\n$$\nG = \\begin{pmatrix}\n1  0  0  1  1 \\\\\n0  1  0  1  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}\n$$\nA codeword $\\mathbf{c} \\in \\mathcal{C}$ is transmitted over a Binary Symmetric Channel (BSC) with a crossover probability $p_e = 0.1$. The receiver uses a Maximum Likelihood (ML) decoder to determine the most likely transmitted codeword.\n\nSuppose the receiver observes the vector $\\mathbf{y} = (0, 0, 1, 0, 0)$. Given this received vector, what is the outcome of the ML decoding process?\n\nA. The decoder correctly identifies and outputs the transmitted codeword.\nB. The decoder outputs a codeword, but it is guaranteed to be incorrect.\nC. The decoder declares a decoding failure as it cannot uniquely identify a single most likely codeword.\nD. The decoder outputs the all-zero codeword $(0, 0, 0, 0, 0)$ because it is the simplest codeword.\nE. The decoder requests a retransmission because the received vector is not a valid codeword.", "solution": "A linear block code generated by $G$ over the binary field maps a message $\\mathbf{u}=(u_{1},u_{2},u_{3})$ to a codeword $\\mathbf{c}=\\mathbf{u}G$ with all operations modulo-$2$. Let the rows of $G$ be\n$$\n\\mathbf{r}_{1}=(1,0,0,1,1),\\quad \\mathbf{r}_{2}=(0,1,0,1,0),\\quad \\mathbf{r}_{3}=(0,0,1,0,1).\n$$\nAll codewords are the binary sums of these rows:\n- For $\\mathbf{u}=(0,0,0)$: $\\mathbf{c}_{0}=(0,0,0,0,0)$.\n- For $\\mathbf{u}=(0,0,1)$: $\\mathbf{c}_{1}=\\mathbf{r}_{3}=(0,0,1,0,1)$.\n- For $\\mathbf{u}=(0,1,0)$: $\\mathbf{c}_{2}=\\mathbf{r}_{2}=(0,1,0,1,0)$.\n- For $\\mathbf{u}=(0,1,1)$: $\\mathbf{c}_{3}=\\mathbf{r}_{2}+\\mathbf{r}_{3}=(0,1,1,1,1)$.\n- For $\\mathbf{u}=(1,0,0)$: $\\mathbf{c}_{4}=\\mathbf{r}_{1}=(1,0,0,1,1)$.\n- For $\\mathbf{u}=(1,0,1)$: $\\mathbf{c}_{5}=\\mathbf{r}_{1}+\\mathbf{r}_{3}=(1,0,1,1,0)$.\n- For $\\mathbfu}=(1,1,0)$: $\\mathbf{c}_{6}=\\mathbf{r}_{1}+\\mathbf{r}_{2}=(1,1,0,0,1)$.\n- For $\\mathbf{u}=(1,1,1)$: $\\mathbf{c}_{7}=\\mathbf{r}_{1}+\\mathbf{r}_{2}+\\mathbf{r}_{3}=(1,1,1,0,0)$.\n\nOver a Binary Symmetric Channel with crossover probability $p_{e}$, the ML decoder chooses\n$$\n\\hat{\\mathbf{c}}=\\arg\\max_{\\mathbf{c}\\in\\mathcal{C}}\\,\\mathbb{P}(\\mathbf{Y}=\\mathbf{y}\\mid \\mathbf{C}=\\mathbf{c})=\\arg\\max_{\\mathbf{c}\\in\\mathcal{C}}\\,p_{e}^{d_{H}(\\mathbf{y},\\mathbf{c})}\\left(1-p_{e}\\right)^{n-d_{H}(\\mathbf{y},\\mathbf{c})},\n$$\nwhere $d_{H}$ is the Hamming distance and $n=5$. For $0p_{e}\\frac{1}{2}$, this is equivalent to minimum Hamming distance decoding:\n$$\n\\hat{\\mathbf{c}}=\\arg\\min_{\\mathbf{c}\\in\\mathcal{C}} d_{H}(\\mathbf{y},\\mathbf{c}).\n$$\nGiven $\\mathbf{y}=(0,0,1,0,0)$, compute distances to all codewords:\n$$\n\\begin{aligned}\nd_{H}\\big((0,0,1,0,0),(0,0,0,0,0)\\big)=1,\\\\\nd_{H}\\big((0,0,1,0,0),(0,0,1,0,1)\\big)=1,\\\\\nd_{H}\\big((0,0,1,0,0),(0,1,0,1,0)\\big)=3,\\\\\nd_{H}\\big((0,0,1,0,0),(0,1,1,1,1)\\big)=3,\\\\\nd_{H}\\big((0,0,1,0,0),(1,0,0,1,1)\\big)=4,\\\\\nd_{H}\\big((0,0,1,0,0),(1,0,1,1,0)\\big)=2,\\\\\nd_{H}\\big((0,0,1,0,0),(1,1,0,0,1)\\big)=4,\\\\\n{H}\\big((0,0,1,0,0),(1,1,1,0,0)\\big)=2.\n\\end{aligned}\n$$\nThe minimum distance is $1$, achieved by two codewords: $(0,0,0,0,0)$ and $(0,0,1,0,1)$. Their likelihoods are equal, namely $p_{e}^{1}(1-p_{e})^{4}$. Therefore, the ML decoder cannot uniquely identify a single most likely codeword and, without an external tie-breaking rule, must declare a decoding failure.\n\nThus, the correct option is C.", "answer": "$$\\boxed{C}$$", "id": "1640448"}]}