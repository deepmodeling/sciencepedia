{"hands_on_practices": [{"introduction": "Let's begin our exploration with the most fundamental information source: a binary source. This practice [@problem_id:1653958] delves into the concept of coding redundancy, the difference between the average codeword length $G$ and the theoretical minimum, the entropy $H$. By analyzing how this redundancy changes with the source probabilities, you will gain a core insight into why optimal codes are not always perfectly efficient and under what conditions this inefficiency is most pronounced.", "problem": "Consider a binary information source that produces symbols from the set $\\{S_1, S_2\\}$. The probability of emitting symbol $S_1$ is $p$, and the probability of emitting symbol $S_2$ is $1-p$, where $0  p  1$.\n\nThe fundamental limit for data compression for this source is given by its Shannon entropy, $H$, defined as:\n$$H(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$$\nThe unit of entropy is bits per symbol.\n\nAn optimal prefix code (such as a Huffman code) for this source will have an average codeword length, denoted by $G$, in bits per symbol. The inefficiency of the code is captured by its redundancy, $\\rho$, defined as the difference between the average codeword length and the entropy:\n$$\\rho(p) = G(p) - H(p)$$\n\nYour task is to analyze the redundancy of the optimal code for this binary source. Determine the value of the probability $p$, constrained to $p \\leq 0.5$, for which the redundancy $\\rho(p)$ is exactly half of its maximum possible value over the domain $0  p  1$.\n\nWhich of the following values is closest to the correct value of $p$?\n\nA. 0.028\n\nB. 0.081\n\nC. 0.110\n\nD. 0.250\n\nE. 0.395", "solution": "The solution involves four main steps: determining the average codeword length $G(p)$, finding the maximum redundancy $\\rho_{max}$, setting up an equation for the desired condition, and testing the given options to find the solution.\n\nStep 1: Determine the average codeword length $G(p)$.\nFor a binary source with two symbols, $S_1$ and $S_2$, an optimal prefix code (like a Huffman code) will assign a single bit to each symbol. For example, the code could be $\\{S_1 \\to 0, S_2 \\to 1\\}$. The length of the codeword for $S_1$ is $l_1=1$ and for $S_2$ is $l_2=1$. The average codeword length $G(p)$ is the weighted average of these lengths, based on their probabilities:\n$$G(p) = p \\cdot l_1 + (1-p) \\cdot l_2 = p \\cdot 1 + (1-p) \\cdot 1 = p + 1 - p = 1$$\nSo, the average codeword length for an optimal code for this binary source is constant and equal to 1 bit per symbol for any $p \\in (0, 1)$.\n\nStep 2: Determine the maximum possible redundancy $\\rho_{max}$.\nThe redundancy is given by $\\rho(p) = G(p) - H(p)$. Substituting the value of $G(p)=1$, we get:\n$$\\rho(p) = 1 - H(p) = 1 - [-p \\log_2(p) - (1-p) \\log_2(1-p)]$$\nTo maximize the redundancy $\\rho(p)$, we must minimize the entropy $H(p)$. The binary entropy function $H(p)$ is defined on $p \\in (0, 1)$. It is a concave function with a maximum value of 1 at $p=0.5$ and minimum values at the boundaries of its domain. Let's examine the limit as $p$ approaches 0:\n$$\\lim_{p \\to 0^+} H(p) = \\lim_{p \\to 0^+} [-p \\log_2(p) - (1-p) \\log_2(1-p)]$$\nWe use the standard limit $\\lim_{x \\to 0^+} x \\log_b(x) = 0$. The second term becomes $\\lim_{p \\to 0^+} -(1-p) \\log_2(1-p) = -(1) \\log_2(1) = 0$.\nThus, $\\lim_{p \\to 0^+} H(p) = 0$. By symmetry, $\\lim_{p \\to 1^-} H(p) = 0$.\nThe minimum value of entropy is 0. Therefore, the maximum redundancy is:\n$$\\rho_{max} = G - H_{min} = 1 - 0 = 1 \\text{ bit per symbol}$$\n\nStep 3: Set up the equation.\nThe problem asks for the value of $p$ (with $p \\le 0.5$) for which the redundancy is half of its maximum value.\n$$\\rho(p) = \\frac{1}{2} \\rho_{max} = \\frac{1}{2} \\cdot 1 = 0.5$$\nSubstituting $\\rho(p) = 1 - H(p)$, we get:\n$$1 - H(p) = 0.5$$\n$$H(p) = 0.5$$\nThis leads to the transcendental equation:\n$$-p \\log_2(p) - (1-p) \\log_2(1-p) = 0.5$$\n\nStep 4: Test the given options.\nWe must find which of the given values of $p$ yields an entropy $H(p)$ closest to 0.5.\n\nA. For $p = 0.028$:\n$H(0.028) = -0.028 \\log_2(0.028) - 0.972 \\log_2(0.972) \\approx -0.028(-5.159) - 0.972(-0.041) \\approx 0.1444 + 0.0398 \\approx 0.184$\n\nB. For $p = 0.081$:\n$H(0.081) = -0.081 \\log_2(0.081) - 0.919 \\log_2(0.919) \\approx -0.081(-3.626) - 0.919(-0.122) \\approx 0.2937 + 0.1121 \\approx 0.406$\n\nC. For $p = 0.110$:\n$H(0.110) = -0.110 \\log_2(0.110) - 0.890 \\log_2(0.890) \\approx -0.110(-3.185) - 0.890(-0.167) \\approx 0.3504 + 0.1486 \\approx 0.499$\n\nD. For $p = 0.250$:\n$H(0.250) = -0.250 \\log_2(0.250) - 0.750 \\log_2(0.750) = -0.25(-2) - 0.75(\\log_2(3)-2) \\approx 0.5 - 0.75(1.585 - 2) \\approx 0.5 - 0.75(-0.415) \\approx 0.5 + 0.3113 \\approx 0.811$\n\nE. For $p = 0.395$:\n$H(0.395) = -0.395 \\log_2(0.395) - 0.605 \\log_2(0.605) \\approx -0.395(-1.340) - 0.605(-0.724) \\approx 0.5293 + 0.4380 \\approx 0.967$\n\nComparing the calculated entropy values to the target value of 0.5:\n- For $p=0.028$, $H \\approx 0.184$\n- For $p=0.081$, $H \\approx 0.406$\n- For $p=0.110$, $H \\approx 0.499$\n- For $p=0.250$, $H \\approx 0.811$\n- For $p=0.395$, $H \\approx 0.967$\n\nThe value of $H(0.110)$ is the closest to 0.5. Therefore, $p=0.110$ is the best choice.", "answer": "$$\\boxed{C}$$", "id": "1653958"}, {"introduction": "Building on the binary case, we now examine a source with three symbols to understand how redundancy behaves in a multi-symbol alphabet. This problem [@problem_id:1653979] presents a seemingly ideal scenario with equiprobable symbols, a situation one might guess has zero redundancy. By constructing the Huffman code and calculating the ratio $\\frac{G(X)}{H(X)}$, you will discover a fundamental reason why the average code length often exceeds the entropy, rooted in the mathematical properties of the probabilities themselves.", "problem": "A discrete memoryless source is described by a random variable $X$ that can take one of three possible symbols, $\\{S_1, S_2, S_3\\}$. The probabilities of these symbols are parameterized by a real number $p$ such that $0  p  1/2$. The probabilities are given by $P(X=S_1) = p$, $P(X=S_2) = p$, and $P(X=S_3) = 1-2p$.\n\nFor this source, two important quantities in information theory can be defined:\n1.  The Shannon entropy, $H(X)$, which is the theoretical lower bound on the average number of bits required to represent a symbol from this source. It is calculated as $H(X) = -\\sum_{i=1}^{3} P(X=S_i) \\log_{2}(P(X=S_i))$.\n2.  The average codeword length, $G(X)$, of a binary Huffman code designed for this source. Huffman coding provides an optimal prefix-free code, and its average length is given by $G(X) = \\sum_{i=1}^{3} P(X=S_i) l_i$, where $l_i$ is the length of the binary codeword assigned to symbol $S_i$.\n\nDetermine the exact value of the ratio $\\frac{G(X)}{H(X)}$ for the specific case where the source symbols are equiprobable, which corresponds to a particular value of $p$. Express your answer as a closed-form analytic expression.", "solution": "For equiprobable symbols, we have $P(X=S_{1})=P(X=S_{2})=P(X=S_{3})$, which with $P(X=S_{1})=P(X=S_{2})=p$ and $P(X=S_{3})=1-2p$ implies $p=\\frac{1}{3}$.\n\nThe Shannon entropy in bits is\n$$\nH(X)=-\\sum_{i=1}^{3} P(X=S_{i}) \\log_{2}\\big(P(X=S_{i})\\big)\n=-3 \\cdot \\frac{1}{3} \\log_{2}\\!\\left(\\frac{1}{3}\\right)\n=-\\log_{2}\\!\\left(\\frac{1}{3}\\right)\n=\\log_{2}(3).\n$$\n\nFor a binary Huffman code with three equiprobable symbols, the optimal code merges two equal-probability symbols first, yielding codeword lengths $\\{1,2,2\\}$. This satisfies the Kraft equality $2^{-1}+2\\cdot 2^{-2}=1$, so it is a valid optimal prefix-free binary code. The average codeword length is\n$$\nG(X)=\\sum_{i=1}^{3} P(X=S_{i})\\,l_{i}\n=\\frac{1}{3}\\cdot 1+\\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2\n=\\frac{5}{3}.\n$$\n\nTherefore, the ratio is\n$$\n\\frac{G(X)}{H(X)}=\\frac{\\frac{5}{3}}{\\log_{2}(3)}=\\frac{5}{3\\log_{2}(3)}.\n$$", "answer": "$$\\boxed{\\frac{5}{3\\log_{2}(3)}}$$", "id": "1653979"}, {"introduction": "Our final practice expands our view from single information sources to systems involving multiple, correlated sources. This thought experiment [@problem_id:1654016] explores the powerful concept of joint encoding, where dependencies between sources are exploited to achieve greater compression efficiency. By calculating the difference in ideal code lengths between separate and joint encoding—a quantity known as mutual information—you will quantify the performance gain from a more holistic coding strategy. Please note that this problem assumes an ideal scenario where average code length equals entropy ($G=H$) to isolate and highlight the gains from leveraging source correlation.", "problem": "In a data compression system, we analyze two correlated binary sources, $X$ and $Y$, which produce symbols from the alphabet $\\{0, 1\\}$. The joint behavior of these sources is described by a joint probability mass function $p(x, y) = P(X=x, Y=y)$. The specific probabilities are given as:\n$p(0, 0) = \\frac{1}{8}$\n$p(0, 1) = \\frac{3}{8}$\n$p(1, 0) = \\frac{3}{8}$\n$p(1, 1) = \\frac{1}{8}$\n\nIn this context, the theoretical minimum average length of a binary code for a source $S$ is its Shannon entropy, which we denote as $G(S)$. This ideal code length is measured in bits and is calculated using the formula $G(S) = H(S) = -\\sum_{i} p_i \\log_2(p_i)$, where $\\{p_i\\}$ is the probability distribution of the symbols from source $S$.\n\nWe can encode the sources $X$ and $Y$ separately, leading to a total ideal average code length of $G(X) + G(Y)$. Alternatively, we can design a single code for the joint source $(X, Y)$, which has an ideal average length of $G(X,Y)$. The difference, $\\Delta G = (G(X) + G(Y)) - G(X,Y)$, represents the redundancy that is eliminated by using a joint encoding scheme. It quantifies the benefit of accounting for the statistical dependence between the sources.\n\nCalculate the value of this redundancy, $\\Delta G$. Express your answer in bits, rounded to four significant figures.", "solution": "We are given a binary pair $(X,Y)$ with joint pmf $p(0,0)=\\frac{1}{8}$, $p(0,1)=\\frac{3}{8}$, $p(1,0)=\\frac{3}{8}$, $p(1,1)=\\frac{1}{8}$. The redundancy is $\\Delta G = G(X)+G(Y)-G(X,Y)$ with $G(\\cdot)=H(\\cdot)$ in bits, and $\\Delta G$ equals the mutual information $I(X;Y)$.\n\nFirst, compute the marginals:\n$$\nP(X=0)=p(0,0)+p(0,1)=\\frac{1}{8}+\\frac{3}{8}=\\frac{1}{2},\\quad P(X=1)=\\frac{1}{2},\n$$\n$$\nP(Y=0)=p(0,0)+p(1,0)=\\frac{1}{8}+\\frac{3}{8}=\\frac{1}{2},\\quad P(Y=1)=\\frac{1}{2}.\n$$\nHence $X$ and $Y$ are both Bernoulli with parameter $\\frac{1}{2}$, so\n$$\nH(X)=H(Y)=-\\sum_{x\\in\\{0,1\\}} P(X=x)\\,\\log_{2} P(X=x) = -2\\cdot \\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\n\nNext, compute the joint entropy:\n$$\nH(X,Y)=-\\sum_{x,y\\in\\{0,1\\}} p(x,y)\\,\\log_{2} p(x,y)\n= -\\Bigg[2\\cdot \\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right) + 2\\cdot \\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right)\\Bigg].\n$$\nUse $\\log_{2}\\!\\left(\\frac{1}{8}\\right)=-3$ and $\\log_{2}\\!\\left(\\frac{3}{8}\\right)=\\log_{2}(3)-3$ to simplify:\n$$\nH(X,Y) = -2\\left[\\frac{1}{8}(-3) + \\frac{3}{8}\\left(\\log_{2} 3 - 3\\right)\\right]\n= -2\\left[-\\frac{3}{8} + \\frac{3}{8}\\log_{2} 3 - \\frac{9}{8}\\right]\n= -2\\left[\\frac{3}{8}\\log_{2} 3 - \\frac{12}{8}\\right].\n$$\nThus,\n$$\nH(X,Y) = 3 - \\frac{3}{4}\\log_{2} 3.\n$$\n\nTherefore,\n$$\n\\Delta G = H(X)+H(Y)-H(X,Y) = 1 + 1 - \\left(3 - \\frac{3}{4}\\log_{2} 3\\right) = \\frac{3}{4}\\log_{2} 3 - 1.\n$$\nNumerically, using $\\log_{2} 3 \\approx 1.5849625007$, we obtain\n$$\n\\Delta G \\approx \\frac{3}{4}\\cdot 1.5849625007 - 1 \\approx 0.1887218755 \\text{ bits}.\n$$\nRounded to four significant figures, this is $0.1887$ bits.", "answer": "$$\\boxed{0.1887}$$", "id": "1654016"}]}