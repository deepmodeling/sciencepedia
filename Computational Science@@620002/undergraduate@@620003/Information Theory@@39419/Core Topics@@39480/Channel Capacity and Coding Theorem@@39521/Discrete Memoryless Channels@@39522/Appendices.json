{"hands_on_practices": [{"introduction": "Understanding the fundamental limits of communication often starts with analyzing idealized scenarios. This exercise explores a special type of channel where the possible outputs for each input are completely separate. This \"partitioned output\" structure [@problem_id:1618511], though a hypothetical construct for pedagogical purposes, brilliantly illustrates a case where the output perfectly reveals the input, making the conditional entropy $H(X|Y)$ zero. This practice sharpens your ability to connect a channel's physical description to its information-theoretic properties, simplifying the capacity calculation to maximizing the input entropy $H(X)$.", "problem": "A novel communication system, termed a \"Partitioned Output Channel,\" is designed for a specialized application. The system has a finite set of input symbols, $\\mathcal{X} = \\{x_1, x_2, \\ldots, x_K\\}$, where $K$ is the total number of distinct input symbols. The set of output symbols is denoted by $\\mathcal{Y}$.\n\nA defining characteristic of this channel is its output structure. The output alphabet $\\mathcal{Y}$ is partitioned into $K$ disjoint, non-empty subsets, $\\mathcal{Y}_1, \\mathcal{Y}_2, \\ldots, \\mathcal{Y}_K$, such that $\\mathcal{Y} = \\bigcup_{i=1}^{K} \\mathcal{Y}_i$ and $\\mathcal{Y}_i \\cap \\mathcal{Y}_j = \\emptyset$ for all $i \\neq j$.\n\nThe channel's behavior is dictated by the following rule: when an input symbol $x_i$ is transmitted, the resulting output symbol $y$ is guaranteed to belong to the corresponding subset $\\mathcal{Y}_i$. Formally, the channel's conditional probability distribution $p(y|x_i)$ satisfies $p(y|x_i) = 0$ if $y \\notin \\mathcal{Y}_i$. For any given input $x_i$, the sum of probabilities over its associated output subset is one, i.e., $\\sum_{y \\in \\mathcal{Y}_i} p(y|x_i) = 1$. The specific probabilities $p(y|x_i)$ for $y \\in \\mathcal{Y}_i$ are non-zero but are not specified further.\n\nAssuming this system is modeled as a discrete memoryless channel, determine its channel capacity, $C$. Express your answer as a symbolic expression in terms of the number of input symbols, $K$. The logarithm in your final expression must be base 2.", "solution": "Let $X$ be the channel input taking values in $\\mathcal{X}=\\{x_{1},\\ldots,x_{K}\\}$ and $Y$ the output taking values in $\\mathcal{Y}=\\bigcup_{i=1}^{K}\\mathcal{Y}_{i}$ with $\\mathcal{Y}_{i}\\cap\\mathcal{Y}_{j}=\\emptyset$ for $i\\neq j$. The capacity of a discrete memoryless channel is\n$$\nC=\\max_{p(x)} I(X;Y),\n$$\nwhere\n$$\nI(X;Y)=H(X)-H(X|Y).\n$$\nBy the partitioned-output property, for each $y\\in\\mathcal{Y}$ there exists a unique index $i$ such that $y\\in\\mathcal{Y}_{i}$. The channel law satisfies $p(y|x_{j})=0$ for all $j\\neq i$, while $p(y|x_{i})>0$. Hence, for any prior $p(x)$,\n$$\np(x_{i}|y)=\\frac{p(y|x_{i})p(x_{i})}{\\sum_{j=1}^{K}p(y|x_{j})p(x_{j})}=\\frac{p(y|x_{i})p(x_{i})}{p(y|x_{i})p(x_{i})}=1,\n$$\nand $p(x_{j}|y)=0$ for $j\\neq i$. Therefore $H(X|Y=y)=0$ for every $y$, which implies\n$$\nH(X|Y)=\\sum_{y}p(y)H(X|Y=y)=0.\n$$\nConsequently,\n$$\nI(X;Y)=H(X).\n$$\nMaximizing mutual information reduces to maximizing the input entropy over $K$ symbols. The entropy bound gives\n$$\nH(X)\\leq \\log_{2}(K),\n$$\nwith equality achieved by the uniform input distribution $p(x_{i})=\\frac{1}{K}$ for all $i$. Hence,\n$$\nC=\\max_{p(x)}I(X;Y)=\\max_{p(x)}H(X)=\\log_{2}(K).\n$$", "answer": "$$\\boxed{\\log_{2}(K)}$$", "id": "1618511"}, {"introduction": "Building on the concept of how channel structure affects capacity, we now turn to deterministic channels. In these systems, the output is a fixed function of the input, meaning there is no ambiguity about the output for a given input, i.e., $H(Y|X)=0$. This exercise [@problem_id:1618445] presents a channel that deterministically sums two independent binary inputs. The challenge lies not in overcoming noise, but in shaping the input statistics to maximize the variety and unpredictability of the output symbols, thereby maximizing the output entropy $H(Y)$ and, consequently, the channel capacity.", "problem": "Consider a digital encoder designed for a specialized communication system. The encoder accepts two independent binary data streams, represented by random variables $X_1$ and $X_2$. Each variable can take a value from the set $\\{0, 1\\}$. The encoder combines a bit from each stream, forming an input pair $(X_1, X_2)$, and maps it to a single output symbol $Y$ according to the deterministic rule $Y = X_1 + X_2$. The output symbol $Y$ is then transmitted. This process defines a discrete memoryless channel whose input alphabet is the set of pairs $\\mathcal{X} = \\{(0,0), (0,1), (1,0), (1,1)\\}$ and whose output alphabet is $\\mathcal{Y} = \\{0, 1, 2\\}$.\n\nThe capacity of this channel is defined as the maximum possible mutual information between the input pair $(X_1, X_2)$ and the output $Y$, under the crucial constraint that the input bits $X_1$ and $X_2$ are always statistically independent.\n\nCalculate the capacity of this channel. Express your answer in bits per channel use.", "solution": "Let $X_{1} \\sim \\operatorname{Bern}(p)$ and $X_{2} \\sim \\operatorname{Bern}(q)$ be independent. The channel is deterministic with rule $Y=X_{1}+X_{2}$, so for any input pair $(X_{1},X_{2})$ the output $Y$ is fixed. Therefore,\n$$\nI\\big((X_{1},X_{2});Y\\big)=H(Y)-H\\big(Y \\mid X_{1},X_{2}\\big)=H(Y).\n$$\nThus the capacity under the independence constraint is\n$$\nC=\\max_{p \\in [0,1],\\, q \\in [0,1]} H(Y).\n$$\nThe distribution of $Y$ induced by independent $X_{1}$ and $X_{2}$ is\n$$\n\\begin{aligned}\nr_{0} &\\triangleq \\Pr(Y=0)=\\Pr(X_{1}=0, X_{2}=0)=(1-p)(1-q),\\\\\nr_{1} &\\triangleq \\Pr(Y=1)=\\Pr(X_{1}=1, X_{2}=0)+\\Pr(X_{1}=0, X_{2}=1)=p(1-q)+(1-p)q=p+q-2pq,\\\\\nr_{2} &\\triangleq \\Pr(Y=2)=\\Pr(X_{1}=1, X_{2}=1)=pq,\n\\end{aligned}\n$$\nwith $r_{0}+r_{1}+r_{2}=1$. The entropy in bits is\n$$\nH(Y)=-\\sum_{y \\in \\{0,1,2\\}} r_{y} \\log_{2} r_{y}.\n$$\nWe maximize $H(Y)$ over $p,q \\in [0,1]$. Compute the partial derivatives using the identity\n$$\n\\frac{\\partial}{\\partial \\theta} H(Y)=-\\sum_{y} \\frac{\\partial r_{y}}{\\partial \\theta} \\log_{2} r_{y},\n$$\nwhich follows from $\\sum_{y} r_{y}=1$. With respect to $p$,\n$$\n\\frac{\\partial r_{0}}{\\partial p}=-(1-q)=q-1,\\quad \\frac{\\partial r_{1}}{\\partial p}=1-2q,\\quad \\frac{\\partial r_{2}}{\\partial p}=q,\n$$\nso the stationarity condition $\\frac{\\partial H}{\\partial p}=0$ is\n$$\n(q-1)\\log_{2} r_{0}+(1-2q)\\log_{2} r_{1}+q\\log_{2} r_{2}=0.\n$$\nSimilarly, with respect to $q$,\n$$\n(p-1)\\log_{2} r_{0}+(1-2p)\\log_{2} r_{1}+p\\log_{2} r_{2}=0.\n$$\nSubtracting these two equations gives\n$$\n(q-p)\\left(\\log_{2} r_{0}-2\\log_{2} r_{1}+\\log_{2} r_{2}\\right)=0,\n$$\nso any interior stationary point must satisfy either $p=q$ or $r_{0}r_{2}=r_{1}^{2}$. The latter is impossible in the interior: writing $p=\\frac{u}{1+u}$ and $q=\\frac{v}{1+v}$ with $u,v \\in [0,\\infty)$ yields $r_{0}r_{2}=\\frac{uv}{(1+u)^{2}(1+v)^{2}}$ and $r_{1}=\\frac{u+v}{(1+u)(1+v)}$, hence $r_{0}r_{2}=r_{1}^{2}$ implies $uv=(u+v)^{2}$, i.e., $u^{2}+uv+v^{2}=0$, which forces $u=v=0$ (a boundary point). Therefore any interior maximizer must have $p=q$.\n\nSet $p=q=a$. Then $Y \\sim \\operatorname{Binomial}(2,a)$ with\n$$\nr_{0}=(1-a)^{2},\\quad r_{1}=2a(1-a),\\quad r_{2}=a^{2}.\n$$\nDefine $g(a)=H(Y)$. By symmetry $g(a)=g(1-a)$, so $a=\\frac{1}{2}$ is a critical point. Computing the derivative using\n$$\n\\frac{d g}{d a}= -\\left(\\frac{d r_{0}}{d a}\\log_{2} r_{0}+\\frac{d r_{1}}{d a}\\log_{2} r_{1}+\\frac{d r_{2}}{d a}\\log_{2} r_{2}\\right),\n$$\nwith $\\frac{d r_{0}}{d a}=2(a-1)$, $\\frac{d r_{1}}{d a}=2-4a$, $\\frac{d r_{2}}{d a}=2a$, gives at $a=\\frac{1}{2}$:\n$$\n\\left.(a-1)\\log_{2} r_{0}+(1-2a)\\log_{2} r_{1}+a\\log_{2} r_{2}\\right|_{a=\\frac{1}{2}}=\\left(-\\frac{1}{2}\\right)\\log_{2}\\left(\\frac{1}{4}\\right)+0\\cdot \\log_{2}\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\log_{2}\\left(\\frac{1}{4}\\right)=0,\n$$\nconfirming stationarity. Boundary cases (e.g., $q=0$ or $q=1$) yield $H(Y)\\leq 1$, since then $Y$ is binary. Therefore the interior stationary point must be the maximizer. Evaluating $H(Y)$ at $a=\\frac{1}{2}$,\n$$\nH(Y)= -\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)-\\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right)-\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)=\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}=\\frac{3}{2}.\n$$\nHence the channel capacity under the independence constraint is $\\frac{3}{2}$ bits per channel use.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1618445"}, {"introduction": "Many practical communication channels exhibit asymmetric noise, where some symbols are more prone to error than others. This practice problem models such a scenario with a binary channel where one symbol is transmitted perfectly, while the other is susceptible to a specific error. To find the capacity [@problem_id:1618455], you must engage with the full expression for mutual information, $I(X;Y) = H(Y) - H(Y|X)$, as both terms are non-trivial. This exercise provides excellent hands-on practice in applying calculus to optimize the input distribution, balancing the trade-off between a high-entropy output and the information lost due to channel noise.", "problem": "Consider a simplified model for an optical communication system that transmits binary data, '0's and '1's, as pulses of light. The system is designed such that a '1' is represented by a high-intensity pulse and a '0' by a low-intensity pulse. The detector is perfectly reliable at identifying high-intensity pulses, so when a '1' is sent, it is always received correctly as a '1'. However, the low-intensity pulse representing a '0' can be affected by stray photons or thermal noise in the detector. This results in a non-zero probability, denoted by $p$, that a sent '0' is incorrectly registered as a '1'. A '0' is received correctly with probability $1-p$. We assume the channel is memoryless, meaning the outcome of each bit transmission is independent of all others, and that the noise parameter $p$ is a constant such that $0 < p < 1$.\n\nLet the random variable $X$ represent the binary input to this channel and $Y$ represent the binary output. Your task is to determine two quantities: the channel capacity, $C$, which is the maximum possible rate of information transmission through this channel, and the specific input probability distribution that achieves this capacity. Let this optimal distribution be defined by the probability of sending a '0', denoted by $w_0 = P(X=0)$.\n\nProvide two analytic expressions in terms of the error probability $p$ and the binary entropy function, which is defined as $H_2(x) = -x \\log_2(x) - (1-x) \\log_2(1-x)$. The first expression should be for the capacity $C$, in bits per channel use, and the second for the optimal input probability $w_0$.", "solution": "Define the binary-input, binary-output channel by the conditional probabilities: for $X \\in \\{0,1\\}$ and $Y \\in \\{0,1\\}$,\n$$\nP(Y=1 \\mid X=1)=1, \\quad P(Y=0 \\mid X=1)=0, \\quad P(Y=1 \\mid X=0)=p, \\quad P(Y=0 \\mid X=0)=1-p,\n$$\nwith $0<p<1$. Let $w_{0}=P(X=0)=q$ and $w_{1}=P(X=1)=1-q$. The output distribution is\n$$\nP(Y=1)=p q+(1-q)=1-q(1-p), \\quad P(Y=0)=(1-p) q.\n$$\nHence the output entropy is\n$$\nH(Y)=H_{2}(P(Y=1))=H_{2}(1-q(1-p))=H_{2}(q(1-p)),\n$$\nusing $H_{2}(x)=H_{2}(1-x)$. The conditional entropy is\n$$\nH(Y \\mid X)=P(X=0) H(Y \\mid X=0)+P(X=1) H(Y \\mid X=1)=q\\,H_{2}(p)+ (1-q)\\cdot 0=q\\,H_{2}(p).\n$$\nTherefore the mutual information as a function of $q$ is\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=H_{2}\\big((1-p) q\\big)-q\\,H_{2}(p).\n$$\nTo find the capacity, maximize $I(X;Y)$ over $q \\in [0,1]$. Differentiate with respect to $q$; using $\\frac{d}{du}H_{2}(u)=\\log_{2}\\!\\left(\\frac{1-u}{u}\\right)$ and the chain rule gives\n$$\n\\frac{d}{dq} I(X;Y)=(1-p)\\,\\log_{2}\\!\\left(\\frac{1-(1-p) q}{(1-p) q}\\right)-H_{2}(p).\n$$\nSetting the derivative to zero yields\n$$\n(1-p)\\,\\log_{2}\\!\\left(\\frac{1-(1-p) q^{\\star}}{(1-p) q^{\\star}}\\right)=H_{2}(p).\n$$\nLet $r^{\\star}=(1-p) q^{\\star}$. Then\n$$\n\\log_{2}\\!\\left(\\frac{1-r^{\\star}}{r^{\\star}}\\right)=\\frac{H_{2}(p)}{1-p}\n\\quad\\Longrightarrow\\quad\n\\frac{1-r^{\\star}}{r^{\\star}}=2^{\\frac{H_{2}(p)}{1-p}}\n\\quad\\Longrightarrow\\quad\nr^{\\star}=\\frac{1}{1+2^{\\frac{H_{2}(p)}{1-p}}}.\n$$\nThus the capacity-achieving input probability of sending a zero is\n$$\nw_{0}^{\\star}=q^{\\star}=\\frac{r^{\\star}}{1-p}=\\frac{1}{(1-p)\\left(1+2^{\\frac{H_{2}(p)}{1-p}}\\right)}.\n$$\nConcavity follows from $\\frac{d^{2}}{du^{2}}H_{2}(u)=-\\frac{1}{\\ln 2}\\left(\\frac{1}{u}+\\frac{1}{1-u}\\right)<0$ and the linear change of variables $u=(1-p) q$, so this stationary point gives the unique global maximum.\n\nEvaluate the capacity at $q^{\\star}$:\n$$\nC=I(X;Y)\\big|_{q=q^{\\star}}=H_{2}(r^{\\star})-\\frac{r^{\\star}}{1-p}\\,H_{2}(p).\n$$\nLet $a=\\frac{H_{2}(p)}{1-p}$ and $A=2^{a}$. With $r^{\\star}=\\frac{1}{1+A}$, compute\n$$\nH_{2}(r^{\\star})=-\\frac{1}{1+A}\\log_{2}\\!\\frac{1}{1+A}-\\frac{A}{1+A}\\log_{2}\\!\\frac{A}{1+A}\n=\\log_{2}(1+A)-\\frac{A}{1+A}\\log_{2}A.\n$$\nSince $\\log_{2}A=a$, we have\n$$\nC=\\left[\\log_{2}(1+A)-\\frac{A}{1+A}a\\right]-\\frac{1}{1-p}\\cdot\\frac{1}{1+A}H_{2}(p)\n=\\log_{2}(1+A)-a.\n$$\nReturning to $a=\\frac{H_{2}(p)}{1-p}$ and $A=2^{a}$ gives the closed form\n$$\nC=\\log_{2}\\!\\left(1+2^{\\frac{H_{2}(p)}{1-p}}\\right)-\\frac{H_{2}(p)}{1-p}.\n$$\nThese expressions provide the channel capacity in bits per channel use and the capacity-achieving input probability of sending a zero.", "answer": "$$\\boxed{\\begin{pmatrix}\\log_{2}\\!\\left(1+2^{\\frac{H_{2}(p)}{1-p}}\\right)-\\frac{H_{2}(p)}{1-p} & \\frac{1}{(1-p)\\left(1+2^{\\frac{H_{2}(p)}{1-p}}\\right)}\\end{pmatrix}}$$", "id": "1618455"}]}