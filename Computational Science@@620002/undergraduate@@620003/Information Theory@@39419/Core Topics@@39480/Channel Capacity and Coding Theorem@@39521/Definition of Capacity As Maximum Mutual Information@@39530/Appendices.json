{"hands_on_practices": [{"introduction": "The definition of channel capacity, $C = \\max_{p(x)} I(X;Y)$, tells us to find the input distribution that maximizes the mutual information between the input $X$ and the output $Y$. This first exercise provides a clear, intuitive scenario to practice this core concept. We will analyze a simple channel where some inputs are transmitted perfectly while one introduces ambiguity, forcing us to consider the trade-off between using all available symbols and maximizing the rate of reliable communication [@problem_id:1617016]. This practice illuminates how the conditional entropy $H(Y|X)$, representing noise or uncertainty, directly penalizes mutual information and why the optimal strategy may involve avoiding \"noisy\" inputs altogether.", "problem": "A communication system is designed with a set of three possible input symbols from the alphabet $\\mathcal{X} = \\{A, B, C\\}$, and a set of two possible output symbols from the alphabet $\\mathcal{Y} = \\{A, B\\}$. The behavior of this discrete memoryless channel is defined by the following conditional probabilities:\n-   An input of $A$ is always received as an output of $A$.\n-   An input of $B$ is always received as an output of $B$.\n-   An input of $C$ results in a random output, which is equally likely to be $A$ or $B$.\n\nThe capacity of a channel is defined as the maximum possible mutual information between the input $X$ and the output $Y$, where the maximization is performed over all possible input probability distributions $P(X)$.\n\nCalculate the capacity of this channel. Express your final answer in units of bits, as a single, exact numerical value. All logarithms should be interpreted as base-2.", "solution": "Let the input distribution be $P(X=A)=a$, $P(X=B)=b$, and $P(X=C)=c$ with $a,b,c\\geq 0$ and $a+b+c=1$. From the channel law,\n- $P(Y=A\\mid X=A)=1$ and $P(Y=B\\mid X=A)=0$,\n- $P(Y=A\\mid X=B)=0$ and $P(Y=B\\mid X=B)=1$,\n- $P(Y=A\\mid X=C)=\\frac{1}{2}$ and $P(Y=B\\mid X=C)=\\frac{1}{2}$.\n\nThus the output distribution is\n$$\nP(Y=A)=a+\\frac{c}{2},\\qquad P(Y=B)=b+\\frac{c}{2}.\n$$\nThe output entropy (with base-2 logarithms) is the binary entropy\n$$\nH(Y)= -\\left(a+\\frac{c}{2}\\right)\\log_{2}\\left(a+\\frac{c}{2}\\right) - \\left(b+\\frac{c}{2}\\right)\\log_{2}\\left(b+\\frac{c}{2}\\right).\n$$\nThe conditional entropy is\n$$\nH(Y\\mid X)= a\\cdot 0 + b\\cdot 0 + c\\cdot 1 = c,\n$$\nsince $Y$ is deterministic given $X=A$ or $X=B$, and $Y$ is equiprobable given $X=C$. Therefore the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)= h_{2}\\!\\left(a+\\frac{c}{2}\\right)-c,\n$$\nwhere $h_{2}(u)=-u\\log_{2}u-(1-u)\\log_{2}(1-u)$.\n\nFor any fixed $c\\in[0,1]$, $h_{2}(u)$ is maximized at $u=\\frac{1}{2}$. The constraint set allows $u=a+\\frac{c}{2}$ to equal $\\frac{1}{2}$ by choosing $a=\\frac{1}{2}-\\frac{c}{2}$ and hence $b=1-a-c=\\frac{1}{2}-\\frac{c}{2}$, which are nonnegative for all $c\\in[0,1]$. Thus, for fixed $c$,\n$$\n\\max_{a,b} I(X;Y)= 1 - c.\n$$\nMaximizing over $c\\in[0,1]$ yields $c^{\\star}=0$ and\n$$\n\\max_{a,b,c} I(X;Y)= 1.\n$$\nThis achieves the general upper bound $I(X;Y)\\leq H(Y)\\leq \\log_{2}|\\mathcal{Y}|=\\log_{2}2=1$, confirming optimality. One achieving distribution is $a=b=\\frac{1}{2}$ and $c=0$, i.e., never use $C$.\n\nTherefore, the channel capacity is $1$ bit.", "answer": "$$\\boxed{1}$$", "id": "1617016"}, {"introduction": "Moving beyond channels with random errors, this next problem explores a channel that is technically \"noiseless,\" meaning the output is a deterministic function of the input, and thus $H(Y|X) = 0$. However, the channel is not perfectly invertible, as different input combinations can produce the same output. In this case, maximizing mutual information simplifies to maximizing the output entropy, $I(X;Y) = H(Y)$ [@problem_id:1648936]. This exercise challenges you to find the input statistics that make the output as uncertain, or varied, as possible, demonstrating that channel capacity can be limited by the structure of the channel's function itself, not just by noise.", "problem": "A simplified digital communication system is designed to transmit information using two parallel, independent binary sources. Let the random variables $X_1$ and $X_2$ represent the bits sent from source 1 and source 2, respectively, where $X_i \\in \\{0, 1\\}$. A fundamental constraint of the system is that the inputs $X_1$ and $X_2$ are always statistically independent.\n\nAt the receiver, a simple adder circuit combines these two signals to produce a single output signal, $Y$, which is the arithmetic sum of the input bits: $Y = X_1 + X_2$. The set of possible input pairs is $\\mathcal{X} = \\{(0,0), (0,1), (1,0), (1,1)\\}$, and the set of possible outputs is $\\mathcal{Y} = \\{0, 1, 2\\}$.\n\nThe operator of this system can choose the statistics of the independent sources. Let $p_1 = P(X_1=1)$ and $p_2 = P(X_2=1)$ be the probabilities that each source sends a '1'. The capacity of this channel is defined as the maximum mutual information $I((X_1, X_2); Y)$ over all possible choices of $p_1$ and $p_2$ in the range $[0, 1]$.\n\nCalculate the capacity of this channel. Express your final answer as a single real number in units of bits per channel use.", "solution": "Let the input to the channel be the random variable $X = (X_1, X_2)$ and the output be $Y = X_1 + X_2$. The capacity $C$ of the channel is defined as the maximum of the mutual information $I(X; Y)$ over all possible input distributions.\n\n$$C = \\max_{p(x)} I(X; Y)$$\n\nThe mutual information is given by $I(X; Y) = H(Y) - H(Y|X)$.\nThe output $Y$ is a deterministic function of the input $X$. Specifically, if we know the input pair $(X_1, X_2)$, the output $Y = X_1 + X_2$ is fully determined. This means the conditional entropy of the output given the input, $H(Y|X)$, is zero.\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) H(Y|X=x) = \\sum_{x \\in \\mathcal{X}} p(x) \\cdot 0 = 0$$\nTherefore, the mutual information simplifies to the entropy of the output:\n$$I(X; Y) = H(Y)$$\nThe capacity is thus the maximum possible entropy of the output, where the maximization is performed over the set of allowed input distributions.\n$$C = \\max_{p(x)} H(Y)$$\nThe problem specifies that the input bits $X_1$ and $X_2$ are independent. Let $P(X_1=1) = p_1$ and $P(X_2=1) = p_2$. The distribution of the input pairs is then:\n$P(X=(0,0)) = (1-p_1)(1-p_2)$\n$P(X=(0,1)) = (1-p_1)p_2$\n$P(X=(1,0)) = p_1(1-p_2)$\n$P(X=(1,1)) = p_1 p_2$\n\nNow, we determine the distribution of the output $Y$. The possible values for $Y$ are $0, 1, 2$.\n$P(Y=0) = P(X=(0,0)) = (1-p_1)(1-p_2)$\n$P(Y=1) = P(X=(0,1)) + P(X=(1,0)) = (1-p_1)p_2 + p_1(1-p_2) = p_1 + p_2 - 2p_1p_2$\n$P(Y=2) = P(X=(1,1)) = p_1 p_2$\n\nThe entropy of the output $Y$ is a function of $p_1$ and $p_2$:\n$$H(Y) = -P(Y=0)\\log_2(P(Y=0)) - P(Y=1)\\log_2(P(Y=1)) - P(Y=2)\\log_2(P(Y=2))$$\nWe need to maximize this function $H(Y; p_1, p_2)$ for $p_1, p_2 \\in [0, 1]$.\n\nThe structure of the problem is symmetric with respect to $X_1$ and $X_2$. Swapping $p_1$ and $p_2$ leaves the probabilities $P(Y=0)$, $P(Y=1)$, and $P(Y=2)$ unchanged. This suggests that the maximum value of $H(Y)$ will occur when $p_1 = p_2$. Let's set $p_1 = p_2 = p$.\n\nWith $p_1=p_2=p$, the output distribution becomes:\n$P(Y=0) = (1-p)(1-p) = (1-p)^2$\n$P(Y=1) = p(1-p) + (1-p)p = 2p(1-p)$\n$P(Y=2) = p \\cdot p = p^2$\nThis is a binomial distribution for the sum of two independent Bernoulli trials, $Y \\sim \\text{Binomial}(2, p)$.\nThe entropy is now a function of a single variable $p$:\n$$H(Y; p) = - (1-p)^2 \\log_2((1-p)^2) - 2p(1-p) \\log_2(2p(1-p)) - p^2 \\log_2(p^2)$$\nTo find the value of $p$ that maximizes this entropy, we can take the derivative with respect to $p$ and set it to zero. Using the natural logarithm $\\ln$ for convenience (since $\\log_2(x) = \\ln(x)/\\ln(2)$), we have:\n$\\frac{d}{dp} H(Y; p) \\propto \\frac{d}{dp} \\left[ -(1-p)^2 \\ln((1-p)^2) - 2p(1-p) \\ln(2p(1-p)) - p^2 \\ln(p^2) \\right]$\nA known result for the entropy of a binomial distribution $\\text{Binomial}(n,p)$ is that it is maximized at $p=1/2$. Let's verify this for $n=2$ by setting the derivative to zero. The derivative is proportional to:\n$$\\frac{d H}{dp} \\propto \\ln\\left(\\frac{1-p}{p}\\right) - (1-2p)\\ln(2)$$\nSetting the derivative to zero:\n$$\\ln\\left(\\frac{1-p}{p}\\right) = (1-2p)\\ln(2) = \\ln(2^{1-2p})$$\n$$\\frac{1-p}{p} = 2^{1-2p}$$\nBy inspection, $p=1/2$ is a solution:\nLHS: $\\frac{1-1/2}{1/2} = \\frac{1/2}{1/2} = 1$.\nRHS: $2^{1-2(1/2)} = 2^{1-1} = 2^0 = 1$.\nThus, the entropy is maximized at $p=1/2$. This corresponds to choosing $p_1=p_2=1/2$.\n\nWith $p_1=p_2=1/2$, the input distribution is uniform over the four possible pairs: $P(X=(x_1,x_2)) = 1/4$ for all $(x_1,x_2) \\in \\mathcal{X}$. The corresponding output distribution is:\n$P(Y=0) = (1-1/2)^2 = 1/4$\n$P(Y=1) = 2(1/2)(1-1/2) = 1/2$\n$P(Y=2) = (1/2)^2 = 1/4$\n\nThe maximum entropy $H(Y)$ is calculated with this optimal distribution:\n$$C = H(Y) = -\\left( \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) \\right)$$\n$$C = -\\left( \\frac{1}{4}(-2) + \\frac{1}{2}(-1) + \\frac{1}{4}(-2) \\right)$$\n$$C = -\\left( -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right)$$\n$$C = \\frac{3}{2} = 1.5$$\nThe capacity of the channel is 1.5 bits per channel use.", "answer": "$$\\boxed{1.5}$$", "id": "1648936"}, {"introduction": "Our final practice elevates the analysis to a more realistic engineering scenario by introducing a resource constraint. In many practical communication systems, transmitting different symbols consumes different amounts of energy, and there is often a strict budget for the average energy used. This problem requires you to find the capacity of a channel not just by maximizing mutual information, but by doing so under a specific average cost constraint [@problem_id:1617058]. This advanced exercise introduces the powerful method of constrained optimization, showing how to balance the goal of high information throughput with the practical limitations of a physical system.", "problem": "A deep-space probe uses a specialized communication system to receive commands from Earth. The system has an input alphabet of three symbols, $\\mathcal{X} = \\{x_1, x_2, x_3\\}$, corresponding to different energy-level signals. The energy cost associated with transmitting each symbol is $c(x_1) = 0$ Joules, $c(x_2) = 3$ Joules, and $c(x_3) = 5$ Joules. Due to extreme power limitations on the probe's transmitter, the long-term average energy cost per transmitted symbol must be exactly $S = 4$ Joules.\n\nThe communication channel is a discrete memoryless channel with output alphabet $\\mathcal{Y} = \\{y_1, y_2, y_3\\}$. The channel's behavior is characterized by the following conditional probabilities:\n- An input of $x_1$ (no signal) is always correctly identified as output $y_1$.\n- An input of $x_2$ (medium signal) is correctly identified as output $y_2$ with probability $3/4$, but due to noise, it may be mistaken for output $y_1$ (a \"dropout\") with probability $1/4$. It is never mistaken for $y_3$.\n- An input of $x_3$ (strong signal) is always correctly identified as output $y_3$.\n\nTo operate the system most efficiently, the engineers must choose an input probability distribution $p(x_1), p(x_2), p(x_3)$ that maximizes the mutual information $I(X;Y)$ subject to the average energy cost constraint.\n\nDetermine the optimal probability $p(x_2)$ for using the second symbol. Express your answer as a closed-form analytic expression.", "solution": "Let $p_{i} \\triangleq p(x_{i})$ with $p_{1}+p_{2}+p_{3}=1$, and impose the exact average cost constraint $3 p_{2}+5 p_{3}=4$. Eliminating $p_{3}$ gives $p_{3}=(4-3 p_{2})/5$ and then $p_{1}=1-p_{2}-p_{3}=(1-2 p_{2})/5$. Feasibility requires $p_{2}\\in[0,1/2]$.\n\nThe channel transition probabilities are\n- $P(y_{1}\\mid x_{1})=1$,\n- $P(y_{1}\\mid x_{2})=\\frac{1}{4}$, $P(y_{2}\\mid x_{2})=\\frac{3}{4}$,\n- $P(y_{3}\\mid x_{3})=1$.\n\nHence the output distribution is\n$$\nq_{1}\\triangleq P(y_{1})=p_{1}+\\frac{1}{4}p_{2}=\\frac{1}{5}-\\frac{3}{20}p_{2}=\\frac{4-3 p_{2}}{20},\\quad\nq_{2}\\triangleq P(y_{2})=\\frac{3}{4}p_{2},\\quad\nq_{3}\\triangleq P(y_{3})=p_{3}=\\frac{4-3 p_{2}}{5}=4 q_{1}.\n$$\nThe conditional entropy is\n$$\nH(Y\\mid X)=p_{1}\\cdot 0+p_{2}\\left[-\\frac{1}{4}\\ln\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\ln\\!\\left(\\frac{3}{4}\\right)\\right]+p_{3}\\cdot 0\n= p_{2}\\,h,\n$$\nwhere $h\\triangleq -\\frac{1}{4}\\ln(1/4)-\\frac{3}{4}\\ln(3/4)$, and\n$$\nH(Y)=-\\sum_{i=1}^{3} q_{i}\\ln q_{i}.\n$$\nTherefore $I(X;Y)=H(Y)-H(Y\\mid X)=-\\sum_{i} q_{i}\\ln q_{i}-p_{2}h$ as a function of $p_{2}$.\n\nDifferentiate with respect to $p_{2}$. Using $q_{1}'=-\\frac{3}{20}$, $q_{2}'=\\frac{3}{4}$, $q_{3}'=-\\frac{3}{5}$ and $\\sum_{i}q_{i}'=0$, we obtain\n$$\n\\frac{d I}{d p_{2}}\n= -\\sum_{i}(\\ln q_{i}+1) q_{i}' - h\n= -\\sum_{i} q_{i}' \\ln q_{i} - h\n= \\frac{3}{20}\\ln q_{1}-\\frac{3}{4}\\ln q_{2}+\\frac{3}{5}\\ln q_{3}-h.\n$$\nSetting $\\frac{d I}{d p_{2}}=0$ yields\n$$\n\\frac{1}{20}\\ln q_{1}-\\frac{1}{4}\\ln q_{2}+\\frac{1}{5}\\ln q_{3}=\\frac{h}{3}\n\\;\\Longleftrightarrow\\;\n\\ln q_{1}-5\\ln q_{2}+4\\ln q_{3}=\\frac{20 h}{3}.\n$$\nExponentiating,\n$$\nq_{1}\\,q_{3}^{4}\\,q_{2}^{-5}=\\exp\\!\\left(\\frac{20 h}{3}\\right).\n$$\nUsing $q_{3}=4 q_{1}$, the left side becomes $4^{4}(q_{1}/q_{2})^{5}$, so\n$$\n4^{4}\\left(\\frac{q_{1}}{q_{2}}\\right)^{5}=\\exp\\!\\left(\\frac{20 h}{3}\\right).\n$$\nCompute the right side from $h$:\n$$\n\\exp(h)=(1/4)^{-1/4}(3/4)^{-3/4}\\;\\Longrightarrow\\;\n\\exp\\!\\left(\\frac{20 h}{3}\\right)=(1/4)^{-5/3}(3/4)^{-5}.\n$$\nTaking fifth roots,\n$$\n4^{4/5}\\,\\frac{q_{1}}{q_{2}}=(1/4)^{-1/3}(3/4)^{-1}=4^{1/3}\\cdot\\frac{4}{3}=\\frac{4^{4/3}}{3},\n$$\nhence\n$$\n\\frac{q_{1}}{q_{2}}=\\frac{4^{4/3-4/5}}{3}=\\frac{4^{8/15}}{3}.\n$$\nBut $q_{1}/q_{2}=\\frac{(4-3 p_{2})/20}{(3/4)p_{2}}=\\frac{4-3 p_{2}}{15 p_{2}}$. Therefore\n$$\n\\frac{4-3 p_{2}}{15 p_{2}}=\\frac{4^{8/15}}{3}\n\\;\\Longrightarrow\\;\n4-3 p_{2}=5\\cdot 4^{8/15}\\, p_{2}\n\\;\\Longrightarrow\\;\np_{2}=\\frac{4}{3+5\\cdot 4^{8/15}}.\n$$\nSince $p_{2}\\in(0,1/2)$ for this value and $I(X;Y)$ is concave in $p_{2}$, this stationary point is the unique global maximizer under the given constraints. Thus the optimal probability of using $x_{2}$ is the closed-form value above.", "answer": "$$\\boxed{\\frac{4}{3+5\\cdot 4^{\\frac{8}{15}}}}$$", "id": "1617058"}]}