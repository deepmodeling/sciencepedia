{"hands_on_practices": [{"introduction": "The Shannon Channel Coding Theorem provides a powerful theoretical limit, but how does this translate into practical system design? This first exercise provides a direct application, allowing you to calculate the size of a codebook—the number of distinct messages you can reliably send—based on a known channel capacity $C$ and block length $n$. This is a fundamental step in understanding the tangible consequences of a channel's capacity for designing communication systems. [@problem_id:1657433]", "problem": "A deep-space probe, \"Aether-Scout 7,\" is transmitting scientific data back to Earth through a noisy communication channel influenced by interstellar plasma. After extensive analysis, mission control engineers have determined the Shannon capacity of this channel to be $C = 0.5$ bits per symbol. The probe's communication system is designed to group data into codewords of a fixed block length of $n = 1000$ symbols.\n\nAccording to the noisy-channel coding theorem, it is possible to design a code that allows for communication with an arbitrarily low probability of error, provided the transmission rate does not exceed the channel capacity. Based on this principle, what is the theoretical maximum number of distinct messages (e.g., unique scientific measurements or status reports) that can be encoded into a single block for reliable transmission?\n\nExpress your answer as a power of 2, in the form $2^k$ for some integer $k$.", "solution": "By the noisy-channel coding theorem, reliable communication (arbitrarily low probability of error) is achievable if the transmission rate per symbol $R$ does not exceed the channel capacity $C$. For a block code of length $n$ symbols that encodes $M$ distinct messages, the code rate in bits per symbol is defined by\n$$\nR=\\frac{1}{n}\\log_{2} M.\n$$\nFor reliable transmission, we require $R \\leq C$. The maximum number of messages is achieved when $R=C$, giving\n$$\n\\log_{2} M_{\\max}=nC \\quad \\Rightarrow \\quad M_{\\max}=2^{nC}.\n$$\nWith $C=\\frac{1}{2}$ bits per symbol and $n=1000$, we obtain\n$$\nM_{\\max}=2^{1000 \\cdot \\frac{1}{2}}=2^{500}.\n$$\nThus, the theoretical maximum number of distinct messages encodable per block for reliable transmission is $2^{500}$.", "answer": "$$\\boxed{2^{500}}$$", "id": "1657433"}, {"introduction": "Real-world communication channels are affected by different types of noise, which we model in distinct ways. This practice challenges you to compare two of the most fundamental models: the Binary Symmetric Channel (BSC), where errors are bit flips, and the Binary Erasure Channel (BEC), where data is lost but the loss is detected. By calculating and comparing their capacities, you will gain insight into how the nature of the noise, not just its probability, impacts the ultimate limit of reliable communication. [@problem_id:1657419]", "problem": "An aerospace engineering team is finalizing the design for a communication system on a long-range exploratory probe. The probe will transmit data back to Earth as a stream of binary bits. The team is evaluating two competing prototype technologies, designated System A and System B, and must select the one that supports the highest theoretical rate of error-free communication.\n\nThe communication channels for the two systems have been characterized as follows:\n\n*   **System A** operates as a Binary Symmetric Channel (BSC). In a BSC, each transmitted bit (0 or 1) has a fixed probability $p$ of being flipped to the opposite value upon reception.\n*   **System B** operates as a Binary Erasure Channel (BEC). In a BEC, each transmitted bit is either received correctly, or it is lost and replaced by an 'erasure' symbol. The receiver knows that an erasure has occurred, but does not know the original bit's value. The probability of a bit being erased is $\\epsilon$.\n\nThrough rigorous testing, the team has determined the characteristic parameters for the two systems under operational conditions to be $p = 0.1$ for System A, and $\\epsilon = 0.2$ for System B.\n\nAccording to information theory, the maximum rate at which information can be transmitted with arbitrarily low error probability is known as the channel capacity. Given the parameters, which of the following statements correctly identifies the superior system?\n\nA. System A is superior because its capacity is greater than that of System B.\n\nB. System B is superior because its capacity is greater than that of System A.\n\nC. Both systems are equally capable, as their capacities are identical for the given parameters.\n\nD. The comparison is inconclusive without specifying the probabilities of transmitting 0s and 1s.", "solution": "The channel capacity is defined as the maximum mutual information over all input distributions: for a given discrete memoryless channel, $C=\\max_{P_{X}} I(X;Y)$.\n\nFor a Binary Symmetric Channel with crossover probability $p$, symmetry implies the maximizing input is uniform, and the capacity in bits per channel use is\n$$\nC_{A}=1-H_{2}(p),\n$$\nwhere the binary entropy function is\n$$\nH_{2}(p)=-p \\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nSubstituting $p=0.1$,\n$$\nH_{2}(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nUsing the change of base $\\log_{2}(x)=\\frac{\\ln x}{\\ln 2}$ and numerical evaluation,\n$$\n\\log_{2}(0.1)\\approx -3.321928,\\quad \\log_{2}(0.9)\\approx -0.152003,\n$$\nso\n$$\nH_{2}(0.1)\\approx 0.1\\times 3.321928+0.9\\times 0.152003\\approx 0.332193+0.136803\\approx 0.468996,\n$$\nand therefore\n$$\nC_{A}\\approx 1-0.468996\\approx 0.531004.\n$$\n\nFor a Binary Erasure Channel with erasure probability $\\epsilon$, the capacity in bits per channel use is\n$$\nC_{B}=1-\\epsilon.\n$$\nSubstituting $\\epsilon=0.2$ gives\n$$\nC_{B}=1-0.2=0.8.\n$$\n\nComparison:\n$$\nC_{A}\\approx 0.5310040.8=C_{B}.\n$$\nHence, System B has the greater capacity. Note that option D is incorrect because capacity already accounts for optimization over input distributions; for both BSC and BEC the uniform input achieves capacity, so no additional specification of input probabilities is needed for comparison.", "answer": "$$\\boxed{B}$$", "id": "1657419"}, {"introduction": "What happens when a communication link fails completely? This exercise explores the critical boundary case of zero capacity, a scenario where the channel output becomes statistically independent of its input. By analyzing a channel before and after a catastrophic failure, you will solidify your understanding that capacity is fundamentally a measure of mutual information and see precisely why a capacity of zero means that no information can be transmitted at all. [@problem_id:1657440]", "problem": "A deep-space probe communicates with a ground station on Earth by transmitting binary data. Initially, the communication channel is accurately modeled as a Binary Symmetric Channel (BSC), where each transmitted bit (0 or 1) has a probability $p = 0.11$ of being flipped to the opposite bit upon reception.\n\nDuring a solar storm, the probe's transmitter is permanently damaged. The new behavior of the transmitter is as follows: regardless of the input bit it is instructed to send, it transmits a '0' with a fixed probability of $\\gamma = 0.45$ and a '1' with a probability of $1-\\gamma = 0.55$. The transmission path itself from the probe to Earth remains error-free after this damage.\n\nYour task is to analyze the impact of this damage on the fundamental limit of reliable communication.\n\n1.  Calculate the capacity of the communication channel *before* the solar storm damage occurred.\n2.  Calculate the capacity of the communication channel *after* the solar storm damage occurred.\n\nExpress both capacities in units of bits per channel use. Round your final numerical answers to three significant figures. If a capacity is exactly zero, state it as 0.", "solution": "Channel capacity for a discrete memoryless channel is defined as\n$$\nC=\\max_{P_{X}} I(X;Y),\n$$\nwhere $I(X;Y)=H(Y)-H(Y|X)$ and the maximization is over all input distributions $P_{X}$.\n\n1) Before the solar storm, the channel is a Binary Symmetric Channel (BSC) with crossover probability $p=0.11$. A BSC is a symmetric channel; hence the capacity is achieved by the uniform input distribution $P_{X}(0)=P_{X}(1)=\\frac{1}{2}$. Under this input,\n$$\nH(Y)=1 \\text{ bit}, \\quad H(Y|X)=H_{2}(p),\n$$\nwhere the binary entropy function in bits is\n$$\nH_{2}(p)=-p \\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nTherefore,\n$$\nC_{\\text{before}}=1-H_{2}(p).\n$$\nSubstituting $p=0.11$ and evaluating numerically using $\\log_{2}(x)=\\frac{\\ln x}{\\ln 2}$ gives\n$$\nH_{2}(0.11)=-0.11 \\log_{2}(0.11)-0.89 \\log_{2}(0.89)\n\\approx 0.499904,\n$$\nso\n$$\nC_{\\text{before}}\\approx 1-0.499904=0.500096 \\text{ bits/channel use}.\n$$\nRounded to three significant figures, $C_{\\text{before}}=0.500$ bits/channel use.\n\n2) After the solar storm, the transmitter outputs $Y$ as $P(Y=0|X=x)=\\gamma=0.45$ and $P(Y=1|X=x)=1-\\gamma=0.55$ for both $x\\in\\{0,1\\}$, and the path is error-free. Thus the output distribution does not depend on the input: $P_{Y|X=x}$ is identical for all $x$. Hence $X$ and $Y$ are independent for any input distribution $P_{X}$, which implies\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y)-H(Y)=0\n$$\nfor every $P_{X}$, and therefore\n$$\nC_{\\text{after}}=0 \\text{ bits/channel use}.\n$$\nRounded to three significant figures, this remains $0$.\n\nThus the capacities are $0.500$ (before) and $0$ (after), in bits per channel use.", "answer": "$$\\boxed{\\begin{pmatrix}0.500  0\\end{pmatrix}}$$", "id": "1657440"}]}