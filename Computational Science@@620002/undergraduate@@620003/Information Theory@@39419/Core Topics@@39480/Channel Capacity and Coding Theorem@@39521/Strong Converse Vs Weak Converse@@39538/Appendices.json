{"hands_on_practices": [{"introduction": "The channel coding theorem presents two converses: a weak one, stating that error probability for rates $R > C$ is bounded away from zero, and a strong one, stating it approaches one. This exercise [@problem_id:1660731] challenges you to identify the fundamental information-theoretic argument for the strong converse, moving beyond surface-level explanations. Understanding this distinction is crucial, as it is rooted in the Asymptotic Equipartition Property (AEP) and the geometry of typical sets, which forms the bedrock of Shannon's theory.", "problem": "Consider a communication system built around a binary asymmetric channel known as the Z-channel. The channel has a binary input alphabet $\\mathcal{X} = \\{0, 1\\}$ and a binary output alphabet $\\mathcal{Y} = \\{0, 1\\}$. Its behavior is defined by the following conditional probabilities: an input 0 is always received as a 0, while an input 1 is received as a 0 with probability $p$ and as a 1 with probability $1-p$.\n\nFor this specific problem, let the crossover probability be $p=1/2$. The transition probabilities are therefore:\n- $P(Y=0|X=0) = 1$\n- $P(Y=1|X=0) = 0$\n- $P(Y=0|X=1) = 1/2$\n- $P(Y=1|X=1) = 1/2$\n\nThe information-theoretic capacity of this channel, which is the maximum rate at which information can be transmitted with arbitrarily low error probability, has been calculated to be $C = \\log_2(5/4)$ bits per channel use.\n\nAn engineer attempts to design a coding scheme for this channel using a codebook containing $M$ distinct binary codewords, each of length $n$. The rate of this code is defined as $R = \\frac{\\log_2(M)}{n}$ bits per channel use. The chosen rate is $R = 1/2$. Note that $R > C$.\n\nIt is a fundamental result of information theory that for any code operating at a rate $R > C$, the average probability of a decoding error, $P_e^{(n)}$, does not just stay above some non-zero constant but actually approaches 1 as the block length $n$ goes to infinity ($P_e^{(n)} \\to 1$ as $n \\to \\infty$). This phenomenon is known as the strong converse to the channel coding theorem.\n\nWhich of the following statements provides the most accurate and fundamental explanation for why this must be the case?\n\nA. For any received sequence $y^n$, the likelihood $P(y^n|x^n)$ for the true transmitted codeword $x^n$ becomes vanishingly small as the block length $n$ increases, making it impossible for the decoder to confidently identify the original message.\n\nB. At rates $R>C$, it is impossible to construct a codebook where the minimum Hamming distance between any two codewords is large enough to correct the number of errors typically introduced by the channel for a large block length $n$.\n\nC. According to Fano's inequality, for a message $W$ chosen from a set of size $M$, the error probability is lower bounded by $P_e^{(n)} \\ge \\frac{H(W|Y^n) - 1}{\\log_2 M}$. Since the channel capacity limits information flow, the conditional entropy $H(W|Y^n)$ cannot be reduced below $n(R-C)$. This forces the lower bound to be positive, but suggests that the error probability simply remains bounded above zero, not that it must approach 1.\n\nD. The set of all \"typical\" output sequences for a large block length $n$ has an effective size of approximately $2^{nH(Y)}$, where $H(Y)$ is the entropy of the output distribution that achieves capacity. Each of the $M=2^{nR}$ codewords in the codebook can produce a set of typical outputs of size roughly $2^{nH(Y|X)}$. Because $R > C = H(Y) - H(Y|X)$, the total \"volume\" of possible output sets from all codewords, $M \\times 2^{nH(Y|X)} = 2^{n(R+H(Y|X))}$, becomes exponentially larger than the space of available typical outputs, $2^{nH(Y)}$. This forces a massive overlap between the possible outcomes of different codewords, making it impossible to uniquely determine the input.\n\nE. The given channel with $p=1/2$ is maximally noisy for the input $X=1$. This high level of noise effectively erases the information content for a significant fraction of the input symbols, and no amount of coding with a block length $n$ can recover this lost information if the transmission rate is too ambitious. Therefore, any rate above a minimal threshold will fail.", "solution": "This problem asks for the fundamental information-theoretic reason why the probability of error $P_e^{(n)}$ approaches 1 when attempting to communicate at a rate $R$ greater than the channel capacity $C$. This is the essence of the strong converse theorem. Let's analyze each option.\n\n**A. For any received sequence $y^n$, the likelihood $P(y^n|x^n)$ for the true transmitted codeword $x^n$ becomes vanishingly small as the block length $n$ increases, making it impossible for the decoder to confidently identify the original message.**\nThis statement is true in that the probability of any specific sequence of length $n$ is typically very small for large $n$. For example, for a non-trivial channel, $P(y^n|x^n)$ will scale roughly as $2^{-nH(Y|X)}$. However, this fact alone does not preclude reliable communication. A maximum likelihood decoder, for instance, does not care about the absolute value of $P(y^n|x^n)$, but rather its value relative to $P(y^n|x'_n)$ for all other codewords $x'_n$ in the codebook. If the probability for the correct codeword is significantly larger than for all incorrect ones, decoding can still be successful. Therefore, this is a weak and incomplete explanation for the failure at $R>C$.\n\n**B. At rates $R>C$, it is impossible to construct a codebook where the minimum Hamming distance between any two codewords is large enough to correct the number of errors typically introduced by the channel for a large block length $n$.**\nThis statement connects to coding theory concepts, particularly for channels like the Binary Symmetric Channel where Hamming distance is a natural metric for error correction capability. While it is true that for $R>C$ one cannot construct codes with sufficient distance properties, this is a consequence of the more fundamental limitation, not the reason itself. Shannon's channel coding theorem and its converse are more general; they apply to *any* sequence of codes and decoders, not just those based on a specific algebraic structure or a Hamming distance decoder. It describes a limit on information flow, which is a more fundamental concept than the geometric properties of a specific codebook.\n\n**C. According to Fano's inequality, for a message $W$ chosen from a set of size $M$, the error probability is lower bounded by $P_e^{(n)} \\ge \\frac{H(W|Y^n) - 1}{\\log_2 M}$. Since the channel capacity limits information flow, the conditional entropy $H(W|Y^n)$ cannot be reduced below $n(R-C)$. This forces the lower bound to be positive, but suggests that the error probability simply remains bounded above zero, not that it must approach 1.**\nThis option accurately describes the reasoning behind the *weak converse* theorem. Using Fano's inequality and the data processing inequality ($I(W;Y^n) \\le I(X^n;Y^n) \\le nC$), we can derive a lower bound on the error probability.\n$H(W) = \\log_2 M = nR$.\n$I(W;Y^n) = H(W) - H(W|Y^n) = nR - H(W|Y^n)$.\nSo, $nR - H(W|Y^n) \\le nC \\implies H(W|Y^n) \\ge n(R-C)$.\nPlugging this into Fano's inequality (using $H(P_e^{(n)}) \\le 1$ for the binary entropy of the error event):\n$n(R-C) \\le H(W|Y^n) \\le 1 + P_e^{(n)} \\log_2(M-1) \\approx 1 + P_e^{(n)} nR$.\nThis rearranges to $P_e^{(n)} \\ge \\frac{n(R-C)-1}{nR} = 1 - \\frac{C}{R} - \\frac{1}{nR}$.\nAs $n \\to \\infty$, this shows $P_e^{(n)} \\ge 1 - C/R$. Since $R>C$, this bound is a positive constant. This proves that the error cannot go to zero. However, the question states that the error probability in fact goes to 1. This option correctly states the result of the weak converse but incorrectly uses it to argue against the premise of the strong converse. Thus, it is a misleading distractor.\n\n**D. The set of all \"typical\" output sequences for a large block length $n$ has an effective size of approximately $2^{nH(Y)}$, where $H(Y)$ is the entropy of the output distribution that achieves capacity. Each of the $M=2^{nR}$ codewords in the codebook can produce a set of typical outputs of size roughly $2^{nH(Y|X)}$. Because $R > C = H(Y) - H(Y|X)$, the total \"volume\" of possible output sets from all codewords, $M \\times 2^{nH(Y|X)} = 2^{n(R+H(Y|X))}$, becomes exponentially larger than the space of available typical outputs, $2^{nH(Y)}$. This forces a massive overlap between the possible outcomes of different codewords, making it impossible to uniquely determine the input.**\nThis is the correct explanation. It provides an intuitive argument based on the Asymptotic Equipartition Property (AEP), which is at the heart of the strong converse proof. The logic is as follows:\n1.  The space of all statistically relevant outputs is the set of typical output sequences, $\\mathcal{A}_{\\epsilon}^{(n)}(Y)$, whose size is approximately $2^{nH(Y)}$.\n2.  For any given input codeword $x^n$, the channel transforms it into one of about $2^{nH(Y|X)}$ possible typical output sequences. This forms the decoding region for $x^n$.\n3.  A good code should partition the space of typical outputs into $M=2^{nR}$ disjoint decoding regions.\n4.  The total volume required by these $M$ regions is $M \\times (\\text{size of one region}) \\approx 2^{nR} \\times 2^{nH(Y|X)} = 2^{n(R+H(Y|X))}$.\n5.  For successful decoding, this total volume must fit inside the space of available outputs: $2^{n(R+H(Y|X))} \\lesssim 2^{nH(Y)}$, which implies $n(R+H(Y|X)) \\le nH(Y)$, or $R \\le H(Y) - H(Y|X) = C$.\n6.  When we attempt communication at a rate $R>C$, we have $R+H(Y|X) > H(Y)$. This means the total volume of all decoding regions is exponentially larger than the space they need to fit in. This isn't just a minor overlap; it's a massive, unavoidable collision. A received typical sequence $y^n$ will not just be compatible with the correct codeword, but with an exponentially large number of other incorrect codewords. The decoder cannot distinguish the true message, and thus the probability of error must approach 1.\n\n**E. The given channel with $p=1/2$ is maximally noisy for the input $X=1$. This high level of noise effectively erases the information content for a significant fraction of the input symbols, and no amount of coding with a block length $n$ can recover this lost information if the transmission rate is too ambitious. Therefore, any rate above a minimal threshold will fail.**\nThis statement contains a grain of truth but is ultimately imprecise and misleading. The channel *is* noisy, but it is not useless. The fact that $P(Y=1|X=0)=0$ means that receiving a 1 guarantees the input was a 1. Information is conveyed. The capacity $C = \\log_2(5/4) \\approx 0.322$ bits/symbol is greater than zero, which proves that reliable communication is possible, as long as the rate $R$ is less than $C$. The failure at $R>C$ is not because the channel is \"too noisy\" in an absolute sense, but because the rate $R$ demands more information to be pushed through the channel per use than its physical limit $C$ allows. The argument is quantitative, not qualitative.\n\nTherefore, option D provides the most accurate and fundamental information-theoretic explanation.", "answer": "$$\\boxed{D}$$", "id": "1660731"}, {"introduction": "The mathematical difference between an error probability that is simply non-zero and one that tends to one can seem abstract. This problem [@problem_id:1660714] provides a powerful analogy using a gambling game to make the consequences of the strong converse visceral and intuitive. By calculating the expected financial outcome in hypothetical universes governed by the weak versus the strong converse, you will gain a tangible appreciation for what \"guaranteed failure\" at rates $R > C$ truly implies.", "problem": "A gambler participates in a game based on the success of a digital communication system. The system transmits messages over a Binary Symmetric Channel (BSC), a channel that independently flips each transmitted bit with a fixed probability $p=0.1$. For each transmission, one of $M$ possible messages is chosen, encoded into a binary sequence of length $n$, and sent over the channel. The system uses a coding scheme with a rate $R = 0.6$ bits per channel use. It is known that this rate is greater than the capacity of the channel.\n\nThe gambler's game consists of $N$ independent rounds. In each round, a new message is transmitted using a very large block length $n$. The gambler has an initial wealth $W_0$ and adopts a fixed betting strategy: in each round, they bet a fraction $f=0.1$ of their current wealth on the event that the message is decoded correctly. If the decoding is successful, the gambler wins the bet, receiving a 1-to-1 payout (i.e., they get their bet back plus an equal amount). If the decoding is incorrect, they lose their bet.\n\nTo analyze the gambler's prospects, consider two hypothetical universes governed by different information-theoretic laws for communication at rates $R$ above capacity:\n\n*   **Scenario A (Weak Converse Universe):** In this universe, the probability of decoding error, $P_e^{(n)}$, is only guaranteed to be bounded below by a positive constant as the block length $n$ grows. For the specific rate and channel in this problem, assume this lower bound is approached, such that for the large block length $n$ used in the game, the effective probability of error is $P_{e,A} = 0.25$.\n\n*   **Scenario B (Strong Converse Universe):** In this universe, which corresponds to our physical reality, the strong converse theorem holds. This theorem states that for any rate $R$ above capacity, the probability of error $P_e^{(n)}$ approaches 1 as the block length $n \\to \\infty$. For the large $n$ used, the effective probability of error is $P_{e,B} = 1$.\n\nCalculate the ratio $\\mathcal{R} = \\frac{E[W_{N,A}]}{E[W_{N,B}]}$, where $E[W_{N,A}]$ and $E[W_{N,B}]$ are the expected final wealths of the gambler after $N$ rounds in Scenario A and Scenario B, respectively. Express your answer as a function of $N$.", "solution": "Let $W_k$ be the gambler's wealth after round $k$, with $W_0$ being the initial wealth. The gambler bets a fraction $f$ of their current wealth in each round.\n\nIn round $k$ (for $k=1, 2, ..., N$), the amount bet is $f W_{k-1}$.\nThere are two possible outcomes for round $k$:\n1.  **Success (Correct Decoding):** The gambler wins the bet. Their wealth increases by the amount they bet.\n    $W_k = W_{k-1} + f W_{k-1} = W_{k-1}(1+f)$.\n2.  **Failure (Decoding Error):** The gambler loses the bet. Their wealth decreases by the amount they bet.\n    $W_k = W_{k-1} - f W_{k-1} = W_{k-1}(1-f)$.\n\nLet $P_e$ be the probability of a decoding error in a given round. The probability of success is $1-P_e$. Since each round is independent, $P_e$ is the same for all $N$ rounds within a given scenario.\n\nWe can find the expected wealth after one round, $E[W_1]$, given the initial wealth $W_0$:\n$$E[W_1 | W_0] = W_0(1+f) \\cdot (1-P_e) + W_0(1-f) \\cdot P_e$$\n$$E[W_1] = W_0 [ (1+f - P_e - f P_e) + (P_e - f P_e) ]$$\n$$E[W_1] = W_0 [ 1 + f - 2f P_e ]$$\nThe factor $(1 + f - 2f P_e)$ is the expected growth factor of the wealth in a single round.\n\nSince each round is independent, the expected wealth after $k$ rounds can be found recursively:\n$$E[W_k] = E[E[W_k | W_{k-1}]] = E[W_{k-1}(1+f-2fP_e)] = E[W_{k-1}] (1+f-2fP_e)$$\nBy applying this relation $N$ times, we find the expected wealth after $N$ rounds:\n$$E[W_N] = W_0 (1 + f - 2f P_e)^N$$\n\nNow, we apply this general formula to the two specified scenarios using the given parameter $f=0.1$.\n\n**Scenario A (Weak Converse Universe):**\nThe probability of error is given as $P_{e,A} = 0.25$.\nThe expected final wealth, $E[W_{N,A}]$, is:\n$$E[W_{N,A}] = W_0 (1 + 0.1 - 2(0.1)(0.25))^N$$\n$$E[W_{N,A}] = W_0 (1.1 - 0.2 \\times 0.25)^N$$\n$$E[W_{N,A}] = W_0 (1.1 - 0.05)^N$$\n$$E[W_{N,A}] = W_0 (1.05)^N$$\n\n**Scenario B (Strong Converse Universe):**\nThe probability of error is given as $P_{e,B} = 1$.\nThe expected final wealth, $E[W_{N,B}]$, is:\n$$E[W_{N,B}] = W_0 (1 + 0.1 - 2(0.1)(1))^N$$\n$$E[W_{N,B}] = W_0 (1.1 - 0.2)^N$$\n$$E[W_{N,B}] = W_0 (0.9)^N$$\n\nFinally, we calculate the required ratio $\\mathcal{R} = \\frac{E[W_{N,A}]}{E[W_{N,B}]}$.\n$$\\mathcal{R} = \\frac{W_0 (1.05)^N}{W_0 (0.9)^N} = \\left(\\frac{1.05}{0.9}\\right)^N$$\nTo simplify the base of the exponent:\n$$\\frac{1.05}{0.9} = \\frac{105}{90} = \\frac{21 \\times 5}{18 \\times 5} = \\frac{21}{18} = \\frac{7 \\times 3}{6 \\times 3} = \\frac{7}{6}$$\nTherefore, the ratio is:\n$$\\mathcal{R} = \\left(\\frac{7}{6}\\right)^N$$\nThis result demonstrates the dramatic difference in outcomes. In the weak converse universe, the gambler has a positive expected return and their expected wealth grows exponentially. In the strong converse universe, their expected wealth decays exponentially towards zero, guaranteeing eventual ruin.", "answer": "$$\\boxed{\\left(\\frac{7}{6}\\right)^{N}}$$", "id": "1660714"}, {"introduction": "Real-world communication systems often operate over channels whose quality fluctuates over time, such as a deep-space link affected by solar activity. This practice problem [@problem_id:1660763] places you in the role of an engineer analyzing such a system, where a fixed-rate code may be operating above capacity in one state and below it in another. By applying both the channel coding theorem and its strong converse, you will learn how to predict the overall long-term performance and reliability of a system in a dynamic environment.", "problem": "A deep-space probe communicates with a ground station on Earth using a sophisticated channel coding scheme. The communication channel can be accurately modeled as a Binary Symmetric Channel (BSC), but its quality fluctuates due to solar activity, which changes on a timescale much longer than the transmission time of a single data block.\n\nThe channel randomly exists in one of two states:\n1.  **\"Quiet State\"**: This state occurs with a probability of $\\alpha = 0.75$. During this state, the channel behaves as a BSC with a bit-flip (crossover) probability of $p_1 = 0.02$.\n2.  **\"Active State\"**: This state occurs with a probability of $1-\\alpha = 0.25$. During this state, increased solar interference causes the channel to behave as a BSC with a higher bit-flip probability of $p_2 = 0.2$.\n\nThe probe uses a fixed-rate error-correcting code designed for very long transmission blocks. The code's rate is fixed at $R = 0.4$ bits per channel use. The capacity $C$ of a BSC with crossover probability $p$ is given by $C = 1 - H_2(p)$, where $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ is the binary entropy function.\n\nAssuming the coding scheme is optimal for the given rate and block length, calculate the long-term probability that a transmitted data block is successfully decoded by the ground station. Express your answer as a decimal rounded to three significant figures.", "solution": "The problem asks for the long-term probability of successfully decoding a message transmitted over a channel that switches between two states. The key is to analyze the communication performance in each state separately and then combine them using their respective probabilities of occurrence. We are given that the coding is for \"very long transmission blocks,\" which implies we should consider the asymptotic behavior as the block length $n \\to \\infty$.\n\n**Step 1: Calculate the channel capacity for the \"Quiet State\".**\nIn the Quiet State, the channel is a BSC with a crossover probability $p_1 = 0.02$. The capacity, $C_1$, is given by:\n$$C_1 = 1 - H_2(p_1) = 1 - H_2(0.02)$$\nThe binary entropy function is $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.\n$$H_2(0.02) = -0.02 \\log_2(0.02) - (1-0.02) \\log_2(1-0.02)$$\n$$H_2(0.02) = -0.02 \\log_2(0.02) - 0.98 \\log_2(0.98)$$\nUsing the property $\\log_2(x) = \\ln(x) / \\ln(2)$:\n$$H_2(0.02) \\approx -0.02 \\times (-5.6438) - 0.98 \\times (-0.0290)$$\n$$H_2(0.02) \\approx 0.11288 + 0.02842 = 0.1413$$\nTherefore, the capacity in the Quiet State is:\n$$C_1 \\approx 1 - 0.1413 = 0.8587 \\text{ bits per channel use}$$\n\n**Step 2: Calculate the channel capacity for the \"Active State\".**\nIn the Active State, the channel is a BSC with a crossover probability $p_2 = 0.2$. The capacity, $C_2$, is:\n$$C_2 = 1 - H_2(p_2) = 1 - H_2(0.2)$$\n$$H_2(0.2) = -0.2 \\log_2(0.2) - (1-0.2) \\log_2(0.8)$$\n$$H_2(0.2) = -0.2 \\log_2(0.2) - 0.8 \\log_2(0.8)$$\n$$H_2(0.2) \\approx -0.2 \\times (-2.3219) - 0.8 \\times (-0.3219)$$\n$$H_2(0.2) \\approx 0.46438 + 0.25752 = 0.7219$$\nTherefore, the capacity in the Active State is:\n$$C_2 \\approx 1 - 0.7219 = 0.2781 \\text{ bits per channel use}$$\n\n**Step 3: Analyze decoding success in each state.**\nThe probe uses a fixed-rate code with rate $R = 0.4$ bits per channel use. We compare this rate with the capacities of the two states.\n\n*   **In the Quiet State:** We have $R = 0.4$ and $C_1 \\approx 0.8587$. Since $R  C_1$, Shannon's channel coding theorem applies. The theorem states that for any rate $R  C$, there exists a sequence of codes (with increasing block length $n$) for which the probability of decoding error, $P_e$, approaches zero. As the problem specifies \"very long transmission blocks\" ($n \\to \\infty$) and an \"optimal\" scheme, we can assume that the probability of error is negligible.\n    $$P(\\text{error} | \\text{Quiet State}) \\to 0$$\n    Therefore, the probability of success in this state is:\n    $$P(\\text{success} | \\text{Quiet State}) = 1 - P(\\text{error} | \\text{Quiet State}) \\to 1$$\n\n*   **In the Active State:** We have $R = 0.4$ and $C_2 \\approx 0.2781$. Here, the code rate is greater than the channel capacity, $R > C_2$. In this regime, reliable communication is impossible. The strong converse to the channel coding theorem for discrete memoryless channels states that for any code with rate $R > C$, the probability of decoding error approaches 1 as the block length $n \\to \\infty$.\n    $$P(\\text{error} | \\text{Active State}) \\to 1$$\n    Therefore, the probability of success in this state is:\n    $$P(\\text{success} | \\text{Active State}) = 1 - P(\\text{error} | \\text{Active State}) \\to 0$$\n\n**Step 4: Calculate the overall long-term probability of successful decoding.**\nThe overall probability of success is the weighted average of the success probabilities in each state, where the weights are the probabilities of the channel being in that state.\nThe probability of being in the Quiet State is $P(\\text{Quiet State}) = \\alpha = 0.75$.\nThe probability of being in the Active State is $P(\\text{Active State}) = 1 - \\alpha = 0.25$.\n\nThe total probability of success, $P(\\text{success})$, is given by the law of total probability:\n$$P(\\text{success}) = P(\\text{success} | \\text{Quiet State}) \\times P(\\text{Quiet State}) + P(\\text{success} | \\text{Active State}) \\times P(\\text{Active State})$$\nSubstituting the values we found:\n$$P(\\text{success}) = (1 \\times 0.75) + (0 \\times 0.25)$$\n$$P(\\text{success}) = 0.75 + 0 = 0.75$$\n\nThe long-term probability that a transmitted data block is successfully decoded is 0.75.\n\n**Step 5: Format the final answer.**\nThe problem asks for the answer as a decimal rounded to three significant figures.\nThe calculated probability is 0.75. To express this with three significant figures, we write it as 0.750.", "answer": "$$\\boxed{0.750}$$", "id": "1660763"}]}