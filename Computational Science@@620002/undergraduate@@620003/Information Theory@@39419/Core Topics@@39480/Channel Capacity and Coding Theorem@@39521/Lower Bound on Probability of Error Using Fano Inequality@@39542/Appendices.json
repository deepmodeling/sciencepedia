{"hands_on_practices": [{"introduction": "This first exercise grounds Fano's inequality in a foundational scenario: a binary symmetric channel. By calculating both the theoretical lower bound and the actual error rate of an optimal decoder, you will see a case where this bound is perfectly tight, demonstrating its direct relevance and power in a simple, concrete setting. This practice provides a crucial first step in building an intuition for how lingering uncertainty, quantified by $H(X|Y)$, directly constrains our ability to make correct decisions. [@problem_id:1638528]", "problem": "A simple digital memory system stores a single bit, $X$, which is written as a 0 or a 1 with equal probability. Due to thermal noise, the bit can flip over time. This process is modeled as a Binary Symmetric Channel (BSC). After a certain period, the bit is read as a value $Y$. The probability of a bit flip (i.e., reading a 0 when a 1 was stored, or reading a 1 when a 0 was stored) is the crossover probability, $\\epsilon = 0.1$.\n\nAn engineer wants to understand the limits of reliability for this memory system. You are asked to perform two calculations:\n\n1.  First, determine the minimum probability of error, $P_e$, that can be achieved. This is accomplished by using an optimal decision rule to determine an estimate, $\\hat{X}$, of the original bit based on the observed bit, $Y$.\n2.  Second, calculate the theoretical lower bound on this probability of error, denoted as $P_{\\text{bound}}$. This fundamental limit is derived from Fano's inequality. For a binary source like this one, the inequality establishes a relationship between the conditional entropy of the input given the output, $H(X|Y)$, and the binary entropy of the error probability, $H_b(P_e)$. The binary entropy function is given by $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.\n\nCalculate the numerical values for both $P_e$ and $P_{\\text{bound}}$. Present your final answer as a pair of numbers $(P_e, P_{\\text{bound}})$ in a single row matrix, with each value rounded to three significant figures.", "solution": "The problem asks for two quantities: the actual minimum probability of error $P_e$ for an optimal decoder, and the theoretical lower bound $P_{\\text{bound}}$ from Fano's inequality. The system is a Binary Symmetric Channel (BSC) with input $X \\in \\{0, 1\\}$, output $Y \\in \\{0, 1\\}$, uniform input distribution $P(X=0) = P(X=1) = 0.5$, and crossover probability $\\epsilon = 0.1$.\n\n**Part 1: Calculation of the minimum probability of error, $P_e$.**\n\nThe optimal decision rule that minimizes the probability of error $P(\\hat{X} \\neq X)$ is the Maximum A Posteriori (MAP) decoder. The MAP rule chooses the input $\\hat{x}$ that maximizes the posterior probability $P(X=x|Y=y)$ for a given observation $y$.\n$$ \\hat{x} = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nUsing Bayes' theorem, the posterior is $P(X=x|Y=y) = \\frac{P(Y=y|X=x) P(X=x)}{P(Y=y)}$.\nSince the input distribution $P(X=x)$ is uniform ($0.5$ for all $x$), and $P(Y=y)$ is a constant for a given $y$, maximizing the posterior probability is equivalent to maximizing the likelihood $P(Y=y|X=x)$. This decision rule is called the Maximum Likelihood (ML) decoder.\n\nThe channel transition probabilities are:\n$P(Y=0|X=0) = 1-\\epsilon = 0.9$\n$P(Y=1|X=0) = \\epsilon = 0.1$\n$P(Y=0|X=1) = \\epsilon = 0.1$\n$P(Y=1|X=1) = 1-\\epsilon = 0.9$\n\nLet's determine the decision rule:\n- If we observe $Y=0$: We compare $P(Y=0|X=0) = 0.9$ and $P(Y=0|X=1) = 0.1$. Since $0.9 > 0.1$, the optimal decision is $\\hat{X}=0$.\n- If we observe $Y=1$: We compare $P(Y=1|X=0) = 0.1$ and $P(Y=1|X=1) = 0.9$. Since $0.9 > 0.1$, the optimal decision is $\\hat{X}=1$.\n\nThe optimal decision rule is simply $\\hat{X} = Y$. An error occurs if $\\hat{X} \\neq X$, which for this rule means an error occurs if $Y \\neq X$. This happens when the bit flips.\n\nThe probability of error, $P_e$, is the total probability of a bit flip:\n$$ P_e = P(Y \\neq X) = P(Y \\neq X | X=0)P(X=0) + P(Y \\neq X | X=1)P(X=1) $$\n$$ P_e = P(Y=1|X=0)P(X=0) + P(Y=0|X=1)P(X=1) $$\nSubstituting the given values:\n$$ P_e = (\\epsilon)(0.5) + (\\epsilon)(0.5) = \\epsilon $$\n$$ P_e = 0.1 $$\n\n**Part 2: Calculation of the Fano lower bound, $P_{\\text{bound}}$.**\n\nFano's inequality for a binary source provides a lower bound on the probability of error, $P_e$, via the relation:\n$$ H(X|Y) \\le H_b(P_e) $$\nThis can be rearranged to give $H_b(P_e) \\ge H(X|Y)$. We need to calculate the conditional entropy $H(X|Y)$. A convenient formula is $H(X|Y) = H(X) - I(X;Y)$, where $I(X;Y)$ is the mutual information.\n\nFirst, let's calculate the entropy of the source, $H(X)$. Since the input is uniform:\n$$ H(X) = -\\sum_{x \\in \\{0,1\\}} P(X=x) \\log_2(P(X=x)) = - (0.5 \\log_2(0.5) + 0.5 \\log_2(0.5)) = - \\log_2(0.5) = 1 \\text{ bit} $$\n\nNext, we calculate the mutual information $I(X;Y) = H(Y) - H(Y|X)$.\nLet's find the conditional entropy $H(Y|X)$:\n$$ H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) H(Y|X=x) $$\nFor a BSC, the entropy of the output conditioned on a specific input is the binary entropy of the crossover probability:\n$H(Y|X=0) = H(Y|X=1) = H_b(\\epsilon)$.\nTherefore, $H(Y|X) = (0.5) H_b(\\epsilon) + (0.5) H_b(\\epsilon) = H_b(\\epsilon)$.\n\nNow we find the output probabilty distribution $P(Y)$ to calculate $H(Y)$.\n$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-\\epsilon)(0.5) + (\\epsilon)(0.5) = 0.5$.\n$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = (\\epsilon)(0.5) + (1-\\epsilon)(0.5) = 0.5$.\nThe output distribution is also uniform, so its entropy is $H(Y)=1$ bit.\n\nThe mutual information is $I(X;Y) = H(Y) - H(Y|X) = 1 - H_b(\\epsilon)$.\nNow we find the conditional entropy required for Fano's inequality:\n$$ H(X|Y) = H(X) - I(X;Y) = 1 - (1 - H_b(\\epsilon)) = H_b(\\epsilon) $$\n\nSubstituting this into the Fano inequality relation $H_b(P_e) \\ge H(X|Y)$:\n$$ H_b(P_e) \\ge H_b(\\epsilon) $$\nThe binary entropy function $H_b(p)$ is symmetric around $p=0.5$ and is strictly increasing for $p \\in [0, 0.5]$. Since the error probability $\\epsilon=0.1$ is in this range, and we expect any reasonable decoder's error probability $P_e$ to also be in this range, we can conclude from the inequality that:\n$$ P_e \\ge \\epsilon $$\nThis means the minimum possible value for $P_e$ is $\\epsilon$. Thus, the lower bound $P_{\\text{bound}}$ is $\\epsilon$.\n$$ P_{\\text{bound}} = \\epsilon = 0.1 $$\n\n**Conclusion and Formatting**\n\nWe have calculated both the actual minimum error probability for this channel and the Fano lower bound.\n$P_e = 0.1$\n$P_{\\text{bound}} = 0.1$\n\nRounding to three significant figures gives $0.100$ for both values. The final answer is the pair $(0.100, 0.100)$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.100  0.100 \\end{pmatrix}}$$", "id": "1638528"}, {"introduction": "Real-world systems use error-correcting codes to improve reliability, but how do we know if our design is any good? This problem explores the interplay between a simple $(3,1)$ repetition code and Fano's fundamental limit. You'll compare the performance of a practical \"majority rule\" decoder against the ultimate theoretical boundary, offering a clear view of the gap between a specific implementation and the best possible performance allowed by the laws of information theory. [@problem_id:1638462]", "problem": "Consider a simple communication system designed to transmit a binary message. The source is a random variable $X$ with alphabet $\\mathcal{X}=\\{0, 1\\}$ and a uniform probability distribution, i.e., $P(X=0) = P(X=1) = 1/2$. To protect against errors, a $(3,1)$ repetition code is used, where the message $X=0$ is encoded into the codeword $C=000$ and $X=1$ is encoded into $C=111$.\n\nThe encoded codeword is transmitted over a Binary Symmetric Channel (BSC) with a crossover probability $p=0.1$. This means each transmitted bit is flipped (from 0 to 1, or 1 to 0) with probability $p$, and transmitted correctly with probability $1-p$, independently of all other bits. The received 3-bit word is denoted by $Y$.\n\nAt the receiver, a majority rule decoder is employed. This decoder estimates the original message as $\\hat{X}=1$ if the received word $Y$ contains two or more 1s; otherwise, it estimates $\\hat{X}=0$.\n\nYour task is to compare the actual performance of this specific decoder to the theoretical limit imposed by Fano's inequality. You will calculate the ratio $R = P_{e,maj} / P_{e,Fano}$, where $P_{e,maj}$ is the probability of error for the majority decoder, and $P_{e,Fano}$ is the information-theoretic lower bound on the probability of any decoder error, as determined by Fano's inequality.\n\nFano's inequality for a binary source implies that for any decoder, the probability of error $P_e$ is bounded by the relation $H(P_e) \\ge H(X|Y)$, where $H(p) = -p\\log_2(p) - (1-p)\\log_2(1-p)$ is the binary entropy function. The lower bound, $P_{e,Fano}$, is the value that satisfies this relationship with equality, i.e., $H(P_{e,Fano}) = H(X|Y)$.\n\nTo find $P_{e,Fano}$ from the value you calculate for $H(X|Y)$, use linear interpolation with the following table of the binary entropy function:\n| $p$      | $H(p)$   |\n|----------|----------|\n| $0.0190$ | $0.1353$ |\n| $0.0200$ | $0.1414$ |\n\nCalculate the ratio $P_{e,maj} / P_{e,Fano}$ and round your final answer to three significant figures.", "solution": "The source is binary uniform, $P(X=0)=P(X=1)=\\frac{1}{2}$. A $(3,1)$ repetition code maps $0 \\mapsto 000$ and $1 \\mapsto 111$. The channel is a BSC with crossover probability $p=0.1$, and each bit flips independently. The majority decoder decides $\\hat{X}=1$ if at least two received bits are $1$, and $\\hat{X}=0$ otherwise.\n\nFirst, compute the majority-decoder error probability. Conditioned on a given $X$, an error occurs if at least two of the three transmitted bits flip. For a BSC with flip probability $p$, this gives\n$$\nP_{e,\\mathrm{maj}}=P(\\mathrm{Bin}(3,p)\\ge 2)=\\binom{3}{2}p^{2}(1-p)+p^{3}=3p^{2}(1-p)+p^{3}.\n$$\nWith $p=0.1$, this yields\n$$\nP_{e,\\mathrm{maj}}=3\\cdot 0.1^{2}\\cdot 0.9+0.1^{3}=0.027+0.001=0.028.\n$$\n\nNext, compute $H(X|Y)$, where $Y$ is the received 3-bit word. For a received word $y$ of Hamming weight $w$, by Bayes’ rule with the uniform prior and channel symmetry,\n$$\nP(X=0\\mid Y=y)=\\frac{p^{w}(1-p)^{3-w}}{p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}},\\quad\nP(X=1\\mid Y=y)=\\frac{p^{3-w}(1-p)^{w}}{p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}}.\n$$\nThis depends only on $w=|y|$, and there are $\\binom{3}{w}$ words of weight $w$. The marginal of a specific $y$ of weight $w$ is\n$$\nP(Y=y)=\\frac{1}{2}\\left[p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}\\right].\n$$\nTherefore,\n$$\nH(X|Y)=\\sum_{w=0}^{3}\\binom{3}{w}\\left[\\frac{1}{2}\\left(p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}\\right)\\right]H\\!\\left(P(X=1\\mid |Y|=w)\\right),\n$$\nwith $H(\\cdot)$ the binary entropy function $H(u)=-u\\log_{2}u-(1-u)\\log_{2}(1-u)$.\n\nFor $p=0.1$, the posteriors simplify:\n- $w=0$: $P(X=1\\mid |Y|=0)=\\dfrac{p^{3}}{(1-p)^{3}+p^{3}}=\\dfrac{0.001}{0.730}=\\dfrac{1}{730}$, and $P(Y=000)=\\dfrac{1}{2}\\left[(1-p)^{3}+p^{3}\\right]=0.365$.\n- $w=3$: $P(X=1\\mid |Y|=3)=\\dfrac{(1-p)^{3}}{(1-p)^{3}+p^{3}}=1-\\dfrac{1}{730}$, and $P(Y=111)=0.365$.\n- $w=1$: $P(X=1\\mid |Y|=1)=p=0.1$, with $3$ such words each of probability $\\dfrac{1}{2}\\left[p(1-p)^{2}+p^{2}(1-p)\\right]=0.045$, totaling $0.135$.\n- $w=2$: $P(X=1\\mid |Y|=2)=1-p=0.9$, with total probability also $0.135$.\n\nBy symmetry $H\\!\\left(\\dfrac{1}{730}\\right)=H\\!\\left(1-\\dfrac{1}{730}\\right)$ and $H(0.1)=H(0.9)$, so\n$$\nH(X|Y)=0.73\\,H\\!\\left(\\frac{1}{730}\\right)+0.27\\,H(0.1).\n$$\nNumerically, $H(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\approx 0.4689955936$, and\n$$\nH\\!\\left(\\frac{1}{730}\\right)=-\\frac{1}{730}\\log_{2}\\!\\left(\\frac{1}{730}\\right)-\\frac{729}{730}\\log_{2}\\!\\left(\\frac{729}{730}\\right)\\approx 0.015002763.\n$$\nThus\n$$\nH(X|Y)\\approx 0.73\\cdot 0.015002763+0.27\\cdot 0.4689955936\\approx 0.137580827.\n$$\n\nFano’s inequality for a binary source states $H(P_{e})\\ge H(X|Y)$. The lower bound $P_{e,\\mathrm{Fano}}$ is defined by equality $H(P_{e,\\mathrm{Fano}})=H(X|Y)$. Using the provided table and linear interpolation between\n$$\n(p,H(p))=(0.0190,\\,0.1353)\\quad\\text{and}\\quad(0.0200,\\,0.1414),\n$$\nwe set\n$$\nP_{e,\\mathrm{Fano}}=0.0190+\\frac{H(X|Y)-0.1353}{0.1414-0.1353}\\times(0.0200-0.0190)\n=0.0190+\\frac{0.137580827-0.1353}{0.0061}\\times 0.001,\n$$\nwhich yields\n$$\nP_{e,\\mathrm{Fano}}\\approx 0.019373905.\n$$\n\nFinally, the requested ratio is\n$$\nR=\\frac{P_{e,\\mathrm{maj}}}{P_{e,\\mathrm{Fano}}}=\\frac{0.028}{0.019373905}\\approx 1.44524,\n$$\nwhich, rounded to three significant figures, is $1.45$.", "answer": "$$\\boxed{1.45}$$", "id": "1638462"}, {"introduction": "The principles of information theory are not confined to binary systems. This exercise challenges you to generalize your understanding by analyzing a ternary symmetric channel, where the alphabet size is three. You will apply the more general form of Fano's inequality, which accounts for non-binary alphabets, to derive a lower bound on the probability of error, reinforcing the universal nature of this powerful concept. [@problem_id:1638469]", "problem": "Consider a discrete memoryless source $X$ that produces one of three possible symbols from the alphabet $\\mathcal{X} = \\{0, 1, 2\\}$. The symbols are generated independently and are uniformly distributed.\n\nThis source symbol is transmitted over a Ternary Symmetric Channel (TSC). The channel has the same input and output alphabet, $\\mathcal{X}$. The TSC is characterized by a single parameter $\\epsilon$, which is the total probability of a symbol error, where $0 \\le \\epsilon  1$. Specifically, the probability that a transmitted symbol is received correctly is $1-\\epsilon$. If an error occurs, the symbol is received as one of the other two symbols with equal probability.\n\nA decoder observes the channel output $Y$ and produces an estimate $\\hat{X}$ of the original symbol $X$. The performance of this communication system is measured by the average probability of error, $P_e = P(\\hat{X} \\neq X)$.\n\nFind an explicit lower bound on the probability of error $P_e$ as a function of the channel parameter $\\epsilon$. Your answer should be expressed in terms of $\\epsilon$ and the base-2 logarithm function.", "solution": "Let $X \\in \\{0,1,2\\}$ be uniform, so $H(X)=\\log_{2} 3$. The ternary symmetric channel has transition probabilities\n$$\nP_{Y|X}(y|x)=\n\\begin{cases}\n1-\\epsilon,  y=x,\\\\\n\\frac{\\epsilon}{2},  y\\neq x.\n\\end{cases}\n$$\nWith a uniform input and channel symmetry, $Y$ is also uniform, so $H(Y)=\\log_{2} 3$. The conditional entropy per input symbol equals the entropy of a row of the transition matrix:\n$$\nH(Y|X)=-(1-\\epsilon)\\log_{2}(1-\\epsilon)-\\frac{\\epsilon}{2}\\log_{2}\\left(\\frac{\\epsilon}{2}\\right)-\\frac{\\epsilon}{2}\\log_{2}\\left(\\frac{\\epsilon}{2}\\right)\n=-(1-\\epsilon)\\log_{2}(1-\\epsilon)-\\epsilon\\log_{2}\\left(\\frac{\\epsilon}{2}\\right).\n$$\nEquivalently,\n$$\nH(Y|X)=-(1-\\epsilon)\\log_{2}(1-\\epsilon)-\\epsilon\\log_{2}\\epsilon+\\epsilon.\n$$\nSince $H(X)=H(Y)=\\log_{2} 3$, it follows that\n$$\nH(X|Y)=H(X)+H(Y|X)-H(Y)=H(Y|X).\n$$\nFor any decoder $\\hat{X}=g(Y)$ with error probability $P_{e}=P(\\hat{X}\\neq X)$, Fano’s inequality gives for an alphabet of size $3$:\n$$\nH(X|Y)\\leq h_{2}(P_{e})+P_{e}\\log_{2}(3-1)=h_{2}(P_{e})+P_{e},\n$$\nwhere $h_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$. Hence\n$$\nh_{2}(P_{e})+P_{e}\\geq H(X|Y)=-(1-\\epsilon)\\log_{2}(1-\\epsilon)-\\epsilon\\log_{2}\\epsilon+\\epsilon.\n$$\nUsing $h_{2}(P_{e})\\leq 1$ for all $P_{e}\\in[0,1]$ yields the explicit lower bound\n$$\nP_{e}\\geq H(X|Y)-1=-(1-\\epsilon)\\log_{2}(1-\\epsilon)-\\epsilon\\log_{2}\\epsilon+\\epsilon-1.\n$$\nThis bound depends only on $\\epsilon$ and the base-2 logarithm function. (Since $P_{e}\\geq 0$, one may take the nonnegative part if desired, but the expression above is a valid explicit lower bound for all $\\epsilon$.)", "answer": "$$\\boxed{-\\,(1-\\epsilon)\\log_{2}(1-\\epsilon)\\;-\\;\\epsilon\\log_{2}\\epsilon\\;+\\;\\epsilon\\;-\\;1}$$", "id": "1638469"}]}