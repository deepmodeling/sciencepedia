{"hands_on_practices": [{"introduction": "We begin our exploration of the Z-channel's capacity by considering an ideal scenario: a channel with zero noise. This exercise [@problem_id:1669132] asks you to calculate the capacity when the crossover probability $p$ is exactly $0$. By analyzing this perfect communication link, you will connect the channel's deterministic behavior to its maximum theoretical information rate, establishing a fundamental benchmark for performance.", "problem": "A specific type of binary communication channel, known as a Z-channel, is characterized by the following properties. It has a binary input alphabet $X = \\{0, 1\\}$ and a binary output alphabet $Y = \\{0, 1\\}$. The conditional probabilities of receiving an output symbol given a sent input symbol are defined as follows:\n- The probability of receiving a '0' given a '0' was sent is 1.\n- The probability of receiving a '1' given a '1' was sent is $1-p$.\n- The probability of receiving a '0' given a '1' was sent is $p$.\n- The probability of receiving a '1' given a '0' was sent is 0.\n\nConsider a particular realization of this channel where the crossover probability is $p = 0$. Calculate the capacity of this channel. Express your answer in bits per symbol.", "solution": "The capacity of a discrete memoryless channel is defined as\n$$\nC=\\max_{P_{X}} I(X;Y),\n$$\nwhere $I(X;Y)=H(Y)-H(Y|X)$ with logarithms in base $2$ so that entropy is measured in bits.\n\nFor the given Z-channel with crossover probability $p=0$, the transition probabilities are $P(Y=0|X=0)=1$ and $P(Y=1|X=1)=1$, with $P(Y=1|X=0)=0$ and $P(Y=0|X=1)=0$. Hence the channel is deterministic with $Y=X$. Therefore,\n$$\nH(Y|X)=0,\n$$\nand since $Y$ is a deterministic bijective function of $X$, the distribution of $Y$ equals that of $X$, so $H(Y)=H(X)$.\n\nLet $P(X=1)=q$ and $P(X=0)=1-q$. Then\n$$\nI(X;Y)=H(Y)=H(X)=-q \\log_{2}(q)-(1-q)\\log_{2}(1-q)\\equiv H_{2}(q).\n$$\nTo maximize $H_{2}(q)$ over $q\\in[0,1]$, compute the derivative:\n$$\n\\frac{d}{dq}H_{2}(q)=-\\log_{2}(q)-\\frac{1}{\\ln 2}+\\log_{2}(1-q)+\\frac{1}{\\ln 2}=\\log_{2}\\!\\left(\\frac{1-q}{q}\\right).\n$$\nSetting this to zero gives\n$$\n\\log_{2}\\!\\left(\\frac{1-q}{q}\\right)=0 \\quad \\Rightarrow \\quad \\frac{1-q}{q}=1 \\quad \\Rightarrow \\quad q=\\frac{1}{2}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}}{dq^{2}}H_{2}(q)=\\frac{1}{\\ln 2}\\left(-\\frac{1}{1-q}-\\frac{1}{q}\\right)<0 \\quad \\text{for } q\\in(0,1),\n$$\nso $q=\\frac{1}{2}$ is the maximizer. Evaluating,\n$$\nH_{2}\\!\\left(\\frac{1}{2}\\right)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\nTherefore,\n$$\nC=\\max_{P_{X}} I(X;Y)=1 \\text{ bit per symbol}.\n$$", "answer": "$$\\boxed{1}$$", "id": "1669132"}, {"introduction": "Having established the capacity of a perfect Z-channel, we now turn to the opposite extreme to deepen our understanding. This practice [@problem_id:1669158] investigates a channel where the crossover probability $p$ is $1$, meaning an input '1' is always received as a '0'. Analyzing this scenario reveals how noise can completely sever the link between input and output, reducing the channel's capacity to zero and rendering it useless for communication.", "problem": "In information theory, a common model for a noisy communication channel is the Binary Asymmetric Channel (BAC), where the probability of a bit flip depends on whether the input bit is a 0 or a 1. A special case of this is the Z-channel, which is characterized by the following conditional transition probabilities for an input $X \\in \\{0, 1\\}$ and output $Y \\in \\{0, 1\\}$:\n\n-   $P(Y=0|X=0) = 1$\n-   $P(Y=1|X=0) = 0$\n-   $P(Y=0|X=1) = p$\n-   $P(Y=1|X=1) = 1-p$\n\nHere, $p$ is the crossover probability, representing the chance that an input '1' is incorrectly received as a '0'. An input '0' is always transmitted correctly. The channel capacity, $C$, represents the highest rate in bits per channel use at which information can be reliably transmitted over this channel. This capacity is found by maximizing the mutual information between the input and output over all possible input probability distributions. The entropy of a discrete random variable $Z$ with probability mass function $P(z)$ is given by $H(Z) = -\\sum_z P(z) \\log_2 P(z)$, measured in bits.\n\nConsider how the behavior of the Z-channel changes with the crossover probability $p$. Which of the following statements correctly describes the channel capacity, $C(p)$, in the limit as the crossover probability $p$ approaches 1?\n\nA. The capacity approaches 1 bit.\n\nB. The capacity approaches $\\frac{1}{2}$ bits.\n\nC. The capacity approaches 0 bits.\n\nD. The capacity approaches $\\log_2(5) - 2$ bits.\n\nE. The capacity is undefined in this limit.", "solution": "Let $X \\in \\{0,1\\}$ with $P(X=1)=q$ and $P(X=0)=1-q$. For the Z-channel with crossover probability $p$, the output $Y$ satisfies\n$$\nP(Y=1)=P(Y=1|X=1)P(X=1)= (1-p)q, \\quad P(Y=0)=1-(1-p)q.\n$$\nThus $Y$ is Bernoulli with parameter $(1-p)q$, so\n$$\nH(Y)=H_{b}((1-p)q),\n$$\nwhere $H_{b}(x)=-x \\log_{2} x-(1-x)\\log_{2}(1-x)$ is the binary entropy function.\n\nThe conditional entropy is\n$$\nH(Y|X)=P(X=0)H(Y|X=0)+P(X=1)H(Y|X=1)=(1-q)\\cdot 0+q\\,H_{b}(p)=q\\,H_{b}(p),\n$$\nsince $Y$ is deterministic given $X=0$ and binary with parameter $p$ given $X=1$.\n\nTherefore the mutual information for input bias $q$ is\n$$\nI(X;Y)=H(Y)-H(Y|X)=H_{b}((1-p)q)-q\\,H_{b}(p),\n$$\nand the channel capacity is\n$$\nC(p)=\\max_{0 \\leq q \\leq 1}\\left[H_{b}((1-p)q)-q\\,H_{b}(p)\\right].\n$$\n\nWe analyze the limit $p \\to 1$. For $p$ sufficiently close to $1$, we have $1-p \\in (0,\\tfrac{1}{2}]$, and for any $q \\in [0,1]$ we also have $(1-p)q \\in [0,\\tfrac{1}{2}]$. The binary entropy function $H_{b}(x)$ is increasing on $[0,\\tfrac{1}{2}]$, hence\n$$\nH_{b}((1-p)q) \\leq H_{b}(1-p).\n$$\nIt follows that for every $q \\in [0,1]$,\n$$\nI(X;Y)=H_{b}((1-p)q)-q\\,H_{b}(p) \\leq H_{b}(1-p).\n$$\nTaking the maximum over $q$ gives\n$$\n0 \\leq C(p) \\leq H_{b}(1-p).\n$$\nSince $H_{b}(x) \\to 0$ as $x \\to 0$, we have $H_{b}(1-p) \\to 0$ as $p \\to 1$. By the squeeze principle,\n$$\n\\lim_{p \\to 1} C(p)=0.\n$$\n\nEquivalently, at $p=1$ the channel output is $Y=0$ with probability $1$ regardless of $X$, so the channel is completely noisy and $I(X;Y)=0$ for all input distributions, confirming that the capacity is $0$ in the limit.\n\nTherefore, the correct choice is that the capacity approaches $0$ bits.", "answer": "$$\\boxed{C}$$", "id": "1669158"}, {"introduction": "The extreme cases of $p=0$ and $p=1$ provide valuable intuition, but finding the capacity for an arbitrary crossover probability $p$ is more complex. This problem [@problem_id:1669124] introduces a powerful numerical tool, the Arimoto-Blahut algorithm, which is used to find the optimal input distribution that achieves capacity. By performing a single iteration of this algorithm, you'll gain hands-on experience with the computational techniques used to solve practical problems in information theory.", "problem": "A simple optical communication link is designed to transmit binary data. An input bit $X=0$ is encoded as 'no light pulse', and an input bit $X=1$ is encoded as a 'light pulse'. The receiver's detector for 'no light pulse' is perfect, so an input $X=0$ is always correctly identified as an output $Y=0$. However, the light pulse detector is imperfect. When a light pulse is sent (input $X=1$), it is sometimes missed and incorrectly registered as 'no light pulse' (output $Y=0$) with a crossover probability $p$, where $0 < p < 1$. This communication system is a model for a Z-channel.\n\nTo optimize the channel for maximum information throughput, we can use the Arimoto-Blahut algorithm, which iteratively refines an input probability distribution $P(X)$ to approach the one that achieves channel capacity. The update rule for an iteration from step $k$ to $k+1$ is given by:\n$$\nP_{k+1}(x) = \\frac{P_k(x) \\exp\\left( \\sum_{y \\in \\mathcal{Y}} P(y|x) \\ln \\frac{P(y|x)}{P_k(y)} \\right)}{Z_{k+1}}\n$$\nwhere $P_k(y) = \\sum_{x' \\in \\mathcal{X}} P_k(x') P(y|x')$ is the output distribution at step $k$, and $Z_{k+1}$ is a normalization constant. The alphabets are $\\mathcal{X} = \\{0, 1\\}$ and $\\mathcal{Y} = \\{0, 1\\}$.\n\nAssume the initial guess for the input distribution is uniform, given by $P_0(X=0) = 1/2$ and $P_0(X=1) = 1/2$. Perform a single iteration of the Arimoto-Blahut algorithm to find the updated input distribution, $P_1(X)$.\n\nExpress your answer as a row vector $(P_1(X=0), P_1(X=1))$ in terms of the crossover probability $p$.", "solution": "The problem asks for the updated input distribution $P_1(X)$ after one iteration of the Arimoto-Blahut algorithm, starting from a uniform distribution $P_0(X)$, for a Z-channel with crossover probability $p$.\n\nFirst, let's establish the channel transition probability matrix $P(y|x)$.\nFor the Z-channel described:\n- If $X=0$, then $Y=0$ with certainty. So, $P(Y=0|X=0) = 1$ and $P(Y=1|X=0) = 0$.\n- If $X=1$, it is mistaken for $Y=0$ with probability $p$. So, $P(Y=0|X=1) = p$ and $P(Y=1|X=1) = 1-p$.\n\nThe transition matrix is:\n$$\nP(Y|X) = \\begin{pmatrix} P(Y=0|X=0) & P(Y=1|X=0) \\\\ P(Y=0|X=1) & P(Y=1|X=1) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ p & 1-p \\end{pmatrix}\n$$\n\nThe initial input distribution is uniform: $P_0(X=0) = 1/2$ and $P_0(X=1) = 1/2$.\n\nThe Arimoto-Blahut iteration consists of several steps:\n\n**Step 1: Calculate the initial output distribution $P_0(y)$.**\nThe output distribution $P_0(y)$ is calculated using the law of total probability: $P_0(y) = \\sum_{x \\in \\mathcal{X}} P_0(x) P(y|x)$.\nFor $y=0$:\n$$\nP_0(Y=0) = P_0(X=0)P(Y=0|X=0) + P_0(X=1)P(Y=0|X=1) = \\left(\\frac{1}{2}\\right)(1) + \\left(\\frac{1}{2}\\right)(p) = \\frac{1+p}{2}\n$$\nFor $y=1$:\n$$\nP_0(Y=1) = P_0(X=0)P(Y=1|X=0) + P_0(X=1)P(Y=1|X=1) = \\left(\\frac{1}{2}\\right)(0) + \\left(\\frac{1}{2}\\right)(1-p) = \\frac{1-p}{2}\n$$\nAs a check, $P_0(Y=0) + P_0(Y=1) = \\frac{1+p}{2} + \\frac{1-p}{2} = \\frac{2}{2} = 1$.\n\n**Step 2: Calculate the un-normalized update terms.**\nThe update rule is $P_{1}(x) \\propto P_0(x) c_x$, where $c_x = \\exp\\left( \\sum_{y} P(y|x) \\ln \\frac{P(y|x)}{P_0(y)} \\right)$. We need to calculate $c_0$ for $x=0$ and $c_1$ for $x=1$.\n\nFor $x=0$:\nThe term in the exponent is the Kullback-Leibler divergence $D(P(Y|X=0) \\| P_0(Y))$.\n$$\n\\ln(c_0) = \\sum_{y \\in \\{0,1\\}} P(y|0) \\ln \\frac{P(y|0)}{P_0(y)} = P(Y=0|X=0) \\ln \\frac{P(Y=0|X=0)}{P_0(Y=0)} + P(Y=1|X=0) \\ln \\frac{P(Y=1|X=0)}{P_0(Y=1)}\n$$\nUsing the convention that $0 \\ln 0 = 0$:\n$$\n\\ln(c_0) = (1) \\ln\\left(\\frac{1}{(1+p)/2}\\right) + (0) \\ln\\left(\\frac{0}{(1-p)/2}\\right) = \\ln\\left(\\frac{2}{1+p}\\right)\n$$\nThus, $c_0 = \\exp\\left(\\ln\\left(\\frac{2}{1+p}\\right)\\right) = \\frac{2}{1+p}$.\n\nFor $x=1$:\nThe term in the exponent is $D(P(Y|X=1) \\| P_0(Y))$.\n$$\n\\ln(c_1) = \\sum_{y \\in \\{0,1\\}} P(y|1) \\ln \\frac{P(y|1)}{P_0(y)} = P(Y=0|X=1) \\ln \\frac{P(Y=0|X=1)}{P_0(Y=0)} + P(Y=1|X=1) \\ln \\frac{P(Y=1|X=1)}{P_0(Y=1)}\n$$\n$$\n\\ln(c_1) = p \\ln\\left(\\frac{p}{(1+p)/2}\\right) + (1-p) \\ln\\left(\\frac{1-p}{(1-p)/2}\\right) = p \\ln\\left(\\frac{2p}{1+p}\\right) + (1-p) \\ln(2)\n$$\nUsing logarithm properties:\n$$\n\\ln(c_1) = p(\\ln(2p) - \\ln(1+p)) + (1-p)\\ln(2) = p(\\ln(2) + \\ln(p) - \\ln(1+p)) + \\ln(2) - p\\ln(2)\n$$\n$$\n\\ln(c_1) = p\\ln(2) + p\\ln(p) - p\\ln(1+p) + \\ln(2) - p\\ln(2) = \\ln(2) + p\\ln(p) - p\\ln(1+p)\n$$\n$$\n\\ln(c_1) = \\ln(2) + p\\ln\\left(\\frac{p}{1+p}\\right) = \\ln(2) + \\ln\\left(\\left(\\frac{p}{1+p}\\right)^p\\right) = \\ln\\left(2 \\left(\\frac{p}{1+p}\\right)^p\\right)\n$$\nThus, $c_1 = 2 \\left(\\frac{p}{1+p}\\right)^p$.\n\n**Step 3: Calculate the updated, un-normalized probabilities.**\nLet $P_1'(x) = P_0(x) c_x$.\nFor $x=0$:\n$$\nP_1'(0) = P_0(0) c_0 = \\frac{1}{2} \\cdot \\frac{2}{1+p} = \\frac{1}{1+p}\n$$\nFor $x=1$:\n$$\nP_1'(1) = P_0(1) c_1 = \\frac{1}{2} \\cdot 2 \\left(\\frac{p}{1+p}\\right)^p = \\left(\\frac{p}{1+p}\\right)^p\n$$\n\n**Step 4: Normalize to find the final distribution $P_1(x)$.**\nThe normalization constant is $Z_1 = P_1'(0) + P_1'(1)$.\n$$\nZ_1 = \\frac{1}{1+p} + \\left(\\frac{p}{1+p}\\right)^p = \\frac{1}{1+p} + \\frac{p^p}{(1+p)^p}\n$$\nNow we find the final probabilities $P_1(x) = P_1'(x) / Z_1$.\n$$\nP_1(X=0) = \\frac{P_1'(0)}{Z_1} = \\frac{\\frac{1}{1+p}}{\\frac{1}{1+p} + \\frac{p^p}{(1+p)^p}}\n$$\nTo simplify, multiply the numerator and the denominator by $(1+p)$:\n$$\nP_1(X=0) = \\frac{1}{1 + (1+p) \\frac{p^p}{(1+p)^p}} = \\frac{1}{1 + p^p (1+p)^{1-p}}\n$$\nFor $P_1(X=1)$:\n$$\nP_1(X=1) = \\frac{P_1'(1)}{Z_1} = \\frac{\\left(\\frac{p}{1+p}\\right)^p}{\\frac{1}{1+p} + \\frac{p^p}{(1+p)^p}}\n$$\nSimilarly, multiply the numerator and the denominator by $(1+p)$:\n$$\nP_1(X=1) = \\frac{(1+p) \\left(\\frac{p}{1+p}\\right)^p}{1 + (1+p) \\frac{p^p}{(1+p)^p}} = \\frac{(1+p) \\frac{p^p}{(1+p)^p}}{1 + p^p (1+p)^{1-p}} = \\frac{p^p (1+p)^{1-p}}{1 + p^p (1+p)^{1-p}}\n$$\n\nThe updated input distribution is the vector $(P_1(X=0), P_1(X=1))$.\n$$\nP_1(X) = \\left( \\frac{1}{1 + p^p (1+p)^{1-p}}, \\frac{p^p (1+p)^{1-p}}{1 + p^p (1+p)^{1-p}} \\right)\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{1 + p^{p} (1+p)^{1-p}} & \\frac{p^{p} (1+p)^{1-p}}{1 + p^{p} (1+p)^{1-p}} \\end{pmatrix}}$$", "id": "1669124"}]}