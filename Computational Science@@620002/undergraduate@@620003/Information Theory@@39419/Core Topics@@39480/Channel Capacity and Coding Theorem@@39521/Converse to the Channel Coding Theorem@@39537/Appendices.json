{"hands_on_practices": [{"introduction": "We begin with a thought experiment involving a \"Collapse Channel,\" a hypothetical setup that maps all inputs to a single output. This exercise [@problem_id:1613884] illustrates the most fundamental implication of the converse theorem: if no information can be distinguished at the output, the channel capacity is zero, and no rate of reliable communication is possible. It provides a stark and clear foundation for understanding the concept of a communication limit.", "id": "1613884", "problem": "Consider a hypothetical discrete memoryless channel called the \"Collapse Channel\". This channel has an input alphabet $\\mathcal{X}$ containing $m$ distinct symbols, where $m \\ge 2$, and an output alphabet $\\mathcal{Y}$ consisting of a single symbol, $\\{c\\}$. The channel is characterized by its transition probabilities: for any input symbol $x \\in \\mathcal{X}$, the output is always $c$. That is, the conditional probability $p(y=c|x) = 1$ for all $x \\in \\mathcal{X}$.\n\nLet the capacity of this channel be $C$, measured in bits per channel use. Based on this definition, determine the value of $C$ and identify the correct implication of the converse to the channel coding theorem for this specific channel.\n\nWhich of the following statements is correct?\n\nA. The capacity is $C = \\log_2(m)$ bits/use. This implies that information can be transmitted reliably up to a rate of $\\log_2(m)$ bits/use.\nB. The capacity is $C=0$ bits/use. This implies that for any transmission rate $R > 0$, it is impossible to achieve an arbitrarily low probability of error.\nC. The capacity is $C=1$ bit/use. This implies that one bit of information can be reliably transmitted per channel use, regardless of the size of the input alphabet.\nD. The capacity is $C=0$ bits/use. This implies that reliable communication is still possible, but only if the block length of the code used is made infinitely long.\nE. The capacity is $C=0$ bits/use. This implies that information can be transmitted without error, but only at an infinitesimally small, non-zero rate $R \\to 0^{+}$.\n\n", "solution": "By definition, the capacity of a discrete memoryless channel is\n$$\nC=\\max_{p(x)} I(X;Y),\n$$\nwhere $I(X;Y)$ is the mutual information between the input $X$ and output $Y$ induced by the input distribution $p(x)$ and the channel transition probabilities.\n\nFor the Collapse Channel, the transition law is $p(y=c \\mid x)=1$ for all $x \\in \\mathcal{X}$. Hence the output $Y$ is a constant random variable equal to $c$, regardless of the input. Using the identity\n$$\nI(X;Y)=H(Y)-H(Y \\mid X),\n$$\nwe evaluate each term:\n- Since $Y$ is constant, its entropy is\n$$\nH(Y)=0.\n$$\n- Because $Y$ is a deterministic function of $X$ (specifically, $Y=c$ for all $X$), the conditional entropy is\n$$\nH(Y \\mid X)=0.\n$$\nTherefore,\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=0-0=0\n$$\nfor every input distribution $p(x)$. Maximizing over $p(x)$ gives\n$$\nC=\\max_{p(x)} I(X;Y)=0 \\text{ bits/use}.\n$$\n\nThe converse to the channel coding theorem states that for any rate $R>C$, the average probability of error of any sequence of codes cannot be made arbitrarily small; specifically, it is bounded away from zero uniformly in block length. Since $C=0$ for this channel, this means that for any $R>0$, it is impossible to achieve an arbitrarily low probability of error. This matches statement B and contradicts statements A, C, D, and E.", "answer": "$$\\boxed{B}$$"}, {"introduction": "Now, let's apply our knowledge to the classic Binary Symmetric Channel (BSC), but with a twist. This problem [@problem_id:1613867] considers a channel so noisy that errors are more frequent than correct transmissions ($p \\gt 0.5$). You will see how a clever post-processing trick at the receiver can make communication feasible and then use the converse theorem to calculate the precise boundary at which a given data rate becomes theoretically impossible.", "id": "1613867", "problem": "A Binary Symmetric Channel (BSC) is defined by its crossover probability $p$, the probability that a bit is flipped during transmission. The channel capacity $C$ of a BSC, which according to the Channel Coding Theorem represents the maximum rate for reliable communication, is given by $C = 1 - H_2(p)$, where $H_2(p) = -p\\log_2(p) - (1-p)\\log_2(1-p)$ is the binary entropy function in bits.\n\nA particular communication system is built using a noisy physical medium which results in a BSC with a crossover probability $p$ that is known to be greater than $0.5$. The converse to the channel coding theorem dictates that reliable communication is possible only if the chosen transmission rate $R$ does not exceed the channel's effective capacity.\n\nThe engineering team intends to transmit data at a fixed rate of $R = \\frac{3}{4}\\log_2(3) - 1$ bits per channel use. By implementing an appropriate data post-processing strategy at the receiver, they can counteract the high noise level and utilize the channel. Determine the maximum possible physical crossover probability $p_0$ that this channel can have while still theoretically permitting reliable communication at the rate $R$.\n\n", "solution": "The capacity of a Binary Symmetric Channel with crossover probability $p$ is $C(p) = 1 - H_{2}(p)$, where $H_{2}(p) = -p \\log_{2}(p) - (1-p)\\log_{2}(1-p)$. The binary entropy function satisfies $H_{2}(p) = H_{2}(1-p)$ and is strictly increasing on $[0, \\tfrac{1}{2}]$.\n\nGiven that the physical channel has $p > \\tfrac{1}{2}$, the receiver can deterministically flip all received bits, which transforms the effective crossover probability to $q = 1 - p \\in (0, \\tfrac{1}{2})$. The capacity available for reliable communication is then\n$$\nC_{\\text{eff}} = 1 - H_{2}(q).\n$$\nBy the converse to the channel coding theorem, reliable communication at rate $R$ is possible only if\n$$\nR \\leq C_{\\text{eff}} = 1 - H_{2}(q).\n$$\nFor a fixed $R$, the boundary (largest admissible $q$ on $[0, \\tfrac{1}{2}]$) is obtained by equality:\n$$\nR = 1 - H_{2}(q_{0}) \\quad \\Longleftrightarrow \\quad H_{2}(q_{0}) = 1 - R.\n$$\nWith $R = \\frac{3}{4}\\log_{2}(3) - 1$, we have\n$$\n1 - R = 1 - \\left(\\frac{3}{4}\\log_{2}(3) - 1\\right) = 2 - \\frac{3}{4}\\log_{2}(3).\n$$\nWe now show that $q_{0} = \\frac{1}{4}$ satisfies this:\n$$\nH_{2}\\!\\left(\\frac{1}{4}\\right) = -\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right) - \\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right)\n= -\\frac{1}{4}(-2) - \\frac{3}{4}\\bigl(\\log_{2}(3) - 2\\bigr)\n= \\frac{1}{2} - \\frac{3}{4}\\log_{2}(3) + \\frac{3}{2}\n= 2 - \\frac{3}{4}\\log_{2}(3).\n$$\nHence $q_{0} = \\frac{1}{4}$. Since $q = 1 - p$, this boundary corresponds to\n$$\np_{0} = 1 - q_{0} = 1 - \\frac{1}{4} = \\frac{3}{4}.\n$$\nFor all $p \\geq p_{0}$ (with $p > \\tfrac{1}{2}$), $C_{\\text{eff}} \\geq R$, so reliable communication at rate $R$ is theoretically feasible. The threshold (closest to $\\tfrac{1}{2}$ on the $p > \\tfrac{1}{2}$ side) is $p_{0} = \\frac{3}{4}$.", "answer": "$$\\boxed{\\frac{3}{4}}$$"}, {"introduction": "The converse to the channel coding theorem is an asymptotic result, a fact with profound practical implications. This exercise [@problem_id:1613859] presents a common source of confusion: a finite-length code that operates above capacity ($R \\gt C$) yet achieves a small, non-zero error probability. Your task is to explain why this observation does not contradict the theorem, thereby clarifying the crucial distinction between the performance of a specific, finite code and the ultimate limits of any sequence of codes as block length grows to infinity.", "id": "1613859", "problem": "An engineer is designing a communication system to transmit data over a noisy channel, which is accurately modeled as a Binary Symmetric Channel (BSC). In a BSC, each transmitted bit is flipped (from 0 to 1 or 1 to 0) with a fixed crossover probability $p$. For this particular channel, the crossover probability is $p = 0.1$.\n\nThe engineer devises a block code with a block length of $n=8$. The code is designed to transmit one of $M=64$ distinct messages in each block. After implementing and testing this code, the engineer reports an average probability of error, $P_e^{(n)}$, of approximately $0.05$.\n\nA student reviewing the design notes that the code's rate $R = \\frac{\\log_2(M)}{n}$ appears to be greater than the channel capacity $C = 1 - H_2(p)$, where $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ is the binary entropy function. The student is concerned that this result violates the converse to the channel coding theorem.\n\nWhich of the following statements provides the correct explanation for this situation?\n\nA. The converse to the channel coding theorem is an asymptotic result. It states that for any sequence of codes with a rate $R > C$, the probability of error must approach 1 as the block length $n$ approaches infinity. It does not forbid the existence of a specific code with a finite block length $n$ and a small, non-zero error probability.\n\nB. The converse theorem is violated. Any code with a rate greater than the channel capacity must have a probability of error equal to 1, regardless of the block length. The engineer's reported error probability of $0.05$ must be the result of a measurement error.\n\nC. The engineer has miscalculated the capacity. For a BSC with $p=0.1$, the capacity is actually greater than the code's rate, which is why a low probability of error is achievable.\n\nD. This result is only possible because the engineer used a highly optimized non-linear code. Shannon's converse theorem only provides a bound for the average performance of randomly constructed linear codes, not for all possible codes.\n\nE. The direct part of the channel coding theorem is more relevant here. It implies that even for rates $R > C$, it is sometimes possible to achieve a low probability of error, although it is not guaranteed.\n\n", "solution": "Compute the code rate. By definition,\n$$\nR=\\frac{\\log_{2}(M)}{n}.\n$$\nWith $M=64$ and $n=8$, use $\\log_{2}(64)=6$ to get\n$$\nR=\\frac{6}{8}=\\frac{3}{4}=0.75.\n$$\n\nCompute the BSC capacity. For a BSC with crossover probability $p$, the capacity is\n$$\nC=1-H_{2}(p),\\quad H_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nSubstitute $p=0.1$:\n$$\nH_{2}(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nUsing $\\log_{2}(0.1)=\\frac{\\ln(0.1)}{\\ln(2)}=-\\frac{\\ln(10)}{\\ln(2)}$ and $\\log_{2}(0.9)=\\frac{\\ln(0.9)}{\\ln(2)}$, one finds numerically that $H_{2}(0.1)\\approx 0.469$, hence\n$$\nC=1-H_{2}(0.1)\\approx 0.531.\n$$\nTherefore,\n$$\nR=0.75>C\\approx 0.531.\n$$\n\nInterpretation relative to the converse. The (strong) converse to the channel coding theorem for discrete memoryless channels, including the BSC, is an asymptotic statement: for any sequence of codes with rate $R>C$, the probability of error $P_{e}^{(n)}$ tends to $1$ as $n\\to\\infty$. It does not preclude the existence of a particular finite-length code with nonzero but small error probability. Hence observing $P_{e}^{(n)}\\approx 0.05$ at $n=8$ and $R>C$ does not violate the converse; it merely cannot persist as $n$ grows while keeping $R>C$.\n\nTherefore, the correct explanation is that the converse is asymptotic and does not forbid isolated finite-$n$ codes of small error probability at rates above capacity, which corresponds to option A. Options B, C, D, and E are incorrect for the following reasons: B falsely applies the converse at finite $n$; C contradicts the computation showing $R>C$; D misstates the scope of the converse, which holds for all codes; E misstates the direct theorem, which guarantees reliability only for $R<C$.", "answer": "$$\\boxed{A}$$"}]}