{"hands_on_practices": [{"introduction": "To build a strong foundation, we begin with the most fundamental application of Fano's inequality. This exercise explores a simple binary memory system modeled as a Binary Symmetric Channel ($BSC$) with a uniform input distribution. By calculating and comparing the actual minimum probability of error against the theoretical bound given by the inequality [@problem_id:1638528], you will see a case where the bound is perfectly tight, providing an intuitive starting point for more complex scenarios.", "problem": "A simple digital memory system stores a single bit, $X$, which is written as a 0 or a 1 with equal probability. Due to thermal noise, the bit can flip over time. This process is modeled as a Binary Symmetric Channel (BSC). After a certain period, the bit is read as a value $Y$. The probability of a bit flip (i.e., reading a 0 when a 1 was stored, or reading a 1 when a 0 was stored) is the crossover probability, $\\epsilon = 0.1$.\n\nAn engineer wants to understand the limits of reliability for this memory system. You are asked to perform two calculations:\n\n1.  First, determine the minimum probability of error, $P_e$, that can be achieved. This is accomplished by using an optimal decision rule to determine an estimate, $\\hat{X}$, of the original bit based on the observed bit, $Y$.\n2.  Second, calculate the theoretical lower bound on this probability of error, denoted as $P_{\\text{bound}}$. This fundamental limit is derived from Fano's inequality. For a binary source like this one, the inequality establishes a relationship between the conditional entropy of the input given the output, $H(X|Y)$, and the binary entropy of the error probability, $H_b(P_e)$. The binary entropy function is given by $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.\n\nCalculate the numerical values for both $P_e$ and $P_{\\text{bound}}$. Present your final answer as a pair of numbers $(P_e, P_{\\text{bound}})$ in a single row matrix, with each value rounded to three significant figures.", "solution": "The problem asks for two quantities: the actual minimum probability of error $P_e$ for an optimal decoder, and the theoretical lower bound $P_{\\text{bound}}$ from Fano's inequality. The system is a Binary Symmetric Channel (BSC) with input $X \\in \\{0, 1\\}$, output $Y \\in \\{0, 1\\}$, uniform input distribution $P(X=0) = P(X=1) = 0.5$, and crossover probability $\\epsilon = 0.1$.\n\n**Part 1: Calculation of the minimum probability of error, $P_e$.**\n\nThe optimal decision rule that minimizes the probability of error $P(\\hat{X} \\neq X)$ is the Maximum A Posteriori (MAP) decoder. The MAP rule chooses the input $\\hat{x}$ that maximizes the posterior probability $P(X=x|Y=y)$ for a given observation $y$.\n$$ \\hat{x} = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nUsing Bayes' theorem, the posterior is $P(X=x|Y=y) = \\frac{P(Y=y|X=x) P(X=x)}{P(Y=y)}$.\nSince the input distribution $P(X=x)$ is uniform ($0.5$ for all $x$), and $P(Y=y)$ is a constant for a given $y$, maximizing the posterior probability is equivalent to maximizing the likelihood $P(Y=y|X=x)$. This decision rule is called the Maximum Likelihood (ML) decoder.\n\nThe channel transition probabilities are:\n$P(Y=0|X=0) = 1-\\epsilon = 0.9$\n$P(Y=1|X=0) = \\epsilon = 0.1$\n$P(Y=0|X=1) = \\epsilon = 0.1$\n$P(Y=1|X=1) = 1-\\epsilon = 0.9$\n\nLet's determine the decision rule:\n- If we observe $Y=0$: We compare $P(Y=0|X=0) = 0.9$ and $P(Y=0|X=1) = 0.1$. Since $0.9 > 0.1$, the optimal decision is $\\hat{X}=0$.\n- If we observe $Y=1$: We compare $P(Y=1|X=0) = 0.1$ and $P(Y=1|X=1) = 0.9$. Since $0.9 > 0.1$, the optimal decision is $\\hat{X}=1$.\n\nThe optimal decision rule is simply $\\hat{X} = Y$. An error occurs if $\\hat{X} \\neq X$, which for this rule means an error occurs if $Y \\neq X$. This happens when the bit flips.\n\nThe probability of error, $P_e$, is the total probability of a bit flip:\n$$ P_e = P(Y \\neq X) = P(Y \\neq X | X=0)P(X=0) + P(Y \\neq X | X=1)P(X=1) $$\n$$ P_e = P(Y=1|X=0)P(X=0) + P(Y=0|X=1)P(X=1) $$\nSubstituting the given values:\n$$ P_e = (\\epsilon)(0.5) + (\\epsilon)(0.5) = \\epsilon $$\n$$ P_e = 0.1 $$\n\n**Part 2: Calculation of the Fano lower bound, $P_{\\text{bound}}$.**\n\nFano's inequality for a binary source provides a lower bound on the probability of error, $P_e$, via the relation:\n$$ H(X|Y) \\le H_b(P_e) $$\nThis can be rearranged to give $H_b(P_e) \\ge H(X|Y)$. We need to calculate the conditional entropy $H(X|Y)$. A convenient formula is $H(X|Y) = H(X) - I(X;Y)$, where $I(X;Y)$ is the mutual information.\n\nFirst, let's calculate the entropy of the source, $H(X)$. Since the input is uniform:\n$$ H(X) = -\\sum_{x \\in \\{0,1\\}} P(X=x) \\log_2(P(X=x)) = - (0.5 \\log_2(0.5) + 0.5 \\log_2(0.5)) = - \\log_2(0.5) = 1 \\text{ bit} $$\n\nNext, we calculate the mutual information $I(X;Y) = H(Y) - H(Y|X)$.\nLet's find the conditional entropy $H(Y|X)$:\n$$ H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) H(Y|X=x) $$\nFor a BSC, the entropy of the output conditioned on a specific input is the binary entropy of the crossover probability:\n$H(Y|X=0) = H(Y|X=1) = H_b(\\epsilon)$.\nTherefore, $H(Y|X) = (0.5) H_b(\\epsilon) + (0.5) H_b(\\epsilon) = H_b(\\epsilon)$.\n\nNow we find the output probabilty distribution $P(Y)$ to calculate $H(Y)$.\n$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-\\epsilon)(0.5) + (\\epsilon)(0.5) = 0.5$.\n$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = (\\epsilon)(0.5) + (1-\\epsilon)(0.5) = 0.5$.\nThe output distribution is also uniform, so its entropy is $H(Y)=1$ bit.\n\nThe mutual information is $I(X;Y) = H(Y) - H(Y|X) = 1 - H_b(\\epsilon)$.\nNow we find the conditional entropy required for Fano's inequality:\n$$ H(X|Y) = H(X) - I(X;Y) = 1 - (1 - H_b(\\epsilon)) = H_b(\\epsilon) $$\n\nSubstituting this into the Fano inequality relation $H_b(P_e) \\ge H(X|Y)$:\n$$ H_b(P_e) \\ge H_b(\\epsilon) $$\nThe binary entropy function $H_b(p)$ is symmetric around $p=0.5$ and is strictly increasing for $p \\in [0, 0.5]$. Since the error probability $\\epsilon=0.1$ is in this range, and we expect any reasonable decoder's error probability $P_e$ to also be in this range, we can conclude from the inequality that:\n$$ P_e \\ge \\epsilon $$\nThis means the minimum possible value for $P_e$ is $\\epsilon$. Thus, the lower bound $P_{\\text{bound}}$ is $\\epsilon$.\n$$ P_{\\text{bound}} = \\epsilon = 0.1 $$\n\n**Conclusion and Formatting**\n\nWe have calculated both the actual minimum error probability for this channel and the Fano lower bound.\n$P_e = 0.1$\n$P_{\\text{bound}} = 0.1$\n\nRounding to three significant figures gives $0.100$ for both values. The final answer is the pair $(0.100, 0.100)$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.100 & 0.100 \\end{pmatrix}}$$", "id": "1638528"}, {"introduction": "Real-world systems often involve more than just two states, and channels can sometimes lose information by design. This practice moves beyond the simple binary case to a scenario with a three-state source whose signal is classified into just two categories by the receiver. This exercise [@problem_id:1638502] will challenge you to apply the core principles of conditional entropy to a non-trivial channel, demonstrating how Fano's inequality provides a performance bound even when alphabets are mismatched.", "problem": "A simple remote environmental sensor is designed to detect one of three possible states of a volcanic vent: Dormant ($S_0$), Gassing ($S_1$), or Erupting ($S_2$). The sensor transmits a signal, which is processed by a receiver. Due to bandwidth limitations, the receiver can only classify the incoming signal into two categories: Low Energy ($V_L$) or High Energy ($V_H$).\n\nThe three environmental states are, a priori, equally likely to occur, i.e., $P(S_0) = P(S_1) = P(S_2) = 1/3$. The channel is characterized by the following conditional probabilities of receiving a High Energy signal given the vent state:\n- $P(V_H | S_0) = 1/4$\n- $P(V_H | S_1) = 1/2$\n- $P(V_H | S_2) = 3/4$\n\nThe probability of receiving a Low Energy signal is complementary, i.e., $P(V_L | S_i) = 1 - P(V_H | S_i)$ for any state $S_i$.\n\nAn observer wants to guess the true state of the vent ($S_i$) based on the received signal ($V_j$). The probability of making an incorrect guess is denoted by $P_e$. A simplified version of Fano's inequality gives a lower bound on this probability.\nCalculate the value of this lower bound, given by the expression $\\frac{H(S|V) - 1}{\\log_2(|\\mathcal{S}| - 1)}$, where $H(S|V)$ is the conditional entropy of the state given the received signal, and $|\\mathcal{S}|$ is the number of possible states. Express your answer as a single closed-form analytic expression using base-2 logarithms.", "solution": "Let the state alphabet be $\\mathcal{S}=\\{S_{0},S_{1},S_{2}\\}$ with $P(S_{i})=\\frac{1}{3}$ for all $i$, and the observation alphabet be $\\mathcal{V}=\\{V_{L},V_{H}\\}$. By the law of total probability,\n$$\nP(V_{H})=\\sum_{i=0}^{2}P(V_{H}\\mid S_{i})P(S_{i})=\\frac{1}{3}\\left(\\frac{1}{4}+\\frac{1}{2}+\\frac{3}{4}\\right)=\\frac{1}{2},\\qquad P(V_{L})=1-P(V_{H})=\\frac{1}{2}.\n$$\nBy Bayesâ€™ rule,\n$$\nP(S_{0}\\mid V_{H})=\\frac{P(V_{H}\\mid S_{0})P(S_{0})}{P(V_{H})}=\\frac{\\frac{1}{4}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{6},\\quad\nP(S_{1}\\mid V_{H})=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3},\\quad\nP(S_{2}\\mid V_{H})=\\frac{\\frac{3}{4}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{2}.\n$$\nSimilarly,\n$$\nP(S_{0}\\mid V_{L})=\\frac{1}{2},\\quad P(S_{1}\\mid V_{L})=\\frac{1}{3},\\quad P(S_{2}\\mid V_{L})=\\frac{1}{6}.\n$$\nThus $H(S\\mid V=v)$ is the same for $v=V_{H}$ and $v=V_{L}$, with distribution $\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}\\right\\}$. Therefore,\n$$\nH(S\\mid V)=\\sum_{v\\in\\{V_{H},V_{L}\\}}P(v)H(S\\mid V=v)\n=H\\!\\left(\\tfrac{1}{2},\\tfrac{1}{3},\\tfrac{1}{6}\\right)\n=-\\left[\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}+\\tfrac{1}{3}\\log_{2}\\tfrac{1}{3}+\\tfrac{1}{6}\\log_{2}\\tfrac{1}{6}\\right].\n$$\nUsing $\\log_{2}\\tfrac{1}{2}=-1$, this becomes\n$$\nH(S\\mid V)=\\tfrac{1}{2}+\\tfrac{1}{3}\\log_{2}3+\\tfrac{1}{6}\\log_{2}6.\n$$\nThe simplified Fano lower bound is\n$$\n\\frac{H(S\\mid V)-1}{\\log_{2}(|\\mathcal{S}|-1)}=\\frac{H(S\\mid V)-1}{\\log_{2}2}=H(S\\mid V)-1,\n$$\nso\n$$\n\\text{LB}= \\left(\\tfrac{1}{2}+\\tfrac{1}{3}\\log_{2}3+\\tfrac{1}{6}\\log_{2}6\\right)-1\n=-\\tfrac{1}{2}+\\tfrac{1}{3}\\log_{2}3+\\tfrac{1}{6}\\log_{2}6.\n$$\nUsing $\\log_{2}6=\\log_{2}2+\\log_{2}3=1+\\log_{2}3$, we simplify to\n$$\n\\text{LB}=-\\tfrac{1}{3}+\\tfrac{1}{2}\\log_{2}3.\n$$", "answer": "$$\\boxed{\\tfrac{1}{2}\\log_{2}3-\\tfrac{1}{3}}$$", "id": "1638502"}, {"introduction": "Theoretical bounds are powerful, but their true value is revealed when compared against practical engineering solutions. This final exercise bridges the gap between theory and practice by analyzing a communication system that uses a repetition code and a majority-rule decoder. You will compute the actual error probability of this specific, suboptimal decoder and compare it to the ultimate limit established by Fano's inequality [@problem_id:1638462], offering a clear metric of how much performance could potentially be gained with a more optimal design.", "problem": "Consider a simple communication system designed to transmit a binary message. The source is a random variable $X$ with alphabet $\\mathcal{X}=\\{0, 1\\}$ and a uniform probability distribution, i.e., $P(X=0) = P(X=1) = 1/2$. To protect against errors, a $(3,1)$ repetition code is used, where the message $X=0$ is encoded into the codeword $C=000$ and $X=1$ is encoded into $C=111$.\n\nThe encoded codeword is transmitted over a Binary Symmetric Channel (BSC) with a crossover probability $p=0.1$. This means each transmitted bit is flipped (from 0 to 1, or 1 to 0) with probability $p$, and transmitted correctly with probability $1-p$, independently of all other bits. The received 3-bit word is denoted by $Y$.\n\nAt the receiver, a majority rule decoder is employed. This decoder estimates the original message as $\\hat{X}=1$ if the received word $Y$ contains two or more 1s; otherwise, it estimates $\\hat{X}=0$.\n\nYour task is to compare the actual performance of this specific decoder to the theoretical limit imposed by Fano's inequality. You will calculate the ratio $R = P_{e,maj} / P_{e,Fano}$, where $P_{e,maj}$ is the probability of error for the majority decoder, and $P_{e,Fano}$ is the information-theoretic lower bound on the probability of any decoder error, as determined by Fano's inequality.\n\nFano's inequality for a binary source implies that for any decoder, the probability of error $P_e$ is bounded by the relation $H(P_e) \\ge H(X|Y)$, where $H(p) = -p\\log_2(p) - (1-p)\\log_2(1-p)$ is the binary entropy function. The lower bound, $P_{e,Fano}$, is the value that satisfies this relationship with equality, i.e., $H(P_{e,Fano}) = H(X|Y)$.\n\nTo find $P_{e,Fano}$ from the value you calculate for $H(X|Y)$, use linear interpolation with the following table of the binary entropy function:\n| $p$      | $H(p)$   |\n|----------|----------|\n| $0.0190$ | $0.1353$ |\n| $0.0200$ | $0.1414$ |\n\nCalculate the ratio $P_{e,maj} / P_{e,Fano}$ and round your final answer to three significant figures.", "solution": "The source is binary uniform, $P(X=0)=P(X=1)=\\frac{1}{2}$. A $(3,1)$ repetition code maps $0 \\mapsto 000$ and $1 \\mapsto 111$. The channel is a BSC with crossover probability $p=0.1$, and each bit flips independently. The majority decoder decides $\\hat{X}=1$ if at least two received bits are $1$, and $\\hat{X}=0$ otherwise.\n\nFirst, compute the majority-decoder error probability. Conditioned on a given $X$, an error occurs if at least two of the three transmitted bits flip. For a BSC with flip probability $p$, this gives\n$$\nP_{e,\\mathrm{maj}}=P(\\mathrm{Bin}(3,p)\\ge 2)=\\binom{3}{2}p^{2}(1-p)+p^{3}=3p^{2}(1-p)+p^{3}.\n$$\nWith $p=0.1$, this yields\n$$\nP_{e,\\mathrm{maj}}=3\\cdot 0.1^{2}\\cdot 0.9+0.1^{3}=0.027+0.001=0.028.\n$$\n\nNext, compute $H(X|Y)$, where $Y$ is the received 3-bit word. For a received word $y$ of Hamming weight $w$, by Bayesâ€™ rule with the uniform prior and channel symmetry,\n$$\nP(X=0\\mid Y=y)=\\frac{p^{w}(1-p)^{3-w}}{p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}},\\quad\nP(X=1\\mid Y=y)=\\frac{p^{3-w}(1-p)^{w}}{p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}}.\n$$\nThis depends only on $w=|y|$, and there are $\\binom{3}{w}$ words of weight $w$. The marginal of a specific $y$ of weight $w$ is\n$$\nP(Y=y)=\\frac{1}{2}\\left[p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}\\right].\n$$\nTherefore,\n$$\nH(X|Y)=\\sum_{w=0}^{3}\\binom{3}{w}\\left[\\frac{1}{2}\\left(p^{w}(1-p)^{3-w}+p^{3-w}(1-p)^{w}\\right)\\right]H\\!\\left(P(X=1\\mid |Y|=w)\\right),\n$$\nwith $H(\\cdot)$ the binary entropy function $H(u)=-u\\log_{2}u-(1-u)\\log_{2}(1-u)$.\n\nFor $p=0.1$, the posteriors simplify:\n- $w=0$: $P(X=1\\mid |Y|=0)=\\dfrac{p^{3}}{(1-p)^{3}+p^{3}}=\\dfrac{0.001}{0.730}=\\dfrac{1}{730}$, and $P(Y=000)=\\dfrac{1}{2}\\left[(1-p)^{3}+p^{3}\\right]=0.365$.\n- $w=3$: $P(X=1\\mid |Y|=3)=\\dfrac{(1-p)^{3}}{(1-p)^{3}+p^{3}}=1-\\dfrac{1}{730}$, and $P(Y=111)=0.365$.\n- $w=1$: $P(X=1\\mid |Y|=1)=\\dfrac{p}{1-p+p}=p=0.1$, with $3$ such words each of probability $\\dfrac{1}{2}\\left[p(1-p)^{2}+p^{2}(1-p)\\right]=0.045$, totaling $0.135$.\n- $w=2$: $P(X=1\\mid |Y|=2)=1-p=0.9$, with total probability also $0.135$.\n\nBy symmetry $H\\!\\left(\\dfrac{1}{730}\\right)=H\\!\\left(1-\\dfrac{1}{730}\\right)$ and $H(0.1)=H(0.9)$, so\n$$\nH(X|Y)=0.73\\,H\\!\\left(\\frac{1}{730}\\right)+0.27\\,H(0.1).\n$$\nNumerically, $H(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\approx 0.4689955936$, and\n$$\nH\\!\\left(\\frac{1}{730}\\right)=-\\frac{1}{730}\\log_{2}\\!\\left(\\frac{1}{730}\\right)-\\frac{729}{730}\\log_{2}\\!\\left(\\frac{729}{730}\\right)\\approx 0.015002763.\n$$\nThus\n$$\nH(X|Y)\\approx 0.73\\cdot 0.015002763+0.27\\cdot 0.4689955936\\approx 0.137580827.\n$$\n\nFanoâ€™s inequality for a binary source states $H(P_{e})\\ge H(X|Y)$. The lower bound $P_{e,\\mathrm{Fano}}$ is defined by equality $H(P_{e,\\mathrm{Fano}})=H(X|Y)$. Using the provided table and linear interpolation between\n$$\n(p,H(p))=(0.0190,\\,0.1353)\\quad\\text{and}\\quad(0.0200,\\,0.1414),\n$$\nwe set\n$$\nP_{e,\\mathrm{Fano}}=0.0190+\\frac{H(X|Y)-0.1353}{0.1414-0.1353}\\times(0.0200-0.0190)\n=0.0190+\\frac{0.137580827-0.1353}{0.0061}\\times 0.001,\n$$\nwhich yields\n$$\nP_{e,\\mathrm{Fano}}\\approx 0.019373905.\n$$\n\nFinally, the requested ratio is\n$$\nR=\\frac{P_{e,\\mathrm{maj}}}{P_{e,\\mathrm{Fano}}}=\\frac{0.028}{0.019373905}\\approx 1.44524,\n$$\nwhich, rounded to three significant figures, is $1.45$.", "answer": "$$\\boxed{1.45}$$", "id": "1638462"}]}