{"hands_on_practices": [{"introduction": "This first practice introduces the Binary Erasure Channel (BEC), a fundamental model where an input bit $X$ is either perfectly received at the output $Y$ or completely lost (erased), but never corrupted. This exercise will guide you in calculating the mutual information for a BEC, providing a concrete measure of how much information survives the erasures, a common scenario in networks where data packets can be dropped [@problem_id:1622685]. Mastering this example is a crucial first step in understanding channels where uncertainty isn't just about errors, but also about missing data.", "problem": "A specialized digital communication system is designed to transmit binary data. The system can be modeled as a discrete memoryless channel, specifically a **Binary Erasure Channel (BEC)**. The input to the channel is a single bit, represented by a random variable $X$ with the alphabet $\\mathcal{X} = \\{0, 1\\}$. The output of the channel is represented by a random variable $Y$. Due to imperfections, the channel sometimes fails to resolve the transmitted bit, outputting a special \"erasure\" symbol, $\\lambda$. Thus, the output alphabet is $\\mathcal{Y} = \\{0, 1, \\lambda\\}$.\n\nThe behavior of the channel is characterized by the following properties:\n1.  The channel never flips a bit. This means that if the output is $0$, the input must have been $0$, and if the output is $1$, the input must have been $1$.\n2.  The probability of an erasure occurring is $\\alpha$, where $0 < \\alpha < 1$. This probability is independent of whether a $0$ or a $1$ was sent.\n\nThe source transmitting the data produces bits with the following probabilities: $P(X=0) = p$ and $P(X=1) = 1-p$, where $0 < p < 1$.\n\nDerive a closed-form analytic expression for the mutual information $I(X;Y)$ between the input $X$ and the output $Y$ for this channel. Your final expression should be in terms of the parameters $p$ and $\\alpha$. In your calculations and final answer, all logarithms must be base 2.", "solution": "The mutual information $I(X;Y)$ can be calculated using the formula $I(X;Y) = H(X) - H(X|Y)$, where $H(X)$ is the entropy of the input and $H(X|Y)$ is the conditional entropy of the input given the output. All logarithms are base 2.\n\nFirst, let's calculate the entropy of the input source, $H(X)$.\nThe random variable $X$ follows a Bernoulli distribution with parameter $p$. Its entropy is given by the binary entropy function:\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} P(X=x) \\log_{2}(P(X=x))$$\n$$H(X) = -[P(X=0) \\log_{2}(P(X=0)) + P(X=1) \\log_{2}(P(X=1))]$$\nSubstituting the given probabilities $P(X=0) = p$ and $P(X=1) = 1-p$:\n$$H(X) = -[p \\log_{2}(p) + (1-p) \\log_{2}(1-p)]$$\n\nNext, we calculate the conditional entropy $H(X|Y)$. The formula is:\n$$H(X|Y) = \\sum_{y \\in \\mathcal{Y}} P(Y=y) H(X|Y=y)$$\nwhere $H(X|Y=y) = -\\sum_{x \\in \\mathcal{X}} P(X=x|Y=y) \\log_{2}(P(X=x|Y=y))$.\n\nTo do this, we need to determine the channel's transition probabilities $P(Y=y|X=x)$, the output probabilities $P(Y=y)$, and the posterior probabilities $P(X=x|Y=y)$.\n\nThe transition probabilities are given by the problem description:\n-   $P(Y=0|X=0) = 1-\\alpha$ (successful transmission of 0)\n-   $P(Y=\\lambda|X=0) = \\alpha$ (erasure of 0)\n-   $P(Y=1|X=0) = 0$ (no bit flips)\n-   $P(Y=1|X=1) = 1-\\alpha$ (successful transmission of 1)\n-   $P(Y=\\lambda|X=1) = \\alpha$ (erasure of 1)\n-   $P(Y=0|X=1) = 0$ (no bit flips)\n\nNow, we find the output probabilities $P(Y=y)$ using the law of total probability, $P(Y=y) = \\sum_{x} P(Y=y|X=x)P(X=x)$:\n-   $P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-\\alpha)p + 0 \\cdot (1-p) = p(1-\\alpha)$\n-   $P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = 0 \\cdot p + (1-\\alpha)(1-p) = (1-p)(1-\\alpha)$\n-   $P(Y=\\lambda) = P(Y=\\lambda|X=0)P(X=0) + P(Y=\\lambda|X=1)P(X=1) = \\alpha p + \\alpha(1-p) = \\alpha(p+1-p) = \\alpha$\n\nNext, we determine the posterior probabilities $P(X=x|Y=y)$ using Bayes' theorem, $P(X=x|Y=y) = \\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}$:\n-   If $Y=0$:\n    $P(X=0|Y=0) = \\frac{P(Y=0|X=0)P(X=0)}{P(Y=0)} = \\frac{(1-\\alpha)p}{p(1-\\alpha)} = 1$.\n    $P(X=1|Y=0) = 0$.\n    The uncertainty about X is zero when Y=0 is received. Thus, $H(X|Y=0) = 0$.\n-   If $Y=1$:\n    $P(X=1|Y=1) = \\frac{P(Y=1|X=1)P(X=1)}{P(Y=1)} = \\frac{(1-\\alpha)(1-p)}{(1-p)(1-\\alpha)} = 1$.\n    $P(X=0|Y=1) = 0$.\n    The uncertainty about X is zero when Y=1 is received. Thus, $H(X|Y=1) = 0$.\n-   If $Y=\\lambda$:\n    $P(X=0|Y=\\lambda) = \\frac{P(Y=\\lambda|X=0)P(X=0)}{P(Y=\\lambda)} = \\frac{\\alpha p}{\\alpha} = p$.\n    $P(X=1|Y=\\lambda) = \\frac{P(Y=\\lambda|X=1)P(X=1)}{P(Y=\\lambda)} = \\frac{\\alpha (1-p)}{\\alpha} = 1-p$.\n    The conditional distribution of $X$ given $Y=\\lambda$ is the same as the original distribution of $X$. The conditional entropy is therefore $H(X|Y=\\lambda) = -[p \\log_{2}(p) + (1-p) \\log_{2}(1-p)] = H(X)$.\n\nNow we can calculate the total conditional entropy $H(X|Y)$:\n$$H(X|Y) = P(Y=0)H(X|Y=0) + P(Y=1)H(X|Y=1) + P(Y=\\lambda)H(X|Y=\\lambda)$$\n$$H(X|Y) = [p(1-\\alpha)] \\cdot 0 + [(1-p)(1-\\alpha)] \\cdot 0 + \\alpha \\cdot H(X)$$\n$$H(X|Y) = \\alpha H(X)$$\n\nFinally, we compute the mutual information:\n$$I(X;Y) = H(X) - H(X|Y) = H(X) - \\alpha H(X) = (1-\\alpha)H(X)$$\nSubstituting the expression for $H(X)$:\n$$I(X;Y) = (1-\\alpha)[-p \\log_{2}(p) - (1-p) \\log_{2}(1-p)]$$\nThis expression represents the mutual information between the input and output of the binary erasure channel for the given source distribution.", "answer": "$$\\boxed{(1-\\alpha)(-p \\log_{2}(p) - (1-p) \\log_{2}(1-p))}$$", "id": "1622685"}, {"introduction": "Moving beyond binary alphabets, this exercise explores a channel with a larger set of inputs affected by additive noise, a common model for signal distortion. By analyzing a hypothetical channel where the output is $Y = (X + Z) \\pmod 7$, you will learn how the algebraic structure and symmetry of the channel can greatly simplify the calculation of mutual information [@problem_id:1622708]. This practice highlights a powerful principle: recognizing symmetry can turn a complicated analysis into an elegant and straightforward solution.", "problem": "Consider a discrete communication channel designed to transmit one of seven possible signal levels, which we represent by the set of symbols $\\mathcal{X} = \\{0, 1, 2, 3, 4, 5, 6\\}$. The input to the channel is a random variable $X$ drawn from this set. For this particular system, each input symbol is chosen with equal probability.\n\nThe channel is affected by a specific type of additive noise. The noise is represented by a random variable $Z$, which can take the value 1 with probability $p$ and the value 0 with probability $1-p$, where $0 < p < 1$. The channel output, denoted by the random variable $Y$, is the sum of the input $X$ and the noise $Z$, computed modulo 7. That is, $Y = (X + Z) \\pmod 7$. The output symbols are also in the set $\\mathcal{Y} = \\{0, 1, 2, 3, 4, 5, 6\\}$.\n\nCalculate the mutual information $I(X;Y)$ between the input $X$ and the output $Y$. Express your answer in nats as an analytic expression in terms of the probability $p$.", "solution": "We are given a discrete memoryless channel over the alphabet $\\mathcal{X}=\\{0,1,2,3,4,5,6\\}$ with uniform input $X$ and output $Y=(X+Z)\\bmod 7$, where the additive noise $Z$ is independent of $X$ and takes values in $\\{0,1\\}$ with $P(Z=1)=p$ and $P(Z=0)=1-p$, where $0<p<1$. The mutual information is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nFirst compute the output distribution. For any $y\\in\\mathcal{Y}$,\n$$\nP(Y=y)=\\sum_{x\\in\\mathcal{X}}P(X=x)P\\bigl(Z\\equiv y-x \\ (\\bmod\\ 7)\\bigr).\n$$\nSince $X$ is uniform, $P(X=x)=\\frac{1}{7}$ for all $x$, and as $x$ ranges over $\\mathcal{X}$, the residues $(y-x)\\bmod 7$ range over all elements of $\\{0,1,2,3,4,5,6\\}$. Therefore,\n$$\nP(Y=y)=\\frac{1}{7}\\sum_{z=0}^{6}P(Z=z)=\\frac{1}{7}\\bigl(P(Z=0)+P(Z=1)\\bigr)=\\frac{1}{7},\n$$\nbecause $P(Z=z)=0$ for $z\\notin\\{0,1\\}$ and $P(Z=0)+P(Z=1)=1$. Hence $Y$ is uniform on $\\mathcal{Y}$ and\n$$\nH(Y)=\\ln 7.\n$$\nNext, compute $H(Y|X)$. Given $X=x$, we have $Y=x+Z \\ (\\bmod\\ 7)$, which is a bijective relabeling of $Z$. Therefore the conditional distribution of $Y$ given $X=x$ is the same as the distribution of $Z$, and the conditional entropy does not depend on $x$:\n$$\nH(Y|X=x)=H(Z), \\quad \\text{so} \\quad H(Y|X)=H(Z).\n$$\nNow\n$$\nH(Z)=-(1-p)\\ln(1-p)-p\\ln p.\n$$\nCombining the pieces,\n$$\nI(X;Y)=H(Y)-H(Y|X)=\\ln 7 -\\bigl(-(1-p)\\ln(1-p)-p\\ln p\\bigr)=\\ln 7 + p\\ln p + (1-p)\\ln(1-p).\n$$\nThis is in nats since all logarithms are natural.", "answer": "$$\\boxed{\\ln 7 + p\\ln p + (1-p)\\ln(1-p)}$$", "id": "1622708"}, {"introduction": "Real-world systems often exhibit complex behaviors that can be modeled as a mixture of simpler processes. This exercise presents a 'Forgetful Binary Channel,' which probabilistically switches between acting as a standard Binary Symmetric Channel and producing random noise independent of the input [@problem_id:1622700]. The challenge is to analyze this composite behavior and demonstrate that it is equivalent to a simpler, well-known channel model, a key skill in engineering analysis where complex systems must be reduced to their essential characteristics.", "problem": "Consider a \"Forgetful Binary Channel\", a discrete memoryless communication channel with a binary input alphabet $X \\in \\{0, 1\\}$ and a binary output alphabet $Y \\in \\{0, 1\\}$. The channel's behavior is governed by two probabilistic modes. With a probability $q$, the channel enters a \"forgetful\" state where it completely ignores the input bit and transmits a random bit, meaning the output $Y$ is $0$ with probability $1/2$ and $1$ with probability $1/2$, regardless of the input $X$. With the remaining probability $1-q$, the channel acts as a standard Binary Symmetric Channel (BSC), which correctly transmits the input bit with probability $1-p$ and flips the input bit (i.e., transmits a $0$ as a $1$ or a $1$ as a $0$) with a crossover probability $p$. The parameters $p$ and $q$ are real numbers in the interval $[0, 1]$.\n\nDetermine the capacity of this Forgetful Binary Channel. Provide your answer as a closed-form analytic expression in terms of the parameters $p$ and $q$. Use base-2 logarithms in your final expression.", "solution": "Let $X \\in \\{0,1\\}$ be the input and $Y \\in \\{0,1\\}$ the output. The channel operates in two modes selected independently for each use:\n- With probability $q$ (forgetful mode), $Y$ is independent of $X$ and is $0$ or $1$ with probability $\\frac{1}{2}$ each.\n- With probability $1-q$ (BSC mode), the channel behaves as a Binary Symmetric Channel with crossover probability $p$.\n\nCondition on the input $X$ and average over the two modes to obtain the overall transition probabilities. For any $x \\in \\{0,1\\}$,\n$$\n\\Pr(Y \\neq X \\mid X=x) \\;=\\; q \\cdot \\frac{1}{2} \\;+\\; (1-q)\\,p,\n$$\nand\n$$\n\\Pr(Y = X \\mid X=x) \\;=\\; q \\cdot \\frac{1}{2} \\;+\\; (1-q)\\,(1-p).\n$$\nDefine the effective crossover probability\n$$\nr \\;\\triangleq\\; (1-q)\\,p \\;+\\; \\frac{q}{2}.\n$$\nThen, for $X=0$, $\\Pr(Y=1 \\mid X=0)=r$ and $\\Pr(Y=0 \\mid X=0)=1-r$, and for $X=1$, $\\Pr(Y=0 \\mid X=1)=r$ and $\\Pr(Y=1 \\mid X=1)=1-r$. Therefore, the overall channel is exactly a Binary Symmetric Channel with crossover probability $r$.\n\nThe capacity of a Binary Symmetric Channel with crossover probability $r$ (in bits per channel use, with base-2 logarithms) is achieved by the uniform input distribution $\\Pr(X=0)=\\Pr(X=1)=\\frac{1}{2}$ and equals\n$$\nC \\;=\\; I(X;Y) \\;=\\; H(Y) - H(Y \\mid X).\n$$\nUnder the uniform input, $H(Y)=1$ for a BSC, and $H(Y \\mid X) = H_{2}(r)$, where the binary entropy function is $H_{2}(r) = - r \\log_{2} r - (1-r) \\log_{2} (1-r)$. Hence,\n$$\nC \\;=\\; 1 - H_{2}(r) \\;=\\; 1 + r \\log_{2} r + (1-r) \\log_{2} (1-r),\n$$\nwith $r = (1-q)p + \\frac{q}{2}$.\n\nSubstituting $r$ in terms of $p$ and $q$ gives the capacity as\n$$\nC(p,q) \\;=\\; 1 + \\left((1-q)p + \\frac{q}{2}\\right)\\log_{2}\\left((1-q)p + \\frac{q}{2}\\right) + \\left(1 - (1-q)p - \\frac{q}{2}\\right)\\log_{2}\\left(1 - (1-q)p - \\frac{q}{2}\\right).\n$$\nThis expression reduces to the standard BSC capacity $1 - H_{2}(p)$ when $q=0$, and to $0$ when $q=1$ (since then $r=\\frac{1}{2}$), as expected.", "answer": "$$\\boxed{1 + \\left((1-q)p + \\frac{q}{2}\\right)\\log_{2}\\left((1-q)p + \\frac{q}{2}\\right) + \\left(1 - (1-q)p - \\frac{q}{2}\\right)\\log_{2}\\left(1 - (1-q)p - \\frac{q}{2}\\right)}$$", "id": "1622700"}]}