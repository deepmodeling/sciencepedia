{"hands_on_practices": [{"introduction": "To truly grasp the concept of channel capacity, it's illuminating to start with an extreme case: a channel that is completely ineffective. This exercise introduces a hypothetical \"Uniform Scrambler Channel,\" where the output is statistically independent of the input.\n\nThis practice [@problem_id:1661911] will guide you to demonstrate that when there is no correlation between what is sent and what is received, the mutual information is zero for any input distribution. This leads to a profound conclusion: the channel capacity is zero, establishing an intuitive lower bound for information transmission.", "problem": "Consider a discrete memoryless channel which we will call the \"Uniform Scrambler Channel\" (USC). The channel has a finite input alphabet $\\mathcal{X}$ and an output alphabet $\\mathcal{Y}$ containing 10 distinct symbols. The defining characteristic of the USC is that for any given input symbol from $\\mathcal{X}$, the conditional probability of receiving any of the 10 specific output symbols from $\\mathcal{Y}$ is the same.\n\nCalculate the capacity of this channel. All logarithms are to be taken in base 2. Express your answer in bits per channel use.", "solution": "The capacity $C$ of a discrete memoryless channel is defined as the maximum mutual information between the input $X$ and the output $Y$ over all possible input distributions $p(x)$.\n$$C = \\max_{p(x)} I(X;Y)$$\nThe mutual information $I(X;Y)$ can be expressed in terms of entropy as:\n$$I(X;Y) = H(Y) - H(Y|X)$$\nwhere $H(Y)$ is the entropy of the output and $H(Y|X)$ is the conditional entropy of the output given the input.\n\nFirst, let's calculate the conditional entropy $H(Y|X)$. It is defined as:\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) H(Y|X=x)$$\nThe entropy of the output $Y$ conditioned on a specific input $x$ is given by:\n$$H(Y|X=x) = -\\sum_{y \\in \\mathcal{Y}} p(y|x) \\log_{2}(p(y|x))$$\nThe problem states that the output alphabet $\\mathcal{Y}$ has 10 symbols, and for any input $x$, the probability of receiving any output symbol $y$ is the same. This means the conditional probability distribution is uniform:\n$$p(y|x) = \\frac{1}{10} \\quad \\text{for all } x \\in \\mathcal{X} \\text{ and } y \\in \\mathcal{Y}$$\nNow, we can calculate $H(Y|X=x)$ for any $x$:\n$$H(Y|X=x) = -\\sum_{j=1}^{10} \\frac{1}{10} \\log_{2}\\left(\\frac{1}{10}\\right)$$\nSince the term inside the sum is constant for all 10 output symbols, we have:\n$$H(Y|X=x) = -10 \\cdot \\frac{1}{10} \\log_{2}\\left(\\frac{1}{10}\\right) = -\\log_{2}(10^{-1}) = \\log_{2}(10)$$\nSince $H(Y|X=x) = \\log_{2}(10)$ for every input symbol $x$, the overall conditional entropy $H(Y|X)$ becomes:\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) (\\log_{2}(10)) = \\log_{2}(10) \\sum_{x \\in \\mathcal{X}} p(x)$$\nSince the sum of probabilities of all possible inputs is 1, $\\sum_{x \\in \\mathcal{X}} p(x) = 1$.\n$$H(Y|X) = \\log_{2}(10)$$\nNext, let's calculate the output entropy $H(Y)$. The probability of a specific output symbol $y$ is given by the law of total probability:\n$$p(y) = \\sum_{x \\in \\mathcal{X}} p(y|x) p(x)$$\nSubstituting the channel's property $p(y|x) = 1/10$:\n$$p(y) = \\sum_{x \\in \\mathcal{X}} \\frac{1}{10} p(x) = \\frac{1}{10} \\sum_{x \\in \\mathcal{X}} p(x) = \\frac{1}{10} \\cdot 1 = \\frac{1}{10}$$\nThis shows that the output probability distribution is also uniform, regardless of the input distribution $p(x)$. The output entropy $H(Y)$ is therefore:\n$$H(Y) = -\\sum_{y \\in \\mathcal{Y}} p(y) \\log_{2}(p(y)) = -\\sum_{j=1}^{10} \\frac{1}{10} \\log_{2}\\left(\\frac{1}{10}\\right)$$\n$$H(Y) = -10 \\cdot \\frac{1}{10} \\log_{2}\\left(\\frac{1}{10}\\right) = \\log_{2}(10)$$\nNow we can compute the mutual information $I(X;Y)$:\n$$I(X;Y) = H(Y) - H(Y|X) = \\log_{2}(10) - \\log_{2}(10) = 0$$\nSince the mutual information is 0 for any arbitrary input distribution $p(x)$, the maximum mutual information is also 0.\n$$C = \\max_{p(x)} (0) = 0$$\nThe capacity of the Uniform Scrambler Channel is 0 bits per channel use. This result makes intuitive sense: if the output is completely random and independent of the input, no information can be transmitted through the channel.", "answer": "$$\\boxed{0}$$", "id": "1661911"}, {"introduction": "After exploring a zero-capacity channel, let's turn to the cornerstone model of noisy communication: the Binary Symmetric Channel (BSC). In many practical scenarios, an engineer might measure a channel's performance (its capacity) and need to deduce its underlying physical characteristics.\n\nThis problem [@problem_id:1661933] inverts the standard calculation, challenging you to determine the channel's crossover probability $p$ from a given capacity. This exercise enhances problem-solving skills and deepens your understanding of the capacity formula for a BSC, $C = 1 - H_2(p)$, by highlighting the crucial, symmetric properties of the binary entropy function $H_2(p)$.", "problem": "An engineer is characterizing a new type of non-volatile memory. The storage process is inherently noisy, meaning that a bit that is intended to be stored as a '0' or a '1' might be read back incorrectly. This memory system can be modeled as a Binary Symmetric Channel (BSC). A BSC has a binary input alphabet, $\\{0, 1\\}$, a binary output alphabet, $\\{0, 1\\}$, and a single parameter $p$ called the crossover probability. The crossover probability $p$ is the probability that a stored bit is flipped upon being read, i.e., a '0' is read as a '1' or a '1' is read as a '0'. The probability that a bit is read correctly is therefore $1-p$.\n\nThe theoretical maximum rate at which information can be reliably stored and retrieved from this memory is given by its channel capacity, $C$. Through experimental measurements, the engineer determines that the capacity of this memory system is precisely $C = 0.5$ bits per symbol.\n\nGiven the options below, determine all possible values for the crossover probability $p$ that are consistent with this measured capacity. The values in the options are rounded to two decimal places. Select all that apply.\n\nA. $0.11$\n\nB. $0.25$\n\nC. $0.50$\n\nD. $0.75$\n\nE. $0.89$", "solution": "For a Binary Symmetric Channel with crossover probability $p$, the capacity (in bits per channel use) is\n$$\nC=1-H_{2}(p),\n$$\nwhere the binary entropy function is\n$$\nH_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nThe measured capacity is $C=\\frac{1}{2}$, so $H_{2}(p)=\\frac{1}{2}$. The function $H_{2}(p)$ satisfies two key properties:\n1) Symmetry: $H_{2}(p)=H_{2}(1-p)$, which follows directly from the definition.\n2) Monotonicity on $[0,\\frac{1}{2}]$: Differentiating,\n$$\n\\frac{d}{dp}H_{2}(p)=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right),\n$$\nwhich is positive for $0<p<\\frac{1}{2}$. Hence, there is a unique $p^{\\star}\\in(0,\\frac{1}{2})$ such that $H_{2}(p^{\\star})=\\frac{1}{2}$, and by symmetry $1-p^{\\star}$ is the other solution in $(\\frac{1}{2},1)$.\n\nTo identify the options that round to two decimals, evaluate $H_{2}(0.11)$:\n$$\nH_{2}(0.11)=-0.11\\log_{2}(0.11)-0.89\\log_{2}(0.89).\n$$\nUsing $\\log_{2}(x)=\\frac{\\ln x}{\\ln 2}$ with numerical approximations,\n$$\n\\log_{2}(0.11)=\\frac{\\ln(0.11)}{\\ln 2}\\approx\\frac{-2.207275}{0.693147}\\approx -3.183,\\quad\n\\log_{2}(0.89)=\\frac{\\ln(0.89)}{\\ln 2}\\approx\\frac{-0.116533}{0.693147}\\approx -0.1681,\n$$\nso\n$$\nH_{2}(0.11)\\approx -0.11(-3.183)-0.89(-0.1681)\\approx 0.350+0.1496\\approx 0.4996\\approx \\frac{1}{2}.\n$$\nThus $p^{\\star}\\approx 0.11$ to two decimals, and by symmetry $1-p^{\\star}\\approx 0.89$.\n\nNow exclude the other options:\n- $p=0.50$ gives $H_{2}(0.50)=1$, so $C=0$, not $\\frac{1}{2}$.\n- For $p=0.25$ with $0.11<p<0.5$ and $H_{2}$ increasing on $[0,\\frac{1}{2}]$, we have $H_{2}(0.25)>H_{2}(0.11)=\\frac{1}{2}$, so $C<\\frac{1}{2}$. By symmetry, $H_{2}(0.75)=H_{2}(0.25)>\\frac{1}{2}$, so $C<\\frac{1}{2}$ there as well.\n\nTherefore, the only options consistent with $C=\\frac{1}{2}$ are $p\\approx 0.11$ and $p\\approx 0.89$, i.e., A and E.", "answer": "$$\\boxed{AE}$$", "id": "1661933"}, {"introduction": "Our understanding of capacity is not limited to binary systems. This final practice expands our scope to channels with multi-symbol alphabets, reflecting the complexity of many modern communication and data monitoring systems.\n\nBy modeling a three-state system as a Ternary Symmetric Channel, you will move beyond the specific BSC formula and apply the general capacity equation for symmetric channels: $C=\\log_{2}(q) - H_{\\text{row}}$. This exercise [@problem_id:1661889] solidifies your ability to generalize information-theoretic principles, ensuring you can calculate the capacity for a wide range of symmetric communication channels.", "problem": "A remote monitoring system uses a simple protocol to transmit the status of a manufacturing process. The system can be in one of three states: 'Stable' (S), 'Alert' (A), or 'Failure' (F). The system sends a corresponding signal from the input alphabet $\\mathcal{X} = \\{S, A, F\\}$ to a central controller.\n\nThe communication channel is known to be noisy. When a signal is transmitted, there is a probability of correct transmission of $p=0.9$. If an error occurs (with probability $1-p$), the transmitted signal is equally likely to be received as either of the two incorrect signals. For example, if 'S' is sent, it is received as 'S' with probability 0.9, but it might be incorrectly received as 'A' or 'F'. This symmetric error pattern holds for all three input signals. The output alphabet at the controller is $\\mathcal{Y} = \\{S, A, F\\}$.\n\nDetermine the channel capacity of this system. Express your answer in bits per channel use, rounded to three significant figures. All logarithms are to be interpreted as base 2.", "solution": "We model the system as a discrete memoryless channel with input alphabet size $q=3$ and transition probabilities\n$$\nP_{Y|X}(y|x)=\n\\begin{cases}\np, & y=x,\\\\\n\\frac{1-p}{q-1}, & y\\neq x,\n\\end{cases}\n$$\nwith $p=0.9$. This is a $q$-ary symmetric channel.\n\nFor any input distribution, the mutual information is $I(X;Y)=H(Y)-H(Y|X)$. Because the channel is symmetric, $H(Y|X=x)$ is the same for all $x$, so\n$$\nH(Y|X)=H_{\\text{row}}=-p\\log_{2}p-(q-1)\\left(\\frac{1-p}{q-1}\\right)\\log_{2}\\left(\\frac{1-p}{q-1}\\right)\n=-p\\log_{2}p-(1-p)\\log_{2}\\left(\\frac{1-p}{q-1}\\right).\n$$\nFor symmetric channels, the capacity is achieved by the uniform input distribution. With $X$ uniform on $q$ symbols, the output is also uniform:\n$$\nP_{Y}(y)=\\sum_{x}\\frac{1}{q}P_{Y|X}(y|x)=\\frac{1}{q}\\left[p+(q-1)\\frac{1-p}{q-1}\\right]=\\frac{1}{q},\n$$\nso $H(Y)=\\log_{2}q$. Therefore the channel capacity is\n$$\nC=\\log_{2}q-H_{\\text{row}}=\\log_{2}q+p\\log_{2}p+(1-p)\\log_{2}\\left(\\frac{1-p}{q-1}\\right).\n$$\n\nSubstitute $q=3$ and $p=0.9$:\n$$\nC=\\log_{2}3+0.9\\log_{2}0.9+0.1\\log_{2}\\left(\\frac{0.1}{2}\\right)\n=\\log_{2}3+0.9\\log_{2}0.9+0.1\\log_{2}0.05.\n$$\nUsing base-2 logarithms,\n$$\n\\log_{2}3\\approx 1.5849625007,\\quad \\log_{2}0.9\\approx -0.1520030934,\\quad \\log_{2}0.05\\approx -4.3219280949,\n$$\nso\n$$\nC\\approx 1.5849625007+0.9(-0.1520030934)+0.1(-4.3219280949)\\approx 1.0159669071.\n$$\nRounded to three significant figures, the capacity is $1.02$ bits per channel use.", "answer": "$$\\boxed{1.02}$$", "id": "1661889"}]}