{"hands_on_practices": [{"introduction": "Our journey into calculating channel capacity begins with the most straightforward case: a deterministic channel. In such a channel, the output is a perfectly predictable function of the input, with no random noise. This exercise [@problem_id:1609635] explores a fundamental principle: for a deterministic channel, the capacity is found by maximizing the entropy, or 'surprise,' of the output. Your task is to find an input distribution that makes the output as unpredictable as possible, thereby unlocking the channel's full potential.", "problem": "A specialized digital communication system is designed to transmit information about a single decimal digit. The input to this channel is a random variable $X$ that can take any integer value from the set $\\mathcal{X} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$. The channel deterministically maps the input digit $X$ to a binary output $Y \\in \\{0, 1\\}$. The mapping rule is as follows: if the input digit $X$ is an even number, the output is $Y=0$; if the input digit $X$ is an odd number, the output is $Y=1$. Calculate the capacity of this communication channel. Express your final answer in bits.", "solution": "The channel deterministically maps each input digit $X \\in \\{0,1,\\dots,9\\}$ to its parity: $Y=0$ for even $X$ and $Y=1$ for odd $X$. The capacity of a discrete memoryless channel is defined as\n$$\nC=\\max_{p(x)} I(X;Y).\n$$\nFor a deterministic channel $Y=f(X)$, the conditional entropy satisfies $H(Y|X)=0$, hence\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nTherefore,\n$$\nC=\\max_{p(x)} H(Y).\n$$\nLet $p \\triangleq \\Pr(Y=1)$ denote the total input probability mass assigned to the odd digits. Because we can choose the input distribution arbitrarily over the ten digits, any $p \\in [0,1]$ is achievable by allocating the total probability mass across the five odd and five even digits accordingly. Thus $Y$ is a Bernoulli random variable with parameter $p$, and\n$$\nH(Y)=-p \\log_{2}(p) - (1-p) \\log_{2}(1-p).\n$$\nTo maximize $H(Y)$ over $p \\in [0,1]$, differentiate:\n$$\n\\frac{d}{dp}H(Y)=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right).\n$$\nSetting the derivative to zero gives $\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=0$, hence $\\frac{1-p}{p}=1$ and $p=\\frac{1}{2}$. The second derivative is\n$$\n\\frac{d^{2}}{dp^{2}}H(Y)=-\\frac{1}{\\ln(2)}\\left(\\frac{1}{p}+\\frac{1}{1-p}\\right)<0 \\quad \\text{for } p \\in (0,1),\n$$\nso $p=\\frac{1}{2}$ is the maximizer. Evaluating at $p=\\frac{1}{2}$,\n$$\nH(Y)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\nThus the channel capacity is $1$ bit.", "answer": "$$\\boxed{1}$$", "id": "1609635"}, {"introduction": "Now, let's introduce noise, but with a twist. This next exercise [@problem_id:1609620] presents a hypothetical scenario involving a binary channel where errors occur, but the receiver has 'side information'—it knows exactly what noise value affected each transmission. This thought experiment is crucial for understanding that channel capacity depends on the mutual information between the input and everything the receiver observes. By working through this problem, you will see how perfect knowledge about the noise can render it harmless, transforming a seemingly flawed channel into a perfect one.", "problem": "Consider a binary communication channel where the input alphabet is $\\mathcal{X} = \\{0, 1\\}$ and the output alphabet is $\\mathcal{Y} = \\{0, 1\\}$. The channel is affected by additive binary noise. Let the input selected by the transmitter be the random variable $X$, and the noise be the random variable $Z$. Both $X$ and $Z$ can take values in $\\{0, 1\\}$. The channel output, $Y$, is given by the relation $Y = X \\oplus Z$, where $\\oplus$ denotes addition modulo 2 (the XOR operation).\n\nThe noise variable $Z$ is statistically independent of the input $X$ and follows a Bernoulli distribution with $P(Z=1) = q$, where $q$ is a constant such that $0 < q < 1$.\n\nA special feature of this communication setup is that the receiver has perfect side information about the noise. This means that for each use of the channel, the receiver observes not only the output $Y$ but also the exact value of the noise realization $Z$. The transmitter, however, has no knowledge of $Z$.\n\nCalculate the capacity $C$ of this channel with side information at the receiver. Express your answer in units of bits per channel use.", "solution": "The capacity of a channel is defined as the maximum mutual information between the input and the output, maximized over all possible input distributions. In this specific problem, the receiver observes both the channel output $Y$ and the noise value $Z$. Therefore, the total information available to the receiver is the pair of random variables $(Y, Z)$. The channel capacity, $C$, is thus given by the maximum mutual information between the input $X$ and the pair $(Y, Z)$.\n\n$$C = \\max_{p(x)} I(X; Y, Z)$$\n\nWe can expand the mutual information term $I(X; Y, Z)$ using the chain rule for mutual information:\n\n$$I(X; Y, Z) = I(X; Z) + I(X; Y | Z)$$\n\nThe problem statement specifies that the input $X$ and the noise $Z$ are statistically independent. The mutual information between two independent random variables is zero. Therefore:\n\n$$I(X; Z) = 0$$\n\nSubstituting this into our expression for mutual information, we get:\n\n$$I(X; Y, Z) = 0 + I(X; Y | Z) = I(X; Y | Z)$$\n\nNow let's analyze the conditional mutual information term, $I(X; Y | Z)$. We can express this in terms of conditional entropies:\n\n$$I(X; Y | Z) = H(X | Z) - H(X | Y, Z)$$\n\nBecause $X$ and $Z$ are independent, knowing the value of $Z$ provides no information about $X$. Thus, the conditional entropy $H(X | Z)$ is equal to the entropy of $X$:\n\n$$H(X | Z) = H(X)$$\n\nNext, we evaluate the term $H(X | Y, Z)$. This represents the uncertainty remaining about the input $X$ after the receiver has observed both the output $Y$ and the noise $Z$. The channel relationship is given as $Y = X \\oplus Z$. Using the properties of the XOR operation, we can solve for $X$:\n\n$$X = Y \\oplus Z$$\n\nSince the receiver knows the exact values of both $Y$ and $Z$ for each transmission, it can compute $X$ with perfect certainty. There is no uncertainty remaining about $X$ once $Y$ and $Z$ are known. Therefore, the conditional entropy $H(X | Y, Z)$ is zero:\n\n$$H(X | Y, Z) = 0$$\n\nSubstituting these entropy results back into the expression for mutual information:\n\n$$I(X; Y, Z) = I(X; Y | Z) = H(X|Z) - H(X|Y,Z) = H(X) - 0 = H(X)$$\n\nSo, the mutual information for any given input distribution $p(x)$ is simply the entropy of the input, $H(X)$. To find the channel capacity $C$, we must maximize this mutual information over all possible input distributions.\n\n$$C = \\max_{p(x)} H(X)$$\n\nThe input $X$ is a binary random variable. The entropy of a binary random variable is maximized when its outcomes are equiprobable, i.e., $P(X=0) = P(X=1) = 1/2$. The maximum value of the entropy is:\n\n$$H_{\\text{max}}(X) = -P(X=0)\\log_2(P(X=0)) - P(X=1)\\log_2(P(X=1))$$\n$$H_{\\text{max}}(X) = -\\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) = - (1)\\log_2\\left(\\frac{1}{2}\\right) = -(-\\log_2(2)) = \\log_2(2) = 1 \\text{ bit}$$\n\nTherefore, the capacity of this channel is 1 bit per channel use. It is noteworthy that the capacity is independent of the noise probability $q$, a direct consequence of the receiver having perfect side information about the noise.", "answer": "$$\\boxed{1}$$", "id": "1609620"}, {"introduction": "Finally, we tackle a problem that reflects the challenges of real-world system design. Communication systems often impose constraints on the input signals to prevent errors or for hardware reasons. This practice [@problem_id:1609670] asks you to find the maximum reliable communication rate over a standard noisy channel—the Binary Symmetric Channel—when the input is not allowed to have consecutive '1's. This advanced problem requires you to synthesize your knowledge of channel properties with the characteristics of the input source, demonstrating how capacity is a delicate interplay between what the channel can handle and what we are allowed to send.", "problem": "A communication system is designed to transmit data over a binary symmetric channel (BSC). The channel has a crossover probability $p$, with $0 < p < 1/2$, meaning the probability of a bit being flipped during transmission is $p$. To mitigate the effects of a specific type of burst error, the system employs a source coding scheme that constrains the input bit stream, denoted by $\\{X_t\\}$, such that no two consecutive '1's can be transmitted.\n\nThis constraint is enforced by generating the input sequence from a stationary first-order Markov source. The state of the source at time $t$ is the value of the previous bit, $X_{t-1}$. The state transitions are governed by the probabilities $P(X_t=0|X_{t-1}=1) = 1$ (enforcing the constraint) and a tunable parameter $\\alpha = P(X_t=1|X_{t-1}=0)$, where $\\alpha$ can be set to any value in the interval $[0, 1]$.\n\nLet $X$ and $Y$ be the random variables for the input and output bits, respectively, once the system has reached its stationary state. Your task is to find the maximum possible value of the mutual information, $I(X;Y)$, that can be achieved by selecting the optimal value of the parameter $\\alpha$.\n\nExpress your final answer in terms of the binary entropy function, $H_b(p) = -p\\log_2(p) - (1-p)\\log_2(1-p)$.", "solution": "The channel is a binary symmetric channel with crossover probability $p$, so for any input bit $X \\in \\{0,1\\}$ the output is $Y = X \\oplus Z$ where $Z \\sim \\text{Bernoulli}(p)$ and is independent of $X$. Therefore, the conditional entropy satisfies\n$$\nH(Y|X) = H_b(p).\n$$\n\nThe input $\\{X_{t}\\}$ is generated by a first-order Markov source with transition probabilities\n$$\nP(X_{t}=1 \\mid X_{t-1}=1) = 0,\\quad P(X_{t}=0 \\mid X_{t-1}=1) = 1,\\quad P(X_{t}=1 \\mid X_{t-1}=0) = \\alpha,\\quad P(X_{t}=0 \\mid X_{t-1}=0) = 1-\\alpha,\n$$\nwhere $\\alpha \\in [0,1]$. Let the stationary distribution be $\\pi_{0} = P(X=0)$ and $\\pi_{1} = P(X=1)$. Stationarity requires\n$$\n\\begin{aligned}\n\\pi_{0} &= \\pi_{0}(1-\\alpha) + \\pi_{1}(1),\\\\\n\\pi_{1} &= \\pi_{0}\\alpha + \\pi_{1}(0),\\\\\n\\pi_{0} + \\pi_{1} &= 1.\n\\end{aligned}\n$$\nFrom $\\pi_{1} = \\pi_{0}\\alpha$ and $\\pi_{0} + \\pi_{0}\\alpha = 1$, we obtain\n$$\n\\pi_{0} = \\frac{1}{1+\\alpha},\\qquad \\pi_{1} = \\frac{\\alpha}{1+\\alpha}.\n$$\n\nFor the BSC, the output marginal is\n$$\nP(Y=1) = P(Y=1 \\mid X=1)\\pi_{1} + P(Y=1 \\mid X=0)\\pi_{0} = (1-p)\\pi_{1} + p\\pi_{0} = p + (1-2p)\\pi_{1}.\n$$\nThus\n$$\nH(Y) = H_b\\big(p + (1-2p)\\pi_{1}\\big) = H_b\\left(p + (1-2p)\\frac{\\alpha}{1+\\alpha}\\right).\n$$\nThe single-letter mutual information is\n$$\nI(X;Y) = H(Y) - H(Y|X) = H_b\\left(p + (1-2p)\\frac{\\alpha}{1+\\alpha}\\right) - H_b(p).\n$$\n\nSince $0 < p < \\frac{1}{2}$, we have $1-2p > 0$. As $\\alpha$ ranges over $[0,1]$, $\\pi_{1} = \\frac{\\alpha}{1+\\alpha}$ ranges over $[0,\\frac{1}{2}]$, so $P(Y=1) = p + (1-2p)\\pi_{1}$ ranges over $[p,\\frac{1}{2}]$. The binary entropy function $H_b(q)$ is strictly increasing on $q \\in [0,\\frac{1}{2}]$, hence $H(Y)$, and therefore $I(X;Y)$, is maximized by maximizing $P(Y=1)$ within its feasible range, i.e., by choosing $P(Y=1) = \\frac{1}{2}$. This is achieved when $\\pi_{1} = \\frac{1}{2}$, which corresponds to $\\alpha = 1$ because\n$$\n\\frac{\\alpha}{1+\\alpha} = \\frac{1}{2} \\quad \\Longleftrightarrow \\quad \\alpha = 1.\n$$\nAt this choice, $H(Y) = H_b\\left(\\frac{1}{2}\\right) = 1$, so the maximum mutual information is\n$$\nI_{\\max}(X;Y) = 1 - H_b(p).\n$$\nThis choice $\\alpha=1$ respects the no-two-consecutive-ones constraint (the input alternates between $0$ and $1$), and the stationary single-letter marginal satisfies $P(X=1)=\\frac{1}{2}$.\n\nTherefore, the maximum achievable $I(X;Y)$ over $\\alpha \\in [0,1]$ is $1 - H_b(p)$.", "answer": "$$\\boxed{1 - H_{b}(p)}$$", "id": "1609670"}]}