{"hands_on_practices": [{"introduction": "We begin our practice with the most fundamental type of channel: a deterministic one. In such channels, the output $Y$ is completely determined by the input $X$, meaning the conditional entropy $H(Y|X)$ is zero. This simplifies the mutual information to just the entropy of the output, $I(X;Y) = H(Y)$. This exercise challenges you to find the capacity of a simple digital processing unit by strategically choosing an input distribution that maximizes the entropy of its output symbols. [@problem_id:1607525]", "problem": "Consider a specialized digital processing unit that accepts a pair of binary inputs, $(X_1, X_2)$, where each $X_i$ can be either 0 or 1. The unit is designed to compute the integer sum of these two bits, producing a single output $Y = X_1 + X_2$. This process defines a discrete memoryless channel, where a 'channel use' corresponds to the transmission of one input pair $(X_1, X_2)$ and the reception of the corresponding sum $Y$. The set of possible input symbols is the set of all possible pairs $(X_1, X_2)$.\n\nDetermine the capacity of this channel. Express your answer as an analytical expression using a base-2 logarithm.", "solution": "Let the input random variable be $X=(X_{1},X_{2})$ taking values in $\\{(0,0),(0,1),(1,0),(1,1)\\}$ and the output be $Y=X_{1}+X_{2}$ taking values in $\\{0,1,2\\}$. The channel is deterministic with mapping\n$$(0,0)\\mapsto 0,\\quad (0,1)\\mapsto 1,\\quad (1,0)\\mapsto 1,\\quad (1,1)\\mapsto 2.$$\n\nBy the capacity definition for a discrete memoryless channel,\n$$C=\\max_{p(x)} I(X;Y).$$\nSince the channel is deterministic, $H(Y\\mid X)=0$, hence\n$$I(X;Y)=H(Y)-H(Y\\mid X)=H(Y).$$\nTherefore,\n$$C=\\max_{p(x)} H(Y),$$\nwhere the distribution of $Y$ is induced by $p(x)$ via\n$$p(y)=\\sum_{x: f(x)=y} p(x),$$\nwith $f(x)=X_{1}+X_{2}$. As each output value has a nonempty preimage, we can realize any distribution on $\\{0,1,2\\}$ by choosing $p(x)$ appropriately. In particular, to maximize $H(Y)$ we choose $Y$ to be uniform:\n$$p(0)=p(1)=p(2)=\\frac{1}{3}.$$\nOne feasible choice is\n$$p(0,0)=\\frac{1}{3},\\quad p(1,1)=\\frac{1}{3},\\quad p(0,1)=\\frac{1}{6},\\quad p(1,0)=\\frac{1}{6},$$\nwhich induces $p(Y=0)=\\frac{1}{3}$, $p(Y=1)=\\frac{1}{3}$, and $p(Y=2)=\\frac{1}{3}$. The corresponding output entropy is\n$$H(Y)=-\\sum_{y\\in\\{0,1,2\\}} p(y)\\log_{2}(p(y))=\\log_{2}(3).$$\nThus the channel capacity is\n$$C=\\log_{2}(3)\\ \\text{bits per channel use}.$$", "answer": "$$\\boxed{\\log_{2}(3)}$$", "id": "1607525"}, {"introduction": "Next, we introduce stochasticity by exploring a channel with a symmetric error structure. Symmetric channels are a cornerstone of information theory because their inherent balance often allows for a significant simplification in calculating capacity; typically, a uniform input distribution is optimal. This practice [@problem_id:1607507] provides an excellent opportunity to identify a \"weakly symmetric\" channel and apply the standard formula $C = \\log_{2}|\\mathcal{Y}| - H(\\text{row})$, reinforcing a powerful and efficient problem-solving technique.", "problem": "Consider a discrete memoryless communication channel with an input alphabet $\\mathcal{X} = \\{A, B, C\\}$ and an output alphabet $\\mathcal{Y} = \\{a, b, c\\}$. The behavior of this channel is characterized by a parameter $p$, where $0 \\le p \\le 1$. The channel's transition probabilities are defined as follows:\n\n- A symbol is transmitted correctly with probability $1-p$. For example, if $A$ is sent, $a$ is received.\n- A symbol is transmitted incorrectly with probability $p$, resulting in a specific \"cyclic\" error. If $A$ is sent, $b$ is received; if $B$ is sent, $c$ is received; and if $C$ is sent, $a$ is received.\n\nMore formally, the conditional probabilities are:\n- $P(Y=a | X=A) = 1-p$\n- $P(Y=b | X=B) = 1-p$\n- $P(Y=c | X=C) = 1-p$\n- $P(Y=b | X=A) = p$\n- $P(Y=c | X=B) = p$\n- $P(Y=a | X=C) = p$\n\nAll other conditional probabilities of the form $P(Y=y | X=x)$ are zero.\n\nDetermine the capacity of this channel. Express your final answer as an analytic expression in terms of the parameter $p$. All logarithms used in the calculation and the final answer must be base 2.", "solution": "Let the input random variable be $X \\in \\{A,B,C\\}$ and the output be $Y \\in \\{a,b,c\\}$. The channel transition probabilities are\n$$\nW(y|x)=\n\\begin{cases}\n1-p & \\text{if } y \\text{ is the correct output for } x,\\\\\np & \\text{if } y \\text{ is the cyclic error output for } x,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nEquivalently, the three rows $W(\\cdot|A)$, $W(\\cdot|B)$, and $W(\\cdot|C)$ are permutations of the multiset $\\{1-p,p,0\\}$. The column sums are\n$$\n\\sum_{x} W(a|x)=(1-p)+0+p=1,\\quad \\sum_{x} W(b|x)=p+(1-p)+0=1,\\quad \\sum_{x} W(c|x)=0+p+(1-p)=1,\n$$\nso all column sums are equal. Therefore, the channel is weakly symmetric.\n\nBy the standard result for weakly symmetric discrete memoryless channels, the capacity is achieved by the uniform input distribution $P_{X}(x)=\\frac{1}{3}$ for all $x \\in \\{A,B,C\\}$, and the capacity equals\n$$\nC=\\log_{2}|\\mathcal{Y}|-H\\bigl(W(\\cdot|x)\\bigr),\n$$\nwhere $H\\bigl(W(\\cdot|x)\\bigr)$ is the entropy (in bits) of any row of the transition matrix (all rows being permutations of each other have the same entropy).\n\nFirst compute $H(Y|X)$. For any fixed input symbol $x$, the conditional distribution $W(\\cdot|x)$ has probabilities $\\{1-p,p,0\\}$, so\n$$\nH(Y|X=x)=-(1-p)\\log_{2}(1-p)-p\\log_{2}p-0\\cdot\\log_{2}0=-(1-p)\\log_{2}(1-p)-p\\log_{2}p.\n$$\nSince $H(Y|X)$ does not depend on $x$, we have\n$$\nH(Y|X)=-(1-p)\\log_{2}(1-p)-p\\log_{2}p.\n$$\n\nUnder the uniform input $P_{X}(x)=\\frac{1}{3}$, the output distribution is uniform because each column sum equals $1$:\n$$\nP_{Y}(y)=\\sum_{x} P_{X}(x)W(y|x)=\\frac{1}{3}\\sum_{x}W(y|x)=\\frac{1}{3},\\quad \\forall y\\in\\{a,b,c\\},\n$$\nhence\n$$\nH(Y)=\\log_{2}3.\n$$\n\nThe mutual information under the capacity-achieving input is\n$$\nI(X;Y)=H(Y)-H(Y|X)=\\log_{2}3+\\;p\\log_{2}p+(1-p)\\log_{2}(1-p).\n$$\nBy weak symmetry, this value is the channel capacity. Therefore,\n$$\nC(p)=\\log_{2}3+\\;p\\log_{2}p+(1-p)\\log_{2}(1-p).\n$$", "answer": "$$\\boxed{\\log_{2}3+p\\log_{2}p+(1-p)\\log_{2}(1-p)}$$", "id": "1607507"}, {"introduction": "Finally, we tackle a more complex and realistic scenario involving an asymmetric channel, where the noise characteristics depend on the specific input symbol. For these channels, we cannot rely on simple symmetry arguments and must return to the fundamental definition of capacity: maximizing the mutual information $I(X;Y)$ over all possible input distributions. This problem [@problem_id:1607552] features a clever construction combining different noise models, where a full analysis reveals a surprisingly simple and elegant expression for capacity.", "problem": "Consider a discrete memoryless channel with a binary input alphabet $X = \\{0, 1\\}$ and a ternary output alphabet $Y = \\{0, 1, E\\}$, where 'E' represents an erasure. The channel's behavior is asymmetric and depends on the input symbol.\n\nThe transition probabilities are defined as follows:\n- When the input is $x=0$, the output is corrupted as if by a Z-channel. The output is $y=1$ with a crossover probability $f$, and $y=0$ with probability $1-f$. The output is never an erasure.\n- When the input is $x=1$, the output is affected by erasures. The output is an erasure $y=E$ with probability $\\epsilon$. Otherwise, the output is $y=1$ with probability $1-\\epsilon$. The output is never $y=0$.\n\nThe noise parameters $f$ and $\\epsilon$ are real numbers in the range $(0, 1)$. Furthermore, these parameters are linked by the relation $f = 1 - \\epsilon$.\n\nDetermine the capacity of this channel. Express your answer as an analytic expression in terms of $\\epsilon$. The capacity should be given in bits per channel use.", "solution": "Let $X \\in \\{0,1\\}$ with $P(X=1)=p$ and $P(X=0)=1-p$. The channel transition law is\n- If $x=0$: $P(Y=1|X=0)=f$, $P(Y=0|X=0)=1-f$, $P(Y=E|X=0)=0$.\n- If $x=1$: $P(Y=E|X=1)=\\epsilon$, $P(Y=1|X=1)=1-\\epsilon$, $P(Y=0|X=1)=0$,\nwith the constraint $f=1-\\epsilon$. Hence\n$$\nP(Y=1|X=0)=1-\\epsilon,\\quad P(Y=0|X=0)=\\epsilon,\\quad P(Y=E|X=0)=0,\n$$\n$$\nP(Y=1|X=1)=1-\\epsilon,\\quad P(Y=E|X=1)=\\epsilon,\\quad P(Y=0|X=1)=0.\n$$\nThe output distribution for a given input prior $p$ is\n$$\nP(Y=1)=(1-\\epsilon)p+(1-\\epsilon)(1-p)=1-\\epsilon,\n$$\n$$\nP(Y=E)=\\epsilon p,\\qquad P(Y=0)=\\epsilon(1-p).\n$$\nBy Bayes' rule, the posterior satisfies\n$$\nP(X=1|Y=E)=1,\\quad P(X=0|Y=0)=1,\\quad P(X=1|Y=1)=p,\\quad P(X=0|Y=1)=1-p,\n$$\nso $Y=E$ or $Y=0$ reveals $X$ perfectly, while $Y=1$ yields no information about $X$ beyond the prior.\n\nThe mutual information is\n$$\nI(X;Y)=H(X)-H(X|Y),\n$$\nwhere $H(\\cdot)$ denotes entropy in bits. Let $H_{\\mathrm{b}}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$ be the binary entropy function. Then $H(X)=H_{\\mathrm{b}}(p)$ and\n$$\nH(X|Y)=P(Y=1)H(X|Y=1)+P(Y=E)H(X|Y=E)+P(Y=0)H(X|Y=0)=(1-\\epsilon)H_{\\mathrm{b}}(p).\n$$\nTherefore\n$$\nI(X;Y)=H_{\\mathrm{b}}(p)-(1-\\epsilon)H_{\\mathrm{b}}(p)=\\epsilon\\,H_{\\mathrm{b}}(p).\n$$\nThe channel capacity is the maximum of $I(X;Y)$ over $p \\in [0,1]$:\n$$\nC=\\max_{p}\\,\\epsilon\\,H_{\\mathrm{b}}(p)=\\epsilon\\max_{p}H_{\\mathrm{b}}(p).\n$$\nSince\n$$\n\\frac{d}{dp}H_{\\mathrm{b}}(p)=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right),\\qquad \\frac{d^{2}}{dp^{2}}H_{\\mathrm{b}}(p)=-\\frac{1}{\\ln 2}\\frac{1}{p(1-p)}<0,\n$$\nthe maximum occurs at $p=\\frac{1}{2}$ with $H_{\\mathrm{b}}\\!\\left(\\frac{1}{2}\\right)=1$. Hence\n$$\nC=\\epsilon.\n$$\nAll logarithms are base 2, so the capacity is in bits per channel use.", "answer": "$$\\boxed{\\epsilon}$$", "id": "1607552"}]}