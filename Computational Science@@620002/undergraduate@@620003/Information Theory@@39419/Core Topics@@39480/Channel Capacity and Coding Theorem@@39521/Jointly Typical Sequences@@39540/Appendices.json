{"hands_on_practices": [{"introduction": "The concept of joint typicality is central to understanding how correlated data sequences behave. This first exercise provides a concrete, step-by-step application of the definition of an $\\epsilon$-jointly typical sequence. By working through the calculations for a short sequence pair, you will solidify your understanding of the three core conditions involving marginal and joint entropies, which form the bedrock of the Asymptotic Equipartition Property (AEP) [@problem_id:1635549].", "problem": "In a simplified model of a wireless sensor network, a sensor node observes two correlated binary events, represented by random variables $X$ and $Y$, where $X, Y \\in \\{0, 1\\}$. The events are generated independently and identically distributed (i.i.d.) at each time step according to a fixed joint probability mass function (PMF), $p(x, y)$. The joint PMF is given by:\n$p(0, 0) = \\frac{1}{2}$\n$p(0, 1) = \\frac{1}{4}$\n$p(1, 0) = \\frac{1}{8}$\n$p(1, 1) = \\frac{1}{8}$\n\nA pair of sequences of length $n$, $(x^n, y^n) = ((x_1, \\dots, x_n), (y_1, \\dots, y_n))$, is said to be $\\epsilon$-jointly typical with respect to $p(x,y)$ if the following three conditions, based on the Shannon entropies $H(X)$, $H(Y)$, and $H(X,Y)$, are all satisfied (using logarithm base 2):\n1. $|-\\frac{1}{n} \\log_{2} p(x^n) - H(X)| \\le \\epsilon$\n2. $|-\\frac{1}{n} \\log_{2} p(y^n) - H(Y)| \\le \\epsilon$\n3. $|-\\frac{1}{n} \\log_{2} p(x^n, y^n) - H(X,Y)| \\le \\epsilon$\n\nHere, $p(x^n) = \\prod_{i=1}^{n} p(x_i)$, $p(y^n) = \\prod_{i=1}^{n} p(y_i)$, and $p(x^n, y^n) = \\prod_{i=1}^{n} p(x_i, y_i)$.\n\nConsider the specific pair of sequences of length $n=4$:\n$x^4 = (0, 1, 0, 0)$\n$y^4 = (0, 0, 1, 0)$\n\nGiven $\\epsilon = 0.3$, determine if this specific pair of sequences is $\\epsilon$-jointly typical for the given PMF. Select the most accurate statement from the options below.\n\nA. Yes, the sequence pair is jointly typical.\n\nB. No, the sequence pair is not jointly typical because the condition based on the marginal entropy $H(X)$ is not satisfied.\n\nC. No, the sequence pair is not jointly typical because the condition based on the marginal entropy $H(Y)$ is not satisfied.\n\nD. No, the sequence pair is not jointly typical because the condition based on the joint entropy $H(X,Y)$ is not satisfied.\n\nE. No, the sequence pair is not jointly typical because more than one of the required conditions are not satisfied.", "solution": "We are given the joint PMF\n$$\np(0,0)=\\frac{1}{2},\\quad p(0,1)=\\frac{1}{4},\\quad p(1,0)=\\frac{1}{8},\\quad p(1,1)=\\frac{1}{8},\n$$\nwhich yields marginals\n$$\np_{X}(0)=\\frac{3}{4},\\quad p_{X}(1)=\\frac{1}{4},\\quad p_{Y}(0)=\\frac{5}{8},\\quad p_{Y}(1)=\\frac{3}{8}.\n$$\nThe entropies (base 2) are\n$$\nH(X)=-\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right),\\quad\nH(Y)=-\\left(\\frac{5}{8}\\log_{2}\\frac{5}{8}+\\frac{3}{8}\\log_{2}\\frac{3}{8}\\right),\n$$\n$$\nH(X,Y)=-\\left(\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{4}\\log_{2}\\frac{1}{4}+\\frac{1}{8}\\log_{2}\\frac{1}{8}+\\frac{1}{8}\\log_{2}\\frac{1}{8}\\right).\n$$\nSince $\\log_{2}\\frac{1}{2}=-1$, $\\log_{2}\\frac{1}{4}=-2$, $\\log_{2}\\frac{1}{8}=-3$, we have\n$$\nH(X,Y)=\\frac{1}{2}\\cdot 1+\\frac{1}{4}\\cdot 2+\\frac{1}{8}\\cdot 3+\\frac{1}{8}\\cdot 3=\\frac{7}{4}.\n$$\n\nNow evaluate the three typicality conditions for $x^{4}=(0,1,0,0)$ and $y^{4}=(0,0,1,0)$ with $\\epsilon=0.3$.\n\n1) Marginal-$X$ condition:\n$$\np(x^{4})=p_{X}(0)^{3}p_{X}(1)^{1}=\\left(\\frac{3}{4}\\right)^{3}\\left(\\frac{1}{4}\\right),\n$$\nso\n$$\n-\\frac{1}{4}\\log_{2}p(x^{4})=-\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)=H(X).\n$$\nThus\n$$\n\\left|-\\frac{1}{4}\\log_{2}p(x^{4})-H(X)\\right|=0\\le\\epsilon.\n$$\n\n2) Marginal-$Y$ condition:\n$$\np(y^{4})=p_{Y}(0)^{3}p_{Y}(1)^{1}=\\left(\\frac{5}{8}\\right)^{3}\\left(\\frac{3}{8}\\right),\n$$\nso\n$$\n-\\frac{1}{4}\\log_{2}p(y^{4})=-\\left(\\frac{3}{4}\\log_{2}\\frac{5}{8}+\\frac{1}{4}\\log_{2}\\frac{3}{8}\\right).\n$$\nThen\n$$\n\\left|-\\frac{1}{4}\\log_{2}p(y^{4})-H(Y)\\right|\n=\\left|-\\left(\\frac{3}{4}a+\\frac{1}{4}b\\right)+\\left(\\frac{5}{8}a+\\frac{3}{8}b\\right)\\right|\n=\\left|\\frac{1}{8}a-\\frac{1}{8}b\\right|\n=\\frac{1}{8}\\left|\\log_{2}\\frac{5}{8}-\\log_{2}\\frac{3}{8}\\right|\n=\\frac{1}{8}\\log_{2}\\frac{5}{3},\n$$\nwhere $a=\\log_{2}\\frac{5}{8}$ and $b=\\log_{2}\\frac{3}{8}$. Since $\\frac{5}{3}<2$, we have $\\log_{2}\\frac{5}{3}<1$, hence\n$$\n\\frac{1}{8}\\log_{2}\\frac{5}{3}<\\frac{1}{8}=0.125<0.3=\\epsilon.\n$$\nThus the $Y$-marginal condition is satisfied.\n\n3) Joint condition:\nThe pairwise sequence probabilities are $p(0,0)=\\frac{1}{2}$ (positions 1 and 4), $p(1,0)=\\frac{1}{8}$ (position 2), and $p(0,1)=\\frac{1}{4}$ (position 3). Therefore\n$$\np(x^{4},y^{4})=\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{8}\\right)\\left(\\frac{1}{4}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{128},\n$$\nso\n$$\n-\\frac{1}{4}\\log_{2}p(x^{4},y^{4})=-\\frac{1}{4}\\log_{2}\\frac{1}{128}=\\frac{7}{4}=H(X,Y),\n$$\nand hence\n$$\n\\left|-\\frac{1}{4}\\log_{2}p(x^{4},y^{4})-H(X,Y)\\right|=0\\le\\epsilon.\n$$\n\nAll three conditions are satisfied with $\\epsilon=0.3$, so the sequence pair is $\\epsilon$-jointly typical. The most accurate choice is A.", "answer": "$$\\boxed{A}$$", "id": "1635549"}, {"introduction": "Beyond verifying if a single sequence is typical, we can use these ideas to understand the structure of communication channels. This practice explores the relationship between a transmitted sequence and the set of possible received sequences that are jointly typical with it in a Binary Symmetric Channel model [@problem_id:1635559]. The solution elegantly demonstrates how the number of these 'plausible' received sequences is directly related to the entropy of the channel noise, $H(\\alpha)$, providing a foundational insight into the principles of channel capacity.", "problem": "A deep-space probe sends a binary data stream to Earth. The data can be modeled as a sequence $X^n = (X_1, \\dots, X_n)$ of independent and identically distributed random variables from a Bernoulli source $X$ with $P(X=1) = p$ and $P(X=0)=1-p$. The transmission occurs over a noisy channel, which can be modeled as a memoryless Binary Symmetric Channel (BSC) with a crossover probability $\\alpha$, where $0 < \\alpha < 1$. The received sequence is $Y^n = (Y_1, \\dots, Y_n)$. This channel model implies that each received bit $Y_i$ is related to the transmitted bit $X_i$ by $Y_i = X_i \\oplus Z_i$, where $\\oplus$ denotes addition modulo 2, and $Z_i$ represents a noise bit from a Bernoulli process $Z$ with $P(Z=1) = \\alpha$. The noise process $Z$ is independent of the source process $X$.\n\nAccording to the Asymptotic Equipartition Property (AEP), for a sufficiently small $\\epsilon > 0$ and a very large sequence length $n$, the size of the set of typical source sequences, denoted by $|A_\\epsilon^{(n)}(X)|$, is well-approximated by $2^{n H(X)}$, where $H(X)$ is the Shannon entropy of the source. Similarly, the size of the set of jointly typical sequences, denoted by $|A_\\epsilon^{(n)}(X,Y)|$, is approximated by $2^{n H(X,Y)}$, where $H(X,Y)$ is the joint entropy.\n\nDetermine the approximate number of distinct received sequences $Y^n$ that can be considered jointly typical with any *single given* typical source sequence $X^n$. This quantity corresponds to the ratio $\\frac{|A_\\epsilon^{(n)}(X,Y)|}{|A_\\epsilon^{(n)}(X)|}$ in the large $n$ limit. Provide a closed-form analytic expression for this ratio. Express your answer in terms of $n$, the crossover probability $\\alpha$, and the binary entropy function $H(\\cdot)$, which is defined as $H(q) = -q \\log_2(q) - (1-q) \\log_2(1-q)$ for $q \\in (0,1)$.", "solution": "We model the i.i.d. Bernoulli source $X$ with $P(X=1)=p$ and the memoryless BSC with crossover probability $\\alpha$, so $Y=X\\oplus Z$, where $Z$ is independent of $X$ and $P(Z=1)=\\alpha$. For a small $\\epsilon>0$ and large $n$, the Asymptotic Equipartition Property implies the approximations\n$$\n|A_{\\epsilon}^{(n)}(X)| \\approx 2^{n H(X)}, \\quad |A_{\\epsilon}^{(n)}(X,Y)| \\approx 2^{n H(X,Y)}.\n$$\nThe number of distinct received sequences $Y^{n}$ that are jointly typical with any single given typical source sequence $X^{n}$ is well-approximated by the conditional typical set size, which is the ratio\n$$\n\\frac{|A_{\\epsilon}^{(n)}(X,Y)|}{|A_{\\epsilon}^{(n)}(X)|} \\approx 2^{n\\left[H(X,Y)-H(X)\\right]} = 2^{n H(Y|X)}.\n$$\nUsing the channel model $Y=X\\oplus Z$ with $Z$ independent of $X$, we evaluate $H(Y|X)$:\n$$\nH(Y|X) = \\sum_{x\\in\\{0,1\\}} P(X=x)\\, H(Y|X=x).\n$$\nFor each $x\\in\\{0,1\\}$, $Y|X=x$ is a Bernoulli random variable with $P(Y\\neq x)=\\alpha$ and $P(Y=x)=1-\\alpha$, so\n$$\nH(Y|X=x) = H(\\alpha),\n$$\nwhere $H(\\alpha)=-\\alpha \\log_{2}(\\alpha)-(1-\\alpha)\\log_{2}(1-\\alpha)$ is the binary entropy function. Hence\n$$\nH(Y|X) = H(\\alpha).\n$$\nTherefore, the desired ratio, which equals the approximate number of jointly typical $Y^{n}$ sequences per given typical $X^{n}$, is\n$$\n2^{n H(Y|X)} = 2^{n H(\\alpha)}.\n$$\nThis expression depends only on $n$ and the crossover probability $\\alpha$, as required.", "answer": "$$\\boxed{2^{n H(\\alpha)}}$$", "id": "1635559"}, {"introduction": "Jointly typical sequences are not just a loose collection; for a large sequence length $n$, they form a very 'thin shell' in the space of all possible sequences, closely adhering to the source statistics. This exercise challenges you to probe the boundaries of this typical set by introducing small, controlled modifications to a perfectly typical sequence [@problem_id:1635561]. By determining the maximum number of changes allowed before the sequence pair becomes non-typical, you will gain a deeper appreciation for the strict constraints that typicality imposes and why it is such a powerful concept for data compression and channel coding.", "problem": "Consider a discrete memoryless source that generates pairs of symbols $(X,Y)$ where $X, Y \\in \\{0, 1\\}$. The joint probability mass function $p(x,y)$ is given by the following table:\n\n| $p(x,y)$ | $y=0$ | $y=1$ |\n| :---: | :---: | :---: |\n| **x=0** | 0.5 | 0.2 |\n| **x=1** | 0.1 | 0.2 |\n\nA sequence pair $(x^n, y^n) = ((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n))$ of length $n=100$ is generated by this source. Let the number of occurrences of the pair $(i,j)$ in the sequence be denoted by $N(i,j)$. The specific sequence pair generated is known to be perfectly representative of the source statistics, meaning that the counts of each symbol pair are exactly $N(i,j) = n \\cdot p(i,j)$ for all $i,j \\in \\{0,1\\}$.\n\nA sequence pair is defined as being in the jointly typical set $A_{\\epsilon}^{(n)}(X,Y)$ for a given tolerance $\\epsilon > 0$ if all three of the following conditions are met (using logarithms in base 2):\n1.  $\\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\leq \\epsilon$\n2.  $\\left| -\\frac{1}{n} \\log_2 p(y^n) - H(Y) \\right| \\leq \\epsilon$\n3.  $\\left| -\\frac{1}{n} \\log_2 p(x^n, y^n) - H(X,Y) \\right| \\leq \\epsilon$\nwhere $p(x^n) = \\prod_{i=1}^{n} p_X(x_i)$, $p(y^n) = \\prod_{i=1}^{n} p_Y(y_i)$, $p(x^n, y^n) = \\prod_{i=1}^{n} p(x_i, y_i)$, and $H(X)$, $H(Y)$, and $H(X,Y)$ are the Shannon entropies of the respective distributions.\n\nA new sequence pair $(x'^n, y'^n)$ is created by modifying $(x^n, y^n)$. Specifically, $m$ instances of the symbol pair $(0,1)$ in the original sequence are swapped to become $(1,0)$. That is, for $m$ distinct indices $k$ where $(x_k, y_k) = (0,1)$, the new pair becomes $(x'_k, y'_k) = (1,0)$. All other symbols in the sequence remain unchanged.\n\nGiven a tolerance $\\epsilon = 0.04$, determine the maximum integer number of swaps, $m_{max}$, that can be performed such that the resulting sequence pair $(x'^n, y'^n)$ remains in the jointly typical set $A_{\\epsilon}^{(n)}(X,Y)$.", "solution": "The source has joint pmf\n$$p(0,0)=0.5,\\quad p(0,1)=0.2,\\quad p(1,0)=0.1,\\quad p(1,1)=0.2,$$\nso the source marginals are\n$$p_{X}(0)=0.7,\\quad p_{X}(1)=0.3,\\qquad p_{Y}(0)=0.6,\\quad p_{Y}(1)=0.4.$$\nThe initial length is $n=100$ with exact counts $N(i,j)=100\\,p(i,j)$.\n\nAfter swapping $m$ instances of $(0,1)$ to $(1,0)$, the new empirical joint type is\n$$q'_{00}=0.5,\\quad q'_{01}=0.2-\\frac{m}{100},\\quad q'_{10}=0.1+\\frac{m}{100},\\quad q'_{11}=0.2,$$\nso the empirical marginals become\n$$\\hat{p}'_{X}(0)=0.7-\\frac{m}{100},\\quad \\hat{p}'_{X}(1)=0.3+\\frac{m}{100},$$\n$$\\hat{p}'_{Y}(0)=0.6+\\frac{m}{100},\\quad \\hat{p}'_{Y}(1)=0.4-\\frac{m}{100}.$$\n\nFor any sequence, the three typicality criteria are\n$$\\left|-\\frac{1}{n}\\log_{2} p(x^{n})-H(X)\\right|\\leq \\epsilon,\\quad \\left|-\\frac{1}{n}\\log_{2} p(y^{n})-H(Y)\\right|\\leq \\epsilon,\\quad \\left|-\\frac{1}{n}\\log_{2} p(x^{n},y^{n})-H(X,Y)\\right|\\leq \\epsilon.$$\nSince the original sequence satisfies each with equality, it suffices to track the deviations induced by $m$ swaps.\n\n1) Joint condition. Let $L_{ij}:=-\\log_{2}p(i,j)$. Then\n$$-\\frac{1}{n}\\log_{2} p(x'^{n},y'^{n})=\\sum_{i,j}q'_{ij}L_{ij}=H(X,Y)+\\sum_{i,j}(q'_{ij}-p(i,j))L_{ij}.$$\nOnly $(0,1)$ and $(1,0)$ change, giving\n$$\\Delta_{\\text{joint}}=\\left(-\\frac{m}{100}\\right)L_{01}+\\left(\\frac{m}{100}\\right)L_{10}=\\frac{m}{100}\\big(L_{10}-L_{01}\\big)=\\frac{m}{100}\\log_{2}\\frac{p(0,1)}{p(1,0)}=\\frac{m}{100}\\log_{2}2=\\frac{m}{100}.$$\nThus the joint criterion requires\n$$\\left|\\Delta_{\\text{joint}}\\right|=\\frac{m}{100}\\leq \\epsilon \\quad\\Longrightarrow\\quad m\\leq 100\\,\\epsilon=4.$$\n\n2) $X$-marginal condition. Define $L_{X0}:=-\\log_{2}0.7$ and $L_{X1}:=-\\log_{2}0.3$. Then\n$$-\\frac{1}{n}\\log_{2}p(x'^{n})=\\hat{p}'_{X}(0)L_{X0}+\\hat{p}'_{X}(1)L_{X1}=H(X)+\\left(\\hat{p}'_{X}(0)-0.7\\right)L_{X0}+\\left(\\hat{p}'_{X}(1)-0.3\\right)L_{X1}.$$\nUsing $\\hat{p}'_{X}(0)-0.7=-\\frac{m}{100}$ and $\\hat{p}'_{X}(1)-0.3=\\frac{m}{100}$,\n$$\\Delta_{X}=\\frac{m}{100}\\big(L_{X1}-L_{X0}\\big)=\\frac{m}{100}\\log_{2}\\frac{0.7}{0.3}=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{7}{3}\\right).$$\nThus the $X$-criterion is\n$$\\left|\\Delta_{X}\\right|=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{7}{3}\\right)\\leq \\epsilon \\quad\\Longrightarrow\\quad m\\leq \\frac{100\\,\\epsilon}{\\log_{2}(7/3)}=\\frac{4}{\\log_{2}(7/3)}.$$\nSince $\\log_{2}(7/3)>1$, this yields $m<4$. Testing integers: $m=4$ would require $\\log_{2}(7/3)\\leq 1$ (false), while $m=3$ requires $\\log_{2}(7/3)\\leq \\frac{4}{3}$, i.e., $\\frac{7}{3}\\leq 2^{4/3}$, which holds. Hence $m\\leq 3$ from the $X$ condition.\n\n3) $Y$-marginal condition. Define $L_{Y0}:=-\\log_{2}0.6$ and $L_{Y1}:=-\\log_{2}0.4$. Then\n$$\\Delta_{Y}=\\left(\\hat{p}'_{Y}(0)-0.6\\right)L_{Y0}+\\left(\\hat{p}'_{Y}(1)-0.4\\right)L_{Y1}=\\frac{m}{100}\\big(L_{Y0}-L_{Y1}\\big)=\\frac{m}{100}\\log_{2}\\frac{0.6}{0.4}=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{3}{2}\\right).$$\nThus\n$$\\left|\\Delta_{Y}\\right|=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{3}{2}\\right)\\leq \\epsilon \\quad\\Longrightarrow\\quad m\\leq \\frac{100\\,\\epsilon}{\\log_{2}(3/2)}=\\frac{4}{\\log_{2}(3/2)}.$$\nSince $\\log_{2}(3/2)<1$, this bound exceeds $4$, and is looser than the joint and $X$ bounds.\n\nCollecting constraints and the feasibility $0\\leq m\\leq N(0,1)=20$, we obtain\n$$m\\leq \\min\\!\\left\\{20,\\;4,\\;\\left\\lfloor\\frac{4}{\\log_{2}(7/3)}\\right\\rfloor,\\;\\left\\lfloor\\frac{4}{\\log_{2}(3/2)}\\right\\rfloor\\right\\}=\\min\\{20,4,3,6\\}=3.$$\nTherefore the maximum integer $m$ such that all three typicality conditions remain satisfied is $m_{\\text{max}}=3$.", "answer": "$$\\boxed{3}$$", "id": "1635561"}]}