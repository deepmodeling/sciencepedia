{"hands_on_practices": [{"introduction": "Before we can determine the minimum rate for transmitting information with a certain level of fidelity, we must first master the concept of a source's intrinsic information content, or entropy. This exercise presents a hypothetical biophysical model to explore the fundamental relationship between the statistical properties of a Bernoulli source and its entropy. By working backward from a given information rate, you will gain a deeper intuition for the binary entropy function $H(p)$ and its symmetric nature [@problem_id:1606652].", "problem": "In a simplified biophysical model, the state of a neuron over discrete time intervals is described as a binary process. In any given interval, the neuron either \"fires\" (represented by a logical 1) or remains \"silent\" (represented by a logical 0). This behavior is modeled as a Bernoulli process, where the probability of the neuron firing is a constant value $p$, and the probability of it remaining silent is $1-p$. The sequence of outputs from this neuron over many time intervals forms a discrete memoryless source.\n\nAn information theorist studying this model measures the average information content, or entropy, of the neuron's output stream. The measured information rate is found to be exactly 0.50 bits per symbol.\n\nBased on this information, which of the following values are possible for the firing probability $p$? Select all that apply.\n\nA. 0.11\n\nB. 0.25\n\nC. 0.50\n\nD. 0.75\n\nE. 0.89\n\nF. 1.00", "solution": "For a Bernoulli source with firing probability $p$, the entropy in bits per symbol is\n$$\nH(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nThe binary entropy function satisfies $H(p)=H(1-p)$ and is maximized at $p=\\frac{1}{2}$ with $H\\left(\\frac{1}{2}\\right)=1$. Therefore, the equation $H(p)=0.50$ has exactly two solutions in the interval $(0,1)$, which are symmetric about $p=\\frac{1}{2}$.\n\nWe evaluate the listed candidates:\n- For $p=0.11$:\n$$\nH(0.11)=-0.11\\log_{2}(0.11)-0.89\\log_{2}(0.89)\\approx -0.11(-3.184)-0.89(-0.168)\\approx 0.350+0.150=0.500.\n$$\nThis value matches the given entropy.\n- For $p=0.25$:\n$$\nH(0.25)=-0.25\\log_{2}(0.25)-0.75\\log_{2}(0.75) = 0.5 - 0.75\\log_{2}(0.75)\\approx 0.5 + 0.311=0.811.\n$$\nThis does not match.\n- For $p=0.50$:\n$$\nH(0.50)=1.\n$$\nThis does not match.\n- For $p=0.75$:\nBy symmetry, $H(0.75)=H(1-0.25)=H(0.25)\\approx 0.811$. This does not match.\n- For $p=0.89$:\nBy symmetry, $H(0.89)=H(1-0.11)=H(0.11)\\approx 0.500$. This value matches.\n- For $p=1.00$:\n$$\nH(1.00)=0.\n$$\nThis does not match.\n\nThus, the values consistent with an entropy of $0.50$ bits per symbol among the options are $p=0.11$ and $p=0.89$.", "answer": "$$\\boxed{AE}$$", "id": "1606652"}, {"introduction": "Having explored the concept of source entropy, we can now address the central theme of rate-distortion theory: quantifying the trade-off between compression and accuracy. This problem challenges you to calculate the minimum required data rate for a memoryless binary source where the symbols are not equally likely, given an acceptable level of error. This practice is essential for understanding how source statistics directly impact the limits of lossy compression [@problem_id:1606619].", "problem": "A discrete memoryless source produces symbols from the binary alphabet $\\mathcal{X} = \\{0, 1\\}$. The source statistics are such that the symbol '0' is seven times more likely to be generated than the symbol '1'. The output of this source is to be compressed and transmitted. The quality of the reconstruction at the receiver is evaluated using a Hamming distortion measure, where the distortion $d(x, \\hat{x})$ between a source symbol $x$ and its reconstruction $\\hat{x}$ is 1 if $x \\neq \\hat{x}$, and 0 if $x = \\hat{x}$.\n\nDetermine the rate-distortion function $R(D)$ for this source at an average distortion level of $D = 0.1$. The rate-distortion function gives the minimum number of bits per symbol required to represent the source while maintaining an average distortion no greater than $D$.\n\nExpress your final answer for the rate in units of bits per symbol, rounded to three significant figures.", "solution": "Let the source be Bernoulli with $P(X=1)=p$ and $P(X=0)=1-p$. The statement “'0' is seven times more likely than '1'” implies\n$$\nP(X=0)=7P(X=1), \\quad P(X=0)+P(X=1)=1 \\;\\Rightarrow\\; P(X=1)=p=\\frac{1}{8},\\; P(X=0)=\\frac{7}{8}.\n$$\nFor a binary memoryless source under Hamming distortion, the rate-distortion function is\n$$\nR(D)=\\begin{cases}\nH_{b}(p)-H_{b}(D), & 0\\leq D\\leq \\min\\{p,1-p\\},\\\\\n0, & D\\geq \\min\\{p,1-p\\},\n\\end{cases}\n$$\nwhere $H_{b}(x)=-x\\log_{2}(x)-(1-x)\\log_{2}(1-x)$ is the binary entropy in bits. Here $\\min\\{p,1-p\\}=\\min\\{\\frac{1}{8},\\frac{7}{8}\\}=\\frac{1}{8}=0.125$. The target distortion is $D=0.1$, which is less than $0.125$, so the first case applies:\n$$\nR(0.1)=H_{b}\\!\\left(\\frac{1}{8}\\right)-H_{b}(0.1).\n$$\nWe compute each term:\n$$\nH_{b}\\!\\left(\\frac{1}{8}\\right)=-\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)-\\frac{7}{8}\\log_{2}\\!\\left(\\frac{7}{8}\\right)\n=\\frac{3}{8}-\\frac{7}{8}\\left(\\log_{2}7-3\\right)\\approx 0.54356\n$$\nand\n$$\nH_{b}(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\approx 0.46900.\n$$\nTherefore,\n$$\nR(0.1)=H_{b}\\!\\left(\\frac{1}{8}\\right)-H_{b}(0.1)\\approx 0.54356 - 0.46900 \\approx 0.07456.\n$$\nRounding to three significant figures gives $R(0.1)\\approx 0.0746$ bits per symbol.", "answer": "$$\\boxed{0.0746}$$", "id": "1606619"}, {"introduction": "Real-world information systems often involve processing and combining multiple data streams. This culminating exercise models such a scenario, where a new source is generated by performing a logical operation on two independent binary sources. To solve this, you must first apply principles of probability to determine the statistical properties of the combined source, and only then calculate its complete rate-distortion function, $R(D)$ [@problem_id:1606664].", "problem": "Two independent, memoryless binary data streams, represented by sequences of random variables $\\{X_{1,i}\\}$ and $\\{X_{2,i}\\}$, are generated by two separate processes. For each time step $i$, the random variable $X_{1,i}$ follows a Bernoulli distribution with $P(X_{1,i}=1) = p = \\frac{1}{3}$, and the random variable $X_{2,i}$ follows a Bernoulli distribution with $P(X_{2,i}=1) = q = \\frac{1}{4}$.\n\nA new data stream $\\{Y_i\\}$ is created by performing a bitwise exclusive OR (XOR) operation on the two source streams, such that $Y_i = X_{1,i} \\oplus X_{2,i}$ for each time step $i$. This new stream $\\{Y_i\\}$ constitutes a memoryless binary source.\n\nWe wish to compress this new source $Y$ and transmit it. The quality of the reconstructed signal $\\hat{Y}$ is evaluated using the Hamming distortion measure, defined as $d(y, \\hat{y}) = 0$ if $y = \\hat{y}$ and $d(y, \\hat{y}) = 1$ if $y \\neq \\hat{y}$. The rate-distortion function, $R(D)$, quantifies the minimum number of bits per symbol required to represent the source $Y$ such that the expected distortion $E[d(Y, \\hat{Y})]$ is no more than a given distortion level $D$.\n\nDetermine the rate-distortion function $R(D)$ for the source $Y$. The final answer should be given as an expression in terms of $D$. Use the base-2 logarithm in your calculations.", "solution": "We first determine the distribution of the XOR source. Since $X_{1,i}$ and $X_{2,i}$ are independent with $P(X_{1,i}=1)=p=\\frac{1}{3}$ and $P(X_{2,i}=1)=q=\\frac{1}{4}$, the XOR output $Y_{i}=X_{1,i}\\oplus X_{2,i}$ is '1' if the inputs are different. The probability of this is:\n$$\nP(Y_{i}=1)=P(X_{1,i}\\neq X_{2,i})=p(1-q)+(1-p)q=p+q-2pq.\n$$\nSubstituting $p=\\frac{1}{3}$ and $q=\\frac{1}{4}$ gives\n$$\nP(Y=1)=\\frac{1}{3}+\\frac{1}{4}-2\\cdot\\frac{1}{3}\\cdot\\frac{1}{4}=\\frac{4}{12}+\\frac{3}{12}-\\frac{2}{12}=\\frac{5}{12}.\n$$\nThus, $Y$ is a Bernoulli source with parameter $r=\\frac{5}{12}$, and $P(Y=0)=1-r=\\frac{7}{12}$.\n\nFor a binary memoryless source $Y\\sim\\text{Bernoulli}(r)$ under Hamming distortion, the rate-distortion function $R(D)$ is given by:\n$$\nR(D)=\\begin{cases}\nH_{b}(r)-H_{b}(D), & 0\\leq D\\leq \\min\\{r,1-r\\},\\\\\n0, & D\\geq \\min\\{r,1-r\\},\n\\end{cases}\n$$\nwhere $H_{b}(x)=-x\\log_{2}(x)-(1-x)\\log_{2}(1-x)$ is the binary entropy function. This standard result arises because for a binary source, the minimum mutual information $I(Y;\\hat{Y})$ subject to a distortion constraint $D$ is achieved by effectively subtracting the 'information' of the allowed errors, $H_{b}(D)$, from the source entropy, $H_{b}(r)$.\n\nFor our source, $r=\\frac{5}{12}$. Since $\\frac{5}{12}  \\frac{1}{2}$, we have $\\min\\{r,1-r\\}=r=\\frac{5}{12}$. Therefore, the rate-distortion function is:\n$$\nR(D)=H_{b}\\!\\left(\\frac{5}{12}\\right)-H_{b}(D), \\quad 0\\leq D\\leq \\frac{5}{12},\n$$\nand\n$$\nR(D)=0, \\quad D\\geq \\frac{5}{12}.\n$$\nWriting this out explicitly, we substitute the expressions for the binary entropy function:\n$$\nH_{b}\\!\\left(\\frac{5}{12}\\right)=-\\frac{5}{12}\\log_{2}\\!\\left(\\frac{5}{12}\\right)-\\frac{7}{12}\\log_{2}\\!\\left(\\frac{7}{12}\\right),\n$$\n$$\nH_{b}(D)=-D\\log_{2}(D)-(1-D)\\log_{2}(1-D).\n$$\nCombining these yields the desired piecewise expression for $R(D)$ in terms of $D$.", "answer": "$$\\boxed{R(D)=\\begin{cases}-\\dfrac{5}{12}\\log_{2}\\!\\left(\\dfrac{5}{12}\\right)-\\dfrac{7}{12}\\log_{2}\\!\\left(\\dfrac{7}{12}\\right)+D\\log_{2}(D)+(1-D)\\log_{2}(1-D),  0\\leq D\\leq \\dfrac{5}{12},\\\\[6pt] 0,  D\\geq \\dfrac{5}{12}.\\end{cases}}$$", "id": "1606664"}]}