{"hands_on_practices": [{"introduction": "Before delving into the complexities of optimizing compression, it is crucial to grasp the fundamental concept of distortion. This first practice problem [@problem_id:1652370] presents a foundational scenario: a zero-rate compression scheme. By representing every possible source symbol with a single, constant value, we can explore how to calculate the resulting average distortion, establishing a simple baseline for performance.", "problem": "Consider a discrete memoryless source, represented by a random variable $X$, which generates symbols from the alphabet $\\mathcal{A} = \\{0, 1, 2, 3\\}$. The source has a uniform probability distribution, meaning each symbol in $\\mathcal{A}$ is equally likely to be generated. A very basic lossy compression scheme is employed where every symbol $x$ from the source is represented by a single, constant value $\\hat{x} = 1.5$. This type of encoding corresponds to a transmission rate of zero, as the representation does not depend on the input symbol.\n\nThe performance of this compression is evaluated using the squared-error distortion measure, given by $d(x, \\hat{x}) = (x - \\hat{x})^2$. Calculate the expected distortion, $D = E[d(X, \\hat{x})]$, for this system. Express your answer as a decimal.", "solution": "The random variable $X$ is uniform on $\\mathcal{A}=\\{0,1,2,3\\}$, so $p(x)=\\frac{1}{4}$ for each $x\\in\\mathcal{A}$. The reconstruction is the constant $\\hat{x}=1.5=\\frac{3}{2}$ and the distortion measure is $d(x,\\hat{x})=(x-\\hat{x})^{2}$. The expected distortion is\n$$\nD=E[d(X,\\hat{x})]=\\sum_{x\\in\\mathcal{A}} p(x)\\,(x-\\hat{x})^{2}=\\sum_{x\\in\\{0,1,2,3\\}} \\frac{1}{4}\\left(x-\\frac{3}{2}\\right)^{2}.\n$$\nEvaluating each term,\n$$\n\\left(0-\\frac{3}{2}\\right)^{2}=\\frac{9}{4},\\quad \\left(1-\\frac{3}{2}\\right)^{2}=\\frac{1}{4},\\quad \\left(2-\\frac{3}{2}\\right)^{2}=\\frac{1}{4},\\quad \\left(3-\\frac{3}{2}\\right)^{2}=\\frac{9}{4}.\n$$\nThus,\n$$\nD=\\frac{1}{4}\\left(\\frac{9}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{9}{4}\\right)=\\frac{1}{4}\\cdot\\frac{20}{4}=\\frac{20}{16}=\\frac{5}{4}=1.25.\n$$\nTherefore, the expected distortion is $1.25$.", "answer": "$$\\boxed{1.25}$$", "id": "1652370"}, {"introduction": "Having established how to calculate distortion, we now explore the central trade-off defined by the rate-distortion theorem. This exercise [@problem_id:1652375] presents a hypothetical scenario where a limited data rate constrains our ability to represent the source perfectly. Your task is to design an optimal quantizer that minimizes squared-error distortion under this rate limit, providing a hands-on feel for how 'rate' dictates the best achievable 'distortion'.", "problem": "A simplified digital environmental sensor is designed to measure temperature. The sensor's output is a discrete random variable, $X$, which can take one of the four integer values from the alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$. Long-term observations have shown that each of these four values is equally likely to occur.\n\nTo conserve power, the sensor's readings must be compressed before being transmitted over a wireless channel. The channel can support a maximum data rate of $R = 1$ bit per sensor reading. The performance of the compression system is evaluated by the average squared-error distortion, defined as $D = E[(X - \\hat{X})^2]$, where $\\hat{X}$ is the reconstructed value after transmission and decompression.\n\nAssuming an optimal compression and decompression scheme is used, what is the theoretical minimum average squared-error distortion $D$ that can be achieved given the channel's rate constraint? Provide your answer as an exact fraction.", "solution": "The problem asks for the minimum achievable average squared-error distortion $D$ for a given source and a rate constraint $R$. This is a classic problem in rate-distortion theory.\n\nThe source variable $X$ takes values from the set $\\mathcal{X} = \\{1, 2, 3, 4\\}$ with equal probability. Therefore, $P(X=x) = \\frac{1}{4}$ for any $x \\in \\mathcal{X}$. The distortion measure is the squared error, $d(x, \\hat{x}) = (x - \\hat{x})^2$. The rate constraint is $R = 1$ bit per symbol.\n\nThe rate $R$ limits the number of distinct values the reconstructed signal $\\hat{X}$ can take. If the reconstruction alphabet $\\hat{\\mathcal{X}}$ has size $|\\hat{\\mathcal{X}}| = M$, then the minimum rate required to represent these values is the entropy of the output, $H(\\hat{X})$. To transmit this information, we need $R \\ge H(\\hat{X})$. The simplest form of encoding uses a fixed-length code, where the number of levels $M$ must satisfy $M \\le 2^R$. In our case, with $R = 1$, we have $M \\le 2^1 = 2$. This means we can use at most two distinct reconstruction levels, let's call them $\\hat{x}_1$ and $\\hat{x}_2$.\n\nThe problem is thus reduced to finding the optimal 2-level quantizer for the source $X$ that minimizes the average squared-error distortion. An optimal quantizer partitions the source alphabet into disjoint sets, and maps all symbols in a given set to a single reconstruction value. For the squared-error distortion measure, the optimal reconstruction value for any set of source symbols is their conditional expectation (i.e., their mean).\n\nWe need to partition the source alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$ into two non-empty subsets, $S_1$ and $S_2$. There are three distinct ways to do this (up to relabeling of the sets):\n1.  $S_1 = \\{1\\}$, $S_2 = \\{2, 3, 4\\}$\n2.  $S_1 = \\{1, 2\\}$, $S_2 = \\{3, 4\\}$\n3.  $S_1 = \\{1, 2, 3\\}$, $S_2 = \\{4\\}$\n\nWe will calculate the minimum average distortion for each case.\n\n**Case 1: Partition $S_1 = \\{1\\}$ and $S_2 = \\{2, 3, 4\\}$**\nThe optimal reconstruction level for $S_1$ is the mean of its elements:\n$\\hat{x}_1 = E[X | X \\in S_1] = 1$.\nThe optimal reconstruction level for $S_2$ is:\n$\\hat{x}_2 = E[X | X \\in S_2] = \\frac{2+3+4}{3} = 3$.\nThe average distortion $D_1$ is calculated by averaging the squared errors for each source symbol:\n$$D_1 = \\sum_{x \\in \\mathcal{X}} P(X=x) d(x, \\hat{x}(x))$$\nwhere $\\hat{x}(x) = \\hat{x}_1$ if $x \\in S_1$ and $\\hat{x}(x) = \\hat{x}_2$ if $x \\in S_2$.\n$$D_1 = P(1)(1-1)^2 + P(2)(2-3)^2 + P(3)(3-3)^2 + P(4)(4-3)^2$$\n$$D_1 = \\frac{1}{4}(0)^2 + \\frac{1}{4}(-1)^2 + \\frac{1}{4}(0)^2 + \\frac{1}{4}(1)^2 = \\frac{1}{4}(0 + 1 + 0 + 1) = \\frac{2}{4} = \\frac{1}{2}$$\n\n**Case 2: Partition $S_1 = \\{1, 2\\}$ and $S_2 = \\{3, 4\\}$**\nThe optimal reconstruction level for $S_1$:\n$\\hat{x}_1 = E[X | X \\in S_1] = \\frac{1+2}{2} = 1.5$.\nThe optimal reconstruction level for $S_2$:\n$\\hat{x}_2 = E[X | X \\in S_2] = \\frac{3+4}{2} = 3.5$.\nThe average distortion $D_2$ is:\n$$D_2 = P(1)(1-1.5)^2 + P(2)(2-1.5)^2 + P(3)(3-3.5)^2 + P(4)(4-3.5)^2$$\n$$D_2 = \\frac{1}{4}(-0.5)^2 + \\frac{1}{4}(0.5)^2 + \\frac{1}{4}(-0.5)^2 + \\frac{1}{4}(0.5)^2$$\n$$D_2 = \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) = \\frac{1}{4}(4 \\times 0.25) = \\frac{1}{4}(1) = \\frac{1}{4}$$\n\n**Case 3: Partition $S_1 = \\{1, 2, 3\\}$ and $S_2 = \\{4\\}$**\nBy symmetry with Case 1, we expect the same distortion, but we will compute it for completeness.\nThe optimal reconstruction level for $S_1$:\n$\\hat{x}_1 = E[X | X \\in S_1] = \\frac{1+2+3}{3} = 2$.\nThe optimal reconstruction level for $S_2$:\n$\\hat{x}_2 = E[X | X \\in S_2] = 4$.\nThe average distortion $D_3$ is:\n$$D_3 = P(1)(1-2)^2 + P(2)(2-2)^2 + P(3)(3-2)^2 + P(4)(4-4)^2$$\n$$D_3 = \\frac{1}{4}(-1)^2 + \\frac{1}{4}(0)^2 + \\frac{1}{4}(1)^2 + \\frac{1}{4}(0)^2 = \\frac{1}{4}(1 + 0 + 1 + 0) = \\frac{2}{4} = \\frac{1}{2}$$\n\nComparing the three cases, the minimum achievable distortion is $D_{\\min} = \\min(D_1, D_2, D_3) = \\min(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{2}) = \\frac{1}{4}$.\n\nFinally, we must verify that this optimal quantizer does not require a rate greater than $R=1$. For the optimal partition (Case 2), the reconstruction levels are $\\hat{x}_1 = 1.5$ and $\\hat{x}_2 = 3.5$. The probabilities of these levels being used are:\n$P(\\hat{X} = \\hat{x}_1) = P(X \\in S_1) = P(X=1) + P(X=2) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n$P(\\hat{X} = \\hat{x}_2) = P(X \\in S_2) = P(X=3) + P(X=4) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\nThe entropy of the output distribution is the minimum rate required to encode it:\n$H(\\hat{X}) = -\\sum_i P(\\hat{X}=\\hat{x}_i)\\log_2 P(\\hat{X}=\\hat{x}_i) = -(\\frac{1}{2}\\log_2\\frac{1}{2} + \\frac{1}{2}\\log_2\\frac{1}{2}) = - \\log_2\\frac{1}{2} = 1$ bit per symbol.\nSince the required rate $H(\\hat{X})=1$ is equal to the available channel rate $R=1$, this minimum distortion is achievable.\n\nTherefore, the theoretical minimum average squared-error distortion is $\\frac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1652375"}, {"introduction": "Rate-distortion theory demonstrates that optimal compression schemes are tailored to the specific statistical properties of the source. This final practice problem [@problem_id:1652583] investigates a common real-world challenge: what happens when a coder designed for one source is applied to another with different statistics? By analyzing this 'mismatched' scenario, you will gain a deeper appreciation for how both the required rate $R$ and the resulting distortion $D$ are fundamentally linked to the input data distribution.", "problem": "In the field of lossy data compression, the rate-distortion function describes the fundamental trade-off between the compression rate and the fidelity of the reconstructed data. Consider a memoryless binary source producing symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The distortion between a source symbol $x$ and its reconstruction $\\hat{x}$ is measured by the Hamming distortion, defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$. The binary entropy function is given by $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.\n\nA compression system is initially designed for a symmetric binary source $X$ where the symbols are equiprobable, i.e., $P(X=1) = 1/2$. The system is constructed to be optimal in the rate-distortion sense, achieving a specific average distortion of $D = E[d(X, \\hat{X})] = 0.1$. For a Bernoulli(1/2) source with Hamming distortion, it is a known result from rate-distortion theory that the optimal compression scheme for a target distortion $D$ (where $0 \\leq D \\leq 1/2$) is realized by a stochastic mapping equivalent to passing the source through a Binary Symmetric Channel (BSC) with a crossover probability $p_c = D$.\n\nThis exact same compression system, which was optimal for the source $X$, is now repurposed to compress a different memoryless binary source, $Y$. This new source is asymmetric, with a probability of producing a '1' given by $P(Y=1) = 1/4$.\n\nCalculate the new average distortion $D' = E[d(Y, \\hat{Y})]$ and the new operational rate $R' = I(Y; \\hat{Y})$ that result from applying this pre-existing compression system to the new source $Y$. Your answer should be a pair of numerical values $(D', R')$. Express the rate $R'$ in bits per symbol. Round both numerical values in your final answer to four significant figures.", "solution": "The problem asks us to evaluate the performance (distortion and rate) of a fixed compression scheme when applied to a source different from the one it was optimized for.\n\nFirst, we identify the properties of the fixed compression scheme. The problem states that the scheme was designed to be rate-distortion optimal for a Bernoulli(1/2) source with a target Hamming distortion of $D = 0.1$. It is also given that for this setup, the optimal scheme is equivalent to a Binary Symmetric Channel (BSC) with a crossover probability $p_c$ equal to the target distortion $D$.\nTherefore, the compression system is a BSC with crossover probability $p_c = 0.1$.\nThis channel is defined by the conditional probabilities:\n$P(\\hat{x}=1|x=0) = p_c = 0.1$\n$P(\\hat{x}=0|x=0) = 1 - p_c = 0.9$\n$P(\\hat{x}=0|x=1) = p_c = 0.1$\n$P(\\hat{x}=1|x=1) = 1 - p_c = 0.9$\nIn general, $P(\\hat{x} \\neq x | x) = p_c$ for any $x \\in \\{0, 1\\}$.\n\nNext, we apply this fixed BSC to the new source $Y$, which is a Bernoulli source with $P(Y=1) = p = 1/4 = 0.25$. Consequently, $P(Y=0) = 1-p = 3/4 = 0.75$. We denote the output of the channel as $\\hat{Y}$.\n\nLet's calculate the new average distortion, $D'$. The distortion is the expected value of the Hamming distortion function, which is equivalent to the probability of error $P(Y \\neq \\hat{Y})$.\nUsing the law of total probability:\n$$D' = P(Y \\neq \\hat{Y}) = \\sum_{y \\in \\{0,1\\}} P(Y=y) P(Y \\neq \\hat{Y} | Y=y)$$\nThe term $P(Y \\neq \\hat{Y} | Y=y)$ is the crossover probability of the channel, which is $p_c$.\n$$D' = P(Y=0) \\cdot p_c + P(Y=1) \\cdot p_c$$\n$$D' = (P(Y=0) + P(Y=1)) \\cdot p_c = 1 \\cdot p_c = p_c$$\nSubstituting the value of $p_c$:\n$$D' = 0.1$$\nRounding to four significant figures, we get $D' = 0.1000$.\n\nNow, let's calculate the new operational rate, $R'$. The rate of a lossy compression scheme is given by the mutual information between the source and its reconstruction, $R' = I(Y; \\hat{Y})$.\nWe can calculate the mutual information using the formula $I(Y; \\hat{Y}) = H(\\hat{Y}) - H(\\hat{Y}|Y)$.\n\nFirst, we find the conditional entropy $H(\\hat{Y}|Y)$:\n$$H(\\hat{Y}|Y) = \\sum_{y \\in \\{0,1\\}} P(Y=y) H(\\hat{Y}|Y=y)$$\nFor a fixed input $y$ to a BSC with crossover $p_c$, the output $\\hat{Y}$ is a Bernoulli random variable with parameter $p_c$ (if we consider the event of a flip). The entropy of this conditional distribution is the binary entropy of the crossover probability, $H_b(p_c)$.\n$$H(\\hat{Y}|Y=y) = H_b(p_c) \\quad \\text{for } y \\in \\{0,1\\}$$\nTherefore,\n$$H(\\hat{Y}|Y) = P(Y=0)H_b(p_c) + P(Y=1)H_b(p_c) = H_b(p_c)$$\nUsing $p_c = 0.1$:\n$$H(\\hat{Y}|Y) = H_b(0.1) = -0.1 \\log_2(0.1) - (1-0.1) \\log_2(0.9) \\approx 0.4689956 \\text{ bits/symbol}$$\n\nNext, we find the entropy of the output, $H(\\hat{Y})$. To do this, we need the probability distribution of $\\hat{Y}$. Let $q = P(\\hat{Y}=1)$.\nUsing the law of total probability:\n$$q = P(\\hat{Y}=1) = P(\\hat{Y}=1|Y=0)P(Y=0) + P(\\hat{Y}=1|Y=1)P(Y=1)$$\nWe have:\n$P(\\hat{Y}=1|Y=0) = p_c = 0.1$\n$P(Y=0) = 0.75$\n$P(\\hat{Y}=1|Y=1) = 1-p_c = 0.9$\n$P(Y=1) = 0.25$\n$$q = (0.1)(0.75) + (0.9)(0.25) = 0.075 + 0.225 = 0.3$$\nSo, the output $\\hat{Y}$ is a Bernoulli(0.3) random variable. Its entropy is:\n$$H(\\hat{Y}) = H_b(0.3) = -0.3 \\log_2(0.3) - (1-0.3) \\log_2(0.7) \\approx 0.8812909 \\text{ bits/symbol}$$\n\nFinally, we can calculate the rate $R'$:\n$$R' = H(\\hat{Y}) - H(\\hat{Y}|Y) \\approx 0.8812909 - 0.4689956$$\n$$R' \\approx 0.4122953 \\text{ bits/symbol}$$\nRounding to four significant figures, we get $R' = 0.4123$ bits/symbol.\n\nThe resulting pair $(D', R')$ is $(0.1000, 0.4123)$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.1000 & 0.4123 \\end{pmatrix}}$$", "id": "1652583"}]}