{"hands_on_practices": [{"introduction": "To begin our exploration of Hamming distortion, we start with the most fundamental case: a completely deterministic system. This exercise removes the complexities of probability to focus on the core definition of average distortion, $D = \\lim_{N\\to\\infty} \\frac{1}{N} \\sum_{i=1}^{N} d_H(X_i, Y_i)$, as the long-term fraction of mismatched symbols. By analyzing a periodic source sequence and a predictable channel error pattern [@problem_id:1628536], you will gain a concrete understanding of how distortion is calculated from the ground up.", "problem": "An infinite binary source sequence, denoted by $S = (X_1, X_2, X_3, \\dots)$, is generated by the deterministic process of endlessly repeating the 4-bit block '1100'. This sequence is then transmitted through a communication channel that introduces errors in a predictable manner. The channel deterministically flips the bit at any position $n$ if and only if $n$ can be expressed in the form $n = 4k+1$, where $k$ is any non-negative integer ($k=0, 1, 2, \\dots$). All other bits are transmitted without alteration. Let the resulting output sequence from the channel be denoted by $S' = (Y_1, Y_2, Y_3, \\dots)$.\n\nThe performance of this system is measured by the average Hamming distortion, $D$, between the source sequence $S$ and the channel output sequence $S'$. It is defined as the long-term average of the per-symbol Hamming distance:\n$$D = \\lim_{N\\to\\infty} \\frac{1}{N} \\sum_{i=1}^{N} d_H(X_i, Y_i)$$\nwhere the per-symbol Hamming distance $d_H(x, y)$ is 1 if the bits $x$ and $y$ are different, and 0 if they are identical.\n\nCalculate the value of the average Hamming distortion $D$.", "solution": "The source is a periodic binary sequence with period 4 given by\n$$(X_{4k+1}, X_{4k+2}, X_{4k+3}, X_{4k+4})=(1,1,0,0) \\quad \\text{for all } k \\in \\{0,1,2,\\dots\\}.$$\nThe channel outputs $Y_{n}$ by flipping exactly when $n=4k+1$ and leaving the bit unchanged otherwise. Hence\n$$\nY_{n}=\\begin{cases}\n1-X_{n}, & n=4k+1 \\text{ for some } k \\in \\{0,1,2,\\dots\\},\\\\\nX_{n}, & \\text{otherwise}.\n\\end{cases}\n$$\nThe per-symbol Hamming distance is defined by $d_{H}(x,y)=1$ if $x\\neq y$ and $d_{H}(x,y)=0$ if $x=y$. Since flipping a binary bit always changes its value, we have\n$$\nd_{H}(X_{n},Y_{n})=\\begin{cases}\n1, & n=4k+1 \\text{ for some } k \\in \\{0,1,2,\\dots\\},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThus $d_{H}(X_{n},Y_{n})$ is the indicator of the event $\\{n \\equiv 1 \\mod 4\\}$. Therefore, the average Hamming distortion is\n$$\nD=\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^{N} d_{H}(X_{n},Y_{n})\n=\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^{N} \\mathbf{1}\\{n \\equiv 1 \\ (\\mathrm{mod}\\ 4)\\}.\n$$\nFor $N=4m+r$ with $m\\in \\mathbb{N}$ and $r\\in\\{0,1,2,3\\}$, the number of indices in $\\{1,\\dots,N\\}$ that satisfy $n\\equiv 1 \\ (\\mathrm{mod}\\ 4)$ is $m$ if $r=0$ and $m+1$ if $r\\in\\{1,2,3\\}$. Hence\n$$\nD=\\lim_{m\\to\\infty}\n\\begin{cases}\n\\frac{m}{4m}, & r=0,\\\\[4pt]\n\\frac{m+1}{4m+r}, & r\\in\\{1,2,3\\},\n\\end{cases}\n=\\frac{1}{4}.\n$$\nEquivalently, by 4-periodicity, within each block of four symbols exactly one position is flipped and three are unchanged, so the average distortion per symbol over any period is $\\frac{1}{4}$, and the long-term average equals this value.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1628536"}, {"introduction": "In practice, perfect data transmission is often too costly, so we use source coding (or quantization) to compress data, a process that intentionally introduces some error to save bandwidth. This practice explores that trade-off by comparing two different schemes for compressing a probabilistic binary source. By calculating and comparing the expected Hamming distortion for each scheme [@problem_id:1628496], you will learn how distortion serves as a critical metric for evaluating the performance of data compression algorithms.", "problem": "A remote weather sensor generates a stream of binary data. The bits are statistically independent, with the probability of generating a '1' being $p$ and a '0' being $1-p$. To conserve bandwidth, the data is processed in blocks of three bits. Each 3-bit block, denoted as $\\mathbf{x} = (x_1, x_2, x_3)$, is mapped to a single representative bit, $\\hat{y}$. This single bit is then used to form a reconstructed 3-bit block, $\\mathbf{\\hat{x}} = (\\hat{y}, \\hat{y}, \\hat{y})$. The error incurred in this process for a single block is measured by the Hamming distortion, which is the number of bit positions in which $\\mathbf{x}$ and $\\mathbf{\\hat{x}}$ differ. The overall performance of a mapping scheme is quantified by its average distortion, which is the distortion for a block averaged over all possible 3-bit source blocks, weighted by their respective probabilities of occurrence.\n\nConsider two different mapping schemes:\nScheme A: The representative bit is the first bit of the block, i.e., $\\hat{y} = x_1$.\nScheme B: The representative bit is the majority bit in the block. For a 3-bit block, this is the bit value that appears two or three times.\n\nFor a source with $p=1/3$, calculate the difference in average distortion between the two schemes, $D_B - D_A$, where $D_A$ and $D_B$ are the average distortions for Scheme A and Scheme B, respectively.\n\nExpress your answer as an exact fraction.", "solution": "Let $X_{1},X_{2},X_{3}$ be independent Bernoulli($p$) random variables representing a source block. The reconstruction is $(\\hat{y},\\hat{y},\\hat{y})$.\n\nScheme A: $\\hat{y}=X_{1}$. The block distortion is the Hamming distance\n$$\nd_{A}=\\sum_{i=1}^{3}\\mathbf{1}\\{X_{i}\\neq X_{1}\\}=\\mathbf{1}\\{X_{2}\\neq X_{1}\\}+\\mathbf{1}\\{X_{3}\\neq X_{1}\\},\n$$\nsince the first position always matches. By linearity of expectation and independence,\n$$\nD_{A}=\\mathbb{E}[d_{A}]=\\sum_{i=2}^{3}\\mathbb{P}(X_{i}\\neq X_{1})\n=2\\big[\\mathbb{P}(X_{1}=0,X_{i}=1)+\\mathbb{P}(X_{1}=1,X_{i}=0)\\big]\n=2\\big[(1-p)p+p(1-p)\\big]=4p(1-p).\n$$\n\nScheme B: $\\hat{y}$ is the majority bit. Let $K=X_{1}+X_{2}+X_{3}\\sim\\text{Binomial}(3,p)$. The distortion equals the number of minority bits, which is $0$ if $K\\in\\{0,3\\}$ and $1$ if $K\\in\\{1,2\\}$. Hence\n$$\nD_{B}=\\mathbb{P}(K=1)+\\mathbb{P}(K=2)=3p(1-p)^{2}+3p^{2}(1-p)=3p(1-p).\n$$\n\nTherefore,\n$$\nD_{B}-D_{A}=3p(1-p)-4p(1-p)=-p(1-p).\n$$\nSubstituting $p=\\frac{1}{3}$,\n$$\nD_{B}-D_{A}=-\\frac{1}{3}\\left(1-\\frac{1}{3}\\right)=-\\frac{1}{3}\\cdot\\frac{2}{3}=-\\frac{2}{9}.\n$$", "answer": "$$\\boxed{-\\frac{2}{9}}$$", "id": "1628496"}, {"introduction": "Does transmitting information always improve things? This final practice delves into a profound question at the heart of communication theory by comparing the performance of two scenarios. The first involves sending a symbol across a noisy channel, while the second involves forgoing transmission entirely in favor of an educated guess at the receiver. This thought experiment [@problem_id:1628544] uses Hamming distortion to quantify the value of communication, forcing us to weigh the errors introduced by a channel against the inherent uncertainty of the source itself.", "problem": "A discrete memoryless source produces binary symbols, $X$, from the set $\\{0, 1\\}$. The probability of producing a '1' is given by $P(X=1) = p_s = 0.2$. We are tasked with evaluating two different schemes for communicating this source symbol to a receiver.\n\n**Scheme A:** The source symbol is transmitted through a Binary Symmetric Channel (BSC). A BSC is a channel model where a transmitted bit is flipped (i.e., a 0 becomes a 1 or a 1 becomes a 0) with a certain probability, known as the crossover probability. For this channel, the crossover probability is $\\epsilon = 0.1$.\n\n**Scheme B:** No information is transmitted. Instead, to reconstruct the source symbol, the receiver always guesses the single, most probable symbol from the source's probability distribution.\n\nThe quality of the reconstruction is measured by the expected Hamming distortion. The Hamming distortion between a source symbol $x$ and a reconstructed symbol $\\hat{x}$ is defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$.\n\nLet $D_A$ be the expected Hamming distortion for Scheme A, and $D_B$ be the expected Hamming distortion for Scheme B. Calculate the difference $D_B - D_A$. Express your final answer as a single real number, rounded to three significant figures.", "solution": "We denote the source by $X \\in \\{0,1\\}$ with $P(X=1)=p_{s}=0.2$ and $P(X=0)=1-p_{s}=0.8$. The Hamming distortion is $d(x,\\hat{x})=0$ if $x=\\hat{x}$ and $d(x,\\hat{x})=1$ if $x\\neq \\hat{x}$. For any reconstruction rule, the expected Hamming distortion equals the probability of reconstruction error:\n$$\nD=\\mathbb{E}[d(X,\\hat{X})]=P(X\\neq \\hat{X}).\n$$\n\nScheme A (BSC with crossover probability $\\epsilon=0.1$): Let $Y$ be the channel output and use the optimal symbol-by-symbol decoder (MAP), which for a BSC with $\\epsilon<\\frac{1}{2}$ decides $\\hat{X}=Y$ provided the prior is not extreme. Check MAP conditions:\nFor $Y=1$, choose $X=1$ if $(1-\\epsilon)p_{s} \\ge \\epsilon(1-p_{s})$, which is equivalent to $p_{s} \\ge \\epsilon$.\nFor $Y=0$, choose $X=0$ if $(1-\\epsilon)(1-p_{s}) \\ge \\epsilon p_{s}$, which is equivalent to $p_{s} \\le 1-\\epsilon$.\nWith $p_{s}=0.2$ and $\\epsilon=0.1$, both $p_{s} \\ge \\epsilon$ and $p_{s} \\le 1-\\epsilon$ hold, so $\\hat{X}=Y$ is optimal. Therefore the error probability is\n$$\nD_{A}=P(\\hat{X}\\neq X)=\\epsilon=0.1.\n$$\n\nScheme B (always guess the most probable symbol): The receiver always outputs $\\hat{X}=0$ since $P(X=0)=0.8$ is larger than $P(X=1)=0.2$. Thus the error occurs exactly when $X=1$, giving\n$$\nD_{B}=P(X\\neq \\hat{X})=P(X=1)=p_{s}=0.2.\n$$\n\nThe required difference is\n$$\nD_{B}-D_{A}=p_{s}-\\epsilon=0.2-0.1=0.1.\n$$\n\nRounded to three significant figures, the result is $0.100$.", "answer": "$$\\boxed{0.100}$$", "id": "1628544"}]}