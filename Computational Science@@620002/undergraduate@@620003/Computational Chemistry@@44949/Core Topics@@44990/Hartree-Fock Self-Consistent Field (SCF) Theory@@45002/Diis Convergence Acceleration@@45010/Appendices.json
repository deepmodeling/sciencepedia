{"hands_on_practices": [{"introduction": "At the heart of the DIIS method is a constrained optimization problem: finding the best linear combination of previous solutions that minimizes an error metric. This problem beautifully translates into a small, elegant system of linear equations. This first practice invites you to roll up your sleeves and solve this system analytically for the simplest non-trivial case, giving you a concrete grasp of the core mathematical machinery before moving on to numerical implementation. [@problem_id:207916]", "problem": "In the Direct Inversion in the Iterative Subspace (DIIS) method for accelerating the convergence of Self-Consistent Field (SCF) calculations, an improved Fock matrix $\\mathbf{F}_{\\text{DIIS}}$ for the next iteration is constructed as a linear combination of Fock matrices from $m$ previous iterations:\n$$\n\\mathbf{F}_{\\text{DIIS}} = \\sum_{i=1}^{m} c_i \\mathbf{F}_i\n$$\nThe coefficients $c_i$ are determined by minimizing the norm of an associated error vector, which is extrapolated in the same way: $\\mathbf{e}_{\\text{DIIS}} = \\sum_{i=1}^{m} c_i \\mathbf{e}_i$. The minimization is performed under the constraint that the coefficients sum to unity, i.e., $\\sum_{i=1}^{m} c_i = 1$.\n\nThis constrained minimization problem can be set up as a system of linear equations. For a basis of $m$ error vectors $\\{\\mathbf{e}_1, \\ldots, \\mathbf{e}_m\\}$, the system takes the form:\n$$\n\\begin{pmatrix}\nB_{11} & B_{12} & \\cdots & B_{1m} & 1 \\\\\nB_{21} & B_{22} & \\cdots & B_{2m} & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{m1} & B_{m2} & \\cdots & B_{mm} & 1 \\\\\n1 & 1 & \\cdots & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nc_1 \\\\\nc_2 \\\\\n\\vdots \\\\\nc_m \\\\\n-\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{pmatrix}\n$$\nwhere $B_{ij} = \\langle \\mathbf{e}_i | \\mathbf{e}_j \\rangle$ is the inner product (e.g., trace of the matrix product) of the error vectors, and $\\lambda$ is a Lagrange multiplier for the normalization constraint.\n\nConsider a DIIS procedure that uses two previous error vectors, $\\mathbf{e}_1$ and $\\mathbf{e}_2$ (i.e., $m=2$). The inner products of these error vectors are given as:\n- $B_{11} = \\langle \\mathbf{e}_1 | \\mathbf{e}_1 \\rangle = A$\n- $B_{22} = \\langle \\mathbf{e}_2 | \\mathbf{e}_2 \\rangle = B$\n- $B_{12} = B_{21} = \\langle \\mathbf{e}_1 | \\mathbf{e}_2 \\rangle = C$\n\nAssuming $A, B, C$ are such that a unique solution exists, determine the analytical expression for the coefficient $c_1$.", "solution": "We have the linear system for $m=2$:\n$$\n\\begin{cases}\nA\\,c_1 + C\\,c_2 - \\lambda = 0,\\\\\nC\\,c_1 + B\\,c_2 - \\lambda = 0,\\\\\nc_1 + c_2 = 1.\n\\end{cases}\n$$\n1. From the first two equations:\n$$\nA\\,c_1 + C\\,c_2 = \\lambda,\n\\quad\nC\\,c_1 + B\\,c_2 = \\lambda\n\\;\\Longrightarrow\\;\nA\\,c_1 + C\\,c_2 = C\\,c_1 + B\\,c_2.\n$$\n2. Rearranging gives\n$$\n(A - C)\\,c_1 = (B - C)\\,c_2\n\\quad\\Longrightarrow\\quad\nc_2 = \\frac{A - C}{\\,B - C\\,}\\,c_1.\n$$\n3. Substitute into the normalization $c_1 + c_2 = 1$:\n$$\nc_1 + \\frac{A - C}{B - C}\\,c_1 = 1\n\\quad\\Longrightarrow\\quad\nc_1\\Bigl(1 + \\frac{A - C}{B - C}\\Bigr) = 1\n\\quad\\Longrightarrow\\quad\nc_1 \\,\\frac{A + B - 2C}{B - C} = 1.\n$$\n4. Hence\n$$\nc_1 = \\frac{B - C}{A + B - 2C}.\n$$", "answer": "$$\\boxed{\\frac{B - C}{A + B - 2C}}$$", "id": "207916"}, {"introduction": "From the elegance of algebra, we move to the power of computation. This exercise guides you through implementing a single, crucial DIIS step as a reusable function, a cornerstone of any modern Self-Consistent Field (SCF) code. You will translate the DIIS equations into a practical algorithm: constructing the $B$ matrix from error vectors, setting up and solving the augmented linear system, and creating the extrapolated matrix, learning to handle potential numerical issues like near-linear dependencies in the process. [@problem_id:2923103]", "problem": "You are asked to implement one step of Direct Inversion in the Iterative Subspace (DIIS), a convergence acceleration method commonly used in self-consistent field procedures in quantum chemistry. The DIIS step should be formulated purely in linear algebra terms, starting from the following definition: Given a collection of residual matrices $\\{ \\mathbf{r}_i \\}_{i=1}^{m}$ and corresponding operator-like matrices $\\{ \\mathbf{F}_i \\}_{i=1}^{m}$, determine coefficients $\\mathbf{c} = (c_1, \\dots, c_m)^\\top$ that minimize the Frobenius norm of the residual combination subject to an affine constraint,\n$$\n\\min_{\\mathbf{c}} \\left\\| \\sum_{i=1}^{m} c_i \\mathbf{r}_i \\right\\|_F^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^{m} c_i = 1.\n$$\nDefine the symmetric matrix $\\mathbf{B} \\in \\mathbb{R}^{m \\times m}$ by the Frobenius inner products,\n$$\nB_{ij} = \\langle \\mathbf{r}_i, \\mathbf{r}_j \\rangle_F = \\mathrm{tr}(\\mathbf{r}_i^\\top \\mathbf{r}_j).\n$$\nTo address potential ill-conditioning, use a Tikhonov-regularized matrix $\\mathbf{B}_\\delta = \\mathbf{B} + \\delta \\mathbf{I}$ with a small scalar $\\delta \\ge 0$. The coefficients $\\mathbf{c}$ and a Lagrange multiplier $\\lambda$ should be obtained by solving the associated linear system enforcing the affine sum constraint. Then form the extrapolated matrix\n$$\n\\mathbf{F}_\\mathrm{DIIS} = \\sum_{i=1}^{m} c_i \\mathbf{F}_i.\n$$\nYour task is to write a complete, runnable program that, for each test case below, computes $\\mathbf{B}$ from the provided residuals, applies the specified regularization $\\delta$, solves for $\\mathbf{c}$ under the affine constraint, constructs $\\mathbf{F}_\\mathrm{DIIS}$, and outputs $\\mathbf{F}_\\mathrm{DIIS}$ flattened in row-major order with each entry rounded to $8$ decimal places.\n\nThe program must handle three separate test cases, each with $m = 3$ previous iterates and small real matrices of size $2 \\times 2$. For each case, the inputs are the lists $\\{\\mathbf{F}_1, \\mathbf{F}_2, \\mathbf{F}_3\\}$ and $\\{\\mathbf{r}_1, \\mathbf{r}_2, \\mathbf{r}_3\\}$, and the scalar $\\delta$. Use the Frobenius inner product to build $\\mathbf{B}$, regularize its diagonal by adding $\\delta$, solve the constrained system for $\\mathbf{c}$, and form the resulting $\\mathbf{F}_\\mathrm{DIIS}$.\n\nTest Suite:\n- Case $1$ (well-conditioned residual geometry):\n  - $\\mathbf{F}_1 = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 0.9 \\end{bmatrix}$, $\\mathbf{F}_2 = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.1 & 0.8 \\end{bmatrix}$, $\\mathbf{F}_3 = \\begin{bmatrix} 0.85 & 0.05 \\\\ 0.05 & 0.75 \\end{bmatrix}$.\n  - $\\mathbf{r}_1 = \\begin{bmatrix} 0.05 & -0.02 \\\\ -0.02 & 0.03 \\end{bmatrix}$, $\\mathbf{r}_2 = \\begin{bmatrix} 0.02 & -0.01 \\\\ -0.01 & 0.015 \\end{bmatrix}$, $\\mathbf{r}_3 = \\begin{bmatrix} 0.01 & -0.005 \\\\ -0.005 & 0.007 \\end{bmatrix}$.\n  - $\\delta = 10^{-10}$.\n- Case $2$ (nearly linearly dependent residuals):\n  - $\\mathbf{F}_1 = \\begin{bmatrix} -0.5 & 0.0 \\\\ 0.0 & -0.4 \\end{bmatrix}$, $\\mathbf{F}_2 = \\begin{bmatrix} -0.45 & 0.0 \\\\ 0.0 & -0.35 \\end{bmatrix}$, $\\mathbf{F}_3 = \\begin{bmatrix} -0.425 & 0.0 \\\\ 0.0 & -0.325 \\end{bmatrix}$.\n  - $\\mathbf{r}_1 = \\begin{bmatrix} 10^{-3} & 2 \\cdot 10^{-3} \\\\ 2 \\cdot 10^{-3} & 4 \\cdot 10^{-3} \\end{bmatrix}$, $\\mathbf{r}_2 = 2 \\mathbf{r}_1$, $\\mathbf{r}_3 = \\tfrac{1}{2} \\mathbf{r}_1$.\n  - $\\delta = 10^{-8}$.\n- Case $3$ (vanishing residuals; boundary behavior):\n  - $\\mathbf{F}_1 = \\begin{bmatrix} 0.3 & -0.1 \\\\ -0.1 & 0.25 \\end{bmatrix}$, $\\mathbf{F}_2 = \\begin{bmatrix} 0.28 & -0.08 \\\\ -0.08 & 0.22 \\end{bmatrix}$, $\\mathbf{F}_3 = \\begin{bmatrix} 0.27 & -0.07 \\\\ -0.07 & 0.21 \\end{bmatrix}$.\n  - $\\mathbf{r}_1 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$, $\\mathbf{r}_2 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$, $\\mathbf{r}_3 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$.\n  - $\\delta = 10^{-6}$.\n\nNumerical and formatting requirements:\n- Use real arithmetic throughout.\n- Construct $\\mathbf{B}$ with entries $B_{ij} = \\sum_{a,b} r_i(a,b) \\, r_j(a,b)$.\n- Add $\\delta$ to the diagonal of $\\mathbf{B}$ before solving the constrained linear system for $\\mathbf{c}$.\n- Compute $\\mathbf{F}_\\mathrm{DIIS} = \\sum_{i=1}^{3} c_i \\mathbf{F}_i$.\n- For each test case, output the flattened row-major list of entries of $\\mathbf{F}_\\mathrm{DIIS}$, each rounded to $8$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of the three flattened lists, enclosed in a single pair of square brackets. For example, an output with three results should look like $[\\,[x_{11}, x_{12}, x_{21}, x_{22}],\\,[y_{11}, y_{12}, y_{21}, y_{22}],\\,[z_{11}, z_{12}, z_{21}, z_{22}]\\,]$ where every $x_{ij}$, $y_{ij}$, and $z_{ij}$ is a floating-point number rounded to $8$ decimal places.", "solution": "The problem statement is a valid and well-posed exercise in linear algebra and numerical optimization, asking for the implementation of a single step of the Direct Inversion in the Iterative Subspace (DIIS) procedure. This method is a cornerstone for accelerating convergence in self-consistent field calculations in quantum chemistry. The problem is scientifically grounded, formally specified, and contains all required data for a unique solution. We will proceed with the formal derivation and implementation.\n\nThe core of the DIIS method is to find a linear combination of previous Fock-like matrices, $\\mathbf{F}_i$, that minimizes the norm of the corresponding linear combination of residual matrices, $\\mathbf{r}_i$. The coefficients of this combination must sum to one to ensure that if all previous iterates were the solution, the new iterate would also be the solution. This is formulated as a constrained optimization problem.\n\nLet $\\{\\mathbf{r}_i\\}_{i=1}^m$ be a set of $m$ residual matrices and $\\{\\mathbf{F}_i\\}_{i=1}^m$ be the corresponding set of operator-like matrices. We seek a vector of coefficients $\\mathbf{c} = (c_1, \\dots, c_m)^\\top$ that solves:\n$$\n\\min_{\\mathbf{c}} \\left\\| \\sum_{i=1}^{m} c_i \\mathbf{r}_i \\right\\|_F^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^{m} c_i = 1\n$$\nThe objective function is the squared Frobenius norm of the linear combination of residuals. We can expand this term using the definition of the Frobenius inner product, $\\langle \\mathbf{A}, \\mathbf{B} \\rangle_F = \\mathrm{tr}(\\mathbf{A}^\\top \\mathbf{B})$. For real matrices, this is equivalent to the sum of the element-wise products.\n$$\nJ(\\mathbf{c}) = \\left\\| \\sum_{i=1}^{m} c_i \\mathbf{r}_i \\right\\|_F^2 = \\left\\langle \\sum_{i=1}^{m} c_i \\mathbf{r}_i, \\sum_{j=1}^{m} c_j \\mathbf{r}_j \\right\\rangle_F = \\sum_{i=1}^{m} \\sum_{j=1}^{m} c_i c_j \\langle \\mathbf{r}_i, \\mathbf{r}_j \\rangle_F\n$$\nWe define a symmetric matrix $\\mathbf{B} \\in \\mathbb{R}^{m \\times m}$ whose elements are the inner products of the residual matrices:\n$$\nB_{ij} = \\langle \\mathbf{r}_i, \\mathbf{r}_j \\rangle_F\n$$\nWith this definition, the objective function becomes a quadratic form in $\\mathbf{c}$:\n$$\nJ(\\mathbf{c}) = \\mathbf{c}^\\top \\mathbf{B} \\mathbf{c}\n$$\nThe constraint is linear: $\\sum_{i=1}^{m} c_i = 1$, which can be written in vector form as $\\mathbf{1}^\\top \\mathbf{c} = 1$, where $\\mathbf{1}$ is a column vector of $m$ ones.\n\nThe problem states that to handle potential linear dependence among the residual vectors (which would make $\\mathbf{B}$ singular), we must use Tikhonov regularization. This involves replacing $\\mathbf{B}$ with a regularized matrix $\\mathbf{B}_\\delta$:\n$$\n\\mathbf{B}_\\delta = \\mathbf{B} + \\delta \\mathbf{I}\n$$\nwhere $\\delta \\ge 0$ is a small scalar and $\\mathbf{I}$ is the $m \\times m$ identity matrix. The optimization problem is now to minimize $\\mathbf{c}^\\top \\mathbf{B}_\\delta \\mathbf{c}$ subject to $\\mathbf{1}^\\top \\mathbf{c} = 1$.\n\nWe solve this using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ for this problem is:\n$$\n\\mathcal{L}(\\mathbf{c}, \\lambda) = \\mathbf{c}^\\top \\mathbf{B}_\\delta \\mathbf{c} - 2\\lambda \\left( \\mathbf{1}^\\top \\mathbf{c} - 1 \\right)\n$$\nwhere $2\\lambda$ is the Lagrange multiplier (the factor of $2$ is for algebraic convenience). To find the minimum, we set the gradient of $\\mathcal{L}$ with respect to $\\mathbf{c}$ and the partial derivative with respect to $\\lambda$ to zero.\n$$\n\\nabla_{\\mathbf{c}} \\mathcal{L} = 2 \\mathbf{B}_\\delta \\mathbf{c} - 2\\lambda \\mathbf{1} = \\mathbf{0} \\quad \\implies \\quad \\mathbf{B}_\\delta \\mathbf{c} - \\lambda \\mathbf{1} = \\mathbf{0}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -2 \\left( \\mathbf{1}^\\top \\mathbf{c} - 1 \\right) = 0 \\quad \\implies \\quad \\mathbf{1}^\\top \\mathbf{c} = 1\n$$\nThese two equations form a system of $m+1$ linear equations for the $m$ coefficients $c_i$ and the multiplier $\\lambda$. We can express this system in block matrix form:\n$$\n\\begin{pmatrix}\n\\mathbf{B}_\\delta & -\\mathbf{1} \\\\\n\\mathbf{1}^\\top & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{c} \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{0} \\\\\n1\n\\end{pmatrix}\n$$\nwhere $\\mathbf{0}$ is the zero vector of size $m$. For the specific case given in the problem, $m=3$, this is a $4 \\times 4$ linear system. The matrix on the left is invertible as long as $\\mathbf{B}_\\delta$ is positive definite, which is guaranteed by $\\delta > 0$ if the original residual vectors are not all zero.\n\nThe computational procedure is as follows:\n1.  Given the residual matrices $\\{\\mathbf{r}_1, \\mathbf{r}_2, \\mathbf{r}_3\\}$, construct the $3 \\times 3$ matrix $\\mathbf{B}$ where $B_{ij} = \\sum_{k,l} (\\mathbf{r}_i)_{kl} (\\mathbf{r}_j)_{kl}$.\n2.  Given the regularization parameter $\\delta$, form $\\mathbf{B}_\\delta = \\mathbf{B} + \\delta\\mathbf{I}$.\n3.  Construct the $4 \\times 4$ augmented matrix $\\mathbf{A}_{\\text{aug}} = \\begin{pmatrix} \\mathbf{B}_\\delta & -\\mathbf{1} \\\\ \\mathbf{1}^\\top & 0 \\end{pmatrix}$ and the right-hand side vector $\\mathbf{b}_{\\text{aug}} = (0, 0, 0, 1)^\\top$.\n4.  Solve the linear system $\\mathbf{A}_{\\text{aug}} \\mathbf{x} = \\mathbf{b}_{\\text{aug}}$ to find the solution vector $\\mathbf{x} = (c_1, c_2, c_3, \\lambda)^\\top$.\n5.  Extract the coefficients $\\mathbf{c} = (c_1, c_2, c_3)^\\top$.\n6.  Using the matrices $\\{\\mathbf{F}_1, \\mathbf{F}_2, \\mathbf{F}_3\\}$, compute the extrapolated matrix $\\mathbf{F}_{\\text{DIIS}}$:\n$$\n\\mathbf{F}_{\\text{DIIS}} = \\sum_{i=1}^3 c_i \\mathbf{F}_i\n$$\nThis procedure will be applied to each of the three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef perform_diis_step(F_matrices, r_matrices, delta):\n    \"\"\"\n    Performs a single step of the DIIS procedure.\n\n    Args:\n        F_matrices (list of np.ndarray): List of operator-like matrices {F_i}.\n        r_matrices (list of np.ndarray): List of residual matrices {r_i}.\n        delta (float): Tikhonov regularization parameter.\n\n    Returns:\n        np.ndarray: The extrapolated matrix F_DIIS.\n    \"\"\"\n    m = len(r_matrices)\n    if m == 0:\n        # Should not happen with problem constraints, but good practice.\n        return np.zeros_like(F_matrices[0]) if F_matrices else np.array([])\n\n    # 1. Construct the B matrix\n    B = np.zeros((m, m))\n    for i in range(m):\n        for j in range(i, m):\n            # Frobenius inner product: <r_i, r_j> = sum of element-wise product\n            inner_product = np.sum(r_matrices[i] * r_matrices[j])\n            B[i, j] = inner_product\n            B[j, i] = inner_product\n\n    # 2. Apply Tikhonov regularization\n    B_delta = B + delta * np.identity(m)\n\n    # 3. Construct the augmented linear system Ax = b for coefficients c and Lagrange multiplier lambda.\n    # The system is:\n    # [ B_delta  -1 ] [ c ] = [ 0 ]\n    # [ 1^T       0 ] [ l ]   [ 1 ]\n    A_aug = np.zeros((m + 1, m + 1))\n    A_aug[:m, :m] = B_delta\n    A_aug[:m, m] = -1.0\n    A_aug[m, :m] = 1.0\n    \n    b_aug = np.zeros(m + 1)\n    b_aug[m] = 1.0\n\n    # 4. Solve the linear system\n    try:\n        x = np.linalg.solve(A_aug, b_aug)\n    except np.linalg.LinAlgError:\n        # This should not occur with regularization, but as a fallback,\n        # return the most recent matrix. This is a common strategy.\n        return F_matrices[-1]\n\n    # 5. Extract coefficients c\n    c = x[:m]\n\n    # 6. Compute the extrapolated matrix F_DIIS\n    F_diis = np.zeros_like(F_matrices[0], dtype=float)\n    for i in range(m):\n        F_diis += c[i] * F_matrices[i]\n\n    return F_diis\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"F_matrices\": [\n                np.array([[1.0, 0.2], [0.2, 0.9]]),\n                np.array([[0.9, 0.1], [0.1, 0.8]]),\n                np.array([[0.85, 0.05], [0.05, 0.75]])\n            ],\n            \"r_matrices\": [\n                np.array([[0.05, -0.02], [-0.02, 0.03]]),\n                np.array([[0.02, -0.01], [-0.01, 0.015]]),\n                np.array([[0.01, -0.005], [-0.005, 0.007]])\n            ],\n            \"delta\": 1e-10\n        },\n        {\n            \"F_matrices\": [\n                np.array([[-0.5, 0.0], [0.0, -0.4]]),\n                np.array([[-0.45, 0.0], [0.0, -0.35]]),\n                np.array([[-0.425, 0.0], [0.0, -0.325]])\n            ],\n            \"r_matrices\": [\n                np.array([[1e-3, 2e-3], [2e-3, 4e-3]]),\n                np.array([[2e-3, 4e-3], [4e-3, 8e-3]]),\n                np.array([[0.5e-3, 1e-3], [1e-3, 2e-3]])\n            ],\n            \"delta\": 1e-8\n        },\n        {\n            \"F_matrices\": [\n                np.array([[0.3, -0.1], [-0.1, 0.25]]),\n                np.array([[0.28, -0.08], [-0.08, 0.22]]),\n                np.array([[0.27, -0.07], [-0.07, 0.21]])\n            ],\n            \"r_matrices\": [\n                np.array([[0.0, 0.0], [0.0, 0.0]]),\n                np.array([[0.0, 0.0], [0.0, 0.0]]),\n                np.array([[0.0, 0.0], [0.0, 0.0]])\n            ],\n            \"delta\": 1e-6\n        }\n    ]\n\n    results_str = []\n    for case in test_cases:\n        F_diis = perform_diis_step(case[\"F_matrices\"], case[\"r_matrices\"], case[\"delta\"])\n        \n        # Flatten the matrix and round to 8 decimal places\n        flat_F = F_diis.flatten()\n        rounded_F = np.round(flat_F, 8)\n        \n        # Format as a string list for the final output\n        results_str.append(str(list(rounded_F)))\n\n    # Combine all results into the final specified format\n    final_output = f\"[{','.join(results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2923103"}, {"introduction": "Now, let's see DIIS in its natural habitat: accelerating a full iterative procedure. In this practice, you will build a minimal SCF loop and use a DIIS routine to drive it to convergence, witnessing its remarkable efficiency. More importantly, by intentionally \"poisoning\" the DIIS memory with a poor solution from the past, you will investigate the algorithm's robustness and its ability to self-correct, revealing why it is such a reliable workhorse in computational chemistry. [@problem_id:2454236]", "problem": "Implement a complete, runnable program that models a minimal self-consistent field (SCF) procedure for a closed-shell two-electron system in a two-dimensional basis with identity overlap, and accelerates convergence using Direct Inversion in the Iterative Subspace (DIIS). Your goal is to investigate the robustness of DIIS by intentionally introducing a stale Fock matrix from an earlier, incorrect iteration into the DIIS subspace (a \"poisoning\" event), and to quantify its effect on the number of iterations required for convergence.\n\nUse the following fundamental base and setup:\n\n- The basis functions are orthonormal, so the overlap matrix is $S = I$.\n- The system has $N_e = 2$ electrons (closed-shell, one doubly occupied spatial orbital).\n- The core Hamiltonian $H$ is a fixed, real symmetric matrix of dimension $2 \\times 2$:\n  $$ H = \\begin{pmatrix} -1.0 & -0.25 \\\\ -0.25 & 0.30 \\end{pmatrix}. $$\n- The Fock matrix $F(P)$ at iteration $k$ is defined by a simple mean-field map,\n  $$ F(P) = H + \\alpha P, $$\n  where $\\alpha = 0.7$ and $P$ is the current density matrix.\n- Given any symmetric $2 \\times 2$ Fock matrix $F$, obtain the next density matrix $P_{\\text{new}}$ by diagonalizing $F$ and occupying the lowest eigenvalue with $2$ electrons:\n  $$ F C = C \\varepsilon, \\quad P_{\\text{new}} = 2\\, c_0 c_0^\\top, $$\n  where $c_0$ is the normalized eigenvector of $F$ corresponding to the lowest eigenvalue.\n- Start from a deliberately poor initial guess $P^{(0)}$ defined as the doubly occupied excited-state orbital of $H$: diagonalize $H$, select the eigenvector associated with its higher eigenvalue, and set $P^{(0)} = 2\\, v_1 v_1^\\top$.\n\nDefine the SCF error at iteration $k$ using the commutator residual (valid for $S = I$),\n$$ E^{(k)} = [F^{(k)}, P^{(k)}] = F^{(k)} P^{(k)} - P^{(k)} F^{(k)}, $$\nand perform DIIS extrapolation of Fock matrices as follows. Maintain a subspace of at most $m$ stored pairs $\\{(F^{(i)}, E^{(i)})\\}$. For $n \\ge 2$ stored error matrices, determine coefficients $c_i$ that minimize the Frobenius norm of the extrapolated residual subject to an affine constraint,\n$$ \\min_{\\{c_i\\}} \\left\\| \\sum_{i=1}^{n} c_i E^{(i)} \\right\\|_F^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} c_i = 1. $$\nLet the symmetric matrix $B \\in \\mathbb{R}^{n \\times n}$ have elements\n$$ B_{ij} = \\langle E^{(i)}, E^{(j)} \\rangle = \\operatorname{Tr}\\!\\left( (E^{(i)})^\\top E^{(j)} \\right). $$\nUsing the method of Lagrange multipliers with multiplier $\\lambda$, this constrained least-squares problem leads to the linear system\n$$\n\\begin{pmatrix}\nB & -\\mathbf{1} \\\\\n\\mathbf{1}^\\top & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{c} \\\\ \\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{0} \\\\ 1\n\\end{pmatrix},\n$$\nwhere $\\mathbf{1}$ is the column vector of $n$ ones, $\\mathbf{0}$ is the $n$-vector of zeros, and $\\mathbf{c} = (c_1,\\dots,c_n)^\\top$. Construct the DIIS-extrapolated Fock matrix\n$$ F_{\\text{DIIS}} = \\sum_{i=1}^{n} c_i F^{(i)}, $$\nand use it to form $P_{\\text{new}}$ via the diagonalization rule above. If fewer than $2$ stored pairs are available, set $F_{\\text{DIIS}} = F^{(k)}$.\n\nTo investigate robustness, introduce a single \"poisoning\" event at a specified iteration index $k_{\\text{poison}}$: at that iteration, instead of appending the current pair $\\left(F^{(k)}, E^{(k)}\\right)$ to the DIIS subspace, append a stale pair $\\left(F^{(j)}, E^{(j)}\\right)$ from an earlier iteration index $j$ (with $j \\ge 0$ and $j < k$). Maintain a maximum subspace size of $m$ by discarding the oldest stored pair whenever the capacity is exceeded. If the DIIS linear system is ill-conditioned, use a small diagonal regularization on $B$ so that the augmented system remains solvable; the regularization must be strictly positive and very small compared to the typical scale of $B$.\n\nConvergence criterion and safeguards:\n\n- Declare convergence when the root-mean-square change in density falls below a tolerance $\\tau = 10^{-8}$:\n  $$ \\Delta_P = \\sqrt{\\frac{1}{4} \\sum_{a,b=1}^{2} \\left( P_{\\text{new},ab} - P_{ab} \\right)^2 } < \\tau. $$\n- Impose a hard cap of $k_{\\max} = 200$ iterations; if this cap is reached, the run is considered non-converged.\n\nYour program must implement the above and run the following test suite. Each test returns the number of iterations $k_{\\text{conv}}$ actually used to reach the density tolerance (or $k_{\\max}$ if not converged):\n\n- Test $1$ (happy path, no poisoning): $m = 4$, $k_{\\text{poison}}$ is absent.\n- Test $2$ (single poisoning, moderate subspace): $m = 4$, one poisoning at $k_{\\text{poison}} = 5$ using the earliest available pair $j = 0$.\n- Test $3$ (boundary subspace size, poisoning): $m = 2$, one poisoning at $k_{\\text{poison}} = 5$ using the earliest available pair $j = 0$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Tests $1$ to $3$:\n  \"[k1,k2,k3]\".\n- All outputs are integers. No units are involved in the outputs for this problem.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It is scientifically grounded in the principles of computational quantum chemistry, is well-posed with a clear algorithmic structure, and is expressed with objective, unambiguous language. The task is to implement a self-consistent field (SCF) procedure for a minimal two-electron, two-basis-function model system, incorporating the Direct Inversion in the Iterative Subspace (DIIS) convergence acceleration technique. A specific feature to investigate is the robustness of DIIS against the intentional introduction of a \"stale\" or \"poison\" Fock matrix into its iterative subspace.\n\nThe solution is implemented as a numerical algorithm that iteratively refines an initial guess for the electronic density matrix until self-consistency is achieved.\n\nThe fundamental constants and matrices are defined as specified:\n- The core Hamiltonian, $H$, a $2 \\times 2$ real symmetric matrix:\n$$ H = \\begin{pmatrix} -1.0 & -0.25 \\\\ -0.25 & 0.30 \\end{pmatrix} $$\n- The electron repulsion scaling factor, $\\alpha = 0.7$.\n- The convergence tolerance for the density matrix change, $\\tau = 10^{-8}$.\n- The maximum number of allowed iterations, $k_{\\max} = 200$.\n\nThe algorithm proceeds through the following steps:\n\n1.  **Initialization**: The SCF procedure begins at iteration $k=0$ with an initial guess for the density matrix, $P^{(0)}$. As specified, this is a deliberately poor guess derived from the ground-state core Hamiltonian's *excited* state. We diagonalize $H$ to find its eigenvalues $\\varepsilon_0, \\varepsilon_1$ and corresponding eigenvectors $v_0, v_1$. The initial density is then constructed from the eigenvector $v_1$ associated with the higher eigenvalue $\\varepsilon_1$:\n    $$ P^{(0)} = 2\\, v_1 v_1^\\top $$\n    A history of all computed Fock and error matrix pairs, $\\{(F^{(i)}, E^{(i)})\\}$, is maintained for the entire run. A separate DIIS subspace, with a maximum capacity of $m$ pairs, is also initialized.\n\n2.  **SCF Iteration Loop**: For each iteration $k = 0, 1, 2, \\dots$ up to $k_{\\max}-1$:\n    a.  **Fock Matrix Construction**: The Fock matrix for the current iteration, $F^{(k)}$, is constructed using the density matrix from the previous step, $P^{(k)}$:\n        $$ F^{(k)} = H + \\alpha P^{(k)} $$\n    b.  **Error Matrix Calculation**: The DIIS error matrix, $E^{(k)}$, is calculated as the commutator of the Fock and density matrices. This form of the residual is valid because the basis is orthonormal, i.e., the overlap matrix is the identity, $S = I$.\n        $$ E^{(k)} = [F^{(k)}, P^{(k)}] = F^{(k)} P^{(k)} - P^{(k)} F^{(k)} $$\n        The pair $(F^{(k)}, E^{(k)})$ is stored in the historical record.\n\n    c.  **DIIS Subspace Management and Extrapolation**:\n        i.  **Poisoning Logic**: If the current iteration $k$ matches the specified poisoning iteration, $k_{\\text{poison}}$, the stale pair $(F^{(j)}, E^{(j)})$ from a specified prior iteration $j$ is selected. Otherwise, the current pair $(F^{(k)}, E^{(k)})$ is selected.\n        ii. **Subspace Update**: The selected pair is added to the DIIS subspace. If the size of the subspace now exceeds the maximum allowed size, $m$, the oldest pair is discarded to maintain capacity.\n        iii. **Extrapolation**: If the DIIS subspace contains fewer than $n=2$ pairs, no extrapolation is performed, and the subsequent Fock matrix is simply the current one, $F^{(k)}$. If $n \\ge 2$, an optimal linear combination of the Fock matrices in the subspace, $F_{\\text{DIIS}} = \\sum_{i=1}^{n} c_i F^{(i)}$, is formed. The coefficients $c_i$ are determined by solving the constrained least-squares problem that minimizes the norm of the extrapolated error, $\\min \\|\\sum_{i=1}^{n} c_i E^{(i)}\\|_F^2$, subject to $\\sum_{i=1}^{n} c_i = 1$. This leads to the $(n+1) \\times (n+1)$ augmented linear system:\n            $$\n            \\begin{pmatrix}\n            B & -\\mathbf{1} \\\\\n            \\mathbf{1}^\\top & 0\n            \\end{pmatrix}\n            \\begin{pmatrix}\n            \\mathbf{c} \\\\ \\lambda\n            \\end{pmatrix}\n            =\n            \\begin{pmatrix}\n            \\mathbf{0} \\\\ 1\n            \\end{pmatrix}\n            $$\n            The matrix $B$ is constructed from the Frobenius inner products of the error matrices in the subspace: $B_{ij} = \\operatorname{Tr}((E^{(i)})^\\top E^{(j)})$. To ensure numerical stability against potential linear dependencies among the error vectors, which would render $B$ singular, a small diagonal regularization term, $\\epsilon I$ with $\\epsilon=10^{-10}$, is added to $B$ before solving the system.\n        iv. **Next Fock Matrix**: The Fock matrix for the next step, $F_{\\text{next}}$, is set to the extrapolated matrix, $F_{\\text{DIIS}}$.\n\n    d.  **Density Matrix Update**: $F_{\\text{next}}$ is diagonalized to find its eigenvalues and eigenvectors. The new density matrix, $P_{\\text{new}}$, is constructed from the eigenvector $c_0$ corresponding to the lowest eigenvalue, representing the single doubly occupied molecular orbital:\n        $$ P_{\\text{new}} = 2\\, c_0 c_0^\\top $$\n\n    e.  **Convergence Check**: The change in the density matrix is quantified by its root-mean-square deviation:\n        $$ \\Delta_P = \\sqrt{\\frac{1}{4} \\sum_{a,b=1}^{2} \\left( P_{\\text{new},ab} - P^{(k)}_{ab} \\right)^2 } $$\n        If $\\Delta_P < \\tau$, the procedure has converged. The loop terminates and the number of iterations, $k+1$, is recorded.\n\n    f.  **Cycle**: If not converged, the density is updated for the next iteration, $P^{(k+1)} \\leftarrow P_{\\text{new}}$, and the loop continues.\n\n3.  **Termination**: If the loop completes $k_{\\max}$ iterations without satisfying the convergence criterion, the run is considered non-converged, and the value $k_{\\max}$ is recorded.\n\nThis complete algorithm is executed for each of the three test cases specified in the problem statement to determine the number of iterations required for convergence.", "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef run_scf(m, k_poison=None, j_poison=None):\n    \"\"\"\n    Runs a minimal SCF procedure with DIIS acceleration.\n\n    Args:\n        m (int): Maximum size of the DIIS subspace.\n        k_poison (int, optional): Iteration index at which to poison the subspace.\n        j_poison (int, optional): Index of the stale data to use for poisoning.\n\n    Returns:\n        int: The number of iterations to converge, or k_max if not converged.\n    \"\"\"\n    # System parameters\n    H_core = np.array([[-1.0, -0.25], [-0.25, 0.30]])\n    alpha = 0.7\n    tol = 1e-8\n    k_max = 200\n    reg_eps = 1e-10  # Regularization parameter for the DIIS B-matrix\n\n    # 1. Initialization: Create a poor initial guess from the excited state of H_core\n    _, eigvecs_H = np.linalg.eigh(H_core)\n    # v1 is the eigenvector for the higher eigenvalue\n    v1 = eigvecs_H[:, 1].reshape(2, 1)\n    P = 2.0 * (v1 @ v1.T)\n\n    diis_space = deque(maxlen=m)\n    history = []  # Stores all (F, E) pairs by iteration index\n\n    for k in range(k_max):\n        # 2a. Fock Matrix Construction\n        F = H_core + alpha * P\n\n        # 2b. Error Matrix Calculation\n        E = F @ P - P @ F\n        history.append((F, E))\n\n        # 2c. DIIS Subspace Management and Extrapolation\n        # i. Poisoning Logic\n        if k == k_poison and j_poison is not None and j_poison < len(history):\n            pair_to_add = history[j_poison]\n        else:\n            pair_to_add = (F, E)\n        \n        # ii. Subspace Update\n        diis_space.append(pair_to_add)\n\n        # iii. Extrapolation\n        if len(diis_space) < 2:\n            F_next = F\n        else:\n            n = len(diis_space)\n            B = np.zeros((n, n))\n            for i in range(n):\n                for j in range(n):\n                    Ei = diis_space[i][1]\n                    Ej = diis_space[j][1]\n                    B[i, j] = np.trace(Ei.T @ Ej)\n\n            # Add regularization for numerical stability\n            B += np.eye(n) * reg_eps\n\n            # Build and solve the augmented linear system\n            aug_B = np.zeros((n + 1, n + 1))\n            aug_B[:n, :n] = B\n            aug_B[:n, n] = -1.0\n            aug_B[n, :n] = 1.0\n            \n            rhs = np.zeros(n + 1)\n            rhs[n] = 1.0\n\n            try:\n                coeffs_lambda = np.linalg.solve(aug_B, rhs)\n                coeffs = coeffs_lambda[:n]\n\n                # iv. Next Fock Matrix\n                F_next = np.zeros_like(F)\n                for i in range(n):\n                    F_next += coeffs[i] * diis_space[i][0]\n            except np.linalg.LinAlgError:\n                # If solver fails despite regularization, fall back to no extrapolation\n                F_next = F\n\n        # 2d. Density Matrix Update\n        eigvals, eigvecs = np.linalg.eigh(F_next)\n        # c0 is the eigenvector for the lowest eigenvalue\n        c0 = eigvecs[:, 0].reshape(2, 1)\n        P_new = 2.0 * (c0 @ c0.T)\n\n        # 2e. Convergence Check\n        delta_P = np.sqrt(np.sum((P_new - P)**2) / 4.0)\n\n        if delta_P < tol:\n            return k + 1\n\n        # 2f. Cycle\n        P = P_new\n\n    return k_max\n\ndef solve():\n    \"\"\"\n    Executes the test suite for the DIIS robustness investigation.\n    \"\"\"\n    test_cases = [\n        # Test 1: m=4, no poisoning\n        {'m': 4},\n        # Test 2: m=4, poison at k=5 with data from j=0\n        {'m': 4, 'k_poison': 5, 'j_poison': 0},\n        # Test 3: m=2, poison at k=5 with data from j=0\n        {'m': 2, 'k_poison': 5, 'j_poison': 0},\n    ]\n\n    results = []\n    for params in test_cases:\n        num_iterations = run_scf(**params)\n        results.append(num_iterations)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2454236"}]}