{"hands_on_practices": [{"introduction": "Our hands-on exploration of optimization algorithms begins with the most fundamental method: Steepest Descent (SD). This first practice [@problem_id:2463036] uses the simple, one-dimensional harmonic oscillator potential to investigate the critical role of the step size, $\\alpha$. By implementing a fixed-step SD algorithm, you will directly observe how the choice of $\\alpha$ determines whether the method successfully finds the minimum, diverges, or fails to converge, providing a core insight into the behavior of all iterative optimizers.", "problem": "You are given a one-dimensional simple harmonic oscillator potential defined by $V(x) = \\tfrac{1}{2} k x^2$, where $k > 0$ is a given force constant and $x$ is the scalar coordinate. Consider the fixed-step steepest descent (SD) iteration given by\n$$\nx_{n+1} = x_n - \\alpha \\,\\frac{dV}{dx}(x_n),\n$$\nwith a constant step size $\\alpha \\in \\mathbb{R}$ and initial value $x_0 \\in \\mathbb{R}$. For this potential, the exact minimizer is at $x^\\star = 0$. Define convergence to the minimizer as the existence of an index $n \\le N_{\\max}$ such that $|x_n| \\le \\varepsilon$, for a given maximum iteration count $N_{\\max}$ and tolerance $\\varepsilon > 0$. If no such $n$ exists within $N_{\\max}$ iterations, declare non-convergence.\n\nWrite a complete, runnable program that, for each of the following test cases, applies the iteration to determine convergence or non-convergence and outputs a boolean result:\n- Test case $1$: $k = 3.0$, $\\alpha = 0.3$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\n- Test case $2$: $k = 3.0$, $\\alpha = 0.7$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\n- Test case $3$: $k = 2.0$, $\\alpha = 1.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\n- Test case $4$: $k = 1.5$, $\\alpha = 0.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\n- Test case $5$: $k = 1.5$, $\\alpha = -0.2$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\n- Test case $6$: $k = 2.0$, $\\alpha = 0.99$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\n\nAll quantities are to be treated as dimensionless for this computational exercise, and the final answers are booleans without physical units. Your program must aggregate the results for the six test cases into a single line of output containing the booleans as a comma-separated list enclosed in square brackets, with no additional text. For example, the output must look like a single Python-style list on one line.", "solution": "The problem statement has been examined and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The task is a standard exercise in numerical optimization applied to a fundamental physical model. We proceed with the analysis and solution.\n\nThe problem asks to determine the convergence of a fixed-step steepest descent (SD) algorithm for minimizing the one-dimensional simple harmonic oscillator potential, given by:\n$$\nV(x) = \\frac{1}{2} k x^2\n$$\nwhere $k > 0$ is the force constant. The minimizer of this potential is located at $x^\\star = 0$.\n\nThe SD iteration is defined as:\n$$\nx_{n+1} = x_n - \\alpha g_n\n$$\nwhere $g_n$ is the gradient of the potential at $x_n$, and $\\alpha$ is a constant step size. For this one-dimensional problem, the gradient is simply the derivative with respect to $x$:\n$$\ng_n = \\frac{dV}{dx}(x_n) = k x_n\n$$\nSubstituting the derivative into the iteration formula yields a linear recurrence relation:\n$$\nx_{n+1} = x_n - \\alpha (k x_n) = (1 - \\alpha k) x_n\n$$\nThis is a geometric progression with a common ratio $r = 1 - \\alpha k$. The solution to this recurrence relation is given by:\n$$\nx_n = (1 - \\alpha k)^n x_0\n$$\nFor the sequence $\\{x_n\\}$ to converge to the minimizer $x^\\star = 0$ as $n \\to \\infty$, the absolute value of the common ratio must be less than $1$:\n$$\n|r| = |1 - \\alpha k| < 1\n$$\nThis inequality is equivalent to the following two conditions:\n$$\n-1 < 1 - \\alpha k < 1\n$$\nLet us analyze these two inequalities separately.\n$1$. The right-hand side inequality:\n$$\n1 - \\alpha k < 1 \\implies -\\alpha k < 0\n$$\nSince the force constant $k$ is given to be positive ($k > 0$), this simplifies to $\\alpha > 0$.\n$2$. The left-hand side inequality:\n$$\n-1 < 1 - \\alpha k \\implies \\alpha k < 2\n$$\nAgain, since $k > 0$, this simplifies to $\\alpha < 2/k$.\n\nCombining these two results, the necessary and sufficient condition for the steepest descent algorithm to converge to the minimum for this potential is:\n$$\n0 < \\alpha < \\frac{2}{k}\n$$\nIf this condition is met, the sequence $\\{x_n\\}$ will converge to $0$. We must now evaluate each test case against this analytical condition. Convergence is defined computationally as reaching a state where $|x_n| \\le \\varepsilon$ for some iteration $n \\le N_{\\max}$.\n\nCase 1: $k = 3.0$, $\\alpha = 0.3$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\nThe convergence range for $\\alpha$ is $0 < \\alpha < 2/3.0$, which is approximately $0 < \\alpha < 0.667$. The given step size $\\alpha = 0.3$ falls within this range. The ratio is $r = 1 - (0.3)(3.0) = 0.1$. Since $|r| < 1$, the iteration will converge. The result is **True**.\n\nCase 2: $k = 3.0$, $\\alpha = 0.7$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\nThe convergence range for $\\alpha$ is $0 < \\alpha < 0.667$. The step size $\\alpha = 0.7$ is outside this range. The ratio is $r = 1 - (0.7)(3.0) = 1 - 2.1 = -1.1$. Since $|r| > 1$, the sequence will diverge, with successive points alternating in sign and increasing in magnitude. The result is **False**.\n\nCase 3: $k = 2.0$, $\\alpha = 1.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\nThe convergence range for $\\alpha$ is $0 < \\alpha < 2/2.0$, or $0 < \\alpha < 1.0$. The step size $\\alpha = 1.0$ is a boundary case where $\\alpha = 2/k$. The ratio is $r = 1 - (1.0)(2.0) = -1.0$. The sequence becomes $x_{n+1} = -x_n$. Starting from $x_0 = 1.0$, the values will be $1.0, -1.0, 1.0, -1.0, \\dots$. The magnitude $|x_n|$ remains fixed at $1.0$ and will never be less than or equal to $\\varepsilon = 10^{-12}$. It does not converge. The result is **False**.\n\nCase 4: $k = 1.5$, $\\alpha = 0.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\nThe step size is $\\alpha = 0.0$. This is a boundary case of the convergence condition $0 < \\alpha < 2/k$. The iteration becomes $x_{n+1} = x_n - 0 \\cdot (kx_n) = x_n$. The position never changes from its initial value $x_0 = 1.0$. Since $|1.0| > \\varepsilon = 10^{-12}$, the condition for convergence is never met. The algorithm stalls. The result is **False**.\n\nCase 5: $k = 1.5$, $\\alpha = -0.2$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\nThe step size $\\alpha = -0.2$ is negative, which violates the convergence condition $\\alpha > 0$. The ratio is $r = 1 - (-0.2)(1.5) = 1 + 0.3 = 1.3$. Since $|r| > 1$, the sequence will diverge monotonically. The result is **False**.\n\nCase 6: $k = 2.0$, $\\alpha = 0.99$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$.\nThe convergence range for $\\alpha$ is $0 < \\alpha < 2/2.0$, or $0 < \\alpha < 1.0$. The step size $\\alpha = 0.99$ is within this range. The ratio is $r = 1 - (0.99)(2.0) = 1 - 1.98 = -0.98$. Since $|r| = |-0.98| < 1$, the iteration will converge, although slowly due to the ratio being close to $-1$, which causes oscillatory convergence. The result is **True**.\n\nA computational program will now be constructed to implement the iteration for each case and verify these analytical conclusions. The program will loop for a maximum of $N_{\\max}$ iterations, checking the convergence condition $|x_n| \\le \\varepsilon$ at each step for $n=0, 1, \\dots, N_{\\max}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the steepest descent convergence problem for six test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, alpha, x0, N_max, epsilon)\n    test_cases = [\n        (3.0, 0.3, 1.0, 5000, 1e-12),\n        (3.0, 0.7, 1.0, 5000, 1e-12),\n        (2.0, 1.0, 1.0, 5000, 1e-12),\n        (1.5, 0.0, 1.0, 5000, 1e-12),\n        (1.5, -0.2, 1.0, 5000, 1e-12),\n        (2.0, 0.99, 1.0, 5000, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        k, alpha, x0, N_max, epsilon = case\n        \n        # Initialize position x and convergence flag\n        x = x0\n        converged = False\n\n        # The problem asks if there is an index n <= N_max such that |x_n| <= epsilon.\n        # This means we check x_0, x_1, ..., x_{N_max}.\n        # This requires N_max+1 checks and N_max updates.\n        for n in range(N_max + 1):\n            \n            # Check for convergence at the current position x_n\n            if abs(x) <= epsilon:\n                converged = True\n                break\n\n            # To avoid an unnecessary update after the last check, we add this condition.\n            if n < N_max:\n                # Calculate the gradient for the simple harmonic oscillator potential V(x) = 1/2*k*x^2.\n                # The gradient (derivative) is dV/dx = k*x.\n                gradient = k * x\n                \n                # Apply the steepest descent update rule.\n                x = x - alpha * gradient\n        \n        results.append(converged)\n\n    # Final print statement in the exact required format.\n    # The output should be a string that looks like a Python list of booleans.\n    # str(True) is 'True', str(False) is 'False', so this works as intended.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2463036"}, {"introduction": "While our first exercise demonstrated basic convergence, real potential energy surfaces are rarely simple parabolas; they are often anisotropic, with different curvatures along different molecular coordinates. This practice [@problem_id:2463052] explores SD on such a surface, posing a fascinating question: can the optimizer get trapped, bouncing between points forever without converging? By calculating the exact step size $s$ that creates this non-convergent two-point limit cycle, you will gain a deeper appreciation for the challenges SD faces in more complex, realistic scenarios.", "problem": "In computational chemistry, local geometry optimization is often performed by minimizing a potential energy surface (PES). Consider the two-dimensional potential energy surface (PES) that approximates a local quadratic well, given in nondimensionalized units by\n$$\nE(x,y) = \\tfrac{1}{2}\\big(x^{2} + 3\\,y^{2}\\big).\n$$\nApply the steepest descent algorithm (SD) with a constant step size parameter $s$ to minimize $E(x,y)$. By definition, the SD update for the coordinates $\\mathbf{r}_{k} = (x_{k},y_{k})$ is\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s\\,\\nabla E(\\mathbf{r}_{k}).\n$$\nAssume the initial condition $\\mathbf{r}_{0} = (0,y_{0})$ with $y_{0} \\neq 0$.\n\nDetermine the value of the constant step size $s$ (expressed as an exact, simplified fraction) for which the SD iterates are trapped in a non-convergent two-point limit cycle, alternating forever between $(0,y_{0})$ and $(0,-y_{0})$. Use nondimensionalized units and do not include units in your answer. No rounding is required.", "solution": "The problem as stated is well-posed and scientifically sound. We shall proceed with the derivation.\n\nThe potential energy surface (PES) is given by the function:\n$$\nE(x,y) = \\frac{1}{2}(x^{2} + 3y^{2})\n$$\nThe steepest descent (SD) algorithm updates the coordinates $\\mathbf{r}_{k} = (x_{k}, y_{k})$ according to the rule:\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s \\nabla E(\\mathbf{r}_{k})\n$$\nwhere $s$ is the constant step size.\n\nFirst, we must compute the gradient of the energy function, $\\nabla E(x,y)$. The partial derivatives are:\n$$\n\\frac{\\partial E}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = x\n$$\n$$\n\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = 3y\n$$\nThus, the gradient vector is:\n$$\n\\nabla E(x,y) = \\begin{pmatrix} x \\\\ 3y \\end{pmatrix}\n$$\nSubstituting the gradient into the SD update rule, we obtain the component-wise recurrence relations for the coordinates $(x_{k}, y_{k})$:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_{k} \\\\ y_{k} \\end{pmatrix} - s \\begin{pmatrix} x_{k} \\\\ 3y_{k} \\end{pmatrix}\n$$\nThis yields two independent equations:\n$$\nx_{k+1} = x_{k} - s x_{k} = (1 - s) x_{k}\n$$\n$$\ny_{k+1} = y_{k} - s (3y_{k}) = (1 - 3s) y_{k}\n$$\nThe initial condition is given as $\\mathbf{r}_{0} = (0, y_{0})$, where $y_{0} \\neq 0$.\n\nLet us analyze the behavior of the $x$-coordinate. With $x_{0} = 0$, the recurrence relation for $x$ gives:\n$$\nx_{1} = (1-s)x_{0} = (1-s)(0) = 0\n$$\nBy induction, it is trivial to show that $x_{k} = 0$ for all integers $k \\ge 0$. This is consistent with the specified limit cycle points $(0, y_{0})$ and $(0, -y_{0})$, which lie on the $y$-axis.\n\nNow, we analyze the behavior of the $y$-coordinate. The problem states that the system enters a two-point limit cycle, alternating between $(0, y_{0})$ and $(0, -y_{0})$. This implies that for even $k$, $\\mathbf{r}_{k} = (0, y_{0})$, and for odd $k$, $\\mathbf{r}_{k} = (0, -y_{0})$.\n\nLet's consider the first step of the iteration, from $k=0$ to $k=1$.\nWe start at $\\mathbf{r}_{0} = (0, y_{0})$. The next point must be $\\mathbf{r}_{1} = (0, -y_{0})$.\nUsing the recurrence relation for $y$ at $k=0$:\n$$\ny_{1} = (1 - 3s) y_{0}\n$$\nFor the iteration to match the limit cycle, we require $y_{1} = -y_{0}$. Therefore:\n$$\n-y_{0} = (1 - 3s) y_{0}\n$$\nSince it is given that $y_{0} \\neq 0$, we can divide both sides by $y_{0}$:\n$$\n-1 = 1 - 3s\n$$\nSolving this equation for the step size $s$:\n$$\n3s = 1 - (-1) = 2\n$$\n$$\ns = \\frac{2}{3}\n$$\nWe must verify that this value of $s$ sustains the limit cycle indefinitely. If $s = \\frac{2}{3}$, the recurrence for $y$ becomes:\n$$\ny_{k+1} = \\left(1 - 3 \\cdot \\frac{2}{3}\\right) y_{k} = (1 - 2) y_{k} = -y_{k}\n$$\nLet's examine the sequence of iterates:\nFor $k=0$: $\\mathbf{r}_{0} = (0, y_{0})$.\nFor $k=1$: $y_{1} = -y_{0}$, so $\\mathbf{r}_{1} = (0, -y_{0})$.\nFor $k=2$: $y_{2} = -y_{1} = -(-y_{0}) = y_{0}$, so $\\mathbf{r}_{2} = (0, y_{0})$.\nFor $k=3$: $y_{3} = -y_{2} = -y_{0}$, so $\\mathbf{r}_{3} = (0, -y_{0})$.\nThe sequence of points is $(0, y_{0}), (0, -y_{0}), (0, y_{0}), (0, -y_{0}), \\dots$, which is precisely the specified two-point limit cycle. The algorithm never converges to the minimum at $(0,0)$.\n\nThe value of the constant step size $s$ that produces this behavior is $\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "2463052"}, {"introduction": "We have seen how Steepest Descent can struggle on even simple anisotropic potentials. This leads us to a more powerful method: the Conjugate Gradient (CG) algorithm, which is designed to accelerate convergence on precisely these kinds of difficult problems. This final practice [@problem_id:2463021] is a direct computational showdown between SD and CG. You will write code to compare their performance on a series of quadratic problems with systematically increasing difficulty, quantified by the matrix condition number $\\kappa$, to see firsthand the dramatic efficiency gains offered by CG.", "problem": "You are given the quadratic energy function $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$, where $\\mathbf{A}$ is a real, symmetric, positive definite matrix of size $n \\times n$. Consider two iterative methods for finding the minimizer of $E(\\mathbf{x})$: Steepest Descent (SD) and Conjugate Gradient (CG). For each method, define the iteration count for a given $\\mathbf{A}$ and initial point $\\mathbf{x}_0$ as the smallest nonnegative integer $m$ such that the residual-based stopping criterion\n$$\n\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon\n$$\nis satisfied, where $\\varepsilon$ is a prescribed tolerance and $\\lVert \\cdot \\rVert_2$ is the Euclidean norm. If the criterion is not satisfied within a prescribed iteration limit $m_{\\max}$, report $m_{\\max}$ as the iteration count.\n\nConstruct a family of diagonal test matrices $\\mathbf{A}_\\kappa$ with prescribed condition numbers by setting $n = 60$, choosing an initial vector $\\mathbf{x}_0 \\in \\mathbb{R}^n$ with entries all equal to $1$, and, for a given condition number $\\kappa \\ge 1$, defining the diagonal entries $\\lambda_i$ of $\\mathbf{A}_\\kappa$ as a geometric progression from $1$ to $\\kappa$:\n$$\n\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}, \\quad i = 1,2,\\dots,n,\n$$\nso that $\\mathbf{A}_\\kappa = \\mathrm{diag}(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)$ has $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\kappa$. Use the residual-based stopping tolerance $\\varepsilon = 10^{-8}$ and an iteration cap $m_{\\max} = 100000$ for both methods.\n\nTest Suite:\n- Matrix dimension: $n = 60$.\n- Initial vector: $\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{60}$.\n- Tolerance: $\\varepsilon = 10^{-8}$.\n- Iteration cap: $m_{\\max} = 100000$.\n- Condition numbers to test: $\\kappa \\in \\{1,5,20,100,500\\}$.\n\nFor each $\\kappa$ in the test suite, compute:\n- The iteration count $m_{\\mathrm{SD}}(\\kappa)$ required by Steepest Descent to satisfy the stopping criterion (or $m_{\\max}$ if not satisfied within the cap).\n- The iteration count $m_{\\mathrm{CG}}(\\kappa)$ required by Conjugate Gradient to satisfy the stopping criterion (or $m_{\\max}$ if not satisfied within the cap).\n\nFinal Output Format:\nYour program should produce a single line of output that is a comma-separated list enclosed in square brackets, with one inner pair per test case. Each inner pair must be the two integers $[m_{\\mathrm{SD}}(\\kappa),m_{\\mathrm{CG}}(\\kappa)]$ in the same order as the listed test condition numbers, and there must be no spaces anywhere in the line. Concretely, for the five specified test cases, the required output format is a single line of the form\n$$\n[[m_{\\mathrm{SD}}(1),m_{\\mathrm{CG}}(1)],[m_{\\mathrm{SD}}(5),m_{\\mathrm{CG}}(5)],[m_{\\mathrm{SD}}(20),m_{\\mathrm{CG}}(20)],[m_{\\mathrm{SD}}(100),m_{\\mathrm{CG}}(100)],[m_{\\mathrm{SD}}(500),m_{\\mathrm{CG}}(500)]].\n$$\nThe entries must be integers and must appear exactly as shown, with only square brackets and commas, and no spaces.", "solution": "The problem statement is critically analyzed and found to be valid.\n\n**1. Problem Validation**\n\n**Step 1: Extracted Givens**\n- Energy function: $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$.\n- Matrix $\\mathbf{A}$: a real, symmetric, positive definite matrix of size $n \\times n$.\n- Iterative methods: Steepest Descent (SD) and Conjugate Gradient (CG).\n- Iteration count $m$: smallest nonnegative integer where the stopping criterion is met.\n- Stopping criterion: $\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon$.\n- Tolerance: $\\varepsilon = 10^{-8}$.\n- Iteration limit: $m_{\\max} = 100000$.\n- Test Matrix Construction: For a condition number $\\kappa$, $\\mathbf{A}_\\kappa = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ with $\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}$ for $i=1, \\dots, n$.\n- Matrix dimension: $n = 60$.\n- Initial vector: $\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}}$.\n- Test Suite: $\\kappa \\in \\{1, 5, 20, 100, 500\\}$.\n- Task: Compute iteration counts $m_{\\mathrm{SD}}(\\kappa)$ and $m_{\\mathrm{CG}}(\\kappa)$ for each $\\kappa$.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, comparing the convergence rates of two fundamental optimization algorithms on a well-defined quadratic form. The minimization of $E(\\mathbf{x})$ is equivalent to solving the linear system $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$, for which the unique solution is $\\mathbf{x} = \\mathbf{0}$ as $\\mathbf{A}$ is positive definite. The residual-based stopping criterion is a standard practice in iterative methods.\n- **Well-Posed:** The problem is well-defined. The construction of the test matrices is unambiguous. The diagonal entries $\\lambda_i$ form a geometric progression from $\\lambda_1 = \\kappa^{\\frac{0}{n-1}} = 1$ to $\\lambda_n = \\kappa^{\\frac{n-1}{n-1}} = \\kappa$. For a symmetric positive definite matrix, the $2$-norm condition number is the ratio of the largest to the smallest eigenvalue, $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\kappa}{1} = \\kappa$. The construction is therefore correct. All parameters ($n, \\mathbf{x}_0, \\varepsilon, m_{\\max}, \\kappa$ values) are explicitly given. The existence and uniqueness of the solution are guaranteed.\n- **Objective:** The problem is formulated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically sound, well-posed, and objective. A solution will be provided.\n\n**2. Solution Derivation**\n\nThe problem requires the minimization of the quadratic function $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$. The minimum of this function occurs where its gradient with respect to $\\mathbf{x}$ is zero. The gradient is given by:\n$$\n\\nabla E(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\n$$\nSetting the gradient to zero, we obtain the linear system $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$. Since the matrix $\\mathbf{A}$ is specified as positive definite, it is invertible, and the unique minimizer of $E(\\mathbf{x})$ is the trivial solution $\\mathbf{x}^* = \\mathbf{0}$.\n\nThe problem is to find the number of iterations required for the Steepest Descent (SD) and Conjugate Gradient (CG) methods to converge to this solution, starting from $\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}}$. The convergence is measured by the stopping criterion $\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon$. The vector $\\mathbf{r}_m = \\mathbf{0} - \\mathbf{A}\\mathbf{x}_m = -\\mathbf{A}\\mathbf{x}_m$ is the residual for the system $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$. Thus, the criterion is equivalent to a relative residual stopping criterion: $\\frac{\\lVert \\mathbf{r}_m \\rVert_2}{\\lVert \\mathbf{r}_0 \\rVert_2} \\le \\varepsilon$.\n\nThe test matrices $\\mathbf{A}_\\kappa$ are diagonal with entries $\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}$ for $i=1, \\dots, n=60$. This construction ensures that the eigenvalues are logarithmically spaced between $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = \\kappa$, giving a matrix with a prescribed condition number $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\kappa$.\n\n**Steepest Descent (SD) Algorithm**\nThe Steepest Descent method is an iterative optimization algorithm that moves in the direction of the negative gradient at each step. For the function $E(\\mathbf{x})$, the search direction at iterate $\\mathbf{x}_k$ is $\\mathbf{p}_k = -\\nabla E(\\mathbf{x}_k) = -\\mathbf{A}\\mathbf{x}_k$. The next iterate is found by a line search: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$. The optimal step size $\\alpha_k$ that minimizes $E(\\mathbf{x}_{k+1})$ is given by:\n$$\n\\alpha_k = \\frac{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{p}_k}{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{p}_k} = \\frac{(-\\mathbf{A}\\mathbf{x}_k)^{\\mathsf{T}}(-\\mathbf{A}\\mathbf{x}_k)}{(-\\mathbf{A}\\mathbf{x}_k)^{\\mathsf{T}}\\mathbf{A}(-\\mathbf{A}\\mathbf{x}_k)}\n$$\nIf we define the residual $\\mathbf{r}_k = -\\mathbf{A}\\mathbf{x}_k$, the algorithm simplifies:\n1. Initialize $\\mathbf{x}_0$, $\\mathbf{r}_0 = -\\mathbf{A}\\mathbf{x}_0$.\n2. For $k=0, 1, 2, \\dots$:\n   a. Compute step size: $\\alpha_k = \\frac{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{r}_k}$.\n   b. Update solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{r}_k$.\n   c. Update residual: $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A}\\mathbf{r}_k$.\n   d. Check stopping criterion.\n\n**Conjugate Gradient (CG) Algorithm**\nThe Conjugate Gradient method improves upon Steepest Descent by choosing search directions that are $\\mathbf{A}$-orthogonal (or \"conjugate\"). This prevents spoiling minimization progress made in previous directions and guarantees convergence in at most $n$ steps in exact arithmetic.\n1. Initialize $\\mathbf{x}_0$, $\\mathbf{r}_0 = -\\mathbf{A}\\mathbf{x}_0$, $\\mathbf{p}_0 = \\mathbf{r}_0$.\n2. For $k=0, 1, 2, \\dots$:\n   a. Compute step size: $\\alpha_k = \\frac{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{p}_k}$.\n   b. Update solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n   c. Update residual: $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A}\\mathbf{p}_k$.\n   d. Check stopping criterion.\n   e. Compute improvement factor: $\\beta_k = \\frac{\\mathbf{r}_{k+1}^{\\mathsf{T}}\\mathbf{r}_{k+1}}{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}$.\n   f. Update search direction: $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$.\n\nThe implementation will follow these algorithms. It will check the stopping criterion for each iterate $\\mathbf{x}_m$ starting from $m=1$ up to $m_{\\max}$, and report the first value of $m$ for which the criterion is satisfied. If the loop completes, $m_{\\max}$ is reported. Since the matrices $\\mathbf{A}_\\kappa$ are diagonal, the matrix-vector products $\\mathbf{A}\\mathbf{v}$ are efficiently computed as element-wise products of the diagonal entries (eigenvalues) with the vector $\\mathbf{v}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_sd(lambdas, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=0 using Steepest Descent for a diagonal matrix A.\n\n    Args:\n        lambdas (np.ndarray): Diagonal entries of matrix A.\n        x0 (np.ndarray): Initial vector.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required for convergence.\n    \"\"\"\n    x = x0.copy()\n    \n    # The term in the stopping criterion is v_m = A @ x_m.\n    # We use the fact that r_m = -v_m for the system Ax=0.\n    v0 = lambdas * x0\n    norm_v0 = np.linalg.norm(v0)\n    \n    # If x0 is the solution, iterations = 0.\n    if norm_v0 == 0:\n        return 0\n\n    # For SD, the search direction is the residual r.\n    # For Ax=0, r = 0 - Ax = -Ax.\n    r = -v0\n    \n    for m in range(1, max_iter + 1):\n        # Efficiently compute A @ r since A is diagonal\n        Ar = lambdas * r\n        \n        # Calculate optimal step size alpha\n        # np.dot is used for vector dot products.\n        r_sq_norm = np.dot(r, r)\n        alpha = r_sq_norm / np.dot(r, Ar)\n        \n        # Update solution and residual\n        x += alpha * r\n        r -= alpha * Ar\n        \n        # Check stopping criterion: ||A*x_m|| / ||A*x_0|| <= tol\n        # Since r_m = -A*x_m, we can use norm(r) which is |r_m|.\n        norm_v_m = np.linalg.norm(r)\n        \n        if norm_v_m / norm_v0 <= tol:\n            return m\n            \n    return max_iter\n\ndef solve_cg(lambdas, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=0 using Conjugate Gradient for a diagonal matrix A.\n\n    Args:\n        lambdas (np.ndarray): Diagonal entries of matrix A.\n        x0 (np.ndarray): Initial vector.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required for convergence.\n    \"\"\"\n    x = x0.copy()\n    \n    v0 = lambdas * x0\n    norm_v0 = np.linalg.norm(v0)\n    \n    if norm_v0 == 0:\n        return 0\n    \n    r = -v0\n    p = r.copy() # Initial search direction\n    rs_old = np.dot(r, r)\n    \n    for m in range(1, max_iter + 1):\n        # Efficiently compute A @ p\n        Ap = lambdas * p\n        \n        alpha = rs_old / np.dot(p, Ap)\n        \n        # Update solution and residual\n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        # Check stopping criterion using the norm of the new residual\n        norm_v_m = np.sqrt(rs_new)\n        if norm_v_m / norm_v0 <= tol:\n            return m\n            \n        # Update search direction\n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n            \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [1.0, 5.0, 20.0, 100.0, 500.0]\n\n    # Parameters from problem description\n    n = 60\n    x0 = np.ones(n)\n    tol = 1e-8\n    max_iter = 100000\n    \n    results = []\n    for kappa in test_cases:\n        # Construct the diagonal entries (eigenvalues) of matrix A\n        if kappa == 1.0:\n            lambdas = np.ones(n)\n        else:\n            i = np.arange(1, n + 1)\n            power = (i - 1) / (n - 1)\n            lambdas = np.power(kappa, power)\n        \n        # Run Steepest Descent and Conjugate Gradient methods\n        m_sd = solve_sd(lambdas, x0, tol, max_iter)\n        m_cg = solve_cg(lambdas, x0, tol, max_iter)\n        \n        result_str = f\"[{m_sd},{m_cg}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2463021"}]}