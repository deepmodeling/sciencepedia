{"hands_on_practices": [{"introduction": "Before diving into the complexities of optimization algorithms, it is crucial to understand their potential points of failure. This thought experiment [@problem_id:2461264] probes the behavior of a standard line-search quasi-Newton method when initiated at an exact stationary point, specifically a local maximum. By reasoning from first principles about the gradient, $\\nabla f$, and the resulting search direction, $\\mathbf{p}_k = -H_k \\nabla f(\\mathbf{x}_k)$, you will uncover a fundamental behavior that highlights why robust optimization methods must handle all types of stationary points gracefully.", "problem": "In molecular geometry optimization within computational chemistry, a common goal is to find a local minimum of a potential energy surface. Consider the smooth potential surface given by the perfectly symmetric hill $$f(x,y) = -x^2 - y^2,$$ and suppose you attempt to minimize it starting from the exact top at $$x = 0, \\; y = 0.$$ You use a standard line-search quasi-Newton method designed for unconstrained minimization (for example, a Broyden–Fletcher–Goldfarb–Shanno method), with an initial inverse metric that is positive definite and a typical Wolfe-type line search. Assume exact arithmetic and no perturbations or regularization.\n\nFrom first principles, recall that a stationary point satisfies $\\nabla f = \\mathbf{0}$, that a descent direction $p$ for a differentiable function at a point with gradient $g$ must satisfy $g^\\top p  0$, and that line-search methods generate trial steps by choosing a direction and then selecting a step length to reduce the objective.\n\nWhich outcome best describes what happens and why?\n\nA. The method immediately terminates at the starting point because the computed search direction is the zero vector and the line search cannot reduce the objective; it reports convergence to a stationary point even though the point is a maximizer.\n\nB. The method takes a step of arbitrary nonzero length along an arbitrary direction because the negative-definite curvature forces the line search to move to the boundary of a trust region, escaping the top.\n\nC. The method computes a Newton step that initially points uphill and is rejected by the Wolfe conditions; this triggers an update that makes the metric negative definite, after which the method rapidly finds a nearby minimum.\n\nD. The method enters a cycling behavior, alternating between orthogonal directions with small steps due to the symmetry of the surface, until numerical noise breaks the cycle.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\nStep 1: Extract Givens\n- Objective function: a smooth potential surface given by $$f(x,y) = -x^2 - y^2$$.\n- Starting point: $$(x_0, y_0) = (0, 0)$$.\n- Optimization algorithm: A standard line-search quasi-Newton method (e.g., BFGS).\n- Initial inverse metric (inverse Hessian approximation) $$H_0$$: Positive definite.\n- Line search conditions: Wolfe-type.\n- Assumptions: Exact arithmetic, no perturbations or regularization.\n- Provided definitions for recall:\n    - Stationary point condition: $\\nabla f = \\mathbf{0}$.\n    - Descent direction $p$ for gradient $g$: $g^\\top p  0$.\n    - Line-search method operation: Choose a direction, then select a step length.\n\nStep 2: Validate Using Extracted Givens\nThe problem describes a classic test case for an unconstrained optimization algorithm. The function $$f(x,y)$$ is a simple, infinitely differentiable quadratic function. The starting point is its unique stationary point. The algorithm specified is a standard workhorse in numerical optimization and computational chemistry. The initial conditions (positive definite $$H_0$$) and assumptions (exact arithmetic) are standard for theoretical analysis of such algorithms.\n\n- **Scientific Grounding**: The problem is well-grounded in the theory of numerical optimization and its application to computational chemistry. The concepts used are fundamental and correctly stated.\n- **Well-Posedness**: The problem is well-posed. The function, starting point, and algorithm type are clearly defined, leading to a determinable outcome.\n- **Objectivity**: The problem is stated in precise, objective mathematical language.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, formalizable problem in numerical analysis.\n\nStep 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution derivation.\n\nThe task is to determine the outcome of applying a line-search quasi-Newton method to minimize the function $$f(x,y) = -x^2 - y^2$$, starting from the point $$(0,0)$$.\n\nLet the vector of variables be $\\mathbf{x} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$. The objective function is $f(\\mathbf{x}) = -\\mathbf{x}^\\top \\mathbf{x}$. The starting point is $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nA line-search quasi-Newton iteration proceeds as follows:\n1.  At step $k$, compute the gradient $g_k = \\nabla f(x_k)$.\n2.  Check for convergence. A standard condition for termination is that the norm of the gradient is sufficiently small, i.e., $\\|g_k\\|  \\epsilon$, where $\\epsilon$ is a small positive tolerance.\n3.  Compute a search direction $p_k$ using the current approximation of the inverse Hessian, $H_k$. The direction is given by $p_k = -H_k g_k$.\n4.  Perform a line search to find a suitable step length $\\alpha_k  0$ such that the Wolfe conditions are satisfied for the new point $x_{k+1} = x_k + \\alpha_k p_k$.\n5.  Update the position to $x_{k+1}$ and update the inverse Hessian approximation to $H_{k+1}$.\n\nLet us analyze the first iteration ($k=0$).\nThe starting point is $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nFirst, we compute the gradient of $f(x,y) = -x^2 - y^2$.\nThe gradient is $g(x,y) = \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} -2x \\\\ -2y \\end{pmatrix}$.\n\nAt the starting point $\\mathbf{x}_0$, the gradient is:\n$$g_0 = g(0,0) = \\begin{pmatrix} -2(0) \\\\ -2(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\mathbf{0}$$\n\nThe problem states that we are using exact arithmetic. The gradient at the starting point is exactly the zero vector.\nThe first step of the algorithm is to check for convergence. Since $\\|g_0\\| = 0$, the convergence criterion $\\|g_k\\|  \\epsilon$ is satisfied for any $\\epsilon  0$. Therefore, the algorithm must terminate immediately at iteration $k=0$.\n\nIf, for a moment, we ignored the termination check and proceeded to calculate the search direction, we would have:\n$$p_0 = -H_0 g_0$$\nSince $g_0 = \\mathbf{0}$, the search direction is $p_0 = -H_0 \\mathbf{0} = \\mathbf{0}$. The search direction is the zero vector, regardless of the initial inverse Hessian approximation $H_0$.\n\nWith a zero search direction, any subsequent step is of the form $x_1 = x_0 + \\alpha_0 p_0 = x_0 + \\alpha_0 \\mathbf{0} = x_0$. No movement from the starting point is possible, and the objective function value cannot be changed, let alone reduced. A line search for $\\alpha_0 > 0$ would fail to find any step that produces a strict decrease in $f$.\n\nThe nature of the stationary point is determined by the Hessian matrix of second derivatives:\n$$H_f(x,y) = \\nabla^2 f(x,y) = \\begin{pmatrix} -2  0 \\\\ 0  -2 \\end{pmatrix}$$\nThe eigenvalues of this matrix are both $-2$. Since the eigenvalues are all negative, the Hessian is negative definite, and the stationary point at $(0,0)$ is a strict local maximum. The algorithm is intended for minimization, but it has found a stationary point. It is not the algorithm's primary responsibility to classify the point; it is designed to find points where the gradient is zero.\n\nBased on this analysis, the method will immediately stop and report that it has converged to a stationary point.\n\nNow, we evaluate the given options.\n\nA. The method immediately terminates at the starting point because the computed search direction is the zero vector and the line search cannot reduce the objective; it reports convergence to a stationary point even though the point is a maximizer.\n- This statement accurately describes the sequence of events. The gradient is zero, so the algorithm terminates based on the standard $\\|g\\|=0$ criterion. If it were to proceed, the search direction $p_0$ would be the zero vector. No move can be made to reduce the objective function. The algorithm reports convergence because it has found a point with zero gradient. This point is indeed a maximizer.\n- Verdict: **Correct**.\n\nB. The method takes a step of arbitrary nonzero length along an arbitrary direction because the negative-definite curvature forces the line search to move to the boundary of a trust region, escaping the top.\n- This option incorrectly describes the algorithm. The problem specifies a line-search method, not a trust-region method. Trust-region methods have mechanisms to handle non-positive definite Hessians, but they operate differently. In a line-search method, the search direction is computed first ($p_0 = \\mathbf{0}$ in this case), and then a step length is sought. The direction is not arbitrary; it is deterministically zero. The concept of a trust region boundary is not applicable.\n- Verdict: **Incorrect**.\n\nC. The method computes a Newton step that initially points uphill and is rejected by the Wolfe conditions; this triggers an update that makes the metric negative definite, after which the method rapidly finds a nearby minimum.\n- The method computes a quasi-Newton step, not a Newton step. The step direction is $p_0 = -H_0 g_0 = \\mathbf{0}$, which is the zero vector. A direction $p$ is \"uphill\" only if $g^\\top p  0$. Here, $g_0^\\top p_0 = \\mathbf{0}^\\top \\mathbf{0} = 0$. The direction is not uphill. Since no step is taken ($s_0 = x_1 - x_0 = \\mathbf{0}$), the Hessian approximation is not updated. The algorithm terminates, so it does not proceed to find any other point.\n- Verdict: **Incorrect**.\n\nD. The method enters a cycling behavior, alternating between orthogonal directions with small steps due to the symmetry of the surface, until numerical noise breaks the cycle.\n- The algorithm terminates at the first step because the gradient is zero. It does not take any steps, small or otherwise. Therefore, no cycling behavior can occur. The problem also explicitly assumes \"exact arithmetic,\" which precludes the influence of numerical noise.\n- Verdict: **Incorrect**.\n\nThe only correct description of the outcome is provided in option A.", "answer": "$$\\boxed{A}$$", "id": "2461264"}, {"introduction": "Trust-region methods enhance optimization stability by defining a region around the current iterate where a quadratic model of the potential energy surface is considered reliable. This exercise [@problem_id:2461214] provides an opportunity to master the core mechanics of solving the trust-region subproblem by hand. The problem is simplified by using a diagonal Hessian approximation, $\\mathbf{B}_k$, which makes the underlying linear algebra tractable and allows you to focus on the essential decision-making process: evaluating the unconstrained step and, if it is too large, finding the optimal constrained step on the boundary of the trust region.", "problem": "In molecular geometry optimization on a Potential Energy Surface (PES), consider a trust-region step computed from a local quadratic model. At iterate $\\mathbf{x}_k \\in \\mathbb{R}^3$, the model is\n$$\nm_k(\\mathbf{s}) \\;=\\; E(\\mathbf{x}_k) \\;+\\; \\mathbf{g}_k^{\\top}\\mathbf{s} \\;+\\; \\frac{1}{2}\\,\\mathbf{s}^{\\top}\\mathbf{B}_k\\,\\mathbf{s},\n$$\nwhere $\\mathbf{g}_k$ is the gradient of the energy and $\\mathbf{B}_k$ is a symmetric positive definite Hessian approximation obtained from a quasi-Newton update such as the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method. The trust-region subproblem is\n$$\n\\min_{\\mathbf{s}\\in\\mathbb{R}^3} \\; \\mathbf{g}_k^{\\top}\\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^{\\top}\\mathbf{B}_k\\,\\mathbf{s}\n\\quad \\text{subject to} \\quad \\|\\mathbf{s}\\|_2 \\le \\Delta_k.\n$$\nSuppose that, due to the choice of internal coordinates and preconditioning, the Hessian approximation is diagonal,\n$$\n\\mathbf{B}_k \\;=\\; \\mathrm{diag}\\!\\big(2,\\,5,\\,7\\big),\n$$\nthe gradient is\n$$\n\\mathbf{g}_k \\;=\\; \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix},\n$$\nand the trust-region radius is\n$$\n\\Delta_k \\;=\\; 1.\n$$\nAll quantities are given in consistent, dimensionless internal units.\n\nCompute the exact predicted model decrease\n$$\nm_k(\\mathbf{0}) \\;-\\; m_k(\\mathbf{s}^\\star),\n$$\nwhere $\\mathbf{s}^\\star$ is the exact global solution of the trust-region subproblem above. Provide your final answer as a single real number. Do not round.", "solution": "The problem provided is a well-posed and scientifically sound exercise in numerical optimization, specific to trust-region methods used in computational chemistry. All necessary data are provided, and there are no internal contradictions or ambiguities. The problem is valid and can be solved.\n\nThe objective is to compute the exact predicted model decrease, which is given by the expression $m_k(\\mathbf{0}) - m_k(\\mathbf{s}^\\star)$. Using the definition of the quadratic model $m_k(\\mathbf{s})$, we have:\n$$\nm_k(\\mathbf{s}) = E(\\mathbf{x}_k) + \\mathbf{g}_k^{\\top}\\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^{\\top}\\mathbf{B}_k\\,\\mathbf{s}\n$$\nAt the current point $\\mathbf{s} = \\mathbf{0}$, the model value is $m_k(\\mathbf{0}) = E(\\mathbf{x}_k)$.\nThe predicted decrease is thus:\n$$\nm_k(\\mathbf{0}) - m_k(\\mathbf{s}^\\star) = E(\\mathbf{x}_k) - \\left( E(\\mathbf{x}_k) + \\mathbf{g}_k^{\\top}\\mathbf{s}^\\star + \\frac{1}{2}\\,(\\mathbf{s}^\\star)^{\\top}\\mathbf{B}_k\\,\\mathbf{s}^\\star \\right) = - \\left( \\mathbf{g}_k^{\\top}\\mathbf{s}^\\star + \\frac{1}{2}\\,(\\mathbf{s}^\\star)^{\\top}\\mathbf{B}_k\\,\\mathbf{s}^\\star \\right)\n$$\nTo evaluate this quantity, we must first determine the optimal step $\\mathbf{s}^\\star$, which is the solution to the trust-region subproblem:\n$$\n\\min_{\\mathbf{s}\\in\\mathbb{R}^3} \\; \\mathbf{g}_k^{\\top}\\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^{\\top}\\mathbf{B}_k\\,\\mathbf{s} \\quad \\text{subject to} \\quad \\|\\mathbf{s}\\|_2 \\le \\Delta_k\n$$\nSince the Hessian approximation $\\mathbf{B}_k = \\mathrm{diag}(2, 5, 7)$ has positive eigenvalues ($2$, $5$, and $7$), it is a positive definite matrix. This ensures that the objective function is strictly convex, and the constrained problem has a unique global solution. The solution is characterized by the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(\\mathbf{s}, \\lambda) = \\mathbf{g}_k^{\\top}\\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^{\\top}\\mathbf{B}_k\\,\\mathbf{s} + \\frac{\\lambda}{2}(\\|\\mathbf{s}\\|_2^2 - \\Delta_k^2)\n$$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier. The stationarity condition, $\\nabla_{\\mathbf{s}}\\mathcal{L}(\\mathbf{s}, \\lambda) = \\mathbf{0}$, gives the secular equation:\n$$\n(\\mathbf{B}_k + \\lambda \\mathbf{I})\\mathbf{s} = -\\mathbf{g}_k\n$$\nWe first examine the unconstrained solution, which corresponds to setting $\\lambda = 0$. The unconstrained step, often called the full Newton step, is $\\mathbf{s}_N = -\\mathbf{B}_k^{-1}\\mathbf{g}_k$. If $\\|\\mathbf{s}_N\\|_2 \\le \\Delta_k$, then this is the solution $\\mathbf{s}^\\star$.\n\nThe given data are $\\mathbf{g}_k = (3, 0, 0)^{\\top}$, $\\mathbf{B}_k = \\mathrm{diag}(2, 5, 7)$, and $\\Delta_k = 1$.\nThe inverse of $\\mathbf{B}_k$ is $\\mathbf{B}_k^{-1} = \\mathrm{diag}(\\frac{1}{2}, \\frac{1}{5}, \\frac{1}{7})$.\nWe compute the unconstrained step:\n$$\n\\mathbf{s}_N = - \\begin{pmatrix} 1/2  0  0 \\\\ 0  1/5  0 \\\\ 0  0  1/7 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -3/2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nWe check its norm: $\\|\\mathbf{s}_N\\|_2 = \\sqrt{(-3/2)^2 + 0^2 + 0^2} = \\frac{3}{2}$.\nSince $\\|\\mathbf{s}_N\\|_2 = \\frac{3}{2}  \\Delta_k = 1$, the unconstrained step lies outside the trust region. Therefore, the solution $\\mathbf{s}^\\star$ must lie on the boundary of the trust region, meaning $\\|\\mathbf{s}^\\star\\|_2 = \\Delta_k = 1$. This implies that the Lagrange multiplier $\\lambda$ must be strictly positive, i.e., $\\lambda  0$.\n\nThe solution is given by $\\mathbf{s}^\\star = -(\\mathbf{B}_k + \\lambda \\mathbf{I})^{-1}\\mathbf{g}_k$. Since $\\mathbf{B}_k$ is diagonal, this is straightforward to compute:\n$$\n\\mathbf{B}_k + \\lambda \\mathbf{I} = \\mathrm{diag}(2+\\lambda, 5+\\lambda, 7+\\lambda)\n$$\n$$\n\\mathbf{s}^\\star(\\lambda) = - \\begin{pmatrix} \\frac{1}{2+\\lambda}  0  0 \\\\ 0  \\frac{1}{5+\\lambda}  0 \\\\ 0  0  \\frac{1}{7+\\lambda} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -3/(2+\\lambda) \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nWe use the constraint $\\|\\mathbf{s}^\\star\\|_2^2 = \\Delta_k^2 = 1^2 = 1$ to find $\\lambda$:\n$$\n\\left( \\frac{-3}{2+\\lambda} \\right)^2 + 0^2 + 0^2 = 1 \\implies \\frac{9}{(2+\\lambda)^2} = 1\n$$\nThis gives $(2+\\lambda)^2 = 9$, so $2+\\lambda = \\pm 3$. The possible values for $\\lambda$ are $\\lambda = 1$ and $\\lambda = -5$.\nThe KKT conditions require $\\lambda \\ge 0$. Furthermore, for the solution to be a minimum, the matrix $\\mathbf{B}_k + \\lambda \\mathbf{I}$ must be positive semi-definite, which requires its eigenvalues ($2+\\lambda, 5+\\lambda, 7+\\lambda$) to be non-negative. This implies $2+\\lambda \\ge 0$, or $\\lambda \\ge -2$.\nThe only value satisfying these conditions is $\\lambda = 1$.\n\nSubstituting $\\lambda=1$ into the expression for $\\mathbf{s}^\\star$ gives the optimal step:\n$$\n\\mathbf{s}^\\star = \\begin{pmatrix} -3/(2+1) \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow we compute the predicted decrease using this step:\n$$\n\\text{Predicted Decrease} = - \\left( \\mathbf{g}_k^{\\top}\\mathbf{s}^\\star + \\frac{1}{2}\\,(\\mathbf{s}^\\star)^{\\top}\\mathbf{B}_k\\,\\mathbf{s}^\\star \\right)\n$$\nThe first term is the dot product of the gradient and the step:\n$$\n\\mathbf{g}_k^{\\top}\\mathbf{s}^\\star = \\begin{pmatrix} 3  0  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = -3\n$$\nThe second term is the quadratic part:\n$$\n(\\mathbf{s}^\\star)^{\\top}\\mathbf{B}_k\\,\\mathbf{s}^\\star = \\begin{pmatrix} -1  0  0 \\end{pmatrix} \\begin{pmatrix} 2  0  0 \\\\ 0  5  0 \\\\ 0  0  7 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1  0  0 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 0 \\\\ 0 \\end{pmatrix} = 2\n$$\nSubstituting these values back into the expression for the predicted decrease:\n$$\n\\text{Predicted Decrease} = - \\left( -3 + \\frac{1}{2}(2) \\right) = -(-3 + 1) = -(-2) = 2\n$$\nThe exact predicted model decrease is $2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2461214"}, {"introduction": "Translating algorithmic theory into working code is a vital skill in computational science. This hands-on coding challenge [@problem_id:2461247] guides you through the implementation of a complete one-dimensional trust-region algorithm. You will apply your code to a classic double-well potential, a system where simpler gradient-based methods can easily fail by overshooting the minimum or ascending towards a maximum. By programming the logic for solving the subproblem, evaluating step quality with the agreement ratio $\\rho_k$, and updating the trust radius $\\Delta_k$, you gain a concrete understanding of how this powerful method navigates a complex energy landscape.", "problem": "Consider the univariate objective function in dimensionless units given by $f(x)=x^4-x^2$ with first derivative $f'(x)=4x^3-2x$ and second derivative $f''(x)=12x^2-2$. The stationary points are the local maxima at $x=0$ and the local minima at $x_{\\pm}=\\pm 1/\\sqrt{2}$. For an iterate $x_k\\in\\mathbb{R}$, define the quadratic model $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ and a trust-region radius $\\Delta_k0$. The trust-region subproblem is to find a step $s_k\\in\\mathbb{R}$ that minimizes $m_k(s)$ subject to the constraint $\\lvert s\\rvert\\le \\Delta_k$. Let the predicted reduction be $\\operatorname{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$ and the actual reduction be $\\operatorname{ared}_k=f(x_k)-f(x_k+s_k)$. Define the acceptance ratio $\\rho_k=\\operatorname{ared}_k/\\operatorname{pred}_k$ when $\\operatorname{pred}_k0$; if $\\operatorname{pred}_k\\le 0$, set $\\rho_k=-\\infty$. A step is accepted if $\\rho_k\\ge \\eta_1$ and rejected otherwise. The trust-region radius is updated by the following rule with fixed parameters $\\eta_1\\in(0,1)$, $\\eta_2\\in(\\eta_1,1)$, $\\gamma_1\\in(0,1)$, and $\\gamma_21$:\n- If $\\rho_k\\eta_1$, set $\\Delta_{k+1}=\\gamma_1\\Delta_k$.\n- If $\\rho_k\\ge \\eta_2$ and $\\lvert s_k\\rvert\\ge 0.8\\,\\Delta_k$, set $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$.\n- Otherwise, set $\\Delta_{k+1}=\\Delta_k$.\nIf a step is rejected, keep $x_{k+1}=x_k$; if it is accepted, set $x_{k+1}=x_k+s_k$. The iteration terminates when either $\\lvert f'(x_k)\\rvert\\le \\varepsilon_g$, $\\lvert s_k\\rvert\\le \\varepsilon_s$ for an accepted step, $\\Delta_k\\le \\varepsilon_s$, or when a fixed iteration cap is reached.\n\nIn one spatial dimension, the unique global minimizer of the quadratic model subject to $\\lvert s\\rvert\\le \\Delta$ is characterized as follows. For given $g\\in\\mathbb{R}$ and $h\\in\\mathbb{R}$ with $g=f'(x)$ and $h=f''(x)$,\n- If $h0$ and the unconstrained minimizer $s_N=-g/h$ satisfies $\\lvert s_N\\rvert\\le \\Delta$, then $s^\\star=s_N$; otherwise $s^\\star=-\\operatorname{sign}(g)\\,\\Delta$.\n- If $h\\le 0$, then $s^\\star=\\Delta$ when $g0$, $s^\\star=-\\Delta$ when $g0$, and $s^\\star=\\Delta$ when $g=0$.\n\nFix the numerical parameters $\\eta_1=0.1$, $\\eta_2=0.9$, $\\gamma_1=0.25$, $\\gamma_2=2$, $\\Delta_{\\max}=10$, gradient tolerance $\\varepsilon_g=10^{-8}$, step tolerance $\\varepsilon_s=10^{-10}$, and a maximum of $100$ iterations. For each test case below, start from the given initial point $x_0$ with the given initial radius $\\Delta_0$, and apply the above iteration using the one-dimensional model minimizer $s_k$ at each step.\n\nDefine three scalar outputs for each test case:\n- $b_1$: the logical value that is true if and only if no accepted step ever increases the objective, that is, for all accepted steps $k$, $f(x_{k+1})\\le f(x_k)$.\n- $b_2$: the logical value that is true if and only if the full unconstrained Newton step at the first iterate, $s_N=-f'(x_0)/f''(x_0)$ (when $f''(x_0)\\ne 0$; define $b_2$ as false if $f''(x_0)=0$), produces a higher objective value, that is, $f(x_0+s_N)f(x_0)$.\n- $d$: the absolute distance from the final accepted iterate $x_{\\mathrm{final}}$ to the nearest local minimizer, $d=\\min\\{\\lvert x_{\\mathrm{final}}-1/\\sqrt{2}\\rvert,\\lvert x_{\\mathrm{final}}+1/\\sqrt{2}\\rvert\\}$.\n\nTest suite parameters to evaluate:\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list $[b_1,b_2,d]$ where $b_1$ and $b_2$ are the lowercase strings \"true\" or \"false\", and $d$ is a decimal-rounded float with exactly six digits after the decimal point. The final output must therefore be a single line of the form\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$,\nwith no spaces anywhere in the line.", "solution": "The problem is valid. It presents a well-posed, self-contained, and scientifically sound exercise in numerical optimization, specifically the application of a trust-region algorithm to a one-dimensional potential energy function. All necessary parameters, algorithmic rules, functions, and termination criteria are provided with mathematical precision. We shall proceed with a complete solution.\n\nThe core of this problem is to implement and analyze a trust-region optimization algorithm. This class of methods is fundamental in computational sciences, particularly in computational chemistry for locating stable molecular geometries, which correspond to minima on a potential energy surface. The objective function $f(x) = x^4 - x^2$ is a canonical one-dimensional double-well potential, representing a system with two stable states (minima) at $x_{\\pm} = \\pm 1/\\sqrt{2}$ and an unstable transition state (maximum) at $x=0$.\n\nA trust-region algorithm iteratively finds a minimum by constructing a simpler model of the objective function, which is trusted only within a neighborhood of the current iterate $x_k$. This neighborhood is a \"trust region\" of radius $\\Delta_k$. The model is a quadratic function, $m_k(s)$, derived from a second-order Taylor expansion of $f(x)$ around $x_k$:\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\nwhere $s$ is the step from $x_k$. This model is minimized with respect to $s$ subject to the constraint $\\lvert s \\rvert \\le \\Delta_k$. This constrained minimization is called the trust-region subproblem.\n\nThe solution to the one-dimensional subproblem, as provided, depends on the curvature of the model, given by the second derivative $h = f''(x_k)$.\n1.  If $h  0$, the model is convex (a parabola opening upwards). The unconstrained minimizer is the Newton step $s_N = -g/h$, where $g = f'(x_k)$. If this step lies within the trust region, i.e., $\\lvert s_N \\rvert \\le \\Delta_k$, it is the optimal step $s_k$. Otherwise, the model is minimized at the boundary of the trust region, $s_k = -\\operatorname{sign}(g)\\Delta_k$, moving as far as possible in the direction of steepest descent.\n2.  If $h \\le 0$, the model is locally concave or linear. Its minimum over the interval $[-\\Delta_k, \\Delta_k]$ must lie at one of the boundaries. The selection between $s = \\Delta_k$ and $s = -\\Delta_k$ is determined by the sign of the gradient $g$, which dictates the direction of descent.\n\nOnce the trial step $s_k$ is computed, its quality is assessed by comparing the *actual reduction* in the objective function, $\\operatorname{ared}_k = f(x_k) - f(x_k + s_k)$, to the *predicted reduction* from the model, $\\operatorname{pred}_k = m_k(0) - m_k(s_k)$. Their ratio, $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$, measures the fidelity of the model.\n\n-   If $\\rho_k$ is close to $1$, the model is an excellent predictor. The step is accepted, and we may expand the trust region ($\\Delta_{k+1} = \\gamma_2 \\Delta_k$) to allow for more aggressive steps, provided the current step was already near the trust region boundary.\n-   If $\\rho_k$ is positive but not large, the model is adequate. The step is accepted, but the trust region size is maintained ($\\Delta_{k+1} = \\Delta_k$).\n-   If $\\rho_k$ is small or negative, the model is poor. The step is rejected ($x_{k+1} = x_k$), and the trust region is shrunk ($\\Delta_{k+1} = \\gamma_1 \\Delta_k$) to improve model accuracy in the subsequent iteration.\n\nThe step acceptance rule is $\\rho_k \\ge \\eta_1$. Since $\\eta_1 = 0.1  0$ and the solution to the subproblem ensures $\\operatorname{pred}_k \\ge 0$, any accepted step must have $\\operatorname{ared}_k \\ge \\eta_1 \\operatorname{pred}_k \\ge 0$. If $\\operatorname{pred}_k  0$, then $\\operatorname{ared}_k  0$, guaranteeing that $f(x_{k+1})  f(x_k)$. A step can only be accepted if the model predicts descent ($\\operatorname{pred}_k  0$). Therefore, the condition for $b_1$—that no accepted step ever increases the objective function—is guaranteed to be true by the very construction of this algorithm. Any deviation would indicate a flawed implementation.\n\nThe output $b_2$ probes the limitation of the pure Newton-Raphson method. The unconstrained Newton step, $s_N = -f'(x_0)/f''(x_0)$, finds the extremum of the quadratic model. Near a maximum, such as at $x_0=0.0$ or $x_0=0.1$, the Hessian $f''(x_0)$ is negative. The Newton step thus seeks the *maximum* of the local quadratic model, which is a poor strategy for minimizing the global function $f(x)$ and is likely to result in an ascent step, i.e., $f(x_0+s_N)  f(x_0)$. The trust-region framework corrects this deficiency by constraining the step size.\n\nThe final output, $d$, measures the accuracy of convergence to one of the true minimizers, $x_{\\pm} = \\pm 1/\\sqrt{2}$. Given the initial points are all non-negative, the algorithm is expected to converge to the positive minimizer, $x_+ = 1/\\sqrt{2}$.\n\nThe implementation will proceed by first defining the objective function and its derivatives. Then, for each test case, the value of $b_2$ is computed. The main iterative loop is then executed, which involves, at each step $k$: checking for termination, solving the trust-region subproblem for $s_k$, evaluating the step quality via $\\rho_k$, and updating the state variables $x_k$ and $\\Delta_k$ according to the specified rules. The value of $b_1$ is tracked throughout the iteration. Upon termination, the final distance $d$ is computed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region optimization problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x):\n        return x**4 - x**2\n\n    def f_prime(x):\n        return 4 * x**3 - 2 * x\n\n    def f_double_prime(x):\n        return 12 * x**2 - 2\n\n    # --- Algorithm Parameters ---\n    eta1 = 0.1\n    eta2 = 0.9\n    gamma1 = 0.25\n    gamma2 = 2.0\n    delta_max = 10.0\n    eps_g = 1e-8\n    eps_s = 1e-10\n    max_iter = 100\n    \n    minimizers = [-1/np.sqrt(2), 1/np.sqrt(2)]\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.1, 0.05),\n        (0.1, 2.0),\n        (0.0, 0.5),\n        (0.1, 0.001)\n    ]\n\n    results = []\n\n    for x0, delta0 in test_cases:\n        # --- Output variable initialization ---\n        b1_flag = True\n        \n        # --- Calculate b2 before starting iterations ---\n        g0 = f_prime(x0)\n        h0 = f_double_prime(x0)\n        \n        if h0 == 0:\n            b2 = False\n        else:\n            s_N = -g0 / h0\n            b2 = f(x0 + s_N)  f(x0)\n            \n        # --- Main Trust-Region Loop ---\n        x_k = x0\n        delta_k = delta0\n        final_x = x0 # Will hold the last accepted iterate value\n        \n        for _ in range(max_iter):\n            g_k = f_prime(x_k)\n            \n            # --- Termination check 1: Gradient ---\n            if abs(g_k) = eps_g:\n                final_x = x_k\n                break\n\n            # --- Solve the trust-region subproblem ---\n            h_k = f_double_prime(x_k)\n            s_k = 0.0\n            \n            if h_k  0:\n                s_N = -g_k / h_k\n                if abs(s_N) = delta_k:\n                    s_k = s_N\n                else:\n                    s_k = -np.sign(g_k) * delta_k\n            else: # h_k = 0\n                if g_k  0:\n                    s_k = delta_k\n                elif g_k  0:\n                    s_k = -delta_k\n                else: # g_k == 0\n                    s_k = delta_k\n\n            # --- Evaluate step quality ---\n            pred_k = -(g_k * s_k + 0.5 * h_k * s_k**2)\n            ared_k = f(x_k) - f(x_k + s_k)\n            \n            rho_k = 0.0\n            # Use small tolerance for pred_k to avoid division by zero instability\n            if pred_k  1e-12: # Check for meaningful predicted reduction\n                rho_k = ared_k / pred_k\n            else:\n                rho_k = -np.inf # Model predicts no improvement or a trivial step\n\n            delta_kp1 = delta_k\n            \n            # --- Step acceptance/rejection and state update ---\n            if rho_k = eta1: # Accept step\n                if f(x_k + s_k)  f(x_k):\n                    b1_flag = False\n                \n                x_k += s_k\n                final_x = x_k\n                \n                # --- Termination check 2: Step size for an accepted step ---\n                if abs(s_k) = eps_s:\n                    break\n                    \n                # --- Update trust radius (for accepted step) ---\n                if rho_k = eta2 and abs(s_k) = 0.8 * delta_k:\n                    delta_kp1 = min(gamma2 * delta_k, delta_max)\n                # else: delta_kp1 remains delta_k\n                \n            else: # Reject step\n                # x_k remains the same\n                delta_kp1 = gamma1 * delta_k\n            \n            delta_k = delta_kp1\n            \n            # --- Termination check 3: Trust radius size ---\n            if delta_k = eps_s:\n                break\n        \n        # --- Calculate final distance d ---\n        d = min(abs(final_x - m) for m in minimizers)\n        \n        # Format results for the current test case\n        b1_str = \"true\" if b1_flag else \"false\"\n        b2_str = \"true\" if b2 else \"false\"\n        d_str = \"{:.6f}\".format(d)\n        results.append(f'[{b1_str},{b2_str},{d_str}]')\n        \n    # --- Final Print ---\n    # The final output is a single line, formatted as a list of lists.\n    print(f\"[[{','.join(results)}]]\")\n\nsolve()\n```", "id": "2461247"}]}