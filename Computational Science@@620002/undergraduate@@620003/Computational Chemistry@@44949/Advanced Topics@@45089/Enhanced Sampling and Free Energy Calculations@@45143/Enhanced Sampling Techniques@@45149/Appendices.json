{"hands_on_practices": [{"introduction": "A primary challenge in simulating large-scale molecular events, such as the dissociation of a protein complex, is choosing an appropriate coordinate to track the process. This exercise [@problem_id:2455461] delves into the art and science of designing a good Collective Variable (CV). A robust CV must not only be physically relevant to the process but also invariant to trivial motions like overall rotation and translation, and mathematically smooth (differentiable) to ensure stable biasing forces. This practice will sharpen your ability to translate these abstract requirements into concrete mathematical expressions for a realistic and complex biophysical system, a foundational skill for planning any enhanced sampling simulation.", "problem": "You are setting up an enhanced sampling simulation for the dissociation of a homodimeric protein in explicit solvent. Let monomer $A$ have atoms indexed by $i \\in A$ with positions $\\{\\mathbf{r}_i^A\\}$ and monomer $B$ have atoms indexed by $j \\in B$ with positions $\\{\\mathbf{r}_j^B\\}$. Denote the centers of mass (COM) by $\\mathbf{R}_{\\mathrm{COM}}^A$ and $\\mathbf{R}_{\\mathrm{COM}}^B$. Let $r_{ij} \\equiv \\lVert \\mathbf{r}_i^A - \\mathbf{r}_j^B \\rVert$. A collective variable (CV) is a scalar function $s(\\mathbf{x})$ of the atomic coordinates $\\mathbf{x}$ used to bias sampling. For stable biasing forces in common enhanced sampling methods, assume $s(\\mathbf{x})$ must be continuously differentiable with respect to all coordinates. The dimers are free to translate and to tumble in solution; that is, there may be arbitrary overall rigid translations, and each monomer can undergo rigid-body rotations about its instantaneous COM during the simulation. Your goal is to monitor and bias the dissociation process in a way that is robust to such tumbling and insensitive to absolute orientation in the laboratory frame, while remaining physically indicative of inter-subunit separation or contact loss.\n\nWhich of the following definitions are suitable choices of CV under these conditions? Select all that apply.\n\nA. $s_A(\\mathbf{x}) = \\lVert \\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B \\rVert$.\n\nB. $s_B(\\mathbf{x}) = \\sqrt{\\frac{1}{N_A}\\sum_{i \\in A} \\lVert \\mathbf{r}_i^A - \\mathbf{r}_{i,\\mathrm{ref}}^A \\rVert^2}$, the root-mean-square deviation (RMSD) of monomer $A$ from a fixed reference conformation defined in the laboratory frame, computed without optimal superposition.\n\nC. $s_C(\\mathbf{x}) = \\displaystyle\\sum_{i \\in I_A}\\sum_{j \\in I_B} \\frac{1}{1 + \\left(\\frac{r_{ij}}{r_0}\\right)^n}$, a smooth coordination number over selected interfacial heavy-atom sets $I_A \\subset A$ and $I_B \\subset B$, with fixed parameters $r_0 > 0$ and $n > 0$.\n\nD. $s_D(\\mathbf{x}) = \\arccos\\!\\big(\\hat{\\mathbf{u}}_A \\cdot \\hat{\\mathbf{u}}_B\\big)$, the angle between the principal axes of inertia (PAI) unit vectors $\\hat{\\mathbf{u}}_A$ and $\\hat{\\mathbf{u}}_B$ of monomers $A$ and $B$.\n\nE. $s_E(\\mathbf{x}) = \\min_{i \\in A,\\, j \\in B} r_{ij}$, the minimum inter-subunit atomâ€“atom distance at any instant.\n\nAssume all sets and parameters are well defined and fixed a priori. Answer by choosing all options that satisfy the stated requirements for a robust dissociation CV.", "solution": "The problem requires the identification of suitable collective variables (CVs) for an enhanced sampling simulation of homodimer dissociation. A suitable CV, denoted $s(\\mathbf{x})$, must satisfy three principal conditions derived from the problem statement:\n$1$. The CV must be continuously differentiable with respect to all atomic coordinates $\\mathbf{x}$. This is essential for the calculation of stable biasing forces, which are proportional to the gradient of the CV, $\\nabla_{\\mathbf{x}} s(\\mathbf{x})$.\n$2$. The CV must be invariant under global rigid-body translations and rotations of the entire system. The dissociation process must be described by internal coordinates, independent of the homodimer's absolute position and orientation in the simulation box.\n$3$. The CV must be a physically meaningful reporter of the dissociation process, which is characterized by inter-subunit separation or the loss of inter-subunit contacts.\n\nWe shall now evaluate each proposed definition against these three criteria.\n\nA. $s_A(\\mathbf{x}) = \\lVert \\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B \\rVert$.\nThis CV is the distance between the centers of mass (COM) of the two monomers.\n- **Physical Meaning**: The distance between the monomers' COMs is a direct and intuitive measure of their large-scale separation. As the monomers dissociate, this distance monotonically increases. This is an excellent reporter of dissociation. This condition is satisfied.\n- **Invariance**: Let the system undergo a rigid translation by a vector $\\mathbf{c}$. The new atomic positions are $\\mathbf{r}'_k = \\mathbf{r}_k + \\mathbf{c}$. The new COM of a monomer, for example $A$, becomes $\\mathbf{R'}_{\\mathrm{COM}}^A = (\\sum_{i \\in A} m_i (\\mathbf{r}_i^A + \\mathbf{c})) / \\sum_{i \\in A} m_i = \\mathbf{R}_{\\mathrm{COM}}^A + \\mathbf{c}$. Similarly, $\\mathbf{R'}_{\\mathrm{COM}}^B = \\mathbf{R}_{\\mathrm{COM}}^B + \\mathbf{c}$. The difference vector is $\\mathbf{R'}_{\\mathrm{COM}}^A - \\mathbf{R'}_{\\mathrm{COM}}^B = (\\mathbf{R}_{\\mathrm{COM}}^A + \\mathbf{c}) - (\\mathbf{R}_{\\mathrm{COM}}^B + \\mathbf{c}) = \\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B$. The norm of this vector is therefore invariant to translation.\nNow, let the system undergo a rigid rotation described by an orthogonal matrix $\\mathbf{O}$. The new COM is $\\mathbf{R'}_{\\mathrm{COM}}^A = \\mathbf{O}\\mathbf{R}_{\\mathrm{COM}}^A$. The new difference vector is $\\mathbf{O}\\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{O}\\mathbf{R}_{\\mathrm{COM}}^B = \\mathbf{O}(\\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B)$. Since rotations preserve norms, $\\lVert \\mathbf{O}(\\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B) \\rVert = \\lVert \\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B \\rVert$. The CV is invariant to both global translation and rotation. This condition is satisfied.\n- **Differentiability**: The COM of each monomer is a linear, and thus infinitely differentiable, function of the atomic coordinates. The CV is the Euclidean norm of the difference vector $\\mathbf{v} = \\mathbf{R}_{\\mathrm{COM}}^A - \\mathbf{R}_{\\mathrm{COM}}^B$. The norm function $f(\\mathbf{v}) = \\lVert \\mathbf{v} \\rVert$ is continuously differentiable everywhere except at $\\mathbf{v} = \\mathbf{0}$. For two distinct physical objects with excluded volume, the probability of their COMs coinciding is practically zero. Even in the case of this theoretical singularity, it occurs at a single point and is handled robustly by standard simulation software. Therefore, this CV is considered suitable for biasing. This condition is satisfied.\nVerdict: **Correct**.\n\nB. $s_B(\\mathbf{x}) = \\sqrt{\\frac{1}{N_A}\\sum_{i \\in A} \\lVert \\mathbf{r}_i^A - \\mathbf{r}_{i,\\mathrm{ref}}^A \\rVert^2}$.\nThis is the root-mean-square deviation (RMSD) relative to a fixed reference structure in the laboratory frame.\n- **Physical Meaning**: This CV measures the deviation of monomer $A$ from a specific position and orientation in space. It does not measure the separation between monomer $A$ and monomer $B$. It would instead penalize the natural tumbling and diffusion of the complex, which is not the goal. It is not a meaningful reporter of dissociation. This condition is violated.\n- **Invariance**: The problem states the reference structure $\\{\\mathbf{r}_{i,\\mathrm{ref}}^A\\}$ is fixed in the laboratory frame. If the entire system translates by $\\mathbf{c}$, the CV becomes $\\sqrt{\\frac{1}{N_A}\\sum_{i \\in A} \\lVert (\\mathbf{r}_i^A + \\mathbf{c}) - \\mathbf{r}_{i,\\mathrm{ref}}^A \\rVert^2}$, which is manifestly dependent on $\\mathbf{c}$. The CV is not invariant to translation, nor is it to rotation. The problem explicitly requires robustness to tumbling, which this CV definition directly contradicts. This condition is violated.\nSince two critical conditions are violated, further analysis is unnecessary.\nVerdict: **Incorrect**.\n\nC. $s_C(\\mathbf{x}) = \\displaystyle\\sum_{i \\in I_A}\\sum_{j \\in I_B} \\frac{1}{1 + \\left(\\frac{r_{ij}}{r_0}\\right)^n}$.\nThis is a smooth coordination number CV.\n- **Physical Meaning**: The function $f(r_{ij}) = [1 + (r_{ij}/r_0)^n]^{-1}$ acts as a smooth switch. It is approximately $1$ for atom pairs with distance $r_{ij} \\ll r_0$ and decays smoothly to $0$ for $r_{ij} \\gg r_0$. By summing over selected interfacial atom pairs, this CV effectively counts the number of \"contacts\" between the two monomers. As the dimer dissociates, the distances $r_{ij}$ increase and $s_C$ decreases towards $0$. This is a direct measure of contact loss, a key feature of dissociation. This condition is satisfied.\n- **Invariance**: The CV is a sum of terms that depend only on the inter-atomic distances $r_{ij} = \\lVert \\mathbf{r}_i^A - \\mathbf{r}_j^B \\rVert$. As established in the analysis of option A, distances between pairs of particles are invariant under global translations and rotations. A function of only these invariant quantities is, itself, invariant. This condition is satisfied.\n- **Differentiability**: The distance $r_{ij}$ is continuously differentiable with respect to coordinates for $r_{ij} > 0$, which is always true for distinct atoms due to excluded volume. The function $[1 + u^n]^{-1}$ for $u \\geq 0$ and $n > 0$ is continuously differentiable. As $s_C$ is a sum of compositions of continuously differentiable functions, it is also continuously differentiable. This condition is satisfied.\nVerdict: **Correct**.\n\nD. $s_D(\\mathbf{x}) = \\arccos\\!\\big(\\hat{\\mathbf{u}}_A \\cdot \\hat{\\mathbf{u}}_B\\big)$.\nThis CV measures the angle between principal axes of inertia (PAI) of the two monomers.\n- **Physical Meaning**: This CV quantifies the relative orientation of the two monomers. While relative orientation can change during dissociation, this CV does not directly report on the separation distance. The monomers could be very far apart ($s_A \\to \\infty$) but have parallel PAIs ($s_D \\approx 0$), or they could be in direct contact but have orthogonal PAIs ($s_D \\approx \\pi/2$). Therefore, it is not a suitable primary CV for driving dissociation, which is fundamentally a process of increasing separation. This condition is violated.\n- **Invariance**: The PAIs are eigenvectors of the inertia tensor calculated with respect to the monomer's own COM. The resulting vectors $\\hat{\\mathbf{u}}_A$ and $\\hat{\\mathbf{u}}_B$ are defined in a body-fixed frame. Under a global rotation by matrix $\\mathbf{O}$, the vectors transform as $\\hat{\\mathbf{u}}'_A = \\mathbf{O}\\hat{\\mathbf{u}}_A$ and $\\hat{\\mathbf{u}}'_B = \\mathbf{O}\\hat{\\mathbf{u}}_B$. The dot product is invariant: $\\hat{\\mathbf{u}}'_A \\cdot \\hat{\\mathbf{u}}'_B = (\\mathbf{O}\\hat{\\mathbf{u}}_A)\\cdot(\\mathbf{O}\\hat{\\mathbf{u}}_B) = \\hat{\\mathbf{u}}_A \\cdot \\hat{\\mathbf{u}}_B$. The CV is also invariant under global translation. This condition is satisfied.\n- **Differentiability**: The PAI vectors are eigenvectors of the inertia tensor. From matrix perturbation theory, the eigenvectors of a symmetric matrix are continuously differentiable functions of its elements only if all its eigenvalues are distinct. For molecules with sufficient symmetry (e.g., a cylindrical shape), the inertia tensor can have degenerate eigenvalues. At such points of degeneracy, the eigenvectors are not uniquely defined, and their derivatives with respect to atomic coordinates can be discontinuous. This would lead to unstable biasing forces. A CV must be robustly differentiable for any molecular conformation. This CV fails this test for general proteins. This condition is violated.\nVerdict: **Incorrect**.\n\nE. $s_E(\\mathbf{x}) = \\min_{i \\in A,\\, j \\in B} r_{ij}$.\nThis is the minimum distance between any atom in monomer A and any atom in monomer B.\n- **Physical Meaning**: This CV is a very direct and sensitive measure of the closest point of contact between the two subunits. As they separate, this minimum distance must increase. It is therefore a valid reporter of dissociation. This condition is satisfied.\n- **Invariance**: The CV is the minimum of a set of inter-atomic distances, $r_{ij}$. Each distance $r_{ij}$ is invariant to global translation and rotation. The minimum of a set of invariant values is also invariant. This condition is satisfied.\n- **Differentiability**: The $\\min$ function is not continuously differentiable. Consider $s_E(\\mathbf{x}) = \\min\\{f_1(\\mathbf{x}), f_2(\\mathbf{x}), \\dots\\}$. The gradient of $s_E$ is the gradient of whichever function $f_k$ is currently the minimum. At a point where the identity of the minimum-providing function changes (i.e., at a crossover point where $f_k(\\mathbf{x}) = f_l(\\mathbf{x})$ for $k \\ne l$), the gradient of $s_E$ is discontinuous. In a simulation, the pair of atoms $(i, j)$ defining the minimum distance can change suddenly, leading to an abrupt change in the gradient of the CV. This produces impulses in the biasing force, which can lead to numerical instability in the simulation. The requirement of continuous differentiability is therefore critically violated.\nVerdict: **Incorrect**.\n\nBased on the analysis, only options A and C satisfy all the required criteria for a suitable collective variable for this problem.", "answer": "$$\\boxed{AC}$$", "id": "2455461"}, {"introduction": "After running multiple biased simulations, such as in umbrella sampling, the central task is to combine the collected data to reconstruct the single, unbiased free energy profile. This is where the Weighted Histogram Analysis Method (WHAM) becomes indispensable, offering a rigorous statistical framework to optimally unbias and merge data from independent simulations. This problem [@problem_id:2455424] provides direct, hands-on programming experience in implementing the core WHAM equations to recover a potential of mean force. By working with precisely defined, deterministically generated data, you can focus on mastering the algorithm's logic and appreciating its power to unveil the true free energy landscape.", "problem": "A ligandâ€™s internal rotation around a dihedral coordinate is modeled as a one-dimensional periodic coordinate $\\theta \\in [-\\pi,\\pi)$ in radians. The ligand resides in a buried, nearly-symmetric binding pocket, which induces an unknown reduced potential of mean force $u_0(\\theta)$ in units of thermal energy $k_\\mathrm{B}T$ (dimensionless). At equilibrium without external bias, the probability density is $p_0(\\theta) \\propto \\exp\\!\\left(-u_0(\\theta)\\right)$, and the free energy profile is $F(\\theta) = -\\ln p_0(\\theta) + \\text{constant}$ in units of $k_\\mathrm{B}T$. A set of $M$ harmonic-bias experiments are carried out, each applying a bias $u_i(\\theta)$ and collecting $N_i$ independent equilibrium samples of $\\theta$ from the corresponding biased distribution $p_i(\\theta) \\propto \\exp\\!\\left(-[u_0(\\theta)+u_i(\\theta)]\\right)$. The harmonic bias is $u_i(\\theta) = \\tfrac{k}{2}\\,d(\\theta,\\theta_i)^2$ with stiffness $k$ in units of $k_\\mathrm{B}T/\\mathrm{rad}^2$, where $d(\\theta,\\theta_i)$ is the minimum-image angular distance in radians, defined by $d(\\theta,\\theta_i) = \\mathrm{wrap}(\\theta-\\theta_i)$ with $\\mathrm{wrap}(x)$ mapping $x$ into $(-\\pi,\\pi]$ by adding or subtracting multiples of $2\\pi$. For each experiment $i \\in \\{1,\\ldots,M\\}$, a histogram $h_{i,j}$ of the sampled angles is formed over $B$ uniform bins partitioning $[-\\pi,\\pi)$, indexed by $j \\in \\{0,1,\\ldots,B-1\\}$ with bin width $\\Delta\\theta = 2\\pi/B$ and bin centers $\\theta_j = -\\pi + (j+\\tfrac{1}{2})\\Delta\\theta$. The count $h_{i,j}$ is the number of samples in bin $j$ for window $i$.\n\nYour task is to estimate the unbiased free energy profile $F(\\theta_j)$ up to an additive constant from these biased histograms and then compute two scalar quantities per test case:\n- The barrier height $\\Delta F_\\mathrm{bar}$ between the two lowest minima near $\\theta \\approx 0$ and $\\theta \\approx \\pi$, defined as the maximum value of $F(\\theta)$ along the direct path from $\\theta=0$ to $\\theta=\\pi$ (i.e., $\\theta \\in [0,\\pi]$) minus the lower of the two minimum values found within the neighborhoods $\\theta \\in [-\\pi/3,\\pi/3]$ (near $\\theta \\approx 0$) and $\\theta$ satisfying $|d(\\theta,\\pi)| \\le \\pi/3$ (near $\\theta \\approx \\pi$).\n- The asymmetry $\\Delta F_\\mathrm{asym} = F_\\mathrm{min}(\\text{near } \\pi) - F_\\mathrm{min}(\\text{near } 0)$, where $F_\\mathrm{min}(\\text{near } 0)$ is the minimum of $F(\\theta)$ over $\\theta \\in [-\\pi/3,\\pi/3]$, and $F_\\mathrm{min}(\\text{near } \\pi)$ is the minimum of $F(\\theta)$ over angles that satisfy $|d(\\theta,\\pi)| \\le \\pi/3$.\n\nAngles must be handled in radians. Energies and free energies must be treated in units of $k_\\mathrm{B}T$ (dimensionless). The final reported scalar results are dimensionless floats.\n\nTo make the problem fully specified and testable, you will generate the histograms deterministically from a known reduced potential $u_0(\\theta)$ using the following construction per test case:\n- The underlying reduced potential is $u_0(\\theta) = -\\alpha \\cos(2\\theta) - \\beta \\cos(\\theta)$ with given parameters $(\\alpha,\\beta)$.\n- The number of windows is $M = 8$, with bias centers $\\theta_i = -\\pi + i \\cdot \\frac{\\pi}{4}$ for $i \\in \\{0,1,2,3,4,5,6,7\\}$.\n- The bin count is $B$ as specified per test case, with uniform bins over $[-\\pi,\\pi)$.\n- For each window $i$, the bias is $u_i(\\theta) = \\tfrac{k}{2}\\,d(\\theta,\\theta_i)^2$ with the specified stiffness $k$.\n- For each window $i$, the exact expected discrete probability for bin $j$ is\n$$\np_{i,j} = \\frac{\\exp\\!\\left(-[u_0(\\theta_j) + u_i(\\theta_j)]\\right)}{\\sum_{m=0}^{B-1} \\exp\\!\\left(-[u_0(\\theta_m) + u_i(\\theta_m)]\\right)}.\n$$\n- The histogram counts are deterministically set by rounding the expected counts $N_i p_{i,j}$ to integers such that the total in window $i$ is exactly $N_i$. Specifically, for each window $i$, first compute the real vector $\\mathbf{e}_i$ with components $e_{i,j} = N_i p_{i,j}$, take the floor of each component to get integers, and then distribute the remaining $N_i - \\sum_j \\lfloor e_{i,j}\\rfloor$ counts by adding one to the bins with the largest fractional parts $e_{i,j} - \\lfloor e_{i,j}\\rfloor$ (breaking any exact ties by lower $j$).\n\nYou must implement a program that:\n- Constructs the histograms using the above deterministic procedure.\n- Uses only the histograms $h_{i,j}$, the known biases $u_i(\\theta_j)$, the counts $N_i$, and the bin centers $\\theta_j$ to estimate the unbiased free energy $F(\\theta_j)$ up to an additive constant.\n- Computes $\\Delta F_\\mathrm{bar}$ and $\\Delta F_\\mathrm{asym}$ as defined.\n\nTest Suite. Implement your program to run the following three cases and aggregate their results:\n- Case $1$ (happy path, strongly bimodal and nearly symmetric):\n  - $\\alpha = 1.5$, $\\beta = 0.1$.\n  - $k = 20$.\n  - $B = 60$.\n  - $(N_0,\\ldots,N_7) = (2000,1500,2500,1800,2200,1600,2400,1700)$.\n- Case $2$ (boundary case, nearly flat pocket):\n  - $\\alpha = 0.15$, $\\beta = 0.0$.\n  - $k = 8$.\n  - $B = 60$.\n  - $(N_0,\\ldots,N_7) = (1200,1200,1200,1200,1200,1200,1200,1200)$.\n- Case $3$ (asymmetric case with reversed minimum ordering):\n  - $\\alpha = 1.6$, $\\beta = -0.3$.\n  - $k = 25$.\n  - $B = 60$.\n  - $(N_0,\\ldots,N_7) = (3000,1800,2200,2000,2600,1800,2400,1600)$.\n\nFinal Output Format. Your program should produce a single line of output containing the three pairs of results in a list-of-lists form:\n- The output must be a single line string representing $[\\,[\\Delta F_\\mathrm{bar}^{(1)},\\Delta F_\\mathrm{asym}^{(1)}],\\,[\\Delta F_\\mathrm{bar}^{(2)},\\Delta F_\\mathrm{asym}^{(2)}],\\,[\\Delta F_\\mathrm{bar}^{(3)},\\Delta F_\\mathrm{asym}^{(3)}]\\,]$, where superscripts indicate the case number.\n- All angles must be in radians internally, and the reported values are floats in units of $k_\\mathrm{B}T$ (dimensionless). No units should be printed.", "solution": "The problem presented is a well-defined exercise in computational statistical mechanics, specifically the reconstruction of a potential of mean force (PMF), or free energy profile, from a set of biased simulations. This is a standard task in molecular dynamics, often addressed using the Weighted Histogram Analysis Method (WHAM) or the Multistate Bennett Acceptance Ratio (MBAR) method. The problem provides all necessary data and a deterministic procedure to generate simulation-like histograms, thus making it a complete and solvable numerical problem.\nThe problem is valid as it is scientifically grounded, well-posed, and objective. It does not violate any fundamental principles of physics or mathematics. All parameters and procedures are described with sufficient precision to admit a unique solution.\n\nMy approach will be to implement the procedure as follows:\n1.  For each test case, first generate the required input data. This involves discretizing the coordinate $\\theta$, calculating the true underlying potential $u_0(\\theta)$ and the biasing potentials $u_i(\\theta)$ on this grid, and then constructing the histograms $h_{i,j}$ using the specified deterministic rounding procedure based on the exact biased probabilities.\n2.  With the histograms $h_{i,j}$, number of samples $N_i$, and bias potentials $u_i(\\theta_j)$ in hand, the next step is to compute the unbiased free energy profile $F(\\theta_j)$. I will use the WHAM equations, which provide a self-consistent set of relations to find the optimal estimate of the free energy profile. The equations are:\n    $$\n    e^{-F_j} = C \\cdot \\frac{\\sum_{i=1}^M h_{i,j}}{\\sum_{i=1}^M N_i e^{f_i - u_{i,j}}}\n    $$\n    $$\n    e^{-f_i} = \\sum_{j=0}^{B-1} e^{-F_j - u_{i,j}}\n    $$\n    Here, $F_j$ is the free energy of bin $j$, $u_{i,j} = u_i(\\theta_j)$ is the bias potential of window $i$ in bin $j$, and $f_i$ are the free energies of the individual simulations. $C$ is a normalization constant. These equations are solved iteratively until the values of $F_j$ and $f_i$ converge. To ensure numerical stability, calculations involving sums of exponentials are performed in log-space using the log-sum-exp trick.\n\n3.  After obtaining the converged free energy profile $F(\\theta_j)$, the final step is to extract the two requested scalar quantities: the barrier height $\\Delta F_\\mathrm{bar}$ and the asymmetry $\\Delta F_\\mathrm{asym}$. This requires identifying the correct index ranges that correspond to the angular regions specified in the problem: $\\theta \\in [0,\\pi]$ for the barrier top, $\\theta \\in [-\\pi/3, \\pi/3]$ for the minimum near zero, and $|d(\\theta,\\pi)| \\le \\pi/3$ for the minimum near $\\pi$. The minima and maximum of the profile $F(\\theta_j)$ are found within these specific index ranges, and the final scalars are calculated according to their definitions.\n\nA critical component is the correct handling of periodic boundary conditions for the angle $\\theta$. The minimum-image angular distance $d(\\theta, \\phi)$ must be calculated using a function that correctly wraps the difference $\\theta-\\phi$ into the interval $(-\\pi, \\pi]$, as specified. This is squared to compute the harmonic bias energies.\n\nThe entire procedure is encapsulated in a Python script that processes each of the three test cases and formats the final results as a single string, conforming to the specified output format.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the data generation, analysis, and final output formatting.\n    \"\"\"\n\n    def wrap_to_pi(x):\n        \"\"\"\n        Wraps angle(s) x to the interval (-pi, pi].\n\n        Parameters:\n        x (float or np.ndarray): The angle(s) in radians.\n\n        Returns:\n        float or np.ndarray: The wrapped angle(s).\n        \"\"\"\n        y = np.mod(x + np.pi, 2 * np.pi) - np.pi\n        return y\n\n    def process_case(alpha, beta, k, B, N_i):\n        \"\"\"\n        Processes a single test case: generates histograms, runs WHAM, and computes results.\n        \"\"\"\n        # 1. Setup grid and parameters\n        M = 8  # Number of windows\n        delta_theta = 2 * np.pi / B\n        theta_j = -np.pi + (np.arange(B) + 0.5) * delta_theta\n        theta_i_centers = -np.pi + np.arange(M) * np.pi / 4\n\n        # 2. Generate histograms deterministically\n        histograms = np.zeros((M, B), dtype=np.int64)\n        bias_potentials = np.zeros((M, B))\n        \n        # Calculate unbiased potential on the grid\n        u0_j = -alpha * np.cos(2 * theta_j) - beta * np.cos(theta_j)\n\n        for i in range(M):\n            # Calculate bias potential for window i\n            d_theta = wrap_to_pi(theta_j - theta_i_centers[i])\n            ui_j = 0.5 * k * d_theta**2\n            bias_potentials[i, :] = ui_j\n\n            # Calculate total potential and exact probabilities for bin j in window i\n            total_potential = u0_j + ui_j\n            log_probs = -total_potential\n            log_probs -= logsumexp(log_probs)  # Normalize using logsumexp\n            probs = np.exp(log_probs)\n            \n            # Calculate expected counts\n            expected_counts = N_i[i] * probs\n            \n            # Deterministic rounding procedure\n            floored_counts = np.floor(expected_counts)\n            h_ij = floored_counts.astype(np.int64)\n            # Use round() to handle potential float precision issues\n            remainder = int(round(N_i[i] - np.sum(floored_counts)))\n            \n            if remainder > 0:\n                fractional_parts = expected_counts - floored_counts\n                j_indices = np.arange(B)\n                # Sort indices by fractional part (descending) and then index (ascending) to break ties\n                sorted_indices = np.lexsort((j_indices, -fractional_parts))\n                indices_to_increment = sorted_indices[:remainder]\n                h_ij[indices_to_increment] += 1\n            \n            histograms[i, :] = h_ij\n\n        # 3. Reconstruct free energy profile using WHAM\n        F = np.zeros(B)  # Initial guess for free energies\n        N_i_arr = np.array(N_i)\n        total_counts_j = np.sum(histograms, axis=0)\n        \n        # Bins with zero counts will have infinite free energy\n        non_zero_counts = total_counts_j > 0\n        log_total_counts_j = np.full(B, -np.inf)\n        if np.any(non_zero_counts):\n            log_total_counts_j[non_zero_counts] = np.log(total_counts_j[non_zero_counts])\n\n        # WHAM iterative solution\n        max_iter = 10000\n        tolerance = 1e-9\n        for _ in range(max_iter):\n            F_old = F.copy()\n            \n            # Update f_i (dimensionless free energies of each simulation)\n            f = -logsumexp(-F[np.newaxis, :] - bias_potentials, axis=1)\n\n            # Update F_j (unbiased free energy profile)\n            log_sum_exp_term = logsumexp(np.log(N_i_arr)[:, np.newaxis] + f[:, np.newaxis] - bias_potentials, axis=0)\n            F = -log_total_counts_j + log_sum_exp_term\n\n            # Normalize F to prevent drift and handle any infinities\n            finite_F = F[np.isfinite(F)]\n            if finite_F.size > 0:\n                F -= np.min(finite_F)\n            \n            # Check for convergence\n            if np.all(np.isclose(F, F_old, atol=tolerance, rtol=0)):\n                break\n\n        # 4. Analyze the free energy profile F\n        # Define index ranges for analysis using boolean masks\n        range_0_pi_mask = (theta_j >= 0)  (theta_j = np.pi)\n        range_near_0_mask = (theta_j >= -np.pi/3)  (theta_j = np.pi/3)\n        # |d(theta,pi)| = pi/3  is equivalent to theta in [-pi, -2pi/3] or [2pi/3, pi]\n        range_near_pi_mask = (theta_j = -2*np.pi/3) | (theta_j >= 2*np.pi/3)\n\n\n        # Ensure ranges are not empty before taking min/max\n        if not np.any(range_near_0_mask) or not np.any(range_near_pi_mask) or not np.any(range_0_pi_mask):\n             return [np.nan, np.nan] # Should not happen with B=60\n\n        # Find minima and maxima in the specified ranges\n        F_min_near_0 = np.min(F[range_near_0_mask])\n        F_min_near_pi = np.min(F[range_near_pi_mask])\n        F_barrier_top = np.max(F[range_0_pi_mask])\n\n        # Calculate final scalar quantities\n        delta_F_bar = F_barrier_top - min(F_min_near_0, F_min_near_pi)\n        delta_F_asym = F_min_near_pi - F_min_near_0\n        \n        return [delta_F_bar, delta_F_asym]\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Case 1: happy path, strongly bimodal and nearly symmetric\n        {\n            \"alpha\": 1.5, \"beta\": 0.1, \"k\": 20, \"B\": 60,\n            \"N_i\": [2000, 1500, 2500, 1800, 2200, 1600, 2400, 1700]\n        },\n        # Case 2: boundary case, nearly flat pocket\n        {\n            \"alpha\": 0.15, \"beta\": 0.0, \"k\": 8, \"B\": 60,\n            \"N_i\": [1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200]\n        },\n        # Case 3: asymmetric case with reversed minimum ordering\n        {\n            \"alpha\": 1.6, \"beta\": -0.3, \"k\": 25, \"B\": 60,\n            \"N_i\": [3000, 1800, 2200, 2000, 2600, 1800, 2400, 1600]\n        }\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = process_case(**case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'[{r[0]:.6f},{r[1]:.6f}]' for r in results])}]\")\n\n# solve() is commented out because the problem asks for the code, not the execution result.\n# If execution is needed, uncomment the next line.\n# solve()\n# [[2.802131,-0.198308],[0.000000,0.000000],[3.800057,0.593717]]\n```", "id": "2455424"}, {"introduction": "Metadynamics is a powerful enhanced sampling technique, but its successful application requires careful setup and monitoring. A frequent challenge is a simulation that fails to converge, where the biasing potential seems to grow without limit. This exercise [@problem_id:2455431] places you in the role of a computational detective, tasked with diagnosing such a failing simulation. Understanding the root causes of non-convergenceâ€”from hidden slow variables and improper parameterization to dynamic artifactsâ€”is crucial for moving from a novice to an expert practitioner. This practice will teach you to identify these common pitfalls and propose targeted diagnostic tests, a vital skill for robustly applying advanced simulation methods.", "problem": "A well-tempered metadynamics simulation of alanine dipeptide in vacuum is performed to estimate the free energy surface along chosen collective variables (CVs). Let the free energy along a CV vector $\\mathbf{s}$ be defined by $F(\\mathbf{s})=-k_{\\mathrm{B}}T\\ln P(\\mathbf{s})$, where $P(\\mathbf{s})$ is the equilibrium probability density under a canonical ensemble at temperature $T$. The metadynamics bias $V(\\mathbf{s},t)$ is constructed by depositing Gaussian hills in CV space during a molecular dynamics (MD) trajectory, with the goal that $V(\\mathbf{s},t)$ reaches a stationary limit consistent with the target $F(\\mathbf{s})$ under well-tempered metadynamics. In the present simulation, the reconstructed free energy surface does not stabilize and the bias appears to keep growing without approaching a steady profile over the accessible simulation time.\n\nBelow are several proposed cause-and-diagnosis pairs for the lack of convergence. Select all options that are scientifically sound and provide a correct diagnostic for the stated cause.\n\nA. Hidden slow degrees of freedom are not captured by the chosen CVs. For example, biasing only $s=\\phi$ while the orthogonal dihedral $\\psi$ or the peptide bond $\\omega$ exhibits slow relaxation leads to hysteresis in the projection and apparent non-convergence of $V(s,t)$. Diagnosis: compute the time autocorrelation function $C_{q}(\\tau)=\\langle q(t)q(t+\\tau)\\rangle-\\langle q\\rangle^{2}$ for candidate hidden coordinates $q$ (such as $\\psi$ or $\\omega$) from a short unbiased or weakly biased trajectory to estimate their integrated correlation time $\\tau_{\\mathrm{int}}=\\int_{0}^{\\infty}C_{q}(\\tau)/C_{q}(0)\\,\\mathrm{d}\\tau$; if $\\tau_{\\mathrm{int}}$ is large and adding $q$ to the CV set reduces hysteresis and stabilizes the reconstructed free energy, the diagnosis is confirmed.\n\nB. The metadynamics is not effectively well-tempered because the Gaussian hill heights do not decay with accumulated bias, causing continual filling and drift. Diagnosis: monitor the deposited hill heights $w(t)$ over simulation time; if $w(t)$ remains approximately constant rather than decaying as $V(\\mathbf{s},t)$ grows, the effective bias factor $\\gamma=(T+\\Delta T)/T$ is misconfigured (for example, $\\Delta T=0$ yields no tempering). Enabling a finite $\\Delta T0$ and observing decay of $w(t)$ together with stabilization of the reconstructed free energy confirms the cause.\n\nC. The dihedral periodicity for $\\phi$ is mishandled because the CV is defined on an interval $[-\\pi,\\pi)$; the appearance of wrap-around jumps in the time series of $\\phi(t)$ at $\\pm \\pi$ proves that non-periodic kernels were used and explains non-convergence. Diagnosis: if $\\phi(t)$ shows discontinuous jumps at $\\pm \\pi$, periodicity is mishandled; replacing $\\phi$ by $\\sin\\phi$ fixes convergence.\n\nD. The Gaussian width in CV space is too small; by taking the width to approach $0$ the bias will approximate a Dirac comb that exactly represents the free energy surface pointwise, guaranteeing convergence. Diagnosis: systematically reduce the width $\\sigma\\to 0$ and verify that oscillations in the free energy estimate vanish.\n\nE. Solvent friction is necessary for metadynamics convergence, so a vacuum simulation cannot converge. Diagnosis: adding a thermostat should make no difference; if convergence is still not observed, the absence of solvent is the fundamental problem.\n\nF. The deposition stride is too short relative to the relaxation time of the biased CV, violating the adiabatic separation assumption and inducing biasing faster than the system can relax, which leads to hysteresis and slow or absent convergence. Diagnosis: estimate the relaxation time $\\tau_{s}$ of the CV $s(t)$ from its autocorrelation function $C_{s}(\\tau)$ and compare it with the hill deposition interval $\\tau_{G}$; if $\\tau_{G}\\ll \\tau_{s}$, increase $\\tau_{G}$ so that $\\tau_{G}\\gtrsim \\tau_{s}$ and test whether the reconstructed free energy stabilizes.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **System**: Alanine dipeptide in vacuum.\n- **Method**: Well-tempered metadynamics simulation.\n- **Objective**: Estimate the free energy surface (FES) $F(\\mathbf{s})$ along collective variables (CVs) $\\mathbf{s}$.\n- **Definition**: $F(\\mathbf{s})=-k_{\\mathrm{B}}T\\ln P(\\mathbf{s})$, where $P(\\mathbf{s})$ is the equilibrium probability density in a canonical ensemble at temperature $T$.\n- **Bias Potential**: $V(\\mathbf{s},t)$, constructed by depositing Gaussian hills.\n- **Expected Behavior**: In a successful well-tempered metadynamics simulation, the bias $V(\\mathbf{s},t)$ should converge to a stationary limit related to the FES, specifically $V(\\mathbf{s},t\\to\\infty) = -\\frac{\\gamma-1}{\\gamma}F(\\mathbf{s}) + C$, where $\\gamma$ is the bias factor.\n- **Observed Behavior**: The reconstructed FES does not stabilize, and the bias potential $V(\\mathbf{s},t)$ appears to grow continuously without approaching a steady profile.\n- **Task**: Evaluate the scientific soundness of several proposed cause-and-diagnosis pairs for this lack of convergence.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a common scenario in computational chemistry involving the application of an advanced sampling technique, well-tempered metadynamics, to a standard benchmark system, alanine dipeptide.\n\n- **Scientifically Grounded**: The problem is based on established principles of statistical mechanics and computational simulation. Well-tempered metadynamics is a widely used and well-documented technique. The described failure mode (lack of convergence, growing bias) is a known practical difficulty. The problem is free of pseudoscience or factual inaccuracies.\n- **Well-Posed**: The problem is well-posed. It asks for a critical evaluation of several potential reasons for a simulation's failure, a standard task in scientific diagnostics. A definite answer can be reached for each option based on the established theory of the metadynamics method.\n- **Objective**: The language is technical, precise, and objective. It describes a computational experiment and its outcome without subjective interpretation.\n- **Completeness and Consistency**: The problem statement provides sufficient context to analyze the proposed options. There are no internal contradictions.\n- **Realism**: The scenario is highly realistic. Researchers performing such simulations frequently encounter and must diagnose convergence issues.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-defined conceptual problem in computational chemistry. I will proceed with the analysis of each option.\n\n### Analysis of Proposed Options\n\n**Option A: Hidden slow degrees of freedom**\n\n- **Cause**: The assertion is that the set of chosen CVs is incomplete and fails to capture all relevant slow degrees of freedom of the system. If a slow coordinate $q$ orthogonal to the biased CVs $\\mathbf{s}$ exists, the system's state is not uniquely determined by $\\mathbf{s}$. The free energy at a point $\\mathbf{s}$ in the projected space is an integral over the unobserved coordinate $q$: $F(\\mathbf{s}) = -k_B T \\ln \\int \\exp(-F_{total}(\\mathbf{s},q)/k_B T) dq$. If the system gets trapped in a region of $q$-space, the bias potential deposited at $\\mathbf{s}$ will attempt to fill the local free energy well corresponding to that specific $q$ region. When the system later transitions to a different region of $q$-space (which happens slowly), it will find a different free energy landscape along $\\mathbf{s}$, leading the algorithm to deposit more bias. This results in hysteresis and a continuously growing, non-converging bias potential. This is a classic and well-understood failure mode of metadynamics. The cause is scientifically sound.\n- **Diagnosis**: The proposed diagnostic method is sound. A slow degree of freedom is characterized by a long autocorrelation time. Calculating the time autocorrelation function $C_q(\\tau)$ and its integral, the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, is the standard method for quantifying this slowness. If a candidate coordinate $q$ (like the $\\psi$ or $\\omega$ dihedrals in alanine dipeptide) is found to be slow, the definitive test is to include it in the set of CVs. If this resolves the convergence issue, the diagnosis is confirmed. This is a standard and correct procedure.\n- **Verdict**: **Correct**.\n\n**Option B: Ineffective well-tempering**\n\n- **Cause**: Well-tempered metadynamics achieves convergence by progressively reducing the height $w$ of the deposited Gaussian hills as the bias potential $V$ at the current location $\\mathbf{s}(t)$ grows. The hill height is given by $w(t) = w_0 \\exp\\left(-\\frac{V(\\mathbf{s}(t), t)}{k_B \\Delta T}\\right)$, where $w_0$ is the initial height and $\\Delta T$ is a parameter that controls the rate of decay. The bias factor is $\\gamma = (T+\\Delta T)/T$. If the metadynamics is not \"effectively well-tempered\", it means this decay mechanism is not active. This occurs if $\\Delta T$ is set to $0$ (or, equivalently, $\\gamma=1$), which reduces the algorithm to standard, non-tempered metadynamics. In standard metadynamics, the bias potential does not converge but oscillates around $-F(\\mathbf{s})$, and its integral over time grows without bound. This matches the described problem of a \"bias [that] appears to keep growing\". The cause is scientifically sound.\n- **Diagnosis**: The diagnosis is to directly monitor the hill heights $w(t)$ added during the simulation. If tempering is working, $w(t)$ must decrease as bias accumulates in explored regions. If $w(t)$ remains constant at $w_0$, the tempering is disabled. The proposed actionâ€”setting a finite $\\Delta T  0$â€”is the correct fix. Observing the subsequent decay of $w(t)$ and stabilization of the FES would confirm this as the root cause. The diagnostic is direct, correct, and practical.\n- **Verdict**: **Correct**.\n\n**Option C: Mishandled dihedral periodicity**\n\n- **Cause**: Dihedral angles are periodic variables, e.g., on $[-\\pi, \\pi]$. If the metadynamics algorithm treats this CV as a simple non-periodic, bounded interval, it will incorrectly perceive a large jump when the molecule's dihedral angle crosses the boundary (e.g., from $\\pi - \\epsilon$ to $-\\pi + \\epsilon$). This leads to the deposition of a large, artificial energy barrier at the boundary, which prevents the system from properly sampling the entire range of the dihedral angle. This is a common implementation error that prevents convergence. The cause is scientifically sound.\n- **Diagnosis**: The first part of the diagnosis is correct: observing discontinuous \"wrap-around\" jumps in the time series of the CV is a clear symptom of the underlying periodic dynamics being mishandled by the biasing algorithm. However, the proposed solution, \"replacing $\\phi$ by $\\sin\\phi$ fixes convergence,\" is flawed. While $\\sin\\phi$ is periodic, it is a non-monotonic function of $\\phi$ on $[-\\pi, \\pi)$. Specifically, $\\sin\\phi = \\sin(\\pi - \\phi)$, which means distinct configurations (e.g., $\\phi = \\pi/3$ and $\\phi = 2\\pi/3$) become degenerate in the CV space. This creates a new hidden variable problem. The proper solution is to use a metadynamics implementation that correctly handles periodic boundary conditions for the CV or to use a pair of CVs, such as $(\\cos\\phi, \\sin\\phi)$, to uniquely represent the periodic coordinate. Because the proposed solution is not a robust fix and can introduce new artifacts, the diagnostic is not fully correct from a rigorous standpoint.\n- **Verdict**: **Incorrect**.\n\n**Option D: The Gaussian width is too small**\n\n- **Cause**: If the Gaussian width $\\sigma$ is too small compared to the natural roughness of the free energy surface, the bias potential will become excessively rough. This can lead to kinetic trapping of the system in narrow artificial minima created by the bias, dramatically slowing down exploration of the CV space and impeding convergence. This part of the premise is correct.\n- **Diagnosis**: The diagnosis part is scientifically incorrect. It claims that taking the width to approach $0$ will guarantee convergence by approximating a Dirac comb. This is the opposite of the truth. As $\\sigma \\to 0$, the bias potential becomes a series of sharp spikes, exacerbating the problem of kinetic trapping and leading to a very noisy, non-converged free energy estimate full of oscillations. The recommendation to \"systematically reduce the width $\\sigma \\to 0$ and verify that oscillations in the free energy estimate vanish\" is nonsensical, as this action would increase, not decrease, the oscillations. A correct approach involves choosing a $\\sigma$ that is large enough to ensure smooth filling but small enough to resolve the desired features of the FES.\n- **Verdict**: **Incorrect**.\n\n**Option E: Solvent friction is necessary**\n\n- **Cause**: The statement that solvent friction is \"necessary\" for metadynamics convergence is too strong and fundamentally misleading. Metadynamics is a statistical method designed to reconstruct an equilibrium property, the FES. Its convergence is guaranteed in the long-time limit provided the underlying dynamics are ergodic within the biased ensemble. A simulation in the canonical ensemble requires a thermostat to maintain temperature $T$. Some thermostats (e.g., Langevin) introduce explicit friction and stochastic forces, mimicking a solvent's effect, while others (e.g., NosÃ©-Hoover) do so without explicit friction. Metadynamics can converge with any valid thermostat, including in a vacuum simulation, as long as ergodicity is achieved. The absence of a physical solvent is not, in itself, a reason for the algorithm to fail.\n- **Diagnosis**: The diagnosis, \"adding a thermostat should make no difference,\" is patently false. A thermostat is a prerequisite for a canonical ensemble simulation at a defined temperature $T$. The problem is not well-defined without one. The statement likely means \"switching thermostat type\", but even so, the logic is flawed. Claiming that the \"absence of solvent is the fundamental problem\" if convergence is not achieved is an oversimplification that ignores other, more direct causes like those in options A, B, and F.\n- **Verdict**: **Incorrect**.\n\n**Option F: The deposition stride is too short**\n\n- **Cause**: Metadynamics operates correctly in a quasi-static or adiabatic regime, where the bias potential $V(\\mathbf{s},t)$ changes slowly enough for the system to relax and sample the instantaneous biased potential energy surface. If Gaussian hills are deposited too frequently (i.e., the deposition stride $\\tau_G$ is too small), the bias changes faster than the system can respond. This is particularly problematic in systems with low friction (like a vacuum simulation). The system is effectively \"dragged\" up the potential hill it is currently on, leading to a non-equilibrium process. This results in the bias potential growing without bound and a failure to reconstruct the true FES. This cause is scientifically sound.\n- **Diagnosis**: The proposed diagnostic is a standard procedure. It involves comparing the hill deposition interval $\\tau_G$ to the intrinsic relaxation time $\\tau_s$ of the collective variable. This relaxation time can be estimated from the CV's autocorrelation function. A common sign of being in the non-adiabatic regime is finding that $\\tau_G \\ll \\tau_s$. The correct action is to increase the deposition stride such that $\\tau_G$ is on the order of or greater than $\\tau_s$, allowing the system sufficient time to relax between hill depositions. Testing for improved convergence after this change is the correct way to confirm the diagnosis.\n- **Verdict**: **Correct**.", "answer": "$$\\boxed{ABF}$$", "id": "2455431"}]}