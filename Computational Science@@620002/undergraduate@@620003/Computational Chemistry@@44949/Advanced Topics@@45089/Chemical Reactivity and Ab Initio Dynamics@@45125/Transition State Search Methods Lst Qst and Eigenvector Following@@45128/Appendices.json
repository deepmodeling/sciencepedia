{"hands_on_practices": [{"introduction": "Finding a transition state often begins with a reasonable guess for the reaction path. This exercise explores the Linear Synchronous Transit (LST) method, which defines the simplest possible path—a straight line—between reactant and product geometries. By finding the energy maximum along this LST path and then analyzing the local curvature using the Hessian matrix, you will connect an intuitive path-based search to the rigorous mathematical criteria for a first-order saddle point [@problem_id:2466338]. This practice is fundamental for understanding why simple path maxima are often just starting points for more sophisticated searches.", "problem": "You are given two analytic two-dimensional potential energy surfaces (PES) and a set of endpoint pairs in Cartesian coordinates. For each endpoint pair, define the Linear Synchronous Transit (LST) path as the straight-line interpolation between the endpoints and determine, on that path, the point at which the PES attains its maximum value. Specifically, for a start point $\\mathbf{A} = (x_0,y_0)$ and an end point $\\mathbf{B} = (x_1,y_1)$, let the path be $\\mathbf{r}(t) = \\mathbf{A} + t(\\mathbf{B}-\\mathbf{A})$ with $t \\in [0,1]$. For each test case, find the value $t^\\star \\in [0,1]$ that maximizes $E(\\mathbf{r}(t))$, compute the corresponding energy $E(\\mathbf{r}(t^\\star))$, and classify the point $\\mathbf{r}(t^\\star)$ by the number $m$ of negative eigenvalues of the Hessian matrix of $E$ at $\\mathbf{r}(t^\\star)$. This classification connects to the concept of a first-order saddle point used in eigenvector following. Report $t^\\star$ and $E(\\mathbf{r}(t^\\star))$ rounded to six decimal places, and $m$ as an integer.\n\nBackground definitions:\n- Linear Synchronous Transit (LST) is a path defined by the straight-line interpolation between two structures.\n- Quadratic Synchronous Transit (QST) uses a quadratic interpolation constrained by two endpoints and optionally a guess.\n- Eigenvector following is a method that exploits the Hessian matrix eigenstructure to converge to a transition state, which, for a first-order transition state on a PES, is a stationary point whose Hessian has exactly one negative eigenvalue.\n\nThe PES functions to be used are:\n1. $E_a(x,y;\\alpha,\\beta,\\gamma,\\delta) = \\left(x^2 - 1\\right)^2 + \\alpha y^2 + \\beta x y + \\gamma x + \\delta y$.\n2. $E_b(x,y) = x^2 + \\left(y^2 - 1\\right)^2$.\n\nLet the Hessian be the matrix of second partial derivatives of $E(x,y)$ with respect to $(x,y)$, and let $m$ be the count of strictly negative eigenvalues of the Hessian at the considered point.\n\nAccuracy and output requirements:\n- Determine $t^\\star$ as the global maximizer of $E(\\mathbf{r}(t))$ over $t \\in [0,1]$ to an absolute accuracy of $10^{-6}$ in $t$ (it is acceptable to achieve this by any mathematically valid means).\n- Compute $E(\\mathbf{r}(t^\\star))$ to at least $10^{-8}$ absolute accuracy.\n- Compute $m$ exactly as the number of negative eigenvalues (count only eigenvalues less than $-10^{-8}$ as negative).\n- Final output format: a single line containing a comma-separated flat list of numbers enclosed in square brackets. For each test case, output the triplet $[t^\\star, E(\\mathbf{r}(t^\\star)), m]$, with $t^\\star$ and $E(\\mathbf{r}(t^\\star))$ each rounded to six decimal places and $m$ as an integer, and concatenate these triplets in order. For example, with two test cases the output would look like $[t_1^\\star,E_1^\\star,m_1,t_2^\\star,E_2^\\star,m_2]$.\n\nTest suite (evaluate in this order):\n- Case 1: $E_a$ with parameters $(\\alpha,\\beta,\\gamma,\\delta) = (1,0,0,0)$; $\\mathbf{A} = (-1,0)$; $\\mathbf{B} = (1,0)$.\n- Case 2: $E_a$ with parameters $(\\alpha,\\beta,\\gamma,\\delta) = (1,0,0,0)$; $\\mathbf{A} = (-1,0)$; $\\mathbf{B} = (-2,0)$.\n- Case 3: $E_a$ with parameters $(\\alpha,\\beta,\\gamma,\\delta) = (1,0.4,0,0)$; $\\mathbf{A} = (-1,0.5)$; $\\mathbf{B} = (1,-0.5)$.\n- Case 4: $E_b$; $\\mathbf{A} = (0,-1)$; $\\mathbf{B} = (0,1)$.\n- Case 5: $E_a$ with parameters $(\\alpha,\\beta,\\gamma,\\delta) = (1,0,0,0)$; $\\mathbf{A} = (2,0)$; $\\mathbf{B} = (1,0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact order of the test cases: $[t_1^\\star,E_1^\\star,m_1,t_2^\\star,E_2^\\star,m_2,t_3^\\star,E_3^\\star,m_3,t_4^\\star,E_4^\\star,m_4,t_5^\\star,E_5^\\star,m_5]$. No physical units are required in this problem.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of computational chemistry, specifically potential energy surface (PES) exploration. The problem is well-posed, providing all necessary mathematical definitions, functions, and parameters to arrive at a unique, verifiable solution. The language is objective and the requirements are unambiguous. We may therefore proceed with the solution.\n\nThe core of the problem is to find the maximum of a given two-dimensional potential energy function, $E(x,y)$, along a specified linear path segment. This path, known as the Linear Synchronous Transit (LST) path, connects a starting molecular geometry $\\mathbf{A}$ to an ending geometry $\\mathbf{B}$.\n\nLet the start point be $\\mathbf{A} = (x_0, y_0)$ and the end point be $\\mathbf{B} = (x_1, y_1)$. The LST path $\\mathbf{r}(t)$ is parameterized by a scalar $t \\in [0,1]$ as a linear interpolation:\n$$\n\\mathbf{r}(t) = \\mathbf{A} + t(\\mathbf{B} - \\mathbf{A}) = (1-t)\\mathbf{A} + t\\mathbf{B}\n$$\nThe components of the path are given by:\n$$\nx(t) = x_0 + t(x_1 - x_0)\n$$\n$$\ny(t) = y_0 + t(y_1 - y_0)\n$$\nSubstituting these path equations into the potential energy function $E(x,y)$ yields a one-dimensional function of the parameter $t$, which we denote as $f(t) = E(\\mathbf{r}(t))$. For both potential energy surfaces provided, $E_a(x,y;\\alpha,\\beta,\\gamma,\\delta)$ and $E_b(x,y)$, this function $f(t)$ is a polynomial in $t$. Specifically, since the highest power in both $E_a$ and $E_b$ is a quartic term (e.g., $(x^2)^2 = x^4$) and $x(t)$ is linear in $t$, the resulting function $f(t)$ is a quartic polynomial of the form $f(t) = c_4 t^4 + c_3 t^3 + c_2 t^2 + c_1 t + c_0$.\n\nTo find the value $t^\\star$ that maximizes $f(t)$ on the closed interval $[0,1]$, we apply the Extreme Value Theorem. The maximum must occur either at the endpoints of the interval, $t=0$ and $t=1$, or at a critical point within the open interval $(0,1)$. Critical points are values of $t$ for which the derivative $f'(t)$ is zero.\n\nThe derivative $f'(t)$ of the quartic polynomial $f(t)$ is a cubic polynomial. The roots of this cubic polynomial can be found algebraically or numerically. Let these roots be $t_c$. The candidates for the location of the maximum, $t^\\star$, are the set $\\{0, 1\\} \\cup \\{ t_c \\mid f'(t_c)=0 \\text{ and } t_c \\in (0,1) \\}$. We evaluate $f(t)$ for each candidate in this set and identify the value of $t$ that yields the global maximum. This value is $t^\\star$, and the maximum energy is $E^\\star = f(t^\\star)$.\n\nOnce $t^\\star$ is determined, we find the corresponding point on the path, $\\mathbf{r}(t^\\star) = (x^\\star, y^\\star)$. The final step is to characterize this point by analyzing the Hessian matrix of the potential energy, $H$, evaluated at $(x^\\star, y^\\star)$. The Hessian is the matrix of second partial derivatives:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 E}{\\partial x^2} & \\frac{\\partial^2 E}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 E}{\\partial y \\partial x} & \\frac{\\partial^2 E}{\\partial y^2} \\end{pmatrix}\n$$\nFor the given potential energy surfaces:\n1.  For $E_a(x,y;\\alpha,\\beta,\\gamma,\\delta) = \\left(x^2 - 1\\right)^2 + \\alpha y^2 + \\beta x y + \\gamma x + \\delta y$, the Hessian is:\n    $$\n    H_a(x,y) = \\begin{pmatrix} 12x^2 - 4 & \\beta \\\\ \\beta & 2\\alpha \\end{pmatrix}\n    $$\n2.  For $E_b(x,y) = x^2 + \\left(y^2 - 1\\right)^2$, the Hessian is:\n    $$\n    H_b(x,y) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 12y^2 - 4 \\end{pmatrix}\n    $$\nWe compute the eigenvalues of the Hessian matrix evaluated at $(x^\\star, y^\\star)$. The quantity $m$ is the number of these eigenvalues that are strictly negative. A first-order saddle point, which corresponds to a transition state, is characterized by $m=1$.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Construct the polynomial representations of $x(t)$ and $y(t)$.\n2.  Use these to construct the quartic polynomial $f(t) = E(\\mathbf{r}(t))$.\n3.  Compute the derivative polynomial, $f'(t)$.\n4.  Find the roots of $f'(t)=0$. Filter for real roots within the interval $(0,1)$.\n5.  Form a candidate set for $t^\\star$ including $0$, $1$, and the filtered roots.\n6.  Evaluate $f(t)$ at all candidate points to find the maximum value $E^\\star$ and the corresponding $t^\\star$.\n7.  Calculate the coordinates $\\mathbf{r}(t^\\star) = (x^\\star, y^\\star)$.\n8.  Evaluate the appropriate Hessian matrix, $H_a$ or $H_b$, at $(x^\\star, y^\\star)$.\n9.  Compute the eigenvalues of this $2 \\times 2$ matrix.\n10. Count the number of eigenvalues, $m$, that are less than $-10^{-8}$.\n11. Report the triplet $[t^\\star, E^\\star, m]$ with the specified formatting.\n\nThis procedure, implemented using numerical polynomial manipulation and root-finding, provides a robust and precise solution to the problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LST path optimization problem for a given set of test cases.\n    \"\"\"\n\n    def get_hessian_a(x, y, alpha, beta, gamma, delta):\n        \"\"\"Computes the Hessian matrix for PES E_a.\"\"\"\n        return np.array([\n            [12 * x**2 - 4, beta],\n            [beta, 2 * alpha]\n        ])\n\n    def get_hessian_b(x, y):\n        \"\"\"Computes the Hessian matrix for PES E_b.\"\"\"\n        return np.array([\n            [2, 0],\n            [0, 12 * y**2 - 4]\n        ])\n\n    test_cases = [\n        {'pes': 'a', 'params': (1.0, 0.0, 0.0, 0.0), 'A': (-1.0, 0.0), 'B': (1.0, 0.0)},\n        {'pes': 'a', 'params': (1.0, 0.0, 0.0, 0.0), 'A': (-1.0, 0.0), 'B': (-2.0, 0.0)},\n        {'pes': 'a', 'params': (1.0, 0.4, 0.0, 0.0), 'A': (-1.0, 0.5), 'B': (1.0, -0.5)},\n        {'pes': 'b', 'params': None, 'A': (0.0, -1.0), 'B': (0.0, 1.0)},\n        {'pes': 'a', 'params': (1.0, 0.0, 0.0, 0.0), 'A': (2.0, 0.0), 'B': (1.0, 0.0)},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        pes_id = case['pes']\n        params = case['params']\n        A = np.array(case['A'])\n        B = np.array(case['B'])\n        \n        V = B - A\n        \n        # Path parameterization: r(t) = A + t*V\n        # x(t) = A[0] + t*V[0], y(t) = A[1] + t*V[1]\n        x_poly = np.poly1d([V[0], A[0]])\n        y_poly = np.poly1d([V[1], A[1]])\n        \n        # Energy along path f(t) = E(r(t))\n        if pes_id == 'a':\n            alpha, beta, gamma, delta = params\n            x2_minus_1 = x_poly**2 - 1.0\n            f_poly = x2_minus_1**2 + alpha * y_poly**2 + beta * x_poly * y_poly + gamma * x_poly + delta * y_poly\n        else:  # pes_id == 'b'\n            y2_minus_1 = y_poly**2 - 1.0\n            f_poly = x_poly**2 + y2_minus_1**2\n            \n        # Find critical points by finding roots of the derivative f'(t)\n        f_prime_poly = f_poly.deriv()\n        critical_points = np.roots(f_prime_poly)\n        \n        # Filter for real roots in the open interval (0, 1)\n        real_critical_points = [\n            t.real for t in critical_points if np.isclose(t.imag, 0) and 0 < t.real < 1\n        ]\n        \n        # Candidate values for t are endpoints and interior critical points\n        t_candidates = [0.0, 1.0] + real_critical_points\n        \n        # Evaluate energy at each candidate to find the maximum\n        energies = f_poly(t_candidates)\n        max_idx = np.argmax(energies)\n        \n        t_star = t_candidates[max_idx]\n        E_star = energies[max_idx]\n        \n        # Coordinates of the maximum energy point\n        r_star = A + t_star * V\n        x_star, y_star = r_star[0], r_star[1]\n        \n        # Hessian analysis at (x_star, y_star)\n        if pes_id == 'a':\n            hessian = get_hessian_a(x_star, y_star, *params)\n        else: # pes_id == 'b'\n            hessian = get_hessian_b(x_star, y_star)\n            \n        eigenvalues = np.linalg.eigvals(hessian)\n        \n        # Count number of negative eigenvalues\n        m = np.sum(eigenvalues < -1e-8)\n        \n        # Store formatted results for this case\n        all_results.extend([f\"{t_star:.6f}\", f\"{E_star:.6f}\", str(m)])\n\n    # Print the final list of results\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "2466338"}, {"introduction": "While an LST maximum provides a good initial guess, true transition states are stationary points that must be located with more advanced algorithms. This problem focuses on the core logic of eigenvector-following methods, which use local curvature information from the Hessian matrix to guide the search. Your task is to analyze the local landscape at a given point and determine the correct step, learning how to 'climb' towards the saddle point along the mode of negative curvature while relaxing in all other directions [@problem_id:2466346].", "problem": "You are optimizing a molecular potential energy surface near a guess geometry using a second-order Taylor expansion in mass-weighted Cartesian coordinates. The local model around a point $\\mathbf{x}_0$ is\n$$\nE(\\mathbf{x}_0+\\mathbf{s}) \\approx E(\\mathbf{x}_0) + \\mathbf{g}^{\\top}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s},\n$$\nwhere $\\mathbf{g}$ is the gradient and $\\mathbf{H}$ is the Hessian matrix at $\\mathbf{x}_0$. You intend to locate a first-order saddle point (a transition state) using an eigenvector-following step. In this step, the direction choice is informed by the eigen-decomposition of $\\mathbf{H}$.\n\nAt the current geometry, you have\n$$\n\\mathbf{g} = \\begin{bmatrix} 0.02 \\\\ -0.04 \\end{bmatrix}\n\\quad\\text{and}\\quad\n\\mathbf{H} = \\begin{bmatrix} -0.20 & 0.05 \\\\ 0.05 & 0.30 \\end{bmatrix},\n$$\nwith energies and derivatives expressed in consistent atomic units and the coordinates mass-weighted. Based on the local quadratic model, determine whether an eigenvector-following step toward a transition state should be taken and, if so, along which eigenvector’s direction.\n\nChoose the best statement:\n\nA. Do not take an eigenvector-following step. The Hessian indicates a local minimum region with no negative curvature; use Linear Synchronous Transit (LST) or Quadratic Synchronous Transit (QST) to generate a better transition state guess.\n\nB. Take a transition-state eigenvector-following step that climbs along the eigenvector associated with the unique negative eigenvalue of $\\mathbf{H}$ and descends along the orthogonal eigenvector.\n\nC. Ignore the Hessian and take a steepest-descent step along $-\\mathbf{g}$; for transition state searches, curvature is generally uninformative.\n\nD. Take an eigenvector-following step along the eigenvector associated with the smallest positive eigenvalue to avoid divergence in the direction of negative curvature.", "solution": "The user has provided a problem from the field of computational chemistry, specifically concerning the optimization of a molecular geometry to find a transition state. The task is to validate the problem statement and, if valid, to determine the correct course of action based on the provided gradient and Hessian matrix.\n\n### Step 1: Problem Validation\n\nFirst, I must extract the given information and validate the problem statement.\n\n**Givens:**\n1.  The potential energy surface $E$ is approximated by a second-order Taylor expansion around a point $\\mathbf{x}_0$:\n    $$\n    E(\\mathbf{x}_0+\\mathbf{s}) \\approx E(\\mathbf{x}_0) + \\mathbf{g}^{\\top}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s}\n    $$\n2.  The coordinates are mass-weighted Cartesian coordinates.\n3.  The goal is to find a first-order saddle point (transition state) using an eigenvector-following step.\n4.  The gradient vector at the current point is $\\mathbf{g} = \\begin{bmatrix} 0.02 \\\\ -0.04 \\end{bmatrix}$.\n5.  The Hessian matrix at the current point is $\\mathbf{H} = \\begin{bmatrix} -0.20 & 0.05 \\\\ 0.05 & 0.30 \\end{bmatrix}$.\n6.  All units are consistent atomic units.\n\n**Validation:**\n1.  **Scientific Grounding:** The problem is firmly grounded in computational chemistry and numerical optimization theory. The local quadratic approximation of a potential energy surface and the use of gradient and Hessian information to guide a geometry optimization are standard, well-established techniques. Eigenvector-following is a canonical method for locating saddle points.\n2.  **Well-Posedness:** The problem provides all necessary data (the Hessian matrix) to determine the local curvature. The question is specific and asks for the correct algorithmic choice based on this information. A unique answer can be derived.\n3.  **Objectivity:** The problem is stated in precise, objective, and quantitative terms.\n\nThe problem statement is scientifically sound, well-posed, and objective. It contains no logical contradictions, missing information, or pseudoscientific claims. Therefore, the problem is **valid**. I shall proceed to solve it.\n\n### Step 2: Solution Derivation\n\nThe strategy for a transition state search depends critically on the local curvature of the potential energy surface, which is described by the Hessian matrix $\\mathbf{H}$. A first-order saddle point, or transition state, is a stationary point where the Hessian has exactly one negative eigenvalue. The eigenvector corresponding to this negative eigenvalue defines the direction of the reaction coordinate at the saddle point.\n\nThe eigenvector-following method is designed to locate such points. It works by maximizing the energy along the direction of one selected eigenvector (the transition mode) while minimizing it in the subspace spanned by all other eigenvectors. For a transition state search to be productive, the current geometry should be in a region where the Hessian already has the correct structure, i.e., at least one negative eigenvalue.\n\nLet us determine the eigenvalues of the given Hessian matrix $\\mathbf{H}$:\n$$\n\\mathbf{H} = \\begin{bmatrix} -0.20 & 0.05 \\\\ 0.05 & 0.30 \\end{bmatrix}\n$$\nThe eigenvalues $\\lambda$ are the solutions to the characteristic equation $\\det(\\mathbf{H} - \\lambda\\mathbf{I}) = 0$:\n$$\n\\det \\begin{pmatrix} -0.20 - \\lambda & 0.05 \\\\ 0.05 & 0.30 - \\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant gives the quadratic equation:\n$$\n(-0.20 - \\lambda)(0.30 - \\lambda) - (0.05)(0.05) = 0\n$$\n$$\n-0.06 - 0.30\\lambda + 0.20\\lambda + \\lambda^2 - 0.0025 = 0\n$$\n$$\n\\lambda^2 - 0.10\\lambda - 0.0625 = 0\n$$\nWe solve for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, where $a=1$, $b=-0.10$, and $c=-0.0625$.\n$$\n\\lambda = \\frac{-(-0.10) \\pm \\sqrt{(-0.10)^2 - 4(1)(-0.0625)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{0.10 \\pm \\sqrt{0.01 + 0.25}}{2}\n$$\n$$\n\\lambda = \\frac{0.10 \\pm \\sqrt{0.26}}{2}\n$$\nThe numerical values of the eigenvalues are:\n$$\n\\lambda_1 = \\frac{0.10 + \\sqrt{0.26}}{2} \\approx \\frac{0.10 + 0.5099}{2} \\approx 0.305\n$$\n$$\n\\lambda_2 = \\frac{0.10 - \\sqrt{0.26}}{2} \\approx \\frac{0.10 - 0.5099}{2} \\approx -0.205\n$$\nThe Hessian matrix has one positive eigenvalue ($\\lambda_1 \\approx 0.305$) and one negative eigenvalue ($\\lambda_2 \\approx -0.205$). This indicates that the Hessian index is $1$. The local region has the correct second-order structure for a first-order saddle point.\n\nSince the goal is to find a transition state, and the local curvature is appropriate for one, an eigenvector-following algorithm is the correct choice. The procedure for a transition state search is to:\n1.  **Maximize** the energy along the mode of negative curvature. This means taking a step \"uphill\" along the eigenvector corresponding to the negative eigenvalue, $\\lambda_2$. This is often called \"climbing\".\n2.  **Minimize** the energy in all other directions. This means taking a step \"downhill\" along the eigenvectors corresponding to all positive eigenvalues. In this $2$-dimensional case, this corresponds to the eigenvector associated with $\\lambda_1$. This is often called \"descending\" or \"relaxing\".\n\nThe current point is not a stationary point, as the gradient $\\mathbf{g}$ is non-zero. The eigenvector-following step will use the gradient projected onto the eigenvectors to move towards the saddle point, which is defined by $\\mathbf{g}=\\mathbf{0}$ and the specified Hessian structure.\n\n### Step 3: Evaluation of Options\n\nNow I will analyze each provided option.\n\n**A. Do not take an eigenvector-following step. The Hessian indicates a local minimum region with no negative curvature; use Linear Synchronous Transit (LST) or Quadratic Synchronous Transit (QST) to generate a better transition state guess.**\nThis statement is incorrect. My analysis shows that the Hessian has one negative eigenvalue. Therefore, the region does not correspond to a local minimum (which would require all eigenvalues to be positive), and it possesses the correct negative curvature for a transition state search. Eigenvector-following is precisely the method to use in this situation. LST/QST are methods for generating an initial guess, not for refining a good one.\n**Verdict: Incorrect.**\n\n**B. Take a transition-state eigenvector-following step that climbs along the eigenvector associated with the unique negative eigenvalue of $\\mathbf{H}$ and descends along the orthogonal eigenvector.**\nThis statement is correct. As derived, the Hessian has a unique negative eigenvalue. The standard eigenvector-following procedure for finding a transition state is to maximize the energy (climb) along the direction of negative curvature and minimize the energy (descend) along all orthogonal directions of positive curvature. In a 2D system, the other eigenvector is necessarily orthogonal.\n**Verdict: Correct.**\n\n**C. Ignore the Hessian and take a steepest-descent step along $-\\mathbf{g}$; for transition state searches, curvature is generally uninformative.**\nThis statement is fundamentally incorrect. A steepest-descent step ($\\mathbf{s} \\propto -\\mathbf{g}$) is an algorithm for finding a local minimum, not a saddle point. For any optimization problem beyond simple minimization, and especially for saddle point searches, the second-derivative information contained in the Hessian is of paramount importance. It defines the character of the stationary point and provides the directions for ascent and descent. The assertion that curvature is \"uninformative\" is false.\n**Verdict: Incorrect.**\n\n**D. Take an eigenvector-following step along the eigenvector associated with the smallest positive eigenvalue to avoid divergence in the direction of negative curvature.**\nThis statement describes a strategy used during a minimization when one unexpectedly encounters a saddle point region. To escape the saddle point and find a minimum, one might indeed ignore the direction of negative curvature. However, the objective here is to *find* the saddle point (transition state), not to avoid it. The correct procedure is to follow the direction of negative curvature uphill towards the saddle point.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "2466346"}, {"introduction": "Understanding the theory is a crucial first step; implementing it provides true mastery. In this final practice, you will build your own simplified transition state optimizer based on the eigenvector-following principle. By coding an algorithm that inverts the gradient component along the \"softest\" mode, you will transform a standard minimization routine into a saddle point search, gaining a powerful, hands-on understanding of how these algorithms navigate a potential energy surface to discover the geometry of a transition state [@problem_id:2466329].", "problem": "You are given a smooth scalar potential energy surface (unitless) defined on $\\mathbb{R}^2$ by the function $V:\\mathbb{R}^2 \\to \\mathbb{R}$,\n$$\nV(x,y) \\;=\\; \\tfrac{1}{4}\\left(x^4 + y^4\\right) \\;+\\; \\tfrac{1}{2}\\left(x^2 - y^2\\right) \\;+\\; 0.1\\,x\\,y,\n$$\nwhere $x \\in \\mathbb{R}$ and $y \\in \\mathbb{R}$. Consider the gradient $\\nabla V(x,y) \\in \\mathbb{R}^2$ and the Hessian matrix $H(x,y) \\in \\mathbb{R}^{2\\times 2}$ of $V$ at $(x,y)$. Let $u(x,y) \\in \\mathbb{R}^2$ denote a unit eigenvector of $H(x,y)$ associated with its smallest eigenvalue. Define the linear operator\n$$\nR(x,y) \\;=\\; I \\;-\\; 2\\,u(x,y)\\,u(x,y)^\\top,\n$$\nwhere $I \\in \\mathbb{R}^{2\\times 2}$ is the identity matrix, and $u(x,y)^\\top$ denotes the transpose of $u(x,y)$. For given step size $\\alpha \\in \\mathbb{R}$ with $\\alpha > 0$, define an iterative map on $\\mathbb{R}^2$ by\n$$\n\\begin{aligned}\n\\mathbf{z}_{k+1} \\;&=\\; \\mathbf{z}_k \\;-\\; \\alpha \\, R(\\mathbf{z}_k)\\,\\nabla V(\\mathbf{z}_k),\\\\\n\\mathbf{z}_0 \\;&=\\; (x_0,y_0)^\\top,\n\\end{aligned}\n$$\nwith $k \\in \\mathbb{N}$, $\\mathbf{z}_k = (x_k,y_k)^\\top \\in \\mathbb{R}^2$. A point $\\mathbf{z}_\\star$ is considered an acceptable numerical transition state estimate (in the sense of an index-$1$ saddle point) if the following conditions hold simultaneously:\n- Stationarity to tolerance: $\\left\\|\\nabla V(\\mathbf{z}_\\star)\\right\\|_2 \\le \\tau$, with $\\tau \\in \\mathbb{R}$, $\\tau > 0$.\n- Index-$1$ Hessian: the Hessian $H(\\mathbf{z}_\\star)$ has exactly one strictly negative eigenvalue below the threshold $-\\lambda_{\\mathrm{tol}}$ and exactly one strictly positive eigenvalue above the threshold $+\\lambda_{\\mathrm{tol}}$, with $\\lambda_{\\mathrm{tol}} \\in \\mathbb{R}$, $\\lambda_{\\mathrm{tol}} > 0$.\n\nImplement the iteration above and apply it independently to each case in the test suite below. For each case, run the iteration until either the stationarity condition is satisfied or a maximum number of iterations $K_{\\max} \\in \\mathbb{N}$ is reached. Then, given the final iterate $\\mathbf{z}_{\\mathrm{final}}$, report a boolean indicating whether $\\mathbf{z}_{\\mathrm{final}}$ satisfies both the stationarity and the index-$1$ Hessian conditions.\n\nAll quantities are unitless. Angles are not used. The numerical thresholds $\\tau$ and $\\lambda_{\\mathrm{tol}}$ are shared across all cases in the test suite.\n\nTest suite (each case is a tuple $(x_0,y_0,\\alpha,K_{\\max})$):\n- Case $\\;1$: $(x_0,y_0,\\alpha,K_{\\max}) \\;=\\; (0.7,\\,0.7,\\,0.05,\\,4000)$.\n- Case $\\;2$: $(x_0,y_0,\\alpha,K_{\\max}) \\;=\\; (-1.2,\\,1.0,\\,0.03,\\,4000)$.\n- Case $\\;3$: $(x_0,y_0,\\alpha,K_{\\max}) \\;=\\; (2.5,\\,-2.5,\\,0.3,\\,500)$.\n\nShared thresholds:\n- Gradient norm tolerance: $\\tau \\;=\\; 10^{-6}$.\n- Hessian eigenvalue threshold: $\\lambda_{\\mathrm{tol}} \\;=\\; 10^{-6}$.\n\nFinal output format:\nYour program should produce a single line of output containing the boolean results for the cases in order as a comma-separated list enclosed in square brackets (for example, $[{\\rm True},{\\rm False},{\\rm True}]$).", "solution": "The problem is subjected to validation.\n\nStep 1: Extract Givens.\n-   Potential Energy Surface: $V(x,y) = \\tfrac{1}{4}\\left(x^4 + y^4\\right) + \\tfrac{1}{2}\\left(x^2 - y^2\\right) + 0.1\\,x\\,y$.\n-   Gradient: $\\nabla V(x,y)$.\n-   Hessian: $H(x,y)$.\n-   Eigenvector: $u(x,y)$ is the unit eigenvector of $H(x,y)$ for the smallest eigenvalue.\n-   Operator: $R(x,y) = I - 2\\,u(x,y)\\,u(x,y)^\\top$.\n-   Iterative map: $\\mathbf{z}_{k+1} = \\mathbf{z}_k - \\alpha \\, R(\\mathbf{z}_k)\\,\\nabla V(\\mathbf{z}_k)$, with $\\mathbf{z}_k = (x_k, y_k)^\\top$.\n-   Initial Conditions and Parameters (Test Suite):\n    -   Case 1: $\\mathbf{z}_0 = (0.7, 0.7)^\\top$, $\\alpha = 0.05$, $K_{\\max} = 4000$.\n    -   Case 2: $\\mathbf{z}_0 = (-1.2, 1.0)^\\top$, $\\alpha = 0.03$, $K_{\\max} = 4000$.\n    -   Case 3: $\\mathbf{z}_0 = (2.5, -2.5)^\\top$, $\\alpha = 0.3$, $K_{\\max} = 500$.\n-   Transition State Conditions:\n    -   Stationarity: $\\|\\nabla V(\\mathbf{z}_\\star)\\|_2 \\le \\tau$, with $\\tau = 10^{-6}$.\n    -   Index-1 Hessian: One eigenvalue must be less than $-\\lambda_{\\mathrm{tol}}$ and one must be greater than $+\\lambda_{\\mathrm{tol}}$, with $\\lambda_{\\mathrm{tol}} = 10^{-6}$.\n-   Task: Implement the iterative method, run it for each case, and report a boolean indicating if the final iterate $\\mathbf{z}_{\\mathrm{final}}$ satisfies both transition state conditions.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. It describes a numerical implementation of an eigenvector-following method to locate an index-1 saddle point on a potential energy surface. This is a standard and well-established technique in computational chemistry for finding transition states. The potential $V(x,y)$ is a smooth, well-defined polynomial function. All required mathematical objects (gradient, Hessian) are well-defined.\n\nThe problem is well-posed. All necessary parameters, initial conditions, and termination criteria ($\\tau$ and $K_{\\max}$) are explicitly provided. The algorithm is deterministic, ensuring a unique output for each test case. The final verification criteria are unambiguous.\n\nThe problem formulation is objective and uses precise mathematical language. There are no contradictions, missing information, or pseudoscientific claims. The setup is a standard computational exercise.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be constructed.\n\nHere follows the solution.\n\nThe objective is to implement and apply an eigenvector-following algorithm to locate an index-1 saddle point, also known as a transition state, on a given two-dimensional potential energy surface $V(x,y)$.\n\nThe potential energy is given by the function $V: \\mathbb{R}^2 \\to \\mathbb{R}$:\n$$\nV(x,y) = \\frac{1}{4}(x^4 + y^4) + \\frac{1}{2}(x^2 - y^2) + 0.1xy\n$$\nFor the iterative algorithm, we require the gradient vector $\\nabla V(x,y)$ and the Hessian matrix $H(x,y)$. These are obtained by differentiation.\n\nThe gradient is:\n$$\n\\nabla V(x,y) = \\begin{pmatrix} \\frac{\\partial V}{\\partial x} \\\\ \\frac{\\partial V}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} x^3 + x + 0.1y \\\\ y^3 - y + 0.1x \\end{pmatrix}\n$$\n\nThe Hessian matrix is:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 V}{\\partial x^2} & \\frac{\\partial^2 V}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 V}{\\partial y \\partial x} & \\frac{\\partial^2 V}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 3x^2 + 1 & 0.1 \\\\ 0.1 & 3y^2 - 1 \\end{pmatrix}\n$$\nThe Hessian is a real symmetric matrix, so its eigenvalues are real and its eigenvectors are orthogonal.\n\nThe iterative algorithm is an eigenvector-following method. The update rule is given by:\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_k - \\alpha R(\\mathbf{z}_k)\\nabla V(\\mathbf{z}_k)\n$$\nwhere $\\mathbf{z}_k = (x_k, y_k)^\\top$ is the position at iteration $k$, and $\\alpha$ is a positive step size. The operator $R(\\mathbf{z}_k)$ is defined as:\n$$\nR(\\mathbf{z}_k) = I - 2u(\\mathbf{z}_k)u(\\mathbf{z}_k)^\\top\n$$\nHere, $I$ is the $2 \\times 2$ identity matrix, and $u(\\mathbf{z}_k)$ is the normalized eigenvector corresponding to the smallest eigenvalue of the Hessian matrix $H(\\mathbf{z}_k)$.\n\nThe operator $R$ is a Householder reflection matrix. It reflects a vector across the hyperplane (in this 2D case, a line) orthogonal to the vector $u$. Let us analyze the effect of $R$ on the gradient vector $\\nabla V$. The gradient can be decomposed into a component parallel to $u$ and a component orthogonal to $u$. Let $v$ be the other normalized eigenvector of $H$ (orthogonal to $u$). We can write $\\nabla V = (\\nabla V \\cdot u)u + (\\nabla V \\cdot v)v$. Applying $R$ to $\\nabla V$:\n$$\nR\\nabla V = (I - 2uu^\\top)((\\nabla V \\cdot u)u + (\\nabla V \\cdot v)v) = (\\nabla V \\cdot u)u + (\\nabla V \\cdot v)v - 2uu^\\top((\\nabla V \\cdot u)u + (\\nabla V \\cdot v)v)\n$$\nSince $u$ and $v$ are orthonormal ($u^\\top u = 1$, $u^\\top v = 0$), the expression simplifies to:\n$$\nR\\nabla V = (\\nabla V \\cdot u)u + (\\nabla V \\cdot v)v - 2(\\nabla V \\cdot u)u = -(\\nabla V \\cdot u)u + (\\nabla V \\cdot v)v\n$$\nThus, the component of the gradient along $u$ is inverted, while the component along $v$ is preserved. The update step becomes:\n$$\n\\mathbf{z}_{k+1} - \\mathbf{z}_k = -\\alpha R\\nabla V = \\alpha(\\nabla V \\cdot u)u - \\alpha(\\nabla V \\cdot v)v\n$$\nThis shows that the algorithm takes a step in the direction of the gradient component along the softest mode $u$ (energy maximization) and opposite to the gradient component along the other mode $v$ (energy minimization). This is the principle of searching for an index-1 saddle point: maximizing energy along one direction while minimizing it along all others.\n\nThe numerical procedure for each test case is as follows:\n1.  Initialize the position vector $\\mathbf{z}_0 = (x_0, y_0)^\\top$.\n2.  Iterate for $k = 0, 1, \\dots, K_{\\max}-1$:\n    a.  Calculate the gradient $\\nabla V(\\mathbf{z}_k)$ and its Euclidean norm $\\|\\nabla V(\\mathbf{z}_k)\\|_2$.\n    b.  If $\\|\\nabla V(\\mathbf{z}_k)\\|_2 \\le \\tau = 10^{-6}$, the point is considered stationary, and the iteration terminates.\n    c.  Calculate the Hessian $H(\\mathbf{z}_k)$.\n    d.  Compute the eigenvalues and eigenvectors of $H(\\mathbf{z}_k)$. Since $H$ is a $2 \\times 2$ real symmetric matrix, this is a standard and robust calculation.\n    e.  Identify the normalized eigenvector $u_k$ corresponding to the smallest (most negative or least positive) eigenvalue.\n    f.  Compute the transformed gradient $R(\\mathbf{z}_k)\\nabla V(\\mathbf{z}_k) = \\nabla V(\\mathbf{z}_k) - 2(u_k \\cdot \\nabla V(\\mathbf{z}_k))u_k$.\n    g.  Update the position: $\\mathbf{z}_{k+1} = \\mathbf{z}_k - \\alpha R(\\mathbf{z}_k)\\nabla V(\\mathbf{z}_k)$.\n3.  After the loop finishes (either by convergence or by reaching $K_{\\max}$), the final iterate is designated $\\mathbf{z}_{\\mathrm{final}}$.\n4.  Verify if $\\mathbf{z}_{\\mathrm{final}}$ is an acceptable transition state:\n    a.  **Stationarity Condition:** Check if $\\|\\nabla V(\\mathbf{z}_{\\mathrm{final}})\\|_2 \\le \\tau$.\n    b.  **Index-1 Hessian Condition:** Compute the eigenvalues of $H(\\mathbf{z}_{\\mathrm{final}})$, say $\\lambda_1 \\le \\lambda_2$. Check if $\\lambda_1 < -\\lambda_{\\mathrm{tol}}$ and $\\lambda_2 > \\lambda_{\\mathrm{tol}}$, where $\\lambda_{\\mathrm{tol}} = 10^{-6}$.\n5.  The final result for the case is true if and only if both conditions are met.\n\nThis procedure is applied independently to each of the three test cases provided. The implementation will use numerical libraries for linear algebra operations.", "answer": "```python\nimport numpy as np\n\ndef V(z: np.ndarray) -> float:\n    \"\"\"\n    Calculates the potential energy V at a point z = (x, y).\n\n    Args:\n        z: A 2D numpy array representing the coordinates (x, y).\n\n    Returns:\n        The scalar potential energy.\n    \"\"\"\n    x, y = z\n    return 0.25 * (x**4 + y**4) + 0.5 * (x**2 - y**2) + 0.1 * x * y\n\ndef grad_V(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates the gradient of the potential energy V at a point z = (x, y).\n\n    Args:\n        z: A 2D numpy array representing the coordinates (x, y).\n\n    Returns:\n        A 2D numpy array for the gradient vector.\n    \"\"\"\n    x, y = z\n    grad_x = x**3 + x + 0.1 * y\n    grad_y = y**3 - y + 0.1 * x\n    return np.array([grad_x, grad_y])\n\ndef hessian_V(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates the Hessian matrix of the potential energy V at a point z = (x, y).\n\n    Args:\n        z: A 2D numpy array representing the coordinates (x, y).\n\n    Returns:\n        A 2x2 numpy array for the Hessian matrix.\n    \"\"\"\n    x, y = z\n    h_xx = 3 * x**2 + 1.0\n    h_yy = 3 * y**2 - 1.0\n    h_xy = 0.1\n    return np.array([[h_xx, h_xy], [h_xy, h_yy]])\n\ndef run_iteration(x0: float, y0: float, alpha: float, Kmax: int, tau: float) -> np.ndarray:\n    \"\"\"\n    Runs the eigenvector-following iteration to find a saddle point.\n\n    Args:\n        x0: Initial x-coordinate.\n        y0: Initial y-coordinate.\n        alpha: Step size.\n        Kmax: Maximum number of iterations.\n        tau: Gradient norm tolerance for convergence.\n\n    Returns:\n        The final coordinates z_final as a numpy array.\n    \"\"\"\n    z = np.array([x0, y0], dtype=np.float64)\n\n    for _ in range(Kmax):\n        grad = grad_V(z)\n        grad_norm = np.linalg.norm(grad)\n\n        if grad_norm <= tau:\n            break\n\n        H = hessian_V(z)\n        \n        # numpy.linalg.eigh is for symmetric/Hermitian matrices.\n        # It returns eigenvalues in ascending order and corresponding eigenvectors.\n        eigenvalues, eigenvectors = np.linalg.eigh(H)\n        \n        # u is the eigenvector corresponding to the smallest eigenvalue.\n        u = eigenvectors[:, 0]\n\n        # Apply the reflection operator R = I - 2*u*u^T to the gradient.\n        # This is computed efficiently as grad - 2 * dot(u, grad) * u\n        R_grad = grad - 2 * np.dot(u, grad) * u\n\n        # Update the position.\n        z = z - alpha * R_grad\n        \n    return z\n\ndef check_transition_state(z: np.ndarray, tau: float, lambda_tol: float) -> bool:\n    \"\"\"\n    Checks if a point z is an acceptable numerical transition state.\n\n    Args:\n        z: The point coordinates (x, y).\n        tau: Gradient norm tolerance.\n        lambda_tol: Hessian eigenvalue threshold.\n\n    Returns:\n        True if the point satisfies both stationarity and index-1 Hessian conditions, False otherwise.\n    \"\"\"\n    # 1. Stationarity condition\n    grad_norm = np.linalg.norm(grad_V(z))\n    is_stationary = grad_norm <= tau\n\n    # 2. Index-1 Hessian condition\n    H_final = hessian_V(z)\n    eigenvalues = np.linalg.eigvalsh(H_final) # More efficient for symmetric\n    \n    # eigenvalues from eigvalsh are sorted in ascending order\n    lambda1, lambda2 = eigenvalues\n    \n    is_index1 = (lambda1 < -lambda_tol) and (lambda2 > lambda_tol)\n\n    return is_stationary and is_index1\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the simulation for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # (x0, y0, alpha, Kmax)\n        (0.7, 0.7, 0.05, 4000),\n        (-1.2, 1.0, 0.03, 4000),\n        (2.5, -2.5, 0.3, 500),\n    ]\n\n    # Shared thresholds\n    tau = 1e-6\n    lambda_tol = 1e-6\n\n    results = []\n    for x0, y0, alpha, Kmax in test_cases:\n        z_final = run_iteration(x0, y0, alpha, Kmax, tau)\n        is_ts = check_transition_state(z_final, tau, lambda_tol)\n        results.append(is_ts)\n\n    # Format the output as specified: [True,False,True]\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2466329"}]}