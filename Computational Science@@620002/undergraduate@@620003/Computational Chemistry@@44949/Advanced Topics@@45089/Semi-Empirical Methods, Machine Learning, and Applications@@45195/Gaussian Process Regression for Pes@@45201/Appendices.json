{"hands_on_practices": [{"introduction": "Building an accurate potential energy surface (PES) requires not just a good model, but strategic placement of training data. This exercise provides a hands-on demonstration of a core tenet of Gaussian Process Regression: its strength in interpolation versus its limitations in extrapolation. By predicting the energy of a saddle point with and without nearby data points, you will directly observe how crucial it is to sample chemically significant regions, such as transition states, to build a reliable PES [@problem_id:2455992].", "problem": "You are given a two-dimensional potential energy surface (PES) defined in dimensionless units by the analytic function\n$$\nV(x,y) = (x^2 - 1)^2 + a\\, y^2,\n$$\nwith parameter $a = 0.5$. This surface has two minima near $(x,y) = (-1,0)$ and $(x,y) = (1,0)$ and a first-order saddle point at $(x,y) = (0,0)$ with energy $V(0,0) = 1$. Consider a zero-mean Gaussian Process (GP) prior for the PES values with a squared-exponential covariance kernel\n$$\nk(\\mathbf{r}, \\mathbf{r}') = \\sigma_f^2 \\exp\\!\\left(-\\tfrac{1}{2}\\sum_{i=1}^{2} \\frac{(r_i - r'_i)^2}{\\ell^2}\\right),\n$$\nwhere $\\mathbf{r} = (x,y)$, $\\ell$ is an isotropic length scale, and $\\sigma_f$ is the signal amplitude. Assume independent and identically distributed Gaussian observation noise with variance $\\sigma_n^2$ added to the energy values. The posterior predictive mean of the GP at a query point $\\mathbf{r}_\\star$ is defined by the standard Gaussian conditioning formula in terms of the kernel matrix on the training inputs, the cross-kernel vector between training inputs and $\\mathbf{r}_\\star$, and the observed energies.\n\nYour task is to implement a program that computes the GP posterior predictive mean at the saddle point $\\mathbf{r}_\\star = (0,0)$, using the exact PES above to generate noiseless training energies, under the following four test cases. All quantities are dimensionless. Use the fixed hyperparameters $\\sigma_f = 1$, and $\\sigma_n^2 = 10^{-12}$ unless otherwise specified. The length scale $\\ell$ is specified per test case below.\n\nDefine the PES parameter $a = 0.5$ exactly as above in all cases.\n\nTraining inputs must be used exactly as listed. For each training input $(x_j,y_j)$, compute the training target $V(x_j,y_j)$ using the given analytic expression.\n\nTest Case 1 (without near-saddle sampling):\n- Length scale: $\\ell = 0.4$.\n- Training inputs (list of $(x,y)$):\n  - $(-1, 0)$, $(1, 0)$,\n  - $(-1, 0.5)$, $(1, 0.5)$, $(-1, -0.5)$, $(1, -0.5)$,\n  - $(0.9, 0)$, $(-0.9, 0)$,\n  - $(-0.75, 0.8)$, $(0.75, -0.8)$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nTest Case 2 (with near-saddle sampling):\n- Length scale: $\\ell = 0.4$.\n- Training inputs: all points from Test Case $1$ plus the additional points $(0.1, 0)$, $(-0.1, 0)$, $(0, 0.1)$, $(0, -0.1)$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nTest Case 3 (boundary case: extremely short correlation without near-saddle sampling):\n- Length scale: $\\ell = 0.1$.\n- Training inputs: identical to Test Case $1$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nTest Case 4 (boundary case: exact observation at the saddle):\n- Length scale: $\\ell = 0.4$.\n- Training inputs: all points from Test Case $1$ plus the exact saddle point $(0,0)$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nProgram input: There is no external input; all data are specified here. Program output: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Test Cases $1,2,3,4$. Each number must be the posterior predictive mean at $(0,0)$ for the respective test case, rounded to $6$ decimal places. For example: \"[m1,m2,m3,m4]\".\n\nYour program must compute all quantities from first principles using the definitions above. All energies must be computed in the specified dimensionless units, and all angles, if any appear, are not relevant here. The final outputs are floats. The four-test-case suite covers the general prediction without local data, the improved prediction with nearby data, the effect of very short correlation length without nearby data, and the limiting case when the query point is itself a training input.", "solution": "The problem statement has been subjected to rigorous validation and is found to be scientifically sound, well-posed, and free of contradictions. It presents a standard, well-defined exercise in the application of Gaussian Process regression for modeling a potential energy surface, a common task in computational chemistry. We shall therefore proceed with a complete, principled solution.\n\nThe objective is to compute the posterior predictive mean of a Gaussian Process (GP) model at a specific query point, the saddle point $\\mathbf{r}_\\star = (0,0)$ of a given two-dimensional potential energy surface (PES). The solution is derived from the fundamental principles of Bayesian inference applied to function spaces.\n\nThe PES is defined by the analytical function:\n$$\nV(\\mathbf{r}) = (x^2 - 1)^2 + a y^2\n$$\nwhere $\\mathbf{r} = (x,y)$ and the parameter $a$ is fixed at $a=0.5$. The true energy at the saddle point $\\mathbf{r}_\\star = (0,0)$ is $V(0,0) = (0^2-1)^2 + 0.5(0^2) = 1$.\n\nWe model the PES using a zero-mean Gaussian Process, $f(\\mathbf{r}) \\sim \\mathcal{GP}(0, k(\\mathbf{r}, \\mathbf{r}'))$. The covariance between the function values at any two points $\\mathbf{r}$ and $\\mathbf{r}'$ is given by the squared-exponential kernel:\n$$\nk(\\mathbf{r}, \\mathbf{r}') = \\sigma_f^2 \\exp\\!\\left(-\\frac{\\|\\mathbf{r} - \\mathbf{r}'\\|^2}{2\\ell^2}\\right)\n$$\nHere, $\\|\\mathbf{r} - \\mathbf{r}'\\|^2$ is the squared Euclidean distance, $\\sigma_f$ is the signal amplitude, and $\\ell$ is the characteristic length scale. The problem specifies $\\sigma_f = 1$.\n\nWe are given a set of $N$ training inputs $\\mathbf{X} = \\{\\mathbf{r}_1, \\ldots, \\mathbf{r}_N\\}$ and corresponding noiseless training targets $\\mathbf{y} = \\{y_1, \\ldots, y_N\\}$, where $y_j = V(\\mathbf{r}_j)$. Although the training targets are noiseless, the model includes an observation noise term with variance $\\sigma_n^2$, which is a standard technique to ensure numerical stability. The posterior distribution of the function value $f_\\star = f(\\mathbf{r}_\\star)$ at a test point $\\mathbf{r}_\\star$, conditioned on the training data $(\\mathbf{X}, \\mathbf{y})$, is also a Gaussian distribution. Its mean, which is our desired quantity, is given by the equation for GP regression:\n$$\n\\bar{f}_\\star = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}\n$$\nwhere:\n- $K$ is the $N \\times N$ Gram matrix of the training inputs, with elements $K_{ij} = k(\\mathbf{r}_i, \\mathbf{r}_j)$.\n- $\\mathbf{k}_\\star$ is the $N \\times 1$ vector of covariances between the training inputs and the test point, with elements $(\\mathbf{k}_\\star)_i = k(\\mathbf{r}_i, \\mathbf{r}_\\star)$.\n- $I$ is the $N \\times N$ identity matrix.\n- $\\sigma_n^2$ is the noise variance, specified as $10^{-12}$. This small value, often called a \"nugget,\" regularizes the matrix $K$, guaranteeing its invertibility.\n\nThe computational procedure is as follows:\n1.  For each test case, assemble the specified training inputs $\\mathbf{X}$ and the fixed hyperparameters $\\ell$, $\\sigma_f$, and $\\sigma_n^2$.\n2.  Compute the training targets $\\mathbf{y}$ by evaluating the function $V(\\mathbf{r})$ at each training input $\\mathbf{r}_j \\in \\mathbf{X}$.\n3.  Construct the kernel matrix $K$ by computing $k(\\mathbf{r}_i, \\mathbf{r}_j)$ for all pairs of training inputs.\n4.  Construct the cross-covariance vector $\\mathbf{k}_\\star$ by computing $k(\\mathbf{r}_i, \\mathbf{r}_\\star)$ for all training inputs $\\mathbf{r}_i$ with respect to the query point $\\mathbf{r}_\\star=(0,0)$.\n5.  Instead of explicit matrix inversion, which is numerically unstable, we solve the more stable linear system $(K + \\sigma_n^2 I) \\boldsymbol{\\alpha} = \\mathbf{y}$ for the weight vector $\\boldsymbol{\\alpha}$.\n6.  The posterior mean is then calculated as the inner product $\\bar{f}_\\star = \\mathbf{k}_\\star^T \\boldsymbol{\\alpha}$.\n\nThis procedure will be applied to each of the four specified test cases, which are designed to probe different aspects of GP regression:\n- **Test Case 1** establishes a baseline prediction where training data are relatively distant from the query point. The prediction will be a weighted average of the training targets, with weights decaying with distance, pulled towards the prior mean of $0$.\n- **Test Case 2** demonstrates the effect of adding local data near the query point. This new information is expected to dominate the prediction, driving the posterior mean much closer to the true value of $V(0,0) = 1$.\n- **Test Case 3** investigates the impact of a very short correlation length $\\ell$. This makes the model assume the function varies rapidly, so the influence of distant training points diminishes sharply. The prediction at $(0,0)$ will revert more strongly towards the prior mean of $0$.\n- **Test Case 4** confirms the interpolation behavior of the GP. By including the query point $(0,0)$ in the training set, the model is constrained to pass very close to the observed value $V(0,0)=1$, with any deviation being a consequence of the small regularization term $\\sigma_n^2$. The result should be extremely close to $1$.\n\nThe implementation will strictly follow this validated methodology.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian Process Regression problem for four test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    # PES analytic function V(x,y)\n    a_param = 0.5\n    def v_pes(r):\n        x, y = r\n        return (x**2 - 1)**2 + a_param * y**2\n\n    # Squared-exponential kernel function k(r, r')\n    def kernel(r1, r2, sigma_f, ell):\n        dist_sq = np.sum((np.array(r1) - np.array(r2))**2)\n        return sigma_f**2 * np.exp(-dist_sq / (2 * ell**2))\n\n    # --- Fixed Parameters ---\n    sigma_f_val = 1.0\n    sigma_n_sq_val = 1e-12\n    r_star = np.array([0.0, 0.0])\n\n    # --- Test Case Definitions ---\n    train_inputs_case1 = [\n        (-1.0, 0.0), (1.0, 0.0),\n        (-1.0, 0.5), (1.0, 0.5), (-1.0, -0.5), (1.0, -0.5),\n        (0.9, 0.0), (-0.9, 0.0),\n        (-0.75, 0.8), (0.75, -0.8)\n    ]\n\n    train_inputs_case2 = train_inputs_case1 + [\n        (0.1, 0.0), (-0.1, 0.0), (0.0, 0.1), (0.0, -0.1)\n    ]\n\n    train_inputs_case4 = train_inputs_case1 + [(0.0, 0.0)]\n\n    test_cases = [\n        {\"ell\": 0.4, \"train_inputs\": train_inputs_case1},\n        {\"ell\": 0.4, \"train_inputs\": train_inputs_case2},\n        {\"ell\": 0.1, \"train_inputs\": train_inputs_case1},\n        {\"ell\": 0.4, \"train_inputs\": train_inputs_case4},\n    ]\n\n    # --- Main Calculation Loop ---\n    results = []\n    for case in test_cases:\n        ell_val = case[\"ell\"]\n        train_inputs = np.array(case[\"train_inputs\"])\n        n_train = len(train_inputs)\n\n        # 1. Generate training targets y\n        y_train = np.array([v_pes(r) for r in train_inputs])\n\n        # 2. Construct kernel matrix K\n        K = np.zeros((n_train, n_train))\n        for i in range(n_train):\n            for j in range(n_train):\n                K[i, j] = kernel(train_inputs[i], train_inputs[j], sigma_f_val, ell_val)\n        \n        # 3. Add regularization term\n        K_reg = K + sigma_n_sq_val * np.identity(n_train)\n        \n        # 4. Construct cross-kernel vector k_star\n        k_star = np.array([kernel(r, r_star, sigma_f_val, ell_val) for r in train_inputs])\n        \n        # 5. Solve for weights alpha\n        alpha = np.linalg.solve(K_reg, y_train)\n        \n        # 6. Compute posterior mean\n        mean_prediction = np.dot(k_star, alpha)\n        \n        results.append(mean_prediction)\n\n    # Round results to 6 decimal places for the final output\n    rounded_results = [round(res, 6) for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, rounded_results))}]\")\n\nsolve()\n```", "id": "2455992"}, {"introduction": "A key feature of Gaussian Process Regression is its ability to quantify uncertainty, which is especially important when predicting into unknown regions of a PES. This practice explores the fundamental behavior of a GPR model when it is asked to extrapolate far beyond its training data [@problem_id:2455949]. By analyzing this common scenario, you will understand why GPR models with standard kernels tend to \"revert to the prior,\" a predictable and essential characteristic for safely applying these models in chemistry.", "problem": "In fitting a $1$-dimensional cut of a molecular Potential Energy Surface (PES) as a function of a bond angle $\\theta$, you use Gaussian Process Regression (GPR) with a constant prior mean $m(\\theta)=m_0$ and a stationary squared-exponential covariance kernel $k(\\theta,\\theta')=\\sigma_f^2\\exp\\!\\big(-(\\theta-\\theta')^2/(2\\ell^2)\\big)$ with finite length scale $\\ell>0$. The training data consist only of energies sampled for angles $\\theta$ in the interval $\\left[\\pi/2,\\,2\\pi/3\\right]$ (i.e., approximately $90^\\circ$ to $120^\\circ$). You then use this GPR model to predict the PES and forces for configurations at angles $\\theta$ outside this interval, including near $\\theta\\approx 0$ and $\\theta\\approx \\pi$. Here, a force component along the angular coordinate is defined as $F_\\theta(\\theta)=-\\partial E(\\theta)/\\partial\\theta$.\n\nWhich option best describes the expected failure mode of this GPR model when used at angles outside the training range?\n\nA. Outside the training range, the predictive mean tends to the prior mean $m_0$ while the predictive variance approaches the prior variance scale, yielding an artificially flat PES with near-zero predicted forces $F_\\theta(\\theta)\\approx 0$ and large uncertainty.\n\nB. Outside the training range, the GPR extrapolates by continuing the last observed slope linearly, and the predictive uncertainty decreases the farther one goes beyond the boundary, often causing runaway dynamics due to steadily increasing forces.\n\nC. Outside the training range, the kernel automatically enforces the periodicity of angular coordinates, so the learned behavior on $\\left[\\pi/2,\\,2\\pi/3\\right]$ is wrapped around to all $\\theta$ modulo $2\\pi$, yielding physically accurate predictions and forces near $\\theta=0$ and $\\theta=\\pi$.\n\nD. Outside the training range, the positive definiteness of the kernel forces the predictive mean to diverge to $+\\infty$ as $|\\theta|$ increases, which makes the predicted forces blow up to infinity in those regions.", "solution": "The user-provided problem statement must first be validated for correctness and coherence.\n\n### Step 1: Extract Givens\n- **System**: A $1$-dimensional cut of a molecular Potential Energy Surface (PES), denoted as a function of a bond angle $\\theta$.\n- **Model**: Gaussian Process Regression (GPR).\n- **Prior Mean Function**: Constant, $m(\\theta) = m_0$.\n- **Covariance Kernel (Function)**: Stationary squared-exponential kernel, $k(\\theta,\\theta')=\\sigma_f^2\\exp\\!\\big(-(\\theta-\\theta')^2/(2\\ell^2)\\big)$.\n- **Kernel Parameters**: Finite length scale $\\ell > 0$ and prior variance $\\sigma_f^2$.\n- **Training Data**: Energy samples for angles $\\theta$ within the interval $\\left[\\pi/2,\\,2\\pi/3\\right]$.\n- **Prediction Task**: Extrapolate the PES and forces to angles $\\theta$ outside the training interval, specifically mentioning regions near $\\theta\\approx 0$ and $\\theta\\approx \\pi$.\n- **Force Definition**: The force component along the angular coordinate is $F_\\theta(\\theta)=-\\partial E(\\theta)/\\partial\\theta$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against standard scientific and mathematical principles.\n- **Scientific Grounding**: The problem is well-grounded. GPR is a standard non-parametric regression technique widely used in computational chemistry to model potential energy surfaces. The squared-exponential (SE) kernel, also known as the radial basis function (RBF) kernel, is one of the most common choices for GPR. The definition of force as the negative gradient of potential energy is a fundamental principle in physics.\n- **Well-Posedness**: The problem is well-posed. It describes a specific GPR model (prior and kernel) and a clear extrapolation scenario. The behavior of a GPR model in regions far from training data is a well-defined and predictable characteristic determined by the model's prior assumptions.\n- **Objectivity**: The language is technical, precise, and objective. There are no subjective or ambiguous statements.\n- **Completeness and Consistency**: The problem provides all necessary information to deduce the qualitative behavior of the model. It is self-contained and free of contradictions. The chosen kernel is explicit, which is crucial for the analysis.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, and complete. It is therefore **valid**. I will proceed with the derivation of the solution and the evaluation of the provided options.\n\n### Solution Derivation\n\nA Gaussian Process is fully defined by its mean function $m(\\theta)$ and covariance function $k(\\theta, \\theta')$. Given a set of training data points $(\\boldsymbol{\\theta}, \\mathbf{y})$, where $\\boldsymbol{\\theta} = \\{\\theta_1, \\dots, \\theta_N\\}$ is the vector of training inputs and $\\mathbf{y} = \\{y_1, \\dots, y_N\\}$ is the vector of observed energies, the predictive distribution for a new test point $\\theta_*$ is a Gaussian distribution with mean $\\mu_*(\\theta_*)$ and variance $\\sigma_*^2(\\theta_*)$.\n\nThe predictive mean and variance are given by the equations:\n$$ \\mu_*(\\theta_*) = m(\\theta_*) + \\mathbf{k}_*^T \\mathbf{K}_y^{-1} (\\mathbf{y} - m(\\boldsymbol{\\theta})) $$\n$$ \\sigma_*^2(\\theta_*) = k(\\theta_*, \\theta_*) - \\mathbf{k}_*^T \\mathbf{K}_y^{-1} \\mathbf{k}_* $$\nwhere:\n- $m(\\boldsymbol{\\theta})$ is the prior mean evaluated at the training points.\n- $\\mathbf{K}$ is the Gram matrix of the training points, with entries $K_{ij} = k(\\theta_i, \\theta_j)$.\n- $\\mathbf{K}_y = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$ is the covariance matrix of the noisy observations, where $\\sigma_n^2$ is the variance of the observation noise (assumed to be zero if not mentioned, but its presence does not alter the conclusion).\n- $\\mathbf{k}_*$ is the vector of covariances between the test point $\\theta_*$ and the training points, with entries $(\\mathbf{k}_*)_i = k(\\theta_*, \\theta_i)$.\n\nThe problem specifies a constant prior mean $m(\\theta) = m_0$ and a squared-exponential kernel $k(\\theta,\\theta')=\\sigma_f^2\\exp\\!\\big(-(\\theta-\\theta')^2/(2\\ell^2)\\big)$. We are interested in the behavior of the model for a test point $\\theta_*$ that is far from the training data range $\\left[\\pi/2,\\,2\\pi/3\\right]$.\n\nFor such a $\\theta_*$, the distance $|\\theta_* - \\theta_i|$ is large for all training points $\\theta_i \\in \\left[\\pi/2,\\,2\\pi/3\\right]$.\nLet us examine the components of the vector $\\mathbf{k}_*$:\n$$ (\\mathbf{k}_*)_i = k(\\theta_*, \\theta_i) = \\sigma_f^2\\exp\\!\\left(-\\frac{(\\theta_*-\\theta_i)^2}{2\\ell^2}\\right) $$\nAs $|\\theta_* - \\theta_i| \\to \\infty$, the exponential term rapidly decays to zero. Consequently, every element of the vector $\\mathbf{k}_*$ approaches $0$. In vector notation, $\\mathbf{k}_* \\to \\mathbf{0}$.\n\nNow we analyze the asymptotic behavior of the predictive mean and variance.\n\n**Predictive Mean Behavior:**\nAs $\\mathbf{k}_* \\to \\mathbf{0}$, the second term in the expression for $\\mu_*(\\theta_*)$ vanishes:\n$$ \\mathbf{k}_*^T \\mathbf{K}_y^{-1} (\\mathbf{y} - m(\\boldsymbol{\\theta})) \\to \\mathbf{0}^T \\mathbf{K}_y^{-1} (\\mathbf{y} - m(\\boldsymbol{\\theta})) = 0 $$\nTherefore, the predictive mean reverts to the prior mean:\n$$ \\mu_*(\\theta_*) \\to m(\\theta_*) = m_0 $$\nSince $m_0$ is a constant, the predicted PES becomes an artificially flat hyper-plane in regions far from the training data.\n\n**Predictive Variance Behavior:**\nSimilarly, we examine the predictive variance $\\sigma_*^2(\\theta_*)$:\n$$ \\sigma_*^2(\\theta_*) = k(\\theta_*, \\theta_*) - \\mathbf{k}_*^T \\mathbf{K}_y^{-1} \\mathbf{k}_* $$\nAs $\\mathbf{k}_* \\to \\mathbf{0}$, the second term, which represents the reduction in uncertainty due to the data, vanishes.\nThe predictive variance thus reverts to the prior variance at that point:\n$$ \\sigma_*^2(\\theta_*) \\to k(\\theta_*, \\theta_*) = \\sigma_f^2\\exp\\!\\left(-\\frac{(\\theta_*-\\theta_*)^2}{2\\ell^2}\\right) = \\sigma_f^2 \\exp(0) = \\sigma_f^2 $$\nThe predictive variance approaches its maximum possible value, $\\sigma_f^2$, which signifies a large degree of uncertainty about the function's value.\n\n**Predicted Force Behavior:**\nThe predicted force is the negative derivative of the predicted mean energy:\n$$ F_\\theta(\\theta_*) = -\\frac{\\partial \\mu_*(\\theta_*)}{\\partial \\theta_*} $$\nIn the extrapolation regime, $\\mu_*(\\theta_*) \\to m_0$. Since $m_0$ is a constant, its derivative is zero.\n$$ F_\\theta(\\theta_*) \\to -\\frac{\\partial}{\\partial \\theta_*} (m_0) = 0 $$\nThe predicted forces decay to zero far from the training data.\n\nIn summary, when extrapolating far from the training data, the GPR model with a constant prior mean and an SE kernel predicts that the function value is the constant prior mean $m_0$, the uncertainty is the prior variance $\\sigma_f^2$, and the forces are approximately zero. This results in an unphysical, flat PES with large associated uncertainty.\n\n### Option-by-Option Analysis\n\n**A. Outside the training range, the predictive mean tends to the prior mean $m_0$ while the predictive variance approaches the prior variance scale, yielding an artificially flat PES with near-zero predicted forces $F_\\theta(\\theta)\\approx 0$ and large uncertainty.**\nThis statement is a perfect summary of our derivation. The predictive mean reverts to the constant prior $m_0$, creating a flat PES. The predictive variance reverts to the prior variance $\\sigma_f^2$, indicating large uncertainty. The derivative of a constant mean is zero, so forces are predicted to be near zero.\n**Verdict: Correct.**\n\n**B. Outside the training range, the GPR extrapolates by continuing the last observed slope linearly, and the predictive uncertainty decreases the farther one goes beyond the boundary, often causing runaway dynamics due to steadily increasing forces.**\nThis is incorrect on several points. Linear extrapolation is characteristic of GPR models with an integrated Brownian motion prior (Wiener process), not a squared-exponential kernel. The SE kernel leads to reversion to the mean. The predictive uncertainty *increases*, not decreases, as one moves away from the data. The predicted forces tend to zero, not increase.\n**Verdict: Incorrect.**\n\n**C. Outside the training range, the kernel automatically enforces the periodicity of angular coordinates, so the learned behavior on $\\left[\\pi/2,\\,2\\pi/3\\right]$ is wrapped around to all $\\theta$ modulo $2\\pi$, yielding physically accurate predictions and forces near $\\theta=0$ and $\\theta=\\pi$.**\nThe squared-exponential kernel, $k(\\theta,\\theta')=\\sigma_f^2\\exp\\!\\big(-(\\theta-\\theta')^2/(2\\ell^2)\\big)$, is a function of the Euclidean distance $(\\theta-\\theta')$ and is not periodic. To model periodic functions, a specific periodic kernel, such as $k_P(\\theta, \\theta') = \\sigma_f^2 \\exp(-\\frac{2\\sin^2((\\theta-\\theta')/2)}{\\ell^2})$, would be required. The specified kernel has no knowledge of the $2\\pi$ periodicity of the coordinate $\\theta$. Therefore, the model will not \"wrap around\" the learned behavior.\n**Verdict: Incorrect.**\n\n**D. Outside the training range, the positive definiteness of the kernel forces the predictive mean to diverge to $+\\infty$ as $|\\theta|$ increases, which makes the predicted forces blow up to infinity in those regions.**\nPositive definiteness is a required property for any covariance function to ensure that the variance of any linear combination of random variables is non-negative. It has no direct bearing on the asymptotic behavior of the predictive mean. As derived, the predictive mean does not diverge; it reverts to the finite constant prior mean $m_0$. Consequently, the forces do not blow up; they tend to zero.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2455949"}, {"introduction": "Real molecular systems exhibit different types of motion, each with its own physical character. This exercise moves beyond simple kernels to the powerful technique of kernel engineering, where you will construct a composite kernel tailored to a molecule's specific degrees of freedom [@problem_id:2456000]. By combining a squared-exponential kernel for bond stretches with a periodic kernel for a dihedral angle, you will implement a sophisticated, physically-motivated model that demonstrates the flexibility and power of GPR for building realistic potential energy surfaces.", "problem": "You are modeling a reduced potential energy surface for a molecular fragment described by two bond stretches and one dihedral angle. Let each configuration be represented by the descriptor vector $\\mathbf{x} = (r_1, r_2, \\phi)$, where $r_1$ and $r_2$ are bond lengths in $\\text{\\AA}$, and $\\phi$ is a dihedral angle in radians. You will assume a Gaussian process prior with zero mean and a composite kernel that is the sum of a squared-exponential term over the bond stretches and a periodic term over the dihedral angle:\n$$\nk(\\mathbf{x},\\mathbf{x}') \\equiv k_{\\mathrm{SE}}(\\mathbf{x},\\mathbf{x}') + k_{\\mathrm{Per}}(\\mathbf{x},\\mathbf{x}'),\n$$\nwhere\n$$\nk_{\\mathrm{SE}}(\\mathbf{x},\\mathbf{x}') = \\sigma_s^2 \\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^{2}\\left(\\frac{r_i - r_i'}{\\ell_s}\\right)^2\\right),\n$$\nand\n$$\nk_{\\mathrm{Per}}(\\mathbf{x},\\mathbf{x}') = \\sigma_p^2 \\exp\\left(-\\frac{2\\sin^2\\left(\\tfrac{\\phi - \\phi'}{2}\\right)}{\\ell_p^2}\\right).\n$$\nUse the hyperparameters $\\sigma_s = 2.5$ $\\text{eV}$, $\\ell_s = 0.2$ $\\text{\\AA}$, $\\sigma_p = 0.4$ $\\text{eV}$, and $\\ell_p = 0.5$ (dimensionless in radians). Assume independent and identically distributed observation noise with variance $\\sigma_n^2$, where $\\sigma_n = 0.02$ $\\text{eV}$.\n\nThe observed training energies are generated from a physically motivated reference energy function composed of two identical Morse bond terms and a torsional term:\n$$\nE_{\\mathrm{ref}}(r_1,r_2,\\phi) = \\sum_{i=1}^{2}\\left[D\\left(1 - e^{-a(r_i - r_0)}\\right)^2 - D\\right] + V_0\\left(1 - \\cos(n\\phi)\\right),\n$$\nwith parameters $D = 4.5$ $\\text{eV}$, $a = 1.7$ $\\text{\\AA}^{-1}$, $r_0 = 1.09$ $\\text{\\AA}$, $V_0 = 0.18$ $\\text{eV}$, and $n = 3$. The training inputs $\\{\\mathbf{x}_j\\}_{j=1}^{N}$ and corresponding outputs $\\{y_j\\}_{j=1}^{N}$ are defined by $y_j = E_{\\mathrm{ref}}(\\mathbf{x}_j)$ at the following $N = 5$ configurations (units: $r$ in $\\text{\\AA}$, $\\phi$ in radians):\n- $\\mathbf{x}_1 = (r_0, r_0, 0.0)$,\n- $\\mathbf{x}_2 = (r_0 + 0.1, r_0, 0.5)$,\n- $\\mathbf{x}_3 = (r_0, r_0 + 0.15, 1.0)$,\n- $\\mathbf{x}_4 = (r_0 + 0.2, r_0 + 0.2, 2.0)$,\n- $\\mathbf{x}_5 = (r_0 - 0.1, r_0, 3.0)$.\n\nAssume a Gaussian process prior with zero mean and covariance function $k(\\cdot,\\cdot)$ as above, and a likelihood with additive Gaussian noise of variance $\\sigma_n^2$. Let the test inputs be the following $M = 4$ configurations (units: $r$ in $\\text{\\AA}$, $\\phi$ in radians):\n- $\\mathbf{x}_1^\\star = (r_0, r_0, 0.0)$,\n- $\\mathbf{x}_2^\\star = (r_0, r_0, \\pi)$,\n- $\\mathbf{x}_3^\\star = (r_0 + 0.3, r_0, -\\pi)$,\n- $\\mathbf{x}_4^\\star = (r_0 + 0.05, r_0 + 0.05, 1.0)$.\n\nCompute the posterior predictive mean of the Gaussian process at each test input, defined as the conditional expectation of the predicted energies given the training data under the joint multivariate normal model with the specified kernel and noise variance. Express all predicted energies in $\\text{eV}$. No angles in degrees are allowed; all angles must be in radians.\n\nYour program must implement the above specification exactly, using the stated hyperparameters and training data generated from $E_{\\mathrm{ref}}$. The test suite consists of the $M = 4$ test configurations listed above and covers the following cases: a point coinciding with a training configuration, a maximally staggered dihedral at $\\phi = \\pi$, a periodic wrap at $\\phi = -\\pi$, and a small symmetric bond stretch with a moderate dihedral. The required outputs are the $M = 4$ posterior mean predictions as real numbers in $\\text{eV}$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ decimal places, for example, $[x_1,x_2,x_3,x_4]$ where each $x_i$ is a decimal numeral representing an energy in $\\text{eV}$.", "solution": "The problem as stated is a well-defined and standard application of Gaussian process regression (GPR) to model a molecular potential energy surface. It is scientifically sound, mathematically consistent, and contains all necessary information for a unique solution. We shall proceed with the derivation.\n\nThe fundamental task is to compute the posterior predictive mean of a Gaussian process (GP) at a set of test points, given a set of training data. A GP defines a distribution over functions, and in this context, we model the potential energy $E$ as a function of the molecular configuration $\\mathbf{x}$, such that $E(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))$.\n\nThe problem specifies a zero mean function, $m(\\mathbf{x}) = 0$, and a composite covariance function (kernel) $k(\\mathbf{x}, \\mathbf{x}')$. The observations, denoted by $y$, are assumed to be noisy measurements of the true energy function $E(\\mathbf{x})$, such that $y = E(\\mathbf{x}) + \\epsilon$, where the noise $\\epsilon$ is drawn from an independent, identically distributed Gaussian distribution with zero mean and variance $\\sigma_n^2$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nLet the training data be a set of $N$ configurations $X = \\{\\mathbf{x}_j\\}_{j=1}^{N}$ and their corresponding observed energies $\\mathbf{y} = \\{y_j\\}_{j=1}^{N}$. Let the test data be a set of $M$ configurations $X_* = \\{\\mathbf{x}_i^\\star\\}_{i=1}^{M}$ for which we wish to predict the energies $\\mathbf{E}_* = \\{E(\\mathbf{x}_i^\\star)\\}_{i=1}^M$.\n\nUnder the GP model, the joint distribution of the observed training outputs $\\mathbf{y}$ and the latent function values at the test points $\\mathbf{E}_*$ is a multivariate Gaussian:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{E}_* \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix},\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I & K(X, X_*) \\\\\nK(X_*, X) & K(X_*, X_*)\n\\end{pmatrix}\n\\right)\n$$\nHere, $K(X, X)$ is the $N \\times N$ matrix of covariances evaluated at all pairs of training points, with entries $(K(X, X))_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. Similarly, $K(X_*, X)$ is the $M \\times N$ matrix with entries $(K(X_*, X))_{ij} = k(\\mathbf{x}_i^\\star, \\mathbf{x}_j)$, and $K(X_*, X_*) $ is the $M \\times M$ matrix of covariances at test points. $I$ is the $N \\times N$ identity matrix.\n\nThe posterior predictive distribution $p(\\mathbf{E}_* | X, \\mathbf{y}, X_*)$ is also Gaussian. Its mean, which is the desired quantity, is given by the standard formula for conditional Gaussian distributions:\n$$\n\\bar{\\mathbf{E}}_* = \\mathbb{E}[\\mathbf{E}_* | X, \\mathbf{y}, X_*] = K(X_*, X) \\left[K(X, X) + \\sigma_n^2 I\\right]^{-1} \\mathbf{y}\n$$\nThe solution involves three main steps:\n\n1.  **Generate Training Data:** The training outputs $\\mathbf{y}$ are not given directly but must be computed using the reference energy function $E_{\\mathrm{ref}}(r_1, r_2, \\phi)$ for each training input $\\mathbf{x}_j$.\n    The function is:\n    $$\n    E_{\\mathrm{ref}}(r_1,r_2,\\phi) = \\sum_{i=1}^{2}\\left[D\\left(1 - e^{-a(r_i - r_0)}\\right)^2 - D\\right] + V_0\\left(1 - \\cos(n\\phi)\\right)\n    $$\n    with parameters $D = 4.5\\ \\text{eV}$, $a = 1.7\\ \\text{\\AA}^{-1}$, $r_0 = 1.09\\ \\text{\\AA}$, $V_0 = 0.18\\ \\text{eV}$, and $n=3$. We compute $y_j = E_{\\mathrm{ref}}(\\mathbf{x}_j)$ for the $N=5$ given training configurations.\n\n2.  **Construct Kernel Matrices:** We must construct the covariance matrices $K(X, X)$ and $K(X_*, X)$. The kernel is a sum of a squared-exponential term for the two bond lengths $(r_1, r_2)$ and a periodic term for the dihedral angle $\\phi$:\n    $$\n    k(\\mathbf{x}, \\mathbf{x}') = \\sigma_s^2 \\exp\\left(-\\frac{1}{2\\ell_s^2}\\sum_{i=1}^{2}(r_i - r_i')^2\\right) + \\sigma_p^2 \\exp\\left(-\\frac{2\\sin^2\\left(\\tfrac{\\phi - \\phi'}{2}\\right)}{\\ell_p^2}\\right)\n    $$\n    The hyperparameters are given as $\\sigma_s = 2.5\\ \\text{eV}$, $\\ell_s = 0.2\\ \\text{\\AA}$, $\\sigma_p = 0.4\\ \\text{eV}$, and $\\ell_p = 0.5$. The noise level is $\\sigma_n = 0.02\\ \\text{eV}$, so the noise variance is $\\sigma_n^2 = 0.0004\\ \\text{eV}^2$. We compute the $5 \\times 5$ matrix $K(X, X)$ and the $4 \\times 5$ matrix $K(X_*, X)$.\n\n3.  **Compute the Posterior Mean:** The final calculation for the predictive mean $\\bar{\\mathbf{E}}_*$ is performed. For numerical stability, we do not explicitly invert the matrix $\\left[K(X, X) + \\sigma_n^2 I\\right]$. Instead, we first solve the linear system of equations:\n    $$\n    \\left[K(X, X) + \\sigma_n^2 I\\right] \\boldsymbol{\\alpha} = \\mathbf{y}\n    $$\n    for the weight vector $\\boldsymbol{\\alpha}$. The posterior mean is then obtained by a matrix-vector product:\n    $$\n    \\bar{\\mathbf{E}}_* = K(X_*, X) \\boldsymbol{\\alpha}\n    $$\nThis procedure is implemented exactly as described to find the GPR-predicted energies for the $M=4$ test configurations. All calculations are performed using floating-point arithmetic with angles in radians.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian process regression problem for a potential energy surface.\n    \"\"\"\n\n    # --- Define Constants and Hyperparameters ---\n    \n    # Reference energy function parameters\n    D = 4.5  # eV\n    a = 1.7  # 1/Angstrom\n    r0 = 1.09 # Angstrom\n    V0 = 0.18 # eV\n    n = 3    # dimensionless\n\n    # GPR kernel hyperparameters\n    sigma_s = 2.5  # eV\n    ell_s = 0.2    # Angstrom\n    sigma_p = 0.4  # eV\n    ell_p = 0.5    # dimensionless (for radians)\n    \n    # Observation noise\n    sigma_n = 0.02 # eV\n\n    # --- Define Training and Test Data ---\n\n    # Training configurations (N=5)\n    train_x = np.array([\n        [r0, r0, 0.0],\n        [r0 + 0.1, r0, 0.5],\n        [r0, r0 + 0.15, 1.0],\n        [r0 + 0.2, r0 + 0.2, 2.0],\n        [r0 - 0.1, r0, 3.0]\n    ])\n\n    # Test configurations (M=4)\n    test_x = np.array([\n        [r0, r0, 0.0],\n        [r0, r0, np.pi],\n        [r0 + 0.3, r0, -np.pi],\n        [r0 + 0.05, r0 + 0.05, 1.0]\n    ])\n\n    # --- Step 1: Generate Training Outputs (y) ---\n\n    def E_ref(r1, r2, phi):\n        \"\"\"Computes the reference energy.\"\"\"\n        morse_term = D * (1 - np.exp(-a * (r1 - r0)))**2 - D + \\\n                     D * (1 - np.exp(-a * (r2 - r0)))**2 - D\n        torsional_term = V0 * (1 - np.cos(n * phi))\n        return morse_term + torsional_term\n\n    train_y = np.array([E_ref(x[0], x[1], x[2]) for x in train_x])\n\n    # --- Step 2: Construct Kernel Matrices ---\n\n    def kernel(x1, x2):\n        \"\"\"Computes the composite kernel value k(x1, x2).\"\"\"\n        r1_1, r2_1, phi_1 = x1\n        r1_2, r2_2, phi_2 = x2\n\n        # Squared-exponential kernel for bond stretches\n        sq_dist_r = (r1_1 - r1_2)**2 + (r2_1 - r2_2)**2\n        k_se = (sigma_s**2) * np.exp(-0.5 * sq_dist_r / (ell_s**2))\n\n        # Periodic kernel for dihedral angle\n        d_phi = phi_1 - phi_2\n        k_per = (sigma_p**2) * np.exp(-2.0 * (np.sin(d_phi / 2.0)**2) / (ell_p**2))\n        \n        return k_se + k_per\n    \n    N = len(train_x)\n    M = len(test_x)\n\n    # K(X, X): training data covariance matrix (N x N)\n    K_XX = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            K_XX[i, j] = kernel(train_x[i], train_x[j])\n\n    # K(X_*, X): test-training data covariance matrix (M x N)\n    K_starX = np.zeros((M, N))\n    for i in range(M):\n        for j in range(N):\n            K_starX[i, j] = kernel(test_x[i], train_x[j])\n\n    # --- Step 3: Compute the Posterior Mean ---\n    \n    # Add noise variance to the diagonal of K(X, X)\n    K_y = K_XX + (sigma_n**2) * np.eye(N)\n    \n    # Solve the linear system for alpha: K_y * alpha = y\n    try:\n        alpha = np.linalg.solve(K_y, train_y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse if solve fails, though not expected here\n        alpha = np.linalg.pinv(K_y) @ train_y\n\n    # Compute posterior predictive mean: E_star = K(X_*, X) * alpha\n    E_star_mean = K_starX @ alpha\n    \n    # --- Final Output Formatting ---\n    \n    results = [f\"{val:.6f}\" for val in E_star_mean]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2456000"}]}