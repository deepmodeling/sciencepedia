{"hands_on_practices": [{"introduction": "Before a machine learning model can predict the energy of a system, it must first be given a numerical representation of the atomic structure. A crucial requirement for this representation is that it must be invariant to physical symmetries like translation, rotation, and the permutation of identical atoms. This exercise guides you through the implementation of a foundational tool for this purpose: a two-body Behler-Parrinello-style symmetry function [@problem_id:2457438]. By building this descriptor from first principles, you will gain a concrete understanding of how these essential physical constraints are encoded into the inputs of a modern machine learning potential.", "problem": "Implement a program that derives and computes a basic two-body Behler–Parrinello-style symmetry function for Argon (Ar) atoms and evaluates it on a small test suite of geometries. The purpose is to connect invariance requirements of machine-learned interatomic potentials to a concrete descriptor and to demonstrate numerical behavior under different parameter choices. The overall context is that the total potential energy surface of a system can be approximated as a sum of atomic contributions, each depending on a localized, symmetry-invariant representation of its neighborhood, as in High-Dimensional Neural Network Potentials (HDNNP). Your task is to derive, from invariance principles, a two-body radial symmetry function and implement it.\n\nStart from the following fundamental base:\n- Translational and rotational invariance of a scalar potential energy imply that a local descriptor for an atom must be constructed from internal coordinates such as interatomic distances.\n- For finite-range interactions and locality in the learned mapping, impose a smooth cutoff at a finite radius so that distant atoms beyond a cutoff do not contribute and forces remain well-behaved.\n- To resolve the radial distribution around an atom, use a radial basis with tunable width and center parameters so that the representation can distinguish environments at different length scales.\n\nFrom these principles, derive and then implement a two-body radial symmetry function $G^2$ for a chosen central atom $i$ of the form\n- a sum over neighbor atoms $j \\neq i$,\n- a smooth, finite-range cutoff function that is $C^1$-continuous and equals zero at the cutoff radius,\n- and a localized radial weight that can shift and sharpen around a chosen distance.\n\nConcretely specify and use the following forms in your derivation and implementation:\n- Use the cosine cutoff\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\nwith the cosine argument in radians.\n- Use a Gaussian-like radial basis\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\nwith width parameter $\\eta$ and shift $R_s$.\n- Combine them in the two-body symmetry function\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\nwhere $r_{ij}$ is the Euclidean distance between atoms $i$ and $j$.\n\nAll atoms are Argon (Ar), treated as a single chemical species, so no species-dependent weighting is required. Distances $r_{ij}$, cutoff $R_c$, and shift $R_s$ must be expressed in Ångström, and $\\eta$ in $\\text{Å}^{-2}$. The cosine function must take its argument in radians.\n\nProgram requirements:\n- Implement a function that, given a set of Cartesian coordinates (in Ångström), an index $i$ for the central atom, and parameters $(\\eta, R_s, R_c)$, computes $G_i^{2}(\\eta, R_s, R_c)$ using the formulas above.\n- Use standard three-dimensional Euclidean distance. Do not apply periodic boundary conditions.\n- Numerical stability: Exclude self-interaction ($j = i$). Distances $r_{ij}$ are strictly nonnegative; do not special-case $r_{ij} = 0$ beyond excluding self-interaction.\n\nTest suite:\nEvaluate $G_i^{2}$ for each of the following five cases. Each case specifies $(\\text{positions}, i, R_c, \\eta, R_s)$, with all distances in Ångström and $\\eta$ in $\\text{Å}^{-2}$:\n- Case A:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- Case B:\n  - positions: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case C:\n  - positions: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case D:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- Case E:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\nOutput specification:\n- For each case, compute a single floating-point value $G_i^{2}$.\n- Round each result to exactly $6$ decimal places using standard rounding.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order A, B, C, D, E. For example, an output with generic placeholders should look like \"[0.123456,0.000000,0.000000,1.234567,0.654321]\".", "solution": "The problem presented is valid, scientifically sound, and well-posed. It requires the derivation and implementation of a two-body radial symmetry function, a fundamental component of modern machine-learned interatomic potentials such as High-Dimensional Neural Network Potentials (HDNNPs). We shall first deduce the form of this function from first principles, and then detail the algorithm for its computation.\n\nThe potential energy $E$ of a system of atoms is a scalar quantity. For it to be physically meaningful, it must be invariant under translation and rotation of the entire system, as well as under the permutation of identical atoms. In the HDNNP scheme, the total energy is decomposed into atomic contributions $E_i$, where $E = \\sum_i E_i$. Each atomic energy $E_i$ is a function of the local environment of atom $i$, characterized by a set of descriptors or \"symmetry functions,\" $\\{G_i\\}$. Therefore, these symmetry functions must themselves be invariant to the aforementioned transformations.\n\n1.  **Translational and Rotational Invariance**: These symmetries dictate that the descriptors for atom $i$ must depend only on the internal coordinates of its local environment, not on the absolute Cartesian coordinates of the atoms in a global frame. The simplest set of internal coordinates consists of the scalar distances $r_{ij}$ between the central atom $i$ and its neighbors $j$. Any function of these distances, $G_i = F(\\{r_{ij}\\}_{j \\neq i})$, is automatically invariant to rigid translation and rotation of the atomic assembly.\n\n2.  **Permutational Invariance**: The energy contribution of atom $i$ must not depend on the arbitrary labeling of its identical neighbors. If atoms $j$ and $k$ are of the same species, swapping them must not change the value of the descriptor. The simplest mathematical construct that satisfies this is a sum over all neighbors. Thus, we propose a descriptor of the form $G_i = \\sum_{j \\neq i} g(r_{ij})$, where $g$ is some function of the interatomic distance. This form is the basis of the two-body symmetry function.\n\n3.  **Locality and Smoothness**: Physical interactions are local in nature; the influence of very distant atoms is negligible. To model this, we introduce a smooth cutoff function, $f_c(r_{ij}; R_c)$, which multiplies the contribution of each neighbor. This function must be equal to $1$ for small distances, and smoothly go to $0$ as the distance $r_{ij}$ approaches a cutoff radius $R_c$. For distances $r_{ij} > R_c$, the contribution is exactly zero. The requirement for smoothness, specifically $C^1$-continuity (continuous first derivative), is critical. The forces on atoms are calculated as the negative gradient of the potential energy, $\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$. Discontinuities in the first derivative of the energy would lead to unphysical, infinite forces. The provided cosine cutoff function is:\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    At the cutoff radius $r = R_c$, the function value is $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$, ensuring continuity. Its derivative is $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$. At $r = R_c$, the derivative is $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$, which matches the derivative of the zero function for $r > R_c$. Thus, the function is $C^1$-continuous as required.\n\n4.  **Radial Resolution**: A simple sum of cutoff functions would only provide a weighted count of neighbors within the cutoff sphere. To create a descriptor that can distinguish different radial structures, we introduce a radial basis function. The specified Gaussian form, $\\exp[-\\eta(r_{ij} - R_s)^2]$, serves this purpose. This function is centered at a distance $R_s$ and has a characteristic width controlled by the parameter $\\eta$. A larger $\\eta$ corresponds to a narrower, more sharply peaked Gaussian. By using a set of these functions with different parameters $(\\eta, R_s)$, one can resolve the radial distribution of neighbors around the central atom $i$.\n\nCombining these four principles—invariance from using distances, permutation symmetry from summation, locality from a smooth cutoff, and resolution from a radial basis—we arrive at the specified two-body radial symmetry function, designated as $G_i^2$:\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\nThe sum is over all atoms $j$ in the system, excluding the central atom $i$. For each neighbor $j$, we calculate its contribution only if its distance $r_{ij}$ from atom $i$ is less than or equal to the cutoff radius $R_c$.\n\nThe computational procedure is as follows:\nGiven a set of Cartesian coordinates for $N$ atoms, $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$, a central atom index $i$, and parameters $\\eta$, $R_s$, and $R_c$:\n1.  Initialize the symmetry function value, $G_i^2$, to $0$.\n2.  Identify the coordinate vector of the central atom, $\\mathbf{r}_i$.\n3.  Iterate through all other atoms $j$ where $j \\in \\{0, 1, ..., N-1\\}$ and $j \\neq i$.\n4.  For each neighbor $j$, compute the Euclidean distance $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$.\n5.  Check if $r_{ij} \\le R_c$. If not, the contribution from atom $j$ is $0$, and we proceed to the next neighbor.\n6.  If $r_{ij} \\le R_c$, calculate the two components of the term:\n    -   The radial basis term: $T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$.\n    -   The cutoff function term: $T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$.\n7.  Add the product of these terms, $T_{\\text{rad}} \\times T_{\\text{cut}}$, to the running sum for $G_i^2$.\n8.  After iterating through all neighbors $j$, the final sum is the value of the symmetry function for atom $i$.\n\nThis procedure will now be implemented and applied to the five specified test cases. All units must be consistent; distances ($r_{ij}$, $R_s$, $R_c$) are in Ångströms ($\\text{Å}$), and the parameter $\\eta$ is in $\\text{Å}^{-2}$, ensuring the argument of the exponential is dimensionless.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] <= 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij <= R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2457438"}, {"introduction": "Once we have a suitable representation of our atomic environments, the next step is to train a model to map these descriptors to energies. The quality of this mapping depends not only on the training data but also on the model's architecture and hyperparameters. In this practice, we investigate the behavior of a kernel ridge regression model by deliberately introducing a 'poisoned' data point with an incorrect energy [@problem_id:2457470]. By analyzing how the model's prediction changes with different kernel length-scales and regularization strengths, you will develop an intuition for how these hyperparameters control the balance between local flexibility and global smoothness, a critical skill for building robust potentials.", "problem": "You will implement and analyze a one-dimensional Machine Learning Potential trained by kernel ridge regression on a diatomic molecule’s bond-stretch coordinate, with and without a deliberately poisoned training point. The task focuses on quantifying how a single egregiously wrong energy label perturbs the learned potential locally (near the poisoned configuration) versus globally (across the entire domain). Your program must be a complete, runnable program that computes specified metrics for a fixed test suite and outputs the results in the required format.\n\nContext and fundamental base:\n- A diatomic potential energy curve is a function of the internuclear distance $r$ in angstroms. A scientifically plausible model is the Morse potential,\n$$\nE_{\\mathrm{true}}(r) = D_e\\left(1 - e^{-a(r-r_e)}\\right)^2,\n$$\nwith depth $D_e$ in electronvolts, stiffness parameter $a$ in inverse angstroms, and equilibrium distance $r_e$ in angstroms. This potential is a smooth, well-tested model of bond stretching and is used here as the ground truth.\n- In supervised regression, kernel ridge regression (KRR) arises from minimizing the squared empirical prediction error plus a Tikhonov regularization term. Given inputs $x_i$ and outputs $y_i$, the hypothesis space induced by a positive-definite kernel $k(\\cdot,\\cdot)$ yields a solution that is a linear combination of kernel sections, and the prediction at a point $x$ has the form $f(x) = \\sum_{i=1}^N \\alpha_i k(x, x_i)$, where the coefficients $\\alpha_i$ are determined by minimizing the regularized empirical risk.\n- The Gaussian kernel with length scale $\\ell$,\n$$\nk(x,z) = \\exp\\left(-\\frac{(x - z)^2}{2\\ell^2}\\right),\n$$\nis a standard, smooth, radial basis function. Its locality is governed by $\\ell$: small $\\ell$ concentrates influence locally; large $\\ell$ spreads influence more globally.\n- The regularization parameter $\\lambda > 0$ controls the trade-off between data fit and smoothness, dampening the influence of any single data point.\n\nTraining and evaluation setup:\n- Use the Morse potential with parameters $D_e = 0.2\\ \\text{eV}$, $a = 2.5\\ \\text{Å}^{-1}$, and $r_e = 0.74\\ \\text{Å}$.\n- Construct a training set of $N=13$ bond lengths $r_i$ uniformly spaced over $[0.6, 1.2]\\ \\text{Å}$, inclusive.\n- Define the clean training labels $y_i^{\\mathrm{clean}} = E_{\\mathrm{true}}(r_i)$ in electronvolts.\n- Define a poisoned training set by selecting one training coordinate $r_p$ from the training grid and replacing its label by $y_p^{\\mathrm{poison}} = y_p^{\\mathrm{clean}} + \\Delta$, with $\\Delta = 1.0$ in electronvolts, while all other labels remain as in the clean set. This single corrupted label models a data poisoning event.\n- Train two KRR models (one on the clean labels and one on the poisoned labels) using the same kernel and the same regularization.\n- Evaluate on a uniform test grid of $M=1001$ points in $[0.5, 1.5]\\ \\text{Å}$, inclusive.\n\nFor each test case below, compute the following three metrics:\n1. Local impact at the poisoned configuration:\n$$\nI_{\\mathrm{local}} = \\left| f_{\\mathrm{poison}}(r_p) - f_{\\mathrm{clean}}(r_p) \\right| \\quad \\text{in electronvolts}.\n$$\n2. Global mean absolute deviation over the test domain:\n$$\nI_{\\mathrm{global}} = \\frac{1}{M}\\sum_{j=1}^{M} \\left| f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) \\right| \\quad \\text{in electronvolts}.\n$$\n3. Relative degradation of accuracy (unitless) measured by the ratio of root-mean-square errors (RMSEs) against the ground truth over the test grid:\n$$\nR_{\\mathrm{RMSE}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - E_{\\mathrm{true}}(r_j^{\\mathrm{test}})\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) - E_{\\mathrm{true}}(r_j^{\\mathrm{test}})\\right)^2}}.\n$$\n\nKernel and regularization specification:\n- Use the Gaussian kernel with length scale $\\ell$ specified per test case.\n- Use Tikhonov regularization parameter $\\lambda$ specified per test case.\n\nTest suite:\nCompute the triplet $\\left(I_{\\mathrm{local}}, I_{\\mathrm{global}}, R_{\\mathrm{RMSE}}\\right)$ for each of the following four cases. In all cases, the poison magnitude is $\\Delta = 1.0\\ \\mathrm{eV}$, and the training point $r_p$ is chosen from the training grid.\n- Case $1$ (happy path, moderate locality): $\\ell = 0.05\\ \\text{Å}$, $\\lambda = 1\\times 10^{-6}$, $r_p = 0.90\\ \\text{Å}$.\n- Case $2$ (more global influence): $\\ell = 0.20\\ \\text{Å}$, $\\lambda = 1\\times 10^{-6}$, $r_p = 0.90\\ \\text{Å}$.\n- Case $3$ (stronger regularization): $\\ell = 0.05\\ \\text{Å}$, $\\lambda = 1\\times 10^{-2}$, $r_p = 0.90\\ \\text{Å}$.\n- Case $4$ (boundary poison): $\\ell = 0.05\\ \\text{Å}$, $\\lambda = 1\\times 10^{-6}$, $r_p = 0.60\\ \\text{Å}$.\n\nImplementation requirements:\n- Implement kernel ridge regression in one dimension by solving the normal equations implied by minimizing the regularized empirical risk over the reproducing kernel Hilbert space induced by the Gaussian kernel.\n- Use only the specified parameters and grids given above. All energies must be in electronvolts, all lengths in angstroms, and angles are not used. Report $I_{\\mathrm{local}}$ and $I_{\\mathrm{global}}$ in electronvolts, and $R_{\\mathrm{RMSE}}$ as a unitless ratio.\n- Round each reported float to $6$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, output a list of four lists, one per test case, each inner list containing the three floats $\\left(I_{\\mathrm{local}}, I_{\\mathrm{global}}, R_{\\mathrm{RMSE}}\\right)$ rounded to $6$ decimals. For example, the printed output must have the form\n$$\n\\left[\\left[i_{1,1},i_{1,2},i_{1,3}\\right],\\left[i_{2,1},i_{2,2},i_{2,3}\\right],\\left[i_{3,1},i_{3,2},i_{3,3}\\right],\\left[i_{4,1},i_{4,2},i_{4,3}\\right]\\right],\n$$\nwhere all $i_{k,m}$ are decimal numbers rounded to $6$ places. The output must be exactly one line and contain no additional text.", "solution": "The problem as stated is valid. It is scientifically grounded in the fields of computational chemistry and machine learning, well-posed with a clear and deterministic procedure, and is formulated with objective, unambiguous language. All necessary parameters and definitions are provided. I will therefore proceed with a full solution.\n\nThe core of the problem is to quantify the impact of a single erroneous data point—a form of data poisoning—on a kernel ridge regression (KRR) model. The model is trained to learn the potential energy surface of a diatomic molecule, for which the ground truth is given by the Morse potential:\n$$E_{\\mathrm{true}}(r) = D_e\\left(1 - e^{-a(r-r_e)}\\right)^2$$\nThe parameters are specified as $D_e = 0.2\\ \\text{eV}$, $a = 2.5\\ \\text{Å}^{-1}$, and $r_e = 0.74\\ \\text{Å}$.\n\nThe KRR model predicts the energy at a given internuclear distance $r$ as a linear combination of kernel functions centered at the training data points $\\{r_i\\}_{i=1}^N$:\n$$f(r) = \\sum_{i=1}^{N} \\alpha_i k(r, r_i)$$\nThe kernel is the Gaussian kernel with a characteristic length scale $\\ell$:\n$$k(r, r') = \\exp\\left(-\\frac{(r - r')^2}{2\\ell^2}\\right)$$\nThe coefficients $\\alpha = (\\alpha_1, \\dots, \\alpha_N)^T$ are determined by minimizing a regularized least-squares cost function. This leads to the following system of linear equations:\n$$(\\mathbf{K} + \\lambda \\mathbf{I}) \\alpha = \\mathbf{y}$$\nHere, $\\mathbf{K}$ is the $N \\times N$ kernel matrix with entries $K_{ij} = k(r_i, r_j)$, $\\mathbf{y}$ is the vector of training energy labels, $\\lambda > 0$ is the Tikhonov regularization parameter, and $\\mathbf{I}$ is the $N \\times N$ identity matrix. This linear system has a unique solution for $\\alpha$, which is then used for prediction.\n\nThe procedure is as follows:\n1.  Construct the training grid of $N=13$ points, $\\mathbf{r}_{\\mathrm{train}}$, uniformly spaced in $[0.6, 1.2]\\ \\text{Å}$.\n2.  Construct the test grid of $M=1001$ points, $\\mathbf{r}_{\\mathrm{test}}$, uniformly spaced in $[0.5, 1.5]\\ \\text{Å}$.\n3.  Generate the \"clean\" training labels, $\\mathbf{y}_{\\mathrm{clean}}$, by evaluating $E_{\\mathrm{true}}(r)$ at each point in $\\mathbf{r}_{\\mathrm{train}}$.\n4.  For each test case, specified by parameters $(\\ell, \\lambda, r_p)$:\n    a.  Create the \"poisoned\" training labels, $\\mathbf{y}_{\\mathrm{poison}}$, by taking $\\mathbf{y}_{\\mathrm{clean}}$ and adding a perturbation $\\Delta = 1.0\\ \\mathrm{eV}$ to the label corresponding to the coordinate $r_p$.\n    b.  Solve for the coefficient vector $\\alpha_{\\mathrm{clean}}$ using $(\\mathbf{r}_{\\mathrm{train}}, \\mathbf{y}_{\\mathrm{clean}})$.\n    c.  Solve for the coefficient vector $\\alpha_{\\mathrm{poison}}$ using $(\\mathbf{r}_{\\mathrm{train}}, \\mathbf{y}_{\\mathrm{poison}})$.\n    d.  Use these coefficients to generate two potential energy functions: $f_{\\mathrm{clean}}(r)$ and $f_{\\mathrm{poison}}(r)$.\n    e.  Evaluate both functions on the test grid $\\mathbf{r}_{\\mathrm{test}}$ and at the specific point $r_p$.\n    f.  Compute the three required metrics:\n        i.  The local impact, $I_{\\mathrm{local}} = | f_{\\mathrm{poison}}(r_p) - f_{\\mathrm{clean}}(r_p) |$.\n        ii. The global mean absolute deviation, $I_{\\mathrm{global}} = \\frac{1}{M}\\sum_{j=1}^{M} | f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) |$.\n        iii. The relative RMSE degradation, $R_{\\mathrm{RMSE}}$, defined as the ratio of the root-mean-square error of $f_{\\mathrm{poison}}$ to the RMSE of $f_{\\mathrm{clean}}$, both with respect to the ground truth $E_{\\mathrm{true}}$ over the test grid.\n\nThis entire computational experiment will be conducted for each of the four specified test cases. The results will be compiled and presented in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the impact of a poisoned data point on a Kernel Ridge Regression\n    model of a diatomic molecule's potential energy curve.\n    \"\"\"\n    # Define constants and problem setup from the statement.\n    DE = 0.2     # eV\n    A = 2.5      # 1/Angstrom\n    RE = 0.74    # Angstrom\n    DELTA = 1.0  # eV\n\n    N_TRAIN = 13\n    N_TEST = 1001\n    \n    # Define training and testing grids.\n    r_train = np.linspace(0.6, 1.2, N_TRAIN)\n    r_test = np.linspace(0.5, 1.5, N_TEST)\n\n    def morse_potential(r, De, a, re):\n        \"\"\"Calculates the Morse potential energy.\"\"\"\n        return De * (1 - np.exp(-a * (r - re)))**2\n\n    def gaussian_kernel(x, z, ell):\n        \"\"\"\n        Computes the Gaussian kernel matrix between two sets of points.\n        x and z are 1D arrays.\n        \"\"\"\n        dist_sq = np.subtract.outer(x, z)**2\n        return np.exp(-dist_sq / (2 * ell**2))\n\n    def train_krr(r_train, y_train, ell, lam):\n        \"\"\"Trains a KRR model and returns the coefficients alpha.\"\"\"\n        K = gaussian_kernel(r_train, r_train, ell)\n        # Solve the linear system (K + lambda*I) * alpha = y\n        A = K + lam * np.identity(K.shape[0])\n        alpha = np.linalg.solve(A, y_train)\n        return alpha\n\n    def predict_krr(r_predict, r_train, alpha, ell):\n        \"\"\"Makes predictions using a trained KRR model.\"\"\"\n        K_pred = gaussian_kernel(r_predict, r_train, ell)\n        return K_pred @ alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (ell, lambda, r_p)\n        (0.05, 1e-6, 0.90), # Case 1: Moderate locality\n        (0.20, 1e-6, 0.90), # Case 2: More global influence\n        (0.05, 1e-2, 0.90), # Case 3: Stronger regularization\n        (0.05, 1e-6, 0.60), # Case 4: Boundary poison\n    ]\n\n    # Generate ground truth data.\n    y_true_train = morse_potential(r_train, DE, A, RE)\n    y_true_test = morse_potential(r_test, DE, A, RE)\n    \n    all_results = []\n    for ell, lam, r_p in test_cases:\n        # Find the index of the poisoned point in the training grid.\n        p_idx = np.argmin(np.abs(r_train - r_p))\n        \n        # Create clean and poisoned training labels.\n        y_clean = np.copy(y_true_train)\n        y_poison = np.copy(y_true_train)\n        y_poison[p_idx] += DELTA\n\n        # Train both clean and poisoned models.\n        alpha_clean = train_krr(r_train, y_clean, ell, lam)\n        alpha_poison = train_krr(r_train, y_poison, ell, lam)\n\n        # Make predictions on the test grid.\n        f_clean_test = predict_krr(r_test, r_train, alpha_clean, ell)\n        f_poison_test = predict_krr(r_test, r_train, alpha_poison, ell)\n\n        # Make predictions at the poisoned point r_p for I_local.\n        f_clean_rp = predict_krr(np.array([r_p]), r_train, alpha_clean, ell)[0]\n        f_poison_rp = predict_krr(np.array([r_p]), r_train, alpha_poison, ell)[0]\n\n        # Metric 1: Local impact I_local\n        i_local = np.abs(f_poison_rp - f_clean_rp)\n\n        # Metric 2: Global mean absolute deviation I_global\n        i_global = np.mean(np.abs(f_poison_test - f_clean_test))\n\n        # Metric 3: Relative degradation of accuracy R_RMSE\n        rmse_clean = np.sqrt(np.mean((f_clean_test - y_true_test)**2))\n        rmse_poison = np.sqrt(np.mean((f_poison_test - y_true_test)**2))\n        \n        if np.isclose(rmse_clean, 0.0):\n            r_rmse = np.inf\n        else:\n            r_rmse = rmse_poison / rmse_clean\n            \n        all_results.append([i_local, i_global, r_rmse])\n\n    # Final print statement in the exact required format.\n    inner_lists_str = []\n    for res in all_results:\n        formatted_res = [f\"{x:.6f}\" for x in res]\n        inner_lists_str.append(f\"[{','.join(formatted_res)}]\")\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "2457470"}, {"introduction": "The ultimate goal of a machine learning potential is often to run large-scale, long-time molecular dynamics simulations. A powerful modern paradigm is to build the potential 'on-the-fly' during the simulation itself, a process known as active learning. This hands-on exercise simulates this entire workflow in a simplified one-dimensional system [@problem_id:2457458]. You will implement a loop where a molecular dynamics trajectory is propagated using the current potential, the model's uncertainty is monitored, and new training data is automatically requested when the model enters an unknown region, allowing the potential to learn and improve as it explores.", "problem": "Implement a complete, runnable program that simulates a one-dimensional, reduced-unit molecular dynamics system for a diatomic molecule while performing an \"on-the-fly\" active learning loop for a Machine Learning Potential (MLP). The MLP is trained to approximate a reference quantum-mechanical (QM) potential and its force. The simulation must pause when an uncertainty metric exceeds a threshold, trigger a QM query to obtain exact energy and force, update the training set, retrain the MLP, and then resume the dynamics.\n\nThe system is a single internal coordinate bond length $r(t)$ with reduced mass $\\mu = 1$ in reduced units. The reference quantum-mechanical potential energy is the Morse potential\n$$\nV_{\\mathrm{QM}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2,\n$$\nwith constants $D_e = 0.20$, $a = 1.5$, and $r_e = 1.0$ in reduced units. The corresponding force is\n$$\nF_{\\mathrm{QM}}(r) = - \\frac{d V_{\\mathrm{QM}}}{dr}.\n$$\n\nThe MLP must be a linear model in a polynomial basis of degree $4$. Define the basis\n$$\n\\phi(r) = \\begin{bmatrix} 1 \\\\ r \\\\ r^2 \\\\ r^3 \\\\ r^4 \\end{bmatrix}, \\quad \\phi'(r) = \\frac{d\\phi}{dr} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 r \\\\ 3 r^2 \\\\ 4 r^3 \\end{bmatrix}.\n$$\nLet the model parameter vector be $w \\in \\mathbb{R}^5$. The predicted energy and force are\n$$\n\\hat{E}(r) = w^\\top \\phi(r), \\quad \\hat{F}(r) = - w^\\top \\phi'(r).\n$$\nTraining data consist of triples $(r_i, E_i, F_i)$, where $E_i = V_{\\mathrm{QM}}(r_i)$ and $F_i = F_{\\mathrm{QM}}(r_i)$. Fit $w$ by linear least squares to minimize the sum of squared residuals of energies and forces with equal weight and with Tikhonov (ridge) regularization of strength $\\lambda = 10^{-8}$. That is, form a linear system with rows $\\phi(r_i)$ targeting $E_i$ and rows $-\\phi'(r_i)$ targeting $F_i$, solve\n$$\n\\min_w \\left\\|A w - y\\right\\|_2^2 + \\lambda \\left\\|w\\right\\|_2^2.\n$$\n\nDefine a committee of two models for uncertainty estimation. Let $w^{(A)}$ be trained on the even-indexed members of the current training set (using zero-based indexing), and $w^{(B)}$ be trained on the odd-indexed members. The uncertainty metric at coordinate $r$ is\n$$\nU(r) = \\left| \\hat{F}^{(A)}(r) - \\hat{F}^{(B)}(r) \\right|.\n$$\nIn addition, define a \"production\" model $w^{(P)}$ trained on the full current training set; this is the model used to generate forces during molecular dynamics integration.\n\nInitialize the training set with two QM points at $r = r_e$ and $r = r_e + 0.20$. Then perform molecular dynamics for $N$ steps with time step $\\Delta t$ using the velocity-Verlet scheme for position $r$ and velocity $v$ under mass $\\mu = 1$:\n$$\nv_{n+\\frac{1}{2}} = v_n + \\frac{\\Delta t}{2} \\, a(r_n), \\quad r_{n+1} = r_n + \\Delta t \\, v_{n+\\frac{1}{2}}, \\quad v_{n+1} = v_{n+\\frac{1}{2}} + \\frac{\\Delta t}{2} \\, a(r_{n+1}),\n$$\nwhere $a(r) = \\hat{F}^{(P)}(r) / \\mu$ is the acceleration computed from the current production model. At each step, before computing the force for integration, evaluate $U(r_n)$. If $U(r_n) > \\tau$ and the number of QM queries so far is strictly less than the budget $B$, then perform a QM query at $r_n$ by adding $(r_n, V_{\\mathrm{QM}}(r_n), F_{\\mathrm{QM}}(r_n))$ to the training set, retrain the committee and production models, and increment the QM query counter. Then proceed with the integration using the updated production model.\n\nAfter $N$ steps, evaluate the mean absolute force error along the visited trajectory positions $\\{r_1, r_2, \\dots, r_N\\}$ using the final production model:\n$$\n\\mathrm{MAE} = \\frac{1}{N} \\sum_{k=1}^{N} \\left| \\hat{F}^{(P)}(r_k) - F_{\\mathrm{QM}}(r_k) \\right|.\n$$\n\nAll quantities are in reduced units. Express the mean absolute error in the reduced force unit (energy unit per length unit). The QM query count is dimensionless.\n\nYour program must implement the above specification exactly and run the following test suite. Each test case is a tuple $(N, \\Delta t, \\tau, B, r_0, v_0)$:\n\n- Test $1$: $N = 200$, $\\Delta t = 0.01$, $\\tau = 0.05$, $B = 10$, $r_0 = 1.30$, $v_0 = 0.00$.\n- Test $2$: $N = 200$, $\\Delta t = 0.01$, $\\tau = 1.00$, $B = 10$, $r_0 = 1.30$, $v_0 = 0.00$.\n- Test $3$: $N = 200$, $\\Delta t = 0.01$, $\\tau = 0.005$, $B = 3$, $r_0 = 1.30$, $v_0 = 0.00$.\n- Test $4$: $N = 100$, $\\Delta t = 0.01$, $\\tau = 0.02$, $B = 10$, $r_0 = 2.00$, $v_0 = 0.00$.\n\nFor each test case, compute two values: the total number of triggered QM queries $Q$ (excluding the two initial seed points) and the mean absolute force error $\\mathrm{MAE}$ defined above. Your program should produce a single line of output containing a flat list of these results for the four tests in order:\n$$\n\\left[Q_1, \\mathrm{MAE}_1, Q_2, \\mathrm{MAE}_2, Q_3, \\mathrm{MAE}_3, Q_4, \\mathrm{MAE}_4\\right].\n$$\nPrint exactly one line in the format shown, with values separated by commas and enclosed in square brackets. No angles or percentages are involved; no unit conversions are required beyond the reduced units stated here.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It provides a complete specification for a computational task in the domain of machine learning potentials for molecular dynamics. All required parameters, initial conditions, algorithms, and evaluation metrics are unambiguously defined. The problem is therefore deemed valid.\n\nThe solution requires the implementation of a one-dimensional molecular dynamics (MD) simulation incorporating an \"on-the-fly\" active learning procedure for a machine learning potential (MLP). The system consists of a single particle with reduced mass $\\mu = 1$ moving along a coordinate $r$. The MLP is trained to reproduce a reference quantum-mechanical (QM) potential energy surface.\n\nThe core components of the implementation are as follows:\n\n1.  **Reference QM Model**: The true potential energy is given by the Morse potential, $V_{\\mathrm{QM}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2$. The parameters are provided as $D_e = 0.20$, $a = 1.5$, and $r_e = 1.0$ in reduced units. The corresponding analytical force, which is the negative gradient of the potential, is $F_{\\mathrm{QM}}(r) = - \\frac{d V_{\\mathrm{QM}}}{dr} = -2 a D_e (1 - e^{-a (r - r_e)}) e^{-a(r-r_e)}$. These functions serve as the ground truth for training the MLP and for evaluating its accuracy.\n\n2.  **Machine Learning Potential (MLP)**: The MLP is a linear model constructed from a polynomial basis of degree $4$. The basis vector at a given position $r$ is $\\phi(r) = \\begin{bmatrix} 1 & r & r^2 & r^3 & r^4 \\end{bmatrix}^\\top$. The MLP predicts the energy as a linear combination of these basis functions: $\\hat{E}(r) = w^\\top \\phi(r)$, where $w \\in \\mathbb{R}^5$ is the vector of model weights. A key feature of such potentials is analytic differentiability, allowing for the direct calculation of forces, known as force-matching. The predicted force is $\\hat{F}(r) = - \\frac{d\\hat{E}}{dr} = - w^\\top \\phi'(r)$, where $\\phi'(r) = \\frac{d\\phi}{dr} = \\begin{bmatrix} 0 & 1 & 2r & 3r^2 & 4r^3 \\end{bmatrix}^\\top$.\n\n3.  **MLP Training**: The weight vector $w$ is optimized by minimizing a loss function that includes both energy and force errors for a set of training points $\\{(r_i, E_i, F_i)\\}$, where $E_i = V_{\\mathrm{QM}}(r_i)$ and $F_i = F_{\\mathrm{QM}}(r_i)$. The optimization problem is a linear least-squares fit with Tikhonov (ridge) regularization, formulated as:\n    $$\n    \\min_w \\left\\|A w - y\\right\\|_2^2 + \\lambda \\left\\|w\\right\\|_2^2\n    $$\n    Here, the matrix $A$ and vector $y$ are constructed by stacking the contributions from each training point. For a set of $M$ training points, $A$ is a $2M \\times 5$ matrix where the first $M$ rows are $\\phi(r_i)^\\top$ and the next $M$ rows are $-\\phi'(r_i)^\\top$. The vector $y$ is a $2M$-dimensional vector containing the target energies $E_i$ followed by the target forces $F_i$. The regularization strength is given as $\\lambda = 10^{-8}$. This is a standard ridge regression problem, and its analytical solution is $w = (A^\\top A + \\lambda I)^{-1} A^\\top y$, where $I$ is the $5 \\times 5$ identity matrix. This linear system is solved numerically.\n\n4.  **Active Learning and Uncertainty Quantification**: The simulation employs an active learning strategy to efficiently improve the MLP's accuracy by adding new training points only in regions where the model is uncertain. Uncertainty is estimated using a committee of two models, with weights $w^{(A)}$ and $w^{(B)}$. The model $w^{(A)}$ is trained on the subset of training data at even-numbered indices (using zero-based indexing), while $w^{(B)}$ is trained on the subset at odd-numbered indices. The uncertainty metric at a position $r$ is defined as the magnitude of the disagreement between the forces predicted by the two committee models: $U(r) = \\left| \\hat{F}^{(A)}(r) - \\hat{F}^{(B)}(r) \\right|$. A third \"production\" model, with weights $w^{(P)}$, is trained on the entire training set and is used to propagate the dynamics.\n\n5.  **Molecular Dynamics Simulation**: The system's trajectory is integrated using the velocity-Verlet algorithm. Given the position $r_n$ and velocity $v_n$ at time step $n$, the state at step $n+1$ is computed as:\n    $$\n    \\begin{aligned}\n    v_{n+\\frac{1}{2}} &= v_n + \\frac{\\Delta t}{2} \\frac{\\hat{F}^{(P)}(r_n)}{\\mu} \\\\\n    r_{n+1} &= r_n + \\Delta t \\, v_{n+\\frac{1}{2}} \\\\\n    v_{n+1} &= v_{n+\\frac{1}{2}} + \\frac{\\Delta t}{2} \\frac{\\hat{F}^{(P)}(r_{n+1})}{\\mu}\n    \\end{aligned}\n    $$\n    where $\\Delta t$ is the time step and the acceleration is derived from the production model's force $\\hat{F}^{(P)}$.\n\nThe complete simulation proceeds as follows:\n- The training set is initialized with two QM data points at $r=r_e=1.0$ and $r=r_e+0.20=1.2$. The three models ($w^{(A)}$, $w^{(B)}$, $w^{(P)}$) are trained on this initial set.\n- The MD simulation starting from $(r_0, v_0)$ runs for $N$ steps.\n- At the beginning of each step $n$, with the system at position $r_n$, the uncertainty $U(r_n)$ is calculated.\n- If $U(r_n)$ is greater than a threshold $\\tau$ and the number of triggered QM queries is less than the budget $B$, a new QM data point is generated at $r_n$ and added to the training set. All three models are then retrained using the updated set. The triggered query count $Q$ is incremented.\n- The velocity-Verlet algorithm then propagates the system to the next state, using the force from the (potentially updated) production model $\\hat{F}^{(P)}$. The new position $r_{n+1}$ is stored.\n- After $N$ steps, the simulation concludes. The mean absolute error (MAE) of the force is computed across the generated trajectory $\\{r_1, r_2, \\dots, r_N\\}$. This is done by comparing the forces from the *final* production model, $\\hat{F}^{(P)}(r_k)$, to the true QM forces, $F_{\\mathrm{QM}}(r_k)$, for each stored position $r_k$.\n- The total number of triggered QM queries $Q$ and the final MAE are the two outputs for each test case.\n\nThe provided Python program implements this entire procedure, iterating through the specified test cases and producing the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the active learning MD simulation for all test cases.\n    \"\"\"\n    \n    # --- Global Constants in Reduced Units ---\n    D_e = 0.20\n    a_morse = 1.5  # Renamed to avoid conflict with acceleration 'a'\n    r_e = 1.0\n    mu = 1.0\n    lambda_reg = 1e-8\n    \n    # --- Quantum Mechanical (QM) Ground Truth ---\n    def V_QM(r):\n        \"\"\"Calculates the Morse potential energy.\"\"\"\n        return D_e * (1.0 - np.exp(-a_morse * (r - r_e)))**2\n\n    def F_QM(r):\n        \"\"\"Calculates the analytical force from the Morse potential.\"\"\"\n        exp_term = np.exp(-a_morse * (r - r_e))\n        return -2.0 * a_morse * D_e * (1.0 - exp_term) * exp_term\n\n    # --- Machine Learning Potential (MLP) Module ---\n    def get_phi(r):\n        \"\"\"Returns the polynomial basis vector.\"\"\"\n        return np.array([1.0, r, r**2, r**3, r**4])\n\n    def get_phi_prime(r):\n        \"\"\"Returns the derivative of the polynomial basis vector.\"\"\"\n        return np.array([0.0, 1.0, 2.0*r, 3.0*r**2, 4.0*r**3])\n\n    def train(r_train, lam_reg):\n        \"\"\"\n        Trains the linear MLP model using ridge regression on both energy and forces.\n        \"\"\"\n        num_points = len(r_train)\n        if num_points == 0:\n            return np.zeros(5)\n\n        A = np.zeros((2 * num_points, 5))\n        y = np.zeros(2 * num_points)\n\n        for i, r_i in enumerate(r_train):\n            E_i = V_QM(r_i)\n            F_i = F_QM(r_i)\n            \n            A[i, :] = get_phi(r_i)\n            A[num_points + i, :] = -get_phi_prime(r_i)\n            \n            y[i] = E_i\n            y[num_points + i] = F_i\n\n        AtA = A.T @ A\n        AtA_reg = AtA + lam_reg * np.identity(5)\n        Aty = A.T @ y\n        \n        # Solve the linear system (A^T A + lambda I) w = A^T y\n        w = linalg.solve(AtA_reg, Aty, assume_a='sym')\n        return w\n\n    def predict_F(r, w):\n        \"\"\"Predicts the force using the trained MLP model.\"\"\"\n        return -w.T @ get_phi_prime(r)\n\n    def run_simulation(N, dt, tau, B, r0, v0):\n        \"\"\"\n        Runs a single active learning MD simulation for a given set of parameters.\n        \"\"\"\n        # 1. Initialize training set and query counter\n        training_set_r = [r_e, r_e + 0.20]\n        qm_queries_triggered = 0\n\n        # 2. Initial model training\n        w_P = train(training_set_r, lambda_reg)\n        w_A = train(training_set_r[0::2], lambda_reg)\n        w_B = train(training_set_r[1::2], lambda_reg)\n\n        r_current = r0\n        v_current = v0\n        r_trajectory = []\n\n        # 3. Main MD loop\n        for _ in range(N):\n            r_n = r_current\n\n            # 3a. Active Learning: Check uncertainty and retrain if needed\n            F_A = predict_F(r_n, w_A)\n            F_B = predict_F(r_n, w_B)\n            uncertainty = abs(F_A - F_B)\n\n            if uncertainty > tau and qm_queries_triggered < B:\n                training_set_r.append(r_n)\n                qm_queries_triggered += 1\n                \n                # Retrain all three models with the new data point\n                w_P = train(training_set_r, lambda_reg)\n                w_A = train(training_set_r[0::2], lambda_reg)\n                w_B = train(training_set_r[1::2], lambda_reg)\n\n            # 3b. Velocity-Verlet Integration\n            # First half-step for velocity\n            force_n = predict_F(r_n, w_P)\n            a_n = force_n / mu\n            v_half = v_current + 0.5 * dt * a_n\n\n            # Full-step for position\n            r_next = r_current + dt * v_half\n            r_trajectory.append(r_next)\n            \n            # Second half-step for velocity\n            force_next = predict_F(r_next, w_P)\n            a_next = force_next / mu\n            v_next = v_half + 0.5 * dt * a_next\n            \n            # Update state for the next iteration\n            r_current = r_next\n            v_current = v_next\n        \n        # 4. Final Evaluation: Calculate Mean Absolute Error (MAE)\n        total_abs_error = 0.0\n        for r_k in r_trajectory:\n            f_pred = predict_F(r_k, w_P) # Use final production model\n            f_true = F_QM(r_k)\n            total_abs_error += abs(f_pred - f_true)\n            \n        mae = total_abs_error / N\n        \n        return qm_queries_triggered, mae\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200, 0.01, 0.05, 10, 1.30, 0.00),\n        (200, 0.01, 1.00, 10, 1.30, 0.00),\n        (200, 0.01, 0.005, 3, 1.30, 0.00),\n        (100, 0.01, 0.02, 10, 2.00, 0.00),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, dt, tau, B, r0, v0 = case\n        q, mae = run_simulation(N, dt, tau, B, r0, v0)\n        results.extend([q, mae])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2457458"}]}