{"hands_on_practices": [{"introduction": "The ergodic hypothesis, which equates long-time averages to ensemble averages, is a cornerstone of statistical mechanics. However, for systems with complex energy landscapes, this hypothesis can break down over practical simulation timescales. This hands-on exercise [@problem_id:2462993] guides you through building a molecular dynamics simulation from first principles to directly visualize this phenomenon, demonstrating how a particle at low temperature can become trapped in a single energy well, failing to explore the full state space available to it.", "problem": "You are to construct and simulate a two-dimensional potential energy surface with multiple minima and use stochastic Molecular Dynamics to test the ergodic hypothesis. Begin from fundamental principles: Newton’s second law, the definition of force as the negative gradient of the potential, and the canonical ensemble as the target for thermostat-driven dynamics. Model temperature using a standard Langevin thermostat that yields the canonical (Boltzmann) stationary distribution in the long-time limit.\n\nDefine a symmetric four-well potential energy surface with wells centered at $(\\pm x_0,\\pm y_0)$ using reduced, dimensionless units where the Boltzmann constant $k_B$ is set to $1$. Let the potential be\n$$\nV(x,y) = \\sum_{s_x\\in\\{-1,+1\\}} \\sum_{s_y\\in\\{-1,+1\\}} \\left[-A \\exp\\!\\left(-\\frac{(x-s_x x_0)^2 + (y-s_y y_0)^2}{2\\sigma^2}\\right)\\right],\n$$\nwith parameters $A>0$, $\\sigma>0$, $x_0>0$, $y_0>0$. The force is defined by $\\mathbf{F}(x,y) = -\\nabla V(x,y)$. Consider a particle of mass $m$ in this potential subject to Langevin dynamics,\n$$\nm\\ddot{\\mathbf{r}} = \\mathbf{F}(\\mathbf{r}) - \\gamma m \\dot{\\mathbf{r}} + \\sqrt{2\\gamma m k_B T}\\,\\boldsymbol{\\eta}(t),\n$$\nwhere $\\gamma>0$ is the friction coefficient, $T$ is the temperature (in reduced units so that $k_B=1$), and $\\boldsymbol{\\eta}(t)$ is Gaussian white noise with zero mean and unit covariance. Use a time-discretization that is consistent with this stochastic differential equation and preserves the correct stationary distribution for the velocities in the thermostat step.\n\nYour program must:\n- Use reduced, dimensionless units throughout and explicitly set $k_B=1$.\n- Implement a symmetric four-well potential with the following fixed parameters: $A=2.0$, $\\sigma=0.35$, $x_0=1.0$, $y_0=1.0$, $m=1.0$, $\\gamma=1.0$, and time step $\\Delta t=0.005$.\n- Use the BAOAB splitting scheme for Langevin dynamics, in which a full step of length $\\Delta t$ consists of:\n  - $B$: a half velocity update with the conservative force,\n  $$\n  \\mathbf{v} \\leftarrow \\mathbf{v} + \\frac{\\Delta t}{2m}\\,\\mathbf{F}(\\mathbf{r}),\n  $$\n  - $A$: a half position update,\n  $$\n  \\mathbf{r} \\leftarrow \\mathbf{r} + \\frac{\\Delta t}{2}\\,\\mathbf{v},\n  $$\n  - $O$: a full Ornstein–Uhlenbeck velocity update,\n  $$\n  \\mathbf{v} \\leftarrow e^{-\\gamma \\Delta t}\\,\\mathbf{v} + \\sqrt{\\frac{k_B T}{m}\\left(1-e^{-2\\gamma \\Delta t}\\right)}\\,\\mathbf{G},\n  $$\n  where $\\mathbf{G}$ is a vector of independent standard normal variates,\n  - $A$: another half position update,\n  $$\n  \\mathbf{r} \\leftarrow \\mathbf{r} + \\frac{\\Delta t}{2}\\,\\mathbf{v},\n  $$\n  - $B$: a final half velocity update with the conservative force at the new position,\n  $$\n  \\mathbf{v} \\leftarrow \\mathbf{v} + \\frac{\\Delta t}{2m}\\,\\mathbf{F}(\\mathbf{r}).\n  $$\n- Initialize the trajectory at the minimum $(x_0,y_0)$ with zero velocity, that is $\\mathbf{r}(0)=(x_0,y_0)$ and $\\mathbf{v}(0)=(0,0)$.\n- Use a fixed random number generator seed equal to $12345$ to ensure reproducibility of the stochastic steps.\n- Define the four basins by quadrants in the $(x,y)$ plane, identified by the sign of $x$ and $y$. Label basins using the integer index\n$$\nb(x,y) = \\begin{cases}\n0 & \\text{if } x\\ge 0 \\text{ and } y\\ge 0,\\\\\n1 & \\text{if } x< 0 \\text{ and } y\\ge 0,\\\\\n2 & \\text{if } x\\ge 0 \\text{ and } y< 0,\\\\\n3 & \\text{if } x< 0 \\text{ and } y< 0.\n\\end{cases}\n$$\n- For each simulation, count the number of distinct basins visited over the entire trajectory, including the initial configuration.\n\nYour task is to demonstrate ergodicity breaking at low temperature by showing that, for sufficiently small $T$ and finite simulation time, the trajectory remains confined to its initial basin, whereas at sufficiently large $T$ over comparable time, multiple basins are visited. To make this concrete and testable, run the following test suite of temperature–length pairs $(T, N_{\\text{steps}})$:\n- $(0.02, 0)$,\n- $(0.02, 20000)$,\n- $(0.02, 60000)$,\n- $(2.0, 20000)$,\n- $(20.0, 20000)$.\n\nFor each pair, perform a simulation of length $N_{\\text{steps}}$ time steps and output the integer number of distinct basins visited.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces between elements. For example, if the five counts are $c_1$ through $c_5$, the output must be in the format `[c_1,c_2,c_3,c_4,c_5]`. No other text should be printed.", "solution": "The problem as stated is scientifically sound, well-posed, and contains all necessary information for a unique, verifiable solution. It requires the implementation of a molecular dynamics simulation to investigate the ergodic hypothesis for a particle on a two-dimensional potential energy surface. We will proceed with the construction of the required simulation.\n\nThe system is defined by a particle of mass $m$ moving in a two-dimensional potential $V(x,y)$. The potential energy surface is a symmetric four-well potential given by the expression:\n$$\nV(x,y) = \\sum_{s_x\\in\\{-1,+1\\}} \\sum_{s_y\\in\\{-1,+1\\}} \\left[-A \\exp\\!\\left(-\\frac{(x-s_x x_0)^2 + (y-s_y y_0)^2}{2\\sigma^2}\\right)\\right]\n$$\nThe parameters are specified as $A=2.0$, $\\sigma=0.35$, $x_0=1.0$, and $y_0=1.0$ in reduced, dimensionless units.\n\nThe conservative force on the particle is the negative gradient of the potential, $\\mathbf{F}(\\mathbf{r}) = -\\nabla V(\\mathbf{r})$, where $\\mathbf{r}=(x,y)$. The components of the force vector, $F_x$ and $F_y$, are derived by differentiation with respect to $x$ and $y$:\n$$\nF_x(x,y) = -\\frac{\\partial V}{\\partial x} = -\\frac{A}{\\sigma^2} \\sum_{s_x, s_y} (x - s_x x_0) \\exp\\left(-\\frac{(x-s_x x_0)^2 + (y-s_y y_0)^2}{2\\sigma^2}\\right)\n$$\n$$\nF_y(x,y) = -\\frac{\\partial V}{\\partial y} = -\\frac{A}{\\sigma^2} \\sum_{s_x, s_y} (y - s_y y_0) \\exp\\left(-\\frac{(x-s_x x_0)^2 + (y-s_y y_0)^2}{2\\sigma^2}\\right)\n$$\nThese expressions will be used to compute the force at each step of the simulation.\n\nThe system's evolution is governed by the Langevin equation, which models a canonical ensemble at temperature $T$ by including friction and stochastic noise terms:\n$$\nm\\ddot{\\mathbf{r}} = \\mathbf{F}(\\mathbf{r}) - \\gamma m \\dot{\\mathbf{r}} + \\sqrt{2\\gamma m k_B T}\\,\\boldsymbol{\\eta}(t)\n$$\nHere, $\\gamma=1.0$ is the friction coefficient, $m=1.0$ is the mass, $k_B=1.0$ is the Boltzmann constant, and $\\boldsymbol{\\eta}(t)$ represents Gaussian white noise. This stochastic differential equation is integrated numerically using the specified BAOAB splitting scheme. This is a symmetric, time-reversible, and volume-preserving integrator that provides accurate sampling of the configuration space for the canonical ensemble. A single integration step of duration $\\Delta t = 0.005$ is composed of the following five sub-steps, applied in sequence:\n\n1.  **B-step (Force propagation)**: Update velocity for a half time-step under the conservative force $\\mathbf{F}$.\n    $$\n    \\mathbf{v} \\leftarrow \\mathbf{v} + \\frac{\\Delta t}{2m}\\,\\mathbf{F}(\\mathbf{r})\n    $$\n2.  **A-step (Position propagation)**: Update position for a half time-step using the new velocity.\n    $$\n    \\mathbf{r} \\leftarrow \\mathbf{r} + \\frac{\\Delta t}{2}\\,\\mathbf{v}\n    $$\n3.  **O-step (Thermostat)**: Update velocity according to the exact solution of the Ornstein-Uhlenbeck process, which models the combined effects of friction and stochastic forces from the thermal bath.\n    $$\n    \\mathbf{v} \\leftarrow e^{-\\gamma \\Delta t}\\,\\mathbf{v} + \\sqrt{\\frac{k_B T}{m}\\left(1-e^{-2\\gamma \\Delta t}\\right)}\\,\\mathbf{G}\n    $$\n    where $\\mathbf{G}$ is a vector of two independent random numbers drawn from a standard normal distribution.\n4.  **A-step (Position propagation)**: Update position for the second half time-step.\n    $$\n    \\mathbf{r} \\leftarrow \\mathbf{r} + \\frac{\\Delta t}{2}\\,\\mathbf{v}\n    $$\n5.  **B-step (Force propagation)**: Update velocity for the final half time-step using the force at the newly computed position.\n    $$\n    \\mathbf{v} \\leftarrow \\mathbf{v} + \\frac{\\Delta t}{2m}\\,\\mathbf{F}(\\mathbf{r})\n    $$\n\nThe simulation protocol is executed for each pair of temperature $T$ and number of steps $N_{\\text{steps}}$ provided in the test suite. For each test case, the system is initialized at position $\\mathbf{r}(0)=(x_0, y_0)=(1.0, 1.0)$ with zero initial velocity $\\mathbf{v}(0)=(0,0)$. To ensure reproducible results, the pseudo-random number generator is seeded with the value $12345$ at the start of each individual simulation run.\n\nDuring each simulation, the position of the particle is monitored. The plane is partitioned into four basins based on the signs of the $x$ and $y$ coordinates. The basin index $b(x,y)$ is determined as follows:\n$$\nb(x,y) = \\begin{cases}\n0 & \\text{if } x\\ge 0 \\text{ and } y\\ge 0,\\\\\n1 & \\text{if } x< 0 \\text{ and } y\\ge 0,\\\\\n2 & \\text{if } x\\ge 0 \\text{ and } y< 0,\\\\\n3 & \\text{if } x< 0 \\text{ and } y< 0.\n\\end{cases}\n$$\nThe set of visited basins is tracked throughout the trajectory, starting with the initial basin. The primary observable is the total number of distinct basins visited over the course of the simulation.\n\nThis analysis serves to test the ergodic hypothesis. Ergodicity implies that, given sufficient time, a system will explore all accessible states with the same energy. In this system, the four wells are energetically equivalent minima. At low temperature, the thermal energy $k_B T$ is small compared to the potential energy barriers separating the wells. The system is therefore expected to remain trapped in its initial basin for the duration of the simulation, exhibiting non-ergodic behavior on this finite timescale. At high temperature, the thermal energy is sufficient to overcome the barriers frequently, allowing the system to explore all four basins and thus behave ergodically. The results from the specified test cases will demonstrate this temperature-dependent transition.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of simulating a particle in a four-well potential\n    using Langevin dynamics to test the ergodic hypothesis.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    A = 2.0\n    sigma = 0.35\n    x0 = 1.0\n    y0 = 1.0\n    m = 1.0\n    gamma = 1.0\n    dt = 0.005\n    k_B = 1.0  # Boltzmann constant in reduced units\n\n    # --- Test Cases (T, N_steps) ---\n    test_cases = [\n        (0.02, 0),\n        (0.02, 20000),\n        (0.02, 60000),\n        (2.0, 20000),\n        (20.0, 20000),\n    ]\n\n    # --- Helper Functions ---\n    def get_force(r: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Calculates the force F = -nabla(V) at position r.\n        \"\"\"\n        x, y = r\n        force = np.zeros(2)\n        s_vals = [-1.0, 1.0]\n        prefactor = A / (sigma**2)\n        \n        for sx in s_vals:\n            for sy in s_vals:\n                dx = x - sx * x0\n                dy = y - sy * y0\n                arg = -(dx**2 + dy**2) / (2 * sigma**2)\n                exp_term = np.exp(arg)\n                force[0] -= prefactor * dx * exp_term\n                force[1] -= prefactor * dy * exp_term\n        return force\n\n    def get_basin(r: np.ndarray) -> int:\n        \"\"\"\n        Determines the basin index for a given position r.\n        \"\"\"\n        x, y = r\n        if x >= 0 and y >= 0:\n            return 0\n        elif x < 0 and y >= 0:\n            return 1\n        elif x >= 0 and y < 0:\n            return 2\n        else:  # x < 0 and y < 0\n            return 3\n\n    results = []\n    # --- Main Simulation Loop ---\n    for T, N_steps in test_cases:\n        # Re-seed for each independent simulation to ensure comparability\n        rng = np.random.default_rng(12345)\n\n        # Initial conditions\n        r = np.array([x0, y0])\n        v = np.array([0.0, 0.0])\n\n        # Track visited basins\n        visited_basins = {get_basin(r)}\n\n        # BAOAB integrator constants for the O-step\n        c1 = np.exp(-gamma * dt)\n        c2 = np.sqrt((k_B * T / m) * (1 - c1**2))\n\n        for _ in range(N_steps):\n            # B-step (half)\n            force = get_force(r)\n            v += (dt / (2 * m)) * force\n\n            # A-step (half)\n            r += (dt / 2) * v\n\n            # O-step\n            G = rng.standard_normal(2)\n            v = c1 * v + c2 * G\n\n            # A-step (half)\n            r += (dt / 2) * v\n\n            # B-step (half)\n            force = get_force(r)\n            v += (dt / (2 * m)) * force\n\n            # Update visited basins\n            visited_basins.add(get_basin(r))\n        \n        results.append(len(visited_basins))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2462993"}, {"introduction": "While visual inspection can suggest ergodicity breaking, a quantitative analysis is essential for rigorous science. This practice introduces powerful tools from time series analysis to diagnose how effectively a simulation is sampling its target ensemble. You will implement a method to compute the statistical inefficiency and an \"ergodicity score\" [@problem_id:2462952], learning a practical technique to assess the convergence and reliability of data from any molecular simulation.", "problem": "You are given discrete-time trajectories of a single scalar observable from synthetic molecular simulations. The goal is to quantify how well a single long trajectory samples the stationary distribution (ergodicity) by comparing the variance of the property across time segments with the variance predicted from time correlations within the trajectory.\n\nStart from the following fundamental base: time averages and ensemble averages in a stationary ergodic process coincide; the sample variance quantifies fluctuations; and time correlations inflate the variance of block averages. For a discrete-time series $\\{x_k\\}_{k=1}^N$ sampled at uniform intervals, define the following quantities from first principles:\n\n- The sample mean $\\mu = \\frac{1}{N}\\sum_{k=1}^N x_k$.\n- The unbiased sample variance $\\sigma^2 = \\frac{1}{N-1}\\sum_{k=1}^N (x_k - \\mu)^2$.\n- The unbiased sample autocovariance at lag $k \\ge 0$,\n  $$C(k) = \\frac{1}{N-k}\\sum_{t=1}^{N-k} \\left(x_t - \\mu\\right)\\left(x_{t+k} - \\mu\\right),$$\n  and the normalized autocorrelation $\\rho(k) = \\frac{C(k)}{C(0)}$ for $k \\ge 1$, with $\\rho(0) = 1$ by definition.\n- The statistical inefficiency $g$, estimated using the initial-positive-sequence truncation rule,\n  $$g = 1 + 2\\sum_{k=1}^{K^\\star} \\rho(k),$$\n  where $K^\\star$ is the largest nonnegative integer such that $\\rho(k) &gt; 0$ for all $1 \\le k \\le K^\\star$, and $K^\\star \\le L-1$. Here $L$ is the common length of each segment defined below. If no positive lags exist, take $K^\\star = 0$ so that $g=1$.\n\nPartition the trajectory into $M$ contiguous, non-overlapping segments of equal length $L = N/M$ (assume $M$ divides $N$). For segment $i \\in \\{1,\\dots,M\\}$, let the segment mean be $\\mu_i = \\frac{1}{L}\\sum_{k=(i-1)L+1}^{iL} x_k$. Let the unbiased sample variance across segment means be\n$$s_\\mu^2 = \\frac{1}{M-1}\\sum_{i=1}^M \\left(\\mu_i - \\bar{\\mu}\\right)^2,$$\nwhere $\\bar{\\mu} = \\frac{1}{M}\\sum_{i=1}^M \\mu_i$.\n\nUnder stationarity and ergodicity, the variance of block (segment) means is inflated by time correlations and is predicted by\n$$\\operatorname{Var}(\\mu_{\\text{block}}) \\approx \\frac{\\sigma^2\\, g}{L}.$$\nDefine the ergodicity score\n$$E = \\frac{s_\\mu^2}{\\sigma^2 g / L}.$$\nFor a long, stationary ergodic trajectory with sufficiently large $L$ compared to the correlation time, one expects $E \\approx 1$. Values $E \\gg 1$ indicate slow exploration or nonstationarity over the observation window.\n\nTask: Write a complete program that, for each of the test cases below, generates the specified time series, partitions it as described, computes $E$ using the definitions above, and outputs the results.\n\nConventions and edge cases:\n- Use unbiased estimators where specified.\n- In computing $g$, implement the truncation by summing $\\rho(k)$ only up to the first lag where $\\rho(k) \\le 0$, and never include lags beyond $L-1$.\n- If $\\sigma^2 = 0$ or $M &lt; 2$, define $E = 0.0$ by convention to avoid undefined operations.\n- All random number generation must be reproducible with the given seeds.\n\nTest suite:\n1. Stationary autoregressive process of order one (AR(1)): $x_k = \\phi x_{k-1} + \\xi_k$, with $\\phi = 0.8$, Gaussian noise $\\xi_k \\sim \\mathcal{N}(0,\\sigma_\\xi^2)$ where $\\sigma_\\xi^2 = 1 - \\phi^2$ so that the stationary variance is $1$. Initialize $x_1$ from $\\mathcal{N}(0,1)$. Use $N = 10000$, $M = 20$, and random seed $12345$.\n2. Nonergodic over the window (piecewise mean): Independent Gaussian samples with a mean shift halfway. Specifically, for $k \\le N/2$, $x_k \\sim \\mathcal{N}(-2, 0.2^2)$, and for $k &gt; N/2$, $x_k \\sim \\mathcal{N}(+2, 0.2^2)$. Use $N = 10000$, $M = 20$, and random seed $24680$ for reproducibility of the Gaussian draws.\n3. Degenerate observable: Constant signal $x_k \\equiv 3.14$. Use $N = 5000$, $M = 10$.\n\nRequired final output format: Your program should produce a single line of output containing the three ergodicity scores as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (e.g., \"[1.000000,2.345678,0.000000]\"). No additional text should be printed.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of statistical mechanics and time series analysis, specifically concerning the estimation of statistical errors in correlated data. The problem is well-posed, with all necessary mathematical definitions, parameters, and computational procedures clearly specified. It is objective and free of ambiguities that would preclude a unique, verifiable solution.\n\nThe task is to compute an ergodicity score, $E$, for three different time series. This score compares the observed variance of block-averaged means, $s_\\mu^2$, with the variance predicted by a model based on stationary process theory, $\\operatorname{Var}(\\mu_{\\text{block}}) \\approx \\frac{\\sigma^2 g}{L}$. A score of $E \\approx 1$ suggests that the trajectory behaves like a sample from a stationary, ergodic process over the observation window. Deviations from $E=1$ can indicate non-stationarity or poor sampling. The calculation will proceed by meticulously implementing the formulas provided in the problem statement.\n\nThe overall procedure for each test case is as follows:\n1.  Generate the time series $\\{x_k\\}_{k=1}^N$ according to the specified model and parameters.\n2.  Handle the specified edge cases: if the number of segments $M < 2$ or the total variance $\\sigma^2 = 0$, the score $E$ is defined as $0.0$.\n3.  Compute the fundamental statistics of the full time series: the sample mean $\\mu = \\frac{1}{N}\\sum_{k=1}^N x_k$ and the unbiased sample variance $\\sigma^2 = \\frac{1}{N-1}\\sum_{k=1}^N (x_k - \\mu)^2$.\n4.  Compute the statistical inefficiency, $g$. This requires several sub-steps:\n    a.  Calculate the sample autocovariance function, $C(k)$, for lags $k$ from $0$ to $L-1$, where $L=N/M$ is the segment length. The problem specifies the unbiased estimator $C(k) = \\frac{1}{N-k}\\sum_{t=1}^{N-k} (x_t - \\mu)(x_{t+k} - \\mu)$.\n    b.  Normalize the autocovariance to get the autocorrelation function, $\\rho(k) = C(k)/C(0)$. Note that $C(0)$ is computed using the formula for $C(k)$ with $k=0$, which corresponds to the biased sample variance.\n    c.  Determine the truncation lag $K^\\star$ as the largest integer such that $\\rho(k) > 0$ for all $1 \\le k \\le K^\\star$ and $K^\\star \\le L-1$.\n    d.  Compute $g$ using the formula $g = 1 + 2\\sum_{k=1}^{K^\\star} \\rho(k)$. If no such positive lags exist, $K^\\star=0$ and $g=1$.\n5.  Compute the variance of block means, $s_\\mu^2$.\n    a.  Partition the time series of length $N$ into $M$ non-overlapping segments of length $L$.\n    b.  Calculate the mean $\\mu_i$ for each segment $i \\in \\{1, \\dots, M\\}$.\n    c.  Calculate the unbiased sample variance of these $M$ segment means, $s_\\mu^2 = \\frac{1}{M-1}\\sum_{i=1}^M (\\mu_i - \\bar{\\mu})^2$, where $\\bar{\\mu}$ is the mean of the segment means.\n6.  Finally, compute the ergodicity score $E$ using the provided formula: $E = \\frac{s_\\mu^2}{\\sigma^2 g / L}$.\n\nThis procedure is applied to each of the three test cases.\n\n-   **Test Case 1 (AR(1) process):** A stationary, ergodic autoregressive process is generated. For such a process, the assumptions underlying the prediction formula are met. Therefore, we expect the observed variance of block means to be close to the predicted variance, yielding an ergodicity score $E \\approx 1$.\n\n-   **Test Case 2 (Piecewise mean):** A non-stationary process is generated by joining two segments with distinctly different means. This violates the stationarity assumption. The overall variance $\\sigma^2$ will be large, dominated by the jump in the mean. The autocorrelation will decay very slowly, leading to a large value for $g$. The variance of block means $s_\\mu^2$ will also be large, as half the blocks have a low mean and half have a high mean. The score $E$ is the ratio of these quantities. The problem states that $E \\gg 1$ suggests non-stationarity, but the result depends on whether the observed variance of block means is larger or smaller than the value predicted by formally applying the stationary-process formula.\n\n-   **Test Case 3 (Degenerate observable):** A constant signal is generated. For this trivial case, the variance of the data is zero. Following the problem's explicit rule, since $\\sigma^2 = 0$, the ergodicity score is immediately determined to be $E=0.0$.\n\nThe implementation will use the `numpy` library for efficient numerical computation, particularly for generating the random variates, calculating variances, and computing the correlation sums required for the autocovariance function. Random number generators will be seeded as specified to ensure reproducibility.", "answer": "```python\nimport numpy as np\n\ndef calculate_ergodicity_score(x: np.ndarray, M: int) -> float:\n    \"\"\"\n    Computes the ergodicity score E for a given time series.\n    \n    Args:\n        x (np.ndarray): The time series data.\n        M (int): The number of segments to partition the data into.\n        \n    Returns:\n        float: The calculated ergodicity score E.\n    \"\"\"\n    N = len(x)\n\n    # Per problem statement conventions for M < 2\n    if M < 2:\n        return 0.0\n\n    L = N // M\n    if L == 0:\n        # This case implies N < M, which is not in the test suite.\n        # But for robustness, we can handle it. s_mu^2 would be ill-defined.\n        return 0.0\n        \n    # Calculate unbiased sample variance of the entire trajectory\n    # This is sigma^2\n    sigma_sq = np.var(x, ddof=1)\n    \n    # Per problem statement conventions for sigma^2 = 0\n    if sigma_sq == 0.0:\n        return 0.0\n\n    # Calculate the statistical inefficiency g\n    mu = np.mean(x)\n    y = x - mu\n    \n    # Use numpy.correlate to get sums for autocovariance calculation\n    # The result `corr_sum` has length 2*N - 1.\n    # The item at index (N-1) corresponds to lag 0.\n    # The item at index (N-1+k) corresponds to lag k.\n    corr_sum = np.correlate(y, y, mode='full')\n    \n    # C(k) = (1/(N-k)) * sum_{t=1}^{N-k} (x_t-mu)(x_{t+k}-mu)\n    # C(0) is calculated with k=0, so denominator is N. This is the biased variance.\n    c0 = corr_sum[N - 1] / N\n    \n    g = 1.0\n    if c0 > 0:\n        # Sum rho(k) according to the initial-positive-sequence rule.\n        # The maximum lag to consider for the sum is K_star <= L-1.\n        for k in range(1, L):\n            # C(k) is an unbiased estimator, so we divide by (N-k)\n            ck = corr_sum[N - 1 + k] / (N - k)\n            rho_k = ck / c0\n            if rho_k > 0:\n                g += 2.0 * rho_k\n            else:\n                # Truncate sum at the first non-positive rho(k)\n                break\n    \n    # Calculate the unbiased sample variance of the segment means, s_mu^2\n    segments = x.reshape((M, L))\n    segment_means = np.mean(segments, axis=1)\n    s_mu_sq = np.var(segment_means, ddof=1)\n    \n    # Calculate the predicted variance of the block mean\n    predicted_var = (sigma_sq * g) / L\n    \n    if predicted_var == 0.0:\n        # This is unlikely if sigma_sq > 0, g >= 1, L > 0.\n        # If it happens, E could be inf or nan. Returning 0.0 is a safe fallback.\n        return 0.0\n\n    # Calculate the final ergodicity score E\n    E = s_mu_sq / predicted_var\n    \n    return E\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (N, M, generator_func, seed)\n        (10000, 20, 'ar1', 12345),\n        (10000, 20, 'piecewise', 24680),\n        (5000, 10, 'constant', None),\n    ]\n\n    results = []\n    \n    for N, M, generator_type, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        x = np.zeros(N)\n\n        if generator_type == 'ar1':\n            phi = 0.8\n            sigma_xi_sq = 1 - phi**2\n            sigma_xi = np.sqrt(sigma_xi_sq)\n            \n            # Initialize from stationary distribution N(0, 1)\n            x[0] = rng.normal(loc=0.0, scale=1.0)\n            noise = rng.normal(loc=0.0, scale=sigma_xi, size=N-1)\n            for k in range(1, N):\n                x[k] = phi * x[k-1] + noise[k-1]\n\n        elif generator_type == 'piecewise':\n            half_n = N // 2\n            mean1, mean2 = -2.0, 2.0\n            std_dev = 0.2\n            \n            x[:half_n] = rng.normal(loc=mean1, scale=std_dev, size=half_n)\n            x[half_n:] = rng.normal(loc=mean2, scale=std_dev, size=N - half_n)\n\n        elif generator_type == 'constant':\n            x[:] = 3.14\n\n        E = calculate_ergodicity_score(x, M)\n        results.append(E)\n\n    # Format the output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2462952"}, {"introduction": "The abstract idea of an ensemble, rooted in the postulate of equal a priori probabilities for all accessible microstates, has profound physical consequences. This exercise explores one such consequence by tackling the famous Gibbs' paradox through computation. By calculating the entropy of mixing for an ideal gas [@problem_id:2462921], you will discover firsthand why the classical notion of distinguishable particles fails and how quantum mechanical indistinguishability is essential for a consistent thermodynamic theory.", "problem": "You will write a complete, runnable program that, under the assumptions of classical statistical mechanics for an ideal gas, computes the entropy change upon removing a partition between two compartments and allowing the contents to mix. You must contrast two counting conventions to demonstrate Gibbs' paradox: one in which particles are treated as distinguishable and one in which particles are treated as indistinguishable. Assume non-interacting point particles in the microcanonical ensemble, where every accessible microstate in a fixed energy shell is equiprobable by the equal a priori postulate implied by the ergodic hypothesis, and define entropy as $S = k_{\\mathrm{B}} \\ln W$, where $W$ is the number of accessible microstates and $k_{\\mathrm{B}}$ is Boltzmann's constant.\n\nConsider a rigid, adiabatic container divided by a removable partition into compartment $A$ of volume $V_A$ and compartment $B$ of volume $V_B$. Initially, compartment $A$ contains $N_A$ particles and compartment $B$ contains $N_B$ particles. The two compartments are at equal number density and equal specific energy, so that upon removal of the partition there is no net exchange of energy other than the increase in accessible configuration space due to mixing. You must use the natural logarithm and express all entropy changes in units of $k_{\\mathrm{B}}$ (that is, report $S/k_{\\mathrm{B}}$ as a dimensionless number).\n\nDefine the two counting conventions as follows:\n- Distinguishable particles: the number of accessible microstates scales as $W \\propto V^{N}$ without any division by factorials of particle numbers.\n- Indistinguishable particles: the number of accessible microstates scales as $W \\propto V^{N} / N!$, where $N!$ is the factorial of the particle count.\n\nFor each test case, compute the entropy change upon mixing, $\\Delta S = S_{\\text{after}} - S_{\\text{before}}$, separately for the two conventions, and report the two values as $S/k_{\\mathrm{B}}$.\n\nUse the following test suite (each test case is a tuple $(N_A, N_B, V_A, V_B)$):\n- Test case $1$: $(100, 100, 1.0, 1.0)$.\n- Test case $2$: $(50, 150, 0.75, 2.25)$.\n- Test case $3$: $(3, 2, 0.3, 0.2)$.\n- Test case $4$: $(1, 1, 0.1, 0.1)$.\n- Test case $5$: $(1000, 1000, 10.0, 10.0)$.\n\nAll five test cases satisfy $N_A/V_A = N_B/V_B$ so that the initial number densities are equal.\n\nYour program must output a single line containing a flat list of real numbers with no spaces, representing for each test case in order:\n- The distinguishable-particle entropy change $\\Delta S_{\\mathrm{dist}}/k_{\\mathrm{B}}$ as a float.\n- The indistinguishable-particle entropy change $\\Delta S_{\\mathrm{indist}}/k_{\\mathrm{B}}$ as a float.\n\nThus the final output should be a single line with the format\n$[d_1,i_1,d_2,i_2,d_3,i_3,d_4,i_4,d_5,i_5]$\nwhere $d_j$ and $i_j$ are the two requested floats for test case $j$. The numbers must be computed from first principles as specified and printed using the natural logarithm, and must be expressed in units of $k_{\\mathrm{B}}$.", "solution": "We are asked to compute entropy changes for mixing in two counting conventions within the classical ideal-gas framework. The microcanonical ensemble assumes all microstates with the same conserved quantities are equally probable by the ergodic hypothesis. Entropy is defined from first principles as $S = k_{\\mathrm{B}} \\ln W$, where $W$ is the count of accessible microstates.\n\nSet up: Initially, there are two independent subsystems, $A$ with $N_A$ particles confined to volume $V_A$ and $B$ with $N_B$ particles confined to volume $V_B$. After removing the partition, every particle has access to the total volume $V = V_A + V_B$. We assume ideal, non-interacting particles and equal number densities and specific energies initially, so momentum-space contributions and any energy-dependent terms cancel between the initial and final states. Therefore, the entropy change upon mixing is determined purely by changes in configuration-space multiplicities.\n\nDistinguishable counting: In classical mechanics without indistinguishability corrections, the configuration-space multiplicity scales as $W \\propto V^{N}$ for $N$ labeled particles in volume $V$. For the initial state,\n$$\nW_{\\text{before}}^{\\text{(dist)}} \\propto V_A^{N_A} \\, V_B^{N_B}.\n$$\nAfter mixing,\n$$\nW_{\\text{after}}^{\\text{(dist)}} \\propto (V_A + V_B)^{N_A + N_B}.\n$$\nHence the entropy change in units of $k_{\\mathrm{B}}$ is\n$$\n\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}}\n= \\ln \\frac{W_{\\text{after}}^{\\text{(dist)}}}{W_{\\text{before}}^{\\text{(dist)}}}\n= (N_A + N_B) \\ln (V_A + V_B) - N_A \\ln V_A - N_B \\ln V_B\n= N_A \\ln \\frac{V_A + V_B}{V_A} + N_B \\ln \\frac{V_A + V_B}{V_B}.\n$$\n\nIndistinguishable counting: For indistinguishable classical particles one divides by factorials to remove permutations of identical particles. For two compartments initially separated (physically disjoint), the multiplicity is\n$$\nW_{\\text{before}}^{\\text{(indist)}} \\propto \\frac{V_A^{N_A}}{N_A!} \\, \\frac{V_B^{N_B}}{N_B!}.\n$$\nAfter removing the partition for identical particles,\n$$\nW_{\\text{after}}^{\\text{(indist)}} \\propto \\frac{(V_A + V_B)^{N_A + N_B}}{(N_A + N_B)!}.\n$$\nThus,\n$$\n\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}}\n= \\ln \\frac{W_{\\text{after}}^{\\text{(indist)}}}{W_{\\text{before}}^{\\text{(indist)}}}\n= \\left[(N_A + N_B) \\ln (V_A + V_B) - \\ln (N_A + N_B)!\\right]\n- \\left[N_A \\ln V_A - \\ln N_A! + N_B \\ln V_B - \\ln N_B!\\right]\n$$\n$$\n= N_A \\ln \\frac{V_A + V_B}{V_A} + N_B \\ln \\frac{V_A + V_B}{V_B}\n- \\left[\\ln (N_A + N_B)! - \\ln N_A! - \\ln N_B!\\right].\n$$\nComparing the two, the indistinguishable result subtracts the multinomial combinatorial factor $\\ln \\frac{(N_A + N_B)!}{N_A! \\, N_B!}$ that counts the ways to assign labels to otherwise identical particles. This subtraction resolves Gibbs' paradox: for equal initial number densities, the extensive $\\mathcal{O}(N)$ contribution to mixing cancels against the combinatorial term in the thermodynamic limit, making the entropy of mixing for identical particles vanish per particle. Finite systems retain a subextensive correction of order $\\mathcal{O}(\\ln N)$.\n\nAlgorithmic realization: For numerical stability, one computes $\\ln n!$ via the logarithm of the Gamma function, using $\\ln n! = \\ln \\Gamma(n+1)$. Given a test case $(N_A, N_B, V_A, V_B)$:\n- Compute $V = V_A + V_B$.\n- Compute $\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} = N_A \\ln (V/V_A) + N_B \\ln (V/V_B)$.\n- Compute $\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}} = \\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} - \\left[\\ln \\Gamma(N_A + N_B + 1) - \\ln \\Gamma(N_A + 1) - \\ln \\Gamma(N_B + 1)\\right]$.\n\nApplication to the test suite:\n- Test case $1$ $(100, 100, 1.0, 1.0)$ yields $\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} = 200 \\ln 2$ and $\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}} = 200 \\ln 2 - \\ln \\binom{200}{100}$, a small subextensive positive number compared to the extensive distinguishable result.\n- Test case $2$ $(50, 150, 0.75, 2.25)$ yields $\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} = 50 \\ln 4 + 150 \\ln \\frac{4}{3}$ and $\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}} = \\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} - \\ln \\binom{200}{50}$.\n- Test case $3$ $(3, 2, 0.3, 0.2)$ yields $\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} = 3 \\ln \\frac{5}{3} + 2 \\ln \\frac{5}{2}$ and $\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}} = \\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} - \\ln 10$.\n- Test case $4$ $(1, 1, 0.1, 0.1)$ yields $\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} = 2 \\ln 2$ and $\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}} = \\ln 2$.\n- Test case $5$ $(1000, 1000, 10.0, 10.0)$ illustrates the thermodynamic limit: $\\frac{\\Delta S_{\\text{dist}}}{k_{\\mathrm{B}}} = 2000 \\ln 2$ whereas $\\frac{\\Delta S_{\\text{indist}}}{k_{\\mathrm{B}}} = 2000 \\ln 2 - \\ln \\binom{2000}{1000}$ is subextensive and grows only like $\\frac{1}{2}\\ln(\\pi N)$, so per particle it tends to zero.\n\nFinally, aggregate the results for the five cases into a single flat list in the specified order and print them on one line as $[d_1,i_1,d_2,i_2,d_3,i_3,d_4,i_4,d_5,i_5]$, with each entry a floating-point number representing $S/k_{\\mathrm{B}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef mixing_entropy_distinguishable(Na, Nb, Va, Vb):\n    V = Va + Vb\n    return Na * np.log(V / Va) + Nb * np.log(V / Vb)\n\ndef mixing_entropy_indistinguishable(Na, Nb, Va, Vb):\n    # Use gamma function for exact ln(n!) stability: ln(n!) = gammaln(n+1)\n    dist = mixing_entropy_distinguishable(Na, Nb, Va, Vb)\n    comb = gammaln(Na + Nb + 1) - gammaln(Na + 1) - gammaln(Nb + 1)\n    return dist - comb\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (N_A, N_B, V_A, V_B)\n    test_cases = [\n        (100, 100, 1.0, 1.0),\n        (50, 150, 0.75, 2.25),\n        (3, 2, 0.3, 0.2),\n        (1, 1, 0.1, 0.1),\n        (1000, 1000, 10.0, 10.0),\n    ]\n\n    results = []\n    for Na, Nb, Va, Vb in test_cases:\n        d = mixing_entropy_distinguishable(Na, Nb, Va, Vb)\n        i = mixing_entropy_indistinguishable(Na, Nb, Va, Vb)\n        # Format as floats; keep a consistent decimal representation\n        results.append(f\"{d:.6f}\")\n        results.append(f\"{i:.6f}\")\n\n    # Final print statement in the exact required format: no spaces\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2462921"}]}