{"hands_on_practices": [{"introduction": "The Linear Speedup Theorem suggests that any computation can be made arbitrarily faster by a constant factor. However, real-world optimizations often introduce their own fixed costs, such as initialization or data transfer latency. This practice problem [@problem_id:1430460] moves from pure theory to a practical scenario, asking you to determine the \"breakeven\" point for a hardware upgrade. By analyzing when the new system's overhead is overcome by its faster processing speed, you will develop a crucial intuition for the trade-offs inherent in any performance enhancement.", "problem": "A computational research lab is evaluating an upgrade for their high-performance computing cluster. Their primary workload consists of a simulation algorithm whose runtime complexity on the current hardware is well-modeled by the function $T(n) = A n^k$, where $n$ represents the size of the input data. The parameters $A$ and $k$ are positive real constants determined by the existing hardware and the specific algorithm.\n\nA proposed hardware accelerator promises a speedup factor of $c$ for the core computations, where $c$ is a real constant greater than 1. However, using the accelerator introduces a fixed, non-negligible setup and data-transfer latency of $K$ seconds, a positive real constant that is independent of the input size $n$. Consequently, the total runtime on the upgraded system is given by the function $T'(n) = \\frac{A n^k}{c} + K$.\n\nThe lab will only consider the upgrade cost-effective for problem sizes where it offers a genuine reduction in runtime. Determine the analytical expression for the minimum input size, denoted as $n_{min}$, such that for all input sizes $n > n_{min}$, the upgraded system is faster than the original system. Express your answer in terms of the parameters $A, k, c,$ and $K$.", "solution": "We seek the condition under which the upgraded system is faster, i.e., $T'(n) < T(n)$. Using the given models, this inequality is\n$$\n\\frac{A n^{k}}{c} + K < A n^{k}.\n$$\nSubtract $\\frac{A n^{k}}{c}$ from both sides to isolate $K$:\n$$\nK < A n^{k} - \\frac{A n^{k}}{c} = A n^{k} \\left(1 - \\frac{1}{c}\\right).\n$$\nSince $c > 1$, we have $1 - \\frac{1}{c} = \\frac{c-1}{c} > 0$. Divide both sides by $A \\frac{c-1}{c}$ (which is positive because $A > 0$ and $c > 1$) to obtain\n$$\nn^{k} > \\frac{K}{A} \\cdot \\frac{c}{c - 1} = \\frac{K c}{A (c - 1)}.\n$$\nBecause $k > 0$, the function $x \\mapsto x^{1/k}$ is strictly increasing on positive reals, so taking the $k$-th root preserves the inequality:\n$$\nn > \\left( \\frac{K c}{A (c - 1)} \\right)^{\\frac{1}{k}}.\n$$\nDefine $n_{min}$ as the threshold where equality holds, noting that the difference $T(n) - T'(n) = A n^{k} \\left(1 - \\frac{1}{c}\\right) - K$ is strictly increasing in $n$ for $k > 0$, ensuring that for all $n > n_{min}$ the upgraded system is faster. Therefore,\n$$\nn_{min} = \\left( \\frac{K c}{A (c - 1)} \\right)^{\\frac{1}{k}}.\n$$", "answer": "$$\\boxed{\\left(\\frac{K c}{A (c - 1)}\\right)^{\\frac{1}{k}}}$$", "id": "1430460"}, {"introduction": "The standard proof of the Linear Speedup Theorem is often presented for a one-dimensional Turing machine tape, but its underlying principles are more general. This exercise [@problem_id:1430455] challenges you to adapt the core concepts of symbol packing and local simulation to a two-dimensional computational grid. By reasoning about how to define \"blocks\" and \"neighborhoods\" in two-dimensional space, you will gain a deeper appreciation for the geometric and algorithmic flexibility of the speedup construction beyond its canonical formulation.", "problem": "Consider a Turing Machine (TM) with a two-dimensional tape, infinite in all four directions (North, South, East, West). Let this machine be $M$. The time complexity of $M$ for an input provided as an initial non-blank configuration on an $n \\times n$ square of tape cells is given by the function $T(n) = \\alpha n^{3} + \\beta n^{2}$, where $\\alpha$ and $\\beta$ are known positive constants.\n\nWe wish to construct a new 2D-tape TM, let's call it $M'$, that solves the same problem but faster. The strategy is to use tape compression. Each cell of $M'$'s tape stores a single \"macro-symbol\" that represents an entire $m \\times m$ block of cells from $M$'s tape, where $m$ is a positive integer called the compression factor.\n\nThe operation of $M'$ consists of two phases: an initial compression phase and a simulation phase.\n1.  **Compression Phase**: $M'$ reads the entire $n \\times n$ input from $M$'s tape and writes the corresponding compressed $(\\lceil n/m \\rceil \\times \\lceil n/m \\rceil)$ configuration onto its own tape. This phase takes a total of $\\gamma n^{2}$ steps, where $\\gamma$ is a known positive constant.\n2.  **Simulation Phase**: $M'$ simulates the execution of $M$. To simulate a block of $m$ consecutive steps of $M$, $M'$ performs a fixed routine that takes a constant number of its own steps. This routine involves:\n    a. A sequence of $K$ moves to read the macro-symbols in its current cell and the 8 adjacent cells (a $3 \\times 3$ neighborhood). $K$ is a given positive integer constant.\n    b. An internal computation within its finite control (which takes zero moves on the tape).\n    c. A single write operation to update the macro-symbol in its current cell.\n    d. A single move to the adjacent macro-cell that now contains the simulated head of $M$. (It is guaranteed that after $m$ steps, the head of $M$ will have moved to a position contained within the $3 \\times 3$ neighborhood of macro-cells).\n\nThe total time for $M'$, $T'(n)$, is the sum of the time for these two phases. The goal is to achieve a linear speedup, defined by the condition $T'(n) \\le \\frac{1}{c} T(n)$ for a desired speedup factor $c>1$ and for all sufficiently large integers $n$.\n\nDetermine the minimum positive integer value for the compression factor $m$ that guarantees this speedup is achieved. Your answer should be a closed-form analytic expression in terms of the constants $c$ and $K$.", "solution": "We model the running time of the compressed simulator $M'$ by counting its steps in the two phases.\n\nCompression phase: By assumption, compressing the $n \\times n$ input into macro-cells takes $\\gamma n^{2}$ steps.\n\nSimulation phase: Let one macro-step be the routine that simulates exactly $m$ steps of $M$ using the fixed sequence: $K$ moves to collect the $3 \\times 3$ neighborhood of macro-cells, $0$ moves for internal computation, $1$ write, and $1$ move to the next macro-cell. Hence each macro-step costs $K+2$ moves of $M'$. To simulate $T(n)$ steps of $M$, $M'$ needs at most $\\lceil T(n)/m \\rceil$ macro-steps. Therefore\n$$\nT'(n) \\le \\gamma n^{2} + (K+2)\\left\\lceil \\frac{T(n)}{m} \\right\\rceil \\le \\gamma n^{2} + \\frac{K+2}{m}T(n) + (K+2).\n$$\nUsing $T(n)=\\alpha n^{3}+\\beta n^{2}$, this yields\n$$\nT'(n) \\le \\gamma n^{2} + \\frac{K+2}{m}\\left(\\alpha n^{3}+\\beta n^{2}\\right) + (K+2).\n$$\n\nTo achieve a linear speedup by a factor $c>1$ for all sufficiently large $n$, we require\n$$\nT'(n) \\le \\frac{1}{c}T(n) = \\frac{1}{c}\\left(\\alpha n^{3}+\\beta n^{2}\\right)\n$$\neventually in $n$. Ignoring the additive constant $(K+2)$, which is dominated for large $n$, this inequality is implied for sufficiently large $n$ if and only if the leading $n^{3}$-term on the left is strictly smaller than the leading $n^{3}$-term on the right. Writing the inequality explicitly and collecting powers of $n$ gives\n$$\n\\left(\\frac{K+2}{m}-\\frac{1}{c}\\right)\\alpha n^{3}+\\left(\\gamma+\\frac{K+2}{m}\\beta-\\frac{1}{c}\\beta\\right)n^{2} \\le 0.\n$$\nSince $\\alpha,\\beta,\\gamma>0$, the $n^{3}$ term must have a strictly negative coefficient to guarantee the inequality for all sufficiently large $n$; equality would leave a positive $n^{2}$ term due to $\\gamma>0$ and thus violate the bound. Therefore we must have\n$$\n\\frac{K+2}{m} - \\frac{1}{c} < 0 \\quad \\Longleftrightarrow \\quad m > c\\,(K+2).\n$$\nBecause $m$ must be a positive integer, the smallest such $m$ is the least integer strictly greater than $c\\,(K+2)$, namely\n$$\nm_{\\min}=\\left\\lfloor c\\,(K+2)\\right\\rfloor + 1.\n$$\nThis condition guarantees that the $n^{3}$ coefficient on the left is strictly smaller than that on the right, so the negative $n^{3}$ gap eventually dominates the positive $n^{2}$ terms (including the compression cost), ensuring $T'(n)\\le \\frac{1}{c}T(n)$ for all sufficiently large $n$.", "answer": "$$\\boxed{\\left\\lfloor c\\,(K+2)\\right\\rfloor + 1}$$", "id": "1430455"}, {"introduction": "While the Linear Speedup Theorem promises faster runtimes, this gain comes at a significant \"hidden\" cost: an explosion in the complexity of the machine itself. This problem [@problem_id:1430470] makes this trade-off explicit by asking you to quantify how the alphabet size and the number of states grow with iterative applications of the speedup procedure. By deriving the relationship between these two complexities, you will uncover why the theorem is a profound statement about the structure of complexity classes, rather than a blueprint for practical, everyday optimization.", "problem": "A team of theoretical computer scientists is analyzing the trade-offs involved in iteratively applying optimization techniques to computational models. They start with a single-tape Turing Machine (TM), denoted $M_0$, which has an initial set of states $Q_0$ and a tape alphabet $\\Gamma_0$. The initial number of states is $|Q_0| = s$ and the initial alphabet size is $|\\Gamma_0| = a$, where $a \\geq 2$ and $s \\geq 1$.\n\nThey generate a sequence of increasingly faster TMs, $M_1, M_2, \\ldots, M_N$, by repeatedly applying a specific procedure based on the linear speedup theorem. The transformation from machine $M_{k-1}$ to $M_k$ for $k \\in \\{1, 2, \\ldots, N\\}$ is governed by a constant symbol-compression factor, an integer $m > 1$. This transformation follows two rules:\n\n1.  **Alphabet Expansion**: The tape alphabet $\\Gamma_k$ of machine $M_k$ is constructed by grouping symbols from $\\Gamma_{k-1}$ into non-overlapping blocks of length $m$. Thus, a single symbol of $M_k$ represents a sequence of $m$ symbols of $M_{k-1}$.\n2.  **State Complexity Growth**: In one computational step, machine $M_k$ simulates the behavior of machine $M_{k-1}$. To achieve this, the finite control of $M_k$ must store not only the current state of $M_{k-1}$ but also the local tape configuration of $M_{k-1}$. The construction protocol specifies that a state in $Q_k$ must encode the current state from $Q_{k-1}$ and the contents of a tape segment of $M_{k-1}$ of length $3m$ symbols (the $m$ symbols corresponding to the current position of $M_k$'s head, plus the $m$ symbols in the block to the immediate left and the $m$ symbols in the block to the immediate right).\n\nAfter $N$ complete iterations of this procedure, we have the final machine $M_N$ with state set $Q_N$ and alphabet $\\Gamma_N$.\n\nDetermine the value of the ratio $\\frac{\\ln(|Q_N|)}{\\ln(|\\Gamma_N|)}$, where $\\ln$ denotes the natural logarithm. Your answer should be a closed-form expression in terms of the initial parameters $s, a, m$, and the number of iterations $N$.", "solution": "Let $g_{k} = |\\Gamma_{k}|$ and $q_{k} = |Q_{k}|$. By the alphabet expansion rule, each symbol of $M_{k}$ encodes a block of $m$ symbols of $M_{k-1}$, so the alphabet sizes satisfy\n$$\ng_{k} = g_{k-1}^{m}, \\quad g_{0} = a.\n$$\nSolving this recursion by repeated substitution gives\n$$\ng_{k} = a^{m^{k}},\n$$\nand in particular\n$$\n|\\Gamma_{N}| = g_{N} = a^{m^{N}}.\n$$\nBy the state complexity rule, a state of $M_{k}$ must encode a state of $M_{k-1}$ together with the contents of a length-$3m$ segment over $\\Gamma_{k-1}$, hence\n$$\nq_{k} = q_{k-1}\\, g_{k-1}^{3m}, \\quad q_{0} = s.\n$$\nUsing $g_{k-1} = a^{m^{k-1}}$, this becomes\n$$\nq_{k} = q_{k-1}\\, a^{3m \\cdot m^{k-1}} = q_{k-1}\\, a^{3 m^{k}}.\n$$\nUnrolling the product yields\n$$\nq_{N} = s \\prod_{i=1}^{N} a^{3 m^{i}} = s\\, a^{3 \\sum_{i=1}^{N} m^{i}}.\n$$\nThe sum is a geometric series:\n$$\n\\sum_{i=1}^{N} m^{i} = \\frac{m(m^{N} - 1)}{m - 1}.\n$$\nTherefore,\n$$\n|Q_{N}| = q_{N} = s\\, a^{\\frac{3m(m^{N}-1)}{m-1}}.\n$$\nWe now compute the ratio of logarithms. Using $\\ln(xy) = \\ln x + \\ln y$ and $\\ln(a^{b}) = b \\ln a$,\n$$\n\\ln(|Q_{N}|) = \\ln(s) + \\frac{3m(m^{N}-1)}{m-1}\\, \\ln(a), \\qquad \\ln(|\\Gamma_{N}|) = m^{N} \\ln(a).\n$$\nHence,\n$$\n\\frac{\\ln(|Q_{N}|)}{\\ln(|\\Gamma_{N}|)} = \\frac{\\ln(s)}{m^{N}\\ln(a)} + \\frac{3m(m^{N}-1)}{(m-1)m^{N}}.\n$$\nThis simplifies to\n$$\n\\frac{\\ln(|Q_{N}|)}{\\ln(|\\Gamma_{N}|)} = \\frac{\\ln(s)}{m^{N}\\ln(a)} + \\frac{3\\bigl(m - m^{1-N}\\bigr)}{m - 1}.\n$$\nThis is the desired closed-form expression in terms of $s, a, m$, and $N$.", "answer": "$$\\boxed{\\frac{\\ln(s)}{m^{N}\\ln(a)}+\\frac{3\\left(m-m^{1-N}\\right)}{m-1}}$$", "id": "1430470"}]}