{"hands_on_practices": [{"introduction": "The All-Pairs Shortest Path (APSP) hypothesis suggests that finding shortest paths in a general weighted graph is a fundamentally cubic-time problem. However, fine-grained analysis demands that we question the scope of such broad claims. This first exercise challenges you to explore why this powerful hypothesis breaks down for the seemingly simpler case of unweighted graphs, revealing a deep connection between graph theory and fast matrix multiplication. [@problem_id:1424347]", "problem": "In computational complexity theory, the All-Pairs Shortest Path (APSP) problem asks for the shortest path distance between every pair of vertices in a given graph. A major open question in fine-grained complexity is whether the standard cubic-time algorithms for APSP, such as the Floyd-Warshall algorithm, are optimal. The APSP hypothesis conjectures that for a directed graph with $n$ vertices and arbitrary real-valued edge weights, any algorithm for solving APSP requires $\\Omega(n^{3-\\delta})$ time for any $\\delta  0$, meaning no truly sub-cubic time algorithm (e.g., $O(n^{2.99})$) exists.\n\nHowever, this conditional lower bound does not apply to the specific case of *unweighted* directed graphs. For this special case, there exist algorithms that provably run in truly sub-cubic time. Which of the following statements provides the most accurate and fundamental reason for why this is true?\n\nA. The problem can be solved by running Breadth-First Search (BFS) from every vertex. Since BFS is a linear-time algorithm, the total time is significantly less than cubic.\n\nB. The problem of finding shortest paths in an unweighted graph can be reduced to a series of matrix multiplications, which can be solved in truly sub-cubic time using advanced algorithms.\n\nC. For unweighted graphs, the APSP problem becomes equivalent to finding connected components, which can be solved in nearly linear time.\n\nD. The APSP hypothesis only applies to graphs containing negative-weight cycles, a condition that is impossible in unweighted graphs.\n\nE. All unweighted directed graphs are also Directed Acyclic Graphs (DAGs), and APSP can be solved more efficiently on DAGs.", "solution": "The core of this question is to understand the algorithmic difference between solving the All-Pairs Shortest Path (APSP) problem on weighted versus unweighted graphs. The standard APSP hypothesis, which posits a near-cubic time lower bound, is built upon the difficulty of handling arbitrary real-valued weights. We need to find the specific property of unweighted graphs that allows this cubic barrier to be broken.\n\nLet's analyze the problem for an unweighted directed graph with $n$ vertices. The shortest path between two vertices, say $u$ and $v$, is simply the minimum number of edges on a path from $u$ to $v$.\n\nThe key insight lies in viewing graph reachability through the lens of linear algebra. Let $A$ be the adjacency matrix of the graph, where $A_{ij} = 1$ if there is an edge from vertex $i$ to vertex $j$, and $A_{ij} = 0$ otherwise. For the purpose of finding shortest paths, let's also set $A_{ii} = 1$ for all $i$, representing a path of length 0 from a vertex to itself.\n\nNow, consider the matrix product $A^2 = A \\times A$, calculated using standard matrix multiplication. An entry $(A^2)_{ij}$ is given by $\\sum_{k=1}^n A_{ik} A_{kj}$. This sum will be greater than zero if and only if there exists at least one vertex $k$ such that there is an edge from $i$ to $k$ and an edge from $k$ to $j$. This corresponds to a path of length 2 from $i$ to $j$. More generally, the entry $(A^k)_{ij}$ in the matrix $A^k$ is non-zero if and only if there is a path of length at most $k$ from vertex $i$ to vertex $j$. (This is true if we use Boolean matrix multiplication; with standard integer multiplication, it counts the number of such paths).\n\nThe shortest path distance from $i$ to $j$, denoted $d(i, j)$, is therefore the smallest integer $k$ such that $(A^k)_{ij}  0$. Since any simple path in the graph can have a length of at most $n-1$, we only need to check powers of $A$ up to $A^{n-1}$.\n\nA naive approach would be to compute $A^2, A^3, \\dots, A^{n-1}$ by repeatedly multiplying by $A$. This would take about $n-2$ matrix multiplications. However, we can be much more efficient using the technique of repeated squaring. We can compute the sequence of matrices $A^2, A^4, A^8, \\dots, A^{2^{\\lceil \\log_2 n \\rceil}}$. This requires only $O(\\log n)$ matrix multiplications. By combining these matrix powers, we can determine the shortest path distances. For instance, the matrix of paths of length at most $k$ can be formed by combining the appropriate powers of $A$ that sum to $k$ in binary. A more sophisticated method (Zwick's algorithm) uses these matrix multiplication ideas to solve APSP.\n\nThe crucial point is the complexity of matrix multiplication. While the standard high-school algorithm takes $O(n^3)$ time to multiply two $n \\times n$ matrices, more advanced algorithms exist that are faster. The first such algorithm was Strassen's algorithm, with a complexity of approximately $O(n^{2.81})$. The current record for the matrix multiplication exponent, denoted by $\\omega$, is approximately $\\omega \\approx 2.37286$.\n\nTherefore, solving APSP on an unweighted graph can be done in $O(n^\\omega \\log n)$ time. Since $\\omega  3$, this complexity is truly sub-cubic. For example, using Strassen's algorithm, the runtime is $O(n^{2.81} \\log n)$, which is faster than $O(n^3)$. This breaks the cubic barrier and shows why the APSP hypothesis, formulated for general weighted graphs, does not apply to the unweighted case.\n\nNow let's evaluate the given options:\n\nA. Running BFS from every vertex is a valid algorithm. A single BFS takes $O(V+E) = O(n+m)$ time, where $m$ is the number of edges. Repeating for all $n$ vertices gives a total complexity of $O(n(n+m))$. For a dense graph where $m = O(n^2)$, this becomes $O(n^3)$, which is not a truly sub-cubic algorithm. For sparse graphs, it can be better (e.g., $O(n^2)$ if $m=O(n)$), but it doesn't provide a sub-cubic algorithm for all unweighted graphs, especially dense ones. So this is not the best explanation for a general sub-cubic breakthrough.\n\nB. This statement correctly identifies that the problem's reduction to fast matrix multiplication is the fundamental reason for the existence of a truly sub-cubic algorithm. As explained above, this approach yields a time complexity of $O(n^\\omega \\log n)$, which is sub-cubic for dense graphs as well.\n\nC. This is incorrect. APSP is about finding distances between all pairs, not just determining if they are in the same connected component. Finding connected components is a much simpler problem.\n\nD. This is incorrect. The APSP hypothesis is typically stated for general real-valued weights and does not require negative-weight cycles. The main challenge comes from handling the additive structure of path weights, which the matrix multiplication trick for unweighted graphs bypasses. Problems on graphs with only non-negative weights are still believed to require near-cubic time in the general case.\n\nE. This is incorrect. An unweighted directed graph can have cycles. A simple directed edge from $i$ to $j$ and another from $j$ to $i$ forms a cycle of length 2. Only if the graph is a DAG can we use topological sorting to solve APSP in $O(n(n+m))$ time, but the unweighted property alone does not guarantee the graph is a DAG.\n\nTherefore, the most accurate and fundamental reason is the reduction to fast matrix multiplication.", "answer": "$$\\boxed{B}$$", "id": "1424347"}, {"introduction": "A central technique in complexity theory is the reduction, which translates one problem into another to transfer hardness results. This practice focuses on reducing a problem of finding disjoint sets—a common task in areas like bioinformatics—to the Orthogonal Vectors (OV) problem, whose hardness is conjectured under the Strong Exponential Time Hypothesis (SETH). Your task is to find the correct translation, a process that is fundamental to proving conditional lower bounds. [@problem_id:1424387]", "problem": "In the field of bioinformatics, a common task is to analyze relationships between different biological samples (e.g., cell types, tissue samples). A researcher is studying a collection of $n$ distinct samples. For each sample, they have identified the set of genes that are \"active\" from a large universal set of $d$ possible genes, denoted $U = \\{g_1, g_2, \\dots, g_d\\}$. Let the set of active genes for sample $i$ be $S_i \\subseteq U$. The researcher assumes that every sample has at least one active gene, i.e., $S_i$ is non-empty for all $i=1, \\dots, n$.\n\nThe researcher is interested in finding if there exists any pair of samples $(i, j)$ with $i \\neq j$ that are \"genetically incompatible,\" meaning their sets of active genes are disjoint ($S_i \\cap S_j = \\emptyset$). To solve this efficiently, they decide to model it as an instance of the Orthogonal Vectors (OV) problem, a well-known computational problem for which specialized algorithms exist.\n\nThe Orthogonal Vectors (OV) problem is defined as follows: Given two sets of vectors, $A = \\{a_1, \\dots, a_m\\}$ and $B = \\{b_1, \\dots, b_p\\}$, where each vector is in $\\{0, 1\\}^d$, determine if there exist indices $i$ and $j$ such that the dot product $a_i \\cdot b_j = 0$. The dot product is over the integers, i.e., $a_i \\cdot b_j = \\sum_{k=1}^d a_{ik} b_{jk}$.\n\nWhich of the following procedures describes a correct and complete reduction from the problem of finding a genetically incompatible pair of samples to the Orthogonal Vectors problem?\n\nA. For each set $S_i$, construct a vector $v_i \\in \\{0, 1\\}^d$ where the $k$-th component is 1 if gene $g_k \\in S_i$, and 0 otherwise. Set both input sets for the OV problem to be this collection of vectors, i.e., $A=B=\\{v_1, \\dots, v_n\\}$. A 'yes' answer to the OV problem implies the existence of a genetically incompatible pair.\n\nB. For each set $S_i$, construct a vector $v_i \\in \\{0, 1\\}^d$ where the $k$-th component is 0 if gene $g_k \\in S_i$, and 1 otherwise. Set both input sets for the OV problem to be this collection, i.e., $A=B=\\{v_1, \\dots, v_n\\}$.\n\nC. Assume $n$ is even. Divide the samples into two groups: $G_1 = \\{S_1, \\dots, S_{n/2}\\}$ and $G_2 = \\{S_{n/2+1}, \\dots, S_n\\}$. For each set $S_i$, construct its characteristic vector $v_i$ (where the $k$-th component is 1 if $g_k \\in S_i$, and 0 otherwise). Set the OV problem inputs as $A=\\{v_1, \\dots, v_{n/2}\\}$ and $B=\\{v_{n/2+1}, \\dots, v_n\\}$.\n\nD. For each gene $g_k \\in U$, construct a vector $u_k \\in \\{0, 1\\}^n$ where the $i$-th component is 1 if gene $g_k$ is in set $S_i$, and 0 otherwise. This creates $d$ vectors of dimension $n$. Set both input sets for the OV problem to be this collection, i.e., $A=B=\\{u_1, \\dots, u_d\\}$.", "solution": "We are given $n$ nonempty sets $S_{1},\\dots,S_{n} \\subseteq U=\\{g_{1},\\dots,g_{d}\\}$ and wish to determine whether there exist $i \\neq j$ such that $S_{i} \\cap S_{j} = \\emptyset$. The Orthogonal Vectors (OV) problem takes two sets $A,B \\subseteq \\{0,1\\}^{d}$ and asks whether there exist $a \\in A$ and $b \\in B$ with $a \\cdot b = 0$, where $a \\cdot b = \\sum_{k=1}^{d} a_{k} b_{k}$.\n\nTo reduce the disjointness query to OV, consider the characteristic vector mapping: for each $S_{i}$, define $v_{i} \\in \\{0,1\\}^{d}$ by $v_{i,k} = 1$ if $g_{k} \\in S_{i}$ and $v_{i,k} = 0$ otherwise. Then for any $i,j$,\n$$\nv_{i} \\cdot v_{j} \\;=\\; \\sum_{k=1}^{d} \\mathbf{1}[g_{k} \\in S_{i}] \\,\\mathbf{1}[g_{k} \\in S_{j}] \\;=\\; |S_{i} \\cap S_{j}|.\n$$\nTherefore,\n$$\nS_{i} \\cap S_{j} = \\emptyset \\quad \\Longleftrightarrow \\quad v_{i} \\cdot v_{j} = 0.\n$$\nIf we set $A=B=\\{v_{1},\\dots,v_{n}\\}$, any OV “yes” instance exhibits $i,j$ with $v_{i} \\cdot v_{j} = 0$, which by the equivalence gives $S_{i} \\cap S_{j} = \\emptyset$. The possibility $i=j$ does not cause false positives because $S_{i} \\neq \\emptyset$ implies $v_{i} \\cdot v_{i} = |S_{i}| \\geq 1$, hence $v_{i} \\cdot v_{i} \\neq 0$. Conversely, if there exists $i \\neq j$ with $S_{i} \\cap S_{j} = \\emptyset$, then $v_{i} \\cdot v_{j} = 0$, and the OV instance on $A=B$ answers “yes.” This establishes both soundness and completeness of procedure A.\n\nNow evaluate the other options:\n\nFor B, the construction uses complement indicators: $v_{i,k}=0$ if $g_{k} \\in S_{i}$ and $v_{i,k}=1$ otherwise. Then\n$$\nv_{i} \\cdot v_{j} \\;=\\; \\sum_{k=1}^{d} \\mathbf{1}[g_{k} \\notin S_{i}] \\,\\mathbf{1}[g_{k} \\notin S_{j}] \\;=\\; |U \\setminus (S_{i} \\cup S_{j})|.\n$$\nThus $v_{i} \\cdot v_{j} = 0$ if and only if $S_{i} \\cup S_{j} = U$, which is not equivalent to $S_{i} \\cap S_{j} = \\emptyset$. Hence B is incorrect.\n\nFor C, splitting samples into two arbitrary halves and setting $A$ to the first half and $B$ to the second half only detects cross-group disjoint pairs. If a disjoint pair lies entirely within one half, it will be missed. Therefore C is incomplete.\n\nFor D, constructing vectors $u_{k} \\in \\{0,1\\}^{n}$ indexed by genes yields $u_{k} \\cdot u_{\\ell}$ equal to the number of samples where both $g_{k}$ and $g_{\\ell}$ are active. Orthogonality then concerns pairs of genes never co-active in any sample, which does not answer whether there exist two samples with disjoint active-gene sets. Hence D does not reduce the intended problem.\n\nTherefore, only procedure A correctly and completely reduces the genetically incompatible pair problem to OV.", "answer": "$$\\boxed{A}$$", "id": "1424387"}, {"introduction": "The 3SUM hypothesis provides the foundation for the conjectured hardness of numerous problems in computational geometry and data structures. This exercise demonstrates how to 'propagate' this hardness to a new problem: finding a three-term arithmetic progression (AP3) within a set of numbers. Successfully tackling this problem requires constructing a creative reduction that transforms an instance of 3SUM into an instance of AP3, thereby showing that a fast algorithm for AP3 would imply a breakthrough for 3SUM. [@problem_id:1424351]", "problem": "In computational complexity theory, the 3SUM problem serves as a foundational hard problem for a class of problems in computational geometry and beyond. Fine-grained complexity aims to understand the precise polynomial time complexity of such problems.\n\nThe **3SUM problem** is defined as follows: given a set $S$ of $n$ integers, determine if there exist three elements $x, y, z \\in S$ such that $x+y+z=0$. The widely believed **3SUM Hypothesis** states that any algorithm for the 3SUM problem requires $\\Omega(n^2)$ time in the worst case. This hypothesis is often extended to the **3-list 3SUM variant**: given three sets of integers $X, Y, Z$, each of size $n$, determine if there exist elements $x \\in X, y \\in Y, z \\in Z$ such that $x+y+z=0$. This variant is also conjectured to require $\\Omega(n^2)$ time.\n\nConsider a related problem, which we will call the **Arithmetic Progression-3 (AP3) problem**: given a set $A$ of $m$ integers, determine if there exist three *distinct* elements $a, b, c \\in A$ that form an arithmetic progression. The condition for three numbers to form an arithmetic progression is that when sorted, the middle term is the average of the other two. This is equivalent to checking if there exists a permutation $(p, q, r)$ of the three distinct elements $(a,b,c)$ such that $p+r=2q$.\n\nAssuming the 3-list 3SUM hypothesis is true, what is the tightest possible worst-case time complexity lower bound for solving the AP3 problem on a set of size $m$?\n\nA. $\\Omega(m \\log m)$\n\nB. $\\Omega(m^{1.5})$\n\nC. $\\Omega(m^2)$\n\nD. $\\Omega(m^2 \\log m)$\n\nE. The problem is solvable in polynomial time, but the 3SUM hypothesis does not imply a quadratic lower bound.", "solution": "We reduce 3-list 3SUM to AP3 with only linear blowup in the input size and constant-factor overhead, which implies the claimed lower bound under the 3-list 3SUM hypothesis.\n\nStart with an arbitrary instance of 3-list 3SUM: three sets of integers $X, Y, Z$, each of size $n$, and the question is whether there exist $x \\in X$, $y \\in Y$, $z \\in Z$ such that\n$$\nx + y + z = 0.\n$$\n\nWe construct in linear time an instance of AP3 on a single set $A$ of size $m = \\Theta(n)$ such that $A$ contains a 3-term arithmetic progression if and only if the original instance of 3-list 3SUM is a YES-instance. Introduce a symbolic parameter $T$ that is larger in absolute value than any fixed finite linear combination of input values (such a choice exists since the input is finite). Define three disjoint “layers”\n$$\nB_{0} := \\{\\, 2x : x \\in X \\,\\}, \\quad B_{1} := \\{\\, 2T - y : y \\in Y \\,\\}, \\quad B_{2} := \\{\\, 4T + 2z : z \\in Z \\,\\},\n$$\nand set\n$$\nA := B_{0} \\cup B_{1} \\cup B_{2}.\n$$\nThis construction is linear-time and $|A| = |B_{0}| + |B_{1}| + |B_{2}| = 3n = \\Theta(n)$, hence $m = \\Theta(n)$.\n\nCompleteness: If there is a solution $(x,y,z)$ with $x + y + z = 0$, pick $a = 2x \\in B_{0}$, $b = 2T - y \\in B_{1}$, $c = 4T + 2z \\in B_{2}$. Then\n$$\na + c = 2x + (4T + 2z) = 4T + 2(x + z) = 4T - 2y = 2(2T - y) = 2b,\n$$\nso $(a,b,c)$ form a 3-term arithmetic progression in $A$.\n\nSoundness and separation of layers: Because elements in $B_{0}$ are $\\Theta(1)$ in magnitude, in $B_{1}$ are centered around $2T$, and in $B_{2}$ are centered around $4T$, and because $T$ is chosen to dominate any fixed linear combination of input values, any 3-term arithmetic progression that mixes elements from different layers must have its smallest element from $B_{0}$, its middle element from $B_{1}$, and its largest element from $B_{2}$. Moreover, for such a triple $(a,b,c) \\in B_{0} \\times B_{1} \\times B_{2}$, the arithmetic-progression condition forces\n$$\na + c = 2b \\;\\;\\Longleftrightarrow\\;\\; 2x + (4T + 2z) = 2(2T - y) \\;\\;\\Longleftrightarrow\\;\\; x + y + z = 0.\n$$\nThus, $A$ contains a cross-layer 3-term arithmetic progression if and only if the original 3-list 3SUM instance is a YES-instance.\n\nThis reduction is linear time and linear size. Therefore, if AP3 on $m$ numbers could be solved in $o(m^{2})$ time, we could solve 3-list 3SUM on $n = \\Theta(m)$ inputs in $o(n^{2})$ time, contradicting the 3-list 3SUM hypothesis. Since the hypothesis asserts a quadratic lower bound (without any superlinear logarithmic factor), the tightest lower bound that follows is\n$$\n\\Omega(m^{2}).\n$$\n\nAmong the given options, this corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1424351"}]}