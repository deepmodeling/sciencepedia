{"hands_on_practices": [{"introduction": "We begin our hands-on journey with a foundational problem in probability and computer science: the Coupon Collector's Problem. This exercise, set in the context of a hypothetical game, demonstrates how to analyze the expected time to completion for a randomized process by breaking it down into a series of simpler steps. Mastering the analysis in this problem [@problem_id:1441266] is crucial, as it introduces the powerful tool of linearity of expectation and its application to sequences of geometric random variables, a pattern that appears frequently in the analysis of randomized algorithms.", "problem": "In the popular online game \"Galaxy of Heroes,\" players can acquire new characters by obtaining \"Hero Shards\" from randomized \"Chrono-Capsules.\" A new character is unlocked only when a player has collected one of each of the distinct shard types associated with that character.\n\nConsider the character \"Zeta Prime,\" who requires a complete set of 5 distinct types of Hero Shards to be unlocked. Each Chrono-Capsule guarantees exactly one Hero Shard for Zeta Prime. The type of shard obtained from a capsule is chosen uniformly at random from the 5 possible types, and each acquisition is an independent event.\n\nWhat is the expected number of Chrono-Capsules a player must open to collect at least one of each of the 5 distinct Hero Shard types for Zeta Prime? Your answer should be an exact value.", "solution": "Let $T$ be the total number of Chrono-Capsules needed to obtain all $5$ distinct shard types. Decompose $T$ as a sum of waiting times:\n$$\nT=\\sum_{k=0}^{4} X_{k},\n$$\nwhere $X_{k}$ is the number of additional capsules needed to obtain a new shard type given that $k$ distinct types have already been collected.\n\nWhen $k$ distinct types are collected, the probability that the next capsule yields a new type is\n$$\np_{k}=\\frac{5-k}{5},\n$$\nsince there are $5-k$ unseen types out of $5$ equally likely types. Because each capsule result is independent and $p_{k}$ remains constant while $k$ is fixed, $X_{k}$ has a geometric distribution (counting the number of trials until the first success), with expectation\n$$\n\\mathbb{E}[X_{k}]=\\frac{1}{p_{k}}=\\frac{5}{5-k}.\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[T]=\\sum_{k=0}^{4} \\mathbb{E}[X_{k}]=\\sum_{k=0}^{4} \\frac{5}{5-k}=5 \\sum_{j=1}^{5} \\frac{1}{j}=5 H_{5},\n$$\nwhere $H_{5}$ is the fifth harmonic number. Compute $H_{5}$ exactly:\n$$\nH_{5}=1+\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}+\\frac{1}{5}=\\frac{60}{60}+\\frac{30}{60}+\\frac{20}{60}+\\frac{15}{60}+\\frac{12}{60}=\\frac{137}{60}.\n$$\nTherefore,\n$$\n\\mathbb{E}[T]=5 \\cdot \\frac{137}{60}=\\frac{685}{60}=\\frac{137}{12}.\n$$", "answer": "$$\\boxed{\\frac{137}{12}}$$", "id": "1441266"}, {"introduction": "Building on the concept of expectation, we now tackle a core challenge in computational complexity: the Maximum 2-Satisfiability (MAX-2-SAT) problem. This practice illustrates the power of the probabilistic method, where an extremely simple randomized strategy can yield a surprisingly strong and guaranteed performance baseline for an NP-hard problem. By calculating the expected fraction of satisfied clauses [@problem_id:1441265], you will gain direct insight into how randomization serves as a potent tool for designing and analyzing approximation algorithms.", "problem": "In the field of computational complexity, the Maximum 2-Satisfiability (MAX-2-SAT) problem is a fundamental optimization challenge. An instance of this problem is defined by a set of $n$ Boolean variables, $V = \\{x_1, x_2, \\dots, x_n\\}$, and a collection of $m$ clauses. Each clause is a disjunction (an OR operation) of exactly two literals. A literal is either a variable $x_i$ or its negation $\\neg x_i$. For any given clause, assume its two literals involve distinct variables (e.g., clauses of the form $x_i \\lor \\neg x_i$ or $x_i \\lor x_i$ are excluded). The goal of MAX-2-SAT is to find a truth assignment—a mapping of each variable to TRUE or FALSE—that maximizes the total number of satisfied clauses.\n\nConsider the simplest randomized algorithm for approximating a solution: each variable $x_i \\in V$ is independently assigned a value of TRUE with probability $1/2$ and FALSE with probability $1/2$.\n\nFor an arbitrary MAX-2-SAT instance with $m > 0$ clauses satisfying the stated conditions, what is the expected fraction of clauses that will be satisfied by applying this randomized assignment once? Express your answer as a single, closed-form analytic expression.", "solution": "Let a clause be $C=(\\ell_{i} \\lor \\ell_{j})$ where $\\ell_{i}$ is a literal on variable $x_{i}$ and $\\ell_{j}$ is a literal on variable $x_{j}$ with $i \\neq j$. Under the randomized assignment where each $x_{k}$ is independently TRUE with probability $\\frac{1}{2}$ and FALSE with probability $\\frac{1}{2}$, any literal (whether $x_{k}$ or $\\neg x_{k}$) is TRUE with probability $\\frac{1}{2}$ and FALSE with probability $\\frac{1}{2}$. Since the literals involve distinct variables and assignments are independent, the events that $\\ell_{i}$ is FALSE and that $\\ell_{j}$ is FALSE are independent. Therefore,\n$$\nP(\\text{$C$ is unsatisfied}) = P(\\ell_{i}\\ \\text{FALSE and}\\ \\ell_{j}\\ \\text{FALSE}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}.\n$$\nHence,\n$$\nP(\\text{$C$ is satisfied}) = 1 - \\frac{1}{4} = \\frac{3}{4}.\n$$\nLet $X_{k}$ be the indicator that the $k$-th clause is satisfied. Then $E[X_{k}] = \\frac{3}{4}$ for all $k$, and by linearity of expectation the expected number of satisfied clauses is\n$$\nE\\Big[\\sum_{k=1}^{m} X_{k}\\Big] = \\sum_{k=1}^{m} E[X_{k}] = m \\cdot \\frac{3}{4}.\n$$\nThe expected fraction of satisfied clauses is\n$$\nE\\left[\\frac{1}{m} \\sum_{k=1}^{m} X_{k}\\right] = \\frac{1}{m} \\cdot m \\cdot \\frac{3}{4} = \\frac{3}{4},\n$$\nwhich is independent of the specific instance, provided $m>0$.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1441265"}, {"introduction": "Our final practice explores a sophisticated application of randomization within the modern domain of big data and streaming algorithms. This problem introduces a probabilistic counter, a clever device for estimating large counts with very little memory, framed here as a hypothetical scenario. The analysis requires more than a direct application of expectation; it challenges you to discover a creative transformation of the system's state to make the problem tractable. By proving that the proposed estimator is unbiased [@problem_id:1441272], you will appreciate how elegant mathematical techniques enable efficient computation under significant resource constraints.", "problem": "In the analysis of large-scale data streams, it is often necessary to count a vast number of events using limited memory. One technique is to use a probabilistic counter. Consider such a counter whose state is represented by an integer value $C$. The counter is initialized to $C_0 = 0$. When the $k$-th signal (for $k = 1, 2, \\dots, N$) arrives, the counter's value $C_{k-1}$ is updated to $C_k$ according to the following rule: the counter increments by one (i.e., $C_k = C_{k-1} + 1$) with probability $p = q^{-C_{k-1}}$, and it remains unchanged (i.e., $C_k = C_{k-1}$) with probability $1-p$. Here, $q$ is a constant real number greater than 1.\n\nThis process is repeated for a total of $N$ incoming signals. The final state of the counter is $C_N$. An analyst proposes an estimator, $X_N$, for the true number of signals $N$. The estimator is defined as a function of the final counter value $C_N$:\n$$X_N = \\frac{q^{C_N} - 1}{q-1}$$\nDetermine the exact expected value of this estimator, $E[X_N]$, after $N$ signals have been received. Express your answer in terms of $N$ and $q$.", "solution": "We analyze the counter as a Markov process with state $C_{k}$ and transition rule: given $C_{k-1}=c$, the next state is $C_{k}=c+1$ with probability $q^{-c}$ and $C_{k}=c$ with probability $1-q^{-c}$, where $q>1$. Define the transformed process $Z_{k}=q^{C_{k}}$. We compute the conditional expectation of $Z_{k}$ given $C_{k-1}=c$:\n$$\n\\mathbb{E}\\!\\left[Z_{k}\\mid C_{k-1}=c\\right]\n= q^{-c}\\,q^{c+1} + \\left(1-q^{-c}\\right)q^{c}\n= q + q^{c} - 1\n= q^{C_{k-1}} + (q-1)\n= Z_{k-1} + (q-1).\n$$\nApplying the law of total expectation (tower property),\n$$\n\\mathbb{E}[Z_{k}] = \\mathbb{E}\\!\\left[\\mathbb{E}[Z_{k}\\mid C_{k-1}]\\right]\n= \\mathbb{E}[Z_{k-1} + (q-1)]\n= \\mathbb{E}[Z_{k-1}] + (q-1).\n$$\nWith the initial condition $C_{0}=0$, we have $Z_{0}=q^{C_{0}}=1$, hence\n$$\n\\mathbb{E}[Z_{N}] = \\mathbb{E}[Z_{0}] + N(q-1) = 1 + N(q-1).\n$$\nThe estimator is $X_{N} = \\frac{q^{C_{N}} - 1}{q-1} = \\frac{Z_{N}-1}{q-1}$. Using linearity of expectation,\n$$\n\\mathbb{E}[X_{N}] = \\frac{\\mathbb{E}[Z_{N}] - 1}{q-1} = \\frac{\\left(1 + N(q-1)\\right) - 1}{q-1} = N.\n$$\nTherefore, the estimator is exactly unbiased, with expected value equal to the true count $N$.", "answer": "$$\\boxed{N}$$", "id": "1441272"}]}