{"hands_on_practices": [{"introduction": "The power of probabilistic algorithms lies not in the correctness of a single run, but in our ability to amplify their reliability to any desired level. This first practice is a foundational exercise in understanding this amplification process. By calculating the number of repetitions needed to reduce the error of a BPP algorithm to an astronomically small value [@problem_id:1422498], you will gain a concrete appreciation for how repeated trials and majority voting can transform a moderately reliable algorithm into one with near-certain correctness.", "problem": "A research group in computational theory has designed a new probabilistic algorithm for a decision problem. This algorithm is known to belong to the complexity class BPP (Bounded-error Probabilistic Polynomial-time), which means that for any given input, it produces the correct 'yes' or 'no' answer with a probability of at least $2/3$. Consequently, the probability of an error on any single run, $\\epsilon_0$, is at most $1/3$.\n\nTo increase confidence in the result, the researchers employ an amplification strategy: they run the algorithm $k$ times independently on the same input and take the majority vote as the final answer. They want to achieve a high degree of certainty for a particularly important computation, requiring that the final probability of an incorrect majority vote is less than $2^{-100}$.\n\nFor this problem, you may use the following standard result from complexity theory, which is a consequence of the Chernoff bound: The error probability of the majority-vote procedure, $\\delta_k$, after $k$ trials is bounded by the inequality:\n$$ \\delta_k \\le \\exp\\left(-2k\\left(\\frac{1}{2} - \\epsilon_0\\right)^2\\right) $$\nwhere $\\epsilon_0$ is the error probability of a single trial, which you should take as its worst-case value.\n\nDetermine the minimum integer number of trials, $k$, required to ensure the final error probability is less than $2^{-100}$. For your calculation, use the approximation $\\ln(2) \\approx 0.69315$.", "solution": "We are given the Chernoff-type bound for the majority-vote error after $k$ independent runs:\n$$\n\\delta_{k} \\le \\exp\\left(-2k\\left(\\frac{1}{2}-\\epsilon_{0}\\right)^{2}\\right),\n$$\nand the worst-case single-run error is $\\epsilon_{0}=\\frac{1}{3}$. Substituting $\\epsilon_{0}=\\frac{1}{3}$ gives\n$$\n\\left(\\frac{1}{2}-\\epsilon_{0}\\right)=\\frac{1}{2}-\\frac{1}{3}=\\frac{1}{6},\n$$\nso\n$$\n\\delta_{k} \\le \\exp\\left(-2k\\left(\\frac{1}{6}\\right)^{2}\\right)=\\exp\\left(-\\frac{k}{18}\\right).\n$$\nTo ensure the final error probability is less than $2^{-100}$, it suffices to require\n$$\n\\exp\\left(-\\frac{k}{18}\\right)  2^{-100}.\n$$\nTaking natural logarithms of both sides yields\n$$\n-\\frac{k}{18}  -100\\,\\ln(2),\n$$\nwhich is equivalent to\n$$\n\\frac{k}{18} > 100\\,\\ln(2) \\quad \\Longrightarrow \\quad k > 1800\\,\\ln(2).\n$$\nSince $k$ must be an integer, the minimum admissible value is\n$$\nk_{\\min}=\\left\\lceil 1800\\,\\ln(2)\\right\\rceil.\n$$\nUsing the given approximation $\\ln(2)\\approx 0.69315$,\n$$\n1800\\,\\ln(2)\\approx 1800\\times 0.69315=1247.67\\ldots,\n$$\nhence\n$$\nk_{\\min}=1248.\n$$", "answer": "$$\\boxed{1248}$$", "id": "1422498"}, {"introduction": "While amplification can reduce error, it isn't without cost; each repetition takes time. This exercise explores the efficiency of amplification by examining how an algorithm's initial error rate affects the amount of work required. The relationship, governed by the Chernoff bound, is surprisingly non-linear. By comparing the total runtime needed for two algorithms with different initial reliabilities [@problem_id:1422496], you will uncover the significant extra effort required to amplify a weaker algorithm, highlighting the crucial role of the initial error term $\\epsilon$ in the efficiency of probabilistic computation.", "problem": "Consider two probabilistic algorithms, $\\mathcal{A}_1$ and $\\mathcal{A}_2$, designed to solve the same decision problem for inputs of size $n$. The runtime for a single execution of either algorithm is given by a polynomial function $T(n)$.\n\nFor any given input, algorithm $\\mathcal{A}_1$ returns the correct answer with a probability of at least $3/4$. In contrast, algorithm $\\mathcal{A}_2$ is less reliable, returning the correct answer with a probability of at least $2/3$. The error in both algorithms is two-sided, meaning they can be incorrect on both 'yes' and 'no' instances of the problem.\n\nTo improve their reliability, a standard amplification procedure is employed for both algorithms. This procedure consists of running the algorithm $k$ independent times and taking the majority vote of the outcomes as the final answer. For each algorithm, we want to determine the minimum number of repetitions, $k_1$ for $\\mathcal{A}_1$ and $k_2$ for $\\mathcal{A}_2$, required to ensure that the final error probability of the amplified algorithm is at most $2^{-n}$.\n\nLet $Time_1(n) = k_1 T(n)$ and $Time_2(n) = k_2 T(n)$ be the total time complexities of the amplified versions of $\\mathcal{A}_1$ and $\\mathcal{A}_2$, respectively. Assuming $n$ is large, determine the value of the ratio $\\frac{Time_2(n)}{Time_1(n)}$.\n\nExpress your answer as an exact fraction.", "solution": "The problem asks for the asymptotic ratio of the total runtimes, $\\frac{Time_2(n)}{Time_1(n)}$, required to amplify two probabilistic algorithms to a target error probability of $2^{-n}$. This ratio is equivalent to the ratio of the number of repetitions required for each algorithm, $\\frac{k_2}{k_1}$, since the single-run time $T(n)$ is the same for both.\n\nFirst, let's establish the error probabilities for a single run of each algorithm.\nFor algorithm $\\mathcal{A}_1$, the success probability is $p_1 = 3/4$, so the error probability is $\\epsilon_1 = 1 - p_1 = 1 - 3/4 = 1/4$.\nFor algorithm $\\mathcal{A}_2$, the success probability is $p_2 = 2/3$, so the error probability is $\\epsilon_2 = 1 - p_2 = 1 - 2/3 = 1/3$.\nBoth error probabilities satisfy $\\epsilon_i  1/2$, which is a necessary condition for amplification by majority voting to be effective.\n\nThe amplification process involves running an algorithm with error $\\epsilon$ for $k$ independent trials and taking the majority vote. Let $X_i$ be an indicator random variable for the $i$-th trial, where $X_i=1$ if the algorithm returns an incorrect answer and $X_i=0$ otherwise. We have $P(X_i=1) = \\epsilon$. The total number of incorrect answers is $S_k = \\sum_{i=1}^k X_i$. The majority vote is incorrect if more than half of the trials are incorrect, i.e., if $S_k > k/2$.\n\nTo bound the probability $P(S_k > k/2)$, we can use a version of the Chernoff bound known as Hoeffding's inequality. For a sum of $k$ independent Bernoulli variables $S_k$ with mean $E[S_k] = k\\epsilon$, the inequality states:\n$$ P(S_k \\ge (1+\\delta)E[S_k]) \\le \\exp\\left(-\\frac{\\delta^2}{3}E[S_k]\\right) $$\nA more direct and commonly used form for BPP amplification states that the probability of the majority outcome being wrong is bounded by:\n$$ P(\\text{error}) \\le \\exp(-2k(1/2 - \\epsilon)^2) $$\nWe want this final error probability to be at most $2^{-n}$. So, we set up the inequality:\n$$ \\exp(-2k(1/2 - \\epsilon)^2) \\le 2^{-n} $$\nTaking the natural logarithm of both sides:\n$$ -2k(1/2 - \\epsilon)^2 \\le n \\ln(2^{-1}) = -n \\ln(2) $$\nMultiplying by $-1$ and reversing the inequality sign:\n$$ 2k(1/2 - \\epsilon)^2 \\ge n \\ln(2) $$\nSolving for $k$, we find the minimum number of repetitions required:\n$$ k \\ge \\frac{n \\ln(2)}{2(1/2 - \\epsilon)^2} $$\nThis can be simplified:\n$$ k \\ge \\frac{n \\ln(2)}{2\\left(\\frac{1-2\\epsilon}{2}\\right)^2} = \\frac{n \\ln(2)}{2 \\frac{(1-2\\epsilon)^2}{4}} = \\frac{2n \\ln(2)}{(1-2\\epsilon)^2} $$\nFor large $n$, we can approximate the minimum required repetitions $k$ by the R.H.S value, as the ceiling function's effect on the ratio becomes negligible.\n$$ k \\approx \\frac{2n \\ln(2)}{(1-2\\epsilon)^2} $$\n\nNow, we calculate the required number of repetitions for each algorithm.\n\nFor algorithm $\\mathcal{A}_1$, with $\\epsilon_1 = 1/4$:\n$$ k_1 \\approx \\frac{2n \\ln(2)}{(1 - 2(1/4))^2} = \\frac{2n \\ln(2)}{(1 - 1/2)^2} = \\frac{2n \\ln(2)}{(1/2)^2} = \\frac{2n \\ln(2)}{1/4} = 8n \\ln(2) $$\n\nFor algorithm $\\mathcal{A}_2$, with $\\epsilon_2 = 1/3$:\n$$ k_2 \\approx \\frac{2n \\ln(2)}{(1 - 2(1/3))^2} = \\frac{2n \\ln(2)}{(1 - 2/3)^2} = \\frac{2n \\ln(2)}{(1/3)^2} = \\frac{2n \\ln(2)}{1/9} = 18n \\ln(2) $$\n\nFinally, we compute the ratio of the total runtimes.\n$$ \\frac{Time_2(n)}{Time_1(n)} = \\frac{k_2 T(n)}{k_1 T(n)} = \\frac{k_2}{k_1} $$\nSubstituting the expressions for $k_1$ and $k_2$:\n$$ \\frac{k_2}{k_1} \\approx \\frac{18n \\ln(2)}{8n \\ln(2)} = \\frac{18}{8} = \\frac{9}{4} $$\nThe ratio is constant and does not depend on $n$ or $T(n)$.", "answer": "$$\\boxed{\\frac{9}{4}}$$", "id": "1422496"}, {"introduction": "Theoretical concepts find their true value when applied to solve practical problems, which often involve real-world constraints like time and money. This final practice situates the concept of BPP amplification within a realistic decision-making context. You will be tasked with choosing between two algorithms, each with its own reliability and cost-per-run, to find the most economical way to meet a stringent reliability target [@problem_id:1422544]. This problem challenges you to move beyond simple calculation and use your understanding of error reduction to make an optimized, cost-effective decision.", "problem": "A cybersecurity firm has access to two different probabilistic algorithms, Algorithm A and Algorithm B, for testing the primality of large numbers used in encryption keys. Both algorithms can be amplified by running them multiple times and taking a majority vote.\n\n- Algorithm A has an initial success probability of $p_A = 0.52$ and costs $c_A = 1.00$ dollar per run.\n- Algorithm B has an initial success probability of $p_B = 0.51$ and costs $c_B = 0.50$ dollars per run.\n\nThe reliability of an amplified algorithm after $k$ independent runs is determined by its failure probability, $P_{\\text{fail}}(k)$. For a given initial success probability $p = 0.5 + \\epsilon$, an upper bound for this failure probability is given by:\n$$P_{\\text{fail}}(k) \\le \\exp(-2k\\epsilon^2)$$\n\nThe firm needs to select one of the algorithms and determine the number of runs required to achieve a success probability of at least $99.99\\%$, while minimizing the total financial cost. What is this minimum possible total cost?\n\nExpress your answer in dollars, rounded to two significant figures.", "solution": "We are given two algorithms with initial success probabilities $p_{A}=0.52$ and $p_{B}=0.51$, so we write $p=0.5+\\epsilon$ with $\\epsilon_{A}=0.02$ and $\\epsilon_{B}=0.01$. After $k$ independent runs with majority vote, the failure probability is bounded by\n$$\nP_{\\text{fail}}(k)\\leq \\exp\\big(-2k\\epsilon^{2}\\big).\n$$\nTo guarantee a success probability at least $0.9999$, we require $P_{\\text{fail}}(k)\\leq 0.0001=10^{-4}$. Thus, for a given $\\epsilon$,\n$$\n\\exp\\big(-2k\\epsilon^{2}\\big)\\leq 10^{-4}.\n$$\nTaking natural logarithms and solving for $k$,\n$$\n-2k\\epsilon^{2}\\leq \\ln\\big(10^{-4}\\big)=-4\\ln(10)\n\\quad\\Longrightarrow\\quad\nk\\geq \\frac{\\ln\\big(10^{4}\\big)}{2\\epsilon^{2}}=\\frac{4\\ln(10)}{2\\epsilon^{2}}=\\frac{2\\ln(10)}{\\epsilon^{2}}.\n$$\nTherefore, the minimal integer number of runs required is\n$$\nk^{\\ast}(\\epsilon)=\\left\\lceil \\frac{2\\ln(10)}{\\epsilon^{2}}\\right\\rceil.\n$$\nFor Algorithm A with $\\epsilon_{A}=0.02$,\n$$\nk_{A}^{\\ast}=\\left\\lceil \\frac{2\\ln(10)}{(0.02)^{2}}\\right\\rceil\n=\\left\\lceil \\frac{2\\ln(10)}{0.0004}\\right\\rceil\n=\\left\\lceil 5000\\ln(10)\\right\\rceil.\n$$\nUsing $\\ln(10)\\approx 2.302585093$, we get\n$$\nk_{A}^{\\ast}=\\left\\lceil 5000\\times 2.302585093\\right\\rceil=\\left\\lceil 11512.925465\\right\\rceil=11513.\n$$\nThe total cost for Algorithm A is then $\\text{Cost}_{A}=c_{A}\\,k_{A}^{\\ast}=1.00\\times 11513=11513$.\n\nFor Algorithm B with $\\epsilon_{B}=0.01$,\n$$\nk_{B}^{\\ast}=\\left\\lceil \\frac{2\\ln(10)}{(0.01)^{2}}\\right\\rceil\n=\\left\\lceil \\frac{2\\ln(10)}{0.0001}\\right\\rceil\n=\\left\\lceil 20000\\ln(10)\\right\\rceil\n=\\left\\lceil 46051.70186\\right\\rceil=46052.\n$$\nThe total cost for Algorithm B is $\\text{Cost}_{B}=c_{B}\\,k_{B}^{\\ast}=0.50\\times 46052=23026$.\n\nComparing, $\\text{Cost}_{A}=11513$ and $\\text{Cost}_{B}=23026$, so the minimum possible total cost is achieved by Algorithm A with 11513 dollars. Rounding to two significant figures gives $1.2\\times 10^{4}$.", "answer": "$$\\boxed{1.2 \\times 10^{4}}$$", "id": "1422544"}]}