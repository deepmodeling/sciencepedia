{"hands_on_practices": [{"introduction": "The Valiant-Vazirani procedure transforms a given formula by adding random linear constraints. This first practice explores a fundamental logical consequence of this process by considering a special boundary case: an initially unsatisfiable formula. Understanding the outcome here solidifies the core principle that this powerful technique works by filtering from an existing set of solutions, rather than creating new ones [@problem_id:1465664].", "problem": "In computational complexity theory, the Valiant-Vazirani theorem provides a powerful randomized reduction related to the Boolean Satisfiability (SAT) problem. The process takes a Boolean formula $\\phi$ on $n$ variables $x_1, \\dots, x_n$ as input and produces a new formula $\\phi'$. The construction of $\\phi'$ from $\\phi$ involves selecting a random integer $k$ from $\\{0, 1, \\dots, n\\}$ and then generating $k$ random linear equations over the finite field $\\text{GF}(2)$. The new formula $\\phi'$ is the conjunction of the original formula $\\phi$ and these $k$ linear equations. That is, $\\phi' = \\phi \\land (\\text{equation}_1) \\land \\dots \\land (\\text{equation}_k)$. A solution string $a$ is a satisfying assignment for $\\phi'$ if and only if it satisfies $\\phi$ and also satisfies all $k$ chosen linear equations.\n\nSuppose we apply this Valiant-Vazirani process to a Boolean formula $\\phi$ that is known to be unsatisfiable. Which of the following statements correctly describes the property of the resulting formula $\\phi'$?\n\nA. The formula $\\phi'$ is guaranteed to have exactly one satisfying assignment.\nB. The formula $\\phi'$ is guaranteed to be unsatisfiable.\nC. The formula $\\phi'$ will be unsatisfiable with a high probability (e.g., at least $1/8$ for an appropriate choice of parameters), but it is not guaranteed.\nD. The formula $\\phi'$ could be either satisfiable or unsatisfiable, with the outcome depending on the specific random linear equations generated.\nE. The construction process is not defined for an unsatisfiable formula and would result in an error.", "solution": "Let's analyze the relationship between the satisfying assignments of the original formula $\\phi$ and the new formula $\\phi'$.\n\nLet $S$ be the set of satisfying assignments for the formula $\\phi$.\nLet $S'$ be the set of satisfying assignments for the formula $\\phi'$.\n\nThe new formula $\\phi'$ is constructed as the logical AND (conjunction) of the original formula $\\phi$ and a set of $k$ additional linear constraints, let's call the conjunction of these constraints $C$. Thus, we can write $\\phi' = \\phi \\land C$.\n\nFor an assignment (a specific setting of the variables $x_1, \\dots, x_n$) to satisfy $\\phi'$, it must satisfy both $\\phi$ and $C$. This means that any satisfying assignment for $\\phi'$ must also be a satisfying assignment for $\\phi$.\n\nThis implies that the set of satisfying assignments for $\\phi'$, $S'$, must be a subset of the set of satisfying assignments for $\\phi$, $S$. We can write this relationship as $S' \\subseteq S$.\n\nThe problem states that the original formula $\\phi$ is unsatisfiable. By definition, an unsatisfiable formula has no satisfying assignments. Therefore, its set of satisfying assignments, $S$, is the empty set: $S = \\emptyset$.\n\nUsing the subset relationship we established, $S' \\subseteq S$, and substituting $S = \\emptyset$, we get $S' \\subseteq \\emptyset$.\n\nThe only subset of the empty set is the empty set itself. Therefore, it must be the case that $S' = \\emptyset$.\n\nIf the set of satisfying assignments for $\\phi'$, $S'$, is the empty set, this means by definition that the formula $\\phi'$ is also unsatisfiable.\n\nThis conclusion is a logical certainty derived from the construction of $\\phi'$. The randomness in the process affects which solutions are filtered out if the original formula is satisfiable. If there are no solutions to begin with, no random choices can create one. Adding more constraints to an already unsatisfiable system of constraints cannot make it satisfiable.\n\nNow let's evaluate the given options:\nA. The formula $\\phi'$ is guaranteed to have exactly one satisfying assignment. This is incorrect. This is the desired outcome of the process when the original formula $\\phi$ *is* satisfiable.\nB. The formula $\\phi'$ is guaranteed to be unsatisfiable. This is correct, as we have just proven.\nC. The formula $\\phi'$ will be unsatisfiable with a high probability but is not guaranteed. This is incorrect. The unsatisfiability of $\\phi'$ is a guaranteed outcome, not a probabilistic one. The probabilistic guarantees of the Valiant-Vazirani theorem apply to the case where $\\phi$ is satisfiable.\nD. The formula $\\phi'$ could be either satisfiable or unsatisfiable. This is incorrect. Adding constraints can only preserve or eliminate satisfying assignments; it can never create them. Since there are no satisfying assignments for $\\phi$, there can be none for $\\phi'$.\nE. The construction process is not defined for an unsatisfiable formula. This is incorrect. The process is a syntactic transformation of the formula, and it is well-defined for any Boolean formula, regardless of its satisfiability.", "answer": "$$\\boxed{B}$$", "id": "1465664"}, {"introduction": "At the heart of the Valiant-Vazirani lemma is the power of random constraints to separate, or \"hash,\" different solutions into distinct buckets. This exercise dives into the core probabilistic mechanism by asking you to calculate the precise probability that a single random linear equation fails to distinguish between two specific, distinct solutions [@problem_id:1465648]. Mastering this calculation is the key to understanding why a sequence of such constraints is so effective at isolating one solution from a multitude.", "problem": "In the field of theoretical computer science, a common technique for dealing with problems that have multiple solutions is to add random constraints to \"isolate\" a single solution. This is a core idea behind the famous Valiant-Vazirani theorem.\n\nConsider a computational search problem where solutions are vectors in an $n$-dimensional space over a finite field. Specifically, the set of solutions $S$ is a subset of $\\mathbb{F}_p^n$, where $\\mathbb{F}_p$ is the finite field with $p$ elements for some prime number $p$, and $n$ is a positive integer representing the number of variables.\n\nTo isolate a solution, we augment the problem with $m$ random linear constraints. Each constraint $k \\in \\{1, 2, \\dots, m\\}$ is of the form $a_k \\cdot x = b_k$, where the vector $a_k \\in \\mathbb{F}_p^n$ and the scalar $b_k \\in \\mathbb{F}_p$ are chosen independently and uniformly at random. The expression $a_k \\cdot x$ denotes the standard dot product $\\sum_{i=1}^n (a_k)_i x_i$, with all arithmetic performed in $\\mathbb{F}_p$. A solution $x \\in S$ is said to have \"survived\" a set of constraints if it satisfies all of them.\n\nSuppose the original problem has at least two solutions. Let $x_A$ and $x_B$ be two distinct, fixed solutions from the set $S$. We are given the fact that both $x_A$ and $x_B$ have survived the addition of the first $m-1$ random constraints.\n\nWhat is the probability that both $x_A$ and $x_B$ also survive the $m$-th random constraint?\n\nYour answer should be a closed-form analytic expression in terms of $p$, $n$, and $m$, as applicable.", "solution": "Let $x_{A},x_{B} \\in \\mathbb{F}_{p}^{n}$ be distinct fixed solutions that have already satisfied the first $m-1$ random linear constraints. The $m$-th constraint is generated independently and uniformly at random by choosing $a_{m} \\in \\mathbb{F}_{p}^{n}$ and $b_{m} \\in \\mathbb{F}_{p}$, independently of everything else.\n\nWe seek the conditional probability that both $x_{A}$ and $x_{B}$ satisfy the $m$-th constraint given they survived the first $m-1$. By independence of the constraints, the $m$-th constraint is independent of the earlier ones, so the conditional probability equals the unconditional probability\n$$\n\\Pr\\big[(a_{m}\\cdot x_{A}=b_{m}) \\wedge (a_{m}\\cdot x_{B}=b_{m})\\big].\n$$\n\nSet $v=x_{A}-x_{B}\\neq 0$. The event that both $x_{A}$ and $x_{B}$ satisfy the same constraint is equivalent to the system\n$$\na_{m}\\cdot x_{A}=b_{m},\\qquad a_{m}\\cdot x_{B}=b_{m},\n$$\nwhich is equivalent to\n$$\na_{m}\\cdot v=0 \\quad \\text{and} \\quad b_{m}=a_{m}\\cdot x_{A}.\n$$\nConsider the linear functional $L_{v}:\\mathbb{F}_{p}^{n}\\to\\mathbb{F}_{p}$ given by $L_{v}(a)=a\\cdot v$. Since $v\\neq 0$, $L_{v}$ is a nonzero linear functional and hence surjective; for uniformly random $a_{m}$, the random variable $a_{m}\\cdot v$ is uniform on $\\mathbb{F}_{p}$. Therefore,\n$$\n\\Pr[a_{m}\\cdot v=0]=\\frac{1}{p}.\n$$\nGiven any fixed $a_{m}$, the scalar $b_{m}$ is chosen independently and uniformly from $\\mathbb{F}_{p}$, so\n$$\n\\Pr[b_{m}=a_{m}\\cdot x_{A}\\mid a_{m}]=\\frac{1}{p}.\n$$\nCombining these independent conditions,\n$$\n\\Pr\\big[(a_{m}\\cdot x_{A}=b_{m}) \\wedge (a_{m}\\cdot x_{B}=b_{m})\\big]\n=\\Pr[a_{m}\\cdot v=0]\\cdot \\Pr[b_{m}=a_{m}\\cdot x_{A}\\mid a_{m}]\n=\\frac{1}{p}\\cdot \\frac{1}{p}\n=\\frac{1}{p^{2}}.\n$$\nHence, conditioned on both $x_{A}$ and $x_{B}$ surviving the first $m-1$ constraints, the probability they both survive the $m$-th constraint is $\\frac{1}{p^{2}}$.", "answer": "$$\\boxed{\\frac{1}{p^{2}}}$$", "id": "1465648"}, {"introduction": "Building upon the foundational concepts, this final practice applies the full Valiant-Vazirani hashing procedure to a concrete scenario. You will synthesize the probabilistic principles to calculate the overall success probability of isolating exactly one solution from a small, known number of initial satisfying assignments [@problem_id:61665]. This problem demonstrates how to combine the pairwise separation probability with a binomial framework, giving you a quantitative grasp of the theorem's power and effectiveness in practice.", "problem": "In computational complexity theory, the Valiant-Vazirani theorem provides a randomized reduction from an NP-complete problem to a related problem that has, with significant probability, a unique solution. This \"solution isolation\" is a powerful tool. Let's analyze the core of this technique.\n\nConsider a 3-satisfiability (3-SAT) instance $\\phi$ on $n$ Boolean variables $x_1, \\dots, x_n$. A satisfying assignment is a vector $s \\in \\{0,1\\}^n$ that makes the formula $\\phi$ true. Let $S \\subset \\{0,1\\}^n$ be the set of all satisfying assignments for $\\phi$.\n\nThe Valiant-Vazirani hashing procedure aims to isolate a single solution from $S$. The procedure is as follows:\n1.  Choose $m$ vectors $v_1, v_2, \\dots, v_m$ independently and uniformly at random from $\\{0,1\\}^n$.\n2.  Construct a new set of constraints on a variable vector $x \\in \\{0,1\\}^n$:\n    $$C = \\{v_i \\cdot x = 0 \\pmod 2 \\mid i=1, \\dots, m\\}$$\n    where $v_i \\cdot x = \\sum_{j=1}^n (v_i)_j x_j$ is the dot product over the finite field $\\mathbb{F}_2$.\n3.  The new set of solutions is $S' = \\{s \\in S \\mid s \\text{ satisfies all constraints in } C\\}$.\n4.  The procedure is deemed a \"success\" if the new set of solutions $S'$ contains exactly one element, i.e., $|S'| = 1$.\n\nSuppose we have a 3-SAT instance for which the set of satisfying assignments $S$ has size $|S| = K$. Furthermore, assume the following properties for the set $S$:\n- All vectors in $S$ are non-zero.\n- The $K$ vectors in $S$ are linearly independent over the field $\\mathbb{F}_2$.\n\nGiven $K=5$ and $m=3$, calculate the exact probability of success.", "solution": "Let the set of $K$ satisfying assignments be $S = \\{s_1, s_2, \\dots, s_K\\}$, where each $s_j \\in \\{0,1\\}^n \\setminus \\{\\mathbf{0}\\}$. We are given that these $K$ vectors are linearly independent over $\\mathbb{F}_2$.\n\nThe procedure selects $m$ vectors $v_1, \\dots, v_m$ uniformly and independently from $\\{0,1\\}^n$. A solution $s_j \\in S$ survives if it is included in the new solution set $S'$, which means it must satisfy all the new constraints:\n$$v_i \\cdot s_j = 0 \\pmod 2 \\quad \\text{for all } i \\in \\{1, \\dots, m\\}.$$\n\nLet's define a \"hash\" for each solution $s_j$ as the vector of outcomes for the $m$ dot products:\n$$h(s_j) = (v_1 \\cdot s_j, v_2 \\cdot s_j, \\dots, v_m \\cdot s_j) \\in \\{0,1\\}^m.$$\nA solution $s_j$ is in $S'$ if and only if its hash is the zero vector, $h(s_j) = \\mathbf{0} \\in \\{0,1\\}^m$. The procedure is successful if exactly one $s_j \\in S$ has $h(s_j) = \\mathbf{0}$.\n\nLet's determine the probability distribution of the hash vectors $h(s_1), \\dots, h(s_K)$.\nConsider a single hash function vector $v_i$. Let us analyze the joint distribution of the random variables $(v_i \\cdot s_1, v_i \\cdot s_2, \\dots, v_i \\cdot s_K)$. This is a vector in $\\{0,1\\}^K$.\nWe are given that the vectors $s_1, \\dots, s_K$ are linearly independent over $\\mathbb{F}_2$. Consider the linear map $L: \\{0,1\\}^n \\to \\{0,1\\}^K$ defined by $L(v) = (v \\cdot s_1, v \\cdot s_2, \\dots, v \\cdot s_K)$. Since the vectors $s_1, \\dots, s_K$ are linearly independent, the linear forms $v \\mapsto v \\cdot s_j$ are also linearly independent. This implies that the map $L$ is surjective.\nSince $v_i$ is chosen uniformly at random from $\\{0,1\\}^n$, the output vector $L(v_i)$ is uniformly distributed over $\\{0,1\\}^K$. That is, for any vector $y \\in \\{0,1\\}^K$, the probability $P(L(v_i) = y) = 1/2^K$.\n\nThe hash vectors are $h(s_j)$, which are the columns of the $m \\times K$ matrix $M$ where $M_{ij} = v_i \\cdot s_j$. The rows of this matrix are the vectors $L(v_i)$. Since the $v_i$ are chosen independently, the rows of $M$ are independent and identically distributed uniform random vectors in $\\{0,1\\}^K$.\nThis means that any specific matrix $M$ has a probability of $(1/2^K)^m = 1/2^{mK}$ of occurring.\n\nWe are interested in the distribution of the columns of $M$, which are the hash vectors $h(s_j)$. Let's show they are independent and uniformly distributed in $\\{0,1\\}^m$.\nThe joint probability of the hash vectors taking specific values $c_1, \\dots, c_K \\in \\{0,1\\}^m$ is:\n$$P(h(s_1)=c_1, \\dots, h(s_K)=c_K) = P(\\text{Matrix } M \\text{ has columns } c_1, \\dots, c_K).$$\nThis defines a specific matrix $M$, so the probability is $1/2^{mK}$.\n\nNow let's find the marginal probability for a single hash vector, say $h(s_j)$:\n$$P(h(s_j) = c_j) = \\sum_{c_k \\in \\{0,1\\}^m, k \\neq j} P(h(s_1)=c_1, \\dots, h(s_K)=c_K).$$\nThere are $(2^m)^{K-1}$ choices for the other $K-1$ columns. So,\n$$P(h(s_j) = c_j) = (2^m)^{K-1} \\cdot \\frac{1}{2^{mK}} = \\frac{2^{m(K-1)}}{2^{mK}} = \\frac{1}{2^m}.$$\nThis confirms that each hash vector $h(s_j)$ is uniformly distributed in $\\{0,1\\}^m$.\n\nFor independence, we check if the joint probability is the product of marginals:\n$$P(h(s_1)=c_1) \\cdots P(h(s_K)=c_K) = \\left(\\frac{1}{2^m}\\right)^K = \\frac{1}{2^{mK}}.$$\nThis matches the joint probability $P(h(s_1)=c_1, \\dots, h(s_K)=c_K)$, so the hash vectors $h(s_1), \\dots, h(s_K)$ are independent.\n\nThe problem is now reduced to the following: we have $K$ independent random vectors, each uniformly distributed in $\\{0,1\\}^m$. What is the probability that exactly one of them is the zero vector?\n\nLet $E_j$ be the event that $h(s_j) = \\mathbf{0}$. The probability of this event is:\n$$p = P(E_j) = P(h(s_j) = \\mathbf{0}) = \\frac{1}{2^m}.$$\nThe event of success is that exactly one of the independent events $E_1, \\dots, E_K$ occurs. This is a classic binomial probability problem. The number of trials is $K$, and the probability of \"success\" for each trial is $p$. The probability of exactly one success is given by the binomial formula:\n$$P(\\text{success}) = \\binom{K}{1} p^1 (1-p)^{K-1} = K p (1-p)^{K-1}.$$\n\nWe are given the parameters $K=5$ and $m=3$. First, we compute $p$:\n$$p = \\frac{1}{2^3} = \\frac{1}{8}.$$\nThen, the probability of failure for a single trial is:\n$$1-p = 1 - \\frac{1}{8} = \\frac{7}{8}.$$\nNow, we substitute these values into the binomial probability formula:\n$$P(\\text{success}) = 5 \\cdot \\left(\\frac{1}{8}\\right) \\cdot \\left(\\frac{7}{8}\\right)^{5-1} = 5 \\cdot \\frac{1}{8} \\cdot \\left(\\frac{7}{8}\\right)^4.$$\n$$P(\\text{success}) = \\frac{5}{8} \\cdot \\frac{7^4}{8^4} = \\frac{5 \\cdot 2401}{8^5} = \\frac{12005}{32768}.$$\nThus, the exact probability of isolating a single solution is $\\frac{12005}{32768}$.", "answer": "$$ \\boxed{\\frac{12005}{32768}} $$", "id": "61665"}]}