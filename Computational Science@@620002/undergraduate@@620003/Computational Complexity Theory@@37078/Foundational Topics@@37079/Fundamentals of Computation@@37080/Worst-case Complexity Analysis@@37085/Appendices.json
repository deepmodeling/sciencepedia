{"hands_on_practices": [{"introduction": "Our exploration into worst-case analysis begins with a classic computational task: finding the intersection of two sets. This exercise [@problem_id:1469548] models a common scenario, such as cross-referencing a user's wishlist with a list of items on sale. By analyzing a straightforward nested-loop approach, you will practice translating algorithmic structure directly into a Big-O expression, such as $O(mn)$, and understand how complexity scales with multiple input sizes.", "problem": "A programmer is tasked with developing a feature for an e-commerce platform. The feature needs to identify which items from a user's wishlist are currently on sale. The wishlist is represented as an array of unique integer product IDs, let's call it `wishlist`, of size $m$. The list of items on sale is represented by a second array of unique integer product IDs, `saleItems`, of size $n$.\n\nTo find the common products, the programmer implements a straightforward algorithm. The algorithm iterates through each product ID in the `wishlist` array. For each product ID from the wishlist, it then iterates through every product ID in the `saleItems` array to check for a match.\n\nWhat is the worst-case time complexity of this algorithm, expressed using Big-O notation, as a function of the array sizes $m$ and $n$?\n\nA. $O(m + n)$\n\nB. $O(m \\log n + n \\log m)$\n\nC. $O(mn)$\n\nD. $O(\\max(m, n))$\n\nE. $O(\\min(m, n))$", "solution": "Let the outer loop iterate over the $m$ elements of the array $wishlist$, and for each element, the inner loop scans all $n$ elements of the array $saleItems$ to check for a match.\n\nAssume each comparison and loop overhead takes constant time $c>0$. Then the total number of constant-time operations in the worst case (when no early termination occurs and each inner loop scans all $n$ items) is\n$$\nT(m,n) = \\sum_{i=1}^{m} \\sum_{j=1}^{n} c = c \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} 1 \\right) = c \\sum_{i=1}^{m} n = c m n.\n$$\nThus, asymptotically,\n$$\nT(m,n) \\in O(mn).\n$$\nNo preprocessing or auxiliary data structures change this bound in the described algorithm, and the uniqueness of IDs does not reduce the worst-case number of comparisons per outer iteration.\n\nTherefore, the worst-case time complexity is $O(mn)$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1469548"}, {"introduction": "Building on the basics, we now examine an algorithm where the worst-case performance is less about the path of execution and more about the specific data being processed. This problem [@problem_id:1469598] involves an in-place duplicate removal function, where the operational cost depends on the data's layout. Your task is to determine not just the complexity, but the characteristics of the input list that would maximize the number of data-shifting operations, a crucial skill in robust algorithm design.", "problem": "A programmer implements a function to remove duplicate elements from a list `L` containing $n$ elements, indexed from 0 to $n-1$. The function modifies the list in-place. Here is a detailed description of the algorithm:\n\nThe algorithm maintains a variable, `current_size`, initialized to $n$. It uses a primary index `i` that iterates from `0` up to, but not including, `current_size`. For each value of `i`, a secondary index `j` iterates from `i+1` up to, but not including, `current_size`.\n\nDuring the inner iteration (the one with index `j`):\n1. The elements `L[i]` and `L[j]` are compared.\n2. If `L[i]` is not equal to `L[j]`, the index `j` is incremented, and the comparison continues with the next element.\n3. If `L[i]` is equal to `L[j]`, a duplicate is found. The element at index `j` is removed by shifting all subsequent elements—from index `j+1` to the end of the current list—one position to the left. Specifically, for each `k` from `j` up to `current_size - 2`, the assignment `L[k] = L[k+1]` is performed. After the shift, `current_size` is decremented by one. The index `j` is *not* incremented, as the new element at `L[j]` (which was previously at `L[j+1]`) must now be compared with `L[i]`.\n4. The inner loop continues until `j` reaches `current_size`.\n\nAfter the inner loop for a given `i` completes, `i` is incremented, and the process repeats until the outer loop condition (`i < current_size`) is no longer met.\n\nYour task is to determine the maximum possible total number of element assignment operations (`L[k] = L[k+1]`) that can occur for a list of initial size $n$, where $n \\geq 2$. Express your answer as a function of $n$.", "solution": "We count only assignment operations of the form $L[k]=L[k+1]$ performed when a duplicate at index $j$ is removed by shifting the suffix left. Suppose the current list size at the moment of a deletion is $s$ and the duplicate being removed is at index $j$. The algorithm performs assignments for $k$ ranging from $j$ to $s-2$ inclusive, which is\n$$\n(s-2)-j+1=s-1-j\n$$\nassignments for that deletion.\n\nOver the entire run, let $D$ be the total number of deletions, and index the deletions by $d=1,2,\\dots,D$. Let $s_{d}$ be the current size when the $d$-th deletion occurs and $j_{d}$ the index removed then. Since each deletion reduces the size by one, $s_{1}=n$ and $s_{d+1}=s_{d}-1$, hence $s_{d}=n-d+1$. At any deletion, the inner-loop index satisfies $j_{d}\\geq 1$ (because $i\\geq 0$ and $j\\geq i+1$). Therefore, the total number of assignments is\n$$\nA=\\sum_{d=1}^{D}\\big(s_{d}-1-j_{d}\\big)\\leq \\sum_{d=1}^{D}\\big(s_{d}-2\\big),\n$$\nwith equality in each term when $j_{d}=1$. The right-hand side is nondecreasing in $D$ because $s_{d}\\geq 2$ for all deletions, so the maximum occurs when the number of deletions is as large as possible, namely $D=n-1$ (which happens exactly when all $n$ elements are equal, so only one remains). In that configuration, the outer index is $i=0$ throughout, and the inner index stays $j=1$ after each deletion, achieving $j_{d}=1$ for every $d$ and thus equality in the bound.\n\nHence, with $D=n-1$ and $s_{d}$ ranging from $n$ down to $2$, the maximum total number of assignments is\n$$\nA_{\\max}=\\sum_{d=1}^{n-1}\\big(s_{d}-2\\big)=\\sum_{s=2}^{n}(s-2)=\\sum_{t=0}^{n-2}t=\\frac{(n-2)(n-1)}{2}.\n$$\nThis bound is achieved by an input list where all $n$ elements are identical, so it is the maximum possible.", "answer": "$$\\boxed{\\frac{(n-1)(n-2)}{2}}$$", "id": "1469598"}, {"introduction": "We conclude our practice by shifting from iterative to recursive algorithms, a cornerstone of advanced computation. This thought experiment [@problem_id:1469561] presents a recursive algorithm with unbalanced subproblems, leading to a recurrence relation like $T(n) = T(n/2) + T(n/4) + n^2$. By analyzing this recurrence, you will learn to identify which part of a recursive process—the recursive calls or the work done at each step—dominates the overall complexity.", "problem": "An experimental algorithm, designed for a specialized form of hierarchical data analysis, operates on an input array of size $n$. For simplicity, assume $n$ is always a power of 4. The algorithm's procedure is defined recursively as follows:\n\n1.  If the input size $n$ is 1, the algorithm terminates, taking a constant amount of time.\n2.  If $n > 1$, the algorithm first performs a global consolidation operation on the entire array. The time required for this operation is proportional to the square of the array's size, given by $c n^2$ for some positive constant $c$.\n3.  After the consolidation, the algorithm recursively calls itself on two distinct, non-overlapping subproblems:\n    a. The first recursive call processes a sub-array of size $n/2$.\n    b. The second recursive call processes a different sub-array of size $n/4$.\n\nThe total time complexity $T(n)$ is the sum of the time for the consolidation step and the time for the two recursive calls.\n\nWhich of the following represents the tightest asymptotic upper and lower bound (Big-Theta notation) for the worst-case time complexity $T(n)$ of this algorithm?\n\nA. $\\Theta(n \\log n)$\n\nB. $\\Theta(n^{\\log_{2}(\\phi)})$, where $\\phi = \\frac{1+\\sqrt{5}}{2}$ is the golden ratio.\n\nC. $\\Theta(n^2)$\n\nD. $\\Theta(n^2 \\log n)$\n\nE. $\\Theta(n^3)$", "solution": "We formalize the recurrence from the description. Let $T(1)=\\Theta(1)$, and for $n>1$,\n$$\nT(n)=T\\!\\left(\\frac{n}{2}\\right)+T\\!\\left(\\frac{n}{4}\\right)+c n^{2},\n$$\nwhere $c>0$ is a constant. We analyze this using the Akra–Bazzi theorem. Write the recurrence in the form $T(x)=\\sum_{i=1}^{k} a_{i} T(b_{i} x)+g(x)$ with $a_{1}=1$, $a_{2}=1$, $b_{1}=\\frac{1}{2}$, $b_{2}=\\frac{1}{4}$, and $g(x)=c x^{2}$. Solve for $p$ from\n$$\n\\sum_{i=1}^{2} a_{i} b_{i}^{p}=1 \\quad \\Longleftrightarrow \\quad \\left(\\frac{1}{2}\\right)^{p}+\\left(\\frac{1}{4}\\right)^{p}=1.\n$$\nLet $y=2^{-p}$. Then $y+y^{2}=1$, i.e.,\n$$\ny^{2}+y-1=0.\n$$\nThe positive root is\n$$\ny=\\frac{-1+\\sqrt{1+4}}{2}=\\frac{-1+\\sqrt{5}}{2}=\\frac{1}{\\phi},\n$$\nwhere $\\phi=\\frac{1+\\sqrt{5}}{2}$ is the golden ratio. Hence $2^{-p}=\\frac{1}{\\phi}$, which implies\n$$\np=\\log_{2}(\\phi).\n$$\nSince $1<\\phi<2$, it follows that $0<p<1$.\n\nBy the Akra–Bazzi theorem,\n$$\nT(x)=\\Theta\\!\\left(x^{p}\\left(1+\\int_{1}^{x} \\frac{g(u)}{u^{p+1}}\\,du\\right)\\right).\n$$\nWith $g(u)=c u^{2}$, the integrand is\n$$\n\\frac{g(u)}{u^{p+1}}=c\\,u^{2-(p+1)}=c\\,u^{1-p}.\n$$\nBecause $1-p>0$, we compute\n$$\n\\int_{1}^{x} c\\,u^{1-p}\\,du=c\\,\\left[\\frac{u^{2-p}}{2-p}\\right]_{1}^{x}=\\frac{c}{2-p}\\left(x^{2-p}-1\\right).\n$$\nTherefore,\n$$\nT(x)=\\Theta\\!\\left(x^{p}\\left(1+\\frac{c}{2-p}\\left(x^{2-p}-1\\right)\\right)\\right)=\\Theta\\!\\left(x^{p}\\cdot x^{2-p}\\right)=\\Theta\\!\\left(x^{2}\\right).\n$$\nRewriting in terms of $n$, we obtain $T(n)=\\Theta(n^{2})$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1469561"}]}