{"hands_on_practices": [{"introduction": "Before diving into complex models, it's essential to master the fundamentals of data preprocessing. This first exercise introduces min-max scaling, a common technique used to normalize features into a fixed range. By manually applying the scaling formula to a small dataset, you will gain a concrete understanding of how this transformation works, which is a foundational skill for preparing data for many machine learning algorithms [@problem_id:1425867].", "problem": "In a systems biology experiment, the abundance of a specific protein is measured at five consecutive time points following a stimulus. The recorded abundance values, in arbitrary fluorescence units, are collected into a vector $x = [5, 10, 20, 25, 90]$. Before incorporating this data into a larger computational model, it is necessary to normalize it. You are tasked with performing min-max scaling to transform the data into the standardized range of $[-1, 1]$.\n\nThe general formula for min-max scaling a data point $x_i$ from an original dataset with minimum value $x_{\\min}$ and maximum value $x_{\\max}$ to a new range $[a, b]$ is:\n\n$$x'_{i} = a + \\frac{(x_i - x_{\\min})(b - a)}{x_{\\max} - x_{\\min}}$$\n\nCalculate the new vector $x'$ containing the scaled abundance values. Express your answer as a row vector of five exact fractions.", "solution": "We are given the data vector $x=[5, 10, 20, 25, 90]$ and asked to apply min-max scaling to the range $[-1, 1]$. The min-max formula is\n$$\nx'_{i} = a + \\frac{(x_i - x_{\\min})(b - a)}{x_{\\max} - x_{\\min}},\n$$\nwith $a=-1$, $b=1$, $x_{\\min}=5$, and $x_{\\max}=90$. Substituting these into the formula gives\n$$\nx'_{i} = -1 + \\frac{(x_i - 5)(1 - (-1))}{90 - 5} = -1 + \\frac{2(x_i - 5)}{85}.\n$$\nWe now compute each scaled value exactly:\n- For $x_{1}=5$: $x'_{1} = -1 + \\frac{2(5-5)}{85} = -1 + 0 = \\frac{-1}{1}$.\n- For $x_{2}=10$: $x'_{2} = -1 + \\frac{2(10-5)}{85} = -1 + \\frac{10}{85} = -1 + \\frac{2}{17} = \\frac{-15}{17}$.\n- For $x_{3}=20$: $x'_{3} = -1 + \\frac{2(20-5)}{85} = -1 + \\frac{30}{85} = -1 + \\frac{6}{17} = \\frac{-11}{17}$.\n- For $x_{4}=25$: $x'_{4} = -1 + \\frac{2(25-5)}{85} = -1 + \\frac{40}{85} = -1 + \\frac{8}{17} = \\frac{-9}{17}$.\n- For $x_{5}=90$: $x'_{5} = -1 + \\frac{2(90-5)}{85} = -1 + \\frac{170}{85} = -1 + 2 = \\frac{1}{1}$.\n\nThus, the scaled vector as exact fractions is\n$$\n\\begin{pmatrix}\n\\frac{-1}{1} & \\frac{-15}{17} & \\frac{-11}{17} & \\frac{-9}{17} & \\frac{1}{1}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{-1}{1} & \\frac{-15}{17} & \\frac{-11}{17} & \\frac{-9}{17} & \\frac{1}{1}\\end{pmatrix}}$$", "id": "1425867"}, {"introduction": "Real-world data is often messy and can contain outliers that disproportionately affect standard statistical measures like the mean and standard deviation. This practice explores the critical difference between standard scaling and robust scaling, which uses the median and interquartile range to mitigate the influence of extreme values. By comparing the results of both methods on a dataset with a clear outlier [@problem_id:1425850], you will learn to select the appropriate normalization strategy to build more reliable and robust models.", "problem": "A systems biology student is analyzing expression data for a particular gene, `Gene-Y`, across seven different cell culture samples. The measured expression levels, in arbitrary units, are recorded as follows:\n`[22.5, 25.0, 24.1, 150.0, 26.2, 23.3, 22.8]`\nThe student observes that the measurement `150.0` is an extreme outlier compared to the rest of the data. To quantify the effect of this outlier on data normalization, the student will normalize the data point `x = 26.2` using two distinct methods: Standard Scaling and Robust Scaling.\n\n**Method 1: Standard Scaling**\nThe standardized value, $z_{std}$, is calculated using the formula:\n$$z_{std} = \\frac{x - \\mu}{\\sigma}$$\nwhere $x$ is the data point to be normalized, $\\mu$ is the arithmetic mean of the entire dataset, and $\\sigma$ is the sample standard deviation. The sample standard deviation is defined as $\\sigma = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_i - \\mu)^2}$ for a dataset with $N$ data points.\n\n**Method 2: Robust Scaling**\nThe robust scaled value, $z_{rob}$, is calculated using the formula:\n$$z_{rob} = \\frac{x - \\text{median}}{ \\text{IQR}}$$\nwhere $x$ is the data point, the median is the middle value of the sorted dataset, and the Interquartile Range (IQR) is the difference between the third quartile ($Q_3$) and the first quartile ($Q_1$). For this problem, you must calculate the quartiles using the following method: sort the dataset, find the overall median, then define $Q_1$ as the median of the data points strictly below the overall median, and $Q_3$ as the median of the data points strictly above the overall median.\n\nYour task is to compute the values of $z_{std}$ and $z_{rob}$ for the data point $x = 26.2$. Present your answer as a pair of values, with $z_{std}$ first and $z_{rob}$ second. Round both values in your final answer to three significant figures.", "solution": "We have the dataset with $N=7$: $[22.5, 25.0, 24.1, 150.0, 26.2, 23.3, 22.8]$ and the target point $x=26.2$.\n\nFor Standard Scaling, compute the arithmetic mean $\\mu$ and the sample standard deviation $\\sigma$. The mean is\n$$\n\\mu=\\frac{1}{7}\\sum_{i=1}^{7}x_{i}=\\frac{293.9}{7}=\\frac{2939}{70}.\n$$\nUsing $\\sigma=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\mu)^{2}}$ with $N-1=6$, first compute the deviations as exact fractions:\n$$\n\\begin{aligned}\n22.5-\\mu&=\\frac{1575}{70}-\\frac{2939}{70}=-\\frac{1364}{70}=-\\frac{682}{35},\\\\\n25.0-\\mu&=\\frac{1750}{70}-\\frac{2939}{70}=-\\frac{1189}{70},\\\\\n24.1-\\mu&=\\frac{1687}{70}-\\frac{2939}{70}=-\\frac{626}{35},\\\\\n150.0-\\mu&=\\frac{10500}{70}-\\frac{2939}{70}=\\frac{7561}{70},\\\\\n26.2-\\mu&=\\frac{1834}{70}-\\frac{2939}{70}=-\\frac{1105}{70}=-\\frac{221}{14},\\\\\n23.3-\\mu&=\\frac{1631}{70}-\\frac{2939}{70}=-\\frac{1308}{70}=-\\frac{654}{35},\\\\\n22.8-\\mu&=\\frac{1596}{70}-\\frac{2939}{70}=-\\frac{1343}{70}.\n\\end{aligned}\n$$\nSquare and sum:\n$$\n\\sum_{i=1}^{7}(x_{i}-\\mu)^{2}=\\frac{465124}{1225}+\\frac{1413721}{4900}+\\frac{391876}{1225}+\\frac{57168721}{4900}+\\frac{48841}{196}+\\frac{427716}{1225}+\\frac{1803649}{4900}.\n$$\nBringing all terms to denominator $4900$ and summing numerators gives\n$$\n\\sum_{i=1}^{7}(x_{i}-\\mu)^{2}=\\frac{66745980}{4900}=\\frac{476757}{35}.\n$$\nHence the sample variance and standard deviation are\n$$\n\\sigma^{2}=\\frac{1}{6}\\cdot\\frac{476757}{35}=\\frac{476757}{210}=\\frac{158919}{70},\\qquad \\sigma=\\sqrt{\\frac{158919}{70}}.\n$$\nTherefore, for $x=26.2$,\n$$\nz_{std}=\\frac{x-\\mu}{\\sigma}=\\frac{-\\frac{221}{14}}{\\sqrt{\\frac{158919}{70}}}=-\\frac{221}{14}\\sqrt{\\frac{70}{158919}}\\approx -0.3313024.\n$$\n\nFor Robust Scaling, sort the data: $[22.5, 22.8, 23.3, 24.1, 25.0, 26.2, 150.0]$. The overall median is the fourth value: $\\text{median}=24.1$. Using the specified quartile method, the lower half strictly below the median is $[22.5, 22.8, 23.3]$ so $Q_{1}=22.8$, and the upper half strictly above the median is $[25.0, 26.2, 150.0]$ so $Q_{3}=26.2$. Thus\n$$\n\\text{IQR}=Q_{3}-Q_{1}=26.2-22.8=3.4,\n$$\nand\n$$\nz_{rob}=\\frac{x-\\text{median}}{\\text{IQR}}=\\frac{26.2-24.1}{3.4}=\\frac{2.1}{3.4}\\approx 0.61764706.\n$$\n\nRounding both values to three significant figures yields $z_{std}\\approx -0.331$ and $z_{rob}\\approx 0.618$.", "answer": "$$\\boxed{\\begin{pmatrix}-0.331 & 0.618\\end{pmatrix}}$$", "id": "1425850"}, {"introduction": "One of the most subtle yet critical errors in machine learning is data leakage, where information from outside the training dataset is inadvertently used to create the model. This advanced problem presents a hypothetical scenario to demonstrate how a seemingly logical per-class normalization strategy can leak information from validation labels into the preprocessing step. Working through this example [@problem_id:3111750] will sharpen your ability to design sound validation pipelines and recognize how flawed preprocessing can lead to misleadingly optimistic performance estimates, a crucial lesson for building models that generalize effectively to unseen data.", "problem": "A binary classification task is defined with label $y \\in \\{0,1\\}$ and a single real-valued feature $x \\in \\mathbb{R}$. The dataset is imbalanced with class prior probabilities $p(y=0)=0.9$ and $p(y=1)=0.1$. The class-conditional distributions of the raw feature are\n$$\nx \\mid y=0 \\sim \\mathcal{N}(10, 25), \\quad x \\mid y=1 \\sim \\mathcal{N}(10, 1).\n$$\nTwo preprocessing pipelines are considered before training and validating a classifier that is consistent with the Bayes decision rule.\n\n- Pipeline $\\mathrm{G}$ (global normalization): compute the global training mean $\\mu_{\\mathrm{train}}$ and standard deviation $\\sigma_{\\mathrm{train}}$ across all training samples irrespective of class, and transform each sample by $z=(x-\\mu_{\\mathrm{train}})/\\sigma_{\\mathrm{train}}$. This transform is fitted on the training split only and applied to validation without using validation labels.\n\n- Pipeline $\\mathrm{PC}$ (per-class unit-variance scaling with labels): for each class $c \\in \\{0,1\\}$, estimate the class-specific standard deviation $\\sigma_c$ from the training split. At validation time, transform each sample by $z=x/\\sigma_y$, that is, divide by the standard deviation corresponding to the sampleâ€™s true validation label $y$. This transform requires validation labels to choose which $\\sigma_c$ to apply.\n\nAssume infinite data and a classifier that implements the Bayes decision rule in the space it is given (that is, in $x$-space for Pipeline $\\mathrm{G}$, and in $z$-space for Pipeline $\\mathrm{PC}$). Which of the following statements are true in this setting?\n\nA. Using Pipeline $\\mathrm{PC}$ leaks label information at validation because the transform depends on the true label $y$, and it can inflate validation accuracy far above the majority-class baseline.\n\nB. Using Pipeline $\\mathrm{G}$ does not leak validation labels and, for the given class-conditional distributions and priors, the Bayes-optimal validation accuracy cannot exceed the majority-class baseline of $0.9$.\n\nC. If instead of dividing by $\\sigma_y$ one performs per-class standardization $z=(x-\\mu_y)/\\sigma_y$ at validation using the true labels (with $\\mu_0=\\mu_1=10$ and $\\sigma_0=5$, $\\sigma_1=1$), this leaks label information in the preprocessing step, but in this specific data configuration it does not inflate validation accuracy because both class-conditional $z$-distributions become identical.\n\nD. In imbalanced datasets, per-class normalization should be applied at validation time using the true labels because it always improves generalization.\n\nE. To avoid leakage, normalization parameters should be computed on the training split without conditioning on labels at validation, and the same fixed transform should be applied to validation and test data without inspecting their labels.", "solution": "The user has provided a problem statement regarding a binary classification task and asks to evaluate the truthfulness of five statements about two different data preprocessing pipelines.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Binary classification task with labels $y \\in \\{0, 1\\}$.\n-   Single real-valued feature $x \\in \\mathbb{R}$.\n-   Class prior probabilities are $p(y=0)=0.9$ and $p(y=1)=0.1$.\n-   Class-conditional distributions are given as:\n    -   $x \\mid y=0 \\sim \\mathcal{N}(10, 25)$, i.e., mean $\\mu_0=10$ and variance $\\sigma_0^2=25$.\n    -   $x \\mid y=1 \\sim \\mathcal{N}(10, 1)$, i.e., mean $\\mu_1=10$ and variance $\\sigma_1^2=1$.\n-   Pipeline G (global normalization): The transformation is $z=(x-\\mu_{\\mathrm{train}})/\\sigma_{\\mathrm{train}}$, where $\\mu_{\\mathrm{train}}$ and $\\sigma_{\\mathrm{train}}$ are the global mean and standard deviation of the training data. This transform is fitted on the training split only and applied to the validation split without using validation labels.\n-   Pipeline PC (per-class unit-variance scaling): The transformation is $z=x/\\sigma_y$, where $\\sigma_y$ is the standard deviation for the sample's true class $y$. This transform requires access to the validation labels.\n-   Assumption: The dataset is of infinite size, which implies that sample statistics (like mean and standard deviation) are equal to their true population values.\n-   Assumption: The classifier implements the Bayes decision rule in the feature space it operates on.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-grounded in Bayesian decision theory, probability, and standard machine learning practices. The use of normal distributions is standard.\n-   **Well-Posed**: The problem is clearly defined. The \"infinite data\" assumption simplifies the calculation of statistical parameters, making the problem deterministic and solvable. The question asks for an evaluation of specific statements, which is a well-posed task.\n-   **Objective**: The problem is stated in precise mathematical and technical language, free from subjectivity.\n-   The problem setup does not violate any of the invalidity criteria. The controversial practice described in Pipeline PC is part of what is being tested, not a flaw in the problem statement itself.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation and Analysis\n\nFirst, we analyze the performance of a Bayes-optimal classifier on the raw, untransformed feature $x$. The Bayes decision rule is to predict the class $k$ that maximizes the posterior probability $p(y=k|x)$. This is equivalent to maximizing the joint probability $p(x,y=k) = p(x|y=k)p(y=k)$.\n\nWe are given:\n-   $p(y=0)=0.9$ and $p(y=1)=0.1$.\n-   $p(x|y=0) = \\frac{1}{\\sqrt{2\\pi \\cdot 25}} \\exp\\left(-\\frac{(x-10)^2}{50}\\right)$.\n-   $p(x|y=1) = \\frac{1}{\\sqrt{2\\pi \\cdot 1}} \\exp\\left(-\\frac{(x-10)^2}{2}\\right)$.\n\nThe classifier will predict $y=1$ if $p(x|y=1)p(y=1) > p(x|y=0)p(y=0)$. Let's evaluate this inequality:\n$$\n\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-10)^2}{2}\\right) \\cdot 0.1 > \\frac{1}{5\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-10)^2}{50}\\right) \\cdot 0.9\n$$\n$$\n0.1 \\cdot \\exp\\left(-\\frac{(x-10)^2}{2}\\right) > 0.18 \\cdot \\exp\\left(-\\frac{(x-10)^2}{50}\\right)\n$$\nDividing by the exponential term on the right (which is always positive):\n$$\n\\frac{0.1}{0.18} > \\exp\\left(\\frac{(x-10)^2}{2} - \\frac{(x-10)^2}{50}\\right)\n$$\n$$\n\\frac{1}{1.8} > \\exp\\left((x-10)^2 \\left(\\frac{25-1}{50}\\right)\\right)\n$$\n$$\n\\frac{1}{1.8} > \\exp\\left(\\frac{12(x-10)^2}{25}\\right)\n$$\nThe argument of the exponential function, $\\frac{12(x-10)^2}{25}$, is always non-negative. This means the exponential term $\\exp\\left(\\frac{12(x-10)^2}{25}\\right)$ is always greater than or equal to $1$. However, the left side of the inequality is $\\frac{1}{1.8} < 1$. Thus, the inequality is never satisfied.\n\nThis means that for any value of $x$, $p(x, y=1) < p(x, y=0)$. The Bayes-optimal classifier will therefore always predict $y=0$.\nThe accuracy of this classifier is the probability of correctly predicting the class label. Since it always predicts $y=0$:\n$$\n\\text{Accuracy} = p(\\text{predict } 0, y=0) + p(\\text{predict } 1, y=1) = p(\\text{predict } 0|y=0)p(y=0) + p(\\text{predict } 1|y=1)p(y=1)\n$$\n$$\n\\text{Accuracy} = 1 \\cdot p(y=0) + 0 \\cdot p(y=1) = 0.9\n$$\nThe accuracy is $0.9$, which is the majority-class baseline.\n\n### Evaluation of Options\n\n**A. Using Pipeline PC leaks label information at validation because the transform depends on the true label $y$, and it can inflate validation accuracy far above the majority-class baseline.**\n\nPipeline PC uses the transform $z=x/\\sigma_y$. At validation, for a sample $(x_{\\text{val}}, y_{\\text{val}})$, the transform applied is $z_{\\text{val}} = x_{\\text{val}}/\\sigma_{y_{\\text{val}}}$. The choice of standard deviation ($\\sigma_0=5$ or $\\sigma_1=1$) depends on the true validation label $y_{\\text{val}}$. This is a form of data leakage, as information from the validation labels is used in the feature preparation step.\n\nLet's analyze the effect on the distributions.\n-   If $y=0$, $x \\sim \\mathcal{N}(10, 25)$. The transform is $z=x/\\sigma_0 = x/5$. The resulting distribution for $z$ is $\\mathcal{N}(10/5, 25/5^2) = \\mathcal{N}(2, 1)$.\n-   If $y=1$, $x \\sim \\mathcal{N}(10, 1)$. The transform is $z=x/\\sigma_1 = x/1$. The resulting distribution for $z$ is $\\mathcal{N}(10, 1)$.\n\nThe classifier now sees a problem of distinguishing between $z|y=0 \\sim \\mathcal{N}(2, 1)$ and $z|y=1 \\sim \\mathcal{N}(10, 1)$. These two distributions are highly separable. The Bayes-optimal classifier will have a decision boundary for $z$ and will achieve a high accuracy. The accuracy will be approximately $p(y=0) \\cdot \\Phi(z^*-2) + p(y=1) \\cdot (1-\\Phi(z^*-10))$, where $z^*$ is the decision boundary. Since the means ($2$ and $10$) are far apart relative to the standard deviation ($1$), both $\\Phi(\\cdot)$ terms will be very close to $1$. Thus, the accuracy will approach $0.9 \\cdot 1 + 0.1 \\cdot 1 = 1.0$. This is a significant inflation above a real-world performance estimate and the baseline of $0.9$.\n\nTherefore, the statement is **Correct**.\n\n**B. Using Pipeline G does not leak validation labels and, for the given class-conditional distributions and priors, the Bayes-optimal validation accuracy cannot exceed the majority-class baseline of $0.9$.**\n\nPipeline G uses $z=(x-\\mu_{\\mathrm{train}})/\\sigma_{\\mathrm{train}}$. The parameters are computed on the training data only. With infinite data, we use the true population parameters for the mixture distribution $p(x) = p(x|y=0)p(y=0) + p(x|y=1)p(y=1)$.\n-   Global mean $\\mu = E[X] = E[X|Y=0]p(Y=0) + E[X|Y=1]p(Y=1) = 10 \\cdot 0.9 + 10 \\cdot 0.1 = 10$.\n-   Global variance $\\sigma^2 = E[X^2] - \\mu^2$.\n    $E[X^2|Y=c] = \\sigma_c^2 + \\mu_c^2$.\n    $E[X^2] = E[X^2|Y=0]p(Y=0) + E[X^2|Y=1]p(Y=1) = (25+10^2) \\cdot 0.9 + (1+10^2) \\cdot 0.1 = 125 \\cdot 0.9 + 101 \\cdot 0.1 = 112.5 + 10.1 = 122.6$.\n    $\\sigma^2 = 122.6 - 10^2 = 22.6$. So $\\sigma = \\sqrt{22.6}$.\n\nThe transform is $z=(x-10)/\\sqrt{22.6}$. This is a fixed affine transformation applied to all samples, irrespective of their labels. Therefore, it does not leak validation labels.\nThis transformation maps the distributions as follows:\n-   $x|y=0 \\sim \\mathcal{N}(10, 25) \\implies z|y=0 \\sim \\mathcal{N}(0, 25/22.6)$.\n-   $x|y=1 \\sim \\mathcal{N}(10, 1) \\implies z|y=1 \\sim \\mathcal{N}(0, 1/22.6)$.\nThe problem in $z$-space is to distinguish between two normal distributions with the same mean ($0$) but different variances. This is structurally identical to the original problem in $x$-space. An invertible affine transformation does not change the separability of the classes for a Bayes classifier. The exact same logic as in the initial derivation applies: the Bayes-optimal classifier will always predict the majority class, $y=0$, and achieve an accuracy of $0.9$. The accuracy does not exceed the majority-class baseline.\n\nTherefore, the statement is **Correct**.\n\n**C. If instead of dividing by $\\sigma_y$ one performs per-class standardization $z=(x-\\mu_y)/\\sigma_y$ at validation using the true labels (with $\\mu_0=\\mu_1=10$ and $\\sigma_0=5$, $\\sigma_1=1$), this leaks label information in the preprocessing step, but in this specific data configuration it does not inflate validation accuracy because both class-conditional $z$-distributions become identical.**\n\nThis procedure uses both $\\mu_y$ and $\\sigma_y$ based on the validation label $y$, so it certainly leaks label information. Let's apply this transformation:\n-   If $y=0$: $x \\sim \\mathcal{N}(\\mu_0=10, \\sigma_0^2=25)$. The transform is $z=(x-\\mu_0)/\\sigma_0 = (x-10)/5$. The resulting distribution is $z|y=0 \\sim \\mathcal{N}\\left(\\frac{10-10}{5}, \\frac{25}{5^2}\\right) = \\mathcal{N}(0,1)$.\n-   If $y=1$: $x \\sim \\mathcal{N}(\\mu_1=10, \\sigma_1^2=1)$. The transform is $z=(x-\\mu_1)/\\sigma_1 = (x-10)/1$. The resulting distribution is $z|y=1 \\sim \\mathcal{N}\\left(\\frac{10-10}{1}, \\frac{1}{1^2}\\right) = \\mathcal{N}(0,1)$.\n\nAfter this transformation, the distributions for both classes become the standard normal distribution $\\mathcal{N}(0,1)$. Since the class-conditional distributions are identical, the feature $z$ provides no information for discriminating between class $0$ and class $1$. The posterior probability equals the prior: $p(y|z) = p(y)$. The Bayes-optimal classifier must always predict the majority class, $y=0$. The accuracy will be $p(y=0)=0.9$. This is the baseline accuracy, so it is not inflated. The statement accurately describes the outcome.\n\nTherefore, the statement is **Correct**.\n\n**D. In imbalanced datasets, per-class normalization should be applied at validation time using the true labels because it always improves generalization.**\n\nThis statement advocates a flawed methodology. Applying any transformation at validation or test time that uses the true labels is a form of data leakage (or \"data snooping\"). The purpose of a validation set is to estimate how a model will perform on future, unseen data for which labels are unknown. Using the labels to process the features creates an artificially easy task for the classifier, leading to an overly optimistic and invalid estimate of its true generalization performance. A model trained this way has learned to exploit the leakage, not the underlying data patterns. When deployed on a true test set (where labels are unavailable for preprocessing), its performance would likely be poor. The claim that it \"always improves generalization\" is false; it invalidates the *estimation* of generalization, and will not improve performance on genuinely new data. As shown in the analysis for option C, it can even make the classes inseparable, thus decreasing accuracy from what might have been achievable.\n\nTherefore, the statement is **Incorrect**.\n\n**E. To avoid leakage, normalization parameters should be computed on the training split without conditioning on labels at validation, and the same fixed transform should be applied to validation and test data without inspecting their labels.**\n\nThis statement describes the canonical, correct procedure for applying preprocessing transformations in a machine learning workflow.\n1.  Fit the preprocessor (e.g., compute mean and standard deviation for a standardizer) on the **training data only**.\n2.  Apply the **fitted** transformation (i.e., using the parameters learned from the training data) to the training data, the validation data, and the test data.\nThis ensures that the validation and test sets remain \"unseen\" during the fitting process, and their evaluation provides an unbiased estimate of the model's generalization performance. It prevents any information from the validation/test sets from leaking into the model or evaluation procedure. This is precisely what Pipeline G describes a part of, and it is the standard best practice.\n\nTherefore, the statement is **Correct**.\n\nFinal tally of correct options: A, B, C, E.", "answer": "$$\\boxed{ABCE}$$", "id": "3111750"}]}