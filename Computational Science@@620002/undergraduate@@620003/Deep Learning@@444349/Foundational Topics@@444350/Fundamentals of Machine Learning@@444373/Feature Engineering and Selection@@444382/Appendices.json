{"hands_on_practices": [{"introduction": "Effective feature engineering often involves translating domain-specific knowledge into a quantitative format that a machine learning model can understand. This exercise [@problem_id:2389767] provides a classic example from computational biology: creating a feature to predict how strongly a protein binds to a specific DNA site. You will implement a method to score DNA sequences using a Position Weight Matrix (PWM), turning a raw sequence into a single, powerful numerical feature representing binding affinity.", "problem": "You are given a Position Weight Matrix (PWM) for a specific transcription factor and a background nucleotide distribution. For any deoxyribonucleic acid (DNA) sequence, define a single numerical feature equal to the predicted binding affinity computed as the maximum log-odds PWM score across all windows and both strands. Use the following precise mathematical definitions.\n\nLet the alphabet be the set of nucleotides $\\{A,C,G,T\\}$. Let $L$ denote the motif length, and let $P \\in \\mathbb{R}^{4 \\times L}$ be a matrix of column-wise probabilities with rows ordered as $(A,C,G,T)$ such that $\\sum_{b \\in \\{A,C,G,T\\}} P[b,i] = 1$ for each column index $i \\in \\{0,1,\\dots,L-1\\}$. Let $Q \\in \\mathbb{R}^{4}$ be a background distribution over nucleotides with $\\sum_{b \\in \\{A,C,G,T\\}} Q[b] = 1$ and $Q[b] > 0$ for all $b$.\n\nDefine the log-odds weight matrix $W \\in \\mathbb{R}^{4 \\times L}$ by\n$$\nW[b,i] = \\ln\\left(\\frac{P[b,i]}{Q[b]}\\right),\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nFor a DNA sequence $S$ of length $n$ with symbols in $\\{A,C,G,T\\}$, and for any window start $t \\in \\{0,1,\\dots,n-L\\}$, define the forward-orientation window score\n$$\n\\mathrm{score}_{\\mathrm{fwd}}(S,t) = \\sum_{j=0}^{L-1} W[S_{t+j}, j],\n$$\nand the reverse-orientation window score (scanning the reverse-complement strand)\n$$\n\\mathrm{score}_{\\mathrm{rev}}(S,t) = \\sum_{j=0}^{L-1} W[\\mathrm{comp}(S_{t+L-1-j}), j],\n$$\nwhere $\\mathrm{comp}(\\cdot)$ maps $A \\leftrightarrow T$ and $C \\leftrightarrow G$.\n\nA window $S[t..t+L-1]$ is valid if and only if all of its symbols are in $\\{A,C,G,T\\}$. For any sequence $S$, define the predicted binding affinity feature\n$$\n\\phi(S) = \\max\\left\\{ \\mathrm{score}_{\\mathrm{fwd}}(S,t), \\mathrm{score}_{\\mathrm{rev}}(S,t) \\,:\\, t \\in \\{0,\\dots,n-L\\}, \\, S[t..t+L-1]\\ \\text{valid} \\right\\}.\n$$\nIf there are no valid windows (including the case $n < L$), then set $\\phi(S) = -\\infty$.\n\nUse the following fixed parameters:\n- Motif length $L = 4$.\n- Background distribution $Q$ ordered as $(A,C,G,T)$:\n$$\nQ = [\\,0.3,\\,0.2,\\,0.2,\\,0.3\\,].\n$$\n- Position Weight Matrix $P$ with rows ordered as $(A,C,G,T)$ and columns indexed $i=0,1,2,3$:\n$$\nP = \\begin{bmatrix}\n0.7 & 0.1 & 0.1 & 0.1 \\\\\n0.1 & 0.1 & 0.1 & 0.1 \\\\\n0.1 & 0.7 & 0.7 & 0.1 \\\\\n0.1 & 0.1 & 0.1 & 0.7 \\\\\n\\end{bmatrix}.\n$$\n\nTest Suite. Compute $\\phi(S)$ for each of the following sequences, in the given order:\n1. $S_1 =$ TTTAGGTAA\n2. $S_2 =$ AG\n3. $S_3 =$ GGGACCTCCC\n4. $S_4 =$ NAGGTN\n5. $S_5 =$ CCCCCCCC\n\nAll scanning must adhere to the definitions above. Windows containing any character outside $\\{A,C,G,T\\}$ are invalid and must be ignored. If no valid window exists for a sequence, output $-\\infty$ for that sequence.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and in the same order as the test cases, for example, $[\\phi(S_1),\\phi(S_2),\\phi(S_3),\\phi(S_4),\\phi(S_5)]$. The values must be real numbers in natural logarithm units, with $-\\infty$ allowed when applicable. No additional text should be printed.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It is scientifically grounded in the principles of bioinformatics, specifically in the modeling of transcription factor binding sites. All parameters, definitions, and mathematical formulations are well-posed, complete, and unambiguous. There are no contradictions, factual errors, or anachronisms with established scientific theory. Therefore, a definitive solution can be derived.\n\nThe task is to compute a numerical feature, designated $\\phi(S)$, for several given deoxyribonucleic acid (DNA) sequences. This feature represents the predicted binding affinity of a transcription factor to the sequence and is defined as the maximum log-odds score obtained by scanning the sequence with a Position Weight Matrix (PWM) on both the forward and reverse-complement strands.\n\nThe fundamental components of this problem are the alphabet of nucleotides $\\mathcal{A} = \\{A, C, G, T\\}$, a motif of length $L=4$, a position-specific probability matrix $P$, and a background nucleotide distribution $Q$.\n\nThe given parameters are:\n- Motif length $L=4$.\n- Background distribution $Q$ for bases $(A,C,G,T)$:\n$$\nQ = [\\,0.3,\\,0.2,\\,0.2,\\,0.3\\,].\n$$\n- Position Weight Matrix $P$ with rows for $(A,C,G,T)$ and columns for positions $i \\in \\{0, 1, 2, 3\\}$:\n$$\nP = \\begin{bmatrix}\n0.7 & 0.1 & 0.1 & 0.1 \\\\\n0.1 & 0.1 & 0.1 & 0.1 \\\\\n0.1 & 0.7 & 0.7 & 0.1 \\\\\n0.1 & 0.1 & 0.1 & 0.7 \\\\\n\\end{bmatrix}.\n$$\n\nThe first step is to construct the log-odds weight matrix $W$ from $P$ and $Q$. Each element $W[b,i]$ is calculated as the natural logarithm of the ratio of the probability of base $b$ at position $i$ in the motif to its background probability. The formula is:\n$$\nW[b,i] = \\ln\\left(\\frac{P[b,i]}{Q[b]}\\right).\n$$\nWe establish a mapping from nucleotide to row index: $A \\to 0$, $C \\to 1$, $G \\to 2$, $T \\to 3$. The resulting matrix $W$ is:\n$$\nW = \\begin{bmatrix}\n\\ln(0.7/0.3) & \\ln(0.1/0.3) & \\ln(0.1/0.3) & \\ln(0.1/0.3) \\\\\n\\ln(0.1/0.2) & \\ln(0.1/0.2) & \\ln(0.1/0.2) & \\ln(0.1/0.2) \\\\\n\\ln(0.1/0.2) & \\ln(0.7/0.2) & \\ln(0.7/0.2) & \\ln(0.1/0.2) \\\\\n\\ln(0.1/0.3) & \\ln(0.1/0.3) & \\ln(0.1/0.3) & \\ln(0.7/0.3) \\\\\n\\end{bmatrix}\n\\approx\n\\begin{bmatrix}\n 0.8473 & -1.0986 & -1.0986 & -1.0986 \\\\\n-0.6931 & -0.6931 & -0.6931 & -0.6931 \\\\\n-0.6931 &  1.2528 &  1.2528 & -0.6931 \\\\\n-1.0986 & -1.0986 & -1.0986 &  0.8473\n\\end{bmatrix}.\n$$\n\nThe second step is to process each sequence $S$ of length $n$. We slide a window of length $L=4$ across the sequence. For each starting position $t \\in \\{0, 1, \\dots, n-L\\}$, we consider the window $S[t..t+L-1]$. A window is valid only if it consists entirely of characters from the set $\\{A,C,G,T\\}$. For each valid window, we compute two scores:\n1.  The forward score, $\\mathrm{score}_{\\mathrm{fwd}}(S,t) = \\sum_{j=0}^{L-1} W[S_{t+j}, j]$.\n2.  The reverse score, $\\mathrm{score}_{\\mathrm{rev}}(S,t) = \\sum_{j=0}^{L-1} W[\\mathrm{comp}(S_{t+L-1-j}), j]$, where $\\mathrm{comp}(\\cdot)$ is the complement map ($A \\leftrightarrow T$, $C \\leftrightarrow G$). This is equivalent to scoring the reverse-complement of the window sequence against $W$.\n\nThe feature $\\phi(S)$ is the maximum value among all scores computed for all valid windows. If no valid windows exist (e.g., if $n<L$ or the sequence contains only invalid characters in all possible windows), $\\phi(S)$ is defined as $-\\infty$.\n\nWe now apply this procedure to the test suite. The maximum score is initialized to $-\\infty$.\n\n1.  $S_1 = \\text{TTTAGGTAA}$ ($n=9$): All windows of length $L=4$ are valid. We scan from $t=0$ to $t=5$. The maximum score is found at window $t=3$, which is the sequence `AGGT`. This sequence has a high probability according to matrix $P$.\n    -   $\\mathrm{score}_{\\mathrm{fwd}}(\\text{`AGGT`}) = W[A,0] + W[G,1] + W[G,2] + W[T,3] = \\ln(7/3) + \\ln(3.5) + \\ln(3.5) + \\ln(7/3) \\approx 4.19987$.\n    -   The reverse complement of `AGGT` is `ACCT`. Its score is $\\mathrm{score}_{\\mathrm{rev}} = W[A,0] + W[C,1] + W[C,2] + W[T,3] = 2\\ln(7/3) + 2\\ln(0.5) \\approx 0.3083$.\n    -   The maximum score for this window is $\\approx 4.19987$. After checking all other windows, this remains the maximum. Thus, $\\phi(S_1) \\approx 4.19987$.\n\n2.  $S_2 = \\text{AG}$ ($n=2$): The sequence length $n=2$ is less than the motif length $L=4$. No windows of length $4$ can be formed. As per the problem definition, $\\phi(S_2) = -\\infty$.\n\n3.  $S_3 = \\text{GGGACCTCCC}$ ($n=10$): All windows are valid. Scanning from $t=0$ to $t=6$, we find the maximum score at window $t=3$, which is the sequence `ACCT`.\n    -   The forward score for `ACCT` is $\\approx 0.3083$.\n    -   The reverse score for `ACCT` is the score of its reverse complement, `AGGT`, which is $\\approx 4.19987$.\n    -   This is the highest score found across all windows and both strands. Thus, $\\phi(S_3) \\approx 4.19987$.\n\n4.  $S_4 = \\text{NAGGTN}$ ($n=6$): This sequence contains the invalid character 'N'. We inspect windows of length $4$:\n    -   $t=0$: `NAGG`. Invalid.\n    -   $t=1$: `AGGT`. Valid.\n    -   $t=2$: `GGTN`. Invalid.\n    -   Only one valid window exists, `AGGT`, at $t=1$. The scores for this window are the same as calculated for $S_1$, with a maximum of $\\approx 4.19987$. Thus, $\\phi(S_4) \\approx 4.19987$.\n\n5.  $S_5 = \\text{CCCCCCCC}$ ($n=8$): All windows are valid and identical, consisting of the sequence `CCCC`.\n    -   The forward score for `CCCC` is $\\mathrm{score}_{\\mathrm{fwd}} = \\sum_{j=0}^{3} W[C,j] = 4 \\times \\ln(0.1/0.2) = 4\\ln(0.5) \\approx -2.7726$.\n    -   The reverse complement of `CCCC` is `GGGG`. The reverse score is the score of `GGGG`: $\\mathrm{score}_{\\mathrm{rev}} = W[G,0] + W[G,1] + W[G,2] + W[G,3] = \\ln(0.5) + \\ln(3.5) + \\ln(3.5) + \\ln(0.5) = 2\\ln(0.5) + 2\\ln(3.5) = 2\\ln(1.75) \\approx 1.1192$.\n    -   The maximum of these two scores is $2\\ln(1.75)$. Thus, $\\phi(S_5) \\approx 1.1192$.\n\nSummary of results:\n$\\phi(S_1) \\approx 4.199870335805202$\n$\\phi(S_2) = -\\infty$\n$\\phi(S_3) \\approx 4.199870335805202$\n$\\phi(S_4) \\approx 4.199870335805202$\n$\\phi(S_5) \\approx 1.119228952482337$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predicted binding affinity for a set of DNA sequences\n    based on a log-odds Position Weight Matrix (PWM) score.\n    \"\"\"\n    # Define fixed parameters as specified in the problem statement.\n    L = 4\n    P = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # Row for 'A'\n        [0.1, 0.1, 0.1, 0.1],  # Row for 'C'\n        [0.1, 0.7, 0.7, 0.1],  # Row for 'G'\n        [0.1, 0.1, 0.1, 0.7]   # Row for 'T'\n    ])\n    Q = np.array([0.3, 0.2, 0.2, 0.3]) # Background for A, C, G, T\n\n    # Calculate the log-odds weight matrix W.\n    # The division is broadcasted: each column of P is divided by the vector Q.\n    # Q[:, np.newaxis] reshapes Q to a column vector for division.\n    W = np.log(P / Q[:, np.newaxis])\n\n    # Helper dictionaries for mapping and sequence manipulation.\n    nuc_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    comp_map = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    valid_nucs = set(nuc_to_idx.keys())\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        \"TTTAGGTAA\",\n        \"AG\",\n        \"GGGACCTCCC\",\n        \"NAGGTN\",\n        \"CCCCCCCC\"\n    ]\n\n    results = []\n    for S in test_cases:\n        n = len(S)\n        max_score = -np.inf\n\n        # Iterate through all possible window starting positions.\n        # The loop range will be empty if n < L, correctly handling such cases.\n        for t in range(n - L + 1):\n            window = S[t:t+L]\n            \n            # A window is valid only if all its characters are in {A, C, G, T}.\n            if not all(c in valid_nucs for c in window):\n                continue\n\n            # Calculate the forward score for the window.\n            fwd_score = sum(W[nuc_to_idx[window[j]], j] for j in range(L))\n            \n            # Calculate the reverse-complement score. This is done by scoring\n            # the reverse complement of the window against the PWM.\n            rev_comp_window = \"\".join(comp_map[c] for c in reversed(window))\n            rev_score = sum(W[nuc_to_idx[rev_comp_window[j]], j] for j in range(L))\n\n            # Update the maximum score found so far for the sequence.\n            current_max = max(fwd_score, rev_score)\n            if current_max > max_score:\n                max_score = current_max\n        \n        results.append(max_score)\n\n    # Final print statement in the exact required format.\n    # The str() function correctly converts float('-inf') to '-inf'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2389767"}, {"introduction": "Raw features are rarely perfect; they often suffer from outliers or come from distributions with heavy tails that can mislead machine learning algorithms. This practice [@problem_id:3124173] focuses on quantile normalization, a powerful rank-based technique to make features more robust. Through a guided simulation, you will see firsthand how this transformation can stabilize feature selection methods that are otherwise sensitive to extreme values, a crucial skill for building reliable models.", "problem": "You are asked to design and implement a complete program that examines how quantile normalization of features affects feature selection in the presence of heavy-tailed distributions and injected outliers. The program must generate synthetic data, perform feature selection using empirical Pearson correlation, apply quantile normalization as a rank-based transform to a standard normal distribution, and measure two quantities for each test case: the shift in the selected feature set caused by quantile normalization under outliers and the gain in robustness to outliers due to quantile normalization, quantified by changes in set similarity with respect to a clean baseline.\n\nThe fundamental base for this task consists of the following well-tested definitions and facts:\n\n- Empirical Pearson correlation between two real-valued variables is a measure of linear association defined from observations and is sensitive to extreme values.\n- Rank-based quantile normalization uses empirical ranks to transform each feature to a target distribution via the inverse cumulative distribution function, reducing the leverage of extreme values while preserving order relations.\n- Heavy-tailed distributions generate extreme observations more frequently than light-tailed distributions, altering empirical moments in finite samples. The Student's t-distribution with low degrees of freedom is a canonical heavy-tailed model.\n- Set similarity can be quantified using the Jaccard index, which compares intersection and union cardinalities and yields a real number in the interval $[0,1]$.\n\nYou must adhere to the following mathematically precise specification:\n\n1. Data generation:\n   - Let $n$ denote the number of samples and let $d$ denote the number of features. Draw the feature matrix $X \\in \\mathbb{R}^{n \\times d}$ with entries independently sampled from a Student's t-distribution with degrees of freedom $\\nu$. This distribution is heavy-tailed for small $\\nu$.\n   - Choose a subset of indices $S_{\\text{true}} \\subset \\{0,1,\\dots,d-1\\}$ of size $s$ to be the signal features.\n   - Let $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{s}$ denote fixed coefficients sampled once for each test case.\n   - Define the target as $y = X_{:,S_{\\text{true}}}\\boldsymbol{\\alpha} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independently of $X$.\n   - Produce two versions of the feature matrix: a clean version $X_{\\text{clean}}$ and an outlier-corrupted version $X_{\\text{out}}$. For outlier corruption, for each entry independently, with probability $p$ add a perturbation of the form $m \\cdot u$, where $m > 0$ is the outlier magnitude and $u \\in \\{-1,+1\\}$ is a random sign.\n\n2. Quantile normalization:\n   - For each feature (each column of $X$), compute empirical ranks $r_i \\in \\{1,2,\\dots,n\\}$ of its values. Map each rank to a probability $q_i = (r_i - 0.5)/n \\in (0,1)$.\n   - Transform to a standard normal distribution by $z_i = \\Phi^{-1}(q_i)$ where $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution. Replace the original feature values with $z_i$ for that feature. Apply this transform independently to each feature.\n\n3. Feature selection:\n   - For a specified integer $k$ with $1 \\le k \\le d$, compute the absolute empirical Pearson correlation $\\rho_j = \\left|\\text{corr}(X_{:,j}, y)\\right|$ for each feature index $j \\in \\{0,1,\\dots,d-1\\}$.\n   - Select the set $S_k$ of $k$ feature indices corresponding to the largest $k$ values of $\\rho_j$, with ties broken by increasing index.\n\n4. Evaluation metrics:\n   - Define the shift due to quantile normalization under outliers as the size of the symmetric difference between the sets selected from outlier-corrupted data without and with quantile normalization, namely\n     $$\\Delta = \\left|S_k(X_{\\text{out}}) \\triangle S_k(\\text{QN}(X_{\\text{out}}))\\right|,$$\n     where $\\text{QN}(\\cdot)$ denotes per-feature quantile normalization and $\\triangle$ denotes symmetric difference.\n   - Define robustness to outliers via the Jaccard index between the selected set from clean data and from outlier data. Compute\n     $$J_{\\text{raw}} = \\frac{\\left|S_k(X_{\\text{clean}}) \\cap S_k(X_{\\text{out}})\\right|}{\\left|S_k(X_{\\text{clean}}) \\cup S_k(X_{\\text{out}})\\right|}, \\quad J_{\\text{qn}} = \\frac{\\left|S_k(\\text{QN}(X_{\\text{clean}})) \\cap S_k(\\text{QN}(X_{\\text{out}}))\\right|}{\\left|S_k(\\text{QN}(X_{\\text{clean}})) \\cup S_k(\\text{QN}(X_{\\text{out}}))\\right|}.$$\n     Report the robustness gain $G = J_{\\text{qn}} - J_{\\text{raw}}$.\n\n5. Program output:\n   - For each test case, output a list $[\\Delta, G]$ containing the symmetric difference size $\\Delta$ as an integer and the robustness gain $G$ as a float.\n   - Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each element of this list must be the two-element list $[\\Delta, G]$ for one test case, e.g., $\\left[[\\Delta_1,G_1],[\\Delta_2,G_2],\\dots\\right]$.\n\nUse the following test suite. For all test cases, ensure that the random generation is reproducible by using the provided seeds. All quantities are dimensionless, and angles are not involved.\n\n- Test case $1$ (general case):\n  - $n=200$, $d=20$, $k=5$, $\\nu=3$, $p=0.05$, $m=25.0$, $\\sigma=1.0$, $s=5$, seed $=42$.\n- Test case $2$ (heavier tails and more outliers):\n  - $n=200$, $d=20$, $k=5$, $\\nu=1$, $p=0.10$, $m=50.0$, $\\sigma=1.0$, $s=5$, seed $=123$.\n- Test case $3$ (no outliers for baseline comparison):\n  - $n=200$, $d=20$, $k=5$, $\\nu=3$, $p=0.0$, $m=0.0$, $\\sigma=1.0$, $s=5$, seed $=7$.\n- Test case $4$ (small sample size and pronounced outliers):\n  - $n=50$, $d=20$, $k=5$, $\\nu=2$, $p=0.10$, $m=40.0$, $\\sigma=1.0$, $s=5$, seed $=2024$.\n- Test case $5$ (higher dimensional feature space):\n  - $n=300$, $d=100$, $k=10$, $\\nu=5$, $p=0.05$, $m=30.0$, $\\sigma=1.0$, $s=8$, seed $=314159$.\n\nYour program must implement the steps above precisely and output a single line of the form $\\left[[\\Delta_1,G_1],[\\Delta_2,G_2],[\\Delta_3,G_3],[\\Delta_4,G_4],[\\Delta_5,G_5]\\right]$ with numerical values computed from your implementation.", "solution": "The problem is valid. It presents a well-defined and scientifically grounded task in computational statistics to evaluate the impact of quantile normalization on feature selection robustness. The problem is self-contained, with all parameters and procedures specified, allowing for a unique and reproducible solution. The minor ambiguity regarding the sampling of signal coefficients and selection of true feature indices is resolved by adopting standard practices in synthetic data generation, which does not compromise the problem's integrity. These assumptions are that the signal coefficients $\\boldsymbol{\\alpha}$ are drawn from a standard normal distribution and the set of true signal features $S_{\\text{true}}$ is chosen uniformly at random.\n\nThe solution is implemented by following the sequence of steps specified in the problem statement. The process for each test case involves data generation, feature transformation, feature selection, and the calculation of evaluation metrics.\n\n**1. Data Generation**\n\nThe foundation of this analysis is a synthetic dataset designed to model the challenges of real-world data, namely heavy-tailed distributions and the presence of outliers. For each test case, we generate a feature matrix $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of samples and $d$ is the number of features.\n\n-   **Base Feature Distribution**: The entries of the clean feature matrix, $X_{\\text{clean}}$, are independent and identically distributed samples from a Student's t-distribution with $\\nu$ degrees of freedom. This distribution is chosen for its heavy tails, which, for small $\\nu$, produce extreme values more frequently than a normal distribution, thus challenging statistical methods sensitive to such values.\n-   **Target Variable**: A ground-truth relationship between features and a target variable $y$ is established. A subset of $s$ features, indexed by the set $S_{\\text{true}}$, is randomly chosen to be the \"signal\" features. Their corresponding coefficients, $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{s}$, are sampled from a standard normal distribution $\\mathcal{N}(0, 1)$. The target $y$ is then constructed as a linear combination of these signal features, with additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$:\n    $$y = X_{\\text{clean},:,S_{\\text{true}}}\\boldsymbol{\\alpha} + \\varepsilon$$\n-   **Outlier Corruption**: To simulate data corruption, an outlier-injected matrix $X_{\\text{out}}$ is created from $X_{\\text{clean}}$. Each element of $X_{\\text{clean}}$ is corrupted with a probability $p$. A corrupted element has an outlier term $m \\cdot u$ added to it, where $m > 0$ is a large magnitude and $u$ is a random sign, either $+1$ or $-1$. This process introduces sparse, high-leverage outliers that can severely distort statistical measures like Pearson correlation.\n\n**2. Quantile Normalization**\n\nQuantile normalization is a non-linear transformation designed to mitigate the influence of a feature's original distribution, particularly extreme values. The procedure is applied independently to each feature column.\n\n1.  For each feature vector, the ranks $r_i$ of its $n$ data points are computed, ranging from $1$ to $n$.\n2.  These ranks are converted into empirical quantiles using the formula $q_i = \\frac{r_i - 0.5}{n}$. This maps the ranks to the interval $(0, 1)$.\n3.  Each quantile $q_i$ is then mapped to a value $z_i$ from a target distribution, which in this case is the standard normal distribution $\\mathcal{N}(0, 1)$. This is achieved by applying the inverse cumulative distribution function (CDF) of the standard normal distribution, also known as the probit function: $z_i = \\Phi^{-1}(q_i)$.\n\nThe resulting feature vector consists of values $\\{z_1, z_2, \\dots, z_n\\}$, which follow a standard normal distribution by construction. This rank-based approach preserves the order of the data points but replaces their magnitudes, effectively neutralizing the impact of outliers.\n\n**3. Feature Selection**\n\nThe feature selection method is based on the empirical Pearson correlation coefficient, which measures linear association.\n\n-   For a given feature matrix (e.g., $X_{\\text{out}}$ or $\\text{QN}(X_{\\text{out}})$) and the fixed target $y$, the absolute Pearson correlation $\\rho_j = |\\text{corr}(X_{:,j}, y)|$ is calculated for each feature $j \\in \\{0, 1, \\dots, d-1\\}$.\n-   The $k$ features with the highest $\\rho_j$ values are selected. To ensure a deterministic outcome, ties in correlation values are resolved by selecting the feature with the smaller index. This is implemented using a lexicographical sort on the primary key (descending absolute correlation) and the secondary key (ascending feature index).\n\n**4. Evaluation Metrics**\n\nTwo metrics are computed to quantify the effect of quantile normalization.\n\n-   **Shift due to Normalization ($\\Delta$)**: This metric captures how much the set of selected features changes when quantile normalization is applied to the outlier-corrupted data. It is the size of the symmetric difference between the feature set selected from the raw outlier data, $S_k(X_{\\text{out}})$, and the set selected from the normalized outlier data, $S_k(\\text{QN}(X_{\\text{out}}))$:\n    $$\\Delta = \\left|S_k(X_{\\text{out}}) \\triangle S_k(\\text{QN}(X_{\\text{out}}))\\right|$$\n-   **Robustness Gain ($G$)**: This metric assesses whether quantile normalization improves the stability of the feature selection process in the presence of outliers. It compares the consistency of feature selection between clean and outlier-corrupted data, with and without normalization. This consistency is measured by the Jaccard index, $J(A, B) = |A \\cap B| / |A \\cup B|$.\n    -   The Jaccard index for raw data is $J_{\\text{raw}} = J(S_k(X_{\\text{clean}}), S_k(X_{\\text{out}}))$.\n    -   The Jaccard index for quantile-normalized data is $J_{\\text{qn}} = J(S_k(\\text{QN}(X_{\\text{clean}})), S_k(\\text{QN}(X_{\\text{out}})))$.\n    -   The robustness gain is the difference: $G = J_{\\text{qn}} - J_{\\text{raw}}$. A positive value of $G$ indicates that quantile normalization makes the feature selection process more robust to outliers.\n\nThe overall program iterates through each test case, performs these steps using a specified random seed for reproducibility, and collects the resulting $[\\Delta, G]$ pairs for final output.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, norm, rankdata\n\ndef quantile_normalize(X):\n    \"\"\"\n    Applies quantile normalization to each column of X, mapping to a standard\n    normal distribution.\n    \"\"\"\n    n_samples = X.shape[0]\n    ranks = rankdata(X, axis=0)  # Ranks are from 1 to n\n    quantiles = (ranks - 0.5) / n_samples\n    X_norm = norm.ppf(quantiles)\n    return X_norm\n\ndef select_features(X, y, k):\n    \"\"\"\n    Selects top k features based on absolute Pearson correlation with y.\n    Ties are broken by selecting the smaller feature index.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Vectorized computation of Pearson correlation\n    X_c = X - np.mean(X, axis=0)\n    y_c = y - np.mean(y)\n\n    # (n-1) * cov = sum((X_i - mu_X) * (y - mu_y)) = X_c.T @ y_c\n    cov_xy = X_c.T @ y_c\n    \n    std_X = np.std(X, axis=0, ddof=1)\n    std_y = np.std(y, ddof=1)\n    \n    corr = np.zeros(n_features)\n    # Mask to avoid division by zero for constant features or if y is constant\n    valid_mask = (std_X > 0) & (std_y > 0)\n    if np.any(valid_mask):\n      denominator = std_X[valid_mask] * std_y\n      corr[valid_mask] = (cov_xy[valid_mask] / (n_samples - 1)) / denominator\n    \n    abs_corr = np.abs(corr)\n    \n    # Tie-breaking: sort by descending abs_corr, then ascending index.\n    # np.lexsort sorts by the last key first.\n    indices = np.arange(n_features)\n    sorted_indices = np.lexsort((indices, -abs_corr))\n    \n    top_k_indices = sorted_indices[:k]\n    \n    return set(top_k_indices)\n\ndef jaccard_index(set1, set2):\n    \"\"\"Computes the Jaccard index between two sets.\"\"\"\n    intersection_size = len(set1.intersection(set2))\n    union_size = len(set1.union(set2))\n    if union_size == 0:\n        return 1.0\n    return intersection_size / union_size\n\ndef run_simulation(n, d, k, nu, p, m, sigma, s, seed):\n    \"\"\"\n    Runs one full simulation for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    # Generate clean feature matrix X from Student's t-distribution\n    X_clean = t.rvs(df=nu, size=(n, d), random_state=rng)\n    \n    # Choose true signal features and coefficients\n    S_true_indices = rng.choice(d, size=s, replace=False)\n    alpha_coeffs = rng.standard_normal(size=s)\n    \n    # Generate target variable y\n    signal = X_clean[:, S_true_indices] @ alpha_coeffs\n    noise = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = signal + noise\n    \n    # Generate outlier-corrupted data\n    X_out = X_clean.copy()\n    outlier_mask = rng.random(size=(n, d)) < p\n    signs = rng.choice([-1, 1], size=(n, d))\n    X_out[outlier_mask] += signs[outlier_mask] * m\n\n    # 2. Quantile Normalization\n    X_clean_qn = quantile_normalize(X_clean)\n    X_out_qn = quantile_normalize(X_out)\n\n    # 3. Feature Selection for all four data versions\n    S_clean = select_features(X_clean, y, k)\n    S_out = select_features(X_out, y, k)\n    S_clean_qn = select_features(X_clean_qn, y, k)\n    S_out_qn = select_features(X_out_qn, y, k)\n\n    # 4. Evaluation Metrics\n    # Shift Delta\n    delta = len(S_out.symmetric_difference(S_out_qn))\n    \n    # Robustness Gain G\n    J_raw = jaccard_index(S_clean, S_out)\n    J_qn = jaccard_index(S_clean_qn, S_out_qn)\n    G = J_qn - J_raw\n    \n    return [delta, G]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, d, k, nu, p, m, sigma, s, seed)\n        (200, 20, 5, 3, 0.05, 25.0, 1.0, 5, 42),\n        (200, 20, 5, 1, 0.10, 50.0, 1.0, 5, 123),\n        (200, 20, 5, 3, 0.0, 0.0, 1.0, 5, 7),\n        (50, 20, 5, 2, 0.10, 40.0, 1.0, 5, 2024),\n        (300, 100, 10, 5, 0.05, 30.0, 1.0, 8, 314159),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format the output string precisely as [[d1,g1],[d2,g2],...]\n    formatted_results = [f\"[{d},{g}]\" for d, g in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3124173"}, {"introduction": "A feature that seems too good to be true often is. This hands-on practice [@problem_id:3124151] addresses the critical pitfall of label leakage, where features are inadvertently contaminated with information from the target variable, leading to deceptively high model performance. You will implement a complete detection pipeline that uses permutation importance and statistical sentinels to flag these problematic features, providing you with an essential tool for debugging and validating your machine learning workflows.", "problem": "You will implement a complete, deterministic procedure to detect label leakage in feature engineering for a binary classifier by intentionally adding leaked features, performing feature selection, computing permutation importances, and applying a safeguard that penalizes suspiciously high importances. The program must be self-contained and produce results for a given test suite. All mathematical symbols in this problem are specified in LaTeX, and every symbol, variable, function, operator, and number is written using inline math mode.\n\nThe foundational base of this problem is the definition of Empirical Risk Minimization (ERM) for training, Binary Cross-Entropy (BCE) for measuring classification error, and permutation-based importance for feature relevance, all of which are well-tested principles in machine learning.\n\nConstruct a synthetic binary classification dataset as follows. For each test case, there are $n$ samples and three feature blocks:\n- Base features of dimension $d_b$ that are informative for the label.\n- Noise sentinel features of dimension $d_n$ that are independent of the label.\n- Leaked features of dimension $d_\\ell$ that are intentionally constructed to carry direct information about the label.\n\nData generation:\n1. Draw base features $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$ independently as $\\mathbf{x}_i^b \\sim \\mathcal{N}(\\mathbf{0}, I)$ for each sample index $i \\in \\{1,\\dots,n\\}$.\n2. Draw a ground-truth weight vector $\\mathbf{w}^\\star \\in \\mathbb{R}^{d_b}$ and bias $b^\\star \\in \\mathbb{R}$ deterministically from a pseudorandom seed, and define the logit $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$. Define the label probability $p_i = \\sigma(z_i)$ where $\\sigma(u) = \\frac{1}{1 + e^{-u}}$, and sample the binary label $y_i \\in \\{0,1\\}$ from a Bernoulli distribution with parameter $p_i$.\n3. Draw noise sentinel features $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$ independently as $\\mathbf{x}_i^n \\sim \\mathcal{N}(\\mathbf{0}, I)$, all independent of $\\mathbf{x}_i^b$ and $y_i$.\n4. Construct leaked features $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$ as $x_{i,j}^\\ell = y_i + \\eta_{i,j}$ for each leaked feature index $j \\in \\{1,\\dots,d_\\ell\\}$, where $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$ and $s_\\ell$ is a provided leak noise standard deviation. Smaller $s_\\ell$ yields stronger leakage.\n5. Form the complete feature vector $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$ with $d = d_b + d_n + d_\\ell$.\n\nSplit the dataset into training and validation sets, using a training fraction of $0.7$ so that the training set has $\\lfloor 0.7n \\rfloor$ samples and the validation set has $n - \\lfloor 0.7n \\rfloor$ samples. Standardize each feature dimension using the training set statistics: for each feature index $j \\in \\{1,\\dots,d\\}$, compute the training mean $m_j$ and standard deviation $s_j$, then transform both training and validation features by $\\tilde{x}_{i,j} = \\frac{x_{i,j} - m_j}{s_j}$.\n\nTrain a feedforward Artificial Neural Network (ANN) classifier with one hidden layer:\n- Hidden layer width is a fixed constant $h$.\n- Hidden activation is the Rectified Linear Unit (ReLU), $\\text{ReLU}(u) = \\max(0,u)$.\n- Output is a single logit with a logistic activation $\\sigma$ to produce $\\hat{y}_i \\in (0,1)$.\n- Minimize the Binary Cross-Entropy (BCE) empirical risk over the training set:\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right),\n$$\nusing full-batch gradient descent for a fixed number of epochs and a fixed learning rate, both specified in the program.\n\nCompute permutation importance on the validation set to quantify reliance on each feature coordinate $j$:\n1. Compute the baseline validation loss $L_0$ using the trained ANN and the unpermuted validation features.\n2. For each feature index $j \\in \\{1,\\dots,d\\}$, independently permute column $j$ of the validation features across samples to produce a perturbed validation set, compute the loss $L_{(j)}$, and define the importance:\n$$\nI_j = L_{(j)} - L_0.\n$$\nLarger $I_j$ indicates greater reliance on feature $j$.\n\nDefine a safeguard against leakage using the noise sentinel features as a reference null distribution:\n- Let $J_n \\subset \\{1,\\dots,d\\}$ denote the set of indices corresponding to the noise sentinel features (the block following the base features).\n- Compute the noise-reference mean and standard deviation of importances:\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j,\n\\qquad\n\\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2}.\n$$\n- Define a threshold $\\tau = \\mu_n + z \\sigma_n$ with a fixed $z$ and a penalty coefficient $\\lambda > 0$.\n- Define the penalized importance for each feature index $j$:\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau).\n$$\n\nPerform selection with and without the safeguard:\n- Unpenalized selection chooses the top-$k$ indices by descending $I_j$.\n- Penalized selection chooses the top-$k$ indices by descending $P_j$.\n- A feature index $j$ is flagged as suspicious if $I_j > \\tau$.\n- Define leakage detection as a boolean that is true if at least one suspiciously flagged feature would be selected by the unpenalized selection but is excluded by the penalized selection:\n$$\n\\text{detected} = \\left( \\left\\{ j \\mid I_j > \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset,\n$$\nwhere $\\text{TopK}(I)$ is the set of indices of the $k$ largest entries of $I$, and similarly for $\\text{TopK}(P)$.\n\nYour program must implement the above procedure and evaluate the following test suite. Each test case is a tuple containing $(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$:\n- Happy path with no leak: $(42, 600, 5, 5, 0, 0.0, 3)$\n- Mild leak: $(43, 600, 5, 5, 1, 0.9, 3)$\n- Strong leak: $(44, 600, 5, 5, 2, 0.05, 3)$\n- Boundary case (smaller sample, more noise sentinels, single strong leak): $(45, 200, 5, 10, 1, 0.1, 3)$\n\nUse fixed hyperparameters for the ANN: hidden width $h = 12$, learning rate $\\alpha = 0.1$, number of epochs $300$. Use $z = 3$ and $\\lambda = 0.8$ for the safeguard. All random operations must use the provided seed per test case to ensure determinism.\n\nFinal output format:\nYour program should produce a single line of output containing the detection booleans for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,b_3,b_4]$), where each $b_i$ is either $True$ or $False$.", "solution": "The user-provided problem has been critically validated and is determined to be sound. It is scientifically grounded in established machine learning principles, is well-posed with a clear, deterministic procedure, and is expressed objectively. Therefore, a complete solution is provided.\n\nThe objective is to construct a deterministic program to detect label leakage in feature engineering. This is achieved by creating a synthetic dataset containing intentionally leaked features, training a neural network classifier, evaluating feature importance using permutations, and applying a statistical safeguard to identify and penalize features with suspiciously high importance relative to known-uninformative features.\n\nThe process is structured as follows:\n\nFirst, a synthetic binary classification dataset is generated for each test case. Each test case is defined by a tuple of parameters: $(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$, which control the random seed, number of samples, and dimensions of the feature blocks. All random operations are seeded to ensure determinism.\n-   Informative base features, $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$, are drawn from a standard normal distribution, $\\mathcal{N}(\\mathbf{0}, I)$.\n-   A ground-truth linear model, $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$, is defined using a deterministically generated weight vector $\\mathbf{w}^\\star$ and bias $b^\\star$. The true probability for the positive class is given by the logistic function, $p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$. Binary labels $y_i \\in \\{0, 1\\}$ are then sampled from a Bernoulli distribution with parameter $p_i$.\n-   Uninformative noise sentinel features, $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$, are drawn from $\\mathcal{N}(\\mathbf{0}, I)$, and are statistically independent of both the base features and the labels.\n-   Potentially leaked features, $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$, are constructed by directly corrupting the label with Gaussian noise: $x_{i,j}^\\ell = y_i + \\eta_{i,j}$, where $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$. The parameter $s_\\ell$ controls the strength of the leak; a smaller $s_\\ell$ implies a stronger, more obvious leak. If $d_\\ell=0$, this block is empty.\n-   The final feature vector for each sample $i$ is the concatenation $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$, where the total dimension is $d = d_b + d_n + d_\\ell$.\n\nNext, the data is preprocessed. The dataset is chronologically split into a training set containing the first $\\lfloor 0.7n \\rfloor$ samples and a validation set with the remaining $n - \\lfloor 0.7n \\rfloor$ samples. To prevent information leakage from the validation set, feature standardization is performed by computing the mean $m_j$ and standard deviation $s_j$ for each feature $j$ on the training set *only*. These statistics are then used to transform both the training and validation data according to $\\tilde{x}_{i,j} = (x_{i,j} - m_j) / s_j$. A small epsilon is added to $s_j$ to prevent division by zero for constant features.\n\nAn Artificial Neural Network (ANN) is then trained on the standardized training data. The network has a single hidden layer of a fixed width $h=12$ with the Rectified Linear Unit (ReLU) activation function, $\\text{ReLU}(u) = \\max(0,u)$. The output layer consists of a single neuron with a logistic activation function $\\sigma(u)$ to predict the probability $\\hat{y}_i$. The model is trained to minimize the Binary Cross-Entropy (BCE) loss over the training set:\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right).\n$$\nTraining is performed using full-batch gradient descent for $300$ epochs with a learning rate of $\\alpha = 0.1$.\n\nAfter training, feature importance is quantified using the permutation importance method on the validation set. This method measures the model's reliance on each feature.\n-   First, the baseline loss, $L_0$, is computed on the original (unpermuted) validation set.\n-   Then, for each feature index $j \\in \\{1, \\dots, d\\}$, the values in the $j$-th column of the validation feature matrix are randomly permuted. The loss $L_{(j)}$ is re-computed on this perturbed dataset.\n-   The permutation importance for feature $j$ is defined as the increase in loss: $I_j = L_{(j)} - L_0$. A large positive value of $I_j$ indicates that the model relies heavily on feature $j$ to make accurate predictions.\n\nA statistical safeguard is implemented to distinguish legitimate high-importance features from suspiciously important ones that may indicate leakage. This safeguard uses the noise sentinel features as a null reference.\n-   The mean $\\mu_n$ and standard deviation $\\sigma_n$ of the importances are calculated exclusively from the set of noise sentinel features:\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j, \\qquad \\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2},\n$$\nwhere $J_n$ is the set of indices for the noise features.\n-   A statistical threshold $\\tau$ is defined as $\\tau = \\mu_n + z \\sigma_n$, using a z-score multiplier of $z=3$. Any feature $j$ with an importance $I_j > \\tau$ is flagged as \"suspicious,\" as its importance is significantly greater than would be expected for an uninformative feature.\n-   A penalized importance score, $P_j$, is then computed for each feature to discount the importance of any suspicious feature:\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau),\n$$\nwith a penalty coefficient of $\\lambda = 0.8$. This penalty reduces the score of features whose importance exceeds the threshold $\\tau$, making them less likely to be selected.\n\nFinally, the leakage detection logic is applied by comparing feature selections made with and without the safeguard.\n-   The unpenalized selection, $\\text{TopK}(I)$, is the set of indices of the top-$k$ features as ranked by the original importances $I_j$.\n-   The penalized selection, $\\text{TopK}(P)$, is the set of indices of the top-$k$ features as ranked by the penalized importances $P_j$.\n-   Leakage is detected (evaluates to True) if there exists at least one feature that is (a) flagged as suspicious (i.e., $I_j > \\tau$), (b) is part of the unpenalized top-$k$ selection, and (c) is excluded from the penalized top-$k$ selection. Formally, detection occurs if the following set is non-empty:\n$$\n\\left( \\left\\{ j \\mid I_j > \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset.\n$$\nThe program executes this entire procedure for each test case in the suite and reports the final boolean detection result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic procedure to detect label leakage in feature engineering.\n    \"\"\"\n\n    test_cases = [\n        # (seed, n, d_b, d_n, d_ell, s_ell, k)\n        (42, 600, 5, 5, 0, 0.0, 3),   # Happy path with no leak\n        (43, 600, 5, 5, 1, 0.9, 3),   # Mild leak\n        (44, 600, 5, 5, 2, 0.05, 3),  # Strong leak\n        (45, 200, 5, 10, 1, 0.1, 3), # Boundary case\n    ]\n\n    results = []\n    \n    # Fixed hyperparameters\n    h = 12\n    learning_rate = 0.1\n    epochs = 300\n    z_score_threshold = 3.0\n    penalty_lambda = 0.8\n    train_frac = 0.7\n\n    for case in test_cases:\n        seed, n, d_b, d_n, d_ell, s_ell, k = case\n        \n        # Set seed for determinism in all random operations\n        np.random.seed(seed)\n        \n        # --- 1. Data Generation ---\n        # Ground-truth weights and bias\n        w_star = np.random.randn(d_b, 1)\n        b_star = np.random.randn(1)\n        \n        # Base features\n        X_b = np.random.randn(n, d_b)\n        \n        # Labels\n        z_logits = X_b @ w_star + b_star\n        p_labels = 1 / (1 + np.exp(-z_logits))\n        y = (np.random.rand(n, 1) < p_labels).astype(float)\n        \n        # Noise sentinel features\n        X_n = np.random.randn(n, d_n)\n        \n        # Leaked features\n        if d_ell > 0:\n            eta = np.random.randn(n, d_ell) * s_ell\n            X_ell = y + eta\n        else:\n            X_ell = np.empty((n, 0))\n            \n        # Concatenate all feature blocks\n        X = np.hstack([X_b, X_n, X_ell])\n        d = d_b + d_n + d_ell\n\n        # --- 2. Data Preprocessing ---\n        n_train = int(np.floor(train_frac * n))\n        X_train, X_val = X[:n_train], X[n_train:]\n        y_train, y_val = y[:n_train], y[n_train:]\n\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0)\n        # Add epsilon for numerical stability if a feature has zero standard deviation\n        train_std[train_std == 0] = 1e-8\n        \n        X_train_std = (X_train - train_mean) / train_std\n        X_val_std = (X_val - train_mean) / train_std\n\n        # --- 3. ANN Model and Training ---\n        # Initialize weights with Xavier/Glorot scaling\n        W1 = np.random.randn(d, h) * np.sqrt(2.0 / (d + h))\n        b1 = np.zeros(h)\n        W2 = np.random.randn(h, 1) * np.sqrt(2.0 / (h + 1))\n        b2 = np.zeros(1)\n\n        def bce_loss(y_true, y_pred, epsilon=1e-9):\n            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n            return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n        # Full-batch gradient descent\n        for _ in range(epochs):\n            # Forward pass\n            z1 = X_train_std @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            y_hat = 1 / (1 + np.exp(-z2))\n\n            # Backward pass\n            d_z2 = (y_hat - y_train) / n_train\n            d_W2 = a1.T @ d_z2\n            d_b2 = np.sum(d_z2, axis=0)\n            \n            d_a1 = d_z2 @ W2.T\n            d_z1 = d_a1 * (z1 > 0)\n            d_W1 = X_train_std.T @ d_z1\n            d_b1 = np.sum(d_z1, axis=0)\n\n            # Update weights\n            W1 -= learning_rate * d_W1\n            b1 -= learning_rate * d_b1\n            W2 -= learning_rate * d_W2\n            b2 -= learning_rate * d_b2\n        \n        def predict(X_in):\n            z1 = X_in @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            return 1 / (1 + np.exp(-z2))\n\n        # --- 4. Permutation Importance ---\n        baseline_loss = bce_loss(y_val, predict(X_val_std))\n        importances = np.zeros(d)\n        \n        for j in range(d):\n            X_val_perm = X_val_std.copy()\n            # Shuffle in place using the global RNG stream\n            np.random.shuffle(X_val_perm[:, j])\n            permuted_loss = bce_loss(y_val, predict(X_val_perm))\n            importances[j] = permuted_loss - baseline_loss\n\n        # --- 5. Safeguard and Detection Logic ---\n        I = importances\n        noise_indices = range(d_b, d_b + d_n)\n        \n        if d_n > 1:\n            I_noise = I[noise_indices]\n            mu_n = np.mean(I_noise)\n            sigma_n = np.std(I_noise)\n        elif d_n == 1:\n            mu_n = I[noise_indices[0]]\n            sigma_n = 0.0 # Std of a single point is 0\n        else: # d_n == 0\n            mu_n = 0.0\n            sigma_n = 0.0 # Cannot compute threshold without sentinels\n\n        tau = mu_n + z_score_threshold * sigma_n\n        \n        # Penalized importance\n        P = I - penalty_lambda * np.maximum(0, I - tau)\n        \n        # Get top-k indices\n        topk_I_indices = np.argsort(I)[-k:][::-1]\n        topk_P_indices = np.argsort(P)[-k:][::-1]\n        \n        # Convert to sets for formal comparison\n        set_topk_I = set(topk_I_indices)\n        set_topk_P = set(topk_P_indices)\n        \n        # Identify suspicious features\n        suspicious_indices = {j for j, imp in enumerate(I) if imp > tau}\n        \n        # Find suspicious features that are selected by I but deselected by P\n        suspiciously_selected_by_I = suspicious_indices.intersection(set_topk_I)\n        deselected_by_P = suspiciously_selected_by_I.difference(set_topk_P)\n        \n        is_detected = len(deselected_by_P) > 0\n        results.append(is_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3124151"}]}