{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of the bias-variance tradeoff, we start with a foundational concept: Mean Squared Error ($MSE$). While we often intuitively prefer estimators that are correct on average (unbiased), the ultimate goal is to minimize the total prediction error. This introductory problem [@problem_id:1934147] provides a simple, yet powerful, illustration of a scenario where a deliberately biased estimator can achieve a lower $MSE$ than a standard, unbiased one, forcing us to look beyond just the bias and consider the full picture.", "problem": "In statistical estimation theory, we often compare different estimators for a parameter based on their performance. Consider a random variable $X$ that follows a Bernoulli distribution with parameter $p$, denoted as $X \\sim \\text{Bernoulli}(p)$, where $p$ is the probability of success ($X=1$) and $1-p$ is the probability of failure ($X=0$).\n\nSuppose we have only a single observation of $X$ to estimate the unknown parameter $p$. Two different estimators are proposed:\n\n1.  The first estimator, $\\hat{p}_1$, is simply the observed outcome: $\\hat{p}_1 = X$.\n2.  The second estimator, $\\hat{p}_2$, is a fixed constant value: $\\hat{p}_2 = 0.8$.\n\nThe quality of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is often measured by its Mean Squared Error (MSE), which is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$, where $E[\\cdot]$ denotes the expectation.\n\nAssuming the true value of the parameter is $p=0.9$, which of the two estimators has a lower MSE?\n\nA. The MSE of $\\hat{p}_1$ is lower.\n\nB. The MSE of $\\hat{p}_2$ is lower.\n\nC. The MSEs are equal.\n\nD. The comparison cannot be made with the given information.", "solution": "We are given a single observation $X \\sim \\text{Bernoulli}(p)$ and two estimators: $\\hat{p}_{1}=X$ and $\\hat{p}_{2}=0.8$. The Mean Squared Error is defined as $\\text{MSE}(\\hat{\\theta})=E[(\\hat{\\theta}-\\theta)^{2}]$. We compute the MSEs symbolically in terms of $p$ and then evaluate at $p=0.9$.\n\nFor $\\hat{p}_{1}=X$, using the bias-variance decomposition,\n$$\n\\text{MSE}(\\hat{p}_{1})= \\operatorname{Var}(X) + \\left(E[X]-p\\right)^{2}.\n$$\nFor $X \\sim \\text{Bernoulli}(p)$, $E[X]=p$ and $\\operatorname{Var}(X)=p(1-p)$. Thus,\n$$\n\\text{MSE}(\\hat{p}_{1})=p(1-p).\n$$\nAlternatively, directly:\n$$\n\\text{MSE}(\\hat{p}_{1})=E[(X-p)^{2}]=E[X^{2}] - 2pE[X] + p^{2} = E[X] - 2p^{2} + p^{2} = p - p^{2} = p(1-p).\n$$\n\nFor $\\hat{p}_{2}=0.8$, since it is a constant,\n$$\n\\text{MSE}(\\hat{p}_{2})=E[(0.8-p)^{2}]=(0.8-p)^{2}.\n$$\n\nNow evaluate at the true value $p=0.9$:\n$$\n\\text{MSE}(\\hat{p}_{1})=0.9(1-0.9)=0.09,\\qquad \\text{MSE}(\\hat{p}_{2})=(0.8-0.9)^{2}=0.01.\n$$\nSince $0.01 < 0.09$, the second estimator has the lower MSE. Therefore, the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1934147"}, {"introduction": "Building on the foundational concepts, we now move to ensembling, a powerful technique for improving predictive performance by combining multiple models. This practice [@problem_id:3180603] guides you through the process of decomposing the error of a stacked ensemble. By calculating the ensemble's bias and variance, you will discover that its performance depends not just on the quality of the individual learners, but critically on the correlation between their errors, providing a clear, quantitative rationale for why diversity among models is a key ingredient for a successful ensemble.", "problem": "Consider a supervised learning setting with a scalar response, where the data-generating process at a fixed input $x$ is $y = f(x) + \\varepsilon$, with $\\varepsilon$ a zero-mean random noise term independent of the training data and the learned predictors, and with variance $\\operatorname{Var}(\\varepsilon) = \\sigma^{2}$. Three base learners produce predictions $\\hat{f}_{1}(x)$, $\\hat{f}_{2}(x)$, and $\\hat{f}_{3}(x)$. Define the prediction error of learner $i$ at $x$ as $e_{i}(x) = \\hat{f}_{i}(x) - f(x)$, and its bias at $x$ as $b_{i}(x) = \\mathbb{E}[\\hat{f}_{i}(x)] - f(x) = \\mathbb{E}[e_{i}(x)]$. A stacked ensemble uses non-negative weights that sum to one, specifically $w = (w_{1}, w_{2}, w_{3}) = (0.5, 0.3, 0.2)$, producing the ensemble predictor $\\hat{f}_{w}(x) = \\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)$.\n\nAt the point $x$ under consideration, suppose the bias vector of the base learners is\n$$\nb(x) = \\begin{pmatrix} 0.20 \\\\ -0.10 \\\\ 0.05 \\end{pmatrix},\n$$\nand the covariance matrix of the base learners’ errors is\n$$\n\\Sigma(x) = \\begin{pmatrix}\n0.25 & 0.10 & 0.05 \\\\\n0.10 & 0.36 & 0.12 \\\\\n0.05 & 0.12 & 0.49\n\\end{pmatrix}.\n$$\nAssume the irreducible noise variance is $\\sigma^{2} = 0.04$.\n\nStarting from the fundamental definitions of bias as $\\mathbb{E}[\\hat{f}(x)] - f(x)$, variance as $\\operatorname{Var}(\\hat{f}(x))$, and the expected squared prediction error $\\mathbb{E}\\big[(\\hat{f}(x) - y)^{2}\\big]$, first derive symbolic expressions for the stacked ensemble’s bias and variance at $x$ in terms of $w$, $b(x)$, and $\\Sigma(x)$. Then, using the provided numerical values, compute the ensemble’s expected squared prediction error at $x$.\n\nExpress your final answer as a single real number. Round your answer to four significant figures. No units are required.", "solution": "The problem asks for the expected squared prediction error of a stacked ensemble predictor at a specific point $x$. The data-generating process is given by $y = f(x) + \\varepsilon$, where $\\varepsilon$ is a zero-mean noise term with variance $\\sigma^2$. The expected squared prediction error, $\\text{Error}(x)$, is defined as $\\mathbb{E}\\big[(\\hat{f}(x) - y)^{2}\\big]$.\n\nWe first decompose this error expression. Let $\\hat{f}_{w}(x)$ be the ensemble predictor.\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - y)^{2}\\big]\n$$\nSubstituting $y = f(x) + \\varepsilon$:\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - (f(x) + \\varepsilon))^{2}\\big] = \\mathbb{E}\\big[((\\hat{f}_{w}(x) - f(x)) - \\varepsilon)^{2}\\big]\n$$\nExpanding the square:\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2} - 2\\varepsilon(\\hat{f}_{w}(x) - f(x)) + \\varepsilon^{2}\\big]\n$$\nBy the linearity of expectation:\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2}\\big] - 2\\mathbb{E}\\big[\\varepsilon(\\hat{f}_{w}(x) - f(x))\\big] + \\mathbb{E}[\\varepsilon^{2}]\n$$\nThe predictor $\\hat{f}_{w}(x)$ is a function of the training data, which is independent of the noise term $\\varepsilon$ for a new observation. Thus, $\\mathbb{E}\\big[\\varepsilon(\\hat{f}_{w}(x) - f(x))\\big] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[\\hat{f}_{w}(x) - f(x)]$. Since $\\mathbb{E}[\\varepsilon]=0$, this term is zero.\nThe last term is $\\mathbb{E}[\\varepsilon^{2}] = \\operatorname{Var}(\\varepsilon) + (\\mathbb{E}[\\varepsilon])^2 = \\sigma^2 + 0^2 = \\sigma^2$.\n\nThe first term, $\\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2}\\big]$, is the Mean Squared Error (MSE) of the predictor. It can be further decomposed. For any random variable $Z$, $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2$. Let $Z = \\hat{f}_{w}(x) - f(x)$.\n$$\n\\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2}\\big] = \\operatorname{Var}(\\hat{f}_{w}(x) - f(x)) + (\\mathbb{E}[\\hat{f}_{w}(x) - f(x)])^2\n$$\nSince $f(x)$ is a fixed, non-random quantity, $\\operatorname{Var}(\\hat{f}_{w}(x) - f(x)) = \\operatorname{Var}(\\hat{f}_{w}(x))$. The term $\\mathbb{E}[\\hat{f}_{w}(x) - f(x)]$ is, by definition, the bias of the ensemble predictor.\nLet $\\text{Bias}(\\hat{f}_{w}(x)) = \\mathbb{E}[\\hat{f}_{w}(x)] - f(x)$.\nSo, the full decomposition is:\n$$\n\\text{Error}(x) = (\\text{Bias}(\\hat{f}_{w}(x)))^2 + \\operatorname{Var}(\\hat{f}_{w}(x)) + \\sigma^2\n$$\nThis is the bias-variance-noise decomposition. Our task is to calculate each of these three terms for the given ensemble.\n\nFirst, we derive the symbolic expression for the ensemble's bias. The ensemble predictor is $\\hat{f}_{w}(x) = \\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)$.\n$$\n\\text{Bias}(\\hat{f}_{w}(x)) = \\mathbb{E}\\left[\\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)\\right] - f(x)\n$$\nUsing linearity of expectation and that $\\sum_{i=1}^{3} w_i = 1$:\n$$\n\\text{Bias}(\\hat{f}_{w}(x)) = \\sum_{i=1}^{3} w_{i}\\,\\mathbb{E}[\\hat{f}_{i}(x)] - \\left(\\sum_{i=1}^{3} w_i\\right)f(x) = \\sum_{i=1}^{3} w_{i} (\\mathbb{E}[\\hat{f}_{i}(x)] - f(x))\n$$\nThis simplifies to a weighted sum of the individual biases, $b_i(x) = \\mathbb{E}[\\hat{f}_{i}(x)] - f(x)$:\n$$\n\\text{Bias}(\\hat{f}_{w}(x)) = \\sum_{i=1}^{3} w_{i}\\,b_{i}(x) = w^T b(x)\n$$\n\nNext, we derive the symbolic expression for the ensemble's variance.\n$$\n\\operatorname{Var}(\\hat{f}_{w}(x)) = \\operatorname{Var}\\left(\\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)\\right)\n$$\nUsing the general formula for the variance of a weighted sum of random variables:\n$$\n\\operatorname{Var}(\\hat{f}_{w}(x)) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} w_{i} w_{j} \\operatorname{Cov}(\\hat{f}_{i}(x), \\hat{f}_{j}(x))\n$$\nThe problem provides the covariance matrix of the base learners' errors, $\\Sigma(x)$, with entries $\\Sigma_{ij}(x) = \\operatorname{Cov}(e_i(x), e_j(x))$. Since $e_i(x) = \\hat{f}_i(x) - f(x)$ and $f(x)$ is a constant, $\\operatorname{Cov}(e_i(x), e_j(x)) = \\operatorname{Cov}(\\hat{f}_i(x), \\hat{f}_j(x))$. Therefore, $\\Sigma(x)$ is the covariance matrix of the predictors $\\hat{f}_i(x)$. The variance of the ensemble can be written in matrix form:\n$$\n\\operatorname{Var}(\\hat{f}_{w}(x)) = w^T \\Sigma(x) w\n$$\n\nNow, we substitute the given numerical values to compute the total error.\nThe weights are $w = \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix}$.\nThe bias vector is $b(x) = \\begin{pmatrix} 0.20 \\\\ -0.10 \\\\ 0.05 \\end{pmatrix}$.\nThe covariance matrix is $\\Sigma(x) = \\begin{pmatrix} 0.25 & 0.10 & 0.05 \\\\ 0.10 & 0.36 & 0.12 \\\\ 0.05 & 0.12 & 0.49 \\end{pmatrix}$.\nThe irreducible noise variance is $\\sigma^2 = 0.04$.\n\n1.  Calculate the squared bias of the ensemble:\n    $$\n    \\text{Bias}(\\hat{f}_{w}(x)) = w^T b(x) = \\begin{pmatrix} 0.5 & 0.3 & 0.2 \\end{pmatrix} \\begin{pmatrix} 0.20 \\\\ -0.10 \\\\ 0.05 \\end{pmatrix}\n    $$\n    $$\n    \\text{Bias}(\\hat{f}_{w}(x)) = (0.5)(0.20) + (0.3)(-0.10) + (0.2)(0.05) = 0.10 - 0.03 + 0.01 = 0.08\n    $$\n    $$\n    (\\text{Bias}(\\hat{f}_{w}(x)))^2 = (0.08)^2 = 0.0064\n    $$\n\n2.  Calculate the variance of the ensemble:\n    $$\n    \\operatorname{Var}(\\hat{f}_{w}(x)) = w^T \\Sigma(x) w\n    $$\n    We first compute the vector $\\Sigma(x)w$:\n    $$\n    \\Sigma(x)w = \\begin{pmatrix} 0.25 & 0.10 & 0.05 \\\\ 0.10 & 0.36 & 0.12 \\\\ 0.05 & 0.12 & 0.49 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} (0.25)(0.5) + (0.10)(0.3) + (0.05)(0.2) \\\\ (0.10)(0.5) + (0.36)(0.3) + (0.12)(0.2) \\\\ (0.05)(0.5) + (0.12)(0.3) + (0.49)(0.2) \\end{pmatrix}\n    $$\n    $$\n    \\Sigma(x)w = \\begin{pmatrix} 0.125 + 0.030 + 0.010 \\\\ 0.050 + 0.108 + 0.024 \\\\ 0.025 + 0.036 + 0.098 \\end{pmatrix} = \\begin{pmatrix} 0.165 \\\\ 0.182 \\\\ 0.159 \\end{pmatrix}\n    $$\n    Now, we compute the quadratic form $w^T(\\Sigma(x)w)$:\n    $$\n    \\operatorname{Var}(\\hat{f}_{w}(x)) = \\begin{pmatrix} 0.5 & 0.3 & 0.2 \\end{pmatrix} \\begin{pmatrix} 0.165 \\\\ 0.182 \\\\ 0.159 \\end{pmatrix}\n    $$\n    $$\n    \\operatorname{Var}(\\hat{f}_{w}(x)) = (0.5)(0.165) + (0.3)(0.182) + (0.2)(0.159) = 0.0825 + 0.0546 + 0.0318 = 0.1689\n    $$\n\n3.  Sum the components to find the total expected error:\n    $$\n    \\text{Error}(x) = (\\text{Bias}(\\hat{f}_{w}(x)))^2 + \\operatorname{Var}(\\hat{f}_{w}(x)) + \\sigma^2\n    $$\n    $$\n    \\text{Error}(x) = 0.0064 + 0.1689 + 0.04\n    $$\n    $$\n    \\text{Error}(x) = 0.2153\n    $$\nThe result $0.2153$ is already expressed to four significant figures.", "answer": "$$\n\\boxed{0.2153}\n$$", "id": "3180603"}, {"introduction": "We conclude by applying our understanding to a practical scenario in modern deep learning: test-time ensembling. This advanced practice [@problem_id:3182030] models and contrasts two common strategies—averaging across checkpoints from a single training run versus averaging across models from independent runs. Through a combination of theoretical derivation and coding, you will explore how the correlation between models, a direct result of the training strategy, fundamentally impacts the ensemble's ability to reduce variance, offering deep insight into why ensembling diverse, independently trained models is such an effective technique for boosting model performance.", "problem": "You will investigate the bias-variance tradeoff by modeling two test-time ensembling strategies for a scalar prediction at a fixed input: ensembling across training checkpoints from a single run versus ensembling across random seeds from multiple independent runs. The goal is to determine how their different correlation structures and systematic biases affect the expected squared prediction error.\n\nFundamental base for derivation. Use only the following generally accepted definitions. Let $\\theta$ denote a deterministic quantity to be estimated and $g$ an estimator that is a random variable due to training randomness:\n- The bias of $g$ is $\\mathbb{E}[g] - \\theta$.\n- The variance of $g$ is $\\mathrm{Var}(g) = \\mathbb{E}[(g - \\mathbb{E}[g])^{2}]$.\n- Under squared loss, the expected squared error relative to $\\theta$ is $\\mathbb{E}[(g - \\theta)^{2}]$.\n\nScenario and assumptions. Fix an input where the true target is $f(x)$ (deterministic). For each ensembling strategy $t \\in \\{c,s\\}$ where $c$ denotes averaging across temporally adjacent checkpoints (temporal smoothing) and $s$ denotes averaging across independently initialized runs (parameter diversity), suppose the $i$-th model prediction is\n$$\n\\hat{y}^{(t)}_{i} \\;=\\; f(x) \\;+\\; b_{t} \\;+\\; \\varepsilon^{(t)}_{i}, \\quad i \\in \\{1,\\dots,K\\},\n$$\nwhere $b_{t}$ is a deterministic systematic bias (possibly different across strategies), and $\\varepsilon^{(t)}_{i}$ are zero-mean random fluctuations induced by training randomness and optimization, with\n$$\n\\mathbb{E}[\\varepsilon^{(t)}_{i}] = 0,\\quad \\mathrm{Var}(\\varepsilon^{(t)}_{i}) = v_{t},\\quad \\mathrm{Corr}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j}) = \\rho_{t}\\ \\text{for } i \\neq j.\n$$\nAssume all $\\varepsilon^{(t)}_{i}$ within a strategy are identically distributed with common variance $v_{t}$ and constant pairwise correlation $\\rho_{t} \\in [-1,1]$. Define the ensemble prediction for strategy $t$ as the arithmetic mean\n$$\n\\bar{y}^{(t)} \\;=\\; \\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}.\n$$\n\nTasks.\n1. Using only the above definitions, derive expressions, in terms of $K$, $b_{t}$, $v_{t}$, and $\\rho_{t}$, for:\n   - the bias of $\\bar{y}^{(t)}$ relative to $f(x)$,\n   - the variance of $\\bar{y}^{(t)}$,\n   - the expected squared error $\\mathbb{E}\\!\\left[(\\bar{y}^{(t)} - f(x))^{2}\\right]$.\n2. Interpret how temporal smoothing across checkpoints (typically larger $\\rho_{c}$) versus parameter diversity across seeds (typically smaller $\\rho_{s}$) changes the variance term as $K$ grows, and how different $b_{c}$ and $b_{s}$ influence the bias term.\n3. Implement a program that, for the test suite below, computes for each case and each strategy:\n   - squared bias $B^{2}_{t}$,\n   - variance $V_{t}$,\n   - expected squared error $E_{t}$,\n   and decides which strategy has the smaller expected squared error.\n\nNumerical and output requirements.\n- Use the following test suite. Each case specifies $(K, b_{c}, v_{c}, \\rho_{c}, b_{s}, v_{s}, \\rho_{s})$:\n  - Case A: $(K = 5,\\ b_{c} = 0.2,\\ v_{c} = 1.0,\\ \\rho_{c} = 0.8,\\ b_{s} = 0.25,\\ v_{s} = 1.0,\\ \\rho_{s} = 0.2)$.\n  - Case B: $(K = 20,\\ b_{c} = 0.1,\\ v_{c} = 0.8,\\ \\rho_{c} = 0.9,\\ b_{s} = 0.1,\\ v_{s} = 0.9,\\ \\rho_{s} = 0.1)$.\n  - Case C: $(K = 1,\\ b_{c} = 0.0,\\ v_{c} = 1.5,\\ \\rho_{c} = 0.0,\\ b_{s} = 0.05,\\ v_{s} = 1.2,\\ \\rho_{s} = 0.0)$.\n  - Case D: $(K = 50,\\ b_{c} = 0.15,\\ v_{c} = 0.7,\\ \\rho_{c} = 0.95,\\ b_{s} = 0.18,\\ v_{s} = 0.7,\\ \\rho_{s} = 0.05)$.\n- For each case, output a list\n  $[B^{2}_{c}, V_{c}, E_{c}, B^{2}_{s}, V_{s}, E_{s}, W]$\n  where $W$ is $0$ if $E_{c} \\le E_{s}$ and $1$ otherwise. Round all floating-point outputs to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these per-case lists, enclosed in square brackets; for example, a line like\n  $[[\\dots],[\\dots],[\\dots],[\\dots]]$.", "solution": "### 1. Derivation of Ensemble Statistics\n\nThe derivations for the bias, variance, and expected squared error of the ensemble prediction $\\bar{y}^{(t)}$ are presented below, using only the definitions provided in the problem statement. The true value to be estimated is $\\theta = f(x)$, and the estimator is $g = \\bar{y}^{(t)}$.\n\n#### Bias of $\\bar{y}^{(t)}$\nThe bias of an estimator $g$ for a quantity $\\theta$ is defined as $\\mathbb{E}[g] - \\theta$. In this context, $g = \\bar{y}^{(t)}$ and $\\theta = f(x)$.\n\nFirst, we compute the expectation of the ensemble mean, $\\mathbb{E}[\\bar{y}^{(t)}]$.\nThe ensemble prediction is defined as the arithmetic mean:\n$$\n\\bar{y}^{(t)} = \\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\bar{y}^{(t)}] = \\mathbb{E}\\left[\\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right] = \\frac{1}{K}\\sum_{i=1}^{K} \\mathbb{E}[\\hat{y}^{(t)}_{i}]\n$$\nThe expectation of a single model's prediction $\\hat{y}^{(t)}_{i} = f(x) + b_{t} + \\varepsilon^{(t)}_{i}$ is found by taking the expectation of its components. Since $f(x)$ and $b_t$ are deterministic quantities and $\\mathbb{E}[\\varepsilon^{(t)}_{i}] = 0$:\n$$\n\\mathbb{E}[\\hat{y}^{(t)}_{i}] = \\mathbb{E}[f(x) + b_{t} + \\varepsilon^{(t)}_{i}] = f(x) + b_{t} + \\mathbb{E}[\\varepsilon^{(t)}_{i}] = f(x) + b_{t}\n$$\nSubstituting this result back into the expression for $\\mathbb{E}[\\bar{y}^{(t)}]$:\n$$\n\\mathbb{E}[\\bar{y}^{(t)}] = \\frac{1}{K}\\sum_{i=1}^{K} (f(x) + b_{t}) = \\frac{1}{K} \\cdot K \\cdot (f(x) + b_{t}) = f(x) + b_{t}\n$$\nThe bias of the ensemble prediction is therefore:\n$$\n\\mathrm{Bias}(\\bar{y}^{(t)}) = \\mathbb{E}[\\bar{y}^{(t)}] - f(x) = (f(x) + b_{t}) - f(x) = b_{t}\n$$\n\n#### Variance of $\\bar{y}^{(t)}$\nThe variance of the ensemble mean is given by:\n$$\n\\mathrm{Var}(\\bar{y}^{(t)}) = \\mathrm{Var}\\left(\\frac{1}{K}\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right) = \\frac{1}{K^2} \\mathrm{Var}\\left(\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right) = \\sum_{i=1}^{K}\\sum_{j=1}^{K} \\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j})\n$$\nThe covariance term $\\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j})$ depends only on the random fluctuation terms $\\varepsilon^{(t)}_{i}$, as $f(x)$ and $b_t$ are deterministic constants:\n$$\n\\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j}) = \\mathrm{Cov}(f(x) + b_t + \\varepsilon^{(t)}_{i}, f(x) + b_t + \\varepsilon^{(t)}_{j}) = \\mathrm{Cov}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j})\n$$\nWe use the provided definitions for the variance and correlation of $\\varepsilon^{(t)}_{i}$:\n- For diagonal terms where $i=j$: $\\mathrm{Cov}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{i}) = \\mathrm{Var}(\\varepsilon^{(t)}_{i}) = v_{t}$.\n- For off-diagonal terms where $i \\neq j$: $\\mathrm{Cov}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j}) = \\sqrt{\\mathrm{Var}(\\varepsilon^{(t)}_{i})\\mathrm{Var}(\\varepsilon^{(t)}_{j})} \\cdot \\mathrm{Corr}(\\varepsilon^{(t)}_{i}, \\varepsilon^{(t)}_{j}) = \\sqrt{v_t \\cdot v_t} \\cdot \\rho_t = v_t \\rho_t$.\n\nThe double summation contains $K$ diagonal terms and $K(K-1)$ off-diagonal terms. Thus, the total variance of the sum is:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{K} \\hat{y}^{(t)}_{i}\\right) = \\sum_{i=1}^{K} \\mathrm{Var}(\\hat{y}^{(t)}_{i}) + \\sum_{i \\neq j} \\mathrm{Cov}(\\hat{y}^{(t)}_{i}, \\hat{y}^{(t)}_{j}) = K \\cdot v_t + K(K-1) \\cdot v_t \\rho_t\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\bar{y}^{(t)})$:\n$$\n\\mathrm{Var}(\\bar{y}^{(t)}) = \\frac{1}{K^2} \\left[ K v_t + K(K-1) v_t \\rho_t \\right] = \\frac{K v_t}{K^2} \\left[ 1 + (K-1)\\rho_t \\right]\n$$\nSimplifying this expression yields the final form for the variance:\n$$\n\\mathrm{Var}(\\bar{y}^{(t)}) = \\frac{v_t}{K} [1 + (K-1)\\rho_t]\n$$\n\n#### Expected Squared Error of $\\bar{y}^{(t)}$\nThe expected squared error (ESE) of an estimator $g$ with respect to a true value $\\theta$ is defined as $\\mathbb{E}[(g-\\theta)^2]$. A fundamental result in statistics is the decomposition of the ESE into the sum of the squared bias and the variance:\n$$\n\\mathbb{E}[(g-\\theta)^2] = (\\mathrm{Bias}(g))^2 + \\mathrm{Var}(g)\n$$\nApplying this to our problem, with $g=\\bar{y}^{(t)}$ and $\\theta=f(x)$:\n$$\n\\mathbb{E}\\!\\left[(\\bar{y}^{(t)} - f(x))^{2}\\right] = (\\mathrm{Bias}(\\bar{y}^{(t)}))^{2} + \\mathrm{Var}(\\bar{y}^{(t)})\n$$\nSubstituting the expressions derived above for the bias and variance, we obtain the expression for the expected squared error of the ensemble mean:\n$$\n\\mathbb{E}\\!\\left[(\\bar{y}^{(t)} - f(x))^{2}\\right] = b_t^2 + \\frac{v_t}{K} [1 + (K-1)\\rho_t]\n$$\n\n### 2. Interpretation of Results\n\nThe derived formulas provide clear insight into the bias-variance tradeoff for the two ensembling strategies.\n\nThe **bias** of the ensemble, $b_t$, is identical to the systematic bias of a single model from that strategy. Ensembling, which is a form of averaging, does not reduce systematic bias. This contribution to the total error, $b_t^2$, acts as an error floor that cannot be mitigated by increasing the ensemble size $K$.\n\nThe **variance** of the ensemble, $V_t = \\frac{v_t}{K} [1 + (K-1)\\rho_t]$, can be rewritten as $V_t = v_t \\left( \\frac{1-\\rho_t}{K} + \\rho_t \\right)$. This form clearly shows the effect of ensembling. As the ensemble size $K$ grows, the first term $\\frac{v_t(1-\\rho_t)}{K}$ diminishes. However, the variance is lower-bounded by the second term:\n$$\n\\lim_{K \\to \\infty} V_t = v_t \\rho_t\n$$\n- **Temporal Smoothing ($t=c$)**: This strategy ensembles predictions from different checkpoints of a single training run. These models are highly correlated, so $\\rho_c$ is large (close to $1$). The variance reduction from ensembling is minimal because the term $v_c \\rho_c$ dominates. Averaging highly similar predictions does little to cancel out their shared noise.\n- **Parameter Diversity ($t=s$)**: This strategy ensembles predictions from independent training runs (different random seeds). These models are less correlated, resulting in a small $\\rho_s$. The variance reduction from ensembling is significant because the term $\\frac{v_s(1-\\rho_s)}{K}$ has a large effect, and the variance floor $v_s \\rho_s$ is low. This strategy is more effective at reducing variance.\n\nThe total expected squared error is the sum of these two components. An optimal strategy must balance a low systematic bias $b_t$ with a structure that yields low prediction correlation $\\rho_t$ to effectively reduce variance.\n\n### 3. Numerical Computation\n\nThe derived expressions for squared bias $B^{2}_{t} = b_t^2$, variance $V_{t} = \\frac{v_t}{K} [1 + (K-1)\\rho_t]$, and expected squared error $E_{t} = B^{2}_{t} + V_{t}$ are implemented in the Python program provided in the `<answer>` section. The program computes these values for each test case and determines the superior strategy based on the lower expected squared error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes ensemble statistics for two strategies and determines the better one.\n    \"\"\"\n    # Test suite: (K, b_c, v_c, rho_c, b_s, v_s, rho_s)\n    test_cases = [\n        (5, 0.2, 1.0, 0.8, 0.25, 1.0, 0.2),    # Case A\n        (20, 0.1, 0.8, 0.9, 0.1, 0.9, 0.1),   # Case B\n        (1, 0.0, 1.5, 0.0, 0.05, 1.2, 0.0),   # Case C\n        (50, 0.15, 0.7, 0.95, 0.18, 0.7, 0.05)   # Case D\n    ]\n\n    all_results_str = []\n    \n    for case in test_cases:\n        K, b_c, v_c, rho_c, b_s, v_s, rho_s = case\n\n        # --- Strategy 'c' (Checkpoints) ---\n        # Squared Bias\n        B2_c = b_c**2\n        # Variance\n        # Handle the K=1 case to avoid floating point issues with (K-1)\n        if K == 1:\n            V_c = v_c\n        else:\n            V_c = (v_c / K) * (1 + (K - 1) * rho_c)\n        # Expected Squared Error\n        E_c = B2_c + V_c\n\n        # --- Strategy 's' (Seeds) ---\n        # Squared Bias\n        B2_s = b_s**2\n        # Variance\n        if K == 1:\n            V_s = v_s\n        else:\n            V_s = (v_s / K) * (1 + (K - 1) * rho_s)\n        # Expected Squared Error\n        E_s = B2_s + V_s\n        \n        # Determine the winner\n        # W = 0 if E_c <= E_s, W = 1 otherwise\n        W = 1 if E_c > E_s else 0\n        \n        # Format the results for the current case\n        case_result_list = [\n            f\"{B2_c:.6f}\",\n            f\"{V_c:.6f}\",\n            f\"{E_c:.6f}\",\n            f\"{B2_s:.6f}\",\n            f\"{V_s:.6f}\",\n            f\"{E_s:.6f}\",\n            str(W)\n        ]\n\n        all_results_str.append(f\"[{','.join(case_result_list)}]\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(all_results_str)}]]\")\n\nsolve()\n```", "id": "3182030"}]}