{"hands_on_practices": [{"introduction": "A cornerstone of machine learning is managing the trade-off between model capacity and generalization. This first exercise provides a concrete way to visualize and quantify this relationship using polynomial regression. By analyzing how training and validation errors change with model complexity, you will learn to identify the point of overfitting and use the \"generalization gap\" to select an optimal model, bridging theory with practical model selection [@problem_id:3107026].", "problem": "You are given observations from polynomial regression models of degree $d$ fitted to the same dataset. For each test case, you are provided, for a set of degrees $d \\in \\mathbb{N}$, the empirical training Mean Squared Error (Mean Squared Error (MSE)) $E_{\\text{train}}(d)$ and the empirical cross-validation MSE $E_{\\text{val}}(d)$. Your task is to predict overfitting risk by regressing the generalization gap $g(d)$ versus $d$, and to classify the optimal degree that minimizes an estimate of the expected generalization error.\n\nFundamental base and definitions:\n- The generalization error at complexity $d$ can be expressed as $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$, where $g(d)$ is the generalization gap defined by $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$. In practice, $E_{\\text{test}}(d)$ is unknown and cross-validation is used as a well-tested proxy, so we estimate the gap as $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- Overfitting risk increasing with model complexity is reflected by a positive slope of $g(d)$ as a function of $d$.\n\nAlgorithmic requirement:\n- For each test case, compute $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$ at each observed degree $d_k$.\n- Fit a straight line $g(d) \\approx a + b\\,d$ to the observed pairs $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (Ordinary Least Squares (OLS)).\n- Form the estimated expected generalization error at each observed degree as $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n- Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. If multiple degrees attain the same minimum within numerical tolerance, choose the smallest such degree (prefer lower complexity).\n- Report two quantities per test case: the estimated slope $\\widehat{b}$ of the gap-versus-degree regression, rounded to $4$ decimal places, and the classified optimal degree $\\widehat{d}^{\\star}$ as an integer.\n\nTest suite:\n- Test case $1$: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n- Test case $2$: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n- Test case $3$: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n- Test case $4$: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n- Test case $5$: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n\nOutput specification:\n- For each test case, output a two-element list $[\\widehat{b}, \\widehat{d}^{\\star}]$, where $\\widehat{b}$ is the slope rounded to $4$ decimal places, and $\\widehat{d}^{\\star}$ is an integer.\n- Your program should produce a single line of output containing one list that aggregates the per-test-case outputs in order, with no spaces. Each element must itself be a two-element list as described. The final printout must be exactly one line.\n\nNo physical units are involved. All angles, if any, are irrelevant here. No percentages are required.\n\nYour program must be a complete, runnable program that defines the test cases above internally and produces the specified single-line output.", "solution": "The problem requires an analysis of model performance as a function of complexity, specifically the degree $d$ of a polynomial regression model. The core of the analysis lies in understanding and modeling the generalization gap, $g(d)$, which represents the difference between a model's performance on unseen data (approximated by validation error, $E_{\\text{val}}$) and its performance on the training data ($E_{\\text{train}}$). A growing gap with increasing complexity is a hallmark of overfitting.\n\nThe specified algorithm aims to create a smoothed estimate of the generalization error to make a more robust model selection decision than simply picking the model with the lowest validation error. The validation error itself can be noisy, and modeling the trend of the generalization gap can help filter out this noise.\n\nThe procedure for each test case is as follows:\n\n1.  **Compute the Empirical Generalization Gap**: For each given polynomial degree $d_k$, we calculate the empirical generalization gap, $g(d_k)$, using the provided training and validation errors:\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **Model the Generalization Gap via OLS**: We hypothesize a linear relationship between the model complexity $d$ and the generalization gap $g(d)$. We fit a line, $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$, to the set of observed points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS). The slope $\\widehat{b}$ and intercept $\\widehat{a}$ are chosen to minimize the sum of squared differences $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$. For a set of $n$ points, the OLS estimators are given by:\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    where $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ and $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ are the sample means. The slope $\\widehat{b}$ serves as a direct measure of overfitting risk; a positive $\\widehat{b}$ indicates that the gap between training and validation performance widens as model complexity increases.\n\n3.  **Estimate the Generalization Error**: Using the linear model for the gap, we construct a smoothed estimate of the true generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$:\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    This estimate combines the directly observed training error, which reflects how well the model fits the data, with a regularized estimate of the generalization penalty, which accounts for complexity.\n\n4.  **Determine the Optimal Degree**: The optimal degree, $\\widehat{d}^{\\star}$, is selected as the degree from the given set $\\{d_k\\}$ that minimizes our estimated generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$.\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    The problem specifies that if a tie occurs, the smallest degree among those that achieve the minimum error should be chosen. This reflects the principle of parsimony (Occam's razor): when all else is equal, prefer the simpler model.\n\nFinally, for each test case, we report the calculated slope $\\widehat{b}$ (a measure of overfitting risk) and the determined optimal degree $\\widehat{d}^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3107026"}, {"introduction": "While model architecture is a key determinant of capacity, the training process itself offers powerful mechanisms for regularization. This practice moves from empirical analysis to a more fundamental, analytical perspective on early stopping. By deriving the optimal stopping time $t^{\\star}$ for a simplified linear model, you will gain a first-principles understanding of how gradient descent first learns the underlying signal and then begins to overfit to noise, demonstrating how stopping early can achieve the best generalization performance [@problem_id:3152387].", "problem": "You are given a simplified, one-dimensional linear regression model that captures the effect of label noise, model capacity, and early stopping on generalization. The data consist of $N \\in \\mathbb{N}$ independent and identically distributed pairs $(x_i, y_i)$, where $x_i \\sim \\mathcal{N}(0, 1)$ and labels are generated by $y_i = \\theta^\\star x_i + \\epsilon_i$. Label noise is modeled as $\\epsilon_i = z_i \\eta_i$ with $z_i \\sim \\operatorname{Bernoulli}(\\rho)$, $\\eta_i \\sim \\mathcal{N}(0, \\sigma_n^2)$, independent of $x_i$. Thus $\\mathbb{E}[\\epsilon_i] = 0$, $\\operatorname{Var}(\\epsilon_i) = \\rho \\sigma_n^2$. The model is a single parameter $w(t) \\in \\mathbb{R}$ trained to minimize the empirical squared loss $L(w) = \\frac{1}{2N} \\sum_{i=1}^N (w x_i - y_i)^2$ by continuous-time gradient flow starting at $w(0) = w_0$.\n\nFundamental base:\n- Empirical squared loss with gradient $\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^N (w x_i - y_i) x_i$.\n- Gradient flow dynamics $\\,\\frac{dw}{dt} = - \\nabla L(w)\\,$ give $\\,\\frac{dw}{dt} = -\\frac{S_{xx}}{N} w + \\frac{S_{xy}}{N}\\,$, where $\\,S_{xx} = \\sum_{i=1}^N x_i^2\\,$ and $\\,S_{xy} = \\sum_{i=1}^N x_i y_i\\,$.\n- The closed-form solution to this linear ordinary differential equation is $\\,w(t) = e^{-a t} w_0 + (1 - e^{-a t}) w_{\\text{ls}}\\,$ with $\\,a := \\frac{S_{xx}}{N}\\,$ and $\\,w_{\\text{ls}} := \\frac{S_{xy}}{S_{xx}}\\,$.\n\nDefine the mean squared generalization error (Mean Squared Error (MSE)) at time $t$ on a fresh test point $(x, y)$ from the same distribution as $\\mathcal{E}(t) = \\mathbb{E}\\big[(w(t) x - \\theta^\\star x)^2\\big]$. Since $\\mathbb{E}[x^2] = 1$, the excess risk ignoring the irreducible noise reduces to $\\mathcal{E}(t) = \\mathbb{E}_{\\text{train}}\\big[(w(t) - \\theta^\\star)^2\\big]$.\n\nTask:\n- Starting from the dynamics and definitions above, and using only standard probability results for Gaussian and chi-square distributions, derive the expression for the early stopping time $t^\\star$ that minimizes $\\mathcal{E}(t)$ under the isotropic approximation $a \\approx 1$ and for $N > 2$. Your derivation must express $t^\\star$ in terms of $B := (w_0 - \\theta^\\star)^2$ and $V := \\operatorname{Var}(w_{\\text{ls}})$, and use the exact identity $\\mathbb{E}\\big[1/S_{xx}\\big] = 1/(N - 2)$ for $N > 2$. You must also state and justify the boundary cases when $N \\leq 2$, when $\\rho = 0$, and when $B = 0$.\n- Implement a program that computes $t^\\star$ for a small test suite of parameter sets $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star)$ using your derived formula and boundary cases, without any simulation.\n\nAssumptions and requirements:\n- Assume $a = 1$ in the closed-form expression for $w(t)$ to isolate the interaction between bias decay and noise-driven variance buildup under isotropic features.\n- Use $\\sigma_\\epsilon^2 := \\rho \\sigma_n^2$ and $V := \\mathbb{E}[\\operatorname{Var}(w_{\\text{ls}} \\mid X)] = \\sigma_\\epsilon^2 \\, \\mathbb{E}[1/S_{xx}]$, with the exact $\\mathbb{E}[1/S_{xx}] = 1/(N - 2)$ for $N > 2$.\n- Boundary conventions:\n  - If $N \\leq 2$, treat $V$ as $+\\infty$ and return $t^\\star = 0$.\n  - If $\\rho = 0$ (or $\\sigma_n^2 = 0$), then $V = 0$; if additionally $B > 0$, return $t^\\star = +\\infty$; if $B = 0$, return $t^\\star = 0$.\n  - If $B = 0$, return $t^\\star = 0$ regardless of other parameters.\n- Use the natural logarithm.\n\nTest suite:\n- Case $1$: $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star) = (100, 0.2, 1.0, 0.0, 1.0)$.\n- Case $2$: $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star) = (100, 0.0, 1.0, 0.0, 1.0)$.\n- Case $3$: $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star) = (2, 0.5, 1.0, 0.0, 1.0)$.\n- Case $4$: $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star) = (100, 0.2, 1.0, 1.0, 1.0)$.\n- Case $5$: $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star) = (100, 0.9, 1.0, 0.0, 1.0)$.\n- Case $6$: $(N, \\rho, \\sigma_n^2, w_0, \\theta^\\star) = (50, 0.3, 4.0, -1.0, 2.0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$).\n- Each entry is $t^\\star$ for the corresponding test case, using $+\\infty$ when applicable.\n- Finite values must be rounded to $6$ decimal places.", "solution": "The objective is to find the optimal early stopping time $t^\\star$ that minimizes the mean squared generalization error $\\mathcal{E}(t) = \\mathbb{E}_{\\text{train}}\\big[(w(t) - \\theta^\\star)^2\\big]$. The parameter $w(t)$ evolves according to gradient flow, and its solution is given, with the simplifying assumption $a = \\frac{S_{xx}}{N} \\approx 1$, as:\n$$ w(t) = e^{-t} w_0 + (1 - e^{-t}) w_{\\text{ls}} $$\nwhere $w_0$ is the initial parameter and $w_{\\text{ls}} = S_{xy}/S_{xx}$ is the ordinary least squares solution for the given training data.\n\nOur first step is to express the error term, $w(t) - \\theta^\\star$, in a more convenient form. By subtracting $\\theta^\\star$ from both sides and rearranging, we can decompose the error into components related to the initial bias and the least squares solution:\n$$ w(t) - \\theta^\\star = e^{-t} w_0 + (1 - e^{-t}) w_{\\text{ls}} - (e^{-t} + 1 - e^{-t})\\theta^\\star $$\n$$ w(t) - \\theta^\\star = e^{-t}(w_0 - \\theta^\\star) + (1 - e^{-t})(w_{\\text{ls}} - \\theta^\\star) $$\nThis expression separates the deterministic initial bias, $w_0 - \\theta^\\star$, from the stochastic component related to the training data, $w_{\\text{ls}} - \\theta^\\star$.\n\nNext, we substitute this into the definition of the generalization error $\\mathcal{E}(t)$ and take the expectation over all possible training sets.\n$$ \\mathcal{E}(t) = \\mathbb{E}\\big[ \\left( e^{-t}(w_0 - \\theta^\\star) + (1 - e^{-t})(w_{\\text{ls}} - \\theta^\\star) \\right)^2 \\big] $$\nExpanding the squared term gives:\n$$ \\mathcal{E}(t) = \\mathbb{E}\\big[ e^{-2t}(w_0 - \\theta^\\star)^2 + (1 - e^{-t})^2(w_{\\text{ls}} - \\theta^\\star)^2 + 2e^{-t}(1-e^{-t})(w_0 - \\theta^\\star)(w_{\\text{ls}} - \\theta^\\star) \\big] $$\nBy linearity of expectation, we can write:\n$$ \\mathcal{E}(t) = e^{-2t}(w_0 - \\theta^\\star)^2 + (1 - e^{-t})^2\\mathbb{E}\\big[(w_{\\text{ls}} - \\theta^\\star)^2\\big] + 2e^{-t}(1-e^{-t})(w_0 - \\theta^\\star)\\mathbb{E}\\big[w_{\\text{ls}} - \\theta^\\star\\big] $$\n\nTo proceed, we must evaluate the expectations involving $w_{\\text{ls}}$. Let us first express $w_{\\text{ls}}$ in terms of the true parameter $\\theta^\\star$ and the label noise $\\epsilon_i$:\n$$ w_{\\text{ls}} = \\frac{\\sum_{i=1}^N x_i y_i}{\\sum_{i=1}^N x_i^2} = \\frac{\\sum_{i=1}^N x_i (\\theta^\\star x_i + \\epsilon_i)}{\\sum_{i=1}^N x_i^2} = \\frac{\\theta^\\star \\sum x_i^2 + \\sum x_i \\epsilon_i}{\\sum x_i^2} = \\theta^\\star + \\frac{\\sum_{i=1}^N x_i \\epsilon_i}{S_{xx}} $$\nThus, $w_{\\text{ls}} - \\theta^\\star = \\frac{\\sum x_i \\epsilon_i}{S_{xx}}$.\n\nWe can now compute the expectation $\\mathbb{E}[w_{\\text{ls}} - \\theta^\\star]$. Using the law of total expectation, $\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y|X]]$, we first condition on the features $X = \\{x_i\\}_{i=1}^N$.\n$$ \\mathbb{E}[w_{\\text{ls}} - \\theta^\\star | X] = \\mathbb{E}\\left[\\frac{\\sum x_i \\epsilon_i}{S_{xx}} \\bigg| X\\right] = \\frac{1}{S_{xx}}\\sum x_i \\mathbb{E}[\\epsilon_i] $$\nSince the noise is zero-mean, $\\mathbb{E}[\\epsilon_i]=0$, we get $\\mathbb{E}[w_{\\text{ls}} - \\theta^\\star | X] = 0$. Taking the outer expectation over $X$ yields $\\mathbb{E}[w_{\\text{ls}} - \\theta^\\star] = \\mathbb{E}_X[0] = 0$. This shows that $w_{\\text{ls}}$ is an unbiased estimator of $\\theta^\\star$.\nConsequently, the cross-term in the expression for $\\mathcal{E}(t)$ vanishes:\n$$ 2e^{-t}(1-e^{-t})(w_0 - \\theta^\\star)\\mathbb{E}\\big[w_{\\text{ls}} - \\theta^\\star\\big] = 0 $$\n\nThe generalization error simplifies to a classic bias-variance form:\n$$ \\mathcal{E}(t) = e^{-2t}(w_0 - \\theta^\\star)^2 + (1 - e^{-t})^2\\mathbb{E}\\big[(w_{\\text{ls}} - \\theta^\\star)^2\\big] $$\nThe term $\\mathbb{E}\\big[(w_{\\text{ls}} - \\theta^\\star)^2\\big]$ is the mean squared error of the least squares estimator. Since the estimator is unbiased, this is equal to its variance, $\\operatorname{Var}(w_{\\text{ls}})$. The problem defines $V := \\operatorname{Var}(w_{\\text{ls}})$. Thus, with $B := (w_0 - \\theta^\\star)^2$, the error is:\n$$ \\mathcal{E}(t) = B e^{-2t} + V (1 - e^{-t})^2 $$\nFor completeness, we verify the expression for $V$ for $N>2$. As specified, $V = \\mathbb{E}[\\operatorname{Var}(w_{\\text{ls}} \\mid X)]$. The conditional variance is:\n$$ \\operatorname{Var}(w_{\\text{ls}} \\mid X) = \\operatorname{Var}\\left(\\theta^\\star + \\frac{\\sum x_i \\epsilon_i}{S_{xx}} \\bigg| X \\right) = \\frac{1}{S_{xx}^2} \\operatorname{Var}\\left(\\sum x_i \\epsilon_i\\right) = \\frac{1}{S_{xx}^2} \\sum x_i^2 \\operatorname{Var}(\\epsilon_i) = \\frac{S_{xx} \\sigma_\\epsilon^2}{S_{xx}^2} = \\frac{\\sigma_\\epsilon^2}{S_{xx}} $$\nwhere $\\sigma_\\epsilon^2 = \\rho \\sigma_n^2$. Taking the expectation over $X$ and using the provided identity $\\mathbb{E}[1/S_{xx}] = 1/(N-2)$ gives:\n$$ V = \\sigma_\\epsilon^2 \\mathbb{E}\\left[\\frac{1}{S_{xx}}\\right] = \\frac{\\sigma_\\epsilon^2}{N-2} $$\nThe total error can be written as $\\mathcal{E}(t) = (B+V)e^{-2t} - 2Ve^{-t} + V$.\n\nTo find the optimal stopping time $t^\\star$, we minimize $\\mathcal{E}(t)$ with respect to $t$. We differentiate $\\mathcal{E}(t)$ and set the derivative to zero:\n$$ \\frac{d\\mathcal{E}}{dt} = B(-2e^{-2t}) + 2V(1-e^{-t})(e^{-t}) = -2B e^{-2t} + 2V e^{-t} - 2V e^{-2t} = -2(B+V)e^{-2t} + 2Ve^{-t} $$\nSetting $\\frac{d\\mathcal{E}}{dt} = 0$:\n$$ 2V e^{-t^\\star} = 2(B+V) e^{-2t^\\star} $$\nAssuming $V > 0$, we can divide by $2Ve^{-t^\\star}$:\n$$ 1 = \\frac{B+V}{V} e^{-t^\\star} \\implies e^{-t^\\star} = \\frac{V}{B+V} $$\nSolving for $t^\\star$ by taking the natural logarithm:\n$$ -t^\\star = \\ln\\left(\\frac{V}{B+V}\\right) \\implies t^\\star = -\\ln\\left(\\frac{V}{B+V}\\right) = \\ln\\left(\\frac{B+V}{V}\\right) = \\ln\\left(1 + \\frac{B}{V}\\right) $$\nThe second derivative is $\\frac{d^2\\mathcal{E}}{dt^2} = 4(B+V)e^{-2t} - 2Ve^{-t}$. At $t=t^\\star$, substituting $e^{-t^\\star} = V/(B+V)$, we find $\\frac{d^2\\mathcal{E}}{dt^2}|_{t^\\star} = \\frac{2V^2}{B+V} > 0$ for $V>0$ and $B \\ge 0$, confirming that $t^\\star$ is a minimum.\n\nFinally, we address the specified boundary cases:\n1.  **$N \\le 2$**: In this case, $V$ is treated as infinite. The argument of the logarithm $1 + B/V$ approaches $1$. Thus, $t^\\star = \\ln(1) = 0$. This aligns with the intuition that if the least-squares estimator has infinite variance, any amount of training is detrimental, so we should not train at all.\n2.  **$B = 0$**: The model starts at the optimal parameter value, $w_0 = \\theta^\\star$. Our formula gives $t^\\star = \\ln(1 + 0/V) = 0$. The error function is $\\mathcal{E}(t) = V(1-e^{-t})^2$, which is minimized at $t=0$. The initial model is perfect, so $t^\\star=0$.\n3.  **$V = 0$ (noiseless case, e.g., $\\rho=0$) and $B>0$**: Our formula for $t^\\star$ is undefined. We return to the derivative: $\\frac{d\\mathcal{E}}{dt} = -2Be^{-2t}$. Since $B>0$, the derivative is always negative, meaning $\\mathcal{E}(t) = Be^{-2t}$ is a strictly decreasing function. The minimum is approached as $t \\to \\infty$. Thus, $t^\\star = +\\infty$. This makes sense: without noise, the model should train indefinitely to converge to the true parameter. If both $B=0$ and $V=0$, the error is always $0$, and we take $t^\\star=0$ by convention.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal early stopping time t* for a series of test cases based on a derived formula.\n    \"\"\"\n\n    test_cases = [\n        # (N, rho, sigma_n^2, w_0, theta_star)\n        (100, 0.2, 1.0, 0.0, 1.0),\n        (100, 0.0, 1.0, 0.0, 1.0),\n        (2, 0.5, 1.0, 0.0, 1.0),\n        (100, 0.2, 1.0, 1.0, 1.0),\n        (100, 0.9, 1.0, 0.0, 1.0),\n        (50, 0.3, 4.0, -1.0, 2.0),\n    ]\n\n    results = []\n    for N, rho, sigma_n_sq, w0, theta_star in test_cases:\n        # Calculate B, the squared initial bias.\n        B = (w0 - theta_star)**2\n\n        # Handle boundary cases as defined in the problem statement.\n        \n        # Case 1: B = 0 (initial model is already optimal).\n        if B == 0:\n            results.append(0.0)\n            continue\n            \n        # Case 2: N <= 2 (variance of OLS estimator is infinite or undefined).\n        if N <= 2:\n            results.append(0.0)\n            continue\n            \n        # For N > 2 and B > 0:\n        \n        # Calculate the variance of the label noise.\n        sigma_epsilon_sq = rho * sigma_n_sq\n\n        # Case 3: V = 0 (noiseless regression).\n        if sigma_epsilon_sq == 0:\n            results.append(np.inf)\n            continue\n            \n        # General case (N > 2, B > 0, V > 0):\n        # V is the variance of the least squares estimator.\n        V = sigma_epsilon_sq / (N - 2)\n        \n        # Calculate t* using the derived formula.\n        t_star = np.log(1 + B / V)\n        results.append(t_star)\n\n    # Format the results for the final output.\n    # Finite values are rounded to 6 decimal places.\n    # np.inf is converted to the string 'inf'.\n    formatted_results = []\n    for r in results:\n        if r == np.inf:\n            formatted_results.append('inf')\n        else:\n            formatted_results.append(f\"{r:.6f}\")\n\n    # Print the final output in the specified format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3152387"}, {"introduction": "Classical wisdom suggests that beyond a certain point, increasing model capacity inevitably leads to worse generalization. This final exercise challenges that notion by guiding you through a simulation of the \"double descent\" phenomenon. Using a linear autoencoder, you will observe how test error, after peaking near the point of data interpolation, can decrease again in the highly overparameterized regime, revealing a more complete and surprising picture of how modern, large-capacity models generalize [@problem_id:3183618].", "problem": "You will implement a simulation to study the double descent phenomenon in a linear autoencoder by varying the bottleneck size. The goal is to connect training reconstruction error and test-time generalization to how model capacity crosses the interpolation threshold and approaches the identity mapping. Use a purely mathematical setup grounded in Empirical Risk Minimization (ERM), Principal Component Analysis (PCA), and orthogonal projection properties, and design a program that produces quantifiable outputs for multiple test cases.\n\nConsider a linear autoencoder with tied weights, where the input is a vector $x \\in \\mathbb{R}^d$, the encoder is $z = W^\\top x$ with $W \\in \\mathbb{R}^{d \\times m}$, and the decoder reconstructs $\\hat{x} = W z = W W^\\top x$. Let the training data matrix be $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ and the test data matrix be $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$. Center the training data by its empirical mean $\\mu_{\\text{train}} \\in \\mathbb{R}^d$, and center the test data by the same $\\mu_{\\text{train}}$ to evaluate generalization for a fixed estimator. For a given bottleneck size $m$ with $1 \\le m \\le d$, define the reconstruction matrix $P_m = W_m W_m^\\top$, where $W_m$ has orthonormal columns spanning an $m$-dimensional subspace of $\\mathbb{R}^d$. The training reconstruction error at bottleneck $m$ is\n$$\nE_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left\\| x_i - P_m x_i \\right\\|_2^2,\n$$\nand the test reconstruction error is\n$$\nE_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left\\| x^{\\text{test}}_j - P_m x^{\\text{test}}_j \\right\\|_2^2,\n$$\nwhere all $x_i$ and $x^{\\text{test}}_j$ are centered by $\\mu_{\\text{train}}$.\n\nFundamental base and modeling assumptions:\n- Empirical Risk Minimization (ERM): The estimator minimizes the average training reconstruction error $E_{\\text{train}}(m)$ over $W$ subject to orthonormal columns.\n- Principal Component Analysis (PCA): The ERM solution for the linear autoencoder with squared error and tied weights yields $W_m$ whose columns are the top $m$ right singular vectors of the centered training matrix (equivalently, the top $m$ eigenvectors of the empirical covariance), thus $P_m$ is the orthogonal projector onto that $m$-dimensional subspace.\n- Orthogonal projections: For any $m$, $P_m$ is an idempotent symmetric matrix with rank $m$, and $\\left\\| x - P_m x \\right\\|_2^2$ is the squared distance from $x$ to the subspace spanned by $W_m$.\n\nCapacity crossing and identity mapping:\n- Interpolation threshold: Let $r = \\operatorname{rank}(X_{\\text{train}})$. When $m \\ge r$, there exist solutions with $E_{\\text{train}}(m) = 0$ because the projector can span the training data subspace. The smallest such $m$ is denoted $m_{\\text{interp}}$.\n- Identity mapping: When $m = d$ and $W_d$ has $d$ orthonormal columns, $P_d = I_d$, which yields $E_{\\text{train}}(d) = 0$ and $E_{\\text{test}}(d) = 0$, connecting capacity to the identity mapping.\n\nDouble descent investigation:\n- As $m$ increases from $1$ to $d$, the test error $E_{\\text{test}}(m)$ may exhibit an initial descent (adding signal-aligned directions), then an ascent near $m \\approx r$ (overfitting to the training subspace), followed by a second descent as $m \\to d$ (approaching the identity mapping). This shape is referred to as double descent.\n\nImplementation requirements:\n- Construct $X_{\\text{train}}$ and $X_{\\text{test}}$ by sampling independently from a zero-mean multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$ in $\\mathbb{R}^d$, where $\\Sigma$ is built by selecting a random orthonormal basis and placing specified eigenvalues on the diagonal. Let $Q \\in \\mathbb{R}^{d \\times d}$ be orthonormal, and set $\\Sigma = Q \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_d) Q^\\top$ with user-specified $\\lambda_i$ values that reflect a few large \"signal\" variances and smaller \"noise\" variances.\n- Use the training mean $\\mu_{\\text{train}}$ to center both $X_{\\text{train}}$ and $X_{\\text{test}}$.\n- Compute the Singular Value Decomposition (SVD) of the centered training matrix to obtain an orthonormal basis $V \\in \\mathbb{R}^{d \\times d}$ of right singular vectors. For each $m$, set $W_m$ to be the first $m$ columns of $V$, and $P_m = W_m W_m^\\top$.\n- For each $m \\in \\{1, 2, \\ldots, d\\}$, compute $E_{\\text{train}}(m)$ and $E_{\\text{test}}(m)$.\n- Let $m_{\\text{peak}}$ be the index $m$ that maximizes $E_{\\text{test}}(m)$ (use the smallest such $m$ in case of ties).\n- Let $m_{\\text{interp}}$ be the smallest $m$ such that $E_{\\text{train}}(m) \\le \\tau$, with tolerance $\\tau = 10^{-12}$.\n- Define a boolean $b_{\\text{dd}}$ indicating whether double descent is observed, using the following criterion:\n    - Let $k = m_{\\text{peak}}$, and consider the sequences before and after the peak. Let $E_{\\text{pre}} = \\{E_{\\text{test}}(1), \\ldots, E_{\\text{test}}(k-1)\\}$ and $E_{\\text{post}} = \\{E_{\\text{test}}(k+1), \\ldots, E_{\\text{test}}(d)\\}$ (empty sequences imply $b_{\\text{dd}} = \\text{False}$).\n    - Double descent is declared if both $\\min(E_{\\text{pre}}) < E_{\\text{test}}(k)$ and $\\min(E_{\\text{post}}) < E_{\\text{test}}(k)$, and there is at least one strict decrease somewhere in $E_{\\text{pre}}$.\n- For each test case, output the list $[m_{\\text{peak}}, \\text{round}(E_{\\text{test}}(m_{\\text{peak}}), 6), m_{\\text{interp}}, b_{\\text{dd}}]$.\n\nTest suite:\nFor each case, parameters are $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed})$ where $d$ is the dimensionality, $n$ is the number of training samples, $n_{\\text{test}}$ is the number of test samples, $s$ is the number of signal dimensions, $\\sigma_{\\text{signal}}^2$ is the signal variance, $\\sigma_{\\text{noise}}^2$ is the noise variance, and $\\text{seed}$ sets the random number generator. Construct eigenvalues by setting the first $s$ to $\\sigma_{\\text{signal}}^2$ and the remaining $d - s$ to $\\sigma_{\\text{noise}}^2$.\n- Case $1$: $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed}) = (\\,60,\\,40,\\,2000,\\,12,\\,5.0,\\,0.5,\\,1\\,)$.\n- Case $2$: $(\\,60,\\,58,\\,2000,\\,12,\\,4.0,\\,0.6,\\,2\\,)$.\n- Case $3$: $(\\,30,\\,15,\\,3000,\\,0,\\,0.0,\\,1.0,\\,3\\,)$.\n- Case $4$: $(\\,20,\\,5,\\,5000,\\,8,\\,3.0,\\,0.3,\\,4\\,)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[m_{\\text{peak}}, \\text{round}(E_{\\text{test}}(m_{\\text{peak}}), 6), m_{\\text{interp}}, b_{\\text{dd}}]$. Floats must be rounded to $6$ decimal places. For example, the output should look like $[[1,0.123456,1,True],[\\ldots],[\\ldots],[\\ldots]]$.", "solution": "The simulation will be implemented by adhering to the theoretical principles and computational steps outlined in the problem statement. The core of the analysis is to relate the model's capacity, controlled by the bottleneck dimension $m$, to its performance on training and test data.\n\n1.  **Theoretical Framework**: The problem specifies a linear autoencoder with tied weights, where the decoder weight matrix is the transpose of the encoder's. The reconstruction is given by $\\hat{x} = W_m W_m^\\top x$. The matrix $P_m = W_m W_m^\\top$ is an orthogonal projector onto the $m$-dimensional subspace spanned by the columns of $W_m$. The principle of Empirical Risk Minimization (ERM) dictates that we must choose $W_m$ to minimize the average training reconstruction error, $E_{\\text{train}}(m)$. For the squared $\\ell_2$-norm loss, this optimization problem is equivalent to Principal Component Analysis (PCA). The optimal subspace is the one spanned by the first $m$ principal components of the centered training data. These components are the right singular vectors of the centered training data matrix, $X_c = X_{\\text{train}} - \\mu_{\\text{train}}$.\n\n2.  **Simulation Algorithm**:\n    -   **Data Generation**: For each test case, we first establish a random number generator with the given seed. We construct the $d \\times d$ covariance matrix $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_d) Q^\\top$. The eigenvalues $\\lambda_i$ are set to create a distinction between \"signal\" and \"noise\" dimensions, and $Q$ is a random orthonormal matrix generated using routines from `scipy.stats`. We then sample $n$ training vectors and $n_{\\text{test}}$ test vectors from the multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$.\n    -   **Data Centering**: The training data is centered by subtracting its empirical mean, $\\mu_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n x_i$, to yield $X_c$. Crucially, the test data is also centered using this same $\\mu_{\\text{train}}$ to evaluate how the learned model generalizes to unseen data from the same distribution.\n    -   **Principal Component Basis**: We compute the Singular Value Decomposition (SVD) of the centered training matrix: $X_c = U S V^\\top$. The columns of $V$ (or rows of $V^\\top$) form an orthonormal basis of principal components for the training data, ordered by the corresponding singular values in $S$.\n    -   **Error Curve Computation**: We iterate through the model capacity $m$ from $1$ to $d$. In each step:\n        -   The projector $P_m$ is formed from the first $m$ principal vectors: $W_m = V_{:, 1:m}$, $P_m = W_m W_m^\\top$.\n        -   The test reconstruction error, $E_{\\text{test}}(m)$, is calculated directly using its definition. This can be computed efficiently as $E_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\|X_{test, c} (I_d - P_m)\\|_F^2$, where $\\|\\cdot\\|_F$ is the Frobenius norm.\n        -   The training reconstruction error, $E_{\\text{train}}(m)$, can be computed even more efficiently. Due to the properties of SVD and PCA, the total reconstruction error on the training set using the top $m$ components is the sum of the squares of the remaining singular values. That is, $E_{\\text{train}}(m) = \\frac{1}{n} \\sum_{k=m+1}^{\\text{rank}} s_k^2$, where $s_k$ are the singular values of $X_c$.\n\n3.  **Analysis and Metric Extraction**: After computing the full error curves $E_{\\text{train}}(m)$ and $E_{\\text{test}}(m)$ for $m=1, \\ldots, d$:\n    -   $m_{\\text{peak}}$ is identified as the smallest $m$ that maximizes the test error curve $E_{\\text{test}}$. This point often signals the onset of overfitting, where the model capacity is just enough to fit the training data but captures spurious correlations that hurt generalization.\n    -   $m_{\\text{interp}}$ is found by locating the first $m$ at which the training error $E_{\\text{train}}(m)$ drops below a numerical tolerance $\\tau = 10^{-12}$. This marks the interpolation threshold, where the model capacity is sufficient to perfectly memorize the training data ($m \\ge \\operatorname{rank}(X_c)$).\n    -   $b_{\\text{dd}}$ is determined by a specific criterion that checks for the characteristic \"U-shape\" before the peak ($m_{\\text{peak}}$) and a subsequent decline in test error, which constitutes the \"second descent\". This second descent occurs as the model capacity approaches $d$, and the projector $P_m$ approaches the identity matrix, which trivially achieves zero error on any data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ortho_group\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (60, 40, 2000, 12, 5.0, 0.5, 1),\n        (60, 58, 2000, 12, 4.0, 0.6, 2),\n        (30, 15, 3000, 0, 0.0, 1.0, 3),\n        (20, 5, 5000, 8, 3.0, 0.3, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(d, n, n_test, s, sigma_signal_sq, sigma_noise_sq, seed):\n    \"\"\"\n    Runs the double descent simulation for a single set of parameters.\n\n    Args:\n        d (int): Dimensionality of the data space.\n        n (int): Number of training samples.\n        n_test (int): Number of test samples.\n        s (int): Number of signal dimensions.\n        sigma_signal_sq (float): Signal variance.\n        sigma_noise_sq (float): Noise variance.\n        seed (int): Random seed.\n\n    Returns:\n        list: A list containing [m_peak, E_test_at_peak, m_interp, b_dd].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct the covariance matrix Sigma\n    lambdas = np.array([sigma_signal_sq] * s + [sigma_noise_sq] * (d - s))\n    Q = ortho_group.rvs(dim=d, random_state=rng)\n    Sigma = Q @ np.diag(lambdas) @ Q.T\n\n    # 2. Generate training and test data\n    mean = np.zeros(d)\n    X_train = rng.multivariate_normal(mean=mean, cov=Sigma, size=n)\n    X_test = rng.multivariate_normal(mean=mean, cov=Sigma, size=n_test)\n\n    # 3. Center data using the training mean\n    mu_train = np.mean(X_train, axis=0)\n    X_train_centered = X_train - mu_train\n    X_test_centered = X_test - mu_train\n\n    # 4. Compute SVD of the centered training data\n    # U, S, Vt where V are the right singular vectors (principal components)\n    U, S_vals, Vt = np.linalg.svd(X_train_centered, full_matrices=False)\n    V = Vt.T\n\n    E_train_curve = []\n    E_test_curve = []\n\n    # Pad singular values array with zeros up to dimension d if necessary\n    S_sq_full = np.zeros(d)\n    S_sq_full[:len(S_vals)] = S_vals**2\n\n    # 5. Iterate over bottleneck size m from 1 to d\n    for m in range(1, d + 1):\n        # Efficiently compute training error using singular values\n        # E_train(m) = (1/n) * sum_{k=m to rank-1} s_k^2\n        train_error = np.sum(S_sq_full[m:]) / n\n        E_train_curve.append(train_error)\n\n        # Compute test error via projection\n        W_m = V[:, :m]\n        P_m = W_m @ W_m.T\n        I = np.identity(d)\n        \n        # Error matrix for test set: X_test_c - X_test_c @ P_m = X_test_c @ (I - P_m)\n        error_matrix_test = X_test_centered @ (I - P_m)\n        # Sum of squared L2 norms is the squared Frobenius norm\n        test_error = np.sum(error_matrix_test**2) / n_test\n        E_test_curve.append(test_error)\n\n    # 6. Calculate the required metrics\n    E_test_curve = np.array(E_test_curve)\n    E_train_curve = np.array(E_train_curve)\n\n    # m_peak: smallest m that maximizes test error\n    m_peak = np.argmax(E_test_curve) + 1\n    e_test_at_peak = E_test_curve[m_peak - 1]\n\n    # m_interp: smallest m where training error is effectively zero\n    tau = 1e-12\n    interp_indices = np.where(E_train_curve <= tau)[0]\n    # The problem setup guarantees interpolation happens\n    m_interp = interp_indices[0] + 1 if len(interp_indices) > 0 else d + 1\n\n    # b_dd: boolean for double descent\n    k = m_peak\n    E_pre = E_test_curve[:k-1]\n    E_post = E_test_curve[k:] # from index k to the end\n\n    b_dd = False\n    # Check for empty sequences before proceeding\n    if E_pre.size > 0 and E_post.size > 0:\n        cond1 = np.min(E_pre) < e_test_at_peak\n        cond2 = np.min(E_post) < e_test_at_peak\n        \n        # \"at least one strict decrease somewhere in E_pre\"\n        # Implemented as not being monotonically non-decreasing\n        # Or more directly, there is at least one adjacent pair with a decrease\n        # This correctly handles sequences of length 1 (returns False)\n        cond3 = any(E_pre[i] > E_pre[i+1] for i in range(len(E_pre)-1))\n\n        if cond1 and cond2 and cond3:\n            b_dd = True\n            \n    return [m_peak, round(e_test_at_peak, 6), m_interp, b_dd]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3183618"}]}