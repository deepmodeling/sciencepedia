{"hands_on_practices": [{"introduction": "A common frustration when training neural networks is watching your loss suddenly become `inf` or `NaN`. This practice explores a primary cause of such numerical instability: exploding gradients. When target values in a regression task are extremely large, the squared error loss can produce astronomical values that exceed the representation limits of standard floating-point numbers, causing training to collapse. This exercise provides a hands-on diagnosis of this issue in a controlled setting and lets you implement two canonical remedies: standardizing the targets and normalizing the gradients, giving you foundational skills for building stable training pipelines. [@problem_id:3178853]", "problem": "You are asked to design and analyze a minimal supervised learning regression experiment that demonstrates how the magnitude of gradients under squared Euclidean loss (also known as $L_2$ loss) can become numerically unstable when target values are extremely large, and how two standard remedies—target standardization and gradient normalization—prevent such instabilities. Work in the following mathematically defined setting.\n\nDataset construction and model:\n- Consider a scalar linear model with parameter $w \\in \\mathbb{R}$ and predictions $\\hat{y}_i = w x_i$.\n- Construct a deterministic dataset of $N$ points with $N = 200$. Let $x_i$ be evenly spaced in the interval $\\left[-5, 5\\right]$ for $i = 1, \\dots, N$.\n- Fix a ground-truth scalar $w_\\star = 2$. For a given scale $s > 0$, define targets by $y_i = s \\cdot w_\\star \\cdot x_i$ for each $i$.\n- Use single-precision floating point ($32$-bit, IEEE $754$) for all training computations to make finite-precision effects observable.\n\nLoss and gradient:\n- Use the mean squared error (squared $L_2$ loss) $$J(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\hat{y}_i - y_i\\right)^2.$$ \n- Starting from $w^{(0)} = 0$, perform $T$ steps of gradient descent with constant step size (learning rate) $\\alpha$:\n  $$w^{(t+1)} = w^{(t)} - \\alpha \\cdot \\nabla J\\left(w^{(t)}\\right), \\quad t = 0, 1, \\dots, T-1.$$\n- You must begin from the fundamental definitions and derive the analytic expression for $\\nabla J(w)$ for this model before implementing it. Do not assume any “shortcut” formula.\n\nInstability definition and diagnostics:\n- A run is considered unstable if at any iteration any of the following occurs in the training state computed in single precision: a non-finite number (Not-a-Number or infinity) appears in the loss, the parameter, or the gradient. This corresponds to detecting numerical overflow or invalid operations.\n- To assess monotonic improvement without causing overflow in diagnostics, also compute a scale-invariant proxy loss\n  $$\\tilde{J}(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\frac{\\hat{y}_i - y_i}{M}\\right)^2,$$\n  where $M = \\max_i |y_i|$ is computed in double precision ($64$-bit) prior to training and then treated as a constant; evaluating $\\tilde{J}(w)$ uses single precision for the arithmetic involving $w$, $x_i$, and $y_i$, but divides by the double-precision constant $M$ cast to single precision. This proxy does not alter training and avoids overflow in the diagnostic itself. A run is considered stably improving if it remains finite and $\\tilde{J}(w)$ is nonincreasing (up to a tolerance $\\varepsilon = 10^{-6}$) over iterations and strictly decreases at least once.\n\nRemedies to implement:\n- Target standardization: Compute the standardized targets $y^{\\mathrm{std}}_i = \\left(y_i - \\mu_y\\right)/\\sigma_y$ with $\\mu_y$ the mean of $y_i$ and $\\sigma_y$ the standard deviation of $y_i$. To avoid overflow while computing these statistics for very large $s$, compute $\\mu_y$ and $\\sigma_y$ in double precision ($64$-bit IEEE $754$) and then cast $y^{\\mathrm{std}}_i$ to single precision for training. Train the same scalar linear model on $(x_i, y^{\\mathrm{std}}_i)$ via gradient descent in single precision with the same $\\alpha$ and $T$.\n- Gradient normalization by per-sample clipping: For the original (unstandardized) targets, compute the per-sample contribution to the gradient of the loss with respect to $w$, clip each contribution to a chosen magnitude prior to summation, and then average to form the batch gradient. All operations here, including the clipping and the summation, must be in single precision.\n\nTest suite:\nUse the following parameter values to produce four runs that collectively test unprotected instability, both remedies, and a benign baseline.\n- Dataset size and inputs: $N = 200$, $x_i$ evenly spaced in $\\left[-5, 5\\right]$.\n- Ground truth and initialization: $w_\\star = 2$, $w^{(0)} = 0$.\n- Training hyperparameters: learning rate $\\alpha = 0.05$, iterations $T = 60$, tolerance $\\varepsilon = 10^{-6}$.\n- Large-scale dataset (intentionally hazardous in single precision): $s_{\\mathrm{large}} = 10^{36}$.\n- Small-scale dataset (benign): $s_{\\mathrm{small}} = 1$.\n- Gradient clipping threshold for per-sample normalization: $g_{\\mathrm{clip}} = 10^{34}$.\n\nFor each run, implement full-batch gradient descent under the following four scenarios:\n- Scenario A (Unstandardized, large scale): train on $y_i = s_{\\mathrm{large}} \\cdot w_\\star \\cdot x_i$ with naive full-batch gradient (no normalization).\n- Scenario B (Target standardized, large scale): compute standardized targets from the same large-scale $y_i$ using double-precision statistics, cast to single precision, then train with naive gradient on the standardized targets.\n- Scenario C (Per-sample gradient clipping, large scale): train on the same large-scale $y_i$ with per-sample gradient contributions clipped to magnitude $g_{\\mathrm{clip}}$ in single precision before averaging.\n- Scenario D (Unstandardized, small scale): train on $y_i = s_{\\mathrm{small}} \\cdot w_\\star \\cdot x_i$ with naive full-batch gradient (no normalization).\n\nAnswer specification:\n- For Scenario A, output a boolean indicating whether instability occurred (true if unstable).\n- For Scenario B, output a boolean indicating whether the run is stably improving as defined above (true if stable and improving).\n- For Scenario C, output a boolean indicating whether the run is stably improving as defined above (true if stable and improving).\n- For Scenario D, output a boolean indicating whether the run is stably improving as defined above (true if stable and improving).\n\nFinal output format:\nYour program should produce a single line of output containing the four boolean results in the order $\\left[\\text{A}, \\text{B}, \\text{C}, \\text{D}\\right]$ as a comma-separated list enclosed in square brackets, for example, $[{\\tt True},{\\tt False},{\\tt True},{\\tt True}]$.", "solution": "The problem statement is a well-defined numerical experiment designed to investigate the effects of finite-precision arithmetic on gradient-based optimization in a supervised learning context. It is scientifically grounded, objective, and provides all necessary specifications to be solved. Therefore, it is a valid problem.\n\nWe proceed with a complete solution, starting from first principles.\n\n### Fundamental Definitions and Gradient Derivation\n\nThe analysis is based on a scalar linear model where predictions $\\hat{y}_i$ for inputs $x_i$ are generated by a single parameter $w \\in \\mathbb{R}$:\n$$\n\\hat{y}_i = w x_i\n$$\nThe objective is to minimize the mean squared error (MSE) loss function, $J(w)$, over a dataset of $N$ points $(x_i, y_i)$:\n$$\nJ(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\hat{y}_i - y_i\\right)^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\left(w x_i - y_i\\right)^2\n$$\nThe parameter $w$ is updated via gradient descent. The gradient, $\\nabla J(w)$, is the derivative of the loss function with respect to the parameter $w$. We derive this from the definition of $J(w)$:\n$$\n\\nabla J(w) = \\frac{dJ}{dw} = \\frac{d}{dw} \\left( \\frac{1}{N}\\sum_{i=1}^{N} (w x_i - y_i)^2 \\right)\n$$\nBy linearity of differentiation, we can move the derivative inside the summation:\n$$\n\\nabla J(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{d}{dw} (w x_i - y_i)^2\n$$\nApplying the chain rule, where the outer function is $u^2$ and the inner function is $u(w) = w x_i - y_i$:\n$$\n\\frac{d}{dw} (w x_i - y_i)^2 = 2(w x_i - y_i) \\cdot \\frac{d}{dw}(w x_i - y_i) = 2(w x_i - y_i) \\cdot x_i\n$$\nSubstituting this back gives the full-batch gradient expression:\n$$\n\\nabla J(w) = \\frac{2}{N}\\sum_{i=1}^{N} (w x_i - y_i) x_i\n$$\nThe gradient descent update rule for $w$ at iteration $t$ with learning rate $\\alpha$ is:\n$$\nw^{(t+1)} = w^{(t)} - \\alpha \\cdot \\nabla J\\left(w^{(t)}\\right)\n$$\nAll training computations are performed using single-precision floating-point numbers ($32$-bit IEEE $754$), which have a finite range and precision. The maximum representable finite value is approximately $3.4 \\times 10^{38}$. Operations resulting in larger values will lead to numerical overflow, producing infinity (`inf`).\n\n### Analysis of Scenarios\n\n**Scenario A (Unstandardized, large scale):**\nHere, the targets are $y_i = s_{\\mathrm{large}} \\cdot w_\\star \\cdot x_i$, with $s_{\\mathrm{large}} = 10^{36}$ and $w_\\star = 2$. The maximum absolute value of $x_i$ is $5$.\nThe maximum magnitude of a target value is $|y_i|_{\\max} = 10^{36} \\cdot 2 \\cdot 5 = 10^{37}$. This value is representable in single precision.\nHowever, instability arises during the computation of the loss and gradient. At the first iteration ($t=0$), $w^{(0)} = 0$. The squared error for a single sample is:\n$$\n\\left(w^{(0)} x_i - y_i\\right)^2 = (-y_i)^2 = y_i^2\n$$\nFor the sample with the largest target magnitude, this becomes $(10^{37})^2 = 10^{74}$. This vastly exceeds the single-precision limit of $\\approx 3.4 \\times 10^{38}$, causing an overflow to `inf`. The loss $J(w^{(0)})$ will therefore be `inf`.\nSimilarly, the per-sample contribution to the unsummed gradient is $2(w^{(0)}x_i - y_i)x_i = -2y_ix_i$. The maximum magnitude is $|-2 \\cdot (10^{36} \\cdot 2 \\cdot 5) \\cdot 5| = 10^{38}$. While this individual term might be representable, summing many such large numbers can also lead to overflow. Crucially, the overflow in the loss calculation is sufficient to classify the run as unstable.\n**Expected Outcome: `True` (instability occurs).**\n\n**Scenario B (Target standardized, large scale):**\nThis remedy preemptively rescales the targets. Standardization is performed using double-precision ($64$-bit) arithmetic to avoid overflow during the computation of statistics. The standardized targets are:\n$$\ny^{\\mathrm{std}}_i = \\frac{y_i - \\mu_y}{\\sigma_y}\n$$\nwhere $\\mu_y$ and $\\sigma_y$ are the mean and standard deviation of the original large-scale targets $y_i$. Since $x_i$ is symmetric around $0$, the mean $\\mu_y = \\mathbb{E}[s w_\\star x_i]$ is $0$. The standard deviation is $\\sigma_y = |s w_\\star| \\sigma_x$.\nThe training is then performed on the new dataset $(x_i, y^{\\mathrm{std}}_i)$. The new target for the parameter $w$ is $w^{\\mathrm{std}}_\\star = (s w_\\star)/\\sigma_y = \\text{sign}(s w_\\star)/\\sigma_x$. With the given $x_i$, $\\sigma_x \\approx 2.89$, so $w^{\\mathrm{std}}_\\star \\approx 1/2.89 \\approx 0.346$.\nAll values in the training process—parameters, targets, predictions, loss, gradients—are now of a small, manageable magnitude. The learning rate $\\alpha=0.05$ is appropriate for this well-conditioned convex problem, ensuring monotonic convergence. The run will be finite. The proxy loss $\\tilde{J}(w)$, which measures performance on the original scale, will be non-increasing and will strictly decrease from its initial value.\n**Expected Outcome: `True` (run is stably improving).**\n\n**Scenario C (Per-sample gradient clipping, large scale):**\nThis remedy addresses the gradient's magnitude directly. The per-sample gradient contribution is $g_i(w) = 2(wx_i - y_i)x_i$.\nAt $t=0$, $w^{(0)}=0$, so $g_i(w^{(0)}) = -2y_ix_i = -2(s w_\\star x_i)x_i = -2 s w_\\star x_i^2$. The magnitude of these contributions can be as large as $|-2 \\cdot 10^{36} \\cdot 2 \\cdot 5^2| = 10^{38}$, which is near the single-precision limit but may not overflow on its own.\nHowever, these values are clipped to the range $[-g_{\\mathrm{clip}}, g_{\\mathrm{clip}}]$, where $g_{\\mathrm{clip}} = 10^{34}$. Since $|g_i(w^{(0)})| > g_{\\mathrm{clip}}$, every non-zero contribution is clipped. The gradient becomes $\\nabla J(w) \\approx -g_{\\mathrm{clip}}$.\nThe first update is $w^{(1)} = w^{(0)} - \\alpha \\cdot \\nabla J(w^{(0)}) \\approx 0 - 0.05 \\cdot (-10^{34}) = 5 \\times 10^{32}$.\nThe parameter $w$ updates remain finite. Although the gradient information is saturated, the direction is correct (increasing $w$ towards the positive target $s w_\\star$). The loss will decrease at each step, albeit sub-optimally. The run will remain finite and the proxy loss will be non-increasing.\n**Expected Outcome: `True` (run is stably improving).**\n\n**Scenario D (Unstandardized, small scale):**\nThis is the benign baseline. The scale is $s_{\\mathrm{small}}=1$. The targets are $y_i = 2x_i$. All values are small. The maximum target magnitude is $10$. There is no risk of numerical overflow. The problem is identical in its geometry to Scenario B, just with a different scaling of the target parameter ($w_\\star=2$ instead of $w^{\\mathrm{std}}_\\star \\approx 0.346$). With a safe learning rate, gradient descent will proceed without issue, exhibiting monotonic convergence.\n**Expected Outcome: `True` (run is stably improving).**", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a supervised learning regression experiment to demonstrate\n    numerical instability and remedies in gradient descent.\n    \"\"\"\n    # Global parameters from the problem\n    N = 200\n    W_STAR = 2.0\n    W0 = 0.0\n    ALPHA = 0.05\n    T = 60\n    EPSILON = 1e-6\n    S_LARGE = 1e36\n    S_SMALL = 1.0\n    G_CLIP = 1e34\n\n    # Data types\n    F32 = np.float32\n    F64 = np.float64\n\n    # Common dataset generation\n    x_64 = np.linspace(-5, 5, N, dtype=F64)\n    x_32 = x_64.astype(F32)\n\n    y_large_64 = S_LARGE * W_STAR * x_64\n    y_large_32 = y_large_64.astype(F32)\n\n    y_small_64 = S_SMALL * W_STAR * x_64\n    y_small_32 = y_small_64.astype(F32)\n\n    def run_scenario_A():\n        w = F32(W0)\n        x = x_32\n        y = y_large_32\n        \n        for _ in range(T):\n            if not np.isfinite(w): return True\n            y_hat = w * x\n            errors = y_hat - y\n            loss = np.mean(errors**2)\n            if not np.isfinite(loss): return True\n            grad = np.mean(F32(2.0) * errors * x)\n            if not np.isfinite(grad): return True\n            w = w - F32(ALPHA) * grad\n        \n        return not np.isfinite(w)\n\n    def run_scenario_B():\n        mu_y_64 = np.mean(y_large_64)\n        sigma_y_64 = np.std(y_large_64)\n        \n        y_std_64 = (y_large_64 - mu_y_64) / sigma_y_64 if sigma_y_64 != 0 else y_large_64 - mu_y_64\n        y_train = y_std_64.astype(F32)\n        x = x_32\n        \n        M_64 = np.max(np.abs(y_large_64))\n        M_32 = F32(M_64)\n        mu_y_32 = F32(mu_y_64)\n        sigma_y_32 = F32(sigma_y_64)\n\n        w = F32(W0)\n        proxy_losses = []\n        is_finite_run = True\n\n        for t in range(T + 1):\n            w_unstd = w * sigma_y_32\n            y_hat_unstd = w_unstd * x + mu_y_32\n            errors_orig_scale = y_hat_unstd - y_large_32\n            \n            proxy_loss = np.mean((errors_orig_scale / M_32)**2) if M_32 != 0 else np.mean(errors_orig_scale**2)\n            \n            if not np.isfinite(proxy_loss):\n                is_finite_run = False\n                break\n            proxy_losses.append(proxy_loss)\n            \n            if t == T: break\n\n            y_hat = w * x\n            errors = y_hat - y_train\n            loss = np.mean(errors**2)\n            grad = np.mean(F32(2.0) * errors * x)\n            \n            if not (np.isfinite(w) and np.isfinite(loss) and np.isfinite(grad)):\n                is_finite_run = False\n                break\n            w = w - F32(ALPHA) * grad\n        \n        if not is_finite_run: return False\n\n        is_non_increasing = all(proxy_losses[i+1] <= proxy_losses[i] + EPSILON for i in range(T))\n        has_decreased = any(proxy_losses[i+1] < proxy_losses[i] for i in range(T))\n            \n        return is_non_increasing and has_decreased\n\n    def run_scenario_C():\n        w = F32(W0)\n        x = x_32\n        y = y_large_32\n        M_64 = np.max(np.abs(y_large_64))\n        M_32 = F32(M_64)\n        clip_val = F32(G_CLIP)\n        \n        proxy_losses = []\n        is_finite_run = True\n\n        for t in range(T + 1):\n            y_hat = w * x\n            errors = y_hat - y\n            proxy_loss = np.mean((errors / M_32)**2) if M_32 != 0 else np.mean(errors**2)\n            if not np.isfinite(proxy_loss):\n                is_finite_run = False\n                break\n            proxy_losses.append(proxy_loss)\n\n            if t == T: break\n\n            loss = np.mean(errors**2)\n            grad_contribs = F32(2.0) * errors * x\n            clipped_contribs = np.clip(grad_contribs, -clip_val, clip_val)\n            grad = np.mean(clipped_contribs)\n\n            if not (np.isfinite(w) and np.isfinite(loss) and np.isfinite(grad)):\n                is_finite_run = False\n                break\n            w = w - F32(ALPHA) * grad\n        \n        if not is_finite_run: return False\n\n        is_non_increasing = all(proxy_losses[i+1] <= proxy_losses[i] + EPSILON for i in range(T))\n        has_decreased = any(proxy_losses[i+1] < proxy_losses[i] for i in range(T))\n            \n        return is_non_increasing and has_decreased\n\n    def run_scenario_D():\n        w = F32(W0)\n        x = x_32\n        y = y_small_32\n        M_64 = np.max(np.abs(y_small_64))\n        M_32 = F32(M_64)\n        \n        proxy_losses = []\n        is_finite_run = True\n\n        for t in range(T + 1):\n            y_hat = w * x\n            errors = y_hat - y\n            proxy_loss = np.mean((errors / M_32)**2) if M_32 != 0 else np.mean(errors**2)\n            if not np.isfinite(proxy_loss):\n                is_finite_run = False\n                break\n            proxy_losses.append(proxy_loss)\n\n            if t == T: break\n            \n            loss = np.mean(errors**2)\n            grad = np.mean(F32(2.0) * errors * x)\n\n            if not (np.isfinite(w) and np.isfinite(loss) and np.isfinite(grad)):\n                is_finite_run = False\n                break\n            w = w - F32(ALPHA) * grad\n\n        if not is_finite_run: return False\n\n        is_non_increasing = all(proxy_losses[i+1] <= proxy_losses[i] + EPSILON for i in range(T))\n        has_decreased = any(proxy_losses[i+1] < proxy_losses[i] for i in range(T))\n        \n        return is_non_increasing and has_decreased\n\n    results = [\n        run_scenario_A(),\n        run_scenario_B(),\n        run_scenario_C(),\n        run_scenario_D(),\n    ]\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3178853"}, {"introduction": "Real-world datasets are rarely perfect and often contain outliers or corrupted labels. While the Mean Squared Error ($L_2$ loss) is a default choice for regression, its quadratic nature makes it extremely sensitive to these outliers, which can severely skew the model. This practice explores how the very design of the loss function can be a powerful tool for robustness. By analytically comparing the behavior of the Absolute Error ($L_1$) loss, the Huber loss, and the generalized Charbonnier loss in the face of adversarial noise, you will gain a deeper understanding of how to build models that are inherently resilient to imperfect data. [@problem_id:3178857]", "problem": "Consider a supervised learning regression task under empirical risk minimization, where the objective is to choose a constant prediction $\\hat{y} \\in \\mathbb{R}$ that minimizes the expected loss over a noisy label model. Let the true label be $y = 0$ and the observed label be $y_{\\text{obs}} = y + \\eta$, where the adversarial label noise $\\eta$ is defined by a two-point distribution: with probability $1 - p$, $\\eta = 0$, and with probability $p$, $\\eta = \\delta$. The scalar $p \\in [0,1]$ is a given probability expressed as a decimal or a fraction, and $\\delta > 0$ is a given magnitude of the adversarial shift.\n\nDefine the per-sample residual as $r = y_{\\text{obs}} - \\hat{y}$ and the expected risk as\n$$\nR(\\hat{y};p,\\delta,\\ell) = \\mathbb{E}[\\ell(r)] = (1-p)\\,\\ell(-\\hat{y}) + p\\,\\ell(\\delta - \\hat{y}),\n$$\nwhere $\\ell$ is a chosen loss function. Consider the following three losses:\n1. The absolute error loss (commonly called the $\\mathcal{L}_1$ loss): $\\ell_{\\mathcal{L}_1}(r) = |r|$.\n2. The Huber loss with threshold $\\kappa > 0$: \n$$\n\\ell_{\\text{Huber}}(r;\\kappa) = \n\\begin{cases}\n\\dfrac{1}{2} r^2, & \\text{if } |r| \\le \\kappa,\\\\\n\\kappa\\left(|r| - \\dfrac{1}{2}\\kappa\\right), & \\text{if } |r| > \\kappa.\n\\end{cases}\n$$\nFor this problem use $\\kappa = 10$.\n3. The generalized Charbonnier loss with parameters $\\epsilon > 0$ and $\\beta \\in (0,1)$:\n$$\n\\ell_{\\text{GC}}(r;\\epsilon,\\beta) = \\left(r^2 + \\epsilon^2\\right)^{\\beta}.\n$$\nFor this problem use $\\epsilon = 1$ and $\\beta = 0.45$.\n\nFor each loss, define the optimal constant predictor\n$$\n\\hat{y}^*_{\\ell}(p,\\delta) \\in \\arg\\min_{\\hat{y} \\in \\mathbb{R}} R(\\hat{y};p,\\delta,\\ell),\n$$\nand define the robustness score for the given loss under $(p,\\delta)$ as the absolute bias\n$$\nb_{\\ell}(p,\\delta) = \\left|\\hat{y}^*_{\\ell}(p,\\delta) - y\\right| = \\left|\\hat{y}^*_{\\ell}(p,\\delta)\\right|.\n$$\n\nYour task is to implement a program that, for each $(p,\\delta)$ in the test suite below, computes the three robustness scores $b_{\\mathcal{L}_1}(p,\\delta)$, $b_{\\text{Huber}}(p,\\delta)$, and $b_{\\text{GC}}(p,\\delta)$ corresponding to $\\ell_{\\mathcal{L}_1}$, $\\ell_{\\text{Huber}}$ with $\\kappa = 10$, and $\\ell_{\\text{GC}}$ with $\\epsilon = 1$ and $\\beta = 0.45$. For the $\\mathcal{L}_1$ loss, resolve the tie at $p = 0.5$ by choosing $\\hat{y}^*_{\\mathcal{L}_1}(p,\\delta) = 0$.\n\nTest suite:\n- Case $1$: $p = 0.1$, $\\delta = 20$.\n- Case $2$: $p = 0.4$, $\\delta = 50$.\n- Case $3$: $p = 0.6$, $\\delta = 50$.\n- Case $4$: $p = 0.9$, $\\delta = 100$.\n- Case $5$: $p = 0.0$, $\\delta = 10$.\n- Case $6$: $p = 1.0$, $\\delta = 10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this outer list corresponds to one test case, ordered as above, and is itself a list of three floating-point numbers $[b_{\\mathcal{L}_1}, b_{\\text{Huber}}, b_{\\text{GC}}]$ for that case. For example, the output format must be\n$$\n[\\,[b_{\\mathcal{L}_1}^{(1)},b_{\\text{Huber}}^{(1)},b_{\\text{GC}}^{(1)}],\\,[b_{\\mathcal{L}_1}^{(2)},b_{\\text{Huber}}^{(2)},b_{\\text{GC}}^{(2)}],\\,\\dots,\\, [b_{\\mathcal{L}_1}^{(6)},b_{\\text{Huber}}^{(6)},b_{\\text{GC}}^{(6)}]\\,].\n$$\nNo physical units apply. Angles are not involved. Probabilities $p$ are given as decimals. The output values are floating-point numbers.", "solution": "The objective is to find the optimal constant predictor $\\hat{y}^*$ that minimizes the expected risk $R(\\hat{y})$. The risk is defined as:\n$$R(\\hat{y};p,\\delta,\\ell) = \\mathbb{E}[\\ell(y_{\\text{obs}} - \\hat{y})] = (1-p)\\,\\ell(-\\hat{y}) + p\\,\\ell(\\delta - \\hat{y})$$\nwhere $y_{\\text{obs}}$ takes the value $0$ with probability $1-p$ and $\\delta$ with probability $p$. The true label is $y=0$. The optimal predictor $\\hat{y}^*$ is a value in $\\mathbb{R}$ that minimizes $R(\\hat{y})$. For the considered loss functions, which are convex, this minimum can be found by setting the derivative of the risk with respect to $\\hat{y}$ to zero.\n\nThe derivative of the risk is obtained using the chain rule:\n$$\\frac{d R}{d \\hat{y}} = (1-p) \\frac{d}{d\\hat{y}}\\ell(-\\hat{y}) + p \\frac{d}{d\\hat{y}}\\ell(\\delta - \\hat{y})$$\n$$\\frac{d R}{d \\hat{y}} = (1-p) \\ell'(-\\hat{y}) \\cdot (-1) + p \\ell'(\\delta - \\hat{y}) \\cdot (-1)$$\n$$\\frac{d R}{d \\hat{y}} = - \\left[ (1-p) \\ell'(-\\hat{y}) + p \\ell'(\\delta - \\hat{y}) \\right]$$\nSetting this derivative to zero gives the optimality condition:\n$$(1-p) \\ell'(-\\hat{y}^*) = -p \\ell'(\\delta - \\hat{y}^*)$$\nWe will solve this equation for $\\hat{y}^*$ for each of the three specified loss functions. The robustness score is then calculated as the absolute bias $b_{\\ell}(p,\\delta) = |\\hat{y}^*_{\\ell}(p,\\delta)|$.\n\n**1. Absolute Error Loss ($\\ell_{\\mathcal{L}_1}(r) = |r|$)**\n\nThe derivative of the absolute error loss is the sign function, $\\ell'_{\\mathcal{L}_1}(r) = \\text{sgn}(r)$. The risk function is $R(\\hat{y}) = (1-p)|-\\hat{y}| + p|\\delta-\\hat{y}|$. The minimizer of this function is the weighted median of the data points $0$ and $\\delta$. A subgradient-based analysis confirms this. The subgradient of $R(\\hat{y})$ is:\n$$\\partial R(\\hat{y}) = (1-p)\\,\\text{sgn}(\\hat{y}) - p\\,\\text{sgn}(\\delta-\\hat{y})$$\nWe seek $\\hat{y}^*$ such that $0 \\in \\partial R(\\hat{y}^*)$. Assuming $0 < \\hat{y}^* < \\delta$, the derivative is $1-2p$.\n- If $p < 0.5$, then $1-2p > 0$. The risk is increasing on $(0, \\delta)$, so the minimum is at $\\hat{y}^*_{\\mathcal{L}_1} = 0$.\n- If $p > 0.5$, then $1-2p < 0$. The risk is decreasing on $(0, \\delta)$, so the minimum is at $\\hat{y}^*_{\\mathcal{L}_1} = \\delta$.\n- If $p = 0.5$, the risk is flat on $[0, \\delta]$, meaning any value in this interval is a minimizer. The problem specifies the tie-breaking rule $\\hat{y}^*_{\\mathcal{L}_1} = 0$.\nTherefore, the optimal predictor is:\n$$\\hat{y}^*_{\\mathcal{L}_1}(p,\\delta) = \\begin{cases} 0 & \\text{if } p \\le 0.5 \\\\ \\delta & \\text{if } p > 0.5 \\end{cases}$$\nThe robustness score is $b_{\\mathcal{L}_1}(p,\\delta) = |\\hat{y}^*_{\\mathcal{L}_1}(p,\\delta)|$.\n\n**2. Huber Loss ($\\ell_{\\text{Huber}}(r; \\kappa)$)**\n\nWith $\\kappa=10$, the derivative of the Huber loss is $\\ell'_{\\text{Huber}}(r;10) = \\text{clip}(r, -10, 10)$. The optimality condition is:\n$$(1-p)\\,\\text{clip}(-\\hat{y}^*, -10, 10) = -p\\,\\text{clip}(\\delta - \\hat{y}^*, -10, 10)$$\nWe analyze this piecewise. We assume $0 < \\hat{y}^* < \\delta$.\n- **Case 1**: Both arguments are in the quadratic region: $|\\hat{y}^*| \\le 10$ and $|\\delta-\\hat{y}^*| \\le 10$.\nThe condition becomes $(1-p)(-\\hat{y}^*) = -p(\\delta - \\hat{y}^*)$, which simplifies to $\\hat{y}^* = p\\delta$. This is valid if $p\\delta \\le 10$ and $\\delta(1-p) \\le 10$.\n\n- **Case 2**: Left term is linear, right is quadratic: $\\hat{y}^* > 10$ and $\\delta-\\hat{y}^* \\le 10$.\nThe condition becomes $(1-p)(-10) = -p(\\delta - \\hat{y}^*)$, which solves to $\\hat{y}^* = \\delta - \\frac{10(1-p)}{p}$. This case applies if $p > 0.5$ and the result is consistent with the assumptions, which holds if $p\\delta > 10$.\n\n- **Case 3**: Left term is quadratic, right is linear: $\\hat{y}^* \\le 10$ and $\\delta-\\hat{y}^* > 10$.\nThe condition becomes $(1-p)(-\\hat{y}^*) = -p(10)$, which solves to $\\hat{y}^* = \\frac{10p}{1-p}$. This case applies if $p < 0.5$ and $\\delta(1-p)>10$.\n\nCombining these cases gives the solution for $\\hat{y}^*_{\\text{Huber}}$:\n- If $p \\le 0.5$:\n  $$\\hat{y}^*_{\\text{Huber}} = \\begin{cases} p\\delta & \\text{if } \\delta(1-p) \\le 10 \\\\ \\frac{10p}{1-p} & \\text{if } \\delta(1-p) > 10 \\end{cases}$$\n- If $p > 0.5$:\n  $$\\hat{y}^*_{\\text{Huber}} = \\begin{cases} p\\delta & \\text{if } p\\delta \\le 10 \\\\ \\delta - \\frac{10(1-p)}{p} & \\text{if } p\\delta > 10 \\end{cases}$$\nFor the edge cases $p=0$ and $p=1$, this logic correctly yields $\\hat{y}^*=0$ and $\\hat{y}^*=\\delta$, respectively. The robustness score is $b_{\\text{Huber}}(p,\\delta) = |\\hat{y}^*_{\\text{Huber}}(p,\\delta)|$.\n\n**3. Generalized Charbonnier Loss ($\\ell_{\\text{GC}}(r; \\epsilon, \\beta)$)**\n\nWith $\\epsilon=1$ and $\\beta=0.45$, the loss is $\\ell_{\\text{GC}}(r) = (r^2+1)^{0.45}$. Its derivative is $\\ell'_{\\text{GC}}(r) = 2\\beta r (r^2+\\epsilon^2)^{\\beta-1} = 0.9 r (r^2+1)^{-0.55}$.\nThe optimality condition is $(1-p)\\ell'_{\\text{GC}}(-\\hat{y}^*) = -p\\ell'_{\\text{GC}}(\\delta - \\hat{y}^*)$. Substituting the derivative gives:\n$$(1-p) [0.9(-\\hat{y}^*)((-\\hat{y}^*)^2+1)^{-0.55}] = -p [0.9(\\delta-\\hat{y}^*)((\\delta-\\hat{y}^*)^2+1)^{-0.55}]$$\nSimplifying and rearranging terms, we get the equation for $\\hat{y}^*$:\n$$(1-p)\\hat{y}^*(\\hat{y}^{*2}+1)^{-0.55} - p(\\delta-\\hat{y}^*)((\\delta-\\hat{y}^*)^2+1)^{-0.55} = 0$$\nThis is a nonlinear equation that cannot be solved in closed form. We must find the root numerically. Let $g(\\hat{y})$ be the left-hand side of the equation.\n- For $p=0$, the equation becomes $\\hat{y}^*(\\hat{y}^{*2}+1)^{-0.55} = 0$, which implies $\\hat{y}^*_{\\text{GC}} = 0$.\n- For $p=1$, the equation becomes $-(\\delta-\\hat{y}^*)((\\delta-\\hat{y}^*)^2+1)^{-0.55} = 0$, which implies $\\hat{y}^*_{\\text{GC}} = \\delta$.\n- For $p \\in (0,1)$, we can observe that $g(0) < 0$ and $g(\\delta) > 0$. Since $g(\\hat{y})$ is continuous and monotonic between $0$ and $\\delta$, a unique root $\\hat{y}^*_{\\text{GC}} \\in (0, \\delta)$ exists. This root can be found using a numerical method like the bisection or Brent's method.\nThe robustness score is $b_{\\text{GC}}(p,\\delta) = |\\hat{y}^*_{\\text{GC}}(p,\\delta)|$.\n\nThe implementation will compute these three scores for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes robustness scores for L1, Huber, and Generalized Charbonnier losses\n    for a set of test cases.\n    \"\"\"\n    \n    # Loss function parameters\n    HUBER_KAPPA = 10.0\n    GC_EPSILON = 1.0\n    GC_BETA = 0.45\n\n    test_cases = [\n        (0.1, 20.0),  # Case 1\n        (0.4, 50.0),  # Case 2\n        (0.6, 50.0),  # Case 3\n        (0.9, 100.0), # Case 4\n        (0.0, 10.0),  # Case 5\n        (1.0, 10.0),  # Case 6\n    ]\n\n    def get_l1_bias(p, delta):\n        \"\"\"Computes the bias for the L1 loss.\"\"\"\n        if p > 0.5:\n            y_star = delta\n        else: # p <= 0.5, including tie-breaking at p = 0.5\n            y_star = 0.0\n        return abs(y_star)\n\n    def get_huber_bias(p, delta, kappa):\n        \"\"\"Computes the bias for the Huber loss.\"\"\"\n        if p <= 0.5:\n            if delta * (1.0 - p) <= kappa:\n                y_star = p * delta\n            else:\n                y_star = (p * kappa) / (1.0 - p)\n        else: # p > 0.5\n            if p * delta <= kappa:\n                y_star = p * delta\n            else:\n                y_star = delta - (kappa * (1.0 - p)) / p\n        return abs(y_star)\n\n    def get_gc_bias(p, delta, epsilon, beta):\n        \"\"\"Computes the bias for the Generalized Charbonnier loss.\"\"\"\n        if p == 0.0:\n            return 0.0\n        if p == 1.0:\n            return float(delta)\n\n        # Define the function whose root we need to find.\n        # This is derived from setting the derivative of the expected risk to zero.\n        def root_function(y_hat):\n            term1 = (1.0 - p) * y_hat * ((y_hat**2 + epsilon**2)**(beta - 1.0))\n            term2 = p * (delta - y_hat) * (((delta - y_hat)**2 + epsilon**2)**(beta - 1.0))\n            return term1 - term2\n\n        # Use Brent's method to find the root in the interval [0, delta].\n        # The function has opposite signs at the interval endpoints for p in (0, 1).\n        y_star = brentq(root_function, 0.0, delta)\n        return abs(y_star)\n\n    results = []\n    for p, delta in test_cases:\n        b_l1 = get_l1_bias(p, delta)\n        b_huber = get_huber_bias(p, delta, HUBER_KAPPA)\n        b_gc = get_gc_bias(p, delta, GC_EPSILON, GC_BETA)\n        \n        results.append([b_l1, b_huber, b_gc])\n\n    # Format the final output string as per requirements.\n    outer_parts = [f\"[{','.join(map(str, inner_list))}]\" for inner_list in results]\n    final_output = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3178857"}, {"introduction": "Beyond simply predicting a value, many applications require that a model's predictions also preserve the relative ordering of the true outcomes. For example, in a product recommendation system, it's more important to rank the best products higher than to predict their exact ratings. This advanced practice guides you in engineering a hybrid loss function that combines a traditional regression loss (Mean Squared Error) with a pairwise ranking penalty. By implementing a gradient descent learner for this custom objective and evaluating it with Kendall's $\\tau$ rank correlation coefficient, you will learn how to tailor your model's training to align with more complex, order-aware performance criteria. [@problem_id:3178841]", "problem": "You are tasked with implementing a batch gradient descent learner for a linear regression model that simultaneously predicts continuous outcomes and enforces order consistency between true and predicted values through a pairwise hinge penalty. Your implementation must be derived from first principles of supervised learning and must output a single metric, Kendall’s $\\tau$ (tau), computed on the training data after optimization.\n\nThe context is supervised learning for regression with a linear predictor. You will start from the following fundamental bases and core definitions:\n\n- Empirical Risk Minimization (ERM): Given data $\\{(x_i, y_i)\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$, learn parameters $w \\in \\mathbb{R}^d$ by minimizing an empirical risk constructed from a well-defined loss.\n- Linear prediction: The model predicts $\\hat y_i = w^\\top x_i$ for each $i$.\n- Mean Squared Error (MSE): The pointwise error is $(y_i - \\hat y_i)^2$, and the empirical average is $\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat y_i)^2$.\n- Pairwise ranking consistency via a hinge penalty: For each unordered pair $\\{i, j\\}$ with $i < j$, the penalty encourages the sign of $y_i - y_j$ to match the sign of $\\hat y_i - \\hat y_j$. The hinge term for a pair is $\\max\\!\\big(0,\\,-(y_i - y_j)(\\hat y_i - \\hat y_j)\\big)$, which is zero for concordant pairs and positive for discordant pairs.\n- Kendall’s $\\tau$: A rank correlation defined as $\\tau = \\frac{C - D}{T}$, where $C$ is the number of concordant pairs, $D$ is the number of discordant pairs, and $T = \\frac{n(n-1)}{2}$ is the total number of unordered pairs. Use the convention that ties contribute zero to the numerator and do not alter the denominator.\n\nYour program must:\n\n1. Construct synthetic datasets for each test case using the following generative process:\n   - Draw features $x_i \\in \\mathbb{R}^d$ independently from a standard normal distribution.\n   - Draw a ground-truth weight vector $w^\\star \\in \\mathbb{R}^d$ from a standard normal distribution unless explicitly specified to be the zero vector.\n   - Draw independent noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n   - Set responses $y_i = (w^\\star)^\\top x_i + \\epsilon_i$.\n   - All random draws must be reproducible via a given integer seed.\n\n2. Minimize the combined empirical risk\n   $$\n   \\mathcal{L}(w) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n (y_i - w^\\top x_i)^2 \\;+\\; \\beta \\cdot \\frac{1}{T} \\sum_{1 \\le i < j \\le n} \\max\\!\\big(0,\\,-(y_i - y_j)\\big(w^\\top x_i - w^\\top x_j\\big)\\big),\n   $$\n   where $\\beta$ is a nonnegative weight that controls the strength of the ranking penalty, and $T = \\frac{n(n-1)}{2}$.\n\n3. Use batch gradient descent with a fixed learning rate to update $w$ for a fixed number of steps. You must derive and implement the gradient updates directly from the definitions above. Do not use any automatic differentiation or external optimization packages.\n\n4. After optimization for each test case, compute Kendall’s $\\tau$ between the true responses $\\{y_i\\}$ and the predictions $\\{\\hat y_i\\}$ on the training data.\n\n5. Produce a single line of output containing the Kendall’s $\\tau$ values for all test cases as a comma-separated list enclosed in square brackets, for example, $\\texttt{[0.123,0.456,0.789]}$.\n\nTest Suite:\nImplement the following five test cases, each specified by a tuple $(n, d, \\sigma, \\beta, \\text{lr}, \\text{steps}, \\text{seed}, w^\\star\\!\\!=\\!0)$, where $n$ is the number of samples, $d$ is the feature dimension, $\\sigma$ is the noise standard deviation, $\\beta$ is the ranking penalty weight, $\\text{lr}$ is the learning rate, $\\text{steps}$ is the number of gradient steps, $\\text{seed}$ is the random seed, and $w^\\star\\!\\!=\\!0$ indicates whether the ground-truth weight vector is the zero vector:\n\n- Case A (happy path): $(n, d, \\sigma, \\beta, \\text{lr}, \\text{steps}, \\text{seed}, w^\\star\\!\\!=\\!0) = (\\,30,\\,3,\\,0.2,\\,1.0,\\,0.05,\\,600,\\,7,\\,\\text{False}\\,)$\n- Case B (high noise): $(\\,30,\\,3,\\,1.2,\\,1.0,\\,0.05,\\,600,\\,11,\\,\\text{False}\\,)$\n- Case C (no ranking term baseline): $(\\,30,\\,3,\\,0.2,\\,0.0,\\,0.05,\\,600,\\,7,\\,\\text{False}\\,)$\n- Case D (degenerate responses, constant $y$): $(\\,20,\\,4,\\,0.0,\\,1.0,\\,0.1,\\,300,\\,19,\\,\\text{True}\\,)$\n- Case E (boundary condition, minimal pairs): $(\\,2,\\,2,\\,0.0,\\,1.0,\\,0.1,\\,100,\\,3,\\,\\text{False}\\,)$\n\nFinal Output Format:\nYour program should produce a single line of output containing the Kendall’s $\\tau$ results for the five cases as a comma-separated list enclosed in square brackets, in the exact order of cases A through E, for example, $\\texttt{[0.XXX,0.YYY,0.ZZZ,0.UU,0.VV]}$. No additional text should be printed.", "solution": "The user-provided problem has been assessed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique and verifiable solution. We will now proceed with a full derivation and implementation.\n\nThe problem requires the implementation of a batch gradient descent algorithm to train a linear regression model. The objective function to be minimized is a composite of the Mean Squared Error (MSE) and a pairwise ranking penalty. After training, the model's performance on the training data is evaluated using Kendall's $\\tau$ rank correlation coefficient.\n\nThe model is a linear predictor, defined as $\\hat{y}_i = w^\\top x_i$, where $x_i \\in \\mathbb{R}^d$ is a vector of features and $w \\in \\mathbb{R}^d$ is the weight vector to be learned.\n\nThe empirical risk, which serves as the loss function $\\mathcal{L}(w)$, is given by:\n$$\n\\mathcal{L}(w) = \\underbrace{\\frac{1}{n}\\sum_{i=1}^n (y_i - w^\\top x_i)^2}_{\\mathcal{L}_{MSE}(w)} + \\beta \\cdot \\underbrace{\\frac{1}{T} \\sum_{1 \\le i < j \\le n} \\max\\!\\big(0,\\,-(y_i - y_j)(w^\\top x_i - w^\\top x_j)\\big)}_{\\mathcal{L}_{rank}(w)}\n$$\nwhere $n$ is the number of samples, $y_i \\in \\mathbb{R}$ are the true continuous outcomes, $\\beta \\ge 0$ is a hyperparameter balancing the two loss components, and $T = \\frac{n(n-1)}{2}$ is the total number of unique pairs of samples.\n\nThe optimization algorithm is batch gradient descent, which iteratively updates the weight vector $w$ according to the rule:\n$$\nw_{t+1} = w_t - \\eta \\cdot \\nabla_w \\mathcal{L}(w_t)\n$$\nwhere $\\eta$ is the learning rate and $\\nabla_w \\mathcal{L}(w_t)$ is the gradient of the loss function with respect to $w$, evaluated at the current weight vector $w_t$. We will initialize the weight vector as $w_0 = 0$.\n\nDue to the linearity of the gradient operator, we can compute the gradient of each component of the loss function separately:\n$$\n\\nabla_w \\mathcal{L}(w) = \\nabla_w \\mathcal{L}_{MSE}(w) + \\beta \\nabla_w \\mathcal{L}_{rank}(w)\n$$\n\nFirst, we derive the gradient of the MSE term, $\\mathcal{L}_{MSE}(w)$:\n$$\n\\nabla_w \\mathcal{L}_{MSE}(w) = \\nabla_w \\left( \\frac{1}{n}\\sum_{i=1}^n (y_i - w^\\top x_i)^2 \\right)\n$$\n$$\n\\nabla_w \\mathcal{L}_{MSE}(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla_w (y_i - w^\\top x_i)^2\n$$\nUsing the chain rule, where $\\nabla_w (w^\\top x_i) = x_i$:\n$$\n\\nabla_w \\mathcal{L}_{MSE}(w) = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w^\\top x_i) \\cdot (-\\nabla_w (w^\\top x_i)) = \\frac{1}{n} \\sum_{i=1}^n -2(y_i - w^\\top x_i) x_i\n$$\nThis can be expressed compactly in matrix notation. Let $X$ be the $n \\times d$ design matrix where each row is $x_i^\\top$, and $y$ be the column vector of true outcomes. The predictions are $\\hat{y} = Xw$. The gradient is:\n$$\n\\nabla_w \\mathcal{L}_{MSE}(w) = -\\frac{2}{n} X^\\top (y - Xw)\n$$\n\nSecond, we derive the gradient of the ranking penalty term, $\\mathcal{L}_{rank}(w)$. The term involves the hinge loss function, $h(z) = \\max(0, z)$, which is not differentiable at $z=0$. We will use its subgradient. A standard choice for gradient-based methods is to define the derivative as $1$ for $z>0$ and $0$ for $z \\le 0$.\nLet $z_{ij}(w) = -(y_i - y_j)(w^\\top x_i - w^\\top x_j) = -(y_i - y_j)w^\\top(x_i - x_j)$. The ranking loss term for a single pair $\\{i,j\\}$ is $\\max(0, z_{ij}(w))$. Its gradient with respect to $w$ is:\n$$\n\\nabla_w \\max(0, z_{ij}(w)) = \\mathbb{I}(z_{ij}(w) > 0) \\cdot \\nabla_w z_{ij}(w)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The condition $z_{ij}(w) > 0$ is equivalent to $-(y_i-y_j)(\\hat{y}_i-\\hat{y}_j) > 0$, which means the pair $\\{i,j\\}$ is discordant (the predicted order is opposite to the true order).\nThe gradient of $z_{ij}(w)$ with respect to $w$ is:\n$$\n\\nabla_w z_{ij}(w) = \\nabla_w \\big( -(y_i - y_j)w^\\top(x_i - x_j) \\big) = -(y_i - y_j)(x_i - x_j)\n$$\nSumming over all pairs $\\{i, j\\}$ with $i < j$, the gradient of the ranking loss is:\n$$\n\\nabla_w \\mathcal{L}_{rank}(w) = \\frac{1}{T} \\sum_{1 \\le i < j \\le n} \\mathbb{I}\\big( (y_i - y_j)(\\hat{y}_i - \\hat{y}_j) < 0 \\big) \\cdot \\big( -(y_i - y_j)(x_i - x_j) \\big)\n$$\n\nCombining the two components, the full batch gradient is:\n$$\n\\nabla_w \\mathcal{L}(w) = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)x_i - \\frac{\\beta}{T} \\sum_{\\substack{1 \\le i < j \\le n \\\\ \\text{discordant pairs}}} (y_i - y_j)(x_i - x_j)\n$$\nThis gradient is computed using all $n$ samples in each step of the gradient descent algorithm.\n\nFinally, after the optimization process completes, we compute Kendall's Tau. Given the true values $\\{y_i\\}$ and final predictions $\\{\\hat{y}_i\\}$, we categorize each pair $\\{i,j\\}$ as concordant, discordant, or tied.\n- A pair is concordant if $(y_i - y_j)$ and $(\\hat{y}_i - \\hat{y}_j)$ have the same sign. Let $C$ be the count of such pairs.\n- A pair is discordant if they have opposite signs. Let $D$ be the count of such pairs.\n- A pair is tied if $y_i = y_j$ or $\\hat{y}_i = \\hat{y}_j$. Tied pairs are ignored in the numerator summation.\n\nKendall's Tau is then calculated as specified:\n$$\n\\tau = \\frac{C - D}{T} = \\frac{C - D}{\\frac{1}{2}n(n-1)}\n$$\n\nThe implementation will follow these derivations, creating synthetic data for each test case as specified, running the batch gradient descent optimizer, and calculating the final Kendall's $\\tau$ value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a batch gradient descent learner for linear regression with a pairwise \n    hinge penalty and evaluates performance using Kendall's tau.\n    \"\"\"\n    test_cases = [\n        # (n, d, sigma, beta, lr, steps, seed, w_star_is_zero)\n        (30, 3, 0.2, 1.0, 0.05, 600, 7, False),      # Case A: Happy path\n        (30, 3, 1.2, 1.0, 0.05, 600, 11, False),     # Case B: High noise\n        (30, 3, 0.2, 0.0, 0.05, 600, 7, False),      # Case C: No ranking term\n        (20, 4, 0.0, 1.0, 0.1, 300, 19, True),       # Case D: Degenerate responses\n        (2, 2, 0.0, 1.0, 0.1, 100, 3, False),        # Case E: Minimal pairs\n    ]\n\n    results = []\n    for case in test_cases:\n        n, d, sigma, beta, lr, steps, seed, w_star_is_zero = case\n        \n        # 1. Construct synthetic dataset using the generative process.\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(n, d))\n        \n        if w_star_is_zero:\n            w_star = np.zeros(d)\n        else:\n            w_star = rng.standard_normal(size=d)\n            \n        noise = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = X @ w_star + noise\n\n        # 2. Minimize the empirical risk using batch gradient descent.\n        # Initialize weight vector to zeros.\n        w = np.zeros(d)\n        \n        # Total number of pairs for ranking loss normalization.\n        T = n * (n - 1) / 2 if n >= 2 else 0\n\n        for _ in range(steps):\n            y_hat = X @ w\n            \n            # --- Gradient of Mean Squared Error term ---\n            residuals = y - y_hat\n            grad_mse = -2.0 / n * X.T @ residuals\n            \n            # --- Gradient of Pairwise Hinge Penalty term ---\n            grad_rank = np.zeros(d)\n            if beta > 0 and T > 0:\n                # Sum over all discordant pairs {i, j}\n                for i in range(n):\n                    for j in range(i + 1, n):\n                        y_diff = y[i] - y[j]\n                        x_diff = X[i, :] - X[j, :]\n                        y_hat_diff = y_hat[i] - y_hat[j]\n                        \n                        # A pair is discordant if (y_i-y_j)*(y_hat_i-y_hat_j) < 0.\n                        # The subgradient is non-zero only for these pairs.\n                        if y_diff * y_hat_diff < 0:\n                            grad_rank += -(y_diff * x_diff)\n                \n                grad_rank /= T\n\n            # --- Combine gradients and update weights ---\n            grad_total = grad_mse + beta * grad_rank\n            w -= lr * grad_total\n\n        # 3. Compute final predictions after optimization.\n        y_hat_final = X @ w\n        \n        # 4. Compute Kendall’s tau on the training data.\n        tau = 0.0\n        if T > 0:\n            concordant_pairs = 0\n            discordant_pairs = 0\n            for i in range(n):\n                for j in range(i + 1, n):\n                    y_diff = y[i] - y[j]\n                    y_hat_diff = y_hat_final[i] - y_hat_final[j]\n                    \n                    # As per problem, ties contribute zero to the numerator.\n                    if y_diff == 0 or y_hat_diff == 0:\n                        continue\n                    \n                    if y_diff * y_hat_diff > 0:\n                        concordant_pairs += 1\n                    else: # y_diff * y_hat_diff < 0\n                        discordant_pairs += 1\n            \n            tau = (concordant_pairs - discordant_pairs) / T\n\n        results.append(tau)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3178841"}]}